Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=211, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 11816-11871
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_022/1010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_022/1010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_022/1010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00786615
Iteration 2/25 | Loss: 0.00182285
Iteration 3/25 | Loss: 0.00125549
Iteration 4/25 | Loss: 0.00118950
Iteration 5/25 | Loss: 0.00118566
Iteration 6/25 | Loss: 0.00118514
Iteration 7/25 | Loss: 0.00118514
Iteration 8/25 | Loss: 0.00118514
Iteration 9/25 | Loss: 0.00118514
Iteration 10/25 | Loss: 0.00118514
Iteration 11/25 | Loss: 0.00118514
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001185138593427837, 0.001185138593427837, 0.001185138593427837, 0.001185138593427837, 0.001185138593427837]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001185138593427837

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35999572
Iteration 2/25 | Loss: 0.00076900
Iteration 3/25 | Loss: 0.00076898
Iteration 4/25 | Loss: 0.00076898
Iteration 5/25 | Loss: 0.00076898
Iteration 6/25 | Loss: 0.00076898
Iteration 7/25 | Loss: 0.00076898
Iteration 8/25 | Loss: 0.00076898
Iteration 9/25 | Loss: 0.00076898
Iteration 10/25 | Loss: 0.00076898
Iteration 11/25 | Loss: 0.00076898
Iteration 12/25 | Loss: 0.00076897
Iteration 13/25 | Loss: 0.00076898
Iteration 14/25 | Loss: 0.00076897
Iteration 15/25 | Loss: 0.00076897
Iteration 16/25 | Loss: 0.00076897
Iteration 17/25 | Loss: 0.00076897
Iteration 18/25 | Loss: 0.00076897
Iteration 19/25 | Loss: 0.00076897
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0007689749472774565, 0.0007689749472774565, 0.0007689749472774565, 0.0007689749472774565, 0.0007689749472774565]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007689749472774565

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00076897
Iteration 2/1000 | Loss: 0.00003965
Iteration 3/1000 | Loss: 0.00002733
Iteration 4/1000 | Loss: 0.00002119
Iteration 5/1000 | Loss: 0.00002016
Iteration 6/1000 | Loss: 0.00001954
Iteration 7/1000 | Loss: 0.00001921
Iteration 8/1000 | Loss: 0.00001897
Iteration 9/1000 | Loss: 0.00001875
Iteration 10/1000 | Loss: 0.00001860
Iteration 11/1000 | Loss: 0.00001856
Iteration 12/1000 | Loss: 0.00001855
Iteration 13/1000 | Loss: 0.00001846
Iteration 14/1000 | Loss: 0.00001839
Iteration 15/1000 | Loss: 0.00001838
Iteration 16/1000 | Loss: 0.00001836
Iteration 17/1000 | Loss: 0.00001836
Iteration 18/1000 | Loss: 0.00001835
Iteration 19/1000 | Loss: 0.00001835
Iteration 20/1000 | Loss: 0.00001835
Iteration 21/1000 | Loss: 0.00001834
Iteration 22/1000 | Loss: 0.00001834
Iteration 23/1000 | Loss: 0.00001834
Iteration 24/1000 | Loss: 0.00001833
Iteration 25/1000 | Loss: 0.00001833
Iteration 26/1000 | Loss: 0.00001832
Iteration 27/1000 | Loss: 0.00001832
Iteration 28/1000 | Loss: 0.00001831
Iteration 29/1000 | Loss: 0.00001826
Iteration 30/1000 | Loss: 0.00001826
Iteration 31/1000 | Loss: 0.00001826
Iteration 32/1000 | Loss: 0.00001825
Iteration 33/1000 | Loss: 0.00001825
Iteration 34/1000 | Loss: 0.00001825
Iteration 35/1000 | Loss: 0.00001825
Iteration 36/1000 | Loss: 0.00001824
Iteration 37/1000 | Loss: 0.00001824
Iteration 38/1000 | Loss: 0.00001824
Iteration 39/1000 | Loss: 0.00001824
Iteration 40/1000 | Loss: 0.00001824
Iteration 41/1000 | Loss: 0.00001824
Iteration 42/1000 | Loss: 0.00001823
Iteration 43/1000 | Loss: 0.00001823
Iteration 44/1000 | Loss: 0.00001822
Iteration 45/1000 | Loss: 0.00001822
Iteration 46/1000 | Loss: 0.00001822
Iteration 47/1000 | Loss: 0.00001821
Iteration 48/1000 | Loss: 0.00001821
Iteration 49/1000 | Loss: 0.00001819
Iteration 50/1000 | Loss: 0.00001818
Iteration 51/1000 | Loss: 0.00001818
Iteration 52/1000 | Loss: 0.00001818
Iteration 53/1000 | Loss: 0.00001817
Iteration 54/1000 | Loss: 0.00001817
Iteration 55/1000 | Loss: 0.00001817
Iteration 56/1000 | Loss: 0.00001817
Iteration 57/1000 | Loss: 0.00001817
Iteration 58/1000 | Loss: 0.00001817
Iteration 59/1000 | Loss: 0.00001817
Iteration 60/1000 | Loss: 0.00001817
Iteration 61/1000 | Loss: 0.00001817
Iteration 62/1000 | Loss: 0.00001817
Iteration 63/1000 | Loss: 0.00001817
Iteration 64/1000 | Loss: 0.00001817
Iteration 65/1000 | Loss: 0.00001817
Iteration 66/1000 | Loss: 0.00001817
Iteration 67/1000 | Loss: 0.00001817
Iteration 68/1000 | Loss: 0.00001816
Iteration 69/1000 | Loss: 0.00001816
Iteration 70/1000 | Loss: 0.00001815
Iteration 71/1000 | Loss: 0.00001815
Iteration 72/1000 | Loss: 0.00001815
Iteration 73/1000 | Loss: 0.00001815
Iteration 74/1000 | Loss: 0.00001815
Iteration 75/1000 | Loss: 0.00001814
Iteration 76/1000 | Loss: 0.00001814
Iteration 77/1000 | Loss: 0.00001814
Iteration 78/1000 | Loss: 0.00001814
Iteration 79/1000 | Loss: 0.00001814
Iteration 80/1000 | Loss: 0.00001813
Iteration 81/1000 | Loss: 0.00001813
Iteration 82/1000 | Loss: 0.00001813
Iteration 83/1000 | Loss: 0.00001813
Iteration 84/1000 | Loss: 0.00001813
Iteration 85/1000 | Loss: 0.00001813
Iteration 86/1000 | Loss: 0.00001813
Iteration 87/1000 | Loss: 0.00001813
Iteration 88/1000 | Loss: 0.00001813
Iteration 89/1000 | Loss: 0.00001813
Iteration 90/1000 | Loss: 0.00001813
Iteration 91/1000 | Loss: 0.00001813
Iteration 92/1000 | Loss: 0.00001812
Iteration 93/1000 | Loss: 0.00001812
Iteration 94/1000 | Loss: 0.00001812
Iteration 95/1000 | Loss: 0.00001812
Iteration 96/1000 | Loss: 0.00001812
Iteration 97/1000 | Loss: 0.00001812
Iteration 98/1000 | Loss: 0.00001812
Iteration 99/1000 | Loss: 0.00001812
Iteration 100/1000 | Loss: 0.00001812
Iteration 101/1000 | Loss: 0.00001811
Iteration 102/1000 | Loss: 0.00001811
Iteration 103/1000 | Loss: 0.00001811
Iteration 104/1000 | Loss: 0.00001811
Iteration 105/1000 | Loss: 0.00001811
Iteration 106/1000 | Loss: 0.00001811
Iteration 107/1000 | Loss: 0.00001811
Iteration 108/1000 | Loss: 0.00001811
Iteration 109/1000 | Loss: 0.00001811
Iteration 110/1000 | Loss: 0.00001811
Iteration 111/1000 | Loss: 0.00001811
Iteration 112/1000 | Loss: 0.00001811
Iteration 113/1000 | Loss: 0.00001810
Iteration 114/1000 | Loss: 0.00001810
Iteration 115/1000 | Loss: 0.00001810
Iteration 116/1000 | Loss: 0.00001810
Iteration 117/1000 | Loss: 0.00001810
Iteration 118/1000 | Loss: 0.00001810
Iteration 119/1000 | Loss: 0.00001810
Iteration 120/1000 | Loss: 0.00001810
Iteration 121/1000 | Loss: 0.00001810
Iteration 122/1000 | Loss: 0.00001810
Iteration 123/1000 | Loss: 0.00001810
Iteration 124/1000 | Loss: 0.00001809
Iteration 125/1000 | Loss: 0.00001809
Iteration 126/1000 | Loss: 0.00001809
Iteration 127/1000 | Loss: 0.00001809
Iteration 128/1000 | Loss: 0.00001809
Iteration 129/1000 | Loss: 0.00001809
Iteration 130/1000 | Loss: 0.00001809
Iteration 131/1000 | Loss: 0.00001809
Iteration 132/1000 | Loss: 0.00001808
Iteration 133/1000 | Loss: 0.00001808
Iteration 134/1000 | Loss: 0.00001808
Iteration 135/1000 | Loss: 0.00001808
Iteration 136/1000 | Loss: 0.00001808
Iteration 137/1000 | Loss: 0.00001808
Iteration 138/1000 | Loss: 0.00001808
Iteration 139/1000 | Loss: 0.00001808
Iteration 140/1000 | Loss: 0.00001808
Iteration 141/1000 | Loss: 0.00001808
Iteration 142/1000 | Loss: 0.00001807
Iteration 143/1000 | Loss: 0.00001807
Iteration 144/1000 | Loss: 0.00001807
Iteration 145/1000 | Loss: 0.00001807
Iteration 146/1000 | Loss: 0.00001807
Iteration 147/1000 | Loss: 0.00001807
Iteration 148/1000 | Loss: 0.00001806
Iteration 149/1000 | Loss: 0.00001806
Iteration 150/1000 | Loss: 0.00001806
Iteration 151/1000 | Loss: 0.00001806
Iteration 152/1000 | Loss: 0.00001806
Iteration 153/1000 | Loss: 0.00001806
Iteration 154/1000 | Loss: 0.00001806
Iteration 155/1000 | Loss: 0.00001806
Iteration 156/1000 | Loss: 0.00001806
Iteration 157/1000 | Loss: 0.00001806
Iteration 158/1000 | Loss: 0.00001806
Iteration 159/1000 | Loss: 0.00001806
Iteration 160/1000 | Loss: 0.00001806
Iteration 161/1000 | Loss: 0.00001806
Iteration 162/1000 | Loss: 0.00001806
Iteration 163/1000 | Loss: 0.00001806
Iteration 164/1000 | Loss: 0.00001806
Iteration 165/1000 | Loss: 0.00001806
Iteration 166/1000 | Loss: 0.00001805
Iteration 167/1000 | Loss: 0.00001805
Iteration 168/1000 | Loss: 0.00001805
Iteration 169/1000 | Loss: 0.00001805
Iteration 170/1000 | Loss: 0.00001805
Iteration 171/1000 | Loss: 0.00001805
Iteration 172/1000 | Loss: 0.00001805
Iteration 173/1000 | Loss: 0.00001805
Iteration 174/1000 | Loss: 0.00001805
Iteration 175/1000 | Loss: 0.00001805
Iteration 176/1000 | Loss: 0.00001805
Iteration 177/1000 | Loss: 0.00001805
Iteration 178/1000 | Loss: 0.00001805
Iteration 179/1000 | Loss: 0.00001805
Iteration 180/1000 | Loss: 0.00001805
Iteration 181/1000 | Loss: 0.00001805
Iteration 182/1000 | Loss: 0.00001805
Iteration 183/1000 | Loss: 0.00001805
Iteration 184/1000 | Loss: 0.00001805
Iteration 185/1000 | Loss: 0.00001804
Iteration 186/1000 | Loss: 0.00001804
Iteration 187/1000 | Loss: 0.00001804
Iteration 188/1000 | Loss: 0.00001804
Iteration 189/1000 | Loss: 0.00001804
Iteration 190/1000 | Loss: 0.00001804
Iteration 191/1000 | Loss: 0.00001804
Iteration 192/1000 | Loss: 0.00001804
Iteration 193/1000 | Loss: 0.00001804
Iteration 194/1000 | Loss: 0.00001804
Iteration 195/1000 | Loss: 0.00001804
Iteration 196/1000 | Loss: 0.00001804
Iteration 197/1000 | Loss: 0.00001804
Iteration 198/1000 | Loss: 0.00001804
Iteration 199/1000 | Loss: 0.00001804
Iteration 200/1000 | Loss: 0.00001804
Iteration 201/1000 | Loss: 0.00001804
Iteration 202/1000 | Loss: 0.00001804
Iteration 203/1000 | Loss: 0.00001804
Iteration 204/1000 | Loss: 0.00001804
Iteration 205/1000 | Loss: 0.00001804
Iteration 206/1000 | Loss: 0.00001804
Iteration 207/1000 | Loss: 0.00001804
Iteration 208/1000 | Loss: 0.00001804
Iteration 209/1000 | Loss: 0.00001804
Iteration 210/1000 | Loss: 0.00001804
Iteration 211/1000 | Loss: 0.00001804
Iteration 212/1000 | Loss: 0.00001804
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 212. Stopping optimization.
Last 5 losses: [1.8036893379758112e-05, 1.8036893379758112e-05, 1.8036893379758112e-05, 1.8036893379758112e-05, 1.8036893379758112e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8036893379758112e-05

Optimization complete. Final v2v error: 3.5654799938201904 mm

Highest mean error: 4.088388442993164 mm for frame 135

Lowest mean error: 3.2992115020751953 mm for frame 109

Saving results

Total time: 39.02717304229736
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_022/1007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_022/1007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_022/1007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00842963
Iteration 2/25 | Loss: 0.00177980
Iteration 3/25 | Loss: 0.00134484
Iteration 4/25 | Loss: 0.00129381
Iteration 5/25 | Loss: 0.00128570
Iteration 6/25 | Loss: 0.00128926
Iteration 7/25 | Loss: 0.00128382
Iteration 8/25 | Loss: 0.00127669
Iteration 9/25 | Loss: 0.00127054
Iteration 10/25 | Loss: 0.00126726
Iteration 11/25 | Loss: 0.00126524
Iteration 12/25 | Loss: 0.00126360
Iteration 13/25 | Loss: 0.00126332
Iteration 14/25 | Loss: 0.00126328
Iteration 15/25 | Loss: 0.00126328
Iteration 16/25 | Loss: 0.00126327
Iteration 17/25 | Loss: 0.00126327
Iteration 18/25 | Loss: 0.00126327
Iteration 19/25 | Loss: 0.00126327
Iteration 20/25 | Loss: 0.00126327
Iteration 21/25 | Loss: 0.00126327
Iteration 22/25 | Loss: 0.00126327
Iteration 23/25 | Loss: 0.00126327
Iteration 24/25 | Loss: 0.00126326
Iteration 25/25 | Loss: 0.00126327

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.27566111
Iteration 2/25 | Loss: 0.00043453
Iteration 3/25 | Loss: 0.00043453
Iteration 4/25 | Loss: 0.00043453
Iteration 5/25 | Loss: 0.00043453
Iteration 6/25 | Loss: 0.00043453
Iteration 7/25 | Loss: 0.00043453
Iteration 8/25 | Loss: 0.00043453
Iteration 9/25 | Loss: 0.00043453
Iteration 10/25 | Loss: 0.00043453
Iteration 11/25 | Loss: 0.00043453
Iteration 12/25 | Loss: 0.00043453
Iteration 13/25 | Loss: 0.00043453
Iteration 14/25 | Loss: 0.00043453
Iteration 15/25 | Loss: 0.00043453
Iteration 16/25 | Loss: 0.00043453
Iteration 17/25 | Loss: 0.00043453
Iteration 18/25 | Loss: 0.00043453
Iteration 19/25 | Loss: 0.00043453
Iteration 20/25 | Loss: 0.00043453
Iteration 21/25 | Loss: 0.00043453
Iteration 22/25 | Loss: 0.00043453
Iteration 23/25 | Loss: 0.00043453
Iteration 24/25 | Loss: 0.00043453
Iteration 25/25 | Loss: 0.00043453

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00043453
Iteration 2/1000 | Loss: 0.00005179
Iteration 3/1000 | Loss: 0.00003518
Iteration 4/1000 | Loss: 0.00003309
Iteration 5/1000 | Loss: 0.00003218
Iteration 6/1000 | Loss: 0.00003140
Iteration 7/1000 | Loss: 0.00003072
Iteration 8/1000 | Loss: 0.00003027
Iteration 9/1000 | Loss: 0.00003001
Iteration 10/1000 | Loss: 0.00002985
Iteration 11/1000 | Loss: 0.00002985
Iteration 12/1000 | Loss: 0.00002980
Iteration 13/1000 | Loss: 0.00002980
Iteration 14/1000 | Loss: 0.00002974
Iteration 15/1000 | Loss: 0.00002974
Iteration 16/1000 | Loss: 0.00002971
Iteration 17/1000 | Loss: 0.00002970
Iteration 18/1000 | Loss: 0.00002969
Iteration 19/1000 | Loss: 0.00002969
Iteration 20/1000 | Loss: 0.00002969
Iteration 21/1000 | Loss: 0.00002969
Iteration 22/1000 | Loss: 0.00002969
Iteration 23/1000 | Loss: 0.00002968
Iteration 24/1000 | Loss: 0.00002968
Iteration 25/1000 | Loss: 0.00002968
Iteration 26/1000 | Loss: 0.00002968
Iteration 27/1000 | Loss: 0.00002968
Iteration 28/1000 | Loss: 0.00002967
Iteration 29/1000 | Loss: 0.00002967
Iteration 30/1000 | Loss: 0.00002966
Iteration 31/1000 | Loss: 0.00002965
Iteration 32/1000 | Loss: 0.00002965
Iteration 33/1000 | Loss: 0.00002965
Iteration 34/1000 | Loss: 0.00002965
Iteration 35/1000 | Loss: 0.00002965
Iteration 36/1000 | Loss: 0.00002965
Iteration 37/1000 | Loss: 0.00002965
Iteration 38/1000 | Loss: 0.00002965
Iteration 39/1000 | Loss: 0.00002965
Iteration 40/1000 | Loss: 0.00002964
Iteration 41/1000 | Loss: 0.00002964
Iteration 42/1000 | Loss: 0.00002964
Iteration 43/1000 | Loss: 0.00002964
Iteration 44/1000 | Loss: 0.00002964
Iteration 45/1000 | Loss: 0.00002964
Iteration 46/1000 | Loss: 0.00002964
Iteration 47/1000 | Loss: 0.00002964
Iteration 48/1000 | Loss: 0.00002964
Iteration 49/1000 | Loss: 0.00002963
Iteration 50/1000 | Loss: 0.00002963
Iteration 51/1000 | Loss: 0.00002963
Iteration 52/1000 | Loss: 0.00002963
Iteration 53/1000 | Loss: 0.00002963
Iteration 54/1000 | Loss: 0.00002963
Iteration 55/1000 | Loss: 0.00002963
Iteration 56/1000 | Loss: 0.00002963
Iteration 57/1000 | Loss: 0.00002963
Iteration 58/1000 | Loss: 0.00002963
Iteration 59/1000 | Loss: 0.00002963
Iteration 60/1000 | Loss: 0.00002963
Iteration 61/1000 | Loss: 0.00002963
Iteration 62/1000 | Loss: 0.00002962
Iteration 63/1000 | Loss: 0.00002962
Iteration 64/1000 | Loss: 0.00002962
Iteration 65/1000 | Loss: 0.00002962
Iteration 66/1000 | Loss: 0.00002962
Iteration 67/1000 | Loss: 0.00002962
Iteration 68/1000 | Loss: 0.00002962
Iteration 69/1000 | Loss: 0.00002962
Iteration 70/1000 | Loss: 0.00002962
Iteration 71/1000 | Loss: 0.00002962
Iteration 72/1000 | Loss: 0.00002962
Iteration 73/1000 | Loss: 0.00002962
Iteration 74/1000 | Loss: 0.00002962
Iteration 75/1000 | Loss: 0.00002962
Iteration 76/1000 | Loss: 0.00002962
Iteration 77/1000 | Loss: 0.00002962
Iteration 78/1000 | Loss: 0.00002962
Iteration 79/1000 | Loss: 0.00002962
Iteration 80/1000 | Loss: 0.00002962
Iteration 81/1000 | Loss: 0.00002962
Iteration 82/1000 | Loss: 0.00002962
Iteration 83/1000 | Loss: 0.00002961
Iteration 84/1000 | Loss: 0.00002961
Iteration 85/1000 | Loss: 0.00002961
Iteration 86/1000 | Loss: 0.00002961
Iteration 87/1000 | Loss: 0.00002961
Iteration 88/1000 | Loss: 0.00002961
Iteration 89/1000 | Loss: 0.00002961
Iteration 90/1000 | Loss: 0.00002961
Iteration 91/1000 | Loss: 0.00002961
Iteration 92/1000 | Loss: 0.00002961
Iteration 93/1000 | Loss: 0.00002961
Iteration 94/1000 | Loss: 0.00002961
Iteration 95/1000 | Loss: 0.00002960
Iteration 96/1000 | Loss: 0.00002960
Iteration 97/1000 | Loss: 0.00002960
Iteration 98/1000 | Loss: 0.00002960
Iteration 99/1000 | Loss: 0.00002960
Iteration 100/1000 | Loss: 0.00002960
Iteration 101/1000 | Loss: 0.00002960
Iteration 102/1000 | Loss: 0.00002960
Iteration 103/1000 | Loss: 0.00002960
Iteration 104/1000 | Loss: 0.00002960
Iteration 105/1000 | Loss: 0.00002960
Iteration 106/1000 | Loss: 0.00002960
Iteration 107/1000 | Loss: 0.00002960
Iteration 108/1000 | Loss: 0.00002960
Iteration 109/1000 | Loss: 0.00002960
Iteration 110/1000 | Loss: 0.00002960
Iteration 111/1000 | Loss: 0.00002960
Iteration 112/1000 | Loss: 0.00002960
Iteration 113/1000 | Loss: 0.00002960
Iteration 114/1000 | Loss: 0.00002960
Iteration 115/1000 | Loss: 0.00002960
Iteration 116/1000 | Loss: 0.00002960
Iteration 117/1000 | Loss: 0.00002960
Iteration 118/1000 | Loss: 0.00002960
Iteration 119/1000 | Loss: 0.00002960
Iteration 120/1000 | Loss: 0.00002960
Iteration 121/1000 | Loss: 0.00002960
Iteration 122/1000 | Loss: 0.00002960
Iteration 123/1000 | Loss: 0.00002960
Iteration 124/1000 | Loss: 0.00002960
Iteration 125/1000 | Loss: 0.00002960
Iteration 126/1000 | Loss: 0.00002960
Iteration 127/1000 | Loss: 0.00002960
Iteration 128/1000 | Loss: 0.00002960
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 128. Stopping optimization.
Last 5 losses: [2.960380879812874e-05, 2.960380879812874e-05, 2.960380879812874e-05, 2.960380879812874e-05, 2.960380879812874e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.960380879812874e-05

Optimization complete. Final v2v error: 4.553563117980957 mm

Highest mean error: 5.159635543823242 mm for frame 134

Lowest mean error: 4.001913547515869 mm for frame 2

Saving results

Total time: 51.780102491378784
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_022/1089/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_022/1089.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_022/1089
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00727658
Iteration 2/25 | Loss: 0.00156025
Iteration 3/25 | Loss: 0.00134450
Iteration 4/25 | Loss: 0.00113745
Iteration 5/25 | Loss: 0.00112257
Iteration 6/25 | Loss: 0.00112231
Iteration 7/25 | Loss: 0.00111873
Iteration 8/25 | Loss: 0.00111544
Iteration 9/25 | Loss: 0.00111454
Iteration 10/25 | Loss: 0.00111335
Iteration 11/25 | Loss: 0.00111199
Iteration 12/25 | Loss: 0.00111092
Iteration 13/25 | Loss: 0.00111068
Iteration 14/25 | Loss: 0.00111060
Iteration 15/25 | Loss: 0.00111057
Iteration 16/25 | Loss: 0.00111056
Iteration 17/25 | Loss: 0.00111056
Iteration 18/25 | Loss: 0.00111056
Iteration 19/25 | Loss: 0.00111056
Iteration 20/25 | Loss: 0.00111056
Iteration 21/25 | Loss: 0.00111056
Iteration 22/25 | Loss: 0.00111055
Iteration 23/25 | Loss: 0.00111055
Iteration 24/25 | Loss: 0.00111055
Iteration 25/25 | Loss: 0.00111055

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.83094609
Iteration 2/25 | Loss: 0.00084275
Iteration 3/25 | Loss: 0.00084274
Iteration 4/25 | Loss: 0.00084274
Iteration 5/25 | Loss: 0.00084274
Iteration 6/25 | Loss: 0.00084274
Iteration 7/25 | Loss: 0.00084274
Iteration 8/25 | Loss: 0.00084274
Iteration 9/25 | Loss: 0.00084274
Iteration 10/25 | Loss: 0.00084274
Iteration 11/25 | Loss: 0.00084274
Iteration 12/25 | Loss: 0.00084274
Iteration 13/25 | Loss: 0.00084274
Iteration 14/25 | Loss: 0.00084274
Iteration 15/25 | Loss: 0.00084274
Iteration 16/25 | Loss: 0.00084274
Iteration 17/25 | Loss: 0.00084274
Iteration 18/25 | Loss: 0.00084274
Iteration 19/25 | Loss: 0.00084274
Iteration 20/25 | Loss: 0.00084274
Iteration 21/25 | Loss: 0.00084274
Iteration 22/25 | Loss: 0.00084274
Iteration 23/25 | Loss: 0.00084274
Iteration 24/25 | Loss: 0.00084274
Iteration 25/25 | Loss: 0.00084274

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00084274
Iteration 2/1000 | Loss: 0.00002686
Iteration 3/1000 | Loss: 0.00017543
Iteration 4/1000 | Loss: 0.00002066
Iteration 5/1000 | Loss: 0.00001894
Iteration 6/1000 | Loss: 0.00001838
Iteration 7/1000 | Loss: 0.00001793
Iteration 8/1000 | Loss: 0.00001752
Iteration 9/1000 | Loss: 0.00001725
Iteration 10/1000 | Loss: 0.00001702
Iteration 11/1000 | Loss: 0.00001694
Iteration 12/1000 | Loss: 0.00001672
Iteration 13/1000 | Loss: 0.00001671
Iteration 14/1000 | Loss: 0.00001665
Iteration 15/1000 | Loss: 0.00001661
Iteration 16/1000 | Loss: 0.00001661
Iteration 17/1000 | Loss: 0.00001660
Iteration 18/1000 | Loss: 0.00001653
Iteration 19/1000 | Loss: 0.00001649
Iteration 20/1000 | Loss: 0.00001640
Iteration 21/1000 | Loss: 0.00001635
Iteration 22/1000 | Loss: 0.00001630
Iteration 23/1000 | Loss: 0.00001629
Iteration 24/1000 | Loss: 0.00001625
Iteration 25/1000 | Loss: 0.00001625
Iteration 26/1000 | Loss: 0.00001625
Iteration 27/1000 | Loss: 0.00001625
Iteration 28/1000 | Loss: 0.00001624
Iteration 29/1000 | Loss: 0.00001624
Iteration 30/1000 | Loss: 0.00001624
Iteration 31/1000 | Loss: 0.00001621
Iteration 32/1000 | Loss: 0.00001620
Iteration 33/1000 | Loss: 0.00001620
Iteration 34/1000 | Loss: 0.00001619
Iteration 35/1000 | Loss: 0.00001618
Iteration 36/1000 | Loss: 0.00001617
Iteration 37/1000 | Loss: 0.00001616
Iteration 38/1000 | Loss: 0.00001616
Iteration 39/1000 | Loss: 0.00001615
Iteration 40/1000 | Loss: 0.00001615
Iteration 41/1000 | Loss: 0.00001614
Iteration 42/1000 | Loss: 0.00001614
Iteration 43/1000 | Loss: 0.00001613
Iteration 44/1000 | Loss: 0.00001612
Iteration 45/1000 | Loss: 0.00001611
Iteration 46/1000 | Loss: 0.00001611
Iteration 47/1000 | Loss: 0.00001611
Iteration 48/1000 | Loss: 0.00001611
Iteration 49/1000 | Loss: 0.00001611
Iteration 50/1000 | Loss: 0.00001611
Iteration 51/1000 | Loss: 0.00001610
Iteration 52/1000 | Loss: 0.00001610
Iteration 53/1000 | Loss: 0.00001610
Iteration 54/1000 | Loss: 0.00001609
Iteration 55/1000 | Loss: 0.00001609
Iteration 56/1000 | Loss: 0.00001608
Iteration 57/1000 | Loss: 0.00001608
Iteration 58/1000 | Loss: 0.00001607
Iteration 59/1000 | Loss: 0.00001607
Iteration 60/1000 | Loss: 0.00001606
Iteration 61/1000 | Loss: 0.00001606
Iteration 62/1000 | Loss: 0.00001605
Iteration 63/1000 | Loss: 0.00001604
Iteration 64/1000 | Loss: 0.00001604
Iteration 65/1000 | Loss: 0.00001603
Iteration 66/1000 | Loss: 0.00001603
Iteration 67/1000 | Loss: 0.00001603
Iteration 68/1000 | Loss: 0.00001603
Iteration 69/1000 | Loss: 0.00001602
Iteration 70/1000 | Loss: 0.00001602
Iteration 71/1000 | Loss: 0.00001602
Iteration 72/1000 | Loss: 0.00001602
Iteration 73/1000 | Loss: 0.00001601
Iteration 74/1000 | Loss: 0.00001601
Iteration 75/1000 | Loss: 0.00001600
Iteration 76/1000 | Loss: 0.00001600
Iteration 77/1000 | Loss: 0.00001599
Iteration 78/1000 | Loss: 0.00001599
Iteration 79/1000 | Loss: 0.00001599
Iteration 80/1000 | Loss: 0.00001599
Iteration 81/1000 | Loss: 0.00001599
Iteration 82/1000 | Loss: 0.00001599
Iteration 83/1000 | Loss: 0.00001599
Iteration 84/1000 | Loss: 0.00001599
Iteration 85/1000 | Loss: 0.00001598
Iteration 86/1000 | Loss: 0.00001598
Iteration 87/1000 | Loss: 0.00001598
Iteration 88/1000 | Loss: 0.00001598
Iteration 89/1000 | Loss: 0.00001598
Iteration 90/1000 | Loss: 0.00001598
Iteration 91/1000 | Loss: 0.00001598
Iteration 92/1000 | Loss: 0.00001598
Iteration 93/1000 | Loss: 0.00001597
Iteration 94/1000 | Loss: 0.00001597
Iteration 95/1000 | Loss: 0.00001597
Iteration 96/1000 | Loss: 0.00001597
Iteration 97/1000 | Loss: 0.00001597
Iteration 98/1000 | Loss: 0.00001597
Iteration 99/1000 | Loss: 0.00001597
Iteration 100/1000 | Loss: 0.00001597
Iteration 101/1000 | Loss: 0.00001597
Iteration 102/1000 | Loss: 0.00001597
Iteration 103/1000 | Loss: 0.00001597
Iteration 104/1000 | Loss: 0.00001597
Iteration 105/1000 | Loss: 0.00001597
Iteration 106/1000 | Loss: 0.00001596
Iteration 107/1000 | Loss: 0.00001596
Iteration 108/1000 | Loss: 0.00001596
Iteration 109/1000 | Loss: 0.00001596
Iteration 110/1000 | Loss: 0.00001596
Iteration 111/1000 | Loss: 0.00001596
Iteration 112/1000 | Loss: 0.00001596
Iteration 113/1000 | Loss: 0.00001596
Iteration 114/1000 | Loss: 0.00001596
Iteration 115/1000 | Loss: 0.00001595
Iteration 116/1000 | Loss: 0.00001595
Iteration 117/1000 | Loss: 0.00001595
Iteration 118/1000 | Loss: 0.00001595
Iteration 119/1000 | Loss: 0.00001595
Iteration 120/1000 | Loss: 0.00001595
Iteration 121/1000 | Loss: 0.00001595
Iteration 122/1000 | Loss: 0.00001595
Iteration 123/1000 | Loss: 0.00001595
Iteration 124/1000 | Loss: 0.00001595
Iteration 125/1000 | Loss: 0.00001595
Iteration 126/1000 | Loss: 0.00001595
Iteration 127/1000 | Loss: 0.00001595
Iteration 128/1000 | Loss: 0.00001595
Iteration 129/1000 | Loss: 0.00001595
Iteration 130/1000 | Loss: 0.00001595
Iteration 131/1000 | Loss: 0.00001595
Iteration 132/1000 | Loss: 0.00001594
Iteration 133/1000 | Loss: 0.00001594
Iteration 134/1000 | Loss: 0.00001594
Iteration 135/1000 | Loss: 0.00001594
Iteration 136/1000 | Loss: 0.00001594
Iteration 137/1000 | Loss: 0.00001594
Iteration 138/1000 | Loss: 0.00001594
Iteration 139/1000 | Loss: 0.00001594
Iteration 140/1000 | Loss: 0.00001594
Iteration 141/1000 | Loss: 0.00001594
Iteration 142/1000 | Loss: 0.00001594
Iteration 143/1000 | Loss: 0.00001594
Iteration 144/1000 | Loss: 0.00001594
Iteration 145/1000 | Loss: 0.00001594
Iteration 146/1000 | Loss: 0.00001594
Iteration 147/1000 | Loss: 0.00001594
Iteration 148/1000 | Loss: 0.00001594
Iteration 149/1000 | Loss: 0.00001594
Iteration 150/1000 | Loss: 0.00001594
Iteration 151/1000 | Loss: 0.00001594
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 151. Stopping optimization.
Last 5 losses: [1.593517117726151e-05, 1.593517117726151e-05, 1.593517117726151e-05, 1.593517117726151e-05, 1.593517117726151e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.593517117726151e-05

Optimization complete. Final v2v error: 3.393028497695923 mm

Highest mean error: 3.7956576347351074 mm for frame 102

Lowest mean error: 3.066866636276245 mm for frame 127

Saving results

Total time: 57.45208382606506
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_022/1036/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_022/1036.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_022/1036
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00452752
Iteration 2/25 | Loss: 0.00123292
Iteration 3/25 | Loss: 0.00112335
Iteration 4/25 | Loss: 0.00110548
Iteration 5/25 | Loss: 0.00109969
Iteration 6/25 | Loss: 0.00109814
Iteration 7/25 | Loss: 0.00109806
Iteration 8/25 | Loss: 0.00109806
Iteration 9/25 | Loss: 0.00109806
Iteration 10/25 | Loss: 0.00109806
Iteration 11/25 | Loss: 0.00109806
Iteration 12/25 | Loss: 0.00109806
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0010980594670400023, 0.0010980594670400023, 0.0010980594670400023, 0.0010980594670400023, 0.0010980594670400023]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010980594670400023

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.31007910
Iteration 2/25 | Loss: 0.00076659
Iteration 3/25 | Loss: 0.00076658
Iteration 4/25 | Loss: 0.00076658
Iteration 5/25 | Loss: 0.00076658
Iteration 6/25 | Loss: 0.00076658
Iteration 7/25 | Loss: 0.00076658
Iteration 8/25 | Loss: 0.00076658
Iteration 9/25 | Loss: 0.00076658
Iteration 10/25 | Loss: 0.00076658
Iteration 11/25 | Loss: 0.00076658
Iteration 12/25 | Loss: 0.00076658
Iteration 13/25 | Loss: 0.00076658
Iteration 14/25 | Loss: 0.00076658
Iteration 15/25 | Loss: 0.00076658
Iteration 16/25 | Loss: 0.00076658
Iteration 17/25 | Loss: 0.00076658
Iteration 18/25 | Loss: 0.00076658
Iteration 19/25 | Loss: 0.00076658
Iteration 20/25 | Loss: 0.00076658
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0007665798766538501, 0.0007665798766538501, 0.0007665798766538501, 0.0007665798766538501, 0.0007665798766538501]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007665798766538501

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00076658
Iteration 2/1000 | Loss: 0.00002965
Iteration 3/1000 | Loss: 0.00001988
Iteration 4/1000 | Loss: 0.00001590
Iteration 5/1000 | Loss: 0.00001512
Iteration 6/1000 | Loss: 0.00001443
Iteration 7/1000 | Loss: 0.00001408
Iteration 8/1000 | Loss: 0.00001367
Iteration 9/1000 | Loss: 0.00001340
Iteration 10/1000 | Loss: 0.00001326
Iteration 11/1000 | Loss: 0.00001307
Iteration 12/1000 | Loss: 0.00001290
Iteration 13/1000 | Loss: 0.00001285
Iteration 14/1000 | Loss: 0.00001282
Iteration 15/1000 | Loss: 0.00001281
Iteration 16/1000 | Loss: 0.00001280
Iteration 17/1000 | Loss: 0.00001280
Iteration 18/1000 | Loss: 0.00001279
Iteration 19/1000 | Loss: 0.00001278
Iteration 20/1000 | Loss: 0.00001278
Iteration 21/1000 | Loss: 0.00001278
Iteration 22/1000 | Loss: 0.00001277
Iteration 23/1000 | Loss: 0.00001274
Iteration 24/1000 | Loss: 0.00001270
Iteration 25/1000 | Loss: 0.00001270
Iteration 26/1000 | Loss: 0.00001266
Iteration 27/1000 | Loss: 0.00001266
Iteration 28/1000 | Loss: 0.00001264
Iteration 29/1000 | Loss: 0.00001264
Iteration 30/1000 | Loss: 0.00001263
Iteration 31/1000 | Loss: 0.00001262
Iteration 32/1000 | Loss: 0.00001262
Iteration 33/1000 | Loss: 0.00001261
Iteration 34/1000 | Loss: 0.00001261
Iteration 35/1000 | Loss: 0.00001260
Iteration 36/1000 | Loss: 0.00001260
Iteration 37/1000 | Loss: 0.00001260
Iteration 38/1000 | Loss: 0.00001260
Iteration 39/1000 | Loss: 0.00001260
Iteration 40/1000 | Loss: 0.00001260
Iteration 41/1000 | Loss: 0.00001260
Iteration 42/1000 | Loss: 0.00001259
Iteration 43/1000 | Loss: 0.00001259
Iteration 44/1000 | Loss: 0.00001259
Iteration 45/1000 | Loss: 0.00001259
Iteration 46/1000 | Loss: 0.00001259
Iteration 47/1000 | Loss: 0.00001259
Iteration 48/1000 | Loss: 0.00001259
Iteration 49/1000 | Loss: 0.00001258
Iteration 50/1000 | Loss: 0.00001258
Iteration 51/1000 | Loss: 0.00001258
Iteration 52/1000 | Loss: 0.00001258
Iteration 53/1000 | Loss: 0.00001258
Iteration 54/1000 | Loss: 0.00001258
Iteration 55/1000 | Loss: 0.00001257
Iteration 56/1000 | Loss: 0.00001257
Iteration 57/1000 | Loss: 0.00001256
Iteration 58/1000 | Loss: 0.00001256
Iteration 59/1000 | Loss: 0.00001256
Iteration 60/1000 | Loss: 0.00001256
Iteration 61/1000 | Loss: 0.00001256
Iteration 62/1000 | Loss: 0.00001255
Iteration 63/1000 | Loss: 0.00001255
Iteration 64/1000 | Loss: 0.00001255
Iteration 65/1000 | Loss: 0.00001255
Iteration 66/1000 | Loss: 0.00001254
Iteration 67/1000 | Loss: 0.00001254
Iteration 68/1000 | Loss: 0.00001254
Iteration 69/1000 | Loss: 0.00001254
Iteration 70/1000 | Loss: 0.00001254
Iteration 71/1000 | Loss: 0.00001253
Iteration 72/1000 | Loss: 0.00001253
Iteration 73/1000 | Loss: 0.00001253
Iteration 74/1000 | Loss: 0.00001253
Iteration 75/1000 | Loss: 0.00001253
Iteration 76/1000 | Loss: 0.00001253
Iteration 77/1000 | Loss: 0.00001253
Iteration 78/1000 | Loss: 0.00001253
Iteration 79/1000 | Loss: 0.00001253
Iteration 80/1000 | Loss: 0.00001252
Iteration 81/1000 | Loss: 0.00001252
Iteration 82/1000 | Loss: 0.00001252
Iteration 83/1000 | Loss: 0.00001252
Iteration 84/1000 | Loss: 0.00001252
Iteration 85/1000 | Loss: 0.00001252
Iteration 86/1000 | Loss: 0.00001251
Iteration 87/1000 | Loss: 0.00001251
Iteration 88/1000 | Loss: 0.00001251
Iteration 89/1000 | Loss: 0.00001250
Iteration 90/1000 | Loss: 0.00001250
Iteration 91/1000 | Loss: 0.00001250
Iteration 92/1000 | Loss: 0.00001250
Iteration 93/1000 | Loss: 0.00001250
Iteration 94/1000 | Loss: 0.00001249
Iteration 95/1000 | Loss: 0.00001249
Iteration 96/1000 | Loss: 0.00001249
Iteration 97/1000 | Loss: 0.00001248
Iteration 98/1000 | Loss: 0.00001248
Iteration 99/1000 | Loss: 0.00001248
Iteration 100/1000 | Loss: 0.00001248
Iteration 101/1000 | Loss: 0.00001247
Iteration 102/1000 | Loss: 0.00001247
Iteration 103/1000 | Loss: 0.00001247
Iteration 104/1000 | Loss: 0.00001246
Iteration 105/1000 | Loss: 0.00001246
Iteration 106/1000 | Loss: 0.00001246
Iteration 107/1000 | Loss: 0.00001245
Iteration 108/1000 | Loss: 0.00001245
Iteration 109/1000 | Loss: 0.00001245
Iteration 110/1000 | Loss: 0.00001244
Iteration 111/1000 | Loss: 0.00001244
Iteration 112/1000 | Loss: 0.00001244
Iteration 113/1000 | Loss: 0.00001243
Iteration 114/1000 | Loss: 0.00001243
Iteration 115/1000 | Loss: 0.00001242
Iteration 116/1000 | Loss: 0.00001242
Iteration 117/1000 | Loss: 0.00001242
Iteration 118/1000 | Loss: 0.00001241
Iteration 119/1000 | Loss: 0.00001241
Iteration 120/1000 | Loss: 0.00001241
Iteration 121/1000 | Loss: 0.00001241
Iteration 122/1000 | Loss: 0.00001241
Iteration 123/1000 | Loss: 0.00001241
Iteration 124/1000 | Loss: 0.00001241
Iteration 125/1000 | Loss: 0.00001241
Iteration 126/1000 | Loss: 0.00001240
Iteration 127/1000 | Loss: 0.00001240
Iteration 128/1000 | Loss: 0.00001240
Iteration 129/1000 | Loss: 0.00001240
Iteration 130/1000 | Loss: 0.00001239
Iteration 131/1000 | Loss: 0.00001239
Iteration 132/1000 | Loss: 0.00001239
Iteration 133/1000 | Loss: 0.00001238
Iteration 134/1000 | Loss: 0.00001238
Iteration 135/1000 | Loss: 0.00001238
Iteration 136/1000 | Loss: 0.00001238
Iteration 137/1000 | Loss: 0.00001238
Iteration 138/1000 | Loss: 0.00001238
Iteration 139/1000 | Loss: 0.00001237
Iteration 140/1000 | Loss: 0.00001237
Iteration 141/1000 | Loss: 0.00001237
Iteration 142/1000 | Loss: 0.00001236
Iteration 143/1000 | Loss: 0.00001236
Iteration 144/1000 | Loss: 0.00001236
Iteration 145/1000 | Loss: 0.00001236
Iteration 146/1000 | Loss: 0.00001236
Iteration 147/1000 | Loss: 0.00001236
Iteration 148/1000 | Loss: 0.00001235
Iteration 149/1000 | Loss: 0.00001235
Iteration 150/1000 | Loss: 0.00001235
Iteration 151/1000 | Loss: 0.00001235
Iteration 152/1000 | Loss: 0.00001235
Iteration 153/1000 | Loss: 0.00001235
Iteration 154/1000 | Loss: 0.00001235
Iteration 155/1000 | Loss: 0.00001235
Iteration 156/1000 | Loss: 0.00001235
Iteration 157/1000 | Loss: 0.00001235
Iteration 158/1000 | Loss: 0.00001234
Iteration 159/1000 | Loss: 0.00001234
Iteration 160/1000 | Loss: 0.00001234
Iteration 161/1000 | Loss: 0.00001234
Iteration 162/1000 | Loss: 0.00001234
Iteration 163/1000 | Loss: 0.00001234
Iteration 164/1000 | Loss: 0.00001234
Iteration 165/1000 | Loss: 0.00001234
Iteration 166/1000 | Loss: 0.00001234
Iteration 167/1000 | Loss: 0.00001233
Iteration 168/1000 | Loss: 0.00001233
Iteration 169/1000 | Loss: 0.00001233
Iteration 170/1000 | Loss: 0.00001233
Iteration 171/1000 | Loss: 0.00001233
Iteration 172/1000 | Loss: 0.00001233
Iteration 173/1000 | Loss: 0.00001232
Iteration 174/1000 | Loss: 0.00001232
Iteration 175/1000 | Loss: 0.00001232
Iteration 176/1000 | Loss: 0.00001232
Iteration 177/1000 | Loss: 0.00001232
Iteration 178/1000 | Loss: 0.00001232
Iteration 179/1000 | Loss: 0.00001232
Iteration 180/1000 | Loss: 0.00001232
Iteration 181/1000 | Loss: 0.00001232
Iteration 182/1000 | Loss: 0.00001232
Iteration 183/1000 | Loss: 0.00001232
Iteration 184/1000 | Loss: 0.00001232
Iteration 185/1000 | Loss: 0.00001232
Iteration 186/1000 | Loss: 0.00001232
Iteration 187/1000 | Loss: 0.00001232
Iteration 188/1000 | Loss: 0.00001231
Iteration 189/1000 | Loss: 0.00001231
Iteration 190/1000 | Loss: 0.00001231
Iteration 191/1000 | Loss: 0.00001231
Iteration 192/1000 | Loss: 0.00001231
Iteration 193/1000 | Loss: 0.00001231
Iteration 194/1000 | Loss: 0.00001231
Iteration 195/1000 | Loss: 0.00001231
Iteration 196/1000 | Loss: 0.00001231
Iteration 197/1000 | Loss: 0.00001231
Iteration 198/1000 | Loss: 0.00001231
Iteration 199/1000 | Loss: 0.00001231
Iteration 200/1000 | Loss: 0.00001231
Iteration 201/1000 | Loss: 0.00001231
Iteration 202/1000 | Loss: 0.00001231
Iteration 203/1000 | Loss: 0.00001231
Iteration 204/1000 | Loss: 0.00001231
Iteration 205/1000 | Loss: 0.00001231
Iteration 206/1000 | Loss: 0.00001231
Iteration 207/1000 | Loss: 0.00001231
Iteration 208/1000 | Loss: 0.00001231
Iteration 209/1000 | Loss: 0.00001231
Iteration 210/1000 | Loss: 0.00001231
Iteration 211/1000 | Loss: 0.00001231
Iteration 212/1000 | Loss: 0.00001230
Iteration 213/1000 | Loss: 0.00001230
Iteration 214/1000 | Loss: 0.00001230
Iteration 215/1000 | Loss: 0.00001230
Iteration 216/1000 | Loss: 0.00001230
Iteration 217/1000 | Loss: 0.00001230
Iteration 218/1000 | Loss: 0.00001230
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 218. Stopping optimization.
Last 5 losses: [1.2304497431614436e-05, 1.2304497431614436e-05, 1.2304497431614436e-05, 1.2304497431614436e-05, 1.2304497431614436e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2304497431614436e-05

Optimization complete. Final v2v error: 2.9854321479797363 mm

Highest mean error: 3.4458625316619873 mm for frame 43

Lowest mean error: 2.5957021713256836 mm for frame 83

Saving results

Total time: 43.47971844673157
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_022/1054/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_022/1054.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_022/1054
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00435206
Iteration 2/25 | Loss: 0.00120325
Iteration 3/25 | Loss: 0.00112104
Iteration 4/25 | Loss: 0.00110909
Iteration 5/25 | Loss: 0.00110476
Iteration 6/25 | Loss: 0.00110444
Iteration 7/25 | Loss: 0.00110444
Iteration 8/25 | Loss: 0.00110444
Iteration 9/25 | Loss: 0.00110444
Iteration 10/25 | Loss: 0.00110444
Iteration 11/25 | Loss: 0.00110444
Iteration 12/25 | Loss: 0.00110444
Iteration 13/25 | Loss: 0.00110444
Iteration 14/25 | Loss: 0.00110444
Iteration 15/25 | Loss: 0.00110444
Iteration 16/25 | Loss: 0.00110444
Iteration 17/25 | Loss: 0.00110444
Iteration 18/25 | Loss: 0.00110444
Iteration 19/25 | Loss: 0.00110444
Iteration 20/25 | Loss: 0.00110444
Iteration 21/25 | Loss: 0.00110444
Iteration 22/25 | Loss: 0.00110444
Iteration 23/25 | Loss: 0.00110444
Iteration 24/25 | Loss: 0.00110444
Iteration 25/25 | Loss: 0.00110444

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34412766
Iteration 2/25 | Loss: 0.00100738
Iteration 3/25 | Loss: 0.00100738
Iteration 4/25 | Loss: 0.00100738
Iteration 5/25 | Loss: 0.00100738
Iteration 6/25 | Loss: 0.00100738
Iteration 7/25 | Loss: 0.00100738
Iteration 8/25 | Loss: 0.00100738
Iteration 9/25 | Loss: 0.00100738
Iteration 10/25 | Loss: 0.00100738
Iteration 11/25 | Loss: 0.00100738
Iteration 12/25 | Loss: 0.00100738
Iteration 13/25 | Loss: 0.00100738
Iteration 14/25 | Loss: 0.00100738
Iteration 15/25 | Loss: 0.00100738
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0010073811281472445, 0.0010073811281472445, 0.0010073811281472445, 0.0010073811281472445, 0.0010073811281472445]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010073811281472445

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00100738
Iteration 2/1000 | Loss: 0.00002132
Iteration 3/1000 | Loss: 0.00001481
Iteration 4/1000 | Loss: 0.00001347
Iteration 5/1000 | Loss: 0.00001270
Iteration 6/1000 | Loss: 0.00001217
Iteration 7/1000 | Loss: 0.00001194
Iteration 8/1000 | Loss: 0.00001167
Iteration 9/1000 | Loss: 0.00001132
Iteration 10/1000 | Loss: 0.00001119
Iteration 11/1000 | Loss: 0.00001117
Iteration 12/1000 | Loss: 0.00001112
Iteration 13/1000 | Loss: 0.00001108
Iteration 14/1000 | Loss: 0.00001106
Iteration 15/1000 | Loss: 0.00001104
Iteration 16/1000 | Loss: 0.00001101
Iteration 17/1000 | Loss: 0.00001101
Iteration 18/1000 | Loss: 0.00001100
Iteration 19/1000 | Loss: 0.00001099
Iteration 20/1000 | Loss: 0.00001098
Iteration 21/1000 | Loss: 0.00001097
Iteration 22/1000 | Loss: 0.00001095
Iteration 23/1000 | Loss: 0.00001095
Iteration 24/1000 | Loss: 0.00001095
Iteration 25/1000 | Loss: 0.00001094
Iteration 26/1000 | Loss: 0.00001094
Iteration 27/1000 | Loss: 0.00001093
Iteration 28/1000 | Loss: 0.00001089
Iteration 29/1000 | Loss: 0.00001086
Iteration 30/1000 | Loss: 0.00001084
Iteration 31/1000 | Loss: 0.00001083
Iteration 32/1000 | Loss: 0.00001081
Iteration 33/1000 | Loss: 0.00001080
Iteration 34/1000 | Loss: 0.00001080
Iteration 35/1000 | Loss: 0.00001080
Iteration 36/1000 | Loss: 0.00001080
Iteration 37/1000 | Loss: 0.00001080
Iteration 38/1000 | Loss: 0.00001079
Iteration 39/1000 | Loss: 0.00001079
Iteration 40/1000 | Loss: 0.00001078
Iteration 41/1000 | Loss: 0.00001078
Iteration 42/1000 | Loss: 0.00001077
Iteration 43/1000 | Loss: 0.00001076
Iteration 44/1000 | Loss: 0.00001075
Iteration 45/1000 | Loss: 0.00001075
Iteration 46/1000 | Loss: 0.00001075
Iteration 47/1000 | Loss: 0.00001075
Iteration 48/1000 | Loss: 0.00001075
Iteration 49/1000 | Loss: 0.00001074
Iteration 50/1000 | Loss: 0.00001071
Iteration 51/1000 | Loss: 0.00001069
Iteration 52/1000 | Loss: 0.00001068
Iteration 53/1000 | Loss: 0.00001068
Iteration 54/1000 | Loss: 0.00001068
Iteration 55/1000 | Loss: 0.00001068
Iteration 56/1000 | Loss: 0.00001068
Iteration 57/1000 | Loss: 0.00001068
Iteration 58/1000 | Loss: 0.00001068
Iteration 59/1000 | Loss: 0.00001068
Iteration 60/1000 | Loss: 0.00001068
Iteration 61/1000 | Loss: 0.00001068
Iteration 62/1000 | Loss: 0.00001068
Iteration 63/1000 | Loss: 0.00001068
Iteration 64/1000 | Loss: 0.00001068
Iteration 65/1000 | Loss: 0.00001068
Iteration 66/1000 | Loss: 0.00001068
Iteration 67/1000 | Loss: 0.00001068
Iteration 68/1000 | Loss: 0.00001068
Iteration 69/1000 | Loss: 0.00001068
Iteration 70/1000 | Loss: 0.00001068
Iteration 71/1000 | Loss: 0.00001068
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 71. Stopping optimization.
Last 5 losses: [1.0682590072974563e-05, 1.0682590072974563e-05, 1.0682590072974563e-05, 1.0682590072974563e-05, 1.0682590072974563e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0682590072974563e-05

Optimization complete. Final v2v error: 2.798414468765259 mm

Highest mean error: 3.1667301654815674 mm for frame 47

Lowest mean error: 2.5475974082946777 mm for frame 12

Saving results

Total time: 32.52709460258484
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_022/1015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_022/1015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_022/1015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00963155
Iteration 2/25 | Loss: 0.00239515
Iteration 3/25 | Loss: 0.00150809
Iteration 4/25 | Loss: 0.00140523
Iteration 5/25 | Loss: 0.00144867
Iteration 6/25 | Loss: 0.00135015
Iteration 7/25 | Loss: 0.00125565
Iteration 8/25 | Loss: 0.00121805
Iteration 9/25 | Loss: 0.00120859
Iteration 10/25 | Loss: 0.00119280
Iteration 11/25 | Loss: 0.00120368
Iteration 12/25 | Loss: 0.00119220
Iteration 13/25 | Loss: 0.00118935
Iteration 14/25 | Loss: 0.00118754
Iteration 15/25 | Loss: 0.00118310
Iteration 16/25 | Loss: 0.00118245
Iteration 17/25 | Loss: 0.00118230
Iteration 18/25 | Loss: 0.00118218
Iteration 19/25 | Loss: 0.00118216
Iteration 20/25 | Loss: 0.00118216
Iteration 21/25 | Loss: 0.00118216
Iteration 22/25 | Loss: 0.00118216
Iteration 23/25 | Loss: 0.00118216
Iteration 24/25 | Loss: 0.00118216
Iteration 25/25 | Loss: 0.00118216

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33597660
Iteration 2/25 | Loss: 0.00092917
Iteration 3/25 | Loss: 0.00084563
Iteration 4/25 | Loss: 0.00084563
Iteration 5/25 | Loss: 0.00084562
Iteration 6/25 | Loss: 0.00084562
Iteration 7/25 | Loss: 0.00084562
Iteration 8/25 | Loss: 0.00084562
Iteration 9/25 | Loss: 0.00084562
Iteration 10/25 | Loss: 0.00084562
Iteration 11/25 | Loss: 0.00084562
Iteration 12/25 | Loss: 0.00084562
Iteration 13/25 | Loss: 0.00084562
Iteration 14/25 | Loss: 0.00084562
Iteration 15/25 | Loss: 0.00084562
Iteration 16/25 | Loss: 0.00084562
Iteration 17/25 | Loss: 0.00084562
Iteration 18/25 | Loss: 0.00084562
Iteration 19/25 | Loss: 0.00084562
Iteration 20/25 | Loss: 0.00084562
Iteration 21/25 | Loss: 0.00084562
Iteration 22/25 | Loss: 0.00084562
Iteration 23/25 | Loss: 0.00084562
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0008456220384687185, 0.0008456220384687185, 0.0008456220384687185, 0.0008456220384687185, 0.0008456220384687185]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008456220384687185

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00084562
Iteration 2/1000 | Loss: 0.00006530
Iteration 3/1000 | Loss: 0.00010340
Iteration 4/1000 | Loss: 0.00003622
Iteration 5/1000 | Loss: 0.00003234
Iteration 6/1000 | Loss: 0.00003061
Iteration 7/1000 | Loss: 0.00002947
Iteration 8/1000 | Loss: 0.00002874
Iteration 9/1000 | Loss: 0.00007945
Iteration 10/1000 | Loss: 0.00002786
Iteration 11/1000 | Loss: 0.00002734
Iteration 12/1000 | Loss: 0.00007980
Iteration 13/1000 | Loss: 0.00047451
Iteration 14/1000 | Loss: 0.00085224
Iteration 15/1000 | Loss: 0.00015040
Iteration 16/1000 | Loss: 0.00003135
Iteration 17/1000 | Loss: 0.00002687
Iteration 18/1000 | Loss: 0.00002567
Iteration 19/1000 | Loss: 0.00002483
Iteration 20/1000 | Loss: 0.00072524
Iteration 21/1000 | Loss: 0.00056008
Iteration 22/1000 | Loss: 0.00013591
Iteration 23/1000 | Loss: 0.00003453
Iteration 24/1000 | Loss: 0.00002794
Iteration 25/1000 | Loss: 0.00002489
Iteration 26/1000 | Loss: 0.00002369
Iteration 27/1000 | Loss: 0.00002312
Iteration 28/1000 | Loss: 0.00002223
Iteration 29/1000 | Loss: 0.00002160
Iteration 30/1000 | Loss: 0.00053468
Iteration 31/1000 | Loss: 0.00002815
Iteration 32/1000 | Loss: 0.00002313
Iteration 33/1000 | Loss: 0.00002119
Iteration 34/1000 | Loss: 0.00002055
Iteration 35/1000 | Loss: 0.00002003
Iteration 36/1000 | Loss: 0.00001960
Iteration 37/1000 | Loss: 0.00001922
Iteration 38/1000 | Loss: 0.00001895
Iteration 39/1000 | Loss: 0.00001878
Iteration 40/1000 | Loss: 0.00001852
Iteration 41/1000 | Loss: 0.00001834
Iteration 42/1000 | Loss: 0.00001815
Iteration 43/1000 | Loss: 0.00001793
Iteration 44/1000 | Loss: 0.00001785
Iteration 45/1000 | Loss: 0.00001781
Iteration 46/1000 | Loss: 0.00001779
Iteration 47/1000 | Loss: 0.00001779
Iteration 48/1000 | Loss: 0.00001778
Iteration 49/1000 | Loss: 0.00001777
Iteration 50/1000 | Loss: 0.00001777
Iteration 51/1000 | Loss: 0.00001777
Iteration 52/1000 | Loss: 0.00001776
Iteration 53/1000 | Loss: 0.00001775
Iteration 54/1000 | Loss: 0.00001775
Iteration 55/1000 | Loss: 0.00001774
Iteration 56/1000 | Loss: 0.00001774
Iteration 57/1000 | Loss: 0.00001773
Iteration 58/1000 | Loss: 0.00001773
Iteration 59/1000 | Loss: 0.00001769
Iteration 60/1000 | Loss: 0.00001769
Iteration 61/1000 | Loss: 0.00001769
Iteration 62/1000 | Loss: 0.00001768
Iteration 63/1000 | Loss: 0.00001767
Iteration 64/1000 | Loss: 0.00001767
Iteration 65/1000 | Loss: 0.00001766
Iteration 66/1000 | Loss: 0.00001766
Iteration 67/1000 | Loss: 0.00001766
Iteration 68/1000 | Loss: 0.00001766
Iteration 69/1000 | Loss: 0.00001766
Iteration 70/1000 | Loss: 0.00001766
Iteration 71/1000 | Loss: 0.00001766
Iteration 72/1000 | Loss: 0.00001766
Iteration 73/1000 | Loss: 0.00001766
Iteration 74/1000 | Loss: 0.00001766
Iteration 75/1000 | Loss: 0.00001765
Iteration 76/1000 | Loss: 0.00001765
Iteration 77/1000 | Loss: 0.00001765
Iteration 78/1000 | Loss: 0.00001764
Iteration 79/1000 | Loss: 0.00001764
Iteration 80/1000 | Loss: 0.00001764
Iteration 81/1000 | Loss: 0.00001763
Iteration 82/1000 | Loss: 0.00001763
Iteration 83/1000 | Loss: 0.00001763
Iteration 84/1000 | Loss: 0.00001762
Iteration 85/1000 | Loss: 0.00001762
Iteration 86/1000 | Loss: 0.00001762
Iteration 87/1000 | Loss: 0.00001762
Iteration 88/1000 | Loss: 0.00001762
Iteration 89/1000 | Loss: 0.00001761
Iteration 90/1000 | Loss: 0.00001761
Iteration 91/1000 | Loss: 0.00001761
Iteration 92/1000 | Loss: 0.00001761
Iteration 93/1000 | Loss: 0.00001761
Iteration 94/1000 | Loss: 0.00001761
Iteration 95/1000 | Loss: 0.00001760
Iteration 96/1000 | Loss: 0.00001760
Iteration 97/1000 | Loss: 0.00001760
Iteration 98/1000 | Loss: 0.00001760
Iteration 99/1000 | Loss: 0.00001760
Iteration 100/1000 | Loss: 0.00001760
Iteration 101/1000 | Loss: 0.00001760
Iteration 102/1000 | Loss: 0.00001760
Iteration 103/1000 | Loss: 0.00001760
Iteration 104/1000 | Loss: 0.00001760
Iteration 105/1000 | Loss: 0.00001760
Iteration 106/1000 | Loss: 0.00001760
Iteration 107/1000 | Loss: 0.00001760
Iteration 108/1000 | Loss: 0.00001760
Iteration 109/1000 | Loss: 0.00001760
Iteration 110/1000 | Loss: 0.00001760
Iteration 111/1000 | Loss: 0.00001760
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 111. Stopping optimization.
Last 5 losses: [1.7599519196664914e-05, 1.7599519196664914e-05, 1.7599519196664914e-05, 1.7599519196664914e-05, 1.7599519196664914e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7599519196664914e-05

Optimization complete. Final v2v error: 3.1858043670654297 mm

Highest mean error: 11.181622505187988 mm for frame 13

Lowest mean error: 2.626296043395996 mm for frame 132

Saving results

Total time: 116.32131218910217
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_022/1053/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_022/1053.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_022/1053
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01015306
Iteration 2/25 | Loss: 0.00241400
Iteration 3/25 | Loss: 0.00183267
Iteration 4/25 | Loss: 0.00171737
Iteration 5/25 | Loss: 0.00142982
Iteration 6/25 | Loss: 0.00146304
Iteration 7/25 | Loss: 0.00138131
Iteration 8/25 | Loss: 0.00133797
Iteration 9/25 | Loss: 0.00137010
Iteration 10/25 | Loss: 0.00139986
Iteration 11/25 | Loss: 0.00131257
Iteration 12/25 | Loss: 0.00129751
Iteration 13/25 | Loss: 0.00131538
Iteration 14/25 | Loss: 0.00130524
Iteration 15/25 | Loss: 0.00130264
Iteration 16/25 | Loss: 0.00125539
Iteration 17/25 | Loss: 0.00123100
Iteration 18/25 | Loss: 0.00122435
Iteration 19/25 | Loss: 0.00122666
Iteration 20/25 | Loss: 0.00122566
Iteration 21/25 | Loss: 0.00122171
Iteration 22/25 | Loss: 0.00122133
Iteration 23/25 | Loss: 0.00122133
Iteration 24/25 | Loss: 0.00122133
Iteration 25/25 | Loss: 0.00122133

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.40696573
Iteration 2/25 | Loss: 0.00127962
Iteration 3/25 | Loss: 0.00120163
Iteration 4/25 | Loss: 0.00120163
Iteration 5/25 | Loss: 0.00120162
Iteration 6/25 | Loss: 0.00120162
Iteration 7/25 | Loss: 0.00120162
Iteration 8/25 | Loss: 0.00120162
Iteration 9/25 | Loss: 0.00120162
Iteration 10/25 | Loss: 0.00120162
Iteration 11/25 | Loss: 0.00120162
Iteration 12/25 | Loss: 0.00120162
Iteration 13/25 | Loss: 0.00120162
Iteration 14/25 | Loss: 0.00120162
Iteration 15/25 | Loss: 0.00120162
Iteration 16/25 | Loss: 0.00120162
Iteration 17/25 | Loss: 0.00120162
Iteration 18/25 | Loss: 0.00120162
Iteration 19/25 | Loss: 0.00120162
Iteration 20/25 | Loss: 0.00120162
Iteration 21/25 | Loss: 0.00120162
Iteration 22/25 | Loss: 0.00120162
Iteration 23/25 | Loss: 0.00120162
Iteration 24/25 | Loss: 0.00120162
Iteration 25/25 | Loss: 0.00120162
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.001201620907522738, 0.001201620907522738, 0.001201620907522738, 0.001201620907522738, 0.001201620907522738]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001201620907522738

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00120162
Iteration 2/1000 | Loss: 0.00010853
Iteration 3/1000 | Loss: 0.00025831
Iteration 4/1000 | Loss: 0.00022345
Iteration 5/1000 | Loss: 0.00010690
Iteration 6/1000 | Loss: 0.00005249
Iteration 7/1000 | Loss: 0.00015899
Iteration 8/1000 | Loss: 0.00050773
Iteration 9/1000 | Loss: 0.00005480
Iteration 10/1000 | Loss: 0.00005850
Iteration 11/1000 | Loss: 0.00004571
Iteration 12/1000 | Loss: 0.00033314
Iteration 13/1000 | Loss: 0.00014015
Iteration 14/1000 | Loss: 0.00005869
Iteration 15/1000 | Loss: 0.00022158
Iteration 16/1000 | Loss: 0.00006454
Iteration 17/1000 | Loss: 0.00004545
Iteration 18/1000 | Loss: 0.00019957
Iteration 19/1000 | Loss: 0.00005379
Iteration 20/1000 | Loss: 0.00013453
Iteration 21/1000 | Loss: 0.00042212
Iteration 22/1000 | Loss: 0.00167490
Iteration 23/1000 | Loss: 0.00016640
Iteration 24/1000 | Loss: 0.00007936
Iteration 25/1000 | Loss: 0.00003668
Iteration 26/1000 | Loss: 0.00068416
Iteration 27/1000 | Loss: 0.00197682
Iteration 28/1000 | Loss: 0.00011486
Iteration 29/1000 | Loss: 0.00190686
Iteration 30/1000 | Loss: 0.00015288
Iteration 31/1000 | Loss: 0.00007258
Iteration 32/1000 | Loss: 0.00006118
Iteration 33/1000 | Loss: 0.00106969
Iteration 34/1000 | Loss: 0.00013029
Iteration 35/1000 | Loss: 0.00023047
Iteration 36/1000 | Loss: 0.00006897
Iteration 37/1000 | Loss: 0.00018965
Iteration 38/1000 | Loss: 0.00005432
Iteration 39/1000 | Loss: 0.00003519
Iteration 40/1000 | Loss: 0.00002569
Iteration 41/1000 | Loss: 0.00004298
Iteration 42/1000 | Loss: 0.00002172
Iteration 43/1000 | Loss: 0.00012840
Iteration 44/1000 | Loss: 0.00002085
Iteration 45/1000 | Loss: 0.00011571
Iteration 46/1000 | Loss: 0.00025686
Iteration 47/1000 | Loss: 0.00006230
Iteration 48/1000 | Loss: 0.00019389
Iteration 49/1000 | Loss: 0.00003771
Iteration 50/1000 | Loss: 0.00004139
Iteration 51/1000 | Loss: 0.00001956
Iteration 52/1000 | Loss: 0.00010514
Iteration 53/1000 | Loss: 0.00093290
Iteration 54/1000 | Loss: 0.00012343
Iteration 55/1000 | Loss: 0.00003802
Iteration 56/1000 | Loss: 0.00010735
Iteration 57/1000 | Loss: 0.00007614
Iteration 58/1000 | Loss: 0.00003619
Iteration 59/1000 | Loss: 0.00003351
Iteration 60/1000 | Loss: 0.00010726
Iteration 61/1000 | Loss: 0.00001877
Iteration 62/1000 | Loss: 0.00001871
Iteration 63/1000 | Loss: 0.00006119
Iteration 64/1000 | Loss: 0.00007725
Iteration 65/1000 | Loss: 0.00021964
Iteration 66/1000 | Loss: 0.00003044
Iteration 67/1000 | Loss: 0.00003822
Iteration 68/1000 | Loss: 0.00008335
Iteration 69/1000 | Loss: 0.00002552
Iteration 70/1000 | Loss: 0.00002154
Iteration 71/1000 | Loss: 0.00001849
Iteration 72/1000 | Loss: 0.00004559
Iteration 73/1000 | Loss: 0.00001829
Iteration 74/1000 | Loss: 0.00001819
Iteration 75/1000 | Loss: 0.00001804
Iteration 76/1000 | Loss: 0.00001803
Iteration 77/1000 | Loss: 0.00001798
Iteration 78/1000 | Loss: 0.00001796
Iteration 79/1000 | Loss: 0.00001794
Iteration 80/1000 | Loss: 0.00001793
Iteration 81/1000 | Loss: 0.00007641
Iteration 82/1000 | Loss: 0.00017001
Iteration 83/1000 | Loss: 0.00002464
Iteration 84/1000 | Loss: 0.00004538
Iteration 85/1000 | Loss: 0.00001815
Iteration 86/1000 | Loss: 0.00008946
Iteration 87/1000 | Loss: 0.00001823
Iteration 88/1000 | Loss: 0.00003161
Iteration 89/1000 | Loss: 0.00001795
Iteration 90/1000 | Loss: 0.00002371
Iteration 91/1000 | Loss: 0.00001788
Iteration 92/1000 | Loss: 0.00001786
Iteration 93/1000 | Loss: 0.00001786
Iteration 94/1000 | Loss: 0.00001786
Iteration 95/1000 | Loss: 0.00001785
Iteration 96/1000 | Loss: 0.00001785
Iteration 97/1000 | Loss: 0.00001785
Iteration 98/1000 | Loss: 0.00001785
Iteration 99/1000 | Loss: 0.00001785
Iteration 100/1000 | Loss: 0.00001785
Iteration 101/1000 | Loss: 0.00001785
Iteration 102/1000 | Loss: 0.00001784
Iteration 103/1000 | Loss: 0.00001784
Iteration 104/1000 | Loss: 0.00001784
Iteration 105/1000 | Loss: 0.00001783
Iteration 106/1000 | Loss: 0.00001783
Iteration 107/1000 | Loss: 0.00001783
Iteration 108/1000 | Loss: 0.00001783
Iteration 109/1000 | Loss: 0.00001782
Iteration 110/1000 | Loss: 0.00001782
Iteration 111/1000 | Loss: 0.00001782
Iteration 112/1000 | Loss: 0.00001782
Iteration 113/1000 | Loss: 0.00001782
Iteration 114/1000 | Loss: 0.00001781
Iteration 115/1000 | Loss: 0.00001781
Iteration 116/1000 | Loss: 0.00001781
Iteration 117/1000 | Loss: 0.00003790
Iteration 118/1000 | Loss: 0.00001794
Iteration 119/1000 | Loss: 0.00001782
Iteration 120/1000 | Loss: 0.00001782
Iteration 121/1000 | Loss: 0.00001782
Iteration 122/1000 | Loss: 0.00001782
Iteration 123/1000 | Loss: 0.00001781
Iteration 124/1000 | Loss: 0.00001781
Iteration 125/1000 | Loss: 0.00001781
Iteration 126/1000 | Loss: 0.00001781
Iteration 127/1000 | Loss: 0.00001781
Iteration 128/1000 | Loss: 0.00001781
Iteration 129/1000 | Loss: 0.00001781
Iteration 130/1000 | Loss: 0.00001781
Iteration 131/1000 | Loss: 0.00001781
Iteration 132/1000 | Loss: 0.00001781
Iteration 133/1000 | Loss: 0.00001781
Iteration 134/1000 | Loss: 0.00001780
Iteration 135/1000 | Loss: 0.00001780
Iteration 136/1000 | Loss: 0.00001780
Iteration 137/1000 | Loss: 0.00001780
Iteration 138/1000 | Loss: 0.00001780
Iteration 139/1000 | Loss: 0.00001780
Iteration 140/1000 | Loss: 0.00001779
Iteration 141/1000 | Loss: 0.00001779
Iteration 142/1000 | Loss: 0.00001779
Iteration 143/1000 | Loss: 0.00001779
Iteration 144/1000 | Loss: 0.00001779
Iteration 145/1000 | Loss: 0.00001779
Iteration 146/1000 | Loss: 0.00001779
Iteration 147/1000 | Loss: 0.00001779
Iteration 148/1000 | Loss: 0.00001779
Iteration 149/1000 | Loss: 0.00001778
Iteration 150/1000 | Loss: 0.00001778
Iteration 151/1000 | Loss: 0.00001778
Iteration 152/1000 | Loss: 0.00001778
Iteration 153/1000 | Loss: 0.00001778
Iteration 154/1000 | Loss: 0.00001778
Iteration 155/1000 | Loss: 0.00001778
Iteration 156/1000 | Loss: 0.00001777
Iteration 157/1000 | Loss: 0.00001777
Iteration 158/1000 | Loss: 0.00001777
Iteration 159/1000 | Loss: 0.00001777
Iteration 160/1000 | Loss: 0.00001777
Iteration 161/1000 | Loss: 0.00001777
Iteration 162/1000 | Loss: 0.00001777
Iteration 163/1000 | Loss: 0.00001777
Iteration 164/1000 | Loss: 0.00001777
Iteration 165/1000 | Loss: 0.00001776
Iteration 166/1000 | Loss: 0.00001776
Iteration 167/1000 | Loss: 0.00001776
Iteration 168/1000 | Loss: 0.00001776
Iteration 169/1000 | Loss: 0.00001776
Iteration 170/1000 | Loss: 0.00001776
Iteration 171/1000 | Loss: 0.00001776
Iteration 172/1000 | Loss: 0.00001776
Iteration 173/1000 | Loss: 0.00001776
Iteration 174/1000 | Loss: 0.00001776
Iteration 175/1000 | Loss: 0.00001776
Iteration 176/1000 | Loss: 0.00001776
Iteration 177/1000 | Loss: 0.00001775
Iteration 178/1000 | Loss: 0.00001775
Iteration 179/1000 | Loss: 0.00001775
Iteration 180/1000 | Loss: 0.00001775
Iteration 181/1000 | Loss: 0.00001775
Iteration 182/1000 | Loss: 0.00001775
Iteration 183/1000 | Loss: 0.00001775
Iteration 184/1000 | Loss: 0.00001775
Iteration 185/1000 | Loss: 0.00001775
Iteration 186/1000 | Loss: 0.00001775
Iteration 187/1000 | Loss: 0.00001775
Iteration 188/1000 | Loss: 0.00001774
Iteration 189/1000 | Loss: 0.00001774
Iteration 190/1000 | Loss: 0.00001774
Iteration 191/1000 | Loss: 0.00001774
Iteration 192/1000 | Loss: 0.00001774
Iteration 193/1000 | Loss: 0.00001774
Iteration 194/1000 | Loss: 0.00005524
Iteration 195/1000 | Loss: 0.00054148
Iteration 196/1000 | Loss: 0.00002218
Iteration 197/1000 | Loss: 0.00003092
Iteration 198/1000 | Loss: 0.00001785
Iteration 199/1000 | Loss: 0.00001783
Iteration 200/1000 | Loss: 0.00001779
Iteration 201/1000 | Loss: 0.00001779
Iteration 202/1000 | Loss: 0.00001778
Iteration 203/1000 | Loss: 0.00001778
Iteration 204/1000 | Loss: 0.00001778
Iteration 205/1000 | Loss: 0.00003567
Iteration 206/1000 | Loss: 0.00002657
Iteration 207/1000 | Loss: 0.00001779
Iteration 208/1000 | Loss: 0.00001773
Iteration 209/1000 | Loss: 0.00001773
Iteration 210/1000 | Loss: 0.00001773
Iteration 211/1000 | Loss: 0.00001773
Iteration 212/1000 | Loss: 0.00001773
Iteration 213/1000 | Loss: 0.00001773
Iteration 214/1000 | Loss: 0.00001773
Iteration 215/1000 | Loss: 0.00001772
Iteration 216/1000 | Loss: 0.00001772
Iteration 217/1000 | Loss: 0.00001772
Iteration 218/1000 | Loss: 0.00001772
Iteration 219/1000 | Loss: 0.00001772
Iteration 220/1000 | Loss: 0.00001772
Iteration 221/1000 | Loss: 0.00001772
Iteration 222/1000 | Loss: 0.00001771
Iteration 223/1000 | Loss: 0.00001771
Iteration 224/1000 | Loss: 0.00001771
Iteration 225/1000 | Loss: 0.00001771
Iteration 226/1000 | Loss: 0.00001771
Iteration 227/1000 | Loss: 0.00001771
Iteration 228/1000 | Loss: 0.00001771
Iteration 229/1000 | Loss: 0.00001771
Iteration 230/1000 | Loss: 0.00001771
Iteration 231/1000 | Loss: 0.00001771
Iteration 232/1000 | Loss: 0.00001771
Iteration 233/1000 | Loss: 0.00001771
Iteration 234/1000 | Loss: 0.00001771
Iteration 235/1000 | Loss: 0.00001771
Iteration 236/1000 | Loss: 0.00001771
Iteration 237/1000 | Loss: 0.00001771
Iteration 238/1000 | Loss: 0.00001771
Iteration 239/1000 | Loss: 0.00001771
Iteration 240/1000 | Loss: 0.00001771
Iteration 241/1000 | Loss: 0.00001771
Iteration 242/1000 | Loss: 0.00001771
Iteration 243/1000 | Loss: 0.00001771
Iteration 244/1000 | Loss: 0.00001771
Iteration 245/1000 | Loss: 0.00001771
Iteration 246/1000 | Loss: 0.00001771
Iteration 247/1000 | Loss: 0.00001771
Iteration 248/1000 | Loss: 0.00001771
Iteration 249/1000 | Loss: 0.00001771
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 249. Stopping optimization.
Last 5 losses: [1.7705668142298236e-05, 1.7705668142298236e-05, 1.7705668142298236e-05, 1.7705668142298236e-05, 1.7705668142298236e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7705668142298236e-05

Optimization complete. Final v2v error: 3.521073579788208 mm

Highest mean error: 4.1927595138549805 mm for frame 80

Lowest mean error: 3.0763440132141113 mm for frame 34

Saving results

Total time: 179.81321907043457
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_022/1037/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_022/1037.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_022/1037
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01041090
Iteration 2/25 | Loss: 0.00233113
Iteration 3/25 | Loss: 0.00169391
Iteration 4/25 | Loss: 0.00167905
Iteration 5/25 | Loss: 0.00167298
Iteration 6/25 | Loss: 0.00160641
Iteration 7/25 | Loss: 0.00155471
Iteration 8/25 | Loss: 0.00154337
Iteration 9/25 | Loss: 0.00148788
Iteration 10/25 | Loss: 0.00145071
Iteration 11/25 | Loss: 0.00143915
Iteration 12/25 | Loss: 0.00142823
Iteration 13/25 | Loss: 0.00142828
Iteration 14/25 | Loss: 0.00143919
Iteration 15/25 | Loss: 0.00142063
Iteration 16/25 | Loss: 0.00141215
Iteration 17/25 | Loss: 0.00141575
Iteration 18/25 | Loss: 0.00141164
Iteration 19/25 | Loss: 0.00141035
Iteration 20/25 | Loss: 0.00140038
Iteration 21/25 | Loss: 0.00140369
Iteration 22/25 | Loss: 0.00140652
Iteration 23/25 | Loss: 0.00140196
Iteration 24/25 | Loss: 0.00140434
Iteration 25/25 | Loss: 0.00140420

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.31820548
Iteration 2/25 | Loss: 0.00358781
Iteration 3/25 | Loss: 0.00322848
Iteration 4/25 | Loss: 0.00322848
Iteration 5/25 | Loss: 0.00322847
Iteration 6/25 | Loss: 0.00322847
Iteration 7/25 | Loss: 0.00322847
Iteration 8/25 | Loss: 0.00322847
Iteration 9/25 | Loss: 0.00322847
Iteration 10/25 | Loss: 0.00322847
Iteration 11/25 | Loss: 0.00322847
Iteration 12/25 | Loss: 0.00322847
Iteration 13/25 | Loss: 0.00322847
Iteration 14/25 | Loss: 0.00322847
Iteration 15/25 | Loss: 0.00322847
Iteration 16/25 | Loss: 0.00322847
Iteration 17/25 | Loss: 0.00322847
Iteration 18/25 | Loss: 0.00322847
Iteration 19/25 | Loss: 0.00322847
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0032284711487591267, 0.0032284711487591267, 0.0032284711487591267, 0.0032284711487591267, 0.0032284711487591267]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0032284711487591267

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00322847
Iteration 2/1000 | Loss: 0.00233804
Iteration 3/1000 | Loss: 0.00061640
Iteration 4/1000 | Loss: 0.00091029
Iteration 5/1000 | Loss: 0.00084237
Iteration 6/1000 | Loss: 0.00076598
Iteration 7/1000 | Loss: 0.00254531
Iteration 8/1000 | Loss: 0.00448216
Iteration 9/1000 | Loss: 0.00396344
Iteration 10/1000 | Loss: 0.00226223
Iteration 11/1000 | Loss: 0.00091075
Iteration 12/1000 | Loss: 0.00027777
Iteration 13/1000 | Loss: 0.00097042
Iteration 14/1000 | Loss: 0.00091344
Iteration 15/1000 | Loss: 0.00242525
Iteration 16/1000 | Loss: 0.00039700
Iteration 17/1000 | Loss: 0.00026569
Iteration 18/1000 | Loss: 0.00035491
Iteration 19/1000 | Loss: 0.00030667
Iteration 20/1000 | Loss: 0.00042796
Iteration 21/1000 | Loss: 0.00429349
Iteration 22/1000 | Loss: 0.00212961
Iteration 23/1000 | Loss: 0.00035122
Iteration 24/1000 | Loss: 0.00087795
Iteration 25/1000 | Loss: 0.00054080
Iteration 26/1000 | Loss: 0.00255977
Iteration 27/1000 | Loss: 0.00128658
Iteration 28/1000 | Loss: 0.00297817
Iteration 29/1000 | Loss: 0.00164387
Iteration 30/1000 | Loss: 0.00122453
Iteration 31/1000 | Loss: 0.00700774
Iteration 32/1000 | Loss: 0.00601354
Iteration 33/1000 | Loss: 0.01636011
Iteration 34/1000 | Loss: 0.00570393
Iteration 35/1000 | Loss: 0.00636577
Iteration 36/1000 | Loss: 0.00565037
Iteration 37/1000 | Loss: 0.00163506
Iteration 38/1000 | Loss: 0.00078787
Iteration 39/1000 | Loss: 0.00179558
Iteration 40/1000 | Loss: 0.00190140
Iteration 41/1000 | Loss: 0.00104945
Iteration 42/1000 | Loss: 0.00163492
Iteration 43/1000 | Loss: 0.00231065
Iteration 44/1000 | Loss: 0.00189417
Iteration 45/1000 | Loss: 0.00236110
Iteration 46/1000 | Loss: 0.00291464
Iteration 47/1000 | Loss: 0.00251946
Iteration 48/1000 | Loss: 0.00323755
Iteration 49/1000 | Loss: 0.00162918
Iteration 50/1000 | Loss: 0.00147106
Iteration 51/1000 | Loss: 0.00077153
Iteration 52/1000 | Loss: 0.00030245
Iteration 53/1000 | Loss: 0.00107174
Iteration 54/1000 | Loss: 0.00100974
Iteration 55/1000 | Loss: 0.00098273
Iteration 56/1000 | Loss: 0.00045650
Iteration 57/1000 | Loss: 0.00073176
Iteration 58/1000 | Loss: 0.00048603
Iteration 59/1000 | Loss: 0.00116022
Iteration 60/1000 | Loss: 0.00035121
Iteration 61/1000 | Loss: 0.00098729
Iteration 62/1000 | Loss: 0.00012183
Iteration 63/1000 | Loss: 0.00033774
Iteration 64/1000 | Loss: 0.00011261
Iteration 65/1000 | Loss: 0.00031396
Iteration 66/1000 | Loss: 0.00013295
Iteration 67/1000 | Loss: 0.00009537
Iteration 68/1000 | Loss: 0.00007146
Iteration 69/1000 | Loss: 0.00010822
Iteration 70/1000 | Loss: 0.00052051
Iteration 71/1000 | Loss: 0.00010573
Iteration 72/1000 | Loss: 0.00010610
Iteration 73/1000 | Loss: 0.00005851
Iteration 74/1000 | Loss: 0.00011644
Iteration 75/1000 | Loss: 0.00030801
Iteration 76/1000 | Loss: 0.00004822
Iteration 77/1000 | Loss: 0.00037944
Iteration 78/1000 | Loss: 0.00108899
Iteration 79/1000 | Loss: 0.00061341
Iteration 80/1000 | Loss: 0.00049076
Iteration 81/1000 | Loss: 0.00022064
Iteration 82/1000 | Loss: 0.00064945
Iteration 83/1000 | Loss: 0.00021515
Iteration 84/1000 | Loss: 0.00044637
Iteration 85/1000 | Loss: 0.00007469
Iteration 86/1000 | Loss: 0.00004561
Iteration 87/1000 | Loss: 0.00043027
Iteration 88/1000 | Loss: 0.00039687
Iteration 89/1000 | Loss: 0.00006694
Iteration 90/1000 | Loss: 0.00006700
Iteration 91/1000 | Loss: 0.00046114
Iteration 92/1000 | Loss: 0.00015132
Iteration 93/1000 | Loss: 0.00005264
Iteration 94/1000 | Loss: 0.00003249
Iteration 95/1000 | Loss: 0.00008546
Iteration 96/1000 | Loss: 0.00004479
Iteration 97/1000 | Loss: 0.00007466
Iteration 98/1000 | Loss: 0.00002754
Iteration 99/1000 | Loss: 0.00006100
Iteration 100/1000 | Loss: 0.00004123
Iteration 101/1000 | Loss: 0.00003802
Iteration 102/1000 | Loss: 0.00007974
Iteration 103/1000 | Loss: 0.00017890
Iteration 104/1000 | Loss: 0.00073614
Iteration 105/1000 | Loss: 0.00010895
Iteration 106/1000 | Loss: 0.00005572
Iteration 107/1000 | Loss: 0.00004308
Iteration 108/1000 | Loss: 0.00005972
Iteration 109/1000 | Loss: 0.00004220
Iteration 110/1000 | Loss: 0.00002677
Iteration 111/1000 | Loss: 0.00005780
Iteration 112/1000 | Loss: 0.00003828
Iteration 113/1000 | Loss: 0.00003831
Iteration 114/1000 | Loss: 0.00003694
Iteration 115/1000 | Loss: 0.00002828
Iteration 116/1000 | Loss: 0.00003514
Iteration 117/1000 | Loss: 0.00003581
Iteration 118/1000 | Loss: 0.00002983
Iteration 119/1000 | Loss: 0.00005476
Iteration 120/1000 | Loss: 0.00004349
Iteration 121/1000 | Loss: 0.00003896
Iteration 122/1000 | Loss: 0.00006705
Iteration 123/1000 | Loss: 0.00011005
Iteration 124/1000 | Loss: 0.00004018
Iteration 125/1000 | Loss: 0.00003602
Iteration 126/1000 | Loss: 0.00003039
Iteration 127/1000 | Loss: 0.00002758
Iteration 128/1000 | Loss: 0.00003000
Iteration 129/1000 | Loss: 0.00007487
Iteration 130/1000 | Loss: 0.00003770
Iteration 131/1000 | Loss: 0.00003614
Iteration 132/1000 | Loss: 0.00005654
Iteration 133/1000 | Loss: 0.00003914
Iteration 134/1000 | Loss: 0.00004213
Iteration 135/1000 | Loss: 0.00004690
Iteration 136/1000 | Loss: 0.00004648
Iteration 137/1000 | Loss: 0.00009143
Iteration 138/1000 | Loss: 0.00004087
Iteration 139/1000 | Loss: 0.00004867
Iteration 140/1000 | Loss: 0.00003886
Iteration 141/1000 | Loss: 0.00002986
Iteration 142/1000 | Loss: 0.00002951
Iteration 143/1000 | Loss: 0.00004239
Iteration 144/1000 | Loss: 0.00010997
Iteration 145/1000 | Loss: 0.00004806
Iteration 146/1000 | Loss: 0.00003189
Iteration 147/1000 | Loss: 0.00007080
Iteration 148/1000 | Loss: 0.00004344
Iteration 149/1000 | Loss: 0.00002910
Iteration 150/1000 | Loss: 0.00002179
Iteration 151/1000 | Loss: 0.00005120
Iteration 152/1000 | Loss: 0.00002012
Iteration 153/1000 | Loss: 0.00002912
Iteration 154/1000 | Loss: 0.00006373
Iteration 155/1000 | Loss: 0.00001894
Iteration 156/1000 | Loss: 0.00002822
Iteration 157/1000 | Loss: 0.00002167
Iteration 158/1000 | Loss: 0.00001849
Iteration 159/1000 | Loss: 0.00001844
Iteration 160/1000 | Loss: 0.00026996
Iteration 161/1000 | Loss: 0.00012674
Iteration 162/1000 | Loss: 0.00014617
Iteration 163/1000 | Loss: 0.00073243
Iteration 164/1000 | Loss: 0.00118134
Iteration 165/1000 | Loss: 0.00079869
Iteration 166/1000 | Loss: 0.00036887
Iteration 167/1000 | Loss: 0.00048140
Iteration 168/1000 | Loss: 0.00030465
Iteration 169/1000 | Loss: 0.00022910
Iteration 170/1000 | Loss: 0.00004822
Iteration 171/1000 | Loss: 0.00047002
Iteration 172/1000 | Loss: 0.00008400
Iteration 173/1000 | Loss: 0.00021921
Iteration 174/1000 | Loss: 0.00002650
Iteration 175/1000 | Loss: 0.00004121
Iteration 176/1000 | Loss: 0.00001836
Iteration 177/1000 | Loss: 0.00002962
Iteration 178/1000 | Loss: 0.00001648
Iteration 179/1000 | Loss: 0.00002561
Iteration 180/1000 | Loss: 0.00002072
Iteration 181/1000 | Loss: 0.00001516
Iteration 182/1000 | Loss: 0.00002962
Iteration 183/1000 | Loss: 0.00002447
Iteration 184/1000 | Loss: 0.00002261
Iteration 185/1000 | Loss: 0.00001919
Iteration 186/1000 | Loss: 0.00003947
Iteration 187/1000 | Loss: 0.00003117
Iteration 188/1000 | Loss: 0.00002154
Iteration 189/1000 | Loss: 0.00001412
Iteration 190/1000 | Loss: 0.00001559
Iteration 191/1000 | Loss: 0.00001517
Iteration 192/1000 | Loss: 0.00001384
Iteration 193/1000 | Loss: 0.00001384
Iteration 194/1000 | Loss: 0.00002922
Iteration 195/1000 | Loss: 0.00001567
Iteration 196/1000 | Loss: 0.00001379
Iteration 197/1000 | Loss: 0.00001379
Iteration 198/1000 | Loss: 0.00001379
Iteration 199/1000 | Loss: 0.00001378
Iteration 200/1000 | Loss: 0.00001450
Iteration 201/1000 | Loss: 0.00002057
Iteration 202/1000 | Loss: 0.00001396
Iteration 203/1000 | Loss: 0.00001396
Iteration 204/1000 | Loss: 0.00001866
Iteration 205/1000 | Loss: 0.00001376
Iteration 206/1000 | Loss: 0.00002357
Iteration 207/1000 | Loss: 0.00001373
Iteration 208/1000 | Loss: 0.00001370
Iteration 209/1000 | Loss: 0.00002071
Iteration 210/1000 | Loss: 0.00002610
Iteration 211/1000 | Loss: 0.00001372
Iteration 212/1000 | Loss: 0.00001370
Iteration 213/1000 | Loss: 0.00001370
Iteration 214/1000 | Loss: 0.00001370
Iteration 215/1000 | Loss: 0.00001467
Iteration 216/1000 | Loss: 0.00001369
Iteration 217/1000 | Loss: 0.00001369
Iteration 218/1000 | Loss: 0.00001369
Iteration 219/1000 | Loss: 0.00001369
Iteration 220/1000 | Loss: 0.00001369
Iteration 221/1000 | Loss: 0.00001369
Iteration 222/1000 | Loss: 0.00001369
Iteration 223/1000 | Loss: 0.00001369
Iteration 224/1000 | Loss: 0.00001369
Iteration 225/1000 | Loss: 0.00001369
Iteration 226/1000 | Loss: 0.00001369
Iteration 227/1000 | Loss: 0.00001369
Iteration 228/1000 | Loss: 0.00001369
Iteration 229/1000 | Loss: 0.00001369
Iteration 230/1000 | Loss: 0.00001369
Iteration 231/1000 | Loss: 0.00001369
Iteration 232/1000 | Loss: 0.00001369
Iteration 233/1000 | Loss: 0.00001369
Iteration 234/1000 | Loss: 0.00001369
Iteration 235/1000 | Loss: 0.00001369
Iteration 236/1000 | Loss: 0.00001369
Iteration 237/1000 | Loss: 0.00001369
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 237. Stopping optimization.
Last 5 losses: [1.3687098544323817e-05, 1.3687098544323817e-05, 1.3687098544323817e-05, 1.3687098544323817e-05, 1.3687098544323817e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3687098544323817e-05

Optimization complete. Final v2v error: 3.042576313018799 mm

Highest mean error: 5.079728126525879 mm for frame 51

Lowest mean error: 2.5374579429626465 mm for frame 94

Saving results

Total time: 344.1943974494934
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_022/1051/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_022/1051.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_022/1051
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00386984
Iteration 2/25 | Loss: 0.00113325
Iteration 3/25 | Loss: 0.00108543
Iteration 4/25 | Loss: 0.00107943
Iteration 5/25 | Loss: 0.00107754
Iteration 6/25 | Loss: 0.00107754
Iteration 7/25 | Loss: 0.00107754
Iteration 8/25 | Loss: 0.00107754
Iteration 9/25 | Loss: 0.00107754
Iteration 10/25 | Loss: 0.00107754
Iteration 11/25 | Loss: 0.00107754
Iteration 12/25 | Loss: 0.00107754
Iteration 13/25 | Loss: 0.00107754
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0010775395203381777, 0.0010775395203381777, 0.0010775395203381777, 0.0010775395203381777, 0.0010775395203381777]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010775395203381777

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38005781
Iteration 2/25 | Loss: 0.00082268
Iteration 3/25 | Loss: 0.00082268
Iteration 4/25 | Loss: 0.00082268
Iteration 5/25 | Loss: 0.00082268
Iteration 6/25 | Loss: 0.00082268
Iteration 7/25 | Loss: 0.00082268
Iteration 8/25 | Loss: 0.00082268
Iteration 9/25 | Loss: 0.00082268
Iteration 10/25 | Loss: 0.00082268
Iteration 11/25 | Loss: 0.00082268
Iteration 12/25 | Loss: 0.00082268
Iteration 13/25 | Loss: 0.00082268
Iteration 14/25 | Loss: 0.00082268
Iteration 15/25 | Loss: 0.00082268
Iteration 16/25 | Loss: 0.00082268
Iteration 17/25 | Loss: 0.00082268
Iteration 18/25 | Loss: 0.00082268
Iteration 19/25 | Loss: 0.00082268
Iteration 20/25 | Loss: 0.00082268
Iteration 21/25 | Loss: 0.00082268
Iteration 22/25 | Loss: 0.00082268
Iteration 23/25 | Loss: 0.00082268
Iteration 24/25 | Loss: 0.00082268
Iteration 25/25 | Loss: 0.00082268

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00082268
Iteration 2/1000 | Loss: 0.00001888
Iteration 3/1000 | Loss: 0.00001269
Iteration 4/1000 | Loss: 0.00001154
Iteration 5/1000 | Loss: 0.00001102
Iteration 6/1000 | Loss: 0.00001079
Iteration 7/1000 | Loss: 0.00001052
Iteration 8/1000 | Loss: 0.00001050
Iteration 9/1000 | Loss: 0.00001044
Iteration 10/1000 | Loss: 0.00001030
Iteration 11/1000 | Loss: 0.00001019
Iteration 12/1000 | Loss: 0.00001019
Iteration 13/1000 | Loss: 0.00001016
Iteration 14/1000 | Loss: 0.00001009
Iteration 15/1000 | Loss: 0.00001008
Iteration 16/1000 | Loss: 0.00001008
Iteration 17/1000 | Loss: 0.00001007
Iteration 18/1000 | Loss: 0.00001000
Iteration 19/1000 | Loss: 0.00000999
Iteration 20/1000 | Loss: 0.00000991
Iteration 21/1000 | Loss: 0.00000991
Iteration 22/1000 | Loss: 0.00000991
Iteration 23/1000 | Loss: 0.00000991
Iteration 24/1000 | Loss: 0.00000991
Iteration 25/1000 | Loss: 0.00000991
Iteration 26/1000 | Loss: 0.00000990
Iteration 27/1000 | Loss: 0.00000990
Iteration 28/1000 | Loss: 0.00000990
Iteration 29/1000 | Loss: 0.00000990
Iteration 30/1000 | Loss: 0.00000990
Iteration 31/1000 | Loss: 0.00000990
Iteration 32/1000 | Loss: 0.00000990
Iteration 33/1000 | Loss: 0.00000990
Iteration 34/1000 | Loss: 0.00000990
Iteration 35/1000 | Loss: 0.00000989
Iteration 36/1000 | Loss: 0.00000989
Iteration 37/1000 | Loss: 0.00000988
Iteration 38/1000 | Loss: 0.00000988
Iteration 39/1000 | Loss: 0.00000987
Iteration 40/1000 | Loss: 0.00000987
Iteration 41/1000 | Loss: 0.00000986
Iteration 42/1000 | Loss: 0.00000985
Iteration 43/1000 | Loss: 0.00000985
Iteration 44/1000 | Loss: 0.00000985
Iteration 45/1000 | Loss: 0.00000982
Iteration 46/1000 | Loss: 0.00000982
Iteration 47/1000 | Loss: 0.00000982
Iteration 48/1000 | Loss: 0.00000981
Iteration 49/1000 | Loss: 0.00000980
Iteration 50/1000 | Loss: 0.00000980
Iteration 51/1000 | Loss: 0.00000979
Iteration 52/1000 | Loss: 0.00000979
Iteration 53/1000 | Loss: 0.00000979
Iteration 54/1000 | Loss: 0.00000979
Iteration 55/1000 | Loss: 0.00000978
Iteration 56/1000 | Loss: 0.00000978
Iteration 57/1000 | Loss: 0.00000977
Iteration 58/1000 | Loss: 0.00000977
Iteration 59/1000 | Loss: 0.00000976
Iteration 60/1000 | Loss: 0.00000976
Iteration 61/1000 | Loss: 0.00000976
Iteration 62/1000 | Loss: 0.00000976
Iteration 63/1000 | Loss: 0.00000975
Iteration 64/1000 | Loss: 0.00000975
Iteration 65/1000 | Loss: 0.00000975
Iteration 66/1000 | Loss: 0.00000975
Iteration 67/1000 | Loss: 0.00000974
Iteration 68/1000 | Loss: 0.00000973
Iteration 69/1000 | Loss: 0.00000973
Iteration 70/1000 | Loss: 0.00000972
Iteration 71/1000 | Loss: 0.00000972
Iteration 72/1000 | Loss: 0.00000972
Iteration 73/1000 | Loss: 0.00000971
Iteration 74/1000 | Loss: 0.00000971
Iteration 75/1000 | Loss: 0.00000971
Iteration 76/1000 | Loss: 0.00000970
Iteration 77/1000 | Loss: 0.00000970
Iteration 78/1000 | Loss: 0.00000970
Iteration 79/1000 | Loss: 0.00000970
Iteration 80/1000 | Loss: 0.00000970
Iteration 81/1000 | Loss: 0.00000970
Iteration 82/1000 | Loss: 0.00000970
Iteration 83/1000 | Loss: 0.00000970
Iteration 84/1000 | Loss: 0.00000970
Iteration 85/1000 | Loss: 0.00000969
Iteration 86/1000 | Loss: 0.00000969
Iteration 87/1000 | Loss: 0.00000969
Iteration 88/1000 | Loss: 0.00000969
Iteration 89/1000 | Loss: 0.00000969
Iteration 90/1000 | Loss: 0.00000969
Iteration 91/1000 | Loss: 0.00000969
Iteration 92/1000 | Loss: 0.00000968
Iteration 93/1000 | Loss: 0.00000968
Iteration 94/1000 | Loss: 0.00000968
Iteration 95/1000 | Loss: 0.00000968
Iteration 96/1000 | Loss: 0.00000968
Iteration 97/1000 | Loss: 0.00000968
Iteration 98/1000 | Loss: 0.00000968
Iteration 99/1000 | Loss: 0.00000968
Iteration 100/1000 | Loss: 0.00000967
Iteration 101/1000 | Loss: 0.00000967
Iteration 102/1000 | Loss: 0.00000967
Iteration 103/1000 | Loss: 0.00000967
Iteration 104/1000 | Loss: 0.00000967
Iteration 105/1000 | Loss: 0.00000967
Iteration 106/1000 | Loss: 0.00000967
Iteration 107/1000 | Loss: 0.00000967
Iteration 108/1000 | Loss: 0.00000967
Iteration 109/1000 | Loss: 0.00000967
Iteration 110/1000 | Loss: 0.00000967
Iteration 111/1000 | Loss: 0.00000967
Iteration 112/1000 | Loss: 0.00000967
Iteration 113/1000 | Loss: 0.00000967
Iteration 114/1000 | Loss: 0.00000966
Iteration 115/1000 | Loss: 0.00000966
Iteration 116/1000 | Loss: 0.00000966
Iteration 117/1000 | Loss: 0.00000965
Iteration 118/1000 | Loss: 0.00000965
Iteration 119/1000 | Loss: 0.00000965
Iteration 120/1000 | Loss: 0.00000964
Iteration 121/1000 | Loss: 0.00000964
Iteration 122/1000 | Loss: 0.00000964
Iteration 123/1000 | Loss: 0.00000964
Iteration 124/1000 | Loss: 0.00000964
Iteration 125/1000 | Loss: 0.00000964
Iteration 126/1000 | Loss: 0.00000964
Iteration 127/1000 | Loss: 0.00000963
Iteration 128/1000 | Loss: 0.00000963
Iteration 129/1000 | Loss: 0.00000963
Iteration 130/1000 | Loss: 0.00000963
Iteration 131/1000 | Loss: 0.00000962
Iteration 132/1000 | Loss: 0.00000962
Iteration 133/1000 | Loss: 0.00000962
Iteration 134/1000 | Loss: 0.00000962
Iteration 135/1000 | Loss: 0.00000962
Iteration 136/1000 | Loss: 0.00000961
Iteration 137/1000 | Loss: 0.00000961
Iteration 138/1000 | Loss: 0.00000961
Iteration 139/1000 | Loss: 0.00000960
Iteration 140/1000 | Loss: 0.00000960
Iteration 141/1000 | Loss: 0.00000960
Iteration 142/1000 | Loss: 0.00000959
Iteration 143/1000 | Loss: 0.00000959
Iteration 144/1000 | Loss: 0.00000959
Iteration 145/1000 | Loss: 0.00000959
Iteration 146/1000 | Loss: 0.00000959
Iteration 147/1000 | Loss: 0.00000959
Iteration 148/1000 | Loss: 0.00000959
Iteration 149/1000 | Loss: 0.00000959
Iteration 150/1000 | Loss: 0.00000959
Iteration 151/1000 | Loss: 0.00000959
Iteration 152/1000 | Loss: 0.00000959
Iteration 153/1000 | Loss: 0.00000958
Iteration 154/1000 | Loss: 0.00000958
Iteration 155/1000 | Loss: 0.00000958
Iteration 156/1000 | Loss: 0.00000958
Iteration 157/1000 | Loss: 0.00000958
Iteration 158/1000 | Loss: 0.00000958
Iteration 159/1000 | Loss: 0.00000958
Iteration 160/1000 | Loss: 0.00000958
Iteration 161/1000 | Loss: 0.00000957
Iteration 162/1000 | Loss: 0.00000957
Iteration 163/1000 | Loss: 0.00000957
Iteration 164/1000 | Loss: 0.00000957
Iteration 165/1000 | Loss: 0.00000957
Iteration 166/1000 | Loss: 0.00000957
Iteration 167/1000 | Loss: 0.00000957
Iteration 168/1000 | Loss: 0.00000956
Iteration 169/1000 | Loss: 0.00000956
Iteration 170/1000 | Loss: 0.00000956
Iteration 171/1000 | Loss: 0.00000956
Iteration 172/1000 | Loss: 0.00000956
Iteration 173/1000 | Loss: 0.00000956
Iteration 174/1000 | Loss: 0.00000956
Iteration 175/1000 | Loss: 0.00000956
Iteration 176/1000 | Loss: 0.00000956
Iteration 177/1000 | Loss: 0.00000956
Iteration 178/1000 | Loss: 0.00000956
Iteration 179/1000 | Loss: 0.00000956
Iteration 180/1000 | Loss: 0.00000956
Iteration 181/1000 | Loss: 0.00000956
Iteration 182/1000 | Loss: 0.00000956
Iteration 183/1000 | Loss: 0.00000956
Iteration 184/1000 | Loss: 0.00000956
Iteration 185/1000 | Loss: 0.00000955
Iteration 186/1000 | Loss: 0.00000955
Iteration 187/1000 | Loss: 0.00000955
Iteration 188/1000 | Loss: 0.00000955
Iteration 189/1000 | Loss: 0.00000955
Iteration 190/1000 | Loss: 0.00000955
Iteration 191/1000 | Loss: 0.00000955
Iteration 192/1000 | Loss: 0.00000955
Iteration 193/1000 | Loss: 0.00000955
Iteration 194/1000 | Loss: 0.00000954
Iteration 195/1000 | Loss: 0.00000954
Iteration 196/1000 | Loss: 0.00000954
Iteration 197/1000 | Loss: 0.00000954
Iteration 198/1000 | Loss: 0.00000953
Iteration 199/1000 | Loss: 0.00000953
Iteration 200/1000 | Loss: 0.00000953
Iteration 201/1000 | Loss: 0.00000953
Iteration 202/1000 | Loss: 0.00000953
Iteration 203/1000 | Loss: 0.00000953
Iteration 204/1000 | Loss: 0.00000952
Iteration 205/1000 | Loss: 0.00000952
Iteration 206/1000 | Loss: 0.00000952
Iteration 207/1000 | Loss: 0.00000952
Iteration 208/1000 | Loss: 0.00000952
Iteration 209/1000 | Loss: 0.00000952
Iteration 210/1000 | Loss: 0.00000952
Iteration 211/1000 | Loss: 0.00000952
Iteration 212/1000 | Loss: 0.00000952
Iteration 213/1000 | Loss: 0.00000952
Iteration 214/1000 | Loss: 0.00000952
Iteration 215/1000 | Loss: 0.00000952
Iteration 216/1000 | Loss: 0.00000952
Iteration 217/1000 | Loss: 0.00000952
Iteration 218/1000 | Loss: 0.00000952
Iteration 219/1000 | Loss: 0.00000952
Iteration 220/1000 | Loss: 0.00000952
Iteration 221/1000 | Loss: 0.00000951
Iteration 222/1000 | Loss: 0.00000951
Iteration 223/1000 | Loss: 0.00000951
Iteration 224/1000 | Loss: 0.00000951
Iteration 225/1000 | Loss: 0.00000951
Iteration 226/1000 | Loss: 0.00000951
Iteration 227/1000 | Loss: 0.00000951
Iteration 228/1000 | Loss: 0.00000951
Iteration 229/1000 | Loss: 0.00000951
Iteration 230/1000 | Loss: 0.00000951
Iteration 231/1000 | Loss: 0.00000951
Iteration 232/1000 | Loss: 0.00000951
Iteration 233/1000 | Loss: 0.00000951
Iteration 234/1000 | Loss: 0.00000951
Iteration 235/1000 | Loss: 0.00000951
Iteration 236/1000 | Loss: 0.00000951
Iteration 237/1000 | Loss: 0.00000951
Iteration 238/1000 | Loss: 0.00000951
Iteration 239/1000 | Loss: 0.00000951
Iteration 240/1000 | Loss: 0.00000951
Iteration 241/1000 | Loss: 0.00000951
Iteration 242/1000 | Loss: 0.00000951
Iteration 243/1000 | Loss: 0.00000951
Iteration 244/1000 | Loss: 0.00000951
Iteration 245/1000 | Loss: 0.00000951
Iteration 246/1000 | Loss: 0.00000951
Iteration 247/1000 | Loss: 0.00000951
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 247. Stopping optimization.
Last 5 losses: [9.514472367300186e-06, 9.514472367300186e-06, 9.514472367300186e-06, 9.514472367300186e-06, 9.514472367300186e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.514472367300186e-06

Optimization complete. Final v2v error: 2.6458284854888916 mm

Highest mean error: 2.8558781147003174 mm for frame 105

Lowest mean error: 2.5296928882598877 mm for frame 89

Saving results

Total time: 37.12619686126709
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_015/1030/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_015/1030.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_015/1030
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00921717
Iteration 2/25 | Loss: 0.00384445
Iteration 3/25 | Loss: 0.00268070
Iteration 4/25 | Loss: 0.00261170
Iteration 5/25 | Loss: 0.00296279
Iteration 6/25 | Loss: 0.00237790
Iteration 7/25 | Loss: 0.00213514
Iteration 8/25 | Loss: 0.00217125
Iteration 9/25 | Loss: 0.00199419
Iteration 10/25 | Loss: 0.00188867
Iteration 11/25 | Loss: 0.00185411
Iteration 12/25 | Loss: 0.00173038
Iteration 13/25 | Loss: 0.00168446
Iteration 14/25 | Loss: 0.00163550
Iteration 15/25 | Loss: 0.00164315
Iteration 16/25 | Loss: 0.00160987
Iteration 17/25 | Loss: 0.00159321
Iteration 18/25 | Loss: 0.00158865
Iteration 19/25 | Loss: 0.00157099
Iteration 20/25 | Loss: 0.00156594
Iteration 21/25 | Loss: 0.00156479
Iteration 22/25 | Loss: 0.00156397
Iteration 23/25 | Loss: 0.00156372
Iteration 24/25 | Loss: 0.00156342
Iteration 25/25 | Loss: 0.00156318

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.88399243
Iteration 2/25 | Loss: 0.00526537
Iteration 3/25 | Loss: 0.00467939
Iteration 4/25 | Loss: 0.00467939
Iteration 5/25 | Loss: 0.00467939
Iteration 6/25 | Loss: 0.00467939
Iteration 7/25 | Loss: 0.00467939
Iteration 8/25 | Loss: 0.00467939
Iteration 9/25 | Loss: 0.00467939
Iteration 10/25 | Loss: 0.00467939
Iteration 11/25 | Loss: 0.00467939
Iteration 12/25 | Loss: 0.00467939
Iteration 13/25 | Loss: 0.00467939
Iteration 14/25 | Loss: 0.00467939
Iteration 15/25 | Loss: 0.00467939
Iteration 16/25 | Loss: 0.00467939
Iteration 17/25 | Loss: 0.00467939
Iteration 18/25 | Loss: 0.00467939
Iteration 19/25 | Loss: 0.00467939
Iteration 20/25 | Loss: 0.00467939
Iteration 21/25 | Loss: 0.00467939
Iteration 22/25 | Loss: 0.00467939
Iteration 23/25 | Loss: 0.00467939
Iteration 24/25 | Loss: 0.00467939
Iteration 25/25 | Loss: 0.00467939

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00467939
Iteration 2/1000 | Loss: 0.00092482
Iteration 3/1000 | Loss: 0.00209800
Iteration 4/1000 | Loss: 0.00224840
Iteration 5/1000 | Loss: 0.00194979
Iteration 6/1000 | Loss: 0.00419884
Iteration 7/1000 | Loss: 0.00183911
Iteration 8/1000 | Loss: 0.00137147
Iteration 9/1000 | Loss: 0.00635848
Iteration 10/1000 | Loss: 0.00043696
Iteration 11/1000 | Loss: 0.00233189
Iteration 12/1000 | Loss: 0.00214563
Iteration 13/1000 | Loss: 0.00087750
Iteration 14/1000 | Loss: 0.00071032
Iteration 15/1000 | Loss: 0.00098738
Iteration 16/1000 | Loss: 0.00030495
Iteration 17/1000 | Loss: 0.00117280
Iteration 18/1000 | Loss: 0.00044366
Iteration 19/1000 | Loss: 0.00035423
Iteration 20/1000 | Loss: 0.00118852
Iteration 21/1000 | Loss: 0.00120675
Iteration 22/1000 | Loss: 0.00275412
Iteration 23/1000 | Loss: 0.00267626
Iteration 24/1000 | Loss: 0.00099400
Iteration 25/1000 | Loss: 0.00097081
Iteration 26/1000 | Loss: 0.00527101
Iteration 27/1000 | Loss: 0.00180207
Iteration 28/1000 | Loss: 0.00123710
Iteration 29/1000 | Loss: 0.00367951
Iteration 30/1000 | Loss: 0.00470098
Iteration 31/1000 | Loss: 0.00204587
Iteration 32/1000 | Loss: 0.00343693
Iteration 33/1000 | Loss: 0.00228952
Iteration 34/1000 | Loss: 0.00185426
Iteration 35/1000 | Loss: 0.00082015
Iteration 36/1000 | Loss: 0.00308817
Iteration 37/1000 | Loss: 0.00094208
Iteration 38/1000 | Loss: 0.00197791
Iteration 39/1000 | Loss: 0.00104534
Iteration 40/1000 | Loss: 0.00142011
Iteration 41/1000 | Loss: 0.00352189
Iteration 42/1000 | Loss: 0.00094286
Iteration 43/1000 | Loss: 0.00293885
Iteration 44/1000 | Loss: 0.00081024
Iteration 45/1000 | Loss: 0.00059676
Iteration 46/1000 | Loss: 0.00071934
Iteration 47/1000 | Loss: 0.00460749
Iteration 48/1000 | Loss: 0.00192538
Iteration 49/1000 | Loss: 0.00155179
Iteration 50/1000 | Loss: 0.00203949
Iteration 51/1000 | Loss: 0.00294822
Iteration 52/1000 | Loss: 0.00065172
Iteration 53/1000 | Loss: 0.00040724
Iteration 54/1000 | Loss: 0.00066990
Iteration 55/1000 | Loss: 0.00041602
Iteration 56/1000 | Loss: 0.00048030
Iteration 57/1000 | Loss: 0.00024420
Iteration 58/1000 | Loss: 0.00069656
Iteration 59/1000 | Loss: 0.00406696
Iteration 60/1000 | Loss: 0.00046262
Iteration 61/1000 | Loss: 0.00078820
Iteration 62/1000 | Loss: 0.00038794
Iteration 63/1000 | Loss: 0.00042302
Iteration 64/1000 | Loss: 0.00011726
Iteration 65/1000 | Loss: 0.00034150
Iteration 66/1000 | Loss: 0.00025235
Iteration 67/1000 | Loss: 0.00031141
Iteration 68/1000 | Loss: 0.00012621
Iteration 69/1000 | Loss: 0.00011160
Iteration 70/1000 | Loss: 0.00068423
Iteration 71/1000 | Loss: 0.00045989
Iteration 72/1000 | Loss: 0.00151619
Iteration 73/1000 | Loss: 0.00061388
Iteration 74/1000 | Loss: 0.00010705
Iteration 75/1000 | Loss: 0.00012697
Iteration 76/1000 | Loss: 0.00024142
Iteration 77/1000 | Loss: 0.00016019
Iteration 78/1000 | Loss: 0.00014891
Iteration 79/1000 | Loss: 0.00014617
Iteration 80/1000 | Loss: 0.00014251
Iteration 81/1000 | Loss: 0.00016853
Iteration 82/1000 | Loss: 0.00051822
Iteration 83/1000 | Loss: 0.00237138
Iteration 84/1000 | Loss: 0.00158930
Iteration 85/1000 | Loss: 0.00165232
Iteration 86/1000 | Loss: 0.00236586
Iteration 87/1000 | Loss: 0.00323643
Iteration 88/1000 | Loss: 0.00102611
Iteration 89/1000 | Loss: 0.00057154
Iteration 90/1000 | Loss: 0.00042288
Iteration 91/1000 | Loss: 0.00085222
Iteration 92/1000 | Loss: 0.00131182
Iteration 93/1000 | Loss: 0.00102898
Iteration 94/1000 | Loss: 0.00101877
Iteration 95/1000 | Loss: 0.00199725
Iteration 96/1000 | Loss: 0.00103025
Iteration 97/1000 | Loss: 0.00061572
Iteration 98/1000 | Loss: 0.00020525
Iteration 99/1000 | Loss: 0.00017712
Iteration 100/1000 | Loss: 0.00175584
Iteration 101/1000 | Loss: 0.00037581
Iteration 102/1000 | Loss: 0.00012018
Iteration 103/1000 | Loss: 0.00034727
Iteration 104/1000 | Loss: 0.00040654
Iteration 105/1000 | Loss: 0.00013117
Iteration 106/1000 | Loss: 0.00007893
Iteration 107/1000 | Loss: 0.00051605
Iteration 108/1000 | Loss: 0.00024614
Iteration 109/1000 | Loss: 0.00016773
Iteration 110/1000 | Loss: 0.00025023
Iteration 111/1000 | Loss: 0.00027137
Iteration 112/1000 | Loss: 0.00008460
Iteration 113/1000 | Loss: 0.00008379
Iteration 114/1000 | Loss: 0.00007455
Iteration 115/1000 | Loss: 0.00028282
Iteration 116/1000 | Loss: 0.00022668
Iteration 117/1000 | Loss: 0.00020463
Iteration 118/1000 | Loss: 0.00019311
Iteration 119/1000 | Loss: 0.00017236
Iteration 120/1000 | Loss: 0.00010069
Iteration 121/1000 | Loss: 0.00024094
Iteration 122/1000 | Loss: 0.00032821
Iteration 123/1000 | Loss: 0.00037777
Iteration 124/1000 | Loss: 0.00031593
Iteration 125/1000 | Loss: 0.00031392
Iteration 126/1000 | Loss: 0.00176014
Iteration 127/1000 | Loss: 0.00072540
Iteration 128/1000 | Loss: 0.00103621
Iteration 129/1000 | Loss: 0.00078077
Iteration 130/1000 | Loss: 0.00085133
Iteration 131/1000 | Loss: 0.00063845
Iteration 132/1000 | Loss: 0.00035403
Iteration 133/1000 | Loss: 0.00036207
Iteration 134/1000 | Loss: 0.00008108
Iteration 135/1000 | Loss: 0.00008152
Iteration 136/1000 | Loss: 0.00029697
Iteration 137/1000 | Loss: 0.00021422
Iteration 138/1000 | Loss: 0.00009941
Iteration 139/1000 | Loss: 0.00037425
Iteration 140/1000 | Loss: 0.00039537
Iteration 141/1000 | Loss: 0.00017106
Iteration 142/1000 | Loss: 0.00053360
Iteration 143/1000 | Loss: 0.00032528
Iteration 144/1000 | Loss: 0.00039250
Iteration 145/1000 | Loss: 0.00030731
Iteration 146/1000 | Loss: 0.00129624
Iteration 147/1000 | Loss: 0.00162042
Iteration 148/1000 | Loss: 0.00220266
Iteration 149/1000 | Loss: 0.00175845
Iteration 150/1000 | Loss: 0.00166531
Iteration 151/1000 | Loss: 0.00057524
Iteration 152/1000 | Loss: 0.00041756
Iteration 153/1000 | Loss: 0.00046780
Iteration 154/1000 | Loss: 0.00014722
Iteration 155/1000 | Loss: 0.00037109
Iteration 156/1000 | Loss: 0.00025407
Iteration 157/1000 | Loss: 0.00034225
Iteration 158/1000 | Loss: 0.00025171
Iteration 159/1000 | Loss: 0.00040972
Iteration 160/1000 | Loss: 0.00017202
Iteration 161/1000 | Loss: 0.00011585
Iteration 162/1000 | Loss: 0.00007267
Iteration 163/1000 | Loss: 0.00026982
Iteration 164/1000 | Loss: 0.00020344
Iteration 165/1000 | Loss: 0.00016217
Iteration 166/1000 | Loss: 0.00018525
Iteration 167/1000 | Loss: 0.00022089
Iteration 168/1000 | Loss: 0.00037253
Iteration 169/1000 | Loss: 0.00037232
Iteration 170/1000 | Loss: 0.00036570
Iteration 171/1000 | Loss: 0.00039539
Iteration 172/1000 | Loss: 0.00035238
Iteration 173/1000 | Loss: 0.00042446
Iteration 174/1000 | Loss: 0.00036031
Iteration 175/1000 | Loss: 0.00011761
Iteration 176/1000 | Loss: 0.00013535
Iteration 177/1000 | Loss: 0.00007231
Iteration 178/1000 | Loss: 0.00013094
Iteration 179/1000 | Loss: 0.00044968
Iteration 180/1000 | Loss: 0.00012816
Iteration 181/1000 | Loss: 0.00006890
Iteration 182/1000 | Loss: 0.00024032
Iteration 183/1000 | Loss: 0.00025344
Iteration 184/1000 | Loss: 0.00046362
Iteration 185/1000 | Loss: 0.00030853
Iteration 186/1000 | Loss: 0.00007833
Iteration 187/1000 | Loss: 0.00008981
Iteration 188/1000 | Loss: 0.00006875
Iteration 189/1000 | Loss: 0.00016873
Iteration 190/1000 | Loss: 0.00018532
Iteration 191/1000 | Loss: 0.00090738
Iteration 192/1000 | Loss: 0.00098071
Iteration 193/1000 | Loss: 0.00012841
Iteration 194/1000 | Loss: 0.00018683
Iteration 195/1000 | Loss: 0.00032156
Iteration 196/1000 | Loss: 0.00016434
Iteration 197/1000 | Loss: 0.00023840
Iteration 198/1000 | Loss: 0.00042634
Iteration 199/1000 | Loss: 0.00022225
Iteration 200/1000 | Loss: 0.00020214
Iteration 201/1000 | Loss: 0.00038071
Iteration 202/1000 | Loss: 0.00022892
Iteration 203/1000 | Loss: 0.00018240
Iteration 204/1000 | Loss: 0.00008061
Iteration 205/1000 | Loss: 0.00007080
Iteration 206/1000 | Loss: 0.00006574
Iteration 207/1000 | Loss: 0.00006494
Iteration 208/1000 | Loss: 0.00006368
Iteration 209/1000 | Loss: 0.00006291
Iteration 210/1000 | Loss: 0.00026062
Iteration 211/1000 | Loss: 0.00018887
Iteration 212/1000 | Loss: 0.00022069
Iteration 213/1000 | Loss: 0.00018153
Iteration 214/1000 | Loss: 0.00013382
Iteration 215/1000 | Loss: 0.00006538
Iteration 216/1000 | Loss: 0.00006248
Iteration 217/1000 | Loss: 0.00006154
Iteration 218/1000 | Loss: 0.00006091
Iteration 219/1000 | Loss: 0.00019073
Iteration 220/1000 | Loss: 0.00021795
Iteration 221/1000 | Loss: 0.00018306
Iteration 222/1000 | Loss: 0.00023011
Iteration 223/1000 | Loss: 0.00018741
Iteration 224/1000 | Loss: 0.00018770
Iteration 225/1000 | Loss: 0.00016073
Iteration 226/1000 | Loss: 0.00020587
Iteration 227/1000 | Loss: 0.00015827
Iteration 228/1000 | Loss: 0.00015467
Iteration 229/1000 | Loss: 0.00015605
Iteration 230/1000 | Loss: 0.00006464
Iteration 231/1000 | Loss: 0.00008321
Iteration 232/1000 | Loss: 0.00006534
Iteration 233/1000 | Loss: 0.00006056
Iteration 234/1000 | Loss: 0.00005979
Iteration 235/1000 | Loss: 0.00005932
Iteration 236/1000 | Loss: 0.00005887
Iteration 237/1000 | Loss: 0.00005852
Iteration 238/1000 | Loss: 0.00005829
Iteration 239/1000 | Loss: 0.00007889
Iteration 240/1000 | Loss: 0.00005804
Iteration 241/1000 | Loss: 0.00005752
Iteration 242/1000 | Loss: 0.00005712
Iteration 243/1000 | Loss: 0.00015827
Iteration 244/1000 | Loss: 0.00010466
Iteration 245/1000 | Loss: 0.00005878
Iteration 246/1000 | Loss: 0.00015146
Iteration 247/1000 | Loss: 0.00024386
Iteration 248/1000 | Loss: 0.00068230
Iteration 249/1000 | Loss: 0.00094751
Iteration 250/1000 | Loss: 0.00019510
Iteration 251/1000 | Loss: 0.00023680
Iteration 252/1000 | Loss: 0.00037721
Iteration 253/1000 | Loss: 0.00017796
Iteration 254/1000 | Loss: 0.00009953
Iteration 255/1000 | Loss: 0.00007179
Iteration 256/1000 | Loss: 0.00009164
Iteration 257/1000 | Loss: 0.00010103
Iteration 258/1000 | Loss: 0.00007207
Iteration 259/1000 | Loss: 0.00007944
Iteration 260/1000 | Loss: 0.00008399
Iteration 261/1000 | Loss: 0.00009871
Iteration 262/1000 | Loss: 0.00009229
Iteration 263/1000 | Loss: 0.00005639
Iteration 264/1000 | Loss: 0.00005523
Iteration 265/1000 | Loss: 0.00005444
Iteration 266/1000 | Loss: 0.00005364
Iteration 267/1000 | Loss: 0.00005321
Iteration 268/1000 | Loss: 0.00025366
Iteration 269/1000 | Loss: 0.00020433
Iteration 270/1000 | Loss: 0.00005320
Iteration 271/1000 | Loss: 0.00012066
Iteration 272/1000 | Loss: 0.00008596
Iteration 273/1000 | Loss: 0.00011806
Iteration 274/1000 | Loss: 0.00005289
Iteration 275/1000 | Loss: 0.00005203
Iteration 276/1000 | Loss: 0.00005194
Iteration 277/1000 | Loss: 0.00005194
Iteration 278/1000 | Loss: 0.00005193
Iteration 279/1000 | Loss: 0.00005170
Iteration 280/1000 | Loss: 0.00027617
Iteration 281/1000 | Loss: 0.00005212
Iteration 282/1000 | Loss: 0.00005140
Iteration 283/1000 | Loss: 0.00005117
Iteration 284/1000 | Loss: 0.00005096
Iteration 285/1000 | Loss: 0.00005080
Iteration 286/1000 | Loss: 0.00005078
Iteration 287/1000 | Loss: 0.00005074
Iteration 288/1000 | Loss: 0.00005073
Iteration 289/1000 | Loss: 0.00005072
Iteration 290/1000 | Loss: 0.00005061
Iteration 291/1000 | Loss: 0.00005060
Iteration 292/1000 | Loss: 0.00005052
Iteration 293/1000 | Loss: 0.00005052
Iteration 294/1000 | Loss: 0.00005050
Iteration 295/1000 | Loss: 0.00005049
Iteration 296/1000 | Loss: 0.00005049
Iteration 297/1000 | Loss: 0.00005049
Iteration 298/1000 | Loss: 0.00005048
Iteration 299/1000 | Loss: 0.00005047
Iteration 300/1000 | Loss: 0.00005042
Iteration 301/1000 | Loss: 0.00005040
Iteration 302/1000 | Loss: 0.00005039
Iteration 303/1000 | Loss: 0.00005039
Iteration 304/1000 | Loss: 0.00005038
Iteration 305/1000 | Loss: 0.00005038
Iteration 306/1000 | Loss: 0.00005037
Iteration 307/1000 | Loss: 0.00005034
Iteration 308/1000 | Loss: 0.00005032
Iteration 309/1000 | Loss: 0.00005032
Iteration 310/1000 | Loss: 0.00005032
Iteration 311/1000 | Loss: 0.00005032
Iteration 312/1000 | Loss: 0.00005032
Iteration 313/1000 | Loss: 0.00005032
Iteration 314/1000 | Loss: 0.00005031
Iteration 315/1000 | Loss: 0.00005031
Iteration 316/1000 | Loss: 0.00005031
Iteration 317/1000 | Loss: 0.00005031
Iteration 318/1000 | Loss: 0.00005031
Iteration 319/1000 | Loss: 0.00005031
Iteration 320/1000 | Loss: 0.00005031
Iteration 321/1000 | Loss: 0.00005031
Iteration 322/1000 | Loss: 0.00005031
Iteration 323/1000 | Loss: 0.00005031
Iteration 324/1000 | Loss: 0.00005031
Iteration 325/1000 | Loss: 0.00005030
Iteration 326/1000 | Loss: 0.00005030
Iteration 327/1000 | Loss: 0.00005030
Iteration 328/1000 | Loss: 0.00005030
Iteration 329/1000 | Loss: 0.00005028
Iteration 330/1000 | Loss: 0.00005028
Iteration 331/1000 | Loss: 0.00005028
Iteration 332/1000 | Loss: 0.00005028
Iteration 333/1000 | Loss: 0.00005028
Iteration 334/1000 | Loss: 0.00005028
Iteration 335/1000 | Loss: 0.00005028
Iteration 336/1000 | Loss: 0.00005028
Iteration 337/1000 | Loss: 0.00005027
Iteration 338/1000 | Loss: 0.00005026
Iteration 339/1000 | Loss: 0.00005026
Iteration 340/1000 | Loss: 0.00005026
Iteration 341/1000 | Loss: 0.00005026
Iteration 342/1000 | Loss: 0.00005025
Iteration 343/1000 | Loss: 0.00005025
Iteration 344/1000 | Loss: 0.00005025
Iteration 345/1000 | Loss: 0.00005024
Iteration 346/1000 | Loss: 0.00005024
Iteration 347/1000 | Loss: 0.00005024
Iteration 348/1000 | Loss: 0.00005023
Iteration 349/1000 | Loss: 0.00005023
Iteration 350/1000 | Loss: 0.00005023
Iteration 351/1000 | Loss: 0.00005023
Iteration 352/1000 | Loss: 0.00005023
Iteration 353/1000 | Loss: 0.00005023
Iteration 354/1000 | Loss: 0.00005023
Iteration 355/1000 | Loss: 0.00005023
Iteration 356/1000 | Loss: 0.00005023
Iteration 357/1000 | Loss: 0.00005023
Iteration 358/1000 | Loss: 0.00005023
Iteration 359/1000 | Loss: 0.00005023
Iteration 360/1000 | Loss: 0.00005022
Iteration 361/1000 | Loss: 0.00005022
Iteration 362/1000 | Loss: 0.00005022
Iteration 363/1000 | Loss: 0.00005022
Iteration 364/1000 | Loss: 0.00005021
Iteration 365/1000 | Loss: 0.00005021
Iteration 366/1000 | Loss: 0.00005021
Iteration 367/1000 | Loss: 0.00005021
Iteration 368/1000 | Loss: 0.00005020
Iteration 369/1000 | Loss: 0.00005020
Iteration 370/1000 | Loss: 0.00005020
Iteration 371/1000 | Loss: 0.00005019
Iteration 372/1000 | Loss: 0.00005018
Iteration 373/1000 | Loss: 0.00005018
Iteration 374/1000 | Loss: 0.00005018
Iteration 375/1000 | Loss: 0.00005017
Iteration 376/1000 | Loss: 0.00005017
Iteration 377/1000 | Loss: 0.00005017
Iteration 378/1000 | Loss: 0.00005016
Iteration 379/1000 | Loss: 0.00005016
Iteration 380/1000 | Loss: 0.00005016
Iteration 381/1000 | Loss: 0.00005015
Iteration 382/1000 | Loss: 0.00005015
Iteration 383/1000 | Loss: 0.00005014
Iteration 384/1000 | Loss: 0.00005014
Iteration 385/1000 | Loss: 0.00005013
Iteration 386/1000 | Loss: 0.00005012
Iteration 387/1000 | Loss: 0.00005012
Iteration 388/1000 | Loss: 0.00005011
Iteration 389/1000 | Loss: 0.00005010
Iteration 390/1000 | Loss: 0.00005009
Iteration 391/1000 | Loss: 0.00005009
Iteration 392/1000 | Loss: 0.00005009
Iteration 393/1000 | Loss: 0.00005009
Iteration 394/1000 | Loss: 0.00005009
Iteration 395/1000 | Loss: 0.00005009
Iteration 396/1000 | Loss: 0.00005009
Iteration 397/1000 | Loss: 0.00005009
Iteration 398/1000 | Loss: 0.00005009
Iteration 399/1000 | Loss: 0.00005009
Iteration 400/1000 | Loss: 0.00005009
Iteration 401/1000 | Loss: 0.00005008
Iteration 402/1000 | Loss: 0.00005008
Iteration 403/1000 | Loss: 0.00005008
Iteration 404/1000 | Loss: 0.00005007
Iteration 405/1000 | Loss: 0.00005007
Iteration 406/1000 | Loss: 0.00005007
Iteration 407/1000 | Loss: 0.00005007
Iteration 408/1000 | Loss: 0.00005007
Iteration 409/1000 | Loss: 0.00005007
Iteration 410/1000 | Loss: 0.00005007
Iteration 411/1000 | Loss: 0.00005007
Iteration 412/1000 | Loss: 0.00005007
Iteration 413/1000 | Loss: 0.00005007
Iteration 414/1000 | Loss: 0.00005006
Iteration 415/1000 | Loss: 0.00005006
Iteration 416/1000 | Loss: 0.00005006
Iteration 417/1000 | Loss: 0.00005006
Iteration 418/1000 | Loss: 0.00005006
Iteration 419/1000 | Loss: 0.00005006
Iteration 420/1000 | Loss: 0.00005006
Iteration 421/1000 | Loss: 0.00005005
Iteration 422/1000 | Loss: 0.00005005
Iteration 423/1000 | Loss: 0.00005005
Iteration 424/1000 | Loss: 0.00005005
Iteration 425/1000 | Loss: 0.00005005
Iteration 426/1000 | Loss: 0.00005005
Iteration 427/1000 | Loss: 0.00005005
Iteration 428/1000 | Loss: 0.00005005
Iteration 429/1000 | Loss: 0.00005005
Iteration 430/1000 | Loss: 0.00005004
Iteration 431/1000 | Loss: 0.00005004
Iteration 432/1000 | Loss: 0.00005004
Iteration 433/1000 | Loss: 0.00005004
Iteration 434/1000 | Loss: 0.00005004
Iteration 435/1000 | Loss: 0.00005004
Iteration 436/1000 | Loss: 0.00005004
Iteration 437/1000 | Loss: 0.00005004
Iteration 438/1000 | Loss: 0.00005003
Iteration 439/1000 | Loss: 0.00005003
Iteration 440/1000 | Loss: 0.00005003
Iteration 441/1000 | Loss: 0.00005003
Iteration 442/1000 | Loss: 0.00005003
Iteration 443/1000 | Loss: 0.00005002
Iteration 444/1000 | Loss: 0.00005002
Iteration 445/1000 | Loss: 0.00005002
Iteration 446/1000 | Loss: 0.00005002
Iteration 447/1000 | Loss: 0.00005001
Iteration 448/1000 | Loss: 0.00005001
Iteration 449/1000 | Loss: 0.00005001
Iteration 450/1000 | Loss: 0.00005001
Iteration 451/1000 | Loss: 0.00005001
Iteration 452/1000 | Loss: 0.00005001
Iteration 453/1000 | Loss: 0.00005001
Iteration 454/1000 | Loss: 0.00005001
Iteration 455/1000 | Loss: 0.00005000
Iteration 456/1000 | Loss: 0.00005000
Iteration 457/1000 | Loss: 0.00005000
Iteration 458/1000 | Loss: 0.00005000
Iteration 459/1000 | Loss: 0.00005000
Iteration 460/1000 | Loss: 0.00005000
Iteration 461/1000 | Loss: 0.00005000
Iteration 462/1000 | Loss: 0.00005000
Iteration 463/1000 | Loss: 0.00005000
Iteration 464/1000 | Loss: 0.00004999
Iteration 465/1000 | Loss: 0.00004999
Iteration 466/1000 | Loss: 0.00004999
Iteration 467/1000 | Loss: 0.00004999
Iteration 468/1000 | Loss: 0.00004999
Iteration 469/1000 | Loss: 0.00004999
Iteration 470/1000 | Loss: 0.00004999
Iteration 471/1000 | Loss: 0.00004999
Iteration 472/1000 | Loss: 0.00004999
Iteration 473/1000 | Loss: 0.00004999
Iteration 474/1000 | Loss: 0.00004999
Iteration 475/1000 | Loss: 0.00004999
Iteration 476/1000 | Loss: 0.00004999
Iteration 477/1000 | Loss: 0.00004999
Iteration 478/1000 | Loss: 0.00004998
Iteration 479/1000 | Loss: 0.00004998
Iteration 480/1000 | Loss: 0.00004998
Iteration 481/1000 | Loss: 0.00004998
Iteration 482/1000 | Loss: 0.00004998
Iteration 483/1000 | Loss: 0.00004998
Iteration 484/1000 | Loss: 0.00004998
Iteration 485/1000 | Loss: 0.00004998
Iteration 486/1000 | Loss: 0.00004998
Iteration 487/1000 | Loss: 0.00004998
Iteration 488/1000 | Loss: 0.00004998
Iteration 489/1000 | Loss: 0.00004998
Iteration 490/1000 | Loss: 0.00004998
Iteration 491/1000 | Loss: 0.00004998
Iteration 492/1000 | Loss: 0.00004998
Iteration 493/1000 | Loss: 0.00004998
Iteration 494/1000 | Loss: 0.00004998
Iteration 495/1000 | Loss: 0.00004998
Iteration 496/1000 | Loss: 0.00004998
Iteration 497/1000 | Loss: 0.00004998
Iteration 498/1000 | Loss: 0.00004998
Iteration 499/1000 | Loss: 0.00004998
Iteration 500/1000 | Loss: 0.00004998
Iteration 501/1000 | Loss: 0.00004998
Iteration 502/1000 | Loss: 0.00004998
Iteration 503/1000 | Loss: 0.00004998
Iteration 504/1000 | Loss: 0.00004997
Iteration 505/1000 | Loss: 0.00004997
Iteration 506/1000 | Loss: 0.00004997
Iteration 507/1000 | Loss: 0.00004997
Iteration 508/1000 | Loss: 0.00004997
Iteration 509/1000 | Loss: 0.00004997
Iteration 510/1000 | Loss: 0.00004997
Iteration 511/1000 | Loss: 0.00004997
Iteration 512/1000 | Loss: 0.00004997
Iteration 513/1000 | Loss: 0.00004997
Iteration 514/1000 | Loss: 0.00004997
Iteration 515/1000 | Loss: 0.00004997
Iteration 516/1000 | Loss: 0.00004997
Iteration 517/1000 | Loss: 0.00004997
Iteration 518/1000 | Loss: 0.00004997
Iteration 519/1000 | Loss: 0.00004997
Iteration 520/1000 | Loss: 0.00004997
Iteration 521/1000 | Loss: 0.00004997
Iteration 522/1000 | Loss: 0.00004997
Iteration 523/1000 | Loss: 0.00004997
Iteration 524/1000 | Loss: 0.00004997
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 524. Stopping optimization.
Last 5 losses: [4.9974707508226857e-05, 4.9974707508226857e-05, 4.9974707508226857e-05, 4.9974707508226857e-05, 4.9974707508226857e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.9974707508226857e-05

Optimization complete. Final v2v error: 3.9333207607269287 mm

Highest mean error: 13.439668655395508 mm for frame 76

Lowest mean error: 2.543074607849121 mm for frame 154

Saving results

Total time: 533.2918727397919
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_015/1078/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_015/1078.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_015/1078
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00899752
Iteration 2/25 | Loss: 0.00137570
Iteration 3/25 | Loss: 0.00126040
Iteration 4/25 | Loss: 0.00124044
Iteration 5/25 | Loss: 0.00123381
Iteration 6/25 | Loss: 0.00123210
Iteration 7/25 | Loss: 0.00123201
Iteration 8/25 | Loss: 0.00123201
Iteration 9/25 | Loss: 0.00123201
Iteration 10/25 | Loss: 0.00123201
Iteration 11/25 | Loss: 0.00123201
Iteration 12/25 | Loss: 0.00123201
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0012320078676566482, 0.0012320078676566482, 0.0012320078676566482, 0.0012320078676566482, 0.0012320078676566482]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012320078676566482

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.31250310
Iteration 2/25 | Loss: 0.00124997
Iteration 3/25 | Loss: 0.00124995
Iteration 4/25 | Loss: 0.00124995
Iteration 5/25 | Loss: 0.00124995
Iteration 6/25 | Loss: 0.00124995
Iteration 7/25 | Loss: 0.00124995
Iteration 8/25 | Loss: 0.00124995
Iteration 9/25 | Loss: 0.00124995
Iteration 10/25 | Loss: 0.00124995
Iteration 11/25 | Loss: 0.00124995
Iteration 12/25 | Loss: 0.00124995
Iteration 13/25 | Loss: 0.00124995
Iteration 14/25 | Loss: 0.00124995
Iteration 15/25 | Loss: 0.00124995
Iteration 16/25 | Loss: 0.00124995
Iteration 17/25 | Loss: 0.00124995
Iteration 18/25 | Loss: 0.00124995
Iteration 19/25 | Loss: 0.00124995
Iteration 20/25 | Loss: 0.00124995
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0012499489821493626, 0.0012499489821493626, 0.0012499489821493626, 0.0012499489821493626, 0.0012499489821493626]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012499489821493626

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00124995
Iteration 2/1000 | Loss: 0.00004718
Iteration 3/1000 | Loss: 0.00003229
Iteration 4/1000 | Loss: 0.00002639
Iteration 5/1000 | Loss: 0.00002430
Iteration 6/1000 | Loss: 0.00002312
Iteration 7/1000 | Loss: 0.00002238
Iteration 8/1000 | Loss: 0.00002178
Iteration 9/1000 | Loss: 0.00002116
Iteration 10/1000 | Loss: 0.00002079
Iteration 11/1000 | Loss: 0.00002049
Iteration 12/1000 | Loss: 0.00002035
Iteration 13/1000 | Loss: 0.00002016
Iteration 14/1000 | Loss: 0.00002011
Iteration 15/1000 | Loss: 0.00001999
Iteration 16/1000 | Loss: 0.00001996
Iteration 17/1000 | Loss: 0.00001995
Iteration 18/1000 | Loss: 0.00001992
Iteration 19/1000 | Loss: 0.00001981
Iteration 20/1000 | Loss: 0.00001978
Iteration 21/1000 | Loss: 0.00001977
Iteration 22/1000 | Loss: 0.00001974
Iteration 23/1000 | Loss: 0.00001974
Iteration 24/1000 | Loss: 0.00001973
Iteration 25/1000 | Loss: 0.00001973
Iteration 26/1000 | Loss: 0.00001972
Iteration 27/1000 | Loss: 0.00001971
Iteration 28/1000 | Loss: 0.00001970
Iteration 29/1000 | Loss: 0.00001970
Iteration 30/1000 | Loss: 0.00001969
Iteration 31/1000 | Loss: 0.00001969
Iteration 32/1000 | Loss: 0.00001969
Iteration 33/1000 | Loss: 0.00001968
Iteration 34/1000 | Loss: 0.00001968
Iteration 35/1000 | Loss: 0.00001968
Iteration 36/1000 | Loss: 0.00001967
Iteration 37/1000 | Loss: 0.00001967
Iteration 38/1000 | Loss: 0.00001967
Iteration 39/1000 | Loss: 0.00001967
Iteration 40/1000 | Loss: 0.00001966
Iteration 41/1000 | Loss: 0.00001965
Iteration 42/1000 | Loss: 0.00001965
Iteration 43/1000 | Loss: 0.00001965
Iteration 44/1000 | Loss: 0.00001964
Iteration 45/1000 | Loss: 0.00001963
Iteration 46/1000 | Loss: 0.00001963
Iteration 47/1000 | Loss: 0.00001963
Iteration 48/1000 | Loss: 0.00001962
Iteration 49/1000 | Loss: 0.00001962
Iteration 50/1000 | Loss: 0.00001961
Iteration 51/1000 | Loss: 0.00001961
Iteration 52/1000 | Loss: 0.00001960
Iteration 53/1000 | Loss: 0.00001960
Iteration 54/1000 | Loss: 0.00001960
Iteration 55/1000 | Loss: 0.00001960
Iteration 56/1000 | Loss: 0.00001959
Iteration 57/1000 | Loss: 0.00001959
Iteration 58/1000 | Loss: 0.00001959
Iteration 59/1000 | Loss: 0.00001959
Iteration 60/1000 | Loss: 0.00001959
Iteration 61/1000 | Loss: 0.00001959
Iteration 62/1000 | Loss: 0.00001959
Iteration 63/1000 | Loss: 0.00001959
Iteration 64/1000 | Loss: 0.00001958
Iteration 65/1000 | Loss: 0.00001957
Iteration 66/1000 | Loss: 0.00001956
Iteration 67/1000 | Loss: 0.00001956
Iteration 68/1000 | Loss: 0.00001956
Iteration 69/1000 | Loss: 0.00001955
Iteration 70/1000 | Loss: 0.00001955
Iteration 71/1000 | Loss: 0.00001955
Iteration 72/1000 | Loss: 0.00001955
Iteration 73/1000 | Loss: 0.00001955
Iteration 74/1000 | Loss: 0.00001954
Iteration 75/1000 | Loss: 0.00001954
Iteration 76/1000 | Loss: 0.00001953
Iteration 77/1000 | Loss: 0.00001952
Iteration 78/1000 | Loss: 0.00001952
Iteration 79/1000 | Loss: 0.00001951
Iteration 80/1000 | Loss: 0.00001951
Iteration 81/1000 | Loss: 0.00001950
Iteration 82/1000 | Loss: 0.00001950
Iteration 83/1000 | Loss: 0.00001950
Iteration 84/1000 | Loss: 0.00001949
Iteration 85/1000 | Loss: 0.00001948
Iteration 86/1000 | Loss: 0.00001948
Iteration 87/1000 | Loss: 0.00001948
Iteration 88/1000 | Loss: 0.00001947
Iteration 89/1000 | Loss: 0.00001947
Iteration 90/1000 | Loss: 0.00001947
Iteration 91/1000 | Loss: 0.00001946
Iteration 92/1000 | Loss: 0.00001946
Iteration 93/1000 | Loss: 0.00001946
Iteration 94/1000 | Loss: 0.00001945
Iteration 95/1000 | Loss: 0.00001945
Iteration 96/1000 | Loss: 0.00001945
Iteration 97/1000 | Loss: 0.00001944
Iteration 98/1000 | Loss: 0.00001944
Iteration 99/1000 | Loss: 0.00001944
Iteration 100/1000 | Loss: 0.00001944
Iteration 101/1000 | Loss: 0.00001943
Iteration 102/1000 | Loss: 0.00001943
Iteration 103/1000 | Loss: 0.00001943
Iteration 104/1000 | Loss: 0.00001943
Iteration 105/1000 | Loss: 0.00001943
Iteration 106/1000 | Loss: 0.00001942
Iteration 107/1000 | Loss: 0.00001942
Iteration 108/1000 | Loss: 0.00001942
Iteration 109/1000 | Loss: 0.00001941
Iteration 110/1000 | Loss: 0.00001941
Iteration 111/1000 | Loss: 0.00001940
Iteration 112/1000 | Loss: 0.00001940
Iteration 113/1000 | Loss: 0.00001940
Iteration 114/1000 | Loss: 0.00001939
Iteration 115/1000 | Loss: 0.00001939
Iteration 116/1000 | Loss: 0.00001939
Iteration 117/1000 | Loss: 0.00001938
Iteration 118/1000 | Loss: 0.00001938
Iteration 119/1000 | Loss: 0.00001938
Iteration 120/1000 | Loss: 0.00001938
Iteration 121/1000 | Loss: 0.00001938
Iteration 122/1000 | Loss: 0.00001937
Iteration 123/1000 | Loss: 0.00001937
Iteration 124/1000 | Loss: 0.00001937
Iteration 125/1000 | Loss: 0.00001937
Iteration 126/1000 | Loss: 0.00001937
Iteration 127/1000 | Loss: 0.00001936
Iteration 128/1000 | Loss: 0.00001936
Iteration 129/1000 | Loss: 0.00001936
Iteration 130/1000 | Loss: 0.00001935
Iteration 131/1000 | Loss: 0.00001935
Iteration 132/1000 | Loss: 0.00001935
Iteration 133/1000 | Loss: 0.00001935
Iteration 134/1000 | Loss: 0.00001934
Iteration 135/1000 | Loss: 0.00001934
Iteration 136/1000 | Loss: 0.00001934
Iteration 137/1000 | Loss: 0.00001934
Iteration 138/1000 | Loss: 0.00001934
Iteration 139/1000 | Loss: 0.00001934
Iteration 140/1000 | Loss: 0.00001934
Iteration 141/1000 | Loss: 0.00001933
Iteration 142/1000 | Loss: 0.00001933
Iteration 143/1000 | Loss: 0.00001933
Iteration 144/1000 | Loss: 0.00001932
Iteration 145/1000 | Loss: 0.00001932
Iteration 146/1000 | Loss: 0.00001932
Iteration 147/1000 | Loss: 0.00001932
Iteration 148/1000 | Loss: 0.00001932
Iteration 149/1000 | Loss: 0.00001932
Iteration 150/1000 | Loss: 0.00001932
Iteration 151/1000 | Loss: 0.00001932
Iteration 152/1000 | Loss: 0.00001931
Iteration 153/1000 | Loss: 0.00001931
Iteration 154/1000 | Loss: 0.00001931
Iteration 155/1000 | Loss: 0.00001931
Iteration 156/1000 | Loss: 0.00001931
Iteration 157/1000 | Loss: 0.00001931
Iteration 158/1000 | Loss: 0.00001931
Iteration 159/1000 | Loss: 0.00001931
Iteration 160/1000 | Loss: 0.00001931
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 160. Stopping optimization.
Last 5 losses: [1.931415135913994e-05, 1.931415135913994e-05, 1.931415135913994e-05, 1.931415135913994e-05, 1.931415135913994e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.931415135913994e-05

Optimization complete. Final v2v error: 3.6962411403656006 mm

Highest mean error: 5.238963603973389 mm for frame 67

Lowest mean error: 3.114037036895752 mm for frame 44

Saving results

Total time: 42.10611820220947
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_015/1013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_015/1013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_015/1013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00755131
Iteration 2/25 | Loss: 0.00156970
Iteration 3/25 | Loss: 0.00128182
Iteration 4/25 | Loss: 0.00125198
Iteration 5/25 | Loss: 0.00124667
Iteration 6/25 | Loss: 0.00124551
Iteration 7/25 | Loss: 0.00124551
Iteration 8/25 | Loss: 0.00124551
Iteration 9/25 | Loss: 0.00124551
Iteration 10/25 | Loss: 0.00124551
Iteration 11/25 | Loss: 0.00124551
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012455128598958254, 0.0012455128598958254, 0.0012455128598958254, 0.0012455128598958254, 0.0012455128598958254]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012455128598958254

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.01271534
Iteration 2/25 | Loss: 0.00131076
Iteration 3/25 | Loss: 0.00131076
Iteration 4/25 | Loss: 0.00131076
Iteration 5/25 | Loss: 0.00131076
Iteration 6/25 | Loss: 0.00131075
Iteration 7/25 | Loss: 0.00131075
Iteration 8/25 | Loss: 0.00131075
Iteration 9/25 | Loss: 0.00131075
Iteration 10/25 | Loss: 0.00131075
Iteration 11/25 | Loss: 0.00131075
Iteration 12/25 | Loss: 0.00131075
Iteration 13/25 | Loss: 0.00131075
Iteration 14/25 | Loss: 0.00131075
Iteration 15/25 | Loss: 0.00131075
Iteration 16/25 | Loss: 0.00131075
Iteration 17/25 | Loss: 0.00131075
Iteration 18/25 | Loss: 0.00131075
Iteration 19/25 | Loss: 0.00131075
Iteration 20/25 | Loss: 0.00131075
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0013107532868161798, 0.0013107532868161798, 0.0013107532868161798, 0.0013107532868161798, 0.0013107532868161798]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013107532868161798

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00131075
Iteration 2/1000 | Loss: 0.00003633
Iteration 3/1000 | Loss: 0.00002384
Iteration 4/1000 | Loss: 0.00002063
Iteration 5/1000 | Loss: 0.00001938
Iteration 6/1000 | Loss: 0.00001855
Iteration 7/1000 | Loss: 0.00001797
Iteration 8/1000 | Loss: 0.00001759
Iteration 9/1000 | Loss: 0.00001733
Iteration 10/1000 | Loss: 0.00001712
Iteration 11/1000 | Loss: 0.00001709
Iteration 12/1000 | Loss: 0.00001684
Iteration 13/1000 | Loss: 0.00001663
Iteration 14/1000 | Loss: 0.00001663
Iteration 15/1000 | Loss: 0.00001656
Iteration 16/1000 | Loss: 0.00001654
Iteration 17/1000 | Loss: 0.00001652
Iteration 18/1000 | Loss: 0.00001650
Iteration 19/1000 | Loss: 0.00001646
Iteration 20/1000 | Loss: 0.00001640
Iteration 21/1000 | Loss: 0.00001637
Iteration 22/1000 | Loss: 0.00001636
Iteration 23/1000 | Loss: 0.00001634
Iteration 24/1000 | Loss: 0.00001631
Iteration 25/1000 | Loss: 0.00001630
Iteration 26/1000 | Loss: 0.00001630
Iteration 27/1000 | Loss: 0.00001629
Iteration 28/1000 | Loss: 0.00001628
Iteration 29/1000 | Loss: 0.00001628
Iteration 30/1000 | Loss: 0.00001627
Iteration 31/1000 | Loss: 0.00001627
Iteration 32/1000 | Loss: 0.00001626
Iteration 33/1000 | Loss: 0.00001626
Iteration 34/1000 | Loss: 0.00001625
Iteration 35/1000 | Loss: 0.00001625
Iteration 36/1000 | Loss: 0.00001621
Iteration 37/1000 | Loss: 0.00001620
Iteration 38/1000 | Loss: 0.00001618
Iteration 39/1000 | Loss: 0.00001618
Iteration 40/1000 | Loss: 0.00001617
Iteration 41/1000 | Loss: 0.00001616
Iteration 42/1000 | Loss: 0.00001616
Iteration 43/1000 | Loss: 0.00001615
Iteration 44/1000 | Loss: 0.00001615
Iteration 45/1000 | Loss: 0.00001614
Iteration 46/1000 | Loss: 0.00001613
Iteration 47/1000 | Loss: 0.00001613
Iteration 48/1000 | Loss: 0.00001612
Iteration 49/1000 | Loss: 0.00001612
Iteration 50/1000 | Loss: 0.00001611
Iteration 51/1000 | Loss: 0.00001611
Iteration 52/1000 | Loss: 0.00001611
Iteration 53/1000 | Loss: 0.00001610
Iteration 54/1000 | Loss: 0.00001610
Iteration 55/1000 | Loss: 0.00001609
Iteration 56/1000 | Loss: 0.00001607
Iteration 57/1000 | Loss: 0.00001607
Iteration 58/1000 | Loss: 0.00001605
Iteration 59/1000 | Loss: 0.00001604
Iteration 60/1000 | Loss: 0.00001604
Iteration 61/1000 | Loss: 0.00001604
Iteration 62/1000 | Loss: 0.00001604
Iteration 63/1000 | Loss: 0.00001603
Iteration 64/1000 | Loss: 0.00001603
Iteration 65/1000 | Loss: 0.00001602
Iteration 66/1000 | Loss: 0.00001602
Iteration 67/1000 | Loss: 0.00001601
Iteration 68/1000 | Loss: 0.00001601
Iteration 69/1000 | Loss: 0.00001601
Iteration 70/1000 | Loss: 0.00001600
Iteration 71/1000 | Loss: 0.00001600
Iteration 72/1000 | Loss: 0.00001600
Iteration 73/1000 | Loss: 0.00001599
Iteration 74/1000 | Loss: 0.00001599
Iteration 75/1000 | Loss: 0.00001599
Iteration 76/1000 | Loss: 0.00001598
Iteration 77/1000 | Loss: 0.00001598
Iteration 78/1000 | Loss: 0.00001597
Iteration 79/1000 | Loss: 0.00001597
Iteration 80/1000 | Loss: 0.00001597
Iteration 81/1000 | Loss: 0.00001596
Iteration 82/1000 | Loss: 0.00001596
Iteration 83/1000 | Loss: 0.00001596
Iteration 84/1000 | Loss: 0.00001596
Iteration 85/1000 | Loss: 0.00001595
Iteration 86/1000 | Loss: 0.00001595
Iteration 87/1000 | Loss: 0.00001595
Iteration 88/1000 | Loss: 0.00001595
Iteration 89/1000 | Loss: 0.00001595
Iteration 90/1000 | Loss: 0.00001595
Iteration 91/1000 | Loss: 0.00001595
Iteration 92/1000 | Loss: 0.00001595
Iteration 93/1000 | Loss: 0.00001595
Iteration 94/1000 | Loss: 0.00001595
Iteration 95/1000 | Loss: 0.00001595
Iteration 96/1000 | Loss: 0.00001595
Iteration 97/1000 | Loss: 0.00001594
Iteration 98/1000 | Loss: 0.00001594
Iteration 99/1000 | Loss: 0.00001594
Iteration 100/1000 | Loss: 0.00001594
Iteration 101/1000 | Loss: 0.00001594
Iteration 102/1000 | Loss: 0.00001593
Iteration 103/1000 | Loss: 0.00001593
Iteration 104/1000 | Loss: 0.00001593
Iteration 105/1000 | Loss: 0.00001592
Iteration 106/1000 | Loss: 0.00001592
Iteration 107/1000 | Loss: 0.00001592
Iteration 108/1000 | Loss: 0.00001592
Iteration 109/1000 | Loss: 0.00001591
Iteration 110/1000 | Loss: 0.00001591
Iteration 111/1000 | Loss: 0.00001591
Iteration 112/1000 | Loss: 0.00001590
Iteration 113/1000 | Loss: 0.00001590
Iteration 114/1000 | Loss: 0.00001590
Iteration 115/1000 | Loss: 0.00001590
Iteration 116/1000 | Loss: 0.00001589
Iteration 117/1000 | Loss: 0.00001589
Iteration 118/1000 | Loss: 0.00001589
Iteration 119/1000 | Loss: 0.00001588
Iteration 120/1000 | Loss: 0.00001588
Iteration 121/1000 | Loss: 0.00001588
Iteration 122/1000 | Loss: 0.00001588
Iteration 123/1000 | Loss: 0.00001587
Iteration 124/1000 | Loss: 0.00001587
Iteration 125/1000 | Loss: 0.00001587
Iteration 126/1000 | Loss: 0.00001587
Iteration 127/1000 | Loss: 0.00001587
Iteration 128/1000 | Loss: 0.00001586
Iteration 129/1000 | Loss: 0.00001586
Iteration 130/1000 | Loss: 0.00001586
Iteration 131/1000 | Loss: 0.00001586
Iteration 132/1000 | Loss: 0.00001585
Iteration 133/1000 | Loss: 0.00001585
Iteration 134/1000 | Loss: 0.00001585
Iteration 135/1000 | Loss: 0.00001585
Iteration 136/1000 | Loss: 0.00001584
Iteration 137/1000 | Loss: 0.00001584
Iteration 138/1000 | Loss: 0.00001584
Iteration 139/1000 | Loss: 0.00001584
Iteration 140/1000 | Loss: 0.00001583
Iteration 141/1000 | Loss: 0.00001583
Iteration 142/1000 | Loss: 0.00001583
Iteration 143/1000 | Loss: 0.00001583
Iteration 144/1000 | Loss: 0.00001583
Iteration 145/1000 | Loss: 0.00001582
Iteration 146/1000 | Loss: 0.00001582
Iteration 147/1000 | Loss: 0.00001582
Iteration 148/1000 | Loss: 0.00001581
Iteration 149/1000 | Loss: 0.00001581
Iteration 150/1000 | Loss: 0.00001581
Iteration 151/1000 | Loss: 0.00001581
Iteration 152/1000 | Loss: 0.00001580
Iteration 153/1000 | Loss: 0.00001580
Iteration 154/1000 | Loss: 0.00001580
Iteration 155/1000 | Loss: 0.00001580
Iteration 156/1000 | Loss: 0.00001580
Iteration 157/1000 | Loss: 0.00001580
Iteration 158/1000 | Loss: 0.00001579
Iteration 159/1000 | Loss: 0.00001579
Iteration 160/1000 | Loss: 0.00001579
Iteration 161/1000 | Loss: 0.00001579
Iteration 162/1000 | Loss: 0.00001579
Iteration 163/1000 | Loss: 0.00001579
Iteration 164/1000 | Loss: 0.00001579
Iteration 165/1000 | Loss: 0.00001578
Iteration 166/1000 | Loss: 0.00001578
Iteration 167/1000 | Loss: 0.00001578
Iteration 168/1000 | Loss: 0.00001578
Iteration 169/1000 | Loss: 0.00001578
Iteration 170/1000 | Loss: 0.00001578
Iteration 171/1000 | Loss: 0.00001577
Iteration 172/1000 | Loss: 0.00001577
Iteration 173/1000 | Loss: 0.00001576
Iteration 174/1000 | Loss: 0.00001576
Iteration 175/1000 | Loss: 0.00001576
Iteration 176/1000 | Loss: 0.00001576
Iteration 177/1000 | Loss: 0.00001575
Iteration 178/1000 | Loss: 0.00001575
Iteration 179/1000 | Loss: 0.00001575
Iteration 180/1000 | Loss: 0.00001574
Iteration 181/1000 | Loss: 0.00001574
Iteration 182/1000 | Loss: 0.00001574
Iteration 183/1000 | Loss: 0.00001573
Iteration 184/1000 | Loss: 0.00001573
Iteration 185/1000 | Loss: 0.00001573
Iteration 186/1000 | Loss: 0.00001573
Iteration 187/1000 | Loss: 0.00001572
Iteration 188/1000 | Loss: 0.00001572
Iteration 189/1000 | Loss: 0.00001572
Iteration 190/1000 | Loss: 0.00001572
Iteration 191/1000 | Loss: 0.00001572
Iteration 192/1000 | Loss: 0.00001572
Iteration 193/1000 | Loss: 0.00001572
Iteration 194/1000 | Loss: 0.00001571
Iteration 195/1000 | Loss: 0.00001571
Iteration 196/1000 | Loss: 0.00001571
Iteration 197/1000 | Loss: 0.00001571
Iteration 198/1000 | Loss: 0.00001571
Iteration 199/1000 | Loss: 0.00001571
Iteration 200/1000 | Loss: 0.00001571
Iteration 201/1000 | Loss: 0.00001571
Iteration 202/1000 | Loss: 0.00001571
Iteration 203/1000 | Loss: 0.00001571
Iteration 204/1000 | Loss: 0.00001571
Iteration 205/1000 | Loss: 0.00001571
Iteration 206/1000 | Loss: 0.00001571
Iteration 207/1000 | Loss: 0.00001571
Iteration 208/1000 | Loss: 0.00001571
Iteration 209/1000 | Loss: 0.00001571
Iteration 210/1000 | Loss: 0.00001571
Iteration 211/1000 | Loss: 0.00001571
Iteration 212/1000 | Loss: 0.00001571
Iteration 213/1000 | Loss: 0.00001571
Iteration 214/1000 | Loss: 0.00001571
Iteration 215/1000 | Loss: 0.00001571
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 215. Stopping optimization.
Last 5 losses: [1.570990752952639e-05, 1.570990752952639e-05, 1.570990752952639e-05, 1.570990752952639e-05, 1.570990752952639e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.570990752952639e-05

Optimization complete. Final v2v error: 3.265988826751709 mm

Highest mean error: 4.748985290527344 mm for frame 161

Lowest mean error: 2.610177755355835 mm for frame 189

Saving results

Total time: 47.02085995674133
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_015/1020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_015/1020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_015/1020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00428262
Iteration 2/25 | Loss: 0.00132911
Iteration 3/25 | Loss: 0.00121876
Iteration 4/25 | Loss: 0.00120929
Iteration 5/25 | Loss: 0.00120704
Iteration 6/25 | Loss: 0.00120704
Iteration 7/25 | Loss: 0.00120704
Iteration 8/25 | Loss: 0.00120704
Iteration 9/25 | Loss: 0.00120704
Iteration 10/25 | Loss: 0.00120704
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.001207037945277989, 0.001207037945277989, 0.001207037945277989, 0.001207037945277989, 0.001207037945277989]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001207037945277989

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.27824306
Iteration 2/25 | Loss: 0.00137875
Iteration 3/25 | Loss: 0.00137875
Iteration 4/25 | Loss: 0.00137875
Iteration 5/25 | Loss: 0.00137875
Iteration 6/25 | Loss: 0.00137875
Iteration 7/25 | Loss: 0.00137875
Iteration 8/25 | Loss: 0.00137875
Iteration 9/25 | Loss: 0.00137875
Iteration 10/25 | Loss: 0.00137875
Iteration 11/25 | Loss: 0.00137875
Iteration 12/25 | Loss: 0.00137875
Iteration 13/25 | Loss: 0.00137875
Iteration 14/25 | Loss: 0.00137875
Iteration 15/25 | Loss: 0.00137875
Iteration 16/25 | Loss: 0.00137875
Iteration 17/25 | Loss: 0.00137875
Iteration 18/25 | Loss: 0.00137875
Iteration 19/25 | Loss: 0.00137875
Iteration 20/25 | Loss: 0.00137875
Iteration 21/25 | Loss: 0.00137875
Iteration 22/25 | Loss: 0.00137875
Iteration 23/25 | Loss: 0.00137875
Iteration 24/25 | Loss: 0.00137875
Iteration 25/25 | Loss: 0.00137875

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00137875
Iteration 2/1000 | Loss: 0.00002345
Iteration 3/1000 | Loss: 0.00001699
Iteration 4/1000 | Loss: 0.00001546
Iteration 5/1000 | Loss: 0.00001458
Iteration 6/1000 | Loss: 0.00001395
Iteration 7/1000 | Loss: 0.00001372
Iteration 8/1000 | Loss: 0.00001353
Iteration 9/1000 | Loss: 0.00001332
Iteration 10/1000 | Loss: 0.00001308
Iteration 11/1000 | Loss: 0.00001303
Iteration 12/1000 | Loss: 0.00001291
Iteration 13/1000 | Loss: 0.00001280
Iteration 14/1000 | Loss: 0.00001276
Iteration 15/1000 | Loss: 0.00001276
Iteration 16/1000 | Loss: 0.00001271
Iteration 17/1000 | Loss: 0.00001261
Iteration 18/1000 | Loss: 0.00001258
Iteration 19/1000 | Loss: 0.00001252
Iteration 20/1000 | Loss: 0.00001250
Iteration 21/1000 | Loss: 0.00001249
Iteration 22/1000 | Loss: 0.00001247
Iteration 23/1000 | Loss: 0.00001246
Iteration 24/1000 | Loss: 0.00001245
Iteration 25/1000 | Loss: 0.00001244
Iteration 26/1000 | Loss: 0.00001234
Iteration 27/1000 | Loss: 0.00001232
Iteration 28/1000 | Loss: 0.00001232
Iteration 29/1000 | Loss: 0.00001229
Iteration 30/1000 | Loss: 0.00001220
Iteration 31/1000 | Loss: 0.00001218
Iteration 32/1000 | Loss: 0.00001218
Iteration 33/1000 | Loss: 0.00001218
Iteration 34/1000 | Loss: 0.00001218
Iteration 35/1000 | Loss: 0.00001218
Iteration 36/1000 | Loss: 0.00001217
Iteration 37/1000 | Loss: 0.00001214
Iteration 38/1000 | Loss: 0.00001213
Iteration 39/1000 | Loss: 0.00001212
Iteration 40/1000 | Loss: 0.00001212
Iteration 41/1000 | Loss: 0.00001212
Iteration 42/1000 | Loss: 0.00001212
Iteration 43/1000 | Loss: 0.00001211
Iteration 44/1000 | Loss: 0.00001211
Iteration 45/1000 | Loss: 0.00001210
Iteration 46/1000 | Loss: 0.00001210
Iteration 47/1000 | Loss: 0.00001209
Iteration 48/1000 | Loss: 0.00001209
Iteration 49/1000 | Loss: 0.00001209
Iteration 50/1000 | Loss: 0.00001209
Iteration 51/1000 | Loss: 0.00001208
Iteration 52/1000 | Loss: 0.00001208
Iteration 53/1000 | Loss: 0.00001208
Iteration 54/1000 | Loss: 0.00001207
Iteration 55/1000 | Loss: 0.00001207
Iteration 56/1000 | Loss: 0.00001207
Iteration 57/1000 | Loss: 0.00001207
Iteration 58/1000 | Loss: 0.00001206
Iteration 59/1000 | Loss: 0.00001206
Iteration 60/1000 | Loss: 0.00001205
Iteration 61/1000 | Loss: 0.00001205
Iteration 62/1000 | Loss: 0.00001205
Iteration 63/1000 | Loss: 0.00001205
Iteration 64/1000 | Loss: 0.00001205
Iteration 65/1000 | Loss: 0.00001204
Iteration 66/1000 | Loss: 0.00001204
Iteration 67/1000 | Loss: 0.00001204
Iteration 68/1000 | Loss: 0.00001204
Iteration 69/1000 | Loss: 0.00001204
Iteration 70/1000 | Loss: 0.00001203
Iteration 71/1000 | Loss: 0.00001203
Iteration 72/1000 | Loss: 0.00001202
Iteration 73/1000 | Loss: 0.00001202
Iteration 74/1000 | Loss: 0.00001202
Iteration 75/1000 | Loss: 0.00001202
Iteration 76/1000 | Loss: 0.00001202
Iteration 77/1000 | Loss: 0.00001202
Iteration 78/1000 | Loss: 0.00001202
Iteration 79/1000 | Loss: 0.00001202
Iteration 80/1000 | Loss: 0.00001201
Iteration 81/1000 | Loss: 0.00001201
Iteration 82/1000 | Loss: 0.00001201
Iteration 83/1000 | Loss: 0.00001201
Iteration 84/1000 | Loss: 0.00001201
Iteration 85/1000 | Loss: 0.00001201
Iteration 86/1000 | Loss: 0.00001201
Iteration 87/1000 | Loss: 0.00001201
Iteration 88/1000 | Loss: 0.00001200
Iteration 89/1000 | Loss: 0.00001200
Iteration 90/1000 | Loss: 0.00001200
Iteration 91/1000 | Loss: 0.00001200
Iteration 92/1000 | Loss: 0.00001199
Iteration 93/1000 | Loss: 0.00001199
Iteration 94/1000 | Loss: 0.00001199
Iteration 95/1000 | Loss: 0.00001198
Iteration 96/1000 | Loss: 0.00001198
Iteration 97/1000 | Loss: 0.00001198
Iteration 98/1000 | Loss: 0.00001197
Iteration 99/1000 | Loss: 0.00001197
Iteration 100/1000 | Loss: 0.00001197
Iteration 101/1000 | Loss: 0.00001197
Iteration 102/1000 | Loss: 0.00001196
Iteration 103/1000 | Loss: 0.00001196
Iteration 104/1000 | Loss: 0.00001195
Iteration 105/1000 | Loss: 0.00001195
Iteration 106/1000 | Loss: 0.00001195
Iteration 107/1000 | Loss: 0.00001195
Iteration 108/1000 | Loss: 0.00001195
Iteration 109/1000 | Loss: 0.00001195
Iteration 110/1000 | Loss: 0.00001195
Iteration 111/1000 | Loss: 0.00001194
Iteration 112/1000 | Loss: 0.00001194
Iteration 113/1000 | Loss: 0.00001194
Iteration 114/1000 | Loss: 0.00001194
Iteration 115/1000 | Loss: 0.00001194
Iteration 116/1000 | Loss: 0.00001193
Iteration 117/1000 | Loss: 0.00001193
Iteration 118/1000 | Loss: 0.00001192
Iteration 119/1000 | Loss: 0.00001192
Iteration 120/1000 | Loss: 0.00001192
Iteration 121/1000 | Loss: 0.00001192
Iteration 122/1000 | Loss: 0.00001191
Iteration 123/1000 | Loss: 0.00001191
Iteration 124/1000 | Loss: 0.00001191
Iteration 125/1000 | Loss: 0.00001191
Iteration 126/1000 | Loss: 0.00001191
Iteration 127/1000 | Loss: 0.00001191
Iteration 128/1000 | Loss: 0.00001191
Iteration 129/1000 | Loss: 0.00001190
Iteration 130/1000 | Loss: 0.00001190
Iteration 131/1000 | Loss: 0.00001190
Iteration 132/1000 | Loss: 0.00001190
Iteration 133/1000 | Loss: 0.00001190
Iteration 134/1000 | Loss: 0.00001190
Iteration 135/1000 | Loss: 0.00001190
Iteration 136/1000 | Loss: 0.00001190
Iteration 137/1000 | Loss: 0.00001190
Iteration 138/1000 | Loss: 0.00001190
Iteration 139/1000 | Loss: 0.00001189
Iteration 140/1000 | Loss: 0.00001189
Iteration 141/1000 | Loss: 0.00001189
Iteration 142/1000 | Loss: 0.00001189
Iteration 143/1000 | Loss: 0.00001189
Iteration 144/1000 | Loss: 0.00001188
Iteration 145/1000 | Loss: 0.00001188
Iteration 146/1000 | Loss: 0.00001187
Iteration 147/1000 | Loss: 0.00001187
Iteration 148/1000 | Loss: 0.00001187
Iteration 149/1000 | Loss: 0.00001186
Iteration 150/1000 | Loss: 0.00001186
Iteration 151/1000 | Loss: 0.00001186
Iteration 152/1000 | Loss: 0.00001186
Iteration 153/1000 | Loss: 0.00001186
Iteration 154/1000 | Loss: 0.00001186
Iteration 155/1000 | Loss: 0.00001186
Iteration 156/1000 | Loss: 0.00001186
Iteration 157/1000 | Loss: 0.00001186
Iteration 158/1000 | Loss: 0.00001186
Iteration 159/1000 | Loss: 0.00001185
Iteration 160/1000 | Loss: 0.00001185
Iteration 161/1000 | Loss: 0.00001185
Iteration 162/1000 | Loss: 0.00001185
Iteration 163/1000 | Loss: 0.00001185
Iteration 164/1000 | Loss: 0.00001185
Iteration 165/1000 | Loss: 0.00001185
Iteration 166/1000 | Loss: 0.00001185
Iteration 167/1000 | Loss: 0.00001185
Iteration 168/1000 | Loss: 0.00001185
Iteration 169/1000 | Loss: 0.00001185
Iteration 170/1000 | Loss: 0.00001185
Iteration 171/1000 | Loss: 0.00001185
Iteration 172/1000 | Loss: 0.00001185
Iteration 173/1000 | Loss: 0.00001185
Iteration 174/1000 | Loss: 0.00001185
Iteration 175/1000 | Loss: 0.00001185
Iteration 176/1000 | Loss: 0.00001185
Iteration 177/1000 | Loss: 0.00001185
Iteration 178/1000 | Loss: 0.00001185
Iteration 179/1000 | Loss: 0.00001185
Iteration 180/1000 | Loss: 0.00001185
Iteration 181/1000 | Loss: 0.00001185
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 181. Stopping optimization.
Last 5 losses: [1.1852769603137858e-05, 1.1852769603137858e-05, 1.1852769603137858e-05, 1.1852769603137858e-05, 1.1852769603137858e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1852769603137858e-05

Optimization complete. Final v2v error: 2.858342170715332 mm

Highest mean error: 3.1094162464141846 mm for frame 128

Lowest mean error: 2.6466543674468994 mm for frame 139

Saving results

Total time: 46.918660402297974
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_015/1009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_015/1009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_015/1009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00788531
Iteration 2/25 | Loss: 0.00139666
Iteration 3/25 | Loss: 0.00119079
Iteration 4/25 | Loss: 0.00117600
Iteration 5/25 | Loss: 0.00117110
Iteration 6/25 | Loss: 0.00116947
Iteration 7/25 | Loss: 0.00116934
Iteration 8/25 | Loss: 0.00116934
Iteration 9/25 | Loss: 0.00116934
Iteration 10/25 | Loss: 0.00116934
Iteration 11/25 | Loss: 0.00116934
Iteration 12/25 | Loss: 0.00116934
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0011693357955664396, 0.0011693357955664396, 0.0011693357955664396, 0.0011693357955664396, 0.0011693357955664396]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011693357955664396

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.11689305
Iteration 2/25 | Loss: 0.00144311
Iteration 3/25 | Loss: 0.00144311
Iteration 4/25 | Loss: 0.00144311
Iteration 5/25 | Loss: 0.00144311
Iteration 6/25 | Loss: 0.00144311
Iteration 7/25 | Loss: 0.00144311
Iteration 8/25 | Loss: 0.00144311
Iteration 9/25 | Loss: 0.00144311
Iteration 10/25 | Loss: 0.00144311
Iteration 11/25 | Loss: 0.00144310
Iteration 12/25 | Loss: 0.00144310
Iteration 13/25 | Loss: 0.00144310
Iteration 14/25 | Loss: 0.00144310
Iteration 15/25 | Loss: 0.00144310
Iteration 16/25 | Loss: 0.00144310
Iteration 17/25 | Loss: 0.00144310
Iteration 18/25 | Loss: 0.00144310
Iteration 19/25 | Loss: 0.00144310
Iteration 20/25 | Loss: 0.00144310
Iteration 21/25 | Loss: 0.00144310
Iteration 22/25 | Loss: 0.00144310
Iteration 23/25 | Loss: 0.00144310
Iteration 24/25 | Loss: 0.00144310
Iteration 25/25 | Loss: 0.00144310

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00144310
Iteration 2/1000 | Loss: 0.00004474
Iteration 3/1000 | Loss: 0.00002760
Iteration 4/1000 | Loss: 0.00002057
Iteration 5/1000 | Loss: 0.00001824
Iteration 6/1000 | Loss: 0.00001683
Iteration 7/1000 | Loss: 0.00001562
Iteration 8/1000 | Loss: 0.00001503
Iteration 9/1000 | Loss: 0.00001442
Iteration 10/1000 | Loss: 0.00001397
Iteration 11/1000 | Loss: 0.00001372
Iteration 12/1000 | Loss: 0.00001347
Iteration 13/1000 | Loss: 0.00001330
Iteration 14/1000 | Loss: 0.00001320
Iteration 15/1000 | Loss: 0.00001316
Iteration 16/1000 | Loss: 0.00001315
Iteration 17/1000 | Loss: 0.00001314
Iteration 18/1000 | Loss: 0.00001314
Iteration 19/1000 | Loss: 0.00001312
Iteration 20/1000 | Loss: 0.00001308
Iteration 21/1000 | Loss: 0.00001306
Iteration 22/1000 | Loss: 0.00001305
Iteration 23/1000 | Loss: 0.00001304
Iteration 24/1000 | Loss: 0.00001304
Iteration 25/1000 | Loss: 0.00001289
Iteration 26/1000 | Loss: 0.00001289
Iteration 27/1000 | Loss: 0.00001288
Iteration 28/1000 | Loss: 0.00001280
Iteration 29/1000 | Loss: 0.00001278
Iteration 30/1000 | Loss: 0.00001278
Iteration 31/1000 | Loss: 0.00001277
Iteration 32/1000 | Loss: 0.00001277
Iteration 33/1000 | Loss: 0.00001276
Iteration 34/1000 | Loss: 0.00001276
Iteration 35/1000 | Loss: 0.00001275
Iteration 36/1000 | Loss: 0.00001274
Iteration 37/1000 | Loss: 0.00001274
Iteration 38/1000 | Loss: 0.00001274
Iteration 39/1000 | Loss: 0.00001274
Iteration 40/1000 | Loss: 0.00001274
Iteration 41/1000 | Loss: 0.00001274
Iteration 42/1000 | Loss: 0.00001273
Iteration 43/1000 | Loss: 0.00001269
Iteration 44/1000 | Loss: 0.00001267
Iteration 45/1000 | Loss: 0.00001267
Iteration 46/1000 | Loss: 0.00001266
Iteration 47/1000 | Loss: 0.00001266
Iteration 48/1000 | Loss: 0.00001266
Iteration 49/1000 | Loss: 0.00001266
Iteration 50/1000 | Loss: 0.00001266
Iteration 51/1000 | Loss: 0.00001266
Iteration 52/1000 | Loss: 0.00001266
Iteration 53/1000 | Loss: 0.00001266
Iteration 54/1000 | Loss: 0.00001266
Iteration 55/1000 | Loss: 0.00001266
Iteration 56/1000 | Loss: 0.00001266
Iteration 57/1000 | Loss: 0.00001265
Iteration 58/1000 | Loss: 0.00001265
Iteration 59/1000 | Loss: 0.00001265
Iteration 60/1000 | Loss: 0.00001264
Iteration 61/1000 | Loss: 0.00001263
Iteration 62/1000 | Loss: 0.00001263
Iteration 63/1000 | Loss: 0.00001261
Iteration 64/1000 | Loss: 0.00001261
Iteration 65/1000 | Loss: 0.00001260
Iteration 66/1000 | Loss: 0.00001260
Iteration 67/1000 | Loss: 0.00001260
Iteration 68/1000 | Loss: 0.00001260
Iteration 69/1000 | Loss: 0.00001259
Iteration 70/1000 | Loss: 0.00001259
Iteration 71/1000 | Loss: 0.00001259
Iteration 72/1000 | Loss: 0.00001259
Iteration 73/1000 | Loss: 0.00001259
Iteration 74/1000 | Loss: 0.00001259
Iteration 75/1000 | Loss: 0.00001259
Iteration 76/1000 | Loss: 0.00001259
Iteration 77/1000 | Loss: 0.00001259
Iteration 78/1000 | Loss: 0.00001259
Iteration 79/1000 | Loss: 0.00001258
Iteration 80/1000 | Loss: 0.00001258
Iteration 81/1000 | Loss: 0.00001258
Iteration 82/1000 | Loss: 0.00001257
Iteration 83/1000 | Loss: 0.00001257
Iteration 84/1000 | Loss: 0.00001257
Iteration 85/1000 | Loss: 0.00001257
Iteration 86/1000 | Loss: 0.00001257
Iteration 87/1000 | Loss: 0.00001257
Iteration 88/1000 | Loss: 0.00001257
Iteration 89/1000 | Loss: 0.00001257
Iteration 90/1000 | Loss: 0.00001257
Iteration 91/1000 | Loss: 0.00001257
Iteration 92/1000 | Loss: 0.00001257
Iteration 93/1000 | Loss: 0.00001257
Iteration 94/1000 | Loss: 0.00001257
Iteration 95/1000 | Loss: 0.00001256
Iteration 96/1000 | Loss: 0.00001256
Iteration 97/1000 | Loss: 0.00001256
Iteration 98/1000 | Loss: 0.00001256
Iteration 99/1000 | Loss: 0.00001256
Iteration 100/1000 | Loss: 0.00001256
Iteration 101/1000 | Loss: 0.00001256
Iteration 102/1000 | Loss: 0.00001256
Iteration 103/1000 | Loss: 0.00001256
Iteration 104/1000 | Loss: 0.00001256
Iteration 105/1000 | Loss: 0.00001255
Iteration 106/1000 | Loss: 0.00001255
Iteration 107/1000 | Loss: 0.00001255
Iteration 108/1000 | Loss: 0.00001255
Iteration 109/1000 | Loss: 0.00001255
Iteration 110/1000 | Loss: 0.00001254
Iteration 111/1000 | Loss: 0.00001254
Iteration 112/1000 | Loss: 0.00001254
Iteration 113/1000 | Loss: 0.00001254
Iteration 114/1000 | Loss: 0.00001254
Iteration 115/1000 | Loss: 0.00001254
Iteration 116/1000 | Loss: 0.00001254
Iteration 117/1000 | Loss: 0.00001254
Iteration 118/1000 | Loss: 0.00001254
Iteration 119/1000 | Loss: 0.00001254
Iteration 120/1000 | Loss: 0.00001254
Iteration 121/1000 | Loss: 0.00001254
Iteration 122/1000 | Loss: 0.00001254
Iteration 123/1000 | Loss: 0.00001254
Iteration 124/1000 | Loss: 0.00001253
Iteration 125/1000 | Loss: 0.00001253
Iteration 126/1000 | Loss: 0.00001253
Iteration 127/1000 | Loss: 0.00001253
Iteration 128/1000 | Loss: 0.00001253
Iteration 129/1000 | Loss: 0.00001253
Iteration 130/1000 | Loss: 0.00001253
Iteration 131/1000 | Loss: 0.00001252
Iteration 132/1000 | Loss: 0.00001252
Iteration 133/1000 | Loss: 0.00001252
Iteration 134/1000 | Loss: 0.00001252
Iteration 135/1000 | Loss: 0.00001252
Iteration 136/1000 | Loss: 0.00001251
Iteration 137/1000 | Loss: 0.00001251
Iteration 138/1000 | Loss: 0.00001251
Iteration 139/1000 | Loss: 0.00001250
Iteration 140/1000 | Loss: 0.00001250
Iteration 141/1000 | Loss: 0.00001250
Iteration 142/1000 | Loss: 0.00001250
Iteration 143/1000 | Loss: 0.00001250
Iteration 144/1000 | Loss: 0.00001250
Iteration 145/1000 | Loss: 0.00001250
Iteration 146/1000 | Loss: 0.00001250
Iteration 147/1000 | Loss: 0.00001250
Iteration 148/1000 | Loss: 0.00001250
Iteration 149/1000 | Loss: 0.00001250
Iteration 150/1000 | Loss: 0.00001250
Iteration 151/1000 | Loss: 0.00001250
Iteration 152/1000 | Loss: 0.00001250
Iteration 153/1000 | Loss: 0.00001250
Iteration 154/1000 | Loss: 0.00001250
Iteration 155/1000 | Loss: 0.00001250
Iteration 156/1000 | Loss: 0.00001250
Iteration 157/1000 | Loss: 0.00001250
Iteration 158/1000 | Loss: 0.00001250
Iteration 159/1000 | Loss: 0.00001250
Iteration 160/1000 | Loss: 0.00001250
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 160. Stopping optimization.
Last 5 losses: [1.250067725777626e-05, 1.250067725777626e-05, 1.250067725777626e-05, 1.250067725777626e-05, 1.250067725777626e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.250067725777626e-05

Optimization complete. Final v2v error: 2.9342947006225586 mm

Highest mean error: 4.212180137634277 mm for frame 69

Lowest mean error: 2.414642333984375 mm for frame 101

Saving results

Total time: 42.5378942489624
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_015/1037/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_015/1037.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_015/1037
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00807910
Iteration 2/25 | Loss: 0.00157277
Iteration 3/25 | Loss: 0.00129902
Iteration 4/25 | Loss: 0.00125709
Iteration 5/25 | Loss: 0.00125707
Iteration 6/25 | Loss: 0.00123161
Iteration 7/25 | Loss: 0.00122565
Iteration 8/25 | Loss: 0.00122495
Iteration 9/25 | Loss: 0.00122482
Iteration 10/25 | Loss: 0.00122480
Iteration 11/25 | Loss: 0.00122480
Iteration 12/25 | Loss: 0.00122480
Iteration 13/25 | Loss: 0.00122479
Iteration 14/25 | Loss: 0.00122479
Iteration 15/25 | Loss: 0.00122479
Iteration 16/25 | Loss: 0.00122479
Iteration 17/25 | Loss: 0.00122479
Iteration 18/25 | Loss: 0.00122479
Iteration 19/25 | Loss: 0.00122479
Iteration 20/25 | Loss: 0.00122479
Iteration 21/25 | Loss: 0.00122479
Iteration 22/25 | Loss: 0.00122479
Iteration 23/25 | Loss: 0.00122479
Iteration 24/25 | Loss: 0.00122478
Iteration 25/25 | Loss: 0.00122478

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.31037676
Iteration 2/25 | Loss: 0.00117462
Iteration 3/25 | Loss: 0.00117461
Iteration 4/25 | Loss: 0.00117461
Iteration 5/25 | Loss: 0.00117461
Iteration 6/25 | Loss: 0.00117460
Iteration 7/25 | Loss: 0.00117460
Iteration 8/25 | Loss: 0.00117460
Iteration 9/25 | Loss: 0.00117460
Iteration 10/25 | Loss: 0.00117460
Iteration 11/25 | Loss: 0.00117460
Iteration 12/25 | Loss: 0.00117460
Iteration 13/25 | Loss: 0.00117460
Iteration 14/25 | Loss: 0.00117460
Iteration 15/25 | Loss: 0.00117460
Iteration 16/25 | Loss: 0.00117460
Iteration 17/25 | Loss: 0.00117460
Iteration 18/25 | Loss: 0.00117460
Iteration 19/25 | Loss: 0.00117460
Iteration 20/25 | Loss: 0.00117460
Iteration 21/25 | Loss: 0.00117460
Iteration 22/25 | Loss: 0.00117460
Iteration 23/25 | Loss: 0.00117460
Iteration 24/25 | Loss: 0.00117460
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0011746027739718556, 0.0011746027739718556, 0.0011746027739718556, 0.0011746027739718556, 0.0011746027739718556]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011746027739718556

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00117460
Iteration 2/1000 | Loss: 0.00003054
Iteration 3/1000 | Loss: 0.00001995
Iteration 4/1000 | Loss: 0.00001787
Iteration 5/1000 | Loss: 0.00001693
Iteration 6/1000 | Loss: 0.00001612
Iteration 7/1000 | Loss: 0.00001569
Iteration 8/1000 | Loss: 0.00001539
Iteration 9/1000 | Loss: 0.00001513
Iteration 10/1000 | Loss: 0.00001488
Iteration 11/1000 | Loss: 0.00001466
Iteration 12/1000 | Loss: 0.00001458
Iteration 13/1000 | Loss: 0.00001447
Iteration 14/1000 | Loss: 0.00001444
Iteration 15/1000 | Loss: 0.00001438
Iteration 16/1000 | Loss: 0.00001436
Iteration 17/1000 | Loss: 0.00001435
Iteration 18/1000 | Loss: 0.00001433
Iteration 19/1000 | Loss: 0.00001433
Iteration 20/1000 | Loss: 0.00001433
Iteration 21/1000 | Loss: 0.00001430
Iteration 22/1000 | Loss: 0.00001429
Iteration 23/1000 | Loss: 0.00001428
Iteration 24/1000 | Loss: 0.00001427
Iteration 25/1000 | Loss: 0.00001427
Iteration 26/1000 | Loss: 0.00001426
Iteration 27/1000 | Loss: 0.00001426
Iteration 28/1000 | Loss: 0.00001425
Iteration 29/1000 | Loss: 0.00001424
Iteration 30/1000 | Loss: 0.00001423
Iteration 31/1000 | Loss: 0.00001421
Iteration 32/1000 | Loss: 0.00001421
Iteration 33/1000 | Loss: 0.00001420
Iteration 34/1000 | Loss: 0.00001420
Iteration 35/1000 | Loss: 0.00001419
Iteration 36/1000 | Loss: 0.00001419
Iteration 37/1000 | Loss: 0.00001418
Iteration 38/1000 | Loss: 0.00001417
Iteration 39/1000 | Loss: 0.00001416
Iteration 40/1000 | Loss: 0.00001415
Iteration 41/1000 | Loss: 0.00001414
Iteration 42/1000 | Loss: 0.00001414
Iteration 43/1000 | Loss: 0.00001413
Iteration 44/1000 | Loss: 0.00001413
Iteration 45/1000 | Loss: 0.00001412
Iteration 46/1000 | Loss: 0.00001412
Iteration 47/1000 | Loss: 0.00001412
Iteration 48/1000 | Loss: 0.00001412
Iteration 49/1000 | Loss: 0.00001411
Iteration 50/1000 | Loss: 0.00001410
Iteration 51/1000 | Loss: 0.00001410
Iteration 52/1000 | Loss: 0.00001409
Iteration 53/1000 | Loss: 0.00001409
Iteration 54/1000 | Loss: 0.00001409
Iteration 55/1000 | Loss: 0.00001408
Iteration 56/1000 | Loss: 0.00001408
Iteration 57/1000 | Loss: 0.00001408
Iteration 58/1000 | Loss: 0.00001407
Iteration 59/1000 | Loss: 0.00001406
Iteration 60/1000 | Loss: 0.00001406
Iteration 61/1000 | Loss: 0.00001406
Iteration 62/1000 | Loss: 0.00001405
Iteration 63/1000 | Loss: 0.00001405
Iteration 64/1000 | Loss: 0.00001405
Iteration 65/1000 | Loss: 0.00001404
Iteration 66/1000 | Loss: 0.00001404
Iteration 67/1000 | Loss: 0.00001404
Iteration 68/1000 | Loss: 0.00001403
Iteration 69/1000 | Loss: 0.00001403
Iteration 70/1000 | Loss: 0.00001403
Iteration 71/1000 | Loss: 0.00001403
Iteration 72/1000 | Loss: 0.00001402
Iteration 73/1000 | Loss: 0.00001402
Iteration 74/1000 | Loss: 0.00001402
Iteration 75/1000 | Loss: 0.00001400
Iteration 76/1000 | Loss: 0.00001400
Iteration 77/1000 | Loss: 0.00001400
Iteration 78/1000 | Loss: 0.00001399
Iteration 79/1000 | Loss: 0.00001399
Iteration 80/1000 | Loss: 0.00001399
Iteration 81/1000 | Loss: 0.00001398
Iteration 82/1000 | Loss: 0.00001398
Iteration 83/1000 | Loss: 0.00001398
Iteration 84/1000 | Loss: 0.00001398
Iteration 85/1000 | Loss: 0.00001398
Iteration 86/1000 | Loss: 0.00001397
Iteration 87/1000 | Loss: 0.00001397
Iteration 88/1000 | Loss: 0.00001396
Iteration 89/1000 | Loss: 0.00001396
Iteration 90/1000 | Loss: 0.00001395
Iteration 91/1000 | Loss: 0.00001395
Iteration 92/1000 | Loss: 0.00001394
Iteration 93/1000 | Loss: 0.00001394
Iteration 94/1000 | Loss: 0.00001394
Iteration 95/1000 | Loss: 0.00001394
Iteration 96/1000 | Loss: 0.00001393
Iteration 97/1000 | Loss: 0.00001393
Iteration 98/1000 | Loss: 0.00001393
Iteration 99/1000 | Loss: 0.00001393
Iteration 100/1000 | Loss: 0.00001393
Iteration 101/1000 | Loss: 0.00001392
Iteration 102/1000 | Loss: 0.00001392
Iteration 103/1000 | Loss: 0.00001392
Iteration 104/1000 | Loss: 0.00001391
Iteration 105/1000 | Loss: 0.00001391
Iteration 106/1000 | Loss: 0.00001391
Iteration 107/1000 | Loss: 0.00001391
Iteration 108/1000 | Loss: 0.00001391
Iteration 109/1000 | Loss: 0.00001391
Iteration 110/1000 | Loss: 0.00001390
Iteration 111/1000 | Loss: 0.00001390
Iteration 112/1000 | Loss: 0.00001389
Iteration 113/1000 | Loss: 0.00001389
Iteration 114/1000 | Loss: 0.00001389
Iteration 115/1000 | Loss: 0.00001388
Iteration 116/1000 | Loss: 0.00001388
Iteration 117/1000 | Loss: 0.00001388
Iteration 118/1000 | Loss: 0.00001388
Iteration 119/1000 | Loss: 0.00001387
Iteration 120/1000 | Loss: 0.00001387
Iteration 121/1000 | Loss: 0.00001387
Iteration 122/1000 | Loss: 0.00001387
Iteration 123/1000 | Loss: 0.00001387
Iteration 124/1000 | Loss: 0.00001387
Iteration 125/1000 | Loss: 0.00001387
Iteration 126/1000 | Loss: 0.00001387
Iteration 127/1000 | Loss: 0.00001387
Iteration 128/1000 | Loss: 0.00001387
Iteration 129/1000 | Loss: 0.00001387
Iteration 130/1000 | Loss: 0.00001387
Iteration 131/1000 | Loss: 0.00001387
Iteration 132/1000 | Loss: 0.00001387
Iteration 133/1000 | Loss: 0.00001387
Iteration 134/1000 | Loss: 0.00001387
Iteration 135/1000 | Loss: 0.00001387
Iteration 136/1000 | Loss: 0.00001387
Iteration 137/1000 | Loss: 0.00001387
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 137. Stopping optimization.
Last 5 losses: [1.3868179848941509e-05, 1.3868179848941509e-05, 1.3868179848941509e-05, 1.3868179848941509e-05, 1.3868179848941509e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3868179848941509e-05

Optimization complete. Final v2v error: 3.1587138175964355 mm

Highest mean error: 3.7652106285095215 mm for frame 213

Lowest mean error: 2.657925605773926 mm for frame 239

Saving results

Total time: 52.748613119125366
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_015/1075/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_015/1075.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_015/1075
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00826311
Iteration 2/25 | Loss: 0.00144485
Iteration 3/25 | Loss: 0.00122033
Iteration 4/25 | Loss: 0.00118820
Iteration 5/25 | Loss: 0.00118146
Iteration 6/25 | Loss: 0.00118029
Iteration 7/25 | Loss: 0.00118029
Iteration 8/25 | Loss: 0.00118029
Iteration 9/25 | Loss: 0.00118029
Iteration 10/25 | Loss: 0.00118029
Iteration 11/25 | Loss: 0.00118029
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001180289196781814, 0.001180289196781814, 0.001180289196781814, 0.001180289196781814, 0.001180289196781814]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001180289196781814

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.91824758
Iteration 2/25 | Loss: 0.00084934
Iteration 3/25 | Loss: 0.00084934
Iteration 4/25 | Loss: 0.00084933
Iteration 5/25 | Loss: 0.00084933
Iteration 6/25 | Loss: 0.00084933
Iteration 7/25 | Loss: 0.00084933
Iteration 8/25 | Loss: 0.00084933
Iteration 9/25 | Loss: 0.00084933
Iteration 10/25 | Loss: 0.00084933
Iteration 11/25 | Loss: 0.00084933
Iteration 12/25 | Loss: 0.00084933
Iteration 13/25 | Loss: 0.00084933
Iteration 14/25 | Loss: 0.00084933
Iteration 15/25 | Loss: 0.00084933
Iteration 16/25 | Loss: 0.00084933
Iteration 17/25 | Loss: 0.00084933
Iteration 18/25 | Loss: 0.00084933
Iteration 19/25 | Loss: 0.00084933
Iteration 20/25 | Loss: 0.00084933
Iteration 21/25 | Loss: 0.00084933
Iteration 22/25 | Loss: 0.00084933
Iteration 23/25 | Loss: 0.00084933
Iteration 24/25 | Loss: 0.00084933
Iteration 25/25 | Loss: 0.00084933
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0008493314380757511, 0.0008493314380757511, 0.0008493314380757511, 0.0008493314380757511, 0.0008493314380757511]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008493314380757511

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00084933
Iteration 2/1000 | Loss: 0.00004343
Iteration 3/1000 | Loss: 0.00003252
Iteration 4/1000 | Loss: 0.00002737
Iteration 5/1000 | Loss: 0.00002546
Iteration 6/1000 | Loss: 0.00002424
Iteration 7/1000 | Loss: 0.00002306
Iteration 8/1000 | Loss: 0.00002220
Iteration 9/1000 | Loss: 0.00002178
Iteration 10/1000 | Loss: 0.00002142
Iteration 11/1000 | Loss: 0.00002116
Iteration 12/1000 | Loss: 0.00002094
Iteration 13/1000 | Loss: 0.00002072
Iteration 14/1000 | Loss: 0.00002052
Iteration 15/1000 | Loss: 0.00002040
Iteration 16/1000 | Loss: 0.00002039
Iteration 17/1000 | Loss: 0.00002039
Iteration 18/1000 | Loss: 0.00002039
Iteration 19/1000 | Loss: 0.00002038
Iteration 20/1000 | Loss: 0.00002036
Iteration 21/1000 | Loss: 0.00002035
Iteration 22/1000 | Loss: 0.00002035
Iteration 23/1000 | Loss: 0.00002034
Iteration 24/1000 | Loss: 0.00002034
Iteration 25/1000 | Loss: 0.00002033
Iteration 26/1000 | Loss: 0.00002031
Iteration 27/1000 | Loss: 0.00002027
Iteration 28/1000 | Loss: 0.00002025
Iteration 29/1000 | Loss: 0.00002025
Iteration 30/1000 | Loss: 0.00002024
Iteration 31/1000 | Loss: 0.00002024
Iteration 32/1000 | Loss: 0.00002023
Iteration 33/1000 | Loss: 0.00002022
Iteration 34/1000 | Loss: 0.00002018
Iteration 35/1000 | Loss: 0.00002017
Iteration 36/1000 | Loss: 0.00002017
Iteration 37/1000 | Loss: 0.00002016
Iteration 38/1000 | Loss: 0.00002015
Iteration 39/1000 | Loss: 0.00002015
Iteration 40/1000 | Loss: 0.00002015
Iteration 41/1000 | Loss: 0.00002015
Iteration 42/1000 | Loss: 0.00002015
Iteration 43/1000 | Loss: 0.00002014
Iteration 44/1000 | Loss: 0.00002014
Iteration 45/1000 | Loss: 0.00002014
Iteration 46/1000 | Loss: 0.00002014
Iteration 47/1000 | Loss: 0.00002014
Iteration 48/1000 | Loss: 0.00002014
Iteration 49/1000 | Loss: 0.00002014
Iteration 50/1000 | Loss: 0.00002014
Iteration 51/1000 | Loss: 0.00002014
Iteration 52/1000 | Loss: 0.00002014
Iteration 53/1000 | Loss: 0.00002014
Iteration 54/1000 | Loss: 0.00002014
Iteration 55/1000 | Loss: 0.00002014
Iteration 56/1000 | Loss: 0.00002014
Iteration 57/1000 | Loss: 0.00002014
Iteration 58/1000 | Loss: 0.00002014
Iteration 59/1000 | Loss: 0.00002014
Iteration 60/1000 | Loss: 0.00002014
Iteration 61/1000 | Loss: 0.00002014
Iteration 62/1000 | Loss: 0.00002014
Iteration 63/1000 | Loss: 0.00002014
Iteration 64/1000 | Loss: 0.00002014
Iteration 65/1000 | Loss: 0.00002014
Iteration 66/1000 | Loss: 0.00002014
Iteration 67/1000 | Loss: 0.00002014
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 67. Stopping optimization.
Last 5 losses: [2.0140621927566826e-05, 2.0140621927566826e-05, 2.0140621927566826e-05, 2.0140621927566826e-05, 2.0140621927566826e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0140621927566826e-05

Optimization complete. Final v2v error: 3.945446491241455 mm

Highest mean error: 4.163186073303223 mm for frame 42

Lowest mean error: 3.839293956756592 mm for frame 119

Saving results

Total time: 33.90247368812561
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_015/1051/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_015/1051.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_015/1051
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00421938
Iteration 2/25 | Loss: 0.00127802
Iteration 3/25 | Loss: 0.00120155
Iteration 4/25 | Loss: 0.00118240
Iteration 5/25 | Loss: 0.00117561
Iteration 6/25 | Loss: 0.00117395
Iteration 7/25 | Loss: 0.00117395
Iteration 8/25 | Loss: 0.00117395
Iteration 9/25 | Loss: 0.00117395
Iteration 10/25 | Loss: 0.00117395
Iteration 11/25 | Loss: 0.00117395
Iteration 12/25 | Loss: 0.00117395
Iteration 13/25 | Loss: 0.00117395
Iteration 14/25 | Loss: 0.00117395
Iteration 15/25 | Loss: 0.00117395
Iteration 16/25 | Loss: 0.00117395
Iteration 17/25 | Loss: 0.00117395
Iteration 18/25 | Loss: 0.00117395
Iteration 19/25 | Loss: 0.00117395
Iteration 20/25 | Loss: 0.00117395
Iteration 21/25 | Loss: 0.00117395
Iteration 22/25 | Loss: 0.00117395
Iteration 23/25 | Loss: 0.00117395
Iteration 24/25 | Loss: 0.00117395
Iteration 25/25 | Loss: 0.00117395

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.30312896
Iteration 2/25 | Loss: 0.00130353
Iteration 3/25 | Loss: 0.00130353
Iteration 4/25 | Loss: 0.00130353
Iteration 5/25 | Loss: 0.00130353
Iteration 6/25 | Loss: 0.00130353
Iteration 7/25 | Loss: 0.00130353
Iteration 8/25 | Loss: 0.00130353
Iteration 9/25 | Loss: 0.00130353
Iteration 10/25 | Loss: 0.00130353
Iteration 11/25 | Loss: 0.00130353
Iteration 12/25 | Loss: 0.00130353
Iteration 13/25 | Loss: 0.00130353
Iteration 14/25 | Loss: 0.00130353
Iteration 15/25 | Loss: 0.00130353
Iteration 16/25 | Loss: 0.00130353
Iteration 17/25 | Loss: 0.00130353
Iteration 18/25 | Loss: 0.00130353
Iteration 19/25 | Loss: 0.00130353
Iteration 20/25 | Loss: 0.00130353
Iteration 21/25 | Loss: 0.00130353
Iteration 22/25 | Loss: 0.00130353
Iteration 23/25 | Loss: 0.00130353
Iteration 24/25 | Loss: 0.00130353
Iteration 25/25 | Loss: 0.00130353

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00130353
Iteration 2/1000 | Loss: 0.00002461
Iteration 3/1000 | Loss: 0.00001743
Iteration 4/1000 | Loss: 0.00001506
Iteration 5/1000 | Loss: 0.00001439
Iteration 6/1000 | Loss: 0.00001372
Iteration 7/1000 | Loss: 0.00001320
Iteration 8/1000 | Loss: 0.00001309
Iteration 9/1000 | Loss: 0.00001283
Iteration 10/1000 | Loss: 0.00001254
Iteration 11/1000 | Loss: 0.00001237
Iteration 12/1000 | Loss: 0.00001234
Iteration 13/1000 | Loss: 0.00001218
Iteration 14/1000 | Loss: 0.00001217
Iteration 15/1000 | Loss: 0.00001213
Iteration 16/1000 | Loss: 0.00001212
Iteration 17/1000 | Loss: 0.00001211
Iteration 18/1000 | Loss: 0.00001209
Iteration 19/1000 | Loss: 0.00001208
Iteration 20/1000 | Loss: 0.00001203
Iteration 21/1000 | Loss: 0.00001201
Iteration 22/1000 | Loss: 0.00001199
Iteration 23/1000 | Loss: 0.00001198
Iteration 24/1000 | Loss: 0.00001196
Iteration 25/1000 | Loss: 0.00001196
Iteration 26/1000 | Loss: 0.00001183
Iteration 27/1000 | Loss: 0.00001180
Iteration 28/1000 | Loss: 0.00001175
Iteration 29/1000 | Loss: 0.00001175
Iteration 30/1000 | Loss: 0.00001173
Iteration 31/1000 | Loss: 0.00001173
Iteration 32/1000 | Loss: 0.00001172
Iteration 33/1000 | Loss: 0.00001172
Iteration 34/1000 | Loss: 0.00001171
Iteration 35/1000 | Loss: 0.00001171
Iteration 36/1000 | Loss: 0.00001170
Iteration 37/1000 | Loss: 0.00001169
Iteration 38/1000 | Loss: 0.00001169
Iteration 39/1000 | Loss: 0.00001169
Iteration 40/1000 | Loss: 0.00001169
Iteration 41/1000 | Loss: 0.00001169
Iteration 42/1000 | Loss: 0.00001168
Iteration 43/1000 | Loss: 0.00001168
Iteration 44/1000 | Loss: 0.00001166
Iteration 45/1000 | Loss: 0.00001164
Iteration 46/1000 | Loss: 0.00001163
Iteration 47/1000 | Loss: 0.00001163
Iteration 48/1000 | Loss: 0.00001162
Iteration 49/1000 | Loss: 0.00001159
Iteration 50/1000 | Loss: 0.00001159
Iteration 51/1000 | Loss: 0.00001159
Iteration 52/1000 | Loss: 0.00001158
Iteration 53/1000 | Loss: 0.00001158
Iteration 54/1000 | Loss: 0.00001157
Iteration 55/1000 | Loss: 0.00001157
Iteration 56/1000 | Loss: 0.00001157
Iteration 57/1000 | Loss: 0.00001157
Iteration 58/1000 | Loss: 0.00001157
Iteration 59/1000 | Loss: 0.00001157
Iteration 60/1000 | Loss: 0.00001156
Iteration 61/1000 | Loss: 0.00001155
Iteration 62/1000 | Loss: 0.00001155
Iteration 63/1000 | Loss: 0.00001155
Iteration 64/1000 | Loss: 0.00001154
Iteration 65/1000 | Loss: 0.00001154
Iteration 66/1000 | Loss: 0.00001154
Iteration 67/1000 | Loss: 0.00001153
Iteration 68/1000 | Loss: 0.00001153
Iteration 69/1000 | Loss: 0.00001153
Iteration 70/1000 | Loss: 0.00001153
Iteration 71/1000 | Loss: 0.00001153
Iteration 72/1000 | Loss: 0.00001152
Iteration 73/1000 | Loss: 0.00001152
Iteration 74/1000 | Loss: 0.00001152
Iteration 75/1000 | Loss: 0.00001152
Iteration 76/1000 | Loss: 0.00001152
Iteration 77/1000 | Loss: 0.00001152
Iteration 78/1000 | Loss: 0.00001152
Iteration 79/1000 | Loss: 0.00001151
Iteration 80/1000 | Loss: 0.00001151
Iteration 81/1000 | Loss: 0.00001151
Iteration 82/1000 | Loss: 0.00001151
Iteration 83/1000 | Loss: 0.00001150
Iteration 84/1000 | Loss: 0.00001150
Iteration 85/1000 | Loss: 0.00001149
Iteration 86/1000 | Loss: 0.00001149
Iteration 87/1000 | Loss: 0.00001149
Iteration 88/1000 | Loss: 0.00001149
Iteration 89/1000 | Loss: 0.00001148
Iteration 90/1000 | Loss: 0.00001148
Iteration 91/1000 | Loss: 0.00001148
Iteration 92/1000 | Loss: 0.00001148
Iteration 93/1000 | Loss: 0.00001148
Iteration 94/1000 | Loss: 0.00001147
Iteration 95/1000 | Loss: 0.00001147
Iteration 96/1000 | Loss: 0.00001147
Iteration 97/1000 | Loss: 0.00001147
Iteration 98/1000 | Loss: 0.00001147
Iteration 99/1000 | Loss: 0.00001147
Iteration 100/1000 | Loss: 0.00001147
Iteration 101/1000 | Loss: 0.00001147
Iteration 102/1000 | Loss: 0.00001146
Iteration 103/1000 | Loss: 0.00001146
Iteration 104/1000 | Loss: 0.00001146
Iteration 105/1000 | Loss: 0.00001146
Iteration 106/1000 | Loss: 0.00001146
Iteration 107/1000 | Loss: 0.00001146
Iteration 108/1000 | Loss: 0.00001146
Iteration 109/1000 | Loss: 0.00001146
Iteration 110/1000 | Loss: 0.00001146
Iteration 111/1000 | Loss: 0.00001146
Iteration 112/1000 | Loss: 0.00001146
Iteration 113/1000 | Loss: 0.00001146
Iteration 114/1000 | Loss: 0.00001146
Iteration 115/1000 | Loss: 0.00001145
Iteration 116/1000 | Loss: 0.00001145
Iteration 117/1000 | Loss: 0.00001145
Iteration 118/1000 | Loss: 0.00001145
Iteration 119/1000 | Loss: 0.00001145
Iteration 120/1000 | Loss: 0.00001145
Iteration 121/1000 | Loss: 0.00001145
Iteration 122/1000 | Loss: 0.00001145
Iteration 123/1000 | Loss: 0.00001145
Iteration 124/1000 | Loss: 0.00001145
Iteration 125/1000 | Loss: 0.00001145
Iteration 126/1000 | Loss: 0.00001145
Iteration 127/1000 | Loss: 0.00001145
Iteration 128/1000 | Loss: 0.00001145
Iteration 129/1000 | Loss: 0.00001144
Iteration 130/1000 | Loss: 0.00001144
Iteration 131/1000 | Loss: 0.00001144
Iteration 132/1000 | Loss: 0.00001144
Iteration 133/1000 | Loss: 0.00001144
Iteration 134/1000 | Loss: 0.00001144
Iteration 135/1000 | Loss: 0.00001144
Iteration 136/1000 | Loss: 0.00001143
Iteration 137/1000 | Loss: 0.00001143
Iteration 138/1000 | Loss: 0.00001143
Iteration 139/1000 | Loss: 0.00001143
Iteration 140/1000 | Loss: 0.00001143
Iteration 141/1000 | Loss: 0.00001142
Iteration 142/1000 | Loss: 0.00001142
Iteration 143/1000 | Loss: 0.00001142
Iteration 144/1000 | Loss: 0.00001142
Iteration 145/1000 | Loss: 0.00001142
Iteration 146/1000 | Loss: 0.00001142
Iteration 147/1000 | Loss: 0.00001142
Iteration 148/1000 | Loss: 0.00001142
Iteration 149/1000 | Loss: 0.00001142
Iteration 150/1000 | Loss: 0.00001142
Iteration 151/1000 | Loss: 0.00001142
Iteration 152/1000 | Loss: 0.00001142
Iteration 153/1000 | Loss: 0.00001142
Iteration 154/1000 | Loss: 0.00001142
Iteration 155/1000 | Loss: 0.00001142
Iteration 156/1000 | Loss: 0.00001142
Iteration 157/1000 | Loss: 0.00001142
Iteration 158/1000 | Loss: 0.00001142
Iteration 159/1000 | Loss: 0.00001142
Iteration 160/1000 | Loss: 0.00001142
Iteration 161/1000 | Loss: 0.00001142
Iteration 162/1000 | Loss: 0.00001142
Iteration 163/1000 | Loss: 0.00001142
Iteration 164/1000 | Loss: 0.00001142
Iteration 165/1000 | Loss: 0.00001142
Iteration 166/1000 | Loss: 0.00001142
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 166. Stopping optimization.
Last 5 losses: [1.1422266652516555e-05, 1.1422266652516555e-05, 1.1422266652516555e-05, 1.1422266652516555e-05, 1.1422266652516555e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1422266652516555e-05

Optimization complete. Final v2v error: 2.943082094192505 mm

Highest mean error: 3.177492141723633 mm for frame 104

Lowest mean error: 2.779749870300293 mm for frame 158

Saving results

Total time: 40.5827112197876
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_015/1055/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_015/1055.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_015/1055
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00399912
Iteration 2/25 | Loss: 0.00128872
Iteration 3/25 | Loss: 0.00119628
Iteration 4/25 | Loss: 0.00118513
Iteration 5/25 | Loss: 0.00118153
Iteration 6/25 | Loss: 0.00118092
Iteration 7/25 | Loss: 0.00118092
Iteration 8/25 | Loss: 0.00118092
Iteration 9/25 | Loss: 0.00118092
Iteration 10/25 | Loss: 0.00118092
Iteration 11/25 | Loss: 0.00118092
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011809173738583922, 0.0011809173738583922, 0.0011809173738583922, 0.0011809173738583922, 0.0011809173738583922]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011809173738583922

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.28839934
Iteration 2/25 | Loss: 0.00154206
Iteration 3/25 | Loss: 0.00154204
Iteration 4/25 | Loss: 0.00154204
Iteration 5/25 | Loss: 0.00154204
Iteration 6/25 | Loss: 0.00154204
Iteration 7/25 | Loss: 0.00154204
Iteration 8/25 | Loss: 0.00154204
Iteration 9/25 | Loss: 0.00154204
Iteration 10/25 | Loss: 0.00154204
Iteration 11/25 | Loss: 0.00154204
Iteration 12/25 | Loss: 0.00154204
Iteration 13/25 | Loss: 0.00154204
Iteration 14/25 | Loss: 0.00154204
Iteration 15/25 | Loss: 0.00154204
Iteration 16/25 | Loss: 0.00154204
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0015420381678268313, 0.0015420381678268313, 0.0015420381678268313, 0.0015420381678268313, 0.0015420381678268313]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015420381678268313

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00154204
Iteration 2/1000 | Loss: 0.00004709
Iteration 3/1000 | Loss: 0.00003046
Iteration 4/1000 | Loss: 0.00002436
Iteration 5/1000 | Loss: 0.00002025
Iteration 6/1000 | Loss: 0.00001867
Iteration 7/1000 | Loss: 0.00001726
Iteration 8/1000 | Loss: 0.00001653
Iteration 9/1000 | Loss: 0.00001605
Iteration 10/1000 | Loss: 0.00001556
Iteration 11/1000 | Loss: 0.00001525
Iteration 12/1000 | Loss: 0.00001504
Iteration 13/1000 | Loss: 0.00001478
Iteration 14/1000 | Loss: 0.00001472
Iteration 15/1000 | Loss: 0.00001470
Iteration 16/1000 | Loss: 0.00001468
Iteration 17/1000 | Loss: 0.00001465
Iteration 18/1000 | Loss: 0.00001464
Iteration 19/1000 | Loss: 0.00001464
Iteration 20/1000 | Loss: 0.00001463
Iteration 21/1000 | Loss: 0.00001462
Iteration 22/1000 | Loss: 0.00001461
Iteration 23/1000 | Loss: 0.00001459
Iteration 24/1000 | Loss: 0.00001457
Iteration 25/1000 | Loss: 0.00001456
Iteration 26/1000 | Loss: 0.00001456
Iteration 27/1000 | Loss: 0.00001455
Iteration 28/1000 | Loss: 0.00001455
Iteration 29/1000 | Loss: 0.00001454
Iteration 30/1000 | Loss: 0.00001454
Iteration 31/1000 | Loss: 0.00001454
Iteration 32/1000 | Loss: 0.00001453
Iteration 33/1000 | Loss: 0.00001453
Iteration 34/1000 | Loss: 0.00001449
Iteration 35/1000 | Loss: 0.00001442
Iteration 36/1000 | Loss: 0.00001439
Iteration 37/1000 | Loss: 0.00001438
Iteration 38/1000 | Loss: 0.00001438
Iteration 39/1000 | Loss: 0.00001437
Iteration 40/1000 | Loss: 0.00001437
Iteration 41/1000 | Loss: 0.00001436
Iteration 42/1000 | Loss: 0.00001436
Iteration 43/1000 | Loss: 0.00001435
Iteration 44/1000 | Loss: 0.00001434
Iteration 45/1000 | Loss: 0.00001434
Iteration 46/1000 | Loss: 0.00001430
Iteration 47/1000 | Loss: 0.00001428
Iteration 48/1000 | Loss: 0.00001426
Iteration 49/1000 | Loss: 0.00001426
Iteration 50/1000 | Loss: 0.00001426
Iteration 51/1000 | Loss: 0.00001423
Iteration 52/1000 | Loss: 0.00001422
Iteration 53/1000 | Loss: 0.00001422
Iteration 54/1000 | Loss: 0.00001421
Iteration 55/1000 | Loss: 0.00001421
Iteration 56/1000 | Loss: 0.00001421
Iteration 57/1000 | Loss: 0.00001420
Iteration 58/1000 | Loss: 0.00001420
Iteration 59/1000 | Loss: 0.00001419
Iteration 60/1000 | Loss: 0.00001419
Iteration 61/1000 | Loss: 0.00001418
Iteration 62/1000 | Loss: 0.00001417
Iteration 63/1000 | Loss: 0.00001417
Iteration 64/1000 | Loss: 0.00001416
Iteration 65/1000 | Loss: 0.00001416
Iteration 66/1000 | Loss: 0.00001415
Iteration 67/1000 | Loss: 0.00001415
Iteration 68/1000 | Loss: 0.00001415
Iteration 69/1000 | Loss: 0.00001414
Iteration 70/1000 | Loss: 0.00001414
Iteration 71/1000 | Loss: 0.00001414
Iteration 72/1000 | Loss: 0.00001413
Iteration 73/1000 | Loss: 0.00001413
Iteration 74/1000 | Loss: 0.00001412
Iteration 75/1000 | Loss: 0.00001412
Iteration 76/1000 | Loss: 0.00001411
Iteration 77/1000 | Loss: 0.00001410
Iteration 78/1000 | Loss: 0.00001409
Iteration 79/1000 | Loss: 0.00001409
Iteration 80/1000 | Loss: 0.00001409
Iteration 81/1000 | Loss: 0.00001408
Iteration 82/1000 | Loss: 0.00001408
Iteration 83/1000 | Loss: 0.00001408
Iteration 84/1000 | Loss: 0.00001408
Iteration 85/1000 | Loss: 0.00001407
Iteration 86/1000 | Loss: 0.00001407
Iteration 87/1000 | Loss: 0.00001407
Iteration 88/1000 | Loss: 0.00001407
Iteration 89/1000 | Loss: 0.00001407
Iteration 90/1000 | Loss: 0.00001406
Iteration 91/1000 | Loss: 0.00001406
Iteration 92/1000 | Loss: 0.00001406
Iteration 93/1000 | Loss: 0.00001406
Iteration 94/1000 | Loss: 0.00001406
Iteration 95/1000 | Loss: 0.00001405
Iteration 96/1000 | Loss: 0.00001405
Iteration 97/1000 | Loss: 0.00001404
Iteration 98/1000 | Loss: 0.00001404
Iteration 99/1000 | Loss: 0.00001404
Iteration 100/1000 | Loss: 0.00001403
Iteration 101/1000 | Loss: 0.00001403
Iteration 102/1000 | Loss: 0.00001403
Iteration 103/1000 | Loss: 0.00001402
Iteration 104/1000 | Loss: 0.00001402
Iteration 105/1000 | Loss: 0.00001402
Iteration 106/1000 | Loss: 0.00001402
Iteration 107/1000 | Loss: 0.00001401
Iteration 108/1000 | Loss: 0.00001401
Iteration 109/1000 | Loss: 0.00001400
Iteration 110/1000 | Loss: 0.00001400
Iteration 111/1000 | Loss: 0.00001400
Iteration 112/1000 | Loss: 0.00001399
Iteration 113/1000 | Loss: 0.00001399
Iteration 114/1000 | Loss: 0.00001399
Iteration 115/1000 | Loss: 0.00001399
Iteration 116/1000 | Loss: 0.00001398
Iteration 117/1000 | Loss: 0.00001398
Iteration 118/1000 | Loss: 0.00001398
Iteration 119/1000 | Loss: 0.00001398
Iteration 120/1000 | Loss: 0.00001397
Iteration 121/1000 | Loss: 0.00001397
Iteration 122/1000 | Loss: 0.00001397
Iteration 123/1000 | Loss: 0.00001396
Iteration 124/1000 | Loss: 0.00001396
Iteration 125/1000 | Loss: 0.00001396
Iteration 126/1000 | Loss: 0.00001396
Iteration 127/1000 | Loss: 0.00001396
Iteration 128/1000 | Loss: 0.00001395
Iteration 129/1000 | Loss: 0.00001395
Iteration 130/1000 | Loss: 0.00001395
Iteration 131/1000 | Loss: 0.00001395
Iteration 132/1000 | Loss: 0.00001395
Iteration 133/1000 | Loss: 0.00001395
Iteration 134/1000 | Loss: 0.00001395
Iteration 135/1000 | Loss: 0.00001394
Iteration 136/1000 | Loss: 0.00001394
Iteration 137/1000 | Loss: 0.00001394
Iteration 138/1000 | Loss: 0.00001394
Iteration 139/1000 | Loss: 0.00001394
Iteration 140/1000 | Loss: 0.00001394
Iteration 141/1000 | Loss: 0.00001393
Iteration 142/1000 | Loss: 0.00001393
Iteration 143/1000 | Loss: 0.00001392
Iteration 144/1000 | Loss: 0.00001392
Iteration 145/1000 | Loss: 0.00001392
Iteration 146/1000 | Loss: 0.00001391
Iteration 147/1000 | Loss: 0.00001391
Iteration 148/1000 | Loss: 0.00001390
Iteration 149/1000 | Loss: 0.00001390
Iteration 150/1000 | Loss: 0.00001389
Iteration 151/1000 | Loss: 0.00001389
Iteration 152/1000 | Loss: 0.00001389
Iteration 153/1000 | Loss: 0.00001389
Iteration 154/1000 | Loss: 0.00001389
Iteration 155/1000 | Loss: 0.00001389
Iteration 156/1000 | Loss: 0.00001389
Iteration 157/1000 | Loss: 0.00001388
Iteration 158/1000 | Loss: 0.00001388
Iteration 159/1000 | Loss: 0.00001388
Iteration 160/1000 | Loss: 0.00001388
Iteration 161/1000 | Loss: 0.00001388
Iteration 162/1000 | Loss: 0.00001388
Iteration 163/1000 | Loss: 0.00001387
Iteration 164/1000 | Loss: 0.00001387
Iteration 165/1000 | Loss: 0.00001387
Iteration 166/1000 | Loss: 0.00001387
Iteration 167/1000 | Loss: 0.00001386
Iteration 168/1000 | Loss: 0.00001386
Iteration 169/1000 | Loss: 0.00001386
Iteration 170/1000 | Loss: 0.00001386
Iteration 171/1000 | Loss: 0.00001386
Iteration 172/1000 | Loss: 0.00001386
Iteration 173/1000 | Loss: 0.00001386
Iteration 174/1000 | Loss: 0.00001386
Iteration 175/1000 | Loss: 0.00001385
Iteration 176/1000 | Loss: 0.00001385
Iteration 177/1000 | Loss: 0.00001385
Iteration 178/1000 | Loss: 0.00001385
Iteration 179/1000 | Loss: 0.00001385
Iteration 180/1000 | Loss: 0.00001385
Iteration 181/1000 | Loss: 0.00001385
Iteration 182/1000 | Loss: 0.00001385
Iteration 183/1000 | Loss: 0.00001384
Iteration 184/1000 | Loss: 0.00001384
Iteration 185/1000 | Loss: 0.00001384
Iteration 186/1000 | Loss: 0.00001384
Iteration 187/1000 | Loss: 0.00001384
Iteration 188/1000 | Loss: 0.00001384
Iteration 189/1000 | Loss: 0.00001383
Iteration 190/1000 | Loss: 0.00001383
Iteration 191/1000 | Loss: 0.00001383
Iteration 192/1000 | Loss: 0.00001383
Iteration 193/1000 | Loss: 0.00001383
Iteration 194/1000 | Loss: 0.00001383
Iteration 195/1000 | Loss: 0.00001383
Iteration 196/1000 | Loss: 0.00001383
Iteration 197/1000 | Loss: 0.00001382
Iteration 198/1000 | Loss: 0.00001382
Iteration 199/1000 | Loss: 0.00001382
Iteration 200/1000 | Loss: 0.00001382
Iteration 201/1000 | Loss: 0.00001382
Iteration 202/1000 | Loss: 0.00001382
Iteration 203/1000 | Loss: 0.00001382
Iteration 204/1000 | Loss: 0.00001382
Iteration 205/1000 | Loss: 0.00001382
Iteration 206/1000 | Loss: 0.00001382
Iteration 207/1000 | Loss: 0.00001382
Iteration 208/1000 | Loss: 0.00001382
Iteration 209/1000 | Loss: 0.00001382
Iteration 210/1000 | Loss: 0.00001382
Iteration 211/1000 | Loss: 0.00001382
Iteration 212/1000 | Loss: 0.00001382
Iteration 213/1000 | Loss: 0.00001382
Iteration 214/1000 | Loss: 0.00001382
Iteration 215/1000 | Loss: 0.00001382
Iteration 216/1000 | Loss: 0.00001381
Iteration 217/1000 | Loss: 0.00001381
Iteration 218/1000 | Loss: 0.00001381
Iteration 219/1000 | Loss: 0.00001381
Iteration 220/1000 | Loss: 0.00001381
Iteration 221/1000 | Loss: 0.00001381
Iteration 222/1000 | Loss: 0.00001381
Iteration 223/1000 | Loss: 0.00001381
Iteration 224/1000 | Loss: 0.00001381
Iteration 225/1000 | Loss: 0.00001381
Iteration 226/1000 | Loss: 0.00001381
Iteration 227/1000 | Loss: 0.00001381
Iteration 228/1000 | Loss: 0.00001381
Iteration 229/1000 | Loss: 0.00001381
Iteration 230/1000 | Loss: 0.00001380
Iteration 231/1000 | Loss: 0.00001380
Iteration 232/1000 | Loss: 0.00001380
Iteration 233/1000 | Loss: 0.00001380
Iteration 234/1000 | Loss: 0.00001380
Iteration 235/1000 | Loss: 0.00001380
Iteration 236/1000 | Loss: 0.00001380
Iteration 237/1000 | Loss: 0.00001380
Iteration 238/1000 | Loss: 0.00001380
Iteration 239/1000 | Loss: 0.00001380
Iteration 240/1000 | Loss: 0.00001380
Iteration 241/1000 | Loss: 0.00001380
Iteration 242/1000 | Loss: 0.00001380
Iteration 243/1000 | Loss: 0.00001380
Iteration 244/1000 | Loss: 0.00001380
Iteration 245/1000 | Loss: 0.00001380
Iteration 246/1000 | Loss: 0.00001380
Iteration 247/1000 | Loss: 0.00001380
Iteration 248/1000 | Loss: 0.00001380
Iteration 249/1000 | Loss: 0.00001380
Iteration 250/1000 | Loss: 0.00001380
Iteration 251/1000 | Loss: 0.00001379
Iteration 252/1000 | Loss: 0.00001379
Iteration 253/1000 | Loss: 0.00001379
Iteration 254/1000 | Loss: 0.00001379
Iteration 255/1000 | Loss: 0.00001379
Iteration 256/1000 | Loss: 0.00001379
Iteration 257/1000 | Loss: 0.00001379
Iteration 258/1000 | Loss: 0.00001379
Iteration 259/1000 | Loss: 0.00001379
Iteration 260/1000 | Loss: 0.00001379
Iteration 261/1000 | Loss: 0.00001379
Iteration 262/1000 | Loss: 0.00001379
Iteration 263/1000 | Loss: 0.00001379
Iteration 264/1000 | Loss: 0.00001379
Iteration 265/1000 | Loss: 0.00001379
Iteration 266/1000 | Loss: 0.00001379
Iteration 267/1000 | Loss: 0.00001379
Iteration 268/1000 | Loss: 0.00001379
Iteration 269/1000 | Loss: 0.00001379
Iteration 270/1000 | Loss: 0.00001378
Iteration 271/1000 | Loss: 0.00001378
Iteration 272/1000 | Loss: 0.00001378
Iteration 273/1000 | Loss: 0.00001378
Iteration 274/1000 | Loss: 0.00001378
Iteration 275/1000 | Loss: 0.00001378
Iteration 276/1000 | Loss: 0.00001378
Iteration 277/1000 | Loss: 0.00001378
Iteration 278/1000 | Loss: 0.00001378
Iteration 279/1000 | Loss: 0.00001378
Iteration 280/1000 | Loss: 0.00001378
Iteration 281/1000 | Loss: 0.00001377
Iteration 282/1000 | Loss: 0.00001377
Iteration 283/1000 | Loss: 0.00001377
Iteration 284/1000 | Loss: 0.00001377
Iteration 285/1000 | Loss: 0.00001377
Iteration 286/1000 | Loss: 0.00001377
Iteration 287/1000 | Loss: 0.00001377
Iteration 288/1000 | Loss: 0.00001377
Iteration 289/1000 | Loss: 0.00001377
Iteration 290/1000 | Loss: 0.00001377
Iteration 291/1000 | Loss: 0.00001377
Iteration 292/1000 | Loss: 0.00001377
Iteration 293/1000 | Loss: 0.00001377
Iteration 294/1000 | Loss: 0.00001376
Iteration 295/1000 | Loss: 0.00001376
Iteration 296/1000 | Loss: 0.00001376
Iteration 297/1000 | Loss: 0.00001376
Iteration 298/1000 | Loss: 0.00001376
Iteration 299/1000 | Loss: 0.00001376
Iteration 300/1000 | Loss: 0.00001376
Iteration 301/1000 | Loss: 0.00001376
Iteration 302/1000 | Loss: 0.00001376
Iteration 303/1000 | Loss: 0.00001376
Iteration 304/1000 | Loss: 0.00001376
Iteration 305/1000 | Loss: 0.00001376
Iteration 306/1000 | Loss: 0.00001375
Iteration 307/1000 | Loss: 0.00001375
Iteration 308/1000 | Loss: 0.00001375
Iteration 309/1000 | Loss: 0.00001375
Iteration 310/1000 | Loss: 0.00001374
Iteration 311/1000 | Loss: 0.00001374
Iteration 312/1000 | Loss: 0.00001374
Iteration 313/1000 | Loss: 0.00001374
Iteration 314/1000 | Loss: 0.00001374
Iteration 315/1000 | Loss: 0.00001374
Iteration 316/1000 | Loss: 0.00001374
Iteration 317/1000 | Loss: 0.00001374
Iteration 318/1000 | Loss: 0.00001374
Iteration 319/1000 | Loss: 0.00001374
Iteration 320/1000 | Loss: 0.00001374
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 320. Stopping optimization.
Last 5 losses: [1.37437236844562e-05, 1.37437236844562e-05, 1.37437236844562e-05, 1.37437236844562e-05, 1.37437236844562e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.37437236844562e-05

Optimization complete. Final v2v error: 3.014143228530884 mm

Highest mean error: 5.037373065948486 mm for frame 87

Lowest mean error: 2.4362893104553223 mm for frame 24

Saving results

Total time: 53.495346784591675
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_015/1043/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_015/1043.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_015/1043
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00940886
Iteration 2/25 | Loss: 0.00316438
Iteration 3/25 | Loss: 0.00219851
Iteration 4/25 | Loss: 0.00174708
Iteration 5/25 | Loss: 0.00171896
Iteration 6/25 | Loss: 0.00169900
Iteration 7/25 | Loss: 0.00167348
Iteration 8/25 | Loss: 0.00162526
Iteration 9/25 | Loss: 0.00159530
Iteration 10/25 | Loss: 0.00157803
Iteration 11/25 | Loss: 0.00156551
Iteration 12/25 | Loss: 0.00155972
Iteration 13/25 | Loss: 0.00157204
Iteration 14/25 | Loss: 0.00154756
Iteration 15/25 | Loss: 0.00155626
Iteration 16/25 | Loss: 0.00154956
Iteration 17/25 | Loss: 0.00154144
Iteration 18/25 | Loss: 0.00154396
Iteration 19/25 | Loss: 0.00153605
Iteration 20/25 | Loss: 0.00153779
Iteration 21/25 | Loss: 0.00153271
Iteration 22/25 | Loss: 0.00153006
Iteration 23/25 | Loss: 0.00152919
Iteration 24/25 | Loss: 0.00152863
Iteration 25/25 | Loss: 0.00152850

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.24954987
Iteration 2/25 | Loss: 0.00483862
Iteration 3/25 | Loss: 0.00439804
Iteration 4/25 | Loss: 0.00439801
Iteration 5/25 | Loss: 0.00439801
Iteration 6/25 | Loss: 0.00439801
Iteration 7/25 | Loss: 0.00439801
Iteration 8/25 | Loss: 0.00439801
Iteration 9/25 | Loss: 0.00439801
Iteration 10/25 | Loss: 0.00439801
Iteration 11/25 | Loss: 0.00439801
Iteration 12/25 | Loss: 0.00439801
Iteration 13/25 | Loss: 0.00439801
Iteration 14/25 | Loss: 0.00439801
Iteration 15/25 | Loss: 0.00439801
Iteration 16/25 | Loss: 0.00439801
Iteration 17/25 | Loss: 0.00439801
Iteration 18/25 | Loss: 0.00439801
Iteration 19/25 | Loss: 0.00439801
Iteration 20/25 | Loss: 0.00439800
Iteration 21/25 | Loss: 0.00439800
Iteration 22/25 | Loss: 0.00439800
Iteration 23/25 | Loss: 0.00439800
Iteration 24/25 | Loss: 0.00439800
Iteration 25/25 | Loss: 0.00439800

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00439800
Iteration 2/1000 | Loss: 0.00155464
Iteration 3/1000 | Loss: 0.00069837
Iteration 4/1000 | Loss: 0.00028736
Iteration 5/1000 | Loss: 0.00028490
Iteration 6/1000 | Loss: 0.00082092
Iteration 7/1000 | Loss: 0.00061471
Iteration 8/1000 | Loss: 0.00031601
Iteration 9/1000 | Loss: 0.00022634
Iteration 10/1000 | Loss: 0.00072320
Iteration 11/1000 | Loss: 0.00022722
Iteration 12/1000 | Loss: 0.00014950
Iteration 13/1000 | Loss: 0.00027504
Iteration 14/1000 | Loss: 0.00128852
Iteration 15/1000 | Loss: 0.00031003
Iteration 16/1000 | Loss: 0.00097130
Iteration 17/1000 | Loss: 0.00209411
Iteration 18/1000 | Loss: 0.00232673
Iteration 19/1000 | Loss: 0.00220284
Iteration 20/1000 | Loss: 0.00300180
Iteration 21/1000 | Loss: 0.00401580
Iteration 22/1000 | Loss: 0.00286711
Iteration 23/1000 | Loss: 0.00174093
Iteration 24/1000 | Loss: 0.00183631
Iteration 25/1000 | Loss: 0.00113538
Iteration 26/1000 | Loss: 0.00034198
Iteration 27/1000 | Loss: 0.00088519
Iteration 28/1000 | Loss: 0.00020325
Iteration 29/1000 | Loss: 0.00035291
Iteration 30/1000 | Loss: 0.00065398
Iteration 31/1000 | Loss: 0.00020768
Iteration 32/1000 | Loss: 0.00039851
Iteration 33/1000 | Loss: 0.00052404
Iteration 34/1000 | Loss: 0.00013125
Iteration 35/1000 | Loss: 0.00011441
Iteration 36/1000 | Loss: 0.00011683
Iteration 37/1000 | Loss: 0.00011293
Iteration 38/1000 | Loss: 0.00015257
Iteration 39/1000 | Loss: 0.00051221
Iteration 40/1000 | Loss: 0.00032243
Iteration 41/1000 | Loss: 0.00017543
Iteration 42/1000 | Loss: 0.00010985
Iteration 43/1000 | Loss: 0.00010660
Iteration 44/1000 | Loss: 0.00021791
Iteration 45/1000 | Loss: 0.00044239
Iteration 46/1000 | Loss: 0.00015150
Iteration 47/1000 | Loss: 0.00018997
Iteration 48/1000 | Loss: 0.00021598
Iteration 49/1000 | Loss: 0.00011440
Iteration 50/1000 | Loss: 0.00009367
Iteration 51/1000 | Loss: 0.00034678
Iteration 52/1000 | Loss: 0.00017507
Iteration 53/1000 | Loss: 0.00010013
Iteration 54/1000 | Loss: 0.00022008
Iteration 55/1000 | Loss: 0.00010073
Iteration 56/1000 | Loss: 0.00010301
Iteration 57/1000 | Loss: 0.00009432
Iteration 58/1000 | Loss: 0.00009714
Iteration 59/1000 | Loss: 0.00015556
Iteration 60/1000 | Loss: 0.00010459
Iteration 61/1000 | Loss: 0.00040439
Iteration 62/1000 | Loss: 0.00048101
Iteration 63/1000 | Loss: 0.00030385
Iteration 64/1000 | Loss: 0.00011052
Iteration 65/1000 | Loss: 0.00034305
Iteration 66/1000 | Loss: 0.00010133
Iteration 67/1000 | Loss: 0.00009060
Iteration 68/1000 | Loss: 0.00011392
Iteration 69/1000 | Loss: 0.00013488
Iteration 70/1000 | Loss: 0.00009530
Iteration 71/1000 | Loss: 0.00009148
Iteration 72/1000 | Loss: 0.00009800
Iteration 73/1000 | Loss: 0.00010090
Iteration 74/1000 | Loss: 0.00009980
Iteration 75/1000 | Loss: 0.00009561
Iteration 76/1000 | Loss: 0.00009015
Iteration 77/1000 | Loss: 0.00052424
Iteration 78/1000 | Loss: 0.00009495
Iteration 79/1000 | Loss: 0.00009825
Iteration 80/1000 | Loss: 0.00009902
Iteration 81/1000 | Loss: 0.00009866
Iteration 82/1000 | Loss: 0.00099532
Iteration 83/1000 | Loss: 0.00009459
Iteration 84/1000 | Loss: 0.00009495
Iteration 85/1000 | Loss: 0.00008858
Iteration 86/1000 | Loss: 0.00008798
Iteration 87/1000 | Loss: 0.00143568
Iteration 88/1000 | Loss: 0.00010249
Iteration 89/1000 | Loss: 0.00008417
Iteration 90/1000 | Loss: 0.00052623
Iteration 91/1000 | Loss: 0.00097911
Iteration 92/1000 | Loss: 0.00055105
Iteration 93/1000 | Loss: 0.00104467
Iteration 94/1000 | Loss: 0.00067080
Iteration 95/1000 | Loss: 0.00015597
Iteration 96/1000 | Loss: 0.00008352
Iteration 97/1000 | Loss: 0.00007842
Iteration 98/1000 | Loss: 0.00008239
Iteration 99/1000 | Loss: 0.00050764
Iteration 100/1000 | Loss: 0.00009719
Iteration 101/1000 | Loss: 0.00007186
Iteration 102/1000 | Loss: 0.00006739
Iteration 103/1000 | Loss: 0.00007443
Iteration 104/1000 | Loss: 0.00085450
Iteration 105/1000 | Loss: 0.00007855
Iteration 106/1000 | Loss: 0.00050622
Iteration 107/1000 | Loss: 0.00007259
Iteration 108/1000 | Loss: 0.00049907
Iteration 109/1000 | Loss: 0.00050735
Iteration 110/1000 | Loss: 0.00006700
Iteration 111/1000 | Loss: 0.00006302
Iteration 112/1000 | Loss: 0.00063481
Iteration 113/1000 | Loss: 0.00031308
Iteration 114/1000 | Loss: 0.00050655
Iteration 115/1000 | Loss: 0.00050424
Iteration 116/1000 | Loss: 0.00006557
Iteration 117/1000 | Loss: 0.00005423
Iteration 118/1000 | Loss: 0.00005858
Iteration 119/1000 | Loss: 0.00005409
Iteration 120/1000 | Loss: 0.00005093
Iteration 121/1000 | Loss: 0.00005655
Iteration 122/1000 | Loss: 0.00050747
Iteration 123/1000 | Loss: 0.00006184
Iteration 124/1000 | Loss: 0.00005607
Iteration 125/1000 | Loss: 0.00005417
Iteration 126/1000 | Loss: 0.00036978
Iteration 127/1000 | Loss: 0.00049671
Iteration 128/1000 | Loss: 0.00005051
Iteration 129/1000 | Loss: 0.00004149
Iteration 130/1000 | Loss: 0.00004963
Iteration 131/1000 | Loss: 0.00004564
Iteration 132/1000 | Loss: 0.00003797
Iteration 133/1000 | Loss: 0.00004790
Iteration 134/1000 | Loss: 0.00004784
Iteration 135/1000 | Loss: 0.00047777
Iteration 136/1000 | Loss: 0.00036345
Iteration 137/1000 | Loss: 0.00036510
Iteration 138/1000 | Loss: 0.00022958
Iteration 139/1000 | Loss: 0.00009222
Iteration 140/1000 | Loss: 0.00004957
Iteration 141/1000 | Loss: 0.00004098
Iteration 142/1000 | Loss: 0.00003713
Iteration 143/1000 | Loss: 0.00044365
Iteration 144/1000 | Loss: 0.00003676
Iteration 145/1000 | Loss: 0.00003186
Iteration 146/1000 | Loss: 0.00003059
Iteration 147/1000 | Loss: 0.00002971
Iteration 148/1000 | Loss: 0.00002898
Iteration 149/1000 | Loss: 0.00084151
Iteration 150/1000 | Loss: 0.00037357
Iteration 151/1000 | Loss: 0.00003122
Iteration 152/1000 | Loss: 0.00002808
Iteration 153/1000 | Loss: 0.00002557
Iteration 154/1000 | Loss: 0.00002384
Iteration 155/1000 | Loss: 0.00002290
Iteration 156/1000 | Loss: 0.00002236
Iteration 157/1000 | Loss: 0.00002201
Iteration 158/1000 | Loss: 0.00002175
Iteration 159/1000 | Loss: 0.00002152
Iteration 160/1000 | Loss: 0.00002129
Iteration 161/1000 | Loss: 0.00002112
Iteration 162/1000 | Loss: 0.00002110
Iteration 163/1000 | Loss: 0.00002099
Iteration 164/1000 | Loss: 0.00002098
Iteration 165/1000 | Loss: 0.00002096
Iteration 166/1000 | Loss: 0.00002096
Iteration 167/1000 | Loss: 0.00002096
Iteration 168/1000 | Loss: 0.00002096
Iteration 169/1000 | Loss: 0.00002096
Iteration 170/1000 | Loss: 0.00002096
Iteration 171/1000 | Loss: 0.00002096
Iteration 172/1000 | Loss: 0.00002096
Iteration 173/1000 | Loss: 0.00002096
Iteration 174/1000 | Loss: 0.00002096
Iteration 175/1000 | Loss: 0.00002096
Iteration 176/1000 | Loss: 0.00002096
Iteration 177/1000 | Loss: 0.00002095
Iteration 178/1000 | Loss: 0.00002095
Iteration 179/1000 | Loss: 0.00002094
Iteration 180/1000 | Loss: 0.00002094
Iteration 181/1000 | Loss: 0.00002094
Iteration 182/1000 | Loss: 0.00002093
Iteration 183/1000 | Loss: 0.00002093
Iteration 184/1000 | Loss: 0.00002092
Iteration 185/1000 | Loss: 0.00002092
Iteration 186/1000 | Loss: 0.00002091
Iteration 187/1000 | Loss: 0.00002089
Iteration 188/1000 | Loss: 0.00002088
Iteration 189/1000 | Loss: 0.00002088
Iteration 190/1000 | Loss: 0.00002088
Iteration 191/1000 | Loss: 0.00002087
Iteration 192/1000 | Loss: 0.00037908
Iteration 193/1000 | Loss: 0.00002215
Iteration 194/1000 | Loss: 0.00002085
Iteration 195/1000 | Loss: 0.00002017
Iteration 196/1000 | Loss: 0.00001949
Iteration 197/1000 | Loss: 0.00001902
Iteration 198/1000 | Loss: 0.00001883
Iteration 199/1000 | Loss: 0.00001878
Iteration 200/1000 | Loss: 0.00001872
Iteration 201/1000 | Loss: 0.00001872
Iteration 202/1000 | Loss: 0.00001872
Iteration 203/1000 | Loss: 0.00001871
Iteration 204/1000 | Loss: 0.00001870
Iteration 205/1000 | Loss: 0.00001870
Iteration 206/1000 | Loss: 0.00001869
Iteration 207/1000 | Loss: 0.00001869
Iteration 208/1000 | Loss: 0.00001869
Iteration 209/1000 | Loss: 0.00001868
Iteration 210/1000 | Loss: 0.00001865
Iteration 211/1000 | Loss: 0.00001864
Iteration 212/1000 | Loss: 0.00001863
Iteration 213/1000 | Loss: 0.00001863
Iteration 214/1000 | Loss: 0.00001860
Iteration 215/1000 | Loss: 0.00001859
Iteration 216/1000 | Loss: 0.00001858
Iteration 217/1000 | Loss: 0.00001858
Iteration 218/1000 | Loss: 0.00001857
Iteration 219/1000 | Loss: 0.00001857
Iteration 220/1000 | Loss: 0.00001857
Iteration 221/1000 | Loss: 0.00001857
Iteration 222/1000 | Loss: 0.00001857
Iteration 223/1000 | Loss: 0.00001857
Iteration 224/1000 | Loss: 0.00001857
Iteration 225/1000 | Loss: 0.00001857
Iteration 226/1000 | Loss: 0.00001857
Iteration 227/1000 | Loss: 0.00001857
Iteration 228/1000 | Loss: 0.00001857
Iteration 229/1000 | Loss: 0.00001856
Iteration 230/1000 | Loss: 0.00001856
Iteration 231/1000 | Loss: 0.00001856
Iteration 232/1000 | Loss: 0.00001856
Iteration 233/1000 | Loss: 0.00001856
Iteration 234/1000 | Loss: 0.00001855
Iteration 235/1000 | Loss: 0.00001855
Iteration 236/1000 | Loss: 0.00001855
Iteration 237/1000 | Loss: 0.00001855
Iteration 238/1000 | Loss: 0.00001855
Iteration 239/1000 | Loss: 0.00001855
Iteration 240/1000 | Loss: 0.00001855
Iteration 241/1000 | Loss: 0.00001854
Iteration 242/1000 | Loss: 0.00001854
Iteration 243/1000 | Loss: 0.00001854
Iteration 244/1000 | Loss: 0.00001854
Iteration 245/1000 | Loss: 0.00001854
Iteration 246/1000 | Loss: 0.00001854
Iteration 247/1000 | Loss: 0.00001853
Iteration 248/1000 | Loss: 0.00001853
Iteration 249/1000 | Loss: 0.00001853
Iteration 250/1000 | Loss: 0.00001853
Iteration 251/1000 | Loss: 0.00001853
Iteration 252/1000 | Loss: 0.00001853
Iteration 253/1000 | Loss: 0.00001853
Iteration 254/1000 | Loss: 0.00001852
Iteration 255/1000 | Loss: 0.00001852
Iteration 256/1000 | Loss: 0.00001851
Iteration 257/1000 | Loss: 0.00001851
Iteration 258/1000 | Loss: 0.00001851
Iteration 259/1000 | Loss: 0.00001851
Iteration 260/1000 | Loss: 0.00001851
Iteration 261/1000 | Loss: 0.00001851
Iteration 262/1000 | Loss: 0.00001850
Iteration 263/1000 | Loss: 0.00001850
Iteration 264/1000 | Loss: 0.00001850
Iteration 265/1000 | Loss: 0.00001849
Iteration 266/1000 | Loss: 0.00001849
Iteration 267/1000 | Loss: 0.00001848
Iteration 268/1000 | Loss: 0.00001848
Iteration 269/1000 | Loss: 0.00001848
Iteration 270/1000 | Loss: 0.00001848
Iteration 271/1000 | Loss: 0.00001848
Iteration 272/1000 | Loss: 0.00001847
Iteration 273/1000 | Loss: 0.00001847
Iteration 274/1000 | Loss: 0.00001847
Iteration 275/1000 | Loss: 0.00001847
Iteration 276/1000 | Loss: 0.00001847
Iteration 277/1000 | Loss: 0.00001846
Iteration 278/1000 | Loss: 0.00001846
Iteration 279/1000 | Loss: 0.00001846
Iteration 280/1000 | Loss: 0.00001846
Iteration 281/1000 | Loss: 0.00001846
Iteration 282/1000 | Loss: 0.00001846
Iteration 283/1000 | Loss: 0.00001846
Iteration 284/1000 | Loss: 0.00001846
Iteration 285/1000 | Loss: 0.00001846
Iteration 286/1000 | Loss: 0.00001846
Iteration 287/1000 | Loss: 0.00001846
Iteration 288/1000 | Loss: 0.00001846
Iteration 289/1000 | Loss: 0.00001846
Iteration 290/1000 | Loss: 0.00001846
Iteration 291/1000 | Loss: 0.00001846
Iteration 292/1000 | Loss: 0.00001846
Iteration 293/1000 | Loss: 0.00001846
Iteration 294/1000 | Loss: 0.00001846
Iteration 295/1000 | Loss: 0.00001846
Iteration 296/1000 | Loss: 0.00001846
Iteration 297/1000 | Loss: 0.00001846
Iteration 298/1000 | Loss: 0.00001845
Iteration 299/1000 | Loss: 0.00001845
Iteration 300/1000 | Loss: 0.00001845
Iteration 301/1000 | Loss: 0.00001845
Iteration 302/1000 | Loss: 0.00001845
Iteration 303/1000 | Loss: 0.00001845
Iteration 304/1000 | Loss: 0.00001845
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 304. Stopping optimization.
Last 5 losses: [1.8454089513397776e-05, 1.8454089513397776e-05, 1.8454089513397776e-05, 1.8454089513397776e-05, 1.8454089513397776e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8454089513397776e-05

Optimization complete. Final v2v error: 3.4369633197784424 mm

Highest mean error: 5.110805034637451 mm for frame 142

Lowest mean error: 2.592073440551758 mm for frame 152

Saving results

Total time: 316.34690594673157
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_015/1085/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_015/1085.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_015/1085
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00805417
Iteration 2/25 | Loss: 0.00124933
Iteration 3/25 | Loss: 0.00116901
Iteration 4/25 | Loss: 0.00116057
Iteration 5/25 | Loss: 0.00115888
Iteration 6/25 | Loss: 0.00115888
Iteration 7/25 | Loss: 0.00115888
Iteration 8/25 | Loss: 0.00115888
Iteration 9/25 | Loss: 0.00115888
Iteration 10/25 | Loss: 0.00115888
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0011588793713599443, 0.0011588793713599443, 0.0011588793713599443, 0.0011588793713599443, 0.0011588793713599443]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011588793713599443

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.29788947
Iteration 2/25 | Loss: 0.00129128
Iteration 3/25 | Loss: 0.00129128
Iteration 4/25 | Loss: 0.00129127
Iteration 5/25 | Loss: 0.00129127
Iteration 6/25 | Loss: 0.00129127
Iteration 7/25 | Loss: 0.00129127
Iteration 8/25 | Loss: 0.00129127
Iteration 9/25 | Loss: 0.00129127
Iteration 10/25 | Loss: 0.00129127
Iteration 11/25 | Loss: 0.00129127
Iteration 12/25 | Loss: 0.00129127
Iteration 13/25 | Loss: 0.00129127
Iteration 14/25 | Loss: 0.00129127
Iteration 15/25 | Loss: 0.00129127
Iteration 16/25 | Loss: 0.00129127
Iteration 17/25 | Loss: 0.00129127
Iteration 18/25 | Loss: 0.00129127
Iteration 19/25 | Loss: 0.00129127
Iteration 20/25 | Loss: 0.00129127
Iteration 21/25 | Loss: 0.00129127
Iteration 22/25 | Loss: 0.00129127
Iteration 23/25 | Loss: 0.00129127
Iteration 24/25 | Loss: 0.00129127
Iteration 25/25 | Loss: 0.00129127

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00129127
Iteration 2/1000 | Loss: 0.00001952
Iteration 3/1000 | Loss: 0.00001395
Iteration 4/1000 | Loss: 0.00001213
Iteration 5/1000 | Loss: 0.00001133
Iteration 6/1000 | Loss: 0.00001079
Iteration 7/1000 | Loss: 0.00001030
Iteration 8/1000 | Loss: 0.00001004
Iteration 9/1000 | Loss: 0.00000998
Iteration 10/1000 | Loss: 0.00000971
Iteration 11/1000 | Loss: 0.00000954
Iteration 12/1000 | Loss: 0.00000954
Iteration 13/1000 | Loss: 0.00000947
Iteration 14/1000 | Loss: 0.00000946
Iteration 15/1000 | Loss: 0.00000940
Iteration 16/1000 | Loss: 0.00000937
Iteration 17/1000 | Loss: 0.00000937
Iteration 18/1000 | Loss: 0.00000937
Iteration 19/1000 | Loss: 0.00000934
Iteration 20/1000 | Loss: 0.00000933
Iteration 21/1000 | Loss: 0.00000932
Iteration 22/1000 | Loss: 0.00000931
Iteration 23/1000 | Loss: 0.00000931
Iteration 24/1000 | Loss: 0.00000931
Iteration 25/1000 | Loss: 0.00000925
Iteration 26/1000 | Loss: 0.00000925
Iteration 27/1000 | Loss: 0.00000924
Iteration 28/1000 | Loss: 0.00000924
Iteration 29/1000 | Loss: 0.00000924
Iteration 30/1000 | Loss: 0.00000924
Iteration 31/1000 | Loss: 0.00000924
Iteration 32/1000 | Loss: 0.00000924
Iteration 33/1000 | Loss: 0.00000924
Iteration 34/1000 | Loss: 0.00000923
Iteration 35/1000 | Loss: 0.00000919
Iteration 36/1000 | Loss: 0.00000919
Iteration 37/1000 | Loss: 0.00000918
Iteration 38/1000 | Loss: 0.00000917
Iteration 39/1000 | Loss: 0.00000914
Iteration 40/1000 | Loss: 0.00000913
Iteration 41/1000 | Loss: 0.00000913
Iteration 42/1000 | Loss: 0.00000912
Iteration 43/1000 | Loss: 0.00000909
Iteration 44/1000 | Loss: 0.00000908
Iteration 45/1000 | Loss: 0.00000907
Iteration 46/1000 | Loss: 0.00000906
Iteration 47/1000 | Loss: 0.00000906
Iteration 48/1000 | Loss: 0.00000904
Iteration 49/1000 | Loss: 0.00000901
Iteration 50/1000 | Loss: 0.00000901
Iteration 51/1000 | Loss: 0.00000901
Iteration 52/1000 | Loss: 0.00000901
Iteration 53/1000 | Loss: 0.00000901
Iteration 54/1000 | Loss: 0.00000901
Iteration 55/1000 | Loss: 0.00000901
Iteration 56/1000 | Loss: 0.00000901
Iteration 57/1000 | Loss: 0.00000900
Iteration 58/1000 | Loss: 0.00000900
Iteration 59/1000 | Loss: 0.00000900
Iteration 60/1000 | Loss: 0.00000900
Iteration 61/1000 | Loss: 0.00000900
Iteration 62/1000 | Loss: 0.00000897
Iteration 63/1000 | Loss: 0.00000897
Iteration 64/1000 | Loss: 0.00000897
Iteration 65/1000 | Loss: 0.00000896
Iteration 66/1000 | Loss: 0.00000896
Iteration 67/1000 | Loss: 0.00000896
Iteration 68/1000 | Loss: 0.00000896
Iteration 69/1000 | Loss: 0.00000896
Iteration 70/1000 | Loss: 0.00000896
Iteration 71/1000 | Loss: 0.00000896
Iteration 72/1000 | Loss: 0.00000896
Iteration 73/1000 | Loss: 0.00000896
Iteration 74/1000 | Loss: 0.00000896
Iteration 75/1000 | Loss: 0.00000896
Iteration 76/1000 | Loss: 0.00000895
Iteration 77/1000 | Loss: 0.00000895
Iteration 78/1000 | Loss: 0.00000895
Iteration 79/1000 | Loss: 0.00000894
Iteration 80/1000 | Loss: 0.00000893
Iteration 81/1000 | Loss: 0.00000892
Iteration 82/1000 | Loss: 0.00000892
Iteration 83/1000 | Loss: 0.00000891
Iteration 84/1000 | Loss: 0.00000891
Iteration 85/1000 | Loss: 0.00000891
Iteration 86/1000 | Loss: 0.00000890
Iteration 87/1000 | Loss: 0.00000890
Iteration 88/1000 | Loss: 0.00000889
Iteration 89/1000 | Loss: 0.00000889
Iteration 90/1000 | Loss: 0.00000889
Iteration 91/1000 | Loss: 0.00000889
Iteration 92/1000 | Loss: 0.00000888
Iteration 93/1000 | Loss: 0.00000888
Iteration 94/1000 | Loss: 0.00000888
Iteration 95/1000 | Loss: 0.00000888
Iteration 96/1000 | Loss: 0.00000888
Iteration 97/1000 | Loss: 0.00000888
Iteration 98/1000 | Loss: 0.00000887
Iteration 99/1000 | Loss: 0.00000887
Iteration 100/1000 | Loss: 0.00000887
Iteration 101/1000 | Loss: 0.00000887
Iteration 102/1000 | Loss: 0.00000886
Iteration 103/1000 | Loss: 0.00000886
Iteration 104/1000 | Loss: 0.00000886
Iteration 105/1000 | Loss: 0.00000885
Iteration 106/1000 | Loss: 0.00000885
Iteration 107/1000 | Loss: 0.00000885
Iteration 108/1000 | Loss: 0.00000885
Iteration 109/1000 | Loss: 0.00000884
Iteration 110/1000 | Loss: 0.00000884
Iteration 111/1000 | Loss: 0.00000884
Iteration 112/1000 | Loss: 0.00000884
Iteration 113/1000 | Loss: 0.00000883
Iteration 114/1000 | Loss: 0.00000883
Iteration 115/1000 | Loss: 0.00000883
Iteration 116/1000 | Loss: 0.00000883
Iteration 117/1000 | Loss: 0.00000883
Iteration 118/1000 | Loss: 0.00000883
Iteration 119/1000 | Loss: 0.00000883
Iteration 120/1000 | Loss: 0.00000883
Iteration 121/1000 | Loss: 0.00000882
Iteration 122/1000 | Loss: 0.00000882
Iteration 123/1000 | Loss: 0.00000882
Iteration 124/1000 | Loss: 0.00000882
Iteration 125/1000 | Loss: 0.00000882
Iteration 126/1000 | Loss: 0.00000882
Iteration 127/1000 | Loss: 0.00000881
Iteration 128/1000 | Loss: 0.00000881
Iteration 129/1000 | Loss: 0.00000881
Iteration 130/1000 | Loss: 0.00000881
Iteration 131/1000 | Loss: 0.00000881
Iteration 132/1000 | Loss: 0.00000881
Iteration 133/1000 | Loss: 0.00000881
Iteration 134/1000 | Loss: 0.00000881
Iteration 135/1000 | Loss: 0.00000881
Iteration 136/1000 | Loss: 0.00000881
Iteration 137/1000 | Loss: 0.00000881
Iteration 138/1000 | Loss: 0.00000880
Iteration 139/1000 | Loss: 0.00000880
Iteration 140/1000 | Loss: 0.00000880
Iteration 141/1000 | Loss: 0.00000880
Iteration 142/1000 | Loss: 0.00000880
Iteration 143/1000 | Loss: 0.00000880
Iteration 144/1000 | Loss: 0.00000880
Iteration 145/1000 | Loss: 0.00000880
Iteration 146/1000 | Loss: 0.00000880
Iteration 147/1000 | Loss: 0.00000880
Iteration 148/1000 | Loss: 0.00000880
Iteration 149/1000 | Loss: 0.00000880
Iteration 150/1000 | Loss: 0.00000880
Iteration 151/1000 | Loss: 0.00000879
Iteration 152/1000 | Loss: 0.00000879
Iteration 153/1000 | Loss: 0.00000879
Iteration 154/1000 | Loss: 0.00000879
Iteration 155/1000 | Loss: 0.00000879
Iteration 156/1000 | Loss: 0.00000879
Iteration 157/1000 | Loss: 0.00000879
Iteration 158/1000 | Loss: 0.00000879
Iteration 159/1000 | Loss: 0.00000879
Iteration 160/1000 | Loss: 0.00000878
Iteration 161/1000 | Loss: 0.00000878
Iteration 162/1000 | Loss: 0.00000878
Iteration 163/1000 | Loss: 0.00000878
Iteration 164/1000 | Loss: 0.00000878
Iteration 165/1000 | Loss: 0.00000878
Iteration 166/1000 | Loss: 0.00000878
Iteration 167/1000 | Loss: 0.00000878
Iteration 168/1000 | Loss: 0.00000878
Iteration 169/1000 | Loss: 0.00000878
Iteration 170/1000 | Loss: 0.00000878
Iteration 171/1000 | Loss: 0.00000878
Iteration 172/1000 | Loss: 0.00000878
Iteration 173/1000 | Loss: 0.00000878
Iteration 174/1000 | Loss: 0.00000878
Iteration 175/1000 | Loss: 0.00000878
Iteration 176/1000 | Loss: 0.00000877
Iteration 177/1000 | Loss: 0.00000877
Iteration 178/1000 | Loss: 0.00000877
Iteration 179/1000 | Loss: 0.00000877
Iteration 180/1000 | Loss: 0.00000877
Iteration 181/1000 | Loss: 0.00000877
Iteration 182/1000 | Loss: 0.00000877
Iteration 183/1000 | Loss: 0.00000877
Iteration 184/1000 | Loss: 0.00000877
Iteration 185/1000 | Loss: 0.00000877
Iteration 186/1000 | Loss: 0.00000877
Iteration 187/1000 | Loss: 0.00000877
Iteration 188/1000 | Loss: 0.00000876
Iteration 189/1000 | Loss: 0.00000876
Iteration 190/1000 | Loss: 0.00000876
Iteration 191/1000 | Loss: 0.00000876
Iteration 192/1000 | Loss: 0.00000876
Iteration 193/1000 | Loss: 0.00000876
Iteration 194/1000 | Loss: 0.00000876
Iteration 195/1000 | Loss: 0.00000876
Iteration 196/1000 | Loss: 0.00000876
Iteration 197/1000 | Loss: 0.00000876
Iteration 198/1000 | Loss: 0.00000876
Iteration 199/1000 | Loss: 0.00000876
Iteration 200/1000 | Loss: 0.00000876
Iteration 201/1000 | Loss: 0.00000876
Iteration 202/1000 | Loss: 0.00000876
Iteration 203/1000 | Loss: 0.00000876
Iteration 204/1000 | Loss: 0.00000876
Iteration 205/1000 | Loss: 0.00000876
Iteration 206/1000 | Loss: 0.00000876
Iteration 207/1000 | Loss: 0.00000876
Iteration 208/1000 | Loss: 0.00000876
Iteration 209/1000 | Loss: 0.00000876
Iteration 210/1000 | Loss: 0.00000876
Iteration 211/1000 | Loss: 0.00000876
Iteration 212/1000 | Loss: 0.00000876
Iteration 213/1000 | Loss: 0.00000876
Iteration 214/1000 | Loss: 0.00000876
Iteration 215/1000 | Loss: 0.00000876
Iteration 216/1000 | Loss: 0.00000876
Iteration 217/1000 | Loss: 0.00000876
Iteration 218/1000 | Loss: 0.00000876
Iteration 219/1000 | Loss: 0.00000876
Iteration 220/1000 | Loss: 0.00000876
Iteration 221/1000 | Loss: 0.00000876
Iteration 222/1000 | Loss: 0.00000876
Iteration 223/1000 | Loss: 0.00000876
Iteration 224/1000 | Loss: 0.00000876
Iteration 225/1000 | Loss: 0.00000876
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 225. Stopping optimization.
Last 5 losses: [8.759813681535888e-06, 8.759813681535888e-06, 8.759813681535888e-06, 8.759813681535888e-06, 8.759813681535888e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.759813681535888e-06

Optimization complete. Final v2v error: 2.533470869064331 mm

Highest mean error: 2.6773056983947754 mm for frame 55

Lowest mean error: 2.4128100872039795 mm for frame 30

Saving results

Total time: 40.64824628829956
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_015/1000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_015/1000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_015/1000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00996938
Iteration 2/25 | Loss: 0.00200380
Iteration 3/25 | Loss: 0.00152875
Iteration 4/25 | Loss: 0.00145767
Iteration 5/25 | Loss: 0.00146590
Iteration 6/25 | Loss: 0.00153023
Iteration 7/25 | Loss: 0.00137187
Iteration 8/25 | Loss: 0.00131124
Iteration 9/25 | Loss: 0.00128990
Iteration 10/25 | Loss: 0.00129097
Iteration 11/25 | Loss: 0.00129206
Iteration 12/25 | Loss: 0.00128984
Iteration 13/25 | Loss: 0.00127701
Iteration 14/25 | Loss: 0.00126544
Iteration 15/25 | Loss: 0.00126500
Iteration 16/25 | Loss: 0.00126642
Iteration 17/25 | Loss: 0.00126163
Iteration 18/25 | Loss: 0.00126061
Iteration 19/25 | Loss: 0.00125045
Iteration 20/25 | Loss: 0.00124758
Iteration 21/25 | Loss: 0.00124675
Iteration 22/25 | Loss: 0.00125087
Iteration 23/25 | Loss: 0.00125212
Iteration 24/25 | Loss: 0.00125067
Iteration 25/25 | Loss: 0.00124938

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.30593240
Iteration 2/25 | Loss: 0.00173032
Iteration 3/25 | Loss: 0.00157005
Iteration 4/25 | Loss: 0.00157005
Iteration 5/25 | Loss: 0.00157005
Iteration 6/25 | Loss: 0.00157005
Iteration 7/25 | Loss: 0.00157005
Iteration 8/25 | Loss: 0.00157005
Iteration 9/25 | Loss: 0.00157005
Iteration 10/25 | Loss: 0.00157005
Iteration 11/25 | Loss: 0.00157005
Iteration 12/25 | Loss: 0.00157005
Iteration 13/25 | Loss: 0.00157005
Iteration 14/25 | Loss: 0.00157005
Iteration 15/25 | Loss: 0.00157005
Iteration 16/25 | Loss: 0.00157005
Iteration 17/25 | Loss: 0.00157005
Iteration 18/25 | Loss: 0.00157005
Iteration 19/25 | Loss: 0.00157005
Iteration 20/25 | Loss: 0.00157005
Iteration 21/25 | Loss: 0.00157005
Iteration 22/25 | Loss: 0.00157005
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0015700472285971045, 0.0015700472285971045, 0.0015700472285971045, 0.0015700472285971045, 0.0015700472285971045]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015700472285971045

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00157005
Iteration 2/1000 | Loss: 0.00031174
Iteration 3/1000 | Loss: 0.00036811
Iteration 4/1000 | Loss: 0.00025850
Iteration 5/1000 | Loss: 0.00016699
Iteration 6/1000 | Loss: 0.00020593
Iteration 7/1000 | Loss: 0.00035062
Iteration 8/1000 | Loss: 0.00034709
Iteration 9/1000 | Loss: 0.00026866
Iteration 10/1000 | Loss: 0.00036352
Iteration 11/1000 | Loss: 0.00041127
Iteration 12/1000 | Loss: 0.00048560
Iteration 13/1000 | Loss: 0.00038677
Iteration 14/1000 | Loss: 0.00032229
Iteration 15/1000 | Loss: 0.00045756
Iteration 16/1000 | Loss: 0.00032890
Iteration 17/1000 | Loss: 0.00034174
Iteration 18/1000 | Loss: 0.00029425
Iteration 19/1000 | Loss: 0.00047878
Iteration 20/1000 | Loss: 0.00029446
Iteration 21/1000 | Loss: 0.00017329
Iteration 22/1000 | Loss: 0.00083194
Iteration 23/1000 | Loss: 0.00083793
Iteration 24/1000 | Loss: 0.00029298
Iteration 25/1000 | Loss: 0.00048882
Iteration 26/1000 | Loss: 0.00044512
Iteration 27/1000 | Loss: 0.00060926
Iteration 28/1000 | Loss: 0.00083914
Iteration 29/1000 | Loss: 0.00068495
Iteration 30/1000 | Loss: 0.00056496
Iteration 31/1000 | Loss: 0.00045240
Iteration 32/1000 | Loss: 0.00034774
Iteration 33/1000 | Loss: 0.00028812
Iteration 34/1000 | Loss: 0.00031021
Iteration 35/1000 | Loss: 0.00021235
Iteration 36/1000 | Loss: 0.00029687
Iteration 37/1000 | Loss: 0.00027566
Iteration 38/1000 | Loss: 0.00031585
Iteration 39/1000 | Loss: 0.00028679
Iteration 40/1000 | Loss: 0.00024515
Iteration 41/1000 | Loss: 0.00027992
Iteration 42/1000 | Loss: 0.00025426
Iteration 43/1000 | Loss: 0.00011825
Iteration 44/1000 | Loss: 0.00036122
Iteration 45/1000 | Loss: 0.00042676
Iteration 46/1000 | Loss: 0.00034731
Iteration 47/1000 | Loss: 0.00048246
Iteration 48/1000 | Loss: 0.00036840
Iteration 49/1000 | Loss: 0.00045085
Iteration 50/1000 | Loss: 0.00012864
Iteration 51/1000 | Loss: 0.00012597
Iteration 52/1000 | Loss: 0.00029604
Iteration 53/1000 | Loss: 0.00008056
Iteration 54/1000 | Loss: 0.00024882
Iteration 55/1000 | Loss: 0.00022656
Iteration 56/1000 | Loss: 0.00032539
Iteration 57/1000 | Loss: 0.00030077
Iteration 58/1000 | Loss: 0.00029110
Iteration 59/1000 | Loss: 0.00038094
Iteration 60/1000 | Loss: 0.00044155
Iteration 61/1000 | Loss: 0.00039083
Iteration 62/1000 | Loss: 0.00035955
Iteration 63/1000 | Loss: 0.00038914
Iteration 64/1000 | Loss: 0.00028293
Iteration 65/1000 | Loss: 0.00039451
Iteration 66/1000 | Loss: 0.00025123
Iteration 67/1000 | Loss: 0.00031331
Iteration 68/1000 | Loss: 0.00063035
Iteration 69/1000 | Loss: 0.00034348
Iteration 70/1000 | Loss: 0.00041637
Iteration 71/1000 | Loss: 0.00020052
Iteration 72/1000 | Loss: 0.00027009
Iteration 73/1000 | Loss: 0.00033760
Iteration 74/1000 | Loss: 0.00030346
Iteration 75/1000 | Loss: 0.00033114
Iteration 76/1000 | Loss: 0.00035515
Iteration 77/1000 | Loss: 0.00032528
Iteration 78/1000 | Loss: 0.00031843
Iteration 79/1000 | Loss: 0.00042555
Iteration 80/1000 | Loss: 0.00034245
Iteration 81/1000 | Loss: 0.00068849
Iteration 82/1000 | Loss: 0.00033004
Iteration 83/1000 | Loss: 0.00047957
Iteration 84/1000 | Loss: 0.00039277
Iteration 85/1000 | Loss: 0.00041184
Iteration 86/1000 | Loss: 0.00034868
Iteration 87/1000 | Loss: 0.00031713
Iteration 88/1000 | Loss: 0.00016474
Iteration 89/1000 | Loss: 0.00030371
Iteration 90/1000 | Loss: 0.00069117
Iteration 91/1000 | Loss: 0.00034497
Iteration 92/1000 | Loss: 0.00037865
Iteration 93/1000 | Loss: 0.00031275
Iteration 94/1000 | Loss: 0.00030604
Iteration 95/1000 | Loss: 0.00091396
Iteration 96/1000 | Loss: 0.00030726
Iteration 97/1000 | Loss: 0.00040663
Iteration 98/1000 | Loss: 0.00038894
Iteration 99/1000 | Loss: 0.00022951
Iteration 100/1000 | Loss: 0.00028531
Iteration 101/1000 | Loss: 0.00037902
Iteration 102/1000 | Loss: 0.00042781
Iteration 103/1000 | Loss: 0.00034737
Iteration 104/1000 | Loss: 0.00015263
Iteration 105/1000 | Loss: 0.00024090
Iteration 106/1000 | Loss: 0.00025786
Iteration 107/1000 | Loss: 0.00026428
Iteration 108/1000 | Loss: 0.00045760
Iteration 109/1000 | Loss: 0.00026218
Iteration 110/1000 | Loss: 0.00033508
Iteration 111/1000 | Loss: 0.00015479
Iteration 112/1000 | Loss: 0.00024379
Iteration 113/1000 | Loss: 0.00012690
Iteration 114/1000 | Loss: 0.00044734
Iteration 115/1000 | Loss: 0.00019020
Iteration 116/1000 | Loss: 0.00117658
Iteration 117/1000 | Loss: 0.00042952
Iteration 118/1000 | Loss: 0.00045766
Iteration 119/1000 | Loss: 0.00028061
Iteration 120/1000 | Loss: 0.00034160
Iteration 121/1000 | Loss: 0.00026055
Iteration 122/1000 | Loss: 0.00023899
Iteration 123/1000 | Loss: 0.00024586
Iteration 124/1000 | Loss: 0.00040692
Iteration 125/1000 | Loss: 0.00005839
Iteration 126/1000 | Loss: 0.00014057
Iteration 127/1000 | Loss: 0.00024434
Iteration 128/1000 | Loss: 0.00037223
Iteration 129/1000 | Loss: 0.00031855
Iteration 130/1000 | Loss: 0.00025112
Iteration 131/1000 | Loss: 0.00061554
Iteration 132/1000 | Loss: 0.00013507
Iteration 133/1000 | Loss: 0.00017156
Iteration 134/1000 | Loss: 0.00032315
Iteration 135/1000 | Loss: 0.00023207
Iteration 136/1000 | Loss: 0.00031952
Iteration 137/1000 | Loss: 0.00037280
Iteration 138/1000 | Loss: 0.00039521
Iteration 139/1000 | Loss: 0.00049228
Iteration 140/1000 | Loss: 0.00024423
Iteration 141/1000 | Loss: 0.00063782
Iteration 142/1000 | Loss: 0.00020579
Iteration 143/1000 | Loss: 0.00017141
Iteration 144/1000 | Loss: 0.00012686
Iteration 145/1000 | Loss: 0.00014191
Iteration 146/1000 | Loss: 0.00012047
Iteration 147/1000 | Loss: 0.00013604
Iteration 148/1000 | Loss: 0.00011265
Iteration 149/1000 | Loss: 0.00010786
Iteration 150/1000 | Loss: 0.00004827
Iteration 151/1000 | Loss: 0.00032879
Iteration 152/1000 | Loss: 0.00003312
Iteration 153/1000 | Loss: 0.00009531
Iteration 154/1000 | Loss: 0.00011670
Iteration 155/1000 | Loss: 0.00010627
Iteration 156/1000 | Loss: 0.00006563
Iteration 157/1000 | Loss: 0.00012306
Iteration 158/1000 | Loss: 0.00026215
Iteration 159/1000 | Loss: 0.00019036
Iteration 160/1000 | Loss: 0.00004517
Iteration 161/1000 | Loss: 0.00018619
Iteration 162/1000 | Loss: 0.00010098
Iteration 163/1000 | Loss: 0.00012562
Iteration 164/1000 | Loss: 0.00022429
Iteration 165/1000 | Loss: 0.00020681
Iteration 166/1000 | Loss: 0.00023956
Iteration 167/1000 | Loss: 0.00007355
Iteration 168/1000 | Loss: 0.00009478
Iteration 169/1000 | Loss: 0.00016444
Iteration 170/1000 | Loss: 0.00013814
Iteration 171/1000 | Loss: 0.00005111
Iteration 172/1000 | Loss: 0.00007424
Iteration 173/1000 | Loss: 0.00007778
Iteration 174/1000 | Loss: 0.00029571
Iteration 175/1000 | Loss: 0.00007871
Iteration 176/1000 | Loss: 0.00018561
Iteration 177/1000 | Loss: 0.00006637
Iteration 178/1000 | Loss: 0.00017194
Iteration 179/1000 | Loss: 0.00010482
Iteration 180/1000 | Loss: 0.00020153
Iteration 181/1000 | Loss: 0.00013553
Iteration 182/1000 | Loss: 0.00017450
Iteration 183/1000 | Loss: 0.00017607
Iteration 184/1000 | Loss: 0.00018559
Iteration 185/1000 | Loss: 0.00016331
Iteration 186/1000 | Loss: 0.00008716
Iteration 187/1000 | Loss: 0.00049442
Iteration 188/1000 | Loss: 0.00002110
Iteration 189/1000 | Loss: 0.00006544
Iteration 190/1000 | Loss: 0.00001719
Iteration 191/1000 | Loss: 0.00001634
Iteration 192/1000 | Loss: 0.00001576
Iteration 193/1000 | Loss: 0.00008619
Iteration 194/1000 | Loss: 0.00001570
Iteration 195/1000 | Loss: 0.00001490
Iteration 196/1000 | Loss: 0.00001453
Iteration 197/1000 | Loss: 0.00001422
Iteration 198/1000 | Loss: 0.00001404
Iteration 199/1000 | Loss: 0.00001392
Iteration 200/1000 | Loss: 0.00001374
Iteration 201/1000 | Loss: 0.00001360
Iteration 202/1000 | Loss: 0.00001354
Iteration 203/1000 | Loss: 0.00001351
Iteration 204/1000 | Loss: 0.00001347
Iteration 205/1000 | Loss: 0.00001346
Iteration 206/1000 | Loss: 0.00001342
Iteration 207/1000 | Loss: 0.00001341
Iteration 208/1000 | Loss: 0.00001340
Iteration 209/1000 | Loss: 0.00001340
Iteration 210/1000 | Loss: 0.00001339
Iteration 211/1000 | Loss: 0.00001339
Iteration 212/1000 | Loss: 0.00001339
Iteration 213/1000 | Loss: 0.00001338
Iteration 214/1000 | Loss: 0.00001338
Iteration 215/1000 | Loss: 0.00001337
Iteration 216/1000 | Loss: 0.00001337
Iteration 217/1000 | Loss: 0.00001337
Iteration 218/1000 | Loss: 0.00001336
Iteration 219/1000 | Loss: 0.00001336
Iteration 220/1000 | Loss: 0.00001336
Iteration 221/1000 | Loss: 0.00001336
Iteration 222/1000 | Loss: 0.00001336
Iteration 223/1000 | Loss: 0.00001336
Iteration 224/1000 | Loss: 0.00001336
Iteration 225/1000 | Loss: 0.00001335
Iteration 226/1000 | Loss: 0.00001335
Iteration 227/1000 | Loss: 0.00001335
Iteration 228/1000 | Loss: 0.00001335
Iteration 229/1000 | Loss: 0.00001335
Iteration 230/1000 | Loss: 0.00001335
Iteration 231/1000 | Loss: 0.00001334
Iteration 232/1000 | Loss: 0.00001334
Iteration 233/1000 | Loss: 0.00001334
Iteration 234/1000 | Loss: 0.00001334
Iteration 235/1000 | Loss: 0.00001333
Iteration 236/1000 | Loss: 0.00001333
Iteration 237/1000 | Loss: 0.00001333
Iteration 238/1000 | Loss: 0.00001332
Iteration 239/1000 | Loss: 0.00001332
Iteration 240/1000 | Loss: 0.00001332
Iteration 241/1000 | Loss: 0.00001332
Iteration 242/1000 | Loss: 0.00001332
Iteration 243/1000 | Loss: 0.00001332
Iteration 244/1000 | Loss: 0.00001332
Iteration 245/1000 | Loss: 0.00001331
Iteration 246/1000 | Loss: 0.00001331
Iteration 247/1000 | Loss: 0.00001331
Iteration 248/1000 | Loss: 0.00001331
Iteration 249/1000 | Loss: 0.00001331
Iteration 250/1000 | Loss: 0.00001331
Iteration 251/1000 | Loss: 0.00001331
Iteration 252/1000 | Loss: 0.00001330
Iteration 253/1000 | Loss: 0.00001330
Iteration 254/1000 | Loss: 0.00001330
Iteration 255/1000 | Loss: 0.00001330
Iteration 256/1000 | Loss: 0.00001330
Iteration 257/1000 | Loss: 0.00001330
Iteration 258/1000 | Loss: 0.00001329
Iteration 259/1000 | Loss: 0.00001329
Iteration 260/1000 | Loss: 0.00001329
Iteration 261/1000 | Loss: 0.00001329
Iteration 262/1000 | Loss: 0.00001328
Iteration 263/1000 | Loss: 0.00001328
Iteration 264/1000 | Loss: 0.00001328
Iteration 265/1000 | Loss: 0.00001327
Iteration 266/1000 | Loss: 0.00001327
Iteration 267/1000 | Loss: 0.00001327
Iteration 268/1000 | Loss: 0.00001327
Iteration 269/1000 | Loss: 0.00001327
Iteration 270/1000 | Loss: 0.00001326
Iteration 271/1000 | Loss: 0.00001326
Iteration 272/1000 | Loss: 0.00001326
Iteration 273/1000 | Loss: 0.00001326
Iteration 274/1000 | Loss: 0.00001326
Iteration 275/1000 | Loss: 0.00001325
Iteration 276/1000 | Loss: 0.00001325
Iteration 277/1000 | Loss: 0.00001325
Iteration 278/1000 | Loss: 0.00001324
Iteration 279/1000 | Loss: 0.00001324
Iteration 280/1000 | Loss: 0.00001324
Iteration 281/1000 | Loss: 0.00001324
Iteration 282/1000 | Loss: 0.00001323
Iteration 283/1000 | Loss: 0.00001323
Iteration 284/1000 | Loss: 0.00001323
Iteration 285/1000 | Loss: 0.00001322
Iteration 286/1000 | Loss: 0.00001322
Iteration 287/1000 | Loss: 0.00001322
Iteration 288/1000 | Loss: 0.00001322
Iteration 289/1000 | Loss: 0.00001321
Iteration 290/1000 | Loss: 0.00001321
Iteration 291/1000 | Loss: 0.00001321
Iteration 292/1000 | Loss: 0.00001321
Iteration 293/1000 | Loss: 0.00001321
Iteration 294/1000 | Loss: 0.00001320
Iteration 295/1000 | Loss: 0.00001320
Iteration 296/1000 | Loss: 0.00001320
Iteration 297/1000 | Loss: 0.00001320
Iteration 298/1000 | Loss: 0.00001320
Iteration 299/1000 | Loss: 0.00001320
Iteration 300/1000 | Loss: 0.00001320
Iteration 301/1000 | Loss: 0.00001320
Iteration 302/1000 | Loss: 0.00001320
Iteration 303/1000 | Loss: 0.00001320
Iteration 304/1000 | Loss: 0.00001320
Iteration 305/1000 | Loss: 0.00001320
Iteration 306/1000 | Loss: 0.00001320
Iteration 307/1000 | Loss: 0.00001320
Iteration 308/1000 | Loss: 0.00001320
Iteration 309/1000 | Loss: 0.00001320
Iteration 310/1000 | Loss: 0.00001320
Iteration 311/1000 | Loss: 0.00001320
Iteration 312/1000 | Loss: 0.00001320
Iteration 313/1000 | Loss: 0.00001320
Iteration 314/1000 | Loss: 0.00001320
Iteration 315/1000 | Loss: 0.00001320
Iteration 316/1000 | Loss: 0.00001320
Iteration 317/1000 | Loss: 0.00001320
Iteration 318/1000 | Loss: 0.00001320
Iteration 319/1000 | Loss: 0.00001320
Iteration 320/1000 | Loss: 0.00001320
Iteration 321/1000 | Loss: 0.00001320
Iteration 322/1000 | Loss: 0.00001320
Iteration 323/1000 | Loss: 0.00001320
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 323. Stopping optimization.
Last 5 losses: [1.3195658539189026e-05, 1.3195658539189026e-05, 1.3195658539189026e-05, 1.3195658539189026e-05, 1.3195658539189026e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3195658539189026e-05

Optimization complete. Final v2v error: 2.9565720558166504 mm

Highest mean error: 5.252574443817139 mm for frame 43

Lowest mean error: 2.5554728507995605 mm for frame 12

Saving results

Total time: 335.5067982673645
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_015/1038/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_015/1038.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_015/1038
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00460244
Iteration 2/25 | Loss: 0.00128714
Iteration 3/25 | Loss: 0.00119960
Iteration 4/25 | Loss: 0.00118893
Iteration 5/25 | Loss: 0.00118705
Iteration 6/25 | Loss: 0.00118705
Iteration 7/25 | Loss: 0.00118705
Iteration 8/25 | Loss: 0.00118705
Iteration 9/25 | Loss: 0.00118705
Iteration 10/25 | Loss: 0.00118705
Iteration 11/25 | Loss: 0.00118705
Iteration 12/25 | Loss: 0.00118705
Iteration 13/25 | Loss: 0.00118705
Iteration 14/25 | Loss: 0.00118705
Iteration 15/25 | Loss: 0.00118705
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0011870511807501316, 0.0011870511807501316, 0.0011870511807501316, 0.0011870511807501316, 0.0011870511807501316]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011870511807501316

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.35859489
Iteration 2/25 | Loss: 0.00117369
Iteration 3/25 | Loss: 0.00117369
Iteration 4/25 | Loss: 0.00117369
Iteration 5/25 | Loss: 0.00117369
Iteration 6/25 | Loss: 0.00117369
Iteration 7/25 | Loss: 0.00117369
Iteration 8/25 | Loss: 0.00117369
Iteration 9/25 | Loss: 0.00117369
Iteration 10/25 | Loss: 0.00117369
Iteration 11/25 | Loss: 0.00117369
Iteration 12/25 | Loss: 0.00117369
Iteration 13/25 | Loss: 0.00117369
Iteration 14/25 | Loss: 0.00117369
Iteration 15/25 | Loss: 0.00117369
Iteration 16/25 | Loss: 0.00117369
Iteration 17/25 | Loss: 0.00117369
Iteration 18/25 | Loss: 0.00117369
Iteration 19/25 | Loss: 0.00117369
Iteration 20/25 | Loss: 0.00117369
Iteration 21/25 | Loss: 0.00117369
Iteration 22/25 | Loss: 0.00117369
Iteration 23/25 | Loss: 0.00117369
Iteration 24/25 | Loss: 0.00117369
Iteration 25/25 | Loss: 0.00117369

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00117369
Iteration 2/1000 | Loss: 0.00001929
Iteration 3/1000 | Loss: 0.00001627
Iteration 4/1000 | Loss: 0.00001533
Iteration 5/1000 | Loss: 0.00001434
Iteration 6/1000 | Loss: 0.00001379
Iteration 7/1000 | Loss: 0.00001335
Iteration 8/1000 | Loss: 0.00001297
Iteration 9/1000 | Loss: 0.00001266
Iteration 10/1000 | Loss: 0.00001241
Iteration 11/1000 | Loss: 0.00001232
Iteration 12/1000 | Loss: 0.00001230
Iteration 13/1000 | Loss: 0.00001228
Iteration 14/1000 | Loss: 0.00001227
Iteration 15/1000 | Loss: 0.00001226
Iteration 16/1000 | Loss: 0.00001210
Iteration 17/1000 | Loss: 0.00001201
Iteration 18/1000 | Loss: 0.00001197
Iteration 19/1000 | Loss: 0.00001180
Iteration 20/1000 | Loss: 0.00001179
Iteration 21/1000 | Loss: 0.00001173
Iteration 22/1000 | Loss: 0.00001171
Iteration 23/1000 | Loss: 0.00001170
Iteration 24/1000 | Loss: 0.00001168
Iteration 25/1000 | Loss: 0.00001166
Iteration 26/1000 | Loss: 0.00001166
Iteration 27/1000 | Loss: 0.00001164
Iteration 28/1000 | Loss: 0.00001162
Iteration 29/1000 | Loss: 0.00001158
Iteration 30/1000 | Loss: 0.00001155
Iteration 31/1000 | Loss: 0.00001154
Iteration 32/1000 | Loss: 0.00001154
Iteration 33/1000 | Loss: 0.00001154
Iteration 34/1000 | Loss: 0.00001154
Iteration 35/1000 | Loss: 0.00001154
Iteration 36/1000 | Loss: 0.00001154
Iteration 37/1000 | Loss: 0.00001152
Iteration 38/1000 | Loss: 0.00001152
Iteration 39/1000 | Loss: 0.00001151
Iteration 40/1000 | Loss: 0.00001150
Iteration 41/1000 | Loss: 0.00001149
Iteration 42/1000 | Loss: 0.00001149
Iteration 43/1000 | Loss: 0.00001149
Iteration 44/1000 | Loss: 0.00001149
Iteration 45/1000 | Loss: 0.00001149
Iteration 46/1000 | Loss: 0.00001148
Iteration 47/1000 | Loss: 0.00001148
Iteration 48/1000 | Loss: 0.00001147
Iteration 49/1000 | Loss: 0.00001146
Iteration 50/1000 | Loss: 0.00001146
Iteration 51/1000 | Loss: 0.00001144
Iteration 52/1000 | Loss: 0.00001144
Iteration 53/1000 | Loss: 0.00001143
Iteration 54/1000 | Loss: 0.00001140
Iteration 55/1000 | Loss: 0.00001140
Iteration 56/1000 | Loss: 0.00001140
Iteration 57/1000 | Loss: 0.00001139
Iteration 58/1000 | Loss: 0.00001139
Iteration 59/1000 | Loss: 0.00001139
Iteration 60/1000 | Loss: 0.00001139
Iteration 61/1000 | Loss: 0.00001139
Iteration 62/1000 | Loss: 0.00001139
Iteration 63/1000 | Loss: 0.00001138
Iteration 64/1000 | Loss: 0.00001137
Iteration 65/1000 | Loss: 0.00001137
Iteration 66/1000 | Loss: 0.00001136
Iteration 67/1000 | Loss: 0.00001136
Iteration 68/1000 | Loss: 0.00001135
Iteration 69/1000 | Loss: 0.00001135
Iteration 70/1000 | Loss: 0.00001135
Iteration 71/1000 | Loss: 0.00001134
Iteration 72/1000 | Loss: 0.00001134
Iteration 73/1000 | Loss: 0.00001134
Iteration 74/1000 | Loss: 0.00001133
Iteration 75/1000 | Loss: 0.00001133
Iteration 76/1000 | Loss: 0.00001133
Iteration 77/1000 | Loss: 0.00001133
Iteration 78/1000 | Loss: 0.00001133
Iteration 79/1000 | Loss: 0.00001132
Iteration 80/1000 | Loss: 0.00001132
Iteration 81/1000 | Loss: 0.00001132
Iteration 82/1000 | Loss: 0.00001132
Iteration 83/1000 | Loss: 0.00001131
Iteration 84/1000 | Loss: 0.00001130
Iteration 85/1000 | Loss: 0.00001130
Iteration 86/1000 | Loss: 0.00001130
Iteration 87/1000 | Loss: 0.00001130
Iteration 88/1000 | Loss: 0.00001130
Iteration 89/1000 | Loss: 0.00001130
Iteration 90/1000 | Loss: 0.00001130
Iteration 91/1000 | Loss: 0.00001130
Iteration 92/1000 | Loss: 0.00001130
Iteration 93/1000 | Loss: 0.00001129
Iteration 94/1000 | Loss: 0.00001129
Iteration 95/1000 | Loss: 0.00001129
Iteration 96/1000 | Loss: 0.00001129
Iteration 97/1000 | Loss: 0.00001128
Iteration 98/1000 | Loss: 0.00001128
Iteration 99/1000 | Loss: 0.00001127
Iteration 100/1000 | Loss: 0.00001127
Iteration 101/1000 | Loss: 0.00001127
Iteration 102/1000 | Loss: 0.00001126
Iteration 103/1000 | Loss: 0.00001126
Iteration 104/1000 | Loss: 0.00001126
Iteration 105/1000 | Loss: 0.00001126
Iteration 106/1000 | Loss: 0.00001126
Iteration 107/1000 | Loss: 0.00001126
Iteration 108/1000 | Loss: 0.00001126
Iteration 109/1000 | Loss: 0.00001126
Iteration 110/1000 | Loss: 0.00001126
Iteration 111/1000 | Loss: 0.00001125
Iteration 112/1000 | Loss: 0.00001125
Iteration 113/1000 | Loss: 0.00001125
Iteration 114/1000 | Loss: 0.00001125
Iteration 115/1000 | Loss: 0.00001125
Iteration 116/1000 | Loss: 0.00001125
Iteration 117/1000 | Loss: 0.00001125
Iteration 118/1000 | Loss: 0.00001124
Iteration 119/1000 | Loss: 0.00001124
Iteration 120/1000 | Loss: 0.00001124
Iteration 121/1000 | Loss: 0.00001124
Iteration 122/1000 | Loss: 0.00001124
Iteration 123/1000 | Loss: 0.00001124
Iteration 124/1000 | Loss: 0.00001124
Iteration 125/1000 | Loss: 0.00001124
Iteration 126/1000 | Loss: 0.00001124
Iteration 127/1000 | Loss: 0.00001124
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 127. Stopping optimization.
Last 5 losses: [1.1239365449000616e-05, 1.1239365449000616e-05, 1.1239365449000616e-05, 1.1239365449000616e-05, 1.1239365449000616e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1239365449000616e-05

Optimization complete. Final v2v error: 2.892066478729248 mm

Highest mean error: 3.0566537380218506 mm for frame 246

Lowest mean error: 2.76110577583313 mm for frame 0

Saving results

Total time: 41.968915700912476
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_015/1025/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_015/1025.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_015/1025
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00990612
Iteration 2/25 | Loss: 0.00175129
Iteration 3/25 | Loss: 0.00134094
Iteration 4/25 | Loss: 0.00128134
Iteration 5/25 | Loss: 0.00127682
Iteration 6/25 | Loss: 0.00127268
Iteration 7/25 | Loss: 0.00126641
Iteration 8/25 | Loss: 0.00124272
Iteration 9/25 | Loss: 0.00123570
Iteration 10/25 | Loss: 0.00123761
Iteration 11/25 | Loss: 0.00123894
Iteration 12/25 | Loss: 0.00123556
Iteration 13/25 | Loss: 0.00123173
Iteration 14/25 | Loss: 0.00122864
Iteration 15/25 | Loss: 0.00122766
Iteration 16/25 | Loss: 0.00123045
Iteration 17/25 | Loss: 0.00122913
Iteration 18/25 | Loss: 0.00123043
Iteration 19/25 | Loss: 0.00123182
Iteration 20/25 | Loss: 0.00122991
Iteration 21/25 | Loss: 0.00122523
Iteration 22/25 | Loss: 0.00122367
Iteration 23/25 | Loss: 0.00122315
Iteration 24/25 | Loss: 0.00122298
Iteration 25/25 | Loss: 0.00122298

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36819637
Iteration 2/25 | Loss: 0.00134175
Iteration 3/25 | Loss: 0.00134175
Iteration 4/25 | Loss: 0.00134175
Iteration 5/25 | Loss: 0.00134175
Iteration 6/25 | Loss: 0.00134174
Iteration 7/25 | Loss: 0.00134174
Iteration 8/25 | Loss: 0.00134174
Iteration 9/25 | Loss: 0.00134174
Iteration 10/25 | Loss: 0.00134174
Iteration 11/25 | Loss: 0.00134174
Iteration 12/25 | Loss: 0.00134174
Iteration 13/25 | Loss: 0.00134174
Iteration 14/25 | Loss: 0.00134174
Iteration 15/25 | Loss: 0.00134174
Iteration 16/25 | Loss: 0.00134174
Iteration 17/25 | Loss: 0.00134174
Iteration 18/25 | Loss: 0.00134174
Iteration 19/25 | Loss: 0.00134174
Iteration 20/25 | Loss: 0.00134174
Iteration 21/25 | Loss: 0.00134174
Iteration 22/25 | Loss: 0.00134174
Iteration 23/25 | Loss: 0.00134174
Iteration 24/25 | Loss: 0.00134174
Iteration 25/25 | Loss: 0.00134174

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00134174
Iteration 2/1000 | Loss: 0.00003010
Iteration 3/1000 | Loss: 0.00001996
Iteration 4/1000 | Loss: 0.00001813
Iteration 5/1000 | Loss: 0.00001698
Iteration 6/1000 | Loss: 0.00001615
Iteration 7/1000 | Loss: 0.00001569
Iteration 8/1000 | Loss: 0.00001538
Iteration 9/1000 | Loss: 0.00001513
Iteration 10/1000 | Loss: 0.00001491
Iteration 11/1000 | Loss: 0.00001491
Iteration 12/1000 | Loss: 0.00001486
Iteration 13/1000 | Loss: 0.00001481
Iteration 14/1000 | Loss: 0.00001479
Iteration 15/1000 | Loss: 0.00001462
Iteration 16/1000 | Loss: 0.00001453
Iteration 17/1000 | Loss: 0.00001449
Iteration 18/1000 | Loss: 0.00001445
Iteration 19/1000 | Loss: 0.00001438
Iteration 20/1000 | Loss: 0.00001431
Iteration 21/1000 | Loss: 0.00001428
Iteration 22/1000 | Loss: 0.00001426
Iteration 23/1000 | Loss: 0.00001425
Iteration 24/1000 | Loss: 0.00001422
Iteration 25/1000 | Loss: 0.00001416
Iteration 26/1000 | Loss: 0.00001412
Iteration 27/1000 | Loss: 0.00001411
Iteration 28/1000 | Loss: 0.00001411
Iteration 29/1000 | Loss: 0.00001410
Iteration 30/1000 | Loss: 0.00001410
Iteration 31/1000 | Loss: 0.00001409
Iteration 32/1000 | Loss: 0.00001408
Iteration 33/1000 | Loss: 0.00001408
Iteration 34/1000 | Loss: 0.00001408
Iteration 35/1000 | Loss: 0.00001407
Iteration 36/1000 | Loss: 0.00001407
Iteration 37/1000 | Loss: 0.00001406
Iteration 38/1000 | Loss: 0.00001406
Iteration 39/1000 | Loss: 0.00001406
Iteration 40/1000 | Loss: 0.00001406
Iteration 41/1000 | Loss: 0.00001405
Iteration 42/1000 | Loss: 0.00001405
Iteration 43/1000 | Loss: 0.00001405
Iteration 44/1000 | Loss: 0.00001405
Iteration 45/1000 | Loss: 0.00001405
Iteration 46/1000 | Loss: 0.00001404
Iteration 47/1000 | Loss: 0.00001404
Iteration 48/1000 | Loss: 0.00001404
Iteration 49/1000 | Loss: 0.00001403
Iteration 50/1000 | Loss: 0.00001403
Iteration 51/1000 | Loss: 0.00001403
Iteration 52/1000 | Loss: 0.00001402
Iteration 53/1000 | Loss: 0.00001402
Iteration 54/1000 | Loss: 0.00001402
Iteration 55/1000 | Loss: 0.00001401
Iteration 56/1000 | Loss: 0.00001401
Iteration 57/1000 | Loss: 0.00001401
Iteration 58/1000 | Loss: 0.00001400
Iteration 59/1000 | Loss: 0.00001400
Iteration 60/1000 | Loss: 0.00001400
Iteration 61/1000 | Loss: 0.00001400
Iteration 62/1000 | Loss: 0.00001400
Iteration 63/1000 | Loss: 0.00001400
Iteration 64/1000 | Loss: 0.00001400
Iteration 65/1000 | Loss: 0.00001400
Iteration 66/1000 | Loss: 0.00001400
Iteration 67/1000 | Loss: 0.00001400
Iteration 68/1000 | Loss: 0.00001400
Iteration 69/1000 | Loss: 0.00001400
Iteration 70/1000 | Loss: 0.00001400
Iteration 71/1000 | Loss: 0.00001399
Iteration 72/1000 | Loss: 0.00001399
Iteration 73/1000 | Loss: 0.00001399
Iteration 74/1000 | Loss: 0.00001399
Iteration 75/1000 | Loss: 0.00001399
Iteration 76/1000 | Loss: 0.00001399
Iteration 77/1000 | Loss: 0.00001399
Iteration 78/1000 | Loss: 0.00001399
Iteration 79/1000 | Loss: 0.00001399
Iteration 80/1000 | Loss: 0.00001399
Iteration 81/1000 | Loss: 0.00001399
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 81. Stopping optimization.
Last 5 losses: [1.3992988897371106e-05, 1.3992988897371106e-05, 1.3992988897371106e-05, 1.3992988897371106e-05, 1.3992988897371106e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3992988897371106e-05

Optimization complete. Final v2v error: 3.152764320373535 mm

Highest mean error: 5.4914631843566895 mm for frame 186

Lowest mean error: 2.7929768562316895 mm for frame 16

Saving results

Total time: 76.88468861579895
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_015/1008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_015/1008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_015/1008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00746811
Iteration 2/25 | Loss: 0.00166265
Iteration 3/25 | Loss: 0.00131352
Iteration 4/25 | Loss: 0.00126967
Iteration 5/25 | Loss: 0.00126799
Iteration 6/25 | Loss: 0.00125864
Iteration 7/25 | Loss: 0.00124412
Iteration 8/25 | Loss: 0.00123769
Iteration 9/25 | Loss: 0.00123127
Iteration 10/25 | Loss: 0.00123236
Iteration 11/25 | Loss: 0.00122903
Iteration 12/25 | Loss: 0.00122911
Iteration 13/25 | Loss: 0.00122698
Iteration 14/25 | Loss: 0.00122538
Iteration 15/25 | Loss: 0.00122425
Iteration 16/25 | Loss: 0.00122378
Iteration 17/25 | Loss: 0.00122363
Iteration 18/25 | Loss: 0.00122357
Iteration 19/25 | Loss: 0.00122357
Iteration 20/25 | Loss: 0.00122357
Iteration 21/25 | Loss: 0.00122357
Iteration 22/25 | Loss: 0.00122357
Iteration 23/25 | Loss: 0.00122357
Iteration 24/25 | Loss: 0.00122357
Iteration 25/25 | Loss: 0.00122357

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.52128744
Iteration 2/25 | Loss: 0.00133218
Iteration 3/25 | Loss: 0.00133214
Iteration 4/25 | Loss: 0.00133214
Iteration 5/25 | Loss: 0.00133214
Iteration 6/25 | Loss: 0.00133214
Iteration 7/25 | Loss: 0.00133214
Iteration 8/25 | Loss: 0.00133214
Iteration 9/25 | Loss: 0.00133214
Iteration 10/25 | Loss: 0.00133214
Iteration 11/25 | Loss: 0.00133214
Iteration 12/25 | Loss: 0.00133214
Iteration 13/25 | Loss: 0.00133214
Iteration 14/25 | Loss: 0.00133214
Iteration 15/25 | Loss: 0.00133214
Iteration 16/25 | Loss: 0.00133214
Iteration 17/25 | Loss: 0.00133214
Iteration 18/25 | Loss: 0.00133214
Iteration 19/25 | Loss: 0.00133214
Iteration 20/25 | Loss: 0.00133214
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.001332139945589006, 0.001332139945589006, 0.001332139945589006, 0.001332139945589006, 0.001332139945589006]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001332139945589006

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00133214
Iteration 2/1000 | Loss: 0.00002342
Iteration 3/1000 | Loss: 0.00002035
Iteration 4/1000 | Loss: 0.00001706
Iteration 5/1000 | Loss: 0.00001650
Iteration 6/1000 | Loss: 0.00001591
Iteration 7/1000 | Loss: 0.00004850
Iteration 8/1000 | Loss: 0.00001537
Iteration 9/1000 | Loss: 0.00018003
Iteration 10/1000 | Loss: 0.00010513
Iteration 11/1000 | Loss: 0.00001519
Iteration 12/1000 | Loss: 0.00016152
Iteration 13/1000 | Loss: 0.00006349
Iteration 14/1000 | Loss: 0.00010645
Iteration 15/1000 | Loss: 0.00017488
Iteration 16/1000 | Loss: 0.00001809
Iteration 17/1000 | Loss: 0.00001545
Iteration 18/1000 | Loss: 0.00001473
Iteration 19/1000 | Loss: 0.00014343
Iteration 20/1000 | Loss: 0.00013388
Iteration 21/1000 | Loss: 0.00020548
Iteration 22/1000 | Loss: 0.00013930
Iteration 23/1000 | Loss: 0.00014123
Iteration 24/1000 | Loss: 0.00014460
Iteration 25/1000 | Loss: 0.00013738
Iteration 26/1000 | Loss: 0.00014784
Iteration 27/1000 | Loss: 0.00002093
Iteration 28/1000 | Loss: 0.00002395
Iteration 29/1000 | Loss: 0.00001758
Iteration 30/1000 | Loss: 0.00001663
Iteration 31/1000 | Loss: 0.00001584
Iteration 32/1000 | Loss: 0.00001499
Iteration 33/1000 | Loss: 0.00004624
Iteration 34/1000 | Loss: 0.00002247
Iteration 35/1000 | Loss: 0.00001416
Iteration 36/1000 | Loss: 0.00001863
Iteration 37/1000 | Loss: 0.00001380
Iteration 38/1000 | Loss: 0.00001375
Iteration 39/1000 | Loss: 0.00001374
Iteration 40/1000 | Loss: 0.00001374
Iteration 41/1000 | Loss: 0.00003303
Iteration 42/1000 | Loss: 0.00001376
Iteration 43/1000 | Loss: 0.00001362
Iteration 44/1000 | Loss: 0.00001362
Iteration 45/1000 | Loss: 0.00001361
Iteration 46/1000 | Loss: 0.00001361
Iteration 47/1000 | Loss: 0.00001360
Iteration 48/1000 | Loss: 0.00001360
Iteration 49/1000 | Loss: 0.00001360
Iteration 50/1000 | Loss: 0.00001360
Iteration 51/1000 | Loss: 0.00001359
Iteration 52/1000 | Loss: 0.00001359
Iteration 53/1000 | Loss: 0.00001359
Iteration 54/1000 | Loss: 0.00001359
Iteration 55/1000 | Loss: 0.00001359
Iteration 56/1000 | Loss: 0.00001359
Iteration 57/1000 | Loss: 0.00001359
Iteration 58/1000 | Loss: 0.00001359
Iteration 59/1000 | Loss: 0.00001359
Iteration 60/1000 | Loss: 0.00001359
Iteration 61/1000 | Loss: 0.00001359
Iteration 62/1000 | Loss: 0.00001359
Iteration 63/1000 | Loss: 0.00001358
Iteration 64/1000 | Loss: 0.00001358
Iteration 65/1000 | Loss: 0.00001358
Iteration 66/1000 | Loss: 0.00001358
Iteration 67/1000 | Loss: 0.00001358
Iteration 68/1000 | Loss: 0.00001358
Iteration 69/1000 | Loss: 0.00001358
Iteration 70/1000 | Loss: 0.00001358
Iteration 71/1000 | Loss: 0.00001358
Iteration 72/1000 | Loss: 0.00001358
Iteration 73/1000 | Loss: 0.00001358
Iteration 74/1000 | Loss: 0.00001358
Iteration 75/1000 | Loss: 0.00001358
Iteration 76/1000 | Loss: 0.00001358
Iteration 77/1000 | Loss: 0.00001358
Iteration 78/1000 | Loss: 0.00001357
Iteration 79/1000 | Loss: 0.00001357
Iteration 80/1000 | Loss: 0.00001357
Iteration 81/1000 | Loss: 0.00001357
Iteration 82/1000 | Loss: 0.00001357
Iteration 83/1000 | Loss: 0.00001357
Iteration 84/1000 | Loss: 0.00001357
Iteration 85/1000 | Loss: 0.00001357
Iteration 86/1000 | Loss: 0.00001357
Iteration 87/1000 | Loss: 0.00001357
Iteration 88/1000 | Loss: 0.00001357
Iteration 89/1000 | Loss: 0.00001357
Iteration 90/1000 | Loss: 0.00001357
Iteration 91/1000 | Loss: 0.00001357
Iteration 92/1000 | Loss: 0.00001356
Iteration 93/1000 | Loss: 0.00001356
Iteration 94/1000 | Loss: 0.00001356
Iteration 95/1000 | Loss: 0.00001356
Iteration 96/1000 | Loss: 0.00001356
Iteration 97/1000 | Loss: 0.00001356
Iteration 98/1000 | Loss: 0.00001356
Iteration 99/1000 | Loss: 0.00001356
Iteration 100/1000 | Loss: 0.00001355
Iteration 101/1000 | Loss: 0.00001355
Iteration 102/1000 | Loss: 0.00001355
Iteration 103/1000 | Loss: 0.00001354
Iteration 104/1000 | Loss: 0.00001354
Iteration 105/1000 | Loss: 0.00001354
Iteration 106/1000 | Loss: 0.00001354
Iteration 107/1000 | Loss: 0.00001354
Iteration 108/1000 | Loss: 0.00001354
Iteration 109/1000 | Loss: 0.00001354
Iteration 110/1000 | Loss: 0.00001354
Iteration 111/1000 | Loss: 0.00001353
Iteration 112/1000 | Loss: 0.00001353
Iteration 113/1000 | Loss: 0.00001353
Iteration 114/1000 | Loss: 0.00001352
Iteration 115/1000 | Loss: 0.00001352
Iteration 116/1000 | Loss: 0.00001352
Iteration 117/1000 | Loss: 0.00001352
Iteration 118/1000 | Loss: 0.00005580
Iteration 119/1000 | Loss: 0.00001476
Iteration 120/1000 | Loss: 0.00002191
Iteration 121/1000 | Loss: 0.00001355
Iteration 122/1000 | Loss: 0.00001350
Iteration 123/1000 | Loss: 0.00001349
Iteration 124/1000 | Loss: 0.00001349
Iteration 125/1000 | Loss: 0.00001348
Iteration 126/1000 | Loss: 0.00001348
Iteration 127/1000 | Loss: 0.00001347
Iteration 128/1000 | Loss: 0.00001347
Iteration 129/1000 | Loss: 0.00001347
Iteration 130/1000 | Loss: 0.00001347
Iteration 131/1000 | Loss: 0.00001347
Iteration 132/1000 | Loss: 0.00001347
Iteration 133/1000 | Loss: 0.00001347
Iteration 134/1000 | Loss: 0.00001347
Iteration 135/1000 | Loss: 0.00001346
Iteration 136/1000 | Loss: 0.00001346
Iteration 137/1000 | Loss: 0.00001346
Iteration 138/1000 | Loss: 0.00001346
Iteration 139/1000 | Loss: 0.00001346
Iteration 140/1000 | Loss: 0.00001346
Iteration 141/1000 | Loss: 0.00001346
Iteration 142/1000 | Loss: 0.00001346
Iteration 143/1000 | Loss: 0.00001345
Iteration 144/1000 | Loss: 0.00001345
Iteration 145/1000 | Loss: 0.00001345
Iteration 146/1000 | Loss: 0.00001344
Iteration 147/1000 | Loss: 0.00001344
Iteration 148/1000 | Loss: 0.00001344
Iteration 149/1000 | Loss: 0.00001343
Iteration 150/1000 | Loss: 0.00001343
Iteration 151/1000 | Loss: 0.00001343
Iteration 152/1000 | Loss: 0.00001342
Iteration 153/1000 | Loss: 0.00001342
Iteration 154/1000 | Loss: 0.00001342
Iteration 155/1000 | Loss: 0.00001342
Iteration 156/1000 | Loss: 0.00001342
Iteration 157/1000 | Loss: 0.00001342
Iteration 158/1000 | Loss: 0.00001342
Iteration 159/1000 | Loss: 0.00001342
Iteration 160/1000 | Loss: 0.00001342
Iteration 161/1000 | Loss: 0.00001342
Iteration 162/1000 | Loss: 0.00001342
Iteration 163/1000 | Loss: 0.00001342
Iteration 164/1000 | Loss: 0.00001341
Iteration 165/1000 | Loss: 0.00001341
Iteration 166/1000 | Loss: 0.00001341
Iteration 167/1000 | Loss: 0.00001341
Iteration 168/1000 | Loss: 0.00001341
Iteration 169/1000 | Loss: 0.00001341
Iteration 170/1000 | Loss: 0.00001341
Iteration 171/1000 | Loss: 0.00001341
Iteration 172/1000 | Loss: 0.00001341
Iteration 173/1000 | Loss: 0.00001341
Iteration 174/1000 | Loss: 0.00001341
Iteration 175/1000 | Loss: 0.00001341
Iteration 176/1000 | Loss: 0.00001341
Iteration 177/1000 | Loss: 0.00001341
Iteration 178/1000 | Loss: 0.00001340
Iteration 179/1000 | Loss: 0.00001340
Iteration 180/1000 | Loss: 0.00001340
Iteration 181/1000 | Loss: 0.00001340
Iteration 182/1000 | Loss: 0.00001340
Iteration 183/1000 | Loss: 0.00001340
Iteration 184/1000 | Loss: 0.00001340
Iteration 185/1000 | Loss: 0.00001340
Iteration 186/1000 | Loss: 0.00001340
Iteration 187/1000 | Loss: 0.00001340
Iteration 188/1000 | Loss: 0.00001340
Iteration 189/1000 | Loss: 0.00001340
Iteration 190/1000 | Loss: 0.00005433
Iteration 191/1000 | Loss: 0.00001504
Iteration 192/1000 | Loss: 0.00001345
Iteration 193/1000 | Loss: 0.00001519
Iteration 194/1000 | Loss: 0.00001341
Iteration 195/1000 | Loss: 0.00001340
Iteration 196/1000 | Loss: 0.00001340
Iteration 197/1000 | Loss: 0.00001339
Iteration 198/1000 | Loss: 0.00001339
Iteration 199/1000 | Loss: 0.00001339
Iteration 200/1000 | Loss: 0.00001338
Iteration 201/1000 | Loss: 0.00001338
Iteration 202/1000 | Loss: 0.00001338
Iteration 203/1000 | Loss: 0.00001338
Iteration 204/1000 | Loss: 0.00001337
Iteration 205/1000 | Loss: 0.00001337
Iteration 206/1000 | Loss: 0.00001337
Iteration 207/1000 | Loss: 0.00001336
Iteration 208/1000 | Loss: 0.00001336
Iteration 209/1000 | Loss: 0.00001336
Iteration 210/1000 | Loss: 0.00001336
Iteration 211/1000 | Loss: 0.00001335
Iteration 212/1000 | Loss: 0.00003498
Iteration 213/1000 | Loss: 0.00001376
Iteration 214/1000 | Loss: 0.00001343
Iteration 215/1000 | Loss: 0.00001343
Iteration 216/1000 | Loss: 0.00001343
Iteration 217/1000 | Loss: 0.00001343
Iteration 218/1000 | Loss: 0.00001343
Iteration 219/1000 | Loss: 0.00001342
Iteration 220/1000 | Loss: 0.00001342
Iteration 221/1000 | Loss: 0.00001342
Iteration 222/1000 | Loss: 0.00001342
Iteration 223/1000 | Loss: 0.00001342
Iteration 224/1000 | Loss: 0.00001342
Iteration 225/1000 | Loss: 0.00001342
Iteration 226/1000 | Loss: 0.00001342
Iteration 227/1000 | Loss: 0.00001342
Iteration 228/1000 | Loss: 0.00001342
Iteration 229/1000 | Loss: 0.00001342
Iteration 230/1000 | Loss: 0.00001341
Iteration 231/1000 | Loss: 0.00001341
Iteration 232/1000 | Loss: 0.00001354
Iteration 233/1000 | Loss: 0.00001376
Iteration 234/1000 | Loss: 0.00001345
Iteration 235/1000 | Loss: 0.00001341
Iteration 236/1000 | Loss: 0.00001340
Iteration 237/1000 | Loss: 0.00001340
Iteration 238/1000 | Loss: 0.00001340
Iteration 239/1000 | Loss: 0.00001340
Iteration 240/1000 | Loss: 0.00001340
Iteration 241/1000 | Loss: 0.00001340
Iteration 242/1000 | Loss: 0.00001340
Iteration 243/1000 | Loss: 0.00001340
Iteration 244/1000 | Loss: 0.00001340
Iteration 245/1000 | Loss: 0.00001340
Iteration 246/1000 | Loss: 0.00001340
Iteration 247/1000 | Loss: 0.00001340
Iteration 248/1000 | Loss: 0.00001340
Iteration 249/1000 | Loss: 0.00001340
Iteration 250/1000 | Loss: 0.00001340
Iteration 251/1000 | Loss: 0.00001340
Iteration 252/1000 | Loss: 0.00001340
Iteration 253/1000 | Loss: 0.00001340
Iteration 254/1000 | Loss: 0.00001340
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 254. Stopping optimization.
Last 5 losses: [1.3396048416325357e-05, 1.3396048416325357e-05, 1.3396048416325357e-05, 1.3396048416325357e-05, 1.3396048416325357e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3396048416325357e-05

Optimization complete. Final v2v error: 3.089049816131592 mm

Highest mean error: 4.1403398513793945 mm for frame 7

Lowest mean error: 2.6667113304138184 mm for frame 207

Saving results

Total time: 128.83385348320007
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_015/1062/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_015/1062.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_015/1062
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00973493
Iteration 2/25 | Loss: 0.00290345
Iteration 3/25 | Loss: 0.00172748
Iteration 4/25 | Loss: 0.00162424
Iteration 5/25 | Loss: 0.00153824
Iteration 6/25 | Loss: 0.00163821
Iteration 7/25 | Loss: 0.00142856
Iteration 8/25 | Loss: 0.00130349
Iteration 9/25 | Loss: 0.00125737
Iteration 10/25 | Loss: 0.00124405
Iteration 11/25 | Loss: 0.00124967
Iteration 12/25 | Loss: 0.00120325
Iteration 13/25 | Loss: 0.00119086
Iteration 14/25 | Loss: 0.00118956
Iteration 15/25 | Loss: 0.00118681
Iteration 16/25 | Loss: 0.00118418
Iteration 17/25 | Loss: 0.00118335
Iteration 18/25 | Loss: 0.00118328
Iteration 19/25 | Loss: 0.00118328
Iteration 20/25 | Loss: 0.00118328
Iteration 21/25 | Loss: 0.00118328
Iteration 22/25 | Loss: 0.00118328
Iteration 23/25 | Loss: 0.00118328
Iteration 24/25 | Loss: 0.00118328
Iteration 25/25 | Loss: 0.00118328

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.28737950
Iteration 2/25 | Loss: 0.00157271
Iteration 3/25 | Loss: 0.00157271
Iteration 4/25 | Loss: 0.00157271
Iteration 5/25 | Loss: 0.00157271
Iteration 6/25 | Loss: 0.00157271
Iteration 7/25 | Loss: 0.00157271
Iteration 8/25 | Loss: 0.00157271
Iteration 9/25 | Loss: 0.00157271
Iteration 10/25 | Loss: 0.00157271
Iteration 11/25 | Loss: 0.00157271
Iteration 12/25 | Loss: 0.00157271
Iteration 13/25 | Loss: 0.00157271
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0015727076679468155, 0.0015727076679468155, 0.0015727076679468155, 0.0015727076679468155, 0.0015727076679468155]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015727076679468155

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00157271
Iteration 2/1000 | Loss: 0.00014871
Iteration 3/1000 | Loss: 0.00100994
Iteration 4/1000 | Loss: 0.00024883
Iteration 5/1000 | Loss: 0.00003422
Iteration 6/1000 | Loss: 0.00008473
Iteration 7/1000 | Loss: 0.00013447
Iteration 8/1000 | Loss: 0.00002278
Iteration 9/1000 | Loss: 0.00007482
Iteration 10/1000 | Loss: 0.00006365
Iteration 11/1000 | Loss: 0.00004456
Iteration 12/1000 | Loss: 0.00021212
Iteration 13/1000 | Loss: 0.00001688
Iteration 14/1000 | Loss: 0.00001449
Iteration 15/1000 | Loss: 0.00002673
Iteration 16/1000 | Loss: 0.00033712
Iteration 17/1000 | Loss: 0.00003089
Iteration 18/1000 | Loss: 0.00004352
Iteration 19/1000 | Loss: 0.00001948
Iteration 20/1000 | Loss: 0.00003031
Iteration 21/1000 | Loss: 0.00002185
Iteration 22/1000 | Loss: 0.00001438
Iteration 23/1000 | Loss: 0.00002463
Iteration 24/1000 | Loss: 0.00008413
Iteration 25/1000 | Loss: 0.00002766
Iteration 26/1000 | Loss: 0.00002279
Iteration 27/1000 | Loss: 0.00001591
Iteration 28/1000 | Loss: 0.00001678
Iteration 29/1000 | Loss: 0.00001363
Iteration 30/1000 | Loss: 0.00001860
Iteration 31/1000 | Loss: 0.00001498
Iteration 32/1000 | Loss: 0.00002036
Iteration 33/1000 | Loss: 0.00006295
Iteration 34/1000 | Loss: 0.00001882
Iteration 35/1000 | Loss: 0.00009117
Iteration 36/1000 | Loss: 0.00008591
Iteration 37/1000 | Loss: 0.00003099
Iteration 38/1000 | Loss: 0.00001430
Iteration 39/1000 | Loss: 0.00001350
Iteration 40/1000 | Loss: 0.00001349
Iteration 41/1000 | Loss: 0.00001349
Iteration 42/1000 | Loss: 0.00001349
Iteration 43/1000 | Loss: 0.00001349
Iteration 44/1000 | Loss: 0.00001349
Iteration 45/1000 | Loss: 0.00001349
Iteration 46/1000 | Loss: 0.00001348
Iteration 47/1000 | Loss: 0.00001347
Iteration 48/1000 | Loss: 0.00002195
Iteration 49/1000 | Loss: 0.00001348
Iteration 50/1000 | Loss: 0.00001355
Iteration 51/1000 | Loss: 0.00001642
Iteration 52/1000 | Loss: 0.00001345
Iteration 53/1000 | Loss: 0.00001345
Iteration 54/1000 | Loss: 0.00001344
Iteration 55/1000 | Loss: 0.00001344
Iteration 56/1000 | Loss: 0.00001344
Iteration 57/1000 | Loss: 0.00001344
Iteration 58/1000 | Loss: 0.00001344
Iteration 59/1000 | Loss: 0.00001344
Iteration 60/1000 | Loss: 0.00001344
Iteration 61/1000 | Loss: 0.00001344
Iteration 62/1000 | Loss: 0.00001344
Iteration 63/1000 | Loss: 0.00001344
Iteration 64/1000 | Loss: 0.00001344
Iteration 65/1000 | Loss: 0.00001344
Iteration 66/1000 | Loss: 0.00001344
Iteration 67/1000 | Loss: 0.00001344
Iteration 68/1000 | Loss: 0.00001344
Iteration 69/1000 | Loss: 0.00001344
Iteration 70/1000 | Loss: 0.00001344
Iteration 71/1000 | Loss: 0.00001344
Iteration 72/1000 | Loss: 0.00001344
Iteration 73/1000 | Loss: 0.00001344
Iteration 74/1000 | Loss: 0.00001344
Iteration 75/1000 | Loss: 0.00001344
Iteration 76/1000 | Loss: 0.00001344
Iteration 77/1000 | Loss: 0.00001344
Iteration 78/1000 | Loss: 0.00001344
Iteration 79/1000 | Loss: 0.00001344
Iteration 80/1000 | Loss: 0.00001344
Iteration 81/1000 | Loss: 0.00001344
Iteration 82/1000 | Loss: 0.00001344
Iteration 83/1000 | Loss: 0.00001344
Iteration 84/1000 | Loss: 0.00001344
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 84. Stopping optimization.
Last 5 losses: [1.3436913832265418e-05, 1.3436913832265418e-05, 1.3436913832265418e-05, 1.3436913832265418e-05, 1.3436913832265418e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3436913832265418e-05

Optimization complete. Final v2v error: 3.1115477085113525 mm

Highest mean error: 4.017330169677734 mm for frame 80

Lowest mean error: 2.7601025104522705 mm for frame 139

Saving results

Total time: 88.16719174385071
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_015/1032/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_015/1032.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_015/1032
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01048790
Iteration 2/25 | Loss: 0.01048790
Iteration 3/25 | Loss: 0.00166166
Iteration 4/25 | Loss: 0.00126042
Iteration 5/25 | Loss: 0.00129769
Iteration 6/25 | Loss: 0.00118242
Iteration 7/25 | Loss: 0.00115848
Iteration 8/25 | Loss: 0.00114521
Iteration 9/25 | Loss: 0.00114039
Iteration 10/25 | Loss: 0.00112805
Iteration 11/25 | Loss: 0.00112402
Iteration 12/25 | Loss: 0.00112490
Iteration 13/25 | Loss: 0.00112074
Iteration 14/25 | Loss: 0.00111939
Iteration 15/25 | Loss: 0.00111911
Iteration 16/25 | Loss: 0.00111904
Iteration 17/25 | Loss: 0.00111902
Iteration 18/25 | Loss: 0.00111901
Iteration 19/25 | Loss: 0.00111901
Iteration 20/25 | Loss: 0.00111901
Iteration 21/25 | Loss: 0.00111900
Iteration 22/25 | Loss: 0.00111900
Iteration 23/25 | Loss: 0.00111900
Iteration 24/25 | Loss: 0.00111899
Iteration 25/25 | Loss: 0.00111899

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.21616018
Iteration 2/25 | Loss: 0.00183015
Iteration 3/25 | Loss: 0.00184108
Iteration 4/25 | Loss: 0.00177311
Iteration 5/25 | Loss: 0.00177311
Iteration 6/25 | Loss: 0.00177311
Iteration 7/25 | Loss: 0.00177311
Iteration 8/25 | Loss: 0.00177311
Iteration 9/25 | Loss: 0.00177311
Iteration 10/25 | Loss: 0.00177311
Iteration 11/25 | Loss: 0.00177311
Iteration 12/25 | Loss: 0.00177311
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0017731094267219305, 0.0017731094267219305, 0.0017731094267219305, 0.0017731094267219305, 0.0017731094267219305]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0017731094267219305

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00177311
Iteration 2/1000 | Loss: 0.00013985
Iteration 3/1000 | Loss: 0.00013696
Iteration 4/1000 | Loss: 0.00001683
Iteration 5/1000 | Loss: 0.00001525
Iteration 6/1000 | Loss: 0.00001418
Iteration 7/1000 | Loss: 0.00001379
Iteration 8/1000 | Loss: 0.00001352
Iteration 9/1000 | Loss: 0.00001314
Iteration 10/1000 | Loss: 0.00001300
Iteration 11/1000 | Loss: 0.00001300
Iteration 12/1000 | Loss: 0.00001288
Iteration 13/1000 | Loss: 0.00001282
Iteration 14/1000 | Loss: 0.00001263
Iteration 15/1000 | Loss: 0.00001261
Iteration 16/1000 | Loss: 0.00001256
Iteration 17/1000 | Loss: 0.00001253
Iteration 18/1000 | Loss: 0.00001253
Iteration 19/1000 | Loss: 0.00001251
Iteration 20/1000 | Loss: 0.00001251
Iteration 21/1000 | Loss: 0.00001250
Iteration 22/1000 | Loss: 0.00001249
Iteration 23/1000 | Loss: 0.00001248
Iteration 24/1000 | Loss: 0.00001247
Iteration 25/1000 | Loss: 0.00001247
Iteration 26/1000 | Loss: 0.00001246
Iteration 27/1000 | Loss: 0.00001245
Iteration 28/1000 | Loss: 0.00001242
Iteration 29/1000 | Loss: 0.00001241
Iteration 30/1000 | Loss: 0.00001241
Iteration 31/1000 | Loss: 0.00001232
Iteration 32/1000 | Loss: 0.00001225
Iteration 33/1000 | Loss: 0.00001225
Iteration 34/1000 | Loss: 0.00001223
Iteration 35/1000 | Loss: 0.00001223
Iteration 36/1000 | Loss: 0.00001222
Iteration 37/1000 | Loss: 0.00001222
Iteration 38/1000 | Loss: 0.00001221
Iteration 39/1000 | Loss: 0.00001220
Iteration 40/1000 | Loss: 0.00001219
Iteration 41/1000 | Loss: 0.00001219
Iteration 42/1000 | Loss: 0.00001219
Iteration 43/1000 | Loss: 0.00001218
Iteration 44/1000 | Loss: 0.00001217
Iteration 45/1000 | Loss: 0.00001217
Iteration 46/1000 | Loss: 0.00001217
Iteration 47/1000 | Loss: 0.00001216
Iteration 48/1000 | Loss: 0.00001216
Iteration 49/1000 | Loss: 0.00001216
Iteration 50/1000 | Loss: 0.00001215
Iteration 51/1000 | Loss: 0.00001214
Iteration 52/1000 | Loss: 0.00001214
Iteration 53/1000 | Loss: 0.00001214
Iteration 54/1000 | Loss: 0.00001214
Iteration 55/1000 | Loss: 0.00001214
Iteration 56/1000 | Loss: 0.00001213
Iteration 57/1000 | Loss: 0.00001213
Iteration 58/1000 | Loss: 0.00001213
Iteration 59/1000 | Loss: 0.00001213
Iteration 60/1000 | Loss: 0.00001213
Iteration 61/1000 | Loss: 0.00001212
Iteration 62/1000 | Loss: 0.00001212
Iteration 63/1000 | Loss: 0.00001212
Iteration 64/1000 | Loss: 0.00001212
Iteration 65/1000 | Loss: 0.00001212
Iteration 66/1000 | Loss: 0.00001212
Iteration 67/1000 | Loss: 0.00001212
Iteration 68/1000 | Loss: 0.00001212
Iteration 69/1000 | Loss: 0.00001212
Iteration 70/1000 | Loss: 0.00001212
Iteration 71/1000 | Loss: 0.00001212
Iteration 72/1000 | Loss: 0.00001212
Iteration 73/1000 | Loss: 0.00001211
Iteration 74/1000 | Loss: 0.00001211
Iteration 75/1000 | Loss: 0.00001211
Iteration 76/1000 | Loss: 0.00001211
Iteration 77/1000 | Loss: 0.00001210
Iteration 78/1000 | Loss: 0.00001210
Iteration 79/1000 | Loss: 0.00001210
Iteration 80/1000 | Loss: 0.00001210
Iteration 81/1000 | Loss: 0.00001210
Iteration 82/1000 | Loss: 0.00001210
Iteration 83/1000 | Loss: 0.00001209
Iteration 84/1000 | Loss: 0.00001209
Iteration 85/1000 | Loss: 0.00001209
Iteration 86/1000 | Loss: 0.00001445
Iteration 87/1000 | Loss: 0.00001250
Iteration 88/1000 | Loss: 0.00001232
Iteration 89/1000 | Loss: 0.00001230
Iteration 90/1000 | Loss: 0.00001211
Iteration 91/1000 | Loss: 0.00001867
Iteration 92/1000 | Loss: 0.00001199
Iteration 93/1000 | Loss: 0.00001197
Iteration 94/1000 | Loss: 0.00001775
Iteration 95/1000 | Loss: 0.00001775
Iteration 96/1000 | Loss: 0.00001178
Iteration 97/1000 | Loss: 0.00002733
Iteration 98/1000 | Loss: 0.00001468
Iteration 99/1000 | Loss: 0.00001172
Iteration 100/1000 | Loss: 0.00001155
Iteration 101/1000 | Loss: 0.00001137
Iteration 102/1000 | Loss: 0.00001125
Iteration 103/1000 | Loss: 0.00001125
Iteration 104/1000 | Loss: 0.00001124
Iteration 105/1000 | Loss: 0.00001118
Iteration 106/1000 | Loss: 0.00001111
Iteration 107/1000 | Loss: 0.00003386
Iteration 108/1000 | Loss: 0.00001371
Iteration 109/1000 | Loss: 0.00001099
Iteration 110/1000 | Loss: 0.00001099
Iteration 111/1000 | Loss: 0.00001099
Iteration 112/1000 | Loss: 0.00001099
Iteration 113/1000 | Loss: 0.00001099
Iteration 114/1000 | Loss: 0.00001099
Iteration 115/1000 | Loss: 0.00001099
Iteration 116/1000 | Loss: 0.00001099
Iteration 117/1000 | Loss: 0.00001098
Iteration 118/1000 | Loss: 0.00001098
Iteration 119/1000 | Loss: 0.00001098
Iteration 120/1000 | Loss: 0.00001098
Iteration 121/1000 | Loss: 0.00001098
Iteration 122/1000 | Loss: 0.00001098
Iteration 123/1000 | Loss: 0.00001098
Iteration 124/1000 | Loss: 0.00001098
Iteration 125/1000 | Loss: 0.00001098
Iteration 126/1000 | Loss: 0.00001097
Iteration 127/1000 | Loss: 0.00001097
Iteration 128/1000 | Loss: 0.00001096
Iteration 129/1000 | Loss: 0.00001096
Iteration 130/1000 | Loss: 0.00001096
Iteration 131/1000 | Loss: 0.00001095
Iteration 132/1000 | Loss: 0.00001095
Iteration 133/1000 | Loss: 0.00001095
Iteration 134/1000 | Loss: 0.00001095
Iteration 135/1000 | Loss: 0.00001094
Iteration 136/1000 | Loss: 0.00001094
Iteration 137/1000 | Loss: 0.00001094
Iteration 138/1000 | Loss: 0.00001094
Iteration 139/1000 | Loss: 0.00001094
Iteration 140/1000 | Loss: 0.00001093
Iteration 141/1000 | Loss: 0.00001093
Iteration 142/1000 | Loss: 0.00001093
Iteration 143/1000 | Loss: 0.00001093
Iteration 144/1000 | Loss: 0.00001093
Iteration 145/1000 | Loss: 0.00001093
Iteration 146/1000 | Loss: 0.00001093
Iteration 147/1000 | Loss: 0.00001092
Iteration 148/1000 | Loss: 0.00001092
Iteration 149/1000 | Loss: 0.00001092
Iteration 150/1000 | Loss: 0.00001092
Iteration 151/1000 | Loss: 0.00001092
Iteration 152/1000 | Loss: 0.00001092
Iteration 153/1000 | Loss: 0.00001092
Iteration 154/1000 | Loss: 0.00001092
Iteration 155/1000 | Loss: 0.00001092
Iteration 156/1000 | Loss: 0.00001091
Iteration 157/1000 | Loss: 0.00001091
Iteration 158/1000 | Loss: 0.00001091
Iteration 159/1000 | Loss: 0.00001090
Iteration 160/1000 | Loss: 0.00001090
Iteration 161/1000 | Loss: 0.00001090
Iteration 162/1000 | Loss: 0.00001090
Iteration 163/1000 | Loss: 0.00001089
Iteration 164/1000 | Loss: 0.00001089
Iteration 165/1000 | Loss: 0.00001089
Iteration 166/1000 | Loss: 0.00001089
Iteration 167/1000 | Loss: 0.00001089
Iteration 168/1000 | Loss: 0.00001089
Iteration 169/1000 | Loss: 0.00001089
Iteration 170/1000 | Loss: 0.00001089
Iteration 171/1000 | Loss: 0.00001089
Iteration 172/1000 | Loss: 0.00001088
Iteration 173/1000 | Loss: 0.00001088
Iteration 174/1000 | Loss: 0.00001088
Iteration 175/1000 | Loss: 0.00001088
Iteration 176/1000 | Loss: 0.00001088
Iteration 177/1000 | Loss: 0.00001088
Iteration 178/1000 | Loss: 0.00001088
Iteration 179/1000 | Loss: 0.00001087
Iteration 180/1000 | Loss: 0.00001087
Iteration 181/1000 | Loss: 0.00001087
Iteration 182/1000 | Loss: 0.00001087
Iteration 183/1000 | Loss: 0.00001087
Iteration 184/1000 | Loss: 0.00001087
Iteration 185/1000 | Loss: 0.00001087
Iteration 186/1000 | Loss: 0.00001087
Iteration 187/1000 | Loss: 0.00001087
Iteration 188/1000 | Loss: 0.00001087
Iteration 189/1000 | Loss: 0.00001086
Iteration 190/1000 | Loss: 0.00001086
Iteration 191/1000 | Loss: 0.00001086
Iteration 192/1000 | Loss: 0.00001086
Iteration 193/1000 | Loss: 0.00001086
Iteration 194/1000 | Loss: 0.00001086
Iteration 195/1000 | Loss: 0.00001086
Iteration 196/1000 | Loss: 0.00001086
Iteration 197/1000 | Loss: 0.00001085
Iteration 198/1000 | Loss: 0.00001085
Iteration 199/1000 | Loss: 0.00001085
Iteration 200/1000 | Loss: 0.00001085
Iteration 201/1000 | Loss: 0.00001085
Iteration 202/1000 | Loss: 0.00001085
Iteration 203/1000 | Loss: 0.00001085
Iteration 204/1000 | Loss: 0.00001085
Iteration 205/1000 | Loss: 0.00001085
Iteration 206/1000 | Loss: 0.00001085
Iteration 207/1000 | Loss: 0.00001085
Iteration 208/1000 | Loss: 0.00001085
Iteration 209/1000 | Loss: 0.00001084
Iteration 210/1000 | Loss: 0.00001084
Iteration 211/1000 | Loss: 0.00001084
Iteration 212/1000 | Loss: 0.00001084
Iteration 213/1000 | Loss: 0.00001084
Iteration 214/1000 | Loss: 0.00001084
Iteration 215/1000 | Loss: 0.00001084
Iteration 216/1000 | Loss: 0.00001083
Iteration 217/1000 | Loss: 0.00001083
Iteration 218/1000 | Loss: 0.00001083
Iteration 219/1000 | Loss: 0.00001083
Iteration 220/1000 | Loss: 0.00001083
Iteration 221/1000 | Loss: 0.00001082
Iteration 222/1000 | Loss: 0.00001082
Iteration 223/1000 | Loss: 0.00001082
Iteration 224/1000 | Loss: 0.00001082
Iteration 225/1000 | Loss: 0.00001082
Iteration 226/1000 | Loss: 0.00001081
Iteration 227/1000 | Loss: 0.00001081
Iteration 228/1000 | Loss: 0.00001081
Iteration 229/1000 | Loss: 0.00001081
Iteration 230/1000 | Loss: 0.00001081
Iteration 231/1000 | Loss: 0.00001081
Iteration 232/1000 | Loss: 0.00001081
Iteration 233/1000 | Loss: 0.00001081
Iteration 234/1000 | Loss: 0.00001081
Iteration 235/1000 | Loss: 0.00001081
Iteration 236/1000 | Loss: 0.00001081
Iteration 237/1000 | Loss: 0.00001081
Iteration 238/1000 | Loss: 0.00001081
Iteration 239/1000 | Loss: 0.00001081
Iteration 240/1000 | Loss: 0.00001928
Iteration 241/1000 | Loss: 0.00001500
Iteration 242/1000 | Loss: 0.00001639
Iteration 243/1000 | Loss: 0.00001381
Iteration 244/1000 | Loss: 0.00001178
Iteration 245/1000 | Loss: 0.00001079
Iteration 246/1000 | Loss: 0.00001079
Iteration 247/1000 | Loss: 0.00001079
Iteration 248/1000 | Loss: 0.00001078
Iteration 249/1000 | Loss: 0.00001078
Iteration 250/1000 | Loss: 0.00001078
Iteration 251/1000 | Loss: 0.00001078
Iteration 252/1000 | Loss: 0.00001078
Iteration 253/1000 | Loss: 0.00001078
Iteration 254/1000 | Loss: 0.00001078
Iteration 255/1000 | Loss: 0.00001078
Iteration 256/1000 | Loss: 0.00001078
Iteration 257/1000 | Loss: 0.00001078
Iteration 258/1000 | Loss: 0.00001078
Iteration 259/1000 | Loss: 0.00001078
Iteration 260/1000 | Loss: 0.00001078
Iteration 261/1000 | Loss: 0.00001078
Iteration 262/1000 | Loss: 0.00001078
Iteration 263/1000 | Loss: 0.00001078
Iteration 264/1000 | Loss: 0.00001078
Iteration 265/1000 | Loss: 0.00001078
Iteration 266/1000 | Loss: 0.00001078
Iteration 267/1000 | Loss: 0.00001078
Iteration 268/1000 | Loss: 0.00001078
Iteration 269/1000 | Loss: 0.00001078
Iteration 270/1000 | Loss: 0.00001078
Iteration 271/1000 | Loss: 0.00001078
Iteration 272/1000 | Loss: 0.00001078
Iteration 273/1000 | Loss: 0.00001078
Iteration 274/1000 | Loss: 0.00001078
Iteration 275/1000 | Loss: 0.00001078
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 275. Stopping optimization.
Last 5 losses: [1.078264722309541e-05, 1.078264722309541e-05, 1.078264722309541e-05, 1.078264722309541e-05, 1.078264722309541e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.078264722309541e-05

Optimization complete. Final v2v error: 2.797548532485962 mm

Highest mean error: 4.966920852661133 mm for frame 150

Lowest mean error: 2.487307548522949 mm for frame 87

Saving results

Total time: 88.37749004364014
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_015/1040/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_015/1040.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_015/1040
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01040298
Iteration 2/25 | Loss: 0.01040298
Iteration 3/25 | Loss: 0.01040298
Iteration 4/25 | Loss: 0.01040297
Iteration 5/25 | Loss: 0.01040297
Iteration 6/25 | Loss: 0.01040297
Iteration 7/25 | Loss: 0.01040297
Iteration 8/25 | Loss: 0.01040297
Iteration 9/25 | Loss: 0.01040296
Iteration 10/25 | Loss: 0.01040296
Iteration 11/25 | Loss: 0.01040296
Iteration 12/25 | Loss: 0.01040296
Iteration 13/25 | Loss: 0.01040296
Iteration 14/25 | Loss: 0.01040295
Iteration 15/25 | Loss: 0.01040295
Iteration 16/25 | Loss: 0.01040294
Iteration 17/25 | Loss: 0.01040294
Iteration 18/25 | Loss: 0.01040294
Iteration 19/25 | Loss: 0.01040294
Iteration 20/25 | Loss: 0.01040293
Iteration 21/25 | Loss: 0.01040293
Iteration 22/25 | Loss: 0.01040293
Iteration 23/25 | Loss: 0.01040293
Iteration 24/25 | Loss: 0.01040293
Iteration 25/25 | Loss: 0.01040293

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.92967796
Iteration 2/25 | Loss: 0.09031760
Iteration 3/25 | Loss: 0.08995878
Iteration 4/25 | Loss: 0.08932789
Iteration 5/25 | Loss: 0.08932786
Iteration 6/25 | Loss: 0.08932786
Iteration 7/25 | Loss: 0.08932786
Iteration 8/25 | Loss: 0.08932783
Iteration 9/25 | Loss: 0.08932783
Iteration 10/25 | Loss: 0.08932783
Iteration 11/25 | Loss: 0.08932783
Iteration 12/25 | Loss: 0.08932783
Iteration 13/25 | Loss: 0.08932783
Iteration 14/25 | Loss: 0.08932783
Iteration 15/25 | Loss: 0.08932783
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.08932782709598541, 0.08932782709598541, 0.08932782709598541, 0.08932782709598541, 0.08932782709598541]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.08932782709598541

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.08932783
Iteration 2/1000 | Loss: 0.00175416
Iteration 3/1000 | Loss: 0.00208182
Iteration 4/1000 | Loss: 0.00210007
Iteration 5/1000 | Loss: 0.00266824
Iteration 6/1000 | Loss: 0.00321037
Iteration 7/1000 | Loss: 0.00143636
Iteration 8/1000 | Loss: 0.00118587
Iteration 9/1000 | Loss: 0.00317986
Iteration 10/1000 | Loss: 0.00011306
Iteration 11/1000 | Loss: 0.00028237
Iteration 12/1000 | Loss: 0.00010942
Iteration 13/1000 | Loss: 0.00050958
Iteration 14/1000 | Loss: 0.00187630
Iteration 15/1000 | Loss: 0.00009791
Iteration 16/1000 | Loss: 0.00007752
Iteration 17/1000 | Loss: 0.00012748
Iteration 18/1000 | Loss: 0.00028374
Iteration 19/1000 | Loss: 0.00006158
Iteration 20/1000 | Loss: 0.00010613
Iteration 21/1000 | Loss: 0.00006258
Iteration 22/1000 | Loss: 0.00006491
Iteration 23/1000 | Loss: 0.00043465
Iteration 24/1000 | Loss: 0.00009568
Iteration 25/1000 | Loss: 0.00010817
Iteration 26/1000 | Loss: 0.00008320
Iteration 27/1000 | Loss: 0.00003754
Iteration 28/1000 | Loss: 0.00005789
Iteration 29/1000 | Loss: 0.00043066
Iteration 30/1000 | Loss: 0.00019091
Iteration 31/1000 | Loss: 0.00012447
Iteration 32/1000 | Loss: 0.00003836
Iteration 33/1000 | Loss: 0.00004201
Iteration 34/1000 | Loss: 0.00027472
Iteration 35/1000 | Loss: 0.00003665
Iteration 36/1000 | Loss: 0.00004626
Iteration 37/1000 | Loss: 0.00011311
Iteration 38/1000 | Loss: 0.00006151
Iteration 39/1000 | Loss: 0.00029682
Iteration 40/1000 | Loss: 0.00090047
Iteration 41/1000 | Loss: 0.00010782
Iteration 42/1000 | Loss: 0.00015501
Iteration 43/1000 | Loss: 0.00004878
Iteration 44/1000 | Loss: 0.00018760
Iteration 45/1000 | Loss: 0.00004553
Iteration 46/1000 | Loss: 0.00013085
Iteration 47/1000 | Loss: 0.00003382
Iteration 48/1000 | Loss: 0.00003815
Iteration 49/1000 | Loss: 0.00003124
Iteration 50/1000 | Loss: 0.00011362
Iteration 51/1000 | Loss: 0.00010610
Iteration 52/1000 | Loss: 0.00040268
Iteration 53/1000 | Loss: 0.00003495
Iteration 54/1000 | Loss: 0.00003786
Iteration 55/1000 | Loss: 0.00006495
Iteration 56/1000 | Loss: 0.00003323
Iteration 57/1000 | Loss: 0.00003435
Iteration 58/1000 | Loss: 0.00003636
Iteration 59/1000 | Loss: 0.00003247
Iteration 60/1000 | Loss: 0.00002642
Iteration 61/1000 | Loss: 0.00002597
Iteration 62/1000 | Loss: 0.00005021
Iteration 63/1000 | Loss: 0.00002568
Iteration 64/1000 | Loss: 0.00002523
Iteration 65/1000 | Loss: 0.00010525
Iteration 66/1000 | Loss: 0.00005411
Iteration 67/1000 | Loss: 0.00002585
Iteration 68/1000 | Loss: 0.00002476
Iteration 69/1000 | Loss: 0.00002476
Iteration 70/1000 | Loss: 0.00002476
Iteration 71/1000 | Loss: 0.00002475
Iteration 72/1000 | Loss: 0.00004454
Iteration 73/1000 | Loss: 0.00033482
Iteration 74/1000 | Loss: 0.00005318
Iteration 75/1000 | Loss: 0.00002786
Iteration 76/1000 | Loss: 0.00002456
Iteration 77/1000 | Loss: 0.00002450
Iteration 78/1000 | Loss: 0.00003231
Iteration 79/1000 | Loss: 0.00004344
Iteration 80/1000 | Loss: 0.00002443
Iteration 81/1000 | Loss: 0.00002438
Iteration 82/1000 | Loss: 0.00002430
Iteration 83/1000 | Loss: 0.00002497
Iteration 84/1000 | Loss: 0.00010727
Iteration 85/1000 | Loss: 0.00020444
Iteration 86/1000 | Loss: 0.00003596
Iteration 87/1000 | Loss: 0.00004472
Iteration 88/1000 | Loss: 0.00011191
Iteration 89/1000 | Loss: 0.00013159
Iteration 90/1000 | Loss: 0.00002703
Iteration 91/1000 | Loss: 0.00008043
Iteration 92/1000 | Loss: 0.00041826
Iteration 93/1000 | Loss: 0.00006744
Iteration 94/1000 | Loss: 0.00003326
Iteration 95/1000 | Loss: 0.00005445
Iteration 96/1000 | Loss: 0.00009570
Iteration 97/1000 | Loss: 0.00004902
Iteration 98/1000 | Loss: 0.00004618
Iteration 99/1000 | Loss: 0.00003235
Iteration 100/1000 | Loss: 0.00002684
Iteration 101/1000 | Loss: 0.00002519
Iteration 102/1000 | Loss: 0.00002660
Iteration 103/1000 | Loss: 0.00002698
Iteration 104/1000 | Loss: 0.00002596
Iteration 105/1000 | Loss: 0.00002999
Iteration 106/1000 | Loss: 0.00004265
Iteration 107/1000 | Loss: 0.00004265
Iteration 108/1000 | Loss: 0.00017295
Iteration 109/1000 | Loss: 0.00005335
Iteration 110/1000 | Loss: 0.00002588
Iteration 111/1000 | Loss: 0.00002413
Iteration 112/1000 | Loss: 0.00004222
Iteration 113/1000 | Loss: 0.00003599
Iteration 114/1000 | Loss: 0.00002697
Iteration 115/1000 | Loss: 0.00003376
Iteration 116/1000 | Loss: 0.00006940
Iteration 117/1000 | Loss: 0.00002457
Iteration 118/1000 | Loss: 0.00002423
Iteration 119/1000 | Loss: 0.00002561
Iteration 120/1000 | Loss: 0.00002379
Iteration 121/1000 | Loss: 0.00002379
Iteration 122/1000 | Loss: 0.00002379
Iteration 123/1000 | Loss: 0.00002378
Iteration 124/1000 | Loss: 0.00002433
Iteration 125/1000 | Loss: 0.00002395
Iteration 126/1000 | Loss: 0.00002396
Iteration 127/1000 | Loss: 0.00002639
Iteration 128/1000 | Loss: 0.00005962
Iteration 129/1000 | Loss: 0.00002487
Iteration 130/1000 | Loss: 0.00002810
Iteration 131/1000 | Loss: 0.00003035
Iteration 132/1000 | Loss: 0.00002448
Iteration 133/1000 | Loss: 0.00003750
Iteration 134/1000 | Loss: 0.00002485
Iteration 135/1000 | Loss: 0.00002792
Iteration 136/1000 | Loss: 0.00002793
Iteration 137/1000 | Loss: 0.00003844
Iteration 138/1000 | Loss: 0.00010855
Iteration 139/1000 | Loss: 0.00002539
Iteration 140/1000 | Loss: 0.00002428
Iteration 141/1000 | Loss: 0.00002425
Iteration 142/1000 | Loss: 0.00002443
Iteration 143/1000 | Loss: 0.00003887
Iteration 144/1000 | Loss: 0.00002428
Iteration 145/1000 | Loss: 0.00002611
Iteration 146/1000 | Loss: 0.00002581
Iteration 147/1000 | Loss: 0.00002581
Iteration 148/1000 | Loss: 0.00002455
Iteration 149/1000 | Loss: 0.00002703
Iteration 150/1000 | Loss: 0.00002432
Iteration 151/1000 | Loss: 0.00002615
Iteration 152/1000 | Loss: 0.00002614
Iteration 153/1000 | Loss: 0.00007183
Iteration 154/1000 | Loss: 0.00003986
Iteration 155/1000 | Loss: 0.00027518
Iteration 156/1000 | Loss: 0.00059419
Iteration 157/1000 | Loss: 0.00032420
Iteration 158/1000 | Loss: 0.00055827
Iteration 159/1000 | Loss: 0.00035472
Iteration 160/1000 | Loss: 0.00003478
Iteration 161/1000 | Loss: 0.00004685
Iteration 162/1000 | Loss: 0.00002937
Iteration 163/1000 | Loss: 0.00009535
Iteration 164/1000 | Loss: 0.00002570
Iteration 165/1000 | Loss: 0.00002415
Iteration 166/1000 | Loss: 0.00003852
Iteration 167/1000 | Loss: 0.00002439
Iteration 168/1000 | Loss: 0.00002403
Iteration 169/1000 | Loss: 0.00002380
Iteration 170/1000 | Loss: 0.00002380
Iteration 171/1000 | Loss: 0.00002420
Iteration 172/1000 | Loss: 0.00002399
Iteration 173/1000 | Loss: 0.00003751
Iteration 174/1000 | Loss: 0.00002410
Iteration 175/1000 | Loss: 0.00002382
Iteration 176/1000 | Loss: 0.00002368
Iteration 177/1000 | Loss: 0.00002368
Iteration 178/1000 | Loss: 0.00002368
Iteration 179/1000 | Loss: 0.00002368
Iteration 180/1000 | Loss: 0.00002368
Iteration 181/1000 | Loss: 0.00002368
Iteration 182/1000 | Loss: 0.00002368
Iteration 183/1000 | Loss: 0.00002368
Iteration 184/1000 | Loss: 0.00002368
Iteration 185/1000 | Loss: 0.00002368
Iteration 186/1000 | Loss: 0.00002368
Iteration 187/1000 | Loss: 0.00002368
Iteration 188/1000 | Loss: 0.00002368
Iteration 189/1000 | Loss: 0.00002368
Iteration 190/1000 | Loss: 0.00002368
Iteration 191/1000 | Loss: 0.00002368
Iteration 192/1000 | Loss: 0.00002368
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 192. Stopping optimization.
Last 5 losses: [2.367843444517348e-05, 2.367843444517348e-05, 2.367843444517348e-05, 2.367843444517348e-05, 2.367843444517348e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.367843444517348e-05

Optimization complete. Final v2v error: 3.737727403640747 mm

Highest mean error: 20.28195571899414 mm for frame 115

Lowest mean error: 2.7887284755706787 mm for frame 134

Saving results

Total time: 254.10967326164246
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_015/1006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_015/1006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_015/1006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01016001
Iteration 2/25 | Loss: 0.00190248
Iteration 3/25 | Loss: 0.00138101
Iteration 4/25 | Loss: 0.00129975
Iteration 5/25 | Loss: 0.00136358
Iteration 6/25 | Loss: 0.00132194
Iteration 7/25 | Loss: 0.00123434
Iteration 8/25 | Loss: 0.00119725
Iteration 9/25 | Loss: 0.00120079
Iteration 10/25 | Loss: 0.00116763
Iteration 11/25 | Loss: 0.00116398
Iteration 12/25 | Loss: 0.00116125
Iteration 13/25 | Loss: 0.00117932
Iteration 14/25 | Loss: 0.00115684
Iteration 15/25 | Loss: 0.00115691
Iteration 16/25 | Loss: 0.00116579
Iteration 17/25 | Loss: 0.00116105
Iteration 18/25 | Loss: 0.00115147
Iteration 19/25 | Loss: 0.00115077
Iteration 20/25 | Loss: 0.00115077
Iteration 21/25 | Loss: 0.00115076
Iteration 22/25 | Loss: 0.00115076
Iteration 23/25 | Loss: 0.00115076
Iteration 24/25 | Loss: 0.00115076
Iteration 25/25 | Loss: 0.00115076

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.28057361
Iteration 2/25 | Loss: 0.00138444
Iteration 3/25 | Loss: 0.00122145
Iteration 4/25 | Loss: 0.00122145
Iteration 5/25 | Loss: 0.00122145
Iteration 6/25 | Loss: 0.00122145
Iteration 7/25 | Loss: 0.00122144
Iteration 8/25 | Loss: 0.00122144
Iteration 9/25 | Loss: 0.00122144
Iteration 10/25 | Loss: 0.00122144
Iteration 11/25 | Loss: 0.00122144
Iteration 12/25 | Loss: 0.00122144
Iteration 13/25 | Loss: 0.00122144
Iteration 14/25 | Loss: 0.00122144
Iteration 15/25 | Loss: 0.00122144
Iteration 16/25 | Loss: 0.00122144
Iteration 17/25 | Loss: 0.00122144
Iteration 18/25 | Loss: 0.00122144
Iteration 19/25 | Loss: 0.00122144
Iteration 20/25 | Loss: 0.00122144
Iteration 21/25 | Loss: 0.00122144
Iteration 22/25 | Loss: 0.00122144
Iteration 23/25 | Loss: 0.00122144
Iteration 24/25 | Loss: 0.00122144
Iteration 25/25 | Loss: 0.00122144

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00122144
Iteration 2/1000 | Loss: 0.00021132
Iteration 3/1000 | Loss: 0.00041646
Iteration 4/1000 | Loss: 0.00006761
Iteration 5/1000 | Loss: 0.00021581
Iteration 6/1000 | Loss: 0.00006370
Iteration 7/1000 | Loss: 0.00004208
Iteration 8/1000 | Loss: 0.00002267
Iteration 9/1000 | Loss: 0.00001930
Iteration 10/1000 | Loss: 0.00002415
Iteration 11/1000 | Loss: 0.00002092
Iteration 12/1000 | Loss: 0.00001152
Iteration 13/1000 | Loss: 0.00001641
Iteration 14/1000 | Loss: 0.00001060
Iteration 15/1000 | Loss: 0.00001050
Iteration 16/1000 | Loss: 0.00001033
Iteration 17/1000 | Loss: 0.00003606
Iteration 18/1000 | Loss: 0.00003776
Iteration 19/1000 | Loss: 0.00001754
Iteration 20/1000 | Loss: 0.00002257
Iteration 21/1000 | Loss: 0.00002774
Iteration 22/1000 | Loss: 0.00000997
Iteration 23/1000 | Loss: 0.00000994
Iteration 24/1000 | Loss: 0.00000993
Iteration 25/1000 | Loss: 0.00000993
Iteration 26/1000 | Loss: 0.00000992
Iteration 27/1000 | Loss: 0.00000992
Iteration 28/1000 | Loss: 0.00000992
Iteration 29/1000 | Loss: 0.00000992
Iteration 30/1000 | Loss: 0.00000991
Iteration 31/1000 | Loss: 0.00000991
Iteration 32/1000 | Loss: 0.00000991
Iteration 33/1000 | Loss: 0.00000991
Iteration 34/1000 | Loss: 0.00000991
Iteration 35/1000 | Loss: 0.00000991
Iteration 36/1000 | Loss: 0.00000991
Iteration 37/1000 | Loss: 0.00000991
Iteration 38/1000 | Loss: 0.00000991
Iteration 39/1000 | Loss: 0.00000990
Iteration 40/1000 | Loss: 0.00000990
Iteration 41/1000 | Loss: 0.00000990
Iteration 42/1000 | Loss: 0.00000989
Iteration 43/1000 | Loss: 0.00000989
Iteration 44/1000 | Loss: 0.00000989
Iteration 45/1000 | Loss: 0.00000989
Iteration 46/1000 | Loss: 0.00000989
Iteration 47/1000 | Loss: 0.00000989
Iteration 48/1000 | Loss: 0.00000988
Iteration 49/1000 | Loss: 0.00000988
Iteration 50/1000 | Loss: 0.00000988
Iteration 51/1000 | Loss: 0.00000988
Iteration 52/1000 | Loss: 0.00000988
Iteration 53/1000 | Loss: 0.00000988
Iteration 54/1000 | Loss: 0.00000988
Iteration 55/1000 | Loss: 0.00000988
Iteration 56/1000 | Loss: 0.00000988
Iteration 57/1000 | Loss: 0.00000988
Iteration 58/1000 | Loss: 0.00000988
Iteration 59/1000 | Loss: 0.00000987
Iteration 60/1000 | Loss: 0.00000987
Iteration 61/1000 | Loss: 0.00000987
Iteration 62/1000 | Loss: 0.00000987
Iteration 63/1000 | Loss: 0.00000986
Iteration 64/1000 | Loss: 0.00001895
Iteration 65/1000 | Loss: 0.00001888
Iteration 66/1000 | Loss: 0.00000983
Iteration 67/1000 | Loss: 0.00000978
Iteration 68/1000 | Loss: 0.00000978
Iteration 69/1000 | Loss: 0.00000977
Iteration 70/1000 | Loss: 0.00000977
Iteration 71/1000 | Loss: 0.00000977
Iteration 72/1000 | Loss: 0.00000977
Iteration 73/1000 | Loss: 0.00000977
Iteration 74/1000 | Loss: 0.00000977
Iteration 75/1000 | Loss: 0.00000977
Iteration 76/1000 | Loss: 0.00000977
Iteration 77/1000 | Loss: 0.00000977
Iteration 78/1000 | Loss: 0.00000977
Iteration 79/1000 | Loss: 0.00000976
Iteration 80/1000 | Loss: 0.00000976
Iteration 81/1000 | Loss: 0.00000976
Iteration 82/1000 | Loss: 0.00000976
Iteration 83/1000 | Loss: 0.00000973
Iteration 84/1000 | Loss: 0.00000972
Iteration 85/1000 | Loss: 0.00000972
Iteration 86/1000 | Loss: 0.00000972
Iteration 87/1000 | Loss: 0.00000971
Iteration 88/1000 | Loss: 0.00000971
Iteration 89/1000 | Loss: 0.00000971
Iteration 90/1000 | Loss: 0.00000971
Iteration 91/1000 | Loss: 0.00000971
Iteration 92/1000 | Loss: 0.00000971
Iteration 93/1000 | Loss: 0.00000971
Iteration 94/1000 | Loss: 0.00000971
Iteration 95/1000 | Loss: 0.00000971
Iteration 96/1000 | Loss: 0.00000971
Iteration 97/1000 | Loss: 0.00000971
Iteration 98/1000 | Loss: 0.00000971
Iteration 99/1000 | Loss: 0.00000971
Iteration 100/1000 | Loss: 0.00000971
Iteration 101/1000 | Loss: 0.00000971
Iteration 102/1000 | Loss: 0.00000971
Iteration 103/1000 | Loss: 0.00000971
Iteration 104/1000 | Loss: 0.00000971
Iteration 105/1000 | Loss: 0.00000971
Iteration 106/1000 | Loss: 0.00000971
Iteration 107/1000 | Loss: 0.00000971
Iteration 108/1000 | Loss: 0.00000971
Iteration 109/1000 | Loss: 0.00000971
Iteration 110/1000 | Loss: 0.00000971
Iteration 111/1000 | Loss: 0.00000971
Iteration 112/1000 | Loss: 0.00000971
Iteration 113/1000 | Loss: 0.00000971
Iteration 114/1000 | Loss: 0.00000971
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 114. Stopping optimization.
Last 5 losses: [9.713860890769865e-06, 9.713860890769865e-06, 9.713860890769865e-06, 9.713860890769865e-06, 9.713860890769865e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.713860890769865e-06

Optimization complete. Final v2v error: 2.7165889739990234 mm

Highest mean error: 2.8296470642089844 mm for frame 16

Lowest mean error: 2.594007968902588 mm for frame 152

Saving results

Total time: 74.11682486534119
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_015/1077/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_015/1077.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_015/1077
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00903331
Iteration 2/25 | Loss: 0.00254552
Iteration 3/25 | Loss: 0.00177748
Iteration 4/25 | Loss: 0.00167974
Iteration 5/25 | Loss: 0.00165576
Iteration 6/25 | Loss: 0.00162100
Iteration 7/25 | Loss: 0.00160764
Iteration 8/25 | Loss: 0.00159308
Iteration 9/25 | Loss: 0.00160932
Iteration 10/25 | Loss: 0.00158489
Iteration 11/25 | Loss: 0.00159175
Iteration 12/25 | Loss: 0.00158325
Iteration 13/25 | Loss: 0.00158261
Iteration 14/25 | Loss: 0.00158129
Iteration 15/25 | Loss: 0.00157948
Iteration 16/25 | Loss: 0.00158284
Iteration 17/25 | Loss: 0.00157790
Iteration 18/25 | Loss: 0.00157762
Iteration 19/25 | Loss: 0.00157746
Iteration 20/25 | Loss: 0.00158214
Iteration 21/25 | Loss: 0.00157999
Iteration 22/25 | Loss: 0.00157722
Iteration 23/25 | Loss: 0.00157717
Iteration 24/25 | Loss: 0.00157715
Iteration 25/25 | Loss: 0.00157715

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.31381702
Iteration 2/25 | Loss: 0.00513884
Iteration 3/25 | Loss: 0.00453377
Iteration 4/25 | Loss: 0.00453377
Iteration 5/25 | Loss: 0.00453377
Iteration 6/25 | Loss: 0.00453377
Iteration 7/25 | Loss: 0.00453377
Iteration 8/25 | Loss: 0.00453377
Iteration 9/25 | Loss: 0.00453377
Iteration 10/25 | Loss: 0.00453377
Iteration 11/25 | Loss: 0.00453377
Iteration 12/25 | Loss: 0.00453377
Iteration 13/25 | Loss: 0.00453377
Iteration 14/25 | Loss: 0.00453377
Iteration 15/25 | Loss: 0.00453377
Iteration 16/25 | Loss: 0.00453377
Iteration 17/25 | Loss: 0.00453377
Iteration 18/25 | Loss: 0.00453377
Iteration 19/25 | Loss: 0.00453377
Iteration 20/25 | Loss: 0.00453377
Iteration 21/25 | Loss: 0.00453377
Iteration 22/25 | Loss: 0.00453377
Iteration 23/25 | Loss: 0.00453377
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.004533769097179174, 0.004533769097179174, 0.004533769097179174, 0.004533769097179174, 0.004533769097179174]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.004533769097179174

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00453377
Iteration 2/1000 | Loss: 0.00662434
Iteration 3/1000 | Loss: 0.00293548
Iteration 4/1000 | Loss: 0.00182100
Iteration 5/1000 | Loss: 0.00227685
Iteration 6/1000 | Loss: 0.00136206
Iteration 7/1000 | Loss: 0.00058968
Iteration 8/1000 | Loss: 0.00052287
Iteration 9/1000 | Loss: 0.00125172
Iteration 10/1000 | Loss: 0.00173201
Iteration 11/1000 | Loss: 0.00048743
Iteration 12/1000 | Loss: 0.00038471
Iteration 13/1000 | Loss: 0.00057866
Iteration 14/1000 | Loss: 0.00024272
Iteration 15/1000 | Loss: 0.00092256
Iteration 16/1000 | Loss: 0.00101107
Iteration 17/1000 | Loss: 0.00019949
Iteration 18/1000 | Loss: 0.00033917
Iteration 19/1000 | Loss: 0.00021973
Iteration 20/1000 | Loss: 0.00034494
Iteration 21/1000 | Loss: 0.00052582
Iteration 22/1000 | Loss: 0.00074582
Iteration 23/1000 | Loss: 0.00015919
Iteration 24/1000 | Loss: 0.00013696
Iteration 25/1000 | Loss: 0.00055789
Iteration 26/1000 | Loss: 0.00017258
Iteration 27/1000 | Loss: 0.00031864
Iteration 28/1000 | Loss: 0.00013545
Iteration 29/1000 | Loss: 0.00012105
Iteration 30/1000 | Loss: 0.00011343
Iteration 31/1000 | Loss: 0.00010965
Iteration 32/1000 | Loss: 0.00010582
Iteration 33/1000 | Loss: 0.00036354
Iteration 34/1000 | Loss: 0.00038716
Iteration 35/1000 | Loss: 0.00086264
Iteration 36/1000 | Loss: 0.00066548
Iteration 37/1000 | Loss: 0.00030862
Iteration 38/1000 | Loss: 0.00038017
Iteration 39/1000 | Loss: 0.00039411
Iteration 40/1000 | Loss: 0.00041202
Iteration 41/1000 | Loss: 0.00034353
Iteration 42/1000 | Loss: 0.00033993
Iteration 43/1000 | Loss: 0.00069646
Iteration 44/1000 | Loss: 0.00066154
Iteration 45/1000 | Loss: 0.00093529
Iteration 46/1000 | Loss: 0.00064571
Iteration 47/1000 | Loss: 0.00046803
Iteration 48/1000 | Loss: 0.00052643
Iteration 49/1000 | Loss: 0.00088087
Iteration 50/1000 | Loss: 0.00064731
Iteration 51/1000 | Loss: 0.00101643
Iteration 52/1000 | Loss: 0.00063583
Iteration 53/1000 | Loss: 0.00076475
Iteration 54/1000 | Loss: 0.00042572
Iteration 55/1000 | Loss: 0.00025628
Iteration 56/1000 | Loss: 0.00040319
Iteration 57/1000 | Loss: 0.00039711
Iteration 58/1000 | Loss: 0.00009517
Iteration 59/1000 | Loss: 0.00036300
Iteration 60/1000 | Loss: 0.00136468
Iteration 61/1000 | Loss: 0.00080308
Iteration 62/1000 | Loss: 0.00135778
Iteration 63/1000 | Loss: 0.00115105
Iteration 64/1000 | Loss: 0.00078565
Iteration 65/1000 | Loss: 0.00060395
Iteration 66/1000 | Loss: 0.00044789
Iteration 67/1000 | Loss: 0.00009286
Iteration 68/1000 | Loss: 0.00012137
Iteration 69/1000 | Loss: 0.00008721
Iteration 70/1000 | Loss: 0.00007918
Iteration 71/1000 | Loss: 0.00007502
Iteration 72/1000 | Loss: 0.00007206
Iteration 73/1000 | Loss: 0.00033075
Iteration 74/1000 | Loss: 0.00087580
Iteration 75/1000 | Loss: 0.00053110
Iteration 76/1000 | Loss: 0.00071762
Iteration 77/1000 | Loss: 0.00053020
Iteration 78/1000 | Loss: 0.00057391
Iteration 79/1000 | Loss: 0.00080173
Iteration 80/1000 | Loss: 0.00114662
Iteration 81/1000 | Loss: 0.00069181
Iteration 82/1000 | Loss: 0.00100781
Iteration 83/1000 | Loss: 0.00060679
Iteration 84/1000 | Loss: 0.00040741
Iteration 85/1000 | Loss: 0.00042765
Iteration 86/1000 | Loss: 0.00007689
Iteration 87/1000 | Loss: 0.00033478
Iteration 88/1000 | Loss: 0.00183038
Iteration 89/1000 | Loss: 0.00173077
Iteration 90/1000 | Loss: 0.00181652
Iteration 91/1000 | Loss: 0.00180999
Iteration 92/1000 | Loss: 0.00132220
Iteration 93/1000 | Loss: 0.00109299
Iteration 94/1000 | Loss: 0.00125221
Iteration 95/1000 | Loss: 0.00084511
Iteration 96/1000 | Loss: 0.00063658
Iteration 97/1000 | Loss: 0.00012058
Iteration 98/1000 | Loss: 0.00060698
Iteration 99/1000 | Loss: 0.00091992
Iteration 100/1000 | Loss: 0.00193612
Iteration 101/1000 | Loss: 0.00224397
Iteration 102/1000 | Loss: 0.00263680
Iteration 103/1000 | Loss: 0.00144058
Iteration 104/1000 | Loss: 0.00106110
Iteration 105/1000 | Loss: 0.00121214
Iteration 106/1000 | Loss: 0.00157362
Iteration 107/1000 | Loss: 0.00089295
Iteration 108/1000 | Loss: 0.00112219
Iteration 109/1000 | Loss: 0.00122069
Iteration 110/1000 | Loss: 0.00064184
Iteration 111/1000 | Loss: 0.00071642
Iteration 112/1000 | Loss: 0.00026875
Iteration 113/1000 | Loss: 0.00024223
Iteration 114/1000 | Loss: 0.00121296
Iteration 115/1000 | Loss: 0.00037670
Iteration 116/1000 | Loss: 0.00042408
Iteration 117/1000 | Loss: 0.00075438
Iteration 118/1000 | Loss: 0.00023653
Iteration 119/1000 | Loss: 0.00023837
Iteration 120/1000 | Loss: 0.00017285
Iteration 121/1000 | Loss: 0.00005745
Iteration 122/1000 | Loss: 0.00005080
Iteration 123/1000 | Loss: 0.00004882
Iteration 124/1000 | Loss: 0.00031425
Iteration 125/1000 | Loss: 0.00096817
Iteration 126/1000 | Loss: 0.00045480
Iteration 127/1000 | Loss: 0.00055900
Iteration 128/1000 | Loss: 0.00041860
Iteration 129/1000 | Loss: 0.00012091
Iteration 130/1000 | Loss: 0.00004669
Iteration 131/1000 | Loss: 0.00004062
Iteration 132/1000 | Loss: 0.00003776
Iteration 133/1000 | Loss: 0.00003490
Iteration 134/1000 | Loss: 0.00003300
Iteration 135/1000 | Loss: 0.00003177
Iteration 136/1000 | Loss: 0.00003088
Iteration 137/1000 | Loss: 0.00003028
Iteration 138/1000 | Loss: 0.00032395
Iteration 139/1000 | Loss: 0.00024558
Iteration 140/1000 | Loss: 0.00022212
Iteration 141/1000 | Loss: 0.00052134
Iteration 142/1000 | Loss: 0.00004511
Iteration 143/1000 | Loss: 0.00004446
Iteration 144/1000 | Loss: 0.00003182
Iteration 145/1000 | Loss: 0.00003405
Iteration 146/1000 | Loss: 0.00002953
Iteration 147/1000 | Loss: 0.00002798
Iteration 148/1000 | Loss: 0.00002711
Iteration 149/1000 | Loss: 0.00002663
Iteration 150/1000 | Loss: 0.00002624
Iteration 151/1000 | Loss: 0.00002580
Iteration 152/1000 | Loss: 0.00002550
Iteration 153/1000 | Loss: 0.00002514
Iteration 154/1000 | Loss: 0.00002485
Iteration 155/1000 | Loss: 0.00030543
Iteration 156/1000 | Loss: 0.00015170
Iteration 157/1000 | Loss: 0.00025849
Iteration 158/1000 | Loss: 0.00021180
Iteration 159/1000 | Loss: 0.00024885
Iteration 160/1000 | Loss: 0.00052949
Iteration 161/1000 | Loss: 0.00039987
Iteration 162/1000 | Loss: 0.00021351
Iteration 163/1000 | Loss: 0.00019874
Iteration 164/1000 | Loss: 0.00003178
Iteration 165/1000 | Loss: 0.00002638
Iteration 166/1000 | Loss: 0.00002534
Iteration 167/1000 | Loss: 0.00002441
Iteration 168/1000 | Loss: 0.00032091
Iteration 169/1000 | Loss: 0.00016323
Iteration 170/1000 | Loss: 0.00037106
Iteration 171/1000 | Loss: 0.00008081
Iteration 172/1000 | Loss: 0.00002634
Iteration 173/1000 | Loss: 0.00002365
Iteration 174/1000 | Loss: 0.00027900
Iteration 175/1000 | Loss: 0.00013947
Iteration 176/1000 | Loss: 0.00006894
Iteration 177/1000 | Loss: 0.00002741
Iteration 178/1000 | Loss: 0.00007889
Iteration 179/1000 | Loss: 0.00010586
Iteration 180/1000 | Loss: 0.00007655
Iteration 181/1000 | Loss: 0.00009633
Iteration 182/1000 | Loss: 0.00007445
Iteration 183/1000 | Loss: 0.00002689
Iteration 184/1000 | Loss: 0.00002469
Iteration 185/1000 | Loss: 0.00002926
Iteration 186/1000 | Loss: 0.00002341
Iteration 187/1000 | Loss: 0.00005752
Iteration 188/1000 | Loss: 0.00005325
Iteration 189/1000 | Loss: 0.00002835
Iteration 190/1000 | Loss: 0.00002528
Iteration 191/1000 | Loss: 0.00003908
Iteration 192/1000 | Loss: 0.00003346
Iteration 193/1000 | Loss: 0.00004322
Iteration 194/1000 | Loss: 0.00002488
Iteration 195/1000 | Loss: 0.00002416
Iteration 196/1000 | Loss: 0.00002343
Iteration 197/1000 | Loss: 0.00002312
Iteration 198/1000 | Loss: 0.00028442
Iteration 199/1000 | Loss: 0.00016086
Iteration 200/1000 | Loss: 0.00002583
Iteration 201/1000 | Loss: 0.00048711
Iteration 202/1000 | Loss: 0.00020045
Iteration 203/1000 | Loss: 0.00020963
Iteration 204/1000 | Loss: 0.00046042
Iteration 205/1000 | Loss: 0.00024928
Iteration 206/1000 | Loss: 0.00033532
Iteration 207/1000 | Loss: 0.00002724
Iteration 208/1000 | Loss: 0.00002441
Iteration 209/1000 | Loss: 0.00002303
Iteration 210/1000 | Loss: 0.00002231
Iteration 211/1000 | Loss: 0.00002166
Iteration 212/1000 | Loss: 0.00002105
Iteration 213/1000 | Loss: 0.00002064
Iteration 214/1000 | Loss: 0.00002043
Iteration 215/1000 | Loss: 0.00002026
Iteration 216/1000 | Loss: 0.00002022
Iteration 217/1000 | Loss: 0.00002013
Iteration 218/1000 | Loss: 0.00002013
Iteration 219/1000 | Loss: 0.00002012
Iteration 220/1000 | Loss: 0.00002011
Iteration 221/1000 | Loss: 0.00002011
Iteration 222/1000 | Loss: 0.00002008
Iteration 223/1000 | Loss: 0.00002007
Iteration 224/1000 | Loss: 0.00002007
Iteration 225/1000 | Loss: 0.00002006
Iteration 226/1000 | Loss: 0.00002005
Iteration 227/1000 | Loss: 0.00002004
Iteration 228/1000 | Loss: 0.00002004
Iteration 229/1000 | Loss: 0.00002004
Iteration 230/1000 | Loss: 0.00002003
Iteration 231/1000 | Loss: 0.00002002
Iteration 232/1000 | Loss: 0.00002002
Iteration 233/1000 | Loss: 0.00002002
Iteration 234/1000 | Loss: 0.00002001
Iteration 235/1000 | Loss: 0.00002001
Iteration 236/1000 | Loss: 0.00002001
Iteration 237/1000 | Loss: 0.00002001
Iteration 238/1000 | Loss: 0.00002001
Iteration 239/1000 | Loss: 0.00002001
Iteration 240/1000 | Loss: 0.00002000
Iteration 241/1000 | Loss: 0.00002000
Iteration 242/1000 | Loss: 0.00002000
Iteration 243/1000 | Loss: 0.00002000
Iteration 244/1000 | Loss: 0.00001999
Iteration 245/1000 | Loss: 0.00001999
Iteration 246/1000 | Loss: 0.00001999
Iteration 247/1000 | Loss: 0.00001999
Iteration 248/1000 | Loss: 0.00001999
Iteration 249/1000 | Loss: 0.00001999
Iteration 250/1000 | Loss: 0.00001999
Iteration 251/1000 | Loss: 0.00001999
Iteration 252/1000 | Loss: 0.00001998
Iteration 253/1000 | Loss: 0.00001998
Iteration 254/1000 | Loss: 0.00001998
Iteration 255/1000 | Loss: 0.00001998
Iteration 256/1000 | Loss: 0.00001998
Iteration 257/1000 | Loss: 0.00001998
Iteration 258/1000 | Loss: 0.00001997
Iteration 259/1000 | Loss: 0.00001997
Iteration 260/1000 | Loss: 0.00001997
Iteration 261/1000 | Loss: 0.00001996
Iteration 262/1000 | Loss: 0.00001995
Iteration 263/1000 | Loss: 0.00001994
Iteration 264/1000 | Loss: 0.00001993
Iteration 265/1000 | Loss: 0.00001993
Iteration 266/1000 | Loss: 0.00001992
Iteration 267/1000 | Loss: 0.00001992
Iteration 268/1000 | Loss: 0.00001992
Iteration 269/1000 | Loss: 0.00001992
Iteration 270/1000 | Loss: 0.00001992
Iteration 271/1000 | Loss: 0.00001992
Iteration 272/1000 | Loss: 0.00001991
Iteration 273/1000 | Loss: 0.00001991
Iteration 274/1000 | Loss: 0.00001991
Iteration 275/1000 | Loss: 0.00001991
Iteration 276/1000 | Loss: 0.00001991
Iteration 277/1000 | Loss: 0.00001991
Iteration 278/1000 | Loss: 0.00001991
Iteration 279/1000 | Loss: 0.00001991
Iteration 280/1000 | Loss: 0.00001991
Iteration 281/1000 | Loss: 0.00001991
Iteration 282/1000 | Loss: 0.00001991
Iteration 283/1000 | Loss: 0.00001991
Iteration 284/1000 | Loss: 0.00001991
Iteration 285/1000 | Loss: 0.00001990
Iteration 286/1000 | Loss: 0.00001990
Iteration 287/1000 | Loss: 0.00001990
Iteration 288/1000 | Loss: 0.00001990
Iteration 289/1000 | Loss: 0.00001990
Iteration 290/1000 | Loss: 0.00001989
Iteration 291/1000 | Loss: 0.00001989
Iteration 292/1000 | Loss: 0.00001989
Iteration 293/1000 | Loss: 0.00001989
Iteration 294/1000 | Loss: 0.00001989
Iteration 295/1000 | Loss: 0.00001989
Iteration 296/1000 | Loss: 0.00001989
Iteration 297/1000 | Loss: 0.00001989
Iteration 298/1000 | Loss: 0.00001989
Iteration 299/1000 | Loss: 0.00001989
Iteration 300/1000 | Loss: 0.00001989
Iteration 301/1000 | Loss: 0.00001988
Iteration 302/1000 | Loss: 0.00001988
Iteration 303/1000 | Loss: 0.00001988
Iteration 304/1000 | Loss: 0.00001988
Iteration 305/1000 | Loss: 0.00001988
Iteration 306/1000 | Loss: 0.00001988
Iteration 307/1000 | Loss: 0.00001987
Iteration 308/1000 | Loss: 0.00001987
Iteration 309/1000 | Loss: 0.00001987
Iteration 310/1000 | Loss: 0.00001987
Iteration 311/1000 | Loss: 0.00001987
Iteration 312/1000 | Loss: 0.00001987
Iteration 313/1000 | Loss: 0.00001987
Iteration 314/1000 | Loss: 0.00001986
Iteration 315/1000 | Loss: 0.00001986
Iteration 316/1000 | Loss: 0.00001986
Iteration 317/1000 | Loss: 0.00001985
Iteration 318/1000 | Loss: 0.00001985
Iteration 319/1000 | Loss: 0.00001985
Iteration 320/1000 | Loss: 0.00001985
Iteration 321/1000 | Loss: 0.00001985
Iteration 322/1000 | Loss: 0.00001985
Iteration 323/1000 | Loss: 0.00001985
Iteration 324/1000 | Loss: 0.00001985
Iteration 325/1000 | Loss: 0.00001985
Iteration 326/1000 | Loss: 0.00001985
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 326. Stopping optimization.
Last 5 losses: [1.98526158783352e-05, 1.98526158783352e-05, 1.98526158783352e-05, 1.98526158783352e-05, 1.98526158783352e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.98526158783352e-05

Optimization complete. Final v2v error: 3.3437001705169678 mm

Highest mean error: 11.541158676147461 mm for frame 77

Lowest mean error: 2.8592114448547363 mm for frame 0

Saving results

Total time: 403.1983211040497
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_015/1073/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_015/1073.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_015/1073
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00814798
Iteration 2/25 | Loss: 0.00126133
Iteration 3/25 | Loss: 0.00118076
Iteration 4/25 | Loss: 0.00116750
Iteration 5/25 | Loss: 0.00116411
Iteration 6/25 | Loss: 0.00116366
Iteration 7/25 | Loss: 0.00116366
Iteration 8/25 | Loss: 0.00116366
Iteration 9/25 | Loss: 0.00116366
Iteration 10/25 | Loss: 0.00116366
Iteration 11/25 | Loss: 0.00116366
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011636593844741583, 0.0011636593844741583, 0.0011636593844741583, 0.0011636593844741583, 0.0011636593844741583]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011636593844741583

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.31774437
Iteration 2/25 | Loss: 0.00126292
Iteration 3/25 | Loss: 0.00126292
Iteration 4/25 | Loss: 0.00126292
Iteration 5/25 | Loss: 0.00126292
Iteration 6/25 | Loss: 0.00126292
Iteration 7/25 | Loss: 0.00126292
Iteration 8/25 | Loss: 0.00126292
Iteration 9/25 | Loss: 0.00126292
Iteration 10/25 | Loss: 0.00126292
Iteration 11/25 | Loss: 0.00126292
Iteration 12/25 | Loss: 0.00126292
Iteration 13/25 | Loss: 0.00126292
Iteration 14/25 | Loss: 0.00126292
Iteration 15/25 | Loss: 0.00126292
Iteration 16/25 | Loss: 0.00126292
Iteration 17/25 | Loss: 0.00126292
Iteration 18/25 | Loss: 0.00126292
Iteration 19/25 | Loss: 0.00126292
Iteration 20/25 | Loss: 0.00126292
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0012629202101379633, 0.0012629202101379633, 0.0012629202101379633, 0.0012629202101379633, 0.0012629202101379633]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012629202101379633

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00126292
Iteration 2/1000 | Loss: 0.00002660
Iteration 3/1000 | Loss: 0.00001781
Iteration 4/1000 | Loss: 0.00001546
Iteration 5/1000 | Loss: 0.00001432
Iteration 6/1000 | Loss: 0.00001370
Iteration 7/1000 | Loss: 0.00001332
Iteration 8/1000 | Loss: 0.00001280
Iteration 9/1000 | Loss: 0.00001249
Iteration 10/1000 | Loss: 0.00001220
Iteration 11/1000 | Loss: 0.00001194
Iteration 12/1000 | Loss: 0.00001192
Iteration 13/1000 | Loss: 0.00001189
Iteration 14/1000 | Loss: 0.00001170
Iteration 15/1000 | Loss: 0.00001160
Iteration 16/1000 | Loss: 0.00001160
Iteration 17/1000 | Loss: 0.00001160
Iteration 18/1000 | Loss: 0.00001148
Iteration 19/1000 | Loss: 0.00001142
Iteration 20/1000 | Loss: 0.00001141
Iteration 21/1000 | Loss: 0.00001140
Iteration 22/1000 | Loss: 0.00001139
Iteration 23/1000 | Loss: 0.00001137
Iteration 24/1000 | Loss: 0.00001137
Iteration 25/1000 | Loss: 0.00001130
Iteration 26/1000 | Loss: 0.00001127
Iteration 27/1000 | Loss: 0.00001126
Iteration 28/1000 | Loss: 0.00001126
Iteration 29/1000 | Loss: 0.00001125
Iteration 30/1000 | Loss: 0.00001125
Iteration 31/1000 | Loss: 0.00001124
Iteration 32/1000 | Loss: 0.00001124
Iteration 33/1000 | Loss: 0.00001123
Iteration 34/1000 | Loss: 0.00001120
Iteration 35/1000 | Loss: 0.00001119
Iteration 36/1000 | Loss: 0.00001117
Iteration 37/1000 | Loss: 0.00001117
Iteration 38/1000 | Loss: 0.00001117
Iteration 39/1000 | Loss: 0.00001117
Iteration 40/1000 | Loss: 0.00001117
Iteration 41/1000 | Loss: 0.00001116
Iteration 42/1000 | Loss: 0.00001115
Iteration 43/1000 | Loss: 0.00001115
Iteration 44/1000 | Loss: 0.00001115
Iteration 45/1000 | Loss: 0.00001114
Iteration 46/1000 | Loss: 0.00001114
Iteration 47/1000 | Loss: 0.00001113
Iteration 48/1000 | Loss: 0.00001113
Iteration 49/1000 | Loss: 0.00001112
Iteration 50/1000 | Loss: 0.00001112
Iteration 51/1000 | Loss: 0.00001111
Iteration 52/1000 | Loss: 0.00001111
Iteration 53/1000 | Loss: 0.00001111
Iteration 54/1000 | Loss: 0.00001111
Iteration 55/1000 | Loss: 0.00001110
Iteration 56/1000 | Loss: 0.00001110
Iteration 57/1000 | Loss: 0.00001110
Iteration 58/1000 | Loss: 0.00001110
Iteration 59/1000 | Loss: 0.00001110
Iteration 60/1000 | Loss: 0.00001110
Iteration 61/1000 | Loss: 0.00001109
Iteration 62/1000 | Loss: 0.00001109
Iteration 63/1000 | Loss: 0.00001109
Iteration 64/1000 | Loss: 0.00001109
Iteration 65/1000 | Loss: 0.00001109
Iteration 66/1000 | Loss: 0.00001109
Iteration 67/1000 | Loss: 0.00001108
Iteration 68/1000 | Loss: 0.00001106
Iteration 69/1000 | Loss: 0.00001106
Iteration 70/1000 | Loss: 0.00001106
Iteration 71/1000 | Loss: 0.00001106
Iteration 72/1000 | Loss: 0.00001106
Iteration 73/1000 | Loss: 0.00001105
Iteration 74/1000 | Loss: 0.00001104
Iteration 75/1000 | Loss: 0.00001104
Iteration 76/1000 | Loss: 0.00001103
Iteration 77/1000 | Loss: 0.00001102
Iteration 78/1000 | Loss: 0.00001102
Iteration 79/1000 | Loss: 0.00001102
Iteration 80/1000 | Loss: 0.00001102
Iteration 81/1000 | Loss: 0.00001102
Iteration 82/1000 | Loss: 0.00001102
Iteration 83/1000 | Loss: 0.00001102
Iteration 84/1000 | Loss: 0.00001102
Iteration 85/1000 | Loss: 0.00001102
Iteration 86/1000 | Loss: 0.00001102
Iteration 87/1000 | Loss: 0.00001102
Iteration 88/1000 | Loss: 0.00001102
Iteration 89/1000 | Loss: 0.00001102
Iteration 90/1000 | Loss: 0.00001102
Iteration 91/1000 | Loss: 0.00001102
Iteration 92/1000 | Loss: 0.00001102
Iteration 93/1000 | Loss: 0.00001102
Iteration 94/1000 | Loss: 0.00001102
Iteration 95/1000 | Loss: 0.00001102
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 95. Stopping optimization.
Last 5 losses: [1.101606812881073e-05, 1.101606812881073e-05, 1.101606812881073e-05, 1.101606812881073e-05, 1.101606812881073e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.101606812881073e-05

Optimization complete. Final v2v error: 2.8982880115509033 mm

Highest mean error: 2.943142890930176 mm for frame 68

Lowest mean error: 2.833948850631714 mm for frame 99

Saving results

Total time: 35.51308250427246
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_015/1094/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_015/1094.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_015/1094
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00444657
Iteration 2/25 | Loss: 0.00132299
Iteration 3/25 | Loss: 0.00122579
Iteration 4/25 | Loss: 0.00121047
Iteration 5/25 | Loss: 0.00120579
Iteration 6/25 | Loss: 0.00120494
Iteration 7/25 | Loss: 0.00120483
Iteration 8/25 | Loss: 0.00120483
Iteration 9/25 | Loss: 0.00120483
Iteration 10/25 | Loss: 0.00120483
Iteration 11/25 | Loss: 0.00120483
Iteration 12/25 | Loss: 0.00120483
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0012048290809616446, 0.0012048290809616446, 0.0012048290809616446, 0.0012048290809616446, 0.0012048290809616446]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012048290809616446

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.28927922
Iteration 2/25 | Loss: 0.00133928
Iteration 3/25 | Loss: 0.00133928
Iteration 4/25 | Loss: 0.00133928
Iteration 5/25 | Loss: 0.00133928
Iteration 6/25 | Loss: 0.00133928
Iteration 7/25 | Loss: 0.00133928
Iteration 8/25 | Loss: 0.00133928
Iteration 9/25 | Loss: 0.00133928
Iteration 10/25 | Loss: 0.00133928
Iteration 11/25 | Loss: 0.00133928
Iteration 12/25 | Loss: 0.00133928
Iteration 13/25 | Loss: 0.00133928
Iteration 14/25 | Loss: 0.00133928
Iteration 15/25 | Loss: 0.00133928
Iteration 16/25 | Loss: 0.00133928
Iteration 17/25 | Loss: 0.00133928
Iteration 18/25 | Loss: 0.00133928
Iteration 19/25 | Loss: 0.00133928
Iteration 20/25 | Loss: 0.00133928
Iteration 21/25 | Loss: 0.00133928
Iteration 22/25 | Loss: 0.00133928
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.00133927958086133, 0.00133927958086133, 0.00133927958086133, 0.00133927958086133, 0.00133927958086133]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00133927958086133

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00133928
Iteration 2/1000 | Loss: 0.00003185
Iteration 3/1000 | Loss: 0.00002442
Iteration 4/1000 | Loss: 0.00002150
Iteration 5/1000 | Loss: 0.00002064
Iteration 6/1000 | Loss: 0.00001990
Iteration 7/1000 | Loss: 0.00001934
Iteration 8/1000 | Loss: 0.00001887
Iteration 9/1000 | Loss: 0.00001849
Iteration 10/1000 | Loss: 0.00001822
Iteration 11/1000 | Loss: 0.00001792
Iteration 12/1000 | Loss: 0.00001773
Iteration 13/1000 | Loss: 0.00001750
Iteration 14/1000 | Loss: 0.00001737
Iteration 15/1000 | Loss: 0.00001736
Iteration 16/1000 | Loss: 0.00001735
Iteration 17/1000 | Loss: 0.00001732
Iteration 18/1000 | Loss: 0.00001724
Iteration 19/1000 | Loss: 0.00001722
Iteration 20/1000 | Loss: 0.00001722
Iteration 21/1000 | Loss: 0.00001718
Iteration 22/1000 | Loss: 0.00001714
Iteration 23/1000 | Loss: 0.00001713
Iteration 24/1000 | Loss: 0.00001708
Iteration 25/1000 | Loss: 0.00001707
Iteration 26/1000 | Loss: 0.00001707
Iteration 27/1000 | Loss: 0.00001706
Iteration 28/1000 | Loss: 0.00001704
Iteration 29/1000 | Loss: 0.00001700
Iteration 30/1000 | Loss: 0.00001699
Iteration 31/1000 | Loss: 0.00001698
Iteration 32/1000 | Loss: 0.00001697
Iteration 33/1000 | Loss: 0.00001697
Iteration 34/1000 | Loss: 0.00001697
Iteration 35/1000 | Loss: 0.00001696
Iteration 36/1000 | Loss: 0.00001695
Iteration 37/1000 | Loss: 0.00001695
Iteration 38/1000 | Loss: 0.00001694
Iteration 39/1000 | Loss: 0.00001693
Iteration 40/1000 | Loss: 0.00001693
Iteration 41/1000 | Loss: 0.00001693
Iteration 42/1000 | Loss: 0.00001693
Iteration 43/1000 | Loss: 0.00001692
Iteration 44/1000 | Loss: 0.00001692
Iteration 45/1000 | Loss: 0.00001692
Iteration 46/1000 | Loss: 0.00001691
Iteration 47/1000 | Loss: 0.00001691
Iteration 48/1000 | Loss: 0.00001690
Iteration 49/1000 | Loss: 0.00001690
Iteration 50/1000 | Loss: 0.00001690
Iteration 51/1000 | Loss: 0.00001690
Iteration 52/1000 | Loss: 0.00001690
Iteration 53/1000 | Loss: 0.00001690
Iteration 54/1000 | Loss: 0.00001690
Iteration 55/1000 | Loss: 0.00001690
Iteration 56/1000 | Loss: 0.00001690
Iteration 57/1000 | Loss: 0.00001690
Iteration 58/1000 | Loss: 0.00001689
Iteration 59/1000 | Loss: 0.00001689
Iteration 60/1000 | Loss: 0.00001689
Iteration 61/1000 | Loss: 0.00001689
Iteration 62/1000 | Loss: 0.00001689
Iteration 63/1000 | Loss: 0.00001688
Iteration 64/1000 | Loss: 0.00001688
Iteration 65/1000 | Loss: 0.00001688
Iteration 66/1000 | Loss: 0.00001687
Iteration 67/1000 | Loss: 0.00001687
Iteration 68/1000 | Loss: 0.00001687
Iteration 69/1000 | Loss: 0.00001686
Iteration 70/1000 | Loss: 0.00001686
Iteration 71/1000 | Loss: 0.00001686
Iteration 72/1000 | Loss: 0.00001686
Iteration 73/1000 | Loss: 0.00001686
Iteration 74/1000 | Loss: 0.00001686
Iteration 75/1000 | Loss: 0.00001686
Iteration 76/1000 | Loss: 0.00001685
Iteration 77/1000 | Loss: 0.00001685
Iteration 78/1000 | Loss: 0.00001685
Iteration 79/1000 | Loss: 0.00001685
Iteration 80/1000 | Loss: 0.00001685
Iteration 81/1000 | Loss: 0.00001685
Iteration 82/1000 | Loss: 0.00001684
Iteration 83/1000 | Loss: 0.00001684
Iteration 84/1000 | Loss: 0.00001684
Iteration 85/1000 | Loss: 0.00001684
Iteration 86/1000 | Loss: 0.00001684
Iteration 87/1000 | Loss: 0.00001683
Iteration 88/1000 | Loss: 0.00001683
Iteration 89/1000 | Loss: 0.00001683
Iteration 90/1000 | Loss: 0.00001683
Iteration 91/1000 | Loss: 0.00001683
Iteration 92/1000 | Loss: 0.00001683
Iteration 93/1000 | Loss: 0.00001682
Iteration 94/1000 | Loss: 0.00001682
Iteration 95/1000 | Loss: 0.00001682
Iteration 96/1000 | Loss: 0.00001682
Iteration 97/1000 | Loss: 0.00001682
Iteration 98/1000 | Loss: 0.00001682
Iteration 99/1000 | Loss: 0.00001682
Iteration 100/1000 | Loss: 0.00001681
Iteration 101/1000 | Loss: 0.00001681
Iteration 102/1000 | Loss: 0.00001681
Iteration 103/1000 | Loss: 0.00001681
Iteration 104/1000 | Loss: 0.00001681
Iteration 105/1000 | Loss: 0.00001681
Iteration 106/1000 | Loss: 0.00001681
Iteration 107/1000 | Loss: 0.00001681
Iteration 108/1000 | Loss: 0.00001681
Iteration 109/1000 | Loss: 0.00001681
Iteration 110/1000 | Loss: 0.00001681
Iteration 111/1000 | Loss: 0.00001681
Iteration 112/1000 | Loss: 0.00001680
Iteration 113/1000 | Loss: 0.00001680
Iteration 114/1000 | Loss: 0.00001680
Iteration 115/1000 | Loss: 0.00001680
Iteration 116/1000 | Loss: 0.00001680
Iteration 117/1000 | Loss: 0.00001680
Iteration 118/1000 | Loss: 0.00001680
Iteration 119/1000 | Loss: 0.00001680
Iteration 120/1000 | Loss: 0.00001680
Iteration 121/1000 | Loss: 0.00001680
Iteration 122/1000 | Loss: 0.00001680
Iteration 123/1000 | Loss: 0.00001680
Iteration 124/1000 | Loss: 0.00001680
Iteration 125/1000 | Loss: 0.00001680
Iteration 126/1000 | Loss: 0.00001680
Iteration 127/1000 | Loss: 0.00001680
Iteration 128/1000 | Loss: 0.00001680
Iteration 129/1000 | Loss: 0.00001680
Iteration 130/1000 | Loss: 0.00001680
Iteration 131/1000 | Loss: 0.00001680
Iteration 132/1000 | Loss: 0.00001680
Iteration 133/1000 | Loss: 0.00001680
Iteration 134/1000 | Loss: 0.00001680
Iteration 135/1000 | Loss: 0.00001680
Iteration 136/1000 | Loss: 0.00001680
Iteration 137/1000 | Loss: 0.00001680
Iteration 138/1000 | Loss: 0.00001680
Iteration 139/1000 | Loss: 0.00001680
Iteration 140/1000 | Loss: 0.00001680
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 140. Stopping optimization.
Last 5 losses: [1.6798690921859816e-05, 1.6798690921859816e-05, 1.6798690921859816e-05, 1.6798690921859816e-05, 1.6798690921859816e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6798690921859816e-05

Optimization complete. Final v2v error: 3.5064852237701416 mm

Highest mean error: 4.218841075897217 mm for frame 30

Lowest mean error: 3.3471944332122803 mm for frame 16

Saving results

Total time: 40.904263973236084
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_015/1084/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_015/1084.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_015/1084
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00918869
Iteration 2/25 | Loss: 0.00164476
Iteration 3/25 | Loss: 0.00132480
Iteration 4/25 | Loss: 0.00126299
Iteration 5/25 | Loss: 0.00124785
Iteration 6/25 | Loss: 0.00124406
Iteration 7/25 | Loss: 0.00124271
Iteration 8/25 | Loss: 0.00124223
Iteration 9/25 | Loss: 0.00124201
Iteration 10/25 | Loss: 0.00124195
Iteration 11/25 | Loss: 0.00124195
Iteration 12/25 | Loss: 0.00124195
Iteration 13/25 | Loss: 0.00124195
Iteration 14/25 | Loss: 0.00124195
Iteration 15/25 | Loss: 0.00124195
Iteration 16/25 | Loss: 0.00124195
Iteration 17/25 | Loss: 0.00124195
Iteration 18/25 | Loss: 0.00124195
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0012419490376487374, 0.0012419490376487374, 0.0012419490376487374, 0.0012419490376487374, 0.0012419490376487374]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012419490376487374

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.24107909
Iteration 2/25 | Loss: 0.00251342
Iteration 3/25 | Loss: 0.00251340
Iteration 4/25 | Loss: 0.00251340
Iteration 5/25 | Loss: 0.00251340
Iteration 6/25 | Loss: 0.00251340
Iteration 7/25 | Loss: 0.00251340
Iteration 8/25 | Loss: 0.00251340
Iteration 9/25 | Loss: 0.00251340
Iteration 10/25 | Loss: 0.00251340
Iteration 11/25 | Loss: 0.00251340
Iteration 12/25 | Loss: 0.00251340
Iteration 13/25 | Loss: 0.00251340
Iteration 14/25 | Loss: 0.00251340
Iteration 15/25 | Loss: 0.00251340
Iteration 16/25 | Loss: 0.00251340
Iteration 17/25 | Loss: 0.00251340
Iteration 18/25 | Loss: 0.00251340
Iteration 19/25 | Loss: 0.00251340
Iteration 20/25 | Loss: 0.00251340
Iteration 21/25 | Loss: 0.00251340
Iteration 22/25 | Loss: 0.00251340
Iteration 23/25 | Loss: 0.00251340
Iteration 24/25 | Loss: 0.00251340
Iteration 25/25 | Loss: 0.00251340

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00251340
Iteration 2/1000 | Loss: 0.00012049
Iteration 3/1000 | Loss: 0.00007828
Iteration 4/1000 | Loss: 0.00006486
Iteration 5/1000 | Loss: 0.00005937
Iteration 6/1000 | Loss: 0.00005686
Iteration 7/1000 | Loss: 0.00005521
Iteration 8/1000 | Loss: 0.00005407
Iteration 9/1000 | Loss: 0.00005291
Iteration 10/1000 | Loss: 0.00005237
Iteration 11/1000 | Loss: 0.00005183
Iteration 12/1000 | Loss: 0.00005149
Iteration 13/1000 | Loss: 0.00005106
Iteration 14/1000 | Loss: 0.00005062
Iteration 15/1000 | Loss: 0.00236944
Iteration 16/1000 | Loss: 0.00437270
Iteration 17/1000 | Loss: 0.00006660
Iteration 18/1000 | Loss: 0.00004755
Iteration 19/1000 | Loss: 0.00003892
Iteration 20/1000 | Loss: 0.00003457
Iteration 21/1000 | Loss: 0.00003021
Iteration 22/1000 | Loss: 0.00002724
Iteration 23/1000 | Loss: 0.00002573
Iteration 24/1000 | Loss: 0.00002451
Iteration 25/1000 | Loss: 0.00002368
Iteration 26/1000 | Loss: 0.00002299
Iteration 27/1000 | Loss: 0.00002260
Iteration 28/1000 | Loss: 0.00002223
Iteration 29/1000 | Loss: 0.00002203
Iteration 30/1000 | Loss: 0.00002189
Iteration 31/1000 | Loss: 0.00002188
Iteration 32/1000 | Loss: 0.00002179
Iteration 33/1000 | Loss: 0.00002160
Iteration 34/1000 | Loss: 0.00002145
Iteration 35/1000 | Loss: 0.00002145
Iteration 36/1000 | Loss: 0.00002143
Iteration 37/1000 | Loss: 0.00002140
Iteration 38/1000 | Loss: 0.00002139
Iteration 39/1000 | Loss: 0.00002138
Iteration 40/1000 | Loss: 0.00002138
Iteration 41/1000 | Loss: 0.00002137
Iteration 42/1000 | Loss: 0.00002137
Iteration 43/1000 | Loss: 0.00002136
Iteration 44/1000 | Loss: 0.00002133
Iteration 45/1000 | Loss: 0.00002133
Iteration 46/1000 | Loss: 0.00002132
Iteration 47/1000 | Loss: 0.00002132
Iteration 48/1000 | Loss: 0.00002131
Iteration 49/1000 | Loss: 0.00002131
Iteration 50/1000 | Loss: 0.00002131
Iteration 51/1000 | Loss: 0.00002131
Iteration 52/1000 | Loss: 0.00002131
Iteration 53/1000 | Loss: 0.00002131
Iteration 54/1000 | Loss: 0.00002131
Iteration 55/1000 | Loss: 0.00002131
Iteration 56/1000 | Loss: 0.00002131
Iteration 57/1000 | Loss: 0.00002130
Iteration 58/1000 | Loss: 0.00002130
Iteration 59/1000 | Loss: 0.00002130
Iteration 60/1000 | Loss: 0.00002130
Iteration 61/1000 | Loss: 0.00002130
Iteration 62/1000 | Loss: 0.00002129
Iteration 63/1000 | Loss: 0.00002129
Iteration 64/1000 | Loss: 0.00002129
Iteration 65/1000 | Loss: 0.00002128
Iteration 66/1000 | Loss: 0.00002128
Iteration 67/1000 | Loss: 0.00002128
Iteration 68/1000 | Loss: 0.00002127
Iteration 69/1000 | Loss: 0.00002127
Iteration 70/1000 | Loss: 0.00002127
Iteration 71/1000 | Loss: 0.00002127
Iteration 72/1000 | Loss: 0.00002126
Iteration 73/1000 | Loss: 0.00002125
Iteration 74/1000 | Loss: 0.00002125
Iteration 75/1000 | Loss: 0.00002125
Iteration 76/1000 | Loss: 0.00002125
Iteration 77/1000 | Loss: 0.00002125
Iteration 78/1000 | Loss: 0.00002125
Iteration 79/1000 | Loss: 0.00002125
Iteration 80/1000 | Loss: 0.00002125
Iteration 81/1000 | Loss: 0.00002124
Iteration 82/1000 | Loss: 0.00002124
Iteration 83/1000 | Loss: 0.00002124
Iteration 84/1000 | Loss: 0.00002124
Iteration 85/1000 | Loss: 0.00002123
Iteration 86/1000 | Loss: 0.00002123
Iteration 87/1000 | Loss: 0.00002123
Iteration 88/1000 | Loss: 0.00002123
Iteration 89/1000 | Loss: 0.00002123
Iteration 90/1000 | Loss: 0.00002123
Iteration 91/1000 | Loss: 0.00002123
Iteration 92/1000 | Loss: 0.00002122
Iteration 93/1000 | Loss: 0.00002122
Iteration 94/1000 | Loss: 0.00002122
Iteration 95/1000 | Loss: 0.00002122
Iteration 96/1000 | Loss: 0.00002122
Iteration 97/1000 | Loss: 0.00002122
Iteration 98/1000 | Loss: 0.00002122
Iteration 99/1000 | Loss: 0.00002122
Iteration 100/1000 | Loss: 0.00002121
Iteration 101/1000 | Loss: 0.00002121
Iteration 102/1000 | Loss: 0.00002121
Iteration 103/1000 | Loss: 0.00002121
Iteration 104/1000 | Loss: 0.00002121
Iteration 105/1000 | Loss: 0.00002121
Iteration 106/1000 | Loss: 0.00002121
Iteration 107/1000 | Loss: 0.00002121
Iteration 108/1000 | Loss: 0.00002121
Iteration 109/1000 | Loss: 0.00002121
Iteration 110/1000 | Loss: 0.00002121
Iteration 111/1000 | Loss: 0.00002121
Iteration 112/1000 | Loss: 0.00002121
Iteration 113/1000 | Loss: 0.00002120
Iteration 114/1000 | Loss: 0.00002120
Iteration 115/1000 | Loss: 0.00002120
Iteration 116/1000 | Loss: 0.00002120
Iteration 117/1000 | Loss: 0.00002120
Iteration 118/1000 | Loss: 0.00002120
Iteration 119/1000 | Loss: 0.00002119
Iteration 120/1000 | Loss: 0.00002119
Iteration 121/1000 | Loss: 0.00002119
Iteration 122/1000 | Loss: 0.00002119
Iteration 123/1000 | Loss: 0.00002119
Iteration 124/1000 | Loss: 0.00002118
Iteration 125/1000 | Loss: 0.00002118
Iteration 126/1000 | Loss: 0.00002118
Iteration 127/1000 | Loss: 0.00002118
Iteration 128/1000 | Loss: 0.00002118
Iteration 129/1000 | Loss: 0.00002118
Iteration 130/1000 | Loss: 0.00002118
Iteration 131/1000 | Loss: 0.00002118
Iteration 132/1000 | Loss: 0.00002118
Iteration 133/1000 | Loss: 0.00002118
Iteration 134/1000 | Loss: 0.00002118
Iteration 135/1000 | Loss: 0.00002118
Iteration 136/1000 | Loss: 0.00002118
Iteration 137/1000 | Loss: 0.00002118
Iteration 138/1000 | Loss: 0.00002118
Iteration 139/1000 | Loss: 0.00002118
Iteration 140/1000 | Loss: 0.00002118
Iteration 141/1000 | Loss: 0.00002118
Iteration 142/1000 | Loss: 0.00002118
Iteration 143/1000 | Loss: 0.00002118
Iteration 144/1000 | Loss: 0.00002118
Iteration 145/1000 | Loss: 0.00002118
Iteration 146/1000 | Loss: 0.00002118
Iteration 147/1000 | Loss: 0.00002118
Iteration 148/1000 | Loss: 0.00002118
Iteration 149/1000 | Loss: 0.00002118
Iteration 150/1000 | Loss: 0.00002118
Iteration 151/1000 | Loss: 0.00002118
Iteration 152/1000 | Loss: 0.00002118
Iteration 153/1000 | Loss: 0.00002118
Iteration 154/1000 | Loss: 0.00002118
Iteration 155/1000 | Loss: 0.00002118
Iteration 156/1000 | Loss: 0.00002118
Iteration 157/1000 | Loss: 0.00002118
Iteration 158/1000 | Loss: 0.00002118
Iteration 159/1000 | Loss: 0.00002118
Iteration 160/1000 | Loss: 0.00002118
Iteration 161/1000 | Loss: 0.00002118
Iteration 162/1000 | Loss: 0.00002118
Iteration 163/1000 | Loss: 0.00002118
Iteration 164/1000 | Loss: 0.00002118
Iteration 165/1000 | Loss: 0.00002118
Iteration 166/1000 | Loss: 0.00002118
Iteration 167/1000 | Loss: 0.00002118
Iteration 168/1000 | Loss: 0.00002118
Iteration 169/1000 | Loss: 0.00002118
Iteration 170/1000 | Loss: 0.00002118
Iteration 171/1000 | Loss: 0.00002118
Iteration 172/1000 | Loss: 0.00002118
Iteration 173/1000 | Loss: 0.00002118
Iteration 174/1000 | Loss: 0.00002118
Iteration 175/1000 | Loss: 0.00002118
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 175. Stopping optimization.
Last 5 losses: [2.1176672817091458e-05, 2.1176672817091458e-05, 2.1176672817091458e-05, 2.1176672817091458e-05, 2.1176672817091458e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1176672817091458e-05

Optimization complete. Final v2v error: 3.8363256454467773 mm

Highest mean error: 4.409725666046143 mm for frame 8

Lowest mean error: 3.5799527168273926 mm for frame 151

Saving results

Total time: 71.29084491729736
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_015/1004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_015/1004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_015/1004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00563318
Iteration 2/25 | Loss: 0.00123499
Iteration 3/25 | Loss: 0.00116694
Iteration 4/25 | Loss: 0.00115779
Iteration 5/25 | Loss: 0.00115467
Iteration 6/25 | Loss: 0.00115398
Iteration 7/25 | Loss: 0.00115398
Iteration 8/25 | Loss: 0.00115398
Iteration 9/25 | Loss: 0.00115398
Iteration 10/25 | Loss: 0.00115398
Iteration 11/25 | Loss: 0.00115398
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011539789848029613, 0.0011539789848029613, 0.0011539789848029613, 0.0011539789848029613, 0.0011539789848029613]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011539789848029613

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.79261017
Iteration 2/25 | Loss: 0.00132633
Iteration 3/25 | Loss: 0.00132633
Iteration 4/25 | Loss: 0.00132632
Iteration 5/25 | Loss: 0.00132632
Iteration 6/25 | Loss: 0.00132632
Iteration 7/25 | Loss: 0.00132632
Iteration 8/25 | Loss: 0.00132632
Iteration 9/25 | Loss: 0.00132632
Iteration 10/25 | Loss: 0.00132632
Iteration 11/25 | Loss: 0.00132632
Iteration 12/25 | Loss: 0.00132632
Iteration 13/25 | Loss: 0.00132632
Iteration 14/25 | Loss: 0.00132632
Iteration 15/25 | Loss: 0.00132632
Iteration 16/25 | Loss: 0.00132632
Iteration 17/25 | Loss: 0.00132632
Iteration 18/25 | Loss: 0.00132632
Iteration 19/25 | Loss: 0.00132632
Iteration 20/25 | Loss: 0.00132632
Iteration 21/25 | Loss: 0.00132632
Iteration 22/25 | Loss: 0.00132632
Iteration 23/25 | Loss: 0.00132632
Iteration 24/25 | Loss: 0.00132632
Iteration 25/25 | Loss: 0.00132632

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00132632
Iteration 2/1000 | Loss: 0.00001675
Iteration 3/1000 | Loss: 0.00001300
Iteration 4/1000 | Loss: 0.00001170
Iteration 5/1000 | Loss: 0.00001107
Iteration 6/1000 | Loss: 0.00001044
Iteration 7/1000 | Loss: 0.00001002
Iteration 8/1000 | Loss: 0.00000976
Iteration 9/1000 | Loss: 0.00000948
Iteration 10/1000 | Loss: 0.00000931
Iteration 11/1000 | Loss: 0.00000929
Iteration 12/1000 | Loss: 0.00000924
Iteration 13/1000 | Loss: 0.00000921
Iteration 14/1000 | Loss: 0.00000916
Iteration 15/1000 | Loss: 0.00000915
Iteration 16/1000 | Loss: 0.00000915
Iteration 17/1000 | Loss: 0.00000914
Iteration 18/1000 | Loss: 0.00000912
Iteration 19/1000 | Loss: 0.00000911
Iteration 20/1000 | Loss: 0.00000906
Iteration 21/1000 | Loss: 0.00000905
Iteration 22/1000 | Loss: 0.00000904
Iteration 23/1000 | Loss: 0.00000904
Iteration 24/1000 | Loss: 0.00000903
Iteration 25/1000 | Loss: 0.00000902
Iteration 26/1000 | Loss: 0.00000902
Iteration 27/1000 | Loss: 0.00000902
Iteration 28/1000 | Loss: 0.00000902
Iteration 29/1000 | Loss: 0.00000897
Iteration 30/1000 | Loss: 0.00000896
Iteration 31/1000 | Loss: 0.00000895
Iteration 32/1000 | Loss: 0.00000895
Iteration 33/1000 | Loss: 0.00000893
Iteration 34/1000 | Loss: 0.00000891
Iteration 35/1000 | Loss: 0.00000890
Iteration 36/1000 | Loss: 0.00000889
Iteration 37/1000 | Loss: 0.00000889
Iteration 38/1000 | Loss: 0.00000889
Iteration 39/1000 | Loss: 0.00000889
Iteration 40/1000 | Loss: 0.00000889
Iteration 41/1000 | Loss: 0.00000889
Iteration 42/1000 | Loss: 0.00000889
Iteration 43/1000 | Loss: 0.00000889
Iteration 44/1000 | Loss: 0.00000889
Iteration 45/1000 | Loss: 0.00000889
Iteration 46/1000 | Loss: 0.00000889
Iteration 47/1000 | Loss: 0.00000888
Iteration 48/1000 | Loss: 0.00000888
Iteration 49/1000 | Loss: 0.00000888
Iteration 50/1000 | Loss: 0.00000888
Iteration 51/1000 | Loss: 0.00000888
Iteration 52/1000 | Loss: 0.00000884
Iteration 53/1000 | Loss: 0.00000883
Iteration 54/1000 | Loss: 0.00000883
Iteration 55/1000 | Loss: 0.00000882
Iteration 56/1000 | Loss: 0.00000881
Iteration 57/1000 | Loss: 0.00000880
Iteration 58/1000 | Loss: 0.00000878
Iteration 59/1000 | Loss: 0.00000877
Iteration 60/1000 | Loss: 0.00000877
Iteration 61/1000 | Loss: 0.00000877
Iteration 62/1000 | Loss: 0.00000877
Iteration 63/1000 | Loss: 0.00000876
Iteration 64/1000 | Loss: 0.00000875
Iteration 65/1000 | Loss: 0.00000875
Iteration 66/1000 | Loss: 0.00000874
Iteration 67/1000 | Loss: 0.00000873
Iteration 68/1000 | Loss: 0.00000873
Iteration 69/1000 | Loss: 0.00000872
Iteration 70/1000 | Loss: 0.00000872
Iteration 71/1000 | Loss: 0.00000872
Iteration 72/1000 | Loss: 0.00000871
Iteration 73/1000 | Loss: 0.00000871
Iteration 74/1000 | Loss: 0.00000871
Iteration 75/1000 | Loss: 0.00000871
Iteration 76/1000 | Loss: 0.00000870
Iteration 77/1000 | Loss: 0.00000870
Iteration 78/1000 | Loss: 0.00000870
Iteration 79/1000 | Loss: 0.00000870
Iteration 80/1000 | Loss: 0.00000870
Iteration 81/1000 | Loss: 0.00000869
Iteration 82/1000 | Loss: 0.00000869
Iteration 83/1000 | Loss: 0.00000869
Iteration 84/1000 | Loss: 0.00000869
Iteration 85/1000 | Loss: 0.00000869
Iteration 86/1000 | Loss: 0.00000868
Iteration 87/1000 | Loss: 0.00000868
Iteration 88/1000 | Loss: 0.00000868
Iteration 89/1000 | Loss: 0.00000868
Iteration 90/1000 | Loss: 0.00000868
Iteration 91/1000 | Loss: 0.00000868
Iteration 92/1000 | Loss: 0.00000868
Iteration 93/1000 | Loss: 0.00000868
Iteration 94/1000 | Loss: 0.00000868
Iteration 95/1000 | Loss: 0.00000867
Iteration 96/1000 | Loss: 0.00000867
Iteration 97/1000 | Loss: 0.00000867
Iteration 98/1000 | Loss: 0.00000867
Iteration 99/1000 | Loss: 0.00000867
Iteration 100/1000 | Loss: 0.00000867
Iteration 101/1000 | Loss: 0.00000867
Iteration 102/1000 | Loss: 0.00000867
Iteration 103/1000 | Loss: 0.00000867
Iteration 104/1000 | Loss: 0.00000867
Iteration 105/1000 | Loss: 0.00000867
Iteration 106/1000 | Loss: 0.00000867
Iteration 107/1000 | Loss: 0.00000867
Iteration 108/1000 | Loss: 0.00000867
Iteration 109/1000 | Loss: 0.00000866
Iteration 110/1000 | Loss: 0.00000866
Iteration 111/1000 | Loss: 0.00000866
Iteration 112/1000 | Loss: 0.00000866
Iteration 113/1000 | Loss: 0.00000866
Iteration 114/1000 | Loss: 0.00000866
Iteration 115/1000 | Loss: 0.00000866
Iteration 116/1000 | Loss: 0.00000866
Iteration 117/1000 | Loss: 0.00000866
Iteration 118/1000 | Loss: 0.00000865
Iteration 119/1000 | Loss: 0.00000865
Iteration 120/1000 | Loss: 0.00000865
Iteration 121/1000 | Loss: 0.00000865
Iteration 122/1000 | Loss: 0.00000865
Iteration 123/1000 | Loss: 0.00000865
Iteration 124/1000 | Loss: 0.00000865
Iteration 125/1000 | Loss: 0.00000865
Iteration 126/1000 | Loss: 0.00000865
Iteration 127/1000 | Loss: 0.00000865
Iteration 128/1000 | Loss: 0.00000865
Iteration 129/1000 | Loss: 0.00000865
Iteration 130/1000 | Loss: 0.00000864
Iteration 131/1000 | Loss: 0.00000864
Iteration 132/1000 | Loss: 0.00000864
Iteration 133/1000 | Loss: 0.00000863
Iteration 134/1000 | Loss: 0.00000863
Iteration 135/1000 | Loss: 0.00000863
Iteration 136/1000 | Loss: 0.00000863
Iteration 137/1000 | Loss: 0.00000862
Iteration 138/1000 | Loss: 0.00000862
Iteration 139/1000 | Loss: 0.00000862
Iteration 140/1000 | Loss: 0.00000862
Iteration 141/1000 | Loss: 0.00000862
Iteration 142/1000 | Loss: 0.00000862
Iteration 143/1000 | Loss: 0.00000862
Iteration 144/1000 | Loss: 0.00000862
Iteration 145/1000 | Loss: 0.00000862
Iteration 146/1000 | Loss: 0.00000862
Iteration 147/1000 | Loss: 0.00000861
Iteration 148/1000 | Loss: 0.00000861
Iteration 149/1000 | Loss: 0.00000861
Iteration 150/1000 | Loss: 0.00000861
Iteration 151/1000 | Loss: 0.00000861
Iteration 152/1000 | Loss: 0.00000861
Iteration 153/1000 | Loss: 0.00000861
Iteration 154/1000 | Loss: 0.00000861
Iteration 155/1000 | Loss: 0.00000860
Iteration 156/1000 | Loss: 0.00000860
Iteration 157/1000 | Loss: 0.00000860
Iteration 158/1000 | Loss: 0.00000860
Iteration 159/1000 | Loss: 0.00000860
Iteration 160/1000 | Loss: 0.00000860
Iteration 161/1000 | Loss: 0.00000860
Iteration 162/1000 | Loss: 0.00000859
Iteration 163/1000 | Loss: 0.00000859
Iteration 164/1000 | Loss: 0.00000859
Iteration 165/1000 | Loss: 0.00000859
Iteration 166/1000 | Loss: 0.00000859
Iteration 167/1000 | Loss: 0.00000859
Iteration 168/1000 | Loss: 0.00000859
Iteration 169/1000 | Loss: 0.00000859
Iteration 170/1000 | Loss: 0.00000859
Iteration 171/1000 | Loss: 0.00000859
Iteration 172/1000 | Loss: 0.00000859
Iteration 173/1000 | Loss: 0.00000859
Iteration 174/1000 | Loss: 0.00000859
Iteration 175/1000 | Loss: 0.00000859
Iteration 176/1000 | Loss: 0.00000859
Iteration 177/1000 | Loss: 0.00000859
Iteration 178/1000 | Loss: 0.00000859
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 178. Stopping optimization.
Last 5 losses: [8.5886349552311e-06, 8.5886349552311e-06, 8.5886349552311e-06, 8.5886349552311e-06, 8.5886349552311e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.5886349552311e-06

Optimization complete. Final v2v error: 2.562485694885254 mm

Highest mean error: 2.914409637451172 mm for frame 118

Lowest mean error: 2.452052116394043 mm for frame 46

Saving results

Total time: 39.09662127494812
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_015/1086/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_015/1086.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_015/1086
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00812365
Iteration 2/25 | Loss: 0.00125212
Iteration 3/25 | Loss: 0.00116657
Iteration 4/25 | Loss: 0.00115977
Iteration 5/25 | Loss: 0.00115840
Iteration 6/25 | Loss: 0.00115840
Iteration 7/25 | Loss: 0.00115840
Iteration 8/25 | Loss: 0.00115840
Iteration 9/25 | Loss: 0.00115840
Iteration 10/25 | Loss: 0.00115840
Iteration 11/25 | Loss: 0.00115840
Iteration 12/25 | Loss: 0.00115840
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0011583976447582245, 0.0011583976447582245, 0.0011583976447582245, 0.0011583976447582245, 0.0011583976447582245]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011583976447582245

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.29291272
Iteration 2/25 | Loss: 0.00133893
Iteration 3/25 | Loss: 0.00133893
Iteration 4/25 | Loss: 0.00133893
Iteration 5/25 | Loss: 0.00133893
Iteration 6/25 | Loss: 0.00133893
Iteration 7/25 | Loss: 0.00133893
Iteration 8/25 | Loss: 0.00133893
Iteration 9/25 | Loss: 0.00133893
Iteration 10/25 | Loss: 0.00133893
Iteration 11/25 | Loss: 0.00133893
Iteration 12/25 | Loss: 0.00133893
Iteration 13/25 | Loss: 0.00133893
Iteration 14/25 | Loss: 0.00133893
Iteration 15/25 | Loss: 0.00133893
Iteration 16/25 | Loss: 0.00133893
Iteration 17/25 | Loss: 0.00133893
Iteration 18/25 | Loss: 0.00133893
Iteration 19/25 | Loss: 0.00133893
Iteration 20/25 | Loss: 0.00133893
Iteration 21/25 | Loss: 0.00133893
Iteration 22/25 | Loss: 0.00133893
Iteration 23/25 | Loss: 0.00133893
Iteration 24/25 | Loss: 0.00133893
Iteration 25/25 | Loss: 0.00133893

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00133893
Iteration 2/1000 | Loss: 0.00001767
Iteration 3/1000 | Loss: 0.00001290
Iteration 4/1000 | Loss: 0.00001179
Iteration 5/1000 | Loss: 0.00001113
Iteration 6/1000 | Loss: 0.00001060
Iteration 7/1000 | Loss: 0.00001018
Iteration 8/1000 | Loss: 0.00000997
Iteration 9/1000 | Loss: 0.00000968
Iteration 10/1000 | Loss: 0.00000947
Iteration 11/1000 | Loss: 0.00000938
Iteration 12/1000 | Loss: 0.00000928
Iteration 13/1000 | Loss: 0.00000919
Iteration 14/1000 | Loss: 0.00000912
Iteration 15/1000 | Loss: 0.00000907
Iteration 16/1000 | Loss: 0.00000906
Iteration 17/1000 | Loss: 0.00000906
Iteration 18/1000 | Loss: 0.00000905
Iteration 19/1000 | Loss: 0.00000904
Iteration 20/1000 | Loss: 0.00000903
Iteration 21/1000 | Loss: 0.00000903
Iteration 22/1000 | Loss: 0.00000903
Iteration 23/1000 | Loss: 0.00000902
Iteration 24/1000 | Loss: 0.00000901
Iteration 25/1000 | Loss: 0.00000901
Iteration 26/1000 | Loss: 0.00000900
Iteration 27/1000 | Loss: 0.00000900
Iteration 28/1000 | Loss: 0.00000899
Iteration 29/1000 | Loss: 0.00000899
Iteration 30/1000 | Loss: 0.00000898
Iteration 31/1000 | Loss: 0.00000897
Iteration 32/1000 | Loss: 0.00000897
Iteration 33/1000 | Loss: 0.00000896
Iteration 34/1000 | Loss: 0.00000896
Iteration 35/1000 | Loss: 0.00000895
Iteration 36/1000 | Loss: 0.00000894
Iteration 37/1000 | Loss: 0.00000894
Iteration 38/1000 | Loss: 0.00000892
Iteration 39/1000 | Loss: 0.00000891
Iteration 40/1000 | Loss: 0.00000889
Iteration 41/1000 | Loss: 0.00000888
Iteration 42/1000 | Loss: 0.00000883
Iteration 43/1000 | Loss: 0.00000883
Iteration 44/1000 | Loss: 0.00000883
Iteration 45/1000 | Loss: 0.00000883
Iteration 46/1000 | Loss: 0.00000876
Iteration 47/1000 | Loss: 0.00000876
Iteration 48/1000 | Loss: 0.00000876
Iteration 49/1000 | Loss: 0.00000876
Iteration 50/1000 | Loss: 0.00000876
Iteration 51/1000 | Loss: 0.00000876
Iteration 52/1000 | Loss: 0.00000876
Iteration 53/1000 | Loss: 0.00000876
Iteration 54/1000 | Loss: 0.00000875
Iteration 55/1000 | Loss: 0.00000875
Iteration 56/1000 | Loss: 0.00000875
Iteration 57/1000 | Loss: 0.00000875
Iteration 58/1000 | Loss: 0.00000875
Iteration 59/1000 | Loss: 0.00000875
Iteration 60/1000 | Loss: 0.00000875
Iteration 61/1000 | Loss: 0.00000875
Iteration 62/1000 | Loss: 0.00000875
Iteration 63/1000 | Loss: 0.00000875
Iteration 64/1000 | Loss: 0.00000875
Iteration 65/1000 | Loss: 0.00000875
Iteration 66/1000 | Loss: 0.00000875
Iteration 67/1000 | Loss: 0.00000875
Iteration 68/1000 | Loss: 0.00000875
Iteration 69/1000 | Loss: 0.00000875
Iteration 70/1000 | Loss: 0.00000875
Iteration 71/1000 | Loss: 0.00000875
Iteration 72/1000 | Loss: 0.00000875
Iteration 73/1000 | Loss: 0.00000875
Iteration 74/1000 | Loss: 0.00000875
Iteration 75/1000 | Loss: 0.00000875
Iteration 76/1000 | Loss: 0.00000875
Iteration 77/1000 | Loss: 0.00000875
Iteration 78/1000 | Loss: 0.00000875
Iteration 79/1000 | Loss: 0.00000875
Iteration 80/1000 | Loss: 0.00000875
Iteration 81/1000 | Loss: 0.00000875
Iteration 82/1000 | Loss: 0.00000875
Iteration 83/1000 | Loss: 0.00000874
Iteration 84/1000 | Loss: 0.00000874
Iteration 85/1000 | Loss: 0.00000874
Iteration 86/1000 | Loss: 0.00000874
Iteration 87/1000 | Loss: 0.00000874
Iteration 88/1000 | Loss: 0.00000874
Iteration 89/1000 | Loss: 0.00000874
Iteration 90/1000 | Loss: 0.00000874
Iteration 91/1000 | Loss: 0.00000874
Iteration 92/1000 | Loss: 0.00000874
Iteration 93/1000 | Loss: 0.00000874
Iteration 94/1000 | Loss: 0.00000874
Iteration 95/1000 | Loss: 0.00000874
Iteration 96/1000 | Loss: 0.00000874
Iteration 97/1000 | Loss: 0.00000874
Iteration 98/1000 | Loss: 0.00000872
Iteration 99/1000 | Loss: 0.00000872
Iteration 100/1000 | Loss: 0.00000871
Iteration 101/1000 | Loss: 0.00000871
Iteration 102/1000 | Loss: 0.00000871
Iteration 103/1000 | Loss: 0.00000870
Iteration 104/1000 | Loss: 0.00000870
Iteration 105/1000 | Loss: 0.00000870
Iteration 106/1000 | Loss: 0.00000869
Iteration 107/1000 | Loss: 0.00000868
Iteration 108/1000 | Loss: 0.00000868
Iteration 109/1000 | Loss: 0.00000867
Iteration 110/1000 | Loss: 0.00000867
Iteration 111/1000 | Loss: 0.00000866
Iteration 112/1000 | Loss: 0.00000866
Iteration 113/1000 | Loss: 0.00000866
Iteration 114/1000 | Loss: 0.00000866
Iteration 115/1000 | Loss: 0.00000865
Iteration 116/1000 | Loss: 0.00000865
Iteration 117/1000 | Loss: 0.00000865
Iteration 118/1000 | Loss: 0.00000865
Iteration 119/1000 | Loss: 0.00000865
Iteration 120/1000 | Loss: 0.00000865
Iteration 121/1000 | Loss: 0.00000865
Iteration 122/1000 | Loss: 0.00000865
Iteration 123/1000 | Loss: 0.00000864
Iteration 124/1000 | Loss: 0.00000864
Iteration 125/1000 | Loss: 0.00000863
Iteration 126/1000 | Loss: 0.00000863
Iteration 127/1000 | Loss: 0.00000863
Iteration 128/1000 | Loss: 0.00000863
Iteration 129/1000 | Loss: 0.00000863
Iteration 130/1000 | Loss: 0.00000863
Iteration 131/1000 | Loss: 0.00000863
Iteration 132/1000 | Loss: 0.00000862
Iteration 133/1000 | Loss: 0.00000861
Iteration 134/1000 | Loss: 0.00000861
Iteration 135/1000 | Loss: 0.00000861
Iteration 136/1000 | Loss: 0.00000861
Iteration 137/1000 | Loss: 0.00000861
Iteration 138/1000 | Loss: 0.00000861
Iteration 139/1000 | Loss: 0.00000860
Iteration 140/1000 | Loss: 0.00000860
Iteration 141/1000 | Loss: 0.00000860
Iteration 142/1000 | Loss: 0.00000860
Iteration 143/1000 | Loss: 0.00000860
Iteration 144/1000 | Loss: 0.00000860
Iteration 145/1000 | Loss: 0.00000859
Iteration 146/1000 | Loss: 0.00000859
Iteration 147/1000 | Loss: 0.00000859
Iteration 148/1000 | Loss: 0.00000859
Iteration 149/1000 | Loss: 0.00000859
Iteration 150/1000 | Loss: 0.00000859
Iteration 151/1000 | Loss: 0.00000859
Iteration 152/1000 | Loss: 0.00000859
Iteration 153/1000 | Loss: 0.00000858
Iteration 154/1000 | Loss: 0.00000858
Iteration 155/1000 | Loss: 0.00000858
Iteration 156/1000 | Loss: 0.00000857
Iteration 157/1000 | Loss: 0.00000857
Iteration 158/1000 | Loss: 0.00000857
Iteration 159/1000 | Loss: 0.00000857
Iteration 160/1000 | Loss: 0.00000857
Iteration 161/1000 | Loss: 0.00000856
Iteration 162/1000 | Loss: 0.00000856
Iteration 163/1000 | Loss: 0.00000856
Iteration 164/1000 | Loss: 0.00000856
Iteration 165/1000 | Loss: 0.00000856
Iteration 166/1000 | Loss: 0.00000856
Iteration 167/1000 | Loss: 0.00000856
Iteration 168/1000 | Loss: 0.00000856
Iteration 169/1000 | Loss: 0.00000856
Iteration 170/1000 | Loss: 0.00000856
Iteration 171/1000 | Loss: 0.00000855
Iteration 172/1000 | Loss: 0.00000855
Iteration 173/1000 | Loss: 0.00000854
Iteration 174/1000 | Loss: 0.00000854
Iteration 175/1000 | Loss: 0.00000854
Iteration 176/1000 | Loss: 0.00000854
Iteration 177/1000 | Loss: 0.00000854
Iteration 178/1000 | Loss: 0.00000854
Iteration 179/1000 | Loss: 0.00000854
Iteration 180/1000 | Loss: 0.00000854
Iteration 181/1000 | Loss: 0.00000854
Iteration 182/1000 | Loss: 0.00000853
Iteration 183/1000 | Loss: 0.00000853
Iteration 184/1000 | Loss: 0.00000853
Iteration 185/1000 | Loss: 0.00000852
Iteration 186/1000 | Loss: 0.00000852
Iteration 187/1000 | Loss: 0.00000852
Iteration 188/1000 | Loss: 0.00000852
Iteration 189/1000 | Loss: 0.00000852
Iteration 190/1000 | Loss: 0.00000852
Iteration 191/1000 | Loss: 0.00000852
Iteration 192/1000 | Loss: 0.00000851
Iteration 193/1000 | Loss: 0.00000851
Iteration 194/1000 | Loss: 0.00000851
Iteration 195/1000 | Loss: 0.00000850
Iteration 196/1000 | Loss: 0.00000850
Iteration 197/1000 | Loss: 0.00000850
Iteration 198/1000 | Loss: 0.00000850
Iteration 199/1000 | Loss: 0.00000850
Iteration 200/1000 | Loss: 0.00000850
Iteration 201/1000 | Loss: 0.00000850
Iteration 202/1000 | Loss: 0.00000850
Iteration 203/1000 | Loss: 0.00000850
Iteration 204/1000 | Loss: 0.00000850
Iteration 205/1000 | Loss: 0.00000849
Iteration 206/1000 | Loss: 0.00000849
Iteration 207/1000 | Loss: 0.00000849
Iteration 208/1000 | Loss: 0.00000849
Iteration 209/1000 | Loss: 0.00000849
Iteration 210/1000 | Loss: 0.00000849
Iteration 211/1000 | Loss: 0.00000849
Iteration 212/1000 | Loss: 0.00000849
Iteration 213/1000 | Loss: 0.00000849
Iteration 214/1000 | Loss: 0.00000848
Iteration 215/1000 | Loss: 0.00000848
Iteration 216/1000 | Loss: 0.00000848
Iteration 217/1000 | Loss: 0.00000848
Iteration 218/1000 | Loss: 0.00000848
Iteration 219/1000 | Loss: 0.00000848
Iteration 220/1000 | Loss: 0.00000848
Iteration 221/1000 | Loss: 0.00000847
Iteration 222/1000 | Loss: 0.00000847
Iteration 223/1000 | Loss: 0.00000847
Iteration 224/1000 | Loss: 0.00000847
Iteration 225/1000 | Loss: 0.00000847
Iteration 226/1000 | Loss: 0.00000846
Iteration 227/1000 | Loss: 0.00000846
Iteration 228/1000 | Loss: 0.00000846
Iteration 229/1000 | Loss: 0.00000846
Iteration 230/1000 | Loss: 0.00000846
Iteration 231/1000 | Loss: 0.00000845
Iteration 232/1000 | Loss: 0.00000845
Iteration 233/1000 | Loss: 0.00000845
Iteration 234/1000 | Loss: 0.00000845
Iteration 235/1000 | Loss: 0.00000845
Iteration 236/1000 | Loss: 0.00000844
Iteration 237/1000 | Loss: 0.00000844
Iteration 238/1000 | Loss: 0.00000844
Iteration 239/1000 | Loss: 0.00000844
Iteration 240/1000 | Loss: 0.00000844
Iteration 241/1000 | Loss: 0.00000844
Iteration 242/1000 | Loss: 0.00000844
Iteration 243/1000 | Loss: 0.00000843
Iteration 244/1000 | Loss: 0.00000843
Iteration 245/1000 | Loss: 0.00000843
Iteration 246/1000 | Loss: 0.00000843
Iteration 247/1000 | Loss: 0.00000843
Iteration 248/1000 | Loss: 0.00000843
Iteration 249/1000 | Loss: 0.00000843
Iteration 250/1000 | Loss: 0.00000843
Iteration 251/1000 | Loss: 0.00000843
Iteration 252/1000 | Loss: 0.00000843
Iteration 253/1000 | Loss: 0.00000843
Iteration 254/1000 | Loss: 0.00000842
Iteration 255/1000 | Loss: 0.00000842
Iteration 256/1000 | Loss: 0.00000842
Iteration 257/1000 | Loss: 0.00000842
Iteration 258/1000 | Loss: 0.00000842
Iteration 259/1000 | Loss: 0.00000842
Iteration 260/1000 | Loss: 0.00000841
Iteration 261/1000 | Loss: 0.00000841
Iteration 262/1000 | Loss: 0.00000841
Iteration 263/1000 | Loss: 0.00000841
Iteration 264/1000 | Loss: 0.00000841
Iteration 265/1000 | Loss: 0.00000841
Iteration 266/1000 | Loss: 0.00000841
Iteration 267/1000 | Loss: 0.00000841
Iteration 268/1000 | Loss: 0.00000841
Iteration 269/1000 | Loss: 0.00000841
Iteration 270/1000 | Loss: 0.00000841
Iteration 271/1000 | Loss: 0.00000841
Iteration 272/1000 | Loss: 0.00000841
Iteration 273/1000 | Loss: 0.00000841
Iteration 274/1000 | Loss: 0.00000841
Iteration 275/1000 | Loss: 0.00000841
Iteration 276/1000 | Loss: 0.00000841
Iteration 277/1000 | Loss: 0.00000841
Iteration 278/1000 | Loss: 0.00000841
Iteration 279/1000 | Loss: 0.00000841
Iteration 280/1000 | Loss: 0.00000841
Iteration 281/1000 | Loss: 0.00000841
Iteration 282/1000 | Loss: 0.00000841
Iteration 283/1000 | Loss: 0.00000841
Iteration 284/1000 | Loss: 0.00000841
Iteration 285/1000 | Loss: 0.00000841
Iteration 286/1000 | Loss: 0.00000841
Iteration 287/1000 | Loss: 0.00000841
Iteration 288/1000 | Loss: 0.00000841
Iteration 289/1000 | Loss: 0.00000841
Iteration 290/1000 | Loss: 0.00000841
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 290. Stopping optimization.
Last 5 losses: [8.410089321841951e-06, 8.410089321841951e-06, 8.410089321841951e-06, 8.410089321841951e-06, 8.410089321841951e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.410089321841951e-06

Optimization complete. Final v2v error: 2.482959032058716 mm

Highest mean error: 2.649937868118286 mm for frame 126

Lowest mean error: 2.367344617843628 mm for frame 201

Saving results

Total time: 48.0142936706543
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_015/1066/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_015/1066.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_015/1066
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01027887
Iteration 2/25 | Loss: 0.00243332
Iteration 3/25 | Loss: 0.00164940
Iteration 4/25 | Loss: 0.00134293
Iteration 5/25 | Loss: 0.00128704
Iteration 6/25 | Loss: 0.00126332
Iteration 7/25 | Loss: 0.00123278
Iteration 8/25 | Loss: 0.00123112
Iteration 9/25 | Loss: 0.00119439
Iteration 10/25 | Loss: 0.00119505
Iteration 11/25 | Loss: 0.00117553
Iteration 12/25 | Loss: 0.00116812
Iteration 13/25 | Loss: 0.00116944
Iteration 14/25 | Loss: 0.00116986
Iteration 15/25 | Loss: 0.00116554
Iteration 16/25 | Loss: 0.00117026
Iteration 17/25 | Loss: 0.00117096
Iteration 18/25 | Loss: 0.00116685
Iteration 19/25 | Loss: 0.00116950
Iteration 20/25 | Loss: 0.00116417
Iteration 21/25 | Loss: 0.00116478
Iteration 22/25 | Loss: 0.00116332
Iteration 23/25 | Loss: 0.00116327
Iteration 24/25 | Loss: 0.00116327
Iteration 25/25 | Loss: 0.00116327

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.80166435
Iteration 2/25 | Loss: 0.00133249
Iteration 3/25 | Loss: 0.00129666
Iteration 4/25 | Loss: 0.00129673
Iteration 5/25 | Loss: 0.00127924
Iteration 6/25 | Loss: 0.00127924
Iteration 7/25 | Loss: 0.00127924
Iteration 8/25 | Loss: 0.00127924
Iteration 9/25 | Loss: 0.00127924
Iteration 10/25 | Loss: 0.00127923
Iteration 11/25 | Loss: 0.00127923
Iteration 12/25 | Loss: 0.00127923
Iteration 13/25 | Loss: 0.00127923
Iteration 14/25 | Loss: 0.00127923
Iteration 15/25 | Loss: 0.00127923
Iteration 16/25 | Loss: 0.00127923
Iteration 17/25 | Loss: 0.00127923
Iteration 18/25 | Loss: 0.00127923
Iteration 19/25 | Loss: 0.00127923
Iteration 20/25 | Loss: 0.00127923
Iteration 21/25 | Loss: 0.00127923
Iteration 22/25 | Loss: 0.00127923
Iteration 23/25 | Loss: 0.00127923
Iteration 24/25 | Loss: 0.00127923
Iteration 25/25 | Loss: 0.00127923

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00127923
Iteration 2/1000 | Loss: 0.00008159
Iteration 3/1000 | Loss: 0.00013129
Iteration 4/1000 | Loss: 0.00008134
Iteration 5/1000 | Loss: 0.00002757
Iteration 6/1000 | Loss: 0.00003513
Iteration 7/1000 | Loss: 0.00004842
Iteration 8/1000 | Loss: 0.00001466
Iteration 9/1000 | Loss: 0.00001402
Iteration 10/1000 | Loss: 0.00003367
Iteration 11/1000 | Loss: 0.00001313
Iteration 12/1000 | Loss: 0.00001306
Iteration 13/1000 | Loss: 0.00005836
Iteration 14/1000 | Loss: 0.00002758
Iteration 15/1000 | Loss: 0.00006681
Iteration 16/1000 | Loss: 0.00008118
Iteration 17/1000 | Loss: 0.00001852
Iteration 18/1000 | Loss: 0.00001382
Iteration 19/1000 | Loss: 0.00001765
Iteration 20/1000 | Loss: 0.00001263
Iteration 21/1000 | Loss: 0.00001232
Iteration 22/1000 | Loss: 0.00001232
Iteration 23/1000 | Loss: 0.00001232
Iteration 24/1000 | Loss: 0.00001232
Iteration 25/1000 | Loss: 0.00001232
Iteration 26/1000 | Loss: 0.00001231
Iteration 27/1000 | Loss: 0.00002058
Iteration 28/1000 | Loss: 0.00002070
Iteration 29/1000 | Loss: 0.00001552
Iteration 30/1000 | Loss: 0.00001858
Iteration 31/1000 | Loss: 0.00001212
Iteration 32/1000 | Loss: 0.00001210
Iteration 33/1000 | Loss: 0.00001209
Iteration 34/1000 | Loss: 0.00003704
Iteration 35/1000 | Loss: 0.00001634
Iteration 36/1000 | Loss: 0.00001196
Iteration 37/1000 | Loss: 0.00001195
Iteration 38/1000 | Loss: 0.00001195
Iteration 39/1000 | Loss: 0.00001195
Iteration 40/1000 | Loss: 0.00001195
Iteration 41/1000 | Loss: 0.00001195
Iteration 42/1000 | Loss: 0.00001195
Iteration 43/1000 | Loss: 0.00001195
Iteration 44/1000 | Loss: 0.00001195
Iteration 45/1000 | Loss: 0.00001622
Iteration 46/1000 | Loss: 0.00006154
Iteration 47/1000 | Loss: 0.00001197
Iteration 48/1000 | Loss: 0.00001184
Iteration 49/1000 | Loss: 0.00001184
Iteration 50/1000 | Loss: 0.00001183
Iteration 51/1000 | Loss: 0.00001183
Iteration 52/1000 | Loss: 0.00001183
Iteration 53/1000 | Loss: 0.00001182
Iteration 54/1000 | Loss: 0.00001182
Iteration 55/1000 | Loss: 0.00001181
Iteration 56/1000 | Loss: 0.00001181
Iteration 57/1000 | Loss: 0.00001181
Iteration 58/1000 | Loss: 0.00001436
Iteration 59/1000 | Loss: 0.00001180
Iteration 60/1000 | Loss: 0.00001180
Iteration 61/1000 | Loss: 0.00001180
Iteration 62/1000 | Loss: 0.00001180
Iteration 63/1000 | Loss: 0.00001180
Iteration 64/1000 | Loss: 0.00001180
Iteration 65/1000 | Loss: 0.00001180
Iteration 66/1000 | Loss: 0.00001179
Iteration 67/1000 | Loss: 0.00001178
Iteration 68/1000 | Loss: 0.00001244
Iteration 69/1000 | Loss: 0.00003790
Iteration 70/1000 | Loss: 0.00001178
Iteration 71/1000 | Loss: 0.00001175
Iteration 72/1000 | Loss: 0.00001174
Iteration 73/1000 | Loss: 0.00001174
Iteration 74/1000 | Loss: 0.00001174
Iteration 75/1000 | Loss: 0.00001174
Iteration 76/1000 | Loss: 0.00001173
Iteration 77/1000 | Loss: 0.00001173
Iteration 78/1000 | Loss: 0.00001172
Iteration 79/1000 | Loss: 0.00001172
Iteration 80/1000 | Loss: 0.00001171
Iteration 81/1000 | Loss: 0.00001171
Iteration 82/1000 | Loss: 0.00001170
Iteration 83/1000 | Loss: 0.00001170
Iteration 84/1000 | Loss: 0.00001169
Iteration 85/1000 | Loss: 0.00001168
Iteration 86/1000 | Loss: 0.00001168
Iteration 87/1000 | Loss: 0.00001167
Iteration 88/1000 | Loss: 0.00001167
Iteration 89/1000 | Loss: 0.00001167
Iteration 90/1000 | Loss: 0.00001167
Iteration 91/1000 | Loss: 0.00001166
Iteration 92/1000 | Loss: 0.00001166
Iteration 93/1000 | Loss: 0.00001166
Iteration 94/1000 | Loss: 0.00001166
Iteration 95/1000 | Loss: 0.00001166
Iteration 96/1000 | Loss: 0.00001166
Iteration 97/1000 | Loss: 0.00001166
Iteration 98/1000 | Loss: 0.00001165
Iteration 99/1000 | Loss: 0.00001165
Iteration 100/1000 | Loss: 0.00001164
Iteration 101/1000 | Loss: 0.00001164
Iteration 102/1000 | Loss: 0.00001164
Iteration 103/1000 | Loss: 0.00001164
Iteration 104/1000 | Loss: 0.00001164
Iteration 105/1000 | Loss: 0.00001164
Iteration 106/1000 | Loss: 0.00001163
Iteration 107/1000 | Loss: 0.00001163
Iteration 108/1000 | Loss: 0.00001163
Iteration 109/1000 | Loss: 0.00001162
Iteration 110/1000 | Loss: 0.00001162
Iteration 111/1000 | Loss: 0.00001162
Iteration 112/1000 | Loss: 0.00001161
Iteration 113/1000 | Loss: 0.00001161
Iteration 114/1000 | Loss: 0.00001161
Iteration 115/1000 | Loss: 0.00001161
Iteration 116/1000 | Loss: 0.00001161
Iteration 117/1000 | Loss: 0.00001161
Iteration 118/1000 | Loss: 0.00001161
Iteration 119/1000 | Loss: 0.00001161
Iteration 120/1000 | Loss: 0.00001161
Iteration 121/1000 | Loss: 0.00001160
Iteration 122/1000 | Loss: 0.00001160
Iteration 123/1000 | Loss: 0.00001160
Iteration 124/1000 | Loss: 0.00001160
Iteration 125/1000 | Loss: 0.00001160
Iteration 126/1000 | Loss: 0.00001160
Iteration 127/1000 | Loss: 0.00001160
Iteration 128/1000 | Loss: 0.00001941
Iteration 129/1000 | Loss: 0.00001163
Iteration 130/1000 | Loss: 0.00001159
Iteration 131/1000 | Loss: 0.00001159
Iteration 132/1000 | Loss: 0.00001158
Iteration 133/1000 | Loss: 0.00001158
Iteration 134/1000 | Loss: 0.00001158
Iteration 135/1000 | Loss: 0.00001200
Iteration 136/1000 | Loss: 0.00001159
Iteration 137/1000 | Loss: 0.00001158
Iteration 138/1000 | Loss: 0.00001158
Iteration 139/1000 | Loss: 0.00001158
Iteration 140/1000 | Loss: 0.00001158
Iteration 141/1000 | Loss: 0.00001158
Iteration 142/1000 | Loss: 0.00001158
Iteration 143/1000 | Loss: 0.00001158
Iteration 144/1000 | Loss: 0.00001158
Iteration 145/1000 | Loss: 0.00001158
Iteration 146/1000 | Loss: 0.00001158
Iteration 147/1000 | Loss: 0.00001157
Iteration 148/1000 | Loss: 0.00001157
Iteration 149/1000 | Loss: 0.00001157
Iteration 150/1000 | Loss: 0.00001157
Iteration 151/1000 | Loss: 0.00001157
Iteration 152/1000 | Loss: 0.00001157
Iteration 153/1000 | Loss: 0.00001157
Iteration 154/1000 | Loss: 0.00001157
Iteration 155/1000 | Loss: 0.00001157
Iteration 156/1000 | Loss: 0.00001157
Iteration 157/1000 | Loss: 0.00001156
Iteration 158/1000 | Loss: 0.00001156
Iteration 159/1000 | Loss: 0.00001156
Iteration 160/1000 | Loss: 0.00001156
Iteration 161/1000 | Loss: 0.00001156
Iteration 162/1000 | Loss: 0.00001156
Iteration 163/1000 | Loss: 0.00001156
Iteration 164/1000 | Loss: 0.00001156
Iteration 165/1000 | Loss: 0.00001156
Iteration 166/1000 | Loss: 0.00001156
Iteration 167/1000 | Loss: 0.00001156
Iteration 168/1000 | Loss: 0.00001156
Iteration 169/1000 | Loss: 0.00001156
Iteration 170/1000 | Loss: 0.00001156
Iteration 171/1000 | Loss: 0.00001156
Iteration 172/1000 | Loss: 0.00001156
Iteration 173/1000 | Loss: 0.00001156
Iteration 174/1000 | Loss: 0.00001156
Iteration 175/1000 | Loss: 0.00001156
Iteration 176/1000 | Loss: 0.00001156
Iteration 177/1000 | Loss: 0.00001156
Iteration 178/1000 | Loss: 0.00001156
Iteration 179/1000 | Loss: 0.00001156
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 179. Stopping optimization.
Last 5 losses: [1.1560989150893874e-05, 1.1560989150893874e-05, 1.1560989150893874e-05, 1.1560989150893874e-05, 1.1560989150893874e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1560989150893874e-05

Optimization complete. Final v2v error: 2.9214770793914795 mm

Highest mean error: 3.2026071548461914 mm for frame 25

Lowest mean error: 2.678807020187378 mm for frame 129

Saving results

Total time: 97.90146780014038
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_015/1005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_015/1005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_015/1005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00973489
Iteration 2/25 | Loss: 0.00240835
Iteration 3/25 | Loss: 0.00170242
Iteration 4/25 | Loss: 0.00162832
Iteration 5/25 | Loss: 0.00155318
Iteration 6/25 | Loss: 0.00152497
Iteration 7/25 | Loss: 0.00155916
Iteration 8/25 | Loss: 0.00145077
Iteration 9/25 | Loss: 0.00129098
Iteration 10/25 | Loss: 0.00125301
Iteration 11/25 | Loss: 0.00121790
Iteration 12/25 | Loss: 0.00122195
Iteration 13/25 | Loss: 0.00119309
Iteration 14/25 | Loss: 0.00118454
Iteration 15/25 | Loss: 0.00118532
Iteration 16/25 | Loss: 0.00118334
Iteration 17/25 | Loss: 0.00118334
Iteration 18/25 | Loss: 0.00118334
Iteration 19/25 | Loss: 0.00118334
Iteration 20/25 | Loss: 0.00118334
Iteration 21/25 | Loss: 0.00118334
Iteration 22/25 | Loss: 0.00118334
Iteration 23/25 | Loss: 0.00118334
Iteration 24/25 | Loss: 0.00118334
Iteration 25/25 | Loss: 0.00118334

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.28713405
Iteration 2/25 | Loss: 0.00157014
Iteration 3/25 | Loss: 0.00130350
Iteration 4/25 | Loss: 0.00130350
Iteration 5/25 | Loss: 0.00130350
Iteration 6/25 | Loss: 0.00130350
Iteration 7/25 | Loss: 0.00130350
Iteration 8/25 | Loss: 0.00130350
Iteration 9/25 | Loss: 0.00130350
Iteration 10/25 | Loss: 0.00130350
Iteration 11/25 | Loss: 0.00130350
Iteration 12/25 | Loss: 0.00130350
Iteration 13/25 | Loss: 0.00130350
Iteration 14/25 | Loss: 0.00130350
Iteration 15/25 | Loss: 0.00130350
Iteration 16/25 | Loss: 0.00130350
Iteration 17/25 | Loss: 0.00130350
Iteration 18/25 | Loss: 0.00130350
Iteration 19/25 | Loss: 0.00130350
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.001303497701883316, 0.001303497701883316, 0.001303497701883316, 0.001303497701883316, 0.001303497701883316]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001303497701883316

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00130350
Iteration 2/1000 | Loss: 0.00024023
Iteration 3/1000 | Loss: 0.00058500
Iteration 4/1000 | Loss: 0.00029436
Iteration 5/1000 | Loss: 0.00005862
Iteration 6/1000 | Loss: 0.00003028
Iteration 7/1000 | Loss: 0.00007332
Iteration 8/1000 | Loss: 0.00004959
Iteration 9/1000 | Loss: 0.00001902
Iteration 10/1000 | Loss: 0.00016169
Iteration 11/1000 | Loss: 0.00021703
Iteration 12/1000 | Loss: 0.00006627
Iteration 13/1000 | Loss: 0.00001469
Iteration 14/1000 | Loss: 0.00005852
Iteration 15/1000 | Loss: 0.00002453
Iteration 16/1000 | Loss: 0.00027348
Iteration 17/1000 | Loss: 0.00002581
Iteration 18/1000 | Loss: 0.00003886
Iteration 19/1000 | Loss: 0.00001786
Iteration 20/1000 | Loss: 0.00003719
Iteration 21/1000 | Loss: 0.00006842
Iteration 22/1000 | Loss: 0.00002764
Iteration 23/1000 | Loss: 0.00001524
Iteration 24/1000 | Loss: 0.00001381
Iteration 25/1000 | Loss: 0.00001380
Iteration 26/1000 | Loss: 0.00002899
Iteration 27/1000 | Loss: 0.00001911
Iteration 28/1000 | Loss: 0.00004116
Iteration 29/1000 | Loss: 0.00001378
Iteration 30/1000 | Loss: 0.00001886
Iteration 31/1000 | Loss: 0.00003403
Iteration 32/1000 | Loss: 0.00001903
Iteration 33/1000 | Loss: 0.00001634
Iteration 34/1000 | Loss: 0.00001423
Iteration 35/1000 | Loss: 0.00002411
Iteration 36/1000 | Loss: 0.00001362
Iteration 37/1000 | Loss: 0.00001358
Iteration 38/1000 | Loss: 0.00001358
Iteration 39/1000 | Loss: 0.00001357
Iteration 40/1000 | Loss: 0.00001356
Iteration 41/1000 | Loss: 0.00001356
Iteration 42/1000 | Loss: 0.00002308
Iteration 43/1000 | Loss: 0.00001354
Iteration 44/1000 | Loss: 0.00001349
Iteration 45/1000 | Loss: 0.00001348
Iteration 46/1000 | Loss: 0.00001348
Iteration 47/1000 | Loss: 0.00001348
Iteration 48/1000 | Loss: 0.00001348
Iteration 49/1000 | Loss: 0.00001348
Iteration 50/1000 | Loss: 0.00001348
Iteration 51/1000 | Loss: 0.00001348
Iteration 52/1000 | Loss: 0.00001348
Iteration 53/1000 | Loss: 0.00001348
Iteration 54/1000 | Loss: 0.00001348
Iteration 55/1000 | Loss: 0.00001348
Iteration 56/1000 | Loss: 0.00001348
Iteration 57/1000 | Loss: 0.00001347
Iteration 58/1000 | Loss: 0.00001347
Iteration 59/1000 | Loss: 0.00001347
Iteration 60/1000 | Loss: 0.00001347
Iteration 61/1000 | Loss: 0.00001347
Iteration 62/1000 | Loss: 0.00001346
Iteration 63/1000 | Loss: 0.00001346
Iteration 64/1000 | Loss: 0.00001346
Iteration 65/1000 | Loss: 0.00001346
Iteration 66/1000 | Loss: 0.00001346
Iteration 67/1000 | Loss: 0.00001912
Iteration 68/1000 | Loss: 0.00001345
Iteration 69/1000 | Loss: 0.00002420
Iteration 70/1000 | Loss: 0.00001345
Iteration 71/1000 | Loss: 0.00001343
Iteration 72/1000 | Loss: 0.00001342
Iteration 73/1000 | Loss: 0.00001342
Iteration 74/1000 | Loss: 0.00001342
Iteration 75/1000 | Loss: 0.00001342
Iteration 76/1000 | Loss: 0.00001341
Iteration 77/1000 | Loss: 0.00001341
Iteration 78/1000 | Loss: 0.00001341
Iteration 79/1000 | Loss: 0.00001341
Iteration 80/1000 | Loss: 0.00001341
Iteration 81/1000 | Loss: 0.00001340
Iteration 82/1000 | Loss: 0.00001920
Iteration 83/1000 | Loss: 0.00001341
Iteration 84/1000 | Loss: 0.00001340
Iteration 85/1000 | Loss: 0.00001339
Iteration 86/1000 | Loss: 0.00001339
Iteration 87/1000 | Loss: 0.00001338
Iteration 88/1000 | Loss: 0.00001338
Iteration 89/1000 | Loss: 0.00001338
Iteration 90/1000 | Loss: 0.00001338
Iteration 91/1000 | Loss: 0.00001338
Iteration 92/1000 | Loss: 0.00001337
Iteration 93/1000 | Loss: 0.00001337
Iteration 94/1000 | Loss: 0.00001337
Iteration 95/1000 | Loss: 0.00001337
Iteration 96/1000 | Loss: 0.00001337
Iteration 97/1000 | Loss: 0.00001337
Iteration 98/1000 | Loss: 0.00001337
Iteration 99/1000 | Loss: 0.00001337
Iteration 100/1000 | Loss: 0.00001337
Iteration 101/1000 | Loss: 0.00001337
Iteration 102/1000 | Loss: 0.00001336
Iteration 103/1000 | Loss: 0.00001336
Iteration 104/1000 | Loss: 0.00001336
Iteration 105/1000 | Loss: 0.00001336
Iteration 106/1000 | Loss: 0.00001336
Iteration 107/1000 | Loss: 0.00001336
Iteration 108/1000 | Loss: 0.00001336
Iteration 109/1000 | Loss: 0.00001336
Iteration 110/1000 | Loss: 0.00001336
Iteration 111/1000 | Loss: 0.00001336
Iteration 112/1000 | Loss: 0.00001336
Iteration 113/1000 | Loss: 0.00001336
Iteration 114/1000 | Loss: 0.00001335
Iteration 115/1000 | Loss: 0.00001335
Iteration 116/1000 | Loss: 0.00001335
Iteration 117/1000 | Loss: 0.00001335
Iteration 118/1000 | Loss: 0.00001335
Iteration 119/1000 | Loss: 0.00001335
Iteration 120/1000 | Loss: 0.00001335
Iteration 121/1000 | Loss: 0.00001335
Iteration 122/1000 | Loss: 0.00001335
Iteration 123/1000 | Loss: 0.00001335
Iteration 124/1000 | Loss: 0.00001335
Iteration 125/1000 | Loss: 0.00003956
Iteration 126/1000 | Loss: 0.00005120
Iteration 127/1000 | Loss: 0.00001347
Iteration 128/1000 | Loss: 0.00004706
Iteration 129/1000 | Loss: 0.00002642
Iteration 130/1000 | Loss: 0.00003740
Iteration 131/1000 | Loss: 0.00004008
Iteration 132/1000 | Loss: 0.00001425
Iteration 133/1000 | Loss: 0.00004182
Iteration 134/1000 | Loss: 0.00001883
Iteration 135/1000 | Loss: 0.00001365
Iteration 136/1000 | Loss: 0.00001336
Iteration 137/1000 | Loss: 0.00001407
Iteration 138/1000 | Loss: 0.00002730
Iteration 139/1000 | Loss: 0.00002196
Iteration 140/1000 | Loss: 0.00002033
Iteration 141/1000 | Loss: 0.00001333
Iteration 142/1000 | Loss: 0.00003419
Iteration 143/1000 | Loss: 0.00001565
Iteration 144/1000 | Loss: 0.00001332
Iteration 145/1000 | Loss: 0.00001332
Iteration 146/1000 | Loss: 0.00001332
Iteration 147/1000 | Loss: 0.00001331
Iteration 148/1000 | Loss: 0.00001331
Iteration 149/1000 | Loss: 0.00001331
Iteration 150/1000 | Loss: 0.00001331
Iteration 151/1000 | Loss: 0.00001331
Iteration 152/1000 | Loss: 0.00001738
Iteration 153/1000 | Loss: 0.00001746
Iteration 154/1000 | Loss: 0.00001724
Iteration 155/1000 | Loss: 0.00001331
Iteration 156/1000 | Loss: 0.00001330
Iteration 157/1000 | Loss: 0.00001330
Iteration 158/1000 | Loss: 0.00001330
Iteration 159/1000 | Loss: 0.00001330
Iteration 160/1000 | Loss: 0.00001330
Iteration 161/1000 | Loss: 0.00001330
Iteration 162/1000 | Loss: 0.00001330
Iteration 163/1000 | Loss: 0.00001330
Iteration 164/1000 | Loss: 0.00001330
Iteration 165/1000 | Loss: 0.00001330
Iteration 166/1000 | Loss: 0.00001330
Iteration 167/1000 | Loss: 0.00001330
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 167. Stopping optimization.
Last 5 losses: [1.3304662388691213e-05, 1.3304662388691213e-05, 1.3304662388691213e-05, 1.3304662388691213e-05, 1.3304662388691213e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3304662388691213e-05

Optimization complete. Final v2v error: 3.08280348777771 mm

Highest mean error: 3.8438572883605957 mm for frame 80

Lowest mean error: 2.726858615875244 mm for frame 139

Saving results

Total time: 115.69195866584778
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_015/1096/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_015/1096.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_015/1096
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01084222
Iteration 2/25 | Loss: 0.01084222
Iteration 3/25 | Loss: 0.01084222
Iteration 4/25 | Loss: 0.01084222
Iteration 5/25 | Loss: 0.01084222
Iteration 6/25 | Loss: 0.01084222
Iteration 7/25 | Loss: 0.01084222
Iteration 8/25 | Loss: 0.01084222
Iteration 9/25 | Loss: 0.01084222
Iteration 10/25 | Loss: 0.01084222
Iteration 11/25 | Loss: 0.01084222
Iteration 12/25 | Loss: 0.01084222
Iteration 13/25 | Loss: 0.01084222
Iteration 14/25 | Loss: 0.01084222
Iteration 15/25 | Loss: 0.01084222
Iteration 16/25 | Loss: 0.01084222
Iteration 17/25 | Loss: 0.01084222
Iteration 18/25 | Loss: 0.01084221
Iteration 19/25 | Loss: 0.01084221
Iteration 20/25 | Loss: 0.01084221
Iteration 21/25 | Loss: 0.01084221
Iteration 22/25 | Loss: 0.01084221
Iteration 23/25 | Loss: 0.01084221
Iteration 24/25 | Loss: 0.01084221
Iteration 25/25 | Loss: 0.01084221

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 9.25656891
Iteration 2/25 | Loss: 0.18595403
Iteration 3/25 | Loss: 0.17778520
Iteration 4/25 | Loss: 0.17485592
Iteration 5/25 | Loss: 0.17485586
Iteration 6/25 | Loss: 0.17485584
Iteration 7/25 | Loss: 0.17485584
Iteration 8/25 | Loss: 0.17485584
Iteration 9/25 | Loss: 0.17485584
Iteration 10/25 | Loss: 0.17485584
Iteration 11/25 | Loss: 0.17485584
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.17485584318637848, 0.17485584318637848, 0.17485584318637848, 0.17485584318637848, 0.17485584318637848]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.17485584318637848

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.17485584
Iteration 2/1000 | Loss: 0.00510760
Iteration 3/1000 | Loss: 0.00057711
Iteration 4/1000 | Loss: 0.00069640
Iteration 5/1000 | Loss: 0.00036480
Iteration 6/1000 | Loss: 0.00011655
Iteration 7/1000 | Loss: 0.00010349
Iteration 8/1000 | Loss: 0.00002611
Iteration 9/1000 | Loss: 0.00014848
Iteration 10/1000 | Loss: 0.00006052
Iteration 11/1000 | Loss: 0.00011121
Iteration 12/1000 | Loss: 0.00002124
Iteration 13/1000 | Loss: 0.00004227
Iteration 14/1000 | Loss: 0.00034673
Iteration 15/1000 | Loss: 0.00005474
Iteration 16/1000 | Loss: 0.00007944
Iteration 17/1000 | Loss: 0.00010899
Iteration 18/1000 | Loss: 0.00015968
Iteration 19/1000 | Loss: 0.00002374
Iteration 20/1000 | Loss: 0.00002429
Iteration 21/1000 | Loss: 0.00001693
Iteration 22/1000 | Loss: 0.00033733
Iteration 23/1000 | Loss: 0.00030592
Iteration 24/1000 | Loss: 0.00066595
Iteration 25/1000 | Loss: 0.00002560
Iteration 26/1000 | Loss: 0.00005680
Iteration 27/1000 | Loss: 0.00001603
Iteration 28/1000 | Loss: 0.00001540
Iteration 29/1000 | Loss: 0.00001481
Iteration 30/1000 | Loss: 0.00001444
Iteration 31/1000 | Loss: 0.00004744
Iteration 32/1000 | Loss: 0.00001762
Iteration 33/1000 | Loss: 0.00001792
Iteration 34/1000 | Loss: 0.00001358
Iteration 35/1000 | Loss: 0.00001325
Iteration 36/1000 | Loss: 0.00002648
Iteration 37/1000 | Loss: 0.00001278
Iteration 38/1000 | Loss: 0.00001255
Iteration 39/1000 | Loss: 0.00011557
Iteration 40/1000 | Loss: 0.00002327
Iteration 41/1000 | Loss: 0.00001235
Iteration 42/1000 | Loss: 0.00003258
Iteration 43/1000 | Loss: 0.00001224
Iteration 44/1000 | Loss: 0.00002114
Iteration 45/1000 | Loss: 0.00001216
Iteration 46/1000 | Loss: 0.00002834
Iteration 47/1000 | Loss: 0.00001213
Iteration 48/1000 | Loss: 0.00001199
Iteration 49/1000 | Loss: 0.00001198
Iteration 50/1000 | Loss: 0.00001197
Iteration 51/1000 | Loss: 0.00001197
Iteration 52/1000 | Loss: 0.00001197
Iteration 53/1000 | Loss: 0.00001197
Iteration 54/1000 | Loss: 0.00001195
Iteration 55/1000 | Loss: 0.00004354
Iteration 56/1000 | Loss: 0.00001213
Iteration 57/1000 | Loss: 0.00001183
Iteration 58/1000 | Loss: 0.00001182
Iteration 59/1000 | Loss: 0.00001182
Iteration 60/1000 | Loss: 0.00001182
Iteration 61/1000 | Loss: 0.00001182
Iteration 62/1000 | Loss: 0.00001182
Iteration 63/1000 | Loss: 0.00001182
Iteration 64/1000 | Loss: 0.00001181
Iteration 65/1000 | Loss: 0.00001181
Iteration 66/1000 | Loss: 0.00001181
Iteration 67/1000 | Loss: 0.00001181
Iteration 68/1000 | Loss: 0.00001180
Iteration 69/1000 | Loss: 0.00001180
Iteration 70/1000 | Loss: 0.00001180
Iteration 71/1000 | Loss: 0.00001179
Iteration 72/1000 | Loss: 0.00001179
Iteration 73/1000 | Loss: 0.00001179
Iteration 74/1000 | Loss: 0.00001178
Iteration 75/1000 | Loss: 0.00001178
Iteration 76/1000 | Loss: 0.00001178
Iteration 77/1000 | Loss: 0.00001177
Iteration 78/1000 | Loss: 0.00001177
Iteration 79/1000 | Loss: 0.00001177
Iteration 80/1000 | Loss: 0.00001177
Iteration 81/1000 | Loss: 0.00001176
Iteration 82/1000 | Loss: 0.00001176
Iteration 83/1000 | Loss: 0.00004793
Iteration 84/1000 | Loss: 0.00005433
Iteration 85/1000 | Loss: 0.00001294
Iteration 86/1000 | Loss: 0.00003041
Iteration 87/1000 | Loss: 0.00002502
Iteration 88/1000 | Loss: 0.00001433
Iteration 89/1000 | Loss: 0.00001169
Iteration 90/1000 | Loss: 0.00001168
Iteration 91/1000 | Loss: 0.00001168
Iteration 92/1000 | Loss: 0.00001168
Iteration 93/1000 | Loss: 0.00001168
Iteration 94/1000 | Loss: 0.00001168
Iteration 95/1000 | Loss: 0.00001168
Iteration 96/1000 | Loss: 0.00004878
Iteration 97/1000 | Loss: 0.00001172
Iteration 98/1000 | Loss: 0.00001169
Iteration 99/1000 | Loss: 0.00001166
Iteration 100/1000 | Loss: 0.00001166
Iteration 101/1000 | Loss: 0.00001166
Iteration 102/1000 | Loss: 0.00001165
Iteration 103/1000 | Loss: 0.00001165
Iteration 104/1000 | Loss: 0.00001165
Iteration 105/1000 | Loss: 0.00001165
Iteration 106/1000 | Loss: 0.00001165
Iteration 107/1000 | Loss: 0.00001164
Iteration 108/1000 | Loss: 0.00001164
Iteration 109/1000 | Loss: 0.00001164
Iteration 110/1000 | Loss: 0.00001164
Iteration 111/1000 | Loss: 0.00001163
Iteration 112/1000 | Loss: 0.00001163
Iteration 113/1000 | Loss: 0.00001162
Iteration 114/1000 | Loss: 0.00001162
Iteration 115/1000 | Loss: 0.00001162
Iteration 116/1000 | Loss: 0.00001162
Iteration 117/1000 | Loss: 0.00001162
Iteration 118/1000 | Loss: 0.00001162
Iteration 119/1000 | Loss: 0.00001161
Iteration 120/1000 | Loss: 0.00001161
Iteration 121/1000 | Loss: 0.00001161
Iteration 122/1000 | Loss: 0.00001161
Iteration 123/1000 | Loss: 0.00001161
Iteration 124/1000 | Loss: 0.00001161
Iteration 125/1000 | Loss: 0.00001161
Iteration 126/1000 | Loss: 0.00001161
Iteration 127/1000 | Loss: 0.00001161
Iteration 128/1000 | Loss: 0.00001160
Iteration 129/1000 | Loss: 0.00001160
Iteration 130/1000 | Loss: 0.00001160
Iteration 131/1000 | Loss: 0.00001159
Iteration 132/1000 | Loss: 0.00001159
Iteration 133/1000 | Loss: 0.00001159
Iteration 134/1000 | Loss: 0.00001159
Iteration 135/1000 | Loss: 0.00001159
Iteration 136/1000 | Loss: 0.00001159
Iteration 137/1000 | Loss: 0.00001159
Iteration 138/1000 | Loss: 0.00001159
Iteration 139/1000 | Loss: 0.00001159
Iteration 140/1000 | Loss: 0.00001159
Iteration 141/1000 | Loss: 0.00001159
Iteration 142/1000 | Loss: 0.00001159
Iteration 143/1000 | Loss: 0.00001159
Iteration 144/1000 | Loss: 0.00001159
Iteration 145/1000 | Loss: 0.00001159
Iteration 146/1000 | Loss: 0.00001159
Iteration 147/1000 | Loss: 0.00001158
Iteration 148/1000 | Loss: 0.00001158
Iteration 149/1000 | Loss: 0.00001158
Iteration 150/1000 | Loss: 0.00001158
Iteration 151/1000 | Loss: 0.00001158
Iteration 152/1000 | Loss: 0.00001158
Iteration 153/1000 | Loss: 0.00001158
Iteration 154/1000 | Loss: 0.00001157
Iteration 155/1000 | Loss: 0.00001157
Iteration 156/1000 | Loss: 0.00001157
Iteration 157/1000 | Loss: 0.00001157
Iteration 158/1000 | Loss: 0.00001157
Iteration 159/1000 | Loss: 0.00001157
Iteration 160/1000 | Loss: 0.00001157
Iteration 161/1000 | Loss: 0.00001157
Iteration 162/1000 | Loss: 0.00001157
Iteration 163/1000 | Loss: 0.00001157
Iteration 164/1000 | Loss: 0.00001157
Iteration 165/1000 | Loss: 0.00001157
Iteration 166/1000 | Loss: 0.00001157
Iteration 167/1000 | Loss: 0.00001157
Iteration 168/1000 | Loss: 0.00001157
Iteration 169/1000 | Loss: 0.00001157
Iteration 170/1000 | Loss: 0.00001157
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 170. Stopping optimization.
Last 5 losses: [1.157020051323343e-05, 1.157020051323343e-05, 1.157020051323343e-05, 1.157020051323343e-05, 1.157020051323343e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.157020051323343e-05

Optimization complete. Final v2v error: 2.9068610668182373 mm

Highest mean error: 3.0832338333129883 mm for frame 198

Lowest mean error: 2.666745901107788 mm for frame 12

Saving results

Total time: 105.2372624874115
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_015/1010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_015/1010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_015/1010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00862283
Iteration 2/25 | Loss: 0.00125812
Iteration 3/25 | Loss: 0.00117920
Iteration 4/25 | Loss: 0.00117002
Iteration 5/25 | Loss: 0.00116731
Iteration 6/25 | Loss: 0.00116690
Iteration 7/25 | Loss: 0.00116690
Iteration 8/25 | Loss: 0.00116690
Iteration 9/25 | Loss: 0.00116690
Iteration 10/25 | Loss: 0.00116690
Iteration 11/25 | Loss: 0.00116690
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001166898524388671, 0.001166898524388671, 0.001166898524388671, 0.001166898524388671, 0.001166898524388671]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001166898524388671

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.66305423
Iteration 2/25 | Loss: 0.00137723
Iteration 3/25 | Loss: 0.00137723
Iteration 4/25 | Loss: 0.00137723
Iteration 5/25 | Loss: 0.00137723
Iteration 6/25 | Loss: 0.00137723
Iteration 7/25 | Loss: 0.00137723
Iteration 8/25 | Loss: 0.00137723
Iteration 9/25 | Loss: 0.00137723
Iteration 10/25 | Loss: 0.00137723
Iteration 11/25 | Loss: 0.00137723
Iteration 12/25 | Loss: 0.00137723
Iteration 13/25 | Loss: 0.00137723
Iteration 14/25 | Loss: 0.00137723
Iteration 15/25 | Loss: 0.00137723
Iteration 16/25 | Loss: 0.00137723
Iteration 17/25 | Loss: 0.00137723
Iteration 18/25 | Loss: 0.00137723
Iteration 19/25 | Loss: 0.00137723
Iteration 20/25 | Loss: 0.00137723
Iteration 21/25 | Loss: 0.00137723
Iteration 22/25 | Loss: 0.00137723
Iteration 23/25 | Loss: 0.00137723
Iteration 24/25 | Loss: 0.00137723
Iteration 25/25 | Loss: 0.00137723

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00137723
Iteration 2/1000 | Loss: 0.00002416
Iteration 3/1000 | Loss: 0.00001585
Iteration 4/1000 | Loss: 0.00001357
Iteration 5/1000 | Loss: 0.00001250
Iteration 6/1000 | Loss: 0.00001194
Iteration 7/1000 | Loss: 0.00001143
Iteration 8/1000 | Loss: 0.00001112
Iteration 9/1000 | Loss: 0.00001080
Iteration 10/1000 | Loss: 0.00001053
Iteration 11/1000 | Loss: 0.00001047
Iteration 12/1000 | Loss: 0.00001043
Iteration 13/1000 | Loss: 0.00001039
Iteration 14/1000 | Loss: 0.00001032
Iteration 15/1000 | Loss: 0.00001025
Iteration 16/1000 | Loss: 0.00001019
Iteration 17/1000 | Loss: 0.00001019
Iteration 18/1000 | Loss: 0.00001018
Iteration 19/1000 | Loss: 0.00001017
Iteration 20/1000 | Loss: 0.00001016
Iteration 21/1000 | Loss: 0.00001015
Iteration 22/1000 | Loss: 0.00001014
Iteration 23/1000 | Loss: 0.00001013
Iteration 24/1000 | Loss: 0.00001012
Iteration 25/1000 | Loss: 0.00001010
Iteration 26/1000 | Loss: 0.00001009
Iteration 27/1000 | Loss: 0.00001009
Iteration 28/1000 | Loss: 0.00001007
Iteration 29/1000 | Loss: 0.00001006
Iteration 30/1000 | Loss: 0.00001003
Iteration 31/1000 | Loss: 0.00000999
Iteration 32/1000 | Loss: 0.00000999
Iteration 33/1000 | Loss: 0.00000999
Iteration 34/1000 | Loss: 0.00000999
Iteration 35/1000 | Loss: 0.00000999
Iteration 36/1000 | Loss: 0.00000999
Iteration 37/1000 | Loss: 0.00000999
Iteration 38/1000 | Loss: 0.00000998
Iteration 39/1000 | Loss: 0.00000998
Iteration 40/1000 | Loss: 0.00000998
Iteration 41/1000 | Loss: 0.00000998
Iteration 42/1000 | Loss: 0.00000998
Iteration 43/1000 | Loss: 0.00000998
Iteration 44/1000 | Loss: 0.00000998
Iteration 45/1000 | Loss: 0.00000998
Iteration 46/1000 | Loss: 0.00000998
Iteration 47/1000 | Loss: 0.00000998
Iteration 48/1000 | Loss: 0.00000997
Iteration 49/1000 | Loss: 0.00000997
Iteration 50/1000 | Loss: 0.00000995
Iteration 51/1000 | Loss: 0.00000994
Iteration 52/1000 | Loss: 0.00000993
Iteration 53/1000 | Loss: 0.00000992
Iteration 54/1000 | Loss: 0.00000991
Iteration 55/1000 | Loss: 0.00000991
Iteration 56/1000 | Loss: 0.00000990
Iteration 57/1000 | Loss: 0.00000990
Iteration 58/1000 | Loss: 0.00000989
Iteration 59/1000 | Loss: 0.00000989
Iteration 60/1000 | Loss: 0.00000989
Iteration 61/1000 | Loss: 0.00000989
Iteration 62/1000 | Loss: 0.00000987
Iteration 63/1000 | Loss: 0.00000987
Iteration 64/1000 | Loss: 0.00000985
Iteration 65/1000 | Loss: 0.00000985
Iteration 66/1000 | Loss: 0.00000985
Iteration 67/1000 | Loss: 0.00000985
Iteration 68/1000 | Loss: 0.00000984
Iteration 69/1000 | Loss: 0.00000984
Iteration 70/1000 | Loss: 0.00000984
Iteration 71/1000 | Loss: 0.00000984
Iteration 72/1000 | Loss: 0.00000983
Iteration 73/1000 | Loss: 0.00000983
Iteration 74/1000 | Loss: 0.00000982
Iteration 75/1000 | Loss: 0.00000982
Iteration 76/1000 | Loss: 0.00000982
Iteration 77/1000 | Loss: 0.00000982
Iteration 78/1000 | Loss: 0.00000981
Iteration 79/1000 | Loss: 0.00000981
Iteration 80/1000 | Loss: 0.00000981
Iteration 81/1000 | Loss: 0.00000981
Iteration 82/1000 | Loss: 0.00000981
Iteration 83/1000 | Loss: 0.00000981
Iteration 84/1000 | Loss: 0.00000981
Iteration 85/1000 | Loss: 0.00000981
Iteration 86/1000 | Loss: 0.00000980
Iteration 87/1000 | Loss: 0.00000980
Iteration 88/1000 | Loss: 0.00000980
Iteration 89/1000 | Loss: 0.00000980
Iteration 90/1000 | Loss: 0.00000980
Iteration 91/1000 | Loss: 0.00000979
Iteration 92/1000 | Loss: 0.00000979
Iteration 93/1000 | Loss: 0.00000979
Iteration 94/1000 | Loss: 0.00000979
Iteration 95/1000 | Loss: 0.00000979
Iteration 96/1000 | Loss: 0.00000979
Iteration 97/1000 | Loss: 0.00000979
Iteration 98/1000 | Loss: 0.00000978
Iteration 99/1000 | Loss: 0.00000978
Iteration 100/1000 | Loss: 0.00000978
Iteration 101/1000 | Loss: 0.00000978
Iteration 102/1000 | Loss: 0.00000978
Iteration 103/1000 | Loss: 0.00000978
Iteration 104/1000 | Loss: 0.00000978
Iteration 105/1000 | Loss: 0.00000978
Iteration 106/1000 | Loss: 0.00000978
Iteration 107/1000 | Loss: 0.00000978
Iteration 108/1000 | Loss: 0.00000977
Iteration 109/1000 | Loss: 0.00000977
Iteration 110/1000 | Loss: 0.00000977
Iteration 111/1000 | Loss: 0.00000977
Iteration 112/1000 | Loss: 0.00000977
Iteration 113/1000 | Loss: 0.00000977
Iteration 114/1000 | Loss: 0.00000977
Iteration 115/1000 | Loss: 0.00000977
Iteration 116/1000 | Loss: 0.00000976
Iteration 117/1000 | Loss: 0.00000976
Iteration 118/1000 | Loss: 0.00000976
Iteration 119/1000 | Loss: 0.00000976
Iteration 120/1000 | Loss: 0.00000976
Iteration 121/1000 | Loss: 0.00000976
Iteration 122/1000 | Loss: 0.00000976
Iteration 123/1000 | Loss: 0.00000976
Iteration 124/1000 | Loss: 0.00000976
Iteration 125/1000 | Loss: 0.00000976
Iteration 126/1000 | Loss: 0.00000975
Iteration 127/1000 | Loss: 0.00000975
Iteration 128/1000 | Loss: 0.00000975
Iteration 129/1000 | Loss: 0.00000975
Iteration 130/1000 | Loss: 0.00000974
Iteration 131/1000 | Loss: 0.00000974
Iteration 132/1000 | Loss: 0.00000974
Iteration 133/1000 | Loss: 0.00000974
Iteration 134/1000 | Loss: 0.00000974
Iteration 135/1000 | Loss: 0.00000974
Iteration 136/1000 | Loss: 0.00000974
Iteration 137/1000 | Loss: 0.00000974
Iteration 138/1000 | Loss: 0.00000974
Iteration 139/1000 | Loss: 0.00000974
Iteration 140/1000 | Loss: 0.00000974
Iteration 141/1000 | Loss: 0.00000973
Iteration 142/1000 | Loss: 0.00000973
Iteration 143/1000 | Loss: 0.00000973
Iteration 144/1000 | Loss: 0.00000973
Iteration 145/1000 | Loss: 0.00000973
Iteration 146/1000 | Loss: 0.00000973
Iteration 147/1000 | Loss: 0.00000973
Iteration 148/1000 | Loss: 0.00000973
Iteration 149/1000 | Loss: 0.00000972
Iteration 150/1000 | Loss: 0.00000972
Iteration 151/1000 | Loss: 0.00000972
Iteration 152/1000 | Loss: 0.00000972
Iteration 153/1000 | Loss: 0.00000972
Iteration 154/1000 | Loss: 0.00000972
Iteration 155/1000 | Loss: 0.00000972
Iteration 156/1000 | Loss: 0.00000972
Iteration 157/1000 | Loss: 0.00000972
Iteration 158/1000 | Loss: 0.00000972
Iteration 159/1000 | Loss: 0.00000972
Iteration 160/1000 | Loss: 0.00000971
Iteration 161/1000 | Loss: 0.00000971
Iteration 162/1000 | Loss: 0.00000971
Iteration 163/1000 | Loss: 0.00000971
Iteration 164/1000 | Loss: 0.00000971
Iteration 165/1000 | Loss: 0.00000971
Iteration 166/1000 | Loss: 0.00000971
Iteration 167/1000 | Loss: 0.00000971
Iteration 168/1000 | Loss: 0.00000971
Iteration 169/1000 | Loss: 0.00000971
Iteration 170/1000 | Loss: 0.00000971
Iteration 171/1000 | Loss: 0.00000971
Iteration 172/1000 | Loss: 0.00000971
Iteration 173/1000 | Loss: 0.00000971
Iteration 174/1000 | Loss: 0.00000971
Iteration 175/1000 | Loss: 0.00000971
Iteration 176/1000 | Loss: 0.00000971
Iteration 177/1000 | Loss: 0.00000970
Iteration 178/1000 | Loss: 0.00000970
Iteration 179/1000 | Loss: 0.00000970
Iteration 180/1000 | Loss: 0.00000970
Iteration 181/1000 | Loss: 0.00000970
Iteration 182/1000 | Loss: 0.00000970
Iteration 183/1000 | Loss: 0.00000970
Iteration 184/1000 | Loss: 0.00000970
Iteration 185/1000 | Loss: 0.00000970
Iteration 186/1000 | Loss: 0.00000970
Iteration 187/1000 | Loss: 0.00000970
Iteration 188/1000 | Loss: 0.00000970
Iteration 189/1000 | Loss: 0.00000970
Iteration 190/1000 | Loss: 0.00000970
Iteration 191/1000 | Loss: 0.00000970
Iteration 192/1000 | Loss: 0.00000970
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 192. Stopping optimization.
Last 5 losses: [9.697542736830655e-06, 9.697542736830655e-06, 9.697542736830655e-06, 9.697542736830655e-06, 9.697542736830655e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.697542736830655e-06

Optimization complete. Final v2v error: 2.6452298164367676 mm

Highest mean error: 3.555732250213623 mm for frame 94

Lowest mean error: 2.433157444000244 mm for frame 3

Saving results

Total time: 40.648531675338745
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_015/1097/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_015/1097.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_015/1097
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00917092
Iteration 2/25 | Loss: 0.00152558
Iteration 3/25 | Loss: 0.00134281
Iteration 4/25 | Loss: 0.00133208
Iteration 5/25 | Loss: 0.00133089
Iteration 6/25 | Loss: 0.00133089
Iteration 7/25 | Loss: 0.00133089
Iteration 8/25 | Loss: 0.00133089
Iteration 9/25 | Loss: 0.00133089
Iteration 10/25 | Loss: 0.00133089
Iteration 11/25 | Loss: 0.00133089
Iteration 12/25 | Loss: 0.00133089
Iteration 13/25 | Loss: 0.00133089
Iteration 14/25 | Loss: 0.00133089
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.001330885337665677, 0.001330885337665677, 0.001330885337665677, 0.001330885337665677, 0.001330885337665677]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001330885337665677

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.59245217
Iteration 2/25 | Loss: 0.00111004
Iteration 3/25 | Loss: 0.00111003
Iteration 4/25 | Loss: 0.00111003
Iteration 5/25 | Loss: 0.00111003
Iteration 6/25 | Loss: 0.00111003
Iteration 7/25 | Loss: 0.00111003
Iteration 8/25 | Loss: 0.00111003
Iteration 9/25 | Loss: 0.00111003
Iteration 10/25 | Loss: 0.00111003
Iteration 11/25 | Loss: 0.00111003
Iteration 12/25 | Loss: 0.00111003
Iteration 13/25 | Loss: 0.00111003
Iteration 14/25 | Loss: 0.00111003
Iteration 15/25 | Loss: 0.00111003
Iteration 16/25 | Loss: 0.00111003
Iteration 17/25 | Loss: 0.00111003
Iteration 18/25 | Loss: 0.00111003
Iteration 19/25 | Loss: 0.00111003
Iteration 20/25 | Loss: 0.00111003
Iteration 21/25 | Loss: 0.00111003
Iteration 22/25 | Loss: 0.00111003
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0011100260308012366, 0.0011100260308012366, 0.0011100260308012366, 0.0011100260308012366, 0.0011100260308012366]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011100260308012366

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00111003
Iteration 2/1000 | Loss: 0.00004305
Iteration 3/1000 | Loss: 0.00002820
Iteration 4/1000 | Loss: 0.00002491
Iteration 5/1000 | Loss: 0.00002293
Iteration 6/1000 | Loss: 0.00002207
Iteration 7/1000 | Loss: 0.00002159
Iteration 8/1000 | Loss: 0.00002126
Iteration 9/1000 | Loss: 0.00002088
Iteration 10/1000 | Loss: 0.00002060
Iteration 11/1000 | Loss: 0.00002035
Iteration 12/1000 | Loss: 0.00002015
Iteration 13/1000 | Loss: 0.00002005
Iteration 14/1000 | Loss: 0.00001996
Iteration 15/1000 | Loss: 0.00001983
Iteration 16/1000 | Loss: 0.00001976
Iteration 17/1000 | Loss: 0.00001972
Iteration 18/1000 | Loss: 0.00001970
Iteration 19/1000 | Loss: 0.00001970
Iteration 20/1000 | Loss: 0.00001969
Iteration 21/1000 | Loss: 0.00001963
Iteration 22/1000 | Loss: 0.00001956
Iteration 23/1000 | Loss: 0.00001952
Iteration 24/1000 | Loss: 0.00001951
Iteration 25/1000 | Loss: 0.00001949
Iteration 26/1000 | Loss: 0.00001948
Iteration 27/1000 | Loss: 0.00001946
Iteration 28/1000 | Loss: 0.00001944
Iteration 29/1000 | Loss: 0.00001944
Iteration 30/1000 | Loss: 0.00001943
Iteration 31/1000 | Loss: 0.00001943
Iteration 32/1000 | Loss: 0.00001940
Iteration 33/1000 | Loss: 0.00001940
Iteration 34/1000 | Loss: 0.00001940
Iteration 35/1000 | Loss: 0.00001940
Iteration 36/1000 | Loss: 0.00001940
Iteration 37/1000 | Loss: 0.00001940
Iteration 38/1000 | Loss: 0.00001940
Iteration 39/1000 | Loss: 0.00001940
Iteration 40/1000 | Loss: 0.00001940
Iteration 41/1000 | Loss: 0.00001940
Iteration 42/1000 | Loss: 0.00001939
Iteration 43/1000 | Loss: 0.00001939
Iteration 44/1000 | Loss: 0.00001939
Iteration 45/1000 | Loss: 0.00001937
Iteration 46/1000 | Loss: 0.00001937
Iteration 47/1000 | Loss: 0.00001937
Iteration 48/1000 | Loss: 0.00001937
Iteration 49/1000 | Loss: 0.00001937
Iteration 50/1000 | Loss: 0.00001937
Iteration 51/1000 | Loss: 0.00001937
Iteration 52/1000 | Loss: 0.00001937
Iteration 53/1000 | Loss: 0.00001936
Iteration 54/1000 | Loss: 0.00001936
Iteration 55/1000 | Loss: 0.00001935
Iteration 56/1000 | Loss: 0.00001935
Iteration 57/1000 | Loss: 0.00001935
Iteration 58/1000 | Loss: 0.00001935
Iteration 59/1000 | Loss: 0.00001935
Iteration 60/1000 | Loss: 0.00001935
Iteration 61/1000 | Loss: 0.00001934
Iteration 62/1000 | Loss: 0.00001934
Iteration 63/1000 | Loss: 0.00001934
Iteration 64/1000 | Loss: 0.00001934
Iteration 65/1000 | Loss: 0.00001934
Iteration 66/1000 | Loss: 0.00001934
Iteration 67/1000 | Loss: 0.00001933
Iteration 68/1000 | Loss: 0.00001932
Iteration 69/1000 | Loss: 0.00001932
Iteration 70/1000 | Loss: 0.00001932
Iteration 71/1000 | Loss: 0.00001932
Iteration 72/1000 | Loss: 0.00001932
Iteration 73/1000 | Loss: 0.00001932
Iteration 74/1000 | Loss: 0.00001932
Iteration 75/1000 | Loss: 0.00001931
Iteration 76/1000 | Loss: 0.00001931
Iteration 77/1000 | Loss: 0.00001931
Iteration 78/1000 | Loss: 0.00001931
Iteration 79/1000 | Loss: 0.00001931
Iteration 80/1000 | Loss: 0.00001931
Iteration 81/1000 | Loss: 0.00001931
Iteration 82/1000 | Loss: 0.00001931
Iteration 83/1000 | Loss: 0.00001931
Iteration 84/1000 | Loss: 0.00001931
Iteration 85/1000 | Loss: 0.00001931
Iteration 86/1000 | Loss: 0.00001930
Iteration 87/1000 | Loss: 0.00001930
Iteration 88/1000 | Loss: 0.00001930
Iteration 89/1000 | Loss: 0.00001930
Iteration 90/1000 | Loss: 0.00001930
Iteration 91/1000 | Loss: 0.00001930
Iteration 92/1000 | Loss: 0.00001929
Iteration 93/1000 | Loss: 0.00001929
Iteration 94/1000 | Loss: 0.00001929
Iteration 95/1000 | Loss: 0.00001929
Iteration 96/1000 | Loss: 0.00001929
Iteration 97/1000 | Loss: 0.00001929
Iteration 98/1000 | Loss: 0.00001929
Iteration 99/1000 | Loss: 0.00001928
Iteration 100/1000 | Loss: 0.00001928
Iteration 101/1000 | Loss: 0.00001927
Iteration 102/1000 | Loss: 0.00001927
Iteration 103/1000 | Loss: 0.00001927
Iteration 104/1000 | Loss: 0.00001927
Iteration 105/1000 | Loss: 0.00001927
Iteration 106/1000 | Loss: 0.00001926
Iteration 107/1000 | Loss: 0.00001926
Iteration 108/1000 | Loss: 0.00001926
Iteration 109/1000 | Loss: 0.00001926
Iteration 110/1000 | Loss: 0.00001925
Iteration 111/1000 | Loss: 0.00001925
Iteration 112/1000 | Loss: 0.00001925
Iteration 113/1000 | Loss: 0.00001925
Iteration 114/1000 | Loss: 0.00001925
Iteration 115/1000 | Loss: 0.00001925
Iteration 116/1000 | Loss: 0.00001925
Iteration 117/1000 | Loss: 0.00001924
Iteration 118/1000 | Loss: 0.00001924
Iteration 119/1000 | Loss: 0.00001924
Iteration 120/1000 | Loss: 0.00001924
Iteration 121/1000 | Loss: 0.00001924
Iteration 122/1000 | Loss: 0.00001924
Iteration 123/1000 | Loss: 0.00001924
Iteration 124/1000 | Loss: 0.00001923
Iteration 125/1000 | Loss: 0.00001923
Iteration 126/1000 | Loss: 0.00001923
Iteration 127/1000 | Loss: 0.00001923
Iteration 128/1000 | Loss: 0.00001923
Iteration 129/1000 | Loss: 0.00001923
Iteration 130/1000 | Loss: 0.00001923
Iteration 131/1000 | Loss: 0.00001923
Iteration 132/1000 | Loss: 0.00001923
Iteration 133/1000 | Loss: 0.00001923
Iteration 134/1000 | Loss: 0.00001923
Iteration 135/1000 | Loss: 0.00001923
Iteration 136/1000 | Loss: 0.00001923
Iteration 137/1000 | Loss: 0.00001923
Iteration 138/1000 | Loss: 0.00001923
Iteration 139/1000 | Loss: 0.00001923
Iteration 140/1000 | Loss: 0.00001923
Iteration 141/1000 | Loss: 0.00001923
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 141. Stopping optimization.
Last 5 losses: [1.9229382814955898e-05, 1.9229382814955898e-05, 1.9229382814955898e-05, 1.9229382814955898e-05, 1.9229382814955898e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9229382814955898e-05

Optimization complete. Final v2v error: 3.6092007160186768 mm

Highest mean error: 5.038614749908447 mm for frame 5

Lowest mean error: 2.9010684490203857 mm for frame 232

Saving results

Total time: 46.284897565841675
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_015/1049/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_015/1049.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_015/1049
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00825988
Iteration 2/25 | Loss: 0.00128521
Iteration 3/25 | Loss: 0.00118076
Iteration 4/25 | Loss: 0.00117072
Iteration 5/25 | Loss: 0.00116866
Iteration 6/25 | Loss: 0.00116845
Iteration 7/25 | Loss: 0.00116845
Iteration 8/25 | Loss: 0.00116845
Iteration 9/25 | Loss: 0.00116845
Iteration 10/25 | Loss: 0.00116845
Iteration 11/25 | Loss: 0.00116845
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011684498749673367, 0.0011684498749673367, 0.0011684498749673367, 0.0011684498749673367, 0.0011684498749673367]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011684498749673367

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.28539741
Iteration 2/25 | Loss: 0.00117918
Iteration 3/25 | Loss: 0.00117916
Iteration 4/25 | Loss: 0.00117916
Iteration 5/25 | Loss: 0.00117916
Iteration 6/25 | Loss: 0.00117916
Iteration 7/25 | Loss: 0.00117916
Iteration 8/25 | Loss: 0.00117916
Iteration 9/25 | Loss: 0.00117916
Iteration 10/25 | Loss: 0.00117916
Iteration 11/25 | Loss: 0.00117916
Iteration 12/25 | Loss: 0.00117916
Iteration 13/25 | Loss: 0.00117916
Iteration 14/25 | Loss: 0.00117916
Iteration 15/25 | Loss: 0.00117916
Iteration 16/25 | Loss: 0.00117916
Iteration 17/25 | Loss: 0.00117916
Iteration 18/25 | Loss: 0.00117916
Iteration 19/25 | Loss: 0.00117916
Iteration 20/25 | Loss: 0.00117916
Iteration 21/25 | Loss: 0.00117916
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0011791581055149436, 0.0011791581055149436, 0.0011791581055149436, 0.0011791581055149436, 0.0011791581055149436]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011791581055149436

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00117916
Iteration 2/1000 | Loss: 0.00001864
Iteration 3/1000 | Loss: 0.00001313
Iteration 4/1000 | Loss: 0.00001189
Iteration 5/1000 | Loss: 0.00001099
Iteration 6/1000 | Loss: 0.00001016
Iteration 7/1000 | Loss: 0.00000967
Iteration 8/1000 | Loss: 0.00000938
Iteration 9/1000 | Loss: 0.00000919
Iteration 10/1000 | Loss: 0.00000898
Iteration 11/1000 | Loss: 0.00000893
Iteration 12/1000 | Loss: 0.00000890
Iteration 13/1000 | Loss: 0.00000890
Iteration 14/1000 | Loss: 0.00000888
Iteration 15/1000 | Loss: 0.00000882
Iteration 16/1000 | Loss: 0.00000878
Iteration 17/1000 | Loss: 0.00000876
Iteration 18/1000 | Loss: 0.00000875
Iteration 19/1000 | Loss: 0.00000875
Iteration 20/1000 | Loss: 0.00000874
Iteration 21/1000 | Loss: 0.00000872
Iteration 22/1000 | Loss: 0.00000872
Iteration 23/1000 | Loss: 0.00000870
Iteration 24/1000 | Loss: 0.00000869
Iteration 25/1000 | Loss: 0.00000868
Iteration 26/1000 | Loss: 0.00000867
Iteration 27/1000 | Loss: 0.00000863
Iteration 28/1000 | Loss: 0.00000862
Iteration 29/1000 | Loss: 0.00000862
Iteration 30/1000 | Loss: 0.00000861
Iteration 31/1000 | Loss: 0.00000861
Iteration 32/1000 | Loss: 0.00000861
Iteration 33/1000 | Loss: 0.00000860
Iteration 34/1000 | Loss: 0.00000859
Iteration 35/1000 | Loss: 0.00000856
Iteration 36/1000 | Loss: 0.00000855
Iteration 37/1000 | Loss: 0.00000855
Iteration 38/1000 | Loss: 0.00000855
Iteration 39/1000 | Loss: 0.00000854
Iteration 40/1000 | Loss: 0.00000854
Iteration 41/1000 | Loss: 0.00000853
Iteration 42/1000 | Loss: 0.00000853
Iteration 43/1000 | Loss: 0.00000852
Iteration 44/1000 | Loss: 0.00000852
Iteration 45/1000 | Loss: 0.00000851
Iteration 46/1000 | Loss: 0.00000851
Iteration 47/1000 | Loss: 0.00000851
Iteration 48/1000 | Loss: 0.00000851
Iteration 49/1000 | Loss: 0.00000850
Iteration 50/1000 | Loss: 0.00000850
Iteration 51/1000 | Loss: 0.00000850
Iteration 52/1000 | Loss: 0.00000850
Iteration 53/1000 | Loss: 0.00000850
Iteration 54/1000 | Loss: 0.00000850
Iteration 55/1000 | Loss: 0.00000850
Iteration 56/1000 | Loss: 0.00000849
Iteration 57/1000 | Loss: 0.00000849
Iteration 58/1000 | Loss: 0.00000849
Iteration 59/1000 | Loss: 0.00000848
Iteration 60/1000 | Loss: 0.00000848
Iteration 61/1000 | Loss: 0.00000848
Iteration 62/1000 | Loss: 0.00000848
Iteration 63/1000 | Loss: 0.00000847
Iteration 64/1000 | Loss: 0.00000847
Iteration 65/1000 | Loss: 0.00000847
Iteration 66/1000 | Loss: 0.00000847
Iteration 67/1000 | Loss: 0.00000847
Iteration 68/1000 | Loss: 0.00000847
Iteration 69/1000 | Loss: 0.00000847
Iteration 70/1000 | Loss: 0.00000847
Iteration 71/1000 | Loss: 0.00000846
Iteration 72/1000 | Loss: 0.00000846
Iteration 73/1000 | Loss: 0.00000846
Iteration 74/1000 | Loss: 0.00000846
Iteration 75/1000 | Loss: 0.00000845
Iteration 76/1000 | Loss: 0.00000845
Iteration 77/1000 | Loss: 0.00000844
Iteration 78/1000 | Loss: 0.00000844
Iteration 79/1000 | Loss: 0.00000844
Iteration 80/1000 | Loss: 0.00000844
Iteration 81/1000 | Loss: 0.00000843
Iteration 82/1000 | Loss: 0.00000843
Iteration 83/1000 | Loss: 0.00000843
Iteration 84/1000 | Loss: 0.00000843
Iteration 85/1000 | Loss: 0.00000843
Iteration 86/1000 | Loss: 0.00000843
Iteration 87/1000 | Loss: 0.00000842
Iteration 88/1000 | Loss: 0.00000842
Iteration 89/1000 | Loss: 0.00000842
Iteration 90/1000 | Loss: 0.00000841
Iteration 91/1000 | Loss: 0.00000841
Iteration 92/1000 | Loss: 0.00000841
Iteration 93/1000 | Loss: 0.00000841
Iteration 94/1000 | Loss: 0.00000841
Iteration 95/1000 | Loss: 0.00000840
Iteration 96/1000 | Loss: 0.00000840
Iteration 97/1000 | Loss: 0.00000840
Iteration 98/1000 | Loss: 0.00000839
Iteration 99/1000 | Loss: 0.00000839
Iteration 100/1000 | Loss: 0.00000839
Iteration 101/1000 | Loss: 0.00000838
Iteration 102/1000 | Loss: 0.00000838
Iteration 103/1000 | Loss: 0.00000837
Iteration 104/1000 | Loss: 0.00000837
Iteration 105/1000 | Loss: 0.00000837
Iteration 106/1000 | Loss: 0.00000836
Iteration 107/1000 | Loss: 0.00000836
Iteration 108/1000 | Loss: 0.00000836
Iteration 109/1000 | Loss: 0.00000836
Iteration 110/1000 | Loss: 0.00000836
Iteration 111/1000 | Loss: 0.00000835
Iteration 112/1000 | Loss: 0.00000835
Iteration 113/1000 | Loss: 0.00000835
Iteration 114/1000 | Loss: 0.00000835
Iteration 115/1000 | Loss: 0.00000835
Iteration 116/1000 | Loss: 0.00000834
Iteration 117/1000 | Loss: 0.00000834
Iteration 118/1000 | Loss: 0.00000834
Iteration 119/1000 | Loss: 0.00000834
Iteration 120/1000 | Loss: 0.00000834
Iteration 121/1000 | Loss: 0.00000834
Iteration 122/1000 | Loss: 0.00000834
Iteration 123/1000 | Loss: 0.00000834
Iteration 124/1000 | Loss: 0.00000834
Iteration 125/1000 | Loss: 0.00000834
Iteration 126/1000 | Loss: 0.00000834
Iteration 127/1000 | Loss: 0.00000834
Iteration 128/1000 | Loss: 0.00000834
Iteration 129/1000 | Loss: 0.00000834
Iteration 130/1000 | Loss: 0.00000833
Iteration 131/1000 | Loss: 0.00000833
Iteration 132/1000 | Loss: 0.00000833
Iteration 133/1000 | Loss: 0.00000833
Iteration 134/1000 | Loss: 0.00000833
Iteration 135/1000 | Loss: 0.00000832
Iteration 136/1000 | Loss: 0.00000832
Iteration 137/1000 | Loss: 0.00000831
Iteration 138/1000 | Loss: 0.00000831
Iteration 139/1000 | Loss: 0.00000831
Iteration 140/1000 | Loss: 0.00000831
Iteration 141/1000 | Loss: 0.00000831
Iteration 142/1000 | Loss: 0.00000831
Iteration 143/1000 | Loss: 0.00000830
Iteration 144/1000 | Loss: 0.00000830
Iteration 145/1000 | Loss: 0.00000830
Iteration 146/1000 | Loss: 0.00000830
Iteration 147/1000 | Loss: 0.00000830
Iteration 148/1000 | Loss: 0.00000830
Iteration 149/1000 | Loss: 0.00000830
Iteration 150/1000 | Loss: 0.00000830
Iteration 151/1000 | Loss: 0.00000830
Iteration 152/1000 | Loss: 0.00000829
Iteration 153/1000 | Loss: 0.00000829
Iteration 154/1000 | Loss: 0.00000829
Iteration 155/1000 | Loss: 0.00000829
Iteration 156/1000 | Loss: 0.00000829
Iteration 157/1000 | Loss: 0.00000829
Iteration 158/1000 | Loss: 0.00000829
Iteration 159/1000 | Loss: 0.00000829
Iteration 160/1000 | Loss: 0.00000828
Iteration 161/1000 | Loss: 0.00000828
Iteration 162/1000 | Loss: 0.00000828
Iteration 163/1000 | Loss: 0.00000828
Iteration 164/1000 | Loss: 0.00000828
Iteration 165/1000 | Loss: 0.00000828
Iteration 166/1000 | Loss: 0.00000828
Iteration 167/1000 | Loss: 0.00000828
Iteration 168/1000 | Loss: 0.00000828
Iteration 169/1000 | Loss: 0.00000828
Iteration 170/1000 | Loss: 0.00000828
Iteration 171/1000 | Loss: 0.00000828
Iteration 172/1000 | Loss: 0.00000828
Iteration 173/1000 | Loss: 0.00000828
Iteration 174/1000 | Loss: 0.00000827
Iteration 175/1000 | Loss: 0.00000827
Iteration 176/1000 | Loss: 0.00000827
Iteration 177/1000 | Loss: 0.00000827
Iteration 178/1000 | Loss: 0.00000827
Iteration 179/1000 | Loss: 0.00000827
Iteration 180/1000 | Loss: 0.00000827
Iteration 181/1000 | Loss: 0.00000827
Iteration 182/1000 | Loss: 0.00000827
Iteration 183/1000 | Loss: 0.00000827
Iteration 184/1000 | Loss: 0.00000827
Iteration 185/1000 | Loss: 0.00000827
Iteration 186/1000 | Loss: 0.00000826
Iteration 187/1000 | Loss: 0.00000826
Iteration 188/1000 | Loss: 0.00000826
Iteration 189/1000 | Loss: 0.00000826
Iteration 190/1000 | Loss: 0.00000826
Iteration 191/1000 | Loss: 0.00000826
Iteration 192/1000 | Loss: 0.00000826
Iteration 193/1000 | Loss: 0.00000826
Iteration 194/1000 | Loss: 0.00000826
Iteration 195/1000 | Loss: 0.00000826
Iteration 196/1000 | Loss: 0.00000826
Iteration 197/1000 | Loss: 0.00000825
Iteration 198/1000 | Loss: 0.00000825
Iteration 199/1000 | Loss: 0.00000825
Iteration 200/1000 | Loss: 0.00000825
Iteration 201/1000 | Loss: 0.00000825
Iteration 202/1000 | Loss: 0.00000825
Iteration 203/1000 | Loss: 0.00000825
Iteration 204/1000 | Loss: 0.00000825
Iteration 205/1000 | Loss: 0.00000825
Iteration 206/1000 | Loss: 0.00000825
Iteration 207/1000 | Loss: 0.00000825
Iteration 208/1000 | Loss: 0.00000824
Iteration 209/1000 | Loss: 0.00000824
Iteration 210/1000 | Loss: 0.00000824
Iteration 211/1000 | Loss: 0.00000824
Iteration 212/1000 | Loss: 0.00000824
Iteration 213/1000 | Loss: 0.00000824
Iteration 214/1000 | Loss: 0.00000824
Iteration 215/1000 | Loss: 0.00000824
Iteration 216/1000 | Loss: 0.00000824
Iteration 217/1000 | Loss: 0.00000824
Iteration 218/1000 | Loss: 0.00000824
Iteration 219/1000 | Loss: 0.00000824
Iteration 220/1000 | Loss: 0.00000823
Iteration 221/1000 | Loss: 0.00000823
Iteration 222/1000 | Loss: 0.00000823
Iteration 223/1000 | Loss: 0.00000823
Iteration 224/1000 | Loss: 0.00000823
Iteration 225/1000 | Loss: 0.00000823
Iteration 226/1000 | Loss: 0.00000823
Iteration 227/1000 | Loss: 0.00000823
Iteration 228/1000 | Loss: 0.00000823
Iteration 229/1000 | Loss: 0.00000823
Iteration 230/1000 | Loss: 0.00000823
Iteration 231/1000 | Loss: 0.00000823
Iteration 232/1000 | Loss: 0.00000823
Iteration 233/1000 | Loss: 0.00000823
Iteration 234/1000 | Loss: 0.00000823
Iteration 235/1000 | Loss: 0.00000823
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 235. Stopping optimization.
Last 5 losses: [8.233319022110663e-06, 8.233319022110663e-06, 8.233319022110663e-06, 8.233319022110663e-06, 8.233319022110663e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.233319022110663e-06

Optimization complete. Final v2v error: 2.4895360469818115 mm

Highest mean error: 2.7423136234283447 mm for frame 31

Lowest mean error: 2.3633339405059814 mm for frame 113

Saving results

Total time: 40.04441213607788
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_015/1039/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_015/1039.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_015/1039
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00790429
Iteration 2/25 | Loss: 0.00125424
Iteration 3/25 | Loss: 0.00118105
Iteration 4/25 | Loss: 0.00116943
Iteration 5/25 | Loss: 0.00116659
Iteration 6/25 | Loss: 0.00116659
Iteration 7/25 | Loss: 0.00116659
Iteration 8/25 | Loss: 0.00116659
Iteration 9/25 | Loss: 0.00116659
Iteration 10/25 | Loss: 0.00116659
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0011665853671729565, 0.0011665853671729565, 0.0011665853671729565, 0.0011665853671729565, 0.0011665853671729565]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011665853671729565

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33124447
Iteration 2/25 | Loss: 0.00131649
Iteration 3/25 | Loss: 0.00131649
Iteration 4/25 | Loss: 0.00131649
Iteration 5/25 | Loss: 0.00131649
Iteration 6/25 | Loss: 0.00131649
Iteration 7/25 | Loss: 0.00131649
Iteration 8/25 | Loss: 0.00131649
Iteration 9/25 | Loss: 0.00131649
Iteration 10/25 | Loss: 0.00131649
Iteration 11/25 | Loss: 0.00131649
Iteration 12/25 | Loss: 0.00131649
Iteration 13/25 | Loss: 0.00131649
Iteration 14/25 | Loss: 0.00131649
Iteration 15/25 | Loss: 0.00131649
Iteration 16/25 | Loss: 0.00131649
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0013164883712306619, 0.0013164883712306619, 0.0013164883712306619, 0.0013164883712306619, 0.0013164883712306619]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013164883712306619

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00131649
Iteration 2/1000 | Loss: 0.00001952
Iteration 3/1000 | Loss: 0.00001501
Iteration 4/1000 | Loss: 0.00001390
Iteration 5/1000 | Loss: 0.00001320
Iteration 6/1000 | Loss: 0.00001280
Iteration 7/1000 | Loss: 0.00001239
Iteration 8/1000 | Loss: 0.00001212
Iteration 9/1000 | Loss: 0.00001179
Iteration 10/1000 | Loss: 0.00001166
Iteration 11/1000 | Loss: 0.00001155
Iteration 12/1000 | Loss: 0.00001143
Iteration 13/1000 | Loss: 0.00001135
Iteration 14/1000 | Loss: 0.00001134
Iteration 15/1000 | Loss: 0.00001133
Iteration 16/1000 | Loss: 0.00001133
Iteration 17/1000 | Loss: 0.00001132
Iteration 18/1000 | Loss: 0.00001121
Iteration 19/1000 | Loss: 0.00001113
Iteration 20/1000 | Loss: 0.00001111
Iteration 21/1000 | Loss: 0.00001110
Iteration 22/1000 | Loss: 0.00001109
Iteration 23/1000 | Loss: 0.00001109
Iteration 24/1000 | Loss: 0.00001108
Iteration 25/1000 | Loss: 0.00001108
Iteration 26/1000 | Loss: 0.00001106
Iteration 27/1000 | Loss: 0.00001098
Iteration 28/1000 | Loss: 0.00001094
Iteration 29/1000 | Loss: 0.00001093
Iteration 30/1000 | Loss: 0.00001093
Iteration 31/1000 | Loss: 0.00001092
Iteration 32/1000 | Loss: 0.00001092
Iteration 33/1000 | Loss: 0.00001092
Iteration 34/1000 | Loss: 0.00001084
Iteration 35/1000 | Loss: 0.00001081
Iteration 36/1000 | Loss: 0.00001081
Iteration 37/1000 | Loss: 0.00001081
Iteration 38/1000 | Loss: 0.00001080
Iteration 39/1000 | Loss: 0.00001079
Iteration 40/1000 | Loss: 0.00001079
Iteration 41/1000 | Loss: 0.00001079
Iteration 42/1000 | Loss: 0.00001078
Iteration 43/1000 | Loss: 0.00001078
Iteration 44/1000 | Loss: 0.00001078
Iteration 45/1000 | Loss: 0.00001078
Iteration 46/1000 | Loss: 0.00001078
Iteration 47/1000 | Loss: 0.00001078
Iteration 48/1000 | Loss: 0.00001078
Iteration 49/1000 | Loss: 0.00001078
Iteration 50/1000 | Loss: 0.00001078
Iteration 51/1000 | Loss: 0.00001078
Iteration 52/1000 | Loss: 0.00001077
Iteration 53/1000 | Loss: 0.00001077
Iteration 54/1000 | Loss: 0.00001077
Iteration 55/1000 | Loss: 0.00001077
Iteration 56/1000 | Loss: 0.00001076
Iteration 57/1000 | Loss: 0.00001076
Iteration 58/1000 | Loss: 0.00001075
Iteration 59/1000 | Loss: 0.00001074
Iteration 60/1000 | Loss: 0.00001074
Iteration 61/1000 | Loss: 0.00001074
Iteration 62/1000 | Loss: 0.00001073
Iteration 63/1000 | Loss: 0.00001073
Iteration 64/1000 | Loss: 0.00001073
Iteration 65/1000 | Loss: 0.00001072
Iteration 66/1000 | Loss: 0.00001072
Iteration 67/1000 | Loss: 0.00001071
Iteration 68/1000 | Loss: 0.00001071
Iteration 69/1000 | Loss: 0.00001071
Iteration 70/1000 | Loss: 0.00001071
Iteration 71/1000 | Loss: 0.00001071
Iteration 72/1000 | Loss: 0.00001071
Iteration 73/1000 | Loss: 0.00001071
Iteration 74/1000 | Loss: 0.00001071
Iteration 75/1000 | Loss: 0.00001070
Iteration 76/1000 | Loss: 0.00001070
Iteration 77/1000 | Loss: 0.00001070
Iteration 78/1000 | Loss: 0.00001070
Iteration 79/1000 | Loss: 0.00001070
Iteration 80/1000 | Loss: 0.00001070
Iteration 81/1000 | Loss: 0.00001070
Iteration 82/1000 | Loss: 0.00001069
Iteration 83/1000 | Loss: 0.00001069
Iteration 84/1000 | Loss: 0.00001069
Iteration 85/1000 | Loss: 0.00001068
Iteration 86/1000 | Loss: 0.00001068
Iteration 87/1000 | Loss: 0.00001067
Iteration 88/1000 | Loss: 0.00001067
Iteration 89/1000 | Loss: 0.00001067
Iteration 90/1000 | Loss: 0.00001067
Iteration 91/1000 | Loss: 0.00001067
Iteration 92/1000 | Loss: 0.00001067
Iteration 93/1000 | Loss: 0.00001066
Iteration 94/1000 | Loss: 0.00001066
Iteration 95/1000 | Loss: 0.00001066
Iteration 96/1000 | Loss: 0.00001066
Iteration 97/1000 | Loss: 0.00001066
Iteration 98/1000 | Loss: 0.00001066
Iteration 99/1000 | Loss: 0.00001065
Iteration 100/1000 | Loss: 0.00001065
Iteration 101/1000 | Loss: 0.00001064
Iteration 102/1000 | Loss: 0.00001064
Iteration 103/1000 | Loss: 0.00001064
Iteration 104/1000 | Loss: 0.00001064
Iteration 105/1000 | Loss: 0.00001064
Iteration 106/1000 | Loss: 0.00001063
Iteration 107/1000 | Loss: 0.00001063
Iteration 108/1000 | Loss: 0.00001063
Iteration 109/1000 | Loss: 0.00001063
Iteration 110/1000 | Loss: 0.00001063
Iteration 111/1000 | Loss: 0.00001063
Iteration 112/1000 | Loss: 0.00001063
Iteration 113/1000 | Loss: 0.00001062
Iteration 114/1000 | Loss: 0.00001062
Iteration 115/1000 | Loss: 0.00001062
Iteration 116/1000 | Loss: 0.00001062
Iteration 117/1000 | Loss: 0.00001061
Iteration 118/1000 | Loss: 0.00001061
Iteration 119/1000 | Loss: 0.00001061
Iteration 120/1000 | Loss: 0.00001061
Iteration 121/1000 | Loss: 0.00001061
Iteration 122/1000 | Loss: 0.00001061
Iteration 123/1000 | Loss: 0.00001061
Iteration 124/1000 | Loss: 0.00001061
Iteration 125/1000 | Loss: 0.00001060
Iteration 126/1000 | Loss: 0.00001060
Iteration 127/1000 | Loss: 0.00001060
Iteration 128/1000 | Loss: 0.00001060
Iteration 129/1000 | Loss: 0.00001060
Iteration 130/1000 | Loss: 0.00001060
Iteration 131/1000 | Loss: 0.00001060
Iteration 132/1000 | Loss: 0.00001060
Iteration 133/1000 | Loss: 0.00001060
Iteration 134/1000 | Loss: 0.00001060
Iteration 135/1000 | Loss: 0.00001059
Iteration 136/1000 | Loss: 0.00001059
Iteration 137/1000 | Loss: 0.00001059
Iteration 138/1000 | Loss: 0.00001059
Iteration 139/1000 | Loss: 0.00001059
Iteration 140/1000 | Loss: 0.00001059
Iteration 141/1000 | Loss: 0.00001059
Iteration 142/1000 | Loss: 0.00001059
Iteration 143/1000 | Loss: 0.00001059
Iteration 144/1000 | Loss: 0.00001059
Iteration 145/1000 | Loss: 0.00001058
Iteration 146/1000 | Loss: 0.00001058
Iteration 147/1000 | Loss: 0.00001058
Iteration 148/1000 | Loss: 0.00001058
Iteration 149/1000 | Loss: 0.00001058
Iteration 150/1000 | Loss: 0.00001058
Iteration 151/1000 | Loss: 0.00001058
Iteration 152/1000 | Loss: 0.00001058
Iteration 153/1000 | Loss: 0.00001058
Iteration 154/1000 | Loss: 0.00001058
Iteration 155/1000 | Loss: 0.00001058
Iteration 156/1000 | Loss: 0.00001058
Iteration 157/1000 | Loss: 0.00001058
Iteration 158/1000 | Loss: 0.00001058
Iteration 159/1000 | Loss: 0.00001058
Iteration 160/1000 | Loss: 0.00001058
Iteration 161/1000 | Loss: 0.00001058
Iteration 162/1000 | Loss: 0.00001057
Iteration 163/1000 | Loss: 0.00001057
Iteration 164/1000 | Loss: 0.00001057
Iteration 165/1000 | Loss: 0.00001057
Iteration 166/1000 | Loss: 0.00001057
Iteration 167/1000 | Loss: 0.00001057
Iteration 168/1000 | Loss: 0.00001056
Iteration 169/1000 | Loss: 0.00001056
Iteration 170/1000 | Loss: 0.00001056
Iteration 171/1000 | Loss: 0.00001056
Iteration 172/1000 | Loss: 0.00001056
Iteration 173/1000 | Loss: 0.00001056
Iteration 174/1000 | Loss: 0.00001056
Iteration 175/1000 | Loss: 0.00001056
Iteration 176/1000 | Loss: 0.00001056
Iteration 177/1000 | Loss: 0.00001056
Iteration 178/1000 | Loss: 0.00001055
Iteration 179/1000 | Loss: 0.00001055
Iteration 180/1000 | Loss: 0.00001055
Iteration 181/1000 | Loss: 0.00001055
Iteration 182/1000 | Loss: 0.00001054
Iteration 183/1000 | Loss: 0.00001054
Iteration 184/1000 | Loss: 0.00001054
Iteration 185/1000 | Loss: 0.00001054
Iteration 186/1000 | Loss: 0.00001054
Iteration 187/1000 | Loss: 0.00001054
Iteration 188/1000 | Loss: 0.00001054
Iteration 189/1000 | Loss: 0.00001054
Iteration 190/1000 | Loss: 0.00001053
Iteration 191/1000 | Loss: 0.00001053
Iteration 192/1000 | Loss: 0.00001053
Iteration 193/1000 | Loss: 0.00001053
Iteration 194/1000 | Loss: 0.00001053
Iteration 195/1000 | Loss: 0.00001053
Iteration 196/1000 | Loss: 0.00001053
Iteration 197/1000 | Loss: 0.00001053
Iteration 198/1000 | Loss: 0.00001053
Iteration 199/1000 | Loss: 0.00001052
Iteration 200/1000 | Loss: 0.00001052
Iteration 201/1000 | Loss: 0.00001052
Iteration 202/1000 | Loss: 0.00001052
Iteration 203/1000 | Loss: 0.00001052
Iteration 204/1000 | Loss: 0.00001052
Iteration 205/1000 | Loss: 0.00001052
Iteration 206/1000 | Loss: 0.00001052
Iteration 207/1000 | Loss: 0.00001052
Iteration 208/1000 | Loss: 0.00001052
Iteration 209/1000 | Loss: 0.00001052
Iteration 210/1000 | Loss: 0.00001052
Iteration 211/1000 | Loss: 0.00001052
Iteration 212/1000 | Loss: 0.00001052
Iteration 213/1000 | Loss: 0.00001052
Iteration 214/1000 | Loss: 0.00001052
Iteration 215/1000 | Loss: 0.00001052
Iteration 216/1000 | Loss: 0.00001052
Iteration 217/1000 | Loss: 0.00001052
Iteration 218/1000 | Loss: 0.00001052
Iteration 219/1000 | Loss: 0.00001052
Iteration 220/1000 | Loss: 0.00001052
Iteration 221/1000 | Loss: 0.00001052
Iteration 222/1000 | Loss: 0.00001052
Iteration 223/1000 | Loss: 0.00001052
Iteration 224/1000 | Loss: 0.00001052
Iteration 225/1000 | Loss: 0.00001052
Iteration 226/1000 | Loss: 0.00001052
Iteration 227/1000 | Loss: 0.00001052
Iteration 228/1000 | Loss: 0.00001052
Iteration 229/1000 | Loss: 0.00001052
Iteration 230/1000 | Loss: 0.00001052
Iteration 231/1000 | Loss: 0.00001052
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 231. Stopping optimization.
Last 5 losses: [1.051975050359033e-05, 1.051975050359033e-05, 1.051975050359033e-05, 1.051975050359033e-05, 1.051975050359033e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.051975050359033e-05

Optimization complete. Final v2v error: 2.8013763427734375 mm

Highest mean error: 3.040844678878784 mm for frame 67

Lowest mean error: 2.692898750305176 mm for frame 143

Saving results

Total time: 44.09395742416382
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_015/1056/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_015/1056.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_015/1056
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01007688
Iteration 2/25 | Loss: 0.00213985
Iteration 3/25 | Loss: 0.00254128
Iteration 4/25 | Loss: 0.00147024
Iteration 5/25 | Loss: 0.00135992
Iteration 6/25 | Loss: 0.00124070
Iteration 7/25 | Loss: 0.00121276
Iteration 8/25 | Loss: 0.00120231
Iteration 9/25 | Loss: 0.00119976
Iteration 10/25 | Loss: 0.00119865
Iteration 11/25 | Loss: 0.00119833
Iteration 12/25 | Loss: 0.00119819
Iteration 13/25 | Loss: 0.00119812
Iteration 14/25 | Loss: 0.00119812
Iteration 15/25 | Loss: 0.00119812
Iteration 16/25 | Loss: 0.00119812
Iteration 17/25 | Loss: 0.00119812
Iteration 18/25 | Loss: 0.00119812
Iteration 19/25 | Loss: 0.00119811
Iteration 20/25 | Loss: 0.00119811
Iteration 21/25 | Loss: 0.00119811
Iteration 22/25 | Loss: 0.00119811
Iteration 23/25 | Loss: 0.00119811
Iteration 24/25 | Loss: 0.00119811
Iteration 25/25 | Loss: 0.00119811

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.40140295
Iteration 2/25 | Loss: 0.00144266
Iteration 3/25 | Loss: 0.00144266
Iteration 4/25 | Loss: 0.00144265
Iteration 5/25 | Loss: 0.00144265
Iteration 6/25 | Loss: 0.00144265
Iteration 7/25 | Loss: 0.00144265
Iteration 8/25 | Loss: 0.00144265
Iteration 9/25 | Loss: 0.00144265
Iteration 10/25 | Loss: 0.00144265
Iteration 11/25 | Loss: 0.00144265
Iteration 12/25 | Loss: 0.00144265
Iteration 13/25 | Loss: 0.00144265
Iteration 14/25 | Loss: 0.00144265
Iteration 15/25 | Loss: 0.00144265
Iteration 16/25 | Loss: 0.00144265
Iteration 17/25 | Loss: 0.00144265
Iteration 18/25 | Loss: 0.00144265
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0014426525449380279, 0.0014426525449380279, 0.0014426525449380279, 0.0014426525449380279, 0.0014426525449380279]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014426525449380279

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00144265
Iteration 2/1000 | Loss: 0.00002717
Iteration 3/1000 | Loss: 0.00001890
Iteration 4/1000 | Loss: 0.00001685
Iteration 5/1000 | Loss: 0.00001526
Iteration 6/1000 | Loss: 0.00001456
Iteration 7/1000 | Loss: 0.00001383
Iteration 8/1000 | Loss: 0.00001334
Iteration 9/1000 | Loss: 0.00001307
Iteration 10/1000 | Loss: 0.00001286
Iteration 11/1000 | Loss: 0.00001266
Iteration 12/1000 | Loss: 0.00001262
Iteration 13/1000 | Loss: 0.00001262
Iteration 14/1000 | Loss: 0.00001255
Iteration 15/1000 | Loss: 0.00001242
Iteration 16/1000 | Loss: 0.00001240
Iteration 17/1000 | Loss: 0.00001240
Iteration 18/1000 | Loss: 0.00001239
Iteration 19/1000 | Loss: 0.00001239
Iteration 20/1000 | Loss: 0.00001236
Iteration 21/1000 | Loss: 0.00001236
Iteration 22/1000 | Loss: 0.00001236
Iteration 23/1000 | Loss: 0.00001236
Iteration 24/1000 | Loss: 0.00001236
Iteration 25/1000 | Loss: 0.00001235
Iteration 26/1000 | Loss: 0.00001235
Iteration 27/1000 | Loss: 0.00001235
Iteration 28/1000 | Loss: 0.00001234
Iteration 29/1000 | Loss: 0.00001234
Iteration 30/1000 | Loss: 0.00001233
Iteration 31/1000 | Loss: 0.00001233
Iteration 32/1000 | Loss: 0.00001232
Iteration 33/1000 | Loss: 0.00001232
Iteration 34/1000 | Loss: 0.00001232
Iteration 35/1000 | Loss: 0.00001231
Iteration 36/1000 | Loss: 0.00001231
Iteration 37/1000 | Loss: 0.00001231
Iteration 38/1000 | Loss: 0.00001230
Iteration 39/1000 | Loss: 0.00001230
Iteration 40/1000 | Loss: 0.00001229
Iteration 41/1000 | Loss: 0.00001228
Iteration 42/1000 | Loss: 0.00001228
Iteration 43/1000 | Loss: 0.00001228
Iteration 44/1000 | Loss: 0.00001227
Iteration 45/1000 | Loss: 0.00001227
Iteration 46/1000 | Loss: 0.00001226
Iteration 47/1000 | Loss: 0.00001226
Iteration 48/1000 | Loss: 0.00001226
Iteration 49/1000 | Loss: 0.00001226
Iteration 50/1000 | Loss: 0.00001225
Iteration 51/1000 | Loss: 0.00001225
Iteration 52/1000 | Loss: 0.00001225
Iteration 53/1000 | Loss: 0.00001224
Iteration 54/1000 | Loss: 0.00001224
Iteration 55/1000 | Loss: 0.00001223
Iteration 56/1000 | Loss: 0.00001223
Iteration 57/1000 | Loss: 0.00001223
Iteration 58/1000 | Loss: 0.00001223
Iteration 59/1000 | Loss: 0.00001223
Iteration 60/1000 | Loss: 0.00001222
Iteration 61/1000 | Loss: 0.00001222
Iteration 62/1000 | Loss: 0.00001222
Iteration 63/1000 | Loss: 0.00001221
Iteration 64/1000 | Loss: 0.00001221
Iteration 65/1000 | Loss: 0.00001220
Iteration 66/1000 | Loss: 0.00001220
Iteration 67/1000 | Loss: 0.00001220
Iteration 68/1000 | Loss: 0.00001220
Iteration 69/1000 | Loss: 0.00001219
Iteration 70/1000 | Loss: 0.00001219
Iteration 71/1000 | Loss: 0.00001219
Iteration 72/1000 | Loss: 0.00001218
Iteration 73/1000 | Loss: 0.00001218
Iteration 74/1000 | Loss: 0.00001217
Iteration 75/1000 | Loss: 0.00001217
Iteration 76/1000 | Loss: 0.00001217
Iteration 77/1000 | Loss: 0.00001216
Iteration 78/1000 | Loss: 0.00001216
Iteration 79/1000 | Loss: 0.00001216
Iteration 80/1000 | Loss: 0.00001215
Iteration 81/1000 | Loss: 0.00001215
Iteration 82/1000 | Loss: 0.00001215
Iteration 83/1000 | Loss: 0.00001214
Iteration 84/1000 | Loss: 0.00001214
Iteration 85/1000 | Loss: 0.00001213
Iteration 86/1000 | Loss: 0.00001213
Iteration 87/1000 | Loss: 0.00001212
Iteration 88/1000 | Loss: 0.00001212
Iteration 89/1000 | Loss: 0.00001212
Iteration 90/1000 | Loss: 0.00001212
Iteration 91/1000 | Loss: 0.00001212
Iteration 92/1000 | Loss: 0.00001211
Iteration 93/1000 | Loss: 0.00001211
Iteration 94/1000 | Loss: 0.00001211
Iteration 95/1000 | Loss: 0.00001211
Iteration 96/1000 | Loss: 0.00001210
Iteration 97/1000 | Loss: 0.00001210
Iteration 98/1000 | Loss: 0.00001210
Iteration 99/1000 | Loss: 0.00001209
Iteration 100/1000 | Loss: 0.00001209
Iteration 101/1000 | Loss: 0.00001209
Iteration 102/1000 | Loss: 0.00001209
Iteration 103/1000 | Loss: 0.00001209
Iteration 104/1000 | Loss: 0.00001209
Iteration 105/1000 | Loss: 0.00001209
Iteration 106/1000 | Loss: 0.00001208
Iteration 107/1000 | Loss: 0.00001208
Iteration 108/1000 | Loss: 0.00001208
Iteration 109/1000 | Loss: 0.00001208
Iteration 110/1000 | Loss: 0.00001207
Iteration 111/1000 | Loss: 0.00001207
Iteration 112/1000 | Loss: 0.00001207
Iteration 113/1000 | Loss: 0.00001206
Iteration 114/1000 | Loss: 0.00001206
Iteration 115/1000 | Loss: 0.00001206
Iteration 116/1000 | Loss: 0.00001205
Iteration 117/1000 | Loss: 0.00001205
Iteration 118/1000 | Loss: 0.00001205
Iteration 119/1000 | Loss: 0.00001204
Iteration 120/1000 | Loss: 0.00001204
Iteration 121/1000 | Loss: 0.00001204
Iteration 122/1000 | Loss: 0.00001204
Iteration 123/1000 | Loss: 0.00001204
Iteration 124/1000 | Loss: 0.00001204
Iteration 125/1000 | Loss: 0.00001204
Iteration 126/1000 | Loss: 0.00001203
Iteration 127/1000 | Loss: 0.00001203
Iteration 128/1000 | Loss: 0.00001203
Iteration 129/1000 | Loss: 0.00001202
Iteration 130/1000 | Loss: 0.00001202
Iteration 131/1000 | Loss: 0.00001202
Iteration 132/1000 | Loss: 0.00001202
Iteration 133/1000 | Loss: 0.00001202
Iteration 134/1000 | Loss: 0.00001202
Iteration 135/1000 | Loss: 0.00001202
Iteration 136/1000 | Loss: 0.00001201
Iteration 137/1000 | Loss: 0.00001201
Iteration 138/1000 | Loss: 0.00001201
Iteration 139/1000 | Loss: 0.00001201
Iteration 140/1000 | Loss: 0.00001201
Iteration 141/1000 | Loss: 0.00001200
Iteration 142/1000 | Loss: 0.00001200
Iteration 143/1000 | Loss: 0.00001200
Iteration 144/1000 | Loss: 0.00001200
Iteration 145/1000 | Loss: 0.00001200
Iteration 146/1000 | Loss: 0.00001200
Iteration 147/1000 | Loss: 0.00001199
Iteration 148/1000 | Loss: 0.00001199
Iteration 149/1000 | Loss: 0.00001199
Iteration 150/1000 | Loss: 0.00001199
Iteration 151/1000 | Loss: 0.00001199
Iteration 152/1000 | Loss: 0.00001198
Iteration 153/1000 | Loss: 0.00001198
Iteration 154/1000 | Loss: 0.00001198
Iteration 155/1000 | Loss: 0.00001198
Iteration 156/1000 | Loss: 0.00001198
Iteration 157/1000 | Loss: 0.00001198
Iteration 158/1000 | Loss: 0.00001198
Iteration 159/1000 | Loss: 0.00001198
Iteration 160/1000 | Loss: 0.00001198
Iteration 161/1000 | Loss: 0.00001198
Iteration 162/1000 | Loss: 0.00001198
Iteration 163/1000 | Loss: 0.00001198
Iteration 164/1000 | Loss: 0.00001198
Iteration 165/1000 | Loss: 0.00001197
Iteration 166/1000 | Loss: 0.00001197
Iteration 167/1000 | Loss: 0.00001197
Iteration 168/1000 | Loss: 0.00001197
Iteration 169/1000 | Loss: 0.00001197
Iteration 170/1000 | Loss: 0.00001197
Iteration 171/1000 | Loss: 0.00001197
Iteration 172/1000 | Loss: 0.00001197
Iteration 173/1000 | Loss: 0.00001197
Iteration 174/1000 | Loss: 0.00001197
Iteration 175/1000 | Loss: 0.00001197
Iteration 176/1000 | Loss: 0.00001196
Iteration 177/1000 | Loss: 0.00001196
Iteration 178/1000 | Loss: 0.00001196
Iteration 179/1000 | Loss: 0.00001196
Iteration 180/1000 | Loss: 0.00001196
Iteration 181/1000 | Loss: 0.00001196
Iteration 182/1000 | Loss: 0.00001196
Iteration 183/1000 | Loss: 0.00001196
Iteration 184/1000 | Loss: 0.00001195
Iteration 185/1000 | Loss: 0.00001195
Iteration 186/1000 | Loss: 0.00001195
Iteration 187/1000 | Loss: 0.00001195
Iteration 188/1000 | Loss: 0.00001195
Iteration 189/1000 | Loss: 0.00001195
Iteration 190/1000 | Loss: 0.00001195
Iteration 191/1000 | Loss: 0.00001195
Iteration 192/1000 | Loss: 0.00001195
Iteration 193/1000 | Loss: 0.00001195
Iteration 194/1000 | Loss: 0.00001195
Iteration 195/1000 | Loss: 0.00001195
Iteration 196/1000 | Loss: 0.00001194
Iteration 197/1000 | Loss: 0.00001194
Iteration 198/1000 | Loss: 0.00001194
Iteration 199/1000 | Loss: 0.00001194
Iteration 200/1000 | Loss: 0.00001194
Iteration 201/1000 | Loss: 0.00001194
Iteration 202/1000 | Loss: 0.00001194
Iteration 203/1000 | Loss: 0.00001194
Iteration 204/1000 | Loss: 0.00001194
Iteration 205/1000 | Loss: 0.00001194
Iteration 206/1000 | Loss: 0.00001194
Iteration 207/1000 | Loss: 0.00001194
Iteration 208/1000 | Loss: 0.00001194
Iteration 209/1000 | Loss: 0.00001194
Iteration 210/1000 | Loss: 0.00001194
Iteration 211/1000 | Loss: 0.00001194
Iteration 212/1000 | Loss: 0.00001194
Iteration 213/1000 | Loss: 0.00001194
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 213. Stopping optimization.
Last 5 losses: [1.1939210708078463e-05, 1.1939210708078463e-05, 1.1939210708078463e-05, 1.1939210708078463e-05, 1.1939210708078463e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1939210708078463e-05

Optimization complete. Final v2v error: 2.914076089859009 mm

Highest mean error: 3.5848591327667236 mm for frame 88

Lowest mean error: 2.5493199825286865 mm for frame 124

Saving results

Total time: 54.82527303695679
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_015/1046/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_015/1046.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_015/1046
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00877690
Iteration 2/25 | Loss: 0.00163499
Iteration 3/25 | Loss: 0.00137083
Iteration 4/25 | Loss: 0.00132511
Iteration 5/25 | Loss: 0.00132615
Iteration 6/25 | Loss: 0.00131952
Iteration 7/25 | Loss: 0.00130620
Iteration 8/25 | Loss: 0.00128937
Iteration 9/25 | Loss: 0.00127264
Iteration 10/25 | Loss: 0.00126572
Iteration 11/25 | Loss: 0.00126030
Iteration 12/25 | Loss: 0.00125790
Iteration 13/25 | Loss: 0.00126401
Iteration 14/25 | Loss: 0.00127472
Iteration 15/25 | Loss: 0.00127046
Iteration 16/25 | Loss: 0.00126045
Iteration 17/25 | Loss: 0.00125415
Iteration 18/25 | Loss: 0.00125212
Iteration 19/25 | Loss: 0.00125153
Iteration 20/25 | Loss: 0.00125139
Iteration 21/25 | Loss: 0.00125137
Iteration 22/25 | Loss: 0.00125137
Iteration 23/25 | Loss: 0.00125136
Iteration 24/25 | Loss: 0.00125136
Iteration 25/25 | Loss: 0.00125136

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.22937775
Iteration 2/25 | Loss: 0.00156088
Iteration 3/25 | Loss: 0.00156087
Iteration 4/25 | Loss: 0.00156087
Iteration 5/25 | Loss: 0.00156087
Iteration 6/25 | Loss: 0.00156086
Iteration 7/25 | Loss: 0.00156086
Iteration 8/25 | Loss: 0.00156086
Iteration 9/25 | Loss: 0.00156086
Iteration 10/25 | Loss: 0.00156086
Iteration 11/25 | Loss: 0.00156086
Iteration 12/25 | Loss: 0.00156086
Iteration 13/25 | Loss: 0.00156086
Iteration 14/25 | Loss: 0.00156086
Iteration 15/25 | Loss: 0.00156086
Iteration 16/25 | Loss: 0.00156086
Iteration 17/25 | Loss: 0.00156086
Iteration 18/25 | Loss: 0.00156086
Iteration 19/25 | Loss: 0.00156086
Iteration 20/25 | Loss: 0.00156086
Iteration 21/25 | Loss: 0.00156086
Iteration 22/25 | Loss: 0.00156086
Iteration 23/25 | Loss: 0.00156086
Iteration 24/25 | Loss: 0.00156086
Iteration 25/25 | Loss: 0.00156086
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0015608619432896376, 0.0015608619432896376, 0.0015608619432896376, 0.0015608619432896376, 0.0015608619432896376]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015608619432896376

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00156086
Iteration 2/1000 | Loss: 0.00014164
Iteration 3/1000 | Loss: 0.00027474
Iteration 4/1000 | Loss: 0.00004071
Iteration 5/1000 | Loss: 0.00003059
Iteration 6/1000 | Loss: 0.00002587
Iteration 7/1000 | Loss: 0.00002431
Iteration 8/1000 | Loss: 0.00002317
Iteration 9/1000 | Loss: 0.00002224
Iteration 10/1000 | Loss: 0.00002156
Iteration 11/1000 | Loss: 0.00002101
Iteration 12/1000 | Loss: 0.00002070
Iteration 13/1000 | Loss: 0.00002044
Iteration 14/1000 | Loss: 0.00002023
Iteration 15/1000 | Loss: 0.00002014
Iteration 16/1000 | Loss: 0.00001996
Iteration 17/1000 | Loss: 0.00001973
Iteration 18/1000 | Loss: 0.00001960
Iteration 19/1000 | Loss: 0.00001958
Iteration 20/1000 | Loss: 0.00001957
Iteration 21/1000 | Loss: 0.00001956
Iteration 22/1000 | Loss: 0.00001955
Iteration 23/1000 | Loss: 0.00001954
Iteration 24/1000 | Loss: 0.00001953
Iteration 25/1000 | Loss: 0.00001947
Iteration 26/1000 | Loss: 0.00001945
Iteration 27/1000 | Loss: 0.00001945
Iteration 28/1000 | Loss: 0.00001944
Iteration 29/1000 | Loss: 0.00001944
Iteration 30/1000 | Loss: 0.00001943
Iteration 31/1000 | Loss: 0.00001941
Iteration 32/1000 | Loss: 0.00001940
Iteration 33/1000 | Loss: 0.00001939
Iteration 34/1000 | Loss: 0.00001938
Iteration 35/1000 | Loss: 0.00001938
Iteration 36/1000 | Loss: 0.00001938
Iteration 37/1000 | Loss: 0.00001937
Iteration 38/1000 | Loss: 0.00001937
Iteration 39/1000 | Loss: 0.00001937
Iteration 40/1000 | Loss: 0.00001936
Iteration 41/1000 | Loss: 0.00001936
Iteration 42/1000 | Loss: 0.00001932
Iteration 43/1000 | Loss: 0.00001931
Iteration 44/1000 | Loss: 0.00001927
Iteration 45/1000 | Loss: 0.00001927
Iteration 46/1000 | Loss: 0.00001925
Iteration 47/1000 | Loss: 0.00001925
Iteration 48/1000 | Loss: 0.00001924
Iteration 49/1000 | Loss: 0.00001924
Iteration 50/1000 | Loss: 0.00001924
Iteration 51/1000 | Loss: 0.00001923
Iteration 52/1000 | Loss: 0.00001923
Iteration 53/1000 | Loss: 0.00001923
Iteration 54/1000 | Loss: 0.00001923
Iteration 55/1000 | Loss: 0.00001922
Iteration 56/1000 | Loss: 0.00001922
Iteration 57/1000 | Loss: 0.00001922
Iteration 58/1000 | Loss: 0.00001921
Iteration 59/1000 | Loss: 0.00001921
Iteration 60/1000 | Loss: 0.00001921
Iteration 61/1000 | Loss: 0.00001921
Iteration 62/1000 | Loss: 0.00001920
Iteration 63/1000 | Loss: 0.00001920
Iteration 64/1000 | Loss: 0.00001920
Iteration 65/1000 | Loss: 0.00001919
Iteration 66/1000 | Loss: 0.00001919
Iteration 67/1000 | Loss: 0.00001919
Iteration 68/1000 | Loss: 0.00001918
Iteration 69/1000 | Loss: 0.00001918
Iteration 70/1000 | Loss: 0.00001918
Iteration 71/1000 | Loss: 0.00001917
Iteration 72/1000 | Loss: 0.00001917
Iteration 73/1000 | Loss: 0.00001917
Iteration 74/1000 | Loss: 0.00001917
Iteration 75/1000 | Loss: 0.00001917
Iteration 76/1000 | Loss: 0.00001917
Iteration 77/1000 | Loss: 0.00001916
Iteration 78/1000 | Loss: 0.00001916
Iteration 79/1000 | Loss: 0.00001916
Iteration 80/1000 | Loss: 0.00001915
Iteration 81/1000 | Loss: 0.00001915
Iteration 82/1000 | Loss: 0.00001914
Iteration 83/1000 | Loss: 0.00001914
Iteration 84/1000 | Loss: 0.00001914
Iteration 85/1000 | Loss: 0.00001914
Iteration 86/1000 | Loss: 0.00001914
Iteration 87/1000 | Loss: 0.00001914
Iteration 88/1000 | Loss: 0.00001914
Iteration 89/1000 | Loss: 0.00001914
Iteration 90/1000 | Loss: 0.00001914
Iteration 91/1000 | Loss: 0.00001914
Iteration 92/1000 | Loss: 0.00001914
Iteration 93/1000 | Loss: 0.00001913
Iteration 94/1000 | Loss: 0.00001913
Iteration 95/1000 | Loss: 0.00001912
Iteration 96/1000 | Loss: 0.00001912
Iteration 97/1000 | Loss: 0.00001912
Iteration 98/1000 | Loss: 0.00001911
Iteration 99/1000 | Loss: 0.00001911
Iteration 100/1000 | Loss: 0.00001911
Iteration 101/1000 | Loss: 0.00001910
Iteration 102/1000 | Loss: 0.00001910
Iteration 103/1000 | Loss: 0.00001910
Iteration 104/1000 | Loss: 0.00001910
Iteration 105/1000 | Loss: 0.00001910
Iteration 106/1000 | Loss: 0.00001910
Iteration 107/1000 | Loss: 0.00001910
Iteration 108/1000 | Loss: 0.00001910
Iteration 109/1000 | Loss: 0.00001910
Iteration 110/1000 | Loss: 0.00001910
Iteration 111/1000 | Loss: 0.00001910
Iteration 112/1000 | Loss: 0.00001910
Iteration 113/1000 | Loss: 0.00001909
Iteration 114/1000 | Loss: 0.00001909
Iteration 115/1000 | Loss: 0.00001908
Iteration 116/1000 | Loss: 0.00001908
Iteration 117/1000 | Loss: 0.00001907
Iteration 118/1000 | Loss: 0.00001907
Iteration 119/1000 | Loss: 0.00001907
Iteration 120/1000 | Loss: 0.00001907
Iteration 121/1000 | Loss: 0.00001907
Iteration 122/1000 | Loss: 0.00001907
Iteration 123/1000 | Loss: 0.00001907
Iteration 124/1000 | Loss: 0.00001907
Iteration 125/1000 | Loss: 0.00001907
Iteration 126/1000 | Loss: 0.00001907
Iteration 127/1000 | Loss: 0.00001907
Iteration 128/1000 | Loss: 0.00001907
Iteration 129/1000 | Loss: 0.00001906
Iteration 130/1000 | Loss: 0.00001906
Iteration 131/1000 | Loss: 0.00001906
Iteration 132/1000 | Loss: 0.00001905
Iteration 133/1000 | Loss: 0.00001905
Iteration 134/1000 | Loss: 0.00001905
Iteration 135/1000 | Loss: 0.00001905
Iteration 136/1000 | Loss: 0.00001905
Iteration 137/1000 | Loss: 0.00001905
Iteration 138/1000 | Loss: 0.00001905
Iteration 139/1000 | Loss: 0.00001905
Iteration 140/1000 | Loss: 0.00001905
Iteration 141/1000 | Loss: 0.00001905
Iteration 142/1000 | Loss: 0.00001905
Iteration 143/1000 | Loss: 0.00001905
Iteration 144/1000 | Loss: 0.00001905
Iteration 145/1000 | Loss: 0.00001905
Iteration 146/1000 | Loss: 0.00001904
Iteration 147/1000 | Loss: 0.00001904
Iteration 148/1000 | Loss: 0.00001904
Iteration 149/1000 | Loss: 0.00001904
Iteration 150/1000 | Loss: 0.00001904
Iteration 151/1000 | Loss: 0.00001904
Iteration 152/1000 | Loss: 0.00001904
Iteration 153/1000 | Loss: 0.00001904
Iteration 154/1000 | Loss: 0.00001904
Iteration 155/1000 | Loss: 0.00001904
Iteration 156/1000 | Loss: 0.00001904
Iteration 157/1000 | Loss: 0.00001903
Iteration 158/1000 | Loss: 0.00001903
Iteration 159/1000 | Loss: 0.00001903
Iteration 160/1000 | Loss: 0.00001903
Iteration 161/1000 | Loss: 0.00001903
Iteration 162/1000 | Loss: 0.00001903
Iteration 163/1000 | Loss: 0.00001903
Iteration 164/1000 | Loss: 0.00001903
Iteration 165/1000 | Loss: 0.00001903
Iteration 166/1000 | Loss: 0.00001903
Iteration 167/1000 | Loss: 0.00001903
Iteration 168/1000 | Loss: 0.00001903
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 168. Stopping optimization.
Last 5 losses: [1.9027957023354247e-05, 1.9027957023354247e-05, 1.9027957023354247e-05, 1.9027957023354247e-05, 1.9027957023354247e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9027957023354247e-05

Optimization complete. Final v2v error: 3.6376149654388428 mm

Highest mean error: 5.6442389488220215 mm for frame 94

Lowest mean error: 3.078045129776001 mm for frame 69

Saving results

Total time: 77.61994814872742
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_015/1001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_015/1001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_015/1001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01019196
Iteration 2/25 | Loss: 0.01019196
Iteration 3/25 | Loss: 0.01019195
Iteration 4/25 | Loss: 0.01019195
Iteration 5/25 | Loss: 0.01019195
Iteration 6/25 | Loss: 0.01019195
Iteration 7/25 | Loss: 0.01019195
Iteration 8/25 | Loss: 0.01019194
Iteration 9/25 | Loss: 0.01019194
Iteration 10/25 | Loss: 0.01019194
Iteration 11/25 | Loss: 0.01019194
Iteration 12/25 | Loss: 0.01019194
Iteration 13/25 | Loss: 0.01019193
Iteration 14/25 | Loss: 0.01019193
Iteration 15/25 | Loss: 0.01019193
Iteration 16/25 | Loss: 0.01019193
Iteration 17/25 | Loss: 0.01019193
Iteration 18/25 | Loss: 0.01019192
Iteration 19/25 | Loss: 0.01019192
Iteration 20/25 | Loss: 0.01019192
Iteration 21/25 | Loss: 0.01019192
Iteration 22/25 | Loss: 0.01019191
Iteration 23/25 | Loss: 0.01019191
Iteration 24/25 | Loss: 0.01019191
Iteration 25/25 | Loss: 0.01019190

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.41273940
Iteration 2/25 | Loss: 0.15007594
Iteration 3/25 | Loss: 0.14755423
Iteration 4/25 | Loss: 0.14724024
Iteration 5/25 | Loss: 0.14724024
Iteration 6/25 | Loss: 0.14724024
Iteration 7/25 | Loss: 0.14724024
Iteration 8/25 | Loss: 0.14724024
Iteration 9/25 | Loss: 0.14724024
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 9. Stopping optimization.
Last 5 losses: [0.14724023640155792, 0.14724023640155792, 0.14724023640155792, 0.14724023640155792, 0.14724023640155792]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.14724023640155792

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.14724024
Iteration 2/1000 | Loss: 0.00194462
Iteration 3/1000 | Loss: 0.00067256
Iteration 4/1000 | Loss: 0.00036470
Iteration 5/1000 | Loss: 0.00073869
Iteration 6/1000 | Loss: 0.00022927
Iteration 7/1000 | Loss: 0.00037573
Iteration 8/1000 | Loss: 0.00122258
Iteration 9/1000 | Loss: 0.00045337
Iteration 10/1000 | Loss: 0.00060391
Iteration 11/1000 | Loss: 0.00038656
Iteration 12/1000 | Loss: 0.00016688
Iteration 13/1000 | Loss: 0.00085089
Iteration 14/1000 | Loss: 0.00003696
Iteration 15/1000 | Loss: 0.00028182
Iteration 16/1000 | Loss: 0.00007487
Iteration 17/1000 | Loss: 0.00018768
Iteration 18/1000 | Loss: 0.00005057
Iteration 19/1000 | Loss: 0.00005812
Iteration 20/1000 | Loss: 0.00008489
Iteration 21/1000 | Loss: 0.00009475
Iteration 22/1000 | Loss: 0.00008617
Iteration 23/1000 | Loss: 0.00007974
Iteration 24/1000 | Loss: 0.00007083
Iteration 25/1000 | Loss: 0.00011354
Iteration 26/1000 | Loss: 0.00005262
Iteration 27/1000 | Loss: 0.00005133
Iteration 28/1000 | Loss: 0.00005231
Iteration 29/1000 | Loss: 0.00001904
Iteration 30/1000 | Loss: 0.00004118
Iteration 31/1000 | Loss: 0.00001558
Iteration 32/1000 | Loss: 0.00011981
Iteration 33/1000 | Loss: 0.00002714
Iteration 34/1000 | Loss: 0.00006571
Iteration 35/1000 | Loss: 0.00001825
Iteration 36/1000 | Loss: 0.00001455
Iteration 37/1000 | Loss: 0.00004652
Iteration 38/1000 | Loss: 0.00003455
Iteration 39/1000 | Loss: 0.00001448
Iteration 40/1000 | Loss: 0.00002652
Iteration 41/1000 | Loss: 0.00001373
Iteration 42/1000 | Loss: 0.00003098
Iteration 43/1000 | Loss: 0.00001598
Iteration 44/1000 | Loss: 0.00001435
Iteration 45/1000 | Loss: 0.00001418
Iteration 46/1000 | Loss: 0.00001289
Iteration 47/1000 | Loss: 0.00003172
Iteration 48/1000 | Loss: 0.00007390
Iteration 49/1000 | Loss: 0.00001266
Iteration 50/1000 | Loss: 0.00001336
Iteration 51/1000 | Loss: 0.00001238
Iteration 52/1000 | Loss: 0.00001237
Iteration 53/1000 | Loss: 0.00001237
Iteration 54/1000 | Loss: 0.00001237
Iteration 55/1000 | Loss: 0.00001237
Iteration 56/1000 | Loss: 0.00001235
Iteration 57/1000 | Loss: 0.00001235
Iteration 58/1000 | Loss: 0.00001235
Iteration 59/1000 | Loss: 0.00001235
Iteration 60/1000 | Loss: 0.00001234
Iteration 61/1000 | Loss: 0.00001234
Iteration 62/1000 | Loss: 0.00001233
Iteration 63/1000 | Loss: 0.00001232
Iteration 64/1000 | Loss: 0.00001231
Iteration 65/1000 | Loss: 0.00001230
Iteration 66/1000 | Loss: 0.00001230
Iteration 67/1000 | Loss: 0.00001251
Iteration 68/1000 | Loss: 0.00003346
Iteration 69/1000 | Loss: 0.00001274
Iteration 70/1000 | Loss: 0.00001316
Iteration 71/1000 | Loss: 0.00001210
Iteration 72/1000 | Loss: 0.00001209
Iteration 73/1000 | Loss: 0.00001209
Iteration 74/1000 | Loss: 0.00001209
Iteration 75/1000 | Loss: 0.00001208
Iteration 76/1000 | Loss: 0.00001208
Iteration 77/1000 | Loss: 0.00001208
Iteration 78/1000 | Loss: 0.00001208
Iteration 79/1000 | Loss: 0.00001207
Iteration 80/1000 | Loss: 0.00001207
Iteration 81/1000 | Loss: 0.00001206
Iteration 82/1000 | Loss: 0.00001206
Iteration 83/1000 | Loss: 0.00001205
Iteration 84/1000 | Loss: 0.00001205
Iteration 85/1000 | Loss: 0.00001205
Iteration 86/1000 | Loss: 0.00001205
Iteration 87/1000 | Loss: 0.00001204
Iteration 88/1000 | Loss: 0.00001204
Iteration 89/1000 | Loss: 0.00001204
Iteration 90/1000 | Loss: 0.00001203
Iteration 91/1000 | Loss: 0.00001202
Iteration 92/1000 | Loss: 0.00001202
Iteration 93/1000 | Loss: 0.00001202
Iteration 94/1000 | Loss: 0.00001202
Iteration 95/1000 | Loss: 0.00001202
Iteration 96/1000 | Loss: 0.00001636
Iteration 97/1000 | Loss: 0.00001266
Iteration 98/1000 | Loss: 0.00001251
Iteration 99/1000 | Loss: 0.00001194
Iteration 100/1000 | Loss: 0.00001193
Iteration 101/1000 | Loss: 0.00001193
Iteration 102/1000 | Loss: 0.00001193
Iteration 103/1000 | Loss: 0.00001193
Iteration 104/1000 | Loss: 0.00001193
Iteration 105/1000 | Loss: 0.00001193
Iteration 106/1000 | Loss: 0.00001193
Iteration 107/1000 | Loss: 0.00001193
Iteration 108/1000 | Loss: 0.00001193
Iteration 109/1000 | Loss: 0.00001193
Iteration 110/1000 | Loss: 0.00001193
Iteration 111/1000 | Loss: 0.00001193
Iteration 112/1000 | Loss: 0.00001192
Iteration 113/1000 | Loss: 0.00001192
Iteration 114/1000 | Loss: 0.00001192
Iteration 115/1000 | Loss: 0.00001192
Iteration 116/1000 | Loss: 0.00001192
Iteration 117/1000 | Loss: 0.00001192
Iteration 118/1000 | Loss: 0.00001192
Iteration 119/1000 | Loss: 0.00001192
Iteration 120/1000 | Loss: 0.00001192
Iteration 121/1000 | Loss: 0.00001192
Iteration 122/1000 | Loss: 0.00001192
Iteration 123/1000 | Loss: 0.00001192
Iteration 124/1000 | Loss: 0.00001192
Iteration 125/1000 | Loss: 0.00001192
Iteration 126/1000 | Loss: 0.00001192
Iteration 127/1000 | Loss: 0.00001192
Iteration 128/1000 | Loss: 0.00001192
Iteration 129/1000 | Loss: 0.00001192
Iteration 130/1000 | Loss: 0.00001192
Iteration 131/1000 | Loss: 0.00001192
Iteration 132/1000 | Loss: 0.00001192
Iteration 133/1000 | Loss: 0.00001192
Iteration 134/1000 | Loss: 0.00001192
Iteration 135/1000 | Loss: 0.00001192
Iteration 136/1000 | Loss: 0.00001192
Iteration 137/1000 | Loss: 0.00001192
Iteration 138/1000 | Loss: 0.00001192
Iteration 139/1000 | Loss: 0.00001192
Iteration 140/1000 | Loss: 0.00001192
Iteration 141/1000 | Loss: 0.00001192
Iteration 142/1000 | Loss: 0.00001192
Iteration 143/1000 | Loss: 0.00001192
Iteration 144/1000 | Loss: 0.00001192
Iteration 145/1000 | Loss: 0.00001192
Iteration 146/1000 | Loss: 0.00001192
Iteration 147/1000 | Loss: 0.00001192
Iteration 148/1000 | Loss: 0.00001192
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 148. Stopping optimization.
Last 5 losses: [1.1924441423616372e-05, 1.1924441423616372e-05, 1.1924441423616372e-05, 1.1924441423616372e-05, 1.1924441423616372e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1924441423616372e-05

Optimization complete. Final v2v error: 2.9572086334228516 mm

Highest mean error: 3.686595916748047 mm for frame 2

Lowest mean error: 2.616185426712036 mm for frame 39

Saving results

Total time: 105.5271565914154
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_015/1057/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_015/1057.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_015/1057
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00428349
Iteration 2/25 | Loss: 0.00127577
Iteration 3/25 | Loss: 0.00118917
Iteration 4/25 | Loss: 0.00117882
Iteration 5/25 | Loss: 0.00117624
Iteration 6/25 | Loss: 0.00117561
Iteration 7/25 | Loss: 0.00117561
Iteration 8/25 | Loss: 0.00117561
Iteration 9/25 | Loss: 0.00117561
Iteration 10/25 | Loss: 0.00117561
Iteration 11/25 | Loss: 0.00117561
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011756118619814515, 0.0011756118619814515, 0.0011756118619814515, 0.0011756118619814515, 0.0011756118619814515]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011756118619814515

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.44529629
Iteration 2/25 | Loss: 0.00130745
Iteration 3/25 | Loss: 0.00130744
Iteration 4/25 | Loss: 0.00130744
Iteration 5/25 | Loss: 0.00130743
Iteration 6/25 | Loss: 0.00130743
Iteration 7/25 | Loss: 0.00130743
Iteration 8/25 | Loss: 0.00130743
Iteration 9/25 | Loss: 0.00130743
Iteration 10/25 | Loss: 0.00130743
Iteration 11/25 | Loss: 0.00130743
Iteration 12/25 | Loss: 0.00130743
Iteration 13/25 | Loss: 0.00130743
Iteration 14/25 | Loss: 0.00130743
Iteration 15/25 | Loss: 0.00130743
Iteration 16/25 | Loss: 0.00130743
Iteration 17/25 | Loss: 0.00130743
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0013074320740997791, 0.0013074320740997791, 0.0013074320740997791, 0.0013074320740997791, 0.0013074320740997791]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013074320740997791

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00130743
Iteration 2/1000 | Loss: 0.00002542
Iteration 3/1000 | Loss: 0.00001852
Iteration 4/1000 | Loss: 0.00001568
Iteration 5/1000 | Loss: 0.00001457
Iteration 6/1000 | Loss: 0.00001378
Iteration 7/1000 | Loss: 0.00001323
Iteration 8/1000 | Loss: 0.00001274
Iteration 9/1000 | Loss: 0.00001246
Iteration 10/1000 | Loss: 0.00001234
Iteration 11/1000 | Loss: 0.00001200
Iteration 12/1000 | Loss: 0.00001180
Iteration 13/1000 | Loss: 0.00001166
Iteration 14/1000 | Loss: 0.00001162
Iteration 15/1000 | Loss: 0.00001161
Iteration 16/1000 | Loss: 0.00001160
Iteration 17/1000 | Loss: 0.00001159
Iteration 18/1000 | Loss: 0.00001158
Iteration 19/1000 | Loss: 0.00001152
Iteration 20/1000 | Loss: 0.00001152
Iteration 21/1000 | Loss: 0.00001149
Iteration 22/1000 | Loss: 0.00001149
Iteration 23/1000 | Loss: 0.00001148
Iteration 24/1000 | Loss: 0.00001148
Iteration 25/1000 | Loss: 0.00001147
Iteration 26/1000 | Loss: 0.00001147
Iteration 27/1000 | Loss: 0.00001146
Iteration 28/1000 | Loss: 0.00001145
Iteration 29/1000 | Loss: 0.00001145
Iteration 30/1000 | Loss: 0.00001144
Iteration 31/1000 | Loss: 0.00001143
Iteration 32/1000 | Loss: 0.00001140
Iteration 33/1000 | Loss: 0.00001139
Iteration 34/1000 | Loss: 0.00001138
Iteration 35/1000 | Loss: 0.00001138
Iteration 36/1000 | Loss: 0.00001137
Iteration 37/1000 | Loss: 0.00001136
Iteration 38/1000 | Loss: 0.00001136
Iteration 39/1000 | Loss: 0.00001132
Iteration 40/1000 | Loss: 0.00001129
Iteration 41/1000 | Loss: 0.00001129
Iteration 42/1000 | Loss: 0.00001128
Iteration 43/1000 | Loss: 0.00001128
Iteration 44/1000 | Loss: 0.00001128
Iteration 45/1000 | Loss: 0.00001127
Iteration 46/1000 | Loss: 0.00001126
Iteration 47/1000 | Loss: 0.00001126
Iteration 48/1000 | Loss: 0.00001125
Iteration 49/1000 | Loss: 0.00001125
Iteration 50/1000 | Loss: 0.00001125
Iteration 51/1000 | Loss: 0.00001124
Iteration 52/1000 | Loss: 0.00001124
Iteration 53/1000 | Loss: 0.00001123
Iteration 54/1000 | Loss: 0.00001123
Iteration 55/1000 | Loss: 0.00001123
Iteration 56/1000 | Loss: 0.00001121
Iteration 57/1000 | Loss: 0.00001120
Iteration 58/1000 | Loss: 0.00001120
Iteration 59/1000 | Loss: 0.00001119
Iteration 60/1000 | Loss: 0.00001119
Iteration 61/1000 | Loss: 0.00001119
Iteration 62/1000 | Loss: 0.00001118
Iteration 63/1000 | Loss: 0.00001117
Iteration 64/1000 | Loss: 0.00001117
Iteration 65/1000 | Loss: 0.00001116
Iteration 66/1000 | Loss: 0.00001116
Iteration 67/1000 | Loss: 0.00001116
Iteration 68/1000 | Loss: 0.00001116
Iteration 69/1000 | Loss: 0.00001116
Iteration 70/1000 | Loss: 0.00001116
Iteration 71/1000 | Loss: 0.00001116
Iteration 72/1000 | Loss: 0.00001115
Iteration 73/1000 | Loss: 0.00001114
Iteration 74/1000 | Loss: 0.00001114
Iteration 75/1000 | Loss: 0.00001114
Iteration 76/1000 | Loss: 0.00001113
Iteration 77/1000 | Loss: 0.00001113
Iteration 78/1000 | Loss: 0.00001112
Iteration 79/1000 | Loss: 0.00001112
Iteration 80/1000 | Loss: 0.00001112
Iteration 81/1000 | Loss: 0.00001111
Iteration 82/1000 | Loss: 0.00001111
Iteration 83/1000 | Loss: 0.00001111
Iteration 84/1000 | Loss: 0.00001111
Iteration 85/1000 | Loss: 0.00001111
Iteration 86/1000 | Loss: 0.00001111
Iteration 87/1000 | Loss: 0.00001110
Iteration 88/1000 | Loss: 0.00001110
Iteration 89/1000 | Loss: 0.00001109
Iteration 90/1000 | Loss: 0.00001109
Iteration 91/1000 | Loss: 0.00001109
Iteration 92/1000 | Loss: 0.00001109
Iteration 93/1000 | Loss: 0.00001109
Iteration 94/1000 | Loss: 0.00001109
Iteration 95/1000 | Loss: 0.00001108
Iteration 96/1000 | Loss: 0.00001108
Iteration 97/1000 | Loss: 0.00001108
Iteration 98/1000 | Loss: 0.00001108
Iteration 99/1000 | Loss: 0.00001108
Iteration 100/1000 | Loss: 0.00001107
Iteration 101/1000 | Loss: 0.00001107
Iteration 102/1000 | Loss: 0.00001107
Iteration 103/1000 | Loss: 0.00001107
Iteration 104/1000 | Loss: 0.00001107
Iteration 105/1000 | Loss: 0.00001107
Iteration 106/1000 | Loss: 0.00001107
Iteration 107/1000 | Loss: 0.00001107
Iteration 108/1000 | Loss: 0.00001106
Iteration 109/1000 | Loss: 0.00001106
Iteration 110/1000 | Loss: 0.00001106
Iteration 111/1000 | Loss: 0.00001106
Iteration 112/1000 | Loss: 0.00001105
Iteration 113/1000 | Loss: 0.00001105
Iteration 114/1000 | Loss: 0.00001105
Iteration 115/1000 | Loss: 0.00001105
Iteration 116/1000 | Loss: 0.00001105
Iteration 117/1000 | Loss: 0.00001105
Iteration 118/1000 | Loss: 0.00001105
Iteration 119/1000 | Loss: 0.00001105
Iteration 120/1000 | Loss: 0.00001105
Iteration 121/1000 | Loss: 0.00001105
Iteration 122/1000 | Loss: 0.00001105
Iteration 123/1000 | Loss: 0.00001105
Iteration 124/1000 | Loss: 0.00001105
Iteration 125/1000 | Loss: 0.00001104
Iteration 126/1000 | Loss: 0.00001104
Iteration 127/1000 | Loss: 0.00001104
Iteration 128/1000 | Loss: 0.00001104
Iteration 129/1000 | Loss: 0.00001104
Iteration 130/1000 | Loss: 0.00001104
Iteration 131/1000 | Loss: 0.00001104
Iteration 132/1000 | Loss: 0.00001104
Iteration 133/1000 | Loss: 0.00001104
Iteration 134/1000 | Loss: 0.00001104
Iteration 135/1000 | Loss: 0.00001104
Iteration 136/1000 | Loss: 0.00001104
Iteration 137/1000 | Loss: 0.00001104
Iteration 138/1000 | Loss: 0.00001103
Iteration 139/1000 | Loss: 0.00001103
Iteration 140/1000 | Loss: 0.00001103
Iteration 141/1000 | Loss: 0.00001103
Iteration 142/1000 | Loss: 0.00001103
Iteration 143/1000 | Loss: 0.00001103
Iteration 144/1000 | Loss: 0.00001103
Iteration 145/1000 | Loss: 0.00001102
Iteration 146/1000 | Loss: 0.00001102
Iteration 147/1000 | Loss: 0.00001102
Iteration 148/1000 | Loss: 0.00001102
Iteration 149/1000 | Loss: 0.00001102
Iteration 150/1000 | Loss: 0.00001102
Iteration 151/1000 | Loss: 0.00001102
Iteration 152/1000 | Loss: 0.00001102
Iteration 153/1000 | Loss: 0.00001102
Iteration 154/1000 | Loss: 0.00001102
Iteration 155/1000 | Loss: 0.00001102
Iteration 156/1000 | Loss: 0.00001102
Iteration 157/1000 | Loss: 0.00001101
Iteration 158/1000 | Loss: 0.00001101
Iteration 159/1000 | Loss: 0.00001101
Iteration 160/1000 | Loss: 0.00001101
Iteration 161/1000 | Loss: 0.00001101
Iteration 162/1000 | Loss: 0.00001101
Iteration 163/1000 | Loss: 0.00001101
Iteration 164/1000 | Loss: 0.00001101
Iteration 165/1000 | Loss: 0.00001101
Iteration 166/1000 | Loss: 0.00001101
Iteration 167/1000 | Loss: 0.00001101
Iteration 168/1000 | Loss: 0.00001101
Iteration 169/1000 | Loss: 0.00001101
Iteration 170/1000 | Loss: 0.00001101
Iteration 171/1000 | Loss: 0.00001101
Iteration 172/1000 | Loss: 0.00001101
Iteration 173/1000 | Loss: 0.00001100
Iteration 174/1000 | Loss: 0.00001100
Iteration 175/1000 | Loss: 0.00001100
Iteration 176/1000 | Loss: 0.00001100
Iteration 177/1000 | Loss: 0.00001100
Iteration 178/1000 | Loss: 0.00001100
Iteration 179/1000 | Loss: 0.00001100
Iteration 180/1000 | Loss: 0.00001100
Iteration 181/1000 | Loss: 0.00001100
Iteration 182/1000 | Loss: 0.00001100
Iteration 183/1000 | Loss: 0.00001100
Iteration 184/1000 | Loss: 0.00001100
Iteration 185/1000 | Loss: 0.00001099
Iteration 186/1000 | Loss: 0.00001099
Iteration 187/1000 | Loss: 0.00001099
Iteration 188/1000 | Loss: 0.00001099
Iteration 189/1000 | Loss: 0.00001099
Iteration 190/1000 | Loss: 0.00001099
Iteration 191/1000 | Loss: 0.00001099
Iteration 192/1000 | Loss: 0.00001099
Iteration 193/1000 | Loss: 0.00001099
Iteration 194/1000 | Loss: 0.00001099
Iteration 195/1000 | Loss: 0.00001099
Iteration 196/1000 | Loss: 0.00001099
Iteration 197/1000 | Loss: 0.00001099
Iteration 198/1000 | Loss: 0.00001099
Iteration 199/1000 | Loss: 0.00001099
Iteration 200/1000 | Loss: 0.00001098
Iteration 201/1000 | Loss: 0.00001098
Iteration 202/1000 | Loss: 0.00001098
Iteration 203/1000 | Loss: 0.00001098
Iteration 204/1000 | Loss: 0.00001098
Iteration 205/1000 | Loss: 0.00001098
Iteration 206/1000 | Loss: 0.00001098
Iteration 207/1000 | Loss: 0.00001098
Iteration 208/1000 | Loss: 0.00001098
Iteration 209/1000 | Loss: 0.00001097
Iteration 210/1000 | Loss: 0.00001097
Iteration 211/1000 | Loss: 0.00001097
Iteration 212/1000 | Loss: 0.00001097
Iteration 213/1000 | Loss: 0.00001097
Iteration 214/1000 | Loss: 0.00001097
Iteration 215/1000 | Loss: 0.00001097
Iteration 216/1000 | Loss: 0.00001097
Iteration 217/1000 | Loss: 0.00001097
Iteration 218/1000 | Loss: 0.00001097
Iteration 219/1000 | Loss: 0.00001097
Iteration 220/1000 | Loss: 0.00001097
Iteration 221/1000 | Loss: 0.00001097
Iteration 222/1000 | Loss: 0.00001097
Iteration 223/1000 | Loss: 0.00001097
Iteration 224/1000 | Loss: 0.00001096
Iteration 225/1000 | Loss: 0.00001096
Iteration 226/1000 | Loss: 0.00001096
Iteration 227/1000 | Loss: 0.00001096
Iteration 228/1000 | Loss: 0.00001096
Iteration 229/1000 | Loss: 0.00001096
Iteration 230/1000 | Loss: 0.00001096
Iteration 231/1000 | Loss: 0.00001096
Iteration 232/1000 | Loss: 0.00001096
Iteration 233/1000 | Loss: 0.00001096
Iteration 234/1000 | Loss: 0.00001096
Iteration 235/1000 | Loss: 0.00001096
Iteration 236/1000 | Loss: 0.00001096
Iteration 237/1000 | Loss: 0.00001096
Iteration 238/1000 | Loss: 0.00001096
Iteration 239/1000 | Loss: 0.00001096
Iteration 240/1000 | Loss: 0.00001096
Iteration 241/1000 | Loss: 0.00001096
Iteration 242/1000 | Loss: 0.00001096
Iteration 243/1000 | Loss: 0.00001096
Iteration 244/1000 | Loss: 0.00001096
Iteration 245/1000 | Loss: 0.00001096
Iteration 246/1000 | Loss: 0.00001096
Iteration 247/1000 | Loss: 0.00001096
Iteration 248/1000 | Loss: 0.00001096
Iteration 249/1000 | Loss: 0.00001096
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 249. Stopping optimization.
Last 5 losses: [1.095971583708888e-05, 1.095971583708888e-05, 1.095971583708888e-05, 1.095971583708888e-05, 1.095971583708888e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.095971583708888e-05

Optimization complete. Final v2v error: 2.842519521713257 mm

Highest mean error: 3.569007396697998 mm for frame 89

Lowest mean error: 2.5291576385498047 mm for frame 41

Saving results

Total time: 44.59052658081055
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_015/1058/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_015/1058.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_015/1058
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00470000
Iteration 2/25 | Loss: 0.00127528
Iteration 3/25 | Loss: 0.00120086
Iteration 4/25 | Loss: 0.00118599
Iteration 5/25 | Loss: 0.00118195
Iteration 6/25 | Loss: 0.00118195
Iteration 7/25 | Loss: 0.00118195
Iteration 8/25 | Loss: 0.00118195
Iteration 9/25 | Loss: 0.00118195
Iteration 10/25 | Loss: 0.00118195
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0011819502105936408, 0.0011819502105936408, 0.0011819502105936408, 0.0011819502105936408, 0.0011819502105936408]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011819502105936408

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.30342221
Iteration 2/25 | Loss: 0.00135578
Iteration 3/25 | Loss: 0.00135577
Iteration 4/25 | Loss: 0.00135577
Iteration 5/25 | Loss: 0.00135577
Iteration 6/25 | Loss: 0.00135577
Iteration 7/25 | Loss: 0.00135577
Iteration 8/25 | Loss: 0.00135577
Iteration 9/25 | Loss: 0.00135577
Iteration 10/25 | Loss: 0.00135577
Iteration 11/25 | Loss: 0.00135577
Iteration 12/25 | Loss: 0.00135577
Iteration 13/25 | Loss: 0.00135577
Iteration 14/25 | Loss: 0.00135577
Iteration 15/25 | Loss: 0.00135577
Iteration 16/25 | Loss: 0.00135577
Iteration 17/25 | Loss: 0.00135577
Iteration 18/25 | Loss: 0.00135577
Iteration 19/25 | Loss: 0.00135577
Iteration 20/25 | Loss: 0.00135577
Iteration 21/25 | Loss: 0.00135577
Iteration 22/25 | Loss: 0.00135577
Iteration 23/25 | Loss: 0.00135577
Iteration 24/25 | Loss: 0.00135577
Iteration 25/25 | Loss: 0.00135577

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00135577
Iteration 2/1000 | Loss: 0.00001954
Iteration 3/1000 | Loss: 0.00001622
Iteration 4/1000 | Loss: 0.00001518
Iteration 5/1000 | Loss: 0.00001466
Iteration 6/1000 | Loss: 0.00001418
Iteration 7/1000 | Loss: 0.00001391
Iteration 8/1000 | Loss: 0.00001356
Iteration 9/1000 | Loss: 0.00001330
Iteration 10/1000 | Loss: 0.00001310
Iteration 11/1000 | Loss: 0.00001295
Iteration 12/1000 | Loss: 0.00001292
Iteration 13/1000 | Loss: 0.00001287
Iteration 14/1000 | Loss: 0.00001277
Iteration 15/1000 | Loss: 0.00001275
Iteration 16/1000 | Loss: 0.00001275
Iteration 17/1000 | Loss: 0.00001274
Iteration 18/1000 | Loss: 0.00001271
Iteration 19/1000 | Loss: 0.00001269
Iteration 20/1000 | Loss: 0.00001265
Iteration 21/1000 | Loss: 0.00001261
Iteration 22/1000 | Loss: 0.00001261
Iteration 23/1000 | Loss: 0.00001259
Iteration 24/1000 | Loss: 0.00001259
Iteration 25/1000 | Loss: 0.00001253
Iteration 26/1000 | Loss: 0.00001252
Iteration 27/1000 | Loss: 0.00001252
Iteration 28/1000 | Loss: 0.00001250
Iteration 29/1000 | Loss: 0.00001249
Iteration 30/1000 | Loss: 0.00001248
Iteration 31/1000 | Loss: 0.00001247
Iteration 32/1000 | Loss: 0.00001246
Iteration 33/1000 | Loss: 0.00001245
Iteration 34/1000 | Loss: 0.00001243
Iteration 35/1000 | Loss: 0.00001243
Iteration 36/1000 | Loss: 0.00001243
Iteration 37/1000 | Loss: 0.00001243
Iteration 38/1000 | Loss: 0.00001243
Iteration 39/1000 | Loss: 0.00001243
Iteration 40/1000 | Loss: 0.00001243
Iteration 41/1000 | Loss: 0.00001242
Iteration 42/1000 | Loss: 0.00001242
Iteration 43/1000 | Loss: 0.00001242
Iteration 44/1000 | Loss: 0.00001238
Iteration 45/1000 | Loss: 0.00001238
Iteration 46/1000 | Loss: 0.00001237
Iteration 47/1000 | Loss: 0.00001237
Iteration 48/1000 | Loss: 0.00001237
Iteration 49/1000 | Loss: 0.00001237
Iteration 50/1000 | Loss: 0.00001237
Iteration 51/1000 | Loss: 0.00001237
Iteration 52/1000 | Loss: 0.00001237
Iteration 53/1000 | Loss: 0.00001237
Iteration 54/1000 | Loss: 0.00001237
Iteration 55/1000 | Loss: 0.00001236
Iteration 56/1000 | Loss: 0.00001234
Iteration 57/1000 | Loss: 0.00001232
Iteration 58/1000 | Loss: 0.00001232
Iteration 59/1000 | Loss: 0.00001232
Iteration 60/1000 | Loss: 0.00001232
Iteration 61/1000 | Loss: 0.00001232
Iteration 62/1000 | Loss: 0.00001232
Iteration 63/1000 | Loss: 0.00001231
Iteration 64/1000 | Loss: 0.00001231
Iteration 65/1000 | Loss: 0.00001231
Iteration 66/1000 | Loss: 0.00001231
Iteration 67/1000 | Loss: 0.00001230
Iteration 68/1000 | Loss: 0.00001230
Iteration 69/1000 | Loss: 0.00001229
Iteration 70/1000 | Loss: 0.00001229
Iteration 71/1000 | Loss: 0.00001229
Iteration 72/1000 | Loss: 0.00001229
Iteration 73/1000 | Loss: 0.00001229
Iteration 74/1000 | Loss: 0.00001228
Iteration 75/1000 | Loss: 0.00001228
Iteration 76/1000 | Loss: 0.00001227
Iteration 77/1000 | Loss: 0.00001226
Iteration 78/1000 | Loss: 0.00001226
Iteration 79/1000 | Loss: 0.00001226
Iteration 80/1000 | Loss: 0.00001226
Iteration 81/1000 | Loss: 0.00001226
Iteration 82/1000 | Loss: 0.00001226
Iteration 83/1000 | Loss: 0.00001226
Iteration 84/1000 | Loss: 0.00001226
Iteration 85/1000 | Loss: 0.00001226
Iteration 86/1000 | Loss: 0.00001226
Iteration 87/1000 | Loss: 0.00001225
Iteration 88/1000 | Loss: 0.00001225
Iteration 89/1000 | Loss: 0.00001224
Iteration 90/1000 | Loss: 0.00001224
Iteration 91/1000 | Loss: 0.00001224
Iteration 92/1000 | Loss: 0.00001224
Iteration 93/1000 | Loss: 0.00001224
Iteration 94/1000 | Loss: 0.00001224
Iteration 95/1000 | Loss: 0.00001224
Iteration 96/1000 | Loss: 0.00001223
Iteration 97/1000 | Loss: 0.00001223
Iteration 98/1000 | Loss: 0.00001222
Iteration 99/1000 | Loss: 0.00001222
Iteration 100/1000 | Loss: 0.00001221
Iteration 101/1000 | Loss: 0.00001221
Iteration 102/1000 | Loss: 0.00001219
Iteration 103/1000 | Loss: 0.00001219
Iteration 104/1000 | Loss: 0.00001218
Iteration 105/1000 | Loss: 0.00001218
Iteration 106/1000 | Loss: 0.00001218
Iteration 107/1000 | Loss: 0.00001217
Iteration 108/1000 | Loss: 0.00001217
Iteration 109/1000 | Loss: 0.00001216
Iteration 110/1000 | Loss: 0.00001216
Iteration 111/1000 | Loss: 0.00001215
Iteration 112/1000 | Loss: 0.00001215
Iteration 113/1000 | Loss: 0.00001214
Iteration 114/1000 | Loss: 0.00001214
Iteration 115/1000 | Loss: 0.00001214
Iteration 116/1000 | Loss: 0.00001214
Iteration 117/1000 | Loss: 0.00001214
Iteration 118/1000 | Loss: 0.00001214
Iteration 119/1000 | Loss: 0.00001214
Iteration 120/1000 | Loss: 0.00001214
Iteration 121/1000 | Loss: 0.00001214
Iteration 122/1000 | Loss: 0.00001214
Iteration 123/1000 | Loss: 0.00001214
Iteration 124/1000 | Loss: 0.00001214
Iteration 125/1000 | Loss: 0.00001214
Iteration 126/1000 | Loss: 0.00001214
Iteration 127/1000 | Loss: 0.00001214
Iteration 128/1000 | Loss: 0.00001214
Iteration 129/1000 | Loss: 0.00001214
Iteration 130/1000 | Loss: 0.00001214
Iteration 131/1000 | Loss: 0.00001214
Iteration 132/1000 | Loss: 0.00001214
Iteration 133/1000 | Loss: 0.00001214
Iteration 134/1000 | Loss: 0.00001214
Iteration 135/1000 | Loss: 0.00001214
Iteration 136/1000 | Loss: 0.00001214
Iteration 137/1000 | Loss: 0.00001214
Iteration 138/1000 | Loss: 0.00001214
Iteration 139/1000 | Loss: 0.00001214
Iteration 140/1000 | Loss: 0.00001214
Iteration 141/1000 | Loss: 0.00001214
Iteration 142/1000 | Loss: 0.00001214
Iteration 143/1000 | Loss: 0.00001214
Iteration 144/1000 | Loss: 0.00001214
Iteration 145/1000 | Loss: 0.00001214
Iteration 146/1000 | Loss: 0.00001214
Iteration 147/1000 | Loss: 0.00001214
Iteration 148/1000 | Loss: 0.00001214
Iteration 149/1000 | Loss: 0.00001214
Iteration 150/1000 | Loss: 0.00001214
Iteration 151/1000 | Loss: 0.00001214
Iteration 152/1000 | Loss: 0.00001214
Iteration 153/1000 | Loss: 0.00001214
Iteration 154/1000 | Loss: 0.00001214
Iteration 155/1000 | Loss: 0.00001214
Iteration 156/1000 | Loss: 0.00001214
Iteration 157/1000 | Loss: 0.00001214
Iteration 158/1000 | Loss: 0.00001214
Iteration 159/1000 | Loss: 0.00001214
Iteration 160/1000 | Loss: 0.00001214
Iteration 161/1000 | Loss: 0.00001214
Iteration 162/1000 | Loss: 0.00001214
Iteration 163/1000 | Loss: 0.00001214
Iteration 164/1000 | Loss: 0.00001214
Iteration 165/1000 | Loss: 0.00001214
Iteration 166/1000 | Loss: 0.00001214
Iteration 167/1000 | Loss: 0.00001214
Iteration 168/1000 | Loss: 0.00001214
Iteration 169/1000 | Loss: 0.00001214
Iteration 170/1000 | Loss: 0.00001214
Iteration 171/1000 | Loss: 0.00001214
Iteration 172/1000 | Loss: 0.00001214
Iteration 173/1000 | Loss: 0.00001214
Iteration 174/1000 | Loss: 0.00001214
Iteration 175/1000 | Loss: 0.00001214
Iteration 176/1000 | Loss: 0.00001214
Iteration 177/1000 | Loss: 0.00001214
Iteration 178/1000 | Loss: 0.00001214
Iteration 179/1000 | Loss: 0.00001214
Iteration 180/1000 | Loss: 0.00001214
Iteration 181/1000 | Loss: 0.00001214
Iteration 182/1000 | Loss: 0.00001214
Iteration 183/1000 | Loss: 0.00001214
Iteration 184/1000 | Loss: 0.00001214
Iteration 185/1000 | Loss: 0.00001214
Iteration 186/1000 | Loss: 0.00001214
Iteration 187/1000 | Loss: 0.00001214
Iteration 188/1000 | Loss: 0.00001214
Iteration 189/1000 | Loss: 0.00001214
Iteration 190/1000 | Loss: 0.00001214
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 190. Stopping optimization.
Last 5 losses: [1.2140103535784874e-05, 1.2140103535784874e-05, 1.2140103535784874e-05, 1.2140103535784874e-05, 1.2140103535784874e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2140103535784874e-05

Optimization complete. Final v2v error: 2.9858505725860596 mm

Highest mean error: 3.344358444213867 mm for frame 141

Lowest mean error: 2.787158727645874 mm for frame 161

Saving results

Total time: 41.82882261276245
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_015/1091/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_015/1091.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_015/1091
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00413613
Iteration 2/25 | Loss: 0.00125579
Iteration 3/25 | Loss: 0.00118375
Iteration 4/25 | Loss: 0.00117780
Iteration 5/25 | Loss: 0.00117573
Iteration 6/25 | Loss: 0.00117566
Iteration 7/25 | Loss: 0.00117566
Iteration 8/25 | Loss: 0.00117566
Iteration 9/25 | Loss: 0.00117566
Iteration 10/25 | Loss: 0.00117566
Iteration 11/25 | Loss: 0.00117566
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011756605235859752, 0.0011756605235859752, 0.0011756605235859752, 0.0011756605235859752, 0.0011756605235859752]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011756605235859752

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.68207502
Iteration 2/25 | Loss: 0.00128385
Iteration 3/25 | Loss: 0.00128384
Iteration 4/25 | Loss: 0.00128384
Iteration 5/25 | Loss: 0.00128384
Iteration 6/25 | Loss: 0.00128384
Iteration 7/25 | Loss: 0.00128384
Iteration 8/25 | Loss: 0.00128384
Iteration 9/25 | Loss: 0.00128384
Iteration 10/25 | Loss: 0.00128384
Iteration 11/25 | Loss: 0.00128384
Iteration 12/25 | Loss: 0.00128384
Iteration 13/25 | Loss: 0.00128384
Iteration 14/25 | Loss: 0.00128384
Iteration 15/25 | Loss: 0.00128384
Iteration 16/25 | Loss: 0.00128384
Iteration 17/25 | Loss: 0.00128384
Iteration 18/25 | Loss: 0.00128384
Iteration 19/25 | Loss: 0.00128384
Iteration 20/25 | Loss: 0.00128384
Iteration 21/25 | Loss: 0.00128384
Iteration 22/25 | Loss: 0.00128384
Iteration 23/25 | Loss: 0.00128384
Iteration 24/25 | Loss: 0.00128384
Iteration 25/25 | Loss: 0.00128384

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00128384
Iteration 2/1000 | Loss: 0.00002234
Iteration 3/1000 | Loss: 0.00001593
Iteration 4/1000 | Loss: 0.00001451
Iteration 5/1000 | Loss: 0.00001356
Iteration 6/1000 | Loss: 0.00001298
Iteration 7/1000 | Loss: 0.00001252
Iteration 8/1000 | Loss: 0.00001221
Iteration 9/1000 | Loss: 0.00001196
Iteration 10/1000 | Loss: 0.00001173
Iteration 11/1000 | Loss: 0.00001161
Iteration 12/1000 | Loss: 0.00001154
Iteration 13/1000 | Loss: 0.00001144
Iteration 14/1000 | Loss: 0.00001139
Iteration 15/1000 | Loss: 0.00001138
Iteration 16/1000 | Loss: 0.00001134
Iteration 17/1000 | Loss: 0.00001131
Iteration 18/1000 | Loss: 0.00001131
Iteration 19/1000 | Loss: 0.00001130
Iteration 20/1000 | Loss: 0.00001129
Iteration 21/1000 | Loss: 0.00001125
Iteration 22/1000 | Loss: 0.00001124
Iteration 23/1000 | Loss: 0.00001122
Iteration 24/1000 | Loss: 0.00001122
Iteration 25/1000 | Loss: 0.00001122
Iteration 26/1000 | Loss: 0.00001121
Iteration 27/1000 | Loss: 0.00001120
Iteration 28/1000 | Loss: 0.00001118
Iteration 29/1000 | Loss: 0.00001118
Iteration 30/1000 | Loss: 0.00001118
Iteration 31/1000 | Loss: 0.00001117
Iteration 32/1000 | Loss: 0.00001117
Iteration 33/1000 | Loss: 0.00001117
Iteration 34/1000 | Loss: 0.00001114
Iteration 35/1000 | Loss: 0.00001114
Iteration 36/1000 | Loss: 0.00001112
Iteration 37/1000 | Loss: 0.00001111
Iteration 38/1000 | Loss: 0.00001108
Iteration 39/1000 | Loss: 0.00001107
Iteration 40/1000 | Loss: 0.00001106
Iteration 41/1000 | Loss: 0.00001105
Iteration 42/1000 | Loss: 0.00001102
Iteration 43/1000 | Loss: 0.00001102
Iteration 44/1000 | Loss: 0.00001101
Iteration 45/1000 | Loss: 0.00001100
Iteration 46/1000 | Loss: 0.00001100
Iteration 47/1000 | Loss: 0.00001099
Iteration 48/1000 | Loss: 0.00001099
Iteration 49/1000 | Loss: 0.00001098
Iteration 50/1000 | Loss: 0.00001097
Iteration 51/1000 | Loss: 0.00001096
Iteration 52/1000 | Loss: 0.00001096
Iteration 53/1000 | Loss: 0.00001096
Iteration 54/1000 | Loss: 0.00001095
Iteration 55/1000 | Loss: 0.00001095
Iteration 56/1000 | Loss: 0.00001095
Iteration 57/1000 | Loss: 0.00001095
Iteration 58/1000 | Loss: 0.00001095
Iteration 59/1000 | Loss: 0.00001095
Iteration 60/1000 | Loss: 0.00001095
Iteration 61/1000 | Loss: 0.00001095
Iteration 62/1000 | Loss: 0.00001095
Iteration 63/1000 | Loss: 0.00001095
Iteration 64/1000 | Loss: 0.00001095
Iteration 65/1000 | Loss: 0.00001095
Iteration 66/1000 | Loss: 0.00001094
Iteration 67/1000 | Loss: 0.00001094
Iteration 68/1000 | Loss: 0.00001094
Iteration 69/1000 | Loss: 0.00001094
Iteration 70/1000 | Loss: 0.00001094
Iteration 71/1000 | Loss: 0.00001094
Iteration 72/1000 | Loss: 0.00001094
Iteration 73/1000 | Loss: 0.00001094
Iteration 74/1000 | Loss: 0.00001094
Iteration 75/1000 | Loss: 0.00001094
Iteration 76/1000 | Loss: 0.00001094
Iteration 77/1000 | Loss: 0.00001093
Iteration 78/1000 | Loss: 0.00001092
Iteration 79/1000 | Loss: 0.00001092
Iteration 80/1000 | Loss: 0.00001091
Iteration 81/1000 | Loss: 0.00001091
Iteration 82/1000 | Loss: 0.00001090
Iteration 83/1000 | Loss: 0.00001090
Iteration 84/1000 | Loss: 0.00001090
Iteration 85/1000 | Loss: 0.00001089
Iteration 86/1000 | Loss: 0.00001089
Iteration 87/1000 | Loss: 0.00001088
Iteration 88/1000 | Loss: 0.00001088
Iteration 89/1000 | Loss: 0.00001087
Iteration 90/1000 | Loss: 0.00001087
Iteration 91/1000 | Loss: 0.00001087
Iteration 92/1000 | Loss: 0.00001086
Iteration 93/1000 | Loss: 0.00001086
Iteration 94/1000 | Loss: 0.00001086
Iteration 95/1000 | Loss: 0.00001085
Iteration 96/1000 | Loss: 0.00001085
Iteration 97/1000 | Loss: 0.00001085
Iteration 98/1000 | Loss: 0.00001084
Iteration 99/1000 | Loss: 0.00001084
Iteration 100/1000 | Loss: 0.00001084
Iteration 101/1000 | Loss: 0.00001084
Iteration 102/1000 | Loss: 0.00001084
Iteration 103/1000 | Loss: 0.00001084
Iteration 104/1000 | Loss: 0.00001084
Iteration 105/1000 | Loss: 0.00001084
Iteration 106/1000 | Loss: 0.00001083
Iteration 107/1000 | Loss: 0.00001083
Iteration 108/1000 | Loss: 0.00001083
Iteration 109/1000 | Loss: 0.00001083
Iteration 110/1000 | Loss: 0.00001083
Iteration 111/1000 | Loss: 0.00001083
Iteration 112/1000 | Loss: 0.00001083
Iteration 113/1000 | Loss: 0.00001083
Iteration 114/1000 | Loss: 0.00001082
Iteration 115/1000 | Loss: 0.00001082
Iteration 116/1000 | Loss: 0.00001081
Iteration 117/1000 | Loss: 0.00001081
Iteration 118/1000 | Loss: 0.00001081
Iteration 119/1000 | Loss: 0.00001080
Iteration 120/1000 | Loss: 0.00001080
Iteration 121/1000 | Loss: 0.00001080
Iteration 122/1000 | Loss: 0.00001080
Iteration 123/1000 | Loss: 0.00001080
Iteration 124/1000 | Loss: 0.00001080
Iteration 125/1000 | Loss: 0.00001079
Iteration 126/1000 | Loss: 0.00001079
Iteration 127/1000 | Loss: 0.00001079
Iteration 128/1000 | Loss: 0.00001079
Iteration 129/1000 | Loss: 0.00001079
Iteration 130/1000 | Loss: 0.00001079
Iteration 131/1000 | Loss: 0.00001079
Iteration 132/1000 | Loss: 0.00001078
Iteration 133/1000 | Loss: 0.00001078
Iteration 134/1000 | Loss: 0.00001078
Iteration 135/1000 | Loss: 0.00001078
Iteration 136/1000 | Loss: 0.00001077
Iteration 137/1000 | Loss: 0.00001077
Iteration 138/1000 | Loss: 0.00001077
Iteration 139/1000 | Loss: 0.00001077
Iteration 140/1000 | Loss: 0.00001077
Iteration 141/1000 | Loss: 0.00001077
Iteration 142/1000 | Loss: 0.00001077
Iteration 143/1000 | Loss: 0.00001077
Iteration 144/1000 | Loss: 0.00001077
Iteration 145/1000 | Loss: 0.00001077
Iteration 146/1000 | Loss: 0.00001077
Iteration 147/1000 | Loss: 0.00001077
Iteration 148/1000 | Loss: 0.00001076
Iteration 149/1000 | Loss: 0.00001076
Iteration 150/1000 | Loss: 0.00001076
Iteration 151/1000 | Loss: 0.00001076
Iteration 152/1000 | Loss: 0.00001076
Iteration 153/1000 | Loss: 0.00001076
Iteration 154/1000 | Loss: 0.00001076
Iteration 155/1000 | Loss: 0.00001076
Iteration 156/1000 | Loss: 0.00001076
Iteration 157/1000 | Loss: 0.00001076
Iteration 158/1000 | Loss: 0.00001075
Iteration 159/1000 | Loss: 0.00001075
Iteration 160/1000 | Loss: 0.00001075
Iteration 161/1000 | Loss: 0.00001075
Iteration 162/1000 | Loss: 0.00001075
Iteration 163/1000 | Loss: 0.00001075
Iteration 164/1000 | Loss: 0.00001075
Iteration 165/1000 | Loss: 0.00001075
Iteration 166/1000 | Loss: 0.00001075
Iteration 167/1000 | Loss: 0.00001075
Iteration 168/1000 | Loss: 0.00001075
Iteration 169/1000 | Loss: 0.00001075
Iteration 170/1000 | Loss: 0.00001075
Iteration 171/1000 | Loss: 0.00001075
Iteration 172/1000 | Loss: 0.00001075
Iteration 173/1000 | Loss: 0.00001075
Iteration 174/1000 | Loss: 0.00001075
Iteration 175/1000 | Loss: 0.00001075
Iteration 176/1000 | Loss: 0.00001074
Iteration 177/1000 | Loss: 0.00001074
Iteration 178/1000 | Loss: 0.00001074
Iteration 179/1000 | Loss: 0.00001074
Iteration 180/1000 | Loss: 0.00001074
Iteration 181/1000 | Loss: 0.00001074
Iteration 182/1000 | Loss: 0.00001074
Iteration 183/1000 | Loss: 0.00001074
Iteration 184/1000 | Loss: 0.00001074
Iteration 185/1000 | Loss: 0.00001074
Iteration 186/1000 | Loss: 0.00001074
Iteration 187/1000 | Loss: 0.00001074
Iteration 188/1000 | Loss: 0.00001074
Iteration 189/1000 | Loss: 0.00001073
Iteration 190/1000 | Loss: 0.00001073
Iteration 191/1000 | Loss: 0.00001073
Iteration 192/1000 | Loss: 0.00001073
Iteration 193/1000 | Loss: 0.00001073
Iteration 194/1000 | Loss: 0.00001073
Iteration 195/1000 | Loss: 0.00001073
Iteration 196/1000 | Loss: 0.00001073
Iteration 197/1000 | Loss: 0.00001073
Iteration 198/1000 | Loss: 0.00001073
Iteration 199/1000 | Loss: 0.00001073
Iteration 200/1000 | Loss: 0.00001073
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 200. Stopping optimization.
Last 5 losses: [1.073492876457749e-05, 1.073492876457749e-05, 1.073492876457749e-05, 1.073492876457749e-05, 1.073492876457749e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.073492876457749e-05

Optimization complete. Final v2v error: 2.8166069984436035 mm

Highest mean error: 3.447157621383667 mm for frame 86

Lowest mean error: 2.501499891281128 mm for frame 42

Saving results

Total time: 39.93414783477783
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_015/1007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_015/1007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_015/1007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00755610
Iteration 2/25 | Loss: 0.00127377
Iteration 3/25 | Loss: 0.00117600
Iteration 4/25 | Loss: 0.00116309
Iteration 5/25 | Loss: 0.00115966
Iteration 6/25 | Loss: 0.00115906
Iteration 7/25 | Loss: 0.00115906
Iteration 8/25 | Loss: 0.00115906
Iteration 9/25 | Loss: 0.00115906
Iteration 10/25 | Loss: 0.00115906
Iteration 11/25 | Loss: 0.00115906
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011590591166168451, 0.0011590591166168451, 0.0011590591166168451, 0.0011590591166168451, 0.0011590591166168451]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011590591166168451

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.84834516
Iteration 2/25 | Loss: 0.00138714
Iteration 3/25 | Loss: 0.00138714
Iteration 4/25 | Loss: 0.00138714
Iteration 5/25 | Loss: 0.00138714
Iteration 6/25 | Loss: 0.00138714
Iteration 7/25 | Loss: 0.00138714
Iteration 8/25 | Loss: 0.00138714
Iteration 9/25 | Loss: 0.00138714
Iteration 10/25 | Loss: 0.00138714
Iteration 11/25 | Loss: 0.00138713
Iteration 12/25 | Loss: 0.00138713
Iteration 13/25 | Loss: 0.00138713
Iteration 14/25 | Loss: 0.00138713
Iteration 15/25 | Loss: 0.00138713
Iteration 16/25 | Loss: 0.00138713
Iteration 17/25 | Loss: 0.00138713
Iteration 18/25 | Loss: 0.00138713
Iteration 19/25 | Loss: 0.00138713
Iteration 20/25 | Loss: 0.00138713
Iteration 21/25 | Loss: 0.00138713
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0013871346600353718, 0.0013871346600353718, 0.0013871346600353718, 0.0013871346600353718, 0.0013871346600353718]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013871346600353718

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00138713
Iteration 2/1000 | Loss: 0.00002121
Iteration 3/1000 | Loss: 0.00001587
Iteration 4/1000 | Loss: 0.00001399
Iteration 5/1000 | Loss: 0.00001280
Iteration 6/1000 | Loss: 0.00001215
Iteration 7/1000 | Loss: 0.00001152
Iteration 8/1000 | Loss: 0.00001108
Iteration 9/1000 | Loss: 0.00001088
Iteration 10/1000 | Loss: 0.00001052
Iteration 11/1000 | Loss: 0.00001036
Iteration 12/1000 | Loss: 0.00001029
Iteration 13/1000 | Loss: 0.00001015
Iteration 14/1000 | Loss: 0.00001012
Iteration 15/1000 | Loss: 0.00001009
Iteration 16/1000 | Loss: 0.00001008
Iteration 17/1000 | Loss: 0.00001007
Iteration 18/1000 | Loss: 0.00001006
Iteration 19/1000 | Loss: 0.00001005
Iteration 20/1000 | Loss: 0.00001004
Iteration 21/1000 | Loss: 0.00001004
Iteration 22/1000 | Loss: 0.00001003
Iteration 23/1000 | Loss: 0.00001003
Iteration 24/1000 | Loss: 0.00001000
Iteration 25/1000 | Loss: 0.00001000
Iteration 26/1000 | Loss: 0.00000999
Iteration 27/1000 | Loss: 0.00000998
Iteration 28/1000 | Loss: 0.00000997
Iteration 29/1000 | Loss: 0.00000997
Iteration 30/1000 | Loss: 0.00000996
Iteration 31/1000 | Loss: 0.00000990
Iteration 32/1000 | Loss: 0.00000985
Iteration 33/1000 | Loss: 0.00000984
Iteration 34/1000 | Loss: 0.00000983
Iteration 35/1000 | Loss: 0.00000980
Iteration 36/1000 | Loss: 0.00000980
Iteration 37/1000 | Loss: 0.00000979
Iteration 38/1000 | Loss: 0.00000975
Iteration 39/1000 | Loss: 0.00000972
Iteration 40/1000 | Loss: 0.00000970
Iteration 41/1000 | Loss: 0.00000969
Iteration 42/1000 | Loss: 0.00000969
Iteration 43/1000 | Loss: 0.00000968
Iteration 44/1000 | Loss: 0.00000968
Iteration 45/1000 | Loss: 0.00000968
Iteration 46/1000 | Loss: 0.00000968
Iteration 47/1000 | Loss: 0.00000967
Iteration 48/1000 | Loss: 0.00000967
Iteration 49/1000 | Loss: 0.00000967
Iteration 50/1000 | Loss: 0.00000966
Iteration 51/1000 | Loss: 0.00000966
Iteration 52/1000 | Loss: 0.00000965
Iteration 53/1000 | Loss: 0.00000965
Iteration 54/1000 | Loss: 0.00000965
Iteration 55/1000 | Loss: 0.00000964
Iteration 56/1000 | Loss: 0.00000964
Iteration 57/1000 | Loss: 0.00000964
Iteration 58/1000 | Loss: 0.00000964
Iteration 59/1000 | Loss: 0.00000964
Iteration 60/1000 | Loss: 0.00000964
Iteration 61/1000 | Loss: 0.00000964
Iteration 62/1000 | Loss: 0.00000964
Iteration 63/1000 | Loss: 0.00000964
Iteration 64/1000 | Loss: 0.00000963
Iteration 65/1000 | Loss: 0.00000963
Iteration 66/1000 | Loss: 0.00000963
Iteration 67/1000 | Loss: 0.00000963
Iteration 68/1000 | Loss: 0.00000963
Iteration 69/1000 | Loss: 0.00000963
Iteration 70/1000 | Loss: 0.00000963
Iteration 71/1000 | Loss: 0.00000961
Iteration 72/1000 | Loss: 0.00000961
Iteration 73/1000 | Loss: 0.00000961
Iteration 74/1000 | Loss: 0.00000961
Iteration 75/1000 | Loss: 0.00000960
Iteration 76/1000 | Loss: 0.00000960
Iteration 77/1000 | Loss: 0.00000960
Iteration 78/1000 | Loss: 0.00000960
Iteration 79/1000 | Loss: 0.00000960
Iteration 80/1000 | Loss: 0.00000960
Iteration 81/1000 | Loss: 0.00000959
Iteration 82/1000 | Loss: 0.00000959
Iteration 83/1000 | Loss: 0.00000959
Iteration 84/1000 | Loss: 0.00000959
Iteration 85/1000 | Loss: 0.00000958
Iteration 86/1000 | Loss: 0.00000958
Iteration 87/1000 | Loss: 0.00000958
Iteration 88/1000 | Loss: 0.00000958
Iteration 89/1000 | Loss: 0.00000958
Iteration 90/1000 | Loss: 0.00000957
Iteration 91/1000 | Loss: 0.00000957
Iteration 92/1000 | Loss: 0.00000956
Iteration 93/1000 | Loss: 0.00000956
Iteration 94/1000 | Loss: 0.00000956
Iteration 95/1000 | Loss: 0.00000956
Iteration 96/1000 | Loss: 0.00000955
Iteration 97/1000 | Loss: 0.00000955
Iteration 98/1000 | Loss: 0.00000954
Iteration 99/1000 | Loss: 0.00000954
Iteration 100/1000 | Loss: 0.00000954
Iteration 101/1000 | Loss: 0.00000954
Iteration 102/1000 | Loss: 0.00000954
Iteration 103/1000 | Loss: 0.00000953
Iteration 104/1000 | Loss: 0.00000953
Iteration 105/1000 | Loss: 0.00000953
Iteration 106/1000 | Loss: 0.00000953
Iteration 107/1000 | Loss: 0.00000953
Iteration 108/1000 | Loss: 0.00000953
Iteration 109/1000 | Loss: 0.00000952
Iteration 110/1000 | Loss: 0.00000952
Iteration 111/1000 | Loss: 0.00000952
Iteration 112/1000 | Loss: 0.00000952
Iteration 113/1000 | Loss: 0.00000952
Iteration 114/1000 | Loss: 0.00000952
Iteration 115/1000 | Loss: 0.00000952
Iteration 116/1000 | Loss: 0.00000952
Iteration 117/1000 | Loss: 0.00000952
Iteration 118/1000 | Loss: 0.00000952
Iteration 119/1000 | Loss: 0.00000951
Iteration 120/1000 | Loss: 0.00000951
Iteration 121/1000 | Loss: 0.00000951
Iteration 122/1000 | Loss: 0.00000951
Iteration 123/1000 | Loss: 0.00000951
Iteration 124/1000 | Loss: 0.00000950
Iteration 125/1000 | Loss: 0.00000950
Iteration 126/1000 | Loss: 0.00000950
Iteration 127/1000 | Loss: 0.00000950
Iteration 128/1000 | Loss: 0.00000950
Iteration 129/1000 | Loss: 0.00000950
Iteration 130/1000 | Loss: 0.00000950
Iteration 131/1000 | Loss: 0.00000949
Iteration 132/1000 | Loss: 0.00000949
Iteration 133/1000 | Loss: 0.00000949
Iteration 134/1000 | Loss: 0.00000948
Iteration 135/1000 | Loss: 0.00000948
Iteration 136/1000 | Loss: 0.00000948
Iteration 137/1000 | Loss: 0.00000948
Iteration 138/1000 | Loss: 0.00000948
Iteration 139/1000 | Loss: 0.00000948
Iteration 140/1000 | Loss: 0.00000948
Iteration 141/1000 | Loss: 0.00000948
Iteration 142/1000 | Loss: 0.00000948
Iteration 143/1000 | Loss: 0.00000948
Iteration 144/1000 | Loss: 0.00000948
Iteration 145/1000 | Loss: 0.00000948
Iteration 146/1000 | Loss: 0.00000948
Iteration 147/1000 | Loss: 0.00000948
Iteration 148/1000 | Loss: 0.00000948
Iteration 149/1000 | Loss: 0.00000947
Iteration 150/1000 | Loss: 0.00000947
Iteration 151/1000 | Loss: 0.00000947
Iteration 152/1000 | Loss: 0.00000947
Iteration 153/1000 | Loss: 0.00000947
Iteration 154/1000 | Loss: 0.00000947
Iteration 155/1000 | Loss: 0.00000947
Iteration 156/1000 | Loss: 0.00000947
Iteration 157/1000 | Loss: 0.00000947
Iteration 158/1000 | Loss: 0.00000946
Iteration 159/1000 | Loss: 0.00000946
Iteration 160/1000 | Loss: 0.00000946
Iteration 161/1000 | Loss: 0.00000946
Iteration 162/1000 | Loss: 0.00000945
Iteration 163/1000 | Loss: 0.00000945
Iteration 164/1000 | Loss: 0.00000945
Iteration 165/1000 | Loss: 0.00000945
Iteration 166/1000 | Loss: 0.00000945
Iteration 167/1000 | Loss: 0.00000945
Iteration 168/1000 | Loss: 0.00000945
Iteration 169/1000 | Loss: 0.00000945
Iteration 170/1000 | Loss: 0.00000945
Iteration 171/1000 | Loss: 0.00000945
Iteration 172/1000 | Loss: 0.00000944
Iteration 173/1000 | Loss: 0.00000944
Iteration 174/1000 | Loss: 0.00000944
Iteration 175/1000 | Loss: 0.00000944
Iteration 176/1000 | Loss: 0.00000944
Iteration 177/1000 | Loss: 0.00000943
Iteration 178/1000 | Loss: 0.00000943
Iteration 179/1000 | Loss: 0.00000943
Iteration 180/1000 | Loss: 0.00000943
Iteration 181/1000 | Loss: 0.00000942
Iteration 182/1000 | Loss: 0.00000942
Iteration 183/1000 | Loss: 0.00000942
Iteration 184/1000 | Loss: 0.00000942
Iteration 185/1000 | Loss: 0.00000942
Iteration 186/1000 | Loss: 0.00000942
Iteration 187/1000 | Loss: 0.00000942
Iteration 188/1000 | Loss: 0.00000942
Iteration 189/1000 | Loss: 0.00000942
Iteration 190/1000 | Loss: 0.00000942
Iteration 191/1000 | Loss: 0.00000942
Iteration 192/1000 | Loss: 0.00000941
Iteration 193/1000 | Loss: 0.00000941
Iteration 194/1000 | Loss: 0.00000941
Iteration 195/1000 | Loss: 0.00000941
Iteration 196/1000 | Loss: 0.00000941
Iteration 197/1000 | Loss: 0.00000941
Iteration 198/1000 | Loss: 0.00000941
Iteration 199/1000 | Loss: 0.00000941
Iteration 200/1000 | Loss: 0.00000941
Iteration 201/1000 | Loss: 0.00000941
Iteration 202/1000 | Loss: 0.00000941
Iteration 203/1000 | Loss: 0.00000941
Iteration 204/1000 | Loss: 0.00000941
Iteration 205/1000 | Loss: 0.00000940
Iteration 206/1000 | Loss: 0.00000940
Iteration 207/1000 | Loss: 0.00000940
Iteration 208/1000 | Loss: 0.00000940
Iteration 209/1000 | Loss: 0.00000939
Iteration 210/1000 | Loss: 0.00000939
Iteration 211/1000 | Loss: 0.00000939
Iteration 212/1000 | Loss: 0.00000939
Iteration 213/1000 | Loss: 0.00000939
Iteration 214/1000 | Loss: 0.00000939
Iteration 215/1000 | Loss: 0.00000939
Iteration 216/1000 | Loss: 0.00000939
Iteration 217/1000 | Loss: 0.00000939
Iteration 218/1000 | Loss: 0.00000939
Iteration 219/1000 | Loss: 0.00000939
Iteration 220/1000 | Loss: 0.00000939
Iteration 221/1000 | Loss: 0.00000939
Iteration 222/1000 | Loss: 0.00000939
Iteration 223/1000 | Loss: 0.00000939
Iteration 224/1000 | Loss: 0.00000939
Iteration 225/1000 | Loss: 0.00000939
Iteration 226/1000 | Loss: 0.00000939
Iteration 227/1000 | Loss: 0.00000938
Iteration 228/1000 | Loss: 0.00000938
Iteration 229/1000 | Loss: 0.00000938
Iteration 230/1000 | Loss: 0.00000938
Iteration 231/1000 | Loss: 0.00000938
Iteration 232/1000 | Loss: 0.00000938
Iteration 233/1000 | Loss: 0.00000938
Iteration 234/1000 | Loss: 0.00000938
Iteration 235/1000 | Loss: 0.00000938
Iteration 236/1000 | Loss: 0.00000938
Iteration 237/1000 | Loss: 0.00000938
Iteration 238/1000 | Loss: 0.00000938
Iteration 239/1000 | Loss: 0.00000938
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 239. Stopping optimization.
Last 5 losses: [9.380420124216471e-06, 9.380420124216471e-06, 9.380420124216471e-06, 9.380420124216471e-06, 9.380420124216471e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.380420124216471e-06

Optimization complete. Final v2v error: 2.677704095840454 mm

Highest mean error: 2.819002628326416 mm for frame 40

Lowest mean error: 2.561802864074707 mm for frame 3

Saving results

Total time: 45.85006070137024
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_015/1029/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_015/1029.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_015/1029
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00480649
Iteration 2/25 | Loss: 0.00140512
Iteration 3/25 | Loss: 0.00122611
Iteration 4/25 | Loss: 0.00120703
Iteration 5/25 | Loss: 0.00119946
Iteration 6/25 | Loss: 0.00119755
Iteration 7/25 | Loss: 0.00119755
Iteration 8/25 | Loss: 0.00119755
Iteration 9/25 | Loss: 0.00119755
Iteration 10/25 | Loss: 0.00119755
Iteration 11/25 | Loss: 0.00119755
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011975527741014957, 0.0011975527741014957, 0.0011975527741014957, 0.0011975527741014957, 0.0011975527741014957]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011975527741014957

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.75671959
Iteration 2/25 | Loss: 0.00133849
Iteration 3/25 | Loss: 0.00133849
Iteration 4/25 | Loss: 0.00133849
Iteration 5/25 | Loss: 0.00133849
Iteration 6/25 | Loss: 0.00133849
Iteration 7/25 | Loss: 0.00133849
Iteration 8/25 | Loss: 0.00133849
Iteration 9/25 | Loss: 0.00133849
Iteration 10/25 | Loss: 0.00133849
Iteration 11/25 | Loss: 0.00133849
Iteration 12/25 | Loss: 0.00133849
Iteration 13/25 | Loss: 0.00133849
Iteration 14/25 | Loss: 0.00133849
Iteration 15/25 | Loss: 0.00133849
Iteration 16/25 | Loss: 0.00133849
Iteration 17/25 | Loss: 0.00133849
Iteration 18/25 | Loss: 0.00133849
Iteration 19/25 | Loss: 0.00133849
Iteration 20/25 | Loss: 0.00133849
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0013384888879954815, 0.0013384888879954815, 0.0013384888879954815, 0.0013384888879954815, 0.0013384888879954815]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013384888879954815

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00133849
Iteration 2/1000 | Loss: 0.00003853
Iteration 3/1000 | Loss: 0.00002491
Iteration 4/1000 | Loss: 0.00002306
Iteration 5/1000 | Loss: 0.00002159
Iteration 6/1000 | Loss: 0.00002084
Iteration 7/1000 | Loss: 0.00002013
Iteration 8/1000 | Loss: 0.00001958
Iteration 9/1000 | Loss: 0.00001927
Iteration 10/1000 | Loss: 0.00001887
Iteration 11/1000 | Loss: 0.00001859
Iteration 12/1000 | Loss: 0.00001833
Iteration 13/1000 | Loss: 0.00001814
Iteration 14/1000 | Loss: 0.00001799
Iteration 15/1000 | Loss: 0.00001796
Iteration 16/1000 | Loss: 0.00001793
Iteration 17/1000 | Loss: 0.00001783
Iteration 18/1000 | Loss: 0.00001773
Iteration 19/1000 | Loss: 0.00001769
Iteration 20/1000 | Loss: 0.00001761
Iteration 21/1000 | Loss: 0.00001761
Iteration 22/1000 | Loss: 0.00001758
Iteration 23/1000 | Loss: 0.00001755
Iteration 24/1000 | Loss: 0.00001750
Iteration 25/1000 | Loss: 0.00001750
Iteration 26/1000 | Loss: 0.00001746
Iteration 27/1000 | Loss: 0.00001741
Iteration 28/1000 | Loss: 0.00001737
Iteration 29/1000 | Loss: 0.00001735
Iteration 30/1000 | Loss: 0.00001734
Iteration 31/1000 | Loss: 0.00001734
Iteration 32/1000 | Loss: 0.00001728
Iteration 33/1000 | Loss: 0.00001719
Iteration 34/1000 | Loss: 0.00001716
Iteration 35/1000 | Loss: 0.00001716
Iteration 36/1000 | Loss: 0.00001716
Iteration 37/1000 | Loss: 0.00001716
Iteration 38/1000 | Loss: 0.00001716
Iteration 39/1000 | Loss: 0.00001716
Iteration 40/1000 | Loss: 0.00001716
Iteration 41/1000 | Loss: 0.00001716
Iteration 42/1000 | Loss: 0.00001716
Iteration 43/1000 | Loss: 0.00001716
Iteration 44/1000 | Loss: 0.00001716
Iteration 45/1000 | Loss: 0.00001716
Iteration 46/1000 | Loss: 0.00001716
Iteration 47/1000 | Loss: 0.00001715
Iteration 48/1000 | Loss: 0.00001715
Iteration 49/1000 | Loss: 0.00001715
Iteration 50/1000 | Loss: 0.00001715
Iteration 51/1000 | Loss: 0.00001713
Iteration 52/1000 | Loss: 0.00001712
Iteration 53/1000 | Loss: 0.00001712
Iteration 54/1000 | Loss: 0.00001712
Iteration 55/1000 | Loss: 0.00001711
Iteration 56/1000 | Loss: 0.00001711
Iteration 57/1000 | Loss: 0.00001711
Iteration 58/1000 | Loss: 0.00001710
Iteration 59/1000 | Loss: 0.00001710
Iteration 60/1000 | Loss: 0.00001710
Iteration 61/1000 | Loss: 0.00001710
Iteration 62/1000 | Loss: 0.00001709
Iteration 63/1000 | Loss: 0.00001709
Iteration 64/1000 | Loss: 0.00001709
Iteration 65/1000 | Loss: 0.00001709
Iteration 66/1000 | Loss: 0.00001709
Iteration 67/1000 | Loss: 0.00001708
Iteration 68/1000 | Loss: 0.00001708
Iteration 69/1000 | Loss: 0.00001708
Iteration 70/1000 | Loss: 0.00001708
Iteration 71/1000 | Loss: 0.00001707
Iteration 72/1000 | Loss: 0.00001707
Iteration 73/1000 | Loss: 0.00001707
Iteration 74/1000 | Loss: 0.00001706
Iteration 75/1000 | Loss: 0.00001706
Iteration 76/1000 | Loss: 0.00001705
Iteration 77/1000 | Loss: 0.00001705
Iteration 78/1000 | Loss: 0.00001704
Iteration 79/1000 | Loss: 0.00001704
Iteration 80/1000 | Loss: 0.00001703
Iteration 81/1000 | Loss: 0.00001702
Iteration 82/1000 | Loss: 0.00001702
Iteration 83/1000 | Loss: 0.00001702
Iteration 84/1000 | Loss: 0.00001701
Iteration 85/1000 | Loss: 0.00001701
Iteration 86/1000 | Loss: 0.00001701
Iteration 87/1000 | Loss: 0.00001700
Iteration 88/1000 | Loss: 0.00001700
Iteration 89/1000 | Loss: 0.00001698
Iteration 90/1000 | Loss: 0.00001697
Iteration 91/1000 | Loss: 0.00001697
Iteration 92/1000 | Loss: 0.00001697
Iteration 93/1000 | Loss: 0.00001695
Iteration 94/1000 | Loss: 0.00001695
Iteration 95/1000 | Loss: 0.00001694
Iteration 96/1000 | Loss: 0.00001694
Iteration 97/1000 | Loss: 0.00001694
Iteration 98/1000 | Loss: 0.00001694
Iteration 99/1000 | Loss: 0.00001694
Iteration 100/1000 | Loss: 0.00001694
Iteration 101/1000 | Loss: 0.00001694
Iteration 102/1000 | Loss: 0.00001694
Iteration 103/1000 | Loss: 0.00001694
Iteration 104/1000 | Loss: 0.00001694
Iteration 105/1000 | Loss: 0.00001694
Iteration 106/1000 | Loss: 0.00001693
Iteration 107/1000 | Loss: 0.00001693
Iteration 108/1000 | Loss: 0.00001692
Iteration 109/1000 | Loss: 0.00001691
Iteration 110/1000 | Loss: 0.00001691
Iteration 111/1000 | Loss: 0.00001691
Iteration 112/1000 | Loss: 0.00001691
Iteration 113/1000 | Loss: 0.00001691
Iteration 114/1000 | Loss: 0.00001691
Iteration 115/1000 | Loss: 0.00001690
Iteration 116/1000 | Loss: 0.00001690
Iteration 117/1000 | Loss: 0.00001690
Iteration 118/1000 | Loss: 0.00001689
Iteration 119/1000 | Loss: 0.00001688
Iteration 120/1000 | Loss: 0.00001688
Iteration 121/1000 | Loss: 0.00001688
Iteration 122/1000 | Loss: 0.00001687
Iteration 123/1000 | Loss: 0.00001687
Iteration 124/1000 | Loss: 0.00001686
Iteration 125/1000 | Loss: 0.00001686
Iteration 126/1000 | Loss: 0.00001686
Iteration 127/1000 | Loss: 0.00001686
Iteration 128/1000 | Loss: 0.00001686
Iteration 129/1000 | Loss: 0.00001685
Iteration 130/1000 | Loss: 0.00001685
Iteration 131/1000 | Loss: 0.00001685
Iteration 132/1000 | Loss: 0.00001685
Iteration 133/1000 | Loss: 0.00001685
Iteration 134/1000 | Loss: 0.00001685
Iteration 135/1000 | Loss: 0.00001685
Iteration 136/1000 | Loss: 0.00001685
Iteration 137/1000 | Loss: 0.00001685
Iteration 138/1000 | Loss: 0.00001685
Iteration 139/1000 | Loss: 0.00001685
Iteration 140/1000 | Loss: 0.00001685
Iteration 141/1000 | Loss: 0.00001684
Iteration 142/1000 | Loss: 0.00001684
Iteration 143/1000 | Loss: 0.00001684
Iteration 144/1000 | Loss: 0.00001684
Iteration 145/1000 | Loss: 0.00001684
Iteration 146/1000 | Loss: 0.00001684
Iteration 147/1000 | Loss: 0.00001684
Iteration 148/1000 | Loss: 0.00001684
Iteration 149/1000 | Loss: 0.00001684
Iteration 150/1000 | Loss: 0.00001683
Iteration 151/1000 | Loss: 0.00001683
Iteration 152/1000 | Loss: 0.00001683
Iteration 153/1000 | Loss: 0.00001683
Iteration 154/1000 | Loss: 0.00001683
Iteration 155/1000 | Loss: 0.00001683
Iteration 156/1000 | Loss: 0.00001682
Iteration 157/1000 | Loss: 0.00001682
Iteration 158/1000 | Loss: 0.00001682
Iteration 159/1000 | Loss: 0.00001682
Iteration 160/1000 | Loss: 0.00001682
Iteration 161/1000 | Loss: 0.00001682
Iteration 162/1000 | Loss: 0.00001682
Iteration 163/1000 | Loss: 0.00001682
Iteration 164/1000 | Loss: 0.00001682
Iteration 165/1000 | Loss: 0.00001681
Iteration 166/1000 | Loss: 0.00001681
Iteration 167/1000 | Loss: 0.00001681
Iteration 168/1000 | Loss: 0.00001681
Iteration 169/1000 | Loss: 0.00001681
Iteration 170/1000 | Loss: 0.00001681
Iteration 171/1000 | Loss: 0.00001681
Iteration 172/1000 | Loss: 0.00001681
Iteration 173/1000 | Loss: 0.00001681
Iteration 174/1000 | Loss: 0.00001681
Iteration 175/1000 | Loss: 0.00001681
Iteration 176/1000 | Loss: 0.00001681
Iteration 177/1000 | Loss: 0.00001680
Iteration 178/1000 | Loss: 0.00001680
Iteration 179/1000 | Loss: 0.00001680
Iteration 180/1000 | Loss: 0.00001680
Iteration 181/1000 | Loss: 0.00001680
Iteration 182/1000 | Loss: 0.00001680
Iteration 183/1000 | Loss: 0.00001680
Iteration 184/1000 | Loss: 0.00001680
Iteration 185/1000 | Loss: 0.00001680
Iteration 186/1000 | Loss: 0.00001680
Iteration 187/1000 | Loss: 0.00001680
Iteration 188/1000 | Loss: 0.00001680
Iteration 189/1000 | Loss: 0.00001680
Iteration 190/1000 | Loss: 0.00001680
Iteration 191/1000 | Loss: 0.00001680
Iteration 192/1000 | Loss: 0.00001680
Iteration 193/1000 | Loss: 0.00001680
Iteration 194/1000 | Loss: 0.00001680
Iteration 195/1000 | Loss: 0.00001680
Iteration 196/1000 | Loss: 0.00001680
Iteration 197/1000 | Loss: 0.00001680
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 197. Stopping optimization.
Last 5 losses: [1.6804246115498245e-05, 1.6804246115498245e-05, 1.6804246115498245e-05, 1.6804246115498245e-05, 1.6804246115498245e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6804246115498245e-05

Optimization complete. Final v2v error: 3.478719711303711 mm

Highest mean error: 4.182411193847656 mm for frame 254

Lowest mean error: 3.294872283935547 mm for frame 31

Saving results

Total time: 58.0808527469635
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_015/1033/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_015/1033.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_015/1033
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00822996
Iteration 2/25 | Loss: 0.00205173
Iteration 3/25 | Loss: 0.00148859
Iteration 4/25 | Loss: 0.00141485
Iteration 5/25 | Loss: 0.00140899
Iteration 6/25 | Loss: 0.00140252
Iteration 7/25 | Loss: 0.00139998
Iteration 8/25 | Loss: 0.00139843
Iteration 9/25 | Loss: 0.00139815
Iteration 10/25 | Loss: 0.00139391
Iteration 11/25 | Loss: 0.00139292
Iteration 12/25 | Loss: 0.00139224
Iteration 13/25 | Loss: 0.00139197
Iteration 14/25 | Loss: 0.00139192
Iteration 15/25 | Loss: 0.00139192
Iteration 16/25 | Loss: 0.00139192
Iteration 17/25 | Loss: 0.00139192
Iteration 18/25 | Loss: 0.00139192
Iteration 19/25 | Loss: 0.00139192
Iteration 20/25 | Loss: 0.00139192
Iteration 21/25 | Loss: 0.00139192
Iteration 22/25 | Loss: 0.00139191
Iteration 23/25 | Loss: 0.00139191
Iteration 24/25 | Loss: 0.00139191
Iteration 25/25 | Loss: 0.00139191

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.13481891
Iteration 2/25 | Loss: 0.00111160
Iteration 3/25 | Loss: 0.00111157
Iteration 4/25 | Loss: 0.00111157
Iteration 5/25 | Loss: 0.00111157
Iteration 6/25 | Loss: 0.00111157
Iteration 7/25 | Loss: 0.00111157
Iteration 8/25 | Loss: 0.00111156
Iteration 9/25 | Loss: 0.00111156
Iteration 10/25 | Loss: 0.00111156
Iteration 11/25 | Loss: 0.00111156
Iteration 12/25 | Loss: 0.00111156
Iteration 13/25 | Loss: 0.00111156
Iteration 14/25 | Loss: 0.00111156
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0011115645756945014, 0.0011115645756945014, 0.0011115645756945014, 0.0011115645756945014, 0.0011115645756945014]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011115645756945014

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00111156
Iteration 2/1000 | Loss: 0.00004958
Iteration 3/1000 | Loss: 0.00003155
Iteration 4/1000 | Loss: 0.00002898
Iteration 5/1000 | Loss: 0.00002779
Iteration 6/1000 | Loss: 0.00002698
Iteration 7/1000 | Loss: 0.00002661
Iteration 8/1000 | Loss: 0.00002633
Iteration 9/1000 | Loss: 0.00002603
Iteration 10/1000 | Loss: 0.00002586
Iteration 11/1000 | Loss: 0.00002585
Iteration 12/1000 | Loss: 0.00002572
Iteration 13/1000 | Loss: 0.00002572
Iteration 14/1000 | Loss: 0.00002572
Iteration 15/1000 | Loss: 0.00002568
Iteration 16/1000 | Loss: 0.00002565
Iteration 17/1000 | Loss: 0.00002565
Iteration 18/1000 | Loss: 0.00002561
Iteration 19/1000 | Loss: 0.00002560
Iteration 20/1000 | Loss: 0.00002560
Iteration 21/1000 | Loss: 0.00002560
Iteration 22/1000 | Loss: 0.00002560
Iteration 23/1000 | Loss: 0.00002560
Iteration 24/1000 | Loss: 0.00002560
Iteration 25/1000 | Loss: 0.00002560
Iteration 26/1000 | Loss: 0.00002558
Iteration 27/1000 | Loss: 0.00002557
Iteration 28/1000 | Loss: 0.00002556
Iteration 29/1000 | Loss: 0.00002555
Iteration 30/1000 | Loss: 0.00002552
Iteration 31/1000 | Loss: 0.00002551
Iteration 32/1000 | Loss: 0.00002550
Iteration 33/1000 | Loss: 0.00002550
Iteration 34/1000 | Loss: 0.00002549
Iteration 35/1000 | Loss: 0.00002545
Iteration 36/1000 | Loss: 0.00002540
Iteration 37/1000 | Loss: 0.00002540
Iteration 38/1000 | Loss: 0.00002540
Iteration 39/1000 | Loss: 0.00002539
Iteration 40/1000 | Loss: 0.00002539
Iteration 41/1000 | Loss: 0.00002539
Iteration 42/1000 | Loss: 0.00002539
Iteration 43/1000 | Loss: 0.00002538
Iteration 44/1000 | Loss: 0.00002538
Iteration 45/1000 | Loss: 0.00002538
Iteration 46/1000 | Loss: 0.00002538
Iteration 47/1000 | Loss: 0.00002538
Iteration 48/1000 | Loss: 0.00002538
Iteration 49/1000 | Loss: 0.00002538
Iteration 50/1000 | Loss: 0.00002537
Iteration 51/1000 | Loss: 0.00002537
Iteration 52/1000 | Loss: 0.00002537
Iteration 53/1000 | Loss: 0.00002537
Iteration 54/1000 | Loss: 0.00002537
Iteration 55/1000 | Loss: 0.00002537
Iteration 56/1000 | Loss: 0.00002537
Iteration 57/1000 | Loss: 0.00002536
Iteration 58/1000 | Loss: 0.00002536
Iteration 59/1000 | Loss: 0.00002536
Iteration 60/1000 | Loss: 0.00002536
Iteration 61/1000 | Loss: 0.00002536
Iteration 62/1000 | Loss: 0.00002536
Iteration 63/1000 | Loss: 0.00002535
Iteration 64/1000 | Loss: 0.00002535
Iteration 65/1000 | Loss: 0.00002535
Iteration 66/1000 | Loss: 0.00002535
Iteration 67/1000 | Loss: 0.00002534
Iteration 68/1000 | Loss: 0.00002534
Iteration 69/1000 | Loss: 0.00002534
Iteration 70/1000 | Loss: 0.00002534
Iteration 71/1000 | Loss: 0.00002534
Iteration 72/1000 | Loss: 0.00002533
Iteration 73/1000 | Loss: 0.00002533
Iteration 74/1000 | Loss: 0.00002533
Iteration 75/1000 | Loss: 0.00002533
Iteration 76/1000 | Loss: 0.00002533
Iteration 77/1000 | Loss: 0.00002533
Iteration 78/1000 | Loss: 0.00002532
Iteration 79/1000 | Loss: 0.00002532
Iteration 80/1000 | Loss: 0.00002532
Iteration 81/1000 | Loss: 0.00002532
Iteration 82/1000 | Loss: 0.00002532
Iteration 83/1000 | Loss: 0.00002531
Iteration 84/1000 | Loss: 0.00002531
Iteration 85/1000 | Loss: 0.00002531
Iteration 86/1000 | Loss: 0.00002531
Iteration 87/1000 | Loss: 0.00002531
Iteration 88/1000 | Loss: 0.00002531
Iteration 89/1000 | Loss: 0.00002531
Iteration 90/1000 | Loss: 0.00002531
Iteration 91/1000 | Loss: 0.00002531
Iteration 92/1000 | Loss: 0.00002531
Iteration 93/1000 | Loss: 0.00002531
Iteration 94/1000 | Loss: 0.00002531
Iteration 95/1000 | Loss: 0.00002531
Iteration 96/1000 | Loss: 0.00002531
Iteration 97/1000 | Loss: 0.00002530
Iteration 98/1000 | Loss: 0.00002530
Iteration 99/1000 | Loss: 0.00002530
Iteration 100/1000 | Loss: 0.00002530
Iteration 101/1000 | Loss: 0.00002529
Iteration 102/1000 | Loss: 0.00002529
Iteration 103/1000 | Loss: 0.00002529
Iteration 104/1000 | Loss: 0.00002529
Iteration 105/1000 | Loss: 0.00002529
Iteration 106/1000 | Loss: 0.00002529
Iteration 107/1000 | Loss: 0.00002529
Iteration 108/1000 | Loss: 0.00002529
Iteration 109/1000 | Loss: 0.00002529
Iteration 110/1000 | Loss: 0.00002529
Iteration 111/1000 | Loss: 0.00002529
Iteration 112/1000 | Loss: 0.00002529
Iteration 113/1000 | Loss: 0.00002529
Iteration 114/1000 | Loss: 0.00002529
Iteration 115/1000 | Loss: 0.00002529
Iteration 116/1000 | Loss: 0.00002529
Iteration 117/1000 | Loss: 0.00002529
Iteration 118/1000 | Loss: 0.00002529
Iteration 119/1000 | Loss: 0.00002528
Iteration 120/1000 | Loss: 0.00002528
Iteration 121/1000 | Loss: 0.00002528
Iteration 122/1000 | Loss: 0.00002528
Iteration 123/1000 | Loss: 0.00002528
Iteration 124/1000 | Loss: 0.00002528
Iteration 125/1000 | Loss: 0.00002528
Iteration 126/1000 | Loss: 0.00002528
Iteration 127/1000 | Loss: 0.00002528
Iteration 128/1000 | Loss: 0.00002528
Iteration 129/1000 | Loss: 0.00002528
Iteration 130/1000 | Loss: 0.00002528
Iteration 131/1000 | Loss: 0.00002528
Iteration 132/1000 | Loss: 0.00002527
Iteration 133/1000 | Loss: 0.00002527
Iteration 134/1000 | Loss: 0.00002527
Iteration 135/1000 | Loss: 0.00002527
Iteration 136/1000 | Loss: 0.00002527
Iteration 137/1000 | Loss: 0.00002527
Iteration 138/1000 | Loss: 0.00002527
Iteration 139/1000 | Loss: 0.00002527
Iteration 140/1000 | Loss: 0.00002527
Iteration 141/1000 | Loss: 0.00002527
Iteration 142/1000 | Loss: 0.00002527
Iteration 143/1000 | Loss: 0.00002527
Iteration 144/1000 | Loss: 0.00002527
Iteration 145/1000 | Loss: 0.00002527
Iteration 146/1000 | Loss: 0.00002527
Iteration 147/1000 | Loss: 0.00002527
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 147. Stopping optimization.
Last 5 losses: [2.5270241167163476e-05, 2.5270241167163476e-05, 2.5270241167163476e-05, 2.5270241167163476e-05, 2.5270241167163476e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.5270241167163476e-05

Optimization complete. Final v2v error: 4.132510662078857 mm

Highest mean error: 4.337403774261475 mm for frame 171

Lowest mean error: 3.9775230884552 mm for frame 68

Saving results

Total time: 57.42618107795715
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_015/1036/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_015/1036.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_015/1036
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00810098
Iteration 2/25 | Loss: 0.00137262
Iteration 3/25 | Loss: 0.00122597
Iteration 4/25 | Loss: 0.00121323
Iteration 5/25 | Loss: 0.00121045
Iteration 6/25 | Loss: 0.00121045
Iteration 7/25 | Loss: 0.00121045
Iteration 8/25 | Loss: 0.00121045
Iteration 9/25 | Loss: 0.00121045
Iteration 10/25 | Loss: 0.00121045
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0012104541528970003, 0.0012104541528970003, 0.0012104541528970003, 0.0012104541528970003, 0.0012104541528970003]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012104541528970003

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.71285486
Iteration 2/25 | Loss: 0.00124109
Iteration 3/25 | Loss: 0.00124105
Iteration 4/25 | Loss: 0.00124105
Iteration 5/25 | Loss: 0.00124105
Iteration 6/25 | Loss: 0.00124105
Iteration 7/25 | Loss: 0.00124105
Iteration 8/25 | Loss: 0.00124105
Iteration 9/25 | Loss: 0.00124105
Iteration 10/25 | Loss: 0.00124105
Iteration 11/25 | Loss: 0.00124105
Iteration 12/25 | Loss: 0.00124105
Iteration 13/25 | Loss: 0.00124105
Iteration 14/25 | Loss: 0.00124105
Iteration 15/25 | Loss: 0.00124105
Iteration 16/25 | Loss: 0.00124105
Iteration 17/25 | Loss: 0.00124105
Iteration 18/25 | Loss: 0.00124105
Iteration 19/25 | Loss: 0.00124105
Iteration 20/25 | Loss: 0.00124105
Iteration 21/25 | Loss: 0.00124105
Iteration 22/25 | Loss: 0.00124105
Iteration 23/25 | Loss: 0.00124105
Iteration 24/25 | Loss: 0.00124105
Iteration 25/25 | Loss: 0.00124105

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00124105
Iteration 2/1000 | Loss: 0.00003110
Iteration 3/1000 | Loss: 0.00002278
Iteration 4/1000 | Loss: 0.00002110
Iteration 5/1000 | Loss: 0.00002017
Iteration 6/1000 | Loss: 0.00001928
Iteration 7/1000 | Loss: 0.00001874
Iteration 8/1000 | Loss: 0.00001827
Iteration 9/1000 | Loss: 0.00001776
Iteration 10/1000 | Loss: 0.00001746
Iteration 11/1000 | Loss: 0.00001723
Iteration 12/1000 | Loss: 0.00001703
Iteration 13/1000 | Loss: 0.00001686
Iteration 14/1000 | Loss: 0.00001679
Iteration 15/1000 | Loss: 0.00001676
Iteration 16/1000 | Loss: 0.00001675
Iteration 17/1000 | Loss: 0.00001660
Iteration 18/1000 | Loss: 0.00001656
Iteration 19/1000 | Loss: 0.00001656
Iteration 20/1000 | Loss: 0.00001655
Iteration 21/1000 | Loss: 0.00001647
Iteration 22/1000 | Loss: 0.00001641
Iteration 23/1000 | Loss: 0.00001640
Iteration 24/1000 | Loss: 0.00001638
Iteration 25/1000 | Loss: 0.00001637
Iteration 26/1000 | Loss: 0.00001636
Iteration 27/1000 | Loss: 0.00001635
Iteration 28/1000 | Loss: 0.00001635
Iteration 29/1000 | Loss: 0.00001633
Iteration 30/1000 | Loss: 0.00001624
Iteration 31/1000 | Loss: 0.00001621
Iteration 32/1000 | Loss: 0.00001621
Iteration 33/1000 | Loss: 0.00001616
Iteration 34/1000 | Loss: 0.00001616
Iteration 35/1000 | Loss: 0.00001615
Iteration 36/1000 | Loss: 0.00001614
Iteration 37/1000 | Loss: 0.00001614
Iteration 38/1000 | Loss: 0.00001613
Iteration 39/1000 | Loss: 0.00001612
Iteration 40/1000 | Loss: 0.00001612
Iteration 41/1000 | Loss: 0.00001611
Iteration 42/1000 | Loss: 0.00001610
Iteration 43/1000 | Loss: 0.00001610
Iteration 44/1000 | Loss: 0.00001610
Iteration 45/1000 | Loss: 0.00001610
Iteration 46/1000 | Loss: 0.00001610
Iteration 47/1000 | Loss: 0.00001610
Iteration 48/1000 | Loss: 0.00001610
Iteration 49/1000 | Loss: 0.00001609
Iteration 50/1000 | Loss: 0.00001609
Iteration 51/1000 | Loss: 0.00001608
Iteration 52/1000 | Loss: 0.00001608
Iteration 53/1000 | Loss: 0.00001608
Iteration 54/1000 | Loss: 0.00001607
Iteration 55/1000 | Loss: 0.00001607
Iteration 56/1000 | Loss: 0.00001607
Iteration 57/1000 | Loss: 0.00001607
Iteration 58/1000 | Loss: 0.00001606
Iteration 59/1000 | Loss: 0.00001606
Iteration 60/1000 | Loss: 0.00001606
Iteration 61/1000 | Loss: 0.00001606
Iteration 62/1000 | Loss: 0.00001606
Iteration 63/1000 | Loss: 0.00001606
Iteration 64/1000 | Loss: 0.00001606
Iteration 65/1000 | Loss: 0.00001606
Iteration 66/1000 | Loss: 0.00001605
Iteration 67/1000 | Loss: 0.00001605
Iteration 68/1000 | Loss: 0.00001605
Iteration 69/1000 | Loss: 0.00001605
Iteration 70/1000 | Loss: 0.00001605
Iteration 71/1000 | Loss: 0.00001605
Iteration 72/1000 | Loss: 0.00001605
Iteration 73/1000 | Loss: 0.00001605
Iteration 74/1000 | Loss: 0.00001605
Iteration 75/1000 | Loss: 0.00001605
Iteration 76/1000 | Loss: 0.00001605
Iteration 77/1000 | Loss: 0.00001604
Iteration 78/1000 | Loss: 0.00001604
Iteration 79/1000 | Loss: 0.00001604
Iteration 80/1000 | Loss: 0.00001604
Iteration 81/1000 | Loss: 0.00001604
Iteration 82/1000 | Loss: 0.00001604
Iteration 83/1000 | Loss: 0.00001604
Iteration 84/1000 | Loss: 0.00001604
Iteration 85/1000 | Loss: 0.00001604
Iteration 86/1000 | Loss: 0.00001604
Iteration 87/1000 | Loss: 0.00001604
Iteration 88/1000 | Loss: 0.00001604
Iteration 89/1000 | Loss: 0.00001604
Iteration 90/1000 | Loss: 0.00001604
Iteration 91/1000 | Loss: 0.00001604
Iteration 92/1000 | Loss: 0.00001604
Iteration 93/1000 | Loss: 0.00001604
Iteration 94/1000 | Loss: 0.00001604
Iteration 95/1000 | Loss: 0.00001604
Iteration 96/1000 | Loss: 0.00001604
Iteration 97/1000 | Loss: 0.00001604
Iteration 98/1000 | Loss: 0.00001604
Iteration 99/1000 | Loss: 0.00001604
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 99. Stopping optimization.
Last 5 losses: [1.6037502064136788e-05, 1.6037502064136788e-05, 1.6037502064136788e-05, 1.6037502064136788e-05, 1.6037502064136788e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6037502064136788e-05

Optimization complete. Final v2v error: 3.4130232334136963 mm

Highest mean error: 4.313181400299072 mm for frame 2

Lowest mean error: 2.8754281997680664 mm for frame 109

Saving results

Total time: 45.92978072166443
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_015/1090/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_015/1090.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_015/1090
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00500326
Iteration 2/25 | Loss: 0.00137464
Iteration 3/25 | Loss: 0.00125784
Iteration 4/25 | Loss: 0.00124572
Iteration 5/25 | Loss: 0.00124314
Iteration 6/25 | Loss: 0.00124231
Iteration 7/25 | Loss: 0.00124231
Iteration 8/25 | Loss: 0.00124231
Iteration 9/25 | Loss: 0.00124231
Iteration 10/25 | Loss: 0.00124231
Iteration 11/25 | Loss: 0.00124231
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001242309808731079, 0.001242309808731079, 0.001242309808731079, 0.001242309808731079, 0.001242309808731079]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001242309808731079

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35576081
Iteration 2/25 | Loss: 0.00128165
Iteration 3/25 | Loss: 0.00128161
Iteration 4/25 | Loss: 0.00128160
Iteration 5/25 | Loss: 0.00128160
Iteration 6/25 | Loss: 0.00128160
Iteration 7/25 | Loss: 0.00128160
Iteration 8/25 | Loss: 0.00128160
Iteration 9/25 | Loss: 0.00128160
Iteration 10/25 | Loss: 0.00128160
Iteration 11/25 | Loss: 0.00128160
Iteration 12/25 | Loss: 0.00128160
Iteration 13/25 | Loss: 0.00128160
Iteration 14/25 | Loss: 0.00128160
Iteration 15/25 | Loss: 0.00128160
Iteration 16/25 | Loss: 0.00128160
Iteration 17/25 | Loss: 0.00128160
Iteration 18/25 | Loss: 0.00128160
Iteration 19/25 | Loss: 0.00128160
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0012816019589081407, 0.0012816019589081407, 0.0012816019589081407, 0.0012816019589081407, 0.0012816019589081407]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012816019589081407

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00128160
Iteration 2/1000 | Loss: 0.00004930
Iteration 3/1000 | Loss: 0.00002872
Iteration 4/1000 | Loss: 0.00002182
Iteration 5/1000 | Loss: 0.00001997
Iteration 6/1000 | Loss: 0.00001881
Iteration 7/1000 | Loss: 0.00001828
Iteration 8/1000 | Loss: 0.00001780
Iteration 9/1000 | Loss: 0.00001731
Iteration 10/1000 | Loss: 0.00001698
Iteration 11/1000 | Loss: 0.00001683
Iteration 12/1000 | Loss: 0.00001677
Iteration 13/1000 | Loss: 0.00001671
Iteration 14/1000 | Loss: 0.00001652
Iteration 15/1000 | Loss: 0.00001638
Iteration 16/1000 | Loss: 0.00001636
Iteration 17/1000 | Loss: 0.00001634
Iteration 18/1000 | Loss: 0.00001634
Iteration 19/1000 | Loss: 0.00001633
Iteration 20/1000 | Loss: 0.00001633
Iteration 21/1000 | Loss: 0.00001632
Iteration 22/1000 | Loss: 0.00001627
Iteration 23/1000 | Loss: 0.00001625
Iteration 24/1000 | Loss: 0.00001620
Iteration 25/1000 | Loss: 0.00001613
Iteration 26/1000 | Loss: 0.00001610
Iteration 27/1000 | Loss: 0.00001609
Iteration 28/1000 | Loss: 0.00001608
Iteration 29/1000 | Loss: 0.00001608
Iteration 30/1000 | Loss: 0.00001606
Iteration 31/1000 | Loss: 0.00001605
Iteration 32/1000 | Loss: 0.00001602
Iteration 33/1000 | Loss: 0.00001601
Iteration 34/1000 | Loss: 0.00001601
Iteration 35/1000 | Loss: 0.00001600
Iteration 36/1000 | Loss: 0.00001600
Iteration 37/1000 | Loss: 0.00001599
Iteration 38/1000 | Loss: 0.00001599
Iteration 39/1000 | Loss: 0.00001599
Iteration 40/1000 | Loss: 0.00001598
Iteration 41/1000 | Loss: 0.00001598
Iteration 42/1000 | Loss: 0.00001597
Iteration 43/1000 | Loss: 0.00001594
Iteration 44/1000 | Loss: 0.00001594
Iteration 45/1000 | Loss: 0.00001594
Iteration 46/1000 | Loss: 0.00001594
Iteration 47/1000 | Loss: 0.00001594
Iteration 48/1000 | Loss: 0.00001594
Iteration 49/1000 | Loss: 0.00001594
Iteration 50/1000 | Loss: 0.00001594
Iteration 51/1000 | Loss: 0.00001592
Iteration 52/1000 | Loss: 0.00001591
Iteration 53/1000 | Loss: 0.00001591
Iteration 54/1000 | Loss: 0.00001590
Iteration 55/1000 | Loss: 0.00001590
Iteration 56/1000 | Loss: 0.00001590
Iteration 57/1000 | Loss: 0.00001590
Iteration 58/1000 | Loss: 0.00001589
Iteration 59/1000 | Loss: 0.00001589
Iteration 60/1000 | Loss: 0.00001589
Iteration 61/1000 | Loss: 0.00001589
Iteration 62/1000 | Loss: 0.00001589
Iteration 63/1000 | Loss: 0.00001589
Iteration 64/1000 | Loss: 0.00001588
Iteration 65/1000 | Loss: 0.00001588
Iteration 66/1000 | Loss: 0.00001588
Iteration 67/1000 | Loss: 0.00001588
Iteration 68/1000 | Loss: 0.00001588
Iteration 69/1000 | Loss: 0.00001587
Iteration 70/1000 | Loss: 0.00001587
Iteration 71/1000 | Loss: 0.00001586
Iteration 72/1000 | Loss: 0.00001586
Iteration 73/1000 | Loss: 0.00001585
Iteration 74/1000 | Loss: 0.00001585
Iteration 75/1000 | Loss: 0.00001585
Iteration 76/1000 | Loss: 0.00001585
Iteration 77/1000 | Loss: 0.00001584
Iteration 78/1000 | Loss: 0.00001584
Iteration 79/1000 | Loss: 0.00001584
Iteration 80/1000 | Loss: 0.00001584
Iteration 81/1000 | Loss: 0.00001583
Iteration 82/1000 | Loss: 0.00001583
Iteration 83/1000 | Loss: 0.00001583
Iteration 84/1000 | Loss: 0.00001583
Iteration 85/1000 | Loss: 0.00001583
Iteration 86/1000 | Loss: 0.00001583
Iteration 87/1000 | Loss: 0.00001582
Iteration 88/1000 | Loss: 0.00001582
Iteration 89/1000 | Loss: 0.00001582
Iteration 90/1000 | Loss: 0.00001582
Iteration 91/1000 | Loss: 0.00001581
Iteration 92/1000 | Loss: 0.00001581
Iteration 93/1000 | Loss: 0.00001581
Iteration 94/1000 | Loss: 0.00001581
Iteration 95/1000 | Loss: 0.00001581
Iteration 96/1000 | Loss: 0.00001581
Iteration 97/1000 | Loss: 0.00001580
Iteration 98/1000 | Loss: 0.00001580
Iteration 99/1000 | Loss: 0.00001580
Iteration 100/1000 | Loss: 0.00001580
Iteration 101/1000 | Loss: 0.00001580
Iteration 102/1000 | Loss: 0.00001580
Iteration 103/1000 | Loss: 0.00001579
Iteration 104/1000 | Loss: 0.00001579
Iteration 105/1000 | Loss: 0.00001579
Iteration 106/1000 | Loss: 0.00001578
Iteration 107/1000 | Loss: 0.00001578
Iteration 108/1000 | Loss: 0.00001578
Iteration 109/1000 | Loss: 0.00001578
Iteration 110/1000 | Loss: 0.00001577
Iteration 111/1000 | Loss: 0.00001577
Iteration 112/1000 | Loss: 0.00001577
Iteration 113/1000 | Loss: 0.00001577
Iteration 114/1000 | Loss: 0.00001576
Iteration 115/1000 | Loss: 0.00001576
Iteration 116/1000 | Loss: 0.00001576
Iteration 117/1000 | Loss: 0.00001576
Iteration 118/1000 | Loss: 0.00001576
Iteration 119/1000 | Loss: 0.00001576
Iteration 120/1000 | Loss: 0.00001576
Iteration 121/1000 | Loss: 0.00001575
Iteration 122/1000 | Loss: 0.00001575
Iteration 123/1000 | Loss: 0.00001575
Iteration 124/1000 | Loss: 0.00001575
Iteration 125/1000 | Loss: 0.00001575
Iteration 126/1000 | Loss: 0.00001574
Iteration 127/1000 | Loss: 0.00001574
Iteration 128/1000 | Loss: 0.00001574
Iteration 129/1000 | Loss: 0.00001574
Iteration 130/1000 | Loss: 0.00001574
Iteration 131/1000 | Loss: 0.00001574
Iteration 132/1000 | Loss: 0.00001574
Iteration 133/1000 | Loss: 0.00001574
Iteration 134/1000 | Loss: 0.00001574
Iteration 135/1000 | Loss: 0.00001574
Iteration 136/1000 | Loss: 0.00001574
Iteration 137/1000 | Loss: 0.00001574
Iteration 138/1000 | Loss: 0.00001574
Iteration 139/1000 | Loss: 0.00001574
Iteration 140/1000 | Loss: 0.00001573
Iteration 141/1000 | Loss: 0.00001573
Iteration 142/1000 | Loss: 0.00001573
Iteration 143/1000 | Loss: 0.00001573
Iteration 144/1000 | Loss: 0.00001573
Iteration 145/1000 | Loss: 0.00001573
Iteration 146/1000 | Loss: 0.00001573
Iteration 147/1000 | Loss: 0.00001573
Iteration 148/1000 | Loss: 0.00001573
Iteration 149/1000 | Loss: 0.00001573
Iteration 150/1000 | Loss: 0.00001573
Iteration 151/1000 | Loss: 0.00001573
Iteration 152/1000 | Loss: 0.00001573
Iteration 153/1000 | Loss: 0.00001573
Iteration 154/1000 | Loss: 0.00001572
Iteration 155/1000 | Loss: 0.00001572
Iteration 156/1000 | Loss: 0.00001572
Iteration 157/1000 | Loss: 0.00001572
Iteration 158/1000 | Loss: 0.00001572
Iteration 159/1000 | Loss: 0.00001572
Iteration 160/1000 | Loss: 0.00001572
Iteration 161/1000 | Loss: 0.00001572
Iteration 162/1000 | Loss: 0.00001572
Iteration 163/1000 | Loss: 0.00001572
Iteration 164/1000 | Loss: 0.00001572
Iteration 165/1000 | Loss: 0.00001572
Iteration 166/1000 | Loss: 0.00001572
Iteration 167/1000 | Loss: 0.00001572
Iteration 168/1000 | Loss: 0.00001572
Iteration 169/1000 | Loss: 0.00001572
Iteration 170/1000 | Loss: 0.00001572
Iteration 171/1000 | Loss: 0.00001572
Iteration 172/1000 | Loss: 0.00001571
Iteration 173/1000 | Loss: 0.00001571
Iteration 174/1000 | Loss: 0.00001571
Iteration 175/1000 | Loss: 0.00001571
Iteration 176/1000 | Loss: 0.00001571
Iteration 177/1000 | Loss: 0.00001571
Iteration 178/1000 | Loss: 0.00001571
Iteration 179/1000 | Loss: 0.00001571
Iteration 180/1000 | Loss: 0.00001571
Iteration 181/1000 | Loss: 0.00001571
Iteration 182/1000 | Loss: 0.00001571
Iteration 183/1000 | Loss: 0.00001571
Iteration 184/1000 | Loss: 0.00001571
Iteration 185/1000 | Loss: 0.00001571
Iteration 186/1000 | Loss: 0.00001571
Iteration 187/1000 | Loss: 0.00001571
Iteration 188/1000 | Loss: 0.00001571
Iteration 189/1000 | Loss: 0.00001571
Iteration 190/1000 | Loss: 0.00001571
Iteration 191/1000 | Loss: 0.00001571
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 191. Stopping optimization.
Last 5 losses: [1.5714666005806066e-05, 1.5714666005806066e-05, 1.5714666005806066e-05, 1.5714666005806066e-05, 1.5714666005806066e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5714666005806066e-05

Optimization complete. Final v2v error: 3.127902030944824 mm

Highest mean error: 5.063779830932617 mm for frame 60

Lowest mean error: 2.462700605392456 mm for frame 87

Saving results

Total time: 42.71885871887207
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_015/1072/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_015/1072.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_015/1072
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00469040
Iteration 2/25 | Loss: 0.00126408
Iteration 3/25 | Loss: 0.00117910
Iteration 4/25 | Loss: 0.00116490
Iteration 5/25 | Loss: 0.00116114
Iteration 6/25 | Loss: 0.00116114
Iteration 7/25 | Loss: 0.00116114
Iteration 8/25 | Loss: 0.00116114
Iteration 9/25 | Loss: 0.00116114
Iteration 10/25 | Loss: 0.00116114
Iteration 11/25 | Loss: 0.00116114
Iteration 12/25 | Loss: 0.00116114
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0011611414374783635, 0.0011611414374783635, 0.0011611414374783635, 0.0011611414374783635, 0.0011611414374783635]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011611414374783635

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.75543880
Iteration 2/25 | Loss: 0.00129900
Iteration 3/25 | Loss: 0.00129899
Iteration 4/25 | Loss: 0.00129899
Iteration 5/25 | Loss: 0.00129899
Iteration 6/25 | Loss: 0.00129899
Iteration 7/25 | Loss: 0.00129899
Iteration 8/25 | Loss: 0.00129899
Iteration 9/25 | Loss: 0.00129899
Iteration 10/25 | Loss: 0.00129899
Iteration 11/25 | Loss: 0.00129899
Iteration 12/25 | Loss: 0.00129899
Iteration 13/25 | Loss: 0.00129899
Iteration 14/25 | Loss: 0.00129899
Iteration 15/25 | Loss: 0.00129899
Iteration 16/25 | Loss: 0.00129899
Iteration 17/25 | Loss: 0.00129899
Iteration 18/25 | Loss: 0.00129899
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.001298986142501235, 0.001298986142501235, 0.001298986142501235, 0.001298986142501235, 0.001298986142501235]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001298986142501235

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00129899
Iteration 2/1000 | Loss: 0.00001761
Iteration 3/1000 | Loss: 0.00001435
Iteration 4/1000 | Loss: 0.00001331
Iteration 5/1000 | Loss: 0.00001245
Iteration 6/1000 | Loss: 0.00001199
Iteration 7/1000 | Loss: 0.00001177
Iteration 8/1000 | Loss: 0.00001129
Iteration 9/1000 | Loss: 0.00001105
Iteration 10/1000 | Loss: 0.00001095
Iteration 11/1000 | Loss: 0.00001089
Iteration 12/1000 | Loss: 0.00001069
Iteration 13/1000 | Loss: 0.00001053
Iteration 14/1000 | Loss: 0.00001036
Iteration 15/1000 | Loss: 0.00001021
Iteration 16/1000 | Loss: 0.00001020
Iteration 17/1000 | Loss: 0.00001018
Iteration 18/1000 | Loss: 0.00001017
Iteration 19/1000 | Loss: 0.00001016
Iteration 20/1000 | Loss: 0.00001015
Iteration 21/1000 | Loss: 0.00001015
Iteration 22/1000 | Loss: 0.00001014
Iteration 23/1000 | Loss: 0.00001013
Iteration 24/1000 | Loss: 0.00001013
Iteration 25/1000 | Loss: 0.00001012
Iteration 26/1000 | Loss: 0.00001011
Iteration 27/1000 | Loss: 0.00001004
Iteration 28/1000 | Loss: 0.00001001
Iteration 29/1000 | Loss: 0.00001001
Iteration 30/1000 | Loss: 0.00001000
Iteration 31/1000 | Loss: 0.00001000
Iteration 32/1000 | Loss: 0.00000998
Iteration 33/1000 | Loss: 0.00000996
Iteration 34/1000 | Loss: 0.00000996
Iteration 35/1000 | Loss: 0.00000995
Iteration 36/1000 | Loss: 0.00000995
Iteration 37/1000 | Loss: 0.00000993
Iteration 38/1000 | Loss: 0.00000993
Iteration 39/1000 | Loss: 0.00000992
Iteration 40/1000 | Loss: 0.00000991
Iteration 41/1000 | Loss: 0.00000991
Iteration 42/1000 | Loss: 0.00000986
Iteration 43/1000 | Loss: 0.00000984
Iteration 44/1000 | Loss: 0.00000983
Iteration 45/1000 | Loss: 0.00000980
Iteration 46/1000 | Loss: 0.00000979
Iteration 47/1000 | Loss: 0.00000977
Iteration 48/1000 | Loss: 0.00000977
Iteration 49/1000 | Loss: 0.00000977
Iteration 50/1000 | Loss: 0.00000977
Iteration 51/1000 | Loss: 0.00000977
Iteration 52/1000 | Loss: 0.00000977
Iteration 53/1000 | Loss: 0.00000977
Iteration 54/1000 | Loss: 0.00000976
Iteration 55/1000 | Loss: 0.00000976
Iteration 56/1000 | Loss: 0.00000974
Iteration 57/1000 | Loss: 0.00000974
Iteration 58/1000 | Loss: 0.00000973
Iteration 59/1000 | Loss: 0.00000972
Iteration 60/1000 | Loss: 0.00000972
Iteration 61/1000 | Loss: 0.00000971
Iteration 62/1000 | Loss: 0.00000971
Iteration 63/1000 | Loss: 0.00000971
Iteration 64/1000 | Loss: 0.00000971
Iteration 65/1000 | Loss: 0.00000971
Iteration 66/1000 | Loss: 0.00000971
Iteration 67/1000 | Loss: 0.00000970
Iteration 68/1000 | Loss: 0.00000970
Iteration 69/1000 | Loss: 0.00000969
Iteration 70/1000 | Loss: 0.00000969
Iteration 71/1000 | Loss: 0.00000969
Iteration 72/1000 | Loss: 0.00000969
Iteration 73/1000 | Loss: 0.00000969
Iteration 74/1000 | Loss: 0.00000968
Iteration 75/1000 | Loss: 0.00000968
Iteration 76/1000 | Loss: 0.00000967
Iteration 77/1000 | Loss: 0.00000967
Iteration 78/1000 | Loss: 0.00000967
Iteration 79/1000 | Loss: 0.00000967
Iteration 80/1000 | Loss: 0.00000967
Iteration 81/1000 | Loss: 0.00000967
Iteration 82/1000 | Loss: 0.00000966
Iteration 83/1000 | Loss: 0.00000965
Iteration 84/1000 | Loss: 0.00000965
Iteration 85/1000 | Loss: 0.00000965
Iteration 86/1000 | Loss: 0.00000964
Iteration 87/1000 | Loss: 0.00000964
Iteration 88/1000 | Loss: 0.00000964
Iteration 89/1000 | Loss: 0.00000964
Iteration 90/1000 | Loss: 0.00000964
Iteration 91/1000 | Loss: 0.00000964
Iteration 92/1000 | Loss: 0.00000964
Iteration 93/1000 | Loss: 0.00000964
Iteration 94/1000 | Loss: 0.00000964
Iteration 95/1000 | Loss: 0.00000963
Iteration 96/1000 | Loss: 0.00000963
Iteration 97/1000 | Loss: 0.00000963
Iteration 98/1000 | Loss: 0.00000963
Iteration 99/1000 | Loss: 0.00000963
Iteration 100/1000 | Loss: 0.00000963
Iteration 101/1000 | Loss: 0.00000963
Iteration 102/1000 | Loss: 0.00000963
Iteration 103/1000 | Loss: 0.00000963
Iteration 104/1000 | Loss: 0.00000963
Iteration 105/1000 | Loss: 0.00000963
Iteration 106/1000 | Loss: 0.00000963
Iteration 107/1000 | Loss: 0.00000963
Iteration 108/1000 | Loss: 0.00000963
Iteration 109/1000 | Loss: 0.00000963
Iteration 110/1000 | Loss: 0.00000963
Iteration 111/1000 | Loss: 0.00000963
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 111. Stopping optimization.
Last 5 losses: [9.628989573684521e-06, 9.628989573684521e-06, 9.628989573684521e-06, 9.628989573684521e-06, 9.628989573684521e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.628989573684521e-06

Optimization complete. Final v2v error: 2.720501661300659 mm

Highest mean error: 2.9460675716400146 mm for frame 111

Lowest mean error: 2.5577075481414795 mm for frame 2

Saving results

Total time: 43.58043456077576
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_015/1060/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_015/1060.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_015/1060
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01000990
Iteration 2/25 | Loss: 0.00197876
Iteration 3/25 | Loss: 0.00156908
Iteration 4/25 | Loss: 0.00140688
Iteration 5/25 | Loss: 0.00136038
Iteration 6/25 | Loss: 0.00136034
Iteration 7/25 | Loss: 0.00130834
Iteration 8/25 | Loss: 0.00128325
Iteration 9/25 | Loss: 0.00126720
Iteration 10/25 | Loss: 0.00125866
Iteration 11/25 | Loss: 0.00125531
Iteration 12/25 | Loss: 0.00125174
Iteration 13/25 | Loss: 0.00123710
Iteration 14/25 | Loss: 0.00123026
Iteration 15/25 | Loss: 0.00122680
Iteration 16/25 | Loss: 0.00122751
Iteration 17/25 | Loss: 0.00122645
Iteration 18/25 | Loss: 0.00121560
Iteration 19/25 | Loss: 0.00121311
Iteration 20/25 | Loss: 0.00121218
Iteration 21/25 | Loss: 0.00121211
Iteration 22/25 | Loss: 0.00121210
Iteration 23/25 | Loss: 0.00121209
Iteration 24/25 | Loss: 0.00121209
Iteration 25/25 | Loss: 0.00121208

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35358477
Iteration 2/25 | Loss: 0.00208939
Iteration 3/25 | Loss: 0.00148568
Iteration 4/25 | Loss: 0.00148568
Iteration 5/25 | Loss: 0.00148568
Iteration 6/25 | Loss: 0.00148568
Iteration 7/25 | Loss: 0.00148567
Iteration 8/25 | Loss: 0.00148567
Iteration 9/25 | Loss: 0.00148567
Iteration 10/25 | Loss: 0.00148567
Iteration 11/25 | Loss: 0.00148567
Iteration 12/25 | Loss: 0.00148567
Iteration 13/25 | Loss: 0.00148567
Iteration 14/25 | Loss: 0.00148567
Iteration 15/25 | Loss: 0.00148567
Iteration 16/25 | Loss: 0.00148567
Iteration 17/25 | Loss: 0.00148567
Iteration 18/25 | Loss: 0.00148567
Iteration 19/25 | Loss: 0.00148567
Iteration 20/25 | Loss: 0.00148567
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0014856734778732061, 0.0014856734778732061, 0.0014856734778732061, 0.0014856734778732061, 0.0014856734778732061]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014856734778732061

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00148567
Iteration 2/1000 | Loss: 0.00051936
Iteration 3/1000 | Loss: 0.00003695
Iteration 4/1000 | Loss: 0.00012430
Iteration 5/1000 | Loss: 0.00003153
Iteration 6/1000 | Loss: 0.00016362
Iteration 7/1000 | Loss: 0.00001816
Iteration 8/1000 | Loss: 0.00004488
Iteration 9/1000 | Loss: 0.00060386
Iteration 10/1000 | Loss: 0.00144253
Iteration 11/1000 | Loss: 0.00002619
Iteration 12/1000 | Loss: 0.00001684
Iteration 13/1000 | Loss: 0.00026249
Iteration 14/1000 | Loss: 0.00001497
Iteration 15/1000 | Loss: 0.00001373
Iteration 16/1000 | Loss: 0.00001297
Iteration 17/1000 | Loss: 0.00003333
Iteration 18/1000 | Loss: 0.00002967
Iteration 19/1000 | Loss: 0.00001230
Iteration 20/1000 | Loss: 0.00001224
Iteration 21/1000 | Loss: 0.00001221
Iteration 22/1000 | Loss: 0.00001211
Iteration 23/1000 | Loss: 0.00001199
Iteration 24/1000 | Loss: 0.00002088
Iteration 25/1000 | Loss: 0.00007771
Iteration 26/1000 | Loss: 0.00001187
Iteration 27/1000 | Loss: 0.00001161
Iteration 28/1000 | Loss: 0.00001159
Iteration 29/1000 | Loss: 0.00001158
Iteration 30/1000 | Loss: 0.00001154
Iteration 31/1000 | Loss: 0.00007290
Iteration 32/1000 | Loss: 0.00001159
Iteration 33/1000 | Loss: 0.00001147
Iteration 34/1000 | Loss: 0.00004554
Iteration 35/1000 | Loss: 0.00001739
Iteration 36/1000 | Loss: 0.00001155
Iteration 37/1000 | Loss: 0.00001909
Iteration 38/1000 | Loss: 0.00001154
Iteration 39/1000 | Loss: 0.00001145
Iteration 40/1000 | Loss: 0.00001143
Iteration 41/1000 | Loss: 0.00001143
Iteration 42/1000 | Loss: 0.00001142
Iteration 43/1000 | Loss: 0.00005993
Iteration 44/1000 | Loss: 0.00006479
Iteration 45/1000 | Loss: 0.00001138
Iteration 46/1000 | Loss: 0.00001136
Iteration 47/1000 | Loss: 0.00001135
Iteration 48/1000 | Loss: 0.00001135
Iteration 49/1000 | Loss: 0.00001134
Iteration 50/1000 | Loss: 0.00001447
Iteration 51/1000 | Loss: 0.00001164
Iteration 52/1000 | Loss: 0.00001134
Iteration 53/1000 | Loss: 0.00001134
Iteration 54/1000 | Loss: 0.00001134
Iteration 55/1000 | Loss: 0.00001134
Iteration 56/1000 | Loss: 0.00001134
Iteration 57/1000 | Loss: 0.00001134
Iteration 58/1000 | Loss: 0.00001134
Iteration 59/1000 | Loss: 0.00001134
Iteration 60/1000 | Loss: 0.00001134
Iteration 61/1000 | Loss: 0.00001134
Iteration 62/1000 | Loss: 0.00001134
Iteration 63/1000 | Loss: 0.00001134
Iteration 64/1000 | Loss: 0.00001134
Iteration 65/1000 | Loss: 0.00001134
Iteration 66/1000 | Loss: 0.00001134
Iteration 67/1000 | Loss: 0.00001134
Iteration 68/1000 | Loss: 0.00001134
Iteration 69/1000 | Loss: 0.00001134
Iteration 70/1000 | Loss: 0.00001134
Iteration 71/1000 | Loss: 0.00001134
Iteration 72/1000 | Loss: 0.00001134
Iteration 73/1000 | Loss: 0.00001134
Iteration 74/1000 | Loss: 0.00001134
Iteration 75/1000 | Loss: 0.00001134
Iteration 76/1000 | Loss: 0.00001134
Iteration 77/1000 | Loss: 0.00001134
Iteration 78/1000 | Loss: 0.00001134
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 78. Stopping optimization.
Last 5 losses: [1.1335321687511168e-05, 1.1335321687511168e-05, 1.1335321687511168e-05, 1.1335321687511168e-05, 1.1335321687511168e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1335321687511168e-05

Optimization complete. Final v2v error: 2.8430869579315186 mm

Highest mean error: 3.897186517715454 mm for frame 34

Lowest mean error: 2.448720693588257 mm for frame 59

Saving results

Total time: 89.88740372657776
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_015/1015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_015/1015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_015/1015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01004181
Iteration 2/25 | Loss: 0.00274086
Iteration 3/25 | Loss: 0.00251346
Iteration 4/25 | Loss: 0.00211206
Iteration 5/25 | Loss: 0.00196299
Iteration 6/25 | Loss: 0.00158061
Iteration 7/25 | Loss: 0.00146541
Iteration 8/25 | Loss: 0.00150901
Iteration 9/25 | Loss: 0.00147106
Iteration 10/25 | Loss: 0.00142959
Iteration 11/25 | Loss: 0.00136432
Iteration 12/25 | Loss: 0.00135757
Iteration 13/25 | Loss: 0.00135678
Iteration 14/25 | Loss: 0.00135044
Iteration 15/25 | Loss: 0.00136214
Iteration 16/25 | Loss: 0.00135602
Iteration 17/25 | Loss: 0.00134070
Iteration 18/25 | Loss: 0.00133891
Iteration 19/25 | Loss: 0.00133489
Iteration 20/25 | Loss: 0.00133617
Iteration 21/25 | Loss: 0.00133477
Iteration 22/25 | Loss: 0.00133308
Iteration 23/25 | Loss: 0.00133287
Iteration 24/25 | Loss: 0.00133595
Iteration 25/25 | Loss: 0.00133313

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.31154299
Iteration 2/25 | Loss: 0.00183379
Iteration 3/25 | Loss: 0.00152512
Iteration 4/25 | Loss: 0.00150497
Iteration 5/25 | Loss: 0.00168829
Iteration 6/25 | Loss: 0.00136986
Iteration 7/25 | Loss: 0.00130789
Iteration 8/25 | Loss: 0.00130789
Iteration 9/25 | Loss: 0.00130789
Iteration 10/25 | Loss: 0.00130789
Iteration 11/25 | Loss: 0.00130789
Iteration 12/25 | Loss: 0.00130789
Iteration 13/25 | Loss: 0.00130789
Iteration 14/25 | Loss: 0.00130789
Iteration 15/25 | Loss: 0.00130789
Iteration 16/25 | Loss: 0.00130789
Iteration 17/25 | Loss: 0.00130789
Iteration 18/25 | Loss: 0.00130789
Iteration 19/25 | Loss: 0.00130789
Iteration 20/25 | Loss: 0.00130789
Iteration 21/25 | Loss: 0.00130789
Iteration 22/25 | Loss: 0.00130789
Iteration 23/25 | Loss: 0.00130789
Iteration 24/25 | Loss: 0.00130789
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0013078892370685935, 0.0013078892370685935, 0.0013078892370685935, 0.0013078892370685935, 0.0013078892370685935]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013078892370685935

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00130789
Iteration 2/1000 | Loss: 0.00132315
Iteration 3/1000 | Loss: 0.00033853
Iteration 4/1000 | Loss: 0.00026290
Iteration 5/1000 | Loss: 0.00015201
Iteration 6/1000 | Loss: 0.00053733
Iteration 7/1000 | Loss: 0.00008853
Iteration 8/1000 | Loss: 0.00050506
Iteration 9/1000 | Loss: 0.00116134
Iteration 10/1000 | Loss: 0.00021574
Iteration 11/1000 | Loss: 0.00034888
Iteration 12/1000 | Loss: 0.00099393
Iteration 13/1000 | Loss: 0.00066151
Iteration 14/1000 | Loss: 0.00007621
Iteration 15/1000 | Loss: 0.00057535
Iteration 16/1000 | Loss: 0.00020250
Iteration 17/1000 | Loss: 0.00028333
Iteration 18/1000 | Loss: 0.00068229
Iteration 19/1000 | Loss: 0.00010553
Iteration 20/1000 | Loss: 0.00005001
Iteration 21/1000 | Loss: 0.00036084
Iteration 22/1000 | Loss: 0.00034962
Iteration 23/1000 | Loss: 0.00004710
Iteration 24/1000 | Loss: 0.00021977
Iteration 25/1000 | Loss: 0.00004928
Iteration 26/1000 | Loss: 0.00021654
Iteration 27/1000 | Loss: 0.00011607
Iteration 28/1000 | Loss: 0.00057537
Iteration 29/1000 | Loss: 0.00014452
Iteration 30/1000 | Loss: 0.00039248
Iteration 31/1000 | Loss: 0.00025552
Iteration 32/1000 | Loss: 0.00018800
Iteration 33/1000 | Loss: 0.00004232
Iteration 34/1000 | Loss: 0.00011844
Iteration 35/1000 | Loss: 0.00017133
Iteration 36/1000 | Loss: 0.00042740
Iteration 37/1000 | Loss: 0.00004860
Iteration 38/1000 | Loss: 0.00042698
Iteration 39/1000 | Loss: 0.00132741
Iteration 40/1000 | Loss: 0.00235903
Iteration 41/1000 | Loss: 0.00231188
Iteration 42/1000 | Loss: 0.00114421
Iteration 43/1000 | Loss: 0.00036854
Iteration 44/1000 | Loss: 0.00050683
Iteration 45/1000 | Loss: 0.00004012
Iteration 46/1000 | Loss: 0.00008021
Iteration 47/1000 | Loss: 0.00004660
Iteration 48/1000 | Loss: 0.00023291
Iteration 49/1000 | Loss: 0.00003634
Iteration 50/1000 | Loss: 0.00004817
Iteration 51/1000 | Loss: 0.00032365
Iteration 52/1000 | Loss: 0.00033541
Iteration 53/1000 | Loss: 0.00005173
Iteration 54/1000 | Loss: 0.00025993
Iteration 55/1000 | Loss: 0.00003743
Iteration 56/1000 | Loss: 0.00003447
Iteration 57/1000 | Loss: 0.00003317
Iteration 58/1000 | Loss: 0.00050473
Iteration 59/1000 | Loss: 0.00018871
Iteration 60/1000 | Loss: 0.00015299
Iteration 61/1000 | Loss: 0.00004513
Iteration 62/1000 | Loss: 0.00031243
Iteration 63/1000 | Loss: 0.00024668
Iteration 64/1000 | Loss: 0.00004787
Iteration 65/1000 | Loss: 0.00003904
Iteration 66/1000 | Loss: 0.00003422
Iteration 67/1000 | Loss: 0.00002941
Iteration 68/1000 | Loss: 0.00044253
Iteration 69/1000 | Loss: 0.00083599
Iteration 70/1000 | Loss: 0.00009618
Iteration 71/1000 | Loss: 0.00004226
Iteration 72/1000 | Loss: 0.00007015
Iteration 73/1000 | Loss: 0.00019981
Iteration 74/1000 | Loss: 0.00015428
Iteration 75/1000 | Loss: 0.00039991
Iteration 76/1000 | Loss: 0.00003194
Iteration 77/1000 | Loss: 0.00004810
Iteration 78/1000 | Loss: 0.00002654
Iteration 79/1000 | Loss: 0.00008982
Iteration 80/1000 | Loss: 0.00016990
Iteration 81/1000 | Loss: 0.00006737
Iteration 82/1000 | Loss: 0.00003059
Iteration 83/1000 | Loss: 0.00002371
Iteration 84/1000 | Loss: 0.00012013
Iteration 85/1000 | Loss: 0.00002333
Iteration 86/1000 | Loss: 0.00002264
Iteration 87/1000 | Loss: 0.00016368
Iteration 88/1000 | Loss: 0.00002324
Iteration 89/1000 | Loss: 0.00002166
Iteration 90/1000 | Loss: 0.00002098
Iteration 91/1000 | Loss: 0.00002066
Iteration 92/1000 | Loss: 0.00002065
Iteration 93/1000 | Loss: 0.00002061
Iteration 94/1000 | Loss: 0.00002060
Iteration 95/1000 | Loss: 0.00002053
Iteration 96/1000 | Loss: 0.00002053
Iteration 97/1000 | Loss: 0.00002053
Iteration 98/1000 | Loss: 0.00002053
Iteration 99/1000 | Loss: 0.00002053
Iteration 100/1000 | Loss: 0.00002053
Iteration 101/1000 | Loss: 0.00002053
Iteration 102/1000 | Loss: 0.00002053
Iteration 103/1000 | Loss: 0.00002053
Iteration 104/1000 | Loss: 0.00002053
Iteration 105/1000 | Loss: 0.00002052
Iteration 106/1000 | Loss: 0.00002052
Iteration 107/1000 | Loss: 0.00002051
Iteration 108/1000 | Loss: 0.00002051
Iteration 109/1000 | Loss: 0.00002050
Iteration 110/1000 | Loss: 0.00002050
Iteration 111/1000 | Loss: 0.00002049
Iteration 112/1000 | Loss: 0.00002049
Iteration 113/1000 | Loss: 0.00002048
Iteration 114/1000 | Loss: 0.00002048
Iteration 115/1000 | Loss: 0.00002048
Iteration 116/1000 | Loss: 0.00002048
Iteration 117/1000 | Loss: 0.00002047
Iteration 118/1000 | Loss: 0.00002047
Iteration 119/1000 | Loss: 0.00002047
Iteration 120/1000 | Loss: 0.00002047
Iteration 121/1000 | Loss: 0.00002047
Iteration 122/1000 | Loss: 0.00002046
Iteration 123/1000 | Loss: 0.00002046
Iteration 124/1000 | Loss: 0.00002045
Iteration 125/1000 | Loss: 0.00002045
Iteration 126/1000 | Loss: 0.00002044
Iteration 127/1000 | Loss: 0.00002044
Iteration 128/1000 | Loss: 0.00002044
Iteration 129/1000 | Loss: 0.00002041
Iteration 130/1000 | Loss: 0.00002040
Iteration 131/1000 | Loss: 0.00002040
Iteration 132/1000 | Loss: 0.00002040
Iteration 133/1000 | Loss: 0.00002039
Iteration 134/1000 | Loss: 0.00002039
Iteration 135/1000 | Loss: 0.00002039
Iteration 136/1000 | Loss: 0.00002039
Iteration 137/1000 | Loss: 0.00002039
Iteration 138/1000 | Loss: 0.00002039
Iteration 139/1000 | Loss: 0.00002039
Iteration 140/1000 | Loss: 0.00002038
Iteration 141/1000 | Loss: 0.00002038
Iteration 142/1000 | Loss: 0.00002038
Iteration 143/1000 | Loss: 0.00002038
Iteration 144/1000 | Loss: 0.00002038
Iteration 145/1000 | Loss: 0.00002038
Iteration 146/1000 | Loss: 0.00002038
Iteration 147/1000 | Loss: 0.00002037
Iteration 148/1000 | Loss: 0.00002037
Iteration 149/1000 | Loss: 0.00002037
Iteration 150/1000 | Loss: 0.00002037
Iteration 151/1000 | Loss: 0.00002036
Iteration 152/1000 | Loss: 0.00002036
Iteration 153/1000 | Loss: 0.00002036
Iteration 154/1000 | Loss: 0.00002036
Iteration 155/1000 | Loss: 0.00002035
Iteration 156/1000 | Loss: 0.00002035
Iteration 157/1000 | Loss: 0.00002035
Iteration 158/1000 | Loss: 0.00002035
Iteration 159/1000 | Loss: 0.00002035
Iteration 160/1000 | Loss: 0.00002034
Iteration 161/1000 | Loss: 0.00002034
Iteration 162/1000 | Loss: 0.00002034
Iteration 163/1000 | Loss: 0.00002033
Iteration 164/1000 | Loss: 0.00002033
Iteration 165/1000 | Loss: 0.00002032
Iteration 166/1000 | Loss: 0.00002031
Iteration 167/1000 | Loss: 0.00002031
Iteration 168/1000 | Loss: 0.00002031
Iteration 169/1000 | Loss: 0.00002031
Iteration 170/1000 | Loss: 0.00002031
Iteration 171/1000 | Loss: 0.00002031
Iteration 172/1000 | Loss: 0.00002031
Iteration 173/1000 | Loss: 0.00002031
Iteration 174/1000 | Loss: 0.00002030
Iteration 175/1000 | Loss: 0.00002030
Iteration 176/1000 | Loss: 0.00002030
Iteration 177/1000 | Loss: 0.00002030
Iteration 178/1000 | Loss: 0.00002029
Iteration 179/1000 | Loss: 0.00002029
Iteration 180/1000 | Loss: 0.00002029
Iteration 181/1000 | Loss: 0.00002029
Iteration 182/1000 | Loss: 0.00002028
Iteration 183/1000 | Loss: 0.00002028
Iteration 184/1000 | Loss: 0.00002028
Iteration 185/1000 | Loss: 0.00002028
Iteration 186/1000 | Loss: 0.00002028
Iteration 187/1000 | Loss: 0.00002028
Iteration 188/1000 | Loss: 0.00002028
Iteration 189/1000 | Loss: 0.00002028
Iteration 190/1000 | Loss: 0.00002028
Iteration 191/1000 | Loss: 0.00002028
Iteration 192/1000 | Loss: 0.00002028
Iteration 193/1000 | Loss: 0.00002027
Iteration 194/1000 | Loss: 0.00002027
Iteration 195/1000 | Loss: 0.00002027
Iteration 196/1000 | Loss: 0.00002027
Iteration 197/1000 | Loss: 0.00002027
Iteration 198/1000 | Loss: 0.00002027
Iteration 199/1000 | Loss: 0.00002027
Iteration 200/1000 | Loss: 0.00002027
Iteration 201/1000 | Loss: 0.00002026
Iteration 202/1000 | Loss: 0.00002026
Iteration 203/1000 | Loss: 0.00002026
Iteration 204/1000 | Loss: 0.00002026
Iteration 205/1000 | Loss: 0.00002026
Iteration 206/1000 | Loss: 0.00002026
Iteration 207/1000 | Loss: 0.00002026
Iteration 208/1000 | Loss: 0.00002026
Iteration 209/1000 | Loss: 0.00002026
Iteration 210/1000 | Loss: 0.00002026
Iteration 211/1000 | Loss: 0.00002026
Iteration 212/1000 | Loss: 0.00002026
Iteration 213/1000 | Loss: 0.00002026
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 213. Stopping optimization.
Last 5 losses: [2.0262317775632255e-05, 2.0262317775632255e-05, 2.0262317775632255e-05, 2.0262317775632255e-05, 2.0262317775632255e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0262317775632255e-05

Optimization complete. Final v2v error: 3.7657480239868164 mm

Highest mean error: 4.113046169281006 mm for frame 100

Lowest mean error: 3.421501636505127 mm for frame 13

Saving results

Total time: 202.95772337913513
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_015/1098/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_015/1098.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_015/1098
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00496271
Iteration 2/25 | Loss: 0.00121747
Iteration 3/25 | Loss: 0.00116014
Iteration 4/25 | Loss: 0.00115116
Iteration 5/25 | Loss: 0.00114823
Iteration 6/25 | Loss: 0.00114753
Iteration 7/25 | Loss: 0.00114753
Iteration 8/25 | Loss: 0.00114753
Iteration 9/25 | Loss: 0.00114753
Iteration 10/25 | Loss: 0.00114753
Iteration 11/25 | Loss: 0.00114753
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011475315550342202, 0.0011475315550342202, 0.0011475315550342202, 0.0011475315550342202, 0.0011475315550342202]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011475315550342202

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43438840
Iteration 2/25 | Loss: 0.00126975
Iteration 3/25 | Loss: 0.00126974
Iteration 4/25 | Loss: 0.00126974
Iteration 5/25 | Loss: 0.00126974
Iteration 6/25 | Loss: 0.00126974
Iteration 7/25 | Loss: 0.00126974
Iteration 8/25 | Loss: 0.00126974
Iteration 9/25 | Loss: 0.00126974
Iteration 10/25 | Loss: 0.00126974
Iteration 11/25 | Loss: 0.00126974
Iteration 12/25 | Loss: 0.00126974
Iteration 13/25 | Loss: 0.00126974
Iteration 14/25 | Loss: 0.00126974
Iteration 15/25 | Loss: 0.00126974
Iteration 16/25 | Loss: 0.00126974
Iteration 17/25 | Loss: 0.00126974
Iteration 18/25 | Loss: 0.00126974
Iteration 19/25 | Loss: 0.00126974
Iteration 20/25 | Loss: 0.00126974
Iteration 21/25 | Loss: 0.00126974
Iteration 22/25 | Loss: 0.00126974
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0012697391211986542, 0.0012697391211986542, 0.0012697391211986542, 0.0012697391211986542, 0.0012697391211986542]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012697391211986542

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00126974
Iteration 2/1000 | Loss: 0.00002156
Iteration 3/1000 | Loss: 0.00001589
Iteration 4/1000 | Loss: 0.00001390
Iteration 5/1000 | Loss: 0.00001292
Iteration 6/1000 | Loss: 0.00001223
Iteration 7/1000 | Loss: 0.00001173
Iteration 8/1000 | Loss: 0.00001122
Iteration 9/1000 | Loss: 0.00001090
Iteration 10/1000 | Loss: 0.00001061
Iteration 11/1000 | Loss: 0.00001039
Iteration 12/1000 | Loss: 0.00001030
Iteration 13/1000 | Loss: 0.00001020
Iteration 14/1000 | Loss: 0.00001015
Iteration 15/1000 | Loss: 0.00001009
Iteration 16/1000 | Loss: 0.00001007
Iteration 17/1000 | Loss: 0.00001006
Iteration 18/1000 | Loss: 0.00001003
Iteration 19/1000 | Loss: 0.00001000
Iteration 20/1000 | Loss: 0.00001000
Iteration 21/1000 | Loss: 0.00000999
Iteration 22/1000 | Loss: 0.00000998
Iteration 23/1000 | Loss: 0.00000991
Iteration 24/1000 | Loss: 0.00000991
Iteration 25/1000 | Loss: 0.00000987
Iteration 26/1000 | Loss: 0.00000985
Iteration 27/1000 | Loss: 0.00000980
Iteration 28/1000 | Loss: 0.00000976
Iteration 29/1000 | Loss: 0.00000973
Iteration 30/1000 | Loss: 0.00000973
Iteration 31/1000 | Loss: 0.00000973
Iteration 32/1000 | Loss: 0.00000972
Iteration 33/1000 | Loss: 0.00000972
Iteration 34/1000 | Loss: 0.00000970
Iteration 35/1000 | Loss: 0.00000969
Iteration 36/1000 | Loss: 0.00000968
Iteration 37/1000 | Loss: 0.00000968
Iteration 38/1000 | Loss: 0.00000963
Iteration 39/1000 | Loss: 0.00000963
Iteration 40/1000 | Loss: 0.00000963
Iteration 41/1000 | Loss: 0.00000962
Iteration 42/1000 | Loss: 0.00000960
Iteration 43/1000 | Loss: 0.00000960
Iteration 44/1000 | Loss: 0.00000960
Iteration 45/1000 | Loss: 0.00000960
Iteration 46/1000 | Loss: 0.00000959
Iteration 47/1000 | Loss: 0.00000959
Iteration 48/1000 | Loss: 0.00000959
Iteration 49/1000 | Loss: 0.00000958
Iteration 50/1000 | Loss: 0.00000958
Iteration 51/1000 | Loss: 0.00000957
Iteration 52/1000 | Loss: 0.00000957
Iteration 53/1000 | Loss: 0.00000956
Iteration 54/1000 | Loss: 0.00000956
Iteration 55/1000 | Loss: 0.00000956
Iteration 56/1000 | Loss: 0.00000955
Iteration 57/1000 | Loss: 0.00000955
Iteration 58/1000 | Loss: 0.00000955
Iteration 59/1000 | Loss: 0.00000955
Iteration 60/1000 | Loss: 0.00000955
Iteration 61/1000 | Loss: 0.00000955
Iteration 62/1000 | Loss: 0.00000955
Iteration 63/1000 | Loss: 0.00000954
Iteration 64/1000 | Loss: 0.00000953
Iteration 65/1000 | Loss: 0.00000953
Iteration 66/1000 | Loss: 0.00000953
Iteration 67/1000 | Loss: 0.00000953
Iteration 68/1000 | Loss: 0.00000953
Iteration 69/1000 | Loss: 0.00000952
Iteration 70/1000 | Loss: 0.00000952
Iteration 71/1000 | Loss: 0.00000952
Iteration 72/1000 | Loss: 0.00000952
Iteration 73/1000 | Loss: 0.00000952
Iteration 74/1000 | Loss: 0.00000952
Iteration 75/1000 | Loss: 0.00000951
Iteration 76/1000 | Loss: 0.00000951
Iteration 77/1000 | Loss: 0.00000951
Iteration 78/1000 | Loss: 0.00000951
Iteration 79/1000 | Loss: 0.00000951
Iteration 80/1000 | Loss: 0.00000950
Iteration 81/1000 | Loss: 0.00000950
Iteration 82/1000 | Loss: 0.00000949
Iteration 83/1000 | Loss: 0.00000949
Iteration 84/1000 | Loss: 0.00000949
Iteration 85/1000 | Loss: 0.00000949
Iteration 86/1000 | Loss: 0.00000949
Iteration 87/1000 | Loss: 0.00000948
Iteration 88/1000 | Loss: 0.00000948
Iteration 89/1000 | Loss: 0.00000948
Iteration 90/1000 | Loss: 0.00000948
Iteration 91/1000 | Loss: 0.00000947
Iteration 92/1000 | Loss: 0.00000947
Iteration 93/1000 | Loss: 0.00000947
Iteration 94/1000 | Loss: 0.00000946
Iteration 95/1000 | Loss: 0.00000946
Iteration 96/1000 | Loss: 0.00000946
Iteration 97/1000 | Loss: 0.00000946
Iteration 98/1000 | Loss: 0.00000946
Iteration 99/1000 | Loss: 0.00000946
Iteration 100/1000 | Loss: 0.00000946
Iteration 101/1000 | Loss: 0.00000946
Iteration 102/1000 | Loss: 0.00000945
Iteration 103/1000 | Loss: 0.00000945
Iteration 104/1000 | Loss: 0.00000945
Iteration 105/1000 | Loss: 0.00000945
Iteration 106/1000 | Loss: 0.00000945
Iteration 107/1000 | Loss: 0.00000945
Iteration 108/1000 | Loss: 0.00000944
Iteration 109/1000 | Loss: 0.00000944
Iteration 110/1000 | Loss: 0.00000944
Iteration 111/1000 | Loss: 0.00000944
Iteration 112/1000 | Loss: 0.00000944
Iteration 113/1000 | Loss: 0.00000944
Iteration 114/1000 | Loss: 0.00000944
Iteration 115/1000 | Loss: 0.00000944
Iteration 116/1000 | Loss: 0.00000943
Iteration 117/1000 | Loss: 0.00000943
Iteration 118/1000 | Loss: 0.00000943
Iteration 119/1000 | Loss: 0.00000943
Iteration 120/1000 | Loss: 0.00000943
Iteration 121/1000 | Loss: 0.00000943
Iteration 122/1000 | Loss: 0.00000943
Iteration 123/1000 | Loss: 0.00000943
Iteration 124/1000 | Loss: 0.00000943
Iteration 125/1000 | Loss: 0.00000943
Iteration 126/1000 | Loss: 0.00000943
Iteration 127/1000 | Loss: 0.00000943
Iteration 128/1000 | Loss: 0.00000943
Iteration 129/1000 | Loss: 0.00000943
Iteration 130/1000 | Loss: 0.00000943
Iteration 131/1000 | Loss: 0.00000943
Iteration 132/1000 | Loss: 0.00000942
Iteration 133/1000 | Loss: 0.00000942
Iteration 134/1000 | Loss: 0.00000942
Iteration 135/1000 | Loss: 0.00000942
Iteration 136/1000 | Loss: 0.00000942
Iteration 137/1000 | Loss: 0.00000942
Iteration 138/1000 | Loss: 0.00000942
Iteration 139/1000 | Loss: 0.00000942
Iteration 140/1000 | Loss: 0.00000942
Iteration 141/1000 | Loss: 0.00000942
Iteration 142/1000 | Loss: 0.00000942
Iteration 143/1000 | Loss: 0.00000942
Iteration 144/1000 | Loss: 0.00000941
Iteration 145/1000 | Loss: 0.00000941
Iteration 146/1000 | Loss: 0.00000941
Iteration 147/1000 | Loss: 0.00000941
Iteration 148/1000 | Loss: 0.00000941
Iteration 149/1000 | Loss: 0.00000941
Iteration 150/1000 | Loss: 0.00000941
Iteration 151/1000 | Loss: 0.00000941
Iteration 152/1000 | Loss: 0.00000941
Iteration 153/1000 | Loss: 0.00000941
Iteration 154/1000 | Loss: 0.00000941
Iteration 155/1000 | Loss: 0.00000941
Iteration 156/1000 | Loss: 0.00000941
Iteration 157/1000 | Loss: 0.00000941
Iteration 158/1000 | Loss: 0.00000941
Iteration 159/1000 | Loss: 0.00000941
Iteration 160/1000 | Loss: 0.00000940
Iteration 161/1000 | Loss: 0.00000940
Iteration 162/1000 | Loss: 0.00000940
Iteration 163/1000 | Loss: 0.00000940
Iteration 164/1000 | Loss: 0.00000940
Iteration 165/1000 | Loss: 0.00000940
Iteration 166/1000 | Loss: 0.00000940
Iteration 167/1000 | Loss: 0.00000940
Iteration 168/1000 | Loss: 0.00000940
Iteration 169/1000 | Loss: 0.00000940
Iteration 170/1000 | Loss: 0.00000940
Iteration 171/1000 | Loss: 0.00000939
Iteration 172/1000 | Loss: 0.00000939
Iteration 173/1000 | Loss: 0.00000939
Iteration 174/1000 | Loss: 0.00000939
Iteration 175/1000 | Loss: 0.00000939
Iteration 176/1000 | Loss: 0.00000939
Iteration 177/1000 | Loss: 0.00000939
Iteration 178/1000 | Loss: 0.00000939
Iteration 179/1000 | Loss: 0.00000939
Iteration 180/1000 | Loss: 0.00000939
Iteration 181/1000 | Loss: 0.00000938
Iteration 182/1000 | Loss: 0.00000938
Iteration 183/1000 | Loss: 0.00000938
Iteration 184/1000 | Loss: 0.00000938
Iteration 185/1000 | Loss: 0.00000938
Iteration 186/1000 | Loss: 0.00000938
Iteration 187/1000 | Loss: 0.00000938
Iteration 188/1000 | Loss: 0.00000938
Iteration 189/1000 | Loss: 0.00000938
Iteration 190/1000 | Loss: 0.00000938
Iteration 191/1000 | Loss: 0.00000938
Iteration 192/1000 | Loss: 0.00000938
Iteration 193/1000 | Loss: 0.00000938
Iteration 194/1000 | Loss: 0.00000938
Iteration 195/1000 | Loss: 0.00000938
Iteration 196/1000 | Loss: 0.00000938
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 196. Stopping optimization.
Last 5 losses: [9.381941708852537e-06, 9.381941708852537e-06, 9.381941708852537e-06, 9.381941708852537e-06, 9.381941708852537e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.381941708852537e-06

Optimization complete. Final v2v error: 2.6756134033203125 mm

Highest mean error: 2.8956048488616943 mm for frame 78

Lowest mean error: 2.552654981613159 mm for frame 97

Saving results

Total time: 43.49309825897217
