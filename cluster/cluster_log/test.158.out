Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=158, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 8848-8903
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_013/1019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_013/1019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_013/1019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00816550
Iteration 2/25 | Loss: 0.00113359
Iteration 3/25 | Loss: 0.00104968
Iteration 4/25 | Loss: 0.00104087
Iteration 5/25 | Loss: 0.00103899
Iteration 6/25 | Loss: 0.00103899
Iteration 7/25 | Loss: 0.00103899
Iteration 8/25 | Loss: 0.00103899
Iteration 9/25 | Loss: 0.00103899
Iteration 10/25 | Loss: 0.00103899
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.00103898870293051, 0.00103898870293051, 0.00103898870293051, 0.00103898870293051, 0.00103898870293051]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00103898870293051

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37836289
Iteration 2/25 | Loss: 0.00066085
Iteration 3/25 | Loss: 0.00066085
Iteration 4/25 | Loss: 0.00066085
Iteration 5/25 | Loss: 0.00066085
Iteration 6/25 | Loss: 0.00066085
Iteration 7/25 | Loss: 0.00066085
Iteration 8/25 | Loss: 0.00066085
Iteration 9/25 | Loss: 0.00066085
Iteration 10/25 | Loss: 0.00066085
Iteration 11/25 | Loss: 0.00066085
Iteration 12/25 | Loss: 0.00066085
Iteration 13/25 | Loss: 0.00066085
Iteration 14/25 | Loss: 0.00066085
Iteration 15/25 | Loss: 0.00066085
Iteration 16/25 | Loss: 0.00066085
Iteration 17/25 | Loss: 0.00066085
Iteration 18/25 | Loss: 0.00066085
Iteration 19/25 | Loss: 0.00066085
Iteration 20/25 | Loss: 0.00066085
Iteration 21/25 | Loss: 0.00066085
Iteration 22/25 | Loss: 0.00066085
Iteration 23/25 | Loss: 0.00066085
Iteration 24/25 | Loss: 0.00066085
Iteration 25/25 | Loss: 0.00066085

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00066085
Iteration 2/1000 | Loss: 0.00001800
Iteration 3/1000 | Loss: 0.00001244
Iteration 4/1000 | Loss: 0.00001136
Iteration 5/1000 | Loss: 0.00001052
Iteration 6/1000 | Loss: 0.00000994
Iteration 7/1000 | Loss: 0.00000963
Iteration 8/1000 | Loss: 0.00000947
Iteration 9/1000 | Loss: 0.00000933
Iteration 10/1000 | Loss: 0.00000912
Iteration 11/1000 | Loss: 0.00000907
Iteration 12/1000 | Loss: 0.00000906
Iteration 13/1000 | Loss: 0.00000902
Iteration 14/1000 | Loss: 0.00000900
Iteration 15/1000 | Loss: 0.00000899
Iteration 16/1000 | Loss: 0.00000899
Iteration 17/1000 | Loss: 0.00000898
Iteration 18/1000 | Loss: 0.00000896
Iteration 19/1000 | Loss: 0.00000895
Iteration 20/1000 | Loss: 0.00000894
Iteration 21/1000 | Loss: 0.00000893
Iteration 22/1000 | Loss: 0.00000893
Iteration 23/1000 | Loss: 0.00000889
Iteration 24/1000 | Loss: 0.00000889
Iteration 25/1000 | Loss: 0.00000887
Iteration 26/1000 | Loss: 0.00000884
Iteration 27/1000 | Loss: 0.00000883
Iteration 28/1000 | Loss: 0.00000880
Iteration 29/1000 | Loss: 0.00000879
Iteration 30/1000 | Loss: 0.00000876
Iteration 31/1000 | Loss: 0.00000876
Iteration 32/1000 | Loss: 0.00000876
Iteration 33/1000 | Loss: 0.00000875
Iteration 34/1000 | Loss: 0.00000875
Iteration 35/1000 | Loss: 0.00000875
Iteration 36/1000 | Loss: 0.00000874
Iteration 37/1000 | Loss: 0.00000874
Iteration 38/1000 | Loss: 0.00000874
Iteration 39/1000 | Loss: 0.00000873
Iteration 40/1000 | Loss: 0.00000873
Iteration 41/1000 | Loss: 0.00000873
Iteration 42/1000 | Loss: 0.00000873
Iteration 43/1000 | Loss: 0.00000873
Iteration 44/1000 | Loss: 0.00000873
Iteration 45/1000 | Loss: 0.00000873
Iteration 46/1000 | Loss: 0.00000873
Iteration 47/1000 | Loss: 0.00000872
Iteration 48/1000 | Loss: 0.00000871
Iteration 49/1000 | Loss: 0.00000871
Iteration 50/1000 | Loss: 0.00000871
Iteration 51/1000 | Loss: 0.00000870
Iteration 52/1000 | Loss: 0.00000870
Iteration 53/1000 | Loss: 0.00000869
Iteration 54/1000 | Loss: 0.00000868
Iteration 55/1000 | Loss: 0.00000868
Iteration 56/1000 | Loss: 0.00000868
Iteration 57/1000 | Loss: 0.00000868
Iteration 58/1000 | Loss: 0.00000867
Iteration 59/1000 | Loss: 0.00000867
Iteration 60/1000 | Loss: 0.00000867
Iteration 61/1000 | Loss: 0.00000867
Iteration 62/1000 | Loss: 0.00000867
Iteration 63/1000 | Loss: 0.00000867
Iteration 64/1000 | Loss: 0.00000867
Iteration 65/1000 | Loss: 0.00000866
Iteration 66/1000 | Loss: 0.00000865
Iteration 67/1000 | Loss: 0.00000864
Iteration 68/1000 | Loss: 0.00000863
Iteration 69/1000 | Loss: 0.00000863
Iteration 70/1000 | Loss: 0.00000862
Iteration 71/1000 | Loss: 0.00000861
Iteration 72/1000 | Loss: 0.00000861
Iteration 73/1000 | Loss: 0.00000860
Iteration 74/1000 | Loss: 0.00000860
Iteration 75/1000 | Loss: 0.00000859
Iteration 76/1000 | Loss: 0.00000858
Iteration 77/1000 | Loss: 0.00000858
Iteration 78/1000 | Loss: 0.00000858
Iteration 79/1000 | Loss: 0.00000858
Iteration 80/1000 | Loss: 0.00000858
Iteration 81/1000 | Loss: 0.00000857
Iteration 82/1000 | Loss: 0.00000857
Iteration 83/1000 | Loss: 0.00000857
Iteration 84/1000 | Loss: 0.00000857
Iteration 85/1000 | Loss: 0.00000856
Iteration 86/1000 | Loss: 0.00000856
Iteration 87/1000 | Loss: 0.00000855
Iteration 88/1000 | Loss: 0.00000855
Iteration 89/1000 | Loss: 0.00000855
Iteration 90/1000 | Loss: 0.00000855
Iteration 91/1000 | Loss: 0.00000855
Iteration 92/1000 | Loss: 0.00000855
Iteration 93/1000 | Loss: 0.00000855
Iteration 94/1000 | Loss: 0.00000854
Iteration 95/1000 | Loss: 0.00000854
Iteration 96/1000 | Loss: 0.00000854
Iteration 97/1000 | Loss: 0.00000854
Iteration 98/1000 | Loss: 0.00000854
Iteration 99/1000 | Loss: 0.00000854
Iteration 100/1000 | Loss: 0.00000854
Iteration 101/1000 | Loss: 0.00000853
Iteration 102/1000 | Loss: 0.00000853
Iteration 103/1000 | Loss: 0.00000853
Iteration 104/1000 | Loss: 0.00000853
Iteration 105/1000 | Loss: 0.00000852
Iteration 106/1000 | Loss: 0.00000852
Iteration 107/1000 | Loss: 0.00000852
Iteration 108/1000 | Loss: 0.00000852
Iteration 109/1000 | Loss: 0.00000851
Iteration 110/1000 | Loss: 0.00000851
Iteration 111/1000 | Loss: 0.00000851
Iteration 112/1000 | Loss: 0.00000851
Iteration 113/1000 | Loss: 0.00000851
Iteration 114/1000 | Loss: 0.00000850
Iteration 115/1000 | Loss: 0.00000850
Iteration 116/1000 | Loss: 0.00000849
Iteration 117/1000 | Loss: 0.00000849
Iteration 118/1000 | Loss: 0.00000849
Iteration 119/1000 | Loss: 0.00000848
Iteration 120/1000 | Loss: 0.00000848
Iteration 121/1000 | Loss: 0.00000848
Iteration 122/1000 | Loss: 0.00000848
Iteration 123/1000 | Loss: 0.00000848
Iteration 124/1000 | Loss: 0.00000848
Iteration 125/1000 | Loss: 0.00000848
Iteration 126/1000 | Loss: 0.00000848
Iteration 127/1000 | Loss: 0.00000847
Iteration 128/1000 | Loss: 0.00000847
Iteration 129/1000 | Loss: 0.00000847
Iteration 130/1000 | Loss: 0.00000847
Iteration 131/1000 | Loss: 0.00000847
Iteration 132/1000 | Loss: 0.00000846
Iteration 133/1000 | Loss: 0.00000846
Iteration 134/1000 | Loss: 0.00000846
Iteration 135/1000 | Loss: 0.00000846
Iteration 136/1000 | Loss: 0.00000846
Iteration 137/1000 | Loss: 0.00000846
Iteration 138/1000 | Loss: 0.00000846
Iteration 139/1000 | Loss: 0.00000846
Iteration 140/1000 | Loss: 0.00000846
Iteration 141/1000 | Loss: 0.00000846
Iteration 142/1000 | Loss: 0.00000846
Iteration 143/1000 | Loss: 0.00000846
Iteration 144/1000 | Loss: 0.00000846
Iteration 145/1000 | Loss: 0.00000846
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 145. Stopping optimization.
Last 5 losses: [8.46063358039828e-06, 8.46063358039828e-06, 8.46063358039828e-06, 8.46063358039828e-06, 8.46063358039828e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.46063358039828e-06

Optimization complete. Final v2v error: 2.477884531021118 mm

Highest mean error: 2.652740001678467 mm for frame 57

Lowest mean error: 2.3444344997406006 mm for frame 213

Saving results

Total time: 40.06753969192505
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_013/1009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_013/1009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_013/1009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00775826
Iteration 2/25 | Loss: 0.00125456
Iteration 3/25 | Loss: 0.00107882
Iteration 4/25 | Loss: 0.00106363
Iteration 5/25 | Loss: 0.00106089
Iteration 6/25 | Loss: 0.00106050
Iteration 7/25 | Loss: 0.00106050
Iteration 8/25 | Loss: 0.00106050
Iteration 9/25 | Loss: 0.00106050
Iteration 10/25 | Loss: 0.00106050
Iteration 11/25 | Loss: 0.00106050
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010605021379888058, 0.0010605021379888058, 0.0010605021379888058, 0.0010605021379888058, 0.0010605021379888058]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010605021379888058

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38499510
Iteration 2/25 | Loss: 0.00070909
Iteration 3/25 | Loss: 0.00070909
Iteration 4/25 | Loss: 0.00070909
Iteration 5/25 | Loss: 0.00070909
Iteration 6/25 | Loss: 0.00070908
Iteration 7/25 | Loss: 0.00070908
Iteration 8/25 | Loss: 0.00070908
Iteration 9/25 | Loss: 0.00070908
Iteration 10/25 | Loss: 0.00070908
Iteration 11/25 | Loss: 0.00070908
Iteration 12/25 | Loss: 0.00070908
Iteration 13/25 | Loss: 0.00070908
Iteration 14/25 | Loss: 0.00070908
Iteration 15/25 | Loss: 0.00070908
Iteration 16/25 | Loss: 0.00070908
Iteration 17/25 | Loss: 0.00070908
Iteration 18/25 | Loss: 0.00070908
Iteration 19/25 | Loss: 0.00070908
Iteration 20/25 | Loss: 0.00070908
Iteration 21/25 | Loss: 0.00070908
Iteration 22/25 | Loss: 0.00070908
Iteration 23/25 | Loss: 0.00070908
Iteration 24/25 | Loss: 0.00070908
Iteration 25/25 | Loss: 0.00070908

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00070908
Iteration 2/1000 | Loss: 0.00002488
Iteration 3/1000 | Loss: 0.00001624
Iteration 4/1000 | Loss: 0.00001270
Iteration 5/1000 | Loss: 0.00001149
Iteration 6/1000 | Loss: 0.00001093
Iteration 7/1000 | Loss: 0.00001050
Iteration 8/1000 | Loss: 0.00001009
Iteration 9/1000 | Loss: 0.00000987
Iteration 10/1000 | Loss: 0.00000985
Iteration 11/1000 | Loss: 0.00000975
Iteration 12/1000 | Loss: 0.00000950
Iteration 13/1000 | Loss: 0.00000949
Iteration 14/1000 | Loss: 0.00000931
Iteration 15/1000 | Loss: 0.00000928
Iteration 16/1000 | Loss: 0.00000922
Iteration 17/1000 | Loss: 0.00000918
Iteration 18/1000 | Loss: 0.00000918
Iteration 19/1000 | Loss: 0.00000914
Iteration 20/1000 | Loss: 0.00000914
Iteration 21/1000 | Loss: 0.00000910
Iteration 22/1000 | Loss: 0.00000910
Iteration 23/1000 | Loss: 0.00000910
Iteration 24/1000 | Loss: 0.00000909
Iteration 25/1000 | Loss: 0.00000909
Iteration 26/1000 | Loss: 0.00000908
Iteration 27/1000 | Loss: 0.00000908
Iteration 28/1000 | Loss: 0.00000907
Iteration 29/1000 | Loss: 0.00000907
Iteration 30/1000 | Loss: 0.00000906
Iteration 31/1000 | Loss: 0.00000906
Iteration 32/1000 | Loss: 0.00000906
Iteration 33/1000 | Loss: 0.00000906
Iteration 34/1000 | Loss: 0.00000905
Iteration 35/1000 | Loss: 0.00000905
Iteration 36/1000 | Loss: 0.00000905
Iteration 37/1000 | Loss: 0.00000904
Iteration 38/1000 | Loss: 0.00000904
Iteration 39/1000 | Loss: 0.00000903
Iteration 40/1000 | Loss: 0.00000903
Iteration 41/1000 | Loss: 0.00000903
Iteration 42/1000 | Loss: 0.00000902
Iteration 43/1000 | Loss: 0.00000902
Iteration 44/1000 | Loss: 0.00000902
Iteration 45/1000 | Loss: 0.00000902
Iteration 46/1000 | Loss: 0.00000902
Iteration 47/1000 | Loss: 0.00000902
Iteration 48/1000 | Loss: 0.00000902
Iteration 49/1000 | Loss: 0.00000902
Iteration 50/1000 | Loss: 0.00000902
Iteration 51/1000 | Loss: 0.00000901
Iteration 52/1000 | Loss: 0.00000901
Iteration 53/1000 | Loss: 0.00000901
Iteration 54/1000 | Loss: 0.00000901
Iteration 55/1000 | Loss: 0.00000901
Iteration 56/1000 | Loss: 0.00000901
Iteration 57/1000 | Loss: 0.00000900
Iteration 58/1000 | Loss: 0.00000900
Iteration 59/1000 | Loss: 0.00000900
Iteration 60/1000 | Loss: 0.00000900
Iteration 61/1000 | Loss: 0.00000900
Iteration 62/1000 | Loss: 0.00000900
Iteration 63/1000 | Loss: 0.00000900
Iteration 64/1000 | Loss: 0.00000900
Iteration 65/1000 | Loss: 0.00000900
Iteration 66/1000 | Loss: 0.00000900
Iteration 67/1000 | Loss: 0.00000899
Iteration 68/1000 | Loss: 0.00000899
Iteration 69/1000 | Loss: 0.00000899
Iteration 70/1000 | Loss: 0.00000899
Iteration 71/1000 | Loss: 0.00000899
Iteration 72/1000 | Loss: 0.00000898
Iteration 73/1000 | Loss: 0.00000898
Iteration 74/1000 | Loss: 0.00000898
Iteration 75/1000 | Loss: 0.00000897
Iteration 76/1000 | Loss: 0.00000897
Iteration 77/1000 | Loss: 0.00000897
Iteration 78/1000 | Loss: 0.00000897
Iteration 79/1000 | Loss: 0.00000897
Iteration 80/1000 | Loss: 0.00000897
Iteration 81/1000 | Loss: 0.00000897
Iteration 82/1000 | Loss: 0.00000896
Iteration 83/1000 | Loss: 0.00000896
Iteration 84/1000 | Loss: 0.00000896
Iteration 85/1000 | Loss: 0.00000896
Iteration 86/1000 | Loss: 0.00000896
Iteration 87/1000 | Loss: 0.00000896
Iteration 88/1000 | Loss: 0.00000896
Iteration 89/1000 | Loss: 0.00000896
Iteration 90/1000 | Loss: 0.00000896
Iteration 91/1000 | Loss: 0.00000896
Iteration 92/1000 | Loss: 0.00000896
Iteration 93/1000 | Loss: 0.00000895
Iteration 94/1000 | Loss: 0.00000895
Iteration 95/1000 | Loss: 0.00000895
Iteration 96/1000 | Loss: 0.00000895
Iteration 97/1000 | Loss: 0.00000895
Iteration 98/1000 | Loss: 0.00000895
Iteration 99/1000 | Loss: 0.00000895
Iteration 100/1000 | Loss: 0.00000895
Iteration 101/1000 | Loss: 0.00000895
Iteration 102/1000 | Loss: 0.00000895
Iteration 103/1000 | Loss: 0.00000895
Iteration 104/1000 | Loss: 0.00000895
Iteration 105/1000 | Loss: 0.00000894
Iteration 106/1000 | Loss: 0.00000894
Iteration 107/1000 | Loss: 0.00000894
Iteration 108/1000 | Loss: 0.00000894
Iteration 109/1000 | Loss: 0.00000894
Iteration 110/1000 | Loss: 0.00000894
Iteration 111/1000 | Loss: 0.00000894
Iteration 112/1000 | Loss: 0.00000894
Iteration 113/1000 | Loss: 0.00000894
Iteration 114/1000 | Loss: 0.00000894
Iteration 115/1000 | Loss: 0.00000894
Iteration 116/1000 | Loss: 0.00000894
Iteration 117/1000 | Loss: 0.00000894
Iteration 118/1000 | Loss: 0.00000893
Iteration 119/1000 | Loss: 0.00000893
Iteration 120/1000 | Loss: 0.00000893
Iteration 121/1000 | Loss: 0.00000893
Iteration 122/1000 | Loss: 0.00000893
Iteration 123/1000 | Loss: 0.00000893
Iteration 124/1000 | Loss: 0.00000892
Iteration 125/1000 | Loss: 0.00000892
Iteration 126/1000 | Loss: 0.00000892
Iteration 127/1000 | Loss: 0.00000892
Iteration 128/1000 | Loss: 0.00000892
Iteration 129/1000 | Loss: 0.00000892
Iteration 130/1000 | Loss: 0.00000891
Iteration 131/1000 | Loss: 0.00000891
Iteration 132/1000 | Loss: 0.00000891
Iteration 133/1000 | Loss: 0.00000891
Iteration 134/1000 | Loss: 0.00000891
Iteration 135/1000 | Loss: 0.00000891
Iteration 136/1000 | Loss: 0.00000891
Iteration 137/1000 | Loss: 0.00000891
Iteration 138/1000 | Loss: 0.00000891
Iteration 139/1000 | Loss: 0.00000891
Iteration 140/1000 | Loss: 0.00000890
Iteration 141/1000 | Loss: 0.00000890
Iteration 142/1000 | Loss: 0.00000890
Iteration 143/1000 | Loss: 0.00000890
Iteration 144/1000 | Loss: 0.00000890
Iteration 145/1000 | Loss: 0.00000890
Iteration 146/1000 | Loss: 0.00000890
Iteration 147/1000 | Loss: 0.00000890
Iteration 148/1000 | Loss: 0.00000889
Iteration 149/1000 | Loss: 0.00000889
Iteration 150/1000 | Loss: 0.00000889
Iteration 151/1000 | Loss: 0.00000889
Iteration 152/1000 | Loss: 0.00000889
Iteration 153/1000 | Loss: 0.00000889
Iteration 154/1000 | Loss: 0.00000889
Iteration 155/1000 | Loss: 0.00000889
Iteration 156/1000 | Loss: 0.00000889
Iteration 157/1000 | Loss: 0.00000889
Iteration 158/1000 | Loss: 0.00000889
Iteration 159/1000 | Loss: 0.00000889
Iteration 160/1000 | Loss: 0.00000889
Iteration 161/1000 | Loss: 0.00000888
Iteration 162/1000 | Loss: 0.00000888
Iteration 163/1000 | Loss: 0.00000888
Iteration 164/1000 | Loss: 0.00000888
Iteration 165/1000 | Loss: 0.00000888
Iteration 166/1000 | Loss: 0.00000888
Iteration 167/1000 | Loss: 0.00000888
Iteration 168/1000 | Loss: 0.00000887
Iteration 169/1000 | Loss: 0.00000887
Iteration 170/1000 | Loss: 0.00000887
Iteration 171/1000 | Loss: 0.00000887
Iteration 172/1000 | Loss: 0.00000887
Iteration 173/1000 | Loss: 0.00000887
Iteration 174/1000 | Loss: 0.00000887
Iteration 175/1000 | Loss: 0.00000887
Iteration 176/1000 | Loss: 0.00000886
Iteration 177/1000 | Loss: 0.00000886
Iteration 178/1000 | Loss: 0.00000886
Iteration 179/1000 | Loss: 0.00000886
Iteration 180/1000 | Loss: 0.00000886
Iteration 181/1000 | Loss: 0.00000886
Iteration 182/1000 | Loss: 0.00000886
Iteration 183/1000 | Loss: 0.00000886
Iteration 184/1000 | Loss: 0.00000886
Iteration 185/1000 | Loss: 0.00000886
Iteration 186/1000 | Loss: 0.00000886
Iteration 187/1000 | Loss: 0.00000886
Iteration 188/1000 | Loss: 0.00000886
Iteration 189/1000 | Loss: 0.00000885
Iteration 190/1000 | Loss: 0.00000885
Iteration 191/1000 | Loss: 0.00000885
Iteration 192/1000 | Loss: 0.00000885
Iteration 193/1000 | Loss: 0.00000884
Iteration 194/1000 | Loss: 0.00000884
Iteration 195/1000 | Loss: 0.00000884
Iteration 196/1000 | Loss: 0.00000884
Iteration 197/1000 | Loss: 0.00000884
Iteration 198/1000 | Loss: 0.00000884
Iteration 199/1000 | Loss: 0.00000884
Iteration 200/1000 | Loss: 0.00000884
Iteration 201/1000 | Loss: 0.00000884
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 201. Stopping optimization.
Last 5 losses: [8.842207535053603e-06, 8.842207535053603e-06, 8.842207535053603e-06, 8.842207535053603e-06, 8.842207535053603e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.842207535053603e-06

Optimization complete. Final v2v error: 2.5540056228637695 mm

Highest mean error: 2.9621708393096924 mm for frame 84

Lowest mean error: 2.4320056438446045 mm for frame 64

Saving results

Total time: 40.33963632583618
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_013/1032/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_013/1032.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_013/1032
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00398216
Iteration 2/25 | Loss: 0.00119203
Iteration 3/25 | Loss: 0.00106707
Iteration 4/25 | Loss: 0.00105280
Iteration 5/25 | Loss: 0.00105052
Iteration 6/25 | Loss: 0.00105004
Iteration 7/25 | Loss: 0.00105004
Iteration 8/25 | Loss: 0.00105004
Iteration 9/25 | Loss: 0.00105004
Iteration 10/25 | Loss: 0.00105004
Iteration 11/25 | Loss: 0.00105004
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010500418720766902, 0.0010500418720766902, 0.0010500418720766902, 0.0010500418720766902, 0.0010500418720766902]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010500418720766902

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37253904
Iteration 2/25 | Loss: 0.00053202
Iteration 3/25 | Loss: 0.00053201
Iteration 4/25 | Loss: 0.00053201
Iteration 5/25 | Loss: 0.00053201
Iteration 6/25 | Loss: 0.00053201
Iteration 7/25 | Loss: 0.00053201
Iteration 8/25 | Loss: 0.00053201
Iteration 9/25 | Loss: 0.00053201
Iteration 10/25 | Loss: 0.00053201
Iteration 11/25 | Loss: 0.00053201
Iteration 12/25 | Loss: 0.00053201
Iteration 13/25 | Loss: 0.00053201
Iteration 14/25 | Loss: 0.00053201
Iteration 15/25 | Loss: 0.00053201
Iteration 16/25 | Loss: 0.00053201
Iteration 17/25 | Loss: 0.00053201
Iteration 18/25 | Loss: 0.00053201
Iteration 19/25 | Loss: 0.00053201
Iteration 20/25 | Loss: 0.00053201
Iteration 21/25 | Loss: 0.00053201
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0005320109194144607, 0.0005320109194144607, 0.0005320109194144607, 0.0005320109194144607, 0.0005320109194144607]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005320109194144607

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00053201
Iteration 2/1000 | Loss: 0.00002101
Iteration 3/1000 | Loss: 0.00001590
Iteration 4/1000 | Loss: 0.00001435
Iteration 5/1000 | Loss: 0.00001347
Iteration 6/1000 | Loss: 0.00001282
Iteration 7/1000 | Loss: 0.00001240
Iteration 8/1000 | Loss: 0.00001218
Iteration 9/1000 | Loss: 0.00001187
Iteration 10/1000 | Loss: 0.00001185
Iteration 11/1000 | Loss: 0.00001167
Iteration 12/1000 | Loss: 0.00001166
Iteration 13/1000 | Loss: 0.00001165
Iteration 14/1000 | Loss: 0.00001159
Iteration 15/1000 | Loss: 0.00001153
Iteration 16/1000 | Loss: 0.00001152
Iteration 17/1000 | Loss: 0.00001146
Iteration 18/1000 | Loss: 0.00001143
Iteration 19/1000 | Loss: 0.00001137
Iteration 20/1000 | Loss: 0.00001137
Iteration 21/1000 | Loss: 0.00001137
Iteration 22/1000 | Loss: 0.00001135
Iteration 23/1000 | Loss: 0.00001134
Iteration 24/1000 | Loss: 0.00001134
Iteration 25/1000 | Loss: 0.00001134
Iteration 26/1000 | Loss: 0.00001133
Iteration 27/1000 | Loss: 0.00001133
Iteration 28/1000 | Loss: 0.00001132
Iteration 29/1000 | Loss: 0.00001130
Iteration 30/1000 | Loss: 0.00001130
Iteration 31/1000 | Loss: 0.00001130
Iteration 32/1000 | Loss: 0.00001130
Iteration 33/1000 | Loss: 0.00001129
Iteration 34/1000 | Loss: 0.00001129
Iteration 35/1000 | Loss: 0.00001129
Iteration 36/1000 | Loss: 0.00001129
Iteration 37/1000 | Loss: 0.00001128
Iteration 38/1000 | Loss: 0.00001127
Iteration 39/1000 | Loss: 0.00001125
Iteration 40/1000 | Loss: 0.00001125
Iteration 41/1000 | Loss: 0.00001123
Iteration 42/1000 | Loss: 0.00001120
Iteration 43/1000 | Loss: 0.00001120
Iteration 44/1000 | Loss: 0.00001120
Iteration 45/1000 | Loss: 0.00001119
Iteration 46/1000 | Loss: 0.00001119
Iteration 47/1000 | Loss: 0.00001119
Iteration 48/1000 | Loss: 0.00001112
Iteration 49/1000 | Loss: 0.00001112
Iteration 50/1000 | Loss: 0.00001111
Iteration 51/1000 | Loss: 0.00001108
Iteration 52/1000 | Loss: 0.00001107
Iteration 53/1000 | Loss: 0.00001107
Iteration 54/1000 | Loss: 0.00001106
Iteration 55/1000 | Loss: 0.00001105
Iteration 56/1000 | Loss: 0.00001104
Iteration 57/1000 | Loss: 0.00001104
Iteration 58/1000 | Loss: 0.00001103
Iteration 59/1000 | Loss: 0.00001103
Iteration 60/1000 | Loss: 0.00001103
Iteration 61/1000 | Loss: 0.00001103
Iteration 62/1000 | Loss: 0.00001103
Iteration 63/1000 | Loss: 0.00001103
Iteration 64/1000 | Loss: 0.00001103
Iteration 65/1000 | Loss: 0.00001103
Iteration 66/1000 | Loss: 0.00001102
Iteration 67/1000 | Loss: 0.00001102
Iteration 68/1000 | Loss: 0.00001102
Iteration 69/1000 | Loss: 0.00001101
Iteration 70/1000 | Loss: 0.00001101
Iteration 71/1000 | Loss: 0.00001101
Iteration 72/1000 | Loss: 0.00001101
Iteration 73/1000 | Loss: 0.00001101
Iteration 74/1000 | Loss: 0.00001101
Iteration 75/1000 | Loss: 0.00001101
Iteration 76/1000 | Loss: 0.00001101
Iteration 77/1000 | Loss: 0.00001101
Iteration 78/1000 | Loss: 0.00001101
Iteration 79/1000 | Loss: 0.00001100
Iteration 80/1000 | Loss: 0.00001100
Iteration 81/1000 | Loss: 0.00001100
Iteration 82/1000 | Loss: 0.00001100
Iteration 83/1000 | Loss: 0.00001100
Iteration 84/1000 | Loss: 0.00001099
Iteration 85/1000 | Loss: 0.00001099
Iteration 86/1000 | Loss: 0.00001098
Iteration 87/1000 | Loss: 0.00001098
Iteration 88/1000 | Loss: 0.00001098
Iteration 89/1000 | Loss: 0.00001097
Iteration 90/1000 | Loss: 0.00001097
Iteration 91/1000 | Loss: 0.00001097
Iteration 92/1000 | Loss: 0.00001097
Iteration 93/1000 | Loss: 0.00001097
Iteration 94/1000 | Loss: 0.00001097
Iteration 95/1000 | Loss: 0.00001097
Iteration 96/1000 | Loss: 0.00001097
Iteration 97/1000 | Loss: 0.00001097
Iteration 98/1000 | Loss: 0.00001096
Iteration 99/1000 | Loss: 0.00001096
Iteration 100/1000 | Loss: 0.00001096
Iteration 101/1000 | Loss: 0.00001095
Iteration 102/1000 | Loss: 0.00001095
Iteration 103/1000 | Loss: 0.00001095
Iteration 104/1000 | Loss: 0.00001095
Iteration 105/1000 | Loss: 0.00001095
Iteration 106/1000 | Loss: 0.00001094
Iteration 107/1000 | Loss: 0.00001094
Iteration 108/1000 | Loss: 0.00001094
Iteration 109/1000 | Loss: 0.00001094
Iteration 110/1000 | Loss: 0.00001094
Iteration 111/1000 | Loss: 0.00001094
Iteration 112/1000 | Loss: 0.00001094
Iteration 113/1000 | Loss: 0.00001094
Iteration 114/1000 | Loss: 0.00001094
Iteration 115/1000 | Loss: 0.00001093
Iteration 116/1000 | Loss: 0.00001093
Iteration 117/1000 | Loss: 0.00001093
Iteration 118/1000 | Loss: 0.00001093
Iteration 119/1000 | Loss: 0.00001093
Iteration 120/1000 | Loss: 0.00001093
Iteration 121/1000 | Loss: 0.00001093
Iteration 122/1000 | Loss: 0.00001093
Iteration 123/1000 | Loss: 0.00001093
Iteration 124/1000 | Loss: 0.00001093
Iteration 125/1000 | Loss: 0.00001092
Iteration 126/1000 | Loss: 0.00001092
Iteration 127/1000 | Loss: 0.00001092
Iteration 128/1000 | Loss: 0.00001092
Iteration 129/1000 | Loss: 0.00001092
Iteration 130/1000 | Loss: 0.00001091
Iteration 131/1000 | Loss: 0.00001091
Iteration 132/1000 | Loss: 0.00001091
Iteration 133/1000 | Loss: 0.00001091
Iteration 134/1000 | Loss: 0.00001091
Iteration 135/1000 | Loss: 0.00001091
Iteration 136/1000 | Loss: 0.00001090
Iteration 137/1000 | Loss: 0.00001090
Iteration 138/1000 | Loss: 0.00001090
Iteration 139/1000 | Loss: 0.00001090
Iteration 140/1000 | Loss: 0.00001089
Iteration 141/1000 | Loss: 0.00001089
Iteration 142/1000 | Loss: 0.00001089
Iteration 143/1000 | Loss: 0.00001089
Iteration 144/1000 | Loss: 0.00001089
Iteration 145/1000 | Loss: 0.00001089
Iteration 146/1000 | Loss: 0.00001089
Iteration 147/1000 | Loss: 0.00001089
Iteration 148/1000 | Loss: 0.00001089
Iteration 149/1000 | Loss: 0.00001088
Iteration 150/1000 | Loss: 0.00001088
Iteration 151/1000 | Loss: 0.00001088
Iteration 152/1000 | Loss: 0.00001088
Iteration 153/1000 | Loss: 0.00001088
Iteration 154/1000 | Loss: 0.00001087
Iteration 155/1000 | Loss: 0.00001087
Iteration 156/1000 | Loss: 0.00001087
Iteration 157/1000 | Loss: 0.00001087
Iteration 158/1000 | Loss: 0.00001087
Iteration 159/1000 | Loss: 0.00001087
Iteration 160/1000 | Loss: 0.00001087
Iteration 161/1000 | Loss: 0.00001087
Iteration 162/1000 | Loss: 0.00001087
Iteration 163/1000 | Loss: 0.00001087
Iteration 164/1000 | Loss: 0.00001087
Iteration 165/1000 | Loss: 0.00001087
Iteration 166/1000 | Loss: 0.00001087
Iteration 167/1000 | Loss: 0.00001087
Iteration 168/1000 | Loss: 0.00001087
Iteration 169/1000 | Loss: 0.00001087
Iteration 170/1000 | Loss: 0.00001087
Iteration 171/1000 | Loss: 0.00001087
Iteration 172/1000 | Loss: 0.00001087
Iteration 173/1000 | Loss: 0.00001087
Iteration 174/1000 | Loss: 0.00001087
Iteration 175/1000 | Loss: 0.00001087
Iteration 176/1000 | Loss: 0.00001087
Iteration 177/1000 | Loss: 0.00001087
Iteration 178/1000 | Loss: 0.00001087
Iteration 179/1000 | Loss: 0.00001087
Iteration 180/1000 | Loss: 0.00001087
Iteration 181/1000 | Loss: 0.00001087
Iteration 182/1000 | Loss: 0.00001087
Iteration 183/1000 | Loss: 0.00001087
Iteration 184/1000 | Loss: 0.00001087
Iteration 185/1000 | Loss: 0.00001087
Iteration 186/1000 | Loss: 0.00001087
Iteration 187/1000 | Loss: 0.00001087
Iteration 188/1000 | Loss: 0.00001087
Iteration 189/1000 | Loss: 0.00001087
Iteration 190/1000 | Loss: 0.00001087
Iteration 191/1000 | Loss: 0.00001087
Iteration 192/1000 | Loss: 0.00001087
Iteration 193/1000 | Loss: 0.00001087
Iteration 194/1000 | Loss: 0.00001087
Iteration 195/1000 | Loss: 0.00001087
Iteration 196/1000 | Loss: 0.00001087
Iteration 197/1000 | Loss: 0.00001087
Iteration 198/1000 | Loss: 0.00001087
Iteration 199/1000 | Loss: 0.00001087
Iteration 200/1000 | Loss: 0.00001087
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 200. Stopping optimization.
Last 5 losses: [1.08719023046433e-05, 1.08719023046433e-05, 1.08719023046433e-05, 1.08719023046433e-05, 1.08719023046433e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.08719023046433e-05

Optimization complete. Final v2v error: 2.808922290802002 mm

Highest mean error: 3.0874297618865967 mm for frame 118

Lowest mean error: 2.545102119445801 mm for frame 11

Saving results

Total time: 41.01004076004028
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_013/1058/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_013/1058.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_013/1058
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00561884
Iteration 2/25 | Loss: 0.00125354
Iteration 3/25 | Loss: 0.00115438
Iteration 4/25 | Loss: 0.00114510
Iteration 5/25 | Loss: 0.00114279
Iteration 6/25 | Loss: 0.00114279
Iteration 7/25 | Loss: 0.00114279
Iteration 8/25 | Loss: 0.00114279
Iteration 9/25 | Loss: 0.00114279
Iteration 10/25 | Loss: 0.00114279
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0011427897261455655, 0.0011427897261455655, 0.0011427897261455655, 0.0011427897261455655, 0.0011427897261455655]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011427897261455655

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 11.81921005
Iteration 2/25 | Loss: 0.00062339
Iteration 3/25 | Loss: 0.00062323
Iteration 4/25 | Loss: 0.00062323
Iteration 5/25 | Loss: 0.00062323
Iteration 6/25 | Loss: 0.00062323
Iteration 7/25 | Loss: 0.00062323
Iteration 8/25 | Loss: 0.00062323
Iteration 9/25 | Loss: 0.00062323
Iteration 10/25 | Loss: 0.00062323
Iteration 11/25 | Loss: 0.00062323
Iteration 12/25 | Loss: 0.00062323
Iteration 13/25 | Loss: 0.00062323
Iteration 14/25 | Loss: 0.00062323
Iteration 15/25 | Loss: 0.00062323
Iteration 16/25 | Loss: 0.00062323
Iteration 17/25 | Loss: 0.00062323
Iteration 18/25 | Loss: 0.00062323
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0006232273881323636, 0.0006232273881323636, 0.0006232273881323636, 0.0006232273881323636, 0.0006232273881323636]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006232273881323636

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00062323
Iteration 2/1000 | Loss: 0.00002459
Iteration 3/1000 | Loss: 0.00001828
Iteration 4/1000 | Loss: 0.00001675
Iteration 5/1000 | Loss: 0.00001616
Iteration 6/1000 | Loss: 0.00001581
Iteration 7/1000 | Loss: 0.00001561
Iteration 8/1000 | Loss: 0.00001531
Iteration 9/1000 | Loss: 0.00001506
Iteration 10/1000 | Loss: 0.00001485
Iteration 11/1000 | Loss: 0.00001472
Iteration 12/1000 | Loss: 0.00001469
Iteration 13/1000 | Loss: 0.00001463
Iteration 14/1000 | Loss: 0.00001450
Iteration 15/1000 | Loss: 0.00001449
Iteration 16/1000 | Loss: 0.00001448
Iteration 17/1000 | Loss: 0.00001444
Iteration 18/1000 | Loss: 0.00001443
Iteration 19/1000 | Loss: 0.00001443
Iteration 20/1000 | Loss: 0.00001443
Iteration 21/1000 | Loss: 0.00001443
Iteration 22/1000 | Loss: 0.00001442
Iteration 23/1000 | Loss: 0.00001442
Iteration 24/1000 | Loss: 0.00001441
Iteration 25/1000 | Loss: 0.00001440
Iteration 26/1000 | Loss: 0.00001440
Iteration 27/1000 | Loss: 0.00001439
Iteration 28/1000 | Loss: 0.00001438
Iteration 29/1000 | Loss: 0.00001437
Iteration 30/1000 | Loss: 0.00001430
Iteration 31/1000 | Loss: 0.00001430
Iteration 32/1000 | Loss: 0.00001415
Iteration 33/1000 | Loss: 0.00001415
Iteration 34/1000 | Loss: 0.00001415
Iteration 35/1000 | Loss: 0.00001414
Iteration 36/1000 | Loss: 0.00001413
Iteration 37/1000 | Loss: 0.00001413
Iteration 38/1000 | Loss: 0.00001412
Iteration 39/1000 | Loss: 0.00001412
Iteration 40/1000 | Loss: 0.00001412
Iteration 41/1000 | Loss: 0.00001412
Iteration 42/1000 | Loss: 0.00001412
Iteration 43/1000 | Loss: 0.00001412
Iteration 44/1000 | Loss: 0.00001411
Iteration 45/1000 | Loss: 0.00001411
Iteration 46/1000 | Loss: 0.00001411
Iteration 47/1000 | Loss: 0.00001407
Iteration 48/1000 | Loss: 0.00001405
Iteration 49/1000 | Loss: 0.00001404
Iteration 50/1000 | Loss: 0.00001403
Iteration 51/1000 | Loss: 0.00001403
Iteration 52/1000 | Loss: 0.00001402
Iteration 53/1000 | Loss: 0.00001401
Iteration 54/1000 | Loss: 0.00001401
Iteration 55/1000 | Loss: 0.00001400
Iteration 56/1000 | Loss: 0.00001400
Iteration 57/1000 | Loss: 0.00001399
Iteration 58/1000 | Loss: 0.00001399
Iteration 59/1000 | Loss: 0.00001398
Iteration 60/1000 | Loss: 0.00001398
Iteration 61/1000 | Loss: 0.00001398
Iteration 62/1000 | Loss: 0.00001398
Iteration 63/1000 | Loss: 0.00001398
Iteration 64/1000 | Loss: 0.00001398
Iteration 65/1000 | Loss: 0.00001397
Iteration 66/1000 | Loss: 0.00001397
Iteration 67/1000 | Loss: 0.00001397
Iteration 68/1000 | Loss: 0.00001396
Iteration 69/1000 | Loss: 0.00001394
Iteration 70/1000 | Loss: 0.00001394
Iteration 71/1000 | Loss: 0.00001394
Iteration 72/1000 | Loss: 0.00001393
Iteration 73/1000 | Loss: 0.00001393
Iteration 74/1000 | Loss: 0.00001392
Iteration 75/1000 | Loss: 0.00001390
Iteration 76/1000 | Loss: 0.00001389
Iteration 77/1000 | Loss: 0.00001388
Iteration 78/1000 | Loss: 0.00001388
Iteration 79/1000 | Loss: 0.00001388
Iteration 80/1000 | Loss: 0.00001388
Iteration 81/1000 | Loss: 0.00001388
Iteration 82/1000 | Loss: 0.00001387
Iteration 83/1000 | Loss: 0.00001387
Iteration 84/1000 | Loss: 0.00001387
Iteration 85/1000 | Loss: 0.00001387
Iteration 86/1000 | Loss: 0.00001387
Iteration 87/1000 | Loss: 0.00001387
Iteration 88/1000 | Loss: 0.00001387
Iteration 89/1000 | Loss: 0.00001387
Iteration 90/1000 | Loss: 0.00001387
Iteration 91/1000 | Loss: 0.00001387
Iteration 92/1000 | Loss: 0.00001387
Iteration 93/1000 | Loss: 0.00001386
Iteration 94/1000 | Loss: 0.00001384
Iteration 95/1000 | Loss: 0.00001384
Iteration 96/1000 | Loss: 0.00001383
Iteration 97/1000 | Loss: 0.00001383
Iteration 98/1000 | Loss: 0.00001383
Iteration 99/1000 | Loss: 0.00001383
Iteration 100/1000 | Loss: 0.00001383
Iteration 101/1000 | Loss: 0.00001382
Iteration 102/1000 | Loss: 0.00001382
Iteration 103/1000 | Loss: 0.00001382
Iteration 104/1000 | Loss: 0.00001382
Iteration 105/1000 | Loss: 0.00001381
Iteration 106/1000 | Loss: 0.00001381
Iteration 107/1000 | Loss: 0.00001381
Iteration 108/1000 | Loss: 0.00001381
Iteration 109/1000 | Loss: 0.00001381
Iteration 110/1000 | Loss: 0.00001381
Iteration 111/1000 | Loss: 0.00001381
Iteration 112/1000 | Loss: 0.00001381
Iteration 113/1000 | Loss: 0.00001380
Iteration 114/1000 | Loss: 0.00001380
Iteration 115/1000 | Loss: 0.00001380
Iteration 116/1000 | Loss: 0.00001380
Iteration 117/1000 | Loss: 0.00001380
Iteration 118/1000 | Loss: 0.00001380
Iteration 119/1000 | Loss: 0.00001380
Iteration 120/1000 | Loss: 0.00001379
Iteration 121/1000 | Loss: 0.00001379
Iteration 122/1000 | Loss: 0.00001379
Iteration 123/1000 | Loss: 0.00001379
Iteration 124/1000 | Loss: 0.00001379
Iteration 125/1000 | Loss: 0.00001379
Iteration 126/1000 | Loss: 0.00001379
Iteration 127/1000 | Loss: 0.00001379
Iteration 128/1000 | Loss: 0.00001378
Iteration 129/1000 | Loss: 0.00001378
Iteration 130/1000 | Loss: 0.00001378
Iteration 131/1000 | Loss: 0.00001378
Iteration 132/1000 | Loss: 0.00001378
Iteration 133/1000 | Loss: 0.00001378
Iteration 134/1000 | Loss: 0.00001378
Iteration 135/1000 | Loss: 0.00001378
Iteration 136/1000 | Loss: 0.00001378
Iteration 137/1000 | Loss: 0.00001378
Iteration 138/1000 | Loss: 0.00001378
Iteration 139/1000 | Loss: 0.00001378
Iteration 140/1000 | Loss: 0.00001378
Iteration 141/1000 | Loss: 0.00001378
Iteration 142/1000 | Loss: 0.00001378
Iteration 143/1000 | Loss: 0.00001378
Iteration 144/1000 | Loss: 0.00001378
Iteration 145/1000 | Loss: 0.00001378
Iteration 146/1000 | Loss: 0.00001378
Iteration 147/1000 | Loss: 0.00001378
Iteration 148/1000 | Loss: 0.00001378
Iteration 149/1000 | Loss: 0.00001378
Iteration 150/1000 | Loss: 0.00001378
Iteration 151/1000 | Loss: 0.00001378
Iteration 152/1000 | Loss: 0.00001378
Iteration 153/1000 | Loss: 0.00001378
Iteration 154/1000 | Loss: 0.00001378
Iteration 155/1000 | Loss: 0.00001378
Iteration 156/1000 | Loss: 0.00001378
Iteration 157/1000 | Loss: 0.00001378
Iteration 158/1000 | Loss: 0.00001378
Iteration 159/1000 | Loss: 0.00001378
Iteration 160/1000 | Loss: 0.00001378
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 160. Stopping optimization.
Last 5 losses: [1.3776517334918026e-05, 1.3776517334918026e-05, 1.3776517334918026e-05, 1.3776517334918026e-05, 1.3776517334918026e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3776517334918026e-05

Optimization complete. Final v2v error: 3.118276834487915 mm

Highest mean error: 3.2892024517059326 mm for frame 232

Lowest mean error: 2.901479721069336 mm for frame 165

Saving results

Total time: 43.72477054595947
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_013/1044/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_013/1044.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_013/1044
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00553808
Iteration 2/25 | Loss: 0.00167462
Iteration 3/25 | Loss: 0.00126657
Iteration 4/25 | Loss: 0.00124057
Iteration 5/25 | Loss: 0.00123765
Iteration 6/25 | Loss: 0.00123700
Iteration 7/25 | Loss: 0.00123700
Iteration 8/25 | Loss: 0.00123700
Iteration 9/25 | Loss: 0.00123700
Iteration 10/25 | Loss: 0.00123700
Iteration 11/25 | Loss: 0.00123700
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001237004529684782, 0.001237004529684782, 0.001237004529684782, 0.001237004529684782, 0.001237004529684782]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001237004529684782

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.00124145
Iteration 2/25 | Loss: 0.00086636
Iteration 3/25 | Loss: 0.00086634
Iteration 4/25 | Loss: 0.00086634
Iteration 5/25 | Loss: 0.00086634
Iteration 6/25 | Loss: 0.00086634
Iteration 7/25 | Loss: 0.00086634
Iteration 8/25 | Loss: 0.00086634
Iteration 9/25 | Loss: 0.00086634
Iteration 10/25 | Loss: 0.00086634
Iteration 11/25 | Loss: 0.00086634
Iteration 12/25 | Loss: 0.00086634
Iteration 13/25 | Loss: 0.00086634
Iteration 14/25 | Loss: 0.00086634
Iteration 15/25 | Loss: 0.00086634
Iteration 16/25 | Loss: 0.00086634
Iteration 17/25 | Loss: 0.00086634
Iteration 18/25 | Loss: 0.00086634
Iteration 19/25 | Loss: 0.00086634
Iteration 20/25 | Loss: 0.00086634
Iteration 21/25 | Loss: 0.00086634
Iteration 22/25 | Loss: 0.00086634
Iteration 23/25 | Loss: 0.00086634
Iteration 24/25 | Loss: 0.00086634
Iteration 25/25 | Loss: 0.00086634
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0008663369808346033, 0.0008663369808346033, 0.0008663369808346033, 0.0008663369808346033, 0.0008663369808346033]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008663369808346033

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00086634
Iteration 2/1000 | Loss: 0.00004923
Iteration 3/1000 | Loss: 0.00003119
Iteration 4/1000 | Loss: 0.00002643
Iteration 5/1000 | Loss: 0.00002483
Iteration 6/1000 | Loss: 0.00002421
Iteration 7/1000 | Loss: 0.00002370
Iteration 8/1000 | Loss: 0.00002309
Iteration 9/1000 | Loss: 0.00002273
Iteration 10/1000 | Loss: 0.00002238
Iteration 11/1000 | Loss: 0.00002211
Iteration 12/1000 | Loss: 0.00002186
Iteration 13/1000 | Loss: 0.00002163
Iteration 14/1000 | Loss: 0.00002143
Iteration 15/1000 | Loss: 0.00002129
Iteration 16/1000 | Loss: 0.00002128
Iteration 17/1000 | Loss: 0.00002126
Iteration 18/1000 | Loss: 0.00002124
Iteration 19/1000 | Loss: 0.00002119
Iteration 20/1000 | Loss: 0.00002112
Iteration 21/1000 | Loss: 0.00002107
Iteration 22/1000 | Loss: 0.00002102
Iteration 23/1000 | Loss: 0.00002101
Iteration 24/1000 | Loss: 0.00002096
Iteration 25/1000 | Loss: 0.00002096
Iteration 26/1000 | Loss: 0.00002094
Iteration 27/1000 | Loss: 0.00002094
Iteration 28/1000 | Loss: 0.00002093
Iteration 29/1000 | Loss: 0.00002091
Iteration 30/1000 | Loss: 0.00002091
Iteration 31/1000 | Loss: 0.00002091
Iteration 32/1000 | Loss: 0.00002091
Iteration 33/1000 | Loss: 0.00002090
Iteration 34/1000 | Loss: 0.00002090
Iteration 35/1000 | Loss: 0.00002089
Iteration 36/1000 | Loss: 0.00002086
Iteration 37/1000 | Loss: 0.00002086
Iteration 38/1000 | Loss: 0.00002086
Iteration 39/1000 | Loss: 0.00002086
Iteration 40/1000 | Loss: 0.00002086
Iteration 41/1000 | Loss: 0.00002086
Iteration 42/1000 | Loss: 0.00002086
Iteration 43/1000 | Loss: 0.00002086
Iteration 44/1000 | Loss: 0.00002085
Iteration 45/1000 | Loss: 0.00002085
Iteration 46/1000 | Loss: 0.00002085
Iteration 47/1000 | Loss: 0.00002085
Iteration 48/1000 | Loss: 0.00002084
Iteration 49/1000 | Loss: 0.00002084
Iteration 50/1000 | Loss: 0.00002083
Iteration 51/1000 | Loss: 0.00002083
Iteration 52/1000 | Loss: 0.00002082
Iteration 53/1000 | Loss: 0.00002082
Iteration 54/1000 | Loss: 0.00002082
Iteration 55/1000 | Loss: 0.00002082
Iteration 56/1000 | Loss: 0.00002081
Iteration 57/1000 | Loss: 0.00002081
Iteration 58/1000 | Loss: 0.00002081
Iteration 59/1000 | Loss: 0.00002081
Iteration 60/1000 | Loss: 0.00002081
Iteration 61/1000 | Loss: 0.00002080
Iteration 62/1000 | Loss: 0.00002080
Iteration 63/1000 | Loss: 0.00002080
Iteration 64/1000 | Loss: 0.00002080
Iteration 65/1000 | Loss: 0.00002080
Iteration 66/1000 | Loss: 0.00002080
Iteration 67/1000 | Loss: 0.00002080
Iteration 68/1000 | Loss: 0.00002080
Iteration 69/1000 | Loss: 0.00002080
Iteration 70/1000 | Loss: 0.00002080
Iteration 71/1000 | Loss: 0.00002080
Iteration 72/1000 | Loss: 0.00002079
Iteration 73/1000 | Loss: 0.00002079
Iteration 74/1000 | Loss: 0.00002079
Iteration 75/1000 | Loss: 0.00002079
Iteration 76/1000 | Loss: 0.00002078
Iteration 77/1000 | Loss: 0.00002078
Iteration 78/1000 | Loss: 0.00002078
Iteration 79/1000 | Loss: 0.00002078
Iteration 80/1000 | Loss: 0.00002078
Iteration 81/1000 | Loss: 0.00002078
Iteration 82/1000 | Loss: 0.00002078
Iteration 83/1000 | Loss: 0.00002078
Iteration 84/1000 | Loss: 0.00002078
Iteration 85/1000 | Loss: 0.00002077
Iteration 86/1000 | Loss: 0.00002077
Iteration 87/1000 | Loss: 0.00002077
Iteration 88/1000 | Loss: 0.00002077
Iteration 89/1000 | Loss: 0.00002076
Iteration 90/1000 | Loss: 0.00002076
Iteration 91/1000 | Loss: 0.00002076
Iteration 92/1000 | Loss: 0.00002076
Iteration 93/1000 | Loss: 0.00002076
Iteration 94/1000 | Loss: 0.00002076
Iteration 95/1000 | Loss: 0.00002076
Iteration 96/1000 | Loss: 0.00002076
Iteration 97/1000 | Loss: 0.00002076
Iteration 98/1000 | Loss: 0.00002076
Iteration 99/1000 | Loss: 0.00002076
Iteration 100/1000 | Loss: 0.00002075
Iteration 101/1000 | Loss: 0.00002075
Iteration 102/1000 | Loss: 0.00002075
Iteration 103/1000 | Loss: 0.00002075
Iteration 104/1000 | Loss: 0.00002075
Iteration 105/1000 | Loss: 0.00002074
Iteration 106/1000 | Loss: 0.00002074
Iteration 107/1000 | Loss: 0.00002074
Iteration 108/1000 | Loss: 0.00002074
Iteration 109/1000 | Loss: 0.00002074
Iteration 110/1000 | Loss: 0.00002074
Iteration 111/1000 | Loss: 0.00002074
Iteration 112/1000 | Loss: 0.00002074
Iteration 113/1000 | Loss: 0.00002073
Iteration 114/1000 | Loss: 0.00002073
Iteration 115/1000 | Loss: 0.00002073
Iteration 116/1000 | Loss: 0.00002073
Iteration 117/1000 | Loss: 0.00002073
Iteration 118/1000 | Loss: 0.00002073
Iteration 119/1000 | Loss: 0.00002073
Iteration 120/1000 | Loss: 0.00002073
Iteration 121/1000 | Loss: 0.00002073
Iteration 122/1000 | Loss: 0.00002073
Iteration 123/1000 | Loss: 0.00002073
Iteration 124/1000 | Loss: 0.00002072
Iteration 125/1000 | Loss: 0.00002072
Iteration 126/1000 | Loss: 0.00002072
Iteration 127/1000 | Loss: 0.00002072
Iteration 128/1000 | Loss: 0.00002072
Iteration 129/1000 | Loss: 0.00002072
Iteration 130/1000 | Loss: 0.00002072
Iteration 131/1000 | Loss: 0.00002072
Iteration 132/1000 | Loss: 0.00002072
Iteration 133/1000 | Loss: 0.00002072
Iteration 134/1000 | Loss: 0.00002072
Iteration 135/1000 | Loss: 0.00002072
Iteration 136/1000 | Loss: 0.00002072
Iteration 137/1000 | Loss: 0.00002072
Iteration 138/1000 | Loss: 0.00002072
Iteration 139/1000 | Loss: 0.00002072
Iteration 140/1000 | Loss: 0.00002071
Iteration 141/1000 | Loss: 0.00002071
Iteration 142/1000 | Loss: 0.00002071
Iteration 143/1000 | Loss: 0.00002071
Iteration 144/1000 | Loss: 0.00002071
Iteration 145/1000 | Loss: 0.00002071
Iteration 146/1000 | Loss: 0.00002071
Iteration 147/1000 | Loss: 0.00002071
Iteration 148/1000 | Loss: 0.00002071
Iteration 149/1000 | Loss: 0.00002071
Iteration 150/1000 | Loss: 0.00002071
Iteration 151/1000 | Loss: 0.00002071
Iteration 152/1000 | Loss: 0.00002070
Iteration 153/1000 | Loss: 0.00002070
Iteration 154/1000 | Loss: 0.00002070
Iteration 155/1000 | Loss: 0.00002070
Iteration 156/1000 | Loss: 0.00002070
Iteration 157/1000 | Loss: 0.00002070
Iteration 158/1000 | Loss: 0.00002070
Iteration 159/1000 | Loss: 0.00002070
Iteration 160/1000 | Loss: 0.00002070
Iteration 161/1000 | Loss: 0.00002070
Iteration 162/1000 | Loss: 0.00002070
Iteration 163/1000 | Loss: 0.00002069
Iteration 164/1000 | Loss: 0.00002069
Iteration 165/1000 | Loss: 0.00002069
Iteration 166/1000 | Loss: 0.00002069
Iteration 167/1000 | Loss: 0.00002069
Iteration 168/1000 | Loss: 0.00002069
Iteration 169/1000 | Loss: 0.00002069
Iteration 170/1000 | Loss: 0.00002069
Iteration 171/1000 | Loss: 0.00002069
Iteration 172/1000 | Loss: 0.00002069
Iteration 173/1000 | Loss: 0.00002069
Iteration 174/1000 | Loss: 0.00002068
Iteration 175/1000 | Loss: 0.00002068
Iteration 176/1000 | Loss: 0.00002068
Iteration 177/1000 | Loss: 0.00002068
Iteration 178/1000 | Loss: 0.00002067
Iteration 179/1000 | Loss: 0.00002067
Iteration 180/1000 | Loss: 0.00002067
Iteration 181/1000 | Loss: 0.00002067
Iteration 182/1000 | Loss: 0.00002067
Iteration 183/1000 | Loss: 0.00002066
Iteration 184/1000 | Loss: 0.00002066
Iteration 185/1000 | Loss: 0.00002066
Iteration 186/1000 | Loss: 0.00002066
Iteration 187/1000 | Loss: 0.00002066
Iteration 188/1000 | Loss: 0.00002066
Iteration 189/1000 | Loss: 0.00002066
Iteration 190/1000 | Loss: 0.00002065
Iteration 191/1000 | Loss: 0.00002065
Iteration 192/1000 | Loss: 0.00002065
Iteration 193/1000 | Loss: 0.00002065
Iteration 194/1000 | Loss: 0.00002065
Iteration 195/1000 | Loss: 0.00002065
Iteration 196/1000 | Loss: 0.00002065
Iteration 197/1000 | Loss: 0.00002065
Iteration 198/1000 | Loss: 0.00002065
Iteration 199/1000 | Loss: 0.00002065
Iteration 200/1000 | Loss: 0.00002065
Iteration 201/1000 | Loss: 0.00002065
Iteration 202/1000 | Loss: 0.00002064
Iteration 203/1000 | Loss: 0.00002064
Iteration 204/1000 | Loss: 0.00002064
Iteration 205/1000 | Loss: 0.00002064
Iteration 206/1000 | Loss: 0.00002064
Iteration 207/1000 | Loss: 0.00002064
Iteration 208/1000 | Loss: 0.00002064
Iteration 209/1000 | Loss: 0.00002064
Iteration 210/1000 | Loss: 0.00002064
Iteration 211/1000 | Loss: 0.00002064
Iteration 212/1000 | Loss: 0.00002063
Iteration 213/1000 | Loss: 0.00002063
Iteration 214/1000 | Loss: 0.00002063
Iteration 215/1000 | Loss: 0.00002063
Iteration 216/1000 | Loss: 0.00002063
Iteration 217/1000 | Loss: 0.00002063
Iteration 218/1000 | Loss: 0.00002063
Iteration 219/1000 | Loss: 0.00002063
Iteration 220/1000 | Loss: 0.00002063
Iteration 221/1000 | Loss: 0.00002063
Iteration 222/1000 | Loss: 0.00002063
Iteration 223/1000 | Loss: 0.00002063
Iteration 224/1000 | Loss: 0.00002063
Iteration 225/1000 | Loss: 0.00002063
Iteration 226/1000 | Loss: 0.00002063
Iteration 227/1000 | Loss: 0.00002062
Iteration 228/1000 | Loss: 0.00002062
Iteration 229/1000 | Loss: 0.00002062
Iteration 230/1000 | Loss: 0.00002062
Iteration 231/1000 | Loss: 0.00002062
Iteration 232/1000 | Loss: 0.00002062
Iteration 233/1000 | Loss: 0.00002062
Iteration 234/1000 | Loss: 0.00002062
Iteration 235/1000 | Loss: 0.00002062
Iteration 236/1000 | Loss: 0.00002062
Iteration 237/1000 | Loss: 0.00002062
Iteration 238/1000 | Loss: 0.00002062
Iteration 239/1000 | Loss: 0.00002062
Iteration 240/1000 | Loss: 0.00002062
Iteration 241/1000 | Loss: 0.00002062
Iteration 242/1000 | Loss: 0.00002062
Iteration 243/1000 | Loss: 0.00002062
Iteration 244/1000 | Loss: 0.00002061
Iteration 245/1000 | Loss: 0.00002061
Iteration 246/1000 | Loss: 0.00002061
Iteration 247/1000 | Loss: 0.00002061
Iteration 248/1000 | Loss: 0.00002061
Iteration 249/1000 | Loss: 0.00002061
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 249. Stopping optimization.
Last 5 losses: [2.0614892491721548e-05, 2.0614892491721548e-05, 2.0614892491721548e-05, 2.0614892491721548e-05, 2.0614892491721548e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0614892491721548e-05

Optimization complete. Final v2v error: 3.586585283279419 mm

Highest mean error: 4.685502529144287 mm for frame 59

Lowest mean error: 2.7124838829040527 mm for frame 137

Saving results

Total time: 51.85375690460205
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_013/1077/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_013/1077.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_013/1077
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00833043
Iteration 2/25 | Loss: 0.00114367
Iteration 3/25 | Loss: 0.00106845
Iteration 4/25 | Loss: 0.00105559
Iteration 5/25 | Loss: 0.00105229
Iteration 6/25 | Loss: 0.00105211
Iteration 7/25 | Loss: 0.00105211
Iteration 8/25 | Loss: 0.00105211
Iteration 9/25 | Loss: 0.00105211
Iteration 10/25 | Loss: 0.00105211
Iteration 11/25 | Loss: 0.00105211
Iteration 12/25 | Loss: 0.00105211
Iteration 13/25 | Loss: 0.00105211
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0010521071963012218, 0.0010521071963012218, 0.0010521071963012218, 0.0010521071963012218, 0.0010521071963012218]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010521071963012218

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39245713
Iteration 2/25 | Loss: 0.00065794
Iteration 3/25 | Loss: 0.00065794
Iteration 4/25 | Loss: 0.00065794
Iteration 5/25 | Loss: 0.00065794
Iteration 6/25 | Loss: 0.00065793
Iteration 7/25 | Loss: 0.00065793
Iteration 8/25 | Loss: 0.00065793
Iteration 9/25 | Loss: 0.00065793
Iteration 10/25 | Loss: 0.00065793
Iteration 11/25 | Loss: 0.00065793
Iteration 12/25 | Loss: 0.00065793
Iteration 13/25 | Loss: 0.00065793
Iteration 14/25 | Loss: 0.00065793
Iteration 15/25 | Loss: 0.00065793
Iteration 16/25 | Loss: 0.00065793
Iteration 17/25 | Loss: 0.00065793
Iteration 18/25 | Loss: 0.00065793
Iteration 19/25 | Loss: 0.00065793
Iteration 20/25 | Loss: 0.00065793
Iteration 21/25 | Loss: 0.00065793
Iteration 22/25 | Loss: 0.00065793
Iteration 23/25 | Loss: 0.00065793
Iteration 24/25 | Loss: 0.00065793
Iteration 25/25 | Loss: 0.00065793

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00065793
Iteration 2/1000 | Loss: 0.00002048
Iteration 3/1000 | Loss: 0.00001506
Iteration 4/1000 | Loss: 0.00001370
Iteration 5/1000 | Loss: 0.00001301
Iteration 6/1000 | Loss: 0.00001259
Iteration 7/1000 | Loss: 0.00001220
Iteration 8/1000 | Loss: 0.00001202
Iteration 9/1000 | Loss: 0.00001186
Iteration 10/1000 | Loss: 0.00001163
Iteration 11/1000 | Loss: 0.00001151
Iteration 12/1000 | Loss: 0.00001151
Iteration 13/1000 | Loss: 0.00001148
Iteration 14/1000 | Loss: 0.00001148
Iteration 15/1000 | Loss: 0.00001147
Iteration 16/1000 | Loss: 0.00001147
Iteration 17/1000 | Loss: 0.00001147
Iteration 18/1000 | Loss: 0.00001147
Iteration 19/1000 | Loss: 0.00001147
Iteration 20/1000 | Loss: 0.00001147
Iteration 21/1000 | Loss: 0.00001145
Iteration 22/1000 | Loss: 0.00001137
Iteration 23/1000 | Loss: 0.00001131
Iteration 24/1000 | Loss: 0.00001128
Iteration 25/1000 | Loss: 0.00001127
Iteration 26/1000 | Loss: 0.00001125
Iteration 27/1000 | Loss: 0.00001125
Iteration 28/1000 | Loss: 0.00001124
Iteration 29/1000 | Loss: 0.00001124
Iteration 30/1000 | Loss: 0.00001123
Iteration 31/1000 | Loss: 0.00001123
Iteration 32/1000 | Loss: 0.00001123
Iteration 33/1000 | Loss: 0.00001122
Iteration 34/1000 | Loss: 0.00001122
Iteration 35/1000 | Loss: 0.00001122
Iteration 36/1000 | Loss: 0.00001122
Iteration 37/1000 | Loss: 0.00001121
Iteration 38/1000 | Loss: 0.00001119
Iteration 39/1000 | Loss: 0.00001119
Iteration 40/1000 | Loss: 0.00001119
Iteration 41/1000 | Loss: 0.00001119
Iteration 42/1000 | Loss: 0.00001119
Iteration 43/1000 | Loss: 0.00001119
Iteration 44/1000 | Loss: 0.00001119
Iteration 45/1000 | Loss: 0.00001119
Iteration 46/1000 | Loss: 0.00001119
Iteration 47/1000 | Loss: 0.00001118
Iteration 48/1000 | Loss: 0.00001118
Iteration 49/1000 | Loss: 0.00001118
Iteration 50/1000 | Loss: 0.00001117
Iteration 51/1000 | Loss: 0.00001117
Iteration 52/1000 | Loss: 0.00001116
Iteration 53/1000 | Loss: 0.00001116
Iteration 54/1000 | Loss: 0.00001116
Iteration 55/1000 | Loss: 0.00001116
Iteration 56/1000 | Loss: 0.00001115
Iteration 57/1000 | Loss: 0.00001115
Iteration 58/1000 | Loss: 0.00001115
Iteration 59/1000 | Loss: 0.00001114
Iteration 60/1000 | Loss: 0.00001113
Iteration 61/1000 | Loss: 0.00001113
Iteration 62/1000 | Loss: 0.00001113
Iteration 63/1000 | Loss: 0.00001112
Iteration 64/1000 | Loss: 0.00001110
Iteration 65/1000 | Loss: 0.00001110
Iteration 66/1000 | Loss: 0.00001110
Iteration 67/1000 | Loss: 0.00001110
Iteration 68/1000 | Loss: 0.00001110
Iteration 69/1000 | Loss: 0.00001110
Iteration 70/1000 | Loss: 0.00001110
Iteration 71/1000 | Loss: 0.00001110
Iteration 72/1000 | Loss: 0.00001110
Iteration 73/1000 | Loss: 0.00001109
Iteration 74/1000 | Loss: 0.00001109
Iteration 75/1000 | Loss: 0.00001109
Iteration 76/1000 | Loss: 0.00001108
Iteration 77/1000 | Loss: 0.00001108
Iteration 78/1000 | Loss: 0.00001107
Iteration 79/1000 | Loss: 0.00001107
Iteration 80/1000 | Loss: 0.00001106
Iteration 81/1000 | Loss: 0.00001105
Iteration 82/1000 | Loss: 0.00001105
Iteration 83/1000 | Loss: 0.00001105
Iteration 84/1000 | Loss: 0.00001104
Iteration 85/1000 | Loss: 0.00001104
Iteration 86/1000 | Loss: 0.00001104
Iteration 87/1000 | Loss: 0.00001104
Iteration 88/1000 | Loss: 0.00001104
Iteration 89/1000 | Loss: 0.00001104
Iteration 90/1000 | Loss: 0.00001104
Iteration 91/1000 | Loss: 0.00001104
Iteration 92/1000 | Loss: 0.00001104
Iteration 93/1000 | Loss: 0.00001103
Iteration 94/1000 | Loss: 0.00001103
Iteration 95/1000 | Loss: 0.00001103
Iteration 96/1000 | Loss: 0.00001103
Iteration 97/1000 | Loss: 0.00001102
Iteration 98/1000 | Loss: 0.00001102
Iteration 99/1000 | Loss: 0.00001101
Iteration 100/1000 | Loss: 0.00001101
Iteration 101/1000 | Loss: 0.00001101
Iteration 102/1000 | Loss: 0.00001101
Iteration 103/1000 | Loss: 0.00001101
Iteration 104/1000 | Loss: 0.00001101
Iteration 105/1000 | Loss: 0.00001101
Iteration 106/1000 | Loss: 0.00001101
Iteration 107/1000 | Loss: 0.00001101
Iteration 108/1000 | Loss: 0.00001101
Iteration 109/1000 | Loss: 0.00001100
Iteration 110/1000 | Loss: 0.00001100
Iteration 111/1000 | Loss: 0.00001099
Iteration 112/1000 | Loss: 0.00001099
Iteration 113/1000 | Loss: 0.00001099
Iteration 114/1000 | Loss: 0.00001099
Iteration 115/1000 | Loss: 0.00001099
Iteration 116/1000 | Loss: 0.00001099
Iteration 117/1000 | Loss: 0.00001099
Iteration 118/1000 | Loss: 0.00001098
Iteration 119/1000 | Loss: 0.00001098
Iteration 120/1000 | Loss: 0.00001098
Iteration 121/1000 | Loss: 0.00001098
Iteration 122/1000 | Loss: 0.00001098
Iteration 123/1000 | Loss: 0.00001098
Iteration 124/1000 | Loss: 0.00001098
Iteration 125/1000 | Loss: 0.00001097
Iteration 126/1000 | Loss: 0.00001097
Iteration 127/1000 | Loss: 0.00001097
Iteration 128/1000 | Loss: 0.00001097
Iteration 129/1000 | Loss: 0.00001097
Iteration 130/1000 | Loss: 0.00001097
Iteration 131/1000 | Loss: 0.00001097
Iteration 132/1000 | Loss: 0.00001097
Iteration 133/1000 | Loss: 0.00001097
Iteration 134/1000 | Loss: 0.00001097
Iteration 135/1000 | Loss: 0.00001096
Iteration 136/1000 | Loss: 0.00001096
Iteration 137/1000 | Loss: 0.00001096
Iteration 138/1000 | Loss: 0.00001096
Iteration 139/1000 | Loss: 0.00001095
Iteration 140/1000 | Loss: 0.00001095
Iteration 141/1000 | Loss: 0.00001095
Iteration 142/1000 | Loss: 0.00001095
Iteration 143/1000 | Loss: 0.00001095
Iteration 144/1000 | Loss: 0.00001095
Iteration 145/1000 | Loss: 0.00001095
Iteration 146/1000 | Loss: 0.00001094
Iteration 147/1000 | Loss: 0.00001094
Iteration 148/1000 | Loss: 0.00001093
Iteration 149/1000 | Loss: 0.00001093
Iteration 150/1000 | Loss: 0.00001093
Iteration 151/1000 | Loss: 0.00001093
Iteration 152/1000 | Loss: 0.00001093
Iteration 153/1000 | Loss: 0.00001093
Iteration 154/1000 | Loss: 0.00001093
Iteration 155/1000 | Loss: 0.00001093
Iteration 156/1000 | Loss: 0.00001092
Iteration 157/1000 | Loss: 0.00001092
Iteration 158/1000 | Loss: 0.00001092
Iteration 159/1000 | Loss: 0.00001092
Iteration 160/1000 | Loss: 0.00001091
Iteration 161/1000 | Loss: 0.00001091
Iteration 162/1000 | Loss: 0.00001090
Iteration 163/1000 | Loss: 0.00001090
Iteration 164/1000 | Loss: 0.00001090
Iteration 165/1000 | Loss: 0.00001090
Iteration 166/1000 | Loss: 0.00001090
Iteration 167/1000 | Loss: 0.00001089
Iteration 168/1000 | Loss: 0.00001089
Iteration 169/1000 | Loss: 0.00001089
Iteration 170/1000 | Loss: 0.00001089
Iteration 171/1000 | Loss: 0.00001088
Iteration 172/1000 | Loss: 0.00001088
Iteration 173/1000 | Loss: 0.00001088
Iteration 174/1000 | Loss: 0.00001088
Iteration 175/1000 | Loss: 0.00001088
Iteration 176/1000 | Loss: 0.00001088
Iteration 177/1000 | Loss: 0.00001088
Iteration 178/1000 | Loss: 0.00001088
Iteration 179/1000 | Loss: 0.00001088
Iteration 180/1000 | Loss: 0.00001088
Iteration 181/1000 | Loss: 0.00001088
Iteration 182/1000 | Loss: 0.00001088
Iteration 183/1000 | Loss: 0.00001088
Iteration 184/1000 | Loss: 0.00001087
Iteration 185/1000 | Loss: 0.00001087
Iteration 186/1000 | Loss: 0.00001087
Iteration 187/1000 | Loss: 0.00001087
Iteration 188/1000 | Loss: 0.00001087
Iteration 189/1000 | Loss: 0.00001087
Iteration 190/1000 | Loss: 0.00001087
Iteration 191/1000 | Loss: 0.00001087
Iteration 192/1000 | Loss: 0.00001087
Iteration 193/1000 | Loss: 0.00001087
Iteration 194/1000 | Loss: 0.00001087
Iteration 195/1000 | Loss: 0.00001087
Iteration 196/1000 | Loss: 0.00001087
Iteration 197/1000 | Loss: 0.00001087
Iteration 198/1000 | Loss: 0.00001087
Iteration 199/1000 | Loss: 0.00001087
Iteration 200/1000 | Loss: 0.00001087
Iteration 201/1000 | Loss: 0.00001087
Iteration 202/1000 | Loss: 0.00001087
Iteration 203/1000 | Loss: 0.00001087
Iteration 204/1000 | Loss: 0.00001087
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 204. Stopping optimization.
Last 5 losses: [1.0867718629015144e-05, 1.0867718629015144e-05, 1.0867718629015144e-05, 1.0867718629015144e-05, 1.0867718629015144e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0867718629015144e-05

Optimization complete. Final v2v error: 2.7930428981781006 mm

Highest mean error: 3.2353665828704834 mm for frame 97

Lowest mean error: 2.6551337242126465 mm for frame 144

Saving results

Total time: 42.40064191818237
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_013/1047/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_013/1047.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_013/1047
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00419492
Iteration 2/25 | Loss: 0.00121415
Iteration 3/25 | Loss: 0.00109363
Iteration 4/25 | Loss: 0.00108453
Iteration 5/25 | Loss: 0.00108297
Iteration 6/25 | Loss: 0.00108270
Iteration 7/25 | Loss: 0.00108270
Iteration 8/25 | Loss: 0.00108270
Iteration 9/25 | Loss: 0.00108270
Iteration 10/25 | Loss: 0.00108270
Iteration 11/25 | Loss: 0.00108270
Iteration 12/25 | Loss: 0.00108270
Iteration 13/25 | Loss: 0.00108270
Iteration 14/25 | Loss: 0.00108270
Iteration 15/25 | Loss: 0.00108270
Iteration 16/25 | Loss: 0.00108270
Iteration 17/25 | Loss: 0.00108270
Iteration 18/25 | Loss: 0.00108270
Iteration 19/25 | Loss: 0.00108270
Iteration 20/25 | Loss: 0.00108270
Iteration 21/25 | Loss: 0.00108270
Iteration 22/25 | Loss: 0.00108270
Iteration 23/25 | Loss: 0.00108270
Iteration 24/25 | Loss: 0.00108270
Iteration 25/25 | Loss: 0.00108270

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.07496977
Iteration 2/25 | Loss: 0.00057054
Iteration 3/25 | Loss: 0.00057052
Iteration 4/25 | Loss: 0.00057052
Iteration 5/25 | Loss: 0.00057052
Iteration 6/25 | Loss: 0.00057052
Iteration 7/25 | Loss: 0.00057052
Iteration 8/25 | Loss: 0.00057052
Iteration 9/25 | Loss: 0.00057052
Iteration 10/25 | Loss: 0.00057052
Iteration 11/25 | Loss: 0.00057052
Iteration 12/25 | Loss: 0.00057052
Iteration 13/25 | Loss: 0.00057052
Iteration 14/25 | Loss: 0.00057052
Iteration 15/25 | Loss: 0.00057052
Iteration 16/25 | Loss: 0.00057052
Iteration 17/25 | Loss: 0.00057052
Iteration 18/25 | Loss: 0.00057052
Iteration 19/25 | Loss: 0.00057052
Iteration 20/25 | Loss: 0.00057052
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0005705166840925813, 0.0005705166840925813, 0.0005705166840925813, 0.0005705166840925813, 0.0005705166840925813]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005705166840925813

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00057052
Iteration 2/1000 | Loss: 0.00002328
Iteration 3/1000 | Loss: 0.00001747
Iteration 4/1000 | Loss: 0.00001588
Iteration 5/1000 | Loss: 0.00001506
Iteration 6/1000 | Loss: 0.00001443
Iteration 7/1000 | Loss: 0.00001395
Iteration 8/1000 | Loss: 0.00001384
Iteration 9/1000 | Loss: 0.00001379
Iteration 10/1000 | Loss: 0.00001354
Iteration 11/1000 | Loss: 0.00001331
Iteration 12/1000 | Loss: 0.00001324
Iteration 13/1000 | Loss: 0.00001321
Iteration 14/1000 | Loss: 0.00001321
Iteration 15/1000 | Loss: 0.00001315
Iteration 16/1000 | Loss: 0.00001311
Iteration 17/1000 | Loss: 0.00001309
Iteration 18/1000 | Loss: 0.00001305
Iteration 19/1000 | Loss: 0.00001302
Iteration 20/1000 | Loss: 0.00001301
Iteration 21/1000 | Loss: 0.00001300
Iteration 22/1000 | Loss: 0.00001296
Iteration 23/1000 | Loss: 0.00001295
Iteration 24/1000 | Loss: 0.00001295
Iteration 25/1000 | Loss: 0.00001294
Iteration 26/1000 | Loss: 0.00001293
Iteration 27/1000 | Loss: 0.00001292
Iteration 28/1000 | Loss: 0.00001291
Iteration 29/1000 | Loss: 0.00001290
Iteration 30/1000 | Loss: 0.00001290
Iteration 31/1000 | Loss: 0.00001288
Iteration 32/1000 | Loss: 0.00001287
Iteration 33/1000 | Loss: 0.00001287
Iteration 34/1000 | Loss: 0.00001287
Iteration 35/1000 | Loss: 0.00001287
Iteration 36/1000 | Loss: 0.00001287
Iteration 37/1000 | Loss: 0.00001287
Iteration 38/1000 | Loss: 0.00001286
Iteration 39/1000 | Loss: 0.00001286
Iteration 40/1000 | Loss: 0.00001285
Iteration 41/1000 | Loss: 0.00001285
Iteration 42/1000 | Loss: 0.00001284
Iteration 43/1000 | Loss: 0.00001284
Iteration 44/1000 | Loss: 0.00001283
Iteration 45/1000 | Loss: 0.00001282
Iteration 46/1000 | Loss: 0.00001282
Iteration 47/1000 | Loss: 0.00001282
Iteration 48/1000 | Loss: 0.00001281
Iteration 49/1000 | Loss: 0.00001281
Iteration 50/1000 | Loss: 0.00001281
Iteration 51/1000 | Loss: 0.00001280
Iteration 52/1000 | Loss: 0.00001279
Iteration 53/1000 | Loss: 0.00001279
Iteration 54/1000 | Loss: 0.00001278
Iteration 55/1000 | Loss: 0.00001278
Iteration 56/1000 | Loss: 0.00001278
Iteration 57/1000 | Loss: 0.00001277
Iteration 58/1000 | Loss: 0.00001276
Iteration 59/1000 | Loss: 0.00001276
Iteration 60/1000 | Loss: 0.00001276
Iteration 61/1000 | Loss: 0.00001276
Iteration 62/1000 | Loss: 0.00001275
Iteration 63/1000 | Loss: 0.00001275
Iteration 64/1000 | Loss: 0.00001274
Iteration 65/1000 | Loss: 0.00001273
Iteration 66/1000 | Loss: 0.00001272
Iteration 67/1000 | Loss: 0.00001271
Iteration 68/1000 | Loss: 0.00001270
Iteration 69/1000 | Loss: 0.00001269
Iteration 70/1000 | Loss: 0.00001267
Iteration 71/1000 | Loss: 0.00001267
Iteration 72/1000 | Loss: 0.00001266
Iteration 73/1000 | Loss: 0.00001266
Iteration 74/1000 | Loss: 0.00001266
Iteration 75/1000 | Loss: 0.00001265
Iteration 76/1000 | Loss: 0.00001265
Iteration 77/1000 | Loss: 0.00001264
Iteration 78/1000 | Loss: 0.00001264
Iteration 79/1000 | Loss: 0.00001264
Iteration 80/1000 | Loss: 0.00001264
Iteration 81/1000 | Loss: 0.00001264
Iteration 82/1000 | Loss: 0.00001264
Iteration 83/1000 | Loss: 0.00001263
Iteration 84/1000 | Loss: 0.00001263
Iteration 85/1000 | Loss: 0.00001263
Iteration 86/1000 | Loss: 0.00001262
Iteration 87/1000 | Loss: 0.00001262
Iteration 88/1000 | Loss: 0.00001261
Iteration 89/1000 | Loss: 0.00001261
Iteration 90/1000 | Loss: 0.00001261
Iteration 91/1000 | Loss: 0.00001261
Iteration 92/1000 | Loss: 0.00001261
Iteration 93/1000 | Loss: 0.00001261
Iteration 94/1000 | Loss: 0.00001260
Iteration 95/1000 | Loss: 0.00001260
Iteration 96/1000 | Loss: 0.00001260
Iteration 97/1000 | Loss: 0.00001260
Iteration 98/1000 | Loss: 0.00001259
Iteration 99/1000 | Loss: 0.00001259
Iteration 100/1000 | Loss: 0.00001259
Iteration 101/1000 | Loss: 0.00001259
Iteration 102/1000 | Loss: 0.00001258
Iteration 103/1000 | Loss: 0.00001258
Iteration 104/1000 | Loss: 0.00001258
Iteration 105/1000 | Loss: 0.00001258
Iteration 106/1000 | Loss: 0.00001258
Iteration 107/1000 | Loss: 0.00001258
Iteration 108/1000 | Loss: 0.00001258
Iteration 109/1000 | Loss: 0.00001258
Iteration 110/1000 | Loss: 0.00001258
Iteration 111/1000 | Loss: 0.00001257
Iteration 112/1000 | Loss: 0.00001257
Iteration 113/1000 | Loss: 0.00001257
Iteration 114/1000 | Loss: 0.00001257
Iteration 115/1000 | Loss: 0.00001257
Iteration 116/1000 | Loss: 0.00001257
Iteration 117/1000 | Loss: 0.00001257
Iteration 118/1000 | Loss: 0.00001257
Iteration 119/1000 | Loss: 0.00001256
Iteration 120/1000 | Loss: 0.00001256
Iteration 121/1000 | Loss: 0.00001256
Iteration 122/1000 | Loss: 0.00001256
Iteration 123/1000 | Loss: 0.00001256
Iteration 124/1000 | Loss: 0.00001256
Iteration 125/1000 | Loss: 0.00001256
Iteration 126/1000 | Loss: 0.00001256
Iteration 127/1000 | Loss: 0.00001256
Iteration 128/1000 | Loss: 0.00001256
Iteration 129/1000 | Loss: 0.00001256
Iteration 130/1000 | Loss: 0.00001256
Iteration 131/1000 | Loss: 0.00001256
Iteration 132/1000 | Loss: 0.00001255
Iteration 133/1000 | Loss: 0.00001255
Iteration 134/1000 | Loss: 0.00001255
Iteration 135/1000 | Loss: 0.00001255
Iteration 136/1000 | Loss: 0.00001255
Iteration 137/1000 | Loss: 0.00001255
Iteration 138/1000 | Loss: 0.00001254
Iteration 139/1000 | Loss: 0.00001254
Iteration 140/1000 | Loss: 0.00001254
Iteration 141/1000 | Loss: 0.00001254
Iteration 142/1000 | Loss: 0.00001254
Iteration 143/1000 | Loss: 0.00001254
Iteration 144/1000 | Loss: 0.00001254
Iteration 145/1000 | Loss: 0.00001254
Iteration 146/1000 | Loss: 0.00001254
Iteration 147/1000 | Loss: 0.00001254
Iteration 148/1000 | Loss: 0.00001254
Iteration 149/1000 | Loss: 0.00001254
Iteration 150/1000 | Loss: 0.00001254
Iteration 151/1000 | Loss: 0.00001254
Iteration 152/1000 | Loss: 0.00001254
Iteration 153/1000 | Loss: 0.00001254
Iteration 154/1000 | Loss: 0.00001253
Iteration 155/1000 | Loss: 0.00001253
Iteration 156/1000 | Loss: 0.00001253
Iteration 157/1000 | Loss: 0.00001253
Iteration 158/1000 | Loss: 0.00001253
Iteration 159/1000 | Loss: 0.00001253
Iteration 160/1000 | Loss: 0.00001253
Iteration 161/1000 | Loss: 0.00001253
Iteration 162/1000 | Loss: 0.00001253
Iteration 163/1000 | Loss: 0.00001253
Iteration 164/1000 | Loss: 0.00001253
Iteration 165/1000 | Loss: 0.00001253
Iteration 166/1000 | Loss: 0.00001253
Iteration 167/1000 | Loss: 0.00001253
Iteration 168/1000 | Loss: 0.00001253
Iteration 169/1000 | Loss: 0.00001253
Iteration 170/1000 | Loss: 0.00001253
Iteration 171/1000 | Loss: 0.00001253
Iteration 172/1000 | Loss: 0.00001253
Iteration 173/1000 | Loss: 0.00001253
Iteration 174/1000 | Loss: 0.00001253
Iteration 175/1000 | Loss: 0.00001253
Iteration 176/1000 | Loss: 0.00001253
Iteration 177/1000 | Loss: 0.00001253
Iteration 178/1000 | Loss: 0.00001253
Iteration 179/1000 | Loss: 0.00001253
Iteration 180/1000 | Loss: 0.00001253
Iteration 181/1000 | Loss: 0.00001253
Iteration 182/1000 | Loss: 0.00001253
Iteration 183/1000 | Loss: 0.00001253
Iteration 184/1000 | Loss: 0.00001253
Iteration 185/1000 | Loss: 0.00001253
Iteration 186/1000 | Loss: 0.00001253
Iteration 187/1000 | Loss: 0.00001253
Iteration 188/1000 | Loss: 0.00001253
Iteration 189/1000 | Loss: 0.00001253
Iteration 190/1000 | Loss: 0.00001253
Iteration 191/1000 | Loss: 0.00001253
Iteration 192/1000 | Loss: 0.00001253
Iteration 193/1000 | Loss: 0.00001253
Iteration 194/1000 | Loss: 0.00001253
Iteration 195/1000 | Loss: 0.00001253
Iteration 196/1000 | Loss: 0.00001253
Iteration 197/1000 | Loss: 0.00001253
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 197. Stopping optimization.
Last 5 losses: [1.2525621968961786e-05, 1.2525621968961786e-05, 1.2525621968961786e-05, 1.2525621968961786e-05, 1.2525621968961786e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2525621968961786e-05

Optimization complete. Final v2v error: 3.0011889934539795 mm

Highest mean error: 3.6139585971832275 mm for frame 104

Lowest mean error: 2.676300525665283 mm for frame 1

Saving results

Total time: 38.953370332717896
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_013/1059/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_013/1059.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_013/1059
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00398213
Iteration 2/25 | Loss: 0.00116361
Iteration 3/25 | Loss: 0.00105996
Iteration 4/25 | Loss: 0.00105218
Iteration 5/25 | Loss: 0.00105032
Iteration 6/25 | Loss: 0.00105030
Iteration 7/25 | Loss: 0.00105030
Iteration 8/25 | Loss: 0.00105030
Iteration 9/25 | Loss: 0.00105030
Iteration 10/25 | Loss: 0.00105030
Iteration 11/25 | Loss: 0.00105030
Iteration 12/25 | Loss: 0.00105030
Iteration 13/25 | Loss: 0.00105030
Iteration 14/25 | Loss: 0.00105030
Iteration 15/25 | Loss: 0.00105030
Iteration 16/25 | Loss: 0.00105030
Iteration 17/25 | Loss: 0.00105030
Iteration 18/25 | Loss: 0.00105030
Iteration 19/25 | Loss: 0.00105030
Iteration 20/25 | Loss: 0.00105030
Iteration 21/25 | Loss: 0.00105030
Iteration 22/25 | Loss: 0.00105030
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0010502992663532495, 0.0010502992663532495, 0.0010502992663532495, 0.0010502992663532495, 0.0010502992663532495]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010502992663532495

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37234282
Iteration 2/25 | Loss: 0.00052968
Iteration 3/25 | Loss: 0.00052968
Iteration 4/25 | Loss: 0.00052968
Iteration 5/25 | Loss: 0.00052968
Iteration 6/25 | Loss: 0.00052968
Iteration 7/25 | Loss: 0.00052968
Iteration 8/25 | Loss: 0.00052968
Iteration 9/25 | Loss: 0.00052967
Iteration 10/25 | Loss: 0.00052967
Iteration 11/25 | Loss: 0.00052967
Iteration 12/25 | Loss: 0.00052967
Iteration 13/25 | Loss: 0.00052967
Iteration 14/25 | Loss: 0.00052967
Iteration 15/25 | Loss: 0.00052967
Iteration 16/25 | Loss: 0.00052967
Iteration 17/25 | Loss: 0.00052967
Iteration 18/25 | Loss: 0.00052967
Iteration 19/25 | Loss: 0.00052967
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0005296742310747504, 0.0005296742310747504, 0.0005296742310747504, 0.0005296742310747504, 0.0005296742310747504]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005296742310747504

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00052967
Iteration 2/1000 | Loss: 0.00002119
Iteration 3/1000 | Loss: 0.00001603
Iteration 4/1000 | Loss: 0.00001460
Iteration 5/1000 | Loss: 0.00001363
Iteration 6/1000 | Loss: 0.00001297
Iteration 7/1000 | Loss: 0.00001255
Iteration 8/1000 | Loss: 0.00001225
Iteration 9/1000 | Loss: 0.00001225
Iteration 10/1000 | Loss: 0.00001199
Iteration 11/1000 | Loss: 0.00001177
Iteration 12/1000 | Loss: 0.00001171
Iteration 13/1000 | Loss: 0.00001166
Iteration 14/1000 | Loss: 0.00001166
Iteration 15/1000 | Loss: 0.00001165
Iteration 16/1000 | Loss: 0.00001165
Iteration 17/1000 | Loss: 0.00001164
Iteration 18/1000 | Loss: 0.00001164
Iteration 19/1000 | Loss: 0.00001154
Iteration 20/1000 | Loss: 0.00001147
Iteration 21/1000 | Loss: 0.00001144
Iteration 22/1000 | Loss: 0.00001143
Iteration 23/1000 | Loss: 0.00001143
Iteration 24/1000 | Loss: 0.00001140
Iteration 25/1000 | Loss: 0.00001140
Iteration 26/1000 | Loss: 0.00001140
Iteration 27/1000 | Loss: 0.00001140
Iteration 28/1000 | Loss: 0.00001139
Iteration 29/1000 | Loss: 0.00001139
Iteration 30/1000 | Loss: 0.00001137
Iteration 31/1000 | Loss: 0.00001137
Iteration 32/1000 | Loss: 0.00001136
Iteration 33/1000 | Loss: 0.00001136
Iteration 34/1000 | Loss: 0.00001134
Iteration 35/1000 | Loss: 0.00001133
Iteration 36/1000 | Loss: 0.00001132
Iteration 37/1000 | Loss: 0.00001132
Iteration 38/1000 | Loss: 0.00001132
Iteration 39/1000 | Loss: 0.00001131
Iteration 40/1000 | Loss: 0.00001131
Iteration 41/1000 | Loss: 0.00001129
Iteration 42/1000 | Loss: 0.00001128
Iteration 43/1000 | Loss: 0.00001128
Iteration 44/1000 | Loss: 0.00001127
Iteration 45/1000 | Loss: 0.00001126
Iteration 46/1000 | Loss: 0.00001123
Iteration 47/1000 | Loss: 0.00001122
Iteration 48/1000 | Loss: 0.00001122
Iteration 49/1000 | Loss: 0.00001121
Iteration 50/1000 | Loss: 0.00001121
Iteration 51/1000 | Loss: 0.00001120
Iteration 52/1000 | Loss: 0.00001120
Iteration 53/1000 | Loss: 0.00001117
Iteration 54/1000 | Loss: 0.00001117
Iteration 55/1000 | Loss: 0.00001116
Iteration 56/1000 | Loss: 0.00001116
Iteration 57/1000 | Loss: 0.00001115
Iteration 58/1000 | Loss: 0.00001114
Iteration 59/1000 | Loss: 0.00001113
Iteration 60/1000 | Loss: 0.00001112
Iteration 61/1000 | Loss: 0.00001111
Iteration 62/1000 | Loss: 0.00001110
Iteration 63/1000 | Loss: 0.00001109
Iteration 64/1000 | Loss: 0.00001109
Iteration 65/1000 | Loss: 0.00001108
Iteration 66/1000 | Loss: 0.00001108
Iteration 67/1000 | Loss: 0.00001108
Iteration 68/1000 | Loss: 0.00001108
Iteration 69/1000 | Loss: 0.00001108
Iteration 70/1000 | Loss: 0.00001108
Iteration 71/1000 | Loss: 0.00001108
Iteration 72/1000 | Loss: 0.00001107
Iteration 73/1000 | Loss: 0.00001107
Iteration 74/1000 | Loss: 0.00001107
Iteration 75/1000 | Loss: 0.00001106
Iteration 76/1000 | Loss: 0.00001106
Iteration 77/1000 | Loss: 0.00001106
Iteration 78/1000 | Loss: 0.00001105
Iteration 79/1000 | Loss: 0.00001105
Iteration 80/1000 | Loss: 0.00001105
Iteration 81/1000 | Loss: 0.00001105
Iteration 82/1000 | Loss: 0.00001105
Iteration 83/1000 | Loss: 0.00001105
Iteration 84/1000 | Loss: 0.00001104
Iteration 85/1000 | Loss: 0.00001104
Iteration 86/1000 | Loss: 0.00001104
Iteration 87/1000 | Loss: 0.00001104
Iteration 88/1000 | Loss: 0.00001104
Iteration 89/1000 | Loss: 0.00001104
Iteration 90/1000 | Loss: 0.00001104
Iteration 91/1000 | Loss: 0.00001104
Iteration 92/1000 | Loss: 0.00001103
Iteration 93/1000 | Loss: 0.00001103
Iteration 94/1000 | Loss: 0.00001102
Iteration 95/1000 | Loss: 0.00001102
Iteration 96/1000 | Loss: 0.00001102
Iteration 97/1000 | Loss: 0.00001101
Iteration 98/1000 | Loss: 0.00001101
Iteration 99/1000 | Loss: 0.00001101
Iteration 100/1000 | Loss: 0.00001101
Iteration 101/1000 | Loss: 0.00001101
Iteration 102/1000 | Loss: 0.00001101
Iteration 103/1000 | Loss: 0.00001101
Iteration 104/1000 | Loss: 0.00001100
Iteration 105/1000 | Loss: 0.00001100
Iteration 106/1000 | Loss: 0.00001100
Iteration 107/1000 | Loss: 0.00001100
Iteration 108/1000 | Loss: 0.00001100
Iteration 109/1000 | Loss: 0.00001100
Iteration 110/1000 | Loss: 0.00001099
Iteration 111/1000 | Loss: 0.00001099
Iteration 112/1000 | Loss: 0.00001098
Iteration 113/1000 | Loss: 0.00001098
Iteration 114/1000 | Loss: 0.00001098
Iteration 115/1000 | Loss: 0.00001098
Iteration 116/1000 | Loss: 0.00001098
Iteration 117/1000 | Loss: 0.00001098
Iteration 118/1000 | Loss: 0.00001098
Iteration 119/1000 | Loss: 0.00001097
Iteration 120/1000 | Loss: 0.00001097
Iteration 121/1000 | Loss: 0.00001097
Iteration 122/1000 | Loss: 0.00001096
Iteration 123/1000 | Loss: 0.00001096
Iteration 124/1000 | Loss: 0.00001096
Iteration 125/1000 | Loss: 0.00001095
Iteration 126/1000 | Loss: 0.00001095
Iteration 127/1000 | Loss: 0.00001095
Iteration 128/1000 | Loss: 0.00001095
Iteration 129/1000 | Loss: 0.00001094
Iteration 130/1000 | Loss: 0.00001094
Iteration 131/1000 | Loss: 0.00001094
Iteration 132/1000 | Loss: 0.00001094
Iteration 133/1000 | Loss: 0.00001094
Iteration 134/1000 | Loss: 0.00001093
Iteration 135/1000 | Loss: 0.00001093
Iteration 136/1000 | Loss: 0.00001093
Iteration 137/1000 | Loss: 0.00001093
Iteration 138/1000 | Loss: 0.00001092
Iteration 139/1000 | Loss: 0.00001092
Iteration 140/1000 | Loss: 0.00001092
Iteration 141/1000 | Loss: 0.00001092
Iteration 142/1000 | Loss: 0.00001092
Iteration 143/1000 | Loss: 0.00001092
Iteration 144/1000 | Loss: 0.00001092
Iteration 145/1000 | Loss: 0.00001091
Iteration 146/1000 | Loss: 0.00001091
Iteration 147/1000 | Loss: 0.00001091
Iteration 148/1000 | Loss: 0.00001091
Iteration 149/1000 | Loss: 0.00001091
Iteration 150/1000 | Loss: 0.00001091
Iteration 151/1000 | Loss: 0.00001091
Iteration 152/1000 | Loss: 0.00001091
Iteration 153/1000 | Loss: 0.00001091
Iteration 154/1000 | Loss: 0.00001091
Iteration 155/1000 | Loss: 0.00001091
Iteration 156/1000 | Loss: 0.00001091
Iteration 157/1000 | Loss: 0.00001091
Iteration 158/1000 | Loss: 0.00001091
Iteration 159/1000 | Loss: 0.00001091
Iteration 160/1000 | Loss: 0.00001090
Iteration 161/1000 | Loss: 0.00001090
Iteration 162/1000 | Loss: 0.00001090
Iteration 163/1000 | Loss: 0.00001090
Iteration 164/1000 | Loss: 0.00001090
Iteration 165/1000 | Loss: 0.00001090
Iteration 166/1000 | Loss: 0.00001090
Iteration 167/1000 | Loss: 0.00001090
Iteration 168/1000 | Loss: 0.00001090
Iteration 169/1000 | Loss: 0.00001090
Iteration 170/1000 | Loss: 0.00001090
Iteration 171/1000 | Loss: 0.00001090
Iteration 172/1000 | Loss: 0.00001089
Iteration 173/1000 | Loss: 0.00001089
Iteration 174/1000 | Loss: 0.00001089
Iteration 175/1000 | Loss: 0.00001089
Iteration 176/1000 | Loss: 0.00001089
Iteration 177/1000 | Loss: 0.00001089
Iteration 178/1000 | Loss: 0.00001089
Iteration 179/1000 | Loss: 0.00001089
Iteration 180/1000 | Loss: 0.00001089
Iteration 181/1000 | Loss: 0.00001089
Iteration 182/1000 | Loss: 0.00001088
Iteration 183/1000 | Loss: 0.00001088
Iteration 184/1000 | Loss: 0.00001088
Iteration 185/1000 | Loss: 0.00001088
Iteration 186/1000 | Loss: 0.00001088
Iteration 187/1000 | Loss: 0.00001088
Iteration 188/1000 | Loss: 0.00001088
Iteration 189/1000 | Loss: 0.00001088
Iteration 190/1000 | Loss: 0.00001088
Iteration 191/1000 | Loss: 0.00001088
Iteration 192/1000 | Loss: 0.00001087
Iteration 193/1000 | Loss: 0.00001087
Iteration 194/1000 | Loss: 0.00001087
Iteration 195/1000 | Loss: 0.00001087
Iteration 196/1000 | Loss: 0.00001087
Iteration 197/1000 | Loss: 0.00001087
Iteration 198/1000 | Loss: 0.00001087
Iteration 199/1000 | Loss: 0.00001087
Iteration 200/1000 | Loss: 0.00001087
Iteration 201/1000 | Loss: 0.00001087
Iteration 202/1000 | Loss: 0.00001087
Iteration 203/1000 | Loss: 0.00001087
Iteration 204/1000 | Loss: 0.00001087
Iteration 205/1000 | Loss: 0.00001087
Iteration 206/1000 | Loss: 0.00001087
Iteration 207/1000 | Loss: 0.00001087
Iteration 208/1000 | Loss: 0.00001087
Iteration 209/1000 | Loss: 0.00001087
Iteration 210/1000 | Loss: 0.00001087
Iteration 211/1000 | Loss: 0.00001087
Iteration 212/1000 | Loss: 0.00001087
Iteration 213/1000 | Loss: 0.00001087
Iteration 214/1000 | Loss: 0.00001087
Iteration 215/1000 | Loss: 0.00001087
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 215. Stopping optimization.
Last 5 losses: [1.0866611773963086e-05, 1.0866611773963086e-05, 1.0866611773963086e-05, 1.0866611773963086e-05, 1.0866611773963086e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0866611773963086e-05

Optimization complete. Final v2v error: 2.807748556137085 mm

Highest mean error: 3.081223964691162 mm for frame 117

Lowest mean error: 2.53011155128479 mm for frame 3

Saving results

Total time: 43.196985483169556
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_013/1012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_013/1012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_013/1012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00369943
Iteration 2/25 | Loss: 0.00112336
Iteration 3/25 | Loss: 0.00107780
Iteration 4/25 | Loss: 0.00106646
Iteration 5/25 | Loss: 0.00106366
Iteration 6/25 | Loss: 0.00106308
Iteration 7/25 | Loss: 0.00106308
Iteration 8/25 | Loss: 0.00106308
Iteration 9/25 | Loss: 0.00106308
Iteration 10/25 | Loss: 0.00106308
Iteration 11/25 | Loss: 0.00106308
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001063083647750318, 0.001063083647750318, 0.001063083647750318, 0.001063083647750318, 0.001063083647750318]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001063083647750318

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39750183
Iteration 2/25 | Loss: 0.00083792
Iteration 3/25 | Loss: 0.00083792
Iteration 4/25 | Loss: 0.00083792
Iteration 5/25 | Loss: 0.00083792
Iteration 6/25 | Loss: 0.00083792
Iteration 7/25 | Loss: 0.00083792
Iteration 8/25 | Loss: 0.00083792
Iteration 9/25 | Loss: 0.00083792
Iteration 10/25 | Loss: 0.00083792
Iteration 11/25 | Loss: 0.00083792
Iteration 12/25 | Loss: 0.00083792
Iteration 13/25 | Loss: 0.00083792
Iteration 14/25 | Loss: 0.00083792
Iteration 15/25 | Loss: 0.00083792
Iteration 16/25 | Loss: 0.00083792
Iteration 17/25 | Loss: 0.00083792
Iteration 18/25 | Loss: 0.00083792
Iteration 19/25 | Loss: 0.00083792
Iteration 20/25 | Loss: 0.00083792
Iteration 21/25 | Loss: 0.00083792
Iteration 22/25 | Loss: 0.00083792
Iteration 23/25 | Loss: 0.00083792
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0008379179053008556, 0.0008379179053008556, 0.0008379179053008556, 0.0008379179053008556, 0.0008379179053008556]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008379179053008556

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00083792
Iteration 2/1000 | Loss: 0.00002314
Iteration 3/1000 | Loss: 0.00001749
Iteration 4/1000 | Loss: 0.00001516
Iteration 5/1000 | Loss: 0.00001448
Iteration 6/1000 | Loss: 0.00001411
Iteration 7/1000 | Loss: 0.00001393
Iteration 8/1000 | Loss: 0.00001383
Iteration 9/1000 | Loss: 0.00001357
Iteration 10/1000 | Loss: 0.00001343
Iteration 11/1000 | Loss: 0.00001340
Iteration 12/1000 | Loss: 0.00001329
Iteration 13/1000 | Loss: 0.00001329
Iteration 14/1000 | Loss: 0.00001329
Iteration 15/1000 | Loss: 0.00001329
Iteration 16/1000 | Loss: 0.00001316
Iteration 17/1000 | Loss: 0.00001312
Iteration 18/1000 | Loss: 0.00001310
Iteration 19/1000 | Loss: 0.00001306
Iteration 20/1000 | Loss: 0.00001306
Iteration 21/1000 | Loss: 0.00001305
Iteration 22/1000 | Loss: 0.00001302
Iteration 23/1000 | Loss: 0.00001302
Iteration 24/1000 | Loss: 0.00001302
Iteration 25/1000 | Loss: 0.00001302
Iteration 26/1000 | Loss: 0.00001302
Iteration 27/1000 | Loss: 0.00001302
Iteration 28/1000 | Loss: 0.00001302
Iteration 29/1000 | Loss: 0.00001302
Iteration 30/1000 | Loss: 0.00001301
Iteration 31/1000 | Loss: 0.00001301
Iteration 32/1000 | Loss: 0.00001301
Iteration 33/1000 | Loss: 0.00001301
Iteration 34/1000 | Loss: 0.00001300
Iteration 35/1000 | Loss: 0.00001299
Iteration 36/1000 | Loss: 0.00001299
Iteration 37/1000 | Loss: 0.00001299
Iteration 38/1000 | Loss: 0.00001298
Iteration 39/1000 | Loss: 0.00001298
Iteration 40/1000 | Loss: 0.00001297
Iteration 41/1000 | Loss: 0.00001297
Iteration 42/1000 | Loss: 0.00001297
Iteration 43/1000 | Loss: 0.00001297
Iteration 44/1000 | Loss: 0.00001296
Iteration 45/1000 | Loss: 0.00001296
Iteration 46/1000 | Loss: 0.00001296
Iteration 47/1000 | Loss: 0.00001296
Iteration 48/1000 | Loss: 0.00001296
Iteration 49/1000 | Loss: 0.00001295
Iteration 50/1000 | Loss: 0.00001295
Iteration 51/1000 | Loss: 0.00001295
Iteration 52/1000 | Loss: 0.00001295
Iteration 53/1000 | Loss: 0.00001294
Iteration 54/1000 | Loss: 0.00001291
Iteration 55/1000 | Loss: 0.00001288
Iteration 56/1000 | Loss: 0.00001288
Iteration 57/1000 | Loss: 0.00001287
Iteration 58/1000 | Loss: 0.00001287
Iteration 59/1000 | Loss: 0.00001286
Iteration 60/1000 | Loss: 0.00001286
Iteration 61/1000 | Loss: 0.00001286
Iteration 62/1000 | Loss: 0.00001286
Iteration 63/1000 | Loss: 0.00001286
Iteration 64/1000 | Loss: 0.00001286
Iteration 65/1000 | Loss: 0.00001286
Iteration 66/1000 | Loss: 0.00001286
Iteration 67/1000 | Loss: 0.00001285
Iteration 68/1000 | Loss: 0.00001285
Iteration 69/1000 | Loss: 0.00001285
Iteration 70/1000 | Loss: 0.00001285
Iteration 71/1000 | Loss: 0.00001285
Iteration 72/1000 | Loss: 0.00001285
Iteration 73/1000 | Loss: 0.00001285
Iteration 74/1000 | Loss: 0.00001285
Iteration 75/1000 | Loss: 0.00001285
Iteration 76/1000 | Loss: 0.00001285
Iteration 77/1000 | Loss: 0.00001285
Iteration 78/1000 | Loss: 0.00001285
Iteration 79/1000 | Loss: 0.00001285
Iteration 80/1000 | Loss: 0.00001284
Iteration 81/1000 | Loss: 0.00001284
Iteration 82/1000 | Loss: 0.00001284
Iteration 83/1000 | Loss: 0.00001284
Iteration 84/1000 | Loss: 0.00001284
Iteration 85/1000 | Loss: 0.00001284
Iteration 86/1000 | Loss: 0.00001284
Iteration 87/1000 | Loss: 0.00001284
Iteration 88/1000 | Loss: 0.00001284
Iteration 89/1000 | Loss: 0.00001284
Iteration 90/1000 | Loss: 0.00001284
Iteration 91/1000 | Loss: 0.00001284
Iteration 92/1000 | Loss: 0.00001283
Iteration 93/1000 | Loss: 0.00001283
Iteration 94/1000 | Loss: 0.00001283
Iteration 95/1000 | Loss: 0.00001283
Iteration 96/1000 | Loss: 0.00001283
Iteration 97/1000 | Loss: 0.00001283
Iteration 98/1000 | Loss: 0.00001283
Iteration 99/1000 | Loss: 0.00001283
Iteration 100/1000 | Loss: 0.00001283
Iteration 101/1000 | Loss: 0.00001283
Iteration 102/1000 | Loss: 0.00001283
Iteration 103/1000 | Loss: 0.00001283
Iteration 104/1000 | Loss: 0.00001283
Iteration 105/1000 | Loss: 0.00001283
Iteration 106/1000 | Loss: 0.00001283
Iteration 107/1000 | Loss: 0.00001283
Iteration 108/1000 | Loss: 0.00001283
Iteration 109/1000 | Loss: 0.00001283
Iteration 110/1000 | Loss: 0.00001283
Iteration 111/1000 | Loss: 0.00001283
Iteration 112/1000 | Loss: 0.00001283
Iteration 113/1000 | Loss: 0.00001283
Iteration 114/1000 | Loss: 0.00001283
Iteration 115/1000 | Loss: 0.00001283
Iteration 116/1000 | Loss: 0.00001283
Iteration 117/1000 | Loss: 0.00001283
Iteration 118/1000 | Loss: 0.00001283
Iteration 119/1000 | Loss: 0.00001283
Iteration 120/1000 | Loss: 0.00001283
Iteration 121/1000 | Loss: 0.00001283
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 121. Stopping optimization.
Last 5 losses: [1.2828305443690624e-05, 1.2828305443690624e-05, 1.2828305443690624e-05, 1.2828305443690624e-05, 1.2828305443690624e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2828305443690624e-05

Optimization complete. Final v2v error: 3.014193058013916 mm

Highest mean error: 3.192354440689087 mm for frame 88

Lowest mean error: 2.8355319499969482 mm for frame 48

Saving results

Total time: 30.74715828895569
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_013/1064/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_013/1064.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_013/1064
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00863231
Iteration 2/25 | Loss: 0.00116326
Iteration 3/25 | Loss: 0.00107903
Iteration 4/25 | Loss: 0.00106316
Iteration 5/25 | Loss: 0.00105874
Iteration 6/25 | Loss: 0.00105819
Iteration 7/25 | Loss: 0.00105819
Iteration 8/25 | Loss: 0.00105819
Iteration 9/25 | Loss: 0.00105819
Iteration 10/25 | Loss: 0.00105819
Iteration 11/25 | Loss: 0.00105819
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010581878013908863, 0.0010581878013908863, 0.0010581878013908863, 0.0010581878013908863, 0.0010581878013908863]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010581878013908863

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.41381407
Iteration 2/25 | Loss: 0.00067114
Iteration 3/25 | Loss: 0.00067112
Iteration 4/25 | Loss: 0.00067112
Iteration 5/25 | Loss: 0.00067112
Iteration 6/25 | Loss: 0.00067112
Iteration 7/25 | Loss: 0.00067112
Iteration 8/25 | Loss: 0.00067112
Iteration 9/25 | Loss: 0.00067112
Iteration 10/25 | Loss: 0.00067112
Iteration 11/25 | Loss: 0.00067112
Iteration 12/25 | Loss: 0.00067112
Iteration 13/25 | Loss: 0.00067112
Iteration 14/25 | Loss: 0.00067112
Iteration 15/25 | Loss: 0.00067112
Iteration 16/25 | Loss: 0.00067112
Iteration 17/25 | Loss: 0.00067112
Iteration 18/25 | Loss: 0.00067112
Iteration 19/25 | Loss: 0.00067112
Iteration 20/25 | Loss: 0.00067112
Iteration 21/25 | Loss: 0.00067112
Iteration 22/25 | Loss: 0.00067112
Iteration 23/25 | Loss: 0.00067112
Iteration 24/25 | Loss: 0.00067112
Iteration 25/25 | Loss: 0.00067112

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00067112
Iteration 2/1000 | Loss: 0.00002097
Iteration 3/1000 | Loss: 0.00001567
Iteration 4/1000 | Loss: 0.00001441
Iteration 5/1000 | Loss: 0.00001382
Iteration 6/1000 | Loss: 0.00001329
Iteration 7/1000 | Loss: 0.00001292
Iteration 8/1000 | Loss: 0.00001268
Iteration 9/1000 | Loss: 0.00001247
Iteration 10/1000 | Loss: 0.00001234
Iteration 11/1000 | Loss: 0.00001224
Iteration 12/1000 | Loss: 0.00001217
Iteration 13/1000 | Loss: 0.00001217
Iteration 14/1000 | Loss: 0.00001208
Iteration 15/1000 | Loss: 0.00001207
Iteration 16/1000 | Loss: 0.00001206
Iteration 17/1000 | Loss: 0.00001200
Iteration 18/1000 | Loss: 0.00001199
Iteration 19/1000 | Loss: 0.00001199
Iteration 20/1000 | Loss: 0.00001198
Iteration 21/1000 | Loss: 0.00001198
Iteration 22/1000 | Loss: 0.00001197
Iteration 23/1000 | Loss: 0.00001196
Iteration 24/1000 | Loss: 0.00001194
Iteration 25/1000 | Loss: 0.00001194
Iteration 26/1000 | Loss: 0.00001194
Iteration 27/1000 | Loss: 0.00001194
Iteration 28/1000 | Loss: 0.00001194
Iteration 29/1000 | Loss: 0.00001194
Iteration 30/1000 | Loss: 0.00001194
Iteration 31/1000 | Loss: 0.00001194
Iteration 32/1000 | Loss: 0.00001194
Iteration 33/1000 | Loss: 0.00001194
Iteration 34/1000 | Loss: 0.00001194
Iteration 35/1000 | Loss: 0.00001194
Iteration 36/1000 | Loss: 0.00001193
Iteration 37/1000 | Loss: 0.00001193
Iteration 38/1000 | Loss: 0.00001193
Iteration 39/1000 | Loss: 0.00001193
Iteration 40/1000 | Loss: 0.00001190
Iteration 41/1000 | Loss: 0.00001189
Iteration 42/1000 | Loss: 0.00001189
Iteration 43/1000 | Loss: 0.00001187
Iteration 44/1000 | Loss: 0.00001187
Iteration 45/1000 | Loss: 0.00001186
Iteration 46/1000 | Loss: 0.00001186
Iteration 47/1000 | Loss: 0.00001186
Iteration 48/1000 | Loss: 0.00001186
Iteration 49/1000 | Loss: 0.00001185
Iteration 50/1000 | Loss: 0.00001185
Iteration 51/1000 | Loss: 0.00001185
Iteration 52/1000 | Loss: 0.00001185
Iteration 53/1000 | Loss: 0.00001185
Iteration 54/1000 | Loss: 0.00001183
Iteration 55/1000 | Loss: 0.00001182
Iteration 56/1000 | Loss: 0.00001182
Iteration 57/1000 | Loss: 0.00001181
Iteration 58/1000 | Loss: 0.00001181
Iteration 59/1000 | Loss: 0.00001181
Iteration 60/1000 | Loss: 0.00001180
Iteration 61/1000 | Loss: 0.00001180
Iteration 62/1000 | Loss: 0.00001180
Iteration 63/1000 | Loss: 0.00001180
Iteration 64/1000 | Loss: 0.00001179
Iteration 65/1000 | Loss: 0.00001179
Iteration 66/1000 | Loss: 0.00001179
Iteration 67/1000 | Loss: 0.00001178
Iteration 68/1000 | Loss: 0.00001178
Iteration 69/1000 | Loss: 0.00001178
Iteration 70/1000 | Loss: 0.00001178
Iteration 71/1000 | Loss: 0.00001178
Iteration 72/1000 | Loss: 0.00001177
Iteration 73/1000 | Loss: 0.00001177
Iteration 74/1000 | Loss: 0.00001177
Iteration 75/1000 | Loss: 0.00001177
Iteration 76/1000 | Loss: 0.00001177
Iteration 77/1000 | Loss: 0.00001176
Iteration 78/1000 | Loss: 0.00001176
Iteration 79/1000 | Loss: 0.00001176
Iteration 80/1000 | Loss: 0.00001176
Iteration 81/1000 | Loss: 0.00001175
Iteration 82/1000 | Loss: 0.00001175
Iteration 83/1000 | Loss: 0.00001174
Iteration 84/1000 | Loss: 0.00001174
Iteration 85/1000 | Loss: 0.00001174
Iteration 86/1000 | Loss: 0.00001173
Iteration 87/1000 | Loss: 0.00001173
Iteration 88/1000 | Loss: 0.00001173
Iteration 89/1000 | Loss: 0.00001173
Iteration 90/1000 | Loss: 0.00001173
Iteration 91/1000 | Loss: 0.00001173
Iteration 92/1000 | Loss: 0.00001172
Iteration 93/1000 | Loss: 0.00001172
Iteration 94/1000 | Loss: 0.00001171
Iteration 95/1000 | Loss: 0.00001171
Iteration 96/1000 | Loss: 0.00001171
Iteration 97/1000 | Loss: 0.00001171
Iteration 98/1000 | Loss: 0.00001171
Iteration 99/1000 | Loss: 0.00001171
Iteration 100/1000 | Loss: 0.00001171
Iteration 101/1000 | Loss: 0.00001171
Iteration 102/1000 | Loss: 0.00001171
Iteration 103/1000 | Loss: 0.00001171
Iteration 104/1000 | Loss: 0.00001170
Iteration 105/1000 | Loss: 0.00001170
Iteration 106/1000 | Loss: 0.00001170
Iteration 107/1000 | Loss: 0.00001170
Iteration 108/1000 | Loss: 0.00001170
Iteration 109/1000 | Loss: 0.00001170
Iteration 110/1000 | Loss: 0.00001170
Iteration 111/1000 | Loss: 0.00001170
Iteration 112/1000 | Loss: 0.00001170
Iteration 113/1000 | Loss: 0.00001170
Iteration 114/1000 | Loss: 0.00001169
Iteration 115/1000 | Loss: 0.00001169
Iteration 116/1000 | Loss: 0.00001169
Iteration 117/1000 | Loss: 0.00001169
Iteration 118/1000 | Loss: 0.00001169
Iteration 119/1000 | Loss: 0.00001169
Iteration 120/1000 | Loss: 0.00001169
Iteration 121/1000 | Loss: 0.00001169
Iteration 122/1000 | Loss: 0.00001169
Iteration 123/1000 | Loss: 0.00001169
Iteration 124/1000 | Loss: 0.00001168
Iteration 125/1000 | Loss: 0.00001168
Iteration 126/1000 | Loss: 0.00001168
Iteration 127/1000 | Loss: 0.00001168
Iteration 128/1000 | Loss: 0.00001168
Iteration 129/1000 | Loss: 0.00001168
Iteration 130/1000 | Loss: 0.00001168
Iteration 131/1000 | Loss: 0.00001168
Iteration 132/1000 | Loss: 0.00001168
Iteration 133/1000 | Loss: 0.00001168
Iteration 134/1000 | Loss: 0.00001168
Iteration 135/1000 | Loss: 0.00001167
Iteration 136/1000 | Loss: 0.00001167
Iteration 137/1000 | Loss: 0.00001167
Iteration 138/1000 | Loss: 0.00001167
Iteration 139/1000 | Loss: 0.00001166
Iteration 140/1000 | Loss: 0.00001166
Iteration 141/1000 | Loss: 0.00001166
Iteration 142/1000 | Loss: 0.00001165
Iteration 143/1000 | Loss: 0.00001165
Iteration 144/1000 | Loss: 0.00001165
Iteration 145/1000 | Loss: 0.00001165
Iteration 146/1000 | Loss: 0.00001165
Iteration 147/1000 | Loss: 0.00001165
Iteration 148/1000 | Loss: 0.00001165
Iteration 149/1000 | Loss: 0.00001165
Iteration 150/1000 | Loss: 0.00001165
Iteration 151/1000 | Loss: 0.00001164
Iteration 152/1000 | Loss: 0.00001164
Iteration 153/1000 | Loss: 0.00001164
Iteration 154/1000 | Loss: 0.00001164
Iteration 155/1000 | Loss: 0.00001164
Iteration 156/1000 | Loss: 0.00001164
Iteration 157/1000 | Loss: 0.00001164
Iteration 158/1000 | Loss: 0.00001164
Iteration 159/1000 | Loss: 0.00001164
Iteration 160/1000 | Loss: 0.00001164
Iteration 161/1000 | Loss: 0.00001163
Iteration 162/1000 | Loss: 0.00001163
Iteration 163/1000 | Loss: 0.00001163
Iteration 164/1000 | Loss: 0.00001163
Iteration 165/1000 | Loss: 0.00001163
Iteration 166/1000 | Loss: 0.00001163
Iteration 167/1000 | Loss: 0.00001163
Iteration 168/1000 | Loss: 0.00001163
Iteration 169/1000 | Loss: 0.00001163
Iteration 170/1000 | Loss: 0.00001163
Iteration 171/1000 | Loss: 0.00001163
Iteration 172/1000 | Loss: 0.00001162
Iteration 173/1000 | Loss: 0.00001162
Iteration 174/1000 | Loss: 0.00001162
Iteration 175/1000 | Loss: 0.00001162
Iteration 176/1000 | Loss: 0.00001162
Iteration 177/1000 | Loss: 0.00001162
Iteration 178/1000 | Loss: 0.00001162
Iteration 179/1000 | Loss: 0.00001162
Iteration 180/1000 | Loss: 0.00001162
Iteration 181/1000 | Loss: 0.00001162
Iteration 182/1000 | Loss: 0.00001162
Iteration 183/1000 | Loss: 0.00001162
Iteration 184/1000 | Loss: 0.00001162
Iteration 185/1000 | Loss: 0.00001162
Iteration 186/1000 | Loss: 0.00001161
Iteration 187/1000 | Loss: 0.00001161
Iteration 188/1000 | Loss: 0.00001161
Iteration 189/1000 | Loss: 0.00001161
Iteration 190/1000 | Loss: 0.00001161
Iteration 191/1000 | Loss: 0.00001161
Iteration 192/1000 | Loss: 0.00001160
Iteration 193/1000 | Loss: 0.00001160
Iteration 194/1000 | Loss: 0.00001160
Iteration 195/1000 | Loss: 0.00001160
Iteration 196/1000 | Loss: 0.00001160
Iteration 197/1000 | Loss: 0.00001160
Iteration 198/1000 | Loss: 0.00001160
Iteration 199/1000 | Loss: 0.00001160
Iteration 200/1000 | Loss: 0.00001160
Iteration 201/1000 | Loss: 0.00001160
Iteration 202/1000 | Loss: 0.00001160
Iteration 203/1000 | Loss: 0.00001160
Iteration 204/1000 | Loss: 0.00001160
Iteration 205/1000 | Loss: 0.00001159
Iteration 206/1000 | Loss: 0.00001159
Iteration 207/1000 | Loss: 0.00001159
Iteration 208/1000 | Loss: 0.00001159
Iteration 209/1000 | Loss: 0.00001159
Iteration 210/1000 | Loss: 0.00001159
Iteration 211/1000 | Loss: 0.00001159
Iteration 212/1000 | Loss: 0.00001159
Iteration 213/1000 | Loss: 0.00001159
Iteration 214/1000 | Loss: 0.00001158
Iteration 215/1000 | Loss: 0.00001158
Iteration 216/1000 | Loss: 0.00001158
Iteration 217/1000 | Loss: 0.00001158
Iteration 218/1000 | Loss: 0.00001158
Iteration 219/1000 | Loss: 0.00001158
Iteration 220/1000 | Loss: 0.00001158
Iteration 221/1000 | Loss: 0.00001158
Iteration 222/1000 | Loss: 0.00001158
Iteration 223/1000 | Loss: 0.00001158
Iteration 224/1000 | Loss: 0.00001158
Iteration 225/1000 | Loss: 0.00001158
Iteration 226/1000 | Loss: 0.00001158
Iteration 227/1000 | Loss: 0.00001158
Iteration 228/1000 | Loss: 0.00001158
Iteration 229/1000 | Loss: 0.00001158
Iteration 230/1000 | Loss: 0.00001158
Iteration 231/1000 | Loss: 0.00001158
Iteration 232/1000 | Loss: 0.00001158
Iteration 233/1000 | Loss: 0.00001158
Iteration 234/1000 | Loss: 0.00001158
Iteration 235/1000 | Loss: 0.00001158
Iteration 236/1000 | Loss: 0.00001158
Iteration 237/1000 | Loss: 0.00001158
Iteration 238/1000 | Loss: 0.00001158
Iteration 239/1000 | Loss: 0.00001158
Iteration 240/1000 | Loss: 0.00001158
Iteration 241/1000 | Loss: 0.00001158
Iteration 242/1000 | Loss: 0.00001158
Iteration 243/1000 | Loss: 0.00001158
Iteration 244/1000 | Loss: 0.00001158
Iteration 245/1000 | Loss: 0.00001158
Iteration 246/1000 | Loss: 0.00001158
Iteration 247/1000 | Loss: 0.00001158
Iteration 248/1000 | Loss: 0.00001158
Iteration 249/1000 | Loss: 0.00001158
Iteration 250/1000 | Loss: 0.00001158
Iteration 251/1000 | Loss: 0.00001158
Iteration 252/1000 | Loss: 0.00001158
Iteration 253/1000 | Loss: 0.00001158
Iteration 254/1000 | Loss: 0.00001158
Iteration 255/1000 | Loss: 0.00001158
Iteration 256/1000 | Loss: 0.00001158
Iteration 257/1000 | Loss: 0.00001158
Iteration 258/1000 | Loss: 0.00001158
Iteration 259/1000 | Loss: 0.00001158
Iteration 260/1000 | Loss: 0.00001158
Iteration 261/1000 | Loss: 0.00001158
Iteration 262/1000 | Loss: 0.00001158
Iteration 263/1000 | Loss: 0.00001158
Iteration 264/1000 | Loss: 0.00001158
Iteration 265/1000 | Loss: 0.00001158
Iteration 266/1000 | Loss: 0.00001158
Iteration 267/1000 | Loss: 0.00001158
Iteration 268/1000 | Loss: 0.00001158
Iteration 269/1000 | Loss: 0.00001158
Iteration 270/1000 | Loss: 0.00001158
Iteration 271/1000 | Loss: 0.00001158
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 271. Stopping optimization.
Last 5 losses: [1.1578109479160048e-05, 1.1578109479160048e-05, 1.1578109479160048e-05, 1.1578109479160048e-05, 1.1578109479160048e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1578109479160048e-05

Optimization complete. Final v2v error: 2.8809754848480225 mm

Highest mean error: 3.8384106159210205 mm for frame 92

Lowest mean error: 2.6465210914611816 mm for frame 67

Saving results

Total time: 46.90594530105591
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_013/1023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_013/1023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_013/1023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00846342
Iteration 2/25 | Loss: 0.00111503
Iteration 3/25 | Loss: 0.00106071
Iteration 4/25 | Loss: 0.00105302
Iteration 5/25 | Loss: 0.00105138
Iteration 6/25 | Loss: 0.00105127
Iteration 7/25 | Loss: 0.00105127
Iteration 8/25 | Loss: 0.00105127
Iteration 9/25 | Loss: 0.00105127
Iteration 10/25 | Loss: 0.00105127
Iteration 11/25 | Loss: 0.00105127
Iteration 12/25 | Loss: 0.00105127
Iteration 13/25 | Loss: 0.00105127
Iteration 14/25 | Loss: 0.00105127
Iteration 15/25 | Loss: 0.00105127
Iteration 16/25 | Loss: 0.00105127
Iteration 17/25 | Loss: 0.00105127
Iteration 18/25 | Loss: 0.00105127
Iteration 19/25 | Loss: 0.00105127
Iteration 20/25 | Loss: 0.00105127
Iteration 21/25 | Loss: 0.00105127
Iteration 22/25 | Loss: 0.00105127
Iteration 23/25 | Loss: 0.00105127
Iteration 24/25 | Loss: 0.00105127
Iteration 25/25 | Loss: 0.00105127

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38422704
Iteration 2/25 | Loss: 0.00067039
Iteration 3/25 | Loss: 0.00067039
Iteration 4/25 | Loss: 0.00067039
Iteration 5/25 | Loss: 0.00067039
Iteration 6/25 | Loss: 0.00067039
Iteration 7/25 | Loss: 0.00067039
Iteration 8/25 | Loss: 0.00067039
Iteration 9/25 | Loss: 0.00067039
Iteration 10/25 | Loss: 0.00067039
Iteration 11/25 | Loss: 0.00067038
Iteration 12/25 | Loss: 0.00067038
Iteration 13/25 | Loss: 0.00067038
Iteration 14/25 | Loss: 0.00067038
Iteration 15/25 | Loss: 0.00067038
Iteration 16/25 | Loss: 0.00067038
Iteration 17/25 | Loss: 0.00067038
Iteration 18/25 | Loss: 0.00067038
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0006703844992443919, 0.0006703844992443919, 0.0006703844992443919, 0.0006703844992443919, 0.0006703844992443919]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006703844992443919

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00067038
Iteration 2/1000 | Loss: 0.00001798
Iteration 3/1000 | Loss: 0.00001332
Iteration 4/1000 | Loss: 0.00001220
Iteration 5/1000 | Loss: 0.00001201
Iteration 6/1000 | Loss: 0.00001163
Iteration 7/1000 | Loss: 0.00001128
Iteration 8/1000 | Loss: 0.00001108
Iteration 9/1000 | Loss: 0.00001107
Iteration 10/1000 | Loss: 0.00001105
Iteration 11/1000 | Loss: 0.00001100
Iteration 12/1000 | Loss: 0.00001098
Iteration 13/1000 | Loss: 0.00001097
Iteration 14/1000 | Loss: 0.00001093
Iteration 15/1000 | Loss: 0.00001085
Iteration 16/1000 | Loss: 0.00001077
Iteration 17/1000 | Loss: 0.00001075
Iteration 18/1000 | Loss: 0.00001075
Iteration 19/1000 | Loss: 0.00001075
Iteration 20/1000 | Loss: 0.00001075
Iteration 21/1000 | Loss: 0.00001074
Iteration 22/1000 | Loss: 0.00001074
Iteration 23/1000 | Loss: 0.00001072
Iteration 24/1000 | Loss: 0.00001072
Iteration 25/1000 | Loss: 0.00001072
Iteration 26/1000 | Loss: 0.00001072
Iteration 27/1000 | Loss: 0.00001071
Iteration 28/1000 | Loss: 0.00001071
Iteration 29/1000 | Loss: 0.00001069
Iteration 30/1000 | Loss: 0.00001068
Iteration 31/1000 | Loss: 0.00001067
Iteration 32/1000 | Loss: 0.00001067
Iteration 33/1000 | Loss: 0.00001066
Iteration 34/1000 | Loss: 0.00001065
Iteration 35/1000 | Loss: 0.00001063
Iteration 36/1000 | Loss: 0.00001062
Iteration 37/1000 | Loss: 0.00001062
Iteration 38/1000 | Loss: 0.00001061
Iteration 39/1000 | Loss: 0.00001061
Iteration 40/1000 | Loss: 0.00001060
Iteration 41/1000 | Loss: 0.00001058
Iteration 42/1000 | Loss: 0.00001058
Iteration 43/1000 | Loss: 0.00001058
Iteration 44/1000 | Loss: 0.00001058
Iteration 45/1000 | Loss: 0.00001057
Iteration 46/1000 | Loss: 0.00001057
Iteration 47/1000 | Loss: 0.00001056
Iteration 48/1000 | Loss: 0.00001053
Iteration 49/1000 | Loss: 0.00001053
Iteration 50/1000 | Loss: 0.00001053
Iteration 51/1000 | Loss: 0.00001053
Iteration 52/1000 | Loss: 0.00001052
Iteration 53/1000 | Loss: 0.00001052
Iteration 54/1000 | Loss: 0.00001052
Iteration 55/1000 | Loss: 0.00001052
Iteration 56/1000 | Loss: 0.00001051
Iteration 57/1000 | Loss: 0.00001051
Iteration 58/1000 | Loss: 0.00001051
Iteration 59/1000 | Loss: 0.00001051
Iteration 60/1000 | Loss: 0.00001051
Iteration 61/1000 | Loss: 0.00001051
Iteration 62/1000 | Loss: 0.00001050
Iteration 63/1000 | Loss: 0.00001050
Iteration 64/1000 | Loss: 0.00001049
Iteration 65/1000 | Loss: 0.00001049
Iteration 66/1000 | Loss: 0.00001049
Iteration 67/1000 | Loss: 0.00001048
Iteration 68/1000 | Loss: 0.00001048
Iteration 69/1000 | Loss: 0.00001048
Iteration 70/1000 | Loss: 0.00001048
Iteration 71/1000 | Loss: 0.00001048
Iteration 72/1000 | Loss: 0.00001048
Iteration 73/1000 | Loss: 0.00001048
Iteration 74/1000 | Loss: 0.00001048
Iteration 75/1000 | Loss: 0.00001048
Iteration 76/1000 | Loss: 0.00001048
Iteration 77/1000 | Loss: 0.00001048
Iteration 78/1000 | Loss: 0.00001048
Iteration 79/1000 | Loss: 0.00001047
Iteration 80/1000 | Loss: 0.00001047
Iteration 81/1000 | Loss: 0.00001046
Iteration 82/1000 | Loss: 0.00001046
Iteration 83/1000 | Loss: 0.00001045
Iteration 84/1000 | Loss: 0.00001045
Iteration 85/1000 | Loss: 0.00001045
Iteration 86/1000 | Loss: 0.00001045
Iteration 87/1000 | Loss: 0.00001045
Iteration 88/1000 | Loss: 0.00001044
Iteration 89/1000 | Loss: 0.00001044
Iteration 90/1000 | Loss: 0.00001044
Iteration 91/1000 | Loss: 0.00001044
Iteration 92/1000 | Loss: 0.00001044
Iteration 93/1000 | Loss: 0.00001044
Iteration 94/1000 | Loss: 0.00001044
Iteration 95/1000 | Loss: 0.00001044
Iteration 96/1000 | Loss: 0.00001044
Iteration 97/1000 | Loss: 0.00001043
Iteration 98/1000 | Loss: 0.00001043
Iteration 99/1000 | Loss: 0.00001043
Iteration 100/1000 | Loss: 0.00001043
Iteration 101/1000 | Loss: 0.00001043
Iteration 102/1000 | Loss: 0.00001043
Iteration 103/1000 | Loss: 0.00001043
Iteration 104/1000 | Loss: 0.00001043
Iteration 105/1000 | Loss: 0.00001043
Iteration 106/1000 | Loss: 0.00001042
Iteration 107/1000 | Loss: 0.00001042
Iteration 108/1000 | Loss: 0.00001042
Iteration 109/1000 | Loss: 0.00001042
Iteration 110/1000 | Loss: 0.00001042
Iteration 111/1000 | Loss: 0.00001042
Iteration 112/1000 | Loss: 0.00001041
Iteration 113/1000 | Loss: 0.00001041
Iteration 114/1000 | Loss: 0.00001041
Iteration 115/1000 | Loss: 0.00001041
Iteration 116/1000 | Loss: 0.00001041
Iteration 117/1000 | Loss: 0.00001041
Iteration 118/1000 | Loss: 0.00001041
Iteration 119/1000 | Loss: 0.00001041
Iteration 120/1000 | Loss: 0.00001041
Iteration 121/1000 | Loss: 0.00001041
Iteration 122/1000 | Loss: 0.00001041
Iteration 123/1000 | Loss: 0.00001041
Iteration 124/1000 | Loss: 0.00001041
Iteration 125/1000 | Loss: 0.00001041
Iteration 126/1000 | Loss: 0.00001040
Iteration 127/1000 | Loss: 0.00001040
Iteration 128/1000 | Loss: 0.00001040
Iteration 129/1000 | Loss: 0.00001040
Iteration 130/1000 | Loss: 0.00001040
Iteration 131/1000 | Loss: 0.00001040
Iteration 132/1000 | Loss: 0.00001039
Iteration 133/1000 | Loss: 0.00001039
Iteration 134/1000 | Loss: 0.00001039
Iteration 135/1000 | Loss: 0.00001038
Iteration 136/1000 | Loss: 0.00001038
Iteration 137/1000 | Loss: 0.00001037
Iteration 138/1000 | Loss: 0.00001037
Iteration 139/1000 | Loss: 0.00001037
Iteration 140/1000 | Loss: 0.00001037
Iteration 141/1000 | Loss: 0.00001037
Iteration 142/1000 | Loss: 0.00001037
Iteration 143/1000 | Loss: 0.00001037
Iteration 144/1000 | Loss: 0.00001037
Iteration 145/1000 | Loss: 0.00001036
Iteration 146/1000 | Loss: 0.00001036
Iteration 147/1000 | Loss: 0.00001036
Iteration 148/1000 | Loss: 0.00001035
Iteration 149/1000 | Loss: 0.00001034
Iteration 150/1000 | Loss: 0.00001033
Iteration 151/1000 | Loss: 0.00001033
Iteration 152/1000 | Loss: 0.00001033
Iteration 153/1000 | Loss: 0.00001032
Iteration 154/1000 | Loss: 0.00001032
Iteration 155/1000 | Loss: 0.00001032
Iteration 156/1000 | Loss: 0.00001032
Iteration 157/1000 | Loss: 0.00001032
Iteration 158/1000 | Loss: 0.00001032
Iteration 159/1000 | Loss: 0.00001032
Iteration 160/1000 | Loss: 0.00001032
Iteration 161/1000 | Loss: 0.00001032
Iteration 162/1000 | Loss: 0.00001032
Iteration 163/1000 | Loss: 0.00001032
Iteration 164/1000 | Loss: 0.00001032
Iteration 165/1000 | Loss: 0.00001031
Iteration 166/1000 | Loss: 0.00001031
Iteration 167/1000 | Loss: 0.00001031
Iteration 168/1000 | Loss: 0.00001031
Iteration 169/1000 | Loss: 0.00001031
Iteration 170/1000 | Loss: 0.00001031
Iteration 171/1000 | Loss: 0.00001031
Iteration 172/1000 | Loss: 0.00001031
Iteration 173/1000 | Loss: 0.00001031
Iteration 174/1000 | Loss: 0.00001031
Iteration 175/1000 | Loss: 0.00001031
Iteration 176/1000 | Loss: 0.00001030
Iteration 177/1000 | Loss: 0.00001030
Iteration 178/1000 | Loss: 0.00001030
Iteration 179/1000 | Loss: 0.00001030
Iteration 180/1000 | Loss: 0.00001030
Iteration 181/1000 | Loss: 0.00001030
Iteration 182/1000 | Loss: 0.00001030
Iteration 183/1000 | Loss: 0.00001030
Iteration 184/1000 | Loss: 0.00001030
Iteration 185/1000 | Loss: 0.00001030
Iteration 186/1000 | Loss: 0.00001030
Iteration 187/1000 | Loss: 0.00001030
Iteration 188/1000 | Loss: 0.00001030
Iteration 189/1000 | Loss: 0.00001029
Iteration 190/1000 | Loss: 0.00001029
Iteration 191/1000 | Loss: 0.00001029
Iteration 192/1000 | Loss: 0.00001029
Iteration 193/1000 | Loss: 0.00001028
Iteration 194/1000 | Loss: 0.00001028
Iteration 195/1000 | Loss: 0.00001028
Iteration 196/1000 | Loss: 0.00001028
Iteration 197/1000 | Loss: 0.00001028
Iteration 198/1000 | Loss: 0.00001028
Iteration 199/1000 | Loss: 0.00001028
Iteration 200/1000 | Loss: 0.00001028
Iteration 201/1000 | Loss: 0.00001028
Iteration 202/1000 | Loss: 0.00001028
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 202. Stopping optimization.
Last 5 losses: [1.0281581126037054e-05, 1.0281581126037054e-05, 1.0281581126037054e-05, 1.0281581126037054e-05, 1.0281581126037054e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0281581126037054e-05

Optimization complete. Final v2v error: 2.732586145401001 mm

Highest mean error: 2.865783929824829 mm for frame 107

Lowest mean error: 2.6563432216644287 mm for frame 138

Saving results

Total time: 35.42562985420227
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_013/1024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_013/1024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_013/1024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00780941
Iteration 2/25 | Loss: 0.00143283
Iteration 3/25 | Loss: 0.00119357
Iteration 4/25 | Loss: 0.00116974
Iteration 5/25 | Loss: 0.00116532
Iteration 6/25 | Loss: 0.00116486
Iteration 7/25 | Loss: 0.00116486
Iteration 8/25 | Loss: 0.00116486
Iteration 9/25 | Loss: 0.00116486
Iteration 10/25 | Loss: 0.00116486
Iteration 11/25 | Loss: 0.00116486
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011648554354906082, 0.0011648554354906082, 0.0011648554354906082, 0.0011648554354906082, 0.0011648554354906082]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011648554354906082

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35680473
Iteration 2/25 | Loss: 0.00059066
Iteration 3/25 | Loss: 0.00059065
Iteration 4/25 | Loss: 0.00059065
Iteration 5/25 | Loss: 0.00059065
Iteration 6/25 | Loss: 0.00059065
Iteration 7/25 | Loss: 0.00059064
Iteration 8/25 | Loss: 0.00059064
Iteration 9/25 | Loss: 0.00059064
Iteration 10/25 | Loss: 0.00059064
Iteration 11/25 | Loss: 0.00059064
Iteration 12/25 | Loss: 0.00059064
Iteration 13/25 | Loss: 0.00059064
Iteration 14/25 | Loss: 0.00059064
Iteration 15/25 | Loss: 0.00059064
Iteration 16/25 | Loss: 0.00059064
Iteration 17/25 | Loss: 0.00059064
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0005906439619138837, 0.0005906439619138837, 0.0005906439619138837, 0.0005906439619138837, 0.0005906439619138837]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005906439619138837

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00059064
Iteration 2/1000 | Loss: 0.00002957
Iteration 3/1000 | Loss: 0.00002275
Iteration 4/1000 | Loss: 0.00002109
Iteration 5/1000 | Loss: 0.00002022
Iteration 6/1000 | Loss: 0.00001954
Iteration 7/1000 | Loss: 0.00001923
Iteration 8/1000 | Loss: 0.00001885
Iteration 9/1000 | Loss: 0.00001866
Iteration 10/1000 | Loss: 0.00001848
Iteration 11/1000 | Loss: 0.00001843
Iteration 12/1000 | Loss: 0.00001836
Iteration 13/1000 | Loss: 0.00001833
Iteration 14/1000 | Loss: 0.00001823
Iteration 15/1000 | Loss: 0.00001819
Iteration 16/1000 | Loss: 0.00001819
Iteration 17/1000 | Loss: 0.00001819
Iteration 18/1000 | Loss: 0.00001815
Iteration 19/1000 | Loss: 0.00001815
Iteration 20/1000 | Loss: 0.00001815
Iteration 21/1000 | Loss: 0.00001815
Iteration 22/1000 | Loss: 0.00001815
Iteration 23/1000 | Loss: 0.00001814
Iteration 24/1000 | Loss: 0.00001813
Iteration 25/1000 | Loss: 0.00001812
Iteration 26/1000 | Loss: 0.00001811
Iteration 27/1000 | Loss: 0.00001811
Iteration 28/1000 | Loss: 0.00001811
Iteration 29/1000 | Loss: 0.00001809
Iteration 30/1000 | Loss: 0.00001808
Iteration 31/1000 | Loss: 0.00001807
Iteration 32/1000 | Loss: 0.00001807
Iteration 33/1000 | Loss: 0.00001807
Iteration 34/1000 | Loss: 0.00001807
Iteration 35/1000 | Loss: 0.00001807
Iteration 36/1000 | Loss: 0.00001807
Iteration 37/1000 | Loss: 0.00001807
Iteration 38/1000 | Loss: 0.00001807
Iteration 39/1000 | Loss: 0.00001807
Iteration 40/1000 | Loss: 0.00001806
Iteration 41/1000 | Loss: 0.00001806
Iteration 42/1000 | Loss: 0.00001805
Iteration 43/1000 | Loss: 0.00001805
Iteration 44/1000 | Loss: 0.00001805
Iteration 45/1000 | Loss: 0.00001804
Iteration 46/1000 | Loss: 0.00001804
Iteration 47/1000 | Loss: 0.00001804
Iteration 48/1000 | Loss: 0.00001803
Iteration 49/1000 | Loss: 0.00001803
Iteration 50/1000 | Loss: 0.00001803
Iteration 51/1000 | Loss: 0.00001803
Iteration 52/1000 | Loss: 0.00001803
Iteration 53/1000 | Loss: 0.00001803
Iteration 54/1000 | Loss: 0.00001803
Iteration 55/1000 | Loss: 0.00001803
Iteration 56/1000 | Loss: 0.00001803
Iteration 57/1000 | Loss: 0.00001802
Iteration 58/1000 | Loss: 0.00001802
Iteration 59/1000 | Loss: 0.00001802
Iteration 60/1000 | Loss: 0.00001802
Iteration 61/1000 | Loss: 0.00001802
Iteration 62/1000 | Loss: 0.00001802
Iteration 63/1000 | Loss: 0.00001802
Iteration 64/1000 | Loss: 0.00001802
Iteration 65/1000 | Loss: 0.00001802
Iteration 66/1000 | Loss: 0.00001801
Iteration 67/1000 | Loss: 0.00001801
Iteration 68/1000 | Loss: 0.00001801
Iteration 69/1000 | Loss: 0.00001800
Iteration 70/1000 | Loss: 0.00001800
Iteration 71/1000 | Loss: 0.00001800
Iteration 72/1000 | Loss: 0.00001800
Iteration 73/1000 | Loss: 0.00001800
Iteration 74/1000 | Loss: 0.00001800
Iteration 75/1000 | Loss: 0.00001800
Iteration 76/1000 | Loss: 0.00001800
Iteration 77/1000 | Loss: 0.00001800
Iteration 78/1000 | Loss: 0.00001800
Iteration 79/1000 | Loss: 0.00001800
Iteration 80/1000 | Loss: 0.00001799
Iteration 81/1000 | Loss: 0.00001799
Iteration 82/1000 | Loss: 0.00001799
Iteration 83/1000 | Loss: 0.00001799
Iteration 84/1000 | Loss: 0.00001799
Iteration 85/1000 | Loss: 0.00001799
Iteration 86/1000 | Loss: 0.00001799
Iteration 87/1000 | Loss: 0.00001799
Iteration 88/1000 | Loss: 0.00001799
Iteration 89/1000 | Loss: 0.00001799
Iteration 90/1000 | Loss: 0.00001799
Iteration 91/1000 | Loss: 0.00001799
Iteration 92/1000 | Loss: 0.00001798
Iteration 93/1000 | Loss: 0.00001798
Iteration 94/1000 | Loss: 0.00001798
Iteration 95/1000 | Loss: 0.00001798
Iteration 96/1000 | Loss: 0.00001798
Iteration 97/1000 | Loss: 0.00001798
Iteration 98/1000 | Loss: 0.00001798
Iteration 99/1000 | Loss: 0.00001798
Iteration 100/1000 | Loss: 0.00001798
Iteration 101/1000 | Loss: 0.00001798
Iteration 102/1000 | Loss: 0.00001798
Iteration 103/1000 | Loss: 0.00001798
Iteration 104/1000 | Loss: 0.00001798
Iteration 105/1000 | Loss: 0.00001798
Iteration 106/1000 | Loss: 0.00001798
Iteration 107/1000 | Loss: 0.00001797
Iteration 108/1000 | Loss: 0.00001797
Iteration 109/1000 | Loss: 0.00001797
Iteration 110/1000 | Loss: 0.00001797
Iteration 111/1000 | Loss: 0.00001797
Iteration 112/1000 | Loss: 0.00001796
Iteration 113/1000 | Loss: 0.00001796
Iteration 114/1000 | Loss: 0.00001796
Iteration 115/1000 | Loss: 0.00001796
Iteration 116/1000 | Loss: 0.00001796
Iteration 117/1000 | Loss: 0.00001796
Iteration 118/1000 | Loss: 0.00001796
Iteration 119/1000 | Loss: 0.00001796
Iteration 120/1000 | Loss: 0.00001795
Iteration 121/1000 | Loss: 0.00001795
Iteration 122/1000 | Loss: 0.00001795
Iteration 123/1000 | Loss: 0.00001794
Iteration 124/1000 | Loss: 0.00001794
Iteration 125/1000 | Loss: 0.00001794
Iteration 126/1000 | Loss: 0.00001794
Iteration 127/1000 | Loss: 0.00001794
Iteration 128/1000 | Loss: 0.00001794
Iteration 129/1000 | Loss: 0.00001794
Iteration 130/1000 | Loss: 0.00001793
Iteration 131/1000 | Loss: 0.00001793
Iteration 132/1000 | Loss: 0.00001793
Iteration 133/1000 | Loss: 0.00001793
Iteration 134/1000 | Loss: 0.00001793
Iteration 135/1000 | Loss: 0.00001792
Iteration 136/1000 | Loss: 0.00001792
Iteration 137/1000 | Loss: 0.00001792
Iteration 138/1000 | Loss: 0.00001792
Iteration 139/1000 | Loss: 0.00001792
Iteration 140/1000 | Loss: 0.00001791
Iteration 141/1000 | Loss: 0.00001791
Iteration 142/1000 | Loss: 0.00001791
Iteration 143/1000 | Loss: 0.00001791
Iteration 144/1000 | Loss: 0.00001791
Iteration 145/1000 | Loss: 0.00001791
Iteration 146/1000 | Loss: 0.00001791
Iteration 147/1000 | Loss: 0.00001791
Iteration 148/1000 | Loss: 0.00001791
Iteration 149/1000 | Loss: 0.00001790
Iteration 150/1000 | Loss: 0.00001790
Iteration 151/1000 | Loss: 0.00001790
Iteration 152/1000 | Loss: 0.00001790
Iteration 153/1000 | Loss: 0.00001790
Iteration 154/1000 | Loss: 0.00001790
Iteration 155/1000 | Loss: 0.00001790
Iteration 156/1000 | Loss: 0.00001790
Iteration 157/1000 | Loss: 0.00001789
Iteration 158/1000 | Loss: 0.00001789
Iteration 159/1000 | Loss: 0.00001789
Iteration 160/1000 | Loss: 0.00001789
Iteration 161/1000 | Loss: 0.00001789
Iteration 162/1000 | Loss: 0.00001789
Iteration 163/1000 | Loss: 0.00001789
Iteration 164/1000 | Loss: 0.00001789
Iteration 165/1000 | Loss: 0.00001789
Iteration 166/1000 | Loss: 0.00001789
Iteration 167/1000 | Loss: 0.00001789
Iteration 168/1000 | Loss: 0.00001789
Iteration 169/1000 | Loss: 0.00001789
Iteration 170/1000 | Loss: 0.00001789
Iteration 171/1000 | Loss: 0.00001789
Iteration 172/1000 | Loss: 0.00001789
Iteration 173/1000 | Loss: 0.00001789
Iteration 174/1000 | Loss: 0.00001789
Iteration 175/1000 | Loss: 0.00001789
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 175. Stopping optimization.
Last 5 losses: [1.7891914467327297e-05, 1.7891914467327297e-05, 1.7891914467327297e-05, 1.7891914467327297e-05, 1.7891914467327297e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7891914467327297e-05

Optimization complete. Final v2v error: 3.5278396606445312 mm

Highest mean error: 3.672712564468384 mm for frame 174

Lowest mean error: 3.3441286087036133 mm for frame 0

Saving results

Total time: 41.4166100025177
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_013/1065/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_013/1065.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_013/1065
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00913028
Iteration 2/25 | Loss: 0.00124469
Iteration 3/25 | Loss: 0.00113103
Iteration 4/25 | Loss: 0.00110851
Iteration 5/25 | Loss: 0.00110070
Iteration 6/25 | Loss: 0.00109895
Iteration 7/25 | Loss: 0.00109895
Iteration 8/25 | Loss: 0.00109895
Iteration 9/25 | Loss: 0.00109895
Iteration 10/25 | Loss: 0.00109895
Iteration 11/25 | Loss: 0.00109895
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001098951674066484, 0.001098951674066484, 0.001098951674066484, 0.001098951674066484, 0.001098951674066484]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001098951674066484

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37539947
Iteration 2/25 | Loss: 0.00070634
Iteration 3/25 | Loss: 0.00070633
Iteration 4/25 | Loss: 0.00070633
Iteration 5/25 | Loss: 0.00070632
Iteration 6/25 | Loss: 0.00070632
Iteration 7/25 | Loss: 0.00070632
Iteration 8/25 | Loss: 0.00070632
Iteration 9/25 | Loss: 0.00070632
Iteration 10/25 | Loss: 0.00070632
Iteration 11/25 | Loss: 0.00070632
Iteration 12/25 | Loss: 0.00070632
Iteration 13/25 | Loss: 0.00070632
Iteration 14/25 | Loss: 0.00070632
Iteration 15/25 | Loss: 0.00070632
Iteration 16/25 | Loss: 0.00070632
Iteration 17/25 | Loss: 0.00070632
Iteration 18/25 | Loss: 0.00070632
Iteration 19/25 | Loss: 0.00070632
Iteration 20/25 | Loss: 0.00070632
Iteration 21/25 | Loss: 0.00070632
Iteration 22/25 | Loss: 0.00070632
Iteration 23/25 | Loss: 0.00070632
Iteration 24/25 | Loss: 0.00070632
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0007063227239996195, 0.0007063227239996195, 0.0007063227239996195, 0.0007063227239996195, 0.0007063227239996195]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007063227239996195

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00070632
Iteration 2/1000 | Loss: 0.00004255
Iteration 3/1000 | Loss: 0.00002640
Iteration 4/1000 | Loss: 0.00002168
Iteration 5/1000 | Loss: 0.00002025
Iteration 6/1000 | Loss: 0.00001936
Iteration 7/1000 | Loss: 0.00001877
Iteration 8/1000 | Loss: 0.00001835
Iteration 9/1000 | Loss: 0.00001791
Iteration 10/1000 | Loss: 0.00001774
Iteration 11/1000 | Loss: 0.00001772
Iteration 12/1000 | Loss: 0.00001760
Iteration 13/1000 | Loss: 0.00001754
Iteration 14/1000 | Loss: 0.00001739
Iteration 15/1000 | Loss: 0.00001736
Iteration 16/1000 | Loss: 0.00001733
Iteration 17/1000 | Loss: 0.00001732
Iteration 18/1000 | Loss: 0.00001732
Iteration 19/1000 | Loss: 0.00001731
Iteration 20/1000 | Loss: 0.00001726
Iteration 21/1000 | Loss: 0.00001725
Iteration 22/1000 | Loss: 0.00001724
Iteration 23/1000 | Loss: 0.00001720
Iteration 24/1000 | Loss: 0.00001720
Iteration 25/1000 | Loss: 0.00001716
Iteration 26/1000 | Loss: 0.00001716
Iteration 27/1000 | Loss: 0.00001715
Iteration 28/1000 | Loss: 0.00001715
Iteration 29/1000 | Loss: 0.00001715
Iteration 30/1000 | Loss: 0.00001714
Iteration 31/1000 | Loss: 0.00001714
Iteration 32/1000 | Loss: 0.00001714
Iteration 33/1000 | Loss: 0.00001713
Iteration 34/1000 | Loss: 0.00001712
Iteration 35/1000 | Loss: 0.00001711
Iteration 36/1000 | Loss: 0.00001711
Iteration 37/1000 | Loss: 0.00001710
Iteration 38/1000 | Loss: 0.00001710
Iteration 39/1000 | Loss: 0.00001709
Iteration 40/1000 | Loss: 0.00001709
Iteration 41/1000 | Loss: 0.00001709
Iteration 42/1000 | Loss: 0.00001708
Iteration 43/1000 | Loss: 0.00001708
Iteration 44/1000 | Loss: 0.00001707
Iteration 45/1000 | Loss: 0.00001706
Iteration 46/1000 | Loss: 0.00001706
Iteration 47/1000 | Loss: 0.00001706
Iteration 48/1000 | Loss: 0.00001705
Iteration 49/1000 | Loss: 0.00001705
Iteration 50/1000 | Loss: 0.00001705
Iteration 51/1000 | Loss: 0.00001703
Iteration 52/1000 | Loss: 0.00001703
Iteration 53/1000 | Loss: 0.00001702
Iteration 54/1000 | Loss: 0.00001702
Iteration 55/1000 | Loss: 0.00001702
Iteration 56/1000 | Loss: 0.00001702
Iteration 57/1000 | Loss: 0.00001702
Iteration 58/1000 | Loss: 0.00001701
Iteration 59/1000 | Loss: 0.00001701
Iteration 60/1000 | Loss: 0.00001700
Iteration 61/1000 | Loss: 0.00001700
Iteration 62/1000 | Loss: 0.00001700
Iteration 63/1000 | Loss: 0.00001699
Iteration 64/1000 | Loss: 0.00001699
Iteration 65/1000 | Loss: 0.00001699
Iteration 66/1000 | Loss: 0.00001699
Iteration 67/1000 | Loss: 0.00001698
Iteration 68/1000 | Loss: 0.00001698
Iteration 69/1000 | Loss: 0.00001698
Iteration 70/1000 | Loss: 0.00001697
Iteration 71/1000 | Loss: 0.00001697
Iteration 72/1000 | Loss: 0.00001697
Iteration 73/1000 | Loss: 0.00001696
Iteration 74/1000 | Loss: 0.00001696
Iteration 75/1000 | Loss: 0.00001695
Iteration 76/1000 | Loss: 0.00001695
Iteration 77/1000 | Loss: 0.00001695
Iteration 78/1000 | Loss: 0.00001695
Iteration 79/1000 | Loss: 0.00001694
Iteration 80/1000 | Loss: 0.00001694
Iteration 81/1000 | Loss: 0.00001694
Iteration 82/1000 | Loss: 0.00001694
Iteration 83/1000 | Loss: 0.00001694
Iteration 84/1000 | Loss: 0.00001693
Iteration 85/1000 | Loss: 0.00001693
Iteration 86/1000 | Loss: 0.00001693
Iteration 87/1000 | Loss: 0.00001692
Iteration 88/1000 | Loss: 0.00001692
Iteration 89/1000 | Loss: 0.00001692
Iteration 90/1000 | Loss: 0.00001691
Iteration 91/1000 | Loss: 0.00001691
Iteration 92/1000 | Loss: 0.00001691
Iteration 93/1000 | Loss: 0.00001690
Iteration 94/1000 | Loss: 0.00001690
Iteration 95/1000 | Loss: 0.00001690
Iteration 96/1000 | Loss: 0.00001690
Iteration 97/1000 | Loss: 0.00001689
Iteration 98/1000 | Loss: 0.00001689
Iteration 99/1000 | Loss: 0.00001689
Iteration 100/1000 | Loss: 0.00001689
Iteration 101/1000 | Loss: 0.00001689
Iteration 102/1000 | Loss: 0.00001689
Iteration 103/1000 | Loss: 0.00001689
Iteration 104/1000 | Loss: 0.00001689
Iteration 105/1000 | Loss: 0.00001689
Iteration 106/1000 | Loss: 0.00001688
Iteration 107/1000 | Loss: 0.00001688
Iteration 108/1000 | Loss: 0.00001688
Iteration 109/1000 | Loss: 0.00001688
Iteration 110/1000 | Loss: 0.00001687
Iteration 111/1000 | Loss: 0.00001687
Iteration 112/1000 | Loss: 0.00001687
Iteration 113/1000 | Loss: 0.00001686
Iteration 114/1000 | Loss: 0.00001686
Iteration 115/1000 | Loss: 0.00001686
Iteration 116/1000 | Loss: 0.00001686
Iteration 117/1000 | Loss: 0.00001686
Iteration 118/1000 | Loss: 0.00001685
Iteration 119/1000 | Loss: 0.00001685
Iteration 120/1000 | Loss: 0.00001685
Iteration 121/1000 | Loss: 0.00001685
Iteration 122/1000 | Loss: 0.00001685
Iteration 123/1000 | Loss: 0.00001685
Iteration 124/1000 | Loss: 0.00001684
Iteration 125/1000 | Loss: 0.00001684
Iteration 126/1000 | Loss: 0.00001684
Iteration 127/1000 | Loss: 0.00001684
Iteration 128/1000 | Loss: 0.00001684
Iteration 129/1000 | Loss: 0.00001683
Iteration 130/1000 | Loss: 0.00001683
Iteration 131/1000 | Loss: 0.00001683
Iteration 132/1000 | Loss: 0.00001683
Iteration 133/1000 | Loss: 0.00001683
Iteration 134/1000 | Loss: 0.00001682
Iteration 135/1000 | Loss: 0.00001682
Iteration 136/1000 | Loss: 0.00001682
Iteration 137/1000 | Loss: 0.00001682
Iteration 138/1000 | Loss: 0.00001682
Iteration 139/1000 | Loss: 0.00001682
Iteration 140/1000 | Loss: 0.00001681
Iteration 141/1000 | Loss: 0.00001681
Iteration 142/1000 | Loss: 0.00001681
Iteration 143/1000 | Loss: 0.00001681
Iteration 144/1000 | Loss: 0.00001681
Iteration 145/1000 | Loss: 0.00001681
Iteration 146/1000 | Loss: 0.00001681
Iteration 147/1000 | Loss: 0.00001681
Iteration 148/1000 | Loss: 0.00001681
Iteration 149/1000 | Loss: 0.00001681
Iteration 150/1000 | Loss: 0.00001681
Iteration 151/1000 | Loss: 0.00001681
Iteration 152/1000 | Loss: 0.00001681
Iteration 153/1000 | Loss: 0.00001681
Iteration 154/1000 | Loss: 0.00001681
Iteration 155/1000 | Loss: 0.00001681
Iteration 156/1000 | Loss: 0.00001681
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 156. Stopping optimization.
Last 5 losses: [1.680775312706828e-05, 1.680775312706828e-05, 1.680775312706828e-05, 1.680775312706828e-05, 1.680775312706828e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.680775312706828e-05

Optimization complete. Final v2v error: 3.3912856578826904 mm

Highest mean error: 5.3652424812316895 mm for frame 70

Lowest mean error: 2.8955860137939453 mm for frame 95

Saving results

Total time: 39.8108229637146
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_013/1069/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_013/1069.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_013/1069
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00913328
Iteration 2/25 | Loss: 0.00125804
Iteration 3/25 | Loss: 0.00113855
Iteration 4/25 | Loss: 0.00111629
Iteration 5/25 | Loss: 0.00110850
Iteration 6/25 | Loss: 0.00110671
Iteration 7/25 | Loss: 0.00110671
Iteration 8/25 | Loss: 0.00110656
Iteration 9/25 | Loss: 0.00110656
Iteration 10/25 | Loss: 0.00110656
Iteration 11/25 | Loss: 0.00110656
Iteration 12/25 | Loss: 0.00110656
Iteration 13/25 | Loss: 0.00110656
Iteration 14/25 | Loss: 0.00110656
Iteration 15/25 | Loss: 0.00110656
Iteration 16/25 | Loss: 0.00110656
Iteration 17/25 | Loss: 0.00110656
Iteration 18/25 | Loss: 0.00110656
Iteration 19/25 | Loss: 0.00110656
Iteration 20/25 | Loss: 0.00110656
Iteration 21/25 | Loss: 0.00110656
Iteration 22/25 | Loss: 0.00110656
Iteration 23/25 | Loss: 0.00110656
Iteration 24/25 | Loss: 0.00110656
Iteration 25/25 | Loss: 0.00110656

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37872088
Iteration 2/25 | Loss: 0.00071564
Iteration 3/25 | Loss: 0.00071563
Iteration 4/25 | Loss: 0.00071563
Iteration 5/25 | Loss: 0.00071563
Iteration 6/25 | Loss: 0.00071563
Iteration 7/25 | Loss: 0.00071563
Iteration 8/25 | Loss: 0.00071563
Iteration 9/25 | Loss: 0.00071563
Iteration 10/25 | Loss: 0.00071563
Iteration 11/25 | Loss: 0.00071563
Iteration 12/25 | Loss: 0.00071563
Iteration 13/25 | Loss: 0.00071563
Iteration 14/25 | Loss: 0.00071563
Iteration 15/25 | Loss: 0.00071563
Iteration 16/25 | Loss: 0.00071563
Iteration 17/25 | Loss: 0.00071563
Iteration 18/25 | Loss: 0.00071563
Iteration 19/25 | Loss: 0.00071563
Iteration 20/25 | Loss: 0.00071563
Iteration 21/25 | Loss: 0.00071563
Iteration 22/25 | Loss: 0.00071563
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0007156255305744708, 0.0007156255305744708, 0.0007156255305744708, 0.0007156255305744708, 0.0007156255305744708]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007156255305744708

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00071563
Iteration 2/1000 | Loss: 0.00004559
Iteration 3/1000 | Loss: 0.00003041
Iteration 4/1000 | Loss: 0.00002412
Iteration 5/1000 | Loss: 0.00002260
Iteration 6/1000 | Loss: 0.00002143
Iteration 7/1000 | Loss: 0.00002082
Iteration 8/1000 | Loss: 0.00002022
Iteration 9/1000 | Loss: 0.00001991
Iteration 10/1000 | Loss: 0.00001962
Iteration 11/1000 | Loss: 0.00001940
Iteration 12/1000 | Loss: 0.00001926
Iteration 13/1000 | Loss: 0.00001914
Iteration 14/1000 | Loss: 0.00001914
Iteration 15/1000 | Loss: 0.00001910
Iteration 16/1000 | Loss: 0.00001909
Iteration 17/1000 | Loss: 0.00001909
Iteration 18/1000 | Loss: 0.00001907
Iteration 19/1000 | Loss: 0.00001904
Iteration 20/1000 | Loss: 0.00001903
Iteration 21/1000 | Loss: 0.00001901
Iteration 22/1000 | Loss: 0.00001901
Iteration 23/1000 | Loss: 0.00001900
Iteration 24/1000 | Loss: 0.00001899
Iteration 25/1000 | Loss: 0.00001899
Iteration 26/1000 | Loss: 0.00001898
Iteration 27/1000 | Loss: 0.00001898
Iteration 28/1000 | Loss: 0.00001897
Iteration 29/1000 | Loss: 0.00001894
Iteration 30/1000 | Loss: 0.00001894
Iteration 31/1000 | Loss: 0.00001893
Iteration 32/1000 | Loss: 0.00001892
Iteration 33/1000 | Loss: 0.00001892
Iteration 34/1000 | Loss: 0.00001892
Iteration 35/1000 | Loss: 0.00001892
Iteration 36/1000 | Loss: 0.00001892
Iteration 37/1000 | Loss: 0.00001892
Iteration 38/1000 | Loss: 0.00001892
Iteration 39/1000 | Loss: 0.00001892
Iteration 40/1000 | Loss: 0.00001892
Iteration 41/1000 | Loss: 0.00001892
Iteration 42/1000 | Loss: 0.00001892
Iteration 43/1000 | Loss: 0.00001892
Iteration 44/1000 | Loss: 0.00001892
Iteration 45/1000 | Loss: 0.00001891
Iteration 46/1000 | Loss: 0.00001890
Iteration 47/1000 | Loss: 0.00001889
Iteration 48/1000 | Loss: 0.00001889
Iteration 49/1000 | Loss: 0.00001889
Iteration 50/1000 | Loss: 0.00001889
Iteration 51/1000 | Loss: 0.00001888
Iteration 52/1000 | Loss: 0.00001888
Iteration 53/1000 | Loss: 0.00001888
Iteration 54/1000 | Loss: 0.00001888
Iteration 55/1000 | Loss: 0.00001887
Iteration 56/1000 | Loss: 0.00001887
Iteration 57/1000 | Loss: 0.00001886
Iteration 58/1000 | Loss: 0.00001886
Iteration 59/1000 | Loss: 0.00001886
Iteration 60/1000 | Loss: 0.00001886
Iteration 61/1000 | Loss: 0.00001886
Iteration 62/1000 | Loss: 0.00001886
Iteration 63/1000 | Loss: 0.00001886
Iteration 64/1000 | Loss: 0.00001886
Iteration 65/1000 | Loss: 0.00001886
Iteration 66/1000 | Loss: 0.00001886
Iteration 67/1000 | Loss: 0.00001886
Iteration 68/1000 | Loss: 0.00001886
Iteration 69/1000 | Loss: 0.00001885
Iteration 70/1000 | Loss: 0.00001885
Iteration 71/1000 | Loss: 0.00001885
Iteration 72/1000 | Loss: 0.00001885
Iteration 73/1000 | Loss: 0.00001885
Iteration 74/1000 | Loss: 0.00001884
Iteration 75/1000 | Loss: 0.00001884
Iteration 76/1000 | Loss: 0.00001884
Iteration 77/1000 | Loss: 0.00001884
Iteration 78/1000 | Loss: 0.00001884
Iteration 79/1000 | Loss: 0.00001884
Iteration 80/1000 | Loss: 0.00001884
Iteration 81/1000 | Loss: 0.00001884
Iteration 82/1000 | Loss: 0.00001884
Iteration 83/1000 | Loss: 0.00001883
Iteration 84/1000 | Loss: 0.00001883
Iteration 85/1000 | Loss: 0.00001883
Iteration 86/1000 | Loss: 0.00001883
Iteration 87/1000 | Loss: 0.00001882
Iteration 88/1000 | Loss: 0.00001882
Iteration 89/1000 | Loss: 0.00001882
Iteration 90/1000 | Loss: 0.00001881
Iteration 91/1000 | Loss: 0.00001881
Iteration 92/1000 | Loss: 0.00001881
Iteration 93/1000 | Loss: 0.00001880
Iteration 94/1000 | Loss: 0.00001880
Iteration 95/1000 | Loss: 0.00001879
Iteration 96/1000 | Loss: 0.00001879
Iteration 97/1000 | Loss: 0.00001879
Iteration 98/1000 | Loss: 0.00001879
Iteration 99/1000 | Loss: 0.00001878
Iteration 100/1000 | Loss: 0.00001878
Iteration 101/1000 | Loss: 0.00001878
Iteration 102/1000 | Loss: 0.00001878
Iteration 103/1000 | Loss: 0.00001878
Iteration 104/1000 | Loss: 0.00001878
Iteration 105/1000 | Loss: 0.00001878
Iteration 106/1000 | Loss: 0.00001877
Iteration 107/1000 | Loss: 0.00001877
Iteration 108/1000 | Loss: 0.00001877
Iteration 109/1000 | Loss: 0.00001877
Iteration 110/1000 | Loss: 0.00001877
Iteration 111/1000 | Loss: 0.00001876
Iteration 112/1000 | Loss: 0.00001876
Iteration 113/1000 | Loss: 0.00001876
Iteration 114/1000 | Loss: 0.00001876
Iteration 115/1000 | Loss: 0.00001875
Iteration 116/1000 | Loss: 0.00001875
Iteration 117/1000 | Loss: 0.00001874
Iteration 118/1000 | Loss: 0.00001874
Iteration 119/1000 | Loss: 0.00001874
Iteration 120/1000 | Loss: 0.00001874
Iteration 121/1000 | Loss: 0.00001873
Iteration 122/1000 | Loss: 0.00001873
Iteration 123/1000 | Loss: 0.00001873
Iteration 124/1000 | Loss: 0.00001872
Iteration 125/1000 | Loss: 0.00001872
Iteration 126/1000 | Loss: 0.00001872
Iteration 127/1000 | Loss: 0.00001872
Iteration 128/1000 | Loss: 0.00001871
Iteration 129/1000 | Loss: 0.00001871
Iteration 130/1000 | Loss: 0.00001871
Iteration 131/1000 | Loss: 0.00001871
Iteration 132/1000 | Loss: 0.00001871
Iteration 133/1000 | Loss: 0.00001871
Iteration 134/1000 | Loss: 0.00001871
Iteration 135/1000 | Loss: 0.00001871
Iteration 136/1000 | Loss: 0.00001870
Iteration 137/1000 | Loss: 0.00001870
Iteration 138/1000 | Loss: 0.00001870
Iteration 139/1000 | Loss: 0.00001870
Iteration 140/1000 | Loss: 0.00001870
Iteration 141/1000 | Loss: 0.00001870
Iteration 142/1000 | Loss: 0.00001870
Iteration 143/1000 | Loss: 0.00001870
Iteration 144/1000 | Loss: 0.00001870
Iteration 145/1000 | Loss: 0.00001870
Iteration 146/1000 | Loss: 0.00001869
Iteration 147/1000 | Loss: 0.00001869
Iteration 148/1000 | Loss: 0.00001869
Iteration 149/1000 | Loss: 0.00001869
Iteration 150/1000 | Loss: 0.00001869
Iteration 151/1000 | Loss: 0.00001868
Iteration 152/1000 | Loss: 0.00001868
Iteration 153/1000 | Loss: 0.00001868
Iteration 154/1000 | Loss: 0.00001868
Iteration 155/1000 | Loss: 0.00001868
Iteration 156/1000 | Loss: 0.00001867
Iteration 157/1000 | Loss: 0.00001867
Iteration 158/1000 | Loss: 0.00001867
Iteration 159/1000 | Loss: 0.00001867
Iteration 160/1000 | Loss: 0.00001867
Iteration 161/1000 | Loss: 0.00001867
Iteration 162/1000 | Loss: 0.00001866
Iteration 163/1000 | Loss: 0.00001866
Iteration 164/1000 | Loss: 0.00001866
Iteration 165/1000 | Loss: 0.00001866
Iteration 166/1000 | Loss: 0.00001866
Iteration 167/1000 | Loss: 0.00001865
Iteration 168/1000 | Loss: 0.00001865
Iteration 169/1000 | Loss: 0.00001865
Iteration 170/1000 | Loss: 0.00001865
Iteration 171/1000 | Loss: 0.00001865
Iteration 172/1000 | Loss: 0.00001865
Iteration 173/1000 | Loss: 0.00001865
Iteration 174/1000 | Loss: 0.00001865
Iteration 175/1000 | Loss: 0.00001865
Iteration 176/1000 | Loss: 0.00001865
Iteration 177/1000 | Loss: 0.00001864
Iteration 178/1000 | Loss: 0.00001864
Iteration 179/1000 | Loss: 0.00001864
Iteration 180/1000 | Loss: 0.00001864
Iteration 181/1000 | Loss: 0.00001864
Iteration 182/1000 | Loss: 0.00001864
Iteration 183/1000 | Loss: 0.00001863
Iteration 184/1000 | Loss: 0.00001863
Iteration 185/1000 | Loss: 0.00001863
Iteration 186/1000 | Loss: 0.00001863
Iteration 187/1000 | Loss: 0.00001863
Iteration 188/1000 | Loss: 0.00001863
Iteration 189/1000 | Loss: 0.00001863
Iteration 190/1000 | Loss: 0.00001862
Iteration 191/1000 | Loss: 0.00001862
Iteration 192/1000 | Loss: 0.00001862
Iteration 193/1000 | Loss: 0.00001862
Iteration 194/1000 | Loss: 0.00001861
Iteration 195/1000 | Loss: 0.00001861
Iteration 196/1000 | Loss: 0.00001861
Iteration 197/1000 | Loss: 0.00001861
Iteration 198/1000 | Loss: 0.00001861
Iteration 199/1000 | Loss: 0.00001861
Iteration 200/1000 | Loss: 0.00001861
Iteration 201/1000 | Loss: 0.00001861
Iteration 202/1000 | Loss: 0.00001861
Iteration 203/1000 | Loss: 0.00001861
Iteration 204/1000 | Loss: 0.00001861
Iteration 205/1000 | Loss: 0.00001861
Iteration 206/1000 | Loss: 0.00001861
Iteration 207/1000 | Loss: 0.00001861
Iteration 208/1000 | Loss: 0.00001861
Iteration 209/1000 | Loss: 0.00001861
Iteration 210/1000 | Loss: 0.00001861
Iteration 211/1000 | Loss: 0.00001861
Iteration 212/1000 | Loss: 0.00001861
Iteration 213/1000 | Loss: 0.00001861
Iteration 214/1000 | Loss: 0.00001861
Iteration 215/1000 | Loss: 0.00001861
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 215. Stopping optimization.
Last 5 losses: [1.8607806850923225e-05, 1.8607806850923225e-05, 1.8607806850923225e-05, 1.8607806850923225e-05, 1.8607806850923225e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8607806850923225e-05

Optimization complete. Final v2v error: 3.5667080879211426 mm

Highest mean error: 5.577000617980957 mm for frame 70

Lowest mean error: 3.0821685791015625 mm for frame 15

Saving results

Total time: 44.03401780128479
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_013/1021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_013/1021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_013/1021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00930481
Iteration 2/25 | Loss: 0.00362035
Iteration 3/25 | Loss: 0.00273337
Iteration 4/25 | Loss: 0.00245702
Iteration 5/25 | Loss: 0.00289773
Iteration 6/25 | Loss: 0.00235794
Iteration 7/25 | Loss: 0.00231227
Iteration 8/25 | Loss: 0.00217638
Iteration 9/25 | Loss: 0.00194039
Iteration 10/25 | Loss: 0.00182165
Iteration 11/25 | Loss: 0.00170925
Iteration 12/25 | Loss: 0.00171080
Iteration 13/25 | Loss: 0.00173480
Iteration 14/25 | Loss: 0.00167941
Iteration 15/25 | Loss: 0.00169040
Iteration 16/25 | Loss: 0.00163256
Iteration 17/25 | Loss: 0.00162510
Iteration 18/25 | Loss: 0.00160856
Iteration 19/25 | Loss: 0.00159670
Iteration 20/25 | Loss: 0.00158139
Iteration 21/25 | Loss: 0.00157085
Iteration 22/25 | Loss: 0.00156483
Iteration 23/25 | Loss: 0.00155452
Iteration 24/25 | Loss: 0.00156489
Iteration 25/25 | Loss: 0.00155491

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.92527127
Iteration 2/25 | Loss: 0.00496595
Iteration 3/25 | Loss: 0.00492323
Iteration 4/25 | Loss: 0.00492323
Iteration 5/25 | Loss: 0.00492323
Iteration 6/25 | Loss: 0.00492323
Iteration 7/25 | Loss: 0.00492322
Iteration 8/25 | Loss: 0.00492322
Iteration 9/25 | Loss: 0.00492322
Iteration 10/25 | Loss: 0.00492322
Iteration 11/25 | Loss: 0.00492322
Iteration 12/25 | Loss: 0.00492322
Iteration 13/25 | Loss: 0.00492322
Iteration 14/25 | Loss: 0.00492322
Iteration 15/25 | Loss: 0.00492322
Iteration 16/25 | Loss: 0.00492322
Iteration 17/25 | Loss: 0.00492322
Iteration 18/25 | Loss: 0.00492322
Iteration 19/25 | Loss: 0.00492322
Iteration 20/25 | Loss: 0.00492322
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0049232239834964275, 0.0049232239834964275, 0.0049232239834964275, 0.0049232239834964275, 0.0049232239834964275]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0049232239834964275

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00492322
Iteration 2/1000 | Loss: 0.00200944
Iteration 3/1000 | Loss: 0.00126862
Iteration 4/1000 | Loss: 0.00087145
Iteration 5/1000 | Loss: 0.00462588
Iteration 6/1000 | Loss: 0.00437015
Iteration 7/1000 | Loss: 0.00282438
Iteration 8/1000 | Loss: 0.00117774
Iteration 9/1000 | Loss: 0.00402395
Iteration 10/1000 | Loss: 0.00197300
Iteration 11/1000 | Loss: 0.00258075
Iteration 12/1000 | Loss: 0.00088978
Iteration 13/1000 | Loss: 0.00196036
Iteration 14/1000 | Loss: 0.00172564
Iteration 15/1000 | Loss: 0.00123330
Iteration 16/1000 | Loss: 0.00088688
Iteration 17/1000 | Loss: 0.00076187
Iteration 18/1000 | Loss: 0.00131092
Iteration 19/1000 | Loss: 0.00165336
Iteration 20/1000 | Loss: 0.00081595
Iteration 21/1000 | Loss: 0.00088640
Iteration 22/1000 | Loss: 0.00065100
Iteration 23/1000 | Loss: 0.00023294
Iteration 24/1000 | Loss: 0.00071135
Iteration 25/1000 | Loss: 0.00085342
Iteration 26/1000 | Loss: 0.00027572
Iteration 27/1000 | Loss: 0.00044445
Iteration 28/1000 | Loss: 0.00036802
Iteration 29/1000 | Loss: 0.00023984
Iteration 30/1000 | Loss: 0.00039148
Iteration 31/1000 | Loss: 0.00157608
Iteration 32/1000 | Loss: 0.00168274
Iteration 33/1000 | Loss: 0.00049525
Iteration 34/1000 | Loss: 0.00032642
Iteration 35/1000 | Loss: 0.00238549
Iteration 36/1000 | Loss: 0.00035413
Iteration 37/1000 | Loss: 0.00021117
Iteration 38/1000 | Loss: 0.00039437
Iteration 39/1000 | Loss: 0.00036752
Iteration 40/1000 | Loss: 0.00033166
Iteration 41/1000 | Loss: 0.00038219
Iteration 42/1000 | Loss: 0.00029087
Iteration 43/1000 | Loss: 0.00031647
Iteration 44/1000 | Loss: 0.00024661
Iteration 45/1000 | Loss: 0.00038686
Iteration 46/1000 | Loss: 0.00211009
Iteration 47/1000 | Loss: 0.00029618
Iteration 48/1000 | Loss: 0.00038284
Iteration 49/1000 | Loss: 0.00033363
Iteration 50/1000 | Loss: 0.00013187
Iteration 51/1000 | Loss: 0.00046383
Iteration 52/1000 | Loss: 0.00021324
Iteration 53/1000 | Loss: 0.00139562
Iteration 54/1000 | Loss: 0.00282011
Iteration 55/1000 | Loss: 0.00145057
Iteration 56/1000 | Loss: 0.00118374
Iteration 57/1000 | Loss: 0.00052104
Iteration 58/1000 | Loss: 0.00055468
Iteration 59/1000 | Loss: 0.00028262
Iteration 60/1000 | Loss: 0.00013347
Iteration 61/1000 | Loss: 0.00077501
Iteration 62/1000 | Loss: 0.00039797
Iteration 63/1000 | Loss: 0.00033040
Iteration 64/1000 | Loss: 0.00031224
Iteration 65/1000 | Loss: 0.00029273
Iteration 66/1000 | Loss: 0.00178547
Iteration 67/1000 | Loss: 0.00019601
Iteration 68/1000 | Loss: 0.00049024
Iteration 69/1000 | Loss: 0.00041849
Iteration 70/1000 | Loss: 0.00025526
Iteration 71/1000 | Loss: 0.00011278
Iteration 72/1000 | Loss: 0.00010846
Iteration 73/1000 | Loss: 0.00010625
Iteration 74/1000 | Loss: 0.00017005
Iteration 75/1000 | Loss: 0.00063955
Iteration 76/1000 | Loss: 0.00022860
Iteration 77/1000 | Loss: 0.00010467
Iteration 78/1000 | Loss: 0.00010326
Iteration 79/1000 | Loss: 0.00020181
Iteration 80/1000 | Loss: 0.00010319
Iteration 81/1000 | Loss: 0.00010103
Iteration 82/1000 | Loss: 0.00024493
Iteration 83/1000 | Loss: 0.00056128
Iteration 84/1000 | Loss: 0.00010305
Iteration 85/1000 | Loss: 0.00010044
Iteration 86/1000 | Loss: 0.00009886
Iteration 87/1000 | Loss: 0.00009678
Iteration 88/1000 | Loss: 0.00009566
Iteration 89/1000 | Loss: 0.00009464
Iteration 90/1000 | Loss: 0.00009383
Iteration 91/1000 | Loss: 0.00056796
Iteration 92/1000 | Loss: 0.00010205
Iteration 93/1000 | Loss: 0.00056845
Iteration 94/1000 | Loss: 0.00097770
Iteration 95/1000 | Loss: 0.00054169
Iteration 96/1000 | Loss: 0.00009704
Iteration 97/1000 | Loss: 0.00009263
Iteration 98/1000 | Loss: 0.00009140
Iteration 99/1000 | Loss: 0.00009040
Iteration 100/1000 | Loss: 0.00071223
Iteration 101/1000 | Loss: 0.00056744
Iteration 102/1000 | Loss: 0.00050622
Iteration 103/1000 | Loss: 0.00157956
Iteration 104/1000 | Loss: 0.00020009
Iteration 105/1000 | Loss: 0.00009015
Iteration 106/1000 | Loss: 0.00046334
Iteration 107/1000 | Loss: 0.00009004
Iteration 108/1000 | Loss: 0.00008768
Iteration 109/1000 | Loss: 0.00008667
Iteration 110/1000 | Loss: 0.00008562
Iteration 111/1000 | Loss: 0.00008469
Iteration 112/1000 | Loss: 0.00008379
Iteration 113/1000 | Loss: 0.00008329
Iteration 114/1000 | Loss: 0.00008281
Iteration 115/1000 | Loss: 0.00008238
Iteration 116/1000 | Loss: 0.00008164
Iteration 117/1000 | Loss: 0.00008105
Iteration 118/1000 | Loss: 0.00008066
Iteration 119/1000 | Loss: 0.00008022
Iteration 120/1000 | Loss: 0.00007975
Iteration 121/1000 | Loss: 0.00007921
Iteration 122/1000 | Loss: 0.00007873
Iteration 123/1000 | Loss: 0.00007834
Iteration 124/1000 | Loss: 0.00007805
Iteration 125/1000 | Loss: 0.00007776
Iteration 126/1000 | Loss: 0.00007751
Iteration 127/1000 | Loss: 0.00007749
Iteration 128/1000 | Loss: 0.00007749
Iteration 129/1000 | Loss: 0.00007733
Iteration 130/1000 | Loss: 0.00007721
Iteration 131/1000 | Loss: 0.00007717
Iteration 132/1000 | Loss: 0.00007717
Iteration 133/1000 | Loss: 0.00007713
Iteration 134/1000 | Loss: 0.00007710
Iteration 135/1000 | Loss: 0.00007709
Iteration 136/1000 | Loss: 0.00007703
Iteration 137/1000 | Loss: 0.00007702
Iteration 138/1000 | Loss: 0.00007700
Iteration 139/1000 | Loss: 0.00007700
Iteration 140/1000 | Loss: 0.00007700
Iteration 141/1000 | Loss: 0.00007699
Iteration 142/1000 | Loss: 0.00007698
Iteration 143/1000 | Loss: 0.00007696
Iteration 144/1000 | Loss: 0.00007696
Iteration 145/1000 | Loss: 0.00007696
Iteration 146/1000 | Loss: 0.00007696
Iteration 147/1000 | Loss: 0.00007696
Iteration 148/1000 | Loss: 0.00007696
Iteration 149/1000 | Loss: 0.00007696
Iteration 150/1000 | Loss: 0.00007696
Iteration 151/1000 | Loss: 0.00007696
Iteration 152/1000 | Loss: 0.00007696
Iteration 153/1000 | Loss: 0.00007695
Iteration 154/1000 | Loss: 0.00007695
Iteration 155/1000 | Loss: 0.00007695
Iteration 156/1000 | Loss: 0.00007694
Iteration 157/1000 | Loss: 0.00007693
Iteration 158/1000 | Loss: 0.00007693
Iteration 159/1000 | Loss: 0.00007693
Iteration 160/1000 | Loss: 0.00007692
Iteration 161/1000 | Loss: 0.00007692
Iteration 162/1000 | Loss: 0.00007692
Iteration 163/1000 | Loss: 0.00007691
Iteration 164/1000 | Loss: 0.00007691
Iteration 165/1000 | Loss: 0.00007691
Iteration 166/1000 | Loss: 0.00007690
Iteration 167/1000 | Loss: 0.00007690
Iteration 168/1000 | Loss: 0.00007690
Iteration 169/1000 | Loss: 0.00007690
Iteration 170/1000 | Loss: 0.00007689
Iteration 171/1000 | Loss: 0.00007688
Iteration 172/1000 | Loss: 0.00007687
Iteration 173/1000 | Loss: 0.00007687
Iteration 174/1000 | Loss: 0.00007686
Iteration 175/1000 | Loss: 0.00007686
Iteration 176/1000 | Loss: 0.00007686
Iteration 177/1000 | Loss: 0.00007685
Iteration 178/1000 | Loss: 0.00007685
Iteration 179/1000 | Loss: 0.00007685
Iteration 180/1000 | Loss: 0.00007685
Iteration 181/1000 | Loss: 0.00007683
Iteration 182/1000 | Loss: 0.00007683
Iteration 183/1000 | Loss: 0.00007683
Iteration 184/1000 | Loss: 0.00007682
Iteration 185/1000 | Loss: 0.00007681
Iteration 186/1000 | Loss: 0.00007681
Iteration 187/1000 | Loss: 0.00007681
Iteration 188/1000 | Loss: 0.00007680
Iteration 189/1000 | Loss: 0.00007680
Iteration 190/1000 | Loss: 0.00007680
Iteration 191/1000 | Loss: 0.00007680
Iteration 192/1000 | Loss: 0.00007680
Iteration 193/1000 | Loss: 0.00007680
Iteration 194/1000 | Loss: 0.00007680
Iteration 195/1000 | Loss: 0.00007680
Iteration 196/1000 | Loss: 0.00007679
Iteration 197/1000 | Loss: 0.00007679
Iteration 198/1000 | Loss: 0.00007679
Iteration 199/1000 | Loss: 0.00007679
Iteration 200/1000 | Loss: 0.00007679
Iteration 201/1000 | Loss: 0.00007679
Iteration 202/1000 | Loss: 0.00007679
Iteration 203/1000 | Loss: 0.00007679
Iteration 204/1000 | Loss: 0.00007678
Iteration 205/1000 | Loss: 0.00007678
Iteration 206/1000 | Loss: 0.00007678
Iteration 207/1000 | Loss: 0.00007678
Iteration 208/1000 | Loss: 0.00007678
Iteration 209/1000 | Loss: 0.00007678
Iteration 210/1000 | Loss: 0.00007677
Iteration 211/1000 | Loss: 0.00007677
Iteration 212/1000 | Loss: 0.00007677
Iteration 213/1000 | Loss: 0.00007677
Iteration 214/1000 | Loss: 0.00007676
Iteration 215/1000 | Loss: 0.00007676
Iteration 216/1000 | Loss: 0.00007676
Iteration 217/1000 | Loss: 0.00007676
Iteration 218/1000 | Loss: 0.00007675
Iteration 219/1000 | Loss: 0.00007675
Iteration 220/1000 | Loss: 0.00007675
Iteration 221/1000 | Loss: 0.00007675
Iteration 222/1000 | Loss: 0.00007675
Iteration 223/1000 | Loss: 0.00007675
Iteration 224/1000 | Loss: 0.00007674
Iteration 225/1000 | Loss: 0.00007674
Iteration 226/1000 | Loss: 0.00007674
Iteration 227/1000 | Loss: 0.00007674
Iteration 228/1000 | Loss: 0.00007674
Iteration 229/1000 | Loss: 0.00007674
Iteration 230/1000 | Loss: 0.00007674
Iteration 231/1000 | Loss: 0.00007673
Iteration 232/1000 | Loss: 0.00007673
Iteration 233/1000 | Loss: 0.00007673
Iteration 234/1000 | Loss: 0.00007673
Iteration 235/1000 | Loss: 0.00007673
Iteration 236/1000 | Loss: 0.00007673
Iteration 237/1000 | Loss: 0.00007673
Iteration 238/1000 | Loss: 0.00007672
Iteration 239/1000 | Loss: 0.00007672
Iteration 240/1000 | Loss: 0.00007672
Iteration 241/1000 | Loss: 0.00007671
Iteration 242/1000 | Loss: 0.00007671
Iteration 243/1000 | Loss: 0.00007671
Iteration 244/1000 | Loss: 0.00007671
Iteration 245/1000 | Loss: 0.00007671
Iteration 246/1000 | Loss: 0.00007670
Iteration 247/1000 | Loss: 0.00007670
Iteration 248/1000 | Loss: 0.00007670
Iteration 249/1000 | Loss: 0.00007669
Iteration 250/1000 | Loss: 0.00007669
Iteration 251/1000 | Loss: 0.00007669
Iteration 252/1000 | Loss: 0.00007668
Iteration 253/1000 | Loss: 0.00007668
Iteration 254/1000 | Loss: 0.00007668
Iteration 255/1000 | Loss: 0.00007667
Iteration 256/1000 | Loss: 0.00007667
Iteration 257/1000 | Loss: 0.00007666
Iteration 258/1000 | Loss: 0.00007666
Iteration 259/1000 | Loss: 0.00007666
Iteration 260/1000 | Loss: 0.00007665
Iteration 261/1000 | Loss: 0.00007665
Iteration 262/1000 | Loss: 0.00007664
Iteration 263/1000 | Loss: 0.00007664
Iteration 264/1000 | Loss: 0.00007662
Iteration 265/1000 | Loss: 0.00007662
Iteration 266/1000 | Loss: 0.00007661
Iteration 267/1000 | Loss: 0.00007661
Iteration 268/1000 | Loss: 0.00007661
Iteration 269/1000 | Loss: 0.00024487
Iteration 270/1000 | Loss: 0.00063180
Iteration 271/1000 | Loss: 0.00014435
Iteration 272/1000 | Loss: 0.00043877
Iteration 273/1000 | Loss: 0.00027719
Iteration 274/1000 | Loss: 0.00020537
Iteration 275/1000 | Loss: 0.00010109
Iteration 276/1000 | Loss: 0.00008130
Iteration 277/1000 | Loss: 0.00007754
Iteration 278/1000 | Loss: 0.00007560
Iteration 279/1000 | Loss: 0.00007369
Iteration 280/1000 | Loss: 0.00007255
Iteration 281/1000 | Loss: 0.00007185
Iteration 282/1000 | Loss: 0.00007130
Iteration 283/1000 | Loss: 0.00007092
Iteration 284/1000 | Loss: 0.00042927
Iteration 285/1000 | Loss: 0.00018245
Iteration 286/1000 | Loss: 0.00033223
Iteration 287/1000 | Loss: 0.00008167
Iteration 288/1000 | Loss: 0.00007694
Iteration 289/1000 | Loss: 0.00007351
Iteration 290/1000 | Loss: 0.00007168
Iteration 291/1000 | Loss: 0.00007071
Iteration 292/1000 | Loss: 0.00007010
Iteration 293/1000 | Loss: 0.00006970
Iteration 294/1000 | Loss: 0.00006939
Iteration 295/1000 | Loss: 0.00006906
Iteration 296/1000 | Loss: 0.00006890
Iteration 297/1000 | Loss: 0.00006887
Iteration 298/1000 | Loss: 0.00006884
Iteration 299/1000 | Loss: 0.00006883
Iteration 300/1000 | Loss: 0.00006879
Iteration 301/1000 | Loss: 0.00006875
Iteration 302/1000 | Loss: 0.00006875
Iteration 303/1000 | Loss: 0.00006875
Iteration 304/1000 | Loss: 0.00006874
Iteration 305/1000 | Loss: 0.00006873
Iteration 306/1000 | Loss: 0.00006873
Iteration 307/1000 | Loss: 0.00006873
Iteration 308/1000 | Loss: 0.00006873
Iteration 309/1000 | Loss: 0.00006873
Iteration 310/1000 | Loss: 0.00006873
Iteration 311/1000 | Loss: 0.00006872
Iteration 312/1000 | Loss: 0.00006872
Iteration 313/1000 | Loss: 0.00006872
Iteration 314/1000 | Loss: 0.00006872
Iteration 315/1000 | Loss: 0.00006872
Iteration 316/1000 | Loss: 0.00006872
Iteration 317/1000 | Loss: 0.00006872
Iteration 318/1000 | Loss: 0.00006872
Iteration 319/1000 | Loss: 0.00006872
Iteration 320/1000 | Loss: 0.00006872
Iteration 321/1000 | Loss: 0.00006872
Iteration 322/1000 | Loss: 0.00006871
Iteration 323/1000 | Loss: 0.00006871
Iteration 324/1000 | Loss: 0.00006871
Iteration 325/1000 | Loss: 0.00006871
Iteration 326/1000 | Loss: 0.00006871
Iteration 327/1000 | Loss: 0.00006871
Iteration 328/1000 | Loss: 0.00006870
Iteration 329/1000 | Loss: 0.00006870
Iteration 330/1000 | Loss: 0.00006870
Iteration 331/1000 | Loss: 0.00006870
Iteration 332/1000 | Loss: 0.00006869
Iteration 333/1000 | Loss: 0.00006869
Iteration 334/1000 | Loss: 0.00006869
Iteration 335/1000 | Loss: 0.00006869
Iteration 336/1000 | Loss: 0.00006869
Iteration 337/1000 | Loss: 0.00006868
Iteration 338/1000 | Loss: 0.00006868
Iteration 339/1000 | Loss: 0.00006868
Iteration 340/1000 | Loss: 0.00006868
Iteration 341/1000 | Loss: 0.00006868
Iteration 342/1000 | Loss: 0.00006868
Iteration 343/1000 | Loss: 0.00006868
Iteration 344/1000 | Loss: 0.00006868
Iteration 345/1000 | Loss: 0.00006868
Iteration 346/1000 | Loss: 0.00006868
Iteration 347/1000 | Loss: 0.00006867
Iteration 348/1000 | Loss: 0.00006867
Iteration 349/1000 | Loss: 0.00006867
Iteration 350/1000 | Loss: 0.00006867
Iteration 351/1000 | Loss: 0.00006867
Iteration 352/1000 | Loss: 0.00006867
Iteration 353/1000 | Loss: 0.00006867
Iteration 354/1000 | Loss: 0.00006867
Iteration 355/1000 | Loss: 0.00006866
Iteration 356/1000 | Loss: 0.00006866
Iteration 357/1000 | Loss: 0.00006866
Iteration 358/1000 | Loss: 0.00006866
Iteration 359/1000 | Loss: 0.00006866
Iteration 360/1000 | Loss: 0.00006866
Iteration 361/1000 | Loss: 0.00006865
Iteration 362/1000 | Loss: 0.00006865
Iteration 363/1000 | Loss: 0.00006865
Iteration 364/1000 | Loss: 0.00006865
Iteration 365/1000 | Loss: 0.00006865
Iteration 366/1000 | Loss: 0.00006865
Iteration 367/1000 | Loss: 0.00006865
Iteration 368/1000 | Loss: 0.00006865
Iteration 369/1000 | Loss: 0.00006865
Iteration 370/1000 | Loss: 0.00006865
Iteration 371/1000 | Loss: 0.00006865
Iteration 372/1000 | Loss: 0.00006865
Iteration 373/1000 | Loss: 0.00006865
Iteration 374/1000 | Loss: 0.00006865
Iteration 375/1000 | Loss: 0.00006865
Iteration 376/1000 | Loss: 0.00006865
Iteration 377/1000 | Loss: 0.00006865
Iteration 378/1000 | Loss: 0.00006865
Iteration 379/1000 | Loss: 0.00006865
Iteration 380/1000 | Loss: 0.00006865
Iteration 381/1000 | Loss: 0.00006865
Iteration 382/1000 | Loss: 0.00006865
Iteration 383/1000 | Loss: 0.00006865
Iteration 384/1000 | Loss: 0.00006865
Iteration 385/1000 | Loss: 0.00006865
Iteration 386/1000 | Loss: 0.00006865
Iteration 387/1000 | Loss: 0.00006865
Iteration 388/1000 | Loss: 0.00006865
Iteration 389/1000 | Loss: 0.00006865
Iteration 390/1000 | Loss: 0.00006865
Iteration 391/1000 | Loss: 0.00006865
Iteration 392/1000 | Loss: 0.00006865
Iteration 393/1000 | Loss: 0.00006865
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 393. Stopping optimization.
Last 5 losses: [6.865010072942823e-05, 6.865010072942823e-05, 6.865010072942823e-05, 6.865010072942823e-05, 6.865010072942823e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 6.865010072942823e-05

Optimization complete. Final v2v error: 4.322605609893799 mm

Highest mean error: 13.624242782592773 mm for frame 76

Lowest mean error: 2.669243574142456 mm for frame 11

Saving results

Total time: 334.3447222709656
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_013/1027/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_013/1027.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_013/1027
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00623089
Iteration 2/25 | Loss: 0.00160300
Iteration 3/25 | Loss: 0.00122423
Iteration 4/25 | Loss: 0.00116063
Iteration 5/25 | Loss: 0.00114214
Iteration 6/25 | Loss: 0.00112749
Iteration 7/25 | Loss: 0.00112595
Iteration 8/25 | Loss: 0.00112072
Iteration 9/25 | Loss: 0.00111862
Iteration 10/25 | Loss: 0.00113602
Iteration 11/25 | Loss: 0.00112326
Iteration 12/25 | Loss: 0.00109911
Iteration 13/25 | Loss: 0.00110143
Iteration 14/25 | Loss: 0.00109975
Iteration 15/25 | Loss: 0.00110234
Iteration 16/25 | Loss: 0.00109730
Iteration 17/25 | Loss: 0.00109596
Iteration 18/25 | Loss: 0.00109322
Iteration 19/25 | Loss: 0.00109869
Iteration 20/25 | Loss: 0.00109417
Iteration 21/25 | Loss: 0.00109866
Iteration 22/25 | Loss: 0.00109323
Iteration 23/25 | Loss: 0.00109383
Iteration 24/25 | Loss: 0.00109252
Iteration 25/25 | Loss: 0.00109301

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.74735332
Iteration 2/25 | Loss: 0.00094909
Iteration 3/25 | Loss: 0.00094906
Iteration 4/25 | Loss: 0.00094906
Iteration 5/25 | Loss: 0.00094906
Iteration 6/25 | Loss: 0.00094906
Iteration 7/25 | Loss: 0.00094906
Iteration 8/25 | Loss: 0.00094906
Iteration 9/25 | Loss: 0.00094906
Iteration 10/25 | Loss: 0.00094906
Iteration 11/25 | Loss: 0.00094906
Iteration 12/25 | Loss: 0.00094906
Iteration 13/25 | Loss: 0.00094906
Iteration 14/25 | Loss: 0.00094906
Iteration 15/25 | Loss: 0.00094906
Iteration 16/25 | Loss: 0.00094906
Iteration 17/25 | Loss: 0.00094906
Iteration 18/25 | Loss: 0.00094906
Iteration 19/25 | Loss: 0.00094906
Iteration 20/25 | Loss: 0.00094906
Iteration 21/25 | Loss: 0.00094906
Iteration 22/25 | Loss: 0.00094906
Iteration 23/25 | Loss: 0.00094906
Iteration 24/25 | Loss: 0.00094906
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0009490568772889674, 0.0009490568772889674, 0.0009490568772889674, 0.0009490568772889674, 0.0009490568772889674]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009490568772889674

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00094906
Iteration 2/1000 | Loss: 0.00005776
Iteration 3/1000 | Loss: 0.00003397
Iteration 4/1000 | Loss: 0.00008910
Iteration 5/1000 | Loss: 0.00003715
Iteration 6/1000 | Loss: 0.00002281
Iteration 7/1000 | Loss: 0.00002826
Iteration 8/1000 | Loss: 0.00005634
Iteration 9/1000 | Loss: 0.00014069
Iteration 10/1000 | Loss: 0.00002715
Iteration 11/1000 | Loss: 0.00002173
Iteration 12/1000 | Loss: 0.00008607
Iteration 13/1000 | Loss: 0.00001896
Iteration 14/1000 | Loss: 0.00004708
Iteration 15/1000 | Loss: 0.00001829
Iteration 16/1000 | Loss: 0.00003978
Iteration 17/1000 | Loss: 0.00001812
Iteration 18/1000 | Loss: 0.00001785
Iteration 19/1000 | Loss: 0.00001763
Iteration 20/1000 | Loss: 0.00001742
Iteration 21/1000 | Loss: 0.00001741
Iteration 22/1000 | Loss: 0.00001740
Iteration 23/1000 | Loss: 0.00001739
Iteration 24/1000 | Loss: 0.00001737
Iteration 25/1000 | Loss: 0.00001736
Iteration 26/1000 | Loss: 0.00001736
Iteration 27/1000 | Loss: 0.00007345
Iteration 28/1000 | Loss: 0.00001996
Iteration 29/1000 | Loss: 0.00001728
Iteration 30/1000 | Loss: 0.00001727
Iteration 31/1000 | Loss: 0.00001726
Iteration 32/1000 | Loss: 0.00001726
Iteration 33/1000 | Loss: 0.00001725
Iteration 34/1000 | Loss: 0.00001724
Iteration 35/1000 | Loss: 0.00001724
Iteration 36/1000 | Loss: 0.00001722
Iteration 37/1000 | Loss: 0.00001722
Iteration 38/1000 | Loss: 0.00001722
Iteration 39/1000 | Loss: 0.00001722
Iteration 40/1000 | Loss: 0.00001722
Iteration 41/1000 | Loss: 0.00001721
Iteration 42/1000 | Loss: 0.00001721
Iteration 43/1000 | Loss: 0.00001717
Iteration 44/1000 | Loss: 0.00001716
Iteration 45/1000 | Loss: 0.00001716
Iteration 46/1000 | Loss: 0.00001715
Iteration 47/1000 | Loss: 0.00001715
Iteration 48/1000 | Loss: 0.00001715
Iteration 49/1000 | Loss: 0.00001714
Iteration 50/1000 | Loss: 0.00001714
Iteration 51/1000 | Loss: 0.00001713
Iteration 52/1000 | Loss: 0.00001713
Iteration 53/1000 | Loss: 0.00001712
Iteration 54/1000 | Loss: 0.00001712
Iteration 55/1000 | Loss: 0.00001712
Iteration 56/1000 | Loss: 0.00001711
Iteration 57/1000 | Loss: 0.00001710
Iteration 58/1000 | Loss: 0.00001710
Iteration 59/1000 | Loss: 0.00001710
Iteration 60/1000 | Loss: 0.00001709
Iteration 61/1000 | Loss: 0.00001709
Iteration 62/1000 | Loss: 0.00001708
Iteration 63/1000 | Loss: 0.00019820
Iteration 64/1000 | Loss: 0.00002621
Iteration 65/1000 | Loss: 0.00002084
Iteration 66/1000 | Loss: 0.00001910
Iteration 67/1000 | Loss: 0.00007257
Iteration 68/1000 | Loss: 0.00004140
Iteration 69/1000 | Loss: 0.00003843
Iteration 70/1000 | Loss: 0.00001865
Iteration 71/1000 | Loss: 0.00001789
Iteration 72/1000 | Loss: 0.00004681
Iteration 73/1000 | Loss: 0.00002136
Iteration 74/1000 | Loss: 0.00003912
Iteration 75/1000 | Loss: 0.00002251
Iteration 76/1000 | Loss: 0.00001920
Iteration 77/1000 | Loss: 0.00001750
Iteration 78/1000 | Loss: 0.00003750
Iteration 79/1000 | Loss: 0.00003454
Iteration 80/1000 | Loss: 0.00003755
Iteration 81/1000 | Loss: 0.00025077
Iteration 82/1000 | Loss: 0.00001764
Iteration 83/1000 | Loss: 0.00005953
Iteration 84/1000 | Loss: 0.00009022
Iteration 85/1000 | Loss: 0.00001946
Iteration 86/1000 | Loss: 0.00001746
Iteration 87/1000 | Loss: 0.00005687
Iteration 88/1000 | Loss: 0.00003944
Iteration 89/1000 | Loss: 0.00002103
Iteration 90/1000 | Loss: 0.00001836
Iteration 91/1000 | Loss: 0.00001798
Iteration 92/1000 | Loss: 0.00001771
Iteration 93/1000 | Loss: 0.00008998
Iteration 94/1000 | Loss: 0.00002108
Iteration 95/1000 | Loss: 0.00004038
Iteration 96/1000 | Loss: 0.00002324
Iteration 97/1000 | Loss: 0.00004134
Iteration 98/1000 | Loss: 0.00001703
Iteration 99/1000 | Loss: 0.00001701
Iteration 100/1000 | Loss: 0.00001701
Iteration 101/1000 | Loss: 0.00004505
Iteration 102/1000 | Loss: 0.00001702
Iteration 103/1000 | Loss: 0.00001687
Iteration 104/1000 | Loss: 0.00001687
Iteration 105/1000 | Loss: 0.00001686
Iteration 106/1000 | Loss: 0.00001685
Iteration 107/1000 | Loss: 0.00001683
Iteration 108/1000 | Loss: 0.00001683
Iteration 109/1000 | Loss: 0.00001683
Iteration 110/1000 | Loss: 0.00001682
Iteration 111/1000 | Loss: 0.00001681
Iteration 112/1000 | Loss: 0.00001681
Iteration 113/1000 | Loss: 0.00001681
Iteration 114/1000 | Loss: 0.00001680
Iteration 115/1000 | Loss: 0.00001678
Iteration 116/1000 | Loss: 0.00001678
Iteration 117/1000 | Loss: 0.00001677
Iteration 118/1000 | Loss: 0.00001676
Iteration 119/1000 | Loss: 0.00001676
Iteration 120/1000 | Loss: 0.00001676
Iteration 121/1000 | Loss: 0.00001676
Iteration 122/1000 | Loss: 0.00001676
Iteration 123/1000 | Loss: 0.00001675
Iteration 124/1000 | Loss: 0.00001675
Iteration 125/1000 | Loss: 0.00001675
Iteration 126/1000 | Loss: 0.00001675
Iteration 127/1000 | Loss: 0.00001675
Iteration 128/1000 | Loss: 0.00001675
Iteration 129/1000 | Loss: 0.00001675
Iteration 130/1000 | Loss: 0.00001674
Iteration 131/1000 | Loss: 0.00001674
Iteration 132/1000 | Loss: 0.00001674
Iteration 133/1000 | Loss: 0.00001674
Iteration 134/1000 | Loss: 0.00001673
Iteration 135/1000 | Loss: 0.00001673
Iteration 136/1000 | Loss: 0.00001673
Iteration 137/1000 | Loss: 0.00001673
Iteration 138/1000 | Loss: 0.00001673
Iteration 139/1000 | Loss: 0.00001673
Iteration 140/1000 | Loss: 0.00001672
Iteration 141/1000 | Loss: 0.00001672
Iteration 142/1000 | Loss: 0.00001672
Iteration 143/1000 | Loss: 0.00001672
Iteration 144/1000 | Loss: 0.00001671
Iteration 145/1000 | Loss: 0.00001671
Iteration 146/1000 | Loss: 0.00001671
Iteration 147/1000 | Loss: 0.00001670
Iteration 148/1000 | Loss: 0.00001670
Iteration 149/1000 | Loss: 0.00001670
Iteration 150/1000 | Loss: 0.00001670
Iteration 151/1000 | Loss: 0.00001669
Iteration 152/1000 | Loss: 0.00001668
Iteration 153/1000 | Loss: 0.00001668
Iteration 154/1000 | Loss: 0.00001668
Iteration 155/1000 | Loss: 0.00001668
Iteration 156/1000 | Loss: 0.00001667
Iteration 157/1000 | Loss: 0.00001667
Iteration 158/1000 | Loss: 0.00001667
Iteration 159/1000 | Loss: 0.00001667
Iteration 160/1000 | Loss: 0.00001667
Iteration 161/1000 | Loss: 0.00001667
Iteration 162/1000 | Loss: 0.00001666
Iteration 163/1000 | Loss: 0.00001666
Iteration 164/1000 | Loss: 0.00001665
Iteration 165/1000 | Loss: 0.00001664
Iteration 166/1000 | Loss: 0.00001664
Iteration 167/1000 | Loss: 0.00001664
Iteration 168/1000 | Loss: 0.00001663
Iteration 169/1000 | Loss: 0.00001663
Iteration 170/1000 | Loss: 0.00001663
Iteration 171/1000 | Loss: 0.00001662
Iteration 172/1000 | Loss: 0.00001662
Iteration 173/1000 | Loss: 0.00001662
Iteration 174/1000 | Loss: 0.00001662
Iteration 175/1000 | Loss: 0.00001662
Iteration 176/1000 | Loss: 0.00001661
Iteration 177/1000 | Loss: 0.00001661
Iteration 178/1000 | Loss: 0.00001661
Iteration 179/1000 | Loss: 0.00001661
Iteration 180/1000 | Loss: 0.00001661
Iteration 181/1000 | Loss: 0.00001660
Iteration 182/1000 | Loss: 0.00001660
Iteration 183/1000 | Loss: 0.00001660
Iteration 184/1000 | Loss: 0.00001660
Iteration 185/1000 | Loss: 0.00001660
Iteration 186/1000 | Loss: 0.00001660
Iteration 187/1000 | Loss: 0.00001660
Iteration 188/1000 | Loss: 0.00001660
Iteration 189/1000 | Loss: 0.00001659
Iteration 190/1000 | Loss: 0.00001659
Iteration 191/1000 | Loss: 0.00001659
Iteration 192/1000 | Loss: 0.00001659
Iteration 193/1000 | Loss: 0.00001659
Iteration 194/1000 | Loss: 0.00001659
Iteration 195/1000 | Loss: 0.00001658
Iteration 196/1000 | Loss: 0.00001658
Iteration 197/1000 | Loss: 0.00001658
Iteration 198/1000 | Loss: 0.00001657
Iteration 199/1000 | Loss: 0.00001656
Iteration 200/1000 | Loss: 0.00001656
Iteration 201/1000 | Loss: 0.00001655
Iteration 202/1000 | Loss: 0.00001655
Iteration 203/1000 | Loss: 0.00001654
Iteration 204/1000 | Loss: 0.00001654
Iteration 205/1000 | Loss: 0.00001654
Iteration 206/1000 | Loss: 0.00001653
Iteration 207/1000 | Loss: 0.00001653
Iteration 208/1000 | Loss: 0.00001653
Iteration 209/1000 | Loss: 0.00001652
Iteration 210/1000 | Loss: 0.00001652
Iteration 211/1000 | Loss: 0.00001652
Iteration 212/1000 | Loss: 0.00001651
Iteration 213/1000 | Loss: 0.00001651
Iteration 214/1000 | Loss: 0.00001651
Iteration 215/1000 | Loss: 0.00002882
Iteration 216/1000 | Loss: 0.00001657
Iteration 217/1000 | Loss: 0.00001921
Iteration 218/1000 | Loss: 0.00001649
Iteration 219/1000 | Loss: 0.00001649
Iteration 220/1000 | Loss: 0.00001649
Iteration 221/1000 | Loss: 0.00001649
Iteration 222/1000 | Loss: 0.00001649
Iteration 223/1000 | Loss: 0.00001649
Iteration 224/1000 | Loss: 0.00001649
Iteration 225/1000 | Loss: 0.00001649
Iteration 226/1000 | Loss: 0.00001649
Iteration 227/1000 | Loss: 0.00001649
Iteration 228/1000 | Loss: 0.00001648
Iteration 229/1000 | Loss: 0.00001648
Iteration 230/1000 | Loss: 0.00001648
Iteration 231/1000 | Loss: 0.00001648
Iteration 232/1000 | Loss: 0.00001648
Iteration 233/1000 | Loss: 0.00001648
Iteration 234/1000 | Loss: 0.00001648
Iteration 235/1000 | Loss: 0.00001648
Iteration 236/1000 | Loss: 0.00001647
Iteration 237/1000 | Loss: 0.00001647
Iteration 238/1000 | Loss: 0.00001647
Iteration 239/1000 | Loss: 0.00001647
Iteration 240/1000 | Loss: 0.00001647
Iteration 241/1000 | Loss: 0.00001647
Iteration 242/1000 | Loss: 0.00001647
Iteration 243/1000 | Loss: 0.00001647
Iteration 244/1000 | Loss: 0.00001647
Iteration 245/1000 | Loss: 0.00001647
Iteration 246/1000 | Loss: 0.00001647
Iteration 247/1000 | Loss: 0.00001647
Iteration 248/1000 | Loss: 0.00001647
Iteration 249/1000 | Loss: 0.00001647
Iteration 250/1000 | Loss: 0.00001647
Iteration 251/1000 | Loss: 0.00001647
Iteration 252/1000 | Loss: 0.00001647
Iteration 253/1000 | Loss: 0.00001647
Iteration 254/1000 | Loss: 0.00001647
Iteration 255/1000 | Loss: 0.00001647
Iteration 256/1000 | Loss: 0.00001647
Iteration 257/1000 | Loss: 0.00001647
Iteration 258/1000 | Loss: 0.00001647
Iteration 259/1000 | Loss: 0.00001647
Iteration 260/1000 | Loss: 0.00001647
Iteration 261/1000 | Loss: 0.00001647
Iteration 262/1000 | Loss: 0.00001647
Iteration 263/1000 | Loss: 0.00001647
Iteration 264/1000 | Loss: 0.00001647
Iteration 265/1000 | Loss: 0.00001647
Iteration 266/1000 | Loss: 0.00001647
Iteration 267/1000 | Loss: 0.00001647
Iteration 268/1000 | Loss: 0.00001647
Iteration 269/1000 | Loss: 0.00001647
Iteration 270/1000 | Loss: 0.00001647
Iteration 271/1000 | Loss: 0.00001647
Iteration 272/1000 | Loss: 0.00001647
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 272. Stopping optimization.
Last 5 losses: [1.6468084140797146e-05, 1.6468084140797146e-05, 1.6468084140797146e-05, 1.6468084140797146e-05, 1.6468084140797146e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6468084140797146e-05

Optimization complete. Final v2v error: 3.3694210052490234 mm

Highest mean error: 4.896164417266846 mm for frame 64

Lowest mean error: 2.4959616661071777 mm for frame 181

Saving results

Total time: 172.67334723472595
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_013/1004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_013/1004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_013/1004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00867059
Iteration 2/25 | Loss: 0.00113376
Iteration 3/25 | Loss: 0.00105954
Iteration 4/25 | Loss: 0.00103933
Iteration 5/25 | Loss: 0.00103173
Iteration 6/25 | Loss: 0.00102951
Iteration 7/25 | Loss: 0.00102886
Iteration 8/25 | Loss: 0.00102886
Iteration 9/25 | Loss: 0.00102886
Iteration 10/25 | Loss: 0.00102886
Iteration 11/25 | Loss: 0.00102886
Iteration 12/25 | Loss: 0.00102886
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0010288560297340155, 0.0010288560297340155, 0.0010288560297340155, 0.0010288560297340155, 0.0010288560297340155]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010288560297340155

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33985090
Iteration 2/25 | Loss: 0.00082661
Iteration 3/25 | Loss: 0.00082660
Iteration 4/25 | Loss: 0.00082660
Iteration 5/25 | Loss: 0.00082660
Iteration 6/25 | Loss: 0.00082660
Iteration 7/25 | Loss: 0.00082660
Iteration 8/25 | Loss: 0.00082660
Iteration 9/25 | Loss: 0.00082660
Iteration 10/25 | Loss: 0.00082660
Iteration 11/25 | Loss: 0.00082660
Iteration 12/25 | Loss: 0.00082660
Iteration 13/25 | Loss: 0.00082660
Iteration 14/25 | Loss: 0.00082660
Iteration 15/25 | Loss: 0.00082660
Iteration 16/25 | Loss: 0.00082660
Iteration 17/25 | Loss: 0.00082660
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0008265965152531862, 0.0008265965152531862, 0.0008265965152531862, 0.0008265965152531862, 0.0008265965152531862]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008265965152531862

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00082660
Iteration 2/1000 | Loss: 0.00004428
Iteration 3/1000 | Loss: 0.00003211
Iteration 4/1000 | Loss: 0.00002305
Iteration 5/1000 | Loss: 0.00002128
Iteration 6/1000 | Loss: 0.00001977
Iteration 7/1000 | Loss: 0.00001879
Iteration 8/1000 | Loss: 0.00001805
Iteration 9/1000 | Loss: 0.00001738
Iteration 10/1000 | Loss: 0.00001690
Iteration 11/1000 | Loss: 0.00001661
Iteration 12/1000 | Loss: 0.00001655
Iteration 13/1000 | Loss: 0.00001637
Iteration 14/1000 | Loss: 0.00001629
Iteration 15/1000 | Loss: 0.00001617
Iteration 16/1000 | Loss: 0.00001609
Iteration 17/1000 | Loss: 0.00001599
Iteration 18/1000 | Loss: 0.00001592
Iteration 19/1000 | Loss: 0.00001585
Iteration 20/1000 | Loss: 0.00001580
Iteration 21/1000 | Loss: 0.00001577
Iteration 22/1000 | Loss: 0.00001575
Iteration 23/1000 | Loss: 0.00001574
Iteration 24/1000 | Loss: 0.00001574
Iteration 25/1000 | Loss: 0.00001573
Iteration 26/1000 | Loss: 0.00001573
Iteration 27/1000 | Loss: 0.00001572
Iteration 28/1000 | Loss: 0.00001572
Iteration 29/1000 | Loss: 0.00001572
Iteration 30/1000 | Loss: 0.00001571
Iteration 31/1000 | Loss: 0.00001571
Iteration 32/1000 | Loss: 0.00001570
Iteration 33/1000 | Loss: 0.00001570
Iteration 34/1000 | Loss: 0.00001570
Iteration 35/1000 | Loss: 0.00001569
Iteration 36/1000 | Loss: 0.00001569
Iteration 37/1000 | Loss: 0.00001568
Iteration 38/1000 | Loss: 0.00001567
Iteration 39/1000 | Loss: 0.00001566
Iteration 40/1000 | Loss: 0.00001566
Iteration 41/1000 | Loss: 0.00001565
Iteration 42/1000 | Loss: 0.00001564
Iteration 43/1000 | Loss: 0.00001564
Iteration 44/1000 | Loss: 0.00001563
Iteration 45/1000 | Loss: 0.00001563
Iteration 46/1000 | Loss: 0.00001563
Iteration 47/1000 | Loss: 0.00001563
Iteration 48/1000 | Loss: 0.00001562
Iteration 49/1000 | Loss: 0.00001562
Iteration 50/1000 | Loss: 0.00001562
Iteration 51/1000 | Loss: 0.00001562
Iteration 52/1000 | Loss: 0.00001561
Iteration 53/1000 | Loss: 0.00001561
Iteration 54/1000 | Loss: 0.00001561
Iteration 55/1000 | Loss: 0.00001561
Iteration 56/1000 | Loss: 0.00001560
Iteration 57/1000 | Loss: 0.00001560
Iteration 58/1000 | Loss: 0.00001560
Iteration 59/1000 | Loss: 0.00001560
Iteration 60/1000 | Loss: 0.00001559
Iteration 61/1000 | Loss: 0.00001559
Iteration 62/1000 | Loss: 0.00001559
Iteration 63/1000 | Loss: 0.00001559
Iteration 64/1000 | Loss: 0.00001559
Iteration 65/1000 | Loss: 0.00001559
Iteration 66/1000 | Loss: 0.00001559
Iteration 67/1000 | Loss: 0.00001558
Iteration 68/1000 | Loss: 0.00001558
Iteration 69/1000 | Loss: 0.00001558
Iteration 70/1000 | Loss: 0.00001558
Iteration 71/1000 | Loss: 0.00001558
Iteration 72/1000 | Loss: 0.00001557
Iteration 73/1000 | Loss: 0.00001557
Iteration 74/1000 | Loss: 0.00001557
Iteration 75/1000 | Loss: 0.00001557
Iteration 76/1000 | Loss: 0.00001556
Iteration 77/1000 | Loss: 0.00001556
Iteration 78/1000 | Loss: 0.00001556
Iteration 79/1000 | Loss: 0.00001555
Iteration 80/1000 | Loss: 0.00001555
Iteration 81/1000 | Loss: 0.00001555
Iteration 82/1000 | Loss: 0.00001555
Iteration 83/1000 | Loss: 0.00001555
Iteration 84/1000 | Loss: 0.00001555
Iteration 85/1000 | Loss: 0.00001555
Iteration 86/1000 | Loss: 0.00001554
Iteration 87/1000 | Loss: 0.00001554
Iteration 88/1000 | Loss: 0.00001554
Iteration 89/1000 | Loss: 0.00001554
Iteration 90/1000 | Loss: 0.00001554
Iteration 91/1000 | Loss: 0.00001553
Iteration 92/1000 | Loss: 0.00001553
Iteration 93/1000 | Loss: 0.00001553
Iteration 94/1000 | Loss: 0.00001553
Iteration 95/1000 | Loss: 0.00001553
Iteration 96/1000 | Loss: 0.00001552
Iteration 97/1000 | Loss: 0.00001552
Iteration 98/1000 | Loss: 0.00001552
Iteration 99/1000 | Loss: 0.00001552
Iteration 100/1000 | Loss: 0.00001552
Iteration 101/1000 | Loss: 0.00001552
Iteration 102/1000 | Loss: 0.00001551
Iteration 103/1000 | Loss: 0.00001551
Iteration 104/1000 | Loss: 0.00001551
Iteration 105/1000 | Loss: 0.00001551
Iteration 106/1000 | Loss: 0.00001551
Iteration 107/1000 | Loss: 0.00001551
Iteration 108/1000 | Loss: 0.00001551
Iteration 109/1000 | Loss: 0.00001551
Iteration 110/1000 | Loss: 0.00001551
Iteration 111/1000 | Loss: 0.00001551
Iteration 112/1000 | Loss: 0.00001551
Iteration 113/1000 | Loss: 0.00001551
Iteration 114/1000 | Loss: 0.00001550
Iteration 115/1000 | Loss: 0.00001550
Iteration 116/1000 | Loss: 0.00001550
Iteration 117/1000 | Loss: 0.00001550
Iteration 118/1000 | Loss: 0.00001550
Iteration 119/1000 | Loss: 0.00001550
Iteration 120/1000 | Loss: 0.00001550
Iteration 121/1000 | Loss: 0.00001550
Iteration 122/1000 | Loss: 0.00001550
Iteration 123/1000 | Loss: 0.00001550
Iteration 124/1000 | Loss: 0.00001549
Iteration 125/1000 | Loss: 0.00001549
Iteration 126/1000 | Loss: 0.00001549
Iteration 127/1000 | Loss: 0.00001549
Iteration 128/1000 | Loss: 0.00001549
Iteration 129/1000 | Loss: 0.00001549
Iteration 130/1000 | Loss: 0.00001549
Iteration 131/1000 | Loss: 0.00001549
Iteration 132/1000 | Loss: 0.00001549
Iteration 133/1000 | Loss: 0.00001549
Iteration 134/1000 | Loss: 0.00001549
Iteration 135/1000 | Loss: 0.00001549
Iteration 136/1000 | Loss: 0.00001549
Iteration 137/1000 | Loss: 0.00001549
Iteration 138/1000 | Loss: 0.00001549
Iteration 139/1000 | Loss: 0.00001549
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 139. Stopping optimization.
Last 5 losses: [1.5491885278606787e-05, 1.5491885278606787e-05, 1.5491885278606787e-05, 1.5491885278606787e-05, 1.5491885278606787e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5491885278606787e-05

Optimization complete. Final v2v error: 3.289355754852295 mm

Highest mean error: 3.8369388580322266 mm for frame 120

Lowest mean error: 2.6209681034088135 mm for frame 69

Saving results

Total time: 41.362420082092285
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_013/1086/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_013/1086.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_013/1086
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00428899
Iteration 2/25 | Loss: 0.00124419
Iteration 3/25 | Loss: 0.00111087
Iteration 4/25 | Loss: 0.00109728
Iteration 5/25 | Loss: 0.00109377
Iteration 6/25 | Loss: 0.00109293
Iteration 7/25 | Loss: 0.00109293
Iteration 8/25 | Loss: 0.00109293
Iteration 9/25 | Loss: 0.00109293
Iteration 10/25 | Loss: 0.00109293
Iteration 11/25 | Loss: 0.00109293
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010929341660812497, 0.0010929341660812497, 0.0010929341660812497, 0.0010929341660812497, 0.0010929341660812497]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010929341660812497

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.62371016
Iteration 2/25 | Loss: 0.00068694
Iteration 3/25 | Loss: 0.00068693
Iteration 4/25 | Loss: 0.00068693
Iteration 5/25 | Loss: 0.00068693
Iteration 6/25 | Loss: 0.00068693
Iteration 7/25 | Loss: 0.00068693
Iteration 8/25 | Loss: 0.00068693
Iteration 9/25 | Loss: 0.00068693
Iteration 10/25 | Loss: 0.00068693
Iteration 11/25 | Loss: 0.00068693
Iteration 12/25 | Loss: 0.00068693
Iteration 13/25 | Loss: 0.00068693
Iteration 14/25 | Loss: 0.00068693
Iteration 15/25 | Loss: 0.00068693
Iteration 16/25 | Loss: 0.00068693
Iteration 17/25 | Loss: 0.00068693
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0006869292701594532, 0.0006869292701594532, 0.0006869292701594532, 0.0006869292701594532, 0.0006869292701594532]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006869292701594532

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00068693
Iteration 2/1000 | Loss: 0.00003012
Iteration 3/1000 | Loss: 0.00001783
Iteration 4/1000 | Loss: 0.00001564
Iteration 5/1000 | Loss: 0.00001466
Iteration 6/1000 | Loss: 0.00001402
Iteration 7/1000 | Loss: 0.00001352
Iteration 8/1000 | Loss: 0.00001329
Iteration 9/1000 | Loss: 0.00001322
Iteration 10/1000 | Loss: 0.00001298
Iteration 11/1000 | Loss: 0.00001284
Iteration 12/1000 | Loss: 0.00001278
Iteration 13/1000 | Loss: 0.00001269
Iteration 14/1000 | Loss: 0.00001268
Iteration 15/1000 | Loss: 0.00001262
Iteration 16/1000 | Loss: 0.00001256
Iteration 17/1000 | Loss: 0.00001252
Iteration 18/1000 | Loss: 0.00001251
Iteration 19/1000 | Loss: 0.00001251
Iteration 20/1000 | Loss: 0.00001251
Iteration 21/1000 | Loss: 0.00001249
Iteration 22/1000 | Loss: 0.00001249
Iteration 23/1000 | Loss: 0.00001249
Iteration 24/1000 | Loss: 0.00001248
Iteration 25/1000 | Loss: 0.00001248
Iteration 26/1000 | Loss: 0.00001247
Iteration 27/1000 | Loss: 0.00001246
Iteration 28/1000 | Loss: 0.00001245
Iteration 29/1000 | Loss: 0.00001245
Iteration 30/1000 | Loss: 0.00001244
Iteration 31/1000 | Loss: 0.00001243
Iteration 32/1000 | Loss: 0.00001243
Iteration 33/1000 | Loss: 0.00001242
Iteration 34/1000 | Loss: 0.00001241
Iteration 35/1000 | Loss: 0.00001241
Iteration 36/1000 | Loss: 0.00001240
Iteration 37/1000 | Loss: 0.00001239
Iteration 38/1000 | Loss: 0.00001239
Iteration 39/1000 | Loss: 0.00001239
Iteration 40/1000 | Loss: 0.00001236
Iteration 41/1000 | Loss: 0.00001236
Iteration 42/1000 | Loss: 0.00001236
Iteration 43/1000 | Loss: 0.00001236
Iteration 44/1000 | Loss: 0.00001235
Iteration 45/1000 | Loss: 0.00001235
Iteration 46/1000 | Loss: 0.00001235
Iteration 47/1000 | Loss: 0.00001234
Iteration 48/1000 | Loss: 0.00001233
Iteration 49/1000 | Loss: 0.00001233
Iteration 50/1000 | Loss: 0.00001232
Iteration 51/1000 | Loss: 0.00001232
Iteration 52/1000 | Loss: 0.00001232
Iteration 53/1000 | Loss: 0.00001231
Iteration 54/1000 | Loss: 0.00001231
Iteration 55/1000 | Loss: 0.00001231
Iteration 56/1000 | Loss: 0.00001231
Iteration 57/1000 | Loss: 0.00001230
Iteration 58/1000 | Loss: 0.00001230
Iteration 59/1000 | Loss: 0.00001230
Iteration 60/1000 | Loss: 0.00001229
Iteration 61/1000 | Loss: 0.00001229
Iteration 62/1000 | Loss: 0.00001229
Iteration 63/1000 | Loss: 0.00001229
Iteration 64/1000 | Loss: 0.00001229
Iteration 65/1000 | Loss: 0.00001229
Iteration 66/1000 | Loss: 0.00001229
Iteration 67/1000 | Loss: 0.00001228
Iteration 68/1000 | Loss: 0.00001228
Iteration 69/1000 | Loss: 0.00001228
Iteration 70/1000 | Loss: 0.00001228
Iteration 71/1000 | Loss: 0.00001227
Iteration 72/1000 | Loss: 0.00001227
Iteration 73/1000 | Loss: 0.00001227
Iteration 74/1000 | Loss: 0.00001226
Iteration 75/1000 | Loss: 0.00001226
Iteration 76/1000 | Loss: 0.00001226
Iteration 77/1000 | Loss: 0.00001226
Iteration 78/1000 | Loss: 0.00001225
Iteration 79/1000 | Loss: 0.00001225
Iteration 80/1000 | Loss: 0.00001225
Iteration 81/1000 | Loss: 0.00001224
Iteration 82/1000 | Loss: 0.00001224
Iteration 83/1000 | Loss: 0.00001224
Iteration 84/1000 | Loss: 0.00001224
Iteration 85/1000 | Loss: 0.00001224
Iteration 86/1000 | Loss: 0.00001224
Iteration 87/1000 | Loss: 0.00001223
Iteration 88/1000 | Loss: 0.00001223
Iteration 89/1000 | Loss: 0.00001223
Iteration 90/1000 | Loss: 0.00001222
Iteration 91/1000 | Loss: 0.00001222
Iteration 92/1000 | Loss: 0.00001221
Iteration 93/1000 | Loss: 0.00001221
Iteration 94/1000 | Loss: 0.00001221
Iteration 95/1000 | Loss: 0.00001221
Iteration 96/1000 | Loss: 0.00001221
Iteration 97/1000 | Loss: 0.00001221
Iteration 98/1000 | Loss: 0.00001221
Iteration 99/1000 | Loss: 0.00001220
Iteration 100/1000 | Loss: 0.00001220
Iteration 101/1000 | Loss: 0.00001220
Iteration 102/1000 | Loss: 0.00001219
Iteration 103/1000 | Loss: 0.00001219
Iteration 104/1000 | Loss: 0.00001219
Iteration 105/1000 | Loss: 0.00001218
Iteration 106/1000 | Loss: 0.00001218
Iteration 107/1000 | Loss: 0.00001218
Iteration 108/1000 | Loss: 0.00001218
Iteration 109/1000 | Loss: 0.00001217
Iteration 110/1000 | Loss: 0.00001217
Iteration 111/1000 | Loss: 0.00001217
Iteration 112/1000 | Loss: 0.00001216
Iteration 113/1000 | Loss: 0.00001216
Iteration 114/1000 | Loss: 0.00001216
Iteration 115/1000 | Loss: 0.00001215
Iteration 116/1000 | Loss: 0.00001215
Iteration 117/1000 | Loss: 0.00001214
Iteration 118/1000 | Loss: 0.00001214
Iteration 119/1000 | Loss: 0.00001214
Iteration 120/1000 | Loss: 0.00001214
Iteration 121/1000 | Loss: 0.00001214
Iteration 122/1000 | Loss: 0.00001213
Iteration 123/1000 | Loss: 0.00001213
Iteration 124/1000 | Loss: 0.00001213
Iteration 125/1000 | Loss: 0.00001213
Iteration 126/1000 | Loss: 0.00001213
Iteration 127/1000 | Loss: 0.00001213
Iteration 128/1000 | Loss: 0.00001212
Iteration 129/1000 | Loss: 0.00001212
Iteration 130/1000 | Loss: 0.00001212
Iteration 131/1000 | Loss: 0.00001212
Iteration 132/1000 | Loss: 0.00001212
Iteration 133/1000 | Loss: 0.00001211
Iteration 134/1000 | Loss: 0.00001211
Iteration 135/1000 | Loss: 0.00001211
Iteration 136/1000 | Loss: 0.00001211
Iteration 137/1000 | Loss: 0.00001210
Iteration 138/1000 | Loss: 0.00001210
Iteration 139/1000 | Loss: 0.00001210
Iteration 140/1000 | Loss: 0.00001210
Iteration 141/1000 | Loss: 0.00001210
Iteration 142/1000 | Loss: 0.00001210
Iteration 143/1000 | Loss: 0.00001210
Iteration 144/1000 | Loss: 0.00001210
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 144. Stopping optimization.
Last 5 losses: [1.2098664228687994e-05, 1.2098664228687994e-05, 1.2098664228687994e-05, 1.2098664228687994e-05, 1.2098664228687994e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2098664228687994e-05

Optimization complete. Final v2v error: 2.9338550567626953 mm

Highest mean error: 3.5480105876922607 mm for frame 116

Lowest mean error: 2.4470787048339844 mm for frame 189

Saving results

Total time: 43.12698745727539
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_013/1010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_013/1010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_013/1010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00401354
Iteration 2/25 | Loss: 0.00124777
Iteration 3/25 | Loss: 0.00109673
Iteration 4/25 | Loss: 0.00107605
Iteration 5/25 | Loss: 0.00107104
Iteration 6/25 | Loss: 0.00106936
Iteration 7/25 | Loss: 0.00106909
Iteration 8/25 | Loss: 0.00106907
Iteration 9/25 | Loss: 0.00106907
Iteration 10/25 | Loss: 0.00106907
Iteration 11/25 | Loss: 0.00106907
Iteration 12/25 | Loss: 0.00106907
Iteration 13/25 | Loss: 0.00106907
Iteration 14/25 | Loss: 0.00106907
Iteration 15/25 | Loss: 0.00106907
Iteration 16/25 | Loss: 0.00106907
Iteration 17/25 | Loss: 0.00106907
Iteration 18/25 | Loss: 0.00106907
Iteration 19/25 | Loss: 0.00106907
Iteration 20/25 | Loss: 0.00106907
Iteration 21/25 | Loss: 0.00106907
Iteration 22/25 | Loss: 0.00106907
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0010690675117075443, 0.0010690675117075443, 0.0010690675117075443, 0.0010690675117075443, 0.0010690675117075443]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010690675117075443

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42917597
Iteration 2/25 | Loss: 0.00078819
Iteration 3/25 | Loss: 0.00078819
Iteration 4/25 | Loss: 0.00078819
Iteration 5/25 | Loss: 0.00078819
Iteration 6/25 | Loss: 0.00078819
Iteration 7/25 | Loss: 0.00078819
Iteration 8/25 | Loss: 0.00078819
Iteration 9/25 | Loss: 0.00078819
Iteration 10/25 | Loss: 0.00078819
Iteration 11/25 | Loss: 0.00078819
Iteration 12/25 | Loss: 0.00078819
Iteration 13/25 | Loss: 0.00078818
Iteration 14/25 | Loss: 0.00078818
Iteration 15/25 | Loss: 0.00078818
Iteration 16/25 | Loss: 0.00078818
Iteration 17/25 | Loss: 0.00078818
Iteration 18/25 | Loss: 0.00078818
Iteration 19/25 | Loss: 0.00078818
Iteration 20/25 | Loss: 0.00078818
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0007881848723627627, 0.0007881848723627627, 0.0007881848723627627, 0.0007881848723627627, 0.0007881848723627627]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007881848723627627

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00078818
Iteration 2/1000 | Loss: 0.00003888
Iteration 3/1000 | Loss: 0.00002321
Iteration 4/1000 | Loss: 0.00001648
Iteration 5/1000 | Loss: 0.00001478
Iteration 6/1000 | Loss: 0.00001385
Iteration 7/1000 | Loss: 0.00001297
Iteration 8/1000 | Loss: 0.00001264
Iteration 9/1000 | Loss: 0.00001229
Iteration 10/1000 | Loss: 0.00001216
Iteration 11/1000 | Loss: 0.00001215
Iteration 12/1000 | Loss: 0.00001203
Iteration 13/1000 | Loss: 0.00001189
Iteration 14/1000 | Loss: 0.00001185
Iteration 15/1000 | Loss: 0.00001183
Iteration 16/1000 | Loss: 0.00001181
Iteration 17/1000 | Loss: 0.00001180
Iteration 18/1000 | Loss: 0.00001174
Iteration 19/1000 | Loss: 0.00001172
Iteration 20/1000 | Loss: 0.00001171
Iteration 21/1000 | Loss: 0.00001171
Iteration 22/1000 | Loss: 0.00001170
Iteration 23/1000 | Loss: 0.00001169
Iteration 24/1000 | Loss: 0.00001169
Iteration 25/1000 | Loss: 0.00001167
Iteration 26/1000 | Loss: 0.00001166
Iteration 27/1000 | Loss: 0.00001166
Iteration 28/1000 | Loss: 0.00001165
Iteration 29/1000 | Loss: 0.00001164
Iteration 30/1000 | Loss: 0.00001163
Iteration 31/1000 | Loss: 0.00001162
Iteration 32/1000 | Loss: 0.00001161
Iteration 33/1000 | Loss: 0.00001158
Iteration 34/1000 | Loss: 0.00001157
Iteration 35/1000 | Loss: 0.00001154
Iteration 36/1000 | Loss: 0.00001153
Iteration 37/1000 | Loss: 0.00001153
Iteration 38/1000 | Loss: 0.00001152
Iteration 39/1000 | Loss: 0.00001152
Iteration 40/1000 | Loss: 0.00001151
Iteration 41/1000 | Loss: 0.00001151
Iteration 42/1000 | Loss: 0.00001150
Iteration 43/1000 | Loss: 0.00001150
Iteration 44/1000 | Loss: 0.00001149
Iteration 45/1000 | Loss: 0.00001149
Iteration 46/1000 | Loss: 0.00001149
Iteration 47/1000 | Loss: 0.00001148
Iteration 48/1000 | Loss: 0.00001148
Iteration 49/1000 | Loss: 0.00001148
Iteration 50/1000 | Loss: 0.00001147
Iteration 51/1000 | Loss: 0.00001147
Iteration 52/1000 | Loss: 0.00001147
Iteration 53/1000 | Loss: 0.00001146
Iteration 54/1000 | Loss: 0.00001146
Iteration 55/1000 | Loss: 0.00001146
Iteration 56/1000 | Loss: 0.00001145
Iteration 57/1000 | Loss: 0.00001145
Iteration 58/1000 | Loss: 0.00001145
Iteration 59/1000 | Loss: 0.00001144
Iteration 60/1000 | Loss: 0.00001144
Iteration 61/1000 | Loss: 0.00001144
Iteration 62/1000 | Loss: 0.00001143
Iteration 63/1000 | Loss: 0.00001143
Iteration 64/1000 | Loss: 0.00001143
Iteration 65/1000 | Loss: 0.00001143
Iteration 66/1000 | Loss: 0.00001143
Iteration 67/1000 | Loss: 0.00001142
Iteration 68/1000 | Loss: 0.00001142
Iteration 69/1000 | Loss: 0.00001142
Iteration 70/1000 | Loss: 0.00001142
Iteration 71/1000 | Loss: 0.00001142
Iteration 72/1000 | Loss: 0.00001142
Iteration 73/1000 | Loss: 0.00001142
Iteration 74/1000 | Loss: 0.00001142
Iteration 75/1000 | Loss: 0.00001142
Iteration 76/1000 | Loss: 0.00001141
Iteration 77/1000 | Loss: 0.00001141
Iteration 78/1000 | Loss: 0.00001141
Iteration 79/1000 | Loss: 0.00001141
Iteration 80/1000 | Loss: 0.00001141
Iteration 81/1000 | Loss: 0.00001141
Iteration 82/1000 | Loss: 0.00001141
Iteration 83/1000 | Loss: 0.00001141
Iteration 84/1000 | Loss: 0.00001141
Iteration 85/1000 | Loss: 0.00001141
Iteration 86/1000 | Loss: 0.00001140
Iteration 87/1000 | Loss: 0.00001140
Iteration 88/1000 | Loss: 0.00001140
Iteration 89/1000 | Loss: 0.00001140
Iteration 90/1000 | Loss: 0.00001140
Iteration 91/1000 | Loss: 0.00001140
Iteration 92/1000 | Loss: 0.00001140
Iteration 93/1000 | Loss: 0.00001140
Iteration 94/1000 | Loss: 0.00001140
Iteration 95/1000 | Loss: 0.00001140
Iteration 96/1000 | Loss: 0.00001140
Iteration 97/1000 | Loss: 0.00001140
Iteration 98/1000 | Loss: 0.00001140
Iteration 99/1000 | Loss: 0.00001139
Iteration 100/1000 | Loss: 0.00001139
Iteration 101/1000 | Loss: 0.00001139
Iteration 102/1000 | Loss: 0.00001139
Iteration 103/1000 | Loss: 0.00001139
Iteration 104/1000 | Loss: 0.00001139
Iteration 105/1000 | Loss: 0.00001139
Iteration 106/1000 | Loss: 0.00001139
Iteration 107/1000 | Loss: 0.00001139
Iteration 108/1000 | Loss: 0.00001139
Iteration 109/1000 | Loss: 0.00001139
Iteration 110/1000 | Loss: 0.00001139
Iteration 111/1000 | Loss: 0.00001139
Iteration 112/1000 | Loss: 0.00001139
Iteration 113/1000 | Loss: 0.00001139
Iteration 114/1000 | Loss: 0.00001139
Iteration 115/1000 | Loss: 0.00001139
Iteration 116/1000 | Loss: 0.00001138
Iteration 117/1000 | Loss: 0.00001138
Iteration 118/1000 | Loss: 0.00001138
Iteration 119/1000 | Loss: 0.00001138
Iteration 120/1000 | Loss: 0.00001138
Iteration 121/1000 | Loss: 0.00001138
Iteration 122/1000 | Loss: 0.00001138
Iteration 123/1000 | Loss: 0.00001138
Iteration 124/1000 | Loss: 0.00001138
Iteration 125/1000 | Loss: 0.00001138
Iteration 126/1000 | Loss: 0.00001138
Iteration 127/1000 | Loss: 0.00001138
Iteration 128/1000 | Loss: 0.00001138
Iteration 129/1000 | Loss: 0.00001138
Iteration 130/1000 | Loss: 0.00001138
Iteration 131/1000 | Loss: 0.00001137
Iteration 132/1000 | Loss: 0.00001137
Iteration 133/1000 | Loss: 0.00001137
Iteration 134/1000 | Loss: 0.00001137
Iteration 135/1000 | Loss: 0.00001137
Iteration 136/1000 | Loss: 0.00001137
Iteration 137/1000 | Loss: 0.00001137
Iteration 138/1000 | Loss: 0.00001137
Iteration 139/1000 | Loss: 0.00001137
Iteration 140/1000 | Loss: 0.00001137
Iteration 141/1000 | Loss: 0.00001137
Iteration 142/1000 | Loss: 0.00001137
Iteration 143/1000 | Loss: 0.00001137
Iteration 144/1000 | Loss: 0.00001137
Iteration 145/1000 | Loss: 0.00001137
Iteration 146/1000 | Loss: 0.00001137
Iteration 147/1000 | Loss: 0.00001137
Iteration 148/1000 | Loss: 0.00001137
Iteration 149/1000 | Loss: 0.00001137
Iteration 150/1000 | Loss: 0.00001137
Iteration 151/1000 | Loss: 0.00001137
Iteration 152/1000 | Loss: 0.00001137
Iteration 153/1000 | Loss: 0.00001137
Iteration 154/1000 | Loss: 0.00001137
Iteration 155/1000 | Loss: 0.00001137
Iteration 156/1000 | Loss: 0.00001137
Iteration 157/1000 | Loss: 0.00001137
Iteration 158/1000 | Loss: 0.00001137
Iteration 159/1000 | Loss: 0.00001137
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 159. Stopping optimization.
Last 5 losses: [1.137261187977856e-05, 1.137261187977856e-05, 1.137261187977856e-05, 1.137261187977856e-05, 1.137261187977856e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.137261187977856e-05

Optimization complete. Final v2v error: 2.874901294708252 mm

Highest mean error: 3.6091244220733643 mm for frame 106

Lowest mean error: 2.377922773361206 mm for frame 160

Saving results

Total time: 39.3298556804657
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_013/1007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_013/1007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_013/1007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00837567
Iteration 2/25 | Loss: 0.00124435
Iteration 3/25 | Loss: 0.00111324
Iteration 4/25 | Loss: 0.00110401
Iteration 5/25 | Loss: 0.00110167
Iteration 6/25 | Loss: 0.00110151
Iteration 7/25 | Loss: 0.00110151
Iteration 8/25 | Loss: 0.00110151
Iteration 9/25 | Loss: 0.00110151
Iteration 10/25 | Loss: 0.00110151
Iteration 11/25 | Loss: 0.00110151
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011015137424692512, 0.0011015137424692512, 0.0011015137424692512, 0.0011015137424692512, 0.0011015137424692512]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011015137424692512

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.47439516
Iteration 2/25 | Loss: 0.00071011
Iteration 3/25 | Loss: 0.00071011
Iteration 4/25 | Loss: 0.00071011
Iteration 5/25 | Loss: 0.00071011
Iteration 6/25 | Loss: 0.00071011
Iteration 7/25 | Loss: 0.00071011
Iteration 8/25 | Loss: 0.00071011
Iteration 9/25 | Loss: 0.00071011
Iteration 10/25 | Loss: 0.00071011
Iteration 11/25 | Loss: 0.00071011
Iteration 12/25 | Loss: 0.00071011
Iteration 13/25 | Loss: 0.00071011
Iteration 14/25 | Loss: 0.00071011
Iteration 15/25 | Loss: 0.00071011
Iteration 16/25 | Loss: 0.00071011
Iteration 17/25 | Loss: 0.00071011
Iteration 18/25 | Loss: 0.00071011
Iteration 19/25 | Loss: 0.00071011
Iteration 20/25 | Loss: 0.00071011
Iteration 21/25 | Loss: 0.00071011
Iteration 22/25 | Loss: 0.00071011
Iteration 23/25 | Loss: 0.00071011
Iteration 24/25 | Loss: 0.00071011
Iteration 25/25 | Loss: 0.00071011

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00071011
Iteration 2/1000 | Loss: 0.00002908
Iteration 3/1000 | Loss: 0.00001574
Iteration 4/1000 | Loss: 0.00001372
Iteration 5/1000 | Loss: 0.00001298
Iteration 6/1000 | Loss: 0.00001243
Iteration 7/1000 | Loss: 0.00001206
Iteration 8/1000 | Loss: 0.00001193
Iteration 9/1000 | Loss: 0.00001173
Iteration 10/1000 | Loss: 0.00001170
Iteration 11/1000 | Loss: 0.00001165
Iteration 12/1000 | Loss: 0.00001164
Iteration 13/1000 | Loss: 0.00001154
Iteration 14/1000 | Loss: 0.00001150
Iteration 15/1000 | Loss: 0.00001149
Iteration 16/1000 | Loss: 0.00001148
Iteration 17/1000 | Loss: 0.00001148
Iteration 18/1000 | Loss: 0.00001145
Iteration 19/1000 | Loss: 0.00001144
Iteration 20/1000 | Loss: 0.00001142
Iteration 21/1000 | Loss: 0.00001142
Iteration 22/1000 | Loss: 0.00001141
Iteration 23/1000 | Loss: 0.00001140
Iteration 24/1000 | Loss: 0.00001140
Iteration 25/1000 | Loss: 0.00001139
Iteration 26/1000 | Loss: 0.00001139
Iteration 27/1000 | Loss: 0.00001138
Iteration 28/1000 | Loss: 0.00001137
Iteration 29/1000 | Loss: 0.00001137
Iteration 30/1000 | Loss: 0.00001137
Iteration 31/1000 | Loss: 0.00001133
Iteration 32/1000 | Loss: 0.00001132
Iteration 33/1000 | Loss: 0.00001132
Iteration 34/1000 | Loss: 0.00001129
Iteration 35/1000 | Loss: 0.00001128
Iteration 36/1000 | Loss: 0.00001128
Iteration 37/1000 | Loss: 0.00001128
Iteration 38/1000 | Loss: 0.00001127
Iteration 39/1000 | Loss: 0.00001127
Iteration 40/1000 | Loss: 0.00001124
Iteration 41/1000 | Loss: 0.00001124
Iteration 42/1000 | Loss: 0.00001123
Iteration 43/1000 | Loss: 0.00001123
Iteration 44/1000 | Loss: 0.00001123
Iteration 45/1000 | Loss: 0.00001122
Iteration 46/1000 | Loss: 0.00001122
Iteration 47/1000 | Loss: 0.00001121
Iteration 48/1000 | Loss: 0.00001121
Iteration 49/1000 | Loss: 0.00001120
Iteration 50/1000 | Loss: 0.00001120
Iteration 51/1000 | Loss: 0.00001120
Iteration 52/1000 | Loss: 0.00001119
Iteration 53/1000 | Loss: 0.00001119
Iteration 54/1000 | Loss: 0.00001119
Iteration 55/1000 | Loss: 0.00001119
Iteration 56/1000 | Loss: 0.00001118
Iteration 57/1000 | Loss: 0.00001118
Iteration 58/1000 | Loss: 0.00001117
Iteration 59/1000 | Loss: 0.00001117
Iteration 60/1000 | Loss: 0.00001117
Iteration 61/1000 | Loss: 0.00001117
Iteration 62/1000 | Loss: 0.00001117
Iteration 63/1000 | Loss: 0.00001117
Iteration 64/1000 | Loss: 0.00001117
Iteration 65/1000 | Loss: 0.00001117
Iteration 66/1000 | Loss: 0.00001117
Iteration 67/1000 | Loss: 0.00001117
Iteration 68/1000 | Loss: 0.00001117
Iteration 69/1000 | Loss: 0.00001116
Iteration 70/1000 | Loss: 0.00001116
Iteration 71/1000 | Loss: 0.00001116
Iteration 72/1000 | Loss: 0.00001116
Iteration 73/1000 | Loss: 0.00001116
Iteration 74/1000 | Loss: 0.00001116
Iteration 75/1000 | Loss: 0.00001116
Iteration 76/1000 | Loss: 0.00001116
Iteration 77/1000 | Loss: 0.00001116
Iteration 78/1000 | Loss: 0.00001115
Iteration 79/1000 | Loss: 0.00001115
Iteration 80/1000 | Loss: 0.00001115
Iteration 81/1000 | Loss: 0.00001115
Iteration 82/1000 | Loss: 0.00001115
Iteration 83/1000 | Loss: 0.00001115
Iteration 84/1000 | Loss: 0.00001115
Iteration 85/1000 | Loss: 0.00001115
Iteration 86/1000 | Loss: 0.00001115
Iteration 87/1000 | Loss: 0.00001115
Iteration 88/1000 | Loss: 0.00001114
Iteration 89/1000 | Loss: 0.00001114
Iteration 90/1000 | Loss: 0.00001114
Iteration 91/1000 | Loss: 0.00001114
Iteration 92/1000 | Loss: 0.00001114
Iteration 93/1000 | Loss: 0.00001114
Iteration 94/1000 | Loss: 0.00001113
Iteration 95/1000 | Loss: 0.00001112
Iteration 96/1000 | Loss: 0.00001112
Iteration 97/1000 | Loss: 0.00001112
Iteration 98/1000 | Loss: 0.00001112
Iteration 99/1000 | Loss: 0.00001112
Iteration 100/1000 | Loss: 0.00001112
Iteration 101/1000 | Loss: 0.00001112
Iteration 102/1000 | Loss: 0.00001112
Iteration 103/1000 | Loss: 0.00001112
Iteration 104/1000 | Loss: 0.00001112
Iteration 105/1000 | Loss: 0.00001111
Iteration 106/1000 | Loss: 0.00001111
Iteration 107/1000 | Loss: 0.00001111
Iteration 108/1000 | Loss: 0.00001111
Iteration 109/1000 | Loss: 0.00001110
Iteration 110/1000 | Loss: 0.00001110
Iteration 111/1000 | Loss: 0.00001110
Iteration 112/1000 | Loss: 0.00001109
Iteration 113/1000 | Loss: 0.00001109
Iteration 114/1000 | Loss: 0.00001109
Iteration 115/1000 | Loss: 0.00001109
Iteration 116/1000 | Loss: 0.00001109
Iteration 117/1000 | Loss: 0.00001109
Iteration 118/1000 | Loss: 0.00001109
Iteration 119/1000 | Loss: 0.00001108
Iteration 120/1000 | Loss: 0.00001108
Iteration 121/1000 | Loss: 0.00001108
Iteration 122/1000 | Loss: 0.00001108
Iteration 123/1000 | Loss: 0.00001108
Iteration 124/1000 | Loss: 0.00001108
Iteration 125/1000 | Loss: 0.00001107
Iteration 126/1000 | Loss: 0.00001107
Iteration 127/1000 | Loss: 0.00001107
Iteration 128/1000 | Loss: 0.00001107
Iteration 129/1000 | Loss: 0.00001107
Iteration 130/1000 | Loss: 0.00001107
Iteration 131/1000 | Loss: 0.00001107
Iteration 132/1000 | Loss: 0.00001106
Iteration 133/1000 | Loss: 0.00001106
Iteration 134/1000 | Loss: 0.00001106
Iteration 135/1000 | Loss: 0.00001106
Iteration 136/1000 | Loss: 0.00001106
Iteration 137/1000 | Loss: 0.00001106
Iteration 138/1000 | Loss: 0.00001106
Iteration 139/1000 | Loss: 0.00001106
Iteration 140/1000 | Loss: 0.00001106
Iteration 141/1000 | Loss: 0.00001106
Iteration 142/1000 | Loss: 0.00001105
Iteration 143/1000 | Loss: 0.00001105
Iteration 144/1000 | Loss: 0.00001105
Iteration 145/1000 | Loss: 0.00001105
Iteration 146/1000 | Loss: 0.00001105
Iteration 147/1000 | Loss: 0.00001105
Iteration 148/1000 | Loss: 0.00001105
Iteration 149/1000 | Loss: 0.00001105
Iteration 150/1000 | Loss: 0.00001104
Iteration 151/1000 | Loss: 0.00001104
Iteration 152/1000 | Loss: 0.00001104
Iteration 153/1000 | Loss: 0.00001104
Iteration 154/1000 | Loss: 0.00001103
Iteration 155/1000 | Loss: 0.00001103
Iteration 156/1000 | Loss: 0.00001103
Iteration 157/1000 | Loss: 0.00001103
Iteration 158/1000 | Loss: 0.00001103
Iteration 159/1000 | Loss: 0.00001103
Iteration 160/1000 | Loss: 0.00001103
Iteration 161/1000 | Loss: 0.00001103
Iteration 162/1000 | Loss: 0.00001103
Iteration 163/1000 | Loss: 0.00001102
Iteration 164/1000 | Loss: 0.00001102
Iteration 165/1000 | Loss: 0.00001102
Iteration 166/1000 | Loss: 0.00001102
Iteration 167/1000 | Loss: 0.00001102
Iteration 168/1000 | Loss: 0.00001102
Iteration 169/1000 | Loss: 0.00001102
Iteration 170/1000 | Loss: 0.00001102
Iteration 171/1000 | Loss: 0.00001102
Iteration 172/1000 | Loss: 0.00001102
Iteration 173/1000 | Loss: 0.00001102
Iteration 174/1000 | Loss: 0.00001102
Iteration 175/1000 | Loss: 0.00001101
Iteration 176/1000 | Loss: 0.00001101
Iteration 177/1000 | Loss: 0.00001101
Iteration 178/1000 | Loss: 0.00001101
Iteration 179/1000 | Loss: 0.00001101
Iteration 180/1000 | Loss: 0.00001101
Iteration 181/1000 | Loss: 0.00001101
Iteration 182/1000 | Loss: 0.00001101
Iteration 183/1000 | Loss: 0.00001101
Iteration 184/1000 | Loss: 0.00001101
Iteration 185/1000 | Loss: 0.00001101
Iteration 186/1000 | Loss: 0.00001101
Iteration 187/1000 | Loss: 0.00001100
Iteration 188/1000 | Loss: 0.00001100
Iteration 189/1000 | Loss: 0.00001100
Iteration 190/1000 | Loss: 0.00001100
Iteration 191/1000 | Loss: 0.00001100
Iteration 192/1000 | Loss: 0.00001100
Iteration 193/1000 | Loss: 0.00001100
Iteration 194/1000 | Loss: 0.00001100
Iteration 195/1000 | Loss: 0.00001100
Iteration 196/1000 | Loss: 0.00001100
Iteration 197/1000 | Loss: 0.00001100
Iteration 198/1000 | Loss: 0.00001100
Iteration 199/1000 | Loss: 0.00001100
Iteration 200/1000 | Loss: 0.00001100
Iteration 201/1000 | Loss: 0.00001100
Iteration 202/1000 | Loss: 0.00001099
Iteration 203/1000 | Loss: 0.00001099
Iteration 204/1000 | Loss: 0.00001099
Iteration 205/1000 | Loss: 0.00001099
Iteration 206/1000 | Loss: 0.00001099
Iteration 207/1000 | Loss: 0.00001099
Iteration 208/1000 | Loss: 0.00001099
Iteration 209/1000 | Loss: 0.00001099
Iteration 210/1000 | Loss: 0.00001099
Iteration 211/1000 | Loss: 0.00001099
Iteration 212/1000 | Loss: 0.00001099
Iteration 213/1000 | Loss: 0.00001099
Iteration 214/1000 | Loss: 0.00001099
Iteration 215/1000 | Loss: 0.00001099
Iteration 216/1000 | Loss: 0.00001099
Iteration 217/1000 | Loss: 0.00001099
Iteration 218/1000 | Loss: 0.00001099
Iteration 219/1000 | Loss: 0.00001099
Iteration 220/1000 | Loss: 0.00001099
Iteration 221/1000 | Loss: 0.00001099
Iteration 222/1000 | Loss: 0.00001099
Iteration 223/1000 | Loss: 0.00001099
Iteration 224/1000 | Loss: 0.00001099
Iteration 225/1000 | Loss: 0.00001099
Iteration 226/1000 | Loss: 0.00001099
Iteration 227/1000 | Loss: 0.00001099
Iteration 228/1000 | Loss: 0.00001099
Iteration 229/1000 | Loss: 0.00001099
Iteration 230/1000 | Loss: 0.00001099
Iteration 231/1000 | Loss: 0.00001099
Iteration 232/1000 | Loss: 0.00001099
Iteration 233/1000 | Loss: 0.00001099
Iteration 234/1000 | Loss: 0.00001099
Iteration 235/1000 | Loss: 0.00001099
Iteration 236/1000 | Loss: 0.00001099
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 236. Stopping optimization.
Last 5 losses: [1.0988167559844442e-05, 1.0988167559844442e-05, 1.0988167559844442e-05, 1.0988167559844442e-05, 1.0988167559844442e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0988167559844442e-05

Optimization complete. Final v2v error: 2.7842445373535156 mm

Highest mean error: 3.6540608406066895 mm for frame 119

Lowest mean error: 2.4873719215393066 mm for frame 34

Saving results

Total time: 41.76162815093994
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_013/1089/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_013/1089.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_013/1089
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00405922
Iteration 2/25 | Loss: 0.00119850
Iteration 3/25 | Loss: 0.00108122
Iteration 4/25 | Loss: 0.00107297
Iteration 5/25 | Loss: 0.00106978
Iteration 6/25 | Loss: 0.00106978
Iteration 7/25 | Loss: 0.00106978
Iteration 8/25 | Loss: 0.00106978
Iteration 9/25 | Loss: 0.00106978
Iteration 10/25 | Loss: 0.00106978
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0010697789257392287, 0.0010697789257392287, 0.0010697789257392287, 0.0010697789257392287, 0.0010697789257392287]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010697789257392287

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.65733588
Iteration 2/25 | Loss: 0.00058476
Iteration 3/25 | Loss: 0.00058476
Iteration 4/25 | Loss: 0.00058476
Iteration 5/25 | Loss: 0.00058476
Iteration 6/25 | Loss: 0.00058475
Iteration 7/25 | Loss: 0.00058475
Iteration 8/25 | Loss: 0.00058475
Iteration 9/25 | Loss: 0.00058475
Iteration 10/25 | Loss: 0.00058475
Iteration 11/25 | Loss: 0.00058475
Iteration 12/25 | Loss: 0.00058475
Iteration 13/25 | Loss: 0.00058475
Iteration 14/25 | Loss: 0.00058475
Iteration 15/25 | Loss: 0.00058475
Iteration 16/25 | Loss: 0.00058475
Iteration 17/25 | Loss: 0.00058475
Iteration 18/25 | Loss: 0.00058475
Iteration 19/25 | Loss: 0.00058475
Iteration 20/25 | Loss: 0.00058475
Iteration 21/25 | Loss: 0.00058475
Iteration 22/25 | Loss: 0.00058475
Iteration 23/25 | Loss: 0.00058475
Iteration 24/25 | Loss: 0.00058475
Iteration 25/25 | Loss: 0.00058475

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00058475
Iteration 2/1000 | Loss: 0.00002450
Iteration 3/1000 | Loss: 0.00001529
Iteration 4/1000 | Loss: 0.00001211
Iteration 5/1000 | Loss: 0.00001125
Iteration 6/1000 | Loss: 0.00001056
Iteration 7/1000 | Loss: 0.00001026
Iteration 8/1000 | Loss: 0.00001021
Iteration 9/1000 | Loss: 0.00001020
Iteration 10/1000 | Loss: 0.00000992
Iteration 11/1000 | Loss: 0.00000965
Iteration 12/1000 | Loss: 0.00000948
Iteration 13/1000 | Loss: 0.00000935
Iteration 14/1000 | Loss: 0.00000934
Iteration 15/1000 | Loss: 0.00000929
Iteration 16/1000 | Loss: 0.00000921
Iteration 17/1000 | Loss: 0.00000918
Iteration 18/1000 | Loss: 0.00000918
Iteration 19/1000 | Loss: 0.00000918
Iteration 20/1000 | Loss: 0.00000918
Iteration 21/1000 | Loss: 0.00000917
Iteration 22/1000 | Loss: 0.00000917
Iteration 23/1000 | Loss: 0.00000917
Iteration 24/1000 | Loss: 0.00000916
Iteration 25/1000 | Loss: 0.00000916
Iteration 26/1000 | Loss: 0.00000916
Iteration 27/1000 | Loss: 0.00000915
Iteration 28/1000 | Loss: 0.00000915
Iteration 29/1000 | Loss: 0.00000915
Iteration 30/1000 | Loss: 0.00000915
Iteration 31/1000 | Loss: 0.00000914
Iteration 32/1000 | Loss: 0.00000914
Iteration 33/1000 | Loss: 0.00000913
Iteration 34/1000 | Loss: 0.00000913
Iteration 35/1000 | Loss: 0.00000913
Iteration 36/1000 | Loss: 0.00000913
Iteration 37/1000 | Loss: 0.00000913
Iteration 38/1000 | Loss: 0.00000913
Iteration 39/1000 | Loss: 0.00000913
Iteration 40/1000 | Loss: 0.00000912
Iteration 41/1000 | Loss: 0.00000912
Iteration 42/1000 | Loss: 0.00000912
Iteration 43/1000 | Loss: 0.00000912
Iteration 44/1000 | Loss: 0.00000912
Iteration 45/1000 | Loss: 0.00000912
Iteration 46/1000 | Loss: 0.00000911
Iteration 47/1000 | Loss: 0.00000911
Iteration 48/1000 | Loss: 0.00000910
Iteration 49/1000 | Loss: 0.00000910
Iteration 50/1000 | Loss: 0.00000910
Iteration 51/1000 | Loss: 0.00000910
Iteration 52/1000 | Loss: 0.00000910
Iteration 53/1000 | Loss: 0.00000909
Iteration 54/1000 | Loss: 0.00000909
Iteration 55/1000 | Loss: 0.00000909
Iteration 56/1000 | Loss: 0.00000909
Iteration 57/1000 | Loss: 0.00000909
Iteration 58/1000 | Loss: 0.00000909
Iteration 59/1000 | Loss: 0.00000909
Iteration 60/1000 | Loss: 0.00000909
Iteration 61/1000 | Loss: 0.00000908
Iteration 62/1000 | Loss: 0.00000908
Iteration 63/1000 | Loss: 0.00000907
Iteration 64/1000 | Loss: 0.00000906
Iteration 65/1000 | Loss: 0.00000906
Iteration 66/1000 | Loss: 0.00000906
Iteration 67/1000 | Loss: 0.00000905
Iteration 68/1000 | Loss: 0.00000904
Iteration 69/1000 | Loss: 0.00000904
Iteration 70/1000 | Loss: 0.00000904
Iteration 71/1000 | Loss: 0.00000904
Iteration 72/1000 | Loss: 0.00000904
Iteration 73/1000 | Loss: 0.00000903
Iteration 74/1000 | Loss: 0.00000903
Iteration 75/1000 | Loss: 0.00000903
Iteration 76/1000 | Loss: 0.00000902
Iteration 77/1000 | Loss: 0.00000902
Iteration 78/1000 | Loss: 0.00000902
Iteration 79/1000 | Loss: 0.00000902
Iteration 80/1000 | Loss: 0.00000902
Iteration 81/1000 | Loss: 0.00000902
Iteration 82/1000 | Loss: 0.00000902
Iteration 83/1000 | Loss: 0.00000902
Iteration 84/1000 | Loss: 0.00000902
Iteration 85/1000 | Loss: 0.00000902
Iteration 86/1000 | Loss: 0.00000902
Iteration 87/1000 | Loss: 0.00000902
Iteration 88/1000 | Loss: 0.00000902
Iteration 89/1000 | Loss: 0.00000902
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 89. Stopping optimization.
Last 5 losses: [9.020984180097003e-06, 9.020984180097003e-06, 9.020984180097003e-06, 9.020984180097003e-06, 9.020984180097003e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.020984180097003e-06

Optimization complete. Final v2v error: 2.602076292037964 mm

Highest mean error: 2.7944021224975586 mm for frame 110

Lowest mean error: 2.4514505863189697 mm for frame 76

Saving results

Total time: 35.74698066711426
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_013/1036/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_013/1036.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_013/1036
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01035219
Iteration 2/25 | Loss: 0.00223135
Iteration 3/25 | Loss: 0.00193281
Iteration 4/25 | Loss: 0.00148738
Iteration 5/25 | Loss: 0.00147833
Iteration 6/25 | Loss: 0.00141245
Iteration 7/25 | Loss: 0.00137420
Iteration 8/25 | Loss: 0.00135667
Iteration 9/25 | Loss: 0.00125025
Iteration 10/25 | Loss: 0.00121126
Iteration 11/25 | Loss: 0.00117108
Iteration 12/25 | Loss: 0.00116384
Iteration 13/25 | Loss: 0.00115099
Iteration 14/25 | Loss: 0.00114107
Iteration 15/25 | Loss: 0.00113393
Iteration 16/25 | Loss: 0.00112612
Iteration 17/25 | Loss: 0.00111519
Iteration 18/25 | Loss: 0.00111063
Iteration 19/25 | Loss: 0.00111237
Iteration 20/25 | Loss: 0.00110594
Iteration 21/25 | Loss: 0.00110004
Iteration 22/25 | Loss: 0.00109836
Iteration 23/25 | Loss: 0.00109781
Iteration 24/25 | Loss: 0.00109760
Iteration 25/25 | Loss: 0.00109751

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.47079062
Iteration 2/25 | Loss: 0.00098416
Iteration 3/25 | Loss: 0.00098416
Iteration 4/25 | Loss: 0.00098416
Iteration 5/25 | Loss: 0.00098415
Iteration 6/25 | Loss: 0.00098415
Iteration 7/25 | Loss: 0.00098415
Iteration 8/25 | Loss: 0.00098415
Iteration 9/25 | Loss: 0.00098415
Iteration 10/25 | Loss: 0.00098415
Iteration 11/25 | Loss: 0.00098415
Iteration 12/25 | Loss: 0.00098415
Iteration 13/25 | Loss: 0.00098415
Iteration 14/25 | Loss: 0.00098415
Iteration 15/25 | Loss: 0.00098415
Iteration 16/25 | Loss: 0.00098415
Iteration 17/25 | Loss: 0.00098415
Iteration 18/25 | Loss: 0.00098415
Iteration 19/25 | Loss: 0.00098415
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0009841531282290816, 0.0009841531282290816, 0.0009841531282290816, 0.0009841531282290816, 0.0009841531282290816]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009841531282290816

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00098415
Iteration 2/1000 | Loss: 0.00036648
Iteration 3/1000 | Loss: 0.00038289
Iteration 4/1000 | Loss: 0.00031593
Iteration 5/1000 | Loss: 0.00023073
Iteration 6/1000 | Loss: 0.00012964
Iteration 7/1000 | Loss: 0.00004777
Iteration 8/1000 | Loss: 0.00024110
Iteration 9/1000 | Loss: 0.00006006
Iteration 10/1000 | Loss: 0.00009096
Iteration 11/1000 | Loss: 0.00006331
Iteration 12/1000 | Loss: 0.00004737
Iteration 13/1000 | Loss: 0.00004138
Iteration 14/1000 | Loss: 0.00013182
Iteration 15/1000 | Loss: 0.00006645
Iteration 16/1000 | Loss: 0.00021496
Iteration 17/1000 | Loss: 0.00013215
Iteration 18/1000 | Loss: 0.00031192
Iteration 19/1000 | Loss: 0.00012825
Iteration 20/1000 | Loss: 0.00026745
Iteration 21/1000 | Loss: 0.00114596
Iteration 22/1000 | Loss: 0.00029352
Iteration 23/1000 | Loss: 0.00031321
Iteration 24/1000 | Loss: 0.00020513
Iteration 25/1000 | Loss: 0.00014056
Iteration 26/1000 | Loss: 0.00004444
Iteration 27/1000 | Loss: 0.00019909
Iteration 28/1000 | Loss: 0.00017613
Iteration 29/1000 | Loss: 0.00006062
Iteration 30/1000 | Loss: 0.00024196
Iteration 31/1000 | Loss: 0.00015992
Iteration 32/1000 | Loss: 0.00014762
Iteration 33/1000 | Loss: 0.00016755
Iteration 34/1000 | Loss: 0.00014721
Iteration 35/1000 | Loss: 0.00025624
Iteration 36/1000 | Loss: 0.00025120
Iteration 37/1000 | Loss: 0.00023974
Iteration 38/1000 | Loss: 0.00005396
Iteration 39/1000 | Loss: 0.00012617
Iteration 40/1000 | Loss: 0.00029223
Iteration 41/1000 | Loss: 0.00094800
Iteration 42/1000 | Loss: 0.00216932
Iteration 43/1000 | Loss: 0.00131133
Iteration 44/1000 | Loss: 0.00041237
Iteration 45/1000 | Loss: 0.00181477
Iteration 46/1000 | Loss: 0.00045404
Iteration 47/1000 | Loss: 0.00056128
Iteration 48/1000 | Loss: 0.00013791
Iteration 49/1000 | Loss: 0.00006951
Iteration 50/1000 | Loss: 0.00003860
Iteration 51/1000 | Loss: 0.00020547
Iteration 52/1000 | Loss: 0.00003403
Iteration 53/1000 | Loss: 0.00021302
Iteration 54/1000 | Loss: 0.00022715
Iteration 55/1000 | Loss: 0.00007807
Iteration 56/1000 | Loss: 0.00004927
Iteration 57/1000 | Loss: 0.00003297
Iteration 58/1000 | Loss: 0.00002261
Iteration 59/1000 | Loss: 0.00005290
Iteration 60/1000 | Loss: 0.00002186
Iteration 61/1000 | Loss: 0.00001731
Iteration 62/1000 | Loss: 0.00001610
Iteration 63/1000 | Loss: 0.00015596
Iteration 64/1000 | Loss: 0.00002073
Iteration 65/1000 | Loss: 0.00001549
Iteration 66/1000 | Loss: 0.00001444
Iteration 67/1000 | Loss: 0.00001413
Iteration 68/1000 | Loss: 0.00012968
Iteration 69/1000 | Loss: 0.00001370
Iteration 70/1000 | Loss: 0.00001344
Iteration 71/1000 | Loss: 0.00001342
Iteration 72/1000 | Loss: 0.00001325
Iteration 73/1000 | Loss: 0.00001318
Iteration 74/1000 | Loss: 0.00068078
Iteration 75/1000 | Loss: 0.00013392
Iteration 76/1000 | Loss: 0.00004294
Iteration 77/1000 | Loss: 0.00001537
Iteration 78/1000 | Loss: 0.00001387
Iteration 79/1000 | Loss: 0.00012746
Iteration 80/1000 | Loss: 0.00001276
Iteration 81/1000 | Loss: 0.00001155
Iteration 82/1000 | Loss: 0.00001134
Iteration 83/1000 | Loss: 0.00001129
Iteration 84/1000 | Loss: 0.00012271
Iteration 85/1000 | Loss: 0.00001132
Iteration 86/1000 | Loss: 0.00001111
Iteration 87/1000 | Loss: 0.00001107
Iteration 88/1000 | Loss: 0.00001106
Iteration 89/1000 | Loss: 0.00001103
Iteration 90/1000 | Loss: 0.00001103
Iteration 91/1000 | Loss: 0.00001103
Iteration 92/1000 | Loss: 0.00001103
Iteration 93/1000 | Loss: 0.00001103
Iteration 94/1000 | Loss: 0.00001102
Iteration 95/1000 | Loss: 0.00001102
Iteration 96/1000 | Loss: 0.00001102
Iteration 97/1000 | Loss: 0.00001102
Iteration 98/1000 | Loss: 0.00001102
Iteration 99/1000 | Loss: 0.00001102
Iteration 100/1000 | Loss: 0.00001102
Iteration 101/1000 | Loss: 0.00001102
Iteration 102/1000 | Loss: 0.00001102
Iteration 103/1000 | Loss: 0.00001102
Iteration 104/1000 | Loss: 0.00001102
Iteration 105/1000 | Loss: 0.00009894
Iteration 106/1000 | Loss: 0.00003641
Iteration 107/1000 | Loss: 0.00008787
Iteration 108/1000 | Loss: 0.00001247
Iteration 109/1000 | Loss: 0.00001118
Iteration 110/1000 | Loss: 0.00001099
Iteration 111/1000 | Loss: 0.00001099
Iteration 112/1000 | Loss: 0.00001099
Iteration 113/1000 | Loss: 0.00001098
Iteration 114/1000 | Loss: 0.00001098
Iteration 115/1000 | Loss: 0.00001098
Iteration 116/1000 | Loss: 0.00001098
Iteration 117/1000 | Loss: 0.00001098
Iteration 118/1000 | Loss: 0.00001098
Iteration 119/1000 | Loss: 0.00001098
Iteration 120/1000 | Loss: 0.00001098
Iteration 121/1000 | Loss: 0.00001098
Iteration 122/1000 | Loss: 0.00001098
Iteration 123/1000 | Loss: 0.00001097
Iteration 124/1000 | Loss: 0.00001097
Iteration 125/1000 | Loss: 0.00001097
Iteration 126/1000 | Loss: 0.00001096
Iteration 127/1000 | Loss: 0.00001096
Iteration 128/1000 | Loss: 0.00001096
Iteration 129/1000 | Loss: 0.00001096
Iteration 130/1000 | Loss: 0.00001096
Iteration 131/1000 | Loss: 0.00001096
Iteration 132/1000 | Loss: 0.00001096
Iteration 133/1000 | Loss: 0.00001096
Iteration 134/1000 | Loss: 0.00001096
Iteration 135/1000 | Loss: 0.00001096
Iteration 136/1000 | Loss: 0.00001096
Iteration 137/1000 | Loss: 0.00001096
Iteration 138/1000 | Loss: 0.00001096
Iteration 139/1000 | Loss: 0.00001096
Iteration 140/1000 | Loss: 0.00001096
Iteration 141/1000 | Loss: 0.00001096
Iteration 142/1000 | Loss: 0.00001096
Iteration 143/1000 | Loss: 0.00001096
Iteration 144/1000 | Loss: 0.00001096
Iteration 145/1000 | Loss: 0.00001096
Iteration 146/1000 | Loss: 0.00001096
Iteration 147/1000 | Loss: 0.00001096
Iteration 148/1000 | Loss: 0.00001096
Iteration 149/1000 | Loss: 0.00001096
Iteration 150/1000 | Loss: 0.00001096
Iteration 151/1000 | Loss: 0.00001096
Iteration 152/1000 | Loss: 0.00001096
Iteration 153/1000 | Loss: 0.00001096
Iteration 154/1000 | Loss: 0.00001096
Iteration 155/1000 | Loss: 0.00001096
Iteration 156/1000 | Loss: 0.00001096
Iteration 157/1000 | Loss: 0.00001096
Iteration 158/1000 | Loss: 0.00001096
Iteration 159/1000 | Loss: 0.00001096
Iteration 160/1000 | Loss: 0.00001096
Iteration 161/1000 | Loss: 0.00001096
Iteration 162/1000 | Loss: 0.00001096
Iteration 163/1000 | Loss: 0.00001096
Iteration 164/1000 | Loss: 0.00001096
Iteration 165/1000 | Loss: 0.00001096
Iteration 166/1000 | Loss: 0.00001096
Iteration 167/1000 | Loss: 0.00001096
Iteration 168/1000 | Loss: 0.00001096
Iteration 169/1000 | Loss: 0.00001096
Iteration 170/1000 | Loss: 0.00001096
Iteration 171/1000 | Loss: 0.00001096
Iteration 172/1000 | Loss: 0.00001096
Iteration 173/1000 | Loss: 0.00001096
Iteration 174/1000 | Loss: 0.00001096
Iteration 175/1000 | Loss: 0.00001096
Iteration 176/1000 | Loss: 0.00001096
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 176. Stopping optimization.
Last 5 losses: [1.095791049010586e-05, 1.095791049010586e-05, 1.095791049010586e-05, 1.095791049010586e-05, 1.095791049010586e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.095791049010586e-05

Optimization complete. Final v2v error: 2.792686700820923 mm

Highest mean error: 3.908193588256836 mm for frame 76

Lowest mean error: 2.530857801437378 mm for frame 28

Saving results

Total time: 176.9483015537262
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_013/1054/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_013/1054.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_013/1054
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01001348
Iteration 2/25 | Loss: 0.01001348
Iteration 3/25 | Loss: 0.01001348
Iteration 4/25 | Loss: 0.01001348
Iteration 5/25 | Loss: 0.01001347
Iteration 6/25 | Loss: 0.01001347
Iteration 7/25 | Loss: 0.01001347
Iteration 8/25 | Loss: 0.01001347
Iteration 9/25 | Loss: 0.01001347
Iteration 10/25 | Loss: 0.01001346
Iteration 11/25 | Loss: 0.01001346
Iteration 12/25 | Loss: 0.01001346
Iteration 13/25 | Loss: 0.01001346
Iteration 14/25 | Loss: 0.01001345
Iteration 15/25 | Loss: 0.01001345
Iteration 16/25 | Loss: 0.01001345
Iteration 17/25 | Loss: 0.01001345
Iteration 18/25 | Loss: 0.01001345
Iteration 19/25 | Loss: 0.01001345
Iteration 20/25 | Loss: 0.01001345
Iteration 21/25 | Loss: 0.01001345
Iteration 22/25 | Loss: 0.01001344
Iteration 23/25 | Loss: 0.01001344
Iteration 24/25 | Loss: 0.01001344
Iteration 25/25 | Loss: 0.01001344

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.67835844
Iteration 2/25 | Loss: 0.13434222
Iteration 3/25 | Loss: 0.13135761
Iteration 4/25 | Loss: 0.13012463
Iteration 5/25 | Loss: 0.13012461
Iteration 6/25 | Loss: 0.13021801
Iteration 7/25 | Loss: 0.13021798
Iteration 8/25 | Loss: 0.13012478
Iteration 9/25 | Loss: 0.13012464
Iteration 10/25 | Loss: 0.13012461
Iteration 11/25 | Loss: 0.13012460
Iteration 12/25 | Loss: 0.13012460
Iteration 13/25 | Loss: 0.13012460
Iteration 14/25 | Loss: 0.13012460
Iteration 15/25 | Loss: 0.13012460
Iteration 16/25 | Loss: 0.13012457
Iteration 17/25 | Loss: 0.13012457
Iteration 18/25 | Loss: 0.13012457
Iteration 19/25 | Loss: 0.13012457
Iteration 20/25 | Loss: 0.13012457
Iteration 21/25 | Loss: 0.13012457
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.13012456893920898, 0.13012456893920898, 0.13012456893920898, 0.13012456893920898, 0.13012456893920898]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.13012456893920898

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.13012457
Iteration 2/1000 | Loss: 0.00313897
Iteration 3/1000 | Loss: 0.00073950
Iteration 4/1000 | Loss: 0.00077180
Iteration 5/1000 | Loss: 0.00035706
Iteration 6/1000 | Loss: 0.00070632
Iteration 7/1000 | Loss: 0.00385713
Iteration 8/1000 | Loss: 0.00054105
Iteration 9/1000 | Loss: 0.00042304
Iteration 10/1000 | Loss: 0.00008084
Iteration 11/1000 | Loss: 0.00053409
Iteration 12/1000 | Loss: 0.00070761
Iteration 13/1000 | Loss: 0.00013624
Iteration 14/1000 | Loss: 0.00025399
Iteration 15/1000 | Loss: 0.00037821
Iteration 16/1000 | Loss: 0.00073181
Iteration 17/1000 | Loss: 0.00017204
Iteration 18/1000 | Loss: 0.00037693
Iteration 19/1000 | Loss: 0.00059755
Iteration 20/1000 | Loss: 0.00010209
Iteration 21/1000 | Loss: 0.00027684
Iteration 22/1000 | Loss: 0.00105512
Iteration 23/1000 | Loss: 0.00540725
Iteration 24/1000 | Loss: 0.00237356
Iteration 25/1000 | Loss: 0.00008765
Iteration 26/1000 | Loss: 0.00022926
Iteration 27/1000 | Loss: 0.00008939
Iteration 28/1000 | Loss: 0.00006996
Iteration 29/1000 | Loss: 0.00018904
Iteration 30/1000 | Loss: 0.00003053
Iteration 31/1000 | Loss: 0.00003797
Iteration 32/1000 | Loss: 0.00004880
Iteration 33/1000 | Loss: 0.00002059
Iteration 34/1000 | Loss: 0.00022131
Iteration 35/1000 | Loss: 0.00002098
Iteration 36/1000 | Loss: 0.00001982
Iteration 37/1000 | Loss: 0.00011181
Iteration 38/1000 | Loss: 0.00054475
Iteration 39/1000 | Loss: 0.00014547
Iteration 40/1000 | Loss: 0.00001921
Iteration 41/1000 | Loss: 0.00011485
Iteration 42/1000 | Loss: 0.00013809
Iteration 43/1000 | Loss: 0.00010954
Iteration 44/1000 | Loss: 0.00005075
Iteration 45/1000 | Loss: 0.00009354
Iteration 46/1000 | Loss: 0.00005874
Iteration 47/1000 | Loss: 0.00021837
Iteration 48/1000 | Loss: 0.00013631
Iteration 49/1000 | Loss: 0.00001710
Iteration 50/1000 | Loss: 0.00001675
Iteration 51/1000 | Loss: 0.00001670
Iteration 52/1000 | Loss: 0.00011244
Iteration 53/1000 | Loss: 0.00001609
Iteration 54/1000 | Loss: 0.00001585
Iteration 55/1000 | Loss: 0.00008718
Iteration 56/1000 | Loss: 0.00070207
Iteration 57/1000 | Loss: 0.00489553
Iteration 58/1000 | Loss: 0.00416210
Iteration 59/1000 | Loss: 0.00117479
Iteration 60/1000 | Loss: 0.00020721
Iteration 61/1000 | Loss: 0.00017688
Iteration 62/1000 | Loss: 0.00086625
Iteration 63/1000 | Loss: 0.00157980
Iteration 64/1000 | Loss: 0.00014504
Iteration 65/1000 | Loss: 0.00017612
Iteration 66/1000 | Loss: 0.00040360
Iteration 67/1000 | Loss: 0.00008630
Iteration 68/1000 | Loss: 0.00002121
Iteration 69/1000 | Loss: 0.00002700
Iteration 70/1000 | Loss: 0.00002826
Iteration 71/1000 | Loss: 0.00006276
Iteration 72/1000 | Loss: 0.00009354
Iteration 73/1000 | Loss: 0.00008848
Iteration 74/1000 | Loss: 0.00001807
Iteration 75/1000 | Loss: 0.00006718
Iteration 76/1000 | Loss: 0.00062025
Iteration 77/1000 | Loss: 0.00276564
Iteration 78/1000 | Loss: 0.00059286
Iteration 79/1000 | Loss: 0.00010597
Iteration 80/1000 | Loss: 0.00002586
Iteration 81/1000 | Loss: 0.00007401
Iteration 82/1000 | Loss: 0.00004944
Iteration 83/1000 | Loss: 0.00001801
Iteration 84/1000 | Loss: 0.00009988
Iteration 85/1000 | Loss: 0.00005079
Iteration 86/1000 | Loss: 0.00003889
Iteration 87/1000 | Loss: 0.00002231
Iteration 88/1000 | Loss: 0.00007604
Iteration 89/1000 | Loss: 0.00025841
Iteration 90/1000 | Loss: 0.00001706
Iteration 91/1000 | Loss: 0.00001643
Iteration 92/1000 | Loss: 0.00001600
Iteration 93/1000 | Loss: 0.00011642
Iteration 94/1000 | Loss: 0.00012702
Iteration 95/1000 | Loss: 0.00002693
Iteration 96/1000 | Loss: 0.00014139
Iteration 97/1000 | Loss: 0.00004194
Iteration 98/1000 | Loss: 0.00001588
Iteration 99/1000 | Loss: 0.00012193
Iteration 100/1000 | Loss: 0.00001602
Iteration 101/1000 | Loss: 0.00001557
Iteration 102/1000 | Loss: 0.00001551
Iteration 103/1000 | Loss: 0.00001550
Iteration 104/1000 | Loss: 0.00001550
Iteration 105/1000 | Loss: 0.00001550
Iteration 106/1000 | Loss: 0.00001549
Iteration 107/1000 | Loss: 0.00001549
Iteration 108/1000 | Loss: 0.00001549
Iteration 109/1000 | Loss: 0.00001548
Iteration 110/1000 | Loss: 0.00001548
Iteration 111/1000 | Loss: 0.00001548
Iteration 112/1000 | Loss: 0.00001547
Iteration 113/1000 | Loss: 0.00001547
Iteration 114/1000 | Loss: 0.00001546
Iteration 115/1000 | Loss: 0.00001546
Iteration 116/1000 | Loss: 0.00001545
Iteration 117/1000 | Loss: 0.00001544
Iteration 118/1000 | Loss: 0.00001543
Iteration 119/1000 | Loss: 0.00001543
Iteration 120/1000 | Loss: 0.00001542
Iteration 121/1000 | Loss: 0.00001542
Iteration 122/1000 | Loss: 0.00001541
Iteration 123/1000 | Loss: 0.00001540
Iteration 124/1000 | Loss: 0.00001539
Iteration 125/1000 | Loss: 0.00001539
Iteration 126/1000 | Loss: 0.00001539
Iteration 127/1000 | Loss: 0.00001539
Iteration 128/1000 | Loss: 0.00001538
Iteration 129/1000 | Loss: 0.00001538
Iteration 130/1000 | Loss: 0.00001538
Iteration 131/1000 | Loss: 0.00001538
Iteration 132/1000 | Loss: 0.00001538
Iteration 133/1000 | Loss: 0.00001537
Iteration 134/1000 | Loss: 0.00001537
Iteration 135/1000 | Loss: 0.00001537
Iteration 136/1000 | Loss: 0.00001537
Iteration 137/1000 | Loss: 0.00001536
Iteration 138/1000 | Loss: 0.00001536
Iteration 139/1000 | Loss: 0.00001536
Iteration 140/1000 | Loss: 0.00001536
Iteration 141/1000 | Loss: 0.00001536
Iteration 142/1000 | Loss: 0.00001536
Iteration 143/1000 | Loss: 0.00001536
Iteration 144/1000 | Loss: 0.00001536
Iteration 145/1000 | Loss: 0.00001536
Iteration 146/1000 | Loss: 0.00001536
Iteration 147/1000 | Loss: 0.00001536
Iteration 148/1000 | Loss: 0.00001536
Iteration 149/1000 | Loss: 0.00001536
Iteration 150/1000 | Loss: 0.00001536
Iteration 151/1000 | Loss: 0.00001535
Iteration 152/1000 | Loss: 0.00001535
Iteration 153/1000 | Loss: 0.00001535
Iteration 154/1000 | Loss: 0.00001534
Iteration 155/1000 | Loss: 0.00001534
Iteration 156/1000 | Loss: 0.00001534
Iteration 157/1000 | Loss: 0.00001534
Iteration 158/1000 | Loss: 0.00001534
Iteration 159/1000 | Loss: 0.00001533
Iteration 160/1000 | Loss: 0.00001533
Iteration 161/1000 | Loss: 0.00001533
Iteration 162/1000 | Loss: 0.00001533
Iteration 163/1000 | Loss: 0.00001533
Iteration 164/1000 | Loss: 0.00001533
Iteration 165/1000 | Loss: 0.00001533
Iteration 166/1000 | Loss: 0.00001533
Iteration 167/1000 | Loss: 0.00001533
Iteration 168/1000 | Loss: 0.00001533
Iteration 169/1000 | Loss: 0.00001533
Iteration 170/1000 | Loss: 0.00001533
Iteration 171/1000 | Loss: 0.00001533
Iteration 172/1000 | Loss: 0.00001533
Iteration 173/1000 | Loss: 0.00001533
Iteration 174/1000 | Loss: 0.00001533
Iteration 175/1000 | Loss: 0.00001533
Iteration 176/1000 | Loss: 0.00001533
Iteration 177/1000 | Loss: 0.00001533
Iteration 178/1000 | Loss: 0.00001533
Iteration 179/1000 | Loss: 0.00001533
Iteration 180/1000 | Loss: 0.00001533
Iteration 181/1000 | Loss: 0.00001533
Iteration 182/1000 | Loss: 0.00001533
Iteration 183/1000 | Loss: 0.00001532
Iteration 184/1000 | Loss: 0.00001532
Iteration 185/1000 | Loss: 0.00001532
Iteration 186/1000 | Loss: 0.00001532
Iteration 187/1000 | Loss: 0.00001532
Iteration 188/1000 | Loss: 0.00001532
Iteration 189/1000 | Loss: 0.00001532
Iteration 190/1000 | Loss: 0.00001532
Iteration 191/1000 | Loss: 0.00001532
Iteration 192/1000 | Loss: 0.00001532
Iteration 193/1000 | Loss: 0.00001532
Iteration 194/1000 | Loss: 0.00001532
Iteration 195/1000 | Loss: 0.00001532
Iteration 196/1000 | Loss: 0.00001532
Iteration 197/1000 | Loss: 0.00001532
Iteration 198/1000 | Loss: 0.00001532
Iteration 199/1000 | Loss: 0.00001532
Iteration 200/1000 | Loss: 0.00001532
Iteration 201/1000 | Loss: 0.00001532
Iteration 202/1000 | Loss: 0.00001532
Iteration 203/1000 | Loss: 0.00001532
Iteration 204/1000 | Loss: 0.00001532
Iteration 205/1000 | Loss: 0.00001532
Iteration 206/1000 | Loss: 0.00001532
Iteration 207/1000 | Loss: 0.00001532
Iteration 208/1000 | Loss: 0.00001532
Iteration 209/1000 | Loss: 0.00001532
Iteration 210/1000 | Loss: 0.00001532
Iteration 211/1000 | Loss: 0.00001532
Iteration 212/1000 | Loss: 0.00001532
Iteration 213/1000 | Loss: 0.00001532
Iteration 214/1000 | Loss: 0.00001532
Iteration 215/1000 | Loss: 0.00001532
Iteration 216/1000 | Loss: 0.00001532
Iteration 217/1000 | Loss: 0.00001532
Iteration 218/1000 | Loss: 0.00001532
Iteration 219/1000 | Loss: 0.00001532
Iteration 220/1000 | Loss: 0.00001532
Iteration 221/1000 | Loss: 0.00001532
Iteration 222/1000 | Loss: 0.00001532
Iteration 223/1000 | Loss: 0.00001532
Iteration 224/1000 | Loss: 0.00001532
Iteration 225/1000 | Loss: 0.00001532
Iteration 226/1000 | Loss: 0.00001532
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 226. Stopping optimization.
Last 5 losses: [1.532479473098647e-05, 1.532479473098647e-05, 1.532479473098647e-05, 1.532479473098647e-05, 1.532479473098647e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.532479473098647e-05

Optimization complete. Final v2v error: 3.3283073902130127 mm

Highest mean error: 4.3188157081604 mm for frame 236

Lowest mean error: 2.7109432220458984 mm for frame 43

Saving results

Total time: 180.53760647773743
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_013/1015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_013/1015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_013/1015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01074270
Iteration 2/25 | Loss: 0.00335078
Iteration 3/25 | Loss: 0.00245146
Iteration 4/25 | Loss: 0.00221230
Iteration 5/25 | Loss: 0.00172060
Iteration 6/25 | Loss: 0.00157032
Iteration 7/25 | Loss: 0.00154896
Iteration 8/25 | Loss: 0.00141796
Iteration 9/25 | Loss: 0.00140078
Iteration 10/25 | Loss: 0.00137996
Iteration 11/25 | Loss: 0.00138861
Iteration 12/25 | Loss: 0.00137073
Iteration 13/25 | Loss: 0.00135326
Iteration 14/25 | Loss: 0.00135120
Iteration 15/25 | Loss: 0.00134732
Iteration 16/25 | Loss: 0.00133465
Iteration 17/25 | Loss: 0.00132999
Iteration 18/25 | Loss: 0.00132834
Iteration 19/25 | Loss: 0.00133093
Iteration 20/25 | Loss: 0.00133033
Iteration 21/25 | Loss: 0.00132955
Iteration 22/25 | Loss: 0.00132713
Iteration 23/25 | Loss: 0.00132627
Iteration 24/25 | Loss: 0.00132631
Iteration 25/25 | Loss: 0.00132630

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.60451329
Iteration 2/25 | Loss: 0.00115110
Iteration 3/25 | Loss: 0.00115110
Iteration 4/25 | Loss: 0.00181656
Iteration 5/25 | Loss: 0.00115110
Iteration 6/25 | Loss: 0.00115110
Iteration 7/25 | Loss: 0.00115110
Iteration 8/25 | Loss: 0.00115110
Iteration 9/25 | Loss: 0.00181528
Iteration 10/25 | Loss: 0.00115110
Iteration 11/25 | Loss: 0.00115110
Iteration 12/25 | Loss: 0.00115110
Iteration 13/25 | Loss: 0.00115110
Iteration 14/25 | Loss: 0.00115110
Iteration 15/25 | Loss: 0.00115110
Iteration 16/25 | Loss: 0.00115110
Iteration 17/25 | Loss: 0.00115110
Iteration 18/25 | Loss: 0.00115110
Iteration 19/25 | Loss: 0.00115110
Iteration 20/25 | Loss: 0.00115110
Iteration 21/25 | Loss: 0.00115110
Iteration 22/25 | Loss: 0.00115110
Iteration 23/25 | Loss: 0.00115110
Iteration 24/25 | Loss: 0.00115110
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0011510952608659863, 0.0011510952608659863, 0.0011510952608659863, 0.0011510952608659863, 0.0011510952608659863]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011510952608659863

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00115110
Iteration 2/1000 | Loss: 0.00012589
Iteration 3/1000 | Loss: 0.00009163
Iteration 4/1000 | Loss: 0.00364565
Iteration 5/1000 | Loss: 0.00062117
Iteration 6/1000 | Loss: 0.00569235
Iteration 7/1000 | Loss: 0.00199748
Iteration 8/1000 | Loss: 0.00010964
Iteration 9/1000 | Loss: 0.00116018
Iteration 10/1000 | Loss: 0.00007714
Iteration 11/1000 | Loss: 0.00006751
Iteration 12/1000 | Loss: 0.00006445
Iteration 13/1000 | Loss: 0.00006231
Iteration 14/1000 | Loss: 0.00006141
Iteration 15/1000 | Loss: 0.00006010
Iteration 16/1000 | Loss: 0.00005928
Iteration 17/1000 | Loss: 0.00005887
Iteration 18/1000 | Loss: 0.00005865
Iteration 19/1000 | Loss: 0.00005838
Iteration 20/1000 | Loss: 0.00005796
Iteration 21/1000 | Loss: 0.00005754
Iteration 22/1000 | Loss: 0.00005714
Iteration 23/1000 | Loss: 0.00005663
Iteration 24/1000 | Loss: 0.00005614
Iteration 25/1000 | Loss: 0.00005570
Iteration 26/1000 | Loss: 0.00005517
Iteration 27/1000 | Loss: 0.00024709
Iteration 28/1000 | Loss: 0.00016614
Iteration 29/1000 | Loss: 0.00028003
Iteration 30/1000 | Loss: 0.00006315
Iteration 31/1000 | Loss: 0.00005611
Iteration 32/1000 | Loss: 0.00005174
Iteration 33/1000 | Loss: 0.00004830
Iteration 34/1000 | Loss: 0.00004632
Iteration 35/1000 | Loss: 0.00004552
Iteration 36/1000 | Loss: 0.00004484
Iteration 37/1000 | Loss: 0.00004425
Iteration 38/1000 | Loss: 0.00004385
Iteration 39/1000 | Loss: 0.00004354
Iteration 40/1000 | Loss: 0.00004334
Iteration 41/1000 | Loss: 0.00004320
Iteration 42/1000 | Loss: 0.00004312
Iteration 43/1000 | Loss: 0.00004310
Iteration 44/1000 | Loss: 0.00004310
Iteration 45/1000 | Loss: 0.00004308
Iteration 46/1000 | Loss: 0.00004307
Iteration 47/1000 | Loss: 0.00004307
Iteration 48/1000 | Loss: 0.00004307
Iteration 49/1000 | Loss: 0.00004306
Iteration 50/1000 | Loss: 0.00004305
Iteration 51/1000 | Loss: 0.00004305
Iteration 52/1000 | Loss: 0.00004305
Iteration 53/1000 | Loss: 0.00004305
Iteration 54/1000 | Loss: 0.00004305
Iteration 55/1000 | Loss: 0.00004305
Iteration 56/1000 | Loss: 0.00004305
Iteration 57/1000 | Loss: 0.00004305
Iteration 58/1000 | Loss: 0.00004304
Iteration 59/1000 | Loss: 0.00004304
Iteration 60/1000 | Loss: 0.00004304
Iteration 61/1000 | Loss: 0.00004304
Iteration 62/1000 | Loss: 0.00004304
Iteration 63/1000 | Loss: 0.00004304
Iteration 64/1000 | Loss: 0.00004304
Iteration 65/1000 | Loss: 0.00004304
Iteration 66/1000 | Loss: 0.00004304
Iteration 67/1000 | Loss: 0.00004304
Iteration 68/1000 | Loss: 0.00004303
Iteration 69/1000 | Loss: 0.00004303
Iteration 70/1000 | Loss: 0.00004303
Iteration 71/1000 | Loss: 0.00004303
Iteration 72/1000 | Loss: 0.00004303
Iteration 73/1000 | Loss: 0.00004302
Iteration 74/1000 | Loss: 0.00004302
Iteration 75/1000 | Loss: 0.00004301
Iteration 76/1000 | Loss: 0.00004301
Iteration 77/1000 | Loss: 0.00004300
Iteration 78/1000 | Loss: 0.00004300
Iteration 79/1000 | Loss: 0.00004300
Iteration 80/1000 | Loss: 0.00004300
Iteration 81/1000 | Loss: 0.00004300
Iteration 82/1000 | Loss: 0.00004321
Iteration 83/1000 | Loss: 0.00004307
Iteration 84/1000 | Loss: 0.00004307
Iteration 85/1000 | Loss: 0.00004306
Iteration 86/1000 | Loss: 0.00004306
Iteration 87/1000 | Loss: 0.00004306
Iteration 88/1000 | Loss: 0.00004306
Iteration 89/1000 | Loss: 0.00004306
Iteration 90/1000 | Loss: 0.00004306
Iteration 91/1000 | Loss: 0.00004306
Iteration 92/1000 | Loss: 0.00004305
Iteration 93/1000 | Loss: 0.00004298
Iteration 94/1000 | Loss: 0.00004297
Iteration 95/1000 | Loss: 0.00004297
Iteration 96/1000 | Loss: 0.00004297
Iteration 97/1000 | Loss: 0.00004297
Iteration 98/1000 | Loss: 0.00004297
Iteration 99/1000 | Loss: 0.00004297
Iteration 100/1000 | Loss: 0.00004297
Iteration 101/1000 | Loss: 0.00004297
Iteration 102/1000 | Loss: 0.00004296
Iteration 103/1000 | Loss: 0.00004298
Iteration 104/1000 | Loss: 0.00004298
Iteration 105/1000 | Loss: 0.00004298
Iteration 106/1000 | Loss: 0.00004298
Iteration 107/1000 | Loss: 0.00004296
Iteration 108/1000 | Loss: 0.00004295
Iteration 109/1000 | Loss: 0.00004294
Iteration 110/1000 | Loss: 0.00004294
Iteration 111/1000 | Loss: 0.00004294
Iteration 112/1000 | Loss: 0.00004294
Iteration 113/1000 | Loss: 0.00004294
Iteration 114/1000 | Loss: 0.00004294
Iteration 115/1000 | Loss: 0.00004294
Iteration 116/1000 | Loss: 0.00004294
Iteration 117/1000 | Loss: 0.00004294
Iteration 118/1000 | Loss: 0.00004294
Iteration 119/1000 | Loss: 0.00004294
Iteration 120/1000 | Loss: 0.00004294
Iteration 121/1000 | Loss: 0.00004294
Iteration 122/1000 | Loss: 0.00004293
Iteration 123/1000 | Loss: 0.00004293
Iteration 124/1000 | Loss: 0.00004293
Iteration 125/1000 | Loss: 0.00004293
Iteration 126/1000 | Loss: 0.00004293
Iteration 127/1000 | Loss: 0.00004293
Iteration 128/1000 | Loss: 0.00004293
Iteration 129/1000 | Loss: 0.00004293
Iteration 130/1000 | Loss: 0.00004293
Iteration 131/1000 | Loss: 0.00004293
Iteration 132/1000 | Loss: 0.00004293
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 132. Stopping optimization.
Last 5 losses: [4.293258461984806e-05, 4.293258461984806e-05, 4.293258461984806e-05, 4.293258461984806e-05, 4.293258461984806e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.293258461984806e-05

Optimization complete. Final v2v error: 4.729005336761475 mm

Highest mean error: 18.2627010345459 mm for frame 53

Lowest mean error: 4.2014288902282715 mm for frame 111

Saving results

Total time: 111.71591091156006
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_013/1053/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_013/1053.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_013/1053
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00472331
Iteration 2/25 | Loss: 0.00113724
Iteration 3/25 | Loss: 0.00106043
Iteration 4/25 | Loss: 0.00105245
Iteration 5/25 | Loss: 0.00104946
Iteration 6/25 | Loss: 0.00104899
Iteration 7/25 | Loss: 0.00104899
Iteration 8/25 | Loss: 0.00104899
Iteration 9/25 | Loss: 0.00104899
Iteration 10/25 | Loss: 0.00104899
Iteration 11/25 | Loss: 0.00104899
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010489918058738112, 0.0010489918058738112, 0.0010489918058738112, 0.0010489918058738112, 0.0010489918058738112]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010489918058738112

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.69899559
Iteration 2/25 | Loss: 0.00066991
Iteration 3/25 | Loss: 0.00066990
Iteration 4/25 | Loss: 0.00066990
Iteration 5/25 | Loss: 0.00066990
Iteration 6/25 | Loss: 0.00066990
Iteration 7/25 | Loss: 0.00066990
Iteration 8/25 | Loss: 0.00066990
Iteration 9/25 | Loss: 0.00066990
Iteration 10/25 | Loss: 0.00066990
Iteration 11/25 | Loss: 0.00066990
Iteration 12/25 | Loss: 0.00066990
Iteration 13/25 | Loss: 0.00066990
Iteration 14/25 | Loss: 0.00066990
Iteration 15/25 | Loss: 0.00066990
Iteration 16/25 | Loss: 0.00066990
Iteration 17/25 | Loss: 0.00066990
Iteration 18/25 | Loss: 0.00066990
Iteration 19/25 | Loss: 0.00066990
Iteration 20/25 | Loss: 0.00066990
Iteration 21/25 | Loss: 0.00066990
Iteration 22/25 | Loss: 0.00066990
Iteration 23/25 | Loss: 0.00066990
Iteration 24/25 | Loss: 0.00066990
Iteration 25/25 | Loss: 0.00066990

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00066990
Iteration 2/1000 | Loss: 0.00002519
Iteration 3/1000 | Loss: 0.00001522
Iteration 4/1000 | Loss: 0.00001277
Iteration 5/1000 | Loss: 0.00001183
Iteration 6/1000 | Loss: 0.00001141
Iteration 7/1000 | Loss: 0.00001101
Iteration 8/1000 | Loss: 0.00001070
Iteration 9/1000 | Loss: 0.00001057
Iteration 10/1000 | Loss: 0.00001033
Iteration 11/1000 | Loss: 0.00001019
Iteration 12/1000 | Loss: 0.00001017
Iteration 13/1000 | Loss: 0.00001012
Iteration 14/1000 | Loss: 0.00001011
Iteration 15/1000 | Loss: 0.00001010
Iteration 16/1000 | Loss: 0.00001009
Iteration 17/1000 | Loss: 0.00001002
Iteration 18/1000 | Loss: 0.00001001
Iteration 19/1000 | Loss: 0.00000999
Iteration 20/1000 | Loss: 0.00000998
Iteration 21/1000 | Loss: 0.00000995
Iteration 22/1000 | Loss: 0.00000995
Iteration 23/1000 | Loss: 0.00000994
Iteration 24/1000 | Loss: 0.00000991
Iteration 25/1000 | Loss: 0.00000989
Iteration 26/1000 | Loss: 0.00000988
Iteration 27/1000 | Loss: 0.00000987
Iteration 28/1000 | Loss: 0.00000986
Iteration 29/1000 | Loss: 0.00000986
Iteration 30/1000 | Loss: 0.00000986
Iteration 31/1000 | Loss: 0.00000985
Iteration 32/1000 | Loss: 0.00000985
Iteration 33/1000 | Loss: 0.00000984
Iteration 34/1000 | Loss: 0.00000983
Iteration 35/1000 | Loss: 0.00000982
Iteration 36/1000 | Loss: 0.00000982
Iteration 37/1000 | Loss: 0.00000982
Iteration 38/1000 | Loss: 0.00000982
Iteration 39/1000 | Loss: 0.00000982
Iteration 40/1000 | Loss: 0.00000982
Iteration 41/1000 | Loss: 0.00000981
Iteration 42/1000 | Loss: 0.00000981
Iteration 43/1000 | Loss: 0.00000981
Iteration 44/1000 | Loss: 0.00000981
Iteration 45/1000 | Loss: 0.00000981
Iteration 46/1000 | Loss: 0.00000981
Iteration 47/1000 | Loss: 0.00000981
Iteration 48/1000 | Loss: 0.00000980
Iteration 49/1000 | Loss: 0.00000979
Iteration 50/1000 | Loss: 0.00000978
Iteration 51/1000 | Loss: 0.00000978
Iteration 52/1000 | Loss: 0.00000977
Iteration 53/1000 | Loss: 0.00000977
Iteration 54/1000 | Loss: 0.00000976
Iteration 55/1000 | Loss: 0.00000976
Iteration 56/1000 | Loss: 0.00000975
Iteration 57/1000 | Loss: 0.00000975
Iteration 58/1000 | Loss: 0.00000975
Iteration 59/1000 | Loss: 0.00000975
Iteration 60/1000 | Loss: 0.00000975
Iteration 61/1000 | Loss: 0.00000973
Iteration 62/1000 | Loss: 0.00000973
Iteration 63/1000 | Loss: 0.00000972
Iteration 64/1000 | Loss: 0.00000972
Iteration 65/1000 | Loss: 0.00000971
Iteration 66/1000 | Loss: 0.00000971
Iteration 67/1000 | Loss: 0.00000970
Iteration 68/1000 | Loss: 0.00000969
Iteration 69/1000 | Loss: 0.00000969
Iteration 70/1000 | Loss: 0.00000969
Iteration 71/1000 | Loss: 0.00000969
Iteration 72/1000 | Loss: 0.00000969
Iteration 73/1000 | Loss: 0.00000969
Iteration 74/1000 | Loss: 0.00000969
Iteration 75/1000 | Loss: 0.00000968
Iteration 76/1000 | Loss: 0.00000968
Iteration 77/1000 | Loss: 0.00000968
Iteration 78/1000 | Loss: 0.00000968
Iteration 79/1000 | Loss: 0.00000968
Iteration 80/1000 | Loss: 0.00000968
Iteration 81/1000 | Loss: 0.00000968
Iteration 82/1000 | Loss: 0.00000968
Iteration 83/1000 | Loss: 0.00000968
Iteration 84/1000 | Loss: 0.00000968
Iteration 85/1000 | Loss: 0.00000967
Iteration 86/1000 | Loss: 0.00000967
Iteration 87/1000 | Loss: 0.00000967
Iteration 88/1000 | Loss: 0.00000967
Iteration 89/1000 | Loss: 0.00000967
Iteration 90/1000 | Loss: 0.00000967
Iteration 91/1000 | Loss: 0.00000967
Iteration 92/1000 | Loss: 0.00000967
Iteration 93/1000 | Loss: 0.00000966
Iteration 94/1000 | Loss: 0.00000966
Iteration 95/1000 | Loss: 0.00000965
Iteration 96/1000 | Loss: 0.00000965
Iteration 97/1000 | Loss: 0.00000965
Iteration 98/1000 | Loss: 0.00000965
Iteration 99/1000 | Loss: 0.00000965
Iteration 100/1000 | Loss: 0.00000965
Iteration 101/1000 | Loss: 0.00000965
Iteration 102/1000 | Loss: 0.00000965
Iteration 103/1000 | Loss: 0.00000964
Iteration 104/1000 | Loss: 0.00000964
Iteration 105/1000 | Loss: 0.00000964
Iteration 106/1000 | Loss: 0.00000964
Iteration 107/1000 | Loss: 0.00000964
Iteration 108/1000 | Loss: 0.00000964
Iteration 109/1000 | Loss: 0.00000964
Iteration 110/1000 | Loss: 0.00000964
Iteration 111/1000 | Loss: 0.00000964
Iteration 112/1000 | Loss: 0.00000964
Iteration 113/1000 | Loss: 0.00000963
Iteration 114/1000 | Loss: 0.00000962
Iteration 115/1000 | Loss: 0.00000962
Iteration 116/1000 | Loss: 0.00000962
Iteration 117/1000 | Loss: 0.00000961
Iteration 118/1000 | Loss: 0.00000961
Iteration 119/1000 | Loss: 0.00000961
Iteration 120/1000 | Loss: 0.00000961
Iteration 121/1000 | Loss: 0.00000961
Iteration 122/1000 | Loss: 0.00000961
Iteration 123/1000 | Loss: 0.00000961
Iteration 124/1000 | Loss: 0.00000961
Iteration 125/1000 | Loss: 0.00000961
Iteration 126/1000 | Loss: 0.00000961
Iteration 127/1000 | Loss: 0.00000961
Iteration 128/1000 | Loss: 0.00000960
Iteration 129/1000 | Loss: 0.00000960
Iteration 130/1000 | Loss: 0.00000959
Iteration 131/1000 | Loss: 0.00000959
Iteration 132/1000 | Loss: 0.00000959
Iteration 133/1000 | Loss: 0.00000959
Iteration 134/1000 | Loss: 0.00000959
Iteration 135/1000 | Loss: 0.00000958
Iteration 136/1000 | Loss: 0.00000958
Iteration 137/1000 | Loss: 0.00000958
Iteration 138/1000 | Loss: 0.00000958
Iteration 139/1000 | Loss: 0.00000958
Iteration 140/1000 | Loss: 0.00000957
Iteration 141/1000 | Loss: 0.00000957
Iteration 142/1000 | Loss: 0.00000957
Iteration 143/1000 | Loss: 0.00000957
Iteration 144/1000 | Loss: 0.00000957
Iteration 145/1000 | Loss: 0.00000957
Iteration 146/1000 | Loss: 0.00000957
Iteration 147/1000 | Loss: 0.00000956
Iteration 148/1000 | Loss: 0.00000956
Iteration 149/1000 | Loss: 0.00000956
Iteration 150/1000 | Loss: 0.00000955
Iteration 151/1000 | Loss: 0.00000955
Iteration 152/1000 | Loss: 0.00000955
Iteration 153/1000 | Loss: 0.00000955
Iteration 154/1000 | Loss: 0.00000954
Iteration 155/1000 | Loss: 0.00000954
Iteration 156/1000 | Loss: 0.00000954
Iteration 157/1000 | Loss: 0.00000954
Iteration 158/1000 | Loss: 0.00000954
Iteration 159/1000 | Loss: 0.00000954
Iteration 160/1000 | Loss: 0.00000954
Iteration 161/1000 | Loss: 0.00000954
Iteration 162/1000 | Loss: 0.00000954
Iteration 163/1000 | Loss: 0.00000953
Iteration 164/1000 | Loss: 0.00000953
Iteration 165/1000 | Loss: 0.00000952
Iteration 166/1000 | Loss: 0.00000952
Iteration 167/1000 | Loss: 0.00000952
Iteration 168/1000 | Loss: 0.00000952
Iteration 169/1000 | Loss: 0.00000952
Iteration 170/1000 | Loss: 0.00000952
Iteration 171/1000 | Loss: 0.00000952
Iteration 172/1000 | Loss: 0.00000951
Iteration 173/1000 | Loss: 0.00000951
Iteration 174/1000 | Loss: 0.00000951
Iteration 175/1000 | Loss: 0.00000951
Iteration 176/1000 | Loss: 0.00000950
Iteration 177/1000 | Loss: 0.00000950
Iteration 178/1000 | Loss: 0.00000950
Iteration 179/1000 | Loss: 0.00000950
Iteration 180/1000 | Loss: 0.00000950
Iteration 181/1000 | Loss: 0.00000950
Iteration 182/1000 | Loss: 0.00000949
Iteration 183/1000 | Loss: 0.00000949
Iteration 184/1000 | Loss: 0.00000949
Iteration 185/1000 | Loss: 0.00000949
Iteration 186/1000 | Loss: 0.00000949
Iteration 187/1000 | Loss: 0.00000949
Iteration 188/1000 | Loss: 0.00000949
Iteration 189/1000 | Loss: 0.00000949
Iteration 190/1000 | Loss: 0.00000949
Iteration 191/1000 | Loss: 0.00000949
Iteration 192/1000 | Loss: 0.00000949
Iteration 193/1000 | Loss: 0.00000948
Iteration 194/1000 | Loss: 0.00000948
Iteration 195/1000 | Loss: 0.00000948
Iteration 196/1000 | Loss: 0.00000948
Iteration 197/1000 | Loss: 0.00000948
Iteration 198/1000 | Loss: 0.00000948
Iteration 199/1000 | Loss: 0.00000948
Iteration 200/1000 | Loss: 0.00000948
Iteration 201/1000 | Loss: 0.00000948
Iteration 202/1000 | Loss: 0.00000948
Iteration 203/1000 | Loss: 0.00000948
Iteration 204/1000 | Loss: 0.00000948
Iteration 205/1000 | Loss: 0.00000948
Iteration 206/1000 | Loss: 0.00000948
Iteration 207/1000 | Loss: 0.00000948
Iteration 208/1000 | Loss: 0.00000948
Iteration 209/1000 | Loss: 0.00000948
Iteration 210/1000 | Loss: 0.00000948
Iteration 211/1000 | Loss: 0.00000948
Iteration 212/1000 | Loss: 0.00000947
Iteration 213/1000 | Loss: 0.00000947
Iteration 214/1000 | Loss: 0.00000947
Iteration 215/1000 | Loss: 0.00000947
Iteration 216/1000 | Loss: 0.00000947
Iteration 217/1000 | Loss: 0.00000947
Iteration 218/1000 | Loss: 0.00000947
Iteration 219/1000 | Loss: 0.00000947
Iteration 220/1000 | Loss: 0.00000947
Iteration 221/1000 | Loss: 0.00000947
Iteration 222/1000 | Loss: 0.00000947
Iteration 223/1000 | Loss: 0.00000947
Iteration 224/1000 | Loss: 0.00000947
Iteration 225/1000 | Loss: 0.00000947
Iteration 226/1000 | Loss: 0.00000947
Iteration 227/1000 | Loss: 0.00000947
Iteration 228/1000 | Loss: 0.00000947
Iteration 229/1000 | Loss: 0.00000947
Iteration 230/1000 | Loss: 0.00000947
Iteration 231/1000 | Loss: 0.00000947
Iteration 232/1000 | Loss: 0.00000947
Iteration 233/1000 | Loss: 0.00000947
Iteration 234/1000 | Loss: 0.00000946
Iteration 235/1000 | Loss: 0.00000946
Iteration 236/1000 | Loss: 0.00000946
Iteration 237/1000 | Loss: 0.00000946
Iteration 238/1000 | Loss: 0.00000946
Iteration 239/1000 | Loss: 0.00000946
Iteration 240/1000 | Loss: 0.00000946
Iteration 241/1000 | Loss: 0.00000946
Iteration 242/1000 | Loss: 0.00000946
Iteration 243/1000 | Loss: 0.00000946
Iteration 244/1000 | Loss: 0.00000946
Iteration 245/1000 | Loss: 0.00000946
Iteration 246/1000 | Loss: 0.00000946
Iteration 247/1000 | Loss: 0.00000946
Iteration 248/1000 | Loss: 0.00000946
Iteration 249/1000 | Loss: 0.00000946
Iteration 250/1000 | Loss: 0.00000946
Iteration 251/1000 | Loss: 0.00000946
Iteration 252/1000 | Loss: 0.00000946
Iteration 253/1000 | Loss: 0.00000946
Iteration 254/1000 | Loss: 0.00000946
Iteration 255/1000 | Loss: 0.00000946
Iteration 256/1000 | Loss: 0.00000945
Iteration 257/1000 | Loss: 0.00000945
Iteration 258/1000 | Loss: 0.00000945
Iteration 259/1000 | Loss: 0.00000945
Iteration 260/1000 | Loss: 0.00000945
Iteration 261/1000 | Loss: 0.00000945
Iteration 262/1000 | Loss: 0.00000945
Iteration 263/1000 | Loss: 0.00000945
Iteration 264/1000 | Loss: 0.00000945
Iteration 265/1000 | Loss: 0.00000945
Iteration 266/1000 | Loss: 0.00000945
Iteration 267/1000 | Loss: 0.00000945
Iteration 268/1000 | Loss: 0.00000945
Iteration 269/1000 | Loss: 0.00000945
Iteration 270/1000 | Loss: 0.00000945
Iteration 271/1000 | Loss: 0.00000945
Iteration 272/1000 | Loss: 0.00000945
Iteration 273/1000 | Loss: 0.00000945
Iteration 274/1000 | Loss: 0.00000945
Iteration 275/1000 | Loss: 0.00000945
Iteration 276/1000 | Loss: 0.00000945
Iteration 277/1000 | Loss: 0.00000945
Iteration 278/1000 | Loss: 0.00000945
Iteration 279/1000 | Loss: 0.00000945
Iteration 280/1000 | Loss: 0.00000945
Iteration 281/1000 | Loss: 0.00000945
Iteration 282/1000 | Loss: 0.00000945
Iteration 283/1000 | Loss: 0.00000945
Iteration 284/1000 | Loss: 0.00000945
Iteration 285/1000 | Loss: 0.00000945
Iteration 286/1000 | Loss: 0.00000945
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 286. Stopping optimization.
Last 5 losses: [9.450735888094641e-06, 9.450735888094641e-06, 9.450735888094641e-06, 9.450735888094641e-06, 9.450735888094641e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.450735888094641e-06

Optimization complete. Final v2v error: 2.6406946182250977 mm

Highest mean error: 2.9575893878936768 mm for frame 75

Lowest mean error: 2.408919095993042 mm for frame 9

Saving results

Total time: 43.58244442939758
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_013/1037/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_013/1037.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_013/1037
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01037097
Iteration 2/25 | Loss: 0.00250727
Iteration 3/25 | Loss: 0.00205120
Iteration 4/25 | Loss: 0.00190489
Iteration 5/25 | Loss: 0.00167223
Iteration 6/25 | Loss: 0.00157575
Iteration 7/25 | Loss: 0.00153389
Iteration 8/25 | Loss: 0.00157705
Iteration 9/25 | Loss: 0.00149938
Iteration 10/25 | Loss: 0.00140060
Iteration 11/25 | Loss: 0.00130533
Iteration 12/25 | Loss: 0.00131263
Iteration 13/25 | Loss: 0.00128938
Iteration 14/25 | Loss: 0.00126572
Iteration 15/25 | Loss: 0.00126845
Iteration 16/25 | Loss: 0.00119379
Iteration 17/25 | Loss: 0.00118596
Iteration 18/25 | Loss: 0.00119122
Iteration 19/25 | Loss: 0.00117976
Iteration 20/25 | Loss: 0.00117547
Iteration 21/25 | Loss: 0.00117372
Iteration 22/25 | Loss: 0.00117335
Iteration 23/25 | Loss: 0.00117979
Iteration 24/25 | Loss: 0.00117261
Iteration 25/25 | Loss: 0.00117215

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.40234566
Iteration 2/25 | Loss: 0.00181514
Iteration 3/25 | Loss: 0.00174604
Iteration 4/25 | Loss: 0.00338582
Iteration 5/25 | Loss: 0.00263794
Iteration 6/25 | Loss: 0.00099104
Iteration 7/25 | Loss: 0.00167400
Iteration 8/25 | Loss: 0.00095144
Iteration 9/25 | Loss: 0.00095143
Iteration 10/25 | Loss: 0.00095143
Iteration 11/25 | Loss: 0.00095143
Iteration 12/25 | Loss: 0.00095143
Iteration 13/25 | Loss: 0.00095143
Iteration 14/25 | Loss: 0.00095143
Iteration 15/25 | Loss: 0.00095143
Iteration 16/25 | Loss: 0.00095143
Iteration 17/25 | Loss: 0.00095143
Iteration 18/25 | Loss: 0.00095143
Iteration 19/25 | Loss: 0.00095143
Iteration 20/25 | Loss: 0.00095143
Iteration 21/25 | Loss: 0.00095143
Iteration 22/25 | Loss: 0.00095143
Iteration 23/25 | Loss: 0.00095143
Iteration 24/25 | Loss: 0.00095143
Iteration 25/25 | Loss: 0.00095143
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0009514322155155241, 0.0009514322155155241, 0.0009514322155155241, 0.0009514322155155241, 0.0009514322155155241]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009514322155155241

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00095143
Iteration 2/1000 | Loss: 0.00062673
Iteration 3/1000 | Loss: 0.00055406
Iteration 4/1000 | Loss: 0.00024480
Iteration 5/1000 | Loss: 0.00002453
Iteration 6/1000 | Loss: 0.00067856
Iteration 7/1000 | Loss: 0.00093269
Iteration 8/1000 | Loss: 0.00022381
Iteration 9/1000 | Loss: 0.00022301
Iteration 10/1000 | Loss: 0.00005934
Iteration 11/1000 | Loss: 0.00002366
Iteration 12/1000 | Loss: 0.00003238
Iteration 13/1000 | Loss: 0.00005701
Iteration 14/1000 | Loss: 0.00001850
Iteration 15/1000 | Loss: 0.00013664
Iteration 16/1000 | Loss: 0.00001733
Iteration 17/1000 | Loss: 0.00001680
Iteration 18/1000 | Loss: 0.00001634
Iteration 19/1000 | Loss: 0.00001605
Iteration 20/1000 | Loss: 0.00001586
Iteration 21/1000 | Loss: 0.00001577
Iteration 22/1000 | Loss: 0.00001568
Iteration 23/1000 | Loss: 0.00001565
Iteration 24/1000 | Loss: 0.00001564
Iteration 25/1000 | Loss: 0.00001560
Iteration 26/1000 | Loss: 0.00001556
Iteration 27/1000 | Loss: 0.00001556
Iteration 28/1000 | Loss: 0.00001555
Iteration 29/1000 | Loss: 0.00001555
Iteration 30/1000 | Loss: 0.00001554
Iteration 31/1000 | Loss: 0.00001554
Iteration 32/1000 | Loss: 0.00001554
Iteration 33/1000 | Loss: 0.00001553
Iteration 34/1000 | Loss: 0.00001552
Iteration 35/1000 | Loss: 0.00001551
Iteration 36/1000 | Loss: 0.00001550
Iteration 37/1000 | Loss: 0.00001548
Iteration 38/1000 | Loss: 0.00001547
Iteration 39/1000 | Loss: 0.00001545
Iteration 40/1000 | Loss: 0.00001544
Iteration 41/1000 | Loss: 0.00001544
Iteration 42/1000 | Loss: 0.00001544
Iteration 43/1000 | Loss: 0.00001543
Iteration 44/1000 | Loss: 0.00001543
Iteration 45/1000 | Loss: 0.00001542
Iteration 46/1000 | Loss: 0.00001539
Iteration 47/1000 | Loss: 0.00001539
Iteration 48/1000 | Loss: 0.00001539
Iteration 49/1000 | Loss: 0.00001539
Iteration 50/1000 | Loss: 0.00001539
Iteration 51/1000 | Loss: 0.00001539
Iteration 52/1000 | Loss: 0.00001539
Iteration 53/1000 | Loss: 0.00001539
Iteration 54/1000 | Loss: 0.00001539
Iteration 55/1000 | Loss: 0.00001539
Iteration 56/1000 | Loss: 0.00001538
Iteration 57/1000 | Loss: 0.00001538
Iteration 58/1000 | Loss: 0.00001538
Iteration 59/1000 | Loss: 0.00001538
Iteration 60/1000 | Loss: 0.00001537
Iteration 61/1000 | Loss: 0.00001536
Iteration 62/1000 | Loss: 0.00001535
Iteration 63/1000 | Loss: 0.00001535
Iteration 64/1000 | Loss: 0.00001534
Iteration 65/1000 | Loss: 0.00001534
Iteration 66/1000 | Loss: 0.00001534
Iteration 67/1000 | Loss: 0.00001533
Iteration 68/1000 | Loss: 0.00001532
Iteration 69/1000 | Loss: 0.00001532
Iteration 70/1000 | Loss: 0.00029958
Iteration 71/1000 | Loss: 0.00032799
Iteration 72/1000 | Loss: 0.00001948
Iteration 73/1000 | Loss: 0.00001592
Iteration 74/1000 | Loss: 0.00001522
Iteration 75/1000 | Loss: 0.00008481
Iteration 76/1000 | Loss: 0.00001570
Iteration 77/1000 | Loss: 0.00001435
Iteration 78/1000 | Loss: 0.00001424
Iteration 79/1000 | Loss: 0.00001423
Iteration 80/1000 | Loss: 0.00001415
Iteration 81/1000 | Loss: 0.00001415
Iteration 82/1000 | Loss: 0.00001415
Iteration 83/1000 | Loss: 0.00001414
Iteration 84/1000 | Loss: 0.00001413
Iteration 85/1000 | Loss: 0.00001409
Iteration 86/1000 | Loss: 0.00001398
Iteration 87/1000 | Loss: 0.00001398
Iteration 88/1000 | Loss: 0.00001397
Iteration 89/1000 | Loss: 0.00001397
Iteration 90/1000 | Loss: 0.00001396
Iteration 91/1000 | Loss: 0.00001395
Iteration 92/1000 | Loss: 0.00001395
Iteration 93/1000 | Loss: 0.00001394
Iteration 94/1000 | Loss: 0.00001393
Iteration 95/1000 | Loss: 0.00001393
Iteration 96/1000 | Loss: 0.00001392
Iteration 97/1000 | Loss: 0.00001392
Iteration 98/1000 | Loss: 0.00001391
Iteration 99/1000 | Loss: 0.00001391
Iteration 100/1000 | Loss: 0.00001391
Iteration 101/1000 | Loss: 0.00001390
Iteration 102/1000 | Loss: 0.00001390
Iteration 103/1000 | Loss: 0.00001390
Iteration 104/1000 | Loss: 0.00001389
Iteration 105/1000 | Loss: 0.00001389
Iteration 106/1000 | Loss: 0.00001388
Iteration 107/1000 | Loss: 0.00001387
Iteration 108/1000 | Loss: 0.00001387
Iteration 109/1000 | Loss: 0.00001386
Iteration 110/1000 | Loss: 0.00001386
Iteration 111/1000 | Loss: 0.00001385
Iteration 112/1000 | Loss: 0.00001385
Iteration 113/1000 | Loss: 0.00001385
Iteration 114/1000 | Loss: 0.00001385
Iteration 115/1000 | Loss: 0.00001384
Iteration 116/1000 | Loss: 0.00001384
Iteration 117/1000 | Loss: 0.00001384
Iteration 118/1000 | Loss: 0.00001384
Iteration 119/1000 | Loss: 0.00001384
Iteration 120/1000 | Loss: 0.00001384
Iteration 121/1000 | Loss: 0.00001383
Iteration 122/1000 | Loss: 0.00001383
Iteration 123/1000 | Loss: 0.00001383
Iteration 124/1000 | Loss: 0.00001382
Iteration 125/1000 | Loss: 0.00001382
Iteration 126/1000 | Loss: 0.00001382
Iteration 127/1000 | Loss: 0.00001382
Iteration 128/1000 | Loss: 0.00001382
Iteration 129/1000 | Loss: 0.00001381
Iteration 130/1000 | Loss: 0.00001381
Iteration 131/1000 | Loss: 0.00001380
Iteration 132/1000 | Loss: 0.00001380
Iteration 133/1000 | Loss: 0.00001380
Iteration 134/1000 | Loss: 0.00001380
Iteration 135/1000 | Loss: 0.00001379
Iteration 136/1000 | Loss: 0.00001379
Iteration 137/1000 | Loss: 0.00001379
Iteration 138/1000 | Loss: 0.00001379
Iteration 139/1000 | Loss: 0.00001379
Iteration 140/1000 | Loss: 0.00001379
Iteration 141/1000 | Loss: 0.00001378
Iteration 142/1000 | Loss: 0.00001378
Iteration 143/1000 | Loss: 0.00001378
Iteration 144/1000 | Loss: 0.00001378
Iteration 145/1000 | Loss: 0.00001378
Iteration 146/1000 | Loss: 0.00001378
Iteration 147/1000 | Loss: 0.00001378
Iteration 148/1000 | Loss: 0.00001378
Iteration 149/1000 | Loss: 0.00001378
Iteration 150/1000 | Loss: 0.00001378
Iteration 151/1000 | Loss: 0.00001378
Iteration 152/1000 | Loss: 0.00001378
Iteration 153/1000 | Loss: 0.00001378
Iteration 154/1000 | Loss: 0.00001377
Iteration 155/1000 | Loss: 0.00001377
Iteration 156/1000 | Loss: 0.00001377
Iteration 157/1000 | Loss: 0.00001377
Iteration 158/1000 | Loss: 0.00001377
Iteration 159/1000 | Loss: 0.00001377
Iteration 160/1000 | Loss: 0.00001377
Iteration 161/1000 | Loss: 0.00001377
Iteration 162/1000 | Loss: 0.00001377
Iteration 163/1000 | Loss: 0.00001377
Iteration 164/1000 | Loss: 0.00001377
Iteration 165/1000 | Loss: 0.00001377
Iteration 166/1000 | Loss: 0.00001377
Iteration 167/1000 | Loss: 0.00001377
Iteration 168/1000 | Loss: 0.00001377
Iteration 169/1000 | Loss: 0.00001377
Iteration 170/1000 | Loss: 0.00001377
Iteration 171/1000 | Loss: 0.00001377
Iteration 172/1000 | Loss: 0.00001377
Iteration 173/1000 | Loss: 0.00001377
Iteration 174/1000 | Loss: 0.00001377
Iteration 175/1000 | Loss: 0.00001377
Iteration 176/1000 | Loss: 0.00001376
Iteration 177/1000 | Loss: 0.00001376
Iteration 178/1000 | Loss: 0.00001376
Iteration 179/1000 | Loss: 0.00001376
Iteration 180/1000 | Loss: 0.00001376
Iteration 181/1000 | Loss: 0.00001376
Iteration 182/1000 | Loss: 0.00001375
Iteration 183/1000 | Loss: 0.00001375
Iteration 184/1000 | Loss: 0.00001375
Iteration 185/1000 | Loss: 0.00001375
Iteration 186/1000 | Loss: 0.00001375
Iteration 187/1000 | Loss: 0.00001375
Iteration 188/1000 | Loss: 0.00001375
Iteration 189/1000 | Loss: 0.00001375
Iteration 190/1000 | Loss: 0.00001375
Iteration 191/1000 | Loss: 0.00001375
Iteration 192/1000 | Loss: 0.00001375
Iteration 193/1000 | Loss: 0.00004804
Iteration 194/1000 | Loss: 0.00001382
Iteration 195/1000 | Loss: 0.00001377
Iteration 196/1000 | Loss: 0.00001377
Iteration 197/1000 | Loss: 0.00001377
Iteration 198/1000 | Loss: 0.00001377
Iteration 199/1000 | Loss: 0.00001376
Iteration 200/1000 | Loss: 0.00001376
Iteration 201/1000 | Loss: 0.00001376
Iteration 202/1000 | Loss: 0.00001376
Iteration 203/1000 | Loss: 0.00001375
Iteration 204/1000 | Loss: 0.00001375
Iteration 205/1000 | Loss: 0.00001375
Iteration 206/1000 | Loss: 0.00001375
Iteration 207/1000 | Loss: 0.00001375
Iteration 208/1000 | Loss: 0.00001375
Iteration 209/1000 | Loss: 0.00001374
Iteration 210/1000 | Loss: 0.00001374
Iteration 211/1000 | Loss: 0.00001374
Iteration 212/1000 | Loss: 0.00001374
Iteration 213/1000 | Loss: 0.00001374
Iteration 214/1000 | Loss: 0.00001374
Iteration 215/1000 | Loss: 0.00001373
Iteration 216/1000 | Loss: 0.00001373
Iteration 217/1000 | Loss: 0.00001373
Iteration 218/1000 | Loss: 0.00001373
Iteration 219/1000 | Loss: 0.00001373
Iteration 220/1000 | Loss: 0.00001373
Iteration 221/1000 | Loss: 0.00001373
Iteration 222/1000 | Loss: 0.00001373
Iteration 223/1000 | Loss: 0.00001373
Iteration 224/1000 | Loss: 0.00001373
Iteration 225/1000 | Loss: 0.00001373
Iteration 226/1000 | Loss: 0.00001373
Iteration 227/1000 | Loss: 0.00001373
Iteration 228/1000 | Loss: 0.00001373
Iteration 229/1000 | Loss: 0.00001373
Iteration 230/1000 | Loss: 0.00001373
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 230. Stopping optimization.
Last 5 losses: [1.3731430954067037e-05, 1.3731430954067037e-05, 1.3731430954067037e-05, 1.3731430954067037e-05, 1.3731430954067037e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3731430954067037e-05

Optimization complete. Final v2v error: 3.121727705001831 mm

Highest mean error: 8.829814910888672 mm for frame 141

Lowest mean error: 2.898902654647827 mm for frame 136

Saving results

Total time: 134.41919708251953
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_013/1051/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_013/1051.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_013/1051
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01044913
Iteration 2/25 | Loss: 0.00376579
Iteration 3/25 | Loss: 0.00266069
Iteration 4/25 | Loss: 0.00213738
Iteration 5/25 | Loss: 0.00177662
Iteration 6/25 | Loss: 0.00169441
Iteration 7/25 | Loss: 0.00156080
Iteration 8/25 | Loss: 0.00152677
Iteration 9/25 | Loss: 0.00151166
Iteration 10/25 | Loss: 0.00148022
Iteration 11/25 | Loss: 0.00146024
Iteration 12/25 | Loss: 0.00145613
Iteration 13/25 | Loss: 0.00145091
Iteration 14/25 | Loss: 0.00145421
Iteration 15/25 | Loss: 0.00144550
Iteration 16/25 | Loss: 0.00143814
Iteration 17/25 | Loss: 0.00143344
Iteration 18/25 | Loss: 0.00143311
Iteration 19/25 | Loss: 0.00142565
Iteration 20/25 | Loss: 0.00142178
Iteration 21/25 | Loss: 0.00141412
Iteration 22/25 | Loss: 0.00141143
Iteration 23/25 | Loss: 0.00141488
Iteration 24/25 | Loss: 0.00140978
Iteration 25/25 | Loss: 0.00141034

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.26004779
Iteration 2/25 | Loss: 0.00382106
Iteration 3/25 | Loss: 0.00314285
Iteration 4/25 | Loss: 0.00314285
Iteration 5/25 | Loss: 0.00314285
Iteration 6/25 | Loss: 0.00314284
Iteration 7/25 | Loss: 0.00314284
Iteration 8/25 | Loss: 0.00314284
Iteration 9/25 | Loss: 0.00314284
Iteration 10/25 | Loss: 0.00314284
Iteration 11/25 | Loss: 0.00314284
Iteration 12/25 | Loss: 0.00314284
Iteration 13/25 | Loss: 0.00314284
Iteration 14/25 | Loss: 0.00314284
Iteration 15/25 | Loss: 0.00314284
Iteration 16/25 | Loss: 0.00314284
Iteration 17/25 | Loss: 0.00314284
Iteration 18/25 | Loss: 0.00314284
Iteration 19/25 | Loss: 0.00314284
Iteration 20/25 | Loss: 0.00314284
Iteration 21/25 | Loss: 0.00314284
Iteration 22/25 | Loss: 0.00314284
Iteration 23/25 | Loss: 0.00314284
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0031428418587893248, 0.0031428418587893248, 0.0031428418587893248, 0.0031428418587893248, 0.0031428418587893248]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0031428418587893248

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00314284
Iteration 2/1000 | Loss: 0.01114269
Iteration 3/1000 | Loss: 0.00347059
Iteration 4/1000 | Loss: 0.00044745
Iteration 5/1000 | Loss: 0.00038818
Iteration 6/1000 | Loss: 0.00127005
Iteration 7/1000 | Loss: 0.00154930
Iteration 8/1000 | Loss: 0.00208060
Iteration 9/1000 | Loss: 0.00038142
Iteration 10/1000 | Loss: 0.00079988
Iteration 11/1000 | Loss: 0.00023137
Iteration 12/1000 | Loss: 0.00058816
Iteration 13/1000 | Loss: 0.00038250
Iteration 14/1000 | Loss: 0.00014873
Iteration 15/1000 | Loss: 0.00037816
Iteration 16/1000 | Loss: 0.00152653
Iteration 17/1000 | Loss: 0.00014490
Iteration 18/1000 | Loss: 0.00065685
Iteration 19/1000 | Loss: 0.00038170
Iteration 20/1000 | Loss: 0.00014002
Iteration 21/1000 | Loss: 0.00105295
Iteration 22/1000 | Loss: 0.00065365
Iteration 23/1000 | Loss: 0.00008937
Iteration 24/1000 | Loss: 0.00014582
Iteration 25/1000 | Loss: 0.00076814
Iteration 26/1000 | Loss: 0.00017379
Iteration 27/1000 | Loss: 0.00034979
Iteration 28/1000 | Loss: 0.00010274
Iteration 29/1000 | Loss: 0.00060778
Iteration 30/1000 | Loss: 0.00468860
Iteration 31/1000 | Loss: 0.00215332
Iteration 32/1000 | Loss: 0.00186211
Iteration 33/1000 | Loss: 0.00091569
Iteration 34/1000 | Loss: 0.00070812
Iteration 35/1000 | Loss: 0.00115815
Iteration 36/1000 | Loss: 0.00198120
Iteration 37/1000 | Loss: 0.00297264
Iteration 38/1000 | Loss: 0.00286111
Iteration 39/1000 | Loss: 0.00354458
Iteration 40/1000 | Loss: 0.00291278
Iteration 41/1000 | Loss: 0.00034419
Iteration 42/1000 | Loss: 0.00013349
Iteration 43/1000 | Loss: 0.00013606
Iteration 44/1000 | Loss: 0.00006802
Iteration 45/1000 | Loss: 0.00008220
Iteration 46/1000 | Loss: 0.00013176
Iteration 47/1000 | Loss: 0.00033531
Iteration 48/1000 | Loss: 0.00024130
Iteration 49/1000 | Loss: 0.00027749
Iteration 50/1000 | Loss: 0.00003495
Iteration 51/1000 | Loss: 0.00009171
Iteration 52/1000 | Loss: 0.00073104
Iteration 53/1000 | Loss: 0.00047818
Iteration 54/1000 | Loss: 0.00061480
Iteration 55/1000 | Loss: 0.00043379
Iteration 56/1000 | Loss: 0.00006337
Iteration 57/1000 | Loss: 0.00003712
Iteration 58/1000 | Loss: 0.00003648
Iteration 59/1000 | Loss: 0.00003485
Iteration 60/1000 | Loss: 0.00002192
Iteration 61/1000 | Loss: 0.00002323
Iteration 62/1000 | Loss: 0.00067898
Iteration 63/1000 | Loss: 0.00003678
Iteration 64/1000 | Loss: 0.00003415
Iteration 65/1000 | Loss: 0.00002402
Iteration 66/1000 | Loss: 0.00001748
Iteration 67/1000 | Loss: 0.00002538
Iteration 68/1000 | Loss: 0.00001666
Iteration 69/1000 | Loss: 0.00001621
Iteration 70/1000 | Loss: 0.00001589
Iteration 71/1000 | Loss: 0.00005275
Iteration 72/1000 | Loss: 0.00011954
Iteration 73/1000 | Loss: 0.00001842
Iteration 74/1000 | Loss: 0.00003189
Iteration 75/1000 | Loss: 0.00004426
Iteration 76/1000 | Loss: 0.00001547
Iteration 77/1000 | Loss: 0.00001535
Iteration 78/1000 | Loss: 0.00001533
Iteration 79/1000 | Loss: 0.00001533
Iteration 80/1000 | Loss: 0.00001533
Iteration 81/1000 | Loss: 0.00001532
Iteration 82/1000 | Loss: 0.00001531
Iteration 83/1000 | Loss: 0.00001531
Iteration 84/1000 | Loss: 0.00001530
Iteration 85/1000 | Loss: 0.00001530
Iteration 86/1000 | Loss: 0.00001529
Iteration 87/1000 | Loss: 0.00001529
Iteration 88/1000 | Loss: 0.00001529
Iteration 89/1000 | Loss: 0.00001528
Iteration 90/1000 | Loss: 0.00001528
Iteration 91/1000 | Loss: 0.00001528
Iteration 92/1000 | Loss: 0.00001527
Iteration 93/1000 | Loss: 0.00001527
Iteration 94/1000 | Loss: 0.00001527
Iteration 95/1000 | Loss: 0.00001526
Iteration 96/1000 | Loss: 0.00001526
Iteration 97/1000 | Loss: 0.00001526
Iteration 98/1000 | Loss: 0.00005254
Iteration 99/1000 | Loss: 0.00006076
Iteration 100/1000 | Loss: 0.00001566
Iteration 101/1000 | Loss: 0.00003320
Iteration 102/1000 | Loss: 0.00006070
Iteration 103/1000 | Loss: 0.00001681
Iteration 104/1000 | Loss: 0.00001820
Iteration 105/1000 | Loss: 0.00001951
Iteration 106/1000 | Loss: 0.00001966
Iteration 107/1000 | Loss: 0.00001516
Iteration 108/1000 | Loss: 0.00001515
Iteration 109/1000 | Loss: 0.00001515
Iteration 110/1000 | Loss: 0.00001515
Iteration 111/1000 | Loss: 0.00001515
Iteration 112/1000 | Loss: 0.00001515
Iteration 113/1000 | Loss: 0.00001515
Iteration 114/1000 | Loss: 0.00001515
Iteration 115/1000 | Loss: 0.00001515
Iteration 116/1000 | Loss: 0.00001515
Iteration 117/1000 | Loss: 0.00001515
Iteration 118/1000 | Loss: 0.00001514
Iteration 119/1000 | Loss: 0.00001514
Iteration 120/1000 | Loss: 0.00001514
Iteration 121/1000 | Loss: 0.00001514
Iteration 122/1000 | Loss: 0.00001514
Iteration 123/1000 | Loss: 0.00001513
Iteration 124/1000 | Loss: 0.00001513
Iteration 125/1000 | Loss: 0.00001512
Iteration 126/1000 | Loss: 0.00001512
Iteration 127/1000 | Loss: 0.00001512
Iteration 128/1000 | Loss: 0.00001512
Iteration 129/1000 | Loss: 0.00001512
Iteration 130/1000 | Loss: 0.00001512
Iteration 131/1000 | Loss: 0.00001512
Iteration 132/1000 | Loss: 0.00001511
Iteration 133/1000 | Loss: 0.00001511
Iteration 134/1000 | Loss: 0.00001511
Iteration 135/1000 | Loss: 0.00001511
Iteration 136/1000 | Loss: 0.00001511
Iteration 137/1000 | Loss: 0.00001511
Iteration 138/1000 | Loss: 0.00001511
Iteration 139/1000 | Loss: 0.00001511
Iteration 140/1000 | Loss: 0.00001511
Iteration 141/1000 | Loss: 0.00001511
Iteration 142/1000 | Loss: 0.00001511
Iteration 143/1000 | Loss: 0.00001511
Iteration 144/1000 | Loss: 0.00001511
Iteration 145/1000 | Loss: 0.00001511
Iteration 146/1000 | Loss: 0.00004402
Iteration 147/1000 | Loss: 0.00001678
Iteration 148/1000 | Loss: 0.00001514
Iteration 149/1000 | Loss: 0.00001514
Iteration 150/1000 | Loss: 0.00001514
Iteration 151/1000 | Loss: 0.00001514
Iteration 152/1000 | Loss: 0.00001514
Iteration 153/1000 | Loss: 0.00001513
Iteration 154/1000 | Loss: 0.00001513
Iteration 155/1000 | Loss: 0.00001513
Iteration 156/1000 | Loss: 0.00001513
Iteration 157/1000 | Loss: 0.00001513
Iteration 158/1000 | Loss: 0.00001513
Iteration 159/1000 | Loss: 0.00001513
Iteration 160/1000 | Loss: 0.00001512
Iteration 161/1000 | Loss: 0.00001512
Iteration 162/1000 | Loss: 0.00001511
Iteration 163/1000 | Loss: 0.00001511
Iteration 164/1000 | Loss: 0.00001510
Iteration 165/1000 | Loss: 0.00001510
Iteration 166/1000 | Loss: 0.00001510
Iteration 167/1000 | Loss: 0.00001509
Iteration 168/1000 | Loss: 0.00001509
Iteration 169/1000 | Loss: 0.00001509
Iteration 170/1000 | Loss: 0.00001509
Iteration 171/1000 | Loss: 0.00001508
Iteration 172/1000 | Loss: 0.00001508
Iteration 173/1000 | Loss: 0.00001508
Iteration 174/1000 | Loss: 0.00001508
Iteration 175/1000 | Loss: 0.00001508
Iteration 176/1000 | Loss: 0.00001508
Iteration 177/1000 | Loss: 0.00001507
Iteration 178/1000 | Loss: 0.00001507
Iteration 179/1000 | Loss: 0.00001507
Iteration 180/1000 | Loss: 0.00001507
Iteration 181/1000 | Loss: 0.00001507
Iteration 182/1000 | Loss: 0.00001507
Iteration 183/1000 | Loss: 0.00001506
Iteration 184/1000 | Loss: 0.00001506
Iteration 185/1000 | Loss: 0.00001506
Iteration 186/1000 | Loss: 0.00001506
Iteration 187/1000 | Loss: 0.00001506
Iteration 188/1000 | Loss: 0.00001506
Iteration 189/1000 | Loss: 0.00001506
Iteration 190/1000 | Loss: 0.00001506
Iteration 191/1000 | Loss: 0.00001506
Iteration 192/1000 | Loss: 0.00001506
Iteration 193/1000 | Loss: 0.00001506
Iteration 194/1000 | Loss: 0.00001506
Iteration 195/1000 | Loss: 0.00001506
Iteration 196/1000 | Loss: 0.00001506
Iteration 197/1000 | Loss: 0.00001506
Iteration 198/1000 | Loss: 0.00001506
Iteration 199/1000 | Loss: 0.00001506
Iteration 200/1000 | Loss: 0.00001506
Iteration 201/1000 | Loss: 0.00001506
Iteration 202/1000 | Loss: 0.00001506
Iteration 203/1000 | Loss: 0.00001506
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 203. Stopping optimization.
Last 5 losses: [1.5056726624607109e-05, 1.5056726624607109e-05, 1.5056726624607109e-05, 1.5056726624607109e-05, 1.5056726624607109e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5056726624607109e-05

Optimization complete. Final v2v error: 3.13989520072937 mm

Highest mean error: 4.984906196594238 mm for frame 62

Lowest mean error: 2.4812519550323486 mm for frame 93

Saving results

Total time: 190.35568070411682
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_36_nl_5493/0023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_nl_5493/0023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_nl_5493/0023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01078550
Iteration 2/25 | Loss: 0.00277338
Iteration 3/25 | Loss: 0.00188608
Iteration 4/25 | Loss: 0.00176085
Iteration 5/25 | Loss: 0.00169391
Iteration 6/25 | Loss: 0.00153692
Iteration 7/25 | Loss: 0.00153333
Iteration 8/25 | Loss: 0.00151270
Iteration 9/25 | Loss: 0.00144390
Iteration 10/25 | Loss: 0.00142639
Iteration 11/25 | Loss: 0.00141739
Iteration 12/25 | Loss: 0.00140725
Iteration 13/25 | Loss: 0.00138753
Iteration 14/25 | Loss: 0.00137802
Iteration 15/25 | Loss: 0.00137180
Iteration 16/25 | Loss: 0.00136351
Iteration 17/25 | Loss: 0.00135858
Iteration 18/25 | Loss: 0.00136405
Iteration 19/25 | Loss: 0.00135522
Iteration 20/25 | Loss: 0.00135287
Iteration 21/25 | Loss: 0.00135169
Iteration 22/25 | Loss: 0.00135207
Iteration 23/25 | Loss: 0.00134939
Iteration 24/25 | Loss: 0.00134894
Iteration 25/25 | Loss: 0.00134875

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.73418164
Iteration 2/25 | Loss: 0.00327273
Iteration 3/25 | Loss: 0.00327273
Iteration 4/25 | Loss: 0.00327273
Iteration 5/25 | Loss: 0.00327272
Iteration 6/25 | Loss: 0.00327272
Iteration 7/25 | Loss: 0.00327272
Iteration 8/25 | Loss: 0.00327272
Iteration 9/25 | Loss: 0.00327272
Iteration 10/25 | Loss: 0.00327272
Iteration 11/25 | Loss: 0.00327272
Iteration 12/25 | Loss: 0.00327272
Iteration 13/25 | Loss: 0.00327272
Iteration 14/25 | Loss: 0.00327272
Iteration 15/25 | Loss: 0.00327272
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.003272722940891981, 0.003272722940891981, 0.003272722940891981, 0.003272722940891981, 0.003272722940891981]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.003272722940891981

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00327272
Iteration 2/1000 | Loss: 0.00048331
Iteration 3/1000 | Loss: 0.00027731
Iteration 4/1000 | Loss: 0.00023066
Iteration 5/1000 | Loss: 0.00054592
Iteration 6/1000 | Loss: 0.00020859
Iteration 7/1000 | Loss: 0.00058622
Iteration 8/1000 | Loss: 0.00018398
Iteration 9/1000 | Loss: 0.00049392
Iteration 10/1000 | Loss: 0.00182285
Iteration 11/1000 | Loss: 0.00032740
Iteration 12/1000 | Loss: 0.00055643
Iteration 13/1000 | Loss: 0.00017418
Iteration 14/1000 | Loss: 0.00078370
Iteration 15/1000 | Loss: 0.00018721
Iteration 16/1000 | Loss: 0.00015639
Iteration 17/1000 | Loss: 0.00014252
Iteration 18/1000 | Loss: 0.00012815
Iteration 19/1000 | Loss: 0.00012097
Iteration 20/1000 | Loss: 0.00011640
Iteration 21/1000 | Loss: 0.00011327
Iteration 22/1000 | Loss: 0.00011134
Iteration 23/1000 | Loss: 0.00010955
Iteration 24/1000 | Loss: 0.00010824
Iteration 25/1000 | Loss: 0.00010739
Iteration 26/1000 | Loss: 0.00018870
Iteration 27/1000 | Loss: 0.00021078
Iteration 28/1000 | Loss: 0.00012861
Iteration 29/1000 | Loss: 0.00011135
Iteration 30/1000 | Loss: 0.00010685
Iteration 31/1000 | Loss: 0.00026006
Iteration 32/1000 | Loss: 0.00013715
Iteration 33/1000 | Loss: 0.00010878
Iteration 34/1000 | Loss: 0.00010267
Iteration 35/1000 | Loss: 0.00009920
Iteration 36/1000 | Loss: 0.00009678
Iteration 37/1000 | Loss: 0.00009569
Iteration 38/1000 | Loss: 0.00009479
Iteration 39/1000 | Loss: 0.00009717
Iteration 40/1000 | Loss: 0.00009527
Iteration 41/1000 | Loss: 0.00009372
Iteration 42/1000 | Loss: 0.00009349
Iteration 43/1000 | Loss: 0.00009333
Iteration 44/1000 | Loss: 0.00009326
Iteration 45/1000 | Loss: 0.00019366
Iteration 46/1000 | Loss: 0.00010344
Iteration 47/1000 | Loss: 0.00009597
Iteration 48/1000 | Loss: 0.00009385
Iteration 49/1000 | Loss: 0.00009221
Iteration 50/1000 | Loss: 0.00009119
Iteration 51/1000 | Loss: 0.00009058
Iteration 52/1000 | Loss: 0.00009019
Iteration 53/1000 | Loss: 0.00008999
Iteration 54/1000 | Loss: 0.00008980
Iteration 55/1000 | Loss: 0.00008976
Iteration 56/1000 | Loss: 0.00008972
Iteration 57/1000 | Loss: 0.00008967
Iteration 58/1000 | Loss: 0.00008967
Iteration 59/1000 | Loss: 0.00008967
Iteration 60/1000 | Loss: 0.00008967
Iteration 61/1000 | Loss: 0.00008967
Iteration 62/1000 | Loss: 0.00008967
Iteration 63/1000 | Loss: 0.00008967
Iteration 64/1000 | Loss: 0.00008967
Iteration 65/1000 | Loss: 0.00008967
Iteration 66/1000 | Loss: 0.00008967
Iteration 67/1000 | Loss: 0.00008966
Iteration 68/1000 | Loss: 0.00008966
Iteration 69/1000 | Loss: 0.00008966
Iteration 70/1000 | Loss: 0.00008966
Iteration 71/1000 | Loss: 0.00008965
Iteration 72/1000 | Loss: 0.00008965
Iteration 73/1000 | Loss: 0.00008965
Iteration 74/1000 | Loss: 0.00008965
Iteration 75/1000 | Loss: 0.00008965
Iteration 76/1000 | Loss: 0.00008965
Iteration 77/1000 | Loss: 0.00008964
Iteration 78/1000 | Loss: 0.00008964
Iteration 79/1000 | Loss: 0.00008964
Iteration 80/1000 | Loss: 0.00008963
Iteration 81/1000 | Loss: 0.00008963
Iteration 82/1000 | Loss: 0.00008963
Iteration 83/1000 | Loss: 0.00008963
Iteration 84/1000 | Loss: 0.00008963
Iteration 85/1000 | Loss: 0.00008963
Iteration 86/1000 | Loss: 0.00008963
Iteration 87/1000 | Loss: 0.00008962
Iteration 88/1000 | Loss: 0.00008962
Iteration 89/1000 | Loss: 0.00008962
Iteration 90/1000 | Loss: 0.00008961
Iteration 91/1000 | Loss: 0.00008961
Iteration 92/1000 | Loss: 0.00008961
Iteration 93/1000 | Loss: 0.00008961
Iteration 94/1000 | Loss: 0.00008961
Iteration 95/1000 | Loss: 0.00008961
Iteration 96/1000 | Loss: 0.00008961
Iteration 97/1000 | Loss: 0.00008960
Iteration 98/1000 | Loss: 0.00008960
Iteration 99/1000 | Loss: 0.00008960
Iteration 100/1000 | Loss: 0.00008960
Iteration 101/1000 | Loss: 0.00008959
Iteration 102/1000 | Loss: 0.00008959
Iteration 103/1000 | Loss: 0.00008959
Iteration 104/1000 | Loss: 0.00008959
Iteration 105/1000 | Loss: 0.00008958
Iteration 106/1000 | Loss: 0.00008958
Iteration 107/1000 | Loss: 0.00008958
Iteration 108/1000 | Loss: 0.00008958
Iteration 109/1000 | Loss: 0.00008958
Iteration 110/1000 | Loss: 0.00008958
Iteration 111/1000 | Loss: 0.00008958
Iteration 112/1000 | Loss: 0.00008958
Iteration 113/1000 | Loss: 0.00008957
Iteration 114/1000 | Loss: 0.00008957
Iteration 115/1000 | Loss: 0.00008957
Iteration 116/1000 | Loss: 0.00008957
Iteration 117/1000 | Loss: 0.00008957
Iteration 118/1000 | Loss: 0.00008957
Iteration 119/1000 | Loss: 0.00008957
Iteration 120/1000 | Loss: 0.00008957
Iteration 121/1000 | Loss: 0.00008957
Iteration 122/1000 | Loss: 0.00008957
Iteration 123/1000 | Loss: 0.00008957
Iteration 124/1000 | Loss: 0.00008957
Iteration 125/1000 | Loss: 0.00008957
Iteration 126/1000 | Loss: 0.00008956
Iteration 127/1000 | Loss: 0.00008956
Iteration 128/1000 | Loss: 0.00008956
Iteration 129/1000 | Loss: 0.00008955
Iteration 130/1000 | Loss: 0.00008955
Iteration 131/1000 | Loss: 0.00008955
Iteration 132/1000 | Loss: 0.00008955
Iteration 133/1000 | Loss: 0.00008955
Iteration 134/1000 | Loss: 0.00008955
Iteration 135/1000 | Loss: 0.00008954
Iteration 136/1000 | Loss: 0.00008954
Iteration 137/1000 | Loss: 0.00008954
Iteration 138/1000 | Loss: 0.00008953
Iteration 139/1000 | Loss: 0.00008953
Iteration 140/1000 | Loss: 0.00008953
Iteration 141/1000 | Loss: 0.00008952
Iteration 142/1000 | Loss: 0.00008952
Iteration 143/1000 | Loss: 0.00008951
Iteration 144/1000 | Loss: 0.00008951
Iteration 145/1000 | Loss: 0.00008951
Iteration 146/1000 | Loss: 0.00008951
Iteration 147/1000 | Loss: 0.00008951
Iteration 148/1000 | Loss: 0.00008951
Iteration 149/1000 | Loss: 0.00008951
Iteration 150/1000 | Loss: 0.00008951
Iteration 151/1000 | Loss: 0.00008950
Iteration 152/1000 | Loss: 0.00008950
Iteration 153/1000 | Loss: 0.00008950
Iteration 154/1000 | Loss: 0.00008950
Iteration 155/1000 | Loss: 0.00008950
Iteration 156/1000 | Loss: 0.00008950
Iteration 157/1000 | Loss: 0.00008950
Iteration 158/1000 | Loss: 0.00008949
Iteration 159/1000 | Loss: 0.00008949
Iteration 160/1000 | Loss: 0.00008949
Iteration 161/1000 | Loss: 0.00008949
Iteration 162/1000 | Loss: 0.00008948
Iteration 163/1000 | Loss: 0.00008948
Iteration 164/1000 | Loss: 0.00008948
Iteration 165/1000 | Loss: 0.00008948
Iteration 166/1000 | Loss: 0.00008948
Iteration 167/1000 | Loss: 0.00008948
Iteration 168/1000 | Loss: 0.00008948
Iteration 169/1000 | Loss: 0.00008948
Iteration 170/1000 | Loss: 0.00008948
Iteration 171/1000 | Loss: 0.00008948
Iteration 172/1000 | Loss: 0.00008948
Iteration 173/1000 | Loss: 0.00008948
Iteration 174/1000 | Loss: 0.00008947
Iteration 175/1000 | Loss: 0.00008947
Iteration 176/1000 | Loss: 0.00008947
Iteration 177/1000 | Loss: 0.00008947
Iteration 178/1000 | Loss: 0.00008947
Iteration 179/1000 | Loss: 0.00008947
Iteration 180/1000 | Loss: 0.00008947
Iteration 181/1000 | Loss: 0.00008947
Iteration 182/1000 | Loss: 0.00008947
Iteration 183/1000 | Loss: 0.00008947
Iteration 184/1000 | Loss: 0.00008947
Iteration 185/1000 | Loss: 0.00008947
Iteration 186/1000 | Loss: 0.00008947
Iteration 187/1000 | Loss: 0.00008946
Iteration 188/1000 | Loss: 0.00008946
Iteration 189/1000 | Loss: 0.00008946
Iteration 190/1000 | Loss: 0.00008946
Iteration 191/1000 | Loss: 0.00008946
Iteration 192/1000 | Loss: 0.00008946
Iteration 193/1000 | Loss: 0.00008946
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 193. Stopping optimization.
Last 5 losses: [8.946442540036514e-05, 8.946442540036514e-05, 8.946442540036514e-05, 8.946442540036514e-05, 8.946442540036514e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.946442540036514e-05

Optimization complete. Final v2v error: 4.877840042114258 mm

Highest mean error: 12.311466217041016 mm for frame 51

Lowest mean error: 2.783207416534424 mm for frame 4

Saving results

Total time: 129.74475717544556
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_36_nl_5493/0022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_nl_5493/0022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_nl_5493/0022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00906275
Iteration 2/25 | Loss: 0.00164391
Iteration 3/25 | Loss: 0.00130700
Iteration 4/25 | Loss: 0.00126859
Iteration 5/25 | Loss: 0.00126527
Iteration 6/25 | Loss: 0.00126512
Iteration 7/25 | Loss: 0.00126512
Iteration 8/25 | Loss: 0.00126512
Iteration 9/25 | Loss: 0.00126512
Iteration 10/25 | Loss: 0.00126512
Iteration 11/25 | Loss: 0.00126512
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012651169672608376, 0.0012651169672608376, 0.0012651169672608376, 0.0012651169672608376, 0.0012651169672608376]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012651169672608376

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.18464530
Iteration 2/25 | Loss: 0.00060486
Iteration 3/25 | Loss: 0.00060486
Iteration 4/25 | Loss: 0.00060486
Iteration 5/25 | Loss: 0.00060486
Iteration 6/25 | Loss: 0.00060486
Iteration 7/25 | Loss: 0.00060486
Iteration 8/25 | Loss: 0.00060486
Iteration 9/25 | Loss: 0.00060486
Iteration 10/25 | Loss: 0.00060486
Iteration 11/25 | Loss: 0.00060486
Iteration 12/25 | Loss: 0.00060486
Iteration 13/25 | Loss: 0.00060486
Iteration 14/25 | Loss: 0.00060486
Iteration 15/25 | Loss: 0.00060486
Iteration 16/25 | Loss: 0.00060486
Iteration 17/25 | Loss: 0.00060486
Iteration 18/25 | Loss: 0.00060486
Iteration 19/25 | Loss: 0.00060486
Iteration 20/25 | Loss: 0.00060486
Iteration 21/25 | Loss: 0.00060486
Iteration 22/25 | Loss: 0.00060486
Iteration 23/25 | Loss: 0.00060486
Iteration 24/25 | Loss: 0.00060486
Iteration 25/25 | Loss: 0.00060486

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00060486
Iteration 2/1000 | Loss: 0.00005450
Iteration 3/1000 | Loss: 0.00003561
Iteration 4/1000 | Loss: 0.00003130
Iteration 5/1000 | Loss: 0.00002966
Iteration 6/1000 | Loss: 0.00002889
Iteration 7/1000 | Loss: 0.00002841
Iteration 8/1000 | Loss: 0.00002808
Iteration 9/1000 | Loss: 0.00002779
Iteration 10/1000 | Loss: 0.00002755
Iteration 11/1000 | Loss: 0.00002745
Iteration 12/1000 | Loss: 0.00002743
Iteration 13/1000 | Loss: 0.00002742
Iteration 14/1000 | Loss: 0.00002739
Iteration 15/1000 | Loss: 0.00002737
Iteration 16/1000 | Loss: 0.00002723
Iteration 17/1000 | Loss: 0.00002716
Iteration 18/1000 | Loss: 0.00002711
Iteration 19/1000 | Loss: 0.00002711
Iteration 20/1000 | Loss: 0.00002711
Iteration 21/1000 | Loss: 0.00002711
Iteration 22/1000 | Loss: 0.00002709
Iteration 23/1000 | Loss: 0.00002709
Iteration 24/1000 | Loss: 0.00002709
Iteration 25/1000 | Loss: 0.00002709
Iteration 26/1000 | Loss: 0.00002709
Iteration 27/1000 | Loss: 0.00002705
Iteration 28/1000 | Loss: 0.00002704
Iteration 29/1000 | Loss: 0.00002704
Iteration 30/1000 | Loss: 0.00002703
Iteration 31/1000 | Loss: 0.00002703
Iteration 32/1000 | Loss: 0.00002702
Iteration 33/1000 | Loss: 0.00002701
Iteration 34/1000 | Loss: 0.00002701
Iteration 35/1000 | Loss: 0.00002700
Iteration 36/1000 | Loss: 0.00002699
Iteration 37/1000 | Loss: 0.00002699
Iteration 38/1000 | Loss: 0.00002698
Iteration 39/1000 | Loss: 0.00002698
Iteration 40/1000 | Loss: 0.00002698
Iteration 41/1000 | Loss: 0.00002698
Iteration 42/1000 | Loss: 0.00002698
Iteration 43/1000 | Loss: 0.00002698
Iteration 44/1000 | Loss: 0.00002698
Iteration 45/1000 | Loss: 0.00002698
Iteration 46/1000 | Loss: 0.00002698
Iteration 47/1000 | Loss: 0.00002697
Iteration 48/1000 | Loss: 0.00002697
Iteration 49/1000 | Loss: 0.00002697
Iteration 50/1000 | Loss: 0.00002697
Iteration 51/1000 | Loss: 0.00002697
Iteration 52/1000 | Loss: 0.00002697
Iteration 53/1000 | Loss: 0.00002697
Iteration 54/1000 | Loss: 0.00002697
Iteration 55/1000 | Loss: 0.00002697
Iteration 56/1000 | Loss: 0.00002697
Iteration 57/1000 | Loss: 0.00002695
Iteration 58/1000 | Loss: 0.00002694
Iteration 59/1000 | Loss: 0.00002693
Iteration 60/1000 | Loss: 0.00002693
Iteration 61/1000 | Loss: 0.00002693
Iteration 62/1000 | Loss: 0.00002693
Iteration 63/1000 | Loss: 0.00002693
Iteration 64/1000 | Loss: 0.00002693
Iteration 65/1000 | Loss: 0.00002693
Iteration 66/1000 | Loss: 0.00002693
Iteration 67/1000 | Loss: 0.00002693
Iteration 68/1000 | Loss: 0.00002693
Iteration 69/1000 | Loss: 0.00002693
Iteration 70/1000 | Loss: 0.00002692
Iteration 71/1000 | Loss: 0.00002692
Iteration 72/1000 | Loss: 0.00002691
Iteration 73/1000 | Loss: 0.00002691
Iteration 74/1000 | Loss: 0.00002691
Iteration 75/1000 | Loss: 0.00002691
Iteration 76/1000 | Loss: 0.00002691
Iteration 77/1000 | Loss: 0.00002691
Iteration 78/1000 | Loss: 0.00002691
Iteration 79/1000 | Loss: 0.00002691
Iteration 80/1000 | Loss: 0.00002690
Iteration 81/1000 | Loss: 0.00002690
Iteration 82/1000 | Loss: 0.00002690
Iteration 83/1000 | Loss: 0.00002690
Iteration 84/1000 | Loss: 0.00002690
Iteration 85/1000 | Loss: 0.00002690
Iteration 86/1000 | Loss: 0.00002690
Iteration 87/1000 | Loss: 0.00002690
Iteration 88/1000 | Loss: 0.00002690
Iteration 89/1000 | Loss: 0.00002690
Iteration 90/1000 | Loss: 0.00002689
Iteration 91/1000 | Loss: 0.00002689
Iteration 92/1000 | Loss: 0.00002689
Iteration 93/1000 | Loss: 0.00002688
Iteration 94/1000 | Loss: 0.00002688
Iteration 95/1000 | Loss: 0.00002688
Iteration 96/1000 | Loss: 0.00002688
Iteration 97/1000 | Loss: 0.00002688
Iteration 98/1000 | Loss: 0.00002688
Iteration 99/1000 | Loss: 0.00002688
Iteration 100/1000 | Loss: 0.00002688
Iteration 101/1000 | Loss: 0.00002688
Iteration 102/1000 | Loss: 0.00002688
Iteration 103/1000 | Loss: 0.00002687
Iteration 104/1000 | Loss: 0.00002687
Iteration 105/1000 | Loss: 0.00002687
Iteration 106/1000 | Loss: 0.00002687
Iteration 107/1000 | Loss: 0.00002687
Iteration 108/1000 | Loss: 0.00002687
Iteration 109/1000 | Loss: 0.00002687
Iteration 110/1000 | Loss: 0.00002687
Iteration 111/1000 | Loss: 0.00002687
Iteration 112/1000 | Loss: 0.00002687
Iteration 113/1000 | Loss: 0.00002687
Iteration 114/1000 | Loss: 0.00002687
Iteration 115/1000 | Loss: 0.00002687
Iteration 116/1000 | Loss: 0.00002687
Iteration 117/1000 | Loss: 0.00002687
Iteration 118/1000 | Loss: 0.00002686
Iteration 119/1000 | Loss: 0.00002686
Iteration 120/1000 | Loss: 0.00002686
Iteration 121/1000 | Loss: 0.00002686
Iteration 122/1000 | Loss: 0.00002686
Iteration 123/1000 | Loss: 0.00002686
Iteration 124/1000 | Loss: 0.00002686
Iteration 125/1000 | Loss: 0.00002686
Iteration 126/1000 | Loss: 0.00002686
Iteration 127/1000 | Loss: 0.00002686
Iteration 128/1000 | Loss: 0.00002686
Iteration 129/1000 | Loss: 0.00002685
Iteration 130/1000 | Loss: 0.00002685
Iteration 131/1000 | Loss: 0.00002685
Iteration 132/1000 | Loss: 0.00002685
Iteration 133/1000 | Loss: 0.00002685
Iteration 134/1000 | Loss: 0.00002685
Iteration 135/1000 | Loss: 0.00002685
Iteration 136/1000 | Loss: 0.00002685
Iteration 137/1000 | Loss: 0.00002685
Iteration 138/1000 | Loss: 0.00002685
Iteration 139/1000 | Loss: 0.00002685
Iteration 140/1000 | Loss: 0.00002685
Iteration 141/1000 | Loss: 0.00002685
Iteration 142/1000 | Loss: 0.00002685
Iteration 143/1000 | Loss: 0.00002684
Iteration 144/1000 | Loss: 0.00002684
Iteration 145/1000 | Loss: 0.00002684
Iteration 146/1000 | Loss: 0.00002684
Iteration 147/1000 | Loss: 0.00002684
Iteration 148/1000 | Loss: 0.00002684
Iteration 149/1000 | Loss: 0.00002684
Iteration 150/1000 | Loss: 0.00002684
Iteration 151/1000 | Loss: 0.00002684
Iteration 152/1000 | Loss: 0.00002684
Iteration 153/1000 | Loss: 0.00002684
Iteration 154/1000 | Loss: 0.00002684
Iteration 155/1000 | Loss: 0.00002684
Iteration 156/1000 | Loss: 0.00002684
Iteration 157/1000 | Loss: 0.00002684
Iteration 158/1000 | Loss: 0.00002683
Iteration 159/1000 | Loss: 0.00002683
Iteration 160/1000 | Loss: 0.00002683
Iteration 161/1000 | Loss: 0.00002683
Iteration 162/1000 | Loss: 0.00002683
Iteration 163/1000 | Loss: 0.00002683
Iteration 164/1000 | Loss: 0.00002683
Iteration 165/1000 | Loss: 0.00002683
Iteration 166/1000 | Loss: 0.00002683
Iteration 167/1000 | Loss: 0.00002683
Iteration 168/1000 | Loss: 0.00002683
Iteration 169/1000 | Loss: 0.00002683
Iteration 170/1000 | Loss: 0.00002683
Iteration 171/1000 | Loss: 0.00002683
Iteration 172/1000 | Loss: 0.00002683
Iteration 173/1000 | Loss: 0.00002683
Iteration 174/1000 | Loss: 0.00002683
Iteration 175/1000 | Loss: 0.00002683
Iteration 176/1000 | Loss: 0.00002683
Iteration 177/1000 | Loss: 0.00002683
Iteration 178/1000 | Loss: 0.00002683
Iteration 179/1000 | Loss: 0.00002682
Iteration 180/1000 | Loss: 0.00002682
Iteration 181/1000 | Loss: 0.00002682
Iteration 182/1000 | Loss: 0.00002682
Iteration 183/1000 | Loss: 0.00002682
Iteration 184/1000 | Loss: 0.00002682
Iteration 185/1000 | Loss: 0.00002682
Iteration 186/1000 | Loss: 0.00002682
Iteration 187/1000 | Loss: 0.00002682
Iteration 188/1000 | Loss: 0.00002682
Iteration 189/1000 | Loss: 0.00002682
Iteration 190/1000 | Loss: 0.00002682
Iteration 191/1000 | Loss: 0.00002682
Iteration 192/1000 | Loss: 0.00002682
Iteration 193/1000 | Loss: 0.00002682
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 193. Stopping optimization.
Last 5 losses: [2.6824902306543663e-05, 2.6824902306543663e-05, 2.6824902306543663e-05, 2.6824902306543663e-05, 2.6824902306543663e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.6824902306543663e-05

Optimization complete. Final v2v error: 4.367724418640137 mm

Highest mean error: 4.996652126312256 mm for frame 142

Lowest mean error: 3.634183883666992 mm for frame 0

Saving results

Total time: 42.840277671813965
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_36_nl_5493/0021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_nl_5493/0021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_nl_5493/0021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01046063
Iteration 2/25 | Loss: 0.01046062
Iteration 3/25 | Loss: 0.00332410
Iteration 4/25 | Loss: 0.00199953
Iteration 5/25 | Loss: 0.00175740
Iteration 6/25 | Loss: 0.00167462
Iteration 7/25 | Loss: 0.00166654
Iteration 8/25 | Loss: 0.00160294
Iteration 9/25 | Loss: 0.00155769
Iteration 10/25 | Loss: 0.00152221
Iteration 11/25 | Loss: 0.00148975
Iteration 12/25 | Loss: 0.00147819
Iteration 13/25 | Loss: 0.00147186
Iteration 14/25 | Loss: 0.00145643
Iteration 15/25 | Loss: 0.00145597
Iteration 16/25 | Loss: 0.00145335
Iteration 17/25 | Loss: 0.00144717
Iteration 18/25 | Loss: 0.00144824
Iteration 19/25 | Loss: 0.00144677
Iteration 20/25 | Loss: 0.00144626
Iteration 21/25 | Loss: 0.00144690
Iteration 22/25 | Loss: 0.00144643
Iteration 23/25 | Loss: 0.00144600
Iteration 24/25 | Loss: 0.00144600
Iteration 25/25 | Loss: 0.00144592

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.21251631
Iteration 2/25 | Loss: 0.00425261
Iteration 3/25 | Loss: 0.00425261
Iteration 4/25 | Loss: 0.00425261
Iteration 5/25 | Loss: 0.00425261
Iteration 6/25 | Loss: 0.00425261
Iteration 7/25 | Loss: 0.00425261
Iteration 8/25 | Loss: 0.00425261
Iteration 9/25 | Loss: 0.00425261
Iteration 10/25 | Loss: 0.00425261
Iteration 11/25 | Loss: 0.00425261
Iteration 12/25 | Loss: 0.00425261
Iteration 13/25 | Loss: 0.00425261
Iteration 14/25 | Loss: 0.00425261
Iteration 15/25 | Loss: 0.00425261
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.004252610262483358, 0.004252610262483358, 0.004252610262483358, 0.004252610262483358, 0.004252610262483358]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.004252610262483358

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00425261
Iteration 2/1000 | Loss: 0.00054886
Iteration 3/1000 | Loss: 0.00040845
Iteration 4/1000 | Loss: 0.00033418
Iteration 5/1000 | Loss: 0.00034187
Iteration 6/1000 | Loss: 0.00030210
Iteration 7/1000 | Loss: 0.00025318
Iteration 8/1000 | Loss: 0.00023842
Iteration 9/1000 | Loss: 0.00022671
Iteration 10/1000 | Loss: 0.00021753
Iteration 11/1000 | Loss: 0.00021079
Iteration 12/1000 | Loss: 0.00038673
Iteration 13/1000 | Loss: 0.00038976
Iteration 14/1000 | Loss: 0.00145776
Iteration 15/1000 | Loss: 0.00875921
Iteration 16/1000 | Loss: 0.00591863
Iteration 17/1000 | Loss: 0.00071130
Iteration 18/1000 | Loss: 0.00142294
Iteration 19/1000 | Loss: 0.00028842
Iteration 20/1000 | Loss: 0.00022137
Iteration 21/1000 | Loss: 0.00015020
Iteration 22/1000 | Loss: 0.00017148
Iteration 23/1000 | Loss: 0.00019613
Iteration 24/1000 | Loss: 0.00007957
Iteration 25/1000 | Loss: 0.00005543
Iteration 26/1000 | Loss: 0.00004891
Iteration 27/1000 | Loss: 0.00003455
Iteration 28/1000 | Loss: 0.00002989
Iteration 29/1000 | Loss: 0.00002611
Iteration 30/1000 | Loss: 0.00002338
Iteration 31/1000 | Loss: 0.00002214
Iteration 32/1000 | Loss: 0.00001874
Iteration 33/1000 | Loss: 0.00001679
Iteration 34/1000 | Loss: 0.00001586
Iteration 35/1000 | Loss: 0.00001594
Iteration 36/1000 | Loss: 0.00001555
Iteration 37/1000 | Loss: 0.00001450
Iteration 38/1000 | Loss: 0.00001425
Iteration 39/1000 | Loss: 0.00001418
Iteration 40/1000 | Loss: 0.00001398
Iteration 41/1000 | Loss: 0.00001389
Iteration 42/1000 | Loss: 0.00001387
Iteration 43/1000 | Loss: 0.00001386
Iteration 44/1000 | Loss: 0.00001385
Iteration 45/1000 | Loss: 0.00001384
Iteration 46/1000 | Loss: 0.00001383
Iteration 47/1000 | Loss: 0.00001383
Iteration 48/1000 | Loss: 0.00001382
Iteration 49/1000 | Loss: 0.00001380
Iteration 50/1000 | Loss: 0.00001491
Iteration 51/1000 | Loss: 0.00001461
Iteration 52/1000 | Loss: 0.00001378
Iteration 53/1000 | Loss: 0.00001378
Iteration 54/1000 | Loss: 0.00001377
Iteration 55/1000 | Loss: 0.00001377
Iteration 56/1000 | Loss: 0.00001377
Iteration 57/1000 | Loss: 0.00001497
Iteration 58/1000 | Loss: 0.00001452
Iteration 59/1000 | Loss: 0.00001375
Iteration 60/1000 | Loss: 0.00001374
Iteration 61/1000 | Loss: 0.00001371
Iteration 62/1000 | Loss: 0.00001371
Iteration 63/1000 | Loss: 0.00001371
Iteration 64/1000 | Loss: 0.00001371
Iteration 65/1000 | Loss: 0.00001371
Iteration 66/1000 | Loss: 0.00001371
Iteration 67/1000 | Loss: 0.00001371
Iteration 68/1000 | Loss: 0.00001370
Iteration 69/1000 | Loss: 0.00001370
Iteration 70/1000 | Loss: 0.00001370
Iteration 71/1000 | Loss: 0.00001370
Iteration 72/1000 | Loss: 0.00001370
Iteration 73/1000 | Loss: 0.00001370
Iteration 74/1000 | Loss: 0.00001370
Iteration 75/1000 | Loss: 0.00001369
Iteration 76/1000 | Loss: 0.00001369
Iteration 77/1000 | Loss: 0.00001369
Iteration 78/1000 | Loss: 0.00001368
Iteration 79/1000 | Loss: 0.00001368
Iteration 80/1000 | Loss: 0.00001368
Iteration 81/1000 | Loss: 0.00001368
Iteration 82/1000 | Loss: 0.00001367
Iteration 83/1000 | Loss: 0.00001367
Iteration 84/1000 | Loss: 0.00001367
Iteration 85/1000 | Loss: 0.00001367
Iteration 86/1000 | Loss: 0.00001367
Iteration 87/1000 | Loss: 0.00001367
Iteration 88/1000 | Loss: 0.00001367
Iteration 89/1000 | Loss: 0.00001367
Iteration 90/1000 | Loss: 0.00001367
Iteration 91/1000 | Loss: 0.00001470
Iteration 92/1000 | Loss: 0.00001379
Iteration 93/1000 | Loss: 0.00001365
Iteration 94/1000 | Loss: 0.00001365
Iteration 95/1000 | Loss: 0.00001365
Iteration 96/1000 | Loss: 0.00001365
Iteration 97/1000 | Loss: 0.00001365
Iteration 98/1000 | Loss: 0.00001365
Iteration 99/1000 | Loss: 0.00001365
Iteration 100/1000 | Loss: 0.00001365
Iteration 101/1000 | Loss: 0.00001365
Iteration 102/1000 | Loss: 0.00001364
Iteration 103/1000 | Loss: 0.00001364
Iteration 104/1000 | Loss: 0.00001364
Iteration 105/1000 | Loss: 0.00001441
Iteration 106/1000 | Loss: 0.00001379
Iteration 107/1000 | Loss: 0.00001378
Iteration 108/1000 | Loss: 0.00001367
Iteration 109/1000 | Loss: 0.00001366
Iteration 110/1000 | Loss: 0.00001365
Iteration 111/1000 | Loss: 0.00001365
Iteration 112/1000 | Loss: 0.00001365
Iteration 113/1000 | Loss: 0.00001365
Iteration 114/1000 | Loss: 0.00001364
Iteration 115/1000 | Loss: 0.00001364
Iteration 116/1000 | Loss: 0.00001364
Iteration 117/1000 | Loss: 0.00001364
Iteration 118/1000 | Loss: 0.00001389
Iteration 119/1000 | Loss: 0.00001374
Iteration 120/1000 | Loss: 0.00001365
Iteration 121/1000 | Loss: 0.00001363
Iteration 122/1000 | Loss: 0.00001363
Iteration 123/1000 | Loss: 0.00001363
Iteration 124/1000 | Loss: 0.00001387
Iteration 125/1000 | Loss: 0.00001372
Iteration 126/1000 | Loss: 0.00001364
Iteration 127/1000 | Loss: 0.00001364
Iteration 128/1000 | Loss: 0.00001381
Iteration 129/1000 | Loss: 0.00001381
Iteration 130/1000 | Loss: 0.00001370
Iteration 131/1000 | Loss: 0.00001367
Iteration 132/1000 | Loss: 0.00001366
Iteration 133/1000 | Loss: 0.00001365
Iteration 134/1000 | Loss: 0.00001364
Iteration 135/1000 | Loss: 0.00001364
Iteration 136/1000 | Loss: 0.00001363
Iteration 137/1000 | Loss: 0.00001362
Iteration 138/1000 | Loss: 0.00001362
Iteration 139/1000 | Loss: 0.00001362
Iteration 140/1000 | Loss: 0.00001361
Iteration 141/1000 | Loss: 0.00001361
Iteration 142/1000 | Loss: 0.00001386
Iteration 143/1000 | Loss: 0.00001369
Iteration 144/1000 | Loss: 0.00001368
Iteration 145/1000 | Loss: 0.00001363
Iteration 146/1000 | Loss: 0.00001362
Iteration 147/1000 | Loss: 0.00001361
Iteration 148/1000 | Loss: 0.00001361
Iteration 149/1000 | Loss: 0.00001360
Iteration 150/1000 | Loss: 0.00001360
Iteration 151/1000 | Loss: 0.00001360
Iteration 152/1000 | Loss: 0.00001360
Iteration 153/1000 | Loss: 0.00001398
Iteration 154/1000 | Loss: 0.00001370
Iteration 155/1000 | Loss: 0.00001363
Iteration 156/1000 | Loss: 0.00001361
Iteration 157/1000 | Loss: 0.00001361
Iteration 158/1000 | Loss: 0.00001361
Iteration 159/1000 | Loss: 0.00001361
Iteration 160/1000 | Loss: 0.00001361
Iteration 161/1000 | Loss: 0.00001361
Iteration 162/1000 | Loss: 0.00001360
Iteration 163/1000 | Loss: 0.00001360
Iteration 164/1000 | Loss: 0.00001360
Iteration 165/1000 | Loss: 0.00001360
Iteration 166/1000 | Loss: 0.00001359
Iteration 167/1000 | Loss: 0.00001359
Iteration 168/1000 | Loss: 0.00001359
Iteration 169/1000 | Loss: 0.00001359
Iteration 170/1000 | Loss: 0.00001359
Iteration 171/1000 | Loss: 0.00001359
Iteration 172/1000 | Loss: 0.00001359
Iteration 173/1000 | Loss: 0.00001359
Iteration 174/1000 | Loss: 0.00001359
Iteration 175/1000 | Loss: 0.00001358
Iteration 176/1000 | Loss: 0.00001446
Iteration 177/1000 | Loss: 0.00001368
Iteration 178/1000 | Loss: 0.00001368
Iteration 179/1000 | Loss: 0.00001367
Iteration 180/1000 | Loss: 0.00001363
Iteration 181/1000 | Loss: 0.00001360
Iteration 182/1000 | Loss: 0.00001359
Iteration 183/1000 | Loss: 0.00001359
Iteration 184/1000 | Loss: 0.00001358
Iteration 185/1000 | Loss: 0.00001358
Iteration 186/1000 | Loss: 0.00001358
Iteration 187/1000 | Loss: 0.00001358
Iteration 188/1000 | Loss: 0.00001358
Iteration 189/1000 | Loss: 0.00001455
Iteration 190/1000 | Loss: 0.00001367
Iteration 191/1000 | Loss: 0.00001359
Iteration 192/1000 | Loss: 0.00001358
Iteration 193/1000 | Loss: 0.00001358
Iteration 194/1000 | Loss: 0.00001358
Iteration 195/1000 | Loss: 0.00001358
Iteration 196/1000 | Loss: 0.00001358
Iteration 197/1000 | Loss: 0.00001358
Iteration 198/1000 | Loss: 0.00001358
Iteration 199/1000 | Loss: 0.00001358
Iteration 200/1000 | Loss: 0.00001357
Iteration 201/1000 | Loss: 0.00001357
Iteration 202/1000 | Loss: 0.00001357
Iteration 203/1000 | Loss: 0.00001357
Iteration 204/1000 | Loss: 0.00001357
Iteration 205/1000 | Loss: 0.00001357
Iteration 206/1000 | Loss: 0.00001357
Iteration 207/1000 | Loss: 0.00001357
Iteration 208/1000 | Loss: 0.00001357
Iteration 209/1000 | Loss: 0.00001357
Iteration 210/1000 | Loss: 0.00001357
Iteration 211/1000 | Loss: 0.00001357
Iteration 212/1000 | Loss: 0.00001357
Iteration 213/1000 | Loss: 0.00001357
Iteration 214/1000 | Loss: 0.00001357
Iteration 215/1000 | Loss: 0.00001357
Iteration 216/1000 | Loss: 0.00001357
Iteration 217/1000 | Loss: 0.00001357
Iteration 218/1000 | Loss: 0.00001357
Iteration 219/1000 | Loss: 0.00001357
Iteration 220/1000 | Loss: 0.00001357
Iteration 221/1000 | Loss: 0.00001357
Iteration 222/1000 | Loss: 0.00001357
Iteration 223/1000 | Loss: 0.00001357
Iteration 224/1000 | Loss: 0.00001357
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 224. Stopping optimization.
Last 5 losses: [1.3566628695116378e-05, 1.3566628695116378e-05, 1.3566628695116378e-05, 1.3566628695116378e-05, 1.3566628695116378e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3566628695116378e-05

Optimization complete. Final v2v error: 2.933819532394409 mm

Highest mean error: 10.657634735107422 mm for frame 35

Lowest mean error: 2.602956771850586 mm for frame 195

Saving results

Total time: 146.3009388446808
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_36_nl_5493/0013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_nl_5493/0013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_nl_5493/0013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00432931
Iteration 2/25 | Loss: 0.00127207
Iteration 3/25 | Loss: 0.00119621
Iteration 4/25 | Loss: 0.00118295
Iteration 5/25 | Loss: 0.00117918
Iteration 6/25 | Loss: 0.00117863
Iteration 7/25 | Loss: 0.00117863
Iteration 8/25 | Loss: 0.00117863
Iteration 9/25 | Loss: 0.00117863
Iteration 10/25 | Loss: 0.00117863
Iteration 11/25 | Loss: 0.00117863
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011786262039095163, 0.0011786262039095163, 0.0011786262039095163, 0.0011786262039095163, 0.0011786262039095163]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011786262039095163

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.34577036
Iteration 2/25 | Loss: 0.00101400
Iteration 3/25 | Loss: 0.00101399
Iteration 4/25 | Loss: 0.00101399
Iteration 5/25 | Loss: 0.00101399
Iteration 6/25 | Loss: 0.00101399
Iteration 7/25 | Loss: 0.00101399
Iteration 8/25 | Loss: 0.00101399
Iteration 9/25 | Loss: 0.00101399
Iteration 10/25 | Loss: 0.00101399
Iteration 11/25 | Loss: 0.00101399
Iteration 12/25 | Loss: 0.00101399
Iteration 13/25 | Loss: 0.00101399
Iteration 14/25 | Loss: 0.00101399
Iteration 15/25 | Loss: 0.00101399
Iteration 16/25 | Loss: 0.00101399
Iteration 17/25 | Loss: 0.00101399
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0010139916557818651, 0.0010139916557818651, 0.0010139916557818651, 0.0010139916557818651, 0.0010139916557818651]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010139916557818651

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00101399
Iteration 2/1000 | Loss: 0.00005507
Iteration 3/1000 | Loss: 0.00002498
Iteration 4/1000 | Loss: 0.00002153
Iteration 5/1000 | Loss: 0.00001975
Iteration 6/1000 | Loss: 0.00001891
Iteration 7/1000 | Loss: 0.00001841
Iteration 8/1000 | Loss: 0.00001807
Iteration 9/1000 | Loss: 0.00001767
Iteration 10/1000 | Loss: 0.00001742
Iteration 11/1000 | Loss: 0.00001725
Iteration 12/1000 | Loss: 0.00001721
Iteration 13/1000 | Loss: 0.00001720
Iteration 14/1000 | Loss: 0.00001717
Iteration 15/1000 | Loss: 0.00001716
Iteration 16/1000 | Loss: 0.00001715
Iteration 17/1000 | Loss: 0.00001715
Iteration 18/1000 | Loss: 0.00001714
Iteration 19/1000 | Loss: 0.00001713
Iteration 20/1000 | Loss: 0.00001712
Iteration 21/1000 | Loss: 0.00001712
Iteration 22/1000 | Loss: 0.00001710
Iteration 23/1000 | Loss: 0.00001705
Iteration 24/1000 | Loss: 0.00001703
Iteration 25/1000 | Loss: 0.00001702
Iteration 26/1000 | Loss: 0.00001701
Iteration 27/1000 | Loss: 0.00001700
Iteration 28/1000 | Loss: 0.00001700
Iteration 29/1000 | Loss: 0.00001699
Iteration 30/1000 | Loss: 0.00001699
Iteration 31/1000 | Loss: 0.00001698
Iteration 32/1000 | Loss: 0.00001697
Iteration 33/1000 | Loss: 0.00001696
Iteration 34/1000 | Loss: 0.00001696
Iteration 35/1000 | Loss: 0.00001695
Iteration 36/1000 | Loss: 0.00001695
Iteration 37/1000 | Loss: 0.00001694
Iteration 38/1000 | Loss: 0.00001694
Iteration 39/1000 | Loss: 0.00001694
Iteration 40/1000 | Loss: 0.00001694
Iteration 41/1000 | Loss: 0.00001693
Iteration 42/1000 | Loss: 0.00001693
Iteration 43/1000 | Loss: 0.00001693
Iteration 44/1000 | Loss: 0.00001693
Iteration 45/1000 | Loss: 0.00001693
Iteration 46/1000 | Loss: 0.00001693
Iteration 47/1000 | Loss: 0.00001692
Iteration 48/1000 | Loss: 0.00001692
Iteration 49/1000 | Loss: 0.00001692
Iteration 50/1000 | Loss: 0.00001691
Iteration 51/1000 | Loss: 0.00001691
Iteration 52/1000 | Loss: 0.00001691
Iteration 53/1000 | Loss: 0.00001691
Iteration 54/1000 | Loss: 0.00001691
Iteration 55/1000 | Loss: 0.00001691
Iteration 56/1000 | Loss: 0.00001691
Iteration 57/1000 | Loss: 0.00001691
Iteration 58/1000 | Loss: 0.00001691
Iteration 59/1000 | Loss: 0.00001691
Iteration 60/1000 | Loss: 0.00001691
Iteration 61/1000 | Loss: 0.00001691
Iteration 62/1000 | Loss: 0.00001691
Iteration 63/1000 | Loss: 0.00001691
Iteration 64/1000 | Loss: 0.00001691
Iteration 65/1000 | Loss: 0.00001691
Iteration 66/1000 | Loss: 0.00001690
Iteration 67/1000 | Loss: 0.00001690
Iteration 68/1000 | Loss: 0.00001690
Iteration 69/1000 | Loss: 0.00001690
Iteration 70/1000 | Loss: 0.00001690
Iteration 71/1000 | Loss: 0.00001690
Iteration 72/1000 | Loss: 0.00001690
Iteration 73/1000 | Loss: 0.00001690
Iteration 74/1000 | Loss: 0.00001690
Iteration 75/1000 | Loss: 0.00001690
Iteration 76/1000 | Loss: 0.00001690
Iteration 77/1000 | Loss: 0.00001690
Iteration 78/1000 | Loss: 0.00001690
Iteration 79/1000 | Loss: 0.00001690
Iteration 80/1000 | Loss: 0.00001690
Iteration 81/1000 | Loss: 0.00001689
Iteration 82/1000 | Loss: 0.00001689
Iteration 83/1000 | Loss: 0.00001689
Iteration 84/1000 | Loss: 0.00001689
Iteration 85/1000 | Loss: 0.00001688
Iteration 86/1000 | Loss: 0.00001688
Iteration 87/1000 | Loss: 0.00001688
Iteration 88/1000 | Loss: 0.00001687
Iteration 89/1000 | Loss: 0.00001687
Iteration 90/1000 | Loss: 0.00001687
Iteration 91/1000 | Loss: 0.00001687
Iteration 92/1000 | Loss: 0.00001686
Iteration 93/1000 | Loss: 0.00001686
Iteration 94/1000 | Loss: 0.00001686
Iteration 95/1000 | Loss: 0.00001686
Iteration 96/1000 | Loss: 0.00001685
Iteration 97/1000 | Loss: 0.00001685
Iteration 98/1000 | Loss: 0.00001685
Iteration 99/1000 | Loss: 0.00001685
Iteration 100/1000 | Loss: 0.00001685
Iteration 101/1000 | Loss: 0.00001685
Iteration 102/1000 | Loss: 0.00001685
Iteration 103/1000 | Loss: 0.00001685
Iteration 104/1000 | Loss: 0.00001685
Iteration 105/1000 | Loss: 0.00001685
Iteration 106/1000 | Loss: 0.00001685
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 106. Stopping optimization.
Last 5 losses: [1.6847592632984743e-05, 1.6847592632984743e-05, 1.6847592632984743e-05, 1.6847592632984743e-05, 1.6847592632984743e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6847592632984743e-05

Optimization complete. Final v2v error: 3.572999954223633 mm

Highest mean error: 4.0479865074157715 mm for frame 177

Lowest mean error: 3.1223347187042236 mm for frame 14

Saving results

Total time: 36.79113745689392
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_36_nl_5493/0016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_nl_5493/0016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_nl_5493/0016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01005048
Iteration 2/25 | Loss: 0.00194055
Iteration 3/25 | Loss: 0.00150372
Iteration 4/25 | Loss: 0.00132789
Iteration 5/25 | Loss: 0.00129413
Iteration 6/25 | Loss: 0.00126418
Iteration 7/25 | Loss: 0.00126044
Iteration 8/25 | Loss: 0.00124158
Iteration 9/25 | Loss: 0.00121730
Iteration 10/25 | Loss: 0.00120393
Iteration 11/25 | Loss: 0.00119870
Iteration 12/25 | Loss: 0.00119754
Iteration 13/25 | Loss: 0.00120394
Iteration 14/25 | Loss: 0.00119867
Iteration 15/25 | Loss: 0.00119568
Iteration 16/25 | Loss: 0.00119158
Iteration 17/25 | Loss: 0.00118871
Iteration 18/25 | Loss: 0.00118911
Iteration 19/25 | Loss: 0.00118907
Iteration 20/25 | Loss: 0.00118836
Iteration 21/25 | Loss: 0.00118865
Iteration 22/25 | Loss: 0.00118781
Iteration 23/25 | Loss: 0.00118766
Iteration 24/25 | Loss: 0.00118823
Iteration 25/25 | Loss: 0.00118876

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.30069578
Iteration 2/25 | Loss: 0.00130460
Iteration 3/25 | Loss: 0.00130459
Iteration 4/25 | Loss: 0.00130459
Iteration 5/25 | Loss: 0.00130459
Iteration 6/25 | Loss: 0.00130459
Iteration 7/25 | Loss: 0.00130459
Iteration 8/25 | Loss: 0.00130459
Iteration 9/25 | Loss: 0.00130459
Iteration 10/25 | Loss: 0.00130459
Iteration 11/25 | Loss: 0.00130459
Iteration 12/25 | Loss: 0.00130459
Iteration 13/25 | Loss: 0.00130459
Iteration 14/25 | Loss: 0.00130459
Iteration 15/25 | Loss: 0.00130459
Iteration 16/25 | Loss: 0.00130459
Iteration 17/25 | Loss: 0.00130459
Iteration 18/25 | Loss: 0.00130459
Iteration 19/25 | Loss: 0.00130459
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0013045917730778456, 0.0013045917730778456, 0.0013045917730778456, 0.0013045917730778456, 0.0013045917730778456]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013045917730778456

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00130459
Iteration 2/1000 | Loss: 0.00015350
Iteration 3/1000 | Loss: 0.00009433
Iteration 4/1000 | Loss: 0.00049206
Iteration 5/1000 | Loss: 0.00012459
Iteration 6/1000 | Loss: 0.00012767
Iteration 7/1000 | Loss: 0.00009780
Iteration 8/1000 | Loss: 0.00012303
Iteration 9/1000 | Loss: 0.00008878
Iteration 10/1000 | Loss: 0.00009999
Iteration 11/1000 | Loss: 0.00009597
Iteration 12/1000 | Loss: 0.00010647
Iteration 13/1000 | Loss: 0.00010107
Iteration 14/1000 | Loss: 0.00024618
Iteration 15/1000 | Loss: 0.00021048
Iteration 16/1000 | Loss: 0.00010558
Iteration 17/1000 | Loss: 0.00019163
Iteration 18/1000 | Loss: 0.00010804
Iteration 19/1000 | Loss: 0.00034496
Iteration 20/1000 | Loss: 0.00051256
Iteration 21/1000 | Loss: 0.00013445
Iteration 22/1000 | Loss: 0.00034462
Iteration 23/1000 | Loss: 0.00063022
Iteration 24/1000 | Loss: 0.00024059
Iteration 25/1000 | Loss: 0.00016984
Iteration 26/1000 | Loss: 0.00020920
Iteration 27/1000 | Loss: 0.00009564
Iteration 28/1000 | Loss: 0.00027237
Iteration 29/1000 | Loss: 0.00010402
Iteration 30/1000 | Loss: 0.00010101
Iteration 31/1000 | Loss: 0.00007718
Iteration 32/1000 | Loss: 0.00027233
Iteration 33/1000 | Loss: 0.00067224
Iteration 34/1000 | Loss: 0.00028495
Iteration 35/1000 | Loss: 0.00114828
Iteration 36/1000 | Loss: 0.00044598
Iteration 37/1000 | Loss: 0.00038211
Iteration 38/1000 | Loss: 0.00015475
Iteration 39/1000 | Loss: 0.00036340
Iteration 40/1000 | Loss: 0.00034620
Iteration 41/1000 | Loss: 0.00067747
Iteration 42/1000 | Loss: 0.00032375
Iteration 43/1000 | Loss: 0.00023700
Iteration 44/1000 | Loss: 0.00050823
Iteration 45/1000 | Loss: 0.00025712
Iteration 46/1000 | Loss: 0.00056271
Iteration 47/1000 | Loss: 0.00032791
Iteration 48/1000 | Loss: 0.00017852
Iteration 49/1000 | Loss: 0.00055887
Iteration 50/1000 | Loss: 0.00013924
Iteration 51/1000 | Loss: 0.00023775
Iteration 52/1000 | Loss: 0.00006936
Iteration 53/1000 | Loss: 0.00006717
Iteration 54/1000 | Loss: 0.00007047
Iteration 55/1000 | Loss: 0.00007985
Iteration 56/1000 | Loss: 0.00008206
Iteration 57/1000 | Loss: 0.00007079
Iteration 58/1000 | Loss: 0.00006001
Iteration 59/1000 | Loss: 0.00007853
Iteration 60/1000 | Loss: 0.00008133
Iteration 61/1000 | Loss: 0.00007730
Iteration 62/1000 | Loss: 0.00007541
Iteration 63/1000 | Loss: 0.00009120
Iteration 64/1000 | Loss: 0.00008239
Iteration 65/1000 | Loss: 0.00009840
Iteration 66/1000 | Loss: 0.00008844
Iteration 67/1000 | Loss: 0.00008833
Iteration 68/1000 | Loss: 0.00007884
Iteration 69/1000 | Loss: 0.00008352
Iteration 70/1000 | Loss: 0.00009282
Iteration 71/1000 | Loss: 0.00008437
Iteration 72/1000 | Loss: 0.00009373
Iteration 73/1000 | Loss: 0.00005273
Iteration 74/1000 | Loss: 0.00007167
Iteration 75/1000 | Loss: 0.00008608
Iteration 76/1000 | Loss: 0.00009420
Iteration 77/1000 | Loss: 0.00008648
Iteration 78/1000 | Loss: 0.00007088
Iteration 79/1000 | Loss: 0.00005489
Iteration 80/1000 | Loss: 0.00006779
Iteration 81/1000 | Loss: 0.00007685
Iteration 82/1000 | Loss: 0.00008225
Iteration 83/1000 | Loss: 0.00007728
Iteration 84/1000 | Loss: 0.00005120
Iteration 85/1000 | Loss: 0.00007606
Iteration 86/1000 | Loss: 0.00007826
Iteration 87/1000 | Loss: 0.00007985
Iteration 88/1000 | Loss: 0.00007990
Iteration 89/1000 | Loss: 0.00007225
Iteration 90/1000 | Loss: 0.00007719
Iteration 91/1000 | Loss: 0.00008442
Iteration 92/1000 | Loss: 0.00007330
Iteration 93/1000 | Loss: 0.00007560
Iteration 94/1000 | Loss: 0.00008201
Iteration 95/1000 | Loss: 0.00007771
Iteration 96/1000 | Loss: 0.00006397
Iteration 97/1000 | Loss: 0.00006966
Iteration 98/1000 | Loss: 0.00007109
Iteration 99/1000 | Loss: 0.00007484
Iteration 100/1000 | Loss: 0.00008258
Iteration 101/1000 | Loss: 0.00009856
Iteration 102/1000 | Loss: 0.00008430
Iteration 103/1000 | Loss: 0.00006964
Iteration 104/1000 | Loss: 0.00008913
Iteration 105/1000 | Loss: 0.00007889
Iteration 106/1000 | Loss: 0.00008885
Iteration 107/1000 | Loss: 0.00007801
Iteration 108/1000 | Loss: 0.00008308
Iteration 109/1000 | Loss: 0.00007676
Iteration 110/1000 | Loss: 0.00008295
Iteration 111/1000 | Loss: 0.00008168
Iteration 112/1000 | Loss: 0.00007316
Iteration 113/1000 | Loss: 0.00007665
Iteration 114/1000 | Loss: 0.00010471
Iteration 115/1000 | Loss: 0.00008717
Iteration 116/1000 | Loss: 0.00008661
Iteration 117/1000 | Loss: 0.00007971
Iteration 118/1000 | Loss: 0.00009732
Iteration 119/1000 | Loss: 0.00007431
Iteration 120/1000 | Loss: 0.00005823
Iteration 121/1000 | Loss: 0.00006187
Iteration 122/1000 | Loss: 0.00005384
Iteration 123/1000 | Loss: 0.00006554
Iteration 124/1000 | Loss: 0.00007863
Iteration 125/1000 | Loss: 0.00006750
Iteration 126/1000 | Loss: 0.00005833
Iteration 127/1000 | Loss: 0.00005290
Iteration 128/1000 | Loss: 0.00006229
Iteration 129/1000 | Loss: 0.00006927
Iteration 130/1000 | Loss: 0.00006942
Iteration 131/1000 | Loss: 0.00006615
Iteration 132/1000 | Loss: 0.00006998
Iteration 133/1000 | Loss: 0.00006888
Iteration 134/1000 | Loss: 0.00007488
Iteration 135/1000 | Loss: 0.00007138
Iteration 136/1000 | Loss: 0.00006538
Iteration 137/1000 | Loss: 0.00006785
Iteration 138/1000 | Loss: 0.00007198
Iteration 139/1000 | Loss: 0.00006979
Iteration 140/1000 | Loss: 0.00006814
Iteration 141/1000 | Loss: 0.00007621
Iteration 142/1000 | Loss: 0.00006906
Iteration 143/1000 | Loss: 0.00007767
Iteration 144/1000 | Loss: 0.00006813
Iteration 145/1000 | Loss: 0.00008277
Iteration 146/1000 | Loss: 0.00006644
Iteration 147/1000 | Loss: 0.00007418
Iteration 148/1000 | Loss: 0.00007068
Iteration 149/1000 | Loss: 0.00007516
Iteration 150/1000 | Loss: 0.00006477
Iteration 151/1000 | Loss: 0.00006954
Iteration 152/1000 | Loss: 0.00006686
Iteration 153/1000 | Loss: 0.00008396
Iteration 154/1000 | Loss: 0.00007636
Iteration 155/1000 | Loss: 0.00007468
Iteration 156/1000 | Loss: 0.00006871
Iteration 157/1000 | Loss: 0.00007483
Iteration 158/1000 | Loss: 0.00006764
Iteration 159/1000 | Loss: 0.00007483
Iteration 160/1000 | Loss: 0.00006193
Iteration 161/1000 | Loss: 0.00007888
Iteration 162/1000 | Loss: 0.00005595
Iteration 163/1000 | Loss: 0.00006494
Iteration 164/1000 | Loss: 0.00006071
Iteration 165/1000 | Loss: 0.00007155
Iteration 166/1000 | Loss: 0.00005639
Iteration 167/1000 | Loss: 0.00006967
Iteration 168/1000 | Loss: 0.00006991
Iteration 169/1000 | Loss: 0.00006754
Iteration 170/1000 | Loss: 0.00007708
Iteration 171/1000 | Loss: 0.00006907
Iteration 172/1000 | Loss: 0.00006509
Iteration 173/1000 | Loss: 0.00004457
Iteration 174/1000 | Loss: 0.00006159
Iteration 175/1000 | Loss: 0.00006849
Iteration 176/1000 | Loss: 0.00006666
Iteration 177/1000 | Loss: 0.00008026
Iteration 178/1000 | Loss: 0.00006677
Iteration 179/1000 | Loss: 0.00008118
Iteration 180/1000 | Loss: 0.00008505
Iteration 181/1000 | Loss: 0.00010340
Iteration 182/1000 | Loss: 0.00007607
Iteration 183/1000 | Loss: 0.00008510
Iteration 184/1000 | Loss: 0.00006218
Iteration 185/1000 | Loss: 0.00007158
Iteration 186/1000 | Loss: 0.00006753
Iteration 187/1000 | Loss: 0.00006975
Iteration 188/1000 | Loss: 0.00007472
Iteration 189/1000 | Loss: 0.00007406
Iteration 190/1000 | Loss: 0.00007456
Iteration 191/1000 | Loss: 0.00008245
Iteration 192/1000 | Loss: 0.00007977
Iteration 193/1000 | Loss: 0.00007183
Iteration 194/1000 | Loss: 0.00007909
Iteration 195/1000 | Loss: 0.00007024
Iteration 196/1000 | Loss: 0.00007167
Iteration 197/1000 | Loss: 0.00009549
Iteration 198/1000 | Loss: 0.00006540
Iteration 199/1000 | Loss: 0.00006678
Iteration 200/1000 | Loss: 0.00007838
Iteration 201/1000 | Loss: 0.00006709
Iteration 202/1000 | Loss: 0.00005332
Iteration 203/1000 | Loss: 0.00006132
Iteration 204/1000 | Loss: 0.00006203
Iteration 205/1000 | Loss: 0.00006914
Iteration 206/1000 | Loss: 0.00006728
Iteration 207/1000 | Loss: 0.00007042
Iteration 208/1000 | Loss: 0.00007992
Iteration 209/1000 | Loss: 0.00007429
Iteration 210/1000 | Loss: 0.00007140
Iteration 211/1000 | Loss: 0.00007666
Iteration 212/1000 | Loss: 0.00007487
Iteration 213/1000 | Loss: 0.00006985
Iteration 214/1000 | Loss: 0.00005346
Iteration 215/1000 | Loss: 0.00006760
Iteration 216/1000 | Loss: 0.00006894
Iteration 217/1000 | Loss: 0.00007856
Iteration 218/1000 | Loss: 0.00007698
Iteration 219/1000 | Loss: 0.00006827
Iteration 220/1000 | Loss: 0.00009900
Iteration 221/1000 | Loss: 0.00004875
Iteration 222/1000 | Loss: 0.00005279
Iteration 223/1000 | Loss: 0.00005875
Iteration 224/1000 | Loss: 0.00007753
Iteration 225/1000 | Loss: 0.00006835
Iteration 226/1000 | Loss: 0.00007778
Iteration 227/1000 | Loss: 0.00006307
Iteration 228/1000 | Loss: 0.00007559
Iteration 229/1000 | Loss: 0.00005950
Iteration 230/1000 | Loss: 0.00007376
Iteration 231/1000 | Loss: 0.00006433
Iteration 232/1000 | Loss: 0.00005791
Iteration 233/1000 | Loss: 0.00006387
Iteration 234/1000 | Loss: 0.00007653
Iteration 235/1000 | Loss: 0.00007252
Iteration 236/1000 | Loss: 0.00007116
Iteration 237/1000 | Loss: 0.00005752
Iteration 238/1000 | Loss: 0.00005369
Iteration 239/1000 | Loss: 0.00004414
Iteration 240/1000 | Loss: 0.00005913
Iteration 241/1000 | Loss: 0.00006268
Iteration 242/1000 | Loss: 0.00006725
Iteration 243/1000 | Loss: 0.00007047
Iteration 244/1000 | Loss: 0.00006621
Iteration 245/1000 | Loss: 0.00007194
Iteration 246/1000 | Loss: 0.00006340
Iteration 247/1000 | Loss: 0.00005645
Iteration 248/1000 | Loss: 0.00006933
Iteration 249/1000 | Loss: 0.00006424
Iteration 250/1000 | Loss: 0.00005684
Iteration 251/1000 | Loss: 0.00006801
Iteration 252/1000 | Loss: 0.00007112
Iteration 253/1000 | Loss: 0.00006062
Iteration 254/1000 | Loss: 0.00007253
Iteration 255/1000 | Loss: 0.00006184
Iteration 256/1000 | Loss: 0.00006997
Iteration 257/1000 | Loss: 0.00006314
Iteration 258/1000 | Loss: 0.00007057
Iteration 259/1000 | Loss: 0.00006264
Iteration 260/1000 | Loss: 0.00007445
Iteration 261/1000 | Loss: 0.00006515
Iteration 262/1000 | Loss: 0.00006597
Iteration 263/1000 | Loss: 0.00006276
Iteration 264/1000 | Loss: 0.00006251
Iteration 265/1000 | Loss: 0.00006427
Iteration 266/1000 | Loss: 0.00006766
Iteration 267/1000 | Loss: 0.00005073
Iteration 268/1000 | Loss: 0.00005967
Iteration 269/1000 | Loss: 0.00006176
Iteration 270/1000 | Loss: 0.00006268
Iteration 271/1000 | Loss: 0.00006522
Iteration 272/1000 | Loss: 0.00004838
Iteration 273/1000 | Loss: 0.00004829
Iteration 274/1000 | Loss: 0.00005922
Iteration 275/1000 | Loss: 0.00003822
Iteration 276/1000 | Loss: 0.00003778
Iteration 277/1000 | Loss: 0.00004785
Iteration 278/1000 | Loss: 0.00003803
Iteration 279/1000 | Loss: 0.00006577
Iteration 280/1000 | Loss: 0.00005622
Iteration 281/1000 | Loss: 0.00005782
Iteration 282/1000 | Loss: 0.00003663
Iteration 283/1000 | Loss: 0.00003096
Iteration 284/1000 | Loss: 0.00003364
Iteration 285/1000 | Loss: 0.00003499
Iteration 286/1000 | Loss: 0.00002678
Iteration 287/1000 | Loss: 0.00003128
Iteration 288/1000 | Loss: 0.00003614
Iteration 289/1000 | Loss: 0.00002597
Iteration 290/1000 | Loss: 0.00002965
Iteration 291/1000 | Loss: 0.00002382
Iteration 292/1000 | Loss: 0.00002894
Iteration 293/1000 | Loss: 0.00002785
Iteration 294/1000 | Loss: 0.00003073
Iteration 295/1000 | Loss: 0.00003112
Iteration 296/1000 | Loss: 0.00002621
Iteration 297/1000 | Loss: 0.00004292
Iteration 298/1000 | Loss: 0.00003162
Iteration 299/1000 | Loss: 0.00002642
Iteration 300/1000 | Loss: 0.00004242
Iteration 301/1000 | Loss: 0.00002399
Iteration 302/1000 | Loss: 0.00002146
Iteration 303/1000 | Loss: 0.00002055
Iteration 304/1000 | Loss: 0.00002008
Iteration 305/1000 | Loss: 0.00001979
Iteration 306/1000 | Loss: 0.00001972
Iteration 307/1000 | Loss: 0.00001971
Iteration 308/1000 | Loss: 0.00001970
Iteration 309/1000 | Loss: 0.00001964
Iteration 310/1000 | Loss: 0.00001963
Iteration 311/1000 | Loss: 0.00001963
Iteration 312/1000 | Loss: 0.00001962
Iteration 313/1000 | Loss: 0.00001962
Iteration 314/1000 | Loss: 0.00001961
Iteration 315/1000 | Loss: 0.00001960
Iteration 316/1000 | Loss: 0.00001960
Iteration 317/1000 | Loss: 0.00001960
Iteration 318/1000 | Loss: 0.00001960
Iteration 319/1000 | Loss: 0.00001960
Iteration 320/1000 | Loss: 0.00001960
Iteration 321/1000 | Loss: 0.00001960
Iteration 322/1000 | Loss: 0.00001960
Iteration 323/1000 | Loss: 0.00001960
Iteration 324/1000 | Loss: 0.00001958
Iteration 325/1000 | Loss: 0.00001958
Iteration 326/1000 | Loss: 0.00001957
Iteration 327/1000 | Loss: 0.00001957
Iteration 328/1000 | Loss: 0.00001957
Iteration 329/1000 | Loss: 0.00001957
Iteration 330/1000 | Loss: 0.00001956
Iteration 331/1000 | Loss: 0.00001956
Iteration 332/1000 | Loss: 0.00001956
Iteration 333/1000 | Loss: 0.00001956
Iteration 334/1000 | Loss: 0.00001956
Iteration 335/1000 | Loss: 0.00001956
Iteration 336/1000 | Loss: 0.00001956
Iteration 337/1000 | Loss: 0.00001956
Iteration 338/1000 | Loss: 0.00001956
Iteration 339/1000 | Loss: 0.00001956
Iteration 340/1000 | Loss: 0.00001956
Iteration 341/1000 | Loss: 0.00001955
Iteration 342/1000 | Loss: 0.00001955
Iteration 343/1000 | Loss: 0.00001955
Iteration 344/1000 | Loss: 0.00001955
Iteration 345/1000 | Loss: 0.00001955
Iteration 346/1000 | Loss: 0.00001955
Iteration 347/1000 | Loss: 0.00001954
Iteration 348/1000 | Loss: 0.00001954
Iteration 349/1000 | Loss: 0.00001954
Iteration 350/1000 | Loss: 0.00001953
Iteration 351/1000 | Loss: 0.00001953
Iteration 352/1000 | Loss: 0.00001953
Iteration 353/1000 | Loss: 0.00001953
Iteration 354/1000 | Loss: 0.00001952
Iteration 355/1000 | Loss: 0.00001951
Iteration 356/1000 | Loss: 0.00001951
Iteration 357/1000 | Loss: 0.00001951
Iteration 358/1000 | Loss: 0.00001951
Iteration 359/1000 | Loss: 0.00001950
Iteration 360/1000 | Loss: 0.00001950
Iteration 361/1000 | Loss: 0.00001950
Iteration 362/1000 | Loss: 0.00001950
Iteration 363/1000 | Loss: 0.00001950
Iteration 364/1000 | Loss: 0.00001949
Iteration 365/1000 | Loss: 0.00001949
Iteration 366/1000 | Loss: 0.00001949
Iteration 367/1000 | Loss: 0.00001949
Iteration 368/1000 | Loss: 0.00001948
Iteration 369/1000 | Loss: 0.00001948
Iteration 370/1000 | Loss: 0.00001948
Iteration 371/1000 | Loss: 0.00001948
Iteration 372/1000 | Loss: 0.00001947
Iteration 373/1000 | Loss: 0.00001947
Iteration 374/1000 | Loss: 0.00001947
Iteration 375/1000 | Loss: 0.00001947
Iteration 376/1000 | Loss: 0.00001947
Iteration 377/1000 | Loss: 0.00001946
Iteration 378/1000 | Loss: 0.00001946
Iteration 379/1000 | Loss: 0.00001945
Iteration 380/1000 | Loss: 0.00001945
Iteration 381/1000 | Loss: 0.00001945
Iteration 382/1000 | Loss: 0.00001944
Iteration 383/1000 | Loss: 0.00001944
Iteration 384/1000 | Loss: 0.00001944
Iteration 385/1000 | Loss: 0.00001944
Iteration 386/1000 | Loss: 0.00001943
Iteration 387/1000 | Loss: 0.00001943
Iteration 388/1000 | Loss: 0.00001942
Iteration 389/1000 | Loss: 0.00001942
Iteration 390/1000 | Loss: 0.00001942
Iteration 391/1000 | Loss: 0.00001942
Iteration 392/1000 | Loss: 0.00001942
Iteration 393/1000 | Loss: 0.00001942
Iteration 394/1000 | Loss: 0.00001942
Iteration 395/1000 | Loss: 0.00001942
Iteration 396/1000 | Loss: 0.00001942
Iteration 397/1000 | Loss: 0.00001941
Iteration 398/1000 | Loss: 0.00001941
Iteration 399/1000 | Loss: 0.00001941
Iteration 400/1000 | Loss: 0.00001941
Iteration 401/1000 | Loss: 0.00001941
Iteration 402/1000 | Loss: 0.00001941
Iteration 403/1000 | Loss: 0.00001940
Iteration 404/1000 | Loss: 0.00001940
Iteration 405/1000 | Loss: 0.00001940
Iteration 406/1000 | Loss: 0.00001940
Iteration 407/1000 | Loss: 0.00001940
Iteration 408/1000 | Loss: 0.00001940
Iteration 409/1000 | Loss: 0.00001940
Iteration 410/1000 | Loss: 0.00001940
Iteration 411/1000 | Loss: 0.00001940
Iteration 412/1000 | Loss: 0.00001940
Iteration 413/1000 | Loss: 0.00001940
Iteration 414/1000 | Loss: 0.00001939
Iteration 415/1000 | Loss: 0.00001939
Iteration 416/1000 | Loss: 0.00001939
Iteration 417/1000 | Loss: 0.00001939
Iteration 418/1000 | Loss: 0.00001939
Iteration 419/1000 | Loss: 0.00001939
Iteration 420/1000 | Loss: 0.00001939
Iteration 421/1000 | Loss: 0.00001939
Iteration 422/1000 | Loss: 0.00001938
Iteration 423/1000 | Loss: 0.00001938
Iteration 424/1000 | Loss: 0.00001938
Iteration 425/1000 | Loss: 0.00001938
Iteration 426/1000 | Loss: 0.00001938
Iteration 427/1000 | Loss: 0.00001938
Iteration 428/1000 | Loss: 0.00001938
Iteration 429/1000 | Loss: 0.00001938
Iteration 430/1000 | Loss: 0.00001938
Iteration 431/1000 | Loss: 0.00001938
Iteration 432/1000 | Loss: 0.00001938
Iteration 433/1000 | Loss: 0.00001938
Iteration 434/1000 | Loss: 0.00001938
Iteration 435/1000 | Loss: 0.00001938
Iteration 436/1000 | Loss: 0.00001938
Iteration 437/1000 | Loss: 0.00001938
Iteration 438/1000 | Loss: 0.00001938
Iteration 439/1000 | Loss: 0.00001937
Iteration 440/1000 | Loss: 0.00001937
Iteration 441/1000 | Loss: 0.00001937
Iteration 442/1000 | Loss: 0.00001937
Iteration 443/1000 | Loss: 0.00001937
Iteration 444/1000 | Loss: 0.00001937
Iteration 445/1000 | Loss: 0.00001937
Iteration 446/1000 | Loss: 0.00001937
Iteration 447/1000 | Loss: 0.00001937
Iteration 448/1000 | Loss: 0.00001937
Iteration 449/1000 | Loss: 0.00001937
Iteration 450/1000 | Loss: 0.00001937
Iteration 451/1000 | Loss: 0.00001937
Iteration 452/1000 | Loss: 0.00001937
Iteration 453/1000 | Loss: 0.00001937
Iteration 454/1000 | Loss: 0.00001937
Iteration 455/1000 | Loss: 0.00001937
Iteration 456/1000 | Loss: 0.00001937
Iteration 457/1000 | Loss: 0.00001937
Iteration 458/1000 | Loss: 0.00001937
Iteration 459/1000 | Loss: 0.00001937
Iteration 460/1000 | Loss: 0.00001937
Iteration 461/1000 | Loss: 0.00001937
Iteration 462/1000 | Loss: 0.00001937
Iteration 463/1000 | Loss: 0.00001937
Iteration 464/1000 | Loss: 0.00001937
Iteration 465/1000 | Loss: 0.00001937
Iteration 466/1000 | Loss: 0.00001937
Iteration 467/1000 | Loss: 0.00001937
Iteration 468/1000 | Loss: 0.00001937
Iteration 469/1000 | Loss: 0.00001937
Iteration 470/1000 | Loss: 0.00001937
Iteration 471/1000 | Loss: 0.00001937
Iteration 472/1000 | Loss: 0.00001937
Iteration 473/1000 | Loss: 0.00001937
Iteration 474/1000 | Loss: 0.00001937
Iteration 475/1000 | Loss: 0.00001937
Iteration 476/1000 | Loss: 0.00001937
Iteration 477/1000 | Loss: 0.00001937
Iteration 478/1000 | Loss: 0.00001937
Iteration 479/1000 | Loss: 0.00001937
Iteration 480/1000 | Loss: 0.00001937
Iteration 481/1000 | Loss: 0.00001937
Iteration 482/1000 | Loss: 0.00001937
Iteration 483/1000 | Loss: 0.00001937
Iteration 484/1000 | Loss: 0.00001937
Iteration 485/1000 | Loss: 0.00001937
Iteration 486/1000 | Loss: 0.00001937
Iteration 487/1000 | Loss: 0.00001937
Iteration 488/1000 | Loss: 0.00001937
Iteration 489/1000 | Loss: 0.00001937
Iteration 490/1000 | Loss: 0.00001937
Iteration 491/1000 | Loss: 0.00001937
Iteration 492/1000 | Loss: 0.00001937
Iteration 493/1000 | Loss: 0.00001937
Iteration 494/1000 | Loss: 0.00001937
Iteration 495/1000 | Loss: 0.00001937
Iteration 496/1000 | Loss: 0.00001937
Iteration 497/1000 | Loss: 0.00001937
Iteration 498/1000 | Loss: 0.00001937
Iteration 499/1000 | Loss: 0.00001937
Iteration 500/1000 | Loss: 0.00001937
Iteration 501/1000 | Loss: 0.00001937
Iteration 502/1000 | Loss: 0.00001937
Iteration 503/1000 | Loss: 0.00001937
Iteration 504/1000 | Loss: 0.00001937
Iteration 505/1000 | Loss: 0.00001937
Iteration 506/1000 | Loss: 0.00001937
Iteration 507/1000 | Loss: 0.00001937
Iteration 508/1000 | Loss: 0.00001937
Iteration 509/1000 | Loss: 0.00001937
Iteration 510/1000 | Loss: 0.00001937
Iteration 511/1000 | Loss: 0.00001937
Iteration 512/1000 | Loss: 0.00001937
Iteration 513/1000 | Loss: 0.00001937
Iteration 514/1000 | Loss: 0.00001937
Iteration 515/1000 | Loss: 0.00001937
Iteration 516/1000 | Loss: 0.00001937
Iteration 517/1000 | Loss: 0.00001937
Iteration 518/1000 | Loss: 0.00001937
Iteration 519/1000 | Loss: 0.00001937
Iteration 520/1000 | Loss: 0.00001937
Iteration 521/1000 | Loss: 0.00001937
Iteration 522/1000 | Loss: 0.00001937
Iteration 523/1000 | Loss: 0.00001937
Iteration 524/1000 | Loss: 0.00001937
Iteration 525/1000 | Loss: 0.00001937
Iteration 526/1000 | Loss: 0.00001937
Iteration 527/1000 | Loss: 0.00001937
Iteration 528/1000 | Loss: 0.00001937
Iteration 529/1000 | Loss: 0.00001937
Iteration 530/1000 | Loss: 0.00001937
Iteration 531/1000 | Loss: 0.00001937
Iteration 532/1000 | Loss: 0.00001937
Iteration 533/1000 | Loss: 0.00001937
Iteration 534/1000 | Loss: 0.00001937
Iteration 535/1000 | Loss: 0.00001937
Iteration 536/1000 | Loss: 0.00001937
Iteration 537/1000 | Loss: 0.00001937
Iteration 538/1000 | Loss: 0.00001937
Iteration 539/1000 | Loss: 0.00001937
Iteration 540/1000 | Loss: 0.00001937
Iteration 541/1000 | Loss: 0.00001937
Iteration 542/1000 | Loss: 0.00001937
Iteration 543/1000 | Loss: 0.00001937
Iteration 544/1000 | Loss: 0.00001937
Iteration 545/1000 | Loss: 0.00001937
Iteration 546/1000 | Loss: 0.00001937
Iteration 547/1000 | Loss: 0.00001937
Iteration 548/1000 | Loss: 0.00001937
Iteration 549/1000 | Loss: 0.00001937
Iteration 550/1000 | Loss: 0.00001937
Iteration 551/1000 | Loss: 0.00001937
Iteration 552/1000 | Loss: 0.00001937
Iteration 553/1000 | Loss: 0.00001937
Iteration 554/1000 | Loss: 0.00001937
Iteration 555/1000 | Loss: 0.00001937
Iteration 556/1000 | Loss: 0.00001937
Iteration 557/1000 | Loss: 0.00001937
Iteration 558/1000 | Loss: 0.00001937
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 558. Stopping optimization.
Last 5 losses: [1.936514127010014e-05, 1.936514127010014e-05, 1.936514127010014e-05, 1.936514127010014e-05, 1.936514127010014e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.936514127010014e-05

Optimization complete. Final v2v error: 3.4276740550994873 mm

Highest mean error: 8.743884086608887 mm for frame 42

Lowest mean error: 2.775409698486328 mm for frame 0

Saving results

Total time: 565.5469000339508
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_36_nl_5493/0001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_nl_5493/0001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_nl_5493/0001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01099534
Iteration 2/25 | Loss: 0.00458760
Iteration 3/25 | Loss: 0.00542235
Iteration 4/25 | Loss: 0.00220786
Iteration 5/25 | Loss: 0.00210475
Iteration 6/25 | Loss: 0.00182472
Iteration 7/25 | Loss: 0.00173887
Iteration 8/25 | Loss: 0.00167348
Iteration 9/25 | Loss: 0.00161735
Iteration 10/25 | Loss: 0.00157524
Iteration 11/25 | Loss: 0.00153696
Iteration 12/25 | Loss: 0.00151502
Iteration 13/25 | Loss: 0.00146897
Iteration 14/25 | Loss: 0.00144473
Iteration 15/25 | Loss: 0.00139471
Iteration 16/25 | Loss: 0.00137179
Iteration 17/25 | Loss: 0.00136383
Iteration 18/25 | Loss: 0.00135783
Iteration 19/25 | Loss: 0.00135288
Iteration 20/25 | Loss: 0.00134271
Iteration 21/25 | Loss: 0.00133353
Iteration 22/25 | Loss: 0.00133115
Iteration 23/25 | Loss: 0.00133310
Iteration 24/25 | Loss: 0.00132804
Iteration 25/25 | Loss: 0.00132732

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.52810353
Iteration 2/25 | Loss: 0.00413179
Iteration 3/25 | Loss: 0.00413179
Iteration 4/25 | Loss: 0.00413179
Iteration 5/25 | Loss: 0.00413179
Iteration 6/25 | Loss: 0.00413179
Iteration 7/25 | Loss: 0.00413179
Iteration 8/25 | Loss: 0.00413179
Iteration 9/25 | Loss: 0.00413179
Iteration 10/25 | Loss: 0.00413179
Iteration 11/25 | Loss: 0.00413179
Iteration 12/25 | Loss: 0.00413179
Iteration 13/25 | Loss: 0.00413179
Iteration 14/25 | Loss: 0.00413179
Iteration 15/25 | Loss: 0.00413179
Iteration 16/25 | Loss: 0.00413179
Iteration 17/25 | Loss: 0.00413179
Iteration 18/25 | Loss: 0.00413179
Iteration 19/25 | Loss: 0.00413179
Iteration 20/25 | Loss: 0.00413179
Iteration 21/25 | Loss: 0.00413179
Iteration 22/25 | Loss: 0.00413179
Iteration 23/25 | Loss: 0.00413179
Iteration 24/25 | Loss: 0.00413179
Iteration 25/25 | Loss: 0.00413179
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.004131789319217205, 0.004131789319217205, 0.004131789319217205, 0.004131789319217205, 0.004131789319217205]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.004131789319217205

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00413179
Iteration 2/1000 | Loss: 0.00048563
Iteration 3/1000 | Loss: 0.00034568
Iteration 4/1000 | Loss: 0.00027074
Iteration 5/1000 | Loss: 0.00022974
Iteration 6/1000 | Loss: 0.00021679
Iteration 7/1000 | Loss: 0.00020274
Iteration 8/1000 | Loss: 0.00019402
Iteration 9/1000 | Loss: 0.00019096
Iteration 10/1000 | Loss: 0.00018827
Iteration 11/1000 | Loss: 0.00018667
Iteration 12/1000 | Loss: 0.00018574
Iteration 13/1000 | Loss: 0.00018517
Iteration 14/1000 | Loss: 0.00018451
Iteration 15/1000 | Loss: 0.00018395
Iteration 16/1000 | Loss: 0.00031244
Iteration 17/1000 | Loss: 0.00031789
Iteration 18/1000 | Loss: 0.00023591
Iteration 19/1000 | Loss: 0.00018469
Iteration 20/1000 | Loss: 0.00018287
Iteration 21/1000 | Loss: 0.00029530
Iteration 22/1000 | Loss: 0.00018502
Iteration 23/1000 | Loss: 0.00018176
Iteration 24/1000 | Loss: 0.00017958
Iteration 25/1000 | Loss: 0.00017842
Iteration 26/1000 | Loss: 0.00017783
Iteration 27/1000 | Loss: 0.00017748
Iteration 28/1000 | Loss: 0.00017721
Iteration 29/1000 | Loss: 0.00017704
Iteration 30/1000 | Loss: 0.00017696
Iteration 31/1000 | Loss: 0.00017692
Iteration 32/1000 | Loss: 0.00017691
Iteration 33/1000 | Loss: 0.00017683
Iteration 34/1000 | Loss: 0.00017679
Iteration 35/1000 | Loss: 0.00017677
Iteration 36/1000 | Loss: 0.00017677
Iteration 37/1000 | Loss: 0.00017677
Iteration 38/1000 | Loss: 0.00017668
Iteration 39/1000 | Loss: 0.00017668
Iteration 40/1000 | Loss: 0.00017667
Iteration 41/1000 | Loss: 0.00017667
Iteration 42/1000 | Loss: 0.00017662
Iteration 43/1000 | Loss: 0.00017662
Iteration 44/1000 | Loss: 0.00017662
Iteration 45/1000 | Loss: 0.00017662
Iteration 46/1000 | Loss: 0.00017662
Iteration 47/1000 | Loss: 0.00017661
Iteration 48/1000 | Loss: 0.00017661
Iteration 49/1000 | Loss: 0.00017660
Iteration 50/1000 | Loss: 0.00017660
Iteration 51/1000 | Loss: 0.00017660
Iteration 52/1000 | Loss: 0.00017659
Iteration 53/1000 | Loss: 0.00017658
Iteration 54/1000 | Loss: 0.00017658
Iteration 55/1000 | Loss: 0.00017657
Iteration 56/1000 | Loss: 0.00017657
Iteration 57/1000 | Loss: 0.00017657
Iteration 58/1000 | Loss: 0.00017657
Iteration 59/1000 | Loss: 0.00017657
Iteration 60/1000 | Loss: 0.00017657
Iteration 61/1000 | Loss: 0.00017657
Iteration 62/1000 | Loss: 0.00017657
Iteration 63/1000 | Loss: 0.00017656
Iteration 64/1000 | Loss: 0.00017656
Iteration 65/1000 | Loss: 0.00017656
Iteration 66/1000 | Loss: 0.00017656
Iteration 67/1000 | Loss: 0.00017656
Iteration 68/1000 | Loss: 0.00017655
Iteration 69/1000 | Loss: 0.00017655
Iteration 70/1000 | Loss: 0.00017655
Iteration 71/1000 | Loss: 0.00017655
Iteration 72/1000 | Loss: 0.00017655
Iteration 73/1000 | Loss: 0.00017655
Iteration 74/1000 | Loss: 0.00017655
Iteration 75/1000 | Loss: 0.00017655
Iteration 76/1000 | Loss: 0.00017654
Iteration 77/1000 | Loss: 0.00017654
Iteration 78/1000 | Loss: 0.00017654
Iteration 79/1000 | Loss: 0.00017654
Iteration 80/1000 | Loss: 0.00017654
Iteration 81/1000 | Loss: 0.00017654
Iteration 82/1000 | Loss: 0.00017654
Iteration 83/1000 | Loss: 0.00017653
Iteration 84/1000 | Loss: 0.00017653
Iteration 85/1000 | Loss: 0.00017653
Iteration 86/1000 | Loss: 0.00017653
Iteration 87/1000 | Loss: 0.00017653
Iteration 88/1000 | Loss: 0.00017653
Iteration 89/1000 | Loss: 0.00017653
Iteration 90/1000 | Loss: 0.00017653
Iteration 91/1000 | Loss: 0.00017652
Iteration 92/1000 | Loss: 0.00017652
Iteration 93/1000 | Loss: 0.00017652
Iteration 94/1000 | Loss: 0.00017652
Iteration 95/1000 | Loss: 0.00017651
Iteration 96/1000 | Loss: 0.00017651
Iteration 97/1000 | Loss: 0.00017651
Iteration 98/1000 | Loss: 0.00017651
Iteration 99/1000 | Loss: 0.00017649
Iteration 100/1000 | Loss: 0.00017649
Iteration 101/1000 | Loss: 0.00017649
Iteration 102/1000 | Loss: 0.00017649
Iteration 103/1000 | Loss: 0.00017649
Iteration 104/1000 | Loss: 0.00017648
Iteration 105/1000 | Loss: 0.00017648
Iteration 106/1000 | Loss: 0.00017648
Iteration 107/1000 | Loss: 0.00017648
Iteration 108/1000 | Loss: 0.00017648
Iteration 109/1000 | Loss: 0.00017648
Iteration 110/1000 | Loss: 0.00017648
Iteration 111/1000 | Loss: 0.00017647
Iteration 112/1000 | Loss: 0.00017647
Iteration 113/1000 | Loss: 0.00017647
Iteration 114/1000 | Loss: 0.00017646
Iteration 115/1000 | Loss: 0.00017646
Iteration 116/1000 | Loss: 0.00017646
Iteration 117/1000 | Loss: 0.00017646
Iteration 118/1000 | Loss: 0.00017646
Iteration 119/1000 | Loss: 0.00017645
Iteration 120/1000 | Loss: 0.00017645
Iteration 121/1000 | Loss: 0.00017645
Iteration 122/1000 | Loss: 0.00017645
Iteration 123/1000 | Loss: 0.00017645
Iteration 124/1000 | Loss: 0.00017644
Iteration 125/1000 | Loss: 0.00017644
Iteration 126/1000 | Loss: 0.00017643
Iteration 127/1000 | Loss: 0.00017642
Iteration 128/1000 | Loss: 0.00017642
Iteration 129/1000 | Loss: 0.00017641
Iteration 130/1000 | Loss: 0.00017640
Iteration 131/1000 | Loss: 0.00017640
Iteration 132/1000 | Loss: 0.00017639
Iteration 133/1000 | Loss: 0.00017639
Iteration 134/1000 | Loss: 0.00017639
Iteration 135/1000 | Loss: 0.00017639
Iteration 136/1000 | Loss: 0.00017638
Iteration 137/1000 | Loss: 0.00017638
Iteration 138/1000 | Loss: 0.00017638
Iteration 139/1000 | Loss: 0.00017637
Iteration 140/1000 | Loss: 0.00017637
Iteration 141/1000 | Loss: 0.00017636
Iteration 142/1000 | Loss: 0.00017636
Iteration 143/1000 | Loss: 0.00017636
Iteration 144/1000 | Loss: 0.00017636
Iteration 145/1000 | Loss: 0.00017636
Iteration 146/1000 | Loss: 0.00017636
Iteration 147/1000 | Loss: 0.00017636
Iteration 148/1000 | Loss: 0.00017635
Iteration 149/1000 | Loss: 0.00017635
Iteration 150/1000 | Loss: 0.00017635
Iteration 151/1000 | Loss: 0.00017635
Iteration 152/1000 | Loss: 0.00017635
Iteration 153/1000 | Loss: 0.00017635
Iteration 154/1000 | Loss: 0.00017635
Iteration 155/1000 | Loss: 0.00017635
Iteration 156/1000 | Loss: 0.00017635
Iteration 157/1000 | Loss: 0.00017635
Iteration 158/1000 | Loss: 0.00017635
Iteration 159/1000 | Loss: 0.00017635
Iteration 160/1000 | Loss: 0.00017635
Iteration 161/1000 | Loss: 0.00017633
Iteration 162/1000 | Loss: 0.00017633
Iteration 163/1000 | Loss: 0.00017633
Iteration 164/1000 | Loss: 0.00017633
Iteration 165/1000 | Loss: 0.00017631
Iteration 166/1000 | Loss: 0.00017631
Iteration 167/1000 | Loss: 0.00017631
Iteration 168/1000 | Loss: 0.00017631
Iteration 169/1000 | Loss: 0.00017631
Iteration 170/1000 | Loss: 0.00017631
Iteration 171/1000 | Loss: 0.00017631
Iteration 172/1000 | Loss: 0.00017630
Iteration 173/1000 | Loss: 0.00017630
Iteration 174/1000 | Loss: 0.00017630
Iteration 175/1000 | Loss: 0.00017628
Iteration 176/1000 | Loss: 0.00017628
Iteration 177/1000 | Loss: 0.00017627
Iteration 178/1000 | Loss: 0.00017627
Iteration 179/1000 | Loss: 0.00017627
Iteration 180/1000 | Loss: 0.00017627
Iteration 181/1000 | Loss: 0.00017627
Iteration 182/1000 | Loss: 0.00017627
Iteration 183/1000 | Loss: 0.00017627
Iteration 184/1000 | Loss: 0.00017627
Iteration 185/1000 | Loss: 0.00017627
Iteration 186/1000 | Loss: 0.00017626
Iteration 187/1000 | Loss: 0.00017626
Iteration 188/1000 | Loss: 0.00017626
Iteration 189/1000 | Loss: 0.00017625
Iteration 190/1000 | Loss: 0.00017625
Iteration 191/1000 | Loss: 0.00017625
Iteration 192/1000 | Loss: 0.00017625
Iteration 193/1000 | Loss: 0.00017625
Iteration 194/1000 | Loss: 0.00017625
Iteration 195/1000 | Loss: 0.00017625
Iteration 196/1000 | Loss: 0.00017625
Iteration 197/1000 | Loss: 0.00017625
Iteration 198/1000 | Loss: 0.00017625
Iteration 199/1000 | Loss: 0.00017624
Iteration 200/1000 | Loss: 0.00017624
Iteration 201/1000 | Loss: 0.00017624
Iteration 202/1000 | Loss: 0.00017624
Iteration 203/1000 | Loss: 0.00017624
Iteration 204/1000 | Loss: 0.00017624
Iteration 205/1000 | Loss: 0.00017624
Iteration 206/1000 | Loss: 0.00017624
Iteration 207/1000 | Loss: 0.00017623
Iteration 208/1000 | Loss: 0.00017623
Iteration 209/1000 | Loss: 0.00017623
Iteration 210/1000 | Loss: 0.00017623
Iteration 211/1000 | Loss: 0.00017623
Iteration 212/1000 | Loss: 0.00017623
Iteration 213/1000 | Loss: 0.00017623
Iteration 214/1000 | Loss: 0.00017623
Iteration 215/1000 | Loss: 0.00017623
Iteration 216/1000 | Loss: 0.00017623
Iteration 217/1000 | Loss: 0.00017623
Iteration 218/1000 | Loss: 0.00017623
Iteration 219/1000 | Loss: 0.00017623
Iteration 220/1000 | Loss: 0.00017623
Iteration 221/1000 | Loss: 0.00017622
Iteration 222/1000 | Loss: 0.00017622
Iteration 223/1000 | Loss: 0.00017621
Iteration 224/1000 | Loss: 0.00017621
Iteration 225/1000 | Loss: 0.00017621
Iteration 226/1000 | Loss: 0.00017620
Iteration 227/1000 | Loss: 0.00017620
Iteration 228/1000 | Loss: 0.00017620
Iteration 229/1000 | Loss: 0.00017620
Iteration 230/1000 | Loss: 0.00017620
Iteration 231/1000 | Loss: 0.00017620
Iteration 232/1000 | Loss: 0.00017619
Iteration 233/1000 | Loss: 0.00017619
Iteration 234/1000 | Loss: 0.00017619
Iteration 235/1000 | Loss: 0.00017619
Iteration 236/1000 | Loss: 0.00017619
Iteration 237/1000 | Loss: 0.00017619
Iteration 238/1000 | Loss: 0.00017619
Iteration 239/1000 | Loss: 0.00017618
Iteration 240/1000 | Loss: 0.00017618
Iteration 241/1000 | Loss: 0.00017618
Iteration 242/1000 | Loss: 0.00017618
Iteration 243/1000 | Loss: 0.00017618
Iteration 244/1000 | Loss: 0.00017618
Iteration 245/1000 | Loss: 0.00017618
Iteration 246/1000 | Loss: 0.00017618
Iteration 247/1000 | Loss: 0.00017618
Iteration 248/1000 | Loss: 0.00017618
Iteration 249/1000 | Loss: 0.00017617
Iteration 250/1000 | Loss: 0.00017617
Iteration 251/1000 | Loss: 0.00017617
Iteration 252/1000 | Loss: 0.00017617
Iteration 253/1000 | Loss: 0.00017616
Iteration 254/1000 | Loss: 0.00017616
Iteration 255/1000 | Loss: 0.00017616
Iteration 256/1000 | Loss: 0.00017616
Iteration 257/1000 | Loss: 0.00017616
Iteration 258/1000 | Loss: 0.00017616
Iteration 259/1000 | Loss: 0.00017616
Iteration 260/1000 | Loss: 0.00017615
Iteration 261/1000 | Loss: 0.00017615
Iteration 262/1000 | Loss: 0.00017615
Iteration 263/1000 | Loss: 0.00017615
Iteration 264/1000 | Loss: 0.00017615
Iteration 265/1000 | Loss: 0.00017615
Iteration 266/1000 | Loss: 0.00017615
Iteration 267/1000 | Loss: 0.00017615
Iteration 268/1000 | Loss: 0.00017615
Iteration 269/1000 | Loss: 0.00017615
Iteration 270/1000 | Loss: 0.00017615
Iteration 271/1000 | Loss: 0.00017615
Iteration 272/1000 | Loss: 0.00017615
Iteration 273/1000 | Loss: 0.00017615
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 273. Stopping optimization.
Last 5 losses: [0.00017614544776733965, 0.00017614544776733965, 0.00017614544776733965, 0.00017614544776733965, 0.00017614544776733965]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00017614544776733965

Optimization complete. Final v2v error: 6.85935640335083 mm

Highest mean error: 13.194833755493164 mm for frame 132

Lowest mean error: 4.1707353591918945 mm for frame 161

Saving results

Total time: 115.13831996917725
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_36_nl_5493/0017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_nl_5493/0017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_nl_5493/0017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00658488
Iteration 2/25 | Loss: 0.00130286
Iteration 3/25 | Loss: 0.00120762
Iteration 4/25 | Loss: 0.00118485
Iteration 5/25 | Loss: 0.00117945
Iteration 6/25 | Loss: 0.00117862
Iteration 7/25 | Loss: 0.00117862
Iteration 8/25 | Loss: 0.00117862
Iteration 9/25 | Loss: 0.00117862
Iteration 10/25 | Loss: 0.00117862
Iteration 11/25 | Loss: 0.00117862
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011786152608692646, 0.0011786152608692646, 0.0011786152608692646, 0.0011786152608692646, 0.0011786152608692646]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011786152608692646

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.47414875
Iteration 2/25 | Loss: 0.00122928
Iteration 3/25 | Loss: 0.00122928
Iteration 4/25 | Loss: 0.00122928
Iteration 5/25 | Loss: 0.00122928
Iteration 6/25 | Loss: 0.00122928
Iteration 7/25 | Loss: 0.00122928
Iteration 8/25 | Loss: 0.00122928
Iteration 9/25 | Loss: 0.00122928
Iteration 10/25 | Loss: 0.00122928
Iteration 11/25 | Loss: 0.00122928
Iteration 12/25 | Loss: 0.00122928
Iteration 13/25 | Loss: 0.00122928
Iteration 14/25 | Loss: 0.00122928
Iteration 15/25 | Loss: 0.00122928
Iteration 16/25 | Loss: 0.00122928
Iteration 17/25 | Loss: 0.00122928
Iteration 18/25 | Loss: 0.00122928
Iteration 19/25 | Loss: 0.00122928
Iteration 20/25 | Loss: 0.00122928
Iteration 21/25 | Loss: 0.00122928
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0012292783940210938, 0.0012292783940210938, 0.0012292783940210938, 0.0012292783940210938, 0.0012292783940210938]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012292783940210938

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00122928
Iteration 2/1000 | Loss: 0.00004158
Iteration 3/1000 | Loss: 0.00002436
Iteration 4/1000 | Loss: 0.00002246
Iteration 5/1000 | Loss: 0.00002043
Iteration 6/1000 | Loss: 0.00001961
Iteration 7/1000 | Loss: 0.00001909
Iteration 8/1000 | Loss: 0.00001868
Iteration 9/1000 | Loss: 0.00001839
Iteration 10/1000 | Loss: 0.00001825
Iteration 11/1000 | Loss: 0.00001812
Iteration 12/1000 | Loss: 0.00001811
Iteration 13/1000 | Loss: 0.00001801
Iteration 14/1000 | Loss: 0.00001800
Iteration 15/1000 | Loss: 0.00001799
Iteration 16/1000 | Loss: 0.00001796
Iteration 17/1000 | Loss: 0.00001790
Iteration 18/1000 | Loss: 0.00001786
Iteration 19/1000 | Loss: 0.00001785
Iteration 20/1000 | Loss: 0.00001784
Iteration 21/1000 | Loss: 0.00001784
Iteration 22/1000 | Loss: 0.00001784
Iteration 23/1000 | Loss: 0.00001783
Iteration 24/1000 | Loss: 0.00001783
Iteration 25/1000 | Loss: 0.00001783
Iteration 26/1000 | Loss: 0.00001783
Iteration 27/1000 | Loss: 0.00001783
Iteration 28/1000 | Loss: 0.00001782
Iteration 29/1000 | Loss: 0.00001782
Iteration 30/1000 | Loss: 0.00001782
Iteration 31/1000 | Loss: 0.00001781
Iteration 32/1000 | Loss: 0.00001781
Iteration 33/1000 | Loss: 0.00001781
Iteration 34/1000 | Loss: 0.00001781
Iteration 35/1000 | Loss: 0.00001781
Iteration 36/1000 | Loss: 0.00001781
Iteration 37/1000 | Loss: 0.00001780
Iteration 38/1000 | Loss: 0.00001780
Iteration 39/1000 | Loss: 0.00001780
Iteration 40/1000 | Loss: 0.00001780
Iteration 41/1000 | Loss: 0.00001780
Iteration 42/1000 | Loss: 0.00001780
Iteration 43/1000 | Loss: 0.00001780
Iteration 44/1000 | Loss: 0.00001780
Iteration 45/1000 | Loss: 0.00001780
Iteration 46/1000 | Loss: 0.00001780
Iteration 47/1000 | Loss: 0.00001780
Iteration 48/1000 | Loss: 0.00001779
Iteration 49/1000 | Loss: 0.00001779
Iteration 50/1000 | Loss: 0.00001779
Iteration 51/1000 | Loss: 0.00001779
Iteration 52/1000 | Loss: 0.00001779
Iteration 53/1000 | Loss: 0.00001779
Iteration 54/1000 | Loss: 0.00001779
Iteration 55/1000 | Loss: 0.00001779
Iteration 56/1000 | Loss: 0.00001779
Iteration 57/1000 | Loss: 0.00001779
Iteration 58/1000 | Loss: 0.00001779
Iteration 59/1000 | Loss: 0.00001778
Iteration 60/1000 | Loss: 0.00001778
Iteration 61/1000 | Loss: 0.00001778
Iteration 62/1000 | Loss: 0.00001778
Iteration 63/1000 | Loss: 0.00001778
Iteration 64/1000 | Loss: 0.00001778
Iteration 65/1000 | Loss: 0.00001778
Iteration 66/1000 | Loss: 0.00001778
Iteration 67/1000 | Loss: 0.00001778
Iteration 68/1000 | Loss: 0.00001778
Iteration 69/1000 | Loss: 0.00001778
Iteration 70/1000 | Loss: 0.00001778
Iteration 71/1000 | Loss: 0.00001778
Iteration 72/1000 | Loss: 0.00001778
Iteration 73/1000 | Loss: 0.00001778
Iteration 74/1000 | Loss: 0.00001778
Iteration 75/1000 | Loss: 0.00001778
Iteration 76/1000 | Loss: 0.00001778
Iteration 77/1000 | Loss: 0.00001778
Iteration 78/1000 | Loss: 0.00001778
Iteration 79/1000 | Loss: 0.00001778
Iteration 80/1000 | Loss: 0.00001778
Iteration 81/1000 | Loss: 0.00001778
Iteration 82/1000 | Loss: 0.00001778
Iteration 83/1000 | Loss: 0.00001778
Iteration 84/1000 | Loss: 0.00001778
Iteration 85/1000 | Loss: 0.00001778
Iteration 86/1000 | Loss: 0.00001778
Iteration 87/1000 | Loss: 0.00001778
Iteration 88/1000 | Loss: 0.00001778
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 88. Stopping optimization.
Last 5 losses: [1.7782656868803315e-05, 1.7782656868803315e-05, 1.7782656868803315e-05, 1.7782656868803315e-05, 1.7782656868803315e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7782656868803315e-05

Optimization complete. Final v2v error: 3.502443313598633 mm

Highest mean error: 4.5733561515808105 mm for frame 102

Lowest mean error: 3.0963640213012695 mm for frame 118

Saving results

Total time: 29.101646423339844
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_36_nl_5493/0012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_nl_5493/0012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_nl_5493/0012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00883803
Iteration 2/25 | Loss: 0.00123431
Iteration 3/25 | Loss: 0.00113608
Iteration 4/25 | Loss: 0.00112506
Iteration 5/25 | Loss: 0.00112256
Iteration 6/25 | Loss: 0.00112256
Iteration 7/25 | Loss: 0.00112256
Iteration 8/25 | Loss: 0.00112256
Iteration 9/25 | Loss: 0.00112256
Iteration 10/25 | Loss: 0.00112256
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0011225603520870209, 0.0011225603520870209, 0.0011225603520870209, 0.0011225603520870209, 0.0011225603520870209]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011225603520870209

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.26727474
Iteration 2/25 | Loss: 0.00123862
Iteration 3/25 | Loss: 0.00123861
Iteration 4/25 | Loss: 0.00123861
Iteration 5/25 | Loss: 0.00123861
Iteration 6/25 | Loss: 0.00123861
Iteration 7/25 | Loss: 0.00123861
Iteration 8/25 | Loss: 0.00123861
Iteration 9/25 | Loss: 0.00123861
Iteration 10/25 | Loss: 0.00123861
Iteration 11/25 | Loss: 0.00123861
Iteration 12/25 | Loss: 0.00123861
Iteration 13/25 | Loss: 0.00123861
Iteration 14/25 | Loss: 0.00123861
Iteration 15/25 | Loss: 0.00123861
Iteration 16/25 | Loss: 0.00123861
Iteration 17/25 | Loss: 0.00123861
Iteration 18/25 | Loss: 0.00123861
Iteration 19/25 | Loss: 0.00123861
Iteration 20/25 | Loss: 0.00123861
Iteration 21/25 | Loss: 0.00123861
Iteration 22/25 | Loss: 0.00123861
Iteration 23/25 | Loss: 0.00123861
Iteration 24/25 | Loss: 0.00123861
Iteration 25/25 | Loss: 0.00123861

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00123861
Iteration 2/1000 | Loss: 0.00003025
Iteration 3/1000 | Loss: 0.00001638
Iteration 4/1000 | Loss: 0.00001433
Iteration 5/1000 | Loss: 0.00001315
Iteration 6/1000 | Loss: 0.00001263
Iteration 7/1000 | Loss: 0.00001229
Iteration 8/1000 | Loss: 0.00001228
Iteration 9/1000 | Loss: 0.00001200
Iteration 10/1000 | Loss: 0.00001171
Iteration 11/1000 | Loss: 0.00001160
Iteration 12/1000 | Loss: 0.00001160
Iteration 13/1000 | Loss: 0.00001159
Iteration 14/1000 | Loss: 0.00001159
Iteration 15/1000 | Loss: 0.00001158
Iteration 16/1000 | Loss: 0.00001157
Iteration 17/1000 | Loss: 0.00001154
Iteration 18/1000 | Loss: 0.00001145
Iteration 19/1000 | Loss: 0.00001144
Iteration 20/1000 | Loss: 0.00001143
Iteration 21/1000 | Loss: 0.00001143
Iteration 22/1000 | Loss: 0.00001142
Iteration 23/1000 | Loss: 0.00001142
Iteration 24/1000 | Loss: 0.00001141
Iteration 25/1000 | Loss: 0.00001141
Iteration 26/1000 | Loss: 0.00001140
Iteration 27/1000 | Loss: 0.00001140
Iteration 28/1000 | Loss: 0.00001140
Iteration 29/1000 | Loss: 0.00001139
Iteration 30/1000 | Loss: 0.00001139
Iteration 31/1000 | Loss: 0.00001139
Iteration 32/1000 | Loss: 0.00001139
Iteration 33/1000 | Loss: 0.00001138
Iteration 34/1000 | Loss: 0.00001137
Iteration 35/1000 | Loss: 0.00001137
Iteration 36/1000 | Loss: 0.00001137
Iteration 37/1000 | Loss: 0.00001137
Iteration 38/1000 | Loss: 0.00001137
Iteration 39/1000 | Loss: 0.00001137
Iteration 40/1000 | Loss: 0.00001137
Iteration 41/1000 | Loss: 0.00001137
Iteration 42/1000 | Loss: 0.00001137
Iteration 43/1000 | Loss: 0.00001137
Iteration 44/1000 | Loss: 0.00001137
Iteration 45/1000 | Loss: 0.00001136
Iteration 46/1000 | Loss: 0.00001136
Iteration 47/1000 | Loss: 0.00001136
Iteration 48/1000 | Loss: 0.00001135
Iteration 49/1000 | Loss: 0.00001134
Iteration 50/1000 | Loss: 0.00001133
Iteration 51/1000 | Loss: 0.00001132
Iteration 52/1000 | Loss: 0.00001132
Iteration 53/1000 | Loss: 0.00001132
Iteration 54/1000 | Loss: 0.00001131
Iteration 55/1000 | Loss: 0.00001131
Iteration 56/1000 | Loss: 0.00001130
Iteration 57/1000 | Loss: 0.00001130
Iteration 58/1000 | Loss: 0.00001130
Iteration 59/1000 | Loss: 0.00001130
Iteration 60/1000 | Loss: 0.00001130
Iteration 61/1000 | Loss: 0.00001130
Iteration 62/1000 | Loss: 0.00001130
Iteration 63/1000 | Loss: 0.00001130
Iteration 64/1000 | Loss: 0.00001130
Iteration 65/1000 | Loss: 0.00001130
Iteration 66/1000 | Loss: 0.00001130
Iteration 67/1000 | Loss: 0.00001129
Iteration 68/1000 | Loss: 0.00001129
Iteration 69/1000 | Loss: 0.00001129
Iteration 70/1000 | Loss: 0.00001129
Iteration 71/1000 | Loss: 0.00001129
Iteration 72/1000 | Loss: 0.00001129
Iteration 73/1000 | Loss: 0.00001129
Iteration 74/1000 | Loss: 0.00001129
Iteration 75/1000 | Loss: 0.00001129
Iteration 76/1000 | Loss: 0.00001128
Iteration 77/1000 | Loss: 0.00001128
Iteration 78/1000 | Loss: 0.00001128
Iteration 79/1000 | Loss: 0.00001128
Iteration 80/1000 | Loss: 0.00001128
Iteration 81/1000 | Loss: 0.00001128
Iteration 82/1000 | Loss: 0.00001128
Iteration 83/1000 | Loss: 0.00001128
Iteration 84/1000 | Loss: 0.00001128
Iteration 85/1000 | Loss: 0.00001128
Iteration 86/1000 | Loss: 0.00001128
Iteration 87/1000 | Loss: 0.00001128
Iteration 88/1000 | Loss: 0.00001128
Iteration 89/1000 | Loss: 0.00001127
Iteration 90/1000 | Loss: 0.00001127
Iteration 91/1000 | Loss: 0.00001127
Iteration 92/1000 | Loss: 0.00001127
Iteration 93/1000 | Loss: 0.00001127
Iteration 94/1000 | Loss: 0.00001127
Iteration 95/1000 | Loss: 0.00001127
Iteration 96/1000 | Loss: 0.00001127
Iteration 97/1000 | Loss: 0.00001127
Iteration 98/1000 | Loss: 0.00001127
Iteration 99/1000 | Loss: 0.00001127
Iteration 100/1000 | Loss: 0.00001127
Iteration 101/1000 | Loss: 0.00001127
Iteration 102/1000 | Loss: 0.00001126
Iteration 103/1000 | Loss: 0.00001126
Iteration 104/1000 | Loss: 0.00001126
Iteration 105/1000 | Loss: 0.00001126
Iteration 106/1000 | Loss: 0.00001126
Iteration 107/1000 | Loss: 0.00001126
Iteration 108/1000 | Loss: 0.00001126
Iteration 109/1000 | Loss: 0.00001126
Iteration 110/1000 | Loss: 0.00001125
Iteration 111/1000 | Loss: 0.00001125
Iteration 112/1000 | Loss: 0.00001125
Iteration 113/1000 | Loss: 0.00001125
Iteration 114/1000 | Loss: 0.00001125
Iteration 115/1000 | Loss: 0.00001125
Iteration 116/1000 | Loss: 0.00001125
Iteration 117/1000 | Loss: 0.00001125
Iteration 118/1000 | Loss: 0.00001124
Iteration 119/1000 | Loss: 0.00001124
Iteration 120/1000 | Loss: 0.00001124
Iteration 121/1000 | Loss: 0.00001124
Iteration 122/1000 | Loss: 0.00001124
Iteration 123/1000 | Loss: 0.00001124
Iteration 124/1000 | Loss: 0.00001124
Iteration 125/1000 | Loss: 0.00001124
Iteration 126/1000 | Loss: 0.00001124
Iteration 127/1000 | Loss: 0.00001124
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 127. Stopping optimization.
Last 5 losses: [1.1241058018640615e-05, 1.1241058018640615e-05, 1.1241058018640615e-05, 1.1241058018640615e-05, 1.1241058018640615e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1241058018640615e-05

Optimization complete. Final v2v error: 2.875037670135498 mm

Highest mean error: 3.1676342487335205 mm for frame 85

Lowest mean error: 2.700462818145752 mm for frame 209

Saving results

Total time: 34.6346378326416
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_36_nl_5493/0006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_nl_5493/0006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_nl_5493/0006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01063181
Iteration 2/25 | Loss: 0.01063181
Iteration 3/25 | Loss: 0.01063180
Iteration 4/25 | Loss: 0.01063180
Iteration 5/25 | Loss: 0.01063180
Iteration 6/25 | Loss: 0.01063180
Iteration 7/25 | Loss: 0.01063179
Iteration 8/25 | Loss: 0.01063179
Iteration 9/25 | Loss: 0.01063179
Iteration 10/25 | Loss: 0.01063179
Iteration 11/25 | Loss: 0.01063178
Iteration 12/25 | Loss: 0.01063178
Iteration 13/25 | Loss: 0.01063178
Iteration 14/25 | Loss: 0.01063178
Iteration 15/25 | Loss: 0.01063178
Iteration 16/25 | Loss: 0.01063177
Iteration 17/25 | Loss: 0.01063177
Iteration 18/25 | Loss: 0.01063177
Iteration 19/25 | Loss: 0.01063176
Iteration 20/25 | Loss: 0.01063176
Iteration 21/25 | Loss: 0.01063175
Iteration 22/25 | Loss: 0.01063175
Iteration 23/25 | Loss: 0.01063175
Iteration 24/25 | Loss: 0.01063175
Iteration 25/25 | Loss: 0.01063175

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.74682415
Iteration 2/25 | Loss: 0.09827624
Iteration 3/25 | Loss: 0.08567579
Iteration 4/25 | Loss: 0.08537860
Iteration 5/25 | Loss: 0.08537859
Iteration 6/25 | Loss: 0.08537856
Iteration 7/25 | Loss: 0.08537855
Iteration 8/25 | Loss: 0.08537855
Iteration 9/25 | Loss: 0.08537855
Iteration 10/25 | Loss: 0.08537855
Iteration 11/25 | Loss: 0.08537854
Iteration 12/25 | Loss: 0.08537854
Iteration 13/25 | Loss: 0.08537854
Iteration 14/25 | Loss: 0.08537854
Iteration 15/25 | Loss: 0.08537854
Iteration 16/25 | Loss: 0.08537854
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.08537853509187698, 0.08537853509187698, 0.08537853509187698, 0.08537853509187698, 0.08537853509187698]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.08537853509187698

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.08537854
Iteration 2/1000 | Loss: 0.00381513
Iteration 3/1000 | Loss: 0.00268731
Iteration 4/1000 | Loss: 0.00059524
Iteration 5/1000 | Loss: 0.00094178
Iteration 6/1000 | Loss: 0.00041480
Iteration 7/1000 | Loss: 0.00029179
Iteration 8/1000 | Loss: 0.00011824
Iteration 9/1000 | Loss: 0.00022220
Iteration 10/1000 | Loss: 0.00007735
Iteration 11/1000 | Loss: 0.00010813
Iteration 12/1000 | Loss: 0.00005423
Iteration 13/1000 | Loss: 0.00004859
Iteration 14/1000 | Loss: 0.00004775
Iteration 15/1000 | Loss: 0.00004126
Iteration 16/1000 | Loss: 0.00007183
Iteration 17/1000 | Loss: 0.00003940
Iteration 18/1000 | Loss: 0.00003592
Iteration 19/1000 | Loss: 0.00003898
Iteration 20/1000 | Loss: 0.00003405
Iteration 21/1000 | Loss: 0.00003582
Iteration 22/1000 | Loss: 0.00003198
Iteration 23/1000 | Loss: 0.00003052
Iteration 24/1000 | Loss: 0.00002950
Iteration 25/1000 | Loss: 0.00002858
Iteration 26/1000 | Loss: 0.00010788
Iteration 27/1000 | Loss: 0.00002755
Iteration 28/1000 | Loss: 0.00002638
Iteration 29/1000 | Loss: 0.00002574
Iteration 30/1000 | Loss: 0.00002524
Iteration 31/1000 | Loss: 0.00002484
Iteration 32/1000 | Loss: 0.00002451
Iteration 33/1000 | Loss: 0.00008396
Iteration 34/1000 | Loss: 0.00005198
Iteration 35/1000 | Loss: 0.00008399
Iteration 36/1000 | Loss: 0.00026509
Iteration 37/1000 | Loss: 0.00017353
Iteration 38/1000 | Loss: 0.00022456
Iteration 39/1000 | Loss: 0.00012985
Iteration 40/1000 | Loss: 0.00015078
Iteration 41/1000 | Loss: 0.00023218
Iteration 42/1000 | Loss: 0.00012324
Iteration 43/1000 | Loss: 0.00058369
Iteration 44/1000 | Loss: 0.00054378
Iteration 45/1000 | Loss: 0.00033099
Iteration 46/1000 | Loss: 0.00032940
Iteration 47/1000 | Loss: 0.00007603
Iteration 48/1000 | Loss: 0.00008944
Iteration 49/1000 | Loss: 0.00003314
Iteration 50/1000 | Loss: 0.00003543
Iteration 51/1000 | Loss: 0.00002807
Iteration 52/1000 | Loss: 0.00002693
Iteration 53/1000 | Loss: 0.00003908
Iteration 54/1000 | Loss: 0.00002677
Iteration 55/1000 | Loss: 0.00002660
Iteration 56/1000 | Loss: 0.00002615
Iteration 57/1000 | Loss: 0.00013343
Iteration 58/1000 | Loss: 0.00002564
Iteration 59/1000 | Loss: 0.00004378
Iteration 60/1000 | Loss: 0.00003132
Iteration 61/1000 | Loss: 0.00004005
Iteration 62/1000 | Loss: 0.00002434
Iteration 63/1000 | Loss: 0.00002412
Iteration 64/1000 | Loss: 0.00002372
Iteration 65/1000 | Loss: 0.00002334
Iteration 66/1000 | Loss: 0.00002313
Iteration 67/1000 | Loss: 0.00002312
Iteration 68/1000 | Loss: 0.00002311
Iteration 69/1000 | Loss: 0.00002310
Iteration 70/1000 | Loss: 0.00002308
Iteration 71/1000 | Loss: 0.00002308
Iteration 72/1000 | Loss: 0.00002306
Iteration 73/1000 | Loss: 0.00011135
Iteration 74/1000 | Loss: 0.00002302
Iteration 75/1000 | Loss: 0.00002285
Iteration 76/1000 | Loss: 0.00002284
Iteration 77/1000 | Loss: 0.00002284
Iteration 78/1000 | Loss: 0.00002283
Iteration 79/1000 | Loss: 0.00002282
Iteration 80/1000 | Loss: 0.00002282
Iteration 81/1000 | Loss: 0.00002280
Iteration 82/1000 | Loss: 0.00002280
Iteration 83/1000 | Loss: 0.00002280
Iteration 84/1000 | Loss: 0.00002280
Iteration 85/1000 | Loss: 0.00002280
Iteration 86/1000 | Loss: 0.00002280
Iteration 87/1000 | Loss: 0.00002280
Iteration 88/1000 | Loss: 0.00002280
Iteration 89/1000 | Loss: 0.00002280
Iteration 90/1000 | Loss: 0.00002280
Iteration 91/1000 | Loss: 0.00002279
Iteration 92/1000 | Loss: 0.00002279
Iteration 93/1000 | Loss: 0.00002279
Iteration 94/1000 | Loss: 0.00002279
Iteration 95/1000 | Loss: 0.00002279
Iteration 96/1000 | Loss: 0.00002279
Iteration 97/1000 | Loss: 0.00002279
Iteration 98/1000 | Loss: 0.00002276
Iteration 99/1000 | Loss: 0.00002275
Iteration 100/1000 | Loss: 0.00002275
Iteration 101/1000 | Loss: 0.00002272
Iteration 102/1000 | Loss: 0.00002272
Iteration 103/1000 | Loss: 0.00002271
Iteration 104/1000 | Loss: 0.00002271
Iteration 105/1000 | Loss: 0.00002268
Iteration 106/1000 | Loss: 0.00002268
Iteration 107/1000 | Loss: 0.00002267
Iteration 108/1000 | Loss: 0.00002267
Iteration 109/1000 | Loss: 0.00002266
Iteration 110/1000 | Loss: 0.00002265
Iteration 111/1000 | Loss: 0.00002265
Iteration 112/1000 | Loss: 0.00002263
Iteration 113/1000 | Loss: 0.00002263
Iteration 114/1000 | Loss: 0.00002263
Iteration 115/1000 | Loss: 0.00002263
Iteration 116/1000 | Loss: 0.00002262
Iteration 117/1000 | Loss: 0.00002262
Iteration 118/1000 | Loss: 0.00002261
Iteration 119/1000 | Loss: 0.00002261
Iteration 120/1000 | Loss: 0.00002261
Iteration 121/1000 | Loss: 0.00002261
Iteration 122/1000 | Loss: 0.00002261
Iteration 123/1000 | Loss: 0.00002261
Iteration 124/1000 | Loss: 0.00002261
Iteration 125/1000 | Loss: 0.00002261
Iteration 126/1000 | Loss: 0.00002261
Iteration 127/1000 | Loss: 0.00002261
Iteration 128/1000 | Loss: 0.00002261
Iteration 129/1000 | Loss: 0.00002260
Iteration 130/1000 | Loss: 0.00002260
Iteration 131/1000 | Loss: 0.00002260
Iteration 132/1000 | Loss: 0.00002260
Iteration 133/1000 | Loss: 0.00002260
Iteration 134/1000 | Loss: 0.00002260
Iteration 135/1000 | Loss: 0.00002259
Iteration 136/1000 | Loss: 0.00002259
Iteration 137/1000 | Loss: 0.00002259
Iteration 138/1000 | Loss: 0.00002259
Iteration 139/1000 | Loss: 0.00002259
Iteration 140/1000 | Loss: 0.00002258
Iteration 141/1000 | Loss: 0.00002258
Iteration 142/1000 | Loss: 0.00002258
Iteration 143/1000 | Loss: 0.00002257
Iteration 144/1000 | Loss: 0.00002257
Iteration 145/1000 | Loss: 0.00002257
Iteration 146/1000 | Loss: 0.00002256
Iteration 147/1000 | Loss: 0.00002256
Iteration 148/1000 | Loss: 0.00002256
Iteration 149/1000 | Loss: 0.00002256
Iteration 150/1000 | Loss: 0.00002256
Iteration 151/1000 | Loss: 0.00002256
Iteration 152/1000 | Loss: 0.00002256
Iteration 153/1000 | Loss: 0.00002256
Iteration 154/1000 | Loss: 0.00002255
Iteration 155/1000 | Loss: 0.00002255
Iteration 156/1000 | Loss: 0.00002255
Iteration 157/1000 | Loss: 0.00002255
Iteration 158/1000 | Loss: 0.00002255
Iteration 159/1000 | Loss: 0.00003987
Iteration 160/1000 | Loss: 0.00002283
Iteration 161/1000 | Loss: 0.00002255
Iteration 162/1000 | Loss: 0.00002255
Iteration 163/1000 | Loss: 0.00002255
Iteration 164/1000 | Loss: 0.00002255
Iteration 165/1000 | Loss: 0.00002254
Iteration 166/1000 | Loss: 0.00002254
Iteration 167/1000 | Loss: 0.00002254
Iteration 168/1000 | Loss: 0.00002254
Iteration 169/1000 | Loss: 0.00002253
Iteration 170/1000 | Loss: 0.00002253
Iteration 171/1000 | Loss: 0.00002253
Iteration 172/1000 | Loss: 0.00002253
Iteration 173/1000 | Loss: 0.00002253
Iteration 174/1000 | Loss: 0.00002253
Iteration 175/1000 | Loss: 0.00002253
Iteration 176/1000 | Loss: 0.00002253
Iteration 177/1000 | Loss: 0.00002253
Iteration 178/1000 | Loss: 0.00002253
Iteration 179/1000 | Loss: 0.00002253
Iteration 180/1000 | Loss: 0.00002252
Iteration 181/1000 | Loss: 0.00002252
Iteration 182/1000 | Loss: 0.00002252
Iteration 183/1000 | Loss: 0.00002252
Iteration 184/1000 | Loss: 0.00002252
Iteration 185/1000 | Loss: 0.00002252
Iteration 186/1000 | Loss: 0.00002252
Iteration 187/1000 | Loss: 0.00002252
Iteration 188/1000 | Loss: 0.00002252
Iteration 189/1000 | Loss: 0.00002252
Iteration 190/1000 | Loss: 0.00002252
Iteration 191/1000 | Loss: 0.00002252
Iteration 192/1000 | Loss: 0.00002252
Iteration 193/1000 | Loss: 0.00002252
Iteration 194/1000 | Loss: 0.00002252
Iteration 195/1000 | Loss: 0.00002252
Iteration 196/1000 | Loss: 0.00002252
Iteration 197/1000 | Loss: 0.00002252
Iteration 198/1000 | Loss: 0.00002251
Iteration 199/1000 | Loss: 0.00002251
Iteration 200/1000 | Loss: 0.00002251
Iteration 201/1000 | Loss: 0.00002251
Iteration 202/1000 | Loss: 0.00002251
Iteration 203/1000 | Loss: 0.00002251
Iteration 204/1000 | Loss: 0.00002251
Iteration 205/1000 | Loss: 0.00002251
Iteration 206/1000 | Loss: 0.00002251
Iteration 207/1000 | Loss: 0.00002251
Iteration 208/1000 | Loss: 0.00002251
Iteration 209/1000 | Loss: 0.00002251
Iteration 210/1000 | Loss: 0.00002251
Iteration 211/1000 | Loss: 0.00002251
Iteration 212/1000 | Loss: 0.00002251
Iteration 213/1000 | Loss: 0.00002251
Iteration 214/1000 | Loss: 0.00002251
Iteration 215/1000 | Loss: 0.00002251
Iteration 216/1000 | Loss: 0.00002251
Iteration 217/1000 | Loss: 0.00002250
Iteration 218/1000 | Loss: 0.00002250
Iteration 219/1000 | Loss: 0.00002250
Iteration 220/1000 | Loss: 0.00002250
Iteration 221/1000 | Loss: 0.00002250
Iteration 222/1000 | Loss: 0.00002250
Iteration 223/1000 | Loss: 0.00002250
Iteration 224/1000 | Loss: 0.00002250
Iteration 225/1000 | Loss: 0.00002250
Iteration 226/1000 | Loss: 0.00002250
Iteration 227/1000 | Loss: 0.00002250
Iteration 228/1000 | Loss: 0.00002250
Iteration 229/1000 | Loss: 0.00002250
Iteration 230/1000 | Loss: 0.00002250
Iteration 231/1000 | Loss: 0.00002250
Iteration 232/1000 | Loss: 0.00002250
Iteration 233/1000 | Loss: 0.00002250
Iteration 234/1000 | Loss: 0.00002250
Iteration 235/1000 | Loss: 0.00002250
Iteration 236/1000 | Loss: 0.00002250
Iteration 237/1000 | Loss: 0.00002250
Iteration 238/1000 | Loss: 0.00002250
Iteration 239/1000 | Loss: 0.00002250
Iteration 240/1000 | Loss: 0.00002250
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 240. Stopping optimization.
Last 5 losses: [2.2499831175082363e-05, 2.2499831175082363e-05, 2.2499831175082363e-05, 2.2499831175082363e-05, 2.2499831175082363e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2499831175082363e-05

Optimization complete. Final v2v error: 3.7609024047851562 mm

Highest mean error: 10.441251754760742 mm for frame 104

Lowest mean error: 3.1440556049346924 mm for frame 147

Saving results

Total time: 137.92798781394958
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_36_nl_5493/0024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_nl_5493/0024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_nl_5493/0024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00948054
Iteration 2/25 | Loss: 0.00132022
Iteration 3/25 | Loss: 0.00117040
Iteration 4/25 | Loss: 0.00115186
Iteration 5/25 | Loss: 0.00114660
Iteration 6/25 | Loss: 0.00114526
Iteration 7/25 | Loss: 0.00114526
Iteration 8/25 | Loss: 0.00114526
Iteration 9/25 | Loss: 0.00114526
Iteration 10/25 | Loss: 0.00114526
Iteration 11/25 | Loss: 0.00114526
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011452556354925036, 0.0011452556354925036, 0.0011452556354925036, 0.0011452556354925036, 0.0011452556354925036]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011452556354925036

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.68065429
Iteration 2/25 | Loss: 0.00110144
Iteration 3/25 | Loss: 0.00110144
Iteration 4/25 | Loss: 0.00110144
Iteration 5/25 | Loss: 0.00110144
Iteration 6/25 | Loss: 0.00110144
Iteration 7/25 | Loss: 0.00110144
Iteration 8/25 | Loss: 0.00110144
Iteration 9/25 | Loss: 0.00110144
Iteration 10/25 | Loss: 0.00110144
Iteration 11/25 | Loss: 0.00110144
Iteration 12/25 | Loss: 0.00110144
Iteration 13/25 | Loss: 0.00110144
Iteration 14/25 | Loss: 0.00110144
Iteration 15/25 | Loss: 0.00110144
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.00110143655911088, 0.00110143655911088, 0.00110143655911088, 0.00110143655911088, 0.00110143655911088]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00110143655911088

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00110144
Iteration 2/1000 | Loss: 0.00003281
Iteration 3/1000 | Loss: 0.00001918
Iteration 4/1000 | Loss: 0.00001714
Iteration 5/1000 | Loss: 0.00001584
Iteration 6/1000 | Loss: 0.00001539
Iteration 7/1000 | Loss: 0.00001487
Iteration 8/1000 | Loss: 0.00001455
Iteration 9/1000 | Loss: 0.00001425
Iteration 10/1000 | Loss: 0.00001406
Iteration 11/1000 | Loss: 0.00001398
Iteration 12/1000 | Loss: 0.00001397
Iteration 13/1000 | Loss: 0.00001392
Iteration 14/1000 | Loss: 0.00001392
Iteration 15/1000 | Loss: 0.00001391
Iteration 16/1000 | Loss: 0.00001391
Iteration 17/1000 | Loss: 0.00001390
Iteration 18/1000 | Loss: 0.00001385
Iteration 19/1000 | Loss: 0.00001384
Iteration 20/1000 | Loss: 0.00001384
Iteration 21/1000 | Loss: 0.00001383
Iteration 22/1000 | Loss: 0.00001382
Iteration 23/1000 | Loss: 0.00001381
Iteration 24/1000 | Loss: 0.00001381
Iteration 25/1000 | Loss: 0.00001380
Iteration 26/1000 | Loss: 0.00001380
Iteration 27/1000 | Loss: 0.00001380
Iteration 28/1000 | Loss: 0.00001380
Iteration 29/1000 | Loss: 0.00001380
Iteration 30/1000 | Loss: 0.00001380
Iteration 31/1000 | Loss: 0.00001380
Iteration 32/1000 | Loss: 0.00001380
Iteration 33/1000 | Loss: 0.00001380
Iteration 34/1000 | Loss: 0.00001380
Iteration 35/1000 | Loss: 0.00001380
Iteration 36/1000 | Loss: 0.00001379
Iteration 37/1000 | Loss: 0.00001379
Iteration 38/1000 | Loss: 0.00001379
Iteration 39/1000 | Loss: 0.00001379
Iteration 40/1000 | Loss: 0.00001379
Iteration 41/1000 | Loss: 0.00001379
Iteration 42/1000 | Loss: 0.00001377
Iteration 43/1000 | Loss: 0.00001377
Iteration 44/1000 | Loss: 0.00001377
Iteration 45/1000 | Loss: 0.00001377
Iteration 46/1000 | Loss: 0.00001376
Iteration 47/1000 | Loss: 0.00001376
Iteration 48/1000 | Loss: 0.00001376
Iteration 49/1000 | Loss: 0.00001376
Iteration 50/1000 | Loss: 0.00001376
Iteration 51/1000 | Loss: 0.00001376
Iteration 52/1000 | Loss: 0.00001376
Iteration 53/1000 | Loss: 0.00001376
Iteration 54/1000 | Loss: 0.00001376
Iteration 55/1000 | Loss: 0.00001376
Iteration 56/1000 | Loss: 0.00001376
Iteration 57/1000 | Loss: 0.00001376
Iteration 58/1000 | Loss: 0.00001376
Iteration 59/1000 | Loss: 0.00001375
Iteration 60/1000 | Loss: 0.00001375
Iteration 61/1000 | Loss: 0.00001375
Iteration 62/1000 | Loss: 0.00001375
Iteration 63/1000 | Loss: 0.00001375
Iteration 64/1000 | Loss: 0.00001375
Iteration 65/1000 | Loss: 0.00001374
Iteration 66/1000 | Loss: 0.00001374
Iteration 67/1000 | Loss: 0.00001374
Iteration 68/1000 | Loss: 0.00001374
Iteration 69/1000 | Loss: 0.00001374
Iteration 70/1000 | Loss: 0.00001374
Iteration 71/1000 | Loss: 0.00001374
Iteration 72/1000 | Loss: 0.00001374
Iteration 73/1000 | Loss: 0.00001374
Iteration 74/1000 | Loss: 0.00001374
Iteration 75/1000 | Loss: 0.00001374
Iteration 76/1000 | Loss: 0.00001374
Iteration 77/1000 | Loss: 0.00001374
Iteration 78/1000 | Loss: 0.00001374
Iteration 79/1000 | Loss: 0.00001374
Iteration 80/1000 | Loss: 0.00001374
Iteration 81/1000 | Loss: 0.00001374
Iteration 82/1000 | Loss: 0.00001374
Iteration 83/1000 | Loss: 0.00001374
Iteration 84/1000 | Loss: 0.00001374
Iteration 85/1000 | Loss: 0.00001374
Iteration 86/1000 | Loss: 0.00001374
Iteration 87/1000 | Loss: 0.00001374
Iteration 88/1000 | Loss: 0.00001374
Iteration 89/1000 | Loss: 0.00001374
Iteration 90/1000 | Loss: 0.00001374
Iteration 91/1000 | Loss: 0.00001374
Iteration 92/1000 | Loss: 0.00001374
Iteration 93/1000 | Loss: 0.00001374
Iteration 94/1000 | Loss: 0.00001374
Iteration 95/1000 | Loss: 0.00001374
Iteration 96/1000 | Loss: 0.00001374
Iteration 97/1000 | Loss: 0.00001374
Iteration 98/1000 | Loss: 0.00001374
Iteration 99/1000 | Loss: 0.00001374
Iteration 100/1000 | Loss: 0.00001374
Iteration 101/1000 | Loss: 0.00001374
Iteration 102/1000 | Loss: 0.00001374
Iteration 103/1000 | Loss: 0.00001374
Iteration 104/1000 | Loss: 0.00001374
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 104. Stopping optimization.
Last 5 losses: [1.3741804650635459e-05, 1.3741804650635459e-05, 1.3741804650635459e-05, 1.3741804650635459e-05, 1.3741804650635459e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3741804650635459e-05

Optimization complete. Final v2v error: 3.071399211883545 mm

Highest mean error: 3.3140242099761963 mm for frame 168

Lowest mean error: 2.783681631088257 mm for frame 0

Saving results

Total time: 33.40455508232117
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_36_nl_5493/0002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_nl_5493/0002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_nl_5493/0002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01078801
Iteration 2/25 | Loss: 0.00229225
Iteration 3/25 | Loss: 0.00170773
Iteration 4/25 | Loss: 0.00145909
Iteration 5/25 | Loss: 0.00148946
Iteration 6/25 | Loss: 0.00135479
Iteration 7/25 | Loss: 0.00128775
Iteration 8/25 | Loss: 0.00126563
Iteration 9/25 | Loss: 0.00125841
Iteration 10/25 | Loss: 0.00125685
Iteration 11/25 | Loss: 0.00126092
Iteration 12/25 | Loss: 0.00126063
Iteration 13/25 | Loss: 0.00125638
Iteration 14/25 | Loss: 0.00125356
Iteration 15/25 | Loss: 0.00125268
Iteration 16/25 | Loss: 0.00125252
Iteration 17/25 | Loss: 0.00125246
Iteration 18/25 | Loss: 0.00125245
Iteration 19/25 | Loss: 0.00125245
Iteration 20/25 | Loss: 0.00125244
Iteration 21/25 | Loss: 0.00125244
Iteration 22/25 | Loss: 0.00125244
Iteration 23/25 | Loss: 0.00125244
Iteration 24/25 | Loss: 0.00125244
Iteration 25/25 | Loss: 0.00125244

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.22930491
Iteration 2/25 | Loss: 0.00117647
Iteration 3/25 | Loss: 0.00117647
Iteration 4/25 | Loss: 0.00117647
Iteration 5/25 | Loss: 0.00117647
Iteration 6/25 | Loss: 0.00117647
Iteration 7/25 | Loss: 0.00117646
Iteration 8/25 | Loss: 0.00117646
Iteration 9/25 | Loss: 0.00117646
Iteration 10/25 | Loss: 0.00117646
Iteration 11/25 | Loss: 0.00117646
Iteration 12/25 | Loss: 0.00117646
Iteration 13/25 | Loss: 0.00117646
Iteration 14/25 | Loss: 0.00117646
Iteration 15/25 | Loss: 0.00117646
Iteration 16/25 | Loss: 0.00117646
Iteration 17/25 | Loss: 0.00117646
Iteration 18/25 | Loss: 0.00117646
Iteration 19/25 | Loss: 0.00117646
Iteration 20/25 | Loss: 0.00117646
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0011764642549678683, 0.0011764642549678683, 0.0011764642549678683, 0.0011764642549678683, 0.0011764642549678683]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011764642549678683

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00117646
Iteration 2/1000 | Loss: 0.00007829
Iteration 3/1000 | Loss: 0.00029717
Iteration 4/1000 | Loss: 0.00018325
Iteration 5/1000 | Loss: 0.00007277
Iteration 6/1000 | Loss: 0.00007123
Iteration 7/1000 | Loss: 0.00006854
Iteration 8/1000 | Loss: 0.00005411
Iteration 9/1000 | Loss: 0.00004153
Iteration 10/1000 | Loss: 0.00003804
Iteration 11/1000 | Loss: 0.00003590
Iteration 12/1000 | Loss: 0.00003473
Iteration 13/1000 | Loss: 0.00003405
Iteration 14/1000 | Loss: 0.00003339
Iteration 15/1000 | Loss: 0.00003293
Iteration 16/1000 | Loss: 0.00017527
Iteration 17/1000 | Loss: 0.00005320
Iteration 18/1000 | Loss: 0.00003693
Iteration 19/1000 | Loss: 0.00003296
Iteration 20/1000 | Loss: 0.00003154
Iteration 21/1000 | Loss: 0.00003085
Iteration 22/1000 | Loss: 0.00003025
Iteration 23/1000 | Loss: 0.00002989
Iteration 24/1000 | Loss: 0.00002976
Iteration 25/1000 | Loss: 0.00002956
Iteration 26/1000 | Loss: 0.00002938
Iteration 27/1000 | Loss: 0.00002931
Iteration 28/1000 | Loss: 0.00002931
Iteration 29/1000 | Loss: 0.00002931
Iteration 30/1000 | Loss: 0.00002931
Iteration 31/1000 | Loss: 0.00002930
Iteration 32/1000 | Loss: 0.00002930
Iteration 33/1000 | Loss: 0.00002930
Iteration 34/1000 | Loss: 0.00002930
Iteration 35/1000 | Loss: 0.00002930
Iteration 36/1000 | Loss: 0.00002930
Iteration 37/1000 | Loss: 0.00002930
Iteration 38/1000 | Loss: 0.00002930
Iteration 39/1000 | Loss: 0.00002930
Iteration 40/1000 | Loss: 0.00002930
Iteration 41/1000 | Loss: 0.00002930
Iteration 42/1000 | Loss: 0.00002930
Iteration 43/1000 | Loss: 0.00002930
Iteration 44/1000 | Loss: 0.00002930
Iteration 45/1000 | Loss: 0.00002927
Iteration 46/1000 | Loss: 0.00002927
Iteration 47/1000 | Loss: 0.00002925
Iteration 48/1000 | Loss: 0.00002924
Iteration 49/1000 | Loss: 0.00002923
Iteration 50/1000 | Loss: 0.00002922
Iteration 51/1000 | Loss: 0.00002922
Iteration 52/1000 | Loss: 0.00002922
Iteration 53/1000 | Loss: 0.00002921
Iteration 54/1000 | Loss: 0.00002921
Iteration 55/1000 | Loss: 0.00002921
Iteration 56/1000 | Loss: 0.00002921
Iteration 57/1000 | Loss: 0.00002921
Iteration 58/1000 | Loss: 0.00002921
Iteration 59/1000 | Loss: 0.00002921
Iteration 60/1000 | Loss: 0.00002920
Iteration 61/1000 | Loss: 0.00002920
Iteration 62/1000 | Loss: 0.00002916
Iteration 63/1000 | Loss: 0.00002916
Iteration 64/1000 | Loss: 0.00002916
Iteration 65/1000 | Loss: 0.00002915
Iteration 66/1000 | Loss: 0.00002913
Iteration 67/1000 | Loss: 0.00002913
Iteration 68/1000 | Loss: 0.00002912
Iteration 69/1000 | Loss: 0.00002912
Iteration 70/1000 | Loss: 0.00002911
Iteration 71/1000 | Loss: 0.00002911
Iteration 72/1000 | Loss: 0.00002909
Iteration 73/1000 | Loss: 0.00002909
Iteration 74/1000 | Loss: 0.00002908
Iteration 75/1000 | Loss: 0.00002908
Iteration 76/1000 | Loss: 0.00002908
Iteration 77/1000 | Loss: 0.00002908
Iteration 78/1000 | Loss: 0.00002907
Iteration 79/1000 | Loss: 0.00002907
Iteration 80/1000 | Loss: 0.00002907
Iteration 81/1000 | Loss: 0.00002907
Iteration 82/1000 | Loss: 0.00002907
Iteration 83/1000 | Loss: 0.00002907
Iteration 84/1000 | Loss: 0.00002907
Iteration 85/1000 | Loss: 0.00002907
Iteration 86/1000 | Loss: 0.00002906
Iteration 87/1000 | Loss: 0.00002906
Iteration 88/1000 | Loss: 0.00002906
Iteration 89/1000 | Loss: 0.00002906
Iteration 90/1000 | Loss: 0.00002906
Iteration 91/1000 | Loss: 0.00002905
Iteration 92/1000 | Loss: 0.00002905
Iteration 93/1000 | Loss: 0.00002905
Iteration 94/1000 | Loss: 0.00002905
Iteration 95/1000 | Loss: 0.00002905
Iteration 96/1000 | Loss: 0.00002905
Iteration 97/1000 | Loss: 0.00002905
Iteration 98/1000 | Loss: 0.00002905
Iteration 99/1000 | Loss: 0.00002905
Iteration 100/1000 | Loss: 0.00002905
Iteration 101/1000 | Loss: 0.00002905
Iteration 102/1000 | Loss: 0.00002905
Iteration 103/1000 | Loss: 0.00002905
Iteration 104/1000 | Loss: 0.00002905
Iteration 105/1000 | Loss: 0.00002905
Iteration 106/1000 | Loss: 0.00002905
Iteration 107/1000 | Loss: 0.00002905
Iteration 108/1000 | Loss: 0.00002905
Iteration 109/1000 | Loss: 0.00002904
Iteration 110/1000 | Loss: 0.00002904
Iteration 111/1000 | Loss: 0.00002904
Iteration 112/1000 | Loss: 0.00002904
Iteration 113/1000 | Loss: 0.00002904
Iteration 114/1000 | Loss: 0.00002904
Iteration 115/1000 | Loss: 0.00002903
Iteration 116/1000 | Loss: 0.00002903
Iteration 117/1000 | Loss: 0.00002903
Iteration 118/1000 | Loss: 0.00002903
Iteration 119/1000 | Loss: 0.00002903
Iteration 120/1000 | Loss: 0.00002902
Iteration 121/1000 | Loss: 0.00002902
Iteration 122/1000 | Loss: 0.00002902
Iteration 123/1000 | Loss: 0.00002902
Iteration 124/1000 | Loss: 0.00002902
Iteration 125/1000 | Loss: 0.00002902
Iteration 126/1000 | Loss: 0.00002902
Iteration 127/1000 | Loss: 0.00002902
Iteration 128/1000 | Loss: 0.00002901
Iteration 129/1000 | Loss: 0.00002901
Iteration 130/1000 | Loss: 0.00002901
Iteration 131/1000 | Loss: 0.00002901
Iteration 132/1000 | Loss: 0.00002901
Iteration 133/1000 | Loss: 0.00002901
Iteration 134/1000 | Loss: 0.00002901
Iteration 135/1000 | Loss: 0.00002901
Iteration 136/1000 | Loss: 0.00002901
Iteration 137/1000 | Loss: 0.00002901
Iteration 138/1000 | Loss: 0.00002901
Iteration 139/1000 | Loss: 0.00002901
Iteration 140/1000 | Loss: 0.00002900
Iteration 141/1000 | Loss: 0.00002900
Iteration 142/1000 | Loss: 0.00002900
Iteration 143/1000 | Loss: 0.00002900
Iteration 144/1000 | Loss: 0.00002900
Iteration 145/1000 | Loss: 0.00002900
Iteration 146/1000 | Loss: 0.00002900
Iteration 147/1000 | Loss: 0.00002900
Iteration 148/1000 | Loss: 0.00002900
Iteration 149/1000 | Loss: 0.00002900
Iteration 150/1000 | Loss: 0.00002900
Iteration 151/1000 | Loss: 0.00002900
Iteration 152/1000 | Loss: 0.00002900
Iteration 153/1000 | Loss: 0.00002900
Iteration 154/1000 | Loss: 0.00002900
Iteration 155/1000 | Loss: 0.00002900
Iteration 156/1000 | Loss: 0.00002900
Iteration 157/1000 | Loss: 0.00002900
Iteration 158/1000 | Loss: 0.00002900
Iteration 159/1000 | Loss: 0.00002900
Iteration 160/1000 | Loss: 0.00002899
Iteration 161/1000 | Loss: 0.00002899
Iteration 162/1000 | Loss: 0.00002899
Iteration 163/1000 | Loss: 0.00002899
Iteration 164/1000 | Loss: 0.00002899
Iteration 165/1000 | Loss: 0.00002899
Iteration 166/1000 | Loss: 0.00002899
Iteration 167/1000 | Loss: 0.00002899
Iteration 168/1000 | Loss: 0.00002899
Iteration 169/1000 | Loss: 0.00002899
Iteration 170/1000 | Loss: 0.00002899
Iteration 171/1000 | Loss: 0.00002899
Iteration 172/1000 | Loss: 0.00002899
Iteration 173/1000 | Loss: 0.00002899
Iteration 174/1000 | Loss: 0.00002899
Iteration 175/1000 | Loss: 0.00002898
Iteration 176/1000 | Loss: 0.00002898
Iteration 177/1000 | Loss: 0.00002898
Iteration 178/1000 | Loss: 0.00002898
Iteration 179/1000 | Loss: 0.00002898
Iteration 180/1000 | Loss: 0.00002898
Iteration 181/1000 | Loss: 0.00002898
Iteration 182/1000 | Loss: 0.00002898
Iteration 183/1000 | Loss: 0.00002898
Iteration 184/1000 | Loss: 0.00002898
Iteration 185/1000 | Loss: 0.00002898
Iteration 186/1000 | Loss: 0.00002898
Iteration 187/1000 | Loss: 0.00002898
Iteration 188/1000 | Loss: 0.00002898
Iteration 189/1000 | Loss: 0.00002898
Iteration 190/1000 | Loss: 0.00002898
Iteration 191/1000 | Loss: 0.00002898
Iteration 192/1000 | Loss: 0.00002898
Iteration 193/1000 | Loss: 0.00002898
Iteration 194/1000 | Loss: 0.00002898
Iteration 195/1000 | Loss: 0.00002898
Iteration 196/1000 | Loss: 0.00002898
Iteration 197/1000 | Loss: 0.00002898
Iteration 198/1000 | Loss: 0.00002898
Iteration 199/1000 | Loss: 0.00002898
Iteration 200/1000 | Loss: 0.00002898
Iteration 201/1000 | Loss: 0.00002898
Iteration 202/1000 | Loss: 0.00002898
Iteration 203/1000 | Loss: 0.00002898
Iteration 204/1000 | Loss: 0.00002898
Iteration 205/1000 | Loss: 0.00002898
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 205. Stopping optimization.
Last 5 losses: [2.8976195608265698e-05, 2.8976195608265698e-05, 2.8976195608265698e-05, 2.8976195608265698e-05, 2.8976195608265698e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.8976195608265698e-05

Optimization complete. Final v2v error: 3.8084006309509277 mm

Highest mean error: 10.047515869140625 mm for frame 115

Lowest mean error: 2.897559404373169 mm for frame 1

Saving results

Total time: 80.05775952339172
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_36_nl_5493/0005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_nl_5493/0005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_nl_5493/0005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00498796
Iteration 2/25 | Loss: 0.00127724
Iteration 3/25 | Loss: 0.00117635
Iteration 4/25 | Loss: 0.00115400
Iteration 5/25 | Loss: 0.00114657
Iteration 6/25 | Loss: 0.00114450
Iteration 7/25 | Loss: 0.00114435
Iteration 8/25 | Loss: 0.00114435
Iteration 9/25 | Loss: 0.00114435
Iteration 10/25 | Loss: 0.00114432
Iteration 11/25 | Loss: 0.00114432
Iteration 12/25 | Loss: 0.00114432
Iteration 13/25 | Loss: 0.00114432
Iteration 14/25 | Loss: 0.00114432
Iteration 15/25 | Loss: 0.00114432
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0011443216353654861, 0.0011443216353654861, 0.0011443216353654861, 0.0011443216353654861, 0.0011443216353654861]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011443216353654861

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.62745786
Iteration 2/25 | Loss: 0.00109002
Iteration 3/25 | Loss: 0.00109002
Iteration 4/25 | Loss: 0.00109002
Iteration 5/25 | Loss: 0.00109002
Iteration 6/25 | Loss: 0.00109002
Iteration 7/25 | Loss: 0.00109002
Iteration 8/25 | Loss: 0.00109002
Iteration 9/25 | Loss: 0.00109002
Iteration 10/25 | Loss: 0.00109002
Iteration 11/25 | Loss: 0.00109002
Iteration 12/25 | Loss: 0.00109002
Iteration 13/25 | Loss: 0.00109002
Iteration 14/25 | Loss: 0.00109002
Iteration 15/25 | Loss: 0.00109002
Iteration 16/25 | Loss: 0.00109002
Iteration 17/25 | Loss: 0.00109002
Iteration 18/25 | Loss: 0.00109002
Iteration 19/25 | Loss: 0.00109002
Iteration 20/25 | Loss: 0.00109002
Iteration 21/25 | Loss: 0.00109002
Iteration 22/25 | Loss: 0.00109002
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0010900188935920596, 0.0010900188935920596, 0.0010900188935920596, 0.0010900188935920596, 0.0010900188935920596]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010900188935920596

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00109002
Iteration 2/1000 | Loss: 0.00004012
Iteration 3/1000 | Loss: 0.00002211
Iteration 4/1000 | Loss: 0.00001996
Iteration 5/1000 | Loss: 0.00001830
Iteration 6/1000 | Loss: 0.00001734
Iteration 7/1000 | Loss: 0.00001659
Iteration 8/1000 | Loss: 0.00001598
Iteration 9/1000 | Loss: 0.00001543
Iteration 10/1000 | Loss: 0.00001507
Iteration 11/1000 | Loss: 0.00001487
Iteration 12/1000 | Loss: 0.00001481
Iteration 13/1000 | Loss: 0.00001480
Iteration 14/1000 | Loss: 0.00001480
Iteration 15/1000 | Loss: 0.00001480
Iteration 16/1000 | Loss: 0.00001476
Iteration 17/1000 | Loss: 0.00001475
Iteration 18/1000 | Loss: 0.00001475
Iteration 19/1000 | Loss: 0.00001475
Iteration 20/1000 | Loss: 0.00001474
Iteration 21/1000 | Loss: 0.00001474
Iteration 22/1000 | Loss: 0.00001474
Iteration 23/1000 | Loss: 0.00001473
Iteration 24/1000 | Loss: 0.00001473
Iteration 25/1000 | Loss: 0.00001472
Iteration 26/1000 | Loss: 0.00001468
Iteration 27/1000 | Loss: 0.00001468
Iteration 28/1000 | Loss: 0.00001467
Iteration 29/1000 | Loss: 0.00001467
Iteration 30/1000 | Loss: 0.00001465
Iteration 31/1000 | Loss: 0.00001465
Iteration 32/1000 | Loss: 0.00001465
Iteration 33/1000 | Loss: 0.00001465
Iteration 34/1000 | Loss: 0.00001464
Iteration 35/1000 | Loss: 0.00001464
Iteration 36/1000 | Loss: 0.00001464
Iteration 37/1000 | Loss: 0.00001464
Iteration 38/1000 | Loss: 0.00001464
Iteration 39/1000 | Loss: 0.00001464
Iteration 40/1000 | Loss: 0.00001464
Iteration 41/1000 | Loss: 0.00001464
Iteration 42/1000 | Loss: 0.00001464
Iteration 43/1000 | Loss: 0.00001464
Iteration 44/1000 | Loss: 0.00001464
Iteration 45/1000 | Loss: 0.00001464
Iteration 46/1000 | Loss: 0.00001464
Iteration 47/1000 | Loss: 0.00001464
Iteration 48/1000 | Loss: 0.00001463
Iteration 49/1000 | Loss: 0.00001463
Iteration 50/1000 | Loss: 0.00001462
Iteration 51/1000 | Loss: 0.00001462
Iteration 52/1000 | Loss: 0.00001462
Iteration 53/1000 | Loss: 0.00001461
Iteration 54/1000 | Loss: 0.00001461
Iteration 55/1000 | Loss: 0.00001460
Iteration 56/1000 | Loss: 0.00001460
Iteration 57/1000 | Loss: 0.00001460
Iteration 58/1000 | Loss: 0.00001460
Iteration 59/1000 | Loss: 0.00001460
Iteration 60/1000 | Loss: 0.00001460
Iteration 61/1000 | Loss: 0.00001460
Iteration 62/1000 | Loss: 0.00001460
Iteration 63/1000 | Loss: 0.00001460
Iteration 64/1000 | Loss: 0.00001460
Iteration 65/1000 | Loss: 0.00001460
Iteration 66/1000 | Loss: 0.00001460
Iteration 67/1000 | Loss: 0.00001460
Iteration 68/1000 | Loss: 0.00001460
Iteration 69/1000 | Loss: 0.00001460
Iteration 70/1000 | Loss: 0.00001460
Iteration 71/1000 | Loss: 0.00001460
Iteration 72/1000 | Loss: 0.00001460
Iteration 73/1000 | Loss: 0.00001460
Iteration 74/1000 | Loss: 0.00001460
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 74. Stopping optimization.
Last 5 losses: [1.4599208952859044e-05, 1.4599208952859044e-05, 1.4599208952859044e-05, 1.4599208952859044e-05, 1.4599208952859044e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4599208952859044e-05

Optimization complete. Final v2v error: 3.3107781410217285 mm

Highest mean error: 3.6361610889434814 mm for frame 136

Lowest mean error: 3.0733253955841064 mm for frame 0

Saving results

Total time: 30.99134612083435
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_36_nl_5493/0003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_nl_5493/0003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_nl_5493/0003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00973382
Iteration 2/25 | Loss: 0.00153299
Iteration 3/25 | Loss: 0.00133365
Iteration 4/25 | Loss: 0.00130055
Iteration 5/25 | Loss: 0.00129281
Iteration 6/25 | Loss: 0.00126528
Iteration 7/25 | Loss: 0.00124936
Iteration 8/25 | Loss: 0.00124612
Iteration 9/25 | Loss: 0.00124557
Iteration 10/25 | Loss: 0.00124528
Iteration 11/25 | Loss: 0.00124510
Iteration 12/25 | Loss: 0.00124508
Iteration 13/25 | Loss: 0.00124507
Iteration 14/25 | Loss: 0.00124507
Iteration 15/25 | Loss: 0.00124507
Iteration 16/25 | Loss: 0.00124507
Iteration 17/25 | Loss: 0.00124507
Iteration 18/25 | Loss: 0.00124506
Iteration 19/25 | Loss: 0.00124506
Iteration 20/25 | Loss: 0.00124506
Iteration 21/25 | Loss: 0.00124506
Iteration 22/25 | Loss: 0.00124506
Iteration 23/25 | Loss: 0.00124506
Iteration 24/25 | Loss: 0.00124506
Iteration 25/25 | Loss: 0.00124506

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.19062471
Iteration 2/25 | Loss: 0.00127757
Iteration 3/25 | Loss: 0.00127757
Iteration 4/25 | Loss: 0.00127757
Iteration 5/25 | Loss: 0.00127757
Iteration 6/25 | Loss: 0.00127757
Iteration 7/25 | Loss: 0.00127757
Iteration 8/25 | Loss: 0.00127757
Iteration 9/25 | Loss: 0.00127757
Iteration 10/25 | Loss: 0.00127757
Iteration 11/25 | Loss: 0.00127757
Iteration 12/25 | Loss: 0.00127757
Iteration 13/25 | Loss: 0.00127757
Iteration 14/25 | Loss: 0.00127757
Iteration 15/25 | Loss: 0.00127757
Iteration 16/25 | Loss: 0.00127757
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.001277568400837481, 0.001277568400837481, 0.001277568400837481, 0.001277568400837481, 0.001277568400837481]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001277568400837481

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00127757
Iteration 2/1000 | Loss: 0.00006825
Iteration 3/1000 | Loss: 0.00003870
Iteration 4/1000 | Loss: 0.00003275
Iteration 5/1000 | Loss: 0.00003057
Iteration 6/1000 | Loss: 0.00002892
Iteration 7/1000 | Loss: 0.00002830
Iteration 8/1000 | Loss: 0.00002766
Iteration 9/1000 | Loss: 0.00002734
Iteration 10/1000 | Loss: 0.00002708
Iteration 11/1000 | Loss: 0.00002683
Iteration 12/1000 | Loss: 0.00002660
Iteration 13/1000 | Loss: 0.00002657
Iteration 14/1000 | Loss: 0.00002652
Iteration 15/1000 | Loss: 0.00002647
Iteration 16/1000 | Loss: 0.00002647
Iteration 17/1000 | Loss: 0.00002644
Iteration 18/1000 | Loss: 0.00002641
Iteration 19/1000 | Loss: 0.00002641
Iteration 20/1000 | Loss: 0.00002640
Iteration 21/1000 | Loss: 0.00002640
Iteration 22/1000 | Loss: 0.00002640
Iteration 23/1000 | Loss: 0.00002637
Iteration 24/1000 | Loss: 0.00002637
Iteration 25/1000 | Loss: 0.00002637
Iteration 26/1000 | Loss: 0.00002637
Iteration 27/1000 | Loss: 0.00002636
Iteration 28/1000 | Loss: 0.00002636
Iteration 29/1000 | Loss: 0.00002636
Iteration 30/1000 | Loss: 0.00002635
Iteration 31/1000 | Loss: 0.00002635
Iteration 32/1000 | Loss: 0.00002632
Iteration 33/1000 | Loss: 0.00002632
Iteration 34/1000 | Loss: 0.00002631
Iteration 35/1000 | Loss: 0.00002631
Iteration 36/1000 | Loss: 0.00002631
Iteration 37/1000 | Loss: 0.00002631
Iteration 38/1000 | Loss: 0.00002631
Iteration 39/1000 | Loss: 0.00002631
Iteration 40/1000 | Loss: 0.00002631
Iteration 41/1000 | Loss: 0.00002631
Iteration 42/1000 | Loss: 0.00002631
Iteration 43/1000 | Loss: 0.00002631
Iteration 44/1000 | Loss: 0.00002631
Iteration 45/1000 | Loss: 0.00002631
Iteration 46/1000 | Loss: 0.00002630
Iteration 47/1000 | Loss: 0.00002630
Iteration 48/1000 | Loss: 0.00002630
Iteration 49/1000 | Loss: 0.00002630
Iteration 50/1000 | Loss: 0.00002630
Iteration 51/1000 | Loss: 0.00002629
Iteration 52/1000 | Loss: 0.00002629
Iteration 53/1000 | Loss: 0.00002628
Iteration 54/1000 | Loss: 0.00002628
Iteration 55/1000 | Loss: 0.00002628
Iteration 56/1000 | Loss: 0.00002628
Iteration 57/1000 | Loss: 0.00002628
Iteration 58/1000 | Loss: 0.00002628
Iteration 59/1000 | Loss: 0.00002628
Iteration 60/1000 | Loss: 0.00002628
Iteration 61/1000 | Loss: 0.00002628
Iteration 62/1000 | Loss: 0.00002627
Iteration 63/1000 | Loss: 0.00002627
Iteration 64/1000 | Loss: 0.00002627
Iteration 65/1000 | Loss: 0.00002627
Iteration 66/1000 | Loss: 0.00002627
Iteration 67/1000 | Loss: 0.00002626
Iteration 68/1000 | Loss: 0.00002626
Iteration 69/1000 | Loss: 0.00002626
Iteration 70/1000 | Loss: 0.00002625
Iteration 71/1000 | Loss: 0.00002625
Iteration 72/1000 | Loss: 0.00002625
Iteration 73/1000 | Loss: 0.00002625
Iteration 74/1000 | Loss: 0.00002625
Iteration 75/1000 | Loss: 0.00002625
Iteration 76/1000 | Loss: 0.00002625
Iteration 77/1000 | Loss: 0.00002625
Iteration 78/1000 | Loss: 0.00002625
Iteration 79/1000 | Loss: 0.00002625
Iteration 80/1000 | Loss: 0.00002625
Iteration 81/1000 | Loss: 0.00002625
Iteration 82/1000 | Loss: 0.00002625
Iteration 83/1000 | Loss: 0.00002625
Iteration 84/1000 | Loss: 0.00002625
Iteration 85/1000 | Loss: 0.00002624
Iteration 86/1000 | Loss: 0.00002624
Iteration 87/1000 | Loss: 0.00002624
Iteration 88/1000 | Loss: 0.00002624
Iteration 89/1000 | Loss: 0.00002624
Iteration 90/1000 | Loss: 0.00002624
Iteration 91/1000 | Loss: 0.00002624
Iteration 92/1000 | Loss: 0.00002623
Iteration 93/1000 | Loss: 0.00002623
Iteration 94/1000 | Loss: 0.00002623
Iteration 95/1000 | Loss: 0.00002622
Iteration 96/1000 | Loss: 0.00002622
Iteration 97/1000 | Loss: 0.00002622
Iteration 98/1000 | Loss: 0.00002622
Iteration 99/1000 | Loss: 0.00002622
Iteration 100/1000 | Loss: 0.00002622
Iteration 101/1000 | Loss: 0.00002621
Iteration 102/1000 | Loss: 0.00002621
Iteration 103/1000 | Loss: 0.00002621
Iteration 104/1000 | Loss: 0.00002621
Iteration 105/1000 | Loss: 0.00002621
Iteration 106/1000 | Loss: 0.00002621
Iteration 107/1000 | Loss: 0.00002620
Iteration 108/1000 | Loss: 0.00002620
Iteration 109/1000 | Loss: 0.00002620
Iteration 110/1000 | Loss: 0.00002619
Iteration 111/1000 | Loss: 0.00002619
Iteration 112/1000 | Loss: 0.00002619
Iteration 113/1000 | Loss: 0.00002619
Iteration 114/1000 | Loss: 0.00002619
Iteration 115/1000 | Loss: 0.00002619
Iteration 116/1000 | Loss: 0.00002619
Iteration 117/1000 | Loss: 0.00002618
Iteration 118/1000 | Loss: 0.00002618
Iteration 119/1000 | Loss: 0.00002618
Iteration 120/1000 | Loss: 0.00002618
Iteration 121/1000 | Loss: 0.00002618
Iteration 122/1000 | Loss: 0.00002618
Iteration 123/1000 | Loss: 0.00002618
Iteration 124/1000 | Loss: 0.00002618
Iteration 125/1000 | Loss: 0.00002618
Iteration 126/1000 | Loss: 0.00002618
Iteration 127/1000 | Loss: 0.00002618
Iteration 128/1000 | Loss: 0.00002617
Iteration 129/1000 | Loss: 0.00002617
Iteration 130/1000 | Loss: 0.00002617
Iteration 131/1000 | Loss: 0.00002617
Iteration 132/1000 | Loss: 0.00002617
Iteration 133/1000 | Loss: 0.00002617
Iteration 134/1000 | Loss: 0.00002616
Iteration 135/1000 | Loss: 0.00002616
Iteration 136/1000 | Loss: 0.00002616
Iteration 137/1000 | Loss: 0.00002616
Iteration 138/1000 | Loss: 0.00002616
Iteration 139/1000 | Loss: 0.00002616
Iteration 140/1000 | Loss: 0.00002616
Iteration 141/1000 | Loss: 0.00002616
Iteration 142/1000 | Loss: 0.00002616
Iteration 143/1000 | Loss: 0.00002616
Iteration 144/1000 | Loss: 0.00002616
Iteration 145/1000 | Loss: 0.00002616
Iteration 146/1000 | Loss: 0.00002616
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 146. Stopping optimization.
Last 5 losses: [2.6158510081586428e-05, 2.6158510081586428e-05, 2.6158510081586428e-05, 2.6158510081586428e-05, 2.6158510081586428e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.6158510081586428e-05

Optimization complete. Final v2v error: 4.291431427001953 mm

Highest mean error: 4.836005210876465 mm for frame 206

Lowest mean error: 3.759305715560913 mm for frame 67

Saving results

Total time: 54.14285612106323
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_36_nl_5493/0019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_nl_5493/0019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_nl_5493/0019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00869328
Iteration 2/25 | Loss: 0.00137160
Iteration 3/25 | Loss: 0.00122592
Iteration 4/25 | Loss: 0.00119276
Iteration 5/25 | Loss: 0.00118481
Iteration 6/25 | Loss: 0.00118296
Iteration 7/25 | Loss: 0.00118267
Iteration 8/25 | Loss: 0.00118267
Iteration 9/25 | Loss: 0.00118267
Iteration 10/25 | Loss: 0.00118267
Iteration 11/25 | Loss: 0.00118267
Iteration 12/25 | Loss: 0.00118267
Iteration 13/25 | Loss: 0.00118267
Iteration 14/25 | Loss: 0.00118267
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0011826696572825313, 0.0011826696572825313, 0.0011826696572825313, 0.0011826696572825313, 0.0011826696572825313]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011826696572825313

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.22606409
Iteration 2/25 | Loss: 0.00113341
Iteration 3/25 | Loss: 0.00113340
Iteration 4/25 | Loss: 0.00113340
Iteration 5/25 | Loss: 0.00113340
Iteration 6/25 | Loss: 0.00113340
Iteration 7/25 | Loss: 0.00113340
Iteration 8/25 | Loss: 0.00113340
Iteration 9/25 | Loss: 0.00113340
Iteration 10/25 | Loss: 0.00113340
Iteration 11/25 | Loss: 0.00113340
Iteration 12/25 | Loss: 0.00113340
Iteration 13/25 | Loss: 0.00113340
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0011334024602547288, 0.0011334024602547288, 0.0011334024602547288, 0.0011334024602547288, 0.0011334024602547288]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011334024602547288

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00113340
Iteration 2/1000 | Loss: 0.00006266
Iteration 3/1000 | Loss: 0.00003241
Iteration 4/1000 | Loss: 0.00002609
Iteration 5/1000 | Loss: 0.00002387
Iteration 6/1000 | Loss: 0.00002236
Iteration 7/1000 | Loss: 0.00002140
Iteration 8/1000 | Loss: 0.00002063
Iteration 9/1000 | Loss: 0.00002020
Iteration 10/1000 | Loss: 0.00001980
Iteration 11/1000 | Loss: 0.00001935
Iteration 12/1000 | Loss: 0.00001903
Iteration 13/1000 | Loss: 0.00001881
Iteration 14/1000 | Loss: 0.00001870
Iteration 15/1000 | Loss: 0.00001866
Iteration 16/1000 | Loss: 0.00001866
Iteration 17/1000 | Loss: 0.00001865
Iteration 18/1000 | Loss: 0.00001861
Iteration 19/1000 | Loss: 0.00001857
Iteration 20/1000 | Loss: 0.00001856
Iteration 21/1000 | Loss: 0.00001856
Iteration 22/1000 | Loss: 0.00001855
Iteration 23/1000 | Loss: 0.00001854
Iteration 24/1000 | Loss: 0.00001854
Iteration 25/1000 | Loss: 0.00001852
Iteration 26/1000 | Loss: 0.00001852
Iteration 27/1000 | Loss: 0.00001851
Iteration 28/1000 | Loss: 0.00001851
Iteration 29/1000 | Loss: 0.00001851
Iteration 30/1000 | Loss: 0.00001851
Iteration 31/1000 | Loss: 0.00001851
Iteration 32/1000 | Loss: 0.00001851
Iteration 33/1000 | Loss: 0.00001851
Iteration 34/1000 | Loss: 0.00001851
Iteration 35/1000 | Loss: 0.00001850
Iteration 36/1000 | Loss: 0.00001850
Iteration 37/1000 | Loss: 0.00001850
Iteration 38/1000 | Loss: 0.00001849
Iteration 39/1000 | Loss: 0.00001849
Iteration 40/1000 | Loss: 0.00001848
Iteration 41/1000 | Loss: 0.00001848
Iteration 42/1000 | Loss: 0.00001847
Iteration 43/1000 | Loss: 0.00001847
Iteration 44/1000 | Loss: 0.00001846
Iteration 45/1000 | Loss: 0.00001846
Iteration 46/1000 | Loss: 0.00001846
Iteration 47/1000 | Loss: 0.00001846
Iteration 48/1000 | Loss: 0.00001845
Iteration 49/1000 | Loss: 0.00001845
Iteration 50/1000 | Loss: 0.00001845
Iteration 51/1000 | Loss: 0.00001844
Iteration 52/1000 | Loss: 0.00001844
Iteration 53/1000 | Loss: 0.00001844
Iteration 54/1000 | Loss: 0.00001844
Iteration 55/1000 | Loss: 0.00001844
Iteration 56/1000 | Loss: 0.00001844
Iteration 57/1000 | Loss: 0.00001843
Iteration 58/1000 | Loss: 0.00001843
Iteration 59/1000 | Loss: 0.00001843
Iteration 60/1000 | Loss: 0.00001843
Iteration 61/1000 | Loss: 0.00001843
Iteration 62/1000 | Loss: 0.00001843
Iteration 63/1000 | Loss: 0.00001842
Iteration 64/1000 | Loss: 0.00001842
Iteration 65/1000 | Loss: 0.00001842
Iteration 66/1000 | Loss: 0.00001842
Iteration 67/1000 | Loss: 0.00001842
Iteration 68/1000 | Loss: 0.00001842
Iteration 69/1000 | Loss: 0.00001842
Iteration 70/1000 | Loss: 0.00001841
Iteration 71/1000 | Loss: 0.00001841
Iteration 72/1000 | Loss: 0.00001841
Iteration 73/1000 | Loss: 0.00001841
Iteration 74/1000 | Loss: 0.00001841
Iteration 75/1000 | Loss: 0.00001841
Iteration 76/1000 | Loss: 0.00001841
Iteration 77/1000 | Loss: 0.00001841
Iteration 78/1000 | Loss: 0.00001841
Iteration 79/1000 | Loss: 0.00001841
Iteration 80/1000 | Loss: 0.00001841
Iteration 81/1000 | Loss: 0.00001840
Iteration 82/1000 | Loss: 0.00001840
Iteration 83/1000 | Loss: 0.00001840
Iteration 84/1000 | Loss: 0.00001840
Iteration 85/1000 | Loss: 0.00001840
Iteration 86/1000 | Loss: 0.00001840
Iteration 87/1000 | Loss: 0.00001840
Iteration 88/1000 | Loss: 0.00001840
Iteration 89/1000 | Loss: 0.00001840
Iteration 90/1000 | Loss: 0.00001840
Iteration 91/1000 | Loss: 0.00001840
Iteration 92/1000 | Loss: 0.00001840
Iteration 93/1000 | Loss: 0.00001840
Iteration 94/1000 | Loss: 0.00001840
Iteration 95/1000 | Loss: 0.00001840
Iteration 96/1000 | Loss: 0.00001839
Iteration 97/1000 | Loss: 0.00001839
Iteration 98/1000 | Loss: 0.00001839
Iteration 99/1000 | Loss: 0.00001839
Iteration 100/1000 | Loss: 0.00001839
Iteration 101/1000 | Loss: 0.00001839
Iteration 102/1000 | Loss: 0.00001839
Iteration 103/1000 | Loss: 0.00001839
Iteration 104/1000 | Loss: 0.00001839
Iteration 105/1000 | Loss: 0.00001839
Iteration 106/1000 | Loss: 0.00001839
Iteration 107/1000 | Loss: 0.00001839
Iteration 108/1000 | Loss: 0.00001839
Iteration 109/1000 | Loss: 0.00001838
Iteration 110/1000 | Loss: 0.00001838
Iteration 111/1000 | Loss: 0.00001838
Iteration 112/1000 | Loss: 0.00001838
Iteration 113/1000 | Loss: 0.00001838
Iteration 114/1000 | Loss: 0.00001838
Iteration 115/1000 | Loss: 0.00001838
Iteration 116/1000 | Loss: 0.00001838
Iteration 117/1000 | Loss: 0.00001838
Iteration 118/1000 | Loss: 0.00001838
Iteration 119/1000 | Loss: 0.00001838
Iteration 120/1000 | Loss: 0.00001838
Iteration 121/1000 | Loss: 0.00001838
Iteration 122/1000 | Loss: 0.00001838
Iteration 123/1000 | Loss: 0.00001838
Iteration 124/1000 | Loss: 0.00001838
Iteration 125/1000 | Loss: 0.00001838
Iteration 126/1000 | Loss: 0.00001838
Iteration 127/1000 | Loss: 0.00001838
Iteration 128/1000 | Loss: 0.00001838
Iteration 129/1000 | Loss: 0.00001838
Iteration 130/1000 | Loss: 0.00001838
Iteration 131/1000 | Loss: 0.00001838
Iteration 132/1000 | Loss: 0.00001838
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 132. Stopping optimization.
Last 5 losses: [1.8381408153800294e-05, 1.8381408153800294e-05, 1.8381408153800294e-05, 1.8381408153800294e-05, 1.8381408153800294e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8381408153800294e-05

Optimization complete. Final v2v error: 3.6881420612335205 mm

Highest mean error: 4.159411907196045 mm for frame 29

Lowest mean error: 3.0574426651000977 mm for frame 6

Saving results

Total time: 37.175410747528076
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_36_nl_5493/0008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_nl_5493/0008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_nl_5493/0008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00493579
Iteration 2/25 | Loss: 0.00137319
Iteration 3/25 | Loss: 0.00121399
Iteration 4/25 | Loss: 0.00119617
Iteration 5/25 | Loss: 0.00119370
Iteration 6/25 | Loss: 0.00119370
Iteration 7/25 | Loss: 0.00119370
Iteration 8/25 | Loss: 0.00119370
Iteration 9/25 | Loss: 0.00119370
Iteration 10/25 | Loss: 0.00119370
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0011937031522393227, 0.0011937031522393227, 0.0011937031522393227, 0.0011937031522393227, 0.0011937031522393227]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011937031522393227

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.26573503
Iteration 2/25 | Loss: 0.00120507
Iteration 3/25 | Loss: 0.00120506
Iteration 4/25 | Loss: 0.00120506
Iteration 5/25 | Loss: 0.00120506
Iteration 6/25 | Loss: 0.00120506
Iteration 7/25 | Loss: 0.00120506
Iteration 8/25 | Loss: 0.00120506
Iteration 9/25 | Loss: 0.00120506
Iteration 10/25 | Loss: 0.00120506
Iteration 11/25 | Loss: 0.00120506
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012050624936819077, 0.0012050624936819077, 0.0012050624936819077, 0.0012050624936819077, 0.0012050624936819077]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012050624936819077

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00120506
Iteration 2/1000 | Loss: 0.00003546
Iteration 3/1000 | Loss: 0.00001909
Iteration 4/1000 | Loss: 0.00001724
Iteration 5/1000 | Loss: 0.00001617
Iteration 6/1000 | Loss: 0.00001552
Iteration 7/1000 | Loss: 0.00001523
Iteration 8/1000 | Loss: 0.00001489
Iteration 9/1000 | Loss: 0.00001448
Iteration 10/1000 | Loss: 0.00001431
Iteration 11/1000 | Loss: 0.00001428
Iteration 12/1000 | Loss: 0.00001422
Iteration 13/1000 | Loss: 0.00001421
Iteration 14/1000 | Loss: 0.00001419
Iteration 15/1000 | Loss: 0.00001416
Iteration 16/1000 | Loss: 0.00001416
Iteration 17/1000 | Loss: 0.00001415
Iteration 18/1000 | Loss: 0.00001415
Iteration 19/1000 | Loss: 0.00001414
Iteration 20/1000 | Loss: 0.00001414
Iteration 21/1000 | Loss: 0.00001413
Iteration 22/1000 | Loss: 0.00001411
Iteration 23/1000 | Loss: 0.00001411
Iteration 24/1000 | Loss: 0.00001411
Iteration 25/1000 | Loss: 0.00001411
Iteration 26/1000 | Loss: 0.00001409
Iteration 27/1000 | Loss: 0.00001409
Iteration 28/1000 | Loss: 0.00001408
Iteration 29/1000 | Loss: 0.00001408
Iteration 30/1000 | Loss: 0.00001406
Iteration 31/1000 | Loss: 0.00001406
Iteration 32/1000 | Loss: 0.00001406
Iteration 33/1000 | Loss: 0.00001405
Iteration 34/1000 | Loss: 0.00001405
Iteration 35/1000 | Loss: 0.00001405
Iteration 36/1000 | Loss: 0.00001405
Iteration 37/1000 | Loss: 0.00001404
Iteration 38/1000 | Loss: 0.00001404
Iteration 39/1000 | Loss: 0.00001404
Iteration 40/1000 | Loss: 0.00001404
Iteration 41/1000 | Loss: 0.00001404
Iteration 42/1000 | Loss: 0.00001404
Iteration 43/1000 | Loss: 0.00001404
Iteration 44/1000 | Loss: 0.00001404
Iteration 45/1000 | Loss: 0.00001404
Iteration 46/1000 | Loss: 0.00001403
Iteration 47/1000 | Loss: 0.00001403
Iteration 48/1000 | Loss: 0.00001403
Iteration 49/1000 | Loss: 0.00001403
Iteration 50/1000 | Loss: 0.00001403
Iteration 51/1000 | Loss: 0.00001403
Iteration 52/1000 | Loss: 0.00001402
Iteration 53/1000 | Loss: 0.00001402
Iteration 54/1000 | Loss: 0.00001402
Iteration 55/1000 | Loss: 0.00001402
Iteration 56/1000 | Loss: 0.00001402
Iteration 57/1000 | Loss: 0.00001402
Iteration 58/1000 | Loss: 0.00001402
Iteration 59/1000 | Loss: 0.00001401
Iteration 60/1000 | Loss: 0.00001401
Iteration 61/1000 | Loss: 0.00001401
Iteration 62/1000 | Loss: 0.00001401
Iteration 63/1000 | Loss: 0.00001401
Iteration 64/1000 | Loss: 0.00001401
Iteration 65/1000 | Loss: 0.00001401
Iteration 66/1000 | Loss: 0.00001401
Iteration 67/1000 | Loss: 0.00001401
Iteration 68/1000 | Loss: 0.00001401
Iteration 69/1000 | Loss: 0.00001401
Iteration 70/1000 | Loss: 0.00001401
Iteration 71/1000 | Loss: 0.00001401
Iteration 72/1000 | Loss: 0.00001401
Iteration 73/1000 | Loss: 0.00001401
Iteration 74/1000 | Loss: 0.00001401
Iteration 75/1000 | Loss: 0.00001401
Iteration 76/1000 | Loss: 0.00001401
Iteration 77/1000 | Loss: 0.00001401
Iteration 78/1000 | Loss: 0.00001401
Iteration 79/1000 | Loss: 0.00001401
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 79. Stopping optimization.
Last 5 losses: [1.4007764548296109e-05, 1.4007764548296109e-05, 1.4007764548296109e-05, 1.4007764548296109e-05, 1.4007764548296109e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4007764548296109e-05

Optimization complete. Final v2v error: 3.22766375541687 mm

Highest mean error: 3.4297821521759033 mm for frame 219

Lowest mean error: 3.064811944961548 mm for frame 37

Saving results

Total time: 31.845866680145264
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_36_nl_5493/0018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_nl_5493/0018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_nl_5493/0018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00772301
Iteration 2/25 | Loss: 0.00152072
Iteration 3/25 | Loss: 0.00131036
Iteration 4/25 | Loss: 0.00127245
Iteration 5/25 | Loss: 0.00126224
Iteration 6/25 | Loss: 0.00125877
Iteration 7/25 | Loss: 0.00125857
Iteration 8/25 | Loss: 0.00125857
Iteration 9/25 | Loss: 0.00125857
Iteration 10/25 | Loss: 0.00125857
Iteration 11/25 | Loss: 0.00125857
Iteration 12/25 | Loss: 0.00125857
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0012585741933435202, 0.0012585741933435202, 0.0012585741933435202, 0.0012585741933435202, 0.0012585741933435202]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012585741933435202

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.49418807
Iteration 2/25 | Loss: 0.00163360
Iteration 3/25 | Loss: 0.00163360
Iteration 4/25 | Loss: 0.00163360
Iteration 5/25 | Loss: 0.00163360
Iteration 6/25 | Loss: 0.00163360
Iteration 7/25 | Loss: 0.00163360
Iteration 8/25 | Loss: 0.00163360
Iteration 9/25 | Loss: 0.00163360
Iteration 10/25 | Loss: 0.00163360
Iteration 11/25 | Loss: 0.00163360
Iteration 12/25 | Loss: 0.00163360
Iteration 13/25 | Loss: 0.00163360
Iteration 14/25 | Loss: 0.00163360
Iteration 15/25 | Loss: 0.00163360
Iteration 16/25 | Loss: 0.00163360
Iteration 17/25 | Loss: 0.00163360
Iteration 18/25 | Loss: 0.00163360
Iteration 19/25 | Loss: 0.00163360
Iteration 20/25 | Loss: 0.00163360
Iteration 21/25 | Loss: 0.00163360
Iteration 22/25 | Loss: 0.00163360
Iteration 23/25 | Loss: 0.00163360
Iteration 24/25 | Loss: 0.00163360
Iteration 25/25 | Loss: 0.00163360

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00163360
Iteration 2/1000 | Loss: 0.00010472
Iteration 3/1000 | Loss: 0.00004320
Iteration 4/1000 | Loss: 0.00003200
Iteration 5/1000 | Loss: 0.00002818
Iteration 6/1000 | Loss: 0.00002561
Iteration 7/1000 | Loss: 0.00002441
Iteration 8/1000 | Loss: 0.00002369
Iteration 9/1000 | Loss: 0.00002305
Iteration 10/1000 | Loss: 0.00002256
Iteration 11/1000 | Loss: 0.00002209
Iteration 12/1000 | Loss: 0.00002173
Iteration 13/1000 | Loss: 0.00002143
Iteration 14/1000 | Loss: 0.00002124
Iteration 15/1000 | Loss: 0.00002111
Iteration 16/1000 | Loss: 0.00002100
Iteration 17/1000 | Loss: 0.00002095
Iteration 18/1000 | Loss: 0.00002092
Iteration 19/1000 | Loss: 0.00002090
Iteration 20/1000 | Loss: 0.00002090
Iteration 21/1000 | Loss: 0.00002087
Iteration 22/1000 | Loss: 0.00002087
Iteration 23/1000 | Loss: 0.00002087
Iteration 24/1000 | Loss: 0.00002087
Iteration 25/1000 | Loss: 0.00002087
Iteration 26/1000 | Loss: 0.00002087
Iteration 27/1000 | Loss: 0.00002087
Iteration 28/1000 | Loss: 0.00002087
Iteration 29/1000 | Loss: 0.00002086
Iteration 30/1000 | Loss: 0.00002086
Iteration 31/1000 | Loss: 0.00002086
Iteration 32/1000 | Loss: 0.00002086
Iteration 33/1000 | Loss: 0.00002084
Iteration 34/1000 | Loss: 0.00002084
Iteration 35/1000 | Loss: 0.00002084
Iteration 36/1000 | Loss: 0.00002083
Iteration 37/1000 | Loss: 0.00002083
Iteration 38/1000 | Loss: 0.00002082
Iteration 39/1000 | Loss: 0.00002082
Iteration 40/1000 | Loss: 0.00002082
Iteration 41/1000 | Loss: 0.00002081
Iteration 42/1000 | Loss: 0.00002081
Iteration 43/1000 | Loss: 0.00002081
Iteration 44/1000 | Loss: 0.00002081
Iteration 45/1000 | Loss: 0.00002080
Iteration 46/1000 | Loss: 0.00002080
Iteration 47/1000 | Loss: 0.00002080
Iteration 48/1000 | Loss: 0.00002079
Iteration 49/1000 | Loss: 0.00002079
Iteration 50/1000 | Loss: 0.00002079
Iteration 51/1000 | Loss: 0.00002079
Iteration 52/1000 | Loss: 0.00002079
Iteration 53/1000 | Loss: 0.00002079
Iteration 54/1000 | Loss: 0.00002079
Iteration 55/1000 | Loss: 0.00002078
Iteration 56/1000 | Loss: 0.00002078
Iteration 57/1000 | Loss: 0.00002078
Iteration 58/1000 | Loss: 0.00002077
Iteration 59/1000 | Loss: 0.00002077
Iteration 60/1000 | Loss: 0.00002077
Iteration 61/1000 | Loss: 0.00002077
Iteration 62/1000 | Loss: 0.00002077
Iteration 63/1000 | Loss: 0.00002076
Iteration 64/1000 | Loss: 0.00002076
Iteration 65/1000 | Loss: 0.00002076
Iteration 66/1000 | Loss: 0.00002076
Iteration 67/1000 | Loss: 0.00002075
Iteration 68/1000 | Loss: 0.00002075
Iteration 69/1000 | Loss: 0.00002075
Iteration 70/1000 | Loss: 0.00002075
Iteration 71/1000 | Loss: 0.00002074
Iteration 72/1000 | Loss: 0.00002074
Iteration 73/1000 | Loss: 0.00002074
Iteration 74/1000 | Loss: 0.00002074
Iteration 75/1000 | Loss: 0.00002074
Iteration 76/1000 | Loss: 0.00002074
Iteration 77/1000 | Loss: 0.00002074
Iteration 78/1000 | Loss: 0.00002074
Iteration 79/1000 | Loss: 0.00002074
Iteration 80/1000 | Loss: 0.00002074
Iteration 81/1000 | Loss: 0.00002074
Iteration 82/1000 | Loss: 0.00002074
Iteration 83/1000 | Loss: 0.00002073
Iteration 84/1000 | Loss: 0.00002073
Iteration 85/1000 | Loss: 0.00002073
Iteration 86/1000 | Loss: 0.00002073
Iteration 87/1000 | Loss: 0.00002073
Iteration 88/1000 | Loss: 0.00002073
Iteration 89/1000 | Loss: 0.00002072
Iteration 90/1000 | Loss: 0.00002072
Iteration 91/1000 | Loss: 0.00002072
Iteration 92/1000 | Loss: 0.00002072
Iteration 93/1000 | Loss: 0.00002072
Iteration 94/1000 | Loss: 0.00002072
Iteration 95/1000 | Loss: 0.00002072
Iteration 96/1000 | Loss: 0.00002072
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 96. Stopping optimization.
Last 5 losses: [2.0722402041428722e-05, 2.0722402041428722e-05, 2.0722402041428722e-05, 2.0722402041428722e-05, 2.0722402041428722e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0722402041428722e-05

Optimization complete. Final v2v error: 3.8696129322052 mm

Highest mean error: 5.120193958282471 mm for frame 65

Lowest mean error: 3.2478199005126953 mm for frame 3

Saving results

Total time: 44.169517278671265
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_36_nl_5493/0000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_nl_5493/0000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_nl_5493/0000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00739407
Iteration 2/25 | Loss: 0.00160156
Iteration 3/25 | Loss: 0.00121120
Iteration 4/25 | Loss: 0.00115765
Iteration 5/25 | Loss: 0.00115087
Iteration 6/25 | Loss: 0.00115616
Iteration 7/25 | Loss: 0.00114949
Iteration 8/25 | Loss: 0.00113518
Iteration 9/25 | Loss: 0.00112609
Iteration 10/25 | Loss: 0.00111586
Iteration 11/25 | Loss: 0.00111034
Iteration 12/25 | Loss: 0.00110584
Iteration 13/25 | Loss: 0.00110530
Iteration 14/25 | Loss: 0.00110497
Iteration 15/25 | Loss: 0.00110432
Iteration 16/25 | Loss: 0.00110323
Iteration 17/25 | Loss: 0.00110369
Iteration 18/25 | Loss: 0.00110377
Iteration 19/25 | Loss: 0.00110352
Iteration 20/25 | Loss: 0.00110510
Iteration 21/25 | Loss: 0.00110313
Iteration 22/25 | Loss: 0.00110388
Iteration 23/25 | Loss: 0.00110393
Iteration 24/25 | Loss: 0.00110414
Iteration 25/25 | Loss: 0.00110404

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.93887401
Iteration 2/25 | Loss: 0.00113695
Iteration 3/25 | Loss: 0.00113090
Iteration 4/25 | Loss: 0.00113090
Iteration 5/25 | Loss: 0.00113090
Iteration 6/25 | Loss: 0.00113090
Iteration 7/25 | Loss: 0.00113090
Iteration 8/25 | Loss: 0.00113090
Iteration 9/25 | Loss: 0.00113090
Iteration 10/25 | Loss: 0.00113090
Iteration 11/25 | Loss: 0.00113090
Iteration 12/25 | Loss: 0.00113090
Iteration 13/25 | Loss: 0.00113090
Iteration 14/25 | Loss: 0.00113090
Iteration 15/25 | Loss: 0.00113090
Iteration 16/25 | Loss: 0.00113090
Iteration 17/25 | Loss: 0.00113090
Iteration 18/25 | Loss: 0.00113090
Iteration 19/25 | Loss: 0.00113090
Iteration 20/25 | Loss: 0.00113090
Iteration 21/25 | Loss: 0.00113090
Iteration 22/25 | Loss: 0.00113090
Iteration 23/25 | Loss: 0.00113090
Iteration 24/25 | Loss: 0.00113090
Iteration 25/25 | Loss: 0.00113090

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00113090
Iteration 2/1000 | Loss: 0.00005807
Iteration 3/1000 | Loss: 0.00011177
Iteration 4/1000 | Loss: 0.00003728
Iteration 5/1000 | Loss: 0.00002540
Iteration 6/1000 | Loss: 0.00007354
Iteration 7/1000 | Loss: 0.00011450
Iteration 8/1000 | Loss: 0.00008774
Iteration 9/1000 | Loss: 0.00002095
Iteration 10/1000 | Loss: 0.00001651
Iteration 11/1000 | Loss: 0.00008928
Iteration 12/1000 | Loss: 0.00001528
Iteration 13/1000 | Loss: 0.00001404
Iteration 14/1000 | Loss: 0.00001353
Iteration 15/1000 | Loss: 0.00001322
Iteration 16/1000 | Loss: 0.00013480
Iteration 17/1000 | Loss: 0.00014077
Iteration 18/1000 | Loss: 0.00009688
Iteration 19/1000 | Loss: 0.00001752
Iteration 20/1000 | Loss: 0.00001595
Iteration 21/1000 | Loss: 0.00001536
Iteration 22/1000 | Loss: 0.00003304
Iteration 23/1000 | Loss: 0.00012566
Iteration 24/1000 | Loss: 0.00011659
Iteration 25/1000 | Loss: 0.00003248
Iteration 26/1000 | Loss: 0.00010459
Iteration 27/1000 | Loss: 0.00011064
Iteration 28/1000 | Loss: 0.00007130
Iteration 29/1000 | Loss: 0.00002004
Iteration 30/1000 | Loss: 0.00001575
Iteration 31/1000 | Loss: 0.00001395
Iteration 32/1000 | Loss: 0.00001266
Iteration 33/1000 | Loss: 0.00001227
Iteration 34/1000 | Loss: 0.00001199
Iteration 35/1000 | Loss: 0.00001182
Iteration 36/1000 | Loss: 0.00001166
Iteration 37/1000 | Loss: 0.00001161
Iteration 38/1000 | Loss: 0.00001158
Iteration 39/1000 | Loss: 0.00001157
Iteration 40/1000 | Loss: 0.00001156
Iteration 41/1000 | Loss: 0.00001155
Iteration 42/1000 | Loss: 0.00001153
Iteration 43/1000 | Loss: 0.00001153
Iteration 44/1000 | Loss: 0.00001152
Iteration 45/1000 | Loss: 0.00001151
Iteration 46/1000 | Loss: 0.00001150
Iteration 47/1000 | Loss: 0.00001149
Iteration 48/1000 | Loss: 0.00001148
Iteration 49/1000 | Loss: 0.00001146
Iteration 50/1000 | Loss: 0.00001146
Iteration 51/1000 | Loss: 0.00001146
Iteration 52/1000 | Loss: 0.00001145
Iteration 53/1000 | Loss: 0.00001145
Iteration 54/1000 | Loss: 0.00001145
Iteration 55/1000 | Loss: 0.00001143
Iteration 56/1000 | Loss: 0.00001142
Iteration 57/1000 | Loss: 0.00001142
Iteration 58/1000 | Loss: 0.00001141
Iteration 59/1000 | Loss: 0.00001140
Iteration 60/1000 | Loss: 0.00001140
Iteration 61/1000 | Loss: 0.00001140
Iteration 62/1000 | Loss: 0.00001139
Iteration 63/1000 | Loss: 0.00001139
Iteration 64/1000 | Loss: 0.00001139
Iteration 65/1000 | Loss: 0.00001138
Iteration 66/1000 | Loss: 0.00001138
Iteration 67/1000 | Loss: 0.00001138
Iteration 68/1000 | Loss: 0.00001137
Iteration 69/1000 | Loss: 0.00001137
Iteration 70/1000 | Loss: 0.00001137
Iteration 71/1000 | Loss: 0.00001137
Iteration 72/1000 | Loss: 0.00001137
Iteration 73/1000 | Loss: 0.00001137
Iteration 74/1000 | Loss: 0.00001137
Iteration 75/1000 | Loss: 0.00001137
Iteration 76/1000 | Loss: 0.00001137
Iteration 77/1000 | Loss: 0.00001136
Iteration 78/1000 | Loss: 0.00001136
Iteration 79/1000 | Loss: 0.00001136
Iteration 80/1000 | Loss: 0.00001135
Iteration 81/1000 | Loss: 0.00001135
Iteration 82/1000 | Loss: 0.00001135
Iteration 83/1000 | Loss: 0.00001135
Iteration 84/1000 | Loss: 0.00001135
Iteration 85/1000 | Loss: 0.00001135
Iteration 86/1000 | Loss: 0.00001135
Iteration 87/1000 | Loss: 0.00001134
Iteration 88/1000 | Loss: 0.00001134
Iteration 89/1000 | Loss: 0.00001134
Iteration 90/1000 | Loss: 0.00001134
Iteration 91/1000 | Loss: 0.00001134
Iteration 92/1000 | Loss: 0.00001134
Iteration 93/1000 | Loss: 0.00001134
Iteration 94/1000 | Loss: 0.00001134
Iteration 95/1000 | Loss: 0.00001134
Iteration 96/1000 | Loss: 0.00001134
Iteration 97/1000 | Loss: 0.00001134
Iteration 98/1000 | Loss: 0.00001133
Iteration 99/1000 | Loss: 0.00001133
Iteration 100/1000 | Loss: 0.00001133
Iteration 101/1000 | Loss: 0.00001133
Iteration 102/1000 | Loss: 0.00001133
Iteration 103/1000 | Loss: 0.00001132
Iteration 104/1000 | Loss: 0.00001132
Iteration 105/1000 | Loss: 0.00001132
Iteration 106/1000 | Loss: 0.00001132
Iteration 107/1000 | Loss: 0.00001132
Iteration 108/1000 | Loss: 0.00001132
Iteration 109/1000 | Loss: 0.00001131
Iteration 110/1000 | Loss: 0.00001131
Iteration 111/1000 | Loss: 0.00001131
Iteration 112/1000 | Loss: 0.00001130
Iteration 113/1000 | Loss: 0.00001130
Iteration 114/1000 | Loss: 0.00001129
Iteration 115/1000 | Loss: 0.00001129
Iteration 116/1000 | Loss: 0.00001129
Iteration 117/1000 | Loss: 0.00001129
Iteration 118/1000 | Loss: 0.00001129
Iteration 119/1000 | Loss: 0.00001129
Iteration 120/1000 | Loss: 0.00001129
Iteration 121/1000 | Loss: 0.00001129
Iteration 122/1000 | Loss: 0.00001129
Iteration 123/1000 | Loss: 0.00001129
Iteration 124/1000 | Loss: 0.00001128
Iteration 125/1000 | Loss: 0.00001128
Iteration 126/1000 | Loss: 0.00001128
Iteration 127/1000 | Loss: 0.00001128
Iteration 128/1000 | Loss: 0.00001128
Iteration 129/1000 | Loss: 0.00001128
Iteration 130/1000 | Loss: 0.00001128
Iteration 131/1000 | Loss: 0.00001128
Iteration 132/1000 | Loss: 0.00001128
Iteration 133/1000 | Loss: 0.00001128
Iteration 134/1000 | Loss: 0.00001128
Iteration 135/1000 | Loss: 0.00001128
Iteration 136/1000 | Loss: 0.00001128
Iteration 137/1000 | Loss: 0.00001128
Iteration 138/1000 | Loss: 0.00001127
Iteration 139/1000 | Loss: 0.00001127
Iteration 140/1000 | Loss: 0.00001127
Iteration 141/1000 | Loss: 0.00001127
Iteration 142/1000 | Loss: 0.00001127
Iteration 143/1000 | Loss: 0.00001127
Iteration 144/1000 | Loss: 0.00001127
Iteration 145/1000 | Loss: 0.00001127
Iteration 146/1000 | Loss: 0.00001127
Iteration 147/1000 | Loss: 0.00001127
Iteration 148/1000 | Loss: 0.00001127
Iteration 149/1000 | Loss: 0.00001127
Iteration 150/1000 | Loss: 0.00001126
Iteration 151/1000 | Loss: 0.00001126
Iteration 152/1000 | Loss: 0.00001126
Iteration 153/1000 | Loss: 0.00001126
Iteration 154/1000 | Loss: 0.00001126
Iteration 155/1000 | Loss: 0.00001126
Iteration 156/1000 | Loss: 0.00001126
Iteration 157/1000 | Loss: 0.00001126
Iteration 158/1000 | Loss: 0.00001126
Iteration 159/1000 | Loss: 0.00001126
Iteration 160/1000 | Loss: 0.00001126
Iteration 161/1000 | Loss: 0.00001126
Iteration 162/1000 | Loss: 0.00001126
Iteration 163/1000 | Loss: 0.00001126
Iteration 164/1000 | Loss: 0.00001126
Iteration 165/1000 | Loss: 0.00001126
Iteration 166/1000 | Loss: 0.00001126
Iteration 167/1000 | Loss: 0.00001125
Iteration 168/1000 | Loss: 0.00001125
Iteration 169/1000 | Loss: 0.00001125
Iteration 170/1000 | Loss: 0.00001125
Iteration 171/1000 | Loss: 0.00001125
Iteration 172/1000 | Loss: 0.00001125
Iteration 173/1000 | Loss: 0.00001125
Iteration 174/1000 | Loss: 0.00001125
Iteration 175/1000 | Loss: 0.00001125
Iteration 176/1000 | Loss: 0.00001125
Iteration 177/1000 | Loss: 0.00001125
Iteration 178/1000 | Loss: 0.00001125
Iteration 179/1000 | Loss: 0.00001125
Iteration 180/1000 | Loss: 0.00001125
Iteration 181/1000 | Loss: 0.00001125
Iteration 182/1000 | Loss: 0.00001125
Iteration 183/1000 | Loss: 0.00001124
Iteration 184/1000 | Loss: 0.00001124
Iteration 185/1000 | Loss: 0.00001124
Iteration 186/1000 | Loss: 0.00001124
Iteration 187/1000 | Loss: 0.00001123
Iteration 188/1000 | Loss: 0.00001123
Iteration 189/1000 | Loss: 0.00001123
Iteration 190/1000 | Loss: 0.00001123
Iteration 191/1000 | Loss: 0.00001123
Iteration 192/1000 | Loss: 0.00001123
Iteration 193/1000 | Loss: 0.00001122
Iteration 194/1000 | Loss: 0.00001122
Iteration 195/1000 | Loss: 0.00001122
Iteration 196/1000 | Loss: 0.00001122
Iteration 197/1000 | Loss: 0.00001122
Iteration 198/1000 | Loss: 0.00001122
Iteration 199/1000 | Loss: 0.00001122
Iteration 200/1000 | Loss: 0.00001122
Iteration 201/1000 | Loss: 0.00001122
Iteration 202/1000 | Loss: 0.00001122
Iteration 203/1000 | Loss: 0.00001122
Iteration 204/1000 | Loss: 0.00001122
Iteration 205/1000 | Loss: 0.00001122
Iteration 206/1000 | Loss: 0.00001121
Iteration 207/1000 | Loss: 0.00001121
Iteration 208/1000 | Loss: 0.00001121
Iteration 209/1000 | Loss: 0.00001121
Iteration 210/1000 | Loss: 0.00001121
Iteration 211/1000 | Loss: 0.00001121
Iteration 212/1000 | Loss: 0.00001121
Iteration 213/1000 | Loss: 0.00001121
Iteration 214/1000 | Loss: 0.00001121
Iteration 215/1000 | Loss: 0.00001121
Iteration 216/1000 | Loss: 0.00001121
Iteration 217/1000 | Loss: 0.00001121
Iteration 218/1000 | Loss: 0.00001121
Iteration 219/1000 | Loss: 0.00001121
Iteration 220/1000 | Loss: 0.00001121
Iteration 221/1000 | Loss: 0.00001121
Iteration 222/1000 | Loss: 0.00001121
Iteration 223/1000 | Loss: 0.00001121
Iteration 224/1000 | Loss: 0.00001121
Iteration 225/1000 | Loss: 0.00001121
Iteration 226/1000 | Loss: 0.00001121
Iteration 227/1000 | Loss: 0.00001120
Iteration 228/1000 | Loss: 0.00001120
Iteration 229/1000 | Loss: 0.00001120
Iteration 230/1000 | Loss: 0.00001120
Iteration 231/1000 | Loss: 0.00001120
Iteration 232/1000 | Loss: 0.00001120
Iteration 233/1000 | Loss: 0.00001120
Iteration 234/1000 | Loss: 0.00001120
Iteration 235/1000 | Loss: 0.00001120
Iteration 236/1000 | Loss: 0.00001120
Iteration 237/1000 | Loss: 0.00001120
Iteration 238/1000 | Loss: 0.00001120
Iteration 239/1000 | Loss: 0.00001120
Iteration 240/1000 | Loss: 0.00001120
Iteration 241/1000 | Loss: 0.00001120
Iteration 242/1000 | Loss: 0.00001120
Iteration 243/1000 | Loss: 0.00001119
Iteration 244/1000 | Loss: 0.00001119
Iteration 245/1000 | Loss: 0.00001119
Iteration 246/1000 | Loss: 0.00001119
Iteration 247/1000 | Loss: 0.00001119
Iteration 248/1000 | Loss: 0.00001119
Iteration 249/1000 | Loss: 0.00001119
Iteration 250/1000 | Loss: 0.00001119
Iteration 251/1000 | Loss: 0.00001119
Iteration 252/1000 | Loss: 0.00001119
Iteration 253/1000 | Loss: 0.00001119
Iteration 254/1000 | Loss: 0.00001119
Iteration 255/1000 | Loss: 0.00001118
Iteration 256/1000 | Loss: 0.00001118
Iteration 257/1000 | Loss: 0.00001118
Iteration 258/1000 | Loss: 0.00001118
Iteration 259/1000 | Loss: 0.00001118
Iteration 260/1000 | Loss: 0.00001118
Iteration 261/1000 | Loss: 0.00001118
Iteration 262/1000 | Loss: 0.00001118
Iteration 263/1000 | Loss: 0.00001118
Iteration 264/1000 | Loss: 0.00001118
Iteration 265/1000 | Loss: 0.00001118
Iteration 266/1000 | Loss: 0.00001118
Iteration 267/1000 | Loss: 0.00001118
Iteration 268/1000 | Loss: 0.00001118
Iteration 269/1000 | Loss: 0.00001118
Iteration 270/1000 | Loss: 0.00001118
Iteration 271/1000 | Loss: 0.00001118
Iteration 272/1000 | Loss: 0.00001118
Iteration 273/1000 | Loss: 0.00001118
Iteration 274/1000 | Loss: 0.00001118
Iteration 275/1000 | Loss: 0.00001118
Iteration 276/1000 | Loss: 0.00001118
Iteration 277/1000 | Loss: 0.00001118
Iteration 278/1000 | Loss: 0.00001118
Iteration 279/1000 | Loss: 0.00001118
Iteration 280/1000 | Loss: 0.00001118
Iteration 281/1000 | Loss: 0.00001118
Iteration 282/1000 | Loss: 0.00001118
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 282. Stopping optimization.
Last 5 losses: [1.1181296031281818e-05, 1.1181296031281818e-05, 1.1181296031281818e-05, 1.1181296031281818e-05, 1.1181296031281818e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1181296031281818e-05

Optimization complete. Final v2v error: 2.7864484786987305 mm

Highest mean error: 9.36209487915039 mm for frame 174

Lowest mean error: 2.4174611568450928 mm for frame 239

Saving results

Total time: 131.93030214309692
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_36_nl_5493/0007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_nl_5493/0007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_nl_5493/0007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00891876
Iteration 2/25 | Loss: 0.00157726
Iteration 3/25 | Loss: 0.00126148
Iteration 4/25 | Loss: 0.00122468
Iteration 5/25 | Loss: 0.00121969
Iteration 6/25 | Loss: 0.00121911
Iteration 7/25 | Loss: 0.00121911
Iteration 8/25 | Loss: 0.00121911
Iteration 9/25 | Loss: 0.00121911
Iteration 10/25 | Loss: 0.00121911
Iteration 11/25 | Loss: 0.00121911
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012191073037683964, 0.0012191073037683964, 0.0012191073037683964, 0.0012191073037683964, 0.0012191073037683964]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012191073037683964

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.89061922
Iteration 2/25 | Loss: 0.00064666
Iteration 3/25 | Loss: 0.00064666
Iteration 4/25 | Loss: 0.00064666
Iteration 5/25 | Loss: 0.00064666
Iteration 6/25 | Loss: 0.00064666
Iteration 7/25 | Loss: 0.00064666
Iteration 8/25 | Loss: 0.00064666
Iteration 9/25 | Loss: 0.00064666
Iteration 10/25 | Loss: 0.00064666
Iteration 11/25 | Loss: 0.00064666
Iteration 12/25 | Loss: 0.00064666
Iteration 13/25 | Loss: 0.00064666
Iteration 14/25 | Loss: 0.00064666
Iteration 15/25 | Loss: 0.00064666
Iteration 16/25 | Loss: 0.00064666
Iteration 17/25 | Loss: 0.00064666
Iteration 18/25 | Loss: 0.00064666
Iteration 19/25 | Loss: 0.00064666
Iteration 20/25 | Loss: 0.00064666
Iteration 21/25 | Loss: 0.00064666
Iteration 22/25 | Loss: 0.00064666
Iteration 23/25 | Loss: 0.00064666
Iteration 24/25 | Loss: 0.00064666
Iteration 25/25 | Loss: 0.00064666

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00064666
Iteration 2/1000 | Loss: 0.00005205
Iteration 3/1000 | Loss: 0.00003163
Iteration 4/1000 | Loss: 0.00002817
Iteration 5/1000 | Loss: 0.00002653
Iteration 6/1000 | Loss: 0.00002573
Iteration 7/1000 | Loss: 0.00002505
Iteration 8/1000 | Loss: 0.00002464
Iteration 9/1000 | Loss: 0.00002443
Iteration 10/1000 | Loss: 0.00002419
Iteration 11/1000 | Loss: 0.00002406
Iteration 12/1000 | Loss: 0.00002397
Iteration 13/1000 | Loss: 0.00002394
Iteration 14/1000 | Loss: 0.00002388
Iteration 15/1000 | Loss: 0.00002386
Iteration 16/1000 | Loss: 0.00002384
Iteration 17/1000 | Loss: 0.00002382
Iteration 18/1000 | Loss: 0.00002382
Iteration 19/1000 | Loss: 0.00002382
Iteration 20/1000 | Loss: 0.00002382
Iteration 21/1000 | Loss: 0.00002382
Iteration 22/1000 | Loss: 0.00002382
Iteration 23/1000 | Loss: 0.00002382
Iteration 24/1000 | Loss: 0.00002382
Iteration 25/1000 | Loss: 0.00002382
Iteration 26/1000 | Loss: 0.00002382
Iteration 27/1000 | Loss: 0.00002382
Iteration 28/1000 | Loss: 0.00002382
Iteration 29/1000 | Loss: 0.00002381
Iteration 30/1000 | Loss: 0.00002381
Iteration 31/1000 | Loss: 0.00002380
Iteration 32/1000 | Loss: 0.00002379
Iteration 33/1000 | Loss: 0.00002379
Iteration 34/1000 | Loss: 0.00002379
Iteration 35/1000 | Loss: 0.00002379
Iteration 36/1000 | Loss: 0.00002379
Iteration 37/1000 | Loss: 0.00002379
Iteration 38/1000 | Loss: 0.00002378
Iteration 39/1000 | Loss: 0.00002378
Iteration 40/1000 | Loss: 0.00002377
Iteration 41/1000 | Loss: 0.00002377
Iteration 42/1000 | Loss: 0.00002377
Iteration 43/1000 | Loss: 0.00002377
Iteration 44/1000 | Loss: 0.00002377
Iteration 45/1000 | Loss: 0.00002377
Iteration 46/1000 | Loss: 0.00002376
Iteration 47/1000 | Loss: 0.00002376
Iteration 48/1000 | Loss: 0.00002376
Iteration 49/1000 | Loss: 0.00002376
Iteration 50/1000 | Loss: 0.00002376
Iteration 51/1000 | Loss: 0.00002376
Iteration 52/1000 | Loss: 0.00002376
Iteration 53/1000 | Loss: 0.00002375
Iteration 54/1000 | Loss: 0.00002375
Iteration 55/1000 | Loss: 0.00002375
Iteration 56/1000 | Loss: 0.00002375
Iteration 57/1000 | Loss: 0.00002375
Iteration 58/1000 | Loss: 0.00002375
Iteration 59/1000 | Loss: 0.00002374
Iteration 60/1000 | Loss: 0.00002374
Iteration 61/1000 | Loss: 0.00002374
Iteration 62/1000 | Loss: 0.00002374
Iteration 63/1000 | Loss: 0.00002374
Iteration 64/1000 | Loss: 0.00002374
Iteration 65/1000 | Loss: 0.00002374
Iteration 66/1000 | Loss: 0.00002374
Iteration 67/1000 | Loss: 0.00002374
Iteration 68/1000 | Loss: 0.00002373
Iteration 69/1000 | Loss: 0.00002373
Iteration 70/1000 | Loss: 0.00002373
Iteration 71/1000 | Loss: 0.00002372
Iteration 72/1000 | Loss: 0.00002372
Iteration 73/1000 | Loss: 0.00002372
Iteration 74/1000 | Loss: 0.00002372
Iteration 75/1000 | Loss: 0.00002372
Iteration 76/1000 | Loss: 0.00002372
Iteration 77/1000 | Loss: 0.00002372
Iteration 78/1000 | Loss: 0.00002372
Iteration 79/1000 | Loss: 0.00002372
Iteration 80/1000 | Loss: 0.00002371
Iteration 81/1000 | Loss: 0.00002371
Iteration 82/1000 | Loss: 0.00002371
Iteration 83/1000 | Loss: 0.00002371
Iteration 84/1000 | Loss: 0.00002371
Iteration 85/1000 | Loss: 0.00002371
Iteration 86/1000 | Loss: 0.00002371
Iteration 87/1000 | Loss: 0.00002371
Iteration 88/1000 | Loss: 0.00002371
Iteration 89/1000 | Loss: 0.00002371
Iteration 90/1000 | Loss: 0.00002371
Iteration 91/1000 | Loss: 0.00002371
Iteration 92/1000 | Loss: 0.00002371
Iteration 93/1000 | Loss: 0.00002371
Iteration 94/1000 | Loss: 0.00002371
Iteration 95/1000 | Loss: 0.00002370
Iteration 96/1000 | Loss: 0.00002370
Iteration 97/1000 | Loss: 0.00002370
Iteration 98/1000 | Loss: 0.00002370
Iteration 99/1000 | Loss: 0.00002370
Iteration 100/1000 | Loss: 0.00002370
Iteration 101/1000 | Loss: 0.00002370
Iteration 102/1000 | Loss: 0.00002370
Iteration 103/1000 | Loss: 0.00002369
Iteration 104/1000 | Loss: 0.00002369
Iteration 105/1000 | Loss: 0.00002369
Iteration 106/1000 | Loss: 0.00002369
Iteration 107/1000 | Loss: 0.00002369
Iteration 108/1000 | Loss: 0.00002369
Iteration 109/1000 | Loss: 0.00002369
Iteration 110/1000 | Loss: 0.00002369
Iteration 111/1000 | Loss: 0.00002369
Iteration 112/1000 | Loss: 0.00002369
Iteration 113/1000 | Loss: 0.00002368
Iteration 114/1000 | Loss: 0.00002368
Iteration 115/1000 | Loss: 0.00002368
Iteration 116/1000 | Loss: 0.00002368
Iteration 117/1000 | Loss: 0.00002368
Iteration 118/1000 | Loss: 0.00002368
Iteration 119/1000 | Loss: 0.00002368
Iteration 120/1000 | Loss: 0.00002368
Iteration 121/1000 | Loss: 0.00002368
Iteration 122/1000 | Loss: 0.00002368
Iteration 123/1000 | Loss: 0.00002368
Iteration 124/1000 | Loss: 0.00002367
Iteration 125/1000 | Loss: 0.00002367
Iteration 126/1000 | Loss: 0.00002367
Iteration 127/1000 | Loss: 0.00002367
Iteration 128/1000 | Loss: 0.00002367
Iteration 129/1000 | Loss: 0.00002367
Iteration 130/1000 | Loss: 0.00002367
Iteration 131/1000 | Loss: 0.00002367
Iteration 132/1000 | Loss: 0.00002367
Iteration 133/1000 | Loss: 0.00002367
Iteration 134/1000 | Loss: 0.00002366
Iteration 135/1000 | Loss: 0.00002366
Iteration 136/1000 | Loss: 0.00002366
Iteration 137/1000 | Loss: 0.00002366
Iteration 138/1000 | Loss: 0.00002366
Iteration 139/1000 | Loss: 0.00002366
Iteration 140/1000 | Loss: 0.00002366
Iteration 141/1000 | Loss: 0.00002366
Iteration 142/1000 | Loss: 0.00002366
Iteration 143/1000 | Loss: 0.00002366
Iteration 144/1000 | Loss: 0.00002365
Iteration 145/1000 | Loss: 0.00002365
Iteration 146/1000 | Loss: 0.00002365
Iteration 147/1000 | Loss: 0.00002365
Iteration 148/1000 | Loss: 0.00002365
Iteration 149/1000 | Loss: 0.00002365
Iteration 150/1000 | Loss: 0.00002365
Iteration 151/1000 | Loss: 0.00002365
Iteration 152/1000 | Loss: 0.00002365
Iteration 153/1000 | Loss: 0.00002365
Iteration 154/1000 | Loss: 0.00002365
Iteration 155/1000 | Loss: 0.00002365
Iteration 156/1000 | Loss: 0.00002364
Iteration 157/1000 | Loss: 0.00002364
Iteration 158/1000 | Loss: 0.00002364
Iteration 159/1000 | Loss: 0.00002364
Iteration 160/1000 | Loss: 0.00002364
Iteration 161/1000 | Loss: 0.00002364
Iteration 162/1000 | Loss: 0.00002364
Iteration 163/1000 | Loss: 0.00002364
Iteration 164/1000 | Loss: 0.00002364
Iteration 165/1000 | Loss: 0.00002363
Iteration 166/1000 | Loss: 0.00002363
Iteration 167/1000 | Loss: 0.00002363
Iteration 168/1000 | Loss: 0.00002363
Iteration 169/1000 | Loss: 0.00002363
Iteration 170/1000 | Loss: 0.00002363
Iteration 171/1000 | Loss: 0.00002363
Iteration 172/1000 | Loss: 0.00002363
Iteration 173/1000 | Loss: 0.00002363
Iteration 174/1000 | Loss: 0.00002363
Iteration 175/1000 | Loss: 0.00002363
Iteration 176/1000 | Loss: 0.00002363
Iteration 177/1000 | Loss: 0.00002363
Iteration 178/1000 | Loss: 0.00002363
Iteration 179/1000 | Loss: 0.00002363
Iteration 180/1000 | Loss: 0.00002363
Iteration 181/1000 | Loss: 0.00002363
Iteration 182/1000 | Loss: 0.00002363
Iteration 183/1000 | Loss: 0.00002363
Iteration 184/1000 | Loss: 0.00002363
Iteration 185/1000 | Loss: 0.00002363
Iteration 186/1000 | Loss: 0.00002363
Iteration 187/1000 | Loss: 0.00002363
Iteration 188/1000 | Loss: 0.00002363
Iteration 189/1000 | Loss: 0.00002363
Iteration 190/1000 | Loss: 0.00002363
Iteration 191/1000 | Loss: 0.00002363
Iteration 192/1000 | Loss: 0.00002363
Iteration 193/1000 | Loss: 0.00002363
Iteration 194/1000 | Loss: 0.00002363
Iteration 195/1000 | Loss: 0.00002363
Iteration 196/1000 | Loss: 0.00002363
Iteration 197/1000 | Loss: 0.00002363
Iteration 198/1000 | Loss: 0.00002363
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 198. Stopping optimization.
Last 5 losses: [2.3625485482625663e-05, 2.3625485482625663e-05, 2.3625485482625663e-05, 2.3625485482625663e-05, 2.3625485482625663e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3625485482625663e-05

Optimization complete. Final v2v error: 4.10791540145874 mm

Highest mean error: 4.367143630981445 mm for frame 92

Lowest mean error: 3.922015905380249 mm for frame 70

Saving results

Total time: 36.321343421936035
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_36_nl_5493/0004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_nl_5493/0004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_nl_5493/0004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00755865
Iteration 2/25 | Loss: 0.00135675
Iteration 3/25 | Loss: 0.00121189
Iteration 4/25 | Loss: 0.00115295
Iteration 5/25 | Loss: 0.00114800
Iteration 6/25 | Loss: 0.00113935
Iteration 7/25 | Loss: 0.00112587
Iteration 8/25 | Loss: 0.00112404
Iteration 9/25 | Loss: 0.00112332
Iteration 10/25 | Loss: 0.00112298
Iteration 11/25 | Loss: 0.00112282
Iteration 12/25 | Loss: 0.00112276
Iteration 13/25 | Loss: 0.00112275
Iteration 14/25 | Loss: 0.00112275
Iteration 15/25 | Loss: 0.00112274
Iteration 16/25 | Loss: 0.00112274
Iteration 17/25 | Loss: 0.00112274
Iteration 18/25 | Loss: 0.00112274
Iteration 19/25 | Loss: 0.00112274
Iteration 20/25 | Loss: 0.00112274
Iteration 21/25 | Loss: 0.00112274
Iteration 22/25 | Loss: 0.00112274
Iteration 23/25 | Loss: 0.00112273
Iteration 24/25 | Loss: 0.00112273
Iteration 25/25 | Loss: 0.00112273

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.90097916
Iteration 2/25 | Loss: 0.00117925
Iteration 3/25 | Loss: 0.00117925
Iteration 4/25 | Loss: 0.00117924
Iteration 5/25 | Loss: 0.00117924
Iteration 6/25 | Loss: 0.00117924
Iteration 7/25 | Loss: 0.00117924
Iteration 8/25 | Loss: 0.00117924
Iteration 9/25 | Loss: 0.00117924
Iteration 10/25 | Loss: 0.00117924
Iteration 11/25 | Loss: 0.00117924
Iteration 12/25 | Loss: 0.00117924
Iteration 13/25 | Loss: 0.00117924
Iteration 14/25 | Loss: 0.00117924
Iteration 15/25 | Loss: 0.00117924
Iteration 16/25 | Loss: 0.00117924
Iteration 17/25 | Loss: 0.00117924
Iteration 18/25 | Loss: 0.00117924
Iteration 19/25 | Loss: 0.00117924
Iteration 20/25 | Loss: 0.00117924
Iteration 21/25 | Loss: 0.00117924
Iteration 22/25 | Loss: 0.00117924
Iteration 23/25 | Loss: 0.00117924
Iteration 24/25 | Loss: 0.00117924
Iteration 25/25 | Loss: 0.00117924

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00117924
Iteration 2/1000 | Loss: 0.00002859
Iteration 3/1000 | Loss: 0.00001848
Iteration 4/1000 | Loss: 0.00001688
Iteration 5/1000 | Loss: 0.00001582
Iteration 6/1000 | Loss: 0.00001519
Iteration 7/1000 | Loss: 0.00001477
Iteration 8/1000 | Loss: 0.00001442
Iteration 9/1000 | Loss: 0.00001416
Iteration 10/1000 | Loss: 0.00001414
Iteration 11/1000 | Loss: 0.00001401
Iteration 12/1000 | Loss: 0.00001400
Iteration 13/1000 | Loss: 0.00001396
Iteration 14/1000 | Loss: 0.00001395
Iteration 15/1000 | Loss: 0.00001394
Iteration 16/1000 | Loss: 0.00001393
Iteration 17/1000 | Loss: 0.00001393
Iteration 18/1000 | Loss: 0.00001389
Iteration 19/1000 | Loss: 0.00001388
Iteration 20/1000 | Loss: 0.00001387
Iteration 21/1000 | Loss: 0.00001387
Iteration 22/1000 | Loss: 0.00001382
Iteration 23/1000 | Loss: 0.00001382
Iteration 24/1000 | Loss: 0.00001381
Iteration 25/1000 | Loss: 0.00001380
Iteration 26/1000 | Loss: 0.00001380
Iteration 27/1000 | Loss: 0.00001379
Iteration 28/1000 | Loss: 0.00001375
Iteration 29/1000 | Loss: 0.00001375
Iteration 30/1000 | Loss: 0.00001375
Iteration 31/1000 | Loss: 0.00001374
Iteration 32/1000 | Loss: 0.00001374
Iteration 33/1000 | Loss: 0.00001372
Iteration 34/1000 | Loss: 0.00001372
Iteration 35/1000 | Loss: 0.00001372
Iteration 36/1000 | Loss: 0.00001372
Iteration 37/1000 | Loss: 0.00001372
Iteration 38/1000 | Loss: 0.00001372
Iteration 39/1000 | Loss: 0.00001372
Iteration 40/1000 | Loss: 0.00001372
Iteration 41/1000 | Loss: 0.00001372
Iteration 42/1000 | Loss: 0.00001372
Iteration 43/1000 | Loss: 0.00001372
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 43. Stopping optimization.
Last 5 losses: [1.3721451978199184e-05, 1.3721451978199184e-05, 1.3721451978199184e-05, 1.3721451978199184e-05, 1.3721451978199184e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3721451978199184e-05

Optimization complete. Final v2v error: 3.1458864212036133 mm

Highest mean error: 3.616894483566284 mm for frame 189

Lowest mean error: 2.8122434616088867 mm for frame 8

Saving results

Total time: 43.84875440597534
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_36_nl_5493/0009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_nl_5493/0009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_nl_5493/0009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00875937
Iteration 2/25 | Loss: 0.00150983
Iteration 3/25 | Loss: 0.00119684
Iteration 4/25 | Loss: 0.00116615
Iteration 5/25 | Loss: 0.00116036
Iteration 6/25 | Loss: 0.00115894
Iteration 7/25 | Loss: 0.00115884
Iteration 8/25 | Loss: 0.00115884
Iteration 9/25 | Loss: 0.00115884
Iteration 10/25 | Loss: 0.00115884
Iteration 11/25 | Loss: 0.00115884
Iteration 12/25 | Loss: 0.00115884
Iteration 13/25 | Loss: 0.00115884
Iteration 14/25 | Loss: 0.00115884
Iteration 15/25 | Loss: 0.00115884
Iteration 16/25 | Loss: 0.00115884
Iteration 17/25 | Loss: 0.00115884
Iteration 18/25 | Loss: 0.00115884
Iteration 19/25 | Loss: 0.00115884
Iteration 20/25 | Loss: 0.00115884
Iteration 21/25 | Loss: 0.00115884
Iteration 22/25 | Loss: 0.00115884
Iteration 23/25 | Loss: 0.00115884
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0011588402558118105, 0.0011588402558118105, 0.0011588402558118105, 0.0011588402558118105, 0.0011588402558118105]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011588402558118105

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.26774430
Iteration 2/25 | Loss: 0.00118877
Iteration 3/25 | Loss: 0.00118877
Iteration 4/25 | Loss: 0.00118877
Iteration 5/25 | Loss: 0.00118877
Iteration 6/25 | Loss: 0.00118877
Iteration 7/25 | Loss: 0.00118877
Iteration 8/25 | Loss: 0.00118877
Iteration 9/25 | Loss: 0.00118877
Iteration 10/25 | Loss: 0.00118877
Iteration 11/25 | Loss: 0.00118877
Iteration 12/25 | Loss: 0.00118877
Iteration 13/25 | Loss: 0.00118877
Iteration 14/25 | Loss: 0.00118877
Iteration 15/25 | Loss: 0.00118877
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0011887659784406424, 0.0011887659784406424, 0.0011887659784406424, 0.0011887659784406424, 0.0011887659784406424]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011887659784406424

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00118877
Iteration 2/1000 | Loss: 0.00004304
Iteration 3/1000 | Loss: 0.00001945
Iteration 4/1000 | Loss: 0.00001660
Iteration 5/1000 | Loss: 0.00001502
Iteration 6/1000 | Loss: 0.00001412
Iteration 7/1000 | Loss: 0.00001353
Iteration 8/1000 | Loss: 0.00001307
Iteration 9/1000 | Loss: 0.00001264
Iteration 10/1000 | Loss: 0.00001233
Iteration 11/1000 | Loss: 0.00001230
Iteration 12/1000 | Loss: 0.00001215
Iteration 13/1000 | Loss: 0.00001200
Iteration 14/1000 | Loss: 0.00001199
Iteration 15/1000 | Loss: 0.00001198
Iteration 16/1000 | Loss: 0.00001197
Iteration 17/1000 | Loss: 0.00001194
Iteration 18/1000 | Loss: 0.00001191
Iteration 19/1000 | Loss: 0.00001190
Iteration 20/1000 | Loss: 0.00001189
Iteration 21/1000 | Loss: 0.00001189
Iteration 22/1000 | Loss: 0.00001188
Iteration 23/1000 | Loss: 0.00001187
Iteration 24/1000 | Loss: 0.00001186
Iteration 25/1000 | Loss: 0.00001186
Iteration 26/1000 | Loss: 0.00001186
Iteration 27/1000 | Loss: 0.00001185
Iteration 28/1000 | Loss: 0.00001184
Iteration 29/1000 | Loss: 0.00001184
Iteration 30/1000 | Loss: 0.00001184
Iteration 31/1000 | Loss: 0.00001183
Iteration 32/1000 | Loss: 0.00001183
Iteration 33/1000 | Loss: 0.00001183
Iteration 34/1000 | Loss: 0.00001183
Iteration 35/1000 | Loss: 0.00001182
Iteration 36/1000 | Loss: 0.00001182
Iteration 37/1000 | Loss: 0.00001182
Iteration 38/1000 | Loss: 0.00001182
Iteration 39/1000 | Loss: 0.00001181
Iteration 40/1000 | Loss: 0.00001181
Iteration 41/1000 | Loss: 0.00001181
Iteration 42/1000 | Loss: 0.00001181
Iteration 43/1000 | Loss: 0.00001180
Iteration 44/1000 | Loss: 0.00001180
Iteration 45/1000 | Loss: 0.00001180
Iteration 46/1000 | Loss: 0.00001180
Iteration 47/1000 | Loss: 0.00001179
Iteration 48/1000 | Loss: 0.00001179
Iteration 49/1000 | Loss: 0.00001179
Iteration 50/1000 | Loss: 0.00001179
Iteration 51/1000 | Loss: 0.00001178
Iteration 52/1000 | Loss: 0.00001178
Iteration 53/1000 | Loss: 0.00001178
Iteration 54/1000 | Loss: 0.00001178
Iteration 55/1000 | Loss: 0.00001178
Iteration 56/1000 | Loss: 0.00001178
Iteration 57/1000 | Loss: 0.00001177
Iteration 58/1000 | Loss: 0.00001177
Iteration 59/1000 | Loss: 0.00001177
Iteration 60/1000 | Loss: 0.00001177
Iteration 61/1000 | Loss: 0.00001177
Iteration 62/1000 | Loss: 0.00001177
Iteration 63/1000 | Loss: 0.00001177
Iteration 64/1000 | Loss: 0.00001177
Iteration 65/1000 | Loss: 0.00001177
Iteration 66/1000 | Loss: 0.00001177
Iteration 67/1000 | Loss: 0.00001177
Iteration 68/1000 | Loss: 0.00001176
Iteration 69/1000 | Loss: 0.00001176
Iteration 70/1000 | Loss: 0.00001176
Iteration 71/1000 | Loss: 0.00001176
Iteration 72/1000 | Loss: 0.00001176
Iteration 73/1000 | Loss: 0.00001176
Iteration 74/1000 | Loss: 0.00001176
Iteration 75/1000 | Loss: 0.00001176
Iteration 76/1000 | Loss: 0.00001176
Iteration 77/1000 | Loss: 0.00001176
Iteration 78/1000 | Loss: 0.00001176
Iteration 79/1000 | Loss: 0.00001176
Iteration 80/1000 | Loss: 0.00001176
Iteration 81/1000 | Loss: 0.00001176
Iteration 82/1000 | Loss: 0.00001176
Iteration 83/1000 | Loss: 0.00001176
Iteration 84/1000 | Loss: 0.00001176
Iteration 85/1000 | Loss: 0.00001176
Iteration 86/1000 | Loss: 0.00001176
Iteration 87/1000 | Loss: 0.00001176
Iteration 88/1000 | Loss: 0.00001176
Iteration 89/1000 | Loss: 0.00001176
Iteration 90/1000 | Loss: 0.00001176
Iteration 91/1000 | Loss: 0.00001176
Iteration 92/1000 | Loss: 0.00001176
Iteration 93/1000 | Loss: 0.00001176
Iteration 94/1000 | Loss: 0.00001176
Iteration 95/1000 | Loss: 0.00001176
Iteration 96/1000 | Loss: 0.00001176
Iteration 97/1000 | Loss: 0.00001176
Iteration 98/1000 | Loss: 0.00001176
Iteration 99/1000 | Loss: 0.00001176
Iteration 100/1000 | Loss: 0.00001176
Iteration 101/1000 | Loss: 0.00001176
Iteration 102/1000 | Loss: 0.00001176
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 102. Stopping optimization.
Last 5 losses: [1.1763408110709861e-05, 1.1763408110709861e-05, 1.1763408110709861e-05, 1.1763408110709861e-05, 1.1763408110709861e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1763408110709861e-05

Optimization complete. Final v2v error: 2.971724510192871 mm

Highest mean error: 3.2773044109344482 mm for frame 29

Lowest mean error: 2.7978744506835938 mm for frame 69

Saving results

Total time: 33.40020418167114
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_36_nl_5493/0011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_nl_5493/0011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_nl_5493/0011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01132506
Iteration 2/25 | Loss: 0.00307969
Iteration 3/25 | Loss: 0.00209392
Iteration 4/25 | Loss: 0.00195418
Iteration 5/25 | Loss: 0.00185614
Iteration 6/25 | Loss: 0.00179201
Iteration 7/25 | Loss: 0.00182852
Iteration 8/25 | Loss: 0.00175941
Iteration 9/25 | Loss: 0.00162418
Iteration 10/25 | Loss: 0.00157628
Iteration 11/25 | Loss: 0.00138481
Iteration 12/25 | Loss: 0.00131610
Iteration 13/25 | Loss: 0.00130717
Iteration 14/25 | Loss: 0.00130345
Iteration 15/25 | Loss: 0.00130289
Iteration 16/25 | Loss: 0.00130288
Iteration 17/25 | Loss: 0.00130288
Iteration 18/25 | Loss: 0.00130288
Iteration 19/25 | Loss: 0.00130288
Iteration 20/25 | Loss: 0.00130288
Iteration 21/25 | Loss: 0.00130288
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.001302881515584886, 0.001302881515584886, 0.001302881515584886, 0.001302881515584886, 0.001302881515584886]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001302881515584886

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.23295915
Iteration 2/25 | Loss: 0.00204572
Iteration 3/25 | Loss: 0.00204572
Iteration 4/25 | Loss: 0.00204572
Iteration 5/25 | Loss: 0.00204572
Iteration 6/25 | Loss: 0.00204572
Iteration 7/25 | Loss: 0.00204572
Iteration 8/25 | Loss: 0.00204572
Iteration 9/25 | Loss: 0.00204572
Iteration 10/25 | Loss: 0.00204572
Iteration 11/25 | Loss: 0.00204572
Iteration 12/25 | Loss: 0.00204572
Iteration 13/25 | Loss: 0.00204572
Iteration 14/25 | Loss: 0.00204572
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0020457173231989145, 0.0020457173231989145, 0.0020457173231989145, 0.0020457173231989145, 0.0020457173231989145]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0020457173231989145

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00204572
Iteration 2/1000 | Loss: 0.00023361
Iteration 3/1000 | Loss: 0.00015971
Iteration 4/1000 | Loss: 0.00013384
Iteration 5/1000 | Loss: 0.00011499
Iteration 6/1000 | Loss: 0.00010872
Iteration 7/1000 | Loss: 0.00010400
Iteration 8/1000 | Loss: 0.00009953
Iteration 9/1000 | Loss: 0.00036335
Iteration 10/1000 | Loss: 0.00034443
Iteration 11/1000 | Loss: 0.00031625
Iteration 12/1000 | Loss: 0.00026330
Iteration 13/1000 | Loss: 0.00009171
Iteration 14/1000 | Loss: 0.00116554
Iteration 15/1000 | Loss: 0.01285087
Iteration 16/1000 | Loss: 0.00166954
Iteration 17/1000 | Loss: 0.00740941
Iteration 18/1000 | Loss: 0.00556916
Iteration 19/1000 | Loss: 0.00860664
Iteration 20/1000 | Loss: 0.00339453
Iteration 21/1000 | Loss: 0.00672808
Iteration 22/1000 | Loss: 0.00443590
Iteration 23/1000 | Loss: 0.00821413
Iteration 24/1000 | Loss: 0.00858713
Iteration 25/1000 | Loss: 0.00993238
Iteration 26/1000 | Loss: 0.00504981
Iteration 27/1000 | Loss: 0.00391749
Iteration 28/1000 | Loss: 0.00163058
Iteration 29/1000 | Loss: 0.00244163
Iteration 30/1000 | Loss: 0.00148972
Iteration 31/1000 | Loss: 0.00035298
Iteration 32/1000 | Loss: 0.00110011
Iteration 33/1000 | Loss: 0.00235563
Iteration 34/1000 | Loss: 0.00471722
Iteration 35/1000 | Loss: 0.00494539
Iteration 36/1000 | Loss: 0.00477605
Iteration 37/1000 | Loss: 0.00268827
Iteration 38/1000 | Loss: 0.00227052
Iteration 39/1000 | Loss: 0.00122544
Iteration 40/1000 | Loss: 0.00160517
Iteration 41/1000 | Loss: 0.00112737
Iteration 42/1000 | Loss: 0.00140886
Iteration 43/1000 | Loss: 0.00075007
Iteration 44/1000 | Loss: 0.00097140
Iteration 45/1000 | Loss: 0.00099689
Iteration 46/1000 | Loss: 0.00096812
Iteration 47/1000 | Loss: 0.00076375
Iteration 48/1000 | Loss: 0.00112831
Iteration 49/1000 | Loss: 0.00082915
Iteration 50/1000 | Loss: 0.00154589
Iteration 51/1000 | Loss: 0.00094958
Iteration 52/1000 | Loss: 0.00084425
Iteration 53/1000 | Loss: 0.00165287
Iteration 54/1000 | Loss: 0.00190220
Iteration 55/1000 | Loss: 0.00212699
Iteration 56/1000 | Loss: 0.00066637
Iteration 57/1000 | Loss: 0.00163935
Iteration 58/1000 | Loss: 0.00039099
Iteration 59/1000 | Loss: 0.00080746
Iteration 60/1000 | Loss: 0.00104309
Iteration 61/1000 | Loss: 0.00143209
Iteration 62/1000 | Loss: 0.00186441
Iteration 63/1000 | Loss: 0.00099533
Iteration 64/1000 | Loss: 0.00168340
Iteration 65/1000 | Loss: 0.00127816
Iteration 66/1000 | Loss: 0.00103374
Iteration 67/1000 | Loss: 0.00125094
Iteration 68/1000 | Loss: 0.00043196
Iteration 69/1000 | Loss: 0.00014619
Iteration 70/1000 | Loss: 0.00065362
Iteration 71/1000 | Loss: 0.00081009
Iteration 72/1000 | Loss: 0.00079536
Iteration 73/1000 | Loss: 0.00116167
Iteration 74/1000 | Loss: 0.00065725
Iteration 75/1000 | Loss: 0.00104649
Iteration 76/1000 | Loss: 0.00107891
Iteration 77/1000 | Loss: 0.00082691
Iteration 78/1000 | Loss: 0.00029903
Iteration 79/1000 | Loss: 0.00012901
Iteration 80/1000 | Loss: 0.00014826
Iteration 81/1000 | Loss: 0.00013339
Iteration 82/1000 | Loss: 0.00011081
Iteration 83/1000 | Loss: 0.00009379
Iteration 84/1000 | Loss: 0.00007292
Iteration 85/1000 | Loss: 0.00063772
Iteration 86/1000 | Loss: 0.00033737
Iteration 87/1000 | Loss: 0.00079225
Iteration 88/1000 | Loss: 0.00010175
Iteration 89/1000 | Loss: 0.00008168
Iteration 90/1000 | Loss: 0.00007100
Iteration 91/1000 | Loss: 0.00068611
Iteration 92/1000 | Loss: 0.00019378
Iteration 93/1000 | Loss: 0.00039621
Iteration 94/1000 | Loss: 0.00033853
Iteration 95/1000 | Loss: 0.00007153
Iteration 96/1000 | Loss: 0.00024880
Iteration 97/1000 | Loss: 0.00007816
Iteration 98/1000 | Loss: 0.00009547
Iteration 99/1000 | Loss: 0.00063954
Iteration 100/1000 | Loss: 0.00030507
Iteration 101/1000 | Loss: 0.00050467
Iteration 102/1000 | Loss: 0.00007583
Iteration 103/1000 | Loss: 0.00006733
Iteration 104/1000 | Loss: 0.00032970
Iteration 105/1000 | Loss: 0.00008551
Iteration 106/1000 | Loss: 0.00006687
Iteration 107/1000 | Loss: 0.00009321
Iteration 108/1000 | Loss: 0.00007232
Iteration 109/1000 | Loss: 0.00059874
Iteration 110/1000 | Loss: 0.00047990
Iteration 111/1000 | Loss: 0.00007435
Iteration 112/1000 | Loss: 0.00006386
Iteration 113/1000 | Loss: 0.00006886
Iteration 114/1000 | Loss: 0.00107008
Iteration 115/1000 | Loss: 0.00033315
Iteration 116/1000 | Loss: 0.00006475
Iteration 117/1000 | Loss: 0.00006573
Iteration 118/1000 | Loss: 0.00057481
Iteration 119/1000 | Loss: 0.00016310
Iteration 120/1000 | Loss: 0.00006719
Iteration 121/1000 | Loss: 0.00186098
Iteration 122/1000 | Loss: 0.00216751
Iteration 123/1000 | Loss: 0.00072566
Iteration 124/1000 | Loss: 0.00021454
Iteration 125/1000 | Loss: 0.00028792
Iteration 126/1000 | Loss: 0.00051913
Iteration 127/1000 | Loss: 0.00056604
Iteration 128/1000 | Loss: 0.00036214
Iteration 129/1000 | Loss: 0.00010263
Iteration 130/1000 | Loss: 0.00015947
Iteration 131/1000 | Loss: 0.00041645
Iteration 132/1000 | Loss: 0.00048911
Iteration 133/1000 | Loss: 0.00045809
Iteration 134/1000 | Loss: 0.00035366
Iteration 135/1000 | Loss: 0.00058411
Iteration 136/1000 | Loss: 0.00032725
Iteration 137/1000 | Loss: 0.00030241
Iteration 138/1000 | Loss: 0.00085113
Iteration 139/1000 | Loss: 0.00079127
Iteration 140/1000 | Loss: 0.00036506
Iteration 141/1000 | Loss: 0.00061076
Iteration 142/1000 | Loss: 0.00045775
Iteration 143/1000 | Loss: 0.00040460
Iteration 144/1000 | Loss: 0.00058783
Iteration 145/1000 | Loss: 0.00062570
Iteration 146/1000 | Loss: 0.00053553
Iteration 147/1000 | Loss: 0.00038463
Iteration 148/1000 | Loss: 0.00064548
Iteration 149/1000 | Loss: 0.00058350
Iteration 150/1000 | Loss: 0.00083846
Iteration 151/1000 | Loss: 0.00055352
Iteration 152/1000 | Loss: 0.00021770
Iteration 153/1000 | Loss: 0.00053936
Iteration 154/1000 | Loss: 0.00043371
Iteration 155/1000 | Loss: 0.00081098
Iteration 156/1000 | Loss: 0.00016422
Iteration 157/1000 | Loss: 0.00041710
Iteration 158/1000 | Loss: 0.00021346
Iteration 159/1000 | Loss: 0.00023955
Iteration 160/1000 | Loss: 0.00069881
Iteration 161/1000 | Loss: 0.00069013
Iteration 162/1000 | Loss: 0.00025239
Iteration 163/1000 | Loss: 0.00015294
Iteration 164/1000 | Loss: 0.00025272
Iteration 165/1000 | Loss: 0.00008276
Iteration 166/1000 | Loss: 0.00012568
Iteration 167/1000 | Loss: 0.00007091
Iteration 168/1000 | Loss: 0.00006767
Iteration 169/1000 | Loss: 0.00006173
Iteration 170/1000 | Loss: 0.00006052
Iteration 171/1000 | Loss: 0.00007977
Iteration 172/1000 | Loss: 0.00005835
Iteration 173/1000 | Loss: 0.00005725
Iteration 174/1000 | Loss: 0.00005652
Iteration 175/1000 | Loss: 0.00045529
Iteration 176/1000 | Loss: 0.00042094
Iteration 177/1000 | Loss: 0.00048542
Iteration 178/1000 | Loss: 0.00026577
Iteration 179/1000 | Loss: 0.00005752
Iteration 180/1000 | Loss: 0.00005828
Iteration 181/1000 | Loss: 0.00005935
Iteration 182/1000 | Loss: 0.00005565
Iteration 183/1000 | Loss: 0.00005543
Iteration 184/1000 | Loss: 0.00007230
Iteration 185/1000 | Loss: 0.00167862
Iteration 186/1000 | Loss: 0.00069291
Iteration 187/1000 | Loss: 0.00010865
Iteration 188/1000 | Loss: 0.00052908
Iteration 189/1000 | Loss: 0.00081214
Iteration 190/1000 | Loss: 0.00087252
Iteration 191/1000 | Loss: 0.00039282
Iteration 192/1000 | Loss: 0.00065009
Iteration 193/1000 | Loss: 0.00072083
Iteration 194/1000 | Loss: 0.00073466
Iteration 195/1000 | Loss: 0.00031689
Iteration 196/1000 | Loss: 0.00042079
Iteration 197/1000 | Loss: 0.00049225
Iteration 198/1000 | Loss: 0.00010568
Iteration 199/1000 | Loss: 0.00006460
Iteration 200/1000 | Loss: 0.00006737
Iteration 201/1000 | Loss: 0.00005633
Iteration 202/1000 | Loss: 0.00008184
Iteration 203/1000 | Loss: 0.00005570
Iteration 204/1000 | Loss: 0.00005543
Iteration 205/1000 | Loss: 0.00008858
Iteration 206/1000 | Loss: 0.00005515
Iteration 207/1000 | Loss: 0.00005502
Iteration 208/1000 | Loss: 0.00005498
Iteration 209/1000 | Loss: 0.00005498
Iteration 210/1000 | Loss: 0.00005496
Iteration 211/1000 | Loss: 0.00005496
Iteration 212/1000 | Loss: 0.00005494
Iteration 213/1000 | Loss: 0.00005494
Iteration 214/1000 | Loss: 0.00005493
Iteration 215/1000 | Loss: 0.00007586
Iteration 216/1000 | Loss: 0.00145387
Iteration 217/1000 | Loss: 0.00240207
Iteration 218/1000 | Loss: 0.00079215
Iteration 219/1000 | Loss: 0.00091545
Iteration 220/1000 | Loss: 0.00060846
Iteration 221/1000 | Loss: 0.00124876
Iteration 222/1000 | Loss: 0.00039238
Iteration 223/1000 | Loss: 0.00041924
Iteration 224/1000 | Loss: 0.00055028
Iteration 225/1000 | Loss: 0.00076934
Iteration 226/1000 | Loss: 0.00040678
Iteration 227/1000 | Loss: 0.00033326
Iteration 228/1000 | Loss: 0.00066120
Iteration 229/1000 | Loss: 0.00055650
Iteration 230/1000 | Loss: 0.00039842
Iteration 231/1000 | Loss: 0.00071772
Iteration 232/1000 | Loss: 0.00063771
Iteration 233/1000 | Loss: 0.00053447
Iteration 234/1000 | Loss: 0.00094392
Iteration 235/1000 | Loss: 0.00126966
Iteration 236/1000 | Loss: 0.00072794
Iteration 237/1000 | Loss: 0.00167457
Iteration 238/1000 | Loss: 0.00011652
Iteration 239/1000 | Loss: 0.00007738
Iteration 240/1000 | Loss: 0.00103527
Iteration 241/1000 | Loss: 0.00066286
Iteration 242/1000 | Loss: 0.00055408
Iteration 243/1000 | Loss: 0.00040694
Iteration 244/1000 | Loss: 0.00049115
Iteration 245/1000 | Loss: 0.00005967
Iteration 246/1000 | Loss: 0.00005817
Iteration 247/1000 | Loss: 0.00006843
Iteration 248/1000 | Loss: 0.00015452
Iteration 249/1000 | Loss: 0.00005291
Iteration 250/1000 | Loss: 0.00005493
Iteration 251/1000 | Loss: 0.00005113
Iteration 252/1000 | Loss: 0.00004891
Iteration 253/1000 | Loss: 0.00004849
Iteration 254/1000 | Loss: 0.00004833
Iteration 255/1000 | Loss: 0.00048503
Iteration 256/1000 | Loss: 0.00048436
Iteration 257/1000 | Loss: 0.00005305
Iteration 258/1000 | Loss: 0.00006238
Iteration 259/1000 | Loss: 0.00004810
Iteration 260/1000 | Loss: 0.00004798
Iteration 261/1000 | Loss: 0.00004795
Iteration 262/1000 | Loss: 0.00004794
Iteration 263/1000 | Loss: 0.00004793
Iteration 264/1000 | Loss: 0.00004791
Iteration 265/1000 | Loss: 0.00004788
Iteration 266/1000 | Loss: 0.00004788
Iteration 267/1000 | Loss: 0.00004788
Iteration 268/1000 | Loss: 0.00004788
Iteration 269/1000 | Loss: 0.00004788
Iteration 270/1000 | Loss: 0.00004788
Iteration 271/1000 | Loss: 0.00004788
Iteration 272/1000 | Loss: 0.00004784
Iteration 273/1000 | Loss: 0.00004784
Iteration 274/1000 | Loss: 0.00004784
Iteration 275/1000 | Loss: 0.00004784
Iteration 276/1000 | Loss: 0.00004784
Iteration 277/1000 | Loss: 0.00004784
Iteration 278/1000 | Loss: 0.00004784
Iteration 279/1000 | Loss: 0.00004784
Iteration 280/1000 | Loss: 0.00004783
Iteration 281/1000 | Loss: 0.00004783
Iteration 282/1000 | Loss: 0.00004783
Iteration 283/1000 | Loss: 0.00004783
Iteration 284/1000 | Loss: 0.00004783
Iteration 285/1000 | Loss: 0.00004783
Iteration 286/1000 | Loss: 0.00004783
Iteration 287/1000 | Loss: 0.00004783
Iteration 288/1000 | Loss: 0.00004783
Iteration 289/1000 | Loss: 0.00004782
Iteration 290/1000 | Loss: 0.00004782
Iteration 291/1000 | Loss: 0.00004782
Iteration 292/1000 | Loss: 0.00004782
Iteration 293/1000 | Loss: 0.00004782
Iteration 294/1000 | Loss: 0.00004782
Iteration 295/1000 | Loss: 0.00004781
Iteration 296/1000 | Loss: 0.00004781
Iteration 297/1000 | Loss: 0.00004781
Iteration 298/1000 | Loss: 0.00004781
Iteration 299/1000 | Loss: 0.00004781
Iteration 300/1000 | Loss: 0.00004781
Iteration 301/1000 | Loss: 0.00004780
Iteration 302/1000 | Loss: 0.00004780
Iteration 303/1000 | Loss: 0.00004780
Iteration 304/1000 | Loss: 0.00004780
Iteration 305/1000 | Loss: 0.00004780
Iteration 306/1000 | Loss: 0.00004780
Iteration 307/1000 | Loss: 0.00004780
Iteration 308/1000 | Loss: 0.00004780
Iteration 309/1000 | Loss: 0.00004780
Iteration 310/1000 | Loss: 0.00004780
Iteration 311/1000 | Loss: 0.00004779
Iteration 312/1000 | Loss: 0.00004779
Iteration 313/1000 | Loss: 0.00004778
Iteration 314/1000 | Loss: 0.00004778
Iteration 315/1000 | Loss: 0.00004778
Iteration 316/1000 | Loss: 0.00004778
Iteration 317/1000 | Loss: 0.00004778
Iteration 318/1000 | Loss: 0.00004778
Iteration 319/1000 | Loss: 0.00004778
Iteration 320/1000 | Loss: 0.00004778
Iteration 321/1000 | Loss: 0.00004778
Iteration 322/1000 | Loss: 0.00004778
Iteration 323/1000 | Loss: 0.00004778
Iteration 324/1000 | Loss: 0.00004778
Iteration 325/1000 | Loss: 0.00004778
Iteration 326/1000 | Loss: 0.00004778
Iteration 327/1000 | Loss: 0.00004778
Iteration 328/1000 | Loss: 0.00004778
Iteration 329/1000 | Loss: 0.00004778
Iteration 330/1000 | Loss: 0.00004778
Iteration 331/1000 | Loss: 0.00004778
Iteration 332/1000 | Loss: 0.00004778
Iteration 333/1000 | Loss: 0.00004778
Iteration 334/1000 | Loss: 0.00004778
Iteration 335/1000 | Loss: 0.00004778
Iteration 336/1000 | Loss: 0.00004778
Iteration 337/1000 | Loss: 0.00004778
Iteration 338/1000 | Loss: 0.00004778
Iteration 339/1000 | Loss: 0.00004778
Iteration 340/1000 | Loss: 0.00004778
Iteration 341/1000 | Loss: 0.00004778
Iteration 342/1000 | Loss: 0.00004778
Iteration 343/1000 | Loss: 0.00004778
Iteration 344/1000 | Loss: 0.00004778
Iteration 345/1000 | Loss: 0.00004778
Iteration 346/1000 | Loss: 0.00004778
Iteration 347/1000 | Loss: 0.00004778
Iteration 348/1000 | Loss: 0.00004778
Iteration 349/1000 | Loss: 0.00004778
Iteration 350/1000 | Loss: 0.00004778
Iteration 351/1000 | Loss: 0.00004778
Iteration 352/1000 | Loss: 0.00004778
Iteration 353/1000 | Loss: 0.00004778
Iteration 354/1000 | Loss: 0.00004778
Iteration 355/1000 | Loss: 0.00004778
Iteration 356/1000 | Loss: 0.00004778
Iteration 357/1000 | Loss: 0.00004778
Iteration 358/1000 | Loss: 0.00004778
Iteration 359/1000 | Loss: 0.00004778
Iteration 360/1000 | Loss: 0.00004778
Iteration 361/1000 | Loss: 0.00004778
Iteration 362/1000 | Loss: 0.00004778
Iteration 363/1000 | Loss: 0.00004778
Iteration 364/1000 | Loss: 0.00004778
Iteration 365/1000 | Loss: 0.00004778
Iteration 366/1000 | Loss: 0.00004778
Iteration 367/1000 | Loss: 0.00004778
Iteration 368/1000 | Loss: 0.00004778
Iteration 369/1000 | Loss: 0.00004778
Iteration 370/1000 | Loss: 0.00004778
Iteration 371/1000 | Loss: 0.00004778
Iteration 372/1000 | Loss: 0.00004778
Iteration 373/1000 | Loss: 0.00004778
Iteration 374/1000 | Loss: 0.00004778
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 374. Stopping optimization.
Last 5 losses: [4.777968570124358e-05, 4.777968570124358e-05, 4.777968570124358e-05, 4.777968570124358e-05, 4.777968570124358e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.777968570124358e-05

Optimization complete. Final v2v error: 3.5732362270355225 mm

Highest mean error: 12.268600463867188 mm for frame 13

Lowest mean error: 2.729909658432007 mm for frame 3

Saving results

Total time: 393.4894142150879
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_36_nl_5493/0020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_nl_5493/0020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_nl_5493/0020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01115399
Iteration 2/25 | Loss: 0.01115398
Iteration 3/25 | Loss: 0.01115398
Iteration 4/25 | Loss: 0.01115398
Iteration 5/25 | Loss: 0.01115398
Iteration 6/25 | Loss: 0.01115398
Iteration 7/25 | Loss: 0.01115398
Iteration 8/25 | Loss: 0.01115398
Iteration 9/25 | Loss: 0.01115398
Iteration 10/25 | Loss: 0.01115397
Iteration 11/25 | Loss: 0.01115397
Iteration 12/25 | Loss: 0.01115397
Iteration 13/25 | Loss: 0.01115397
Iteration 14/25 | Loss: 0.01115397
Iteration 15/25 | Loss: 0.01115397
Iteration 16/25 | Loss: 0.01115397
Iteration 17/25 | Loss: 0.01115397
Iteration 18/25 | Loss: 0.01115396
Iteration 19/25 | Loss: 0.01115396
Iteration 20/25 | Loss: 0.01115396
Iteration 21/25 | Loss: 0.01115396
Iteration 22/25 | Loss: 0.01115396
Iteration 23/25 | Loss: 0.01115396
Iteration 24/25 | Loss: 0.01115396
Iteration 25/25 | Loss: 0.01115396

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.76976395
Iteration 2/25 | Loss: 0.06179642
Iteration 3/25 | Loss: 0.06178160
Iteration 4/25 | Loss: 0.06178159
Iteration 5/25 | Loss: 0.06178159
Iteration 6/25 | Loss: 0.06178159
Iteration 7/25 | Loss: 0.06178159
Iteration 8/25 | Loss: 0.06178159
Iteration 9/25 | Loss: 0.06178160
Iteration 10/25 | Loss: 0.06178160
Iteration 11/25 | Loss: 0.06178159
Iteration 12/25 | Loss: 0.06178159
Iteration 13/25 | Loss: 0.06178159
Iteration 14/25 | Loss: 0.06178159
Iteration 15/25 | Loss: 0.06178159
Iteration 16/25 | Loss: 0.06178159
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.06178158521652222, 0.06178158521652222, 0.06178158521652222, 0.06178158521652222, 0.06178158521652222]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.06178158521652222

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.06178159
Iteration 2/1000 | Loss: 0.00401123
Iteration 3/1000 | Loss: 0.00059235
Iteration 4/1000 | Loss: 0.00030060
Iteration 5/1000 | Loss: 0.00015673
Iteration 6/1000 | Loss: 0.00010471
Iteration 7/1000 | Loss: 0.00007901
Iteration 8/1000 | Loss: 0.00006514
Iteration 9/1000 | Loss: 0.00005598
Iteration 10/1000 | Loss: 0.00004950
Iteration 11/1000 | Loss: 0.00004329
Iteration 12/1000 | Loss: 0.00003876
Iteration 13/1000 | Loss: 0.00003504
Iteration 14/1000 | Loss: 0.00003204
Iteration 15/1000 | Loss: 0.00027139
Iteration 16/1000 | Loss: 0.00018372
Iteration 17/1000 | Loss: 0.00012804
Iteration 18/1000 | Loss: 0.00015768
Iteration 19/1000 | Loss: 0.00013001
Iteration 20/1000 | Loss: 0.00014319
Iteration 21/1000 | Loss: 0.00013395
Iteration 22/1000 | Loss: 0.00003077
Iteration 23/1000 | Loss: 0.00002725
Iteration 24/1000 | Loss: 0.00002548
Iteration 25/1000 | Loss: 0.00002429
Iteration 26/1000 | Loss: 0.00002379
Iteration 27/1000 | Loss: 0.00002310
Iteration 28/1000 | Loss: 0.00002254
Iteration 29/1000 | Loss: 0.00002243
Iteration 30/1000 | Loss: 0.00002199
Iteration 31/1000 | Loss: 0.00002170
Iteration 32/1000 | Loss: 0.00002146
Iteration 33/1000 | Loss: 0.00002139
Iteration 34/1000 | Loss: 0.00002119
Iteration 35/1000 | Loss: 0.00002107
Iteration 36/1000 | Loss: 0.00002096
Iteration 37/1000 | Loss: 0.00002089
Iteration 38/1000 | Loss: 0.00002089
Iteration 39/1000 | Loss: 0.00002087
Iteration 40/1000 | Loss: 0.00002086
Iteration 41/1000 | Loss: 0.00002086
Iteration 42/1000 | Loss: 0.00002086
Iteration 43/1000 | Loss: 0.00002085
Iteration 44/1000 | Loss: 0.00002085
Iteration 45/1000 | Loss: 0.00002085
Iteration 46/1000 | Loss: 0.00002085
Iteration 47/1000 | Loss: 0.00002085
Iteration 48/1000 | Loss: 0.00002085
Iteration 49/1000 | Loss: 0.00002085
Iteration 50/1000 | Loss: 0.00002085
Iteration 51/1000 | Loss: 0.00002085
Iteration 52/1000 | Loss: 0.00002080
Iteration 53/1000 | Loss: 0.00002080
Iteration 54/1000 | Loss: 0.00002075
Iteration 55/1000 | Loss: 0.00002074
Iteration 56/1000 | Loss: 0.00002072
Iteration 57/1000 | Loss: 0.00002072
Iteration 58/1000 | Loss: 0.00002071
Iteration 59/1000 | Loss: 0.00002071
Iteration 60/1000 | Loss: 0.00002071
Iteration 61/1000 | Loss: 0.00002071
Iteration 62/1000 | Loss: 0.00002071
Iteration 63/1000 | Loss: 0.00002070
Iteration 64/1000 | Loss: 0.00002070
Iteration 65/1000 | Loss: 0.00002068
Iteration 66/1000 | Loss: 0.00002067
Iteration 67/1000 | Loss: 0.00002067
Iteration 68/1000 | Loss: 0.00002067
Iteration 69/1000 | Loss: 0.00002067
Iteration 70/1000 | Loss: 0.00002066
Iteration 71/1000 | Loss: 0.00002066
Iteration 72/1000 | Loss: 0.00002066
Iteration 73/1000 | Loss: 0.00002065
Iteration 74/1000 | Loss: 0.00002064
Iteration 75/1000 | Loss: 0.00002063
Iteration 76/1000 | Loss: 0.00002063
Iteration 77/1000 | Loss: 0.00002061
Iteration 78/1000 | Loss: 0.00002061
Iteration 79/1000 | Loss: 0.00002061
Iteration 80/1000 | Loss: 0.00002060
Iteration 81/1000 | Loss: 0.00002060
Iteration 82/1000 | Loss: 0.00002060
Iteration 83/1000 | Loss: 0.00002059
Iteration 84/1000 | Loss: 0.00002059
Iteration 85/1000 | Loss: 0.00002059
Iteration 86/1000 | Loss: 0.00002058
Iteration 87/1000 | Loss: 0.00002058
Iteration 88/1000 | Loss: 0.00002058
Iteration 89/1000 | Loss: 0.00002057
Iteration 90/1000 | Loss: 0.00002057
Iteration 91/1000 | Loss: 0.00002057
Iteration 92/1000 | Loss: 0.00002056
Iteration 93/1000 | Loss: 0.00002056
Iteration 94/1000 | Loss: 0.00002056
Iteration 95/1000 | Loss: 0.00002056
Iteration 96/1000 | Loss: 0.00002055
Iteration 97/1000 | Loss: 0.00002055
Iteration 98/1000 | Loss: 0.00002055
Iteration 99/1000 | Loss: 0.00002055
Iteration 100/1000 | Loss: 0.00002055
Iteration 101/1000 | Loss: 0.00002054
Iteration 102/1000 | Loss: 0.00002054
Iteration 103/1000 | Loss: 0.00002054
Iteration 104/1000 | Loss: 0.00002054
Iteration 105/1000 | Loss: 0.00002054
Iteration 106/1000 | Loss: 0.00002053
Iteration 107/1000 | Loss: 0.00002053
Iteration 108/1000 | Loss: 0.00002053
Iteration 109/1000 | Loss: 0.00002053
Iteration 110/1000 | Loss: 0.00002052
Iteration 111/1000 | Loss: 0.00002052
Iteration 112/1000 | Loss: 0.00002051
Iteration 113/1000 | Loss: 0.00002051
Iteration 114/1000 | Loss: 0.00002050
Iteration 115/1000 | Loss: 0.00002050
Iteration 116/1000 | Loss: 0.00002050
Iteration 117/1000 | Loss: 0.00002049
Iteration 118/1000 | Loss: 0.00002049
Iteration 119/1000 | Loss: 0.00002049
Iteration 120/1000 | Loss: 0.00002049
Iteration 121/1000 | Loss: 0.00002048
Iteration 122/1000 | Loss: 0.00002063
Iteration 123/1000 | Loss: 0.00002063
Iteration 124/1000 | Loss: 0.00002063
Iteration 125/1000 | Loss: 0.00002063
Iteration 126/1000 | Loss: 0.00002062
Iteration 127/1000 | Loss: 0.00002062
Iteration 128/1000 | Loss: 0.00002062
Iteration 129/1000 | Loss: 0.00002062
Iteration 130/1000 | Loss: 0.00002062
Iteration 131/1000 | Loss: 0.00002062
Iteration 132/1000 | Loss: 0.00002061
Iteration 133/1000 | Loss: 0.00002053
Iteration 134/1000 | Loss: 0.00002052
Iteration 135/1000 | Loss: 0.00002052
Iteration 136/1000 | Loss: 0.00002052
Iteration 137/1000 | Loss: 0.00002052
Iteration 138/1000 | Loss: 0.00002052
Iteration 139/1000 | Loss: 0.00002052
Iteration 140/1000 | Loss: 0.00002052
Iteration 141/1000 | Loss: 0.00002052
Iteration 142/1000 | Loss: 0.00002052
Iteration 143/1000 | Loss: 0.00002052
Iteration 144/1000 | Loss: 0.00002051
Iteration 145/1000 | Loss: 0.00002051
Iteration 146/1000 | Loss: 0.00002051
Iteration 147/1000 | Loss: 0.00002051
Iteration 148/1000 | Loss: 0.00002051
Iteration 149/1000 | Loss: 0.00002051
Iteration 150/1000 | Loss: 0.00002051
Iteration 151/1000 | Loss: 0.00002051
Iteration 152/1000 | Loss: 0.00002051
Iteration 153/1000 | Loss: 0.00002051
Iteration 154/1000 | Loss: 0.00002051
Iteration 155/1000 | Loss: 0.00002050
Iteration 156/1000 | Loss: 0.00002050
Iteration 157/1000 | Loss: 0.00002050
Iteration 158/1000 | Loss: 0.00002050
Iteration 159/1000 | Loss: 0.00002050
Iteration 160/1000 | Loss: 0.00002050
Iteration 161/1000 | Loss: 0.00002050
Iteration 162/1000 | Loss: 0.00002050
Iteration 163/1000 | Loss: 0.00002050
Iteration 164/1000 | Loss: 0.00002050
Iteration 165/1000 | Loss: 0.00002050
Iteration 166/1000 | Loss: 0.00002049
Iteration 167/1000 | Loss: 0.00002049
Iteration 168/1000 | Loss: 0.00002049
Iteration 169/1000 | Loss: 0.00002049
Iteration 170/1000 | Loss: 0.00002049
Iteration 171/1000 | Loss: 0.00002049
Iteration 172/1000 | Loss: 0.00002049
Iteration 173/1000 | Loss: 0.00002049
Iteration 174/1000 | Loss: 0.00002049
Iteration 175/1000 | Loss: 0.00002049
Iteration 176/1000 | Loss: 0.00002049
Iteration 177/1000 | Loss: 0.00002049
Iteration 178/1000 | Loss: 0.00002049
Iteration 179/1000 | Loss: 0.00002049
Iteration 180/1000 | Loss: 0.00002049
Iteration 181/1000 | Loss: 0.00002049
Iteration 182/1000 | Loss: 0.00002049
Iteration 183/1000 | Loss: 0.00002049
Iteration 184/1000 | Loss: 0.00002049
Iteration 185/1000 | Loss: 0.00002049
Iteration 186/1000 | Loss: 0.00002049
Iteration 187/1000 | Loss: 0.00002049
Iteration 188/1000 | Loss: 0.00002049
Iteration 189/1000 | Loss: 0.00002049
Iteration 190/1000 | Loss: 0.00002049
Iteration 191/1000 | Loss: 0.00002049
Iteration 192/1000 | Loss: 0.00002049
Iteration 193/1000 | Loss: 0.00002049
Iteration 194/1000 | Loss: 0.00002049
Iteration 195/1000 | Loss: 0.00002049
Iteration 196/1000 | Loss: 0.00002049
Iteration 197/1000 | Loss: 0.00002049
Iteration 198/1000 | Loss: 0.00002049
Iteration 199/1000 | Loss: 0.00002049
Iteration 200/1000 | Loss: 0.00002049
Iteration 201/1000 | Loss: 0.00002049
Iteration 202/1000 | Loss: 0.00002049
Iteration 203/1000 | Loss: 0.00002049
Iteration 204/1000 | Loss: 0.00002049
Iteration 205/1000 | Loss: 0.00002049
Iteration 206/1000 | Loss: 0.00002049
Iteration 207/1000 | Loss: 0.00002049
Iteration 208/1000 | Loss: 0.00002049
Iteration 209/1000 | Loss: 0.00002049
Iteration 210/1000 | Loss: 0.00002049
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 210. Stopping optimization.
Last 5 losses: [2.0491517716436647e-05, 2.0491517716436647e-05, 2.0491517716436647e-05, 2.0491517716436647e-05, 2.0491517716436647e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0491517716436647e-05

Optimization complete. Final v2v error: 3.3864428997039795 mm

Highest mean error: 14.851139068603516 mm for frame 72

Lowest mean error: 2.6042110919952393 mm for frame 90

Saving results

Total time: 81.74151134490967
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_36_nl_5493/0014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_nl_5493/0014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_nl_5493/0014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01076222
Iteration 2/25 | Loss: 0.00243104
Iteration 3/25 | Loss: 0.00327539
Iteration 4/25 | Loss: 0.00227759
Iteration 5/25 | Loss: 0.00186195
Iteration 6/25 | Loss: 0.00156604
Iteration 7/25 | Loss: 0.00131895
Iteration 8/25 | Loss: 0.00125135
Iteration 9/25 | Loss: 0.00121848
Iteration 10/25 | Loss: 0.00118754
Iteration 11/25 | Loss: 0.00120430
Iteration 12/25 | Loss: 0.00116533
Iteration 13/25 | Loss: 0.00114967
Iteration 14/25 | Loss: 0.00114286
Iteration 15/25 | Loss: 0.00118978
Iteration 16/25 | Loss: 0.00115832
Iteration 17/25 | Loss: 0.00115331
Iteration 18/25 | Loss: 0.00114143
Iteration 19/25 | Loss: 0.00116019
Iteration 20/25 | Loss: 0.00114059
Iteration 21/25 | Loss: 0.00113231
Iteration 22/25 | Loss: 0.00112444
Iteration 23/25 | Loss: 0.00112210
Iteration 24/25 | Loss: 0.00111904
Iteration 25/25 | Loss: 0.00111707

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34006631
Iteration 2/25 | Loss: 0.00160391
Iteration 3/25 | Loss: 0.00115412
Iteration 4/25 | Loss: 0.00115412
Iteration 5/25 | Loss: 0.00115412
Iteration 6/25 | Loss: 0.00115412
Iteration 7/25 | Loss: 0.00115412
Iteration 8/25 | Loss: 0.00115412
Iteration 9/25 | Loss: 0.00115412
Iteration 10/25 | Loss: 0.00115412
Iteration 11/25 | Loss: 0.00115412
Iteration 12/25 | Loss: 0.00115412
Iteration 13/25 | Loss: 0.00115412
Iteration 14/25 | Loss: 0.00115412
Iteration 15/25 | Loss: 0.00115412
Iteration 16/25 | Loss: 0.00115412
Iteration 17/25 | Loss: 0.00115412
Iteration 18/25 | Loss: 0.00115412
Iteration 19/25 | Loss: 0.00115412
Iteration 20/25 | Loss: 0.00115412
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0011541161220520735, 0.0011541161220520735, 0.0011541161220520735, 0.0011541161220520735, 0.0011541161220520735]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011541161220520735

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00115412
Iteration 2/1000 | Loss: 0.00006499
Iteration 3/1000 | Loss: 0.00071909
Iteration 4/1000 | Loss: 0.00003687
Iteration 5/1000 | Loss: 0.00003416
Iteration 6/1000 | Loss: 0.00003200
Iteration 7/1000 | Loss: 0.00060231
Iteration 8/1000 | Loss: 0.00004943
Iteration 9/1000 | Loss: 0.00003354
Iteration 10/1000 | Loss: 0.00003068
Iteration 11/1000 | Loss: 0.00002894
Iteration 12/1000 | Loss: 0.00002757
Iteration 13/1000 | Loss: 0.00018661
Iteration 14/1000 | Loss: 0.00003241
Iteration 15/1000 | Loss: 0.00002777
Iteration 16/1000 | Loss: 0.00002667
Iteration 17/1000 | Loss: 0.00002538
Iteration 18/1000 | Loss: 0.00002475
Iteration 19/1000 | Loss: 0.00002450
Iteration 20/1000 | Loss: 0.00002438
Iteration 21/1000 | Loss: 0.00002428
Iteration 22/1000 | Loss: 0.00002415
Iteration 23/1000 | Loss: 0.00002414
Iteration 24/1000 | Loss: 0.00002408
Iteration 25/1000 | Loss: 0.00002404
Iteration 26/1000 | Loss: 0.00002404
Iteration 27/1000 | Loss: 0.00002404
Iteration 28/1000 | Loss: 0.00002404
Iteration 29/1000 | Loss: 0.00002404
Iteration 30/1000 | Loss: 0.00002404
Iteration 31/1000 | Loss: 0.00002404
Iteration 32/1000 | Loss: 0.00002404
Iteration 33/1000 | Loss: 0.00002403
Iteration 34/1000 | Loss: 0.00002403
Iteration 35/1000 | Loss: 0.00002403
Iteration 36/1000 | Loss: 0.00002403
Iteration 37/1000 | Loss: 0.00002397
Iteration 38/1000 | Loss: 0.00002396
Iteration 39/1000 | Loss: 0.00002395
Iteration 40/1000 | Loss: 0.00002394
Iteration 41/1000 | Loss: 0.00002394
Iteration 42/1000 | Loss: 0.00002394
Iteration 43/1000 | Loss: 0.00002393
Iteration 44/1000 | Loss: 0.00002393
Iteration 45/1000 | Loss: 0.00002391
Iteration 46/1000 | Loss: 0.00002390
Iteration 47/1000 | Loss: 0.00002388
Iteration 48/1000 | Loss: 0.00002388
Iteration 49/1000 | Loss: 0.00002387
Iteration 50/1000 | Loss: 0.00002387
Iteration 51/1000 | Loss: 0.00002386
Iteration 52/1000 | Loss: 0.00002386
Iteration 53/1000 | Loss: 0.00002386
Iteration 54/1000 | Loss: 0.00002386
Iteration 55/1000 | Loss: 0.00002386
Iteration 56/1000 | Loss: 0.00002385
Iteration 57/1000 | Loss: 0.00002385
Iteration 58/1000 | Loss: 0.00002385
Iteration 59/1000 | Loss: 0.00002384
Iteration 60/1000 | Loss: 0.00002383
Iteration 61/1000 | Loss: 0.00002383
Iteration 62/1000 | Loss: 0.00002383
Iteration 63/1000 | Loss: 0.00002383
Iteration 64/1000 | Loss: 0.00002383
Iteration 65/1000 | Loss: 0.00002383
Iteration 66/1000 | Loss: 0.00002382
Iteration 67/1000 | Loss: 0.00002382
Iteration 68/1000 | Loss: 0.00002382
Iteration 69/1000 | Loss: 0.00002382
Iteration 70/1000 | Loss: 0.00002382
Iteration 71/1000 | Loss: 0.00002381
Iteration 72/1000 | Loss: 0.00002381
Iteration 73/1000 | Loss: 0.00002381
Iteration 74/1000 | Loss: 0.00002381
Iteration 75/1000 | Loss: 0.00002380
Iteration 76/1000 | Loss: 0.00002380
Iteration 77/1000 | Loss: 0.00002380
Iteration 78/1000 | Loss: 0.00002379
Iteration 79/1000 | Loss: 0.00002379
Iteration 80/1000 | Loss: 0.00002379
Iteration 81/1000 | Loss: 0.00002379
Iteration 82/1000 | Loss: 0.00002379
Iteration 83/1000 | Loss: 0.00002379
Iteration 84/1000 | Loss: 0.00002379
Iteration 85/1000 | Loss: 0.00002378
Iteration 86/1000 | Loss: 0.00002378
Iteration 87/1000 | Loss: 0.00002378
Iteration 88/1000 | Loss: 0.00002378
Iteration 89/1000 | Loss: 0.00002378
Iteration 90/1000 | Loss: 0.00002377
Iteration 91/1000 | Loss: 0.00002377
Iteration 92/1000 | Loss: 0.00002377
Iteration 93/1000 | Loss: 0.00002377
Iteration 94/1000 | Loss: 0.00002376
Iteration 95/1000 | Loss: 0.00002376
Iteration 96/1000 | Loss: 0.00002376
Iteration 97/1000 | Loss: 0.00002376
Iteration 98/1000 | Loss: 0.00002376
Iteration 99/1000 | Loss: 0.00002376
Iteration 100/1000 | Loss: 0.00002376
Iteration 101/1000 | Loss: 0.00002376
Iteration 102/1000 | Loss: 0.00002376
Iteration 103/1000 | Loss: 0.00002376
Iteration 104/1000 | Loss: 0.00002375
Iteration 105/1000 | Loss: 0.00002375
Iteration 106/1000 | Loss: 0.00002375
Iteration 107/1000 | Loss: 0.00002375
Iteration 108/1000 | Loss: 0.00002375
Iteration 109/1000 | Loss: 0.00002374
Iteration 110/1000 | Loss: 0.00002374
Iteration 111/1000 | Loss: 0.00002374
Iteration 112/1000 | Loss: 0.00002374
Iteration 113/1000 | Loss: 0.00002374
Iteration 114/1000 | Loss: 0.00002374
Iteration 115/1000 | Loss: 0.00002374
Iteration 116/1000 | Loss: 0.00002374
Iteration 117/1000 | Loss: 0.00002374
Iteration 118/1000 | Loss: 0.00002374
Iteration 119/1000 | Loss: 0.00002374
Iteration 120/1000 | Loss: 0.00002374
Iteration 121/1000 | Loss: 0.00002374
Iteration 122/1000 | Loss: 0.00002374
Iteration 123/1000 | Loss: 0.00002374
Iteration 124/1000 | Loss: 0.00002373
Iteration 125/1000 | Loss: 0.00002373
Iteration 126/1000 | Loss: 0.00002373
Iteration 127/1000 | Loss: 0.00002373
Iteration 128/1000 | Loss: 0.00002373
Iteration 129/1000 | Loss: 0.00002373
Iteration 130/1000 | Loss: 0.00002373
Iteration 131/1000 | Loss: 0.00002373
Iteration 132/1000 | Loss: 0.00002372
Iteration 133/1000 | Loss: 0.00002372
Iteration 134/1000 | Loss: 0.00002372
Iteration 135/1000 | Loss: 0.00002372
Iteration 136/1000 | Loss: 0.00002372
Iteration 137/1000 | Loss: 0.00002372
Iteration 138/1000 | Loss: 0.00002372
Iteration 139/1000 | Loss: 0.00002372
Iteration 140/1000 | Loss: 0.00002372
Iteration 141/1000 | Loss: 0.00002372
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 141. Stopping optimization.
Last 5 losses: [2.3720476747257635e-05, 2.3720476747257635e-05, 2.3720476747257635e-05, 2.3720476747257635e-05, 2.3720476747257635e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3720476747257635e-05

Optimization complete. Final v2v error: 3.8350069522857666 mm

Highest mean error: 9.275856018066406 mm for frame 135

Lowest mean error: 3.0968217849731445 mm for frame 170

Saving results

Total time: 100.60881042480469
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_36_nl_5493/0015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_nl_5493/0015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_nl_5493/0015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01046707
Iteration 2/25 | Loss: 0.00239612
Iteration 3/25 | Loss: 0.00150048
Iteration 4/25 | Loss: 0.00141292
Iteration 5/25 | Loss: 0.00139536
Iteration 6/25 | Loss: 0.00129301
Iteration 7/25 | Loss: 0.00124668
Iteration 8/25 | Loss: 0.00124478
Iteration 9/25 | Loss: 0.00119498
Iteration 10/25 | Loss: 0.00119472
Iteration 11/25 | Loss: 0.00118170
Iteration 12/25 | Loss: 0.00117401
Iteration 13/25 | Loss: 0.00117770
Iteration 14/25 | Loss: 0.00117150
Iteration 15/25 | Loss: 0.00117106
Iteration 16/25 | Loss: 0.00117098
Iteration 17/25 | Loss: 0.00117098
Iteration 18/25 | Loss: 0.00117098
Iteration 19/25 | Loss: 0.00117098
Iteration 20/25 | Loss: 0.00117098
Iteration 21/25 | Loss: 0.00117097
Iteration 22/25 | Loss: 0.00117097
Iteration 23/25 | Loss: 0.00117097
Iteration 24/25 | Loss: 0.00117097
Iteration 25/25 | Loss: 0.00117097

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.25039005
Iteration 2/25 | Loss: 0.00116403
Iteration 3/25 | Loss: 0.00116403
Iteration 4/25 | Loss: 0.00116403
Iteration 5/25 | Loss: 0.00116403
Iteration 6/25 | Loss: 0.00116403
Iteration 7/25 | Loss: 0.00116403
Iteration 8/25 | Loss: 0.00116403
Iteration 9/25 | Loss: 0.00116403
Iteration 10/25 | Loss: 0.00116403
Iteration 11/25 | Loss: 0.00116403
Iteration 12/25 | Loss: 0.00116403
Iteration 13/25 | Loss: 0.00116403
Iteration 14/25 | Loss: 0.00116403
Iteration 15/25 | Loss: 0.00116403
Iteration 16/25 | Loss: 0.00116403
Iteration 17/25 | Loss: 0.00116403
Iteration 18/25 | Loss: 0.00116403
Iteration 19/25 | Loss: 0.00116403
Iteration 20/25 | Loss: 0.00116403
Iteration 21/25 | Loss: 0.00116403
Iteration 22/25 | Loss: 0.00116403
Iteration 23/25 | Loss: 0.00116403
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0011640309821814299, 0.0011640309821814299, 0.0011640309821814299, 0.0011640309821814299, 0.0011640309821814299]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011640309821814299

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00116403
Iteration 2/1000 | Loss: 0.00007393
Iteration 3/1000 | Loss: 0.00006496
Iteration 4/1000 | Loss: 0.00005730
Iteration 5/1000 | Loss: 0.00005197
Iteration 6/1000 | Loss: 0.00005144
Iteration 7/1000 | Loss: 0.00008912
Iteration 8/1000 | Loss: 0.00015323
Iteration 9/1000 | Loss: 0.00005963
Iteration 10/1000 | Loss: 0.00006950
Iteration 11/1000 | Loss: 0.00002528
Iteration 12/1000 | Loss: 0.00002244
Iteration 13/1000 | Loss: 0.00002760
Iteration 14/1000 | Loss: 0.00001982
Iteration 15/1000 | Loss: 0.00001885
Iteration 16/1000 | Loss: 0.00001818
Iteration 17/1000 | Loss: 0.00001758
Iteration 18/1000 | Loss: 0.00001701
Iteration 19/1000 | Loss: 0.00001665
Iteration 20/1000 | Loss: 0.00001643
Iteration 21/1000 | Loss: 0.00001639
Iteration 22/1000 | Loss: 0.00001634
Iteration 23/1000 | Loss: 0.00001618
Iteration 24/1000 | Loss: 0.00001606
Iteration 25/1000 | Loss: 0.00001600
Iteration 26/1000 | Loss: 0.00001599
Iteration 27/1000 | Loss: 0.00001598
Iteration 28/1000 | Loss: 0.00001597
Iteration 29/1000 | Loss: 0.00001594
Iteration 30/1000 | Loss: 0.00001593
Iteration 31/1000 | Loss: 0.00001593
Iteration 32/1000 | Loss: 0.00001591
Iteration 33/1000 | Loss: 0.00001591
Iteration 34/1000 | Loss: 0.00001591
Iteration 35/1000 | Loss: 0.00001591
Iteration 36/1000 | Loss: 0.00001591
Iteration 37/1000 | Loss: 0.00001591
Iteration 38/1000 | Loss: 0.00001591
Iteration 39/1000 | Loss: 0.00001590
Iteration 40/1000 | Loss: 0.00001590
Iteration 41/1000 | Loss: 0.00001590
Iteration 42/1000 | Loss: 0.00001589
Iteration 43/1000 | Loss: 0.00001589
Iteration 44/1000 | Loss: 0.00028031
Iteration 45/1000 | Loss: 0.00002226
Iteration 46/1000 | Loss: 0.00001798
Iteration 47/1000 | Loss: 0.00001623
Iteration 48/1000 | Loss: 0.00001510
Iteration 49/1000 | Loss: 0.00001449
Iteration 50/1000 | Loss: 0.00001418
Iteration 51/1000 | Loss: 0.00001409
Iteration 52/1000 | Loss: 0.00001408
Iteration 53/1000 | Loss: 0.00001405
Iteration 54/1000 | Loss: 0.00001403
Iteration 55/1000 | Loss: 0.00001403
Iteration 56/1000 | Loss: 0.00001402
Iteration 57/1000 | Loss: 0.00001401
Iteration 58/1000 | Loss: 0.00001399
Iteration 59/1000 | Loss: 0.00001399
Iteration 60/1000 | Loss: 0.00001398
Iteration 61/1000 | Loss: 0.00001398
Iteration 62/1000 | Loss: 0.00001397
Iteration 63/1000 | Loss: 0.00001397
Iteration 64/1000 | Loss: 0.00001397
Iteration 65/1000 | Loss: 0.00001397
Iteration 66/1000 | Loss: 0.00001397
Iteration 67/1000 | Loss: 0.00001396
Iteration 68/1000 | Loss: 0.00001396
Iteration 69/1000 | Loss: 0.00001395
Iteration 70/1000 | Loss: 0.00001395
Iteration 71/1000 | Loss: 0.00001395
Iteration 72/1000 | Loss: 0.00001395
Iteration 73/1000 | Loss: 0.00001394
Iteration 74/1000 | Loss: 0.00001394
Iteration 75/1000 | Loss: 0.00001394
Iteration 76/1000 | Loss: 0.00001394
Iteration 77/1000 | Loss: 0.00001394
Iteration 78/1000 | Loss: 0.00001394
Iteration 79/1000 | Loss: 0.00001393
Iteration 80/1000 | Loss: 0.00001393
Iteration 81/1000 | Loss: 0.00001393
Iteration 82/1000 | Loss: 0.00001393
Iteration 83/1000 | Loss: 0.00001392
Iteration 84/1000 | Loss: 0.00001392
Iteration 85/1000 | Loss: 0.00001392
Iteration 86/1000 | Loss: 0.00001392
Iteration 87/1000 | Loss: 0.00001392
Iteration 88/1000 | Loss: 0.00001392
Iteration 89/1000 | Loss: 0.00001392
Iteration 90/1000 | Loss: 0.00001391
Iteration 91/1000 | Loss: 0.00001391
Iteration 92/1000 | Loss: 0.00001391
Iteration 93/1000 | Loss: 0.00001391
Iteration 94/1000 | Loss: 0.00001391
Iteration 95/1000 | Loss: 0.00001391
Iteration 96/1000 | Loss: 0.00001391
Iteration 97/1000 | Loss: 0.00001391
Iteration 98/1000 | Loss: 0.00001391
Iteration 99/1000 | Loss: 0.00001391
Iteration 100/1000 | Loss: 0.00001391
Iteration 101/1000 | Loss: 0.00001390
Iteration 102/1000 | Loss: 0.00001390
Iteration 103/1000 | Loss: 0.00001390
Iteration 104/1000 | Loss: 0.00001390
Iteration 105/1000 | Loss: 0.00001390
Iteration 106/1000 | Loss: 0.00001390
Iteration 107/1000 | Loss: 0.00001390
Iteration 108/1000 | Loss: 0.00001390
Iteration 109/1000 | Loss: 0.00001390
Iteration 110/1000 | Loss: 0.00001389
Iteration 111/1000 | Loss: 0.00001389
Iteration 112/1000 | Loss: 0.00001389
Iteration 113/1000 | Loss: 0.00001389
Iteration 114/1000 | Loss: 0.00001389
Iteration 115/1000 | Loss: 0.00001389
Iteration 116/1000 | Loss: 0.00001389
Iteration 117/1000 | Loss: 0.00001388
Iteration 118/1000 | Loss: 0.00001388
Iteration 119/1000 | Loss: 0.00001388
Iteration 120/1000 | Loss: 0.00001388
Iteration 121/1000 | Loss: 0.00001388
Iteration 122/1000 | Loss: 0.00001388
Iteration 123/1000 | Loss: 0.00001388
Iteration 124/1000 | Loss: 0.00001388
Iteration 125/1000 | Loss: 0.00001388
Iteration 126/1000 | Loss: 0.00001388
Iteration 127/1000 | Loss: 0.00001388
Iteration 128/1000 | Loss: 0.00001388
Iteration 129/1000 | Loss: 0.00001388
Iteration 130/1000 | Loss: 0.00001388
Iteration 131/1000 | Loss: 0.00001388
Iteration 132/1000 | Loss: 0.00001388
Iteration 133/1000 | Loss: 0.00001388
Iteration 134/1000 | Loss: 0.00001388
Iteration 135/1000 | Loss: 0.00001388
Iteration 136/1000 | Loss: 0.00001388
Iteration 137/1000 | Loss: 0.00001388
Iteration 138/1000 | Loss: 0.00001388
Iteration 139/1000 | Loss: 0.00001388
Iteration 140/1000 | Loss: 0.00001388
Iteration 141/1000 | Loss: 0.00001388
Iteration 142/1000 | Loss: 0.00001388
Iteration 143/1000 | Loss: 0.00001388
Iteration 144/1000 | Loss: 0.00001388
Iteration 145/1000 | Loss: 0.00001388
Iteration 146/1000 | Loss: 0.00001388
Iteration 147/1000 | Loss: 0.00001388
Iteration 148/1000 | Loss: 0.00001388
Iteration 149/1000 | Loss: 0.00001388
Iteration 150/1000 | Loss: 0.00001388
Iteration 151/1000 | Loss: 0.00001388
Iteration 152/1000 | Loss: 0.00001388
Iteration 153/1000 | Loss: 0.00001388
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 153. Stopping optimization.
Last 5 losses: [1.3878365280106664e-05, 1.3878365280106664e-05, 1.3878365280106664e-05, 1.3878365280106664e-05, 1.3878365280106664e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3878365280106664e-05

Optimization complete. Final v2v error: 3.1406829357147217 mm

Highest mean error: 9.65481185913086 mm for frame 196

Lowest mean error: 2.607166290283203 mm for frame 72

Saving results

Total time: 93.3579216003418
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_36_nl_5493/0010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_nl_5493/0010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_nl_5493/0010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01080825
Iteration 2/25 | Loss: 0.01080825
Iteration 3/25 | Loss: 0.01080825
Iteration 4/25 | Loss: 0.00407100
Iteration 5/25 | Loss: 0.00333345
Iteration 6/25 | Loss: 0.00263823
Iteration 7/25 | Loss: 0.00230171
Iteration 8/25 | Loss: 0.00236058
Iteration 9/25 | Loss: 0.00213100
Iteration 10/25 | Loss: 0.00197411
Iteration 11/25 | Loss: 0.00188080
Iteration 12/25 | Loss: 0.00180898
Iteration 13/25 | Loss: 0.00174574
Iteration 14/25 | Loss: 0.00173264
Iteration 15/25 | Loss: 0.00169562
Iteration 16/25 | Loss: 0.00168392
Iteration 17/25 | Loss: 0.00169248
Iteration 18/25 | Loss: 0.00170243
Iteration 19/25 | Loss: 0.00164811
Iteration 20/25 | Loss: 0.00162627
Iteration 21/25 | Loss: 0.00161797
Iteration 22/25 | Loss: 0.00161123
Iteration 23/25 | Loss: 0.00161387
Iteration 24/25 | Loss: 0.00159984
Iteration 25/25 | Loss: 0.00159672

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.21012485
Iteration 2/25 | Loss: 0.00650354
Iteration 3/25 | Loss: 0.00433565
Iteration 4/25 | Loss: 0.00429642
Iteration 5/25 | Loss: 0.00429642
Iteration 6/25 | Loss: 0.00429642
Iteration 7/25 | Loss: 0.00429642
Iteration 8/25 | Loss: 0.00429642
Iteration 9/25 | Loss: 0.00429642
Iteration 10/25 | Loss: 0.00429642
Iteration 11/25 | Loss: 0.00429642
Iteration 12/25 | Loss: 0.00429642
Iteration 13/25 | Loss: 0.00429642
Iteration 14/25 | Loss: 0.00429642
Iteration 15/25 | Loss: 0.00429642
Iteration 16/25 | Loss: 0.00429642
Iteration 17/25 | Loss: 0.00429642
Iteration 18/25 | Loss: 0.00429642
Iteration 19/25 | Loss: 0.00429642
Iteration 20/25 | Loss: 0.00429642
Iteration 21/25 | Loss: 0.00429642
Iteration 22/25 | Loss: 0.00429642
Iteration 23/25 | Loss: 0.00429642
Iteration 24/25 | Loss: 0.00429642
Iteration 25/25 | Loss: 0.00429642

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00429642
Iteration 2/1000 | Loss: 0.00187253
Iteration 3/1000 | Loss: 0.00209797
Iteration 4/1000 | Loss: 0.00113563
Iteration 5/1000 | Loss: 0.00084694
Iteration 6/1000 | Loss: 0.00048195
Iteration 7/1000 | Loss: 0.00068929
Iteration 8/1000 | Loss: 0.00042401
Iteration 9/1000 | Loss: 0.00049320
Iteration 10/1000 | Loss: 0.00050082
Iteration 11/1000 | Loss: 0.00070325
Iteration 12/1000 | Loss: 0.00050890
Iteration 13/1000 | Loss: 0.00235737
Iteration 14/1000 | Loss: 0.00101665
Iteration 15/1000 | Loss: 0.00044452
Iteration 16/1000 | Loss: 0.00029983
Iteration 17/1000 | Loss: 0.00057700
Iteration 18/1000 | Loss: 0.00056605
Iteration 19/1000 | Loss: 0.00112934
Iteration 20/1000 | Loss: 0.00104300
Iteration 21/1000 | Loss: 0.00089414
Iteration 22/1000 | Loss: 0.00080450
Iteration 23/1000 | Loss: 0.00073223
Iteration 24/1000 | Loss: 0.00062642
Iteration 25/1000 | Loss: 0.00078411
Iteration 26/1000 | Loss: 0.00208713
Iteration 27/1000 | Loss: 0.00115853
Iteration 28/1000 | Loss: 0.00131232
Iteration 29/1000 | Loss: 0.00088038
Iteration 30/1000 | Loss: 0.00199063
Iteration 31/1000 | Loss: 0.00362971
Iteration 32/1000 | Loss: 0.00286316
Iteration 33/1000 | Loss: 0.00218139
Iteration 34/1000 | Loss: 0.00026527
Iteration 35/1000 | Loss: 0.00078367
Iteration 36/1000 | Loss: 0.00110179
Iteration 37/1000 | Loss: 0.00080328
Iteration 38/1000 | Loss: 0.00116751
Iteration 39/1000 | Loss: 0.00031366
Iteration 40/1000 | Loss: 0.00064536
Iteration 41/1000 | Loss: 0.00076350
Iteration 42/1000 | Loss: 0.00078825
Iteration 43/1000 | Loss: 0.00312417
Iteration 44/1000 | Loss: 0.00054912
Iteration 45/1000 | Loss: 0.00027588
Iteration 46/1000 | Loss: 0.00035833
Iteration 47/1000 | Loss: 0.00129013
Iteration 48/1000 | Loss: 0.00120616
Iteration 49/1000 | Loss: 0.00177978
Iteration 50/1000 | Loss: 0.00014795
Iteration 51/1000 | Loss: 0.00051627
Iteration 52/1000 | Loss: 0.00012184
Iteration 53/1000 | Loss: 0.00012749
Iteration 54/1000 | Loss: 0.00030553
Iteration 55/1000 | Loss: 0.00040628
Iteration 56/1000 | Loss: 0.00036958
Iteration 57/1000 | Loss: 0.00013953
Iteration 58/1000 | Loss: 0.00021510
Iteration 59/1000 | Loss: 0.00013813
Iteration 60/1000 | Loss: 0.00015413
Iteration 61/1000 | Loss: 0.00008073
Iteration 62/1000 | Loss: 0.00012219
Iteration 63/1000 | Loss: 0.00017266
Iteration 64/1000 | Loss: 0.00007112
Iteration 65/1000 | Loss: 0.00015229
Iteration 66/1000 | Loss: 0.00019270
Iteration 67/1000 | Loss: 0.00145341
Iteration 68/1000 | Loss: 0.00118700
Iteration 69/1000 | Loss: 0.00106940
Iteration 70/1000 | Loss: 0.00011824
Iteration 71/1000 | Loss: 0.00049728
Iteration 72/1000 | Loss: 0.00013048
Iteration 73/1000 | Loss: 0.00009231
Iteration 74/1000 | Loss: 0.00011323
Iteration 75/1000 | Loss: 0.00009770
Iteration 76/1000 | Loss: 0.00005246
Iteration 77/1000 | Loss: 0.00011162
Iteration 78/1000 | Loss: 0.00022060
Iteration 79/1000 | Loss: 0.00008423
Iteration 80/1000 | Loss: 0.00005163
Iteration 81/1000 | Loss: 0.00005923
Iteration 82/1000 | Loss: 0.00003208
Iteration 83/1000 | Loss: 0.00007709
Iteration 84/1000 | Loss: 0.00005632
Iteration 85/1000 | Loss: 0.00003185
Iteration 86/1000 | Loss: 0.00006367
Iteration 87/1000 | Loss: 0.00002864
Iteration 88/1000 | Loss: 0.00004499
Iteration 89/1000 | Loss: 0.00002806
Iteration 90/1000 | Loss: 0.00004803
Iteration 91/1000 | Loss: 0.00002784
Iteration 92/1000 | Loss: 0.00003516
Iteration 93/1000 | Loss: 0.00002772
Iteration 94/1000 | Loss: 0.00002768
Iteration 95/1000 | Loss: 0.00002767
Iteration 96/1000 | Loss: 0.00002767
Iteration 97/1000 | Loss: 0.00002767
Iteration 98/1000 | Loss: 0.00002767
Iteration 99/1000 | Loss: 0.00002767
Iteration 100/1000 | Loss: 0.00002767
Iteration 101/1000 | Loss: 0.00002766
Iteration 102/1000 | Loss: 0.00002766
Iteration 103/1000 | Loss: 0.00002766
Iteration 104/1000 | Loss: 0.00002765
Iteration 105/1000 | Loss: 0.00002765
Iteration 106/1000 | Loss: 0.00002765
Iteration 107/1000 | Loss: 0.00002765
Iteration 108/1000 | Loss: 0.00004580
Iteration 109/1000 | Loss: 0.00004000
Iteration 110/1000 | Loss: 0.00002771
Iteration 111/1000 | Loss: 0.00002756
Iteration 112/1000 | Loss: 0.00002756
Iteration 113/1000 | Loss: 0.00002756
Iteration 114/1000 | Loss: 0.00002756
Iteration 115/1000 | Loss: 0.00002756
Iteration 116/1000 | Loss: 0.00002755
Iteration 117/1000 | Loss: 0.00002754
Iteration 118/1000 | Loss: 0.00002754
Iteration 119/1000 | Loss: 0.00002754
Iteration 120/1000 | Loss: 0.00002754
Iteration 121/1000 | Loss: 0.00002753
Iteration 122/1000 | Loss: 0.00002753
Iteration 123/1000 | Loss: 0.00002753
Iteration 124/1000 | Loss: 0.00002753
Iteration 125/1000 | Loss: 0.00002753
Iteration 126/1000 | Loss: 0.00002753
Iteration 127/1000 | Loss: 0.00002767
Iteration 128/1000 | Loss: 0.00002766
Iteration 129/1000 | Loss: 0.00002766
Iteration 130/1000 | Loss: 0.00002766
Iteration 131/1000 | Loss: 0.00002766
Iteration 132/1000 | Loss: 0.00002766
Iteration 133/1000 | Loss: 0.00002765
Iteration 134/1000 | Loss: 0.00002765
Iteration 135/1000 | Loss: 0.00003012
Iteration 136/1000 | Loss: 0.00002751
Iteration 137/1000 | Loss: 0.00002748
Iteration 138/1000 | Loss: 0.00002748
Iteration 139/1000 | Loss: 0.00002748
Iteration 140/1000 | Loss: 0.00002748
Iteration 141/1000 | Loss: 0.00002747
Iteration 142/1000 | Loss: 0.00002747
Iteration 143/1000 | Loss: 0.00002747
Iteration 144/1000 | Loss: 0.00002747
Iteration 145/1000 | Loss: 0.00002747
Iteration 146/1000 | Loss: 0.00002747
Iteration 147/1000 | Loss: 0.00002747
Iteration 148/1000 | Loss: 0.00002747
Iteration 149/1000 | Loss: 0.00002746
Iteration 150/1000 | Loss: 0.00002746
Iteration 151/1000 | Loss: 0.00002746
Iteration 152/1000 | Loss: 0.00002746
Iteration 153/1000 | Loss: 0.00002746
Iteration 154/1000 | Loss: 0.00002746
Iteration 155/1000 | Loss: 0.00002746
Iteration 156/1000 | Loss: 0.00003664
Iteration 157/1000 | Loss: 0.00003664
Iteration 158/1000 | Loss: 0.00002907
Iteration 159/1000 | Loss: 0.00003989
Iteration 160/1000 | Loss: 0.00002743
Iteration 161/1000 | Loss: 0.00002743
Iteration 162/1000 | Loss: 0.00002742
Iteration 163/1000 | Loss: 0.00002742
Iteration 164/1000 | Loss: 0.00002742
Iteration 165/1000 | Loss: 0.00002742
Iteration 166/1000 | Loss: 0.00002742
Iteration 167/1000 | Loss: 0.00002742
Iteration 168/1000 | Loss: 0.00002742
Iteration 169/1000 | Loss: 0.00002742
Iteration 170/1000 | Loss: 0.00002742
Iteration 171/1000 | Loss: 0.00002742
Iteration 172/1000 | Loss: 0.00002742
Iteration 173/1000 | Loss: 0.00002741
Iteration 174/1000 | Loss: 0.00002741
Iteration 175/1000 | Loss: 0.00002741
Iteration 176/1000 | Loss: 0.00002741
Iteration 177/1000 | Loss: 0.00002741
Iteration 178/1000 | Loss: 0.00002741
Iteration 179/1000 | Loss: 0.00002741
Iteration 180/1000 | Loss: 0.00002741
Iteration 181/1000 | Loss: 0.00002741
Iteration 182/1000 | Loss: 0.00002741
Iteration 183/1000 | Loss: 0.00002741
Iteration 184/1000 | Loss: 0.00002741
Iteration 185/1000 | Loss: 0.00002741
Iteration 186/1000 | Loss: 0.00002741
Iteration 187/1000 | Loss: 0.00002741
Iteration 188/1000 | Loss: 0.00002741
Iteration 189/1000 | Loss: 0.00002741
Iteration 190/1000 | Loss: 0.00002741
Iteration 191/1000 | Loss: 0.00002741
Iteration 192/1000 | Loss: 0.00002741
Iteration 193/1000 | Loss: 0.00002740
Iteration 194/1000 | Loss: 0.00002740
Iteration 195/1000 | Loss: 0.00002740
Iteration 196/1000 | Loss: 0.00002740
Iteration 197/1000 | Loss: 0.00002740
Iteration 198/1000 | Loss: 0.00002740
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 198. Stopping optimization.
Last 5 losses: [2.740486706898082e-05, 2.740486706898082e-05, 2.740486706898082e-05, 2.740486706898082e-05, 2.740486706898082e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.740486706898082e-05

Optimization complete. Final v2v error: 3.3304269313812256 mm

Highest mean error: 21.717693328857422 mm for frame 176

Lowest mean error: 2.820812463760376 mm for frame 78

Saving results

Total time: 218.1895215511322
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_it_4031/0023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_it_4031/0023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_it_4031/0023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01040785
Iteration 2/25 | Loss: 0.01040784
Iteration 3/25 | Loss: 0.00325929
Iteration 4/25 | Loss: 0.00201906
Iteration 5/25 | Loss: 0.00188742
Iteration 6/25 | Loss: 0.00186140
Iteration 7/25 | Loss: 0.00181344
Iteration 8/25 | Loss: 0.00179599
Iteration 9/25 | Loss: 0.00174493
Iteration 10/25 | Loss: 0.00172103
Iteration 11/25 | Loss: 0.00169592
Iteration 12/25 | Loss: 0.00166796
Iteration 13/25 | Loss: 0.00163135
Iteration 14/25 | Loss: 0.00161399
Iteration 15/25 | Loss: 0.00160514
Iteration 16/25 | Loss: 0.00160415
Iteration 17/25 | Loss: 0.00160128
Iteration 18/25 | Loss: 0.00159279
Iteration 19/25 | Loss: 0.00159085
Iteration 20/25 | Loss: 0.00159567
Iteration 21/25 | Loss: 0.00158920
Iteration 22/25 | Loss: 0.00158400
Iteration 23/25 | Loss: 0.00158123
Iteration 24/25 | Loss: 0.00158053
Iteration 25/25 | Loss: 0.00158039

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.25601578
Iteration 2/25 | Loss: 0.01266766
Iteration 3/25 | Loss: 0.00321505
Iteration 4/25 | Loss: 0.00321502
Iteration 5/25 | Loss: 0.00321502
Iteration 6/25 | Loss: 0.00321502
Iteration 7/25 | Loss: 0.00321501
Iteration 8/25 | Loss: 0.00321501
Iteration 9/25 | Loss: 0.00321501
Iteration 10/25 | Loss: 0.00321501
Iteration 11/25 | Loss: 0.00321501
Iteration 12/25 | Loss: 0.00321501
Iteration 13/25 | Loss: 0.00321501
Iteration 14/25 | Loss: 0.00321501
Iteration 15/25 | Loss: 0.00321501
Iteration 16/25 | Loss: 0.00321501
Iteration 17/25 | Loss: 0.00321501
Iteration 18/25 | Loss: 0.00321501
Iteration 19/25 | Loss: 0.00321501
Iteration 20/25 | Loss: 0.00321501
Iteration 21/25 | Loss: 0.00321501
Iteration 22/25 | Loss: 0.00321501
Iteration 23/25 | Loss: 0.00321501
Iteration 24/25 | Loss: 0.00321501
Iteration 25/25 | Loss: 0.00321501

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00321501
Iteration 2/1000 | Loss: 0.00518026
Iteration 3/1000 | Loss: 0.00871752
Iteration 4/1000 | Loss: 0.00298855
Iteration 5/1000 | Loss: 0.00073582
Iteration 6/1000 | Loss: 0.00114592
Iteration 7/1000 | Loss: 0.00284173
Iteration 8/1000 | Loss: 0.00176014
Iteration 9/1000 | Loss: 0.00399302
Iteration 10/1000 | Loss: 0.00215166
Iteration 11/1000 | Loss: 0.00331534
Iteration 12/1000 | Loss: 0.00505516
Iteration 13/1000 | Loss: 0.00319261
Iteration 14/1000 | Loss: 0.00272034
Iteration 15/1000 | Loss: 0.00236542
Iteration 16/1000 | Loss: 0.00307149
Iteration 17/1000 | Loss: 0.00242750
Iteration 18/1000 | Loss: 0.00097525
Iteration 19/1000 | Loss: 0.00234430
Iteration 20/1000 | Loss: 0.00083032
Iteration 21/1000 | Loss: 0.00109471
Iteration 22/1000 | Loss: 0.00497749
Iteration 23/1000 | Loss: 0.00759567
Iteration 24/1000 | Loss: 0.00598089
Iteration 25/1000 | Loss: 0.00572228
Iteration 26/1000 | Loss: 0.00628551
Iteration 27/1000 | Loss: 0.00459530
Iteration 28/1000 | Loss: 0.00371916
Iteration 29/1000 | Loss: 0.00098609
Iteration 30/1000 | Loss: 0.00041001
Iteration 31/1000 | Loss: 0.00250450
Iteration 32/1000 | Loss: 0.00076399
Iteration 33/1000 | Loss: 0.00099190
Iteration 34/1000 | Loss: 0.00067829
Iteration 35/1000 | Loss: 0.00041498
Iteration 36/1000 | Loss: 0.00038775
Iteration 37/1000 | Loss: 0.00032341
Iteration 38/1000 | Loss: 0.00063269
Iteration 39/1000 | Loss: 0.00163675
Iteration 40/1000 | Loss: 0.00017444
Iteration 41/1000 | Loss: 0.00039424
Iteration 42/1000 | Loss: 0.00033204
Iteration 43/1000 | Loss: 0.00033837
Iteration 44/1000 | Loss: 0.00035661
Iteration 45/1000 | Loss: 0.00017357
Iteration 46/1000 | Loss: 0.00005953
Iteration 47/1000 | Loss: 0.00024490
Iteration 48/1000 | Loss: 0.00006834
Iteration 49/1000 | Loss: 0.00004822
Iteration 50/1000 | Loss: 0.00032017
Iteration 51/1000 | Loss: 0.00004771
Iteration 52/1000 | Loss: 0.00004096
Iteration 53/1000 | Loss: 0.00042741
Iteration 54/1000 | Loss: 0.00045117
Iteration 55/1000 | Loss: 0.00036890
Iteration 56/1000 | Loss: 0.00017546
Iteration 57/1000 | Loss: 0.00004654
Iteration 58/1000 | Loss: 0.00015081
Iteration 59/1000 | Loss: 0.00032743
Iteration 60/1000 | Loss: 0.00014815
Iteration 61/1000 | Loss: 0.00010053
Iteration 62/1000 | Loss: 0.00005566
Iteration 63/1000 | Loss: 0.00003331
Iteration 64/1000 | Loss: 0.00002816
Iteration 65/1000 | Loss: 0.00025786
Iteration 66/1000 | Loss: 0.00008416
Iteration 67/1000 | Loss: 0.00002688
Iteration 68/1000 | Loss: 0.00013952
Iteration 69/1000 | Loss: 0.00002117
Iteration 70/1000 | Loss: 0.00002025
Iteration 71/1000 | Loss: 0.00001876
Iteration 72/1000 | Loss: 0.00022151
Iteration 73/1000 | Loss: 0.00003067
Iteration 74/1000 | Loss: 0.00002755
Iteration 75/1000 | Loss: 0.00020998
Iteration 76/1000 | Loss: 0.00008456
Iteration 77/1000 | Loss: 0.00003559
Iteration 78/1000 | Loss: 0.00004727
Iteration 79/1000 | Loss: 0.00021169
Iteration 80/1000 | Loss: 0.00001790
Iteration 81/1000 | Loss: 0.00001720
Iteration 82/1000 | Loss: 0.00001689
Iteration 83/1000 | Loss: 0.00001651
Iteration 84/1000 | Loss: 0.00001633
Iteration 85/1000 | Loss: 0.00001632
Iteration 86/1000 | Loss: 0.00001631
Iteration 87/1000 | Loss: 0.00001630
Iteration 88/1000 | Loss: 0.00001629
Iteration 89/1000 | Loss: 0.00001622
Iteration 90/1000 | Loss: 0.00001608
Iteration 91/1000 | Loss: 0.00001598
Iteration 92/1000 | Loss: 0.00020384
Iteration 93/1000 | Loss: 0.00001667
Iteration 94/1000 | Loss: 0.00001598
Iteration 95/1000 | Loss: 0.00001583
Iteration 96/1000 | Loss: 0.00001582
Iteration 97/1000 | Loss: 0.00001582
Iteration 98/1000 | Loss: 0.00001582
Iteration 99/1000 | Loss: 0.00001581
Iteration 100/1000 | Loss: 0.00001581
Iteration 101/1000 | Loss: 0.00001581
Iteration 102/1000 | Loss: 0.00001580
Iteration 103/1000 | Loss: 0.00001580
Iteration 104/1000 | Loss: 0.00001580
Iteration 105/1000 | Loss: 0.00001579
Iteration 106/1000 | Loss: 0.00001579
Iteration 107/1000 | Loss: 0.00001579
Iteration 108/1000 | Loss: 0.00001579
Iteration 109/1000 | Loss: 0.00001578
Iteration 110/1000 | Loss: 0.00001578
Iteration 111/1000 | Loss: 0.00001578
Iteration 112/1000 | Loss: 0.00001578
Iteration 113/1000 | Loss: 0.00001578
Iteration 114/1000 | Loss: 0.00001578
Iteration 115/1000 | Loss: 0.00001578
Iteration 116/1000 | Loss: 0.00001578
Iteration 117/1000 | Loss: 0.00001578
Iteration 118/1000 | Loss: 0.00001577
Iteration 119/1000 | Loss: 0.00001577
Iteration 120/1000 | Loss: 0.00001577
Iteration 121/1000 | Loss: 0.00001577
Iteration 122/1000 | Loss: 0.00001577
Iteration 123/1000 | Loss: 0.00001577
Iteration 124/1000 | Loss: 0.00001577
Iteration 125/1000 | Loss: 0.00001577
Iteration 126/1000 | Loss: 0.00001577
Iteration 127/1000 | Loss: 0.00001577
Iteration 128/1000 | Loss: 0.00001577
Iteration 129/1000 | Loss: 0.00001577
Iteration 130/1000 | Loss: 0.00001577
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 130. Stopping optimization.
Last 5 losses: [1.5773392078699544e-05, 1.5773392078699544e-05, 1.5773392078699544e-05, 1.5773392078699544e-05, 1.5773392078699544e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5773392078699544e-05

Optimization complete. Final v2v error: 3.1768505573272705 mm

Highest mean error: 13.011846542358398 mm for frame 33

Lowest mean error: 2.7728888988494873 mm for frame 71

Saving results

Total time: 195.94653415679932
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_it_4031/0022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_it_4031/0022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_it_4031/0022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00955600
Iteration 2/25 | Loss: 0.00375964
Iteration 3/25 | Loss: 0.00282502
Iteration 4/25 | Loss: 0.00235216
Iteration 5/25 | Loss: 0.00257160
Iteration 6/25 | Loss: 0.00228420
Iteration 7/25 | Loss: 0.00207961
Iteration 8/25 | Loss: 0.00198722
Iteration 9/25 | Loss: 0.00172543
Iteration 10/25 | Loss: 0.00165472
Iteration 11/25 | Loss: 0.00162613
Iteration 12/25 | Loss: 0.00159578
Iteration 13/25 | Loss: 0.00153244
Iteration 14/25 | Loss: 0.00145130
Iteration 15/25 | Loss: 0.00142911
Iteration 16/25 | Loss: 0.00140497
Iteration 17/25 | Loss: 0.00140158
Iteration 18/25 | Loss: 0.00138043
Iteration 19/25 | Loss: 0.00138153
Iteration 20/25 | Loss: 0.00139057
Iteration 21/25 | Loss: 0.00140628
Iteration 22/25 | Loss: 0.00138082
Iteration 23/25 | Loss: 0.00139376
Iteration 24/25 | Loss: 0.00139005
Iteration 25/25 | Loss: 0.00138894

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.87684608
Iteration 2/25 | Loss: 0.00604410
Iteration 3/25 | Loss: 0.00561382
Iteration 4/25 | Loss: 0.00561382
Iteration 5/25 | Loss: 0.00561382
Iteration 6/25 | Loss: 0.00561382
Iteration 7/25 | Loss: 0.00561382
Iteration 8/25 | Loss: 0.00561382
Iteration 9/25 | Loss: 0.00561382
Iteration 10/25 | Loss: 0.00561382
Iteration 11/25 | Loss: 0.00561382
Iteration 12/25 | Loss: 0.00561382
Iteration 13/25 | Loss: 0.00561382
Iteration 14/25 | Loss: 0.00561382
Iteration 15/25 | Loss: 0.00561382
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.005613815039396286, 0.005613815039396286, 0.005613815039396286, 0.005613815039396286, 0.005613815039396286]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.005613815039396286

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00561382
Iteration 2/1000 | Loss: 0.00257556
Iteration 3/1000 | Loss: 0.00169431
Iteration 4/1000 | Loss: 0.00157243
Iteration 5/1000 | Loss: 0.00631856
Iteration 6/1000 | Loss: 0.00134054
Iteration 7/1000 | Loss: 0.00108808
Iteration 8/1000 | Loss: 0.00348845
Iteration 9/1000 | Loss: 0.00224718
Iteration 10/1000 | Loss: 0.00139101
Iteration 11/1000 | Loss: 0.00249828
Iteration 12/1000 | Loss: 0.00084058
Iteration 13/1000 | Loss: 0.00114087
Iteration 14/1000 | Loss: 0.00094448
Iteration 15/1000 | Loss: 0.00301952
Iteration 16/1000 | Loss: 0.00079531
Iteration 17/1000 | Loss: 0.00177951
Iteration 18/1000 | Loss: 0.00062594
Iteration 19/1000 | Loss: 0.00077099
Iteration 20/1000 | Loss: 0.00056863
Iteration 21/1000 | Loss: 0.00032867
Iteration 22/1000 | Loss: 0.00083182
Iteration 23/1000 | Loss: 0.00088369
Iteration 24/1000 | Loss: 0.00072995
Iteration 25/1000 | Loss: 0.00120036
Iteration 26/1000 | Loss: 0.00133448
Iteration 27/1000 | Loss: 0.00135744
Iteration 28/1000 | Loss: 0.00068421
Iteration 29/1000 | Loss: 0.00052756
Iteration 30/1000 | Loss: 0.00143260
Iteration 31/1000 | Loss: 0.00026964
Iteration 32/1000 | Loss: 0.00269967
Iteration 33/1000 | Loss: 0.00220752
Iteration 34/1000 | Loss: 0.00315859
Iteration 35/1000 | Loss: 0.00123500
Iteration 36/1000 | Loss: 0.00019468
Iteration 37/1000 | Loss: 0.00017039
Iteration 38/1000 | Loss: 0.00074430
Iteration 39/1000 | Loss: 0.00022958
Iteration 40/1000 | Loss: 0.00046779
Iteration 41/1000 | Loss: 0.00089445
Iteration 42/1000 | Loss: 0.00034268
Iteration 43/1000 | Loss: 0.00056896
Iteration 44/1000 | Loss: 0.00017357
Iteration 45/1000 | Loss: 0.00034645
Iteration 46/1000 | Loss: 0.00024420
Iteration 47/1000 | Loss: 0.00069724
Iteration 48/1000 | Loss: 0.00031112
Iteration 49/1000 | Loss: 0.00075794
Iteration 50/1000 | Loss: 0.00019950
Iteration 51/1000 | Loss: 0.00015746
Iteration 52/1000 | Loss: 0.00076793
Iteration 53/1000 | Loss: 0.00131777
Iteration 54/1000 | Loss: 0.00079557
Iteration 55/1000 | Loss: 0.00174422
Iteration 56/1000 | Loss: 0.00043995
Iteration 57/1000 | Loss: 0.00042073
Iteration 58/1000 | Loss: 0.00129159
Iteration 59/1000 | Loss: 0.00142992
Iteration 60/1000 | Loss: 0.00051890
Iteration 61/1000 | Loss: 0.00032986
Iteration 62/1000 | Loss: 0.00171670
Iteration 63/1000 | Loss: 0.00103240
Iteration 64/1000 | Loss: 0.00017701
Iteration 65/1000 | Loss: 0.00022771
Iteration 66/1000 | Loss: 0.00014203
Iteration 67/1000 | Loss: 0.00098694
Iteration 68/1000 | Loss: 0.00068099
Iteration 69/1000 | Loss: 0.00037764
Iteration 70/1000 | Loss: 0.00043130
Iteration 71/1000 | Loss: 0.00071815
Iteration 72/1000 | Loss: 0.00024323
Iteration 73/1000 | Loss: 0.00019666
Iteration 74/1000 | Loss: 0.00061836
Iteration 75/1000 | Loss: 0.00027844
Iteration 76/1000 | Loss: 0.00031258
Iteration 77/1000 | Loss: 0.00105966
Iteration 78/1000 | Loss: 0.00020492
Iteration 79/1000 | Loss: 0.00043225
Iteration 80/1000 | Loss: 0.00067052
Iteration 81/1000 | Loss: 0.00018978
Iteration 82/1000 | Loss: 0.00015052
Iteration 83/1000 | Loss: 0.00015265
Iteration 84/1000 | Loss: 0.00014422
Iteration 85/1000 | Loss: 0.00033541
Iteration 86/1000 | Loss: 0.00017877
Iteration 87/1000 | Loss: 0.00019705
Iteration 88/1000 | Loss: 0.00048559
Iteration 89/1000 | Loss: 0.00088311
Iteration 90/1000 | Loss: 0.00087970
Iteration 91/1000 | Loss: 0.00021582
Iteration 92/1000 | Loss: 0.00023658
Iteration 93/1000 | Loss: 0.00015271
Iteration 94/1000 | Loss: 0.00057194
Iteration 95/1000 | Loss: 0.00021297
Iteration 96/1000 | Loss: 0.00013230
Iteration 97/1000 | Loss: 0.00051712
Iteration 98/1000 | Loss: 0.00022362
Iteration 99/1000 | Loss: 0.00045579
Iteration 100/1000 | Loss: 0.00016093
Iteration 101/1000 | Loss: 0.00016266
Iteration 102/1000 | Loss: 0.00016521
Iteration 103/1000 | Loss: 0.00013386
Iteration 104/1000 | Loss: 0.00064589
Iteration 105/1000 | Loss: 0.00015823
Iteration 106/1000 | Loss: 0.00031386
Iteration 107/1000 | Loss: 0.00017768
Iteration 108/1000 | Loss: 0.00041396
Iteration 109/1000 | Loss: 0.00016727
Iteration 110/1000 | Loss: 0.00030345
Iteration 111/1000 | Loss: 0.00019795
Iteration 112/1000 | Loss: 0.00016693
Iteration 113/1000 | Loss: 0.00030139
Iteration 114/1000 | Loss: 0.00013973
Iteration 115/1000 | Loss: 0.00066106
Iteration 116/1000 | Loss: 0.00034353
Iteration 117/1000 | Loss: 0.00030633
Iteration 118/1000 | Loss: 0.00033236
Iteration 119/1000 | Loss: 0.00051821
Iteration 120/1000 | Loss: 0.00030825
Iteration 121/1000 | Loss: 0.00027517
Iteration 122/1000 | Loss: 0.00024522
Iteration 123/1000 | Loss: 0.00026335
Iteration 124/1000 | Loss: 0.00018365
Iteration 125/1000 | Loss: 0.00055510
Iteration 126/1000 | Loss: 0.00025267
Iteration 127/1000 | Loss: 0.00013286
Iteration 128/1000 | Loss: 0.00012464
Iteration 129/1000 | Loss: 0.00012308
Iteration 130/1000 | Loss: 0.00031363
Iteration 131/1000 | Loss: 0.00013244
Iteration 132/1000 | Loss: 0.00018621
Iteration 133/1000 | Loss: 0.00011909
Iteration 134/1000 | Loss: 0.00011684
Iteration 135/1000 | Loss: 0.00011469
Iteration 136/1000 | Loss: 0.00011433
Iteration 137/1000 | Loss: 0.00022547
Iteration 138/1000 | Loss: 0.00057597
Iteration 139/1000 | Loss: 0.00026146
Iteration 140/1000 | Loss: 0.00011781
Iteration 141/1000 | Loss: 0.00012686
Iteration 142/1000 | Loss: 0.00019910
Iteration 143/1000 | Loss: 0.00145608
Iteration 144/1000 | Loss: 0.00012833
Iteration 145/1000 | Loss: 0.00036846
Iteration 146/1000 | Loss: 0.00011320
Iteration 147/1000 | Loss: 0.00041828
Iteration 148/1000 | Loss: 0.00011070
Iteration 149/1000 | Loss: 0.00011225
Iteration 150/1000 | Loss: 0.00011060
Iteration 151/1000 | Loss: 0.00011120
Iteration 152/1000 | Loss: 0.00011089
Iteration 153/1000 | Loss: 0.00011112
Iteration 154/1000 | Loss: 0.00011139
Iteration 155/1000 | Loss: 0.00011063
Iteration 156/1000 | Loss: 0.00011121
Iteration 157/1000 | Loss: 0.00011097
Iteration 158/1000 | Loss: 0.00011007
Iteration 159/1000 | Loss: 0.00011130
Iteration 160/1000 | Loss: 0.00011126
Iteration 161/1000 | Loss: 0.00011057
Iteration 162/1000 | Loss: 0.00050325
Iteration 163/1000 | Loss: 0.00033488
Iteration 164/1000 | Loss: 0.00025520
Iteration 165/1000 | Loss: 0.00051103
Iteration 166/1000 | Loss: 0.00062995
Iteration 167/1000 | Loss: 0.00098936
Iteration 168/1000 | Loss: 0.00024110
Iteration 169/1000 | Loss: 0.00013964
Iteration 170/1000 | Loss: 0.00012153
Iteration 171/1000 | Loss: 0.00094884
Iteration 172/1000 | Loss: 0.00011037
Iteration 173/1000 | Loss: 0.00052232
Iteration 174/1000 | Loss: 0.00010102
Iteration 175/1000 | Loss: 0.00009622
Iteration 176/1000 | Loss: 0.00009333
Iteration 177/1000 | Loss: 0.00009188
Iteration 178/1000 | Loss: 0.00009042
Iteration 179/1000 | Loss: 0.00008935
Iteration 180/1000 | Loss: 0.00033385
Iteration 181/1000 | Loss: 0.00022326
Iteration 182/1000 | Loss: 0.00011200
Iteration 183/1000 | Loss: 0.00008834
Iteration 184/1000 | Loss: 0.00008773
Iteration 185/1000 | Loss: 0.00008720
Iteration 186/1000 | Loss: 0.00008680
Iteration 187/1000 | Loss: 0.00008658
Iteration 188/1000 | Loss: 0.00052795
Iteration 189/1000 | Loss: 0.00008732
Iteration 190/1000 | Loss: 0.00008640
Iteration 191/1000 | Loss: 0.00008637
Iteration 192/1000 | Loss: 0.00008626
Iteration 193/1000 | Loss: 0.00008620
Iteration 194/1000 | Loss: 0.00008618
Iteration 195/1000 | Loss: 0.00008618
Iteration 196/1000 | Loss: 0.00008610
Iteration 197/1000 | Loss: 0.00008604
Iteration 198/1000 | Loss: 0.00008603
Iteration 199/1000 | Loss: 0.00008603
Iteration 200/1000 | Loss: 0.00008602
Iteration 201/1000 | Loss: 0.00008601
Iteration 202/1000 | Loss: 0.00008600
Iteration 203/1000 | Loss: 0.00008599
Iteration 204/1000 | Loss: 0.00008589
Iteration 205/1000 | Loss: 0.00008585
Iteration 206/1000 | Loss: 0.00008572
Iteration 207/1000 | Loss: 0.00008555
Iteration 208/1000 | Loss: 0.00008524
Iteration 209/1000 | Loss: 0.00008497
Iteration 210/1000 | Loss: 0.00022384
Iteration 211/1000 | Loss: 0.00017760
Iteration 212/1000 | Loss: 0.00044938
Iteration 213/1000 | Loss: 0.00008806
Iteration 214/1000 | Loss: 0.00008614
Iteration 215/1000 | Loss: 0.00008503
Iteration 216/1000 | Loss: 0.00008370
Iteration 217/1000 | Loss: 0.00008322
Iteration 218/1000 | Loss: 0.00008285
Iteration 219/1000 | Loss: 0.00008257
Iteration 220/1000 | Loss: 0.00008243
Iteration 221/1000 | Loss: 0.00008228
Iteration 222/1000 | Loss: 0.00008214
Iteration 223/1000 | Loss: 0.00008209
Iteration 224/1000 | Loss: 0.00008206
Iteration 225/1000 | Loss: 0.00008206
Iteration 226/1000 | Loss: 0.00008205
Iteration 227/1000 | Loss: 0.00008205
Iteration 228/1000 | Loss: 0.00008204
Iteration 229/1000 | Loss: 0.00008204
Iteration 230/1000 | Loss: 0.00008203
Iteration 231/1000 | Loss: 0.00008203
Iteration 232/1000 | Loss: 0.00008199
Iteration 233/1000 | Loss: 0.00008195
Iteration 234/1000 | Loss: 0.00008191
Iteration 235/1000 | Loss: 0.00008185
Iteration 236/1000 | Loss: 0.00008183
Iteration 237/1000 | Loss: 0.00008183
Iteration 238/1000 | Loss: 0.00008183
Iteration 239/1000 | Loss: 0.00008182
Iteration 240/1000 | Loss: 0.00008180
Iteration 241/1000 | Loss: 0.00008180
Iteration 242/1000 | Loss: 0.00008179
Iteration 243/1000 | Loss: 0.00008179
Iteration 244/1000 | Loss: 0.00008178
Iteration 245/1000 | Loss: 0.00008178
Iteration 246/1000 | Loss: 0.00008177
Iteration 247/1000 | Loss: 0.00008177
Iteration 248/1000 | Loss: 0.00008176
Iteration 249/1000 | Loss: 0.00008176
Iteration 250/1000 | Loss: 0.00008175
Iteration 251/1000 | Loss: 0.00008173
Iteration 252/1000 | Loss: 0.00008173
Iteration 253/1000 | Loss: 0.00008172
Iteration 254/1000 | Loss: 0.00008172
Iteration 255/1000 | Loss: 0.00008171
Iteration 256/1000 | Loss: 0.00008171
Iteration 257/1000 | Loss: 0.00008170
Iteration 258/1000 | Loss: 0.00008170
Iteration 259/1000 | Loss: 0.00008170
Iteration 260/1000 | Loss: 0.00008169
Iteration 261/1000 | Loss: 0.00008169
Iteration 262/1000 | Loss: 0.00008168
Iteration 263/1000 | Loss: 0.00008168
Iteration 264/1000 | Loss: 0.00008168
Iteration 265/1000 | Loss: 0.00008167
Iteration 266/1000 | Loss: 0.00008165
Iteration 267/1000 | Loss: 0.00008164
Iteration 268/1000 | Loss: 0.00008163
Iteration 269/1000 | Loss: 0.00008162
Iteration 270/1000 | Loss: 0.00008149
Iteration 271/1000 | Loss: 0.00008143
Iteration 272/1000 | Loss: 0.00008133
Iteration 273/1000 | Loss: 0.00008133
Iteration 274/1000 | Loss: 0.00008132
Iteration 275/1000 | Loss: 0.00021585
Iteration 276/1000 | Loss: 0.00044692
Iteration 277/1000 | Loss: 0.00055478
Iteration 278/1000 | Loss: 0.00051072
Iteration 279/1000 | Loss: 0.00039502
Iteration 280/1000 | Loss: 0.00013123
Iteration 281/1000 | Loss: 0.00009513
Iteration 282/1000 | Loss: 0.00008461
Iteration 283/1000 | Loss: 0.00007952
Iteration 284/1000 | Loss: 0.00007575
Iteration 285/1000 | Loss: 0.00021343
Iteration 286/1000 | Loss: 0.00007405
Iteration 287/1000 | Loss: 0.00007230
Iteration 288/1000 | Loss: 0.00007148
Iteration 289/1000 | Loss: 0.00007043
Iteration 290/1000 | Loss: 0.00006976
Iteration 291/1000 | Loss: 0.00006927
Iteration 292/1000 | Loss: 0.00037184
Iteration 293/1000 | Loss: 0.00007023
Iteration 294/1000 | Loss: 0.00006873
Iteration 295/1000 | Loss: 0.00006855
Iteration 296/1000 | Loss: 0.00006840
Iteration 297/1000 | Loss: 0.00006821
Iteration 298/1000 | Loss: 0.00006818
Iteration 299/1000 | Loss: 0.00006817
Iteration 300/1000 | Loss: 0.00006815
Iteration 301/1000 | Loss: 0.00006815
Iteration 302/1000 | Loss: 0.00006814
Iteration 303/1000 | Loss: 0.00006814
Iteration 304/1000 | Loss: 0.00006814
Iteration 305/1000 | Loss: 0.00006814
Iteration 306/1000 | Loss: 0.00006814
Iteration 307/1000 | Loss: 0.00006814
Iteration 308/1000 | Loss: 0.00006814
Iteration 309/1000 | Loss: 0.00006814
Iteration 310/1000 | Loss: 0.00006814
Iteration 311/1000 | Loss: 0.00006814
Iteration 312/1000 | Loss: 0.00006814
Iteration 313/1000 | Loss: 0.00006814
Iteration 314/1000 | Loss: 0.00006814
Iteration 315/1000 | Loss: 0.00006813
Iteration 316/1000 | Loss: 0.00006813
Iteration 317/1000 | Loss: 0.00006812
Iteration 318/1000 | Loss: 0.00006811
Iteration 319/1000 | Loss: 0.00006808
Iteration 320/1000 | Loss: 0.00006804
Iteration 321/1000 | Loss: 0.00006804
Iteration 322/1000 | Loss: 0.00006804
Iteration 323/1000 | Loss: 0.00006804
Iteration 324/1000 | Loss: 0.00006804
Iteration 325/1000 | Loss: 0.00006804
Iteration 326/1000 | Loss: 0.00006804
Iteration 327/1000 | Loss: 0.00006804
Iteration 328/1000 | Loss: 0.00006804
Iteration 329/1000 | Loss: 0.00006804
Iteration 330/1000 | Loss: 0.00006803
Iteration 331/1000 | Loss: 0.00006803
Iteration 332/1000 | Loss: 0.00006802
Iteration 333/1000 | Loss: 0.00006802
Iteration 334/1000 | Loss: 0.00006801
Iteration 335/1000 | Loss: 0.00006801
Iteration 336/1000 | Loss: 0.00006801
Iteration 337/1000 | Loss: 0.00006800
Iteration 338/1000 | Loss: 0.00006800
Iteration 339/1000 | Loss: 0.00006799
Iteration 340/1000 | Loss: 0.00006799
Iteration 341/1000 | Loss: 0.00006798
Iteration 342/1000 | Loss: 0.00006798
Iteration 343/1000 | Loss: 0.00006796
Iteration 344/1000 | Loss: 0.00006795
Iteration 345/1000 | Loss: 0.00006794
Iteration 346/1000 | Loss: 0.00006794
Iteration 347/1000 | Loss: 0.00006794
Iteration 348/1000 | Loss: 0.00006794
Iteration 349/1000 | Loss: 0.00006794
Iteration 350/1000 | Loss: 0.00006794
Iteration 351/1000 | Loss: 0.00006794
Iteration 352/1000 | Loss: 0.00006793
Iteration 353/1000 | Loss: 0.00006792
Iteration 354/1000 | Loss: 0.00006791
Iteration 355/1000 | Loss: 0.00006791
Iteration 356/1000 | Loss: 0.00006790
Iteration 357/1000 | Loss: 0.00006788
Iteration 358/1000 | Loss: 0.00006787
Iteration 359/1000 | Loss: 0.00006787
Iteration 360/1000 | Loss: 0.00006786
Iteration 361/1000 | Loss: 0.00006786
Iteration 362/1000 | Loss: 0.00006785
Iteration 363/1000 | Loss: 0.00006785
Iteration 364/1000 | Loss: 0.00006785
Iteration 365/1000 | Loss: 0.00006784
Iteration 366/1000 | Loss: 0.00006784
Iteration 367/1000 | Loss: 0.00006784
Iteration 368/1000 | Loss: 0.00006783
Iteration 369/1000 | Loss: 0.00006783
Iteration 370/1000 | Loss: 0.00006783
Iteration 371/1000 | Loss: 0.00006783
Iteration 372/1000 | Loss: 0.00006782
Iteration 373/1000 | Loss: 0.00006782
Iteration 374/1000 | Loss: 0.00006781
Iteration 375/1000 | Loss: 0.00006780
Iteration 376/1000 | Loss: 0.00006779
Iteration 377/1000 | Loss: 0.00006779
Iteration 378/1000 | Loss: 0.00006778
Iteration 379/1000 | Loss: 0.00006777
Iteration 380/1000 | Loss: 0.00006776
Iteration 381/1000 | Loss: 0.00006776
Iteration 382/1000 | Loss: 0.00006775
Iteration 383/1000 | Loss: 0.00006774
Iteration 384/1000 | Loss: 0.00006774
Iteration 385/1000 | Loss: 0.00006771
Iteration 386/1000 | Loss: 0.00006770
Iteration 387/1000 | Loss: 0.00006770
Iteration 388/1000 | Loss: 0.00006769
Iteration 389/1000 | Loss: 0.00006767
Iteration 390/1000 | Loss: 0.00006767
Iteration 391/1000 | Loss: 0.00006766
Iteration 392/1000 | Loss: 0.00006766
Iteration 393/1000 | Loss: 0.00006766
Iteration 394/1000 | Loss: 0.00006766
Iteration 395/1000 | Loss: 0.00006765
Iteration 396/1000 | Loss: 0.00006765
Iteration 397/1000 | Loss: 0.00006765
Iteration 398/1000 | Loss: 0.00006765
Iteration 399/1000 | Loss: 0.00006765
Iteration 400/1000 | Loss: 0.00006765
Iteration 401/1000 | Loss: 0.00006765
Iteration 402/1000 | Loss: 0.00006765
Iteration 403/1000 | Loss: 0.00006765
Iteration 404/1000 | Loss: 0.00006764
Iteration 405/1000 | Loss: 0.00006764
Iteration 406/1000 | Loss: 0.00006764
Iteration 407/1000 | Loss: 0.00006763
Iteration 408/1000 | Loss: 0.00006762
Iteration 409/1000 | Loss: 0.00006761
Iteration 410/1000 | Loss: 0.00006761
Iteration 411/1000 | Loss: 0.00006761
Iteration 412/1000 | Loss: 0.00006761
Iteration 413/1000 | Loss: 0.00006761
Iteration 414/1000 | Loss: 0.00006761
Iteration 415/1000 | Loss: 0.00006761
Iteration 416/1000 | Loss: 0.00006760
Iteration 417/1000 | Loss: 0.00006757
Iteration 418/1000 | Loss: 0.00006757
Iteration 419/1000 | Loss: 0.00006756
Iteration 420/1000 | Loss: 0.00006756
Iteration 421/1000 | Loss: 0.00006756
Iteration 422/1000 | Loss: 0.00006754
Iteration 423/1000 | Loss: 0.00006754
Iteration 424/1000 | Loss: 0.00006752
Iteration 425/1000 | Loss: 0.00006751
Iteration 426/1000 | Loss: 0.00006750
Iteration 427/1000 | Loss: 0.00006749
Iteration 428/1000 | Loss: 0.00006749
Iteration 429/1000 | Loss: 0.00006748
Iteration 430/1000 | Loss: 0.00006748
Iteration 431/1000 | Loss: 0.00006748
Iteration 432/1000 | Loss: 0.00006747
Iteration 433/1000 | Loss: 0.00006747
Iteration 434/1000 | Loss: 0.00006746
Iteration 435/1000 | Loss: 0.00006746
Iteration 436/1000 | Loss: 0.00006746
Iteration 437/1000 | Loss: 0.00006745
Iteration 438/1000 | Loss: 0.00006745
Iteration 439/1000 | Loss: 0.00006745
Iteration 440/1000 | Loss: 0.00006745
Iteration 441/1000 | Loss: 0.00006745
Iteration 442/1000 | Loss: 0.00006745
Iteration 443/1000 | Loss: 0.00006744
Iteration 444/1000 | Loss: 0.00006744
Iteration 445/1000 | Loss: 0.00006744
Iteration 446/1000 | Loss: 0.00021228
Iteration 447/1000 | Loss: 0.00049567
Iteration 448/1000 | Loss: 0.00015814
Iteration 449/1000 | Loss: 0.00016849
Iteration 450/1000 | Loss: 0.00007934
Iteration 451/1000 | Loss: 0.00007246
Iteration 452/1000 | Loss: 0.00006861
Iteration 453/1000 | Loss: 0.00006561
Iteration 454/1000 | Loss: 0.00006355
Iteration 455/1000 | Loss: 0.00006233
Iteration 456/1000 | Loss: 0.00006160
Iteration 457/1000 | Loss: 0.00006102
Iteration 458/1000 | Loss: 0.00006068
Iteration 459/1000 | Loss: 0.00006045
Iteration 460/1000 | Loss: 0.00006043
Iteration 461/1000 | Loss: 0.00006032
Iteration 462/1000 | Loss: 0.00006015
Iteration 463/1000 | Loss: 0.00006010
Iteration 464/1000 | Loss: 0.00006006
Iteration 465/1000 | Loss: 0.00006005
Iteration 466/1000 | Loss: 0.00006003
Iteration 467/1000 | Loss: 0.00006003
Iteration 468/1000 | Loss: 0.00006002
Iteration 469/1000 | Loss: 0.00006002
Iteration 470/1000 | Loss: 0.00006001
Iteration 471/1000 | Loss: 0.00005997
Iteration 472/1000 | Loss: 0.00005997
Iteration 473/1000 | Loss: 0.00005996
Iteration 474/1000 | Loss: 0.00005995
Iteration 475/1000 | Loss: 0.00005995
Iteration 476/1000 | Loss: 0.00005994
Iteration 477/1000 | Loss: 0.00005994
Iteration 478/1000 | Loss: 0.00005993
Iteration 479/1000 | Loss: 0.00005993
Iteration 480/1000 | Loss: 0.00005993
Iteration 481/1000 | Loss: 0.00005992
Iteration 482/1000 | Loss: 0.00005992
Iteration 483/1000 | Loss: 0.00005992
Iteration 484/1000 | Loss: 0.00005991
Iteration 485/1000 | Loss: 0.00005991
Iteration 486/1000 | Loss: 0.00005991
Iteration 487/1000 | Loss: 0.00005991
Iteration 488/1000 | Loss: 0.00005990
Iteration 489/1000 | Loss: 0.00005990
Iteration 490/1000 | Loss: 0.00005990
Iteration 491/1000 | Loss: 0.00005990
Iteration 492/1000 | Loss: 0.00005990
Iteration 493/1000 | Loss: 0.00005990
Iteration 494/1000 | Loss: 0.00005990
Iteration 495/1000 | Loss: 0.00005990
Iteration 496/1000 | Loss: 0.00005990
Iteration 497/1000 | Loss: 0.00005989
Iteration 498/1000 | Loss: 0.00005989
Iteration 499/1000 | Loss: 0.00005989
Iteration 500/1000 | Loss: 0.00005989
Iteration 501/1000 | Loss: 0.00005989
Iteration 502/1000 | Loss: 0.00005989
Iteration 503/1000 | Loss: 0.00005988
Iteration 504/1000 | Loss: 0.00005988
Iteration 505/1000 | Loss: 0.00005988
Iteration 506/1000 | Loss: 0.00005988
Iteration 507/1000 | Loss: 0.00005988
Iteration 508/1000 | Loss: 0.00005988
Iteration 509/1000 | Loss: 0.00005988
Iteration 510/1000 | Loss: 0.00005988
Iteration 511/1000 | Loss: 0.00005988
Iteration 512/1000 | Loss: 0.00005988
Iteration 513/1000 | Loss: 0.00005988
Iteration 514/1000 | Loss: 0.00005987
Iteration 515/1000 | Loss: 0.00005987
Iteration 516/1000 | Loss: 0.00005987
Iteration 517/1000 | Loss: 0.00005987
Iteration 518/1000 | Loss: 0.00005987
Iteration 519/1000 | Loss: 0.00005987
Iteration 520/1000 | Loss: 0.00005986
Iteration 521/1000 | Loss: 0.00005986
Iteration 522/1000 | Loss: 0.00005986
Iteration 523/1000 | Loss: 0.00005986
Iteration 524/1000 | Loss: 0.00005986
Iteration 525/1000 | Loss: 0.00005986
Iteration 526/1000 | Loss: 0.00005986
Iteration 527/1000 | Loss: 0.00005986
Iteration 528/1000 | Loss: 0.00005986
Iteration 529/1000 | Loss: 0.00005986
Iteration 530/1000 | Loss: 0.00005986
Iteration 531/1000 | Loss: 0.00005986
Iteration 532/1000 | Loss: 0.00005986
Iteration 533/1000 | Loss: 0.00005986
Iteration 534/1000 | Loss: 0.00005985
Iteration 535/1000 | Loss: 0.00005985
Iteration 536/1000 | Loss: 0.00005985
Iteration 537/1000 | Loss: 0.00005985
Iteration 538/1000 | Loss: 0.00005985
Iteration 539/1000 | Loss: 0.00005985
Iteration 540/1000 | Loss: 0.00005985
Iteration 541/1000 | Loss: 0.00005985
Iteration 542/1000 | Loss: 0.00005985
Iteration 543/1000 | Loss: 0.00005985
Iteration 544/1000 | Loss: 0.00005985
Iteration 545/1000 | Loss: 0.00005985
Iteration 546/1000 | Loss: 0.00005985
Iteration 547/1000 | Loss: 0.00005985
Iteration 548/1000 | Loss: 0.00005985
Iteration 549/1000 | Loss: 0.00005985
Iteration 550/1000 | Loss: 0.00005985
Iteration 551/1000 | Loss: 0.00005985
Iteration 552/1000 | Loss: 0.00005985
Iteration 553/1000 | Loss: 0.00005985
Iteration 554/1000 | Loss: 0.00005985
Iteration 555/1000 | Loss: 0.00005985
Iteration 556/1000 | Loss: 0.00005985
Iteration 557/1000 | Loss: 0.00005985
Iteration 558/1000 | Loss: 0.00005985
Iteration 559/1000 | Loss: 0.00005985
Iteration 560/1000 | Loss: 0.00005985
Iteration 561/1000 | Loss: 0.00005985
Iteration 562/1000 | Loss: 0.00005985
Iteration 563/1000 | Loss: 0.00005985
Iteration 564/1000 | Loss: 0.00005985
Iteration 565/1000 | Loss: 0.00005985
Iteration 566/1000 | Loss: 0.00005985
Iteration 567/1000 | Loss: 0.00005985
Iteration 568/1000 | Loss: 0.00005985
Iteration 569/1000 | Loss: 0.00005985
Iteration 570/1000 | Loss: 0.00005985
Iteration 571/1000 | Loss: 0.00005985
Iteration 572/1000 | Loss: 0.00005985
Iteration 573/1000 | Loss: 0.00005985
Iteration 574/1000 | Loss: 0.00005985
Iteration 575/1000 | Loss: 0.00005985
Iteration 576/1000 | Loss: 0.00005985
Iteration 577/1000 | Loss: 0.00005985
Iteration 578/1000 | Loss: 0.00005985
Iteration 579/1000 | Loss: 0.00005985
Iteration 580/1000 | Loss: 0.00005985
Iteration 581/1000 | Loss: 0.00005985
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 581. Stopping optimization.
Last 5 losses: [5.984572271700017e-05, 5.984572271700017e-05, 5.984572271700017e-05, 5.984572271700017e-05, 5.984572271700017e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 5.984572271700017e-05

Optimization complete. Final v2v error: 3.9442362785339355 mm

Highest mean error: 14.134160041809082 mm for frame 93

Lowest mean error: 2.1952016353607178 mm for frame 4

Saving results

Total time: 506.73589062690735
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_it_4031/0021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_it_4031/0021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_it_4031/0021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00445069
Iteration 2/25 | Loss: 0.00109598
Iteration 3/25 | Loss: 0.00096823
Iteration 4/25 | Loss: 0.00095143
Iteration 5/25 | Loss: 0.00094724
Iteration 6/25 | Loss: 0.00094560
Iteration 7/25 | Loss: 0.00094540
Iteration 8/25 | Loss: 0.00094540
Iteration 9/25 | Loss: 0.00094540
Iteration 10/25 | Loss: 0.00094540
Iteration 11/25 | Loss: 0.00094540
Iteration 12/25 | Loss: 0.00094540
Iteration 13/25 | Loss: 0.00094540
Iteration 14/25 | Loss: 0.00094540
Iteration 15/25 | Loss: 0.00094540
Iteration 16/25 | Loss: 0.00094540
Iteration 17/25 | Loss: 0.00094540
Iteration 18/25 | Loss: 0.00094540
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0009454024257138371, 0.0009454024257138371, 0.0009454024257138371, 0.0009454024257138371, 0.0009454024257138371]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009454024257138371

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.29370129
Iteration 2/25 | Loss: 0.00119803
Iteration 3/25 | Loss: 0.00119802
Iteration 4/25 | Loss: 0.00119801
Iteration 5/25 | Loss: 0.00119801
Iteration 6/25 | Loss: 0.00119801
Iteration 7/25 | Loss: 0.00119801
Iteration 8/25 | Loss: 0.00119801
Iteration 9/25 | Loss: 0.00119801
Iteration 10/25 | Loss: 0.00119801
Iteration 11/25 | Loss: 0.00119801
Iteration 12/25 | Loss: 0.00119801
Iteration 13/25 | Loss: 0.00119801
Iteration 14/25 | Loss: 0.00119801
Iteration 15/25 | Loss: 0.00119801
Iteration 16/25 | Loss: 0.00119801
Iteration 17/25 | Loss: 0.00119801
Iteration 18/25 | Loss: 0.00119801
Iteration 19/25 | Loss: 0.00119801
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0011980115668848157, 0.0011980115668848157, 0.0011980115668848157, 0.0011980115668848157, 0.0011980115668848157]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011980115668848157

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00119801
Iteration 2/1000 | Loss: 0.00003118
Iteration 3/1000 | Loss: 0.00001866
Iteration 4/1000 | Loss: 0.00001337
Iteration 5/1000 | Loss: 0.00001176
Iteration 6/1000 | Loss: 0.00001100
Iteration 7/1000 | Loss: 0.00001061
Iteration 8/1000 | Loss: 0.00001030
Iteration 9/1000 | Loss: 0.00001020
Iteration 10/1000 | Loss: 0.00001005
Iteration 11/1000 | Loss: 0.00001004
Iteration 12/1000 | Loss: 0.00000998
Iteration 13/1000 | Loss: 0.00000993
Iteration 14/1000 | Loss: 0.00000990
Iteration 15/1000 | Loss: 0.00000987
Iteration 16/1000 | Loss: 0.00000983
Iteration 17/1000 | Loss: 0.00000983
Iteration 18/1000 | Loss: 0.00000979
Iteration 19/1000 | Loss: 0.00000977
Iteration 20/1000 | Loss: 0.00000976
Iteration 21/1000 | Loss: 0.00000976
Iteration 22/1000 | Loss: 0.00000968
Iteration 23/1000 | Loss: 0.00000968
Iteration 24/1000 | Loss: 0.00000962
Iteration 25/1000 | Loss: 0.00000962
Iteration 26/1000 | Loss: 0.00000962
Iteration 27/1000 | Loss: 0.00000962
Iteration 28/1000 | Loss: 0.00000960
Iteration 29/1000 | Loss: 0.00000960
Iteration 30/1000 | Loss: 0.00000955
Iteration 31/1000 | Loss: 0.00000955
Iteration 32/1000 | Loss: 0.00000955
Iteration 33/1000 | Loss: 0.00000955
Iteration 34/1000 | Loss: 0.00000955
Iteration 35/1000 | Loss: 0.00000955
Iteration 36/1000 | Loss: 0.00000955
Iteration 37/1000 | Loss: 0.00000955
Iteration 38/1000 | Loss: 0.00000955
Iteration 39/1000 | Loss: 0.00000955
Iteration 40/1000 | Loss: 0.00000954
Iteration 41/1000 | Loss: 0.00000953
Iteration 42/1000 | Loss: 0.00000953
Iteration 43/1000 | Loss: 0.00000953
Iteration 44/1000 | Loss: 0.00000953
Iteration 45/1000 | Loss: 0.00000953
Iteration 46/1000 | Loss: 0.00000953
Iteration 47/1000 | Loss: 0.00000953
Iteration 48/1000 | Loss: 0.00000952
Iteration 49/1000 | Loss: 0.00000951
Iteration 50/1000 | Loss: 0.00000951
Iteration 51/1000 | Loss: 0.00000950
Iteration 52/1000 | Loss: 0.00000950
Iteration 53/1000 | Loss: 0.00000950
Iteration 54/1000 | Loss: 0.00000949
Iteration 55/1000 | Loss: 0.00000949
Iteration 56/1000 | Loss: 0.00000949
Iteration 57/1000 | Loss: 0.00000949
Iteration 58/1000 | Loss: 0.00000949
Iteration 59/1000 | Loss: 0.00000949
Iteration 60/1000 | Loss: 0.00000948
Iteration 61/1000 | Loss: 0.00000948
Iteration 62/1000 | Loss: 0.00000948
Iteration 63/1000 | Loss: 0.00000948
Iteration 64/1000 | Loss: 0.00000947
Iteration 65/1000 | Loss: 0.00000947
Iteration 66/1000 | Loss: 0.00000947
Iteration 67/1000 | Loss: 0.00000947
Iteration 68/1000 | Loss: 0.00000947
Iteration 69/1000 | Loss: 0.00000947
Iteration 70/1000 | Loss: 0.00000947
Iteration 71/1000 | Loss: 0.00000947
Iteration 72/1000 | Loss: 0.00000947
Iteration 73/1000 | Loss: 0.00000947
Iteration 74/1000 | Loss: 0.00000947
Iteration 75/1000 | Loss: 0.00000946
Iteration 76/1000 | Loss: 0.00000946
Iteration 77/1000 | Loss: 0.00000946
Iteration 78/1000 | Loss: 0.00000946
Iteration 79/1000 | Loss: 0.00000946
Iteration 80/1000 | Loss: 0.00000946
Iteration 81/1000 | Loss: 0.00000945
Iteration 82/1000 | Loss: 0.00000945
Iteration 83/1000 | Loss: 0.00000945
Iteration 84/1000 | Loss: 0.00000944
Iteration 85/1000 | Loss: 0.00000944
Iteration 86/1000 | Loss: 0.00000944
Iteration 87/1000 | Loss: 0.00000944
Iteration 88/1000 | Loss: 0.00000944
Iteration 89/1000 | Loss: 0.00000944
Iteration 90/1000 | Loss: 0.00000944
Iteration 91/1000 | Loss: 0.00000944
Iteration 92/1000 | Loss: 0.00000944
Iteration 93/1000 | Loss: 0.00000943
Iteration 94/1000 | Loss: 0.00000943
Iteration 95/1000 | Loss: 0.00000943
Iteration 96/1000 | Loss: 0.00000943
Iteration 97/1000 | Loss: 0.00000943
Iteration 98/1000 | Loss: 0.00000943
Iteration 99/1000 | Loss: 0.00000943
Iteration 100/1000 | Loss: 0.00000942
Iteration 101/1000 | Loss: 0.00000942
Iteration 102/1000 | Loss: 0.00000942
Iteration 103/1000 | Loss: 0.00000942
Iteration 104/1000 | Loss: 0.00000942
Iteration 105/1000 | Loss: 0.00000942
Iteration 106/1000 | Loss: 0.00000942
Iteration 107/1000 | Loss: 0.00000942
Iteration 108/1000 | Loss: 0.00000941
Iteration 109/1000 | Loss: 0.00000941
Iteration 110/1000 | Loss: 0.00000941
Iteration 111/1000 | Loss: 0.00000941
Iteration 112/1000 | Loss: 0.00000941
Iteration 113/1000 | Loss: 0.00000941
Iteration 114/1000 | Loss: 0.00000941
Iteration 115/1000 | Loss: 0.00000941
Iteration 116/1000 | Loss: 0.00000940
Iteration 117/1000 | Loss: 0.00000940
Iteration 118/1000 | Loss: 0.00000940
Iteration 119/1000 | Loss: 0.00000940
Iteration 120/1000 | Loss: 0.00000940
Iteration 121/1000 | Loss: 0.00000940
Iteration 122/1000 | Loss: 0.00000939
Iteration 123/1000 | Loss: 0.00000939
Iteration 124/1000 | Loss: 0.00000939
Iteration 125/1000 | Loss: 0.00000939
Iteration 126/1000 | Loss: 0.00000939
Iteration 127/1000 | Loss: 0.00000939
Iteration 128/1000 | Loss: 0.00000939
Iteration 129/1000 | Loss: 0.00000939
Iteration 130/1000 | Loss: 0.00000939
Iteration 131/1000 | Loss: 0.00000939
Iteration 132/1000 | Loss: 0.00000939
Iteration 133/1000 | Loss: 0.00000939
Iteration 134/1000 | Loss: 0.00000939
Iteration 135/1000 | Loss: 0.00000939
Iteration 136/1000 | Loss: 0.00000939
Iteration 137/1000 | Loss: 0.00000939
Iteration 138/1000 | Loss: 0.00000939
Iteration 139/1000 | Loss: 0.00000938
Iteration 140/1000 | Loss: 0.00000938
Iteration 141/1000 | Loss: 0.00000938
Iteration 142/1000 | Loss: 0.00000938
Iteration 143/1000 | Loss: 0.00000938
Iteration 144/1000 | Loss: 0.00000938
Iteration 145/1000 | Loss: 0.00000937
Iteration 146/1000 | Loss: 0.00000937
Iteration 147/1000 | Loss: 0.00000937
Iteration 148/1000 | Loss: 0.00000937
Iteration 149/1000 | Loss: 0.00000937
Iteration 150/1000 | Loss: 0.00000936
Iteration 151/1000 | Loss: 0.00000936
Iteration 152/1000 | Loss: 0.00000936
Iteration 153/1000 | Loss: 0.00000936
Iteration 154/1000 | Loss: 0.00000936
Iteration 155/1000 | Loss: 0.00000936
Iteration 156/1000 | Loss: 0.00000936
Iteration 157/1000 | Loss: 0.00000936
Iteration 158/1000 | Loss: 0.00000936
Iteration 159/1000 | Loss: 0.00000936
Iteration 160/1000 | Loss: 0.00000936
Iteration 161/1000 | Loss: 0.00000935
Iteration 162/1000 | Loss: 0.00000935
Iteration 163/1000 | Loss: 0.00000935
Iteration 164/1000 | Loss: 0.00000935
Iteration 165/1000 | Loss: 0.00000935
Iteration 166/1000 | Loss: 0.00000935
Iteration 167/1000 | Loss: 0.00000935
Iteration 168/1000 | Loss: 0.00000935
Iteration 169/1000 | Loss: 0.00000935
Iteration 170/1000 | Loss: 0.00000935
Iteration 171/1000 | Loss: 0.00000935
Iteration 172/1000 | Loss: 0.00000935
Iteration 173/1000 | Loss: 0.00000935
Iteration 174/1000 | Loss: 0.00000935
Iteration 175/1000 | Loss: 0.00000935
Iteration 176/1000 | Loss: 0.00000935
Iteration 177/1000 | Loss: 0.00000935
Iteration 178/1000 | Loss: 0.00000935
Iteration 179/1000 | Loss: 0.00000934
Iteration 180/1000 | Loss: 0.00000934
Iteration 181/1000 | Loss: 0.00000934
Iteration 182/1000 | Loss: 0.00000934
Iteration 183/1000 | Loss: 0.00000934
Iteration 184/1000 | Loss: 0.00000934
Iteration 185/1000 | Loss: 0.00000934
Iteration 186/1000 | Loss: 0.00000934
Iteration 187/1000 | Loss: 0.00000934
Iteration 188/1000 | Loss: 0.00000934
Iteration 189/1000 | Loss: 0.00000934
Iteration 190/1000 | Loss: 0.00000934
Iteration 191/1000 | Loss: 0.00000934
Iteration 192/1000 | Loss: 0.00000934
Iteration 193/1000 | Loss: 0.00000934
Iteration 194/1000 | Loss: 0.00000934
Iteration 195/1000 | Loss: 0.00000934
Iteration 196/1000 | Loss: 0.00000933
Iteration 197/1000 | Loss: 0.00000933
Iteration 198/1000 | Loss: 0.00000933
Iteration 199/1000 | Loss: 0.00000933
Iteration 200/1000 | Loss: 0.00000933
Iteration 201/1000 | Loss: 0.00000933
Iteration 202/1000 | Loss: 0.00000933
Iteration 203/1000 | Loss: 0.00000933
Iteration 204/1000 | Loss: 0.00000933
Iteration 205/1000 | Loss: 0.00000933
Iteration 206/1000 | Loss: 0.00000933
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 206. Stopping optimization.
Last 5 losses: [9.332804438599851e-06, 9.332804438599851e-06, 9.332804438599851e-06, 9.332804438599851e-06, 9.332804438599851e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.332804438599851e-06

Optimization complete. Final v2v error: 2.536910057067871 mm

Highest mean error: 2.9294824600219727 mm for frame 105

Lowest mean error: 2.1665289402008057 mm for frame 64

Saving results

Total time: 40.520986557006836
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_it_4031/0013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_it_4031/0013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_it_4031/0013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01002117
Iteration 2/25 | Loss: 0.00369277
Iteration 3/25 | Loss: 0.00257767
Iteration 4/25 | Loss: 0.00217003
Iteration 5/25 | Loss: 0.00211642
Iteration 6/25 | Loss: 0.00205168
Iteration 7/25 | Loss: 0.00175372
Iteration 8/25 | Loss: 0.00156185
Iteration 9/25 | Loss: 0.00150751
Iteration 10/25 | Loss: 0.00147467
Iteration 11/25 | Loss: 0.00138454
Iteration 12/25 | Loss: 0.00133581
Iteration 13/25 | Loss: 0.00133548
Iteration 14/25 | Loss: 0.00130904
Iteration 15/25 | Loss: 0.00129861
Iteration 16/25 | Loss: 0.00129562
Iteration 17/25 | Loss: 0.00129527
Iteration 18/25 | Loss: 0.00129358
Iteration 19/25 | Loss: 0.00129114
Iteration 20/25 | Loss: 0.00129169
Iteration 21/25 | Loss: 0.00129139
Iteration 22/25 | Loss: 0.00128567
Iteration 23/25 | Loss: 0.00128340
Iteration 24/25 | Loss: 0.00128320
Iteration 25/25 | Loss: 0.00128313

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.24703193
Iteration 2/25 | Loss: 0.00250629
Iteration 3/25 | Loss: 0.00250629
Iteration 4/25 | Loss: 0.00250629
Iteration 5/25 | Loss: 0.00250629
Iteration 6/25 | Loss: 0.00250629
Iteration 7/25 | Loss: 0.00250629
Iteration 8/25 | Loss: 0.00250629
Iteration 9/25 | Loss: 0.00250629
Iteration 10/25 | Loss: 0.00250629
Iteration 11/25 | Loss: 0.00250629
Iteration 12/25 | Loss: 0.00250629
Iteration 13/25 | Loss: 0.00250629
Iteration 14/25 | Loss: 0.00250629
Iteration 15/25 | Loss: 0.00250629
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0025062893982976675, 0.0025062893982976675, 0.0025062893982976675, 0.0025062893982976675, 0.0025062893982976675]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0025062893982976675

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00250629
Iteration 2/1000 | Loss: 0.00075020
Iteration 3/1000 | Loss: 0.00053094
Iteration 4/1000 | Loss: 0.00042431
Iteration 5/1000 | Loss: 0.00028061
Iteration 6/1000 | Loss: 0.00021829
Iteration 7/1000 | Loss: 0.00018666
Iteration 8/1000 | Loss: 0.00016773
Iteration 9/1000 | Loss: 0.00061865
Iteration 10/1000 | Loss: 0.00076125
Iteration 11/1000 | Loss: 0.00266492
Iteration 12/1000 | Loss: 0.00546816
Iteration 13/1000 | Loss: 0.00030201
Iteration 14/1000 | Loss: 0.00017928
Iteration 15/1000 | Loss: 0.00011900
Iteration 16/1000 | Loss: 0.00023465
Iteration 17/1000 | Loss: 0.00012993
Iteration 18/1000 | Loss: 0.00006701
Iteration 19/1000 | Loss: 0.00015827
Iteration 20/1000 | Loss: 0.00004814
Iteration 21/1000 | Loss: 0.00004175
Iteration 22/1000 | Loss: 0.00003720
Iteration 23/1000 | Loss: 0.00003292
Iteration 24/1000 | Loss: 0.00003064
Iteration 25/1000 | Loss: 0.00002764
Iteration 26/1000 | Loss: 0.00002569
Iteration 27/1000 | Loss: 0.00002374
Iteration 28/1000 | Loss: 0.00012091
Iteration 29/1000 | Loss: 0.00002485
Iteration 30/1000 | Loss: 0.00002150
Iteration 31/1000 | Loss: 0.00002057
Iteration 32/1000 | Loss: 0.00001928
Iteration 33/1000 | Loss: 0.00001848
Iteration 34/1000 | Loss: 0.00001794
Iteration 35/1000 | Loss: 0.00001762
Iteration 36/1000 | Loss: 0.00001739
Iteration 37/1000 | Loss: 0.00001733
Iteration 38/1000 | Loss: 0.00001714
Iteration 39/1000 | Loss: 0.00001708
Iteration 40/1000 | Loss: 0.00001704
Iteration 41/1000 | Loss: 0.00001703
Iteration 42/1000 | Loss: 0.00001703
Iteration 43/1000 | Loss: 0.00001703
Iteration 44/1000 | Loss: 0.00001702
Iteration 45/1000 | Loss: 0.00001701
Iteration 46/1000 | Loss: 0.00001701
Iteration 47/1000 | Loss: 0.00001700
Iteration 48/1000 | Loss: 0.00001698
Iteration 49/1000 | Loss: 0.00001698
Iteration 50/1000 | Loss: 0.00001698
Iteration 51/1000 | Loss: 0.00001698
Iteration 52/1000 | Loss: 0.00001697
Iteration 53/1000 | Loss: 0.00001697
Iteration 54/1000 | Loss: 0.00001697
Iteration 55/1000 | Loss: 0.00001697
Iteration 56/1000 | Loss: 0.00001697
Iteration 57/1000 | Loss: 0.00001697
Iteration 58/1000 | Loss: 0.00001697
Iteration 59/1000 | Loss: 0.00001696
Iteration 60/1000 | Loss: 0.00001694
Iteration 61/1000 | Loss: 0.00001694
Iteration 62/1000 | Loss: 0.00001693
Iteration 63/1000 | Loss: 0.00001693
Iteration 64/1000 | Loss: 0.00001693
Iteration 65/1000 | Loss: 0.00001693
Iteration 66/1000 | Loss: 0.00001692
Iteration 67/1000 | Loss: 0.00001692
Iteration 68/1000 | Loss: 0.00001692
Iteration 69/1000 | Loss: 0.00001692
Iteration 70/1000 | Loss: 0.00001692
Iteration 71/1000 | Loss: 0.00001692
Iteration 72/1000 | Loss: 0.00001691
Iteration 73/1000 | Loss: 0.00001691
Iteration 74/1000 | Loss: 0.00001691
Iteration 75/1000 | Loss: 0.00001691
Iteration 76/1000 | Loss: 0.00001691
Iteration 77/1000 | Loss: 0.00001691
Iteration 78/1000 | Loss: 0.00001691
Iteration 79/1000 | Loss: 0.00001691
Iteration 80/1000 | Loss: 0.00001691
Iteration 81/1000 | Loss: 0.00001691
Iteration 82/1000 | Loss: 0.00001691
Iteration 83/1000 | Loss: 0.00001691
Iteration 84/1000 | Loss: 0.00001691
Iteration 85/1000 | Loss: 0.00001691
Iteration 86/1000 | Loss: 0.00001691
Iteration 87/1000 | Loss: 0.00001690
Iteration 88/1000 | Loss: 0.00001690
Iteration 89/1000 | Loss: 0.00001690
Iteration 90/1000 | Loss: 0.00001690
Iteration 91/1000 | Loss: 0.00001690
Iteration 92/1000 | Loss: 0.00001690
Iteration 93/1000 | Loss: 0.00001690
Iteration 94/1000 | Loss: 0.00001690
Iteration 95/1000 | Loss: 0.00001690
Iteration 96/1000 | Loss: 0.00001690
Iteration 97/1000 | Loss: 0.00001689
Iteration 98/1000 | Loss: 0.00001689
Iteration 99/1000 | Loss: 0.00001689
Iteration 100/1000 | Loss: 0.00001689
Iteration 101/1000 | Loss: 0.00001689
Iteration 102/1000 | Loss: 0.00001689
Iteration 103/1000 | Loss: 0.00001688
Iteration 104/1000 | Loss: 0.00001688
Iteration 105/1000 | Loss: 0.00001688
Iteration 106/1000 | Loss: 0.00001688
Iteration 107/1000 | Loss: 0.00001688
Iteration 108/1000 | Loss: 0.00001688
Iteration 109/1000 | Loss: 0.00001688
Iteration 110/1000 | Loss: 0.00001687
Iteration 111/1000 | Loss: 0.00001687
Iteration 112/1000 | Loss: 0.00001687
Iteration 113/1000 | Loss: 0.00001687
Iteration 114/1000 | Loss: 0.00001687
Iteration 115/1000 | Loss: 0.00001687
Iteration 116/1000 | Loss: 0.00001687
Iteration 117/1000 | Loss: 0.00001687
Iteration 118/1000 | Loss: 0.00001687
Iteration 119/1000 | Loss: 0.00001687
Iteration 120/1000 | Loss: 0.00001687
Iteration 121/1000 | Loss: 0.00001687
Iteration 122/1000 | Loss: 0.00001687
Iteration 123/1000 | Loss: 0.00001687
Iteration 124/1000 | Loss: 0.00001687
Iteration 125/1000 | Loss: 0.00001687
Iteration 126/1000 | Loss: 0.00001687
Iteration 127/1000 | Loss: 0.00001687
Iteration 128/1000 | Loss: 0.00001687
Iteration 129/1000 | Loss: 0.00001686
Iteration 130/1000 | Loss: 0.00001686
Iteration 131/1000 | Loss: 0.00001686
Iteration 132/1000 | Loss: 0.00001686
Iteration 133/1000 | Loss: 0.00001686
Iteration 134/1000 | Loss: 0.00001686
Iteration 135/1000 | Loss: 0.00001686
Iteration 136/1000 | Loss: 0.00001686
Iteration 137/1000 | Loss: 0.00001686
Iteration 138/1000 | Loss: 0.00001686
Iteration 139/1000 | Loss: 0.00001686
Iteration 140/1000 | Loss: 0.00001686
Iteration 141/1000 | Loss: 0.00001686
Iteration 142/1000 | Loss: 0.00001686
Iteration 143/1000 | Loss: 0.00001686
Iteration 144/1000 | Loss: 0.00001686
Iteration 145/1000 | Loss: 0.00001686
Iteration 146/1000 | Loss: 0.00001686
Iteration 147/1000 | Loss: 0.00001685
Iteration 148/1000 | Loss: 0.00001685
Iteration 149/1000 | Loss: 0.00001685
Iteration 150/1000 | Loss: 0.00001685
Iteration 151/1000 | Loss: 0.00001685
Iteration 152/1000 | Loss: 0.00001685
Iteration 153/1000 | Loss: 0.00001685
Iteration 154/1000 | Loss: 0.00001685
Iteration 155/1000 | Loss: 0.00001685
Iteration 156/1000 | Loss: 0.00001685
Iteration 157/1000 | Loss: 0.00001685
Iteration 158/1000 | Loss: 0.00001685
Iteration 159/1000 | Loss: 0.00001685
Iteration 160/1000 | Loss: 0.00001685
Iteration 161/1000 | Loss: 0.00001685
Iteration 162/1000 | Loss: 0.00001685
Iteration 163/1000 | Loss: 0.00001685
Iteration 164/1000 | Loss: 0.00001685
Iteration 165/1000 | Loss: 0.00001685
Iteration 166/1000 | Loss: 0.00001685
Iteration 167/1000 | Loss: 0.00001685
Iteration 168/1000 | Loss: 0.00001685
Iteration 169/1000 | Loss: 0.00001685
Iteration 170/1000 | Loss: 0.00001685
Iteration 171/1000 | Loss: 0.00001685
Iteration 172/1000 | Loss: 0.00001685
Iteration 173/1000 | Loss: 0.00001685
Iteration 174/1000 | Loss: 0.00001685
Iteration 175/1000 | Loss: 0.00001685
Iteration 176/1000 | Loss: 0.00001685
Iteration 177/1000 | Loss: 0.00001685
Iteration 178/1000 | Loss: 0.00001685
Iteration 179/1000 | Loss: 0.00001685
Iteration 180/1000 | Loss: 0.00001685
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 180. Stopping optimization.
Last 5 losses: [1.6847327060531825e-05, 1.6847327060531825e-05, 1.6847327060531825e-05, 1.6847327060531825e-05, 1.6847327060531825e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6847327060531825e-05

Optimization complete. Final v2v error: 2.97786808013916 mm

Highest mean error: 12.716874122619629 mm for frame 212

Lowest mean error: 2.6879465579986572 mm for frame 4

Saving results

Total time: 122.38209271430969
