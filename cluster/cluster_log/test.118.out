Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=118, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 6608-6663
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_40_nl_6353/0006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_40_nl_6353/0006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_40_nl_6353/0006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01100117
Iteration 2/25 | Loss: 0.00230505
Iteration 3/25 | Loss: 0.00177419
Iteration 4/25 | Loss: 0.00161561
Iteration 5/25 | Loss: 0.00160571
Iteration 6/25 | Loss: 0.00160334
Iteration 7/25 | Loss: 0.00158404
Iteration 8/25 | Loss: 0.00151858
Iteration 9/25 | Loss: 0.00149664
Iteration 10/25 | Loss: 0.00149373
Iteration 11/25 | Loss: 0.00147197
Iteration 12/25 | Loss: 0.00146620
Iteration 13/25 | Loss: 0.00146069
Iteration 14/25 | Loss: 0.00146212
Iteration 15/25 | Loss: 0.00145197
Iteration 16/25 | Loss: 0.00145034
Iteration 17/25 | Loss: 0.00144949
Iteration 18/25 | Loss: 0.00145186
Iteration 19/25 | Loss: 0.00145064
Iteration 20/25 | Loss: 0.00144759
Iteration 21/25 | Loss: 0.00144966
Iteration 22/25 | Loss: 0.00145348
Iteration 23/25 | Loss: 0.00145112
Iteration 24/25 | Loss: 0.00145003
Iteration 25/25 | Loss: 0.00144815

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46278656
Iteration 2/25 | Loss: 0.00231171
Iteration 3/25 | Loss: 0.00231171
Iteration 4/25 | Loss: 0.00231171
Iteration 5/25 | Loss: 0.00231171
Iteration 6/25 | Loss: 0.00231171
Iteration 7/25 | Loss: 0.00231171
Iteration 8/25 | Loss: 0.00231171
Iteration 9/25 | Loss: 0.00231171
Iteration 10/25 | Loss: 0.00231171
Iteration 11/25 | Loss: 0.00231171
Iteration 12/25 | Loss: 0.00231171
Iteration 13/25 | Loss: 0.00231171
Iteration 14/25 | Loss: 0.00231171
Iteration 15/25 | Loss: 0.00231171
Iteration 16/25 | Loss: 0.00231171
Iteration 17/25 | Loss: 0.00231171
Iteration 18/25 | Loss: 0.00231171
Iteration 19/25 | Loss: 0.00231171
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0023117088712751865, 0.0023117088712751865, 0.0023117088712751865, 0.0023117088712751865, 0.0023117088712751865]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0023117088712751865

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00231171
Iteration 2/1000 | Loss: 0.00021684
Iteration 3/1000 | Loss: 0.00014938
Iteration 4/1000 | Loss: 0.00042654
Iteration 5/1000 | Loss: 0.00035859
Iteration 6/1000 | Loss: 0.00023585
Iteration 7/1000 | Loss: 0.00044928
Iteration 8/1000 | Loss: 0.00029830
Iteration 9/1000 | Loss: 0.00030494
Iteration 10/1000 | Loss: 0.00047752
Iteration 11/1000 | Loss: 0.00038111
Iteration 12/1000 | Loss: 0.00037599
Iteration 13/1000 | Loss: 0.00037348
Iteration 14/1000 | Loss: 0.00030459
Iteration 15/1000 | Loss: 0.00057320
Iteration 16/1000 | Loss: 0.00078544
Iteration 17/1000 | Loss: 0.00055853
Iteration 18/1000 | Loss: 0.00030008
Iteration 19/1000 | Loss: 0.00042055
Iteration 20/1000 | Loss: 0.00046699
Iteration 21/1000 | Loss: 0.00044077
Iteration 22/1000 | Loss: 0.00042807
Iteration 23/1000 | Loss: 0.00091756
Iteration 24/1000 | Loss: 0.00049897
Iteration 25/1000 | Loss: 0.00056313
Iteration 26/1000 | Loss: 0.00048909
Iteration 27/1000 | Loss: 0.00078322
Iteration 28/1000 | Loss: 0.00018653
Iteration 29/1000 | Loss: 0.00011948
Iteration 30/1000 | Loss: 0.00005000
Iteration 31/1000 | Loss: 0.00100365
Iteration 32/1000 | Loss: 0.00010930
Iteration 33/1000 | Loss: 0.00025599
Iteration 34/1000 | Loss: 0.00040133
Iteration 35/1000 | Loss: 0.00090238
Iteration 36/1000 | Loss: 0.00005638
Iteration 37/1000 | Loss: 0.00048855
Iteration 38/1000 | Loss: 0.00029892
Iteration 39/1000 | Loss: 0.00044869
Iteration 40/1000 | Loss: 0.00042141
Iteration 41/1000 | Loss: 0.00028610
Iteration 42/1000 | Loss: 0.00036993
Iteration 43/1000 | Loss: 0.00029207
Iteration 44/1000 | Loss: 0.00039670
Iteration 45/1000 | Loss: 0.00047957
Iteration 46/1000 | Loss: 0.00050819
Iteration 47/1000 | Loss: 0.00035913
Iteration 48/1000 | Loss: 0.00007060
Iteration 49/1000 | Loss: 0.00005399
Iteration 50/1000 | Loss: 0.00016933
Iteration 51/1000 | Loss: 0.00008337
Iteration 52/1000 | Loss: 0.00007951
Iteration 53/1000 | Loss: 0.00005381
Iteration 54/1000 | Loss: 0.00040572
Iteration 55/1000 | Loss: 0.00025438
Iteration 56/1000 | Loss: 0.00022264
Iteration 57/1000 | Loss: 0.00004165
Iteration 58/1000 | Loss: 0.00003875
Iteration 59/1000 | Loss: 0.00003667
Iteration 60/1000 | Loss: 0.00003498
Iteration 61/1000 | Loss: 0.00004899
Iteration 62/1000 | Loss: 0.00003961
Iteration 63/1000 | Loss: 0.00003614
Iteration 64/1000 | Loss: 0.00003376
Iteration 65/1000 | Loss: 0.00003257
Iteration 66/1000 | Loss: 0.00003224
Iteration 67/1000 | Loss: 0.00004356
Iteration 68/1000 | Loss: 0.00003418
Iteration 69/1000 | Loss: 0.00003269
Iteration 70/1000 | Loss: 0.00003191
Iteration 71/1000 | Loss: 0.00003128
Iteration 72/1000 | Loss: 0.00018823
Iteration 73/1000 | Loss: 0.00003684
Iteration 74/1000 | Loss: 0.00003479
Iteration 75/1000 | Loss: 0.00003311
Iteration 76/1000 | Loss: 0.00019520
Iteration 77/1000 | Loss: 0.00022549
Iteration 78/1000 | Loss: 0.00007188
Iteration 79/1000 | Loss: 0.00025204
Iteration 80/1000 | Loss: 0.00017461
Iteration 81/1000 | Loss: 0.00020498
Iteration 82/1000 | Loss: 0.00004451
Iteration 83/1000 | Loss: 0.00021591
Iteration 84/1000 | Loss: 0.00018117
Iteration 85/1000 | Loss: 0.00014610
Iteration 86/1000 | Loss: 0.00015478
Iteration 87/1000 | Loss: 0.00016614
Iteration 88/1000 | Loss: 0.00014231
Iteration 89/1000 | Loss: 0.00014669
Iteration 90/1000 | Loss: 0.00013149
Iteration 91/1000 | Loss: 0.00011053
Iteration 92/1000 | Loss: 0.00011374
Iteration 93/1000 | Loss: 0.00010303
Iteration 94/1000 | Loss: 0.00009493
Iteration 95/1000 | Loss: 0.00010265
Iteration 96/1000 | Loss: 0.00004625
Iteration 97/1000 | Loss: 0.00003636
Iteration 98/1000 | Loss: 0.00003489
Iteration 99/1000 | Loss: 0.00003406
Iteration 100/1000 | Loss: 0.00003344
Iteration 101/1000 | Loss: 0.00003313
Iteration 102/1000 | Loss: 0.00003293
Iteration 103/1000 | Loss: 0.00017724
Iteration 104/1000 | Loss: 0.00016448
Iteration 105/1000 | Loss: 0.00004707
Iteration 106/1000 | Loss: 0.00003892
Iteration 107/1000 | Loss: 0.00019860
Iteration 108/1000 | Loss: 0.00003909
Iteration 109/1000 | Loss: 0.00003716
Iteration 110/1000 | Loss: 0.00003625
Iteration 111/1000 | Loss: 0.00003537
Iteration 112/1000 | Loss: 0.00003467
Iteration 113/1000 | Loss: 0.00003405
Iteration 114/1000 | Loss: 0.00021903
Iteration 115/1000 | Loss: 0.00003896
Iteration 116/1000 | Loss: 0.00020563
Iteration 117/1000 | Loss: 0.00004315
Iteration 118/1000 | Loss: 0.00003690
Iteration 119/1000 | Loss: 0.00004500
Iteration 120/1000 | Loss: 0.00003362
Iteration 121/1000 | Loss: 0.00003260
Iteration 122/1000 | Loss: 0.00003206
Iteration 123/1000 | Loss: 0.00020904
Iteration 124/1000 | Loss: 0.00019946
Iteration 125/1000 | Loss: 0.00013300
Iteration 126/1000 | Loss: 0.00011157
Iteration 127/1000 | Loss: 0.00008577
Iteration 128/1000 | Loss: 0.00004124
Iteration 129/1000 | Loss: 0.00004329
Iteration 130/1000 | Loss: 0.00005372
Iteration 131/1000 | Loss: 0.00004162
Iteration 132/1000 | Loss: 0.00004919
Iteration 133/1000 | Loss: 0.00003920
Iteration 134/1000 | Loss: 0.00004446
Iteration 135/1000 | Loss: 0.00003962
Iteration 136/1000 | Loss: 0.00004611
Iteration 137/1000 | Loss: 0.00004204
Iteration 138/1000 | Loss: 0.00003387
Iteration 139/1000 | Loss: 0.00003326
Iteration 140/1000 | Loss: 0.00004751
Iteration 141/1000 | Loss: 0.00004262
Iteration 142/1000 | Loss: 0.00004872
Iteration 143/1000 | Loss: 0.00017658
Iteration 144/1000 | Loss: 0.00014438
Iteration 145/1000 | Loss: 0.00007262
Iteration 146/1000 | Loss: 0.00019506
Iteration 147/1000 | Loss: 0.00011084
Iteration 148/1000 | Loss: 0.00003459
Iteration 149/1000 | Loss: 0.00003362
Iteration 150/1000 | Loss: 0.00025680
Iteration 151/1000 | Loss: 0.00004244
Iteration 152/1000 | Loss: 0.00003716
Iteration 153/1000 | Loss: 0.00003526
Iteration 154/1000 | Loss: 0.00003339
Iteration 155/1000 | Loss: 0.00003255
Iteration 156/1000 | Loss: 0.00003205
Iteration 157/1000 | Loss: 0.00003171
Iteration 158/1000 | Loss: 0.00003159
Iteration 159/1000 | Loss: 0.00018208
Iteration 160/1000 | Loss: 0.00016679
Iteration 161/1000 | Loss: 0.00006621
Iteration 162/1000 | Loss: 0.00013499
Iteration 163/1000 | Loss: 0.00005771
Iteration 164/1000 | Loss: 0.00009677
Iteration 165/1000 | Loss: 0.00006523
Iteration 166/1000 | Loss: 0.00003390
Iteration 167/1000 | Loss: 0.00013941
Iteration 168/1000 | Loss: 0.00003499
Iteration 169/1000 | Loss: 0.00015475
Iteration 170/1000 | Loss: 0.00012198
Iteration 171/1000 | Loss: 0.00012876
Iteration 172/1000 | Loss: 0.00004134
Iteration 173/1000 | Loss: 0.00012481
Iteration 174/1000 | Loss: 0.00004715
Iteration 175/1000 | Loss: 0.00003727
Iteration 176/1000 | Loss: 0.00004455
Iteration 177/1000 | Loss: 0.00004399
Iteration 178/1000 | Loss: 0.00003972
Iteration 179/1000 | Loss: 0.00005355
Iteration 180/1000 | Loss: 0.00003668
Iteration 181/1000 | Loss: 0.00003433
Iteration 182/1000 | Loss: 0.00003276
Iteration 183/1000 | Loss: 0.00003193
Iteration 184/1000 | Loss: 0.00003153
Iteration 185/1000 | Loss: 0.00003115
Iteration 186/1000 | Loss: 0.00003077
Iteration 187/1000 | Loss: 0.00003053
Iteration 188/1000 | Loss: 0.00003027
Iteration 189/1000 | Loss: 0.00003916
Iteration 190/1000 | Loss: 0.00003171
Iteration 191/1000 | Loss: 0.00003106
Iteration 192/1000 | Loss: 0.00003074
Iteration 193/1000 | Loss: 0.00003064
Iteration 194/1000 | Loss: 0.00003061
Iteration 195/1000 | Loss: 0.00003060
Iteration 196/1000 | Loss: 0.00003059
Iteration 197/1000 | Loss: 0.00003048
Iteration 198/1000 | Loss: 0.00003047
Iteration 199/1000 | Loss: 0.00003047
Iteration 200/1000 | Loss: 0.00003047
Iteration 201/1000 | Loss: 0.00003043
Iteration 202/1000 | Loss: 0.00003042
Iteration 203/1000 | Loss: 0.00003041
Iteration 204/1000 | Loss: 0.00003033
Iteration 205/1000 | Loss: 0.00003031
Iteration 206/1000 | Loss: 0.00003030
Iteration 207/1000 | Loss: 0.00003029
Iteration 208/1000 | Loss: 0.00003029
Iteration 209/1000 | Loss: 0.00002994
Iteration 210/1000 | Loss: 0.00002958
Iteration 211/1000 | Loss: 0.00002941
Iteration 212/1000 | Loss: 0.00002935
Iteration 213/1000 | Loss: 0.00002930
Iteration 214/1000 | Loss: 0.00002930
Iteration 215/1000 | Loss: 0.00002928
Iteration 216/1000 | Loss: 0.00002928
Iteration 217/1000 | Loss: 0.00002927
Iteration 218/1000 | Loss: 0.00002926
Iteration 219/1000 | Loss: 0.00002926
Iteration 220/1000 | Loss: 0.00002926
Iteration 221/1000 | Loss: 0.00002925
Iteration 222/1000 | Loss: 0.00002925
Iteration 223/1000 | Loss: 0.00002924
Iteration 224/1000 | Loss: 0.00002924
Iteration 225/1000 | Loss: 0.00002923
Iteration 226/1000 | Loss: 0.00002922
Iteration 227/1000 | Loss: 0.00002922
Iteration 228/1000 | Loss: 0.00002922
Iteration 229/1000 | Loss: 0.00002922
Iteration 230/1000 | Loss: 0.00002921
Iteration 231/1000 | Loss: 0.00002921
Iteration 232/1000 | Loss: 0.00002921
Iteration 233/1000 | Loss: 0.00002920
Iteration 234/1000 | Loss: 0.00002920
Iteration 235/1000 | Loss: 0.00002919
Iteration 236/1000 | Loss: 0.00002919
Iteration 237/1000 | Loss: 0.00002917
Iteration 238/1000 | Loss: 0.00002917
Iteration 239/1000 | Loss: 0.00002917
Iteration 240/1000 | Loss: 0.00002917
Iteration 241/1000 | Loss: 0.00002917
Iteration 242/1000 | Loss: 0.00002916
Iteration 243/1000 | Loss: 0.00002916
Iteration 244/1000 | Loss: 0.00002915
Iteration 245/1000 | Loss: 0.00002915
Iteration 246/1000 | Loss: 0.00002915
Iteration 247/1000 | Loss: 0.00002914
Iteration 248/1000 | Loss: 0.00002914
Iteration 249/1000 | Loss: 0.00002913
Iteration 250/1000 | Loss: 0.00002913
Iteration 251/1000 | Loss: 0.00002913
Iteration 252/1000 | Loss: 0.00002913
Iteration 253/1000 | Loss: 0.00002913
Iteration 254/1000 | Loss: 0.00002913
Iteration 255/1000 | Loss: 0.00002913
Iteration 256/1000 | Loss: 0.00002913
Iteration 257/1000 | Loss: 0.00002912
Iteration 258/1000 | Loss: 0.00002912
Iteration 259/1000 | Loss: 0.00002911
Iteration 260/1000 | Loss: 0.00002911
Iteration 261/1000 | Loss: 0.00002911
Iteration 262/1000 | Loss: 0.00002911
Iteration 263/1000 | Loss: 0.00002911
Iteration 264/1000 | Loss: 0.00002910
Iteration 265/1000 | Loss: 0.00002910
Iteration 266/1000 | Loss: 0.00002910
Iteration 267/1000 | Loss: 0.00002910
Iteration 268/1000 | Loss: 0.00002910
Iteration 269/1000 | Loss: 0.00002910
Iteration 270/1000 | Loss: 0.00002910
Iteration 271/1000 | Loss: 0.00002910
Iteration 272/1000 | Loss: 0.00002909
Iteration 273/1000 | Loss: 0.00002909
Iteration 274/1000 | Loss: 0.00002909
Iteration 275/1000 | Loss: 0.00002909
Iteration 276/1000 | Loss: 0.00002909
Iteration 277/1000 | Loss: 0.00002909
Iteration 278/1000 | Loss: 0.00002908
Iteration 279/1000 | Loss: 0.00002908
Iteration 280/1000 | Loss: 0.00002908
Iteration 281/1000 | Loss: 0.00002908
Iteration 282/1000 | Loss: 0.00002908
Iteration 283/1000 | Loss: 0.00002908
Iteration 284/1000 | Loss: 0.00002908
Iteration 285/1000 | Loss: 0.00002908
Iteration 286/1000 | Loss: 0.00002908
Iteration 287/1000 | Loss: 0.00002908
Iteration 288/1000 | Loss: 0.00002908
Iteration 289/1000 | Loss: 0.00002908
Iteration 290/1000 | Loss: 0.00002908
Iteration 291/1000 | Loss: 0.00002908
Iteration 292/1000 | Loss: 0.00002908
Iteration 293/1000 | Loss: 0.00002908
Iteration 294/1000 | Loss: 0.00002907
Iteration 295/1000 | Loss: 0.00002907
Iteration 296/1000 | Loss: 0.00002907
Iteration 297/1000 | Loss: 0.00002907
Iteration 298/1000 | Loss: 0.00002907
Iteration 299/1000 | Loss: 0.00002907
Iteration 300/1000 | Loss: 0.00002907
Iteration 301/1000 | Loss: 0.00002907
Iteration 302/1000 | Loss: 0.00002907
Iteration 303/1000 | Loss: 0.00002907
Iteration 304/1000 | Loss: 0.00002907
Iteration 305/1000 | Loss: 0.00002907
Iteration 306/1000 | Loss: 0.00002907
Iteration 307/1000 | Loss: 0.00002907
Iteration 308/1000 | Loss: 0.00002907
Iteration 309/1000 | Loss: 0.00002907
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 309. Stopping optimization.
Last 5 losses: [2.9072616598568857e-05, 2.9072616598568857e-05, 2.9072616598568857e-05, 2.9072616598568857e-05, 2.9072616598568857e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.9072616598568857e-05

Optimization complete. Final v2v error: 4.509238243103027 mm

Highest mean error: 13.899303436279297 mm for frame 222

Lowest mean error: 3.886600971221924 mm for frame 257

Saving results

Total time: 387.7469358444214
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_40_nl_6353/0024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_40_nl_6353/0024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_40_nl_6353/0024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00946624
Iteration 2/25 | Loss: 0.00149437
Iteration 3/25 | Loss: 0.00138057
Iteration 4/25 | Loss: 0.00136814
Iteration 5/25 | Loss: 0.00136442
Iteration 6/25 | Loss: 0.00136307
Iteration 7/25 | Loss: 0.00136307
Iteration 8/25 | Loss: 0.00136307
Iteration 9/25 | Loss: 0.00136307
Iteration 10/25 | Loss: 0.00136307
Iteration 11/25 | Loss: 0.00136307
Iteration 12/25 | Loss: 0.00136307
Iteration 13/25 | Loss: 0.00136307
Iteration 14/25 | Loss: 0.00136307
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0013630674220621586, 0.0013630674220621586, 0.0013630674220621586, 0.0013630674220621586, 0.0013630674220621586]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013630674220621586

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.52234221
Iteration 2/25 | Loss: 0.00158155
Iteration 3/25 | Loss: 0.00158155
Iteration 4/25 | Loss: 0.00158155
Iteration 5/25 | Loss: 0.00158155
Iteration 6/25 | Loss: 0.00158155
Iteration 7/25 | Loss: 0.00158155
Iteration 8/25 | Loss: 0.00158155
Iteration 9/25 | Loss: 0.00158155
Iteration 10/25 | Loss: 0.00158155
Iteration 11/25 | Loss: 0.00158155
Iteration 12/25 | Loss: 0.00158155
Iteration 13/25 | Loss: 0.00158155
Iteration 14/25 | Loss: 0.00158155
Iteration 15/25 | Loss: 0.00158155
Iteration 16/25 | Loss: 0.00158155
Iteration 17/25 | Loss: 0.00158155
Iteration 18/25 | Loss: 0.00158155
Iteration 19/25 | Loss: 0.00158155
Iteration 20/25 | Loss: 0.00158155
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0015815476654097438, 0.0015815476654097438, 0.0015815476654097438, 0.0015815476654097438, 0.0015815476654097438]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015815476654097438

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00158155
Iteration 2/1000 | Loss: 0.00003748
Iteration 3/1000 | Loss: 0.00002904
Iteration 4/1000 | Loss: 0.00002623
Iteration 5/1000 | Loss: 0.00002517
Iteration 6/1000 | Loss: 0.00002401
Iteration 7/1000 | Loss: 0.00002338
Iteration 8/1000 | Loss: 0.00002301
Iteration 9/1000 | Loss: 0.00002266
Iteration 10/1000 | Loss: 0.00002246
Iteration 11/1000 | Loss: 0.00002240
Iteration 12/1000 | Loss: 0.00002239
Iteration 13/1000 | Loss: 0.00002238
Iteration 14/1000 | Loss: 0.00002238
Iteration 15/1000 | Loss: 0.00002231
Iteration 16/1000 | Loss: 0.00002231
Iteration 17/1000 | Loss: 0.00002230
Iteration 18/1000 | Loss: 0.00002229
Iteration 19/1000 | Loss: 0.00002226
Iteration 20/1000 | Loss: 0.00002225
Iteration 21/1000 | Loss: 0.00002225
Iteration 22/1000 | Loss: 0.00002224
Iteration 23/1000 | Loss: 0.00002224
Iteration 24/1000 | Loss: 0.00002222
Iteration 25/1000 | Loss: 0.00002221
Iteration 26/1000 | Loss: 0.00002220
Iteration 27/1000 | Loss: 0.00002219
Iteration 28/1000 | Loss: 0.00002219
Iteration 29/1000 | Loss: 0.00002218
Iteration 30/1000 | Loss: 0.00002217
Iteration 31/1000 | Loss: 0.00002217
Iteration 32/1000 | Loss: 0.00002216
Iteration 33/1000 | Loss: 0.00002216
Iteration 34/1000 | Loss: 0.00002215
Iteration 35/1000 | Loss: 0.00002215
Iteration 36/1000 | Loss: 0.00002215
Iteration 37/1000 | Loss: 0.00002215
Iteration 38/1000 | Loss: 0.00002215
Iteration 39/1000 | Loss: 0.00002215
Iteration 40/1000 | Loss: 0.00002215
Iteration 41/1000 | Loss: 0.00002215
Iteration 42/1000 | Loss: 0.00002215
Iteration 43/1000 | Loss: 0.00002215
Iteration 44/1000 | Loss: 0.00002214
Iteration 45/1000 | Loss: 0.00002214
Iteration 46/1000 | Loss: 0.00002214
Iteration 47/1000 | Loss: 0.00002214
Iteration 48/1000 | Loss: 0.00002214
Iteration 49/1000 | Loss: 0.00002214
Iteration 50/1000 | Loss: 0.00002214
Iteration 51/1000 | Loss: 0.00002214
Iteration 52/1000 | Loss: 0.00002214
Iteration 53/1000 | Loss: 0.00002214
Iteration 54/1000 | Loss: 0.00002213
Iteration 55/1000 | Loss: 0.00002213
Iteration 56/1000 | Loss: 0.00002213
Iteration 57/1000 | Loss: 0.00002213
Iteration 58/1000 | Loss: 0.00002213
Iteration 59/1000 | Loss: 0.00002213
Iteration 60/1000 | Loss: 0.00002213
Iteration 61/1000 | Loss: 0.00002213
Iteration 62/1000 | Loss: 0.00002212
Iteration 63/1000 | Loss: 0.00002212
Iteration 64/1000 | Loss: 0.00002211
Iteration 65/1000 | Loss: 0.00002211
Iteration 66/1000 | Loss: 0.00002211
Iteration 67/1000 | Loss: 0.00002210
Iteration 68/1000 | Loss: 0.00002210
Iteration 69/1000 | Loss: 0.00002210
Iteration 70/1000 | Loss: 0.00002209
Iteration 71/1000 | Loss: 0.00002209
Iteration 72/1000 | Loss: 0.00002209
Iteration 73/1000 | Loss: 0.00002209
Iteration 74/1000 | Loss: 0.00002209
Iteration 75/1000 | Loss: 0.00002209
Iteration 76/1000 | Loss: 0.00002208
Iteration 77/1000 | Loss: 0.00002207
Iteration 78/1000 | Loss: 0.00002207
Iteration 79/1000 | Loss: 0.00002206
Iteration 80/1000 | Loss: 0.00002206
Iteration 81/1000 | Loss: 0.00002206
Iteration 82/1000 | Loss: 0.00002206
Iteration 83/1000 | Loss: 0.00002206
Iteration 84/1000 | Loss: 0.00002206
Iteration 85/1000 | Loss: 0.00002206
Iteration 86/1000 | Loss: 0.00002206
Iteration 87/1000 | Loss: 0.00002206
Iteration 88/1000 | Loss: 0.00002206
Iteration 89/1000 | Loss: 0.00002206
Iteration 90/1000 | Loss: 0.00002205
Iteration 91/1000 | Loss: 0.00002205
Iteration 92/1000 | Loss: 0.00002205
Iteration 93/1000 | Loss: 0.00002205
Iteration 94/1000 | Loss: 0.00002205
Iteration 95/1000 | Loss: 0.00002204
Iteration 96/1000 | Loss: 0.00002204
Iteration 97/1000 | Loss: 0.00002204
Iteration 98/1000 | Loss: 0.00002204
Iteration 99/1000 | Loss: 0.00002204
Iteration 100/1000 | Loss: 0.00002204
Iteration 101/1000 | Loss: 0.00002204
Iteration 102/1000 | Loss: 0.00002203
Iteration 103/1000 | Loss: 0.00002203
Iteration 104/1000 | Loss: 0.00002203
Iteration 105/1000 | Loss: 0.00002203
Iteration 106/1000 | Loss: 0.00002203
Iteration 107/1000 | Loss: 0.00002203
Iteration 108/1000 | Loss: 0.00002203
Iteration 109/1000 | Loss: 0.00002203
Iteration 110/1000 | Loss: 0.00002203
Iteration 111/1000 | Loss: 0.00002203
Iteration 112/1000 | Loss: 0.00002202
Iteration 113/1000 | Loss: 0.00002202
Iteration 114/1000 | Loss: 0.00002202
Iteration 115/1000 | Loss: 0.00002202
Iteration 116/1000 | Loss: 0.00002202
Iteration 117/1000 | Loss: 0.00002202
Iteration 118/1000 | Loss: 0.00002202
Iteration 119/1000 | Loss: 0.00002202
Iteration 120/1000 | Loss: 0.00002202
Iteration 121/1000 | Loss: 0.00002202
Iteration 122/1000 | Loss: 0.00002202
Iteration 123/1000 | Loss: 0.00002202
Iteration 124/1000 | Loss: 0.00002201
Iteration 125/1000 | Loss: 0.00002201
Iteration 126/1000 | Loss: 0.00002201
Iteration 127/1000 | Loss: 0.00002201
Iteration 128/1000 | Loss: 0.00002201
Iteration 129/1000 | Loss: 0.00002201
Iteration 130/1000 | Loss: 0.00002201
Iteration 131/1000 | Loss: 0.00002201
Iteration 132/1000 | Loss: 0.00002201
Iteration 133/1000 | Loss: 0.00002201
Iteration 134/1000 | Loss: 0.00002201
Iteration 135/1000 | Loss: 0.00002201
Iteration 136/1000 | Loss: 0.00002201
Iteration 137/1000 | Loss: 0.00002201
Iteration 138/1000 | Loss: 0.00002200
Iteration 139/1000 | Loss: 0.00002200
Iteration 140/1000 | Loss: 0.00002200
Iteration 141/1000 | Loss: 0.00002200
Iteration 142/1000 | Loss: 0.00002200
Iteration 143/1000 | Loss: 0.00002200
Iteration 144/1000 | Loss: 0.00002200
Iteration 145/1000 | Loss: 0.00002200
Iteration 146/1000 | Loss: 0.00002200
Iteration 147/1000 | Loss: 0.00002200
Iteration 148/1000 | Loss: 0.00002200
Iteration 149/1000 | Loss: 0.00002200
Iteration 150/1000 | Loss: 0.00002200
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 150. Stopping optimization.
Last 5 losses: [2.2002586774760857e-05, 2.2002586774760857e-05, 2.2002586774760857e-05, 2.2002586774760857e-05, 2.2002586774760857e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2002586774760857e-05

Optimization complete. Final v2v error: 3.9973487854003906 mm

Highest mean error: 4.428544044494629 mm for frame 66

Lowest mean error: 3.614471435546875 mm for frame 10

Saving results

Total time: 34.371132135391235
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_40_nl_6353/0002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_40_nl_6353/0002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_40_nl_6353/0002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01149453
Iteration 2/25 | Loss: 0.00283331
Iteration 3/25 | Loss: 0.00218733
Iteration 4/25 | Loss: 0.00209220
Iteration 5/25 | Loss: 0.00192508
Iteration 6/25 | Loss: 0.00193591
Iteration 7/25 | Loss: 0.00169181
Iteration 8/25 | Loss: 0.00164493
Iteration 9/25 | Loss: 0.00163613
Iteration 10/25 | Loss: 0.00162418
Iteration 11/25 | Loss: 0.00162421
Iteration 12/25 | Loss: 0.00161740
Iteration 13/25 | Loss: 0.00161825
Iteration 14/25 | Loss: 0.00161265
Iteration 15/25 | Loss: 0.00160340
Iteration 16/25 | Loss: 0.00159873
Iteration 17/25 | Loss: 0.00160285
Iteration 18/25 | Loss: 0.00159920
Iteration 19/25 | Loss: 0.00159989
Iteration 20/25 | Loss: 0.00159818
Iteration 21/25 | Loss: 0.00159240
Iteration 22/25 | Loss: 0.00159652
Iteration 23/25 | Loss: 0.00159225
Iteration 24/25 | Loss: 0.00160062
Iteration 25/25 | Loss: 0.00159443

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.56595242
Iteration 2/25 | Loss: 0.00355784
Iteration 3/25 | Loss: 0.00343486
Iteration 4/25 | Loss: 0.00343486
Iteration 5/25 | Loss: 0.00343486
Iteration 6/25 | Loss: 0.00343486
Iteration 7/25 | Loss: 0.00343486
Iteration 8/25 | Loss: 0.00343486
Iteration 9/25 | Loss: 0.00343486
Iteration 10/25 | Loss: 0.00343486
Iteration 11/25 | Loss: 0.00343486
Iteration 12/25 | Loss: 0.00343486
Iteration 13/25 | Loss: 0.00343486
Iteration 14/25 | Loss: 0.00343486
Iteration 15/25 | Loss: 0.00343486
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.003434860147535801, 0.003434860147535801, 0.003434860147535801, 0.003434860147535801, 0.003434860147535801]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.003434860147535801

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00343486
Iteration 2/1000 | Loss: 0.00039952
Iteration 3/1000 | Loss: 0.00115686
Iteration 4/1000 | Loss: 0.00059578
Iteration 5/1000 | Loss: 0.00038045
Iteration 6/1000 | Loss: 0.00085618
Iteration 7/1000 | Loss: 0.00038981
Iteration 8/1000 | Loss: 0.00092484
Iteration 9/1000 | Loss: 0.00198575
Iteration 10/1000 | Loss: 0.00077110
Iteration 11/1000 | Loss: 0.00276788
Iteration 12/1000 | Loss: 0.00250264
Iteration 13/1000 | Loss: 0.00129122
Iteration 14/1000 | Loss: 0.00066724
Iteration 15/1000 | Loss: 0.00047963
Iteration 16/1000 | Loss: 0.00030596
Iteration 17/1000 | Loss: 0.00078135
Iteration 18/1000 | Loss: 0.00049195
Iteration 19/1000 | Loss: 0.00046024
Iteration 20/1000 | Loss: 0.00042947
Iteration 21/1000 | Loss: 0.00042359
Iteration 22/1000 | Loss: 0.00024668
Iteration 23/1000 | Loss: 0.00016360
Iteration 24/1000 | Loss: 0.00021375
Iteration 25/1000 | Loss: 0.00027320
Iteration 26/1000 | Loss: 0.00024015
Iteration 27/1000 | Loss: 0.00022556
Iteration 28/1000 | Loss: 0.00031815
Iteration 29/1000 | Loss: 0.00046499
Iteration 30/1000 | Loss: 0.00098215
Iteration 31/1000 | Loss: 0.00030016
Iteration 32/1000 | Loss: 0.00030635
Iteration 33/1000 | Loss: 0.00018285
Iteration 34/1000 | Loss: 0.00021490
Iteration 35/1000 | Loss: 0.00064706
Iteration 36/1000 | Loss: 0.00077663
Iteration 37/1000 | Loss: 0.00028966
Iteration 38/1000 | Loss: 0.00040927
Iteration 39/1000 | Loss: 0.00025353
Iteration 40/1000 | Loss: 0.00028421
Iteration 41/1000 | Loss: 0.00016879
Iteration 42/1000 | Loss: 0.00017971
Iteration 43/1000 | Loss: 0.00026896
Iteration 44/1000 | Loss: 0.00026221
Iteration 45/1000 | Loss: 0.00029003
Iteration 46/1000 | Loss: 0.00032301
Iteration 47/1000 | Loss: 0.00032262
Iteration 48/1000 | Loss: 0.00019498
Iteration 49/1000 | Loss: 0.00030651
Iteration 50/1000 | Loss: 0.00023218
Iteration 51/1000 | Loss: 0.00024830
Iteration 52/1000 | Loss: 0.00026484
Iteration 53/1000 | Loss: 0.00034176
Iteration 54/1000 | Loss: 0.00033646
Iteration 55/1000 | Loss: 0.00030519
Iteration 56/1000 | Loss: 0.00067364
Iteration 57/1000 | Loss: 0.00048788
Iteration 58/1000 | Loss: 0.00029408
Iteration 59/1000 | Loss: 0.00071062
Iteration 60/1000 | Loss: 0.00025154
Iteration 61/1000 | Loss: 0.00019825
Iteration 62/1000 | Loss: 0.00018831
Iteration 63/1000 | Loss: 0.00033500
Iteration 64/1000 | Loss: 0.00028806
Iteration 65/1000 | Loss: 0.00011425
Iteration 66/1000 | Loss: 0.00194733
Iteration 67/1000 | Loss: 0.00042748
Iteration 68/1000 | Loss: 0.00059465
Iteration 69/1000 | Loss: 0.00039855
Iteration 70/1000 | Loss: 0.00070098
Iteration 71/1000 | Loss: 0.00156631
Iteration 72/1000 | Loss: 0.00017910
Iteration 73/1000 | Loss: 0.00008524
Iteration 74/1000 | Loss: 0.00007247
Iteration 75/1000 | Loss: 0.00019038
Iteration 76/1000 | Loss: 0.00021268
Iteration 77/1000 | Loss: 0.00023758
Iteration 78/1000 | Loss: 0.00013728
Iteration 79/1000 | Loss: 0.00056718
Iteration 80/1000 | Loss: 0.00021819
Iteration 81/1000 | Loss: 0.00022326
Iteration 82/1000 | Loss: 0.00018255
Iteration 83/1000 | Loss: 0.00090477
Iteration 84/1000 | Loss: 0.00027767
Iteration 85/1000 | Loss: 0.00006796
Iteration 86/1000 | Loss: 0.00006161
Iteration 87/1000 | Loss: 0.00005753
Iteration 88/1000 | Loss: 0.00084434
Iteration 89/1000 | Loss: 0.00037941
Iteration 90/1000 | Loss: 0.00078113
Iteration 91/1000 | Loss: 0.00080883
Iteration 92/1000 | Loss: 0.00080400
Iteration 93/1000 | Loss: 0.00036701
Iteration 94/1000 | Loss: 0.00064962
Iteration 95/1000 | Loss: 0.00077130
Iteration 96/1000 | Loss: 0.00042471
Iteration 97/1000 | Loss: 0.00007638
Iteration 98/1000 | Loss: 0.00005707
Iteration 99/1000 | Loss: 0.00004873
Iteration 100/1000 | Loss: 0.00004591
Iteration 101/1000 | Loss: 0.00004306
Iteration 102/1000 | Loss: 0.00068211
Iteration 103/1000 | Loss: 0.00004506
Iteration 104/1000 | Loss: 0.00004145
Iteration 105/1000 | Loss: 0.00003933
Iteration 106/1000 | Loss: 0.00003719
Iteration 107/1000 | Loss: 0.00003622
Iteration 108/1000 | Loss: 0.00004851
Iteration 109/1000 | Loss: 0.00003713
Iteration 110/1000 | Loss: 0.00003519
Iteration 111/1000 | Loss: 0.00004833
Iteration 112/1000 | Loss: 0.00003453
Iteration 113/1000 | Loss: 0.00004529
Iteration 114/1000 | Loss: 0.00003532
Iteration 115/1000 | Loss: 0.00003372
Iteration 116/1000 | Loss: 0.00003358
Iteration 117/1000 | Loss: 0.00003355
Iteration 118/1000 | Loss: 0.00003352
Iteration 119/1000 | Loss: 0.00003351
Iteration 120/1000 | Loss: 0.00003350
Iteration 121/1000 | Loss: 0.00003350
Iteration 122/1000 | Loss: 0.00003350
Iteration 123/1000 | Loss: 0.00003349
Iteration 124/1000 | Loss: 0.00003348
Iteration 125/1000 | Loss: 0.00003346
Iteration 126/1000 | Loss: 0.00003345
Iteration 127/1000 | Loss: 0.00003342
Iteration 128/1000 | Loss: 0.00003342
Iteration 129/1000 | Loss: 0.00003341
Iteration 130/1000 | Loss: 0.00003341
Iteration 131/1000 | Loss: 0.00003341
Iteration 132/1000 | Loss: 0.00003340
Iteration 133/1000 | Loss: 0.00003340
Iteration 134/1000 | Loss: 0.00003339
Iteration 135/1000 | Loss: 0.00003337
Iteration 136/1000 | Loss: 0.00003337
Iteration 137/1000 | Loss: 0.00003337
Iteration 138/1000 | Loss: 0.00003337
Iteration 139/1000 | Loss: 0.00003336
Iteration 140/1000 | Loss: 0.00003335
Iteration 141/1000 | Loss: 0.00003334
Iteration 142/1000 | Loss: 0.00003333
Iteration 143/1000 | Loss: 0.00003333
Iteration 144/1000 | Loss: 0.00003333
Iteration 145/1000 | Loss: 0.00003332
Iteration 146/1000 | Loss: 0.00003332
Iteration 147/1000 | Loss: 0.00003331
Iteration 148/1000 | Loss: 0.00003331
Iteration 149/1000 | Loss: 0.00003330
Iteration 150/1000 | Loss: 0.00003330
Iteration 151/1000 | Loss: 0.00003329
Iteration 152/1000 | Loss: 0.00003329
Iteration 153/1000 | Loss: 0.00003329
Iteration 154/1000 | Loss: 0.00003329
Iteration 155/1000 | Loss: 0.00003327
Iteration 156/1000 | Loss: 0.00003325
Iteration 157/1000 | Loss: 0.00003325
Iteration 158/1000 | Loss: 0.00003325
Iteration 159/1000 | Loss: 0.00003324
Iteration 160/1000 | Loss: 0.00003324
Iteration 161/1000 | Loss: 0.00003324
Iteration 162/1000 | Loss: 0.00003324
Iteration 163/1000 | Loss: 0.00003324
Iteration 164/1000 | Loss: 0.00004987
Iteration 165/1000 | Loss: 0.00004987
Iteration 166/1000 | Loss: 0.00004008
Iteration 167/1000 | Loss: 0.00003404
Iteration 168/1000 | Loss: 0.00003321
Iteration 169/1000 | Loss: 0.00003314
Iteration 170/1000 | Loss: 0.00003314
Iteration 171/1000 | Loss: 0.00003313
Iteration 172/1000 | Loss: 0.00003313
Iteration 173/1000 | Loss: 0.00003313
Iteration 174/1000 | Loss: 0.00003312
Iteration 175/1000 | Loss: 0.00003312
Iteration 176/1000 | Loss: 0.00003311
Iteration 177/1000 | Loss: 0.00003311
Iteration 178/1000 | Loss: 0.00003311
Iteration 179/1000 | Loss: 0.00003311
Iteration 180/1000 | Loss: 0.00003311
Iteration 181/1000 | Loss: 0.00003311
Iteration 182/1000 | Loss: 0.00003311
Iteration 183/1000 | Loss: 0.00003311
Iteration 184/1000 | Loss: 0.00003311
Iteration 185/1000 | Loss: 0.00003311
Iteration 186/1000 | Loss: 0.00003311
Iteration 187/1000 | Loss: 0.00003311
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 187. Stopping optimization.
Last 5 losses: [3.3114756661234424e-05, 3.3114756661234424e-05, 3.3114756661234424e-05, 3.3114756661234424e-05, 3.3114756661234424e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.3114756661234424e-05

Optimization complete. Final v2v error: 4.915841579437256 mm

Highest mean error: 5.944364547729492 mm for frame 4

Lowest mean error: 4.332418918609619 mm for frame 212

Saving results

Total time: 249.90802597999573
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_40_nl_6353/0005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_40_nl_6353/0005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_40_nl_6353/0005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01177845
Iteration 2/25 | Loss: 0.00339961
Iteration 3/25 | Loss: 0.00255046
Iteration 4/25 | Loss: 0.00231097
Iteration 5/25 | Loss: 0.00207102
Iteration 6/25 | Loss: 0.00185559
Iteration 7/25 | Loss: 0.00171440
Iteration 8/25 | Loss: 0.00158014
Iteration 9/25 | Loss: 0.00155363
Iteration 10/25 | Loss: 0.00149680
Iteration 11/25 | Loss: 0.00144775
Iteration 12/25 | Loss: 0.00141000
Iteration 13/25 | Loss: 0.00137615
Iteration 14/25 | Loss: 0.00136750
Iteration 15/25 | Loss: 0.00135947
Iteration 16/25 | Loss: 0.00133919
Iteration 17/25 | Loss: 0.00133296
Iteration 18/25 | Loss: 0.00132961
Iteration 19/25 | Loss: 0.00133970
Iteration 20/25 | Loss: 0.00133501
Iteration 21/25 | Loss: 0.00133182
Iteration 22/25 | Loss: 0.00133009
Iteration 23/25 | Loss: 0.00132428
Iteration 24/25 | Loss: 0.00132026
Iteration 25/25 | Loss: 0.00131957

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.63307649
Iteration 2/25 | Loss: 0.00126500
Iteration 3/25 | Loss: 0.00126500
Iteration 4/25 | Loss: 0.00126499
Iteration 5/25 | Loss: 0.00126499
Iteration 6/25 | Loss: 0.00126499
Iteration 7/25 | Loss: 0.00126499
Iteration 8/25 | Loss: 0.00126499
Iteration 9/25 | Loss: 0.00126499
Iteration 10/25 | Loss: 0.00126499
Iteration 11/25 | Loss: 0.00126499
Iteration 12/25 | Loss: 0.00126499
Iteration 13/25 | Loss: 0.00126499
Iteration 14/25 | Loss: 0.00126499
Iteration 15/25 | Loss: 0.00126499
Iteration 16/25 | Loss: 0.00126499
Iteration 17/25 | Loss: 0.00126499
Iteration 18/25 | Loss: 0.00126499
Iteration 19/25 | Loss: 0.00126499
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0012649941490963101, 0.0012649941490963101, 0.0012649941490963101, 0.0012649941490963101, 0.0012649941490963101]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012649941490963101

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00126499
Iteration 2/1000 | Loss: 0.00011402
Iteration 3/1000 | Loss: 0.00008324
Iteration 4/1000 | Loss: 0.00007251
Iteration 5/1000 | Loss: 0.00006674
Iteration 6/1000 | Loss: 0.00006385
Iteration 7/1000 | Loss: 0.00006173
Iteration 8/1000 | Loss: 0.00006055
Iteration 9/1000 | Loss: 0.00036194
Iteration 10/1000 | Loss: 0.00018020
Iteration 11/1000 | Loss: 0.00006529
Iteration 12/1000 | Loss: 0.00006024
Iteration 13/1000 | Loss: 0.00005567
Iteration 14/1000 | Loss: 0.00005372
Iteration 15/1000 | Loss: 0.00005309
Iteration 16/1000 | Loss: 0.00005258
Iteration 17/1000 | Loss: 0.00005221
Iteration 18/1000 | Loss: 0.00005208
Iteration 19/1000 | Loss: 0.00005205
Iteration 20/1000 | Loss: 0.00005205
Iteration 21/1000 | Loss: 0.00005205
Iteration 22/1000 | Loss: 0.00005204
Iteration 23/1000 | Loss: 0.00005204
Iteration 24/1000 | Loss: 0.00005193
Iteration 25/1000 | Loss: 0.00005191
Iteration 26/1000 | Loss: 0.00005190
Iteration 27/1000 | Loss: 0.00005189
Iteration 28/1000 | Loss: 0.00005189
Iteration 29/1000 | Loss: 0.00005189
Iteration 30/1000 | Loss: 0.00005189
Iteration 31/1000 | Loss: 0.00005189
Iteration 32/1000 | Loss: 0.00005189
Iteration 33/1000 | Loss: 0.00005188
Iteration 34/1000 | Loss: 0.00005188
Iteration 35/1000 | Loss: 0.00005188
Iteration 36/1000 | Loss: 0.00005187
Iteration 37/1000 | Loss: 0.00005187
Iteration 38/1000 | Loss: 0.00005187
Iteration 39/1000 | Loss: 0.00005187
Iteration 40/1000 | Loss: 0.00005187
Iteration 41/1000 | Loss: 0.00005187
Iteration 42/1000 | Loss: 0.00005187
Iteration 43/1000 | Loss: 0.00005187
Iteration 44/1000 | Loss: 0.00005186
Iteration 45/1000 | Loss: 0.00005186
Iteration 46/1000 | Loss: 0.00005186
Iteration 47/1000 | Loss: 0.00005186
Iteration 48/1000 | Loss: 0.00005186
Iteration 49/1000 | Loss: 0.00005186
Iteration 50/1000 | Loss: 0.00005185
Iteration 51/1000 | Loss: 0.00005185
Iteration 52/1000 | Loss: 0.00005185
Iteration 53/1000 | Loss: 0.00005185
Iteration 54/1000 | Loss: 0.00005184
Iteration 55/1000 | Loss: 0.00005184
Iteration 56/1000 | Loss: 0.00005184
Iteration 57/1000 | Loss: 0.00005184
Iteration 58/1000 | Loss: 0.00005184
Iteration 59/1000 | Loss: 0.00005183
Iteration 60/1000 | Loss: 0.00005183
Iteration 61/1000 | Loss: 0.00005183
Iteration 62/1000 | Loss: 0.00005183
Iteration 63/1000 | Loss: 0.00005183
Iteration 64/1000 | Loss: 0.00005183
Iteration 65/1000 | Loss: 0.00005182
Iteration 66/1000 | Loss: 0.00005182
Iteration 67/1000 | Loss: 0.00005182
Iteration 68/1000 | Loss: 0.00005181
Iteration 69/1000 | Loss: 0.00005181
Iteration 70/1000 | Loss: 0.00005181
Iteration 71/1000 | Loss: 0.00005181
Iteration 72/1000 | Loss: 0.00005180
Iteration 73/1000 | Loss: 0.00005180
Iteration 74/1000 | Loss: 0.00005180
Iteration 75/1000 | Loss: 0.00005180
Iteration 76/1000 | Loss: 0.00005180
Iteration 77/1000 | Loss: 0.00005180
Iteration 78/1000 | Loss: 0.00005180
Iteration 79/1000 | Loss: 0.00005179
Iteration 80/1000 | Loss: 0.00005179
Iteration 81/1000 | Loss: 0.00005179
Iteration 82/1000 | Loss: 0.00005179
Iteration 83/1000 | Loss: 0.00005179
Iteration 84/1000 | Loss: 0.00005179
Iteration 85/1000 | Loss: 0.00005179
Iteration 86/1000 | Loss: 0.00005179
Iteration 87/1000 | Loss: 0.00005178
Iteration 88/1000 | Loss: 0.00005178
Iteration 89/1000 | Loss: 0.00005178
Iteration 90/1000 | Loss: 0.00005178
Iteration 91/1000 | Loss: 0.00005178
Iteration 92/1000 | Loss: 0.00005178
Iteration 93/1000 | Loss: 0.00005178
Iteration 94/1000 | Loss: 0.00005178
Iteration 95/1000 | Loss: 0.00005178
Iteration 96/1000 | Loss: 0.00005177
Iteration 97/1000 | Loss: 0.00005177
Iteration 98/1000 | Loss: 0.00005177
Iteration 99/1000 | Loss: 0.00005177
Iteration 100/1000 | Loss: 0.00005177
Iteration 101/1000 | Loss: 0.00005177
Iteration 102/1000 | Loss: 0.00005176
Iteration 103/1000 | Loss: 0.00005176
Iteration 104/1000 | Loss: 0.00005176
Iteration 105/1000 | Loss: 0.00005176
Iteration 106/1000 | Loss: 0.00005176
Iteration 107/1000 | Loss: 0.00005176
Iteration 108/1000 | Loss: 0.00005176
Iteration 109/1000 | Loss: 0.00005176
Iteration 110/1000 | Loss: 0.00005176
Iteration 111/1000 | Loss: 0.00005176
Iteration 112/1000 | Loss: 0.00005176
Iteration 113/1000 | Loss: 0.00005176
Iteration 114/1000 | Loss: 0.00005175
Iteration 115/1000 | Loss: 0.00005175
Iteration 116/1000 | Loss: 0.00005175
Iteration 117/1000 | Loss: 0.00005175
Iteration 118/1000 | Loss: 0.00005175
Iteration 119/1000 | Loss: 0.00005175
Iteration 120/1000 | Loss: 0.00005174
Iteration 121/1000 | Loss: 0.00005174
Iteration 122/1000 | Loss: 0.00005174
Iteration 123/1000 | Loss: 0.00005174
Iteration 124/1000 | Loss: 0.00005174
Iteration 125/1000 | Loss: 0.00005174
Iteration 126/1000 | Loss: 0.00005174
Iteration 127/1000 | Loss: 0.00005174
Iteration 128/1000 | Loss: 0.00005173
Iteration 129/1000 | Loss: 0.00005173
Iteration 130/1000 | Loss: 0.00005173
Iteration 131/1000 | Loss: 0.00005173
Iteration 132/1000 | Loss: 0.00005173
Iteration 133/1000 | Loss: 0.00005173
Iteration 134/1000 | Loss: 0.00005173
Iteration 135/1000 | Loss: 0.00005173
Iteration 136/1000 | Loss: 0.00005173
Iteration 137/1000 | Loss: 0.00005173
Iteration 138/1000 | Loss: 0.00005173
Iteration 139/1000 | Loss: 0.00005173
Iteration 140/1000 | Loss: 0.00005173
Iteration 141/1000 | Loss: 0.00005173
Iteration 142/1000 | Loss: 0.00005173
Iteration 143/1000 | Loss: 0.00005173
Iteration 144/1000 | Loss: 0.00005173
Iteration 145/1000 | Loss: 0.00005173
Iteration 146/1000 | Loss: 0.00005173
Iteration 147/1000 | Loss: 0.00005173
Iteration 148/1000 | Loss: 0.00005173
Iteration 149/1000 | Loss: 0.00005173
Iteration 150/1000 | Loss: 0.00005173
Iteration 151/1000 | Loss: 0.00005173
Iteration 152/1000 | Loss: 0.00005173
Iteration 153/1000 | Loss: 0.00005173
Iteration 154/1000 | Loss: 0.00005173
Iteration 155/1000 | Loss: 0.00005173
Iteration 156/1000 | Loss: 0.00005173
Iteration 157/1000 | Loss: 0.00005173
Iteration 158/1000 | Loss: 0.00005173
Iteration 159/1000 | Loss: 0.00005173
Iteration 160/1000 | Loss: 0.00005173
Iteration 161/1000 | Loss: 0.00005173
Iteration 162/1000 | Loss: 0.00005173
Iteration 163/1000 | Loss: 0.00005173
Iteration 164/1000 | Loss: 0.00005173
Iteration 165/1000 | Loss: 0.00005173
Iteration 166/1000 | Loss: 0.00005173
Iteration 167/1000 | Loss: 0.00005173
Iteration 168/1000 | Loss: 0.00005173
Iteration 169/1000 | Loss: 0.00005173
Iteration 170/1000 | Loss: 0.00005173
Iteration 171/1000 | Loss: 0.00005173
Iteration 172/1000 | Loss: 0.00005173
Iteration 173/1000 | Loss: 0.00005173
Iteration 174/1000 | Loss: 0.00005173
Iteration 175/1000 | Loss: 0.00005173
Iteration 176/1000 | Loss: 0.00005173
Iteration 177/1000 | Loss: 0.00005173
Iteration 178/1000 | Loss: 0.00005173
Iteration 179/1000 | Loss: 0.00005173
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 179. Stopping optimization.
Last 5 losses: [5.1729271945077926e-05, 5.1729271945077926e-05, 5.1729271945077926e-05, 5.1729271945077926e-05, 5.1729271945077926e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 5.1729271945077926e-05

Optimization complete. Final v2v error: 5.581899166107178 mm

Highest mean error: 18.628564834594727 mm for frame 41

Lowest mean error: 4.9959540367126465 mm for frame 14

Saving results

Total time: 81.39106702804565
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_40_nl_6353/0003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_40_nl_6353/0003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_40_nl_6353/0003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00430601
Iteration 2/25 | Loss: 0.00144118
Iteration 3/25 | Loss: 0.00136651
Iteration 4/25 | Loss: 0.00134979
Iteration 5/25 | Loss: 0.00134409
Iteration 6/25 | Loss: 0.00134265
Iteration 7/25 | Loss: 0.00134233
Iteration 8/25 | Loss: 0.00134233
Iteration 9/25 | Loss: 0.00134233
Iteration 10/25 | Loss: 0.00134233
Iteration 11/25 | Loss: 0.00134233
Iteration 12/25 | Loss: 0.00134233
Iteration 13/25 | Loss: 0.00134233
Iteration 14/25 | Loss: 0.00134233
Iteration 15/25 | Loss: 0.00134233
Iteration 16/25 | Loss: 0.00134233
Iteration 17/25 | Loss: 0.00134233
Iteration 18/25 | Loss: 0.00134233
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0013423297787085176, 0.0013423297787085176, 0.0013423297787085176, 0.0013423297787085176, 0.0013423297787085176]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013423297787085176

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.02012777
Iteration 2/25 | Loss: 0.00149109
Iteration 3/25 | Loss: 0.00149109
Iteration 4/25 | Loss: 0.00149109
Iteration 5/25 | Loss: 0.00149109
Iteration 6/25 | Loss: 0.00149109
Iteration 7/25 | Loss: 0.00149109
Iteration 8/25 | Loss: 0.00149109
Iteration 9/25 | Loss: 0.00149109
Iteration 10/25 | Loss: 0.00149109
Iteration 11/25 | Loss: 0.00149109
Iteration 12/25 | Loss: 0.00149109
Iteration 13/25 | Loss: 0.00149109
Iteration 14/25 | Loss: 0.00149109
Iteration 15/25 | Loss: 0.00149109
Iteration 16/25 | Loss: 0.00149109
Iteration 17/25 | Loss: 0.00149109
Iteration 18/25 | Loss: 0.00149109
Iteration 19/25 | Loss: 0.00149109
Iteration 20/25 | Loss: 0.00149109
Iteration 21/25 | Loss: 0.00149109
Iteration 22/25 | Loss: 0.00149109
Iteration 23/25 | Loss: 0.00149109
Iteration 24/25 | Loss: 0.00149109
Iteration 25/25 | Loss: 0.00149109

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00149109
Iteration 2/1000 | Loss: 0.00004285
Iteration 3/1000 | Loss: 0.00003339
Iteration 4/1000 | Loss: 0.00003072
Iteration 5/1000 | Loss: 0.00002948
Iteration 6/1000 | Loss: 0.00002884
Iteration 7/1000 | Loss: 0.00002840
Iteration 8/1000 | Loss: 0.00002806
Iteration 9/1000 | Loss: 0.00002769
Iteration 10/1000 | Loss: 0.00002752
Iteration 11/1000 | Loss: 0.00002751
Iteration 12/1000 | Loss: 0.00002750
Iteration 13/1000 | Loss: 0.00002743
Iteration 14/1000 | Loss: 0.00002741
Iteration 15/1000 | Loss: 0.00002741
Iteration 16/1000 | Loss: 0.00002737
Iteration 17/1000 | Loss: 0.00002731
Iteration 18/1000 | Loss: 0.00002730
Iteration 19/1000 | Loss: 0.00002723
Iteration 20/1000 | Loss: 0.00002719
Iteration 21/1000 | Loss: 0.00002719
Iteration 22/1000 | Loss: 0.00002718
Iteration 23/1000 | Loss: 0.00002718
Iteration 24/1000 | Loss: 0.00002718
Iteration 25/1000 | Loss: 0.00002717
Iteration 26/1000 | Loss: 0.00002716
Iteration 27/1000 | Loss: 0.00002716
Iteration 28/1000 | Loss: 0.00002715
Iteration 29/1000 | Loss: 0.00002715
Iteration 30/1000 | Loss: 0.00002715
Iteration 31/1000 | Loss: 0.00002714
Iteration 32/1000 | Loss: 0.00002714
Iteration 33/1000 | Loss: 0.00002713
Iteration 34/1000 | Loss: 0.00002713
Iteration 35/1000 | Loss: 0.00002713
Iteration 36/1000 | Loss: 0.00002713
Iteration 37/1000 | Loss: 0.00002713
Iteration 38/1000 | Loss: 0.00002712
Iteration 39/1000 | Loss: 0.00002712
Iteration 40/1000 | Loss: 0.00002712
Iteration 41/1000 | Loss: 0.00002712
Iteration 42/1000 | Loss: 0.00002712
Iteration 43/1000 | Loss: 0.00002712
Iteration 44/1000 | Loss: 0.00002712
Iteration 45/1000 | Loss: 0.00002712
Iteration 46/1000 | Loss: 0.00002712
Iteration 47/1000 | Loss: 0.00002712
Iteration 48/1000 | Loss: 0.00002712
Iteration 49/1000 | Loss: 0.00002712
Iteration 50/1000 | Loss: 0.00002711
Iteration 51/1000 | Loss: 0.00002711
Iteration 52/1000 | Loss: 0.00002711
Iteration 53/1000 | Loss: 0.00002711
Iteration 54/1000 | Loss: 0.00002710
Iteration 55/1000 | Loss: 0.00002710
Iteration 56/1000 | Loss: 0.00002710
Iteration 57/1000 | Loss: 0.00002710
Iteration 58/1000 | Loss: 0.00002710
Iteration 59/1000 | Loss: 0.00002710
Iteration 60/1000 | Loss: 0.00002710
Iteration 61/1000 | Loss: 0.00002710
Iteration 62/1000 | Loss: 0.00002709
Iteration 63/1000 | Loss: 0.00002709
Iteration 64/1000 | Loss: 0.00002709
Iteration 65/1000 | Loss: 0.00002709
Iteration 66/1000 | Loss: 0.00002709
Iteration 67/1000 | Loss: 0.00002709
Iteration 68/1000 | Loss: 0.00002709
Iteration 69/1000 | Loss: 0.00002709
Iteration 70/1000 | Loss: 0.00002708
Iteration 71/1000 | Loss: 0.00002708
Iteration 72/1000 | Loss: 0.00002708
Iteration 73/1000 | Loss: 0.00002708
Iteration 74/1000 | Loss: 0.00002707
Iteration 75/1000 | Loss: 0.00002707
Iteration 76/1000 | Loss: 0.00002707
Iteration 77/1000 | Loss: 0.00002707
Iteration 78/1000 | Loss: 0.00002707
Iteration 79/1000 | Loss: 0.00002707
Iteration 80/1000 | Loss: 0.00002707
Iteration 81/1000 | Loss: 0.00002706
Iteration 82/1000 | Loss: 0.00002706
Iteration 83/1000 | Loss: 0.00002706
Iteration 84/1000 | Loss: 0.00002706
Iteration 85/1000 | Loss: 0.00002706
Iteration 86/1000 | Loss: 0.00002705
Iteration 87/1000 | Loss: 0.00002705
Iteration 88/1000 | Loss: 0.00002705
Iteration 89/1000 | Loss: 0.00002705
Iteration 90/1000 | Loss: 0.00002705
Iteration 91/1000 | Loss: 0.00002705
Iteration 92/1000 | Loss: 0.00002705
Iteration 93/1000 | Loss: 0.00002705
Iteration 94/1000 | Loss: 0.00002705
Iteration 95/1000 | Loss: 0.00002704
Iteration 96/1000 | Loss: 0.00002704
Iteration 97/1000 | Loss: 0.00002704
Iteration 98/1000 | Loss: 0.00002704
Iteration 99/1000 | Loss: 0.00002704
Iteration 100/1000 | Loss: 0.00002704
Iteration 101/1000 | Loss: 0.00002704
Iteration 102/1000 | Loss: 0.00002704
Iteration 103/1000 | Loss: 0.00002704
Iteration 104/1000 | Loss: 0.00002704
Iteration 105/1000 | Loss: 0.00002703
Iteration 106/1000 | Loss: 0.00002703
Iteration 107/1000 | Loss: 0.00002703
Iteration 108/1000 | Loss: 0.00002703
Iteration 109/1000 | Loss: 0.00002703
Iteration 110/1000 | Loss: 0.00002703
Iteration 111/1000 | Loss: 0.00002703
Iteration 112/1000 | Loss: 0.00002702
Iteration 113/1000 | Loss: 0.00002702
Iteration 114/1000 | Loss: 0.00002702
Iteration 115/1000 | Loss: 0.00002702
Iteration 116/1000 | Loss: 0.00002702
Iteration 117/1000 | Loss: 0.00002702
Iteration 118/1000 | Loss: 0.00002702
Iteration 119/1000 | Loss: 0.00002702
Iteration 120/1000 | Loss: 0.00002702
Iteration 121/1000 | Loss: 0.00002702
Iteration 122/1000 | Loss: 0.00002702
Iteration 123/1000 | Loss: 0.00002702
Iteration 124/1000 | Loss: 0.00002702
Iteration 125/1000 | Loss: 0.00002702
Iteration 126/1000 | Loss: 0.00002701
Iteration 127/1000 | Loss: 0.00002701
Iteration 128/1000 | Loss: 0.00002701
Iteration 129/1000 | Loss: 0.00002701
Iteration 130/1000 | Loss: 0.00002701
Iteration 131/1000 | Loss: 0.00002701
Iteration 132/1000 | Loss: 0.00002701
Iteration 133/1000 | Loss: 0.00002701
Iteration 134/1000 | Loss: 0.00002701
Iteration 135/1000 | Loss: 0.00002701
Iteration 136/1000 | Loss: 0.00002701
Iteration 137/1000 | Loss: 0.00002701
Iteration 138/1000 | Loss: 0.00002701
Iteration 139/1000 | Loss: 0.00002701
Iteration 140/1000 | Loss: 0.00002701
Iteration 141/1000 | Loss: 0.00002701
Iteration 142/1000 | Loss: 0.00002701
Iteration 143/1000 | Loss: 0.00002701
Iteration 144/1000 | Loss: 0.00002701
Iteration 145/1000 | Loss: 0.00002701
Iteration 146/1000 | Loss: 0.00002701
Iteration 147/1000 | Loss: 0.00002701
Iteration 148/1000 | Loss: 0.00002701
Iteration 149/1000 | Loss: 0.00002701
Iteration 150/1000 | Loss: 0.00002701
Iteration 151/1000 | Loss: 0.00002701
Iteration 152/1000 | Loss: 0.00002701
Iteration 153/1000 | Loss: 0.00002701
Iteration 154/1000 | Loss: 0.00002701
Iteration 155/1000 | Loss: 0.00002701
Iteration 156/1000 | Loss: 0.00002701
Iteration 157/1000 | Loss: 0.00002701
Iteration 158/1000 | Loss: 0.00002701
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 158. Stopping optimization.
Last 5 losses: [2.700661207200028e-05, 2.700661207200028e-05, 2.700661207200028e-05, 2.700661207200028e-05, 2.700661207200028e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.700661207200028e-05

Optimization complete. Final v2v error: 4.505526542663574 mm

Highest mean error: 4.753605842590332 mm for frame 78

Lowest mean error: 4.218771934509277 mm for frame 3

Saving results

Total time: 35.01280760765076
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_40_nl_6353/0019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_40_nl_6353/0019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_40_nl_6353/0019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01118343
Iteration 2/25 | Loss: 0.00384449
Iteration 3/25 | Loss: 0.00275296
Iteration 4/25 | Loss: 0.00237004
Iteration 5/25 | Loss: 0.00194004
Iteration 6/25 | Loss: 0.00182754
Iteration 7/25 | Loss: 0.00171907
Iteration 8/25 | Loss: 0.00162671
Iteration 9/25 | Loss: 0.00152082
Iteration 10/25 | Loss: 0.00144992
Iteration 11/25 | Loss: 0.00137231
Iteration 12/25 | Loss: 0.00135383
Iteration 13/25 | Loss: 0.00134035
Iteration 14/25 | Loss: 0.00132439
Iteration 15/25 | Loss: 0.00131429
Iteration 16/25 | Loss: 0.00131044
Iteration 17/25 | Loss: 0.00131081
Iteration 18/25 | Loss: 0.00130477
Iteration 19/25 | Loss: 0.00130608
Iteration 20/25 | Loss: 0.00130170
Iteration 21/25 | Loss: 0.00129996
Iteration 22/25 | Loss: 0.00129951
Iteration 23/25 | Loss: 0.00129949
Iteration 24/25 | Loss: 0.00129949
Iteration 25/25 | Loss: 0.00129949

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.57091355
Iteration 2/25 | Loss: 0.00259659
Iteration 3/25 | Loss: 0.00236908
Iteration 4/25 | Loss: 0.00236908
Iteration 5/25 | Loss: 0.00236908
Iteration 6/25 | Loss: 0.00236908
Iteration 7/25 | Loss: 0.00236908
Iteration 8/25 | Loss: 0.00236908
Iteration 9/25 | Loss: 0.00236908
Iteration 10/25 | Loss: 0.00236908
Iteration 11/25 | Loss: 0.00236908
Iteration 12/25 | Loss: 0.00236908
Iteration 13/25 | Loss: 0.00236908
Iteration 14/25 | Loss: 0.00236908
Iteration 15/25 | Loss: 0.00236908
Iteration 16/25 | Loss: 0.00236908
Iteration 17/25 | Loss: 0.00236908
Iteration 18/25 | Loss: 0.00236908
Iteration 19/25 | Loss: 0.00236908
Iteration 20/25 | Loss: 0.00236908
Iteration 21/25 | Loss: 0.00236908
Iteration 22/25 | Loss: 0.00236908
Iteration 23/25 | Loss: 0.00236908
Iteration 24/25 | Loss: 0.00236908
Iteration 25/25 | Loss: 0.00236908

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00236908
Iteration 2/1000 | Loss: 0.00128197
Iteration 3/1000 | Loss: 0.00092963
Iteration 4/1000 | Loss: 0.00055953
Iteration 5/1000 | Loss: 0.00096533
Iteration 6/1000 | Loss: 0.00059970
Iteration 7/1000 | Loss: 0.00141043
Iteration 8/1000 | Loss: 0.00305259
Iteration 9/1000 | Loss: 0.00447492
Iteration 10/1000 | Loss: 0.00052388
Iteration 11/1000 | Loss: 0.00078342
Iteration 12/1000 | Loss: 0.00031532
Iteration 13/1000 | Loss: 0.00062348
Iteration 14/1000 | Loss: 0.00051485
Iteration 15/1000 | Loss: 0.00061251
Iteration 16/1000 | Loss: 0.00101401
Iteration 17/1000 | Loss: 0.00158218
Iteration 18/1000 | Loss: 0.00231613
Iteration 19/1000 | Loss: 0.00472467
Iteration 20/1000 | Loss: 0.00084858
Iteration 21/1000 | Loss: 0.00222741
Iteration 22/1000 | Loss: 0.00080220
Iteration 23/1000 | Loss: 0.00070520
Iteration 24/1000 | Loss: 0.00080510
Iteration 25/1000 | Loss: 0.00067830
Iteration 26/1000 | Loss: 0.00061606
Iteration 27/1000 | Loss: 0.00081116
Iteration 28/1000 | Loss: 0.00090481
Iteration 29/1000 | Loss: 0.00066291
Iteration 30/1000 | Loss: 0.00111775
Iteration 31/1000 | Loss: 0.00032532
Iteration 32/1000 | Loss: 0.00103670
Iteration 33/1000 | Loss: 0.00053690
Iteration 34/1000 | Loss: 0.00054689
Iteration 35/1000 | Loss: 0.00045257
Iteration 36/1000 | Loss: 0.00053164
Iteration 37/1000 | Loss: 0.00118841
Iteration 38/1000 | Loss: 0.00116768
Iteration 39/1000 | Loss: 0.00116905
Iteration 40/1000 | Loss: 0.00034426
Iteration 41/1000 | Loss: 0.00078833
Iteration 42/1000 | Loss: 0.00028000
Iteration 43/1000 | Loss: 0.00018353
Iteration 44/1000 | Loss: 0.00062982
Iteration 45/1000 | Loss: 0.00044499
Iteration 46/1000 | Loss: 0.00140776
Iteration 47/1000 | Loss: 0.00058798
Iteration 48/1000 | Loss: 0.00040938
Iteration 49/1000 | Loss: 0.00049018
Iteration 50/1000 | Loss: 0.00031499
Iteration 51/1000 | Loss: 0.00044355
Iteration 52/1000 | Loss: 0.00038658
Iteration 53/1000 | Loss: 0.00042036
Iteration 54/1000 | Loss: 0.00046427
Iteration 55/1000 | Loss: 0.00032037
Iteration 56/1000 | Loss: 0.00098878
Iteration 57/1000 | Loss: 0.00111450
Iteration 58/1000 | Loss: 0.00048908
Iteration 59/1000 | Loss: 0.00049184
Iteration 60/1000 | Loss: 0.00037805
Iteration 61/1000 | Loss: 0.00022294
Iteration 62/1000 | Loss: 0.00028407
Iteration 63/1000 | Loss: 0.00054533
Iteration 64/1000 | Loss: 0.00021484
Iteration 65/1000 | Loss: 0.00024296
Iteration 66/1000 | Loss: 0.00037673
Iteration 67/1000 | Loss: 0.00081188
Iteration 68/1000 | Loss: 0.00039617
Iteration 69/1000 | Loss: 0.00037511
Iteration 70/1000 | Loss: 0.00022666
Iteration 71/1000 | Loss: 0.00017065
Iteration 72/1000 | Loss: 0.00042469
Iteration 73/1000 | Loss: 0.00042741
Iteration 74/1000 | Loss: 0.00052259
Iteration 75/1000 | Loss: 0.00051027
Iteration 76/1000 | Loss: 0.00069369
Iteration 77/1000 | Loss: 0.00059253
Iteration 78/1000 | Loss: 0.00053976
Iteration 79/1000 | Loss: 0.00028132
Iteration 80/1000 | Loss: 0.00014626
Iteration 81/1000 | Loss: 0.00016702
Iteration 82/1000 | Loss: 0.00012883
Iteration 83/1000 | Loss: 0.00021943
Iteration 84/1000 | Loss: 0.00016714
Iteration 85/1000 | Loss: 0.00037273
Iteration 86/1000 | Loss: 0.00012245
Iteration 87/1000 | Loss: 0.00014022
Iteration 88/1000 | Loss: 0.00059328
Iteration 89/1000 | Loss: 0.00123693
Iteration 90/1000 | Loss: 0.00026750
Iteration 91/1000 | Loss: 0.00018571
Iteration 92/1000 | Loss: 0.00036309
Iteration 93/1000 | Loss: 0.00072165
Iteration 94/1000 | Loss: 0.00068496
Iteration 95/1000 | Loss: 0.00069317
Iteration 96/1000 | Loss: 0.00030201
Iteration 97/1000 | Loss: 0.00023677
Iteration 98/1000 | Loss: 0.00036494
Iteration 99/1000 | Loss: 0.00067097
Iteration 100/1000 | Loss: 0.00037287
Iteration 101/1000 | Loss: 0.00040190
Iteration 102/1000 | Loss: 0.00022362
Iteration 103/1000 | Loss: 0.00029827
Iteration 104/1000 | Loss: 0.00016235
Iteration 105/1000 | Loss: 0.00048682
Iteration 106/1000 | Loss: 0.00120310
Iteration 107/1000 | Loss: 0.00020980
Iteration 108/1000 | Loss: 0.00042759
Iteration 109/1000 | Loss: 0.00048016
Iteration 110/1000 | Loss: 0.00021820
Iteration 111/1000 | Loss: 0.00028386
Iteration 112/1000 | Loss: 0.00011176
Iteration 113/1000 | Loss: 0.00011932
Iteration 114/1000 | Loss: 0.00015445
Iteration 115/1000 | Loss: 0.00009338
Iteration 116/1000 | Loss: 0.00015930
Iteration 117/1000 | Loss: 0.00014549
Iteration 118/1000 | Loss: 0.00088332
Iteration 119/1000 | Loss: 0.00033558
Iteration 120/1000 | Loss: 0.00030657
Iteration 121/1000 | Loss: 0.00009221
Iteration 122/1000 | Loss: 0.00032445
Iteration 123/1000 | Loss: 0.00056888
Iteration 124/1000 | Loss: 0.00024973
Iteration 125/1000 | Loss: 0.00012773
Iteration 126/1000 | Loss: 0.00013270
Iteration 127/1000 | Loss: 0.00019548
Iteration 128/1000 | Loss: 0.00023236
Iteration 129/1000 | Loss: 0.00007514
Iteration 130/1000 | Loss: 0.00013946
Iteration 131/1000 | Loss: 0.00012673
Iteration 132/1000 | Loss: 0.00040502
Iteration 133/1000 | Loss: 0.00030473
Iteration 134/1000 | Loss: 0.00040077
Iteration 135/1000 | Loss: 0.00010558
Iteration 136/1000 | Loss: 0.00030368
Iteration 137/1000 | Loss: 0.00021093
Iteration 138/1000 | Loss: 0.00020862
Iteration 139/1000 | Loss: 0.00043188
Iteration 140/1000 | Loss: 0.00080723
Iteration 141/1000 | Loss: 0.00022480
Iteration 142/1000 | Loss: 0.00041002
Iteration 143/1000 | Loss: 0.00032055
Iteration 144/1000 | Loss: 0.00060974
Iteration 145/1000 | Loss: 0.00049753
Iteration 146/1000 | Loss: 0.00063466
Iteration 147/1000 | Loss: 0.00037907
Iteration 148/1000 | Loss: 0.00020234
Iteration 149/1000 | Loss: 0.00052289
Iteration 150/1000 | Loss: 0.00035606
Iteration 151/1000 | Loss: 0.00035947
Iteration 152/1000 | Loss: 0.00099084
Iteration 153/1000 | Loss: 0.00182817
Iteration 154/1000 | Loss: 0.00049206
Iteration 155/1000 | Loss: 0.00055058
Iteration 156/1000 | Loss: 0.00021987
Iteration 157/1000 | Loss: 0.00039970
Iteration 158/1000 | Loss: 0.00032408
Iteration 159/1000 | Loss: 0.00021044
Iteration 160/1000 | Loss: 0.00025433
Iteration 161/1000 | Loss: 0.00013775
Iteration 162/1000 | Loss: 0.00017078
Iteration 163/1000 | Loss: 0.00026250
Iteration 164/1000 | Loss: 0.00019403
Iteration 165/1000 | Loss: 0.00010544
Iteration 166/1000 | Loss: 0.00044932
Iteration 167/1000 | Loss: 0.00010028
Iteration 168/1000 | Loss: 0.00032487
Iteration 169/1000 | Loss: 0.00010976
Iteration 170/1000 | Loss: 0.00015796
Iteration 171/1000 | Loss: 0.00045078
Iteration 172/1000 | Loss: 0.00059696
Iteration 173/1000 | Loss: 0.00032138
Iteration 174/1000 | Loss: 0.00054674
Iteration 175/1000 | Loss: 0.00042325
Iteration 176/1000 | Loss: 0.00006207
Iteration 177/1000 | Loss: 0.00005330
Iteration 178/1000 | Loss: 0.00044155
Iteration 179/1000 | Loss: 0.00035663
Iteration 180/1000 | Loss: 0.00027754
Iteration 181/1000 | Loss: 0.00013050
Iteration 182/1000 | Loss: 0.00010262
Iteration 183/1000 | Loss: 0.00029569
Iteration 184/1000 | Loss: 0.00004751
Iteration 185/1000 | Loss: 0.00024286
Iteration 186/1000 | Loss: 0.00004137
Iteration 187/1000 | Loss: 0.00009000
Iteration 188/1000 | Loss: 0.00003802
Iteration 189/1000 | Loss: 0.00003698
Iteration 190/1000 | Loss: 0.00023895
Iteration 191/1000 | Loss: 0.00006104
Iteration 192/1000 | Loss: 0.00036307
Iteration 193/1000 | Loss: 0.00031979
Iteration 194/1000 | Loss: 0.00010233
Iteration 195/1000 | Loss: 0.00033272
Iteration 196/1000 | Loss: 0.00037534
Iteration 197/1000 | Loss: 0.00036323
Iteration 198/1000 | Loss: 0.00020847
Iteration 199/1000 | Loss: 0.00004469
Iteration 200/1000 | Loss: 0.00004054
Iteration 201/1000 | Loss: 0.00003643
Iteration 202/1000 | Loss: 0.00003407
Iteration 203/1000 | Loss: 0.00003340
Iteration 204/1000 | Loss: 0.00003300
Iteration 205/1000 | Loss: 0.00003271
Iteration 206/1000 | Loss: 0.00016958
Iteration 207/1000 | Loss: 0.00003239
Iteration 208/1000 | Loss: 0.00003238
Iteration 209/1000 | Loss: 0.00003227
Iteration 210/1000 | Loss: 0.00003226
Iteration 211/1000 | Loss: 0.00003225
Iteration 212/1000 | Loss: 0.00003223
Iteration 213/1000 | Loss: 0.00003223
Iteration 214/1000 | Loss: 0.00003223
Iteration 215/1000 | Loss: 0.00003222
Iteration 216/1000 | Loss: 0.00003221
Iteration 217/1000 | Loss: 0.00003221
Iteration 218/1000 | Loss: 0.00003220
Iteration 219/1000 | Loss: 0.00003220
Iteration 220/1000 | Loss: 0.00003219
Iteration 221/1000 | Loss: 0.00003218
Iteration 222/1000 | Loss: 0.00003212
Iteration 223/1000 | Loss: 0.00003212
Iteration 224/1000 | Loss: 0.00003211
Iteration 225/1000 | Loss: 0.00003210
Iteration 226/1000 | Loss: 0.00003210
Iteration 227/1000 | Loss: 0.00003209
Iteration 228/1000 | Loss: 0.00003206
Iteration 229/1000 | Loss: 0.00003205
Iteration 230/1000 | Loss: 0.00003204
Iteration 231/1000 | Loss: 0.00003204
Iteration 232/1000 | Loss: 0.00003204
Iteration 233/1000 | Loss: 0.00003204
Iteration 234/1000 | Loss: 0.00003203
Iteration 235/1000 | Loss: 0.00003203
Iteration 236/1000 | Loss: 0.00003202
Iteration 237/1000 | Loss: 0.00003202
Iteration 238/1000 | Loss: 0.00003201
Iteration 239/1000 | Loss: 0.00003201
Iteration 240/1000 | Loss: 0.00003201
Iteration 241/1000 | Loss: 0.00003201
Iteration 242/1000 | Loss: 0.00003200
Iteration 243/1000 | Loss: 0.00003200
Iteration 244/1000 | Loss: 0.00003200
Iteration 245/1000 | Loss: 0.00003199
Iteration 246/1000 | Loss: 0.00003199
Iteration 247/1000 | Loss: 0.00003199
Iteration 248/1000 | Loss: 0.00003199
Iteration 249/1000 | Loss: 0.00003199
Iteration 250/1000 | Loss: 0.00003199
Iteration 251/1000 | Loss: 0.00003199
Iteration 252/1000 | Loss: 0.00003199
Iteration 253/1000 | Loss: 0.00003198
Iteration 254/1000 | Loss: 0.00003198
Iteration 255/1000 | Loss: 0.00003198
Iteration 256/1000 | Loss: 0.00003197
Iteration 257/1000 | Loss: 0.00003197
Iteration 258/1000 | Loss: 0.00003197
Iteration 259/1000 | Loss: 0.00003197
Iteration 260/1000 | Loss: 0.00003197
Iteration 261/1000 | Loss: 0.00003197
Iteration 262/1000 | Loss: 0.00003196
Iteration 263/1000 | Loss: 0.00003196
Iteration 264/1000 | Loss: 0.00003196
Iteration 265/1000 | Loss: 0.00003196
Iteration 266/1000 | Loss: 0.00003196
Iteration 267/1000 | Loss: 0.00003195
Iteration 268/1000 | Loss: 0.00003195
Iteration 269/1000 | Loss: 0.00003195
Iteration 270/1000 | Loss: 0.00003195
Iteration 271/1000 | Loss: 0.00003194
Iteration 272/1000 | Loss: 0.00003194
Iteration 273/1000 | Loss: 0.00003194
Iteration 274/1000 | Loss: 0.00003194
Iteration 275/1000 | Loss: 0.00003194
Iteration 276/1000 | Loss: 0.00003194
Iteration 277/1000 | Loss: 0.00003194
Iteration 278/1000 | Loss: 0.00003193
Iteration 279/1000 | Loss: 0.00003193
Iteration 280/1000 | Loss: 0.00003193
Iteration 281/1000 | Loss: 0.00003193
Iteration 282/1000 | Loss: 0.00003193
Iteration 283/1000 | Loss: 0.00003193
Iteration 284/1000 | Loss: 0.00003193
Iteration 285/1000 | Loss: 0.00003193
Iteration 286/1000 | Loss: 0.00003193
Iteration 287/1000 | Loss: 0.00003193
Iteration 288/1000 | Loss: 0.00003193
Iteration 289/1000 | Loss: 0.00003193
Iteration 290/1000 | Loss: 0.00003193
Iteration 291/1000 | Loss: 0.00003193
Iteration 292/1000 | Loss: 0.00003193
Iteration 293/1000 | Loss: 0.00003193
Iteration 294/1000 | Loss: 0.00003193
Iteration 295/1000 | Loss: 0.00003192
Iteration 296/1000 | Loss: 0.00003192
Iteration 297/1000 | Loss: 0.00003192
Iteration 298/1000 | Loss: 0.00003192
Iteration 299/1000 | Loss: 0.00003192
Iteration 300/1000 | Loss: 0.00003192
Iteration 301/1000 | Loss: 0.00003192
Iteration 302/1000 | Loss: 0.00003192
Iteration 303/1000 | Loss: 0.00003192
Iteration 304/1000 | Loss: 0.00003192
Iteration 305/1000 | Loss: 0.00003192
Iteration 306/1000 | Loss: 0.00003192
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 306. Stopping optimization.
Last 5 losses: [3.192358781234361e-05, 3.192358781234361e-05, 3.192358781234361e-05, 3.192358781234361e-05, 3.192358781234361e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.192358781234361e-05

Optimization complete. Final v2v error: 4.421492576599121 mm

Highest mean error: 14.567926406860352 mm for frame 115

Lowest mean error: 3.877608060836792 mm for frame 142

Saving results

Total time: 385.895498752594
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_40_nl_6353/0008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_40_nl_6353/0008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_40_nl_6353/0008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00487873
Iteration 2/25 | Loss: 0.00148518
Iteration 3/25 | Loss: 0.00135093
Iteration 4/25 | Loss: 0.00133636
Iteration 5/25 | Loss: 0.00133020
Iteration 6/25 | Loss: 0.00132861
Iteration 7/25 | Loss: 0.00132861
Iteration 8/25 | Loss: 0.00132861
Iteration 9/25 | Loss: 0.00132861
Iteration 10/25 | Loss: 0.00132861
Iteration 11/25 | Loss: 0.00132861
Iteration 12/25 | Loss: 0.00132861
Iteration 13/25 | Loss: 0.00132861
Iteration 14/25 | Loss: 0.00132861
Iteration 15/25 | Loss: 0.00132861
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0013286113971844316, 0.0013286113971844316, 0.0013286113971844316, 0.0013286113971844316, 0.0013286113971844316]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013286113971844316

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.53163338
Iteration 2/25 | Loss: 0.00163331
Iteration 3/25 | Loss: 0.00163331
Iteration 4/25 | Loss: 0.00163331
Iteration 5/25 | Loss: 0.00163331
Iteration 6/25 | Loss: 0.00163331
Iteration 7/25 | Loss: 0.00163331
Iteration 8/25 | Loss: 0.00163331
Iteration 9/25 | Loss: 0.00163331
Iteration 10/25 | Loss: 0.00163331
Iteration 11/25 | Loss: 0.00163331
Iteration 12/25 | Loss: 0.00163331
Iteration 13/25 | Loss: 0.00163331
Iteration 14/25 | Loss: 0.00163331
Iteration 15/25 | Loss: 0.00163331
Iteration 16/25 | Loss: 0.00163331
Iteration 17/25 | Loss: 0.00163331
Iteration 18/25 | Loss: 0.00163331
Iteration 19/25 | Loss: 0.00163331
Iteration 20/25 | Loss: 0.00163331
Iteration 21/25 | Loss: 0.00163331
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0016333115054294467, 0.0016333115054294467, 0.0016333115054294467, 0.0016333115054294467, 0.0016333115054294467]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0016333115054294467

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00163331
Iteration 2/1000 | Loss: 0.00002937
Iteration 3/1000 | Loss: 0.00002267
Iteration 4/1000 | Loss: 0.00002128
Iteration 5/1000 | Loss: 0.00002056
Iteration 6/1000 | Loss: 0.00002009
Iteration 7/1000 | Loss: 0.00001984
Iteration 8/1000 | Loss: 0.00001960
Iteration 9/1000 | Loss: 0.00001947
Iteration 10/1000 | Loss: 0.00001945
Iteration 11/1000 | Loss: 0.00001944
Iteration 12/1000 | Loss: 0.00001944
Iteration 13/1000 | Loss: 0.00001936
Iteration 14/1000 | Loss: 0.00001925
Iteration 15/1000 | Loss: 0.00001920
Iteration 16/1000 | Loss: 0.00001920
Iteration 17/1000 | Loss: 0.00001920
Iteration 18/1000 | Loss: 0.00001920
Iteration 19/1000 | Loss: 0.00001920
Iteration 20/1000 | Loss: 0.00001920
Iteration 21/1000 | Loss: 0.00001920
Iteration 22/1000 | Loss: 0.00001919
Iteration 23/1000 | Loss: 0.00001918
Iteration 24/1000 | Loss: 0.00001918
Iteration 25/1000 | Loss: 0.00001917
Iteration 26/1000 | Loss: 0.00001917
Iteration 27/1000 | Loss: 0.00001916
Iteration 28/1000 | Loss: 0.00001916
Iteration 29/1000 | Loss: 0.00001915
Iteration 30/1000 | Loss: 0.00001915
Iteration 31/1000 | Loss: 0.00001915
Iteration 32/1000 | Loss: 0.00001914
Iteration 33/1000 | Loss: 0.00001914
Iteration 34/1000 | Loss: 0.00001913
Iteration 35/1000 | Loss: 0.00001913
Iteration 36/1000 | Loss: 0.00001913
Iteration 37/1000 | Loss: 0.00001912
Iteration 38/1000 | Loss: 0.00001912
Iteration 39/1000 | Loss: 0.00001912
Iteration 40/1000 | Loss: 0.00001911
Iteration 41/1000 | Loss: 0.00001911
Iteration 42/1000 | Loss: 0.00001911
Iteration 43/1000 | Loss: 0.00001910
Iteration 44/1000 | Loss: 0.00001910
Iteration 45/1000 | Loss: 0.00001910
Iteration 46/1000 | Loss: 0.00001909
Iteration 47/1000 | Loss: 0.00001909
Iteration 48/1000 | Loss: 0.00001909
Iteration 49/1000 | Loss: 0.00001908
Iteration 50/1000 | Loss: 0.00001908
Iteration 51/1000 | Loss: 0.00001908
Iteration 52/1000 | Loss: 0.00001908
Iteration 53/1000 | Loss: 0.00001908
Iteration 54/1000 | Loss: 0.00001908
Iteration 55/1000 | Loss: 0.00001907
Iteration 56/1000 | Loss: 0.00001907
Iteration 57/1000 | Loss: 0.00001907
Iteration 58/1000 | Loss: 0.00001906
Iteration 59/1000 | Loss: 0.00001906
Iteration 60/1000 | Loss: 0.00001906
Iteration 61/1000 | Loss: 0.00001906
Iteration 62/1000 | Loss: 0.00001906
Iteration 63/1000 | Loss: 0.00001906
Iteration 64/1000 | Loss: 0.00001906
Iteration 65/1000 | Loss: 0.00001906
Iteration 66/1000 | Loss: 0.00001906
Iteration 67/1000 | Loss: 0.00001906
Iteration 68/1000 | Loss: 0.00001905
Iteration 69/1000 | Loss: 0.00001905
Iteration 70/1000 | Loss: 0.00001905
Iteration 71/1000 | Loss: 0.00001905
Iteration 72/1000 | Loss: 0.00001905
Iteration 73/1000 | Loss: 0.00001904
Iteration 74/1000 | Loss: 0.00001904
Iteration 75/1000 | Loss: 0.00001904
Iteration 76/1000 | Loss: 0.00001904
Iteration 77/1000 | Loss: 0.00001903
Iteration 78/1000 | Loss: 0.00001903
Iteration 79/1000 | Loss: 0.00001903
Iteration 80/1000 | Loss: 0.00001903
Iteration 81/1000 | Loss: 0.00001903
Iteration 82/1000 | Loss: 0.00001903
Iteration 83/1000 | Loss: 0.00001903
Iteration 84/1000 | Loss: 0.00001903
Iteration 85/1000 | Loss: 0.00001903
Iteration 86/1000 | Loss: 0.00001902
Iteration 87/1000 | Loss: 0.00001902
Iteration 88/1000 | Loss: 0.00001902
Iteration 89/1000 | Loss: 0.00001902
Iteration 90/1000 | Loss: 0.00001902
Iteration 91/1000 | Loss: 0.00001902
Iteration 92/1000 | Loss: 0.00001902
Iteration 93/1000 | Loss: 0.00001902
Iteration 94/1000 | Loss: 0.00001902
Iteration 95/1000 | Loss: 0.00001902
Iteration 96/1000 | Loss: 0.00001902
Iteration 97/1000 | Loss: 0.00001902
Iteration 98/1000 | Loss: 0.00001902
Iteration 99/1000 | Loss: 0.00001902
Iteration 100/1000 | Loss: 0.00001902
Iteration 101/1000 | Loss: 0.00001902
Iteration 102/1000 | Loss: 0.00001902
Iteration 103/1000 | Loss: 0.00001902
Iteration 104/1000 | Loss: 0.00001902
Iteration 105/1000 | Loss: 0.00001902
Iteration 106/1000 | Loss: 0.00001902
Iteration 107/1000 | Loss: 0.00001902
Iteration 108/1000 | Loss: 0.00001902
Iteration 109/1000 | Loss: 0.00001902
Iteration 110/1000 | Loss: 0.00001902
Iteration 111/1000 | Loss: 0.00001902
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 111. Stopping optimization.
Last 5 losses: [1.9016511942027137e-05, 1.9016511942027137e-05, 1.9016511942027137e-05, 1.9016511942027137e-05, 1.9016511942027137e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9016511942027137e-05

Optimization complete. Final v2v error: 3.8325674533843994 mm

Highest mean error: 4.072422504425049 mm for frame 3

Lowest mean error: 3.7025949954986572 mm for frame 63

Saving results

Total time: 34.54051661491394
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_40_nl_6353/0018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_40_nl_6353/0018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_40_nl_6353/0018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00646810
Iteration 2/25 | Loss: 0.00166471
Iteration 3/25 | Loss: 0.00148445
Iteration 4/25 | Loss: 0.00145565
Iteration 5/25 | Loss: 0.00144452
Iteration 6/25 | Loss: 0.00144178
Iteration 7/25 | Loss: 0.00144083
Iteration 8/25 | Loss: 0.00144083
Iteration 9/25 | Loss: 0.00144083
Iteration 10/25 | Loss: 0.00144083
Iteration 11/25 | Loss: 0.00144083
Iteration 12/25 | Loss: 0.00144083
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0014408258721232414, 0.0014408258721232414, 0.0014408258721232414, 0.0014408258721232414, 0.0014408258721232414]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014408258721232414

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.52645373
Iteration 2/25 | Loss: 0.00228482
Iteration 3/25 | Loss: 0.00228477
Iteration 4/25 | Loss: 0.00228476
Iteration 5/25 | Loss: 0.00228476
Iteration 6/25 | Loss: 0.00228476
Iteration 7/25 | Loss: 0.00228476
Iteration 8/25 | Loss: 0.00228476
Iteration 9/25 | Loss: 0.00228476
Iteration 10/25 | Loss: 0.00228476
Iteration 11/25 | Loss: 0.00228476
Iteration 12/25 | Loss: 0.00228476
Iteration 13/25 | Loss: 0.00228476
Iteration 14/25 | Loss: 0.00228476
Iteration 15/25 | Loss: 0.00228476
Iteration 16/25 | Loss: 0.00228476
Iteration 17/25 | Loss: 0.00228476
Iteration 18/25 | Loss: 0.00228476
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0022847619839012623, 0.0022847619839012623, 0.0022847619839012623, 0.0022847619839012623, 0.0022847619839012623]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0022847619839012623

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00228476
Iteration 2/1000 | Loss: 0.00005304
Iteration 3/1000 | Loss: 0.00003757
Iteration 4/1000 | Loss: 0.00003368
Iteration 5/1000 | Loss: 0.00003142
Iteration 6/1000 | Loss: 0.00003045
Iteration 7/1000 | Loss: 0.00002910
Iteration 8/1000 | Loss: 0.00002841
Iteration 9/1000 | Loss: 0.00002800
Iteration 10/1000 | Loss: 0.00002763
Iteration 11/1000 | Loss: 0.00002737
Iteration 12/1000 | Loss: 0.00002706
Iteration 13/1000 | Loss: 0.00002685
Iteration 14/1000 | Loss: 0.00002670
Iteration 15/1000 | Loss: 0.00002668
Iteration 16/1000 | Loss: 0.00002668
Iteration 17/1000 | Loss: 0.00002662
Iteration 18/1000 | Loss: 0.00002661
Iteration 19/1000 | Loss: 0.00002661
Iteration 20/1000 | Loss: 0.00002661
Iteration 21/1000 | Loss: 0.00002660
Iteration 22/1000 | Loss: 0.00002660
Iteration 23/1000 | Loss: 0.00002659
Iteration 24/1000 | Loss: 0.00002659
Iteration 25/1000 | Loss: 0.00002658
Iteration 26/1000 | Loss: 0.00002658
Iteration 27/1000 | Loss: 0.00002658
Iteration 28/1000 | Loss: 0.00002658
Iteration 29/1000 | Loss: 0.00002656
Iteration 30/1000 | Loss: 0.00002656
Iteration 31/1000 | Loss: 0.00002656
Iteration 32/1000 | Loss: 0.00002656
Iteration 33/1000 | Loss: 0.00002656
Iteration 34/1000 | Loss: 0.00002656
Iteration 35/1000 | Loss: 0.00002656
Iteration 36/1000 | Loss: 0.00002655
Iteration 37/1000 | Loss: 0.00002655
Iteration 38/1000 | Loss: 0.00002654
Iteration 39/1000 | Loss: 0.00002654
Iteration 40/1000 | Loss: 0.00002654
Iteration 41/1000 | Loss: 0.00002653
Iteration 42/1000 | Loss: 0.00002652
Iteration 43/1000 | Loss: 0.00002652
Iteration 44/1000 | Loss: 0.00002651
Iteration 45/1000 | Loss: 0.00002651
Iteration 46/1000 | Loss: 0.00002650
Iteration 47/1000 | Loss: 0.00002649
Iteration 48/1000 | Loss: 0.00002648
Iteration 49/1000 | Loss: 0.00002648
Iteration 50/1000 | Loss: 0.00002647
Iteration 51/1000 | Loss: 0.00002646
Iteration 52/1000 | Loss: 0.00002646
Iteration 53/1000 | Loss: 0.00002645
Iteration 54/1000 | Loss: 0.00002645
Iteration 55/1000 | Loss: 0.00002645
Iteration 56/1000 | Loss: 0.00002644
Iteration 57/1000 | Loss: 0.00002644
Iteration 58/1000 | Loss: 0.00002644
Iteration 59/1000 | Loss: 0.00002643
Iteration 60/1000 | Loss: 0.00002642
Iteration 61/1000 | Loss: 0.00002642
Iteration 62/1000 | Loss: 0.00002642
Iteration 63/1000 | Loss: 0.00002642
Iteration 64/1000 | Loss: 0.00002642
Iteration 65/1000 | Loss: 0.00002642
Iteration 66/1000 | Loss: 0.00002642
Iteration 67/1000 | Loss: 0.00002641
Iteration 68/1000 | Loss: 0.00002641
Iteration 69/1000 | Loss: 0.00002641
Iteration 70/1000 | Loss: 0.00002641
Iteration 71/1000 | Loss: 0.00002641
Iteration 72/1000 | Loss: 0.00002641
Iteration 73/1000 | Loss: 0.00002641
Iteration 74/1000 | Loss: 0.00002641
Iteration 75/1000 | Loss: 0.00002641
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 75. Stopping optimization.
Last 5 losses: [2.641323044372257e-05, 2.641323044372257e-05, 2.641323044372257e-05, 2.641323044372257e-05, 2.641323044372257e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.641323044372257e-05

Optimization complete. Final v2v error: 4.485018730163574 mm

Highest mean error: 5.050642490386963 mm for frame 83

Lowest mean error: 4.1216888427734375 mm for frame 122

Saving results

Total time: 40.473676681518555
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_40_nl_6353/0000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_40_nl_6353/0000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_40_nl_6353/0000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00818273
Iteration 2/25 | Loss: 0.00151242
Iteration 3/25 | Loss: 0.00141810
Iteration 4/25 | Loss: 0.00139607
Iteration 5/25 | Loss: 0.00138278
Iteration 6/25 | Loss: 0.00137646
Iteration 7/25 | Loss: 0.00138041
Iteration 8/25 | Loss: 0.00137440
Iteration 9/25 | Loss: 0.00137394
Iteration 10/25 | Loss: 0.00137375
Iteration 11/25 | Loss: 0.00137365
Iteration 12/25 | Loss: 0.00137360
Iteration 13/25 | Loss: 0.00137360
Iteration 14/25 | Loss: 0.00137360
Iteration 15/25 | Loss: 0.00137360
Iteration 16/25 | Loss: 0.00137360
Iteration 17/25 | Loss: 0.00137360
Iteration 18/25 | Loss: 0.00137359
Iteration 19/25 | Loss: 0.00137359
Iteration 20/25 | Loss: 0.00137359
Iteration 21/25 | Loss: 0.00137359
Iteration 22/25 | Loss: 0.00137359
Iteration 23/25 | Loss: 0.00137359
Iteration 24/25 | Loss: 0.00137359
Iteration 25/25 | Loss: 0.00137359

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.07878160
Iteration 2/25 | Loss: 0.00151371
Iteration 3/25 | Loss: 0.00151371
Iteration 4/25 | Loss: 0.00151371
Iteration 5/25 | Loss: 0.00151371
Iteration 6/25 | Loss: 0.00151371
Iteration 7/25 | Loss: 0.00151371
Iteration 8/25 | Loss: 0.00151371
Iteration 9/25 | Loss: 0.00151371
Iteration 10/25 | Loss: 0.00151371
Iteration 11/25 | Loss: 0.00151371
Iteration 12/25 | Loss: 0.00151371
Iteration 13/25 | Loss: 0.00151371
Iteration 14/25 | Loss: 0.00151371
Iteration 15/25 | Loss: 0.00151371
Iteration 16/25 | Loss: 0.00151371
Iteration 17/25 | Loss: 0.00151371
Iteration 18/25 | Loss: 0.00151371
Iteration 19/25 | Loss: 0.00151371
Iteration 20/25 | Loss: 0.00151371
Iteration 21/25 | Loss: 0.00151371
Iteration 22/25 | Loss: 0.00151371
Iteration 23/25 | Loss: 0.00151371
Iteration 24/25 | Loss: 0.00151371
Iteration 25/25 | Loss: 0.00151371

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00151371
Iteration 2/1000 | Loss: 0.00003506
Iteration 3/1000 | Loss: 0.00003072
Iteration 4/1000 | Loss: 0.00002872
Iteration 5/1000 | Loss: 0.00002760
Iteration 6/1000 | Loss: 0.00002675
Iteration 7/1000 | Loss: 0.00002618
Iteration 8/1000 | Loss: 0.00002571
Iteration 9/1000 | Loss: 0.00002544
Iteration 10/1000 | Loss: 0.00002542
Iteration 11/1000 | Loss: 0.00002533
Iteration 12/1000 | Loss: 0.00002532
Iteration 13/1000 | Loss: 0.00002517
Iteration 14/1000 | Loss: 0.00002516
Iteration 15/1000 | Loss: 0.00002516
Iteration 16/1000 | Loss: 0.00002515
Iteration 17/1000 | Loss: 0.00002514
Iteration 18/1000 | Loss: 0.00002514
Iteration 19/1000 | Loss: 0.00002510
Iteration 20/1000 | Loss: 0.00002510
Iteration 21/1000 | Loss: 0.00002508
Iteration 22/1000 | Loss: 0.00002508
Iteration 23/1000 | Loss: 0.00002507
Iteration 24/1000 | Loss: 0.00002507
Iteration 25/1000 | Loss: 0.00002507
Iteration 26/1000 | Loss: 0.00002507
Iteration 27/1000 | Loss: 0.00002507
Iteration 28/1000 | Loss: 0.00002506
Iteration 29/1000 | Loss: 0.00002506
Iteration 30/1000 | Loss: 0.00002502
Iteration 31/1000 | Loss: 0.00002502
Iteration 32/1000 | Loss: 0.00002502
Iteration 33/1000 | Loss: 0.00002501
Iteration 34/1000 | Loss: 0.00002501
Iteration 35/1000 | Loss: 0.00002501
Iteration 36/1000 | Loss: 0.00002500
Iteration 37/1000 | Loss: 0.00002500
Iteration 38/1000 | Loss: 0.00002499
Iteration 39/1000 | Loss: 0.00002499
Iteration 40/1000 | Loss: 0.00002499
Iteration 41/1000 | Loss: 0.00002498
Iteration 42/1000 | Loss: 0.00002498
Iteration 43/1000 | Loss: 0.00002498
Iteration 44/1000 | Loss: 0.00002497
Iteration 45/1000 | Loss: 0.00002497
Iteration 46/1000 | Loss: 0.00002496
Iteration 47/1000 | Loss: 0.00002495
Iteration 48/1000 | Loss: 0.00002495
Iteration 49/1000 | Loss: 0.00002495
Iteration 50/1000 | Loss: 0.00002495
Iteration 51/1000 | Loss: 0.00002495
Iteration 52/1000 | Loss: 0.00002495
Iteration 53/1000 | Loss: 0.00002495
Iteration 54/1000 | Loss: 0.00002495
Iteration 55/1000 | Loss: 0.00002494
Iteration 56/1000 | Loss: 0.00002494
Iteration 57/1000 | Loss: 0.00002494
Iteration 58/1000 | Loss: 0.00002494
Iteration 59/1000 | Loss: 0.00002494
Iteration 60/1000 | Loss: 0.00002493
Iteration 61/1000 | Loss: 0.00002493
Iteration 62/1000 | Loss: 0.00002493
Iteration 63/1000 | Loss: 0.00002492
Iteration 64/1000 | Loss: 0.00002492
Iteration 65/1000 | Loss: 0.00002492
Iteration 66/1000 | Loss: 0.00002492
Iteration 67/1000 | Loss: 0.00002491
Iteration 68/1000 | Loss: 0.00002491
Iteration 69/1000 | Loss: 0.00002491
Iteration 70/1000 | Loss: 0.00002491
Iteration 71/1000 | Loss: 0.00002491
Iteration 72/1000 | Loss: 0.00002491
Iteration 73/1000 | Loss: 0.00002491
Iteration 74/1000 | Loss: 0.00002491
Iteration 75/1000 | Loss: 0.00002491
Iteration 76/1000 | Loss: 0.00002491
Iteration 77/1000 | Loss: 0.00002490
Iteration 78/1000 | Loss: 0.00002490
Iteration 79/1000 | Loss: 0.00002490
Iteration 80/1000 | Loss: 0.00002490
Iteration 81/1000 | Loss: 0.00002490
Iteration 82/1000 | Loss: 0.00002490
Iteration 83/1000 | Loss: 0.00002490
Iteration 84/1000 | Loss: 0.00002490
Iteration 85/1000 | Loss: 0.00002490
Iteration 86/1000 | Loss: 0.00002490
Iteration 87/1000 | Loss: 0.00002490
Iteration 88/1000 | Loss: 0.00002490
Iteration 89/1000 | Loss: 0.00002490
Iteration 90/1000 | Loss: 0.00002490
Iteration 91/1000 | Loss: 0.00002490
Iteration 92/1000 | Loss: 0.00002489
Iteration 93/1000 | Loss: 0.00002489
Iteration 94/1000 | Loss: 0.00002489
Iteration 95/1000 | Loss: 0.00002489
Iteration 96/1000 | Loss: 0.00002489
Iteration 97/1000 | Loss: 0.00002489
Iteration 98/1000 | Loss: 0.00002489
Iteration 99/1000 | Loss: 0.00002489
Iteration 100/1000 | Loss: 0.00002489
Iteration 101/1000 | Loss: 0.00002489
Iteration 102/1000 | Loss: 0.00002488
Iteration 103/1000 | Loss: 0.00002488
Iteration 104/1000 | Loss: 0.00002488
Iteration 105/1000 | Loss: 0.00002488
Iteration 106/1000 | Loss: 0.00002488
Iteration 107/1000 | Loss: 0.00002488
Iteration 108/1000 | Loss: 0.00002488
Iteration 109/1000 | Loss: 0.00002488
Iteration 110/1000 | Loss: 0.00002488
Iteration 111/1000 | Loss: 0.00002488
Iteration 112/1000 | Loss: 0.00002488
Iteration 113/1000 | Loss: 0.00002488
Iteration 114/1000 | Loss: 0.00002488
Iteration 115/1000 | Loss: 0.00002488
Iteration 116/1000 | Loss: 0.00002488
Iteration 117/1000 | Loss: 0.00002488
Iteration 118/1000 | Loss: 0.00002488
Iteration 119/1000 | Loss: 0.00002488
Iteration 120/1000 | Loss: 0.00002488
Iteration 121/1000 | Loss: 0.00002488
Iteration 122/1000 | Loss: 0.00002488
Iteration 123/1000 | Loss: 0.00002488
Iteration 124/1000 | Loss: 0.00002488
Iteration 125/1000 | Loss: 0.00002488
Iteration 126/1000 | Loss: 0.00002488
Iteration 127/1000 | Loss: 0.00002488
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 127. Stopping optimization.
Last 5 losses: [2.4880084311007522e-05, 2.4880084311007522e-05, 2.4880084311007522e-05, 2.4880084311007522e-05, 2.4880084311007522e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4880084311007522e-05

Optimization complete. Final v2v error: 4.300008296966553 mm

Highest mean error: 4.598127365112305 mm for frame 150

Lowest mean error: 4.114043712615967 mm for frame 0

Saving results

Total time: 50.2418155670166
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_40_nl_6353/0007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_40_nl_6353/0007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_40_nl_6353/0007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01109233
Iteration 2/25 | Loss: 0.00192759
Iteration 3/25 | Loss: 0.00159818
Iteration 4/25 | Loss: 0.00160056
Iteration 5/25 | Loss: 0.00156272
Iteration 6/25 | Loss: 0.00152494
Iteration 7/25 | Loss: 0.00151749
Iteration 8/25 | Loss: 0.00149950
Iteration 9/25 | Loss: 0.00147525
Iteration 10/25 | Loss: 0.00146854
Iteration 11/25 | Loss: 0.00146369
Iteration 12/25 | Loss: 0.00146391
Iteration 13/25 | Loss: 0.00146290
Iteration 14/25 | Loss: 0.00146577
Iteration 15/25 | Loss: 0.00146248
Iteration 16/25 | Loss: 0.00145910
Iteration 17/25 | Loss: 0.00146154
Iteration 18/25 | Loss: 0.00145944
Iteration 19/25 | Loss: 0.00145820
Iteration 20/25 | Loss: 0.00146148
Iteration 21/25 | Loss: 0.00146302
Iteration 22/25 | Loss: 0.00146080
Iteration 23/25 | Loss: 0.00146068
Iteration 24/25 | Loss: 0.00145798
Iteration 25/25 | Loss: 0.00145789

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.41079462
Iteration 2/25 | Loss: 0.00190966
Iteration 3/25 | Loss: 0.00190962
Iteration 4/25 | Loss: 0.00190961
Iteration 5/25 | Loss: 0.00190961
Iteration 6/25 | Loss: 0.00190961
Iteration 7/25 | Loss: 0.00190961
Iteration 8/25 | Loss: 0.00190961
Iteration 9/25 | Loss: 0.00190961
Iteration 10/25 | Loss: 0.00190961
Iteration 11/25 | Loss: 0.00190961
Iteration 12/25 | Loss: 0.00190961
Iteration 13/25 | Loss: 0.00190961
Iteration 14/25 | Loss: 0.00190961
Iteration 15/25 | Loss: 0.00190961
Iteration 16/25 | Loss: 0.00190961
Iteration 17/25 | Loss: 0.00190961
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0019096133764833212, 0.0019096133764833212, 0.0019096133764833212, 0.0019096133764833212, 0.0019096133764833212]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0019096133764833212

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00190961
Iteration 2/1000 | Loss: 0.00007607
Iteration 3/1000 | Loss: 0.00005635
Iteration 4/1000 | Loss: 0.00020473
Iteration 5/1000 | Loss: 0.00005504
Iteration 6/1000 | Loss: 0.00014349
Iteration 7/1000 | Loss: 0.00016521
Iteration 8/1000 | Loss: 0.00011881
Iteration 9/1000 | Loss: 0.00016637
Iteration 10/1000 | Loss: 0.00014112
Iteration 11/1000 | Loss: 0.00008829
Iteration 12/1000 | Loss: 0.00015303
Iteration 13/1000 | Loss: 0.00013791
Iteration 14/1000 | Loss: 0.00004997
Iteration 15/1000 | Loss: 0.00055696
Iteration 16/1000 | Loss: 0.00008373
Iteration 17/1000 | Loss: 0.00011300
Iteration 18/1000 | Loss: 0.00055836
Iteration 19/1000 | Loss: 0.00021371
Iteration 20/1000 | Loss: 0.00024464
Iteration 21/1000 | Loss: 0.00005020
Iteration 22/1000 | Loss: 0.00004568
Iteration 23/1000 | Loss: 0.00004255
Iteration 24/1000 | Loss: 0.00004113
Iteration 25/1000 | Loss: 0.00003933
Iteration 26/1000 | Loss: 0.00003806
Iteration 27/1000 | Loss: 0.00003734
Iteration 28/1000 | Loss: 0.00003672
Iteration 29/1000 | Loss: 0.00003607
Iteration 30/1000 | Loss: 0.00003562
Iteration 31/1000 | Loss: 0.00003531
Iteration 32/1000 | Loss: 0.00003507
Iteration 33/1000 | Loss: 0.00003489
Iteration 34/1000 | Loss: 0.00003472
Iteration 35/1000 | Loss: 0.00003471
Iteration 36/1000 | Loss: 0.00003470
Iteration 37/1000 | Loss: 0.00003470
Iteration 38/1000 | Loss: 0.00003467
Iteration 39/1000 | Loss: 0.00003463
Iteration 40/1000 | Loss: 0.00003460
Iteration 41/1000 | Loss: 0.00003459
Iteration 42/1000 | Loss: 0.00003459
Iteration 43/1000 | Loss: 0.00003458
Iteration 44/1000 | Loss: 0.00003458
Iteration 45/1000 | Loss: 0.00003457
Iteration 46/1000 | Loss: 0.00003457
Iteration 47/1000 | Loss: 0.00003457
Iteration 48/1000 | Loss: 0.00003456
Iteration 49/1000 | Loss: 0.00003456
Iteration 50/1000 | Loss: 0.00003455
Iteration 51/1000 | Loss: 0.00003454
Iteration 52/1000 | Loss: 0.00003454
Iteration 53/1000 | Loss: 0.00003453
Iteration 54/1000 | Loss: 0.00003453
Iteration 55/1000 | Loss: 0.00003453
Iteration 56/1000 | Loss: 0.00003452
Iteration 57/1000 | Loss: 0.00003452
Iteration 58/1000 | Loss: 0.00003452
Iteration 59/1000 | Loss: 0.00003451
Iteration 60/1000 | Loss: 0.00003449
Iteration 61/1000 | Loss: 0.00003449
Iteration 62/1000 | Loss: 0.00003448
Iteration 63/1000 | Loss: 0.00003448
Iteration 64/1000 | Loss: 0.00003448
Iteration 65/1000 | Loss: 0.00003448
Iteration 66/1000 | Loss: 0.00003448
Iteration 67/1000 | Loss: 0.00003447
Iteration 68/1000 | Loss: 0.00003447
Iteration 69/1000 | Loss: 0.00003447
Iteration 70/1000 | Loss: 0.00003447
Iteration 71/1000 | Loss: 0.00003447
Iteration 72/1000 | Loss: 0.00003447
Iteration 73/1000 | Loss: 0.00003447
Iteration 74/1000 | Loss: 0.00003446
Iteration 75/1000 | Loss: 0.00003446
Iteration 76/1000 | Loss: 0.00003446
Iteration 77/1000 | Loss: 0.00003446
Iteration 78/1000 | Loss: 0.00003446
Iteration 79/1000 | Loss: 0.00003445
Iteration 80/1000 | Loss: 0.00003445
Iteration 81/1000 | Loss: 0.00003445
Iteration 82/1000 | Loss: 0.00003445
Iteration 83/1000 | Loss: 0.00003445
Iteration 84/1000 | Loss: 0.00003445
Iteration 85/1000 | Loss: 0.00003445
Iteration 86/1000 | Loss: 0.00003444
Iteration 87/1000 | Loss: 0.00003444
Iteration 88/1000 | Loss: 0.00003444
Iteration 89/1000 | Loss: 0.00003444
Iteration 90/1000 | Loss: 0.00003444
Iteration 91/1000 | Loss: 0.00003443
Iteration 92/1000 | Loss: 0.00003443
Iteration 93/1000 | Loss: 0.00003443
Iteration 94/1000 | Loss: 0.00003443
Iteration 95/1000 | Loss: 0.00003443
Iteration 96/1000 | Loss: 0.00003442
Iteration 97/1000 | Loss: 0.00003442
Iteration 98/1000 | Loss: 0.00003442
Iteration 99/1000 | Loss: 0.00003442
Iteration 100/1000 | Loss: 0.00003442
Iteration 101/1000 | Loss: 0.00003442
Iteration 102/1000 | Loss: 0.00003442
Iteration 103/1000 | Loss: 0.00003442
Iteration 104/1000 | Loss: 0.00003442
Iteration 105/1000 | Loss: 0.00003442
Iteration 106/1000 | Loss: 0.00003441
Iteration 107/1000 | Loss: 0.00003441
Iteration 108/1000 | Loss: 0.00003441
Iteration 109/1000 | Loss: 0.00003441
Iteration 110/1000 | Loss: 0.00003441
Iteration 111/1000 | Loss: 0.00003441
Iteration 112/1000 | Loss: 0.00003441
Iteration 113/1000 | Loss: 0.00003441
Iteration 114/1000 | Loss: 0.00003441
Iteration 115/1000 | Loss: 0.00003441
Iteration 116/1000 | Loss: 0.00003441
Iteration 117/1000 | Loss: 0.00003441
Iteration 118/1000 | Loss: 0.00003440
Iteration 119/1000 | Loss: 0.00003440
Iteration 120/1000 | Loss: 0.00003440
Iteration 121/1000 | Loss: 0.00003440
Iteration 122/1000 | Loss: 0.00003440
Iteration 123/1000 | Loss: 0.00003440
Iteration 124/1000 | Loss: 0.00003440
Iteration 125/1000 | Loss: 0.00003440
Iteration 126/1000 | Loss: 0.00003439
Iteration 127/1000 | Loss: 0.00003439
Iteration 128/1000 | Loss: 0.00003439
Iteration 129/1000 | Loss: 0.00003439
Iteration 130/1000 | Loss: 0.00003438
Iteration 131/1000 | Loss: 0.00003438
Iteration 132/1000 | Loss: 0.00003437
Iteration 133/1000 | Loss: 0.00003437
Iteration 134/1000 | Loss: 0.00003437
Iteration 135/1000 | Loss: 0.00003437
Iteration 136/1000 | Loss: 0.00003437
Iteration 137/1000 | Loss: 0.00003437
Iteration 138/1000 | Loss: 0.00003437
Iteration 139/1000 | Loss: 0.00003436
Iteration 140/1000 | Loss: 0.00003436
Iteration 141/1000 | Loss: 0.00003436
Iteration 142/1000 | Loss: 0.00003436
Iteration 143/1000 | Loss: 0.00003436
Iteration 144/1000 | Loss: 0.00003436
Iteration 145/1000 | Loss: 0.00003436
Iteration 146/1000 | Loss: 0.00003436
Iteration 147/1000 | Loss: 0.00003436
Iteration 148/1000 | Loss: 0.00003436
Iteration 149/1000 | Loss: 0.00003436
Iteration 150/1000 | Loss: 0.00003436
Iteration 151/1000 | Loss: 0.00003436
Iteration 152/1000 | Loss: 0.00003436
Iteration 153/1000 | Loss: 0.00003436
Iteration 154/1000 | Loss: 0.00003436
Iteration 155/1000 | Loss: 0.00003436
Iteration 156/1000 | Loss: 0.00003436
Iteration 157/1000 | Loss: 0.00003436
Iteration 158/1000 | Loss: 0.00003436
Iteration 159/1000 | Loss: 0.00003436
Iteration 160/1000 | Loss: 0.00003436
Iteration 161/1000 | Loss: 0.00003436
Iteration 162/1000 | Loss: 0.00003436
Iteration 163/1000 | Loss: 0.00003436
Iteration 164/1000 | Loss: 0.00003436
Iteration 165/1000 | Loss: 0.00003436
Iteration 166/1000 | Loss: 0.00003436
Iteration 167/1000 | Loss: 0.00003436
Iteration 168/1000 | Loss: 0.00003436
Iteration 169/1000 | Loss: 0.00003436
Iteration 170/1000 | Loss: 0.00003436
Iteration 171/1000 | Loss: 0.00003436
Iteration 172/1000 | Loss: 0.00003436
Iteration 173/1000 | Loss: 0.00003436
Iteration 174/1000 | Loss: 0.00003436
Iteration 175/1000 | Loss: 0.00003436
Iteration 176/1000 | Loss: 0.00003436
Iteration 177/1000 | Loss: 0.00003436
Iteration 178/1000 | Loss: 0.00003436
Iteration 179/1000 | Loss: 0.00003436
Iteration 180/1000 | Loss: 0.00003436
Iteration 181/1000 | Loss: 0.00003436
Iteration 182/1000 | Loss: 0.00003436
Iteration 183/1000 | Loss: 0.00003436
Iteration 184/1000 | Loss: 0.00003436
Iteration 185/1000 | Loss: 0.00003436
Iteration 186/1000 | Loss: 0.00003436
Iteration 187/1000 | Loss: 0.00003436
Iteration 188/1000 | Loss: 0.00003436
Iteration 189/1000 | Loss: 0.00003436
Iteration 190/1000 | Loss: 0.00003436
Iteration 191/1000 | Loss: 0.00003436
Iteration 192/1000 | Loss: 0.00003436
Iteration 193/1000 | Loss: 0.00003436
Iteration 194/1000 | Loss: 0.00003436
Iteration 195/1000 | Loss: 0.00003436
Iteration 196/1000 | Loss: 0.00003436
Iteration 197/1000 | Loss: 0.00003436
Iteration 198/1000 | Loss: 0.00003436
Iteration 199/1000 | Loss: 0.00003436
Iteration 200/1000 | Loss: 0.00003436
Iteration 201/1000 | Loss: 0.00003436
Iteration 202/1000 | Loss: 0.00003436
Iteration 203/1000 | Loss: 0.00003436
Iteration 204/1000 | Loss: 0.00003436
Iteration 205/1000 | Loss: 0.00003436
Iteration 206/1000 | Loss: 0.00003436
Iteration 207/1000 | Loss: 0.00003436
Iteration 208/1000 | Loss: 0.00003436
Iteration 209/1000 | Loss: 0.00003436
Iteration 210/1000 | Loss: 0.00003436
Iteration 211/1000 | Loss: 0.00003436
Iteration 212/1000 | Loss: 0.00003436
Iteration 213/1000 | Loss: 0.00003436
Iteration 214/1000 | Loss: 0.00003436
Iteration 215/1000 | Loss: 0.00003436
Iteration 216/1000 | Loss: 0.00003436
Iteration 217/1000 | Loss: 0.00003436
Iteration 218/1000 | Loss: 0.00003436
Iteration 219/1000 | Loss: 0.00003436
Iteration 220/1000 | Loss: 0.00003436
Iteration 221/1000 | Loss: 0.00003436
Iteration 222/1000 | Loss: 0.00003436
Iteration 223/1000 | Loss: 0.00003436
Iteration 224/1000 | Loss: 0.00003436
Iteration 225/1000 | Loss: 0.00003436
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 225. Stopping optimization.
Last 5 losses: [3.435508187976666e-05, 3.435508187976666e-05, 3.435508187976666e-05, 3.435508187976666e-05, 3.435508187976666e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.435508187976666e-05

Optimization complete. Final v2v error: 4.952016830444336 mm

Highest mean error: 6.262556552886963 mm for frame 77

Lowest mean error: 3.922586679458618 mm for frame 170

Saving results

Total time: 110.74959444999695
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_40_nl_6353/0004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_40_nl_6353/0004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_40_nl_6353/0004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00491314
Iteration 2/25 | Loss: 0.00153741
Iteration 3/25 | Loss: 0.00142832
Iteration 4/25 | Loss: 0.00140392
Iteration 5/25 | Loss: 0.00139842
Iteration 6/25 | Loss: 0.00139691
Iteration 7/25 | Loss: 0.00139691
Iteration 8/25 | Loss: 0.00139691
Iteration 9/25 | Loss: 0.00139691
Iteration 10/25 | Loss: 0.00139691
Iteration 11/25 | Loss: 0.00139691
Iteration 12/25 | Loss: 0.00139691
Iteration 13/25 | Loss: 0.00139691
Iteration 14/25 | Loss: 0.00139691
Iteration 15/25 | Loss: 0.00139691
Iteration 16/25 | Loss: 0.00139691
Iteration 17/25 | Loss: 0.00139691
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.001396912382915616, 0.001396912382915616, 0.001396912382915616, 0.001396912382915616, 0.001396912382915616]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001396912382915616

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.89659882
Iteration 2/25 | Loss: 0.00149411
Iteration 3/25 | Loss: 0.00149411
Iteration 4/25 | Loss: 0.00149410
Iteration 5/25 | Loss: 0.00149410
Iteration 6/25 | Loss: 0.00149410
Iteration 7/25 | Loss: 0.00149410
Iteration 8/25 | Loss: 0.00149410
Iteration 9/25 | Loss: 0.00149410
Iteration 10/25 | Loss: 0.00149410
Iteration 11/25 | Loss: 0.00149410
Iteration 12/25 | Loss: 0.00149410
Iteration 13/25 | Loss: 0.00149410
Iteration 14/25 | Loss: 0.00149410
Iteration 15/25 | Loss: 0.00149410
Iteration 16/25 | Loss: 0.00149410
Iteration 17/25 | Loss: 0.00149410
Iteration 18/25 | Loss: 0.00149410
Iteration 19/25 | Loss: 0.00149410
Iteration 20/25 | Loss: 0.00149410
Iteration 21/25 | Loss: 0.00149410
Iteration 22/25 | Loss: 0.00149410
Iteration 23/25 | Loss: 0.00149410
Iteration 24/25 | Loss: 0.00149410
Iteration 25/25 | Loss: 0.00149410

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00149410
Iteration 2/1000 | Loss: 0.00004476
Iteration 3/1000 | Loss: 0.00003720
Iteration 4/1000 | Loss: 0.00003478
Iteration 5/1000 | Loss: 0.00003328
Iteration 6/1000 | Loss: 0.00003236
Iteration 7/1000 | Loss: 0.00003169
Iteration 8/1000 | Loss: 0.00003124
Iteration 9/1000 | Loss: 0.00003089
Iteration 10/1000 | Loss: 0.00003077
Iteration 11/1000 | Loss: 0.00003075
Iteration 12/1000 | Loss: 0.00003074
Iteration 13/1000 | Loss: 0.00003074
Iteration 14/1000 | Loss: 0.00003066
Iteration 15/1000 | Loss: 0.00003065
Iteration 16/1000 | Loss: 0.00003063
Iteration 17/1000 | Loss: 0.00003062
Iteration 18/1000 | Loss: 0.00003062
Iteration 19/1000 | Loss: 0.00003061
Iteration 20/1000 | Loss: 0.00003061
Iteration 21/1000 | Loss: 0.00003060
Iteration 22/1000 | Loss: 0.00003060
Iteration 23/1000 | Loss: 0.00003060
Iteration 24/1000 | Loss: 0.00003060
Iteration 25/1000 | Loss: 0.00003060
Iteration 26/1000 | Loss: 0.00003060
Iteration 27/1000 | Loss: 0.00003059
Iteration 28/1000 | Loss: 0.00003059
Iteration 29/1000 | Loss: 0.00003059
Iteration 30/1000 | Loss: 0.00003059
Iteration 31/1000 | Loss: 0.00003059
Iteration 32/1000 | Loss: 0.00003059
Iteration 33/1000 | Loss: 0.00003059
Iteration 34/1000 | Loss: 0.00003059
Iteration 35/1000 | Loss: 0.00003059
Iteration 36/1000 | Loss: 0.00003059
Iteration 37/1000 | Loss: 0.00003058
Iteration 38/1000 | Loss: 0.00003058
Iteration 39/1000 | Loss: 0.00003058
Iteration 40/1000 | Loss: 0.00003057
Iteration 41/1000 | Loss: 0.00003057
Iteration 42/1000 | Loss: 0.00003056
Iteration 43/1000 | Loss: 0.00003056
Iteration 44/1000 | Loss: 0.00003056
Iteration 45/1000 | Loss: 0.00003056
Iteration 46/1000 | Loss: 0.00003055
Iteration 47/1000 | Loss: 0.00003055
Iteration 48/1000 | Loss: 0.00003055
Iteration 49/1000 | Loss: 0.00003055
Iteration 50/1000 | Loss: 0.00003055
Iteration 51/1000 | Loss: 0.00003054
Iteration 52/1000 | Loss: 0.00003054
Iteration 53/1000 | Loss: 0.00003054
Iteration 54/1000 | Loss: 0.00003053
Iteration 55/1000 | Loss: 0.00003053
Iteration 56/1000 | Loss: 0.00003053
Iteration 57/1000 | Loss: 0.00003053
Iteration 58/1000 | Loss: 0.00003053
Iteration 59/1000 | Loss: 0.00003053
Iteration 60/1000 | Loss: 0.00003053
Iteration 61/1000 | Loss: 0.00003053
Iteration 62/1000 | Loss: 0.00003053
Iteration 63/1000 | Loss: 0.00003052
Iteration 64/1000 | Loss: 0.00003052
Iteration 65/1000 | Loss: 0.00003052
Iteration 66/1000 | Loss: 0.00003052
Iteration 67/1000 | Loss: 0.00003052
Iteration 68/1000 | Loss: 0.00003052
Iteration 69/1000 | Loss: 0.00003052
Iteration 70/1000 | Loss: 0.00003051
Iteration 71/1000 | Loss: 0.00003051
Iteration 72/1000 | Loss: 0.00003050
Iteration 73/1000 | Loss: 0.00003050
Iteration 74/1000 | Loss: 0.00003050
Iteration 75/1000 | Loss: 0.00003049
Iteration 76/1000 | Loss: 0.00003049
Iteration 77/1000 | Loss: 0.00003049
Iteration 78/1000 | Loss: 0.00003049
Iteration 79/1000 | Loss: 0.00003048
Iteration 80/1000 | Loss: 0.00003048
Iteration 81/1000 | Loss: 0.00003048
Iteration 82/1000 | Loss: 0.00003048
Iteration 83/1000 | Loss: 0.00003048
Iteration 84/1000 | Loss: 0.00003047
Iteration 85/1000 | Loss: 0.00003047
Iteration 86/1000 | Loss: 0.00003047
Iteration 87/1000 | Loss: 0.00003047
Iteration 88/1000 | Loss: 0.00003046
Iteration 89/1000 | Loss: 0.00003046
Iteration 90/1000 | Loss: 0.00003046
Iteration 91/1000 | Loss: 0.00003045
Iteration 92/1000 | Loss: 0.00003045
Iteration 93/1000 | Loss: 0.00003045
Iteration 94/1000 | Loss: 0.00003045
Iteration 95/1000 | Loss: 0.00003044
Iteration 96/1000 | Loss: 0.00003044
Iteration 97/1000 | Loss: 0.00003044
Iteration 98/1000 | Loss: 0.00003044
Iteration 99/1000 | Loss: 0.00003044
Iteration 100/1000 | Loss: 0.00003044
Iteration 101/1000 | Loss: 0.00003044
Iteration 102/1000 | Loss: 0.00003044
Iteration 103/1000 | Loss: 0.00003043
Iteration 104/1000 | Loss: 0.00003043
Iteration 105/1000 | Loss: 0.00003043
Iteration 106/1000 | Loss: 0.00003043
Iteration 107/1000 | Loss: 0.00003043
Iteration 108/1000 | Loss: 0.00003043
Iteration 109/1000 | Loss: 0.00003042
Iteration 110/1000 | Loss: 0.00003042
Iteration 111/1000 | Loss: 0.00003042
Iteration 112/1000 | Loss: 0.00003042
Iteration 113/1000 | Loss: 0.00003042
Iteration 114/1000 | Loss: 0.00003042
Iteration 115/1000 | Loss: 0.00003041
Iteration 116/1000 | Loss: 0.00003041
Iteration 117/1000 | Loss: 0.00003041
Iteration 118/1000 | Loss: 0.00003041
Iteration 119/1000 | Loss: 0.00003041
Iteration 120/1000 | Loss: 0.00003040
Iteration 121/1000 | Loss: 0.00003040
Iteration 122/1000 | Loss: 0.00003040
Iteration 123/1000 | Loss: 0.00003040
Iteration 124/1000 | Loss: 0.00003040
Iteration 125/1000 | Loss: 0.00003039
Iteration 126/1000 | Loss: 0.00003039
Iteration 127/1000 | Loss: 0.00003039
Iteration 128/1000 | Loss: 0.00003039
Iteration 129/1000 | Loss: 0.00003039
Iteration 130/1000 | Loss: 0.00003038
Iteration 131/1000 | Loss: 0.00003038
Iteration 132/1000 | Loss: 0.00003038
Iteration 133/1000 | Loss: 0.00003038
Iteration 134/1000 | Loss: 0.00003038
Iteration 135/1000 | Loss: 0.00003038
Iteration 136/1000 | Loss: 0.00003038
Iteration 137/1000 | Loss: 0.00003037
Iteration 138/1000 | Loss: 0.00003037
Iteration 139/1000 | Loss: 0.00003037
Iteration 140/1000 | Loss: 0.00003037
Iteration 141/1000 | Loss: 0.00003037
Iteration 142/1000 | Loss: 0.00003037
Iteration 143/1000 | Loss: 0.00003037
Iteration 144/1000 | Loss: 0.00003037
Iteration 145/1000 | Loss: 0.00003037
Iteration 146/1000 | Loss: 0.00003037
Iteration 147/1000 | Loss: 0.00003036
Iteration 148/1000 | Loss: 0.00003036
Iteration 149/1000 | Loss: 0.00003036
Iteration 150/1000 | Loss: 0.00003036
Iteration 151/1000 | Loss: 0.00003036
Iteration 152/1000 | Loss: 0.00003036
Iteration 153/1000 | Loss: 0.00003036
Iteration 154/1000 | Loss: 0.00003036
Iteration 155/1000 | Loss: 0.00003036
Iteration 156/1000 | Loss: 0.00003036
Iteration 157/1000 | Loss: 0.00003036
Iteration 158/1000 | Loss: 0.00003036
Iteration 159/1000 | Loss: 0.00003035
Iteration 160/1000 | Loss: 0.00003035
Iteration 161/1000 | Loss: 0.00003035
Iteration 162/1000 | Loss: 0.00003035
Iteration 163/1000 | Loss: 0.00003035
Iteration 164/1000 | Loss: 0.00003035
Iteration 165/1000 | Loss: 0.00003035
Iteration 166/1000 | Loss: 0.00003035
Iteration 167/1000 | Loss: 0.00003035
Iteration 168/1000 | Loss: 0.00003035
Iteration 169/1000 | Loss: 0.00003035
Iteration 170/1000 | Loss: 0.00003035
Iteration 171/1000 | Loss: 0.00003035
Iteration 172/1000 | Loss: 0.00003035
Iteration 173/1000 | Loss: 0.00003035
Iteration 174/1000 | Loss: 0.00003035
Iteration 175/1000 | Loss: 0.00003035
Iteration 176/1000 | Loss: 0.00003035
Iteration 177/1000 | Loss: 0.00003035
Iteration 178/1000 | Loss: 0.00003035
Iteration 179/1000 | Loss: 0.00003035
Iteration 180/1000 | Loss: 0.00003035
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 180. Stopping optimization.
Last 5 losses: [3.0345641789608635e-05, 3.0345641789608635e-05, 3.0345641789608635e-05, 3.0345641789608635e-05, 3.0345641789608635e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.0345641789608635e-05

Optimization complete. Final v2v error: 4.761545658111572 mm

Highest mean error: 5.114049434661865 mm for frame 198

Lowest mean error: 4.311692237854004 mm for frame 227

Saving results

Total time: 40.01686239242554
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_40_nl_6353/0009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_40_nl_6353/0009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_40_nl_6353/0009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01171617
Iteration 2/25 | Loss: 0.00326763
Iteration 3/25 | Loss: 0.00228698
Iteration 4/25 | Loss: 0.00219661
Iteration 5/25 | Loss: 0.00214455
Iteration 6/25 | Loss: 0.00189918
Iteration 7/25 | Loss: 0.00184252
Iteration 8/25 | Loss: 0.00179337
Iteration 9/25 | Loss: 0.00177170
Iteration 10/25 | Loss: 0.00175677
Iteration 11/25 | Loss: 0.00177166
Iteration 12/25 | Loss: 0.00175436
Iteration 13/25 | Loss: 0.00174676
Iteration 14/25 | Loss: 0.00174403
Iteration 15/25 | Loss: 0.00174441
Iteration 16/25 | Loss: 0.00174539
Iteration 17/25 | Loss: 0.00173284
Iteration 18/25 | Loss: 0.00172075
Iteration 19/25 | Loss: 0.00171209
Iteration 20/25 | Loss: 0.00170996
Iteration 21/25 | Loss: 0.00170441
Iteration 22/25 | Loss: 0.00170384
Iteration 23/25 | Loss: 0.00170628
Iteration 24/25 | Loss: 0.00170492
Iteration 25/25 | Loss: 0.00170419

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.95145917
Iteration 2/25 | Loss: 0.00438963
Iteration 3/25 | Loss: 0.00438963
Iteration 4/25 | Loss: 0.00438963
Iteration 5/25 | Loss: 0.00438963
Iteration 6/25 | Loss: 0.00438963
Iteration 7/25 | Loss: 0.00438963
Iteration 8/25 | Loss: 0.00438963
Iteration 9/25 | Loss: 0.00438963
Iteration 10/25 | Loss: 0.00438963
Iteration 11/25 | Loss: 0.00438963
Iteration 12/25 | Loss: 0.00438963
Iteration 13/25 | Loss: 0.00438963
Iteration 14/25 | Loss: 0.00438963
Iteration 15/25 | Loss: 0.00438963
Iteration 16/25 | Loss: 0.00438963
Iteration 17/25 | Loss: 0.00438963
Iteration 18/25 | Loss: 0.00438963
Iteration 19/25 | Loss: 0.00438963
Iteration 20/25 | Loss: 0.00438963
Iteration 21/25 | Loss: 0.00438963
Iteration 22/25 | Loss: 0.00438963
Iteration 23/25 | Loss: 0.00438963
Iteration 24/25 | Loss: 0.00438963
Iteration 25/25 | Loss: 0.00438963

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00438963
Iteration 2/1000 | Loss: 0.00082959
Iteration 3/1000 | Loss: 0.00043542
Iteration 4/1000 | Loss: 0.00092314
Iteration 5/1000 | Loss: 0.00140714
Iteration 6/1000 | Loss: 0.00088414
Iteration 7/1000 | Loss: 0.00038914
Iteration 8/1000 | Loss: 0.00074727
Iteration 9/1000 | Loss: 0.00027587
Iteration 10/1000 | Loss: 0.00044464
Iteration 11/1000 | Loss: 0.00157958
Iteration 12/1000 | Loss: 0.00139664
Iteration 13/1000 | Loss: 0.00272494
Iteration 14/1000 | Loss: 0.00263797
Iteration 15/1000 | Loss: 0.00154311
Iteration 16/1000 | Loss: 0.00124098
Iteration 17/1000 | Loss: 0.00154909
Iteration 18/1000 | Loss: 0.00087953
Iteration 19/1000 | Loss: 0.00110101
Iteration 20/1000 | Loss: 0.00094491
Iteration 21/1000 | Loss: 0.00076104
Iteration 22/1000 | Loss: 0.00044857
Iteration 23/1000 | Loss: 0.00057354
Iteration 24/1000 | Loss: 0.00110586
Iteration 25/1000 | Loss: 0.00092368
Iteration 26/1000 | Loss: 0.00112400
Iteration 27/1000 | Loss: 0.00066093
Iteration 28/1000 | Loss: 0.00062492
Iteration 29/1000 | Loss: 0.00050438
Iteration 30/1000 | Loss: 0.00153890
Iteration 31/1000 | Loss: 0.00059820
Iteration 32/1000 | Loss: 0.00169096
Iteration 33/1000 | Loss: 0.00077156
Iteration 34/1000 | Loss: 0.00175638
Iteration 35/1000 | Loss: 0.00075556
Iteration 36/1000 | Loss: 0.00156742
Iteration 37/1000 | Loss: 0.00476950
Iteration 38/1000 | Loss: 0.00199079
Iteration 39/1000 | Loss: 0.00084789
Iteration 40/1000 | Loss: 0.00321520
Iteration 41/1000 | Loss: 0.00083272
Iteration 42/1000 | Loss: 0.00031773
Iteration 43/1000 | Loss: 0.00187719
Iteration 44/1000 | Loss: 0.00234560
Iteration 45/1000 | Loss: 0.00298585
Iteration 46/1000 | Loss: 0.00342601
Iteration 47/1000 | Loss: 0.00359314
Iteration 48/1000 | Loss: 0.00262048
Iteration 49/1000 | Loss: 0.00302337
Iteration 50/1000 | Loss: 0.00120793
Iteration 51/1000 | Loss: 0.00150352
Iteration 52/1000 | Loss: 0.00101318
Iteration 53/1000 | Loss: 0.00135025
Iteration 54/1000 | Loss: 0.00110177
Iteration 55/1000 | Loss: 0.00306532
Iteration 56/1000 | Loss: 0.00207026
Iteration 57/1000 | Loss: 0.00340896
Iteration 58/1000 | Loss: 0.00183272
Iteration 59/1000 | Loss: 0.00206959
Iteration 60/1000 | Loss: 0.00122517
Iteration 61/1000 | Loss: 0.00072306
Iteration 62/1000 | Loss: 0.00061428
Iteration 63/1000 | Loss: 0.00048723
Iteration 64/1000 | Loss: 0.00137488
Iteration 65/1000 | Loss: 0.00086990
Iteration 66/1000 | Loss: 0.00228942
Iteration 67/1000 | Loss: 0.00098307
Iteration 68/1000 | Loss: 0.00246131
Iteration 69/1000 | Loss: 0.00101399
Iteration 70/1000 | Loss: 0.00239048
Iteration 71/1000 | Loss: 0.00122514
Iteration 72/1000 | Loss: 0.00407942
Iteration 73/1000 | Loss: 0.00639398
Iteration 74/1000 | Loss: 0.00464216
Iteration 75/1000 | Loss: 0.00461617
Iteration 76/1000 | Loss: 0.00199076
Iteration 77/1000 | Loss: 0.00234728
Iteration 78/1000 | Loss: 0.00225020
Iteration 79/1000 | Loss: 0.00156699
Iteration 80/1000 | Loss: 0.00128643
Iteration 81/1000 | Loss: 0.00060096
Iteration 82/1000 | Loss: 0.00036380
Iteration 83/1000 | Loss: 0.00140555
Iteration 84/1000 | Loss: 0.00084209
Iteration 85/1000 | Loss: 0.00088458
Iteration 86/1000 | Loss: 0.00064814
Iteration 87/1000 | Loss: 0.00109891
Iteration 88/1000 | Loss: 0.00124681
Iteration 89/1000 | Loss: 0.00071941
Iteration 90/1000 | Loss: 0.00020176
Iteration 91/1000 | Loss: 0.00074143
Iteration 92/1000 | Loss: 0.00022777
Iteration 93/1000 | Loss: 0.00072236
Iteration 94/1000 | Loss: 0.00136664
Iteration 95/1000 | Loss: 0.00101945
Iteration 96/1000 | Loss: 0.00161049
Iteration 97/1000 | Loss: 0.00115756
Iteration 98/1000 | Loss: 0.00060271
Iteration 99/1000 | Loss: 0.00041858
Iteration 100/1000 | Loss: 0.00024454
Iteration 101/1000 | Loss: 0.00014779
Iteration 102/1000 | Loss: 0.00191971
Iteration 103/1000 | Loss: 0.00021045
Iteration 104/1000 | Loss: 0.00070718
Iteration 105/1000 | Loss: 0.00240998
Iteration 106/1000 | Loss: 0.00146242
Iteration 107/1000 | Loss: 0.00041363
Iteration 108/1000 | Loss: 0.00085040
Iteration 109/1000 | Loss: 0.00211182
Iteration 110/1000 | Loss: 0.00165798
Iteration 111/1000 | Loss: 0.00111625
Iteration 112/1000 | Loss: 0.00046649
Iteration 113/1000 | Loss: 0.00048650
Iteration 114/1000 | Loss: 0.00052890
Iteration 115/1000 | Loss: 0.00062622
Iteration 116/1000 | Loss: 0.00029172
Iteration 117/1000 | Loss: 0.00053089
Iteration 118/1000 | Loss: 0.00042775
Iteration 119/1000 | Loss: 0.00054353
Iteration 120/1000 | Loss: 0.00108089
Iteration 121/1000 | Loss: 0.00341177
Iteration 122/1000 | Loss: 0.00106544
Iteration 123/1000 | Loss: 0.00136342
Iteration 124/1000 | Loss: 0.00104217
Iteration 125/1000 | Loss: 0.00084307
Iteration 126/1000 | Loss: 0.00079220
Iteration 127/1000 | Loss: 0.00148459
Iteration 128/1000 | Loss: 0.00198987
Iteration 129/1000 | Loss: 0.00121796
Iteration 130/1000 | Loss: 0.00110133
Iteration 131/1000 | Loss: 0.00115143
Iteration 132/1000 | Loss: 0.00072301
Iteration 133/1000 | Loss: 0.00086316
Iteration 134/1000 | Loss: 0.00072385
Iteration 135/1000 | Loss: 0.00084020
Iteration 136/1000 | Loss: 0.00070044
Iteration 137/1000 | Loss: 0.00014657
Iteration 138/1000 | Loss: 0.00025840
Iteration 139/1000 | Loss: 0.00012519
Iteration 140/1000 | Loss: 0.00133166
Iteration 141/1000 | Loss: 0.00054009
Iteration 142/1000 | Loss: 0.00023396
Iteration 143/1000 | Loss: 0.00017910
Iteration 144/1000 | Loss: 0.00104831
Iteration 145/1000 | Loss: 0.00043637
Iteration 146/1000 | Loss: 0.00016726
Iteration 147/1000 | Loss: 0.00104754
Iteration 148/1000 | Loss: 0.00036920
Iteration 149/1000 | Loss: 0.00013159
Iteration 150/1000 | Loss: 0.00012384
Iteration 151/1000 | Loss: 0.00099626
Iteration 152/1000 | Loss: 0.00012088
Iteration 153/1000 | Loss: 0.00011154
Iteration 154/1000 | Loss: 0.00060356
Iteration 155/1000 | Loss: 0.00015398
Iteration 156/1000 | Loss: 0.00010600
Iteration 157/1000 | Loss: 0.00009761
Iteration 158/1000 | Loss: 0.00009507
Iteration 159/1000 | Loss: 0.00009366
Iteration 160/1000 | Loss: 0.00009271
Iteration 161/1000 | Loss: 0.00009185
Iteration 162/1000 | Loss: 0.00009268
Iteration 163/1000 | Loss: 0.00069441
Iteration 164/1000 | Loss: 0.00010565
Iteration 165/1000 | Loss: 0.00009692
Iteration 166/1000 | Loss: 0.00009166
Iteration 167/1000 | Loss: 0.00008937
Iteration 168/1000 | Loss: 0.00008849
Iteration 169/1000 | Loss: 0.00008801
Iteration 170/1000 | Loss: 0.00008777
Iteration 171/1000 | Loss: 0.00008729
Iteration 172/1000 | Loss: 0.00008703
Iteration 173/1000 | Loss: 0.00008683
Iteration 174/1000 | Loss: 0.00008673
Iteration 175/1000 | Loss: 0.00008670
Iteration 176/1000 | Loss: 0.00008669
Iteration 177/1000 | Loss: 0.00008667
Iteration 178/1000 | Loss: 0.00008664
Iteration 179/1000 | Loss: 0.00008663
Iteration 180/1000 | Loss: 0.00008663
Iteration 181/1000 | Loss: 0.00008663
Iteration 182/1000 | Loss: 0.00008662
Iteration 183/1000 | Loss: 0.00008662
Iteration 184/1000 | Loss: 0.00008662
Iteration 185/1000 | Loss: 0.00008661
Iteration 186/1000 | Loss: 0.00008660
Iteration 187/1000 | Loss: 0.00008659
Iteration 188/1000 | Loss: 0.00008656
Iteration 189/1000 | Loss: 0.00008656
Iteration 190/1000 | Loss: 0.00008652
Iteration 191/1000 | Loss: 0.00008652
Iteration 192/1000 | Loss: 0.00008651
Iteration 193/1000 | Loss: 0.00008651
Iteration 194/1000 | Loss: 0.00008651
Iteration 195/1000 | Loss: 0.00008651
Iteration 196/1000 | Loss: 0.00008650
Iteration 197/1000 | Loss: 0.00008649
Iteration 198/1000 | Loss: 0.00008649
Iteration 199/1000 | Loss: 0.00008649
Iteration 200/1000 | Loss: 0.00008647
Iteration 201/1000 | Loss: 0.00008647
Iteration 202/1000 | Loss: 0.00008646
Iteration 203/1000 | Loss: 0.00008642
Iteration 204/1000 | Loss: 0.00008642
Iteration 205/1000 | Loss: 0.00008641
Iteration 206/1000 | Loss: 0.00008641
Iteration 207/1000 | Loss: 0.00008641
Iteration 208/1000 | Loss: 0.00008641
Iteration 209/1000 | Loss: 0.00008640
Iteration 210/1000 | Loss: 0.00008640
Iteration 211/1000 | Loss: 0.00008640
Iteration 212/1000 | Loss: 0.00008639
Iteration 213/1000 | Loss: 0.00008639
Iteration 214/1000 | Loss: 0.00008639
Iteration 215/1000 | Loss: 0.00008639
Iteration 216/1000 | Loss: 0.00008639
Iteration 217/1000 | Loss: 0.00008639
Iteration 218/1000 | Loss: 0.00008638
Iteration 219/1000 | Loss: 0.00008638
Iteration 220/1000 | Loss: 0.00008638
Iteration 221/1000 | Loss: 0.00008637
Iteration 222/1000 | Loss: 0.00008637
Iteration 223/1000 | Loss: 0.00008637
Iteration 224/1000 | Loss: 0.00008637
Iteration 225/1000 | Loss: 0.00008637
Iteration 226/1000 | Loss: 0.00008637
Iteration 227/1000 | Loss: 0.00008636
Iteration 228/1000 | Loss: 0.00008636
Iteration 229/1000 | Loss: 0.00008636
Iteration 230/1000 | Loss: 0.00008635
Iteration 231/1000 | Loss: 0.00008635
Iteration 232/1000 | Loss: 0.00008635
Iteration 233/1000 | Loss: 0.00008635
Iteration 234/1000 | Loss: 0.00008635
Iteration 235/1000 | Loss: 0.00008635
Iteration 236/1000 | Loss: 0.00008635
Iteration 237/1000 | Loss: 0.00008635
Iteration 238/1000 | Loss: 0.00008635
Iteration 239/1000 | Loss: 0.00008634
Iteration 240/1000 | Loss: 0.00008634
Iteration 241/1000 | Loss: 0.00008634
Iteration 242/1000 | Loss: 0.00008634
Iteration 243/1000 | Loss: 0.00008633
Iteration 244/1000 | Loss: 0.00008633
Iteration 245/1000 | Loss: 0.00008633
Iteration 246/1000 | Loss: 0.00008633
Iteration 247/1000 | Loss: 0.00008633
Iteration 248/1000 | Loss: 0.00008633
Iteration 249/1000 | Loss: 0.00008633
Iteration 250/1000 | Loss: 0.00008633
Iteration 251/1000 | Loss: 0.00008633
Iteration 252/1000 | Loss: 0.00008633
Iteration 253/1000 | Loss: 0.00008633
Iteration 254/1000 | Loss: 0.00008633
Iteration 255/1000 | Loss: 0.00008633
Iteration 256/1000 | Loss: 0.00008633
Iteration 257/1000 | Loss: 0.00008633
Iteration 258/1000 | Loss: 0.00008633
Iteration 259/1000 | Loss: 0.00008632
Iteration 260/1000 | Loss: 0.00008632
Iteration 261/1000 | Loss: 0.00008633
Iteration 262/1000 | Loss: 0.00008632
Iteration 263/1000 | Loss: 0.00008632
Iteration 264/1000 | Loss: 0.00008632
Iteration 265/1000 | Loss: 0.00008633
Iteration 266/1000 | Loss: 0.00008632
Iteration 267/1000 | Loss: 0.00008632
Iteration 268/1000 | Loss: 0.00008632
Iteration 269/1000 | Loss: 0.00008632
Iteration 270/1000 | Loss: 0.00008632
Iteration 271/1000 | Loss: 0.00008632
Iteration 272/1000 | Loss: 0.00008633
Iteration 273/1000 | Loss: 0.00008633
Iteration 274/1000 | Loss: 0.00008632
Iteration 275/1000 | Loss: 0.00008632
Iteration 276/1000 | Loss: 0.00008632
Iteration 277/1000 | Loss: 0.00008633
Iteration 278/1000 | Loss: 0.00008633
Iteration 279/1000 | Loss: 0.00008632
Iteration 280/1000 | Loss: 0.00008632
Iteration 281/1000 | Loss: 0.00008632
Iteration 282/1000 | Loss: 0.00008632
Iteration 283/1000 | Loss: 0.00008632
Iteration 284/1000 | Loss: 0.00008632
Iteration 285/1000 | Loss: 0.00008632
Iteration 286/1000 | Loss: 0.00008632
Iteration 287/1000 | Loss: 0.00008633
Iteration 288/1000 | Loss: 0.00008632
Iteration 289/1000 | Loss: 0.00008633
Iteration 290/1000 | Loss: 0.00008633
Iteration 291/1000 | Loss: 0.00008633
Iteration 292/1000 | Loss: 0.00008632
Iteration 293/1000 | Loss: 0.00008632
Iteration 294/1000 | Loss: 0.00008633
Iteration 295/1000 | Loss: 0.00008633
Iteration 296/1000 | Loss: 0.00008632
Iteration 297/1000 | Loss: 0.00008632
Iteration 298/1000 | Loss: 0.00008632
Iteration 299/1000 | Loss: 0.00008632
Iteration 300/1000 | Loss: 0.00008632
Iteration 301/1000 | Loss: 0.00008632
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 301. Stopping optimization.
Last 5 losses: [8.632499520899728e-05, 8.632499520899728e-05, 8.632499520899728e-05, 8.632499520899728e-05, 8.632499520899728e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.632499520899728e-05

Optimization complete. Final v2v error: 5.672069072723389 mm

Highest mean error: 13.737797737121582 mm for frame 44

Lowest mean error: 4.367794990539551 mm for frame 1

Saving results

Total time: 292.64061093330383
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_40_nl_6353/0011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_40_nl_6353/0011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_40_nl_6353/0011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00527211
Iteration 2/25 | Loss: 0.00157049
Iteration 3/25 | Loss: 0.00140765
Iteration 4/25 | Loss: 0.00139021
Iteration 5/25 | Loss: 0.00138382
Iteration 6/25 | Loss: 0.00138213
Iteration 7/25 | Loss: 0.00138171
Iteration 8/25 | Loss: 0.00138171
Iteration 9/25 | Loss: 0.00138171
Iteration 10/25 | Loss: 0.00138171
Iteration 11/25 | Loss: 0.00138171
Iteration 12/25 | Loss: 0.00138171
Iteration 13/25 | Loss: 0.00138171
Iteration 14/25 | Loss: 0.00138171
Iteration 15/25 | Loss: 0.00138171
Iteration 16/25 | Loss: 0.00138171
Iteration 17/25 | Loss: 0.00138171
Iteration 18/25 | Loss: 0.00138171
Iteration 19/25 | Loss: 0.00138171
Iteration 20/25 | Loss: 0.00138171
Iteration 21/25 | Loss: 0.00138171
Iteration 22/25 | Loss: 0.00138171
Iteration 23/25 | Loss: 0.00138171
Iteration 24/25 | Loss: 0.00138171
Iteration 25/25 | Loss: 0.00138171

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.58061433
Iteration 2/25 | Loss: 0.00165036
Iteration 3/25 | Loss: 0.00165034
Iteration 4/25 | Loss: 0.00165034
Iteration 5/25 | Loss: 0.00165034
Iteration 6/25 | Loss: 0.00165034
Iteration 7/25 | Loss: 0.00165034
Iteration 8/25 | Loss: 0.00165034
Iteration 9/25 | Loss: 0.00165034
Iteration 10/25 | Loss: 0.00165034
Iteration 11/25 | Loss: 0.00165034
Iteration 12/25 | Loss: 0.00165034
Iteration 13/25 | Loss: 0.00165034
Iteration 14/25 | Loss: 0.00165034
Iteration 15/25 | Loss: 0.00165034
Iteration 16/25 | Loss: 0.00165034
Iteration 17/25 | Loss: 0.00165034
Iteration 18/25 | Loss: 0.00165034
Iteration 19/25 | Loss: 0.00165034
Iteration 20/25 | Loss: 0.00165034
Iteration 21/25 | Loss: 0.00165034
Iteration 22/25 | Loss: 0.00165034
Iteration 23/25 | Loss: 0.00165034
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0016503360820934176, 0.0016503360820934176, 0.0016503360820934176, 0.0016503360820934176, 0.0016503360820934176]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0016503360820934176

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00165034
Iteration 2/1000 | Loss: 0.00004544
Iteration 3/1000 | Loss: 0.00003000
Iteration 4/1000 | Loss: 0.00002676
Iteration 5/1000 | Loss: 0.00002555
Iteration 6/1000 | Loss: 0.00002487
Iteration 7/1000 | Loss: 0.00002448
Iteration 8/1000 | Loss: 0.00002415
Iteration 9/1000 | Loss: 0.00002397
Iteration 10/1000 | Loss: 0.00002376
Iteration 11/1000 | Loss: 0.00002372
Iteration 12/1000 | Loss: 0.00002366
Iteration 13/1000 | Loss: 0.00002365
Iteration 14/1000 | Loss: 0.00002364
Iteration 15/1000 | Loss: 0.00002355
Iteration 16/1000 | Loss: 0.00002352
Iteration 17/1000 | Loss: 0.00002352
Iteration 18/1000 | Loss: 0.00002351
Iteration 19/1000 | Loss: 0.00002351
Iteration 20/1000 | Loss: 0.00002350
Iteration 21/1000 | Loss: 0.00002350
Iteration 22/1000 | Loss: 0.00002350
Iteration 23/1000 | Loss: 0.00002350
Iteration 24/1000 | Loss: 0.00002350
Iteration 25/1000 | Loss: 0.00002350
Iteration 26/1000 | Loss: 0.00002350
Iteration 27/1000 | Loss: 0.00002349
Iteration 28/1000 | Loss: 0.00002349
Iteration 29/1000 | Loss: 0.00002349
Iteration 30/1000 | Loss: 0.00002349
Iteration 31/1000 | Loss: 0.00002349
Iteration 32/1000 | Loss: 0.00002349
Iteration 33/1000 | Loss: 0.00002349
Iteration 34/1000 | Loss: 0.00002349
Iteration 35/1000 | Loss: 0.00002348
Iteration 36/1000 | Loss: 0.00002348
Iteration 37/1000 | Loss: 0.00002348
Iteration 38/1000 | Loss: 0.00002346
Iteration 39/1000 | Loss: 0.00002346
Iteration 40/1000 | Loss: 0.00002346
Iteration 41/1000 | Loss: 0.00002345
Iteration 42/1000 | Loss: 0.00002345
Iteration 43/1000 | Loss: 0.00002345
Iteration 44/1000 | Loss: 0.00002344
Iteration 45/1000 | Loss: 0.00002344
Iteration 46/1000 | Loss: 0.00002344
Iteration 47/1000 | Loss: 0.00002344
Iteration 48/1000 | Loss: 0.00002343
Iteration 49/1000 | Loss: 0.00002343
Iteration 50/1000 | Loss: 0.00002343
Iteration 51/1000 | Loss: 0.00002343
Iteration 52/1000 | Loss: 0.00002343
Iteration 53/1000 | Loss: 0.00002343
Iteration 54/1000 | Loss: 0.00002343
Iteration 55/1000 | Loss: 0.00002342
Iteration 56/1000 | Loss: 0.00002342
Iteration 57/1000 | Loss: 0.00002342
Iteration 58/1000 | Loss: 0.00002342
Iteration 59/1000 | Loss: 0.00002342
Iteration 60/1000 | Loss: 0.00002342
Iteration 61/1000 | Loss: 0.00002342
Iteration 62/1000 | Loss: 0.00002342
Iteration 63/1000 | Loss: 0.00002342
Iteration 64/1000 | Loss: 0.00002342
Iteration 65/1000 | Loss: 0.00002342
Iteration 66/1000 | Loss: 0.00002342
Iteration 67/1000 | Loss: 0.00002341
Iteration 68/1000 | Loss: 0.00002341
Iteration 69/1000 | Loss: 0.00002341
Iteration 70/1000 | Loss: 0.00002341
Iteration 71/1000 | Loss: 0.00002341
Iteration 72/1000 | Loss: 0.00002341
Iteration 73/1000 | Loss: 0.00002341
Iteration 74/1000 | Loss: 0.00002341
Iteration 75/1000 | Loss: 0.00002341
Iteration 76/1000 | Loss: 0.00002340
Iteration 77/1000 | Loss: 0.00002340
Iteration 78/1000 | Loss: 0.00002340
Iteration 79/1000 | Loss: 0.00002340
Iteration 80/1000 | Loss: 0.00002340
Iteration 81/1000 | Loss: 0.00002339
Iteration 82/1000 | Loss: 0.00002339
Iteration 83/1000 | Loss: 0.00002339
Iteration 84/1000 | Loss: 0.00002339
Iteration 85/1000 | Loss: 0.00002339
Iteration 86/1000 | Loss: 0.00002338
Iteration 87/1000 | Loss: 0.00002338
Iteration 88/1000 | Loss: 0.00002338
Iteration 89/1000 | Loss: 0.00002338
Iteration 90/1000 | Loss: 0.00002338
Iteration 91/1000 | Loss: 0.00002338
Iteration 92/1000 | Loss: 0.00002338
Iteration 93/1000 | Loss: 0.00002338
Iteration 94/1000 | Loss: 0.00002338
Iteration 95/1000 | Loss: 0.00002338
Iteration 96/1000 | Loss: 0.00002338
Iteration 97/1000 | Loss: 0.00002338
Iteration 98/1000 | Loss: 0.00002337
Iteration 99/1000 | Loss: 0.00002337
Iteration 100/1000 | Loss: 0.00002337
Iteration 101/1000 | Loss: 0.00002337
Iteration 102/1000 | Loss: 0.00002337
Iteration 103/1000 | Loss: 0.00002337
Iteration 104/1000 | Loss: 0.00002337
Iteration 105/1000 | Loss: 0.00002337
Iteration 106/1000 | Loss: 0.00002337
Iteration 107/1000 | Loss: 0.00002337
Iteration 108/1000 | Loss: 0.00002337
Iteration 109/1000 | Loss: 0.00002337
Iteration 110/1000 | Loss: 0.00002337
Iteration 111/1000 | Loss: 0.00002337
Iteration 112/1000 | Loss: 0.00002337
Iteration 113/1000 | Loss: 0.00002337
Iteration 114/1000 | Loss: 0.00002337
Iteration 115/1000 | Loss: 0.00002337
Iteration 116/1000 | Loss: 0.00002337
Iteration 117/1000 | Loss: 0.00002337
Iteration 118/1000 | Loss: 0.00002337
Iteration 119/1000 | Loss: 0.00002337
Iteration 120/1000 | Loss: 0.00002337
Iteration 121/1000 | Loss: 0.00002337
Iteration 122/1000 | Loss: 0.00002337
Iteration 123/1000 | Loss: 0.00002337
Iteration 124/1000 | Loss: 0.00002337
Iteration 125/1000 | Loss: 0.00002337
Iteration 126/1000 | Loss: 0.00002337
Iteration 127/1000 | Loss: 0.00002337
Iteration 128/1000 | Loss: 0.00002337
Iteration 129/1000 | Loss: 0.00002337
Iteration 130/1000 | Loss: 0.00002337
Iteration 131/1000 | Loss: 0.00002337
Iteration 132/1000 | Loss: 0.00002337
Iteration 133/1000 | Loss: 0.00002336
Iteration 134/1000 | Loss: 0.00002336
Iteration 135/1000 | Loss: 0.00002336
Iteration 136/1000 | Loss: 0.00002336
Iteration 137/1000 | Loss: 0.00002336
Iteration 138/1000 | Loss: 0.00002336
Iteration 139/1000 | Loss: 0.00002336
Iteration 140/1000 | Loss: 0.00002336
Iteration 141/1000 | Loss: 0.00002336
Iteration 142/1000 | Loss: 0.00002336
Iteration 143/1000 | Loss: 0.00002336
Iteration 144/1000 | Loss: 0.00002336
Iteration 145/1000 | Loss: 0.00002336
Iteration 146/1000 | Loss: 0.00002336
Iteration 147/1000 | Loss: 0.00002336
Iteration 148/1000 | Loss: 0.00002336
Iteration 149/1000 | Loss: 0.00002336
Iteration 150/1000 | Loss: 0.00002336
Iteration 151/1000 | Loss: 0.00002336
Iteration 152/1000 | Loss: 0.00002336
Iteration 153/1000 | Loss: 0.00002336
Iteration 154/1000 | Loss: 0.00002336
Iteration 155/1000 | Loss: 0.00002336
Iteration 156/1000 | Loss: 0.00002336
Iteration 157/1000 | Loss: 0.00002336
Iteration 158/1000 | Loss: 0.00002336
Iteration 159/1000 | Loss: 0.00002336
Iteration 160/1000 | Loss: 0.00002336
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 160. Stopping optimization.
Last 5 losses: [2.336486431886442e-05, 2.336486431886442e-05, 2.336486431886442e-05, 2.336486431886442e-05, 2.336486431886442e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.336486431886442e-05

Optimization complete. Final v2v error: 4.2152628898620605 mm

Highest mean error: 4.638178825378418 mm for frame 71

Lowest mean error: 3.8487515449523926 mm for frame 144

Saving results

Total time: 33.792627573013306
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_40_nl_6353/0020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_40_nl_6353/0020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_40_nl_6353/0020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00455289
Iteration 2/25 | Loss: 0.00143058
Iteration 3/25 | Loss: 0.00134543
Iteration 4/25 | Loss: 0.00133439
Iteration 5/25 | Loss: 0.00133119
Iteration 6/25 | Loss: 0.00133020
Iteration 7/25 | Loss: 0.00132993
Iteration 8/25 | Loss: 0.00132993
Iteration 9/25 | Loss: 0.00132993
Iteration 10/25 | Loss: 0.00132993
Iteration 11/25 | Loss: 0.00132993
Iteration 12/25 | Loss: 0.00132993
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0013299287529662251, 0.0013299287529662251, 0.0013299287529662251, 0.0013299287529662251, 0.0013299287529662251]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013299287529662251

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.55209768
Iteration 2/25 | Loss: 0.00169434
Iteration 3/25 | Loss: 0.00169434
Iteration 4/25 | Loss: 0.00169433
Iteration 5/25 | Loss: 0.00169433
Iteration 6/25 | Loss: 0.00169433
Iteration 7/25 | Loss: 0.00169433
Iteration 8/25 | Loss: 0.00169433
Iteration 9/25 | Loss: 0.00169433
Iteration 10/25 | Loss: 0.00169433
Iteration 11/25 | Loss: 0.00169433
Iteration 12/25 | Loss: 0.00169433
Iteration 13/25 | Loss: 0.00169433
Iteration 14/25 | Loss: 0.00169433
Iteration 15/25 | Loss: 0.00169433
Iteration 16/25 | Loss: 0.00169433
Iteration 17/25 | Loss: 0.00169433
Iteration 18/25 | Loss: 0.00169433
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0016943324590101838, 0.0016943324590101838, 0.0016943324590101838, 0.0016943324590101838, 0.0016943324590101838]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0016943324590101838

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00169433
Iteration 2/1000 | Loss: 0.00004866
Iteration 3/1000 | Loss: 0.00003044
Iteration 4/1000 | Loss: 0.00002632
Iteration 5/1000 | Loss: 0.00002383
Iteration 6/1000 | Loss: 0.00002260
Iteration 7/1000 | Loss: 0.00002183
Iteration 8/1000 | Loss: 0.00002144
Iteration 9/1000 | Loss: 0.00002126
Iteration 10/1000 | Loss: 0.00002117
Iteration 11/1000 | Loss: 0.00002117
Iteration 12/1000 | Loss: 0.00002101
Iteration 13/1000 | Loss: 0.00002101
Iteration 14/1000 | Loss: 0.00002097
Iteration 15/1000 | Loss: 0.00002096
Iteration 16/1000 | Loss: 0.00002089
Iteration 17/1000 | Loss: 0.00002089
Iteration 18/1000 | Loss: 0.00002087
Iteration 19/1000 | Loss: 0.00002085
Iteration 20/1000 | Loss: 0.00002085
Iteration 21/1000 | Loss: 0.00002085
Iteration 22/1000 | Loss: 0.00002085
Iteration 23/1000 | Loss: 0.00002085
Iteration 24/1000 | Loss: 0.00002085
Iteration 25/1000 | Loss: 0.00002085
Iteration 26/1000 | Loss: 0.00002084
Iteration 27/1000 | Loss: 0.00002084
Iteration 28/1000 | Loss: 0.00002084
Iteration 29/1000 | Loss: 0.00002084
Iteration 30/1000 | Loss: 0.00002084
Iteration 31/1000 | Loss: 0.00002084
Iteration 32/1000 | Loss: 0.00002083
Iteration 33/1000 | Loss: 0.00002081
Iteration 34/1000 | Loss: 0.00002081
Iteration 35/1000 | Loss: 0.00002081
Iteration 36/1000 | Loss: 0.00002081
Iteration 37/1000 | Loss: 0.00002080
Iteration 38/1000 | Loss: 0.00002079
Iteration 39/1000 | Loss: 0.00002079
Iteration 40/1000 | Loss: 0.00002078
Iteration 41/1000 | Loss: 0.00002078
Iteration 42/1000 | Loss: 0.00002077
Iteration 43/1000 | Loss: 0.00002077
Iteration 44/1000 | Loss: 0.00002076
Iteration 45/1000 | Loss: 0.00002076
Iteration 46/1000 | Loss: 0.00002076
Iteration 47/1000 | Loss: 0.00002075
Iteration 48/1000 | Loss: 0.00002075
Iteration 49/1000 | Loss: 0.00002074
Iteration 50/1000 | Loss: 0.00002074
Iteration 51/1000 | Loss: 0.00002074
Iteration 52/1000 | Loss: 0.00002073
Iteration 53/1000 | Loss: 0.00002073
Iteration 54/1000 | Loss: 0.00002073
Iteration 55/1000 | Loss: 0.00002072
Iteration 56/1000 | Loss: 0.00002072
Iteration 57/1000 | Loss: 0.00002072
Iteration 58/1000 | Loss: 0.00002071
Iteration 59/1000 | Loss: 0.00002071
Iteration 60/1000 | Loss: 0.00002071
Iteration 61/1000 | Loss: 0.00002070
Iteration 62/1000 | Loss: 0.00002070
Iteration 63/1000 | Loss: 0.00002070
Iteration 64/1000 | Loss: 0.00002069
Iteration 65/1000 | Loss: 0.00002069
Iteration 66/1000 | Loss: 0.00002069
Iteration 67/1000 | Loss: 0.00002069
Iteration 68/1000 | Loss: 0.00002069
Iteration 69/1000 | Loss: 0.00002068
Iteration 70/1000 | Loss: 0.00002068
Iteration 71/1000 | Loss: 0.00002068
Iteration 72/1000 | Loss: 0.00002068
Iteration 73/1000 | Loss: 0.00002068
Iteration 74/1000 | Loss: 0.00002068
Iteration 75/1000 | Loss: 0.00002068
Iteration 76/1000 | Loss: 0.00002068
Iteration 77/1000 | Loss: 0.00002068
Iteration 78/1000 | Loss: 0.00002067
Iteration 79/1000 | Loss: 0.00002067
Iteration 80/1000 | Loss: 0.00002067
Iteration 81/1000 | Loss: 0.00002067
Iteration 82/1000 | Loss: 0.00002067
Iteration 83/1000 | Loss: 0.00002067
Iteration 84/1000 | Loss: 0.00002067
Iteration 85/1000 | Loss: 0.00002067
Iteration 86/1000 | Loss: 0.00002067
Iteration 87/1000 | Loss: 0.00002067
Iteration 88/1000 | Loss: 0.00002067
Iteration 89/1000 | Loss: 0.00002066
Iteration 90/1000 | Loss: 0.00002066
Iteration 91/1000 | Loss: 0.00002066
Iteration 92/1000 | Loss: 0.00002066
Iteration 93/1000 | Loss: 0.00002066
Iteration 94/1000 | Loss: 0.00002066
Iteration 95/1000 | Loss: 0.00002066
Iteration 96/1000 | Loss: 0.00002066
Iteration 97/1000 | Loss: 0.00002066
Iteration 98/1000 | Loss: 0.00002066
Iteration 99/1000 | Loss: 0.00002066
Iteration 100/1000 | Loss: 0.00002066
Iteration 101/1000 | Loss: 0.00002066
Iteration 102/1000 | Loss: 0.00002066
Iteration 103/1000 | Loss: 0.00002066
Iteration 104/1000 | Loss: 0.00002065
Iteration 105/1000 | Loss: 0.00002065
Iteration 106/1000 | Loss: 0.00002065
Iteration 107/1000 | Loss: 0.00002065
Iteration 108/1000 | Loss: 0.00002065
Iteration 109/1000 | Loss: 0.00002065
Iteration 110/1000 | Loss: 0.00002065
Iteration 111/1000 | Loss: 0.00002065
Iteration 112/1000 | Loss: 0.00002065
Iteration 113/1000 | Loss: 0.00002065
Iteration 114/1000 | Loss: 0.00002064
Iteration 115/1000 | Loss: 0.00002064
Iteration 116/1000 | Loss: 0.00002064
Iteration 117/1000 | Loss: 0.00002064
Iteration 118/1000 | Loss: 0.00002064
Iteration 119/1000 | Loss: 0.00002064
Iteration 120/1000 | Loss: 0.00002064
Iteration 121/1000 | Loss: 0.00002064
Iteration 122/1000 | Loss: 0.00002064
Iteration 123/1000 | Loss: 0.00002064
Iteration 124/1000 | Loss: 0.00002064
Iteration 125/1000 | Loss: 0.00002064
Iteration 126/1000 | Loss: 0.00002064
Iteration 127/1000 | Loss: 0.00002063
Iteration 128/1000 | Loss: 0.00002063
Iteration 129/1000 | Loss: 0.00002063
Iteration 130/1000 | Loss: 0.00002063
Iteration 131/1000 | Loss: 0.00002063
Iteration 132/1000 | Loss: 0.00002063
Iteration 133/1000 | Loss: 0.00002063
Iteration 134/1000 | Loss: 0.00002063
Iteration 135/1000 | Loss: 0.00002063
Iteration 136/1000 | Loss: 0.00002063
Iteration 137/1000 | Loss: 0.00002063
Iteration 138/1000 | Loss: 0.00002063
Iteration 139/1000 | Loss: 0.00002063
Iteration 140/1000 | Loss: 0.00002063
Iteration 141/1000 | Loss: 0.00002062
Iteration 142/1000 | Loss: 0.00002062
Iteration 143/1000 | Loss: 0.00002062
Iteration 144/1000 | Loss: 0.00002062
Iteration 145/1000 | Loss: 0.00002062
Iteration 146/1000 | Loss: 0.00002062
Iteration 147/1000 | Loss: 0.00002062
Iteration 148/1000 | Loss: 0.00002062
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 148. Stopping optimization.
Last 5 losses: [2.0624094759114087e-05, 2.0624094759114087e-05, 2.0624094759114087e-05, 2.0624094759114087e-05, 2.0624094759114087e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0624094759114087e-05

Optimization complete. Final v2v error: 3.9555094242095947 mm

Highest mean error: 4.6414055824279785 mm for frame 56

Lowest mean error: 3.7123312950134277 mm for frame 130

Saving results

Total time: 33.14474153518677
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_40_nl_6353/0014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_40_nl_6353/0014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_40_nl_6353/0014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00581334
Iteration 2/25 | Loss: 0.00156578
Iteration 3/25 | Loss: 0.00147208
Iteration 4/25 | Loss: 0.00145079
Iteration 5/25 | Loss: 0.00144586
Iteration 6/25 | Loss: 0.00144505
Iteration 7/25 | Loss: 0.00144505
Iteration 8/25 | Loss: 0.00144505
Iteration 9/25 | Loss: 0.00144505
Iteration 10/25 | Loss: 0.00144505
Iteration 11/25 | Loss: 0.00144505
Iteration 12/25 | Loss: 0.00144505
Iteration 13/25 | Loss: 0.00144505
Iteration 14/25 | Loss: 0.00144505
Iteration 15/25 | Loss: 0.00144505
Iteration 16/25 | Loss: 0.00144505
Iteration 17/25 | Loss: 0.00144505
Iteration 18/25 | Loss: 0.00144505
Iteration 19/25 | Loss: 0.00144505
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0014450463932007551, 0.0014450463932007551, 0.0014450463932007551, 0.0014450463932007551, 0.0014450463932007551]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014450463932007551

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.52164602
Iteration 2/25 | Loss: 0.00174504
Iteration 3/25 | Loss: 0.00174500
Iteration 4/25 | Loss: 0.00174500
Iteration 5/25 | Loss: 0.00174500
Iteration 6/25 | Loss: 0.00174500
Iteration 7/25 | Loss: 0.00174500
Iteration 8/25 | Loss: 0.00174500
Iteration 9/25 | Loss: 0.00174500
Iteration 10/25 | Loss: 0.00174500
Iteration 11/25 | Loss: 0.00174500
Iteration 12/25 | Loss: 0.00174500
Iteration 13/25 | Loss: 0.00174500
Iteration 14/25 | Loss: 0.00174500
Iteration 15/25 | Loss: 0.00174500
Iteration 16/25 | Loss: 0.00174500
Iteration 17/25 | Loss: 0.00174500
Iteration 18/25 | Loss: 0.00174500
Iteration 19/25 | Loss: 0.00174500
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0017449998995289207, 0.0017449998995289207, 0.0017449998995289207, 0.0017449998995289207, 0.0017449998995289207]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0017449998995289207

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00174500
Iteration 2/1000 | Loss: 0.00003672
Iteration 3/1000 | Loss: 0.00002924
Iteration 4/1000 | Loss: 0.00002608
Iteration 5/1000 | Loss: 0.00002468
Iteration 6/1000 | Loss: 0.00002365
Iteration 7/1000 | Loss: 0.00002287
Iteration 8/1000 | Loss: 0.00002238
Iteration 9/1000 | Loss: 0.00002212
Iteration 10/1000 | Loss: 0.00002198
Iteration 11/1000 | Loss: 0.00002194
Iteration 12/1000 | Loss: 0.00002180
Iteration 13/1000 | Loss: 0.00002174
Iteration 14/1000 | Loss: 0.00002172
Iteration 15/1000 | Loss: 0.00002172
Iteration 16/1000 | Loss: 0.00002169
Iteration 17/1000 | Loss: 0.00002167
Iteration 18/1000 | Loss: 0.00002167
Iteration 19/1000 | Loss: 0.00002167
Iteration 20/1000 | Loss: 0.00002166
Iteration 21/1000 | Loss: 0.00002166
Iteration 22/1000 | Loss: 0.00002162
Iteration 23/1000 | Loss: 0.00002162
Iteration 24/1000 | Loss: 0.00002161
Iteration 25/1000 | Loss: 0.00002160
Iteration 26/1000 | Loss: 0.00002160
Iteration 27/1000 | Loss: 0.00002159
Iteration 28/1000 | Loss: 0.00002159
Iteration 29/1000 | Loss: 0.00002159
Iteration 30/1000 | Loss: 0.00002159
Iteration 31/1000 | Loss: 0.00002159
Iteration 32/1000 | Loss: 0.00002159
Iteration 33/1000 | Loss: 0.00002159
Iteration 34/1000 | Loss: 0.00002159
Iteration 35/1000 | Loss: 0.00002159
Iteration 36/1000 | Loss: 0.00002159
Iteration 37/1000 | Loss: 0.00002158
Iteration 38/1000 | Loss: 0.00002158
Iteration 39/1000 | Loss: 0.00002158
Iteration 40/1000 | Loss: 0.00002158
Iteration 41/1000 | Loss: 0.00002158
Iteration 42/1000 | Loss: 0.00002158
Iteration 43/1000 | Loss: 0.00002158
Iteration 44/1000 | Loss: 0.00002157
Iteration 45/1000 | Loss: 0.00002157
Iteration 46/1000 | Loss: 0.00002157
Iteration 47/1000 | Loss: 0.00002156
Iteration 48/1000 | Loss: 0.00002156
Iteration 49/1000 | Loss: 0.00002156
Iteration 50/1000 | Loss: 0.00002156
Iteration 51/1000 | Loss: 0.00002156
Iteration 52/1000 | Loss: 0.00002155
Iteration 53/1000 | Loss: 0.00002155
Iteration 54/1000 | Loss: 0.00002155
Iteration 55/1000 | Loss: 0.00002154
Iteration 56/1000 | Loss: 0.00002154
Iteration 57/1000 | Loss: 0.00002154
Iteration 58/1000 | Loss: 0.00002153
Iteration 59/1000 | Loss: 0.00002153
Iteration 60/1000 | Loss: 0.00002153
Iteration 61/1000 | Loss: 0.00002153
Iteration 62/1000 | Loss: 0.00002152
Iteration 63/1000 | Loss: 0.00002152
Iteration 64/1000 | Loss: 0.00002152
Iteration 65/1000 | Loss: 0.00002152
Iteration 66/1000 | Loss: 0.00002152
Iteration 67/1000 | Loss: 0.00002152
Iteration 68/1000 | Loss: 0.00002152
Iteration 69/1000 | Loss: 0.00002151
Iteration 70/1000 | Loss: 0.00002151
Iteration 71/1000 | Loss: 0.00002151
Iteration 72/1000 | Loss: 0.00002150
Iteration 73/1000 | Loss: 0.00002150
Iteration 74/1000 | Loss: 0.00002150
Iteration 75/1000 | Loss: 0.00002150
Iteration 76/1000 | Loss: 0.00002149
Iteration 77/1000 | Loss: 0.00002149
Iteration 78/1000 | Loss: 0.00002149
Iteration 79/1000 | Loss: 0.00002149
Iteration 80/1000 | Loss: 0.00002149
Iteration 81/1000 | Loss: 0.00002149
Iteration 82/1000 | Loss: 0.00002149
Iteration 83/1000 | Loss: 0.00002149
Iteration 84/1000 | Loss: 0.00002149
Iteration 85/1000 | Loss: 0.00002149
Iteration 86/1000 | Loss: 0.00002148
Iteration 87/1000 | Loss: 0.00002148
Iteration 88/1000 | Loss: 0.00002148
Iteration 89/1000 | Loss: 0.00002148
Iteration 90/1000 | Loss: 0.00002148
Iteration 91/1000 | Loss: 0.00002148
Iteration 92/1000 | Loss: 0.00002148
Iteration 93/1000 | Loss: 0.00002147
Iteration 94/1000 | Loss: 0.00002147
Iteration 95/1000 | Loss: 0.00002147
Iteration 96/1000 | Loss: 0.00002147
Iteration 97/1000 | Loss: 0.00002147
Iteration 98/1000 | Loss: 0.00002147
Iteration 99/1000 | Loss: 0.00002147
Iteration 100/1000 | Loss: 0.00002146
Iteration 101/1000 | Loss: 0.00002146
Iteration 102/1000 | Loss: 0.00002146
Iteration 103/1000 | Loss: 0.00002146
Iteration 104/1000 | Loss: 0.00002146
Iteration 105/1000 | Loss: 0.00002146
Iteration 106/1000 | Loss: 0.00002146
Iteration 107/1000 | Loss: 0.00002146
Iteration 108/1000 | Loss: 0.00002146
Iteration 109/1000 | Loss: 0.00002146
Iteration 110/1000 | Loss: 0.00002146
Iteration 111/1000 | Loss: 0.00002146
Iteration 112/1000 | Loss: 0.00002146
Iteration 113/1000 | Loss: 0.00002145
Iteration 114/1000 | Loss: 0.00002145
Iteration 115/1000 | Loss: 0.00002145
Iteration 116/1000 | Loss: 0.00002145
Iteration 117/1000 | Loss: 0.00002145
Iteration 118/1000 | Loss: 0.00002145
Iteration 119/1000 | Loss: 0.00002145
Iteration 120/1000 | Loss: 0.00002145
Iteration 121/1000 | Loss: 0.00002145
Iteration 122/1000 | Loss: 0.00002145
Iteration 123/1000 | Loss: 0.00002145
Iteration 124/1000 | Loss: 0.00002145
Iteration 125/1000 | Loss: 0.00002145
Iteration 126/1000 | Loss: 0.00002145
Iteration 127/1000 | Loss: 0.00002144
Iteration 128/1000 | Loss: 0.00002144
Iteration 129/1000 | Loss: 0.00002144
Iteration 130/1000 | Loss: 0.00002144
Iteration 131/1000 | Loss: 0.00002144
Iteration 132/1000 | Loss: 0.00002144
Iteration 133/1000 | Loss: 0.00002144
Iteration 134/1000 | Loss: 0.00002144
Iteration 135/1000 | Loss: 0.00002144
Iteration 136/1000 | Loss: 0.00002144
Iteration 137/1000 | Loss: 0.00002144
Iteration 138/1000 | Loss: 0.00002144
Iteration 139/1000 | Loss: 0.00002144
Iteration 140/1000 | Loss: 0.00002144
Iteration 141/1000 | Loss: 0.00002144
Iteration 142/1000 | Loss: 0.00002144
Iteration 143/1000 | Loss: 0.00002143
Iteration 144/1000 | Loss: 0.00002143
Iteration 145/1000 | Loss: 0.00002143
Iteration 146/1000 | Loss: 0.00002143
Iteration 147/1000 | Loss: 0.00002143
Iteration 148/1000 | Loss: 0.00002143
Iteration 149/1000 | Loss: 0.00002143
Iteration 150/1000 | Loss: 0.00002143
Iteration 151/1000 | Loss: 0.00002143
Iteration 152/1000 | Loss: 0.00002143
Iteration 153/1000 | Loss: 0.00002143
Iteration 154/1000 | Loss: 0.00002143
Iteration 155/1000 | Loss: 0.00002143
Iteration 156/1000 | Loss: 0.00002143
Iteration 157/1000 | Loss: 0.00002143
Iteration 158/1000 | Loss: 0.00002143
Iteration 159/1000 | Loss: 0.00002143
Iteration 160/1000 | Loss: 0.00002143
Iteration 161/1000 | Loss: 0.00002143
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 161. Stopping optimization.
Last 5 losses: [2.142622906831093e-05, 2.142622906831093e-05, 2.142622906831093e-05, 2.142622906831093e-05, 2.142622906831093e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.142622906831093e-05

Optimization complete. Final v2v error: 4.005892276763916 mm

Highest mean error: 4.108040809631348 mm for frame 47

Lowest mean error: 3.906970262527466 mm for frame 208

Saving results

Total time: 41.45485329627991
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_40_nl_6353/0015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_40_nl_6353/0015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_40_nl_6353/0015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00936635
Iteration 2/25 | Loss: 0.00151793
Iteration 3/25 | Loss: 0.00140434
Iteration 4/25 | Loss: 0.00138899
Iteration 5/25 | Loss: 0.00138374
Iteration 6/25 | Loss: 0.00138278
Iteration 7/25 | Loss: 0.00138278
Iteration 8/25 | Loss: 0.00138278
Iteration 9/25 | Loss: 0.00138278
Iteration 10/25 | Loss: 0.00138278
Iteration 11/25 | Loss: 0.00138278
Iteration 12/25 | Loss: 0.00138278
Iteration 13/25 | Loss: 0.00138278
Iteration 14/25 | Loss: 0.00138278
Iteration 15/25 | Loss: 0.00138278
Iteration 16/25 | Loss: 0.00138278
Iteration 17/25 | Loss: 0.00138278
Iteration 18/25 | Loss: 0.00138278
Iteration 19/25 | Loss: 0.00138278
Iteration 20/25 | Loss: 0.00138278
Iteration 21/25 | Loss: 0.00138278
Iteration 22/25 | Loss: 0.00138278
Iteration 23/25 | Loss: 0.00138278
Iteration 24/25 | Loss: 0.00138278
Iteration 25/25 | Loss: 0.00138278

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.85628521
Iteration 2/25 | Loss: 0.00162326
Iteration 3/25 | Loss: 0.00162326
Iteration 4/25 | Loss: 0.00162326
Iteration 5/25 | Loss: 0.00162326
Iteration 6/25 | Loss: 0.00162326
Iteration 7/25 | Loss: 0.00162326
Iteration 8/25 | Loss: 0.00162326
Iteration 9/25 | Loss: 0.00162326
Iteration 10/25 | Loss: 0.00162326
Iteration 11/25 | Loss: 0.00162326
Iteration 12/25 | Loss: 0.00162326
Iteration 13/25 | Loss: 0.00162326
Iteration 14/25 | Loss: 0.00162326
Iteration 15/25 | Loss: 0.00162326
Iteration 16/25 | Loss: 0.00162326
Iteration 17/25 | Loss: 0.00162326
Iteration 18/25 | Loss: 0.00162326
Iteration 19/25 | Loss: 0.00162326
Iteration 20/25 | Loss: 0.00162326
Iteration 21/25 | Loss: 0.00162326
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0016232593916356564, 0.0016232593916356564, 0.0016232593916356564, 0.0016232593916356564, 0.0016232593916356564]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0016232593916356564

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00162326
Iteration 2/1000 | Loss: 0.00004179
Iteration 3/1000 | Loss: 0.00003377
Iteration 4/1000 | Loss: 0.00003133
Iteration 5/1000 | Loss: 0.00002985
Iteration 6/1000 | Loss: 0.00002901
Iteration 7/1000 | Loss: 0.00002845
Iteration 8/1000 | Loss: 0.00002809
Iteration 9/1000 | Loss: 0.00002777
Iteration 10/1000 | Loss: 0.00002765
Iteration 11/1000 | Loss: 0.00002763
Iteration 12/1000 | Loss: 0.00002761
Iteration 13/1000 | Loss: 0.00002760
Iteration 14/1000 | Loss: 0.00002759
Iteration 15/1000 | Loss: 0.00002749
Iteration 16/1000 | Loss: 0.00002746
Iteration 17/1000 | Loss: 0.00002743
Iteration 18/1000 | Loss: 0.00002739
Iteration 19/1000 | Loss: 0.00002736
Iteration 20/1000 | Loss: 0.00002735
Iteration 21/1000 | Loss: 0.00002734
Iteration 22/1000 | Loss: 0.00002733
Iteration 23/1000 | Loss: 0.00002732
Iteration 24/1000 | Loss: 0.00002732
Iteration 25/1000 | Loss: 0.00002729
Iteration 26/1000 | Loss: 0.00002727
Iteration 27/1000 | Loss: 0.00002725
Iteration 28/1000 | Loss: 0.00002723
Iteration 29/1000 | Loss: 0.00002722
Iteration 30/1000 | Loss: 0.00002721
Iteration 31/1000 | Loss: 0.00002719
Iteration 32/1000 | Loss: 0.00002719
Iteration 33/1000 | Loss: 0.00002718
Iteration 34/1000 | Loss: 0.00002718
Iteration 35/1000 | Loss: 0.00002718
Iteration 36/1000 | Loss: 0.00002713
Iteration 37/1000 | Loss: 0.00002712
Iteration 38/1000 | Loss: 0.00002711
Iteration 39/1000 | Loss: 0.00002711
Iteration 40/1000 | Loss: 0.00002710
Iteration 41/1000 | Loss: 0.00002710
Iteration 42/1000 | Loss: 0.00002709
Iteration 43/1000 | Loss: 0.00002709
Iteration 44/1000 | Loss: 0.00002709
Iteration 45/1000 | Loss: 0.00002709
Iteration 46/1000 | Loss: 0.00002709
Iteration 47/1000 | Loss: 0.00002709
Iteration 48/1000 | Loss: 0.00002708
Iteration 49/1000 | Loss: 0.00002708
Iteration 50/1000 | Loss: 0.00002708
Iteration 51/1000 | Loss: 0.00002708
Iteration 52/1000 | Loss: 0.00002708
Iteration 53/1000 | Loss: 0.00002708
Iteration 54/1000 | Loss: 0.00002708
Iteration 55/1000 | Loss: 0.00002708
Iteration 56/1000 | Loss: 0.00002708
Iteration 57/1000 | Loss: 0.00002707
Iteration 58/1000 | Loss: 0.00002707
Iteration 59/1000 | Loss: 0.00002707
Iteration 60/1000 | Loss: 0.00002706
Iteration 61/1000 | Loss: 0.00002706
Iteration 62/1000 | Loss: 0.00002706
Iteration 63/1000 | Loss: 0.00002706
Iteration 64/1000 | Loss: 0.00002705
Iteration 65/1000 | Loss: 0.00002705
Iteration 66/1000 | Loss: 0.00002705
Iteration 67/1000 | Loss: 0.00002705
Iteration 68/1000 | Loss: 0.00002704
Iteration 69/1000 | Loss: 0.00002704
Iteration 70/1000 | Loss: 0.00002704
Iteration 71/1000 | Loss: 0.00002704
Iteration 72/1000 | Loss: 0.00002704
Iteration 73/1000 | Loss: 0.00002703
Iteration 74/1000 | Loss: 0.00002703
Iteration 75/1000 | Loss: 0.00002703
Iteration 76/1000 | Loss: 0.00002703
Iteration 77/1000 | Loss: 0.00002702
Iteration 78/1000 | Loss: 0.00002702
Iteration 79/1000 | Loss: 0.00002702
Iteration 80/1000 | Loss: 0.00002702
Iteration 81/1000 | Loss: 0.00002702
Iteration 82/1000 | Loss: 0.00002702
Iteration 83/1000 | Loss: 0.00002702
Iteration 84/1000 | Loss: 0.00002702
Iteration 85/1000 | Loss: 0.00002702
Iteration 86/1000 | Loss: 0.00002702
Iteration 87/1000 | Loss: 0.00002702
Iteration 88/1000 | Loss: 0.00002702
Iteration 89/1000 | Loss: 0.00002702
Iteration 90/1000 | Loss: 0.00002702
Iteration 91/1000 | Loss: 0.00002702
Iteration 92/1000 | Loss: 0.00002702
Iteration 93/1000 | Loss: 0.00002702
Iteration 94/1000 | Loss: 0.00002701
Iteration 95/1000 | Loss: 0.00002701
Iteration 96/1000 | Loss: 0.00002701
Iteration 97/1000 | Loss: 0.00002701
Iteration 98/1000 | Loss: 0.00002701
Iteration 99/1000 | Loss: 0.00002701
Iteration 100/1000 | Loss: 0.00002701
Iteration 101/1000 | Loss: 0.00002701
Iteration 102/1000 | Loss: 0.00002701
Iteration 103/1000 | Loss: 0.00002701
Iteration 104/1000 | Loss: 0.00002701
Iteration 105/1000 | Loss: 0.00002701
Iteration 106/1000 | Loss: 0.00002701
Iteration 107/1000 | Loss: 0.00002701
Iteration 108/1000 | Loss: 0.00002701
Iteration 109/1000 | Loss: 0.00002701
Iteration 110/1000 | Loss: 0.00002701
Iteration 111/1000 | Loss: 0.00002701
Iteration 112/1000 | Loss: 0.00002701
Iteration 113/1000 | Loss: 0.00002701
Iteration 114/1000 | Loss: 0.00002701
Iteration 115/1000 | Loss: 0.00002701
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 115. Stopping optimization.
Last 5 losses: [2.7014402803615667e-05, 2.7014402803615667e-05, 2.7014402803615667e-05, 2.7014402803615667e-05, 2.7014402803615667e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.7014402803615667e-05

Optimization complete. Final v2v error: 4.43878698348999 mm

Highest mean error: 5.208216667175293 mm for frame 114

Lowest mean error: 3.9363582134246826 mm for frame 1

Saving results

Total time: 35.70722818374634
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_40_nl_6353/0010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_40_nl_6353/0010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_40_nl_6353/0010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00461810
Iteration 2/25 | Loss: 0.00162783
Iteration 3/25 | Loss: 0.00136518
Iteration 4/25 | Loss: 0.00133462
Iteration 5/25 | Loss: 0.00132747
Iteration 6/25 | Loss: 0.00132533
Iteration 7/25 | Loss: 0.00132460
Iteration 8/25 | Loss: 0.00132460
Iteration 9/25 | Loss: 0.00132460
Iteration 10/25 | Loss: 0.00132460
Iteration 11/25 | Loss: 0.00132460
Iteration 12/25 | Loss: 0.00132460
Iteration 13/25 | Loss: 0.00132460
Iteration 14/25 | Loss: 0.00132460
Iteration 15/25 | Loss: 0.00132460
Iteration 16/25 | Loss: 0.00132460
Iteration 17/25 | Loss: 0.00132460
Iteration 18/25 | Loss: 0.00132460
Iteration 19/25 | Loss: 0.00132460
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0013246005401015282, 0.0013246005401015282, 0.0013246005401015282, 0.0013246005401015282, 0.0013246005401015282]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013246005401015282

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.53284943
Iteration 2/25 | Loss: 0.00150456
Iteration 3/25 | Loss: 0.00150456
Iteration 4/25 | Loss: 0.00150456
Iteration 5/25 | Loss: 0.00150456
Iteration 6/25 | Loss: 0.00150456
Iteration 7/25 | Loss: 0.00150456
Iteration 8/25 | Loss: 0.00150456
Iteration 9/25 | Loss: 0.00150455
Iteration 10/25 | Loss: 0.00150455
Iteration 11/25 | Loss: 0.00150455
Iteration 12/25 | Loss: 0.00150455
Iteration 13/25 | Loss: 0.00150455
Iteration 14/25 | Loss: 0.00150455
Iteration 15/25 | Loss: 0.00150455
Iteration 16/25 | Loss: 0.00150455
Iteration 17/25 | Loss: 0.00150455
Iteration 18/25 | Loss: 0.00150455
Iteration 19/25 | Loss: 0.00150455
Iteration 20/25 | Loss: 0.00150455
Iteration 21/25 | Loss: 0.00150455
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.001504554646089673, 0.001504554646089673, 0.001504554646089673, 0.001504554646089673, 0.001504554646089673]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001504554646089673

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00150455
Iteration 2/1000 | Loss: 0.00004034
Iteration 3/1000 | Loss: 0.00002889
Iteration 4/1000 | Loss: 0.00002595
Iteration 5/1000 | Loss: 0.00002451
Iteration 6/1000 | Loss: 0.00002343
Iteration 7/1000 | Loss: 0.00002275
Iteration 8/1000 | Loss: 0.00002222
Iteration 9/1000 | Loss: 0.00002197
Iteration 10/1000 | Loss: 0.00002193
Iteration 11/1000 | Loss: 0.00002167
Iteration 12/1000 | Loss: 0.00002152
Iteration 13/1000 | Loss: 0.00002151
Iteration 14/1000 | Loss: 0.00002143
Iteration 15/1000 | Loss: 0.00002130
Iteration 16/1000 | Loss: 0.00002129
Iteration 17/1000 | Loss: 0.00002129
Iteration 18/1000 | Loss: 0.00002128
Iteration 19/1000 | Loss: 0.00002126
Iteration 20/1000 | Loss: 0.00002123
Iteration 21/1000 | Loss: 0.00002123
Iteration 22/1000 | Loss: 0.00002119
Iteration 23/1000 | Loss: 0.00002119
Iteration 24/1000 | Loss: 0.00002119
Iteration 25/1000 | Loss: 0.00002119
Iteration 26/1000 | Loss: 0.00002119
Iteration 27/1000 | Loss: 0.00002118
Iteration 28/1000 | Loss: 0.00002118
Iteration 29/1000 | Loss: 0.00002118
Iteration 30/1000 | Loss: 0.00002118
Iteration 31/1000 | Loss: 0.00002118
Iteration 32/1000 | Loss: 0.00002117
Iteration 33/1000 | Loss: 0.00002117
Iteration 34/1000 | Loss: 0.00002116
Iteration 35/1000 | Loss: 0.00002115
Iteration 36/1000 | Loss: 0.00002115
Iteration 37/1000 | Loss: 0.00002115
Iteration 38/1000 | Loss: 0.00002115
Iteration 39/1000 | Loss: 0.00002114
Iteration 40/1000 | Loss: 0.00002114
Iteration 41/1000 | Loss: 0.00002113
Iteration 42/1000 | Loss: 0.00002113
Iteration 43/1000 | Loss: 0.00002113
Iteration 44/1000 | Loss: 0.00002113
Iteration 45/1000 | Loss: 0.00002112
Iteration 46/1000 | Loss: 0.00002112
Iteration 47/1000 | Loss: 0.00002112
Iteration 48/1000 | Loss: 0.00002112
Iteration 49/1000 | Loss: 0.00002111
Iteration 50/1000 | Loss: 0.00002111
Iteration 51/1000 | Loss: 0.00002111
Iteration 52/1000 | Loss: 0.00002110
Iteration 53/1000 | Loss: 0.00002110
Iteration 54/1000 | Loss: 0.00002110
Iteration 55/1000 | Loss: 0.00002109
Iteration 56/1000 | Loss: 0.00002109
Iteration 57/1000 | Loss: 0.00002109
Iteration 58/1000 | Loss: 0.00002108
Iteration 59/1000 | Loss: 0.00002108
Iteration 60/1000 | Loss: 0.00002108
Iteration 61/1000 | Loss: 0.00002108
Iteration 62/1000 | Loss: 0.00002107
Iteration 63/1000 | Loss: 0.00002107
Iteration 64/1000 | Loss: 0.00002107
Iteration 65/1000 | Loss: 0.00002106
Iteration 66/1000 | Loss: 0.00002106
Iteration 67/1000 | Loss: 0.00002106
Iteration 68/1000 | Loss: 0.00002106
Iteration 69/1000 | Loss: 0.00002105
Iteration 70/1000 | Loss: 0.00002105
Iteration 71/1000 | Loss: 0.00002105
Iteration 72/1000 | Loss: 0.00002105
Iteration 73/1000 | Loss: 0.00002105
Iteration 74/1000 | Loss: 0.00002105
Iteration 75/1000 | Loss: 0.00002105
Iteration 76/1000 | Loss: 0.00002105
Iteration 77/1000 | Loss: 0.00002104
Iteration 78/1000 | Loss: 0.00002104
Iteration 79/1000 | Loss: 0.00002104
Iteration 80/1000 | Loss: 0.00002104
Iteration 81/1000 | Loss: 0.00002104
Iteration 82/1000 | Loss: 0.00002104
Iteration 83/1000 | Loss: 0.00002104
Iteration 84/1000 | Loss: 0.00002104
Iteration 85/1000 | Loss: 0.00002104
Iteration 86/1000 | Loss: 0.00002103
Iteration 87/1000 | Loss: 0.00002103
Iteration 88/1000 | Loss: 0.00002103
Iteration 89/1000 | Loss: 0.00002103
Iteration 90/1000 | Loss: 0.00002103
Iteration 91/1000 | Loss: 0.00002103
Iteration 92/1000 | Loss: 0.00002103
Iteration 93/1000 | Loss: 0.00002103
Iteration 94/1000 | Loss: 0.00002103
Iteration 95/1000 | Loss: 0.00002103
Iteration 96/1000 | Loss: 0.00002102
Iteration 97/1000 | Loss: 0.00002102
Iteration 98/1000 | Loss: 0.00002102
Iteration 99/1000 | Loss: 0.00002102
Iteration 100/1000 | Loss: 0.00002102
Iteration 101/1000 | Loss: 0.00002101
Iteration 102/1000 | Loss: 0.00002101
Iteration 103/1000 | Loss: 0.00002101
Iteration 104/1000 | Loss: 0.00002101
Iteration 105/1000 | Loss: 0.00002101
Iteration 106/1000 | Loss: 0.00002101
Iteration 107/1000 | Loss: 0.00002101
Iteration 108/1000 | Loss: 0.00002101
Iteration 109/1000 | Loss: 0.00002101
Iteration 110/1000 | Loss: 0.00002101
Iteration 111/1000 | Loss: 0.00002101
Iteration 112/1000 | Loss: 0.00002100
Iteration 113/1000 | Loss: 0.00002100
Iteration 114/1000 | Loss: 0.00002100
Iteration 115/1000 | Loss: 0.00002100
Iteration 116/1000 | Loss: 0.00002100
Iteration 117/1000 | Loss: 0.00002100
Iteration 118/1000 | Loss: 0.00002100
Iteration 119/1000 | Loss: 0.00002100
Iteration 120/1000 | Loss: 0.00002100
Iteration 121/1000 | Loss: 0.00002100
Iteration 122/1000 | Loss: 0.00002100
Iteration 123/1000 | Loss: 0.00002100
Iteration 124/1000 | Loss: 0.00002100
Iteration 125/1000 | Loss: 0.00002100
Iteration 126/1000 | Loss: 0.00002100
Iteration 127/1000 | Loss: 0.00002100
Iteration 128/1000 | Loss: 0.00002100
Iteration 129/1000 | Loss: 0.00002100
Iteration 130/1000 | Loss: 0.00002099
Iteration 131/1000 | Loss: 0.00002099
Iteration 132/1000 | Loss: 0.00002099
Iteration 133/1000 | Loss: 0.00002099
Iteration 134/1000 | Loss: 0.00002099
Iteration 135/1000 | Loss: 0.00002099
Iteration 136/1000 | Loss: 0.00002099
Iteration 137/1000 | Loss: 0.00002099
Iteration 138/1000 | Loss: 0.00002099
Iteration 139/1000 | Loss: 0.00002099
Iteration 140/1000 | Loss: 0.00002099
Iteration 141/1000 | Loss: 0.00002099
Iteration 142/1000 | Loss: 0.00002099
Iteration 143/1000 | Loss: 0.00002099
Iteration 144/1000 | Loss: 0.00002099
Iteration 145/1000 | Loss: 0.00002099
Iteration 146/1000 | Loss: 0.00002099
Iteration 147/1000 | Loss: 0.00002099
Iteration 148/1000 | Loss: 0.00002099
Iteration 149/1000 | Loss: 0.00002099
Iteration 150/1000 | Loss: 0.00002099
Iteration 151/1000 | Loss: 0.00002099
Iteration 152/1000 | Loss: 0.00002099
Iteration 153/1000 | Loss: 0.00002099
Iteration 154/1000 | Loss: 0.00002099
Iteration 155/1000 | Loss: 0.00002099
Iteration 156/1000 | Loss: 0.00002099
Iteration 157/1000 | Loss: 0.00002099
Iteration 158/1000 | Loss: 0.00002099
Iteration 159/1000 | Loss: 0.00002099
Iteration 160/1000 | Loss: 0.00002099
Iteration 161/1000 | Loss: 0.00002099
Iteration 162/1000 | Loss: 0.00002099
Iteration 163/1000 | Loss: 0.00002099
Iteration 164/1000 | Loss: 0.00002099
Iteration 165/1000 | Loss: 0.00002099
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 165. Stopping optimization.
Last 5 losses: [2.0991989003960043e-05, 2.0991989003960043e-05, 2.0991989003960043e-05, 2.0991989003960043e-05, 2.0991989003960043e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0991989003960043e-05

Optimization complete. Final v2v error: 3.9801177978515625 mm

Highest mean error: 4.5062031745910645 mm for frame 79

Lowest mean error: 3.781848669052124 mm for frame 30

Saving results

Total time: 38.3470733165741
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_006/1030/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_006/1030.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_006/1030
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00380049
Iteration 2/25 | Loss: 0.00075033
Iteration 3/25 | Loss: 0.00067542
Iteration 4/25 | Loss: 0.00065252
Iteration 5/25 | Loss: 0.00064751
Iteration 6/25 | Loss: 0.00064667
Iteration 7/25 | Loss: 0.00064666
Iteration 8/25 | Loss: 0.00064666
Iteration 9/25 | Loss: 0.00064666
Iteration 10/25 | Loss: 0.00064666
Iteration 11/25 | Loss: 0.00064666
Iteration 12/25 | Loss: 0.00064666
Iteration 13/25 | Loss: 0.00064666
Iteration 14/25 | Loss: 0.00064666
Iteration 15/25 | Loss: 0.00064666
Iteration 16/25 | Loss: 0.00064666
Iteration 17/25 | Loss: 0.00064666
Iteration 18/25 | Loss: 0.00064666
Iteration 19/25 | Loss: 0.00064666
Iteration 20/25 | Loss: 0.00064666
Iteration 21/25 | Loss: 0.00064666
Iteration 22/25 | Loss: 0.00064666
Iteration 23/25 | Loss: 0.00064666
Iteration 24/25 | Loss: 0.00064666
Iteration 25/25 | Loss: 0.00064666

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44529808
Iteration 2/25 | Loss: 0.00025567
Iteration 3/25 | Loss: 0.00025567
Iteration 4/25 | Loss: 0.00025567
Iteration 5/25 | Loss: 0.00025567
Iteration 6/25 | Loss: 0.00025567
Iteration 7/25 | Loss: 0.00025567
Iteration 8/25 | Loss: 0.00025566
Iteration 9/25 | Loss: 0.00025566
Iteration 10/25 | Loss: 0.00025566
Iteration 11/25 | Loss: 0.00025566
Iteration 12/25 | Loss: 0.00025566
Iteration 13/25 | Loss: 0.00025566
Iteration 14/25 | Loss: 0.00025566
Iteration 15/25 | Loss: 0.00025566
Iteration 16/25 | Loss: 0.00025566
Iteration 17/25 | Loss: 0.00025566
Iteration 18/25 | Loss: 0.00025566
Iteration 19/25 | Loss: 0.00025566
Iteration 20/25 | Loss: 0.00025566
Iteration 21/25 | Loss: 0.00025566
Iteration 22/25 | Loss: 0.00025566
Iteration 23/25 | Loss: 0.00025566
Iteration 24/25 | Loss: 0.00025566
Iteration 25/25 | Loss: 0.00025566

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00025566
Iteration 2/1000 | Loss: 0.00003767
Iteration 3/1000 | Loss: 0.00002629
Iteration 4/1000 | Loss: 0.00002145
Iteration 5/1000 | Loss: 0.00002014
Iteration 6/1000 | Loss: 0.00001903
Iteration 7/1000 | Loss: 0.00001823
Iteration 8/1000 | Loss: 0.00001781
Iteration 9/1000 | Loss: 0.00001744
Iteration 10/1000 | Loss: 0.00001726
Iteration 11/1000 | Loss: 0.00001715
Iteration 12/1000 | Loss: 0.00001706
Iteration 13/1000 | Loss: 0.00001702
Iteration 14/1000 | Loss: 0.00001701
Iteration 15/1000 | Loss: 0.00001701
Iteration 16/1000 | Loss: 0.00001701
Iteration 17/1000 | Loss: 0.00001700
Iteration 18/1000 | Loss: 0.00001700
Iteration 19/1000 | Loss: 0.00001700
Iteration 20/1000 | Loss: 0.00001699
Iteration 21/1000 | Loss: 0.00001699
Iteration 22/1000 | Loss: 0.00001699
Iteration 23/1000 | Loss: 0.00001694
Iteration 24/1000 | Loss: 0.00001694
Iteration 25/1000 | Loss: 0.00001693
Iteration 26/1000 | Loss: 0.00001692
Iteration 27/1000 | Loss: 0.00001692
Iteration 28/1000 | Loss: 0.00001691
Iteration 29/1000 | Loss: 0.00001690
Iteration 30/1000 | Loss: 0.00001690
Iteration 31/1000 | Loss: 0.00001690
Iteration 32/1000 | Loss: 0.00001689
Iteration 33/1000 | Loss: 0.00001689
Iteration 34/1000 | Loss: 0.00001689
Iteration 35/1000 | Loss: 0.00001689
Iteration 36/1000 | Loss: 0.00001689
Iteration 37/1000 | Loss: 0.00001689
Iteration 38/1000 | Loss: 0.00001689
Iteration 39/1000 | Loss: 0.00001689
Iteration 40/1000 | Loss: 0.00001688
Iteration 41/1000 | Loss: 0.00001688
Iteration 42/1000 | Loss: 0.00001688
Iteration 43/1000 | Loss: 0.00001687
Iteration 44/1000 | Loss: 0.00001687
Iteration 45/1000 | Loss: 0.00001687
Iteration 46/1000 | Loss: 0.00001687
Iteration 47/1000 | Loss: 0.00001686
Iteration 48/1000 | Loss: 0.00001686
Iteration 49/1000 | Loss: 0.00001686
Iteration 50/1000 | Loss: 0.00001686
Iteration 51/1000 | Loss: 0.00001686
Iteration 52/1000 | Loss: 0.00001686
Iteration 53/1000 | Loss: 0.00001685
Iteration 54/1000 | Loss: 0.00001685
Iteration 55/1000 | Loss: 0.00001685
Iteration 56/1000 | Loss: 0.00001685
Iteration 57/1000 | Loss: 0.00001685
Iteration 58/1000 | Loss: 0.00001685
Iteration 59/1000 | Loss: 0.00001685
Iteration 60/1000 | Loss: 0.00001685
Iteration 61/1000 | Loss: 0.00001685
Iteration 62/1000 | Loss: 0.00001685
Iteration 63/1000 | Loss: 0.00001685
Iteration 64/1000 | Loss: 0.00001685
Iteration 65/1000 | Loss: 0.00001685
Iteration 66/1000 | Loss: 0.00001685
Iteration 67/1000 | Loss: 0.00001685
Iteration 68/1000 | Loss: 0.00001685
Iteration 69/1000 | Loss: 0.00001684
Iteration 70/1000 | Loss: 0.00001684
Iteration 71/1000 | Loss: 0.00001684
Iteration 72/1000 | Loss: 0.00001684
Iteration 73/1000 | Loss: 0.00001684
Iteration 74/1000 | Loss: 0.00001684
Iteration 75/1000 | Loss: 0.00001684
Iteration 76/1000 | Loss: 0.00001684
Iteration 77/1000 | Loss: 0.00001684
Iteration 78/1000 | Loss: 0.00001683
Iteration 79/1000 | Loss: 0.00001683
Iteration 80/1000 | Loss: 0.00001683
Iteration 81/1000 | Loss: 0.00001683
Iteration 82/1000 | Loss: 0.00001682
Iteration 83/1000 | Loss: 0.00001682
Iteration 84/1000 | Loss: 0.00001682
Iteration 85/1000 | Loss: 0.00001681
Iteration 86/1000 | Loss: 0.00001681
Iteration 87/1000 | Loss: 0.00001681
Iteration 88/1000 | Loss: 0.00001681
Iteration 89/1000 | Loss: 0.00001681
Iteration 90/1000 | Loss: 0.00001681
Iteration 91/1000 | Loss: 0.00001681
Iteration 92/1000 | Loss: 0.00001681
Iteration 93/1000 | Loss: 0.00001681
Iteration 94/1000 | Loss: 0.00001680
Iteration 95/1000 | Loss: 0.00001680
Iteration 96/1000 | Loss: 0.00001680
Iteration 97/1000 | Loss: 0.00001680
Iteration 98/1000 | Loss: 0.00001679
Iteration 99/1000 | Loss: 0.00001679
Iteration 100/1000 | Loss: 0.00001679
Iteration 101/1000 | Loss: 0.00001679
Iteration 102/1000 | Loss: 0.00001679
Iteration 103/1000 | Loss: 0.00001679
Iteration 104/1000 | Loss: 0.00001679
Iteration 105/1000 | Loss: 0.00001679
Iteration 106/1000 | Loss: 0.00001679
Iteration 107/1000 | Loss: 0.00001679
Iteration 108/1000 | Loss: 0.00001679
Iteration 109/1000 | Loss: 0.00001678
Iteration 110/1000 | Loss: 0.00001678
Iteration 111/1000 | Loss: 0.00001678
Iteration 112/1000 | Loss: 0.00001678
Iteration 113/1000 | Loss: 0.00001678
Iteration 114/1000 | Loss: 0.00001678
Iteration 115/1000 | Loss: 0.00001678
Iteration 116/1000 | Loss: 0.00001678
Iteration 117/1000 | Loss: 0.00001678
Iteration 118/1000 | Loss: 0.00001677
Iteration 119/1000 | Loss: 0.00001676
Iteration 120/1000 | Loss: 0.00001676
Iteration 121/1000 | Loss: 0.00001676
Iteration 122/1000 | Loss: 0.00001675
Iteration 123/1000 | Loss: 0.00001675
Iteration 124/1000 | Loss: 0.00001675
Iteration 125/1000 | Loss: 0.00001675
Iteration 126/1000 | Loss: 0.00001675
Iteration 127/1000 | Loss: 0.00001675
Iteration 128/1000 | Loss: 0.00001675
Iteration 129/1000 | Loss: 0.00001675
Iteration 130/1000 | Loss: 0.00001675
Iteration 131/1000 | Loss: 0.00001675
Iteration 132/1000 | Loss: 0.00001675
Iteration 133/1000 | Loss: 0.00001675
Iteration 134/1000 | Loss: 0.00001675
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 134. Stopping optimization.
Last 5 losses: [1.6753287127357908e-05, 1.6753287127357908e-05, 1.6753287127357908e-05, 1.6753287127357908e-05, 1.6753287127357908e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6753287127357908e-05

Optimization complete. Final v2v error: 3.4369547367095947 mm

Highest mean error: 3.631593942642212 mm for frame 127

Lowest mean error: 3.3109982013702393 mm for frame 72

Saving results

Total time: 34.616891384124756
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_006/1078/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_006/1078.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_006/1078
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01085017
Iteration 2/25 | Loss: 0.00201519
Iteration 3/25 | Loss: 0.00118622
Iteration 4/25 | Loss: 0.00118903
Iteration 5/25 | Loss: 0.00111538
Iteration 6/25 | Loss: 0.00095958
Iteration 7/25 | Loss: 0.00090893
Iteration 8/25 | Loss: 0.00101186
Iteration 9/25 | Loss: 0.00116038
Iteration 10/25 | Loss: 0.00087391
Iteration 11/25 | Loss: 0.00089766
Iteration 12/25 | Loss: 0.00087354
Iteration 13/25 | Loss: 0.00078872
Iteration 14/25 | Loss: 0.00074978
Iteration 15/25 | Loss: 0.00080080
Iteration 16/25 | Loss: 0.00074840
Iteration 17/25 | Loss: 0.00074893
Iteration 18/25 | Loss: 0.00074443
Iteration 19/25 | Loss: 0.00074551
Iteration 20/25 | Loss: 0.00074881
Iteration 21/25 | Loss: 0.00073255
Iteration 22/25 | Loss: 0.00074099
Iteration 23/25 | Loss: 0.00073185
Iteration 24/25 | Loss: 0.00073897
Iteration 25/25 | Loss: 0.00073834

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.59939802
Iteration 2/25 | Loss: 0.00338962
Iteration 3/25 | Loss: 0.00338962
Iteration 4/25 | Loss: 0.00338962
Iteration 5/25 | Loss: 0.00338962
Iteration 6/25 | Loss: 0.00338962
Iteration 7/25 | Loss: 0.00338962
Iteration 8/25 | Loss: 0.00338962
Iteration 9/25 | Loss: 0.00338962
Iteration 10/25 | Loss: 0.00338962
Iteration 11/25 | Loss: 0.00338962
Iteration 12/25 | Loss: 0.00338962
Iteration 13/25 | Loss: 0.00338962
Iteration 14/25 | Loss: 0.00338962
Iteration 15/25 | Loss: 0.00338962
Iteration 16/25 | Loss: 0.00338962
Iteration 17/25 | Loss: 0.00338962
Iteration 18/25 | Loss: 0.00338962
Iteration 19/25 | Loss: 0.00338962
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0033896195236593485, 0.0033896195236593485, 0.0033896195236593485, 0.0033896195236593485, 0.0033896195236593485]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0033896195236593485

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00338962
Iteration 2/1000 | Loss: 0.00824113
Iteration 3/1000 | Loss: 0.00404649
Iteration 4/1000 | Loss: 0.00435997
Iteration 5/1000 | Loss: 0.00560990
Iteration 6/1000 | Loss: 0.00266966
Iteration 7/1000 | Loss: 0.00854791
Iteration 8/1000 | Loss: 0.00522694
Iteration 9/1000 | Loss: 0.00241560
Iteration 10/1000 | Loss: 0.00194795
Iteration 11/1000 | Loss: 0.00218566
Iteration 12/1000 | Loss: 0.00140067
Iteration 13/1000 | Loss: 0.00204511
Iteration 14/1000 | Loss: 0.00115333
Iteration 15/1000 | Loss: 0.00256428
Iteration 16/1000 | Loss: 0.00161945
Iteration 17/1000 | Loss: 0.00120757
Iteration 18/1000 | Loss: 0.00197689
Iteration 19/1000 | Loss: 0.00218623
Iteration 20/1000 | Loss: 0.00277190
Iteration 21/1000 | Loss: 0.00209265
Iteration 22/1000 | Loss: 0.00242962
Iteration 23/1000 | Loss: 0.00234019
Iteration 24/1000 | Loss: 0.00129754
Iteration 25/1000 | Loss: 0.00251615
Iteration 26/1000 | Loss: 0.00168891
Iteration 27/1000 | Loss: 0.00176112
Iteration 28/1000 | Loss: 0.00162800
Iteration 29/1000 | Loss: 0.00127487
Iteration 30/1000 | Loss: 0.00092748
Iteration 31/1000 | Loss: 0.00073708
Iteration 32/1000 | Loss: 0.00032978
Iteration 33/1000 | Loss: 0.00027844
Iteration 34/1000 | Loss: 0.00009670
Iteration 35/1000 | Loss: 0.00002526
Iteration 36/1000 | Loss: 0.00042020
Iteration 37/1000 | Loss: 0.00024387
Iteration 38/1000 | Loss: 0.00059550
Iteration 39/1000 | Loss: 0.00087139
Iteration 40/1000 | Loss: 0.00037098
Iteration 41/1000 | Loss: 0.00005337
Iteration 42/1000 | Loss: 0.00003003
Iteration 43/1000 | Loss: 0.00002601
Iteration 44/1000 | Loss: 0.00002005
Iteration 45/1000 | Loss: 0.00001667
Iteration 46/1000 | Loss: 0.00001471
Iteration 47/1000 | Loss: 0.00001417
Iteration 48/1000 | Loss: 0.00001377
Iteration 49/1000 | Loss: 0.00001343
Iteration 50/1000 | Loss: 0.00001319
Iteration 51/1000 | Loss: 0.00001291
Iteration 52/1000 | Loss: 0.00001270
Iteration 53/1000 | Loss: 0.00001269
Iteration 54/1000 | Loss: 0.00001263
Iteration 55/1000 | Loss: 0.00001262
Iteration 56/1000 | Loss: 0.00001249
Iteration 57/1000 | Loss: 0.00001248
Iteration 58/1000 | Loss: 0.00001247
Iteration 59/1000 | Loss: 0.00001245
Iteration 60/1000 | Loss: 0.00001244
Iteration 61/1000 | Loss: 0.00001244
Iteration 62/1000 | Loss: 0.00001243
Iteration 63/1000 | Loss: 0.00001242
Iteration 64/1000 | Loss: 0.00001242
Iteration 65/1000 | Loss: 0.00001239
Iteration 66/1000 | Loss: 0.00001233
Iteration 67/1000 | Loss: 0.00001233
Iteration 68/1000 | Loss: 0.00001232
Iteration 69/1000 | Loss: 0.00001232
Iteration 70/1000 | Loss: 0.00001231
Iteration 71/1000 | Loss: 0.00001231
Iteration 72/1000 | Loss: 0.00001231
Iteration 73/1000 | Loss: 0.00001231
Iteration 74/1000 | Loss: 0.00001231
Iteration 75/1000 | Loss: 0.00001230
Iteration 76/1000 | Loss: 0.00001230
Iteration 77/1000 | Loss: 0.00001230
Iteration 78/1000 | Loss: 0.00001230
Iteration 79/1000 | Loss: 0.00001230
Iteration 80/1000 | Loss: 0.00001230
Iteration 81/1000 | Loss: 0.00001229
Iteration 82/1000 | Loss: 0.00001228
Iteration 83/1000 | Loss: 0.00001228
Iteration 84/1000 | Loss: 0.00001228
Iteration 85/1000 | Loss: 0.00001227
Iteration 86/1000 | Loss: 0.00001227
Iteration 87/1000 | Loss: 0.00001227
Iteration 88/1000 | Loss: 0.00001227
Iteration 89/1000 | Loss: 0.00001226
Iteration 90/1000 | Loss: 0.00001226
Iteration 91/1000 | Loss: 0.00001226
Iteration 92/1000 | Loss: 0.00001226
Iteration 93/1000 | Loss: 0.00001226
Iteration 94/1000 | Loss: 0.00001226
Iteration 95/1000 | Loss: 0.00001225
Iteration 96/1000 | Loss: 0.00001225
Iteration 97/1000 | Loss: 0.00001225
Iteration 98/1000 | Loss: 0.00001225
Iteration 99/1000 | Loss: 0.00001225
Iteration 100/1000 | Loss: 0.00001225
Iteration 101/1000 | Loss: 0.00001225
Iteration 102/1000 | Loss: 0.00001225
Iteration 103/1000 | Loss: 0.00001225
Iteration 104/1000 | Loss: 0.00001224
Iteration 105/1000 | Loss: 0.00001224
Iteration 106/1000 | Loss: 0.00001224
Iteration 107/1000 | Loss: 0.00001224
Iteration 108/1000 | Loss: 0.00001224
Iteration 109/1000 | Loss: 0.00001224
Iteration 110/1000 | Loss: 0.00001224
Iteration 111/1000 | Loss: 0.00001223
Iteration 112/1000 | Loss: 0.00001223
Iteration 113/1000 | Loss: 0.00001223
Iteration 114/1000 | Loss: 0.00001222
Iteration 115/1000 | Loss: 0.00001222
Iteration 116/1000 | Loss: 0.00001222
Iteration 117/1000 | Loss: 0.00001221
Iteration 118/1000 | Loss: 0.00001221
Iteration 119/1000 | Loss: 0.00001221
Iteration 120/1000 | Loss: 0.00001221
Iteration 121/1000 | Loss: 0.00001221
Iteration 122/1000 | Loss: 0.00001220
Iteration 123/1000 | Loss: 0.00001220
Iteration 124/1000 | Loss: 0.00001220
Iteration 125/1000 | Loss: 0.00001220
Iteration 126/1000 | Loss: 0.00001219
Iteration 127/1000 | Loss: 0.00001219
Iteration 128/1000 | Loss: 0.00001219
Iteration 129/1000 | Loss: 0.00001219
Iteration 130/1000 | Loss: 0.00001219
Iteration 131/1000 | Loss: 0.00001219
Iteration 132/1000 | Loss: 0.00001219
Iteration 133/1000 | Loss: 0.00001219
Iteration 134/1000 | Loss: 0.00001219
Iteration 135/1000 | Loss: 0.00001219
Iteration 136/1000 | Loss: 0.00001219
Iteration 137/1000 | Loss: 0.00001219
Iteration 138/1000 | Loss: 0.00001219
Iteration 139/1000 | Loss: 0.00001219
Iteration 140/1000 | Loss: 0.00001219
Iteration 141/1000 | Loss: 0.00001219
Iteration 142/1000 | Loss: 0.00001219
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 142. Stopping optimization.
Last 5 losses: [1.219184650835814e-05, 1.219184650835814e-05, 1.219184650835814e-05, 1.219184650835814e-05, 1.219184650835814e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.219184650835814e-05

Optimization complete. Final v2v error: 2.8760528564453125 mm

Highest mean error: 9.013866424560547 mm for frame 65

Lowest mean error: 2.5132460594177246 mm for frame 31

Saving results

Total time: 127.59468746185303
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_006/1013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_006/1013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_006/1013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00889449
Iteration 2/25 | Loss: 0.00121009
Iteration 3/25 | Loss: 0.00083678
Iteration 4/25 | Loss: 0.00079129
Iteration 5/25 | Loss: 0.00077790
Iteration 6/25 | Loss: 0.00077494
Iteration 7/25 | Loss: 0.00077447
Iteration 8/25 | Loss: 0.00077447
Iteration 9/25 | Loss: 0.00077447
Iteration 10/25 | Loss: 0.00077447
Iteration 11/25 | Loss: 0.00077447
Iteration 12/25 | Loss: 0.00077447
Iteration 13/25 | Loss: 0.00077447
Iteration 14/25 | Loss: 0.00077447
Iteration 15/25 | Loss: 0.00077447
Iteration 16/25 | Loss: 0.00077447
Iteration 17/25 | Loss: 0.00077447
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0007744703907519579, 0.0007744703907519579, 0.0007744703907519579, 0.0007744703907519579, 0.0007744703907519579]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007744703907519579

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36069930
Iteration 2/25 | Loss: 0.00040549
Iteration 3/25 | Loss: 0.00040549
Iteration 4/25 | Loss: 0.00040549
Iteration 5/25 | Loss: 0.00040549
Iteration 6/25 | Loss: 0.00040549
Iteration 7/25 | Loss: 0.00040549
Iteration 8/25 | Loss: 0.00040549
Iteration 9/25 | Loss: 0.00040549
Iteration 10/25 | Loss: 0.00040549
Iteration 11/25 | Loss: 0.00040549
Iteration 12/25 | Loss: 0.00040549
Iteration 13/25 | Loss: 0.00040549
Iteration 14/25 | Loss: 0.00040549
Iteration 15/25 | Loss: 0.00040549
Iteration 16/25 | Loss: 0.00040549
Iteration 17/25 | Loss: 0.00040549
Iteration 18/25 | Loss: 0.00040549
Iteration 19/25 | Loss: 0.00040549
Iteration 20/25 | Loss: 0.00040549
Iteration 21/25 | Loss: 0.00040549
Iteration 22/25 | Loss: 0.00040549
Iteration 23/25 | Loss: 0.00040549
Iteration 24/25 | Loss: 0.00040549
Iteration 25/25 | Loss: 0.00040549

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00040549
Iteration 2/1000 | Loss: 0.00005049
Iteration 3/1000 | Loss: 0.00003536
Iteration 4/1000 | Loss: 0.00002989
Iteration 5/1000 | Loss: 0.00002835
Iteration 6/1000 | Loss: 0.00002719
Iteration 7/1000 | Loss: 0.00002668
Iteration 8/1000 | Loss: 0.00002621
Iteration 9/1000 | Loss: 0.00002584
Iteration 10/1000 | Loss: 0.00002559
Iteration 11/1000 | Loss: 0.00002537
Iteration 12/1000 | Loss: 0.00002532
Iteration 13/1000 | Loss: 0.00002516
Iteration 14/1000 | Loss: 0.00002512
Iteration 15/1000 | Loss: 0.00002510
Iteration 16/1000 | Loss: 0.00002505
Iteration 17/1000 | Loss: 0.00002505
Iteration 18/1000 | Loss: 0.00002501
Iteration 19/1000 | Loss: 0.00002501
Iteration 20/1000 | Loss: 0.00002498
Iteration 21/1000 | Loss: 0.00002497
Iteration 22/1000 | Loss: 0.00002496
Iteration 23/1000 | Loss: 0.00002494
Iteration 24/1000 | Loss: 0.00002488
Iteration 25/1000 | Loss: 0.00002488
Iteration 26/1000 | Loss: 0.00002482
Iteration 27/1000 | Loss: 0.00002481
Iteration 28/1000 | Loss: 0.00002479
Iteration 29/1000 | Loss: 0.00002479
Iteration 30/1000 | Loss: 0.00002479
Iteration 31/1000 | Loss: 0.00002479
Iteration 32/1000 | Loss: 0.00002479
Iteration 33/1000 | Loss: 0.00002479
Iteration 34/1000 | Loss: 0.00002479
Iteration 35/1000 | Loss: 0.00002479
Iteration 36/1000 | Loss: 0.00002479
Iteration 37/1000 | Loss: 0.00002479
Iteration 38/1000 | Loss: 0.00002479
Iteration 39/1000 | Loss: 0.00002478
Iteration 40/1000 | Loss: 0.00002478
Iteration 41/1000 | Loss: 0.00002478
Iteration 42/1000 | Loss: 0.00002478
Iteration 43/1000 | Loss: 0.00002478
Iteration 44/1000 | Loss: 0.00002478
Iteration 45/1000 | Loss: 0.00002478
Iteration 46/1000 | Loss: 0.00002478
Iteration 47/1000 | Loss: 0.00002478
Iteration 48/1000 | Loss: 0.00002477
Iteration 49/1000 | Loss: 0.00002477
Iteration 50/1000 | Loss: 0.00002477
Iteration 51/1000 | Loss: 0.00002477
Iteration 52/1000 | Loss: 0.00002477
Iteration 53/1000 | Loss: 0.00002476
Iteration 54/1000 | Loss: 0.00002476
Iteration 55/1000 | Loss: 0.00002476
Iteration 56/1000 | Loss: 0.00002475
Iteration 57/1000 | Loss: 0.00002475
Iteration 58/1000 | Loss: 0.00002475
Iteration 59/1000 | Loss: 0.00002475
Iteration 60/1000 | Loss: 0.00002474
Iteration 61/1000 | Loss: 0.00002474
Iteration 62/1000 | Loss: 0.00002474
Iteration 63/1000 | Loss: 0.00002474
Iteration 64/1000 | Loss: 0.00002473
Iteration 65/1000 | Loss: 0.00002473
Iteration 66/1000 | Loss: 0.00002473
Iteration 67/1000 | Loss: 0.00002473
Iteration 68/1000 | Loss: 0.00002473
Iteration 69/1000 | Loss: 0.00002472
Iteration 70/1000 | Loss: 0.00002472
Iteration 71/1000 | Loss: 0.00002472
Iteration 72/1000 | Loss: 0.00002472
Iteration 73/1000 | Loss: 0.00002472
Iteration 74/1000 | Loss: 0.00002472
Iteration 75/1000 | Loss: 0.00002471
Iteration 76/1000 | Loss: 0.00002471
Iteration 77/1000 | Loss: 0.00002471
Iteration 78/1000 | Loss: 0.00002471
Iteration 79/1000 | Loss: 0.00002471
Iteration 80/1000 | Loss: 0.00002471
Iteration 81/1000 | Loss: 0.00002471
Iteration 82/1000 | Loss: 0.00002471
Iteration 83/1000 | Loss: 0.00002470
Iteration 84/1000 | Loss: 0.00002470
Iteration 85/1000 | Loss: 0.00002470
Iteration 86/1000 | Loss: 0.00002470
Iteration 87/1000 | Loss: 0.00002470
Iteration 88/1000 | Loss: 0.00002470
Iteration 89/1000 | Loss: 0.00002470
Iteration 90/1000 | Loss: 0.00002469
Iteration 91/1000 | Loss: 0.00002469
Iteration 92/1000 | Loss: 0.00002469
Iteration 93/1000 | Loss: 0.00002469
Iteration 94/1000 | Loss: 0.00002468
Iteration 95/1000 | Loss: 0.00002468
Iteration 96/1000 | Loss: 0.00002468
Iteration 97/1000 | Loss: 0.00002467
Iteration 98/1000 | Loss: 0.00002467
Iteration 99/1000 | Loss: 0.00002466
Iteration 100/1000 | Loss: 0.00002466
Iteration 101/1000 | Loss: 0.00002466
Iteration 102/1000 | Loss: 0.00002466
Iteration 103/1000 | Loss: 0.00002466
Iteration 104/1000 | Loss: 0.00002465
Iteration 105/1000 | Loss: 0.00002465
Iteration 106/1000 | Loss: 0.00002465
Iteration 107/1000 | Loss: 0.00002465
Iteration 108/1000 | Loss: 0.00002464
Iteration 109/1000 | Loss: 0.00002464
Iteration 110/1000 | Loss: 0.00002464
Iteration 111/1000 | Loss: 0.00002464
Iteration 112/1000 | Loss: 0.00002463
Iteration 113/1000 | Loss: 0.00002463
Iteration 114/1000 | Loss: 0.00002463
Iteration 115/1000 | Loss: 0.00002462
Iteration 116/1000 | Loss: 0.00002462
Iteration 117/1000 | Loss: 0.00002462
Iteration 118/1000 | Loss: 0.00002462
Iteration 119/1000 | Loss: 0.00002462
Iteration 120/1000 | Loss: 0.00002462
Iteration 121/1000 | Loss: 0.00002462
Iteration 122/1000 | Loss: 0.00002462
Iteration 123/1000 | Loss: 0.00002462
Iteration 124/1000 | Loss: 0.00002462
Iteration 125/1000 | Loss: 0.00002462
Iteration 126/1000 | Loss: 0.00002461
Iteration 127/1000 | Loss: 0.00002461
Iteration 128/1000 | Loss: 0.00002461
Iteration 129/1000 | Loss: 0.00002461
Iteration 130/1000 | Loss: 0.00002461
Iteration 131/1000 | Loss: 0.00002461
Iteration 132/1000 | Loss: 0.00002461
Iteration 133/1000 | Loss: 0.00002461
Iteration 134/1000 | Loss: 0.00002461
Iteration 135/1000 | Loss: 0.00002460
Iteration 136/1000 | Loss: 0.00002460
Iteration 137/1000 | Loss: 0.00002460
Iteration 138/1000 | Loss: 0.00002460
Iteration 139/1000 | Loss: 0.00002460
Iteration 140/1000 | Loss: 0.00002460
Iteration 141/1000 | Loss: 0.00002460
Iteration 142/1000 | Loss: 0.00002460
Iteration 143/1000 | Loss: 0.00002459
Iteration 144/1000 | Loss: 0.00002459
Iteration 145/1000 | Loss: 0.00002459
Iteration 146/1000 | Loss: 0.00002459
Iteration 147/1000 | Loss: 0.00002459
Iteration 148/1000 | Loss: 0.00002459
Iteration 149/1000 | Loss: 0.00002459
Iteration 150/1000 | Loss: 0.00002458
Iteration 151/1000 | Loss: 0.00002458
Iteration 152/1000 | Loss: 0.00002458
Iteration 153/1000 | Loss: 0.00002458
Iteration 154/1000 | Loss: 0.00002458
Iteration 155/1000 | Loss: 0.00002458
Iteration 156/1000 | Loss: 0.00002458
Iteration 157/1000 | Loss: 0.00002458
Iteration 158/1000 | Loss: 0.00002458
Iteration 159/1000 | Loss: 0.00002458
Iteration 160/1000 | Loss: 0.00002458
Iteration 161/1000 | Loss: 0.00002458
Iteration 162/1000 | Loss: 0.00002458
Iteration 163/1000 | Loss: 0.00002458
Iteration 164/1000 | Loss: 0.00002458
Iteration 165/1000 | Loss: 0.00002458
Iteration 166/1000 | Loss: 0.00002458
Iteration 167/1000 | Loss: 0.00002458
Iteration 168/1000 | Loss: 0.00002458
Iteration 169/1000 | Loss: 0.00002458
Iteration 170/1000 | Loss: 0.00002458
Iteration 171/1000 | Loss: 0.00002458
Iteration 172/1000 | Loss: 0.00002458
Iteration 173/1000 | Loss: 0.00002458
Iteration 174/1000 | Loss: 0.00002458
Iteration 175/1000 | Loss: 0.00002458
Iteration 176/1000 | Loss: 0.00002458
Iteration 177/1000 | Loss: 0.00002458
Iteration 178/1000 | Loss: 0.00002458
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 178. Stopping optimization.
Last 5 losses: [2.4578273951192386e-05, 2.4578273951192386e-05, 2.4578273951192386e-05, 2.4578273951192386e-05, 2.4578273951192386e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4578273951192386e-05

Optimization complete. Final v2v error: 3.9757256507873535 mm

Highest mean error: 6.069734573364258 mm for frame 101

Lowest mean error: 3.0596845149993896 mm for frame 126

Saving results

Total time: 45.38418626785278
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_006/1020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_006/1020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_006/1020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00461251
Iteration 2/25 | Loss: 0.00089389
Iteration 3/25 | Loss: 0.00076449
Iteration 4/25 | Loss: 0.00072636
Iteration 5/25 | Loss: 0.00071122
Iteration 6/25 | Loss: 0.00070812
Iteration 7/25 | Loss: 0.00070660
Iteration 8/25 | Loss: 0.00070648
Iteration 9/25 | Loss: 0.00070648
Iteration 10/25 | Loss: 0.00070648
Iteration 11/25 | Loss: 0.00070648
Iteration 12/25 | Loss: 0.00070648
Iteration 13/25 | Loss: 0.00070648
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.000706477731000632, 0.000706477731000632, 0.000706477731000632, 0.000706477731000632, 0.000706477731000632]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000706477731000632

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.84850168
Iteration 2/25 | Loss: 0.00030914
Iteration 3/25 | Loss: 0.00030912
Iteration 4/25 | Loss: 0.00030912
Iteration 5/25 | Loss: 0.00030912
Iteration 6/25 | Loss: 0.00030912
Iteration 7/25 | Loss: 0.00030912
Iteration 8/25 | Loss: 0.00030912
Iteration 9/25 | Loss: 0.00030912
Iteration 10/25 | Loss: 0.00030912
Iteration 11/25 | Loss: 0.00030912
Iteration 12/25 | Loss: 0.00030912
Iteration 13/25 | Loss: 0.00030912
Iteration 14/25 | Loss: 0.00030912
Iteration 15/25 | Loss: 0.00030912
Iteration 16/25 | Loss: 0.00030912
Iteration 17/25 | Loss: 0.00030912
Iteration 18/25 | Loss: 0.00030912
Iteration 19/25 | Loss: 0.00030912
Iteration 20/25 | Loss: 0.00030912
Iteration 21/25 | Loss: 0.00030912
Iteration 22/25 | Loss: 0.00030912
Iteration 23/25 | Loss: 0.00030912
Iteration 24/25 | Loss: 0.00030912
Iteration 25/25 | Loss: 0.00030912

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00030912
Iteration 2/1000 | Loss: 0.00003293
Iteration 3/1000 | Loss: 0.00002627
Iteration 4/1000 | Loss: 0.00002484
Iteration 5/1000 | Loss: 0.00002400
Iteration 6/1000 | Loss: 0.00002348
Iteration 7/1000 | Loss: 0.00002305
Iteration 8/1000 | Loss: 0.00002273
Iteration 9/1000 | Loss: 0.00002252
Iteration 10/1000 | Loss: 0.00002243
Iteration 11/1000 | Loss: 0.00002228
Iteration 12/1000 | Loss: 0.00002227
Iteration 13/1000 | Loss: 0.00002227
Iteration 14/1000 | Loss: 0.00002220
Iteration 15/1000 | Loss: 0.00002220
Iteration 16/1000 | Loss: 0.00002215
Iteration 17/1000 | Loss: 0.00002215
Iteration 18/1000 | Loss: 0.00002215
Iteration 19/1000 | Loss: 0.00002214
Iteration 20/1000 | Loss: 0.00002213
Iteration 21/1000 | Loss: 0.00002213
Iteration 22/1000 | Loss: 0.00002213
Iteration 23/1000 | Loss: 0.00002213
Iteration 24/1000 | Loss: 0.00002213
Iteration 25/1000 | Loss: 0.00002213
Iteration 26/1000 | Loss: 0.00002212
Iteration 27/1000 | Loss: 0.00002212
Iteration 28/1000 | Loss: 0.00002212
Iteration 29/1000 | Loss: 0.00002212
Iteration 30/1000 | Loss: 0.00002211
Iteration 31/1000 | Loss: 0.00002210
Iteration 32/1000 | Loss: 0.00002209
Iteration 33/1000 | Loss: 0.00002209
Iteration 34/1000 | Loss: 0.00002209
Iteration 35/1000 | Loss: 0.00002209
Iteration 36/1000 | Loss: 0.00002208
Iteration 37/1000 | Loss: 0.00002208
Iteration 38/1000 | Loss: 0.00002208
Iteration 39/1000 | Loss: 0.00002208
Iteration 40/1000 | Loss: 0.00002207
Iteration 41/1000 | Loss: 0.00002207
Iteration 42/1000 | Loss: 0.00002207
Iteration 43/1000 | Loss: 0.00002207
Iteration 44/1000 | Loss: 0.00002207
Iteration 45/1000 | Loss: 0.00002207
Iteration 46/1000 | Loss: 0.00002206
Iteration 47/1000 | Loss: 0.00002206
Iteration 48/1000 | Loss: 0.00002206
Iteration 49/1000 | Loss: 0.00002206
Iteration 50/1000 | Loss: 0.00002206
Iteration 51/1000 | Loss: 0.00002206
Iteration 52/1000 | Loss: 0.00002206
Iteration 53/1000 | Loss: 0.00002206
Iteration 54/1000 | Loss: 0.00002206
Iteration 55/1000 | Loss: 0.00002206
Iteration 56/1000 | Loss: 0.00002206
Iteration 57/1000 | Loss: 0.00002205
Iteration 58/1000 | Loss: 0.00002205
Iteration 59/1000 | Loss: 0.00002204
Iteration 60/1000 | Loss: 0.00002204
Iteration 61/1000 | Loss: 0.00002204
Iteration 62/1000 | Loss: 0.00002203
Iteration 63/1000 | Loss: 0.00002203
Iteration 64/1000 | Loss: 0.00002202
Iteration 65/1000 | Loss: 0.00002202
Iteration 66/1000 | Loss: 0.00002202
Iteration 67/1000 | Loss: 0.00002202
Iteration 68/1000 | Loss: 0.00002201
Iteration 69/1000 | Loss: 0.00002201
Iteration 70/1000 | Loss: 0.00002201
Iteration 71/1000 | Loss: 0.00002200
Iteration 72/1000 | Loss: 0.00002200
Iteration 73/1000 | Loss: 0.00002200
Iteration 74/1000 | Loss: 0.00002200
Iteration 75/1000 | Loss: 0.00002200
Iteration 76/1000 | Loss: 0.00002199
Iteration 77/1000 | Loss: 0.00002199
Iteration 78/1000 | Loss: 0.00002198
Iteration 79/1000 | Loss: 0.00002198
Iteration 80/1000 | Loss: 0.00002198
Iteration 81/1000 | Loss: 0.00002198
Iteration 82/1000 | Loss: 0.00002198
Iteration 83/1000 | Loss: 0.00002198
Iteration 84/1000 | Loss: 0.00002198
Iteration 85/1000 | Loss: 0.00002198
Iteration 86/1000 | Loss: 0.00002198
Iteration 87/1000 | Loss: 0.00002198
Iteration 88/1000 | Loss: 0.00002198
Iteration 89/1000 | Loss: 0.00002197
Iteration 90/1000 | Loss: 0.00002197
Iteration 91/1000 | Loss: 0.00002197
Iteration 92/1000 | Loss: 0.00002197
Iteration 93/1000 | Loss: 0.00002197
Iteration 94/1000 | Loss: 0.00002197
Iteration 95/1000 | Loss: 0.00002196
Iteration 96/1000 | Loss: 0.00002196
Iteration 97/1000 | Loss: 0.00002196
Iteration 98/1000 | Loss: 0.00002196
Iteration 99/1000 | Loss: 0.00002196
Iteration 100/1000 | Loss: 0.00002196
Iteration 101/1000 | Loss: 0.00002196
Iteration 102/1000 | Loss: 0.00002195
Iteration 103/1000 | Loss: 0.00002195
Iteration 104/1000 | Loss: 0.00002195
Iteration 105/1000 | Loss: 0.00002195
Iteration 106/1000 | Loss: 0.00002195
Iteration 107/1000 | Loss: 0.00002195
Iteration 108/1000 | Loss: 0.00002195
Iteration 109/1000 | Loss: 0.00002194
Iteration 110/1000 | Loss: 0.00002194
Iteration 111/1000 | Loss: 0.00002194
Iteration 112/1000 | Loss: 0.00002194
Iteration 113/1000 | Loss: 0.00002194
Iteration 114/1000 | Loss: 0.00002194
Iteration 115/1000 | Loss: 0.00002194
Iteration 116/1000 | Loss: 0.00002194
Iteration 117/1000 | Loss: 0.00002194
Iteration 118/1000 | Loss: 0.00002194
Iteration 119/1000 | Loss: 0.00002193
Iteration 120/1000 | Loss: 0.00002193
Iteration 121/1000 | Loss: 0.00002193
Iteration 122/1000 | Loss: 0.00002193
Iteration 123/1000 | Loss: 0.00002193
Iteration 124/1000 | Loss: 0.00002193
Iteration 125/1000 | Loss: 0.00002193
Iteration 126/1000 | Loss: 0.00002193
Iteration 127/1000 | Loss: 0.00002193
Iteration 128/1000 | Loss: 0.00002192
Iteration 129/1000 | Loss: 0.00002192
Iteration 130/1000 | Loss: 0.00002192
Iteration 131/1000 | Loss: 0.00002192
Iteration 132/1000 | Loss: 0.00002192
Iteration 133/1000 | Loss: 0.00002192
Iteration 134/1000 | Loss: 0.00002192
Iteration 135/1000 | Loss: 0.00002192
Iteration 136/1000 | Loss: 0.00002192
Iteration 137/1000 | Loss: 0.00002191
Iteration 138/1000 | Loss: 0.00002191
Iteration 139/1000 | Loss: 0.00002191
Iteration 140/1000 | Loss: 0.00002191
Iteration 141/1000 | Loss: 0.00002191
Iteration 142/1000 | Loss: 0.00002191
Iteration 143/1000 | Loss: 0.00002191
Iteration 144/1000 | Loss: 0.00002191
Iteration 145/1000 | Loss: 0.00002191
Iteration 146/1000 | Loss: 0.00002191
Iteration 147/1000 | Loss: 0.00002191
Iteration 148/1000 | Loss: 0.00002191
Iteration 149/1000 | Loss: 0.00002191
Iteration 150/1000 | Loss: 0.00002191
Iteration 151/1000 | Loss: 0.00002191
Iteration 152/1000 | Loss: 0.00002191
Iteration 153/1000 | Loss: 0.00002190
Iteration 154/1000 | Loss: 0.00002190
Iteration 155/1000 | Loss: 0.00002190
Iteration 156/1000 | Loss: 0.00002190
Iteration 157/1000 | Loss: 0.00002190
Iteration 158/1000 | Loss: 0.00002190
Iteration 159/1000 | Loss: 0.00002190
Iteration 160/1000 | Loss: 0.00002190
Iteration 161/1000 | Loss: 0.00002190
Iteration 162/1000 | Loss: 0.00002190
Iteration 163/1000 | Loss: 0.00002190
Iteration 164/1000 | Loss: 0.00002190
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 164. Stopping optimization.
Last 5 losses: [2.1900537831243128e-05, 2.1900537831243128e-05, 2.1900537831243128e-05, 2.1900537831243128e-05, 2.1900537831243128e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1900537831243128e-05

Optimization complete. Final v2v error: 3.902989387512207 mm

Highest mean error: 4.426730632781982 mm for frame 227

Lowest mean error: 3.5049731731414795 mm for frame 72

Saving results

Total time: 43.73651313781738
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_006/1009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_006/1009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_006/1009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00944801
Iteration 2/25 | Loss: 0.00119424
Iteration 3/25 | Loss: 0.00089730
Iteration 4/25 | Loss: 0.00085332
Iteration 5/25 | Loss: 0.00086126
Iteration 6/25 | Loss: 0.00083687
Iteration 7/25 | Loss: 0.00081464
Iteration 8/25 | Loss: 0.00080679
Iteration 9/25 | Loss: 0.00081350
Iteration 10/25 | Loss: 0.00081106
Iteration 11/25 | Loss: 0.00080435
Iteration 12/25 | Loss: 0.00080186
Iteration 13/25 | Loss: 0.00080144
Iteration 14/25 | Loss: 0.00080582
Iteration 15/25 | Loss: 0.00080477
Iteration 16/25 | Loss: 0.00080309
Iteration 17/25 | Loss: 0.00080030
Iteration 18/25 | Loss: 0.00079995
Iteration 19/25 | Loss: 0.00079983
Iteration 20/25 | Loss: 0.00079983
Iteration 21/25 | Loss: 0.00079983
Iteration 22/25 | Loss: 0.00079983
Iteration 23/25 | Loss: 0.00079983
Iteration 24/25 | Loss: 0.00079983
Iteration 25/25 | Loss: 0.00079983

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42151701
Iteration 2/25 | Loss: 0.00036494
Iteration 3/25 | Loss: 0.00036489
Iteration 4/25 | Loss: 0.00036488
Iteration 5/25 | Loss: 0.00036488
Iteration 6/25 | Loss: 0.00036488
Iteration 7/25 | Loss: 0.00036488
Iteration 8/25 | Loss: 0.00036488
Iteration 9/25 | Loss: 0.00036488
Iteration 10/25 | Loss: 0.00036488
Iteration 11/25 | Loss: 0.00036488
Iteration 12/25 | Loss: 0.00036488
Iteration 13/25 | Loss: 0.00036488
Iteration 14/25 | Loss: 0.00036488
Iteration 15/25 | Loss: 0.00036488
Iteration 16/25 | Loss: 0.00036488
Iteration 17/25 | Loss: 0.00036488
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.00036488266778178513, 0.00036488266778178513, 0.00036488266778178513, 0.00036488266778178513, 0.00036488266778178513]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00036488266778178513

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00036488
Iteration 2/1000 | Loss: 0.00004250
Iteration 3/1000 | Loss: 0.00002999
Iteration 4/1000 | Loss: 0.00002773
Iteration 5/1000 | Loss: 0.00002671
Iteration 6/1000 | Loss: 0.00002592
Iteration 7/1000 | Loss: 0.00002535
Iteration 8/1000 | Loss: 0.00002485
Iteration 9/1000 | Loss: 0.00002450
Iteration 10/1000 | Loss: 0.00002425
Iteration 11/1000 | Loss: 0.00002410
Iteration 12/1000 | Loss: 0.00002407
Iteration 13/1000 | Loss: 0.00002397
Iteration 14/1000 | Loss: 0.00002393
Iteration 15/1000 | Loss: 0.00002393
Iteration 16/1000 | Loss: 0.00002392
Iteration 17/1000 | Loss: 0.00002386
Iteration 18/1000 | Loss: 0.00002381
Iteration 19/1000 | Loss: 0.00002374
Iteration 20/1000 | Loss: 0.00002374
Iteration 21/1000 | Loss: 0.00002372
Iteration 22/1000 | Loss: 0.00002371
Iteration 23/1000 | Loss: 0.00002371
Iteration 24/1000 | Loss: 0.00002371
Iteration 25/1000 | Loss: 0.00002370
Iteration 26/1000 | Loss: 0.00002370
Iteration 27/1000 | Loss: 0.00002370
Iteration 28/1000 | Loss: 0.00002370
Iteration 29/1000 | Loss: 0.00002370
Iteration 30/1000 | Loss: 0.00002370
Iteration 31/1000 | Loss: 0.00002370
Iteration 32/1000 | Loss: 0.00002370
Iteration 33/1000 | Loss: 0.00002370
Iteration 34/1000 | Loss: 0.00002369
Iteration 35/1000 | Loss: 0.00002368
Iteration 36/1000 | Loss: 0.00002367
Iteration 37/1000 | Loss: 0.00002367
Iteration 38/1000 | Loss: 0.00002367
Iteration 39/1000 | Loss: 0.00002366
Iteration 40/1000 | Loss: 0.00002366
Iteration 41/1000 | Loss: 0.00002366
Iteration 42/1000 | Loss: 0.00002365
Iteration 43/1000 | Loss: 0.00002365
Iteration 44/1000 | Loss: 0.00002365
Iteration 45/1000 | Loss: 0.00002365
Iteration 46/1000 | Loss: 0.00002364
Iteration 47/1000 | Loss: 0.00002364
Iteration 48/1000 | Loss: 0.00002364
Iteration 49/1000 | Loss: 0.00002364
Iteration 50/1000 | Loss: 0.00002363
Iteration 51/1000 | Loss: 0.00002363
Iteration 52/1000 | Loss: 0.00002363
Iteration 53/1000 | Loss: 0.00002363
Iteration 54/1000 | Loss: 0.00002363
Iteration 55/1000 | Loss: 0.00002362
Iteration 56/1000 | Loss: 0.00002362
Iteration 57/1000 | Loss: 0.00002362
Iteration 58/1000 | Loss: 0.00002362
Iteration 59/1000 | Loss: 0.00002362
Iteration 60/1000 | Loss: 0.00002362
Iteration 61/1000 | Loss: 0.00002362
Iteration 62/1000 | Loss: 0.00002361
Iteration 63/1000 | Loss: 0.00002361
Iteration 64/1000 | Loss: 0.00002361
Iteration 65/1000 | Loss: 0.00002360
Iteration 66/1000 | Loss: 0.00002360
Iteration 67/1000 | Loss: 0.00002360
Iteration 68/1000 | Loss: 0.00002360
Iteration 69/1000 | Loss: 0.00002360
Iteration 70/1000 | Loss: 0.00002359
Iteration 71/1000 | Loss: 0.00002359
Iteration 72/1000 | Loss: 0.00002359
Iteration 73/1000 | Loss: 0.00002358
Iteration 74/1000 | Loss: 0.00002358
Iteration 75/1000 | Loss: 0.00002358
Iteration 76/1000 | Loss: 0.00002358
Iteration 77/1000 | Loss: 0.00002358
Iteration 78/1000 | Loss: 0.00002358
Iteration 79/1000 | Loss: 0.00002358
Iteration 80/1000 | Loss: 0.00002358
Iteration 81/1000 | Loss: 0.00002358
Iteration 82/1000 | Loss: 0.00002358
Iteration 83/1000 | Loss: 0.00002357
Iteration 84/1000 | Loss: 0.00002357
Iteration 85/1000 | Loss: 0.00002357
Iteration 86/1000 | Loss: 0.00002357
Iteration 87/1000 | Loss: 0.00002357
Iteration 88/1000 | Loss: 0.00002357
Iteration 89/1000 | Loss: 0.00002357
Iteration 90/1000 | Loss: 0.00002357
Iteration 91/1000 | Loss: 0.00002357
Iteration 92/1000 | Loss: 0.00002357
Iteration 93/1000 | Loss: 0.00002357
Iteration 94/1000 | Loss: 0.00002356
Iteration 95/1000 | Loss: 0.00002356
Iteration 96/1000 | Loss: 0.00002356
Iteration 97/1000 | Loss: 0.00002356
Iteration 98/1000 | Loss: 0.00002356
Iteration 99/1000 | Loss: 0.00002356
Iteration 100/1000 | Loss: 0.00002356
Iteration 101/1000 | Loss: 0.00002356
Iteration 102/1000 | Loss: 0.00002356
Iteration 103/1000 | Loss: 0.00002356
Iteration 104/1000 | Loss: 0.00002355
Iteration 105/1000 | Loss: 0.00002355
Iteration 106/1000 | Loss: 0.00002355
Iteration 107/1000 | Loss: 0.00002355
Iteration 108/1000 | Loss: 0.00002355
Iteration 109/1000 | Loss: 0.00002355
Iteration 110/1000 | Loss: 0.00002355
Iteration 111/1000 | Loss: 0.00002355
Iteration 112/1000 | Loss: 0.00002355
Iteration 113/1000 | Loss: 0.00002355
Iteration 114/1000 | Loss: 0.00002355
Iteration 115/1000 | Loss: 0.00002355
Iteration 116/1000 | Loss: 0.00002354
Iteration 117/1000 | Loss: 0.00002354
Iteration 118/1000 | Loss: 0.00002354
Iteration 119/1000 | Loss: 0.00002354
Iteration 120/1000 | Loss: 0.00002354
Iteration 121/1000 | Loss: 0.00002354
Iteration 122/1000 | Loss: 0.00002354
Iteration 123/1000 | Loss: 0.00002354
Iteration 124/1000 | Loss: 0.00002354
Iteration 125/1000 | Loss: 0.00002353
Iteration 126/1000 | Loss: 0.00002353
Iteration 127/1000 | Loss: 0.00002353
Iteration 128/1000 | Loss: 0.00002353
Iteration 129/1000 | Loss: 0.00002353
Iteration 130/1000 | Loss: 0.00002353
Iteration 131/1000 | Loss: 0.00002352
Iteration 132/1000 | Loss: 0.00002352
Iteration 133/1000 | Loss: 0.00002352
Iteration 134/1000 | Loss: 0.00002352
Iteration 135/1000 | Loss: 0.00002351
Iteration 136/1000 | Loss: 0.00002351
Iteration 137/1000 | Loss: 0.00002351
Iteration 138/1000 | Loss: 0.00002351
Iteration 139/1000 | Loss: 0.00002351
Iteration 140/1000 | Loss: 0.00002351
Iteration 141/1000 | Loss: 0.00002351
Iteration 142/1000 | Loss: 0.00002350
Iteration 143/1000 | Loss: 0.00002350
Iteration 144/1000 | Loss: 0.00002350
Iteration 145/1000 | Loss: 0.00002350
Iteration 146/1000 | Loss: 0.00002350
Iteration 147/1000 | Loss: 0.00002350
Iteration 148/1000 | Loss: 0.00002350
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 148. Stopping optimization.
Last 5 losses: [2.3504107957705855e-05, 2.3504107957705855e-05, 2.3504107957705855e-05, 2.3504107957705855e-05, 2.3504107957705855e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3504107957705855e-05

Optimization complete. Final v2v error: 3.9773950576782227 mm

Highest mean error: 4.803978443145752 mm for frame 88

Lowest mean error: 3.5669076442718506 mm for frame 233

Saving results

Total time: 70.10919499397278
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_006/1037/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_006/1037.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_006/1037
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00340664
Iteration 2/25 | Loss: 0.00070845
Iteration 3/25 | Loss: 0.00060659
Iteration 4/25 | Loss: 0.00059110
Iteration 5/25 | Loss: 0.00058451
Iteration 6/25 | Loss: 0.00058328
Iteration 7/25 | Loss: 0.00058323
Iteration 8/25 | Loss: 0.00058323
Iteration 9/25 | Loss: 0.00058323
Iteration 10/25 | Loss: 0.00058323
Iteration 11/25 | Loss: 0.00058323
Iteration 12/25 | Loss: 0.00058323
Iteration 13/25 | Loss: 0.00058323
Iteration 14/25 | Loss: 0.00058323
Iteration 15/25 | Loss: 0.00058323
Iteration 16/25 | Loss: 0.00058323
Iteration 17/25 | Loss: 0.00058323
Iteration 18/25 | Loss: 0.00058323
Iteration 19/25 | Loss: 0.00058323
Iteration 20/25 | Loss: 0.00058323
Iteration 21/25 | Loss: 0.00058323
Iteration 22/25 | Loss: 0.00058323
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.000583232322242111, 0.000583232322242111, 0.000583232322242111, 0.000583232322242111, 0.000583232322242111]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000583232322242111

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.71695304
Iteration 2/25 | Loss: 0.00029741
Iteration 3/25 | Loss: 0.00029741
Iteration 4/25 | Loss: 0.00029741
Iteration 5/25 | Loss: 0.00029741
Iteration 6/25 | Loss: 0.00029741
Iteration 7/25 | Loss: 0.00029741
Iteration 8/25 | Loss: 0.00029741
Iteration 9/25 | Loss: 0.00029741
Iteration 10/25 | Loss: 0.00029741
Iteration 11/25 | Loss: 0.00029741
Iteration 12/25 | Loss: 0.00029741
Iteration 13/25 | Loss: 0.00029741
Iteration 14/25 | Loss: 0.00029741
Iteration 15/25 | Loss: 0.00029741
Iteration 16/25 | Loss: 0.00029741
Iteration 17/25 | Loss: 0.00029741
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0002974086964968592, 0.0002974086964968592, 0.0002974086964968592, 0.0002974086964968592, 0.0002974086964968592]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0002974086964968592

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00029741
Iteration 2/1000 | Loss: 0.00001977
Iteration 3/1000 | Loss: 0.00001415
Iteration 4/1000 | Loss: 0.00001317
Iteration 5/1000 | Loss: 0.00001260
Iteration 6/1000 | Loss: 0.00001229
Iteration 7/1000 | Loss: 0.00001197
Iteration 8/1000 | Loss: 0.00001180
Iteration 9/1000 | Loss: 0.00001178
Iteration 10/1000 | Loss: 0.00001172
Iteration 11/1000 | Loss: 0.00001171
Iteration 12/1000 | Loss: 0.00001165
Iteration 13/1000 | Loss: 0.00001164
Iteration 14/1000 | Loss: 0.00001163
Iteration 15/1000 | Loss: 0.00001162
Iteration 16/1000 | Loss: 0.00001160
Iteration 17/1000 | Loss: 0.00001160
Iteration 18/1000 | Loss: 0.00001160
Iteration 19/1000 | Loss: 0.00001159
Iteration 20/1000 | Loss: 0.00001158
Iteration 21/1000 | Loss: 0.00001157
Iteration 22/1000 | Loss: 0.00001157
Iteration 23/1000 | Loss: 0.00001156
Iteration 24/1000 | Loss: 0.00001155
Iteration 25/1000 | Loss: 0.00001152
Iteration 26/1000 | Loss: 0.00001150
Iteration 27/1000 | Loss: 0.00001150
Iteration 28/1000 | Loss: 0.00001149
Iteration 29/1000 | Loss: 0.00001149
Iteration 30/1000 | Loss: 0.00001148
Iteration 31/1000 | Loss: 0.00001148
Iteration 32/1000 | Loss: 0.00001146
Iteration 33/1000 | Loss: 0.00001146
Iteration 34/1000 | Loss: 0.00001145
Iteration 35/1000 | Loss: 0.00001144
Iteration 36/1000 | Loss: 0.00001144
Iteration 37/1000 | Loss: 0.00001140
Iteration 38/1000 | Loss: 0.00001140
Iteration 39/1000 | Loss: 0.00001140
Iteration 40/1000 | Loss: 0.00001139
Iteration 41/1000 | Loss: 0.00001139
Iteration 42/1000 | Loss: 0.00001139
Iteration 43/1000 | Loss: 0.00001138
Iteration 44/1000 | Loss: 0.00001137
Iteration 45/1000 | Loss: 0.00001136
Iteration 46/1000 | Loss: 0.00001136
Iteration 47/1000 | Loss: 0.00001136
Iteration 48/1000 | Loss: 0.00001136
Iteration 49/1000 | Loss: 0.00001136
Iteration 50/1000 | Loss: 0.00001136
Iteration 51/1000 | Loss: 0.00001135
Iteration 52/1000 | Loss: 0.00001135
Iteration 53/1000 | Loss: 0.00001135
Iteration 54/1000 | Loss: 0.00001134
Iteration 55/1000 | Loss: 0.00001133
Iteration 56/1000 | Loss: 0.00001133
Iteration 57/1000 | Loss: 0.00001133
Iteration 58/1000 | Loss: 0.00001132
Iteration 59/1000 | Loss: 0.00001132
Iteration 60/1000 | Loss: 0.00001132
Iteration 61/1000 | Loss: 0.00001132
Iteration 62/1000 | Loss: 0.00001132
Iteration 63/1000 | Loss: 0.00001131
Iteration 64/1000 | Loss: 0.00001131
Iteration 65/1000 | Loss: 0.00001130
Iteration 66/1000 | Loss: 0.00001130
Iteration 67/1000 | Loss: 0.00001130
Iteration 68/1000 | Loss: 0.00001129
Iteration 69/1000 | Loss: 0.00001129
Iteration 70/1000 | Loss: 0.00001129
Iteration 71/1000 | Loss: 0.00001128
Iteration 72/1000 | Loss: 0.00001128
Iteration 73/1000 | Loss: 0.00001127
Iteration 74/1000 | Loss: 0.00001127
Iteration 75/1000 | Loss: 0.00001126
Iteration 76/1000 | Loss: 0.00001126
Iteration 77/1000 | Loss: 0.00001126
Iteration 78/1000 | Loss: 0.00001126
Iteration 79/1000 | Loss: 0.00001126
Iteration 80/1000 | Loss: 0.00001126
Iteration 81/1000 | Loss: 0.00001126
Iteration 82/1000 | Loss: 0.00001126
Iteration 83/1000 | Loss: 0.00001126
Iteration 84/1000 | Loss: 0.00001126
Iteration 85/1000 | Loss: 0.00001126
Iteration 86/1000 | Loss: 0.00001126
Iteration 87/1000 | Loss: 0.00001126
Iteration 88/1000 | Loss: 0.00001126
Iteration 89/1000 | Loss: 0.00001126
Iteration 90/1000 | Loss: 0.00001125
Iteration 91/1000 | Loss: 0.00001125
Iteration 92/1000 | Loss: 0.00001124
Iteration 93/1000 | Loss: 0.00001124
Iteration 94/1000 | Loss: 0.00001124
Iteration 95/1000 | Loss: 0.00001123
Iteration 96/1000 | Loss: 0.00001123
Iteration 97/1000 | Loss: 0.00001123
Iteration 98/1000 | Loss: 0.00001122
Iteration 99/1000 | Loss: 0.00001122
Iteration 100/1000 | Loss: 0.00001121
Iteration 101/1000 | Loss: 0.00001121
Iteration 102/1000 | Loss: 0.00001121
Iteration 103/1000 | Loss: 0.00001121
Iteration 104/1000 | Loss: 0.00001120
Iteration 105/1000 | Loss: 0.00001120
Iteration 106/1000 | Loss: 0.00001120
Iteration 107/1000 | Loss: 0.00001119
Iteration 108/1000 | Loss: 0.00001119
Iteration 109/1000 | Loss: 0.00001119
Iteration 110/1000 | Loss: 0.00001119
Iteration 111/1000 | Loss: 0.00001119
Iteration 112/1000 | Loss: 0.00001119
Iteration 113/1000 | Loss: 0.00001118
Iteration 114/1000 | Loss: 0.00001118
Iteration 115/1000 | Loss: 0.00001118
Iteration 116/1000 | Loss: 0.00001118
Iteration 117/1000 | Loss: 0.00001118
Iteration 118/1000 | Loss: 0.00001118
Iteration 119/1000 | Loss: 0.00001118
Iteration 120/1000 | Loss: 0.00001118
Iteration 121/1000 | Loss: 0.00001118
Iteration 122/1000 | Loss: 0.00001117
Iteration 123/1000 | Loss: 0.00001117
Iteration 124/1000 | Loss: 0.00001117
Iteration 125/1000 | Loss: 0.00001117
Iteration 126/1000 | Loss: 0.00001117
Iteration 127/1000 | Loss: 0.00001117
Iteration 128/1000 | Loss: 0.00001117
Iteration 129/1000 | Loss: 0.00001116
Iteration 130/1000 | Loss: 0.00001116
Iteration 131/1000 | Loss: 0.00001116
Iteration 132/1000 | Loss: 0.00001116
Iteration 133/1000 | Loss: 0.00001116
Iteration 134/1000 | Loss: 0.00001116
Iteration 135/1000 | Loss: 0.00001116
Iteration 136/1000 | Loss: 0.00001116
Iteration 137/1000 | Loss: 0.00001116
Iteration 138/1000 | Loss: 0.00001116
Iteration 139/1000 | Loss: 0.00001116
Iteration 140/1000 | Loss: 0.00001116
Iteration 141/1000 | Loss: 0.00001116
Iteration 142/1000 | Loss: 0.00001116
Iteration 143/1000 | Loss: 0.00001116
Iteration 144/1000 | Loss: 0.00001116
Iteration 145/1000 | Loss: 0.00001116
Iteration 146/1000 | Loss: 0.00001116
Iteration 147/1000 | Loss: 0.00001116
Iteration 148/1000 | Loss: 0.00001116
Iteration 149/1000 | Loss: 0.00001116
Iteration 150/1000 | Loss: 0.00001115
Iteration 151/1000 | Loss: 0.00001115
Iteration 152/1000 | Loss: 0.00001115
Iteration 153/1000 | Loss: 0.00001115
Iteration 154/1000 | Loss: 0.00001115
Iteration 155/1000 | Loss: 0.00001115
Iteration 156/1000 | Loss: 0.00001115
Iteration 157/1000 | Loss: 0.00001115
Iteration 158/1000 | Loss: 0.00001115
Iteration 159/1000 | Loss: 0.00001115
Iteration 160/1000 | Loss: 0.00001115
Iteration 161/1000 | Loss: 0.00001115
Iteration 162/1000 | Loss: 0.00001115
Iteration 163/1000 | Loss: 0.00001115
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 163. Stopping optimization.
Last 5 losses: [1.1153337254654616e-05, 1.1153337254654616e-05, 1.1153337254654616e-05, 1.1153337254654616e-05, 1.1153337254654616e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1153337254654616e-05

Optimization complete. Final v2v error: 2.829132318496704 mm

Highest mean error: 3.3492934703826904 mm for frame 130

Lowest mean error: 2.6789443492889404 mm for frame 119

Saving results

Total time: 39.6095609664917
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_006/1075/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_006/1075.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_006/1075
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00793740
Iteration 2/25 | Loss: 0.00076445
Iteration 3/25 | Loss: 0.00064771
Iteration 4/25 | Loss: 0.00062560
Iteration 5/25 | Loss: 0.00061807
Iteration 6/25 | Loss: 0.00061644
Iteration 7/25 | Loss: 0.00061576
Iteration 8/25 | Loss: 0.00061565
Iteration 9/25 | Loss: 0.00061565
Iteration 10/25 | Loss: 0.00061565
Iteration 11/25 | Loss: 0.00061565
Iteration 12/25 | Loss: 0.00061565
Iteration 13/25 | Loss: 0.00061565
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0006156545132398605, 0.0006156545132398605, 0.0006156545132398605, 0.0006156545132398605, 0.0006156545132398605]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006156545132398605

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46615732
Iteration 2/25 | Loss: 0.00029877
Iteration 3/25 | Loss: 0.00029876
Iteration 4/25 | Loss: 0.00029876
Iteration 5/25 | Loss: 0.00029876
Iteration 6/25 | Loss: 0.00029876
Iteration 7/25 | Loss: 0.00029876
Iteration 8/25 | Loss: 0.00029876
Iteration 9/25 | Loss: 0.00029876
Iteration 10/25 | Loss: 0.00029876
Iteration 11/25 | Loss: 0.00029876
Iteration 12/25 | Loss: 0.00029876
Iteration 13/25 | Loss: 0.00029876
Iteration 14/25 | Loss: 0.00029876
Iteration 15/25 | Loss: 0.00029876
Iteration 16/25 | Loss: 0.00029876
Iteration 17/25 | Loss: 0.00029876
Iteration 18/25 | Loss: 0.00029876
Iteration 19/25 | Loss: 0.00029876
Iteration 20/25 | Loss: 0.00029876
Iteration 21/25 | Loss: 0.00029876
Iteration 22/25 | Loss: 0.00029876
Iteration 23/25 | Loss: 0.00029876
Iteration 24/25 | Loss: 0.00029876
Iteration 25/25 | Loss: 0.00029876
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.00029875553445890546, 0.00029875553445890546, 0.00029875553445890546, 0.00029875553445890546, 0.00029875553445890546]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00029875553445890546

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00029876
Iteration 2/1000 | Loss: 0.00003914
Iteration 3/1000 | Loss: 0.00002494
Iteration 4/1000 | Loss: 0.00001917
Iteration 5/1000 | Loss: 0.00001804
Iteration 6/1000 | Loss: 0.00001715
Iteration 7/1000 | Loss: 0.00001660
Iteration 8/1000 | Loss: 0.00001602
Iteration 9/1000 | Loss: 0.00001572
Iteration 10/1000 | Loss: 0.00001562
Iteration 11/1000 | Loss: 0.00001548
Iteration 12/1000 | Loss: 0.00001534
Iteration 13/1000 | Loss: 0.00001523
Iteration 14/1000 | Loss: 0.00001523
Iteration 15/1000 | Loss: 0.00001508
Iteration 16/1000 | Loss: 0.00001504
Iteration 17/1000 | Loss: 0.00001503
Iteration 18/1000 | Loss: 0.00001502
Iteration 19/1000 | Loss: 0.00001501
Iteration 20/1000 | Loss: 0.00001501
Iteration 21/1000 | Loss: 0.00001500
Iteration 22/1000 | Loss: 0.00001499
Iteration 23/1000 | Loss: 0.00001499
Iteration 24/1000 | Loss: 0.00001498
Iteration 25/1000 | Loss: 0.00001498
Iteration 26/1000 | Loss: 0.00001497
Iteration 27/1000 | Loss: 0.00001497
Iteration 28/1000 | Loss: 0.00001496
Iteration 29/1000 | Loss: 0.00001496
Iteration 30/1000 | Loss: 0.00001495
Iteration 31/1000 | Loss: 0.00001495
Iteration 32/1000 | Loss: 0.00001494
Iteration 33/1000 | Loss: 0.00001494
Iteration 34/1000 | Loss: 0.00001493
Iteration 35/1000 | Loss: 0.00001493
Iteration 36/1000 | Loss: 0.00001493
Iteration 37/1000 | Loss: 0.00001492
Iteration 38/1000 | Loss: 0.00001492
Iteration 39/1000 | Loss: 0.00001492
Iteration 40/1000 | Loss: 0.00001492
Iteration 41/1000 | Loss: 0.00001491
Iteration 42/1000 | Loss: 0.00001491
Iteration 43/1000 | Loss: 0.00001491
Iteration 44/1000 | Loss: 0.00001491
Iteration 45/1000 | Loss: 0.00001490
Iteration 46/1000 | Loss: 0.00001490
Iteration 47/1000 | Loss: 0.00001490
Iteration 48/1000 | Loss: 0.00001489
Iteration 49/1000 | Loss: 0.00001489
Iteration 50/1000 | Loss: 0.00001489
Iteration 51/1000 | Loss: 0.00001488
Iteration 52/1000 | Loss: 0.00001488
Iteration 53/1000 | Loss: 0.00001488
Iteration 54/1000 | Loss: 0.00001487
Iteration 55/1000 | Loss: 0.00001487
Iteration 56/1000 | Loss: 0.00001487
Iteration 57/1000 | Loss: 0.00001487
Iteration 58/1000 | Loss: 0.00001487
Iteration 59/1000 | Loss: 0.00001486
Iteration 60/1000 | Loss: 0.00001486
Iteration 61/1000 | Loss: 0.00001486
Iteration 62/1000 | Loss: 0.00001486
Iteration 63/1000 | Loss: 0.00001486
Iteration 64/1000 | Loss: 0.00001486
Iteration 65/1000 | Loss: 0.00001485
Iteration 66/1000 | Loss: 0.00001485
Iteration 67/1000 | Loss: 0.00001485
Iteration 68/1000 | Loss: 0.00001485
Iteration 69/1000 | Loss: 0.00001485
Iteration 70/1000 | Loss: 0.00001485
Iteration 71/1000 | Loss: 0.00001485
Iteration 72/1000 | Loss: 0.00001485
Iteration 73/1000 | Loss: 0.00001484
Iteration 74/1000 | Loss: 0.00001484
Iteration 75/1000 | Loss: 0.00001484
Iteration 76/1000 | Loss: 0.00001484
Iteration 77/1000 | Loss: 0.00001484
Iteration 78/1000 | Loss: 0.00001484
Iteration 79/1000 | Loss: 0.00001484
Iteration 80/1000 | Loss: 0.00001484
Iteration 81/1000 | Loss: 0.00001483
Iteration 82/1000 | Loss: 0.00001483
Iteration 83/1000 | Loss: 0.00001483
Iteration 84/1000 | Loss: 0.00001483
Iteration 85/1000 | Loss: 0.00001483
Iteration 86/1000 | Loss: 0.00001483
Iteration 87/1000 | Loss: 0.00001483
Iteration 88/1000 | Loss: 0.00001483
Iteration 89/1000 | Loss: 0.00001483
Iteration 90/1000 | Loss: 0.00001483
Iteration 91/1000 | Loss: 0.00001483
Iteration 92/1000 | Loss: 0.00001482
Iteration 93/1000 | Loss: 0.00001482
Iteration 94/1000 | Loss: 0.00001482
Iteration 95/1000 | Loss: 0.00001482
Iteration 96/1000 | Loss: 0.00001482
Iteration 97/1000 | Loss: 0.00001482
Iteration 98/1000 | Loss: 0.00001482
Iteration 99/1000 | Loss: 0.00001482
Iteration 100/1000 | Loss: 0.00001482
Iteration 101/1000 | Loss: 0.00001482
Iteration 102/1000 | Loss: 0.00001482
Iteration 103/1000 | Loss: 0.00001482
Iteration 104/1000 | Loss: 0.00001482
Iteration 105/1000 | Loss: 0.00001482
Iteration 106/1000 | Loss: 0.00001482
Iteration 107/1000 | Loss: 0.00001482
Iteration 108/1000 | Loss: 0.00001482
Iteration 109/1000 | Loss: 0.00001482
Iteration 110/1000 | Loss: 0.00001482
Iteration 111/1000 | Loss: 0.00001482
Iteration 112/1000 | Loss: 0.00001482
Iteration 113/1000 | Loss: 0.00001482
Iteration 114/1000 | Loss: 0.00001482
Iteration 115/1000 | Loss: 0.00001482
Iteration 116/1000 | Loss: 0.00001482
Iteration 117/1000 | Loss: 0.00001482
Iteration 118/1000 | Loss: 0.00001482
Iteration 119/1000 | Loss: 0.00001482
Iteration 120/1000 | Loss: 0.00001482
Iteration 121/1000 | Loss: 0.00001482
Iteration 122/1000 | Loss: 0.00001482
Iteration 123/1000 | Loss: 0.00001482
Iteration 124/1000 | Loss: 0.00001482
Iteration 125/1000 | Loss: 0.00001482
Iteration 126/1000 | Loss: 0.00001482
Iteration 127/1000 | Loss: 0.00001482
Iteration 128/1000 | Loss: 0.00001482
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 128. Stopping optimization.
Last 5 losses: [1.481680737924762e-05, 1.481680737924762e-05, 1.481680737924762e-05, 1.481680737924762e-05, 1.481680737924762e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.481680737924762e-05

Optimization complete. Final v2v error: 3.214590072631836 mm

Highest mean error: 3.7649617195129395 mm for frame 127

Lowest mean error: 2.854799270629883 mm for frame 31

Saving results

Total time: 35.25624322891235
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_006/1051/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_006/1051.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_006/1051
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01099951
Iteration 2/25 | Loss: 0.00114592
Iteration 3/25 | Loss: 0.00071368
Iteration 4/25 | Loss: 0.00066289
Iteration 5/25 | Loss: 0.00064910
Iteration 6/25 | Loss: 0.00064687
Iteration 7/25 | Loss: 0.00064687
Iteration 8/25 | Loss: 0.00064687
Iteration 9/25 | Loss: 0.00064687
Iteration 10/25 | Loss: 0.00064687
Iteration 11/25 | Loss: 0.00064687
Iteration 12/25 | Loss: 0.00064687
Iteration 13/25 | Loss: 0.00064687
Iteration 14/25 | Loss: 0.00064687
Iteration 15/25 | Loss: 0.00064687
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0006468683714047074, 0.0006468683714047074, 0.0006468683714047074, 0.0006468683714047074, 0.0006468683714047074]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006468683714047074

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.39052129
Iteration 2/25 | Loss: 0.00022355
Iteration 3/25 | Loss: 0.00022355
Iteration 4/25 | Loss: 0.00022355
Iteration 5/25 | Loss: 0.00022355
Iteration 6/25 | Loss: 0.00022355
Iteration 7/25 | Loss: 0.00022355
Iteration 8/25 | Loss: 0.00022355
Iteration 9/25 | Loss: 0.00022355
Iteration 10/25 | Loss: 0.00022355
Iteration 11/25 | Loss: 0.00022355
Iteration 12/25 | Loss: 0.00022355
Iteration 13/25 | Loss: 0.00022355
Iteration 14/25 | Loss: 0.00022355
Iteration 15/25 | Loss: 0.00022355
Iteration 16/25 | Loss: 0.00022355
Iteration 17/25 | Loss: 0.00022355
Iteration 18/25 | Loss: 0.00022355
Iteration 19/25 | Loss: 0.00022355
Iteration 20/25 | Loss: 0.00022355
Iteration 21/25 | Loss: 0.00022355
Iteration 22/25 | Loss: 0.00022355
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.00022354502289090306, 0.00022354502289090306, 0.00022354502289090306, 0.00022354502289090306, 0.00022354502289090306]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00022354502289090306

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00022355
Iteration 2/1000 | Loss: 0.00002537
Iteration 3/1000 | Loss: 0.00001822
Iteration 4/1000 | Loss: 0.00001595
Iteration 5/1000 | Loss: 0.00001485
Iteration 6/1000 | Loss: 0.00001428
Iteration 7/1000 | Loss: 0.00001382
Iteration 8/1000 | Loss: 0.00001341
Iteration 9/1000 | Loss: 0.00001331
Iteration 10/1000 | Loss: 0.00001324
Iteration 11/1000 | Loss: 0.00001311
Iteration 12/1000 | Loss: 0.00001298
Iteration 13/1000 | Loss: 0.00001288
Iteration 14/1000 | Loss: 0.00001286
Iteration 15/1000 | Loss: 0.00001284
Iteration 16/1000 | Loss: 0.00001283
Iteration 17/1000 | Loss: 0.00001283
Iteration 18/1000 | Loss: 0.00001283
Iteration 19/1000 | Loss: 0.00001282
Iteration 20/1000 | Loss: 0.00001282
Iteration 21/1000 | Loss: 0.00001281
Iteration 22/1000 | Loss: 0.00001281
Iteration 23/1000 | Loss: 0.00001280
Iteration 24/1000 | Loss: 0.00001280
Iteration 25/1000 | Loss: 0.00001279
Iteration 26/1000 | Loss: 0.00001279
Iteration 27/1000 | Loss: 0.00001279
Iteration 28/1000 | Loss: 0.00001279
Iteration 29/1000 | Loss: 0.00001279
Iteration 30/1000 | Loss: 0.00001279
Iteration 31/1000 | Loss: 0.00001278
Iteration 32/1000 | Loss: 0.00001278
Iteration 33/1000 | Loss: 0.00001278
Iteration 34/1000 | Loss: 0.00001278
Iteration 35/1000 | Loss: 0.00001278
Iteration 36/1000 | Loss: 0.00001278
Iteration 37/1000 | Loss: 0.00001278
Iteration 38/1000 | Loss: 0.00001277
Iteration 39/1000 | Loss: 0.00001277
Iteration 40/1000 | Loss: 0.00001277
Iteration 41/1000 | Loss: 0.00001277
Iteration 42/1000 | Loss: 0.00001276
Iteration 43/1000 | Loss: 0.00001276
Iteration 44/1000 | Loss: 0.00001276
Iteration 45/1000 | Loss: 0.00001276
Iteration 46/1000 | Loss: 0.00001275
Iteration 47/1000 | Loss: 0.00001275
Iteration 48/1000 | Loss: 0.00001275
Iteration 49/1000 | Loss: 0.00001275
Iteration 50/1000 | Loss: 0.00001275
Iteration 51/1000 | Loss: 0.00001275
Iteration 52/1000 | Loss: 0.00001274
Iteration 53/1000 | Loss: 0.00001274
Iteration 54/1000 | Loss: 0.00001273
Iteration 55/1000 | Loss: 0.00001273
Iteration 56/1000 | Loss: 0.00001273
Iteration 57/1000 | Loss: 0.00001273
Iteration 58/1000 | Loss: 0.00001272
Iteration 59/1000 | Loss: 0.00001272
Iteration 60/1000 | Loss: 0.00001272
Iteration 61/1000 | Loss: 0.00001271
Iteration 62/1000 | Loss: 0.00001271
Iteration 63/1000 | Loss: 0.00001271
Iteration 64/1000 | Loss: 0.00001271
Iteration 65/1000 | Loss: 0.00001271
Iteration 66/1000 | Loss: 0.00001270
Iteration 67/1000 | Loss: 0.00001270
Iteration 68/1000 | Loss: 0.00001270
Iteration 69/1000 | Loss: 0.00001270
Iteration 70/1000 | Loss: 0.00001270
Iteration 71/1000 | Loss: 0.00001270
Iteration 72/1000 | Loss: 0.00001270
Iteration 73/1000 | Loss: 0.00001270
Iteration 74/1000 | Loss: 0.00001270
Iteration 75/1000 | Loss: 0.00001270
Iteration 76/1000 | Loss: 0.00001270
Iteration 77/1000 | Loss: 0.00001270
Iteration 78/1000 | Loss: 0.00001270
Iteration 79/1000 | Loss: 0.00001270
Iteration 80/1000 | Loss: 0.00001270
Iteration 81/1000 | Loss: 0.00001270
Iteration 82/1000 | Loss: 0.00001270
Iteration 83/1000 | Loss: 0.00001270
Iteration 84/1000 | Loss: 0.00001270
Iteration 85/1000 | Loss: 0.00001270
Iteration 86/1000 | Loss: 0.00001270
Iteration 87/1000 | Loss: 0.00001270
Iteration 88/1000 | Loss: 0.00001270
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 88. Stopping optimization.
Last 5 losses: [1.2700896149908658e-05, 1.2700896149908658e-05, 1.2700896149908658e-05, 1.2700896149908658e-05, 1.2700896149908658e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2700896149908658e-05

Optimization complete. Final v2v error: 3.0214829444885254 mm

Highest mean error: 3.294426202774048 mm for frame 75

Lowest mean error: 2.824448823928833 mm for frame 105

Saving results

Total time: 34.91944599151611
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_006/1055/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_006/1055.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_006/1055
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01119305
Iteration 2/25 | Loss: 0.01119304
Iteration 3/25 | Loss: 0.01119304
Iteration 4/25 | Loss: 0.01119303
Iteration 5/25 | Loss: 0.01119303
Iteration 6/25 | Loss: 0.00401403
Iteration 7/25 | Loss: 0.00269314
Iteration 8/25 | Loss: 0.00223460
Iteration 9/25 | Loss: 0.00191375
Iteration 10/25 | Loss: 0.00178291
Iteration 11/25 | Loss: 0.00202767
Iteration 12/25 | Loss: 0.00206235
Iteration 13/25 | Loss: 0.00155819
Iteration 14/25 | Loss: 0.00131350
Iteration 15/25 | Loss: 0.00125494
Iteration 16/25 | Loss: 0.00119821
Iteration 17/25 | Loss: 0.00119204
Iteration 18/25 | Loss: 0.00116862
Iteration 19/25 | Loss: 0.00115272
Iteration 20/25 | Loss: 0.00113828
Iteration 21/25 | Loss: 0.00114030
Iteration 22/25 | Loss: 0.00113399
Iteration 23/25 | Loss: 0.00112897
Iteration 24/25 | Loss: 0.00111943
Iteration 25/25 | Loss: 0.00111633

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.64906114
Iteration 2/25 | Loss: 0.00079002
Iteration 3/25 | Loss: 0.00079002
Iteration 4/25 | Loss: 0.00079001
Iteration 5/25 | Loss: 0.00079001
Iteration 6/25 | Loss: 0.00079001
Iteration 7/25 | Loss: 0.00079001
Iteration 8/25 | Loss: 0.00079001
Iteration 9/25 | Loss: 0.00079001
Iteration 10/25 | Loss: 0.00079001
Iteration 11/25 | Loss: 0.00079001
Iteration 12/25 | Loss: 0.00079001
Iteration 13/25 | Loss: 0.00079001
Iteration 14/25 | Loss: 0.00079001
Iteration 15/25 | Loss: 0.00079001
Iteration 16/25 | Loss: 0.00079001
Iteration 17/25 | Loss: 0.00079001
Iteration 18/25 | Loss: 0.00079001
Iteration 19/25 | Loss: 0.00079001
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.000790013000369072, 0.000790013000369072, 0.000790013000369072, 0.000790013000369072, 0.000790013000369072]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000790013000369072

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00079001
Iteration 2/1000 | Loss: 0.00028042
Iteration 3/1000 | Loss: 0.00024882
Iteration 4/1000 | Loss: 0.00027809
Iteration 5/1000 | Loss: 0.00034595
Iteration 6/1000 | Loss: 0.00033202
Iteration 7/1000 | Loss: 0.00075820
Iteration 8/1000 | Loss: 0.00160956
Iteration 9/1000 | Loss: 0.00059055
Iteration 10/1000 | Loss: 0.00066276
Iteration 11/1000 | Loss: 0.00067637
Iteration 12/1000 | Loss: 0.00053988
Iteration 13/1000 | Loss: 0.00016191
Iteration 14/1000 | Loss: 0.00023795
Iteration 15/1000 | Loss: 0.00053480
Iteration 16/1000 | Loss: 0.00048095
Iteration 17/1000 | Loss: 0.00158006
Iteration 18/1000 | Loss: 0.00071141
Iteration 19/1000 | Loss: 0.00052244
Iteration 20/1000 | Loss: 0.00070006
Iteration 21/1000 | Loss: 0.00085094
Iteration 22/1000 | Loss: 0.00031878
Iteration 23/1000 | Loss: 0.00066007
Iteration 24/1000 | Loss: 0.00071929
Iteration 25/1000 | Loss: 0.00015760
Iteration 26/1000 | Loss: 0.00063966
Iteration 27/1000 | Loss: 0.00046501
Iteration 28/1000 | Loss: 0.00038866
Iteration 29/1000 | Loss: 0.00053687
Iteration 30/1000 | Loss: 0.00049569
Iteration 31/1000 | Loss: 0.00076849
Iteration 32/1000 | Loss: 0.00046822
Iteration 33/1000 | Loss: 0.00141478
Iteration 34/1000 | Loss: 0.00060745
Iteration 35/1000 | Loss: 0.00093163
Iteration 36/1000 | Loss: 0.00057941
Iteration 37/1000 | Loss: 0.00033952
Iteration 38/1000 | Loss: 0.00064713
Iteration 39/1000 | Loss: 0.00033111
Iteration 40/1000 | Loss: 0.00056151
Iteration 41/1000 | Loss: 0.00010810
Iteration 42/1000 | Loss: 0.00032521
Iteration 43/1000 | Loss: 0.00134149
Iteration 44/1000 | Loss: 0.00187088
Iteration 45/1000 | Loss: 0.00042765
Iteration 46/1000 | Loss: 0.00043494
Iteration 47/1000 | Loss: 0.00021900
Iteration 48/1000 | Loss: 0.00032022
Iteration 49/1000 | Loss: 0.00011614
Iteration 50/1000 | Loss: 0.00008308
Iteration 51/1000 | Loss: 0.00018322
Iteration 52/1000 | Loss: 0.00022862
Iteration 53/1000 | Loss: 0.00012602
Iteration 54/1000 | Loss: 0.00020691
Iteration 55/1000 | Loss: 0.00017078
Iteration 56/1000 | Loss: 0.00017249
Iteration 57/1000 | Loss: 0.00011166
Iteration 58/1000 | Loss: 0.00026348
Iteration 59/1000 | Loss: 0.00018306
Iteration 60/1000 | Loss: 0.00033732
Iteration 61/1000 | Loss: 0.00032907
Iteration 62/1000 | Loss: 0.00028099
Iteration 63/1000 | Loss: 0.00025805
Iteration 64/1000 | Loss: 0.00024303
Iteration 65/1000 | Loss: 0.00091390
Iteration 66/1000 | Loss: 0.00027469
Iteration 67/1000 | Loss: 0.00040011
Iteration 68/1000 | Loss: 0.00030057
Iteration 69/1000 | Loss: 0.00045587
Iteration 70/1000 | Loss: 0.00026777
Iteration 71/1000 | Loss: 0.00031560
Iteration 72/1000 | Loss: 0.00025540
Iteration 73/1000 | Loss: 0.00024575
Iteration 74/1000 | Loss: 0.00016805
Iteration 75/1000 | Loss: 0.00012410
Iteration 76/1000 | Loss: 0.00026487
Iteration 77/1000 | Loss: 0.00021548
Iteration 78/1000 | Loss: 0.00020379
Iteration 79/1000 | Loss: 0.00017485
Iteration 80/1000 | Loss: 0.00016790
Iteration 81/1000 | Loss: 0.00023886
Iteration 82/1000 | Loss: 0.00019775
Iteration 83/1000 | Loss: 0.00032968
Iteration 84/1000 | Loss: 0.00014461
Iteration 85/1000 | Loss: 0.00025318
Iteration 86/1000 | Loss: 0.00015836
Iteration 87/1000 | Loss: 0.00007120
Iteration 88/1000 | Loss: 0.00014380
Iteration 89/1000 | Loss: 0.00013399
Iteration 90/1000 | Loss: 0.00029511
Iteration 91/1000 | Loss: 0.00159033
Iteration 92/1000 | Loss: 0.00015190
Iteration 93/1000 | Loss: 0.00013524
Iteration 94/1000 | Loss: 0.00048625
Iteration 95/1000 | Loss: 0.00029291
Iteration 96/1000 | Loss: 0.00047816
Iteration 97/1000 | Loss: 0.00026014
Iteration 98/1000 | Loss: 0.00021459
Iteration 99/1000 | Loss: 0.00011882
Iteration 100/1000 | Loss: 0.00008835
Iteration 101/1000 | Loss: 0.00016795
Iteration 102/1000 | Loss: 0.00011462
Iteration 103/1000 | Loss: 0.00024444
Iteration 104/1000 | Loss: 0.00016465
Iteration 105/1000 | Loss: 0.00015837
Iteration 106/1000 | Loss: 0.00031416
Iteration 107/1000 | Loss: 0.00012164
Iteration 108/1000 | Loss: 0.00026088
Iteration 109/1000 | Loss: 0.00012199
Iteration 110/1000 | Loss: 0.00030818
Iteration 111/1000 | Loss: 0.00018596
Iteration 112/1000 | Loss: 0.00013544
Iteration 113/1000 | Loss: 0.00014851
Iteration 114/1000 | Loss: 0.00009534
Iteration 115/1000 | Loss: 0.00014921
Iteration 116/1000 | Loss: 0.00014979
Iteration 117/1000 | Loss: 0.00007103
Iteration 118/1000 | Loss: 0.00010734
Iteration 119/1000 | Loss: 0.00010467
Iteration 120/1000 | Loss: 0.00032995
Iteration 121/1000 | Loss: 0.00016709
Iteration 122/1000 | Loss: 0.00025860
Iteration 123/1000 | Loss: 0.00016996
Iteration 124/1000 | Loss: 0.00025581
Iteration 125/1000 | Loss: 0.00020753
Iteration 126/1000 | Loss: 0.00009052
Iteration 127/1000 | Loss: 0.00014974
Iteration 128/1000 | Loss: 0.00012733
Iteration 129/1000 | Loss: 0.00013678
Iteration 130/1000 | Loss: 0.00012702
Iteration 131/1000 | Loss: 0.00012830
Iteration 132/1000 | Loss: 0.00012659
Iteration 133/1000 | Loss: 0.00036815
Iteration 134/1000 | Loss: 0.00015505
Iteration 135/1000 | Loss: 0.00012926
Iteration 136/1000 | Loss: 0.00008877
Iteration 137/1000 | Loss: 0.00011427
Iteration 138/1000 | Loss: 0.00021172
Iteration 139/1000 | Loss: 0.00012529
Iteration 140/1000 | Loss: 0.00012265
Iteration 141/1000 | Loss: 0.00018448
Iteration 142/1000 | Loss: 0.00009473
Iteration 143/1000 | Loss: 0.00012050
Iteration 144/1000 | Loss: 0.00012650
Iteration 145/1000 | Loss: 0.00014326
Iteration 146/1000 | Loss: 0.00012055
Iteration 147/1000 | Loss: 0.00018965
Iteration 148/1000 | Loss: 0.00016674
Iteration 149/1000 | Loss: 0.00014057
Iteration 150/1000 | Loss: 0.00034777
Iteration 151/1000 | Loss: 0.00018318
Iteration 152/1000 | Loss: 0.00013773
Iteration 153/1000 | Loss: 0.00017216
Iteration 154/1000 | Loss: 0.00013571
Iteration 155/1000 | Loss: 0.00042727
Iteration 156/1000 | Loss: 0.00013869
Iteration 157/1000 | Loss: 0.00020729
Iteration 158/1000 | Loss: 0.00023148
Iteration 159/1000 | Loss: 0.00012836
Iteration 160/1000 | Loss: 0.00009015
Iteration 161/1000 | Loss: 0.00011259
Iteration 162/1000 | Loss: 0.00009860
Iteration 163/1000 | Loss: 0.00009745
Iteration 164/1000 | Loss: 0.00019537
Iteration 165/1000 | Loss: 0.00019685
Iteration 166/1000 | Loss: 0.00014948
Iteration 167/1000 | Loss: 0.00016033
Iteration 168/1000 | Loss: 0.00012942
Iteration 169/1000 | Loss: 0.00011363
Iteration 170/1000 | Loss: 0.00013884
Iteration 171/1000 | Loss: 0.00011363
Iteration 172/1000 | Loss: 0.00013442
Iteration 173/1000 | Loss: 0.00011528
Iteration 174/1000 | Loss: 0.00014848
Iteration 175/1000 | Loss: 0.00040242
Iteration 176/1000 | Loss: 0.00025628
Iteration 177/1000 | Loss: 0.00015386
Iteration 178/1000 | Loss: 0.00017721
Iteration 179/1000 | Loss: 0.00014303
Iteration 180/1000 | Loss: 0.00021607
Iteration 181/1000 | Loss: 0.00015964
Iteration 182/1000 | Loss: 0.00012993
Iteration 183/1000 | Loss: 0.00020969
Iteration 184/1000 | Loss: 0.00013600
Iteration 185/1000 | Loss: 0.00019953
Iteration 186/1000 | Loss: 0.00016360
Iteration 187/1000 | Loss: 0.00010612
Iteration 188/1000 | Loss: 0.00009739
Iteration 189/1000 | Loss: 0.00012835
Iteration 190/1000 | Loss: 0.00008775
Iteration 191/1000 | Loss: 0.00017920
Iteration 192/1000 | Loss: 0.00017796
Iteration 193/1000 | Loss: 0.00016927
Iteration 194/1000 | Loss: 0.00014653
Iteration 195/1000 | Loss: 0.00016004
Iteration 196/1000 | Loss: 0.00017414
Iteration 197/1000 | Loss: 0.00017083
Iteration 198/1000 | Loss: 0.00013606
Iteration 199/1000 | Loss: 0.00015405
Iteration 200/1000 | Loss: 0.00026789
Iteration 201/1000 | Loss: 0.00017538
Iteration 202/1000 | Loss: 0.00013663
Iteration 203/1000 | Loss: 0.00016703
Iteration 204/1000 | Loss: 0.00010090
Iteration 205/1000 | Loss: 0.00013268
Iteration 206/1000 | Loss: 0.00011985
Iteration 207/1000 | Loss: 0.00013281
Iteration 208/1000 | Loss: 0.00011638
Iteration 209/1000 | Loss: 0.00022896
Iteration 210/1000 | Loss: 0.00020928
Iteration 211/1000 | Loss: 0.00027569
Iteration 212/1000 | Loss: 0.00021603
Iteration 213/1000 | Loss: 0.00024792
Iteration 214/1000 | Loss: 0.00009637
Iteration 215/1000 | Loss: 0.00006625
Iteration 216/1000 | Loss: 0.00010180
Iteration 217/1000 | Loss: 0.00006423
Iteration 218/1000 | Loss: 0.00008906
Iteration 219/1000 | Loss: 0.00006359
Iteration 220/1000 | Loss: 0.00014471
Iteration 221/1000 | Loss: 0.00006373
Iteration 222/1000 | Loss: 0.00006327
Iteration 223/1000 | Loss: 0.00006292
Iteration 224/1000 | Loss: 0.00006257
Iteration 225/1000 | Loss: 0.00006252
Iteration 226/1000 | Loss: 0.00006233
Iteration 227/1000 | Loss: 0.00044044
Iteration 228/1000 | Loss: 0.00006433
Iteration 229/1000 | Loss: 0.00006267
Iteration 230/1000 | Loss: 0.00006207
Iteration 231/1000 | Loss: 0.00006198
Iteration 232/1000 | Loss: 0.00006198
Iteration 233/1000 | Loss: 0.00006198
Iteration 234/1000 | Loss: 0.00006198
Iteration 235/1000 | Loss: 0.00006198
Iteration 236/1000 | Loss: 0.00006198
Iteration 237/1000 | Loss: 0.00006198
Iteration 238/1000 | Loss: 0.00006198
Iteration 239/1000 | Loss: 0.00006198
Iteration 240/1000 | Loss: 0.00006198
Iteration 241/1000 | Loss: 0.00006197
Iteration 242/1000 | Loss: 0.00006197
Iteration 243/1000 | Loss: 0.00006197
Iteration 244/1000 | Loss: 0.00006196
Iteration 245/1000 | Loss: 0.00006196
Iteration 246/1000 | Loss: 0.00006196
Iteration 247/1000 | Loss: 0.00006196
Iteration 248/1000 | Loss: 0.00006196
Iteration 249/1000 | Loss: 0.00006196
Iteration 250/1000 | Loss: 0.00006195
Iteration 251/1000 | Loss: 0.00006195
Iteration 252/1000 | Loss: 0.00006195
Iteration 253/1000 | Loss: 0.00006195
Iteration 254/1000 | Loss: 0.00006195
Iteration 255/1000 | Loss: 0.00006195
Iteration 256/1000 | Loss: 0.00006195
Iteration 257/1000 | Loss: 0.00006195
Iteration 258/1000 | Loss: 0.00006195
Iteration 259/1000 | Loss: 0.00006195
Iteration 260/1000 | Loss: 0.00006195
Iteration 261/1000 | Loss: 0.00006195
Iteration 262/1000 | Loss: 0.00006195
Iteration 263/1000 | Loss: 0.00006195
Iteration 264/1000 | Loss: 0.00006195
Iteration 265/1000 | Loss: 0.00006195
Iteration 266/1000 | Loss: 0.00006195
Iteration 267/1000 | Loss: 0.00006195
Iteration 268/1000 | Loss: 0.00006195
Iteration 269/1000 | Loss: 0.00006195
Iteration 270/1000 | Loss: 0.00006195
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 270. Stopping optimization.
Last 5 losses: [6.195189052959904e-05, 6.195189052959904e-05, 6.195189052959904e-05, 6.195189052959904e-05, 6.195189052959904e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 6.195189052959904e-05

Optimization complete. Final v2v error: 5.5466227531433105 mm

Highest mean error: 6.5593791007995605 mm for frame 53

Lowest mean error: 3.4611175060272217 mm for frame 59

Saving results

Total time: 413.50625801086426
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_006/1043/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_006/1043.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_006/1043
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00802101
Iteration 2/25 | Loss: 0.00108640
Iteration 3/25 | Loss: 0.00070151
Iteration 4/25 | Loss: 0.00063443
Iteration 5/25 | Loss: 0.00062299
Iteration 6/25 | Loss: 0.00061992
Iteration 7/25 | Loss: 0.00061896
Iteration 8/25 | Loss: 0.00061896
Iteration 9/25 | Loss: 0.00061896
Iteration 10/25 | Loss: 0.00061896
Iteration 11/25 | Loss: 0.00061896
Iteration 12/25 | Loss: 0.00061896
Iteration 13/25 | Loss: 0.00061891
Iteration 14/25 | Loss: 0.00061891
Iteration 15/25 | Loss: 0.00061891
Iteration 16/25 | Loss: 0.00061891
Iteration 17/25 | Loss: 0.00061891
Iteration 18/25 | Loss: 0.00061891
Iteration 19/25 | Loss: 0.00061891
Iteration 20/25 | Loss: 0.00061891
Iteration 21/25 | Loss: 0.00061891
Iteration 22/25 | Loss: 0.00061891
Iteration 23/25 | Loss: 0.00061891
Iteration 24/25 | Loss: 0.00061891
Iteration 25/25 | Loss: 0.00061891

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46758795
Iteration 2/25 | Loss: 0.00031336
Iteration 3/25 | Loss: 0.00031336
Iteration 4/25 | Loss: 0.00031336
Iteration 5/25 | Loss: 0.00031336
Iteration 6/25 | Loss: 0.00031336
Iteration 7/25 | Loss: 0.00031336
Iteration 8/25 | Loss: 0.00031336
Iteration 9/25 | Loss: 0.00031336
Iteration 10/25 | Loss: 0.00031336
Iteration 11/25 | Loss: 0.00031336
Iteration 12/25 | Loss: 0.00031336
Iteration 13/25 | Loss: 0.00031336
Iteration 14/25 | Loss: 0.00031336
Iteration 15/25 | Loss: 0.00031336
Iteration 16/25 | Loss: 0.00031336
Iteration 17/25 | Loss: 0.00031336
Iteration 18/25 | Loss: 0.00031336
Iteration 19/25 | Loss: 0.00031336
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.00031335503445006907, 0.00031335503445006907, 0.00031335503445006907, 0.00031335503445006907, 0.00031335503445006907]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00031335503445006907

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00031336
Iteration 2/1000 | Loss: 0.00002019
Iteration 3/1000 | Loss: 0.00001462
Iteration 4/1000 | Loss: 0.00001331
Iteration 5/1000 | Loss: 0.00001256
Iteration 6/1000 | Loss: 0.00001210
Iteration 7/1000 | Loss: 0.00001182
Iteration 8/1000 | Loss: 0.00001166
Iteration 9/1000 | Loss: 0.00001161
Iteration 10/1000 | Loss: 0.00001155
Iteration 11/1000 | Loss: 0.00001148
Iteration 12/1000 | Loss: 0.00001131
Iteration 13/1000 | Loss: 0.00001127
Iteration 14/1000 | Loss: 0.00001119
Iteration 15/1000 | Loss: 0.00001114
Iteration 16/1000 | Loss: 0.00001111
Iteration 17/1000 | Loss: 0.00001110
Iteration 18/1000 | Loss: 0.00001110
Iteration 19/1000 | Loss: 0.00001108
Iteration 20/1000 | Loss: 0.00001108
Iteration 21/1000 | Loss: 0.00001107
Iteration 22/1000 | Loss: 0.00001107
Iteration 23/1000 | Loss: 0.00001107
Iteration 24/1000 | Loss: 0.00001106
Iteration 25/1000 | Loss: 0.00001106
Iteration 26/1000 | Loss: 0.00001105
Iteration 27/1000 | Loss: 0.00001105
Iteration 28/1000 | Loss: 0.00001105
Iteration 29/1000 | Loss: 0.00001104
Iteration 30/1000 | Loss: 0.00001103
Iteration 31/1000 | Loss: 0.00001102
Iteration 32/1000 | Loss: 0.00001102
Iteration 33/1000 | Loss: 0.00001098
Iteration 34/1000 | Loss: 0.00001094
Iteration 35/1000 | Loss: 0.00001094
Iteration 36/1000 | Loss: 0.00001092
Iteration 37/1000 | Loss: 0.00001092
Iteration 38/1000 | Loss: 0.00001092
Iteration 39/1000 | Loss: 0.00001091
Iteration 40/1000 | Loss: 0.00001091
Iteration 41/1000 | Loss: 0.00001091
Iteration 42/1000 | Loss: 0.00001090
Iteration 43/1000 | Loss: 0.00001090
Iteration 44/1000 | Loss: 0.00001090
Iteration 45/1000 | Loss: 0.00001090
Iteration 46/1000 | Loss: 0.00001090
Iteration 47/1000 | Loss: 0.00001089
Iteration 48/1000 | Loss: 0.00001089
Iteration 49/1000 | Loss: 0.00001089
Iteration 50/1000 | Loss: 0.00001089
Iteration 51/1000 | Loss: 0.00001088
Iteration 52/1000 | Loss: 0.00001088
Iteration 53/1000 | Loss: 0.00001088
Iteration 54/1000 | Loss: 0.00001088
Iteration 55/1000 | Loss: 0.00001088
Iteration 56/1000 | Loss: 0.00001088
Iteration 57/1000 | Loss: 0.00001088
Iteration 58/1000 | Loss: 0.00001088
Iteration 59/1000 | Loss: 0.00001087
Iteration 60/1000 | Loss: 0.00001087
Iteration 61/1000 | Loss: 0.00001087
Iteration 62/1000 | Loss: 0.00001087
Iteration 63/1000 | Loss: 0.00001087
Iteration 64/1000 | Loss: 0.00001086
Iteration 65/1000 | Loss: 0.00001086
Iteration 66/1000 | Loss: 0.00001085
Iteration 67/1000 | Loss: 0.00001085
Iteration 68/1000 | Loss: 0.00001085
Iteration 69/1000 | Loss: 0.00001085
Iteration 70/1000 | Loss: 0.00001084
Iteration 71/1000 | Loss: 0.00001084
Iteration 72/1000 | Loss: 0.00001084
Iteration 73/1000 | Loss: 0.00001084
Iteration 74/1000 | Loss: 0.00001084
Iteration 75/1000 | Loss: 0.00001084
Iteration 76/1000 | Loss: 0.00001083
Iteration 77/1000 | Loss: 0.00001083
Iteration 78/1000 | Loss: 0.00001083
Iteration 79/1000 | Loss: 0.00001083
Iteration 80/1000 | Loss: 0.00001083
Iteration 81/1000 | Loss: 0.00001083
Iteration 82/1000 | Loss: 0.00001083
Iteration 83/1000 | Loss: 0.00001083
Iteration 84/1000 | Loss: 0.00001083
Iteration 85/1000 | Loss: 0.00001083
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 85. Stopping optimization.
Last 5 losses: [1.0831568033609074e-05, 1.0831568033609074e-05, 1.0831568033609074e-05, 1.0831568033609074e-05, 1.0831568033609074e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0831568033609074e-05

Optimization complete. Final v2v error: 2.7691245079040527 mm

Highest mean error: 2.9703381061553955 mm for frame 58

Lowest mean error: 2.6080636978149414 mm for frame 23

Saving results

Total time: 38.324379205703735
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_006/1085/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_006/1085.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_006/1085
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00452166
Iteration 2/25 | Loss: 0.00078587
Iteration 3/25 | Loss: 0.00063637
Iteration 4/25 | Loss: 0.00060885
Iteration 5/25 | Loss: 0.00060206
Iteration 6/25 | Loss: 0.00059969
Iteration 7/25 | Loss: 0.00059885
Iteration 8/25 | Loss: 0.00059872
Iteration 9/25 | Loss: 0.00059872
Iteration 10/25 | Loss: 0.00059872
Iteration 11/25 | Loss: 0.00059872
Iteration 12/25 | Loss: 0.00059872
Iteration 13/25 | Loss: 0.00059872
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0005987187614664435, 0.0005987187614664435, 0.0005987187614664435, 0.0005987187614664435, 0.0005987187614664435]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005987187614664435

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46197414
Iteration 2/25 | Loss: 0.00021318
Iteration 3/25 | Loss: 0.00021316
Iteration 4/25 | Loss: 0.00021316
Iteration 5/25 | Loss: 0.00021316
Iteration 6/25 | Loss: 0.00021316
Iteration 7/25 | Loss: 0.00021316
Iteration 8/25 | Loss: 0.00021316
Iteration 9/25 | Loss: 0.00021316
Iteration 10/25 | Loss: 0.00021316
Iteration 11/25 | Loss: 0.00021316
Iteration 12/25 | Loss: 0.00021316
Iteration 13/25 | Loss: 0.00021316
Iteration 14/25 | Loss: 0.00021316
Iteration 15/25 | Loss: 0.00021316
Iteration 16/25 | Loss: 0.00021316
Iteration 17/25 | Loss: 0.00021316
Iteration 18/25 | Loss: 0.00021316
Iteration 19/25 | Loss: 0.00021316
Iteration 20/25 | Loss: 0.00021316
Iteration 21/25 | Loss: 0.00021316
Iteration 22/25 | Loss: 0.00021316
Iteration 23/25 | Loss: 0.00021316
Iteration 24/25 | Loss: 0.00021316
Iteration 25/25 | Loss: 0.00021316

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00021316
Iteration 2/1000 | Loss: 0.00002807
Iteration 3/1000 | Loss: 0.00001625
Iteration 4/1000 | Loss: 0.00001335
Iteration 5/1000 | Loss: 0.00001251
Iteration 6/1000 | Loss: 0.00001206
Iteration 7/1000 | Loss: 0.00001174
Iteration 8/1000 | Loss: 0.00001157
Iteration 9/1000 | Loss: 0.00001152
Iteration 10/1000 | Loss: 0.00001151
Iteration 11/1000 | Loss: 0.00001150
Iteration 12/1000 | Loss: 0.00001147
Iteration 13/1000 | Loss: 0.00001145
Iteration 14/1000 | Loss: 0.00001145
Iteration 15/1000 | Loss: 0.00001144
Iteration 16/1000 | Loss: 0.00001140
Iteration 17/1000 | Loss: 0.00001140
Iteration 18/1000 | Loss: 0.00001140
Iteration 19/1000 | Loss: 0.00001140
Iteration 20/1000 | Loss: 0.00001139
Iteration 21/1000 | Loss: 0.00001139
Iteration 22/1000 | Loss: 0.00001139
Iteration 23/1000 | Loss: 0.00001139
Iteration 24/1000 | Loss: 0.00001139
Iteration 25/1000 | Loss: 0.00001139
Iteration 26/1000 | Loss: 0.00001139
Iteration 27/1000 | Loss: 0.00001139
Iteration 28/1000 | Loss: 0.00001139
Iteration 29/1000 | Loss: 0.00001138
Iteration 30/1000 | Loss: 0.00001138
Iteration 31/1000 | Loss: 0.00001136
Iteration 32/1000 | Loss: 0.00001135
Iteration 33/1000 | Loss: 0.00001134
Iteration 34/1000 | Loss: 0.00001134
Iteration 35/1000 | Loss: 0.00001134
Iteration 36/1000 | Loss: 0.00001133
Iteration 37/1000 | Loss: 0.00001133
Iteration 38/1000 | Loss: 0.00001133
Iteration 39/1000 | Loss: 0.00001132
Iteration 40/1000 | Loss: 0.00001132
Iteration 41/1000 | Loss: 0.00001131
Iteration 42/1000 | Loss: 0.00001131
Iteration 43/1000 | Loss: 0.00001131
Iteration 44/1000 | Loss: 0.00001131
Iteration 45/1000 | Loss: 0.00001130
Iteration 46/1000 | Loss: 0.00001130
Iteration 47/1000 | Loss: 0.00001129
Iteration 48/1000 | Loss: 0.00001129
Iteration 49/1000 | Loss: 0.00001129
Iteration 50/1000 | Loss: 0.00001129
Iteration 51/1000 | Loss: 0.00001129
Iteration 52/1000 | Loss: 0.00001129
Iteration 53/1000 | Loss: 0.00001129
Iteration 54/1000 | Loss: 0.00001128
Iteration 55/1000 | Loss: 0.00001127
Iteration 56/1000 | Loss: 0.00001126
Iteration 57/1000 | Loss: 0.00001126
Iteration 58/1000 | Loss: 0.00001125
Iteration 59/1000 | Loss: 0.00001125
Iteration 60/1000 | Loss: 0.00001125
Iteration 61/1000 | Loss: 0.00001125
Iteration 62/1000 | Loss: 0.00001125
Iteration 63/1000 | Loss: 0.00001124
Iteration 64/1000 | Loss: 0.00001124
Iteration 65/1000 | Loss: 0.00001124
Iteration 66/1000 | Loss: 0.00001122
Iteration 67/1000 | Loss: 0.00001122
Iteration 68/1000 | Loss: 0.00001121
Iteration 69/1000 | Loss: 0.00001121
Iteration 70/1000 | Loss: 0.00001121
Iteration 71/1000 | Loss: 0.00001121
Iteration 72/1000 | Loss: 0.00001120
Iteration 73/1000 | Loss: 0.00001120
Iteration 74/1000 | Loss: 0.00001119
Iteration 75/1000 | Loss: 0.00001119
Iteration 76/1000 | Loss: 0.00001119
Iteration 77/1000 | Loss: 0.00001119
Iteration 78/1000 | Loss: 0.00001118
Iteration 79/1000 | Loss: 0.00001118
Iteration 80/1000 | Loss: 0.00001118
Iteration 81/1000 | Loss: 0.00001117
Iteration 82/1000 | Loss: 0.00001117
Iteration 83/1000 | Loss: 0.00001117
Iteration 84/1000 | Loss: 0.00001116
Iteration 85/1000 | Loss: 0.00001116
Iteration 86/1000 | Loss: 0.00001116
Iteration 87/1000 | Loss: 0.00001115
Iteration 88/1000 | Loss: 0.00001115
Iteration 89/1000 | Loss: 0.00001115
Iteration 90/1000 | Loss: 0.00001115
Iteration 91/1000 | Loss: 0.00001115
Iteration 92/1000 | Loss: 0.00001115
Iteration 93/1000 | Loss: 0.00001114
Iteration 94/1000 | Loss: 0.00001114
Iteration 95/1000 | Loss: 0.00001114
Iteration 96/1000 | Loss: 0.00001114
Iteration 97/1000 | Loss: 0.00001113
Iteration 98/1000 | Loss: 0.00001113
Iteration 99/1000 | Loss: 0.00001113
Iteration 100/1000 | Loss: 0.00001113
Iteration 101/1000 | Loss: 0.00001113
Iteration 102/1000 | Loss: 0.00001113
Iteration 103/1000 | Loss: 0.00001113
Iteration 104/1000 | Loss: 0.00001112
Iteration 105/1000 | Loss: 0.00001112
Iteration 106/1000 | Loss: 0.00001112
Iteration 107/1000 | Loss: 0.00001112
Iteration 108/1000 | Loss: 0.00001112
Iteration 109/1000 | Loss: 0.00001112
Iteration 110/1000 | Loss: 0.00001111
Iteration 111/1000 | Loss: 0.00001111
Iteration 112/1000 | Loss: 0.00001111
Iteration 113/1000 | Loss: 0.00001111
Iteration 114/1000 | Loss: 0.00001111
Iteration 115/1000 | Loss: 0.00001111
Iteration 116/1000 | Loss: 0.00001111
Iteration 117/1000 | Loss: 0.00001111
Iteration 118/1000 | Loss: 0.00001111
Iteration 119/1000 | Loss: 0.00001110
Iteration 120/1000 | Loss: 0.00001110
Iteration 121/1000 | Loss: 0.00001110
Iteration 122/1000 | Loss: 0.00001110
Iteration 123/1000 | Loss: 0.00001110
Iteration 124/1000 | Loss: 0.00001110
Iteration 125/1000 | Loss: 0.00001110
Iteration 126/1000 | Loss: 0.00001110
Iteration 127/1000 | Loss: 0.00001110
Iteration 128/1000 | Loss: 0.00001110
Iteration 129/1000 | Loss: 0.00001110
Iteration 130/1000 | Loss: 0.00001109
Iteration 131/1000 | Loss: 0.00001109
Iteration 132/1000 | Loss: 0.00001109
Iteration 133/1000 | Loss: 0.00001109
Iteration 134/1000 | Loss: 0.00001109
Iteration 135/1000 | Loss: 0.00001109
Iteration 136/1000 | Loss: 0.00001109
Iteration 137/1000 | Loss: 0.00001109
Iteration 138/1000 | Loss: 0.00001109
Iteration 139/1000 | Loss: 0.00001109
Iteration 140/1000 | Loss: 0.00001109
Iteration 141/1000 | Loss: 0.00001108
Iteration 142/1000 | Loss: 0.00001108
Iteration 143/1000 | Loss: 0.00001108
Iteration 144/1000 | Loss: 0.00001108
Iteration 145/1000 | Loss: 0.00001108
Iteration 146/1000 | Loss: 0.00001108
Iteration 147/1000 | Loss: 0.00001108
Iteration 148/1000 | Loss: 0.00001108
Iteration 149/1000 | Loss: 0.00001108
Iteration 150/1000 | Loss: 0.00001108
Iteration 151/1000 | Loss: 0.00001108
Iteration 152/1000 | Loss: 0.00001108
Iteration 153/1000 | Loss: 0.00001108
Iteration 154/1000 | Loss: 0.00001108
Iteration 155/1000 | Loss: 0.00001108
Iteration 156/1000 | Loss: 0.00001108
Iteration 157/1000 | Loss: 0.00001108
Iteration 158/1000 | Loss: 0.00001107
Iteration 159/1000 | Loss: 0.00001107
Iteration 160/1000 | Loss: 0.00001107
Iteration 161/1000 | Loss: 0.00001107
Iteration 162/1000 | Loss: 0.00001107
Iteration 163/1000 | Loss: 0.00001107
Iteration 164/1000 | Loss: 0.00001107
Iteration 165/1000 | Loss: 0.00001107
Iteration 166/1000 | Loss: 0.00001107
Iteration 167/1000 | Loss: 0.00001107
Iteration 168/1000 | Loss: 0.00001107
Iteration 169/1000 | Loss: 0.00001107
Iteration 170/1000 | Loss: 0.00001107
Iteration 171/1000 | Loss: 0.00001107
Iteration 172/1000 | Loss: 0.00001107
Iteration 173/1000 | Loss: 0.00001106
Iteration 174/1000 | Loss: 0.00001106
Iteration 175/1000 | Loss: 0.00001106
Iteration 176/1000 | Loss: 0.00001106
Iteration 177/1000 | Loss: 0.00001106
Iteration 178/1000 | Loss: 0.00001106
Iteration 179/1000 | Loss: 0.00001106
Iteration 180/1000 | Loss: 0.00001106
Iteration 181/1000 | Loss: 0.00001106
Iteration 182/1000 | Loss: 0.00001106
Iteration 183/1000 | Loss: 0.00001106
Iteration 184/1000 | Loss: 0.00001106
Iteration 185/1000 | Loss: 0.00001106
Iteration 186/1000 | Loss: 0.00001106
Iteration 187/1000 | Loss: 0.00001105
Iteration 188/1000 | Loss: 0.00001105
Iteration 189/1000 | Loss: 0.00001105
Iteration 190/1000 | Loss: 0.00001105
Iteration 191/1000 | Loss: 0.00001105
Iteration 192/1000 | Loss: 0.00001105
Iteration 193/1000 | Loss: 0.00001105
Iteration 194/1000 | Loss: 0.00001105
Iteration 195/1000 | Loss: 0.00001105
Iteration 196/1000 | Loss: 0.00001105
Iteration 197/1000 | Loss: 0.00001105
Iteration 198/1000 | Loss: 0.00001105
Iteration 199/1000 | Loss: 0.00001105
Iteration 200/1000 | Loss: 0.00001105
Iteration 201/1000 | Loss: 0.00001105
Iteration 202/1000 | Loss: 0.00001105
Iteration 203/1000 | Loss: 0.00001105
Iteration 204/1000 | Loss: 0.00001104
Iteration 205/1000 | Loss: 0.00001104
Iteration 206/1000 | Loss: 0.00001104
Iteration 207/1000 | Loss: 0.00001104
Iteration 208/1000 | Loss: 0.00001104
Iteration 209/1000 | Loss: 0.00001104
Iteration 210/1000 | Loss: 0.00001104
Iteration 211/1000 | Loss: 0.00001104
Iteration 212/1000 | Loss: 0.00001104
Iteration 213/1000 | Loss: 0.00001104
Iteration 214/1000 | Loss: 0.00001104
Iteration 215/1000 | Loss: 0.00001104
Iteration 216/1000 | Loss: 0.00001104
Iteration 217/1000 | Loss: 0.00001104
Iteration 218/1000 | Loss: 0.00001104
Iteration 219/1000 | Loss: 0.00001104
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 219. Stopping optimization.
Last 5 losses: [1.1042008736694697e-05, 1.1042008736694697e-05, 1.1042008736694697e-05, 1.1042008736694697e-05, 1.1042008736694697e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1042008736694697e-05

Optimization complete. Final v2v error: 2.7139089107513428 mm

Highest mean error: 3.3420472145080566 mm for frame 62

Lowest mean error: 2.4508280754089355 mm for frame 20

Saving results

Total time: 37.07515621185303
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_006/1000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_006/1000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_006/1000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00432000
Iteration 2/25 | Loss: 0.00101412
Iteration 3/25 | Loss: 0.00067584
Iteration 4/25 | Loss: 0.00063927
Iteration 5/25 | Loss: 0.00062876
Iteration 6/25 | Loss: 0.00062468
Iteration 7/25 | Loss: 0.00062365
Iteration 8/25 | Loss: 0.00062352
Iteration 9/25 | Loss: 0.00062352
Iteration 10/25 | Loss: 0.00062352
Iteration 11/25 | Loss: 0.00062352
Iteration 12/25 | Loss: 0.00062352
Iteration 13/25 | Loss: 0.00062352
Iteration 14/25 | Loss: 0.00062352
Iteration 15/25 | Loss: 0.00062352
Iteration 16/25 | Loss: 0.00062352
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0006235159235075116, 0.0006235159235075116, 0.0006235159235075116, 0.0006235159235075116, 0.0006235159235075116]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006235159235075116

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.49538505
Iteration 2/25 | Loss: 0.00029417
Iteration 3/25 | Loss: 0.00029415
Iteration 4/25 | Loss: 0.00029415
Iteration 5/25 | Loss: 0.00029415
Iteration 6/25 | Loss: 0.00029415
Iteration 7/25 | Loss: 0.00029415
Iteration 8/25 | Loss: 0.00029415
Iteration 9/25 | Loss: 0.00029415
Iteration 10/25 | Loss: 0.00029415
Iteration 11/25 | Loss: 0.00029415
Iteration 12/25 | Loss: 0.00029415
Iteration 13/25 | Loss: 0.00029415
Iteration 14/25 | Loss: 0.00029415
Iteration 15/25 | Loss: 0.00029415
Iteration 16/25 | Loss: 0.00029415
Iteration 17/25 | Loss: 0.00029415
Iteration 18/25 | Loss: 0.00029415
Iteration 19/25 | Loss: 0.00029415
Iteration 20/25 | Loss: 0.00029415
Iteration 21/25 | Loss: 0.00029415
Iteration 22/25 | Loss: 0.00029415
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.000294147728709504, 0.000294147728709504, 0.000294147728709504, 0.000294147728709504, 0.000294147728709504]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000294147728709504

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00029415
Iteration 2/1000 | Loss: 0.00003174
Iteration 3/1000 | Loss: 0.00001783
Iteration 4/1000 | Loss: 0.00001550
Iteration 5/1000 | Loss: 0.00001454
Iteration 6/1000 | Loss: 0.00001401
Iteration 7/1000 | Loss: 0.00001371
Iteration 8/1000 | Loss: 0.00001342
Iteration 9/1000 | Loss: 0.00001326
Iteration 10/1000 | Loss: 0.00001322
Iteration 11/1000 | Loss: 0.00001321
Iteration 12/1000 | Loss: 0.00001320
Iteration 13/1000 | Loss: 0.00001316
Iteration 14/1000 | Loss: 0.00001307
Iteration 15/1000 | Loss: 0.00001307
Iteration 16/1000 | Loss: 0.00001306
Iteration 17/1000 | Loss: 0.00001303
Iteration 18/1000 | Loss: 0.00001302
Iteration 19/1000 | Loss: 0.00001301
Iteration 20/1000 | Loss: 0.00001300
Iteration 21/1000 | Loss: 0.00001300
Iteration 22/1000 | Loss: 0.00001300
Iteration 23/1000 | Loss: 0.00001299
Iteration 24/1000 | Loss: 0.00001299
Iteration 25/1000 | Loss: 0.00001298
Iteration 26/1000 | Loss: 0.00001298
Iteration 27/1000 | Loss: 0.00001297
Iteration 28/1000 | Loss: 0.00001297
Iteration 29/1000 | Loss: 0.00001297
Iteration 30/1000 | Loss: 0.00001297
Iteration 31/1000 | Loss: 0.00001297
Iteration 32/1000 | Loss: 0.00001296
Iteration 33/1000 | Loss: 0.00001296
Iteration 34/1000 | Loss: 0.00001296
Iteration 35/1000 | Loss: 0.00001296
Iteration 36/1000 | Loss: 0.00001296
Iteration 37/1000 | Loss: 0.00001296
Iteration 38/1000 | Loss: 0.00001296
Iteration 39/1000 | Loss: 0.00001296
Iteration 40/1000 | Loss: 0.00001296
Iteration 41/1000 | Loss: 0.00001296
Iteration 42/1000 | Loss: 0.00001295
Iteration 43/1000 | Loss: 0.00001294
Iteration 44/1000 | Loss: 0.00001294
Iteration 45/1000 | Loss: 0.00001293
Iteration 46/1000 | Loss: 0.00001293
Iteration 47/1000 | Loss: 0.00001293
Iteration 48/1000 | Loss: 0.00001293
Iteration 49/1000 | Loss: 0.00001293
Iteration 50/1000 | Loss: 0.00001293
Iteration 51/1000 | Loss: 0.00001292
Iteration 52/1000 | Loss: 0.00001292
Iteration 53/1000 | Loss: 0.00001290
Iteration 54/1000 | Loss: 0.00001290
Iteration 55/1000 | Loss: 0.00001289
Iteration 56/1000 | Loss: 0.00001289
Iteration 57/1000 | Loss: 0.00001289
Iteration 58/1000 | Loss: 0.00001289
Iteration 59/1000 | Loss: 0.00001289
Iteration 60/1000 | Loss: 0.00001289
Iteration 61/1000 | Loss: 0.00001289
Iteration 62/1000 | Loss: 0.00001289
Iteration 63/1000 | Loss: 0.00001288
Iteration 64/1000 | Loss: 0.00001288
Iteration 65/1000 | Loss: 0.00001287
Iteration 66/1000 | Loss: 0.00001287
Iteration 67/1000 | Loss: 0.00001287
Iteration 68/1000 | Loss: 0.00001287
Iteration 69/1000 | Loss: 0.00001287
Iteration 70/1000 | Loss: 0.00001287
Iteration 71/1000 | Loss: 0.00001286
Iteration 72/1000 | Loss: 0.00001286
Iteration 73/1000 | Loss: 0.00001286
Iteration 74/1000 | Loss: 0.00001286
Iteration 75/1000 | Loss: 0.00001286
Iteration 76/1000 | Loss: 0.00001286
Iteration 77/1000 | Loss: 0.00001285
Iteration 78/1000 | Loss: 0.00001285
Iteration 79/1000 | Loss: 0.00001285
Iteration 80/1000 | Loss: 0.00001285
Iteration 81/1000 | Loss: 0.00001285
Iteration 82/1000 | Loss: 0.00001284
Iteration 83/1000 | Loss: 0.00001284
Iteration 84/1000 | Loss: 0.00001284
Iteration 85/1000 | Loss: 0.00001284
Iteration 86/1000 | Loss: 0.00001284
Iteration 87/1000 | Loss: 0.00001284
Iteration 88/1000 | Loss: 0.00001284
Iteration 89/1000 | Loss: 0.00001284
Iteration 90/1000 | Loss: 0.00001283
Iteration 91/1000 | Loss: 0.00001283
Iteration 92/1000 | Loss: 0.00001283
Iteration 93/1000 | Loss: 0.00001283
Iteration 94/1000 | Loss: 0.00001283
Iteration 95/1000 | Loss: 0.00001283
Iteration 96/1000 | Loss: 0.00001283
Iteration 97/1000 | Loss: 0.00001283
Iteration 98/1000 | Loss: 0.00001282
Iteration 99/1000 | Loss: 0.00001282
Iteration 100/1000 | Loss: 0.00001282
Iteration 101/1000 | Loss: 0.00001281
Iteration 102/1000 | Loss: 0.00001281
Iteration 103/1000 | Loss: 0.00001281
Iteration 104/1000 | Loss: 0.00001281
Iteration 105/1000 | Loss: 0.00001280
Iteration 106/1000 | Loss: 0.00001280
Iteration 107/1000 | Loss: 0.00001280
Iteration 108/1000 | Loss: 0.00001280
Iteration 109/1000 | Loss: 0.00001279
Iteration 110/1000 | Loss: 0.00001279
Iteration 111/1000 | Loss: 0.00001279
Iteration 112/1000 | Loss: 0.00001279
Iteration 113/1000 | Loss: 0.00001279
Iteration 114/1000 | Loss: 0.00001279
Iteration 115/1000 | Loss: 0.00001279
Iteration 116/1000 | Loss: 0.00001278
Iteration 117/1000 | Loss: 0.00001278
Iteration 118/1000 | Loss: 0.00001278
Iteration 119/1000 | Loss: 0.00001278
Iteration 120/1000 | Loss: 0.00001278
Iteration 121/1000 | Loss: 0.00001278
Iteration 122/1000 | Loss: 0.00001278
Iteration 123/1000 | Loss: 0.00001277
Iteration 124/1000 | Loss: 0.00001277
Iteration 125/1000 | Loss: 0.00001277
Iteration 126/1000 | Loss: 0.00001277
Iteration 127/1000 | Loss: 0.00001277
Iteration 128/1000 | Loss: 0.00001277
Iteration 129/1000 | Loss: 0.00001277
Iteration 130/1000 | Loss: 0.00001277
Iteration 131/1000 | Loss: 0.00001277
Iteration 132/1000 | Loss: 0.00001277
Iteration 133/1000 | Loss: 0.00001277
Iteration 134/1000 | Loss: 0.00001277
Iteration 135/1000 | Loss: 0.00001277
Iteration 136/1000 | Loss: 0.00001276
Iteration 137/1000 | Loss: 0.00001276
Iteration 138/1000 | Loss: 0.00001276
Iteration 139/1000 | Loss: 0.00001276
Iteration 140/1000 | Loss: 0.00001276
Iteration 141/1000 | Loss: 0.00001276
Iteration 142/1000 | Loss: 0.00001276
Iteration 143/1000 | Loss: 0.00001275
Iteration 144/1000 | Loss: 0.00001275
Iteration 145/1000 | Loss: 0.00001275
Iteration 146/1000 | Loss: 0.00001275
Iteration 147/1000 | Loss: 0.00001275
Iteration 148/1000 | Loss: 0.00001275
Iteration 149/1000 | Loss: 0.00001275
Iteration 150/1000 | Loss: 0.00001275
Iteration 151/1000 | Loss: 0.00001275
Iteration 152/1000 | Loss: 0.00001275
Iteration 153/1000 | Loss: 0.00001275
Iteration 154/1000 | Loss: 0.00001275
Iteration 155/1000 | Loss: 0.00001275
Iteration 156/1000 | Loss: 0.00001275
Iteration 157/1000 | Loss: 0.00001274
Iteration 158/1000 | Loss: 0.00001274
Iteration 159/1000 | Loss: 0.00001274
Iteration 160/1000 | Loss: 0.00001274
Iteration 161/1000 | Loss: 0.00001274
Iteration 162/1000 | Loss: 0.00001274
Iteration 163/1000 | Loss: 0.00001274
Iteration 164/1000 | Loss: 0.00001273
Iteration 165/1000 | Loss: 0.00001273
Iteration 166/1000 | Loss: 0.00001273
Iteration 167/1000 | Loss: 0.00001273
Iteration 168/1000 | Loss: 0.00001273
Iteration 169/1000 | Loss: 0.00001273
Iteration 170/1000 | Loss: 0.00001273
Iteration 171/1000 | Loss: 0.00001273
Iteration 172/1000 | Loss: 0.00001273
Iteration 173/1000 | Loss: 0.00001273
Iteration 174/1000 | Loss: 0.00001273
Iteration 175/1000 | Loss: 0.00001273
Iteration 176/1000 | Loss: 0.00001272
Iteration 177/1000 | Loss: 0.00001272
Iteration 178/1000 | Loss: 0.00001272
Iteration 179/1000 | Loss: 0.00001272
Iteration 180/1000 | Loss: 0.00001272
Iteration 181/1000 | Loss: 0.00001272
Iteration 182/1000 | Loss: 0.00001272
Iteration 183/1000 | Loss: 0.00001272
Iteration 184/1000 | Loss: 0.00001272
Iteration 185/1000 | Loss: 0.00001272
Iteration 186/1000 | Loss: 0.00001271
Iteration 187/1000 | Loss: 0.00001271
Iteration 188/1000 | Loss: 0.00001271
Iteration 189/1000 | Loss: 0.00001271
Iteration 190/1000 | Loss: 0.00001271
Iteration 191/1000 | Loss: 0.00001271
Iteration 192/1000 | Loss: 0.00001271
Iteration 193/1000 | Loss: 0.00001271
Iteration 194/1000 | Loss: 0.00001271
Iteration 195/1000 | Loss: 0.00001271
Iteration 196/1000 | Loss: 0.00001271
Iteration 197/1000 | Loss: 0.00001271
Iteration 198/1000 | Loss: 0.00001271
Iteration 199/1000 | Loss: 0.00001271
Iteration 200/1000 | Loss: 0.00001271
Iteration 201/1000 | Loss: 0.00001271
Iteration 202/1000 | Loss: 0.00001271
Iteration 203/1000 | Loss: 0.00001271
Iteration 204/1000 | Loss: 0.00001271
Iteration 205/1000 | Loss: 0.00001271
Iteration 206/1000 | Loss: 0.00001271
Iteration 207/1000 | Loss: 0.00001271
Iteration 208/1000 | Loss: 0.00001271
Iteration 209/1000 | Loss: 0.00001271
Iteration 210/1000 | Loss: 0.00001271
Iteration 211/1000 | Loss: 0.00001271
Iteration 212/1000 | Loss: 0.00001271
Iteration 213/1000 | Loss: 0.00001271
Iteration 214/1000 | Loss: 0.00001271
Iteration 215/1000 | Loss: 0.00001271
Iteration 216/1000 | Loss: 0.00001271
Iteration 217/1000 | Loss: 0.00001271
Iteration 218/1000 | Loss: 0.00001271
Iteration 219/1000 | Loss: 0.00001271
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 219. Stopping optimization.
Last 5 losses: [1.2710441296803765e-05, 1.2710441296803765e-05, 1.2710441296803765e-05, 1.2710441296803765e-05, 1.2710441296803765e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2710441296803765e-05

Optimization complete. Final v2v error: 2.925571918487549 mm

Highest mean error: 3.937854528427124 mm for frame 56

Lowest mean error: 2.2841081619262695 mm for frame 119

Saving results

Total time: 38.49085450172424
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_006/1038/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_006/1038.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_006/1038
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00277484
Iteration 2/25 | Loss: 0.00098453
Iteration 3/25 | Loss: 0.00071369
Iteration 4/25 | Loss: 0.00065695
Iteration 5/25 | Loss: 0.00064031
Iteration 6/25 | Loss: 0.00063360
Iteration 7/25 | Loss: 0.00063170
Iteration 8/25 | Loss: 0.00063100
Iteration 9/25 | Loss: 0.00063078
Iteration 10/25 | Loss: 0.00063075
Iteration 11/25 | Loss: 0.00063075
Iteration 12/25 | Loss: 0.00063075
Iteration 13/25 | Loss: 0.00063075
Iteration 14/25 | Loss: 0.00063075
Iteration 15/25 | Loss: 0.00063075
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0006307488074526191, 0.0006307488074526191, 0.0006307488074526191, 0.0006307488074526191, 0.0006307488074526191]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006307488074526191

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45666265
Iteration 2/25 | Loss: 0.00028145
Iteration 3/25 | Loss: 0.00028145
Iteration 4/25 | Loss: 0.00028145
Iteration 5/25 | Loss: 0.00028145
Iteration 6/25 | Loss: 0.00028145
Iteration 7/25 | Loss: 0.00028145
Iteration 8/25 | Loss: 0.00028145
Iteration 9/25 | Loss: 0.00028144
Iteration 10/25 | Loss: 0.00028144
Iteration 11/25 | Loss: 0.00028144
Iteration 12/25 | Loss: 0.00028144
Iteration 13/25 | Loss: 0.00028144
Iteration 14/25 | Loss: 0.00028144
Iteration 15/25 | Loss: 0.00028144
Iteration 16/25 | Loss: 0.00028144
Iteration 17/25 | Loss: 0.00028144
Iteration 18/25 | Loss: 0.00028144
Iteration 19/25 | Loss: 0.00028144
Iteration 20/25 | Loss: 0.00028144
Iteration 21/25 | Loss: 0.00028144
Iteration 22/25 | Loss: 0.00028144
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0002814444887917489, 0.0002814444887917489, 0.0002814444887917489, 0.0002814444887917489, 0.0002814444887917489]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0002814444887917489

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00028144
Iteration 2/1000 | Loss: 0.00003162
Iteration 3/1000 | Loss: 0.00002205
Iteration 4/1000 | Loss: 0.00001902
Iteration 5/1000 | Loss: 0.00001754
Iteration 6/1000 | Loss: 0.00001679
Iteration 7/1000 | Loss: 0.00001624
Iteration 8/1000 | Loss: 0.00001585
Iteration 9/1000 | Loss: 0.00001557
Iteration 10/1000 | Loss: 0.00001549
Iteration 11/1000 | Loss: 0.00001544
Iteration 12/1000 | Loss: 0.00001542
Iteration 13/1000 | Loss: 0.00001520
Iteration 14/1000 | Loss: 0.00001512
Iteration 15/1000 | Loss: 0.00001497
Iteration 16/1000 | Loss: 0.00001496
Iteration 17/1000 | Loss: 0.00001491
Iteration 18/1000 | Loss: 0.00001485
Iteration 19/1000 | Loss: 0.00001484
Iteration 20/1000 | Loss: 0.00001483
Iteration 21/1000 | Loss: 0.00001483
Iteration 22/1000 | Loss: 0.00001482
Iteration 23/1000 | Loss: 0.00001481
Iteration 24/1000 | Loss: 0.00001478
Iteration 25/1000 | Loss: 0.00001477
Iteration 26/1000 | Loss: 0.00001474
Iteration 27/1000 | Loss: 0.00001474
Iteration 28/1000 | Loss: 0.00001473
Iteration 29/1000 | Loss: 0.00001473
Iteration 30/1000 | Loss: 0.00001471
Iteration 31/1000 | Loss: 0.00001471
Iteration 32/1000 | Loss: 0.00001470
Iteration 33/1000 | Loss: 0.00001470
Iteration 34/1000 | Loss: 0.00001469
Iteration 35/1000 | Loss: 0.00001469
Iteration 36/1000 | Loss: 0.00001469
Iteration 37/1000 | Loss: 0.00001468
Iteration 38/1000 | Loss: 0.00001468
Iteration 39/1000 | Loss: 0.00001467
Iteration 40/1000 | Loss: 0.00001467
Iteration 41/1000 | Loss: 0.00001467
Iteration 42/1000 | Loss: 0.00001466
Iteration 43/1000 | Loss: 0.00001466
Iteration 44/1000 | Loss: 0.00001466
Iteration 45/1000 | Loss: 0.00001466
Iteration 46/1000 | Loss: 0.00001465
Iteration 47/1000 | Loss: 0.00001465
Iteration 48/1000 | Loss: 0.00001464
Iteration 49/1000 | Loss: 0.00001464
Iteration 50/1000 | Loss: 0.00001464
Iteration 51/1000 | Loss: 0.00001464
Iteration 52/1000 | Loss: 0.00001464
Iteration 53/1000 | Loss: 0.00001464
Iteration 54/1000 | Loss: 0.00001463
Iteration 55/1000 | Loss: 0.00001463
Iteration 56/1000 | Loss: 0.00001463
Iteration 57/1000 | Loss: 0.00001463
Iteration 58/1000 | Loss: 0.00001463
Iteration 59/1000 | Loss: 0.00001462
Iteration 60/1000 | Loss: 0.00001462
Iteration 61/1000 | Loss: 0.00001462
Iteration 62/1000 | Loss: 0.00001461
Iteration 63/1000 | Loss: 0.00001461
Iteration 64/1000 | Loss: 0.00001461
Iteration 65/1000 | Loss: 0.00001461
Iteration 66/1000 | Loss: 0.00001461
Iteration 67/1000 | Loss: 0.00001461
Iteration 68/1000 | Loss: 0.00001461
Iteration 69/1000 | Loss: 0.00001460
Iteration 70/1000 | Loss: 0.00001460
Iteration 71/1000 | Loss: 0.00001460
Iteration 72/1000 | Loss: 0.00001460
Iteration 73/1000 | Loss: 0.00001460
Iteration 74/1000 | Loss: 0.00001460
Iteration 75/1000 | Loss: 0.00001460
Iteration 76/1000 | Loss: 0.00001460
Iteration 77/1000 | Loss: 0.00001460
Iteration 78/1000 | Loss: 0.00001460
Iteration 79/1000 | Loss: 0.00001460
Iteration 80/1000 | Loss: 0.00001459
Iteration 81/1000 | Loss: 0.00001459
Iteration 82/1000 | Loss: 0.00001459
Iteration 83/1000 | Loss: 0.00001459
Iteration 84/1000 | Loss: 0.00001459
Iteration 85/1000 | Loss: 0.00001459
Iteration 86/1000 | Loss: 0.00001459
Iteration 87/1000 | Loss: 0.00001458
Iteration 88/1000 | Loss: 0.00001458
Iteration 89/1000 | Loss: 0.00001458
Iteration 90/1000 | Loss: 0.00001458
Iteration 91/1000 | Loss: 0.00001458
Iteration 92/1000 | Loss: 0.00001458
Iteration 93/1000 | Loss: 0.00001458
Iteration 94/1000 | Loss: 0.00001458
Iteration 95/1000 | Loss: 0.00001457
Iteration 96/1000 | Loss: 0.00001457
Iteration 97/1000 | Loss: 0.00001457
Iteration 98/1000 | Loss: 0.00001457
Iteration 99/1000 | Loss: 0.00001457
Iteration 100/1000 | Loss: 0.00001457
Iteration 101/1000 | Loss: 0.00001457
Iteration 102/1000 | Loss: 0.00001457
Iteration 103/1000 | Loss: 0.00001456
Iteration 104/1000 | Loss: 0.00001456
Iteration 105/1000 | Loss: 0.00001456
Iteration 106/1000 | Loss: 0.00001456
Iteration 107/1000 | Loss: 0.00001456
Iteration 108/1000 | Loss: 0.00001456
Iteration 109/1000 | Loss: 0.00001456
Iteration 110/1000 | Loss: 0.00001456
Iteration 111/1000 | Loss: 0.00001455
Iteration 112/1000 | Loss: 0.00001455
Iteration 113/1000 | Loss: 0.00001455
Iteration 114/1000 | Loss: 0.00001455
Iteration 115/1000 | Loss: 0.00001455
Iteration 116/1000 | Loss: 0.00001455
Iteration 117/1000 | Loss: 0.00001455
Iteration 118/1000 | Loss: 0.00001455
Iteration 119/1000 | Loss: 0.00001454
Iteration 120/1000 | Loss: 0.00001454
Iteration 121/1000 | Loss: 0.00001454
Iteration 122/1000 | Loss: 0.00001454
Iteration 123/1000 | Loss: 0.00001454
Iteration 124/1000 | Loss: 0.00001453
Iteration 125/1000 | Loss: 0.00001453
Iteration 126/1000 | Loss: 0.00001453
Iteration 127/1000 | Loss: 0.00001453
Iteration 128/1000 | Loss: 0.00001453
Iteration 129/1000 | Loss: 0.00001452
Iteration 130/1000 | Loss: 0.00001452
Iteration 131/1000 | Loss: 0.00001452
Iteration 132/1000 | Loss: 0.00001452
Iteration 133/1000 | Loss: 0.00001452
Iteration 134/1000 | Loss: 0.00001452
Iteration 135/1000 | Loss: 0.00001452
Iteration 136/1000 | Loss: 0.00001452
Iteration 137/1000 | Loss: 0.00001451
Iteration 138/1000 | Loss: 0.00001451
Iteration 139/1000 | Loss: 0.00001451
Iteration 140/1000 | Loss: 0.00001451
Iteration 141/1000 | Loss: 0.00001451
Iteration 142/1000 | Loss: 0.00001451
Iteration 143/1000 | Loss: 0.00001451
Iteration 144/1000 | Loss: 0.00001451
Iteration 145/1000 | Loss: 0.00001451
Iteration 146/1000 | Loss: 0.00001451
Iteration 147/1000 | Loss: 0.00001451
Iteration 148/1000 | Loss: 0.00001451
Iteration 149/1000 | Loss: 0.00001450
Iteration 150/1000 | Loss: 0.00001450
Iteration 151/1000 | Loss: 0.00001450
Iteration 152/1000 | Loss: 0.00001450
Iteration 153/1000 | Loss: 0.00001450
Iteration 154/1000 | Loss: 0.00001450
Iteration 155/1000 | Loss: 0.00001450
Iteration 156/1000 | Loss: 0.00001450
Iteration 157/1000 | Loss: 0.00001450
Iteration 158/1000 | Loss: 0.00001450
Iteration 159/1000 | Loss: 0.00001450
Iteration 160/1000 | Loss: 0.00001450
Iteration 161/1000 | Loss: 0.00001450
Iteration 162/1000 | Loss: 0.00001450
Iteration 163/1000 | Loss: 0.00001450
Iteration 164/1000 | Loss: 0.00001450
Iteration 165/1000 | Loss: 0.00001449
Iteration 166/1000 | Loss: 0.00001449
Iteration 167/1000 | Loss: 0.00001449
Iteration 168/1000 | Loss: 0.00001449
Iteration 169/1000 | Loss: 0.00001449
Iteration 170/1000 | Loss: 0.00001449
Iteration 171/1000 | Loss: 0.00001449
Iteration 172/1000 | Loss: 0.00001449
Iteration 173/1000 | Loss: 0.00001449
Iteration 174/1000 | Loss: 0.00001449
Iteration 175/1000 | Loss: 0.00001449
Iteration 176/1000 | Loss: 0.00001449
Iteration 177/1000 | Loss: 0.00001449
Iteration 178/1000 | Loss: 0.00001449
Iteration 179/1000 | Loss: 0.00001449
Iteration 180/1000 | Loss: 0.00001448
Iteration 181/1000 | Loss: 0.00001448
Iteration 182/1000 | Loss: 0.00001448
Iteration 183/1000 | Loss: 0.00001448
Iteration 184/1000 | Loss: 0.00001448
Iteration 185/1000 | Loss: 0.00001448
Iteration 186/1000 | Loss: 0.00001448
Iteration 187/1000 | Loss: 0.00001448
Iteration 188/1000 | Loss: 0.00001448
Iteration 189/1000 | Loss: 0.00001448
Iteration 190/1000 | Loss: 0.00001448
Iteration 191/1000 | Loss: 0.00001448
Iteration 192/1000 | Loss: 0.00001448
Iteration 193/1000 | Loss: 0.00001448
Iteration 194/1000 | Loss: 0.00001448
Iteration 195/1000 | Loss: 0.00001448
Iteration 196/1000 | Loss: 0.00001448
Iteration 197/1000 | Loss: 0.00001448
Iteration 198/1000 | Loss: 0.00001448
Iteration 199/1000 | Loss: 0.00001448
Iteration 200/1000 | Loss: 0.00001448
Iteration 201/1000 | Loss: 0.00001448
Iteration 202/1000 | Loss: 0.00001448
Iteration 203/1000 | Loss: 0.00001448
Iteration 204/1000 | Loss: 0.00001448
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 204. Stopping optimization.
Last 5 losses: [1.4475076568487566e-05, 1.4475076568487566e-05, 1.4475076568487566e-05, 1.4475076568487566e-05, 1.4475076568487566e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4475076568487566e-05

Optimization complete. Final v2v error: 3.207526206970215 mm

Highest mean error: 3.867481231689453 mm for frame 106

Lowest mean error: 2.9002585411071777 mm for frame 12

Saving results

Total time: 45.793110847473145
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_006/1025/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_006/1025.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_006/1025
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00421143
Iteration 2/25 | Loss: 0.00079333
Iteration 3/25 | Loss: 0.00068158
Iteration 4/25 | Loss: 0.00065159
Iteration 5/25 | Loss: 0.00064703
Iteration 6/25 | Loss: 0.00064627
Iteration 7/25 | Loss: 0.00064621
Iteration 8/25 | Loss: 0.00064621
Iteration 9/25 | Loss: 0.00064621
Iteration 10/25 | Loss: 0.00064621
Iteration 11/25 | Loss: 0.00064621
Iteration 12/25 | Loss: 0.00064621
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.000646207423415035, 0.000646207423415035, 0.000646207423415035, 0.000646207423415035, 0.000646207423415035]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000646207423415035

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.47089934
Iteration 2/25 | Loss: 0.00027207
Iteration 3/25 | Loss: 0.00027207
Iteration 4/25 | Loss: 0.00027207
Iteration 5/25 | Loss: 0.00027207
Iteration 6/25 | Loss: 0.00027207
Iteration 7/25 | Loss: 0.00027207
Iteration 8/25 | Loss: 0.00027207
Iteration 9/25 | Loss: 0.00027207
Iteration 10/25 | Loss: 0.00027206
Iteration 11/25 | Loss: 0.00027206
Iteration 12/25 | Loss: 0.00027206
Iteration 13/25 | Loss: 0.00027206
Iteration 14/25 | Loss: 0.00027206
Iteration 15/25 | Loss: 0.00027206
Iteration 16/25 | Loss: 0.00027206
Iteration 17/25 | Loss: 0.00027206
Iteration 18/25 | Loss: 0.00027206
Iteration 19/25 | Loss: 0.00027206
Iteration 20/25 | Loss: 0.00027206
Iteration 21/25 | Loss: 0.00027206
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.00027206473168917, 0.00027206473168917, 0.00027206473168917, 0.00027206473168917, 0.00027206473168917]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00027206473168917

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00027206
Iteration 2/1000 | Loss: 0.00002952
Iteration 3/1000 | Loss: 0.00002086
Iteration 4/1000 | Loss: 0.00001939
Iteration 5/1000 | Loss: 0.00001833
Iteration 6/1000 | Loss: 0.00001774
Iteration 7/1000 | Loss: 0.00001732
Iteration 8/1000 | Loss: 0.00001712
Iteration 9/1000 | Loss: 0.00001698
Iteration 10/1000 | Loss: 0.00001692
Iteration 11/1000 | Loss: 0.00001692
Iteration 12/1000 | Loss: 0.00001689
Iteration 13/1000 | Loss: 0.00001683
Iteration 14/1000 | Loss: 0.00001683
Iteration 15/1000 | Loss: 0.00001683
Iteration 16/1000 | Loss: 0.00001683
Iteration 17/1000 | Loss: 0.00001683
Iteration 18/1000 | Loss: 0.00001683
Iteration 19/1000 | Loss: 0.00001683
Iteration 20/1000 | Loss: 0.00001683
Iteration 21/1000 | Loss: 0.00001683
Iteration 22/1000 | Loss: 0.00001683
Iteration 23/1000 | Loss: 0.00001683
Iteration 24/1000 | Loss: 0.00001683
Iteration 25/1000 | Loss: 0.00001682
Iteration 26/1000 | Loss: 0.00001682
Iteration 27/1000 | Loss: 0.00001682
Iteration 28/1000 | Loss: 0.00001681
Iteration 29/1000 | Loss: 0.00001681
Iteration 30/1000 | Loss: 0.00001679
Iteration 31/1000 | Loss: 0.00001679
Iteration 32/1000 | Loss: 0.00001675
Iteration 33/1000 | Loss: 0.00001675
Iteration 34/1000 | Loss: 0.00001675
Iteration 35/1000 | Loss: 0.00001674
Iteration 36/1000 | Loss: 0.00001674
Iteration 37/1000 | Loss: 0.00001673
Iteration 38/1000 | Loss: 0.00001673
Iteration 39/1000 | Loss: 0.00001673
Iteration 40/1000 | Loss: 0.00001672
Iteration 41/1000 | Loss: 0.00001671
Iteration 42/1000 | Loss: 0.00001671
Iteration 43/1000 | Loss: 0.00001671
Iteration 44/1000 | Loss: 0.00001670
Iteration 45/1000 | Loss: 0.00001670
Iteration 46/1000 | Loss: 0.00001670
Iteration 47/1000 | Loss: 0.00001670
Iteration 48/1000 | Loss: 0.00001670
Iteration 49/1000 | Loss: 0.00001670
Iteration 50/1000 | Loss: 0.00001670
Iteration 51/1000 | Loss: 0.00001670
Iteration 52/1000 | Loss: 0.00001670
Iteration 53/1000 | Loss: 0.00001670
Iteration 54/1000 | Loss: 0.00001670
Iteration 55/1000 | Loss: 0.00001670
Iteration 56/1000 | Loss: 0.00001670
Iteration 57/1000 | Loss: 0.00001670
Iteration 58/1000 | Loss: 0.00001669
Iteration 59/1000 | Loss: 0.00001668
Iteration 60/1000 | Loss: 0.00001668
Iteration 61/1000 | Loss: 0.00001668
Iteration 62/1000 | Loss: 0.00001668
Iteration 63/1000 | Loss: 0.00001668
Iteration 64/1000 | Loss: 0.00001668
Iteration 65/1000 | Loss: 0.00001668
Iteration 66/1000 | Loss: 0.00001668
Iteration 67/1000 | Loss: 0.00001668
Iteration 68/1000 | Loss: 0.00001667
Iteration 69/1000 | Loss: 0.00001667
Iteration 70/1000 | Loss: 0.00001667
Iteration 71/1000 | Loss: 0.00001665
Iteration 72/1000 | Loss: 0.00001665
Iteration 73/1000 | Loss: 0.00001665
Iteration 74/1000 | Loss: 0.00001664
Iteration 75/1000 | Loss: 0.00001664
Iteration 76/1000 | Loss: 0.00001664
Iteration 77/1000 | Loss: 0.00001664
Iteration 78/1000 | Loss: 0.00001664
Iteration 79/1000 | Loss: 0.00001663
Iteration 80/1000 | Loss: 0.00001663
Iteration 81/1000 | Loss: 0.00001663
Iteration 82/1000 | Loss: 0.00001662
Iteration 83/1000 | Loss: 0.00001662
Iteration 84/1000 | Loss: 0.00001661
Iteration 85/1000 | Loss: 0.00001661
Iteration 86/1000 | Loss: 0.00001661
Iteration 87/1000 | Loss: 0.00001661
Iteration 88/1000 | Loss: 0.00001660
Iteration 89/1000 | Loss: 0.00001660
Iteration 90/1000 | Loss: 0.00001660
Iteration 91/1000 | Loss: 0.00001659
Iteration 92/1000 | Loss: 0.00001659
Iteration 93/1000 | Loss: 0.00001659
Iteration 94/1000 | Loss: 0.00001658
Iteration 95/1000 | Loss: 0.00001658
Iteration 96/1000 | Loss: 0.00001657
Iteration 97/1000 | Loss: 0.00001657
Iteration 98/1000 | Loss: 0.00001656
Iteration 99/1000 | Loss: 0.00001656
Iteration 100/1000 | Loss: 0.00001654
Iteration 101/1000 | Loss: 0.00001653
Iteration 102/1000 | Loss: 0.00001653
Iteration 103/1000 | Loss: 0.00001653
Iteration 104/1000 | Loss: 0.00001652
Iteration 105/1000 | Loss: 0.00001652
Iteration 106/1000 | Loss: 0.00001652
Iteration 107/1000 | Loss: 0.00001652
Iteration 108/1000 | Loss: 0.00001652
Iteration 109/1000 | Loss: 0.00001651
Iteration 110/1000 | Loss: 0.00001651
Iteration 111/1000 | Loss: 0.00001651
Iteration 112/1000 | Loss: 0.00001651
Iteration 113/1000 | Loss: 0.00001651
Iteration 114/1000 | Loss: 0.00001651
Iteration 115/1000 | Loss: 0.00001651
Iteration 116/1000 | Loss: 0.00001651
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 116. Stopping optimization.
Last 5 losses: [1.651208003750071e-05, 1.651208003750071e-05, 1.651208003750071e-05, 1.651208003750071e-05, 1.651208003750071e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.651208003750071e-05

Optimization complete. Final v2v error: 3.434325695037842 mm

Highest mean error: 3.6121888160705566 mm for frame 90

Lowest mean error: 3.180488109588623 mm for frame 128

Saving results

Total time: 32.895347356796265
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_006/1008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_006/1008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_006/1008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01059234
Iteration 2/25 | Loss: 0.00192137
Iteration 3/25 | Loss: 0.00113996
Iteration 4/25 | Loss: 0.00142816
Iteration 5/25 | Loss: 0.00120417
Iteration 6/25 | Loss: 0.00095265
Iteration 7/25 | Loss: 0.00082570
Iteration 8/25 | Loss: 0.00075411
Iteration 9/25 | Loss: 0.00070360
Iteration 10/25 | Loss: 0.00069872
Iteration 11/25 | Loss: 0.00068754
Iteration 12/25 | Loss: 0.00068282
Iteration 13/25 | Loss: 0.00068172
Iteration 14/25 | Loss: 0.00068094
Iteration 15/25 | Loss: 0.00068043
Iteration 16/25 | Loss: 0.00067963
Iteration 17/25 | Loss: 0.00068050
Iteration 18/25 | Loss: 0.00067850
Iteration 19/25 | Loss: 0.00067663
Iteration 20/25 | Loss: 0.00067562
Iteration 21/25 | Loss: 0.00067537
Iteration 22/25 | Loss: 0.00067536
Iteration 23/25 | Loss: 0.00067535
Iteration 24/25 | Loss: 0.00067535
Iteration 25/25 | Loss: 0.00067535

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46785176
Iteration 2/25 | Loss: 0.00060949
Iteration 3/25 | Loss: 0.00060949
Iteration 4/25 | Loss: 0.00060949
Iteration 5/25 | Loss: 0.00060949
Iteration 6/25 | Loss: 0.00060949
Iteration 7/25 | Loss: 0.00060949
Iteration 8/25 | Loss: 0.00060949
Iteration 9/25 | Loss: 0.00060949
Iteration 10/25 | Loss: 0.00060949
Iteration 11/25 | Loss: 0.00060949
Iteration 12/25 | Loss: 0.00060949
Iteration 13/25 | Loss: 0.00060949
Iteration 14/25 | Loss: 0.00060949
Iteration 15/25 | Loss: 0.00060949
Iteration 16/25 | Loss: 0.00060949
Iteration 17/25 | Loss: 0.00060949
Iteration 18/25 | Loss: 0.00060949
Iteration 19/25 | Loss: 0.00060949
Iteration 20/25 | Loss: 0.00060949
Iteration 21/25 | Loss: 0.00060949
Iteration 22/25 | Loss: 0.00060949
Iteration 23/25 | Loss: 0.00060949
Iteration 24/25 | Loss: 0.00060949
Iteration 25/25 | Loss: 0.00060949

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00060949
Iteration 2/1000 | Loss: 0.00095270
Iteration 3/1000 | Loss: 0.00009199
Iteration 4/1000 | Loss: 0.00004501
Iteration 5/1000 | Loss: 0.00003206
Iteration 6/1000 | Loss: 0.00002551
Iteration 7/1000 | Loss: 0.00002024
Iteration 8/1000 | Loss: 0.00001788
Iteration 9/1000 | Loss: 0.00001650
Iteration 10/1000 | Loss: 0.00001566
Iteration 11/1000 | Loss: 0.00001507
Iteration 12/1000 | Loss: 0.00001467
Iteration 13/1000 | Loss: 0.00001445
Iteration 14/1000 | Loss: 0.00001422
Iteration 15/1000 | Loss: 0.00001412
Iteration 16/1000 | Loss: 0.00001408
Iteration 17/1000 | Loss: 0.00001402
Iteration 18/1000 | Loss: 0.00001394
Iteration 19/1000 | Loss: 0.00001393
Iteration 20/1000 | Loss: 0.00001392
Iteration 21/1000 | Loss: 0.00001390
Iteration 22/1000 | Loss: 0.00001388
Iteration 23/1000 | Loss: 0.00001388
Iteration 24/1000 | Loss: 0.00001387
Iteration 25/1000 | Loss: 0.00001387
Iteration 26/1000 | Loss: 0.00001387
Iteration 27/1000 | Loss: 0.00001386
Iteration 28/1000 | Loss: 0.00001386
Iteration 29/1000 | Loss: 0.00001386
Iteration 30/1000 | Loss: 0.00001386
Iteration 31/1000 | Loss: 0.00001386
Iteration 32/1000 | Loss: 0.00001385
Iteration 33/1000 | Loss: 0.00001385
Iteration 34/1000 | Loss: 0.00001385
Iteration 35/1000 | Loss: 0.00001385
Iteration 36/1000 | Loss: 0.00001385
Iteration 37/1000 | Loss: 0.00001385
Iteration 38/1000 | Loss: 0.00001385
Iteration 39/1000 | Loss: 0.00001384
Iteration 40/1000 | Loss: 0.00001383
Iteration 41/1000 | Loss: 0.00001383
Iteration 42/1000 | Loss: 0.00001382
Iteration 43/1000 | Loss: 0.00001382
Iteration 44/1000 | Loss: 0.00001382
Iteration 45/1000 | Loss: 0.00001381
Iteration 46/1000 | Loss: 0.00001381
Iteration 47/1000 | Loss: 0.00001380
Iteration 48/1000 | Loss: 0.00001380
Iteration 49/1000 | Loss: 0.00001380
Iteration 50/1000 | Loss: 0.00001380
Iteration 51/1000 | Loss: 0.00001379
Iteration 52/1000 | Loss: 0.00001379
Iteration 53/1000 | Loss: 0.00001379
Iteration 54/1000 | Loss: 0.00001379
Iteration 55/1000 | Loss: 0.00001378
Iteration 56/1000 | Loss: 0.00001378
Iteration 57/1000 | Loss: 0.00001378
Iteration 58/1000 | Loss: 0.00001378
Iteration 59/1000 | Loss: 0.00001377
Iteration 60/1000 | Loss: 0.00001377
Iteration 61/1000 | Loss: 0.00001377
Iteration 62/1000 | Loss: 0.00001376
Iteration 63/1000 | Loss: 0.00001376
Iteration 64/1000 | Loss: 0.00001376
Iteration 65/1000 | Loss: 0.00001376
Iteration 66/1000 | Loss: 0.00001376
Iteration 67/1000 | Loss: 0.00001375
Iteration 68/1000 | Loss: 0.00001375
Iteration 69/1000 | Loss: 0.00001375
Iteration 70/1000 | Loss: 0.00001375
Iteration 71/1000 | Loss: 0.00001375
Iteration 72/1000 | Loss: 0.00001375
Iteration 73/1000 | Loss: 0.00001374
Iteration 74/1000 | Loss: 0.00001374
Iteration 75/1000 | Loss: 0.00001374
Iteration 76/1000 | Loss: 0.00001374
Iteration 77/1000 | Loss: 0.00001374
Iteration 78/1000 | Loss: 0.00001373
Iteration 79/1000 | Loss: 0.00001373
Iteration 80/1000 | Loss: 0.00001373
Iteration 81/1000 | Loss: 0.00001373
Iteration 82/1000 | Loss: 0.00001373
Iteration 83/1000 | Loss: 0.00001372
Iteration 84/1000 | Loss: 0.00001372
Iteration 85/1000 | Loss: 0.00001372
Iteration 86/1000 | Loss: 0.00001372
Iteration 87/1000 | Loss: 0.00001371
Iteration 88/1000 | Loss: 0.00001371
Iteration 89/1000 | Loss: 0.00001371
Iteration 90/1000 | Loss: 0.00001371
Iteration 91/1000 | Loss: 0.00001370
Iteration 92/1000 | Loss: 0.00001370
Iteration 93/1000 | Loss: 0.00001370
Iteration 94/1000 | Loss: 0.00001370
Iteration 95/1000 | Loss: 0.00001370
Iteration 96/1000 | Loss: 0.00001370
Iteration 97/1000 | Loss: 0.00001370
Iteration 98/1000 | Loss: 0.00001369
Iteration 99/1000 | Loss: 0.00001369
Iteration 100/1000 | Loss: 0.00001369
Iteration 101/1000 | Loss: 0.00001369
Iteration 102/1000 | Loss: 0.00001369
Iteration 103/1000 | Loss: 0.00001369
Iteration 104/1000 | Loss: 0.00001369
Iteration 105/1000 | Loss: 0.00001369
Iteration 106/1000 | Loss: 0.00001369
Iteration 107/1000 | Loss: 0.00001368
Iteration 108/1000 | Loss: 0.00001368
Iteration 109/1000 | Loss: 0.00001368
Iteration 110/1000 | Loss: 0.00001368
Iteration 111/1000 | Loss: 0.00001368
Iteration 112/1000 | Loss: 0.00001368
Iteration 113/1000 | Loss: 0.00001368
Iteration 114/1000 | Loss: 0.00001368
Iteration 115/1000 | Loss: 0.00001368
Iteration 116/1000 | Loss: 0.00001368
Iteration 117/1000 | Loss: 0.00001367
Iteration 118/1000 | Loss: 0.00001367
Iteration 119/1000 | Loss: 0.00001367
Iteration 120/1000 | Loss: 0.00001367
Iteration 121/1000 | Loss: 0.00001367
Iteration 122/1000 | Loss: 0.00001367
Iteration 123/1000 | Loss: 0.00001367
Iteration 124/1000 | Loss: 0.00001366
Iteration 125/1000 | Loss: 0.00001366
Iteration 126/1000 | Loss: 0.00001366
Iteration 127/1000 | Loss: 0.00001366
Iteration 128/1000 | Loss: 0.00001366
Iteration 129/1000 | Loss: 0.00001366
Iteration 130/1000 | Loss: 0.00001365
Iteration 131/1000 | Loss: 0.00001365
Iteration 132/1000 | Loss: 0.00001365
Iteration 133/1000 | Loss: 0.00001365
Iteration 134/1000 | Loss: 0.00001364
Iteration 135/1000 | Loss: 0.00001364
Iteration 136/1000 | Loss: 0.00001364
Iteration 137/1000 | Loss: 0.00001364
Iteration 138/1000 | Loss: 0.00001364
Iteration 139/1000 | Loss: 0.00001364
Iteration 140/1000 | Loss: 0.00001364
Iteration 141/1000 | Loss: 0.00001363
Iteration 142/1000 | Loss: 0.00001363
Iteration 143/1000 | Loss: 0.00001363
Iteration 144/1000 | Loss: 0.00001363
Iteration 145/1000 | Loss: 0.00001363
Iteration 146/1000 | Loss: 0.00001363
Iteration 147/1000 | Loss: 0.00001363
Iteration 148/1000 | Loss: 0.00001363
Iteration 149/1000 | Loss: 0.00001363
Iteration 150/1000 | Loss: 0.00001363
Iteration 151/1000 | Loss: 0.00001363
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 151. Stopping optimization.
Last 5 losses: [1.3631930414703675e-05, 1.3631930414703675e-05, 1.3631930414703675e-05, 1.3631930414703675e-05, 1.3631930414703675e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3631930414703675e-05

Optimization complete. Final v2v error: 3.1188971996307373 mm

Highest mean error: 3.611050844192505 mm for frame 34

Lowest mean error: 2.7656774520874023 mm for frame 108

Saving results

Total time: 67.50180220603943
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_006/1062/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_006/1062.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_006/1062
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01107887
Iteration 2/25 | Loss: 0.00246529
Iteration 3/25 | Loss: 0.00155284
Iteration 4/25 | Loss: 0.00192596
Iteration 5/25 | Loss: 0.00114028
Iteration 6/25 | Loss: 0.00106679
Iteration 7/25 | Loss: 0.00089066
Iteration 8/25 | Loss: 0.00082762
Iteration 9/25 | Loss: 0.00081825
Iteration 10/25 | Loss: 0.00081428
Iteration 11/25 | Loss: 0.00081288
Iteration 12/25 | Loss: 0.00081192
Iteration 13/25 | Loss: 0.00081160
Iteration 14/25 | Loss: 0.00081146
Iteration 15/25 | Loss: 0.00081141
Iteration 16/25 | Loss: 0.00081141
Iteration 17/25 | Loss: 0.00081141
Iteration 18/25 | Loss: 0.00081141
Iteration 19/25 | Loss: 0.00081141
Iteration 20/25 | Loss: 0.00081141
Iteration 21/25 | Loss: 0.00081141
Iteration 22/25 | Loss: 0.00081141
Iteration 23/25 | Loss: 0.00081141
Iteration 24/25 | Loss: 0.00081140
Iteration 25/25 | Loss: 0.00081140

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.27777660
Iteration 2/25 | Loss: 0.00022633
Iteration 3/25 | Loss: 0.00022633
Iteration 4/25 | Loss: 0.00022633
Iteration 5/25 | Loss: 0.00022633
Iteration 6/25 | Loss: 0.00022633
Iteration 7/25 | Loss: 0.00022633
Iteration 8/25 | Loss: 0.00022633
Iteration 9/25 | Loss: 0.00022633
Iteration 10/25 | Loss: 0.00022633
Iteration 11/25 | Loss: 0.00022633
Iteration 12/25 | Loss: 0.00022633
Iteration 13/25 | Loss: 0.00022633
Iteration 14/25 | Loss: 0.00022633
Iteration 15/25 | Loss: 0.00022633
Iteration 16/25 | Loss: 0.00022633
Iteration 17/25 | Loss: 0.00022633
Iteration 18/25 | Loss: 0.00022633
Iteration 19/25 | Loss: 0.00022633
Iteration 20/25 | Loss: 0.00022633
Iteration 21/25 | Loss: 0.00022633
Iteration 22/25 | Loss: 0.00022633
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.00022633009939454496, 0.00022633009939454496, 0.00022633009939454496, 0.00022633009939454496, 0.00022633009939454496]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00022633009939454496

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00022633
Iteration 2/1000 | Loss: 0.00003890
Iteration 3/1000 | Loss: 0.00002815
Iteration 4/1000 | Loss: 0.00002583
Iteration 5/1000 | Loss: 0.00002481
Iteration 6/1000 | Loss: 0.00002406
Iteration 7/1000 | Loss: 0.00002361
Iteration 8/1000 | Loss: 0.00002328
Iteration 9/1000 | Loss: 0.00002312
Iteration 10/1000 | Loss: 0.00002312
Iteration 11/1000 | Loss: 0.00002300
Iteration 12/1000 | Loss: 0.00002296
Iteration 13/1000 | Loss: 0.00002293
Iteration 14/1000 | Loss: 0.00002292
Iteration 15/1000 | Loss: 0.00002292
Iteration 16/1000 | Loss: 0.00002292
Iteration 17/1000 | Loss: 0.00002292
Iteration 18/1000 | Loss: 0.00002292
Iteration 19/1000 | Loss: 0.00002291
Iteration 20/1000 | Loss: 0.00002286
Iteration 21/1000 | Loss: 0.00002285
Iteration 22/1000 | Loss: 0.00002281
Iteration 23/1000 | Loss: 0.00002281
Iteration 24/1000 | Loss: 0.00002281
Iteration 25/1000 | Loss: 0.00002281
Iteration 26/1000 | Loss: 0.00002281
Iteration 27/1000 | Loss: 0.00002281
Iteration 28/1000 | Loss: 0.00002280
Iteration 29/1000 | Loss: 0.00002279
Iteration 30/1000 | Loss: 0.00002279
Iteration 31/1000 | Loss: 0.00002278
Iteration 32/1000 | Loss: 0.00002278
Iteration 33/1000 | Loss: 0.00002278
Iteration 34/1000 | Loss: 0.00002277
Iteration 35/1000 | Loss: 0.00002277
Iteration 36/1000 | Loss: 0.00002276
Iteration 37/1000 | Loss: 0.00002276
Iteration 38/1000 | Loss: 0.00002276
Iteration 39/1000 | Loss: 0.00002275
Iteration 40/1000 | Loss: 0.00002275
Iteration 41/1000 | Loss: 0.00002274
Iteration 42/1000 | Loss: 0.00002273
Iteration 43/1000 | Loss: 0.00002273
Iteration 44/1000 | Loss: 0.00002273
Iteration 45/1000 | Loss: 0.00002272
Iteration 46/1000 | Loss: 0.00002272
Iteration 47/1000 | Loss: 0.00002272
Iteration 48/1000 | Loss: 0.00002272
Iteration 49/1000 | Loss: 0.00002272
Iteration 50/1000 | Loss: 0.00002271
Iteration 51/1000 | Loss: 0.00002271
Iteration 52/1000 | Loss: 0.00002271
Iteration 53/1000 | Loss: 0.00002271
Iteration 54/1000 | Loss: 0.00002271
Iteration 55/1000 | Loss: 0.00002271
Iteration 56/1000 | Loss: 0.00002271
Iteration 57/1000 | Loss: 0.00002271
Iteration 58/1000 | Loss: 0.00002270
Iteration 59/1000 | Loss: 0.00002270
Iteration 60/1000 | Loss: 0.00002270
Iteration 61/1000 | Loss: 0.00002270
Iteration 62/1000 | Loss: 0.00002270
Iteration 63/1000 | Loss: 0.00002270
Iteration 64/1000 | Loss: 0.00002269
Iteration 65/1000 | Loss: 0.00002269
Iteration 66/1000 | Loss: 0.00002269
Iteration 67/1000 | Loss: 0.00002269
Iteration 68/1000 | Loss: 0.00002269
Iteration 69/1000 | Loss: 0.00002269
Iteration 70/1000 | Loss: 0.00002269
Iteration 71/1000 | Loss: 0.00002269
Iteration 72/1000 | Loss: 0.00002269
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 72. Stopping optimization.
Last 5 losses: [2.269290416734293e-05, 2.269290416734293e-05, 2.269290416734293e-05, 2.269290416734293e-05, 2.269290416734293e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.269290416734293e-05

Optimization complete. Final v2v error: 3.7737529277801514 mm

Highest mean error: 4.313116550445557 mm for frame 239

Lowest mean error: 3.4014012813568115 mm for frame 17

Saving results

Total time: 50.63822865486145
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_006/1032/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_006/1032.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_006/1032
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00771372
Iteration 2/25 | Loss: 0.00148834
Iteration 3/25 | Loss: 0.00089045
Iteration 4/25 | Loss: 0.00078760
Iteration 5/25 | Loss: 0.00075827
Iteration 6/25 | Loss: 0.00075044
Iteration 7/25 | Loss: 0.00074815
Iteration 8/25 | Loss: 0.00074754
Iteration 9/25 | Loss: 0.00074745
Iteration 10/25 | Loss: 0.00074745
Iteration 11/25 | Loss: 0.00074745
Iteration 12/25 | Loss: 0.00074745
Iteration 13/25 | Loss: 0.00074745
Iteration 14/25 | Loss: 0.00074745
Iteration 15/25 | Loss: 0.00074745
Iteration 16/25 | Loss: 0.00074745
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0007474456797353923, 0.0007474456797353923, 0.0007474456797353923, 0.0007474456797353923, 0.0007474456797353923]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007474456797353923

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.57690752
Iteration 2/25 | Loss: 0.00028283
Iteration 3/25 | Loss: 0.00028282
Iteration 4/25 | Loss: 0.00028282
Iteration 5/25 | Loss: 0.00028282
Iteration 6/25 | Loss: 0.00028282
Iteration 7/25 | Loss: 0.00028282
Iteration 8/25 | Loss: 0.00028282
Iteration 9/25 | Loss: 0.00028282
Iteration 10/25 | Loss: 0.00028282
Iteration 11/25 | Loss: 0.00028282
Iteration 12/25 | Loss: 0.00028282
Iteration 13/25 | Loss: 0.00028282
Iteration 14/25 | Loss: 0.00028282
Iteration 15/25 | Loss: 0.00028282
Iteration 16/25 | Loss: 0.00028282
Iteration 17/25 | Loss: 0.00028282
Iteration 18/25 | Loss: 0.00028282
Iteration 19/25 | Loss: 0.00028282
Iteration 20/25 | Loss: 0.00028282
Iteration 21/25 | Loss: 0.00028282
Iteration 22/25 | Loss: 0.00028282
Iteration 23/25 | Loss: 0.00028282
Iteration 24/25 | Loss: 0.00028282
Iteration 25/25 | Loss: 0.00028282

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00028282
Iteration 2/1000 | Loss: 0.00004563
Iteration 3/1000 | Loss: 0.00003306
Iteration 4/1000 | Loss: 0.00002829
Iteration 5/1000 | Loss: 0.00002610
Iteration 6/1000 | Loss: 0.00002438
Iteration 7/1000 | Loss: 0.00002340
Iteration 8/1000 | Loss: 0.00002251
Iteration 9/1000 | Loss: 0.00002188
Iteration 10/1000 | Loss: 0.00002152
Iteration 11/1000 | Loss: 0.00002114
Iteration 12/1000 | Loss: 0.00002090
Iteration 13/1000 | Loss: 0.00002073
Iteration 14/1000 | Loss: 0.00002060
Iteration 15/1000 | Loss: 0.00002057
Iteration 16/1000 | Loss: 0.00002056
Iteration 17/1000 | Loss: 0.00002056
Iteration 18/1000 | Loss: 0.00002055
Iteration 19/1000 | Loss: 0.00002054
Iteration 20/1000 | Loss: 0.00002053
Iteration 21/1000 | Loss: 0.00002052
Iteration 22/1000 | Loss: 0.00002052
Iteration 23/1000 | Loss: 0.00002052
Iteration 24/1000 | Loss: 0.00002051
Iteration 25/1000 | Loss: 0.00002050
Iteration 26/1000 | Loss: 0.00002050
Iteration 27/1000 | Loss: 0.00002050
Iteration 28/1000 | Loss: 0.00002049
Iteration 29/1000 | Loss: 0.00002049
Iteration 30/1000 | Loss: 0.00002048
Iteration 31/1000 | Loss: 0.00002048
Iteration 32/1000 | Loss: 0.00002048
Iteration 33/1000 | Loss: 0.00002047
Iteration 34/1000 | Loss: 0.00002047
Iteration 35/1000 | Loss: 0.00002047
Iteration 36/1000 | Loss: 0.00002047
Iteration 37/1000 | Loss: 0.00002046
Iteration 38/1000 | Loss: 0.00002046
Iteration 39/1000 | Loss: 0.00002046
Iteration 40/1000 | Loss: 0.00002046
Iteration 41/1000 | Loss: 0.00002046
Iteration 42/1000 | Loss: 0.00002046
Iteration 43/1000 | Loss: 0.00002045
Iteration 44/1000 | Loss: 0.00002045
Iteration 45/1000 | Loss: 0.00002045
Iteration 46/1000 | Loss: 0.00002045
Iteration 47/1000 | Loss: 0.00002044
Iteration 48/1000 | Loss: 0.00002044
Iteration 49/1000 | Loss: 0.00002044
Iteration 50/1000 | Loss: 0.00002043
Iteration 51/1000 | Loss: 0.00002043
Iteration 52/1000 | Loss: 0.00002043
Iteration 53/1000 | Loss: 0.00002043
Iteration 54/1000 | Loss: 0.00002042
Iteration 55/1000 | Loss: 0.00002042
Iteration 56/1000 | Loss: 0.00002042
Iteration 57/1000 | Loss: 0.00002041
Iteration 58/1000 | Loss: 0.00002041
Iteration 59/1000 | Loss: 0.00002041
Iteration 60/1000 | Loss: 0.00002040
Iteration 61/1000 | Loss: 0.00002040
Iteration 62/1000 | Loss: 0.00002040
Iteration 63/1000 | Loss: 0.00002040
Iteration 64/1000 | Loss: 0.00002040
Iteration 65/1000 | Loss: 0.00002039
Iteration 66/1000 | Loss: 0.00002039
Iteration 67/1000 | Loss: 0.00002039
Iteration 68/1000 | Loss: 0.00002038
Iteration 69/1000 | Loss: 0.00002038
Iteration 70/1000 | Loss: 0.00002038
Iteration 71/1000 | Loss: 0.00002038
Iteration 72/1000 | Loss: 0.00002037
Iteration 73/1000 | Loss: 0.00002037
Iteration 74/1000 | Loss: 0.00002037
Iteration 75/1000 | Loss: 0.00002036
Iteration 76/1000 | Loss: 0.00002036
Iteration 77/1000 | Loss: 0.00002036
Iteration 78/1000 | Loss: 0.00002036
Iteration 79/1000 | Loss: 0.00002035
Iteration 80/1000 | Loss: 0.00002035
Iteration 81/1000 | Loss: 0.00002035
Iteration 82/1000 | Loss: 0.00002035
Iteration 83/1000 | Loss: 0.00002035
Iteration 84/1000 | Loss: 0.00002035
Iteration 85/1000 | Loss: 0.00002035
Iteration 86/1000 | Loss: 0.00002035
Iteration 87/1000 | Loss: 0.00002034
Iteration 88/1000 | Loss: 0.00002034
Iteration 89/1000 | Loss: 0.00002034
Iteration 90/1000 | Loss: 0.00002034
Iteration 91/1000 | Loss: 0.00002033
Iteration 92/1000 | Loss: 0.00002033
Iteration 93/1000 | Loss: 0.00002033
Iteration 94/1000 | Loss: 0.00002033
Iteration 95/1000 | Loss: 0.00002033
Iteration 96/1000 | Loss: 0.00002033
Iteration 97/1000 | Loss: 0.00002033
Iteration 98/1000 | Loss: 0.00002033
Iteration 99/1000 | Loss: 0.00002033
Iteration 100/1000 | Loss: 0.00002033
Iteration 101/1000 | Loss: 0.00002033
Iteration 102/1000 | Loss: 0.00002033
Iteration 103/1000 | Loss: 0.00002032
Iteration 104/1000 | Loss: 0.00002032
Iteration 105/1000 | Loss: 0.00002032
Iteration 106/1000 | Loss: 0.00002032
Iteration 107/1000 | Loss: 0.00002032
Iteration 108/1000 | Loss: 0.00002032
Iteration 109/1000 | Loss: 0.00002032
Iteration 110/1000 | Loss: 0.00002032
Iteration 111/1000 | Loss: 0.00002032
Iteration 112/1000 | Loss: 0.00002032
Iteration 113/1000 | Loss: 0.00002032
Iteration 114/1000 | Loss: 0.00002032
Iteration 115/1000 | Loss: 0.00002032
Iteration 116/1000 | Loss: 0.00002032
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 116. Stopping optimization.
Last 5 losses: [2.0315539586590603e-05, 2.0315539586590603e-05, 2.0315539586590603e-05, 2.0315539586590603e-05, 2.0315539586590603e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0315539586590603e-05

Optimization complete. Final v2v error: 3.7189974784851074 mm

Highest mean error: 5.664477825164795 mm for frame 33

Lowest mean error: 2.9368927478790283 mm for frame 1

Saving results

Total time: 40.08183550834656
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_006/1040/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_006/1040.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_006/1040
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01007950
Iteration 2/25 | Loss: 0.00425748
Iteration 3/25 | Loss: 0.00302901
Iteration 4/25 | Loss: 0.00287469
Iteration 5/25 | Loss: 0.00199558
Iteration 6/25 | Loss: 0.00211654
Iteration 7/25 | Loss: 0.00182523
Iteration 8/25 | Loss: 0.00172407
Iteration 9/25 | Loss: 0.00164332
Iteration 10/25 | Loss: 0.00164042
Iteration 11/25 | Loss: 0.00159797
Iteration 12/25 | Loss: 0.00156847
Iteration 13/25 | Loss: 0.00154976
Iteration 14/25 | Loss: 0.00152301
Iteration 15/25 | Loss: 0.00149523
Iteration 16/25 | Loss: 0.00149949
Iteration 17/25 | Loss: 0.00148029
Iteration 18/25 | Loss: 0.00147869
Iteration 19/25 | Loss: 0.00147033
Iteration 20/25 | Loss: 0.00145703
Iteration 21/25 | Loss: 0.00145465
Iteration 22/25 | Loss: 0.00144654
Iteration 23/25 | Loss: 0.00143814
Iteration 24/25 | Loss: 0.00143353
Iteration 25/25 | Loss: 0.00143040

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38458371
Iteration 2/25 | Loss: 0.02383104
Iteration 3/25 | Loss: 0.00453047
Iteration 4/25 | Loss: 0.00453044
Iteration 5/25 | Loss: 0.00453044
Iteration 6/25 | Loss: 0.00453044
Iteration 7/25 | Loss: 0.00453044
Iteration 8/25 | Loss: 0.00453044
Iteration 9/25 | Loss: 0.00453044
Iteration 10/25 | Loss: 0.00453044
Iteration 11/25 | Loss: 0.00453044
Iteration 12/25 | Loss: 0.00453044
Iteration 13/25 | Loss: 0.00453044
Iteration 14/25 | Loss: 0.00453044
Iteration 15/25 | Loss: 0.00453044
Iteration 16/25 | Loss: 0.00453044
Iteration 17/25 | Loss: 0.00453044
Iteration 18/25 | Loss: 0.00453044
Iteration 19/25 | Loss: 0.00453044
Iteration 20/25 | Loss: 0.00453044
Iteration 21/25 | Loss: 0.00453044
Iteration 22/25 | Loss: 0.00453044
Iteration 23/25 | Loss: 0.00453044
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.004530439618974924, 0.004530439618974924, 0.004530439618974924, 0.004530439618974924, 0.004530439618974924]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.004530439618974924

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00453044
Iteration 2/1000 | Loss: 0.01237415
Iteration 3/1000 | Loss: 0.00077205
Iteration 4/1000 | Loss: 0.00116205
Iteration 5/1000 | Loss: 0.00449832
Iteration 6/1000 | Loss: 0.01417915
Iteration 7/1000 | Loss: 0.00106524
Iteration 8/1000 | Loss: 0.00114065
Iteration 9/1000 | Loss: 0.00093553
Iteration 10/1000 | Loss: 0.00112642
Iteration 11/1000 | Loss: 0.00510247
Iteration 12/1000 | Loss: 0.00813860
Iteration 13/1000 | Loss: 0.02469349
Iteration 14/1000 | Loss: 0.00455004
Iteration 15/1000 | Loss: 0.00400340
Iteration 16/1000 | Loss: 0.00304198
Iteration 17/1000 | Loss: 0.00240898
Iteration 18/1000 | Loss: 0.00587163
Iteration 19/1000 | Loss: 0.00819502
Iteration 20/1000 | Loss: 0.00111033
Iteration 21/1000 | Loss: 0.00397084
Iteration 22/1000 | Loss: 0.00020885
Iteration 23/1000 | Loss: 0.00058934
Iteration 24/1000 | Loss: 0.00036360
Iteration 25/1000 | Loss: 0.00243898
Iteration 26/1000 | Loss: 0.00329086
Iteration 27/1000 | Loss: 0.00271092
Iteration 28/1000 | Loss: 0.00009033
Iteration 29/1000 | Loss: 0.00175629
Iteration 30/1000 | Loss: 0.00005748
Iteration 31/1000 | Loss: 0.00005584
Iteration 32/1000 | Loss: 0.00003888
Iteration 33/1000 | Loss: 0.00077849
Iteration 34/1000 | Loss: 0.00036109
Iteration 35/1000 | Loss: 0.00002940
Iteration 36/1000 | Loss: 0.00002557
Iteration 37/1000 | Loss: 0.00044116
Iteration 38/1000 | Loss: 0.00232030
Iteration 39/1000 | Loss: 0.00088604
Iteration 40/1000 | Loss: 0.00034343
Iteration 41/1000 | Loss: 0.00002309
Iteration 42/1000 | Loss: 0.00002083
Iteration 43/1000 | Loss: 0.00001955
Iteration 44/1000 | Loss: 0.00040150
Iteration 45/1000 | Loss: 0.00001888
Iteration 46/1000 | Loss: 0.00021367
Iteration 47/1000 | Loss: 0.00069954
Iteration 48/1000 | Loss: 0.00072837
Iteration 49/1000 | Loss: 0.00033330
Iteration 50/1000 | Loss: 0.00001911
Iteration 51/1000 | Loss: 0.00013170
Iteration 52/1000 | Loss: 0.00009591
Iteration 53/1000 | Loss: 0.00001769
Iteration 54/1000 | Loss: 0.00011148
Iteration 55/1000 | Loss: 0.00001792
Iteration 56/1000 | Loss: 0.00001744
Iteration 57/1000 | Loss: 0.00001718
Iteration 58/1000 | Loss: 0.00001709
Iteration 59/1000 | Loss: 0.00001706
Iteration 60/1000 | Loss: 0.00001699
Iteration 61/1000 | Loss: 0.00001697
Iteration 62/1000 | Loss: 0.00001697
Iteration 63/1000 | Loss: 0.00001696
Iteration 64/1000 | Loss: 0.00001696
Iteration 65/1000 | Loss: 0.00001696
Iteration 66/1000 | Loss: 0.00001695
Iteration 67/1000 | Loss: 0.00001693
Iteration 68/1000 | Loss: 0.00001693
Iteration 69/1000 | Loss: 0.00001693
Iteration 70/1000 | Loss: 0.00001693
Iteration 71/1000 | Loss: 0.00001693
Iteration 72/1000 | Loss: 0.00001693
Iteration 73/1000 | Loss: 0.00001692
Iteration 74/1000 | Loss: 0.00001692
Iteration 75/1000 | Loss: 0.00001692
Iteration 76/1000 | Loss: 0.00001692
Iteration 77/1000 | Loss: 0.00001692
Iteration 78/1000 | Loss: 0.00001692
Iteration 79/1000 | Loss: 0.00001691
Iteration 80/1000 | Loss: 0.00001691
Iteration 81/1000 | Loss: 0.00001691
Iteration 82/1000 | Loss: 0.00001690
Iteration 83/1000 | Loss: 0.00001690
Iteration 84/1000 | Loss: 0.00001690
Iteration 85/1000 | Loss: 0.00001689
Iteration 86/1000 | Loss: 0.00001689
Iteration 87/1000 | Loss: 0.00001689
Iteration 88/1000 | Loss: 0.00001688
Iteration 89/1000 | Loss: 0.00001688
Iteration 90/1000 | Loss: 0.00001688
Iteration 91/1000 | Loss: 0.00001688
Iteration 92/1000 | Loss: 0.00001688
Iteration 93/1000 | Loss: 0.00001688
Iteration 94/1000 | Loss: 0.00001687
Iteration 95/1000 | Loss: 0.00001687
Iteration 96/1000 | Loss: 0.00001687
Iteration 97/1000 | Loss: 0.00001687
Iteration 98/1000 | Loss: 0.00001687
Iteration 99/1000 | Loss: 0.00001687
Iteration 100/1000 | Loss: 0.00001686
Iteration 101/1000 | Loss: 0.00001686
Iteration 102/1000 | Loss: 0.00001686
Iteration 103/1000 | Loss: 0.00001686
Iteration 104/1000 | Loss: 0.00001686
Iteration 105/1000 | Loss: 0.00001686
Iteration 106/1000 | Loss: 0.00001686
Iteration 107/1000 | Loss: 0.00001685
Iteration 108/1000 | Loss: 0.00001685
Iteration 109/1000 | Loss: 0.00001685
Iteration 110/1000 | Loss: 0.00001685
Iteration 111/1000 | Loss: 0.00001685
Iteration 112/1000 | Loss: 0.00001685
Iteration 113/1000 | Loss: 0.00001685
Iteration 114/1000 | Loss: 0.00001685
Iteration 115/1000 | Loss: 0.00001685
Iteration 116/1000 | Loss: 0.00001684
Iteration 117/1000 | Loss: 0.00001684
Iteration 118/1000 | Loss: 0.00001684
Iteration 119/1000 | Loss: 0.00001683
Iteration 120/1000 | Loss: 0.00001683
Iteration 121/1000 | Loss: 0.00001682
Iteration 122/1000 | Loss: 0.00001682
Iteration 123/1000 | Loss: 0.00001682
Iteration 124/1000 | Loss: 0.00001682
Iteration 125/1000 | Loss: 0.00001681
Iteration 126/1000 | Loss: 0.00001680
Iteration 127/1000 | Loss: 0.00001680
Iteration 128/1000 | Loss: 0.00001680
Iteration 129/1000 | Loss: 0.00001680
Iteration 130/1000 | Loss: 0.00001680
Iteration 131/1000 | Loss: 0.00001680
Iteration 132/1000 | Loss: 0.00001680
Iteration 133/1000 | Loss: 0.00001680
Iteration 134/1000 | Loss: 0.00001680
Iteration 135/1000 | Loss: 0.00001680
Iteration 136/1000 | Loss: 0.00001680
Iteration 137/1000 | Loss: 0.00001680
Iteration 138/1000 | Loss: 0.00001680
Iteration 139/1000 | Loss: 0.00001680
Iteration 140/1000 | Loss: 0.00001679
Iteration 141/1000 | Loss: 0.00001679
Iteration 142/1000 | Loss: 0.00001679
Iteration 143/1000 | Loss: 0.00001678
Iteration 144/1000 | Loss: 0.00001678
Iteration 145/1000 | Loss: 0.00001678
Iteration 146/1000 | Loss: 0.00001678
Iteration 147/1000 | Loss: 0.00001677
Iteration 148/1000 | Loss: 0.00001677
Iteration 149/1000 | Loss: 0.00001677
Iteration 150/1000 | Loss: 0.00001677
Iteration 151/1000 | Loss: 0.00001677
Iteration 152/1000 | Loss: 0.00001677
Iteration 153/1000 | Loss: 0.00001676
Iteration 154/1000 | Loss: 0.00001676
Iteration 155/1000 | Loss: 0.00001676
Iteration 156/1000 | Loss: 0.00001676
Iteration 157/1000 | Loss: 0.00001676
Iteration 158/1000 | Loss: 0.00001676
Iteration 159/1000 | Loss: 0.00001675
Iteration 160/1000 | Loss: 0.00001675
Iteration 161/1000 | Loss: 0.00001675
Iteration 162/1000 | Loss: 0.00001675
Iteration 163/1000 | Loss: 0.00001675
Iteration 164/1000 | Loss: 0.00001675
Iteration 165/1000 | Loss: 0.00001675
Iteration 166/1000 | Loss: 0.00001675
Iteration 167/1000 | Loss: 0.00001675
Iteration 168/1000 | Loss: 0.00001675
Iteration 169/1000 | Loss: 0.00001674
Iteration 170/1000 | Loss: 0.00001674
Iteration 171/1000 | Loss: 0.00001674
Iteration 172/1000 | Loss: 0.00001674
Iteration 173/1000 | Loss: 0.00001674
Iteration 174/1000 | Loss: 0.00001674
Iteration 175/1000 | Loss: 0.00001674
Iteration 176/1000 | Loss: 0.00001674
Iteration 177/1000 | Loss: 0.00001674
Iteration 178/1000 | Loss: 0.00001674
Iteration 179/1000 | Loss: 0.00001674
Iteration 180/1000 | Loss: 0.00001674
Iteration 181/1000 | Loss: 0.00001674
Iteration 182/1000 | Loss: 0.00001674
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 182. Stopping optimization.
Last 5 losses: [1.674328450462781e-05, 1.674328450462781e-05, 1.674328450462781e-05, 1.674328450462781e-05, 1.674328450462781e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.674328450462781e-05

Optimization complete. Final v2v error: 3.56349515914917 mm

Highest mean error: 3.8076884746551514 mm for frame 182

Lowest mean error: 3.32065749168396 mm for frame 93

Saving results

Total time: 157.24154591560364
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_006/1006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_006/1006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_006/1006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00677629
Iteration 2/25 | Loss: 0.00086254
Iteration 3/25 | Loss: 0.00065706
Iteration 4/25 | Loss: 0.00062701
Iteration 5/25 | Loss: 0.00061617
Iteration 6/25 | Loss: 0.00061388
Iteration 7/25 | Loss: 0.00061327
Iteration 8/25 | Loss: 0.00061316
Iteration 9/25 | Loss: 0.00061316
Iteration 10/25 | Loss: 0.00061316
Iteration 11/25 | Loss: 0.00061316
Iteration 12/25 | Loss: 0.00061316
Iteration 13/25 | Loss: 0.00061316
Iteration 14/25 | Loss: 0.00061316
Iteration 15/25 | Loss: 0.00061316
Iteration 16/25 | Loss: 0.00061316
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0006131614209152758, 0.0006131614209152758, 0.0006131614209152758, 0.0006131614209152758, 0.0006131614209152758]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006131614209152758

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.55806160
Iteration 2/25 | Loss: 0.00031271
Iteration 3/25 | Loss: 0.00031271
Iteration 4/25 | Loss: 0.00031271
Iteration 5/25 | Loss: 0.00031271
Iteration 6/25 | Loss: 0.00031271
Iteration 7/25 | Loss: 0.00031271
Iteration 8/25 | Loss: 0.00031271
Iteration 9/25 | Loss: 0.00031271
Iteration 10/25 | Loss: 0.00031271
Iteration 11/25 | Loss: 0.00031270
Iteration 12/25 | Loss: 0.00031270
Iteration 13/25 | Loss: 0.00031270
Iteration 14/25 | Loss: 0.00031270
Iteration 15/25 | Loss: 0.00031270
Iteration 16/25 | Loss: 0.00031270
Iteration 17/25 | Loss: 0.00031270
Iteration 18/25 | Loss: 0.00031270
Iteration 19/25 | Loss: 0.00031270
Iteration 20/25 | Loss: 0.00031270
Iteration 21/25 | Loss: 0.00031270
Iteration 22/25 | Loss: 0.00031270
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.00031270485487766564, 0.00031270485487766564, 0.00031270485487766564, 0.00031270485487766564, 0.00031270485487766564]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00031270485487766564

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00031270
Iteration 2/1000 | Loss: 0.00003108
Iteration 3/1000 | Loss: 0.00001952
Iteration 4/1000 | Loss: 0.00001630
Iteration 5/1000 | Loss: 0.00001556
Iteration 6/1000 | Loss: 0.00001504
Iteration 7/1000 | Loss: 0.00001470
Iteration 8/1000 | Loss: 0.00001441
Iteration 9/1000 | Loss: 0.00001433
Iteration 10/1000 | Loss: 0.00001421
Iteration 11/1000 | Loss: 0.00001415
Iteration 12/1000 | Loss: 0.00001409
Iteration 13/1000 | Loss: 0.00001405
Iteration 14/1000 | Loss: 0.00001403
Iteration 15/1000 | Loss: 0.00001403
Iteration 16/1000 | Loss: 0.00001401
Iteration 17/1000 | Loss: 0.00001400
Iteration 18/1000 | Loss: 0.00001400
Iteration 19/1000 | Loss: 0.00001399
Iteration 20/1000 | Loss: 0.00001398
Iteration 21/1000 | Loss: 0.00001394
Iteration 22/1000 | Loss: 0.00001393
Iteration 23/1000 | Loss: 0.00001391
Iteration 24/1000 | Loss: 0.00001387
Iteration 25/1000 | Loss: 0.00001387
Iteration 26/1000 | Loss: 0.00001385
Iteration 27/1000 | Loss: 0.00001381
Iteration 28/1000 | Loss: 0.00001381
Iteration 29/1000 | Loss: 0.00001381
Iteration 30/1000 | Loss: 0.00001381
Iteration 31/1000 | Loss: 0.00001381
Iteration 32/1000 | Loss: 0.00001380
Iteration 33/1000 | Loss: 0.00001380
Iteration 34/1000 | Loss: 0.00001380
Iteration 35/1000 | Loss: 0.00001379
Iteration 36/1000 | Loss: 0.00001378
Iteration 37/1000 | Loss: 0.00001378
Iteration 38/1000 | Loss: 0.00001378
Iteration 39/1000 | Loss: 0.00001377
Iteration 40/1000 | Loss: 0.00001377
Iteration 41/1000 | Loss: 0.00001376
Iteration 42/1000 | Loss: 0.00001376
Iteration 43/1000 | Loss: 0.00001376
Iteration 44/1000 | Loss: 0.00001376
Iteration 45/1000 | Loss: 0.00001375
Iteration 46/1000 | Loss: 0.00001375
Iteration 47/1000 | Loss: 0.00001375
Iteration 48/1000 | Loss: 0.00001374
Iteration 49/1000 | Loss: 0.00001374
Iteration 50/1000 | Loss: 0.00001374
Iteration 51/1000 | Loss: 0.00001374
Iteration 52/1000 | Loss: 0.00001373
Iteration 53/1000 | Loss: 0.00001373
Iteration 54/1000 | Loss: 0.00001373
Iteration 55/1000 | Loss: 0.00001372
Iteration 56/1000 | Loss: 0.00001372
Iteration 57/1000 | Loss: 0.00001372
Iteration 58/1000 | Loss: 0.00001372
Iteration 59/1000 | Loss: 0.00001372
Iteration 60/1000 | Loss: 0.00001371
Iteration 61/1000 | Loss: 0.00001371
Iteration 62/1000 | Loss: 0.00001370
Iteration 63/1000 | Loss: 0.00001370
Iteration 64/1000 | Loss: 0.00001370
Iteration 65/1000 | Loss: 0.00001370
Iteration 66/1000 | Loss: 0.00001370
Iteration 67/1000 | Loss: 0.00001370
Iteration 68/1000 | Loss: 0.00001370
Iteration 69/1000 | Loss: 0.00001369
Iteration 70/1000 | Loss: 0.00001369
Iteration 71/1000 | Loss: 0.00001369
Iteration 72/1000 | Loss: 0.00001369
Iteration 73/1000 | Loss: 0.00001368
Iteration 74/1000 | Loss: 0.00001368
Iteration 75/1000 | Loss: 0.00001368
Iteration 76/1000 | Loss: 0.00001367
Iteration 77/1000 | Loss: 0.00001367
Iteration 78/1000 | Loss: 0.00001367
Iteration 79/1000 | Loss: 0.00001367
Iteration 80/1000 | Loss: 0.00001367
Iteration 81/1000 | Loss: 0.00001367
Iteration 82/1000 | Loss: 0.00001367
Iteration 83/1000 | Loss: 0.00001367
Iteration 84/1000 | Loss: 0.00001366
Iteration 85/1000 | Loss: 0.00001366
Iteration 86/1000 | Loss: 0.00001366
Iteration 87/1000 | Loss: 0.00001366
Iteration 88/1000 | Loss: 0.00001365
Iteration 89/1000 | Loss: 0.00001365
Iteration 90/1000 | Loss: 0.00001365
Iteration 91/1000 | Loss: 0.00001364
Iteration 92/1000 | Loss: 0.00001364
Iteration 93/1000 | Loss: 0.00001364
Iteration 94/1000 | Loss: 0.00001364
Iteration 95/1000 | Loss: 0.00001364
Iteration 96/1000 | Loss: 0.00001364
Iteration 97/1000 | Loss: 0.00001363
Iteration 98/1000 | Loss: 0.00001363
Iteration 99/1000 | Loss: 0.00001363
Iteration 100/1000 | Loss: 0.00001363
Iteration 101/1000 | Loss: 0.00001362
Iteration 102/1000 | Loss: 0.00001362
Iteration 103/1000 | Loss: 0.00001362
Iteration 104/1000 | Loss: 0.00001362
Iteration 105/1000 | Loss: 0.00001362
Iteration 106/1000 | Loss: 0.00001362
Iteration 107/1000 | Loss: 0.00001361
Iteration 108/1000 | Loss: 0.00001361
Iteration 109/1000 | Loss: 0.00001361
Iteration 110/1000 | Loss: 0.00001361
Iteration 111/1000 | Loss: 0.00001361
Iteration 112/1000 | Loss: 0.00001361
Iteration 113/1000 | Loss: 0.00001361
Iteration 114/1000 | Loss: 0.00001361
Iteration 115/1000 | Loss: 0.00001361
Iteration 116/1000 | Loss: 0.00001361
Iteration 117/1000 | Loss: 0.00001361
Iteration 118/1000 | Loss: 0.00001361
Iteration 119/1000 | Loss: 0.00001361
Iteration 120/1000 | Loss: 0.00001361
Iteration 121/1000 | Loss: 0.00001360
Iteration 122/1000 | Loss: 0.00001360
Iteration 123/1000 | Loss: 0.00001360
Iteration 124/1000 | Loss: 0.00001360
Iteration 125/1000 | Loss: 0.00001360
Iteration 126/1000 | Loss: 0.00001360
Iteration 127/1000 | Loss: 0.00001360
Iteration 128/1000 | Loss: 0.00001360
Iteration 129/1000 | Loss: 0.00001360
Iteration 130/1000 | Loss: 0.00001360
Iteration 131/1000 | Loss: 0.00001360
Iteration 132/1000 | Loss: 0.00001360
Iteration 133/1000 | Loss: 0.00001360
Iteration 134/1000 | Loss: 0.00001360
Iteration 135/1000 | Loss: 0.00001360
Iteration 136/1000 | Loss: 0.00001360
Iteration 137/1000 | Loss: 0.00001360
Iteration 138/1000 | Loss: 0.00001360
Iteration 139/1000 | Loss: 0.00001360
Iteration 140/1000 | Loss: 0.00001360
Iteration 141/1000 | Loss: 0.00001360
Iteration 142/1000 | Loss: 0.00001360
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 142. Stopping optimization.
Last 5 losses: [1.3598109035228845e-05, 1.3598109035228845e-05, 1.3598109035228845e-05, 1.3598109035228845e-05, 1.3598109035228845e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3598109035228845e-05

Optimization complete. Final v2v error: 3.1308016777038574 mm

Highest mean error: 3.9762332439422607 mm for frame 73

Lowest mean error: 2.7042880058288574 mm for frame 117

Saving results

Total time: 37.03152680397034
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_006/1077/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_006/1077.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_006/1077
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01078688
Iteration 2/25 | Loss: 0.00133698
Iteration 3/25 | Loss: 0.00085683
Iteration 4/25 | Loss: 0.00081295
Iteration 5/25 | Loss: 0.00080128
Iteration 6/25 | Loss: 0.00078953
Iteration 7/25 | Loss: 0.00076685
Iteration 8/25 | Loss: 0.00078016
Iteration 9/25 | Loss: 0.00076511
Iteration 10/25 | Loss: 0.00077043
Iteration 11/25 | Loss: 0.00076179
Iteration 12/25 | Loss: 0.00076717
Iteration 13/25 | Loss: 0.00076145
Iteration 14/25 | Loss: 0.00076232
Iteration 15/25 | Loss: 0.00076165
Iteration 16/25 | Loss: 0.00077424
Iteration 17/25 | Loss: 0.00076170
Iteration 18/25 | Loss: 0.00076649
Iteration 19/25 | Loss: 0.00076164
Iteration 20/25 | Loss: 0.00076102
Iteration 21/25 | Loss: 0.00076659
Iteration 22/25 | Loss: 0.00076075
Iteration 23/25 | Loss: 0.00076561
Iteration 24/25 | Loss: 0.00076561
Iteration 25/25 | Loss: 0.00076817

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.59022117
Iteration 2/25 | Loss: 0.00036728
Iteration 3/25 | Loss: 0.00036728
Iteration 4/25 | Loss: 0.00036728
Iteration 5/25 | Loss: 0.00036728
Iteration 6/25 | Loss: 0.00036728
Iteration 7/25 | Loss: 0.00036728
Iteration 8/25 | Loss: 0.00036728
Iteration 9/25 | Loss: 0.00036727
Iteration 10/25 | Loss: 0.00036727
Iteration 11/25 | Loss: 0.00036727
Iteration 12/25 | Loss: 0.00036727
Iteration 13/25 | Loss: 0.00036727
Iteration 14/25 | Loss: 0.00036727
Iteration 15/25 | Loss: 0.00036727
Iteration 16/25 | Loss: 0.00036727
Iteration 17/25 | Loss: 0.00036727
Iteration 18/25 | Loss: 0.00036727
Iteration 19/25 | Loss: 0.00036727
Iteration 20/25 | Loss: 0.00036727
Iteration 21/25 | Loss: 0.00036727
Iteration 22/25 | Loss: 0.00036727
Iteration 23/25 | Loss: 0.00036727
Iteration 24/25 | Loss: 0.00036727
Iteration 25/25 | Loss: 0.00036727

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00036727
Iteration 2/1000 | Loss: 0.00003567
Iteration 3/1000 | Loss: 0.00002522
Iteration 4/1000 | Loss: 0.00002385
Iteration 5/1000 | Loss: 0.00002293
Iteration 6/1000 | Loss: 0.00002222
Iteration 7/1000 | Loss: 0.00002169
Iteration 8/1000 | Loss: 0.00002142
Iteration 9/1000 | Loss: 0.00002121
Iteration 10/1000 | Loss: 0.00002120
Iteration 11/1000 | Loss: 0.00002110
Iteration 12/1000 | Loss: 0.00002109
Iteration 13/1000 | Loss: 0.00002097
Iteration 14/1000 | Loss: 0.00002091
Iteration 15/1000 | Loss: 0.00002087
Iteration 16/1000 | Loss: 0.00002087
Iteration 17/1000 | Loss: 0.00002085
Iteration 18/1000 | Loss: 0.00002085
Iteration 19/1000 | Loss: 0.00002085
Iteration 20/1000 | Loss: 0.00002085
Iteration 21/1000 | Loss: 0.00002084
Iteration 22/1000 | Loss: 0.00002084
Iteration 23/1000 | Loss: 0.00002082
Iteration 24/1000 | Loss: 0.00002082
Iteration 25/1000 | Loss: 0.00002082
Iteration 26/1000 | Loss: 0.00002081
Iteration 27/1000 | Loss: 0.00002081
Iteration 28/1000 | Loss: 0.00002080
Iteration 29/1000 | Loss: 0.00002079
Iteration 30/1000 | Loss: 0.00002079
Iteration 31/1000 | Loss: 0.00002079
Iteration 32/1000 | Loss: 0.00002078
Iteration 33/1000 | Loss: 0.00002078
Iteration 34/1000 | Loss: 0.00002077
Iteration 35/1000 | Loss: 0.00002077
Iteration 36/1000 | Loss: 0.00002077
Iteration 37/1000 | Loss: 0.00002077
Iteration 38/1000 | Loss: 0.00002076
Iteration 39/1000 | Loss: 0.00002076
Iteration 40/1000 | Loss: 0.00002076
Iteration 41/1000 | Loss: 0.00002075
Iteration 42/1000 | Loss: 0.00002075
Iteration 43/1000 | Loss: 0.00002075
Iteration 44/1000 | Loss: 0.00002074
Iteration 45/1000 | Loss: 0.00002074
Iteration 46/1000 | Loss: 0.00002074
Iteration 47/1000 | Loss: 0.00002074
Iteration 48/1000 | Loss: 0.00002073
Iteration 49/1000 | Loss: 0.00002073
Iteration 50/1000 | Loss: 0.00002073
Iteration 51/1000 | Loss: 0.00002073
Iteration 52/1000 | Loss: 0.00002073
Iteration 53/1000 | Loss: 0.00002073
Iteration 54/1000 | Loss: 0.00002073
Iteration 55/1000 | Loss: 0.00002072
Iteration 56/1000 | Loss: 0.00002072
Iteration 57/1000 | Loss: 0.00002072
Iteration 58/1000 | Loss: 0.00002072
Iteration 59/1000 | Loss: 0.00002072
Iteration 60/1000 | Loss: 0.00002071
Iteration 61/1000 | Loss: 0.00002071
Iteration 62/1000 | Loss: 0.00002071
Iteration 63/1000 | Loss: 0.00002071
Iteration 64/1000 | Loss: 0.00002071
Iteration 65/1000 | Loss: 0.00002070
Iteration 66/1000 | Loss: 0.00002070
Iteration 67/1000 | Loss: 0.00002070
Iteration 68/1000 | Loss: 0.00002070
Iteration 69/1000 | Loss: 0.00002070
Iteration 70/1000 | Loss: 0.00002070
Iteration 71/1000 | Loss: 0.00002070
Iteration 72/1000 | Loss: 0.00002069
Iteration 73/1000 | Loss: 0.00002069
Iteration 74/1000 | Loss: 0.00002069
Iteration 75/1000 | Loss: 0.00002068
Iteration 76/1000 | Loss: 0.00002068
Iteration 77/1000 | Loss: 0.00002068
Iteration 78/1000 | Loss: 0.00002067
Iteration 79/1000 | Loss: 0.00002067
Iteration 80/1000 | Loss: 0.00002067
Iteration 81/1000 | Loss: 0.00002067
Iteration 82/1000 | Loss: 0.00002067
Iteration 83/1000 | Loss: 0.00002067
Iteration 84/1000 | Loss: 0.00002067
Iteration 85/1000 | Loss: 0.00002067
Iteration 86/1000 | Loss: 0.00002067
Iteration 87/1000 | Loss: 0.00002066
Iteration 88/1000 | Loss: 0.00002066
Iteration 89/1000 | Loss: 0.00002066
Iteration 90/1000 | Loss: 0.00002066
Iteration 91/1000 | Loss: 0.00002066
Iteration 92/1000 | Loss: 0.00002066
Iteration 93/1000 | Loss: 0.00002066
Iteration 94/1000 | Loss: 0.00002066
Iteration 95/1000 | Loss: 0.00002066
Iteration 96/1000 | Loss: 0.00002066
Iteration 97/1000 | Loss: 0.00002066
Iteration 98/1000 | Loss: 0.00002066
Iteration 99/1000 | Loss: 0.00002066
Iteration 100/1000 | Loss: 0.00002066
Iteration 101/1000 | Loss: 0.00002066
Iteration 102/1000 | Loss: 0.00002066
Iteration 103/1000 | Loss: 0.00002066
Iteration 104/1000 | Loss: 0.00002066
Iteration 105/1000 | Loss: 0.00002066
Iteration 106/1000 | Loss: 0.00002066
Iteration 107/1000 | Loss: 0.00002066
Iteration 108/1000 | Loss: 0.00002066
Iteration 109/1000 | Loss: 0.00002066
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 109. Stopping optimization.
Last 5 losses: [2.0662060705944896e-05, 2.0662060705944896e-05, 2.0662060705944896e-05, 2.0662060705944896e-05, 2.0662060705944896e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0662060705944896e-05

Optimization complete. Final v2v error: 3.6752121448516846 mm

Highest mean error: 9.278681755065918 mm for frame 108

Lowest mean error: 3.259349822998047 mm for frame 61

Saving results

Total time: 65.25034046173096
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_006/1073/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_006/1073.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_006/1073
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00945016
Iteration 2/25 | Loss: 0.00095812
Iteration 3/25 | Loss: 0.00078244
Iteration 4/25 | Loss: 0.00072923
Iteration 5/25 | Loss: 0.00071336
Iteration 6/25 | Loss: 0.00071091
Iteration 7/25 | Loss: 0.00071043
Iteration 8/25 | Loss: 0.00071043
Iteration 9/25 | Loss: 0.00071043
Iteration 10/25 | Loss: 0.00071043
Iteration 11/25 | Loss: 0.00071043
Iteration 12/25 | Loss: 0.00071043
Iteration 13/25 | Loss: 0.00071043
Iteration 14/25 | Loss: 0.00071043
Iteration 15/25 | Loss: 0.00071043
Iteration 16/25 | Loss: 0.00071043
Iteration 17/25 | Loss: 0.00071043
Iteration 18/25 | Loss: 0.00071043
Iteration 19/25 | Loss: 0.00071043
Iteration 20/25 | Loss: 0.00071043
Iteration 21/25 | Loss: 0.00071043
Iteration 22/25 | Loss: 0.00071043
Iteration 23/25 | Loss: 0.00071043
Iteration 24/25 | Loss: 0.00071043
Iteration 25/25 | Loss: 0.00071043

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.47591829
Iteration 2/25 | Loss: 0.00043886
Iteration 3/25 | Loss: 0.00043883
Iteration 4/25 | Loss: 0.00043883
Iteration 5/25 | Loss: 0.00043883
Iteration 6/25 | Loss: 0.00043883
Iteration 7/25 | Loss: 0.00043883
Iteration 8/25 | Loss: 0.00043883
Iteration 9/25 | Loss: 0.00043882
Iteration 10/25 | Loss: 0.00043882
Iteration 11/25 | Loss: 0.00043882
Iteration 12/25 | Loss: 0.00043882
Iteration 13/25 | Loss: 0.00043882
Iteration 14/25 | Loss: 0.00043882
Iteration 15/25 | Loss: 0.00043882
Iteration 16/25 | Loss: 0.00043882
Iteration 17/25 | Loss: 0.00043882
Iteration 18/25 | Loss: 0.00043882
Iteration 19/25 | Loss: 0.00043882
Iteration 20/25 | Loss: 0.00043882
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0004388245870359242, 0.0004388245870359242, 0.0004388245870359242, 0.0004388245870359242, 0.0004388245870359242]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0004388245870359242

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00043882
Iteration 2/1000 | Loss: 0.00004031
Iteration 3/1000 | Loss: 0.00002631
Iteration 4/1000 | Loss: 0.00002464
Iteration 5/1000 | Loss: 0.00002387
Iteration 6/1000 | Loss: 0.00002328
Iteration 7/1000 | Loss: 0.00002291
Iteration 8/1000 | Loss: 0.00002256
Iteration 9/1000 | Loss: 0.00002223
Iteration 10/1000 | Loss: 0.00002192
Iteration 11/1000 | Loss: 0.00002170
Iteration 12/1000 | Loss: 0.00002162
Iteration 13/1000 | Loss: 0.00002160
Iteration 14/1000 | Loss: 0.00002160
Iteration 15/1000 | Loss: 0.00002159
Iteration 16/1000 | Loss: 0.00002153
Iteration 17/1000 | Loss: 0.00002150
Iteration 18/1000 | Loss: 0.00002149
Iteration 19/1000 | Loss: 0.00002149
Iteration 20/1000 | Loss: 0.00002148
Iteration 21/1000 | Loss: 0.00002146
Iteration 22/1000 | Loss: 0.00002145
Iteration 23/1000 | Loss: 0.00002145
Iteration 24/1000 | Loss: 0.00002145
Iteration 25/1000 | Loss: 0.00002145
Iteration 26/1000 | Loss: 0.00002145
Iteration 27/1000 | Loss: 0.00002145
Iteration 28/1000 | Loss: 0.00002145
Iteration 29/1000 | Loss: 0.00002143
Iteration 30/1000 | Loss: 0.00002143
Iteration 31/1000 | Loss: 0.00002142
Iteration 32/1000 | Loss: 0.00002142
Iteration 33/1000 | Loss: 0.00002142
Iteration 34/1000 | Loss: 0.00002141
Iteration 35/1000 | Loss: 0.00002141
Iteration 36/1000 | Loss: 0.00002140
Iteration 37/1000 | Loss: 0.00002140
Iteration 38/1000 | Loss: 0.00002139
Iteration 39/1000 | Loss: 0.00002139
Iteration 40/1000 | Loss: 0.00002138
Iteration 41/1000 | Loss: 0.00002138
Iteration 42/1000 | Loss: 0.00002138
Iteration 43/1000 | Loss: 0.00002137
Iteration 44/1000 | Loss: 0.00002137
Iteration 45/1000 | Loss: 0.00002137
Iteration 46/1000 | Loss: 0.00002137
Iteration 47/1000 | Loss: 0.00002137
Iteration 48/1000 | Loss: 0.00002136
Iteration 49/1000 | Loss: 0.00002136
Iteration 50/1000 | Loss: 0.00002136
Iteration 51/1000 | Loss: 0.00002136
Iteration 52/1000 | Loss: 0.00002136
Iteration 53/1000 | Loss: 0.00002135
Iteration 54/1000 | Loss: 0.00002135
Iteration 55/1000 | Loss: 0.00002135
Iteration 56/1000 | Loss: 0.00002135
Iteration 57/1000 | Loss: 0.00002135
Iteration 58/1000 | Loss: 0.00002135
Iteration 59/1000 | Loss: 0.00002134
Iteration 60/1000 | Loss: 0.00002134
Iteration 61/1000 | Loss: 0.00002134
Iteration 62/1000 | Loss: 0.00002134
Iteration 63/1000 | Loss: 0.00002134
Iteration 64/1000 | Loss: 0.00002134
Iteration 65/1000 | Loss: 0.00002134
Iteration 66/1000 | Loss: 0.00002133
Iteration 67/1000 | Loss: 0.00002133
Iteration 68/1000 | Loss: 0.00002133
Iteration 69/1000 | Loss: 0.00002133
Iteration 70/1000 | Loss: 0.00002133
Iteration 71/1000 | Loss: 0.00002133
Iteration 72/1000 | Loss: 0.00002133
Iteration 73/1000 | Loss: 0.00002133
Iteration 74/1000 | Loss: 0.00002133
Iteration 75/1000 | Loss: 0.00002132
Iteration 76/1000 | Loss: 0.00002132
Iteration 77/1000 | Loss: 0.00002132
Iteration 78/1000 | Loss: 0.00002132
Iteration 79/1000 | Loss: 0.00002131
Iteration 80/1000 | Loss: 0.00002131
Iteration 81/1000 | Loss: 0.00002131
Iteration 82/1000 | Loss: 0.00002131
Iteration 83/1000 | Loss: 0.00002131
Iteration 84/1000 | Loss: 0.00002131
Iteration 85/1000 | Loss: 0.00002131
Iteration 86/1000 | Loss: 0.00002130
Iteration 87/1000 | Loss: 0.00002130
Iteration 88/1000 | Loss: 0.00002130
Iteration 89/1000 | Loss: 0.00002130
Iteration 90/1000 | Loss: 0.00002130
Iteration 91/1000 | Loss: 0.00002130
Iteration 92/1000 | Loss: 0.00002130
Iteration 93/1000 | Loss: 0.00002130
Iteration 94/1000 | Loss: 0.00002130
Iteration 95/1000 | Loss: 0.00002130
Iteration 96/1000 | Loss: 0.00002130
Iteration 97/1000 | Loss: 0.00002129
Iteration 98/1000 | Loss: 0.00002129
Iteration 99/1000 | Loss: 0.00002129
Iteration 100/1000 | Loss: 0.00002129
Iteration 101/1000 | Loss: 0.00002129
Iteration 102/1000 | Loss: 0.00002129
Iteration 103/1000 | Loss: 0.00002129
Iteration 104/1000 | Loss: 0.00002129
Iteration 105/1000 | Loss: 0.00002129
Iteration 106/1000 | Loss: 0.00002129
Iteration 107/1000 | Loss: 0.00002129
Iteration 108/1000 | Loss: 0.00002129
Iteration 109/1000 | Loss: 0.00002129
Iteration 110/1000 | Loss: 0.00002129
Iteration 111/1000 | Loss: 0.00002129
Iteration 112/1000 | Loss: 0.00002129
Iteration 113/1000 | Loss: 0.00002129
Iteration 114/1000 | Loss: 0.00002129
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 114. Stopping optimization.
Last 5 losses: [2.129041422449518e-05, 2.129041422449518e-05, 2.129041422449518e-05, 2.129041422449518e-05, 2.129041422449518e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.129041422449518e-05

Optimization complete. Final v2v error: 3.8560657501220703 mm

Highest mean error: 4.214879035949707 mm for frame 164

Lowest mean error: 3.4876632690429688 mm for frame 47

Saving results

Total time: 35.29280471801758
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_006/1094/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_006/1094.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_006/1094
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01040721
Iteration 2/25 | Loss: 0.00272631
Iteration 3/25 | Loss: 0.00168670
Iteration 4/25 | Loss: 0.00147491
Iteration 5/25 | Loss: 0.00158610
Iteration 6/25 | Loss: 0.00121143
Iteration 7/25 | Loss: 0.00105948
Iteration 8/25 | Loss: 0.00097977
Iteration 9/25 | Loss: 0.00092242
Iteration 10/25 | Loss: 0.00087915
Iteration 11/25 | Loss: 0.00085831
Iteration 12/25 | Loss: 0.00084949
Iteration 13/25 | Loss: 0.00084116
Iteration 14/25 | Loss: 0.00083614
Iteration 15/25 | Loss: 0.00082766
Iteration 16/25 | Loss: 0.00082249
Iteration 17/25 | Loss: 0.00082122
Iteration 18/25 | Loss: 0.00081951
Iteration 19/25 | Loss: 0.00083097
Iteration 20/25 | Loss: 0.00082367
Iteration 21/25 | Loss: 0.00081565
Iteration 22/25 | Loss: 0.00082140
Iteration 23/25 | Loss: 0.00081212
Iteration 24/25 | Loss: 0.00080800
Iteration 25/25 | Loss: 0.00080503

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45478201
Iteration 2/25 | Loss: 0.00266207
Iteration 3/25 | Loss: 0.00262451
Iteration 4/25 | Loss: 0.00262451
Iteration 5/25 | Loss: 0.00262450
Iteration 6/25 | Loss: 0.00262450
Iteration 7/25 | Loss: 0.00262450
Iteration 8/25 | Loss: 0.00262450
Iteration 9/25 | Loss: 0.00262450
Iteration 10/25 | Loss: 0.00262450
Iteration 11/25 | Loss: 0.00262450
Iteration 12/25 | Loss: 0.00262450
Iteration 13/25 | Loss: 0.00262450
Iteration 14/25 | Loss: 0.00262450
Iteration 15/25 | Loss: 0.00262450
Iteration 16/25 | Loss: 0.00262450
Iteration 17/25 | Loss: 0.00262450
Iteration 18/25 | Loss: 0.00262450
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0026244998443871737, 0.0026244998443871737, 0.0026244998443871737, 0.0026244998443871737, 0.0026244998443871737]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0026244998443871737

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00262450
Iteration 2/1000 | Loss: 0.00685962
Iteration 3/1000 | Loss: 0.00059952
Iteration 4/1000 | Loss: 0.00036307
Iteration 5/1000 | Loss: 0.00044451
Iteration 6/1000 | Loss: 0.00030496
Iteration 7/1000 | Loss: 0.00020190
Iteration 8/1000 | Loss: 0.00020240
Iteration 9/1000 | Loss: 0.00030626
Iteration 10/1000 | Loss: 0.00026712
Iteration 11/1000 | Loss: 0.00015498
Iteration 12/1000 | Loss: 0.00046962
Iteration 13/1000 | Loss: 0.00019690
Iteration 14/1000 | Loss: 0.00017809
Iteration 15/1000 | Loss: 0.00028774
Iteration 16/1000 | Loss: 0.00031131
Iteration 17/1000 | Loss: 0.00021512
Iteration 18/1000 | Loss: 0.00010506
Iteration 19/1000 | Loss: 0.00008348
Iteration 20/1000 | Loss: 0.00014852
Iteration 21/1000 | Loss: 0.00025763
Iteration 22/1000 | Loss: 0.00015388
Iteration 23/1000 | Loss: 0.00032215
Iteration 24/1000 | Loss: 0.00031693
Iteration 25/1000 | Loss: 0.00017790
Iteration 26/1000 | Loss: 0.00012794
Iteration 27/1000 | Loss: 0.00015065
Iteration 28/1000 | Loss: 0.00016455
Iteration 29/1000 | Loss: 0.00006245
Iteration 30/1000 | Loss: 0.00014551
Iteration 31/1000 | Loss: 0.00022882
Iteration 32/1000 | Loss: 0.00022003
Iteration 33/1000 | Loss: 0.00015693
Iteration 34/1000 | Loss: 0.00028718
Iteration 35/1000 | Loss: 0.00059015
Iteration 36/1000 | Loss: 0.00047897
Iteration 37/1000 | Loss: 0.00019464
Iteration 38/1000 | Loss: 0.00054491
Iteration 39/1000 | Loss: 0.00021668
Iteration 40/1000 | Loss: 0.00004685
Iteration 41/1000 | Loss: 0.00006818
Iteration 42/1000 | Loss: 0.00018222
Iteration 43/1000 | Loss: 0.00033722
Iteration 44/1000 | Loss: 0.00028402
Iteration 45/1000 | Loss: 0.00014021
Iteration 46/1000 | Loss: 0.00021778
Iteration 47/1000 | Loss: 0.00029568
Iteration 48/1000 | Loss: 0.00032225
Iteration 49/1000 | Loss: 0.00017041
Iteration 50/1000 | Loss: 0.00018521
Iteration 51/1000 | Loss: 0.00037179
Iteration 52/1000 | Loss: 0.00043768
Iteration 53/1000 | Loss: 0.00026963
Iteration 54/1000 | Loss: 0.00007252
Iteration 55/1000 | Loss: 0.00008513
Iteration 56/1000 | Loss: 0.00016453
Iteration 57/1000 | Loss: 0.00036121
Iteration 58/1000 | Loss: 0.00037538
Iteration 59/1000 | Loss: 0.00027524
Iteration 60/1000 | Loss: 0.00023973
Iteration 61/1000 | Loss: 0.00014747
Iteration 62/1000 | Loss: 0.00018058
Iteration 63/1000 | Loss: 0.00014845
Iteration 64/1000 | Loss: 0.00022630
Iteration 65/1000 | Loss: 0.00015988
Iteration 66/1000 | Loss: 0.00016838
Iteration 67/1000 | Loss: 0.00014318
Iteration 68/1000 | Loss: 0.00019319
Iteration 69/1000 | Loss: 0.00015930
Iteration 70/1000 | Loss: 0.00019445
Iteration 71/1000 | Loss: 0.00011633
Iteration 72/1000 | Loss: 0.00009487
Iteration 73/1000 | Loss: 0.00016753
Iteration 74/1000 | Loss: 0.00014474
Iteration 75/1000 | Loss: 0.00014758
Iteration 76/1000 | Loss: 0.00019332
Iteration 77/1000 | Loss: 0.00048878
Iteration 78/1000 | Loss: 0.00025693
Iteration 79/1000 | Loss: 0.00045793
Iteration 80/1000 | Loss: 0.00022074
Iteration 81/1000 | Loss: 0.00020720
Iteration 82/1000 | Loss: 0.00024019
Iteration 83/1000 | Loss: 0.00016747
Iteration 84/1000 | Loss: 0.00021334
Iteration 85/1000 | Loss: 0.00016618
Iteration 86/1000 | Loss: 0.00027714
Iteration 87/1000 | Loss: 0.00019552
Iteration 88/1000 | Loss: 0.00026798
Iteration 89/1000 | Loss: 0.00010112
Iteration 90/1000 | Loss: 0.00008388
Iteration 91/1000 | Loss: 0.00006054
Iteration 92/1000 | Loss: 0.00008076
Iteration 93/1000 | Loss: 0.00004577
Iteration 94/1000 | Loss: 0.00004485
Iteration 95/1000 | Loss: 0.00003710
Iteration 96/1000 | Loss: 0.00003316
Iteration 97/1000 | Loss: 0.00004466
Iteration 98/1000 | Loss: 0.00005179
Iteration 99/1000 | Loss: 0.00004615
Iteration 100/1000 | Loss: 0.00005219
Iteration 101/1000 | Loss: 0.00006051
Iteration 102/1000 | Loss: 0.00005166
Iteration 103/1000 | Loss: 0.00005373
Iteration 104/1000 | Loss: 0.00004511
Iteration 105/1000 | Loss: 0.00005042
Iteration 106/1000 | Loss: 0.00004961
Iteration 107/1000 | Loss: 0.00004610
Iteration 108/1000 | Loss: 0.00005297
Iteration 109/1000 | Loss: 0.00005582
Iteration 110/1000 | Loss: 0.00004866
Iteration 111/1000 | Loss: 0.00004744
Iteration 112/1000 | Loss: 0.00004779
Iteration 113/1000 | Loss: 0.00004665
Iteration 114/1000 | Loss: 0.00006021
Iteration 115/1000 | Loss: 0.00006881
Iteration 116/1000 | Loss: 0.00007045
Iteration 117/1000 | Loss: 0.00018060
Iteration 118/1000 | Loss: 0.00025606
Iteration 119/1000 | Loss: 0.00004512
Iteration 120/1000 | Loss: 0.00001803
Iteration 121/1000 | Loss: 0.00001756
Iteration 122/1000 | Loss: 0.00002129
Iteration 123/1000 | Loss: 0.00001650
Iteration 124/1000 | Loss: 0.00002970
Iteration 125/1000 | Loss: 0.00022826
Iteration 126/1000 | Loss: 0.00018762
Iteration 127/1000 | Loss: 0.00023981
Iteration 128/1000 | Loss: 0.00019487
Iteration 129/1000 | Loss: 0.00018234
Iteration 130/1000 | Loss: 0.00017693
Iteration 131/1000 | Loss: 0.00018059
Iteration 132/1000 | Loss: 0.00016573
Iteration 133/1000 | Loss: 0.00014448
Iteration 134/1000 | Loss: 0.00005467
Iteration 135/1000 | Loss: 0.00003219
Iteration 136/1000 | Loss: 0.00004886
Iteration 137/1000 | Loss: 0.00003665
Iteration 138/1000 | Loss: 0.00001866
Iteration 139/1000 | Loss: 0.00003723
Iteration 140/1000 | Loss: 0.00003134
Iteration 141/1000 | Loss: 0.00002889
Iteration 142/1000 | Loss: 0.00003048
Iteration 143/1000 | Loss: 0.00003084
Iteration 144/1000 | Loss: 0.00003993
Iteration 145/1000 | Loss: 0.00003678
Iteration 146/1000 | Loss: 0.00003104
Iteration 147/1000 | Loss: 0.00003142
Iteration 148/1000 | Loss: 0.00002685
Iteration 149/1000 | Loss: 0.00002810
Iteration 150/1000 | Loss: 0.00002582
Iteration 151/1000 | Loss: 0.00002822
Iteration 152/1000 | Loss: 0.00002439
Iteration 153/1000 | Loss: 0.00002464
Iteration 154/1000 | Loss: 0.00002796
Iteration 155/1000 | Loss: 0.00002890
Iteration 156/1000 | Loss: 0.00003089
Iteration 157/1000 | Loss: 0.00002916
Iteration 158/1000 | Loss: 0.00002763
Iteration 159/1000 | Loss: 0.00002675
Iteration 160/1000 | Loss: 0.00003263
Iteration 161/1000 | Loss: 0.00002838
Iteration 162/1000 | Loss: 0.00003905
Iteration 163/1000 | Loss: 0.00012361
Iteration 164/1000 | Loss: 0.00038083
Iteration 165/1000 | Loss: 0.00034790
Iteration 166/1000 | Loss: 0.00019076
Iteration 167/1000 | Loss: 0.00008706
Iteration 168/1000 | Loss: 0.00026222
Iteration 169/1000 | Loss: 0.00002616
Iteration 170/1000 | Loss: 0.00002822
Iteration 171/1000 | Loss: 0.00001957
Iteration 172/1000 | Loss: 0.00002869
Iteration 173/1000 | Loss: 0.00001692
Iteration 174/1000 | Loss: 0.00003273
Iteration 175/1000 | Loss: 0.00002537
Iteration 176/1000 | Loss: 0.00002607
Iteration 177/1000 | Loss: 0.00002567
Iteration 178/1000 | Loss: 0.00002029
Iteration 179/1000 | Loss: 0.00002974
Iteration 180/1000 | Loss: 0.00004225
Iteration 181/1000 | Loss: 0.00001831
Iteration 182/1000 | Loss: 0.00001426
Iteration 183/1000 | Loss: 0.00002136
Iteration 184/1000 | Loss: 0.00001509
Iteration 185/1000 | Loss: 0.00001263
Iteration 186/1000 | Loss: 0.00001278
Iteration 187/1000 | Loss: 0.00001255
Iteration 188/1000 | Loss: 0.00001225
Iteration 189/1000 | Loss: 0.00001225
Iteration 190/1000 | Loss: 0.00001224
Iteration 191/1000 | Loss: 0.00001221
Iteration 192/1000 | Loss: 0.00001219
Iteration 193/1000 | Loss: 0.00001218
Iteration 194/1000 | Loss: 0.00001217
Iteration 195/1000 | Loss: 0.00001217
Iteration 196/1000 | Loss: 0.00001217
Iteration 197/1000 | Loss: 0.00001217
Iteration 198/1000 | Loss: 0.00001217
Iteration 199/1000 | Loss: 0.00001217
Iteration 200/1000 | Loss: 0.00001217
Iteration 201/1000 | Loss: 0.00001217
Iteration 202/1000 | Loss: 0.00001216
Iteration 203/1000 | Loss: 0.00001216
Iteration 204/1000 | Loss: 0.00001216
Iteration 205/1000 | Loss: 0.00001214
Iteration 206/1000 | Loss: 0.00001212
Iteration 207/1000 | Loss: 0.00001212
Iteration 208/1000 | Loss: 0.00001211
Iteration 209/1000 | Loss: 0.00001211
Iteration 210/1000 | Loss: 0.00001211
Iteration 211/1000 | Loss: 0.00001211
Iteration 212/1000 | Loss: 0.00001211
Iteration 213/1000 | Loss: 0.00001210
Iteration 214/1000 | Loss: 0.00001210
Iteration 215/1000 | Loss: 0.00001209
Iteration 216/1000 | Loss: 0.00001209
Iteration 217/1000 | Loss: 0.00001208
Iteration 218/1000 | Loss: 0.00001208
Iteration 219/1000 | Loss: 0.00001208
Iteration 220/1000 | Loss: 0.00001208
Iteration 221/1000 | Loss: 0.00001208
Iteration 222/1000 | Loss: 0.00001207
Iteration 223/1000 | Loss: 0.00001207
Iteration 224/1000 | Loss: 0.00001207
Iteration 225/1000 | Loss: 0.00001207
Iteration 226/1000 | Loss: 0.00001207
Iteration 227/1000 | Loss: 0.00001207
Iteration 228/1000 | Loss: 0.00001207
Iteration 229/1000 | Loss: 0.00001207
Iteration 230/1000 | Loss: 0.00001206
Iteration 231/1000 | Loss: 0.00001206
Iteration 232/1000 | Loss: 0.00001206
Iteration 233/1000 | Loss: 0.00001206
Iteration 234/1000 | Loss: 0.00001205
Iteration 235/1000 | Loss: 0.00001205
Iteration 236/1000 | Loss: 0.00001205
Iteration 237/1000 | Loss: 0.00001205
Iteration 238/1000 | Loss: 0.00001205
Iteration 239/1000 | Loss: 0.00001205
Iteration 240/1000 | Loss: 0.00001205
Iteration 241/1000 | Loss: 0.00001205
Iteration 242/1000 | Loss: 0.00001205
Iteration 243/1000 | Loss: 0.00001205
Iteration 244/1000 | Loss: 0.00001204
Iteration 245/1000 | Loss: 0.00001204
Iteration 246/1000 | Loss: 0.00001203
Iteration 247/1000 | Loss: 0.00001203
Iteration 248/1000 | Loss: 0.00001202
Iteration 249/1000 | Loss: 0.00001202
Iteration 250/1000 | Loss: 0.00001202
Iteration 251/1000 | Loss: 0.00001202
Iteration 252/1000 | Loss: 0.00001202
Iteration 253/1000 | Loss: 0.00001202
Iteration 254/1000 | Loss: 0.00001202
Iteration 255/1000 | Loss: 0.00001202
Iteration 256/1000 | Loss: 0.00001201
Iteration 257/1000 | Loss: 0.00001201
Iteration 258/1000 | Loss: 0.00001201
Iteration 259/1000 | Loss: 0.00001201
Iteration 260/1000 | Loss: 0.00001201
Iteration 261/1000 | Loss: 0.00001201
Iteration 262/1000 | Loss: 0.00001201
Iteration 263/1000 | Loss: 0.00001201
Iteration 264/1000 | Loss: 0.00001200
Iteration 265/1000 | Loss: 0.00001200
Iteration 266/1000 | Loss: 0.00001200
Iteration 267/1000 | Loss: 0.00001200
Iteration 268/1000 | Loss: 0.00001684
Iteration 269/1000 | Loss: 0.00001198
Iteration 270/1000 | Loss: 0.00001197
Iteration 271/1000 | Loss: 0.00001196
Iteration 272/1000 | Loss: 0.00001196
Iteration 273/1000 | Loss: 0.00001196
Iteration 274/1000 | Loss: 0.00001196
Iteration 275/1000 | Loss: 0.00001196
Iteration 276/1000 | Loss: 0.00001195
Iteration 277/1000 | Loss: 0.00001195
Iteration 278/1000 | Loss: 0.00001195
Iteration 279/1000 | Loss: 0.00001195
Iteration 280/1000 | Loss: 0.00001195
Iteration 281/1000 | Loss: 0.00001194
Iteration 282/1000 | Loss: 0.00001194
Iteration 283/1000 | Loss: 0.00001194
Iteration 284/1000 | Loss: 0.00001194
Iteration 285/1000 | Loss: 0.00001194
Iteration 286/1000 | Loss: 0.00001193
Iteration 287/1000 | Loss: 0.00001193
Iteration 288/1000 | Loss: 0.00001193
Iteration 289/1000 | Loss: 0.00001551
Iteration 290/1000 | Loss: 0.00001472
Iteration 291/1000 | Loss: 0.00001190
Iteration 292/1000 | Loss: 0.00001189
Iteration 293/1000 | Loss: 0.00001189
Iteration 294/1000 | Loss: 0.00001189
Iteration 295/1000 | Loss: 0.00001189
Iteration 296/1000 | Loss: 0.00001189
Iteration 297/1000 | Loss: 0.00001188
Iteration 298/1000 | Loss: 0.00001188
Iteration 299/1000 | Loss: 0.00001188
Iteration 300/1000 | Loss: 0.00001188
Iteration 301/1000 | Loss: 0.00001188
Iteration 302/1000 | Loss: 0.00001188
Iteration 303/1000 | Loss: 0.00001188
Iteration 304/1000 | Loss: 0.00001188
Iteration 305/1000 | Loss: 0.00001188
Iteration 306/1000 | Loss: 0.00001188
Iteration 307/1000 | Loss: 0.00001188
Iteration 308/1000 | Loss: 0.00001188
Iteration 309/1000 | Loss: 0.00001188
Iteration 310/1000 | Loss: 0.00001188
Iteration 311/1000 | Loss: 0.00001188
Iteration 312/1000 | Loss: 0.00001188
Iteration 313/1000 | Loss: 0.00001188
Iteration 314/1000 | Loss: 0.00001188
Iteration 315/1000 | Loss: 0.00001188
Iteration 316/1000 | Loss: 0.00001188
Iteration 317/1000 | Loss: 0.00001188
Iteration 318/1000 | Loss: 0.00001188
Iteration 319/1000 | Loss: 0.00001188
Iteration 320/1000 | Loss: 0.00001188
Iteration 321/1000 | Loss: 0.00001188
Iteration 322/1000 | Loss: 0.00001188
Iteration 323/1000 | Loss: 0.00001188
Iteration 324/1000 | Loss: 0.00001188
Iteration 325/1000 | Loss: 0.00001188
Iteration 326/1000 | Loss: 0.00001188
Iteration 327/1000 | Loss: 0.00001188
Iteration 328/1000 | Loss: 0.00001188
Iteration 329/1000 | Loss: 0.00001188
Iteration 330/1000 | Loss: 0.00001188
Iteration 331/1000 | Loss: 0.00001188
Iteration 332/1000 | Loss: 0.00001188
Iteration 333/1000 | Loss: 0.00001188
Iteration 334/1000 | Loss: 0.00001188
Iteration 335/1000 | Loss: 0.00001188
Iteration 336/1000 | Loss: 0.00001188
Iteration 337/1000 | Loss: 0.00001188
Iteration 338/1000 | Loss: 0.00001188
Iteration 339/1000 | Loss: 0.00001188
Iteration 340/1000 | Loss: 0.00001188
Iteration 341/1000 | Loss: 0.00001188
Iteration 342/1000 | Loss: 0.00001188
Iteration 343/1000 | Loss: 0.00001188
Iteration 344/1000 | Loss: 0.00001188
Iteration 345/1000 | Loss: 0.00001188
Iteration 346/1000 | Loss: 0.00001188
Iteration 347/1000 | Loss: 0.00001188
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 347. Stopping optimization.
Last 5 losses: [1.1876917596964631e-05, 1.1876917596964631e-05, 1.1876917596964631e-05, 1.1876917596964631e-05, 1.1876917596964631e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1876917596964631e-05

Optimization complete. Final v2v error: 2.9096908569335938 mm

Highest mean error: 3.951406955718994 mm for frame 187

Lowest mean error: 2.467658758163452 mm for frame 111

Saving results

Total time: 362.1572723388672
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_006/1084/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_006/1084.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_006/1084
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00861222
Iteration 2/25 | Loss: 0.00082852
Iteration 3/25 | Loss: 0.00063532
Iteration 4/25 | Loss: 0.00060679
Iteration 5/25 | Loss: 0.00059977
Iteration 6/25 | Loss: 0.00059743
Iteration 7/25 | Loss: 0.00059717
Iteration 8/25 | Loss: 0.00059717
Iteration 9/25 | Loss: 0.00059717
Iteration 10/25 | Loss: 0.00059717
Iteration 11/25 | Loss: 0.00059717
Iteration 12/25 | Loss: 0.00059717
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0005971707287244499, 0.0005971707287244499, 0.0005971707287244499, 0.0005971707287244499, 0.0005971707287244499]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005971707287244499

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46139646
Iteration 2/25 | Loss: 0.00025253
Iteration 3/25 | Loss: 0.00025251
Iteration 4/25 | Loss: 0.00025251
Iteration 5/25 | Loss: 0.00025250
Iteration 6/25 | Loss: 0.00025250
Iteration 7/25 | Loss: 0.00025250
Iteration 8/25 | Loss: 0.00025250
Iteration 9/25 | Loss: 0.00025250
Iteration 10/25 | Loss: 0.00025250
Iteration 11/25 | Loss: 0.00025250
Iteration 12/25 | Loss: 0.00025250
Iteration 13/25 | Loss: 0.00025250
Iteration 14/25 | Loss: 0.00025250
Iteration 15/25 | Loss: 0.00025250
Iteration 16/25 | Loss: 0.00025250
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0002525038144085556, 0.0002525038144085556, 0.0002525038144085556, 0.0002525038144085556, 0.0002525038144085556]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0002525038144085556

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00025250
Iteration 2/1000 | Loss: 0.00002069
Iteration 3/1000 | Loss: 0.00001597
Iteration 4/1000 | Loss: 0.00001415
Iteration 5/1000 | Loss: 0.00001318
Iteration 6/1000 | Loss: 0.00001257
Iteration 7/1000 | Loss: 0.00001215
Iteration 8/1000 | Loss: 0.00001196
Iteration 9/1000 | Loss: 0.00001192
Iteration 10/1000 | Loss: 0.00001191
Iteration 11/1000 | Loss: 0.00001185
Iteration 12/1000 | Loss: 0.00001174
Iteration 13/1000 | Loss: 0.00001170
Iteration 14/1000 | Loss: 0.00001169
Iteration 15/1000 | Loss: 0.00001168
Iteration 16/1000 | Loss: 0.00001167
Iteration 17/1000 | Loss: 0.00001166
Iteration 18/1000 | Loss: 0.00001166
Iteration 19/1000 | Loss: 0.00001165
Iteration 20/1000 | Loss: 0.00001165
Iteration 21/1000 | Loss: 0.00001162
Iteration 22/1000 | Loss: 0.00001158
Iteration 23/1000 | Loss: 0.00001155
Iteration 24/1000 | Loss: 0.00001154
Iteration 25/1000 | Loss: 0.00001154
Iteration 26/1000 | Loss: 0.00001154
Iteration 27/1000 | Loss: 0.00001153
Iteration 28/1000 | Loss: 0.00001153
Iteration 29/1000 | Loss: 0.00001153
Iteration 30/1000 | Loss: 0.00001153
Iteration 31/1000 | Loss: 0.00001152
Iteration 32/1000 | Loss: 0.00001152
Iteration 33/1000 | Loss: 0.00001152
Iteration 34/1000 | Loss: 0.00001152
Iteration 35/1000 | Loss: 0.00001152
Iteration 36/1000 | Loss: 0.00001152
Iteration 37/1000 | Loss: 0.00001152
Iteration 38/1000 | Loss: 0.00001152
Iteration 39/1000 | Loss: 0.00001152
Iteration 40/1000 | Loss: 0.00001152
Iteration 41/1000 | Loss: 0.00001151
Iteration 42/1000 | Loss: 0.00001151
Iteration 43/1000 | Loss: 0.00001151
Iteration 44/1000 | Loss: 0.00001151
Iteration 45/1000 | Loss: 0.00001151
Iteration 46/1000 | Loss: 0.00001151
Iteration 47/1000 | Loss: 0.00001151
Iteration 48/1000 | Loss: 0.00001151
Iteration 49/1000 | Loss: 0.00001151
Iteration 50/1000 | Loss: 0.00001151
Iteration 51/1000 | Loss: 0.00001150
Iteration 52/1000 | Loss: 0.00001150
Iteration 53/1000 | Loss: 0.00001150
Iteration 54/1000 | Loss: 0.00001150
Iteration 55/1000 | Loss: 0.00001150
Iteration 56/1000 | Loss: 0.00001150
Iteration 57/1000 | Loss: 0.00001150
Iteration 58/1000 | Loss: 0.00001150
Iteration 59/1000 | Loss: 0.00001150
Iteration 60/1000 | Loss: 0.00001150
Iteration 61/1000 | Loss: 0.00001150
Iteration 62/1000 | Loss: 0.00001149
Iteration 63/1000 | Loss: 0.00001149
Iteration 64/1000 | Loss: 0.00001149
Iteration 65/1000 | Loss: 0.00001149
Iteration 66/1000 | Loss: 0.00001149
Iteration 67/1000 | Loss: 0.00001149
Iteration 68/1000 | Loss: 0.00001149
Iteration 69/1000 | Loss: 0.00001149
Iteration 70/1000 | Loss: 0.00001148
Iteration 71/1000 | Loss: 0.00001148
Iteration 72/1000 | Loss: 0.00001148
Iteration 73/1000 | Loss: 0.00001148
Iteration 74/1000 | Loss: 0.00001148
Iteration 75/1000 | Loss: 0.00001148
Iteration 76/1000 | Loss: 0.00001148
Iteration 77/1000 | Loss: 0.00001148
Iteration 78/1000 | Loss: 0.00001148
Iteration 79/1000 | Loss: 0.00001147
Iteration 80/1000 | Loss: 0.00001147
Iteration 81/1000 | Loss: 0.00001147
Iteration 82/1000 | Loss: 0.00001147
Iteration 83/1000 | Loss: 0.00001147
Iteration 84/1000 | Loss: 0.00001147
Iteration 85/1000 | Loss: 0.00001147
Iteration 86/1000 | Loss: 0.00001147
Iteration 87/1000 | Loss: 0.00001147
Iteration 88/1000 | Loss: 0.00001147
Iteration 89/1000 | Loss: 0.00001147
Iteration 90/1000 | Loss: 0.00001147
Iteration 91/1000 | Loss: 0.00001147
Iteration 92/1000 | Loss: 0.00001147
Iteration 93/1000 | Loss: 0.00001146
Iteration 94/1000 | Loss: 0.00001146
Iteration 95/1000 | Loss: 0.00001146
Iteration 96/1000 | Loss: 0.00001146
Iteration 97/1000 | Loss: 0.00001146
Iteration 98/1000 | Loss: 0.00001146
Iteration 99/1000 | Loss: 0.00001146
Iteration 100/1000 | Loss: 0.00001146
Iteration 101/1000 | Loss: 0.00001146
Iteration 102/1000 | Loss: 0.00001146
Iteration 103/1000 | Loss: 0.00001146
Iteration 104/1000 | Loss: 0.00001146
Iteration 105/1000 | Loss: 0.00001145
Iteration 106/1000 | Loss: 0.00001145
Iteration 107/1000 | Loss: 0.00001145
Iteration 108/1000 | Loss: 0.00001145
Iteration 109/1000 | Loss: 0.00001145
Iteration 110/1000 | Loss: 0.00001145
Iteration 111/1000 | Loss: 0.00001145
Iteration 112/1000 | Loss: 0.00001145
Iteration 113/1000 | Loss: 0.00001144
Iteration 114/1000 | Loss: 0.00001144
Iteration 115/1000 | Loss: 0.00001144
Iteration 116/1000 | Loss: 0.00001144
Iteration 117/1000 | Loss: 0.00001144
Iteration 118/1000 | Loss: 0.00001144
Iteration 119/1000 | Loss: 0.00001144
Iteration 120/1000 | Loss: 0.00001144
Iteration 121/1000 | Loss: 0.00001144
Iteration 122/1000 | Loss: 0.00001144
Iteration 123/1000 | Loss: 0.00001144
Iteration 124/1000 | Loss: 0.00001144
Iteration 125/1000 | Loss: 0.00001144
Iteration 126/1000 | Loss: 0.00001144
Iteration 127/1000 | Loss: 0.00001144
Iteration 128/1000 | Loss: 0.00001144
Iteration 129/1000 | Loss: 0.00001144
Iteration 130/1000 | Loss: 0.00001144
Iteration 131/1000 | Loss: 0.00001144
Iteration 132/1000 | Loss: 0.00001144
Iteration 133/1000 | Loss: 0.00001144
Iteration 134/1000 | Loss: 0.00001144
Iteration 135/1000 | Loss: 0.00001143
Iteration 136/1000 | Loss: 0.00001143
Iteration 137/1000 | Loss: 0.00001143
Iteration 138/1000 | Loss: 0.00001143
Iteration 139/1000 | Loss: 0.00001143
Iteration 140/1000 | Loss: 0.00001143
Iteration 141/1000 | Loss: 0.00001143
Iteration 142/1000 | Loss: 0.00001143
Iteration 143/1000 | Loss: 0.00001143
Iteration 144/1000 | Loss: 0.00001143
Iteration 145/1000 | Loss: 0.00001143
Iteration 146/1000 | Loss: 0.00001143
Iteration 147/1000 | Loss: 0.00001143
Iteration 148/1000 | Loss: 0.00001143
Iteration 149/1000 | Loss: 0.00001143
Iteration 150/1000 | Loss: 0.00001143
Iteration 151/1000 | Loss: 0.00001143
Iteration 152/1000 | Loss: 0.00001143
Iteration 153/1000 | Loss: 0.00001143
Iteration 154/1000 | Loss: 0.00001143
Iteration 155/1000 | Loss: 0.00001143
Iteration 156/1000 | Loss: 0.00001143
Iteration 157/1000 | Loss: 0.00001143
Iteration 158/1000 | Loss: 0.00001143
Iteration 159/1000 | Loss: 0.00001143
Iteration 160/1000 | Loss: 0.00001143
Iteration 161/1000 | Loss: 0.00001143
Iteration 162/1000 | Loss: 0.00001143
Iteration 163/1000 | Loss: 0.00001143
Iteration 164/1000 | Loss: 0.00001143
Iteration 165/1000 | Loss: 0.00001143
Iteration 166/1000 | Loss: 0.00001143
Iteration 167/1000 | Loss: 0.00001143
Iteration 168/1000 | Loss: 0.00001143
Iteration 169/1000 | Loss: 0.00001143
Iteration 170/1000 | Loss: 0.00001143
Iteration 171/1000 | Loss: 0.00001143
Iteration 172/1000 | Loss: 0.00001143
Iteration 173/1000 | Loss: 0.00001143
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 173. Stopping optimization.
Last 5 losses: [1.1428673133195844e-05, 1.1428673133195844e-05, 1.1428673133195844e-05, 1.1428673133195844e-05, 1.1428673133195844e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1428673133195844e-05

Optimization complete. Final v2v error: 2.8850045204162598 mm

Highest mean error: 3.086216688156128 mm for frame 3

Lowest mean error: 2.7693357467651367 mm for frame 138

Saving results

Total time: 33.13793444633484
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_006/1004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_006/1004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_006/1004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00863102
Iteration 2/25 | Loss: 0.00113003
Iteration 3/25 | Loss: 0.00089536
Iteration 4/25 | Loss: 0.00079949
Iteration 5/25 | Loss: 0.00078905
Iteration 6/25 | Loss: 0.00078810
Iteration 7/25 | Loss: 0.00078781
Iteration 8/25 | Loss: 0.00078781
Iteration 9/25 | Loss: 0.00078781
Iteration 10/25 | Loss: 0.00078781
Iteration 11/25 | Loss: 0.00078781
Iteration 12/25 | Loss: 0.00078781
Iteration 13/25 | Loss: 0.00078781
Iteration 14/25 | Loss: 0.00078781
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0007878144970163703, 0.0007878144970163703, 0.0007878144970163703, 0.0007878144970163703, 0.0007878144970163703]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007878144970163703

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43004620
Iteration 2/25 | Loss: 0.00034326
Iteration 3/25 | Loss: 0.00034326
Iteration 4/25 | Loss: 0.00034326
Iteration 5/25 | Loss: 0.00034326
Iteration 6/25 | Loss: 0.00034326
Iteration 7/25 | Loss: 0.00034326
Iteration 8/25 | Loss: 0.00034326
Iteration 9/25 | Loss: 0.00034326
Iteration 10/25 | Loss: 0.00034326
Iteration 11/25 | Loss: 0.00034326
Iteration 12/25 | Loss: 0.00034326
Iteration 13/25 | Loss: 0.00034326
Iteration 14/25 | Loss: 0.00034326
Iteration 15/25 | Loss: 0.00034326
Iteration 16/25 | Loss: 0.00034326
Iteration 17/25 | Loss: 0.00034326
Iteration 18/25 | Loss: 0.00034326
Iteration 19/25 | Loss: 0.00034326
Iteration 20/25 | Loss: 0.00034326
Iteration 21/25 | Loss: 0.00034326
Iteration 22/25 | Loss: 0.00034326
Iteration 23/25 | Loss: 0.00034326
Iteration 24/25 | Loss: 0.00034326
Iteration 25/25 | Loss: 0.00034326

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00034326
Iteration 2/1000 | Loss: 0.00004632
Iteration 3/1000 | Loss: 0.00003116
Iteration 4/1000 | Loss: 0.00002816
Iteration 5/1000 | Loss: 0.00002691
Iteration 6/1000 | Loss: 0.00002583
Iteration 7/1000 | Loss: 0.00002509
Iteration 8/1000 | Loss: 0.00002438
Iteration 9/1000 | Loss: 0.00002406
Iteration 10/1000 | Loss: 0.00002382
Iteration 11/1000 | Loss: 0.00002381
Iteration 12/1000 | Loss: 0.00002373
Iteration 13/1000 | Loss: 0.00002369
Iteration 14/1000 | Loss: 0.00002357
Iteration 15/1000 | Loss: 0.00002357
Iteration 16/1000 | Loss: 0.00002355
Iteration 17/1000 | Loss: 0.00002351
Iteration 18/1000 | Loss: 0.00002347
Iteration 19/1000 | Loss: 0.00002346
Iteration 20/1000 | Loss: 0.00002345
Iteration 21/1000 | Loss: 0.00002345
Iteration 22/1000 | Loss: 0.00002345
Iteration 23/1000 | Loss: 0.00002344
Iteration 24/1000 | Loss: 0.00002344
Iteration 25/1000 | Loss: 0.00002344
Iteration 26/1000 | Loss: 0.00002344
Iteration 27/1000 | Loss: 0.00002344
Iteration 28/1000 | Loss: 0.00002344
Iteration 29/1000 | Loss: 0.00002344
Iteration 30/1000 | Loss: 0.00002344
Iteration 31/1000 | Loss: 0.00002344
Iteration 32/1000 | Loss: 0.00002344
Iteration 33/1000 | Loss: 0.00002344
Iteration 34/1000 | Loss: 0.00002343
Iteration 35/1000 | Loss: 0.00002343
Iteration 36/1000 | Loss: 0.00002342
Iteration 37/1000 | Loss: 0.00002341
Iteration 38/1000 | Loss: 0.00002341
Iteration 39/1000 | Loss: 0.00002341
Iteration 40/1000 | Loss: 0.00002340
Iteration 41/1000 | Loss: 0.00002340
Iteration 42/1000 | Loss: 0.00002340
Iteration 43/1000 | Loss: 0.00002339
Iteration 44/1000 | Loss: 0.00002339
Iteration 45/1000 | Loss: 0.00002339
Iteration 46/1000 | Loss: 0.00002338
Iteration 47/1000 | Loss: 0.00002338
Iteration 48/1000 | Loss: 0.00002338
Iteration 49/1000 | Loss: 0.00002338
Iteration 50/1000 | Loss: 0.00002337
Iteration 51/1000 | Loss: 0.00002337
Iteration 52/1000 | Loss: 0.00002336
Iteration 53/1000 | Loss: 0.00002336
Iteration 54/1000 | Loss: 0.00002336
Iteration 55/1000 | Loss: 0.00002336
Iteration 56/1000 | Loss: 0.00002335
Iteration 57/1000 | Loss: 0.00002335
Iteration 58/1000 | Loss: 0.00002335
Iteration 59/1000 | Loss: 0.00002335
Iteration 60/1000 | Loss: 0.00002335
Iteration 61/1000 | Loss: 0.00002334
Iteration 62/1000 | Loss: 0.00002334
Iteration 63/1000 | Loss: 0.00002334
Iteration 64/1000 | Loss: 0.00002334
Iteration 65/1000 | Loss: 0.00002334
Iteration 66/1000 | Loss: 0.00002333
Iteration 67/1000 | Loss: 0.00002333
Iteration 68/1000 | Loss: 0.00002333
Iteration 69/1000 | Loss: 0.00002333
Iteration 70/1000 | Loss: 0.00002332
Iteration 71/1000 | Loss: 0.00002332
Iteration 72/1000 | Loss: 0.00002332
Iteration 73/1000 | Loss: 0.00002332
Iteration 74/1000 | Loss: 0.00002331
Iteration 75/1000 | Loss: 0.00002331
Iteration 76/1000 | Loss: 0.00002331
Iteration 77/1000 | Loss: 0.00002331
Iteration 78/1000 | Loss: 0.00002330
Iteration 79/1000 | Loss: 0.00002330
Iteration 80/1000 | Loss: 0.00002330
Iteration 81/1000 | Loss: 0.00002330
Iteration 82/1000 | Loss: 0.00002330
Iteration 83/1000 | Loss: 0.00002330
Iteration 84/1000 | Loss: 0.00002330
Iteration 85/1000 | Loss: 0.00002330
Iteration 86/1000 | Loss: 0.00002330
Iteration 87/1000 | Loss: 0.00002330
Iteration 88/1000 | Loss: 0.00002330
Iteration 89/1000 | Loss: 0.00002330
Iteration 90/1000 | Loss: 0.00002330
Iteration 91/1000 | Loss: 0.00002329
Iteration 92/1000 | Loss: 0.00002329
Iteration 93/1000 | Loss: 0.00002329
Iteration 94/1000 | Loss: 0.00002329
Iteration 95/1000 | Loss: 0.00002329
Iteration 96/1000 | Loss: 0.00002329
Iteration 97/1000 | Loss: 0.00002329
Iteration 98/1000 | Loss: 0.00002328
Iteration 99/1000 | Loss: 0.00002328
Iteration 100/1000 | Loss: 0.00002328
Iteration 101/1000 | Loss: 0.00002328
Iteration 102/1000 | Loss: 0.00002327
Iteration 103/1000 | Loss: 0.00002327
Iteration 104/1000 | Loss: 0.00002327
Iteration 105/1000 | Loss: 0.00002327
Iteration 106/1000 | Loss: 0.00002326
Iteration 107/1000 | Loss: 0.00002326
Iteration 108/1000 | Loss: 0.00002326
Iteration 109/1000 | Loss: 0.00002326
Iteration 110/1000 | Loss: 0.00002325
Iteration 111/1000 | Loss: 0.00002325
Iteration 112/1000 | Loss: 0.00002325
Iteration 113/1000 | Loss: 0.00002325
Iteration 114/1000 | Loss: 0.00002325
Iteration 115/1000 | Loss: 0.00002325
Iteration 116/1000 | Loss: 0.00002325
Iteration 117/1000 | Loss: 0.00002325
Iteration 118/1000 | Loss: 0.00002325
Iteration 119/1000 | Loss: 0.00002325
Iteration 120/1000 | Loss: 0.00002325
Iteration 121/1000 | Loss: 0.00002325
Iteration 122/1000 | Loss: 0.00002324
Iteration 123/1000 | Loss: 0.00002324
Iteration 124/1000 | Loss: 0.00002324
Iteration 125/1000 | Loss: 0.00002324
Iteration 126/1000 | Loss: 0.00002324
Iteration 127/1000 | Loss: 0.00002324
Iteration 128/1000 | Loss: 0.00002324
Iteration 129/1000 | Loss: 0.00002324
Iteration 130/1000 | Loss: 0.00002324
Iteration 131/1000 | Loss: 0.00002324
Iteration 132/1000 | Loss: 0.00002324
Iteration 133/1000 | Loss: 0.00002324
Iteration 134/1000 | Loss: 0.00002324
Iteration 135/1000 | Loss: 0.00002324
Iteration 136/1000 | Loss: 0.00002324
Iteration 137/1000 | Loss: 0.00002324
Iteration 138/1000 | Loss: 0.00002324
Iteration 139/1000 | Loss: 0.00002324
Iteration 140/1000 | Loss: 0.00002324
Iteration 141/1000 | Loss: 0.00002324
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 141. Stopping optimization.
Last 5 losses: [2.3240896553033963e-05, 2.3240896553033963e-05, 2.3240896553033963e-05, 2.3240896553033963e-05, 2.3240896553033963e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3240896553033963e-05

Optimization complete. Final v2v error: 4.038087368011475 mm

Highest mean error: 4.2936859130859375 mm for frame 144

Lowest mean error: 3.9382214546203613 mm for frame 115

Saving results

Total time: 36.20821022987366
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_006/1086/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_006/1086.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_006/1086
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00420926
Iteration 2/25 | Loss: 0.00101543
Iteration 3/25 | Loss: 0.00069964
Iteration 4/25 | Loss: 0.00064056
Iteration 5/25 | Loss: 0.00063208
Iteration 6/25 | Loss: 0.00062864
Iteration 7/25 | Loss: 0.00062763
Iteration 8/25 | Loss: 0.00062753
Iteration 9/25 | Loss: 0.00062753
Iteration 10/25 | Loss: 0.00062753
Iteration 11/25 | Loss: 0.00062753
Iteration 12/25 | Loss: 0.00062753
Iteration 13/25 | Loss: 0.00062753
Iteration 14/25 | Loss: 0.00062753
Iteration 15/25 | Loss: 0.00062753
Iteration 16/25 | Loss: 0.00062753
Iteration 17/25 | Loss: 0.00062753
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0006275332416407764, 0.0006275332416407764, 0.0006275332416407764, 0.0006275332416407764, 0.0006275332416407764]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006275332416407764

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 7.03168058
Iteration 2/25 | Loss: 0.00029552
Iteration 3/25 | Loss: 0.00029552
Iteration 4/25 | Loss: 0.00029552
Iteration 5/25 | Loss: 0.00029552
Iteration 6/25 | Loss: 0.00029552
Iteration 7/25 | Loss: 0.00029552
Iteration 8/25 | Loss: 0.00029552
Iteration 9/25 | Loss: 0.00029552
Iteration 10/25 | Loss: 0.00029552
Iteration 11/25 | Loss: 0.00029552
Iteration 12/25 | Loss: 0.00029552
Iteration 13/25 | Loss: 0.00029552
Iteration 14/25 | Loss: 0.00029552
Iteration 15/25 | Loss: 0.00029552
Iteration 16/25 | Loss: 0.00029551
Iteration 17/25 | Loss: 0.00029552
Iteration 18/25 | Loss: 0.00029552
Iteration 19/25 | Loss: 0.00029552
Iteration 20/25 | Loss: 0.00029552
Iteration 21/25 | Loss: 0.00029552
Iteration 22/25 | Loss: 0.00029552
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.00029551502666436136, 0.00029551502666436136, 0.00029551502666436136, 0.00029551502666436136, 0.00029551502666436136]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00029551502666436136

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00029551
Iteration 2/1000 | Loss: 0.00002618
Iteration 3/1000 | Loss: 0.00001704
Iteration 4/1000 | Loss: 0.00001550
Iteration 5/1000 | Loss: 0.00001460
Iteration 6/1000 | Loss: 0.00001408
Iteration 7/1000 | Loss: 0.00001373
Iteration 8/1000 | Loss: 0.00001346
Iteration 9/1000 | Loss: 0.00001342
Iteration 10/1000 | Loss: 0.00001334
Iteration 11/1000 | Loss: 0.00001333
Iteration 12/1000 | Loss: 0.00001325
Iteration 13/1000 | Loss: 0.00001322
Iteration 14/1000 | Loss: 0.00001322
Iteration 15/1000 | Loss: 0.00001321
Iteration 16/1000 | Loss: 0.00001321
Iteration 17/1000 | Loss: 0.00001320
Iteration 18/1000 | Loss: 0.00001319
Iteration 19/1000 | Loss: 0.00001312
Iteration 20/1000 | Loss: 0.00001311
Iteration 21/1000 | Loss: 0.00001309
Iteration 22/1000 | Loss: 0.00001308
Iteration 23/1000 | Loss: 0.00001308
Iteration 24/1000 | Loss: 0.00001307
Iteration 25/1000 | Loss: 0.00001307
Iteration 26/1000 | Loss: 0.00001307
Iteration 27/1000 | Loss: 0.00001306
Iteration 28/1000 | Loss: 0.00001306
Iteration 29/1000 | Loss: 0.00001305
Iteration 30/1000 | Loss: 0.00001305
Iteration 31/1000 | Loss: 0.00001304
Iteration 32/1000 | Loss: 0.00001304
Iteration 33/1000 | Loss: 0.00001303
Iteration 34/1000 | Loss: 0.00001303
Iteration 35/1000 | Loss: 0.00001303
Iteration 36/1000 | Loss: 0.00001302
Iteration 37/1000 | Loss: 0.00001302
Iteration 38/1000 | Loss: 0.00001302
Iteration 39/1000 | Loss: 0.00001301
Iteration 40/1000 | Loss: 0.00001300
Iteration 41/1000 | Loss: 0.00001300
Iteration 42/1000 | Loss: 0.00001299
Iteration 43/1000 | Loss: 0.00001298
Iteration 44/1000 | Loss: 0.00001297
Iteration 45/1000 | Loss: 0.00001297
Iteration 46/1000 | Loss: 0.00001295
Iteration 47/1000 | Loss: 0.00001295
Iteration 48/1000 | Loss: 0.00001294
Iteration 49/1000 | Loss: 0.00001293
Iteration 50/1000 | Loss: 0.00001293
Iteration 51/1000 | Loss: 0.00001293
Iteration 52/1000 | Loss: 0.00001292
Iteration 53/1000 | Loss: 0.00001292
Iteration 54/1000 | Loss: 0.00001292
Iteration 55/1000 | Loss: 0.00001292
Iteration 56/1000 | Loss: 0.00001291
Iteration 57/1000 | Loss: 0.00001291
Iteration 58/1000 | Loss: 0.00001291
Iteration 59/1000 | Loss: 0.00001291
Iteration 60/1000 | Loss: 0.00001291
Iteration 61/1000 | Loss: 0.00001291
Iteration 62/1000 | Loss: 0.00001291
Iteration 63/1000 | Loss: 0.00001291
Iteration 64/1000 | Loss: 0.00001291
Iteration 65/1000 | Loss: 0.00001290
Iteration 66/1000 | Loss: 0.00001290
Iteration 67/1000 | Loss: 0.00001290
Iteration 68/1000 | Loss: 0.00001290
Iteration 69/1000 | Loss: 0.00001290
Iteration 70/1000 | Loss: 0.00001290
Iteration 71/1000 | Loss: 0.00001290
Iteration 72/1000 | Loss: 0.00001289
Iteration 73/1000 | Loss: 0.00001289
Iteration 74/1000 | Loss: 0.00001289
Iteration 75/1000 | Loss: 0.00001289
Iteration 76/1000 | Loss: 0.00001289
Iteration 77/1000 | Loss: 0.00001289
Iteration 78/1000 | Loss: 0.00001288
Iteration 79/1000 | Loss: 0.00001288
Iteration 80/1000 | Loss: 0.00001288
Iteration 81/1000 | Loss: 0.00001288
Iteration 82/1000 | Loss: 0.00001288
Iteration 83/1000 | Loss: 0.00001288
Iteration 84/1000 | Loss: 0.00001288
Iteration 85/1000 | Loss: 0.00001288
Iteration 86/1000 | Loss: 0.00001288
Iteration 87/1000 | Loss: 0.00001288
Iteration 88/1000 | Loss: 0.00001288
Iteration 89/1000 | Loss: 0.00001288
Iteration 90/1000 | Loss: 0.00001288
Iteration 91/1000 | Loss: 0.00001287
Iteration 92/1000 | Loss: 0.00001287
Iteration 93/1000 | Loss: 0.00001287
Iteration 94/1000 | Loss: 0.00001287
Iteration 95/1000 | Loss: 0.00001287
Iteration 96/1000 | Loss: 0.00001286
Iteration 97/1000 | Loss: 0.00001286
Iteration 98/1000 | Loss: 0.00001286
Iteration 99/1000 | Loss: 0.00001286
Iteration 100/1000 | Loss: 0.00001286
Iteration 101/1000 | Loss: 0.00001286
Iteration 102/1000 | Loss: 0.00001286
Iteration 103/1000 | Loss: 0.00001286
Iteration 104/1000 | Loss: 0.00001286
Iteration 105/1000 | Loss: 0.00001286
Iteration 106/1000 | Loss: 0.00001286
Iteration 107/1000 | Loss: 0.00001285
Iteration 108/1000 | Loss: 0.00001285
Iteration 109/1000 | Loss: 0.00001285
Iteration 110/1000 | Loss: 0.00001285
Iteration 111/1000 | Loss: 0.00001285
Iteration 112/1000 | Loss: 0.00001285
Iteration 113/1000 | Loss: 0.00001285
Iteration 114/1000 | Loss: 0.00001284
Iteration 115/1000 | Loss: 0.00001284
Iteration 116/1000 | Loss: 0.00001284
Iteration 117/1000 | Loss: 0.00001284
Iteration 118/1000 | Loss: 0.00001284
Iteration 119/1000 | Loss: 0.00001284
Iteration 120/1000 | Loss: 0.00001284
Iteration 121/1000 | Loss: 0.00001283
Iteration 122/1000 | Loss: 0.00001283
Iteration 123/1000 | Loss: 0.00001283
Iteration 124/1000 | Loss: 0.00001283
Iteration 125/1000 | Loss: 0.00001283
Iteration 126/1000 | Loss: 0.00001283
Iteration 127/1000 | Loss: 0.00001282
Iteration 128/1000 | Loss: 0.00001282
Iteration 129/1000 | Loss: 0.00001282
Iteration 130/1000 | Loss: 0.00001282
Iteration 131/1000 | Loss: 0.00001282
Iteration 132/1000 | Loss: 0.00001282
Iteration 133/1000 | Loss: 0.00001282
Iteration 134/1000 | Loss: 0.00001281
Iteration 135/1000 | Loss: 0.00001281
Iteration 136/1000 | Loss: 0.00001281
Iteration 137/1000 | Loss: 0.00001281
Iteration 138/1000 | Loss: 0.00001281
Iteration 139/1000 | Loss: 0.00001281
Iteration 140/1000 | Loss: 0.00001280
Iteration 141/1000 | Loss: 0.00001280
Iteration 142/1000 | Loss: 0.00001280
Iteration 143/1000 | Loss: 0.00001280
Iteration 144/1000 | Loss: 0.00001280
Iteration 145/1000 | Loss: 0.00001280
Iteration 146/1000 | Loss: 0.00001280
Iteration 147/1000 | Loss: 0.00001280
Iteration 148/1000 | Loss: 0.00001280
Iteration 149/1000 | Loss: 0.00001279
Iteration 150/1000 | Loss: 0.00001279
Iteration 151/1000 | Loss: 0.00001279
Iteration 152/1000 | Loss: 0.00001279
Iteration 153/1000 | Loss: 0.00001279
Iteration 154/1000 | Loss: 0.00001279
Iteration 155/1000 | Loss: 0.00001279
Iteration 156/1000 | Loss: 0.00001279
Iteration 157/1000 | Loss: 0.00001279
Iteration 158/1000 | Loss: 0.00001279
Iteration 159/1000 | Loss: 0.00001279
Iteration 160/1000 | Loss: 0.00001279
Iteration 161/1000 | Loss: 0.00001279
Iteration 162/1000 | Loss: 0.00001279
Iteration 163/1000 | Loss: 0.00001279
Iteration 164/1000 | Loss: 0.00001279
Iteration 165/1000 | Loss: 0.00001279
Iteration 166/1000 | Loss: 0.00001279
Iteration 167/1000 | Loss: 0.00001279
Iteration 168/1000 | Loss: 0.00001279
Iteration 169/1000 | Loss: 0.00001279
Iteration 170/1000 | Loss: 0.00001279
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 170. Stopping optimization.
Last 5 losses: [1.279218395211501e-05, 1.279218395211501e-05, 1.279218395211501e-05, 1.279218395211501e-05, 1.279218395211501e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.279218395211501e-05

Optimization complete. Final v2v error: 3.0573337078094482 mm

Highest mean error: 3.6420552730560303 mm for frame 75

Lowest mean error: 2.741075277328491 mm for frame 109

Saving results

Total time: 36.79375123977661
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_006/1066/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_006/1066.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_006/1066
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01000583
Iteration 2/25 | Loss: 0.00159094
Iteration 3/25 | Loss: 0.00100581
Iteration 4/25 | Loss: 0.00093949
Iteration 5/25 | Loss: 0.00093200
Iteration 6/25 | Loss: 0.00093099
Iteration 7/25 | Loss: 0.00093099
Iteration 8/25 | Loss: 0.00093099
Iteration 9/25 | Loss: 0.00093099
Iteration 10/25 | Loss: 0.00093099
Iteration 11/25 | Loss: 0.00093099
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0009309880551882088, 0.0009309880551882088, 0.0009309880551882088, 0.0009309880551882088, 0.0009309880551882088]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009309880551882088

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.44778872
Iteration 2/25 | Loss: 0.00028067
Iteration 3/25 | Loss: 0.00028066
Iteration 4/25 | Loss: 0.00028066
Iteration 5/25 | Loss: 0.00028066
Iteration 6/25 | Loss: 0.00028066
Iteration 7/25 | Loss: 0.00028066
Iteration 8/25 | Loss: 0.00028066
Iteration 9/25 | Loss: 0.00028066
Iteration 10/25 | Loss: 0.00028066
Iteration 11/25 | Loss: 0.00028066
Iteration 12/25 | Loss: 0.00028066
Iteration 13/25 | Loss: 0.00028066
Iteration 14/25 | Loss: 0.00028066
Iteration 15/25 | Loss: 0.00028066
Iteration 16/25 | Loss: 0.00028066
Iteration 17/25 | Loss: 0.00028066
Iteration 18/25 | Loss: 0.00028066
Iteration 19/25 | Loss: 0.00028066
Iteration 20/25 | Loss: 0.00028066
Iteration 21/25 | Loss: 0.00028066
Iteration 22/25 | Loss: 0.00028066
Iteration 23/25 | Loss: 0.00028066
Iteration 24/25 | Loss: 0.00028066
Iteration 25/25 | Loss: 0.00028066

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00028066
Iteration 2/1000 | Loss: 0.00004603
Iteration 3/1000 | Loss: 0.00003100
Iteration 4/1000 | Loss: 0.00002796
Iteration 5/1000 | Loss: 0.00002720
Iteration 6/1000 | Loss: 0.00002649
Iteration 7/1000 | Loss: 0.00002619
Iteration 8/1000 | Loss: 0.00002593
Iteration 9/1000 | Loss: 0.00002582
Iteration 10/1000 | Loss: 0.00002578
Iteration 11/1000 | Loss: 0.00002563
Iteration 12/1000 | Loss: 0.00002553
Iteration 13/1000 | Loss: 0.00002552
Iteration 14/1000 | Loss: 0.00002550
Iteration 15/1000 | Loss: 0.00002550
Iteration 16/1000 | Loss: 0.00002549
Iteration 17/1000 | Loss: 0.00002549
Iteration 18/1000 | Loss: 0.00002549
Iteration 19/1000 | Loss: 0.00002549
Iteration 20/1000 | Loss: 0.00002549
Iteration 21/1000 | Loss: 0.00002549
Iteration 22/1000 | Loss: 0.00002549
Iteration 23/1000 | Loss: 0.00002548
Iteration 24/1000 | Loss: 0.00002547
Iteration 25/1000 | Loss: 0.00002547
Iteration 26/1000 | Loss: 0.00002547
Iteration 27/1000 | Loss: 0.00002547
Iteration 28/1000 | Loss: 0.00002547
Iteration 29/1000 | Loss: 0.00002546
Iteration 30/1000 | Loss: 0.00002546
Iteration 31/1000 | Loss: 0.00002546
Iteration 32/1000 | Loss: 0.00002545
Iteration 33/1000 | Loss: 0.00002545
Iteration 34/1000 | Loss: 0.00002545
Iteration 35/1000 | Loss: 0.00002545
Iteration 36/1000 | Loss: 0.00002545
Iteration 37/1000 | Loss: 0.00002545
Iteration 38/1000 | Loss: 0.00002545
Iteration 39/1000 | Loss: 0.00002545
Iteration 40/1000 | Loss: 0.00002545
Iteration 41/1000 | Loss: 0.00002545
Iteration 42/1000 | Loss: 0.00002544
Iteration 43/1000 | Loss: 0.00002544
Iteration 44/1000 | Loss: 0.00002544
Iteration 45/1000 | Loss: 0.00002544
Iteration 46/1000 | Loss: 0.00002544
Iteration 47/1000 | Loss: 0.00002544
Iteration 48/1000 | Loss: 0.00002544
Iteration 49/1000 | Loss: 0.00002544
Iteration 50/1000 | Loss: 0.00002544
Iteration 51/1000 | Loss: 0.00002544
Iteration 52/1000 | Loss: 0.00002543
Iteration 53/1000 | Loss: 0.00002543
Iteration 54/1000 | Loss: 0.00002543
Iteration 55/1000 | Loss: 0.00002543
Iteration 56/1000 | Loss: 0.00002543
Iteration 57/1000 | Loss: 0.00002543
Iteration 58/1000 | Loss: 0.00002543
Iteration 59/1000 | Loss: 0.00002542
Iteration 60/1000 | Loss: 0.00002542
Iteration 61/1000 | Loss: 0.00002542
Iteration 62/1000 | Loss: 0.00002542
Iteration 63/1000 | Loss: 0.00002542
Iteration 64/1000 | Loss: 0.00002541
Iteration 65/1000 | Loss: 0.00002541
Iteration 66/1000 | Loss: 0.00002541
Iteration 67/1000 | Loss: 0.00002541
Iteration 68/1000 | Loss: 0.00002541
Iteration 69/1000 | Loss: 0.00002541
Iteration 70/1000 | Loss: 0.00002541
Iteration 71/1000 | Loss: 0.00002541
Iteration 72/1000 | Loss: 0.00002541
Iteration 73/1000 | Loss: 0.00002541
Iteration 74/1000 | Loss: 0.00002541
Iteration 75/1000 | Loss: 0.00002540
Iteration 76/1000 | Loss: 0.00002540
Iteration 77/1000 | Loss: 0.00002540
Iteration 78/1000 | Loss: 0.00002540
Iteration 79/1000 | Loss: 0.00002540
Iteration 80/1000 | Loss: 0.00002540
Iteration 81/1000 | Loss: 0.00002540
Iteration 82/1000 | Loss: 0.00002540
Iteration 83/1000 | Loss: 0.00002540
Iteration 84/1000 | Loss: 0.00002539
Iteration 85/1000 | Loss: 0.00002539
Iteration 86/1000 | Loss: 0.00002539
Iteration 87/1000 | Loss: 0.00002539
Iteration 88/1000 | Loss: 0.00002539
Iteration 89/1000 | Loss: 0.00002539
Iteration 90/1000 | Loss: 0.00002539
Iteration 91/1000 | Loss: 0.00002539
Iteration 92/1000 | Loss: 0.00002539
Iteration 93/1000 | Loss: 0.00002539
Iteration 94/1000 | Loss: 0.00002539
Iteration 95/1000 | Loss: 0.00002539
Iteration 96/1000 | Loss: 0.00002539
Iteration 97/1000 | Loss: 0.00002539
Iteration 98/1000 | Loss: 0.00002538
Iteration 99/1000 | Loss: 0.00002538
Iteration 100/1000 | Loss: 0.00002538
Iteration 101/1000 | Loss: 0.00002538
Iteration 102/1000 | Loss: 0.00002537
Iteration 103/1000 | Loss: 0.00002537
Iteration 104/1000 | Loss: 0.00002537
Iteration 105/1000 | Loss: 0.00002537
Iteration 106/1000 | Loss: 0.00002537
Iteration 107/1000 | Loss: 0.00002537
Iteration 108/1000 | Loss: 0.00002536
Iteration 109/1000 | Loss: 0.00002536
Iteration 110/1000 | Loss: 0.00002536
Iteration 111/1000 | Loss: 0.00002536
Iteration 112/1000 | Loss: 0.00002536
Iteration 113/1000 | Loss: 0.00002536
Iteration 114/1000 | Loss: 0.00002536
Iteration 115/1000 | Loss: 0.00002536
Iteration 116/1000 | Loss: 0.00002536
Iteration 117/1000 | Loss: 0.00002536
Iteration 118/1000 | Loss: 0.00002536
Iteration 119/1000 | Loss: 0.00002536
Iteration 120/1000 | Loss: 0.00002535
Iteration 121/1000 | Loss: 0.00002535
Iteration 122/1000 | Loss: 0.00002535
Iteration 123/1000 | Loss: 0.00002535
Iteration 124/1000 | Loss: 0.00002535
Iteration 125/1000 | Loss: 0.00002535
Iteration 126/1000 | Loss: 0.00002535
Iteration 127/1000 | Loss: 0.00002535
Iteration 128/1000 | Loss: 0.00002535
Iteration 129/1000 | Loss: 0.00002535
Iteration 130/1000 | Loss: 0.00002535
Iteration 131/1000 | Loss: 0.00002535
Iteration 132/1000 | Loss: 0.00002535
Iteration 133/1000 | Loss: 0.00002535
Iteration 134/1000 | Loss: 0.00002534
Iteration 135/1000 | Loss: 0.00002534
Iteration 136/1000 | Loss: 0.00002534
Iteration 137/1000 | Loss: 0.00002534
Iteration 138/1000 | Loss: 0.00002534
Iteration 139/1000 | Loss: 0.00002534
Iteration 140/1000 | Loss: 0.00002534
Iteration 141/1000 | Loss: 0.00002534
Iteration 142/1000 | Loss: 0.00002534
Iteration 143/1000 | Loss: 0.00002534
Iteration 144/1000 | Loss: 0.00002533
Iteration 145/1000 | Loss: 0.00002533
Iteration 146/1000 | Loss: 0.00002533
Iteration 147/1000 | Loss: 0.00002533
Iteration 148/1000 | Loss: 0.00002533
Iteration 149/1000 | Loss: 0.00002533
Iteration 150/1000 | Loss: 0.00002533
Iteration 151/1000 | Loss: 0.00002533
Iteration 152/1000 | Loss: 0.00002533
Iteration 153/1000 | Loss: 0.00002533
Iteration 154/1000 | Loss: 0.00002533
Iteration 155/1000 | Loss: 0.00002533
Iteration 156/1000 | Loss: 0.00002533
Iteration 157/1000 | Loss: 0.00002533
Iteration 158/1000 | Loss: 0.00002533
Iteration 159/1000 | Loss: 0.00002533
Iteration 160/1000 | Loss: 0.00002533
Iteration 161/1000 | Loss: 0.00002533
Iteration 162/1000 | Loss: 0.00002533
Iteration 163/1000 | Loss: 0.00002533
Iteration 164/1000 | Loss: 0.00002533
Iteration 165/1000 | Loss: 0.00002533
Iteration 166/1000 | Loss: 0.00002532
Iteration 167/1000 | Loss: 0.00002532
Iteration 168/1000 | Loss: 0.00002532
Iteration 169/1000 | Loss: 0.00002532
Iteration 170/1000 | Loss: 0.00002532
Iteration 171/1000 | Loss: 0.00002532
Iteration 172/1000 | Loss: 0.00002532
Iteration 173/1000 | Loss: 0.00002531
Iteration 174/1000 | Loss: 0.00002531
Iteration 175/1000 | Loss: 0.00002531
Iteration 176/1000 | Loss: 0.00002531
Iteration 177/1000 | Loss: 0.00002530
Iteration 178/1000 | Loss: 0.00002530
Iteration 179/1000 | Loss: 0.00002530
Iteration 180/1000 | Loss: 0.00002530
Iteration 181/1000 | Loss: 0.00002530
Iteration 182/1000 | Loss: 0.00002530
Iteration 183/1000 | Loss: 0.00002529
Iteration 184/1000 | Loss: 0.00002529
Iteration 185/1000 | Loss: 0.00002529
Iteration 186/1000 | Loss: 0.00002529
Iteration 187/1000 | Loss: 0.00002529
Iteration 188/1000 | Loss: 0.00002529
Iteration 189/1000 | Loss: 0.00002529
Iteration 190/1000 | Loss: 0.00002529
Iteration 191/1000 | Loss: 0.00002529
Iteration 192/1000 | Loss: 0.00002529
Iteration 193/1000 | Loss: 0.00002529
Iteration 194/1000 | Loss: 0.00002529
Iteration 195/1000 | Loss: 0.00002529
Iteration 196/1000 | Loss: 0.00002529
Iteration 197/1000 | Loss: 0.00002529
Iteration 198/1000 | Loss: 0.00002529
Iteration 199/1000 | Loss: 0.00002529
Iteration 200/1000 | Loss: 0.00002529
Iteration 201/1000 | Loss: 0.00002529
Iteration 202/1000 | Loss: 0.00002529
Iteration 203/1000 | Loss: 0.00002529
Iteration 204/1000 | Loss: 0.00002529
Iteration 205/1000 | Loss: 0.00002529
Iteration 206/1000 | Loss: 0.00002529
Iteration 207/1000 | Loss: 0.00002529
Iteration 208/1000 | Loss: 0.00002529
Iteration 209/1000 | Loss: 0.00002529
Iteration 210/1000 | Loss: 0.00002529
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 210. Stopping optimization.
Last 5 losses: [2.5286783056799322e-05, 2.5286783056799322e-05, 2.5286783056799322e-05, 2.5286783056799322e-05, 2.5286783056799322e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.5286783056799322e-05

Optimization complete. Final v2v error: 4.090481281280518 mm

Highest mean error: 4.881306171417236 mm for frame 0

Lowest mean error: 3.7591958045959473 mm for frame 45

Saving results

Total time: 36.68059301376343
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_006/1005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_006/1005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_006/1005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01119421
Iteration 2/25 | Loss: 0.00254030
Iteration 3/25 | Loss: 0.00188340
Iteration 4/25 | Loss: 0.00168090
Iteration 5/25 | Loss: 0.00135090
Iteration 6/25 | Loss: 0.00111530
Iteration 7/25 | Loss: 0.00103206
Iteration 8/25 | Loss: 0.00100461
Iteration 9/25 | Loss: 0.00097811
Iteration 10/25 | Loss: 0.00095505
Iteration 11/25 | Loss: 0.00094197
Iteration 12/25 | Loss: 0.00093123
Iteration 13/25 | Loss: 0.00092934
Iteration 14/25 | Loss: 0.00092860
Iteration 15/25 | Loss: 0.00091336
Iteration 16/25 | Loss: 0.00089462
Iteration 17/25 | Loss: 0.00088832
Iteration 18/25 | Loss: 0.00089003
Iteration 19/25 | Loss: 0.00088396
Iteration 20/25 | Loss: 0.00087500
Iteration 21/25 | Loss: 0.00087789
Iteration 22/25 | Loss: 0.00087394
Iteration 23/25 | Loss: 0.00086494
Iteration 24/25 | Loss: 0.00085786
Iteration 25/25 | Loss: 0.00084956

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39538670
Iteration 2/25 | Loss: 0.00675268
Iteration 3/25 | Loss: 0.00187018
Iteration 4/25 | Loss: 0.00187018
Iteration 5/25 | Loss: 0.00187018
Iteration 6/25 | Loss: 0.00187018
Iteration 7/25 | Loss: 0.00187018
Iteration 8/25 | Loss: 0.00187018
Iteration 9/25 | Loss: 0.00187018
Iteration 10/25 | Loss: 0.00187018
Iteration 11/25 | Loss: 0.00187018
Iteration 12/25 | Loss: 0.00187018
Iteration 13/25 | Loss: 0.00187018
Iteration 14/25 | Loss: 0.00187018
Iteration 15/25 | Loss: 0.00187018
Iteration 16/25 | Loss: 0.00187018
Iteration 17/25 | Loss: 0.00187018
Iteration 18/25 | Loss: 0.00187018
Iteration 19/25 | Loss: 0.00187018
Iteration 20/25 | Loss: 0.00187018
Iteration 21/25 | Loss: 0.00187018
Iteration 22/25 | Loss: 0.00187018
Iteration 23/25 | Loss: 0.00187018
Iteration 24/25 | Loss: 0.00187018
Iteration 25/25 | Loss: 0.00187018

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00187018
Iteration 2/1000 | Loss: 0.00503543
Iteration 3/1000 | Loss: 0.00081727
Iteration 4/1000 | Loss: 0.00067690
Iteration 5/1000 | Loss: 0.00068208
Iteration 6/1000 | Loss: 0.00059067
Iteration 7/1000 | Loss: 0.00065890
Iteration 8/1000 | Loss: 0.00072895
Iteration 9/1000 | Loss: 0.00113584
Iteration 10/1000 | Loss: 0.00142059
Iteration 11/1000 | Loss: 0.00093866
Iteration 12/1000 | Loss: 0.00085418
Iteration 13/1000 | Loss: 0.00109205
Iteration 14/1000 | Loss: 0.00077102
Iteration 15/1000 | Loss: 0.00066388
Iteration 16/1000 | Loss: 0.00061617
Iteration 17/1000 | Loss: 0.00068375
Iteration 18/1000 | Loss: 0.00099173
Iteration 19/1000 | Loss: 0.00086698
Iteration 20/1000 | Loss: 0.00060659
Iteration 21/1000 | Loss: 0.00087022
Iteration 22/1000 | Loss: 0.00137824
Iteration 23/1000 | Loss: 0.00077719
Iteration 24/1000 | Loss: 0.00076734
Iteration 25/1000 | Loss: 0.00045964
Iteration 26/1000 | Loss: 0.00042320
Iteration 27/1000 | Loss: 0.00042206
Iteration 28/1000 | Loss: 0.00090379
Iteration 29/1000 | Loss: 0.00039794
Iteration 30/1000 | Loss: 0.00025575
Iteration 31/1000 | Loss: 0.00049855
Iteration 32/1000 | Loss: 0.00040457
Iteration 33/1000 | Loss: 0.00083469
Iteration 34/1000 | Loss: 0.00057644
Iteration 35/1000 | Loss: 0.00065369
Iteration 36/1000 | Loss: 0.00020371
Iteration 37/1000 | Loss: 0.00067652
Iteration 38/1000 | Loss: 0.00021986
Iteration 39/1000 | Loss: 0.00045711
Iteration 40/1000 | Loss: 0.00107943
Iteration 41/1000 | Loss: 0.00061526
Iteration 42/1000 | Loss: 0.00027224
Iteration 43/1000 | Loss: 0.00051597
Iteration 44/1000 | Loss: 0.00064244
Iteration 45/1000 | Loss: 0.00112072
Iteration 46/1000 | Loss: 0.00047732
Iteration 47/1000 | Loss: 0.00071138
Iteration 48/1000 | Loss: 0.00118709
Iteration 49/1000 | Loss: 0.00081272
Iteration 50/1000 | Loss: 0.00051149
Iteration 51/1000 | Loss: 0.00071929
Iteration 52/1000 | Loss: 0.00043491
Iteration 53/1000 | Loss: 0.00065057
Iteration 54/1000 | Loss: 0.00069019
Iteration 55/1000 | Loss: 0.00052913
Iteration 56/1000 | Loss: 0.00056485
Iteration 57/1000 | Loss: 0.00061255
Iteration 58/1000 | Loss: 0.00062596
Iteration 59/1000 | Loss: 0.00023792
Iteration 60/1000 | Loss: 0.00023542
Iteration 61/1000 | Loss: 0.00015668
Iteration 62/1000 | Loss: 0.00014943
Iteration 63/1000 | Loss: 0.00020776
Iteration 64/1000 | Loss: 0.00020499
Iteration 65/1000 | Loss: 0.00037839
Iteration 66/1000 | Loss: 0.00022152
Iteration 67/1000 | Loss: 0.00096947
Iteration 68/1000 | Loss: 0.00032150
Iteration 69/1000 | Loss: 0.00012854
Iteration 70/1000 | Loss: 0.00017643
Iteration 71/1000 | Loss: 0.00007667
Iteration 72/1000 | Loss: 0.00007387
Iteration 73/1000 | Loss: 0.00022695
Iteration 74/1000 | Loss: 0.00007387
Iteration 75/1000 | Loss: 0.00015014
Iteration 76/1000 | Loss: 0.00018246
Iteration 77/1000 | Loss: 0.00006695
Iteration 78/1000 | Loss: 0.00006235
Iteration 79/1000 | Loss: 0.00018858
Iteration 80/1000 | Loss: 0.00006418
Iteration 81/1000 | Loss: 0.00008833
Iteration 82/1000 | Loss: 0.00005993
Iteration 83/1000 | Loss: 0.00005913
Iteration 84/1000 | Loss: 0.00017727
Iteration 85/1000 | Loss: 0.00011641
Iteration 86/1000 | Loss: 0.00010357
Iteration 87/1000 | Loss: 0.00005963
Iteration 88/1000 | Loss: 0.00005733
Iteration 89/1000 | Loss: 0.00005579
Iteration 90/1000 | Loss: 0.00011278
Iteration 91/1000 | Loss: 0.00008011
Iteration 92/1000 | Loss: 0.00048378
Iteration 93/1000 | Loss: 0.00040503
Iteration 94/1000 | Loss: 0.00043361
Iteration 95/1000 | Loss: 0.00043123
Iteration 96/1000 | Loss: 0.00080905
Iteration 97/1000 | Loss: 0.00044116
Iteration 98/1000 | Loss: 0.00081820
Iteration 99/1000 | Loss: 0.00081257
Iteration 100/1000 | Loss: 0.00005128
Iteration 101/1000 | Loss: 0.00004274
Iteration 102/1000 | Loss: 0.00041956
Iteration 103/1000 | Loss: 0.00003554
Iteration 104/1000 | Loss: 0.00040999
Iteration 105/1000 | Loss: 0.00003163
Iteration 106/1000 | Loss: 0.00040398
Iteration 107/1000 | Loss: 0.00003277
Iteration 108/1000 | Loss: 0.00002636
Iteration 109/1000 | Loss: 0.00002372
Iteration 110/1000 | Loss: 0.00035684
Iteration 111/1000 | Loss: 0.00002408
Iteration 112/1000 | Loss: 0.00002073
Iteration 113/1000 | Loss: 0.00001959
Iteration 114/1000 | Loss: 0.00001824
Iteration 115/1000 | Loss: 0.00001768
Iteration 116/1000 | Loss: 0.00001736
Iteration 117/1000 | Loss: 0.00001712
Iteration 118/1000 | Loss: 0.00001688
Iteration 119/1000 | Loss: 0.00001686
Iteration 120/1000 | Loss: 0.00001683
Iteration 121/1000 | Loss: 0.00001672
Iteration 122/1000 | Loss: 0.00001671
Iteration 123/1000 | Loss: 0.00001670
Iteration 124/1000 | Loss: 0.00001667
Iteration 125/1000 | Loss: 0.00001666
Iteration 126/1000 | Loss: 0.00001663
Iteration 127/1000 | Loss: 0.00001662
Iteration 128/1000 | Loss: 0.00001662
Iteration 129/1000 | Loss: 0.00001661
Iteration 130/1000 | Loss: 0.00001660
Iteration 131/1000 | Loss: 0.00001659
Iteration 132/1000 | Loss: 0.00001659
Iteration 133/1000 | Loss: 0.00001659
Iteration 134/1000 | Loss: 0.00001658
Iteration 135/1000 | Loss: 0.00001658
Iteration 136/1000 | Loss: 0.00001657
Iteration 137/1000 | Loss: 0.00001657
Iteration 138/1000 | Loss: 0.00001657
Iteration 139/1000 | Loss: 0.00001656
Iteration 140/1000 | Loss: 0.00001656
Iteration 141/1000 | Loss: 0.00001655
Iteration 142/1000 | Loss: 0.00001655
Iteration 143/1000 | Loss: 0.00001655
Iteration 144/1000 | Loss: 0.00001655
Iteration 145/1000 | Loss: 0.00001654
Iteration 146/1000 | Loss: 0.00001654
Iteration 147/1000 | Loss: 0.00001654
Iteration 148/1000 | Loss: 0.00001653
Iteration 149/1000 | Loss: 0.00001653
Iteration 150/1000 | Loss: 0.00001652
Iteration 151/1000 | Loss: 0.00001651
Iteration 152/1000 | Loss: 0.00001651
Iteration 153/1000 | Loss: 0.00001649
Iteration 154/1000 | Loss: 0.00001649
Iteration 155/1000 | Loss: 0.00001649
Iteration 156/1000 | Loss: 0.00001648
Iteration 157/1000 | Loss: 0.00001648
Iteration 158/1000 | Loss: 0.00001647
Iteration 159/1000 | Loss: 0.00001647
Iteration 160/1000 | Loss: 0.00001647
Iteration 161/1000 | Loss: 0.00001646
Iteration 162/1000 | Loss: 0.00001646
Iteration 163/1000 | Loss: 0.00001646
Iteration 164/1000 | Loss: 0.00001646
Iteration 165/1000 | Loss: 0.00001646
Iteration 166/1000 | Loss: 0.00001646
Iteration 167/1000 | Loss: 0.00001646
Iteration 168/1000 | Loss: 0.00001645
Iteration 169/1000 | Loss: 0.00001645
Iteration 170/1000 | Loss: 0.00001644
Iteration 171/1000 | Loss: 0.00001644
Iteration 172/1000 | Loss: 0.00001644
Iteration 173/1000 | Loss: 0.00001644
Iteration 174/1000 | Loss: 0.00001643
Iteration 175/1000 | Loss: 0.00001643
Iteration 176/1000 | Loss: 0.00001643
Iteration 177/1000 | Loss: 0.00001643
Iteration 178/1000 | Loss: 0.00001643
Iteration 179/1000 | Loss: 0.00001643
Iteration 180/1000 | Loss: 0.00001643
Iteration 181/1000 | Loss: 0.00001643
Iteration 182/1000 | Loss: 0.00001643
Iteration 183/1000 | Loss: 0.00001643
Iteration 184/1000 | Loss: 0.00001642
Iteration 185/1000 | Loss: 0.00001642
Iteration 186/1000 | Loss: 0.00001642
Iteration 187/1000 | Loss: 0.00001642
Iteration 188/1000 | Loss: 0.00001642
Iteration 189/1000 | Loss: 0.00001642
Iteration 190/1000 | Loss: 0.00001641
Iteration 191/1000 | Loss: 0.00001641
Iteration 192/1000 | Loss: 0.00001641
Iteration 193/1000 | Loss: 0.00001641
Iteration 194/1000 | Loss: 0.00001641
Iteration 195/1000 | Loss: 0.00001640
Iteration 196/1000 | Loss: 0.00001640
Iteration 197/1000 | Loss: 0.00001640
Iteration 198/1000 | Loss: 0.00001640
Iteration 199/1000 | Loss: 0.00001640
Iteration 200/1000 | Loss: 0.00001640
Iteration 201/1000 | Loss: 0.00001639
Iteration 202/1000 | Loss: 0.00001639
Iteration 203/1000 | Loss: 0.00001639
Iteration 204/1000 | Loss: 0.00001639
Iteration 205/1000 | Loss: 0.00001639
Iteration 206/1000 | Loss: 0.00001639
Iteration 207/1000 | Loss: 0.00001638
Iteration 208/1000 | Loss: 0.00001638
Iteration 209/1000 | Loss: 0.00001638
Iteration 210/1000 | Loss: 0.00001638
Iteration 211/1000 | Loss: 0.00001638
Iteration 212/1000 | Loss: 0.00001638
Iteration 213/1000 | Loss: 0.00001637
Iteration 214/1000 | Loss: 0.00001637
Iteration 215/1000 | Loss: 0.00001637
Iteration 216/1000 | Loss: 0.00001636
Iteration 217/1000 | Loss: 0.00001636
Iteration 218/1000 | Loss: 0.00001636
Iteration 219/1000 | Loss: 0.00001636
Iteration 220/1000 | Loss: 0.00001636
Iteration 221/1000 | Loss: 0.00001636
Iteration 222/1000 | Loss: 0.00001636
Iteration 223/1000 | Loss: 0.00001636
Iteration 224/1000 | Loss: 0.00001636
Iteration 225/1000 | Loss: 0.00001636
Iteration 226/1000 | Loss: 0.00001636
Iteration 227/1000 | Loss: 0.00001635
Iteration 228/1000 | Loss: 0.00001635
Iteration 229/1000 | Loss: 0.00001635
Iteration 230/1000 | Loss: 0.00001635
Iteration 231/1000 | Loss: 0.00001635
Iteration 232/1000 | Loss: 0.00001635
Iteration 233/1000 | Loss: 0.00001634
Iteration 234/1000 | Loss: 0.00001634
Iteration 235/1000 | Loss: 0.00001634
Iteration 236/1000 | Loss: 0.00001634
Iteration 237/1000 | Loss: 0.00001634
Iteration 238/1000 | Loss: 0.00001634
Iteration 239/1000 | Loss: 0.00001634
Iteration 240/1000 | Loss: 0.00001634
Iteration 241/1000 | Loss: 0.00001634
Iteration 242/1000 | Loss: 0.00001634
Iteration 243/1000 | Loss: 0.00001634
Iteration 244/1000 | Loss: 0.00001634
Iteration 245/1000 | Loss: 0.00001634
Iteration 246/1000 | Loss: 0.00001634
Iteration 247/1000 | Loss: 0.00001634
Iteration 248/1000 | Loss: 0.00001634
Iteration 249/1000 | Loss: 0.00001634
Iteration 250/1000 | Loss: 0.00001634
Iteration 251/1000 | Loss: 0.00001634
Iteration 252/1000 | Loss: 0.00001634
Iteration 253/1000 | Loss: 0.00001634
Iteration 254/1000 | Loss: 0.00001634
Iteration 255/1000 | Loss: 0.00001634
Iteration 256/1000 | Loss: 0.00001634
Iteration 257/1000 | Loss: 0.00001634
Iteration 258/1000 | Loss: 0.00001634
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 258. Stopping optimization.
Last 5 losses: [1.633816100365948e-05, 1.633816100365948e-05, 1.633816100365948e-05, 1.633816100365948e-05, 1.633816100365948e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.633816100365948e-05

Optimization complete. Final v2v error: 3.4026830196380615 mm

Highest mean error: 3.968698024749756 mm for frame 230

Lowest mean error: 3.1133391857147217 mm for frame 195

Saving results

Total time: 256.3121314048767
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_006/1096/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_006/1096.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_006/1096
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00839188
Iteration 2/25 | Loss: 0.00081858
Iteration 3/25 | Loss: 0.00071159
Iteration 4/25 | Loss: 0.00066545
Iteration 5/25 | Loss: 0.00065034
Iteration 6/25 | Loss: 0.00064816
Iteration 7/25 | Loss: 0.00064816
Iteration 8/25 | Loss: 0.00064816
Iteration 9/25 | Loss: 0.00064816
Iteration 10/25 | Loss: 0.00064816
Iteration 11/25 | Loss: 0.00064816
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0006481565069407225, 0.0006481565069407225, 0.0006481565069407225, 0.0006481565069407225, 0.0006481565069407225]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006481565069407225

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.67173672
Iteration 2/25 | Loss: 0.00030027
Iteration 3/25 | Loss: 0.00030020
Iteration 4/25 | Loss: 0.00030019
Iteration 5/25 | Loss: 0.00030019
Iteration 6/25 | Loss: 0.00030019
Iteration 7/25 | Loss: 0.00030019
Iteration 8/25 | Loss: 0.00030019
Iteration 9/25 | Loss: 0.00030019
Iteration 10/25 | Loss: 0.00030019
Iteration 11/25 | Loss: 0.00030019
Iteration 12/25 | Loss: 0.00030019
Iteration 13/25 | Loss: 0.00030019
Iteration 14/25 | Loss: 0.00030019
Iteration 15/25 | Loss: 0.00030019
Iteration 16/25 | Loss: 0.00030019
Iteration 17/25 | Loss: 0.00030019
Iteration 18/25 | Loss: 0.00030019
Iteration 19/25 | Loss: 0.00030019
Iteration 20/25 | Loss: 0.00030019
Iteration 21/25 | Loss: 0.00030019
Iteration 22/25 | Loss: 0.00030019
Iteration 23/25 | Loss: 0.00030019
Iteration 24/25 | Loss: 0.00030019
Iteration 25/25 | Loss: 0.00030019

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00030019
Iteration 2/1000 | Loss: 0.00003975
Iteration 3/1000 | Loss: 0.00002315
Iteration 4/1000 | Loss: 0.00002072
Iteration 5/1000 | Loss: 0.00001954
Iteration 6/1000 | Loss: 0.00001893
Iteration 7/1000 | Loss: 0.00001840
Iteration 8/1000 | Loss: 0.00001796
Iteration 9/1000 | Loss: 0.00001759
Iteration 10/1000 | Loss: 0.00001738
Iteration 11/1000 | Loss: 0.00001717
Iteration 12/1000 | Loss: 0.00001717
Iteration 13/1000 | Loss: 0.00001712
Iteration 14/1000 | Loss: 0.00001710
Iteration 15/1000 | Loss: 0.00001710
Iteration 16/1000 | Loss: 0.00001709
Iteration 17/1000 | Loss: 0.00001709
Iteration 18/1000 | Loss: 0.00001708
Iteration 19/1000 | Loss: 0.00001707
Iteration 20/1000 | Loss: 0.00001707
Iteration 21/1000 | Loss: 0.00001706
Iteration 22/1000 | Loss: 0.00001705
Iteration 23/1000 | Loss: 0.00001705
Iteration 24/1000 | Loss: 0.00001704
Iteration 25/1000 | Loss: 0.00001704
Iteration 26/1000 | Loss: 0.00001703
Iteration 27/1000 | Loss: 0.00001702
Iteration 28/1000 | Loss: 0.00001701
Iteration 29/1000 | Loss: 0.00001701
Iteration 30/1000 | Loss: 0.00001700
Iteration 31/1000 | Loss: 0.00001700
Iteration 32/1000 | Loss: 0.00001699
Iteration 33/1000 | Loss: 0.00001699
Iteration 34/1000 | Loss: 0.00001698
Iteration 35/1000 | Loss: 0.00001698
Iteration 36/1000 | Loss: 0.00001698
Iteration 37/1000 | Loss: 0.00001698
Iteration 38/1000 | Loss: 0.00001697
Iteration 39/1000 | Loss: 0.00001697
Iteration 40/1000 | Loss: 0.00001696
Iteration 41/1000 | Loss: 0.00001696
Iteration 42/1000 | Loss: 0.00001696
Iteration 43/1000 | Loss: 0.00001696
Iteration 44/1000 | Loss: 0.00001696
Iteration 45/1000 | Loss: 0.00001696
Iteration 46/1000 | Loss: 0.00001695
Iteration 47/1000 | Loss: 0.00001695
Iteration 48/1000 | Loss: 0.00001695
Iteration 49/1000 | Loss: 0.00001694
Iteration 50/1000 | Loss: 0.00001694
Iteration 51/1000 | Loss: 0.00001694
Iteration 52/1000 | Loss: 0.00001693
Iteration 53/1000 | Loss: 0.00001693
Iteration 54/1000 | Loss: 0.00001693
Iteration 55/1000 | Loss: 0.00001693
Iteration 56/1000 | Loss: 0.00001693
Iteration 57/1000 | Loss: 0.00001692
Iteration 58/1000 | Loss: 0.00001692
Iteration 59/1000 | Loss: 0.00001691
Iteration 60/1000 | Loss: 0.00001691
Iteration 61/1000 | Loss: 0.00001691
Iteration 62/1000 | Loss: 0.00001691
Iteration 63/1000 | Loss: 0.00001691
Iteration 64/1000 | Loss: 0.00001691
Iteration 65/1000 | Loss: 0.00001691
Iteration 66/1000 | Loss: 0.00001690
Iteration 67/1000 | Loss: 0.00001690
Iteration 68/1000 | Loss: 0.00001690
Iteration 69/1000 | Loss: 0.00001690
Iteration 70/1000 | Loss: 0.00001690
Iteration 71/1000 | Loss: 0.00001690
Iteration 72/1000 | Loss: 0.00001690
Iteration 73/1000 | Loss: 0.00001689
Iteration 74/1000 | Loss: 0.00001689
Iteration 75/1000 | Loss: 0.00001689
Iteration 76/1000 | Loss: 0.00001689
Iteration 77/1000 | Loss: 0.00001689
Iteration 78/1000 | Loss: 0.00001689
Iteration 79/1000 | Loss: 0.00001689
Iteration 80/1000 | Loss: 0.00001689
Iteration 81/1000 | Loss: 0.00001689
Iteration 82/1000 | Loss: 0.00001689
Iteration 83/1000 | Loss: 0.00001688
Iteration 84/1000 | Loss: 0.00001688
Iteration 85/1000 | Loss: 0.00001688
Iteration 86/1000 | Loss: 0.00001688
Iteration 87/1000 | Loss: 0.00001688
Iteration 88/1000 | Loss: 0.00001688
Iteration 89/1000 | Loss: 0.00001688
Iteration 90/1000 | Loss: 0.00001688
Iteration 91/1000 | Loss: 0.00001688
Iteration 92/1000 | Loss: 0.00001688
Iteration 93/1000 | Loss: 0.00001688
Iteration 94/1000 | Loss: 0.00001688
Iteration 95/1000 | Loss: 0.00001688
Iteration 96/1000 | Loss: 0.00001688
Iteration 97/1000 | Loss: 0.00001688
Iteration 98/1000 | Loss: 0.00001688
Iteration 99/1000 | Loss: 0.00001688
Iteration 100/1000 | Loss: 0.00001687
Iteration 101/1000 | Loss: 0.00001687
Iteration 102/1000 | Loss: 0.00001687
Iteration 103/1000 | Loss: 0.00001687
Iteration 104/1000 | Loss: 0.00001687
Iteration 105/1000 | Loss: 0.00001687
Iteration 106/1000 | Loss: 0.00001687
Iteration 107/1000 | Loss: 0.00001686
Iteration 108/1000 | Loss: 0.00001686
Iteration 109/1000 | Loss: 0.00001686
Iteration 110/1000 | Loss: 0.00001686
Iteration 111/1000 | Loss: 0.00001686
Iteration 112/1000 | Loss: 0.00001686
Iteration 113/1000 | Loss: 0.00001686
Iteration 114/1000 | Loss: 0.00001686
Iteration 115/1000 | Loss: 0.00001686
Iteration 116/1000 | Loss: 0.00001686
Iteration 117/1000 | Loss: 0.00001686
Iteration 118/1000 | Loss: 0.00001686
Iteration 119/1000 | Loss: 0.00001686
Iteration 120/1000 | Loss: 0.00001685
Iteration 121/1000 | Loss: 0.00001685
Iteration 122/1000 | Loss: 0.00001685
Iteration 123/1000 | Loss: 0.00001685
Iteration 124/1000 | Loss: 0.00001685
Iteration 125/1000 | Loss: 0.00001685
Iteration 126/1000 | Loss: 0.00001685
Iteration 127/1000 | Loss: 0.00001685
Iteration 128/1000 | Loss: 0.00001685
Iteration 129/1000 | Loss: 0.00001685
Iteration 130/1000 | Loss: 0.00001685
Iteration 131/1000 | Loss: 0.00001684
Iteration 132/1000 | Loss: 0.00001684
Iteration 133/1000 | Loss: 0.00001684
Iteration 134/1000 | Loss: 0.00001684
Iteration 135/1000 | Loss: 0.00001684
Iteration 136/1000 | Loss: 0.00001684
Iteration 137/1000 | Loss: 0.00001684
Iteration 138/1000 | Loss: 0.00001684
Iteration 139/1000 | Loss: 0.00001684
Iteration 140/1000 | Loss: 0.00001684
Iteration 141/1000 | Loss: 0.00001684
Iteration 142/1000 | Loss: 0.00001684
Iteration 143/1000 | Loss: 0.00001684
Iteration 144/1000 | Loss: 0.00001684
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 144. Stopping optimization.
Last 5 losses: [1.684042945271358e-05, 1.684042945271358e-05, 1.684042945271358e-05, 1.684042945271358e-05, 1.684042945271358e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.684042945271358e-05

Optimization complete. Final v2v error: 3.4744367599487305 mm

Highest mean error: 3.9631407260894775 mm for frame 184

Lowest mean error: 3.166335105895996 mm for frame 238

Saving results

Total time: 39.88728141784668
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_006/1010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_006/1010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_006/1010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00408957
Iteration 2/25 | Loss: 0.00087563
Iteration 3/25 | Loss: 0.00074615
Iteration 4/25 | Loss: 0.00071728
Iteration 5/25 | Loss: 0.00070747
Iteration 6/25 | Loss: 0.00070610
Iteration 7/25 | Loss: 0.00070561
Iteration 8/25 | Loss: 0.00070554
Iteration 9/25 | Loss: 0.00070554
Iteration 10/25 | Loss: 0.00070554
Iteration 11/25 | Loss: 0.00070554
Iteration 12/25 | Loss: 0.00070554
Iteration 13/25 | Loss: 0.00070554
Iteration 14/25 | Loss: 0.00070554
Iteration 15/25 | Loss: 0.00070554
Iteration 16/25 | Loss: 0.00070554
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0007055420428514481, 0.0007055420428514481, 0.0007055420428514481, 0.0007055420428514481, 0.0007055420428514481]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007055420428514481

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45503259
Iteration 2/25 | Loss: 0.00031367
Iteration 3/25 | Loss: 0.00031367
Iteration 4/25 | Loss: 0.00031367
Iteration 5/25 | Loss: 0.00031367
Iteration 6/25 | Loss: 0.00031367
Iteration 7/25 | Loss: 0.00031367
Iteration 8/25 | Loss: 0.00031367
Iteration 9/25 | Loss: 0.00031367
Iteration 10/25 | Loss: 0.00031367
Iteration 11/25 | Loss: 0.00031367
Iteration 12/25 | Loss: 0.00031367
Iteration 13/25 | Loss: 0.00031367
Iteration 14/25 | Loss: 0.00031367
Iteration 15/25 | Loss: 0.00031367
Iteration 16/25 | Loss: 0.00031367
Iteration 17/25 | Loss: 0.00031367
Iteration 18/25 | Loss: 0.00031367
Iteration 19/25 | Loss: 0.00031367
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.00031366810435429215, 0.00031366810435429215, 0.00031366810435429215, 0.00031366810435429215, 0.00031366810435429215]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00031366810435429215

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00031367
Iteration 2/1000 | Loss: 0.00005153
Iteration 3/1000 | Loss: 0.00003423
Iteration 4/1000 | Loss: 0.00003094
Iteration 5/1000 | Loss: 0.00002929
Iteration 6/1000 | Loss: 0.00002827
Iteration 7/1000 | Loss: 0.00002752
Iteration 8/1000 | Loss: 0.00002699
Iteration 9/1000 | Loss: 0.00002673
Iteration 10/1000 | Loss: 0.00002651
Iteration 11/1000 | Loss: 0.00002637
Iteration 12/1000 | Loss: 0.00002634
Iteration 13/1000 | Loss: 0.00002630
Iteration 14/1000 | Loss: 0.00002626
Iteration 15/1000 | Loss: 0.00002625
Iteration 16/1000 | Loss: 0.00002624
Iteration 17/1000 | Loss: 0.00002618
Iteration 18/1000 | Loss: 0.00002614
Iteration 19/1000 | Loss: 0.00002613
Iteration 20/1000 | Loss: 0.00002613
Iteration 21/1000 | Loss: 0.00002612
Iteration 22/1000 | Loss: 0.00002612
Iteration 23/1000 | Loss: 0.00002611
Iteration 24/1000 | Loss: 0.00002611
Iteration 25/1000 | Loss: 0.00002610
Iteration 26/1000 | Loss: 0.00002610
Iteration 27/1000 | Loss: 0.00002607
Iteration 28/1000 | Loss: 0.00002607
Iteration 29/1000 | Loss: 0.00002607
Iteration 30/1000 | Loss: 0.00002606
Iteration 31/1000 | Loss: 0.00002606
Iteration 32/1000 | Loss: 0.00002604
Iteration 33/1000 | Loss: 0.00002604
Iteration 34/1000 | Loss: 0.00002599
Iteration 35/1000 | Loss: 0.00002598
Iteration 36/1000 | Loss: 0.00002597
Iteration 37/1000 | Loss: 0.00002597
Iteration 38/1000 | Loss: 0.00002596
Iteration 39/1000 | Loss: 0.00002595
Iteration 40/1000 | Loss: 0.00002594
Iteration 41/1000 | Loss: 0.00002593
Iteration 42/1000 | Loss: 0.00002593
Iteration 43/1000 | Loss: 0.00002593
Iteration 44/1000 | Loss: 0.00002591
Iteration 45/1000 | Loss: 0.00002589
Iteration 46/1000 | Loss: 0.00002588
Iteration 47/1000 | Loss: 0.00002588
Iteration 48/1000 | Loss: 0.00002588
Iteration 49/1000 | Loss: 0.00002588
Iteration 50/1000 | Loss: 0.00002587
Iteration 51/1000 | Loss: 0.00002587
Iteration 52/1000 | Loss: 0.00002587
Iteration 53/1000 | Loss: 0.00002585
Iteration 54/1000 | Loss: 0.00002585
Iteration 55/1000 | Loss: 0.00002585
Iteration 56/1000 | Loss: 0.00002584
Iteration 57/1000 | Loss: 0.00002584
Iteration 58/1000 | Loss: 0.00002584
Iteration 59/1000 | Loss: 0.00002584
Iteration 60/1000 | Loss: 0.00002584
Iteration 61/1000 | Loss: 0.00002584
Iteration 62/1000 | Loss: 0.00002583
Iteration 63/1000 | Loss: 0.00002583
Iteration 64/1000 | Loss: 0.00002583
Iteration 65/1000 | Loss: 0.00002583
Iteration 66/1000 | Loss: 0.00002583
Iteration 67/1000 | Loss: 0.00002583
Iteration 68/1000 | Loss: 0.00002583
Iteration 69/1000 | Loss: 0.00002582
Iteration 70/1000 | Loss: 0.00002582
Iteration 71/1000 | Loss: 0.00002582
Iteration 72/1000 | Loss: 0.00002582
Iteration 73/1000 | Loss: 0.00002582
Iteration 74/1000 | Loss: 0.00002581
Iteration 75/1000 | Loss: 0.00002581
Iteration 76/1000 | Loss: 0.00002581
Iteration 77/1000 | Loss: 0.00002581
Iteration 78/1000 | Loss: 0.00002581
Iteration 79/1000 | Loss: 0.00002581
Iteration 80/1000 | Loss: 0.00002581
Iteration 81/1000 | Loss: 0.00002581
Iteration 82/1000 | Loss: 0.00002581
Iteration 83/1000 | Loss: 0.00002581
Iteration 84/1000 | Loss: 0.00002581
Iteration 85/1000 | Loss: 0.00002581
Iteration 86/1000 | Loss: 0.00002581
Iteration 87/1000 | Loss: 0.00002581
Iteration 88/1000 | Loss: 0.00002581
Iteration 89/1000 | Loss: 0.00002581
Iteration 90/1000 | Loss: 0.00002581
Iteration 91/1000 | Loss: 0.00002581
Iteration 92/1000 | Loss: 0.00002581
Iteration 93/1000 | Loss: 0.00002581
Iteration 94/1000 | Loss: 0.00002581
Iteration 95/1000 | Loss: 0.00002581
Iteration 96/1000 | Loss: 0.00002581
Iteration 97/1000 | Loss: 0.00002581
Iteration 98/1000 | Loss: 0.00002581
Iteration 99/1000 | Loss: 0.00002581
Iteration 100/1000 | Loss: 0.00002581
Iteration 101/1000 | Loss: 0.00002581
Iteration 102/1000 | Loss: 0.00002581
Iteration 103/1000 | Loss: 0.00002581
Iteration 104/1000 | Loss: 0.00002581
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 104. Stopping optimization.
Last 5 losses: [2.581210355856456e-05, 2.581210355856456e-05, 2.581210355856456e-05, 2.581210355856456e-05, 2.581210355856456e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.581210355856456e-05

Optimization complete. Final v2v error: 4.207295894622803 mm

Highest mean error: 4.594475269317627 mm for frame 15

Lowest mean error: 3.812472105026245 mm for frame 33

Saving results

Total time: 35.07734823226929
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_006/1097/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_006/1097.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_006/1097
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00854112
Iteration 2/25 | Loss: 0.00082979
Iteration 3/25 | Loss: 0.00063470
Iteration 4/25 | Loss: 0.00059874
Iteration 5/25 | Loss: 0.00058975
Iteration 6/25 | Loss: 0.00058748
Iteration 7/25 | Loss: 0.00058705
Iteration 8/25 | Loss: 0.00058705
Iteration 9/25 | Loss: 0.00058705
Iteration 10/25 | Loss: 0.00058705
Iteration 11/25 | Loss: 0.00058705
Iteration 12/25 | Loss: 0.00058705
Iteration 13/25 | Loss: 0.00058705
Iteration 14/25 | Loss: 0.00058705
Iteration 15/25 | Loss: 0.00058705
Iteration 16/25 | Loss: 0.00058705
Iteration 17/25 | Loss: 0.00058705
Iteration 18/25 | Loss: 0.00058705
Iteration 19/25 | Loss: 0.00058705
Iteration 20/25 | Loss: 0.00058705
Iteration 21/25 | Loss: 0.00058705
Iteration 22/25 | Loss: 0.00058705
Iteration 23/25 | Loss: 0.00058705
Iteration 24/25 | Loss: 0.00058705
Iteration 25/25 | Loss: 0.00058705

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46041298
Iteration 2/25 | Loss: 0.00024400
Iteration 3/25 | Loss: 0.00024400
Iteration 4/25 | Loss: 0.00024400
Iteration 5/25 | Loss: 0.00024400
Iteration 6/25 | Loss: 0.00024399
Iteration 7/25 | Loss: 0.00024399
Iteration 8/25 | Loss: 0.00024399
Iteration 9/25 | Loss: 0.00024399
Iteration 10/25 | Loss: 0.00024399
Iteration 11/25 | Loss: 0.00024399
Iteration 12/25 | Loss: 0.00024399
Iteration 13/25 | Loss: 0.00024399
Iteration 14/25 | Loss: 0.00024399
Iteration 15/25 | Loss: 0.00024399
Iteration 16/25 | Loss: 0.00024399
Iteration 17/25 | Loss: 0.00024399
Iteration 18/25 | Loss: 0.00024399
Iteration 19/25 | Loss: 0.00024399
Iteration 20/25 | Loss: 0.00024399
Iteration 21/25 | Loss: 0.00024399
Iteration 22/25 | Loss: 0.00024399
Iteration 23/25 | Loss: 0.00024399
Iteration 24/25 | Loss: 0.00024399
Iteration 25/25 | Loss: 0.00024399
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.00024399338872171938, 0.00024399338872171938, 0.00024399338872171938, 0.00024399338872171938, 0.00024399338872171938]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00024399338872171938

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00024399
Iteration 2/1000 | Loss: 0.00001919
Iteration 3/1000 | Loss: 0.00001405
Iteration 4/1000 | Loss: 0.00001295
Iteration 5/1000 | Loss: 0.00001210
Iteration 6/1000 | Loss: 0.00001171
Iteration 7/1000 | Loss: 0.00001143
Iteration 8/1000 | Loss: 0.00001142
Iteration 9/1000 | Loss: 0.00001137
Iteration 10/1000 | Loss: 0.00001136
Iteration 11/1000 | Loss: 0.00001131
Iteration 12/1000 | Loss: 0.00001115
Iteration 13/1000 | Loss: 0.00001112
Iteration 14/1000 | Loss: 0.00001109
Iteration 15/1000 | Loss: 0.00001108
Iteration 16/1000 | Loss: 0.00001108
Iteration 17/1000 | Loss: 0.00001107
Iteration 18/1000 | Loss: 0.00001107
Iteration 19/1000 | Loss: 0.00001106
Iteration 20/1000 | Loss: 0.00001105
Iteration 21/1000 | Loss: 0.00001104
Iteration 22/1000 | Loss: 0.00001104
Iteration 23/1000 | Loss: 0.00001104
Iteration 24/1000 | Loss: 0.00001103
Iteration 25/1000 | Loss: 0.00001103
Iteration 26/1000 | Loss: 0.00001103
Iteration 27/1000 | Loss: 0.00001102
Iteration 28/1000 | Loss: 0.00001102
Iteration 29/1000 | Loss: 0.00001102
Iteration 30/1000 | Loss: 0.00001100
Iteration 31/1000 | Loss: 0.00001099
Iteration 32/1000 | Loss: 0.00001099
Iteration 33/1000 | Loss: 0.00001098
Iteration 34/1000 | Loss: 0.00001098
Iteration 35/1000 | Loss: 0.00001098
Iteration 36/1000 | Loss: 0.00001098
Iteration 37/1000 | Loss: 0.00001098
Iteration 38/1000 | Loss: 0.00001098
Iteration 39/1000 | Loss: 0.00001097
Iteration 40/1000 | Loss: 0.00001097
Iteration 41/1000 | Loss: 0.00001097
Iteration 42/1000 | Loss: 0.00001097
Iteration 43/1000 | Loss: 0.00001097
Iteration 44/1000 | Loss: 0.00001096
Iteration 45/1000 | Loss: 0.00001096
Iteration 46/1000 | Loss: 0.00001096
Iteration 47/1000 | Loss: 0.00001095
Iteration 48/1000 | Loss: 0.00001095
Iteration 49/1000 | Loss: 0.00001095
Iteration 50/1000 | Loss: 0.00001095
Iteration 51/1000 | Loss: 0.00001095
Iteration 52/1000 | Loss: 0.00001095
Iteration 53/1000 | Loss: 0.00001094
Iteration 54/1000 | Loss: 0.00001094
Iteration 55/1000 | Loss: 0.00001094
Iteration 56/1000 | Loss: 0.00001094
Iteration 57/1000 | Loss: 0.00001093
Iteration 58/1000 | Loss: 0.00001093
Iteration 59/1000 | Loss: 0.00001093
Iteration 60/1000 | Loss: 0.00001092
Iteration 61/1000 | Loss: 0.00001092
Iteration 62/1000 | Loss: 0.00001092
Iteration 63/1000 | Loss: 0.00001091
Iteration 64/1000 | Loss: 0.00001090
Iteration 65/1000 | Loss: 0.00001090
Iteration 66/1000 | Loss: 0.00001090
Iteration 67/1000 | Loss: 0.00001090
Iteration 68/1000 | Loss: 0.00001090
Iteration 69/1000 | Loss: 0.00001089
Iteration 70/1000 | Loss: 0.00001089
Iteration 71/1000 | Loss: 0.00001089
Iteration 72/1000 | Loss: 0.00001089
Iteration 73/1000 | Loss: 0.00001089
Iteration 74/1000 | Loss: 0.00001088
Iteration 75/1000 | Loss: 0.00001088
Iteration 76/1000 | Loss: 0.00001088
Iteration 77/1000 | Loss: 0.00001088
Iteration 78/1000 | Loss: 0.00001088
Iteration 79/1000 | Loss: 0.00001088
Iteration 80/1000 | Loss: 0.00001088
Iteration 81/1000 | Loss: 0.00001088
Iteration 82/1000 | Loss: 0.00001088
Iteration 83/1000 | Loss: 0.00001087
Iteration 84/1000 | Loss: 0.00001087
Iteration 85/1000 | Loss: 0.00001087
Iteration 86/1000 | Loss: 0.00001087
Iteration 87/1000 | Loss: 0.00001087
Iteration 88/1000 | Loss: 0.00001087
Iteration 89/1000 | Loss: 0.00001087
Iteration 90/1000 | Loss: 0.00001087
Iteration 91/1000 | Loss: 0.00001087
Iteration 92/1000 | Loss: 0.00001087
Iteration 93/1000 | Loss: 0.00001086
Iteration 94/1000 | Loss: 0.00001086
Iteration 95/1000 | Loss: 0.00001086
Iteration 96/1000 | Loss: 0.00001086
Iteration 97/1000 | Loss: 0.00001086
Iteration 98/1000 | Loss: 0.00001086
Iteration 99/1000 | Loss: 0.00001086
Iteration 100/1000 | Loss: 0.00001085
Iteration 101/1000 | Loss: 0.00001085
Iteration 102/1000 | Loss: 0.00001085
Iteration 103/1000 | Loss: 0.00001085
Iteration 104/1000 | Loss: 0.00001085
Iteration 105/1000 | Loss: 0.00001085
Iteration 106/1000 | Loss: 0.00001085
Iteration 107/1000 | Loss: 0.00001085
Iteration 108/1000 | Loss: 0.00001085
Iteration 109/1000 | Loss: 0.00001084
Iteration 110/1000 | Loss: 0.00001083
Iteration 111/1000 | Loss: 0.00001083
Iteration 112/1000 | Loss: 0.00001082
Iteration 113/1000 | Loss: 0.00001082
Iteration 114/1000 | Loss: 0.00001082
Iteration 115/1000 | Loss: 0.00001082
Iteration 116/1000 | Loss: 0.00001082
Iteration 117/1000 | Loss: 0.00001082
Iteration 118/1000 | Loss: 0.00001081
Iteration 119/1000 | Loss: 0.00001081
Iteration 120/1000 | Loss: 0.00001081
Iteration 121/1000 | Loss: 0.00001080
Iteration 122/1000 | Loss: 0.00001080
Iteration 123/1000 | Loss: 0.00001078
Iteration 124/1000 | Loss: 0.00001078
Iteration 125/1000 | Loss: 0.00001078
Iteration 126/1000 | Loss: 0.00001078
Iteration 127/1000 | Loss: 0.00001078
Iteration 128/1000 | Loss: 0.00001078
Iteration 129/1000 | Loss: 0.00001077
Iteration 130/1000 | Loss: 0.00001077
Iteration 131/1000 | Loss: 0.00001077
Iteration 132/1000 | Loss: 0.00001077
Iteration 133/1000 | Loss: 0.00001077
Iteration 134/1000 | Loss: 0.00001077
Iteration 135/1000 | Loss: 0.00001077
Iteration 136/1000 | Loss: 0.00001077
Iteration 137/1000 | Loss: 0.00001077
Iteration 138/1000 | Loss: 0.00001077
Iteration 139/1000 | Loss: 0.00001077
Iteration 140/1000 | Loss: 0.00001076
Iteration 141/1000 | Loss: 0.00001076
Iteration 142/1000 | Loss: 0.00001076
Iteration 143/1000 | Loss: 0.00001076
Iteration 144/1000 | Loss: 0.00001076
Iteration 145/1000 | Loss: 0.00001076
Iteration 146/1000 | Loss: 0.00001076
Iteration 147/1000 | Loss: 0.00001075
Iteration 148/1000 | Loss: 0.00001075
Iteration 149/1000 | Loss: 0.00001075
Iteration 150/1000 | Loss: 0.00001075
Iteration 151/1000 | Loss: 0.00001075
Iteration 152/1000 | Loss: 0.00001074
Iteration 153/1000 | Loss: 0.00001074
Iteration 154/1000 | Loss: 0.00001073
Iteration 155/1000 | Loss: 0.00001073
Iteration 156/1000 | Loss: 0.00001073
Iteration 157/1000 | Loss: 0.00001073
Iteration 158/1000 | Loss: 0.00001073
Iteration 159/1000 | Loss: 0.00001073
Iteration 160/1000 | Loss: 0.00001073
Iteration 161/1000 | Loss: 0.00001073
Iteration 162/1000 | Loss: 0.00001072
Iteration 163/1000 | Loss: 0.00001072
Iteration 164/1000 | Loss: 0.00001072
Iteration 165/1000 | Loss: 0.00001072
Iteration 166/1000 | Loss: 0.00001072
Iteration 167/1000 | Loss: 0.00001072
Iteration 168/1000 | Loss: 0.00001072
Iteration 169/1000 | Loss: 0.00001072
Iteration 170/1000 | Loss: 0.00001072
Iteration 171/1000 | Loss: 0.00001072
Iteration 172/1000 | Loss: 0.00001072
Iteration 173/1000 | Loss: 0.00001072
Iteration 174/1000 | Loss: 0.00001072
Iteration 175/1000 | Loss: 0.00001072
Iteration 176/1000 | Loss: 0.00001072
Iteration 177/1000 | Loss: 0.00001072
Iteration 178/1000 | Loss: 0.00001072
Iteration 179/1000 | Loss: 0.00001072
Iteration 180/1000 | Loss: 0.00001072
Iteration 181/1000 | Loss: 0.00001072
Iteration 182/1000 | Loss: 0.00001072
Iteration 183/1000 | Loss: 0.00001072
Iteration 184/1000 | Loss: 0.00001072
Iteration 185/1000 | Loss: 0.00001072
Iteration 186/1000 | Loss: 0.00001072
Iteration 187/1000 | Loss: 0.00001072
Iteration 188/1000 | Loss: 0.00001072
Iteration 189/1000 | Loss: 0.00001072
Iteration 190/1000 | Loss: 0.00001072
Iteration 191/1000 | Loss: 0.00001072
Iteration 192/1000 | Loss: 0.00001072
Iteration 193/1000 | Loss: 0.00001072
Iteration 194/1000 | Loss: 0.00001072
Iteration 195/1000 | Loss: 0.00001072
Iteration 196/1000 | Loss: 0.00001072
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 196. Stopping optimization.
Last 5 losses: [1.071510723704705e-05, 1.071510723704705e-05, 1.071510723704705e-05, 1.071510723704705e-05, 1.071510723704705e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.071510723704705e-05

Optimization complete. Final v2v error: 2.7585227489471436 mm

Highest mean error: 2.995422124862671 mm for frame 65

Lowest mean error: 2.561455011367798 mm for frame 3

Saving results

Total time: 41.838214635849
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_006/1049/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_006/1049.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_006/1049
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01163513
Iteration 2/25 | Loss: 0.00163049
Iteration 3/25 | Loss: 0.00093705
Iteration 4/25 | Loss: 0.00085879
Iteration 5/25 | Loss: 0.00084117
Iteration 6/25 | Loss: 0.00083648
Iteration 7/25 | Loss: 0.00083582
Iteration 8/25 | Loss: 0.00083582
Iteration 9/25 | Loss: 0.00083582
Iteration 10/25 | Loss: 0.00083582
Iteration 11/25 | Loss: 0.00083582
Iteration 12/25 | Loss: 0.00083582
Iteration 13/25 | Loss: 0.00083582
Iteration 14/25 | Loss: 0.00083582
Iteration 15/25 | Loss: 0.00083582
Iteration 16/25 | Loss: 0.00083582
Iteration 17/25 | Loss: 0.00083582
Iteration 18/25 | Loss: 0.00083582
Iteration 19/25 | Loss: 0.00083582
Iteration 20/25 | Loss: 0.00083582
Iteration 21/25 | Loss: 0.00083582
Iteration 22/25 | Loss: 0.00083582
Iteration 23/25 | Loss: 0.00083582
Iteration 24/25 | Loss: 0.00083582
Iteration 25/25 | Loss: 0.00083582

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.06046224
Iteration 2/25 | Loss: 0.00021580
Iteration 3/25 | Loss: 0.00021580
Iteration 4/25 | Loss: 0.00021579
Iteration 5/25 | Loss: 0.00021579
Iteration 6/25 | Loss: 0.00021579
Iteration 7/25 | Loss: 0.00021579
Iteration 8/25 | Loss: 0.00021579
Iteration 9/25 | Loss: 0.00021579
Iteration 10/25 | Loss: 0.00021579
Iteration 11/25 | Loss: 0.00021579
Iteration 12/25 | Loss: 0.00021579
Iteration 13/25 | Loss: 0.00021579
Iteration 14/25 | Loss: 0.00021579
Iteration 15/25 | Loss: 0.00021579
Iteration 16/25 | Loss: 0.00021579
Iteration 17/25 | Loss: 0.00021579
Iteration 18/25 | Loss: 0.00021579
Iteration 19/25 | Loss: 0.00021579
Iteration 20/25 | Loss: 0.00021579
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.00021579257736448199, 0.00021579257736448199, 0.00021579257736448199, 0.00021579257736448199, 0.00021579257736448199]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00021579257736448199

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00021579
Iteration 2/1000 | Loss: 0.00005404
Iteration 3/1000 | Loss: 0.00003639
Iteration 4/1000 | Loss: 0.00003287
Iteration 5/1000 | Loss: 0.00003140
Iteration 6/1000 | Loss: 0.00003064
Iteration 7/1000 | Loss: 0.00003000
Iteration 8/1000 | Loss: 0.00002963
Iteration 9/1000 | Loss: 0.00002928
Iteration 10/1000 | Loss: 0.00002906
Iteration 11/1000 | Loss: 0.00002904
Iteration 12/1000 | Loss: 0.00002888
Iteration 13/1000 | Loss: 0.00002885
Iteration 14/1000 | Loss: 0.00002873
Iteration 15/1000 | Loss: 0.00002865
Iteration 16/1000 | Loss: 0.00002855
Iteration 17/1000 | Loss: 0.00002854
Iteration 18/1000 | Loss: 0.00002854
Iteration 19/1000 | Loss: 0.00002854
Iteration 20/1000 | Loss: 0.00002854
Iteration 21/1000 | Loss: 0.00002854
Iteration 22/1000 | Loss: 0.00002853
Iteration 23/1000 | Loss: 0.00002853
Iteration 24/1000 | Loss: 0.00002853
Iteration 25/1000 | Loss: 0.00002853
Iteration 26/1000 | Loss: 0.00002852
Iteration 27/1000 | Loss: 0.00002851
Iteration 28/1000 | Loss: 0.00002851
Iteration 29/1000 | Loss: 0.00002851
Iteration 30/1000 | Loss: 0.00002851
Iteration 31/1000 | Loss: 0.00002850
Iteration 32/1000 | Loss: 0.00002850
Iteration 33/1000 | Loss: 0.00002849
Iteration 34/1000 | Loss: 0.00002849
Iteration 35/1000 | Loss: 0.00002848
Iteration 36/1000 | Loss: 0.00002848
Iteration 37/1000 | Loss: 0.00002848
Iteration 38/1000 | Loss: 0.00002846
Iteration 39/1000 | Loss: 0.00002846
Iteration 40/1000 | Loss: 0.00002846
Iteration 41/1000 | Loss: 0.00002846
Iteration 42/1000 | Loss: 0.00002846
Iteration 43/1000 | Loss: 0.00002846
Iteration 44/1000 | Loss: 0.00002846
Iteration 45/1000 | Loss: 0.00002845
Iteration 46/1000 | Loss: 0.00002845
Iteration 47/1000 | Loss: 0.00002845
Iteration 48/1000 | Loss: 0.00002844
Iteration 49/1000 | Loss: 0.00002844
Iteration 50/1000 | Loss: 0.00002844
Iteration 51/1000 | Loss: 0.00002842
Iteration 52/1000 | Loss: 0.00002842
Iteration 53/1000 | Loss: 0.00002842
Iteration 54/1000 | Loss: 0.00002842
Iteration 55/1000 | Loss: 0.00002842
Iteration 56/1000 | Loss: 0.00002842
Iteration 57/1000 | Loss: 0.00002842
Iteration 58/1000 | Loss: 0.00002842
Iteration 59/1000 | Loss: 0.00002840
Iteration 60/1000 | Loss: 0.00002840
Iteration 61/1000 | Loss: 0.00002840
Iteration 62/1000 | Loss: 0.00002839
Iteration 63/1000 | Loss: 0.00002839
Iteration 64/1000 | Loss: 0.00002839
Iteration 65/1000 | Loss: 0.00002838
Iteration 66/1000 | Loss: 0.00002838
Iteration 67/1000 | Loss: 0.00002838
Iteration 68/1000 | Loss: 0.00002838
Iteration 69/1000 | Loss: 0.00002838
Iteration 70/1000 | Loss: 0.00002837
Iteration 71/1000 | Loss: 0.00002837
Iteration 72/1000 | Loss: 0.00002837
Iteration 73/1000 | Loss: 0.00002837
Iteration 74/1000 | Loss: 0.00002837
Iteration 75/1000 | Loss: 0.00002836
Iteration 76/1000 | Loss: 0.00002836
Iteration 77/1000 | Loss: 0.00002836
Iteration 78/1000 | Loss: 0.00002836
Iteration 79/1000 | Loss: 0.00002836
Iteration 80/1000 | Loss: 0.00002836
Iteration 81/1000 | Loss: 0.00002835
Iteration 82/1000 | Loss: 0.00002835
Iteration 83/1000 | Loss: 0.00002835
Iteration 84/1000 | Loss: 0.00002835
Iteration 85/1000 | Loss: 0.00002835
Iteration 86/1000 | Loss: 0.00002835
Iteration 87/1000 | Loss: 0.00002835
Iteration 88/1000 | Loss: 0.00002835
Iteration 89/1000 | Loss: 0.00002835
Iteration 90/1000 | Loss: 0.00002835
Iteration 91/1000 | Loss: 0.00002835
Iteration 92/1000 | Loss: 0.00002835
Iteration 93/1000 | Loss: 0.00002835
Iteration 94/1000 | Loss: 0.00002835
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 94. Stopping optimization.
Last 5 losses: [2.8353368179523386e-05, 2.8353368179523386e-05, 2.8353368179523386e-05, 2.8353368179523386e-05, 2.8353368179523386e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.8353368179523386e-05

Optimization complete. Final v2v error: 4.307007789611816 mm

Highest mean error: 5.596362590789795 mm for frame 57

Lowest mean error: 3.731539011001587 mm for frame 33

Saving results

Total time: 40.94175887107849
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_006/1039/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_006/1039.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_006/1039
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00930853
Iteration 2/25 | Loss: 0.00139662
Iteration 3/25 | Loss: 0.00087352
Iteration 4/25 | Loss: 0.00076023
Iteration 5/25 | Loss: 0.00075500
Iteration 6/25 | Loss: 0.00075413
Iteration 7/25 | Loss: 0.00075412
Iteration 8/25 | Loss: 0.00075412
Iteration 9/25 | Loss: 0.00075412
Iteration 10/25 | Loss: 0.00075412
Iteration 11/25 | Loss: 0.00075412
Iteration 12/25 | Loss: 0.00075412
Iteration 13/25 | Loss: 0.00075412
Iteration 14/25 | Loss: 0.00075412
Iteration 15/25 | Loss: 0.00075412
Iteration 16/25 | Loss: 0.00075412
Iteration 17/25 | Loss: 0.00075412
Iteration 18/25 | Loss: 0.00075412
Iteration 19/25 | Loss: 0.00075412
Iteration 20/25 | Loss: 0.00075412
Iteration 21/25 | Loss: 0.00075412
Iteration 22/25 | Loss: 0.00075412
Iteration 23/25 | Loss: 0.00075412
Iteration 24/25 | Loss: 0.00075412
Iteration 25/25 | Loss: 0.00075412

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.45254898
Iteration 2/25 | Loss: 0.00034339
Iteration 3/25 | Loss: 0.00034334
Iteration 4/25 | Loss: 0.00034334
Iteration 5/25 | Loss: 0.00034334
Iteration 6/25 | Loss: 0.00034334
Iteration 7/25 | Loss: 0.00034334
Iteration 8/25 | Loss: 0.00034334
Iteration 9/25 | Loss: 0.00034334
Iteration 10/25 | Loss: 0.00034334
Iteration 11/25 | Loss: 0.00034334
Iteration 12/25 | Loss: 0.00034334
Iteration 13/25 | Loss: 0.00034334
Iteration 14/25 | Loss: 0.00034334
Iteration 15/25 | Loss: 0.00034334
Iteration 16/25 | Loss: 0.00034334
Iteration 17/25 | Loss: 0.00034334
Iteration 18/25 | Loss: 0.00034334
Iteration 19/25 | Loss: 0.00034334
Iteration 20/25 | Loss: 0.00034334
Iteration 21/25 | Loss: 0.00034334
Iteration 22/25 | Loss: 0.00034334
Iteration 23/25 | Loss: 0.00034334
Iteration 24/25 | Loss: 0.00034334
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0003433400415815413, 0.0003433400415815413, 0.0003433400415815413, 0.0003433400415815413, 0.0003433400415815413]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0003433400415815413

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00034334
Iteration 2/1000 | Loss: 0.00004461
Iteration 3/1000 | Loss: 0.00003296
Iteration 4/1000 | Loss: 0.00002557
Iteration 5/1000 | Loss: 0.00002365
Iteration 6/1000 | Loss: 0.00002242
Iteration 7/1000 | Loss: 0.00002196
Iteration 8/1000 | Loss: 0.00002150
Iteration 9/1000 | Loss: 0.00002110
Iteration 10/1000 | Loss: 0.00002081
Iteration 11/1000 | Loss: 0.00002038
Iteration 12/1000 | Loss: 0.00002009
Iteration 13/1000 | Loss: 0.00001979
Iteration 14/1000 | Loss: 0.00001961
Iteration 15/1000 | Loss: 0.00001951
Iteration 16/1000 | Loss: 0.00001951
Iteration 17/1000 | Loss: 0.00001950
Iteration 18/1000 | Loss: 0.00001950
Iteration 19/1000 | Loss: 0.00001946
Iteration 20/1000 | Loss: 0.00001943
Iteration 21/1000 | Loss: 0.00001943
Iteration 22/1000 | Loss: 0.00001939
Iteration 23/1000 | Loss: 0.00001938
Iteration 24/1000 | Loss: 0.00001937
Iteration 25/1000 | Loss: 0.00001937
Iteration 26/1000 | Loss: 0.00001936
Iteration 27/1000 | Loss: 0.00001929
Iteration 28/1000 | Loss: 0.00001926
Iteration 29/1000 | Loss: 0.00001926
Iteration 30/1000 | Loss: 0.00001926
Iteration 31/1000 | Loss: 0.00001926
Iteration 32/1000 | Loss: 0.00001926
Iteration 33/1000 | Loss: 0.00001926
Iteration 34/1000 | Loss: 0.00001926
Iteration 35/1000 | Loss: 0.00001925
Iteration 36/1000 | Loss: 0.00001925
Iteration 37/1000 | Loss: 0.00001925
Iteration 38/1000 | Loss: 0.00001924
Iteration 39/1000 | Loss: 0.00001924
Iteration 40/1000 | Loss: 0.00001924
Iteration 41/1000 | Loss: 0.00001924
Iteration 42/1000 | Loss: 0.00001924
Iteration 43/1000 | Loss: 0.00001924
Iteration 44/1000 | Loss: 0.00001923
Iteration 45/1000 | Loss: 0.00001923
Iteration 46/1000 | Loss: 0.00001923
Iteration 47/1000 | Loss: 0.00001923
Iteration 48/1000 | Loss: 0.00001923
Iteration 49/1000 | Loss: 0.00001923
Iteration 50/1000 | Loss: 0.00001923
Iteration 51/1000 | Loss: 0.00001922
Iteration 52/1000 | Loss: 0.00001922
Iteration 53/1000 | Loss: 0.00001921
Iteration 54/1000 | Loss: 0.00001920
Iteration 55/1000 | Loss: 0.00001920
Iteration 56/1000 | Loss: 0.00001920
Iteration 57/1000 | Loss: 0.00001920
Iteration 58/1000 | Loss: 0.00001919
Iteration 59/1000 | Loss: 0.00001919
Iteration 60/1000 | Loss: 0.00001919
Iteration 61/1000 | Loss: 0.00001918
Iteration 62/1000 | Loss: 0.00001918
Iteration 63/1000 | Loss: 0.00001918
Iteration 64/1000 | Loss: 0.00001918
Iteration 65/1000 | Loss: 0.00001917
Iteration 66/1000 | Loss: 0.00001917
Iteration 67/1000 | Loss: 0.00001917
Iteration 68/1000 | Loss: 0.00001917
Iteration 69/1000 | Loss: 0.00001917
Iteration 70/1000 | Loss: 0.00001917
Iteration 71/1000 | Loss: 0.00001917
Iteration 72/1000 | Loss: 0.00001917
Iteration 73/1000 | Loss: 0.00001917
Iteration 74/1000 | Loss: 0.00001917
Iteration 75/1000 | Loss: 0.00001917
Iteration 76/1000 | Loss: 0.00001917
Iteration 77/1000 | Loss: 0.00001916
Iteration 78/1000 | Loss: 0.00001916
Iteration 79/1000 | Loss: 0.00001916
Iteration 80/1000 | Loss: 0.00001916
Iteration 81/1000 | Loss: 0.00001916
Iteration 82/1000 | Loss: 0.00001916
Iteration 83/1000 | Loss: 0.00001916
Iteration 84/1000 | Loss: 0.00001916
Iteration 85/1000 | Loss: 0.00001916
Iteration 86/1000 | Loss: 0.00001916
Iteration 87/1000 | Loss: 0.00001916
Iteration 88/1000 | Loss: 0.00001915
Iteration 89/1000 | Loss: 0.00001915
Iteration 90/1000 | Loss: 0.00001915
Iteration 91/1000 | Loss: 0.00001915
Iteration 92/1000 | Loss: 0.00001915
Iteration 93/1000 | Loss: 0.00001915
Iteration 94/1000 | Loss: 0.00001915
Iteration 95/1000 | Loss: 0.00001914
Iteration 96/1000 | Loss: 0.00001914
Iteration 97/1000 | Loss: 0.00001914
Iteration 98/1000 | Loss: 0.00001914
Iteration 99/1000 | Loss: 0.00001914
Iteration 100/1000 | Loss: 0.00001914
Iteration 101/1000 | Loss: 0.00001913
Iteration 102/1000 | Loss: 0.00001913
Iteration 103/1000 | Loss: 0.00001913
Iteration 104/1000 | Loss: 0.00001913
Iteration 105/1000 | Loss: 0.00001913
Iteration 106/1000 | Loss: 0.00001913
Iteration 107/1000 | Loss: 0.00001913
Iteration 108/1000 | Loss: 0.00001913
Iteration 109/1000 | Loss: 0.00001913
Iteration 110/1000 | Loss: 0.00001913
Iteration 111/1000 | Loss: 0.00001913
Iteration 112/1000 | Loss: 0.00001913
Iteration 113/1000 | Loss: 0.00001913
Iteration 114/1000 | Loss: 0.00001913
Iteration 115/1000 | Loss: 0.00001913
Iteration 116/1000 | Loss: 0.00001913
Iteration 117/1000 | Loss: 0.00001913
Iteration 118/1000 | Loss: 0.00001913
Iteration 119/1000 | Loss: 0.00001913
Iteration 120/1000 | Loss: 0.00001913
Iteration 121/1000 | Loss: 0.00001913
Iteration 122/1000 | Loss: 0.00001913
Iteration 123/1000 | Loss: 0.00001913
Iteration 124/1000 | Loss: 0.00001913
Iteration 125/1000 | Loss: 0.00001913
Iteration 126/1000 | Loss: 0.00001912
Iteration 127/1000 | Loss: 0.00001912
Iteration 128/1000 | Loss: 0.00001912
Iteration 129/1000 | Loss: 0.00001912
Iteration 130/1000 | Loss: 0.00001912
Iteration 131/1000 | Loss: 0.00001912
Iteration 132/1000 | Loss: 0.00001912
Iteration 133/1000 | Loss: 0.00001912
Iteration 134/1000 | Loss: 0.00001912
Iteration 135/1000 | Loss: 0.00001912
Iteration 136/1000 | Loss: 0.00001912
Iteration 137/1000 | Loss: 0.00001912
Iteration 138/1000 | Loss: 0.00001912
Iteration 139/1000 | Loss: 0.00001912
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 139. Stopping optimization.
Last 5 losses: [1.9124356185784563e-05, 1.9124356185784563e-05, 1.9124356185784563e-05, 1.9124356185784563e-05, 1.9124356185784563e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9124356185784563e-05

Optimization complete. Final v2v error: 3.6626899242401123 mm

Highest mean error: 3.853220224380493 mm for frame 112

Lowest mean error: 3.488222122192383 mm for frame 71

Saving results

Total time: 39.07390594482422
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_006/1056/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_006/1056.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_006/1056
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01075362
Iteration 2/25 | Loss: 0.00194941
Iteration 3/25 | Loss: 0.00134424
Iteration 4/25 | Loss: 0.00168668
Iteration 5/25 | Loss: 0.00102140
Iteration 6/25 | Loss: 0.00097025
Iteration 7/25 | Loss: 0.00093248
Iteration 8/25 | Loss: 0.00093231
Iteration 9/25 | Loss: 0.00095485
Iteration 10/25 | Loss: 0.00086070
Iteration 11/25 | Loss: 0.00080721
Iteration 12/25 | Loss: 0.00079947
Iteration 13/25 | Loss: 0.00078412
Iteration 14/25 | Loss: 0.00077271
Iteration 15/25 | Loss: 0.00075780
Iteration 16/25 | Loss: 0.00075274
Iteration 17/25 | Loss: 0.00075049
Iteration 18/25 | Loss: 0.00074260
Iteration 19/25 | Loss: 0.00074902
Iteration 20/25 | Loss: 0.00074152
Iteration 21/25 | Loss: 0.00073469
Iteration 22/25 | Loss: 0.00073361
Iteration 23/25 | Loss: 0.00073488
Iteration 24/25 | Loss: 0.00072654
Iteration 25/25 | Loss: 0.00072726

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.64987683
Iteration 2/25 | Loss: 0.00100196
Iteration 3/25 | Loss: 0.00100196
Iteration 4/25 | Loss: 0.00100196
Iteration 5/25 | Loss: 0.00100196
Iteration 6/25 | Loss: 0.00100196
Iteration 7/25 | Loss: 0.00100196
Iteration 8/25 | Loss: 0.00100196
Iteration 9/25 | Loss: 0.00100196
Iteration 10/25 | Loss: 0.00100196
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0010019562905654311, 0.0010019562905654311, 0.0010019562905654311, 0.0010019562905654311, 0.0010019562905654311]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010019562905654311

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00100196
Iteration 2/1000 | Loss: 0.00075383
Iteration 3/1000 | Loss: 0.00098944
Iteration 4/1000 | Loss: 0.00103515
Iteration 5/1000 | Loss: 0.00085066
Iteration 6/1000 | Loss: 0.00127913
Iteration 7/1000 | Loss: 0.00130252
Iteration 8/1000 | Loss: 0.00126734
Iteration 9/1000 | Loss: 0.00127003
Iteration 10/1000 | Loss: 0.00126198
Iteration 11/1000 | Loss: 0.00125792
Iteration 12/1000 | Loss: 0.00101294
Iteration 13/1000 | Loss: 0.00113116
Iteration 14/1000 | Loss: 0.00092596
Iteration 15/1000 | Loss: 0.00073813
Iteration 16/1000 | Loss: 0.00096572
Iteration 17/1000 | Loss: 0.00089075
Iteration 18/1000 | Loss: 0.00117863
Iteration 19/1000 | Loss: 0.00118100
Iteration 20/1000 | Loss: 0.00118240
Iteration 21/1000 | Loss: 0.00113020
Iteration 22/1000 | Loss: 0.00104217
Iteration 23/1000 | Loss: 0.00118994
Iteration 24/1000 | Loss: 0.00112244
Iteration 25/1000 | Loss: 0.00120421
Iteration 26/1000 | Loss: 0.00106243
Iteration 27/1000 | Loss: 0.00122763
Iteration 28/1000 | Loss: 0.00118205
Iteration 29/1000 | Loss: 0.00109154
Iteration 30/1000 | Loss: 0.00121449
Iteration 31/1000 | Loss: 0.00104794
Iteration 32/1000 | Loss: 0.00095556
Iteration 33/1000 | Loss: 0.00086847
Iteration 34/1000 | Loss: 0.00124552
Iteration 35/1000 | Loss: 0.00127448
Iteration 36/1000 | Loss: 0.00121540
Iteration 37/1000 | Loss: 0.00127521
Iteration 38/1000 | Loss: 0.00130316
Iteration 39/1000 | Loss: 0.00142569
Iteration 40/1000 | Loss: 0.00138876
Iteration 41/1000 | Loss: 0.00132755
Iteration 42/1000 | Loss: 0.00138644
Iteration 43/1000 | Loss: 0.00150989
Iteration 44/1000 | Loss: 0.00103735
Iteration 45/1000 | Loss: 0.00073274
Iteration 46/1000 | Loss: 0.00077659
Iteration 47/1000 | Loss: 0.00130725
Iteration 48/1000 | Loss: 0.00138164
Iteration 49/1000 | Loss: 0.00286573
Iteration 50/1000 | Loss: 0.00125181
Iteration 51/1000 | Loss: 0.00110482
Iteration 52/1000 | Loss: 0.00114371
Iteration 53/1000 | Loss: 0.00100933
Iteration 54/1000 | Loss: 0.00074994
Iteration 55/1000 | Loss: 0.00107592
Iteration 56/1000 | Loss: 0.00049381
Iteration 57/1000 | Loss: 0.00068534
Iteration 58/1000 | Loss: 0.00072097
Iteration 59/1000 | Loss: 0.00096229
Iteration 60/1000 | Loss: 0.00127651
Iteration 61/1000 | Loss: 0.00087546
Iteration 62/1000 | Loss: 0.00128789
Iteration 63/1000 | Loss: 0.00103142
Iteration 64/1000 | Loss: 0.00092930
Iteration 65/1000 | Loss: 0.00071857
Iteration 66/1000 | Loss: 0.00083119
Iteration 67/1000 | Loss: 0.00122032
Iteration 68/1000 | Loss: 0.00111042
Iteration 69/1000 | Loss: 0.00063693
Iteration 70/1000 | Loss: 0.00045529
Iteration 71/1000 | Loss: 0.00025365
Iteration 72/1000 | Loss: 0.00039462
Iteration 73/1000 | Loss: 0.00027901
Iteration 74/1000 | Loss: 0.00058240
Iteration 75/1000 | Loss: 0.00055946
Iteration 76/1000 | Loss: 0.00177837
Iteration 77/1000 | Loss: 0.00068380
Iteration 78/1000 | Loss: 0.00037605
Iteration 79/1000 | Loss: 0.00054055
Iteration 80/1000 | Loss: 0.00020218
Iteration 81/1000 | Loss: 0.00073404
Iteration 82/1000 | Loss: 0.00122895
Iteration 83/1000 | Loss: 0.00059713
Iteration 84/1000 | Loss: 0.00068121
Iteration 85/1000 | Loss: 0.00089611
Iteration 86/1000 | Loss: 0.00022994
Iteration 87/1000 | Loss: 0.00122861
Iteration 88/1000 | Loss: 0.00035224
Iteration 89/1000 | Loss: 0.00053486
Iteration 90/1000 | Loss: 0.00048508
Iteration 91/1000 | Loss: 0.00038588
Iteration 92/1000 | Loss: 0.00049975
Iteration 93/1000 | Loss: 0.00040527
Iteration 94/1000 | Loss: 0.00209554
Iteration 95/1000 | Loss: 0.00077281
Iteration 96/1000 | Loss: 0.00064577
Iteration 97/1000 | Loss: 0.00064843
Iteration 98/1000 | Loss: 0.00073658
Iteration 99/1000 | Loss: 0.00079659
Iteration 100/1000 | Loss: 0.00053466
Iteration 101/1000 | Loss: 0.00059608
Iteration 102/1000 | Loss: 0.00050827
Iteration 103/1000 | Loss: 0.00049780
Iteration 104/1000 | Loss: 0.00048358
Iteration 105/1000 | Loss: 0.00059239
Iteration 106/1000 | Loss: 0.00045754
Iteration 107/1000 | Loss: 0.00055742
Iteration 108/1000 | Loss: 0.00035165
Iteration 109/1000 | Loss: 0.00070886
Iteration 110/1000 | Loss: 0.00060638
Iteration 111/1000 | Loss: 0.00024313
Iteration 112/1000 | Loss: 0.00026728
Iteration 113/1000 | Loss: 0.00195940
Iteration 114/1000 | Loss: 0.00047042
Iteration 115/1000 | Loss: 0.00058382
Iteration 116/1000 | Loss: 0.00025354
Iteration 117/1000 | Loss: 0.00014923
Iteration 118/1000 | Loss: 0.00010369
Iteration 119/1000 | Loss: 0.00004232
Iteration 120/1000 | Loss: 0.00003927
Iteration 121/1000 | Loss: 0.00003636
Iteration 122/1000 | Loss: 0.00018631
Iteration 123/1000 | Loss: 0.00011285
Iteration 124/1000 | Loss: 0.00003502
Iteration 125/1000 | Loss: 0.00110956
Iteration 126/1000 | Loss: 0.00041333
Iteration 127/1000 | Loss: 0.00079060
Iteration 128/1000 | Loss: 0.00063838
Iteration 129/1000 | Loss: 0.00062172
Iteration 130/1000 | Loss: 0.00046976
Iteration 131/1000 | Loss: 0.00004432
Iteration 132/1000 | Loss: 0.00003714
Iteration 133/1000 | Loss: 0.00068283
Iteration 134/1000 | Loss: 0.00024404
Iteration 135/1000 | Loss: 0.00036629
Iteration 136/1000 | Loss: 0.00066820
Iteration 137/1000 | Loss: 0.00069899
Iteration 138/1000 | Loss: 0.00064425
Iteration 139/1000 | Loss: 0.00125873
Iteration 140/1000 | Loss: 0.00149935
Iteration 141/1000 | Loss: 0.00060058
Iteration 142/1000 | Loss: 0.00003437
Iteration 143/1000 | Loss: 0.00068860
Iteration 144/1000 | Loss: 0.00032398
Iteration 145/1000 | Loss: 0.00002918
Iteration 146/1000 | Loss: 0.00055753
Iteration 147/1000 | Loss: 0.00007303
Iteration 148/1000 | Loss: 0.00023519
Iteration 149/1000 | Loss: 0.00032566
Iteration 150/1000 | Loss: 0.00005276
Iteration 151/1000 | Loss: 0.00003897
Iteration 152/1000 | Loss: 0.00059405
Iteration 153/1000 | Loss: 0.00040791
Iteration 154/1000 | Loss: 0.00016385
Iteration 155/1000 | Loss: 0.00030948
Iteration 156/1000 | Loss: 0.00029283
Iteration 157/1000 | Loss: 0.00050171
Iteration 158/1000 | Loss: 0.00019048
Iteration 159/1000 | Loss: 0.00042710
Iteration 160/1000 | Loss: 0.00034690
Iteration 161/1000 | Loss: 0.00004967
Iteration 162/1000 | Loss: 0.00003372
Iteration 163/1000 | Loss: 0.00002714
Iteration 164/1000 | Loss: 0.00002398
Iteration 165/1000 | Loss: 0.00002208
Iteration 166/1000 | Loss: 0.00002042
Iteration 167/1000 | Loss: 0.00001936
Iteration 168/1000 | Loss: 0.00001863
Iteration 169/1000 | Loss: 0.00001798
Iteration 170/1000 | Loss: 0.00001752
Iteration 171/1000 | Loss: 0.00001700
Iteration 172/1000 | Loss: 0.00001649
Iteration 173/1000 | Loss: 0.00001616
Iteration 174/1000 | Loss: 0.00001612
Iteration 175/1000 | Loss: 0.00001592
Iteration 176/1000 | Loss: 0.00001575
Iteration 177/1000 | Loss: 0.00001571
Iteration 178/1000 | Loss: 0.00001569
Iteration 179/1000 | Loss: 0.00001569
Iteration 180/1000 | Loss: 0.00001569
Iteration 181/1000 | Loss: 0.00001568
Iteration 182/1000 | Loss: 0.00001568
Iteration 183/1000 | Loss: 0.00001567
Iteration 184/1000 | Loss: 0.00001567
Iteration 185/1000 | Loss: 0.00001567
Iteration 186/1000 | Loss: 0.00001566
Iteration 187/1000 | Loss: 0.00001566
Iteration 188/1000 | Loss: 0.00001566
Iteration 189/1000 | Loss: 0.00001566
Iteration 190/1000 | Loss: 0.00001565
Iteration 191/1000 | Loss: 0.00001565
Iteration 192/1000 | Loss: 0.00001564
Iteration 193/1000 | Loss: 0.00001564
Iteration 194/1000 | Loss: 0.00001564
Iteration 195/1000 | Loss: 0.00001563
Iteration 196/1000 | Loss: 0.00001563
Iteration 197/1000 | Loss: 0.00001563
Iteration 198/1000 | Loss: 0.00001562
Iteration 199/1000 | Loss: 0.00001562
Iteration 200/1000 | Loss: 0.00001562
Iteration 201/1000 | Loss: 0.00001561
Iteration 202/1000 | Loss: 0.00001561
Iteration 203/1000 | Loss: 0.00001560
Iteration 204/1000 | Loss: 0.00001560
Iteration 205/1000 | Loss: 0.00001560
Iteration 206/1000 | Loss: 0.00001560
Iteration 207/1000 | Loss: 0.00001559
Iteration 208/1000 | Loss: 0.00001559
Iteration 209/1000 | Loss: 0.00001558
Iteration 210/1000 | Loss: 0.00001558
Iteration 211/1000 | Loss: 0.00001558
Iteration 212/1000 | Loss: 0.00001557
Iteration 213/1000 | Loss: 0.00001557
Iteration 214/1000 | Loss: 0.00001557
Iteration 215/1000 | Loss: 0.00001557
Iteration 216/1000 | Loss: 0.00001557
Iteration 217/1000 | Loss: 0.00001556
Iteration 218/1000 | Loss: 0.00001556
Iteration 219/1000 | Loss: 0.00001556
Iteration 220/1000 | Loss: 0.00001556
Iteration 221/1000 | Loss: 0.00001556
Iteration 222/1000 | Loss: 0.00001556
Iteration 223/1000 | Loss: 0.00001556
Iteration 224/1000 | Loss: 0.00001555
Iteration 225/1000 | Loss: 0.00001555
Iteration 226/1000 | Loss: 0.00001555
Iteration 227/1000 | Loss: 0.00001555
Iteration 228/1000 | Loss: 0.00001554
Iteration 229/1000 | Loss: 0.00001554
Iteration 230/1000 | Loss: 0.00001554
Iteration 231/1000 | Loss: 0.00001554
Iteration 232/1000 | Loss: 0.00001554
Iteration 233/1000 | Loss: 0.00001553
Iteration 234/1000 | Loss: 0.00001553
Iteration 235/1000 | Loss: 0.00001553
Iteration 236/1000 | Loss: 0.00001553
Iteration 237/1000 | Loss: 0.00001553
Iteration 238/1000 | Loss: 0.00001553
Iteration 239/1000 | Loss: 0.00001553
Iteration 240/1000 | Loss: 0.00001553
Iteration 241/1000 | Loss: 0.00001553
Iteration 242/1000 | Loss: 0.00001553
Iteration 243/1000 | Loss: 0.00001553
Iteration 244/1000 | Loss: 0.00001552
Iteration 245/1000 | Loss: 0.00001552
Iteration 246/1000 | Loss: 0.00001552
Iteration 247/1000 | Loss: 0.00001552
Iteration 248/1000 | Loss: 0.00001552
Iteration 249/1000 | Loss: 0.00001552
Iteration 250/1000 | Loss: 0.00001552
Iteration 251/1000 | Loss: 0.00001552
Iteration 252/1000 | Loss: 0.00001552
Iteration 253/1000 | Loss: 0.00001552
Iteration 254/1000 | Loss: 0.00001552
Iteration 255/1000 | Loss: 0.00001552
Iteration 256/1000 | Loss: 0.00001552
Iteration 257/1000 | Loss: 0.00001552
Iteration 258/1000 | Loss: 0.00001552
Iteration 259/1000 | Loss: 0.00001552
Iteration 260/1000 | Loss: 0.00001552
Iteration 261/1000 | Loss: 0.00001552
Iteration 262/1000 | Loss: 0.00001552
Iteration 263/1000 | Loss: 0.00001552
Iteration 264/1000 | Loss: 0.00001552
Iteration 265/1000 | Loss: 0.00001552
Iteration 266/1000 | Loss: 0.00001552
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 266. Stopping optimization.
Last 5 losses: [1.5522191461059265e-05, 1.5522191461059265e-05, 1.5522191461059265e-05, 1.5522191461059265e-05, 1.5522191461059265e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5522191461059265e-05

Optimization complete. Final v2v error: 3.3371798992156982 mm

Highest mean error: 4.0155415534973145 mm for frame 74

Lowest mean error: 2.9886767864227295 mm for frame 90

Saving results

Total time: 289.69955253601074
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_006/1046/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_006/1046.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_006/1046
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00860317
Iteration 2/25 | Loss: 0.00106225
Iteration 3/25 | Loss: 0.00077677
Iteration 4/25 | Loss: 0.00072143
Iteration 5/25 | Loss: 0.00070487
Iteration 6/25 | Loss: 0.00070114
Iteration 7/25 | Loss: 0.00070080
Iteration 8/25 | Loss: 0.00070080
Iteration 9/25 | Loss: 0.00070080
Iteration 10/25 | Loss: 0.00070080
Iteration 11/25 | Loss: 0.00070080
Iteration 12/25 | Loss: 0.00070080
Iteration 13/25 | Loss: 0.00070080
Iteration 14/25 | Loss: 0.00070080
Iteration 15/25 | Loss: 0.00070080
Iteration 16/25 | Loss: 0.00070080
Iteration 17/25 | Loss: 0.00070080
Iteration 18/25 | Loss: 0.00070080
Iteration 19/25 | Loss: 0.00070080
Iteration 20/25 | Loss: 0.00070080
Iteration 21/25 | Loss: 0.00070080
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0007007993990555406, 0.0007007993990555406, 0.0007007993990555406, 0.0007007993990555406, 0.0007007993990555406]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007007993990555406

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43008161
Iteration 2/25 | Loss: 0.00031253
Iteration 3/25 | Loss: 0.00031248
Iteration 4/25 | Loss: 0.00031247
Iteration 5/25 | Loss: 0.00031247
Iteration 6/25 | Loss: 0.00031247
Iteration 7/25 | Loss: 0.00031247
Iteration 8/25 | Loss: 0.00031247
Iteration 9/25 | Loss: 0.00031247
Iteration 10/25 | Loss: 0.00031247
Iteration 11/25 | Loss: 0.00031247
Iteration 12/25 | Loss: 0.00031247
Iteration 13/25 | Loss: 0.00031247
Iteration 14/25 | Loss: 0.00031247
Iteration 15/25 | Loss: 0.00031247
Iteration 16/25 | Loss: 0.00031247
Iteration 17/25 | Loss: 0.00031247
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.00031247216975316405, 0.00031247216975316405, 0.00031247216975316405, 0.00031247216975316405, 0.00031247216975316405]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00031247216975316405

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00031247
Iteration 2/1000 | Loss: 0.00003711
Iteration 3/1000 | Loss: 0.00002502
Iteration 4/1000 | Loss: 0.00002162
Iteration 5/1000 | Loss: 0.00001999
Iteration 6/1000 | Loss: 0.00001884
Iteration 7/1000 | Loss: 0.00001827
Iteration 8/1000 | Loss: 0.00001780
Iteration 9/1000 | Loss: 0.00001744
Iteration 10/1000 | Loss: 0.00001715
Iteration 11/1000 | Loss: 0.00001695
Iteration 12/1000 | Loss: 0.00001678
Iteration 13/1000 | Loss: 0.00001672
Iteration 14/1000 | Loss: 0.00001669
Iteration 15/1000 | Loss: 0.00001668
Iteration 16/1000 | Loss: 0.00001668
Iteration 17/1000 | Loss: 0.00001667
Iteration 18/1000 | Loss: 0.00001666
Iteration 19/1000 | Loss: 0.00001665
Iteration 20/1000 | Loss: 0.00001665
Iteration 21/1000 | Loss: 0.00001664
Iteration 22/1000 | Loss: 0.00001664
Iteration 23/1000 | Loss: 0.00001662
Iteration 24/1000 | Loss: 0.00001662
Iteration 25/1000 | Loss: 0.00001661
Iteration 26/1000 | Loss: 0.00001660
Iteration 27/1000 | Loss: 0.00001660
Iteration 28/1000 | Loss: 0.00001659
Iteration 29/1000 | Loss: 0.00001659
Iteration 30/1000 | Loss: 0.00001658
Iteration 31/1000 | Loss: 0.00001658
Iteration 32/1000 | Loss: 0.00001658
Iteration 33/1000 | Loss: 0.00001657
Iteration 34/1000 | Loss: 0.00001657
Iteration 35/1000 | Loss: 0.00001656
Iteration 36/1000 | Loss: 0.00001654
Iteration 37/1000 | Loss: 0.00001654
Iteration 38/1000 | Loss: 0.00001654
Iteration 39/1000 | Loss: 0.00001654
Iteration 40/1000 | Loss: 0.00001654
Iteration 41/1000 | Loss: 0.00001652
Iteration 42/1000 | Loss: 0.00001651
Iteration 43/1000 | Loss: 0.00001651
Iteration 44/1000 | Loss: 0.00001651
Iteration 45/1000 | Loss: 0.00001651
Iteration 46/1000 | Loss: 0.00001650
Iteration 47/1000 | Loss: 0.00001650
Iteration 48/1000 | Loss: 0.00001650
Iteration 49/1000 | Loss: 0.00001650
Iteration 50/1000 | Loss: 0.00001649
Iteration 51/1000 | Loss: 0.00001649
Iteration 52/1000 | Loss: 0.00001649
Iteration 53/1000 | Loss: 0.00001649
Iteration 54/1000 | Loss: 0.00001648
Iteration 55/1000 | Loss: 0.00001648
Iteration 56/1000 | Loss: 0.00001648
Iteration 57/1000 | Loss: 0.00001648
Iteration 58/1000 | Loss: 0.00001648
Iteration 59/1000 | Loss: 0.00001648
Iteration 60/1000 | Loss: 0.00001647
Iteration 61/1000 | Loss: 0.00001647
Iteration 62/1000 | Loss: 0.00001647
Iteration 63/1000 | Loss: 0.00001647
Iteration 64/1000 | Loss: 0.00001646
Iteration 65/1000 | Loss: 0.00001646
Iteration 66/1000 | Loss: 0.00001646
Iteration 67/1000 | Loss: 0.00001646
Iteration 68/1000 | Loss: 0.00001646
Iteration 69/1000 | Loss: 0.00001646
Iteration 70/1000 | Loss: 0.00001646
Iteration 71/1000 | Loss: 0.00001646
Iteration 72/1000 | Loss: 0.00001645
Iteration 73/1000 | Loss: 0.00001645
Iteration 74/1000 | Loss: 0.00001645
Iteration 75/1000 | Loss: 0.00001645
Iteration 76/1000 | Loss: 0.00001644
Iteration 77/1000 | Loss: 0.00001644
Iteration 78/1000 | Loss: 0.00001644
Iteration 79/1000 | Loss: 0.00001644
Iteration 80/1000 | Loss: 0.00001644
Iteration 81/1000 | Loss: 0.00001644
Iteration 82/1000 | Loss: 0.00001644
Iteration 83/1000 | Loss: 0.00001644
Iteration 84/1000 | Loss: 0.00001644
Iteration 85/1000 | Loss: 0.00001643
Iteration 86/1000 | Loss: 0.00001643
Iteration 87/1000 | Loss: 0.00001643
Iteration 88/1000 | Loss: 0.00001642
Iteration 89/1000 | Loss: 0.00001642
Iteration 90/1000 | Loss: 0.00001642
Iteration 91/1000 | Loss: 0.00001642
Iteration 92/1000 | Loss: 0.00001642
Iteration 93/1000 | Loss: 0.00001641
Iteration 94/1000 | Loss: 0.00001641
Iteration 95/1000 | Loss: 0.00001641
Iteration 96/1000 | Loss: 0.00001641
Iteration 97/1000 | Loss: 0.00001641
Iteration 98/1000 | Loss: 0.00001641
Iteration 99/1000 | Loss: 0.00001641
Iteration 100/1000 | Loss: 0.00001641
Iteration 101/1000 | Loss: 0.00001641
Iteration 102/1000 | Loss: 0.00001641
Iteration 103/1000 | Loss: 0.00001641
Iteration 104/1000 | Loss: 0.00001641
Iteration 105/1000 | Loss: 0.00001641
Iteration 106/1000 | Loss: 0.00001641
Iteration 107/1000 | Loss: 0.00001641
Iteration 108/1000 | Loss: 0.00001641
Iteration 109/1000 | Loss: 0.00001640
Iteration 110/1000 | Loss: 0.00001640
Iteration 111/1000 | Loss: 0.00001640
Iteration 112/1000 | Loss: 0.00001640
Iteration 113/1000 | Loss: 0.00001640
Iteration 114/1000 | Loss: 0.00001640
Iteration 115/1000 | Loss: 0.00001639
Iteration 116/1000 | Loss: 0.00001639
Iteration 117/1000 | Loss: 0.00001639
Iteration 118/1000 | Loss: 0.00001639
Iteration 119/1000 | Loss: 0.00001639
Iteration 120/1000 | Loss: 0.00001639
Iteration 121/1000 | Loss: 0.00001639
Iteration 122/1000 | Loss: 0.00001639
Iteration 123/1000 | Loss: 0.00001639
Iteration 124/1000 | Loss: 0.00001639
Iteration 125/1000 | Loss: 0.00001639
Iteration 126/1000 | Loss: 0.00001639
Iteration 127/1000 | Loss: 0.00001639
Iteration 128/1000 | Loss: 0.00001639
Iteration 129/1000 | Loss: 0.00001639
Iteration 130/1000 | Loss: 0.00001639
Iteration 131/1000 | Loss: 0.00001639
Iteration 132/1000 | Loss: 0.00001639
Iteration 133/1000 | Loss: 0.00001639
Iteration 134/1000 | Loss: 0.00001639
Iteration 135/1000 | Loss: 0.00001639
Iteration 136/1000 | Loss: 0.00001639
Iteration 137/1000 | Loss: 0.00001639
Iteration 138/1000 | Loss: 0.00001639
Iteration 139/1000 | Loss: 0.00001639
Iteration 140/1000 | Loss: 0.00001639
Iteration 141/1000 | Loss: 0.00001639
Iteration 142/1000 | Loss: 0.00001639
Iteration 143/1000 | Loss: 0.00001639
Iteration 144/1000 | Loss: 0.00001639
Iteration 145/1000 | Loss: 0.00001639
Iteration 146/1000 | Loss: 0.00001639
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 146. Stopping optimization.
Last 5 losses: [1.638521825952921e-05, 1.638521825952921e-05, 1.638521825952921e-05, 1.638521825952921e-05, 1.638521825952921e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.638521825952921e-05

Optimization complete. Final v2v error: 3.3358757495880127 mm

Highest mean error: 4.0454325675964355 mm for frame 220

Lowest mean error: 2.919811248779297 mm for frame 161

Saving results

Total time: 41.750240087509155
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_006/1001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_006/1001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_006/1001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00910528
Iteration 2/25 | Loss: 0.00098876
Iteration 3/25 | Loss: 0.00078074
Iteration 4/25 | Loss: 0.00073496
Iteration 5/25 | Loss: 0.00071596
Iteration 6/25 | Loss: 0.00071061
Iteration 7/25 | Loss: 0.00070879
Iteration 8/25 | Loss: 0.00070836
Iteration 9/25 | Loss: 0.00070836
Iteration 10/25 | Loss: 0.00070836
Iteration 11/25 | Loss: 0.00070836
Iteration 12/25 | Loss: 0.00070836
Iteration 13/25 | Loss: 0.00070836
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.000708363892044872, 0.000708363892044872, 0.000708363892044872, 0.000708363892044872, 0.000708363892044872]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000708363892044872

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44247973
Iteration 2/25 | Loss: 0.00025040
Iteration 3/25 | Loss: 0.00025037
Iteration 4/25 | Loss: 0.00025037
Iteration 5/25 | Loss: 0.00025037
Iteration 6/25 | Loss: 0.00025037
Iteration 7/25 | Loss: 0.00025036
Iteration 8/25 | Loss: 0.00025036
Iteration 9/25 | Loss: 0.00025036
Iteration 10/25 | Loss: 0.00025036
Iteration 11/25 | Loss: 0.00025036
Iteration 12/25 | Loss: 0.00025036
Iteration 13/25 | Loss: 0.00025036
Iteration 14/25 | Loss: 0.00025036
Iteration 15/25 | Loss: 0.00025036
Iteration 16/25 | Loss: 0.00025036
Iteration 17/25 | Loss: 0.00025036
Iteration 18/25 | Loss: 0.00025036
Iteration 19/25 | Loss: 0.00025036
Iteration 20/25 | Loss: 0.00025036
Iteration 21/25 | Loss: 0.00025036
Iteration 22/25 | Loss: 0.00025036
Iteration 23/25 | Loss: 0.00025036
Iteration 24/25 | Loss: 0.00025036
Iteration 25/25 | Loss: 0.00025036

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00025036
Iteration 2/1000 | Loss: 0.00004810
Iteration 3/1000 | Loss: 0.00003494
Iteration 4/1000 | Loss: 0.00003126
Iteration 5/1000 | Loss: 0.00002963
Iteration 6/1000 | Loss: 0.00002832
Iteration 7/1000 | Loss: 0.00002736
Iteration 8/1000 | Loss: 0.00002674
Iteration 9/1000 | Loss: 0.00002621
Iteration 10/1000 | Loss: 0.00002593
Iteration 11/1000 | Loss: 0.00002574
Iteration 12/1000 | Loss: 0.00002554
Iteration 13/1000 | Loss: 0.00002545
Iteration 14/1000 | Loss: 0.00002544
Iteration 15/1000 | Loss: 0.00002534
Iteration 16/1000 | Loss: 0.00002531
Iteration 17/1000 | Loss: 0.00002531
Iteration 18/1000 | Loss: 0.00002528
Iteration 19/1000 | Loss: 0.00002525
Iteration 20/1000 | Loss: 0.00002525
Iteration 21/1000 | Loss: 0.00002523
Iteration 22/1000 | Loss: 0.00002523
Iteration 23/1000 | Loss: 0.00002522
Iteration 24/1000 | Loss: 0.00002521
Iteration 25/1000 | Loss: 0.00002520
Iteration 26/1000 | Loss: 0.00002520
Iteration 27/1000 | Loss: 0.00002519
Iteration 28/1000 | Loss: 0.00002519
Iteration 29/1000 | Loss: 0.00002518
Iteration 30/1000 | Loss: 0.00002518
Iteration 31/1000 | Loss: 0.00002517
Iteration 32/1000 | Loss: 0.00002517
Iteration 33/1000 | Loss: 0.00002516
Iteration 34/1000 | Loss: 0.00002516
Iteration 35/1000 | Loss: 0.00002515
Iteration 36/1000 | Loss: 0.00002515
Iteration 37/1000 | Loss: 0.00002515
Iteration 38/1000 | Loss: 0.00002515
Iteration 39/1000 | Loss: 0.00002514
Iteration 40/1000 | Loss: 0.00002514
Iteration 41/1000 | Loss: 0.00002514
Iteration 42/1000 | Loss: 0.00002514
Iteration 43/1000 | Loss: 0.00002514
Iteration 44/1000 | Loss: 0.00002514
Iteration 45/1000 | Loss: 0.00002514
Iteration 46/1000 | Loss: 0.00002514
Iteration 47/1000 | Loss: 0.00002514
Iteration 48/1000 | Loss: 0.00002514
Iteration 49/1000 | Loss: 0.00002513
Iteration 50/1000 | Loss: 0.00002513
Iteration 51/1000 | Loss: 0.00002513
Iteration 52/1000 | Loss: 0.00002513
Iteration 53/1000 | Loss: 0.00002512
Iteration 54/1000 | Loss: 0.00002512
Iteration 55/1000 | Loss: 0.00002512
Iteration 56/1000 | Loss: 0.00002512
Iteration 57/1000 | Loss: 0.00002511
Iteration 58/1000 | Loss: 0.00002511
Iteration 59/1000 | Loss: 0.00002511
Iteration 60/1000 | Loss: 0.00002511
Iteration 61/1000 | Loss: 0.00002511
Iteration 62/1000 | Loss: 0.00002511
Iteration 63/1000 | Loss: 0.00002510
Iteration 64/1000 | Loss: 0.00002510
Iteration 65/1000 | Loss: 0.00002510
Iteration 66/1000 | Loss: 0.00002509
Iteration 67/1000 | Loss: 0.00002509
Iteration 68/1000 | Loss: 0.00002509
Iteration 69/1000 | Loss: 0.00002508
Iteration 70/1000 | Loss: 0.00002508
Iteration 71/1000 | Loss: 0.00002508
Iteration 72/1000 | Loss: 0.00002508
Iteration 73/1000 | Loss: 0.00002508
Iteration 74/1000 | Loss: 0.00002507
Iteration 75/1000 | Loss: 0.00002507
Iteration 76/1000 | Loss: 0.00002507
Iteration 77/1000 | Loss: 0.00002507
Iteration 78/1000 | Loss: 0.00002507
Iteration 79/1000 | Loss: 0.00002507
Iteration 80/1000 | Loss: 0.00002507
Iteration 81/1000 | Loss: 0.00002507
Iteration 82/1000 | Loss: 0.00002507
Iteration 83/1000 | Loss: 0.00002506
Iteration 84/1000 | Loss: 0.00002506
Iteration 85/1000 | Loss: 0.00002506
Iteration 86/1000 | Loss: 0.00002506
Iteration 87/1000 | Loss: 0.00002506
Iteration 88/1000 | Loss: 0.00002506
Iteration 89/1000 | Loss: 0.00002505
Iteration 90/1000 | Loss: 0.00002505
Iteration 91/1000 | Loss: 0.00002505
Iteration 92/1000 | Loss: 0.00002505
Iteration 93/1000 | Loss: 0.00002505
Iteration 94/1000 | Loss: 0.00002504
Iteration 95/1000 | Loss: 0.00002504
Iteration 96/1000 | Loss: 0.00002504
Iteration 97/1000 | Loss: 0.00002504
Iteration 98/1000 | Loss: 0.00002504
Iteration 99/1000 | Loss: 0.00002504
Iteration 100/1000 | Loss: 0.00002504
Iteration 101/1000 | Loss: 0.00002504
Iteration 102/1000 | Loss: 0.00002504
Iteration 103/1000 | Loss: 0.00002504
Iteration 104/1000 | Loss: 0.00002503
Iteration 105/1000 | Loss: 0.00002503
Iteration 106/1000 | Loss: 0.00002503
Iteration 107/1000 | Loss: 0.00002503
Iteration 108/1000 | Loss: 0.00002503
Iteration 109/1000 | Loss: 0.00002502
Iteration 110/1000 | Loss: 0.00002502
Iteration 111/1000 | Loss: 0.00002502
Iteration 112/1000 | Loss: 0.00002501
Iteration 113/1000 | Loss: 0.00002501
Iteration 114/1000 | Loss: 0.00002501
Iteration 115/1000 | Loss: 0.00002501
Iteration 116/1000 | Loss: 0.00002501
Iteration 117/1000 | Loss: 0.00002501
Iteration 118/1000 | Loss: 0.00002500
Iteration 119/1000 | Loss: 0.00002500
Iteration 120/1000 | Loss: 0.00002500
Iteration 121/1000 | Loss: 0.00002499
Iteration 122/1000 | Loss: 0.00002499
Iteration 123/1000 | Loss: 0.00002499
Iteration 124/1000 | Loss: 0.00002499
Iteration 125/1000 | Loss: 0.00002499
Iteration 126/1000 | Loss: 0.00002499
Iteration 127/1000 | Loss: 0.00002499
Iteration 128/1000 | Loss: 0.00002499
Iteration 129/1000 | Loss: 0.00002499
Iteration 130/1000 | Loss: 0.00002499
Iteration 131/1000 | Loss: 0.00002499
Iteration 132/1000 | Loss: 0.00002499
Iteration 133/1000 | Loss: 0.00002499
Iteration 134/1000 | Loss: 0.00002499
Iteration 135/1000 | Loss: 0.00002499
Iteration 136/1000 | Loss: 0.00002499
Iteration 137/1000 | Loss: 0.00002499
Iteration 138/1000 | Loss: 0.00002499
Iteration 139/1000 | Loss: 0.00002499
Iteration 140/1000 | Loss: 0.00002499
Iteration 141/1000 | Loss: 0.00002499
Iteration 142/1000 | Loss: 0.00002499
Iteration 143/1000 | Loss: 0.00002499
Iteration 144/1000 | Loss: 0.00002499
Iteration 145/1000 | Loss: 0.00002499
Iteration 146/1000 | Loss: 0.00002499
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 146. Stopping optimization.
Last 5 losses: [2.4990131350932643e-05, 2.4990131350932643e-05, 2.4990131350932643e-05, 2.4990131350932643e-05, 2.4990131350932643e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4990131350932643e-05

Optimization complete. Final v2v error: 4.090284824371338 mm

Highest mean error: 5.730896949768066 mm for frame 67

Lowest mean error: 3.3681671619415283 mm for frame 97

Saving results

Total time: 39.302149534225464
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_006/1057/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_006/1057.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_006/1057
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00887281
Iteration 2/25 | Loss: 0.00079483
Iteration 3/25 | Loss: 0.00064310
Iteration 4/25 | Loss: 0.00062013
Iteration 5/25 | Loss: 0.00061256
Iteration 6/25 | Loss: 0.00061120
Iteration 7/25 | Loss: 0.00061084
Iteration 8/25 | Loss: 0.00061084
Iteration 9/25 | Loss: 0.00061084
Iteration 10/25 | Loss: 0.00061084
Iteration 11/25 | Loss: 0.00061084
Iteration 12/25 | Loss: 0.00061084
Iteration 13/25 | Loss: 0.00061084
Iteration 14/25 | Loss: 0.00061084
Iteration 15/25 | Loss: 0.00061084
Iteration 16/25 | Loss: 0.00061084
Iteration 17/25 | Loss: 0.00061084
Iteration 18/25 | Loss: 0.00061084
Iteration 19/25 | Loss: 0.00061084
Iteration 20/25 | Loss: 0.00061084
Iteration 21/25 | Loss: 0.00061084
Iteration 22/25 | Loss: 0.00061084
Iteration 23/25 | Loss: 0.00061084
Iteration 24/25 | Loss: 0.00061084
Iteration 25/25 | Loss: 0.00061084

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.10327744
Iteration 2/25 | Loss: 0.00030945
Iteration 3/25 | Loss: 0.00030945
Iteration 4/25 | Loss: 0.00030945
Iteration 5/25 | Loss: 0.00030945
Iteration 6/25 | Loss: 0.00030945
Iteration 7/25 | Loss: 0.00030945
Iteration 8/25 | Loss: 0.00030945
Iteration 9/25 | Loss: 0.00030945
Iteration 10/25 | Loss: 0.00030945
Iteration 11/25 | Loss: 0.00030945
Iteration 12/25 | Loss: 0.00030945
Iteration 13/25 | Loss: 0.00030945
Iteration 14/25 | Loss: 0.00030945
Iteration 15/25 | Loss: 0.00030945
Iteration 16/25 | Loss: 0.00030945
Iteration 17/25 | Loss: 0.00030945
Iteration 18/25 | Loss: 0.00030945
Iteration 19/25 | Loss: 0.00030945
Iteration 20/25 | Loss: 0.00030945
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.00030944618629291654, 0.00030944618629291654, 0.00030944618629291654, 0.00030944618629291654, 0.00030944618629291654]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00030944618629291654

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00030945
Iteration 2/1000 | Loss: 0.00002663
Iteration 3/1000 | Loss: 0.00001707
Iteration 4/1000 | Loss: 0.00001470
Iteration 5/1000 | Loss: 0.00001397
Iteration 6/1000 | Loss: 0.00001366
Iteration 7/1000 | Loss: 0.00001335
Iteration 8/1000 | Loss: 0.00001309
Iteration 9/1000 | Loss: 0.00001304
Iteration 10/1000 | Loss: 0.00001301
Iteration 11/1000 | Loss: 0.00001300
Iteration 12/1000 | Loss: 0.00001300
Iteration 13/1000 | Loss: 0.00001300
Iteration 14/1000 | Loss: 0.00001299
Iteration 15/1000 | Loss: 0.00001299
Iteration 16/1000 | Loss: 0.00001298
Iteration 17/1000 | Loss: 0.00001293
Iteration 18/1000 | Loss: 0.00001287
Iteration 19/1000 | Loss: 0.00001281
Iteration 20/1000 | Loss: 0.00001281
Iteration 21/1000 | Loss: 0.00001281
Iteration 22/1000 | Loss: 0.00001278
Iteration 23/1000 | Loss: 0.00001278
Iteration 24/1000 | Loss: 0.00001277
Iteration 25/1000 | Loss: 0.00001277
Iteration 26/1000 | Loss: 0.00001273
Iteration 27/1000 | Loss: 0.00001273
Iteration 28/1000 | Loss: 0.00001273
Iteration 29/1000 | Loss: 0.00001269
Iteration 30/1000 | Loss: 0.00001269
Iteration 31/1000 | Loss: 0.00001268
Iteration 32/1000 | Loss: 0.00001268
Iteration 33/1000 | Loss: 0.00001267
Iteration 34/1000 | Loss: 0.00001264
Iteration 35/1000 | Loss: 0.00001264
Iteration 36/1000 | Loss: 0.00001263
Iteration 37/1000 | Loss: 0.00001263
Iteration 38/1000 | Loss: 0.00001263
Iteration 39/1000 | Loss: 0.00001259
Iteration 40/1000 | Loss: 0.00001259
Iteration 41/1000 | Loss: 0.00001259
Iteration 42/1000 | Loss: 0.00001259
Iteration 43/1000 | Loss: 0.00001259
Iteration 44/1000 | Loss: 0.00001259
Iteration 45/1000 | Loss: 0.00001259
Iteration 46/1000 | Loss: 0.00001259
Iteration 47/1000 | Loss: 0.00001259
Iteration 48/1000 | Loss: 0.00001259
Iteration 49/1000 | Loss: 0.00001258
Iteration 50/1000 | Loss: 0.00001258
Iteration 51/1000 | Loss: 0.00001256
Iteration 52/1000 | Loss: 0.00001256
Iteration 53/1000 | Loss: 0.00001256
Iteration 54/1000 | Loss: 0.00001256
Iteration 55/1000 | Loss: 0.00001256
Iteration 56/1000 | Loss: 0.00001256
Iteration 57/1000 | Loss: 0.00001256
Iteration 58/1000 | Loss: 0.00001256
Iteration 59/1000 | Loss: 0.00001255
Iteration 60/1000 | Loss: 0.00001255
Iteration 61/1000 | Loss: 0.00001255
Iteration 62/1000 | Loss: 0.00001255
Iteration 63/1000 | Loss: 0.00001255
Iteration 64/1000 | Loss: 0.00001255
Iteration 65/1000 | Loss: 0.00001255
Iteration 66/1000 | Loss: 0.00001255
Iteration 67/1000 | Loss: 0.00001255
Iteration 68/1000 | Loss: 0.00001255
Iteration 69/1000 | Loss: 0.00001255
Iteration 70/1000 | Loss: 0.00001255
Iteration 71/1000 | Loss: 0.00001255
Iteration 72/1000 | Loss: 0.00001254
Iteration 73/1000 | Loss: 0.00001254
Iteration 74/1000 | Loss: 0.00001254
Iteration 75/1000 | Loss: 0.00001254
Iteration 76/1000 | Loss: 0.00001254
Iteration 77/1000 | Loss: 0.00001254
Iteration 78/1000 | Loss: 0.00001254
Iteration 79/1000 | Loss: 0.00001253
Iteration 80/1000 | Loss: 0.00001253
Iteration 81/1000 | Loss: 0.00001253
Iteration 82/1000 | Loss: 0.00001253
Iteration 83/1000 | Loss: 0.00001252
Iteration 84/1000 | Loss: 0.00001252
Iteration 85/1000 | Loss: 0.00001252
Iteration 86/1000 | Loss: 0.00001252
Iteration 87/1000 | Loss: 0.00001252
Iteration 88/1000 | Loss: 0.00001251
Iteration 89/1000 | Loss: 0.00001251
Iteration 90/1000 | Loss: 0.00001251
Iteration 91/1000 | Loss: 0.00001251
Iteration 92/1000 | Loss: 0.00001251
Iteration 93/1000 | Loss: 0.00001251
Iteration 94/1000 | Loss: 0.00001251
Iteration 95/1000 | Loss: 0.00001250
Iteration 96/1000 | Loss: 0.00001250
Iteration 97/1000 | Loss: 0.00001250
Iteration 98/1000 | Loss: 0.00001249
Iteration 99/1000 | Loss: 0.00001249
Iteration 100/1000 | Loss: 0.00001249
Iteration 101/1000 | Loss: 0.00001248
Iteration 102/1000 | Loss: 0.00001248
Iteration 103/1000 | Loss: 0.00001247
Iteration 104/1000 | Loss: 0.00001247
Iteration 105/1000 | Loss: 0.00001247
Iteration 106/1000 | Loss: 0.00001247
Iteration 107/1000 | Loss: 0.00001247
Iteration 108/1000 | Loss: 0.00001247
Iteration 109/1000 | Loss: 0.00001247
Iteration 110/1000 | Loss: 0.00001246
Iteration 111/1000 | Loss: 0.00001246
Iteration 112/1000 | Loss: 0.00001246
Iteration 113/1000 | Loss: 0.00001246
Iteration 114/1000 | Loss: 0.00001246
Iteration 115/1000 | Loss: 0.00001246
Iteration 116/1000 | Loss: 0.00001246
Iteration 117/1000 | Loss: 0.00001246
Iteration 118/1000 | Loss: 0.00001246
Iteration 119/1000 | Loss: 0.00001246
Iteration 120/1000 | Loss: 0.00001246
Iteration 121/1000 | Loss: 0.00001246
Iteration 122/1000 | Loss: 0.00001246
Iteration 123/1000 | Loss: 0.00001245
Iteration 124/1000 | Loss: 0.00001245
Iteration 125/1000 | Loss: 0.00001245
Iteration 126/1000 | Loss: 0.00001244
Iteration 127/1000 | Loss: 0.00001244
Iteration 128/1000 | Loss: 0.00001244
Iteration 129/1000 | Loss: 0.00001244
Iteration 130/1000 | Loss: 0.00001244
Iteration 131/1000 | Loss: 0.00001244
Iteration 132/1000 | Loss: 0.00001244
Iteration 133/1000 | Loss: 0.00001244
Iteration 134/1000 | Loss: 0.00001244
Iteration 135/1000 | Loss: 0.00001244
Iteration 136/1000 | Loss: 0.00001244
Iteration 137/1000 | Loss: 0.00001243
Iteration 138/1000 | Loss: 0.00001243
Iteration 139/1000 | Loss: 0.00001243
Iteration 140/1000 | Loss: 0.00001243
Iteration 141/1000 | Loss: 0.00001243
Iteration 142/1000 | Loss: 0.00001243
Iteration 143/1000 | Loss: 0.00001243
Iteration 144/1000 | Loss: 0.00001243
Iteration 145/1000 | Loss: 0.00001243
Iteration 146/1000 | Loss: 0.00001243
Iteration 147/1000 | Loss: 0.00001243
Iteration 148/1000 | Loss: 0.00001243
Iteration 149/1000 | Loss: 0.00001243
Iteration 150/1000 | Loss: 0.00001243
Iteration 151/1000 | Loss: 0.00001243
Iteration 152/1000 | Loss: 0.00001243
Iteration 153/1000 | Loss: 0.00001242
Iteration 154/1000 | Loss: 0.00001242
Iteration 155/1000 | Loss: 0.00001242
Iteration 156/1000 | Loss: 0.00001242
Iteration 157/1000 | Loss: 0.00001242
Iteration 158/1000 | Loss: 0.00001242
Iteration 159/1000 | Loss: 0.00001242
Iteration 160/1000 | Loss: 0.00001242
Iteration 161/1000 | Loss: 0.00001242
Iteration 162/1000 | Loss: 0.00001242
Iteration 163/1000 | Loss: 0.00001241
Iteration 164/1000 | Loss: 0.00001241
Iteration 165/1000 | Loss: 0.00001241
Iteration 166/1000 | Loss: 0.00001241
Iteration 167/1000 | Loss: 0.00001241
Iteration 168/1000 | Loss: 0.00001241
Iteration 169/1000 | Loss: 0.00001241
Iteration 170/1000 | Loss: 0.00001241
Iteration 171/1000 | Loss: 0.00001241
Iteration 172/1000 | Loss: 0.00001241
Iteration 173/1000 | Loss: 0.00001241
Iteration 174/1000 | Loss: 0.00001241
Iteration 175/1000 | Loss: 0.00001241
Iteration 176/1000 | Loss: 0.00001241
Iteration 177/1000 | Loss: 0.00001241
Iteration 178/1000 | Loss: 0.00001241
Iteration 179/1000 | Loss: 0.00001241
Iteration 180/1000 | Loss: 0.00001241
Iteration 181/1000 | Loss: 0.00001241
Iteration 182/1000 | Loss: 0.00001241
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 182. Stopping optimization.
Last 5 losses: [1.2407599569996819e-05, 1.2407599569996819e-05, 1.2407599569996819e-05, 1.2407599569996819e-05, 1.2407599569996819e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2407599569996819e-05

Optimization complete. Final v2v error: 2.94909930229187 mm

Highest mean error: 3.611515522003174 mm for frame 51

Lowest mean error: 2.767359733581543 mm for frame 12

Saving results

Total time: 36.00361680984497
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_006/1058/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_006/1058.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_006/1058
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00542743
Iteration 2/25 | Loss: 0.00098363
Iteration 3/25 | Loss: 0.00071838
Iteration 4/25 | Loss: 0.00067236
Iteration 5/25 | Loss: 0.00066190
Iteration 6/25 | Loss: 0.00065878
Iteration 7/25 | Loss: 0.00065817
Iteration 8/25 | Loss: 0.00065813
Iteration 9/25 | Loss: 0.00065813
Iteration 10/25 | Loss: 0.00065813
Iteration 11/25 | Loss: 0.00065813
Iteration 12/25 | Loss: 0.00065813
Iteration 13/25 | Loss: 0.00065813
Iteration 14/25 | Loss: 0.00065813
Iteration 15/25 | Loss: 0.00065813
Iteration 16/25 | Loss: 0.00065813
Iteration 17/25 | Loss: 0.00065813
Iteration 18/25 | Loss: 0.00065813
Iteration 19/25 | Loss: 0.00065813
Iteration 20/25 | Loss: 0.00065813
Iteration 21/25 | Loss: 0.00065813
Iteration 22/25 | Loss: 0.00065813
Iteration 23/25 | Loss: 0.00065813
Iteration 24/25 | Loss: 0.00065813
Iteration 25/25 | Loss: 0.00065813
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0006581301568076015, 0.0006581301568076015, 0.0006581301568076015, 0.0006581301568076015, 0.0006581301568076015]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006581301568076015

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.53368568
Iteration 2/25 | Loss: 0.00027806
Iteration 3/25 | Loss: 0.00027805
Iteration 4/25 | Loss: 0.00027805
Iteration 5/25 | Loss: 0.00027805
Iteration 6/25 | Loss: 0.00027805
Iteration 7/25 | Loss: 0.00027805
Iteration 8/25 | Loss: 0.00027805
Iteration 9/25 | Loss: 0.00027805
Iteration 10/25 | Loss: 0.00027805
Iteration 11/25 | Loss: 0.00027805
Iteration 12/25 | Loss: 0.00027805
Iteration 13/25 | Loss: 0.00027805
Iteration 14/25 | Loss: 0.00027805
Iteration 15/25 | Loss: 0.00027805
Iteration 16/25 | Loss: 0.00027805
Iteration 17/25 | Loss: 0.00027805
Iteration 18/25 | Loss: 0.00027805
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0002780459763016552, 0.0002780459763016552, 0.0002780459763016552, 0.0002780459763016552, 0.0002780459763016552]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0002780459763016552

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00027805
Iteration 2/1000 | Loss: 0.00003180
Iteration 3/1000 | Loss: 0.00002211
Iteration 4/1000 | Loss: 0.00001944
Iteration 5/1000 | Loss: 0.00001851
Iteration 6/1000 | Loss: 0.00001792
Iteration 7/1000 | Loss: 0.00001760
Iteration 8/1000 | Loss: 0.00001727
Iteration 9/1000 | Loss: 0.00001707
Iteration 10/1000 | Loss: 0.00001699
Iteration 11/1000 | Loss: 0.00001684
Iteration 12/1000 | Loss: 0.00001679
Iteration 13/1000 | Loss: 0.00001675
Iteration 14/1000 | Loss: 0.00001674
Iteration 15/1000 | Loss: 0.00001668
Iteration 16/1000 | Loss: 0.00001668
Iteration 17/1000 | Loss: 0.00001667
Iteration 18/1000 | Loss: 0.00001663
Iteration 19/1000 | Loss: 0.00001662
Iteration 20/1000 | Loss: 0.00001662
Iteration 21/1000 | Loss: 0.00001661
Iteration 22/1000 | Loss: 0.00001660
Iteration 23/1000 | Loss: 0.00001653
Iteration 24/1000 | Loss: 0.00001650
Iteration 25/1000 | Loss: 0.00001649
Iteration 26/1000 | Loss: 0.00001649
Iteration 27/1000 | Loss: 0.00001649
Iteration 28/1000 | Loss: 0.00001649
Iteration 29/1000 | Loss: 0.00001648
Iteration 30/1000 | Loss: 0.00001647
Iteration 31/1000 | Loss: 0.00001646
Iteration 32/1000 | Loss: 0.00001646
Iteration 33/1000 | Loss: 0.00001645
Iteration 34/1000 | Loss: 0.00001644
Iteration 35/1000 | Loss: 0.00001644
Iteration 36/1000 | Loss: 0.00001644
Iteration 37/1000 | Loss: 0.00001643
Iteration 38/1000 | Loss: 0.00001643
Iteration 39/1000 | Loss: 0.00001642
Iteration 40/1000 | Loss: 0.00001641
Iteration 41/1000 | Loss: 0.00001641
Iteration 42/1000 | Loss: 0.00001641
Iteration 43/1000 | Loss: 0.00001640
Iteration 44/1000 | Loss: 0.00001640
Iteration 45/1000 | Loss: 0.00001640
Iteration 46/1000 | Loss: 0.00001640
Iteration 47/1000 | Loss: 0.00001640
Iteration 48/1000 | Loss: 0.00001640
Iteration 49/1000 | Loss: 0.00001640
Iteration 50/1000 | Loss: 0.00001639
Iteration 51/1000 | Loss: 0.00001639
Iteration 52/1000 | Loss: 0.00001638
Iteration 53/1000 | Loss: 0.00001638
Iteration 54/1000 | Loss: 0.00001638
Iteration 55/1000 | Loss: 0.00001637
Iteration 56/1000 | Loss: 0.00001637
Iteration 57/1000 | Loss: 0.00001637
Iteration 58/1000 | Loss: 0.00001637
Iteration 59/1000 | Loss: 0.00001637
Iteration 60/1000 | Loss: 0.00001637
Iteration 61/1000 | Loss: 0.00001636
Iteration 62/1000 | Loss: 0.00001636
Iteration 63/1000 | Loss: 0.00001636
Iteration 64/1000 | Loss: 0.00001636
Iteration 65/1000 | Loss: 0.00001636
Iteration 66/1000 | Loss: 0.00001635
Iteration 67/1000 | Loss: 0.00001635
Iteration 68/1000 | Loss: 0.00001635
Iteration 69/1000 | Loss: 0.00001634
Iteration 70/1000 | Loss: 0.00001634
Iteration 71/1000 | Loss: 0.00001634
Iteration 72/1000 | Loss: 0.00001633
Iteration 73/1000 | Loss: 0.00001633
Iteration 74/1000 | Loss: 0.00001633
Iteration 75/1000 | Loss: 0.00001633
Iteration 76/1000 | Loss: 0.00001632
Iteration 77/1000 | Loss: 0.00001632
Iteration 78/1000 | Loss: 0.00001632
Iteration 79/1000 | Loss: 0.00001632
Iteration 80/1000 | Loss: 0.00001632
Iteration 81/1000 | Loss: 0.00001631
Iteration 82/1000 | Loss: 0.00001631
Iteration 83/1000 | Loss: 0.00001631
Iteration 84/1000 | Loss: 0.00001631
Iteration 85/1000 | Loss: 0.00001630
Iteration 86/1000 | Loss: 0.00001630
Iteration 87/1000 | Loss: 0.00001630
Iteration 88/1000 | Loss: 0.00001630
Iteration 89/1000 | Loss: 0.00001630
Iteration 90/1000 | Loss: 0.00001630
Iteration 91/1000 | Loss: 0.00001630
Iteration 92/1000 | Loss: 0.00001629
Iteration 93/1000 | Loss: 0.00001629
Iteration 94/1000 | Loss: 0.00001629
Iteration 95/1000 | Loss: 0.00001629
Iteration 96/1000 | Loss: 0.00001629
Iteration 97/1000 | Loss: 0.00001629
Iteration 98/1000 | Loss: 0.00001629
Iteration 99/1000 | Loss: 0.00001629
Iteration 100/1000 | Loss: 0.00001629
Iteration 101/1000 | Loss: 0.00001629
Iteration 102/1000 | Loss: 0.00001629
Iteration 103/1000 | Loss: 0.00001628
Iteration 104/1000 | Loss: 0.00001628
Iteration 105/1000 | Loss: 0.00001628
Iteration 106/1000 | Loss: 0.00001628
Iteration 107/1000 | Loss: 0.00001628
Iteration 108/1000 | Loss: 0.00001628
Iteration 109/1000 | Loss: 0.00001628
Iteration 110/1000 | Loss: 0.00001628
Iteration 111/1000 | Loss: 0.00001628
Iteration 112/1000 | Loss: 0.00001628
Iteration 113/1000 | Loss: 0.00001628
Iteration 114/1000 | Loss: 0.00001628
Iteration 115/1000 | Loss: 0.00001627
Iteration 116/1000 | Loss: 0.00001627
Iteration 117/1000 | Loss: 0.00001627
Iteration 118/1000 | Loss: 0.00001627
Iteration 119/1000 | Loss: 0.00001627
Iteration 120/1000 | Loss: 0.00001627
Iteration 121/1000 | Loss: 0.00001627
Iteration 122/1000 | Loss: 0.00001627
Iteration 123/1000 | Loss: 0.00001627
Iteration 124/1000 | Loss: 0.00001626
Iteration 125/1000 | Loss: 0.00001626
Iteration 126/1000 | Loss: 0.00001626
Iteration 127/1000 | Loss: 0.00001626
Iteration 128/1000 | Loss: 0.00001626
Iteration 129/1000 | Loss: 0.00001626
Iteration 130/1000 | Loss: 0.00001626
Iteration 131/1000 | Loss: 0.00001626
Iteration 132/1000 | Loss: 0.00001626
Iteration 133/1000 | Loss: 0.00001626
Iteration 134/1000 | Loss: 0.00001626
Iteration 135/1000 | Loss: 0.00001626
Iteration 136/1000 | Loss: 0.00001626
Iteration 137/1000 | Loss: 0.00001626
Iteration 138/1000 | Loss: 0.00001626
Iteration 139/1000 | Loss: 0.00001626
Iteration 140/1000 | Loss: 0.00001626
Iteration 141/1000 | Loss: 0.00001625
Iteration 142/1000 | Loss: 0.00001625
Iteration 143/1000 | Loss: 0.00001625
Iteration 144/1000 | Loss: 0.00001625
Iteration 145/1000 | Loss: 0.00001625
Iteration 146/1000 | Loss: 0.00001625
Iteration 147/1000 | Loss: 0.00001625
Iteration 148/1000 | Loss: 0.00001625
Iteration 149/1000 | Loss: 0.00001625
Iteration 150/1000 | Loss: 0.00001625
Iteration 151/1000 | Loss: 0.00001625
Iteration 152/1000 | Loss: 0.00001625
Iteration 153/1000 | Loss: 0.00001625
Iteration 154/1000 | Loss: 0.00001625
Iteration 155/1000 | Loss: 0.00001625
Iteration 156/1000 | Loss: 0.00001625
Iteration 157/1000 | Loss: 0.00001625
Iteration 158/1000 | Loss: 0.00001625
Iteration 159/1000 | Loss: 0.00001624
Iteration 160/1000 | Loss: 0.00001624
Iteration 161/1000 | Loss: 0.00001624
Iteration 162/1000 | Loss: 0.00001624
Iteration 163/1000 | Loss: 0.00001624
Iteration 164/1000 | Loss: 0.00001624
Iteration 165/1000 | Loss: 0.00001624
Iteration 166/1000 | Loss: 0.00001624
Iteration 167/1000 | Loss: 0.00001624
Iteration 168/1000 | Loss: 0.00001624
Iteration 169/1000 | Loss: 0.00001624
Iteration 170/1000 | Loss: 0.00001624
Iteration 171/1000 | Loss: 0.00001624
Iteration 172/1000 | Loss: 0.00001624
Iteration 173/1000 | Loss: 0.00001624
Iteration 174/1000 | Loss: 0.00001624
Iteration 175/1000 | Loss: 0.00001624
Iteration 176/1000 | Loss: 0.00001624
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 176. Stopping optimization.
Last 5 losses: [1.6235750081250444e-05, 1.6235750081250444e-05, 1.6235750081250444e-05, 1.6235750081250444e-05, 1.6235750081250444e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6235750081250444e-05

Optimization complete. Final v2v error: 3.424405574798584 mm

Highest mean error: 3.7182717323303223 mm for frame 0

Lowest mean error: 2.998385190963745 mm for frame 122

Saving results

Total time: 39.63026022911072
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_006/1091/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_006/1091.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_006/1091
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00904993
Iteration 2/25 | Loss: 0.00138640
Iteration 3/25 | Loss: 0.00092343
Iteration 4/25 | Loss: 0.00085177
Iteration 5/25 | Loss: 0.00083748
Iteration 6/25 | Loss: 0.00083687
Iteration 7/25 | Loss: 0.00083854
Iteration 8/25 | Loss: 0.00082338
Iteration 9/25 | Loss: 0.00080893
Iteration 10/25 | Loss: 0.00080194
Iteration 11/25 | Loss: 0.00079847
Iteration 12/25 | Loss: 0.00079354
Iteration 13/25 | Loss: 0.00079183
Iteration 14/25 | Loss: 0.00078925
Iteration 15/25 | Loss: 0.00078638
Iteration 16/25 | Loss: 0.00078413
Iteration 17/25 | Loss: 0.00078325
Iteration 18/25 | Loss: 0.00078292
Iteration 19/25 | Loss: 0.00078285
Iteration 20/25 | Loss: 0.00078285
Iteration 21/25 | Loss: 0.00078285
Iteration 22/25 | Loss: 0.00078285
Iteration 23/25 | Loss: 0.00078285
Iteration 24/25 | Loss: 0.00078285
Iteration 25/25 | Loss: 0.00078284

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.99715668
Iteration 2/25 | Loss: 0.00028687
Iteration 3/25 | Loss: 0.00028686
Iteration 4/25 | Loss: 0.00028686
Iteration 5/25 | Loss: 0.00028686
Iteration 6/25 | Loss: 0.00028686
Iteration 7/25 | Loss: 0.00028686
Iteration 8/25 | Loss: 0.00028686
Iteration 9/25 | Loss: 0.00028686
Iteration 10/25 | Loss: 0.00028686
Iteration 11/25 | Loss: 0.00028686
Iteration 12/25 | Loss: 0.00028686
Iteration 13/25 | Loss: 0.00028686
Iteration 14/25 | Loss: 0.00028686
Iteration 15/25 | Loss: 0.00028686
Iteration 16/25 | Loss: 0.00028686
Iteration 17/25 | Loss: 0.00028686
Iteration 18/25 | Loss: 0.00028686
Iteration 19/25 | Loss: 0.00028686
Iteration 20/25 | Loss: 0.00028686
Iteration 21/25 | Loss: 0.00028686
Iteration 22/25 | Loss: 0.00028686
Iteration 23/25 | Loss: 0.00028686
Iteration 24/25 | Loss: 0.00028686
Iteration 25/25 | Loss: 0.00028686

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00028686
Iteration 2/1000 | Loss: 0.00004159
Iteration 3/1000 | Loss: 0.00003462
Iteration 4/1000 | Loss: 0.00003274
Iteration 5/1000 | Loss: 0.00003099
Iteration 6/1000 | Loss: 0.00002975
Iteration 7/1000 | Loss: 0.00002883
Iteration 8/1000 | Loss: 0.00002841
Iteration 9/1000 | Loss: 0.00002816
Iteration 10/1000 | Loss: 0.00002800
Iteration 11/1000 | Loss: 0.00002795
Iteration 12/1000 | Loss: 0.00002792
Iteration 13/1000 | Loss: 0.00002790
Iteration 14/1000 | Loss: 0.00002789
Iteration 15/1000 | Loss: 0.00002789
Iteration 16/1000 | Loss: 0.00002789
Iteration 17/1000 | Loss: 0.00002789
Iteration 18/1000 | Loss: 0.00002789
Iteration 19/1000 | Loss: 0.00002789
Iteration 20/1000 | Loss: 0.00002789
Iteration 21/1000 | Loss: 0.00002789
Iteration 22/1000 | Loss: 0.00002789
Iteration 23/1000 | Loss: 0.00002789
Iteration 24/1000 | Loss: 0.00002788
Iteration 25/1000 | Loss: 0.00002788
Iteration 26/1000 | Loss: 0.00002788
Iteration 27/1000 | Loss: 0.00002785
Iteration 28/1000 | Loss: 0.00002785
Iteration 29/1000 | Loss: 0.00002783
Iteration 30/1000 | Loss: 0.00002783
Iteration 31/1000 | Loss: 0.00002783
Iteration 32/1000 | Loss: 0.00002782
Iteration 33/1000 | Loss: 0.00002782
Iteration 34/1000 | Loss: 0.00002782
Iteration 35/1000 | Loss: 0.00002782
Iteration 36/1000 | Loss: 0.00002782
Iteration 37/1000 | Loss: 0.00002782
Iteration 38/1000 | Loss: 0.00002782
Iteration 39/1000 | Loss: 0.00002782
Iteration 40/1000 | Loss: 0.00002781
Iteration 41/1000 | Loss: 0.00002781
Iteration 42/1000 | Loss: 0.00002780
Iteration 43/1000 | Loss: 0.00002780
Iteration 44/1000 | Loss: 0.00002780
Iteration 45/1000 | Loss: 0.00002779
Iteration 46/1000 | Loss: 0.00002779
Iteration 47/1000 | Loss: 0.00002778
Iteration 48/1000 | Loss: 0.00002778
Iteration 49/1000 | Loss: 0.00002778
Iteration 50/1000 | Loss: 0.00002777
Iteration 51/1000 | Loss: 0.00002777
Iteration 52/1000 | Loss: 0.00002776
Iteration 53/1000 | Loss: 0.00002776
Iteration 54/1000 | Loss: 0.00002773
Iteration 55/1000 | Loss: 0.00002772
Iteration 56/1000 | Loss: 0.00002772
Iteration 57/1000 | Loss: 0.00002772
Iteration 58/1000 | Loss: 0.00002772
Iteration 59/1000 | Loss: 0.00002772
Iteration 60/1000 | Loss: 0.00002772
Iteration 61/1000 | Loss: 0.00002772
Iteration 62/1000 | Loss: 0.00002772
Iteration 63/1000 | Loss: 0.00002772
Iteration 64/1000 | Loss: 0.00002772
Iteration 65/1000 | Loss: 0.00002772
Iteration 66/1000 | Loss: 0.00002772
Iteration 67/1000 | Loss: 0.00002772
Iteration 68/1000 | Loss: 0.00002772
Iteration 69/1000 | Loss: 0.00002772
Iteration 70/1000 | Loss: 0.00002772
Iteration 71/1000 | Loss: 0.00002772
Iteration 72/1000 | Loss: 0.00002772
Iteration 73/1000 | Loss: 0.00002772
Iteration 74/1000 | Loss: 0.00002772
Iteration 75/1000 | Loss: 0.00002772
Iteration 76/1000 | Loss: 0.00002772
Iteration 77/1000 | Loss: 0.00002772
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 77. Stopping optimization.
Last 5 losses: [2.7715797841665335e-05, 2.7715797841665335e-05, 2.7715797841665335e-05, 2.7715797841665335e-05, 2.7715797841665335e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.7715797841665335e-05

Optimization complete. Final v2v error: 4.417590618133545 mm

Highest mean error: 4.643644332885742 mm for frame 42

Lowest mean error: 4.279597759246826 mm for frame 117

Saving results

Total time: 52.114264726638794
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_006/1007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_006/1007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_006/1007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00445477
Iteration 2/25 | Loss: 0.00110790
Iteration 3/25 | Loss: 0.00079920
Iteration 4/25 | Loss: 0.00075256
Iteration 5/25 | Loss: 0.00074127
Iteration 6/25 | Loss: 0.00073874
Iteration 7/25 | Loss: 0.00073778
Iteration 8/25 | Loss: 0.00073763
Iteration 9/25 | Loss: 0.00073763
Iteration 10/25 | Loss: 0.00073763
Iteration 11/25 | Loss: 0.00073763
Iteration 12/25 | Loss: 0.00073763
Iteration 13/25 | Loss: 0.00073763
Iteration 14/25 | Loss: 0.00073763
Iteration 15/25 | Loss: 0.00073763
Iteration 16/25 | Loss: 0.00073763
Iteration 17/25 | Loss: 0.00073763
Iteration 18/25 | Loss: 0.00073763
Iteration 19/25 | Loss: 0.00073763
Iteration 20/25 | Loss: 0.00073763
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0007376308203674853, 0.0007376308203674853, 0.0007376308203674853, 0.0007376308203674853, 0.0007376308203674853]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007376308203674853

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.20850754
Iteration 2/25 | Loss: 0.00025640
Iteration 3/25 | Loss: 0.00025638
Iteration 4/25 | Loss: 0.00025638
Iteration 5/25 | Loss: 0.00025638
Iteration 6/25 | Loss: 0.00025638
Iteration 7/25 | Loss: 0.00025638
Iteration 8/25 | Loss: 0.00025638
Iteration 9/25 | Loss: 0.00025638
Iteration 10/25 | Loss: 0.00025638
Iteration 11/25 | Loss: 0.00025638
Iteration 12/25 | Loss: 0.00025638
Iteration 13/25 | Loss: 0.00025638
Iteration 14/25 | Loss: 0.00025638
Iteration 15/25 | Loss: 0.00025638
Iteration 16/25 | Loss: 0.00025638
Iteration 17/25 | Loss: 0.00025638
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0002563791931606829, 0.0002563791931606829, 0.0002563791931606829, 0.0002563791931606829, 0.0002563791931606829]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0002563791931606829

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00025638
Iteration 2/1000 | Loss: 0.00006260
Iteration 3/1000 | Loss: 0.00004697
Iteration 4/1000 | Loss: 0.00003880
Iteration 5/1000 | Loss: 0.00003625
Iteration 6/1000 | Loss: 0.00003495
Iteration 7/1000 | Loss: 0.00003385
Iteration 8/1000 | Loss: 0.00003326
Iteration 9/1000 | Loss: 0.00003262
Iteration 10/1000 | Loss: 0.00003213
Iteration 11/1000 | Loss: 0.00003180
Iteration 12/1000 | Loss: 0.00003156
Iteration 13/1000 | Loss: 0.00003134
Iteration 14/1000 | Loss: 0.00003117
Iteration 15/1000 | Loss: 0.00003116
Iteration 16/1000 | Loss: 0.00003112
Iteration 17/1000 | Loss: 0.00003109
Iteration 18/1000 | Loss: 0.00003094
Iteration 19/1000 | Loss: 0.00003093
Iteration 20/1000 | Loss: 0.00003084
Iteration 21/1000 | Loss: 0.00003082
Iteration 22/1000 | Loss: 0.00003082
Iteration 23/1000 | Loss: 0.00003080
Iteration 24/1000 | Loss: 0.00003079
Iteration 25/1000 | Loss: 0.00003079
Iteration 26/1000 | Loss: 0.00003078
Iteration 27/1000 | Loss: 0.00003078
Iteration 28/1000 | Loss: 0.00003078
Iteration 29/1000 | Loss: 0.00003077
Iteration 30/1000 | Loss: 0.00003077
Iteration 31/1000 | Loss: 0.00003077
Iteration 32/1000 | Loss: 0.00003076
Iteration 33/1000 | Loss: 0.00003076
Iteration 34/1000 | Loss: 0.00003074
Iteration 35/1000 | Loss: 0.00003073
Iteration 36/1000 | Loss: 0.00003072
Iteration 37/1000 | Loss: 0.00003072
Iteration 38/1000 | Loss: 0.00003070
Iteration 39/1000 | Loss: 0.00003067
Iteration 40/1000 | Loss: 0.00003067
Iteration 41/1000 | Loss: 0.00003066
Iteration 42/1000 | Loss: 0.00003066
Iteration 43/1000 | Loss: 0.00003065
Iteration 44/1000 | Loss: 0.00003065
Iteration 45/1000 | Loss: 0.00003065
Iteration 46/1000 | Loss: 0.00003065
Iteration 47/1000 | Loss: 0.00003065
Iteration 48/1000 | Loss: 0.00003065
Iteration 49/1000 | Loss: 0.00003064
Iteration 50/1000 | Loss: 0.00003064
Iteration 51/1000 | Loss: 0.00003064
Iteration 52/1000 | Loss: 0.00003064
Iteration 53/1000 | Loss: 0.00003064
Iteration 54/1000 | Loss: 0.00003064
Iteration 55/1000 | Loss: 0.00003064
Iteration 56/1000 | Loss: 0.00003064
Iteration 57/1000 | Loss: 0.00003064
Iteration 58/1000 | Loss: 0.00003064
Iteration 59/1000 | Loss: 0.00003064
Iteration 60/1000 | Loss: 0.00003064
Iteration 61/1000 | Loss: 0.00003064
Iteration 62/1000 | Loss: 0.00003063
Iteration 63/1000 | Loss: 0.00003063
Iteration 64/1000 | Loss: 0.00003063
Iteration 65/1000 | Loss: 0.00003063
Iteration 66/1000 | Loss: 0.00003063
Iteration 67/1000 | Loss: 0.00003061
Iteration 68/1000 | Loss: 0.00003060
Iteration 69/1000 | Loss: 0.00003059
Iteration 70/1000 | Loss: 0.00003059
Iteration 71/1000 | Loss: 0.00003059
Iteration 72/1000 | Loss: 0.00003058
Iteration 73/1000 | Loss: 0.00003058
Iteration 74/1000 | Loss: 0.00003058
Iteration 75/1000 | Loss: 0.00003057
Iteration 76/1000 | Loss: 0.00003057
Iteration 77/1000 | Loss: 0.00003056
Iteration 78/1000 | Loss: 0.00003055
Iteration 79/1000 | Loss: 0.00003055
Iteration 80/1000 | Loss: 0.00003055
Iteration 81/1000 | Loss: 0.00003054
Iteration 82/1000 | Loss: 0.00003054
Iteration 83/1000 | Loss: 0.00003054
Iteration 84/1000 | Loss: 0.00003053
Iteration 85/1000 | Loss: 0.00003053
Iteration 86/1000 | Loss: 0.00003052
Iteration 87/1000 | Loss: 0.00003052
Iteration 88/1000 | Loss: 0.00003052
Iteration 89/1000 | Loss: 0.00003051
Iteration 90/1000 | Loss: 0.00003051
Iteration 91/1000 | Loss: 0.00003050
Iteration 92/1000 | Loss: 0.00003050
Iteration 93/1000 | Loss: 0.00003050
Iteration 94/1000 | Loss: 0.00003049
Iteration 95/1000 | Loss: 0.00003049
Iteration 96/1000 | Loss: 0.00003049
Iteration 97/1000 | Loss: 0.00003049
Iteration 98/1000 | Loss: 0.00003049
Iteration 99/1000 | Loss: 0.00003049
Iteration 100/1000 | Loss: 0.00003048
Iteration 101/1000 | Loss: 0.00003048
Iteration 102/1000 | Loss: 0.00003048
Iteration 103/1000 | Loss: 0.00003048
Iteration 104/1000 | Loss: 0.00003048
Iteration 105/1000 | Loss: 0.00003047
Iteration 106/1000 | Loss: 0.00003047
Iteration 107/1000 | Loss: 0.00003047
Iteration 108/1000 | Loss: 0.00003046
Iteration 109/1000 | Loss: 0.00003046
Iteration 110/1000 | Loss: 0.00003046
Iteration 111/1000 | Loss: 0.00003045
Iteration 112/1000 | Loss: 0.00003045
Iteration 113/1000 | Loss: 0.00003044
Iteration 114/1000 | Loss: 0.00003044
Iteration 115/1000 | Loss: 0.00003044
Iteration 116/1000 | Loss: 0.00003044
Iteration 117/1000 | Loss: 0.00003043
Iteration 118/1000 | Loss: 0.00003043
Iteration 119/1000 | Loss: 0.00003043
Iteration 120/1000 | Loss: 0.00003042
Iteration 121/1000 | Loss: 0.00003042
Iteration 122/1000 | Loss: 0.00003042
Iteration 123/1000 | Loss: 0.00003041
Iteration 124/1000 | Loss: 0.00003041
Iteration 125/1000 | Loss: 0.00003041
Iteration 126/1000 | Loss: 0.00003040
Iteration 127/1000 | Loss: 0.00003040
Iteration 128/1000 | Loss: 0.00003040
Iteration 129/1000 | Loss: 0.00003040
Iteration 130/1000 | Loss: 0.00003039
Iteration 131/1000 | Loss: 0.00003039
Iteration 132/1000 | Loss: 0.00003039
Iteration 133/1000 | Loss: 0.00003039
Iteration 134/1000 | Loss: 0.00003038
Iteration 135/1000 | Loss: 0.00003038
Iteration 136/1000 | Loss: 0.00003038
Iteration 137/1000 | Loss: 0.00003038
Iteration 138/1000 | Loss: 0.00003038
Iteration 139/1000 | Loss: 0.00003038
Iteration 140/1000 | Loss: 0.00003038
Iteration 141/1000 | Loss: 0.00003038
Iteration 142/1000 | Loss: 0.00003038
Iteration 143/1000 | Loss: 0.00003038
Iteration 144/1000 | Loss: 0.00003038
Iteration 145/1000 | Loss: 0.00003038
Iteration 146/1000 | Loss: 0.00003037
Iteration 147/1000 | Loss: 0.00003037
Iteration 148/1000 | Loss: 0.00003037
Iteration 149/1000 | Loss: 0.00003037
Iteration 150/1000 | Loss: 0.00003037
Iteration 151/1000 | Loss: 0.00003037
Iteration 152/1000 | Loss: 0.00003037
Iteration 153/1000 | Loss: 0.00003037
Iteration 154/1000 | Loss: 0.00003037
Iteration 155/1000 | Loss: 0.00003037
Iteration 156/1000 | Loss: 0.00003036
Iteration 157/1000 | Loss: 0.00003036
Iteration 158/1000 | Loss: 0.00003036
Iteration 159/1000 | Loss: 0.00003036
Iteration 160/1000 | Loss: 0.00003035
Iteration 161/1000 | Loss: 0.00003035
Iteration 162/1000 | Loss: 0.00003035
Iteration 163/1000 | Loss: 0.00003035
Iteration 164/1000 | Loss: 0.00003035
Iteration 165/1000 | Loss: 0.00003035
Iteration 166/1000 | Loss: 0.00003035
Iteration 167/1000 | Loss: 0.00003035
Iteration 168/1000 | Loss: 0.00003035
Iteration 169/1000 | Loss: 0.00003034
Iteration 170/1000 | Loss: 0.00003034
Iteration 171/1000 | Loss: 0.00003034
Iteration 172/1000 | Loss: 0.00003034
Iteration 173/1000 | Loss: 0.00003034
Iteration 174/1000 | Loss: 0.00003034
Iteration 175/1000 | Loss: 0.00003034
Iteration 176/1000 | Loss: 0.00003034
Iteration 177/1000 | Loss: 0.00003034
Iteration 178/1000 | Loss: 0.00003034
Iteration 179/1000 | Loss: 0.00003034
Iteration 180/1000 | Loss: 0.00003034
Iteration 181/1000 | Loss: 0.00003034
Iteration 182/1000 | Loss: 0.00003033
Iteration 183/1000 | Loss: 0.00003033
Iteration 184/1000 | Loss: 0.00003033
Iteration 185/1000 | Loss: 0.00003033
Iteration 186/1000 | Loss: 0.00003033
Iteration 187/1000 | Loss: 0.00003033
Iteration 188/1000 | Loss: 0.00003033
Iteration 189/1000 | Loss: 0.00003033
Iteration 190/1000 | Loss: 0.00003033
Iteration 191/1000 | Loss: 0.00003032
Iteration 192/1000 | Loss: 0.00003032
Iteration 193/1000 | Loss: 0.00003032
Iteration 194/1000 | Loss: 0.00003032
Iteration 195/1000 | Loss: 0.00003032
Iteration 196/1000 | Loss: 0.00003032
Iteration 197/1000 | Loss: 0.00003032
Iteration 198/1000 | Loss: 0.00003032
Iteration 199/1000 | Loss: 0.00003031
Iteration 200/1000 | Loss: 0.00003031
Iteration 201/1000 | Loss: 0.00003031
Iteration 202/1000 | Loss: 0.00003031
Iteration 203/1000 | Loss: 0.00003031
Iteration 204/1000 | Loss: 0.00003031
Iteration 205/1000 | Loss: 0.00003031
Iteration 206/1000 | Loss: 0.00003031
Iteration 207/1000 | Loss: 0.00003031
Iteration 208/1000 | Loss: 0.00003031
Iteration 209/1000 | Loss: 0.00003031
Iteration 210/1000 | Loss: 0.00003031
Iteration 211/1000 | Loss: 0.00003031
Iteration 212/1000 | Loss: 0.00003031
Iteration 213/1000 | Loss: 0.00003030
Iteration 214/1000 | Loss: 0.00003030
Iteration 215/1000 | Loss: 0.00003030
Iteration 216/1000 | Loss: 0.00003030
Iteration 217/1000 | Loss: 0.00003030
Iteration 218/1000 | Loss: 0.00003030
Iteration 219/1000 | Loss: 0.00003030
Iteration 220/1000 | Loss: 0.00003030
Iteration 221/1000 | Loss: 0.00003030
Iteration 222/1000 | Loss: 0.00003030
Iteration 223/1000 | Loss: 0.00003030
Iteration 224/1000 | Loss: 0.00003030
Iteration 225/1000 | Loss: 0.00003030
Iteration 226/1000 | Loss: 0.00003030
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 226. Stopping optimization.
Last 5 losses: [3.0300843718578108e-05, 3.0300843718578108e-05, 3.0300843718578108e-05, 3.0300843718578108e-05, 3.0300843718578108e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.0300843718578108e-05

Optimization complete. Final v2v error: 4.458829402923584 mm

Highest mean error: 6.089057922363281 mm for frame 80

Lowest mean error: 3.4227054119110107 mm for frame 48

Saving results

Total time: 49.60827040672302
