Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=245, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 13720-13775
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_003/1052/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_003/1052.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_003/1052
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01024688
Iteration 2/25 | Loss: 0.00140965
Iteration 3/25 | Loss: 0.00127372
Iteration 4/25 | Loss: 0.00125931
Iteration 5/25 | Loss: 0.00125693
Iteration 6/25 | Loss: 0.00125655
Iteration 7/25 | Loss: 0.00125655
Iteration 8/25 | Loss: 0.00125655
Iteration 9/25 | Loss: 0.00125655
Iteration 10/25 | Loss: 0.00125655
Iteration 11/25 | Loss: 0.00125655
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001256554969586432, 0.001256554969586432, 0.001256554969586432, 0.001256554969586432, 0.001256554969586432]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001256554969586432

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.89036715
Iteration 2/25 | Loss: 0.00112414
Iteration 3/25 | Loss: 0.00112413
Iteration 4/25 | Loss: 0.00112413
Iteration 5/25 | Loss: 0.00112412
Iteration 6/25 | Loss: 0.00112412
Iteration 7/25 | Loss: 0.00112412
Iteration 8/25 | Loss: 0.00112412
Iteration 9/25 | Loss: 0.00112412
Iteration 10/25 | Loss: 0.00112412
Iteration 11/25 | Loss: 0.00112412
Iteration 12/25 | Loss: 0.00112412
Iteration 13/25 | Loss: 0.00112412
Iteration 14/25 | Loss: 0.00112412
Iteration 15/25 | Loss: 0.00112412
Iteration 16/25 | Loss: 0.00112412
Iteration 17/25 | Loss: 0.00112412
Iteration 18/25 | Loss: 0.00112412
Iteration 19/25 | Loss: 0.00112412
Iteration 20/25 | Loss: 0.00112412
Iteration 21/25 | Loss: 0.00112412
Iteration 22/25 | Loss: 0.00112412
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0011241222964599729, 0.0011241222964599729, 0.0011241222964599729, 0.0011241222964599729, 0.0011241222964599729]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011241222964599729

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00112412
Iteration 2/1000 | Loss: 0.00004376
Iteration 3/1000 | Loss: 0.00002720
Iteration 4/1000 | Loss: 0.00002120
Iteration 5/1000 | Loss: 0.00001822
Iteration 6/1000 | Loss: 0.00001720
Iteration 7/1000 | Loss: 0.00001673
Iteration 8/1000 | Loss: 0.00001629
Iteration 9/1000 | Loss: 0.00001598
Iteration 10/1000 | Loss: 0.00001572
Iteration 11/1000 | Loss: 0.00001564
Iteration 12/1000 | Loss: 0.00001558
Iteration 13/1000 | Loss: 0.00001556
Iteration 14/1000 | Loss: 0.00001550
Iteration 15/1000 | Loss: 0.00001546
Iteration 16/1000 | Loss: 0.00001544
Iteration 17/1000 | Loss: 0.00001542
Iteration 18/1000 | Loss: 0.00001531
Iteration 19/1000 | Loss: 0.00001529
Iteration 20/1000 | Loss: 0.00001527
Iteration 21/1000 | Loss: 0.00001527
Iteration 22/1000 | Loss: 0.00001526
Iteration 23/1000 | Loss: 0.00001526
Iteration 24/1000 | Loss: 0.00001526
Iteration 25/1000 | Loss: 0.00001525
Iteration 26/1000 | Loss: 0.00001525
Iteration 27/1000 | Loss: 0.00001524
Iteration 28/1000 | Loss: 0.00001524
Iteration 29/1000 | Loss: 0.00001524
Iteration 30/1000 | Loss: 0.00001523
Iteration 31/1000 | Loss: 0.00001522
Iteration 32/1000 | Loss: 0.00001521
Iteration 33/1000 | Loss: 0.00001515
Iteration 34/1000 | Loss: 0.00001512
Iteration 35/1000 | Loss: 0.00001512
Iteration 36/1000 | Loss: 0.00001511
Iteration 37/1000 | Loss: 0.00001510
Iteration 38/1000 | Loss: 0.00001509
Iteration 39/1000 | Loss: 0.00001509
Iteration 40/1000 | Loss: 0.00001509
Iteration 41/1000 | Loss: 0.00001509
Iteration 42/1000 | Loss: 0.00001508
Iteration 43/1000 | Loss: 0.00001508
Iteration 44/1000 | Loss: 0.00001508
Iteration 45/1000 | Loss: 0.00001508
Iteration 46/1000 | Loss: 0.00001507
Iteration 47/1000 | Loss: 0.00001507
Iteration 48/1000 | Loss: 0.00001507
Iteration 49/1000 | Loss: 0.00001505
Iteration 50/1000 | Loss: 0.00001505
Iteration 51/1000 | Loss: 0.00001504
Iteration 52/1000 | Loss: 0.00001504
Iteration 53/1000 | Loss: 0.00001504
Iteration 54/1000 | Loss: 0.00001504
Iteration 55/1000 | Loss: 0.00001503
Iteration 56/1000 | Loss: 0.00001503
Iteration 57/1000 | Loss: 0.00001503
Iteration 58/1000 | Loss: 0.00001502
Iteration 59/1000 | Loss: 0.00001502
Iteration 60/1000 | Loss: 0.00001502
Iteration 61/1000 | Loss: 0.00001502
Iteration 62/1000 | Loss: 0.00001501
Iteration 63/1000 | Loss: 0.00001501
Iteration 64/1000 | Loss: 0.00001500
Iteration 65/1000 | Loss: 0.00001500
Iteration 66/1000 | Loss: 0.00001498
Iteration 67/1000 | Loss: 0.00001498
Iteration 68/1000 | Loss: 0.00001498
Iteration 69/1000 | Loss: 0.00001498
Iteration 70/1000 | Loss: 0.00001498
Iteration 71/1000 | Loss: 0.00001498
Iteration 72/1000 | Loss: 0.00001498
Iteration 73/1000 | Loss: 0.00001497
Iteration 74/1000 | Loss: 0.00001497
Iteration 75/1000 | Loss: 0.00001496
Iteration 76/1000 | Loss: 0.00001495
Iteration 77/1000 | Loss: 0.00001495
Iteration 78/1000 | Loss: 0.00001495
Iteration 79/1000 | Loss: 0.00001494
Iteration 80/1000 | Loss: 0.00001494
Iteration 81/1000 | Loss: 0.00001494
Iteration 82/1000 | Loss: 0.00001494
Iteration 83/1000 | Loss: 0.00001494
Iteration 84/1000 | Loss: 0.00001494
Iteration 85/1000 | Loss: 0.00001494
Iteration 86/1000 | Loss: 0.00001493
Iteration 87/1000 | Loss: 0.00001493
Iteration 88/1000 | Loss: 0.00001493
Iteration 89/1000 | Loss: 0.00001493
Iteration 90/1000 | Loss: 0.00001493
Iteration 91/1000 | Loss: 0.00001493
Iteration 92/1000 | Loss: 0.00001493
Iteration 93/1000 | Loss: 0.00001493
Iteration 94/1000 | Loss: 0.00001493
Iteration 95/1000 | Loss: 0.00001493
Iteration 96/1000 | Loss: 0.00001492
Iteration 97/1000 | Loss: 0.00001492
Iteration 98/1000 | Loss: 0.00001492
Iteration 99/1000 | Loss: 0.00001492
Iteration 100/1000 | Loss: 0.00001492
Iteration 101/1000 | Loss: 0.00001492
Iteration 102/1000 | Loss: 0.00001492
Iteration 103/1000 | Loss: 0.00001491
Iteration 104/1000 | Loss: 0.00001491
Iteration 105/1000 | Loss: 0.00001491
Iteration 106/1000 | Loss: 0.00001491
Iteration 107/1000 | Loss: 0.00001491
Iteration 108/1000 | Loss: 0.00001491
Iteration 109/1000 | Loss: 0.00001491
Iteration 110/1000 | Loss: 0.00001490
Iteration 111/1000 | Loss: 0.00001490
Iteration 112/1000 | Loss: 0.00001490
Iteration 113/1000 | Loss: 0.00001490
Iteration 114/1000 | Loss: 0.00001489
Iteration 115/1000 | Loss: 0.00001489
Iteration 116/1000 | Loss: 0.00001489
Iteration 117/1000 | Loss: 0.00001489
Iteration 118/1000 | Loss: 0.00001489
Iteration 119/1000 | Loss: 0.00001489
Iteration 120/1000 | Loss: 0.00001489
Iteration 121/1000 | Loss: 0.00001489
Iteration 122/1000 | Loss: 0.00001488
Iteration 123/1000 | Loss: 0.00001488
Iteration 124/1000 | Loss: 0.00001488
Iteration 125/1000 | Loss: 0.00001488
Iteration 126/1000 | Loss: 0.00001488
Iteration 127/1000 | Loss: 0.00001488
Iteration 128/1000 | Loss: 0.00001488
Iteration 129/1000 | Loss: 0.00001488
Iteration 130/1000 | Loss: 0.00001488
Iteration 131/1000 | Loss: 0.00001488
Iteration 132/1000 | Loss: 0.00001488
Iteration 133/1000 | Loss: 0.00001487
Iteration 134/1000 | Loss: 0.00001487
Iteration 135/1000 | Loss: 0.00001487
Iteration 136/1000 | Loss: 0.00001487
Iteration 137/1000 | Loss: 0.00001487
Iteration 138/1000 | Loss: 0.00001487
Iteration 139/1000 | Loss: 0.00001487
Iteration 140/1000 | Loss: 0.00001487
Iteration 141/1000 | Loss: 0.00001487
Iteration 142/1000 | Loss: 0.00001487
Iteration 143/1000 | Loss: 0.00001487
Iteration 144/1000 | Loss: 0.00001486
Iteration 145/1000 | Loss: 0.00001486
Iteration 146/1000 | Loss: 0.00001486
Iteration 147/1000 | Loss: 0.00001486
Iteration 148/1000 | Loss: 0.00001485
Iteration 149/1000 | Loss: 0.00001485
Iteration 150/1000 | Loss: 0.00001485
Iteration 151/1000 | Loss: 0.00001485
Iteration 152/1000 | Loss: 0.00001485
Iteration 153/1000 | Loss: 0.00001485
Iteration 154/1000 | Loss: 0.00001485
Iteration 155/1000 | Loss: 0.00001485
Iteration 156/1000 | Loss: 0.00001485
Iteration 157/1000 | Loss: 0.00001485
Iteration 158/1000 | Loss: 0.00001485
Iteration 159/1000 | Loss: 0.00001485
Iteration 160/1000 | Loss: 0.00001485
Iteration 161/1000 | Loss: 0.00001485
Iteration 162/1000 | Loss: 0.00001485
Iteration 163/1000 | Loss: 0.00001485
Iteration 164/1000 | Loss: 0.00001485
Iteration 165/1000 | Loss: 0.00001485
Iteration 166/1000 | Loss: 0.00001485
Iteration 167/1000 | Loss: 0.00001485
Iteration 168/1000 | Loss: 0.00001485
Iteration 169/1000 | Loss: 0.00001485
Iteration 170/1000 | Loss: 0.00001485
Iteration 171/1000 | Loss: 0.00001485
Iteration 172/1000 | Loss: 0.00001485
Iteration 173/1000 | Loss: 0.00001485
Iteration 174/1000 | Loss: 0.00001485
Iteration 175/1000 | Loss: 0.00001485
Iteration 176/1000 | Loss: 0.00001485
Iteration 177/1000 | Loss: 0.00001485
Iteration 178/1000 | Loss: 0.00001485
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 178. Stopping optimization.
Last 5 losses: [1.484792028350057e-05, 1.484792028350057e-05, 1.484792028350057e-05, 1.484792028350057e-05, 1.484792028350057e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.484792028350057e-05

Optimization complete. Final v2v error: 3.2499866485595703 mm

Highest mean error: 3.696681261062622 mm for frame 50

Lowest mean error: 2.9427168369293213 mm for frame 15

Saving results

Total time: 39.335644483566284
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_003/1029/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_003/1029.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_003/1029
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00588453
Iteration 2/25 | Loss: 0.00146808
Iteration 3/25 | Loss: 0.00136345
Iteration 4/25 | Loss: 0.00123597
Iteration 5/25 | Loss: 0.00122991
Iteration 6/25 | Loss: 0.00122584
Iteration 7/25 | Loss: 0.00122205
Iteration 8/25 | Loss: 0.00121533
Iteration 9/25 | Loss: 0.00121423
Iteration 10/25 | Loss: 0.00121297
Iteration 11/25 | Loss: 0.00121249
Iteration 12/25 | Loss: 0.00121458
Iteration 13/25 | Loss: 0.00121097
Iteration 14/25 | Loss: 0.00120977
Iteration 15/25 | Loss: 0.00120941
Iteration 16/25 | Loss: 0.00120937
Iteration 17/25 | Loss: 0.00120936
Iteration 18/25 | Loss: 0.00120936
Iteration 19/25 | Loss: 0.00120936
Iteration 20/25 | Loss: 0.00120936
Iteration 21/25 | Loss: 0.00120936
Iteration 22/25 | Loss: 0.00120936
Iteration 23/25 | Loss: 0.00120936
Iteration 24/25 | Loss: 0.00120936
Iteration 25/25 | Loss: 0.00120936

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.16336012
Iteration 2/25 | Loss: 0.00119018
Iteration 3/25 | Loss: 0.00119018
Iteration 4/25 | Loss: 0.00119018
Iteration 5/25 | Loss: 0.00119018
Iteration 6/25 | Loss: 0.00119018
Iteration 7/25 | Loss: 0.00119018
Iteration 8/25 | Loss: 0.00119018
Iteration 9/25 | Loss: 0.00119018
Iteration 10/25 | Loss: 0.00119018
Iteration 11/25 | Loss: 0.00119018
Iteration 12/25 | Loss: 0.00119018
Iteration 13/25 | Loss: 0.00119018
Iteration 14/25 | Loss: 0.00119018
Iteration 15/25 | Loss: 0.00119018
Iteration 16/25 | Loss: 0.00119018
Iteration 17/25 | Loss: 0.00119018
Iteration 18/25 | Loss: 0.00119018
Iteration 19/25 | Loss: 0.00119018
Iteration 20/25 | Loss: 0.00119018
Iteration 21/25 | Loss: 0.00119018
Iteration 22/25 | Loss: 0.00119018
Iteration 23/25 | Loss: 0.00119018
Iteration 24/25 | Loss: 0.00119018
Iteration 25/25 | Loss: 0.00119018

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00119018
Iteration 2/1000 | Loss: 0.00002156
Iteration 3/1000 | Loss: 0.00013064
Iteration 4/1000 | Loss: 0.00002727
Iteration 5/1000 | Loss: 0.00001650
Iteration 6/1000 | Loss: 0.00001412
Iteration 7/1000 | Loss: 0.00001336
Iteration 8/1000 | Loss: 0.00001292
Iteration 9/1000 | Loss: 0.00001255
Iteration 10/1000 | Loss: 0.00001230
Iteration 11/1000 | Loss: 0.00016512
Iteration 12/1000 | Loss: 0.00014528
Iteration 13/1000 | Loss: 0.00001239
Iteration 14/1000 | Loss: 0.00001198
Iteration 15/1000 | Loss: 0.00001197
Iteration 16/1000 | Loss: 0.00001197
Iteration 17/1000 | Loss: 0.00015273
Iteration 18/1000 | Loss: 0.00005150
Iteration 19/1000 | Loss: 0.00001198
Iteration 20/1000 | Loss: 0.00013993
Iteration 21/1000 | Loss: 0.00001654
Iteration 22/1000 | Loss: 0.00005868
Iteration 23/1000 | Loss: 0.00001617
Iteration 24/1000 | Loss: 0.00002458
Iteration 25/1000 | Loss: 0.00001200
Iteration 26/1000 | Loss: 0.00001191
Iteration 27/1000 | Loss: 0.00001183
Iteration 28/1000 | Loss: 0.00001182
Iteration 29/1000 | Loss: 0.00001182
Iteration 30/1000 | Loss: 0.00001181
Iteration 31/1000 | Loss: 0.00001181
Iteration 32/1000 | Loss: 0.00001181
Iteration 33/1000 | Loss: 0.00001180
Iteration 34/1000 | Loss: 0.00001177
Iteration 35/1000 | Loss: 0.00001177
Iteration 36/1000 | Loss: 0.00001177
Iteration 37/1000 | Loss: 0.00001176
Iteration 38/1000 | Loss: 0.00001176
Iteration 39/1000 | Loss: 0.00001176
Iteration 40/1000 | Loss: 0.00001175
Iteration 41/1000 | Loss: 0.00001175
Iteration 42/1000 | Loss: 0.00001175
Iteration 43/1000 | Loss: 0.00001174
Iteration 44/1000 | Loss: 0.00001174
Iteration 45/1000 | Loss: 0.00001173
Iteration 46/1000 | Loss: 0.00001173
Iteration 47/1000 | Loss: 0.00001173
Iteration 48/1000 | Loss: 0.00001172
Iteration 49/1000 | Loss: 0.00001172
Iteration 50/1000 | Loss: 0.00001171
Iteration 51/1000 | Loss: 0.00001171
Iteration 52/1000 | Loss: 0.00001171
Iteration 53/1000 | Loss: 0.00001170
Iteration 54/1000 | Loss: 0.00001169
Iteration 55/1000 | Loss: 0.00001168
Iteration 56/1000 | Loss: 0.00001168
Iteration 57/1000 | Loss: 0.00001168
Iteration 58/1000 | Loss: 0.00001168
Iteration 59/1000 | Loss: 0.00001168
Iteration 60/1000 | Loss: 0.00001167
Iteration 61/1000 | Loss: 0.00001167
Iteration 62/1000 | Loss: 0.00001167
Iteration 63/1000 | Loss: 0.00001167
Iteration 64/1000 | Loss: 0.00001167
Iteration 65/1000 | Loss: 0.00001167
Iteration 66/1000 | Loss: 0.00001166
Iteration 67/1000 | Loss: 0.00001166
Iteration 68/1000 | Loss: 0.00001166
Iteration 69/1000 | Loss: 0.00001166
Iteration 70/1000 | Loss: 0.00001166
Iteration 71/1000 | Loss: 0.00001166
Iteration 72/1000 | Loss: 0.00001166
Iteration 73/1000 | Loss: 0.00001165
Iteration 74/1000 | Loss: 0.00001165
Iteration 75/1000 | Loss: 0.00001164
Iteration 76/1000 | Loss: 0.00001164
Iteration 77/1000 | Loss: 0.00001164
Iteration 78/1000 | Loss: 0.00001164
Iteration 79/1000 | Loss: 0.00001163
Iteration 80/1000 | Loss: 0.00001163
Iteration 81/1000 | Loss: 0.00001162
Iteration 82/1000 | Loss: 0.00001162
Iteration 83/1000 | Loss: 0.00001162
Iteration 84/1000 | Loss: 0.00001162
Iteration 85/1000 | Loss: 0.00001162
Iteration 86/1000 | Loss: 0.00001162
Iteration 87/1000 | Loss: 0.00001162
Iteration 88/1000 | Loss: 0.00001162
Iteration 89/1000 | Loss: 0.00001162
Iteration 90/1000 | Loss: 0.00001162
Iteration 91/1000 | Loss: 0.00001162
Iteration 92/1000 | Loss: 0.00001161
Iteration 93/1000 | Loss: 0.00001161
Iteration 94/1000 | Loss: 0.00001161
Iteration 95/1000 | Loss: 0.00001161
Iteration 96/1000 | Loss: 0.00001161
Iteration 97/1000 | Loss: 0.00001161
Iteration 98/1000 | Loss: 0.00001161
Iteration 99/1000 | Loss: 0.00001160
Iteration 100/1000 | Loss: 0.00001160
Iteration 101/1000 | Loss: 0.00001160
Iteration 102/1000 | Loss: 0.00001160
Iteration 103/1000 | Loss: 0.00001160
Iteration 104/1000 | Loss: 0.00001160
Iteration 105/1000 | Loss: 0.00001160
Iteration 106/1000 | Loss: 0.00001160
Iteration 107/1000 | Loss: 0.00001160
Iteration 108/1000 | Loss: 0.00001160
Iteration 109/1000 | Loss: 0.00001160
Iteration 110/1000 | Loss: 0.00001160
Iteration 111/1000 | Loss: 0.00001160
Iteration 112/1000 | Loss: 0.00001160
Iteration 113/1000 | Loss: 0.00001160
Iteration 114/1000 | Loss: 0.00001160
Iteration 115/1000 | Loss: 0.00001159
Iteration 116/1000 | Loss: 0.00001159
Iteration 117/1000 | Loss: 0.00001159
Iteration 118/1000 | Loss: 0.00001159
Iteration 119/1000 | Loss: 0.00001159
Iteration 120/1000 | Loss: 0.00001159
Iteration 121/1000 | Loss: 0.00001158
Iteration 122/1000 | Loss: 0.00001158
Iteration 123/1000 | Loss: 0.00001157
Iteration 124/1000 | Loss: 0.00001157
Iteration 125/1000 | Loss: 0.00001156
Iteration 126/1000 | Loss: 0.00001156
Iteration 127/1000 | Loss: 0.00001156
Iteration 128/1000 | Loss: 0.00001156
Iteration 129/1000 | Loss: 0.00001156
Iteration 130/1000 | Loss: 0.00001155
Iteration 131/1000 | Loss: 0.00001155
Iteration 132/1000 | Loss: 0.00020362
Iteration 133/1000 | Loss: 0.00001740
Iteration 134/1000 | Loss: 0.00001539
Iteration 135/1000 | Loss: 0.00001198
Iteration 136/1000 | Loss: 0.00001153
Iteration 137/1000 | Loss: 0.00001150
Iteration 138/1000 | Loss: 0.00001150
Iteration 139/1000 | Loss: 0.00001148
Iteration 140/1000 | Loss: 0.00001148
Iteration 141/1000 | Loss: 0.00001146
Iteration 142/1000 | Loss: 0.00001146
Iteration 143/1000 | Loss: 0.00001146
Iteration 144/1000 | Loss: 0.00001145
Iteration 145/1000 | Loss: 0.00001145
Iteration 146/1000 | Loss: 0.00001145
Iteration 147/1000 | Loss: 0.00001144
Iteration 148/1000 | Loss: 0.00001144
Iteration 149/1000 | Loss: 0.00001144
Iteration 150/1000 | Loss: 0.00001144
Iteration 151/1000 | Loss: 0.00001144
Iteration 152/1000 | Loss: 0.00001143
Iteration 153/1000 | Loss: 0.00001143
Iteration 154/1000 | Loss: 0.00001143
Iteration 155/1000 | Loss: 0.00001143
Iteration 156/1000 | Loss: 0.00001143
Iteration 157/1000 | Loss: 0.00001143
Iteration 158/1000 | Loss: 0.00001143
Iteration 159/1000 | Loss: 0.00001143
Iteration 160/1000 | Loss: 0.00001142
Iteration 161/1000 | Loss: 0.00001142
Iteration 162/1000 | Loss: 0.00001142
Iteration 163/1000 | Loss: 0.00001142
Iteration 164/1000 | Loss: 0.00001142
Iteration 165/1000 | Loss: 0.00001142
Iteration 166/1000 | Loss: 0.00001142
Iteration 167/1000 | Loss: 0.00001142
Iteration 168/1000 | Loss: 0.00001142
Iteration 169/1000 | Loss: 0.00001142
Iteration 170/1000 | Loss: 0.00001142
Iteration 171/1000 | Loss: 0.00001142
Iteration 172/1000 | Loss: 0.00001142
Iteration 173/1000 | Loss: 0.00001142
Iteration 174/1000 | Loss: 0.00001142
Iteration 175/1000 | Loss: 0.00001142
Iteration 176/1000 | Loss: 0.00001142
Iteration 177/1000 | Loss: 0.00001142
Iteration 178/1000 | Loss: 0.00001142
Iteration 179/1000 | Loss: 0.00001142
Iteration 180/1000 | Loss: 0.00001142
Iteration 181/1000 | Loss: 0.00001142
Iteration 182/1000 | Loss: 0.00001142
Iteration 183/1000 | Loss: 0.00001142
Iteration 184/1000 | Loss: 0.00001142
Iteration 185/1000 | Loss: 0.00001142
Iteration 186/1000 | Loss: 0.00001142
Iteration 187/1000 | Loss: 0.00001141
Iteration 188/1000 | Loss: 0.00001141
Iteration 189/1000 | Loss: 0.00001141
Iteration 190/1000 | Loss: 0.00001141
Iteration 191/1000 | Loss: 0.00001141
Iteration 192/1000 | Loss: 0.00001141
Iteration 193/1000 | Loss: 0.00001141
Iteration 194/1000 | Loss: 0.00001141
Iteration 195/1000 | Loss: 0.00001141
Iteration 196/1000 | Loss: 0.00001141
Iteration 197/1000 | Loss: 0.00001141
Iteration 198/1000 | Loss: 0.00001141
Iteration 199/1000 | Loss: 0.00001141
Iteration 200/1000 | Loss: 0.00001141
Iteration 201/1000 | Loss: 0.00001141
Iteration 202/1000 | Loss: 0.00001141
Iteration 203/1000 | Loss: 0.00001141
Iteration 204/1000 | Loss: 0.00001141
Iteration 205/1000 | Loss: 0.00001140
Iteration 206/1000 | Loss: 0.00001140
Iteration 207/1000 | Loss: 0.00001140
Iteration 208/1000 | Loss: 0.00001140
Iteration 209/1000 | Loss: 0.00001140
Iteration 210/1000 | Loss: 0.00001140
Iteration 211/1000 | Loss: 0.00001140
Iteration 212/1000 | Loss: 0.00001140
Iteration 213/1000 | Loss: 0.00001140
Iteration 214/1000 | Loss: 0.00001140
Iteration 215/1000 | Loss: 0.00001140
Iteration 216/1000 | Loss: 0.00001140
Iteration 217/1000 | Loss: 0.00001140
Iteration 218/1000 | Loss: 0.00001140
Iteration 219/1000 | Loss: 0.00001140
Iteration 220/1000 | Loss: 0.00001140
Iteration 221/1000 | Loss: 0.00001140
Iteration 222/1000 | Loss: 0.00001140
Iteration 223/1000 | Loss: 0.00001140
Iteration 224/1000 | Loss: 0.00001140
Iteration 225/1000 | Loss: 0.00001140
Iteration 226/1000 | Loss: 0.00001140
Iteration 227/1000 | Loss: 0.00001140
Iteration 228/1000 | Loss: 0.00001140
Iteration 229/1000 | Loss: 0.00001140
Iteration 230/1000 | Loss: 0.00001140
Iteration 231/1000 | Loss: 0.00001140
Iteration 232/1000 | Loss: 0.00001140
Iteration 233/1000 | Loss: 0.00001140
Iteration 234/1000 | Loss: 0.00001140
Iteration 235/1000 | Loss: 0.00001140
Iteration 236/1000 | Loss: 0.00001140
Iteration 237/1000 | Loss: 0.00001140
Iteration 238/1000 | Loss: 0.00001140
Iteration 239/1000 | Loss: 0.00001140
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 239. Stopping optimization.
Last 5 losses: [1.1400180483178701e-05, 1.1400180483178701e-05, 1.1400180483178701e-05, 1.1400180483178701e-05, 1.1400180483178701e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1400180483178701e-05

Optimization complete. Final v2v error: 2.871413469314575 mm

Highest mean error: 3.237750768661499 mm for frame 66

Lowest mean error: 2.594362735748291 mm for frame 129

Saving results

Total time: 90.78102517127991
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_003/1006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_003/1006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_003/1006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00770155
Iteration 2/25 | Loss: 0.00168400
Iteration 3/25 | Loss: 0.00139901
Iteration 4/25 | Loss: 0.00136475
Iteration 5/25 | Loss: 0.00136052
Iteration 6/25 | Loss: 0.00136002
Iteration 7/25 | Loss: 0.00136002
Iteration 8/25 | Loss: 0.00136002
Iteration 9/25 | Loss: 0.00136002
Iteration 10/25 | Loss: 0.00136002
Iteration 11/25 | Loss: 0.00136002
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013600243255496025, 0.0013600243255496025, 0.0013600243255496025, 0.0013600243255496025, 0.0013600243255496025]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013600243255496025

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.30302715
Iteration 2/25 | Loss: 0.00115125
Iteration 3/25 | Loss: 0.00115122
Iteration 4/25 | Loss: 0.00115122
Iteration 5/25 | Loss: 0.00115122
Iteration 6/25 | Loss: 0.00115122
Iteration 7/25 | Loss: 0.00115122
Iteration 8/25 | Loss: 0.00115122
Iteration 9/25 | Loss: 0.00115122
Iteration 10/25 | Loss: 0.00115122
Iteration 11/25 | Loss: 0.00115122
Iteration 12/25 | Loss: 0.00115122
Iteration 13/25 | Loss: 0.00115122
Iteration 14/25 | Loss: 0.00115122
Iteration 15/25 | Loss: 0.00115122
Iteration 16/25 | Loss: 0.00115122
Iteration 17/25 | Loss: 0.00115122
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0011512166820466518, 0.0011512166820466518, 0.0011512166820466518, 0.0011512166820466518, 0.0011512166820466518]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011512166820466518

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00115122
Iteration 2/1000 | Loss: 0.00004182
Iteration 3/1000 | Loss: 0.00003125
Iteration 4/1000 | Loss: 0.00002815
Iteration 5/1000 | Loss: 0.00002676
Iteration 6/1000 | Loss: 0.00002611
Iteration 7/1000 | Loss: 0.00002558
Iteration 8/1000 | Loss: 0.00002507
Iteration 9/1000 | Loss: 0.00002469
Iteration 10/1000 | Loss: 0.00002454
Iteration 11/1000 | Loss: 0.00002424
Iteration 12/1000 | Loss: 0.00002393
Iteration 13/1000 | Loss: 0.00002373
Iteration 14/1000 | Loss: 0.00002365
Iteration 15/1000 | Loss: 0.00002343
Iteration 16/1000 | Loss: 0.00002326
Iteration 17/1000 | Loss: 0.00002319
Iteration 18/1000 | Loss: 0.00002308
Iteration 19/1000 | Loss: 0.00002307
Iteration 20/1000 | Loss: 0.00002306
Iteration 21/1000 | Loss: 0.00002305
Iteration 22/1000 | Loss: 0.00002305
Iteration 23/1000 | Loss: 0.00002304
Iteration 24/1000 | Loss: 0.00002304
Iteration 25/1000 | Loss: 0.00002303
Iteration 26/1000 | Loss: 0.00002303
Iteration 27/1000 | Loss: 0.00002302
Iteration 28/1000 | Loss: 0.00002302
Iteration 29/1000 | Loss: 0.00002302
Iteration 30/1000 | Loss: 0.00002302
Iteration 31/1000 | Loss: 0.00002302
Iteration 32/1000 | Loss: 0.00002302
Iteration 33/1000 | Loss: 0.00002302
Iteration 34/1000 | Loss: 0.00002302
Iteration 35/1000 | Loss: 0.00002302
Iteration 36/1000 | Loss: 0.00002302
Iteration 37/1000 | Loss: 0.00002302
Iteration 38/1000 | Loss: 0.00002301
Iteration 39/1000 | Loss: 0.00002301
Iteration 40/1000 | Loss: 0.00002300
Iteration 41/1000 | Loss: 0.00002300
Iteration 42/1000 | Loss: 0.00002299
Iteration 43/1000 | Loss: 0.00002299
Iteration 44/1000 | Loss: 0.00002298
Iteration 45/1000 | Loss: 0.00002298
Iteration 46/1000 | Loss: 0.00002298
Iteration 47/1000 | Loss: 0.00002298
Iteration 48/1000 | Loss: 0.00002297
Iteration 49/1000 | Loss: 0.00002297
Iteration 50/1000 | Loss: 0.00002297
Iteration 51/1000 | Loss: 0.00002296
Iteration 52/1000 | Loss: 0.00002294
Iteration 53/1000 | Loss: 0.00002294
Iteration 54/1000 | Loss: 0.00002293
Iteration 55/1000 | Loss: 0.00002290
Iteration 56/1000 | Loss: 0.00002289
Iteration 57/1000 | Loss: 0.00002289
Iteration 58/1000 | Loss: 0.00002289
Iteration 59/1000 | Loss: 0.00002289
Iteration 60/1000 | Loss: 0.00002289
Iteration 61/1000 | Loss: 0.00002289
Iteration 62/1000 | Loss: 0.00002289
Iteration 63/1000 | Loss: 0.00002289
Iteration 64/1000 | Loss: 0.00002289
Iteration 65/1000 | Loss: 0.00002289
Iteration 66/1000 | Loss: 0.00002289
Iteration 67/1000 | Loss: 0.00002289
Iteration 68/1000 | Loss: 0.00002288
Iteration 69/1000 | Loss: 0.00002288
Iteration 70/1000 | Loss: 0.00002288
Iteration 71/1000 | Loss: 0.00002287
Iteration 72/1000 | Loss: 0.00002287
Iteration 73/1000 | Loss: 0.00002287
Iteration 74/1000 | Loss: 0.00002286
Iteration 75/1000 | Loss: 0.00002286
Iteration 76/1000 | Loss: 0.00002286
Iteration 77/1000 | Loss: 0.00002286
Iteration 78/1000 | Loss: 0.00002286
Iteration 79/1000 | Loss: 0.00002285
Iteration 80/1000 | Loss: 0.00002285
Iteration 81/1000 | Loss: 0.00002285
Iteration 82/1000 | Loss: 0.00002284
Iteration 83/1000 | Loss: 0.00002284
Iteration 84/1000 | Loss: 0.00002284
Iteration 85/1000 | Loss: 0.00002283
Iteration 86/1000 | Loss: 0.00002283
Iteration 87/1000 | Loss: 0.00002283
Iteration 88/1000 | Loss: 0.00002282
Iteration 89/1000 | Loss: 0.00002282
Iteration 90/1000 | Loss: 0.00002282
Iteration 91/1000 | Loss: 0.00002282
Iteration 92/1000 | Loss: 0.00002282
Iteration 93/1000 | Loss: 0.00002282
Iteration 94/1000 | Loss: 0.00002282
Iteration 95/1000 | Loss: 0.00002282
Iteration 96/1000 | Loss: 0.00002282
Iteration 97/1000 | Loss: 0.00002282
Iteration 98/1000 | Loss: 0.00002281
Iteration 99/1000 | Loss: 0.00002281
Iteration 100/1000 | Loss: 0.00002281
Iteration 101/1000 | Loss: 0.00002281
Iteration 102/1000 | Loss: 0.00002281
Iteration 103/1000 | Loss: 0.00002281
Iteration 104/1000 | Loss: 0.00002281
Iteration 105/1000 | Loss: 0.00002281
Iteration 106/1000 | Loss: 0.00002281
Iteration 107/1000 | Loss: 0.00002281
Iteration 108/1000 | Loss: 0.00002281
Iteration 109/1000 | Loss: 0.00002281
Iteration 110/1000 | Loss: 0.00002280
Iteration 111/1000 | Loss: 0.00002280
Iteration 112/1000 | Loss: 0.00002280
Iteration 113/1000 | Loss: 0.00002280
Iteration 114/1000 | Loss: 0.00002280
Iteration 115/1000 | Loss: 0.00002280
Iteration 116/1000 | Loss: 0.00002280
Iteration 117/1000 | Loss: 0.00002280
Iteration 118/1000 | Loss: 0.00002280
Iteration 119/1000 | Loss: 0.00002280
Iteration 120/1000 | Loss: 0.00002280
Iteration 121/1000 | Loss: 0.00002280
Iteration 122/1000 | Loss: 0.00002280
Iteration 123/1000 | Loss: 0.00002280
Iteration 124/1000 | Loss: 0.00002280
Iteration 125/1000 | Loss: 0.00002280
Iteration 126/1000 | Loss: 0.00002280
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 126. Stopping optimization.
Last 5 losses: [2.2799320504418574e-05, 2.2799320504418574e-05, 2.2799320504418574e-05, 2.2799320504418574e-05, 2.2799320504418574e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2799320504418574e-05

Optimization complete. Final v2v error: 3.9737355709075928 mm

Highest mean error: 4.224564552307129 mm for frame 75

Lowest mean error: 3.834324836730957 mm for frame 4

Saving results

Total time: 37.14868354797363
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_003/1033/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_003/1033.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_003/1033
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00778501
Iteration 2/25 | Loss: 0.00152158
Iteration 3/25 | Loss: 0.00125292
Iteration 4/25 | Loss: 0.00123107
Iteration 5/25 | Loss: 0.00122851
Iteration 6/25 | Loss: 0.00122845
Iteration 7/25 | Loss: 0.00122845
Iteration 8/25 | Loss: 0.00122845
Iteration 9/25 | Loss: 0.00122845
Iteration 10/25 | Loss: 0.00122845
Iteration 11/25 | Loss: 0.00122845
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012284533586353064, 0.0012284533586353064, 0.0012284533586353064, 0.0012284533586353064, 0.0012284533586353064]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012284533586353064

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.31716394
Iteration 2/25 | Loss: 0.00105312
Iteration 3/25 | Loss: 0.00105312
Iteration 4/25 | Loss: 0.00105312
Iteration 5/25 | Loss: 0.00105312
Iteration 6/25 | Loss: 0.00105312
Iteration 7/25 | Loss: 0.00105312
Iteration 8/25 | Loss: 0.00105312
Iteration 9/25 | Loss: 0.00105312
Iteration 10/25 | Loss: 0.00105312
Iteration 11/25 | Loss: 0.00105312
Iteration 12/25 | Loss: 0.00105312
Iteration 13/25 | Loss: 0.00105312
Iteration 14/25 | Loss: 0.00105312
Iteration 15/25 | Loss: 0.00105312
Iteration 16/25 | Loss: 0.00105312
Iteration 17/25 | Loss: 0.00105312
Iteration 18/25 | Loss: 0.00105312
Iteration 19/25 | Loss: 0.00105312
Iteration 20/25 | Loss: 0.00105312
Iteration 21/25 | Loss: 0.00105312
Iteration 22/25 | Loss: 0.00105312
Iteration 23/25 | Loss: 0.00105311
Iteration 24/25 | Loss: 0.00105312
Iteration 25/25 | Loss: 0.00105311

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00105312
Iteration 2/1000 | Loss: 0.00002714
Iteration 3/1000 | Loss: 0.00001954
Iteration 4/1000 | Loss: 0.00001734
Iteration 5/1000 | Loss: 0.00001620
Iteration 6/1000 | Loss: 0.00001528
Iteration 7/1000 | Loss: 0.00001484
Iteration 8/1000 | Loss: 0.00001434
Iteration 9/1000 | Loss: 0.00001402
Iteration 10/1000 | Loss: 0.00001366
Iteration 11/1000 | Loss: 0.00001347
Iteration 12/1000 | Loss: 0.00001329
Iteration 13/1000 | Loss: 0.00001325
Iteration 14/1000 | Loss: 0.00001318
Iteration 15/1000 | Loss: 0.00001317
Iteration 16/1000 | Loss: 0.00001316
Iteration 17/1000 | Loss: 0.00001310
Iteration 18/1000 | Loss: 0.00001304
Iteration 19/1000 | Loss: 0.00001304
Iteration 20/1000 | Loss: 0.00001303
Iteration 21/1000 | Loss: 0.00001302
Iteration 22/1000 | Loss: 0.00001301
Iteration 23/1000 | Loss: 0.00001300
Iteration 24/1000 | Loss: 0.00001299
Iteration 25/1000 | Loss: 0.00001299
Iteration 26/1000 | Loss: 0.00001298
Iteration 27/1000 | Loss: 0.00001298
Iteration 28/1000 | Loss: 0.00001297
Iteration 29/1000 | Loss: 0.00001297
Iteration 30/1000 | Loss: 0.00001296
Iteration 31/1000 | Loss: 0.00001296
Iteration 32/1000 | Loss: 0.00001295
Iteration 33/1000 | Loss: 0.00001295
Iteration 34/1000 | Loss: 0.00001294
Iteration 35/1000 | Loss: 0.00001294
Iteration 36/1000 | Loss: 0.00001293
Iteration 37/1000 | Loss: 0.00001293
Iteration 38/1000 | Loss: 0.00001293
Iteration 39/1000 | Loss: 0.00001292
Iteration 40/1000 | Loss: 0.00001291
Iteration 41/1000 | Loss: 0.00001290
Iteration 42/1000 | Loss: 0.00001290
Iteration 43/1000 | Loss: 0.00001289
Iteration 44/1000 | Loss: 0.00001289
Iteration 45/1000 | Loss: 0.00001289
Iteration 46/1000 | Loss: 0.00001288
Iteration 47/1000 | Loss: 0.00001288
Iteration 48/1000 | Loss: 0.00001287
Iteration 49/1000 | Loss: 0.00001287
Iteration 50/1000 | Loss: 0.00001286
Iteration 51/1000 | Loss: 0.00001286
Iteration 52/1000 | Loss: 0.00001285
Iteration 53/1000 | Loss: 0.00001285
Iteration 54/1000 | Loss: 0.00001284
Iteration 55/1000 | Loss: 0.00001283
Iteration 56/1000 | Loss: 0.00001283
Iteration 57/1000 | Loss: 0.00001283
Iteration 58/1000 | Loss: 0.00001282
Iteration 59/1000 | Loss: 0.00001282
Iteration 60/1000 | Loss: 0.00001282
Iteration 61/1000 | Loss: 0.00001281
Iteration 62/1000 | Loss: 0.00001281
Iteration 63/1000 | Loss: 0.00001281
Iteration 64/1000 | Loss: 0.00001281
Iteration 65/1000 | Loss: 0.00001281
Iteration 66/1000 | Loss: 0.00001281
Iteration 67/1000 | Loss: 0.00001280
Iteration 68/1000 | Loss: 0.00001280
Iteration 69/1000 | Loss: 0.00001280
Iteration 70/1000 | Loss: 0.00001280
Iteration 71/1000 | Loss: 0.00001280
Iteration 72/1000 | Loss: 0.00001280
Iteration 73/1000 | Loss: 0.00001280
Iteration 74/1000 | Loss: 0.00001280
Iteration 75/1000 | Loss: 0.00001279
Iteration 76/1000 | Loss: 0.00001279
Iteration 77/1000 | Loss: 0.00001279
Iteration 78/1000 | Loss: 0.00001278
Iteration 79/1000 | Loss: 0.00001278
Iteration 80/1000 | Loss: 0.00001277
Iteration 81/1000 | Loss: 0.00001277
Iteration 82/1000 | Loss: 0.00001277
Iteration 83/1000 | Loss: 0.00001276
Iteration 84/1000 | Loss: 0.00001276
Iteration 85/1000 | Loss: 0.00001274
Iteration 86/1000 | Loss: 0.00001274
Iteration 87/1000 | Loss: 0.00001274
Iteration 88/1000 | Loss: 0.00001274
Iteration 89/1000 | Loss: 0.00001274
Iteration 90/1000 | Loss: 0.00001274
Iteration 91/1000 | Loss: 0.00001274
Iteration 92/1000 | Loss: 0.00001273
Iteration 93/1000 | Loss: 0.00001273
Iteration 94/1000 | Loss: 0.00001272
Iteration 95/1000 | Loss: 0.00001272
Iteration 96/1000 | Loss: 0.00001272
Iteration 97/1000 | Loss: 0.00001272
Iteration 98/1000 | Loss: 0.00001271
Iteration 99/1000 | Loss: 0.00001271
Iteration 100/1000 | Loss: 0.00001271
Iteration 101/1000 | Loss: 0.00001271
Iteration 102/1000 | Loss: 0.00001271
Iteration 103/1000 | Loss: 0.00001270
Iteration 104/1000 | Loss: 0.00001270
Iteration 105/1000 | Loss: 0.00001270
Iteration 106/1000 | Loss: 0.00001270
Iteration 107/1000 | Loss: 0.00001270
Iteration 108/1000 | Loss: 0.00001269
Iteration 109/1000 | Loss: 0.00001269
Iteration 110/1000 | Loss: 0.00001269
Iteration 111/1000 | Loss: 0.00001269
Iteration 112/1000 | Loss: 0.00001269
Iteration 113/1000 | Loss: 0.00001269
Iteration 114/1000 | Loss: 0.00001269
Iteration 115/1000 | Loss: 0.00001269
Iteration 116/1000 | Loss: 0.00001269
Iteration 117/1000 | Loss: 0.00001269
Iteration 118/1000 | Loss: 0.00001269
Iteration 119/1000 | Loss: 0.00001269
Iteration 120/1000 | Loss: 0.00001269
Iteration 121/1000 | Loss: 0.00001269
Iteration 122/1000 | Loss: 0.00001269
Iteration 123/1000 | Loss: 0.00001269
Iteration 124/1000 | Loss: 0.00001269
Iteration 125/1000 | Loss: 0.00001268
Iteration 126/1000 | Loss: 0.00001267
Iteration 127/1000 | Loss: 0.00001267
Iteration 128/1000 | Loss: 0.00001267
Iteration 129/1000 | Loss: 0.00001267
Iteration 130/1000 | Loss: 0.00001266
Iteration 131/1000 | Loss: 0.00001266
Iteration 132/1000 | Loss: 0.00001266
Iteration 133/1000 | Loss: 0.00001266
Iteration 134/1000 | Loss: 0.00001266
Iteration 135/1000 | Loss: 0.00001265
Iteration 136/1000 | Loss: 0.00001265
Iteration 137/1000 | Loss: 0.00001265
Iteration 138/1000 | Loss: 0.00001265
Iteration 139/1000 | Loss: 0.00001265
Iteration 140/1000 | Loss: 0.00001265
Iteration 141/1000 | Loss: 0.00001265
Iteration 142/1000 | Loss: 0.00001265
Iteration 143/1000 | Loss: 0.00001264
Iteration 144/1000 | Loss: 0.00001264
Iteration 145/1000 | Loss: 0.00001264
Iteration 146/1000 | Loss: 0.00001264
Iteration 147/1000 | Loss: 0.00001264
Iteration 148/1000 | Loss: 0.00001264
Iteration 149/1000 | Loss: 0.00001264
Iteration 150/1000 | Loss: 0.00001264
Iteration 151/1000 | Loss: 0.00001264
Iteration 152/1000 | Loss: 0.00001264
Iteration 153/1000 | Loss: 0.00001264
Iteration 154/1000 | Loss: 0.00001264
Iteration 155/1000 | Loss: 0.00001264
Iteration 156/1000 | Loss: 0.00001263
Iteration 157/1000 | Loss: 0.00001263
Iteration 158/1000 | Loss: 0.00001263
Iteration 159/1000 | Loss: 0.00001263
Iteration 160/1000 | Loss: 0.00001263
Iteration 161/1000 | Loss: 0.00001263
Iteration 162/1000 | Loss: 0.00001263
Iteration 163/1000 | Loss: 0.00001263
Iteration 164/1000 | Loss: 0.00001263
Iteration 165/1000 | Loss: 0.00001263
Iteration 166/1000 | Loss: 0.00001263
Iteration 167/1000 | Loss: 0.00001263
Iteration 168/1000 | Loss: 0.00001263
Iteration 169/1000 | Loss: 0.00001262
Iteration 170/1000 | Loss: 0.00001262
Iteration 171/1000 | Loss: 0.00001262
Iteration 172/1000 | Loss: 0.00001262
Iteration 173/1000 | Loss: 0.00001262
Iteration 174/1000 | Loss: 0.00001262
Iteration 175/1000 | Loss: 0.00001262
Iteration 176/1000 | Loss: 0.00001262
Iteration 177/1000 | Loss: 0.00001262
Iteration 178/1000 | Loss: 0.00001262
Iteration 179/1000 | Loss: 0.00001262
Iteration 180/1000 | Loss: 0.00001262
Iteration 181/1000 | Loss: 0.00001262
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 181. Stopping optimization.
Last 5 losses: [1.2623014299606439e-05, 1.2623014299606439e-05, 1.2623014299606439e-05, 1.2623014299606439e-05, 1.2623014299606439e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2623014299606439e-05

Optimization complete. Final v2v error: 2.996821880340576 mm

Highest mean error: 3.343689441680908 mm for frame 123

Lowest mean error: 2.7209980487823486 mm for frame 80

Saving results

Total time: 39.135558128356934
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_003/1005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_003/1005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_003/1005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00773437
Iteration 2/25 | Loss: 0.00153224
Iteration 3/25 | Loss: 0.00129930
Iteration 4/25 | Loss: 0.00126402
Iteration 5/25 | Loss: 0.00125826
Iteration 6/25 | Loss: 0.00125738
Iteration 7/25 | Loss: 0.00125738
Iteration 8/25 | Loss: 0.00125738
Iteration 9/25 | Loss: 0.00125738
Iteration 10/25 | Loss: 0.00125738
Iteration 11/25 | Loss: 0.00125738
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012573802378028631, 0.0012573802378028631, 0.0012573802378028631, 0.0012573802378028631, 0.0012573802378028631]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012573802378028631

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.19697654
Iteration 2/25 | Loss: 0.00103756
Iteration 3/25 | Loss: 0.00103754
Iteration 4/25 | Loss: 0.00103754
Iteration 5/25 | Loss: 0.00103754
Iteration 6/25 | Loss: 0.00103754
Iteration 7/25 | Loss: 0.00103753
Iteration 8/25 | Loss: 0.00103753
Iteration 9/25 | Loss: 0.00103753
Iteration 10/25 | Loss: 0.00103753
Iteration 11/25 | Loss: 0.00103753
Iteration 12/25 | Loss: 0.00103753
Iteration 13/25 | Loss: 0.00103753
Iteration 14/25 | Loss: 0.00103753
Iteration 15/25 | Loss: 0.00103753
Iteration 16/25 | Loss: 0.00103753
Iteration 17/25 | Loss: 0.00103753
Iteration 18/25 | Loss: 0.00103753
Iteration 19/25 | Loss: 0.00103753
Iteration 20/25 | Loss: 0.00103753
Iteration 21/25 | Loss: 0.00103753
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.001037533744238317, 0.001037533744238317, 0.001037533744238317, 0.001037533744238317, 0.001037533744238317]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001037533744238317

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00103753
Iteration 2/1000 | Loss: 0.00004849
Iteration 3/1000 | Loss: 0.00003309
Iteration 4/1000 | Loss: 0.00002755
Iteration 5/1000 | Loss: 0.00002588
Iteration 6/1000 | Loss: 0.00002488
Iteration 7/1000 | Loss: 0.00002380
Iteration 8/1000 | Loss: 0.00002338
Iteration 9/1000 | Loss: 0.00002299
Iteration 10/1000 | Loss: 0.00002261
Iteration 11/1000 | Loss: 0.00002224
Iteration 12/1000 | Loss: 0.00002220
Iteration 13/1000 | Loss: 0.00002210
Iteration 14/1000 | Loss: 0.00002209
Iteration 15/1000 | Loss: 0.00002209
Iteration 16/1000 | Loss: 0.00002208
Iteration 17/1000 | Loss: 0.00002201
Iteration 18/1000 | Loss: 0.00002181
Iteration 19/1000 | Loss: 0.00002178
Iteration 20/1000 | Loss: 0.00002169
Iteration 21/1000 | Loss: 0.00002168
Iteration 22/1000 | Loss: 0.00002159
Iteration 23/1000 | Loss: 0.00002151
Iteration 24/1000 | Loss: 0.00002148
Iteration 25/1000 | Loss: 0.00002144
Iteration 26/1000 | Loss: 0.00002138
Iteration 27/1000 | Loss: 0.00002133
Iteration 28/1000 | Loss: 0.00002133
Iteration 29/1000 | Loss: 0.00002133
Iteration 30/1000 | Loss: 0.00002133
Iteration 31/1000 | Loss: 0.00002132
Iteration 32/1000 | Loss: 0.00002132
Iteration 33/1000 | Loss: 0.00002130
Iteration 34/1000 | Loss: 0.00002127
Iteration 35/1000 | Loss: 0.00002127
Iteration 36/1000 | Loss: 0.00002127
Iteration 37/1000 | Loss: 0.00002127
Iteration 38/1000 | Loss: 0.00002127
Iteration 39/1000 | Loss: 0.00002127
Iteration 40/1000 | Loss: 0.00002127
Iteration 41/1000 | Loss: 0.00002127
Iteration 42/1000 | Loss: 0.00002127
Iteration 43/1000 | Loss: 0.00002123
Iteration 44/1000 | Loss: 0.00002123
Iteration 45/1000 | Loss: 0.00002122
Iteration 46/1000 | Loss: 0.00002122
Iteration 47/1000 | Loss: 0.00002121
Iteration 48/1000 | Loss: 0.00002121
Iteration 49/1000 | Loss: 0.00002121
Iteration 50/1000 | Loss: 0.00002121
Iteration 51/1000 | Loss: 0.00002121
Iteration 52/1000 | Loss: 0.00002119
Iteration 53/1000 | Loss: 0.00002119
Iteration 54/1000 | Loss: 0.00002118
Iteration 55/1000 | Loss: 0.00002118
Iteration 56/1000 | Loss: 0.00002118
Iteration 57/1000 | Loss: 0.00002118
Iteration 58/1000 | Loss: 0.00002118
Iteration 59/1000 | Loss: 0.00002117
Iteration 60/1000 | Loss: 0.00002117
Iteration 61/1000 | Loss: 0.00002117
Iteration 62/1000 | Loss: 0.00002116
Iteration 63/1000 | Loss: 0.00002116
Iteration 64/1000 | Loss: 0.00002116
Iteration 65/1000 | Loss: 0.00002115
Iteration 66/1000 | Loss: 0.00002115
Iteration 67/1000 | Loss: 0.00002115
Iteration 68/1000 | Loss: 0.00002115
Iteration 69/1000 | Loss: 0.00002114
Iteration 70/1000 | Loss: 0.00002114
Iteration 71/1000 | Loss: 0.00002114
Iteration 72/1000 | Loss: 0.00002113
Iteration 73/1000 | Loss: 0.00002113
Iteration 74/1000 | Loss: 0.00002113
Iteration 75/1000 | Loss: 0.00002113
Iteration 76/1000 | Loss: 0.00002113
Iteration 77/1000 | Loss: 0.00002112
Iteration 78/1000 | Loss: 0.00002112
Iteration 79/1000 | Loss: 0.00002112
Iteration 80/1000 | Loss: 0.00002112
Iteration 81/1000 | Loss: 0.00002112
Iteration 82/1000 | Loss: 0.00002112
Iteration 83/1000 | Loss: 0.00002112
Iteration 84/1000 | Loss: 0.00002112
Iteration 85/1000 | Loss: 0.00002112
Iteration 86/1000 | Loss: 0.00002112
Iteration 87/1000 | Loss: 0.00002111
Iteration 88/1000 | Loss: 0.00002111
Iteration 89/1000 | Loss: 0.00002111
Iteration 90/1000 | Loss: 0.00002111
Iteration 91/1000 | Loss: 0.00002110
Iteration 92/1000 | Loss: 0.00002110
Iteration 93/1000 | Loss: 0.00002110
Iteration 94/1000 | Loss: 0.00002109
Iteration 95/1000 | Loss: 0.00002109
Iteration 96/1000 | Loss: 0.00002109
Iteration 97/1000 | Loss: 0.00002109
Iteration 98/1000 | Loss: 0.00002109
Iteration 99/1000 | Loss: 0.00002109
Iteration 100/1000 | Loss: 0.00002109
Iteration 101/1000 | Loss: 0.00002109
Iteration 102/1000 | Loss: 0.00002108
Iteration 103/1000 | Loss: 0.00002108
Iteration 104/1000 | Loss: 0.00002108
Iteration 105/1000 | Loss: 0.00002108
Iteration 106/1000 | Loss: 0.00002107
Iteration 107/1000 | Loss: 0.00002107
Iteration 108/1000 | Loss: 0.00002107
Iteration 109/1000 | Loss: 0.00002107
Iteration 110/1000 | Loss: 0.00002106
Iteration 111/1000 | Loss: 0.00002106
Iteration 112/1000 | Loss: 0.00002106
Iteration 113/1000 | Loss: 0.00002106
Iteration 114/1000 | Loss: 0.00002106
Iteration 115/1000 | Loss: 0.00002105
Iteration 116/1000 | Loss: 0.00002105
Iteration 117/1000 | Loss: 0.00002105
Iteration 118/1000 | Loss: 0.00002105
Iteration 119/1000 | Loss: 0.00002104
Iteration 120/1000 | Loss: 0.00002104
Iteration 121/1000 | Loss: 0.00002104
Iteration 122/1000 | Loss: 0.00002104
Iteration 123/1000 | Loss: 0.00002104
Iteration 124/1000 | Loss: 0.00002104
Iteration 125/1000 | Loss: 0.00002104
Iteration 126/1000 | Loss: 0.00002104
Iteration 127/1000 | Loss: 0.00002104
Iteration 128/1000 | Loss: 0.00002103
Iteration 129/1000 | Loss: 0.00002103
Iteration 130/1000 | Loss: 0.00002103
Iteration 131/1000 | Loss: 0.00002103
Iteration 132/1000 | Loss: 0.00002103
Iteration 133/1000 | Loss: 0.00002103
Iteration 134/1000 | Loss: 0.00002102
Iteration 135/1000 | Loss: 0.00002102
Iteration 136/1000 | Loss: 0.00002102
Iteration 137/1000 | Loss: 0.00002102
Iteration 138/1000 | Loss: 0.00002102
Iteration 139/1000 | Loss: 0.00002102
Iteration 140/1000 | Loss: 0.00002102
Iteration 141/1000 | Loss: 0.00002102
Iteration 142/1000 | Loss: 0.00002102
Iteration 143/1000 | Loss: 0.00002102
Iteration 144/1000 | Loss: 0.00002102
Iteration 145/1000 | Loss: 0.00002102
Iteration 146/1000 | Loss: 0.00002102
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 146. Stopping optimization.
Last 5 losses: [2.1019824998802505e-05, 2.1019824998802505e-05, 2.1019824998802505e-05, 2.1019824998802505e-05, 2.1019824998802505e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1019824998802505e-05

Optimization complete. Final v2v error: 3.816378593444824 mm

Highest mean error: 4.880430221557617 mm for frame 138

Lowest mean error: 3.2402141094207764 mm for frame 58

Saving results

Total time: 47.49118089675903
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_003/1043/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_003/1043.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_003/1043
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00802173
Iteration 2/25 | Loss: 0.00143569
Iteration 3/25 | Loss: 0.00125969
Iteration 4/25 | Loss: 0.00124057
Iteration 5/25 | Loss: 0.00123527
Iteration 6/25 | Loss: 0.00123436
Iteration 7/25 | Loss: 0.00123436
Iteration 8/25 | Loss: 0.00123436
Iteration 9/25 | Loss: 0.00123436
Iteration 10/25 | Loss: 0.00123436
Iteration 11/25 | Loss: 0.00123436
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012343626003712416, 0.0012343626003712416, 0.0012343626003712416, 0.0012343626003712416, 0.0012343626003712416]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012343626003712416

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.27198660
Iteration 2/25 | Loss: 0.00129923
Iteration 3/25 | Loss: 0.00129923
Iteration 4/25 | Loss: 0.00129923
Iteration 5/25 | Loss: 0.00129923
Iteration 6/25 | Loss: 0.00129923
Iteration 7/25 | Loss: 0.00129923
Iteration 8/25 | Loss: 0.00129922
Iteration 9/25 | Loss: 0.00129922
Iteration 10/25 | Loss: 0.00129922
Iteration 11/25 | Loss: 0.00129922
Iteration 12/25 | Loss: 0.00129922
Iteration 13/25 | Loss: 0.00129922
Iteration 14/25 | Loss: 0.00129922
Iteration 15/25 | Loss: 0.00129922
Iteration 16/25 | Loss: 0.00129922
Iteration 17/25 | Loss: 0.00129922
Iteration 18/25 | Loss: 0.00129922
Iteration 19/25 | Loss: 0.00129922
Iteration 20/25 | Loss: 0.00129922
Iteration 21/25 | Loss: 0.00129922
Iteration 22/25 | Loss: 0.00129922
Iteration 23/25 | Loss: 0.00129922
Iteration 24/25 | Loss: 0.00129922
Iteration 25/25 | Loss: 0.00129922

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00129922
Iteration 2/1000 | Loss: 0.00004418
Iteration 3/1000 | Loss: 0.00002985
Iteration 4/1000 | Loss: 0.00002546
Iteration 5/1000 | Loss: 0.00002341
Iteration 6/1000 | Loss: 0.00002187
Iteration 7/1000 | Loss: 0.00002071
Iteration 8/1000 | Loss: 0.00002003
Iteration 9/1000 | Loss: 0.00001952
Iteration 10/1000 | Loss: 0.00001920
Iteration 11/1000 | Loss: 0.00001889
Iteration 12/1000 | Loss: 0.00001867
Iteration 13/1000 | Loss: 0.00001846
Iteration 14/1000 | Loss: 0.00001826
Iteration 15/1000 | Loss: 0.00001823
Iteration 16/1000 | Loss: 0.00001818
Iteration 17/1000 | Loss: 0.00001817
Iteration 18/1000 | Loss: 0.00001815
Iteration 19/1000 | Loss: 0.00001811
Iteration 20/1000 | Loss: 0.00001811
Iteration 21/1000 | Loss: 0.00001807
Iteration 22/1000 | Loss: 0.00001804
Iteration 23/1000 | Loss: 0.00001801
Iteration 24/1000 | Loss: 0.00001796
Iteration 25/1000 | Loss: 0.00001789
Iteration 26/1000 | Loss: 0.00001788
Iteration 27/1000 | Loss: 0.00001785
Iteration 28/1000 | Loss: 0.00001785
Iteration 29/1000 | Loss: 0.00001784
Iteration 30/1000 | Loss: 0.00001784
Iteration 31/1000 | Loss: 0.00001783
Iteration 32/1000 | Loss: 0.00001783
Iteration 33/1000 | Loss: 0.00001782
Iteration 34/1000 | Loss: 0.00001782
Iteration 35/1000 | Loss: 0.00001781
Iteration 36/1000 | Loss: 0.00001781
Iteration 37/1000 | Loss: 0.00001780
Iteration 38/1000 | Loss: 0.00001780
Iteration 39/1000 | Loss: 0.00001780
Iteration 40/1000 | Loss: 0.00001779
Iteration 41/1000 | Loss: 0.00001779
Iteration 42/1000 | Loss: 0.00001779
Iteration 43/1000 | Loss: 0.00001779
Iteration 44/1000 | Loss: 0.00001778
Iteration 45/1000 | Loss: 0.00001778
Iteration 46/1000 | Loss: 0.00001778
Iteration 47/1000 | Loss: 0.00001778
Iteration 48/1000 | Loss: 0.00001777
Iteration 49/1000 | Loss: 0.00001777
Iteration 50/1000 | Loss: 0.00001777
Iteration 51/1000 | Loss: 0.00001776
Iteration 52/1000 | Loss: 0.00001776
Iteration 53/1000 | Loss: 0.00001775
Iteration 54/1000 | Loss: 0.00001775
Iteration 55/1000 | Loss: 0.00001775
Iteration 56/1000 | Loss: 0.00001774
Iteration 57/1000 | Loss: 0.00001774
Iteration 58/1000 | Loss: 0.00001774
Iteration 59/1000 | Loss: 0.00001774
Iteration 60/1000 | Loss: 0.00001773
Iteration 61/1000 | Loss: 0.00001773
Iteration 62/1000 | Loss: 0.00001773
Iteration 63/1000 | Loss: 0.00001772
Iteration 64/1000 | Loss: 0.00001772
Iteration 65/1000 | Loss: 0.00001772
Iteration 66/1000 | Loss: 0.00001772
Iteration 67/1000 | Loss: 0.00001772
Iteration 68/1000 | Loss: 0.00001772
Iteration 69/1000 | Loss: 0.00001771
Iteration 70/1000 | Loss: 0.00001771
Iteration 71/1000 | Loss: 0.00001771
Iteration 72/1000 | Loss: 0.00001771
Iteration 73/1000 | Loss: 0.00001771
Iteration 74/1000 | Loss: 0.00001771
Iteration 75/1000 | Loss: 0.00001770
Iteration 76/1000 | Loss: 0.00001770
Iteration 77/1000 | Loss: 0.00001770
Iteration 78/1000 | Loss: 0.00001770
Iteration 79/1000 | Loss: 0.00001769
Iteration 80/1000 | Loss: 0.00001769
Iteration 81/1000 | Loss: 0.00001769
Iteration 82/1000 | Loss: 0.00001769
Iteration 83/1000 | Loss: 0.00001769
Iteration 84/1000 | Loss: 0.00001769
Iteration 85/1000 | Loss: 0.00001769
Iteration 86/1000 | Loss: 0.00001768
Iteration 87/1000 | Loss: 0.00001768
Iteration 88/1000 | Loss: 0.00001768
Iteration 89/1000 | Loss: 0.00001768
Iteration 90/1000 | Loss: 0.00001768
Iteration 91/1000 | Loss: 0.00001768
Iteration 92/1000 | Loss: 0.00001767
Iteration 93/1000 | Loss: 0.00001767
Iteration 94/1000 | Loss: 0.00001767
Iteration 95/1000 | Loss: 0.00001767
Iteration 96/1000 | Loss: 0.00001767
Iteration 97/1000 | Loss: 0.00001767
Iteration 98/1000 | Loss: 0.00001767
Iteration 99/1000 | Loss: 0.00001767
Iteration 100/1000 | Loss: 0.00001767
Iteration 101/1000 | Loss: 0.00001767
Iteration 102/1000 | Loss: 0.00001767
Iteration 103/1000 | Loss: 0.00001767
Iteration 104/1000 | Loss: 0.00001767
Iteration 105/1000 | Loss: 0.00001767
Iteration 106/1000 | Loss: 0.00001766
Iteration 107/1000 | Loss: 0.00001766
Iteration 108/1000 | Loss: 0.00001766
Iteration 109/1000 | Loss: 0.00001766
Iteration 110/1000 | Loss: 0.00001765
Iteration 111/1000 | Loss: 0.00001765
Iteration 112/1000 | Loss: 0.00001765
Iteration 113/1000 | Loss: 0.00001765
Iteration 114/1000 | Loss: 0.00001764
Iteration 115/1000 | Loss: 0.00001764
Iteration 116/1000 | Loss: 0.00001764
Iteration 117/1000 | Loss: 0.00001764
Iteration 118/1000 | Loss: 0.00001763
Iteration 119/1000 | Loss: 0.00001762
Iteration 120/1000 | Loss: 0.00001762
Iteration 121/1000 | Loss: 0.00001762
Iteration 122/1000 | Loss: 0.00001762
Iteration 123/1000 | Loss: 0.00001762
Iteration 124/1000 | Loss: 0.00001761
Iteration 125/1000 | Loss: 0.00001761
Iteration 126/1000 | Loss: 0.00001761
Iteration 127/1000 | Loss: 0.00001761
Iteration 128/1000 | Loss: 0.00001760
Iteration 129/1000 | Loss: 0.00001760
Iteration 130/1000 | Loss: 0.00001760
Iteration 131/1000 | Loss: 0.00001759
Iteration 132/1000 | Loss: 0.00001759
Iteration 133/1000 | Loss: 0.00001759
Iteration 134/1000 | Loss: 0.00001759
Iteration 135/1000 | Loss: 0.00001759
Iteration 136/1000 | Loss: 0.00001758
Iteration 137/1000 | Loss: 0.00001758
Iteration 138/1000 | Loss: 0.00001758
Iteration 139/1000 | Loss: 0.00001758
Iteration 140/1000 | Loss: 0.00001758
Iteration 141/1000 | Loss: 0.00001758
Iteration 142/1000 | Loss: 0.00001758
Iteration 143/1000 | Loss: 0.00001758
Iteration 144/1000 | Loss: 0.00001758
Iteration 145/1000 | Loss: 0.00001758
Iteration 146/1000 | Loss: 0.00001758
Iteration 147/1000 | Loss: 0.00001758
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 147. Stopping optimization.
Last 5 losses: [1.757947939040605e-05, 1.757947939040605e-05, 1.757947939040605e-05, 1.757947939040605e-05, 1.757947939040605e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.757947939040605e-05

Optimization complete. Final v2v error: 3.5303001403808594 mm

Highest mean error: 4.816161632537842 mm for frame 152

Lowest mean error: 2.700624465942383 mm for frame 2

Saving results

Total time: 48.565919399261475
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_003/1026/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_003/1026.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_003/1026
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00407218
Iteration 2/25 | Loss: 0.00139030
Iteration 3/25 | Loss: 0.00127225
Iteration 4/25 | Loss: 0.00125960
Iteration 5/25 | Loss: 0.00125581
Iteration 6/25 | Loss: 0.00125458
Iteration 7/25 | Loss: 0.00125440
Iteration 8/25 | Loss: 0.00125440
Iteration 9/25 | Loss: 0.00125440
Iteration 10/25 | Loss: 0.00125440
Iteration 11/25 | Loss: 0.00125440
Iteration 12/25 | Loss: 0.00125440
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0012543993070721626, 0.0012543993070721626, 0.0012543993070721626, 0.0012543993070721626, 0.0012543993070721626]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012543993070721626

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42196333
Iteration 2/25 | Loss: 0.00124023
Iteration 3/25 | Loss: 0.00124022
Iteration 4/25 | Loss: 0.00124022
Iteration 5/25 | Loss: 0.00124022
Iteration 6/25 | Loss: 0.00124022
Iteration 7/25 | Loss: 0.00124022
Iteration 8/25 | Loss: 0.00124022
Iteration 9/25 | Loss: 0.00124022
Iteration 10/25 | Loss: 0.00124022
Iteration 11/25 | Loss: 0.00124022
Iteration 12/25 | Loss: 0.00124022
Iteration 13/25 | Loss: 0.00124022
Iteration 14/25 | Loss: 0.00124022
Iteration 15/25 | Loss: 0.00124022
Iteration 16/25 | Loss: 0.00124022
Iteration 17/25 | Loss: 0.00124022
Iteration 18/25 | Loss: 0.00124022
Iteration 19/25 | Loss: 0.00124022
Iteration 20/25 | Loss: 0.00124022
Iteration 21/25 | Loss: 0.00124022
Iteration 22/25 | Loss: 0.00124022
Iteration 23/25 | Loss: 0.00124022
Iteration 24/25 | Loss: 0.00124022
Iteration 25/25 | Loss: 0.00124022

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00124022
Iteration 2/1000 | Loss: 0.00003640
Iteration 3/1000 | Loss: 0.00002228
Iteration 4/1000 | Loss: 0.00001950
Iteration 5/1000 | Loss: 0.00001776
Iteration 6/1000 | Loss: 0.00001682
Iteration 7/1000 | Loss: 0.00001623
Iteration 8/1000 | Loss: 0.00001575
Iteration 9/1000 | Loss: 0.00001544
Iteration 10/1000 | Loss: 0.00001517
Iteration 11/1000 | Loss: 0.00001496
Iteration 12/1000 | Loss: 0.00001473
Iteration 13/1000 | Loss: 0.00001471
Iteration 14/1000 | Loss: 0.00001464
Iteration 15/1000 | Loss: 0.00001460
Iteration 16/1000 | Loss: 0.00001449
Iteration 17/1000 | Loss: 0.00001442
Iteration 18/1000 | Loss: 0.00001437
Iteration 19/1000 | Loss: 0.00001433
Iteration 20/1000 | Loss: 0.00001433
Iteration 21/1000 | Loss: 0.00001432
Iteration 22/1000 | Loss: 0.00001432
Iteration 23/1000 | Loss: 0.00001432
Iteration 24/1000 | Loss: 0.00001431
Iteration 25/1000 | Loss: 0.00001431
Iteration 26/1000 | Loss: 0.00001431
Iteration 27/1000 | Loss: 0.00001430
Iteration 28/1000 | Loss: 0.00001430
Iteration 29/1000 | Loss: 0.00001430
Iteration 30/1000 | Loss: 0.00001429
Iteration 31/1000 | Loss: 0.00001429
Iteration 32/1000 | Loss: 0.00001428
Iteration 33/1000 | Loss: 0.00001428
Iteration 34/1000 | Loss: 0.00001428
Iteration 35/1000 | Loss: 0.00001428
Iteration 36/1000 | Loss: 0.00001428
Iteration 37/1000 | Loss: 0.00001427
Iteration 38/1000 | Loss: 0.00001427
Iteration 39/1000 | Loss: 0.00001427
Iteration 40/1000 | Loss: 0.00001426
Iteration 41/1000 | Loss: 0.00001426
Iteration 42/1000 | Loss: 0.00001425
Iteration 43/1000 | Loss: 0.00001425
Iteration 44/1000 | Loss: 0.00001425
Iteration 45/1000 | Loss: 0.00001424
Iteration 46/1000 | Loss: 0.00001424
Iteration 47/1000 | Loss: 0.00001424
Iteration 48/1000 | Loss: 0.00001424
Iteration 49/1000 | Loss: 0.00001424
Iteration 50/1000 | Loss: 0.00001423
Iteration 51/1000 | Loss: 0.00001423
Iteration 52/1000 | Loss: 0.00001423
Iteration 53/1000 | Loss: 0.00001423
Iteration 54/1000 | Loss: 0.00001423
Iteration 55/1000 | Loss: 0.00001422
Iteration 56/1000 | Loss: 0.00001422
Iteration 57/1000 | Loss: 0.00001422
Iteration 58/1000 | Loss: 0.00001422
Iteration 59/1000 | Loss: 0.00001421
Iteration 60/1000 | Loss: 0.00001421
Iteration 61/1000 | Loss: 0.00001421
Iteration 62/1000 | Loss: 0.00001420
Iteration 63/1000 | Loss: 0.00001420
Iteration 64/1000 | Loss: 0.00001420
Iteration 65/1000 | Loss: 0.00001420
Iteration 66/1000 | Loss: 0.00001420
Iteration 67/1000 | Loss: 0.00001420
Iteration 68/1000 | Loss: 0.00001419
Iteration 69/1000 | Loss: 0.00001419
Iteration 70/1000 | Loss: 0.00001419
Iteration 71/1000 | Loss: 0.00001418
Iteration 72/1000 | Loss: 0.00001418
Iteration 73/1000 | Loss: 0.00001418
Iteration 74/1000 | Loss: 0.00001417
Iteration 75/1000 | Loss: 0.00001417
Iteration 76/1000 | Loss: 0.00001417
Iteration 77/1000 | Loss: 0.00001417
Iteration 78/1000 | Loss: 0.00001417
Iteration 79/1000 | Loss: 0.00001416
Iteration 80/1000 | Loss: 0.00001416
Iteration 81/1000 | Loss: 0.00001416
Iteration 82/1000 | Loss: 0.00001416
Iteration 83/1000 | Loss: 0.00001416
Iteration 84/1000 | Loss: 0.00001415
Iteration 85/1000 | Loss: 0.00001415
Iteration 86/1000 | Loss: 0.00001415
Iteration 87/1000 | Loss: 0.00001415
Iteration 88/1000 | Loss: 0.00001414
Iteration 89/1000 | Loss: 0.00001414
Iteration 90/1000 | Loss: 0.00001413
Iteration 91/1000 | Loss: 0.00001413
Iteration 92/1000 | Loss: 0.00001413
Iteration 93/1000 | Loss: 0.00001412
Iteration 94/1000 | Loss: 0.00001412
Iteration 95/1000 | Loss: 0.00001412
Iteration 96/1000 | Loss: 0.00001412
Iteration 97/1000 | Loss: 0.00001412
Iteration 98/1000 | Loss: 0.00001411
Iteration 99/1000 | Loss: 0.00001411
Iteration 100/1000 | Loss: 0.00001411
Iteration 101/1000 | Loss: 0.00001411
Iteration 102/1000 | Loss: 0.00001411
Iteration 103/1000 | Loss: 0.00001411
Iteration 104/1000 | Loss: 0.00001411
Iteration 105/1000 | Loss: 0.00001411
Iteration 106/1000 | Loss: 0.00001411
Iteration 107/1000 | Loss: 0.00001411
Iteration 108/1000 | Loss: 0.00001410
Iteration 109/1000 | Loss: 0.00001410
Iteration 110/1000 | Loss: 0.00001410
Iteration 111/1000 | Loss: 0.00001410
Iteration 112/1000 | Loss: 0.00001409
Iteration 113/1000 | Loss: 0.00001409
Iteration 114/1000 | Loss: 0.00001409
Iteration 115/1000 | Loss: 0.00001409
Iteration 116/1000 | Loss: 0.00001409
Iteration 117/1000 | Loss: 0.00001409
Iteration 118/1000 | Loss: 0.00001409
Iteration 119/1000 | Loss: 0.00001409
Iteration 120/1000 | Loss: 0.00001408
Iteration 121/1000 | Loss: 0.00001408
Iteration 122/1000 | Loss: 0.00001408
Iteration 123/1000 | Loss: 0.00001408
Iteration 124/1000 | Loss: 0.00001408
Iteration 125/1000 | Loss: 0.00001408
Iteration 126/1000 | Loss: 0.00001408
Iteration 127/1000 | Loss: 0.00001408
Iteration 128/1000 | Loss: 0.00001408
Iteration 129/1000 | Loss: 0.00001408
Iteration 130/1000 | Loss: 0.00001408
Iteration 131/1000 | Loss: 0.00001408
Iteration 132/1000 | Loss: 0.00001407
Iteration 133/1000 | Loss: 0.00001407
Iteration 134/1000 | Loss: 0.00001407
Iteration 135/1000 | Loss: 0.00001407
Iteration 136/1000 | Loss: 0.00001407
Iteration 137/1000 | Loss: 0.00001407
Iteration 138/1000 | Loss: 0.00001407
Iteration 139/1000 | Loss: 0.00001407
Iteration 140/1000 | Loss: 0.00001407
Iteration 141/1000 | Loss: 0.00001407
Iteration 142/1000 | Loss: 0.00001407
Iteration 143/1000 | Loss: 0.00001407
Iteration 144/1000 | Loss: 0.00001407
Iteration 145/1000 | Loss: 0.00001407
Iteration 146/1000 | Loss: 0.00001407
Iteration 147/1000 | Loss: 0.00001406
Iteration 148/1000 | Loss: 0.00001406
Iteration 149/1000 | Loss: 0.00001406
Iteration 150/1000 | Loss: 0.00001406
Iteration 151/1000 | Loss: 0.00001406
Iteration 152/1000 | Loss: 0.00001406
Iteration 153/1000 | Loss: 0.00001406
Iteration 154/1000 | Loss: 0.00001406
Iteration 155/1000 | Loss: 0.00001406
Iteration 156/1000 | Loss: 0.00001406
Iteration 157/1000 | Loss: 0.00001406
Iteration 158/1000 | Loss: 0.00001406
Iteration 159/1000 | Loss: 0.00001406
Iteration 160/1000 | Loss: 0.00001406
Iteration 161/1000 | Loss: 0.00001406
Iteration 162/1000 | Loss: 0.00001405
Iteration 163/1000 | Loss: 0.00001405
Iteration 164/1000 | Loss: 0.00001405
Iteration 165/1000 | Loss: 0.00001405
Iteration 166/1000 | Loss: 0.00001405
Iteration 167/1000 | Loss: 0.00001405
Iteration 168/1000 | Loss: 0.00001405
Iteration 169/1000 | Loss: 0.00001405
Iteration 170/1000 | Loss: 0.00001404
Iteration 171/1000 | Loss: 0.00001404
Iteration 172/1000 | Loss: 0.00001404
Iteration 173/1000 | Loss: 0.00001404
Iteration 174/1000 | Loss: 0.00001404
Iteration 175/1000 | Loss: 0.00001404
Iteration 176/1000 | Loss: 0.00001404
Iteration 177/1000 | Loss: 0.00001404
Iteration 178/1000 | Loss: 0.00001404
Iteration 179/1000 | Loss: 0.00001404
Iteration 180/1000 | Loss: 0.00001404
Iteration 181/1000 | Loss: 0.00001404
Iteration 182/1000 | Loss: 0.00001404
Iteration 183/1000 | Loss: 0.00001404
Iteration 184/1000 | Loss: 0.00001403
Iteration 185/1000 | Loss: 0.00001403
Iteration 186/1000 | Loss: 0.00001403
Iteration 187/1000 | Loss: 0.00001403
Iteration 188/1000 | Loss: 0.00001403
Iteration 189/1000 | Loss: 0.00001403
Iteration 190/1000 | Loss: 0.00001402
Iteration 191/1000 | Loss: 0.00001402
Iteration 192/1000 | Loss: 0.00001402
Iteration 193/1000 | Loss: 0.00001402
Iteration 194/1000 | Loss: 0.00001402
Iteration 195/1000 | Loss: 0.00001402
Iteration 196/1000 | Loss: 0.00001402
Iteration 197/1000 | Loss: 0.00001402
Iteration 198/1000 | Loss: 0.00001402
Iteration 199/1000 | Loss: 0.00001402
Iteration 200/1000 | Loss: 0.00001402
Iteration 201/1000 | Loss: 0.00001402
Iteration 202/1000 | Loss: 0.00001402
Iteration 203/1000 | Loss: 0.00001402
Iteration 204/1000 | Loss: 0.00001402
Iteration 205/1000 | Loss: 0.00001402
Iteration 206/1000 | Loss: 0.00001402
Iteration 207/1000 | Loss: 0.00001402
Iteration 208/1000 | Loss: 0.00001402
Iteration 209/1000 | Loss: 0.00001402
Iteration 210/1000 | Loss: 0.00001402
Iteration 211/1000 | Loss: 0.00001402
Iteration 212/1000 | Loss: 0.00001402
Iteration 213/1000 | Loss: 0.00001402
Iteration 214/1000 | Loss: 0.00001402
Iteration 215/1000 | Loss: 0.00001402
Iteration 216/1000 | Loss: 0.00001402
Iteration 217/1000 | Loss: 0.00001402
Iteration 218/1000 | Loss: 0.00001402
Iteration 219/1000 | Loss: 0.00001402
Iteration 220/1000 | Loss: 0.00001402
Iteration 221/1000 | Loss: 0.00001402
Iteration 222/1000 | Loss: 0.00001402
Iteration 223/1000 | Loss: 0.00001402
Iteration 224/1000 | Loss: 0.00001402
Iteration 225/1000 | Loss: 0.00001402
Iteration 226/1000 | Loss: 0.00001402
Iteration 227/1000 | Loss: 0.00001402
Iteration 228/1000 | Loss: 0.00001402
Iteration 229/1000 | Loss: 0.00001402
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 229. Stopping optimization.
Last 5 losses: [1.4022480172570795e-05, 1.4022480172570795e-05, 1.4022480172570795e-05, 1.4022480172570795e-05, 1.4022480172570795e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4022480172570795e-05

Optimization complete. Final v2v error: 3.1817467212677 mm

Highest mean error: 4.55303430557251 mm for frame 67

Lowest mean error: 2.8494718074798584 mm for frame 44

Saving results

Total time: 43.266467809677124
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_003/1063/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_003/1063.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_003/1063
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00425101
Iteration 2/25 | Loss: 0.00126755
Iteration 3/25 | Loss: 0.00120061
Iteration 4/25 | Loss: 0.00119239
Iteration 5/25 | Loss: 0.00118963
Iteration 6/25 | Loss: 0.00118893
Iteration 7/25 | Loss: 0.00118893
Iteration 8/25 | Loss: 0.00118893
Iteration 9/25 | Loss: 0.00118893
Iteration 10/25 | Loss: 0.00118893
Iteration 11/25 | Loss: 0.00118893
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001188932335935533, 0.001188932335935533, 0.001188932335935533, 0.001188932335935533, 0.001188932335935533]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001188932335935533

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.76861882
Iteration 2/25 | Loss: 0.00129939
Iteration 3/25 | Loss: 0.00129937
Iteration 4/25 | Loss: 0.00129937
Iteration 5/25 | Loss: 0.00129937
Iteration 6/25 | Loss: 0.00129937
Iteration 7/25 | Loss: 0.00129937
Iteration 8/25 | Loss: 0.00129937
Iteration 9/25 | Loss: 0.00129937
Iteration 10/25 | Loss: 0.00129937
Iteration 11/25 | Loss: 0.00129937
Iteration 12/25 | Loss: 0.00129937
Iteration 13/25 | Loss: 0.00129937
Iteration 14/25 | Loss: 0.00129937
Iteration 15/25 | Loss: 0.00129937
Iteration 16/25 | Loss: 0.00129937
Iteration 17/25 | Loss: 0.00129937
Iteration 18/25 | Loss: 0.00129937
Iteration 19/25 | Loss: 0.00129937
Iteration 20/25 | Loss: 0.00129937
Iteration 21/25 | Loss: 0.00129937
Iteration 22/25 | Loss: 0.00129937
Iteration 23/25 | Loss: 0.00129937
Iteration 24/25 | Loss: 0.00129937
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0012993667041882873, 0.0012993667041882873, 0.0012993667041882873, 0.0012993667041882873, 0.0012993667041882873]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012993667041882873

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00129937
Iteration 2/1000 | Loss: 0.00002951
Iteration 3/1000 | Loss: 0.00001850
Iteration 4/1000 | Loss: 0.00001556
Iteration 5/1000 | Loss: 0.00001465
Iteration 6/1000 | Loss: 0.00001386
Iteration 7/1000 | Loss: 0.00001335
Iteration 8/1000 | Loss: 0.00001296
Iteration 9/1000 | Loss: 0.00001264
Iteration 10/1000 | Loss: 0.00001238
Iteration 11/1000 | Loss: 0.00001213
Iteration 12/1000 | Loss: 0.00001196
Iteration 13/1000 | Loss: 0.00001191
Iteration 14/1000 | Loss: 0.00001185
Iteration 15/1000 | Loss: 0.00001184
Iteration 16/1000 | Loss: 0.00001182
Iteration 17/1000 | Loss: 0.00001180
Iteration 18/1000 | Loss: 0.00001178
Iteration 19/1000 | Loss: 0.00001177
Iteration 20/1000 | Loss: 0.00001175
Iteration 21/1000 | Loss: 0.00001174
Iteration 22/1000 | Loss: 0.00001173
Iteration 23/1000 | Loss: 0.00001172
Iteration 24/1000 | Loss: 0.00001171
Iteration 25/1000 | Loss: 0.00001170
Iteration 26/1000 | Loss: 0.00001169
Iteration 27/1000 | Loss: 0.00001169
Iteration 28/1000 | Loss: 0.00001168
Iteration 29/1000 | Loss: 0.00001167
Iteration 30/1000 | Loss: 0.00001166
Iteration 31/1000 | Loss: 0.00001166
Iteration 32/1000 | Loss: 0.00001166
Iteration 33/1000 | Loss: 0.00001166
Iteration 34/1000 | Loss: 0.00001166
Iteration 35/1000 | Loss: 0.00001165
Iteration 36/1000 | Loss: 0.00001164
Iteration 37/1000 | Loss: 0.00001164
Iteration 38/1000 | Loss: 0.00001164
Iteration 39/1000 | Loss: 0.00001163
Iteration 40/1000 | Loss: 0.00001163
Iteration 41/1000 | Loss: 0.00001162
Iteration 42/1000 | Loss: 0.00001162
Iteration 43/1000 | Loss: 0.00001161
Iteration 44/1000 | Loss: 0.00001160
Iteration 45/1000 | Loss: 0.00001160
Iteration 46/1000 | Loss: 0.00001159
Iteration 47/1000 | Loss: 0.00001159
Iteration 48/1000 | Loss: 0.00001159
Iteration 49/1000 | Loss: 0.00001159
Iteration 50/1000 | Loss: 0.00001158
Iteration 51/1000 | Loss: 0.00001158
Iteration 52/1000 | Loss: 0.00001157
Iteration 53/1000 | Loss: 0.00001157
Iteration 54/1000 | Loss: 0.00001157
Iteration 55/1000 | Loss: 0.00001156
Iteration 56/1000 | Loss: 0.00001156
Iteration 57/1000 | Loss: 0.00001155
Iteration 58/1000 | Loss: 0.00001155
Iteration 59/1000 | Loss: 0.00001155
Iteration 60/1000 | Loss: 0.00001154
Iteration 61/1000 | Loss: 0.00001154
Iteration 62/1000 | Loss: 0.00001154
Iteration 63/1000 | Loss: 0.00001154
Iteration 64/1000 | Loss: 0.00001153
Iteration 65/1000 | Loss: 0.00001153
Iteration 66/1000 | Loss: 0.00001150
Iteration 67/1000 | Loss: 0.00001150
Iteration 68/1000 | Loss: 0.00001149
Iteration 69/1000 | Loss: 0.00001149
Iteration 70/1000 | Loss: 0.00001148
Iteration 71/1000 | Loss: 0.00001148
Iteration 72/1000 | Loss: 0.00001148
Iteration 73/1000 | Loss: 0.00001147
Iteration 74/1000 | Loss: 0.00001147
Iteration 75/1000 | Loss: 0.00001146
Iteration 76/1000 | Loss: 0.00001146
Iteration 77/1000 | Loss: 0.00001145
Iteration 78/1000 | Loss: 0.00001145
Iteration 79/1000 | Loss: 0.00001145
Iteration 80/1000 | Loss: 0.00001145
Iteration 81/1000 | Loss: 0.00001144
Iteration 82/1000 | Loss: 0.00001144
Iteration 83/1000 | Loss: 0.00001144
Iteration 84/1000 | Loss: 0.00001144
Iteration 85/1000 | Loss: 0.00001144
Iteration 86/1000 | Loss: 0.00001143
Iteration 87/1000 | Loss: 0.00001142
Iteration 88/1000 | Loss: 0.00001142
Iteration 89/1000 | Loss: 0.00001141
Iteration 90/1000 | Loss: 0.00001141
Iteration 91/1000 | Loss: 0.00001141
Iteration 92/1000 | Loss: 0.00001141
Iteration 93/1000 | Loss: 0.00001140
Iteration 94/1000 | Loss: 0.00001140
Iteration 95/1000 | Loss: 0.00001140
Iteration 96/1000 | Loss: 0.00001140
Iteration 97/1000 | Loss: 0.00001140
Iteration 98/1000 | Loss: 0.00001140
Iteration 99/1000 | Loss: 0.00001140
Iteration 100/1000 | Loss: 0.00001139
Iteration 101/1000 | Loss: 0.00001139
Iteration 102/1000 | Loss: 0.00001139
Iteration 103/1000 | Loss: 0.00001138
Iteration 104/1000 | Loss: 0.00001138
Iteration 105/1000 | Loss: 0.00001138
Iteration 106/1000 | Loss: 0.00001138
Iteration 107/1000 | Loss: 0.00001137
Iteration 108/1000 | Loss: 0.00001137
Iteration 109/1000 | Loss: 0.00001137
Iteration 110/1000 | Loss: 0.00001137
Iteration 111/1000 | Loss: 0.00001137
Iteration 112/1000 | Loss: 0.00001137
Iteration 113/1000 | Loss: 0.00001137
Iteration 114/1000 | Loss: 0.00001137
Iteration 115/1000 | Loss: 0.00001137
Iteration 116/1000 | Loss: 0.00001137
Iteration 117/1000 | Loss: 0.00001137
Iteration 118/1000 | Loss: 0.00001137
Iteration 119/1000 | Loss: 0.00001136
Iteration 120/1000 | Loss: 0.00001136
Iteration 121/1000 | Loss: 0.00001136
Iteration 122/1000 | Loss: 0.00001136
Iteration 123/1000 | Loss: 0.00001136
Iteration 124/1000 | Loss: 0.00001136
Iteration 125/1000 | Loss: 0.00001136
Iteration 126/1000 | Loss: 0.00001136
Iteration 127/1000 | Loss: 0.00001136
Iteration 128/1000 | Loss: 0.00001136
Iteration 129/1000 | Loss: 0.00001135
Iteration 130/1000 | Loss: 0.00001135
Iteration 131/1000 | Loss: 0.00001135
Iteration 132/1000 | Loss: 0.00001135
Iteration 133/1000 | Loss: 0.00001135
Iteration 134/1000 | Loss: 0.00001134
Iteration 135/1000 | Loss: 0.00001134
Iteration 136/1000 | Loss: 0.00001134
Iteration 137/1000 | Loss: 0.00001134
Iteration 138/1000 | Loss: 0.00001134
Iteration 139/1000 | Loss: 0.00001134
Iteration 140/1000 | Loss: 0.00001134
Iteration 141/1000 | Loss: 0.00001134
Iteration 142/1000 | Loss: 0.00001134
Iteration 143/1000 | Loss: 0.00001134
Iteration 144/1000 | Loss: 0.00001134
Iteration 145/1000 | Loss: 0.00001133
Iteration 146/1000 | Loss: 0.00001133
Iteration 147/1000 | Loss: 0.00001133
Iteration 148/1000 | Loss: 0.00001133
Iteration 149/1000 | Loss: 0.00001133
Iteration 150/1000 | Loss: 0.00001132
Iteration 151/1000 | Loss: 0.00001132
Iteration 152/1000 | Loss: 0.00001132
Iteration 153/1000 | Loss: 0.00001132
Iteration 154/1000 | Loss: 0.00001131
Iteration 155/1000 | Loss: 0.00001131
Iteration 156/1000 | Loss: 0.00001131
Iteration 157/1000 | Loss: 0.00001131
Iteration 158/1000 | Loss: 0.00001131
Iteration 159/1000 | Loss: 0.00001131
Iteration 160/1000 | Loss: 0.00001131
Iteration 161/1000 | Loss: 0.00001131
Iteration 162/1000 | Loss: 0.00001131
Iteration 163/1000 | Loss: 0.00001131
Iteration 164/1000 | Loss: 0.00001131
Iteration 165/1000 | Loss: 0.00001130
Iteration 166/1000 | Loss: 0.00001130
Iteration 167/1000 | Loss: 0.00001130
Iteration 168/1000 | Loss: 0.00001130
Iteration 169/1000 | Loss: 0.00001130
Iteration 170/1000 | Loss: 0.00001130
Iteration 171/1000 | Loss: 0.00001130
Iteration 172/1000 | Loss: 0.00001130
Iteration 173/1000 | Loss: 0.00001129
Iteration 174/1000 | Loss: 0.00001129
Iteration 175/1000 | Loss: 0.00001129
Iteration 176/1000 | Loss: 0.00001129
Iteration 177/1000 | Loss: 0.00001129
Iteration 178/1000 | Loss: 0.00001129
Iteration 179/1000 | Loss: 0.00001129
Iteration 180/1000 | Loss: 0.00001129
Iteration 181/1000 | Loss: 0.00001129
Iteration 182/1000 | Loss: 0.00001129
Iteration 183/1000 | Loss: 0.00001129
Iteration 184/1000 | Loss: 0.00001129
Iteration 185/1000 | Loss: 0.00001129
Iteration 186/1000 | Loss: 0.00001129
Iteration 187/1000 | Loss: 0.00001129
Iteration 188/1000 | Loss: 0.00001128
Iteration 189/1000 | Loss: 0.00001128
Iteration 190/1000 | Loss: 0.00001128
Iteration 191/1000 | Loss: 0.00001128
Iteration 192/1000 | Loss: 0.00001128
Iteration 193/1000 | Loss: 0.00001128
Iteration 194/1000 | Loss: 0.00001128
Iteration 195/1000 | Loss: 0.00001128
Iteration 196/1000 | Loss: 0.00001128
Iteration 197/1000 | Loss: 0.00001128
Iteration 198/1000 | Loss: 0.00001128
Iteration 199/1000 | Loss: 0.00001128
Iteration 200/1000 | Loss: 0.00001128
Iteration 201/1000 | Loss: 0.00001127
Iteration 202/1000 | Loss: 0.00001127
Iteration 203/1000 | Loss: 0.00001127
Iteration 204/1000 | Loss: 0.00001127
Iteration 205/1000 | Loss: 0.00001127
Iteration 206/1000 | Loss: 0.00001127
Iteration 207/1000 | Loss: 0.00001127
Iteration 208/1000 | Loss: 0.00001127
Iteration 209/1000 | Loss: 0.00001126
Iteration 210/1000 | Loss: 0.00001126
Iteration 211/1000 | Loss: 0.00001126
Iteration 212/1000 | Loss: 0.00001126
Iteration 213/1000 | Loss: 0.00001126
Iteration 214/1000 | Loss: 0.00001126
Iteration 215/1000 | Loss: 0.00001126
Iteration 216/1000 | Loss: 0.00001126
Iteration 217/1000 | Loss: 0.00001126
Iteration 218/1000 | Loss: 0.00001126
Iteration 219/1000 | Loss: 0.00001126
Iteration 220/1000 | Loss: 0.00001125
Iteration 221/1000 | Loss: 0.00001125
Iteration 222/1000 | Loss: 0.00001125
Iteration 223/1000 | Loss: 0.00001125
Iteration 224/1000 | Loss: 0.00001125
Iteration 225/1000 | Loss: 0.00001125
Iteration 226/1000 | Loss: 0.00001125
Iteration 227/1000 | Loss: 0.00001125
Iteration 228/1000 | Loss: 0.00001125
Iteration 229/1000 | Loss: 0.00001125
Iteration 230/1000 | Loss: 0.00001125
Iteration 231/1000 | Loss: 0.00001125
Iteration 232/1000 | Loss: 0.00001125
Iteration 233/1000 | Loss: 0.00001124
Iteration 234/1000 | Loss: 0.00001124
Iteration 235/1000 | Loss: 0.00001124
Iteration 236/1000 | Loss: 0.00001124
Iteration 237/1000 | Loss: 0.00001124
Iteration 238/1000 | Loss: 0.00001124
Iteration 239/1000 | Loss: 0.00001124
Iteration 240/1000 | Loss: 0.00001124
Iteration 241/1000 | Loss: 0.00001124
Iteration 242/1000 | Loss: 0.00001124
Iteration 243/1000 | Loss: 0.00001124
Iteration 244/1000 | Loss: 0.00001124
Iteration 245/1000 | Loss: 0.00001124
Iteration 246/1000 | Loss: 0.00001124
Iteration 247/1000 | Loss: 0.00001124
Iteration 248/1000 | Loss: 0.00001124
Iteration 249/1000 | Loss: 0.00001124
Iteration 250/1000 | Loss: 0.00001124
Iteration 251/1000 | Loss: 0.00001124
Iteration 252/1000 | Loss: 0.00001124
Iteration 253/1000 | Loss: 0.00001124
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 253. Stopping optimization.
Last 5 losses: [1.1238492334086914e-05, 1.1238492334086914e-05, 1.1238492334086914e-05, 1.1238492334086914e-05, 1.1238492334086914e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1238492334086914e-05

Optimization complete. Final v2v error: 2.8598902225494385 mm

Highest mean error: 3.2430498600006104 mm for frame 80

Lowest mean error: 2.4653429985046387 mm for frame 3

Saving results

Total time: 44.23119401931763
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_003/1095/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_003/1095.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_003/1095
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01020093
Iteration 2/25 | Loss: 0.00253831
Iteration 3/25 | Loss: 0.00192577
Iteration 4/25 | Loss: 0.00199100
Iteration 5/25 | Loss: 0.00185188
Iteration 6/25 | Loss: 0.00159686
Iteration 7/25 | Loss: 0.00156586
Iteration 8/25 | Loss: 0.00141125
Iteration 9/25 | Loss: 0.00140043
Iteration 10/25 | Loss: 0.00138665
Iteration 11/25 | Loss: 0.00141101
Iteration 12/25 | Loss: 0.00135869
Iteration 13/25 | Loss: 0.00134102
Iteration 14/25 | Loss: 0.00134930
Iteration 15/25 | Loss: 0.00135465
Iteration 16/25 | Loss: 0.00134435
Iteration 17/25 | Loss: 0.00134397
Iteration 18/25 | Loss: 0.00134559
Iteration 19/25 | Loss: 0.00134469
Iteration 20/25 | Loss: 0.00135147
Iteration 21/25 | Loss: 0.00135960
Iteration 22/25 | Loss: 0.00132102
Iteration 23/25 | Loss: 0.00130225
Iteration 24/25 | Loss: 0.00129510
Iteration 25/25 | Loss: 0.00128685

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36163640
Iteration 2/25 | Loss: 0.00198941
Iteration 3/25 | Loss: 0.00198941
Iteration 4/25 | Loss: 0.00198941
Iteration 5/25 | Loss: 0.00198940
Iteration 6/25 | Loss: 0.00198940
Iteration 7/25 | Loss: 0.00198940
Iteration 8/25 | Loss: 0.00198940
Iteration 9/25 | Loss: 0.00198940
Iteration 10/25 | Loss: 0.00198940
Iteration 11/25 | Loss: 0.00198940
Iteration 12/25 | Loss: 0.00198940
Iteration 13/25 | Loss: 0.00198940
Iteration 14/25 | Loss: 0.00198940
Iteration 15/25 | Loss: 0.00198940
Iteration 16/25 | Loss: 0.00198940
Iteration 17/25 | Loss: 0.00198940
Iteration 18/25 | Loss: 0.00198940
Iteration 19/25 | Loss: 0.00198940
Iteration 20/25 | Loss: 0.00198940
Iteration 21/25 | Loss: 0.00198940
Iteration 22/25 | Loss: 0.00198940
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0019894035067409277, 0.0019894035067409277, 0.0019894035067409277, 0.0019894035067409277, 0.0019894035067409277]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0019894035067409277

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00198940
Iteration 2/1000 | Loss: 0.00017762
Iteration 3/1000 | Loss: 0.00014178
Iteration 4/1000 | Loss: 0.00168996
Iteration 5/1000 | Loss: 0.00009973
Iteration 6/1000 | Loss: 0.00022657
Iteration 7/1000 | Loss: 0.00037333
Iteration 8/1000 | Loss: 0.00024874
Iteration 9/1000 | Loss: 0.00011662
Iteration 10/1000 | Loss: 0.00023240
Iteration 11/1000 | Loss: 0.00027407
Iteration 12/1000 | Loss: 0.00014934
Iteration 13/1000 | Loss: 0.00025479
Iteration 14/1000 | Loss: 0.00029438
Iteration 15/1000 | Loss: 0.00024495
Iteration 16/1000 | Loss: 0.00008642
Iteration 17/1000 | Loss: 0.00059425
Iteration 18/1000 | Loss: 0.00020293
Iteration 19/1000 | Loss: 0.00012143
Iteration 20/1000 | Loss: 0.00013643
Iteration 21/1000 | Loss: 0.00026476
Iteration 22/1000 | Loss: 0.00030210
Iteration 23/1000 | Loss: 0.00018407
Iteration 24/1000 | Loss: 0.00024121
Iteration 25/1000 | Loss: 0.00042373
Iteration 26/1000 | Loss: 0.00041198
Iteration 27/1000 | Loss: 0.00033047
Iteration 28/1000 | Loss: 0.00017799
Iteration 29/1000 | Loss: 0.00026814
Iteration 30/1000 | Loss: 0.00017660
Iteration 31/1000 | Loss: 0.00022503
Iteration 32/1000 | Loss: 0.00019810
Iteration 33/1000 | Loss: 0.00018523
Iteration 34/1000 | Loss: 0.00019923
Iteration 35/1000 | Loss: 0.00033810
Iteration 36/1000 | Loss: 0.00023901
Iteration 37/1000 | Loss: 0.00021691
Iteration 38/1000 | Loss: 0.00026761
Iteration 39/1000 | Loss: 0.00023979
Iteration 40/1000 | Loss: 0.00022891
Iteration 41/1000 | Loss: 0.00194434
Iteration 42/1000 | Loss: 0.00313518
Iteration 43/1000 | Loss: 0.00117641
Iteration 44/1000 | Loss: 0.00100544
Iteration 45/1000 | Loss: 0.00033372
Iteration 46/1000 | Loss: 0.00032076
Iteration 47/1000 | Loss: 0.00043537
Iteration 48/1000 | Loss: 0.00032099
Iteration 49/1000 | Loss: 0.00051001
Iteration 50/1000 | Loss: 0.00053290
Iteration 51/1000 | Loss: 0.00030053
Iteration 52/1000 | Loss: 0.00022781
Iteration 53/1000 | Loss: 0.00030135
Iteration 54/1000 | Loss: 0.00027713
Iteration 55/1000 | Loss: 0.00074939
Iteration 56/1000 | Loss: 0.00053347
Iteration 57/1000 | Loss: 0.00016903
Iteration 58/1000 | Loss: 0.00074533
Iteration 59/1000 | Loss: 0.00043711
Iteration 60/1000 | Loss: 0.00085420
Iteration 61/1000 | Loss: 0.00043550
Iteration 62/1000 | Loss: 0.00016384
Iteration 63/1000 | Loss: 0.00021116
Iteration 64/1000 | Loss: 0.00017560
Iteration 65/1000 | Loss: 0.00021433
Iteration 66/1000 | Loss: 0.00022635
Iteration 67/1000 | Loss: 0.00024817
Iteration 68/1000 | Loss: 0.00015387
Iteration 69/1000 | Loss: 0.00037137
Iteration 70/1000 | Loss: 0.00021604
Iteration 71/1000 | Loss: 0.00090240
Iteration 72/1000 | Loss: 0.00066802
Iteration 73/1000 | Loss: 0.00034700
Iteration 74/1000 | Loss: 0.00092557
Iteration 75/1000 | Loss: 0.00075270
Iteration 76/1000 | Loss: 0.00047300
Iteration 77/1000 | Loss: 0.00038646
Iteration 78/1000 | Loss: 0.00032461
Iteration 79/1000 | Loss: 0.00035224
Iteration 80/1000 | Loss: 0.00028278
Iteration 81/1000 | Loss: 0.00039735
Iteration 82/1000 | Loss: 0.00040901
Iteration 83/1000 | Loss: 0.00035257
Iteration 84/1000 | Loss: 0.00097831
Iteration 85/1000 | Loss: 0.00091623
Iteration 86/1000 | Loss: 0.00097886
Iteration 87/1000 | Loss: 0.00029652
Iteration 88/1000 | Loss: 0.00032657
Iteration 89/1000 | Loss: 0.00030746
Iteration 90/1000 | Loss: 0.00043389
Iteration 91/1000 | Loss: 0.00039328
Iteration 92/1000 | Loss: 0.00056678
Iteration 93/1000 | Loss: 0.00028288
Iteration 94/1000 | Loss: 0.00023978
Iteration 95/1000 | Loss: 0.00033335
Iteration 96/1000 | Loss: 0.00030298
Iteration 97/1000 | Loss: 0.00038344
Iteration 98/1000 | Loss: 0.00033590
Iteration 99/1000 | Loss: 0.00101761
Iteration 100/1000 | Loss: 0.00080385
Iteration 101/1000 | Loss: 0.00059497
Iteration 102/1000 | Loss: 0.00019130
Iteration 103/1000 | Loss: 0.00033131
Iteration 104/1000 | Loss: 0.00028899
Iteration 105/1000 | Loss: 0.00029550
Iteration 106/1000 | Loss: 0.00028401
Iteration 107/1000 | Loss: 0.00039766
Iteration 108/1000 | Loss: 0.00035723
Iteration 109/1000 | Loss: 0.00038398
Iteration 110/1000 | Loss: 0.00023841
Iteration 111/1000 | Loss: 0.00036961
Iteration 112/1000 | Loss: 0.00022704
Iteration 113/1000 | Loss: 0.00029589
Iteration 114/1000 | Loss: 0.00040832
Iteration 115/1000 | Loss: 0.00027796
Iteration 116/1000 | Loss: 0.00024958
Iteration 117/1000 | Loss: 0.00021895
Iteration 118/1000 | Loss: 0.00028169
Iteration 119/1000 | Loss: 0.00044285
Iteration 120/1000 | Loss: 0.00041629
Iteration 121/1000 | Loss: 0.00041805
Iteration 122/1000 | Loss: 0.00032396
Iteration 123/1000 | Loss: 0.00035934
Iteration 124/1000 | Loss: 0.00109046
Iteration 125/1000 | Loss: 0.00061095
Iteration 126/1000 | Loss: 0.00035109
Iteration 127/1000 | Loss: 0.00036080
Iteration 128/1000 | Loss: 0.00051492
Iteration 129/1000 | Loss: 0.00069851
Iteration 130/1000 | Loss: 0.00056659
Iteration 131/1000 | Loss: 0.00023782
Iteration 132/1000 | Loss: 0.00027657
Iteration 133/1000 | Loss: 0.00031509
Iteration 134/1000 | Loss: 0.00023419
Iteration 135/1000 | Loss: 0.00018101
Iteration 136/1000 | Loss: 0.00018200
Iteration 137/1000 | Loss: 0.00024005
Iteration 138/1000 | Loss: 0.00027433
Iteration 139/1000 | Loss: 0.00020202
Iteration 140/1000 | Loss: 0.00023122
Iteration 141/1000 | Loss: 0.00032199
Iteration 142/1000 | Loss: 0.00027430
Iteration 143/1000 | Loss: 0.00021805
Iteration 144/1000 | Loss: 0.00006115
Iteration 145/1000 | Loss: 0.00030892
Iteration 146/1000 | Loss: 0.00043765
Iteration 147/1000 | Loss: 0.00011177
Iteration 148/1000 | Loss: 0.00022184
Iteration 149/1000 | Loss: 0.00013826
Iteration 150/1000 | Loss: 0.00015702
Iteration 151/1000 | Loss: 0.00012134
Iteration 152/1000 | Loss: 0.00012968
Iteration 153/1000 | Loss: 0.00011051
Iteration 154/1000 | Loss: 0.00011694
Iteration 155/1000 | Loss: 0.00021598
Iteration 156/1000 | Loss: 0.00013400
Iteration 157/1000 | Loss: 0.00011824
Iteration 158/1000 | Loss: 0.00010664
Iteration 159/1000 | Loss: 0.00008267
Iteration 160/1000 | Loss: 0.00012960
Iteration 161/1000 | Loss: 0.00019397
Iteration 162/1000 | Loss: 0.00020862
Iteration 163/1000 | Loss: 0.00022979
Iteration 164/1000 | Loss: 0.00012002
Iteration 165/1000 | Loss: 0.00008152
Iteration 166/1000 | Loss: 0.00010266
Iteration 167/1000 | Loss: 0.00005864
Iteration 168/1000 | Loss: 0.00008203
Iteration 169/1000 | Loss: 0.00009953
Iteration 170/1000 | Loss: 0.00008040
Iteration 171/1000 | Loss: 0.00021588
Iteration 172/1000 | Loss: 0.00022478
Iteration 173/1000 | Loss: 0.00025026
Iteration 174/1000 | Loss: 0.00016954
Iteration 175/1000 | Loss: 0.00020036
Iteration 176/1000 | Loss: 0.00033069
Iteration 177/1000 | Loss: 0.00032201
Iteration 178/1000 | Loss: 0.00022862
Iteration 179/1000 | Loss: 0.00024844
Iteration 180/1000 | Loss: 0.00026139
Iteration 181/1000 | Loss: 0.00024434
Iteration 182/1000 | Loss: 0.00025976
Iteration 183/1000 | Loss: 0.00024771
Iteration 184/1000 | Loss: 0.00026073
Iteration 185/1000 | Loss: 0.00013855
Iteration 186/1000 | Loss: 0.00018051
Iteration 187/1000 | Loss: 0.00026509
Iteration 188/1000 | Loss: 0.00012767
Iteration 189/1000 | Loss: 0.00037710
Iteration 190/1000 | Loss: 0.00032196
Iteration 191/1000 | Loss: 0.00035007
Iteration 192/1000 | Loss: 0.00015696
Iteration 193/1000 | Loss: 0.00013110
Iteration 194/1000 | Loss: 0.00013389
Iteration 195/1000 | Loss: 0.00010765
Iteration 196/1000 | Loss: 0.00012012
Iteration 197/1000 | Loss: 0.00016892
Iteration 198/1000 | Loss: 0.00011305
Iteration 199/1000 | Loss: 0.00010081
Iteration 200/1000 | Loss: 0.00037773
Iteration 201/1000 | Loss: 0.00040526
Iteration 202/1000 | Loss: 0.00040634
Iteration 203/1000 | Loss: 0.00015375
Iteration 204/1000 | Loss: 0.00012715
Iteration 205/1000 | Loss: 0.00010586
Iteration 206/1000 | Loss: 0.00011655
Iteration 207/1000 | Loss: 0.00012489
Iteration 208/1000 | Loss: 0.00031114
Iteration 209/1000 | Loss: 0.00017232
Iteration 210/1000 | Loss: 0.00013053
Iteration 211/1000 | Loss: 0.00011452
Iteration 212/1000 | Loss: 0.00022640
Iteration 213/1000 | Loss: 0.00011140
Iteration 214/1000 | Loss: 0.00010412
Iteration 215/1000 | Loss: 0.00012918
Iteration 216/1000 | Loss: 0.00025304
Iteration 217/1000 | Loss: 0.00016369
Iteration 218/1000 | Loss: 0.00011471
Iteration 219/1000 | Loss: 0.00013881
Iteration 220/1000 | Loss: 0.00012038
Iteration 221/1000 | Loss: 0.00012948
Iteration 222/1000 | Loss: 0.00013849
Iteration 223/1000 | Loss: 0.00011077
Iteration 224/1000 | Loss: 0.00012596
Iteration 225/1000 | Loss: 0.00013753
Iteration 226/1000 | Loss: 0.00011077
Iteration 227/1000 | Loss: 0.00013093
Iteration 228/1000 | Loss: 0.00010567
Iteration 229/1000 | Loss: 0.00009163
Iteration 230/1000 | Loss: 0.00009949
Iteration 231/1000 | Loss: 0.00012732
Iteration 232/1000 | Loss: 0.00011805
Iteration 233/1000 | Loss: 0.00011396
Iteration 234/1000 | Loss: 0.00009980
Iteration 235/1000 | Loss: 0.00009828
Iteration 236/1000 | Loss: 0.00010282
Iteration 237/1000 | Loss: 0.00011459
Iteration 238/1000 | Loss: 0.00010954
Iteration 239/1000 | Loss: 0.00008532
Iteration 240/1000 | Loss: 0.00018169
Iteration 241/1000 | Loss: 0.00010703
Iteration 242/1000 | Loss: 0.00012491
Iteration 243/1000 | Loss: 0.00010037
Iteration 244/1000 | Loss: 0.00010862
Iteration 245/1000 | Loss: 0.00013454
Iteration 246/1000 | Loss: 0.00026193
Iteration 247/1000 | Loss: 0.00038260
Iteration 248/1000 | Loss: 0.00063315
Iteration 249/1000 | Loss: 0.00095731
Iteration 250/1000 | Loss: 0.00076354
Iteration 251/1000 | Loss: 0.00012906
Iteration 252/1000 | Loss: 0.00014303
Iteration 253/1000 | Loss: 0.00009327
Iteration 254/1000 | Loss: 0.00012719
Iteration 255/1000 | Loss: 0.00009078
Iteration 256/1000 | Loss: 0.00028733
Iteration 257/1000 | Loss: 0.00021544
Iteration 258/1000 | Loss: 0.00010192
Iteration 259/1000 | Loss: 0.00019408
Iteration 260/1000 | Loss: 0.00025466
Iteration 261/1000 | Loss: 0.00010320
Iteration 262/1000 | Loss: 0.00031055
Iteration 263/1000 | Loss: 0.00031197
Iteration 264/1000 | Loss: 0.00010155
Iteration 265/1000 | Loss: 0.00010906
Iteration 266/1000 | Loss: 0.00037980
Iteration 267/1000 | Loss: 0.00036213
Iteration 268/1000 | Loss: 0.00007628
Iteration 269/1000 | Loss: 0.00007130
Iteration 270/1000 | Loss: 0.00036340
Iteration 271/1000 | Loss: 0.00033837
Iteration 272/1000 | Loss: 0.00011198
Iteration 273/1000 | Loss: 0.00007286
Iteration 274/1000 | Loss: 0.00011698
Iteration 275/1000 | Loss: 0.00010031
Iteration 276/1000 | Loss: 0.00010681
Iteration 277/1000 | Loss: 0.00011407
Iteration 278/1000 | Loss: 0.00009272
Iteration 279/1000 | Loss: 0.00008936
Iteration 280/1000 | Loss: 0.00010604
Iteration 281/1000 | Loss: 0.00010641
Iteration 282/1000 | Loss: 0.00010173
Iteration 283/1000 | Loss: 0.00010277
Iteration 284/1000 | Loss: 0.00010385
Iteration 285/1000 | Loss: 0.00013272
Iteration 286/1000 | Loss: 0.00010351
Iteration 287/1000 | Loss: 0.00012157
Iteration 288/1000 | Loss: 0.00006361
Iteration 289/1000 | Loss: 0.00011853
Iteration 290/1000 | Loss: 0.00010855
Iteration 291/1000 | Loss: 0.00012003
Iteration 292/1000 | Loss: 0.00011658
Iteration 293/1000 | Loss: 0.00006488
Iteration 294/1000 | Loss: 0.00006927
Iteration 295/1000 | Loss: 0.00007622
Iteration 296/1000 | Loss: 0.00011512
Iteration 297/1000 | Loss: 0.00010627
Iteration 298/1000 | Loss: 0.00010887
Iteration 299/1000 | Loss: 0.00010006
Iteration 300/1000 | Loss: 0.00010176
Iteration 301/1000 | Loss: 0.00009638
Iteration 302/1000 | Loss: 0.00009596
Iteration 303/1000 | Loss: 0.00009338
Iteration 304/1000 | Loss: 0.00010353
Iteration 305/1000 | Loss: 0.00009830
Iteration 306/1000 | Loss: 0.00010342
Iteration 307/1000 | Loss: 0.00010454
Iteration 308/1000 | Loss: 0.00010770
Iteration 309/1000 | Loss: 0.00010107
Iteration 310/1000 | Loss: 0.00010327
Iteration 311/1000 | Loss: 0.00010008
Iteration 312/1000 | Loss: 0.00009610
Iteration 313/1000 | Loss: 0.00006858
Iteration 314/1000 | Loss: 0.00009875
Iteration 315/1000 | Loss: 0.00010693
Iteration 316/1000 | Loss: 0.00009864
Iteration 317/1000 | Loss: 0.00010556
Iteration 318/1000 | Loss: 0.00009557
Iteration 319/1000 | Loss: 0.00010359
Iteration 320/1000 | Loss: 0.00010576
Iteration 321/1000 | Loss: 0.00010355
Iteration 322/1000 | Loss: 0.00009508
Iteration 323/1000 | Loss: 0.00011075
Iteration 324/1000 | Loss: 0.00010853
Iteration 325/1000 | Loss: 0.00010502
Iteration 326/1000 | Loss: 0.00010817
Iteration 327/1000 | Loss: 0.00010080
Iteration 328/1000 | Loss: 0.00010673
Iteration 329/1000 | Loss: 0.00010339
Iteration 330/1000 | Loss: 0.00010481
Iteration 331/1000 | Loss: 0.00009922
Iteration 332/1000 | Loss: 0.00009784
Iteration 333/1000 | Loss: 0.00012222
Iteration 334/1000 | Loss: 0.00005662
Iteration 335/1000 | Loss: 0.00043366
Iteration 336/1000 | Loss: 0.00004753
Iteration 337/1000 | Loss: 0.00003360
Iteration 338/1000 | Loss: 0.00003546
Iteration 339/1000 | Loss: 0.00003788
Iteration 340/1000 | Loss: 0.00006962
Iteration 341/1000 | Loss: 0.00002593
Iteration 342/1000 | Loss: 0.00004892
Iteration 343/1000 | Loss: 0.00003911
Iteration 344/1000 | Loss: 0.00003960
Iteration 345/1000 | Loss: 0.00003492
Iteration 346/1000 | Loss: 0.00006244
Iteration 347/1000 | Loss: 0.00007179
Iteration 348/1000 | Loss: 0.00005146
Iteration 349/1000 | Loss: 0.00005066
Iteration 350/1000 | Loss: 0.00007921
Iteration 351/1000 | Loss: 0.00003958
Iteration 352/1000 | Loss: 0.00005990
Iteration 353/1000 | Loss: 0.00001823
Iteration 354/1000 | Loss: 0.00002740
Iteration 355/1000 | Loss: 0.00001709
Iteration 356/1000 | Loss: 0.00001759
Iteration 357/1000 | Loss: 0.00003262
Iteration 358/1000 | Loss: 0.00001988
Iteration 359/1000 | Loss: 0.00003288
Iteration 360/1000 | Loss: 0.00001863
Iteration 361/1000 | Loss: 0.00003080
Iteration 362/1000 | Loss: 0.00003339
Iteration 363/1000 | Loss: 0.00003578
Iteration 364/1000 | Loss: 0.00002190
Iteration 365/1000 | Loss: 0.00003482
Iteration 366/1000 | Loss: 0.00003120
Iteration 367/1000 | Loss: 0.00003980
Iteration 368/1000 | Loss: 0.00003063
Iteration 369/1000 | Loss: 0.00003283
Iteration 370/1000 | Loss: 0.00003485
Iteration 371/1000 | Loss: 0.00003453
Iteration 372/1000 | Loss: 0.00003026
Iteration 373/1000 | Loss: 0.00003357
Iteration 374/1000 | Loss: 0.00003157
Iteration 375/1000 | Loss: 0.00003333
Iteration 376/1000 | Loss: 0.00003382
Iteration 377/1000 | Loss: 0.00003474
Iteration 378/1000 | Loss: 0.00003024
Iteration 379/1000 | Loss: 0.00003232
Iteration 380/1000 | Loss: 0.00002947
Iteration 381/1000 | Loss: 0.00003174
Iteration 382/1000 | Loss: 0.00003231
Iteration 383/1000 | Loss: 0.00003274
Iteration 384/1000 | Loss: 0.00002976
Iteration 385/1000 | Loss: 0.00003223
Iteration 386/1000 | Loss: 0.00002931
Iteration 387/1000 | Loss: 0.00003208
Iteration 388/1000 | Loss: 0.00002897
Iteration 389/1000 | Loss: 0.00003145
Iteration 390/1000 | Loss: 0.00002903
Iteration 391/1000 | Loss: 0.00003068
Iteration 392/1000 | Loss: 0.00003009
Iteration 393/1000 | Loss: 0.00002974
Iteration 394/1000 | Loss: 0.00002831
Iteration 395/1000 | Loss: 0.00003093
Iteration 396/1000 | Loss: 0.00002761
Iteration 397/1000 | Loss: 0.00003065
Iteration 398/1000 | Loss: 0.00002873
Iteration 399/1000 | Loss: 0.00002923
Iteration 400/1000 | Loss: 0.00002895
Iteration 401/1000 | Loss: 0.00003196
Iteration 402/1000 | Loss: 0.00003055
Iteration 403/1000 | Loss: 0.00003462
Iteration 404/1000 | Loss: 0.00003283
Iteration 405/1000 | Loss: 0.00002746
Iteration 406/1000 | Loss: 0.00004043
Iteration 407/1000 | Loss: 0.00001973
Iteration 408/1000 | Loss: 0.00001667
Iteration 409/1000 | Loss: 0.00001568
Iteration 410/1000 | Loss: 0.00001484
Iteration 411/1000 | Loss: 0.00001431
Iteration 412/1000 | Loss: 0.00001399
Iteration 413/1000 | Loss: 0.00001393
Iteration 414/1000 | Loss: 0.00001385
Iteration 415/1000 | Loss: 0.00001381
Iteration 416/1000 | Loss: 0.00001378
Iteration 417/1000 | Loss: 0.00001377
Iteration 418/1000 | Loss: 0.00001377
Iteration 419/1000 | Loss: 0.00001376
Iteration 420/1000 | Loss: 0.00001375
Iteration 421/1000 | Loss: 0.00001374
Iteration 422/1000 | Loss: 0.00001374
Iteration 423/1000 | Loss: 0.00001373
Iteration 424/1000 | Loss: 0.00001372
Iteration 425/1000 | Loss: 0.00001372
Iteration 426/1000 | Loss: 0.00001371
Iteration 427/1000 | Loss: 0.00001371
Iteration 428/1000 | Loss: 0.00001370
Iteration 429/1000 | Loss: 0.00001367
Iteration 430/1000 | Loss: 0.00001364
Iteration 431/1000 | Loss: 0.00001363
Iteration 432/1000 | Loss: 0.00001363
Iteration 433/1000 | Loss: 0.00001363
Iteration 434/1000 | Loss: 0.00001358
Iteration 435/1000 | Loss: 0.00001357
Iteration 436/1000 | Loss: 0.00001357
Iteration 437/1000 | Loss: 0.00001356
Iteration 438/1000 | Loss: 0.00001355
Iteration 439/1000 | Loss: 0.00001354
Iteration 440/1000 | Loss: 0.00001353
Iteration 441/1000 | Loss: 0.00001353
Iteration 442/1000 | Loss: 0.00001352
Iteration 443/1000 | Loss: 0.00001348
Iteration 444/1000 | Loss: 0.00001348
Iteration 445/1000 | Loss: 0.00001347
Iteration 446/1000 | Loss: 0.00001344
Iteration 447/1000 | Loss: 0.00001344
Iteration 448/1000 | Loss: 0.00001344
Iteration 449/1000 | Loss: 0.00001343
Iteration 450/1000 | Loss: 0.00001343
Iteration 451/1000 | Loss: 0.00001343
Iteration 452/1000 | Loss: 0.00001343
Iteration 453/1000 | Loss: 0.00001342
Iteration 454/1000 | Loss: 0.00001341
Iteration 455/1000 | Loss: 0.00001340
Iteration 456/1000 | Loss: 0.00001340
Iteration 457/1000 | Loss: 0.00001340
Iteration 458/1000 | Loss: 0.00001340
Iteration 459/1000 | Loss: 0.00001339
Iteration 460/1000 | Loss: 0.00001339
Iteration 461/1000 | Loss: 0.00001338
Iteration 462/1000 | Loss: 0.00001338
Iteration 463/1000 | Loss: 0.00001338
Iteration 464/1000 | Loss: 0.00001338
Iteration 465/1000 | Loss: 0.00001338
Iteration 466/1000 | Loss: 0.00001338
Iteration 467/1000 | Loss: 0.00001337
Iteration 468/1000 | Loss: 0.00001337
Iteration 469/1000 | Loss: 0.00001337
Iteration 470/1000 | Loss: 0.00001337
Iteration 471/1000 | Loss: 0.00001337
Iteration 472/1000 | Loss: 0.00001336
Iteration 473/1000 | Loss: 0.00001336
Iteration 474/1000 | Loss: 0.00001336
Iteration 475/1000 | Loss: 0.00001336
Iteration 476/1000 | Loss: 0.00001336
Iteration 477/1000 | Loss: 0.00001336
Iteration 478/1000 | Loss: 0.00001336
Iteration 479/1000 | Loss: 0.00001336
Iteration 480/1000 | Loss: 0.00001335
Iteration 481/1000 | Loss: 0.00001335
Iteration 482/1000 | Loss: 0.00001335
Iteration 483/1000 | Loss: 0.00001335
Iteration 484/1000 | Loss: 0.00001335
Iteration 485/1000 | Loss: 0.00001335
Iteration 486/1000 | Loss: 0.00001335
Iteration 487/1000 | Loss: 0.00001335
Iteration 488/1000 | Loss: 0.00001335
Iteration 489/1000 | Loss: 0.00001334
Iteration 490/1000 | Loss: 0.00001334
Iteration 491/1000 | Loss: 0.00001334
Iteration 492/1000 | Loss: 0.00001334
Iteration 493/1000 | Loss: 0.00001334
Iteration 494/1000 | Loss: 0.00001334
Iteration 495/1000 | Loss: 0.00001334
Iteration 496/1000 | Loss: 0.00001334
Iteration 497/1000 | Loss: 0.00001334
Iteration 498/1000 | Loss: 0.00001334
Iteration 499/1000 | Loss: 0.00001334
Iteration 500/1000 | Loss: 0.00001334
Iteration 501/1000 | Loss: 0.00001334
Iteration 502/1000 | Loss: 0.00001334
Iteration 503/1000 | Loss: 0.00001334
Iteration 504/1000 | Loss: 0.00001334
Iteration 505/1000 | Loss: 0.00001334
Iteration 506/1000 | Loss: 0.00001334
Iteration 507/1000 | Loss: 0.00001334
Iteration 508/1000 | Loss: 0.00001333
Iteration 509/1000 | Loss: 0.00001333
Iteration 510/1000 | Loss: 0.00001333
Iteration 511/1000 | Loss: 0.00001333
Iteration 512/1000 | Loss: 0.00001333
Iteration 513/1000 | Loss: 0.00001333
Iteration 514/1000 | Loss: 0.00001332
Iteration 515/1000 | Loss: 0.00001332
Iteration 516/1000 | Loss: 0.00001332
Iteration 517/1000 | Loss: 0.00001332
Iteration 518/1000 | Loss: 0.00001332
Iteration 519/1000 | Loss: 0.00001331
Iteration 520/1000 | Loss: 0.00001331
Iteration 521/1000 | Loss: 0.00001331
Iteration 522/1000 | Loss: 0.00001331
Iteration 523/1000 | Loss: 0.00001331
Iteration 524/1000 | Loss: 0.00001331
Iteration 525/1000 | Loss: 0.00001331
Iteration 526/1000 | Loss: 0.00001331
Iteration 527/1000 | Loss: 0.00001331
Iteration 528/1000 | Loss: 0.00001331
Iteration 529/1000 | Loss: 0.00001331
Iteration 530/1000 | Loss: 0.00001331
Iteration 531/1000 | Loss: 0.00001330
Iteration 532/1000 | Loss: 0.00001330
Iteration 533/1000 | Loss: 0.00001330
Iteration 534/1000 | Loss: 0.00001330
Iteration 535/1000 | Loss: 0.00001330
Iteration 536/1000 | Loss: 0.00001330
Iteration 537/1000 | Loss: 0.00001330
Iteration 538/1000 | Loss: 0.00001330
Iteration 539/1000 | Loss: 0.00001330
Iteration 540/1000 | Loss: 0.00001330
Iteration 541/1000 | Loss: 0.00001330
Iteration 542/1000 | Loss: 0.00001330
Iteration 543/1000 | Loss: 0.00001330
Iteration 544/1000 | Loss: 0.00001330
Iteration 545/1000 | Loss: 0.00001330
Iteration 546/1000 | Loss: 0.00001330
Iteration 547/1000 | Loss: 0.00001330
Iteration 548/1000 | Loss: 0.00001330
Iteration 549/1000 | Loss: 0.00001330
Iteration 550/1000 | Loss: 0.00001329
Iteration 551/1000 | Loss: 0.00001329
Iteration 552/1000 | Loss: 0.00001329
Iteration 553/1000 | Loss: 0.00001329
Iteration 554/1000 | Loss: 0.00001329
Iteration 555/1000 | Loss: 0.00001329
Iteration 556/1000 | Loss: 0.00001329
Iteration 557/1000 | Loss: 0.00001329
Iteration 558/1000 | Loss: 0.00001329
Iteration 559/1000 | Loss: 0.00001329
Iteration 560/1000 | Loss: 0.00001329
Iteration 561/1000 | Loss: 0.00001329
Iteration 562/1000 | Loss: 0.00001329
Iteration 563/1000 | Loss: 0.00001329
Iteration 564/1000 | Loss: 0.00001329
Iteration 565/1000 | Loss: 0.00001329
Iteration 566/1000 | Loss: 0.00001329
Iteration 567/1000 | Loss: 0.00001329
Iteration 568/1000 | Loss: 0.00001329
Iteration 569/1000 | Loss: 0.00001329
Iteration 570/1000 | Loss: 0.00001329
Iteration 571/1000 | Loss: 0.00001329
Iteration 572/1000 | Loss: 0.00001329
Iteration 573/1000 | Loss: 0.00001329
Iteration 574/1000 | Loss: 0.00001329
Iteration 575/1000 | Loss: 0.00001329
Iteration 576/1000 | Loss: 0.00001329
Iteration 577/1000 | Loss: 0.00001329
Iteration 578/1000 | Loss: 0.00001329
Iteration 579/1000 | Loss: 0.00001329
Iteration 580/1000 | Loss: 0.00001329
Iteration 581/1000 | Loss: 0.00001329
Iteration 582/1000 | Loss: 0.00001329
Iteration 583/1000 | Loss: 0.00001329
Iteration 584/1000 | Loss: 0.00001329
Iteration 585/1000 | Loss: 0.00001329
Iteration 586/1000 | Loss: 0.00001329
Iteration 587/1000 | Loss: 0.00001329
Iteration 588/1000 | Loss: 0.00001329
Iteration 589/1000 | Loss: 0.00001329
Iteration 590/1000 | Loss: 0.00001329
Iteration 591/1000 | Loss: 0.00001329
Iteration 592/1000 | Loss: 0.00001329
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 592. Stopping optimization.
Last 5 losses: [1.3290465176396538e-05, 1.3290465176396538e-05, 1.3290465176396538e-05, 1.3290465176396538e-05, 1.3290465176396538e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3290465176396538e-05

Optimization complete. Final v2v error: 3.08829927444458 mm

Highest mean error: 4.678506374359131 mm for frame 77

Lowest mean error: 2.619170904159546 mm for frame 112

Saving results

Total time: 641.1999526023865
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_003/1088/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_003/1088.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_003/1088
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00861170
Iteration 2/25 | Loss: 0.00147921
Iteration 3/25 | Loss: 0.00126988
Iteration 4/25 | Loss: 0.00125067
Iteration 5/25 | Loss: 0.00124878
Iteration 6/25 | Loss: 0.00124878
Iteration 7/25 | Loss: 0.00124878
Iteration 8/25 | Loss: 0.00124878
Iteration 9/25 | Loss: 0.00124878
Iteration 10/25 | Loss: 0.00124878
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0012487821513786912, 0.0012487821513786912, 0.0012487821513786912, 0.0012487821513786912, 0.0012487821513786912]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012487821513786912

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.89196491
Iteration 2/25 | Loss: 0.00077470
Iteration 3/25 | Loss: 0.00077470
Iteration 4/25 | Loss: 0.00077470
Iteration 5/25 | Loss: 0.00077469
Iteration 6/25 | Loss: 0.00077469
Iteration 7/25 | Loss: 0.00077469
Iteration 8/25 | Loss: 0.00077469
Iteration 9/25 | Loss: 0.00077469
Iteration 10/25 | Loss: 0.00077469
Iteration 11/25 | Loss: 0.00077469
Iteration 12/25 | Loss: 0.00077469
Iteration 13/25 | Loss: 0.00077469
Iteration 14/25 | Loss: 0.00077469
Iteration 15/25 | Loss: 0.00077469
Iteration 16/25 | Loss: 0.00077469
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.000774693617131561, 0.000774693617131561, 0.000774693617131561, 0.000774693617131561, 0.000774693617131561]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000774693617131561

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00077469
Iteration 2/1000 | Loss: 0.00003071
Iteration 3/1000 | Loss: 0.00002328
Iteration 4/1000 | Loss: 0.00002107
Iteration 5/1000 | Loss: 0.00002031
Iteration 6/1000 | Loss: 0.00001947
Iteration 7/1000 | Loss: 0.00001895
Iteration 8/1000 | Loss: 0.00001861
Iteration 9/1000 | Loss: 0.00001831
Iteration 10/1000 | Loss: 0.00001809
Iteration 11/1000 | Loss: 0.00001801
Iteration 12/1000 | Loss: 0.00001797
Iteration 13/1000 | Loss: 0.00001796
Iteration 14/1000 | Loss: 0.00001786
Iteration 15/1000 | Loss: 0.00001773
Iteration 16/1000 | Loss: 0.00001773
Iteration 17/1000 | Loss: 0.00001765
Iteration 18/1000 | Loss: 0.00001764
Iteration 19/1000 | Loss: 0.00001763
Iteration 20/1000 | Loss: 0.00001761
Iteration 21/1000 | Loss: 0.00001761
Iteration 22/1000 | Loss: 0.00001760
Iteration 23/1000 | Loss: 0.00001760
Iteration 24/1000 | Loss: 0.00001760
Iteration 25/1000 | Loss: 0.00001760
Iteration 26/1000 | Loss: 0.00001760
Iteration 27/1000 | Loss: 0.00001760
Iteration 28/1000 | Loss: 0.00001760
Iteration 29/1000 | Loss: 0.00001760
Iteration 30/1000 | Loss: 0.00001760
Iteration 31/1000 | Loss: 0.00001760
Iteration 32/1000 | Loss: 0.00001759
Iteration 33/1000 | Loss: 0.00001759
Iteration 34/1000 | Loss: 0.00001759
Iteration 35/1000 | Loss: 0.00001759
Iteration 36/1000 | Loss: 0.00001758
Iteration 37/1000 | Loss: 0.00001758
Iteration 38/1000 | Loss: 0.00001758
Iteration 39/1000 | Loss: 0.00001758
Iteration 40/1000 | Loss: 0.00001758
Iteration 41/1000 | Loss: 0.00001758
Iteration 42/1000 | Loss: 0.00001758
Iteration 43/1000 | Loss: 0.00001757
Iteration 44/1000 | Loss: 0.00001757
Iteration 45/1000 | Loss: 0.00001757
Iteration 46/1000 | Loss: 0.00001757
Iteration 47/1000 | Loss: 0.00001757
Iteration 48/1000 | Loss: 0.00001757
Iteration 49/1000 | Loss: 0.00001756
Iteration 50/1000 | Loss: 0.00001756
Iteration 51/1000 | Loss: 0.00001756
Iteration 52/1000 | Loss: 0.00001756
Iteration 53/1000 | Loss: 0.00001756
Iteration 54/1000 | Loss: 0.00001756
Iteration 55/1000 | Loss: 0.00001756
Iteration 56/1000 | Loss: 0.00001756
Iteration 57/1000 | Loss: 0.00001755
Iteration 58/1000 | Loss: 0.00001755
Iteration 59/1000 | Loss: 0.00001755
Iteration 60/1000 | Loss: 0.00001755
Iteration 61/1000 | Loss: 0.00001754
Iteration 62/1000 | Loss: 0.00001754
Iteration 63/1000 | Loss: 0.00001754
Iteration 64/1000 | Loss: 0.00001754
Iteration 65/1000 | Loss: 0.00001754
Iteration 66/1000 | Loss: 0.00001754
Iteration 67/1000 | Loss: 0.00001754
Iteration 68/1000 | Loss: 0.00001754
Iteration 69/1000 | Loss: 0.00001754
Iteration 70/1000 | Loss: 0.00001754
Iteration 71/1000 | Loss: 0.00001754
Iteration 72/1000 | Loss: 0.00001754
Iteration 73/1000 | Loss: 0.00001754
Iteration 74/1000 | Loss: 0.00001753
Iteration 75/1000 | Loss: 0.00001753
Iteration 76/1000 | Loss: 0.00001753
Iteration 77/1000 | Loss: 0.00001753
Iteration 78/1000 | Loss: 0.00001753
Iteration 79/1000 | Loss: 0.00001753
Iteration 80/1000 | Loss: 0.00001753
Iteration 81/1000 | Loss: 0.00001753
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 81. Stopping optimization.
Last 5 losses: [1.7534275684738532e-05, 1.7534275684738532e-05, 1.7534275684738532e-05, 1.7534275684738532e-05, 1.7534275684738532e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7534275684738532e-05

Optimization complete. Final v2v error: 3.541062831878662 mm

Highest mean error: 3.754026174545288 mm for frame 17

Lowest mean error: 3.3874897956848145 mm for frame 91

Saving results

Total time: 30.440101861953735
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_003/1096/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_003/1096.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_003/1096
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00929077
Iteration 2/25 | Loss: 0.00147413
Iteration 3/25 | Loss: 0.00131138
Iteration 4/25 | Loss: 0.00127922
Iteration 5/25 | Loss: 0.00127227
Iteration 6/25 | Loss: 0.00127145
Iteration 7/25 | Loss: 0.00127145
Iteration 8/25 | Loss: 0.00127145
Iteration 9/25 | Loss: 0.00127145
Iteration 10/25 | Loss: 0.00127145
Iteration 11/25 | Loss: 0.00127145
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001271450542844832, 0.001271450542844832, 0.001271450542844832, 0.001271450542844832, 0.001271450542844832]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001271450542844832

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.98932505
Iteration 2/25 | Loss: 0.00172700
Iteration 3/25 | Loss: 0.00172695
Iteration 4/25 | Loss: 0.00172695
Iteration 5/25 | Loss: 0.00172695
Iteration 6/25 | Loss: 0.00172695
Iteration 7/25 | Loss: 0.00172695
Iteration 8/25 | Loss: 0.00172695
Iteration 9/25 | Loss: 0.00172695
Iteration 10/25 | Loss: 0.00172695
Iteration 11/25 | Loss: 0.00172695
Iteration 12/25 | Loss: 0.00172695
Iteration 13/25 | Loss: 0.00172695
Iteration 14/25 | Loss: 0.00172695
Iteration 15/25 | Loss: 0.00172695
Iteration 16/25 | Loss: 0.00172695
Iteration 17/25 | Loss: 0.00172695
Iteration 18/25 | Loss: 0.00172695
Iteration 19/25 | Loss: 0.00172695
Iteration 20/25 | Loss: 0.00172695
Iteration 21/25 | Loss: 0.00172695
Iteration 22/25 | Loss: 0.00172695
Iteration 23/25 | Loss: 0.00172695
Iteration 24/25 | Loss: 0.00172695
Iteration 25/25 | Loss: 0.00172695

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00172695
Iteration 2/1000 | Loss: 0.00003303
Iteration 3/1000 | Loss: 0.00002326
Iteration 4/1000 | Loss: 0.00002149
Iteration 5/1000 | Loss: 0.00002038
Iteration 6/1000 | Loss: 0.00001985
Iteration 7/1000 | Loss: 0.00001937
Iteration 8/1000 | Loss: 0.00001899
Iteration 9/1000 | Loss: 0.00001851
Iteration 10/1000 | Loss: 0.00001821
Iteration 11/1000 | Loss: 0.00001793
Iteration 12/1000 | Loss: 0.00001776
Iteration 13/1000 | Loss: 0.00001770
Iteration 14/1000 | Loss: 0.00001766
Iteration 15/1000 | Loss: 0.00001757
Iteration 16/1000 | Loss: 0.00001753
Iteration 17/1000 | Loss: 0.00001749
Iteration 18/1000 | Loss: 0.00001748
Iteration 19/1000 | Loss: 0.00001748
Iteration 20/1000 | Loss: 0.00001744
Iteration 21/1000 | Loss: 0.00001744
Iteration 22/1000 | Loss: 0.00001743
Iteration 23/1000 | Loss: 0.00001743
Iteration 24/1000 | Loss: 0.00001742
Iteration 25/1000 | Loss: 0.00001740
Iteration 26/1000 | Loss: 0.00001740
Iteration 27/1000 | Loss: 0.00001739
Iteration 28/1000 | Loss: 0.00001739
Iteration 29/1000 | Loss: 0.00001739
Iteration 30/1000 | Loss: 0.00001738
Iteration 31/1000 | Loss: 0.00001738
Iteration 32/1000 | Loss: 0.00001737
Iteration 33/1000 | Loss: 0.00001734
Iteration 34/1000 | Loss: 0.00001734
Iteration 35/1000 | Loss: 0.00001734
Iteration 36/1000 | Loss: 0.00001734
Iteration 37/1000 | Loss: 0.00001734
Iteration 38/1000 | Loss: 0.00001734
Iteration 39/1000 | Loss: 0.00001734
Iteration 40/1000 | Loss: 0.00001734
Iteration 41/1000 | Loss: 0.00001734
Iteration 42/1000 | Loss: 0.00001734
Iteration 43/1000 | Loss: 0.00001733
Iteration 44/1000 | Loss: 0.00001733
Iteration 45/1000 | Loss: 0.00001733
Iteration 46/1000 | Loss: 0.00001733
Iteration 47/1000 | Loss: 0.00001732
Iteration 48/1000 | Loss: 0.00001732
Iteration 49/1000 | Loss: 0.00001732
Iteration 50/1000 | Loss: 0.00001731
Iteration 51/1000 | Loss: 0.00001731
Iteration 52/1000 | Loss: 0.00001731
Iteration 53/1000 | Loss: 0.00001731
Iteration 54/1000 | Loss: 0.00001731
Iteration 55/1000 | Loss: 0.00001731
Iteration 56/1000 | Loss: 0.00001730
Iteration 57/1000 | Loss: 0.00001730
Iteration 58/1000 | Loss: 0.00001730
Iteration 59/1000 | Loss: 0.00001730
Iteration 60/1000 | Loss: 0.00001729
Iteration 61/1000 | Loss: 0.00001729
Iteration 62/1000 | Loss: 0.00001729
Iteration 63/1000 | Loss: 0.00001729
Iteration 64/1000 | Loss: 0.00001729
Iteration 65/1000 | Loss: 0.00001729
Iteration 66/1000 | Loss: 0.00001729
Iteration 67/1000 | Loss: 0.00001728
Iteration 68/1000 | Loss: 0.00001728
Iteration 69/1000 | Loss: 0.00001727
Iteration 70/1000 | Loss: 0.00001727
Iteration 71/1000 | Loss: 0.00001727
Iteration 72/1000 | Loss: 0.00001727
Iteration 73/1000 | Loss: 0.00001727
Iteration 74/1000 | Loss: 0.00001727
Iteration 75/1000 | Loss: 0.00001727
Iteration 76/1000 | Loss: 0.00001727
Iteration 77/1000 | Loss: 0.00001726
Iteration 78/1000 | Loss: 0.00001726
Iteration 79/1000 | Loss: 0.00001726
Iteration 80/1000 | Loss: 0.00001726
Iteration 81/1000 | Loss: 0.00001726
Iteration 82/1000 | Loss: 0.00001726
Iteration 83/1000 | Loss: 0.00001726
Iteration 84/1000 | Loss: 0.00001726
Iteration 85/1000 | Loss: 0.00001726
Iteration 86/1000 | Loss: 0.00001726
Iteration 87/1000 | Loss: 0.00001726
Iteration 88/1000 | Loss: 0.00001726
Iteration 89/1000 | Loss: 0.00001726
Iteration 90/1000 | Loss: 0.00001726
Iteration 91/1000 | Loss: 0.00001725
Iteration 92/1000 | Loss: 0.00001725
Iteration 93/1000 | Loss: 0.00001725
Iteration 94/1000 | Loss: 0.00001725
Iteration 95/1000 | Loss: 0.00001725
Iteration 96/1000 | Loss: 0.00001724
Iteration 97/1000 | Loss: 0.00001724
Iteration 98/1000 | Loss: 0.00001724
Iteration 99/1000 | Loss: 0.00001724
Iteration 100/1000 | Loss: 0.00001724
Iteration 101/1000 | Loss: 0.00001723
Iteration 102/1000 | Loss: 0.00001723
Iteration 103/1000 | Loss: 0.00001723
Iteration 104/1000 | Loss: 0.00001722
Iteration 105/1000 | Loss: 0.00001722
Iteration 106/1000 | Loss: 0.00001722
Iteration 107/1000 | Loss: 0.00001722
Iteration 108/1000 | Loss: 0.00001722
Iteration 109/1000 | Loss: 0.00001721
Iteration 110/1000 | Loss: 0.00001721
Iteration 111/1000 | Loss: 0.00001721
Iteration 112/1000 | Loss: 0.00001721
Iteration 113/1000 | Loss: 0.00001721
Iteration 114/1000 | Loss: 0.00001721
Iteration 115/1000 | Loss: 0.00001721
Iteration 116/1000 | Loss: 0.00001720
Iteration 117/1000 | Loss: 0.00001720
Iteration 118/1000 | Loss: 0.00001720
Iteration 119/1000 | Loss: 0.00001720
Iteration 120/1000 | Loss: 0.00001720
Iteration 121/1000 | Loss: 0.00001720
Iteration 122/1000 | Loss: 0.00001720
Iteration 123/1000 | Loss: 0.00001720
Iteration 124/1000 | Loss: 0.00001720
Iteration 125/1000 | Loss: 0.00001719
Iteration 126/1000 | Loss: 0.00001719
Iteration 127/1000 | Loss: 0.00001719
Iteration 128/1000 | Loss: 0.00001719
Iteration 129/1000 | Loss: 0.00001719
Iteration 130/1000 | Loss: 0.00001719
Iteration 131/1000 | Loss: 0.00001719
Iteration 132/1000 | Loss: 0.00001718
Iteration 133/1000 | Loss: 0.00001718
Iteration 134/1000 | Loss: 0.00001718
Iteration 135/1000 | Loss: 0.00001718
Iteration 136/1000 | Loss: 0.00001718
Iteration 137/1000 | Loss: 0.00001718
Iteration 138/1000 | Loss: 0.00001718
Iteration 139/1000 | Loss: 0.00001718
Iteration 140/1000 | Loss: 0.00001718
Iteration 141/1000 | Loss: 0.00001718
Iteration 142/1000 | Loss: 0.00001718
Iteration 143/1000 | Loss: 0.00001717
Iteration 144/1000 | Loss: 0.00001717
Iteration 145/1000 | Loss: 0.00001717
Iteration 146/1000 | Loss: 0.00001717
Iteration 147/1000 | Loss: 0.00001717
Iteration 148/1000 | Loss: 0.00001717
Iteration 149/1000 | Loss: 0.00001716
Iteration 150/1000 | Loss: 0.00001716
Iteration 151/1000 | Loss: 0.00001716
Iteration 152/1000 | Loss: 0.00001716
Iteration 153/1000 | Loss: 0.00001716
Iteration 154/1000 | Loss: 0.00001716
Iteration 155/1000 | Loss: 0.00001716
Iteration 156/1000 | Loss: 0.00001716
Iteration 157/1000 | Loss: 0.00001716
Iteration 158/1000 | Loss: 0.00001716
Iteration 159/1000 | Loss: 0.00001716
Iteration 160/1000 | Loss: 0.00001716
Iteration 161/1000 | Loss: 0.00001716
Iteration 162/1000 | Loss: 0.00001716
Iteration 163/1000 | Loss: 0.00001716
Iteration 164/1000 | Loss: 0.00001716
Iteration 165/1000 | Loss: 0.00001716
Iteration 166/1000 | Loss: 0.00001716
Iteration 167/1000 | Loss: 0.00001716
Iteration 168/1000 | Loss: 0.00001716
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 168. Stopping optimization.
Last 5 losses: [1.715720100037288e-05, 1.715720100037288e-05, 1.715720100037288e-05, 1.715720100037288e-05, 1.715720100037288e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.715720100037288e-05

Optimization complete. Final v2v error: 3.549219846725464 mm

Highest mean error: 3.9890365600585938 mm for frame 44

Lowest mean error: 3.149825096130371 mm for frame 135

Saving results

Total time: 39.25302171707153
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_003/1076/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_003/1076.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_003/1076
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00625140
Iteration 2/25 | Loss: 0.00127656
Iteration 3/25 | Loss: 0.00119895
Iteration 4/25 | Loss: 0.00118888
Iteration 5/25 | Loss: 0.00118528
Iteration 6/25 | Loss: 0.00118452
Iteration 7/25 | Loss: 0.00118452
Iteration 8/25 | Loss: 0.00118452
Iteration 9/25 | Loss: 0.00118452
Iteration 10/25 | Loss: 0.00118452
Iteration 11/25 | Loss: 0.00118452
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011845241533592343, 0.0011845241533592343, 0.0011845241533592343, 0.0011845241533592343, 0.0011845241533592343]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011845241533592343

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36613488
Iteration 2/25 | Loss: 0.00116836
Iteration 3/25 | Loss: 0.00116836
Iteration 4/25 | Loss: 0.00116836
Iteration 5/25 | Loss: 0.00116836
Iteration 6/25 | Loss: 0.00116836
Iteration 7/25 | Loss: 0.00116835
Iteration 8/25 | Loss: 0.00116835
Iteration 9/25 | Loss: 0.00116835
Iteration 10/25 | Loss: 0.00116835
Iteration 11/25 | Loss: 0.00116835
Iteration 12/25 | Loss: 0.00116835
Iteration 13/25 | Loss: 0.00116835
Iteration 14/25 | Loss: 0.00116835
Iteration 15/25 | Loss: 0.00116835
Iteration 16/25 | Loss: 0.00116835
Iteration 17/25 | Loss: 0.00116835
Iteration 18/25 | Loss: 0.00116835
Iteration 19/25 | Loss: 0.00116835
Iteration 20/25 | Loss: 0.00116835
Iteration 21/25 | Loss: 0.00116835
Iteration 22/25 | Loss: 0.00116835
Iteration 23/25 | Loss: 0.00116835
Iteration 24/25 | Loss: 0.00116835
Iteration 25/25 | Loss: 0.00116835

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00116835
Iteration 2/1000 | Loss: 0.00001888
Iteration 3/1000 | Loss: 0.00001311
Iteration 4/1000 | Loss: 0.00001211
Iteration 5/1000 | Loss: 0.00001133
Iteration 6/1000 | Loss: 0.00001094
Iteration 7/1000 | Loss: 0.00001053
Iteration 8/1000 | Loss: 0.00001032
Iteration 9/1000 | Loss: 0.00001021
Iteration 10/1000 | Loss: 0.00001002
Iteration 11/1000 | Loss: 0.00000979
Iteration 12/1000 | Loss: 0.00000978
Iteration 13/1000 | Loss: 0.00000978
Iteration 14/1000 | Loss: 0.00000976
Iteration 15/1000 | Loss: 0.00000967
Iteration 16/1000 | Loss: 0.00000963
Iteration 17/1000 | Loss: 0.00000962
Iteration 18/1000 | Loss: 0.00000961
Iteration 19/1000 | Loss: 0.00000956
Iteration 20/1000 | Loss: 0.00000956
Iteration 21/1000 | Loss: 0.00000955
Iteration 22/1000 | Loss: 0.00000955
Iteration 23/1000 | Loss: 0.00000954
Iteration 24/1000 | Loss: 0.00000951
Iteration 25/1000 | Loss: 0.00000951
Iteration 26/1000 | Loss: 0.00000949
Iteration 27/1000 | Loss: 0.00000949
Iteration 28/1000 | Loss: 0.00000947
Iteration 29/1000 | Loss: 0.00000946
Iteration 30/1000 | Loss: 0.00000946
Iteration 31/1000 | Loss: 0.00000946
Iteration 32/1000 | Loss: 0.00000946
Iteration 33/1000 | Loss: 0.00000946
Iteration 34/1000 | Loss: 0.00000944
Iteration 35/1000 | Loss: 0.00000944
Iteration 36/1000 | Loss: 0.00000942
Iteration 37/1000 | Loss: 0.00000941
Iteration 38/1000 | Loss: 0.00000940
Iteration 39/1000 | Loss: 0.00000939
Iteration 40/1000 | Loss: 0.00000939
Iteration 41/1000 | Loss: 0.00000938
Iteration 42/1000 | Loss: 0.00000938
Iteration 43/1000 | Loss: 0.00000937
Iteration 44/1000 | Loss: 0.00000937
Iteration 45/1000 | Loss: 0.00000936
Iteration 46/1000 | Loss: 0.00000936
Iteration 47/1000 | Loss: 0.00000935
Iteration 48/1000 | Loss: 0.00000934
Iteration 49/1000 | Loss: 0.00000933
Iteration 50/1000 | Loss: 0.00000933
Iteration 51/1000 | Loss: 0.00000933
Iteration 52/1000 | Loss: 0.00000932
Iteration 53/1000 | Loss: 0.00000932
Iteration 54/1000 | Loss: 0.00000932
Iteration 55/1000 | Loss: 0.00000932
Iteration 56/1000 | Loss: 0.00000932
Iteration 57/1000 | Loss: 0.00000931
Iteration 58/1000 | Loss: 0.00000931
Iteration 59/1000 | Loss: 0.00000931
Iteration 60/1000 | Loss: 0.00000930
Iteration 61/1000 | Loss: 0.00000930
Iteration 62/1000 | Loss: 0.00000929
Iteration 63/1000 | Loss: 0.00000929
Iteration 64/1000 | Loss: 0.00000928
Iteration 65/1000 | Loss: 0.00000928
Iteration 66/1000 | Loss: 0.00000928
Iteration 67/1000 | Loss: 0.00000928
Iteration 68/1000 | Loss: 0.00000928
Iteration 69/1000 | Loss: 0.00000928
Iteration 70/1000 | Loss: 0.00000927
Iteration 71/1000 | Loss: 0.00000927
Iteration 72/1000 | Loss: 0.00000927
Iteration 73/1000 | Loss: 0.00000927
Iteration 74/1000 | Loss: 0.00000927
Iteration 75/1000 | Loss: 0.00000927
Iteration 76/1000 | Loss: 0.00000927
Iteration 77/1000 | Loss: 0.00000926
Iteration 78/1000 | Loss: 0.00000926
Iteration 79/1000 | Loss: 0.00000926
Iteration 80/1000 | Loss: 0.00000926
Iteration 81/1000 | Loss: 0.00000926
Iteration 82/1000 | Loss: 0.00000925
Iteration 83/1000 | Loss: 0.00000924
Iteration 84/1000 | Loss: 0.00000924
Iteration 85/1000 | Loss: 0.00000924
Iteration 86/1000 | Loss: 0.00000924
Iteration 87/1000 | Loss: 0.00000924
Iteration 88/1000 | Loss: 0.00000924
Iteration 89/1000 | Loss: 0.00000924
Iteration 90/1000 | Loss: 0.00000924
Iteration 91/1000 | Loss: 0.00000924
Iteration 92/1000 | Loss: 0.00000924
Iteration 93/1000 | Loss: 0.00000924
Iteration 94/1000 | Loss: 0.00000923
Iteration 95/1000 | Loss: 0.00000923
Iteration 96/1000 | Loss: 0.00000923
Iteration 97/1000 | Loss: 0.00000923
Iteration 98/1000 | Loss: 0.00000923
Iteration 99/1000 | Loss: 0.00000923
Iteration 100/1000 | Loss: 0.00000922
Iteration 101/1000 | Loss: 0.00000922
Iteration 102/1000 | Loss: 0.00000921
Iteration 103/1000 | Loss: 0.00000921
Iteration 104/1000 | Loss: 0.00000920
Iteration 105/1000 | Loss: 0.00000920
Iteration 106/1000 | Loss: 0.00000919
Iteration 107/1000 | Loss: 0.00000919
Iteration 108/1000 | Loss: 0.00000919
Iteration 109/1000 | Loss: 0.00000919
Iteration 110/1000 | Loss: 0.00000919
Iteration 111/1000 | Loss: 0.00000918
Iteration 112/1000 | Loss: 0.00000918
Iteration 113/1000 | Loss: 0.00000917
Iteration 114/1000 | Loss: 0.00000917
Iteration 115/1000 | Loss: 0.00000917
Iteration 116/1000 | Loss: 0.00000917
Iteration 117/1000 | Loss: 0.00000917
Iteration 118/1000 | Loss: 0.00000917
Iteration 119/1000 | Loss: 0.00000917
Iteration 120/1000 | Loss: 0.00000917
Iteration 121/1000 | Loss: 0.00000917
Iteration 122/1000 | Loss: 0.00000917
Iteration 123/1000 | Loss: 0.00000916
Iteration 124/1000 | Loss: 0.00000916
Iteration 125/1000 | Loss: 0.00000916
Iteration 126/1000 | Loss: 0.00000916
Iteration 127/1000 | Loss: 0.00000916
Iteration 128/1000 | Loss: 0.00000915
Iteration 129/1000 | Loss: 0.00000915
Iteration 130/1000 | Loss: 0.00000915
Iteration 131/1000 | Loss: 0.00000915
Iteration 132/1000 | Loss: 0.00000915
Iteration 133/1000 | Loss: 0.00000915
Iteration 134/1000 | Loss: 0.00000915
Iteration 135/1000 | Loss: 0.00000915
Iteration 136/1000 | Loss: 0.00000915
Iteration 137/1000 | Loss: 0.00000914
Iteration 138/1000 | Loss: 0.00000914
Iteration 139/1000 | Loss: 0.00000914
Iteration 140/1000 | Loss: 0.00000913
Iteration 141/1000 | Loss: 0.00000913
Iteration 142/1000 | Loss: 0.00000913
Iteration 143/1000 | Loss: 0.00000913
Iteration 144/1000 | Loss: 0.00000913
Iteration 145/1000 | Loss: 0.00000912
Iteration 146/1000 | Loss: 0.00000912
Iteration 147/1000 | Loss: 0.00000912
Iteration 148/1000 | Loss: 0.00000912
Iteration 149/1000 | Loss: 0.00000912
Iteration 150/1000 | Loss: 0.00000912
Iteration 151/1000 | Loss: 0.00000912
Iteration 152/1000 | Loss: 0.00000912
Iteration 153/1000 | Loss: 0.00000911
Iteration 154/1000 | Loss: 0.00000911
Iteration 155/1000 | Loss: 0.00000911
Iteration 156/1000 | Loss: 0.00000911
Iteration 157/1000 | Loss: 0.00000911
Iteration 158/1000 | Loss: 0.00000911
Iteration 159/1000 | Loss: 0.00000911
Iteration 160/1000 | Loss: 0.00000911
Iteration 161/1000 | Loss: 0.00000911
Iteration 162/1000 | Loss: 0.00000911
Iteration 163/1000 | Loss: 0.00000910
Iteration 164/1000 | Loss: 0.00000910
Iteration 165/1000 | Loss: 0.00000910
Iteration 166/1000 | Loss: 0.00000910
Iteration 167/1000 | Loss: 0.00000910
Iteration 168/1000 | Loss: 0.00000910
Iteration 169/1000 | Loss: 0.00000910
Iteration 170/1000 | Loss: 0.00000910
Iteration 171/1000 | Loss: 0.00000910
Iteration 172/1000 | Loss: 0.00000910
Iteration 173/1000 | Loss: 0.00000910
Iteration 174/1000 | Loss: 0.00000910
Iteration 175/1000 | Loss: 0.00000910
Iteration 176/1000 | Loss: 0.00000910
Iteration 177/1000 | Loss: 0.00000910
Iteration 178/1000 | Loss: 0.00000910
Iteration 179/1000 | Loss: 0.00000910
Iteration 180/1000 | Loss: 0.00000910
Iteration 181/1000 | Loss: 0.00000910
Iteration 182/1000 | Loss: 0.00000910
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 182. Stopping optimization.
Last 5 losses: [9.09882874111645e-06, 9.09882874111645e-06, 9.09882874111645e-06, 9.09882874111645e-06, 9.09882874111645e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.09882874111645e-06

Optimization complete. Final v2v error: 2.5896477699279785 mm

Highest mean error: 3.2079246044158936 mm for frame 80

Lowest mean error: 2.4140138626098633 mm for frame 105

Saving results

Total time: 36.632483959198
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_003/1075/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_003/1075.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_003/1075
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01036169
Iteration 2/25 | Loss: 0.01036169
Iteration 3/25 | Loss: 0.00260050
Iteration 4/25 | Loss: 0.00193218
Iteration 5/25 | Loss: 0.00166504
Iteration 6/25 | Loss: 0.00154120
Iteration 7/25 | Loss: 0.00154622
Iteration 8/25 | Loss: 0.00139463
Iteration 9/25 | Loss: 0.00132712
Iteration 10/25 | Loss: 0.00128428
Iteration 11/25 | Loss: 0.00126256
Iteration 12/25 | Loss: 0.00124061
Iteration 13/25 | Loss: 0.00122953
Iteration 14/25 | Loss: 0.00122627
Iteration 15/25 | Loss: 0.00122138
Iteration 16/25 | Loss: 0.00122277
Iteration 17/25 | Loss: 0.00122094
Iteration 18/25 | Loss: 0.00122089
Iteration 19/25 | Loss: 0.00122089
Iteration 20/25 | Loss: 0.00122089
Iteration 21/25 | Loss: 0.00122089
Iteration 22/25 | Loss: 0.00122089
Iteration 23/25 | Loss: 0.00122089
Iteration 24/25 | Loss: 0.00122089
Iteration 25/25 | Loss: 0.00122089

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 6.68177509
Iteration 2/25 | Loss: 0.00151005
Iteration 3/25 | Loss: 0.00129604
Iteration 4/25 | Loss: 0.00129604
Iteration 5/25 | Loss: 0.00129604
Iteration 6/25 | Loss: 0.00129604
Iteration 7/25 | Loss: 0.00129604
Iteration 8/25 | Loss: 0.00129604
Iteration 9/25 | Loss: 0.00129604
Iteration 10/25 | Loss: 0.00129604
Iteration 11/25 | Loss: 0.00129604
Iteration 12/25 | Loss: 0.00129604
Iteration 13/25 | Loss: 0.00129604
Iteration 14/25 | Loss: 0.00129604
Iteration 15/25 | Loss: 0.00129604
Iteration 16/25 | Loss: 0.00129604
Iteration 17/25 | Loss: 0.00129604
Iteration 18/25 | Loss: 0.00129604
Iteration 19/25 | Loss: 0.00129604
Iteration 20/25 | Loss: 0.00129604
Iteration 21/25 | Loss: 0.00129604
Iteration 22/25 | Loss: 0.00129604
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0012960369931533933, 0.0012960369931533933, 0.0012960369931533933, 0.0012960369931533933, 0.0012960369931533933]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012960369931533933

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00129604
Iteration 2/1000 | Loss: 0.00012504
Iteration 3/1000 | Loss: 0.00009611
Iteration 4/1000 | Loss: 0.00007875
Iteration 5/1000 | Loss: 0.00021883
Iteration 6/1000 | Loss: 0.00003450
Iteration 7/1000 | Loss: 0.00008247
Iteration 8/1000 | Loss: 0.00020155
Iteration 9/1000 | Loss: 0.00010477
Iteration 10/1000 | Loss: 0.00002713
Iteration 11/1000 | Loss: 0.00008259
Iteration 12/1000 | Loss: 0.00332134
Iteration 13/1000 | Loss: 0.00167751
Iteration 14/1000 | Loss: 0.00009560
Iteration 15/1000 | Loss: 0.00037757
Iteration 16/1000 | Loss: 0.00006549
Iteration 17/1000 | Loss: 0.00002244
Iteration 18/1000 | Loss: 0.00032052
Iteration 19/1000 | Loss: 0.00007743
Iteration 20/1000 | Loss: 0.00001704
Iteration 21/1000 | Loss: 0.00005021
Iteration 22/1000 | Loss: 0.00001524
Iteration 23/1000 | Loss: 0.00016904
Iteration 24/1000 | Loss: 0.00011392
Iteration 25/1000 | Loss: 0.00002499
Iteration 26/1000 | Loss: 0.00002708
Iteration 27/1000 | Loss: 0.00001347
Iteration 28/1000 | Loss: 0.00006541
Iteration 29/1000 | Loss: 0.00001633
Iteration 30/1000 | Loss: 0.00004748
Iteration 31/1000 | Loss: 0.00001784
Iteration 32/1000 | Loss: 0.00001251
Iteration 33/1000 | Loss: 0.00002095
Iteration 34/1000 | Loss: 0.00001220
Iteration 35/1000 | Loss: 0.00007573
Iteration 36/1000 | Loss: 0.00001428
Iteration 37/1000 | Loss: 0.00001453
Iteration 38/1000 | Loss: 0.00001741
Iteration 39/1000 | Loss: 0.00004646
Iteration 40/1000 | Loss: 0.00001466
Iteration 41/1000 | Loss: 0.00001214
Iteration 42/1000 | Loss: 0.00001187
Iteration 43/1000 | Loss: 0.00001187
Iteration 44/1000 | Loss: 0.00001187
Iteration 45/1000 | Loss: 0.00001187
Iteration 46/1000 | Loss: 0.00001187
Iteration 47/1000 | Loss: 0.00001187
Iteration 48/1000 | Loss: 0.00001187
Iteration 49/1000 | Loss: 0.00001187
Iteration 50/1000 | Loss: 0.00001187
Iteration 51/1000 | Loss: 0.00001187
Iteration 52/1000 | Loss: 0.00001186
Iteration 53/1000 | Loss: 0.00001186
Iteration 54/1000 | Loss: 0.00001185
Iteration 55/1000 | Loss: 0.00001185
Iteration 56/1000 | Loss: 0.00001856
Iteration 57/1000 | Loss: 0.00001279
Iteration 58/1000 | Loss: 0.00001184
Iteration 59/1000 | Loss: 0.00001184
Iteration 60/1000 | Loss: 0.00001184
Iteration 61/1000 | Loss: 0.00001184
Iteration 62/1000 | Loss: 0.00001184
Iteration 63/1000 | Loss: 0.00001184
Iteration 64/1000 | Loss: 0.00001183
Iteration 65/1000 | Loss: 0.00001183
Iteration 66/1000 | Loss: 0.00001183
Iteration 67/1000 | Loss: 0.00001183
Iteration 68/1000 | Loss: 0.00001183
Iteration 69/1000 | Loss: 0.00001183
Iteration 70/1000 | Loss: 0.00001183
Iteration 71/1000 | Loss: 0.00001182
Iteration 72/1000 | Loss: 0.00001182
Iteration 73/1000 | Loss: 0.00001182
Iteration 74/1000 | Loss: 0.00001181
Iteration 75/1000 | Loss: 0.00001181
Iteration 76/1000 | Loss: 0.00001181
Iteration 77/1000 | Loss: 0.00001180
Iteration 78/1000 | Loss: 0.00001180
Iteration 79/1000 | Loss: 0.00001180
Iteration 80/1000 | Loss: 0.00001180
Iteration 81/1000 | Loss: 0.00001179
Iteration 82/1000 | Loss: 0.00001179
Iteration 83/1000 | Loss: 0.00001178
Iteration 84/1000 | Loss: 0.00001178
Iteration 85/1000 | Loss: 0.00001178
Iteration 86/1000 | Loss: 0.00001177
Iteration 87/1000 | Loss: 0.00001177
Iteration 88/1000 | Loss: 0.00001176
Iteration 89/1000 | Loss: 0.00001176
Iteration 90/1000 | Loss: 0.00001176
Iteration 91/1000 | Loss: 0.00001175
Iteration 92/1000 | Loss: 0.00001175
Iteration 93/1000 | Loss: 0.00001175
Iteration 94/1000 | Loss: 0.00001174
Iteration 95/1000 | Loss: 0.00001174
Iteration 96/1000 | Loss: 0.00001173
Iteration 97/1000 | Loss: 0.00001173
Iteration 98/1000 | Loss: 0.00001172
Iteration 99/1000 | Loss: 0.00001172
Iteration 100/1000 | Loss: 0.00001172
Iteration 101/1000 | Loss: 0.00001172
Iteration 102/1000 | Loss: 0.00001172
Iteration 103/1000 | Loss: 0.00001172
Iteration 104/1000 | Loss: 0.00001172
Iteration 105/1000 | Loss: 0.00001172
Iteration 106/1000 | Loss: 0.00001172
Iteration 107/1000 | Loss: 0.00001171
Iteration 108/1000 | Loss: 0.00001171
Iteration 109/1000 | Loss: 0.00001171
Iteration 110/1000 | Loss: 0.00001171
Iteration 111/1000 | Loss: 0.00001171
Iteration 112/1000 | Loss: 0.00001171
Iteration 113/1000 | Loss: 0.00001171
Iteration 114/1000 | Loss: 0.00001171
Iteration 115/1000 | Loss: 0.00001171
Iteration 116/1000 | Loss: 0.00001171
Iteration 117/1000 | Loss: 0.00001171
Iteration 118/1000 | Loss: 0.00001171
Iteration 119/1000 | Loss: 0.00001171
Iteration 120/1000 | Loss: 0.00001170
Iteration 121/1000 | Loss: 0.00001170
Iteration 122/1000 | Loss: 0.00001170
Iteration 123/1000 | Loss: 0.00001170
Iteration 124/1000 | Loss: 0.00001170
Iteration 125/1000 | Loss: 0.00001170
Iteration 126/1000 | Loss: 0.00001170
Iteration 127/1000 | Loss: 0.00001170
Iteration 128/1000 | Loss: 0.00001170
Iteration 129/1000 | Loss: 0.00001170
Iteration 130/1000 | Loss: 0.00001170
Iteration 131/1000 | Loss: 0.00001170
Iteration 132/1000 | Loss: 0.00001170
Iteration 133/1000 | Loss: 0.00001170
Iteration 134/1000 | Loss: 0.00001170
Iteration 135/1000 | Loss: 0.00001170
Iteration 136/1000 | Loss: 0.00001170
Iteration 137/1000 | Loss: 0.00001170
Iteration 138/1000 | Loss: 0.00001170
Iteration 139/1000 | Loss: 0.00001170
Iteration 140/1000 | Loss: 0.00001170
Iteration 141/1000 | Loss: 0.00001170
Iteration 142/1000 | Loss: 0.00001170
Iteration 143/1000 | Loss: 0.00001170
Iteration 144/1000 | Loss: 0.00001170
Iteration 145/1000 | Loss: 0.00001170
Iteration 146/1000 | Loss: 0.00001170
Iteration 147/1000 | Loss: 0.00001170
Iteration 148/1000 | Loss: 0.00001170
Iteration 149/1000 | Loss: 0.00001170
Iteration 150/1000 | Loss: 0.00001170
Iteration 151/1000 | Loss: 0.00001170
Iteration 152/1000 | Loss: 0.00001170
Iteration 153/1000 | Loss: 0.00001170
Iteration 154/1000 | Loss: 0.00001170
Iteration 155/1000 | Loss: 0.00001170
Iteration 156/1000 | Loss: 0.00001170
Iteration 157/1000 | Loss: 0.00001170
Iteration 158/1000 | Loss: 0.00001170
Iteration 159/1000 | Loss: 0.00001170
Iteration 160/1000 | Loss: 0.00001170
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 160. Stopping optimization.
Last 5 losses: [1.1701200492097996e-05, 1.1701200492097996e-05, 1.1701200492097996e-05, 1.1701200492097996e-05, 1.1701200492097996e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1701200492097996e-05

Optimization complete. Final v2v error: 2.9465479850769043 mm

Highest mean error: 3.486093282699585 mm for frame 88

Lowest mean error: 2.7075581550598145 mm for frame 24

Saving results

Total time: 101.73813462257385
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_003/1041/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_003/1041.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_003/1041
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00685394
Iteration 2/25 | Loss: 0.00135307
Iteration 3/25 | Loss: 0.00122951
Iteration 4/25 | Loss: 0.00120551
Iteration 5/25 | Loss: 0.00119891
Iteration 6/25 | Loss: 0.00119742
Iteration 7/25 | Loss: 0.00119742
Iteration 8/25 | Loss: 0.00119742
Iteration 9/25 | Loss: 0.00119742
Iteration 10/25 | Loss: 0.00119742
Iteration 11/25 | Loss: 0.00119742
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011974208755418658, 0.0011974208755418658, 0.0011974208755418658, 0.0011974208755418658, 0.0011974208755418658]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011974208755418658

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.93660831
Iteration 2/25 | Loss: 0.00142898
Iteration 3/25 | Loss: 0.00142898
Iteration 4/25 | Loss: 0.00142898
Iteration 5/25 | Loss: 0.00142898
Iteration 6/25 | Loss: 0.00142898
Iteration 7/25 | Loss: 0.00142898
Iteration 8/25 | Loss: 0.00142898
Iteration 9/25 | Loss: 0.00142898
Iteration 10/25 | Loss: 0.00142898
Iteration 11/25 | Loss: 0.00142898
Iteration 12/25 | Loss: 0.00142898
Iteration 13/25 | Loss: 0.00142898
Iteration 14/25 | Loss: 0.00142898
Iteration 15/25 | Loss: 0.00142898
Iteration 16/25 | Loss: 0.00142898
Iteration 17/25 | Loss: 0.00142898
Iteration 18/25 | Loss: 0.00142898
Iteration 19/25 | Loss: 0.00142898
Iteration 20/25 | Loss: 0.00142898
Iteration 21/25 | Loss: 0.00142898
Iteration 22/25 | Loss: 0.00142898
Iteration 23/25 | Loss: 0.00142898
Iteration 24/25 | Loss: 0.00142898
Iteration 25/25 | Loss: 0.00142898

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00142898
Iteration 2/1000 | Loss: 0.00002865
Iteration 3/1000 | Loss: 0.00001966
Iteration 4/1000 | Loss: 0.00001764
Iteration 5/1000 | Loss: 0.00001656
Iteration 6/1000 | Loss: 0.00001558
Iteration 7/1000 | Loss: 0.00001505
Iteration 8/1000 | Loss: 0.00001465
Iteration 9/1000 | Loss: 0.00001431
Iteration 10/1000 | Loss: 0.00001407
Iteration 11/1000 | Loss: 0.00001384
Iteration 12/1000 | Loss: 0.00001376
Iteration 13/1000 | Loss: 0.00001355
Iteration 14/1000 | Loss: 0.00001337
Iteration 15/1000 | Loss: 0.00001325
Iteration 16/1000 | Loss: 0.00001314
Iteration 17/1000 | Loss: 0.00001311
Iteration 18/1000 | Loss: 0.00001311
Iteration 19/1000 | Loss: 0.00001310
Iteration 20/1000 | Loss: 0.00001301
Iteration 21/1000 | Loss: 0.00001299
Iteration 22/1000 | Loss: 0.00001291
Iteration 23/1000 | Loss: 0.00001290
Iteration 24/1000 | Loss: 0.00001289
Iteration 25/1000 | Loss: 0.00001288
Iteration 26/1000 | Loss: 0.00001288
Iteration 27/1000 | Loss: 0.00001288
Iteration 28/1000 | Loss: 0.00001287
Iteration 29/1000 | Loss: 0.00001287
Iteration 30/1000 | Loss: 0.00001286
Iteration 31/1000 | Loss: 0.00001286
Iteration 32/1000 | Loss: 0.00001286
Iteration 33/1000 | Loss: 0.00001286
Iteration 34/1000 | Loss: 0.00001285
Iteration 35/1000 | Loss: 0.00001284
Iteration 36/1000 | Loss: 0.00001284
Iteration 37/1000 | Loss: 0.00001284
Iteration 38/1000 | Loss: 0.00001284
Iteration 39/1000 | Loss: 0.00001284
Iteration 40/1000 | Loss: 0.00001284
Iteration 41/1000 | Loss: 0.00001284
Iteration 42/1000 | Loss: 0.00001283
Iteration 43/1000 | Loss: 0.00001283
Iteration 44/1000 | Loss: 0.00001283
Iteration 45/1000 | Loss: 0.00001283
Iteration 46/1000 | Loss: 0.00001282
Iteration 47/1000 | Loss: 0.00001282
Iteration 48/1000 | Loss: 0.00001282
Iteration 49/1000 | Loss: 0.00001282
Iteration 50/1000 | Loss: 0.00001282
Iteration 51/1000 | Loss: 0.00001282
Iteration 52/1000 | Loss: 0.00001282
Iteration 53/1000 | Loss: 0.00001281
Iteration 54/1000 | Loss: 0.00001280
Iteration 55/1000 | Loss: 0.00001280
Iteration 56/1000 | Loss: 0.00001280
Iteration 57/1000 | Loss: 0.00001280
Iteration 58/1000 | Loss: 0.00001280
Iteration 59/1000 | Loss: 0.00001279
Iteration 60/1000 | Loss: 0.00001279
Iteration 61/1000 | Loss: 0.00001279
Iteration 62/1000 | Loss: 0.00001279
Iteration 63/1000 | Loss: 0.00001278
Iteration 64/1000 | Loss: 0.00001278
Iteration 65/1000 | Loss: 0.00001278
Iteration 66/1000 | Loss: 0.00001278
Iteration 67/1000 | Loss: 0.00001278
Iteration 68/1000 | Loss: 0.00001278
Iteration 69/1000 | Loss: 0.00001277
Iteration 70/1000 | Loss: 0.00001277
Iteration 71/1000 | Loss: 0.00001277
Iteration 72/1000 | Loss: 0.00001277
Iteration 73/1000 | Loss: 0.00001277
Iteration 74/1000 | Loss: 0.00001276
Iteration 75/1000 | Loss: 0.00001276
Iteration 76/1000 | Loss: 0.00001276
Iteration 77/1000 | Loss: 0.00001276
Iteration 78/1000 | Loss: 0.00001275
Iteration 79/1000 | Loss: 0.00001275
Iteration 80/1000 | Loss: 0.00001275
Iteration 81/1000 | Loss: 0.00001275
Iteration 82/1000 | Loss: 0.00001275
Iteration 83/1000 | Loss: 0.00001274
Iteration 84/1000 | Loss: 0.00001274
Iteration 85/1000 | Loss: 0.00001274
Iteration 86/1000 | Loss: 0.00001274
Iteration 87/1000 | Loss: 0.00001274
Iteration 88/1000 | Loss: 0.00001274
Iteration 89/1000 | Loss: 0.00001273
Iteration 90/1000 | Loss: 0.00001273
Iteration 91/1000 | Loss: 0.00001272
Iteration 92/1000 | Loss: 0.00001271
Iteration 93/1000 | Loss: 0.00001271
Iteration 94/1000 | Loss: 0.00001271
Iteration 95/1000 | Loss: 0.00001271
Iteration 96/1000 | Loss: 0.00001270
Iteration 97/1000 | Loss: 0.00001270
Iteration 98/1000 | Loss: 0.00001270
Iteration 99/1000 | Loss: 0.00001270
Iteration 100/1000 | Loss: 0.00001270
Iteration 101/1000 | Loss: 0.00001270
Iteration 102/1000 | Loss: 0.00001270
Iteration 103/1000 | Loss: 0.00001270
Iteration 104/1000 | Loss: 0.00001270
Iteration 105/1000 | Loss: 0.00001270
Iteration 106/1000 | Loss: 0.00001270
Iteration 107/1000 | Loss: 0.00001270
Iteration 108/1000 | Loss: 0.00001270
Iteration 109/1000 | Loss: 0.00001270
Iteration 110/1000 | Loss: 0.00001269
Iteration 111/1000 | Loss: 0.00001269
Iteration 112/1000 | Loss: 0.00001269
Iteration 113/1000 | Loss: 0.00001269
Iteration 114/1000 | Loss: 0.00001268
Iteration 115/1000 | Loss: 0.00001268
Iteration 116/1000 | Loss: 0.00001268
Iteration 117/1000 | Loss: 0.00001268
Iteration 118/1000 | Loss: 0.00001268
Iteration 119/1000 | Loss: 0.00001268
Iteration 120/1000 | Loss: 0.00001267
Iteration 121/1000 | Loss: 0.00001267
Iteration 122/1000 | Loss: 0.00001267
Iteration 123/1000 | Loss: 0.00001267
Iteration 124/1000 | Loss: 0.00001267
Iteration 125/1000 | Loss: 0.00001266
Iteration 126/1000 | Loss: 0.00001266
Iteration 127/1000 | Loss: 0.00001266
Iteration 128/1000 | Loss: 0.00001266
Iteration 129/1000 | Loss: 0.00001266
Iteration 130/1000 | Loss: 0.00001265
Iteration 131/1000 | Loss: 0.00001265
Iteration 132/1000 | Loss: 0.00001265
Iteration 133/1000 | Loss: 0.00001265
Iteration 134/1000 | Loss: 0.00001264
Iteration 135/1000 | Loss: 0.00001264
Iteration 136/1000 | Loss: 0.00001264
Iteration 137/1000 | Loss: 0.00001264
Iteration 138/1000 | Loss: 0.00001263
Iteration 139/1000 | Loss: 0.00001263
Iteration 140/1000 | Loss: 0.00001263
Iteration 141/1000 | Loss: 0.00001263
Iteration 142/1000 | Loss: 0.00001263
Iteration 143/1000 | Loss: 0.00001263
Iteration 144/1000 | Loss: 0.00001263
Iteration 145/1000 | Loss: 0.00001263
Iteration 146/1000 | Loss: 0.00001262
Iteration 147/1000 | Loss: 0.00001262
Iteration 148/1000 | Loss: 0.00001262
Iteration 149/1000 | Loss: 0.00001262
Iteration 150/1000 | Loss: 0.00001262
Iteration 151/1000 | Loss: 0.00001262
Iteration 152/1000 | Loss: 0.00001262
Iteration 153/1000 | Loss: 0.00001262
Iteration 154/1000 | Loss: 0.00001261
Iteration 155/1000 | Loss: 0.00001261
Iteration 156/1000 | Loss: 0.00001261
Iteration 157/1000 | Loss: 0.00001261
Iteration 158/1000 | Loss: 0.00001261
Iteration 159/1000 | Loss: 0.00001261
Iteration 160/1000 | Loss: 0.00001261
Iteration 161/1000 | Loss: 0.00001261
Iteration 162/1000 | Loss: 0.00001261
Iteration 163/1000 | Loss: 0.00001261
Iteration 164/1000 | Loss: 0.00001261
Iteration 165/1000 | Loss: 0.00001261
Iteration 166/1000 | Loss: 0.00001261
Iteration 167/1000 | Loss: 0.00001261
Iteration 168/1000 | Loss: 0.00001261
Iteration 169/1000 | Loss: 0.00001261
Iteration 170/1000 | Loss: 0.00001261
Iteration 171/1000 | Loss: 0.00001261
Iteration 172/1000 | Loss: 0.00001261
Iteration 173/1000 | Loss: 0.00001261
Iteration 174/1000 | Loss: 0.00001261
Iteration 175/1000 | Loss: 0.00001261
Iteration 176/1000 | Loss: 0.00001261
Iteration 177/1000 | Loss: 0.00001261
Iteration 178/1000 | Loss: 0.00001261
Iteration 179/1000 | Loss: 0.00001261
Iteration 180/1000 | Loss: 0.00001261
Iteration 181/1000 | Loss: 0.00001261
Iteration 182/1000 | Loss: 0.00001261
Iteration 183/1000 | Loss: 0.00001261
Iteration 184/1000 | Loss: 0.00001261
Iteration 185/1000 | Loss: 0.00001261
Iteration 186/1000 | Loss: 0.00001261
Iteration 187/1000 | Loss: 0.00001261
Iteration 188/1000 | Loss: 0.00001261
Iteration 189/1000 | Loss: 0.00001261
Iteration 190/1000 | Loss: 0.00001261
Iteration 191/1000 | Loss: 0.00001261
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 191. Stopping optimization.
Last 5 losses: [1.2608921679202467e-05, 1.2608921679202467e-05, 1.2608921679202467e-05, 1.2608921679202467e-05, 1.2608921679202467e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2608921679202467e-05

Optimization complete. Final v2v error: 3.064286470413208 mm

Highest mean error: 3.401628017425537 mm for frame 70

Lowest mean error: 2.823719024658203 mm for frame 43

Saving results

Total time: 49.72320008277893
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_003/1074/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_003/1074.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_003/1074
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00776344
Iteration 2/25 | Loss: 0.00148275
Iteration 3/25 | Loss: 0.00126977
Iteration 4/25 | Loss: 0.00124721
Iteration 5/25 | Loss: 0.00124298
Iteration 6/25 | Loss: 0.00124228
Iteration 7/25 | Loss: 0.00124228
Iteration 8/25 | Loss: 0.00124228
Iteration 9/25 | Loss: 0.00124228
Iteration 10/25 | Loss: 0.00124228
Iteration 11/25 | Loss: 0.00124228
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012422798899933696, 0.0012422798899933696, 0.0012422798899933696, 0.0012422798899933696, 0.0012422798899933696]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012422798899933696

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.30390620
Iteration 2/25 | Loss: 0.00099823
Iteration 3/25 | Loss: 0.00099822
Iteration 4/25 | Loss: 0.00099822
Iteration 5/25 | Loss: 0.00099822
Iteration 6/25 | Loss: 0.00099822
Iteration 7/25 | Loss: 0.00099822
Iteration 8/25 | Loss: 0.00099822
Iteration 9/25 | Loss: 0.00099822
Iteration 10/25 | Loss: 0.00099822
Iteration 11/25 | Loss: 0.00099822
Iteration 12/25 | Loss: 0.00099822
Iteration 13/25 | Loss: 0.00099822
Iteration 14/25 | Loss: 0.00099822
Iteration 15/25 | Loss: 0.00099822
Iteration 16/25 | Loss: 0.00099822
Iteration 17/25 | Loss: 0.00099822
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0009982151677832007, 0.0009982151677832007, 0.0009982151677832007, 0.0009982151677832007, 0.0009982151677832007]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009982151677832007

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00099822
Iteration 2/1000 | Loss: 0.00003085
Iteration 3/1000 | Loss: 0.00002290
Iteration 4/1000 | Loss: 0.00002060
Iteration 5/1000 | Loss: 0.00001933
Iteration 6/1000 | Loss: 0.00001849
Iteration 7/1000 | Loss: 0.00001801
Iteration 8/1000 | Loss: 0.00001761
Iteration 9/1000 | Loss: 0.00001714
Iteration 10/1000 | Loss: 0.00001693
Iteration 11/1000 | Loss: 0.00001681
Iteration 12/1000 | Loss: 0.00001663
Iteration 13/1000 | Loss: 0.00001646
Iteration 14/1000 | Loss: 0.00001638
Iteration 15/1000 | Loss: 0.00001638
Iteration 16/1000 | Loss: 0.00001633
Iteration 17/1000 | Loss: 0.00001632
Iteration 18/1000 | Loss: 0.00001631
Iteration 19/1000 | Loss: 0.00001630
Iteration 20/1000 | Loss: 0.00001630
Iteration 21/1000 | Loss: 0.00001627
Iteration 22/1000 | Loss: 0.00001625
Iteration 23/1000 | Loss: 0.00001624
Iteration 24/1000 | Loss: 0.00001623
Iteration 25/1000 | Loss: 0.00001622
Iteration 26/1000 | Loss: 0.00001621
Iteration 27/1000 | Loss: 0.00001621
Iteration 28/1000 | Loss: 0.00001621
Iteration 29/1000 | Loss: 0.00001620
Iteration 30/1000 | Loss: 0.00001620
Iteration 31/1000 | Loss: 0.00001620
Iteration 32/1000 | Loss: 0.00001619
Iteration 33/1000 | Loss: 0.00001619
Iteration 34/1000 | Loss: 0.00001618
Iteration 35/1000 | Loss: 0.00001618
Iteration 36/1000 | Loss: 0.00001617
Iteration 37/1000 | Loss: 0.00001617
Iteration 38/1000 | Loss: 0.00001616
Iteration 39/1000 | Loss: 0.00001616
Iteration 40/1000 | Loss: 0.00001615
Iteration 41/1000 | Loss: 0.00001615
Iteration 42/1000 | Loss: 0.00001615
Iteration 43/1000 | Loss: 0.00001614
Iteration 44/1000 | Loss: 0.00001614
Iteration 45/1000 | Loss: 0.00001614
Iteration 46/1000 | Loss: 0.00001613
Iteration 47/1000 | Loss: 0.00001613
Iteration 48/1000 | Loss: 0.00001613
Iteration 49/1000 | Loss: 0.00001613
Iteration 50/1000 | Loss: 0.00001613
Iteration 51/1000 | Loss: 0.00001613
Iteration 52/1000 | Loss: 0.00001613
Iteration 53/1000 | Loss: 0.00001613
Iteration 54/1000 | Loss: 0.00001613
Iteration 55/1000 | Loss: 0.00001613
Iteration 56/1000 | Loss: 0.00001613
Iteration 57/1000 | Loss: 0.00001613
Iteration 58/1000 | Loss: 0.00001613
Iteration 59/1000 | Loss: 0.00001613
Iteration 60/1000 | Loss: 0.00001613
Iteration 61/1000 | Loss: 0.00001613
Iteration 62/1000 | Loss: 0.00001612
Iteration 63/1000 | Loss: 0.00001612
Iteration 64/1000 | Loss: 0.00001612
Iteration 65/1000 | Loss: 0.00001612
Iteration 66/1000 | Loss: 0.00001612
Iteration 67/1000 | Loss: 0.00001612
Iteration 68/1000 | Loss: 0.00001612
Iteration 69/1000 | Loss: 0.00001612
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 69. Stopping optimization.
Last 5 losses: [1.6124498870340176e-05, 1.6124498870340176e-05, 1.6124498870340176e-05, 1.6124498870340176e-05, 1.6124498870340176e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6124498870340176e-05

Optimization complete. Final v2v error: 3.422621011734009 mm

Highest mean error: 3.9220359325408936 mm for frame 156

Lowest mean error: 3.110152244567871 mm for frame 58

Saving results

Total time: 34.89984893798828
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_003/1094/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_003/1094.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_003/1094
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00764437
Iteration 2/25 | Loss: 0.00169589
Iteration 3/25 | Loss: 0.00143945
Iteration 4/25 | Loss: 0.00140731
Iteration 5/25 | Loss: 0.00139793
Iteration 6/25 | Loss: 0.00139528
Iteration 7/25 | Loss: 0.00139459
Iteration 8/25 | Loss: 0.00139458
Iteration 9/25 | Loss: 0.00139458
Iteration 10/25 | Loss: 0.00139458
Iteration 11/25 | Loss: 0.00139458
Iteration 12/25 | Loss: 0.00139458
Iteration 13/25 | Loss: 0.00139458
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0013945798855274916, 0.0013945798855274916, 0.0013945798855274916, 0.0013945798855274916, 0.0013945798855274916]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013945798855274916

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 8.82984066
Iteration 2/25 | Loss: 0.00134669
Iteration 3/25 | Loss: 0.00134129
Iteration 4/25 | Loss: 0.00134129
Iteration 5/25 | Loss: 0.00134129
Iteration 6/25 | Loss: 0.00134129
Iteration 7/25 | Loss: 0.00134129
Iteration 8/25 | Loss: 0.00134129
Iteration 9/25 | Loss: 0.00134129
Iteration 10/25 | Loss: 0.00134129
Iteration 11/25 | Loss: 0.00134129
Iteration 12/25 | Loss: 0.00134129
Iteration 13/25 | Loss: 0.00134129
Iteration 14/25 | Loss: 0.00134129
Iteration 15/25 | Loss: 0.00134129
Iteration 16/25 | Loss: 0.00134129
Iteration 17/25 | Loss: 0.00134129
Iteration 18/25 | Loss: 0.00134129
Iteration 19/25 | Loss: 0.00134129
Iteration 20/25 | Loss: 0.00134129
Iteration 21/25 | Loss: 0.00134129
Iteration 22/25 | Loss: 0.00134129
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0013412880944088101, 0.0013412880944088101, 0.0013412880944088101, 0.0013412880944088101, 0.0013412880944088101]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013412880944088101

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00134129
Iteration 2/1000 | Loss: 0.00009262
Iteration 3/1000 | Loss: 0.00005383
Iteration 4/1000 | Loss: 0.00004480
Iteration 5/1000 | Loss: 0.00004169
Iteration 6/1000 | Loss: 0.00004017
Iteration 7/1000 | Loss: 0.00003892
Iteration 8/1000 | Loss: 0.00003819
Iteration 9/1000 | Loss: 0.00003750
Iteration 10/1000 | Loss: 0.00003704
Iteration 11/1000 | Loss: 0.00003662
Iteration 12/1000 | Loss: 0.00003629
Iteration 13/1000 | Loss: 0.00003609
Iteration 14/1000 | Loss: 0.00003587
Iteration 15/1000 | Loss: 0.00003566
Iteration 16/1000 | Loss: 0.00003547
Iteration 17/1000 | Loss: 0.00003532
Iteration 18/1000 | Loss: 0.00003517
Iteration 19/1000 | Loss: 0.00003516
Iteration 20/1000 | Loss: 0.00003515
Iteration 21/1000 | Loss: 0.00003511
Iteration 22/1000 | Loss: 0.00003509
Iteration 23/1000 | Loss: 0.00003507
Iteration 24/1000 | Loss: 0.00003507
Iteration 25/1000 | Loss: 0.00003507
Iteration 26/1000 | Loss: 0.00003502
Iteration 27/1000 | Loss: 0.00003501
Iteration 28/1000 | Loss: 0.00003501
Iteration 29/1000 | Loss: 0.00003497
Iteration 30/1000 | Loss: 0.00003497
Iteration 31/1000 | Loss: 0.00003493
Iteration 32/1000 | Loss: 0.00003489
Iteration 33/1000 | Loss: 0.00003489
Iteration 34/1000 | Loss: 0.00003485
Iteration 35/1000 | Loss: 0.00003485
Iteration 36/1000 | Loss: 0.00003485
Iteration 37/1000 | Loss: 0.00003484
Iteration 38/1000 | Loss: 0.00003484
Iteration 39/1000 | Loss: 0.00003483
Iteration 40/1000 | Loss: 0.00003483
Iteration 41/1000 | Loss: 0.00003483
Iteration 42/1000 | Loss: 0.00003483
Iteration 43/1000 | Loss: 0.00003482
Iteration 44/1000 | Loss: 0.00003482
Iteration 45/1000 | Loss: 0.00003482
Iteration 46/1000 | Loss: 0.00003482
Iteration 47/1000 | Loss: 0.00003482
Iteration 48/1000 | Loss: 0.00003482
Iteration 49/1000 | Loss: 0.00003481
Iteration 50/1000 | Loss: 0.00003481
Iteration 51/1000 | Loss: 0.00003481
Iteration 52/1000 | Loss: 0.00003480
Iteration 53/1000 | Loss: 0.00003480
Iteration 54/1000 | Loss: 0.00003480
Iteration 55/1000 | Loss: 0.00003480
Iteration 56/1000 | Loss: 0.00003480
Iteration 57/1000 | Loss: 0.00003480
Iteration 58/1000 | Loss: 0.00003479
Iteration 59/1000 | Loss: 0.00003479
Iteration 60/1000 | Loss: 0.00003479
Iteration 61/1000 | Loss: 0.00003479
Iteration 62/1000 | Loss: 0.00003478
Iteration 63/1000 | Loss: 0.00003477
Iteration 64/1000 | Loss: 0.00003477
Iteration 65/1000 | Loss: 0.00003477
Iteration 66/1000 | Loss: 0.00003477
Iteration 67/1000 | Loss: 0.00003477
Iteration 68/1000 | Loss: 0.00003477
Iteration 69/1000 | Loss: 0.00003476
Iteration 70/1000 | Loss: 0.00003476
Iteration 71/1000 | Loss: 0.00003476
Iteration 72/1000 | Loss: 0.00003476
Iteration 73/1000 | Loss: 0.00003476
Iteration 74/1000 | Loss: 0.00003476
Iteration 75/1000 | Loss: 0.00003475
Iteration 76/1000 | Loss: 0.00003475
Iteration 77/1000 | Loss: 0.00003475
Iteration 78/1000 | Loss: 0.00003475
Iteration 79/1000 | Loss: 0.00003475
Iteration 80/1000 | Loss: 0.00003475
Iteration 81/1000 | Loss: 0.00003475
Iteration 82/1000 | Loss: 0.00003475
Iteration 83/1000 | Loss: 0.00003475
Iteration 84/1000 | Loss: 0.00003475
Iteration 85/1000 | Loss: 0.00003475
Iteration 86/1000 | Loss: 0.00003475
Iteration 87/1000 | Loss: 0.00003475
Iteration 88/1000 | Loss: 0.00003474
Iteration 89/1000 | Loss: 0.00003474
Iteration 90/1000 | Loss: 0.00003474
Iteration 91/1000 | Loss: 0.00003474
Iteration 92/1000 | Loss: 0.00003474
Iteration 93/1000 | Loss: 0.00003474
Iteration 94/1000 | Loss: 0.00003474
Iteration 95/1000 | Loss: 0.00003474
Iteration 96/1000 | Loss: 0.00003474
Iteration 97/1000 | Loss: 0.00003473
Iteration 98/1000 | Loss: 0.00003473
Iteration 99/1000 | Loss: 0.00003473
Iteration 100/1000 | Loss: 0.00003473
Iteration 101/1000 | Loss: 0.00003473
Iteration 102/1000 | Loss: 0.00003473
Iteration 103/1000 | Loss: 0.00003473
Iteration 104/1000 | Loss: 0.00003473
Iteration 105/1000 | Loss: 0.00003473
Iteration 106/1000 | Loss: 0.00003473
Iteration 107/1000 | Loss: 0.00003473
Iteration 108/1000 | Loss: 0.00003473
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 108. Stopping optimization.
Last 5 losses: [3.473482502158731e-05, 3.473482502158731e-05, 3.473482502158731e-05, 3.473482502158731e-05, 3.473482502158731e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.473482502158731e-05

Optimization complete. Final v2v error: 4.7162885665893555 mm

Highest mean error: 7.409874439239502 mm for frame 145

Lowest mean error: 3.5801284313201904 mm for frame 12

Saving results

Total time: 53.67952919006348
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_003/1066/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_003/1066.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_003/1066
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00800615
Iteration 2/25 | Loss: 0.00160000
Iteration 3/25 | Loss: 0.00133802
Iteration 4/25 | Loss: 0.00129490
Iteration 5/25 | Loss: 0.00128529
Iteration 6/25 | Loss: 0.00128354
Iteration 7/25 | Loss: 0.00128354
Iteration 8/25 | Loss: 0.00128354
Iteration 9/25 | Loss: 0.00128354
Iteration 10/25 | Loss: 0.00128354
Iteration 11/25 | Loss: 0.00128354
Iteration 12/25 | Loss: 0.00128354
Iteration 13/25 | Loss: 0.00128354
Iteration 14/25 | Loss: 0.00128354
Iteration 15/25 | Loss: 0.00128354
Iteration 16/25 | Loss: 0.00128354
Iteration 17/25 | Loss: 0.00128354
Iteration 18/25 | Loss: 0.00128354
Iteration 19/25 | Loss: 0.00128354
Iteration 20/25 | Loss: 0.00128354
Iteration 21/25 | Loss: 0.00128354
Iteration 22/25 | Loss: 0.00128354
Iteration 23/25 | Loss: 0.00128354
Iteration 24/25 | Loss: 0.00128354
Iteration 25/25 | Loss: 0.00128354

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.30823112
Iteration 2/25 | Loss: 0.00109909
Iteration 3/25 | Loss: 0.00109909
Iteration 4/25 | Loss: 0.00109909
Iteration 5/25 | Loss: 0.00109909
Iteration 6/25 | Loss: 0.00109909
Iteration 7/25 | Loss: 0.00109908
Iteration 8/25 | Loss: 0.00109908
Iteration 9/25 | Loss: 0.00109908
Iteration 10/25 | Loss: 0.00109908
Iteration 11/25 | Loss: 0.00109908
Iteration 12/25 | Loss: 0.00109908
Iteration 13/25 | Loss: 0.00109908
Iteration 14/25 | Loss: 0.00109908
Iteration 15/25 | Loss: 0.00109908
Iteration 16/25 | Loss: 0.00109908
Iteration 17/25 | Loss: 0.00109908
Iteration 18/25 | Loss: 0.00109908
Iteration 19/25 | Loss: 0.00109908
Iteration 20/25 | Loss: 0.00109908
Iteration 21/25 | Loss: 0.00109908
Iteration 22/25 | Loss: 0.00109908
Iteration 23/25 | Loss: 0.00109908
Iteration 24/25 | Loss: 0.00109908
Iteration 25/25 | Loss: 0.00109908

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00109908
Iteration 2/1000 | Loss: 0.00003240
Iteration 3/1000 | Loss: 0.00002438
Iteration 4/1000 | Loss: 0.00002290
Iteration 5/1000 | Loss: 0.00002185
Iteration 6/1000 | Loss: 0.00002116
Iteration 7/1000 | Loss: 0.00002070
Iteration 8/1000 | Loss: 0.00002038
Iteration 9/1000 | Loss: 0.00001996
Iteration 10/1000 | Loss: 0.00001973
Iteration 11/1000 | Loss: 0.00001969
Iteration 12/1000 | Loss: 0.00001965
Iteration 13/1000 | Loss: 0.00001956
Iteration 14/1000 | Loss: 0.00001951
Iteration 15/1000 | Loss: 0.00001950
Iteration 16/1000 | Loss: 0.00001945
Iteration 17/1000 | Loss: 0.00001937
Iteration 18/1000 | Loss: 0.00001930
Iteration 19/1000 | Loss: 0.00001928
Iteration 20/1000 | Loss: 0.00001927
Iteration 21/1000 | Loss: 0.00001927
Iteration 22/1000 | Loss: 0.00001926
Iteration 23/1000 | Loss: 0.00001925
Iteration 24/1000 | Loss: 0.00001923
Iteration 25/1000 | Loss: 0.00001923
Iteration 26/1000 | Loss: 0.00001923
Iteration 27/1000 | Loss: 0.00001923
Iteration 28/1000 | Loss: 0.00001923
Iteration 29/1000 | Loss: 0.00001923
Iteration 30/1000 | Loss: 0.00001922
Iteration 31/1000 | Loss: 0.00001919
Iteration 32/1000 | Loss: 0.00001918
Iteration 33/1000 | Loss: 0.00001918
Iteration 34/1000 | Loss: 0.00001918
Iteration 35/1000 | Loss: 0.00001918
Iteration 36/1000 | Loss: 0.00001917
Iteration 37/1000 | Loss: 0.00001916
Iteration 38/1000 | Loss: 0.00001916
Iteration 39/1000 | Loss: 0.00001913
Iteration 40/1000 | Loss: 0.00001911
Iteration 41/1000 | Loss: 0.00001910
Iteration 42/1000 | Loss: 0.00001909
Iteration 43/1000 | Loss: 0.00001909
Iteration 44/1000 | Loss: 0.00001909
Iteration 45/1000 | Loss: 0.00001908
Iteration 46/1000 | Loss: 0.00001908
Iteration 47/1000 | Loss: 0.00001907
Iteration 48/1000 | Loss: 0.00001907
Iteration 49/1000 | Loss: 0.00001907
Iteration 50/1000 | Loss: 0.00001907
Iteration 51/1000 | Loss: 0.00001906
Iteration 52/1000 | Loss: 0.00001906
Iteration 53/1000 | Loss: 0.00001906
Iteration 54/1000 | Loss: 0.00001906
Iteration 55/1000 | Loss: 0.00001905
Iteration 56/1000 | Loss: 0.00001905
Iteration 57/1000 | Loss: 0.00001905
Iteration 58/1000 | Loss: 0.00001905
Iteration 59/1000 | Loss: 0.00001905
Iteration 60/1000 | Loss: 0.00001905
Iteration 61/1000 | Loss: 0.00001904
Iteration 62/1000 | Loss: 0.00001904
Iteration 63/1000 | Loss: 0.00001904
Iteration 64/1000 | Loss: 0.00001904
Iteration 65/1000 | Loss: 0.00001904
Iteration 66/1000 | Loss: 0.00001904
Iteration 67/1000 | Loss: 0.00001903
Iteration 68/1000 | Loss: 0.00001903
Iteration 69/1000 | Loss: 0.00001903
Iteration 70/1000 | Loss: 0.00001903
Iteration 71/1000 | Loss: 0.00001903
Iteration 72/1000 | Loss: 0.00001902
Iteration 73/1000 | Loss: 0.00001902
Iteration 74/1000 | Loss: 0.00001902
Iteration 75/1000 | Loss: 0.00001902
Iteration 76/1000 | Loss: 0.00001902
Iteration 77/1000 | Loss: 0.00001902
Iteration 78/1000 | Loss: 0.00001901
Iteration 79/1000 | Loss: 0.00001901
Iteration 80/1000 | Loss: 0.00001901
Iteration 81/1000 | Loss: 0.00001901
Iteration 82/1000 | Loss: 0.00001901
Iteration 83/1000 | Loss: 0.00001900
Iteration 84/1000 | Loss: 0.00001900
Iteration 85/1000 | Loss: 0.00001900
Iteration 86/1000 | Loss: 0.00001900
Iteration 87/1000 | Loss: 0.00001900
Iteration 88/1000 | Loss: 0.00001900
Iteration 89/1000 | Loss: 0.00001900
Iteration 90/1000 | Loss: 0.00001900
Iteration 91/1000 | Loss: 0.00001899
Iteration 92/1000 | Loss: 0.00001899
Iteration 93/1000 | Loss: 0.00001899
Iteration 94/1000 | Loss: 0.00001899
Iteration 95/1000 | Loss: 0.00001898
Iteration 96/1000 | Loss: 0.00001898
Iteration 97/1000 | Loss: 0.00001898
Iteration 98/1000 | Loss: 0.00001898
Iteration 99/1000 | Loss: 0.00001897
Iteration 100/1000 | Loss: 0.00001897
Iteration 101/1000 | Loss: 0.00001897
Iteration 102/1000 | Loss: 0.00001897
Iteration 103/1000 | Loss: 0.00001896
Iteration 104/1000 | Loss: 0.00001896
Iteration 105/1000 | Loss: 0.00001896
Iteration 106/1000 | Loss: 0.00001896
Iteration 107/1000 | Loss: 0.00001896
Iteration 108/1000 | Loss: 0.00001896
Iteration 109/1000 | Loss: 0.00001895
Iteration 110/1000 | Loss: 0.00001895
Iteration 111/1000 | Loss: 0.00001895
Iteration 112/1000 | Loss: 0.00001895
Iteration 113/1000 | Loss: 0.00001895
Iteration 114/1000 | Loss: 0.00001895
Iteration 115/1000 | Loss: 0.00001894
Iteration 116/1000 | Loss: 0.00001894
Iteration 117/1000 | Loss: 0.00001894
Iteration 118/1000 | Loss: 0.00001894
Iteration 119/1000 | Loss: 0.00001894
Iteration 120/1000 | Loss: 0.00001894
Iteration 121/1000 | Loss: 0.00001894
Iteration 122/1000 | Loss: 0.00001893
Iteration 123/1000 | Loss: 0.00001893
Iteration 124/1000 | Loss: 0.00001893
Iteration 125/1000 | Loss: 0.00001892
Iteration 126/1000 | Loss: 0.00001892
Iteration 127/1000 | Loss: 0.00001892
Iteration 128/1000 | Loss: 0.00001891
Iteration 129/1000 | Loss: 0.00001891
Iteration 130/1000 | Loss: 0.00001891
Iteration 131/1000 | Loss: 0.00001891
Iteration 132/1000 | Loss: 0.00001891
Iteration 133/1000 | Loss: 0.00001890
Iteration 134/1000 | Loss: 0.00001890
Iteration 135/1000 | Loss: 0.00001890
Iteration 136/1000 | Loss: 0.00001890
Iteration 137/1000 | Loss: 0.00001889
Iteration 138/1000 | Loss: 0.00001889
Iteration 139/1000 | Loss: 0.00001889
Iteration 140/1000 | Loss: 0.00001889
Iteration 141/1000 | Loss: 0.00001889
Iteration 142/1000 | Loss: 0.00001889
Iteration 143/1000 | Loss: 0.00001889
Iteration 144/1000 | Loss: 0.00001888
Iteration 145/1000 | Loss: 0.00001888
Iteration 146/1000 | Loss: 0.00001888
Iteration 147/1000 | Loss: 0.00001888
Iteration 148/1000 | Loss: 0.00001888
Iteration 149/1000 | Loss: 0.00001888
Iteration 150/1000 | Loss: 0.00001888
Iteration 151/1000 | Loss: 0.00001888
Iteration 152/1000 | Loss: 0.00001888
Iteration 153/1000 | Loss: 0.00001888
Iteration 154/1000 | Loss: 0.00001888
Iteration 155/1000 | Loss: 0.00001888
Iteration 156/1000 | Loss: 0.00001887
Iteration 157/1000 | Loss: 0.00001887
Iteration 158/1000 | Loss: 0.00001887
Iteration 159/1000 | Loss: 0.00001886
Iteration 160/1000 | Loss: 0.00001886
Iteration 161/1000 | Loss: 0.00001886
Iteration 162/1000 | Loss: 0.00001885
Iteration 163/1000 | Loss: 0.00001885
Iteration 164/1000 | Loss: 0.00001885
Iteration 165/1000 | Loss: 0.00001885
Iteration 166/1000 | Loss: 0.00001885
Iteration 167/1000 | Loss: 0.00001885
Iteration 168/1000 | Loss: 0.00001885
Iteration 169/1000 | Loss: 0.00001885
Iteration 170/1000 | Loss: 0.00001884
Iteration 171/1000 | Loss: 0.00001884
Iteration 172/1000 | Loss: 0.00001884
Iteration 173/1000 | Loss: 0.00001884
Iteration 174/1000 | Loss: 0.00001884
Iteration 175/1000 | Loss: 0.00001884
Iteration 176/1000 | Loss: 0.00001884
Iteration 177/1000 | Loss: 0.00001884
Iteration 178/1000 | Loss: 0.00001884
Iteration 179/1000 | Loss: 0.00001884
Iteration 180/1000 | Loss: 0.00001884
Iteration 181/1000 | Loss: 0.00001884
Iteration 182/1000 | Loss: 0.00001883
Iteration 183/1000 | Loss: 0.00001883
Iteration 184/1000 | Loss: 0.00001883
Iteration 185/1000 | Loss: 0.00001883
Iteration 186/1000 | Loss: 0.00001883
Iteration 187/1000 | Loss: 0.00001883
Iteration 188/1000 | Loss: 0.00001883
Iteration 189/1000 | Loss: 0.00001883
Iteration 190/1000 | Loss: 0.00001883
Iteration 191/1000 | Loss: 0.00001883
Iteration 192/1000 | Loss: 0.00001883
Iteration 193/1000 | Loss: 0.00001883
Iteration 194/1000 | Loss: 0.00001883
Iteration 195/1000 | Loss: 0.00001883
Iteration 196/1000 | Loss: 0.00001883
Iteration 197/1000 | Loss: 0.00001883
Iteration 198/1000 | Loss: 0.00001883
Iteration 199/1000 | Loss: 0.00001883
Iteration 200/1000 | Loss: 0.00001883
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 200. Stopping optimization.
Last 5 losses: [1.8828817701432854e-05, 1.8828817701432854e-05, 1.8828817701432854e-05, 1.8828817701432854e-05, 1.8828817701432854e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8828817701432854e-05

Optimization complete. Final v2v error: 3.6386139392852783 mm

Highest mean error: 4.234830856323242 mm for frame 197

Lowest mean error: 3.273322343826294 mm for frame 0

Saving results

Total time: 47.180726766586304
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_003/1046/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_003/1046.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_003/1046
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01012759
Iteration 2/25 | Loss: 0.00178620
Iteration 3/25 | Loss: 0.00139632
Iteration 4/25 | Loss: 0.00134084
Iteration 5/25 | Loss: 0.00134128
Iteration 6/25 | Loss: 0.00131484
Iteration 7/25 | Loss: 0.00130874
Iteration 8/25 | Loss: 0.00130736
Iteration 9/25 | Loss: 0.00130706
Iteration 10/25 | Loss: 0.00130705
Iteration 11/25 | Loss: 0.00130705
Iteration 12/25 | Loss: 0.00130705
Iteration 13/25 | Loss: 0.00130705
Iteration 14/25 | Loss: 0.00130705
Iteration 15/25 | Loss: 0.00130705
Iteration 16/25 | Loss: 0.00130705
Iteration 17/25 | Loss: 0.00130705
Iteration 18/25 | Loss: 0.00130705
Iteration 19/25 | Loss: 0.00130705
Iteration 20/25 | Loss: 0.00130705
Iteration 21/25 | Loss: 0.00130705
Iteration 22/25 | Loss: 0.00130705
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0013070517452433705, 0.0013070517452433705, 0.0013070517452433705, 0.0013070517452433705, 0.0013070517452433705]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013070517452433705

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.26577961
Iteration 2/25 | Loss: 0.00136818
Iteration 3/25 | Loss: 0.00136818
Iteration 4/25 | Loss: 0.00136818
Iteration 5/25 | Loss: 0.00136818
Iteration 6/25 | Loss: 0.00136818
Iteration 7/25 | Loss: 0.00136818
Iteration 8/25 | Loss: 0.00136818
Iteration 9/25 | Loss: 0.00136818
Iteration 10/25 | Loss: 0.00136818
Iteration 11/25 | Loss: 0.00136818
Iteration 12/25 | Loss: 0.00136818
Iteration 13/25 | Loss: 0.00136818
Iteration 14/25 | Loss: 0.00136818
Iteration 15/25 | Loss: 0.00136818
Iteration 16/25 | Loss: 0.00136818
Iteration 17/25 | Loss: 0.00136818
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.001368176774121821, 0.001368176774121821, 0.001368176774121821, 0.001368176774121821, 0.001368176774121821]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001368176774121821

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00136818
Iteration 2/1000 | Loss: 0.00003215
Iteration 3/1000 | Loss: 0.00002426
Iteration 4/1000 | Loss: 0.00002251
Iteration 5/1000 | Loss: 0.00002164
Iteration 6/1000 | Loss: 0.00002120
Iteration 7/1000 | Loss: 0.00002080
Iteration 8/1000 | Loss: 0.00002071
Iteration 9/1000 | Loss: 0.00002055
Iteration 10/1000 | Loss: 0.00002034
Iteration 11/1000 | Loss: 0.00002018
Iteration 12/1000 | Loss: 0.00001999
Iteration 13/1000 | Loss: 0.00001968
Iteration 14/1000 | Loss: 0.00001947
Iteration 15/1000 | Loss: 0.00001938
Iteration 16/1000 | Loss: 0.00001938
Iteration 17/1000 | Loss: 0.00001937
Iteration 18/1000 | Loss: 0.00001936
Iteration 19/1000 | Loss: 0.00001936
Iteration 20/1000 | Loss: 0.00001934
Iteration 21/1000 | Loss: 0.00001931
Iteration 22/1000 | Loss: 0.00001930
Iteration 23/1000 | Loss: 0.00001930
Iteration 24/1000 | Loss: 0.00001929
Iteration 25/1000 | Loss: 0.00001928
Iteration 26/1000 | Loss: 0.00001927
Iteration 27/1000 | Loss: 0.00001923
Iteration 28/1000 | Loss: 0.00001922
Iteration 29/1000 | Loss: 0.00001922
Iteration 30/1000 | Loss: 0.00001922
Iteration 31/1000 | Loss: 0.00001921
Iteration 32/1000 | Loss: 0.00001920
Iteration 33/1000 | Loss: 0.00001920
Iteration 34/1000 | Loss: 0.00001919
Iteration 35/1000 | Loss: 0.00001919
Iteration 36/1000 | Loss: 0.00001919
Iteration 37/1000 | Loss: 0.00001918
Iteration 38/1000 | Loss: 0.00001917
Iteration 39/1000 | Loss: 0.00001917
Iteration 40/1000 | Loss: 0.00001916
Iteration 41/1000 | Loss: 0.00001916
Iteration 42/1000 | Loss: 0.00001916
Iteration 43/1000 | Loss: 0.00001915
Iteration 44/1000 | Loss: 0.00001915
Iteration 45/1000 | Loss: 0.00001915
Iteration 46/1000 | Loss: 0.00001915
Iteration 47/1000 | Loss: 0.00001915
Iteration 48/1000 | Loss: 0.00001915
Iteration 49/1000 | Loss: 0.00001915
Iteration 50/1000 | Loss: 0.00001915
Iteration 51/1000 | Loss: 0.00001914
Iteration 52/1000 | Loss: 0.00001914
Iteration 53/1000 | Loss: 0.00001914
Iteration 54/1000 | Loss: 0.00001913
Iteration 55/1000 | Loss: 0.00001913
Iteration 56/1000 | Loss: 0.00001913
Iteration 57/1000 | Loss: 0.00001912
Iteration 58/1000 | Loss: 0.00001912
Iteration 59/1000 | Loss: 0.00001912
Iteration 60/1000 | Loss: 0.00001912
Iteration 61/1000 | Loss: 0.00001912
Iteration 62/1000 | Loss: 0.00001911
Iteration 63/1000 | Loss: 0.00001911
Iteration 64/1000 | Loss: 0.00001910
Iteration 65/1000 | Loss: 0.00001910
Iteration 66/1000 | Loss: 0.00001910
Iteration 67/1000 | Loss: 0.00001909
Iteration 68/1000 | Loss: 0.00001909
Iteration 69/1000 | Loss: 0.00001909
Iteration 70/1000 | Loss: 0.00001909
Iteration 71/1000 | Loss: 0.00001909
Iteration 72/1000 | Loss: 0.00001909
Iteration 73/1000 | Loss: 0.00001909
Iteration 74/1000 | Loss: 0.00001909
Iteration 75/1000 | Loss: 0.00001909
Iteration 76/1000 | Loss: 0.00001909
Iteration 77/1000 | Loss: 0.00001909
Iteration 78/1000 | Loss: 0.00001909
Iteration 79/1000 | Loss: 0.00001909
Iteration 80/1000 | Loss: 0.00001909
Iteration 81/1000 | Loss: 0.00001909
Iteration 82/1000 | Loss: 0.00001909
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 82. Stopping optimization.
Last 5 losses: [1.9086646716459654e-05, 1.9086646716459654e-05, 1.9086646716459654e-05, 1.9086646716459654e-05, 1.9086646716459654e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9086646716459654e-05

Optimization complete. Final v2v error: 3.689460039138794 mm

Highest mean error: 3.781312942504883 mm for frame 82

Lowest mean error: 3.486466646194458 mm for frame 173

Saving results

Total time: 39.90976333618164
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_003/1087/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_003/1087.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_003/1087
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00787039
Iteration 2/25 | Loss: 0.00160556
Iteration 3/25 | Loss: 0.00137951
Iteration 4/25 | Loss: 0.00135886
Iteration 5/25 | Loss: 0.00131712
Iteration 6/25 | Loss: 0.00131324
Iteration 7/25 | Loss: 0.00130757
Iteration 8/25 | Loss: 0.00130338
Iteration 9/25 | Loss: 0.00129883
Iteration 10/25 | Loss: 0.00129681
Iteration 11/25 | Loss: 0.00129608
Iteration 12/25 | Loss: 0.00129577
Iteration 13/25 | Loss: 0.00129563
Iteration 14/25 | Loss: 0.00129562
Iteration 15/25 | Loss: 0.00129562
Iteration 16/25 | Loss: 0.00129562
Iteration 17/25 | Loss: 0.00129562
Iteration 18/25 | Loss: 0.00129562
Iteration 19/25 | Loss: 0.00129562
Iteration 20/25 | Loss: 0.00129562
Iteration 21/25 | Loss: 0.00129561
Iteration 22/25 | Loss: 0.00129561
Iteration 23/25 | Loss: 0.00129561
Iteration 24/25 | Loss: 0.00129561
Iteration 25/25 | Loss: 0.00129561

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.56573749
Iteration 2/25 | Loss: 0.00101601
Iteration 3/25 | Loss: 0.00101601
Iteration 4/25 | Loss: 0.00101601
Iteration 5/25 | Loss: 0.00101601
Iteration 6/25 | Loss: 0.00101601
Iteration 7/25 | Loss: 0.00101601
Iteration 8/25 | Loss: 0.00101600
Iteration 9/25 | Loss: 0.00101600
Iteration 10/25 | Loss: 0.00101600
Iteration 11/25 | Loss: 0.00101600
Iteration 12/25 | Loss: 0.00101600
Iteration 13/25 | Loss: 0.00101600
Iteration 14/25 | Loss: 0.00101600
Iteration 15/25 | Loss: 0.00101600
Iteration 16/25 | Loss: 0.00101600
Iteration 17/25 | Loss: 0.00101600
Iteration 18/25 | Loss: 0.00101600
Iteration 19/25 | Loss: 0.00101600
Iteration 20/25 | Loss: 0.00101600
Iteration 21/25 | Loss: 0.00101600
Iteration 22/25 | Loss: 0.00101600
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0010160037782043219, 0.0010160037782043219, 0.0010160037782043219, 0.0010160037782043219, 0.0010160037782043219]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010160037782043219

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00101600
Iteration 2/1000 | Loss: 0.00004680
Iteration 3/1000 | Loss: 0.00002715
Iteration 4/1000 | Loss: 0.00002229
Iteration 5/1000 | Loss: 0.00002107
Iteration 6/1000 | Loss: 0.00002028
Iteration 7/1000 | Loss: 0.00001964
Iteration 8/1000 | Loss: 0.00001916
Iteration 9/1000 | Loss: 0.00001888
Iteration 10/1000 | Loss: 0.00001859
Iteration 11/1000 | Loss: 0.00001838
Iteration 12/1000 | Loss: 0.00001838
Iteration 13/1000 | Loss: 0.00001825
Iteration 14/1000 | Loss: 0.00001815
Iteration 15/1000 | Loss: 0.00001811
Iteration 16/1000 | Loss: 0.00001809
Iteration 17/1000 | Loss: 0.00001809
Iteration 18/1000 | Loss: 0.00001808
Iteration 19/1000 | Loss: 0.00001808
Iteration 20/1000 | Loss: 0.00001807
Iteration 21/1000 | Loss: 0.00001802
Iteration 22/1000 | Loss: 0.00001802
Iteration 23/1000 | Loss: 0.00001799
Iteration 24/1000 | Loss: 0.00001798
Iteration 25/1000 | Loss: 0.00001798
Iteration 26/1000 | Loss: 0.00001797
Iteration 27/1000 | Loss: 0.00001796
Iteration 28/1000 | Loss: 0.00001792
Iteration 29/1000 | Loss: 0.00001791
Iteration 30/1000 | Loss: 0.00001790
Iteration 31/1000 | Loss: 0.00001788
Iteration 32/1000 | Loss: 0.00001787
Iteration 33/1000 | Loss: 0.00001787
Iteration 34/1000 | Loss: 0.00001787
Iteration 35/1000 | Loss: 0.00001786
Iteration 36/1000 | Loss: 0.00001786
Iteration 37/1000 | Loss: 0.00001786
Iteration 38/1000 | Loss: 0.00001786
Iteration 39/1000 | Loss: 0.00001785
Iteration 40/1000 | Loss: 0.00001785
Iteration 41/1000 | Loss: 0.00001784
Iteration 42/1000 | Loss: 0.00001784
Iteration 43/1000 | Loss: 0.00001784
Iteration 44/1000 | Loss: 0.00001783
Iteration 45/1000 | Loss: 0.00001783
Iteration 46/1000 | Loss: 0.00001783
Iteration 47/1000 | Loss: 0.00001782
Iteration 48/1000 | Loss: 0.00001782
Iteration 49/1000 | Loss: 0.00001782
Iteration 50/1000 | Loss: 0.00001781
Iteration 51/1000 | Loss: 0.00001781
Iteration 52/1000 | Loss: 0.00001781
Iteration 53/1000 | Loss: 0.00001780
Iteration 54/1000 | Loss: 0.00001780
Iteration 55/1000 | Loss: 0.00001779
Iteration 56/1000 | Loss: 0.00001779
Iteration 57/1000 | Loss: 0.00001778
Iteration 58/1000 | Loss: 0.00001778
Iteration 59/1000 | Loss: 0.00001778
Iteration 60/1000 | Loss: 0.00001778
Iteration 61/1000 | Loss: 0.00001778
Iteration 62/1000 | Loss: 0.00001778
Iteration 63/1000 | Loss: 0.00001778
Iteration 64/1000 | Loss: 0.00001778
Iteration 65/1000 | Loss: 0.00001777
Iteration 66/1000 | Loss: 0.00001777
Iteration 67/1000 | Loss: 0.00001777
Iteration 68/1000 | Loss: 0.00001777
Iteration 69/1000 | Loss: 0.00001777
Iteration 70/1000 | Loss: 0.00001777
Iteration 71/1000 | Loss: 0.00001777
Iteration 72/1000 | Loss: 0.00001776
Iteration 73/1000 | Loss: 0.00001776
Iteration 74/1000 | Loss: 0.00001776
Iteration 75/1000 | Loss: 0.00001776
Iteration 76/1000 | Loss: 0.00001775
Iteration 77/1000 | Loss: 0.00001775
Iteration 78/1000 | Loss: 0.00001775
Iteration 79/1000 | Loss: 0.00001775
Iteration 80/1000 | Loss: 0.00001774
Iteration 81/1000 | Loss: 0.00001774
Iteration 82/1000 | Loss: 0.00001774
Iteration 83/1000 | Loss: 0.00001774
Iteration 84/1000 | Loss: 0.00001774
Iteration 85/1000 | Loss: 0.00001774
Iteration 86/1000 | Loss: 0.00001774
Iteration 87/1000 | Loss: 0.00001774
Iteration 88/1000 | Loss: 0.00001774
Iteration 89/1000 | Loss: 0.00001773
Iteration 90/1000 | Loss: 0.00001773
Iteration 91/1000 | Loss: 0.00001773
Iteration 92/1000 | Loss: 0.00001773
Iteration 93/1000 | Loss: 0.00001773
Iteration 94/1000 | Loss: 0.00001773
Iteration 95/1000 | Loss: 0.00001773
Iteration 96/1000 | Loss: 0.00001772
Iteration 97/1000 | Loss: 0.00001772
Iteration 98/1000 | Loss: 0.00001772
Iteration 99/1000 | Loss: 0.00001772
Iteration 100/1000 | Loss: 0.00001772
Iteration 101/1000 | Loss: 0.00001772
Iteration 102/1000 | Loss: 0.00001772
Iteration 103/1000 | Loss: 0.00001772
Iteration 104/1000 | Loss: 0.00001772
Iteration 105/1000 | Loss: 0.00001772
Iteration 106/1000 | Loss: 0.00001772
Iteration 107/1000 | Loss: 0.00001772
Iteration 108/1000 | Loss: 0.00001772
Iteration 109/1000 | Loss: 0.00001772
Iteration 110/1000 | Loss: 0.00001772
Iteration 111/1000 | Loss: 0.00001772
Iteration 112/1000 | Loss: 0.00001772
Iteration 113/1000 | Loss: 0.00001772
Iteration 114/1000 | Loss: 0.00001772
Iteration 115/1000 | Loss: 0.00001772
Iteration 116/1000 | Loss: 0.00001772
Iteration 117/1000 | Loss: 0.00001772
Iteration 118/1000 | Loss: 0.00001772
Iteration 119/1000 | Loss: 0.00001772
Iteration 120/1000 | Loss: 0.00001772
Iteration 121/1000 | Loss: 0.00001772
Iteration 122/1000 | Loss: 0.00001772
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 122. Stopping optimization.
Last 5 losses: [1.7717919035931118e-05, 1.7717919035931118e-05, 1.7717919035931118e-05, 1.7717919035931118e-05, 1.7717919035931118e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7717919035931118e-05

Optimization complete. Final v2v error: 3.6169464588165283 mm

Highest mean error: 4.216072082519531 mm for frame 67

Lowest mean error: 3.211986780166626 mm for frame 182

Saving results

Total time: 56.34701919555664
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_003/1035/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_003/1035.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_003/1035
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00989395
Iteration 2/25 | Loss: 0.00989394
Iteration 3/25 | Loss: 0.00249577
Iteration 4/25 | Loss: 0.00200886
Iteration 5/25 | Loss: 0.00192267
Iteration 6/25 | Loss: 0.00189591
Iteration 7/25 | Loss: 0.00181462
Iteration 8/25 | Loss: 0.00180570
Iteration 9/25 | Loss: 0.00174302
Iteration 10/25 | Loss: 0.00172647
Iteration 11/25 | Loss: 0.00173120
Iteration 12/25 | Loss: 0.00170602
Iteration 13/25 | Loss: 0.00168184
Iteration 14/25 | Loss: 0.00168027
Iteration 15/25 | Loss: 0.00166773
Iteration 16/25 | Loss: 0.00166611
Iteration 17/25 | Loss: 0.00166590
Iteration 18/25 | Loss: 0.00166644
Iteration 19/25 | Loss: 0.00166267
Iteration 20/25 | Loss: 0.00166251
Iteration 21/25 | Loss: 0.00166181
Iteration 22/25 | Loss: 0.00166189
Iteration 23/25 | Loss: 0.00166185
Iteration 24/25 | Loss: 0.00166136
Iteration 25/25 | Loss: 0.00166218

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.27992857
Iteration 2/25 | Loss: 0.00477247
Iteration 3/25 | Loss: 0.00418880
Iteration 4/25 | Loss: 0.00418880
Iteration 5/25 | Loss: 0.00418880
Iteration 6/25 | Loss: 0.00418880
Iteration 7/25 | Loss: 0.00418879
Iteration 8/25 | Loss: 0.00418879
Iteration 9/25 | Loss: 0.00418879
Iteration 10/25 | Loss: 0.00418879
Iteration 11/25 | Loss: 0.00418879
Iteration 12/25 | Loss: 0.00418879
Iteration 13/25 | Loss: 0.00418879
Iteration 14/25 | Loss: 0.00418879
Iteration 15/25 | Loss: 0.00418879
Iteration 16/25 | Loss: 0.00418879
Iteration 17/25 | Loss: 0.00418879
Iteration 18/25 | Loss: 0.00418879
Iteration 19/25 | Loss: 0.00418879
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.004188792780041695, 0.004188792780041695, 0.004188792780041695, 0.004188792780041695, 0.004188792780041695]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.004188792780041695

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00418879
Iteration 2/1000 | Loss: 0.00089959
Iteration 3/1000 | Loss: 0.00040678
Iteration 4/1000 | Loss: 0.00039892
Iteration 5/1000 | Loss: 0.00038475
Iteration 6/1000 | Loss: 0.00030481
Iteration 7/1000 | Loss: 0.00027913
Iteration 8/1000 | Loss: 0.00046173
Iteration 9/1000 | Loss: 0.00033102
Iteration 10/1000 | Loss: 0.00026094
Iteration 11/1000 | Loss: 0.00025890
Iteration 12/1000 | Loss: 0.00024931
Iteration 13/1000 | Loss: 0.00024183
Iteration 14/1000 | Loss: 0.00022885
Iteration 15/1000 | Loss: 0.00023467
Iteration 16/1000 | Loss: 0.00022110
Iteration 17/1000 | Loss: 0.00021647
Iteration 18/1000 | Loss: 0.00033160
Iteration 19/1000 | Loss: 0.00036071
Iteration 20/1000 | Loss: 0.00094116
Iteration 21/1000 | Loss: 0.00565772
Iteration 22/1000 | Loss: 0.00360005
Iteration 23/1000 | Loss: 0.00860315
Iteration 24/1000 | Loss: 0.00488613
Iteration 25/1000 | Loss: 0.00144952
Iteration 26/1000 | Loss: 0.00091769
Iteration 27/1000 | Loss: 0.00080469
Iteration 28/1000 | Loss: 0.00070369
Iteration 29/1000 | Loss: 0.00059965
Iteration 30/1000 | Loss: 0.00036460
Iteration 31/1000 | Loss: 0.00028557
Iteration 32/1000 | Loss: 0.00023301
Iteration 33/1000 | Loss: 0.00023478
Iteration 34/1000 | Loss: 0.00025628
Iteration 35/1000 | Loss: 0.00013349
Iteration 36/1000 | Loss: 0.00010716
Iteration 37/1000 | Loss: 0.00030051
Iteration 38/1000 | Loss: 0.00010919
Iteration 39/1000 | Loss: 0.00008452
Iteration 40/1000 | Loss: 0.00008826
Iteration 41/1000 | Loss: 0.00008028
Iteration 42/1000 | Loss: 0.00011816
Iteration 43/1000 | Loss: 0.00007204
Iteration 44/1000 | Loss: 0.00006710
Iteration 45/1000 | Loss: 0.00007144
Iteration 46/1000 | Loss: 0.00006348
Iteration 47/1000 | Loss: 0.00006006
Iteration 48/1000 | Loss: 0.00005846
Iteration 49/1000 | Loss: 0.00005718
Iteration 50/1000 | Loss: 0.00005633
Iteration 51/1000 | Loss: 0.00005561
Iteration 52/1000 | Loss: 0.00005481
Iteration 53/1000 | Loss: 0.00005441
Iteration 54/1000 | Loss: 0.00005790
Iteration 55/1000 | Loss: 0.00005394
Iteration 56/1000 | Loss: 0.00005375
Iteration 57/1000 | Loss: 0.00005368
Iteration 58/1000 | Loss: 0.00005365
Iteration 59/1000 | Loss: 0.00005357
Iteration 60/1000 | Loss: 0.00005655
Iteration 61/1000 | Loss: 0.00005503
Iteration 62/1000 | Loss: 0.00005358
Iteration 63/1000 | Loss: 0.00005315
Iteration 64/1000 | Loss: 0.00005315
Iteration 65/1000 | Loss: 0.00005315
Iteration 66/1000 | Loss: 0.00005315
Iteration 67/1000 | Loss: 0.00005315
Iteration 68/1000 | Loss: 0.00005314
Iteration 69/1000 | Loss: 0.00005314
Iteration 70/1000 | Loss: 0.00005314
Iteration 71/1000 | Loss: 0.00005314
Iteration 72/1000 | Loss: 0.00005314
Iteration 73/1000 | Loss: 0.00005314
Iteration 74/1000 | Loss: 0.00005313
Iteration 75/1000 | Loss: 0.00005301
Iteration 76/1000 | Loss: 0.00005280
Iteration 77/1000 | Loss: 0.00021690
Iteration 78/1000 | Loss: 0.00005787
Iteration 79/1000 | Loss: 0.00005385
Iteration 80/1000 | Loss: 0.00005273
Iteration 81/1000 | Loss: 0.00005218
Iteration 82/1000 | Loss: 0.00005193
Iteration 83/1000 | Loss: 0.00005175
Iteration 84/1000 | Loss: 0.00005169
Iteration 85/1000 | Loss: 0.00005165
Iteration 86/1000 | Loss: 0.00005164
Iteration 87/1000 | Loss: 0.00005163
Iteration 88/1000 | Loss: 0.00005162
Iteration 89/1000 | Loss: 0.00005162
Iteration 90/1000 | Loss: 0.00005162
Iteration 91/1000 | Loss: 0.00005162
Iteration 92/1000 | Loss: 0.00005162
Iteration 93/1000 | Loss: 0.00005162
Iteration 94/1000 | Loss: 0.00005162
Iteration 95/1000 | Loss: 0.00005162
Iteration 96/1000 | Loss: 0.00005162
Iteration 97/1000 | Loss: 0.00005162
Iteration 98/1000 | Loss: 0.00005162
Iteration 99/1000 | Loss: 0.00005162
Iteration 100/1000 | Loss: 0.00005162
Iteration 101/1000 | Loss: 0.00005162
Iteration 102/1000 | Loss: 0.00005161
Iteration 103/1000 | Loss: 0.00005161
Iteration 104/1000 | Loss: 0.00005161
Iteration 105/1000 | Loss: 0.00005161
Iteration 106/1000 | Loss: 0.00005161
Iteration 107/1000 | Loss: 0.00005161
Iteration 108/1000 | Loss: 0.00005161
Iteration 109/1000 | Loss: 0.00005161
Iteration 110/1000 | Loss: 0.00005161
Iteration 111/1000 | Loss: 0.00005161
Iteration 112/1000 | Loss: 0.00005161
Iteration 113/1000 | Loss: 0.00005161
Iteration 114/1000 | Loss: 0.00005161
Iteration 115/1000 | Loss: 0.00005161
Iteration 116/1000 | Loss: 0.00005161
Iteration 117/1000 | Loss: 0.00005160
Iteration 118/1000 | Loss: 0.00005160
Iteration 119/1000 | Loss: 0.00005160
Iteration 120/1000 | Loss: 0.00005160
Iteration 121/1000 | Loss: 0.00005160
Iteration 122/1000 | Loss: 0.00005160
Iteration 123/1000 | Loss: 0.00005160
Iteration 124/1000 | Loss: 0.00005160
Iteration 125/1000 | Loss: 0.00005160
Iteration 126/1000 | Loss: 0.00005160
Iteration 127/1000 | Loss: 0.00005160
Iteration 128/1000 | Loss: 0.00005160
Iteration 129/1000 | Loss: 0.00005159
Iteration 130/1000 | Loss: 0.00005159
Iteration 131/1000 | Loss: 0.00005159
Iteration 132/1000 | Loss: 0.00005159
Iteration 133/1000 | Loss: 0.00005159
Iteration 134/1000 | Loss: 0.00005159
Iteration 135/1000 | Loss: 0.00005159
Iteration 136/1000 | Loss: 0.00005159
Iteration 137/1000 | Loss: 0.00005159
Iteration 138/1000 | Loss: 0.00005159
Iteration 139/1000 | Loss: 0.00005159
Iteration 140/1000 | Loss: 0.00005159
Iteration 141/1000 | Loss: 0.00005159
Iteration 142/1000 | Loss: 0.00005159
Iteration 143/1000 | Loss: 0.00005159
Iteration 144/1000 | Loss: 0.00005158
Iteration 145/1000 | Loss: 0.00005158
Iteration 146/1000 | Loss: 0.00005158
Iteration 147/1000 | Loss: 0.00005158
Iteration 148/1000 | Loss: 0.00005158
Iteration 149/1000 | Loss: 0.00005158
Iteration 150/1000 | Loss: 0.00005158
Iteration 151/1000 | Loss: 0.00005158
Iteration 152/1000 | Loss: 0.00005157
Iteration 153/1000 | Loss: 0.00005157
Iteration 154/1000 | Loss: 0.00005157
Iteration 155/1000 | Loss: 0.00005157
Iteration 156/1000 | Loss: 0.00005157
Iteration 157/1000 | Loss: 0.00005157
Iteration 158/1000 | Loss: 0.00005157
Iteration 159/1000 | Loss: 0.00005157
Iteration 160/1000 | Loss: 0.00005157
Iteration 161/1000 | Loss: 0.00005157
Iteration 162/1000 | Loss: 0.00005157
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 162. Stopping optimization.
Last 5 losses: [5.157266787136905e-05, 5.157266787136905e-05, 5.157266787136905e-05, 5.157266787136905e-05, 5.157266787136905e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 5.157266787136905e-05

Optimization complete. Final v2v error: 3.8036997318267822 mm

Highest mean error: 10.79397201538086 mm for frame 152

Lowest mean error: 2.8652379512786865 mm for frame 206

Saving results

Total time: 170.17134642601013
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_003/1093/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_003/1093.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_003/1093
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00724719
Iteration 2/25 | Loss: 0.00140020
Iteration 3/25 | Loss: 0.00126868
Iteration 4/25 | Loss: 0.00121484
Iteration 5/25 | Loss: 0.00120125
Iteration 6/25 | Loss: 0.00119278
Iteration 7/25 | Loss: 0.00119124
Iteration 8/25 | Loss: 0.00118782
Iteration 9/25 | Loss: 0.00118673
Iteration 10/25 | Loss: 0.00118973
Iteration 11/25 | Loss: 0.00119154
Iteration 12/25 | Loss: 0.00118927
Iteration 13/25 | Loss: 0.00118922
Iteration 14/25 | Loss: 0.00119056
Iteration 15/25 | Loss: 0.00118965
Iteration 16/25 | Loss: 0.00119056
Iteration 17/25 | Loss: 0.00118993
Iteration 18/25 | Loss: 0.00118843
Iteration 19/25 | Loss: 0.00118676
Iteration 20/25 | Loss: 0.00118581
Iteration 21/25 | Loss: 0.00118532
Iteration 22/25 | Loss: 0.00118527
Iteration 23/25 | Loss: 0.00118526
Iteration 24/25 | Loss: 0.00118526
Iteration 25/25 | Loss: 0.00118526

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.67031121
Iteration 2/25 | Loss: 0.00177693
Iteration 3/25 | Loss: 0.00177689
Iteration 4/25 | Loss: 0.00177689
Iteration 5/25 | Loss: 0.00177688
Iteration 6/25 | Loss: 0.00177688
Iteration 7/25 | Loss: 0.00177688
Iteration 8/25 | Loss: 0.00177688
Iteration 9/25 | Loss: 0.00177688
Iteration 10/25 | Loss: 0.00177688
Iteration 11/25 | Loss: 0.00177688
Iteration 12/25 | Loss: 0.00177688
Iteration 13/25 | Loss: 0.00177688
Iteration 14/25 | Loss: 0.00177688
Iteration 15/25 | Loss: 0.00177688
Iteration 16/25 | Loss: 0.00177688
Iteration 17/25 | Loss: 0.00177688
Iteration 18/25 | Loss: 0.00177688
Iteration 19/25 | Loss: 0.00177688
Iteration 20/25 | Loss: 0.00177688
Iteration 21/25 | Loss: 0.00177688
Iteration 22/25 | Loss: 0.00177688
Iteration 23/25 | Loss: 0.00177688
Iteration 24/25 | Loss: 0.00177688
Iteration 25/25 | Loss: 0.00177688

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00177688
Iteration 2/1000 | Loss: 0.00006961
Iteration 3/1000 | Loss: 0.00012579
Iteration 4/1000 | Loss: 0.00002237
Iteration 5/1000 | Loss: 0.00001990
Iteration 6/1000 | Loss: 0.00006859
Iteration 7/1000 | Loss: 0.00010981
Iteration 8/1000 | Loss: 0.00009256
Iteration 9/1000 | Loss: 0.00013718
Iteration 10/1000 | Loss: 0.00013093
Iteration 11/1000 | Loss: 0.00010191
Iteration 12/1000 | Loss: 0.00008689
Iteration 13/1000 | Loss: 0.00005892
Iteration 14/1000 | Loss: 0.00006954
Iteration 15/1000 | Loss: 0.00004456
Iteration 16/1000 | Loss: 0.00014200
Iteration 17/1000 | Loss: 0.00019628
Iteration 18/1000 | Loss: 0.00012631
Iteration 19/1000 | Loss: 0.00010356
Iteration 20/1000 | Loss: 0.00002453
Iteration 21/1000 | Loss: 0.00002111
Iteration 22/1000 | Loss: 0.00020618
Iteration 23/1000 | Loss: 0.00015244
Iteration 24/1000 | Loss: 0.00012217
Iteration 25/1000 | Loss: 0.00003206
Iteration 26/1000 | Loss: 0.00015529
Iteration 27/1000 | Loss: 0.00014096
Iteration 28/1000 | Loss: 0.00021013
Iteration 29/1000 | Loss: 0.00002577
Iteration 30/1000 | Loss: 0.00001680
Iteration 31/1000 | Loss: 0.00001543
Iteration 32/1000 | Loss: 0.00001493
Iteration 33/1000 | Loss: 0.00001460
Iteration 34/1000 | Loss: 0.00001440
Iteration 35/1000 | Loss: 0.00001415
Iteration 36/1000 | Loss: 0.00001397
Iteration 37/1000 | Loss: 0.00001395
Iteration 38/1000 | Loss: 0.00001395
Iteration 39/1000 | Loss: 0.00001394
Iteration 40/1000 | Loss: 0.00001394
Iteration 41/1000 | Loss: 0.00001380
Iteration 42/1000 | Loss: 0.00001364
Iteration 43/1000 | Loss: 0.00001361
Iteration 44/1000 | Loss: 0.00001353
Iteration 45/1000 | Loss: 0.00001351
Iteration 46/1000 | Loss: 0.00001349
Iteration 47/1000 | Loss: 0.00001343
Iteration 48/1000 | Loss: 0.00001343
Iteration 49/1000 | Loss: 0.00001341
Iteration 50/1000 | Loss: 0.00001341
Iteration 51/1000 | Loss: 0.00001341
Iteration 52/1000 | Loss: 0.00001341
Iteration 53/1000 | Loss: 0.00001340
Iteration 54/1000 | Loss: 0.00001340
Iteration 55/1000 | Loss: 0.00001340
Iteration 56/1000 | Loss: 0.00001340
Iteration 57/1000 | Loss: 0.00001339
Iteration 58/1000 | Loss: 0.00001339
Iteration 59/1000 | Loss: 0.00001339
Iteration 60/1000 | Loss: 0.00001338
Iteration 61/1000 | Loss: 0.00001338
Iteration 62/1000 | Loss: 0.00001338
Iteration 63/1000 | Loss: 0.00001337
Iteration 64/1000 | Loss: 0.00001336
Iteration 65/1000 | Loss: 0.00001336
Iteration 66/1000 | Loss: 0.00001333
Iteration 67/1000 | Loss: 0.00001333
Iteration 68/1000 | Loss: 0.00001332
Iteration 69/1000 | Loss: 0.00001332
Iteration 70/1000 | Loss: 0.00001332
Iteration 71/1000 | Loss: 0.00001331
Iteration 72/1000 | Loss: 0.00001329
Iteration 73/1000 | Loss: 0.00001329
Iteration 74/1000 | Loss: 0.00001329
Iteration 75/1000 | Loss: 0.00001328
Iteration 76/1000 | Loss: 0.00001328
Iteration 77/1000 | Loss: 0.00001327
Iteration 78/1000 | Loss: 0.00001327
Iteration 79/1000 | Loss: 0.00001327
Iteration 80/1000 | Loss: 0.00001327
Iteration 81/1000 | Loss: 0.00001327
Iteration 82/1000 | Loss: 0.00001327
Iteration 83/1000 | Loss: 0.00001327
Iteration 84/1000 | Loss: 0.00001327
Iteration 85/1000 | Loss: 0.00001327
Iteration 86/1000 | Loss: 0.00001326
Iteration 87/1000 | Loss: 0.00001326
Iteration 88/1000 | Loss: 0.00001326
Iteration 89/1000 | Loss: 0.00001326
Iteration 90/1000 | Loss: 0.00001326
Iteration 91/1000 | Loss: 0.00001326
Iteration 92/1000 | Loss: 0.00001325
Iteration 93/1000 | Loss: 0.00001325
Iteration 94/1000 | Loss: 0.00001325
Iteration 95/1000 | Loss: 0.00001325
Iteration 96/1000 | Loss: 0.00001325
Iteration 97/1000 | Loss: 0.00001325
Iteration 98/1000 | Loss: 0.00001325
Iteration 99/1000 | Loss: 0.00001325
Iteration 100/1000 | Loss: 0.00001325
Iteration 101/1000 | Loss: 0.00001324
Iteration 102/1000 | Loss: 0.00001324
Iteration 103/1000 | Loss: 0.00001324
Iteration 104/1000 | Loss: 0.00001324
Iteration 105/1000 | Loss: 0.00001324
Iteration 106/1000 | Loss: 0.00001324
Iteration 107/1000 | Loss: 0.00001324
Iteration 108/1000 | Loss: 0.00001324
Iteration 109/1000 | Loss: 0.00001324
Iteration 110/1000 | Loss: 0.00001324
Iteration 111/1000 | Loss: 0.00001324
Iteration 112/1000 | Loss: 0.00001324
Iteration 113/1000 | Loss: 0.00001324
Iteration 114/1000 | Loss: 0.00001323
Iteration 115/1000 | Loss: 0.00001323
Iteration 116/1000 | Loss: 0.00001323
Iteration 117/1000 | Loss: 0.00001323
Iteration 118/1000 | Loss: 0.00001323
Iteration 119/1000 | Loss: 0.00001323
Iteration 120/1000 | Loss: 0.00001323
Iteration 121/1000 | Loss: 0.00001323
Iteration 122/1000 | Loss: 0.00001323
Iteration 123/1000 | Loss: 0.00001323
Iteration 124/1000 | Loss: 0.00001323
Iteration 125/1000 | Loss: 0.00001322
Iteration 126/1000 | Loss: 0.00001322
Iteration 127/1000 | Loss: 0.00001322
Iteration 128/1000 | Loss: 0.00001322
Iteration 129/1000 | Loss: 0.00001322
Iteration 130/1000 | Loss: 0.00001322
Iteration 131/1000 | Loss: 0.00001322
Iteration 132/1000 | Loss: 0.00001322
Iteration 133/1000 | Loss: 0.00001322
Iteration 134/1000 | Loss: 0.00001322
Iteration 135/1000 | Loss: 0.00001321
Iteration 136/1000 | Loss: 0.00001321
Iteration 137/1000 | Loss: 0.00001321
Iteration 138/1000 | Loss: 0.00001321
Iteration 139/1000 | Loss: 0.00001321
Iteration 140/1000 | Loss: 0.00001321
Iteration 141/1000 | Loss: 0.00001321
Iteration 142/1000 | Loss: 0.00001321
Iteration 143/1000 | Loss: 0.00001321
Iteration 144/1000 | Loss: 0.00001321
Iteration 145/1000 | Loss: 0.00001321
Iteration 146/1000 | Loss: 0.00001320
Iteration 147/1000 | Loss: 0.00001320
Iteration 148/1000 | Loss: 0.00001320
Iteration 149/1000 | Loss: 0.00001320
Iteration 150/1000 | Loss: 0.00001320
Iteration 151/1000 | Loss: 0.00001320
Iteration 152/1000 | Loss: 0.00001320
Iteration 153/1000 | Loss: 0.00001320
Iteration 154/1000 | Loss: 0.00001319
Iteration 155/1000 | Loss: 0.00001319
Iteration 156/1000 | Loss: 0.00001319
Iteration 157/1000 | Loss: 0.00001319
Iteration 158/1000 | Loss: 0.00001319
Iteration 159/1000 | Loss: 0.00001319
Iteration 160/1000 | Loss: 0.00001319
Iteration 161/1000 | Loss: 0.00001319
Iteration 162/1000 | Loss: 0.00001318
Iteration 163/1000 | Loss: 0.00001318
Iteration 164/1000 | Loss: 0.00001318
Iteration 165/1000 | Loss: 0.00001318
Iteration 166/1000 | Loss: 0.00001317
Iteration 167/1000 | Loss: 0.00001317
Iteration 168/1000 | Loss: 0.00001317
Iteration 169/1000 | Loss: 0.00001317
Iteration 170/1000 | Loss: 0.00001316
Iteration 171/1000 | Loss: 0.00001316
Iteration 172/1000 | Loss: 0.00001316
Iteration 173/1000 | Loss: 0.00001316
Iteration 174/1000 | Loss: 0.00001316
Iteration 175/1000 | Loss: 0.00001316
Iteration 176/1000 | Loss: 0.00001316
Iteration 177/1000 | Loss: 0.00001316
Iteration 178/1000 | Loss: 0.00001316
Iteration 179/1000 | Loss: 0.00001316
Iteration 180/1000 | Loss: 0.00001316
Iteration 181/1000 | Loss: 0.00001316
Iteration 182/1000 | Loss: 0.00001315
Iteration 183/1000 | Loss: 0.00001315
Iteration 184/1000 | Loss: 0.00001315
Iteration 185/1000 | Loss: 0.00001315
Iteration 186/1000 | Loss: 0.00001315
Iteration 187/1000 | Loss: 0.00001315
Iteration 188/1000 | Loss: 0.00001315
Iteration 189/1000 | Loss: 0.00001315
Iteration 190/1000 | Loss: 0.00001315
Iteration 191/1000 | Loss: 0.00001315
Iteration 192/1000 | Loss: 0.00001315
Iteration 193/1000 | Loss: 0.00001315
Iteration 194/1000 | Loss: 0.00001315
Iteration 195/1000 | Loss: 0.00001315
Iteration 196/1000 | Loss: 0.00001315
Iteration 197/1000 | Loss: 0.00001315
Iteration 198/1000 | Loss: 0.00001315
Iteration 199/1000 | Loss: 0.00001315
Iteration 200/1000 | Loss: 0.00001315
Iteration 201/1000 | Loss: 0.00001314
Iteration 202/1000 | Loss: 0.00001314
Iteration 203/1000 | Loss: 0.00001314
Iteration 204/1000 | Loss: 0.00001314
Iteration 205/1000 | Loss: 0.00001314
Iteration 206/1000 | Loss: 0.00001314
Iteration 207/1000 | Loss: 0.00001314
Iteration 208/1000 | Loss: 0.00001314
Iteration 209/1000 | Loss: 0.00001314
Iteration 210/1000 | Loss: 0.00001314
Iteration 211/1000 | Loss: 0.00001314
Iteration 212/1000 | Loss: 0.00001314
Iteration 213/1000 | Loss: 0.00001314
Iteration 214/1000 | Loss: 0.00001313
Iteration 215/1000 | Loss: 0.00001313
Iteration 216/1000 | Loss: 0.00001313
Iteration 217/1000 | Loss: 0.00001313
Iteration 218/1000 | Loss: 0.00001313
Iteration 219/1000 | Loss: 0.00001313
Iteration 220/1000 | Loss: 0.00001313
Iteration 221/1000 | Loss: 0.00001313
Iteration 222/1000 | Loss: 0.00001313
Iteration 223/1000 | Loss: 0.00001313
Iteration 224/1000 | Loss: 0.00001313
Iteration 225/1000 | Loss: 0.00001313
Iteration 226/1000 | Loss: 0.00001313
Iteration 227/1000 | Loss: 0.00001313
Iteration 228/1000 | Loss: 0.00001313
Iteration 229/1000 | Loss: 0.00001313
Iteration 230/1000 | Loss: 0.00001313
Iteration 231/1000 | Loss: 0.00001313
Iteration 232/1000 | Loss: 0.00001313
Iteration 233/1000 | Loss: 0.00001313
Iteration 234/1000 | Loss: 0.00001313
Iteration 235/1000 | Loss: 0.00001313
Iteration 236/1000 | Loss: 0.00001313
Iteration 237/1000 | Loss: 0.00001313
Iteration 238/1000 | Loss: 0.00001313
Iteration 239/1000 | Loss: 0.00001313
Iteration 240/1000 | Loss: 0.00001313
Iteration 241/1000 | Loss: 0.00001313
Iteration 242/1000 | Loss: 0.00001313
Iteration 243/1000 | Loss: 0.00001313
Iteration 244/1000 | Loss: 0.00001313
Iteration 245/1000 | Loss: 0.00001313
Iteration 246/1000 | Loss: 0.00001313
Iteration 247/1000 | Loss: 0.00001313
Iteration 248/1000 | Loss: 0.00001313
Iteration 249/1000 | Loss: 0.00001313
Iteration 250/1000 | Loss: 0.00001313
Iteration 251/1000 | Loss: 0.00001313
Iteration 252/1000 | Loss: 0.00001313
Iteration 253/1000 | Loss: 0.00001313
Iteration 254/1000 | Loss: 0.00001313
Iteration 255/1000 | Loss: 0.00001313
Iteration 256/1000 | Loss: 0.00001313
Iteration 257/1000 | Loss: 0.00001313
Iteration 258/1000 | Loss: 0.00001313
Iteration 259/1000 | Loss: 0.00001313
Iteration 260/1000 | Loss: 0.00001313
Iteration 261/1000 | Loss: 0.00001313
Iteration 262/1000 | Loss: 0.00001313
Iteration 263/1000 | Loss: 0.00001313
Iteration 264/1000 | Loss: 0.00001313
Iteration 265/1000 | Loss: 0.00001313
Iteration 266/1000 | Loss: 0.00001313
Iteration 267/1000 | Loss: 0.00001313
Iteration 268/1000 | Loss: 0.00001313
Iteration 269/1000 | Loss: 0.00001313
Iteration 270/1000 | Loss: 0.00001313
Iteration 271/1000 | Loss: 0.00001313
Iteration 272/1000 | Loss: 0.00001313
Iteration 273/1000 | Loss: 0.00001313
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 273. Stopping optimization.
Last 5 losses: [1.3129557373758871e-05, 1.3129557373758871e-05, 1.3129557373758871e-05, 1.3129557373758871e-05, 1.3129557373758871e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3129557373758871e-05

Optimization complete. Final v2v error: 3.1116557121276855 mm

Highest mean error: 3.8560616970062256 mm for frame 20

Lowest mean error: 2.8038330078125 mm for frame 126

Saving results

Total time: 109.68487358093262
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_003/1098/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_003/1098.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_003/1098
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01075352
Iteration 2/25 | Loss: 0.01075352
Iteration 3/25 | Loss: 0.00346018
Iteration 4/25 | Loss: 0.00286678
Iteration 5/25 | Loss: 0.00266384
Iteration 6/25 | Loss: 0.00248196
Iteration 7/25 | Loss: 0.00225484
Iteration 8/25 | Loss: 0.00214825
Iteration 9/25 | Loss: 0.00204889
Iteration 10/25 | Loss: 0.00196974
Iteration 11/25 | Loss: 0.00190263
Iteration 12/25 | Loss: 0.00182087
Iteration 13/25 | Loss: 0.00178520
Iteration 14/25 | Loss: 0.00173628
Iteration 15/25 | Loss: 0.00169413
Iteration 16/25 | Loss: 0.00166247
Iteration 17/25 | Loss: 0.00161619
Iteration 18/25 | Loss: 0.00160740
Iteration 19/25 | Loss: 0.00159397
Iteration 20/25 | Loss: 0.00156963
Iteration 21/25 | Loss: 0.00156327
Iteration 22/25 | Loss: 0.00154805
Iteration 23/25 | Loss: 0.00154501
Iteration 24/25 | Loss: 0.00153746
Iteration 25/25 | Loss: 0.00152761

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.13897264
Iteration 2/25 | Loss: 0.00273681
Iteration 3/25 | Loss: 0.00273680
Iteration 4/25 | Loss: 0.00273680
Iteration 5/25 | Loss: 0.00273679
Iteration 6/25 | Loss: 0.00273679
Iteration 7/25 | Loss: 0.00273679
Iteration 8/25 | Loss: 0.00273679
Iteration 9/25 | Loss: 0.00273679
Iteration 10/25 | Loss: 0.00273679
Iteration 11/25 | Loss: 0.00273679
Iteration 12/25 | Loss: 0.00273679
Iteration 13/25 | Loss: 0.00273679
Iteration 14/25 | Loss: 0.00273679
Iteration 15/25 | Loss: 0.00273679
Iteration 16/25 | Loss: 0.00273679
Iteration 17/25 | Loss: 0.00273679
Iteration 18/25 | Loss: 0.00273679
Iteration 19/25 | Loss: 0.00273679
Iteration 20/25 | Loss: 0.00273679
Iteration 21/25 | Loss: 0.00273679
Iteration 22/25 | Loss: 0.00273679
Iteration 23/25 | Loss: 0.00273679
Iteration 24/25 | Loss: 0.00273679
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0027367917355149984, 0.0027367917355149984, 0.0027367917355149984, 0.0027367917355149984, 0.0027367917355149984]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0027367917355149984

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00273679
Iteration 2/1000 | Loss: 0.00040630
Iteration 3/1000 | Loss: 0.00049442
Iteration 4/1000 | Loss: 0.00055421
Iteration 5/1000 | Loss: 0.00069654
Iteration 6/1000 | Loss: 0.00078515
Iteration 7/1000 | Loss: 0.00053460
Iteration 8/1000 | Loss: 0.00027654
Iteration 9/1000 | Loss: 0.00032329
Iteration 10/1000 | Loss: 0.00042341
Iteration 11/1000 | Loss: 0.00035750
Iteration 12/1000 | Loss: 0.00031589
Iteration 13/1000 | Loss: 0.00031919
Iteration 14/1000 | Loss: 0.00032656
Iteration 15/1000 | Loss: 0.00045152
Iteration 16/1000 | Loss: 0.00023818
Iteration 17/1000 | Loss: 0.00027299
Iteration 18/1000 | Loss: 0.00062345
Iteration 19/1000 | Loss: 0.00048867
Iteration 20/1000 | Loss: 0.00046940
Iteration 21/1000 | Loss: 0.00048949
Iteration 22/1000 | Loss: 0.00052741
Iteration 23/1000 | Loss: 0.00027828
Iteration 24/1000 | Loss: 0.00032888
Iteration 25/1000 | Loss: 0.00035410
Iteration 26/1000 | Loss: 0.00032135
Iteration 27/1000 | Loss: 0.00067411
Iteration 28/1000 | Loss: 0.00045981
Iteration 29/1000 | Loss: 0.00048801
Iteration 30/1000 | Loss: 0.00033411
Iteration 31/1000 | Loss: 0.00035766
Iteration 32/1000 | Loss: 0.00042104
Iteration 33/1000 | Loss: 0.00027140
Iteration 34/1000 | Loss: 0.00058228
Iteration 35/1000 | Loss: 0.00064570
Iteration 36/1000 | Loss: 0.00049999
Iteration 37/1000 | Loss: 0.00041317
Iteration 38/1000 | Loss: 0.00035381
Iteration 39/1000 | Loss: 0.00030823
Iteration 40/1000 | Loss: 0.00034589
Iteration 41/1000 | Loss: 0.00044330
Iteration 42/1000 | Loss: 0.00028616
Iteration 43/1000 | Loss: 0.00080529
Iteration 44/1000 | Loss: 0.00031719
Iteration 45/1000 | Loss: 0.00025161
Iteration 46/1000 | Loss: 0.00026066
Iteration 47/1000 | Loss: 0.00038307
Iteration 48/1000 | Loss: 0.00040990
Iteration 49/1000 | Loss: 0.00041083
Iteration 50/1000 | Loss: 0.00033099
Iteration 51/1000 | Loss: 0.00016848
Iteration 52/1000 | Loss: 0.00016149
Iteration 53/1000 | Loss: 0.00014983
Iteration 54/1000 | Loss: 0.00016357
Iteration 55/1000 | Loss: 0.00016363
Iteration 56/1000 | Loss: 0.00027342
Iteration 57/1000 | Loss: 0.00038274
Iteration 58/1000 | Loss: 0.00035963
Iteration 59/1000 | Loss: 0.00037930
Iteration 60/1000 | Loss: 0.00049237
Iteration 61/1000 | Loss: 0.00038974
Iteration 62/1000 | Loss: 0.00028939
Iteration 63/1000 | Loss: 0.00018772
Iteration 64/1000 | Loss: 0.00044353
Iteration 65/1000 | Loss: 0.00033009
Iteration 66/1000 | Loss: 0.00030601
Iteration 67/1000 | Loss: 0.00031364
Iteration 68/1000 | Loss: 0.00046864
Iteration 69/1000 | Loss: 0.00053338
Iteration 70/1000 | Loss: 0.00056124
Iteration 71/1000 | Loss: 0.00024323
Iteration 72/1000 | Loss: 0.00021241
Iteration 73/1000 | Loss: 0.00038373
Iteration 74/1000 | Loss: 0.00045430
Iteration 75/1000 | Loss: 0.00026357
Iteration 76/1000 | Loss: 0.00057854
Iteration 77/1000 | Loss: 0.00060749
Iteration 78/1000 | Loss: 0.00032454
Iteration 79/1000 | Loss: 0.00063434
Iteration 80/1000 | Loss: 0.00032640
Iteration 81/1000 | Loss: 0.00040119
Iteration 82/1000 | Loss: 0.00053385
Iteration 83/1000 | Loss: 0.00061863
Iteration 84/1000 | Loss: 0.00059231
Iteration 85/1000 | Loss: 0.00041158
Iteration 86/1000 | Loss: 0.00039427
Iteration 87/1000 | Loss: 0.00046501
Iteration 88/1000 | Loss: 0.00055881
Iteration 89/1000 | Loss: 0.00037685
Iteration 90/1000 | Loss: 0.00025249
Iteration 91/1000 | Loss: 0.00027009
Iteration 92/1000 | Loss: 0.00027872
Iteration 93/1000 | Loss: 0.00027414
Iteration 94/1000 | Loss: 0.00025411
Iteration 95/1000 | Loss: 0.00070700
Iteration 96/1000 | Loss: 0.00071824
Iteration 97/1000 | Loss: 0.00045067
Iteration 98/1000 | Loss: 0.00063193
Iteration 99/1000 | Loss: 0.00110549
Iteration 100/1000 | Loss: 0.00109548
Iteration 101/1000 | Loss: 0.00082568
Iteration 102/1000 | Loss: 0.00046029
Iteration 103/1000 | Loss: 0.00035454
Iteration 104/1000 | Loss: 0.00057566
Iteration 105/1000 | Loss: 0.00043518
Iteration 106/1000 | Loss: 0.00078204
Iteration 107/1000 | Loss: 0.00028251
Iteration 108/1000 | Loss: 0.00037122
Iteration 109/1000 | Loss: 0.00028502
Iteration 110/1000 | Loss: 0.00028767
Iteration 111/1000 | Loss: 0.00044082
Iteration 112/1000 | Loss: 0.00048167
Iteration 113/1000 | Loss: 0.00033860
Iteration 114/1000 | Loss: 0.00047250
Iteration 115/1000 | Loss: 0.00027432
Iteration 116/1000 | Loss: 0.00012007
Iteration 117/1000 | Loss: 0.00028211
Iteration 118/1000 | Loss: 0.00028800
Iteration 119/1000 | Loss: 0.00012803
Iteration 120/1000 | Loss: 0.00015545
Iteration 121/1000 | Loss: 0.00016779
Iteration 122/1000 | Loss: 0.00020290
Iteration 123/1000 | Loss: 0.00018511
Iteration 124/1000 | Loss: 0.00043257
Iteration 125/1000 | Loss: 0.00012494
Iteration 126/1000 | Loss: 0.00010523
Iteration 127/1000 | Loss: 0.00012964
Iteration 128/1000 | Loss: 0.00007558
Iteration 129/1000 | Loss: 0.00007032
Iteration 130/1000 | Loss: 0.00008839
Iteration 131/1000 | Loss: 0.00019257
Iteration 132/1000 | Loss: 0.00007231
Iteration 133/1000 | Loss: 0.00009197
Iteration 134/1000 | Loss: 0.00007142
Iteration 135/1000 | Loss: 0.00006586
Iteration 136/1000 | Loss: 0.00016999
Iteration 137/1000 | Loss: 0.00007412
Iteration 138/1000 | Loss: 0.00006655
Iteration 139/1000 | Loss: 0.00026595
Iteration 140/1000 | Loss: 0.00022963
Iteration 141/1000 | Loss: 0.00014143
Iteration 142/1000 | Loss: 0.00006038
Iteration 143/1000 | Loss: 0.00014589
Iteration 144/1000 | Loss: 0.00006576
Iteration 145/1000 | Loss: 0.00007234
Iteration 146/1000 | Loss: 0.00008009
Iteration 147/1000 | Loss: 0.00007861
Iteration 148/1000 | Loss: 0.00006451
Iteration 149/1000 | Loss: 0.00006244
Iteration 150/1000 | Loss: 0.00010859
Iteration 151/1000 | Loss: 0.00026853
Iteration 152/1000 | Loss: 0.00022283
Iteration 153/1000 | Loss: 0.00012936
Iteration 154/1000 | Loss: 0.00009033
Iteration 155/1000 | Loss: 0.00006755
Iteration 156/1000 | Loss: 0.00010213
Iteration 157/1000 | Loss: 0.00010030
Iteration 158/1000 | Loss: 0.00005628
Iteration 159/1000 | Loss: 0.00008767
Iteration 160/1000 | Loss: 0.00005061
Iteration 161/1000 | Loss: 0.00007668
Iteration 162/1000 | Loss: 0.00005914
Iteration 163/1000 | Loss: 0.00008063
Iteration 164/1000 | Loss: 0.00009465
Iteration 165/1000 | Loss: 0.00050211
Iteration 166/1000 | Loss: 0.00016284
Iteration 167/1000 | Loss: 0.00006068
Iteration 168/1000 | Loss: 0.00005660
Iteration 169/1000 | Loss: 0.00004618
Iteration 170/1000 | Loss: 0.00004395
Iteration 171/1000 | Loss: 0.00004297
Iteration 172/1000 | Loss: 0.00008169
Iteration 173/1000 | Loss: 0.00005354
Iteration 174/1000 | Loss: 0.00015005
Iteration 175/1000 | Loss: 0.00006207
Iteration 176/1000 | Loss: 0.00005084
Iteration 177/1000 | Loss: 0.00007510
Iteration 178/1000 | Loss: 0.00004719
Iteration 179/1000 | Loss: 0.00005473
Iteration 180/1000 | Loss: 0.00006155
Iteration 181/1000 | Loss: 0.00005398
Iteration 182/1000 | Loss: 0.00007205
Iteration 183/1000 | Loss: 0.00005255
Iteration 184/1000 | Loss: 0.00005373
Iteration 185/1000 | Loss: 0.00006522
Iteration 186/1000 | Loss: 0.00005483
Iteration 187/1000 | Loss: 0.00007295
Iteration 188/1000 | Loss: 0.00005692
Iteration 189/1000 | Loss: 0.00005550
Iteration 190/1000 | Loss: 0.00005262
Iteration 191/1000 | Loss: 0.00005401
Iteration 192/1000 | Loss: 0.00004897
Iteration 193/1000 | Loss: 0.00004093
Iteration 194/1000 | Loss: 0.00004071
Iteration 195/1000 | Loss: 0.00004068
Iteration 196/1000 | Loss: 0.00004694
Iteration 197/1000 | Loss: 0.00005252
Iteration 198/1000 | Loss: 0.00005288
Iteration 199/1000 | Loss: 0.00005326
Iteration 200/1000 | Loss: 0.00007511
Iteration 201/1000 | Loss: 0.00005787
Iteration 202/1000 | Loss: 0.00005019
Iteration 203/1000 | Loss: 0.00004793
Iteration 204/1000 | Loss: 0.00005203
Iteration 205/1000 | Loss: 0.00005340
Iteration 206/1000 | Loss: 0.00005194
Iteration 207/1000 | Loss: 0.00005326
Iteration 208/1000 | Loss: 0.00005187
Iteration 209/1000 | Loss: 0.00005308
Iteration 210/1000 | Loss: 0.00005077
Iteration 211/1000 | Loss: 0.00005235
Iteration 212/1000 | Loss: 0.00005123
Iteration 213/1000 | Loss: 0.00004368
Iteration 214/1000 | Loss: 0.00004389
Iteration 215/1000 | Loss: 0.00004388
Iteration 216/1000 | Loss: 0.00005191
Iteration 217/1000 | Loss: 0.00005171
Iteration 218/1000 | Loss: 0.00006582
Iteration 219/1000 | Loss: 0.00004401
Iteration 220/1000 | Loss: 0.00005103
Iteration 221/1000 | Loss: 0.00005589
Iteration 222/1000 | Loss: 0.00005971
Iteration 223/1000 | Loss: 0.00005828
Iteration 224/1000 | Loss: 0.00006525
Iteration 225/1000 | Loss: 0.00005083
Iteration 226/1000 | Loss: 0.00005188
Iteration 227/1000 | Loss: 0.00005315
Iteration 228/1000 | Loss: 0.00005154
Iteration 229/1000 | Loss: 0.00007141
Iteration 230/1000 | Loss: 0.00005158
Iteration 231/1000 | Loss: 0.00005537
Iteration 232/1000 | Loss: 0.00005885
Iteration 233/1000 | Loss: 0.00005270
Iteration 234/1000 | Loss: 0.00005094
Iteration 235/1000 | Loss: 0.00004298
Iteration 236/1000 | Loss: 0.00004395
Iteration 237/1000 | Loss: 0.00004334
Iteration 238/1000 | Loss: 0.00006084
Iteration 239/1000 | Loss: 0.00005264
Iteration 240/1000 | Loss: 0.00006209
Iteration 241/1000 | Loss: 0.00005195
Iteration 242/1000 | Loss: 0.00006338
Iteration 243/1000 | Loss: 0.00005175
Iteration 244/1000 | Loss: 0.00005219
Iteration 245/1000 | Loss: 0.00005213
Iteration 246/1000 | Loss: 0.00006647
Iteration 247/1000 | Loss: 0.00005360
Iteration 248/1000 | Loss: 0.00005275
Iteration 249/1000 | Loss: 0.00008011
Iteration 250/1000 | Loss: 0.00004491
Iteration 251/1000 | Loss: 0.00005235
Iteration 252/1000 | Loss: 0.00005262
Iteration 253/1000 | Loss: 0.00005273
Iteration 254/1000 | Loss: 0.00005330
Iteration 255/1000 | Loss: 0.00004492
Iteration 256/1000 | Loss: 0.00006623
Iteration 257/1000 | Loss: 0.00005423
Iteration 258/1000 | Loss: 0.00005267
Iteration 259/1000 | Loss: 0.00004972
Iteration 260/1000 | Loss: 0.00005599
Iteration 261/1000 | Loss: 0.00007247
Iteration 262/1000 | Loss: 0.00004593
Iteration 263/1000 | Loss: 0.00005433
Iteration 264/1000 | Loss: 0.00004657
Iteration 265/1000 | Loss: 0.00004723
Iteration 266/1000 | Loss: 0.00004068
Iteration 267/1000 | Loss: 0.00005914
Iteration 268/1000 | Loss: 0.00010152
Iteration 269/1000 | Loss: 0.00004418
Iteration 270/1000 | Loss: 0.00004459
Iteration 271/1000 | Loss: 0.00004325
Iteration 272/1000 | Loss: 0.00003982
Iteration 273/1000 | Loss: 0.00003982
Iteration 274/1000 | Loss: 0.00003982
Iteration 275/1000 | Loss: 0.00003982
Iteration 276/1000 | Loss: 0.00003982
Iteration 277/1000 | Loss: 0.00003982
Iteration 278/1000 | Loss: 0.00003982
Iteration 279/1000 | Loss: 0.00003982
Iteration 280/1000 | Loss: 0.00003982
Iteration 281/1000 | Loss: 0.00003981
Iteration 282/1000 | Loss: 0.00003981
Iteration 283/1000 | Loss: 0.00003981
Iteration 284/1000 | Loss: 0.00003981
Iteration 285/1000 | Loss: 0.00003981
Iteration 286/1000 | Loss: 0.00003980
Iteration 287/1000 | Loss: 0.00003980
Iteration 288/1000 | Loss: 0.00003979
Iteration 289/1000 | Loss: 0.00003979
Iteration 290/1000 | Loss: 0.00003979
Iteration 291/1000 | Loss: 0.00003979
Iteration 292/1000 | Loss: 0.00003978
Iteration 293/1000 | Loss: 0.00003978
Iteration 294/1000 | Loss: 0.00003977
Iteration 295/1000 | Loss: 0.00003977
Iteration 296/1000 | Loss: 0.00003977
Iteration 297/1000 | Loss: 0.00003976
Iteration 298/1000 | Loss: 0.00003976
Iteration 299/1000 | Loss: 0.00003976
Iteration 300/1000 | Loss: 0.00003976
Iteration 301/1000 | Loss: 0.00003976
Iteration 302/1000 | Loss: 0.00003976
Iteration 303/1000 | Loss: 0.00003976
Iteration 304/1000 | Loss: 0.00003976
Iteration 305/1000 | Loss: 0.00003976
Iteration 306/1000 | Loss: 0.00003976
Iteration 307/1000 | Loss: 0.00003975
Iteration 308/1000 | Loss: 0.00003975
Iteration 309/1000 | Loss: 0.00003974
Iteration 310/1000 | Loss: 0.00003974
Iteration 311/1000 | Loss: 0.00003974
Iteration 312/1000 | Loss: 0.00003974
Iteration 313/1000 | Loss: 0.00003974
Iteration 314/1000 | Loss: 0.00003974
Iteration 315/1000 | Loss: 0.00003974
Iteration 316/1000 | Loss: 0.00003974
Iteration 317/1000 | Loss: 0.00003974
Iteration 318/1000 | Loss: 0.00003974
Iteration 319/1000 | Loss: 0.00003973
Iteration 320/1000 | Loss: 0.00003973
Iteration 321/1000 | Loss: 0.00003973
Iteration 322/1000 | Loss: 0.00003972
Iteration 323/1000 | Loss: 0.00003972
Iteration 324/1000 | Loss: 0.00003972
Iteration 325/1000 | Loss: 0.00003972
Iteration 326/1000 | Loss: 0.00003972
Iteration 327/1000 | Loss: 0.00003972
Iteration 328/1000 | Loss: 0.00003972
Iteration 329/1000 | Loss: 0.00003972
Iteration 330/1000 | Loss: 0.00003972
Iteration 331/1000 | Loss: 0.00003972
Iteration 332/1000 | Loss: 0.00003972
Iteration 333/1000 | Loss: 0.00003972
Iteration 334/1000 | Loss: 0.00003971
Iteration 335/1000 | Loss: 0.00003971
Iteration 336/1000 | Loss: 0.00003971
Iteration 337/1000 | Loss: 0.00003971
Iteration 338/1000 | Loss: 0.00003971
Iteration 339/1000 | Loss: 0.00003971
Iteration 340/1000 | Loss: 0.00003971
Iteration 341/1000 | Loss: 0.00003971
Iteration 342/1000 | Loss: 0.00003971
Iteration 343/1000 | Loss: 0.00003970
Iteration 344/1000 | Loss: 0.00003970
Iteration 345/1000 | Loss: 0.00003970
Iteration 346/1000 | Loss: 0.00003970
Iteration 347/1000 | Loss: 0.00003969
Iteration 348/1000 | Loss: 0.00003962
Iteration 349/1000 | Loss: 0.00003961
Iteration 350/1000 | Loss: 0.00003961
Iteration 351/1000 | Loss: 0.00003961
Iteration 352/1000 | Loss: 0.00003961
Iteration 353/1000 | Loss: 0.00003960
Iteration 354/1000 | Loss: 0.00003960
Iteration 355/1000 | Loss: 0.00003960
Iteration 356/1000 | Loss: 0.00003960
Iteration 357/1000 | Loss: 0.00003960
Iteration 358/1000 | Loss: 0.00003960
Iteration 359/1000 | Loss: 0.00003960
Iteration 360/1000 | Loss: 0.00003960
Iteration 361/1000 | Loss: 0.00003960
Iteration 362/1000 | Loss: 0.00003960
Iteration 363/1000 | Loss: 0.00003960
Iteration 364/1000 | Loss: 0.00003960
Iteration 365/1000 | Loss: 0.00003960
Iteration 366/1000 | Loss: 0.00003960
Iteration 367/1000 | Loss: 0.00003960
Iteration 368/1000 | Loss: 0.00003960
Iteration 369/1000 | Loss: 0.00003960
Iteration 370/1000 | Loss: 0.00003960
Iteration 371/1000 | Loss: 0.00003960
Iteration 372/1000 | Loss: 0.00003960
Iteration 373/1000 | Loss: 0.00003960
Iteration 374/1000 | Loss: 0.00003960
Iteration 375/1000 | Loss: 0.00003960
Iteration 376/1000 | Loss: 0.00003960
Iteration 377/1000 | Loss: 0.00003959
Iteration 378/1000 | Loss: 0.00006167
Iteration 379/1000 | Loss: 0.00003967
Iteration 380/1000 | Loss: 0.00003955
Iteration 381/1000 | Loss: 0.00003955
Iteration 382/1000 | Loss: 0.00003954
Iteration 383/1000 | Loss: 0.00003954
Iteration 384/1000 | Loss: 0.00003954
Iteration 385/1000 | Loss: 0.00003954
Iteration 386/1000 | Loss: 0.00003954
Iteration 387/1000 | Loss: 0.00003954
Iteration 388/1000 | Loss: 0.00003954
Iteration 389/1000 | Loss: 0.00003954
Iteration 390/1000 | Loss: 0.00003954
Iteration 391/1000 | Loss: 0.00003953
Iteration 392/1000 | Loss: 0.00003953
Iteration 393/1000 | Loss: 0.00003953
Iteration 394/1000 | Loss: 0.00003953
Iteration 395/1000 | Loss: 0.00003953
Iteration 396/1000 | Loss: 0.00003953
Iteration 397/1000 | Loss: 0.00003953
Iteration 398/1000 | Loss: 0.00003953
Iteration 399/1000 | Loss: 0.00003953
Iteration 400/1000 | Loss: 0.00003952
Iteration 401/1000 | Loss: 0.00003952
Iteration 402/1000 | Loss: 0.00003952
Iteration 403/1000 | Loss: 0.00003952
Iteration 404/1000 | Loss: 0.00003952
Iteration 405/1000 | Loss: 0.00003952
Iteration 406/1000 | Loss: 0.00003952
Iteration 407/1000 | Loss: 0.00003952
Iteration 408/1000 | Loss: 0.00003952
Iteration 409/1000 | Loss: 0.00003952
Iteration 410/1000 | Loss: 0.00003952
Iteration 411/1000 | Loss: 0.00003952
Iteration 412/1000 | Loss: 0.00003951
Iteration 413/1000 | Loss: 0.00003951
Iteration 414/1000 | Loss: 0.00003951
Iteration 415/1000 | Loss: 0.00003951
Iteration 416/1000 | Loss: 0.00003951
Iteration 417/1000 | Loss: 0.00003951
Iteration 418/1000 | Loss: 0.00003951
Iteration 419/1000 | Loss: 0.00003951
Iteration 420/1000 | Loss: 0.00003951
Iteration 421/1000 | Loss: 0.00003951
Iteration 422/1000 | Loss: 0.00003951
Iteration 423/1000 | Loss: 0.00003951
Iteration 424/1000 | Loss: 0.00003951
Iteration 425/1000 | Loss: 0.00003950
Iteration 426/1000 | Loss: 0.00003950
Iteration 427/1000 | Loss: 0.00003950
Iteration 428/1000 | Loss: 0.00003950
Iteration 429/1000 | Loss: 0.00003950
Iteration 430/1000 | Loss: 0.00003950
Iteration 431/1000 | Loss: 0.00003950
Iteration 432/1000 | Loss: 0.00003950
Iteration 433/1000 | Loss: 0.00003950
Iteration 434/1000 | Loss: 0.00003950
Iteration 435/1000 | Loss: 0.00003950
Iteration 436/1000 | Loss: 0.00003950
Iteration 437/1000 | Loss: 0.00003950
Iteration 438/1000 | Loss: 0.00003950
Iteration 439/1000 | Loss: 0.00003950
Iteration 440/1000 | Loss: 0.00003950
Iteration 441/1000 | Loss: 0.00003950
Iteration 442/1000 | Loss: 0.00003950
Iteration 443/1000 | Loss: 0.00003949
Iteration 444/1000 | Loss: 0.00003949
Iteration 445/1000 | Loss: 0.00003949
Iteration 446/1000 | Loss: 0.00003949
Iteration 447/1000 | Loss: 0.00003949
Iteration 448/1000 | Loss: 0.00003949
Iteration 449/1000 | Loss: 0.00003949
Iteration 450/1000 | Loss: 0.00003948
Iteration 451/1000 | Loss: 0.00003948
Iteration 452/1000 | Loss: 0.00003948
Iteration 453/1000 | Loss: 0.00003948
Iteration 454/1000 | Loss: 0.00003948
Iteration 455/1000 | Loss: 0.00003948
Iteration 456/1000 | Loss: 0.00003947
Iteration 457/1000 | Loss: 0.00003947
Iteration 458/1000 | Loss: 0.00003947
Iteration 459/1000 | Loss: 0.00003947
Iteration 460/1000 | Loss: 0.00003947
Iteration 461/1000 | Loss: 0.00003947
Iteration 462/1000 | Loss: 0.00003946
Iteration 463/1000 | Loss: 0.00003946
Iteration 464/1000 | Loss: 0.00003946
Iteration 465/1000 | Loss: 0.00003946
Iteration 466/1000 | Loss: 0.00003945
Iteration 467/1000 | Loss: 0.00003945
Iteration 468/1000 | Loss: 0.00003945
Iteration 469/1000 | Loss: 0.00003944
Iteration 470/1000 | Loss: 0.00003944
Iteration 471/1000 | Loss: 0.00003944
Iteration 472/1000 | Loss: 0.00003944
Iteration 473/1000 | Loss: 0.00003944
Iteration 474/1000 | Loss: 0.00003944
Iteration 475/1000 | Loss: 0.00003944
Iteration 476/1000 | Loss: 0.00003944
Iteration 477/1000 | Loss: 0.00003944
Iteration 478/1000 | Loss: 0.00003944
Iteration 479/1000 | Loss: 0.00003944
Iteration 480/1000 | Loss: 0.00003944
Iteration 481/1000 | Loss: 0.00003944
Iteration 482/1000 | Loss: 0.00003944
Iteration 483/1000 | Loss: 0.00003944
Iteration 484/1000 | Loss: 0.00003944
Iteration 485/1000 | Loss: 0.00003944
Iteration 486/1000 | Loss: 0.00003944
Iteration 487/1000 | Loss: 0.00003944
Iteration 488/1000 | Loss: 0.00003944
Iteration 489/1000 | Loss: 0.00003944
Iteration 490/1000 | Loss: 0.00003944
Iteration 491/1000 | Loss: 0.00003944
Iteration 492/1000 | Loss: 0.00003944
Iteration 493/1000 | Loss: 0.00003944
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 493. Stopping optimization.
Last 5 losses: [3.943735282518901e-05, 3.943735282518901e-05, 3.943735282518901e-05, 3.943735282518901e-05, 3.943735282518901e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.943735282518901e-05

Optimization complete. Final v2v error: 4.268588066101074 mm

Highest mean error: 11.037726402282715 mm for frame 180

Lowest mean error: 3.4606077671051025 mm for frame 18

Saving results

Total time: 502.4350037574768
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_003/1011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_003/1011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_003/1011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00396995
Iteration 2/25 | Loss: 0.00125717
Iteration 3/25 | Loss: 0.00118597
Iteration 4/25 | Loss: 0.00117435
Iteration 5/25 | Loss: 0.00117062
Iteration 6/25 | Loss: 0.00117047
Iteration 7/25 | Loss: 0.00117047
Iteration 8/25 | Loss: 0.00117047
Iteration 9/25 | Loss: 0.00117047
Iteration 10/25 | Loss: 0.00117047
Iteration 11/25 | Loss: 0.00117047
Iteration 12/25 | Loss: 0.00117047
Iteration 13/25 | Loss: 0.00117047
Iteration 14/25 | Loss: 0.00117047
Iteration 15/25 | Loss: 0.00117047
Iteration 16/25 | Loss: 0.00117047
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0011704680509865284, 0.0011704680509865284, 0.0011704680509865284, 0.0011704680509865284, 0.0011704680509865284]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011704680509865284

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.93902302
Iteration 2/25 | Loss: 0.00110671
Iteration 3/25 | Loss: 0.00110670
Iteration 4/25 | Loss: 0.00110670
Iteration 5/25 | Loss: 0.00110670
Iteration 6/25 | Loss: 0.00110670
Iteration 7/25 | Loss: 0.00110670
Iteration 8/25 | Loss: 0.00110670
Iteration 9/25 | Loss: 0.00110670
Iteration 10/25 | Loss: 0.00110670
Iteration 11/25 | Loss: 0.00110670
Iteration 12/25 | Loss: 0.00110670
Iteration 13/25 | Loss: 0.00110670
Iteration 14/25 | Loss: 0.00110670
Iteration 15/25 | Loss: 0.00110670
Iteration 16/25 | Loss: 0.00110670
Iteration 17/25 | Loss: 0.00110670
Iteration 18/25 | Loss: 0.00110670
Iteration 19/25 | Loss: 0.00110670
Iteration 20/25 | Loss: 0.00110670
Iteration 21/25 | Loss: 0.00110670
Iteration 22/25 | Loss: 0.00110670
Iteration 23/25 | Loss: 0.00110670
Iteration 24/25 | Loss: 0.00110670
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0011066998122259974, 0.0011066998122259974, 0.0011066998122259974, 0.0011066998122259974, 0.0011066998122259974]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011066998122259974

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00110670
Iteration 2/1000 | Loss: 0.00002128
Iteration 3/1000 | Loss: 0.00001563
Iteration 4/1000 | Loss: 0.00001387
Iteration 5/1000 | Loss: 0.00001274
Iteration 6/1000 | Loss: 0.00001205
Iteration 7/1000 | Loss: 0.00001162
Iteration 8/1000 | Loss: 0.00001137
Iteration 9/1000 | Loss: 0.00001103
Iteration 10/1000 | Loss: 0.00001079
Iteration 11/1000 | Loss: 0.00001075
Iteration 12/1000 | Loss: 0.00001067
Iteration 13/1000 | Loss: 0.00001064
Iteration 14/1000 | Loss: 0.00001057
Iteration 15/1000 | Loss: 0.00001044
Iteration 16/1000 | Loss: 0.00001044
Iteration 17/1000 | Loss: 0.00001039
Iteration 18/1000 | Loss: 0.00001039
Iteration 19/1000 | Loss: 0.00001036
Iteration 20/1000 | Loss: 0.00001031
Iteration 21/1000 | Loss: 0.00001027
Iteration 22/1000 | Loss: 0.00001026
Iteration 23/1000 | Loss: 0.00001022
Iteration 24/1000 | Loss: 0.00001022
Iteration 25/1000 | Loss: 0.00001019
Iteration 26/1000 | Loss: 0.00001019
Iteration 27/1000 | Loss: 0.00001017
Iteration 28/1000 | Loss: 0.00001016
Iteration 29/1000 | Loss: 0.00001015
Iteration 30/1000 | Loss: 0.00001015
Iteration 31/1000 | Loss: 0.00001014
Iteration 32/1000 | Loss: 0.00001013
Iteration 33/1000 | Loss: 0.00001009
Iteration 34/1000 | Loss: 0.00001005
Iteration 35/1000 | Loss: 0.00001005
Iteration 36/1000 | Loss: 0.00001004
Iteration 37/1000 | Loss: 0.00001004
Iteration 38/1000 | Loss: 0.00001003
Iteration 39/1000 | Loss: 0.00001003
Iteration 40/1000 | Loss: 0.00001002
Iteration 41/1000 | Loss: 0.00001001
Iteration 42/1000 | Loss: 0.00001001
Iteration 43/1000 | Loss: 0.00001001
Iteration 44/1000 | Loss: 0.00001000
Iteration 45/1000 | Loss: 0.00001000
Iteration 46/1000 | Loss: 0.00001000
Iteration 47/1000 | Loss: 0.00001000
Iteration 48/1000 | Loss: 0.00000999
Iteration 49/1000 | Loss: 0.00000999
Iteration 50/1000 | Loss: 0.00000999
Iteration 51/1000 | Loss: 0.00000999
Iteration 52/1000 | Loss: 0.00000999
Iteration 53/1000 | Loss: 0.00000999
Iteration 54/1000 | Loss: 0.00000998
Iteration 55/1000 | Loss: 0.00000997
Iteration 56/1000 | Loss: 0.00000997
Iteration 57/1000 | Loss: 0.00000997
Iteration 58/1000 | Loss: 0.00000997
Iteration 59/1000 | Loss: 0.00000997
Iteration 60/1000 | Loss: 0.00000997
Iteration 61/1000 | Loss: 0.00000997
Iteration 62/1000 | Loss: 0.00000996
Iteration 63/1000 | Loss: 0.00000996
Iteration 64/1000 | Loss: 0.00000996
Iteration 65/1000 | Loss: 0.00000995
Iteration 66/1000 | Loss: 0.00000994
Iteration 67/1000 | Loss: 0.00000994
Iteration 68/1000 | Loss: 0.00000994
Iteration 69/1000 | Loss: 0.00000993
Iteration 70/1000 | Loss: 0.00000993
Iteration 71/1000 | Loss: 0.00000992
Iteration 72/1000 | Loss: 0.00000992
Iteration 73/1000 | Loss: 0.00000992
Iteration 74/1000 | Loss: 0.00000992
Iteration 75/1000 | Loss: 0.00000992
Iteration 76/1000 | Loss: 0.00000991
Iteration 77/1000 | Loss: 0.00000991
Iteration 78/1000 | Loss: 0.00000991
Iteration 79/1000 | Loss: 0.00000991
Iteration 80/1000 | Loss: 0.00000991
Iteration 81/1000 | Loss: 0.00000990
Iteration 82/1000 | Loss: 0.00000990
Iteration 83/1000 | Loss: 0.00000990
Iteration 84/1000 | Loss: 0.00000989
Iteration 85/1000 | Loss: 0.00000989
Iteration 86/1000 | Loss: 0.00000989
Iteration 87/1000 | Loss: 0.00000989
Iteration 88/1000 | Loss: 0.00000989
Iteration 89/1000 | Loss: 0.00000989
Iteration 90/1000 | Loss: 0.00000989
Iteration 91/1000 | Loss: 0.00000989
Iteration 92/1000 | Loss: 0.00000988
Iteration 93/1000 | Loss: 0.00000988
Iteration 94/1000 | Loss: 0.00000988
Iteration 95/1000 | Loss: 0.00000988
Iteration 96/1000 | Loss: 0.00000988
Iteration 97/1000 | Loss: 0.00000988
Iteration 98/1000 | Loss: 0.00000987
Iteration 99/1000 | Loss: 0.00000987
Iteration 100/1000 | Loss: 0.00000987
Iteration 101/1000 | Loss: 0.00000987
Iteration 102/1000 | Loss: 0.00000987
Iteration 103/1000 | Loss: 0.00000987
Iteration 104/1000 | Loss: 0.00000987
Iteration 105/1000 | Loss: 0.00000987
Iteration 106/1000 | Loss: 0.00000987
Iteration 107/1000 | Loss: 0.00000986
Iteration 108/1000 | Loss: 0.00000986
Iteration 109/1000 | Loss: 0.00000986
Iteration 110/1000 | Loss: 0.00000986
Iteration 111/1000 | Loss: 0.00000985
Iteration 112/1000 | Loss: 0.00000985
Iteration 113/1000 | Loss: 0.00000985
Iteration 114/1000 | Loss: 0.00000985
Iteration 115/1000 | Loss: 0.00000984
Iteration 116/1000 | Loss: 0.00000984
Iteration 117/1000 | Loss: 0.00000984
Iteration 118/1000 | Loss: 0.00000984
Iteration 119/1000 | Loss: 0.00000984
Iteration 120/1000 | Loss: 0.00000984
Iteration 121/1000 | Loss: 0.00000983
Iteration 122/1000 | Loss: 0.00000983
Iteration 123/1000 | Loss: 0.00000983
Iteration 124/1000 | Loss: 0.00000983
Iteration 125/1000 | Loss: 0.00000983
Iteration 126/1000 | Loss: 0.00000983
Iteration 127/1000 | Loss: 0.00000983
Iteration 128/1000 | Loss: 0.00000983
Iteration 129/1000 | Loss: 0.00000983
Iteration 130/1000 | Loss: 0.00000983
Iteration 131/1000 | Loss: 0.00000983
Iteration 132/1000 | Loss: 0.00000982
Iteration 133/1000 | Loss: 0.00000982
Iteration 134/1000 | Loss: 0.00000982
Iteration 135/1000 | Loss: 0.00000982
Iteration 136/1000 | Loss: 0.00000982
Iteration 137/1000 | Loss: 0.00000982
Iteration 138/1000 | Loss: 0.00000982
Iteration 139/1000 | Loss: 0.00000982
Iteration 140/1000 | Loss: 0.00000982
Iteration 141/1000 | Loss: 0.00000982
Iteration 142/1000 | Loss: 0.00000982
Iteration 143/1000 | Loss: 0.00000981
Iteration 144/1000 | Loss: 0.00000981
Iteration 145/1000 | Loss: 0.00000981
Iteration 146/1000 | Loss: 0.00000981
Iteration 147/1000 | Loss: 0.00000981
Iteration 148/1000 | Loss: 0.00000981
Iteration 149/1000 | Loss: 0.00000981
Iteration 150/1000 | Loss: 0.00000981
Iteration 151/1000 | Loss: 0.00000981
Iteration 152/1000 | Loss: 0.00000981
Iteration 153/1000 | Loss: 0.00000981
Iteration 154/1000 | Loss: 0.00000980
Iteration 155/1000 | Loss: 0.00000980
Iteration 156/1000 | Loss: 0.00000980
Iteration 157/1000 | Loss: 0.00000980
Iteration 158/1000 | Loss: 0.00000979
Iteration 159/1000 | Loss: 0.00000979
Iteration 160/1000 | Loss: 0.00000979
Iteration 161/1000 | Loss: 0.00000979
Iteration 162/1000 | Loss: 0.00000979
Iteration 163/1000 | Loss: 0.00000979
Iteration 164/1000 | Loss: 0.00000979
Iteration 165/1000 | Loss: 0.00000979
Iteration 166/1000 | Loss: 0.00000979
Iteration 167/1000 | Loss: 0.00000979
Iteration 168/1000 | Loss: 0.00000978
Iteration 169/1000 | Loss: 0.00000978
Iteration 170/1000 | Loss: 0.00000978
Iteration 171/1000 | Loss: 0.00000978
Iteration 172/1000 | Loss: 0.00000978
Iteration 173/1000 | Loss: 0.00000978
Iteration 174/1000 | Loss: 0.00000977
Iteration 175/1000 | Loss: 0.00000977
Iteration 176/1000 | Loss: 0.00000977
Iteration 177/1000 | Loss: 0.00000977
Iteration 178/1000 | Loss: 0.00000977
Iteration 179/1000 | Loss: 0.00000977
Iteration 180/1000 | Loss: 0.00000977
Iteration 181/1000 | Loss: 0.00000977
Iteration 182/1000 | Loss: 0.00000976
Iteration 183/1000 | Loss: 0.00000976
Iteration 184/1000 | Loss: 0.00000976
Iteration 185/1000 | Loss: 0.00000976
Iteration 186/1000 | Loss: 0.00000976
Iteration 187/1000 | Loss: 0.00000976
Iteration 188/1000 | Loss: 0.00000976
Iteration 189/1000 | Loss: 0.00000976
Iteration 190/1000 | Loss: 0.00000976
Iteration 191/1000 | Loss: 0.00000976
Iteration 192/1000 | Loss: 0.00000976
Iteration 193/1000 | Loss: 0.00000976
Iteration 194/1000 | Loss: 0.00000975
Iteration 195/1000 | Loss: 0.00000975
Iteration 196/1000 | Loss: 0.00000975
Iteration 197/1000 | Loss: 0.00000975
Iteration 198/1000 | Loss: 0.00000975
Iteration 199/1000 | Loss: 0.00000975
Iteration 200/1000 | Loss: 0.00000975
Iteration 201/1000 | Loss: 0.00000975
Iteration 202/1000 | Loss: 0.00000975
Iteration 203/1000 | Loss: 0.00000975
Iteration 204/1000 | Loss: 0.00000975
Iteration 205/1000 | Loss: 0.00000975
Iteration 206/1000 | Loss: 0.00000975
Iteration 207/1000 | Loss: 0.00000975
Iteration 208/1000 | Loss: 0.00000975
Iteration 209/1000 | Loss: 0.00000975
Iteration 210/1000 | Loss: 0.00000975
Iteration 211/1000 | Loss: 0.00000975
Iteration 212/1000 | Loss: 0.00000975
Iteration 213/1000 | Loss: 0.00000974
Iteration 214/1000 | Loss: 0.00000974
Iteration 215/1000 | Loss: 0.00000974
Iteration 216/1000 | Loss: 0.00000974
Iteration 217/1000 | Loss: 0.00000974
Iteration 218/1000 | Loss: 0.00000974
Iteration 219/1000 | Loss: 0.00000974
Iteration 220/1000 | Loss: 0.00000974
Iteration 221/1000 | Loss: 0.00000974
Iteration 222/1000 | Loss: 0.00000974
Iteration 223/1000 | Loss: 0.00000974
Iteration 224/1000 | Loss: 0.00000974
Iteration 225/1000 | Loss: 0.00000974
Iteration 226/1000 | Loss: 0.00000974
Iteration 227/1000 | Loss: 0.00000974
Iteration 228/1000 | Loss: 0.00000974
Iteration 229/1000 | Loss: 0.00000973
Iteration 230/1000 | Loss: 0.00000973
Iteration 231/1000 | Loss: 0.00000973
Iteration 232/1000 | Loss: 0.00000973
Iteration 233/1000 | Loss: 0.00000973
Iteration 234/1000 | Loss: 0.00000973
Iteration 235/1000 | Loss: 0.00000973
Iteration 236/1000 | Loss: 0.00000973
Iteration 237/1000 | Loss: 0.00000973
Iteration 238/1000 | Loss: 0.00000973
Iteration 239/1000 | Loss: 0.00000973
Iteration 240/1000 | Loss: 0.00000973
Iteration 241/1000 | Loss: 0.00000973
Iteration 242/1000 | Loss: 0.00000973
Iteration 243/1000 | Loss: 0.00000973
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 243. Stopping optimization.
Last 5 losses: [9.730277270136867e-06, 9.730277270136867e-06, 9.730277270136867e-06, 9.730277270136867e-06, 9.730277270136867e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.730277270136867e-06

Optimization complete. Final v2v error: 2.6912639141082764 mm

Highest mean error: 2.982724905014038 mm for frame 81

Lowest mean error: 2.5864298343658447 mm for frame 195

Saving results

Total time: 46.257351875305176
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_003/1017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_003/1017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_003/1017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00790765
Iteration 2/25 | Loss: 0.00127603
Iteration 3/25 | Loss: 0.00118867
Iteration 4/25 | Loss: 0.00118008
Iteration 5/25 | Loss: 0.00117798
Iteration 6/25 | Loss: 0.00117798
Iteration 7/25 | Loss: 0.00117798
Iteration 8/25 | Loss: 0.00117798
Iteration 9/25 | Loss: 0.00117798
Iteration 10/25 | Loss: 0.00117798
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.001177977304905653, 0.001177977304905653, 0.001177977304905653, 0.001177977304905653, 0.001177977304905653]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001177977304905653

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32645214
Iteration 2/25 | Loss: 0.00118245
Iteration 3/25 | Loss: 0.00118244
Iteration 4/25 | Loss: 0.00118244
Iteration 5/25 | Loss: 0.00118244
Iteration 6/25 | Loss: 0.00118244
Iteration 7/25 | Loss: 0.00118244
Iteration 8/25 | Loss: 0.00118244
Iteration 9/25 | Loss: 0.00118244
Iteration 10/25 | Loss: 0.00118244
Iteration 11/25 | Loss: 0.00118244
Iteration 12/25 | Loss: 0.00118244
Iteration 13/25 | Loss: 0.00118244
Iteration 14/25 | Loss: 0.00118244
Iteration 15/25 | Loss: 0.00118244
Iteration 16/25 | Loss: 0.00118244
Iteration 17/25 | Loss: 0.00118244
Iteration 18/25 | Loss: 0.00118244
Iteration 19/25 | Loss: 0.00118244
Iteration 20/25 | Loss: 0.00118244
Iteration 21/25 | Loss: 0.00118244
Iteration 22/25 | Loss: 0.00118244
Iteration 23/25 | Loss: 0.00118244
Iteration 24/25 | Loss: 0.00118244
Iteration 25/25 | Loss: 0.00118244

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00118244
Iteration 2/1000 | Loss: 0.00002172
Iteration 3/1000 | Loss: 0.00001499
Iteration 4/1000 | Loss: 0.00001328
Iteration 5/1000 | Loss: 0.00001252
Iteration 6/1000 | Loss: 0.00001184
Iteration 7/1000 | Loss: 0.00001133
Iteration 8/1000 | Loss: 0.00001105
Iteration 9/1000 | Loss: 0.00001079
Iteration 10/1000 | Loss: 0.00001052
Iteration 11/1000 | Loss: 0.00001047
Iteration 12/1000 | Loss: 0.00001044
Iteration 13/1000 | Loss: 0.00001043
Iteration 14/1000 | Loss: 0.00001042
Iteration 15/1000 | Loss: 0.00001042
Iteration 16/1000 | Loss: 0.00001041
Iteration 17/1000 | Loss: 0.00001040
Iteration 18/1000 | Loss: 0.00001033
Iteration 19/1000 | Loss: 0.00001027
Iteration 20/1000 | Loss: 0.00001025
Iteration 21/1000 | Loss: 0.00001023
Iteration 22/1000 | Loss: 0.00001022
Iteration 23/1000 | Loss: 0.00001021
Iteration 24/1000 | Loss: 0.00001014
Iteration 25/1000 | Loss: 0.00001011
Iteration 26/1000 | Loss: 0.00001011
Iteration 27/1000 | Loss: 0.00001011
Iteration 28/1000 | Loss: 0.00001011
Iteration 29/1000 | Loss: 0.00001010
Iteration 30/1000 | Loss: 0.00001009
Iteration 31/1000 | Loss: 0.00001008
Iteration 32/1000 | Loss: 0.00001007
Iteration 33/1000 | Loss: 0.00001007
Iteration 34/1000 | Loss: 0.00001006
Iteration 35/1000 | Loss: 0.00001006
Iteration 36/1000 | Loss: 0.00001005
Iteration 37/1000 | Loss: 0.00001005
Iteration 38/1000 | Loss: 0.00001004
Iteration 39/1000 | Loss: 0.00001003
Iteration 40/1000 | Loss: 0.00001001
Iteration 41/1000 | Loss: 0.00001001
Iteration 42/1000 | Loss: 0.00000999
Iteration 43/1000 | Loss: 0.00000999
Iteration 44/1000 | Loss: 0.00000998
Iteration 45/1000 | Loss: 0.00000997
Iteration 46/1000 | Loss: 0.00000997
Iteration 47/1000 | Loss: 0.00000996
Iteration 48/1000 | Loss: 0.00000993
Iteration 49/1000 | Loss: 0.00000993
Iteration 50/1000 | Loss: 0.00000993
Iteration 51/1000 | Loss: 0.00000993
Iteration 52/1000 | Loss: 0.00000992
Iteration 53/1000 | Loss: 0.00000992
Iteration 54/1000 | Loss: 0.00000990
Iteration 55/1000 | Loss: 0.00000989
Iteration 56/1000 | Loss: 0.00000989
Iteration 57/1000 | Loss: 0.00000988
Iteration 58/1000 | Loss: 0.00000988
Iteration 59/1000 | Loss: 0.00000987
Iteration 60/1000 | Loss: 0.00000987
Iteration 61/1000 | Loss: 0.00000983
Iteration 62/1000 | Loss: 0.00000983
Iteration 63/1000 | Loss: 0.00000983
Iteration 64/1000 | Loss: 0.00000983
Iteration 65/1000 | Loss: 0.00000983
Iteration 66/1000 | Loss: 0.00000982
Iteration 67/1000 | Loss: 0.00000982
Iteration 68/1000 | Loss: 0.00000981
Iteration 69/1000 | Loss: 0.00000980
Iteration 70/1000 | Loss: 0.00000980
Iteration 71/1000 | Loss: 0.00000979
Iteration 72/1000 | Loss: 0.00000979
Iteration 73/1000 | Loss: 0.00000979
Iteration 74/1000 | Loss: 0.00000978
Iteration 75/1000 | Loss: 0.00000978
Iteration 76/1000 | Loss: 0.00000977
Iteration 77/1000 | Loss: 0.00000977
Iteration 78/1000 | Loss: 0.00000976
Iteration 79/1000 | Loss: 0.00000976
Iteration 80/1000 | Loss: 0.00000975
Iteration 81/1000 | Loss: 0.00000975
Iteration 82/1000 | Loss: 0.00000975
Iteration 83/1000 | Loss: 0.00000975
Iteration 84/1000 | Loss: 0.00000975
Iteration 85/1000 | Loss: 0.00000975
Iteration 86/1000 | Loss: 0.00000974
Iteration 87/1000 | Loss: 0.00000974
Iteration 88/1000 | Loss: 0.00000973
Iteration 89/1000 | Loss: 0.00000973
Iteration 90/1000 | Loss: 0.00000972
Iteration 91/1000 | Loss: 0.00000972
Iteration 92/1000 | Loss: 0.00000972
Iteration 93/1000 | Loss: 0.00000971
Iteration 94/1000 | Loss: 0.00000971
Iteration 95/1000 | Loss: 0.00000971
Iteration 96/1000 | Loss: 0.00000971
Iteration 97/1000 | Loss: 0.00000970
Iteration 98/1000 | Loss: 0.00000970
Iteration 99/1000 | Loss: 0.00000970
Iteration 100/1000 | Loss: 0.00000970
Iteration 101/1000 | Loss: 0.00000970
Iteration 102/1000 | Loss: 0.00000970
Iteration 103/1000 | Loss: 0.00000970
Iteration 104/1000 | Loss: 0.00000970
Iteration 105/1000 | Loss: 0.00000970
Iteration 106/1000 | Loss: 0.00000970
Iteration 107/1000 | Loss: 0.00000969
Iteration 108/1000 | Loss: 0.00000969
Iteration 109/1000 | Loss: 0.00000969
Iteration 110/1000 | Loss: 0.00000969
Iteration 111/1000 | Loss: 0.00000969
Iteration 112/1000 | Loss: 0.00000969
Iteration 113/1000 | Loss: 0.00000969
Iteration 114/1000 | Loss: 0.00000969
Iteration 115/1000 | Loss: 0.00000969
Iteration 116/1000 | Loss: 0.00000968
Iteration 117/1000 | Loss: 0.00000968
Iteration 118/1000 | Loss: 0.00000968
Iteration 119/1000 | Loss: 0.00000968
Iteration 120/1000 | Loss: 0.00000968
Iteration 121/1000 | Loss: 0.00000968
Iteration 122/1000 | Loss: 0.00000967
Iteration 123/1000 | Loss: 0.00000967
Iteration 124/1000 | Loss: 0.00000967
Iteration 125/1000 | Loss: 0.00000967
Iteration 126/1000 | Loss: 0.00000966
Iteration 127/1000 | Loss: 0.00000966
Iteration 128/1000 | Loss: 0.00000965
Iteration 129/1000 | Loss: 0.00000965
Iteration 130/1000 | Loss: 0.00000965
Iteration 131/1000 | Loss: 0.00000965
Iteration 132/1000 | Loss: 0.00000965
Iteration 133/1000 | Loss: 0.00000965
Iteration 134/1000 | Loss: 0.00000964
Iteration 135/1000 | Loss: 0.00000964
Iteration 136/1000 | Loss: 0.00000964
Iteration 137/1000 | Loss: 0.00000964
Iteration 138/1000 | Loss: 0.00000963
Iteration 139/1000 | Loss: 0.00000963
Iteration 140/1000 | Loss: 0.00000963
Iteration 141/1000 | Loss: 0.00000963
Iteration 142/1000 | Loss: 0.00000963
Iteration 143/1000 | Loss: 0.00000963
Iteration 144/1000 | Loss: 0.00000963
Iteration 145/1000 | Loss: 0.00000963
Iteration 146/1000 | Loss: 0.00000963
Iteration 147/1000 | Loss: 0.00000963
Iteration 148/1000 | Loss: 0.00000963
Iteration 149/1000 | Loss: 0.00000963
Iteration 150/1000 | Loss: 0.00000963
Iteration 151/1000 | Loss: 0.00000963
Iteration 152/1000 | Loss: 0.00000963
Iteration 153/1000 | Loss: 0.00000963
Iteration 154/1000 | Loss: 0.00000963
Iteration 155/1000 | Loss: 0.00000963
Iteration 156/1000 | Loss: 0.00000963
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 156. Stopping optimization.
Last 5 losses: [9.628703082853463e-06, 9.628703082853463e-06, 9.628703082853463e-06, 9.628703082853463e-06, 9.628703082853463e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.628703082853463e-06

Optimization complete. Final v2v error: 2.65738844871521 mm

Highest mean error: 2.8495519161224365 mm for frame 66

Lowest mean error: 2.495208978652954 mm for frame 25

Saving results

Total time: 39.91383194923401
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_003/1097/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_003/1097.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_003/1097
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00956520
Iteration 2/25 | Loss: 0.00264691
Iteration 3/25 | Loss: 0.00207887
Iteration 4/25 | Loss: 0.00198536
Iteration 5/25 | Loss: 0.00195613
Iteration 6/25 | Loss: 0.00181893
Iteration 7/25 | Loss: 0.00170140
Iteration 8/25 | Loss: 0.00152145
Iteration 9/25 | Loss: 0.00141244
Iteration 10/25 | Loss: 0.00141179
Iteration 11/25 | Loss: 0.00134632
Iteration 12/25 | Loss: 0.00128569
Iteration 13/25 | Loss: 0.00127634
Iteration 14/25 | Loss: 0.00128884
Iteration 15/25 | Loss: 0.00126834
Iteration 16/25 | Loss: 0.00125969
Iteration 17/25 | Loss: 0.00125881
Iteration 18/25 | Loss: 0.00125871
Iteration 19/25 | Loss: 0.00125870
Iteration 20/25 | Loss: 0.00125870
Iteration 21/25 | Loss: 0.00125869
Iteration 22/25 | Loss: 0.00125869
Iteration 23/25 | Loss: 0.00125869
Iteration 24/25 | Loss: 0.00125869
Iteration 25/25 | Loss: 0.00125869

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.31817019
Iteration 2/25 | Loss: 0.00087527
Iteration 3/25 | Loss: 0.00087527
Iteration 4/25 | Loss: 0.00087527
Iteration 5/25 | Loss: 0.00087527
Iteration 6/25 | Loss: 0.00087527
Iteration 7/25 | Loss: 0.00087527
Iteration 8/25 | Loss: 0.00087527
Iteration 9/25 | Loss: 0.00087527
Iteration 10/25 | Loss: 0.00087527
Iteration 11/25 | Loss: 0.00087527
Iteration 12/25 | Loss: 0.00087527
Iteration 13/25 | Loss: 0.00087527
Iteration 14/25 | Loss: 0.00087527
Iteration 15/25 | Loss: 0.00087527
Iteration 16/25 | Loss: 0.00087527
Iteration 17/25 | Loss: 0.00087527
Iteration 18/25 | Loss: 0.00087527
Iteration 19/25 | Loss: 0.00087527
Iteration 20/25 | Loss: 0.00087527
Iteration 21/25 | Loss: 0.00087527
Iteration 22/25 | Loss: 0.00087527
Iteration 23/25 | Loss: 0.00087527
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0008752724970690906, 0.0008752724970690906, 0.0008752724970690906, 0.0008752724970690906, 0.0008752724970690906]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008752724970690906

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00087527
Iteration 2/1000 | Loss: 0.00003984
Iteration 3/1000 | Loss: 0.00002842
Iteration 4/1000 | Loss: 0.00002434
Iteration 5/1000 | Loss: 0.00002247
Iteration 6/1000 | Loss: 0.00002148
Iteration 7/1000 | Loss: 0.00002081
Iteration 8/1000 | Loss: 0.00002036
Iteration 9/1000 | Loss: 0.00001996
Iteration 10/1000 | Loss: 0.00001961
Iteration 11/1000 | Loss: 0.00001955
Iteration 12/1000 | Loss: 0.00037326
Iteration 13/1000 | Loss: 0.00002274
Iteration 14/1000 | Loss: 0.00001897
Iteration 15/1000 | Loss: 0.00001702
Iteration 16/1000 | Loss: 0.00001510
Iteration 17/1000 | Loss: 0.00001385
Iteration 18/1000 | Loss: 0.00001317
Iteration 19/1000 | Loss: 0.00001287
Iteration 20/1000 | Loss: 0.00001261
Iteration 21/1000 | Loss: 0.00001230
Iteration 22/1000 | Loss: 0.00001209
Iteration 23/1000 | Loss: 0.00001196
Iteration 24/1000 | Loss: 0.00001190
Iteration 25/1000 | Loss: 0.00001188
Iteration 26/1000 | Loss: 0.00001187
Iteration 27/1000 | Loss: 0.00001186
Iteration 28/1000 | Loss: 0.00001185
Iteration 29/1000 | Loss: 0.00001185
Iteration 30/1000 | Loss: 0.00001185
Iteration 31/1000 | Loss: 0.00001185
Iteration 32/1000 | Loss: 0.00001184
Iteration 33/1000 | Loss: 0.00001183
Iteration 34/1000 | Loss: 0.00001183
Iteration 35/1000 | Loss: 0.00001179
Iteration 36/1000 | Loss: 0.00001179
Iteration 37/1000 | Loss: 0.00001179
Iteration 38/1000 | Loss: 0.00001178
Iteration 39/1000 | Loss: 0.00001178
Iteration 40/1000 | Loss: 0.00001175
Iteration 41/1000 | Loss: 0.00001175
Iteration 42/1000 | Loss: 0.00001174
Iteration 43/1000 | Loss: 0.00001173
Iteration 44/1000 | Loss: 0.00001172
Iteration 45/1000 | Loss: 0.00001172
Iteration 46/1000 | Loss: 0.00001168
Iteration 47/1000 | Loss: 0.00001168
Iteration 48/1000 | Loss: 0.00001168
Iteration 49/1000 | Loss: 0.00001168
Iteration 50/1000 | Loss: 0.00001168
Iteration 51/1000 | Loss: 0.00001168
Iteration 52/1000 | Loss: 0.00001168
Iteration 53/1000 | Loss: 0.00001168
Iteration 54/1000 | Loss: 0.00001168
Iteration 55/1000 | Loss: 0.00001168
Iteration 56/1000 | Loss: 0.00001168
Iteration 57/1000 | Loss: 0.00001167
Iteration 58/1000 | Loss: 0.00001167
Iteration 59/1000 | Loss: 0.00001167
Iteration 60/1000 | Loss: 0.00001167
Iteration 61/1000 | Loss: 0.00001167
Iteration 62/1000 | Loss: 0.00001167
Iteration 63/1000 | Loss: 0.00001167
Iteration 64/1000 | Loss: 0.00001166
Iteration 65/1000 | Loss: 0.00001166
Iteration 66/1000 | Loss: 0.00001166
Iteration 67/1000 | Loss: 0.00001166
Iteration 68/1000 | Loss: 0.00001166
Iteration 69/1000 | Loss: 0.00001166
Iteration 70/1000 | Loss: 0.00001166
Iteration 71/1000 | Loss: 0.00001166
Iteration 72/1000 | Loss: 0.00001166
Iteration 73/1000 | Loss: 0.00001166
Iteration 74/1000 | Loss: 0.00001166
Iteration 75/1000 | Loss: 0.00001166
Iteration 76/1000 | Loss: 0.00001166
Iteration 77/1000 | Loss: 0.00001166
Iteration 78/1000 | Loss: 0.00001166
Iteration 79/1000 | Loss: 0.00001166
Iteration 80/1000 | Loss: 0.00001166
Iteration 81/1000 | Loss: 0.00001166
Iteration 82/1000 | Loss: 0.00001166
Iteration 83/1000 | Loss: 0.00001166
Iteration 84/1000 | Loss: 0.00001166
Iteration 85/1000 | Loss: 0.00001166
Iteration 86/1000 | Loss: 0.00001166
Iteration 87/1000 | Loss: 0.00001166
Iteration 88/1000 | Loss: 0.00001166
Iteration 89/1000 | Loss: 0.00001166
Iteration 90/1000 | Loss: 0.00001166
Iteration 91/1000 | Loss: 0.00001166
Iteration 92/1000 | Loss: 0.00001166
Iteration 93/1000 | Loss: 0.00001166
Iteration 94/1000 | Loss: 0.00001166
Iteration 95/1000 | Loss: 0.00001166
Iteration 96/1000 | Loss: 0.00001166
Iteration 97/1000 | Loss: 0.00001166
Iteration 98/1000 | Loss: 0.00001166
Iteration 99/1000 | Loss: 0.00001166
Iteration 100/1000 | Loss: 0.00001166
Iteration 101/1000 | Loss: 0.00001166
Iteration 102/1000 | Loss: 0.00001166
Iteration 103/1000 | Loss: 0.00001166
Iteration 104/1000 | Loss: 0.00001166
Iteration 105/1000 | Loss: 0.00001166
Iteration 106/1000 | Loss: 0.00001166
Iteration 107/1000 | Loss: 0.00001166
Iteration 108/1000 | Loss: 0.00001166
Iteration 109/1000 | Loss: 0.00001166
Iteration 110/1000 | Loss: 0.00001166
Iteration 111/1000 | Loss: 0.00001166
Iteration 112/1000 | Loss: 0.00001166
Iteration 113/1000 | Loss: 0.00001166
Iteration 114/1000 | Loss: 0.00001166
Iteration 115/1000 | Loss: 0.00001166
Iteration 116/1000 | Loss: 0.00001166
Iteration 117/1000 | Loss: 0.00001166
Iteration 118/1000 | Loss: 0.00001166
Iteration 119/1000 | Loss: 0.00001166
Iteration 120/1000 | Loss: 0.00001166
Iteration 121/1000 | Loss: 0.00001166
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 121. Stopping optimization.
Last 5 losses: [1.1657026334432885e-05, 1.1657026334432885e-05, 1.1657026334432885e-05, 1.1657026334432885e-05, 1.1657026334432885e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1657026334432885e-05

Optimization complete. Final v2v error: 2.9687857627868652 mm

Highest mean error: 3.424562454223633 mm for frame 13

Lowest mean error: 2.8108267784118652 mm for frame 3

Saving results

Total time: 66.17063474655151
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_003/1062/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_003/1062.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_003/1062
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00398691
Iteration 2/25 | Loss: 0.00130587
Iteration 3/25 | Loss: 0.00123254
Iteration 4/25 | Loss: 0.00122675
Iteration 5/25 | Loss: 0.00122518
Iteration 6/25 | Loss: 0.00122518
Iteration 7/25 | Loss: 0.00122518
Iteration 8/25 | Loss: 0.00122518
Iteration 9/25 | Loss: 0.00122518
Iteration 10/25 | Loss: 0.00122518
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0012251758016645908, 0.0012251758016645908, 0.0012251758016645908, 0.0012251758016645908, 0.0012251758016645908]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012251758016645908

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32906044
Iteration 2/25 | Loss: 0.00118020
Iteration 3/25 | Loss: 0.00118020
Iteration 4/25 | Loss: 0.00118020
Iteration 5/25 | Loss: 0.00118020
Iteration 6/25 | Loss: 0.00118020
Iteration 7/25 | Loss: 0.00118020
Iteration 8/25 | Loss: 0.00118020
Iteration 9/25 | Loss: 0.00118020
Iteration 10/25 | Loss: 0.00118019
Iteration 11/25 | Loss: 0.00118019
Iteration 12/25 | Loss: 0.00118019
Iteration 13/25 | Loss: 0.00118019
Iteration 14/25 | Loss: 0.00118019
Iteration 15/25 | Loss: 0.00118019
Iteration 16/25 | Loss: 0.00118019
Iteration 17/25 | Loss: 0.00118019
Iteration 18/25 | Loss: 0.00118019
Iteration 19/25 | Loss: 0.00118019
Iteration 20/25 | Loss: 0.00118019
Iteration 21/25 | Loss: 0.00118019
Iteration 22/25 | Loss: 0.00118019
Iteration 23/25 | Loss: 0.00118019
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0011801945511251688, 0.0011801945511251688, 0.0011801945511251688, 0.0011801945511251688, 0.0011801945511251688]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011801945511251688

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00118019
Iteration 2/1000 | Loss: 0.00002859
Iteration 3/1000 | Loss: 0.00001965
Iteration 4/1000 | Loss: 0.00001755
Iteration 5/1000 | Loss: 0.00001656
Iteration 6/1000 | Loss: 0.00001588
Iteration 7/1000 | Loss: 0.00001549
Iteration 8/1000 | Loss: 0.00001520
Iteration 9/1000 | Loss: 0.00001497
Iteration 10/1000 | Loss: 0.00001476
Iteration 11/1000 | Loss: 0.00001462
Iteration 12/1000 | Loss: 0.00001443
Iteration 13/1000 | Loss: 0.00001429
Iteration 14/1000 | Loss: 0.00001419
Iteration 15/1000 | Loss: 0.00001419
Iteration 16/1000 | Loss: 0.00001417
Iteration 17/1000 | Loss: 0.00001417
Iteration 18/1000 | Loss: 0.00001416
Iteration 19/1000 | Loss: 0.00001416
Iteration 20/1000 | Loss: 0.00001415
Iteration 21/1000 | Loss: 0.00001415
Iteration 22/1000 | Loss: 0.00001414
Iteration 23/1000 | Loss: 0.00001413
Iteration 24/1000 | Loss: 0.00001412
Iteration 25/1000 | Loss: 0.00001412
Iteration 26/1000 | Loss: 0.00001411
Iteration 27/1000 | Loss: 0.00001411
Iteration 28/1000 | Loss: 0.00001410
Iteration 29/1000 | Loss: 0.00001409
Iteration 30/1000 | Loss: 0.00001409
Iteration 31/1000 | Loss: 0.00001408
Iteration 32/1000 | Loss: 0.00001407
Iteration 33/1000 | Loss: 0.00001407
Iteration 34/1000 | Loss: 0.00001407
Iteration 35/1000 | Loss: 0.00001406
Iteration 36/1000 | Loss: 0.00001406
Iteration 37/1000 | Loss: 0.00001405
Iteration 38/1000 | Loss: 0.00001405
Iteration 39/1000 | Loss: 0.00001404
Iteration 40/1000 | Loss: 0.00001404
Iteration 41/1000 | Loss: 0.00001403
Iteration 42/1000 | Loss: 0.00001403
Iteration 43/1000 | Loss: 0.00001403
Iteration 44/1000 | Loss: 0.00001402
Iteration 45/1000 | Loss: 0.00001402
Iteration 46/1000 | Loss: 0.00001402
Iteration 47/1000 | Loss: 0.00001401
Iteration 48/1000 | Loss: 0.00001401
Iteration 49/1000 | Loss: 0.00001400
Iteration 50/1000 | Loss: 0.00001400
Iteration 51/1000 | Loss: 0.00001400
Iteration 52/1000 | Loss: 0.00001400
Iteration 53/1000 | Loss: 0.00001400
Iteration 54/1000 | Loss: 0.00001400
Iteration 55/1000 | Loss: 0.00001400
Iteration 56/1000 | Loss: 0.00001399
Iteration 57/1000 | Loss: 0.00001399
Iteration 58/1000 | Loss: 0.00001399
Iteration 59/1000 | Loss: 0.00001399
Iteration 60/1000 | Loss: 0.00001399
Iteration 61/1000 | Loss: 0.00001398
Iteration 62/1000 | Loss: 0.00001398
Iteration 63/1000 | Loss: 0.00001396
Iteration 64/1000 | Loss: 0.00001396
Iteration 65/1000 | Loss: 0.00001396
Iteration 66/1000 | Loss: 0.00001396
Iteration 67/1000 | Loss: 0.00001396
Iteration 68/1000 | Loss: 0.00001395
Iteration 69/1000 | Loss: 0.00001395
Iteration 70/1000 | Loss: 0.00001395
Iteration 71/1000 | Loss: 0.00001395
Iteration 72/1000 | Loss: 0.00001394
Iteration 73/1000 | Loss: 0.00001393
Iteration 74/1000 | Loss: 0.00001393
Iteration 75/1000 | Loss: 0.00001392
Iteration 76/1000 | Loss: 0.00001392
Iteration 77/1000 | Loss: 0.00001392
Iteration 78/1000 | Loss: 0.00001392
Iteration 79/1000 | Loss: 0.00001392
Iteration 80/1000 | Loss: 0.00001391
Iteration 81/1000 | Loss: 0.00001391
Iteration 82/1000 | Loss: 0.00001391
Iteration 83/1000 | Loss: 0.00001390
Iteration 84/1000 | Loss: 0.00001390
Iteration 85/1000 | Loss: 0.00001389
Iteration 86/1000 | Loss: 0.00001389
Iteration 87/1000 | Loss: 0.00001389
Iteration 88/1000 | Loss: 0.00001389
Iteration 89/1000 | Loss: 0.00001388
Iteration 90/1000 | Loss: 0.00001388
Iteration 91/1000 | Loss: 0.00001388
Iteration 92/1000 | Loss: 0.00001388
Iteration 93/1000 | Loss: 0.00001388
Iteration 94/1000 | Loss: 0.00001388
Iteration 95/1000 | Loss: 0.00001387
Iteration 96/1000 | Loss: 0.00001387
Iteration 97/1000 | Loss: 0.00001386
Iteration 98/1000 | Loss: 0.00001385
Iteration 99/1000 | Loss: 0.00001385
Iteration 100/1000 | Loss: 0.00001385
Iteration 101/1000 | Loss: 0.00001384
Iteration 102/1000 | Loss: 0.00001384
Iteration 103/1000 | Loss: 0.00001383
Iteration 104/1000 | Loss: 0.00001383
Iteration 105/1000 | Loss: 0.00001383
Iteration 106/1000 | Loss: 0.00001382
Iteration 107/1000 | Loss: 0.00001382
Iteration 108/1000 | Loss: 0.00001382
Iteration 109/1000 | Loss: 0.00001381
Iteration 110/1000 | Loss: 0.00001381
Iteration 111/1000 | Loss: 0.00001381
Iteration 112/1000 | Loss: 0.00001381
Iteration 113/1000 | Loss: 0.00001381
Iteration 114/1000 | Loss: 0.00001381
Iteration 115/1000 | Loss: 0.00001381
Iteration 116/1000 | Loss: 0.00001381
Iteration 117/1000 | Loss: 0.00001381
Iteration 118/1000 | Loss: 0.00001380
Iteration 119/1000 | Loss: 0.00001380
Iteration 120/1000 | Loss: 0.00001380
Iteration 121/1000 | Loss: 0.00001380
Iteration 122/1000 | Loss: 0.00001380
Iteration 123/1000 | Loss: 0.00001380
Iteration 124/1000 | Loss: 0.00001380
Iteration 125/1000 | Loss: 0.00001379
Iteration 126/1000 | Loss: 0.00001379
Iteration 127/1000 | Loss: 0.00001379
Iteration 128/1000 | Loss: 0.00001379
Iteration 129/1000 | Loss: 0.00001379
Iteration 130/1000 | Loss: 0.00001379
Iteration 131/1000 | Loss: 0.00001379
Iteration 132/1000 | Loss: 0.00001379
Iteration 133/1000 | Loss: 0.00001379
Iteration 134/1000 | Loss: 0.00001379
Iteration 135/1000 | Loss: 0.00001378
Iteration 136/1000 | Loss: 0.00001378
Iteration 137/1000 | Loss: 0.00001378
Iteration 138/1000 | Loss: 0.00001378
Iteration 139/1000 | Loss: 0.00001378
Iteration 140/1000 | Loss: 0.00001378
Iteration 141/1000 | Loss: 0.00001378
Iteration 142/1000 | Loss: 0.00001378
Iteration 143/1000 | Loss: 0.00001378
Iteration 144/1000 | Loss: 0.00001378
Iteration 145/1000 | Loss: 0.00001378
Iteration 146/1000 | Loss: 0.00001378
Iteration 147/1000 | Loss: 0.00001378
Iteration 148/1000 | Loss: 0.00001378
Iteration 149/1000 | Loss: 0.00001378
Iteration 150/1000 | Loss: 0.00001377
Iteration 151/1000 | Loss: 0.00001377
Iteration 152/1000 | Loss: 0.00001377
Iteration 153/1000 | Loss: 0.00001377
Iteration 154/1000 | Loss: 0.00001377
Iteration 155/1000 | Loss: 0.00001377
Iteration 156/1000 | Loss: 0.00001377
Iteration 157/1000 | Loss: 0.00001377
Iteration 158/1000 | Loss: 0.00001377
Iteration 159/1000 | Loss: 0.00001377
Iteration 160/1000 | Loss: 0.00001377
Iteration 161/1000 | Loss: 0.00001377
Iteration 162/1000 | Loss: 0.00001377
Iteration 163/1000 | Loss: 0.00001377
Iteration 164/1000 | Loss: 0.00001377
Iteration 165/1000 | Loss: 0.00001377
Iteration 166/1000 | Loss: 0.00001377
Iteration 167/1000 | Loss: 0.00001377
Iteration 168/1000 | Loss: 0.00001377
Iteration 169/1000 | Loss: 0.00001377
Iteration 170/1000 | Loss: 0.00001377
Iteration 171/1000 | Loss: 0.00001377
Iteration 172/1000 | Loss: 0.00001377
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 172. Stopping optimization.
Last 5 losses: [1.3766345546173397e-05, 1.3766345546173397e-05, 1.3766345546173397e-05, 1.3766345546173397e-05, 1.3766345546173397e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3766345546173397e-05

Optimization complete. Final v2v error: 3.082451820373535 mm

Highest mean error: 3.303973913192749 mm for frame 109

Lowest mean error: 2.7613961696624756 mm for frame 65

Saving results

Total time: 38.04351449012756
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_003/1050/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_003/1050.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_003/1050
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00809442
Iteration 2/25 | Loss: 0.00135337
Iteration 3/25 | Loss: 0.00124536
Iteration 4/25 | Loss: 0.00123791
Iteration 5/25 | Loss: 0.00123549
Iteration 6/25 | Loss: 0.00123525
Iteration 7/25 | Loss: 0.00123525
Iteration 8/25 | Loss: 0.00123525
Iteration 9/25 | Loss: 0.00123525
Iteration 10/25 | Loss: 0.00123525
Iteration 11/25 | Loss: 0.00123525
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012352534104138613, 0.0012352534104138613, 0.0012352534104138613, 0.0012352534104138613, 0.0012352534104138613]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012352534104138613

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.74596548
Iteration 2/25 | Loss: 0.00112335
Iteration 3/25 | Loss: 0.00112331
Iteration 4/25 | Loss: 0.00112331
Iteration 5/25 | Loss: 0.00112331
Iteration 6/25 | Loss: 0.00112331
Iteration 7/25 | Loss: 0.00112331
Iteration 8/25 | Loss: 0.00112331
Iteration 9/25 | Loss: 0.00112331
Iteration 10/25 | Loss: 0.00112331
Iteration 11/25 | Loss: 0.00112331
Iteration 12/25 | Loss: 0.00112331
Iteration 13/25 | Loss: 0.00112331
Iteration 14/25 | Loss: 0.00112331
Iteration 15/25 | Loss: 0.00112331
Iteration 16/25 | Loss: 0.00112331
Iteration 17/25 | Loss: 0.00112331
Iteration 18/25 | Loss: 0.00112331
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0011233073892071843, 0.0011233073892071843, 0.0011233073892071843, 0.0011233073892071843, 0.0011233073892071843]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011233073892071843

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00112331
Iteration 2/1000 | Loss: 0.00003058
Iteration 3/1000 | Loss: 0.00002307
Iteration 4/1000 | Loss: 0.00002149
Iteration 5/1000 | Loss: 0.00002038
Iteration 6/1000 | Loss: 0.00001947
Iteration 7/1000 | Loss: 0.00001894
Iteration 8/1000 | Loss: 0.00001847
Iteration 9/1000 | Loss: 0.00001802
Iteration 10/1000 | Loss: 0.00001774
Iteration 11/1000 | Loss: 0.00001752
Iteration 12/1000 | Loss: 0.00001736
Iteration 13/1000 | Loss: 0.00001734
Iteration 14/1000 | Loss: 0.00001725
Iteration 15/1000 | Loss: 0.00001722
Iteration 16/1000 | Loss: 0.00001718
Iteration 17/1000 | Loss: 0.00001709
Iteration 18/1000 | Loss: 0.00001709
Iteration 19/1000 | Loss: 0.00001706
Iteration 20/1000 | Loss: 0.00001706
Iteration 21/1000 | Loss: 0.00001705
Iteration 22/1000 | Loss: 0.00001703
Iteration 23/1000 | Loss: 0.00001703
Iteration 24/1000 | Loss: 0.00001702
Iteration 25/1000 | Loss: 0.00001702
Iteration 26/1000 | Loss: 0.00001701
Iteration 27/1000 | Loss: 0.00001701
Iteration 28/1000 | Loss: 0.00001700
Iteration 29/1000 | Loss: 0.00001698
Iteration 30/1000 | Loss: 0.00001698
Iteration 31/1000 | Loss: 0.00001697
Iteration 32/1000 | Loss: 0.00001696
Iteration 33/1000 | Loss: 0.00001693
Iteration 34/1000 | Loss: 0.00001693
Iteration 35/1000 | Loss: 0.00001692
Iteration 36/1000 | Loss: 0.00001692
Iteration 37/1000 | Loss: 0.00001690
Iteration 38/1000 | Loss: 0.00001690
Iteration 39/1000 | Loss: 0.00001690
Iteration 40/1000 | Loss: 0.00001689
Iteration 41/1000 | Loss: 0.00001689
Iteration 42/1000 | Loss: 0.00001689
Iteration 43/1000 | Loss: 0.00001688
Iteration 44/1000 | Loss: 0.00001688
Iteration 45/1000 | Loss: 0.00001688
Iteration 46/1000 | Loss: 0.00001687
Iteration 47/1000 | Loss: 0.00001687
Iteration 48/1000 | Loss: 0.00001686
Iteration 49/1000 | Loss: 0.00001684
Iteration 50/1000 | Loss: 0.00001684
Iteration 51/1000 | Loss: 0.00001684
Iteration 52/1000 | Loss: 0.00001684
Iteration 53/1000 | Loss: 0.00001684
Iteration 54/1000 | Loss: 0.00001684
Iteration 55/1000 | Loss: 0.00001684
Iteration 56/1000 | Loss: 0.00001684
Iteration 57/1000 | Loss: 0.00001684
Iteration 58/1000 | Loss: 0.00001683
Iteration 59/1000 | Loss: 0.00001683
Iteration 60/1000 | Loss: 0.00001683
Iteration 61/1000 | Loss: 0.00001678
Iteration 62/1000 | Loss: 0.00001676
Iteration 63/1000 | Loss: 0.00001675
Iteration 64/1000 | Loss: 0.00001675
Iteration 65/1000 | Loss: 0.00001674
Iteration 66/1000 | Loss: 0.00001674
Iteration 67/1000 | Loss: 0.00001674
Iteration 68/1000 | Loss: 0.00001674
Iteration 69/1000 | Loss: 0.00001674
Iteration 70/1000 | Loss: 0.00001674
Iteration 71/1000 | Loss: 0.00001673
Iteration 72/1000 | Loss: 0.00001673
Iteration 73/1000 | Loss: 0.00001672
Iteration 74/1000 | Loss: 0.00001672
Iteration 75/1000 | Loss: 0.00001672
Iteration 76/1000 | Loss: 0.00001672
Iteration 77/1000 | Loss: 0.00001671
Iteration 78/1000 | Loss: 0.00001671
Iteration 79/1000 | Loss: 0.00001670
Iteration 80/1000 | Loss: 0.00001670
Iteration 81/1000 | Loss: 0.00001669
Iteration 82/1000 | Loss: 0.00001669
Iteration 83/1000 | Loss: 0.00001669
Iteration 84/1000 | Loss: 0.00001669
Iteration 85/1000 | Loss: 0.00001669
Iteration 86/1000 | Loss: 0.00001669
Iteration 87/1000 | Loss: 0.00001669
Iteration 88/1000 | Loss: 0.00001669
Iteration 89/1000 | Loss: 0.00001669
Iteration 90/1000 | Loss: 0.00001668
Iteration 91/1000 | Loss: 0.00001668
Iteration 92/1000 | Loss: 0.00001668
Iteration 93/1000 | Loss: 0.00001667
Iteration 94/1000 | Loss: 0.00001667
Iteration 95/1000 | Loss: 0.00001667
Iteration 96/1000 | Loss: 0.00001667
Iteration 97/1000 | Loss: 0.00001667
Iteration 98/1000 | Loss: 0.00001667
Iteration 99/1000 | Loss: 0.00001666
Iteration 100/1000 | Loss: 0.00001666
Iteration 101/1000 | Loss: 0.00001666
Iteration 102/1000 | Loss: 0.00001666
Iteration 103/1000 | Loss: 0.00001666
Iteration 104/1000 | Loss: 0.00001666
Iteration 105/1000 | Loss: 0.00001666
Iteration 106/1000 | Loss: 0.00001666
Iteration 107/1000 | Loss: 0.00001666
Iteration 108/1000 | Loss: 0.00001666
Iteration 109/1000 | Loss: 0.00001665
Iteration 110/1000 | Loss: 0.00001665
Iteration 111/1000 | Loss: 0.00001665
Iteration 112/1000 | Loss: 0.00001665
Iteration 113/1000 | Loss: 0.00001665
Iteration 114/1000 | Loss: 0.00001665
Iteration 115/1000 | Loss: 0.00001665
Iteration 116/1000 | Loss: 0.00001665
Iteration 117/1000 | Loss: 0.00001665
Iteration 118/1000 | Loss: 0.00001665
Iteration 119/1000 | Loss: 0.00001665
Iteration 120/1000 | Loss: 0.00001665
Iteration 121/1000 | Loss: 0.00001665
Iteration 122/1000 | Loss: 0.00001665
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 122. Stopping optimization.
Last 5 losses: [1.6652913473080844e-05, 1.6652913473080844e-05, 1.6652913473080844e-05, 1.6652913473080844e-05, 1.6652913473080844e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6652913473080844e-05

Optimization complete. Final v2v error: 3.469625234603882 mm

Highest mean error: 4.347735404968262 mm for frame 2

Lowest mean error: 2.9224612712860107 mm for frame 109

Saving results

Total time: 44.78693771362305
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_003/1080/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_003/1080.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_003/1080
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00408763
Iteration 2/25 | Loss: 0.00130082
Iteration 3/25 | Loss: 0.00121675
Iteration 4/25 | Loss: 0.00120937
Iteration 5/25 | Loss: 0.00120657
Iteration 6/25 | Loss: 0.00120657
Iteration 7/25 | Loss: 0.00120657
Iteration 8/25 | Loss: 0.00120657
Iteration 9/25 | Loss: 0.00120657
Iteration 10/25 | Loss: 0.00120657
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0012065712362527847, 0.0012065712362527847, 0.0012065712362527847, 0.0012065712362527847, 0.0012065712362527847]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012065712362527847

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.59893513
Iteration 2/25 | Loss: 0.00101776
Iteration 3/25 | Loss: 0.00101775
Iteration 4/25 | Loss: 0.00101775
Iteration 5/25 | Loss: 0.00101775
Iteration 6/25 | Loss: 0.00101775
Iteration 7/25 | Loss: 0.00101775
Iteration 8/25 | Loss: 0.00101775
Iteration 9/25 | Loss: 0.00101775
Iteration 10/25 | Loss: 0.00101775
Iteration 11/25 | Loss: 0.00101775
Iteration 12/25 | Loss: 0.00101775
Iteration 13/25 | Loss: 0.00101775
Iteration 14/25 | Loss: 0.00101775
Iteration 15/25 | Loss: 0.00101775
Iteration 16/25 | Loss: 0.00101775
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0010177528019994497, 0.0010177528019994497, 0.0010177528019994497, 0.0010177528019994497, 0.0010177528019994497]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010177528019994497

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00101775
Iteration 2/1000 | Loss: 0.00002602
Iteration 3/1000 | Loss: 0.00001707
Iteration 4/1000 | Loss: 0.00001340
Iteration 5/1000 | Loss: 0.00001242
Iteration 6/1000 | Loss: 0.00001169
Iteration 7/1000 | Loss: 0.00001138
Iteration 8/1000 | Loss: 0.00001108
Iteration 9/1000 | Loss: 0.00001067
Iteration 10/1000 | Loss: 0.00001044
Iteration 11/1000 | Loss: 0.00001032
Iteration 12/1000 | Loss: 0.00001018
Iteration 13/1000 | Loss: 0.00001014
Iteration 14/1000 | Loss: 0.00001013
Iteration 15/1000 | Loss: 0.00001013
Iteration 16/1000 | Loss: 0.00001012
Iteration 17/1000 | Loss: 0.00001008
Iteration 18/1000 | Loss: 0.00001006
Iteration 19/1000 | Loss: 0.00001005
Iteration 20/1000 | Loss: 0.00001004
Iteration 21/1000 | Loss: 0.00001002
Iteration 22/1000 | Loss: 0.00001001
Iteration 23/1000 | Loss: 0.00001000
Iteration 24/1000 | Loss: 0.00001000
Iteration 25/1000 | Loss: 0.00000999
Iteration 26/1000 | Loss: 0.00000999
Iteration 27/1000 | Loss: 0.00000996
Iteration 28/1000 | Loss: 0.00000995
Iteration 29/1000 | Loss: 0.00000995
Iteration 30/1000 | Loss: 0.00000995
Iteration 31/1000 | Loss: 0.00000994
Iteration 32/1000 | Loss: 0.00000994
Iteration 33/1000 | Loss: 0.00000992
Iteration 34/1000 | Loss: 0.00000992
Iteration 35/1000 | Loss: 0.00000991
Iteration 36/1000 | Loss: 0.00000991
Iteration 37/1000 | Loss: 0.00000990
Iteration 38/1000 | Loss: 0.00000989
Iteration 39/1000 | Loss: 0.00000989
Iteration 40/1000 | Loss: 0.00000987
Iteration 41/1000 | Loss: 0.00000987
Iteration 42/1000 | Loss: 0.00000987
Iteration 43/1000 | Loss: 0.00000987
Iteration 44/1000 | Loss: 0.00000987
Iteration 45/1000 | Loss: 0.00000987
Iteration 46/1000 | Loss: 0.00000987
Iteration 47/1000 | Loss: 0.00000987
Iteration 48/1000 | Loss: 0.00000986
Iteration 49/1000 | Loss: 0.00000986
Iteration 50/1000 | Loss: 0.00000986
Iteration 51/1000 | Loss: 0.00000986
Iteration 52/1000 | Loss: 0.00000985
Iteration 53/1000 | Loss: 0.00000985
Iteration 54/1000 | Loss: 0.00000983
Iteration 55/1000 | Loss: 0.00000983
Iteration 56/1000 | Loss: 0.00000983
Iteration 57/1000 | Loss: 0.00000983
Iteration 58/1000 | Loss: 0.00000982
Iteration 59/1000 | Loss: 0.00000982
Iteration 60/1000 | Loss: 0.00000982
Iteration 61/1000 | Loss: 0.00000981
Iteration 62/1000 | Loss: 0.00000981
Iteration 63/1000 | Loss: 0.00000981
Iteration 64/1000 | Loss: 0.00000981
Iteration 65/1000 | Loss: 0.00000981
Iteration 66/1000 | Loss: 0.00000981
Iteration 67/1000 | Loss: 0.00000981
Iteration 68/1000 | Loss: 0.00000981
Iteration 69/1000 | Loss: 0.00000980
Iteration 70/1000 | Loss: 0.00000980
Iteration 71/1000 | Loss: 0.00000980
Iteration 72/1000 | Loss: 0.00000980
Iteration 73/1000 | Loss: 0.00000979
Iteration 74/1000 | Loss: 0.00000979
Iteration 75/1000 | Loss: 0.00000979
Iteration 76/1000 | Loss: 0.00000979
Iteration 77/1000 | Loss: 0.00000979
Iteration 78/1000 | Loss: 0.00000979
Iteration 79/1000 | Loss: 0.00000979
Iteration 80/1000 | Loss: 0.00000979
Iteration 81/1000 | Loss: 0.00000978
Iteration 82/1000 | Loss: 0.00000978
Iteration 83/1000 | Loss: 0.00000978
Iteration 84/1000 | Loss: 0.00000978
Iteration 85/1000 | Loss: 0.00000978
Iteration 86/1000 | Loss: 0.00000978
Iteration 87/1000 | Loss: 0.00000977
Iteration 88/1000 | Loss: 0.00000977
Iteration 89/1000 | Loss: 0.00000977
Iteration 90/1000 | Loss: 0.00000976
Iteration 91/1000 | Loss: 0.00000976
Iteration 92/1000 | Loss: 0.00000975
Iteration 93/1000 | Loss: 0.00000975
Iteration 94/1000 | Loss: 0.00000975
Iteration 95/1000 | Loss: 0.00000974
Iteration 96/1000 | Loss: 0.00000974
Iteration 97/1000 | Loss: 0.00000974
Iteration 98/1000 | Loss: 0.00000973
Iteration 99/1000 | Loss: 0.00000973
Iteration 100/1000 | Loss: 0.00000973
Iteration 101/1000 | Loss: 0.00000973
Iteration 102/1000 | Loss: 0.00000973
Iteration 103/1000 | Loss: 0.00000973
Iteration 104/1000 | Loss: 0.00000973
Iteration 105/1000 | Loss: 0.00000973
Iteration 106/1000 | Loss: 0.00000973
Iteration 107/1000 | Loss: 0.00000973
Iteration 108/1000 | Loss: 0.00000972
Iteration 109/1000 | Loss: 0.00000972
Iteration 110/1000 | Loss: 0.00000972
Iteration 111/1000 | Loss: 0.00000972
Iteration 112/1000 | Loss: 0.00000971
Iteration 113/1000 | Loss: 0.00000971
Iteration 114/1000 | Loss: 0.00000971
Iteration 115/1000 | Loss: 0.00000970
Iteration 116/1000 | Loss: 0.00000970
Iteration 117/1000 | Loss: 0.00000970
Iteration 118/1000 | Loss: 0.00000970
Iteration 119/1000 | Loss: 0.00000970
Iteration 120/1000 | Loss: 0.00000970
Iteration 121/1000 | Loss: 0.00000970
Iteration 122/1000 | Loss: 0.00000970
Iteration 123/1000 | Loss: 0.00000970
Iteration 124/1000 | Loss: 0.00000970
Iteration 125/1000 | Loss: 0.00000970
Iteration 126/1000 | Loss: 0.00000970
Iteration 127/1000 | Loss: 0.00000970
Iteration 128/1000 | Loss: 0.00000970
Iteration 129/1000 | Loss: 0.00000970
Iteration 130/1000 | Loss: 0.00000970
Iteration 131/1000 | Loss: 0.00000970
Iteration 132/1000 | Loss: 0.00000970
Iteration 133/1000 | Loss: 0.00000970
Iteration 134/1000 | Loss: 0.00000970
Iteration 135/1000 | Loss: 0.00000970
Iteration 136/1000 | Loss: 0.00000970
Iteration 137/1000 | Loss: 0.00000970
Iteration 138/1000 | Loss: 0.00000970
Iteration 139/1000 | Loss: 0.00000970
Iteration 140/1000 | Loss: 0.00000970
Iteration 141/1000 | Loss: 0.00000970
Iteration 142/1000 | Loss: 0.00000970
Iteration 143/1000 | Loss: 0.00000970
Iteration 144/1000 | Loss: 0.00000970
Iteration 145/1000 | Loss: 0.00000970
Iteration 146/1000 | Loss: 0.00000970
Iteration 147/1000 | Loss: 0.00000970
Iteration 148/1000 | Loss: 0.00000970
Iteration 149/1000 | Loss: 0.00000970
Iteration 150/1000 | Loss: 0.00000970
Iteration 151/1000 | Loss: 0.00000970
Iteration 152/1000 | Loss: 0.00000970
Iteration 153/1000 | Loss: 0.00000970
Iteration 154/1000 | Loss: 0.00000970
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 154. Stopping optimization.
Last 5 losses: [9.696054803498555e-06, 9.696054803498555e-06, 9.696054803498555e-06, 9.696054803498555e-06, 9.696054803498555e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.696054803498555e-06

Optimization complete. Final v2v error: 2.6960580348968506 mm

Highest mean error: 2.8737070560455322 mm for frame 89

Lowest mean error: 2.5457091331481934 mm for frame 84

Saving results

Total time: 41.03259778022766
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_003/1019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_003/1019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_003/1019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01029105
Iteration 2/25 | Loss: 0.00188942
Iteration 3/25 | Loss: 0.00171670
Iteration 4/25 | Loss: 0.00154872
Iteration 5/25 | Loss: 0.00155198
Iteration 6/25 | Loss: 0.00148702
Iteration 7/25 | Loss: 0.00144239
Iteration 8/25 | Loss: 0.00141365
Iteration 9/25 | Loss: 0.00141025
Iteration 10/25 | Loss: 0.00140799
Iteration 11/25 | Loss: 0.00140344
Iteration 12/25 | Loss: 0.00139413
Iteration 13/25 | Loss: 0.00138468
Iteration 14/25 | Loss: 0.00137767
Iteration 15/25 | Loss: 0.00137288
Iteration 16/25 | Loss: 0.00137421
Iteration 17/25 | Loss: 0.00137611
Iteration 18/25 | Loss: 0.00137547
Iteration 19/25 | Loss: 0.00137217
Iteration 20/25 | Loss: 0.00137302
Iteration 21/25 | Loss: 0.00137101
Iteration 22/25 | Loss: 0.00136885
Iteration 23/25 | Loss: 0.00137169
Iteration 24/25 | Loss: 0.00136955
Iteration 25/25 | Loss: 0.00137207

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.60265326
Iteration 2/25 | Loss: 0.00166662
Iteration 3/25 | Loss: 0.00168846
Iteration 4/25 | Loss: 0.00172170
Iteration 5/25 | Loss: 0.00158856
Iteration 6/25 | Loss: 0.00158856
Iteration 7/25 | Loss: 0.00158856
Iteration 8/25 | Loss: 0.00158855
Iteration 9/25 | Loss: 0.00158855
Iteration 10/25 | Loss: 0.00158855
Iteration 11/25 | Loss: 0.00158855
Iteration 12/25 | Loss: 0.00158855
Iteration 13/25 | Loss: 0.00158855
Iteration 14/25 | Loss: 0.00158855
Iteration 15/25 | Loss: 0.00158855
Iteration 16/25 | Loss: 0.00158855
Iteration 17/25 | Loss: 0.00161042
Iteration 18/25 | Loss: 0.00158855
Iteration 19/25 | Loss: 0.00158855
Iteration 20/25 | Loss: 0.00158855
Iteration 21/25 | Loss: 0.00158855
Iteration 22/25 | Loss: 0.00158855
Iteration 23/25 | Loss: 0.00158855
Iteration 24/25 | Loss: 0.00158855
Iteration 25/25 | Loss: 0.00158855

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00158855
Iteration 2/1000 | Loss: 0.00112604
Iteration 3/1000 | Loss: 0.00012122
Iteration 4/1000 | Loss: 0.00021967
Iteration 5/1000 | Loss: 0.00016111
Iteration 6/1000 | Loss: 0.00015538
Iteration 7/1000 | Loss: 0.00010985
Iteration 8/1000 | Loss: 0.00084078
Iteration 9/1000 | Loss: 0.00072272
Iteration 10/1000 | Loss: 0.00059489
Iteration 11/1000 | Loss: 0.00008014
Iteration 12/1000 | Loss: 0.00057438
Iteration 13/1000 | Loss: 0.00066522
Iteration 14/1000 | Loss: 0.00046974
Iteration 15/1000 | Loss: 0.00062867
Iteration 16/1000 | Loss: 0.00058051
Iteration 17/1000 | Loss: 0.00052682
Iteration 18/1000 | Loss: 0.00035599
Iteration 19/1000 | Loss: 0.00035954
Iteration 20/1000 | Loss: 0.00085375
Iteration 21/1000 | Loss: 0.00051452
Iteration 22/1000 | Loss: 0.00048380
Iteration 23/1000 | Loss: 0.00022404
Iteration 24/1000 | Loss: 0.00078795
Iteration 25/1000 | Loss: 0.00052536
Iteration 26/1000 | Loss: 0.00031688
Iteration 27/1000 | Loss: 0.00056903
Iteration 28/1000 | Loss: 0.00112584
Iteration 29/1000 | Loss: 0.00040758
Iteration 30/1000 | Loss: 0.00055419
Iteration 31/1000 | Loss: 0.00055757
Iteration 32/1000 | Loss: 0.00044205
Iteration 33/1000 | Loss: 0.00047276
Iteration 34/1000 | Loss: 0.00151681
Iteration 35/1000 | Loss: 0.00016386
Iteration 36/1000 | Loss: 0.00004752
Iteration 37/1000 | Loss: 0.00005448
Iteration 38/1000 | Loss: 0.00011609
Iteration 39/1000 | Loss: 0.00074609
Iteration 40/1000 | Loss: 0.00082985
Iteration 41/1000 | Loss: 0.00062754
Iteration 42/1000 | Loss: 0.00006786
Iteration 43/1000 | Loss: 0.00006032
Iteration 44/1000 | Loss: 0.00004315
Iteration 45/1000 | Loss: 0.00003358
Iteration 46/1000 | Loss: 0.00005320
Iteration 47/1000 | Loss: 0.00010888
Iteration 48/1000 | Loss: 0.00004325
Iteration 49/1000 | Loss: 0.00005496
Iteration 50/1000 | Loss: 0.00007336
Iteration 51/1000 | Loss: 0.00005688
Iteration 52/1000 | Loss: 0.00004540
Iteration 53/1000 | Loss: 0.00003867
Iteration 54/1000 | Loss: 0.00004075
Iteration 55/1000 | Loss: 0.00003448
Iteration 56/1000 | Loss: 0.00011290
Iteration 57/1000 | Loss: 0.00007811
Iteration 58/1000 | Loss: 0.00004585
Iteration 59/1000 | Loss: 0.00010542
Iteration 60/1000 | Loss: 0.00006410
Iteration 61/1000 | Loss: 0.00004895
Iteration 62/1000 | Loss: 0.00007361
Iteration 63/1000 | Loss: 0.00003868
Iteration 64/1000 | Loss: 0.00004441
Iteration 65/1000 | Loss: 0.00010156
Iteration 66/1000 | Loss: 0.00007794
Iteration 67/1000 | Loss: 0.00009436
Iteration 68/1000 | Loss: 0.00052404
Iteration 69/1000 | Loss: 0.00052360
Iteration 70/1000 | Loss: 0.00005910
Iteration 71/1000 | Loss: 0.00009862
Iteration 72/1000 | Loss: 0.00003205
Iteration 73/1000 | Loss: 0.00002945
Iteration 74/1000 | Loss: 0.00032286
Iteration 75/1000 | Loss: 0.00051186
Iteration 76/1000 | Loss: 0.00026300
Iteration 77/1000 | Loss: 0.00049737
Iteration 78/1000 | Loss: 0.00008559
Iteration 79/1000 | Loss: 0.00024925
Iteration 80/1000 | Loss: 0.00029751
Iteration 81/1000 | Loss: 0.00018317
Iteration 82/1000 | Loss: 0.00010823
Iteration 83/1000 | Loss: 0.00021338
Iteration 84/1000 | Loss: 0.00024269
Iteration 85/1000 | Loss: 0.00026572
Iteration 86/1000 | Loss: 0.00026215
Iteration 87/1000 | Loss: 0.00016726
Iteration 88/1000 | Loss: 0.00013718
Iteration 89/1000 | Loss: 0.00005231
Iteration 90/1000 | Loss: 0.00003963
Iteration 91/1000 | Loss: 0.00003696
Iteration 92/1000 | Loss: 0.00003673
Iteration 93/1000 | Loss: 0.00019645
Iteration 94/1000 | Loss: 0.00024406
Iteration 95/1000 | Loss: 0.00004392
Iteration 96/1000 | Loss: 0.00037740
Iteration 97/1000 | Loss: 0.00035184
Iteration 98/1000 | Loss: 0.00013909
Iteration 99/1000 | Loss: 0.00031744
Iteration 100/1000 | Loss: 0.00015451
Iteration 101/1000 | Loss: 0.00029526
Iteration 102/1000 | Loss: 0.00053342
Iteration 103/1000 | Loss: 0.00041791
Iteration 104/1000 | Loss: 0.00013583
Iteration 105/1000 | Loss: 0.00004675
Iteration 106/1000 | Loss: 0.00002943
Iteration 107/1000 | Loss: 0.00002689
Iteration 108/1000 | Loss: 0.00002882
Iteration 109/1000 | Loss: 0.00002595
Iteration 110/1000 | Loss: 0.00002651
Iteration 111/1000 | Loss: 0.00002614
Iteration 112/1000 | Loss: 0.00003353
Iteration 113/1000 | Loss: 0.00002753
Iteration 114/1000 | Loss: 0.00002416
Iteration 115/1000 | Loss: 0.00002707
Iteration 116/1000 | Loss: 0.00002298
Iteration 117/1000 | Loss: 0.00002198
Iteration 118/1000 | Loss: 0.00002135
Iteration 119/1000 | Loss: 0.00002104
Iteration 120/1000 | Loss: 0.00002074
Iteration 121/1000 | Loss: 0.00002056
Iteration 122/1000 | Loss: 0.00002599
Iteration 123/1000 | Loss: 0.00002721
Iteration 124/1000 | Loss: 0.00002099
Iteration 125/1000 | Loss: 0.00002652
Iteration 126/1000 | Loss: 0.00002746
Iteration 127/1000 | Loss: 0.00002354
Iteration 128/1000 | Loss: 0.00002167
Iteration 129/1000 | Loss: 0.00002069
Iteration 130/1000 | Loss: 0.00002035
Iteration 131/1000 | Loss: 0.00002750
Iteration 132/1000 | Loss: 0.00002628
Iteration 133/1000 | Loss: 0.00002764
Iteration 134/1000 | Loss: 0.00002708
Iteration 135/1000 | Loss: 0.00002758
Iteration 136/1000 | Loss: 0.00002563
Iteration 137/1000 | Loss: 0.00005790
Iteration 138/1000 | Loss: 0.00046571
Iteration 139/1000 | Loss: 0.00015769
Iteration 140/1000 | Loss: 0.00012031
Iteration 141/1000 | Loss: 0.00010817
Iteration 142/1000 | Loss: 0.00004025
Iteration 143/1000 | Loss: 0.00013860
Iteration 144/1000 | Loss: 0.00004765
Iteration 145/1000 | Loss: 0.00002578
Iteration 146/1000 | Loss: 0.00002326
Iteration 147/1000 | Loss: 0.00002157
Iteration 148/1000 | Loss: 0.00002053
Iteration 149/1000 | Loss: 0.00002017
Iteration 150/1000 | Loss: 0.00001992
Iteration 151/1000 | Loss: 0.00001969
Iteration 152/1000 | Loss: 0.00001958
Iteration 153/1000 | Loss: 0.00001946
Iteration 154/1000 | Loss: 0.00001946
Iteration 155/1000 | Loss: 0.00001943
Iteration 156/1000 | Loss: 0.00001942
Iteration 157/1000 | Loss: 0.00001941
Iteration 158/1000 | Loss: 0.00001941
Iteration 159/1000 | Loss: 0.00001940
Iteration 160/1000 | Loss: 0.00001940
Iteration 161/1000 | Loss: 0.00001940
Iteration 162/1000 | Loss: 0.00001940
Iteration 163/1000 | Loss: 0.00001939
Iteration 164/1000 | Loss: 0.00001939
Iteration 165/1000 | Loss: 0.00001939
Iteration 166/1000 | Loss: 0.00001936
Iteration 167/1000 | Loss: 0.00001936
Iteration 168/1000 | Loss: 0.00001936
Iteration 169/1000 | Loss: 0.00001935
Iteration 170/1000 | Loss: 0.00001935
Iteration 171/1000 | Loss: 0.00001935
Iteration 172/1000 | Loss: 0.00001934
Iteration 173/1000 | Loss: 0.00001933
Iteration 174/1000 | Loss: 0.00001933
Iteration 175/1000 | Loss: 0.00001933
Iteration 176/1000 | Loss: 0.00001932
Iteration 177/1000 | Loss: 0.00001932
Iteration 178/1000 | Loss: 0.00001932
Iteration 179/1000 | Loss: 0.00001932
Iteration 180/1000 | Loss: 0.00001932
Iteration 181/1000 | Loss: 0.00001932
Iteration 182/1000 | Loss: 0.00001932
Iteration 183/1000 | Loss: 0.00001932
Iteration 184/1000 | Loss: 0.00001932
Iteration 185/1000 | Loss: 0.00001931
Iteration 186/1000 | Loss: 0.00001931
Iteration 187/1000 | Loss: 0.00001931
Iteration 188/1000 | Loss: 0.00001931
Iteration 189/1000 | Loss: 0.00001931
Iteration 190/1000 | Loss: 0.00001931
Iteration 191/1000 | Loss: 0.00001931
Iteration 192/1000 | Loss: 0.00001931
Iteration 193/1000 | Loss: 0.00001931
Iteration 194/1000 | Loss: 0.00001931
Iteration 195/1000 | Loss: 0.00001931
Iteration 196/1000 | Loss: 0.00001931
Iteration 197/1000 | Loss: 0.00001931
Iteration 198/1000 | Loss: 0.00001930
Iteration 199/1000 | Loss: 0.00001930
Iteration 200/1000 | Loss: 0.00001930
Iteration 201/1000 | Loss: 0.00001930
Iteration 202/1000 | Loss: 0.00001930
Iteration 203/1000 | Loss: 0.00001930
Iteration 204/1000 | Loss: 0.00001930
Iteration 205/1000 | Loss: 0.00001930
Iteration 206/1000 | Loss: 0.00001929
Iteration 207/1000 | Loss: 0.00001929
Iteration 208/1000 | Loss: 0.00001929
Iteration 209/1000 | Loss: 0.00001929
Iteration 210/1000 | Loss: 0.00001929
Iteration 211/1000 | Loss: 0.00001929
Iteration 212/1000 | Loss: 0.00001929
Iteration 213/1000 | Loss: 0.00001929
Iteration 214/1000 | Loss: 0.00001929
Iteration 215/1000 | Loss: 0.00001929
Iteration 216/1000 | Loss: 0.00001929
Iteration 217/1000 | Loss: 0.00001929
Iteration 218/1000 | Loss: 0.00001929
Iteration 219/1000 | Loss: 0.00001928
Iteration 220/1000 | Loss: 0.00001928
Iteration 221/1000 | Loss: 0.00001928
Iteration 222/1000 | Loss: 0.00001928
Iteration 223/1000 | Loss: 0.00001928
Iteration 224/1000 | Loss: 0.00001927
Iteration 225/1000 | Loss: 0.00001927
Iteration 226/1000 | Loss: 0.00001927
Iteration 227/1000 | Loss: 0.00001927
Iteration 228/1000 | Loss: 0.00001927
Iteration 229/1000 | Loss: 0.00001927
Iteration 230/1000 | Loss: 0.00001927
Iteration 231/1000 | Loss: 0.00001927
Iteration 232/1000 | Loss: 0.00001927
Iteration 233/1000 | Loss: 0.00001927
Iteration 234/1000 | Loss: 0.00001927
Iteration 235/1000 | Loss: 0.00001927
Iteration 236/1000 | Loss: 0.00001927
Iteration 237/1000 | Loss: 0.00001927
Iteration 238/1000 | Loss: 0.00001927
Iteration 239/1000 | Loss: 0.00001927
Iteration 240/1000 | Loss: 0.00001927
Iteration 241/1000 | Loss: 0.00001927
Iteration 242/1000 | Loss: 0.00001927
Iteration 243/1000 | Loss: 0.00001927
Iteration 244/1000 | Loss: 0.00001927
Iteration 245/1000 | Loss: 0.00001927
Iteration 246/1000 | Loss: 0.00001927
Iteration 247/1000 | Loss: 0.00001927
Iteration 248/1000 | Loss: 0.00001927
Iteration 249/1000 | Loss: 0.00001927
Iteration 250/1000 | Loss: 0.00001927
Iteration 251/1000 | Loss: 0.00001927
Iteration 252/1000 | Loss: 0.00001927
Iteration 253/1000 | Loss: 0.00001927
Iteration 254/1000 | Loss: 0.00001927
Iteration 255/1000 | Loss: 0.00001927
Iteration 256/1000 | Loss: 0.00001927
Iteration 257/1000 | Loss: 0.00001927
Iteration 258/1000 | Loss: 0.00001927
Iteration 259/1000 | Loss: 0.00001927
Iteration 260/1000 | Loss: 0.00001927
Iteration 261/1000 | Loss: 0.00001927
Iteration 262/1000 | Loss: 0.00001927
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 262. Stopping optimization.
Last 5 losses: [1.927000266732648e-05, 1.927000266732648e-05, 1.927000266732648e-05, 1.927000266732648e-05, 1.927000266732648e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.927000266732648e-05

Optimization complete. Final v2v error: 3.7329013347625732 mm

Highest mean error: 5.949458599090576 mm for frame 73

Lowest mean error: 3.439893960952759 mm for frame 63

Saving results

Total time: 304.7150926589966
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_003/1009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_003/1009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_003/1009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00876350
Iteration 2/25 | Loss: 0.00173614
Iteration 3/25 | Loss: 0.00141823
Iteration 4/25 | Loss: 0.00143286
Iteration 5/25 | Loss: 0.00137260
Iteration 6/25 | Loss: 0.00136232
Iteration 7/25 | Loss: 0.00134913
Iteration 8/25 | Loss: 0.00135447
Iteration 9/25 | Loss: 0.00135402
Iteration 10/25 | Loss: 0.00133141
Iteration 11/25 | Loss: 0.00133144
Iteration 12/25 | Loss: 0.00132110
Iteration 13/25 | Loss: 0.00131212
Iteration 14/25 | Loss: 0.00130732
Iteration 15/25 | Loss: 0.00131211
Iteration 16/25 | Loss: 0.00131383
Iteration 17/25 | Loss: 0.00131318
Iteration 18/25 | Loss: 0.00131501
Iteration 19/25 | Loss: 0.00131943
Iteration 20/25 | Loss: 0.00131639
Iteration 21/25 | Loss: 0.00131836
Iteration 22/25 | Loss: 0.00131695
Iteration 23/25 | Loss: 0.00132331
Iteration 24/25 | Loss: 0.00131122
Iteration 25/25 | Loss: 0.00130919

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.63366175
Iteration 2/25 | Loss: 0.00195617
Iteration 3/25 | Loss: 0.00195611
Iteration 4/25 | Loss: 0.00195611
Iteration 5/25 | Loss: 0.00195611
Iteration 6/25 | Loss: 0.00195611
Iteration 7/25 | Loss: 0.00195611
Iteration 8/25 | Loss: 0.00195611
Iteration 9/25 | Loss: 0.00195611
Iteration 10/25 | Loss: 0.00195611
Iteration 11/25 | Loss: 0.00195611
Iteration 12/25 | Loss: 0.00195611
Iteration 13/25 | Loss: 0.00195611
Iteration 14/25 | Loss: 0.00195611
Iteration 15/25 | Loss: 0.00195611
Iteration 16/25 | Loss: 0.00195611
Iteration 17/25 | Loss: 0.00195611
Iteration 18/25 | Loss: 0.00195611
Iteration 19/25 | Loss: 0.00195611
Iteration 20/25 | Loss: 0.00195611
Iteration 21/25 | Loss: 0.00195611
Iteration 22/25 | Loss: 0.00195611
Iteration 23/25 | Loss: 0.00195611
Iteration 24/25 | Loss: 0.00195611
Iteration 25/25 | Loss: 0.00195611

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00195611
Iteration 2/1000 | Loss: 0.00011974
Iteration 3/1000 | Loss: 0.00014744
Iteration 4/1000 | Loss: 0.00025157
Iteration 5/1000 | Loss: 0.00206002
Iteration 6/1000 | Loss: 0.00144423
Iteration 7/1000 | Loss: 0.00084109
Iteration 8/1000 | Loss: 0.00141331
Iteration 9/1000 | Loss: 0.00012820
Iteration 10/1000 | Loss: 0.00014519
Iteration 11/1000 | Loss: 0.00018761
Iteration 12/1000 | Loss: 0.00005829
Iteration 13/1000 | Loss: 0.00025370
Iteration 14/1000 | Loss: 0.00051072
Iteration 15/1000 | Loss: 0.00015073
Iteration 16/1000 | Loss: 0.00011400
Iteration 17/1000 | Loss: 0.00005002
Iteration 18/1000 | Loss: 0.00018977
Iteration 19/1000 | Loss: 0.00016732
Iteration 20/1000 | Loss: 0.00096643
Iteration 21/1000 | Loss: 0.00021946
Iteration 22/1000 | Loss: 0.00006982
Iteration 23/1000 | Loss: 0.00004699
Iteration 24/1000 | Loss: 0.00030096
Iteration 25/1000 | Loss: 0.00005828
Iteration 26/1000 | Loss: 0.00021066
Iteration 27/1000 | Loss: 0.00024572
Iteration 28/1000 | Loss: 0.00006096
Iteration 29/1000 | Loss: 0.00005117
Iteration 30/1000 | Loss: 0.00004834
Iteration 31/1000 | Loss: 0.00005248
Iteration 32/1000 | Loss: 0.00018637
Iteration 33/1000 | Loss: 0.00030517
Iteration 34/1000 | Loss: 0.00007803
Iteration 35/1000 | Loss: 0.00004872
Iteration 36/1000 | Loss: 0.00017502
Iteration 37/1000 | Loss: 0.00088862
Iteration 38/1000 | Loss: 0.00007088
Iteration 39/1000 | Loss: 0.00019158
Iteration 40/1000 | Loss: 0.00030753
Iteration 41/1000 | Loss: 0.00021565
Iteration 42/1000 | Loss: 0.00018620
Iteration 43/1000 | Loss: 0.00018808
Iteration 44/1000 | Loss: 0.00018045
Iteration 45/1000 | Loss: 0.00004947
Iteration 46/1000 | Loss: 0.00004332
Iteration 47/1000 | Loss: 0.00020276
Iteration 48/1000 | Loss: 0.00006230
Iteration 49/1000 | Loss: 0.00004815
Iteration 50/1000 | Loss: 0.00004371
Iteration 51/1000 | Loss: 0.00004353
Iteration 52/1000 | Loss: 0.00004323
Iteration 53/1000 | Loss: 0.00005790
Iteration 54/1000 | Loss: 0.00006004
Iteration 55/1000 | Loss: 0.00005779
Iteration 56/1000 | Loss: 0.00005824
Iteration 57/1000 | Loss: 0.00024538
Iteration 58/1000 | Loss: 0.00007694
Iteration 59/1000 | Loss: 0.00007458
Iteration 60/1000 | Loss: 0.00005988
Iteration 61/1000 | Loss: 0.00010125
Iteration 62/1000 | Loss: 0.00003318
Iteration 63/1000 | Loss: 0.00009205
Iteration 64/1000 | Loss: 0.00005045
Iteration 65/1000 | Loss: 0.00004276
Iteration 66/1000 | Loss: 0.00004648
Iteration 67/1000 | Loss: 0.00004388
Iteration 68/1000 | Loss: 0.00003263
Iteration 69/1000 | Loss: 0.00005436
Iteration 70/1000 | Loss: 0.00005140
Iteration 71/1000 | Loss: 0.00004365
Iteration 72/1000 | Loss: 0.00003256
Iteration 73/1000 | Loss: 0.00012077
Iteration 74/1000 | Loss: 0.00003181
Iteration 75/1000 | Loss: 0.00003092
Iteration 76/1000 | Loss: 0.00003041
Iteration 77/1000 | Loss: 0.00004235
Iteration 78/1000 | Loss: 0.00003088
Iteration 79/1000 | Loss: 0.00002990
Iteration 80/1000 | Loss: 0.00002927
Iteration 81/1000 | Loss: 0.00022195
Iteration 82/1000 | Loss: 0.00003267
Iteration 83/1000 | Loss: 0.00003454
Iteration 84/1000 | Loss: 0.00002903
Iteration 85/1000 | Loss: 0.00002881
Iteration 86/1000 | Loss: 0.00002878
Iteration 87/1000 | Loss: 0.00002871
Iteration 88/1000 | Loss: 0.00002870
Iteration 89/1000 | Loss: 0.00002870
Iteration 90/1000 | Loss: 0.00014311
Iteration 91/1000 | Loss: 0.00002934
Iteration 92/1000 | Loss: 0.00002868
Iteration 93/1000 | Loss: 0.00002862
Iteration 94/1000 | Loss: 0.00002854
Iteration 95/1000 | Loss: 0.00002846
Iteration 96/1000 | Loss: 0.00002844
Iteration 97/1000 | Loss: 0.00003501
Iteration 98/1000 | Loss: 0.00002898
Iteration 99/1000 | Loss: 0.00002861
Iteration 100/1000 | Loss: 0.00002837
Iteration 101/1000 | Loss: 0.00002827
Iteration 102/1000 | Loss: 0.00002826
Iteration 103/1000 | Loss: 0.00002824
Iteration 104/1000 | Loss: 0.00002818
Iteration 105/1000 | Loss: 0.00002811
Iteration 106/1000 | Loss: 0.00002811
Iteration 107/1000 | Loss: 0.00002810
Iteration 108/1000 | Loss: 0.00002810
Iteration 109/1000 | Loss: 0.00002807
Iteration 110/1000 | Loss: 0.00002807
Iteration 111/1000 | Loss: 0.00002807
Iteration 112/1000 | Loss: 0.00002807
Iteration 113/1000 | Loss: 0.00002806
Iteration 114/1000 | Loss: 0.00002805
Iteration 115/1000 | Loss: 0.00002804
Iteration 116/1000 | Loss: 0.00002803
Iteration 117/1000 | Loss: 0.00002788
Iteration 118/1000 | Loss: 0.00002783
Iteration 119/1000 | Loss: 0.00002780
Iteration 120/1000 | Loss: 0.00002780
Iteration 121/1000 | Loss: 0.00002779
Iteration 122/1000 | Loss: 0.00002778
Iteration 123/1000 | Loss: 0.00002766
Iteration 124/1000 | Loss: 0.00002763
Iteration 125/1000 | Loss: 0.00002763
Iteration 126/1000 | Loss: 0.00002762
Iteration 127/1000 | Loss: 0.00002762
Iteration 128/1000 | Loss: 0.00002756
Iteration 129/1000 | Loss: 0.00002755
Iteration 130/1000 | Loss: 0.00002755
Iteration 131/1000 | Loss: 0.00002754
Iteration 132/1000 | Loss: 0.00002751
Iteration 133/1000 | Loss: 0.00002751
Iteration 134/1000 | Loss: 0.00002751
Iteration 135/1000 | Loss: 0.00002750
Iteration 136/1000 | Loss: 0.00002750
Iteration 137/1000 | Loss: 0.00002750
Iteration 138/1000 | Loss: 0.00002749
Iteration 139/1000 | Loss: 0.00002749
Iteration 140/1000 | Loss: 0.00002749
Iteration 141/1000 | Loss: 0.00002748
Iteration 142/1000 | Loss: 0.00002748
Iteration 143/1000 | Loss: 0.00002748
Iteration 144/1000 | Loss: 0.00002747
Iteration 145/1000 | Loss: 0.00002747
Iteration 146/1000 | Loss: 0.00002747
Iteration 147/1000 | Loss: 0.00002747
Iteration 148/1000 | Loss: 0.00002747
Iteration 149/1000 | Loss: 0.00002747
Iteration 150/1000 | Loss: 0.00002747
Iteration 151/1000 | Loss: 0.00002747
Iteration 152/1000 | Loss: 0.00002747
Iteration 153/1000 | Loss: 0.00002747
Iteration 154/1000 | Loss: 0.00002747
Iteration 155/1000 | Loss: 0.00002747
Iteration 156/1000 | Loss: 0.00002746
Iteration 157/1000 | Loss: 0.00002746
Iteration 158/1000 | Loss: 0.00002746
Iteration 159/1000 | Loss: 0.00002746
Iteration 160/1000 | Loss: 0.00002746
Iteration 161/1000 | Loss: 0.00002746
Iteration 162/1000 | Loss: 0.00002746
Iteration 163/1000 | Loss: 0.00002746
Iteration 164/1000 | Loss: 0.00002746
Iteration 165/1000 | Loss: 0.00002745
Iteration 166/1000 | Loss: 0.00002745
Iteration 167/1000 | Loss: 0.00002744
Iteration 168/1000 | Loss: 0.00002743
Iteration 169/1000 | Loss: 0.00002742
Iteration 170/1000 | Loss: 0.00002742
Iteration 171/1000 | Loss: 0.00002741
Iteration 172/1000 | Loss: 0.00002741
Iteration 173/1000 | Loss: 0.00002741
Iteration 174/1000 | Loss: 0.00002741
Iteration 175/1000 | Loss: 0.00002741
Iteration 176/1000 | Loss: 0.00002740
Iteration 177/1000 | Loss: 0.00002740
Iteration 178/1000 | Loss: 0.00002740
Iteration 179/1000 | Loss: 0.00002740
Iteration 180/1000 | Loss: 0.00002739
Iteration 181/1000 | Loss: 0.00002739
Iteration 182/1000 | Loss: 0.00002739
Iteration 183/1000 | Loss: 0.00002738
Iteration 184/1000 | Loss: 0.00002738
Iteration 185/1000 | Loss: 0.00002738
Iteration 186/1000 | Loss: 0.00002737
Iteration 187/1000 | Loss: 0.00002737
Iteration 188/1000 | Loss: 0.00002737
Iteration 189/1000 | Loss: 0.00002736
Iteration 190/1000 | Loss: 0.00002735
Iteration 191/1000 | Loss: 0.00002735
Iteration 192/1000 | Loss: 0.00002734
Iteration 193/1000 | Loss: 0.00002734
Iteration 194/1000 | Loss: 0.00002734
Iteration 195/1000 | Loss: 0.00002733
Iteration 196/1000 | Loss: 0.00002733
Iteration 197/1000 | Loss: 0.00002733
Iteration 198/1000 | Loss: 0.00002733
Iteration 199/1000 | Loss: 0.00002732
Iteration 200/1000 | Loss: 0.00002732
Iteration 201/1000 | Loss: 0.00002732
Iteration 202/1000 | Loss: 0.00002732
Iteration 203/1000 | Loss: 0.00002731
Iteration 204/1000 | Loss: 0.00002731
Iteration 205/1000 | Loss: 0.00002731
Iteration 206/1000 | Loss: 0.00002730
Iteration 207/1000 | Loss: 0.00002730
Iteration 208/1000 | Loss: 0.00002730
Iteration 209/1000 | Loss: 0.00002730
Iteration 210/1000 | Loss: 0.00002730
Iteration 211/1000 | Loss: 0.00002730
Iteration 212/1000 | Loss: 0.00002730
Iteration 213/1000 | Loss: 0.00002729
Iteration 214/1000 | Loss: 0.00002729
Iteration 215/1000 | Loss: 0.00002729
Iteration 216/1000 | Loss: 0.00002729
Iteration 217/1000 | Loss: 0.00002729
Iteration 218/1000 | Loss: 0.00002729
Iteration 219/1000 | Loss: 0.00002729
Iteration 220/1000 | Loss: 0.00002729
Iteration 221/1000 | Loss: 0.00002728
Iteration 222/1000 | Loss: 0.00002728
Iteration 223/1000 | Loss: 0.00002728
Iteration 224/1000 | Loss: 0.00002728
Iteration 225/1000 | Loss: 0.00002727
Iteration 226/1000 | Loss: 0.00002727
Iteration 227/1000 | Loss: 0.00002727
Iteration 228/1000 | Loss: 0.00002727
Iteration 229/1000 | Loss: 0.00002727
Iteration 230/1000 | Loss: 0.00002727
Iteration 231/1000 | Loss: 0.00002727
Iteration 232/1000 | Loss: 0.00002727
Iteration 233/1000 | Loss: 0.00002726
Iteration 234/1000 | Loss: 0.00002726
Iteration 235/1000 | Loss: 0.00002726
Iteration 236/1000 | Loss: 0.00002726
Iteration 237/1000 | Loss: 0.00002726
Iteration 238/1000 | Loss: 0.00002726
Iteration 239/1000 | Loss: 0.00002726
Iteration 240/1000 | Loss: 0.00002726
Iteration 241/1000 | Loss: 0.00002726
Iteration 242/1000 | Loss: 0.00002726
Iteration 243/1000 | Loss: 0.00002726
Iteration 244/1000 | Loss: 0.00002726
Iteration 245/1000 | Loss: 0.00002726
Iteration 246/1000 | Loss: 0.00002726
Iteration 247/1000 | Loss: 0.00002726
Iteration 248/1000 | Loss: 0.00002726
Iteration 249/1000 | Loss: 0.00002726
Iteration 250/1000 | Loss: 0.00002726
Iteration 251/1000 | Loss: 0.00002726
Iteration 252/1000 | Loss: 0.00002726
Iteration 253/1000 | Loss: 0.00002726
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 253. Stopping optimization.
Last 5 losses: [2.7261830837232992e-05, 2.7261830837232992e-05, 2.7261830837232992e-05, 2.7261830837232992e-05, 2.7261830837232992e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.7261830837232992e-05

Optimization complete. Final v2v error: 4.280536651611328 mm

Highest mean error: 6.859376907348633 mm for frame 43

Lowest mean error: 3.0194313526153564 mm for frame 81

Saving results

Total time: 223.14656043052673
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_003/1032/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_003/1032.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_003/1032
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00770351
Iteration 2/25 | Loss: 0.00168076
Iteration 3/25 | Loss: 0.00142466
Iteration 4/25 | Loss: 0.00139987
Iteration 5/25 | Loss: 0.00139588
Iteration 6/25 | Loss: 0.00139584
Iteration 7/25 | Loss: 0.00139584
Iteration 8/25 | Loss: 0.00139584
Iteration 9/25 | Loss: 0.00139584
Iteration 10/25 | Loss: 0.00139584
Iteration 11/25 | Loss: 0.00139584
Iteration 12/25 | Loss: 0.00139584
Iteration 13/25 | Loss: 0.00139584
Iteration 14/25 | Loss: 0.00139584
Iteration 15/25 | Loss: 0.00139584
Iteration 16/25 | Loss: 0.00139584
Iteration 17/25 | Loss: 0.00139584
Iteration 18/25 | Loss: 0.00139584
Iteration 19/25 | Loss: 0.00139584
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0013958379859104753, 0.0013958379859104753, 0.0013958379859104753, 0.0013958379859104753, 0.0013958379859104753]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013958379859104753

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.13063073
Iteration 2/25 | Loss: 0.00128733
Iteration 3/25 | Loss: 0.00128729
Iteration 4/25 | Loss: 0.00128729
Iteration 5/25 | Loss: 0.00128729
Iteration 6/25 | Loss: 0.00128729
Iteration 7/25 | Loss: 0.00128729
Iteration 8/25 | Loss: 0.00128729
Iteration 9/25 | Loss: 0.00128729
Iteration 10/25 | Loss: 0.00128729
Iteration 11/25 | Loss: 0.00128729
Iteration 12/25 | Loss: 0.00128729
Iteration 13/25 | Loss: 0.00128729
Iteration 14/25 | Loss: 0.00128729
Iteration 15/25 | Loss: 0.00128729
Iteration 16/25 | Loss: 0.00128729
Iteration 17/25 | Loss: 0.00128729
Iteration 18/25 | Loss: 0.00128729
Iteration 19/25 | Loss: 0.00128729
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0012872876832261682, 0.0012872876832261682, 0.0012872876832261682, 0.0012872876832261682, 0.0012872876832261682]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012872876832261682

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00128729
Iteration 2/1000 | Loss: 0.00004616
Iteration 3/1000 | Loss: 0.00003245
Iteration 4/1000 | Loss: 0.00002979
Iteration 5/1000 | Loss: 0.00002875
Iteration 6/1000 | Loss: 0.00002793
Iteration 7/1000 | Loss: 0.00002732
Iteration 8/1000 | Loss: 0.00002692
Iteration 9/1000 | Loss: 0.00002648
Iteration 10/1000 | Loss: 0.00002602
Iteration 11/1000 | Loss: 0.00002560
Iteration 12/1000 | Loss: 0.00002534
Iteration 13/1000 | Loss: 0.00002505
Iteration 14/1000 | Loss: 0.00002476
Iteration 15/1000 | Loss: 0.00002452
Iteration 16/1000 | Loss: 0.00002437
Iteration 17/1000 | Loss: 0.00002421
Iteration 18/1000 | Loss: 0.00002418
Iteration 19/1000 | Loss: 0.00002415
Iteration 20/1000 | Loss: 0.00002408
Iteration 21/1000 | Loss: 0.00002407
Iteration 22/1000 | Loss: 0.00002405
Iteration 23/1000 | Loss: 0.00002404
Iteration 24/1000 | Loss: 0.00002404
Iteration 25/1000 | Loss: 0.00002403
Iteration 26/1000 | Loss: 0.00002402
Iteration 27/1000 | Loss: 0.00002401
Iteration 28/1000 | Loss: 0.00002401
Iteration 29/1000 | Loss: 0.00002401
Iteration 30/1000 | Loss: 0.00002401
Iteration 31/1000 | Loss: 0.00002401
Iteration 32/1000 | Loss: 0.00002401
Iteration 33/1000 | Loss: 0.00002401
Iteration 34/1000 | Loss: 0.00002401
Iteration 35/1000 | Loss: 0.00002401
Iteration 36/1000 | Loss: 0.00002401
Iteration 37/1000 | Loss: 0.00002401
Iteration 38/1000 | Loss: 0.00002400
Iteration 39/1000 | Loss: 0.00002400
Iteration 40/1000 | Loss: 0.00002400
Iteration 41/1000 | Loss: 0.00002400
Iteration 42/1000 | Loss: 0.00002399
Iteration 43/1000 | Loss: 0.00002399
Iteration 44/1000 | Loss: 0.00002398
Iteration 45/1000 | Loss: 0.00002398
Iteration 46/1000 | Loss: 0.00002398
Iteration 47/1000 | Loss: 0.00002398
Iteration 48/1000 | Loss: 0.00002398
Iteration 49/1000 | Loss: 0.00002397
Iteration 50/1000 | Loss: 0.00002395
Iteration 51/1000 | Loss: 0.00002395
Iteration 52/1000 | Loss: 0.00002395
Iteration 53/1000 | Loss: 0.00002395
Iteration 54/1000 | Loss: 0.00002395
Iteration 55/1000 | Loss: 0.00002393
Iteration 56/1000 | Loss: 0.00002393
Iteration 57/1000 | Loss: 0.00002393
Iteration 58/1000 | Loss: 0.00002393
Iteration 59/1000 | Loss: 0.00002393
Iteration 60/1000 | Loss: 0.00002393
Iteration 61/1000 | Loss: 0.00002392
Iteration 62/1000 | Loss: 0.00002392
Iteration 63/1000 | Loss: 0.00002392
Iteration 64/1000 | Loss: 0.00002392
Iteration 65/1000 | Loss: 0.00002392
Iteration 66/1000 | Loss: 0.00002392
Iteration 67/1000 | Loss: 0.00002392
Iteration 68/1000 | Loss: 0.00002392
Iteration 69/1000 | Loss: 0.00002392
Iteration 70/1000 | Loss: 0.00002392
Iteration 71/1000 | Loss: 0.00002391
Iteration 72/1000 | Loss: 0.00002391
Iteration 73/1000 | Loss: 0.00002391
Iteration 74/1000 | Loss: 0.00002391
Iteration 75/1000 | Loss: 0.00002390
Iteration 76/1000 | Loss: 0.00002390
Iteration 77/1000 | Loss: 0.00002390
Iteration 78/1000 | Loss: 0.00002390
Iteration 79/1000 | Loss: 0.00002390
Iteration 80/1000 | Loss: 0.00002390
Iteration 81/1000 | Loss: 0.00002390
Iteration 82/1000 | Loss: 0.00002389
Iteration 83/1000 | Loss: 0.00002389
Iteration 84/1000 | Loss: 0.00002389
Iteration 85/1000 | Loss: 0.00002389
Iteration 86/1000 | Loss: 0.00002389
Iteration 87/1000 | Loss: 0.00002389
Iteration 88/1000 | Loss: 0.00002389
Iteration 89/1000 | Loss: 0.00002389
Iteration 90/1000 | Loss: 0.00002389
Iteration 91/1000 | Loss: 0.00002389
Iteration 92/1000 | Loss: 0.00002389
Iteration 93/1000 | Loss: 0.00002388
Iteration 94/1000 | Loss: 0.00002388
Iteration 95/1000 | Loss: 0.00002388
Iteration 96/1000 | Loss: 0.00002388
Iteration 97/1000 | Loss: 0.00002388
Iteration 98/1000 | Loss: 0.00002388
Iteration 99/1000 | Loss: 0.00002388
Iteration 100/1000 | Loss: 0.00002388
Iteration 101/1000 | Loss: 0.00002388
Iteration 102/1000 | Loss: 0.00002387
Iteration 103/1000 | Loss: 0.00002387
Iteration 104/1000 | Loss: 0.00002387
Iteration 105/1000 | Loss: 0.00002387
Iteration 106/1000 | Loss: 0.00002387
Iteration 107/1000 | Loss: 0.00002387
Iteration 108/1000 | Loss: 0.00002387
Iteration 109/1000 | Loss: 0.00002387
Iteration 110/1000 | Loss: 0.00002387
Iteration 111/1000 | Loss: 0.00002387
Iteration 112/1000 | Loss: 0.00002387
Iteration 113/1000 | Loss: 0.00002387
Iteration 114/1000 | Loss: 0.00002387
Iteration 115/1000 | Loss: 0.00002386
Iteration 116/1000 | Loss: 0.00002386
Iteration 117/1000 | Loss: 0.00002386
Iteration 118/1000 | Loss: 0.00002386
Iteration 119/1000 | Loss: 0.00002386
Iteration 120/1000 | Loss: 0.00002386
Iteration 121/1000 | Loss: 0.00002385
Iteration 122/1000 | Loss: 0.00002385
Iteration 123/1000 | Loss: 0.00002385
Iteration 124/1000 | Loss: 0.00002385
Iteration 125/1000 | Loss: 0.00002385
Iteration 126/1000 | Loss: 0.00002385
Iteration 127/1000 | Loss: 0.00002385
Iteration 128/1000 | Loss: 0.00002385
Iteration 129/1000 | Loss: 0.00002385
Iteration 130/1000 | Loss: 0.00002384
Iteration 131/1000 | Loss: 0.00002384
Iteration 132/1000 | Loss: 0.00002384
Iteration 133/1000 | Loss: 0.00002384
Iteration 134/1000 | Loss: 0.00002384
Iteration 135/1000 | Loss: 0.00002384
Iteration 136/1000 | Loss: 0.00002384
Iteration 137/1000 | Loss: 0.00002384
Iteration 138/1000 | Loss: 0.00002384
Iteration 139/1000 | Loss: 0.00002383
Iteration 140/1000 | Loss: 0.00002383
Iteration 141/1000 | Loss: 0.00002383
Iteration 142/1000 | Loss: 0.00002383
Iteration 143/1000 | Loss: 0.00002383
Iteration 144/1000 | Loss: 0.00002383
Iteration 145/1000 | Loss: 0.00002383
Iteration 146/1000 | Loss: 0.00002383
Iteration 147/1000 | Loss: 0.00002383
Iteration 148/1000 | Loss: 0.00002383
Iteration 149/1000 | Loss: 0.00002383
Iteration 150/1000 | Loss: 0.00002383
Iteration 151/1000 | Loss: 0.00002383
Iteration 152/1000 | Loss: 0.00002382
Iteration 153/1000 | Loss: 0.00002382
Iteration 154/1000 | Loss: 0.00002382
Iteration 155/1000 | Loss: 0.00002382
Iteration 156/1000 | Loss: 0.00002381
Iteration 157/1000 | Loss: 0.00002381
Iteration 158/1000 | Loss: 0.00002381
Iteration 159/1000 | Loss: 0.00002381
Iteration 160/1000 | Loss: 0.00002380
Iteration 161/1000 | Loss: 0.00002380
Iteration 162/1000 | Loss: 0.00002380
Iteration 163/1000 | Loss: 0.00002380
Iteration 164/1000 | Loss: 0.00002380
Iteration 165/1000 | Loss: 0.00002380
Iteration 166/1000 | Loss: 0.00002380
Iteration 167/1000 | Loss: 0.00002380
Iteration 168/1000 | Loss: 0.00002380
Iteration 169/1000 | Loss: 0.00002380
Iteration 170/1000 | Loss: 0.00002380
Iteration 171/1000 | Loss: 0.00002380
Iteration 172/1000 | Loss: 0.00002379
Iteration 173/1000 | Loss: 0.00002379
Iteration 174/1000 | Loss: 0.00002379
Iteration 175/1000 | Loss: 0.00002379
Iteration 176/1000 | Loss: 0.00002379
Iteration 177/1000 | Loss: 0.00002379
Iteration 178/1000 | Loss: 0.00002379
Iteration 179/1000 | Loss: 0.00002379
Iteration 180/1000 | Loss: 0.00002379
Iteration 181/1000 | Loss: 0.00002379
Iteration 182/1000 | Loss: 0.00002379
Iteration 183/1000 | Loss: 0.00002379
Iteration 184/1000 | Loss: 0.00002379
Iteration 185/1000 | Loss: 0.00002379
Iteration 186/1000 | Loss: 0.00002379
Iteration 187/1000 | Loss: 0.00002379
Iteration 188/1000 | Loss: 0.00002379
Iteration 189/1000 | Loss: 0.00002379
Iteration 190/1000 | Loss: 0.00002379
Iteration 191/1000 | Loss: 0.00002379
Iteration 192/1000 | Loss: 0.00002379
Iteration 193/1000 | Loss: 0.00002379
Iteration 194/1000 | Loss: 0.00002379
Iteration 195/1000 | Loss: 0.00002378
Iteration 196/1000 | Loss: 0.00002378
Iteration 197/1000 | Loss: 0.00002378
Iteration 198/1000 | Loss: 0.00002378
Iteration 199/1000 | Loss: 0.00002378
Iteration 200/1000 | Loss: 0.00002378
Iteration 201/1000 | Loss: 0.00002378
Iteration 202/1000 | Loss: 0.00002378
Iteration 203/1000 | Loss: 0.00002378
Iteration 204/1000 | Loss: 0.00002378
Iteration 205/1000 | Loss: 0.00002378
Iteration 206/1000 | Loss: 0.00002378
Iteration 207/1000 | Loss: 0.00002378
Iteration 208/1000 | Loss: 0.00002378
Iteration 209/1000 | Loss: 0.00002378
Iteration 210/1000 | Loss: 0.00002378
Iteration 211/1000 | Loss: 0.00002378
Iteration 212/1000 | Loss: 0.00002378
Iteration 213/1000 | Loss: 0.00002378
Iteration 214/1000 | Loss: 0.00002378
Iteration 215/1000 | Loss: 0.00002378
Iteration 216/1000 | Loss: 0.00002378
Iteration 217/1000 | Loss: 0.00002378
Iteration 218/1000 | Loss: 0.00002378
Iteration 219/1000 | Loss: 0.00002378
Iteration 220/1000 | Loss: 0.00002378
Iteration 221/1000 | Loss: 0.00002378
Iteration 222/1000 | Loss: 0.00002378
Iteration 223/1000 | Loss: 0.00002378
Iteration 224/1000 | Loss: 0.00002378
Iteration 225/1000 | Loss: 0.00002378
Iteration 226/1000 | Loss: 0.00002378
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 226. Stopping optimization.
Last 5 losses: [2.3783581127645448e-05, 2.3783581127645448e-05, 2.3783581127645448e-05, 2.3783581127645448e-05, 2.3783581127645448e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3783581127645448e-05

Optimization complete. Final v2v error: 3.940382480621338 mm

Highest mean error: 5.032301425933838 mm for frame 209

Lowest mean error: 3.4435601234436035 mm for frame 64

Saving results

Total time: 52.97288274765015
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_003/1058/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_003/1058.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_003/1058
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00946139
Iteration 2/25 | Loss: 0.00287313
Iteration 3/25 | Loss: 0.00211492
Iteration 4/25 | Loss: 0.00205671
Iteration 5/25 | Loss: 0.00186846
Iteration 6/25 | Loss: 0.00166240
Iteration 7/25 | Loss: 0.00154884
Iteration 8/25 | Loss: 0.00149916
Iteration 9/25 | Loss: 0.00148921
Iteration 10/25 | Loss: 0.00149354
Iteration 11/25 | Loss: 0.00144210
Iteration 12/25 | Loss: 0.00142512
Iteration 13/25 | Loss: 0.00142061
Iteration 14/25 | Loss: 0.00141809
Iteration 15/25 | Loss: 0.00141714
Iteration 16/25 | Loss: 0.00141624
Iteration 17/25 | Loss: 0.00141576
Iteration 18/25 | Loss: 0.00141512
Iteration 19/25 | Loss: 0.00141453
Iteration 20/25 | Loss: 0.00141412
Iteration 21/25 | Loss: 0.00141383
Iteration 22/25 | Loss: 0.00141356
Iteration 23/25 | Loss: 0.00141330
Iteration 24/25 | Loss: 0.00141293
Iteration 25/25 | Loss: 0.00141264

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.31584895
Iteration 2/25 | Loss: 0.00142972
Iteration 3/25 | Loss: 0.00142972
Iteration 4/25 | Loss: 0.00142972
Iteration 5/25 | Loss: 0.00142972
Iteration 6/25 | Loss: 0.00142972
Iteration 7/25 | Loss: 0.00142972
Iteration 8/25 | Loss: 0.00142972
Iteration 9/25 | Loss: 0.00142972
Iteration 10/25 | Loss: 0.00142972
Iteration 11/25 | Loss: 0.00142972
Iteration 12/25 | Loss: 0.00142972
Iteration 13/25 | Loss: 0.00142972
Iteration 14/25 | Loss: 0.00142972
Iteration 15/25 | Loss: 0.00142972
Iteration 16/25 | Loss: 0.00142972
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.001429721130989492, 0.001429721130989492, 0.001429721130989492, 0.001429721130989492, 0.001429721130989492]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001429721130989492

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00142972
Iteration 2/1000 | Loss: 0.00148504
Iteration 3/1000 | Loss: 0.00060852
Iteration 4/1000 | Loss: 0.00096598
Iteration 5/1000 | Loss: 0.00017678
Iteration 6/1000 | Loss: 0.00008732
Iteration 7/1000 | Loss: 0.00035641
Iteration 8/1000 | Loss: 0.00012814
Iteration 9/1000 | Loss: 0.00007447
Iteration 10/1000 | Loss: 0.00038537
Iteration 11/1000 | Loss: 0.00019836
Iteration 12/1000 | Loss: 0.00037948
Iteration 13/1000 | Loss: 0.00020260
Iteration 14/1000 | Loss: 0.00006635
Iteration 15/1000 | Loss: 0.00006403
Iteration 16/1000 | Loss: 0.00099003
Iteration 17/1000 | Loss: 0.00113123
Iteration 18/1000 | Loss: 0.00104999
Iteration 19/1000 | Loss: 0.00009447
Iteration 20/1000 | Loss: 0.00038343
Iteration 21/1000 | Loss: 0.00006638
Iteration 22/1000 | Loss: 0.00106852
Iteration 23/1000 | Loss: 0.00139146
Iteration 24/1000 | Loss: 0.00035197
Iteration 25/1000 | Loss: 0.00079681
Iteration 26/1000 | Loss: 0.00042931
Iteration 27/1000 | Loss: 0.00008358
Iteration 28/1000 | Loss: 0.00005206
Iteration 29/1000 | Loss: 0.00096156
Iteration 30/1000 | Loss: 0.00048732
Iteration 31/1000 | Loss: 0.00039756
Iteration 32/1000 | Loss: 0.00032992
Iteration 33/1000 | Loss: 0.00091843
Iteration 34/1000 | Loss: 0.00005486
Iteration 35/1000 | Loss: 0.00007656
Iteration 36/1000 | Loss: 0.00004220
Iteration 37/1000 | Loss: 0.00018191
Iteration 38/1000 | Loss: 0.00030035
Iteration 39/1000 | Loss: 0.00003351
Iteration 40/1000 | Loss: 0.00081679
Iteration 41/1000 | Loss: 0.00008879
Iteration 42/1000 | Loss: 0.00010010
Iteration 43/1000 | Loss: 0.00003013
Iteration 44/1000 | Loss: 0.00039033
Iteration 45/1000 | Loss: 0.00003852
Iteration 46/1000 | Loss: 0.00002972
Iteration 47/1000 | Loss: 0.00002795
Iteration 48/1000 | Loss: 0.00002729
Iteration 49/1000 | Loss: 0.00002693
Iteration 50/1000 | Loss: 0.00002664
Iteration 51/1000 | Loss: 0.00002638
Iteration 52/1000 | Loss: 0.00002617
Iteration 53/1000 | Loss: 0.00002613
Iteration 54/1000 | Loss: 0.00002602
Iteration 55/1000 | Loss: 0.00002599
Iteration 56/1000 | Loss: 0.00002590
Iteration 57/1000 | Loss: 0.00002579
Iteration 58/1000 | Loss: 0.00002579
Iteration 59/1000 | Loss: 0.00002578
Iteration 60/1000 | Loss: 0.00002577
Iteration 61/1000 | Loss: 0.00002576
Iteration 62/1000 | Loss: 0.00002575
Iteration 63/1000 | Loss: 0.00002575
Iteration 64/1000 | Loss: 0.00002574
Iteration 65/1000 | Loss: 0.00002574
Iteration 66/1000 | Loss: 0.00002574
Iteration 67/1000 | Loss: 0.00002573
Iteration 68/1000 | Loss: 0.00002573
Iteration 69/1000 | Loss: 0.00002572
Iteration 70/1000 | Loss: 0.00002572
Iteration 71/1000 | Loss: 0.00002571
Iteration 72/1000 | Loss: 0.00002571
Iteration 73/1000 | Loss: 0.00002571
Iteration 74/1000 | Loss: 0.00002571
Iteration 75/1000 | Loss: 0.00002570
Iteration 76/1000 | Loss: 0.00002570
Iteration 77/1000 | Loss: 0.00002570
Iteration 78/1000 | Loss: 0.00002569
Iteration 79/1000 | Loss: 0.00002569
Iteration 80/1000 | Loss: 0.00002569
Iteration 81/1000 | Loss: 0.00002568
Iteration 82/1000 | Loss: 0.00002568
Iteration 83/1000 | Loss: 0.00002568
Iteration 84/1000 | Loss: 0.00002568
Iteration 85/1000 | Loss: 0.00002567
Iteration 86/1000 | Loss: 0.00002567
Iteration 87/1000 | Loss: 0.00002567
Iteration 88/1000 | Loss: 0.00002567
Iteration 89/1000 | Loss: 0.00002566
Iteration 90/1000 | Loss: 0.00002566
Iteration 91/1000 | Loss: 0.00002566
Iteration 92/1000 | Loss: 0.00002566
Iteration 93/1000 | Loss: 0.00002566
Iteration 94/1000 | Loss: 0.00002565
Iteration 95/1000 | Loss: 0.00002565
Iteration 96/1000 | Loss: 0.00002565
Iteration 97/1000 | Loss: 0.00002565
Iteration 98/1000 | Loss: 0.00002564
Iteration 99/1000 | Loss: 0.00002562
Iteration 100/1000 | Loss: 0.00002562
Iteration 101/1000 | Loss: 0.00002562
Iteration 102/1000 | Loss: 0.00002562
Iteration 103/1000 | Loss: 0.00002562
Iteration 104/1000 | Loss: 0.00002562
Iteration 105/1000 | Loss: 0.00002561
Iteration 106/1000 | Loss: 0.00002561
Iteration 107/1000 | Loss: 0.00002561
Iteration 108/1000 | Loss: 0.00002561
Iteration 109/1000 | Loss: 0.00002561
Iteration 110/1000 | Loss: 0.00002560
Iteration 111/1000 | Loss: 0.00002560
Iteration 112/1000 | Loss: 0.00002560
Iteration 113/1000 | Loss: 0.00002560
Iteration 114/1000 | Loss: 0.00002560
Iteration 115/1000 | Loss: 0.00002560
Iteration 116/1000 | Loss: 0.00002559
Iteration 117/1000 | Loss: 0.00002559
Iteration 118/1000 | Loss: 0.00002559
Iteration 119/1000 | Loss: 0.00002559
Iteration 120/1000 | Loss: 0.00002559
Iteration 121/1000 | Loss: 0.00002559
Iteration 122/1000 | Loss: 0.00002559
Iteration 123/1000 | Loss: 0.00002559
Iteration 124/1000 | Loss: 0.00002559
Iteration 125/1000 | Loss: 0.00002559
Iteration 126/1000 | Loss: 0.00002558
Iteration 127/1000 | Loss: 0.00002558
Iteration 128/1000 | Loss: 0.00002558
Iteration 129/1000 | Loss: 0.00002558
Iteration 130/1000 | Loss: 0.00002558
Iteration 131/1000 | Loss: 0.00002558
Iteration 132/1000 | Loss: 0.00002558
Iteration 133/1000 | Loss: 0.00002558
Iteration 134/1000 | Loss: 0.00002558
Iteration 135/1000 | Loss: 0.00002558
Iteration 136/1000 | Loss: 0.00002558
Iteration 137/1000 | Loss: 0.00002558
Iteration 138/1000 | Loss: 0.00002558
Iteration 139/1000 | Loss: 0.00002558
Iteration 140/1000 | Loss: 0.00002558
Iteration 141/1000 | Loss: 0.00002557
Iteration 142/1000 | Loss: 0.00002557
Iteration 143/1000 | Loss: 0.00002557
Iteration 144/1000 | Loss: 0.00002557
Iteration 145/1000 | Loss: 0.00002557
Iteration 146/1000 | Loss: 0.00002557
Iteration 147/1000 | Loss: 0.00002557
Iteration 148/1000 | Loss: 0.00002557
Iteration 149/1000 | Loss: 0.00002557
Iteration 150/1000 | Loss: 0.00002556
Iteration 151/1000 | Loss: 0.00002556
Iteration 152/1000 | Loss: 0.00002556
Iteration 153/1000 | Loss: 0.00002556
Iteration 154/1000 | Loss: 0.00002556
Iteration 155/1000 | Loss: 0.00002556
Iteration 156/1000 | Loss: 0.00002556
Iteration 157/1000 | Loss: 0.00002556
Iteration 158/1000 | Loss: 0.00002556
Iteration 159/1000 | Loss: 0.00002556
Iteration 160/1000 | Loss: 0.00002556
Iteration 161/1000 | Loss: 0.00002556
Iteration 162/1000 | Loss: 0.00002556
Iteration 163/1000 | Loss: 0.00002556
Iteration 164/1000 | Loss: 0.00002556
Iteration 165/1000 | Loss: 0.00002556
Iteration 166/1000 | Loss: 0.00002556
Iteration 167/1000 | Loss: 0.00002556
Iteration 168/1000 | Loss: 0.00002556
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 168. Stopping optimization.
Last 5 losses: [2.556208346504718e-05, 2.556208346504718e-05, 2.556208346504718e-05, 2.556208346504718e-05, 2.556208346504718e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.556208346504718e-05

Optimization complete. Final v2v error: 4.072661876678467 mm

Highest mean error: 11.640445709228516 mm for frame 131

Lowest mean error: 3.863621711730957 mm for frame 97

Saving results

Total time: 127.40121269226074
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_003/1044/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_003/1044.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_003/1044
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00558149
Iteration 2/25 | Loss: 0.00140720
Iteration 3/25 | Loss: 0.00131620
Iteration 4/25 | Loss: 0.00131049
Iteration 5/25 | Loss: 0.00130846
Iteration 6/25 | Loss: 0.00130846
Iteration 7/25 | Loss: 0.00130846
Iteration 8/25 | Loss: 0.00130846
Iteration 9/25 | Loss: 0.00130846
Iteration 10/25 | Loss: 0.00130846
Iteration 11/25 | Loss: 0.00130846
Iteration 12/25 | Loss: 0.00130846
Iteration 13/25 | Loss: 0.00130846
Iteration 14/25 | Loss: 0.00130846
Iteration 15/25 | Loss: 0.00130846
Iteration 16/25 | Loss: 0.00130846
Iteration 17/25 | Loss: 0.00130846
Iteration 18/25 | Loss: 0.00130846
Iteration 19/25 | Loss: 0.00130846
Iteration 20/25 | Loss: 0.00130846
Iteration 21/25 | Loss: 0.00130846
Iteration 22/25 | Loss: 0.00130846
Iteration 23/25 | Loss: 0.00130846
Iteration 24/25 | Loss: 0.00130846
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0013084649108350277, 0.0013084649108350277, 0.0013084649108350277, 0.0013084649108350277, 0.0013084649108350277]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013084649108350277

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.77928495
Iteration 2/25 | Loss: 0.00103125
Iteration 3/25 | Loss: 0.00103051
Iteration 4/25 | Loss: 0.00103051
Iteration 5/25 | Loss: 0.00103051
Iteration 6/25 | Loss: 0.00103051
Iteration 7/25 | Loss: 0.00103051
Iteration 8/25 | Loss: 0.00103051
Iteration 9/25 | Loss: 0.00103051
Iteration 10/25 | Loss: 0.00103051
Iteration 11/25 | Loss: 0.00103051
Iteration 12/25 | Loss: 0.00103051
Iteration 13/25 | Loss: 0.00103051
Iteration 14/25 | Loss: 0.00103051
Iteration 15/25 | Loss: 0.00103051
Iteration 16/25 | Loss: 0.00103051
Iteration 17/25 | Loss: 0.00103051
Iteration 18/25 | Loss: 0.00103051
Iteration 19/25 | Loss: 0.00103051
Iteration 20/25 | Loss: 0.00103051
Iteration 21/25 | Loss: 0.00103051
Iteration 22/25 | Loss: 0.00103051
Iteration 23/25 | Loss: 0.00103051
Iteration 24/25 | Loss: 0.00103051
Iteration 25/25 | Loss: 0.00103051
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.001030509825795889, 0.001030509825795889, 0.001030509825795889, 0.001030509825795889, 0.001030509825795889]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001030509825795889

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00103051
Iteration 2/1000 | Loss: 0.00002902
Iteration 3/1000 | Loss: 0.00002251
Iteration 4/1000 | Loss: 0.00002077
Iteration 5/1000 | Loss: 0.00001974
Iteration 6/1000 | Loss: 0.00001930
Iteration 7/1000 | Loss: 0.00001900
Iteration 8/1000 | Loss: 0.00001870
Iteration 9/1000 | Loss: 0.00001837
Iteration 10/1000 | Loss: 0.00001818
Iteration 11/1000 | Loss: 0.00001795
Iteration 12/1000 | Loss: 0.00001771
Iteration 13/1000 | Loss: 0.00001752
Iteration 14/1000 | Loss: 0.00001739
Iteration 15/1000 | Loss: 0.00001730
Iteration 16/1000 | Loss: 0.00001714
Iteration 17/1000 | Loss: 0.00001711
Iteration 18/1000 | Loss: 0.00001702
Iteration 19/1000 | Loss: 0.00001691
Iteration 20/1000 | Loss: 0.00001691
Iteration 21/1000 | Loss: 0.00001688
Iteration 22/1000 | Loss: 0.00001685
Iteration 23/1000 | Loss: 0.00001684
Iteration 24/1000 | Loss: 0.00001684
Iteration 25/1000 | Loss: 0.00001683
Iteration 26/1000 | Loss: 0.00001679
Iteration 27/1000 | Loss: 0.00001678
Iteration 28/1000 | Loss: 0.00001676
Iteration 29/1000 | Loss: 0.00001676
Iteration 30/1000 | Loss: 0.00001676
Iteration 31/1000 | Loss: 0.00001675
Iteration 32/1000 | Loss: 0.00001675
Iteration 33/1000 | Loss: 0.00001675
Iteration 34/1000 | Loss: 0.00001675
Iteration 35/1000 | Loss: 0.00001675
Iteration 36/1000 | Loss: 0.00001674
Iteration 37/1000 | Loss: 0.00001674
Iteration 38/1000 | Loss: 0.00001673
Iteration 39/1000 | Loss: 0.00001673
Iteration 40/1000 | Loss: 0.00001673
Iteration 41/1000 | Loss: 0.00001673
Iteration 42/1000 | Loss: 0.00001672
Iteration 43/1000 | Loss: 0.00001672
Iteration 44/1000 | Loss: 0.00001672
Iteration 45/1000 | Loss: 0.00001671
Iteration 46/1000 | Loss: 0.00001671
Iteration 47/1000 | Loss: 0.00001671
Iteration 48/1000 | Loss: 0.00001671
Iteration 49/1000 | Loss: 0.00001671
Iteration 50/1000 | Loss: 0.00001671
Iteration 51/1000 | Loss: 0.00001671
Iteration 52/1000 | Loss: 0.00001671
Iteration 53/1000 | Loss: 0.00001670
Iteration 54/1000 | Loss: 0.00001670
Iteration 55/1000 | Loss: 0.00001670
Iteration 56/1000 | Loss: 0.00001670
Iteration 57/1000 | Loss: 0.00001669
Iteration 58/1000 | Loss: 0.00001669
Iteration 59/1000 | Loss: 0.00001669
Iteration 60/1000 | Loss: 0.00001668
Iteration 61/1000 | Loss: 0.00001668
Iteration 62/1000 | Loss: 0.00001668
Iteration 63/1000 | Loss: 0.00001668
Iteration 64/1000 | Loss: 0.00001668
Iteration 65/1000 | Loss: 0.00001668
Iteration 66/1000 | Loss: 0.00001668
Iteration 67/1000 | Loss: 0.00001668
Iteration 68/1000 | Loss: 0.00001668
Iteration 69/1000 | Loss: 0.00001667
Iteration 70/1000 | Loss: 0.00001667
Iteration 71/1000 | Loss: 0.00001667
Iteration 72/1000 | Loss: 0.00001667
Iteration 73/1000 | Loss: 0.00001667
Iteration 74/1000 | Loss: 0.00001667
Iteration 75/1000 | Loss: 0.00001667
Iteration 76/1000 | Loss: 0.00001666
Iteration 77/1000 | Loss: 0.00001666
Iteration 78/1000 | Loss: 0.00001666
Iteration 79/1000 | Loss: 0.00001666
Iteration 80/1000 | Loss: 0.00001666
Iteration 81/1000 | Loss: 0.00001666
Iteration 82/1000 | Loss: 0.00001666
Iteration 83/1000 | Loss: 0.00001666
Iteration 84/1000 | Loss: 0.00001666
Iteration 85/1000 | Loss: 0.00001666
Iteration 86/1000 | Loss: 0.00001665
Iteration 87/1000 | Loss: 0.00001665
Iteration 88/1000 | Loss: 0.00001665
Iteration 89/1000 | Loss: 0.00001664
Iteration 90/1000 | Loss: 0.00001664
Iteration 91/1000 | Loss: 0.00001664
Iteration 92/1000 | Loss: 0.00001664
Iteration 93/1000 | Loss: 0.00001664
Iteration 94/1000 | Loss: 0.00001664
Iteration 95/1000 | Loss: 0.00001664
Iteration 96/1000 | Loss: 0.00001664
Iteration 97/1000 | Loss: 0.00001664
Iteration 98/1000 | Loss: 0.00001664
Iteration 99/1000 | Loss: 0.00001664
Iteration 100/1000 | Loss: 0.00001664
Iteration 101/1000 | Loss: 0.00001664
Iteration 102/1000 | Loss: 0.00001664
Iteration 103/1000 | Loss: 0.00001664
Iteration 104/1000 | Loss: 0.00001664
Iteration 105/1000 | Loss: 0.00001664
Iteration 106/1000 | Loss: 0.00001664
Iteration 107/1000 | Loss: 0.00001664
Iteration 108/1000 | Loss: 0.00001664
Iteration 109/1000 | Loss: 0.00001664
Iteration 110/1000 | Loss: 0.00001664
Iteration 111/1000 | Loss: 0.00001664
Iteration 112/1000 | Loss: 0.00001664
Iteration 113/1000 | Loss: 0.00001664
Iteration 114/1000 | Loss: 0.00001664
Iteration 115/1000 | Loss: 0.00001664
Iteration 116/1000 | Loss: 0.00001664
Iteration 117/1000 | Loss: 0.00001664
Iteration 118/1000 | Loss: 0.00001664
Iteration 119/1000 | Loss: 0.00001664
Iteration 120/1000 | Loss: 0.00001664
Iteration 121/1000 | Loss: 0.00001664
Iteration 122/1000 | Loss: 0.00001664
Iteration 123/1000 | Loss: 0.00001664
Iteration 124/1000 | Loss: 0.00001664
Iteration 125/1000 | Loss: 0.00001664
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 125. Stopping optimization.
Last 5 losses: [1.6635918655083515e-05, 1.6635918655083515e-05, 1.6635918655083515e-05, 1.6635918655083515e-05, 1.6635918655083515e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6635918655083515e-05

Optimization complete. Final v2v error: 3.3982644081115723 mm

Highest mean error: 3.6297049522399902 mm for frame 137

Lowest mean error: 3.213470697402954 mm for frame 16

Saving results

Total time: 45.601886510849
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_003/1077/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_003/1077.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_003/1077
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00405189
Iteration 2/25 | Loss: 0.00131136
Iteration 3/25 | Loss: 0.00121656
Iteration 4/25 | Loss: 0.00120790
Iteration 5/25 | Loss: 0.00120577
Iteration 6/25 | Loss: 0.00120503
Iteration 7/25 | Loss: 0.00120503
Iteration 8/25 | Loss: 0.00120503
Iteration 9/25 | Loss: 0.00120503
Iteration 10/25 | Loss: 0.00120503
Iteration 11/25 | Loss: 0.00120503
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012050330406054854, 0.0012050330406054854, 0.0012050330406054854, 0.0012050330406054854, 0.0012050330406054854]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012050330406054854

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.29386270
Iteration 2/25 | Loss: 0.00153303
Iteration 3/25 | Loss: 0.00153301
Iteration 4/25 | Loss: 0.00153301
Iteration 5/25 | Loss: 0.00153300
Iteration 6/25 | Loss: 0.00153300
Iteration 7/25 | Loss: 0.00153300
Iteration 8/25 | Loss: 0.00153300
Iteration 9/25 | Loss: 0.00153300
Iteration 10/25 | Loss: 0.00153300
Iteration 11/25 | Loss: 0.00153300
Iteration 12/25 | Loss: 0.00153300
Iteration 13/25 | Loss: 0.00153300
Iteration 14/25 | Loss: 0.00153300
Iteration 15/25 | Loss: 0.00153300
Iteration 16/25 | Loss: 0.00153300
Iteration 17/25 | Loss: 0.00153300
Iteration 18/25 | Loss: 0.00153300
Iteration 19/25 | Loss: 0.00153300
Iteration 20/25 | Loss: 0.00153300
Iteration 21/25 | Loss: 0.00153300
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0015330014284700155, 0.0015330014284700155, 0.0015330014284700155, 0.0015330014284700155, 0.0015330014284700155]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015330014284700155

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00153300
Iteration 2/1000 | Loss: 0.00002361
Iteration 3/1000 | Loss: 0.00001519
Iteration 4/1000 | Loss: 0.00001326
Iteration 5/1000 | Loss: 0.00001227
Iteration 6/1000 | Loss: 0.00001174
Iteration 7/1000 | Loss: 0.00001138
Iteration 8/1000 | Loss: 0.00001120
Iteration 9/1000 | Loss: 0.00001100
Iteration 10/1000 | Loss: 0.00001092
Iteration 11/1000 | Loss: 0.00001091
Iteration 12/1000 | Loss: 0.00001078
Iteration 13/1000 | Loss: 0.00001075
Iteration 14/1000 | Loss: 0.00001073
Iteration 15/1000 | Loss: 0.00001072
Iteration 16/1000 | Loss: 0.00001072
Iteration 17/1000 | Loss: 0.00001072
Iteration 18/1000 | Loss: 0.00001071
Iteration 19/1000 | Loss: 0.00001068
Iteration 20/1000 | Loss: 0.00001067
Iteration 21/1000 | Loss: 0.00001065
Iteration 22/1000 | Loss: 0.00001062
Iteration 23/1000 | Loss: 0.00001060
Iteration 24/1000 | Loss: 0.00001060
Iteration 25/1000 | Loss: 0.00001058
Iteration 26/1000 | Loss: 0.00001058
Iteration 27/1000 | Loss: 0.00001057
Iteration 28/1000 | Loss: 0.00001057
Iteration 29/1000 | Loss: 0.00001056
Iteration 30/1000 | Loss: 0.00001056
Iteration 31/1000 | Loss: 0.00001055
Iteration 32/1000 | Loss: 0.00001055
Iteration 33/1000 | Loss: 0.00001054
Iteration 34/1000 | Loss: 0.00001053
Iteration 35/1000 | Loss: 0.00001053
Iteration 36/1000 | Loss: 0.00001052
Iteration 37/1000 | Loss: 0.00001051
Iteration 38/1000 | Loss: 0.00001050
Iteration 39/1000 | Loss: 0.00001047
Iteration 40/1000 | Loss: 0.00001047
Iteration 41/1000 | Loss: 0.00001046
Iteration 42/1000 | Loss: 0.00001041
Iteration 43/1000 | Loss: 0.00001039
Iteration 44/1000 | Loss: 0.00001037
Iteration 45/1000 | Loss: 0.00001037
Iteration 46/1000 | Loss: 0.00001037
Iteration 47/1000 | Loss: 0.00001037
Iteration 48/1000 | Loss: 0.00001037
Iteration 49/1000 | Loss: 0.00001037
Iteration 50/1000 | Loss: 0.00001037
Iteration 51/1000 | Loss: 0.00001037
Iteration 52/1000 | Loss: 0.00001037
Iteration 53/1000 | Loss: 0.00001037
Iteration 54/1000 | Loss: 0.00001036
Iteration 55/1000 | Loss: 0.00001036
Iteration 56/1000 | Loss: 0.00001035
Iteration 57/1000 | Loss: 0.00001034
Iteration 58/1000 | Loss: 0.00001034
Iteration 59/1000 | Loss: 0.00001034
Iteration 60/1000 | Loss: 0.00001033
Iteration 61/1000 | Loss: 0.00001033
Iteration 62/1000 | Loss: 0.00001032
Iteration 63/1000 | Loss: 0.00001032
Iteration 64/1000 | Loss: 0.00001031
Iteration 65/1000 | Loss: 0.00001031
Iteration 66/1000 | Loss: 0.00001031
Iteration 67/1000 | Loss: 0.00001031
Iteration 68/1000 | Loss: 0.00001031
Iteration 69/1000 | Loss: 0.00001030
Iteration 70/1000 | Loss: 0.00001030
Iteration 71/1000 | Loss: 0.00001030
Iteration 72/1000 | Loss: 0.00001030
Iteration 73/1000 | Loss: 0.00001030
Iteration 74/1000 | Loss: 0.00001030
Iteration 75/1000 | Loss: 0.00001029
Iteration 76/1000 | Loss: 0.00001029
Iteration 77/1000 | Loss: 0.00001029
Iteration 78/1000 | Loss: 0.00001029
Iteration 79/1000 | Loss: 0.00001029
Iteration 80/1000 | Loss: 0.00001029
Iteration 81/1000 | Loss: 0.00001029
Iteration 82/1000 | Loss: 0.00001028
Iteration 83/1000 | Loss: 0.00001028
Iteration 84/1000 | Loss: 0.00001028
Iteration 85/1000 | Loss: 0.00001027
Iteration 86/1000 | Loss: 0.00001027
Iteration 87/1000 | Loss: 0.00001027
Iteration 88/1000 | Loss: 0.00001027
Iteration 89/1000 | Loss: 0.00001027
Iteration 90/1000 | Loss: 0.00001026
Iteration 91/1000 | Loss: 0.00001026
Iteration 92/1000 | Loss: 0.00001026
Iteration 93/1000 | Loss: 0.00001026
Iteration 94/1000 | Loss: 0.00001026
Iteration 95/1000 | Loss: 0.00001026
Iteration 96/1000 | Loss: 0.00001026
Iteration 97/1000 | Loss: 0.00001026
Iteration 98/1000 | Loss: 0.00001026
Iteration 99/1000 | Loss: 0.00001025
Iteration 100/1000 | Loss: 0.00001025
Iteration 101/1000 | Loss: 0.00001024
Iteration 102/1000 | Loss: 0.00001024
Iteration 103/1000 | Loss: 0.00001024
Iteration 104/1000 | Loss: 0.00001024
Iteration 105/1000 | Loss: 0.00001024
Iteration 106/1000 | Loss: 0.00001024
Iteration 107/1000 | Loss: 0.00001024
Iteration 108/1000 | Loss: 0.00001024
Iteration 109/1000 | Loss: 0.00001024
Iteration 110/1000 | Loss: 0.00001023
Iteration 111/1000 | Loss: 0.00001023
Iteration 112/1000 | Loss: 0.00001023
Iteration 113/1000 | Loss: 0.00001023
Iteration 114/1000 | Loss: 0.00001023
Iteration 115/1000 | Loss: 0.00001023
Iteration 116/1000 | Loss: 0.00001022
Iteration 117/1000 | Loss: 0.00001022
Iteration 118/1000 | Loss: 0.00001022
Iteration 119/1000 | Loss: 0.00001022
Iteration 120/1000 | Loss: 0.00001021
Iteration 121/1000 | Loss: 0.00001021
Iteration 122/1000 | Loss: 0.00001021
Iteration 123/1000 | Loss: 0.00001021
Iteration 124/1000 | Loss: 0.00001020
Iteration 125/1000 | Loss: 0.00001020
Iteration 126/1000 | Loss: 0.00001020
Iteration 127/1000 | Loss: 0.00001020
Iteration 128/1000 | Loss: 0.00001020
Iteration 129/1000 | Loss: 0.00001020
Iteration 130/1000 | Loss: 0.00001020
Iteration 131/1000 | Loss: 0.00001019
Iteration 132/1000 | Loss: 0.00001019
Iteration 133/1000 | Loss: 0.00001019
Iteration 134/1000 | Loss: 0.00001019
Iteration 135/1000 | Loss: 0.00001019
Iteration 136/1000 | Loss: 0.00001019
Iteration 137/1000 | Loss: 0.00001018
Iteration 138/1000 | Loss: 0.00001018
Iteration 139/1000 | Loss: 0.00001017
Iteration 140/1000 | Loss: 0.00001017
Iteration 141/1000 | Loss: 0.00001016
Iteration 142/1000 | Loss: 0.00001016
Iteration 143/1000 | Loss: 0.00001016
Iteration 144/1000 | Loss: 0.00001015
Iteration 145/1000 | Loss: 0.00001015
Iteration 146/1000 | Loss: 0.00001014
Iteration 147/1000 | Loss: 0.00001014
Iteration 148/1000 | Loss: 0.00001013
Iteration 149/1000 | Loss: 0.00001013
Iteration 150/1000 | Loss: 0.00001013
Iteration 151/1000 | Loss: 0.00001013
Iteration 152/1000 | Loss: 0.00001010
Iteration 153/1000 | Loss: 0.00001010
Iteration 154/1000 | Loss: 0.00001009
Iteration 155/1000 | Loss: 0.00001009
Iteration 156/1000 | Loss: 0.00001009
Iteration 157/1000 | Loss: 0.00001009
Iteration 158/1000 | Loss: 0.00001009
Iteration 159/1000 | Loss: 0.00001009
Iteration 160/1000 | Loss: 0.00001009
Iteration 161/1000 | Loss: 0.00001009
Iteration 162/1000 | Loss: 0.00001009
Iteration 163/1000 | Loss: 0.00001007
Iteration 164/1000 | Loss: 0.00001007
Iteration 165/1000 | Loss: 0.00001007
Iteration 166/1000 | Loss: 0.00001007
Iteration 167/1000 | Loss: 0.00001007
Iteration 168/1000 | Loss: 0.00001007
Iteration 169/1000 | Loss: 0.00001007
Iteration 170/1000 | Loss: 0.00001006
Iteration 171/1000 | Loss: 0.00001006
Iteration 172/1000 | Loss: 0.00001006
Iteration 173/1000 | Loss: 0.00001006
Iteration 174/1000 | Loss: 0.00001006
Iteration 175/1000 | Loss: 0.00001006
Iteration 176/1000 | Loss: 0.00001005
Iteration 177/1000 | Loss: 0.00001005
Iteration 178/1000 | Loss: 0.00001005
Iteration 179/1000 | Loss: 0.00001005
Iteration 180/1000 | Loss: 0.00001004
Iteration 181/1000 | Loss: 0.00001004
Iteration 182/1000 | Loss: 0.00001004
Iteration 183/1000 | Loss: 0.00001004
Iteration 184/1000 | Loss: 0.00001004
Iteration 185/1000 | Loss: 0.00001003
Iteration 186/1000 | Loss: 0.00001003
Iteration 187/1000 | Loss: 0.00001003
Iteration 188/1000 | Loss: 0.00001003
Iteration 189/1000 | Loss: 0.00001003
Iteration 190/1000 | Loss: 0.00001003
Iteration 191/1000 | Loss: 0.00001003
Iteration 192/1000 | Loss: 0.00001003
Iteration 193/1000 | Loss: 0.00001003
Iteration 194/1000 | Loss: 0.00001003
Iteration 195/1000 | Loss: 0.00001003
Iteration 196/1000 | Loss: 0.00001002
Iteration 197/1000 | Loss: 0.00001002
Iteration 198/1000 | Loss: 0.00001002
Iteration 199/1000 | Loss: 0.00001002
Iteration 200/1000 | Loss: 0.00001002
Iteration 201/1000 | Loss: 0.00001002
Iteration 202/1000 | Loss: 0.00001002
Iteration 203/1000 | Loss: 0.00001002
Iteration 204/1000 | Loss: 0.00001002
Iteration 205/1000 | Loss: 0.00001002
Iteration 206/1000 | Loss: 0.00001002
Iteration 207/1000 | Loss: 0.00001002
Iteration 208/1000 | Loss: 0.00001002
Iteration 209/1000 | Loss: 0.00001002
Iteration 210/1000 | Loss: 0.00001002
Iteration 211/1000 | Loss: 0.00001002
Iteration 212/1000 | Loss: 0.00001002
Iteration 213/1000 | Loss: 0.00001002
Iteration 214/1000 | Loss: 0.00001002
Iteration 215/1000 | Loss: 0.00001002
Iteration 216/1000 | Loss: 0.00001002
Iteration 217/1000 | Loss: 0.00001002
Iteration 218/1000 | Loss: 0.00001002
Iteration 219/1000 | Loss: 0.00001002
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 219. Stopping optimization.
Last 5 losses: [1.0016195119533222e-05, 1.0016195119533222e-05, 1.0016195119533222e-05, 1.0016195119533222e-05, 1.0016195119533222e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0016195119533222e-05

Optimization complete. Final v2v error: 2.7161879539489746 mm

Highest mean error: 2.9780890941619873 mm for frame 84

Lowest mean error: 2.5319290161132812 mm for frame 4

Saving results

Total time: 41.36658787727356
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_003/1047/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_003/1047.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_003/1047
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00653383
Iteration 2/25 | Loss: 0.00151706
Iteration 3/25 | Loss: 0.00133596
Iteration 4/25 | Loss: 0.00131815
Iteration 5/25 | Loss: 0.00131536
Iteration 6/25 | Loss: 0.00131476
Iteration 7/25 | Loss: 0.00131476
Iteration 8/25 | Loss: 0.00131476
Iteration 9/25 | Loss: 0.00131476
Iteration 10/25 | Loss: 0.00131476
Iteration 11/25 | Loss: 0.00131476
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013147636782377958, 0.0013147636782377958, 0.0013147636782377958, 0.0013147636782377958, 0.0013147636782377958]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013147636782377958

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 10.93335915
Iteration 2/25 | Loss: 0.00103221
Iteration 3/25 | Loss: 0.00103201
Iteration 4/25 | Loss: 0.00103201
Iteration 5/25 | Loss: 0.00103201
Iteration 6/25 | Loss: 0.00103201
Iteration 7/25 | Loss: 0.00103201
Iteration 8/25 | Loss: 0.00103201
Iteration 9/25 | Loss: 0.00103201
Iteration 10/25 | Loss: 0.00103201
Iteration 11/25 | Loss: 0.00103201
Iteration 12/25 | Loss: 0.00103201
Iteration 13/25 | Loss: 0.00103201
Iteration 14/25 | Loss: 0.00103201
Iteration 15/25 | Loss: 0.00103201
Iteration 16/25 | Loss: 0.00103201
Iteration 17/25 | Loss: 0.00103201
Iteration 18/25 | Loss: 0.00103201
Iteration 19/25 | Loss: 0.00103201
Iteration 20/25 | Loss: 0.00103201
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0010320083238184452, 0.0010320083238184452, 0.0010320083238184452, 0.0010320083238184452, 0.0010320083238184452]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010320083238184452

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00103201
Iteration 2/1000 | Loss: 0.00004442
Iteration 3/1000 | Loss: 0.00002779
Iteration 4/1000 | Loss: 0.00002360
Iteration 5/1000 | Loss: 0.00002215
Iteration 6/1000 | Loss: 0.00002136
Iteration 7/1000 | Loss: 0.00002077
Iteration 8/1000 | Loss: 0.00002031
Iteration 9/1000 | Loss: 0.00001991
Iteration 10/1000 | Loss: 0.00001960
Iteration 11/1000 | Loss: 0.00001934
Iteration 12/1000 | Loss: 0.00001914
Iteration 13/1000 | Loss: 0.00001904
Iteration 14/1000 | Loss: 0.00001895
Iteration 15/1000 | Loss: 0.00001889
Iteration 16/1000 | Loss: 0.00001887
Iteration 17/1000 | Loss: 0.00001886
Iteration 18/1000 | Loss: 0.00001885
Iteration 19/1000 | Loss: 0.00001884
Iteration 20/1000 | Loss: 0.00001883
Iteration 21/1000 | Loss: 0.00001883
Iteration 22/1000 | Loss: 0.00001881
Iteration 23/1000 | Loss: 0.00001880
Iteration 24/1000 | Loss: 0.00001879
Iteration 25/1000 | Loss: 0.00001878
Iteration 26/1000 | Loss: 0.00001877
Iteration 27/1000 | Loss: 0.00001877
Iteration 28/1000 | Loss: 0.00001877
Iteration 29/1000 | Loss: 0.00001876
Iteration 30/1000 | Loss: 0.00001875
Iteration 31/1000 | Loss: 0.00001874
Iteration 32/1000 | Loss: 0.00001871
Iteration 33/1000 | Loss: 0.00001871
Iteration 34/1000 | Loss: 0.00001870
Iteration 35/1000 | Loss: 0.00001870
Iteration 36/1000 | Loss: 0.00001869
Iteration 37/1000 | Loss: 0.00001866
Iteration 38/1000 | Loss: 0.00001861
Iteration 39/1000 | Loss: 0.00001856
Iteration 40/1000 | Loss: 0.00001854
Iteration 41/1000 | Loss: 0.00001853
Iteration 42/1000 | Loss: 0.00001852
Iteration 43/1000 | Loss: 0.00001852
Iteration 44/1000 | Loss: 0.00001852
Iteration 45/1000 | Loss: 0.00001851
Iteration 46/1000 | Loss: 0.00001851
Iteration 47/1000 | Loss: 0.00001850
Iteration 48/1000 | Loss: 0.00001850
Iteration 49/1000 | Loss: 0.00001850
Iteration 50/1000 | Loss: 0.00001849
Iteration 51/1000 | Loss: 0.00001849
Iteration 52/1000 | Loss: 0.00001849
Iteration 53/1000 | Loss: 0.00001848
Iteration 54/1000 | Loss: 0.00001848
Iteration 55/1000 | Loss: 0.00001847
Iteration 56/1000 | Loss: 0.00001847
Iteration 57/1000 | Loss: 0.00001847
Iteration 58/1000 | Loss: 0.00001845
Iteration 59/1000 | Loss: 0.00001844
Iteration 60/1000 | Loss: 0.00001844
Iteration 61/1000 | Loss: 0.00001844
Iteration 62/1000 | Loss: 0.00001843
Iteration 63/1000 | Loss: 0.00001842
Iteration 64/1000 | Loss: 0.00001842
Iteration 65/1000 | Loss: 0.00001842
Iteration 66/1000 | Loss: 0.00001842
Iteration 67/1000 | Loss: 0.00001842
Iteration 68/1000 | Loss: 0.00001841
Iteration 69/1000 | Loss: 0.00001841
Iteration 70/1000 | Loss: 0.00001841
Iteration 71/1000 | Loss: 0.00001841
Iteration 72/1000 | Loss: 0.00001841
Iteration 73/1000 | Loss: 0.00001841
Iteration 74/1000 | Loss: 0.00001841
Iteration 75/1000 | Loss: 0.00001841
Iteration 76/1000 | Loss: 0.00001841
Iteration 77/1000 | Loss: 0.00001841
Iteration 78/1000 | Loss: 0.00001841
Iteration 79/1000 | Loss: 0.00001840
Iteration 80/1000 | Loss: 0.00001840
Iteration 81/1000 | Loss: 0.00001840
Iteration 82/1000 | Loss: 0.00001840
Iteration 83/1000 | Loss: 0.00001840
Iteration 84/1000 | Loss: 0.00001840
Iteration 85/1000 | Loss: 0.00001840
Iteration 86/1000 | Loss: 0.00001840
Iteration 87/1000 | Loss: 0.00001840
Iteration 88/1000 | Loss: 0.00001840
Iteration 89/1000 | Loss: 0.00001839
Iteration 90/1000 | Loss: 0.00001839
Iteration 91/1000 | Loss: 0.00001839
Iteration 92/1000 | Loss: 0.00001839
Iteration 93/1000 | Loss: 0.00001839
Iteration 94/1000 | Loss: 0.00001838
Iteration 95/1000 | Loss: 0.00001838
Iteration 96/1000 | Loss: 0.00001838
Iteration 97/1000 | Loss: 0.00001838
Iteration 98/1000 | Loss: 0.00001837
Iteration 99/1000 | Loss: 0.00001837
Iteration 100/1000 | Loss: 0.00001837
Iteration 101/1000 | Loss: 0.00001837
Iteration 102/1000 | Loss: 0.00001837
Iteration 103/1000 | Loss: 0.00001837
Iteration 104/1000 | Loss: 0.00001836
Iteration 105/1000 | Loss: 0.00001836
Iteration 106/1000 | Loss: 0.00001836
Iteration 107/1000 | Loss: 0.00001836
Iteration 108/1000 | Loss: 0.00001836
Iteration 109/1000 | Loss: 0.00001835
Iteration 110/1000 | Loss: 0.00001835
Iteration 111/1000 | Loss: 0.00001835
Iteration 112/1000 | Loss: 0.00001835
Iteration 113/1000 | Loss: 0.00001835
Iteration 114/1000 | Loss: 0.00001835
Iteration 115/1000 | Loss: 0.00001834
Iteration 116/1000 | Loss: 0.00001834
Iteration 117/1000 | Loss: 0.00001834
Iteration 118/1000 | Loss: 0.00001834
Iteration 119/1000 | Loss: 0.00001834
Iteration 120/1000 | Loss: 0.00001834
Iteration 121/1000 | Loss: 0.00001834
Iteration 122/1000 | Loss: 0.00001833
Iteration 123/1000 | Loss: 0.00001833
Iteration 124/1000 | Loss: 0.00001833
Iteration 125/1000 | Loss: 0.00001833
Iteration 126/1000 | Loss: 0.00001832
Iteration 127/1000 | Loss: 0.00001832
Iteration 128/1000 | Loss: 0.00001832
Iteration 129/1000 | Loss: 0.00001832
Iteration 130/1000 | Loss: 0.00001832
Iteration 131/1000 | Loss: 0.00001832
Iteration 132/1000 | Loss: 0.00001831
Iteration 133/1000 | Loss: 0.00001831
Iteration 134/1000 | Loss: 0.00001831
Iteration 135/1000 | Loss: 0.00001831
Iteration 136/1000 | Loss: 0.00001831
Iteration 137/1000 | Loss: 0.00001831
Iteration 138/1000 | Loss: 0.00001831
Iteration 139/1000 | Loss: 0.00001831
Iteration 140/1000 | Loss: 0.00001831
Iteration 141/1000 | Loss: 0.00001831
Iteration 142/1000 | Loss: 0.00001831
Iteration 143/1000 | Loss: 0.00001830
Iteration 144/1000 | Loss: 0.00001830
Iteration 145/1000 | Loss: 0.00001830
Iteration 146/1000 | Loss: 0.00001830
Iteration 147/1000 | Loss: 0.00001830
Iteration 148/1000 | Loss: 0.00001830
Iteration 149/1000 | Loss: 0.00001830
Iteration 150/1000 | Loss: 0.00001830
Iteration 151/1000 | Loss: 0.00001829
Iteration 152/1000 | Loss: 0.00001829
Iteration 153/1000 | Loss: 0.00001829
Iteration 154/1000 | Loss: 0.00001829
Iteration 155/1000 | Loss: 0.00001829
Iteration 156/1000 | Loss: 0.00001829
Iteration 157/1000 | Loss: 0.00001829
Iteration 158/1000 | Loss: 0.00001829
Iteration 159/1000 | Loss: 0.00001829
Iteration 160/1000 | Loss: 0.00001829
Iteration 161/1000 | Loss: 0.00001829
Iteration 162/1000 | Loss: 0.00001829
Iteration 163/1000 | Loss: 0.00001829
Iteration 164/1000 | Loss: 0.00001829
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 164. Stopping optimization.
Last 5 losses: [1.8293447283213027e-05, 1.8293447283213027e-05, 1.8293447283213027e-05, 1.8293447283213027e-05, 1.8293447283213027e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8293447283213027e-05

Optimization complete. Final v2v error: 3.552859306335449 mm

Highest mean error: 4.302699565887451 mm for frame 124

Lowest mean error: 2.7882392406463623 mm for frame 21

Saving results

Total time: 40.66099572181702
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_003/1059/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_003/1059.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_003/1059
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00391647
Iteration 2/25 | Loss: 0.00124709
Iteration 3/25 | Loss: 0.00120204
Iteration 4/25 | Loss: 0.00119689
Iteration 5/25 | Loss: 0.00119551
Iteration 6/25 | Loss: 0.00119551
Iteration 7/25 | Loss: 0.00119551
Iteration 8/25 | Loss: 0.00119551
Iteration 9/25 | Loss: 0.00119551
Iteration 10/25 | Loss: 0.00119551
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0011955065419897437, 0.0011955065419897437, 0.0011955065419897437, 0.0011955065419897437, 0.0011955065419897437]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011955065419897437

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35083735
Iteration 2/25 | Loss: 0.00118287
Iteration 3/25 | Loss: 0.00118286
Iteration 4/25 | Loss: 0.00118286
Iteration 5/25 | Loss: 0.00118286
Iteration 6/25 | Loss: 0.00118286
Iteration 7/25 | Loss: 0.00118286
Iteration 8/25 | Loss: 0.00118286
Iteration 9/25 | Loss: 0.00118286
Iteration 10/25 | Loss: 0.00118286
Iteration 11/25 | Loss: 0.00118286
Iteration 12/25 | Loss: 0.00118286
Iteration 13/25 | Loss: 0.00118286
Iteration 14/25 | Loss: 0.00118286
Iteration 15/25 | Loss: 0.00118286
Iteration 16/25 | Loss: 0.00118286
Iteration 17/25 | Loss: 0.00118286
Iteration 18/25 | Loss: 0.00118286
Iteration 19/25 | Loss: 0.00118286
Iteration 20/25 | Loss: 0.00118286
Iteration 21/25 | Loss: 0.00118286
Iteration 22/25 | Loss: 0.00118286
Iteration 23/25 | Loss: 0.00118286
Iteration 24/25 | Loss: 0.00118286
Iteration 25/25 | Loss: 0.00118286

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00118286
Iteration 2/1000 | Loss: 0.00002109
Iteration 3/1000 | Loss: 0.00001423
Iteration 4/1000 | Loss: 0.00001279
Iteration 5/1000 | Loss: 0.00001212
Iteration 6/1000 | Loss: 0.00001176
Iteration 7/1000 | Loss: 0.00001146
Iteration 8/1000 | Loss: 0.00001145
Iteration 9/1000 | Loss: 0.00001123
Iteration 10/1000 | Loss: 0.00001103
Iteration 11/1000 | Loss: 0.00001089
Iteration 12/1000 | Loss: 0.00001087
Iteration 13/1000 | Loss: 0.00001080
Iteration 14/1000 | Loss: 0.00001080
Iteration 15/1000 | Loss: 0.00001079
Iteration 16/1000 | Loss: 0.00001079
Iteration 17/1000 | Loss: 0.00001079
Iteration 18/1000 | Loss: 0.00001074
Iteration 19/1000 | Loss: 0.00001074
Iteration 20/1000 | Loss: 0.00001073
Iteration 21/1000 | Loss: 0.00001073
Iteration 22/1000 | Loss: 0.00001073
Iteration 23/1000 | Loss: 0.00001070
Iteration 24/1000 | Loss: 0.00001070
Iteration 25/1000 | Loss: 0.00001065
Iteration 26/1000 | Loss: 0.00001065
Iteration 27/1000 | Loss: 0.00001065
Iteration 28/1000 | Loss: 0.00001064
Iteration 29/1000 | Loss: 0.00001061
Iteration 30/1000 | Loss: 0.00001061
Iteration 31/1000 | Loss: 0.00001060
Iteration 32/1000 | Loss: 0.00001060
Iteration 33/1000 | Loss: 0.00001060
Iteration 34/1000 | Loss: 0.00001060
Iteration 35/1000 | Loss: 0.00001060
Iteration 36/1000 | Loss: 0.00001059
Iteration 37/1000 | Loss: 0.00001059
Iteration 38/1000 | Loss: 0.00001059
Iteration 39/1000 | Loss: 0.00001059
Iteration 40/1000 | Loss: 0.00001058
Iteration 41/1000 | Loss: 0.00001057
Iteration 42/1000 | Loss: 0.00001056
Iteration 43/1000 | Loss: 0.00001056
Iteration 44/1000 | Loss: 0.00001056
Iteration 45/1000 | Loss: 0.00001056
Iteration 46/1000 | Loss: 0.00001056
Iteration 47/1000 | Loss: 0.00001055
Iteration 48/1000 | Loss: 0.00001055
Iteration 49/1000 | Loss: 0.00001055
Iteration 50/1000 | Loss: 0.00001054
Iteration 51/1000 | Loss: 0.00001054
Iteration 52/1000 | Loss: 0.00001053
Iteration 53/1000 | Loss: 0.00001053
Iteration 54/1000 | Loss: 0.00001053
Iteration 55/1000 | Loss: 0.00001052
Iteration 56/1000 | Loss: 0.00001052
Iteration 57/1000 | Loss: 0.00001052
Iteration 58/1000 | Loss: 0.00001051
Iteration 59/1000 | Loss: 0.00001051
Iteration 60/1000 | Loss: 0.00001051
Iteration 61/1000 | Loss: 0.00001051
Iteration 62/1000 | Loss: 0.00001051
Iteration 63/1000 | Loss: 0.00001050
Iteration 64/1000 | Loss: 0.00001050
Iteration 65/1000 | Loss: 0.00001050
Iteration 66/1000 | Loss: 0.00001049
Iteration 67/1000 | Loss: 0.00001049
Iteration 68/1000 | Loss: 0.00001049
Iteration 69/1000 | Loss: 0.00001048
Iteration 70/1000 | Loss: 0.00001048
Iteration 71/1000 | Loss: 0.00001047
Iteration 72/1000 | Loss: 0.00001047
Iteration 73/1000 | Loss: 0.00001047
Iteration 74/1000 | Loss: 0.00001047
Iteration 75/1000 | Loss: 0.00001047
Iteration 76/1000 | Loss: 0.00001047
Iteration 77/1000 | Loss: 0.00001047
Iteration 78/1000 | Loss: 0.00001047
Iteration 79/1000 | Loss: 0.00001047
Iteration 80/1000 | Loss: 0.00001047
Iteration 81/1000 | Loss: 0.00001046
Iteration 82/1000 | Loss: 0.00001046
Iteration 83/1000 | Loss: 0.00001046
Iteration 84/1000 | Loss: 0.00001045
Iteration 85/1000 | Loss: 0.00001045
Iteration 86/1000 | Loss: 0.00001045
Iteration 87/1000 | Loss: 0.00001044
Iteration 88/1000 | Loss: 0.00001044
Iteration 89/1000 | Loss: 0.00001044
Iteration 90/1000 | Loss: 0.00001043
Iteration 91/1000 | Loss: 0.00001043
Iteration 92/1000 | Loss: 0.00001043
Iteration 93/1000 | Loss: 0.00001043
Iteration 94/1000 | Loss: 0.00001043
Iteration 95/1000 | Loss: 0.00001043
Iteration 96/1000 | Loss: 0.00001043
Iteration 97/1000 | Loss: 0.00001043
Iteration 98/1000 | Loss: 0.00001043
Iteration 99/1000 | Loss: 0.00001042
Iteration 100/1000 | Loss: 0.00001041
Iteration 101/1000 | Loss: 0.00001041
Iteration 102/1000 | Loss: 0.00001041
Iteration 103/1000 | Loss: 0.00001041
Iteration 104/1000 | Loss: 0.00001041
Iteration 105/1000 | Loss: 0.00001041
Iteration 106/1000 | Loss: 0.00001040
Iteration 107/1000 | Loss: 0.00001040
Iteration 108/1000 | Loss: 0.00001040
Iteration 109/1000 | Loss: 0.00001040
Iteration 110/1000 | Loss: 0.00001040
Iteration 111/1000 | Loss: 0.00001040
Iteration 112/1000 | Loss: 0.00001040
Iteration 113/1000 | Loss: 0.00001040
Iteration 114/1000 | Loss: 0.00001040
Iteration 115/1000 | Loss: 0.00001040
Iteration 116/1000 | Loss: 0.00001040
Iteration 117/1000 | Loss: 0.00001039
Iteration 118/1000 | Loss: 0.00001039
Iteration 119/1000 | Loss: 0.00001039
Iteration 120/1000 | Loss: 0.00001039
Iteration 121/1000 | Loss: 0.00001039
Iteration 122/1000 | Loss: 0.00001039
Iteration 123/1000 | Loss: 0.00001039
Iteration 124/1000 | Loss: 0.00001039
Iteration 125/1000 | Loss: 0.00001039
Iteration 126/1000 | Loss: 0.00001039
Iteration 127/1000 | Loss: 0.00001039
Iteration 128/1000 | Loss: 0.00001039
Iteration 129/1000 | Loss: 0.00001039
Iteration 130/1000 | Loss: 0.00001039
Iteration 131/1000 | Loss: 0.00001039
Iteration 132/1000 | Loss: 0.00001039
Iteration 133/1000 | Loss: 0.00001039
Iteration 134/1000 | Loss: 0.00001039
Iteration 135/1000 | Loss: 0.00001039
Iteration 136/1000 | Loss: 0.00001039
Iteration 137/1000 | Loss: 0.00001039
Iteration 138/1000 | Loss: 0.00001039
Iteration 139/1000 | Loss: 0.00001039
Iteration 140/1000 | Loss: 0.00001039
Iteration 141/1000 | Loss: 0.00001039
Iteration 142/1000 | Loss: 0.00001039
Iteration 143/1000 | Loss: 0.00001039
Iteration 144/1000 | Loss: 0.00001039
Iteration 145/1000 | Loss: 0.00001039
Iteration 146/1000 | Loss: 0.00001039
Iteration 147/1000 | Loss: 0.00001039
Iteration 148/1000 | Loss: 0.00001039
Iteration 149/1000 | Loss: 0.00001039
Iteration 150/1000 | Loss: 0.00001039
Iteration 151/1000 | Loss: 0.00001039
Iteration 152/1000 | Loss: 0.00001039
Iteration 153/1000 | Loss: 0.00001039
Iteration 154/1000 | Loss: 0.00001039
Iteration 155/1000 | Loss: 0.00001039
Iteration 156/1000 | Loss: 0.00001039
Iteration 157/1000 | Loss: 0.00001039
Iteration 158/1000 | Loss: 0.00001039
Iteration 159/1000 | Loss: 0.00001039
Iteration 160/1000 | Loss: 0.00001039
Iteration 161/1000 | Loss: 0.00001039
Iteration 162/1000 | Loss: 0.00001039
Iteration 163/1000 | Loss: 0.00001039
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 163. Stopping optimization.
Last 5 losses: [1.0390706847829279e-05, 1.0390706847829279e-05, 1.0390706847829279e-05, 1.0390706847829279e-05, 1.0390706847829279e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0390706847829279e-05

Optimization complete. Final v2v error: 2.781785726547241 mm

Highest mean error: 3.001499652862549 mm for frame 102

Lowest mean error: 2.691889524459839 mm for frame 88

Saving results

Total time: 33.05692386627197
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_003/1012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_003/1012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_003/1012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00826836
Iteration 2/25 | Loss: 0.00146111
Iteration 3/25 | Loss: 0.00133578
Iteration 4/25 | Loss: 0.00131529
Iteration 5/25 | Loss: 0.00130933
Iteration 6/25 | Loss: 0.00130836
Iteration 7/25 | Loss: 0.00130836
Iteration 8/25 | Loss: 0.00130836
Iteration 9/25 | Loss: 0.00130836
Iteration 10/25 | Loss: 0.00130836
Iteration 11/25 | Loss: 0.00130836
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013083592057228088, 0.0013083592057228088, 0.0013083592057228088, 0.0013083592057228088, 0.0013083592057228088]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013083592057228088

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.27713418
Iteration 2/25 | Loss: 0.00103619
Iteration 3/25 | Loss: 0.00103612
Iteration 4/25 | Loss: 0.00103612
Iteration 5/25 | Loss: 0.00103612
Iteration 6/25 | Loss: 0.00103612
Iteration 7/25 | Loss: 0.00103612
Iteration 8/25 | Loss: 0.00103612
Iteration 9/25 | Loss: 0.00103612
Iteration 10/25 | Loss: 0.00103612
Iteration 11/25 | Loss: 0.00103611
Iteration 12/25 | Loss: 0.00103611
Iteration 13/25 | Loss: 0.00103611
Iteration 14/25 | Loss: 0.00103611
Iteration 15/25 | Loss: 0.00103611
Iteration 16/25 | Loss: 0.00103611
Iteration 17/25 | Loss: 0.00103611
Iteration 18/25 | Loss: 0.00103611
Iteration 19/25 | Loss: 0.00103611
Iteration 20/25 | Loss: 0.00103611
Iteration 21/25 | Loss: 0.00103611
Iteration 22/25 | Loss: 0.00103611
Iteration 23/25 | Loss: 0.00103611
Iteration 24/25 | Loss: 0.00103611
Iteration 25/25 | Loss: 0.00103611

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00103611
Iteration 2/1000 | Loss: 0.00005232
Iteration 3/1000 | Loss: 0.00003561
Iteration 4/1000 | Loss: 0.00003215
Iteration 5/1000 | Loss: 0.00003090
Iteration 6/1000 | Loss: 0.00002979
Iteration 7/1000 | Loss: 0.00002897
Iteration 8/1000 | Loss: 0.00002852
Iteration 9/1000 | Loss: 0.00002801
Iteration 10/1000 | Loss: 0.00002766
Iteration 11/1000 | Loss: 0.00002741
Iteration 12/1000 | Loss: 0.00002732
Iteration 13/1000 | Loss: 0.00002716
Iteration 14/1000 | Loss: 0.00002713
Iteration 15/1000 | Loss: 0.00002712
Iteration 16/1000 | Loss: 0.00002712
Iteration 17/1000 | Loss: 0.00002696
Iteration 18/1000 | Loss: 0.00002694
Iteration 19/1000 | Loss: 0.00002693
Iteration 20/1000 | Loss: 0.00002692
Iteration 21/1000 | Loss: 0.00002691
Iteration 22/1000 | Loss: 0.00002686
Iteration 23/1000 | Loss: 0.00002685
Iteration 24/1000 | Loss: 0.00002684
Iteration 25/1000 | Loss: 0.00002683
Iteration 26/1000 | Loss: 0.00002683
Iteration 27/1000 | Loss: 0.00002682
Iteration 28/1000 | Loss: 0.00002677
Iteration 29/1000 | Loss: 0.00002676
Iteration 30/1000 | Loss: 0.00002674
Iteration 31/1000 | Loss: 0.00002673
Iteration 32/1000 | Loss: 0.00002673
Iteration 33/1000 | Loss: 0.00002673
Iteration 34/1000 | Loss: 0.00002673
Iteration 35/1000 | Loss: 0.00002673
Iteration 36/1000 | Loss: 0.00002673
Iteration 37/1000 | Loss: 0.00002672
Iteration 38/1000 | Loss: 0.00002672
Iteration 39/1000 | Loss: 0.00002671
Iteration 40/1000 | Loss: 0.00002671
Iteration 41/1000 | Loss: 0.00002671
Iteration 42/1000 | Loss: 0.00002670
Iteration 43/1000 | Loss: 0.00002670
Iteration 44/1000 | Loss: 0.00002670
Iteration 45/1000 | Loss: 0.00002670
Iteration 46/1000 | Loss: 0.00002670
Iteration 47/1000 | Loss: 0.00002670
Iteration 48/1000 | Loss: 0.00002670
Iteration 49/1000 | Loss: 0.00002670
Iteration 50/1000 | Loss: 0.00002669
Iteration 51/1000 | Loss: 0.00002669
Iteration 52/1000 | Loss: 0.00002669
Iteration 53/1000 | Loss: 0.00002668
Iteration 54/1000 | Loss: 0.00002668
Iteration 55/1000 | Loss: 0.00002668
Iteration 56/1000 | Loss: 0.00002667
Iteration 57/1000 | Loss: 0.00002667
Iteration 58/1000 | Loss: 0.00002667
Iteration 59/1000 | Loss: 0.00002667
Iteration 60/1000 | Loss: 0.00002666
Iteration 61/1000 | Loss: 0.00002666
Iteration 62/1000 | Loss: 0.00002666
Iteration 63/1000 | Loss: 0.00002665
Iteration 64/1000 | Loss: 0.00002665
Iteration 65/1000 | Loss: 0.00002665
Iteration 66/1000 | Loss: 0.00002664
Iteration 67/1000 | Loss: 0.00002664
Iteration 68/1000 | Loss: 0.00002664
Iteration 69/1000 | Loss: 0.00002664
Iteration 70/1000 | Loss: 0.00002664
Iteration 71/1000 | Loss: 0.00002664
Iteration 72/1000 | Loss: 0.00002664
Iteration 73/1000 | Loss: 0.00002664
Iteration 74/1000 | Loss: 0.00002664
Iteration 75/1000 | Loss: 0.00002664
Iteration 76/1000 | Loss: 0.00002664
Iteration 77/1000 | Loss: 0.00002664
Iteration 78/1000 | Loss: 0.00002664
Iteration 79/1000 | Loss: 0.00002664
Iteration 80/1000 | Loss: 0.00002664
Iteration 81/1000 | Loss: 0.00002664
Iteration 82/1000 | Loss: 0.00002664
Iteration 83/1000 | Loss: 0.00002664
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 83. Stopping optimization.
Last 5 losses: [2.663985469553154e-05, 2.663985469553154e-05, 2.663985469553154e-05, 2.663985469553154e-05, 2.663985469553154e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.663985469553154e-05

Optimization complete. Final v2v error: 4.286643981933594 mm

Highest mean error: 4.5632123947143555 mm for frame 22

Lowest mean error: 3.960749626159668 mm for frame 120

Saving results

Total time: 34.32552886009216
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_003/1064/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_003/1064.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_003/1064
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00941302
Iteration 2/25 | Loss: 0.00171204
Iteration 3/25 | Loss: 0.00141746
Iteration 4/25 | Loss: 0.00139636
Iteration 5/25 | Loss: 0.00139028
Iteration 6/25 | Loss: 0.00139028
Iteration 7/25 | Loss: 0.00139028
Iteration 8/25 | Loss: 0.00139028
Iteration 9/25 | Loss: 0.00139028
Iteration 10/25 | Loss: 0.00139028
Iteration 11/25 | Loss: 0.00139028
Iteration 12/25 | Loss: 0.00139028
Iteration 13/25 | Loss: 0.00139028
Iteration 14/25 | Loss: 0.00139028
Iteration 15/25 | Loss: 0.00139028
Iteration 16/25 | Loss: 0.00139028
Iteration 17/25 | Loss: 0.00139028
Iteration 18/25 | Loss: 0.00139028
Iteration 19/25 | Loss: 0.00139028
Iteration 20/25 | Loss: 0.00139028
Iteration 21/25 | Loss: 0.00139028
Iteration 22/25 | Loss: 0.00139028
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0013902818318456411, 0.0013902818318456411, 0.0013902818318456411, 0.0013902818318456411, 0.0013902818318456411]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013902818318456411

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.80800098
Iteration 2/25 | Loss: 0.00131084
Iteration 3/25 | Loss: 0.00131084
Iteration 4/25 | Loss: 0.00131084
Iteration 5/25 | Loss: 0.00131083
Iteration 6/25 | Loss: 0.00131083
Iteration 7/25 | Loss: 0.00131083
Iteration 8/25 | Loss: 0.00131083
Iteration 9/25 | Loss: 0.00131083
Iteration 10/25 | Loss: 0.00131083
Iteration 11/25 | Loss: 0.00131083
Iteration 12/25 | Loss: 0.00131083
Iteration 13/25 | Loss: 0.00131083
Iteration 14/25 | Loss: 0.00131083
Iteration 15/25 | Loss: 0.00131083
Iteration 16/25 | Loss: 0.00131083
Iteration 17/25 | Loss: 0.00131083
Iteration 18/25 | Loss: 0.00131083
Iteration 19/25 | Loss: 0.00131083
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0013108327984809875, 0.0013108327984809875, 0.0013108327984809875, 0.0013108327984809875, 0.0013108327984809875]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013108327984809875

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00131083
Iteration 2/1000 | Loss: 0.00005387
Iteration 3/1000 | Loss: 0.00003957
Iteration 4/1000 | Loss: 0.00003635
Iteration 5/1000 | Loss: 0.00003474
Iteration 6/1000 | Loss: 0.00003375
Iteration 7/1000 | Loss: 0.00003313
Iteration 8/1000 | Loss: 0.00003254
Iteration 9/1000 | Loss: 0.00003214
Iteration 10/1000 | Loss: 0.00003178
Iteration 11/1000 | Loss: 0.00003141
Iteration 12/1000 | Loss: 0.00003118
Iteration 13/1000 | Loss: 0.00003092
Iteration 14/1000 | Loss: 0.00003064
Iteration 15/1000 | Loss: 0.00003043
Iteration 16/1000 | Loss: 0.00003031
Iteration 17/1000 | Loss: 0.00003013
Iteration 18/1000 | Loss: 0.00003009
Iteration 19/1000 | Loss: 0.00003002
Iteration 20/1000 | Loss: 0.00002994
Iteration 21/1000 | Loss: 0.00002986
Iteration 22/1000 | Loss: 0.00002986
Iteration 23/1000 | Loss: 0.00002985
Iteration 24/1000 | Loss: 0.00002982
Iteration 25/1000 | Loss: 0.00002979
Iteration 26/1000 | Loss: 0.00002976
Iteration 27/1000 | Loss: 0.00002975
Iteration 28/1000 | Loss: 0.00002972
Iteration 29/1000 | Loss: 0.00002972
Iteration 30/1000 | Loss: 0.00002972
Iteration 31/1000 | Loss: 0.00002971
Iteration 32/1000 | Loss: 0.00002971
Iteration 33/1000 | Loss: 0.00002971
Iteration 34/1000 | Loss: 0.00002971
Iteration 35/1000 | Loss: 0.00002971
Iteration 36/1000 | Loss: 0.00002971
Iteration 37/1000 | Loss: 0.00002971
Iteration 38/1000 | Loss: 0.00002971
Iteration 39/1000 | Loss: 0.00002971
Iteration 40/1000 | Loss: 0.00002971
Iteration 41/1000 | Loss: 0.00002971
Iteration 42/1000 | Loss: 0.00002971
Iteration 43/1000 | Loss: 0.00002970
Iteration 44/1000 | Loss: 0.00002970
Iteration 45/1000 | Loss: 0.00002970
Iteration 46/1000 | Loss: 0.00002969
Iteration 47/1000 | Loss: 0.00002968
Iteration 48/1000 | Loss: 0.00002968
Iteration 49/1000 | Loss: 0.00002968
Iteration 50/1000 | Loss: 0.00002967
Iteration 51/1000 | Loss: 0.00002967
Iteration 52/1000 | Loss: 0.00002967
Iteration 53/1000 | Loss: 0.00002966
Iteration 54/1000 | Loss: 0.00002966
Iteration 55/1000 | Loss: 0.00002966
Iteration 56/1000 | Loss: 0.00002966
Iteration 57/1000 | Loss: 0.00002966
Iteration 58/1000 | Loss: 0.00002966
Iteration 59/1000 | Loss: 0.00002966
Iteration 60/1000 | Loss: 0.00002966
Iteration 61/1000 | Loss: 0.00002966
Iteration 62/1000 | Loss: 0.00002966
Iteration 63/1000 | Loss: 0.00002965
Iteration 64/1000 | Loss: 0.00002965
Iteration 65/1000 | Loss: 0.00002965
Iteration 66/1000 | Loss: 0.00002965
Iteration 67/1000 | Loss: 0.00002965
Iteration 68/1000 | Loss: 0.00002965
Iteration 69/1000 | Loss: 0.00002965
Iteration 70/1000 | Loss: 0.00002965
Iteration 71/1000 | Loss: 0.00002965
Iteration 72/1000 | Loss: 0.00002965
Iteration 73/1000 | Loss: 0.00002964
Iteration 74/1000 | Loss: 0.00002964
Iteration 75/1000 | Loss: 0.00002963
Iteration 76/1000 | Loss: 0.00002963
Iteration 77/1000 | Loss: 0.00002963
Iteration 78/1000 | Loss: 0.00002963
Iteration 79/1000 | Loss: 0.00002963
Iteration 80/1000 | Loss: 0.00002963
Iteration 81/1000 | Loss: 0.00002963
Iteration 82/1000 | Loss: 0.00002963
Iteration 83/1000 | Loss: 0.00002963
Iteration 84/1000 | Loss: 0.00002963
Iteration 85/1000 | Loss: 0.00002962
Iteration 86/1000 | Loss: 0.00002962
Iteration 87/1000 | Loss: 0.00002962
Iteration 88/1000 | Loss: 0.00002962
Iteration 89/1000 | Loss: 0.00002962
Iteration 90/1000 | Loss: 0.00002962
Iteration 91/1000 | Loss: 0.00002962
Iteration 92/1000 | Loss: 0.00002962
Iteration 93/1000 | Loss: 0.00002962
Iteration 94/1000 | Loss: 0.00002962
Iteration 95/1000 | Loss: 0.00002962
Iteration 96/1000 | Loss: 0.00002962
Iteration 97/1000 | Loss: 0.00002962
Iteration 98/1000 | Loss: 0.00002962
Iteration 99/1000 | Loss: 0.00002962
Iteration 100/1000 | Loss: 0.00002962
Iteration 101/1000 | Loss: 0.00002962
Iteration 102/1000 | Loss: 0.00002962
Iteration 103/1000 | Loss: 0.00002962
Iteration 104/1000 | Loss: 0.00002962
Iteration 105/1000 | Loss: 0.00002962
Iteration 106/1000 | Loss: 0.00002962
Iteration 107/1000 | Loss: 0.00002962
Iteration 108/1000 | Loss: 0.00002962
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 108. Stopping optimization.
Last 5 losses: [2.9619990527862683e-05, 2.9619990527862683e-05, 2.9619990527862683e-05, 2.9619990527862683e-05, 2.9619990527862683e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.9619990527862683e-05

Optimization complete. Final v2v error: 4.534445285797119 mm

Highest mean error: 5.576076984405518 mm for frame 89

Lowest mean error: 3.594907522201538 mm for frame 0

Saving results

Total time: 47.73639488220215
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_003/1023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_003/1023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_003/1023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00824926
Iteration 2/25 | Loss: 0.00148188
Iteration 3/25 | Loss: 0.00126256
Iteration 4/25 | Loss: 0.00123625
Iteration 5/25 | Loss: 0.00123257
Iteration 6/25 | Loss: 0.00123463
Iteration 7/25 | Loss: 0.00123270
Iteration 8/25 | Loss: 0.00122889
Iteration 9/25 | Loss: 0.00123037
Iteration 10/25 | Loss: 0.00122877
Iteration 11/25 | Loss: 0.00122647
Iteration 12/25 | Loss: 0.00122438
Iteration 13/25 | Loss: 0.00122331
Iteration 14/25 | Loss: 0.00122296
Iteration 15/25 | Loss: 0.00122292
Iteration 16/25 | Loss: 0.00122292
Iteration 17/25 | Loss: 0.00122292
Iteration 18/25 | Loss: 0.00122292
Iteration 19/25 | Loss: 0.00122292
Iteration 20/25 | Loss: 0.00122292
Iteration 21/25 | Loss: 0.00122292
Iteration 22/25 | Loss: 0.00122292
Iteration 23/25 | Loss: 0.00122292
Iteration 24/25 | Loss: 0.00122291
Iteration 25/25 | Loss: 0.00122291

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.61932993
Iteration 2/25 | Loss: 0.00109032
Iteration 3/25 | Loss: 0.00109027
Iteration 4/25 | Loss: 0.00109026
Iteration 5/25 | Loss: 0.00109026
Iteration 6/25 | Loss: 0.00109026
Iteration 7/25 | Loss: 0.00109026
Iteration 8/25 | Loss: 0.00109026
Iteration 9/25 | Loss: 0.00109026
Iteration 10/25 | Loss: 0.00109026
Iteration 11/25 | Loss: 0.00109026
Iteration 12/25 | Loss: 0.00109026
Iteration 13/25 | Loss: 0.00109026
Iteration 14/25 | Loss: 0.00109026
Iteration 15/25 | Loss: 0.00109026
Iteration 16/25 | Loss: 0.00109026
Iteration 17/25 | Loss: 0.00109026
Iteration 18/25 | Loss: 0.00109026
Iteration 19/25 | Loss: 0.00109026
Iteration 20/25 | Loss: 0.00109026
Iteration 21/25 | Loss: 0.00109026
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0010902613867074251, 0.0010902613867074251, 0.0010902613867074251, 0.0010902613867074251, 0.0010902613867074251]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010902613867074251

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00109026
Iteration 2/1000 | Loss: 0.00003021
Iteration 3/1000 | Loss: 0.00002292
Iteration 4/1000 | Loss: 0.00002074
Iteration 5/1000 | Loss: 0.00001932
Iteration 6/1000 | Loss: 0.00001846
Iteration 7/1000 | Loss: 0.00001789
Iteration 8/1000 | Loss: 0.00001742
Iteration 9/1000 | Loss: 0.00001704
Iteration 10/1000 | Loss: 0.00001670
Iteration 11/1000 | Loss: 0.00001642
Iteration 12/1000 | Loss: 0.00001629
Iteration 13/1000 | Loss: 0.00031900
Iteration 14/1000 | Loss: 0.00002819
Iteration 15/1000 | Loss: 0.00001938
Iteration 16/1000 | Loss: 0.00001655
Iteration 17/1000 | Loss: 0.00001569
Iteration 18/1000 | Loss: 0.00001536
Iteration 19/1000 | Loss: 0.00001506
Iteration 20/1000 | Loss: 0.00001478
Iteration 21/1000 | Loss: 0.00001466
Iteration 22/1000 | Loss: 0.00001464
Iteration 23/1000 | Loss: 0.00001462
Iteration 24/1000 | Loss: 0.00001462
Iteration 25/1000 | Loss: 0.00001461
Iteration 26/1000 | Loss: 0.00001461
Iteration 27/1000 | Loss: 0.00001461
Iteration 28/1000 | Loss: 0.00001460
Iteration 29/1000 | Loss: 0.00001460
Iteration 30/1000 | Loss: 0.00001459
Iteration 31/1000 | Loss: 0.00001458
Iteration 32/1000 | Loss: 0.00001458
Iteration 33/1000 | Loss: 0.00001458
Iteration 34/1000 | Loss: 0.00001458
Iteration 35/1000 | Loss: 0.00001457
Iteration 36/1000 | Loss: 0.00001457
Iteration 37/1000 | Loss: 0.00001457
Iteration 38/1000 | Loss: 0.00001456
Iteration 39/1000 | Loss: 0.00001456
Iteration 40/1000 | Loss: 0.00001456
Iteration 41/1000 | Loss: 0.00001455
Iteration 42/1000 | Loss: 0.00001455
Iteration 43/1000 | Loss: 0.00001454
Iteration 44/1000 | Loss: 0.00001454
Iteration 45/1000 | Loss: 0.00001453
Iteration 46/1000 | Loss: 0.00001453
Iteration 47/1000 | Loss: 0.00001452
Iteration 48/1000 | Loss: 0.00001451
Iteration 49/1000 | Loss: 0.00001451
Iteration 50/1000 | Loss: 0.00001450
Iteration 51/1000 | Loss: 0.00001450
Iteration 52/1000 | Loss: 0.00001447
Iteration 53/1000 | Loss: 0.00001447
Iteration 54/1000 | Loss: 0.00001446
Iteration 55/1000 | Loss: 0.00001446
Iteration 56/1000 | Loss: 0.00001445
Iteration 57/1000 | Loss: 0.00001445
Iteration 58/1000 | Loss: 0.00001444
Iteration 59/1000 | Loss: 0.00001444
Iteration 60/1000 | Loss: 0.00001444
Iteration 61/1000 | Loss: 0.00001444
Iteration 62/1000 | Loss: 0.00001444
Iteration 63/1000 | Loss: 0.00001443
Iteration 64/1000 | Loss: 0.00001443
Iteration 65/1000 | Loss: 0.00001443
Iteration 66/1000 | Loss: 0.00001443
Iteration 67/1000 | Loss: 0.00001443
Iteration 68/1000 | Loss: 0.00001443
Iteration 69/1000 | Loss: 0.00001443
Iteration 70/1000 | Loss: 0.00001442
Iteration 71/1000 | Loss: 0.00001442
Iteration 72/1000 | Loss: 0.00001442
Iteration 73/1000 | Loss: 0.00001442
Iteration 74/1000 | Loss: 0.00001441
Iteration 75/1000 | Loss: 0.00001438
Iteration 76/1000 | Loss: 0.00001437
Iteration 77/1000 | Loss: 0.00001437
Iteration 78/1000 | Loss: 0.00001436
Iteration 79/1000 | Loss: 0.00001436
Iteration 80/1000 | Loss: 0.00001435
Iteration 81/1000 | Loss: 0.00001435
Iteration 82/1000 | Loss: 0.00001435
Iteration 83/1000 | Loss: 0.00001434
Iteration 84/1000 | Loss: 0.00001434
Iteration 85/1000 | Loss: 0.00001434
Iteration 86/1000 | Loss: 0.00001434
Iteration 87/1000 | Loss: 0.00001433
Iteration 88/1000 | Loss: 0.00001433
Iteration 89/1000 | Loss: 0.00001433
Iteration 90/1000 | Loss: 0.00001432
Iteration 91/1000 | Loss: 0.00001432
Iteration 92/1000 | Loss: 0.00001432
Iteration 93/1000 | Loss: 0.00001431
Iteration 94/1000 | Loss: 0.00001431
Iteration 95/1000 | Loss: 0.00001431
Iteration 96/1000 | Loss: 0.00001431
Iteration 97/1000 | Loss: 0.00001431
Iteration 98/1000 | Loss: 0.00001430
Iteration 99/1000 | Loss: 0.00001430
Iteration 100/1000 | Loss: 0.00001430
Iteration 101/1000 | Loss: 0.00001430
Iteration 102/1000 | Loss: 0.00001430
Iteration 103/1000 | Loss: 0.00001430
Iteration 104/1000 | Loss: 0.00001429
Iteration 105/1000 | Loss: 0.00001429
Iteration 106/1000 | Loss: 0.00001429
Iteration 107/1000 | Loss: 0.00001428
Iteration 108/1000 | Loss: 0.00001428
Iteration 109/1000 | Loss: 0.00001428
Iteration 110/1000 | Loss: 0.00001428
Iteration 111/1000 | Loss: 0.00001428
Iteration 112/1000 | Loss: 0.00001427
Iteration 113/1000 | Loss: 0.00001427
Iteration 114/1000 | Loss: 0.00001427
Iteration 115/1000 | Loss: 0.00001427
Iteration 116/1000 | Loss: 0.00001427
Iteration 117/1000 | Loss: 0.00001427
Iteration 118/1000 | Loss: 0.00001427
Iteration 119/1000 | Loss: 0.00001427
Iteration 120/1000 | Loss: 0.00001426
Iteration 121/1000 | Loss: 0.00001426
Iteration 122/1000 | Loss: 0.00001426
Iteration 123/1000 | Loss: 0.00001426
Iteration 124/1000 | Loss: 0.00001425
Iteration 125/1000 | Loss: 0.00001425
Iteration 126/1000 | Loss: 0.00001425
Iteration 127/1000 | Loss: 0.00001425
Iteration 128/1000 | Loss: 0.00001425
Iteration 129/1000 | Loss: 0.00001425
Iteration 130/1000 | Loss: 0.00001425
Iteration 131/1000 | Loss: 0.00001424
Iteration 132/1000 | Loss: 0.00001424
Iteration 133/1000 | Loss: 0.00001424
Iteration 134/1000 | Loss: 0.00001424
Iteration 135/1000 | Loss: 0.00001424
Iteration 136/1000 | Loss: 0.00001424
Iteration 137/1000 | Loss: 0.00001424
Iteration 138/1000 | Loss: 0.00001424
Iteration 139/1000 | Loss: 0.00001423
Iteration 140/1000 | Loss: 0.00001423
Iteration 141/1000 | Loss: 0.00001423
Iteration 142/1000 | Loss: 0.00001423
Iteration 143/1000 | Loss: 0.00001423
Iteration 144/1000 | Loss: 0.00001422
Iteration 145/1000 | Loss: 0.00001422
Iteration 146/1000 | Loss: 0.00001422
Iteration 147/1000 | Loss: 0.00001421
Iteration 148/1000 | Loss: 0.00001421
Iteration 149/1000 | Loss: 0.00001421
Iteration 150/1000 | Loss: 0.00001420
Iteration 151/1000 | Loss: 0.00001420
Iteration 152/1000 | Loss: 0.00001420
Iteration 153/1000 | Loss: 0.00001420
Iteration 154/1000 | Loss: 0.00001420
Iteration 155/1000 | Loss: 0.00001420
Iteration 156/1000 | Loss: 0.00001420
Iteration 157/1000 | Loss: 0.00001420
Iteration 158/1000 | Loss: 0.00001420
Iteration 159/1000 | Loss: 0.00001420
Iteration 160/1000 | Loss: 0.00001419
Iteration 161/1000 | Loss: 0.00001419
Iteration 162/1000 | Loss: 0.00001419
Iteration 163/1000 | Loss: 0.00001419
Iteration 164/1000 | Loss: 0.00001419
Iteration 165/1000 | Loss: 0.00001419
Iteration 166/1000 | Loss: 0.00001419
Iteration 167/1000 | Loss: 0.00001419
Iteration 168/1000 | Loss: 0.00001419
Iteration 169/1000 | Loss: 0.00001419
Iteration 170/1000 | Loss: 0.00001419
Iteration 171/1000 | Loss: 0.00001419
Iteration 172/1000 | Loss: 0.00001419
Iteration 173/1000 | Loss: 0.00001419
Iteration 174/1000 | Loss: 0.00001419
Iteration 175/1000 | Loss: 0.00001419
Iteration 176/1000 | Loss: 0.00001419
Iteration 177/1000 | Loss: 0.00001419
Iteration 178/1000 | Loss: 0.00001419
Iteration 179/1000 | Loss: 0.00001419
Iteration 180/1000 | Loss: 0.00001419
Iteration 181/1000 | Loss: 0.00001419
Iteration 182/1000 | Loss: 0.00001419
Iteration 183/1000 | Loss: 0.00001419
Iteration 184/1000 | Loss: 0.00001419
Iteration 185/1000 | Loss: 0.00001419
Iteration 186/1000 | Loss: 0.00001419
Iteration 187/1000 | Loss: 0.00001419
Iteration 188/1000 | Loss: 0.00001419
Iteration 189/1000 | Loss: 0.00001419
Iteration 190/1000 | Loss: 0.00001419
Iteration 191/1000 | Loss: 0.00001419
Iteration 192/1000 | Loss: 0.00001419
Iteration 193/1000 | Loss: 0.00001419
Iteration 194/1000 | Loss: 0.00001419
Iteration 195/1000 | Loss: 0.00001419
Iteration 196/1000 | Loss: 0.00001419
Iteration 197/1000 | Loss: 0.00001419
Iteration 198/1000 | Loss: 0.00001419
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 198. Stopping optimization.
Last 5 losses: [1.41869477374712e-05, 1.41869477374712e-05, 1.41869477374712e-05, 1.41869477374712e-05, 1.41869477374712e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.41869477374712e-05

Optimization complete. Final v2v error: 3.18483829498291 mm

Highest mean error: 4.338199138641357 mm for frame 183

Lowest mean error: 2.751580238342285 mm for frame 128

Saving results

Total time: 75.84041357040405
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_003/1024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_003/1024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_003/1024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00454071
Iteration 2/25 | Loss: 0.00143277
Iteration 3/25 | Loss: 0.00127849
Iteration 4/25 | Loss: 0.00127102
Iteration 5/25 | Loss: 0.00127084
Iteration 6/25 | Loss: 0.00127084
Iteration 7/25 | Loss: 0.00127084
Iteration 8/25 | Loss: 0.00127084
Iteration 9/25 | Loss: 0.00127084
Iteration 10/25 | Loss: 0.00127084
Iteration 11/25 | Loss: 0.00127084
Iteration 12/25 | Loss: 0.00127084
Iteration 13/25 | Loss: 0.00127084
Iteration 14/25 | Loss: 0.00127084
Iteration 15/25 | Loss: 0.00127084
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0012708395952358842, 0.0012708395952358842, 0.0012708395952358842, 0.0012708395952358842, 0.0012708395952358842]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012708395952358842

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.29645669
Iteration 2/25 | Loss: 0.00096357
Iteration 3/25 | Loss: 0.00096357
Iteration 4/25 | Loss: 0.00096357
Iteration 5/25 | Loss: 0.00096357
Iteration 6/25 | Loss: 0.00096357
Iteration 7/25 | Loss: 0.00096356
Iteration 8/25 | Loss: 0.00096356
Iteration 9/25 | Loss: 0.00096356
Iteration 10/25 | Loss: 0.00096356
Iteration 11/25 | Loss: 0.00096356
Iteration 12/25 | Loss: 0.00096356
Iteration 13/25 | Loss: 0.00096356
Iteration 14/25 | Loss: 0.00096356
Iteration 15/25 | Loss: 0.00096356
Iteration 16/25 | Loss: 0.00096356
Iteration 17/25 | Loss: 0.00096356
Iteration 18/25 | Loss: 0.00096356
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0009635636233724654, 0.0009635636233724654, 0.0009635636233724654, 0.0009635636233724654, 0.0009635636233724654]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009635636233724654

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00096356
Iteration 2/1000 | Loss: 0.00002715
Iteration 3/1000 | Loss: 0.00001791
Iteration 4/1000 | Loss: 0.00001432
Iteration 5/1000 | Loss: 0.00001344
Iteration 6/1000 | Loss: 0.00001307
Iteration 7/1000 | Loss: 0.00001273
Iteration 8/1000 | Loss: 0.00001246
Iteration 9/1000 | Loss: 0.00001244
Iteration 10/1000 | Loss: 0.00001219
Iteration 11/1000 | Loss: 0.00001203
Iteration 12/1000 | Loss: 0.00001187
Iteration 13/1000 | Loss: 0.00001182
Iteration 14/1000 | Loss: 0.00001166
Iteration 15/1000 | Loss: 0.00001155
Iteration 16/1000 | Loss: 0.00001148
Iteration 17/1000 | Loss: 0.00001148
Iteration 18/1000 | Loss: 0.00001143
Iteration 19/1000 | Loss: 0.00001141
Iteration 20/1000 | Loss: 0.00001128
Iteration 21/1000 | Loss: 0.00001126
Iteration 22/1000 | Loss: 0.00001125
Iteration 23/1000 | Loss: 0.00001125
Iteration 24/1000 | Loss: 0.00001124
Iteration 25/1000 | Loss: 0.00001120
Iteration 26/1000 | Loss: 0.00001114
Iteration 27/1000 | Loss: 0.00001112
Iteration 28/1000 | Loss: 0.00001108
Iteration 29/1000 | Loss: 0.00001104
Iteration 30/1000 | Loss: 0.00001104
Iteration 31/1000 | Loss: 0.00001104
Iteration 32/1000 | Loss: 0.00001104
Iteration 33/1000 | Loss: 0.00001103
Iteration 34/1000 | Loss: 0.00001103
Iteration 35/1000 | Loss: 0.00001103
Iteration 36/1000 | Loss: 0.00001103
Iteration 37/1000 | Loss: 0.00001103
Iteration 38/1000 | Loss: 0.00001103
Iteration 39/1000 | Loss: 0.00001103
Iteration 40/1000 | Loss: 0.00001103
Iteration 41/1000 | Loss: 0.00001103
Iteration 42/1000 | Loss: 0.00001103
Iteration 43/1000 | Loss: 0.00001102
Iteration 44/1000 | Loss: 0.00001102
Iteration 45/1000 | Loss: 0.00001102
Iteration 46/1000 | Loss: 0.00001102
Iteration 47/1000 | Loss: 0.00001102
Iteration 48/1000 | Loss: 0.00001102
Iteration 49/1000 | Loss: 0.00001101
Iteration 50/1000 | Loss: 0.00001101
Iteration 51/1000 | Loss: 0.00001100
Iteration 52/1000 | Loss: 0.00001100
Iteration 53/1000 | Loss: 0.00001100
Iteration 54/1000 | Loss: 0.00001100
Iteration 55/1000 | Loss: 0.00001099
Iteration 56/1000 | Loss: 0.00001099
Iteration 57/1000 | Loss: 0.00001099
Iteration 58/1000 | Loss: 0.00001099
Iteration 59/1000 | Loss: 0.00001099
Iteration 60/1000 | Loss: 0.00001099
Iteration 61/1000 | Loss: 0.00001098
Iteration 62/1000 | Loss: 0.00001098
Iteration 63/1000 | Loss: 0.00001098
Iteration 64/1000 | Loss: 0.00001098
Iteration 65/1000 | Loss: 0.00001097
Iteration 66/1000 | Loss: 0.00001097
Iteration 67/1000 | Loss: 0.00001096
Iteration 68/1000 | Loss: 0.00001096
Iteration 69/1000 | Loss: 0.00001095
Iteration 70/1000 | Loss: 0.00001095
Iteration 71/1000 | Loss: 0.00001095
Iteration 72/1000 | Loss: 0.00001094
Iteration 73/1000 | Loss: 0.00001094
Iteration 74/1000 | Loss: 0.00001094
Iteration 75/1000 | Loss: 0.00001094
Iteration 76/1000 | Loss: 0.00001094
Iteration 77/1000 | Loss: 0.00001094
Iteration 78/1000 | Loss: 0.00001093
Iteration 79/1000 | Loss: 0.00001093
Iteration 80/1000 | Loss: 0.00001093
Iteration 81/1000 | Loss: 0.00001093
Iteration 82/1000 | Loss: 0.00001093
Iteration 83/1000 | Loss: 0.00001093
Iteration 84/1000 | Loss: 0.00001093
Iteration 85/1000 | Loss: 0.00001093
Iteration 86/1000 | Loss: 0.00001093
Iteration 87/1000 | Loss: 0.00001093
Iteration 88/1000 | Loss: 0.00001092
Iteration 89/1000 | Loss: 0.00001092
Iteration 90/1000 | Loss: 0.00001092
Iteration 91/1000 | Loss: 0.00001092
Iteration 92/1000 | Loss: 0.00001092
Iteration 93/1000 | Loss: 0.00001092
Iteration 94/1000 | Loss: 0.00001092
Iteration 95/1000 | Loss: 0.00001092
Iteration 96/1000 | Loss: 0.00001092
Iteration 97/1000 | Loss: 0.00001092
Iteration 98/1000 | Loss: 0.00001092
Iteration 99/1000 | Loss: 0.00001091
Iteration 100/1000 | Loss: 0.00001091
Iteration 101/1000 | Loss: 0.00001091
Iteration 102/1000 | Loss: 0.00001091
Iteration 103/1000 | Loss: 0.00001091
Iteration 104/1000 | Loss: 0.00001091
Iteration 105/1000 | Loss: 0.00001091
Iteration 106/1000 | Loss: 0.00001091
Iteration 107/1000 | Loss: 0.00001091
Iteration 108/1000 | Loss: 0.00001091
Iteration 109/1000 | Loss: 0.00001091
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 109. Stopping optimization.
Last 5 losses: [1.0912543984886725e-05, 1.0912543984886725e-05, 1.0912543984886725e-05, 1.0912543984886725e-05, 1.0912543984886725e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0912543984886725e-05

Optimization complete. Final v2v error: 2.8785548210144043 mm

Highest mean error: 3.1022441387176514 mm for frame 95

Lowest mean error: 2.708714723587036 mm for frame 198

Saving results

Total time: 38.235483169555664
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_003/1065/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_003/1065.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_003/1065
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00759295
Iteration 2/25 | Loss: 0.00149016
Iteration 3/25 | Loss: 0.00130430
Iteration 4/25 | Loss: 0.00128321
Iteration 5/25 | Loss: 0.00127818
Iteration 6/25 | Loss: 0.00127726
Iteration 7/25 | Loss: 0.00127726
Iteration 8/25 | Loss: 0.00127726
Iteration 9/25 | Loss: 0.00127726
Iteration 10/25 | Loss: 0.00127726
Iteration 11/25 | Loss: 0.00127726
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012772574555128813, 0.0012772574555128813, 0.0012772574555128813, 0.0012772574555128813, 0.0012772574555128813]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012772574555128813

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.35563922
Iteration 2/25 | Loss: 0.00122806
Iteration 3/25 | Loss: 0.00122805
Iteration 4/25 | Loss: 0.00122805
Iteration 5/25 | Loss: 0.00122805
Iteration 6/25 | Loss: 0.00122805
Iteration 7/25 | Loss: 0.00122805
Iteration 8/25 | Loss: 0.00122805
Iteration 9/25 | Loss: 0.00122805
Iteration 10/25 | Loss: 0.00122805
Iteration 11/25 | Loss: 0.00122805
Iteration 12/25 | Loss: 0.00122805
Iteration 13/25 | Loss: 0.00122805
Iteration 14/25 | Loss: 0.00122805
Iteration 15/25 | Loss: 0.00122805
Iteration 16/25 | Loss: 0.00122805
Iteration 17/25 | Loss: 0.00122805
Iteration 18/25 | Loss: 0.00122805
Iteration 19/25 | Loss: 0.00122805
Iteration 20/25 | Loss: 0.00122805
Iteration 21/25 | Loss: 0.00122805
Iteration 22/25 | Loss: 0.00122805
Iteration 23/25 | Loss: 0.00122805
Iteration 24/25 | Loss: 0.00122805
Iteration 25/25 | Loss: 0.00122805

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00122805
Iteration 2/1000 | Loss: 0.00004255
Iteration 3/1000 | Loss: 0.00002356
Iteration 4/1000 | Loss: 0.00002075
Iteration 5/1000 | Loss: 0.00001919
Iteration 6/1000 | Loss: 0.00001839
Iteration 7/1000 | Loss: 0.00001784
Iteration 8/1000 | Loss: 0.00001755
Iteration 9/1000 | Loss: 0.00001727
Iteration 10/1000 | Loss: 0.00001701
Iteration 11/1000 | Loss: 0.00001679
Iteration 12/1000 | Loss: 0.00001675
Iteration 13/1000 | Loss: 0.00001664
Iteration 14/1000 | Loss: 0.00001655
Iteration 15/1000 | Loss: 0.00001651
Iteration 16/1000 | Loss: 0.00001646
Iteration 17/1000 | Loss: 0.00001644
Iteration 18/1000 | Loss: 0.00001643
Iteration 19/1000 | Loss: 0.00001642
Iteration 20/1000 | Loss: 0.00001641
Iteration 21/1000 | Loss: 0.00001636
Iteration 22/1000 | Loss: 0.00001635
Iteration 23/1000 | Loss: 0.00001634
Iteration 24/1000 | Loss: 0.00001634
Iteration 25/1000 | Loss: 0.00001633
Iteration 26/1000 | Loss: 0.00001629
Iteration 27/1000 | Loss: 0.00001626
Iteration 28/1000 | Loss: 0.00001625
Iteration 29/1000 | Loss: 0.00001625
Iteration 30/1000 | Loss: 0.00001625
Iteration 31/1000 | Loss: 0.00001625
Iteration 32/1000 | Loss: 0.00001624
Iteration 33/1000 | Loss: 0.00001624
Iteration 34/1000 | Loss: 0.00001624
Iteration 35/1000 | Loss: 0.00001623
Iteration 36/1000 | Loss: 0.00001623
Iteration 37/1000 | Loss: 0.00001623
Iteration 38/1000 | Loss: 0.00001622
Iteration 39/1000 | Loss: 0.00001621
Iteration 40/1000 | Loss: 0.00001621
Iteration 41/1000 | Loss: 0.00001621
Iteration 42/1000 | Loss: 0.00001621
Iteration 43/1000 | Loss: 0.00001620
Iteration 44/1000 | Loss: 0.00001620
Iteration 45/1000 | Loss: 0.00001620
Iteration 46/1000 | Loss: 0.00001619
Iteration 47/1000 | Loss: 0.00001619
Iteration 48/1000 | Loss: 0.00001618
Iteration 49/1000 | Loss: 0.00001617
Iteration 50/1000 | Loss: 0.00001617
Iteration 51/1000 | Loss: 0.00001617
Iteration 52/1000 | Loss: 0.00001616
Iteration 53/1000 | Loss: 0.00001615
Iteration 54/1000 | Loss: 0.00001615
Iteration 55/1000 | Loss: 0.00001615
Iteration 56/1000 | Loss: 0.00001615
Iteration 57/1000 | Loss: 0.00001614
Iteration 58/1000 | Loss: 0.00001614
Iteration 59/1000 | Loss: 0.00001614
Iteration 60/1000 | Loss: 0.00001614
Iteration 61/1000 | Loss: 0.00001613
Iteration 62/1000 | Loss: 0.00001613
Iteration 63/1000 | Loss: 0.00001613
Iteration 64/1000 | Loss: 0.00001612
Iteration 65/1000 | Loss: 0.00001612
Iteration 66/1000 | Loss: 0.00001612
Iteration 67/1000 | Loss: 0.00001612
Iteration 68/1000 | Loss: 0.00001611
Iteration 69/1000 | Loss: 0.00001611
Iteration 70/1000 | Loss: 0.00001610
Iteration 71/1000 | Loss: 0.00001610
Iteration 72/1000 | Loss: 0.00001609
Iteration 73/1000 | Loss: 0.00001609
Iteration 74/1000 | Loss: 0.00001608
Iteration 75/1000 | Loss: 0.00001607
Iteration 76/1000 | Loss: 0.00001607
Iteration 77/1000 | Loss: 0.00001607
Iteration 78/1000 | Loss: 0.00001606
Iteration 79/1000 | Loss: 0.00001605
Iteration 80/1000 | Loss: 0.00001605
Iteration 81/1000 | Loss: 0.00001604
Iteration 82/1000 | Loss: 0.00001604
Iteration 83/1000 | Loss: 0.00001603
Iteration 84/1000 | Loss: 0.00001603
Iteration 85/1000 | Loss: 0.00001603
Iteration 86/1000 | Loss: 0.00001603
Iteration 87/1000 | Loss: 0.00001603
Iteration 88/1000 | Loss: 0.00001602
Iteration 89/1000 | Loss: 0.00001601
Iteration 90/1000 | Loss: 0.00001601
Iteration 91/1000 | Loss: 0.00001601
Iteration 92/1000 | Loss: 0.00001601
Iteration 93/1000 | Loss: 0.00001600
Iteration 94/1000 | Loss: 0.00001599
Iteration 95/1000 | Loss: 0.00001599
Iteration 96/1000 | Loss: 0.00001599
Iteration 97/1000 | Loss: 0.00001599
Iteration 98/1000 | Loss: 0.00001599
Iteration 99/1000 | Loss: 0.00001598
Iteration 100/1000 | Loss: 0.00001598
Iteration 101/1000 | Loss: 0.00001598
Iteration 102/1000 | Loss: 0.00001598
Iteration 103/1000 | Loss: 0.00001598
Iteration 104/1000 | Loss: 0.00001598
Iteration 105/1000 | Loss: 0.00001597
Iteration 106/1000 | Loss: 0.00001597
Iteration 107/1000 | Loss: 0.00001597
Iteration 108/1000 | Loss: 0.00001597
Iteration 109/1000 | Loss: 0.00001597
Iteration 110/1000 | Loss: 0.00001596
Iteration 111/1000 | Loss: 0.00001596
Iteration 112/1000 | Loss: 0.00001596
Iteration 113/1000 | Loss: 0.00001596
Iteration 114/1000 | Loss: 0.00001596
Iteration 115/1000 | Loss: 0.00001596
Iteration 116/1000 | Loss: 0.00001595
Iteration 117/1000 | Loss: 0.00001595
Iteration 118/1000 | Loss: 0.00001595
Iteration 119/1000 | Loss: 0.00001595
Iteration 120/1000 | Loss: 0.00001594
Iteration 121/1000 | Loss: 0.00001594
Iteration 122/1000 | Loss: 0.00001594
Iteration 123/1000 | Loss: 0.00001594
Iteration 124/1000 | Loss: 0.00001594
Iteration 125/1000 | Loss: 0.00001594
Iteration 126/1000 | Loss: 0.00001594
Iteration 127/1000 | Loss: 0.00001594
Iteration 128/1000 | Loss: 0.00001594
Iteration 129/1000 | Loss: 0.00001594
Iteration 130/1000 | Loss: 0.00001594
Iteration 131/1000 | Loss: 0.00001594
Iteration 132/1000 | Loss: 0.00001594
Iteration 133/1000 | Loss: 0.00001594
Iteration 134/1000 | Loss: 0.00001594
Iteration 135/1000 | Loss: 0.00001594
Iteration 136/1000 | Loss: 0.00001594
Iteration 137/1000 | Loss: 0.00001594
Iteration 138/1000 | Loss: 0.00001594
Iteration 139/1000 | Loss: 0.00001594
Iteration 140/1000 | Loss: 0.00001594
Iteration 141/1000 | Loss: 0.00001594
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 141. Stopping optimization.
Last 5 losses: [1.5941472156555392e-05, 1.5941472156555392e-05, 1.5941472156555392e-05, 1.5941472156555392e-05, 1.5941472156555392e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5941472156555392e-05

Optimization complete. Final v2v error: 3.3632593154907227 mm

Highest mean error: 4.305664539337158 mm for frame 142

Lowest mean error: 2.9512791633605957 mm for frame 114

Saving results

Total time: 45.07475280761719
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_003/1069/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_003/1069.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_003/1069
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00457202
Iteration 2/25 | Loss: 0.00132001
Iteration 3/25 | Loss: 0.00124683
Iteration 4/25 | Loss: 0.00123878
Iteration 5/25 | Loss: 0.00123564
Iteration 6/25 | Loss: 0.00123542
Iteration 7/25 | Loss: 0.00123542
Iteration 8/25 | Loss: 0.00123542
Iteration 9/25 | Loss: 0.00123542
Iteration 10/25 | Loss: 0.00123542
Iteration 11/25 | Loss: 0.00123542
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012354230275377631, 0.0012354230275377631, 0.0012354230275377631, 0.0012354230275377631, 0.0012354230275377631]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012354230275377631

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32471025
Iteration 2/25 | Loss: 0.00109425
Iteration 3/25 | Loss: 0.00109425
Iteration 4/25 | Loss: 0.00109425
Iteration 5/25 | Loss: 0.00109425
Iteration 6/25 | Loss: 0.00109425
Iteration 7/25 | Loss: 0.00109425
Iteration 8/25 | Loss: 0.00109425
Iteration 9/25 | Loss: 0.00109425
Iteration 10/25 | Loss: 0.00109425
Iteration 11/25 | Loss: 0.00109425
Iteration 12/25 | Loss: 0.00109425
Iteration 13/25 | Loss: 0.00109425
Iteration 14/25 | Loss: 0.00109425
Iteration 15/25 | Loss: 0.00109425
Iteration 16/25 | Loss: 0.00109425
Iteration 17/25 | Loss: 0.00109425
Iteration 18/25 | Loss: 0.00109425
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.001094247680157423, 0.001094247680157423, 0.001094247680157423, 0.001094247680157423, 0.001094247680157423]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001094247680157423

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00109425
Iteration 2/1000 | Loss: 0.00002228
Iteration 3/1000 | Loss: 0.00001715
Iteration 4/1000 | Loss: 0.00001565
Iteration 5/1000 | Loss: 0.00001499
Iteration 6/1000 | Loss: 0.00001464
Iteration 7/1000 | Loss: 0.00001449
Iteration 8/1000 | Loss: 0.00001423
Iteration 9/1000 | Loss: 0.00001404
Iteration 10/1000 | Loss: 0.00001376
Iteration 11/1000 | Loss: 0.00001360
Iteration 12/1000 | Loss: 0.00001357
Iteration 13/1000 | Loss: 0.00001353
Iteration 14/1000 | Loss: 0.00001352
Iteration 15/1000 | Loss: 0.00001346
Iteration 16/1000 | Loss: 0.00001343
Iteration 17/1000 | Loss: 0.00001342
Iteration 18/1000 | Loss: 0.00001340
Iteration 19/1000 | Loss: 0.00001338
Iteration 20/1000 | Loss: 0.00001338
Iteration 21/1000 | Loss: 0.00001337
Iteration 22/1000 | Loss: 0.00001335
Iteration 23/1000 | Loss: 0.00001335
Iteration 24/1000 | Loss: 0.00001334
Iteration 25/1000 | Loss: 0.00001334
Iteration 26/1000 | Loss: 0.00001334
Iteration 27/1000 | Loss: 0.00001334
Iteration 28/1000 | Loss: 0.00001332
Iteration 29/1000 | Loss: 0.00001331
Iteration 30/1000 | Loss: 0.00001331
Iteration 31/1000 | Loss: 0.00001331
Iteration 32/1000 | Loss: 0.00001331
Iteration 33/1000 | Loss: 0.00001330
Iteration 34/1000 | Loss: 0.00001330
Iteration 35/1000 | Loss: 0.00001330
Iteration 36/1000 | Loss: 0.00001330
Iteration 37/1000 | Loss: 0.00001330
Iteration 38/1000 | Loss: 0.00001330
Iteration 39/1000 | Loss: 0.00001330
Iteration 40/1000 | Loss: 0.00001330
Iteration 41/1000 | Loss: 0.00001330
Iteration 42/1000 | Loss: 0.00001330
Iteration 43/1000 | Loss: 0.00001330
Iteration 44/1000 | Loss: 0.00001330
Iteration 45/1000 | Loss: 0.00001329
Iteration 46/1000 | Loss: 0.00001329
Iteration 47/1000 | Loss: 0.00001329
Iteration 48/1000 | Loss: 0.00001329
Iteration 49/1000 | Loss: 0.00001329
Iteration 50/1000 | Loss: 0.00001328
Iteration 51/1000 | Loss: 0.00001328
Iteration 52/1000 | Loss: 0.00001328
Iteration 53/1000 | Loss: 0.00001327
Iteration 54/1000 | Loss: 0.00001327
Iteration 55/1000 | Loss: 0.00001326
Iteration 56/1000 | Loss: 0.00001326
Iteration 57/1000 | Loss: 0.00001325
Iteration 58/1000 | Loss: 0.00001325
Iteration 59/1000 | Loss: 0.00001325
Iteration 60/1000 | Loss: 0.00001324
Iteration 61/1000 | Loss: 0.00001324
Iteration 62/1000 | Loss: 0.00001324
Iteration 63/1000 | Loss: 0.00001323
Iteration 64/1000 | Loss: 0.00001322
Iteration 65/1000 | Loss: 0.00001322
Iteration 66/1000 | Loss: 0.00001321
Iteration 67/1000 | Loss: 0.00001321
Iteration 68/1000 | Loss: 0.00001320
Iteration 69/1000 | Loss: 0.00001320
Iteration 70/1000 | Loss: 0.00001319
Iteration 71/1000 | Loss: 0.00001319
Iteration 72/1000 | Loss: 0.00001319
Iteration 73/1000 | Loss: 0.00001319
Iteration 74/1000 | Loss: 0.00001318
Iteration 75/1000 | Loss: 0.00001318
Iteration 76/1000 | Loss: 0.00001318
Iteration 77/1000 | Loss: 0.00001318
Iteration 78/1000 | Loss: 0.00001318
Iteration 79/1000 | Loss: 0.00001318
Iteration 80/1000 | Loss: 0.00001318
Iteration 81/1000 | Loss: 0.00001318
Iteration 82/1000 | Loss: 0.00001318
Iteration 83/1000 | Loss: 0.00001317
Iteration 84/1000 | Loss: 0.00001317
Iteration 85/1000 | Loss: 0.00001317
Iteration 86/1000 | Loss: 0.00001317
Iteration 87/1000 | Loss: 0.00001317
Iteration 88/1000 | Loss: 0.00001317
Iteration 89/1000 | Loss: 0.00001317
Iteration 90/1000 | Loss: 0.00001317
Iteration 91/1000 | Loss: 0.00001317
Iteration 92/1000 | Loss: 0.00001317
Iteration 93/1000 | Loss: 0.00001317
Iteration 94/1000 | Loss: 0.00001316
Iteration 95/1000 | Loss: 0.00001316
Iteration 96/1000 | Loss: 0.00001316
Iteration 97/1000 | Loss: 0.00001316
Iteration 98/1000 | Loss: 0.00001316
Iteration 99/1000 | Loss: 0.00001316
Iteration 100/1000 | Loss: 0.00001316
Iteration 101/1000 | Loss: 0.00001315
Iteration 102/1000 | Loss: 0.00001315
Iteration 103/1000 | Loss: 0.00001315
Iteration 104/1000 | Loss: 0.00001315
Iteration 105/1000 | Loss: 0.00001315
Iteration 106/1000 | Loss: 0.00001315
Iteration 107/1000 | Loss: 0.00001315
Iteration 108/1000 | Loss: 0.00001315
Iteration 109/1000 | Loss: 0.00001315
Iteration 110/1000 | Loss: 0.00001314
Iteration 111/1000 | Loss: 0.00001314
Iteration 112/1000 | Loss: 0.00001314
Iteration 113/1000 | Loss: 0.00001314
Iteration 114/1000 | Loss: 0.00001313
Iteration 115/1000 | Loss: 0.00001313
Iteration 116/1000 | Loss: 0.00001313
Iteration 117/1000 | Loss: 0.00001313
Iteration 118/1000 | Loss: 0.00001313
Iteration 119/1000 | Loss: 0.00001313
Iteration 120/1000 | Loss: 0.00001313
Iteration 121/1000 | Loss: 0.00001313
Iteration 122/1000 | Loss: 0.00001313
Iteration 123/1000 | Loss: 0.00001313
Iteration 124/1000 | Loss: 0.00001313
Iteration 125/1000 | Loss: 0.00001313
Iteration 126/1000 | Loss: 0.00001313
Iteration 127/1000 | Loss: 0.00001313
Iteration 128/1000 | Loss: 0.00001313
Iteration 129/1000 | Loss: 0.00001313
Iteration 130/1000 | Loss: 0.00001313
Iteration 131/1000 | Loss: 0.00001313
Iteration 132/1000 | Loss: 0.00001313
Iteration 133/1000 | Loss: 0.00001313
Iteration 134/1000 | Loss: 0.00001313
Iteration 135/1000 | Loss: 0.00001313
Iteration 136/1000 | Loss: 0.00001313
Iteration 137/1000 | Loss: 0.00001313
Iteration 138/1000 | Loss: 0.00001313
Iteration 139/1000 | Loss: 0.00001313
Iteration 140/1000 | Loss: 0.00001313
Iteration 141/1000 | Loss: 0.00001313
Iteration 142/1000 | Loss: 0.00001313
Iteration 143/1000 | Loss: 0.00001313
Iteration 144/1000 | Loss: 0.00001313
Iteration 145/1000 | Loss: 0.00001313
Iteration 146/1000 | Loss: 0.00001313
Iteration 147/1000 | Loss: 0.00001313
Iteration 148/1000 | Loss: 0.00001313
Iteration 149/1000 | Loss: 0.00001313
Iteration 150/1000 | Loss: 0.00001313
Iteration 151/1000 | Loss: 0.00001313
Iteration 152/1000 | Loss: 0.00001313
Iteration 153/1000 | Loss: 0.00001313
Iteration 154/1000 | Loss: 0.00001313
Iteration 155/1000 | Loss: 0.00001313
Iteration 156/1000 | Loss: 0.00001313
Iteration 157/1000 | Loss: 0.00001313
Iteration 158/1000 | Loss: 0.00001313
Iteration 159/1000 | Loss: 0.00001313
Iteration 160/1000 | Loss: 0.00001313
Iteration 161/1000 | Loss: 0.00001313
Iteration 162/1000 | Loss: 0.00001313
Iteration 163/1000 | Loss: 0.00001313
Iteration 164/1000 | Loss: 0.00001313
Iteration 165/1000 | Loss: 0.00001313
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 165. Stopping optimization.
Last 5 losses: [1.3126189514878206e-05, 1.3126189514878206e-05, 1.3126189514878206e-05, 1.3126189514878206e-05, 1.3126189514878206e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3126189514878206e-05

Optimization complete. Final v2v error: 3.054203748703003 mm

Highest mean error: 3.3371565341949463 mm for frame 175

Lowest mean error: 2.863081693649292 mm for frame 0

Saving results

Total time: 38.31577444076538
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_003/1021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_003/1021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_003/1021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00971848
Iteration 2/25 | Loss: 0.00250694
Iteration 3/25 | Loss: 0.00172722
Iteration 4/25 | Loss: 0.00164906
Iteration 5/25 | Loss: 0.00158188
Iteration 6/25 | Loss: 0.00156241
Iteration 7/25 | Loss: 0.00164708
Iteration 8/25 | Loss: 0.00138337
Iteration 9/25 | Loss: 0.00128715
Iteration 10/25 | Loss: 0.00130272
Iteration 11/25 | Loss: 0.00123631
Iteration 12/25 | Loss: 0.00125228
Iteration 13/25 | Loss: 0.00121215
Iteration 14/25 | Loss: 0.00120655
Iteration 15/25 | Loss: 0.00120575
Iteration 16/25 | Loss: 0.00121014
Iteration 17/25 | Loss: 0.00120567
Iteration 18/25 | Loss: 0.00120567
Iteration 19/25 | Loss: 0.00120567
Iteration 20/25 | Loss: 0.00120567
Iteration 21/25 | Loss: 0.00120567
Iteration 22/25 | Loss: 0.00120566
Iteration 23/25 | Loss: 0.00120566
Iteration 24/25 | Loss: 0.00120566
Iteration 25/25 | Loss: 0.00120566

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.31728411
Iteration 2/25 | Loss: 0.00154362
Iteration 3/25 | Loss: 0.00113123
Iteration 4/25 | Loss: 0.00113123
Iteration 5/25 | Loss: 0.00113123
Iteration 6/25 | Loss: 0.00113123
Iteration 7/25 | Loss: 0.00113123
Iteration 8/25 | Loss: 0.00113123
Iteration 9/25 | Loss: 0.00113123
Iteration 10/25 | Loss: 0.00113123
Iteration 11/25 | Loss: 0.00113123
Iteration 12/25 | Loss: 0.00113123
Iteration 13/25 | Loss: 0.00113123
Iteration 14/25 | Loss: 0.00113123
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0011312281712889671, 0.0011312281712889671, 0.0011312281712889671, 0.0011312281712889671, 0.0011312281712889671]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011312281712889671

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00113123
Iteration 2/1000 | Loss: 0.00013377
Iteration 3/1000 | Loss: 0.00042066
Iteration 4/1000 | Loss: 0.00027589
Iteration 5/1000 | Loss: 0.00013019
Iteration 6/1000 | Loss: 0.00009644
Iteration 7/1000 | Loss: 0.00001929
Iteration 8/1000 | Loss: 0.00004131
Iteration 9/1000 | Loss: 0.00001614
Iteration 10/1000 | Loss: 0.00008717
Iteration 11/1000 | Loss: 0.00001603
Iteration 12/1000 | Loss: 0.00004266
Iteration 13/1000 | Loss: 0.00003503
Iteration 14/1000 | Loss: 0.00007586
Iteration 15/1000 | Loss: 0.00003493
Iteration 16/1000 | Loss: 0.00003846
Iteration 17/1000 | Loss: 0.00003799
Iteration 18/1000 | Loss: 0.00002100
Iteration 19/1000 | Loss: 0.00001449
Iteration 20/1000 | Loss: 0.00001449
Iteration 21/1000 | Loss: 0.00004613
Iteration 22/1000 | Loss: 0.00007105
Iteration 23/1000 | Loss: 0.00008065
Iteration 24/1000 | Loss: 0.00001499
Iteration 25/1000 | Loss: 0.00001425
Iteration 26/1000 | Loss: 0.00001425
Iteration 27/1000 | Loss: 0.00001425
Iteration 28/1000 | Loss: 0.00001423
Iteration 29/1000 | Loss: 0.00001422
Iteration 30/1000 | Loss: 0.00001421
Iteration 31/1000 | Loss: 0.00001420
Iteration 32/1000 | Loss: 0.00001419
Iteration 33/1000 | Loss: 0.00001419
Iteration 34/1000 | Loss: 0.00001419
Iteration 35/1000 | Loss: 0.00001419
Iteration 36/1000 | Loss: 0.00001419
Iteration 37/1000 | Loss: 0.00001418
Iteration 38/1000 | Loss: 0.00001418
Iteration 39/1000 | Loss: 0.00001418
Iteration 40/1000 | Loss: 0.00001418
Iteration 41/1000 | Loss: 0.00001418
Iteration 42/1000 | Loss: 0.00001418
Iteration 43/1000 | Loss: 0.00001417
Iteration 44/1000 | Loss: 0.00001417
Iteration 45/1000 | Loss: 0.00001417
Iteration 46/1000 | Loss: 0.00001417
Iteration 47/1000 | Loss: 0.00001417
Iteration 48/1000 | Loss: 0.00001417
Iteration 49/1000 | Loss: 0.00001417
Iteration 50/1000 | Loss: 0.00001417
Iteration 51/1000 | Loss: 0.00001417
Iteration 52/1000 | Loss: 0.00001417
Iteration 53/1000 | Loss: 0.00001417
Iteration 54/1000 | Loss: 0.00001417
Iteration 55/1000 | Loss: 0.00001417
Iteration 56/1000 | Loss: 0.00001417
Iteration 57/1000 | Loss: 0.00001417
Iteration 58/1000 | Loss: 0.00001416
Iteration 59/1000 | Loss: 0.00001416
Iteration 60/1000 | Loss: 0.00001416
Iteration 61/1000 | Loss: 0.00001416
Iteration 62/1000 | Loss: 0.00001416
Iteration 63/1000 | Loss: 0.00001416
Iteration 64/1000 | Loss: 0.00001416
Iteration 65/1000 | Loss: 0.00001416
Iteration 66/1000 | Loss: 0.00001416
Iteration 67/1000 | Loss: 0.00001415
Iteration 68/1000 | Loss: 0.00001415
Iteration 69/1000 | Loss: 0.00002460
Iteration 70/1000 | Loss: 0.00001416
Iteration 71/1000 | Loss: 0.00001413
Iteration 72/1000 | Loss: 0.00001413
Iteration 73/1000 | Loss: 0.00001412
Iteration 74/1000 | Loss: 0.00001412
Iteration 75/1000 | Loss: 0.00001412
Iteration 76/1000 | Loss: 0.00001412
Iteration 77/1000 | Loss: 0.00001412
Iteration 78/1000 | Loss: 0.00001412
Iteration 79/1000 | Loss: 0.00001412
Iteration 80/1000 | Loss: 0.00001412
Iteration 81/1000 | Loss: 0.00001412
Iteration 82/1000 | Loss: 0.00001412
Iteration 83/1000 | Loss: 0.00001412
Iteration 84/1000 | Loss: 0.00001412
Iteration 85/1000 | Loss: 0.00001412
Iteration 86/1000 | Loss: 0.00001412
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 86. Stopping optimization.
Last 5 losses: [1.4117072169028688e-05, 1.4117072169028688e-05, 1.4117072169028688e-05, 1.4117072169028688e-05, 1.4117072169028688e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4117072169028688e-05

Optimization complete. Final v2v error: 3.1800119876861572 mm

Highest mean error: 3.7530124187469482 mm for frame 81

Lowest mean error: 2.794663667678833 mm for frame 140

Saving results

Total time: 69.07193398475647
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_003/1027/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_003/1027.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_003/1027
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01006322
Iteration 2/25 | Loss: 0.00212478
Iteration 3/25 | Loss: 0.00253064
Iteration 4/25 | Loss: 0.00147014
Iteration 5/25 | Loss: 0.00137226
Iteration 6/25 | Loss: 0.00125701
Iteration 7/25 | Loss: 0.00124050
Iteration 8/25 | Loss: 0.00123407
Iteration 9/25 | Loss: 0.00122908
Iteration 10/25 | Loss: 0.00122631
Iteration 11/25 | Loss: 0.00122520
Iteration 12/25 | Loss: 0.00122483
Iteration 13/25 | Loss: 0.00122472
Iteration 14/25 | Loss: 0.00122469
Iteration 15/25 | Loss: 0.00122469
Iteration 16/25 | Loss: 0.00122468
Iteration 17/25 | Loss: 0.00122464
Iteration 18/25 | Loss: 0.00122464
Iteration 19/25 | Loss: 0.00122464
Iteration 20/25 | Loss: 0.00122464
Iteration 21/25 | Loss: 0.00122463
Iteration 22/25 | Loss: 0.00122463
Iteration 23/25 | Loss: 0.00122463
Iteration 24/25 | Loss: 0.00122463
Iteration 25/25 | Loss: 0.00122463

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43092644
Iteration 2/25 | Loss: 0.00128265
Iteration 3/25 | Loss: 0.00128265
Iteration 4/25 | Loss: 0.00128265
Iteration 5/25 | Loss: 0.00128265
Iteration 6/25 | Loss: 0.00128265
Iteration 7/25 | Loss: 0.00128265
Iteration 8/25 | Loss: 0.00128265
Iteration 9/25 | Loss: 0.00128265
Iteration 10/25 | Loss: 0.00128265
Iteration 11/25 | Loss: 0.00128265
Iteration 12/25 | Loss: 0.00128265
Iteration 13/25 | Loss: 0.00128265
Iteration 14/25 | Loss: 0.00128265
Iteration 15/25 | Loss: 0.00128265
Iteration 16/25 | Loss: 0.00128265
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.001282649696804583, 0.001282649696804583, 0.001282649696804583, 0.001282649696804583, 0.001282649696804583]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001282649696804583

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00128265
Iteration 2/1000 | Loss: 0.00003453
Iteration 3/1000 | Loss: 0.00002401
Iteration 4/1000 | Loss: 0.00001941
Iteration 5/1000 | Loss: 0.00001715
Iteration 6/1000 | Loss: 0.00001582
Iteration 7/1000 | Loss: 0.00001496
Iteration 8/1000 | Loss: 0.00001427
Iteration 9/1000 | Loss: 0.00001395
Iteration 10/1000 | Loss: 0.00001368
Iteration 11/1000 | Loss: 0.00001349
Iteration 12/1000 | Loss: 0.00001331
Iteration 13/1000 | Loss: 0.00001325
Iteration 14/1000 | Loss: 0.00001321
Iteration 15/1000 | Loss: 0.00001321
Iteration 16/1000 | Loss: 0.00001311
Iteration 17/1000 | Loss: 0.00001308
Iteration 18/1000 | Loss: 0.00001307
Iteration 19/1000 | Loss: 0.00001307
Iteration 20/1000 | Loss: 0.00001307
Iteration 21/1000 | Loss: 0.00001306
Iteration 22/1000 | Loss: 0.00001306
Iteration 23/1000 | Loss: 0.00001306
Iteration 24/1000 | Loss: 0.00001306
Iteration 25/1000 | Loss: 0.00001304
Iteration 26/1000 | Loss: 0.00001304
Iteration 27/1000 | Loss: 0.00001304
Iteration 28/1000 | Loss: 0.00001304
Iteration 29/1000 | Loss: 0.00001304
Iteration 30/1000 | Loss: 0.00001304
Iteration 31/1000 | Loss: 0.00001304
Iteration 32/1000 | Loss: 0.00001304
Iteration 33/1000 | Loss: 0.00001303
Iteration 34/1000 | Loss: 0.00001303
Iteration 35/1000 | Loss: 0.00001303
Iteration 36/1000 | Loss: 0.00001303
Iteration 37/1000 | Loss: 0.00001303
Iteration 38/1000 | Loss: 0.00001303
Iteration 39/1000 | Loss: 0.00001302
Iteration 40/1000 | Loss: 0.00001302
Iteration 41/1000 | Loss: 0.00001301
Iteration 42/1000 | Loss: 0.00001301
Iteration 43/1000 | Loss: 0.00001301
Iteration 44/1000 | Loss: 0.00001301
Iteration 45/1000 | Loss: 0.00001300
Iteration 46/1000 | Loss: 0.00001300
Iteration 47/1000 | Loss: 0.00001300
Iteration 48/1000 | Loss: 0.00001298
Iteration 49/1000 | Loss: 0.00001298
Iteration 50/1000 | Loss: 0.00001297
Iteration 51/1000 | Loss: 0.00001297
Iteration 52/1000 | Loss: 0.00001297
Iteration 53/1000 | Loss: 0.00001297
Iteration 54/1000 | Loss: 0.00001297
Iteration 55/1000 | Loss: 0.00001297
Iteration 56/1000 | Loss: 0.00001297
Iteration 57/1000 | Loss: 0.00001297
Iteration 58/1000 | Loss: 0.00001296
Iteration 59/1000 | Loss: 0.00001296
Iteration 60/1000 | Loss: 0.00001295
Iteration 61/1000 | Loss: 0.00001295
Iteration 62/1000 | Loss: 0.00001295
Iteration 63/1000 | Loss: 0.00001294
Iteration 64/1000 | Loss: 0.00001294
Iteration 65/1000 | Loss: 0.00001294
Iteration 66/1000 | Loss: 0.00001293
Iteration 67/1000 | Loss: 0.00001293
Iteration 68/1000 | Loss: 0.00001293
Iteration 69/1000 | Loss: 0.00001293
Iteration 70/1000 | Loss: 0.00001293
Iteration 71/1000 | Loss: 0.00001293
Iteration 72/1000 | Loss: 0.00001293
Iteration 73/1000 | Loss: 0.00001293
Iteration 74/1000 | Loss: 0.00001293
Iteration 75/1000 | Loss: 0.00001293
Iteration 76/1000 | Loss: 0.00001293
Iteration 77/1000 | Loss: 0.00001292
Iteration 78/1000 | Loss: 0.00001292
Iteration 79/1000 | Loss: 0.00001292
Iteration 80/1000 | Loss: 0.00001291
Iteration 81/1000 | Loss: 0.00001291
Iteration 82/1000 | Loss: 0.00001291
Iteration 83/1000 | Loss: 0.00001291
Iteration 84/1000 | Loss: 0.00001291
Iteration 85/1000 | Loss: 0.00001291
Iteration 86/1000 | Loss: 0.00001290
Iteration 87/1000 | Loss: 0.00001290
Iteration 88/1000 | Loss: 0.00001290
Iteration 89/1000 | Loss: 0.00001290
Iteration 90/1000 | Loss: 0.00001290
Iteration 91/1000 | Loss: 0.00001290
Iteration 92/1000 | Loss: 0.00001290
Iteration 93/1000 | Loss: 0.00001290
Iteration 94/1000 | Loss: 0.00001290
Iteration 95/1000 | Loss: 0.00001289
Iteration 96/1000 | Loss: 0.00001289
Iteration 97/1000 | Loss: 0.00001288
Iteration 98/1000 | Loss: 0.00001287
Iteration 99/1000 | Loss: 0.00001287
Iteration 100/1000 | Loss: 0.00001287
Iteration 101/1000 | Loss: 0.00001287
Iteration 102/1000 | Loss: 0.00001286
Iteration 103/1000 | Loss: 0.00001286
Iteration 104/1000 | Loss: 0.00001286
Iteration 105/1000 | Loss: 0.00001286
Iteration 106/1000 | Loss: 0.00001285
Iteration 107/1000 | Loss: 0.00001285
Iteration 108/1000 | Loss: 0.00001284
Iteration 109/1000 | Loss: 0.00001284
Iteration 110/1000 | Loss: 0.00001284
Iteration 111/1000 | Loss: 0.00001284
Iteration 112/1000 | Loss: 0.00001284
Iteration 113/1000 | Loss: 0.00001283
Iteration 114/1000 | Loss: 0.00001283
Iteration 115/1000 | Loss: 0.00001283
Iteration 116/1000 | Loss: 0.00001282
Iteration 117/1000 | Loss: 0.00001282
Iteration 118/1000 | Loss: 0.00001282
Iteration 119/1000 | Loss: 0.00001281
Iteration 120/1000 | Loss: 0.00001281
Iteration 121/1000 | Loss: 0.00001280
Iteration 122/1000 | Loss: 0.00001280
Iteration 123/1000 | Loss: 0.00001280
Iteration 124/1000 | Loss: 0.00001280
Iteration 125/1000 | Loss: 0.00001280
Iteration 126/1000 | Loss: 0.00001280
Iteration 127/1000 | Loss: 0.00001279
Iteration 128/1000 | Loss: 0.00001279
Iteration 129/1000 | Loss: 0.00001279
Iteration 130/1000 | Loss: 0.00001277
Iteration 131/1000 | Loss: 0.00001277
Iteration 132/1000 | Loss: 0.00001277
Iteration 133/1000 | Loss: 0.00001277
Iteration 134/1000 | Loss: 0.00001277
Iteration 135/1000 | Loss: 0.00001277
Iteration 136/1000 | Loss: 0.00001277
Iteration 137/1000 | Loss: 0.00001277
Iteration 138/1000 | Loss: 0.00001277
Iteration 139/1000 | Loss: 0.00001276
Iteration 140/1000 | Loss: 0.00001276
Iteration 141/1000 | Loss: 0.00001276
Iteration 142/1000 | Loss: 0.00001276
Iteration 143/1000 | Loss: 0.00001275
Iteration 144/1000 | Loss: 0.00001275
Iteration 145/1000 | Loss: 0.00001275
Iteration 146/1000 | Loss: 0.00001274
Iteration 147/1000 | Loss: 0.00001274
Iteration 148/1000 | Loss: 0.00001274
Iteration 149/1000 | Loss: 0.00001273
Iteration 150/1000 | Loss: 0.00001273
Iteration 151/1000 | Loss: 0.00001273
Iteration 152/1000 | Loss: 0.00001273
Iteration 153/1000 | Loss: 0.00001273
Iteration 154/1000 | Loss: 0.00001273
Iteration 155/1000 | Loss: 0.00001273
Iteration 156/1000 | Loss: 0.00001273
Iteration 157/1000 | Loss: 0.00001273
Iteration 158/1000 | Loss: 0.00001273
Iteration 159/1000 | Loss: 0.00001272
Iteration 160/1000 | Loss: 0.00001272
Iteration 161/1000 | Loss: 0.00001272
Iteration 162/1000 | Loss: 0.00001272
Iteration 163/1000 | Loss: 0.00001272
Iteration 164/1000 | Loss: 0.00001272
Iteration 165/1000 | Loss: 0.00001272
Iteration 166/1000 | Loss: 0.00001271
Iteration 167/1000 | Loss: 0.00001271
Iteration 168/1000 | Loss: 0.00001271
Iteration 169/1000 | Loss: 0.00001271
Iteration 170/1000 | Loss: 0.00001271
Iteration 171/1000 | Loss: 0.00001271
Iteration 172/1000 | Loss: 0.00001271
Iteration 173/1000 | Loss: 0.00001271
Iteration 174/1000 | Loss: 0.00001271
Iteration 175/1000 | Loss: 0.00001271
Iteration 176/1000 | Loss: 0.00001271
Iteration 177/1000 | Loss: 0.00001271
Iteration 178/1000 | Loss: 0.00001271
Iteration 179/1000 | Loss: 0.00001271
Iteration 180/1000 | Loss: 0.00001271
Iteration 181/1000 | Loss: 0.00001270
Iteration 182/1000 | Loss: 0.00001270
Iteration 183/1000 | Loss: 0.00001270
Iteration 184/1000 | Loss: 0.00001270
Iteration 185/1000 | Loss: 0.00001270
Iteration 186/1000 | Loss: 0.00001270
Iteration 187/1000 | Loss: 0.00001270
Iteration 188/1000 | Loss: 0.00001270
Iteration 189/1000 | Loss: 0.00001270
Iteration 190/1000 | Loss: 0.00001270
Iteration 191/1000 | Loss: 0.00001270
Iteration 192/1000 | Loss: 0.00001270
Iteration 193/1000 | Loss: 0.00001269
Iteration 194/1000 | Loss: 0.00001269
Iteration 195/1000 | Loss: 0.00001269
Iteration 196/1000 | Loss: 0.00001269
Iteration 197/1000 | Loss: 0.00001269
Iteration 198/1000 | Loss: 0.00001269
Iteration 199/1000 | Loss: 0.00001269
Iteration 200/1000 | Loss: 0.00001269
Iteration 201/1000 | Loss: 0.00001269
Iteration 202/1000 | Loss: 0.00001269
Iteration 203/1000 | Loss: 0.00001269
Iteration 204/1000 | Loss: 0.00001268
Iteration 205/1000 | Loss: 0.00001268
Iteration 206/1000 | Loss: 0.00001268
Iteration 207/1000 | Loss: 0.00001268
Iteration 208/1000 | Loss: 0.00001268
Iteration 209/1000 | Loss: 0.00001268
Iteration 210/1000 | Loss: 0.00001268
Iteration 211/1000 | Loss: 0.00001268
Iteration 212/1000 | Loss: 0.00001268
Iteration 213/1000 | Loss: 0.00001268
Iteration 214/1000 | Loss: 0.00001268
Iteration 215/1000 | Loss: 0.00001268
Iteration 216/1000 | Loss: 0.00001267
Iteration 217/1000 | Loss: 0.00001267
Iteration 218/1000 | Loss: 0.00001267
Iteration 219/1000 | Loss: 0.00001267
Iteration 220/1000 | Loss: 0.00001267
Iteration 221/1000 | Loss: 0.00001267
Iteration 222/1000 | Loss: 0.00001267
Iteration 223/1000 | Loss: 0.00001267
Iteration 224/1000 | Loss: 0.00001267
Iteration 225/1000 | Loss: 0.00001267
Iteration 226/1000 | Loss: 0.00001267
Iteration 227/1000 | Loss: 0.00001267
Iteration 228/1000 | Loss: 0.00001267
Iteration 229/1000 | Loss: 0.00001267
Iteration 230/1000 | Loss: 0.00001267
Iteration 231/1000 | Loss: 0.00001267
Iteration 232/1000 | Loss: 0.00001267
Iteration 233/1000 | Loss: 0.00001267
Iteration 234/1000 | Loss: 0.00001267
Iteration 235/1000 | Loss: 0.00001267
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 235. Stopping optimization.
Last 5 losses: [1.2671319382207002e-05, 1.2671319382207002e-05, 1.2671319382207002e-05, 1.2671319382207002e-05, 1.2671319382207002e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2671319382207002e-05

Optimization complete. Final v2v error: 2.9962871074676514 mm

Highest mean error: 3.6494979858398438 mm for frame 88

Lowest mean error: 2.638244152069092 mm for frame 124

Saving results

Total time: 58.94113802909851
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_003/1004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_003/1004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_003/1004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00908806
Iteration 2/25 | Loss: 0.00160512
Iteration 3/25 | Loss: 0.00134074
Iteration 4/25 | Loss: 0.00131940
Iteration 5/25 | Loss: 0.00131235
Iteration 6/25 | Loss: 0.00131067
Iteration 7/25 | Loss: 0.00131065
Iteration 8/25 | Loss: 0.00131065
Iteration 9/25 | Loss: 0.00131065
Iteration 10/25 | Loss: 0.00131065
Iteration 11/25 | Loss: 0.00131065
Iteration 12/25 | Loss: 0.00131065
Iteration 13/25 | Loss: 0.00131065
Iteration 14/25 | Loss: 0.00131065
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.001310645486228168, 0.001310645486228168, 0.001310645486228168, 0.001310645486228168, 0.001310645486228168]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001310645486228168

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.11570978
Iteration 2/25 | Loss: 0.00107590
Iteration 3/25 | Loss: 0.00107590
Iteration 4/25 | Loss: 0.00107590
Iteration 5/25 | Loss: 0.00107590
Iteration 6/25 | Loss: 0.00107590
Iteration 7/25 | Loss: 0.00107590
Iteration 8/25 | Loss: 0.00107589
Iteration 9/25 | Loss: 0.00107589
Iteration 10/25 | Loss: 0.00107589
Iteration 11/25 | Loss: 0.00107589
Iteration 12/25 | Loss: 0.00107589
Iteration 13/25 | Loss: 0.00107589
Iteration 14/25 | Loss: 0.00107589
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.001075894688256085, 0.001075894688256085, 0.001075894688256085, 0.001075894688256085, 0.001075894688256085]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001075894688256085

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00107589
Iteration 2/1000 | Loss: 0.00005971
Iteration 3/1000 | Loss: 0.00004039
Iteration 4/1000 | Loss: 0.00003219
Iteration 5/1000 | Loss: 0.00003043
Iteration 6/1000 | Loss: 0.00002896
Iteration 7/1000 | Loss: 0.00002829
Iteration 8/1000 | Loss: 0.00002778
Iteration 9/1000 | Loss: 0.00002709
Iteration 10/1000 | Loss: 0.00002661
Iteration 11/1000 | Loss: 0.00002627
Iteration 12/1000 | Loss: 0.00002591
Iteration 13/1000 | Loss: 0.00002571
Iteration 14/1000 | Loss: 0.00002549
Iteration 15/1000 | Loss: 0.00002529
Iteration 16/1000 | Loss: 0.00002513
Iteration 17/1000 | Loss: 0.00002495
Iteration 18/1000 | Loss: 0.00002481
Iteration 19/1000 | Loss: 0.00002480
Iteration 20/1000 | Loss: 0.00002478
Iteration 21/1000 | Loss: 0.00002474
Iteration 22/1000 | Loss: 0.00002471
Iteration 23/1000 | Loss: 0.00002470
Iteration 24/1000 | Loss: 0.00002470
Iteration 25/1000 | Loss: 0.00002469
Iteration 26/1000 | Loss: 0.00002468
Iteration 27/1000 | Loss: 0.00002466
Iteration 28/1000 | Loss: 0.00002466
Iteration 29/1000 | Loss: 0.00002464
Iteration 30/1000 | Loss: 0.00002464
Iteration 31/1000 | Loss: 0.00002463
Iteration 32/1000 | Loss: 0.00002457
Iteration 33/1000 | Loss: 0.00002450
Iteration 34/1000 | Loss: 0.00002444
Iteration 35/1000 | Loss: 0.00002443
Iteration 36/1000 | Loss: 0.00002442
Iteration 37/1000 | Loss: 0.00002442
Iteration 38/1000 | Loss: 0.00002441
Iteration 39/1000 | Loss: 0.00002441
Iteration 40/1000 | Loss: 0.00002440
Iteration 41/1000 | Loss: 0.00002440
Iteration 42/1000 | Loss: 0.00002440
Iteration 43/1000 | Loss: 0.00002439
Iteration 44/1000 | Loss: 0.00002439
Iteration 45/1000 | Loss: 0.00002439
Iteration 46/1000 | Loss: 0.00002438
Iteration 47/1000 | Loss: 0.00002438
Iteration 48/1000 | Loss: 0.00002438
Iteration 49/1000 | Loss: 0.00002438
Iteration 50/1000 | Loss: 0.00002437
Iteration 51/1000 | Loss: 0.00002437
Iteration 52/1000 | Loss: 0.00002436
Iteration 53/1000 | Loss: 0.00002436
Iteration 54/1000 | Loss: 0.00002436
Iteration 55/1000 | Loss: 0.00002436
Iteration 56/1000 | Loss: 0.00002436
Iteration 57/1000 | Loss: 0.00002435
Iteration 58/1000 | Loss: 0.00002435
Iteration 59/1000 | Loss: 0.00002435
Iteration 60/1000 | Loss: 0.00002435
Iteration 61/1000 | Loss: 0.00002435
Iteration 62/1000 | Loss: 0.00002435
Iteration 63/1000 | Loss: 0.00002435
Iteration 64/1000 | Loss: 0.00002435
Iteration 65/1000 | Loss: 0.00002435
Iteration 66/1000 | Loss: 0.00002434
Iteration 67/1000 | Loss: 0.00002434
Iteration 68/1000 | Loss: 0.00002434
Iteration 69/1000 | Loss: 0.00002434
Iteration 70/1000 | Loss: 0.00002433
Iteration 71/1000 | Loss: 0.00002433
Iteration 72/1000 | Loss: 0.00002433
Iteration 73/1000 | Loss: 0.00002433
Iteration 74/1000 | Loss: 0.00002433
Iteration 75/1000 | Loss: 0.00002433
Iteration 76/1000 | Loss: 0.00002433
Iteration 77/1000 | Loss: 0.00002433
Iteration 78/1000 | Loss: 0.00002433
Iteration 79/1000 | Loss: 0.00002433
Iteration 80/1000 | Loss: 0.00002432
Iteration 81/1000 | Loss: 0.00002432
Iteration 82/1000 | Loss: 0.00002432
Iteration 83/1000 | Loss: 0.00002432
Iteration 84/1000 | Loss: 0.00002432
Iteration 85/1000 | Loss: 0.00002431
Iteration 86/1000 | Loss: 0.00002431
Iteration 87/1000 | Loss: 0.00002431
Iteration 88/1000 | Loss: 0.00002431
Iteration 89/1000 | Loss: 0.00002431
Iteration 90/1000 | Loss: 0.00002431
Iteration 91/1000 | Loss: 0.00002430
Iteration 92/1000 | Loss: 0.00002430
Iteration 93/1000 | Loss: 0.00002430
Iteration 94/1000 | Loss: 0.00002430
Iteration 95/1000 | Loss: 0.00002430
Iteration 96/1000 | Loss: 0.00002430
Iteration 97/1000 | Loss: 0.00002430
Iteration 98/1000 | Loss: 0.00002430
Iteration 99/1000 | Loss: 0.00002430
Iteration 100/1000 | Loss: 0.00002430
Iteration 101/1000 | Loss: 0.00002429
Iteration 102/1000 | Loss: 0.00002429
Iteration 103/1000 | Loss: 0.00002429
Iteration 104/1000 | Loss: 0.00002429
Iteration 105/1000 | Loss: 0.00002429
Iteration 106/1000 | Loss: 0.00002429
Iteration 107/1000 | Loss: 0.00002429
Iteration 108/1000 | Loss: 0.00002428
Iteration 109/1000 | Loss: 0.00002428
Iteration 110/1000 | Loss: 0.00002428
Iteration 111/1000 | Loss: 0.00002428
Iteration 112/1000 | Loss: 0.00002428
Iteration 113/1000 | Loss: 0.00002427
Iteration 114/1000 | Loss: 0.00002427
Iteration 115/1000 | Loss: 0.00002427
Iteration 116/1000 | Loss: 0.00002427
Iteration 117/1000 | Loss: 0.00002427
Iteration 118/1000 | Loss: 0.00002427
Iteration 119/1000 | Loss: 0.00002426
Iteration 120/1000 | Loss: 0.00002426
Iteration 121/1000 | Loss: 0.00002426
Iteration 122/1000 | Loss: 0.00002426
Iteration 123/1000 | Loss: 0.00002426
Iteration 124/1000 | Loss: 0.00002426
Iteration 125/1000 | Loss: 0.00002426
Iteration 126/1000 | Loss: 0.00002426
Iteration 127/1000 | Loss: 0.00002426
Iteration 128/1000 | Loss: 0.00002426
Iteration 129/1000 | Loss: 0.00002426
Iteration 130/1000 | Loss: 0.00002426
Iteration 131/1000 | Loss: 0.00002426
Iteration 132/1000 | Loss: 0.00002426
Iteration 133/1000 | Loss: 0.00002426
Iteration 134/1000 | Loss: 0.00002425
Iteration 135/1000 | Loss: 0.00002425
Iteration 136/1000 | Loss: 0.00002425
Iteration 137/1000 | Loss: 0.00002425
Iteration 138/1000 | Loss: 0.00002425
Iteration 139/1000 | Loss: 0.00002425
Iteration 140/1000 | Loss: 0.00002425
Iteration 141/1000 | Loss: 0.00002425
Iteration 142/1000 | Loss: 0.00002425
Iteration 143/1000 | Loss: 0.00002425
Iteration 144/1000 | Loss: 0.00002425
Iteration 145/1000 | Loss: 0.00002425
Iteration 146/1000 | Loss: 0.00002425
Iteration 147/1000 | Loss: 0.00002425
Iteration 148/1000 | Loss: 0.00002425
Iteration 149/1000 | Loss: 0.00002425
Iteration 150/1000 | Loss: 0.00002425
Iteration 151/1000 | Loss: 0.00002425
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 151. Stopping optimization.
Last 5 losses: [2.425049024168402e-05, 2.425049024168402e-05, 2.425049024168402e-05, 2.425049024168402e-05, 2.425049024168402e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.425049024168402e-05

Optimization complete. Final v2v error: 4.016630172729492 mm

Highest mean error: 5.311130523681641 mm for frame 85

Lowest mean error: 3.171527624130249 mm for frame 121

Saving results

Total time: 46.596582889556885
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_003/1086/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_003/1086.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_003/1086
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00815591
Iteration 2/25 | Loss: 0.00149268
Iteration 3/25 | Loss: 0.00130100
Iteration 4/25 | Loss: 0.00126596
Iteration 5/25 | Loss: 0.00125107
Iteration 6/25 | Loss: 0.00124837
Iteration 7/25 | Loss: 0.00124680
Iteration 8/25 | Loss: 0.00124600
Iteration 9/25 | Loss: 0.00125037
Iteration 10/25 | Loss: 0.00124304
Iteration 11/25 | Loss: 0.00124115
Iteration 12/25 | Loss: 0.00124062
Iteration 13/25 | Loss: 0.00124055
Iteration 14/25 | Loss: 0.00124055
Iteration 15/25 | Loss: 0.00124055
Iteration 16/25 | Loss: 0.00124055
Iteration 17/25 | Loss: 0.00124055
Iteration 18/25 | Loss: 0.00124055
Iteration 19/25 | Loss: 0.00124055
Iteration 20/25 | Loss: 0.00124055
Iteration 21/25 | Loss: 0.00124055
Iteration 22/25 | Loss: 0.00124055
Iteration 23/25 | Loss: 0.00124055
Iteration 24/25 | Loss: 0.00124054
Iteration 25/25 | Loss: 0.00124054

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.89489365
Iteration 2/25 | Loss: 0.00134492
Iteration 3/25 | Loss: 0.00134492
Iteration 4/25 | Loss: 0.00134491
Iteration 5/25 | Loss: 0.00134491
Iteration 6/25 | Loss: 0.00134491
Iteration 7/25 | Loss: 0.00134491
Iteration 8/25 | Loss: 0.00134491
Iteration 9/25 | Loss: 0.00134491
Iteration 10/25 | Loss: 0.00134491
Iteration 11/25 | Loss: 0.00134491
Iteration 12/25 | Loss: 0.00134491
Iteration 13/25 | Loss: 0.00134491
Iteration 14/25 | Loss: 0.00134491
Iteration 15/25 | Loss: 0.00134491
Iteration 16/25 | Loss: 0.00134491
Iteration 17/25 | Loss: 0.00134491
Iteration 18/25 | Loss: 0.00134491
Iteration 19/25 | Loss: 0.00134491
Iteration 20/25 | Loss: 0.00134491
Iteration 21/25 | Loss: 0.00134491
Iteration 22/25 | Loss: 0.00134491
Iteration 23/25 | Loss: 0.00134491
Iteration 24/25 | Loss: 0.00134491
Iteration 25/25 | Loss: 0.00134491
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0013449095422402024, 0.0013449095422402024, 0.0013449095422402024, 0.0013449095422402024, 0.0013449095422402024]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013449095422402024

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00134491
Iteration 2/1000 | Loss: 0.00003293
Iteration 3/1000 | Loss: 0.00002121
Iteration 4/1000 | Loss: 0.00001823
Iteration 5/1000 | Loss: 0.00001718
Iteration 6/1000 | Loss: 0.00001626
Iteration 7/1000 | Loss: 0.00001590
Iteration 8/1000 | Loss: 0.00001553
Iteration 9/1000 | Loss: 0.00001529
Iteration 10/1000 | Loss: 0.00001529
Iteration 11/1000 | Loss: 0.00001511
Iteration 12/1000 | Loss: 0.00001494
Iteration 13/1000 | Loss: 0.00001485
Iteration 14/1000 | Loss: 0.00001475
Iteration 15/1000 | Loss: 0.00001474
Iteration 16/1000 | Loss: 0.00001469
Iteration 17/1000 | Loss: 0.00001465
Iteration 18/1000 | Loss: 0.00001462
Iteration 19/1000 | Loss: 0.00001460
Iteration 20/1000 | Loss: 0.00001459
Iteration 21/1000 | Loss: 0.00001458
Iteration 22/1000 | Loss: 0.00001456
Iteration 23/1000 | Loss: 0.00001455
Iteration 24/1000 | Loss: 0.00001453
Iteration 25/1000 | Loss: 0.00001452
Iteration 26/1000 | Loss: 0.00001451
Iteration 27/1000 | Loss: 0.00001451
Iteration 28/1000 | Loss: 0.00001450
Iteration 29/1000 | Loss: 0.00001448
Iteration 30/1000 | Loss: 0.00001448
Iteration 31/1000 | Loss: 0.00001447
Iteration 32/1000 | Loss: 0.00001445
Iteration 33/1000 | Loss: 0.00001445
Iteration 34/1000 | Loss: 0.00001444
Iteration 35/1000 | Loss: 0.00001443
Iteration 36/1000 | Loss: 0.00001442
Iteration 37/1000 | Loss: 0.00001441
Iteration 38/1000 | Loss: 0.00001441
Iteration 39/1000 | Loss: 0.00001440
Iteration 40/1000 | Loss: 0.00001440
Iteration 41/1000 | Loss: 0.00001439
Iteration 42/1000 | Loss: 0.00001438
Iteration 43/1000 | Loss: 0.00001438
Iteration 44/1000 | Loss: 0.00001438
Iteration 45/1000 | Loss: 0.00001437
Iteration 46/1000 | Loss: 0.00001437
Iteration 47/1000 | Loss: 0.00001436
Iteration 48/1000 | Loss: 0.00001436
Iteration 49/1000 | Loss: 0.00001435
Iteration 50/1000 | Loss: 0.00001435
Iteration 51/1000 | Loss: 0.00001435
Iteration 52/1000 | Loss: 0.00001435
Iteration 53/1000 | Loss: 0.00001435
Iteration 54/1000 | Loss: 0.00001435
Iteration 55/1000 | Loss: 0.00001435
Iteration 56/1000 | Loss: 0.00001435
Iteration 57/1000 | Loss: 0.00001434
Iteration 58/1000 | Loss: 0.00001434
Iteration 59/1000 | Loss: 0.00001434
Iteration 60/1000 | Loss: 0.00001434
Iteration 61/1000 | Loss: 0.00001434
Iteration 62/1000 | Loss: 0.00001434
Iteration 63/1000 | Loss: 0.00001434
Iteration 64/1000 | Loss: 0.00001434
Iteration 65/1000 | Loss: 0.00001434
Iteration 66/1000 | Loss: 0.00001434
Iteration 67/1000 | Loss: 0.00001433
Iteration 68/1000 | Loss: 0.00001433
Iteration 69/1000 | Loss: 0.00001432
Iteration 70/1000 | Loss: 0.00001432
Iteration 71/1000 | Loss: 0.00001432
Iteration 72/1000 | Loss: 0.00001431
Iteration 73/1000 | Loss: 0.00001431
Iteration 74/1000 | Loss: 0.00001431
Iteration 75/1000 | Loss: 0.00001431
Iteration 76/1000 | Loss: 0.00001430
Iteration 77/1000 | Loss: 0.00001430
Iteration 78/1000 | Loss: 0.00001430
Iteration 79/1000 | Loss: 0.00001430
Iteration 80/1000 | Loss: 0.00001430
Iteration 81/1000 | Loss: 0.00001430
Iteration 82/1000 | Loss: 0.00001430
Iteration 83/1000 | Loss: 0.00001430
Iteration 84/1000 | Loss: 0.00001429
Iteration 85/1000 | Loss: 0.00001429
Iteration 86/1000 | Loss: 0.00001429
Iteration 87/1000 | Loss: 0.00001429
Iteration 88/1000 | Loss: 0.00001428
Iteration 89/1000 | Loss: 0.00001428
Iteration 90/1000 | Loss: 0.00001428
Iteration 91/1000 | Loss: 0.00001428
Iteration 92/1000 | Loss: 0.00001427
Iteration 93/1000 | Loss: 0.00001427
Iteration 94/1000 | Loss: 0.00001427
Iteration 95/1000 | Loss: 0.00001427
Iteration 96/1000 | Loss: 0.00001426
Iteration 97/1000 | Loss: 0.00001426
Iteration 98/1000 | Loss: 0.00001426
Iteration 99/1000 | Loss: 0.00001425
Iteration 100/1000 | Loss: 0.00001425
Iteration 101/1000 | Loss: 0.00001425
Iteration 102/1000 | Loss: 0.00001425
Iteration 103/1000 | Loss: 0.00001424
Iteration 104/1000 | Loss: 0.00001424
Iteration 105/1000 | Loss: 0.00001424
Iteration 106/1000 | Loss: 0.00001424
Iteration 107/1000 | Loss: 0.00001423
Iteration 108/1000 | Loss: 0.00001423
Iteration 109/1000 | Loss: 0.00001423
Iteration 110/1000 | Loss: 0.00001423
Iteration 111/1000 | Loss: 0.00001423
Iteration 112/1000 | Loss: 0.00001423
Iteration 113/1000 | Loss: 0.00001423
Iteration 114/1000 | Loss: 0.00001423
Iteration 115/1000 | Loss: 0.00001422
Iteration 116/1000 | Loss: 0.00001422
Iteration 117/1000 | Loss: 0.00001422
Iteration 118/1000 | Loss: 0.00001422
Iteration 119/1000 | Loss: 0.00001422
Iteration 120/1000 | Loss: 0.00001422
Iteration 121/1000 | Loss: 0.00001422
Iteration 122/1000 | Loss: 0.00001422
Iteration 123/1000 | Loss: 0.00001422
Iteration 124/1000 | Loss: 0.00001422
Iteration 125/1000 | Loss: 0.00001421
Iteration 126/1000 | Loss: 0.00001421
Iteration 127/1000 | Loss: 0.00001421
Iteration 128/1000 | Loss: 0.00001421
Iteration 129/1000 | Loss: 0.00001421
Iteration 130/1000 | Loss: 0.00001420
Iteration 131/1000 | Loss: 0.00001420
Iteration 132/1000 | Loss: 0.00001420
Iteration 133/1000 | Loss: 0.00001420
Iteration 134/1000 | Loss: 0.00001420
Iteration 135/1000 | Loss: 0.00001420
Iteration 136/1000 | Loss: 0.00001420
Iteration 137/1000 | Loss: 0.00001419
Iteration 138/1000 | Loss: 0.00001419
Iteration 139/1000 | Loss: 0.00001419
Iteration 140/1000 | Loss: 0.00001419
Iteration 141/1000 | Loss: 0.00001419
Iteration 142/1000 | Loss: 0.00001419
Iteration 143/1000 | Loss: 0.00001419
Iteration 144/1000 | Loss: 0.00001419
Iteration 145/1000 | Loss: 0.00001419
Iteration 146/1000 | Loss: 0.00001419
Iteration 147/1000 | Loss: 0.00001419
Iteration 148/1000 | Loss: 0.00001419
Iteration 149/1000 | Loss: 0.00001419
Iteration 150/1000 | Loss: 0.00001419
Iteration 151/1000 | Loss: 0.00001419
Iteration 152/1000 | Loss: 0.00001418
Iteration 153/1000 | Loss: 0.00001418
Iteration 154/1000 | Loss: 0.00001418
Iteration 155/1000 | Loss: 0.00001418
Iteration 156/1000 | Loss: 0.00001418
Iteration 157/1000 | Loss: 0.00001418
Iteration 158/1000 | Loss: 0.00001418
Iteration 159/1000 | Loss: 0.00001418
Iteration 160/1000 | Loss: 0.00001418
Iteration 161/1000 | Loss: 0.00001418
Iteration 162/1000 | Loss: 0.00001418
Iteration 163/1000 | Loss: 0.00001418
Iteration 164/1000 | Loss: 0.00001418
Iteration 165/1000 | Loss: 0.00001417
Iteration 166/1000 | Loss: 0.00001417
Iteration 167/1000 | Loss: 0.00001417
Iteration 168/1000 | Loss: 0.00001417
Iteration 169/1000 | Loss: 0.00001417
Iteration 170/1000 | Loss: 0.00001417
Iteration 171/1000 | Loss: 0.00001417
Iteration 172/1000 | Loss: 0.00001417
Iteration 173/1000 | Loss: 0.00001417
Iteration 174/1000 | Loss: 0.00001416
Iteration 175/1000 | Loss: 0.00001416
Iteration 176/1000 | Loss: 0.00001416
Iteration 177/1000 | Loss: 0.00001416
Iteration 178/1000 | Loss: 0.00001416
Iteration 179/1000 | Loss: 0.00001416
Iteration 180/1000 | Loss: 0.00001416
Iteration 181/1000 | Loss: 0.00001416
Iteration 182/1000 | Loss: 0.00001416
Iteration 183/1000 | Loss: 0.00001416
Iteration 184/1000 | Loss: 0.00001416
Iteration 185/1000 | Loss: 0.00001416
Iteration 186/1000 | Loss: 0.00001416
Iteration 187/1000 | Loss: 0.00001416
Iteration 188/1000 | Loss: 0.00001416
Iteration 189/1000 | Loss: 0.00001416
Iteration 190/1000 | Loss: 0.00001416
Iteration 191/1000 | Loss: 0.00001416
Iteration 192/1000 | Loss: 0.00001415
Iteration 193/1000 | Loss: 0.00001415
Iteration 194/1000 | Loss: 0.00001415
Iteration 195/1000 | Loss: 0.00001415
Iteration 196/1000 | Loss: 0.00001415
Iteration 197/1000 | Loss: 0.00001415
Iteration 198/1000 | Loss: 0.00001415
Iteration 199/1000 | Loss: 0.00001415
Iteration 200/1000 | Loss: 0.00001415
Iteration 201/1000 | Loss: 0.00001415
Iteration 202/1000 | Loss: 0.00001415
Iteration 203/1000 | Loss: 0.00001414
Iteration 204/1000 | Loss: 0.00001414
Iteration 205/1000 | Loss: 0.00001414
Iteration 206/1000 | Loss: 0.00001414
Iteration 207/1000 | Loss: 0.00001414
Iteration 208/1000 | Loss: 0.00001414
Iteration 209/1000 | Loss: 0.00001414
Iteration 210/1000 | Loss: 0.00001414
Iteration 211/1000 | Loss: 0.00001414
Iteration 212/1000 | Loss: 0.00001414
Iteration 213/1000 | Loss: 0.00001413
Iteration 214/1000 | Loss: 0.00001413
Iteration 215/1000 | Loss: 0.00001413
Iteration 216/1000 | Loss: 0.00001413
Iteration 217/1000 | Loss: 0.00001413
Iteration 218/1000 | Loss: 0.00001413
Iteration 219/1000 | Loss: 0.00001413
Iteration 220/1000 | Loss: 0.00001413
Iteration 221/1000 | Loss: 0.00001413
Iteration 222/1000 | Loss: 0.00001413
Iteration 223/1000 | Loss: 0.00001413
Iteration 224/1000 | Loss: 0.00001413
Iteration 225/1000 | Loss: 0.00001413
Iteration 226/1000 | Loss: 0.00001412
Iteration 227/1000 | Loss: 0.00001412
Iteration 228/1000 | Loss: 0.00001412
Iteration 229/1000 | Loss: 0.00001412
Iteration 230/1000 | Loss: 0.00001412
Iteration 231/1000 | Loss: 0.00001412
Iteration 232/1000 | Loss: 0.00001412
Iteration 233/1000 | Loss: 0.00001412
Iteration 234/1000 | Loss: 0.00001412
Iteration 235/1000 | Loss: 0.00001412
Iteration 236/1000 | Loss: 0.00001412
Iteration 237/1000 | Loss: 0.00001412
Iteration 238/1000 | Loss: 0.00001412
Iteration 239/1000 | Loss: 0.00001412
Iteration 240/1000 | Loss: 0.00001412
Iteration 241/1000 | Loss: 0.00001412
Iteration 242/1000 | Loss: 0.00001412
Iteration 243/1000 | Loss: 0.00001412
Iteration 244/1000 | Loss: 0.00001412
Iteration 245/1000 | Loss: 0.00001412
Iteration 246/1000 | Loss: 0.00001412
Iteration 247/1000 | Loss: 0.00001411
Iteration 248/1000 | Loss: 0.00001411
Iteration 249/1000 | Loss: 0.00001411
Iteration 250/1000 | Loss: 0.00001411
Iteration 251/1000 | Loss: 0.00001411
Iteration 252/1000 | Loss: 0.00001411
Iteration 253/1000 | Loss: 0.00001411
Iteration 254/1000 | Loss: 0.00001411
Iteration 255/1000 | Loss: 0.00001411
Iteration 256/1000 | Loss: 0.00001411
Iteration 257/1000 | Loss: 0.00001411
Iteration 258/1000 | Loss: 0.00001411
Iteration 259/1000 | Loss: 0.00001411
Iteration 260/1000 | Loss: 0.00001411
Iteration 261/1000 | Loss: 0.00001411
Iteration 262/1000 | Loss: 0.00001411
Iteration 263/1000 | Loss: 0.00001411
Iteration 264/1000 | Loss: 0.00001411
Iteration 265/1000 | Loss: 0.00001411
Iteration 266/1000 | Loss: 0.00001411
Iteration 267/1000 | Loss: 0.00001411
Iteration 268/1000 | Loss: 0.00001411
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 268. Stopping optimization.
Last 5 losses: [1.4113801626081113e-05, 1.4113801626081113e-05, 1.4113801626081113e-05, 1.4113801626081113e-05, 1.4113801626081113e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4113801626081113e-05

Optimization complete. Final v2v error: 3.1965832710266113 mm

Highest mean error: 3.814974069595337 mm for frame 80

Lowest mean error: 2.676431894302368 mm for frame 126

Saving results

Total time: 56.43501114845276
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_003/1010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_003/1010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_003/1010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00752929
Iteration 2/25 | Loss: 0.00153096
Iteration 3/25 | Loss: 0.00127097
Iteration 4/25 | Loss: 0.00125292
Iteration 5/25 | Loss: 0.00125292
Iteration 6/25 | Loss: 0.00125292
Iteration 7/25 | Loss: 0.00125292
Iteration 8/25 | Loss: 0.00125292
Iteration 9/25 | Loss: 0.00125292
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 9. Stopping optimization.
Last 5 losses: [0.0012529201339930296, 0.0012529201339930296, 0.0012529201339930296, 0.0012529201339930296, 0.0012529201339930296]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012529201339930296

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.28453553
Iteration 2/25 | Loss: 0.00082996
Iteration 3/25 | Loss: 0.00082995
Iteration 4/25 | Loss: 0.00082995
Iteration 5/25 | Loss: 0.00082995
Iteration 6/25 | Loss: 0.00082995
Iteration 7/25 | Loss: 0.00082995
Iteration 8/25 | Loss: 0.00082994
Iteration 9/25 | Loss: 0.00082994
Iteration 10/25 | Loss: 0.00082994
Iteration 11/25 | Loss: 0.00082994
Iteration 12/25 | Loss: 0.00082994
Iteration 13/25 | Loss: 0.00082994
Iteration 14/25 | Loss: 0.00082994
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0008299446199089289, 0.0008299446199089289, 0.0008299446199089289, 0.0008299446199089289, 0.0008299446199089289]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008299446199089289

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00082994
Iteration 2/1000 | Loss: 0.00002535
Iteration 3/1000 | Loss: 0.00001772
Iteration 4/1000 | Loss: 0.00001578
Iteration 5/1000 | Loss: 0.00001492
Iteration 6/1000 | Loss: 0.00001445
Iteration 7/1000 | Loss: 0.00001404
Iteration 8/1000 | Loss: 0.00001362
Iteration 9/1000 | Loss: 0.00001324
Iteration 10/1000 | Loss: 0.00001317
Iteration 11/1000 | Loss: 0.00001295
Iteration 12/1000 | Loss: 0.00001295
Iteration 13/1000 | Loss: 0.00001278
Iteration 14/1000 | Loss: 0.00001277
Iteration 15/1000 | Loss: 0.00001275
Iteration 16/1000 | Loss: 0.00001274
Iteration 17/1000 | Loss: 0.00001273
Iteration 18/1000 | Loss: 0.00001273
Iteration 19/1000 | Loss: 0.00001271
Iteration 20/1000 | Loss: 0.00001265
Iteration 21/1000 | Loss: 0.00001257
Iteration 22/1000 | Loss: 0.00001256
Iteration 23/1000 | Loss: 0.00001247
Iteration 24/1000 | Loss: 0.00001245
Iteration 25/1000 | Loss: 0.00001244
Iteration 26/1000 | Loss: 0.00001244
Iteration 27/1000 | Loss: 0.00001244
Iteration 28/1000 | Loss: 0.00001244
Iteration 29/1000 | Loss: 0.00001243
Iteration 30/1000 | Loss: 0.00001243
Iteration 31/1000 | Loss: 0.00001241
Iteration 32/1000 | Loss: 0.00001241
Iteration 33/1000 | Loss: 0.00001241
Iteration 34/1000 | Loss: 0.00001240
Iteration 35/1000 | Loss: 0.00001239
Iteration 36/1000 | Loss: 0.00001237
Iteration 37/1000 | Loss: 0.00001236
Iteration 38/1000 | Loss: 0.00001235
Iteration 39/1000 | Loss: 0.00001228
Iteration 40/1000 | Loss: 0.00001226
Iteration 41/1000 | Loss: 0.00001221
Iteration 42/1000 | Loss: 0.00001221
Iteration 43/1000 | Loss: 0.00001220
Iteration 44/1000 | Loss: 0.00001220
Iteration 45/1000 | Loss: 0.00001219
Iteration 46/1000 | Loss: 0.00001219
Iteration 47/1000 | Loss: 0.00001219
Iteration 48/1000 | Loss: 0.00001219
Iteration 49/1000 | Loss: 0.00001219
Iteration 50/1000 | Loss: 0.00001219
Iteration 51/1000 | Loss: 0.00001218
Iteration 52/1000 | Loss: 0.00001218
Iteration 53/1000 | Loss: 0.00001217
Iteration 54/1000 | Loss: 0.00001217
Iteration 55/1000 | Loss: 0.00001216
Iteration 56/1000 | Loss: 0.00001215
Iteration 57/1000 | Loss: 0.00001215
Iteration 58/1000 | Loss: 0.00001214
Iteration 59/1000 | Loss: 0.00001213
Iteration 60/1000 | Loss: 0.00001212
Iteration 61/1000 | Loss: 0.00001211
Iteration 62/1000 | Loss: 0.00001211
Iteration 63/1000 | Loss: 0.00001211
Iteration 64/1000 | Loss: 0.00001211
Iteration 65/1000 | Loss: 0.00001210
Iteration 66/1000 | Loss: 0.00001210
Iteration 67/1000 | Loss: 0.00001210
Iteration 68/1000 | Loss: 0.00001210
Iteration 69/1000 | Loss: 0.00001209
Iteration 70/1000 | Loss: 0.00001209
Iteration 71/1000 | Loss: 0.00001208
Iteration 72/1000 | Loss: 0.00001208
Iteration 73/1000 | Loss: 0.00001208
Iteration 74/1000 | Loss: 0.00001207
Iteration 75/1000 | Loss: 0.00001207
Iteration 76/1000 | Loss: 0.00001207
Iteration 77/1000 | Loss: 0.00001206
Iteration 78/1000 | Loss: 0.00001206
Iteration 79/1000 | Loss: 0.00001206
Iteration 80/1000 | Loss: 0.00001206
Iteration 81/1000 | Loss: 0.00001205
Iteration 82/1000 | Loss: 0.00001205
Iteration 83/1000 | Loss: 0.00001205
Iteration 84/1000 | Loss: 0.00001204
Iteration 85/1000 | Loss: 0.00001204
Iteration 86/1000 | Loss: 0.00001204
Iteration 87/1000 | Loss: 0.00001204
Iteration 88/1000 | Loss: 0.00001204
Iteration 89/1000 | Loss: 0.00001204
Iteration 90/1000 | Loss: 0.00001204
Iteration 91/1000 | Loss: 0.00001204
Iteration 92/1000 | Loss: 0.00001204
Iteration 93/1000 | Loss: 0.00001203
Iteration 94/1000 | Loss: 0.00001203
Iteration 95/1000 | Loss: 0.00001203
Iteration 96/1000 | Loss: 0.00001203
Iteration 97/1000 | Loss: 0.00001202
Iteration 98/1000 | Loss: 0.00001202
Iteration 99/1000 | Loss: 0.00001202
Iteration 100/1000 | Loss: 0.00001202
Iteration 101/1000 | Loss: 0.00001202
Iteration 102/1000 | Loss: 0.00001202
Iteration 103/1000 | Loss: 0.00001202
Iteration 104/1000 | Loss: 0.00001202
Iteration 105/1000 | Loss: 0.00001202
Iteration 106/1000 | Loss: 0.00001202
Iteration 107/1000 | Loss: 0.00001202
Iteration 108/1000 | Loss: 0.00001202
Iteration 109/1000 | Loss: 0.00001201
Iteration 110/1000 | Loss: 0.00001201
Iteration 111/1000 | Loss: 0.00001201
Iteration 112/1000 | Loss: 0.00001201
Iteration 113/1000 | Loss: 0.00001200
Iteration 114/1000 | Loss: 0.00001200
Iteration 115/1000 | Loss: 0.00001200
Iteration 116/1000 | Loss: 0.00001199
Iteration 117/1000 | Loss: 0.00001199
Iteration 118/1000 | Loss: 0.00001199
Iteration 119/1000 | Loss: 0.00001199
Iteration 120/1000 | Loss: 0.00001199
Iteration 121/1000 | Loss: 0.00001199
Iteration 122/1000 | Loss: 0.00001199
Iteration 123/1000 | Loss: 0.00001199
Iteration 124/1000 | Loss: 0.00001198
Iteration 125/1000 | Loss: 0.00001198
Iteration 126/1000 | Loss: 0.00001198
Iteration 127/1000 | Loss: 0.00001198
Iteration 128/1000 | Loss: 0.00001198
Iteration 129/1000 | Loss: 0.00001198
Iteration 130/1000 | Loss: 0.00001198
Iteration 131/1000 | Loss: 0.00001198
Iteration 132/1000 | Loss: 0.00001198
Iteration 133/1000 | Loss: 0.00001198
Iteration 134/1000 | Loss: 0.00001198
Iteration 135/1000 | Loss: 0.00001198
Iteration 136/1000 | Loss: 0.00001198
Iteration 137/1000 | Loss: 0.00001198
Iteration 138/1000 | Loss: 0.00001198
Iteration 139/1000 | Loss: 0.00001198
Iteration 140/1000 | Loss: 0.00001198
Iteration 141/1000 | Loss: 0.00001198
Iteration 142/1000 | Loss: 0.00001198
Iteration 143/1000 | Loss: 0.00001198
Iteration 144/1000 | Loss: 0.00001198
Iteration 145/1000 | Loss: 0.00001198
Iteration 146/1000 | Loss: 0.00001198
Iteration 147/1000 | Loss: 0.00001198
Iteration 148/1000 | Loss: 0.00001198
Iteration 149/1000 | Loss: 0.00001198
Iteration 150/1000 | Loss: 0.00001198
Iteration 151/1000 | Loss: 0.00001198
Iteration 152/1000 | Loss: 0.00001198
Iteration 153/1000 | Loss: 0.00001198
Iteration 154/1000 | Loss: 0.00001198
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 154. Stopping optimization.
Last 5 losses: [1.1975716915912926e-05, 1.1975716915912926e-05, 1.1975716915912926e-05, 1.1975716915912926e-05, 1.1975716915912926e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1975716915912926e-05

Optimization complete. Final v2v error: 2.9326703548431396 mm

Highest mean error: 3.1324000358581543 mm for frame 144

Lowest mean error: 2.7883105278015137 mm for frame 181

Saving results

Total time: 41.14289474487305
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_003/1007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_003/1007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_003/1007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00373122
Iteration 2/25 | Loss: 0.00126454
Iteration 3/25 | Loss: 0.00118831
Iteration 4/25 | Loss: 0.00117888
Iteration 5/25 | Loss: 0.00117603
Iteration 6/25 | Loss: 0.00117519
Iteration 7/25 | Loss: 0.00117517
Iteration 8/25 | Loss: 0.00117517
Iteration 9/25 | Loss: 0.00117517
Iteration 10/25 | Loss: 0.00117517
Iteration 11/25 | Loss: 0.00117517
Iteration 12/25 | Loss: 0.00117517
Iteration 13/25 | Loss: 0.00117517
Iteration 14/25 | Loss: 0.00117517
Iteration 15/25 | Loss: 0.00117517
Iteration 16/25 | Loss: 0.00117517
Iteration 17/25 | Loss: 0.00117517
Iteration 18/25 | Loss: 0.00117517
Iteration 19/25 | Loss: 0.00117517
Iteration 20/25 | Loss: 0.00117517
Iteration 21/25 | Loss: 0.00117517
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0011751728598028421, 0.0011751728598028421, 0.0011751728598028421, 0.0011751728598028421, 0.0011751728598028421]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011751728598028421

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.72328138
Iteration 2/25 | Loss: 0.00127743
Iteration 3/25 | Loss: 0.00127742
Iteration 4/25 | Loss: 0.00127742
Iteration 5/25 | Loss: 0.00127742
Iteration 6/25 | Loss: 0.00127742
Iteration 7/25 | Loss: 0.00127742
Iteration 8/25 | Loss: 0.00127742
Iteration 9/25 | Loss: 0.00127742
Iteration 10/25 | Loss: 0.00127742
Iteration 11/25 | Loss: 0.00127742
Iteration 12/25 | Loss: 0.00127742
Iteration 13/25 | Loss: 0.00127742
Iteration 14/25 | Loss: 0.00127742
Iteration 15/25 | Loss: 0.00127742
Iteration 16/25 | Loss: 0.00127742
Iteration 17/25 | Loss: 0.00127742
Iteration 18/25 | Loss: 0.00127742
Iteration 19/25 | Loss: 0.00127742
Iteration 20/25 | Loss: 0.00127742
Iteration 21/25 | Loss: 0.00127742
Iteration 22/25 | Loss: 0.00127742
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0012774199713021517, 0.0012774199713021517, 0.0012774199713021517, 0.0012774199713021517, 0.0012774199713021517]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012774199713021517

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00127742
Iteration 2/1000 | Loss: 0.00002558
Iteration 3/1000 | Loss: 0.00001616
Iteration 4/1000 | Loss: 0.00001305
Iteration 5/1000 | Loss: 0.00001190
Iteration 6/1000 | Loss: 0.00001095
Iteration 7/1000 | Loss: 0.00001046
Iteration 8/1000 | Loss: 0.00000996
Iteration 9/1000 | Loss: 0.00000974
Iteration 10/1000 | Loss: 0.00000955
Iteration 11/1000 | Loss: 0.00000936
Iteration 12/1000 | Loss: 0.00000935
Iteration 13/1000 | Loss: 0.00000933
Iteration 14/1000 | Loss: 0.00000932
Iteration 15/1000 | Loss: 0.00000930
Iteration 16/1000 | Loss: 0.00000930
Iteration 17/1000 | Loss: 0.00000928
Iteration 18/1000 | Loss: 0.00000928
Iteration 19/1000 | Loss: 0.00000928
Iteration 20/1000 | Loss: 0.00000927
Iteration 21/1000 | Loss: 0.00000926
Iteration 22/1000 | Loss: 0.00000923
Iteration 23/1000 | Loss: 0.00000923
Iteration 24/1000 | Loss: 0.00000921
Iteration 25/1000 | Loss: 0.00000921
Iteration 26/1000 | Loss: 0.00000920
Iteration 27/1000 | Loss: 0.00000919
Iteration 28/1000 | Loss: 0.00000916
Iteration 29/1000 | Loss: 0.00000911
Iteration 30/1000 | Loss: 0.00000910
Iteration 31/1000 | Loss: 0.00000908
Iteration 32/1000 | Loss: 0.00000908
Iteration 33/1000 | Loss: 0.00000907
Iteration 34/1000 | Loss: 0.00000907
Iteration 35/1000 | Loss: 0.00000906
Iteration 36/1000 | Loss: 0.00000903
Iteration 37/1000 | Loss: 0.00000902
Iteration 38/1000 | Loss: 0.00000902
Iteration 39/1000 | Loss: 0.00000902
Iteration 40/1000 | Loss: 0.00000902
Iteration 41/1000 | Loss: 0.00000902
Iteration 42/1000 | Loss: 0.00000902
Iteration 43/1000 | Loss: 0.00000902
Iteration 44/1000 | Loss: 0.00000902
Iteration 45/1000 | Loss: 0.00000902
Iteration 46/1000 | Loss: 0.00000901
Iteration 47/1000 | Loss: 0.00000901
Iteration 48/1000 | Loss: 0.00000901
Iteration 49/1000 | Loss: 0.00000900
Iteration 50/1000 | Loss: 0.00000899
Iteration 51/1000 | Loss: 0.00000899
Iteration 52/1000 | Loss: 0.00000898
Iteration 53/1000 | Loss: 0.00000898
Iteration 54/1000 | Loss: 0.00000897
Iteration 55/1000 | Loss: 0.00000897
Iteration 56/1000 | Loss: 0.00000897
Iteration 57/1000 | Loss: 0.00000895
Iteration 58/1000 | Loss: 0.00000893
Iteration 59/1000 | Loss: 0.00000893
Iteration 60/1000 | Loss: 0.00000893
Iteration 61/1000 | Loss: 0.00000893
Iteration 62/1000 | Loss: 0.00000893
Iteration 63/1000 | Loss: 0.00000893
Iteration 64/1000 | Loss: 0.00000892
Iteration 65/1000 | Loss: 0.00000892
Iteration 66/1000 | Loss: 0.00000892
Iteration 67/1000 | Loss: 0.00000892
Iteration 68/1000 | Loss: 0.00000891
Iteration 69/1000 | Loss: 0.00000891
Iteration 70/1000 | Loss: 0.00000891
Iteration 71/1000 | Loss: 0.00000890
Iteration 72/1000 | Loss: 0.00000890
Iteration 73/1000 | Loss: 0.00000890
Iteration 74/1000 | Loss: 0.00000889
Iteration 75/1000 | Loss: 0.00000889
Iteration 76/1000 | Loss: 0.00000888
Iteration 77/1000 | Loss: 0.00000888
Iteration 78/1000 | Loss: 0.00000888
Iteration 79/1000 | Loss: 0.00000887
Iteration 80/1000 | Loss: 0.00000887
Iteration 81/1000 | Loss: 0.00000887
Iteration 82/1000 | Loss: 0.00000886
Iteration 83/1000 | Loss: 0.00000886
Iteration 84/1000 | Loss: 0.00000886
Iteration 85/1000 | Loss: 0.00000886
Iteration 86/1000 | Loss: 0.00000885
Iteration 87/1000 | Loss: 0.00000885
Iteration 88/1000 | Loss: 0.00000884
Iteration 89/1000 | Loss: 0.00000884
Iteration 90/1000 | Loss: 0.00000884
Iteration 91/1000 | Loss: 0.00000883
Iteration 92/1000 | Loss: 0.00000883
Iteration 93/1000 | Loss: 0.00000883
Iteration 94/1000 | Loss: 0.00000882
Iteration 95/1000 | Loss: 0.00000882
Iteration 96/1000 | Loss: 0.00000882
Iteration 97/1000 | Loss: 0.00000882
Iteration 98/1000 | Loss: 0.00000882
Iteration 99/1000 | Loss: 0.00000881
Iteration 100/1000 | Loss: 0.00000881
Iteration 101/1000 | Loss: 0.00000881
Iteration 102/1000 | Loss: 0.00000880
Iteration 103/1000 | Loss: 0.00000880
Iteration 104/1000 | Loss: 0.00000880
Iteration 105/1000 | Loss: 0.00000880
Iteration 106/1000 | Loss: 0.00000880
Iteration 107/1000 | Loss: 0.00000879
Iteration 108/1000 | Loss: 0.00000879
Iteration 109/1000 | Loss: 0.00000879
Iteration 110/1000 | Loss: 0.00000879
Iteration 111/1000 | Loss: 0.00000879
Iteration 112/1000 | Loss: 0.00000879
Iteration 113/1000 | Loss: 0.00000878
Iteration 114/1000 | Loss: 0.00000878
Iteration 115/1000 | Loss: 0.00000878
Iteration 116/1000 | Loss: 0.00000878
Iteration 117/1000 | Loss: 0.00000878
Iteration 118/1000 | Loss: 0.00000878
Iteration 119/1000 | Loss: 0.00000878
Iteration 120/1000 | Loss: 0.00000878
Iteration 121/1000 | Loss: 0.00000877
Iteration 122/1000 | Loss: 0.00000877
Iteration 123/1000 | Loss: 0.00000877
Iteration 124/1000 | Loss: 0.00000876
Iteration 125/1000 | Loss: 0.00000876
Iteration 126/1000 | Loss: 0.00000875
Iteration 127/1000 | Loss: 0.00000875
Iteration 128/1000 | Loss: 0.00000875
Iteration 129/1000 | Loss: 0.00000875
Iteration 130/1000 | Loss: 0.00000875
Iteration 131/1000 | Loss: 0.00000875
Iteration 132/1000 | Loss: 0.00000874
Iteration 133/1000 | Loss: 0.00000874
Iteration 134/1000 | Loss: 0.00000874
Iteration 135/1000 | Loss: 0.00000874
Iteration 136/1000 | Loss: 0.00000874
Iteration 137/1000 | Loss: 0.00000874
Iteration 138/1000 | Loss: 0.00000873
Iteration 139/1000 | Loss: 0.00000873
Iteration 140/1000 | Loss: 0.00000873
Iteration 141/1000 | Loss: 0.00000873
Iteration 142/1000 | Loss: 0.00000873
Iteration 143/1000 | Loss: 0.00000873
Iteration 144/1000 | Loss: 0.00000873
Iteration 145/1000 | Loss: 0.00000872
Iteration 146/1000 | Loss: 0.00000872
Iteration 147/1000 | Loss: 0.00000872
Iteration 148/1000 | Loss: 0.00000872
Iteration 149/1000 | Loss: 0.00000872
Iteration 150/1000 | Loss: 0.00000872
Iteration 151/1000 | Loss: 0.00000872
Iteration 152/1000 | Loss: 0.00000872
Iteration 153/1000 | Loss: 0.00000872
Iteration 154/1000 | Loss: 0.00000872
Iteration 155/1000 | Loss: 0.00000872
Iteration 156/1000 | Loss: 0.00000872
Iteration 157/1000 | Loss: 0.00000871
Iteration 158/1000 | Loss: 0.00000871
Iteration 159/1000 | Loss: 0.00000871
Iteration 160/1000 | Loss: 0.00000871
Iteration 161/1000 | Loss: 0.00000871
Iteration 162/1000 | Loss: 0.00000871
Iteration 163/1000 | Loss: 0.00000870
Iteration 164/1000 | Loss: 0.00000870
Iteration 165/1000 | Loss: 0.00000870
Iteration 166/1000 | Loss: 0.00000870
Iteration 167/1000 | Loss: 0.00000870
Iteration 168/1000 | Loss: 0.00000870
Iteration 169/1000 | Loss: 0.00000870
Iteration 170/1000 | Loss: 0.00000870
Iteration 171/1000 | Loss: 0.00000870
Iteration 172/1000 | Loss: 0.00000870
Iteration 173/1000 | Loss: 0.00000870
Iteration 174/1000 | Loss: 0.00000869
Iteration 175/1000 | Loss: 0.00000869
Iteration 176/1000 | Loss: 0.00000869
Iteration 177/1000 | Loss: 0.00000869
Iteration 178/1000 | Loss: 0.00000869
Iteration 179/1000 | Loss: 0.00000869
Iteration 180/1000 | Loss: 0.00000869
Iteration 181/1000 | Loss: 0.00000869
Iteration 182/1000 | Loss: 0.00000869
Iteration 183/1000 | Loss: 0.00000869
Iteration 184/1000 | Loss: 0.00000869
Iteration 185/1000 | Loss: 0.00000869
Iteration 186/1000 | Loss: 0.00000869
Iteration 187/1000 | Loss: 0.00000869
Iteration 188/1000 | Loss: 0.00000869
Iteration 189/1000 | Loss: 0.00000869
Iteration 190/1000 | Loss: 0.00000869
Iteration 191/1000 | Loss: 0.00000868
Iteration 192/1000 | Loss: 0.00000868
Iteration 193/1000 | Loss: 0.00000868
Iteration 194/1000 | Loss: 0.00000868
Iteration 195/1000 | Loss: 0.00000868
Iteration 196/1000 | Loss: 0.00000868
Iteration 197/1000 | Loss: 0.00000868
Iteration 198/1000 | Loss: 0.00000868
Iteration 199/1000 | Loss: 0.00000868
Iteration 200/1000 | Loss: 0.00000868
Iteration 201/1000 | Loss: 0.00000867
Iteration 202/1000 | Loss: 0.00000867
Iteration 203/1000 | Loss: 0.00000867
Iteration 204/1000 | Loss: 0.00000867
Iteration 205/1000 | Loss: 0.00000867
Iteration 206/1000 | Loss: 0.00000867
Iteration 207/1000 | Loss: 0.00000867
Iteration 208/1000 | Loss: 0.00000867
Iteration 209/1000 | Loss: 0.00000867
Iteration 210/1000 | Loss: 0.00000867
Iteration 211/1000 | Loss: 0.00000867
Iteration 212/1000 | Loss: 0.00000867
Iteration 213/1000 | Loss: 0.00000867
Iteration 214/1000 | Loss: 0.00000866
Iteration 215/1000 | Loss: 0.00000866
Iteration 216/1000 | Loss: 0.00000866
Iteration 217/1000 | Loss: 0.00000866
Iteration 218/1000 | Loss: 0.00000866
Iteration 219/1000 | Loss: 0.00000866
Iteration 220/1000 | Loss: 0.00000866
Iteration 221/1000 | Loss: 0.00000866
Iteration 222/1000 | Loss: 0.00000866
Iteration 223/1000 | Loss: 0.00000866
Iteration 224/1000 | Loss: 0.00000866
Iteration 225/1000 | Loss: 0.00000866
Iteration 226/1000 | Loss: 0.00000866
Iteration 227/1000 | Loss: 0.00000866
Iteration 228/1000 | Loss: 0.00000866
Iteration 229/1000 | Loss: 0.00000866
Iteration 230/1000 | Loss: 0.00000866
Iteration 231/1000 | Loss: 0.00000866
Iteration 232/1000 | Loss: 0.00000866
Iteration 233/1000 | Loss: 0.00000866
Iteration 234/1000 | Loss: 0.00000865
Iteration 235/1000 | Loss: 0.00000865
Iteration 236/1000 | Loss: 0.00000865
Iteration 237/1000 | Loss: 0.00000865
Iteration 238/1000 | Loss: 0.00000865
Iteration 239/1000 | Loss: 0.00000865
Iteration 240/1000 | Loss: 0.00000865
Iteration 241/1000 | Loss: 0.00000865
Iteration 242/1000 | Loss: 0.00000865
Iteration 243/1000 | Loss: 0.00000865
Iteration 244/1000 | Loss: 0.00000865
Iteration 245/1000 | Loss: 0.00000865
Iteration 246/1000 | Loss: 0.00000865
Iteration 247/1000 | Loss: 0.00000865
Iteration 248/1000 | Loss: 0.00000865
Iteration 249/1000 | Loss: 0.00000865
Iteration 250/1000 | Loss: 0.00000865
Iteration 251/1000 | Loss: 0.00000865
Iteration 252/1000 | Loss: 0.00000865
Iteration 253/1000 | Loss: 0.00000865
Iteration 254/1000 | Loss: 0.00000865
Iteration 255/1000 | Loss: 0.00000865
Iteration 256/1000 | Loss: 0.00000865
Iteration 257/1000 | Loss: 0.00000865
Iteration 258/1000 | Loss: 0.00000865
Iteration 259/1000 | Loss: 0.00000865
Iteration 260/1000 | Loss: 0.00000865
Iteration 261/1000 | Loss: 0.00000865
Iteration 262/1000 | Loss: 0.00000865
Iteration 263/1000 | Loss: 0.00000865
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 263. Stopping optimization.
Last 5 losses: [8.648155926493928e-06, 8.648155926493928e-06, 8.648155926493928e-06, 8.648155926493928e-06, 8.648155926493928e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.648155926493928e-06

Optimization complete. Final v2v error: 2.531575918197632 mm

Highest mean error: 2.996432304382324 mm for frame 77

Lowest mean error: 2.429226875305176 mm for frame 115

Saving results

Total time: 45.375099658966064
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_003/1089/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_003/1089.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_003/1089
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00855522
Iteration 2/25 | Loss: 0.00235320
Iteration 3/25 | Loss: 0.00181548
Iteration 4/25 | Loss: 0.00157072
Iteration 5/25 | Loss: 0.00151158
Iteration 6/25 | Loss: 0.00152754
Iteration 7/25 | Loss: 0.00151479
Iteration 8/25 | Loss: 0.00147449
Iteration 9/25 | Loss: 0.00145950
Iteration 10/25 | Loss: 0.00145818
Iteration 11/25 | Loss: 0.00144657
Iteration 12/25 | Loss: 0.00144640
Iteration 13/25 | Loss: 0.00145019
Iteration 14/25 | Loss: 0.00144756
Iteration 15/25 | Loss: 0.00144353
Iteration 16/25 | Loss: 0.00144253
Iteration 17/25 | Loss: 0.00144476
Iteration 18/25 | Loss: 0.00144425
Iteration 19/25 | Loss: 0.00144384
Iteration 20/25 | Loss: 0.00144273
Iteration 21/25 | Loss: 0.00144494
Iteration 22/25 | Loss: 0.00144344
Iteration 23/25 | Loss: 0.00144732
Iteration 24/25 | Loss: 0.00144541
Iteration 25/25 | Loss: 0.00144580

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.09844303
Iteration 2/25 | Loss: 0.00105902
Iteration 3/25 | Loss: 0.00105902
Iteration 4/25 | Loss: 0.00105901
Iteration 5/25 | Loss: 0.00105901
Iteration 6/25 | Loss: 0.00105901
Iteration 7/25 | Loss: 0.00105901
Iteration 8/25 | Loss: 0.00105901
Iteration 9/25 | Loss: 0.00105901
Iteration 10/25 | Loss: 0.00105901
Iteration 11/25 | Loss: 0.00105901
Iteration 12/25 | Loss: 0.00105901
Iteration 13/25 | Loss: 0.00105901
Iteration 14/25 | Loss: 0.00105901
Iteration 15/25 | Loss: 0.00105901
Iteration 16/25 | Loss: 0.00105901
Iteration 17/25 | Loss: 0.00105901
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0010590122547000647, 0.0010590122547000647, 0.0010590122547000647, 0.0010590122547000647, 0.0010590122547000647]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010590122547000647

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00105901
Iteration 2/1000 | Loss: 0.00013370
Iteration 3/1000 | Loss: 0.00014357
Iteration 4/1000 | Loss: 0.00028609
Iteration 5/1000 | Loss: 0.00018536
Iteration 6/1000 | Loss: 0.00012974
Iteration 7/1000 | Loss: 0.00012888
Iteration 8/1000 | Loss: 0.00016614
Iteration 9/1000 | Loss: 0.00022614
Iteration 10/1000 | Loss: 0.00023652
Iteration 11/1000 | Loss: 0.00022500
Iteration 12/1000 | Loss: 0.00024244
Iteration 13/1000 | Loss: 0.00019002
Iteration 14/1000 | Loss: 0.00026739
Iteration 15/1000 | Loss: 0.00022050
Iteration 16/1000 | Loss: 0.00020718
Iteration 17/1000 | Loss: 0.00023431
Iteration 18/1000 | Loss: 0.00015591
Iteration 19/1000 | Loss: 0.00017193
Iteration 20/1000 | Loss: 0.00019752
Iteration 21/1000 | Loss: 0.00017068
Iteration 22/1000 | Loss: 0.00022885
Iteration 23/1000 | Loss: 0.00017757
Iteration 24/1000 | Loss: 0.00018786
Iteration 25/1000 | Loss: 0.00024471
Iteration 26/1000 | Loss: 0.00019118
Iteration 27/1000 | Loss: 0.00028495
Iteration 28/1000 | Loss: 0.00033732
Iteration 29/1000 | Loss: 0.00024150
Iteration 30/1000 | Loss: 0.00023139
Iteration 31/1000 | Loss: 0.00020995
Iteration 32/1000 | Loss: 0.00019757
Iteration 33/1000 | Loss: 0.00019856
Iteration 34/1000 | Loss: 0.00018598
Iteration 35/1000 | Loss: 0.00019170
Iteration 36/1000 | Loss: 0.00024430
Iteration 37/1000 | Loss: 0.00024901
Iteration 38/1000 | Loss: 0.00024209
Iteration 39/1000 | Loss: 0.00024909
Iteration 40/1000 | Loss: 0.00024389
Iteration 41/1000 | Loss: 0.00025034
Iteration 42/1000 | Loss: 0.00012695
Iteration 43/1000 | Loss: 0.00030870
Iteration 44/1000 | Loss: 0.00026921
Iteration 45/1000 | Loss: 0.00022977
Iteration 46/1000 | Loss: 0.00027026
Iteration 47/1000 | Loss: 0.00030946
Iteration 48/1000 | Loss: 0.00018285
Iteration 49/1000 | Loss: 0.00017156
Iteration 50/1000 | Loss: 0.00020235
Iteration 51/1000 | Loss: 0.00024270
Iteration 52/1000 | Loss: 0.00026219
Iteration 53/1000 | Loss: 0.00022803
Iteration 54/1000 | Loss: 0.00026378
Iteration 55/1000 | Loss: 0.00019537
Iteration 56/1000 | Loss: 0.00015563
Iteration 57/1000 | Loss: 0.00025289
Iteration 58/1000 | Loss: 0.00013239
Iteration 59/1000 | Loss: 0.00015319
Iteration 60/1000 | Loss: 0.00020987
Iteration 61/1000 | Loss: 0.00020668
Iteration 62/1000 | Loss: 0.00025851
Iteration 63/1000 | Loss: 0.00020077
Iteration 64/1000 | Loss: 0.00024933
Iteration 65/1000 | Loss: 0.00016225
Iteration 66/1000 | Loss: 0.00029478
Iteration 67/1000 | Loss: 0.00016327
Iteration 68/1000 | Loss: 0.00010967
Iteration 69/1000 | Loss: 0.00007664
Iteration 70/1000 | Loss: 0.00009915
Iteration 71/1000 | Loss: 0.00006292
Iteration 72/1000 | Loss: 0.00007731
Iteration 73/1000 | Loss: 0.00014847
Iteration 74/1000 | Loss: 0.00007182
Iteration 75/1000 | Loss: 0.00006908
Iteration 76/1000 | Loss: 0.00006420
Iteration 77/1000 | Loss: 0.00007679
Iteration 78/1000 | Loss: 0.00014977
Iteration 79/1000 | Loss: 0.00007486
Iteration 80/1000 | Loss: 0.00006302
Iteration 81/1000 | Loss: 0.00018332
Iteration 82/1000 | Loss: 0.00008576
Iteration 83/1000 | Loss: 0.00006962
Iteration 84/1000 | Loss: 0.00005498
Iteration 85/1000 | Loss: 0.00006511
Iteration 86/1000 | Loss: 0.00015491
Iteration 87/1000 | Loss: 0.00017208
Iteration 88/1000 | Loss: 0.00007921
Iteration 89/1000 | Loss: 0.00010043
Iteration 90/1000 | Loss: 0.00013965
Iteration 91/1000 | Loss: 0.00010731
Iteration 92/1000 | Loss: 0.00006672
Iteration 93/1000 | Loss: 0.00007638
Iteration 94/1000 | Loss: 0.00008815
Iteration 95/1000 | Loss: 0.00005472
Iteration 96/1000 | Loss: 0.00006192
Iteration 97/1000 | Loss: 0.00009178
Iteration 98/1000 | Loss: 0.00008890
Iteration 99/1000 | Loss: 0.00008840
Iteration 100/1000 | Loss: 0.00009711
Iteration 101/1000 | Loss: 0.00008144
Iteration 102/1000 | Loss: 0.00008777
Iteration 103/1000 | Loss: 0.00014476
Iteration 104/1000 | Loss: 0.00010903
Iteration 105/1000 | Loss: 0.00009178
Iteration 106/1000 | Loss: 0.00014608
Iteration 107/1000 | Loss: 0.00008457
Iteration 108/1000 | Loss: 0.00011996
Iteration 109/1000 | Loss: 0.00011761
Iteration 110/1000 | Loss: 0.00009242
Iteration 111/1000 | Loss: 0.00017194
Iteration 112/1000 | Loss: 0.00012167
Iteration 113/1000 | Loss: 0.00006167
Iteration 114/1000 | Loss: 0.00007153
Iteration 115/1000 | Loss: 0.00011982
Iteration 116/1000 | Loss: 0.00006226
Iteration 117/1000 | Loss: 0.00007142
Iteration 118/1000 | Loss: 0.00006283
Iteration 119/1000 | Loss: 0.00007939
Iteration 120/1000 | Loss: 0.00007811
Iteration 121/1000 | Loss: 0.00008366
Iteration 122/1000 | Loss: 0.00006688
Iteration 123/1000 | Loss: 0.00006572
Iteration 124/1000 | Loss: 0.00007811
Iteration 125/1000 | Loss: 0.00007735
Iteration 126/1000 | Loss: 0.00007073
Iteration 127/1000 | Loss: 0.00005667
Iteration 128/1000 | Loss: 0.00004997
Iteration 129/1000 | Loss: 0.00006878
Iteration 130/1000 | Loss: 0.00006407
Iteration 131/1000 | Loss: 0.00004820
Iteration 132/1000 | Loss: 0.00006121
Iteration 133/1000 | Loss: 0.00005580
Iteration 134/1000 | Loss: 0.00005175
Iteration 135/1000 | Loss: 0.00008424
Iteration 136/1000 | Loss: 0.00005614
Iteration 137/1000 | Loss: 0.00005745
Iteration 138/1000 | Loss: 0.00005018
Iteration 139/1000 | Loss: 0.00005095
Iteration 140/1000 | Loss: 0.00004364
Iteration 141/1000 | Loss: 0.00003875
Iteration 142/1000 | Loss: 0.00004156
Iteration 143/1000 | Loss: 0.00004070
Iteration 144/1000 | Loss: 0.00004322
Iteration 145/1000 | Loss: 0.00004832
Iteration 146/1000 | Loss: 0.00004406
Iteration 147/1000 | Loss: 0.00011829
Iteration 148/1000 | Loss: 0.00006514
Iteration 149/1000 | Loss: 0.00007506
Iteration 150/1000 | Loss: 0.00006262
Iteration 151/1000 | Loss: 0.00008139
Iteration 152/1000 | Loss: 0.00005885
Iteration 153/1000 | Loss: 0.00006982
Iteration 154/1000 | Loss: 0.00003611
Iteration 155/1000 | Loss: 0.00003106
Iteration 156/1000 | Loss: 0.00002832
Iteration 157/1000 | Loss: 0.00002645
Iteration 158/1000 | Loss: 0.00002656
Iteration 159/1000 | Loss: 0.00003094
Iteration 160/1000 | Loss: 0.00002536
Iteration 161/1000 | Loss: 0.00002373
Iteration 162/1000 | Loss: 0.00003418
Iteration 163/1000 | Loss: 0.00003416
Iteration 164/1000 | Loss: 0.00002870
Iteration 165/1000 | Loss: 0.00003758
Iteration 166/1000 | Loss: 0.00003256
Iteration 167/1000 | Loss: 0.00003005
Iteration 168/1000 | Loss: 0.00002860
Iteration 169/1000 | Loss: 0.00003189
Iteration 170/1000 | Loss: 0.00002972
Iteration 171/1000 | Loss: 0.00002780
Iteration 172/1000 | Loss: 0.00003116
Iteration 173/1000 | Loss: 0.00002876
Iteration 174/1000 | Loss: 0.00002800
Iteration 175/1000 | Loss: 0.00002477
Iteration 176/1000 | Loss: 0.00003105
Iteration 177/1000 | Loss: 0.00002566
Iteration 178/1000 | Loss: 0.00002982
Iteration 179/1000 | Loss: 0.00003305
Iteration 180/1000 | Loss: 0.00002238
Iteration 181/1000 | Loss: 0.00003267
Iteration 182/1000 | Loss: 0.00002669
Iteration 183/1000 | Loss: 0.00002739
Iteration 184/1000 | Loss: 0.00002554
Iteration 185/1000 | Loss: 0.00002704
Iteration 186/1000 | Loss: 0.00002337
Iteration 187/1000 | Loss: 0.00002565
Iteration 188/1000 | Loss: 0.00002703
Iteration 189/1000 | Loss: 0.00002680
Iteration 190/1000 | Loss: 0.00002515
Iteration 191/1000 | Loss: 0.00002674
Iteration 192/1000 | Loss: 0.00002497
Iteration 193/1000 | Loss: 0.00002841
Iteration 194/1000 | Loss: 0.00003360
Iteration 195/1000 | Loss: 0.00003346
Iteration 196/1000 | Loss: 0.00003159
Iteration 197/1000 | Loss: 0.00003143
Iteration 198/1000 | Loss: 0.00002696
Iteration 199/1000 | Loss: 0.00002864
Iteration 200/1000 | Loss: 0.00002858
Iteration 201/1000 | Loss: 0.00002502
Iteration 202/1000 | Loss: 0.00002714
Iteration 203/1000 | Loss: 0.00002699
Iteration 204/1000 | Loss: 0.00002696
Iteration 205/1000 | Loss: 0.00002563
Iteration 206/1000 | Loss: 0.00002699
Iteration 207/1000 | Loss: 0.00003151
Iteration 208/1000 | Loss: 0.00002732
Iteration 209/1000 | Loss: 0.00003350
Iteration 210/1000 | Loss: 0.00003030
Iteration 211/1000 | Loss: 0.00002666
Iteration 212/1000 | Loss: 0.00003065
Iteration 213/1000 | Loss: 0.00002720
Iteration 214/1000 | Loss: 0.00003251
Iteration 215/1000 | Loss: 0.00003005
Iteration 216/1000 | Loss: 0.00002810
Iteration 217/1000 | Loss: 0.00002930
Iteration 218/1000 | Loss: 0.00002952
Iteration 219/1000 | Loss: 0.00003143
Iteration 220/1000 | Loss: 0.00002901
Iteration 221/1000 | Loss: 0.00002335
Iteration 222/1000 | Loss: 0.00002615
Iteration 223/1000 | Loss: 0.00003068
Iteration 224/1000 | Loss: 0.00003171
Iteration 225/1000 | Loss: 0.00003065
Iteration 226/1000 | Loss: 0.00003126
Iteration 227/1000 | Loss: 0.00003113
Iteration 228/1000 | Loss: 0.00003269
Iteration 229/1000 | Loss: 0.00003102
Iteration 230/1000 | Loss: 0.00003150
Iteration 231/1000 | Loss: 0.00002205
Iteration 232/1000 | Loss: 0.00002120
Iteration 233/1000 | Loss: 0.00002069
Iteration 234/1000 | Loss: 0.00002045
Iteration 235/1000 | Loss: 0.00002041
Iteration 236/1000 | Loss: 0.00002039
Iteration 237/1000 | Loss: 0.00002034
Iteration 238/1000 | Loss: 0.00002034
Iteration 239/1000 | Loss: 0.00002032
Iteration 240/1000 | Loss: 0.00002032
Iteration 241/1000 | Loss: 0.00002031
Iteration 242/1000 | Loss: 0.00002031
Iteration 243/1000 | Loss: 0.00002031
Iteration 244/1000 | Loss: 0.00002031
Iteration 245/1000 | Loss: 0.00002031
Iteration 246/1000 | Loss: 0.00002031
Iteration 247/1000 | Loss: 0.00002031
Iteration 248/1000 | Loss: 0.00002031
Iteration 249/1000 | Loss: 0.00002031
Iteration 250/1000 | Loss: 0.00002031
Iteration 251/1000 | Loss: 0.00002030
Iteration 252/1000 | Loss: 0.00002030
Iteration 253/1000 | Loss: 0.00002030
Iteration 254/1000 | Loss: 0.00002030
Iteration 255/1000 | Loss: 0.00002030
Iteration 256/1000 | Loss: 0.00002030
Iteration 257/1000 | Loss: 0.00002030
Iteration 258/1000 | Loss: 0.00002029
Iteration 259/1000 | Loss: 0.00002029
Iteration 260/1000 | Loss: 0.00002029
Iteration 261/1000 | Loss: 0.00002029
Iteration 262/1000 | Loss: 0.00002028
Iteration 263/1000 | Loss: 0.00002028
Iteration 264/1000 | Loss: 0.00002028
Iteration 265/1000 | Loss: 0.00002028
Iteration 266/1000 | Loss: 0.00002027
Iteration 267/1000 | Loss: 0.00002027
Iteration 268/1000 | Loss: 0.00002027
Iteration 269/1000 | Loss: 0.00002027
Iteration 270/1000 | Loss: 0.00002027
Iteration 271/1000 | Loss: 0.00002026
Iteration 272/1000 | Loss: 0.00002026
Iteration 273/1000 | Loss: 0.00002026
Iteration 274/1000 | Loss: 0.00002026
Iteration 275/1000 | Loss: 0.00002026
Iteration 276/1000 | Loss: 0.00002026
Iteration 277/1000 | Loss: 0.00002026
Iteration 278/1000 | Loss: 0.00002026
Iteration 279/1000 | Loss: 0.00002025
Iteration 280/1000 | Loss: 0.00002025
Iteration 281/1000 | Loss: 0.00002025
Iteration 282/1000 | Loss: 0.00002025
Iteration 283/1000 | Loss: 0.00002025
Iteration 284/1000 | Loss: 0.00002025
Iteration 285/1000 | Loss: 0.00002025
Iteration 286/1000 | Loss: 0.00002025
Iteration 287/1000 | Loss: 0.00002025
Iteration 288/1000 | Loss: 0.00002025
Iteration 289/1000 | Loss: 0.00002025
Iteration 290/1000 | Loss: 0.00002024
Iteration 291/1000 | Loss: 0.00002024
Iteration 292/1000 | Loss: 0.00002024
Iteration 293/1000 | Loss: 0.00002024
Iteration 294/1000 | Loss: 0.00002024
Iteration 295/1000 | Loss: 0.00002024
Iteration 296/1000 | Loss: 0.00002024
Iteration 297/1000 | Loss: 0.00002024
Iteration 298/1000 | Loss: 0.00002023
Iteration 299/1000 | Loss: 0.00002023
Iteration 300/1000 | Loss: 0.00002023
Iteration 301/1000 | Loss: 0.00002023
Iteration 302/1000 | Loss: 0.00002023
Iteration 303/1000 | Loss: 0.00002023
Iteration 304/1000 | Loss: 0.00002023
Iteration 305/1000 | Loss: 0.00002023
Iteration 306/1000 | Loss: 0.00002023
Iteration 307/1000 | Loss: 0.00002023
Iteration 308/1000 | Loss: 0.00002023
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 308. Stopping optimization.
Last 5 losses: [2.022920671151951e-05, 2.022920671151951e-05, 2.022920671151951e-05, 2.022920671151951e-05, 2.022920671151951e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.022920671151951e-05

Optimization complete. Final v2v error: 3.7351934909820557 mm

Highest mean error: 4.983030796051025 mm for frame 17

Lowest mean error: 3.549996852874756 mm for frame 135

Saving results

Total time: 434.54995226860046
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_003/1036/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_003/1036.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_003/1036
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01061850
Iteration 2/25 | Loss: 0.00194000
Iteration 3/25 | Loss: 0.00149052
Iteration 4/25 | Loss: 0.00139931
Iteration 5/25 | Loss: 0.00138492
Iteration 6/25 | Loss: 0.00135643
Iteration 7/25 | Loss: 0.00130584
Iteration 8/25 | Loss: 0.00126463
Iteration 9/25 | Loss: 0.00123316
Iteration 10/25 | Loss: 0.00123298
Iteration 11/25 | Loss: 0.00122927
Iteration 12/25 | Loss: 0.00122870
Iteration 13/25 | Loss: 0.00122802
Iteration 14/25 | Loss: 0.00122801
Iteration 15/25 | Loss: 0.00122801
Iteration 16/25 | Loss: 0.00122801
Iteration 17/25 | Loss: 0.00122801
Iteration 18/25 | Loss: 0.00122801
Iteration 19/25 | Loss: 0.00122801
Iteration 20/25 | Loss: 0.00122801
Iteration 21/25 | Loss: 0.00122801
Iteration 22/25 | Loss: 0.00122801
Iteration 23/25 | Loss: 0.00122801
Iteration 24/25 | Loss: 0.00122801
Iteration 25/25 | Loss: 0.00122801

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.48469269
Iteration 2/25 | Loss: 0.00133117
Iteration 3/25 | Loss: 0.00133117
Iteration 4/25 | Loss: 0.00133117
Iteration 5/25 | Loss: 0.00133117
Iteration 6/25 | Loss: 0.00133117
Iteration 7/25 | Loss: 0.00133117
Iteration 8/25 | Loss: 0.00133117
Iteration 9/25 | Loss: 0.00133117
Iteration 10/25 | Loss: 0.00133117
Iteration 11/25 | Loss: 0.00133117
Iteration 12/25 | Loss: 0.00133117
Iteration 13/25 | Loss: 0.00133117
Iteration 14/25 | Loss: 0.00133117
Iteration 15/25 | Loss: 0.00133117
Iteration 16/25 | Loss: 0.00133117
Iteration 17/25 | Loss: 0.00133117
Iteration 18/25 | Loss: 0.00133117
Iteration 19/25 | Loss: 0.00133117
Iteration 20/25 | Loss: 0.00133117
Iteration 21/25 | Loss: 0.00133117
Iteration 22/25 | Loss: 0.00133117
Iteration 23/25 | Loss: 0.00133117
Iteration 24/25 | Loss: 0.00133117
Iteration 25/25 | Loss: 0.00133117

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00133117
Iteration 2/1000 | Loss: 0.00003445
Iteration 3/1000 | Loss: 0.00002474
Iteration 4/1000 | Loss: 0.00002134
Iteration 5/1000 | Loss: 0.00001997
Iteration 6/1000 | Loss: 0.00001890
Iteration 7/1000 | Loss: 0.00004139
Iteration 8/1000 | Loss: 0.00002254
Iteration 9/1000 | Loss: 0.00001787
Iteration 10/1000 | Loss: 0.00001750
Iteration 11/1000 | Loss: 0.00001748
Iteration 12/1000 | Loss: 0.00001742
Iteration 13/1000 | Loss: 0.00001721
Iteration 14/1000 | Loss: 0.00001708
Iteration 15/1000 | Loss: 0.00001706
Iteration 16/1000 | Loss: 0.00001704
Iteration 17/1000 | Loss: 0.00001703
Iteration 18/1000 | Loss: 0.00001696
Iteration 19/1000 | Loss: 0.00001691
Iteration 20/1000 | Loss: 0.00005086
Iteration 21/1000 | Loss: 0.00025557
Iteration 22/1000 | Loss: 0.00006693
Iteration 23/1000 | Loss: 0.00003463
Iteration 24/1000 | Loss: 0.00001875
Iteration 25/1000 | Loss: 0.00001680
Iteration 26/1000 | Loss: 0.00001668
Iteration 27/1000 | Loss: 0.00001662
Iteration 28/1000 | Loss: 0.00001661
Iteration 29/1000 | Loss: 0.00001661
Iteration 30/1000 | Loss: 0.00001661
Iteration 31/1000 | Loss: 0.00001661
Iteration 32/1000 | Loss: 0.00001661
Iteration 33/1000 | Loss: 0.00001661
Iteration 34/1000 | Loss: 0.00001661
Iteration 35/1000 | Loss: 0.00001661
Iteration 36/1000 | Loss: 0.00001661
Iteration 37/1000 | Loss: 0.00001661
Iteration 38/1000 | Loss: 0.00001661
Iteration 39/1000 | Loss: 0.00001661
Iteration 40/1000 | Loss: 0.00001660
Iteration 41/1000 | Loss: 0.00001660
Iteration 42/1000 | Loss: 0.00001660
Iteration 43/1000 | Loss: 0.00001660
Iteration 44/1000 | Loss: 0.00001659
Iteration 45/1000 | Loss: 0.00001659
Iteration 46/1000 | Loss: 0.00001658
Iteration 47/1000 | Loss: 0.00001655
Iteration 48/1000 | Loss: 0.00001654
Iteration 49/1000 | Loss: 0.00001653
Iteration 50/1000 | Loss: 0.00001652
Iteration 51/1000 | Loss: 0.00001652
Iteration 52/1000 | Loss: 0.00001652
Iteration 53/1000 | Loss: 0.00001651
Iteration 54/1000 | Loss: 0.00001651
Iteration 55/1000 | Loss: 0.00001651
Iteration 56/1000 | Loss: 0.00001650
Iteration 57/1000 | Loss: 0.00001650
Iteration 58/1000 | Loss: 0.00001650
Iteration 59/1000 | Loss: 0.00001649
Iteration 60/1000 | Loss: 0.00001649
Iteration 61/1000 | Loss: 0.00001649
Iteration 62/1000 | Loss: 0.00001649
Iteration 63/1000 | Loss: 0.00001649
Iteration 64/1000 | Loss: 0.00001649
Iteration 65/1000 | Loss: 0.00001648
Iteration 66/1000 | Loss: 0.00001648
Iteration 67/1000 | Loss: 0.00001648
Iteration 68/1000 | Loss: 0.00001648
Iteration 69/1000 | Loss: 0.00001648
Iteration 70/1000 | Loss: 0.00001648
Iteration 71/1000 | Loss: 0.00001648
Iteration 72/1000 | Loss: 0.00001647
Iteration 73/1000 | Loss: 0.00001646
Iteration 74/1000 | Loss: 0.00001646
Iteration 75/1000 | Loss: 0.00001646
Iteration 76/1000 | Loss: 0.00001646
Iteration 77/1000 | Loss: 0.00001646
Iteration 78/1000 | Loss: 0.00001645
Iteration 79/1000 | Loss: 0.00001645
Iteration 80/1000 | Loss: 0.00001645
Iteration 81/1000 | Loss: 0.00001645
Iteration 82/1000 | Loss: 0.00001645
Iteration 83/1000 | Loss: 0.00001645
Iteration 84/1000 | Loss: 0.00001644
Iteration 85/1000 | Loss: 0.00001644
Iteration 86/1000 | Loss: 0.00001644
Iteration 87/1000 | Loss: 0.00001644
Iteration 88/1000 | Loss: 0.00001644
Iteration 89/1000 | Loss: 0.00001644
Iteration 90/1000 | Loss: 0.00001643
Iteration 91/1000 | Loss: 0.00001643
Iteration 92/1000 | Loss: 0.00001643
Iteration 93/1000 | Loss: 0.00001642
Iteration 94/1000 | Loss: 0.00001641
Iteration 95/1000 | Loss: 0.00001641
Iteration 96/1000 | Loss: 0.00005559
Iteration 97/1000 | Loss: 0.00002372
Iteration 98/1000 | Loss: 0.00006043
Iteration 99/1000 | Loss: 0.00001655
Iteration 100/1000 | Loss: 0.00001634
Iteration 101/1000 | Loss: 0.00001634
Iteration 102/1000 | Loss: 0.00001633
Iteration 103/1000 | Loss: 0.00001631
Iteration 104/1000 | Loss: 0.00001631
Iteration 105/1000 | Loss: 0.00001631
Iteration 106/1000 | Loss: 0.00001631
Iteration 107/1000 | Loss: 0.00001631
Iteration 108/1000 | Loss: 0.00001631
Iteration 109/1000 | Loss: 0.00001631
Iteration 110/1000 | Loss: 0.00001631
Iteration 111/1000 | Loss: 0.00001630
Iteration 112/1000 | Loss: 0.00001630
Iteration 113/1000 | Loss: 0.00001630
Iteration 114/1000 | Loss: 0.00001630
Iteration 115/1000 | Loss: 0.00001630
Iteration 116/1000 | Loss: 0.00001630
Iteration 117/1000 | Loss: 0.00001629
Iteration 118/1000 | Loss: 0.00001629
Iteration 119/1000 | Loss: 0.00001629
Iteration 120/1000 | Loss: 0.00001628
Iteration 121/1000 | Loss: 0.00001628
Iteration 122/1000 | Loss: 0.00001628
Iteration 123/1000 | Loss: 0.00001628
Iteration 124/1000 | Loss: 0.00001628
Iteration 125/1000 | Loss: 0.00001628
Iteration 126/1000 | Loss: 0.00001628
Iteration 127/1000 | Loss: 0.00001627
Iteration 128/1000 | Loss: 0.00001627
Iteration 129/1000 | Loss: 0.00001627
Iteration 130/1000 | Loss: 0.00001627
Iteration 131/1000 | Loss: 0.00001627
Iteration 132/1000 | Loss: 0.00001626
Iteration 133/1000 | Loss: 0.00001626
Iteration 134/1000 | Loss: 0.00001626
Iteration 135/1000 | Loss: 0.00001626
Iteration 136/1000 | Loss: 0.00001626
Iteration 137/1000 | Loss: 0.00001625
Iteration 138/1000 | Loss: 0.00001625
Iteration 139/1000 | Loss: 0.00001625
Iteration 140/1000 | Loss: 0.00001625
Iteration 141/1000 | Loss: 0.00001625
Iteration 142/1000 | Loss: 0.00001625
Iteration 143/1000 | Loss: 0.00001625
Iteration 144/1000 | Loss: 0.00001624
Iteration 145/1000 | Loss: 0.00001624
Iteration 146/1000 | Loss: 0.00001624
Iteration 147/1000 | Loss: 0.00001623
Iteration 148/1000 | Loss: 0.00001623
Iteration 149/1000 | Loss: 0.00001623
Iteration 150/1000 | Loss: 0.00001623
Iteration 151/1000 | Loss: 0.00001623
Iteration 152/1000 | Loss: 0.00001622
Iteration 153/1000 | Loss: 0.00001622
Iteration 154/1000 | Loss: 0.00001622
Iteration 155/1000 | Loss: 0.00001622
Iteration 156/1000 | Loss: 0.00001622
Iteration 157/1000 | Loss: 0.00001622
Iteration 158/1000 | Loss: 0.00001622
Iteration 159/1000 | Loss: 0.00001622
Iteration 160/1000 | Loss: 0.00001622
Iteration 161/1000 | Loss: 0.00001622
Iteration 162/1000 | Loss: 0.00001621
Iteration 163/1000 | Loss: 0.00001621
Iteration 164/1000 | Loss: 0.00001621
Iteration 165/1000 | Loss: 0.00001621
Iteration 166/1000 | Loss: 0.00001621
Iteration 167/1000 | Loss: 0.00001621
Iteration 168/1000 | Loss: 0.00001621
Iteration 169/1000 | Loss: 0.00001621
Iteration 170/1000 | Loss: 0.00001621
Iteration 171/1000 | Loss: 0.00005834
Iteration 172/1000 | Loss: 0.00002192
Iteration 173/1000 | Loss: 0.00003404
Iteration 174/1000 | Loss: 0.00001626
Iteration 175/1000 | Loss: 0.00001623
Iteration 176/1000 | Loss: 0.00001619
Iteration 177/1000 | Loss: 0.00001619
Iteration 178/1000 | Loss: 0.00001619
Iteration 179/1000 | Loss: 0.00001619
Iteration 180/1000 | Loss: 0.00001619
Iteration 181/1000 | Loss: 0.00001619
Iteration 182/1000 | Loss: 0.00001619
Iteration 183/1000 | Loss: 0.00001619
Iteration 184/1000 | Loss: 0.00001618
Iteration 185/1000 | Loss: 0.00001618
Iteration 186/1000 | Loss: 0.00001618
Iteration 187/1000 | Loss: 0.00001618
Iteration 188/1000 | Loss: 0.00001618
Iteration 189/1000 | Loss: 0.00001618
Iteration 190/1000 | Loss: 0.00001618
Iteration 191/1000 | Loss: 0.00001618
Iteration 192/1000 | Loss: 0.00001618
Iteration 193/1000 | Loss: 0.00001618
Iteration 194/1000 | Loss: 0.00001618
Iteration 195/1000 | Loss: 0.00001618
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 195. Stopping optimization.
Last 5 losses: [1.6182670151465572e-05, 1.6182670151465572e-05, 1.6182670151465572e-05, 1.6182670151465572e-05, 1.6182670151465572e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6182670151465572e-05

Optimization complete. Final v2v error: 3.3675854206085205 mm

Highest mean error: 4.725832939147949 mm for frame 80

Lowest mean error: 2.8680639266967773 mm for frame 35

Saving results

Total time: 73.14777255058289
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_003/1054/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_003/1054.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_003/1054
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00860074
Iteration 2/25 | Loss: 0.00182953
Iteration 3/25 | Loss: 0.00149521
Iteration 4/25 | Loss: 0.00143839
Iteration 5/25 | Loss: 0.00149893
Iteration 6/25 | Loss: 0.00143322
Iteration 7/25 | Loss: 0.00135251
Iteration 8/25 | Loss: 0.00132836
Iteration 9/25 | Loss: 0.00132280
Iteration 10/25 | Loss: 0.00130798
Iteration 11/25 | Loss: 0.00130374
Iteration 12/25 | Loss: 0.00130114
Iteration 13/25 | Loss: 0.00128363
Iteration 14/25 | Loss: 0.00127919
Iteration 15/25 | Loss: 0.00128284
Iteration 16/25 | Loss: 0.00128237
Iteration 17/25 | Loss: 0.00128031
Iteration 18/25 | Loss: 0.00127753
Iteration 19/25 | Loss: 0.00127676
Iteration 20/25 | Loss: 0.00127660
Iteration 21/25 | Loss: 0.00127654
Iteration 22/25 | Loss: 0.00127653
Iteration 23/25 | Loss: 0.00127653
Iteration 24/25 | Loss: 0.00127653
Iteration 25/25 | Loss: 0.00127653

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 6.49107265
Iteration 2/25 | Loss: 0.00126739
Iteration 3/25 | Loss: 0.00126733
Iteration 4/25 | Loss: 0.00126732
Iteration 5/25 | Loss: 0.00126732
Iteration 6/25 | Loss: 0.00126732
Iteration 7/25 | Loss: 0.00126732
Iteration 8/25 | Loss: 0.00126732
Iteration 9/25 | Loss: 0.00126732
Iteration 10/25 | Loss: 0.00126732
Iteration 11/25 | Loss: 0.00126732
Iteration 12/25 | Loss: 0.00126732
Iteration 13/25 | Loss: 0.00126732
Iteration 14/25 | Loss: 0.00126732
Iteration 15/25 | Loss: 0.00126732
Iteration 16/25 | Loss: 0.00126732
Iteration 17/25 | Loss: 0.00126732
Iteration 18/25 | Loss: 0.00126732
Iteration 19/25 | Loss: 0.00126732
Iteration 20/25 | Loss: 0.00126732
Iteration 21/25 | Loss: 0.00126732
Iteration 22/25 | Loss: 0.00126732
Iteration 23/25 | Loss: 0.00126732
Iteration 24/25 | Loss: 0.00126732
Iteration 25/25 | Loss: 0.00126732

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00126732
Iteration 2/1000 | Loss: 0.00006031
Iteration 3/1000 | Loss: 0.00016688
Iteration 4/1000 | Loss: 0.00005638
Iteration 5/1000 | Loss: 0.00002844
Iteration 6/1000 | Loss: 0.00002361
Iteration 7/1000 | Loss: 0.00005171
Iteration 8/1000 | Loss: 0.00003549
Iteration 9/1000 | Loss: 0.00004641
Iteration 10/1000 | Loss: 0.00003777
Iteration 11/1000 | Loss: 0.00003004
Iteration 12/1000 | Loss: 0.00002299
Iteration 13/1000 | Loss: 0.00002022
Iteration 14/1000 | Loss: 0.00001917
Iteration 15/1000 | Loss: 0.00001838
Iteration 16/1000 | Loss: 0.00001805
Iteration 17/1000 | Loss: 0.00001786
Iteration 18/1000 | Loss: 0.00001752
Iteration 19/1000 | Loss: 0.00001738
Iteration 20/1000 | Loss: 0.00001717
Iteration 21/1000 | Loss: 0.00001709
Iteration 22/1000 | Loss: 0.00001707
Iteration 23/1000 | Loss: 0.00001701
Iteration 24/1000 | Loss: 0.00001700
Iteration 25/1000 | Loss: 0.00001699
Iteration 26/1000 | Loss: 0.00001680
Iteration 27/1000 | Loss: 0.00001674
Iteration 28/1000 | Loss: 0.00001673
Iteration 29/1000 | Loss: 0.00001672
Iteration 30/1000 | Loss: 0.00001672
Iteration 31/1000 | Loss: 0.00001672
Iteration 32/1000 | Loss: 0.00001672
Iteration 33/1000 | Loss: 0.00001672
Iteration 34/1000 | Loss: 0.00001672
Iteration 35/1000 | Loss: 0.00001671
Iteration 36/1000 | Loss: 0.00001671
Iteration 37/1000 | Loss: 0.00001671
Iteration 38/1000 | Loss: 0.00001671
Iteration 39/1000 | Loss: 0.00001671
Iteration 40/1000 | Loss: 0.00001671
Iteration 41/1000 | Loss: 0.00001671
Iteration 42/1000 | Loss: 0.00001671
Iteration 43/1000 | Loss: 0.00001671
Iteration 44/1000 | Loss: 0.00001670
Iteration 45/1000 | Loss: 0.00001670
Iteration 46/1000 | Loss: 0.00001669
Iteration 47/1000 | Loss: 0.00001669
Iteration 48/1000 | Loss: 0.00001669
Iteration 49/1000 | Loss: 0.00001668
Iteration 50/1000 | Loss: 0.00001668
Iteration 51/1000 | Loss: 0.00001668
Iteration 52/1000 | Loss: 0.00001668
Iteration 53/1000 | Loss: 0.00001668
Iteration 54/1000 | Loss: 0.00001667
Iteration 55/1000 | Loss: 0.00001667
Iteration 56/1000 | Loss: 0.00001667
Iteration 57/1000 | Loss: 0.00001667
Iteration 58/1000 | Loss: 0.00001666
Iteration 59/1000 | Loss: 0.00001666
Iteration 60/1000 | Loss: 0.00001666
Iteration 61/1000 | Loss: 0.00001665
Iteration 62/1000 | Loss: 0.00001665
Iteration 63/1000 | Loss: 0.00001665
Iteration 64/1000 | Loss: 0.00001665
Iteration 65/1000 | Loss: 0.00001665
Iteration 66/1000 | Loss: 0.00001664
Iteration 67/1000 | Loss: 0.00001664
Iteration 68/1000 | Loss: 0.00001664
Iteration 69/1000 | Loss: 0.00001664
Iteration 70/1000 | Loss: 0.00001664
Iteration 71/1000 | Loss: 0.00001664
Iteration 72/1000 | Loss: 0.00001664
Iteration 73/1000 | Loss: 0.00001664
Iteration 74/1000 | Loss: 0.00001664
Iteration 75/1000 | Loss: 0.00001663
Iteration 76/1000 | Loss: 0.00001663
Iteration 77/1000 | Loss: 0.00001663
Iteration 78/1000 | Loss: 0.00001662
Iteration 79/1000 | Loss: 0.00001662
Iteration 80/1000 | Loss: 0.00001662
Iteration 81/1000 | Loss: 0.00001662
Iteration 82/1000 | Loss: 0.00001662
Iteration 83/1000 | Loss: 0.00001662
Iteration 84/1000 | Loss: 0.00001661
Iteration 85/1000 | Loss: 0.00001661
Iteration 86/1000 | Loss: 0.00001661
Iteration 87/1000 | Loss: 0.00001661
Iteration 88/1000 | Loss: 0.00001660
Iteration 89/1000 | Loss: 0.00001660
Iteration 90/1000 | Loss: 0.00001660
Iteration 91/1000 | Loss: 0.00001660
Iteration 92/1000 | Loss: 0.00001660
Iteration 93/1000 | Loss: 0.00001660
Iteration 94/1000 | Loss: 0.00001660
Iteration 95/1000 | Loss: 0.00001659
Iteration 96/1000 | Loss: 0.00001659
Iteration 97/1000 | Loss: 0.00001659
Iteration 98/1000 | Loss: 0.00001659
Iteration 99/1000 | Loss: 0.00001659
Iteration 100/1000 | Loss: 0.00001659
Iteration 101/1000 | Loss: 0.00001659
Iteration 102/1000 | Loss: 0.00001658
Iteration 103/1000 | Loss: 0.00001658
Iteration 104/1000 | Loss: 0.00001658
Iteration 105/1000 | Loss: 0.00001657
Iteration 106/1000 | Loss: 0.00001657
Iteration 107/1000 | Loss: 0.00001656
Iteration 108/1000 | Loss: 0.00001656
Iteration 109/1000 | Loss: 0.00001656
Iteration 110/1000 | Loss: 0.00001656
Iteration 111/1000 | Loss: 0.00001656
Iteration 112/1000 | Loss: 0.00001655
Iteration 113/1000 | Loss: 0.00001655
Iteration 114/1000 | Loss: 0.00001655
Iteration 115/1000 | Loss: 0.00001655
Iteration 116/1000 | Loss: 0.00001654
Iteration 117/1000 | Loss: 0.00001654
Iteration 118/1000 | Loss: 0.00001654
Iteration 119/1000 | Loss: 0.00001653
Iteration 120/1000 | Loss: 0.00001653
Iteration 121/1000 | Loss: 0.00001652
Iteration 122/1000 | Loss: 0.00001652
Iteration 123/1000 | Loss: 0.00001651
Iteration 124/1000 | Loss: 0.00001651
Iteration 125/1000 | Loss: 0.00001651
Iteration 126/1000 | Loss: 0.00001651
Iteration 127/1000 | Loss: 0.00001651
Iteration 128/1000 | Loss: 0.00001651
Iteration 129/1000 | Loss: 0.00001650
Iteration 130/1000 | Loss: 0.00001650
Iteration 131/1000 | Loss: 0.00001650
Iteration 132/1000 | Loss: 0.00001650
Iteration 133/1000 | Loss: 0.00001650
Iteration 134/1000 | Loss: 0.00001649
Iteration 135/1000 | Loss: 0.00001649
Iteration 136/1000 | Loss: 0.00001649
Iteration 137/1000 | Loss: 0.00001649
Iteration 138/1000 | Loss: 0.00001648
Iteration 139/1000 | Loss: 0.00001648
Iteration 140/1000 | Loss: 0.00001648
Iteration 141/1000 | Loss: 0.00001648
Iteration 142/1000 | Loss: 0.00001648
Iteration 143/1000 | Loss: 0.00001648
Iteration 144/1000 | Loss: 0.00001648
Iteration 145/1000 | Loss: 0.00001648
Iteration 146/1000 | Loss: 0.00001648
Iteration 147/1000 | Loss: 0.00001647
Iteration 148/1000 | Loss: 0.00001647
Iteration 149/1000 | Loss: 0.00001646
Iteration 150/1000 | Loss: 0.00001645
Iteration 151/1000 | Loss: 0.00001645
Iteration 152/1000 | Loss: 0.00001645
Iteration 153/1000 | Loss: 0.00001645
Iteration 154/1000 | Loss: 0.00001645
Iteration 155/1000 | Loss: 0.00001645
Iteration 156/1000 | Loss: 0.00001645
Iteration 157/1000 | Loss: 0.00001645
Iteration 158/1000 | Loss: 0.00001645
Iteration 159/1000 | Loss: 0.00001644
Iteration 160/1000 | Loss: 0.00001644
Iteration 161/1000 | Loss: 0.00001644
Iteration 162/1000 | Loss: 0.00001644
Iteration 163/1000 | Loss: 0.00001644
Iteration 164/1000 | Loss: 0.00001644
Iteration 165/1000 | Loss: 0.00001644
Iteration 166/1000 | Loss: 0.00001644
Iteration 167/1000 | Loss: 0.00001644
Iteration 168/1000 | Loss: 0.00001643
Iteration 169/1000 | Loss: 0.00001643
Iteration 170/1000 | Loss: 0.00001643
Iteration 171/1000 | Loss: 0.00001643
Iteration 172/1000 | Loss: 0.00001642
Iteration 173/1000 | Loss: 0.00001642
Iteration 174/1000 | Loss: 0.00001642
Iteration 175/1000 | Loss: 0.00001640
Iteration 176/1000 | Loss: 0.00001640
Iteration 177/1000 | Loss: 0.00001640
Iteration 178/1000 | Loss: 0.00001640
Iteration 179/1000 | Loss: 0.00001640
Iteration 180/1000 | Loss: 0.00001640
Iteration 181/1000 | Loss: 0.00001640
Iteration 182/1000 | Loss: 0.00001639
Iteration 183/1000 | Loss: 0.00001639
Iteration 184/1000 | Loss: 0.00001639
Iteration 185/1000 | Loss: 0.00001638
Iteration 186/1000 | Loss: 0.00001638
Iteration 187/1000 | Loss: 0.00001638
Iteration 188/1000 | Loss: 0.00001638
Iteration 189/1000 | Loss: 0.00001637
Iteration 190/1000 | Loss: 0.00001637
Iteration 191/1000 | Loss: 0.00001637
Iteration 192/1000 | Loss: 0.00001637
Iteration 193/1000 | Loss: 0.00001637
Iteration 194/1000 | Loss: 0.00001637
Iteration 195/1000 | Loss: 0.00001637
Iteration 196/1000 | Loss: 0.00001636
Iteration 197/1000 | Loss: 0.00001636
Iteration 198/1000 | Loss: 0.00001636
Iteration 199/1000 | Loss: 0.00001636
Iteration 200/1000 | Loss: 0.00001636
Iteration 201/1000 | Loss: 0.00001636
Iteration 202/1000 | Loss: 0.00001636
Iteration 203/1000 | Loss: 0.00001636
Iteration 204/1000 | Loss: 0.00001636
Iteration 205/1000 | Loss: 0.00001636
Iteration 206/1000 | Loss: 0.00001635
Iteration 207/1000 | Loss: 0.00001635
Iteration 208/1000 | Loss: 0.00001635
Iteration 209/1000 | Loss: 0.00001635
Iteration 210/1000 | Loss: 0.00001635
Iteration 211/1000 | Loss: 0.00001635
Iteration 212/1000 | Loss: 0.00001635
Iteration 213/1000 | Loss: 0.00001635
Iteration 214/1000 | Loss: 0.00001635
Iteration 215/1000 | Loss: 0.00001635
Iteration 216/1000 | Loss: 0.00001635
Iteration 217/1000 | Loss: 0.00001635
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 217. Stopping optimization.
Last 5 losses: [1.635121225262992e-05, 1.635121225262992e-05, 1.635121225262992e-05, 1.635121225262992e-05, 1.635121225262992e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.635121225262992e-05

Optimization complete. Final v2v error: 3.340773820877075 mm

Highest mean error: 5.22622013092041 mm for frame 95

Lowest mean error: 2.9256174564361572 mm for frame 52

Saving results

Total time: 84.31570792198181
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_003/1015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_003/1015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_003/1015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00821681
Iteration 2/25 | Loss: 0.00154302
Iteration 3/25 | Loss: 0.00130194
Iteration 4/25 | Loss: 0.00128027
Iteration 5/25 | Loss: 0.00127826
Iteration 6/25 | Loss: 0.00127826
Iteration 7/25 | Loss: 0.00127826
Iteration 8/25 | Loss: 0.00127826
Iteration 9/25 | Loss: 0.00127826
Iteration 10/25 | Loss: 0.00127826
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0012782583944499493, 0.0012782583944499493, 0.0012782583944499493, 0.0012782583944499493, 0.0012782583944499493]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012782583944499493

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.94792765
Iteration 2/25 | Loss: 0.00065240
Iteration 3/25 | Loss: 0.00065239
Iteration 4/25 | Loss: 0.00065239
Iteration 5/25 | Loss: 0.00065239
Iteration 6/25 | Loss: 0.00065239
Iteration 7/25 | Loss: 0.00065239
Iteration 8/25 | Loss: 0.00065239
Iteration 9/25 | Loss: 0.00065239
Iteration 10/25 | Loss: 0.00065239
Iteration 11/25 | Loss: 0.00065239
Iteration 12/25 | Loss: 0.00065239
Iteration 13/25 | Loss: 0.00065239
Iteration 14/25 | Loss: 0.00065239
Iteration 15/25 | Loss: 0.00065239
Iteration 16/25 | Loss: 0.00065239
Iteration 17/25 | Loss: 0.00065239
Iteration 18/25 | Loss: 0.00065239
Iteration 19/25 | Loss: 0.00065239
Iteration 20/25 | Loss: 0.00065239
Iteration 21/25 | Loss: 0.00065239
Iteration 22/25 | Loss: 0.00065239
Iteration 23/25 | Loss: 0.00065239
Iteration 24/25 | Loss: 0.00065239
Iteration 25/25 | Loss: 0.00065239

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00065239
Iteration 2/1000 | Loss: 0.00003265
Iteration 3/1000 | Loss: 0.00002681
Iteration 4/1000 | Loss: 0.00002441
Iteration 5/1000 | Loss: 0.00002341
Iteration 6/1000 | Loss: 0.00002287
Iteration 7/1000 | Loss: 0.00002230
Iteration 8/1000 | Loss: 0.00002187
Iteration 9/1000 | Loss: 0.00002158
Iteration 10/1000 | Loss: 0.00002135
Iteration 11/1000 | Loss: 0.00002122
Iteration 12/1000 | Loss: 0.00002111
Iteration 13/1000 | Loss: 0.00002110
Iteration 14/1000 | Loss: 0.00002096
Iteration 15/1000 | Loss: 0.00002091
Iteration 16/1000 | Loss: 0.00002088
Iteration 17/1000 | Loss: 0.00002088
Iteration 18/1000 | Loss: 0.00002084
Iteration 19/1000 | Loss: 0.00002084
Iteration 20/1000 | Loss: 0.00002084
Iteration 21/1000 | Loss: 0.00002083
Iteration 22/1000 | Loss: 0.00002083
Iteration 23/1000 | Loss: 0.00002082
Iteration 24/1000 | Loss: 0.00002082
Iteration 25/1000 | Loss: 0.00002079
Iteration 26/1000 | Loss: 0.00002078
Iteration 27/1000 | Loss: 0.00002078
Iteration 28/1000 | Loss: 0.00002078
Iteration 29/1000 | Loss: 0.00002077
Iteration 30/1000 | Loss: 0.00002075
Iteration 31/1000 | Loss: 0.00002074
Iteration 32/1000 | Loss: 0.00002074
Iteration 33/1000 | Loss: 0.00002074
Iteration 34/1000 | Loss: 0.00002074
Iteration 35/1000 | Loss: 0.00002074
Iteration 36/1000 | Loss: 0.00002074
Iteration 37/1000 | Loss: 0.00002074
Iteration 38/1000 | Loss: 0.00002074
Iteration 39/1000 | Loss: 0.00002073
Iteration 40/1000 | Loss: 0.00002073
Iteration 41/1000 | Loss: 0.00002072
Iteration 42/1000 | Loss: 0.00002072
Iteration 43/1000 | Loss: 0.00002072
Iteration 44/1000 | Loss: 0.00002072
Iteration 45/1000 | Loss: 0.00002071
Iteration 46/1000 | Loss: 0.00002071
Iteration 47/1000 | Loss: 0.00002071
Iteration 48/1000 | Loss: 0.00002070
Iteration 49/1000 | Loss: 0.00002066
Iteration 50/1000 | Loss: 0.00002066
Iteration 51/1000 | Loss: 0.00002066
Iteration 52/1000 | Loss: 0.00002066
Iteration 53/1000 | Loss: 0.00002066
Iteration 54/1000 | Loss: 0.00002065
Iteration 55/1000 | Loss: 0.00002065
Iteration 56/1000 | Loss: 0.00002065
Iteration 57/1000 | Loss: 0.00002065
Iteration 58/1000 | Loss: 0.00002065
Iteration 59/1000 | Loss: 0.00002065
Iteration 60/1000 | Loss: 0.00002064
Iteration 61/1000 | Loss: 0.00002064
Iteration 62/1000 | Loss: 0.00002064
Iteration 63/1000 | Loss: 0.00002063
Iteration 64/1000 | Loss: 0.00002063
Iteration 65/1000 | Loss: 0.00002062
Iteration 66/1000 | Loss: 0.00002062
Iteration 67/1000 | Loss: 0.00002062
Iteration 68/1000 | Loss: 0.00002062
Iteration 69/1000 | Loss: 0.00002062
Iteration 70/1000 | Loss: 0.00002062
Iteration 71/1000 | Loss: 0.00002061
Iteration 72/1000 | Loss: 0.00002061
Iteration 73/1000 | Loss: 0.00002058
Iteration 74/1000 | Loss: 0.00002056
Iteration 75/1000 | Loss: 0.00002056
Iteration 76/1000 | Loss: 0.00002055
Iteration 77/1000 | Loss: 0.00002055
Iteration 78/1000 | Loss: 0.00002055
Iteration 79/1000 | Loss: 0.00002055
Iteration 80/1000 | Loss: 0.00002055
Iteration 81/1000 | Loss: 0.00002055
Iteration 82/1000 | Loss: 0.00002055
Iteration 83/1000 | Loss: 0.00002055
Iteration 84/1000 | Loss: 0.00002054
Iteration 85/1000 | Loss: 0.00002054
Iteration 86/1000 | Loss: 0.00002054
Iteration 87/1000 | Loss: 0.00002053
Iteration 88/1000 | Loss: 0.00002053
Iteration 89/1000 | Loss: 0.00002053
Iteration 90/1000 | Loss: 0.00002053
Iteration 91/1000 | Loss: 0.00002053
Iteration 92/1000 | Loss: 0.00002053
Iteration 93/1000 | Loss: 0.00002053
Iteration 94/1000 | Loss: 0.00002053
Iteration 95/1000 | Loss: 0.00002053
Iteration 96/1000 | Loss: 0.00002053
Iteration 97/1000 | Loss: 0.00002052
Iteration 98/1000 | Loss: 0.00002052
Iteration 99/1000 | Loss: 0.00002052
Iteration 100/1000 | Loss: 0.00002052
Iteration 101/1000 | Loss: 0.00002052
Iteration 102/1000 | Loss: 0.00002052
Iteration 103/1000 | Loss: 0.00002052
Iteration 104/1000 | Loss: 0.00002052
Iteration 105/1000 | Loss: 0.00002052
Iteration 106/1000 | Loss: 0.00002052
Iteration 107/1000 | Loss: 0.00002052
Iteration 108/1000 | Loss: 0.00002052
Iteration 109/1000 | Loss: 0.00002052
Iteration 110/1000 | Loss: 0.00002051
Iteration 111/1000 | Loss: 0.00002051
Iteration 112/1000 | Loss: 0.00002051
Iteration 113/1000 | Loss: 0.00002051
Iteration 114/1000 | Loss: 0.00002051
Iteration 115/1000 | Loss: 0.00002051
Iteration 116/1000 | Loss: 0.00002050
Iteration 117/1000 | Loss: 0.00002050
Iteration 118/1000 | Loss: 0.00002050
Iteration 119/1000 | Loss: 0.00002050
Iteration 120/1000 | Loss: 0.00002050
Iteration 121/1000 | Loss: 0.00002050
Iteration 122/1000 | Loss: 0.00002050
Iteration 123/1000 | Loss: 0.00002050
Iteration 124/1000 | Loss: 0.00002050
Iteration 125/1000 | Loss: 0.00002050
Iteration 126/1000 | Loss: 0.00002050
Iteration 127/1000 | Loss: 0.00002050
Iteration 128/1000 | Loss: 0.00002050
Iteration 129/1000 | Loss: 0.00002049
Iteration 130/1000 | Loss: 0.00002049
Iteration 131/1000 | Loss: 0.00002049
Iteration 132/1000 | Loss: 0.00002049
Iteration 133/1000 | Loss: 0.00002049
Iteration 134/1000 | Loss: 0.00002049
Iteration 135/1000 | Loss: 0.00002049
Iteration 136/1000 | Loss: 0.00002049
Iteration 137/1000 | Loss: 0.00002049
Iteration 138/1000 | Loss: 0.00002048
Iteration 139/1000 | Loss: 0.00002048
Iteration 140/1000 | Loss: 0.00002048
Iteration 141/1000 | Loss: 0.00002048
Iteration 142/1000 | Loss: 0.00002048
Iteration 143/1000 | Loss: 0.00002048
Iteration 144/1000 | Loss: 0.00002048
Iteration 145/1000 | Loss: 0.00002048
Iteration 146/1000 | Loss: 0.00002048
Iteration 147/1000 | Loss: 0.00002048
Iteration 148/1000 | Loss: 0.00002047
Iteration 149/1000 | Loss: 0.00002047
Iteration 150/1000 | Loss: 0.00002047
Iteration 151/1000 | Loss: 0.00002047
Iteration 152/1000 | Loss: 0.00002047
Iteration 153/1000 | Loss: 0.00002047
Iteration 154/1000 | Loss: 0.00002046
Iteration 155/1000 | Loss: 0.00002046
Iteration 156/1000 | Loss: 0.00002046
Iteration 157/1000 | Loss: 0.00002046
Iteration 158/1000 | Loss: 0.00002046
Iteration 159/1000 | Loss: 0.00002046
Iteration 160/1000 | Loss: 0.00002046
Iteration 161/1000 | Loss: 0.00002046
Iteration 162/1000 | Loss: 0.00002046
Iteration 163/1000 | Loss: 0.00002046
Iteration 164/1000 | Loss: 0.00002046
Iteration 165/1000 | Loss: 0.00002046
Iteration 166/1000 | Loss: 0.00002046
Iteration 167/1000 | Loss: 0.00002046
Iteration 168/1000 | Loss: 0.00002046
Iteration 169/1000 | Loss: 0.00002046
Iteration 170/1000 | Loss: 0.00002046
Iteration 171/1000 | Loss: 0.00002046
Iteration 172/1000 | Loss: 0.00002046
Iteration 173/1000 | Loss: 0.00002046
Iteration 174/1000 | Loss: 0.00002046
Iteration 175/1000 | Loss: 0.00002046
Iteration 176/1000 | Loss: 0.00002046
Iteration 177/1000 | Loss: 0.00002046
Iteration 178/1000 | Loss: 0.00002046
Iteration 179/1000 | Loss: 0.00002046
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 179. Stopping optimization.
Last 5 losses: [2.0458890503505245e-05, 2.0458890503505245e-05, 2.0458890503505245e-05, 2.0458890503505245e-05, 2.0458890503505245e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0458890503505245e-05

Optimization complete. Final v2v error: 3.8144288063049316 mm

Highest mean error: 4.139583587646484 mm for frame 64

Lowest mean error: 3.5896904468536377 mm for frame 124

Saving results

Total time: 39.97195386886597
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_003/1053/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_003/1053.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_003/1053
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00963643
Iteration 2/25 | Loss: 0.00265204
Iteration 3/25 | Loss: 0.00216689
Iteration 4/25 | Loss: 0.00210113
Iteration 5/25 | Loss: 0.00200869
Iteration 6/25 | Loss: 0.00200147
Iteration 7/25 | Loss: 0.00198489
Iteration 8/25 | Loss: 0.00197105
Iteration 9/25 | Loss: 0.00193633
Iteration 10/25 | Loss: 0.00193132
Iteration 11/25 | Loss: 0.00191327
Iteration 12/25 | Loss: 0.00189672
Iteration 13/25 | Loss: 0.00189424
Iteration 14/25 | Loss: 0.00189608
Iteration 15/25 | Loss: 0.00188958
Iteration 16/25 | Loss: 0.00189736
Iteration 17/25 | Loss: 0.00188492
Iteration 18/25 | Loss: 0.00188335
Iteration 19/25 | Loss: 0.00188291
Iteration 20/25 | Loss: 0.00188247
Iteration 21/25 | Loss: 0.00188171
Iteration 22/25 | Loss: 0.00188548
Iteration 23/25 | Loss: 0.00187963
Iteration 24/25 | Loss: 0.00187886
Iteration 25/25 | Loss: 0.00187865

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.30239320
Iteration 2/25 | Loss: 0.00542217
Iteration 3/25 | Loss: 0.00542216
Iteration 4/25 | Loss: 0.00520712
Iteration 5/25 | Loss: 0.00520704
Iteration 6/25 | Loss: 0.00520703
Iteration 7/25 | Loss: 0.00520703
Iteration 8/25 | Loss: 0.00520703
Iteration 9/25 | Loss: 0.00520703
Iteration 10/25 | Loss: 0.00520703
Iteration 11/25 | Loss: 0.00520703
Iteration 12/25 | Loss: 0.00520703
Iteration 13/25 | Loss: 0.00520703
Iteration 14/25 | Loss: 0.00520703
Iteration 15/25 | Loss: 0.00520703
Iteration 16/25 | Loss: 0.00520703
Iteration 17/25 | Loss: 0.00520703
Iteration 18/25 | Loss: 0.00520703
Iteration 19/25 | Loss: 0.00520703
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.00520703149959445, 0.00520703149959445, 0.00520703149959445, 0.00520703149959445, 0.00520703149959445]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00520703149959445

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00520703
Iteration 2/1000 | Loss: 0.00068935
Iteration 3/1000 | Loss: 0.00065604
Iteration 4/1000 | Loss: 0.00076964
Iteration 5/1000 | Loss: 0.00075577
Iteration 6/1000 | Loss: 0.00150619
Iteration 7/1000 | Loss: 0.00299083
Iteration 8/1000 | Loss: 0.00879620
Iteration 9/1000 | Loss: 0.00055094
Iteration 10/1000 | Loss: 0.00117956
Iteration 11/1000 | Loss: 0.00439523
Iteration 12/1000 | Loss: 0.00177592
Iteration 13/1000 | Loss: 0.00074205
Iteration 14/1000 | Loss: 0.00040283
Iteration 15/1000 | Loss: 0.00063206
Iteration 16/1000 | Loss: 0.00040765
Iteration 17/1000 | Loss: 0.00282349
Iteration 18/1000 | Loss: 0.00036121
Iteration 19/1000 | Loss: 0.00107621
Iteration 20/1000 | Loss: 0.00065798
Iteration 21/1000 | Loss: 0.00102113
Iteration 22/1000 | Loss: 0.00031651
Iteration 23/1000 | Loss: 0.00049062
Iteration 24/1000 | Loss: 0.00081157
Iteration 25/1000 | Loss: 0.00467682
Iteration 26/1000 | Loss: 0.04088076
Iteration 27/1000 | Loss: 0.00542835
Iteration 28/1000 | Loss: 0.01183850
Iteration 29/1000 | Loss: 0.00353339
Iteration 30/1000 | Loss: 0.00225369
Iteration 31/1000 | Loss: 0.00044774
Iteration 32/1000 | Loss: 0.00025994
Iteration 33/1000 | Loss: 0.00016449
Iteration 34/1000 | Loss: 0.00045252
Iteration 35/1000 | Loss: 0.00011565
Iteration 36/1000 | Loss: 0.00055166
Iteration 37/1000 | Loss: 0.00006822
Iteration 38/1000 | Loss: 0.00032643
Iteration 39/1000 | Loss: 0.00004561
Iteration 40/1000 | Loss: 0.00003768
Iteration 41/1000 | Loss: 0.00003314
Iteration 42/1000 | Loss: 0.00017087
Iteration 43/1000 | Loss: 0.00002772
Iteration 44/1000 | Loss: 0.00002611
Iteration 45/1000 | Loss: 0.00002361
Iteration 46/1000 | Loss: 0.00023746
Iteration 47/1000 | Loss: 0.00002124
Iteration 48/1000 | Loss: 0.00002009
Iteration 49/1000 | Loss: 0.00025091
Iteration 50/1000 | Loss: 0.00025387
Iteration 51/1000 | Loss: 0.00116353
Iteration 52/1000 | Loss: 0.00064556
Iteration 53/1000 | Loss: 0.00030019
Iteration 54/1000 | Loss: 0.00084909
Iteration 55/1000 | Loss: 0.00013802
Iteration 56/1000 | Loss: 0.00003787
Iteration 57/1000 | Loss: 0.00003487
Iteration 58/1000 | Loss: 0.00001789
Iteration 59/1000 | Loss: 0.00005415
Iteration 60/1000 | Loss: 0.00001787
Iteration 61/1000 | Loss: 0.00029877
Iteration 62/1000 | Loss: 0.00004433
Iteration 63/1000 | Loss: 0.00010081
Iteration 64/1000 | Loss: 0.00013345
Iteration 65/1000 | Loss: 0.00008273
Iteration 66/1000 | Loss: 0.00016741
Iteration 67/1000 | Loss: 0.00017983
Iteration 68/1000 | Loss: 0.00002072
Iteration 69/1000 | Loss: 0.00015913
Iteration 70/1000 | Loss: 0.00001725
Iteration 71/1000 | Loss: 0.00001715
Iteration 72/1000 | Loss: 0.00006279
Iteration 73/1000 | Loss: 0.00001886
Iteration 74/1000 | Loss: 0.00002507
Iteration 75/1000 | Loss: 0.00001701
Iteration 76/1000 | Loss: 0.00001697
Iteration 77/1000 | Loss: 0.00001687
Iteration 78/1000 | Loss: 0.00001686
Iteration 79/1000 | Loss: 0.00001686
Iteration 80/1000 | Loss: 0.00001686
Iteration 81/1000 | Loss: 0.00001686
Iteration 82/1000 | Loss: 0.00001686
Iteration 83/1000 | Loss: 0.00001684
Iteration 84/1000 | Loss: 0.00001684
Iteration 85/1000 | Loss: 0.00001684
Iteration 86/1000 | Loss: 0.00001684
Iteration 87/1000 | Loss: 0.00001684
Iteration 88/1000 | Loss: 0.00001684
Iteration 89/1000 | Loss: 0.00001683
Iteration 90/1000 | Loss: 0.00001683
Iteration 91/1000 | Loss: 0.00001683
Iteration 92/1000 | Loss: 0.00001683
Iteration 93/1000 | Loss: 0.00001681
Iteration 94/1000 | Loss: 0.00001681
Iteration 95/1000 | Loss: 0.00001681
Iteration 96/1000 | Loss: 0.00001679
Iteration 97/1000 | Loss: 0.00001679
Iteration 98/1000 | Loss: 0.00001678
Iteration 99/1000 | Loss: 0.00001678
Iteration 100/1000 | Loss: 0.00001678
Iteration 101/1000 | Loss: 0.00001677
Iteration 102/1000 | Loss: 0.00001676
Iteration 103/1000 | Loss: 0.00001676
Iteration 104/1000 | Loss: 0.00001676
Iteration 105/1000 | Loss: 0.00001676
Iteration 106/1000 | Loss: 0.00001676
Iteration 107/1000 | Loss: 0.00001676
Iteration 108/1000 | Loss: 0.00001676
Iteration 109/1000 | Loss: 0.00001676
Iteration 110/1000 | Loss: 0.00001676
Iteration 111/1000 | Loss: 0.00001675
Iteration 112/1000 | Loss: 0.00001675
Iteration 113/1000 | Loss: 0.00001675
Iteration 114/1000 | Loss: 0.00001674
Iteration 115/1000 | Loss: 0.00015889
Iteration 116/1000 | Loss: 0.00001695
Iteration 117/1000 | Loss: 0.00001676
Iteration 118/1000 | Loss: 0.00001674
Iteration 119/1000 | Loss: 0.00001673
Iteration 120/1000 | Loss: 0.00001673
Iteration 121/1000 | Loss: 0.00001673
Iteration 122/1000 | Loss: 0.00001673
Iteration 123/1000 | Loss: 0.00001672
Iteration 124/1000 | Loss: 0.00001672
Iteration 125/1000 | Loss: 0.00001672
Iteration 126/1000 | Loss: 0.00001672
Iteration 127/1000 | Loss: 0.00001672
Iteration 128/1000 | Loss: 0.00001672
Iteration 129/1000 | Loss: 0.00001672
Iteration 130/1000 | Loss: 0.00001672
Iteration 131/1000 | Loss: 0.00001672
Iteration 132/1000 | Loss: 0.00001672
Iteration 133/1000 | Loss: 0.00001672
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 133. Stopping optimization.
Last 5 losses: [1.6720618077670224e-05, 1.6720618077670224e-05, 1.6720618077670224e-05, 1.6720618077670224e-05, 1.6720618077670224e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6720618077670224e-05

Optimization complete. Final v2v error: 3.5344884395599365 mm

Highest mean error: 3.8263494968414307 mm for frame 0

Lowest mean error: 3.3313040733337402 mm for frame 142

Saving results

Total time: 163.00171947479248
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_003/1037/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_003/1037.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_003/1037
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00826004
Iteration 2/25 | Loss: 0.00131219
Iteration 3/25 | Loss: 0.00121363
Iteration 4/25 | Loss: 0.00120086
Iteration 5/25 | Loss: 0.00119678
Iteration 6/25 | Loss: 0.00119604
Iteration 7/25 | Loss: 0.00119604
Iteration 8/25 | Loss: 0.00119604
Iteration 9/25 | Loss: 0.00119604
Iteration 10/25 | Loss: 0.00119604
Iteration 11/25 | Loss: 0.00119604
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011960436822846532, 0.0011960436822846532, 0.0011960436822846532, 0.0011960436822846532, 0.0011960436822846532]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011960436822846532

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.98952115
Iteration 2/25 | Loss: 0.00125939
Iteration 3/25 | Loss: 0.00125939
Iteration 4/25 | Loss: 0.00125939
Iteration 5/25 | Loss: 0.00125939
Iteration 6/25 | Loss: 0.00125939
Iteration 7/25 | Loss: 0.00125939
Iteration 8/25 | Loss: 0.00125939
Iteration 9/25 | Loss: 0.00125939
Iteration 10/25 | Loss: 0.00125939
Iteration 11/25 | Loss: 0.00125939
Iteration 12/25 | Loss: 0.00125939
Iteration 13/25 | Loss: 0.00125939
Iteration 14/25 | Loss: 0.00125939
Iteration 15/25 | Loss: 0.00125939
Iteration 16/25 | Loss: 0.00125939
Iteration 17/25 | Loss: 0.00125939
Iteration 18/25 | Loss: 0.00125939
Iteration 19/25 | Loss: 0.00125939
Iteration 20/25 | Loss: 0.00125939
Iteration 21/25 | Loss: 0.00125939
Iteration 22/25 | Loss: 0.00125939
Iteration 23/25 | Loss: 0.00125939
Iteration 24/25 | Loss: 0.00125938
Iteration 25/25 | Loss: 0.00125939

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00125938
Iteration 2/1000 | Loss: 0.00001966
Iteration 3/1000 | Loss: 0.00001427
Iteration 4/1000 | Loss: 0.00001305
Iteration 5/1000 | Loss: 0.00001231
Iteration 6/1000 | Loss: 0.00001193
Iteration 7/1000 | Loss: 0.00001167
Iteration 8/1000 | Loss: 0.00001166
Iteration 9/1000 | Loss: 0.00001146
Iteration 10/1000 | Loss: 0.00001125
Iteration 11/1000 | Loss: 0.00001115
Iteration 12/1000 | Loss: 0.00001115
Iteration 13/1000 | Loss: 0.00001113
Iteration 14/1000 | Loss: 0.00001112
Iteration 15/1000 | Loss: 0.00001104
Iteration 16/1000 | Loss: 0.00001100
Iteration 17/1000 | Loss: 0.00001093
Iteration 18/1000 | Loss: 0.00001092
Iteration 19/1000 | Loss: 0.00001088
Iteration 20/1000 | Loss: 0.00001087
Iteration 21/1000 | Loss: 0.00001087
Iteration 22/1000 | Loss: 0.00001087
Iteration 23/1000 | Loss: 0.00001086
Iteration 24/1000 | Loss: 0.00001086
Iteration 25/1000 | Loss: 0.00001086
Iteration 26/1000 | Loss: 0.00001086
Iteration 27/1000 | Loss: 0.00001086
Iteration 28/1000 | Loss: 0.00001084
Iteration 29/1000 | Loss: 0.00001083
Iteration 30/1000 | Loss: 0.00001081
Iteration 31/1000 | Loss: 0.00001081
Iteration 32/1000 | Loss: 0.00001081
Iteration 33/1000 | Loss: 0.00001080
Iteration 34/1000 | Loss: 0.00001080
Iteration 35/1000 | Loss: 0.00001080
Iteration 36/1000 | Loss: 0.00001079
Iteration 37/1000 | Loss: 0.00001079
Iteration 38/1000 | Loss: 0.00001079
Iteration 39/1000 | Loss: 0.00001078
Iteration 40/1000 | Loss: 0.00001077
Iteration 41/1000 | Loss: 0.00001077
Iteration 42/1000 | Loss: 0.00001076
Iteration 43/1000 | Loss: 0.00001076
Iteration 44/1000 | Loss: 0.00001075
Iteration 45/1000 | Loss: 0.00001075
Iteration 46/1000 | Loss: 0.00001073
Iteration 47/1000 | Loss: 0.00001073
Iteration 48/1000 | Loss: 0.00001072
Iteration 49/1000 | Loss: 0.00001072
Iteration 50/1000 | Loss: 0.00001072
Iteration 51/1000 | Loss: 0.00001071
Iteration 52/1000 | Loss: 0.00001071
Iteration 53/1000 | Loss: 0.00001070
Iteration 54/1000 | Loss: 0.00001070
Iteration 55/1000 | Loss: 0.00001067
Iteration 56/1000 | Loss: 0.00001067
Iteration 57/1000 | Loss: 0.00001063
Iteration 58/1000 | Loss: 0.00001062
Iteration 59/1000 | Loss: 0.00001062
Iteration 60/1000 | Loss: 0.00001061
Iteration 61/1000 | Loss: 0.00001061
Iteration 62/1000 | Loss: 0.00001061
Iteration 63/1000 | Loss: 0.00001060
Iteration 64/1000 | Loss: 0.00001060
Iteration 65/1000 | Loss: 0.00001059
Iteration 66/1000 | Loss: 0.00001059
Iteration 67/1000 | Loss: 0.00001059
Iteration 68/1000 | Loss: 0.00001059
Iteration 69/1000 | Loss: 0.00001058
Iteration 70/1000 | Loss: 0.00001058
Iteration 71/1000 | Loss: 0.00001058
Iteration 72/1000 | Loss: 0.00001057
Iteration 73/1000 | Loss: 0.00001057
Iteration 74/1000 | Loss: 0.00001057
Iteration 75/1000 | Loss: 0.00001057
Iteration 76/1000 | Loss: 0.00001056
Iteration 77/1000 | Loss: 0.00001056
Iteration 78/1000 | Loss: 0.00001055
Iteration 79/1000 | Loss: 0.00001055
Iteration 80/1000 | Loss: 0.00001055
Iteration 81/1000 | Loss: 0.00001055
Iteration 82/1000 | Loss: 0.00001054
Iteration 83/1000 | Loss: 0.00001054
Iteration 84/1000 | Loss: 0.00001054
Iteration 85/1000 | Loss: 0.00001054
Iteration 86/1000 | Loss: 0.00001054
Iteration 87/1000 | Loss: 0.00001054
Iteration 88/1000 | Loss: 0.00001053
Iteration 89/1000 | Loss: 0.00001053
Iteration 90/1000 | Loss: 0.00001053
Iteration 91/1000 | Loss: 0.00001052
Iteration 92/1000 | Loss: 0.00001052
Iteration 93/1000 | Loss: 0.00001052
Iteration 94/1000 | Loss: 0.00001052
Iteration 95/1000 | Loss: 0.00001052
Iteration 96/1000 | Loss: 0.00001051
Iteration 97/1000 | Loss: 0.00001051
Iteration 98/1000 | Loss: 0.00001051
Iteration 99/1000 | Loss: 0.00001051
Iteration 100/1000 | Loss: 0.00001051
Iteration 101/1000 | Loss: 0.00001051
Iteration 102/1000 | Loss: 0.00001051
Iteration 103/1000 | Loss: 0.00001050
Iteration 104/1000 | Loss: 0.00001050
Iteration 105/1000 | Loss: 0.00001050
Iteration 106/1000 | Loss: 0.00001050
Iteration 107/1000 | Loss: 0.00001050
Iteration 108/1000 | Loss: 0.00001050
Iteration 109/1000 | Loss: 0.00001049
Iteration 110/1000 | Loss: 0.00001049
Iteration 111/1000 | Loss: 0.00001049
Iteration 112/1000 | Loss: 0.00001049
Iteration 113/1000 | Loss: 0.00001049
Iteration 114/1000 | Loss: 0.00001049
Iteration 115/1000 | Loss: 0.00001048
Iteration 116/1000 | Loss: 0.00001048
Iteration 117/1000 | Loss: 0.00001048
Iteration 118/1000 | Loss: 0.00001048
Iteration 119/1000 | Loss: 0.00001048
Iteration 120/1000 | Loss: 0.00001048
Iteration 121/1000 | Loss: 0.00001048
Iteration 122/1000 | Loss: 0.00001048
Iteration 123/1000 | Loss: 0.00001048
Iteration 124/1000 | Loss: 0.00001047
Iteration 125/1000 | Loss: 0.00001047
Iteration 126/1000 | Loss: 0.00001047
Iteration 127/1000 | Loss: 0.00001047
Iteration 128/1000 | Loss: 0.00001047
Iteration 129/1000 | Loss: 0.00001047
Iteration 130/1000 | Loss: 0.00001047
Iteration 131/1000 | Loss: 0.00001046
Iteration 132/1000 | Loss: 0.00001046
Iteration 133/1000 | Loss: 0.00001046
Iteration 134/1000 | Loss: 0.00001046
Iteration 135/1000 | Loss: 0.00001046
Iteration 136/1000 | Loss: 0.00001046
Iteration 137/1000 | Loss: 0.00001046
Iteration 138/1000 | Loss: 0.00001046
Iteration 139/1000 | Loss: 0.00001046
Iteration 140/1000 | Loss: 0.00001046
Iteration 141/1000 | Loss: 0.00001046
Iteration 142/1000 | Loss: 0.00001046
Iteration 143/1000 | Loss: 0.00001046
Iteration 144/1000 | Loss: 0.00001046
Iteration 145/1000 | Loss: 0.00001046
Iteration 146/1000 | Loss: 0.00001046
Iteration 147/1000 | Loss: 0.00001046
Iteration 148/1000 | Loss: 0.00001046
Iteration 149/1000 | Loss: 0.00001045
Iteration 150/1000 | Loss: 0.00001045
Iteration 151/1000 | Loss: 0.00001045
Iteration 152/1000 | Loss: 0.00001045
Iteration 153/1000 | Loss: 0.00001045
Iteration 154/1000 | Loss: 0.00001045
Iteration 155/1000 | Loss: 0.00001045
Iteration 156/1000 | Loss: 0.00001045
Iteration 157/1000 | Loss: 0.00001045
Iteration 158/1000 | Loss: 0.00001045
Iteration 159/1000 | Loss: 0.00001045
Iteration 160/1000 | Loss: 0.00001045
Iteration 161/1000 | Loss: 0.00001045
Iteration 162/1000 | Loss: 0.00001045
Iteration 163/1000 | Loss: 0.00001045
Iteration 164/1000 | Loss: 0.00001044
Iteration 165/1000 | Loss: 0.00001044
Iteration 166/1000 | Loss: 0.00001044
Iteration 167/1000 | Loss: 0.00001044
Iteration 168/1000 | Loss: 0.00001044
Iteration 169/1000 | Loss: 0.00001044
Iteration 170/1000 | Loss: 0.00001044
Iteration 171/1000 | Loss: 0.00001044
Iteration 172/1000 | Loss: 0.00001044
Iteration 173/1000 | Loss: 0.00001044
Iteration 174/1000 | Loss: 0.00001044
Iteration 175/1000 | Loss: 0.00001044
Iteration 176/1000 | Loss: 0.00001044
Iteration 177/1000 | Loss: 0.00001044
Iteration 178/1000 | Loss: 0.00001044
Iteration 179/1000 | Loss: 0.00001044
Iteration 180/1000 | Loss: 0.00001044
Iteration 181/1000 | Loss: 0.00001044
Iteration 182/1000 | Loss: 0.00001044
Iteration 183/1000 | Loss: 0.00001044
Iteration 184/1000 | Loss: 0.00001044
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 184. Stopping optimization.
Last 5 losses: [1.0436509001010563e-05, 1.0436509001010563e-05, 1.0436509001010563e-05, 1.0436509001010563e-05, 1.0436509001010563e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0436509001010563e-05

Optimization complete. Final v2v error: 2.7275257110595703 mm

Highest mean error: 3.583209753036499 mm for frame 91

Lowest mean error: 2.4587454795837402 mm for frame 128

Saving results

Total time: 40.22614884376526
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_003/1051/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_003/1051.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_003/1051
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00780805
Iteration 2/25 | Loss: 0.00133924
Iteration 3/25 | Loss: 0.00120590
Iteration 4/25 | Loss: 0.00119442
Iteration 5/25 | Loss: 0.00119222
Iteration 6/25 | Loss: 0.00119222
Iteration 7/25 | Loss: 0.00119222
Iteration 8/25 | Loss: 0.00119222
Iteration 9/25 | Loss: 0.00119222
Iteration 10/25 | Loss: 0.00119222
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0011922175763174891, 0.0011922175763174891, 0.0011922175763174891, 0.0011922175763174891, 0.0011922175763174891]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011922175763174891

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32078934
Iteration 2/25 | Loss: 0.00119851
Iteration 3/25 | Loss: 0.00119851
Iteration 4/25 | Loss: 0.00119851
Iteration 5/25 | Loss: 0.00119850
Iteration 6/25 | Loss: 0.00119850
Iteration 7/25 | Loss: 0.00119850
Iteration 8/25 | Loss: 0.00119850
Iteration 9/25 | Loss: 0.00119850
Iteration 10/25 | Loss: 0.00119850
Iteration 11/25 | Loss: 0.00119850
Iteration 12/25 | Loss: 0.00119850
Iteration 13/25 | Loss: 0.00119850
Iteration 14/25 | Loss: 0.00119850
Iteration 15/25 | Loss: 0.00119850
Iteration 16/25 | Loss: 0.00119850
Iteration 17/25 | Loss: 0.00119850
Iteration 18/25 | Loss: 0.00119850
Iteration 19/25 | Loss: 0.00119850
Iteration 20/25 | Loss: 0.00119850
Iteration 21/25 | Loss: 0.00119850
Iteration 22/25 | Loss: 0.00119850
Iteration 23/25 | Loss: 0.00119850
Iteration 24/25 | Loss: 0.00119850
Iteration 25/25 | Loss: 0.00119850

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00119850
Iteration 2/1000 | Loss: 0.00002502
Iteration 3/1000 | Loss: 0.00001680
Iteration 4/1000 | Loss: 0.00001410
Iteration 5/1000 | Loss: 0.00001295
Iteration 6/1000 | Loss: 0.00001232
Iteration 7/1000 | Loss: 0.00001186
Iteration 8/1000 | Loss: 0.00001157
Iteration 9/1000 | Loss: 0.00001150
Iteration 10/1000 | Loss: 0.00001120
Iteration 11/1000 | Loss: 0.00001107
Iteration 12/1000 | Loss: 0.00001105
Iteration 13/1000 | Loss: 0.00001104
Iteration 14/1000 | Loss: 0.00001103
Iteration 15/1000 | Loss: 0.00001100
Iteration 16/1000 | Loss: 0.00001089
Iteration 17/1000 | Loss: 0.00001085
Iteration 18/1000 | Loss: 0.00001078
Iteration 19/1000 | Loss: 0.00001074
Iteration 20/1000 | Loss: 0.00001073
Iteration 21/1000 | Loss: 0.00001072
Iteration 22/1000 | Loss: 0.00001071
Iteration 23/1000 | Loss: 0.00001071
Iteration 24/1000 | Loss: 0.00001070
Iteration 25/1000 | Loss: 0.00001069
Iteration 26/1000 | Loss: 0.00001069
Iteration 27/1000 | Loss: 0.00001069
Iteration 28/1000 | Loss: 0.00001069
Iteration 29/1000 | Loss: 0.00001068
Iteration 30/1000 | Loss: 0.00001067
Iteration 31/1000 | Loss: 0.00001064
Iteration 32/1000 | Loss: 0.00001064
Iteration 33/1000 | Loss: 0.00001063
Iteration 34/1000 | Loss: 0.00001062
Iteration 35/1000 | Loss: 0.00001061
Iteration 36/1000 | Loss: 0.00001061
Iteration 37/1000 | Loss: 0.00001061
Iteration 38/1000 | Loss: 0.00001060
Iteration 39/1000 | Loss: 0.00001060
Iteration 40/1000 | Loss: 0.00001060
Iteration 41/1000 | Loss: 0.00001059
Iteration 42/1000 | Loss: 0.00001059
Iteration 43/1000 | Loss: 0.00001059
Iteration 44/1000 | Loss: 0.00001059
Iteration 45/1000 | Loss: 0.00001058
Iteration 46/1000 | Loss: 0.00001057
Iteration 47/1000 | Loss: 0.00001054
Iteration 48/1000 | Loss: 0.00001054
Iteration 49/1000 | Loss: 0.00001053
Iteration 50/1000 | Loss: 0.00001053
Iteration 51/1000 | Loss: 0.00001053
Iteration 52/1000 | Loss: 0.00001052
Iteration 53/1000 | Loss: 0.00001052
Iteration 54/1000 | Loss: 0.00001050
Iteration 55/1000 | Loss: 0.00001049
Iteration 56/1000 | Loss: 0.00001049
Iteration 57/1000 | Loss: 0.00001049
Iteration 58/1000 | Loss: 0.00001049
Iteration 59/1000 | Loss: 0.00001049
Iteration 60/1000 | Loss: 0.00001049
Iteration 61/1000 | Loss: 0.00001049
Iteration 62/1000 | Loss: 0.00001049
Iteration 63/1000 | Loss: 0.00001049
Iteration 64/1000 | Loss: 0.00001048
Iteration 65/1000 | Loss: 0.00001047
Iteration 66/1000 | Loss: 0.00001047
Iteration 67/1000 | Loss: 0.00001046
Iteration 68/1000 | Loss: 0.00001046
Iteration 69/1000 | Loss: 0.00001046
Iteration 70/1000 | Loss: 0.00001046
Iteration 71/1000 | Loss: 0.00001046
Iteration 72/1000 | Loss: 0.00001046
Iteration 73/1000 | Loss: 0.00001046
Iteration 74/1000 | Loss: 0.00001046
Iteration 75/1000 | Loss: 0.00001045
Iteration 76/1000 | Loss: 0.00001045
Iteration 77/1000 | Loss: 0.00001045
Iteration 78/1000 | Loss: 0.00001044
Iteration 79/1000 | Loss: 0.00001044
Iteration 80/1000 | Loss: 0.00001044
Iteration 81/1000 | Loss: 0.00001043
Iteration 82/1000 | Loss: 0.00001043
Iteration 83/1000 | Loss: 0.00001043
Iteration 84/1000 | Loss: 0.00001042
Iteration 85/1000 | Loss: 0.00001042
Iteration 86/1000 | Loss: 0.00001042
Iteration 87/1000 | Loss: 0.00001042
Iteration 88/1000 | Loss: 0.00001041
Iteration 89/1000 | Loss: 0.00001041
Iteration 90/1000 | Loss: 0.00001041
Iteration 91/1000 | Loss: 0.00001041
Iteration 92/1000 | Loss: 0.00001041
Iteration 93/1000 | Loss: 0.00001041
Iteration 94/1000 | Loss: 0.00001041
Iteration 95/1000 | Loss: 0.00001041
Iteration 96/1000 | Loss: 0.00001041
Iteration 97/1000 | Loss: 0.00001040
Iteration 98/1000 | Loss: 0.00001040
Iteration 99/1000 | Loss: 0.00001040
Iteration 100/1000 | Loss: 0.00001040
Iteration 101/1000 | Loss: 0.00001040
Iteration 102/1000 | Loss: 0.00001040
Iteration 103/1000 | Loss: 0.00001039
Iteration 104/1000 | Loss: 0.00001039
Iteration 105/1000 | Loss: 0.00001039
Iteration 106/1000 | Loss: 0.00001039
Iteration 107/1000 | Loss: 0.00001039
Iteration 108/1000 | Loss: 0.00001039
Iteration 109/1000 | Loss: 0.00001039
Iteration 110/1000 | Loss: 0.00001039
Iteration 111/1000 | Loss: 0.00001039
Iteration 112/1000 | Loss: 0.00001039
Iteration 113/1000 | Loss: 0.00001039
Iteration 114/1000 | Loss: 0.00001039
Iteration 115/1000 | Loss: 0.00001039
Iteration 116/1000 | Loss: 0.00001039
Iteration 117/1000 | Loss: 0.00001039
Iteration 118/1000 | Loss: 0.00001039
Iteration 119/1000 | Loss: 0.00001039
Iteration 120/1000 | Loss: 0.00001039
Iteration 121/1000 | Loss: 0.00001039
Iteration 122/1000 | Loss: 0.00001039
Iteration 123/1000 | Loss: 0.00001039
Iteration 124/1000 | Loss: 0.00001039
Iteration 125/1000 | Loss: 0.00001039
Iteration 126/1000 | Loss: 0.00001039
Iteration 127/1000 | Loss: 0.00001039
Iteration 128/1000 | Loss: 0.00001039
Iteration 129/1000 | Loss: 0.00001039
Iteration 130/1000 | Loss: 0.00001039
Iteration 131/1000 | Loss: 0.00001039
Iteration 132/1000 | Loss: 0.00001039
Iteration 133/1000 | Loss: 0.00001039
Iteration 134/1000 | Loss: 0.00001039
Iteration 135/1000 | Loss: 0.00001039
Iteration 136/1000 | Loss: 0.00001039
Iteration 137/1000 | Loss: 0.00001039
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 137. Stopping optimization.
Last 5 losses: [1.0390477655164432e-05, 1.0390477655164432e-05, 1.0390477655164432e-05, 1.0390477655164432e-05, 1.0390477655164432e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0390477655164432e-05

Optimization complete. Final v2v error: 2.745591163635254 mm

Highest mean error: 3.0422537326812744 mm for frame 84

Lowest mean error: 2.551018714904785 mm for frame 35

Saving results

Total time: 36.40616154670715
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_37_nl_5760/0023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_nl_5760/0023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_nl_5760/0023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01033328
Iteration 2/25 | Loss: 0.00422512
Iteration 3/25 | Loss: 0.00321742
Iteration 4/25 | Loss: 0.00276544
Iteration 5/25 | Loss: 0.00278296
Iteration 6/25 | Loss: 0.00262328
Iteration 7/25 | Loss: 0.00234141
Iteration 8/25 | Loss: 0.00222678
Iteration 9/25 | Loss: 0.00201854
Iteration 10/25 | Loss: 0.00197484
Iteration 11/25 | Loss: 0.00184667
Iteration 12/25 | Loss: 0.00181569
Iteration 13/25 | Loss: 0.00177675
Iteration 14/25 | Loss: 0.00175613
Iteration 15/25 | Loss: 0.00173918
Iteration 16/25 | Loss: 0.00172860
Iteration 17/25 | Loss: 0.00173153
Iteration 18/25 | Loss: 0.00171813
Iteration 19/25 | Loss: 0.00171418
Iteration 20/25 | Loss: 0.00172862
Iteration 21/25 | Loss: 0.00170293
Iteration 22/25 | Loss: 0.00170059
Iteration 23/25 | Loss: 0.00169861
Iteration 24/25 | Loss: 0.00169723
Iteration 25/25 | Loss: 0.00169487

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.93761194
Iteration 2/25 | Loss: 0.01113516
Iteration 3/25 | Loss: 0.00595633
Iteration 4/25 | Loss: 0.00595633
Iteration 5/25 | Loss: 0.00595633
Iteration 6/25 | Loss: 0.00595633
Iteration 7/25 | Loss: 0.00595633
Iteration 8/25 | Loss: 0.00595633
Iteration 9/25 | Loss: 0.00595633
Iteration 10/25 | Loss: 0.00595633
Iteration 11/25 | Loss: 0.00595633
Iteration 12/25 | Loss: 0.00595633
Iteration 13/25 | Loss: 0.00595633
Iteration 14/25 | Loss: 0.00595633
Iteration 15/25 | Loss: 0.00595633
Iteration 16/25 | Loss: 0.00595633
Iteration 17/25 | Loss: 0.00595633
Iteration 18/25 | Loss: 0.00595633
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0059563289396464825, 0.0059563289396464825, 0.0059563289396464825, 0.0059563289396464825, 0.0059563289396464825]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0059563289396464825

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00595633
Iteration 2/1000 | Loss: 0.00084892
Iteration 3/1000 | Loss: 0.00070688
Iteration 4/1000 | Loss: 0.00126030
Iteration 5/1000 | Loss: 0.00185634
Iteration 6/1000 | Loss: 0.00237434
Iteration 7/1000 | Loss: 0.00107006
Iteration 8/1000 | Loss: 0.00108965
Iteration 9/1000 | Loss: 0.00167973
Iteration 10/1000 | Loss: 0.00053462
Iteration 11/1000 | Loss: 0.00160282
Iteration 12/1000 | Loss: 0.00059738
Iteration 13/1000 | Loss: 0.00177271
Iteration 14/1000 | Loss: 0.00184663
Iteration 15/1000 | Loss: 0.00125275
Iteration 16/1000 | Loss: 0.00194168
Iteration 17/1000 | Loss: 0.00111484
Iteration 18/1000 | Loss: 0.00064594
Iteration 19/1000 | Loss: 0.00182324
Iteration 20/1000 | Loss: 0.00282561
Iteration 21/1000 | Loss: 0.00114098
Iteration 22/1000 | Loss: 0.00156673
Iteration 23/1000 | Loss: 0.00077781
Iteration 24/1000 | Loss: 0.00076698
Iteration 25/1000 | Loss: 0.00019649
Iteration 26/1000 | Loss: 0.00018529
Iteration 27/1000 | Loss: 0.00017232
Iteration 28/1000 | Loss: 0.00016390
Iteration 29/1000 | Loss: 0.00061132
Iteration 30/1000 | Loss: 0.00107771
Iteration 31/1000 | Loss: 0.00015187
Iteration 32/1000 | Loss: 0.00128793
Iteration 33/1000 | Loss: 0.00188675
Iteration 34/1000 | Loss: 0.00087172
Iteration 35/1000 | Loss: 0.00049980
Iteration 36/1000 | Loss: 0.00017793
Iteration 37/1000 | Loss: 0.00015735
Iteration 38/1000 | Loss: 0.00024867
Iteration 39/1000 | Loss: 0.00023117
Iteration 40/1000 | Loss: 0.00020891
Iteration 41/1000 | Loss: 0.00027925
Iteration 42/1000 | Loss: 0.00027780
Iteration 43/1000 | Loss: 0.00102152
Iteration 44/1000 | Loss: 0.00027473
Iteration 45/1000 | Loss: 0.00111528
Iteration 46/1000 | Loss: 0.00045272
Iteration 47/1000 | Loss: 0.00028518
Iteration 48/1000 | Loss: 0.00023024
Iteration 49/1000 | Loss: 0.00026994
Iteration 50/1000 | Loss: 0.00020751
Iteration 51/1000 | Loss: 0.00026155
Iteration 52/1000 | Loss: 0.00029359
Iteration 53/1000 | Loss: 0.00028416
Iteration 54/1000 | Loss: 0.00146868
Iteration 55/1000 | Loss: 0.00027993
Iteration 56/1000 | Loss: 0.00028558
Iteration 57/1000 | Loss: 0.00026413
Iteration 58/1000 | Loss: 0.00022821
Iteration 59/1000 | Loss: 0.00022550
Iteration 60/1000 | Loss: 0.00027272
Iteration 61/1000 | Loss: 0.00028207
Iteration 62/1000 | Loss: 0.00027063
Iteration 63/1000 | Loss: 0.00028287
Iteration 64/1000 | Loss: 0.00029280
Iteration 65/1000 | Loss: 0.00083071
Iteration 66/1000 | Loss: 0.00652113
Iteration 67/1000 | Loss: 0.00944687
Iteration 68/1000 | Loss: 0.00237093
Iteration 69/1000 | Loss: 0.00134846
Iteration 70/1000 | Loss: 0.00130805
Iteration 71/1000 | Loss: 0.00088770
Iteration 72/1000 | Loss: 0.00022720
Iteration 73/1000 | Loss: 0.00028791
Iteration 74/1000 | Loss: 0.00139939
Iteration 75/1000 | Loss: 0.00175667
Iteration 76/1000 | Loss: 0.00034786
Iteration 77/1000 | Loss: 0.00113177
Iteration 78/1000 | Loss: 0.00367609
Iteration 79/1000 | Loss: 0.00121750
Iteration 80/1000 | Loss: 0.00090496
Iteration 81/1000 | Loss: 0.00097467
Iteration 82/1000 | Loss: 0.00284522
Iteration 83/1000 | Loss: 0.00032327
Iteration 84/1000 | Loss: 0.00013154
Iteration 85/1000 | Loss: 0.00011178
Iteration 86/1000 | Loss: 0.00011252
Iteration 87/1000 | Loss: 0.00028165
Iteration 88/1000 | Loss: 0.00021736
Iteration 89/1000 | Loss: 0.00023346
Iteration 90/1000 | Loss: 0.00010015
Iteration 91/1000 | Loss: 0.00009764
Iteration 92/1000 | Loss: 0.00009812
Iteration 93/1000 | Loss: 0.00009850
Iteration 94/1000 | Loss: 0.00010100
Iteration 95/1000 | Loss: 0.00009672
Iteration 96/1000 | Loss: 0.00009303
Iteration 97/1000 | Loss: 0.00009364
Iteration 98/1000 | Loss: 0.00008693
Iteration 99/1000 | Loss: 0.00008861
Iteration 100/1000 | Loss: 0.00008327
Iteration 101/1000 | Loss: 0.00008779
Iteration 102/1000 | Loss: 0.00009736
Iteration 103/1000 | Loss: 0.00008761
Iteration 104/1000 | Loss: 0.00155363
Iteration 105/1000 | Loss: 0.00214147
Iteration 106/1000 | Loss: 0.00251816
Iteration 107/1000 | Loss: 0.00209797
Iteration 108/1000 | Loss: 0.00126942
Iteration 109/1000 | Loss: 0.00012027
Iteration 110/1000 | Loss: 0.00009587
Iteration 111/1000 | Loss: 0.00074106
Iteration 112/1000 | Loss: 0.00034828
Iteration 113/1000 | Loss: 0.00009384
Iteration 114/1000 | Loss: 0.00008575
Iteration 115/1000 | Loss: 0.00008695
Iteration 116/1000 | Loss: 0.00008394
Iteration 117/1000 | Loss: 0.00008450
Iteration 118/1000 | Loss: 0.00008473
Iteration 119/1000 | Loss: 0.00009436
Iteration 120/1000 | Loss: 0.00008496
Iteration 121/1000 | Loss: 0.00008088
Iteration 122/1000 | Loss: 0.00007358
Iteration 123/1000 | Loss: 0.00007663
Iteration 124/1000 | Loss: 0.00007090
Iteration 125/1000 | Loss: 0.00008305
Iteration 126/1000 | Loss: 0.00008682
Iteration 127/1000 | Loss: 0.00007799
Iteration 128/1000 | Loss: 0.00007302
Iteration 129/1000 | Loss: 0.00008316
Iteration 130/1000 | Loss: 0.00008209
Iteration 131/1000 | Loss: 0.00008506
Iteration 132/1000 | Loss: 0.00008265
Iteration 133/1000 | Loss: 0.00008410
Iteration 134/1000 | Loss: 0.00008325
Iteration 135/1000 | Loss: 0.00006607
Iteration 136/1000 | Loss: 0.00075639
Iteration 137/1000 | Loss: 0.00048737
Iteration 138/1000 | Loss: 0.00030426
Iteration 139/1000 | Loss: 0.00028331
Iteration 140/1000 | Loss: 0.00023963
Iteration 141/1000 | Loss: 0.00122683
Iteration 142/1000 | Loss: 0.00007619
Iteration 143/1000 | Loss: 0.00006747
Iteration 144/1000 | Loss: 0.00008005
Iteration 145/1000 | Loss: 0.00007147
Iteration 146/1000 | Loss: 0.00020159
Iteration 147/1000 | Loss: 0.00008300
Iteration 148/1000 | Loss: 0.00009165
Iteration 149/1000 | Loss: 0.00008044
Iteration 150/1000 | Loss: 0.00007716
Iteration 151/1000 | Loss: 0.00007889
Iteration 152/1000 | Loss: 0.00007370
Iteration 153/1000 | Loss: 0.00006215
Iteration 154/1000 | Loss: 0.00005925
Iteration 155/1000 | Loss: 0.00007322
Iteration 156/1000 | Loss: 0.00007659
Iteration 157/1000 | Loss: 0.00006936
Iteration 158/1000 | Loss: 0.00007663
Iteration 159/1000 | Loss: 0.00006735
Iteration 160/1000 | Loss: 0.00006910
Iteration 161/1000 | Loss: 0.00007323
Iteration 162/1000 | Loss: 0.00007336
Iteration 163/1000 | Loss: 0.00006651
Iteration 164/1000 | Loss: 0.00007131
Iteration 165/1000 | Loss: 0.00007076
Iteration 166/1000 | Loss: 0.00007227
Iteration 167/1000 | Loss: 0.00006960
Iteration 168/1000 | Loss: 0.00007527
Iteration 169/1000 | Loss: 0.00006991
Iteration 170/1000 | Loss: 0.00007165
Iteration 171/1000 | Loss: 0.00007337
Iteration 172/1000 | Loss: 0.00007235
Iteration 173/1000 | Loss: 0.00079452
Iteration 174/1000 | Loss: 0.00057733
Iteration 175/1000 | Loss: 0.00006304
Iteration 176/1000 | Loss: 0.00007585
Iteration 177/1000 | Loss: 0.00073808
Iteration 178/1000 | Loss: 0.00055243
Iteration 179/1000 | Loss: 0.00152002
Iteration 180/1000 | Loss: 0.00074443
Iteration 181/1000 | Loss: 0.00080810
Iteration 182/1000 | Loss: 0.00083133
Iteration 183/1000 | Loss: 0.00041394
Iteration 184/1000 | Loss: 0.00051557
Iteration 185/1000 | Loss: 0.00010934
Iteration 186/1000 | Loss: 0.00006055
Iteration 187/1000 | Loss: 0.00005820
Iteration 188/1000 | Loss: 0.00005625
Iteration 189/1000 | Loss: 0.00005423
Iteration 190/1000 | Loss: 0.00005327
Iteration 191/1000 | Loss: 0.00005260
Iteration 192/1000 | Loss: 0.00005219
Iteration 193/1000 | Loss: 0.00005191
Iteration 194/1000 | Loss: 0.00005160
Iteration 195/1000 | Loss: 0.00053286
Iteration 196/1000 | Loss: 0.00060457
Iteration 197/1000 | Loss: 0.00094541
Iteration 198/1000 | Loss: 0.00021534
Iteration 199/1000 | Loss: 0.00152020
Iteration 200/1000 | Loss: 0.00015081
Iteration 201/1000 | Loss: 0.00041423
Iteration 202/1000 | Loss: 0.00012385
Iteration 203/1000 | Loss: 0.00038112
Iteration 204/1000 | Loss: 0.00025738
Iteration 205/1000 | Loss: 0.00015607
Iteration 206/1000 | Loss: 0.00022127
Iteration 207/1000 | Loss: 0.00040833
Iteration 208/1000 | Loss: 0.00011364
Iteration 209/1000 | Loss: 0.00005459
Iteration 210/1000 | Loss: 0.00005378
Iteration 211/1000 | Loss: 0.00018533
Iteration 212/1000 | Loss: 0.00013702
Iteration 213/1000 | Loss: 0.00089524
Iteration 214/1000 | Loss: 0.00097226
Iteration 215/1000 | Loss: 0.00147180
Iteration 216/1000 | Loss: 0.00021086
Iteration 217/1000 | Loss: 0.00007955
Iteration 218/1000 | Loss: 0.00109531
Iteration 219/1000 | Loss: 0.00020378
Iteration 220/1000 | Loss: 0.00009279
Iteration 221/1000 | Loss: 0.00006184
Iteration 222/1000 | Loss: 0.00024094
Iteration 223/1000 | Loss: 0.00030580
Iteration 224/1000 | Loss: 0.00078805
Iteration 225/1000 | Loss: 0.00024411
Iteration 226/1000 | Loss: 0.00008062
Iteration 227/1000 | Loss: 0.00016238
Iteration 228/1000 | Loss: 0.00097252
Iteration 229/1000 | Loss: 0.00016645
Iteration 230/1000 | Loss: 0.00018442
Iteration 231/1000 | Loss: 0.00020836
Iteration 232/1000 | Loss: 0.00017047
Iteration 233/1000 | Loss: 0.00027049
Iteration 234/1000 | Loss: 0.00029006
Iteration 235/1000 | Loss: 0.00032220
Iteration 236/1000 | Loss: 0.00034677
Iteration 237/1000 | Loss: 0.00023064
Iteration 238/1000 | Loss: 0.00042292
Iteration 239/1000 | Loss: 0.00039801
Iteration 240/1000 | Loss: 0.00019066
Iteration 241/1000 | Loss: 0.00034359
Iteration 242/1000 | Loss: 0.00024621
Iteration 243/1000 | Loss: 0.00011939
Iteration 244/1000 | Loss: 0.00010197
Iteration 245/1000 | Loss: 0.00031638
Iteration 246/1000 | Loss: 0.00024330
Iteration 247/1000 | Loss: 0.00009792
Iteration 248/1000 | Loss: 0.00024141
Iteration 249/1000 | Loss: 0.00016998
Iteration 250/1000 | Loss: 0.00027699
Iteration 251/1000 | Loss: 0.00018601
Iteration 252/1000 | Loss: 0.00044018
Iteration 253/1000 | Loss: 0.00023326
Iteration 254/1000 | Loss: 0.00034195
Iteration 255/1000 | Loss: 0.00071057
Iteration 256/1000 | Loss: 0.00045237
Iteration 257/1000 | Loss: 0.00029539
Iteration 258/1000 | Loss: 0.00082834
Iteration 259/1000 | Loss: 0.00012682
Iteration 260/1000 | Loss: 0.00007237
Iteration 261/1000 | Loss: 0.00010360
Iteration 262/1000 | Loss: 0.00011603
Iteration 263/1000 | Loss: 0.00029965
Iteration 264/1000 | Loss: 0.00020750
Iteration 265/1000 | Loss: 0.00022266
Iteration 266/1000 | Loss: 0.00082980
Iteration 267/1000 | Loss: 0.00014076
Iteration 268/1000 | Loss: 0.00005622
Iteration 269/1000 | Loss: 0.00005376
Iteration 270/1000 | Loss: 0.00005226
Iteration 271/1000 | Loss: 0.00026468
Iteration 272/1000 | Loss: 0.00045658
Iteration 273/1000 | Loss: 0.00024506
Iteration 274/1000 | Loss: 0.00014875
Iteration 275/1000 | Loss: 0.00035307
Iteration 276/1000 | Loss: 0.00037335
Iteration 277/1000 | Loss: 0.00011872
Iteration 278/1000 | Loss: 0.00005751
Iteration 279/1000 | Loss: 0.00005438
Iteration 280/1000 | Loss: 0.00005214
Iteration 281/1000 | Loss: 0.00020176
Iteration 282/1000 | Loss: 0.00016731
Iteration 283/1000 | Loss: 0.00005041
Iteration 284/1000 | Loss: 0.00004936
Iteration 285/1000 | Loss: 0.00034053
Iteration 286/1000 | Loss: 0.00012378
Iteration 287/1000 | Loss: 0.00015445
Iteration 288/1000 | Loss: 0.00005737
Iteration 289/1000 | Loss: 0.00006768
Iteration 290/1000 | Loss: 0.00017052
Iteration 291/1000 | Loss: 0.00008412
Iteration 292/1000 | Loss: 0.00012123
Iteration 293/1000 | Loss: 0.00012508
Iteration 294/1000 | Loss: 0.00006813
Iteration 295/1000 | Loss: 0.00017753
Iteration 296/1000 | Loss: 0.00014968
Iteration 297/1000 | Loss: 0.00014639
Iteration 298/1000 | Loss: 0.00012796
Iteration 299/1000 | Loss: 0.00027923
Iteration 300/1000 | Loss: 0.00116174
Iteration 301/1000 | Loss: 0.00016163
Iteration 302/1000 | Loss: 0.00039073
Iteration 303/1000 | Loss: 0.00019064
Iteration 304/1000 | Loss: 0.00007156
Iteration 305/1000 | Loss: 0.00018364
Iteration 306/1000 | Loss: 0.00006708
Iteration 307/1000 | Loss: 0.00016875
Iteration 308/1000 | Loss: 0.00013587
Iteration 309/1000 | Loss: 0.00012270
Iteration 310/1000 | Loss: 0.00013595
Iteration 311/1000 | Loss: 0.00012397
Iteration 312/1000 | Loss: 0.00013725
Iteration 313/1000 | Loss: 0.00011065
Iteration 314/1000 | Loss: 0.00022363
Iteration 315/1000 | Loss: 0.00078719
Iteration 316/1000 | Loss: 0.00020679
Iteration 317/1000 | Loss: 0.00132310
Iteration 318/1000 | Loss: 0.00017614
Iteration 319/1000 | Loss: 0.00011659
Iteration 320/1000 | Loss: 0.00029115
Iteration 321/1000 | Loss: 0.00008388
Iteration 322/1000 | Loss: 0.00138224
Iteration 323/1000 | Loss: 0.00031547
Iteration 324/1000 | Loss: 0.00020784
Iteration 325/1000 | Loss: 0.00009581
Iteration 326/1000 | Loss: 0.00007743
Iteration 327/1000 | Loss: 0.00015496
Iteration 328/1000 | Loss: 0.00028411
Iteration 329/1000 | Loss: 0.00025003
Iteration 330/1000 | Loss: 0.00016804
Iteration 331/1000 | Loss: 0.00005978
Iteration 332/1000 | Loss: 0.00054100
Iteration 333/1000 | Loss: 0.00041490
Iteration 334/1000 | Loss: 0.00018810
Iteration 335/1000 | Loss: 0.00014475
Iteration 336/1000 | Loss: 0.00067427
Iteration 337/1000 | Loss: 0.00068809
Iteration 338/1000 | Loss: 0.00027359
Iteration 339/1000 | Loss: 0.00016599
Iteration 340/1000 | Loss: 0.00047047
Iteration 341/1000 | Loss: 0.00006784
Iteration 342/1000 | Loss: 0.00009390
Iteration 343/1000 | Loss: 0.00005478
Iteration 344/1000 | Loss: 0.00005244
Iteration 345/1000 | Loss: 0.00005129
Iteration 346/1000 | Loss: 0.00005065
Iteration 347/1000 | Loss: 0.00005000
Iteration 348/1000 | Loss: 0.00025922
Iteration 349/1000 | Loss: 0.00011861
Iteration 350/1000 | Loss: 0.00005189
Iteration 351/1000 | Loss: 0.00030254
Iteration 352/1000 | Loss: 0.00014749
Iteration 353/1000 | Loss: 0.00024585
Iteration 354/1000 | Loss: 0.00012679
Iteration 355/1000 | Loss: 0.00023609
Iteration 356/1000 | Loss: 0.00005175
Iteration 357/1000 | Loss: 0.00010134
Iteration 358/1000 | Loss: 0.00022751
Iteration 359/1000 | Loss: 0.00010044
Iteration 360/1000 | Loss: 0.00103818
Iteration 361/1000 | Loss: 0.00039750
Iteration 362/1000 | Loss: 0.00054021
Iteration 363/1000 | Loss: 0.00035047
Iteration 364/1000 | Loss: 0.00075580
Iteration 365/1000 | Loss: 0.00029252
Iteration 366/1000 | Loss: 0.00133851
Iteration 367/1000 | Loss: 0.00007315
Iteration 368/1000 | Loss: 0.00005914
Iteration 369/1000 | Loss: 0.00005450
Iteration 370/1000 | Loss: 0.00005283
Iteration 371/1000 | Loss: 0.00006047
Iteration 372/1000 | Loss: 0.00005360
Iteration 373/1000 | Loss: 0.00005137
Iteration 374/1000 | Loss: 0.00005473
Iteration 375/1000 | Loss: 0.00004856
Iteration 376/1000 | Loss: 0.00004765
Iteration 377/1000 | Loss: 0.00079417
Iteration 378/1000 | Loss: 0.00044691
Iteration 379/1000 | Loss: 0.00069757
Iteration 380/1000 | Loss: 0.00004794
Iteration 381/1000 | Loss: 0.00004652
Iteration 382/1000 | Loss: 0.00004588
Iteration 383/1000 | Loss: 0.00004526
Iteration 384/1000 | Loss: 0.00079248
Iteration 385/1000 | Loss: 0.00046954
Iteration 386/1000 | Loss: 0.00006265
Iteration 387/1000 | Loss: 0.00005380
Iteration 388/1000 | Loss: 0.00004590
Iteration 389/1000 | Loss: 0.00004472
Iteration 390/1000 | Loss: 0.00004429
Iteration 391/1000 | Loss: 0.00004410
Iteration 392/1000 | Loss: 0.00004389
Iteration 393/1000 | Loss: 0.00078689
Iteration 394/1000 | Loss: 0.00006939
Iteration 395/1000 | Loss: 0.00005145
Iteration 396/1000 | Loss: 0.00004476
Iteration 397/1000 | Loss: 0.00004206
Iteration 398/1000 | Loss: 0.00004089
Iteration 399/1000 | Loss: 0.00004026
Iteration 400/1000 | Loss: 0.00003988
Iteration 401/1000 | Loss: 0.00003963
Iteration 402/1000 | Loss: 0.00003956
Iteration 403/1000 | Loss: 0.00003936
Iteration 404/1000 | Loss: 0.00003928
Iteration 405/1000 | Loss: 0.00003928
Iteration 406/1000 | Loss: 0.00003927
Iteration 407/1000 | Loss: 0.00003927
Iteration 408/1000 | Loss: 0.00003926
Iteration 409/1000 | Loss: 0.00003926
Iteration 410/1000 | Loss: 0.00003925
Iteration 411/1000 | Loss: 0.00003925
Iteration 412/1000 | Loss: 0.00003925
Iteration 413/1000 | Loss: 0.00003922
Iteration 414/1000 | Loss: 0.00003922
Iteration 415/1000 | Loss: 0.00003922
Iteration 416/1000 | Loss: 0.00003922
Iteration 417/1000 | Loss: 0.00003921
Iteration 418/1000 | Loss: 0.00003921
Iteration 419/1000 | Loss: 0.00003920
Iteration 420/1000 | Loss: 0.00003920
Iteration 421/1000 | Loss: 0.00003920
Iteration 422/1000 | Loss: 0.00003919
Iteration 423/1000 | Loss: 0.00003919
Iteration 424/1000 | Loss: 0.00003919
Iteration 425/1000 | Loss: 0.00003918
Iteration 426/1000 | Loss: 0.00003918
Iteration 427/1000 | Loss: 0.00003918
Iteration 428/1000 | Loss: 0.00003918
Iteration 429/1000 | Loss: 0.00003918
Iteration 430/1000 | Loss: 0.00003917
Iteration 431/1000 | Loss: 0.00003917
Iteration 432/1000 | Loss: 0.00003917
Iteration 433/1000 | Loss: 0.00003917
Iteration 434/1000 | Loss: 0.00003917
Iteration 435/1000 | Loss: 0.00003917
Iteration 436/1000 | Loss: 0.00003917
Iteration 437/1000 | Loss: 0.00003917
Iteration 438/1000 | Loss: 0.00003917
Iteration 439/1000 | Loss: 0.00003916
Iteration 440/1000 | Loss: 0.00003916
Iteration 441/1000 | Loss: 0.00003916
Iteration 442/1000 | Loss: 0.00003916
Iteration 443/1000 | Loss: 0.00003916
Iteration 444/1000 | Loss: 0.00003916
Iteration 445/1000 | Loss: 0.00003916
Iteration 446/1000 | Loss: 0.00003916
Iteration 447/1000 | Loss: 0.00003915
Iteration 448/1000 | Loss: 0.00003915
Iteration 449/1000 | Loss: 0.00003915
Iteration 450/1000 | Loss: 0.00003915
Iteration 451/1000 | Loss: 0.00003914
Iteration 452/1000 | Loss: 0.00003914
Iteration 453/1000 | Loss: 0.00003914
Iteration 454/1000 | Loss: 0.00003914
Iteration 455/1000 | Loss: 0.00003914
Iteration 456/1000 | Loss: 0.00003913
Iteration 457/1000 | Loss: 0.00003913
Iteration 458/1000 | Loss: 0.00003913
Iteration 459/1000 | Loss: 0.00003912
Iteration 460/1000 | Loss: 0.00003912
Iteration 461/1000 | Loss: 0.00003912
Iteration 462/1000 | Loss: 0.00003911
Iteration 463/1000 | Loss: 0.00003911
Iteration 464/1000 | Loss: 0.00003911
Iteration 465/1000 | Loss: 0.00003910
Iteration 466/1000 | Loss: 0.00003910
Iteration 467/1000 | Loss: 0.00003910
Iteration 468/1000 | Loss: 0.00003910
Iteration 469/1000 | Loss: 0.00003910
Iteration 470/1000 | Loss: 0.00003909
Iteration 471/1000 | Loss: 0.00003909
Iteration 472/1000 | Loss: 0.00003909
Iteration 473/1000 | Loss: 0.00003909
Iteration 474/1000 | Loss: 0.00003909
Iteration 475/1000 | Loss: 0.00003909
Iteration 476/1000 | Loss: 0.00003909
Iteration 477/1000 | Loss: 0.00003909
Iteration 478/1000 | Loss: 0.00003908
Iteration 479/1000 | Loss: 0.00003908
Iteration 480/1000 | Loss: 0.00003908
Iteration 481/1000 | Loss: 0.00003908
Iteration 482/1000 | Loss: 0.00003908
Iteration 483/1000 | Loss: 0.00003907
Iteration 484/1000 | Loss: 0.00003907
Iteration 485/1000 | Loss: 0.00003907
Iteration 486/1000 | Loss: 0.00003907
Iteration 487/1000 | Loss: 0.00003907
Iteration 488/1000 | Loss: 0.00003907
Iteration 489/1000 | Loss: 0.00003907
Iteration 490/1000 | Loss: 0.00003907
Iteration 491/1000 | Loss: 0.00003907
Iteration 492/1000 | Loss: 0.00003906
Iteration 493/1000 | Loss: 0.00003906
Iteration 494/1000 | Loss: 0.00003906
Iteration 495/1000 | Loss: 0.00003906
Iteration 496/1000 | Loss: 0.00003905
Iteration 497/1000 | Loss: 0.00003905
Iteration 498/1000 | Loss: 0.00003905
Iteration 499/1000 | Loss: 0.00003905
Iteration 500/1000 | Loss: 0.00003905
Iteration 501/1000 | Loss: 0.00003905
Iteration 502/1000 | Loss: 0.00003905
Iteration 503/1000 | Loss: 0.00003905
Iteration 504/1000 | Loss: 0.00003905
Iteration 505/1000 | Loss: 0.00003905
Iteration 506/1000 | Loss: 0.00003905
Iteration 507/1000 | Loss: 0.00003905
Iteration 508/1000 | Loss: 0.00003905
Iteration 509/1000 | Loss: 0.00003905
Iteration 510/1000 | Loss: 0.00003905
Iteration 511/1000 | Loss: 0.00003905
Iteration 512/1000 | Loss: 0.00003905
Iteration 513/1000 | Loss: 0.00003905
Iteration 514/1000 | Loss: 0.00003904
Iteration 515/1000 | Loss: 0.00003904
Iteration 516/1000 | Loss: 0.00003904
Iteration 517/1000 | Loss: 0.00003904
Iteration 518/1000 | Loss: 0.00003904
Iteration 519/1000 | Loss: 0.00003904
Iteration 520/1000 | Loss: 0.00003904
Iteration 521/1000 | Loss: 0.00003904
Iteration 522/1000 | Loss: 0.00003904
Iteration 523/1000 | Loss: 0.00003904
Iteration 524/1000 | Loss: 0.00003904
Iteration 525/1000 | Loss: 0.00003904
Iteration 526/1000 | Loss: 0.00003904
Iteration 527/1000 | Loss: 0.00003904
Iteration 528/1000 | Loss: 0.00003904
Iteration 529/1000 | Loss: 0.00003904
Iteration 530/1000 | Loss: 0.00003904
Iteration 531/1000 | Loss: 0.00003904
Iteration 532/1000 | Loss: 0.00003904
Iteration 533/1000 | Loss: 0.00003904
Iteration 534/1000 | Loss: 0.00003904
Iteration 535/1000 | Loss: 0.00003904
Iteration 536/1000 | Loss: 0.00003903
Iteration 537/1000 | Loss: 0.00003903
Iteration 538/1000 | Loss: 0.00003903
Iteration 539/1000 | Loss: 0.00003903
Iteration 540/1000 | Loss: 0.00003903
Iteration 541/1000 | Loss: 0.00003903
Iteration 542/1000 | Loss: 0.00003903
Iteration 543/1000 | Loss: 0.00003903
Iteration 544/1000 | Loss: 0.00003903
Iteration 545/1000 | Loss: 0.00003903
Iteration 546/1000 | Loss: 0.00003903
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 546. Stopping optimization.
Last 5 losses: [3.9032303902786225e-05, 3.9032303902786225e-05, 3.9032303902786225e-05, 3.9032303902786225e-05, 3.9032303902786225e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.9032303902786225e-05

Optimization complete. Final v2v error: 3.950004816055298 mm

Highest mean error: 12.980064392089844 mm for frame 94

Lowest mean error: 2.984464645385742 mm for frame 99

Saving results

Total time: 624.8506109714508
