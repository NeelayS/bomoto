Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=91, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 5096-5151
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alison_posed_001/1050/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alison_posed_001/1050.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alison_posed_001/1050
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00471539
Iteration 2/25 | Loss: 0.00147053
Iteration 3/25 | Loss: 0.00132469
Iteration 4/25 | Loss: 0.00130431
Iteration 5/25 | Loss: 0.00130008
Iteration 6/25 | Loss: 0.00129874
Iteration 7/25 | Loss: 0.00129874
Iteration 8/25 | Loss: 0.00129874
Iteration 9/25 | Loss: 0.00129874
Iteration 10/25 | Loss: 0.00129874
Iteration 11/25 | Loss: 0.00129874
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012987448135390878, 0.0012987448135390878, 0.0012987448135390878, 0.0012987448135390878, 0.0012987448135390878]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012987448135390878

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.25152957
Iteration 2/25 | Loss: 0.00086471
Iteration 3/25 | Loss: 0.00086468
Iteration 4/25 | Loss: 0.00086468
Iteration 5/25 | Loss: 0.00086468
Iteration 6/25 | Loss: 0.00086468
Iteration 7/25 | Loss: 0.00086468
Iteration 8/25 | Loss: 0.00086468
Iteration 9/25 | Loss: 0.00086468
Iteration 10/25 | Loss: 0.00086468
Iteration 11/25 | Loss: 0.00086468
Iteration 12/25 | Loss: 0.00086468
Iteration 13/25 | Loss: 0.00086468
Iteration 14/25 | Loss: 0.00086468
Iteration 15/25 | Loss: 0.00086468
Iteration 16/25 | Loss: 0.00086468
Iteration 17/25 | Loss: 0.00086468
Iteration 18/25 | Loss: 0.00086468
Iteration 19/25 | Loss: 0.00086468
Iteration 20/25 | Loss: 0.00086468
Iteration 21/25 | Loss: 0.00086468
Iteration 22/25 | Loss: 0.00086468
Iteration 23/25 | Loss: 0.00086468
Iteration 24/25 | Loss: 0.00086468
Iteration 25/25 | Loss: 0.00086468

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00086468
Iteration 2/1000 | Loss: 0.00007908
Iteration 3/1000 | Loss: 0.00004424
Iteration 4/1000 | Loss: 0.00003563
Iteration 5/1000 | Loss: 0.00003315
Iteration 6/1000 | Loss: 0.00003167
Iteration 7/1000 | Loss: 0.00003052
Iteration 8/1000 | Loss: 0.00002993
Iteration 9/1000 | Loss: 0.00002931
Iteration 10/1000 | Loss: 0.00002881
Iteration 11/1000 | Loss: 0.00002844
Iteration 12/1000 | Loss: 0.00002823
Iteration 13/1000 | Loss: 0.00002800
Iteration 14/1000 | Loss: 0.00002783
Iteration 15/1000 | Loss: 0.00002771
Iteration 16/1000 | Loss: 0.00002765
Iteration 17/1000 | Loss: 0.00002763
Iteration 18/1000 | Loss: 0.00002754
Iteration 19/1000 | Loss: 0.00002740
Iteration 20/1000 | Loss: 0.00002738
Iteration 21/1000 | Loss: 0.00002735
Iteration 22/1000 | Loss: 0.00002733
Iteration 23/1000 | Loss: 0.00002730
Iteration 24/1000 | Loss: 0.00002729
Iteration 25/1000 | Loss: 0.00002728
Iteration 26/1000 | Loss: 0.00002727
Iteration 27/1000 | Loss: 0.00002727
Iteration 28/1000 | Loss: 0.00002726
Iteration 29/1000 | Loss: 0.00002726
Iteration 30/1000 | Loss: 0.00002725
Iteration 31/1000 | Loss: 0.00002722
Iteration 32/1000 | Loss: 0.00002716
Iteration 33/1000 | Loss: 0.00002716
Iteration 34/1000 | Loss: 0.00002713
Iteration 35/1000 | Loss: 0.00002712
Iteration 36/1000 | Loss: 0.00002710
Iteration 37/1000 | Loss: 0.00002710
Iteration 38/1000 | Loss: 0.00002708
Iteration 39/1000 | Loss: 0.00002706
Iteration 40/1000 | Loss: 0.00002706
Iteration 41/1000 | Loss: 0.00002698
Iteration 42/1000 | Loss: 0.00002697
Iteration 43/1000 | Loss: 0.00002697
Iteration 44/1000 | Loss: 0.00002697
Iteration 45/1000 | Loss: 0.00002694
Iteration 46/1000 | Loss: 0.00002693
Iteration 47/1000 | Loss: 0.00002692
Iteration 48/1000 | Loss: 0.00002692
Iteration 49/1000 | Loss: 0.00002692
Iteration 50/1000 | Loss: 0.00002691
Iteration 51/1000 | Loss: 0.00002691
Iteration 52/1000 | Loss: 0.00002691
Iteration 53/1000 | Loss: 0.00002691
Iteration 54/1000 | Loss: 0.00002691
Iteration 55/1000 | Loss: 0.00002691
Iteration 56/1000 | Loss: 0.00002691
Iteration 57/1000 | Loss: 0.00002691
Iteration 58/1000 | Loss: 0.00002691
Iteration 59/1000 | Loss: 0.00002691
Iteration 60/1000 | Loss: 0.00002691
Iteration 61/1000 | Loss: 0.00002691
Iteration 62/1000 | Loss: 0.00002691
Iteration 63/1000 | Loss: 0.00002691
Iteration 64/1000 | Loss: 0.00002691
Iteration 65/1000 | Loss: 0.00002690
Iteration 66/1000 | Loss: 0.00002690
Iteration 67/1000 | Loss: 0.00002690
Iteration 68/1000 | Loss: 0.00002690
Iteration 69/1000 | Loss: 0.00002689
Iteration 70/1000 | Loss: 0.00002689
Iteration 71/1000 | Loss: 0.00002689
Iteration 72/1000 | Loss: 0.00002689
Iteration 73/1000 | Loss: 0.00002689
Iteration 74/1000 | Loss: 0.00002689
Iteration 75/1000 | Loss: 0.00002689
Iteration 76/1000 | Loss: 0.00002689
Iteration 77/1000 | Loss: 0.00002688
Iteration 78/1000 | Loss: 0.00002688
Iteration 79/1000 | Loss: 0.00002688
Iteration 80/1000 | Loss: 0.00002688
Iteration 81/1000 | Loss: 0.00002688
Iteration 82/1000 | Loss: 0.00002688
Iteration 83/1000 | Loss: 0.00002688
Iteration 84/1000 | Loss: 0.00002688
Iteration 85/1000 | Loss: 0.00002688
Iteration 86/1000 | Loss: 0.00002687
Iteration 87/1000 | Loss: 0.00002687
Iteration 88/1000 | Loss: 0.00002687
Iteration 89/1000 | Loss: 0.00002687
Iteration 90/1000 | Loss: 0.00002687
Iteration 91/1000 | Loss: 0.00002687
Iteration 92/1000 | Loss: 0.00002687
Iteration 93/1000 | Loss: 0.00002687
Iteration 94/1000 | Loss: 0.00002686
Iteration 95/1000 | Loss: 0.00002686
Iteration 96/1000 | Loss: 0.00002686
Iteration 97/1000 | Loss: 0.00002686
Iteration 98/1000 | Loss: 0.00002686
Iteration 99/1000 | Loss: 0.00002686
Iteration 100/1000 | Loss: 0.00002685
Iteration 101/1000 | Loss: 0.00002685
Iteration 102/1000 | Loss: 0.00002685
Iteration 103/1000 | Loss: 0.00002685
Iteration 104/1000 | Loss: 0.00002685
Iteration 105/1000 | Loss: 0.00002685
Iteration 106/1000 | Loss: 0.00002685
Iteration 107/1000 | Loss: 0.00002685
Iteration 108/1000 | Loss: 0.00002685
Iteration 109/1000 | Loss: 0.00002685
Iteration 110/1000 | Loss: 0.00002685
Iteration 111/1000 | Loss: 0.00002684
Iteration 112/1000 | Loss: 0.00002684
Iteration 113/1000 | Loss: 0.00002684
Iteration 114/1000 | Loss: 0.00002684
Iteration 115/1000 | Loss: 0.00002684
Iteration 116/1000 | Loss: 0.00002684
Iteration 117/1000 | Loss: 0.00002684
Iteration 118/1000 | Loss: 0.00002684
Iteration 119/1000 | Loss: 0.00002684
Iteration 120/1000 | Loss: 0.00002684
Iteration 121/1000 | Loss: 0.00002684
Iteration 122/1000 | Loss: 0.00002684
Iteration 123/1000 | Loss: 0.00002683
Iteration 124/1000 | Loss: 0.00002683
Iteration 125/1000 | Loss: 0.00002683
Iteration 126/1000 | Loss: 0.00002682
Iteration 127/1000 | Loss: 0.00002682
Iteration 128/1000 | Loss: 0.00002682
Iteration 129/1000 | Loss: 0.00002682
Iteration 130/1000 | Loss: 0.00002682
Iteration 131/1000 | Loss: 0.00002682
Iteration 132/1000 | Loss: 0.00002682
Iteration 133/1000 | Loss: 0.00002681
Iteration 134/1000 | Loss: 0.00002681
Iteration 135/1000 | Loss: 0.00002681
Iteration 136/1000 | Loss: 0.00002681
Iteration 137/1000 | Loss: 0.00002681
Iteration 138/1000 | Loss: 0.00002681
Iteration 139/1000 | Loss: 0.00002681
Iteration 140/1000 | Loss: 0.00002681
Iteration 141/1000 | Loss: 0.00002681
Iteration 142/1000 | Loss: 0.00002681
Iteration 143/1000 | Loss: 0.00002681
Iteration 144/1000 | Loss: 0.00002681
Iteration 145/1000 | Loss: 0.00002681
Iteration 146/1000 | Loss: 0.00002681
Iteration 147/1000 | Loss: 0.00002680
Iteration 148/1000 | Loss: 0.00002680
Iteration 149/1000 | Loss: 0.00002680
Iteration 150/1000 | Loss: 0.00002680
Iteration 151/1000 | Loss: 0.00002680
Iteration 152/1000 | Loss: 0.00002680
Iteration 153/1000 | Loss: 0.00002680
Iteration 154/1000 | Loss: 0.00002680
Iteration 155/1000 | Loss: 0.00002680
Iteration 156/1000 | Loss: 0.00002680
Iteration 157/1000 | Loss: 0.00002680
Iteration 158/1000 | Loss: 0.00002679
Iteration 159/1000 | Loss: 0.00002679
Iteration 160/1000 | Loss: 0.00002679
Iteration 161/1000 | Loss: 0.00002679
Iteration 162/1000 | Loss: 0.00002679
Iteration 163/1000 | Loss: 0.00002679
Iteration 164/1000 | Loss: 0.00002679
Iteration 165/1000 | Loss: 0.00002679
Iteration 166/1000 | Loss: 0.00002678
Iteration 167/1000 | Loss: 0.00002678
Iteration 168/1000 | Loss: 0.00002678
Iteration 169/1000 | Loss: 0.00002678
Iteration 170/1000 | Loss: 0.00002678
Iteration 171/1000 | Loss: 0.00002678
Iteration 172/1000 | Loss: 0.00002678
Iteration 173/1000 | Loss: 0.00002678
Iteration 174/1000 | Loss: 0.00002678
Iteration 175/1000 | Loss: 0.00002678
Iteration 176/1000 | Loss: 0.00002677
Iteration 177/1000 | Loss: 0.00002677
Iteration 178/1000 | Loss: 0.00002677
Iteration 179/1000 | Loss: 0.00002677
Iteration 180/1000 | Loss: 0.00002677
Iteration 181/1000 | Loss: 0.00002677
Iteration 182/1000 | Loss: 0.00002677
Iteration 183/1000 | Loss: 0.00002677
Iteration 184/1000 | Loss: 0.00002677
Iteration 185/1000 | Loss: 0.00002677
Iteration 186/1000 | Loss: 0.00002677
Iteration 187/1000 | Loss: 0.00002677
Iteration 188/1000 | Loss: 0.00002677
Iteration 189/1000 | Loss: 0.00002677
Iteration 190/1000 | Loss: 0.00002677
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 190. Stopping optimization.
Last 5 losses: [2.677035445231013e-05, 2.677035445231013e-05, 2.677035445231013e-05, 2.677035445231013e-05, 2.677035445231013e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.677035445231013e-05

Optimization complete. Final v2v error: 4.164567947387695 mm

Highest mean error: 5.83920955657959 mm for frame 73

Lowest mean error: 3.353904962539673 mm for frame 35

Saving results

Total time: 49.151066303253174
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alison_posed_001/1080/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alison_posed_001/1080.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alison_posed_001/1080
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00987596
Iteration 2/25 | Loss: 0.00253248
Iteration 3/25 | Loss: 0.00183777
Iteration 4/25 | Loss: 0.00171061
Iteration 5/25 | Loss: 0.00176766
Iteration 6/25 | Loss: 0.00162440
Iteration 7/25 | Loss: 0.00155693
Iteration 8/25 | Loss: 0.00148135
Iteration 9/25 | Loss: 0.00144417
Iteration 10/25 | Loss: 0.00148273
Iteration 11/25 | Loss: 0.00139339
Iteration 12/25 | Loss: 0.00137820
Iteration 13/25 | Loss: 0.00133273
Iteration 14/25 | Loss: 0.00133258
Iteration 15/25 | Loss: 0.00131619
Iteration 16/25 | Loss: 0.00132492
Iteration 17/25 | Loss: 0.00130363
Iteration 18/25 | Loss: 0.00129652
Iteration 19/25 | Loss: 0.00130535
Iteration 20/25 | Loss: 0.00129712
Iteration 21/25 | Loss: 0.00129460
Iteration 22/25 | Loss: 0.00129232
Iteration 23/25 | Loss: 0.00129393
Iteration 24/25 | Loss: 0.00129276
Iteration 25/25 | Loss: 0.00129382

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.85162592
Iteration 2/25 | Loss: 0.00188178
Iteration 3/25 | Loss: 0.00125559
Iteration 4/25 | Loss: 0.00125555
Iteration 5/25 | Loss: 0.00125555
Iteration 6/25 | Loss: 0.00125555
Iteration 7/25 | Loss: 0.00125555
Iteration 8/25 | Loss: 0.00125555
Iteration 9/25 | Loss: 0.00125555
Iteration 10/25 | Loss: 0.00125555
Iteration 11/25 | Loss: 0.00125555
Iteration 12/25 | Loss: 0.00125555
Iteration 13/25 | Loss: 0.00125555
Iteration 14/25 | Loss: 0.00125555
Iteration 15/25 | Loss: 0.00125555
Iteration 16/25 | Loss: 0.00125555
Iteration 17/25 | Loss: 0.00125555
Iteration 18/25 | Loss: 0.00125555
Iteration 19/25 | Loss: 0.00125555
Iteration 20/25 | Loss: 0.00125555
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0012555496068671346, 0.0012555496068671346, 0.0012555496068671346, 0.0012555496068671346, 0.0012555496068671346]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012555496068671346

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00125555
Iteration 2/1000 | Loss: 0.00124770
Iteration 3/1000 | Loss: 0.00042662
Iteration 4/1000 | Loss: 0.00113203
Iteration 5/1000 | Loss: 0.00123982
Iteration 6/1000 | Loss: 0.00180925
Iteration 7/1000 | Loss: 0.00056024
Iteration 8/1000 | Loss: 0.00060611
Iteration 9/1000 | Loss: 0.00031976
Iteration 10/1000 | Loss: 0.00255687
Iteration 11/1000 | Loss: 0.00184166
Iteration 12/1000 | Loss: 0.00384891
Iteration 13/1000 | Loss: 0.00195525
Iteration 14/1000 | Loss: 0.00179362
Iteration 15/1000 | Loss: 0.00134418
Iteration 16/1000 | Loss: 0.00214340
Iteration 17/1000 | Loss: 0.00461971
Iteration 18/1000 | Loss: 0.00073786
Iteration 19/1000 | Loss: 0.00108012
Iteration 20/1000 | Loss: 0.00090189
Iteration 21/1000 | Loss: 0.00027334
Iteration 22/1000 | Loss: 0.00112958
Iteration 23/1000 | Loss: 0.00053951
Iteration 24/1000 | Loss: 0.00167286
Iteration 25/1000 | Loss: 0.00029845
Iteration 26/1000 | Loss: 0.00025204
Iteration 27/1000 | Loss: 0.00032991
Iteration 28/1000 | Loss: 0.00028387
Iteration 29/1000 | Loss: 0.00057106
Iteration 30/1000 | Loss: 0.00060750
Iteration 31/1000 | Loss: 0.00030417
Iteration 32/1000 | Loss: 0.00015852
Iteration 33/1000 | Loss: 0.00122252
Iteration 34/1000 | Loss: 0.00178698
Iteration 35/1000 | Loss: 0.00188522
Iteration 36/1000 | Loss: 0.00225054
Iteration 37/1000 | Loss: 0.00147904
Iteration 38/1000 | Loss: 0.00091554
Iteration 39/1000 | Loss: 0.00041602
Iteration 40/1000 | Loss: 0.00056991
Iteration 41/1000 | Loss: 0.00055671
Iteration 42/1000 | Loss: 0.00030869
Iteration 43/1000 | Loss: 0.00020027
Iteration 44/1000 | Loss: 0.00006264
Iteration 45/1000 | Loss: 0.00028423
Iteration 46/1000 | Loss: 0.00040008
Iteration 47/1000 | Loss: 0.00008005
Iteration 48/1000 | Loss: 0.00032790
Iteration 49/1000 | Loss: 0.00016379
Iteration 50/1000 | Loss: 0.00039180
Iteration 51/1000 | Loss: 0.00022106
Iteration 52/1000 | Loss: 0.00017156
Iteration 53/1000 | Loss: 0.00037293
Iteration 54/1000 | Loss: 0.00021143
Iteration 55/1000 | Loss: 0.00032738
Iteration 56/1000 | Loss: 0.00013953
Iteration 57/1000 | Loss: 0.00035074
Iteration 58/1000 | Loss: 0.00022916
Iteration 59/1000 | Loss: 0.00020430
Iteration 60/1000 | Loss: 0.00025361
Iteration 61/1000 | Loss: 0.00004725
Iteration 62/1000 | Loss: 0.00003516
Iteration 63/1000 | Loss: 0.00013321
Iteration 64/1000 | Loss: 0.00003673
Iteration 65/1000 | Loss: 0.00063042
Iteration 66/1000 | Loss: 0.00063811
Iteration 67/1000 | Loss: 0.00030454
Iteration 68/1000 | Loss: 0.00053268
Iteration 69/1000 | Loss: 0.00029021
Iteration 70/1000 | Loss: 0.00057503
Iteration 71/1000 | Loss: 0.00033825
Iteration 72/1000 | Loss: 0.00060014
Iteration 73/1000 | Loss: 0.00009214
Iteration 74/1000 | Loss: 0.00069344
Iteration 75/1000 | Loss: 0.00055076
Iteration 76/1000 | Loss: 0.00061549
Iteration 77/1000 | Loss: 0.00053473
Iteration 78/1000 | Loss: 0.00027595
Iteration 79/1000 | Loss: 0.00082182
Iteration 80/1000 | Loss: 0.00029793
Iteration 81/1000 | Loss: 0.00023755
Iteration 82/1000 | Loss: 0.00003132
Iteration 83/1000 | Loss: 0.00002787
Iteration 84/1000 | Loss: 0.00109032
Iteration 85/1000 | Loss: 0.00069411
Iteration 86/1000 | Loss: 0.00083561
Iteration 87/1000 | Loss: 0.00063654
Iteration 88/1000 | Loss: 0.00002938
Iteration 89/1000 | Loss: 0.00057291
Iteration 90/1000 | Loss: 0.00023159
Iteration 91/1000 | Loss: 0.00002634
Iteration 92/1000 | Loss: 0.00002482
Iteration 93/1000 | Loss: 0.00002418
Iteration 94/1000 | Loss: 0.00002642
Iteration 95/1000 | Loss: 0.00084637
Iteration 96/1000 | Loss: 0.00036070
Iteration 97/1000 | Loss: 0.00002397
Iteration 98/1000 | Loss: 0.00076360
Iteration 99/1000 | Loss: 0.00026778
Iteration 100/1000 | Loss: 0.00002624
Iteration 101/1000 | Loss: 0.00058529
Iteration 102/1000 | Loss: 0.00003780
Iteration 103/1000 | Loss: 0.00061335
Iteration 104/1000 | Loss: 0.00040075
Iteration 105/1000 | Loss: 0.00011391
Iteration 106/1000 | Loss: 0.00002443
Iteration 107/1000 | Loss: 0.00088877
Iteration 108/1000 | Loss: 0.00096991
Iteration 109/1000 | Loss: 0.00018816
Iteration 110/1000 | Loss: 0.00116913
Iteration 111/1000 | Loss: 0.00023553
Iteration 112/1000 | Loss: 0.00052707
Iteration 113/1000 | Loss: 0.00041201
Iteration 114/1000 | Loss: 0.00013844
Iteration 115/1000 | Loss: 0.00070904
Iteration 116/1000 | Loss: 0.00053490
Iteration 117/1000 | Loss: 0.00002448
Iteration 118/1000 | Loss: 0.00107236
Iteration 119/1000 | Loss: 0.00056392
Iteration 120/1000 | Loss: 0.00076777
Iteration 121/1000 | Loss: 0.00035206
Iteration 122/1000 | Loss: 0.00048136
Iteration 123/1000 | Loss: 0.00096394
Iteration 124/1000 | Loss: 0.00149119
Iteration 125/1000 | Loss: 0.00048169
Iteration 126/1000 | Loss: 0.00033011
Iteration 127/1000 | Loss: 0.00027862
Iteration 128/1000 | Loss: 0.00043684
Iteration 129/1000 | Loss: 0.00011248
Iteration 130/1000 | Loss: 0.00003661
Iteration 131/1000 | Loss: 0.00003057
Iteration 132/1000 | Loss: 0.00039408
Iteration 133/1000 | Loss: 0.00002477
Iteration 134/1000 | Loss: 0.00003061
Iteration 135/1000 | Loss: 0.00023532
Iteration 136/1000 | Loss: 0.00008383
Iteration 137/1000 | Loss: 0.00002100
Iteration 138/1000 | Loss: 0.00011733
Iteration 139/1000 | Loss: 0.00010685
Iteration 140/1000 | Loss: 0.00002075
Iteration 141/1000 | Loss: 0.00001968
Iteration 142/1000 | Loss: 0.00001891
Iteration 143/1000 | Loss: 0.00001815
Iteration 144/1000 | Loss: 0.00001772
Iteration 145/1000 | Loss: 0.00001741
Iteration 146/1000 | Loss: 0.00001710
Iteration 147/1000 | Loss: 0.00001701
Iteration 148/1000 | Loss: 0.00001683
Iteration 149/1000 | Loss: 0.00001669
Iteration 150/1000 | Loss: 0.00001668
Iteration 151/1000 | Loss: 0.00001655
Iteration 152/1000 | Loss: 0.00001644
Iteration 153/1000 | Loss: 0.00001642
Iteration 154/1000 | Loss: 0.00001642
Iteration 155/1000 | Loss: 0.00001641
Iteration 156/1000 | Loss: 0.00001641
Iteration 157/1000 | Loss: 0.00001640
Iteration 158/1000 | Loss: 0.00001636
Iteration 159/1000 | Loss: 0.00001633
Iteration 160/1000 | Loss: 0.00001633
Iteration 161/1000 | Loss: 0.00001632
Iteration 162/1000 | Loss: 0.00001632
Iteration 163/1000 | Loss: 0.00001631
Iteration 164/1000 | Loss: 0.00001630
Iteration 165/1000 | Loss: 0.00001629
Iteration 166/1000 | Loss: 0.00001629
Iteration 167/1000 | Loss: 0.00001629
Iteration 168/1000 | Loss: 0.00001628
Iteration 169/1000 | Loss: 0.00001628
Iteration 170/1000 | Loss: 0.00001627
Iteration 171/1000 | Loss: 0.00001625
Iteration 172/1000 | Loss: 0.00001625
Iteration 173/1000 | Loss: 0.00001625
Iteration 174/1000 | Loss: 0.00001624
Iteration 175/1000 | Loss: 0.00001623
Iteration 176/1000 | Loss: 0.00001623
Iteration 177/1000 | Loss: 0.00001623
Iteration 178/1000 | Loss: 0.00001622
Iteration 179/1000 | Loss: 0.00001622
Iteration 180/1000 | Loss: 0.00001621
Iteration 181/1000 | Loss: 0.00001621
Iteration 182/1000 | Loss: 0.00001620
Iteration 183/1000 | Loss: 0.00001620
Iteration 184/1000 | Loss: 0.00001620
Iteration 185/1000 | Loss: 0.00001619
Iteration 186/1000 | Loss: 0.00001619
Iteration 187/1000 | Loss: 0.00001619
Iteration 188/1000 | Loss: 0.00001619
Iteration 189/1000 | Loss: 0.00001619
Iteration 190/1000 | Loss: 0.00001618
Iteration 191/1000 | Loss: 0.00001618
Iteration 192/1000 | Loss: 0.00001618
Iteration 193/1000 | Loss: 0.00031365
Iteration 194/1000 | Loss: 0.00003930
Iteration 195/1000 | Loss: 0.00001647
Iteration 196/1000 | Loss: 0.00001622
Iteration 197/1000 | Loss: 0.00001619
Iteration 198/1000 | Loss: 0.00001614
Iteration 199/1000 | Loss: 0.00001613
Iteration 200/1000 | Loss: 0.00001613
Iteration 201/1000 | Loss: 0.00001612
Iteration 202/1000 | Loss: 0.00001612
Iteration 203/1000 | Loss: 0.00001611
Iteration 204/1000 | Loss: 0.00001611
Iteration 205/1000 | Loss: 0.00001611
Iteration 206/1000 | Loss: 0.00001611
Iteration 207/1000 | Loss: 0.00001611
Iteration 208/1000 | Loss: 0.00001611
Iteration 209/1000 | Loss: 0.00001610
Iteration 210/1000 | Loss: 0.00001610
Iteration 211/1000 | Loss: 0.00001610
Iteration 212/1000 | Loss: 0.00001609
Iteration 213/1000 | Loss: 0.00001609
Iteration 214/1000 | Loss: 0.00001609
Iteration 215/1000 | Loss: 0.00001608
Iteration 216/1000 | Loss: 0.00001608
Iteration 217/1000 | Loss: 0.00001607
Iteration 218/1000 | Loss: 0.00001607
Iteration 219/1000 | Loss: 0.00001607
Iteration 220/1000 | Loss: 0.00001606
Iteration 221/1000 | Loss: 0.00001606
Iteration 222/1000 | Loss: 0.00001606
Iteration 223/1000 | Loss: 0.00001605
Iteration 224/1000 | Loss: 0.00001605
Iteration 225/1000 | Loss: 0.00001605
Iteration 226/1000 | Loss: 0.00001605
Iteration 227/1000 | Loss: 0.00001605
Iteration 228/1000 | Loss: 0.00001604
Iteration 229/1000 | Loss: 0.00001604
Iteration 230/1000 | Loss: 0.00001604
Iteration 231/1000 | Loss: 0.00001604
Iteration 232/1000 | Loss: 0.00001604
Iteration 233/1000 | Loss: 0.00001603
Iteration 234/1000 | Loss: 0.00001603
Iteration 235/1000 | Loss: 0.00001603
Iteration 236/1000 | Loss: 0.00001603
Iteration 237/1000 | Loss: 0.00001603
Iteration 238/1000 | Loss: 0.00001602
Iteration 239/1000 | Loss: 0.00001602
Iteration 240/1000 | Loss: 0.00001602
Iteration 241/1000 | Loss: 0.00001602
Iteration 242/1000 | Loss: 0.00001601
Iteration 243/1000 | Loss: 0.00001601
Iteration 244/1000 | Loss: 0.00001601
Iteration 245/1000 | Loss: 0.00001601
Iteration 246/1000 | Loss: 0.00001601
Iteration 247/1000 | Loss: 0.00001601
Iteration 248/1000 | Loss: 0.00001601
Iteration 249/1000 | Loss: 0.00001601
Iteration 250/1000 | Loss: 0.00001601
Iteration 251/1000 | Loss: 0.00001601
Iteration 252/1000 | Loss: 0.00001601
Iteration 253/1000 | Loss: 0.00001601
Iteration 254/1000 | Loss: 0.00001600
Iteration 255/1000 | Loss: 0.00001600
Iteration 256/1000 | Loss: 0.00001600
Iteration 257/1000 | Loss: 0.00001600
Iteration 258/1000 | Loss: 0.00001600
Iteration 259/1000 | Loss: 0.00001600
Iteration 260/1000 | Loss: 0.00001600
Iteration 261/1000 | Loss: 0.00001600
Iteration 262/1000 | Loss: 0.00001600
Iteration 263/1000 | Loss: 0.00001600
Iteration 264/1000 | Loss: 0.00001600
Iteration 265/1000 | Loss: 0.00001600
Iteration 266/1000 | Loss: 0.00001600
Iteration 267/1000 | Loss: 0.00001600
Iteration 268/1000 | Loss: 0.00001600
Iteration 269/1000 | Loss: 0.00001600
Iteration 270/1000 | Loss: 0.00001600
Iteration 271/1000 | Loss: 0.00001599
Iteration 272/1000 | Loss: 0.00001599
Iteration 273/1000 | Loss: 0.00001599
Iteration 274/1000 | Loss: 0.00001599
Iteration 275/1000 | Loss: 0.00001599
Iteration 276/1000 | Loss: 0.00001599
Iteration 277/1000 | Loss: 0.00001599
Iteration 278/1000 | Loss: 0.00001599
Iteration 279/1000 | Loss: 0.00001599
Iteration 280/1000 | Loss: 0.00001599
Iteration 281/1000 | Loss: 0.00001599
Iteration 282/1000 | Loss: 0.00001599
Iteration 283/1000 | Loss: 0.00001599
Iteration 284/1000 | Loss: 0.00001599
Iteration 285/1000 | Loss: 0.00001599
Iteration 286/1000 | Loss: 0.00001599
Iteration 287/1000 | Loss: 0.00001599
Iteration 288/1000 | Loss: 0.00001599
Iteration 289/1000 | Loss: 0.00001599
Iteration 290/1000 | Loss: 0.00001599
Iteration 291/1000 | Loss: 0.00001599
Iteration 292/1000 | Loss: 0.00001599
Iteration 293/1000 | Loss: 0.00001599
Iteration 294/1000 | Loss: 0.00001599
Iteration 295/1000 | Loss: 0.00001599
Iteration 296/1000 | Loss: 0.00001599
Iteration 297/1000 | Loss: 0.00001599
Iteration 298/1000 | Loss: 0.00001599
Iteration 299/1000 | Loss: 0.00001599
Iteration 300/1000 | Loss: 0.00001599
Iteration 301/1000 | Loss: 0.00001599
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 301. Stopping optimization.
Last 5 losses: [1.598781454958953e-05, 1.598781454958953e-05, 1.598781454958953e-05, 1.598781454958953e-05, 1.598781454958953e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.598781454958953e-05

Optimization complete. Final v2v error: 3.3019354343414307 mm

Highest mean error: 5.508035659790039 mm for frame 106

Lowest mean error: 2.9189486503601074 mm for frame 97

Saving results

Total time: 267.5186893939972
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alison_posed_001/1019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alison_posed_001/1019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alison_posed_001/1019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00418575
Iteration 2/25 | Loss: 0.00132369
Iteration 3/25 | Loss: 0.00122172
Iteration 4/25 | Loss: 0.00121459
Iteration 5/25 | Loss: 0.00121330
Iteration 6/25 | Loss: 0.00121292
Iteration 7/25 | Loss: 0.00121292
Iteration 8/25 | Loss: 0.00121292
Iteration 9/25 | Loss: 0.00121292
Iteration 10/25 | Loss: 0.00121292
Iteration 11/25 | Loss: 0.00121292
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012129159877076745, 0.0012129159877076745, 0.0012129159877076745, 0.0012129159877076745, 0.0012129159877076745]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012129159877076745

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.47686791
Iteration 2/25 | Loss: 0.00077017
Iteration 3/25 | Loss: 0.00077017
Iteration 4/25 | Loss: 0.00077017
Iteration 5/25 | Loss: 0.00077017
Iteration 6/25 | Loss: 0.00077017
Iteration 7/25 | Loss: 0.00077017
Iteration 8/25 | Loss: 0.00077017
Iteration 9/25 | Loss: 0.00077017
Iteration 10/25 | Loss: 0.00077017
Iteration 11/25 | Loss: 0.00077017
Iteration 12/25 | Loss: 0.00077016
Iteration 13/25 | Loss: 0.00077016
Iteration 14/25 | Loss: 0.00077016
Iteration 15/25 | Loss: 0.00077016
Iteration 16/25 | Loss: 0.00077016
Iteration 17/25 | Loss: 0.00077016
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0007701648282818496, 0.0007701648282818496, 0.0007701648282818496, 0.0007701648282818496, 0.0007701648282818496]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007701648282818496

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00077016
Iteration 2/1000 | Loss: 0.00002435
Iteration 3/1000 | Loss: 0.00001714
Iteration 4/1000 | Loss: 0.00001482
Iteration 5/1000 | Loss: 0.00001374
Iteration 6/1000 | Loss: 0.00001292
Iteration 7/1000 | Loss: 0.00001248
Iteration 8/1000 | Loss: 0.00001225
Iteration 9/1000 | Loss: 0.00001219
Iteration 10/1000 | Loss: 0.00001209
Iteration 11/1000 | Loss: 0.00001202
Iteration 12/1000 | Loss: 0.00001201
Iteration 13/1000 | Loss: 0.00001199
Iteration 14/1000 | Loss: 0.00001196
Iteration 15/1000 | Loss: 0.00001195
Iteration 16/1000 | Loss: 0.00001191
Iteration 17/1000 | Loss: 0.00001190
Iteration 18/1000 | Loss: 0.00001188
Iteration 19/1000 | Loss: 0.00001187
Iteration 20/1000 | Loss: 0.00001184
Iteration 21/1000 | Loss: 0.00001182
Iteration 22/1000 | Loss: 0.00001181
Iteration 23/1000 | Loss: 0.00001179
Iteration 24/1000 | Loss: 0.00001179
Iteration 25/1000 | Loss: 0.00001178
Iteration 26/1000 | Loss: 0.00001178
Iteration 27/1000 | Loss: 0.00001178
Iteration 28/1000 | Loss: 0.00001177
Iteration 29/1000 | Loss: 0.00001176
Iteration 30/1000 | Loss: 0.00001176
Iteration 31/1000 | Loss: 0.00001175
Iteration 32/1000 | Loss: 0.00001175
Iteration 33/1000 | Loss: 0.00001175
Iteration 34/1000 | Loss: 0.00001174
Iteration 35/1000 | Loss: 0.00001174
Iteration 36/1000 | Loss: 0.00001174
Iteration 37/1000 | Loss: 0.00001174
Iteration 38/1000 | Loss: 0.00001174
Iteration 39/1000 | Loss: 0.00001174
Iteration 40/1000 | Loss: 0.00001174
Iteration 41/1000 | Loss: 0.00001174
Iteration 42/1000 | Loss: 0.00001173
Iteration 43/1000 | Loss: 0.00001173
Iteration 44/1000 | Loss: 0.00001173
Iteration 45/1000 | Loss: 0.00001172
Iteration 46/1000 | Loss: 0.00001172
Iteration 47/1000 | Loss: 0.00001172
Iteration 48/1000 | Loss: 0.00001172
Iteration 49/1000 | Loss: 0.00001172
Iteration 50/1000 | Loss: 0.00001172
Iteration 51/1000 | Loss: 0.00001172
Iteration 52/1000 | Loss: 0.00001172
Iteration 53/1000 | Loss: 0.00001172
Iteration 54/1000 | Loss: 0.00001172
Iteration 55/1000 | Loss: 0.00001172
Iteration 56/1000 | Loss: 0.00001172
Iteration 57/1000 | Loss: 0.00001172
Iteration 58/1000 | Loss: 0.00001172
Iteration 59/1000 | Loss: 0.00001172
Iteration 60/1000 | Loss: 0.00001172
Iteration 61/1000 | Loss: 0.00001172
Iteration 62/1000 | Loss: 0.00001172
Iteration 63/1000 | Loss: 0.00001172
Iteration 64/1000 | Loss: 0.00001172
Iteration 65/1000 | Loss: 0.00001172
Iteration 66/1000 | Loss: 0.00001172
Iteration 67/1000 | Loss: 0.00001172
Iteration 68/1000 | Loss: 0.00001172
Iteration 69/1000 | Loss: 0.00001172
Iteration 70/1000 | Loss: 0.00001172
Iteration 71/1000 | Loss: 0.00001172
Iteration 72/1000 | Loss: 0.00001172
Iteration 73/1000 | Loss: 0.00001172
Iteration 74/1000 | Loss: 0.00001172
Iteration 75/1000 | Loss: 0.00001172
Iteration 76/1000 | Loss: 0.00001172
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 76. Stopping optimization.
Last 5 losses: [1.1720242582669016e-05, 1.1720242582669016e-05, 1.1720242582669016e-05, 1.1720242582669016e-05, 1.1720242582669016e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1720242582669016e-05

Optimization complete. Final v2v error: 2.926363468170166 mm

Highest mean error: 3.714808464050293 mm for frame 54

Lowest mean error: 2.766179084777832 mm for frame 152

Saving results

Total time: 27.84915041923523
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alison_posed_001/1009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alison_posed_001/1009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alison_posed_001/1009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00625622
Iteration 2/25 | Loss: 0.00136100
Iteration 3/25 | Loss: 0.00127229
Iteration 4/25 | Loss: 0.00125365
Iteration 5/25 | Loss: 0.00124792
Iteration 6/25 | Loss: 0.00124693
Iteration 7/25 | Loss: 0.00124693
Iteration 8/25 | Loss: 0.00124693
Iteration 9/25 | Loss: 0.00124693
Iteration 10/25 | Loss: 0.00124693
Iteration 11/25 | Loss: 0.00124693
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012469326611608267, 0.0012469326611608267, 0.0012469326611608267, 0.0012469326611608267, 0.0012469326611608267]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012469326611608267

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.66653776
Iteration 2/25 | Loss: 0.00079367
Iteration 3/25 | Loss: 0.00079367
Iteration 4/25 | Loss: 0.00079367
Iteration 5/25 | Loss: 0.00079366
Iteration 6/25 | Loss: 0.00079366
Iteration 7/25 | Loss: 0.00079366
Iteration 8/25 | Loss: 0.00079366
Iteration 9/25 | Loss: 0.00079366
Iteration 10/25 | Loss: 0.00079366
Iteration 11/25 | Loss: 0.00079366
Iteration 12/25 | Loss: 0.00079366
Iteration 13/25 | Loss: 0.00079366
Iteration 14/25 | Loss: 0.00079366
Iteration 15/25 | Loss: 0.00079366
Iteration 16/25 | Loss: 0.00079366
Iteration 17/25 | Loss: 0.00079366
Iteration 18/25 | Loss: 0.00079366
Iteration 19/25 | Loss: 0.00079366
Iteration 20/25 | Loss: 0.00079366
Iteration 21/25 | Loss: 0.00079366
Iteration 22/25 | Loss: 0.00079366
Iteration 23/25 | Loss: 0.00079366
Iteration 24/25 | Loss: 0.00079366
Iteration 25/25 | Loss: 0.00079366

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00079366
Iteration 2/1000 | Loss: 0.00002839
Iteration 3/1000 | Loss: 0.00002153
Iteration 4/1000 | Loss: 0.00001977
Iteration 5/1000 | Loss: 0.00001884
Iteration 6/1000 | Loss: 0.00001834
Iteration 7/1000 | Loss: 0.00001798
Iteration 8/1000 | Loss: 0.00001770
Iteration 9/1000 | Loss: 0.00001747
Iteration 10/1000 | Loss: 0.00001723
Iteration 11/1000 | Loss: 0.00001717
Iteration 12/1000 | Loss: 0.00001716
Iteration 13/1000 | Loss: 0.00001707
Iteration 14/1000 | Loss: 0.00001690
Iteration 15/1000 | Loss: 0.00001687
Iteration 16/1000 | Loss: 0.00001685
Iteration 17/1000 | Loss: 0.00001684
Iteration 18/1000 | Loss: 0.00001684
Iteration 19/1000 | Loss: 0.00001680
Iteration 20/1000 | Loss: 0.00001680
Iteration 21/1000 | Loss: 0.00001679
Iteration 22/1000 | Loss: 0.00001675
Iteration 23/1000 | Loss: 0.00001671
Iteration 24/1000 | Loss: 0.00001671
Iteration 25/1000 | Loss: 0.00001665
Iteration 26/1000 | Loss: 0.00001665
Iteration 27/1000 | Loss: 0.00001662
Iteration 28/1000 | Loss: 0.00001662
Iteration 29/1000 | Loss: 0.00001660
Iteration 30/1000 | Loss: 0.00001660
Iteration 31/1000 | Loss: 0.00001659
Iteration 32/1000 | Loss: 0.00001659
Iteration 33/1000 | Loss: 0.00001659
Iteration 34/1000 | Loss: 0.00001658
Iteration 35/1000 | Loss: 0.00001657
Iteration 36/1000 | Loss: 0.00001657
Iteration 37/1000 | Loss: 0.00001655
Iteration 38/1000 | Loss: 0.00001655
Iteration 39/1000 | Loss: 0.00001655
Iteration 40/1000 | Loss: 0.00001654
Iteration 41/1000 | Loss: 0.00001654
Iteration 42/1000 | Loss: 0.00001654
Iteration 43/1000 | Loss: 0.00001654
Iteration 44/1000 | Loss: 0.00001653
Iteration 45/1000 | Loss: 0.00001651
Iteration 46/1000 | Loss: 0.00001651
Iteration 47/1000 | Loss: 0.00001650
Iteration 48/1000 | Loss: 0.00001650
Iteration 49/1000 | Loss: 0.00001649
Iteration 50/1000 | Loss: 0.00001649
Iteration 51/1000 | Loss: 0.00001649
Iteration 52/1000 | Loss: 0.00001648
Iteration 53/1000 | Loss: 0.00001648
Iteration 54/1000 | Loss: 0.00001648
Iteration 55/1000 | Loss: 0.00001647
Iteration 56/1000 | Loss: 0.00001647
Iteration 57/1000 | Loss: 0.00001646
Iteration 58/1000 | Loss: 0.00001646
Iteration 59/1000 | Loss: 0.00001646
Iteration 60/1000 | Loss: 0.00001646
Iteration 61/1000 | Loss: 0.00001645
Iteration 62/1000 | Loss: 0.00001645
Iteration 63/1000 | Loss: 0.00001645
Iteration 64/1000 | Loss: 0.00001645
Iteration 65/1000 | Loss: 0.00001644
Iteration 66/1000 | Loss: 0.00001644
Iteration 67/1000 | Loss: 0.00001643
Iteration 68/1000 | Loss: 0.00001643
Iteration 69/1000 | Loss: 0.00001643
Iteration 70/1000 | Loss: 0.00001643
Iteration 71/1000 | Loss: 0.00001643
Iteration 72/1000 | Loss: 0.00001643
Iteration 73/1000 | Loss: 0.00001643
Iteration 74/1000 | Loss: 0.00001643
Iteration 75/1000 | Loss: 0.00001643
Iteration 76/1000 | Loss: 0.00001642
Iteration 77/1000 | Loss: 0.00001642
Iteration 78/1000 | Loss: 0.00001642
Iteration 79/1000 | Loss: 0.00001642
Iteration 80/1000 | Loss: 0.00001642
Iteration 81/1000 | Loss: 0.00001642
Iteration 82/1000 | Loss: 0.00001642
Iteration 83/1000 | Loss: 0.00001642
Iteration 84/1000 | Loss: 0.00001642
Iteration 85/1000 | Loss: 0.00001642
Iteration 86/1000 | Loss: 0.00001642
Iteration 87/1000 | Loss: 0.00001642
Iteration 88/1000 | Loss: 0.00001642
Iteration 89/1000 | Loss: 0.00001641
Iteration 90/1000 | Loss: 0.00001641
Iteration 91/1000 | Loss: 0.00001641
Iteration 92/1000 | Loss: 0.00001641
Iteration 93/1000 | Loss: 0.00001641
Iteration 94/1000 | Loss: 0.00001641
Iteration 95/1000 | Loss: 0.00001641
Iteration 96/1000 | Loss: 0.00001641
Iteration 97/1000 | Loss: 0.00001641
Iteration 98/1000 | Loss: 0.00001641
Iteration 99/1000 | Loss: 0.00001641
Iteration 100/1000 | Loss: 0.00001640
Iteration 101/1000 | Loss: 0.00001640
Iteration 102/1000 | Loss: 0.00001640
Iteration 103/1000 | Loss: 0.00001640
Iteration 104/1000 | Loss: 0.00001640
Iteration 105/1000 | Loss: 0.00001640
Iteration 106/1000 | Loss: 0.00001640
Iteration 107/1000 | Loss: 0.00001640
Iteration 108/1000 | Loss: 0.00001640
Iteration 109/1000 | Loss: 0.00001639
Iteration 110/1000 | Loss: 0.00001639
Iteration 111/1000 | Loss: 0.00001639
Iteration 112/1000 | Loss: 0.00001639
Iteration 113/1000 | Loss: 0.00001639
Iteration 114/1000 | Loss: 0.00001639
Iteration 115/1000 | Loss: 0.00001639
Iteration 116/1000 | Loss: 0.00001639
Iteration 117/1000 | Loss: 0.00001639
Iteration 118/1000 | Loss: 0.00001639
Iteration 119/1000 | Loss: 0.00001639
Iteration 120/1000 | Loss: 0.00001639
Iteration 121/1000 | Loss: 0.00001639
Iteration 122/1000 | Loss: 0.00001639
Iteration 123/1000 | Loss: 0.00001639
Iteration 124/1000 | Loss: 0.00001639
Iteration 125/1000 | Loss: 0.00001639
Iteration 126/1000 | Loss: 0.00001639
Iteration 127/1000 | Loss: 0.00001639
Iteration 128/1000 | Loss: 0.00001639
Iteration 129/1000 | Loss: 0.00001639
Iteration 130/1000 | Loss: 0.00001639
Iteration 131/1000 | Loss: 0.00001639
Iteration 132/1000 | Loss: 0.00001639
Iteration 133/1000 | Loss: 0.00001639
Iteration 134/1000 | Loss: 0.00001639
Iteration 135/1000 | Loss: 0.00001639
Iteration 136/1000 | Loss: 0.00001639
Iteration 137/1000 | Loss: 0.00001639
Iteration 138/1000 | Loss: 0.00001639
Iteration 139/1000 | Loss: 0.00001639
Iteration 140/1000 | Loss: 0.00001639
Iteration 141/1000 | Loss: 0.00001639
Iteration 142/1000 | Loss: 0.00001639
Iteration 143/1000 | Loss: 0.00001639
Iteration 144/1000 | Loss: 0.00001639
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 144. Stopping optimization.
Last 5 losses: [1.6388010408263654e-05, 1.6388010408263654e-05, 1.6388010408263654e-05, 1.6388010408263654e-05, 1.6388010408263654e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6388010408263654e-05

Optimization complete. Final v2v error: 3.387960910797119 mm

Highest mean error: 3.8338022232055664 mm for frame 89

Lowest mean error: 3.152629852294922 mm for frame 115

Saving results

Total time: 35.81827640533447
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alison_posed_001/1032/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alison_posed_001/1032.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alison_posed_001/1032
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00827154
Iteration 2/25 | Loss: 0.00137287
Iteration 3/25 | Loss: 0.00126672
Iteration 4/25 | Loss: 0.00125530
Iteration 5/25 | Loss: 0.00125219
Iteration 6/25 | Loss: 0.00125219
Iteration 7/25 | Loss: 0.00125219
Iteration 8/25 | Loss: 0.00125219
Iteration 9/25 | Loss: 0.00125219
Iteration 10/25 | Loss: 0.00125219
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0012521934695541859, 0.0012521934695541859, 0.0012521934695541859, 0.0012521934695541859, 0.0012521934695541859]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012521934695541859

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.84532464
Iteration 2/25 | Loss: 0.00084518
Iteration 3/25 | Loss: 0.00084518
Iteration 4/25 | Loss: 0.00084518
Iteration 5/25 | Loss: 0.00084517
Iteration 6/25 | Loss: 0.00084517
Iteration 7/25 | Loss: 0.00084517
Iteration 8/25 | Loss: 0.00084517
Iteration 9/25 | Loss: 0.00084517
Iteration 10/25 | Loss: 0.00084517
Iteration 11/25 | Loss: 0.00084517
Iteration 12/25 | Loss: 0.00084517
Iteration 13/25 | Loss: 0.00084517
Iteration 14/25 | Loss: 0.00084517
Iteration 15/25 | Loss: 0.00084517
Iteration 16/25 | Loss: 0.00084517
Iteration 17/25 | Loss: 0.00084517
Iteration 18/25 | Loss: 0.00084517
Iteration 19/25 | Loss: 0.00084517
Iteration 20/25 | Loss: 0.00084517
Iteration 21/25 | Loss: 0.00084517
Iteration 22/25 | Loss: 0.00084517
Iteration 23/25 | Loss: 0.00084517
Iteration 24/25 | Loss: 0.00084517
Iteration 25/25 | Loss: 0.00084517

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00084517
Iteration 2/1000 | Loss: 0.00002428
Iteration 3/1000 | Loss: 0.00001953
Iteration 4/1000 | Loss: 0.00001769
Iteration 5/1000 | Loss: 0.00001679
Iteration 6/1000 | Loss: 0.00001619
Iteration 7/1000 | Loss: 0.00001581
Iteration 8/1000 | Loss: 0.00001554
Iteration 9/1000 | Loss: 0.00001526
Iteration 10/1000 | Loss: 0.00001509
Iteration 11/1000 | Loss: 0.00001508
Iteration 12/1000 | Loss: 0.00001498
Iteration 13/1000 | Loss: 0.00001488
Iteration 14/1000 | Loss: 0.00001482
Iteration 15/1000 | Loss: 0.00001479
Iteration 16/1000 | Loss: 0.00001478
Iteration 17/1000 | Loss: 0.00001473
Iteration 18/1000 | Loss: 0.00001473
Iteration 19/1000 | Loss: 0.00001472
Iteration 20/1000 | Loss: 0.00001471
Iteration 21/1000 | Loss: 0.00001471
Iteration 22/1000 | Loss: 0.00001470
Iteration 23/1000 | Loss: 0.00001470
Iteration 24/1000 | Loss: 0.00001468
Iteration 25/1000 | Loss: 0.00001464
Iteration 26/1000 | Loss: 0.00001463
Iteration 27/1000 | Loss: 0.00001462
Iteration 28/1000 | Loss: 0.00001459
Iteration 29/1000 | Loss: 0.00001458
Iteration 30/1000 | Loss: 0.00001458
Iteration 31/1000 | Loss: 0.00001457
Iteration 32/1000 | Loss: 0.00001457
Iteration 33/1000 | Loss: 0.00001456
Iteration 34/1000 | Loss: 0.00001456
Iteration 35/1000 | Loss: 0.00001455
Iteration 36/1000 | Loss: 0.00001455
Iteration 37/1000 | Loss: 0.00001452
Iteration 38/1000 | Loss: 0.00001452
Iteration 39/1000 | Loss: 0.00001448
Iteration 40/1000 | Loss: 0.00001445
Iteration 41/1000 | Loss: 0.00001444
Iteration 42/1000 | Loss: 0.00001444
Iteration 43/1000 | Loss: 0.00001444
Iteration 44/1000 | Loss: 0.00001444
Iteration 45/1000 | Loss: 0.00001443
Iteration 46/1000 | Loss: 0.00001443
Iteration 47/1000 | Loss: 0.00001442
Iteration 48/1000 | Loss: 0.00001441
Iteration 49/1000 | Loss: 0.00001441
Iteration 50/1000 | Loss: 0.00001441
Iteration 51/1000 | Loss: 0.00001441
Iteration 52/1000 | Loss: 0.00001441
Iteration 53/1000 | Loss: 0.00001440
Iteration 54/1000 | Loss: 0.00001440
Iteration 55/1000 | Loss: 0.00001440
Iteration 56/1000 | Loss: 0.00001439
Iteration 57/1000 | Loss: 0.00001439
Iteration 58/1000 | Loss: 0.00001439
Iteration 59/1000 | Loss: 0.00001438
Iteration 60/1000 | Loss: 0.00001438
Iteration 61/1000 | Loss: 0.00001438
Iteration 62/1000 | Loss: 0.00001438
Iteration 63/1000 | Loss: 0.00001437
Iteration 64/1000 | Loss: 0.00001437
Iteration 65/1000 | Loss: 0.00001437
Iteration 66/1000 | Loss: 0.00001437
Iteration 67/1000 | Loss: 0.00001437
Iteration 68/1000 | Loss: 0.00001437
Iteration 69/1000 | Loss: 0.00001436
Iteration 70/1000 | Loss: 0.00001436
Iteration 71/1000 | Loss: 0.00001436
Iteration 72/1000 | Loss: 0.00001436
Iteration 73/1000 | Loss: 0.00001436
Iteration 74/1000 | Loss: 0.00001435
Iteration 75/1000 | Loss: 0.00001435
Iteration 76/1000 | Loss: 0.00001434
Iteration 77/1000 | Loss: 0.00001434
Iteration 78/1000 | Loss: 0.00001434
Iteration 79/1000 | Loss: 0.00001434
Iteration 80/1000 | Loss: 0.00001434
Iteration 81/1000 | Loss: 0.00001433
Iteration 82/1000 | Loss: 0.00001433
Iteration 83/1000 | Loss: 0.00001433
Iteration 84/1000 | Loss: 0.00001432
Iteration 85/1000 | Loss: 0.00001432
Iteration 86/1000 | Loss: 0.00001432
Iteration 87/1000 | Loss: 0.00001432
Iteration 88/1000 | Loss: 0.00001431
Iteration 89/1000 | Loss: 0.00001431
Iteration 90/1000 | Loss: 0.00001431
Iteration 91/1000 | Loss: 0.00001431
Iteration 92/1000 | Loss: 0.00001430
Iteration 93/1000 | Loss: 0.00001430
Iteration 94/1000 | Loss: 0.00001430
Iteration 95/1000 | Loss: 0.00001430
Iteration 96/1000 | Loss: 0.00001430
Iteration 97/1000 | Loss: 0.00001430
Iteration 98/1000 | Loss: 0.00001430
Iteration 99/1000 | Loss: 0.00001430
Iteration 100/1000 | Loss: 0.00001430
Iteration 101/1000 | Loss: 0.00001429
Iteration 102/1000 | Loss: 0.00001429
Iteration 103/1000 | Loss: 0.00001429
Iteration 104/1000 | Loss: 0.00001429
Iteration 105/1000 | Loss: 0.00001429
Iteration 106/1000 | Loss: 0.00001429
Iteration 107/1000 | Loss: 0.00001429
Iteration 108/1000 | Loss: 0.00001429
Iteration 109/1000 | Loss: 0.00001429
Iteration 110/1000 | Loss: 0.00001429
Iteration 111/1000 | Loss: 0.00001429
Iteration 112/1000 | Loss: 0.00001428
Iteration 113/1000 | Loss: 0.00001428
Iteration 114/1000 | Loss: 0.00001428
Iteration 115/1000 | Loss: 0.00001428
Iteration 116/1000 | Loss: 0.00001428
Iteration 117/1000 | Loss: 0.00001428
Iteration 118/1000 | Loss: 0.00001428
Iteration 119/1000 | Loss: 0.00001428
Iteration 120/1000 | Loss: 0.00001428
Iteration 121/1000 | Loss: 0.00001428
Iteration 122/1000 | Loss: 0.00001428
Iteration 123/1000 | Loss: 0.00001428
Iteration 124/1000 | Loss: 0.00001428
Iteration 125/1000 | Loss: 0.00001428
Iteration 126/1000 | Loss: 0.00001428
Iteration 127/1000 | Loss: 0.00001428
Iteration 128/1000 | Loss: 0.00001428
Iteration 129/1000 | Loss: 0.00001428
Iteration 130/1000 | Loss: 0.00001428
Iteration 131/1000 | Loss: 0.00001428
Iteration 132/1000 | Loss: 0.00001428
Iteration 133/1000 | Loss: 0.00001428
Iteration 134/1000 | Loss: 0.00001428
Iteration 135/1000 | Loss: 0.00001428
Iteration 136/1000 | Loss: 0.00001428
Iteration 137/1000 | Loss: 0.00001428
Iteration 138/1000 | Loss: 0.00001428
Iteration 139/1000 | Loss: 0.00001428
Iteration 140/1000 | Loss: 0.00001428
Iteration 141/1000 | Loss: 0.00001428
Iteration 142/1000 | Loss: 0.00001428
Iteration 143/1000 | Loss: 0.00001428
Iteration 144/1000 | Loss: 0.00001428
Iteration 145/1000 | Loss: 0.00001428
Iteration 146/1000 | Loss: 0.00001428
Iteration 147/1000 | Loss: 0.00001428
Iteration 148/1000 | Loss: 0.00001428
Iteration 149/1000 | Loss: 0.00001428
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 149. Stopping optimization.
Last 5 losses: [1.4276693036663346e-05, 1.4276693036663346e-05, 1.4276693036663346e-05, 1.4276693036663346e-05, 1.4276693036663346e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4276693036663346e-05

Optimization complete. Final v2v error: 3.2072415351867676 mm

Highest mean error: 3.972170352935791 mm for frame 105

Lowest mean error: 2.96490216255188 mm for frame 71

Saving results

Total time: 37.04091548919678
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alison_posed_001/1058/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alison_posed_001/1058.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alison_posed_001/1058
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00502845
Iteration 2/25 | Loss: 0.00134913
Iteration 3/25 | Loss: 0.00126494
Iteration 4/25 | Loss: 0.00124894
Iteration 5/25 | Loss: 0.00124574
Iteration 6/25 | Loss: 0.00124460
Iteration 7/25 | Loss: 0.00124460
Iteration 8/25 | Loss: 0.00124460
Iteration 9/25 | Loss: 0.00124457
Iteration 10/25 | Loss: 0.00124457
Iteration 11/25 | Loss: 0.00124457
Iteration 12/25 | Loss: 0.00124457
Iteration 13/25 | Loss: 0.00124457
Iteration 14/25 | Loss: 0.00124457
Iteration 15/25 | Loss: 0.00124457
Iteration 16/25 | Loss: 0.00124457
Iteration 17/25 | Loss: 0.00124457
Iteration 18/25 | Loss: 0.00124457
Iteration 19/25 | Loss: 0.00124457
Iteration 20/25 | Loss: 0.00124457
Iteration 21/25 | Loss: 0.00124457
Iteration 22/25 | Loss: 0.00124457
Iteration 23/25 | Loss: 0.00124457
Iteration 24/25 | Loss: 0.00124457
Iteration 25/25 | Loss: 0.00124457

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.07604575
Iteration 2/25 | Loss: 0.00059775
Iteration 3/25 | Loss: 0.00059770
Iteration 4/25 | Loss: 0.00059770
Iteration 5/25 | Loss: 0.00059770
Iteration 6/25 | Loss: 0.00059770
Iteration 7/25 | Loss: 0.00059769
Iteration 8/25 | Loss: 0.00059769
Iteration 9/25 | Loss: 0.00059769
Iteration 10/25 | Loss: 0.00059769
Iteration 11/25 | Loss: 0.00059769
Iteration 12/25 | Loss: 0.00059769
Iteration 13/25 | Loss: 0.00059769
Iteration 14/25 | Loss: 0.00059769
Iteration 15/25 | Loss: 0.00059769
Iteration 16/25 | Loss: 0.00059769
Iteration 17/25 | Loss: 0.00059769
Iteration 18/25 | Loss: 0.00059769
Iteration 19/25 | Loss: 0.00059769
Iteration 20/25 | Loss: 0.00059769
Iteration 21/25 | Loss: 0.00059769
Iteration 22/25 | Loss: 0.00059769
Iteration 23/25 | Loss: 0.00059769
Iteration 24/25 | Loss: 0.00059769
Iteration 25/25 | Loss: 0.00059769

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00059769
Iteration 2/1000 | Loss: 0.00005030
Iteration 3/1000 | Loss: 0.00003452
Iteration 4/1000 | Loss: 0.00002852
Iteration 5/1000 | Loss: 0.00002702
Iteration 6/1000 | Loss: 0.00002560
Iteration 7/1000 | Loss: 0.00002463
Iteration 8/1000 | Loss: 0.00002381
Iteration 9/1000 | Loss: 0.00002353
Iteration 10/1000 | Loss: 0.00002309
Iteration 11/1000 | Loss: 0.00002280
Iteration 12/1000 | Loss: 0.00002248
Iteration 13/1000 | Loss: 0.00002214
Iteration 14/1000 | Loss: 0.00002192
Iteration 15/1000 | Loss: 0.00002189
Iteration 16/1000 | Loss: 0.00002156
Iteration 17/1000 | Loss: 0.00002140
Iteration 18/1000 | Loss: 0.00002139
Iteration 19/1000 | Loss: 0.00002129
Iteration 20/1000 | Loss: 0.00002128
Iteration 21/1000 | Loss: 0.00002118
Iteration 22/1000 | Loss: 0.00002118
Iteration 23/1000 | Loss: 0.00002117
Iteration 24/1000 | Loss: 0.00002114
Iteration 25/1000 | Loss: 0.00002108
Iteration 26/1000 | Loss: 0.00002107
Iteration 27/1000 | Loss: 0.00002107
Iteration 28/1000 | Loss: 0.00002106
Iteration 29/1000 | Loss: 0.00002106
Iteration 30/1000 | Loss: 0.00002105
Iteration 31/1000 | Loss: 0.00002105
Iteration 32/1000 | Loss: 0.00002102
Iteration 33/1000 | Loss: 0.00002101
Iteration 34/1000 | Loss: 0.00002099
Iteration 35/1000 | Loss: 0.00002098
Iteration 36/1000 | Loss: 0.00002098
Iteration 37/1000 | Loss: 0.00002098
Iteration 38/1000 | Loss: 0.00002097
Iteration 39/1000 | Loss: 0.00002097
Iteration 40/1000 | Loss: 0.00002096
Iteration 41/1000 | Loss: 0.00002096
Iteration 42/1000 | Loss: 0.00002096
Iteration 43/1000 | Loss: 0.00002096
Iteration 44/1000 | Loss: 0.00002096
Iteration 45/1000 | Loss: 0.00002096
Iteration 46/1000 | Loss: 0.00002096
Iteration 47/1000 | Loss: 0.00002096
Iteration 48/1000 | Loss: 0.00002096
Iteration 49/1000 | Loss: 0.00002096
Iteration 50/1000 | Loss: 0.00002096
Iteration 51/1000 | Loss: 0.00002095
Iteration 52/1000 | Loss: 0.00002095
Iteration 53/1000 | Loss: 0.00002095
Iteration 54/1000 | Loss: 0.00002095
Iteration 55/1000 | Loss: 0.00002095
Iteration 56/1000 | Loss: 0.00002095
Iteration 57/1000 | Loss: 0.00002095
Iteration 58/1000 | Loss: 0.00002095
Iteration 59/1000 | Loss: 0.00002095
Iteration 60/1000 | Loss: 0.00002095
Iteration 61/1000 | Loss: 0.00002095
Iteration 62/1000 | Loss: 0.00002095
Iteration 63/1000 | Loss: 0.00002095
Iteration 64/1000 | Loss: 0.00002094
Iteration 65/1000 | Loss: 0.00002094
Iteration 66/1000 | Loss: 0.00002094
Iteration 67/1000 | Loss: 0.00002094
Iteration 68/1000 | Loss: 0.00002094
Iteration 69/1000 | Loss: 0.00002094
Iteration 70/1000 | Loss: 0.00002094
Iteration 71/1000 | Loss: 0.00002094
Iteration 72/1000 | Loss: 0.00002094
Iteration 73/1000 | Loss: 0.00002094
Iteration 74/1000 | Loss: 0.00002094
Iteration 75/1000 | Loss: 0.00002093
Iteration 76/1000 | Loss: 0.00002093
Iteration 77/1000 | Loss: 0.00002093
Iteration 78/1000 | Loss: 0.00002093
Iteration 79/1000 | Loss: 0.00002093
Iteration 80/1000 | Loss: 0.00002092
Iteration 81/1000 | Loss: 0.00002092
Iteration 82/1000 | Loss: 0.00002092
Iteration 83/1000 | Loss: 0.00002092
Iteration 84/1000 | Loss: 0.00002091
Iteration 85/1000 | Loss: 0.00002091
Iteration 86/1000 | Loss: 0.00002091
Iteration 87/1000 | Loss: 0.00002091
Iteration 88/1000 | Loss: 0.00002091
Iteration 89/1000 | Loss: 0.00002091
Iteration 90/1000 | Loss: 0.00002091
Iteration 91/1000 | Loss: 0.00002091
Iteration 92/1000 | Loss: 0.00002091
Iteration 93/1000 | Loss: 0.00002091
Iteration 94/1000 | Loss: 0.00002091
Iteration 95/1000 | Loss: 0.00002090
Iteration 96/1000 | Loss: 0.00002090
Iteration 97/1000 | Loss: 0.00002089
Iteration 98/1000 | Loss: 0.00002089
Iteration 99/1000 | Loss: 0.00002089
Iteration 100/1000 | Loss: 0.00002089
Iteration 101/1000 | Loss: 0.00002089
Iteration 102/1000 | Loss: 0.00002089
Iteration 103/1000 | Loss: 0.00002089
Iteration 104/1000 | Loss: 0.00002089
Iteration 105/1000 | Loss: 0.00002089
Iteration 106/1000 | Loss: 0.00002089
Iteration 107/1000 | Loss: 0.00002089
Iteration 108/1000 | Loss: 0.00002089
Iteration 109/1000 | Loss: 0.00002089
Iteration 110/1000 | Loss: 0.00002089
Iteration 111/1000 | Loss: 0.00002089
Iteration 112/1000 | Loss: 0.00002089
Iteration 113/1000 | Loss: 0.00002089
Iteration 114/1000 | Loss: 0.00002089
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 114. Stopping optimization.
Last 5 losses: [2.0889954612357542e-05, 2.0889954612357542e-05, 2.0889954612357542e-05, 2.0889954612357542e-05, 2.0889954612357542e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0889954612357542e-05

Optimization complete. Final v2v error: 3.834540605545044 mm

Highest mean error: 3.8661534786224365 mm for frame 10

Lowest mean error: 3.8066136837005615 mm for frame 54

Saving results

Total time: 40.32559323310852
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alison_posed_001/1044/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alison_posed_001/1044.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alison_posed_001/1044
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00794549
Iteration 2/25 | Loss: 0.00148974
Iteration 3/25 | Loss: 0.00131033
Iteration 4/25 | Loss: 0.00128490
Iteration 5/25 | Loss: 0.00127476
Iteration 6/25 | Loss: 0.00127349
Iteration 7/25 | Loss: 0.00127349
Iteration 8/25 | Loss: 0.00127349
Iteration 9/25 | Loss: 0.00127349
Iteration 10/25 | Loss: 0.00127349
Iteration 11/25 | Loss: 0.00127349
Iteration 12/25 | Loss: 0.00127349
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0012734865304082632, 0.0012734865304082632, 0.0012734865304082632, 0.0012734865304082632, 0.0012734865304082632]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012734865304082632

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.87744045
Iteration 2/25 | Loss: 0.00061995
Iteration 3/25 | Loss: 0.00061995
Iteration 4/25 | Loss: 0.00061995
Iteration 5/25 | Loss: 0.00061995
Iteration 6/25 | Loss: 0.00061995
Iteration 7/25 | Loss: 0.00061995
Iteration 8/25 | Loss: 0.00061995
Iteration 9/25 | Loss: 0.00061995
Iteration 10/25 | Loss: 0.00061995
Iteration 11/25 | Loss: 0.00061995
Iteration 12/25 | Loss: 0.00061995
Iteration 13/25 | Loss: 0.00061995
Iteration 14/25 | Loss: 0.00061995
Iteration 15/25 | Loss: 0.00061995
Iteration 16/25 | Loss: 0.00061995
Iteration 17/25 | Loss: 0.00061995
Iteration 18/25 | Loss: 0.00061995
Iteration 19/25 | Loss: 0.00061995
Iteration 20/25 | Loss: 0.00061995
Iteration 21/25 | Loss: 0.00061995
Iteration 22/25 | Loss: 0.00061995
Iteration 23/25 | Loss: 0.00061995
Iteration 24/25 | Loss: 0.00061995
Iteration 25/25 | Loss: 0.00061995

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00061995
Iteration 2/1000 | Loss: 0.00004365
Iteration 3/1000 | Loss: 0.00003229
Iteration 4/1000 | Loss: 0.00002848
Iteration 5/1000 | Loss: 0.00002663
Iteration 6/1000 | Loss: 0.00002498
Iteration 7/1000 | Loss: 0.00002423
Iteration 8/1000 | Loss: 0.00002374
Iteration 9/1000 | Loss: 0.00002322
Iteration 10/1000 | Loss: 0.00002274
Iteration 11/1000 | Loss: 0.00002248
Iteration 12/1000 | Loss: 0.00002232
Iteration 13/1000 | Loss: 0.00002232
Iteration 14/1000 | Loss: 0.00002222
Iteration 15/1000 | Loss: 0.00002217
Iteration 16/1000 | Loss: 0.00002216
Iteration 17/1000 | Loss: 0.00002205
Iteration 18/1000 | Loss: 0.00002202
Iteration 19/1000 | Loss: 0.00002201
Iteration 20/1000 | Loss: 0.00002201
Iteration 21/1000 | Loss: 0.00002200
Iteration 22/1000 | Loss: 0.00002200
Iteration 23/1000 | Loss: 0.00002193
Iteration 24/1000 | Loss: 0.00002190
Iteration 25/1000 | Loss: 0.00002189
Iteration 26/1000 | Loss: 0.00002188
Iteration 27/1000 | Loss: 0.00002187
Iteration 28/1000 | Loss: 0.00002187
Iteration 29/1000 | Loss: 0.00002184
Iteration 30/1000 | Loss: 0.00002184
Iteration 31/1000 | Loss: 0.00002184
Iteration 32/1000 | Loss: 0.00002184
Iteration 33/1000 | Loss: 0.00002184
Iteration 34/1000 | Loss: 0.00002183
Iteration 35/1000 | Loss: 0.00002183
Iteration 36/1000 | Loss: 0.00002183
Iteration 37/1000 | Loss: 0.00002183
Iteration 38/1000 | Loss: 0.00002183
Iteration 39/1000 | Loss: 0.00002183
Iteration 40/1000 | Loss: 0.00002183
Iteration 41/1000 | Loss: 0.00002182
Iteration 42/1000 | Loss: 0.00002182
Iteration 43/1000 | Loss: 0.00002181
Iteration 44/1000 | Loss: 0.00002181
Iteration 45/1000 | Loss: 0.00002181
Iteration 46/1000 | Loss: 0.00002181
Iteration 47/1000 | Loss: 0.00002181
Iteration 48/1000 | Loss: 0.00002181
Iteration 49/1000 | Loss: 0.00002180
Iteration 50/1000 | Loss: 0.00002180
Iteration 51/1000 | Loss: 0.00002180
Iteration 52/1000 | Loss: 0.00002179
Iteration 53/1000 | Loss: 0.00002179
Iteration 54/1000 | Loss: 0.00002179
Iteration 55/1000 | Loss: 0.00002178
Iteration 56/1000 | Loss: 0.00002178
Iteration 57/1000 | Loss: 0.00002178
Iteration 58/1000 | Loss: 0.00002178
Iteration 59/1000 | Loss: 0.00002178
Iteration 60/1000 | Loss: 0.00002178
Iteration 61/1000 | Loss: 0.00002177
Iteration 62/1000 | Loss: 0.00002177
Iteration 63/1000 | Loss: 0.00002177
Iteration 64/1000 | Loss: 0.00002177
Iteration 65/1000 | Loss: 0.00002177
Iteration 66/1000 | Loss: 0.00002177
Iteration 67/1000 | Loss: 0.00002177
Iteration 68/1000 | Loss: 0.00002177
Iteration 69/1000 | Loss: 0.00002176
Iteration 70/1000 | Loss: 0.00002176
Iteration 71/1000 | Loss: 0.00002176
Iteration 72/1000 | Loss: 0.00002175
Iteration 73/1000 | Loss: 0.00002175
Iteration 74/1000 | Loss: 0.00002175
Iteration 75/1000 | Loss: 0.00002174
Iteration 76/1000 | Loss: 0.00002174
Iteration 77/1000 | Loss: 0.00002174
Iteration 78/1000 | Loss: 0.00002174
Iteration 79/1000 | Loss: 0.00002174
Iteration 80/1000 | Loss: 0.00002174
Iteration 81/1000 | Loss: 0.00002174
Iteration 82/1000 | Loss: 0.00002174
Iteration 83/1000 | Loss: 0.00002174
Iteration 84/1000 | Loss: 0.00002174
Iteration 85/1000 | Loss: 0.00002174
Iteration 86/1000 | Loss: 0.00002174
Iteration 87/1000 | Loss: 0.00002174
Iteration 88/1000 | Loss: 0.00002173
Iteration 89/1000 | Loss: 0.00002173
Iteration 90/1000 | Loss: 0.00002173
Iteration 91/1000 | Loss: 0.00002173
Iteration 92/1000 | Loss: 0.00002173
Iteration 93/1000 | Loss: 0.00002172
Iteration 94/1000 | Loss: 0.00002172
Iteration 95/1000 | Loss: 0.00002172
Iteration 96/1000 | Loss: 0.00002172
Iteration 97/1000 | Loss: 0.00002172
Iteration 98/1000 | Loss: 0.00002172
Iteration 99/1000 | Loss: 0.00002172
Iteration 100/1000 | Loss: 0.00002172
Iteration 101/1000 | Loss: 0.00002171
Iteration 102/1000 | Loss: 0.00002171
Iteration 103/1000 | Loss: 0.00002171
Iteration 104/1000 | Loss: 0.00002171
Iteration 105/1000 | Loss: 0.00002171
Iteration 106/1000 | Loss: 0.00002171
Iteration 107/1000 | Loss: 0.00002170
Iteration 108/1000 | Loss: 0.00002170
Iteration 109/1000 | Loss: 0.00002170
Iteration 110/1000 | Loss: 0.00002170
Iteration 111/1000 | Loss: 0.00002170
Iteration 112/1000 | Loss: 0.00002170
Iteration 113/1000 | Loss: 0.00002170
Iteration 114/1000 | Loss: 0.00002170
Iteration 115/1000 | Loss: 0.00002170
Iteration 116/1000 | Loss: 0.00002170
Iteration 117/1000 | Loss: 0.00002170
Iteration 118/1000 | Loss: 0.00002170
Iteration 119/1000 | Loss: 0.00002170
Iteration 120/1000 | Loss: 0.00002170
Iteration 121/1000 | Loss: 0.00002170
Iteration 122/1000 | Loss: 0.00002170
Iteration 123/1000 | Loss: 0.00002170
Iteration 124/1000 | Loss: 0.00002170
Iteration 125/1000 | Loss: 0.00002170
Iteration 126/1000 | Loss: 0.00002170
Iteration 127/1000 | Loss: 0.00002170
Iteration 128/1000 | Loss: 0.00002170
Iteration 129/1000 | Loss: 0.00002170
Iteration 130/1000 | Loss: 0.00002170
Iteration 131/1000 | Loss: 0.00002170
Iteration 132/1000 | Loss: 0.00002170
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 132. Stopping optimization.
Last 5 losses: [2.1699763237847947e-05, 2.1699763237847947e-05, 2.1699763237847947e-05, 2.1699763237847947e-05, 2.1699763237847947e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1699763237847947e-05

Optimization complete. Final v2v error: 3.9516091346740723 mm

Highest mean error: 4.2856011390686035 mm for frame 99

Lowest mean error: 3.6037981510162354 mm for frame 42

Saving results

Total time: 41.779709815979004
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alison_posed_001/1077/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alison_posed_001/1077.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alison_posed_001/1077
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00870685
Iteration 2/25 | Loss: 0.00169953
Iteration 3/25 | Loss: 0.00142664
Iteration 4/25 | Loss: 0.00139561
Iteration 5/25 | Loss: 0.00138927
Iteration 6/25 | Loss: 0.00139155
Iteration 7/25 | Loss: 0.00137830
Iteration 8/25 | Loss: 0.00137538
Iteration 9/25 | Loss: 0.00136123
Iteration 10/25 | Loss: 0.00134839
Iteration 11/25 | Loss: 0.00133829
Iteration 12/25 | Loss: 0.00133224
Iteration 13/25 | Loss: 0.00133033
Iteration 14/25 | Loss: 0.00132988
Iteration 15/25 | Loss: 0.00132975
Iteration 16/25 | Loss: 0.00132972
Iteration 17/25 | Loss: 0.00132972
Iteration 18/25 | Loss: 0.00132971
Iteration 19/25 | Loss: 0.00132971
Iteration 20/25 | Loss: 0.00132970
Iteration 21/25 | Loss: 0.00132970
Iteration 22/25 | Loss: 0.00132970
Iteration 23/25 | Loss: 0.00132970
Iteration 24/25 | Loss: 0.00132970
Iteration 25/25 | Loss: 0.00132970

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.99109924
Iteration 2/25 | Loss: 0.00056073
Iteration 3/25 | Loss: 0.00056072
Iteration 4/25 | Loss: 0.00056072
Iteration 5/25 | Loss: 0.00056072
Iteration 6/25 | Loss: 0.00056072
Iteration 7/25 | Loss: 0.00056072
Iteration 8/25 | Loss: 0.00056072
Iteration 9/25 | Loss: 0.00056072
Iteration 10/25 | Loss: 0.00056072
Iteration 11/25 | Loss: 0.00056072
Iteration 12/25 | Loss: 0.00056072
Iteration 13/25 | Loss: 0.00056072
Iteration 14/25 | Loss: 0.00056072
Iteration 15/25 | Loss: 0.00056072
Iteration 16/25 | Loss: 0.00056072
Iteration 17/25 | Loss: 0.00056072
Iteration 18/25 | Loss: 0.00056072
Iteration 19/25 | Loss: 0.00056072
Iteration 20/25 | Loss: 0.00056072
Iteration 21/25 | Loss: 0.00056072
Iteration 22/25 | Loss: 0.00056072
Iteration 23/25 | Loss: 0.00056072
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.000560716784093529, 0.000560716784093529, 0.000560716784093529, 0.000560716784093529, 0.000560716784093529]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000560716784093529

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00056072
Iteration 2/1000 | Loss: 0.00005129
Iteration 3/1000 | Loss: 0.00004088
Iteration 4/1000 | Loss: 0.00003761
Iteration 5/1000 | Loss: 0.00003625
Iteration 6/1000 | Loss: 0.00003483
Iteration 7/1000 | Loss: 0.00003381
Iteration 8/1000 | Loss: 0.00003336
Iteration 9/1000 | Loss: 0.00003284
Iteration 10/1000 | Loss: 0.00003244
Iteration 11/1000 | Loss: 0.00003208
Iteration 12/1000 | Loss: 0.00003190
Iteration 13/1000 | Loss: 0.00003182
Iteration 14/1000 | Loss: 0.00003181
Iteration 15/1000 | Loss: 0.00003181
Iteration 16/1000 | Loss: 0.00003180
Iteration 17/1000 | Loss: 0.00003164
Iteration 18/1000 | Loss: 0.00003162
Iteration 19/1000 | Loss: 0.00003157
Iteration 20/1000 | Loss: 0.00003154
Iteration 21/1000 | Loss: 0.00003153
Iteration 22/1000 | Loss: 0.00003152
Iteration 23/1000 | Loss: 0.00003152
Iteration 24/1000 | Loss: 0.00003152
Iteration 25/1000 | Loss: 0.00003152
Iteration 26/1000 | Loss: 0.00003152
Iteration 27/1000 | Loss: 0.00003152
Iteration 28/1000 | Loss: 0.00003152
Iteration 29/1000 | Loss: 0.00003152
Iteration 30/1000 | Loss: 0.00003152
Iteration 31/1000 | Loss: 0.00003152
Iteration 32/1000 | Loss: 0.00003152
Iteration 33/1000 | Loss: 0.00003152
Iteration 34/1000 | Loss: 0.00003151
Iteration 35/1000 | Loss: 0.00003151
Iteration 36/1000 | Loss: 0.00003151
Iteration 37/1000 | Loss: 0.00003151
Iteration 38/1000 | Loss: 0.00003151
Iteration 39/1000 | Loss: 0.00003151
Iteration 40/1000 | Loss: 0.00003151
Iteration 41/1000 | Loss: 0.00003151
Iteration 42/1000 | Loss: 0.00003151
Iteration 43/1000 | Loss: 0.00003151
Iteration 44/1000 | Loss: 0.00003151
Iteration 45/1000 | Loss: 0.00003150
Iteration 46/1000 | Loss: 0.00003150
Iteration 47/1000 | Loss: 0.00003149
Iteration 48/1000 | Loss: 0.00003149
Iteration 49/1000 | Loss: 0.00003149
Iteration 50/1000 | Loss: 0.00003149
Iteration 51/1000 | Loss: 0.00003149
Iteration 52/1000 | Loss: 0.00003149
Iteration 53/1000 | Loss: 0.00003149
Iteration 54/1000 | Loss: 0.00003149
Iteration 55/1000 | Loss: 0.00003148
Iteration 56/1000 | Loss: 0.00003148
Iteration 57/1000 | Loss: 0.00003148
Iteration 58/1000 | Loss: 0.00003148
Iteration 59/1000 | Loss: 0.00003147
Iteration 60/1000 | Loss: 0.00003147
Iteration 61/1000 | Loss: 0.00003147
Iteration 62/1000 | Loss: 0.00003146
Iteration 63/1000 | Loss: 0.00003146
Iteration 64/1000 | Loss: 0.00003146
Iteration 65/1000 | Loss: 0.00003146
Iteration 66/1000 | Loss: 0.00003146
Iteration 67/1000 | Loss: 0.00003146
Iteration 68/1000 | Loss: 0.00003146
Iteration 69/1000 | Loss: 0.00003145
Iteration 70/1000 | Loss: 0.00003145
Iteration 71/1000 | Loss: 0.00003145
Iteration 72/1000 | Loss: 0.00003145
Iteration 73/1000 | Loss: 0.00003145
Iteration 74/1000 | Loss: 0.00003145
Iteration 75/1000 | Loss: 0.00003145
Iteration 76/1000 | Loss: 0.00003145
Iteration 77/1000 | Loss: 0.00003145
Iteration 78/1000 | Loss: 0.00003144
Iteration 79/1000 | Loss: 0.00003144
Iteration 80/1000 | Loss: 0.00003144
Iteration 81/1000 | Loss: 0.00003144
Iteration 82/1000 | Loss: 0.00003144
Iteration 83/1000 | Loss: 0.00003144
Iteration 84/1000 | Loss: 0.00003144
Iteration 85/1000 | Loss: 0.00003144
Iteration 86/1000 | Loss: 0.00003144
Iteration 87/1000 | Loss: 0.00003143
Iteration 88/1000 | Loss: 0.00003143
Iteration 89/1000 | Loss: 0.00003143
Iteration 90/1000 | Loss: 0.00003143
Iteration 91/1000 | Loss: 0.00003143
Iteration 92/1000 | Loss: 0.00003143
Iteration 93/1000 | Loss: 0.00003143
Iteration 94/1000 | Loss: 0.00003143
Iteration 95/1000 | Loss: 0.00003143
Iteration 96/1000 | Loss: 0.00003143
Iteration 97/1000 | Loss: 0.00003143
Iteration 98/1000 | Loss: 0.00003142
Iteration 99/1000 | Loss: 0.00003142
Iteration 100/1000 | Loss: 0.00003142
Iteration 101/1000 | Loss: 0.00003142
Iteration 102/1000 | Loss: 0.00003142
Iteration 103/1000 | Loss: 0.00003141
Iteration 104/1000 | Loss: 0.00003141
Iteration 105/1000 | Loss: 0.00003141
Iteration 106/1000 | Loss: 0.00003141
Iteration 107/1000 | Loss: 0.00003140
Iteration 108/1000 | Loss: 0.00003140
Iteration 109/1000 | Loss: 0.00003140
Iteration 110/1000 | Loss: 0.00003140
Iteration 111/1000 | Loss: 0.00003140
Iteration 112/1000 | Loss: 0.00003140
Iteration 113/1000 | Loss: 0.00003140
Iteration 114/1000 | Loss: 0.00003140
Iteration 115/1000 | Loss: 0.00003140
Iteration 116/1000 | Loss: 0.00003140
Iteration 117/1000 | Loss: 0.00003140
Iteration 118/1000 | Loss: 0.00003140
Iteration 119/1000 | Loss: 0.00003140
Iteration 120/1000 | Loss: 0.00003139
Iteration 121/1000 | Loss: 0.00003139
Iteration 122/1000 | Loss: 0.00003139
Iteration 123/1000 | Loss: 0.00003139
Iteration 124/1000 | Loss: 0.00003138
Iteration 125/1000 | Loss: 0.00003138
Iteration 126/1000 | Loss: 0.00003138
Iteration 127/1000 | Loss: 0.00003138
Iteration 128/1000 | Loss: 0.00003138
Iteration 129/1000 | Loss: 0.00003138
Iteration 130/1000 | Loss: 0.00003137
Iteration 131/1000 | Loss: 0.00003137
Iteration 132/1000 | Loss: 0.00003137
Iteration 133/1000 | Loss: 0.00003137
Iteration 134/1000 | Loss: 0.00003137
Iteration 135/1000 | Loss: 0.00003137
Iteration 136/1000 | Loss: 0.00003137
Iteration 137/1000 | Loss: 0.00003137
Iteration 138/1000 | Loss: 0.00003137
Iteration 139/1000 | Loss: 0.00003137
Iteration 140/1000 | Loss: 0.00003137
Iteration 141/1000 | Loss: 0.00003137
Iteration 142/1000 | Loss: 0.00003137
Iteration 143/1000 | Loss: 0.00003136
Iteration 144/1000 | Loss: 0.00003136
Iteration 145/1000 | Loss: 0.00003136
Iteration 146/1000 | Loss: 0.00003136
Iteration 147/1000 | Loss: 0.00003136
Iteration 148/1000 | Loss: 0.00003136
Iteration 149/1000 | Loss: 0.00003136
Iteration 150/1000 | Loss: 0.00003136
Iteration 151/1000 | Loss: 0.00003136
Iteration 152/1000 | Loss: 0.00003136
Iteration 153/1000 | Loss: 0.00003136
Iteration 154/1000 | Loss: 0.00003136
Iteration 155/1000 | Loss: 0.00003136
Iteration 156/1000 | Loss: 0.00003136
Iteration 157/1000 | Loss: 0.00003136
Iteration 158/1000 | Loss: 0.00003136
Iteration 159/1000 | Loss: 0.00003136
Iteration 160/1000 | Loss: 0.00003136
Iteration 161/1000 | Loss: 0.00003136
Iteration 162/1000 | Loss: 0.00003136
Iteration 163/1000 | Loss: 0.00003136
Iteration 164/1000 | Loss: 0.00003136
Iteration 165/1000 | Loss: 0.00003136
Iteration 166/1000 | Loss: 0.00003136
Iteration 167/1000 | Loss: 0.00003136
Iteration 168/1000 | Loss: 0.00003136
Iteration 169/1000 | Loss: 0.00003136
Iteration 170/1000 | Loss: 0.00003136
Iteration 171/1000 | Loss: 0.00003136
Iteration 172/1000 | Loss: 0.00003136
Iteration 173/1000 | Loss: 0.00003136
Iteration 174/1000 | Loss: 0.00003136
Iteration 175/1000 | Loss: 0.00003136
Iteration 176/1000 | Loss: 0.00003136
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 176. Stopping optimization.
Last 5 losses: [3.1358340493170545e-05, 3.1358340493170545e-05, 3.1358340493170545e-05, 3.1358340493170545e-05, 3.1358340493170545e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.1358340493170545e-05

Optimization complete. Final v2v error: 4.658867835998535 mm

Highest mean error: 4.8888349533081055 mm for frame 134

Lowest mean error: 4.533921718597412 mm for frame 39

Saving results

Total time: 56.72485899925232
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alison_posed_001/1047/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alison_posed_001/1047.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alison_posed_001/1047
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01027879
Iteration 2/25 | Loss: 0.00235312
Iteration 3/25 | Loss: 0.00179986
Iteration 4/25 | Loss: 0.00190245
Iteration 5/25 | Loss: 0.00167902
Iteration 6/25 | Loss: 0.00169004
Iteration 7/25 | Loss: 0.00162482
Iteration 8/25 | Loss: 0.00149383
Iteration 9/25 | Loss: 0.00147173
Iteration 10/25 | Loss: 0.00139194
Iteration 11/25 | Loss: 0.00134264
Iteration 12/25 | Loss: 0.00132304
Iteration 13/25 | Loss: 0.00133566
Iteration 14/25 | Loss: 0.00131395
Iteration 15/25 | Loss: 0.00132748
Iteration 16/25 | Loss: 0.00130221
Iteration 17/25 | Loss: 0.00129645
Iteration 18/25 | Loss: 0.00129883
Iteration 19/25 | Loss: 0.00130230
Iteration 20/25 | Loss: 0.00129484
Iteration 21/25 | Loss: 0.00129506
Iteration 22/25 | Loss: 0.00129463
Iteration 23/25 | Loss: 0.00129455
Iteration 24/25 | Loss: 0.00129385
Iteration 25/25 | Loss: 0.00129407

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.68045163
Iteration 2/25 | Loss: 0.00107135
Iteration 3/25 | Loss: 0.00107135
Iteration 4/25 | Loss: 0.00107135
Iteration 5/25 | Loss: 0.00107135
Iteration 6/25 | Loss: 0.00107135
Iteration 7/25 | Loss: 0.00107135
Iteration 8/25 | Loss: 0.00107135
Iteration 9/25 | Loss: 0.00107135
Iteration 10/25 | Loss: 0.00107135
Iteration 11/25 | Loss: 0.00107135
Iteration 12/25 | Loss: 0.00107135
Iteration 13/25 | Loss: 0.00107135
Iteration 14/25 | Loss: 0.00107135
Iteration 15/25 | Loss: 0.00107135
Iteration 16/25 | Loss: 0.00107135
Iteration 17/25 | Loss: 0.00107135
Iteration 18/25 | Loss: 0.00107135
Iteration 19/25 | Loss: 0.00107135
Iteration 20/25 | Loss: 0.00107135
Iteration 21/25 | Loss: 0.00107135
Iteration 22/25 | Loss: 0.00107135
Iteration 23/25 | Loss: 0.00107135
Iteration 24/25 | Loss: 0.00107135
Iteration 25/25 | Loss: 0.00107135

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00107135
Iteration 2/1000 | Loss: 0.00006738
Iteration 3/1000 | Loss: 0.00025423
Iteration 4/1000 | Loss: 0.00008853
Iteration 5/1000 | Loss: 0.00009028
Iteration 6/1000 | Loss: 0.00007302
Iteration 7/1000 | Loss: 0.00004376
Iteration 8/1000 | Loss: 0.00006684
Iteration 9/1000 | Loss: 0.00023102
Iteration 10/1000 | Loss: 0.00005891
Iteration 11/1000 | Loss: 0.00014350
Iteration 12/1000 | Loss: 0.00004449
Iteration 13/1000 | Loss: 0.00012292
Iteration 14/1000 | Loss: 0.00007293
Iteration 15/1000 | Loss: 0.00006605
Iteration 16/1000 | Loss: 0.00006350
Iteration 17/1000 | Loss: 0.00009139
Iteration 18/1000 | Loss: 0.00010157
Iteration 19/1000 | Loss: 0.00010372
Iteration 20/1000 | Loss: 0.00008519
Iteration 21/1000 | Loss: 0.00007434
Iteration 22/1000 | Loss: 0.00004523
Iteration 23/1000 | Loss: 0.00005450
Iteration 24/1000 | Loss: 0.00004564
Iteration 25/1000 | Loss: 0.00004726
Iteration 26/1000 | Loss: 0.00004725
Iteration 27/1000 | Loss: 0.00004683
Iteration 28/1000 | Loss: 0.00004366
Iteration 29/1000 | Loss: 0.00028505
Iteration 30/1000 | Loss: 0.00006157
Iteration 31/1000 | Loss: 0.00007155
Iteration 32/1000 | Loss: 0.00005860
Iteration 33/1000 | Loss: 0.00005757
Iteration 34/1000 | Loss: 0.00005743
Iteration 35/1000 | Loss: 0.00007306
Iteration 36/1000 | Loss: 0.00007005
Iteration 37/1000 | Loss: 0.00004176
Iteration 38/1000 | Loss: 0.00014435
Iteration 39/1000 | Loss: 0.00004172
Iteration 40/1000 | Loss: 0.00006103
Iteration 41/1000 | Loss: 0.00006058
Iteration 42/1000 | Loss: 0.00005915
Iteration 43/1000 | Loss: 0.00005896
Iteration 44/1000 | Loss: 0.00005588
Iteration 45/1000 | Loss: 0.00004374
Iteration 46/1000 | Loss: 0.00003863
Iteration 47/1000 | Loss: 0.00003493
Iteration 48/1000 | Loss: 0.00003334
Iteration 49/1000 | Loss: 0.00003271
Iteration 50/1000 | Loss: 0.00003227
Iteration 51/1000 | Loss: 0.00003174
Iteration 52/1000 | Loss: 0.00132992
Iteration 53/1000 | Loss: 0.00004577
Iteration 54/1000 | Loss: 0.00003431
Iteration 55/1000 | Loss: 0.00002939
Iteration 56/1000 | Loss: 0.00024801
Iteration 57/1000 | Loss: 0.00004365
Iteration 58/1000 | Loss: 0.00003346
Iteration 59/1000 | Loss: 0.00002253
Iteration 60/1000 | Loss: 0.00004720
Iteration 61/1000 | Loss: 0.00002152
Iteration 62/1000 | Loss: 0.00002077
Iteration 63/1000 | Loss: 0.00002036
Iteration 64/1000 | Loss: 0.00001988
Iteration 65/1000 | Loss: 0.00001957
Iteration 66/1000 | Loss: 0.00001929
Iteration 67/1000 | Loss: 0.00001908
Iteration 68/1000 | Loss: 0.00001893
Iteration 69/1000 | Loss: 0.00001883
Iteration 70/1000 | Loss: 0.00001880
Iteration 71/1000 | Loss: 0.00001879
Iteration 72/1000 | Loss: 0.00001879
Iteration 73/1000 | Loss: 0.00001876
Iteration 74/1000 | Loss: 0.00001869
Iteration 75/1000 | Loss: 0.00001864
Iteration 76/1000 | Loss: 0.00001864
Iteration 77/1000 | Loss: 0.00001863
Iteration 78/1000 | Loss: 0.00001863
Iteration 79/1000 | Loss: 0.00001862
Iteration 80/1000 | Loss: 0.00001862
Iteration 81/1000 | Loss: 0.00001862
Iteration 82/1000 | Loss: 0.00001862
Iteration 83/1000 | Loss: 0.00001861
Iteration 84/1000 | Loss: 0.00001861
Iteration 85/1000 | Loss: 0.00001861
Iteration 86/1000 | Loss: 0.00001861
Iteration 87/1000 | Loss: 0.00001861
Iteration 88/1000 | Loss: 0.00001861
Iteration 89/1000 | Loss: 0.00001861
Iteration 90/1000 | Loss: 0.00001860
Iteration 91/1000 | Loss: 0.00001860
Iteration 92/1000 | Loss: 0.00001860
Iteration 93/1000 | Loss: 0.00001860
Iteration 94/1000 | Loss: 0.00001860
Iteration 95/1000 | Loss: 0.00001860
Iteration 96/1000 | Loss: 0.00001860
Iteration 97/1000 | Loss: 0.00001860
Iteration 98/1000 | Loss: 0.00001860
Iteration 99/1000 | Loss: 0.00001860
Iteration 100/1000 | Loss: 0.00001859
Iteration 101/1000 | Loss: 0.00001859
Iteration 102/1000 | Loss: 0.00001859
Iteration 103/1000 | Loss: 0.00001859
Iteration 104/1000 | Loss: 0.00001859
Iteration 105/1000 | Loss: 0.00001859
Iteration 106/1000 | Loss: 0.00001859
Iteration 107/1000 | Loss: 0.00001859
Iteration 108/1000 | Loss: 0.00001859
Iteration 109/1000 | Loss: 0.00001859
Iteration 110/1000 | Loss: 0.00001859
Iteration 111/1000 | Loss: 0.00001858
Iteration 112/1000 | Loss: 0.00001858
Iteration 113/1000 | Loss: 0.00001858
Iteration 114/1000 | Loss: 0.00001858
Iteration 115/1000 | Loss: 0.00001858
Iteration 116/1000 | Loss: 0.00001858
Iteration 117/1000 | Loss: 0.00001857
Iteration 118/1000 | Loss: 0.00001857
Iteration 119/1000 | Loss: 0.00001857
Iteration 120/1000 | Loss: 0.00001857
Iteration 121/1000 | Loss: 0.00001857
Iteration 122/1000 | Loss: 0.00001857
Iteration 123/1000 | Loss: 0.00001857
Iteration 124/1000 | Loss: 0.00001856
Iteration 125/1000 | Loss: 0.00001856
Iteration 126/1000 | Loss: 0.00001856
Iteration 127/1000 | Loss: 0.00001856
Iteration 128/1000 | Loss: 0.00001856
Iteration 129/1000 | Loss: 0.00001856
Iteration 130/1000 | Loss: 0.00001855
Iteration 131/1000 | Loss: 0.00001855
Iteration 132/1000 | Loss: 0.00001855
Iteration 133/1000 | Loss: 0.00001855
Iteration 134/1000 | Loss: 0.00001855
Iteration 135/1000 | Loss: 0.00001855
Iteration 136/1000 | Loss: 0.00001855
Iteration 137/1000 | Loss: 0.00001855
Iteration 138/1000 | Loss: 0.00001855
Iteration 139/1000 | Loss: 0.00001855
Iteration 140/1000 | Loss: 0.00001855
Iteration 141/1000 | Loss: 0.00001855
Iteration 142/1000 | Loss: 0.00001854
Iteration 143/1000 | Loss: 0.00001854
Iteration 144/1000 | Loss: 0.00001854
Iteration 145/1000 | Loss: 0.00001854
Iteration 146/1000 | Loss: 0.00001854
Iteration 147/1000 | Loss: 0.00001854
Iteration 148/1000 | Loss: 0.00001854
Iteration 149/1000 | Loss: 0.00001854
Iteration 150/1000 | Loss: 0.00001854
Iteration 151/1000 | Loss: 0.00001854
Iteration 152/1000 | Loss: 0.00001854
Iteration 153/1000 | Loss: 0.00001854
Iteration 154/1000 | Loss: 0.00001854
Iteration 155/1000 | Loss: 0.00001854
Iteration 156/1000 | Loss: 0.00001854
Iteration 157/1000 | Loss: 0.00001854
Iteration 158/1000 | Loss: 0.00001854
Iteration 159/1000 | Loss: 0.00001854
Iteration 160/1000 | Loss: 0.00001854
Iteration 161/1000 | Loss: 0.00001854
Iteration 162/1000 | Loss: 0.00001854
Iteration 163/1000 | Loss: 0.00001854
Iteration 164/1000 | Loss: 0.00001854
Iteration 165/1000 | Loss: 0.00001853
Iteration 166/1000 | Loss: 0.00001853
Iteration 167/1000 | Loss: 0.00001853
Iteration 168/1000 | Loss: 0.00001853
Iteration 169/1000 | Loss: 0.00001853
Iteration 170/1000 | Loss: 0.00001853
Iteration 171/1000 | Loss: 0.00001853
Iteration 172/1000 | Loss: 0.00001853
Iteration 173/1000 | Loss: 0.00001853
Iteration 174/1000 | Loss: 0.00001853
Iteration 175/1000 | Loss: 0.00001853
Iteration 176/1000 | Loss: 0.00001853
Iteration 177/1000 | Loss: 0.00001853
Iteration 178/1000 | Loss: 0.00001853
Iteration 179/1000 | Loss: 0.00001853
Iteration 180/1000 | Loss: 0.00001853
Iteration 181/1000 | Loss: 0.00001853
Iteration 182/1000 | Loss: 0.00001853
Iteration 183/1000 | Loss: 0.00001853
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 183. Stopping optimization.
Last 5 losses: [1.8529026419855654e-05, 1.8529026419855654e-05, 1.8529026419855654e-05, 1.8529026419855654e-05, 1.8529026419855654e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8529026419855654e-05

Optimization complete. Final v2v error: 3.4018023014068604 mm

Highest mean error: 10.504358291625977 mm for frame 94

Lowest mean error: 3.065877914428711 mm for frame 30

Saving results

Total time: 145.38899421691895
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alison_posed_001/1059/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alison_posed_001/1059.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alison_posed_001/1059
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00408815
Iteration 2/25 | Loss: 0.00132829
Iteration 3/25 | Loss: 0.00125776
Iteration 4/25 | Loss: 0.00124489
Iteration 5/25 | Loss: 0.00124003
Iteration 6/25 | Loss: 0.00123976
Iteration 7/25 | Loss: 0.00123976
Iteration 8/25 | Loss: 0.00123976
Iteration 9/25 | Loss: 0.00123976
Iteration 10/25 | Loss: 0.00123976
Iteration 11/25 | Loss: 0.00123976
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012397590326145291, 0.0012397590326145291, 0.0012397590326145291, 0.0012397590326145291, 0.0012397590326145291]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012397590326145291

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.47832322
Iteration 2/25 | Loss: 0.00081423
Iteration 3/25 | Loss: 0.00081423
Iteration 4/25 | Loss: 0.00081423
Iteration 5/25 | Loss: 0.00081423
Iteration 6/25 | Loss: 0.00081423
Iteration 7/25 | Loss: 0.00081423
Iteration 8/25 | Loss: 0.00081423
Iteration 9/25 | Loss: 0.00081423
Iteration 10/25 | Loss: 0.00081423
Iteration 11/25 | Loss: 0.00081423
Iteration 12/25 | Loss: 0.00081422
Iteration 13/25 | Loss: 0.00081422
Iteration 14/25 | Loss: 0.00081422
Iteration 15/25 | Loss: 0.00081422
Iteration 16/25 | Loss: 0.00081422
Iteration 17/25 | Loss: 0.00081422
Iteration 18/25 | Loss: 0.00081422
Iteration 19/25 | Loss: 0.00081422
Iteration 20/25 | Loss: 0.00081422
Iteration 21/25 | Loss: 0.00081422
Iteration 22/25 | Loss: 0.00081422
Iteration 23/25 | Loss: 0.00081422
Iteration 24/25 | Loss: 0.00081422
Iteration 25/25 | Loss: 0.00081422

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00081422
Iteration 2/1000 | Loss: 0.00002694
Iteration 3/1000 | Loss: 0.00001861
Iteration 4/1000 | Loss: 0.00001605
Iteration 5/1000 | Loss: 0.00001527
Iteration 6/1000 | Loss: 0.00001478
Iteration 7/1000 | Loss: 0.00001445
Iteration 8/1000 | Loss: 0.00001425
Iteration 9/1000 | Loss: 0.00001396
Iteration 10/1000 | Loss: 0.00001391
Iteration 11/1000 | Loss: 0.00001390
Iteration 12/1000 | Loss: 0.00001389
Iteration 13/1000 | Loss: 0.00001380
Iteration 14/1000 | Loss: 0.00001374
Iteration 15/1000 | Loss: 0.00001363
Iteration 16/1000 | Loss: 0.00001361
Iteration 17/1000 | Loss: 0.00001358
Iteration 18/1000 | Loss: 0.00001358
Iteration 19/1000 | Loss: 0.00001358
Iteration 20/1000 | Loss: 0.00001357
Iteration 21/1000 | Loss: 0.00001357
Iteration 22/1000 | Loss: 0.00001357
Iteration 23/1000 | Loss: 0.00001356
Iteration 24/1000 | Loss: 0.00001356
Iteration 25/1000 | Loss: 0.00001354
Iteration 26/1000 | Loss: 0.00001353
Iteration 27/1000 | Loss: 0.00001351
Iteration 28/1000 | Loss: 0.00001351
Iteration 29/1000 | Loss: 0.00001350
Iteration 30/1000 | Loss: 0.00001350
Iteration 31/1000 | Loss: 0.00001349
Iteration 32/1000 | Loss: 0.00001349
Iteration 33/1000 | Loss: 0.00001349
Iteration 34/1000 | Loss: 0.00001348
Iteration 35/1000 | Loss: 0.00001348
Iteration 36/1000 | Loss: 0.00001348
Iteration 37/1000 | Loss: 0.00001348
Iteration 38/1000 | Loss: 0.00001347
Iteration 39/1000 | Loss: 0.00001347
Iteration 40/1000 | Loss: 0.00001346
Iteration 41/1000 | Loss: 0.00001343
Iteration 42/1000 | Loss: 0.00001343
Iteration 43/1000 | Loss: 0.00001342
Iteration 44/1000 | Loss: 0.00001342
Iteration 45/1000 | Loss: 0.00001342
Iteration 46/1000 | Loss: 0.00001342
Iteration 47/1000 | Loss: 0.00001341
Iteration 48/1000 | Loss: 0.00001341
Iteration 49/1000 | Loss: 0.00001339
Iteration 50/1000 | Loss: 0.00001332
Iteration 51/1000 | Loss: 0.00001329
Iteration 52/1000 | Loss: 0.00001328
Iteration 53/1000 | Loss: 0.00001327
Iteration 54/1000 | Loss: 0.00001327
Iteration 55/1000 | Loss: 0.00001327
Iteration 56/1000 | Loss: 0.00001327
Iteration 57/1000 | Loss: 0.00001327
Iteration 58/1000 | Loss: 0.00001326
Iteration 59/1000 | Loss: 0.00001326
Iteration 60/1000 | Loss: 0.00001326
Iteration 61/1000 | Loss: 0.00001326
Iteration 62/1000 | Loss: 0.00001326
Iteration 63/1000 | Loss: 0.00001326
Iteration 64/1000 | Loss: 0.00001326
Iteration 65/1000 | Loss: 0.00001326
Iteration 66/1000 | Loss: 0.00001325
Iteration 67/1000 | Loss: 0.00001325
Iteration 68/1000 | Loss: 0.00001325
Iteration 69/1000 | Loss: 0.00001325
Iteration 70/1000 | Loss: 0.00001325
Iteration 71/1000 | Loss: 0.00001324
Iteration 72/1000 | Loss: 0.00001323
Iteration 73/1000 | Loss: 0.00001323
Iteration 74/1000 | Loss: 0.00001323
Iteration 75/1000 | Loss: 0.00001322
Iteration 76/1000 | Loss: 0.00001322
Iteration 77/1000 | Loss: 0.00001322
Iteration 78/1000 | Loss: 0.00001322
Iteration 79/1000 | Loss: 0.00001322
Iteration 80/1000 | Loss: 0.00001321
Iteration 81/1000 | Loss: 0.00001321
Iteration 82/1000 | Loss: 0.00001321
Iteration 83/1000 | Loss: 0.00001321
Iteration 84/1000 | Loss: 0.00001321
Iteration 85/1000 | Loss: 0.00001321
Iteration 86/1000 | Loss: 0.00001321
Iteration 87/1000 | Loss: 0.00001321
Iteration 88/1000 | Loss: 0.00001321
Iteration 89/1000 | Loss: 0.00001320
Iteration 90/1000 | Loss: 0.00001320
Iteration 91/1000 | Loss: 0.00001319
Iteration 92/1000 | Loss: 0.00001319
Iteration 93/1000 | Loss: 0.00001319
Iteration 94/1000 | Loss: 0.00001319
Iteration 95/1000 | Loss: 0.00001319
Iteration 96/1000 | Loss: 0.00001319
Iteration 97/1000 | Loss: 0.00001318
Iteration 98/1000 | Loss: 0.00001318
Iteration 99/1000 | Loss: 0.00001318
Iteration 100/1000 | Loss: 0.00001318
Iteration 101/1000 | Loss: 0.00001318
Iteration 102/1000 | Loss: 0.00001317
Iteration 103/1000 | Loss: 0.00001317
Iteration 104/1000 | Loss: 0.00001317
Iteration 105/1000 | Loss: 0.00001317
Iteration 106/1000 | Loss: 0.00001317
Iteration 107/1000 | Loss: 0.00001317
Iteration 108/1000 | Loss: 0.00001317
Iteration 109/1000 | Loss: 0.00001316
Iteration 110/1000 | Loss: 0.00001316
Iteration 111/1000 | Loss: 0.00001316
Iteration 112/1000 | Loss: 0.00001315
Iteration 113/1000 | Loss: 0.00001315
Iteration 114/1000 | Loss: 0.00001315
Iteration 115/1000 | Loss: 0.00001315
Iteration 116/1000 | Loss: 0.00001315
Iteration 117/1000 | Loss: 0.00001315
Iteration 118/1000 | Loss: 0.00001315
Iteration 119/1000 | Loss: 0.00001315
Iteration 120/1000 | Loss: 0.00001315
Iteration 121/1000 | Loss: 0.00001314
Iteration 122/1000 | Loss: 0.00001314
Iteration 123/1000 | Loss: 0.00001314
Iteration 124/1000 | Loss: 0.00001314
Iteration 125/1000 | Loss: 0.00001314
Iteration 126/1000 | Loss: 0.00001314
Iteration 127/1000 | Loss: 0.00001314
Iteration 128/1000 | Loss: 0.00001314
Iteration 129/1000 | Loss: 0.00001313
Iteration 130/1000 | Loss: 0.00001313
Iteration 131/1000 | Loss: 0.00001313
Iteration 132/1000 | Loss: 0.00001313
Iteration 133/1000 | Loss: 0.00001313
Iteration 134/1000 | Loss: 0.00001313
Iteration 135/1000 | Loss: 0.00001313
Iteration 136/1000 | Loss: 0.00001313
Iteration 137/1000 | Loss: 0.00001313
Iteration 138/1000 | Loss: 0.00001313
Iteration 139/1000 | Loss: 0.00001312
Iteration 140/1000 | Loss: 0.00001312
Iteration 141/1000 | Loss: 0.00001312
Iteration 142/1000 | Loss: 0.00001312
Iteration 143/1000 | Loss: 0.00001312
Iteration 144/1000 | Loss: 0.00001312
Iteration 145/1000 | Loss: 0.00001312
Iteration 146/1000 | Loss: 0.00001312
Iteration 147/1000 | Loss: 0.00001312
Iteration 148/1000 | Loss: 0.00001312
Iteration 149/1000 | Loss: 0.00001312
Iteration 150/1000 | Loss: 0.00001312
Iteration 151/1000 | Loss: 0.00001312
Iteration 152/1000 | Loss: 0.00001312
Iteration 153/1000 | Loss: 0.00001312
Iteration 154/1000 | Loss: 0.00001312
Iteration 155/1000 | Loss: 0.00001312
Iteration 156/1000 | Loss: 0.00001312
Iteration 157/1000 | Loss: 0.00001312
Iteration 158/1000 | Loss: 0.00001312
Iteration 159/1000 | Loss: 0.00001312
Iteration 160/1000 | Loss: 0.00001312
Iteration 161/1000 | Loss: 0.00001312
Iteration 162/1000 | Loss: 0.00001312
Iteration 163/1000 | Loss: 0.00001312
Iteration 164/1000 | Loss: 0.00001312
Iteration 165/1000 | Loss: 0.00001312
Iteration 166/1000 | Loss: 0.00001312
Iteration 167/1000 | Loss: 0.00001312
Iteration 168/1000 | Loss: 0.00001312
Iteration 169/1000 | Loss: 0.00001312
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 169. Stopping optimization.
Last 5 losses: [1.3115050023770891e-05, 1.3115050023770891e-05, 1.3115050023770891e-05, 1.3115050023770891e-05, 1.3115050023770891e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3115050023770891e-05

Optimization complete. Final v2v error: 3.112445592880249 mm

Highest mean error: 3.2083795070648193 mm for frame 49

Lowest mean error: 3.048076629638672 mm for frame 168

Saving results

Total time: 38.36464285850525
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alison_posed_001/1012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alison_posed_001/1012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alison_posed_001/1012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00434055
Iteration 2/25 | Loss: 0.00131212
Iteration 3/25 | Loss: 0.00124640
Iteration 4/25 | Loss: 0.00123309
Iteration 5/25 | Loss: 0.00123044
Iteration 6/25 | Loss: 0.00123022
Iteration 7/25 | Loss: 0.00123022
Iteration 8/25 | Loss: 0.00123022
Iteration 9/25 | Loss: 0.00123022
Iteration 10/25 | Loss: 0.00123022
Iteration 11/25 | Loss: 0.00123022
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012302182149142027, 0.0012302182149142027, 0.0012302182149142027, 0.0012302182149142027, 0.0012302182149142027]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012302182149142027

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45946956
Iteration 2/25 | Loss: 0.00072486
Iteration 3/25 | Loss: 0.00072486
Iteration 4/25 | Loss: 0.00072486
Iteration 5/25 | Loss: 0.00072486
Iteration 6/25 | Loss: 0.00072486
Iteration 7/25 | Loss: 0.00072486
Iteration 8/25 | Loss: 0.00072485
Iteration 9/25 | Loss: 0.00072485
Iteration 10/25 | Loss: 0.00072485
Iteration 11/25 | Loss: 0.00072485
Iteration 12/25 | Loss: 0.00072485
Iteration 13/25 | Loss: 0.00072485
Iteration 14/25 | Loss: 0.00072485
Iteration 15/25 | Loss: 0.00072485
Iteration 16/25 | Loss: 0.00072485
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.000724854355212301, 0.000724854355212301, 0.000724854355212301, 0.000724854355212301, 0.000724854355212301]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000724854355212301

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00072485
Iteration 2/1000 | Loss: 0.00003148
Iteration 3/1000 | Loss: 0.00002015
Iteration 4/1000 | Loss: 0.00001840
Iteration 5/1000 | Loss: 0.00001761
Iteration 6/1000 | Loss: 0.00001702
Iteration 7/1000 | Loss: 0.00001665
Iteration 8/1000 | Loss: 0.00001649
Iteration 9/1000 | Loss: 0.00001631
Iteration 10/1000 | Loss: 0.00001607
Iteration 11/1000 | Loss: 0.00001604
Iteration 12/1000 | Loss: 0.00001590
Iteration 13/1000 | Loss: 0.00001587
Iteration 14/1000 | Loss: 0.00001573
Iteration 15/1000 | Loss: 0.00001573
Iteration 16/1000 | Loss: 0.00001571
Iteration 17/1000 | Loss: 0.00001570
Iteration 18/1000 | Loss: 0.00001570
Iteration 19/1000 | Loss: 0.00001570
Iteration 20/1000 | Loss: 0.00001567
Iteration 21/1000 | Loss: 0.00001567
Iteration 22/1000 | Loss: 0.00001566
Iteration 23/1000 | Loss: 0.00001565
Iteration 24/1000 | Loss: 0.00001565
Iteration 25/1000 | Loss: 0.00001565
Iteration 26/1000 | Loss: 0.00001565
Iteration 27/1000 | Loss: 0.00001564
Iteration 28/1000 | Loss: 0.00001564
Iteration 29/1000 | Loss: 0.00001564
Iteration 30/1000 | Loss: 0.00001563
Iteration 31/1000 | Loss: 0.00001561
Iteration 32/1000 | Loss: 0.00001561
Iteration 33/1000 | Loss: 0.00001560
Iteration 34/1000 | Loss: 0.00001560
Iteration 35/1000 | Loss: 0.00001559
Iteration 36/1000 | Loss: 0.00001559
Iteration 37/1000 | Loss: 0.00001558
Iteration 38/1000 | Loss: 0.00001558
Iteration 39/1000 | Loss: 0.00001557
Iteration 40/1000 | Loss: 0.00001556
Iteration 41/1000 | Loss: 0.00001556
Iteration 42/1000 | Loss: 0.00001555
Iteration 43/1000 | Loss: 0.00001555
Iteration 44/1000 | Loss: 0.00001554
Iteration 45/1000 | Loss: 0.00001554
Iteration 46/1000 | Loss: 0.00001551
Iteration 47/1000 | Loss: 0.00001551
Iteration 48/1000 | Loss: 0.00001551
Iteration 49/1000 | Loss: 0.00001551
Iteration 50/1000 | Loss: 0.00001551
Iteration 51/1000 | Loss: 0.00001551
Iteration 52/1000 | Loss: 0.00001551
Iteration 53/1000 | Loss: 0.00001551
Iteration 54/1000 | Loss: 0.00001550
Iteration 55/1000 | Loss: 0.00001550
Iteration 56/1000 | Loss: 0.00001550
Iteration 57/1000 | Loss: 0.00001550
Iteration 58/1000 | Loss: 0.00001550
Iteration 59/1000 | Loss: 0.00001550
Iteration 60/1000 | Loss: 0.00001549
Iteration 61/1000 | Loss: 0.00001547
Iteration 62/1000 | Loss: 0.00001546
Iteration 63/1000 | Loss: 0.00001545
Iteration 64/1000 | Loss: 0.00001545
Iteration 65/1000 | Loss: 0.00001544
Iteration 66/1000 | Loss: 0.00001544
Iteration 67/1000 | Loss: 0.00001542
Iteration 68/1000 | Loss: 0.00001538
Iteration 69/1000 | Loss: 0.00001538
Iteration 70/1000 | Loss: 0.00001538
Iteration 71/1000 | Loss: 0.00001534
Iteration 72/1000 | Loss: 0.00001534
Iteration 73/1000 | Loss: 0.00001533
Iteration 74/1000 | Loss: 0.00001533
Iteration 75/1000 | Loss: 0.00001532
Iteration 76/1000 | Loss: 0.00001532
Iteration 77/1000 | Loss: 0.00001532
Iteration 78/1000 | Loss: 0.00001531
Iteration 79/1000 | Loss: 0.00001531
Iteration 80/1000 | Loss: 0.00001530
Iteration 81/1000 | Loss: 0.00001530
Iteration 82/1000 | Loss: 0.00001529
Iteration 83/1000 | Loss: 0.00001529
Iteration 84/1000 | Loss: 0.00001529
Iteration 85/1000 | Loss: 0.00001529
Iteration 86/1000 | Loss: 0.00001529
Iteration 87/1000 | Loss: 0.00001529
Iteration 88/1000 | Loss: 0.00001529
Iteration 89/1000 | Loss: 0.00001529
Iteration 90/1000 | Loss: 0.00001529
Iteration 91/1000 | Loss: 0.00001528
Iteration 92/1000 | Loss: 0.00001528
Iteration 93/1000 | Loss: 0.00001527
Iteration 94/1000 | Loss: 0.00001526
Iteration 95/1000 | Loss: 0.00001526
Iteration 96/1000 | Loss: 0.00001526
Iteration 97/1000 | Loss: 0.00001526
Iteration 98/1000 | Loss: 0.00001526
Iteration 99/1000 | Loss: 0.00001526
Iteration 100/1000 | Loss: 0.00001526
Iteration 101/1000 | Loss: 0.00001526
Iteration 102/1000 | Loss: 0.00001526
Iteration 103/1000 | Loss: 0.00001526
Iteration 104/1000 | Loss: 0.00001526
Iteration 105/1000 | Loss: 0.00001525
Iteration 106/1000 | Loss: 0.00001525
Iteration 107/1000 | Loss: 0.00001525
Iteration 108/1000 | Loss: 0.00001525
Iteration 109/1000 | Loss: 0.00001524
Iteration 110/1000 | Loss: 0.00001524
Iteration 111/1000 | Loss: 0.00001524
Iteration 112/1000 | Loss: 0.00001524
Iteration 113/1000 | Loss: 0.00001523
Iteration 114/1000 | Loss: 0.00001523
Iteration 115/1000 | Loss: 0.00001523
Iteration 116/1000 | Loss: 0.00001522
Iteration 117/1000 | Loss: 0.00001522
Iteration 118/1000 | Loss: 0.00001522
Iteration 119/1000 | Loss: 0.00001521
Iteration 120/1000 | Loss: 0.00001521
Iteration 121/1000 | Loss: 0.00001521
Iteration 122/1000 | Loss: 0.00001521
Iteration 123/1000 | Loss: 0.00001520
Iteration 124/1000 | Loss: 0.00001520
Iteration 125/1000 | Loss: 0.00001520
Iteration 126/1000 | Loss: 0.00001520
Iteration 127/1000 | Loss: 0.00001520
Iteration 128/1000 | Loss: 0.00001520
Iteration 129/1000 | Loss: 0.00001520
Iteration 130/1000 | Loss: 0.00001519
Iteration 131/1000 | Loss: 0.00001519
Iteration 132/1000 | Loss: 0.00001519
Iteration 133/1000 | Loss: 0.00001519
Iteration 134/1000 | Loss: 0.00001519
Iteration 135/1000 | Loss: 0.00001519
Iteration 136/1000 | Loss: 0.00001519
Iteration 137/1000 | Loss: 0.00001519
Iteration 138/1000 | Loss: 0.00001519
Iteration 139/1000 | Loss: 0.00001519
Iteration 140/1000 | Loss: 0.00001519
Iteration 141/1000 | Loss: 0.00001518
Iteration 142/1000 | Loss: 0.00001518
Iteration 143/1000 | Loss: 0.00001518
Iteration 144/1000 | Loss: 0.00001518
Iteration 145/1000 | Loss: 0.00001518
Iteration 146/1000 | Loss: 0.00001518
Iteration 147/1000 | Loss: 0.00001518
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 147. Stopping optimization.
Last 5 losses: [1.5182052266027313e-05, 1.5182052266027313e-05, 1.5182052266027313e-05, 1.5182052266027313e-05, 1.5182052266027313e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5182052266027313e-05

Optimization complete. Final v2v error: 3.3220033645629883 mm

Highest mean error: 3.557525396347046 mm for frame 114

Lowest mean error: 3.170560836791992 mm for frame 157

Saving results

Total time: 36.378244161605835
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alison_posed_001/1064/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alison_posed_001/1064.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alison_posed_001/1064
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00465769
Iteration 2/25 | Loss: 0.00141469
Iteration 3/25 | Loss: 0.00127706
Iteration 4/25 | Loss: 0.00126202
Iteration 5/25 | Loss: 0.00125941
Iteration 6/25 | Loss: 0.00125941
Iteration 7/25 | Loss: 0.00125941
Iteration 8/25 | Loss: 0.00125941
Iteration 9/25 | Loss: 0.00125941
Iteration 10/25 | Loss: 0.00125941
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0012594141298905015, 0.0012594141298905015, 0.0012594141298905015, 0.0012594141298905015, 0.0012594141298905015]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012594141298905015

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.47225738
Iteration 2/25 | Loss: 0.00080862
Iteration 3/25 | Loss: 0.00080862
Iteration 4/25 | Loss: 0.00080862
Iteration 5/25 | Loss: 0.00080862
Iteration 6/25 | Loss: 0.00080862
Iteration 7/25 | Loss: 0.00080862
Iteration 8/25 | Loss: 0.00080862
Iteration 9/25 | Loss: 0.00080862
Iteration 10/25 | Loss: 0.00080862
Iteration 11/25 | Loss: 0.00080862
Iteration 12/25 | Loss: 0.00080862
Iteration 13/25 | Loss: 0.00080862
Iteration 14/25 | Loss: 0.00080862
Iteration 15/25 | Loss: 0.00080862
Iteration 16/25 | Loss: 0.00080862
Iteration 17/25 | Loss: 0.00080862
Iteration 18/25 | Loss: 0.00080862
Iteration 19/25 | Loss: 0.00080862
Iteration 20/25 | Loss: 0.00080862
Iteration 21/25 | Loss: 0.00080862
Iteration 22/25 | Loss: 0.00080862
Iteration 23/25 | Loss: 0.00080862
Iteration 24/25 | Loss: 0.00080862
Iteration 25/25 | Loss: 0.00080862

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00080862
Iteration 2/1000 | Loss: 0.00004190
Iteration 3/1000 | Loss: 0.00002717
Iteration 4/1000 | Loss: 0.00002486
Iteration 5/1000 | Loss: 0.00002335
Iteration 6/1000 | Loss: 0.00002218
Iteration 7/1000 | Loss: 0.00002153
Iteration 8/1000 | Loss: 0.00002107
Iteration 9/1000 | Loss: 0.00002074
Iteration 10/1000 | Loss: 0.00002052
Iteration 11/1000 | Loss: 0.00002034
Iteration 12/1000 | Loss: 0.00002019
Iteration 13/1000 | Loss: 0.00002008
Iteration 14/1000 | Loss: 0.00001994
Iteration 15/1000 | Loss: 0.00001988
Iteration 16/1000 | Loss: 0.00001986
Iteration 17/1000 | Loss: 0.00001984
Iteration 18/1000 | Loss: 0.00001982
Iteration 19/1000 | Loss: 0.00001979
Iteration 20/1000 | Loss: 0.00001978
Iteration 21/1000 | Loss: 0.00001976
Iteration 22/1000 | Loss: 0.00001965
Iteration 23/1000 | Loss: 0.00001965
Iteration 24/1000 | Loss: 0.00001962
Iteration 25/1000 | Loss: 0.00001961
Iteration 26/1000 | Loss: 0.00001961
Iteration 27/1000 | Loss: 0.00001960
Iteration 28/1000 | Loss: 0.00001960
Iteration 29/1000 | Loss: 0.00001959
Iteration 30/1000 | Loss: 0.00001958
Iteration 31/1000 | Loss: 0.00001958
Iteration 32/1000 | Loss: 0.00001958
Iteration 33/1000 | Loss: 0.00001958
Iteration 34/1000 | Loss: 0.00001957
Iteration 35/1000 | Loss: 0.00001957
Iteration 36/1000 | Loss: 0.00001957
Iteration 37/1000 | Loss: 0.00001956
Iteration 38/1000 | Loss: 0.00001956
Iteration 39/1000 | Loss: 0.00001956
Iteration 40/1000 | Loss: 0.00001956
Iteration 41/1000 | Loss: 0.00001955
Iteration 42/1000 | Loss: 0.00001955
Iteration 43/1000 | Loss: 0.00001954
Iteration 44/1000 | Loss: 0.00001954
Iteration 45/1000 | Loss: 0.00001952
Iteration 46/1000 | Loss: 0.00001952
Iteration 47/1000 | Loss: 0.00001952
Iteration 48/1000 | Loss: 0.00001952
Iteration 49/1000 | Loss: 0.00001952
Iteration 50/1000 | Loss: 0.00001952
Iteration 51/1000 | Loss: 0.00001952
Iteration 52/1000 | Loss: 0.00001951
Iteration 53/1000 | Loss: 0.00001951
Iteration 54/1000 | Loss: 0.00001950
Iteration 55/1000 | Loss: 0.00001950
Iteration 56/1000 | Loss: 0.00001949
Iteration 57/1000 | Loss: 0.00001949
Iteration 58/1000 | Loss: 0.00001948
Iteration 59/1000 | Loss: 0.00001948
Iteration 60/1000 | Loss: 0.00001947
Iteration 61/1000 | Loss: 0.00001947
Iteration 62/1000 | Loss: 0.00001947
Iteration 63/1000 | Loss: 0.00001947
Iteration 64/1000 | Loss: 0.00001947
Iteration 65/1000 | Loss: 0.00001947
Iteration 66/1000 | Loss: 0.00001947
Iteration 67/1000 | Loss: 0.00001947
Iteration 68/1000 | Loss: 0.00001947
Iteration 69/1000 | Loss: 0.00001946
Iteration 70/1000 | Loss: 0.00001946
Iteration 71/1000 | Loss: 0.00001946
Iteration 72/1000 | Loss: 0.00001946
Iteration 73/1000 | Loss: 0.00001945
Iteration 74/1000 | Loss: 0.00001945
Iteration 75/1000 | Loss: 0.00001945
Iteration 76/1000 | Loss: 0.00001945
Iteration 77/1000 | Loss: 0.00001944
Iteration 78/1000 | Loss: 0.00001944
Iteration 79/1000 | Loss: 0.00001944
Iteration 80/1000 | Loss: 0.00001944
Iteration 81/1000 | Loss: 0.00001944
Iteration 82/1000 | Loss: 0.00001944
Iteration 83/1000 | Loss: 0.00001943
Iteration 84/1000 | Loss: 0.00001943
Iteration 85/1000 | Loss: 0.00001943
Iteration 86/1000 | Loss: 0.00001943
Iteration 87/1000 | Loss: 0.00001943
Iteration 88/1000 | Loss: 0.00001943
Iteration 89/1000 | Loss: 0.00001942
Iteration 90/1000 | Loss: 0.00001942
Iteration 91/1000 | Loss: 0.00001942
Iteration 92/1000 | Loss: 0.00001942
Iteration 93/1000 | Loss: 0.00001942
Iteration 94/1000 | Loss: 0.00001942
Iteration 95/1000 | Loss: 0.00001942
Iteration 96/1000 | Loss: 0.00001942
Iteration 97/1000 | Loss: 0.00001941
Iteration 98/1000 | Loss: 0.00001941
Iteration 99/1000 | Loss: 0.00001941
Iteration 100/1000 | Loss: 0.00001941
Iteration 101/1000 | Loss: 0.00001941
Iteration 102/1000 | Loss: 0.00001941
Iteration 103/1000 | Loss: 0.00001941
Iteration 104/1000 | Loss: 0.00001941
Iteration 105/1000 | Loss: 0.00001941
Iteration 106/1000 | Loss: 0.00001941
Iteration 107/1000 | Loss: 0.00001941
Iteration 108/1000 | Loss: 0.00001941
Iteration 109/1000 | Loss: 0.00001941
Iteration 110/1000 | Loss: 0.00001941
Iteration 111/1000 | Loss: 0.00001941
Iteration 112/1000 | Loss: 0.00001940
Iteration 113/1000 | Loss: 0.00001940
Iteration 114/1000 | Loss: 0.00001940
Iteration 115/1000 | Loss: 0.00001940
Iteration 116/1000 | Loss: 0.00001940
Iteration 117/1000 | Loss: 0.00001940
Iteration 118/1000 | Loss: 0.00001940
Iteration 119/1000 | Loss: 0.00001940
Iteration 120/1000 | Loss: 0.00001940
Iteration 121/1000 | Loss: 0.00001940
Iteration 122/1000 | Loss: 0.00001940
Iteration 123/1000 | Loss: 0.00001940
Iteration 124/1000 | Loss: 0.00001940
Iteration 125/1000 | Loss: 0.00001940
Iteration 126/1000 | Loss: 0.00001940
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 126. Stopping optimization.
Last 5 losses: [1.940234869834967e-05, 1.940234869834967e-05, 1.940234869834967e-05, 1.940234869834967e-05, 1.940234869834967e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.940234869834967e-05

Optimization complete. Final v2v error: 3.665189027786255 mm

Highest mean error: 4.561092853546143 mm for frame 116

Lowest mean error: 2.90850830078125 mm for frame 202

Saving results

Total time: 43.6134614944458
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alison_posed_001/1023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alison_posed_001/1023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alison_posed_001/1023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00961053
Iteration 2/25 | Loss: 0.00250917
Iteration 3/25 | Loss: 0.00208198
Iteration 4/25 | Loss: 0.00195793
Iteration 5/25 | Loss: 0.00165231
Iteration 6/25 | Loss: 0.00151181
Iteration 7/25 | Loss: 0.00145219
Iteration 8/25 | Loss: 0.00140184
Iteration 9/25 | Loss: 0.00140101
Iteration 10/25 | Loss: 0.00136971
Iteration 11/25 | Loss: 0.00136626
Iteration 12/25 | Loss: 0.00134877
Iteration 13/25 | Loss: 0.00137013
Iteration 14/25 | Loss: 0.00132351
Iteration 15/25 | Loss: 0.00131566
Iteration 16/25 | Loss: 0.00132021
Iteration 17/25 | Loss: 0.00131356
Iteration 18/25 | Loss: 0.00131081
Iteration 19/25 | Loss: 0.00130926
Iteration 20/25 | Loss: 0.00130713
Iteration 21/25 | Loss: 0.00130667
Iteration 22/25 | Loss: 0.00130570
Iteration 23/25 | Loss: 0.00130286
Iteration 24/25 | Loss: 0.00129893
Iteration 25/25 | Loss: 0.00129799

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.47835135
Iteration 2/25 | Loss: 0.00078130
Iteration 3/25 | Loss: 0.00078130
Iteration 4/25 | Loss: 0.00078130
Iteration 5/25 | Loss: 0.00078130
Iteration 6/25 | Loss: 0.00078130
Iteration 7/25 | Loss: 0.00078130
Iteration 8/25 | Loss: 0.00078130
Iteration 9/25 | Loss: 0.00078130
Iteration 10/25 | Loss: 0.00078130
Iteration 11/25 | Loss: 0.00078130
Iteration 12/25 | Loss: 0.00078130
Iteration 13/25 | Loss: 0.00078130
Iteration 14/25 | Loss: 0.00078130
Iteration 15/25 | Loss: 0.00078130
Iteration 16/25 | Loss: 0.00078130
Iteration 17/25 | Loss: 0.00078130
Iteration 18/25 | Loss: 0.00078130
Iteration 19/25 | Loss: 0.00078130
Iteration 20/25 | Loss: 0.00078130
Iteration 21/25 | Loss: 0.00078130
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0007812989642843604, 0.0007812989642843604, 0.0007812989642843604, 0.0007812989642843604, 0.0007812989642843604]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007812989642843604

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00078130
Iteration 2/1000 | Loss: 0.00004709
Iteration 3/1000 | Loss: 0.00003296
Iteration 4/1000 | Loss: 0.00002989
Iteration 5/1000 | Loss: 0.00002826
Iteration 6/1000 | Loss: 0.00002715
Iteration 7/1000 | Loss: 0.00002644
Iteration 8/1000 | Loss: 0.00002593
Iteration 9/1000 | Loss: 0.00024097
Iteration 10/1000 | Loss: 0.00030831
Iteration 11/1000 | Loss: 0.00002781
Iteration 12/1000 | Loss: 0.00002377
Iteration 13/1000 | Loss: 0.00002119
Iteration 14/1000 | Loss: 0.00001907
Iteration 15/1000 | Loss: 0.00001763
Iteration 16/1000 | Loss: 0.00001698
Iteration 17/1000 | Loss: 0.00001646
Iteration 18/1000 | Loss: 0.00001602
Iteration 19/1000 | Loss: 0.00001567
Iteration 20/1000 | Loss: 0.00001539
Iteration 21/1000 | Loss: 0.00001523
Iteration 22/1000 | Loss: 0.00001519
Iteration 23/1000 | Loss: 0.00001505
Iteration 24/1000 | Loss: 0.00001504
Iteration 25/1000 | Loss: 0.00001501
Iteration 26/1000 | Loss: 0.00001501
Iteration 27/1000 | Loss: 0.00001497
Iteration 28/1000 | Loss: 0.00001497
Iteration 29/1000 | Loss: 0.00001497
Iteration 30/1000 | Loss: 0.00001496
Iteration 31/1000 | Loss: 0.00001495
Iteration 32/1000 | Loss: 0.00001492
Iteration 33/1000 | Loss: 0.00001492
Iteration 34/1000 | Loss: 0.00001487
Iteration 35/1000 | Loss: 0.00001486
Iteration 36/1000 | Loss: 0.00001486
Iteration 37/1000 | Loss: 0.00001485
Iteration 38/1000 | Loss: 0.00001485
Iteration 39/1000 | Loss: 0.00001485
Iteration 40/1000 | Loss: 0.00001485
Iteration 41/1000 | Loss: 0.00001485
Iteration 42/1000 | Loss: 0.00001485
Iteration 43/1000 | Loss: 0.00001485
Iteration 44/1000 | Loss: 0.00001484
Iteration 45/1000 | Loss: 0.00001484
Iteration 46/1000 | Loss: 0.00001484
Iteration 47/1000 | Loss: 0.00001484
Iteration 48/1000 | Loss: 0.00001484
Iteration 49/1000 | Loss: 0.00001483
Iteration 50/1000 | Loss: 0.00001483
Iteration 51/1000 | Loss: 0.00001483
Iteration 52/1000 | Loss: 0.00001483
Iteration 53/1000 | Loss: 0.00001483
Iteration 54/1000 | Loss: 0.00001483
Iteration 55/1000 | Loss: 0.00001482
Iteration 56/1000 | Loss: 0.00001482
Iteration 57/1000 | Loss: 0.00001482
Iteration 58/1000 | Loss: 0.00001482
Iteration 59/1000 | Loss: 0.00001481
Iteration 60/1000 | Loss: 0.00001481
Iteration 61/1000 | Loss: 0.00001481
Iteration 62/1000 | Loss: 0.00001481
Iteration 63/1000 | Loss: 0.00001481
Iteration 64/1000 | Loss: 0.00001481
Iteration 65/1000 | Loss: 0.00001480
Iteration 66/1000 | Loss: 0.00001480
Iteration 67/1000 | Loss: 0.00001480
Iteration 68/1000 | Loss: 0.00001480
Iteration 69/1000 | Loss: 0.00001480
Iteration 70/1000 | Loss: 0.00001480
Iteration 71/1000 | Loss: 0.00001480
Iteration 72/1000 | Loss: 0.00001480
Iteration 73/1000 | Loss: 0.00001480
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 73. Stopping optimization.
Last 5 losses: [1.4804807506152429e-05, 1.4804807506152429e-05, 1.4804807506152429e-05, 1.4804807506152429e-05, 1.4804807506152429e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4804807506152429e-05

Optimization complete. Final v2v error: 3.288661003112793 mm

Highest mean error: 3.627497911453247 mm for frame 104

Lowest mean error: 3.058537006378174 mm for frame 3

Saving results

Total time: 78.07400035858154
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alison_posed_001/1024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alison_posed_001/1024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alison_posed_001/1024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01039889
Iteration 2/25 | Loss: 0.00193289
Iteration 3/25 | Loss: 0.00146617
Iteration 4/25 | Loss: 0.00139720
Iteration 5/25 | Loss: 0.00136835
Iteration 6/25 | Loss: 0.00136360
Iteration 7/25 | Loss: 0.00136267
Iteration 8/25 | Loss: 0.00136238
Iteration 9/25 | Loss: 0.00136237
Iteration 10/25 | Loss: 0.00136237
Iteration 11/25 | Loss: 0.00136237
Iteration 12/25 | Loss: 0.00136237
Iteration 13/25 | Loss: 0.00136237
Iteration 14/25 | Loss: 0.00136237
Iteration 15/25 | Loss: 0.00136237
Iteration 16/25 | Loss: 0.00136237
Iteration 17/25 | Loss: 0.00136237
Iteration 18/25 | Loss: 0.00136237
Iteration 19/25 | Loss: 0.00136237
Iteration 20/25 | Loss: 0.00136237
Iteration 21/25 | Loss: 0.00136237
Iteration 22/25 | Loss: 0.00136237
Iteration 23/25 | Loss: 0.00136237
Iteration 24/25 | Loss: 0.00136237
Iteration 25/25 | Loss: 0.00136237

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.92403042
Iteration 2/25 | Loss: 0.00073163
Iteration 3/25 | Loss: 0.00073157
Iteration 4/25 | Loss: 0.00073157
Iteration 5/25 | Loss: 0.00073157
Iteration 6/25 | Loss: 0.00073157
Iteration 7/25 | Loss: 0.00073157
Iteration 8/25 | Loss: 0.00073157
Iteration 9/25 | Loss: 0.00073157
Iteration 10/25 | Loss: 0.00073157
Iteration 11/25 | Loss: 0.00073157
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0007315679104067385, 0.0007315679104067385, 0.0007315679104067385, 0.0007315679104067385, 0.0007315679104067385]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007315679104067385

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00073157
Iteration 2/1000 | Loss: 0.00007191
Iteration 3/1000 | Loss: 0.00004944
Iteration 4/1000 | Loss: 0.00003969
Iteration 5/1000 | Loss: 0.00003648
Iteration 6/1000 | Loss: 0.00003491
Iteration 7/1000 | Loss: 0.00003342
Iteration 8/1000 | Loss: 0.00003254
Iteration 9/1000 | Loss: 0.00003181
Iteration 10/1000 | Loss: 0.00003129
Iteration 11/1000 | Loss: 0.00003086
Iteration 12/1000 | Loss: 0.00003041
Iteration 13/1000 | Loss: 0.00003008
Iteration 14/1000 | Loss: 0.00002980
Iteration 15/1000 | Loss: 0.00002974
Iteration 16/1000 | Loss: 0.00002954
Iteration 17/1000 | Loss: 0.00002938
Iteration 18/1000 | Loss: 0.00002936
Iteration 19/1000 | Loss: 0.00002926
Iteration 20/1000 | Loss: 0.00002922
Iteration 21/1000 | Loss: 0.00002919
Iteration 22/1000 | Loss: 0.00002915
Iteration 23/1000 | Loss: 0.00002908
Iteration 24/1000 | Loss: 0.00002906
Iteration 25/1000 | Loss: 0.00002901
Iteration 26/1000 | Loss: 0.00002899
Iteration 27/1000 | Loss: 0.00002897
Iteration 28/1000 | Loss: 0.00002895
Iteration 29/1000 | Loss: 0.00002894
Iteration 30/1000 | Loss: 0.00002893
Iteration 31/1000 | Loss: 0.00002892
Iteration 32/1000 | Loss: 0.00002891
Iteration 33/1000 | Loss: 0.00002891
Iteration 34/1000 | Loss: 0.00002891
Iteration 35/1000 | Loss: 0.00002890
Iteration 36/1000 | Loss: 0.00002887
Iteration 37/1000 | Loss: 0.00002885
Iteration 38/1000 | Loss: 0.00002885
Iteration 39/1000 | Loss: 0.00002884
Iteration 40/1000 | Loss: 0.00002883
Iteration 41/1000 | Loss: 0.00002883
Iteration 42/1000 | Loss: 0.00002882
Iteration 43/1000 | Loss: 0.00002880
Iteration 44/1000 | Loss: 0.00002879
Iteration 45/1000 | Loss: 0.00002878
Iteration 46/1000 | Loss: 0.00002878
Iteration 47/1000 | Loss: 0.00002877
Iteration 48/1000 | Loss: 0.00002875
Iteration 49/1000 | Loss: 0.00002874
Iteration 50/1000 | Loss: 0.00002874
Iteration 51/1000 | Loss: 0.00002874
Iteration 52/1000 | Loss: 0.00002873
Iteration 53/1000 | Loss: 0.00002873
Iteration 54/1000 | Loss: 0.00002873
Iteration 55/1000 | Loss: 0.00002872
Iteration 56/1000 | Loss: 0.00002872
Iteration 57/1000 | Loss: 0.00002872
Iteration 58/1000 | Loss: 0.00002872
Iteration 59/1000 | Loss: 0.00002872
Iteration 60/1000 | Loss: 0.00002872
Iteration 61/1000 | Loss: 0.00002871
Iteration 62/1000 | Loss: 0.00002871
Iteration 63/1000 | Loss: 0.00002871
Iteration 64/1000 | Loss: 0.00002871
Iteration 65/1000 | Loss: 0.00002871
Iteration 66/1000 | Loss: 0.00002871
Iteration 67/1000 | Loss: 0.00002871
Iteration 68/1000 | Loss: 0.00002871
Iteration 69/1000 | Loss: 0.00002870
Iteration 70/1000 | Loss: 0.00002870
Iteration 71/1000 | Loss: 0.00002870
Iteration 72/1000 | Loss: 0.00002870
Iteration 73/1000 | Loss: 0.00002870
Iteration 74/1000 | Loss: 0.00002869
Iteration 75/1000 | Loss: 0.00002869
Iteration 76/1000 | Loss: 0.00002869
Iteration 77/1000 | Loss: 0.00002868
Iteration 78/1000 | Loss: 0.00002868
Iteration 79/1000 | Loss: 0.00002868
Iteration 80/1000 | Loss: 0.00002868
Iteration 81/1000 | Loss: 0.00002868
Iteration 82/1000 | Loss: 0.00002868
Iteration 83/1000 | Loss: 0.00002867
Iteration 84/1000 | Loss: 0.00002867
Iteration 85/1000 | Loss: 0.00002867
Iteration 86/1000 | Loss: 0.00002866
Iteration 87/1000 | Loss: 0.00002866
Iteration 88/1000 | Loss: 0.00002866
Iteration 89/1000 | Loss: 0.00002865
Iteration 90/1000 | Loss: 0.00002865
Iteration 91/1000 | Loss: 0.00002865
Iteration 92/1000 | Loss: 0.00002864
Iteration 93/1000 | Loss: 0.00002864
Iteration 94/1000 | Loss: 0.00002864
Iteration 95/1000 | Loss: 0.00002864
Iteration 96/1000 | Loss: 0.00002864
Iteration 97/1000 | Loss: 0.00002864
Iteration 98/1000 | Loss: 0.00002864
Iteration 99/1000 | Loss: 0.00002864
Iteration 100/1000 | Loss: 0.00002863
Iteration 101/1000 | Loss: 0.00002863
Iteration 102/1000 | Loss: 0.00002863
Iteration 103/1000 | Loss: 0.00002862
Iteration 104/1000 | Loss: 0.00002862
Iteration 105/1000 | Loss: 0.00002862
Iteration 106/1000 | Loss: 0.00002862
Iteration 107/1000 | Loss: 0.00002862
Iteration 108/1000 | Loss: 0.00002861
Iteration 109/1000 | Loss: 0.00002861
Iteration 110/1000 | Loss: 0.00002861
Iteration 111/1000 | Loss: 0.00002861
Iteration 112/1000 | Loss: 0.00002861
Iteration 113/1000 | Loss: 0.00002861
Iteration 114/1000 | Loss: 0.00002861
Iteration 115/1000 | Loss: 0.00002861
Iteration 116/1000 | Loss: 0.00002861
Iteration 117/1000 | Loss: 0.00002861
Iteration 118/1000 | Loss: 0.00002861
Iteration 119/1000 | Loss: 0.00002861
Iteration 120/1000 | Loss: 0.00002861
Iteration 121/1000 | Loss: 0.00002861
Iteration 122/1000 | Loss: 0.00002861
Iteration 123/1000 | Loss: 0.00002861
Iteration 124/1000 | Loss: 0.00002861
Iteration 125/1000 | Loss: 0.00002861
Iteration 126/1000 | Loss: 0.00002861
Iteration 127/1000 | Loss: 0.00002861
Iteration 128/1000 | Loss: 0.00002861
Iteration 129/1000 | Loss: 0.00002861
Iteration 130/1000 | Loss: 0.00002861
Iteration 131/1000 | Loss: 0.00002861
Iteration 132/1000 | Loss: 0.00002861
Iteration 133/1000 | Loss: 0.00002861
Iteration 134/1000 | Loss: 0.00002861
Iteration 135/1000 | Loss: 0.00002861
Iteration 136/1000 | Loss: 0.00002861
Iteration 137/1000 | Loss: 0.00002861
Iteration 138/1000 | Loss: 0.00002861
Iteration 139/1000 | Loss: 0.00002861
Iteration 140/1000 | Loss: 0.00002861
Iteration 141/1000 | Loss: 0.00002861
Iteration 142/1000 | Loss: 0.00002861
Iteration 143/1000 | Loss: 0.00002861
Iteration 144/1000 | Loss: 0.00002861
Iteration 145/1000 | Loss: 0.00002861
Iteration 146/1000 | Loss: 0.00002861
Iteration 147/1000 | Loss: 0.00002861
Iteration 148/1000 | Loss: 0.00002861
Iteration 149/1000 | Loss: 0.00002861
Iteration 150/1000 | Loss: 0.00002861
Iteration 151/1000 | Loss: 0.00002861
Iteration 152/1000 | Loss: 0.00002861
Iteration 153/1000 | Loss: 0.00002861
Iteration 154/1000 | Loss: 0.00002861
Iteration 155/1000 | Loss: 0.00002861
Iteration 156/1000 | Loss: 0.00002861
Iteration 157/1000 | Loss: 0.00002861
Iteration 158/1000 | Loss: 0.00002861
Iteration 159/1000 | Loss: 0.00002861
Iteration 160/1000 | Loss: 0.00002861
Iteration 161/1000 | Loss: 0.00002861
Iteration 162/1000 | Loss: 0.00002861
Iteration 163/1000 | Loss: 0.00002861
Iteration 164/1000 | Loss: 0.00002861
Iteration 165/1000 | Loss: 0.00002861
Iteration 166/1000 | Loss: 0.00002861
Iteration 167/1000 | Loss: 0.00002861
Iteration 168/1000 | Loss: 0.00002861
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 168. Stopping optimization.
Last 5 losses: [2.8606558771571144e-05, 2.8606558771571144e-05, 2.8606558771571144e-05, 2.8606558771571144e-05, 2.8606558771571144e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.8606558771571144e-05

Optimization complete. Final v2v error: 4.389226913452148 mm

Highest mean error: 5.146791458129883 mm for frame 163

Lowest mean error: 3.885305166244507 mm for frame 136

Saving results

Total time: 51.32741403579712
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alison_posed_001/1065/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alison_posed_001/1065.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alison_posed_001/1065
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01085554
Iteration 2/25 | Loss: 0.00242787
Iteration 3/25 | Loss: 0.00166007
Iteration 4/25 | Loss: 0.00143826
Iteration 5/25 | Loss: 0.00141070
Iteration 6/25 | Loss: 0.00141491
Iteration 7/25 | Loss: 0.00149338
Iteration 8/25 | Loss: 0.00143477
Iteration 9/25 | Loss: 0.00135316
Iteration 10/25 | Loss: 0.00133736
Iteration 11/25 | Loss: 0.00135139
Iteration 12/25 | Loss: 0.00134197
Iteration 13/25 | Loss: 0.00134494
Iteration 14/25 | Loss: 0.00133906
Iteration 15/25 | Loss: 0.00134004
Iteration 16/25 | Loss: 0.00133780
Iteration 17/25 | Loss: 0.00133688
Iteration 18/25 | Loss: 0.00133872
Iteration 19/25 | Loss: 0.00133051
Iteration 20/25 | Loss: 0.00133046
Iteration 21/25 | Loss: 0.00133131
Iteration 22/25 | Loss: 0.00133085
Iteration 23/25 | Loss: 0.00134889
Iteration 24/25 | Loss: 0.00134419
Iteration 25/25 | Loss: 0.00134598

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.50745654
Iteration 2/25 | Loss: 0.00137616
Iteration 3/25 | Loss: 0.00136484
Iteration 4/25 | Loss: 0.00136484
Iteration 5/25 | Loss: 0.00136484
Iteration 6/25 | Loss: 0.00136484
Iteration 7/25 | Loss: 0.00136484
Iteration 8/25 | Loss: 0.00136484
Iteration 9/25 | Loss: 0.00136484
Iteration 10/25 | Loss: 0.00136484
Iteration 11/25 | Loss: 0.00136484
Iteration 12/25 | Loss: 0.00136484
Iteration 13/25 | Loss: 0.00136484
Iteration 14/25 | Loss: 0.00136484
Iteration 15/25 | Loss: 0.00136484
Iteration 16/25 | Loss: 0.00136484
Iteration 17/25 | Loss: 0.00136484
Iteration 18/25 | Loss: 0.00136484
Iteration 19/25 | Loss: 0.00136484
Iteration 20/25 | Loss: 0.00136484
Iteration 21/25 | Loss: 0.00136484
Iteration 22/25 | Loss: 0.00136484
Iteration 23/25 | Loss: 0.00136484
Iteration 24/25 | Loss: 0.00136484
Iteration 25/25 | Loss: 0.00136484

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00136484
Iteration 2/1000 | Loss: 0.00043585
Iteration 3/1000 | Loss: 0.00054720
Iteration 4/1000 | Loss: 0.00042436
Iteration 5/1000 | Loss: 0.00042663
Iteration 6/1000 | Loss: 0.00030103
Iteration 7/1000 | Loss: 0.00036820
Iteration 8/1000 | Loss: 0.00028716
Iteration 9/1000 | Loss: 0.00029780
Iteration 10/1000 | Loss: 0.00030049
Iteration 11/1000 | Loss: 0.00026854
Iteration 12/1000 | Loss: 0.00025615
Iteration 13/1000 | Loss: 0.00026124
Iteration 14/1000 | Loss: 0.00026070
Iteration 15/1000 | Loss: 0.00027290
Iteration 16/1000 | Loss: 0.00007679
Iteration 17/1000 | Loss: 0.00004295
Iteration 18/1000 | Loss: 0.00003905
Iteration 19/1000 | Loss: 0.00038902
Iteration 20/1000 | Loss: 0.00021639
Iteration 21/1000 | Loss: 0.00021655
Iteration 22/1000 | Loss: 0.00009623
Iteration 23/1000 | Loss: 0.00032604
Iteration 24/1000 | Loss: 0.00015822
Iteration 25/1000 | Loss: 0.00014746
Iteration 26/1000 | Loss: 0.00083689
Iteration 27/1000 | Loss: 0.00084981
Iteration 28/1000 | Loss: 0.00039588
Iteration 29/1000 | Loss: 0.00013982
Iteration 30/1000 | Loss: 0.00004268
Iteration 31/1000 | Loss: 0.00004031
Iteration 32/1000 | Loss: 0.00027013
Iteration 33/1000 | Loss: 0.00011754
Iteration 34/1000 | Loss: 0.00039128
Iteration 35/1000 | Loss: 0.00053954
Iteration 36/1000 | Loss: 0.00039856
Iteration 37/1000 | Loss: 0.00065427
Iteration 38/1000 | Loss: 0.00013983
Iteration 39/1000 | Loss: 0.00062456
Iteration 40/1000 | Loss: 0.00061443
Iteration 41/1000 | Loss: 0.00048620
Iteration 42/1000 | Loss: 0.00116574
Iteration 43/1000 | Loss: 0.00041920
Iteration 44/1000 | Loss: 0.00016198
Iteration 45/1000 | Loss: 0.00048656
Iteration 46/1000 | Loss: 0.00042146
Iteration 47/1000 | Loss: 0.00054969
Iteration 48/1000 | Loss: 0.00044969
Iteration 49/1000 | Loss: 0.00047374
Iteration 50/1000 | Loss: 0.00027887
Iteration 51/1000 | Loss: 0.00024665
Iteration 52/1000 | Loss: 0.00044672
Iteration 53/1000 | Loss: 0.00032679
Iteration 54/1000 | Loss: 0.00065006
Iteration 55/1000 | Loss: 0.00067396
Iteration 56/1000 | Loss: 0.00048843
Iteration 57/1000 | Loss: 0.00026219
Iteration 58/1000 | Loss: 0.00036500
Iteration 59/1000 | Loss: 0.00033913
Iteration 60/1000 | Loss: 0.00027848
Iteration 61/1000 | Loss: 0.00017240
Iteration 62/1000 | Loss: 0.00050095
Iteration 63/1000 | Loss: 0.00011824
Iteration 64/1000 | Loss: 0.00003848
Iteration 65/1000 | Loss: 0.00008414
Iteration 66/1000 | Loss: 0.00093361
Iteration 67/1000 | Loss: 0.00049363
Iteration 68/1000 | Loss: 0.00074364
Iteration 69/1000 | Loss: 0.00052470
Iteration 70/1000 | Loss: 0.00051809
Iteration 71/1000 | Loss: 0.00045039
Iteration 72/1000 | Loss: 0.00038347
Iteration 73/1000 | Loss: 0.00030940
Iteration 74/1000 | Loss: 0.00007343
Iteration 75/1000 | Loss: 0.00016738
Iteration 76/1000 | Loss: 0.00007923
Iteration 77/1000 | Loss: 0.00003708
Iteration 78/1000 | Loss: 0.00003693
Iteration 79/1000 | Loss: 0.00020672
Iteration 80/1000 | Loss: 0.00023643
Iteration 81/1000 | Loss: 0.00007811
Iteration 82/1000 | Loss: 0.00040135
Iteration 83/1000 | Loss: 0.00022085
Iteration 84/1000 | Loss: 0.00032810
Iteration 85/1000 | Loss: 0.00010383
Iteration 86/1000 | Loss: 0.00015117
Iteration 87/1000 | Loss: 0.00029348
Iteration 88/1000 | Loss: 0.00028333
Iteration 89/1000 | Loss: 0.00004278
Iteration 90/1000 | Loss: 0.00019183
Iteration 91/1000 | Loss: 0.00014877
Iteration 92/1000 | Loss: 0.00018179
Iteration 93/1000 | Loss: 0.00023450
Iteration 94/1000 | Loss: 0.00009668
Iteration 95/1000 | Loss: 0.00030793
Iteration 96/1000 | Loss: 0.00040259
Iteration 97/1000 | Loss: 0.00038320
Iteration 98/1000 | Loss: 0.00006389
Iteration 99/1000 | Loss: 0.00009260
Iteration 100/1000 | Loss: 0.00021704
Iteration 101/1000 | Loss: 0.00021405
Iteration 102/1000 | Loss: 0.00010851
Iteration 103/1000 | Loss: 0.00025346
Iteration 104/1000 | Loss: 0.00037996
Iteration 105/1000 | Loss: 0.00059654
Iteration 106/1000 | Loss: 0.00034894
Iteration 107/1000 | Loss: 0.00032473
Iteration 108/1000 | Loss: 0.00006050
Iteration 109/1000 | Loss: 0.00018751
Iteration 110/1000 | Loss: 0.00005666
Iteration 111/1000 | Loss: 0.00003829
Iteration 112/1000 | Loss: 0.00005180
Iteration 113/1000 | Loss: 0.00006724
Iteration 114/1000 | Loss: 0.00008298
Iteration 115/1000 | Loss: 0.00006114
Iteration 116/1000 | Loss: 0.00003722
Iteration 117/1000 | Loss: 0.00003430
Iteration 118/1000 | Loss: 0.00003386
Iteration 119/1000 | Loss: 0.00006674
Iteration 120/1000 | Loss: 0.00003272
Iteration 121/1000 | Loss: 0.00039220
Iteration 122/1000 | Loss: 0.00024076
Iteration 123/1000 | Loss: 0.00051358
Iteration 124/1000 | Loss: 0.00042195
Iteration 125/1000 | Loss: 0.00014953
Iteration 126/1000 | Loss: 0.00013919
Iteration 127/1000 | Loss: 0.00008395
Iteration 128/1000 | Loss: 0.00019466
Iteration 129/1000 | Loss: 0.00006195
Iteration 130/1000 | Loss: 0.00003766
Iteration 131/1000 | Loss: 0.00019132
Iteration 132/1000 | Loss: 0.00008821
Iteration 133/1000 | Loss: 0.00049080
Iteration 134/1000 | Loss: 0.00062927
Iteration 135/1000 | Loss: 0.00051455
Iteration 136/1000 | Loss: 0.00070897
Iteration 137/1000 | Loss: 0.00043025
Iteration 138/1000 | Loss: 0.00020268
Iteration 139/1000 | Loss: 0.00020765
Iteration 140/1000 | Loss: 0.00003160
Iteration 141/1000 | Loss: 0.00003039
Iteration 142/1000 | Loss: 0.00004120
Iteration 143/1000 | Loss: 0.00002877
Iteration 144/1000 | Loss: 0.00025740
Iteration 145/1000 | Loss: 0.00007967
Iteration 146/1000 | Loss: 0.00018540
Iteration 147/1000 | Loss: 0.00006911
Iteration 148/1000 | Loss: 0.00002835
Iteration 149/1000 | Loss: 0.00029783
Iteration 150/1000 | Loss: 0.00009758
Iteration 151/1000 | Loss: 0.00002795
Iteration 152/1000 | Loss: 0.00007470
Iteration 153/1000 | Loss: 0.00049633
Iteration 154/1000 | Loss: 0.00007092
Iteration 155/1000 | Loss: 0.00003627
Iteration 156/1000 | Loss: 0.00010407
Iteration 157/1000 | Loss: 0.00032391
Iteration 158/1000 | Loss: 0.00002946
Iteration 159/1000 | Loss: 0.00006756
Iteration 160/1000 | Loss: 0.00005328
Iteration 161/1000 | Loss: 0.00002587
Iteration 162/1000 | Loss: 0.00004102
Iteration 163/1000 | Loss: 0.00002478
Iteration 164/1000 | Loss: 0.00003600
Iteration 165/1000 | Loss: 0.00028835
Iteration 166/1000 | Loss: 0.00024537
Iteration 167/1000 | Loss: 0.00018496
Iteration 168/1000 | Loss: 0.00004917
Iteration 169/1000 | Loss: 0.00008328
Iteration 170/1000 | Loss: 0.00003260
Iteration 171/1000 | Loss: 0.00002675
Iteration 172/1000 | Loss: 0.00002379
Iteration 173/1000 | Loss: 0.00025395
Iteration 174/1000 | Loss: 0.00034626
Iteration 175/1000 | Loss: 0.00024662
Iteration 176/1000 | Loss: 0.00023213
Iteration 177/1000 | Loss: 0.00024123
Iteration 178/1000 | Loss: 0.00019385
Iteration 179/1000 | Loss: 0.00023653
Iteration 180/1000 | Loss: 0.00018915
Iteration 181/1000 | Loss: 0.00023659
Iteration 182/1000 | Loss: 0.00017351
Iteration 183/1000 | Loss: 0.00003198
Iteration 184/1000 | Loss: 0.00012072
Iteration 185/1000 | Loss: 0.00002894
Iteration 186/1000 | Loss: 0.00003033
Iteration 187/1000 | Loss: 0.00003947
Iteration 188/1000 | Loss: 0.00002255
Iteration 189/1000 | Loss: 0.00002828
Iteration 190/1000 | Loss: 0.00002903
Iteration 191/1000 | Loss: 0.00002542
Iteration 192/1000 | Loss: 0.00002245
Iteration 193/1000 | Loss: 0.00002244
Iteration 194/1000 | Loss: 0.00002531
Iteration 195/1000 | Loss: 0.00002241
Iteration 196/1000 | Loss: 0.00002241
Iteration 197/1000 | Loss: 0.00002241
Iteration 198/1000 | Loss: 0.00002240
Iteration 199/1000 | Loss: 0.00002240
Iteration 200/1000 | Loss: 0.00002240
Iteration 201/1000 | Loss: 0.00002240
Iteration 202/1000 | Loss: 0.00002240
Iteration 203/1000 | Loss: 0.00002240
Iteration 204/1000 | Loss: 0.00002240
Iteration 205/1000 | Loss: 0.00002240
Iteration 206/1000 | Loss: 0.00002240
Iteration 207/1000 | Loss: 0.00002551
Iteration 208/1000 | Loss: 0.00005395
Iteration 209/1000 | Loss: 0.00002315
Iteration 210/1000 | Loss: 0.00002227
Iteration 211/1000 | Loss: 0.00002223
Iteration 212/1000 | Loss: 0.00002220
Iteration 213/1000 | Loss: 0.00002220
Iteration 214/1000 | Loss: 0.00002219
Iteration 215/1000 | Loss: 0.00002219
Iteration 216/1000 | Loss: 0.00002218
Iteration 217/1000 | Loss: 0.00002218
Iteration 218/1000 | Loss: 0.00002218
Iteration 219/1000 | Loss: 0.00002217
Iteration 220/1000 | Loss: 0.00003017
Iteration 221/1000 | Loss: 0.00002350
Iteration 222/1000 | Loss: 0.00002215
Iteration 223/1000 | Loss: 0.00002214
Iteration 224/1000 | Loss: 0.00002214
Iteration 225/1000 | Loss: 0.00002214
Iteration 226/1000 | Loss: 0.00002214
Iteration 227/1000 | Loss: 0.00002214
Iteration 228/1000 | Loss: 0.00002214
Iteration 229/1000 | Loss: 0.00002214
Iteration 230/1000 | Loss: 0.00002214
Iteration 231/1000 | Loss: 0.00002214
Iteration 232/1000 | Loss: 0.00002214
Iteration 233/1000 | Loss: 0.00002214
Iteration 234/1000 | Loss: 0.00002214
Iteration 235/1000 | Loss: 0.00002214
Iteration 236/1000 | Loss: 0.00002213
Iteration 237/1000 | Loss: 0.00002406
Iteration 238/1000 | Loss: 0.00002210
Iteration 239/1000 | Loss: 0.00002210
Iteration 240/1000 | Loss: 0.00002210
Iteration 241/1000 | Loss: 0.00002210
Iteration 242/1000 | Loss: 0.00002210
Iteration 243/1000 | Loss: 0.00002210
Iteration 244/1000 | Loss: 0.00002209
Iteration 245/1000 | Loss: 0.00002209
Iteration 246/1000 | Loss: 0.00002209
Iteration 247/1000 | Loss: 0.00002209
Iteration 248/1000 | Loss: 0.00002209
Iteration 249/1000 | Loss: 0.00002209
Iteration 250/1000 | Loss: 0.00002209
Iteration 251/1000 | Loss: 0.00002209
Iteration 252/1000 | Loss: 0.00002208
Iteration 253/1000 | Loss: 0.00002208
Iteration 254/1000 | Loss: 0.00002208
Iteration 255/1000 | Loss: 0.00002208
Iteration 256/1000 | Loss: 0.00002208
Iteration 257/1000 | Loss: 0.00002208
Iteration 258/1000 | Loss: 0.00002208
Iteration 259/1000 | Loss: 0.00002208
Iteration 260/1000 | Loss: 0.00002208
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 260. Stopping optimization.
Last 5 losses: [2.2083819203544408e-05, 2.2083819203544408e-05, 2.2083819203544408e-05, 2.2083819203544408e-05, 2.2083819203544408e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2083819203544408e-05

Optimization complete. Final v2v error: 3.6974902153015137 mm

Highest mean error: 9.889108657836914 mm for frame 93

Lowest mean error: 3.1296846866607666 mm for frame 63

Saving results

Total time: 314.7875757217407
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alison_posed_001/1069/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alison_posed_001/1069.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alison_posed_001/1069
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00828908
Iteration 2/25 | Loss: 0.00143341
Iteration 3/25 | Loss: 0.00128673
Iteration 4/25 | Loss: 0.00126345
Iteration 5/25 | Loss: 0.00125693
Iteration 6/25 | Loss: 0.00125664
Iteration 7/25 | Loss: 0.00125664
Iteration 8/25 | Loss: 0.00125664
Iteration 9/25 | Loss: 0.00125664
Iteration 10/25 | Loss: 0.00125664
Iteration 11/25 | Loss: 0.00125664
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001256638439372182, 0.001256638439372182, 0.001256638439372182, 0.001256638439372182, 0.001256638439372182]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001256638439372182

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.05350053
Iteration 2/25 | Loss: 0.00055828
Iteration 3/25 | Loss: 0.00055827
Iteration 4/25 | Loss: 0.00055827
Iteration 5/25 | Loss: 0.00055827
Iteration 6/25 | Loss: 0.00055827
Iteration 7/25 | Loss: 0.00055827
Iteration 8/25 | Loss: 0.00055826
Iteration 9/25 | Loss: 0.00055826
Iteration 10/25 | Loss: 0.00055826
Iteration 11/25 | Loss: 0.00055826
Iteration 12/25 | Loss: 0.00055826
Iteration 13/25 | Loss: 0.00055826
Iteration 14/25 | Loss: 0.00055826
Iteration 15/25 | Loss: 0.00055826
Iteration 16/25 | Loss: 0.00055826
Iteration 17/25 | Loss: 0.00055826
Iteration 18/25 | Loss: 0.00055826
Iteration 19/25 | Loss: 0.00055826
Iteration 20/25 | Loss: 0.00055826
Iteration 21/25 | Loss: 0.00055826
Iteration 22/25 | Loss: 0.00055826
Iteration 23/25 | Loss: 0.00055826
Iteration 24/25 | Loss: 0.00055826
Iteration 25/25 | Loss: 0.00055826

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00055826
Iteration 2/1000 | Loss: 0.00004130
Iteration 3/1000 | Loss: 0.00003148
Iteration 4/1000 | Loss: 0.00002920
Iteration 5/1000 | Loss: 0.00002761
Iteration 6/1000 | Loss: 0.00002623
Iteration 7/1000 | Loss: 0.00002559
Iteration 8/1000 | Loss: 0.00002506
Iteration 9/1000 | Loss: 0.00002483
Iteration 10/1000 | Loss: 0.00002449
Iteration 11/1000 | Loss: 0.00002424
Iteration 12/1000 | Loss: 0.00002405
Iteration 13/1000 | Loss: 0.00002387
Iteration 14/1000 | Loss: 0.00002375
Iteration 15/1000 | Loss: 0.00002373
Iteration 16/1000 | Loss: 0.00002368
Iteration 17/1000 | Loss: 0.00002367
Iteration 18/1000 | Loss: 0.00002366
Iteration 19/1000 | Loss: 0.00002366
Iteration 20/1000 | Loss: 0.00002360
Iteration 21/1000 | Loss: 0.00002357
Iteration 22/1000 | Loss: 0.00002356
Iteration 23/1000 | Loss: 0.00002356
Iteration 24/1000 | Loss: 0.00002355
Iteration 25/1000 | Loss: 0.00002355
Iteration 26/1000 | Loss: 0.00002355
Iteration 27/1000 | Loss: 0.00002354
Iteration 28/1000 | Loss: 0.00002354
Iteration 29/1000 | Loss: 0.00002353
Iteration 30/1000 | Loss: 0.00002352
Iteration 31/1000 | Loss: 0.00002349
Iteration 32/1000 | Loss: 0.00002348
Iteration 33/1000 | Loss: 0.00002348
Iteration 34/1000 | Loss: 0.00002348
Iteration 35/1000 | Loss: 0.00002348
Iteration 36/1000 | Loss: 0.00002348
Iteration 37/1000 | Loss: 0.00002348
Iteration 38/1000 | Loss: 0.00002348
Iteration 39/1000 | Loss: 0.00002347
Iteration 40/1000 | Loss: 0.00002347
Iteration 41/1000 | Loss: 0.00002347
Iteration 42/1000 | Loss: 0.00002346
Iteration 43/1000 | Loss: 0.00002346
Iteration 44/1000 | Loss: 0.00002346
Iteration 45/1000 | Loss: 0.00002346
Iteration 46/1000 | Loss: 0.00002346
Iteration 47/1000 | Loss: 0.00002346
Iteration 48/1000 | Loss: 0.00002346
Iteration 49/1000 | Loss: 0.00002346
Iteration 50/1000 | Loss: 0.00002346
Iteration 51/1000 | Loss: 0.00002346
Iteration 52/1000 | Loss: 0.00002346
Iteration 53/1000 | Loss: 0.00002345
Iteration 54/1000 | Loss: 0.00002345
Iteration 55/1000 | Loss: 0.00002345
Iteration 56/1000 | Loss: 0.00002345
Iteration 57/1000 | Loss: 0.00002345
Iteration 58/1000 | Loss: 0.00002345
Iteration 59/1000 | Loss: 0.00002345
Iteration 60/1000 | Loss: 0.00002345
Iteration 61/1000 | Loss: 0.00002345
Iteration 62/1000 | Loss: 0.00002344
Iteration 63/1000 | Loss: 0.00002344
Iteration 64/1000 | Loss: 0.00002344
Iteration 65/1000 | Loss: 0.00002344
Iteration 66/1000 | Loss: 0.00002344
Iteration 67/1000 | Loss: 0.00002344
Iteration 68/1000 | Loss: 0.00002344
Iteration 69/1000 | Loss: 0.00002344
Iteration 70/1000 | Loss: 0.00002344
Iteration 71/1000 | Loss: 0.00002343
Iteration 72/1000 | Loss: 0.00002343
Iteration 73/1000 | Loss: 0.00002343
Iteration 74/1000 | Loss: 0.00002343
Iteration 75/1000 | Loss: 0.00002343
Iteration 76/1000 | Loss: 0.00002343
Iteration 77/1000 | Loss: 0.00002343
Iteration 78/1000 | Loss: 0.00002343
Iteration 79/1000 | Loss: 0.00002343
Iteration 80/1000 | Loss: 0.00002342
Iteration 81/1000 | Loss: 0.00002342
Iteration 82/1000 | Loss: 0.00002342
Iteration 83/1000 | Loss: 0.00002342
Iteration 84/1000 | Loss: 0.00002342
Iteration 85/1000 | Loss: 0.00002342
Iteration 86/1000 | Loss: 0.00002342
Iteration 87/1000 | Loss: 0.00002341
Iteration 88/1000 | Loss: 0.00002341
Iteration 89/1000 | Loss: 0.00002341
Iteration 90/1000 | Loss: 0.00002341
Iteration 91/1000 | Loss: 0.00002341
Iteration 92/1000 | Loss: 0.00002341
Iteration 93/1000 | Loss: 0.00002341
Iteration 94/1000 | Loss: 0.00002341
Iteration 95/1000 | Loss: 0.00002341
Iteration 96/1000 | Loss: 0.00002341
Iteration 97/1000 | Loss: 0.00002341
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 97. Stopping optimization.
Last 5 losses: [2.3410157155012712e-05, 2.3410157155012712e-05, 2.3410157155012712e-05, 2.3410157155012712e-05, 2.3410157155012712e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3410157155012712e-05

Optimization complete. Final v2v error: 4.095798015594482 mm

Highest mean error: 4.721249103546143 mm for frame 239

Lowest mean error: 3.6992595195770264 mm for frame 3

Saving results

Total time: 39.08720397949219
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alison_posed_001/1021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alison_posed_001/1021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alison_posed_001/1021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01024631
Iteration 2/25 | Loss: 0.00466630
Iteration 3/25 | Loss: 0.00320331
Iteration 4/25 | Loss: 0.00266779
Iteration 5/25 | Loss: 0.00246930
Iteration 6/25 | Loss: 0.00235910
Iteration 7/25 | Loss: 0.00223598
Iteration 8/25 | Loss: 0.00210776
Iteration 9/25 | Loss: 0.00208204
Iteration 10/25 | Loss: 0.00197644
Iteration 11/25 | Loss: 0.00198034
Iteration 12/25 | Loss: 0.00192957
Iteration 13/25 | Loss: 0.00190901
Iteration 14/25 | Loss: 0.00187235
Iteration 15/25 | Loss: 0.00186353
Iteration 16/25 | Loss: 0.00189539
Iteration 17/25 | Loss: 0.00186337
Iteration 18/25 | Loss: 0.00189502
Iteration 19/25 | Loss: 0.00186517
Iteration 20/25 | Loss: 0.00180890
Iteration 21/25 | Loss: 0.00179089
Iteration 22/25 | Loss: 0.00180061
Iteration 23/25 | Loss: 0.00178735
Iteration 24/25 | Loss: 0.00178344
Iteration 25/25 | Loss: 0.00178443

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46381807
Iteration 2/25 | Loss: 0.00482916
Iteration 3/25 | Loss: 0.00323405
Iteration 4/25 | Loss: 0.00323332
Iteration 5/25 | Loss: 0.00323332
Iteration 6/25 | Loss: 0.00323331
Iteration 7/25 | Loss: 0.00323331
Iteration 8/25 | Loss: 0.00323331
Iteration 9/25 | Loss: 0.00323331
Iteration 10/25 | Loss: 0.00323331
Iteration 11/25 | Loss: 0.00323331
Iteration 12/25 | Loss: 0.00323331
Iteration 13/25 | Loss: 0.00323331
Iteration 14/25 | Loss: 0.00323331
Iteration 15/25 | Loss: 0.00323331
Iteration 16/25 | Loss: 0.00323331
Iteration 17/25 | Loss: 0.00323331
Iteration 18/25 | Loss: 0.00323331
Iteration 19/25 | Loss: 0.00323331
Iteration 20/25 | Loss: 0.00323331
Iteration 21/25 | Loss: 0.00323331
Iteration 22/25 | Loss: 0.00323331
Iteration 23/25 | Loss: 0.00323331
Iteration 24/25 | Loss: 0.00323331
Iteration 25/25 | Loss: 0.00323331

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00323331
Iteration 2/1000 | Loss: 0.00212369
Iteration 3/1000 | Loss: 0.00232456
Iteration 4/1000 | Loss: 0.00096659
Iteration 5/1000 | Loss: 0.00156566
Iteration 6/1000 | Loss: 0.00051906
Iteration 7/1000 | Loss: 0.00147930
Iteration 8/1000 | Loss: 0.00060843
Iteration 9/1000 | Loss: 0.00115566
Iteration 10/1000 | Loss: 0.00084743
Iteration 11/1000 | Loss: 0.00078432
Iteration 12/1000 | Loss: 0.00079536
Iteration 13/1000 | Loss: 0.00047702
Iteration 14/1000 | Loss: 0.00030499
Iteration 15/1000 | Loss: 0.00029788
Iteration 16/1000 | Loss: 0.00037043
Iteration 17/1000 | Loss: 0.00038175
Iteration 18/1000 | Loss: 0.00024023
Iteration 19/1000 | Loss: 0.00023259
Iteration 20/1000 | Loss: 0.00024538
Iteration 21/1000 | Loss: 0.00026860
Iteration 22/1000 | Loss: 0.00022245
Iteration 23/1000 | Loss: 0.00016685
Iteration 24/1000 | Loss: 0.00016331
Iteration 25/1000 | Loss: 0.00016492
Iteration 26/1000 | Loss: 0.00047843
Iteration 27/1000 | Loss: 0.00016896
Iteration 28/1000 | Loss: 0.00016444
Iteration 29/1000 | Loss: 0.00086044
Iteration 30/1000 | Loss: 0.00018065
Iteration 31/1000 | Loss: 0.00052489
Iteration 32/1000 | Loss: 0.00045175
Iteration 33/1000 | Loss: 0.00043758
Iteration 34/1000 | Loss: 0.00015896
Iteration 35/1000 | Loss: 0.00036109
Iteration 36/1000 | Loss: 0.00023534
Iteration 37/1000 | Loss: 0.00016218
Iteration 38/1000 | Loss: 0.00056059
Iteration 39/1000 | Loss: 0.00015387
Iteration 40/1000 | Loss: 0.00036090
Iteration 41/1000 | Loss: 0.00015297
Iteration 42/1000 | Loss: 0.00014873
Iteration 43/1000 | Loss: 0.00014760
Iteration 44/1000 | Loss: 0.00018639
Iteration 45/1000 | Loss: 0.00100677
Iteration 46/1000 | Loss: 0.00065290
Iteration 47/1000 | Loss: 0.00075214
Iteration 48/1000 | Loss: 0.00015141
Iteration 49/1000 | Loss: 0.00014618
Iteration 50/1000 | Loss: 0.00014417
Iteration 51/1000 | Loss: 0.00014281
Iteration 52/1000 | Loss: 0.00079215
Iteration 53/1000 | Loss: 0.00024169
Iteration 54/1000 | Loss: 0.00031493
Iteration 55/1000 | Loss: 0.00014226
Iteration 56/1000 | Loss: 0.00014085
Iteration 57/1000 | Loss: 0.00017405
Iteration 58/1000 | Loss: 0.00013962
Iteration 59/1000 | Loss: 0.00016951
Iteration 60/1000 | Loss: 0.00013817
Iteration 61/1000 | Loss: 0.00017631
Iteration 62/1000 | Loss: 0.00013753
Iteration 63/1000 | Loss: 0.00016598
Iteration 64/1000 | Loss: 0.00060790
Iteration 65/1000 | Loss: 0.00060489
Iteration 66/1000 | Loss: 0.00017831
Iteration 67/1000 | Loss: 0.00029269
Iteration 68/1000 | Loss: 0.00013585
Iteration 69/1000 | Loss: 0.00016346
Iteration 70/1000 | Loss: 0.00017173
Iteration 71/1000 | Loss: 0.00013496
Iteration 72/1000 | Loss: 0.00013450
Iteration 73/1000 | Loss: 0.00013410
Iteration 74/1000 | Loss: 0.00013356
Iteration 75/1000 | Loss: 0.00055237
Iteration 76/1000 | Loss: 0.00039945
Iteration 77/1000 | Loss: 0.00019050
Iteration 78/1000 | Loss: 0.00016667
Iteration 79/1000 | Loss: 0.00013312
Iteration 80/1000 | Loss: 0.00018914
Iteration 81/1000 | Loss: 0.00013262
Iteration 82/1000 | Loss: 0.00016646
Iteration 83/1000 | Loss: 0.00013230
Iteration 84/1000 | Loss: 0.00013209
Iteration 85/1000 | Loss: 0.00017887
Iteration 86/1000 | Loss: 0.00013181
Iteration 87/1000 | Loss: 0.00013179
Iteration 88/1000 | Loss: 0.00015922
Iteration 89/1000 | Loss: 0.00013941
Iteration 90/1000 | Loss: 0.00013163
Iteration 91/1000 | Loss: 0.00013156
Iteration 92/1000 | Loss: 0.00013155
Iteration 93/1000 | Loss: 0.00013145
Iteration 94/1000 | Loss: 0.00013141
Iteration 95/1000 | Loss: 0.00013133
Iteration 96/1000 | Loss: 0.00013133
Iteration 97/1000 | Loss: 0.00013133
Iteration 98/1000 | Loss: 0.00013133
Iteration 99/1000 | Loss: 0.00013132
Iteration 100/1000 | Loss: 0.00013132
Iteration 101/1000 | Loss: 0.00013132
Iteration 102/1000 | Loss: 0.00013132
Iteration 103/1000 | Loss: 0.00013132
Iteration 104/1000 | Loss: 0.00013132
Iteration 105/1000 | Loss: 0.00013132
Iteration 106/1000 | Loss: 0.00013132
Iteration 107/1000 | Loss: 0.00013132
Iteration 108/1000 | Loss: 0.00013128
Iteration 109/1000 | Loss: 0.00013127
Iteration 110/1000 | Loss: 0.00013126
Iteration 111/1000 | Loss: 0.00013126
Iteration 112/1000 | Loss: 0.00013126
Iteration 113/1000 | Loss: 0.00013125
Iteration 114/1000 | Loss: 0.00013125
Iteration 115/1000 | Loss: 0.00013123
Iteration 116/1000 | Loss: 0.00013122
Iteration 117/1000 | Loss: 0.00013122
Iteration 118/1000 | Loss: 0.00013121
Iteration 119/1000 | Loss: 0.00013120
Iteration 120/1000 | Loss: 0.00013118
Iteration 121/1000 | Loss: 0.00013118
Iteration 122/1000 | Loss: 0.00013118
Iteration 123/1000 | Loss: 0.00013117
Iteration 124/1000 | Loss: 0.00013117
Iteration 125/1000 | Loss: 0.00013117
Iteration 126/1000 | Loss: 0.00013117
Iteration 127/1000 | Loss: 0.00013117
Iteration 128/1000 | Loss: 0.00013116
Iteration 129/1000 | Loss: 0.00013116
Iteration 130/1000 | Loss: 0.00013116
Iteration 131/1000 | Loss: 0.00016670
Iteration 132/1000 | Loss: 0.00013120
Iteration 133/1000 | Loss: 0.00013112
Iteration 134/1000 | Loss: 0.00013109
Iteration 135/1000 | Loss: 0.00013108
Iteration 136/1000 | Loss: 0.00013108
Iteration 137/1000 | Loss: 0.00013107
Iteration 138/1000 | Loss: 0.00013107
Iteration 139/1000 | Loss: 0.00013107
Iteration 140/1000 | Loss: 0.00013106
Iteration 141/1000 | Loss: 0.00013106
Iteration 142/1000 | Loss: 0.00013106
Iteration 143/1000 | Loss: 0.00013106
Iteration 144/1000 | Loss: 0.00013106
Iteration 145/1000 | Loss: 0.00013105
Iteration 146/1000 | Loss: 0.00013105
Iteration 147/1000 | Loss: 0.00013104
Iteration 148/1000 | Loss: 0.00013104
Iteration 149/1000 | Loss: 0.00013104
Iteration 150/1000 | Loss: 0.00013103
Iteration 151/1000 | Loss: 0.00013103
Iteration 152/1000 | Loss: 0.00013103
Iteration 153/1000 | Loss: 0.00013103
Iteration 154/1000 | Loss: 0.00013103
Iteration 155/1000 | Loss: 0.00013103
Iteration 156/1000 | Loss: 0.00013103
Iteration 157/1000 | Loss: 0.00013102
Iteration 158/1000 | Loss: 0.00013102
Iteration 159/1000 | Loss: 0.00013102
Iteration 160/1000 | Loss: 0.00013102
Iteration 161/1000 | Loss: 0.00013102
Iteration 162/1000 | Loss: 0.00013102
Iteration 163/1000 | Loss: 0.00013101
Iteration 164/1000 | Loss: 0.00013101
Iteration 165/1000 | Loss: 0.00013101
Iteration 166/1000 | Loss: 0.00013101
Iteration 167/1000 | Loss: 0.00013100
Iteration 168/1000 | Loss: 0.00013100
Iteration 169/1000 | Loss: 0.00013100
Iteration 170/1000 | Loss: 0.00013100
Iteration 171/1000 | Loss: 0.00013099
Iteration 172/1000 | Loss: 0.00013099
Iteration 173/1000 | Loss: 0.00013099
Iteration 174/1000 | Loss: 0.00013099
Iteration 175/1000 | Loss: 0.00013099
Iteration 176/1000 | Loss: 0.00013098
Iteration 177/1000 | Loss: 0.00013098
Iteration 178/1000 | Loss: 0.00013098
Iteration 179/1000 | Loss: 0.00013098
Iteration 180/1000 | Loss: 0.00013098
Iteration 181/1000 | Loss: 0.00013098
Iteration 182/1000 | Loss: 0.00013098
Iteration 183/1000 | Loss: 0.00013097
Iteration 184/1000 | Loss: 0.00013097
Iteration 185/1000 | Loss: 0.00013097
Iteration 186/1000 | Loss: 0.00013097
Iteration 187/1000 | Loss: 0.00013097
Iteration 188/1000 | Loss: 0.00013097
Iteration 189/1000 | Loss: 0.00013096
Iteration 190/1000 | Loss: 0.00013096
Iteration 191/1000 | Loss: 0.00013096
Iteration 192/1000 | Loss: 0.00013095
Iteration 193/1000 | Loss: 0.00013095
Iteration 194/1000 | Loss: 0.00013095
Iteration 195/1000 | Loss: 0.00013095
Iteration 196/1000 | Loss: 0.00013094
Iteration 197/1000 | Loss: 0.00013094
Iteration 198/1000 | Loss: 0.00013094
Iteration 199/1000 | Loss: 0.00013094
Iteration 200/1000 | Loss: 0.00013094
Iteration 201/1000 | Loss: 0.00013094
Iteration 202/1000 | Loss: 0.00013094
Iteration 203/1000 | Loss: 0.00013094
Iteration 204/1000 | Loss: 0.00013094
Iteration 205/1000 | Loss: 0.00013094
Iteration 206/1000 | Loss: 0.00013094
Iteration 207/1000 | Loss: 0.00013094
Iteration 208/1000 | Loss: 0.00013094
Iteration 209/1000 | Loss: 0.00013094
Iteration 210/1000 | Loss: 0.00013094
Iteration 211/1000 | Loss: 0.00013094
Iteration 212/1000 | Loss: 0.00013094
Iteration 213/1000 | Loss: 0.00013094
Iteration 214/1000 | Loss: 0.00013094
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 214. Stopping optimization.
Last 5 losses: [0.00013093666348140687, 0.00013093666348140687, 0.00013093666348140687, 0.00013093666348140687, 0.00013093666348140687]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00013093666348140687

Optimization complete. Final v2v error: 6.774771690368652 mm

Highest mean error: 11.144670486450195 mm for frame 51

Lowest mean error: 4.1774139404296875 mm for frame 116

Saving results

Total time: 185.34654903411865
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alison_posed_001/1027/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alison_posed_001/1027.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alison_posed_001/1027
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01002046
Iteration 2/25 | Loss: 0.00317715
Iteration 3/25 | Loss: 0.00203413
Iteration 4/25 | Loss: 0.00189393
Iteration 5/25 | Loss: 0.00184218
Iteration 6/25 | Loss: 0.00177932
Iteration 7/25 | Loss: 0.00173465
Iteration 8/25 | Loss: 0.00179197
Iteration 9/25 | Loss: 0.00175097
Iteration 10/25 | Loss: 0.00165725
Iteration 11/25 | Loss: 0.00164250
Iteration 12/25 | Loss: 0.00161738
Iteration 13/25 | Loss: 0.00159678
Iteration 14/25 | Loss: 0.00159742
Iteration 15/25 | Loss: 0.00159593
Iteration 16/25 | Loss: 0.00158823
Iteration 17/25 | Loss: 0.00160614
Iteration 18/25 | Loss: 0.00160743
Iteration 19/25 | Loss: 0.00158289
Iteration 20/25 | Loss: 0.00157147
Iteration 21/25 | Loss: 0.00156540
Iteration 22/25 | Loss: 0.00156106
Iteration 23/25 | Loss: 0.00155954
Iteration 24/25 | Loss: 0.00155813
Iteration 25/25 | Loss: 0.00155423

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46568131
Iteration 2/25 | Loss: 0.00537219
Iteration 3/25 | Loss: 0.00326101
Iteration 4/25 | Loss: 0.00325836
Iteration 5/25 | Loss: 0.00325498
Iteration 6/25 | Loss: 0.00325474
Iteration 7/25 | Loss: 0.00325327
Iteration 8/25 | Loss: 0.00325327
Iteration 9/25 | Loss: 0.00325327
Iteration 10/25 | Loss: 0.00325327
Iteration 11/25 | Loss: 0.00325327
Iteration 12/25 | Loss: 0.00325327
Iteration 13/25 | Loss: 0.00325327
Iteration 14/25 | Loss: 0.00325327
Iteration 15/25 | Loss: 0.00325327
Iteration 16/25 | Loss: 0.00325327
Iteration 17/25 | Loss: 0.00325327
Iteration 18/25 | Loss: 0.00325327
Iteration 19/25 | Loss: 0.00325327
Iteration 20/25 | Loss: 0.00325327
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0032532699406147003, 0.0032532699406147003, 0.0032532699406147003, 0.0032532699406147003, 0.0032532699406147003]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0032532699406147003

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00325327
Iteration 2/1000 | Loss: 0.00270776
Iteration 3/1000 | Loss: 0.00128710
Iteration 4/1000 | Loss: 0.00238965
Iteration 5/1000 | Loss: 0.00203849
Iteration 6/1000 | Loss: 0.00227306
Iteration 7/1000 | Loss: 0.00120030
Iteration 8/1000 | Loss: 0.00232663
Iteration 9/1000 | Loss: 0.00218751
Iteration 10/1000 | Loss: 0.00270639
Iteration 11/1000 | Loss: 0.00181086
Iteration 12/1000 | Loss: 0.00052556
Iteration 13/1000 | Loss: 0.00077869
Iteration 14/1000 | Loss: 0.00179087
Iteration 15/1000 | Loss: 0.00220854
Iteration 16/1000 | Loss: 0.00051679
Iteration 17/1000 | Loss: 0.00132613
Iteration 18/1000 | Loss: 0.00083619
Iteration 19/1000 | Loss: 0.00104467
Iteration 20/1000 | Loss: 0.00110586
Iteration 21/1000 | Loss: 0.00064020
Iteration 22/1000 | Loss: 0.00231104
Iteration 23/1000 | Loss: 0.00040396
Iteration 24/1000 | Loss: 0.00057847
Iteration 25/1000 | Loss: 0.00047193
Iteration 26/1000 | Loss: 0.00028242
Iteration 27/1000 | Loss: 0.00014432
Iteration 28/1000 | Loss: 0.00107158
Iteration 29/1000 | Loss: 0.00105555
Iteration 30/1000 | Loss: 0.00092357
Iteration 31/1000 | Loss: 0.00082692
Iteration 32/1000 | Loss: 0.00101720
Iteration 33/1000 | Loss: 0.00049856
Iteration 34/1000 | Loss: 0.00070684
Iteration 35/1000 | Loss: 0.00051441
Iteration 36/1000 | Loss: 0.00049074
Iteration 37/1000 | Loss: 0.00081265
Iteration 38/1000 | Loss: 0.00156958
Iteration 39/1000 | Loss: 0.00056694
Iteration 40/1000 | Loss: 0.00296137
Iteration 41/1000 | Loss: 0.00259613
Iteration 42/1000 | Loss: 0.00127457
Iteration 43/1000 | Loss: 0.00156533
Iteration 44/1000 | Loss: 0.00032990
Iteration 45/1000 | Loss: 0.00015276
Iteration 46/1000 | Loss: 0.00013307
Iteration 47/1000 | Loss: 0.00020326
Iteration 48/1000 | Loss: 0.00227540
Iteration 49/1000 | Loss: 0.00155998
Iteration 50/1000 | Loss: 0.00105299
Iteration 51/1000 | Loss: 0.00141083
Iteration 52/1000 | Loss: 0.00103019
Iteration 53/1000 | Loss: 0.00022528
Iteration 54/1000 | Loss: 0.00043830
Iteration 55/1000 | Loss: 0.00095845
Iteration 56/1000 | Loss: 0.00106038
Iteration 57/1000 | Loss: 0.00101557
Iteration 58/1000 | Loss: 0.00053772
Iteration 59/1000 | Loss: 0.00029276
Iteration 60/1000 | Loss: 0.00059609
Iteration 61/1000 | Loss: 0.00173238
Iteration 62/1000 | Loss: 0.00154302
Iteration 63/1000 | Loss: 0.00055964
Iteration 64/1000 | Loss: 0.00012026
Iteration 65/1000 | Loss: 0.00027722
Iteration 66/1000 | Loss: 0.00035840
Iteration 67/1000 | Loss: 0.00022884
Iteration 68/1000 | Loss: 0.00012021
Iteration 69/1000 | Loss: 0.00010951
Iteration 70/1000 | Loss: 0.00080951
Iteration 71/1000 | Loss: 0.00049127
Iteration 72/1000 | Loss: 0.00016183
Iteration 73/1000 | Loss: 0.00009838
Iteration 74/1000 | Loss: 0.00026691
Iteration 75/1000 | Loss: 0.00054093
Iteration 76/1000 | Loss: 0.00010354
Iteration 77/1000 | Loss: 0.00024338
Iteration 78/1000 | Loss: 0.00030848
Iteration 79/1000 | Loss: 0.00016782
Iteration 80/1000 | Loss: 0.00009029
Iteration 81/1000 | Loss: 0.00027229
Iteration 82/1000 | Loss: 0.00063967
Iteration 83/1000 | Loss: 0.00009175
Iteration 84/1000 | Loss: 0.00007977
Iteration 85/1000 | Loss: 0.00019024
Iteration 86/1000 | Loss: 0.00010048
Iteration 87/1000 | Loss: 0.00010836
Iteration 88/1000 | Loss: 0.00016899
Iteration 89/1000 | Loss: 0.00020723
Iteration 90/1000 | Loss: 0.00007107
Iteration 91/1000 | Loss: 0.00026446
Iteration 92/1000 | Loss: 0.00010598
Iteration 93/1000 | Loss: 0.00012260
Iteration 94/1000 | Loss: 0.00013447
Iteration 95/1000 | Loss: 0.00007858
Iteration 96/1000 | Loss: 0.00047123
Iteration 97/1000 | Loss: 0.00005841
Iteration 98/1000 | Loss: 0.00005457
Iteration 99/1000 | Loss: 0.00026770
Iteration 100/1000 | Loss: 0.00005351
Iteration 101/1000 | Loss: 0.00012830
Iteration 102/1000 | Loss: 0.00023123
Iteration 103/1000 | Loss: 0.00046565
Iteration 104/1000 | Loss: 0.00050128
Iteration 105/1000 | Loss: 0.00006575
Iteration 106/1000 | Loss: 0.00007572
Iteration 107/1000 | Loss: 0.00006180
Iteration 108/1000 | Loss: 0.00007832
Iteration 109/1000 | Loss: 0.00005688
Iteration 110/1000 | Loss: 0.00007811
Iteration 111/1000 | Loss: 0.00005028
Iteration 112/1000 | Loss: 0.00007762
Iteration 113/1000 | Loss: 0.00036454
Iteration 114/1000 | Loss: 0.00007831
Iteration 115/1000 | Loss: 0.00005089
Iteration 116/1000 | Loss: 0.00004445
Iteration 117/1000 | Loss: 0.00004058
Iteration 118/1000 | Loss: 0.00010849
Iteration 119/1000 | Loss: 0.00003938
Iteration 120/1000 | Loss: 0.00003875
Iteration 121/1000 | Loss: 0.00003809
Iteration 122/1000 | Loss: 0.00003766
Iteration 123/1000 | Loss: 0.00014096
Iteration 124/1000 | Loss: 0.00004204
Iteration 125/1000 | Loss: 0.00006934
Iteration 126/1000 | Loss: 0.00006036
Iteration 127/1000 | Loss: 0.00004888
Iteration 128/1000 | Loss: 0.00005129
Iteration 129/1000 | Loss: 0.00003669
Iteration 130/1000 | Loss: 0.00003648
Iteration 131/1000 | Loss: 0.00003646
Iteration 132/1000 | Loss: 0.00003645
Iteration 133/1000 | Loss: 0.00012620
Iteration 134/1000 | Loss: 0.00008528
Iteration 135/1000 | Loss: 0.00003999
Iteration 136/1000 | Loss: 0.00009011
Iteration 137/1000 | Loss: 0.00003925
Iteration 138/1000 | Loss: 0.00003653
Iteration 139/1000 | Loss: 0.00010220
Iteration 140/1000 | Loss: 0.00003744
Iteration 141/1000 | Loss: 0.00015141
Iteration 142/1000 | Loss: 0.00095136
Iteration 143/1000 | Loss: 0.00008301
Iteration 144/1000 | Loss: 0.00005838
Iteration 145/1000 | Loss: 0.00003603
Iteration 146/1000 | Loss: 0.00009619
Iteration 147/1000 | Loss: 0.00003473
Iteration 148/1000 | Loss: 0.00003453
Iteration 149/1000 | Loss: 0.00005860
Iteration 150/1000 | Loss: 0.00003449
Iteration 151/1000 | Loss: 0.00004170
Iteration 152/1000 | Loss: 0.00003440
Iteration 153/1000 | Loss: 0.00003437
Iteration 154/1000 | Loss: 0.00003437
Iteration 155/1000 | Loss: 0.00003436
Iteration 156/1000 | Loss: 0.00003435
Iteration 157/1000 | Loss: 0.00003435
Iteration 158/1000 | Loss: 0.00003434
Iteration 159/1000 | Loss: 0.00003434
Iteration 160/1000 | Loss: 0.00003434
Iteration 161/1000 | Loss: 0.00003434
Iteration 162/1000 | Loss: 0.00003433
Iteration 163/1000 | Loss: 0.00003433
Iteration 164/1000 | Loss: 0.00003433
Iteration 165/1000 | Loss: 0.00003432
Iteration 166/1000 | Loss: 0.00003431
Iteration 167/1000 | Loss: 0.00003431
Iteration 168/1000 | Loss: 0.00003430
Iteration 169/1000 | Loss: 0.00003429
Iteration 170/1000 | Loss: 0.00003429
Iteration 171/1000 | Loss: 0.00003429
Iteration 172/1000 | Loss: 0.00003429
Iteration 173/1000 | Loss: 0.00003429
Iteration 174/1000 | Loss: 0.00003429
Iteration 175/1000 | Loss: 0.00003428
Iteration 176/1000 | Loss: 0.00003428
Iteration 177/1000 | Loss: 0.00003428
Iteration 178/1000 | Loss: 0.00003427
Iteration 179/1000 | Loss: 0.00003427
Iteration 180/1000 | Loss: 0.00003427
Iteration 181/1000 | Loss: 0.00003426
Iteration 182/1000 | Loss: 0.00003424
Iteration 183/1000 | Loss: 0.00003424
Iteration 184/1000 | Loss: 0.00003424
Iteration 185/1000 | Loss: 0.00003424
Iteration 186/1000 | Loss: 0.00003424
Iteration 187/1000 | Loss: 0.00003422
Iteration 188/1000 | Loss: 0.00003422
Iteration 189/1000 | Loss: 0.00003421
Iteration 190/1000 | Loss: 0.00003421
Iteration 191/1000 | Loss: 0.00004741
Iteration 192/1000 | Loss: 0.00003463
Iteration 193/1000 | Loss: 0.00003416
Iteration 194/1000 | Loss: 0.00003416
Iteration 195/1000 | Loss: 0.00003416
Iteration 196/1000 | Loss: 0.00003416
Iteration 197/1000 | Loss: 0.00003416
Iteration 198/1000 | Loss: 0.00003416
Iteration 199/1000 | Loss: 0.00003416
Iteration 200/1000 | Loss: 0.00003416
Iteration 201/1000 | Loss: 0.00003416
Iteration 202/1000 | Loss: 0.00003416
Iteration 203/1000 | Loss: 0.00003415
Iteration 204/1000 | Loss: 0.00003414
Iteration 205/1000 | Loss: 0.00003414
Iteration 206/1000 | Loss: 0.00003414
Iteration 207/1000 | Loss: 0.00003412
Iteration 208/1000 | Loss: 0.00003412
Iteration 209/1000 | Loss: 0.00003412
Iteration 210/1000 | Loss: 0.00003412
Iteration 211/1000 | Loss: 0.00003412
Iteration 212/1000 | Loss: 0.00003412
Iteration 213/1000 | Loss: 0.00003412
Iteration 214/1000 | Loss: 0.00003412
Iteration 215/1000 | Loss: 0.00003412
Iteration 216/1000 | Loss: 0.00003412
Iteration 217/1000 | Loss: 0.00003412
Iteration 218/1000 | Loss: 0.00003411
Iteration 219/1000 | Loss: 0.00003411
Iteration 220/1000 | Loss: 0.00003411
Iteration 221/1000 | Loss: 0.00003410
Iteration 222/1000 | Loss: 0.00003410
Iteration 223/1000 | Loss: 0.00003409
Iteration 224/1000 | Loss: 0.00003661
Iteration 225/1000 | Loss: 0.00009981
Iteration 226/1000 | Loss: 0.00005410
Iteration 227/1000 | Loss: 0.00009236
Iteration 228/1000 | Loss: 0.00003905
Iteration 229/1000 | Loss: 0.00003534
Iteration 230/1000 | Loss: 0.00003918
Iteration 231/1000 | Loss: 0.00003405
Iteration 232/1000 | Loss: 0.00007095
Iteration 233/1000 | Loss: 0.00003953
Iteration 234/1000 | Loss: 0.00021529
Iteration 235/1000 | Loss: 0.00013925
Iteration 236/1000 | Loss: 0.00005643
Iteration 237/1000 | Loss: 0.00005115
Iteration 238/1000 | Loss: 0.00003596
Iteration 239/1000 | Loss: 0.00003449
Iteration 240/1000 | Loss: 0.00006466
Iteration 241/1000 | Loss: 0.00004431
Iteration 242/1000 | Loss: 0.00003520
Iteration 243/1000 | Loss: 0.00003761
Iteration 244/1000 | Loss: 0.00003191
Iteration 245/1000 | Loss: 0.00004104
Iteration 246/1000 | Loss: 0.00003148
Iteration 247/1000 | Loss: 0.00003141
Iteration 248/1000 | Loss: 0.00003141
Iteration 249/1000 | Loss: 0.00003140
Iteration 250/1000 | Loss: 0.00009522
Iteration 251/1000 | Loss: 0.00021075
Iteration 252/1000 | Loss: 0.00074659
Iteration 253/1000 | Loss: 0.00042156
Iteration 254/1000 | Loss: 0.00058069
Iteration 255/1000 | Loss: 0.00045742
Iteration 256/1000 | Loss: 0.00003898
Iteration 257/1000 | Loss: 0.00003833
Iteration 258/1000 | Loss: 0.00003459
Iteration 259/1000 | Loss: 0.00003232
Iteration 260/1000 | Loss: 0.00003151
Iteration 261/1000 | Loss: 0.00003113
Iteration 262/1000 | Loss: 0.00003113
Iteration 263/1000 | Loss: 0.00003113
Iteration 264/1000 | Loss: 0.00003113
Iteration 265/1000 | Loss: 0.00003112
Iteration 266/1000 | Loss: 0.00003112
Iteration 267/1000 | Loss: 0.00003112
Iteration 268/1000 | Loss: 0.00003112
Iteration 269/1000 | Loss: 0.00003112
Iteration 270/1000 | Loss: 0.00003112
Iteration 271/1000 | Loss: 0.00003112
Iteration 272/1000 | Loss: 0.00003112
Iteration 273/1000 | Loss: 0.00003129
Iteration 274/1000 | Loss: 0.00003107
Iteration 275/1000 | Loss: 0.00003106
Iteration 276/1000 | Loss: 0.00003106
Iteration 277/1000 | Loss: 0.00007961
Iteration 278/1000 | Loss: 0.00003119
Iteration 279/1000 | Loss: 0.00004251
Iteration 280/1000 | Loss: 0.00003142
Iteration 281/1000 | Loss: 0.00003388
Iteration 282/1000 | Loss: 0.00006566
Iteration 283/1000 | Loss: 0.00003156
Iteration 284/1000 | Loss: 0.00006201
Iteration 285/1000 | Loss: 0.00003104
Iteration 286/1000 | Loss: 0.00003104
Iteration 287/1000 | Loss: 0.00003104
Iteration 288/1000 | Loss: 0.00003104
Iteration 289/1000 | Loss: 0.00003103
Iteration 290/1000 | Loss: 0.00003103
Iteration 291/1000 | Loss: 0.00003103
Iteration 292/1000 | Loss: 0.00003103
Iteration 293/1000 | Loss: 0.00003103
Iteration 294/1000 | Loss: 0.00003103
Iteration 295/1000 | Loss: 0.00003102
Iteration 296/1000 | Loss: 0.00003101
Iteration 297/1000 | Loss: 0.00004589
Iteration 298/1000 | Loss: 0.00003105
Iteration 299/1000 | Loss: 0.00003103
Iteration 300/1000 | Loss: 0.00003103
Iteration 301/1000 | Loss: 0.00003103
Iteration 302/1000 | Loss: 0.00003102
Iteration 303/1000 | Loss: 0.00003102
Iteration 304/1000 | Loss: 0.00003102
Iteration 305/1000 | Loss: 0.00003213
Iteration 306/1000 | Loss: 0.00003119
Iteration 307/1000 | Loss: 0.00003099
Iteration 308/1000 | Loss: 0.00003099
Iteration 309/1000 | Loss: 0.00003099
Iteration 310/1000 | Loss: 0.00003099
Iteration 311/1000 | Loss: 0.00003099
Iteration 312/1000 | Loss: 0.00003099
Iteration 313/1000 | Loss: 0.00003099
Iteration 314/1000 | Loss: 0.00003099
Iteration 315/1000 | Loss: 0.00003099
Iteration 316/1000 | Loss: 0.00003099
Iteration 317/1000 | Loss: 0.00003099
Iteration 318/1000 | Loss: 0.00003099
Iteration 319/1000 | Loss: 0.00003099
Iteration 320/1000 | Loss: 0.00003099
Iteration 321/1000 | Loss: 0.00003099
Iteration 322/1000 | Loss: 0.00003099
Iteration 323/1000 | Loss: 0.00003099
Iteration 324/1000 | Loss: 0.00003099
Iteration 325/1000 | Loss: 0.00003099
Iteration 326/1000 | Loss: 0.00003099
Iteration 327/1000 | Loss: 0.00003099
Iteration 328/1000 | Loss: 0.00003099
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 328. Stopping optimization.
Last 5 losses: [3.099034438491799e-05, 3.099034438491799e-05, 3.099034438491799e-05, 3.099034438491799e-05, 3.099034438491799e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.099034438491799e-05

Optimization complete. Final v2v error: 3.8474748134613037 mm

Highest mean error: 10.954771041870117 mm for frame 144

Lowest mean error: 2.9794726371765137 mm for frame 20

Saving results

Total time: 378.590092420578
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alison_posed_001/1004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alison_posed_001/1004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alison_posed_001/1004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00801665
Iteration 2/25 | Loss: 0.00164869
Iteration 3/25 | Loss: 0.00147713
Iteration 4/25 | Loss: 0.00146125
Iteration 5/25 | Loss: 0.00145676
Iteration 6/25 | Loss: 0.00145643
Iteration 7/25 | Loss: 0.00145643
Iteration 8/25 | Loss: 0.00145643
Iteration 9/25 | Loss: 0.00145643
Iteration 10/25 | Loss: 0.00145643
Iteration 11/25 | Loss: 0.00145643
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001456434722058475, 0.001456434722058475, 0.001456434722058475, 0.001456434722058475, 0.001456434722058475]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001456434722058475

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 8.10488510
Iteration 2/25 | Loss: 0.00119838
Iteration 3/25 | Loss: 0.00119837
Iteration 4/25 | Loss: 0.00119837
Iteration 5/25 | Loss: 0.00119837
Iteration 6/25 | Loss: 0.00119837
Iteration 7/25 | Loss: 0.00119837
Iteration 8/25 | Loss: 0.00119837
Iteration 9/25 | Loss: 0.00119837
Iteration 10/25 | Loss: 0.00119837
Iteration 11/25 | Loss: 0.00119837
Iteration 12/25 | Loss: 0.00119837
Iteration 13/25 | Loss: 0.00119837
Iteration 14/25 | Loss: 0.00119837
Iteration 15/25 | Loss: 0.00119837
Iteration 16/25 | Loss: 0.00119837
Iteration 17/25 | Loss: 0.00119837
Iteration 18/25 | Loss: 0.00119837
Iteration 19/25 | Loss: 0.00119837
Iteration 20/25 | Loss: 0.00119837
Iteration 21/25 | Loss: 0.00119837
Iteration 22/25 | Loss: 0.00119837
Iteration 23/25 | Loss: 0.00119837
Iteration 24/25 | Loss: 0.00119837
Iteration 25/25 | Loss: 0.00119837

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00119837
Iteration 2/1000 | Loss: 0.00005454
Iteration 3/1000 | Loss: 0.00003714
Iteration 4/1000 | Loss: 0.00003188
Iteration 5/1000 | Loss: 0.00002977
Iteration 6/1000 | Loss: 0.00002866
Iteration 7/1000 | Loss: 0.00002787
Iteration 8/1000 | Loss: 0.00002747
Iteration 9/1000 | Loss: 0.00002716
Iteration 10/1000 | Loss: 0.00002696
Iteration 11/1000 | Loss: 0.00002681
Iteration 12/1000 | Loss: 0.00002680
Iteration 13/1000 | Loss: 0.00002678
Iteration 14/1000 | Loss: 0.00002676
Iteration 15/1000 | Loss: 0.00002676
Iteration 16/1000 | Loss: 0.00002676
Iteration 17/1000 | Loss: 0.00002676
Iteration 18/1000 | Loss: 0.00002676
Iteration 19/1000 | Loss: 0.00002676
Iteration 20/1000 | Loss: 0.00002676
Iteration 21/1000 | Loss: 0.00002676
Iteration 22/1000 | Loss: 0.00002675
Iteration 23/1000 | Loss: 0.00002673
Iteration 24/1000 | Loss: 0.00002672
Iteration 25/1000 | Loss: 0.00002672
Iteration 26/1000 | Loss: 0.00002671
Iteration 27/1000 | Loss: 0.00002663
Iteration 28/1000 | Loss: 0.00002661
Iteration 29/1000 | Loss: 0.00002660
Iteration 30/1000 | Loss: 0.00002659
Iteration 31/1000 | Loss: 0.00002659
Iteration 32/1000 | Loss: 0.00002658
Iteration 33/1000 | Loss: 0.00002658
Iteration 34/1000 | Loss: 0.00002657
Iteration 35/1000 | Loss: 0.00002657
Iteration 36/1000 | Loss: 0.00002656
Iteration 37/1000 | Loss: 0.00002655
Iteration 38/1000 | Loss: 0.00002655
Iteration 39/1000 | Loss: 0.00002654
Iteration 40/1000 | Loss: 0.00002654
Iteration 41/1000 | Loss: 0.00002654
Iteration 42/1000 | Loss: 0.00002654
Iteration 43/1000 | Loss: 0.00002653
Iteration 44/1000 | Loss: 0.00002652
Iteration 45/1000 | Loss: 0.00002652
Iteration 46/1000 | Loss: 0.00002652
Iteration 47/1000 | Loss: 0.00002652
Iteration 48/1000 | Loss: 0.00002652
Iteration 49/1000 | Loss: 0.00002652
Iteration 50/1000 | Loss: 0.00002652
Iteration 51/1000 | Loss: 0.00002651
Iteration 52/1000 | Loss: 0.00002651
Iteration 53/1000 | Loss: 0.00002650
Iteration 54/1000 | Loss: 0.00002650
Iteration 55/1000 | Loss: 0.00002650
Iteration 56/1000 | Loss: 0.00002649
Iteration 57/1000 | Loss: 0.00002649
Iteration 58/1000 | Loss: 0.00002649
Iteration 59/1000 | Loss: 0.00002649
Iteration 60/1000 | Loss: 0.00002649
Iteration 61/1000 | Loss: 0.00002649
Iteration 62/1000 | Loss: 0.00002649
Iteration 63/1000 | Loss: 0.00002648
Iteration 64/1000 | Loss: 0.00002648
Iteration 65/1000 | Loss: 0.00002648
Iteration 66/1000 | Loss: 0.00002648
Iteration 67/1000 | Loss: 0.00002648
Iteration 68/1000 | Loss: 0.00002648
Iteration 69/1000 | Loss: 0.00002648
Iteration 70/1000 | Loss: 0.00002648
Iteration 71/1000 | Loss: 0.00002648
Iteration 72/1000 | Loss: 0.00002647
Iteration 73/1000 | Loss: 0.00002647
Iteration 74/1000 | Loss: 0.00002647
Iteration 75/1000 | Loss: 0.00002647
Iteration 76/1000 | Loss: 0.00002647
Iteration 77/1000 | Loss: 0.00002647
Iteration 78/1000 | Loss: 0.00002647
Iteration 79/1000 | Loss: 0.00002647
Iteration 80/1000 | Loss: 0.00002646
Iteration 81/1000 | Loss: 0.00002646
Iteration 82/1000 | Loss: 0.00002646
Iteration 83/1000 | Loss: 0.00002646
Iteration 84/1000 | Loss: 0.00002646
Iteration 85/1000 | Loss: 0.00002646
Iteration 86/1000 | Loss: 0.00002646
Iteration 87/1000 | Loss: 0.00002646
Iteration 88/1000 | Loss: 0.00002646
Iteration 89/1000 | Loss: 0.00002646
Iteration 90/1000 | Loss: 0.00002646
Iteration 91/1000 | Loss: 0.00002646
Iteration 92/1000 | Loss: 0.00002646
Iteration 93/1000 | Loss: 0.00002646
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 93. Stopping optimization.
Last 5 losses: [2.6460935259819962e-05, 2.6460935259819962e-05, 2.6460935259819962e-05, 2.6460935259819962e-05, 2.6460935259819962e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.6460935259819962e-05

Optimization complete. Final v2v error: 4.287978172302246 mm

Highest mean error: 4.807122707366943 mm for frame 114

Lowest mean error: 3.9017438888549805 mm for frame 173

Saving results

Total time: 30.884718418121338
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alison_posed_001/1086/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alison_posed_001/1086.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alison_posed_001/1086
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00626658
Iteration 2/25 | Loss: 0.00174693
Iteration 3/25 | Loss: 0.00137587
Iteration 4/25 | Loss: 0.00133522
Iteration 5/25 | Loss: 0.00132916
Iteration 6/25 | Loss: 0.00132784
Iteration 7/25 | Loss: 0.00132767
Iteration 8/25 | Loss: 0.00132767
Iteration 9/25 | Loss: 0.00132767
Iteration 10/25 | Loss: 0.00132767
Iteration 11/25 | Loss: 0.00132767
Iteration 12/25 | Loss: 0.00132767
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0013276736717671156, 0.0013276736717671156, 0.0013276736717671156, 0.0013276736717671156, 0.0013276736717671156]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013276736717671156

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.25593019
Iteration 2/25 | Loss: 0.00079666
Iteration 3/25 | Loss: 0.00079663
Iteration 4/25 | Loss: 0.00079663
Iteration 5/25 | Loss: 0.00079663
Iteration 6/25 | Loss: 0.00079662
Iteration 7/25 | Loss: 0.00079662
Iteration 8/25 | Loss: 0.00079662
Iteration 9/25 | Loss: 0.00079662
Iteration 10/25 | Loss: 0.00079662
Iteration 11/25 | Loss: 0.00079662
Iteration 12/25 | Loss: 0.00079662
Iteration 13/25 | Loss: 0.00079662
Iteration 14/25 | Loss: 0.00079662
Iteration 15/25 | Loss: 0.00079662
Iteration 16/25 | Loss: 0.00079662
Iteration 17/25 | Loss: 0.00079662
Iteration 18/25 | Loss: 0.00079662
Iteration 19/25 | Loss: 0.00079662
Iteration 20/25 | Loss: 0.00079662
Iteration 21/25 | Loss: 0.00079662
Iteration 22/25 | Loss: 0.00079662
Iteration 23/25 | Loss: 0.00079662
Iteration 24/25 | Loss: 0.00079662
Iteration 25/25 | Loss: 0.00079662

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00079662
Iteration 2/1000 | Loss: 0.00004511
Iteration 3/1000 | Loss: 0.00003020
Iteration 4/1000 | Loss: 0.00002465
Iteration 5/1000 | Loss: 0.00002295
Iteration 6/1000 | Loss: 0.00002198
Iteration 7/1000 | Loss: 0.00002132
Iteration 8/1000 | Loss: 0.00002079
Iteration 9/1000 | Loss: 0.00002043
Iteration 10/1000 | Loss: 0.00002016
Iteration 11/1000 | Loss: 0.00001994
Iteration 12/1000 | Loss: 0.00001989
Iteration 13/1000 | Loss: 0.00001984
Iteration 14/1000 | Loss: 0.00001983
Iteration 15/1000 | Loss: 0.00001974
Iteration 16/1000 | Loss: 0.00001968
Iteration 17/1000 | Loss: 0.00001963
Iteration 18/1000 | Loss: 0.00001956
Iteration 19/1000 | Loss: 0.00001956
Iteration 20/1000 | Loss: 0.00001949
Iteration 21/1000 | Loss: 0.00001949
Iteration 22/1000 | Loss: 0.00001949
Iteration 23/1000 | Loss: 0.00001949
Iteration 24/1000 | Loss: 0.00001949
Iteration 25/1000 | Loss: 0.00001949
Iteration 26/1000 | Loss: 0.00001949
Iteration 27/1000 | Loss: 0.00001948
Iteration 28/1000 | Loss: 0.00001948
Iteration 29/1000 | Loss: 0.00001945
Iteration 30/1000 | Loss: 0.00001944
Iteration 31/1000 | Loss: 0.00001944
Iteration 32/1000 | Loss: 0.00001944
Iteration 33/1000 | Loss: 0.00001943
Iteration 34/1000 | Loss: 0.00001931
Iteration 35/1000 | Loss: 0.00001929
Iteration 36/1000 | Loss: 0.00001926
Iteration 37/1000 | Loss: 0.00001926
Iteration 38/1000 | Loss: 0.00001925
Iteration 39/1000 | Loss: 0.00001925
Iteration 40/1000 | Loss: 0.00001925
Iteration 41/1000 | Loss: 0.00001925
Iteration 42/1000 | Loss: 0.00001924
Iteration 43/1000 | Loss: 0.00001924
Iteration 44/1000 | Loss: 0.00001923
Iteration 45/1000 | Loss: 0.00001923
Iteration 46/1000 | Loss: 0.00001922
Iteration 47/1000 | Loss: 0.00001922
Iteration 48/1000 | Loss: 0.00001922
Iteration 49/1000 | Loss: 0.00001922
Iteration 50/1000 | Loss: 0.00001922
Iteration 51/1000 | Loss: 0.00001922
Iteration 52/1000 | Loss: 0.00001921
Iteration 53/1000 | Loss: 0.00001921
Iteration 54/1000 | Loss: 0.00001921
Iteration 55/1000 | Loss: 0.00001921
Iteration 56/1000 | Loss: 0.00001921
Iteration 57/1000 | Loss: 0.00001921
Iteration 58/1000 | Loss: 0.00001921
Iteration 59/1000 | Loss: 0.00001920
Iteration 60/1000 | Loss: 0.00001920
Iteration 61/1000 | Loss: 0.00001920
Iteration 62/1000 | Loss: 0.00001919
Iteration 63/1000 | Loss: 0.00001919
Iteration 64/1000 | Loss: 0.00001919
Iteration 65/1000 | Loss: 0.00001918
Iteration 66/1000 | Loss: 0.00001918
Iteration 67/1000 | Loss: 0.00001918
Iteration 68/1000 | Loss: 0.00001917
Iteration 69/1000 | Loss: 0.00001917
Iteration 70/1000 | Loss: 0.00001916
Iteration 71/1000 | Loss: 0.00001916
Iteration 72/1000 | Loss: 0.00001915
Iteration 73/1000 | Loss: 0.00001915
Iteration 74/1000 | Loss: 0.00001915
Iteration 75/1000 | Loss: 0.00001915
Iteration 76/1000 | Loss: 0.00001915
Iteration 77/1000 | Loss: 0.00001915
Iteration 78/1000 | Loss: 0.00001915
Iteration 79/1000 | Loss: 0.00001915
Iteration 80/1000 | Loss: 0.00001914
Iteration 81/1000 | Loss: 0.00001914
Iteration 82/1000 | Loss: 0.00001914
Iteration 83/1000 | Loss: 0.00001914
Iteration 84/1000 | Loss: 0.00001914
Iteration 85/1000 | Loss: 0.00001914
Iteration 86/1000 | Loss: 0.00001914
Iteration 87/1000 | Loss: 0.00001914
Iteration 88/1000 | Loss: 0.00001914
Iteration 89/1000 | Loss: 0.00001914
Iteration 90/1000 | Loss: 0.00001914
Iteration 91/1000 | Loss: 0.00001914
Iteration 92/1000 | Loss: 0.00001914
Iteration 93/1000 | Loss: 0.00001914
Iteration 94/1000 | Loss: 0.00001914
Iteration 95/1000 | Loss: 0.00001914
Iteration 96/1000 | Loss: 0.00001914
Iteration 97/1000 | Loss: 0.00001914
Iteration 98/1000 | Loss: 0.00001914
Iteration 99/1000 | Loss: 0.00001914
Iteration 100/1000 | Loss: 0.00001914
Iteration 101/1000 | Loss: 0.00001914
Iteration 102/1000 | Loss: 0.00001914
Iteration 103/1000 | Loss: 0.00001914
Iteration 104/1000 | Loss: 0.00001914
Iteration 105/1000 | Loss: 0.00001914
Iteration 106/1000 | Loss: 0.00001914
Iteration 107/1000 | Loss: 0.00001914
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 107. Stopping optimization.
Last 5 losses: [1.914218046294991e-05, 1.914218046294991e-05, 1.914218046294991e-05, 1.914218046294991e-05, 1.914218046294991e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.914218046294991e-05

Optimization complete. Final v2v error: 3.6765363216400146 mm

Highest mean error: 4.254128932952881 mm for frame 54

Lowest mean error: 3.5187931060791016 mm for frame 1

Saving results

Total time: 36.13597249984741
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alison_posed_001/1010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alison_posed_001/1010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alison_posed_001/1010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00411927
Iteration 2/25 | Loss: 0.00138943
Iteration 3/25 | Loss: 0.00126738
Iteration 4/25 | Loss: 0.00124938
Iteration 5/25 | Loss: 0.00124253
Iteration 6/25 | Loss: 0.00124054
Iteration 7/25 | Loss: 0.00124008
Iteration 8/25 | Loss: 0.00124008
Iteration 9/25 | Loss: 0.00124008
Iteration 10/25 | Loss: 0.00124008
Iteration 11/25 | Loss: 0.00124008
Iteration 12/25 | Loss: 0.00124008
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0012400783598423004, 0.0012400783598423004, 0.0012400783598423004, 0.0012400783598423004, 0.0012400783598423004]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012400783598423004

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43824065
Iteration 2/25 | Loss: 0.00083129
Iteration 3/25 | Loss: 0.00083129
Iteration 4/25 | Loss: 0.00083129
Iteration 5/25 | Loss: 0.00083129
Iteration 6/25 | Loss: 0.00083129
Iteration 7/25 | Loss: 0.00083129
Iteration 8/25 | Loss: 0.00083129
Iteration 9/25 | Loss: 0.00083129
Iteration 10/25 | Loss: 0.00083129
Iteration 11/25 | Loss: 0.00083129
Iteration 12/25 | Loss: 0.00083129
Iteration 13/25 | Loss: 0.00083129
Iteration 14/25 | Loss: 0.00083129
Iteration 15/25 | Loss: 0.00083129
Iteration 16/25 | Loss: 0.00083129
Iteration 17/25 | Loss: 0.00083129
Iteration 18/25 | Loss: 0.00083129
Iteration 19/25 | Loss: 0.00083129
Iteration 20/25 | Loss: 0.00083129
Iteration 21/25 | Loss: 0.00083129
Iteration 22/25 | Loss: 0.00083129
Iteration 23/25 | Loss: 0.00083129
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0008312896243296564, 0.0008312896243296564, 0.0008312896243296564, 0.0008312896243296564, 0.0008312896243296564]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008312896243296564

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00083129
Iteration 2/1000 | Loss: 0.00007792
Iteration 3/1000 | Loss: 0.00004852
Iteration 4/1000 | Loss: 0.00003678
Iteration 5/1000 | Loss: 0.00003080
Iteration 6/1000 | Loss: 0.00002747
Iteration 7/1000 | Loss: 0.00002534
Iteration 8/1000 | Loss: 0.00002382
Iteration 9/1000 | Loss: 0.00002284
Iteration 10/1000 | Loss: 0.00002205
Iteration 11/1000 | Loss: 0.00002155
Iteration 12/1000 | Loss: 0.00002122
Iteration 13/1000 | Loss: 0.00002096
Iteration 14/1000 | Loss: 0.00002075
Iteration 15/1000 | Loss: 0.00002056
Iteration 16/1000 | Loss: 0.00002048
Iteration 17/1000 | Loss: 0.00002045
Iteration 18/1000 | Loss: 0.00002039
Iteration 19/1000 | Loss: 0.00002029
Iteration 20/1000 | Loss: 0.00002019
Iteration 21/1000 | Loss: 0.00002018
Iteration 22/1000 | Loss: 0.00002017
Iteration 23/1000 | Loss: 0.00002014
Iteration 24/1000 | Loss: 0.00002014
Iteration 25/1000 | Loss: 0.00002014
Iteration 26/1000 | Loss: 0.00002013
Iteration 27/1000 | Loss: 0.00002013
Iteration 28/1000 | Loss: 0.00002013
Iteration 29/1000 | Loss: 0.00002012
Iteration 30/1000 | Loss: 0.00002012
Iteration 31/1000 | Loss: 0.00002012
Iteration 32/1000 | Loss: 0.00002011
Iteration 33/1000 | Loss: 0.00002011
Iteration 34/1000 | Loss: 0.00002010
Iteration 35/1000 | Loss: 0.00002010
Iteration 36/1000 | Loss: 0.00002009
Iteration 37/1000 | Loss: 0.00002009
Iteration 38/1000 | Loss: 0.00002009
Iteration 39/1000 | Loss: 0.00002009
Iteration 40/1000 | Loss: 0.00002009
Iteration 41/1000 | Loss: 0.00002009
Iteration 42/1000 | Loss: 0.00002008
Iteration 43/1000 | Loss: 0.00002008
Iteration 44/1000 | Loss: 0.00002008
Iteration 45/1000 | Loss: 0.00002008
Iteration 46/1000 | Loss: 0.00002007
Iteration 47/1000 | Loss: 0.00002007
Iteration 48/1000 | Loss: 0.00002007
Iteration 49/1000 | Loss: 0.00002007
Iteration 50/1000 | Loss: 0.00002007
Iteration 51/1000 | Loss: 0.00002007
Iteration 52/1000 | Loss: 0.00002007
Iteration 53/1000 | Loss: 0.00002007
Iteration 54/1000 | Loss: 0.00002006
Iteration 55/1000 | Loss: 0.00002006
Iteration 56/1000 | Loss: 0.00002006
Iteration 57/1000 | Loss: 0.00002006
Iteration 58/1000 | Loss: 0.00002006
Iteration 59/1000 | Loss: 0.00002006
Iteration 60/1000 | Loss: 0.00002006
Iteration 61/1000 | Loss: 0.00002006
Iteration 62/1000 | Loss: 0.00002006
Iteration 63/1000 | Loss: 0.00002006
Iteration 64/1000 | Loss: 0.00002006
Iteration 65/1000 | Loss: 0.00002005
Iteration 66/1000 | Loss: 0.00002004
Iteration 67/1000 | Loss: 0.00002004
Iteration 68/1000 | Loss: 0.00002004
Iteration 69/1000 | Loss: 0.00002004
Iteration 70/1000 | Loss: 0.00002004
Iteration 71/1000 | Loss: 0.00002003
Iteration 72/1000 | Loss: 0.00002003
Iteration 73/1000 | Loss: 0.00002003
Iteration 74/1000 | Loss: 0.00002003
Iteration 75/1000 | Loss: 0.00002002
Iteration 76/1000 | Loss: 0.00002002
Iteration 77/1000 | Loss: 0.00002002
Iteration 78/1000 | Loss: 0.00002001
Iteration 79/1000 | Loss: 0.00002001
Iteration 80/1000 | Loss: 0.00002001
Iteration 81/1000 | Loss: 0.00002001
Iteration 82/1000 | Loss: 0.00002001
Iteration 83/1000 | Loss: 0.00002000
Iteration 84/1000 | Loss: 0.00002000
Iteration 85/1000 | Loss: 0.00002000
Iteration 86/1000 | Loss: 0.00001999
Iteration 87/1000 | Loss: 0.00001999
Iteration 88/1000 | Loss: 0.00001999
Iteration 89/1000 | Loss: 0.00001999
Iteration 90/1000 | Loss: 0.00001999
Iteration 91/1000 | Loss: 0.00001999
Iteration 92/1000 | Loss: 0.00001999
Iteration 93/1000 | Loss: 0.00001999
Iteration 94/1000 | Loss: 0.00001998
Iteration 95/1000 | Loss: 0.00001998
Iteration 96/1000 | Loss: 0.00001998
Iteration 97/1000 | Loss: 0.00001998
Iteration 98/1000 | Loss: 0.00001998
Iteration 99/1000 | Loss: 0.00001998
Iteration 100/1000 | Loss: 0.00001998
Iteration 101/1000 | Loss: 0.00001998
Iteration 102/1000 | Loss: 0.00001997
Iteration 103/1000 | Loss: 0.00001997
Iteration 104/1000 | Loss: 0.00001997
Iteration 105/1000 | Loss: 0.00001997
Iteration 106/1000 | Loss: 0.00001997
Iteration 107/1000 | Loss: 0.00001996
Iteration 108/1000 | Loss: 0.00001996
Iteration 109/1000 | Loss: 0.00001996
Iteration 110/1000 | Loss: 0.00001996
Iteration 111/1000 | Loss: 0.00001996
Iteration 112/1000 | Loss: 0.00001996
Iteration 113/1000 | Loss: 0.00001995
Iteration 114/1000 | Loss: 0.00001995
Iteration 115/1000 | Loss: 0.00001995
Iteration 116/1000 | Loss: 0.00001995
Iteration 117/1000 | Loss: 0.00001995
Iteration 118/1000 | Loss: 0.00001995
Iteration 119/1000 | Loss: 0.00001994
Iteration 120/1000 | Loss: 0.00001994
Iteration 121/1000 | Loss: 0.00001994
Iteration 122/1000 | Loss: 0.00001993
Iteration 123/1000 | Loss: 0.00001993
Iteration 124/1000 | Loss: 0.00001993
Iteration 125/1000 | Loss: 0.00001992
Iteration 126/1000 | Loss: 0.00001992
Iteration 127/1000 | Loss: 0.00001992
Iteration 128/1000 | Loss: 0.00001992
Iteration 129/1000 | Loss: 0.00001992
Iteration 130/1000 | Loss: 0.00001991
Iteration 131/1000 | Loss: 0.00001991
Iteration 132/1000 | Loss: 0.00001991
Iteration 133/1000 | Loss: 0.00001991
Iteration 134/1000 | Loss: 0.00001990
Iteration 135/1000 | Loss: 0.00001990
Iteration 136/1000 | Loss: 0.00001990
Iteration 137/1000 | Loss: 0.00001990
Iteration 138/1000 | Loss: 0.00001990
Iteration 139/1000 | Loss: 0.00001990
Iteration 140/1000 | Loss: 0.00001990
Iteration 141/1000 | Loss: 0.00001990
Iteration 142/1000 | Loss: 0.00001990
Iteration 143/1000 | Loss: 0.00001990
Iteration 144/1000 | Loss: 0.00001990
Iteration 145/1000 | Loss: 0.00001990
Iteration 146/1000 | Loss: 0.00001990
Iteration 147/1000 | Loss: 0.00001989
Iteration 148/1000 | Loss: 0.00001989
Iteration 149/1000 | Loss: 0.00001989
Iteration 150/1000 | Loss: 0.00001989
Iteration 151/1000 | Loss: 0.00001989
Iteration 152/1000 | Loss: 0.00001989
Iteration 153/1000 | Loss: 0.00001989
Iteration 154/1000 | Loss: 0.00001989
Iteration 155/1000 | Loss: 0.00001989
Iteration 156/1000 | Loss: 0.00001989
Iteration 157/1000 | Loss: 0.00001989
Iteration 158/1000 | Loss: 0.00001989
Iteration 159/1000 | Loss: 0.00001989
Iteration 160/1000 | Loss: 0.00001989
Iteration 161/1000 | Loss: 0.00001989
Iteration 162/1000 | Loss: 0.00001988
Iteration 163/1000 | Loss: 0.00001988
Iteration 164/1000 | Loss: 0.00001988
Iteration 165/1000 | Loss: 0.00001988
Iteration 166/1000 | Loss: 0.00001988
Iteration 167/1000 | Loss: 0.00001988
Iteration 168/1000 | Loss: 0.00001988
Iteration 169/1000 | Loss: 0.00001988
Iteration 170/1000 | Loss: 0.00001988
Iteration 171/1000 | Loss: 0.00001988
Iteration 172/1000 | Loss: 0.00001988
Iteration 173/1000 | Loss: 0.00001988
Iteration 174/1000 | Loss: 0.00001988
Iteration 175/1000 | Loss: 0.00001988
Iteration 176/1000 | Loss: 0.00001988
Iteration 177/1000 | Loss: 0.00001988
Iteration 178/1000 | Loss: 0.00001988
Iteration 179/1000 | Loss: 0.00001987
Iteration 180/1000 | Loss: 0.00001987
Iteration 181/1000 | Loss: 0.00001987
Iteration 182/1000 | Loss: 0.00001987
Iteration 183/1000 | Loss: 0.00001987
Iteration 184/1000 | Loss: 0.00001987
Iteration 185/1000 | Loss: 0.00001987
Iteration 186/1000 | Loss: 0.00001987
Iteration 187/1000 | Loss: 0.00001987
Iteration 188/1000 | Loss: 0.00001987
Iteration 189/1000 | Loss: 0.00001987
Iteration 190/1000 | Loss: 0.00001987
Iteration 191/1000 | Loss: 0.00001987
Iteration 192/1000 | Loss: 0.00001987
Iteration 193/1000 | Loss: 0.00001987
Iteration 194/1000 | Loss: 0.00001987
Iteration 195/1000 | Loss: 0.00001986
Iteration 196/1000 | Loss: 0.00001986
Iteration 197/1000 | Loss: 0.00001986
Iteration 198/1000 | Loss: 0.00001986
Iteration 199/1000 | Loss: 0.00001986
Iteration 200/1000 | Loss: 0.00001986
Iteration 201/1000 | Loss: 0.00001986
Iteration 202/1000 | Loss: 0.00001986
Iteration 203/1000 | Loss: 0.00001986
Iteration 204/1000 | Loss: 0.00001986
Iteration 205/1000 | Loss: 0.00001986
Iteration 206/1000 | Loss: 0.00001986
Iteration 207/1000 | Loss: 0.00001986
Iteration 208/1000 | Loss: 0.00001986
Iteration 209/1000 | Loss: 0.00001985
Iteration 210/1000 | Loss: 0.00001985
Iteration 211/1000 | Loss: 0.00001985
Iteration 212/1000 | Loss: 0.00001985
Iteration 213/1000 | Loss: 0.00001985
Iteration 214/1000 | Loss: 0.00001985
Iteration 215/1000 | Loss: 0.00001985
Iteration 216/1000 | Loss: 0.00001985
Iteration 217/1000 | Loss: 0.00001985
Iteration 218/1000 | Loss: 0.00001985
Iteration 219/1000 | Loss: 0.00001985
Iteration 220/1000 | Loss: 0.00001985
Iteration 221/1000 | Loss: 0.00001985
Iteration 222/1000 | Loss: 0.00001985
Iteration 223/1000 | Loss: 0.00001985
Iteration 224/1000 | Loss: 0.00001985
Iteration 225/1000 | Loss: 0.00001985
Iteration 226/1000 | Loss: 0.00001985
Iteration 227/1000 | Loss: 0.00001985
Iteration 228/1000 | Loss: 0.00001985
Iteration 229/1000 | Loss: 0.00001985
Iteration 230/1000 | Loss: 0.00001985
Iteration 231/1000 | Loss: 0.00001985
Iteration 232/1000 | Loss: 0.00001985
Iteration 233/1000 | Loss: 0.00001985
Iteration 234/1000 | Loss: 0.00001985
Iteration 235/1000 | Loss: 0.00001985
Iteration 236/1000 | Loss: 0.00001985
Iteration 237/1000 | Loss: 0.00001985
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 237. Stopping optimization.
Last 5 losses: [1.9853003323078156e-05, 1.9853003323078156e-05, 1.9853003323078156e-05, 1.9853003323078156e-05, 1.9853003323078156e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9853003323078156e-05

Optimization complete. Final v2v error: 3.699765205383301 mm

Highest mean error: 4.700824737548828 mm for frame 97

Lowest mean error: 2.7988014221191406 mm for frame 105

Saving results

Total time: 47.432560205459595
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alison_posed_001/1007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alison_posed_001/1007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alison_posed_001/1007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01060631
Iteration 2/25 | Loss: 0.01060631
Iteration 3/25 | Loss: 0.00163050
Iteration 4/25 | Loss: 0.00141462
Iteration 5/25 | Loss: 0.00131817
Iteration 6/25 | Loss: 0.00131744
Iteration 7/25 | Loss: 0.00128702
Iteration 8/25 | Loss: 0.00128476
Iteration 9/25 | Loss: 0.00128175
Iteration 10/25 | Loss: 0.00128333
Iteration 11/25 | Loss: 0.00128060
Iteration 12/25 | Loss: 0.00128317
Iteration 13/25 | Loss: 0.00128101
Iteration 14/25 | Loss: 0.00128038
Iteration 15/25 | Loss: 0.00127949
Iteration 16/25 | Loss: 0.00127863
Iteration 17/25 | Loss: 0.00127843
Iteration 18/25 | Loss: 0.00127836
Iteration 19/25 | Loss: 0.00127836
Iteration 20/25 | Loss: 0.00127836
Iteration 21/25 | Loss: 0.00127836
Iteration 22/25 | Loss: 0.00127835
Iteration 23/25 | Loss: 0.00127835
Iteration 24/25 | Loss: 0.00127835
Iteration 25/25 | Loss: 0.00127835

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 8.54883862
Iteration 2/25 | Loss: 0.00090227
Iteration 3/25 | Loss: 0.00086934
Iteration 4/25 | Loss: 0.00086934
Iteration 5/25 | Loss: 0.00086934
Iteration 6/25 | Loss: 0.00086934
Iteration 7/25 | Loss: 0.00086934
Iteration 8/25 | Loss: 0.00086934
Iteration 9/25 | Loss: 0.00086934
Iteration 10/25 | Loss: 0.00086934
Iteration 11/25 | Loss: 0.00086934
Iteration 12/25 | Loss: 0.00086934
Iteration 13/25 | Loss: 0.00086934
Iteration 14/25 | Loss: 0.00086934
Iteration 15/25 | Loss: 0.00086934
Iteration 16/25 | Loss: 0.00086934
Iteration 17/25 | Loss: 0.00086934
Iteration 18/25 | Loss: 0.00086934
Iteration 19/25 | Loss: 0.00086934
Iteration 20/25 | Loss: 0.00086934
Iteration 21/25 | Loss: 0.00086934
Iteration 22/25 | Loss: 0.00086934
Iteration 23/25 | Loss: 0.00086934
Iteration 24/25 | Loss: 0.00086934
Iteration 25/25 | Loss: 0.00086934

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00086934
Iteration 2/1000 | Loss: 0.00002495
Iteration 3/1000 | Loss: 0.00001863
Iteration 4/1000 | Loss: 0.00012905
Iteration 5/1000 | Loss: 0.00001673
Iteration 6/1000 | Loss: 0.00003139
Iteration 7/1000 | Loss: 0.00004826
Iteration 8/1000 | Loss: 0.00016016
Iteration 9/1000 | Loss: 0.00002970
Iteration 10/1000 | Loss: 0.00003687
Iteration 11/1000 | Loss: 0.00001569
Iteration 12/1000 | Loss: 0.00001820
Iteration 13/1000 | Loss: 0.00010670
Iteration 14/1000 | Loss: 0.00001640
Iteration 15/1000 | Loss: 0.00001518
Iteration 16/1000 | Loss: 0.00004290
Iteration 17/1000 | Loss: 0.00001972
Iteration 18/1000 | Loss: 0.00001491
Iteration 19/1000 | Loss: 0.00002950
Iteration 20/1000 | Loss: 0.00001480
Iteration 21/1000 | Loss: 0.00001473
Iteration 22/1000 | Loss: 0.00002610
Iteration 23/1000 | Loss: 0.00012751
Iteration 24/1000 | Loss: 0.00001712
Iteration 25/1000 | Loss: 0.00011537
Iteration 26/1000 | Loss: 0.00001491
Iteration 27/1000 | Loss: 0.00006897
Iteration 28/1000 | Loss: 0.00002207
Iteration 29/1000 | Loss: 0.00001978
Iteration 30/1000 | Loss: 0.00005536
Iteration 31/1000 | Loss: 0.00001998
Iteration 32/1000 | Loss: 0.00001440
Iteration 33/1000 | Loss: 0.00001436
Iteration 34/1000 | Loss: 0.00001435
Iteration 35/1000 | Loss: 0.00001434
Iteration 36/1000 | Loss: 0.00001434
Iteration 37/1000 | Loss: 0.00001434
Iteration 38/1000 | Loss: 0.00001433
Iteration 39/1000 | Loss: 0.00001433
Iteration 40/1000 | Loss: 0.00001433
Iteration 41/1000 | Loss: 0.00001433
Iteration 42/1000 | Loss: 0.00001432
Iteration 43/1000 | Loss: 0.00001432
Iteration 44/1000 | Loss: 0.00001432
Iteration 45/1000 | Loss: 0.00001432
Iteration 46/1000 | Loss: 0.00001431
Iteration 47/1000 | Loss: 0.00001431
Iteration 48/1000 | Loss: 0.00001431
Iteration 49/1000 | Loss: 0.00001431
Iteration 50/1000 | Loss: 0.00001431
Iteration 51/1000 | Loss: 0.00001431
Iteration 52/1000 | Loss: 0.00001431
Iteration 53/1000 | Loss: 0.00001431
Iteration 54/1000 | Loss: 0.00001431
Iteration 55/1000 | Loss: 0.00001431
Iteration 56/1000 | Loss: 0.00001431
Iteration 57/1000 | Loss: 0.00001430
Iteration 58/1000 | Loss: 0.00001430
Iteration 59/1000 | Loss: 0.00001430
Iteration 60/1000 | Loss: 0.00001430
Iteration 61/1000 | Loss: 0.00001430
Iteration 62/1000 | Loss: 0.00001430
Iteration 63/1000 | Loss: 0.00001430
Iteration 64/1000 | Loss: 0.00001430
Iteration 65/1000 | Loss: 0.00001430
Iteration 66/1000 | Loss: 0.00001429
Iteration 67/1000 | Loss: 0.00001429
Iteration 68/1000 | Loss: 0.00001429
Iteration 69/1000 | Loss: 0.00001429
Iteration 70/1000 | Loss: 0.00001429
Iteration 71/1000 | Loss: 0.00001429
Iteration 72/1000 | Loss: 0.00001429
Iteration 73/1000 | Loss: 0.00001429
Iteration 74/1000 | Loss: 0.00001429
Iteration 75/1000 | Loss: 0.00002848
Iteration 76/1000 | Loss: 0.00009392
Iteration 77/1000 | Loss: 0.00011272
Iteration 78/1000 | Loss: 0.00002553
Iteration 79/1000 | Loss: 0.00001515
Iteration 80/1000 | Loss: 0.00006786
Iteration 81/1000 | Loss: 0.00012192
Iteration 82/1000 | Loss: 0.00005563
Iteration 83/1000 | Loss: 0.00003077
Iteration 84/1000 | Loss: 0.00004178
Iteration 85/1000 | Loss: 0.00009039
Iteration 86/1000 | Loss: 0.00001438
Iteration 87/1000 | Loss: 0.00003979
Iteration 88/1000 | Loss: 0.00001432
Iteration 89/1000 | Loss: 0.00001427
Iteration 90/1000 | Loss: 0.00001425
Iteration 91/1000 | Loss: 0.00001425
Iteration 92/1000 | Loss: 0.00001425
Iteration 93/1000 | Loss: 0.00001425
Iteration 94/1000 | Loss: 0.00001424
Iteration 95/1000 | Loss: 0.00001424
Iteration 96/1000 | Loss: 0.00001424
Iteration 97/1000 | Loss: 0.00001424
Iteration 98/1000 | Loss: 0.00001424
Iteration 99/1000 | Loss: 0.00001424
Iteration 100/1000 | Loss: 0.00001424
Iteration 101/1000 | Loss: 0.00001424
Iteration 102/1000 | Loss: 0.00001424
Iteration 103/1000 | Loss: 0.00001424
Iteration 104/1000 | Loss: 0.00001422
Iteration 105/1000 | Loss: 0.00001421
Iteration 106/1000 | Loss: 0.00001420
Iteration 107/1000 | Loss: 0.00001420
Iteration 108/1000 | Loss: 0.00001420
Iteration 109/1000 | Loss: 0.00001420
Iteration 110/1000 | Loss: 0.00001419
Iteration 111/1000 | Loss: 0.00001419
Iteration 112/1000 | Loss: 0.00001419
Iteration 113/1000 | Loss: 0.00001419
Iteration 114/1000 | Loss: 0.00001419
Iteration 115/1000 | Loss: 0.00001418
Iteration 116/1000 | Loss: 0.00001418
Iteration 117/1000 | Loss: 0.00001417
Iteration 118/1000 | Loss: 0.00001417
Iteration 119/1000 | Loss: 0.00001416
Iteration 120/1000 | Loss: 0.00001416
Iteration 121/1000 | Loss: 0.00001416
Iteration 122/1000 | Loss: 0.00001416
Iteration 123/1000 | Loss: 0.00001416
Iteration 124/1000 | Loss: 0.00001416
Iteration 125/1000 | Loss: 0.00001416
Iteration 126/1000 | Loss: 0.00001416
Iteration 127/1000 | Loss: 0.00001416
Iteration 128/1000 | Loss: 0.00001415
Iteration 129/1000 | Loss: 0.00001415
Iteration 130/1000 | Loss: 0.00001415
Iteration 131/1000 | Loss: 0.00001415
Iteration 132/1000 | Loss: 0.00001415
Iteration 133/1000 | Loss: 0.00001414
Iteration 134/1000 | Loss: 0.00001414
Iteration 135/1000 | Loss: 0.00001414
Iteration 136/1000 | Loss: 0.00001414
Iteration 137/1000 | Loss: 0.00001414
Iteration 138/1000 | Loss: 0.00001414
Iteration 139/1000 | Loss: 0.00001414
Iteration 140/1000 | Loss: 0.00001414
Iteration 141/1000 | Loss: 0.00001414
Iteration 142/1000 | Loss: 0.00001414
Iteration 143/1000 | Loss: 0.00001413
Iteration 144/1000 | Loss: 0.00001413
Iteration 145/1000 | Loss: 0.00001413
Iteration 146/1000 | Loss: 0.00001413
Iteration 147/1000 | Loss: 0.00001413
Iteration 148/1000 | Loss: 0.00001413
Iteration 149/1000 | Loss: 0.00001413
Iteration 150/1000 | Loss: 0.00001413
Iteration 151/1000 | Loss: 0.00001413
Iteration 152/1000 | Loss: 0.00001413
Iteration 153/1000 | Loss: 0.00001413
Iteration 154/1000 | Loss: 0.00001412
Iteration 155/1000 | Loss: 0.00001412
Iteration 156/1000 | Loss: 0.00001412
Iteration 157/1000 | Loss: 0.00001412
Iteration 158/1000 | Loss: 0.00001412
Iteration 159/1000 | Loss: 0.00001412
Iteration 160/1000 | Loss: 0.00001412
Iteration 161/1000 | Loss: 0.00001412
Iteration 162/1000 | Loss: 0.00001412
Iteration 163/1000 | Loss: 0.00001412
Iteration 164/1000 | Loss: 0.00001412
Iteration 165/1000 | Loss: 0.00001411
Iteration 166/1000 | Loss: 0.00001411
Iteration 167/1000 | Loss: 0.00001411
Iteration 168/1000 | Loss: 0.00001411
Iteration 169/1000 | Loss: 0.00001411
Iteration 170/1000 | Loss: 0.00001411
Iteration 171/1000 | Loss: 0.00001411
Iteration 172/1000 | Loss: 0.00001411
Iteration 173/1000 | Loss: 0.00001411
Iteration 174/1000 | Loss: 0.00001411
Iteration 175/1000 | Loss: 0.00001411
Iteration 176/1000 | Loss: 0.00001410
Iteration 177/1000 | Loss: 0.00001410
Iteration 178/1000 | Loss: 0.00001410
Iteration 179/1000 | Loss: 0.00001410
Iteration 180/1000 | Loss: 0.00001410
Iteration 181/1000 | Loss: 0.00001410
Iteration 182/1000 | Loss: 0.00001410
Iteration 183/1000 | Loss: 0.00001410
Iteration 184/1000 | Loss: 0.00001410
Iteration 185/1000 | Loss: 0.00001410
Iteration 186/1000 | Loss: 0.00002653
Iteration 187/1000 | Loss: 0.00006339
Iteration 188/1000 | Loss: 0.00001439
Iteration 189/1000 | Loss: 0.00002699
Iteration 190/1000 | Loss: 0.00001421
Iteration 191/1000 | Loss: 0.00001419
Iteration 192/1000 | Loss: 0.00001412
Iteration 193/1000 | Loss: 0.00001409
Iteration 194/1000 | Loss: 0.00001409
Iteration 195/1000 | Loss: 0.00001408
Iteration 196/1000 | Loss: 0.00001408
Iteration 197/1000 | Loss: 0.00001406
Iteration 198/1000 | Loss: 0.00001406
Iteration 199/1000 | Loss: 0.00001406
Iteration 200/1000 | Loss: 0.00001405
Iteration 201/1000 | Loss: 0.00001405
Iteration 202/1000 | Loss: 0.00001405
Iteration 203/1000 | Loss: 0.00001405
Iteration 204/1000 | Loss: 0.00001404
Iteration 205/1000 | Loss: 0.00001404
Iteration 206/1000 | Loss: 0.00001404
Iteration 207/1000 | Loss: 0.00001404
Iteration 208/1000 | Loss: 0.00001403
Iteration 209/1000 | Loss: 0.00001403
Iteration 210/1000 | Loss: 0.00001403
Iteration 211/1000 | Loss: 0.00001403
Iteration 212/1000 | Loss: 0.00001403
Iteration 213/1000 | Loss: 0.00001403
Iteration 214/1000 | Loss: 0.00001403
Iteration 215/1000 | Loss: 0.00001403
Iteration 216/1000 | Loss: 0.00001403
Iteration 217/1000 | Loss: 0.00001403
Iteration 218/1000 | Loss: 0.00001403
Iteration 219/1000 | Loss: 0.00001403
Iteration 220/1000 | Loss: 0.00001403
Iteration 221/1000 | Loss: 0.00001402
Iteration 222/1000 | Loss: 0.00001402
Iteration 223/1000 | Loss: 0.00001402
Iteration 224/1000 | Loss: 0.00001402
Iteration 225/1000 | Loss: 0.00001402
Iteration 226/1000 | Loss: 0.00001402
Iteration 227/1000 | Loss: 0.00001402
Iteration 228/1000 | Loss: 0.00001402
Iteration 229/1000 | Loss: 0.00001402
Iteration 230/1000 | Loss: 0.00001402
Iteration 231/1000 | Loss: 0.00001402
Iteration 232/1000 | Loss: 0.00001402
Iteration 233/1000 | Loss: 0.00001402
Iteration 234/1000 | Loss: 0.00001402
Iteration 235/1000 | Loss: 0.00001402
Iteration 236/1000 | Loss: 0.00001402
Iteration 237/1000 | Loss: 0.00001402
Iteration 238/1000 | Loss: 0.00001402
Iteration 239/1000 | Loss: 0.00001402
Iteration 240/1000 | Loss: 0.00001402
Iteration 241/1000 | Loss: 0.00001402
Iteration 242/1000 | Loss: 0.00001402
Iteration 243/1000 | Loss: 0.00001402
Iteration 244/1000 | Loss: 0.00001402
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 244. Stopping optimization.
Last 5 losses: [1.4021971765032504e-05, 1.4021971765032504e-05, 1.4021971765032504e-05, 1.4021971765032504e-05, 1.4021971765032504e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4021971765032504e-05

Optimization complete. Final v2v error: 3.1296751499176025 mm

Highest mean error: 3.452984571456909 mm for frame 100

Lowest mean error: 2.9202873706817627 mm for frame 158

Saving results

Total time: 114.10635113716125
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alison_posed_001/1089/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alison_posed_001/1089.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alison_posed_001/1089
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00706736
Iteration 2/25 | Loss: 0.00138734
Iteration 3/25 | Loss: 0.00128678
Iteration 4/25 | Loss: 0.00125489
Iteration 5/25 | Loss: 0.00125043
Iteration 6/25 | Loss: 0.00125002
Iteration 7/25 | Loss: 0.00124339
Iteration 8/25 | Loss: 0.00124363
Iteration 9/25 | Loss: 0.00124095
Iteration 10/25 | Loss: 0.00124277
Iteration 11/25 | Loss: 0.00124055
Iteration 12/25 | Loss: 0.00124036
Iteration 13/25 | Loss: 0.00124167
Iteration 14/25 | Loss: 0.00124007
Iteration 15/25 | Loss: 0.00124007
Iteration 16/25 | Loss: 0.00124007
Iteration 17/25 | Loss: 0.00124007
Iteration 18/25 | Loss: 0.00124007
Iteration 19/25 | Loss: 0.00124007
Iteration 20/25 | Loss: 0.00124007
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.001240066601894796, 0.001240066601894796, 0.001240066601894796, 0.001240066601894796, 0.001240066601894796]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001240066601894796

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.67665315
Iteration 2/25 | Loss: 0.00078759
Iteration 3/25 | Loss: 0.00076552
Iteration 4/25 | Loss: 0.00076552
Iteration 5/25 | Loss: 0.00076552
Iteration 6/25 | Loss: 0.00076552
Iteration 7/25 | Loss: 0.00076552
Iteration 8/25 | Loss: 0.00076551
Iteration 9/25 | Loss: 0.00076551
Iteration 10/25 | Loss: 0.00076551
Iteration 11/25 | Loss: 0.00076551
Iteration 12/25 | Loss: 0.00076551
Iteration 13/25 | Loss: 0.00076551
Iteration 14/25 | Loss: 0.00076551
Iteration 15/25 | Loss: 0.00076551
Iteration 16/25 | Loss: 0.00076551
Iteration 17/25 | Loss: 0.00076551
Iteration 18/25 | Loss: 0.00076551
Iteration 19/25 | Loss: 0.00076551
Iteration 20/25 | Loss: 0.00076551
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0007655139197595417, 0.0007655139197595417, 0.0007655139197595417, 0.0007655139197595417, 0.0007655139197595417]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007655139197595417

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00076551
Iteration 2/1000 | Loss: 0.00005540
Iteration 3/1000 | Loss: 0.00002078
Iteration 4/1000 | Loss: 0.00004503
Iteration 5/1000 | Loss: 0.00003095
Iteration 6/1000 | Loss: 0.00001840
Iteration 7/1000 | Loss: 0.00002120
Iteration 8/1000 | Loss: 0.00001729
Iteration 9/1000 | Loss: 0.00001715
Iteration 10/1000 | Loss: 0.00001693
Iteration 11/1000 | Loss: 0.00028858
Iteration 12/1000 | Loss: 0.00001802
Iteration 13/1000 | Loss: 0.00001632
Iteration 14/1000 | Loss: 0.00001578
Iteration 15/1000 | Loss: 0.00001544
Iteration 16/1000 | Loss: 0.00001514
Iteration 17/1000 | Loss: 0.00001505
Iteration 18/1000 | Loss: 0.00001504
Iteration 19/1000 | Loss: 0.00001503
Iteration 20/1000 | Loss: 0.00001503
Iteration 21/1000 | Loss: 0.00001499
Iteration 22/1000 | Loss: 0.00001498
Iteration 23/1000 | Loss: 0.00001494
Iteration 24/1000 | Loss: 0.00001494
Iteration 25/1000 | Loss: 0.00001487
Iteration 26/1000 | Loss: 0.00001486
Iteration 27/1000 | Loss: 0.00001482
Iteration 28/1000 | Loss: 0.00001481
Iteration 29/1000 | Loss: 0.00001481
Iteration 30/1000 | Loss: 0.00001481
Iteration 31/1000 | Loss: 0.00001480
Iteration 32/1000 | Loss: 0.00001477
Iteration 33/1000 | Loss: 0.00001475
Iteration 34/1000 | Loss: 0.00001474
Iteration 35/1000 | Loss: 0.00001474
Iteration 36/1000 | Loss: 0.00001474
Iteration 37/1000 | Loss: 0.00001473
Iteration 38/1000 | Loss: 0.00001471
Iteration 39/1000 | Loss: 0.00001470
Iteration 40/1000 | Loss: 0.00001469
Iteration 41/1000 | Loss: 0.00001463
Iteration 42/1000 | Loss: 0.00001462
Iteration 43/1000 | Loss: 0.00001462
Iteration 44/1000 | Loss: 0.00001462
Iteration 45/1000 | Loss: 0.00001461
Iteration 46/1000 | Loss: 0.00001461
Iteration 47/1000 | Loss: 0.00001461
Iteration 48/1000 | Loss: 0.00001460
Iteration 49/1000 | Loss: 0.00001460
Iteration 50/1000 | Loss: 0.00001460
Iteration 51/1000 | Loss: 0.00001460
Iteration 52/1000 | Loss: 0.00001459
Iteration 53/1000 | Loss: 0.00001459
Iteration 54/1000 | Loss: 0.00001459
Iteration 55/1000 | Loss: 0.00001458
Iteration 56/1000 | Loss: 0.00001458
Iteration 57/1000 | Loss: 0.00001458
Iteration 58/1000 | Loss: 0.00001457
Iteration 59/1000 | Loss: 0.00001457
Iteration 60/1000 | Loss: 0.00001457
Iteration 61/1000 | Loss: 0.00001457
Iteration 62/1000 | Loss: 0.00001457
Iteration 63/1000 | Loss: 0.00001456
Iteration 64/1000 | Loss: 0.00001456
Iteration 65/1000 | Loss: 0.00001456
Iteration 66/1000 | Loss: 0.00001455
Iteration 67/1000 | Loss: 0.00001455
Iteration 68/1000 | Loss: 0.00001455
Iteration 69/1000 | Loss: 0.00001455
Iteration 70/1000 | Loss: 0.00001455
Iteration 71/1000 | Loss: 0.00001454
Iteration 72/1000 | Loss: 0.00001454
Iteration 73/1000 | Loss: 0.00001454
Iteration 74/1000 | Loss: 0.00001454
Iteration 75/1000 | Loss: 0.00001453
Iteration 76/1000 | Loss: 0.00001453
Iteration 77/1000 | Loss: 0.00001452
Iteration 78/1000 | Loss: 0.00001452
Iteration 79/1000 | Loss: 0.00001452
Iteration 80/1000 | Loss: 0.00001451
Iteration 81/1000 | Loss: 0.00001451
Iteration 82/1000 | Loss: 0.00001451
Iteration 83/1000 | Loss: 0.00001451
Iteration 84/1000 | Loss: 0.00001451
Iteration 85/1000 | Loss: 0.00001451
Iteration 86/1000 | Loss: 0.00001451
Iteration 87/1000 | Loss: 0.00001451
Iteration 88/1000 | Loss: 0.00001450
Iteration 89/1000 | Loss: 0.00001450
Iteration 90/1000 | Loss: 0.00001450
Iteration 91/1000 | Loss: 0.00001450
Iteration 92/1000 | Loss: 0.00001450
Iteration 93/1000 | Loss: 0.00001450
Iteration 94/1000 | Loss: 0.00001450
Iteration 95/1000 | Loss: 0.00001449
Iteration 96/1000 | Loss: 0.00001448
Iteration 97/1000 | Loss: 0.00001448
Iteration 98/1000 | Loss: 0.00001448
Iteration 99/1000 | Loss: 0.00001448
Iteration 100/1000 | Loss: 0.00001448
Iteration 101/1000 | Loss: 0.00001447
Iteration 102/1000 | Loss: 0.00001447
Iteration 103/1000 | Loss: 0.00001447
Iteration 104/1000 | Loss: 0.00001447
Iteration 105/1000 | Loss: 0.00001447
Iteration 106/1000 | Loss: 0.00001447
Iteration 107/1000 | Loss: 0.00001447
Iteration 108/1000 | Loss: 0.00001447
Iteration 109/1000 | Loss: 0.00001447
Iteration 110/1000 | Loss: 0.00001447
Iteration 111/1000 | Loss: 0.00001447
Iteration 112/1000 | Loss: 0.00001446
Iteration 113/1000 | Loss: 0.00001446
Iteration 114/1000 | Loss: 0.00001446
Iteration 115/1000 | Loss: 0.00001445
Iteration 116/1000 | Loss: 0.00001445
Iteration 117/1000 | Loss: 0.00001445
Iteration 118/1000 | Loss: 0.00001444
Iteration 119/1000 | Loss: 0.00001444
Iteration 120/1000 | Loss: 0.00001444
Iteration 121/1000 | Loss: 0.00001444
Iteration 122/1000 | Loss: 0.00001444
Iteration 123/1000 | Loss: 0.00001444
Iteration 124/1000 | Loss: 0.00001444
Iteration 125/1000 | Loss: 0.00001444
Iteration 126/1000 | Loss: 0.00001443
Iteration 127/1000 | Loss: 0.00001443
Iteration 128/1000 | Loss: 0.00001443
Iteration 129/1000 | Loss: 0.00001443
Iteration 130/1000 | Loss: 0.00001443
Iteration 131/1000 | Loss: 0.00001443
Iteration 132/1000 | Loss: 0.00001443
Iteration 133/1000 | Loss: 0.00001443
Iteration 134/1000 | Loss: 0.00001443
Iteration 135/1000 | Loss: 0.00001443
Iteration 136/1000 | Loss: 0.00001443
Iteration 137/1000 | Loss: 0.00001443
Iteration 138/1000 | Loss: 0.00001443
Iteration 139/1000 | Loss: 0.00001443
Iteration 140/1000 | Loss: 0.00001443
Iteration 141/1000 | Loss: 0.00001443
Iteration 142/1000 | Loss: 0.00001443
Iteration 143/1000 | Loss: 0.00001443
Iteration 144/1000 | Loss: 0.00001443
Iteration 145/1000 | Loss: 0.00001443
Iteration 146/1000 | Loss: 0.00001442
Iteration 147/1000 | Loss: 0.00001442
Iteration 148/1000 | Loss: 0.00001442
Iteration 149/1000 | Loss: 0.00001442
Iteration 150/1000 | Loss: 0.00001442
Iteration 151/1000 | Loss: 0.00001442
Iteration 152/1000 | Loss: 0.00001442
Iteration 153/1000 | Loss: 0.00001442
Iteration 154/1000 | Loss: 0.00001442
Iteration 155/1000 | Loss: 0.00001442
Iteration 156/1000 | Loss: 0.00001442
Iteration 157/1000 | Loss: 0.00001442
Iteration 158/1000 | Loss: 0.00001442
Iteration 159/1000 | Loss: 0.00001442
Iteration 160/1000 | Loss: 0.00001442
Iteration 161/1000 | Loss: 0.00001442
Iteration 162/1000 | Loss: 0.00001442
Iteration 163/1000 | Loss: 0.00001442
Iteration 164/1000 | Loss: 0.00001442
Iteration 165/1000 | Loss: 0.00001442
Iteration 166/1000 | Loss: 0.00001442
Iteration 167/1000 | Loss: 0.00001442
Iteration 168/1000 | Loss: 0.00001442
Iteration 169/1000 | Loss: 0.00001442
Iteration 170/1000 | Loss: 0.00001442
Iteration 171/1000 | Loss: 0.00001442
Iteration 172/1000 | Loss: 0.00001442
Iteration 173/1000 | Loss: 0.00001441
Iteration 174/1000 | Loss: 0.00001441
Iteration 175/1000 | Loss: 0.00001441
Iteration 176/1000 | Loss: 0.00001441
Iteration 177/1000 | Loss: 0.00001441
Iteration 178/1000 | Loss: 0.00001441
Iteration 179/1000 | Loss: 0.00001441
Iteration 180/1000 | Loss: 0.00001441
Iteration 181/1000 | Loss: 0.00001441
Iteration 182/1000 | Loss: 0.00001441
Iteration 183/1000 | Loss: 0.00001441
Iteration 184/1000 | Loss: 0.00001441
Iteration 185/1000 | Loss: 0.00001441
Iteration 186/1000 | Loss: 0.00001441
Iteration 187/1000 | Loss: 0.00001441
Iteration 188/1000 | Loss: 0.00001441
Iteration 189/1000 | Loss: 0.00001441
Iteration 190/1000 | Loss: 0.00001441
Iteration 191/1000 | Loss: 0.00001441
Iteration 192/1000 | Loss: 0.00001441
Iteration 193/1000 | Loss: 0.00001441
Iteration 194/1000 | Loss: 0.00001441
Iteration 195/1000 | Loss: 0.00001441
Iteration 196/1000 | Loss: 0.00001441
Iteration 197/1000 | Loss: 0.00001441
Iteration 198/1000 | Loss: 0.00001441
Iteration 199/1000 | Loss: 0.00001441
Iteration 200/1000 | Loss: 0.00001441
Iteration 201/1000 | Loss: 0.00001441
Iteration 202/1000 | Loss: 0.00001441
Iteration 203/1000 | Loss: 0.00001441
Iteration 204/1000 | Loss: 0.00001441
Iteration 205/1000 | Loss: 0.00001441
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 205. Stopping optimization.
Last 5 losses: [1.4414769793802407e-05, 1.4414769793802407e-05, 1.4414769793802407e-05, 1.4414769793802407e-05, 1.4414769793802407e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4414769793802407e-05

Optimization complete. Final v2v error: 3.224557638168335 mm

Highest mean error: 4.070694923400879 mm for frame 140

Lowest mean error: 2.989356517791748 mm for frame 246

Saving results

Total time: 70.24962329864502
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alison_posed_001/1036/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alison_posed_001/1036.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alison_posed_001/1036
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00657621
Iteration 2/25 | Loss: 0.00156789
Iteration 3/25 | Loss: 0.00134648
Iteration 4/25 | Loss: 0.00130721
Iteration 5/25 | Loss: 0.00129597
Iteration 6/25 | Loss: 0.00129387
Iteration 7/25 | Loss: 0.00129748
Iteration 8/25 | Loss: 0.00128765
Iteration 9/25 | Loss: 0.00128123
Iteration 10/25 | Loss: 0.00127745
Iteration 11/25 | Loss: 0.00127669
Iteration 12/25 | Loss: 0.00127663
Iteration 13/25 | Loss: 0.00127663
Iteration 14/25 | Loss: 0.00127663
Iteration 15/25 | Loss: 0.00127663
Iteration 16/25 | Loss: 0.00127663
Iteration 17/25 | Loss: 0.00127663
Iteration 18/25 | Loss: 0.00127662
Iteration 19/25 | Loss: 0.00127662
Iteration 20/25 | Loss: 0.00127662
Iteration 21/25 | Loss: 0.00127662
Iteration 22/25 | Loss: 0.00127662
Iteration 23/25 | Loss: 0.00127662
Iteration 24/25 | Loss: 0.00127662
Iteration 25/25 | Loss: 0.00127662

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.08811879
Iteration 2/25 | Loss: 0.00089065
Iteration 3/25 | Loss: 0.00089032
Iteration 4/25 | Loss: 0.00089032
Iteration 5/25 | Loss: 0.00089032
Iteration 6/25 | Loss: 0.00089031
Iteration 7/25 | Loss: 0.00089031
Iteration 8/25 | Loss: 0.00089031
Iteration 9/25 | Loss: 0.00089031
Iteration 10/25 | Loss: 0.00089031
Iteration 11/25 | Loss: 0.00089031
Iteration 12/25 | Loss: 0.00089031
Iteration 13/25 | Loss: 0.00089031
Iteration 14/25 | Loss: 0.00089031
Iteration 15/25 | Loss: 0.00089031
Iteration 16/25 | Loss: 0.00089031
Iteration 17/25 | Loss: 0.00089031
Iteration 18/25 | Loss: 0.00089031
Iteration 19/25 | Loss: 0.00089031
Iteration 20/25 | Loss: 0.00089031
Iteration 21/25 | Loss: 0.00089031
Iteration 22/25 | Loss: 0.00089031
Iteration 23/25 | Loss: 0.00089031
Iteration 24/25 | Loss: 0.00089031
Iteration 25/25 | Loss: 0.00089031

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00089031
Iteration 2/1000 | Loss: 0.00006746
Iteration 3/1000 | Loss: 0.00004267
Iteration 4/1000 | Loss: 0.00003322
Iteration 5/1000 | Loss: 0.00003002
Iteration 6/1000 | Loss: 0.00002814
Iteration 7/1000 | Loss: 0.00002702
Iteration 8/1000 | Loss: 0.00002619
Iteration 9/1000 | Loss: 0.00002564
Iteration 10/1000 | Loss: 0.00002519
Iteration 11/1000 | Loss: 0.00002485
Iteration 12/1000 | Loss: 0.00002455
Iteration 13/1000 | Loss: 0.00002428
Iteration 14/1000 | Loss: 0.00002408
Iteration 15/1000 | Loss: 0.00002393
Iteration 16/1000 | Loss: 0.00002390
Iteration 17/1000 | Loss: 0.00002389
Iteration 18/1000 | Loss: 0.00002374
Iteration 19/1000 | Loss: 0.00002371
Iteration 20/1000 | Loss: 0.00002368
Iteration 21/1000 | Loss: 0.00002365
Iteration 22/1000 | Loss: 0.00002363
Iteration 23/1000 | Loss: 0.00002363
Iteration 24/1000 | Loss: 0.00002362
Iteration 25/1000 | Loss: 0.00002362
Iteration 26/1000 | Loss: 0.00002356
Iteration 27/1000 | Loss: 0.00002355
Iteration 28/1000 | Loss: 0.00002354
Iteration 29/1000 | Loss: 0.00002354
Iteration 30/1000 | Loss: 0.00002349
Iteration 31/1000 | Loss: 0.00002349
Iteration 32/1000 | Loss: 0.00002347
Iteration 33/1000 | Loss: 0.00002347
Iteration 34/1000 | Loss: 0.00002346
Iteration 35/1000 | Loss: 0.00002346
Iteration 36/1000 | Loss: 0.00002345
Iteration 37/1000 | Loss: 0.00002345
Iteration 38/1000 | Loss: 0.00002344
Iteration 39/1000 | Loss: 0.00002344
Iteration 40/1000 | Loss: 0.00002343
Iteration 41/1000 | Loss: 0.00002343
Iteration 42/1000 | Loss: 0.00002343
Iteration 43/1000 | Loss: 0.00002342
Iteration 44/1000 | Loss: 0.00002342
Iteration 45/1000 | Loss: 0.00002341
Iteration 46/1000 | Loss: 0.00002341
Iteration 47/1000 | Loss: 0.00002340
Iteration 48/1000 | Loss: 0.00002339
Iteration 49/1000 | Loss: 0.00002338
Iteration 50/1000 | Loss: 0.00002338
Iteration 51/1000 | Loss: 0.00002337
Iteration 52/1000 | Loss: 0.00002337
Iteration 53/1000 | Loss: 0.00002337
Iteration 54/1000 | Loss: 0.00002336
Iteration 55/1000 | Loss: 0.00002335
Iteration 56/1000 | Loss: 0.00002335
Iteration 57/1000 | Loss: 0.00002335
Iteration 58/1000 | Loss: 0.00002334
Iteration 59/1000 | Loss: 0.00002334
Iteration 60/1000 | Loss: 0.00002334
Iteration 61/1000 | Loss: 0.00002333
Iteration 62/1000 | Loss: 0.00002333
Iteration 63/1000 | Loss: 0.00002332
Iteration 64/1000 | Loss: 0.00002332
Iteration 65/1000 | Loss: 0.00002331
Iteration 66/1000 | Loss: 0.00002331
Iteration 67/1000 | Loss: 0.00002331
Iteration 68/1000 | Loss: 0.00002330
Iteration 69/1000 | Loss: 0.00002330
Iteration 70/1000 | Loss: 0.00002330
Iteration 71/1000 | Loss: 0.00002330
Iteration 72/1000 | Loss: 0.00002329
Iteration 73/1000 | Loss: 0.00002328
Iteration 74/1000 | Loss: 0.00002328
Iteration 75/1000 | Loss: 0.00002327
Iteration 76/1000 | Loss: 0.00002327
Iteration 77/1000 | Loss: 0.00002327
Iteration 78/1000 | Loss: 0.00002326
Iteration 79/1000 | Loss: 0.00002326
Iteration 80/1000 | Loss: 0.00002326
Iteration 81/1000 | Loss: 0.00002325
Iteration 82/1000 | Loss: 0.00002325
Iteration 83/1000 | Loss: 0.00002325
Iteration 84/1000 | Loss: 0.00002324
Iteration 85/1000 | Loss: 0.00002324
Iteration 86/1000 | Loss: 0.00002324
Iteration 87/1000 | Loss: 0.00002323
Iteration 88/1000 | Loss: 0.00002323
Iteration 89/1000 | Loss: 0.00002322
Iteration 90/1000 | Loss: 0.00002321
Iteration 91/1000 | Loss: 0.00002321
Iteration 92/1000 | Loss: 0.00002321
Iteration 93/1000 | Loss: 0.00002321
Iteration 94/1000 | Loss: 0.00002321
Iteration 95/1000 | Loss: 0.00002321
Iteration 96/1000 | Loss: 0.00002321
Iteration 97/1000 | Loss: 0.00002320
Iteration 98/1000 | Loss: 0.00002320
Iteration 99/1000 | Loss: 0.00002320
Iteration 100/1000 | Loss: 0.00002320
Iteration 101/1000 | Loss: 0.00002320
Iteration 102/1000 | Loss: 0.00002320
Iteration 103/1000 | Loss: 0.00002320
Iteration 104/1000 | Loss: 0.00002320
Iteration 105/1000 | Loss: 0.00002320
Iteration 106/1000 | Loss: 0.00002320
Iteration 107/1000 | Loss: 0.00002320
Iteration 108/1000 | Loss: 0.00002320
Iteration 109/1000 | Loss: 0.00002320
Iteration 110/1000 | Loss: 0.00002320
Iteration 111/1000 | Loss: 0.00002319
Iteration 112/1000 | Loss: 0.00002319
Iteration 113/1000 | Loss: 0.00002319
Iteration 114/1000 | Loss: 0.00002318
Iteration 115/1000 | Loss: 0.00002318
Iteration 116/1000 | Loss: 0.00002318
Iteration 117/1000 | Loss: 0.00002317
Iteration 118/1000 | Loss: 0.00002317
Iteration 119/1000 | Loss: 0.00002317
Iteration 120/1000 | Loss: 0.00002317
Iteration 121/1000 | Loss: 0.00002317
Iteration 122/1000 | Loss: 0.00002316
Iteration 123/1000 | Loss: 0.00002316
Iteration 124/1000 | Loss: 0.00002316
Iteration 125/1000 | Loss: 0.00002316
Iteration 126/1000 | Loss: 0.00002316
Iteration 127/1000 | Loss: 0.00002316
Iteration 128/1000 | Loss: 0.00002316
Iteration 129/1000 | Loss: 0.00002315
Iteration 130/1000 | Loss: 0.00002315
Iteration 131/1000 | Loss: 0.00002315
Iteration 132/1000 | Loss: 0.00002314
Iteration 133/1000 | Loss: 0.00002314
Iteration 134/1000 | Loss: 0.00002314
Iteration 135/1000 | Loss: 0.00002314
Iteration 136/1000 | Loss: 0.00002314
Iteration 137/1000 | Loss: 0.00002314
Iteration 138/1000 | Loss: 0.00002313
Iteration 139/1000 | Loss: 0.00002313
Iteration 140/1000 | Loss: 0.00002313
Iteration 141/1000 | Loss: 0.00002313
Iteration 142/1000 | Loss: 0.00002313
Iteration 143/1000 | Loss: 0.00002313
Iteration 144/1000 | Loss: 0.00002313
Iteration 145/1000 | Loss: 0.00002312
Iteration 146/1000 | Loss: 0.00002312
Iteration 147/1000 | Loss: 0.00002312
Iteration 148/1000 | Loss: 0.00002311
Iteration 149/1000 | Loss: 0.00002311
Iteration 150/1000 | Loss: 0.00002311
Iteration 151/1000 | Loss: 0.00002310
Iteration 152/1000 | Loss: 0.00002310
Iteration 153/1000 | Loss: 0.00002310
Iteration 154/1000 | Loss: 0.00002309
Iteration 155/1000 | Loss: 0.00002309
Iteration 156/1000 | Loss: 0.00002309
Iteration 157/1000 | Loss: 0.00002309
Iteration 158/1000 | Loss: 0.00002309
Iteration 159/1000 | Loss: 0.00002309
Iteration 160/1000 | Loss: 0.00002308
Iteration 161/1000 | Loss: 0.00002308
Iteration 162/1000 | Loss: 0.00002308
Iteration 163/1000 | Loss: 0.00002308
Iteration 164/1000 | Loss: 0.00002307
Iteration 165/1000 | Loss: 0.00002307
Iteration 166/1000 | Loss: 0.00002307
Iteration 167/1000 | Loss: 0.00002307
Iteration 168/1000 | Loss: 0.00002307
Iteration 169/1000 | Loss: 0.00002307
Iteration 170/1000 | Loss: 0.00002307
Iteration 171/1000 | Loss: 0.00002307
Iteration 172/1000 | Loss: 0.00002306
Iteration 173/1000 | Loss: 0.00002306
Iteration 174/1000 | Loss: 0.00002306
Iteration 175/1000 | Loss: 0.00002306
Iteration 176/1000 | Loss: 0.00002306
Iteration 177/1000 | Loss: 0.00002306
Iteration 178/1000 | Loss: 0.00002306
Iteration 179/1000 | Loss: 0.00002306
Iteration 180/1000 | Loss: 0.00002306
Iteration 181/1000 | Loss: 0.00002305
Iteration 182/1000 | Loss: 0.00002305
Iteration 183/1000 | Loss: 0.00002305
Iteration 184/1000 | Loss: 0.00002305
Iteration 185/1000 | Loss: 0.00002305
Iteration 186/1000 | Loss: 0.00002304
Iteration 187/1000 | Loss: 0.00002304
Iteration 188/1000 | Loss: 0.00002304
Iteration 189/1000 | Loss: 0.00002304
Iteration 190/1000 | Loss: 0.00002304
Iteration 191/1000 | Loss: 0.00002304
Iteration 192/1000 | Loss: 0.00002304
Iteration 193/1000 | Loss: 0.00002304
Iteration 194/1000 | Loss: 0.00002303
Iteration 195/1000 | Loss: 0.00002303
Iteration 196/1000 | Loss: 0.00002303
Iteration 197/1000 | Loss: 0.00002303
Iteration 198/1000 | Loss: 0.00002303
Iteration 199/1000 | Loss: 0.00002303
Iteration 200/1000 | Loss: 0.00002303
Iteration 201/1000 | Loss: 0.00002303
Iteration 202/1000 | Loss: 0.00002303
Iteration 203/1000 | Loss: 0.00002303
Iteration 204/1000 | Loss: 0.00002303
Iteration 205/1000 | Loss: 0.00002302
Iteration 206/1000 | Loss: 0.00002302
Iteration 207/1000 | Loss: 0.00002302
Iteration 208/1000 | Loss: 0.00002302
Iteration 209/1000 | Loss: 0.00002302
Iteration 210/1000 | Loss: 0.00002302
Iteration 211/1000 | Loss: 0.00002302
Iteration 212/1000 | Loss: 0.00002302
Iteration 213/1000 | Loss: 0.00002302
Iteration 214/1000 | Loss: 0.00002302
Iteration 215/1000 | Loss: 0.00002302
Iteration 216/1000 | Loss: 0.00002301
Iteration 217/1000 | Loss: 0.00002301
Iteration 218/1000 | Loss: 0.00002301
Iteration 219/1000 | Loss: 0.00002301
Iteration 220/1000 | Loss: 0.00002301
Iteration 221/1000 | Loss: 0.00002301
Iteration 222/1000 | Loss: 0.00002301
Iteration 223/1000 | Loss: 0.00002301
Iteration 224/1000 | Loss: 0.00002301
Iteration 225/1000 | Loss: 0.00002301
Iteration 226/1000 | Loss: 0.00002301
Iteration 227/1000 | Loss: 0.00002301
Iteration 228/1000 | Loss: 0.00002301
Iteration 229/1000 | Loss: 0.00002301
Iteration 230/1000 | Loss: 0.00002301
Iteration 231/1000 | Loss: 0.00002301
Iteration 232/1000 | Loss: 0.00002301
Iteration 233/1000 | Loss: 0.00002301
Iteration 234/1000 | Loss: 0.00002301
Iteration 235/1000 | Loss: 0.00002301
Iteration 236/1000 | Loss: 0.00002301
Iteration 237/1000 | Loss: 0.00002301
Iteration 238/1000 | Loss: 0.00002301
Iteration 239/1000 | Loss: 0.00002301
Iteration 240/1000 | Loss: 0.00002301
Iteration 241/1000 | Loss: 0.00002301
Iteration 242/1000 | Loss: 0.00002301
Iteration 243/1000 | Loss: 0.00002301
Iteration 244/1000 | Loss: 0.00002301
Iteration 245/1000 | Loss: 0.00002301
Iteration 246/1000 | Loss: 0.00002301
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 246. Stopping optimization.
Last 5 losses: [2.30067362281261e-05, 2.30067362281261e-05, 2.30067362281261e-05, 2.30067362281261e-05, 2.30067362281261e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.30067362281261e-05

Optimization complete. Final v2v error: 3.862694025039673 mm

Highest mean error: 6.20309591293335 mm for frame 120

Lowest mean error: 3.0505661964416504 mm for frame 12

Saving results

Total time: 70.18109178543091
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alison_posed_001/1054/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alison_posed_001/1054.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alison_posed_001/1054
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01060235
Iteration 2/25 | Loss: 0.01060235
Iteration 3/25 | Loss: 0.00290100
Iteration 4/25 | Loss: 0.00196177
Iteration 5/25 | Loss: 0.00149390
Iteration 6/25 | Loss: 0.00143701
Iteration 7/25 | Loss: 0.00141291
Iteration 8/25 | Loss: 0.00137874
Iteration 9/25 | Loss: 0.00138279
Iteration 10/25 | Loss: 0.00134280
Iteration 11/25 | Loss: 0.00134035
Iteration 12/25 | Loss: 0.00133923
Iteration 13/25 | Loss: 0.00133866
Iteration 14/25 | Loss: 0.00133805
Iteration 15/25 | Loss: 0.00133783
Iteration 16/25 | Loss: 0.00133704
Iteration 17/25 | Loss: 0.00133704
Iteration 18/25 | Loss: 0.00133695
Iteration 19/25 | Loss: 0.00133694
Iteration 20/25 | Loss: 0.00133694
Iteration 21/25 | Loss: 0.00133694
Iteration 22/25 | Loss: 0.00133694
Iteration 23/25 | Loss: 0.00133694
Iteration 24/25 | Loss: 0.00133694
Iteration 25/25 | Loss: 0.00133693

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.48182344
Iteration 2/25 | Loss: 0.00088029
Iteration 3/25 | Loss: 0.00084787
Iteration 4/25 | Loss: 0.00084787
Iteration 5/25 | Loss: 0.00084787
Iteration 6/25 | Loss: 0.00084787
Iteration 7/25 | Loss: 0.00084787
Iteration 8/25 | Loss: 0.00084787
Iteration 9/25 | Loss: 0.00084787
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 9. Stopping optimization.
Last 5 losses: [0.0008478683303110301, 0.0008478683303110301, 0.0008478683303110301, 0.0008478683303110301, 0.0008478683303110301]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008478683303110301

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00084787
Iteration 2/1000 | Loss: 0.00009279
Iteration 3/1000 | Loss: 0.00006728
Iteration 4/1000 | Loss: 0.00003962
Iteration 5/1000 | Loss: 0.00003699
Iteration 6/1000 | Loss: 0.00003535
Iteration 7/1000 | Loss: 0.00004474
Iteration 8/1000 | Loss: 0.00003373
Iteration 9/1000 | Loss: 0.00005524
Iteration 10/1000 | Loss: 0.00003258
Iteration 11/1000 | Loss: 0.00021070
Iteration 12/1000 | Loss: 0.00003213
Iteration 13/1000 | Loss: 0.00003192
Iteration 14/1000 | Loss: 0.00003179
Iteration 15/1000 | Loss: 0.00003153
Iteration 16/1000 | Loss: 0.00003147
Iteration 17/1000 | Loss: 0.00003147
Iteration 18/1000 | Loss: 0.00003143
Iteration 19/1000 | Loss: 0.00003135
Iteration 20/1000 | Loss: 0.00003122
Iteration 21/1000 | Loss: 0.00003112
Iteration 22/1000 | Loss: 0.00003111
Iteration 23/1000 | Loss: 0.00003111
Iteration 24/1000 | Loss: 0.00003110
Iteration 25/1000 | Loss: 0.00003110
Iteration 26/1000 | Loss: 0.00003110
Iteration 27/1000 | Loss: 0.00003110
Iteration 28/1000 | Loss: 0.00003109
Iteration 29/1000 | Loss: 0.00003109
Iteration 30/1000 | Loss: 0.00003105
Iteration 31/1000 | Loss: 0.00004028
Iteration 32/1000 | Loss: 0.00003236
Iteration 33/1000 | Loss: 0.00003086
Iteration 34/1000 | Loss: 0.00003086
Iteration 35/1000 | Loss: 0.00003086
Iteration 36/1000 | Loss: 0.00003086
Iteration 37/1000 | Loss: 0.00003086
Iteration 38/1000 | Loss: 0.00003086
Iteration 39/1000 | Loss: 0.00003086
Iteration 40/1000 | Loss: 0.00003086
Iteration 41/1000 | Loss: 0.00003086
Iteration 42/1000 | Loss: 0.00003085
Iteration 43/1000 | Loss: 0.00003085
Iteration 44/1000 | Loss: 0.00003085
Iteration 45/1000 | Loss: 0.00003085
Iteration 46/1000 | Loss: 0.00003085
Iteration 47/1000 | Loss: 0.00003085
Iteration 48/1000 | Loss: 0.00003085
Iteration 49/1000 | Loss: 0.00003085
Iteration 50/1000 | Loss: 0.00003085
Iteration 51/1000 | Loss: 0.00003085
Iteration 52/1000 | Loss: 0.00003085
Iteration 53/1000 | Loss: 0.00003085
Iteration 54/1000 | Loss: 0.00003085
Iteration 55/1000 | Loss: 0.00003085
Iteration 56/1000 | Loss: 0.00003085
Iteration 57/1000 | Loss: 0.00003085
Iteration 58/1000 | Loss: 0.00003085
Iteration 59/1000 | Loss: 0.00003085
Iteration 60/1000 | Loss: 0.00003085
Iteration 61/1000 | Loss: 0.00003085
Iteration 62/1000 | Loss: 0.00003085
Iteration 63/1000 | Loss: 0.00003085
Iteration 64/1000 | Loss: 0.00003085
Iteration 65/1000 | Loss: 0.00003085
Iteration 66/1000 | Loss: 0.00003085
Iteration 67/1000 | Loss: 0.00003085
Iteration 68/1000 | Loss: 0.00003085
Iteration 69/1000 | Loss: 0.00003085
Iteration 70/1000 | Loss: 0.00003085
Iteration 71/1000 | Loss: 0.00003085
Iteration 72/1000 | Loss: 0.00003085
Iteration 73/1000 | Loss: 0.00003085
Iteration 74/1000 | Loss: 0.00003085
Iteration 75/1000 | Loss: 0.00003085
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 75. Stopping optimization.
Last 5 losses: [3.084914351347834e-05, 3.084914351347834e-05, 3.084914351347834e-05, 3.084914351347834e-05, 3.084914351347834e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.084914351347834e-05

Optimization complete. Final v2v error: 4.723478317260742 mm

Highest mean error: 5.063109397888184 mm for frame 8

Lowest mean error: 3.652430772781372 mm for frame 0

Saving results

Total time: 61.15272521972656
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alison_posed_001/1015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alison_posed_001/1015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alison_posed_001/1015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00399204
Iteration 2/25 | Loss: 0.00137773
Iteration 3/25 | Loss: 0.00128683
Iteration 4/25 | Loss: 0.00127562
Iteration 5/25 | Loss: 0.00127308
Iteration 6/25 | Loss: 0.00127263
Iteration 7/25 | Loss: 0.00127263
Iteration 8/25 | Loss: 0.00127263
Iteration 9/25 | Loss: 0.00127263
Iteration 10/25 | Loss: 0.00127263
Iteration 11/25 | Loss: 0.00127263
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012726257555186749, 0.0012726257555186749, 0.0012726257555186749, 0.0012726257555186749, 0.0012726257555186749]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012726257555186749

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.47567701
Iteration 2/25 | Loss: 0.00085040
Iteration 3/25 | Loss: 0.00085040
Iteration 4/25 | Loss: 0.00085040
Iteration 5/25 | Loss: 0.00085040
Iteration 6/25 | Loss: 0.00085040
Iteration 7/25 | Loss: 0.00085040
Iteration 8/25 | Loss: 0.00085040
Iteration 9/25 | Loss: 0.00085040
Iteration 10/25 | Loss: 0.00085040
Iteration 11/25 | Loss: 0.00085040
Iteration 12/25 | Loss: 0.00085040
Iteration 13/25 | Loss: 0.00085040
Iteration 14/25 | Loss: 0.00085040
Iteration 15/25 | Loss: 0.00085040
Iteration 16/25 | Loss: 0.00085039
Iteration 17/25 | Loss: 0.00085040
Iteration 18/25 | Loss: 0.00085040
Iteration 19/25 | Loss: 0.00085039
Iteration 20/25 | Loss: 0.00085039
Iteration 21/25 | Loss: 0.00085039
Iteration 22/25 | Loss: 0.00085039
Iteration 23/25 | Loss: 0.00085039
Iteration 24/25 | Loss: 0.00085039
Iteration 25/25 | Loss: 0.00085039

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00085039
Iteration 2/1000 | Loss: 0.00003951
Iteration 3/1000 | Loss: 0.00002447
Iteration 4/1000 | Loss: 0.00002130
Iteration 5/1000 | Loss: 0.00001966
Iteration 6/1000 | Loss: 0.00001862
Iteration 7/1000 | Loss: 0.00001806
Iteration 8/1000 | Loss: 0.00001771
Iteration 9/1000 | Loss: 0.00001751
Iteration 10/1000 | Loss: 0.00001737
Iteration 11/1000 | Loss: 0.00001720
Iteration 12/1000 | Loss: 0.00001714
Iteration 13/1000 | Loss: 0.00001703
Iteration 14/1000 | Loss: 0.00001698
Iteration 15/1000 | Loss: 0.00001695
Iteration 16/1000 | Loss: 0.00001694
Iteration 17/1000 | Loss: 0.00001694
Iteration 18/1000 | Loss: 0.00001694
Iteration 19/1000 | Loss: 0.00001693
Iteration 20/1000 | Loss: 0.00001692
Iteration 21/1000 | Loss: 0.00001692
Iteration 22/1000 | Loss: 0.00001691
Iteration 23/1000 | Loss: 0.00001688
Iteration 24/1000 | Loss: 0.00001688
Iteration 25/1000 | Loss: 0.00001687
Iteration 26/1000 | Loss: 0.00001687
Iteration 27/1000 | Loss: 0.00001686
Iteration 28/1000 | Loss: 0.00001685
Iteration 29/1000 | Loss: 0.00001683
Iteration 30/1000 | Loss: 0.00001682
Iteration 31/1000 | Loss: 0.00001682
Iteration 32/1000 | Loss: 0.00001682
Iteration 33/1000 | Loss: 0.00001682
Iteration 34/1000 | Loss: 0.00001682
Iteration 35/1000 | Loss: 0.00001682
Iteration 36/1000 | Loss: 0.00001681
Iteration 37/1000 | Loss: 0.00001680
Iteration 38/1000 | Loss: 0.00001679
Iteration 39/1000 | Loss: 0.00001679
Iteration 40/1000 | Loss: 0.00001679
Iteration 41/1000 | Loss: 0.00001678
Iteration 42/1000 | Loss: 0.00001678
Iteration 43/1000 | Loss: 0.00001677
Iteration 44/1000 | Loss: 0.00001677
Iteration 45/1000 | Loss: 0.00001677
Iteration 46/1000 | Loss: 0.00001677
Iteration 47/1000 | Loss: 0.00001676
Iteration 48/1000 | Loss: 0.00001676
Iteration 49/1000 | Loss: 0.00001676
Iteration 50/1000 | Loss: 0.00001676
Iteration 51/1000 | Loss: 0.00001676
Iteration 52/1000 | Loss: 0.00001676
Iteration 53/1000 | Loss: 0.00001675
Iteration 54/1000 | Loss: 0.00001675
Iteration 55/1000 | Loss: 0.00001675
Iteration 56/1000 | Loss: 0.00001674
Iteration 57/1000 | Loss: 0.00001674
Iteration 58/1000 | Loss: 0.00001673
Iteration 59/1000 | Loss: 0.00001673
Iteration 60/1000 | Loss: 0.00001673
Iteration 61/1000 | Loss: 0.00001673
Iteration 62/1000 | Loss: 0.00001672
Iteration 63/1000 | Loss: 0.00001672
Iteration 64/1000 | Loss: 0.00001672
Iteration 65/1000 | Loss: 0.00001672
Iteration 66/1000 | Loss: 0.00001672
Iteration 67/1000 | Loss: 0.00001671
Iteration 68/1000 | Loss: 0.00001671
Iteration 69/1000 | Loss: 0.00001671
Iteration 70/1000 | Loss: 0.00001670
Iteration 71/1000 | Loss: 0.00001670
Iteration 72/1000 | Loss: 0.00001670
Iteration 73/1000 | Loss: 0.00001670
Iteration 74/1000 | Loss: 0.00001670
Iteration 75/1000 | Loss: 0.00001669
Iteration 76/1000 | Loss: 0.00001669
Iteration 77/1000 | Loss: 0.00001669
Iteration 78/1000 | Loss: 0.00001669
Iteration 79/1000 | Loss: 0.00001669
Iteration 80/1000 | Loss: 0.00001668
Iteration 81/1000 | Loss: 0.00001668
Iteration 82/1000 | Loss: 0.00001668
Iteration 83/1000 | Loss: 0.00001668
Iteration 84/1000 | Loss: 0.00001668
Iteration 85/1000 | Loss: 0.00001667
Iteration 86/1000 | Loss: 0.00001667
Iteration 87/1000 | Loss: 0.00001667
Iteration 88/1000 | Loss: 0.00001666
Iteration 89/1000 | Loss: 0.00001666
Iteration 90/1000 | Loss: 0.00001666
Iteration 91/1000 | Loss: 0.00001666
Iteration 92/1000 | Loss: 0.00001666
Iteration 93/1000 | Loss: 0.00001665
Iteration 94/1000 | Loss: 0.00001665
Iteration 95/1000 | Loss: 0.00001665
Iteration 96/1000 | Loss: 0.00001665
Iteration 97/1000 | Loss: 0.00001665
Iteration 98/1000 | Loss: 0.00001665
Iteration 99/1000 | Loss: 0.00001665
Iteration 100/1000 | Loss: 0.00001665
Iteration 101/1000 | Loss: 0.00001665
Iteration 102/1000 | Loss: 0.00001664
Iteration 103/1000 | Loss: 0.00001664
Iteration 104/1000 | Loss: 0.00001664
Iteration 105/1000 | Loss: 0.00001663
Iteration 106/1000 | Loss: 0.00001663
Iteration 107/1000 | Loss: 0.00001663
Iteration 108/1000 | Loss: 0.00001663
Iteration 109/1000 | Loss: 0.00001663
Iteration 110/1000 | Loss: 0.00001663
Iteration 111/1000 | Loss: 0.00001662
Iteration 112/1000 | Loss: 0.00001662
Iteration 113/1000 | Loss: 0.00001662
Iteration 114/1000 | Loss: 0.00001662
Iteration 115/1000 | Loss: 0.00001661
Iteration 116/1000 | Loss: 0.00001661
Iteration 117/1000 | Loss: 0.00001660
Iteration 118/1000 | Loss: 0.00001660
Iteration 119/1000 | Loss: 0.00001660
Iteration 120/1000 | Loss: 0.00001660
Iteration 121/1000 | Loss: 0.00001660
Iteration 122/1000 | Loss: 0.00001660
Iteration 123/1000 | Loss: 0.00001659
Iteration 124/1000 | Loss: 0.00001659
Iteration 125/1000 | Loss: 0.00001659
Iteration 126/1000 | Loss: 0.00001659
Iteration 127/1000 | Loss: 0.00001659
Iteration 128/1000 | Loss: 0.00001659
Iteration 129/1000 | Loss: 0.00001658
Iteration 130/1000 | Loss: 0.00001658
Iteration 131/1000 | Loss: 0.00001658
Iteration 132/1000 | Loss: 0.00001658
Iteration 133/1000 | Loss: 0.00001658
Iteration 134/1000 | Loss: 0.00001658
Iteration 135/1000 | Loss: 0.00001658
Iteration 136/1000 | Loss: 0.00001658
Iteration 137/1000 | Loss: 0.00001658
Iteration 138/1000 | Loss: 0.00001658
Iteration 139/1000 | Loss: 0.00001658
Iteration 140/1000 | Loss: 0.00001657
Iteration 141/1000 | Loss: 0.00001657
Iteration 142/1000 | Loss: 0.00001657
Iteration 143/1000 | Loss: 0.00001657
Iteration 144/1000 | Loss: 0.00001657
Iteration 145/1000 | Loss: 0.00001657
Iteration 146/1000 | Loss: 0.00001656
Iteration 147/1000 | Loss: 0.00001656
Iteration 148/1000 | Loss: 0.00001656
Iteration 149/1000 | Loss: 0.00001656
Iteration 150/1000 | Loss: 0.00001656
Iteration 151/1000 | Loss: 0.00001656
Iteration 152/1000 | Loss: 0.00001656
Iteration 153/1000 | Loss: 0.00001655
Iteration 154/1000 | Loss: 0.00001655
Iteration 155/1000 | Loss: 0.00001655
Iteration 156/1000 | Loss: 0.00001655
Iteration 157/1000 | Loss: 0.00001655
Iteration 158/1000 | Loss: 0.00001655
Iteration 159/1000 | Loss: 0.00001655
Iteration 160/1000 | Loss: 0.00001655
Iteration 161/1000 | Loss: 0.00001655
Iteration 162/1000 | Loss: 0.00001655
Iteration 163/1000 | Loss: 0.00001655
Iteration 164/1000 | Loss: 0.00001655
Iteration 165/1000 | Loss: 0.00001655
Iteration 166/1000 | Loss: 0.00001654
Iteration 167/1000 | Loss: 0.00001654
Iteration 168/1000 | Loss: 0.00001654
Iteration 169/1000 | Loss: 0.00001654
Iteration 170/1000 | Loss: 0.00001654
Iteration 171/1000 | Loss: 0.00001654
Iteration 172/1000 | Loss: 0.00001654
Iteration 173/1000 | Loss: 0.00001654
Iteration 174/1000 | Loss: 0.00001654
Iteration 175/1000 | Loss: 0.00001653
Iteration 176/1000 | Loss: 0.00001653
Iteration 177/1000 | Loss: 0.00001653
Iteration 178/1000 | Loss: 0.00001653
Iteration 179/1000 | Loss: 0.00001653
Iteration 180/1000 | Loss: 0.00001653
Iteration 181/1000 | Loss: 0.00001653
Iteration 182/1000 | Loss: 0.00001653
Iteration 183/1000 | Loss: 0.00001653
Iteration 184/1000 | Loss: 0.00001653
Iteration 185/1000 | Loss: 0.00001653
Iteration 186/1000 | Loss: 0.00001653
Iteration 187/1000 | Loss: 0.00001653
Iteration 188/1000 | Loss: 0.00001653
Iteration 189/1000 | Loss: 0.00001653
Iteration 190/1000 | Loss: 0.00001653
Iteration 191/1000 | Loss: 0.00001653
Iteration 192/1000 | Loss: 0.00001652
Iteration 193/1000 | Loss: 0.00001652
Iteration 194/1000 | Loss: 0.00001652
Iteration 195/1000 | Loss: 0.00001652
Iteration 196/1000 | Loss: 0.00001652
Iteration 197/1000 | Loss: 0.00001652
Iteration 198/1000 | Loss: 0.00001652
Iteration 199/1000 | Loss: 0.00001652
Iteration 200/1000 | Loss: 0.00001652
Iteration 201/1000 | Loss: 0.00001652
Iteration 202/1000 | Loss: 0.00001652
Iteration 203/1000 | Loss: 0.00001652
Iteration 204/1000 | Loss: 0.00001652
Iteration 205/1000 | Loss: 0.00001652
Iteration 206/1000 | Loss: 0.00001652
Iteration 207/1000 | Loss: 0.00001652
Iteration 208/1000 | Loss: 0.00001652
Iteration 209/1000 | Loss: 0.00001651
Iteration 210/1000 | Loss: 0.00001651
Iteration 211/1000 | Loss: 0.00001651
Iteration 212/1000 | Loss: 0.00001651
Iteration 213/1000 | Loss: 0.00001651
Iteration 214/1000 | Loss: 0.00001651
Iteration 215/1000 | Loss: 0.00001651
Iteration 216/1000 | Loss: 0.00001651
Iteration 217/1000 | Loss: 0.00001651
Iteration 218/1000 | Loss: 0.00001651
Iteration 219/1000 | Loss: 0.00001651
Iteration 220/1000 | Loss: 0.00001651
Iteration 221/1000 | Loss: 0.00001651
Iteration 222/1000 | Loss: 0.00001651
Iteration 223/1000 | Loss: 0.00001651
Iteration 224/1000 | Loss: 0.00001650
Iteration 225/1000 | Loss: 0.00001650
Iteration 226/1000 | Loss: 0.00001650
Iteration 227/1000 | Loss: 0.00001650
Iteration 228/1000 | Loss: 0.00001650
Iteration 229/1000 | Loss: 0.00001650
Iteration 230/1000 | Loss: 0.00001650
Iteration 231/1000 | Loss: 0.00001650
Iteration 232/1000 | Loss: 0.00001650
Iteration 233/1000 | Loss: 0.00001650
Iteration 234/1000 | Loss: 0.00001650
Iteration 235/1000 | Loss: 0.00001650
Iteration 236/1000 | Loss: 0.00001650
Iteration 237/1000 | Loss: 0.00001650
Iteration 238/1000 | Loss: 0.00001650
Iteration 239/1000 | Loss: 0.00001650
Iteration 240/1000 | Loss: 0.00001650
Iteration 241/1000 | Loss: 0.00001649
Iteration 242/1000 | Loss: 0.00001649
Iteration 243/1000 | Loss: 0.00001649
Iteration 244/1000 | Loss: 0.00001649
Iteration 245/1000 | Loss: 0.00001649
Iteration 246/1000 | Loss: 0.00001649
Iteration 247/1000 | Loss: 0.00001649
Iteration 248/1000 | Loss: 0.00001649
Iteration 249/1000 | Loss: 0.00001649
Iteration 250/1000 | Loss: 0.00001649
Iteration 251/1000 | Loss: 0.00001649
Iteration 252/1000 | Loss: 0.00001649
Iteration 253/1000 | Loss: 0.00001649
Iteration 254/1000 | Loss: 0.00001649
Iteration 255/1000 | Loss: 0.00001649
Iteration 256/1000 | Loss: 0.00001649
Iteration 257/1000 | Loss: 0.00001649
Iteration 258/1000 | Loss: 0.00001649
Iteration 259/1000 | Loss: 0.00001649
Iteration 260/1000 | Loss: 0.00001649
Iteration 261/1000 | Loss: 0.00001648
Iteration 262/1000 | Loss: 0.00001648
Iteration 263/1000 | Loss: 0.00001648
Iteration 264/1000 | Loss: 0.00001648
Iteration 265/1000 | Loss: 0.00001648
Iteration 266/1000 | Loss: 0.00001648
Iteration 267/1000 | Loss: 0.00001648
Iteration 268/1000 | Loss: 0.00001648
Iteration 269/1000 | Loss: 0.00001648
Iteration 270/1000 | Loss: 0.00001648
Iteration 271/1000 | Loss: 0.00001648
Iteration 272/1000 | Loss: 0.00001648
Iteration 273/1000 | Loss: 0.00001648
Iteration 274/1000 | Loss: 0.00001648
Iteration 275/1000 | Loss: 0.00001648
Iteration 276/1000 | Loss: 0.00001648
Iteration 277/1000 | Loss: 0.00001648
Iteration 278/1000 | Loss: 0.00001648
Iteration 279/1000 | Loss: 0.00001648
Iteration 280/1000 | Loss: 0.00001647
Iteration 281/1000 | Loss: 0.00001647
Iteration 282/1000 | Loss: 0.00001647
Iteration 283/1000 | Loss: 0.00001647
Iteration 284/1000 | Loss: 0.00001647
Iteration 285/1000 | Loss: 0.00001647
Iteration 286/1000 | Loss: 0.00001647
Iteration 287/1000 | Loss: 0.00001647
Iteration 288/1000 | Loss: 0.00001647
Iteration 289/1000 | Loss: 0.00001647
Iteration 290/1000 | Loss: 0.00001647
Iteration 291/1000 | Loss: 0.00001647
Iteration 292/1000 | Loss: 0.00001647
Iteration 293/1000 | Loss: 0.00001647
Iteration 294/1000 | Loss: 0.00001647
Iteration 295/1000 | Loss: 0.00001647
Iteration 296/1000 | Loss: 0.00001647
Iteration 297/1000 | Loss: 0.00001647
Iteration 298/1000 | Loss: 0.00001647
Iteration 299/1000 | Loss: 0.00001647
Iteration 300/1000 | Loss: 0.00001647
Iteration 301/1000 | Loss: 0.00001647
Iteration 302/1000 | Loss: 0.00001647
Iteration 303/1000 | Loss: 0.00001647
Iteration 304/1000 | Loss: 0.00001647
Iteration 305/1000 | Loss: 0.00001647
Iteration 306/1000 | Loss: 0.00001647
Iteration 307/1000 | Loss: 0.00001647
Iteration 308/1000 | Loss: 0.00001647
Iteration 309/1000 | Loss: 0.00001647
Iteration 310/1000 | Loss: 0.00001647
Iteration 311/1000 | Loss: 0.00001647
Iteration 312/1000 | Loss: 0.00001647
Iteration 313/1000 | Loss: 0.00001647
Iteration 314/1000 | Loss: 0.00001647
Iteration 315/1000 | Loss: 0.00001647
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 315. Stopping optimization.
Last 5 losses: [1.6471654816996306e-05, 1.6471654816996306e-05, 1.6471654816996306e-05, 1.6471654816996306e-05, 1.6471654816996306e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6471654816996306e-05

Optimization complete. Final v2v error: 3.342855930328369 mm

Highest mean error: 3.7854530811309814 mm for frame 87

Lowest mean error: 2.8487446308135986 mm for frame 16

Saving results

Total time: 45.86418533325195
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alison_posed_001/1053/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alison_posed_001/1053.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alison_posed_001/1053
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00712631
Iteration 2/25 | Loss: 0.00172796
Iteration 3/25 | Loss: 0.00146285
Iteration 4/25 | Loss: 0.00143109
Iteration 5/25 | Loss: 0.00142464
Iteration 6/25 | Loss: 0.00142413
Iteration 7/25 | Loss: 0.00142413
Iteration 8/25 | Loss: 0.00142413
Iteration 9/25 | Loss: 0.00142413
Iteration 10/25 | Loss: 0.00142413
Iteration 11/25 | Loss: 0.00142413
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0014241317985579371, 0.0014241317985579371, 0.0014241317985579371, 0.0014241317985579371, 0.0014241317985579371]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014241317985579371

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.55352318
Iteration 2/25 | Loss: 0.00105699
Iteration 3/25 | Loss: 0.00105699
Iteration 4/25 | Loss: 0.00105698
Iteration 5/25 | Loss: 0.00105698
Iteration 6/25 | Loss: 0.00105698
Iteration 7/25 | Loss: 0.00105698
Iteration 8/25 | Loss: 0.00105698
Iteration 9/25 | Loss: 0.00105698
Iteration 10/25 | Loss: 0.00105698
Iteration 11/25 | Loss: 0.00105698
Iteration 12/25 | Loss: 0.00105698
Iteration 13/25 | Loss: 0.00105698
Iteration 14/25 | Loss: 0.00105698
Iteration 15/25 | Loss: 0.00105698
Iteration 16/25 | Loss: 0.00105698
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0010569830192252994, 0.0010569830192252994, 0.0010569830192252994, 0.0010569830192252994, 0.0010569830192252994]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010569830192252994

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00105698
Iteration 2/1000 | Loss: 0.00006728
Iteration 3/1000 | Loss: 0.00004143
Iteration 4/1000 | Loss: 0.00003533
Iteration 5/1000 | Loss: 0.00003355
Iteration 6/1000 | Loss: 0.00003216
Iteration 7/1000 | Loss: 0.00003098
Iteration 8/1000 | Loss: 0.00003042
Iteration 9/1000 | Loss: 0.00002989
Iteration 10/1000 | Loss: 0.00002949
Iteration 11/1000 | Loss: 0.00002917
Iteration 12/1000 | Loss: 0.00002889
Iteration 13/1000 | Loss: 0.00002875
Iteration 14/1000 | Loss: 0.00002869
Iteration 15/1000 | Loss: 0.00002864
Iteration 16/1000 | Loss: 0.00002861
Iteration 17/1000 | Loss: 0.00002859
Iteration 18/1000 | Loss: 0.00002858
Iteration 19/1000 | Loss: 0.00002858
Iteration 20/1000 | Loss: 0.00002858
Iteration 21/1000 | Loss: 0.00002857
Iteration 22/1000 | Loss: 0.00002857
Iteration 23/1000 | Loss: 0.00002856
Iteration 24/1000 | Loss: 0.00002856
Iteration 25/1000 | Loss: 0.00002856
Iteration 26/1000 | Loss: 0.00002855
Iteration 27/1000 | Loss: 0.00002855
Iteration 28/1000 | Loss: 0.00002855
Iteration 29/1000 | Loss: 0.00002855
Iteration 30/1000 | Loss: 0.00002854
Iteration 31/1000 | Loss: 0.00002854
Iteration 32/1000 | Loss: 0.00002854
Iteration 33/1000 | Loss: 0.00002853
Iteration 34/1000 | Loss: 0.00002853
Iteration 35/1000 | Loss: 0.00002853
Iteration 36/1000 | Loss: 0.00002853
Iteration 37/1000 | Loss: 0.00002853
Iteration 38/1000 | Loss: 0.00002853
Iteration 39/1000 | Loss: 0.00002852
Iteration 40/1000 | Loss: 0.00002852
Iteration 41/1000 | Loss: 0.00002852
Iteration 42/1000 | Loss: 0.00002852
Iteration 43/1000 | Loss: 0.00002852
Iteration 44/1000 | Loss: 0.00002852
Iteration 45/1000 | Loss: 0.00002852
Iteration 46/1000 | Loss: 0.00002852
Iteration 47/1000 | Loss: 0.00002852
Iteration 48/1000 | Loss: 0.00002852
Iteration 49/1000 | Loss: 0.00002852
Iteration 50/1000 | Loss: 0.00002852
Iteration 51/1000 | Loss: 0.00002852
Iteration 52/1000 | Loss: 0.00002852
Iteration 53/1000 | Loss: 0.00002851
Iteration 54/1000 | Loss: 0.00002851
Iteration 55/1000 | Loss: 0.00002851
Iteration 56/1000 | Loss: 0.00002851
Iteration 57/1000 | Loss: 0.00002850
Iteration 58/1000 | Loss: 0.00002850
Iteration 59/1000 | Loss: 0.00002850
Iteration 60/1000 | Loss: 0.00002850
Iteration 61/1000 | Loss: 0.00002850
Iteration 62/1000 | Loss: 0.00002850
Iteration 63/1000 | Loss: 0.00002850
Iteration 64/1000 | Loss: 0.00002850
Iteration 65/1000 | Loss: 0.00002850
Iteration 66/1000 | Loss: 0.00002850
Iteration 67/1000 | Loss: 0.00002850
Iteration 68/1000 | Loss: 0.00002850
Iteration 69/1000 | Loss: 0.00002850
Iteration 70/1000 | Loss: 0.00002850
Iteration 71/1000 | Loss: 0.00002850
Iteration 72/1000 | Loss: 0.00002850
Iteration 73/1000 | Loss: 0.00002850
Iteration 74/1000 | Loss: 0.00002850
Iteration 75/1000 | Loss: 0.00002850
Iteration 76/1000 | Loss: 0.00002850
Iteration 77/1000 | Loss: 0.00002850
Iteration 78/1000 | Loss: 0.00002850
Iteration 79/1000 | Loss: 0.00002850
Iteration 80/1000 | Loss: 0.00002850
Iteration 81/1000 | Loss: 0.00002850
Iteration 82/1000 | Loss: 0.00002850
Iteration 83/1000 | Loss: 0.00002850
Iteration 84/1000 | Loss: 0.00002850
Iteration 85/1000 | Loss: 0.00002850
Iteration 86/1000 | Loss: 0.00002850
Iteration 87/1000 | Loss: 0.00002850
Iteration 88/1000 | Loss: 0.00002850
Iteration 89/1000 | Loss: 0.00002850
Iteration 90/1000 | Loss: 0.00002850
Iteration 91/1000 | Loss: 0.00002850
Iteration 92/1000 | Loss: 0.00002850
Iteration 93/1000 | Loss: 0.00002850
Iteration 94/1000 | Loss: 0.00002850
Iteration 95/1000 | Loss: 0.00002850
Iteration 96/1000 | Loss: 0.00002850
Iteration 97/1000 | Loss: 0.00002850
Iteration 98/1000 | Loss: 0.00002850
Iteration 99/1000 | Loss: 0.00002850
Iteration 100/1000 | Loss: 0.00002850
Iteration 101/1000 | Loss: 0.00002850
Iteration 102/1000 | Loss: 0.00002850
Iteration 103/1000 | Loss: 0.00002850
Iteration 104/1000 | Loss: 0.00002850
Iteration 105/1000 | Loss: 0.00002850
Iteration 106/1000 | Loss: 0.00002850
Iteration 107/1000 | Loss: 0.00002850
Iteration 108/1000 | Loss: 0.00002850
Iteration 109/1000 | Loss: 0.00002850
Iteration 110/1000 | Loss: 0.00002850
Iteration 111/1000 | Loss: 0.00002850
Iteration 112/1000 | Loss: 0.00002850
Iteration 113/1000 | Loss: 0.00002850
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 113. Stopping optimization.
Last 5 losses: [2.84968082269188e-05, 2.84968082269188e-05, 2.84968082269188e-05, 2.84968082269188e-05, 2.84968082269188e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.84968082269188e-05

Optimization complete. Final v2v error: 4.442063331604004 mm

Highest mean error: 5.10113000869751 mm for frame 51

Lowest mean error: 3.964855909347534 mm for frame 171

Saving results

Total time: 36.36092519760132
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alison_posed_001/1037/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alison_posed_001/1037.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alison_posed_001/1037
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00482591
Iteration 2/25 | Loss: 0.00132161
Iteration 3/25 | Loss: 0.00125200
Iteration 4/25 | Loss: 0.00123929
Iteration 5/25 | Loss: 0.00123454
Iteration 6/25 | Loss: 0.00123359
Iteration 7/25 | Loss: 0.00123354
Iteration 8/25 | Loss: 0.00123354
Iteration 9/25 | Loss: 0.00123354
Iteration 10/25 | Loss: 0.00123354
Iteration 11/25 | Loss: 0.00123354
Iteration 12/25 | Loss: 0.00123354
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0012335428036749363, 0.0012335428036749363, 0.0012335428036749363, 0.0012335428036749363, 0.0012335428036749363]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012335428036749363

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.41263342
Iteration 2/25 | Loss: 0.00079459
Iteration 3/25 | Loss: 0.00079459
Iteration 4/25 | Loss: 0.00079458
Iteration 5/25 | Loss: 0.00079458
Iteration 6/25 | Loss: 0.00079458
Iteration 7/25 | Loss: 0.00079458
Iteration 8/25 | Loss: 0.00079458
Iteration 9/25 | Loss: 0.00079458
Iteration 10/25 | Loss: 0.00079458
Iteration 11/25 | Loss: 0.00079458
Iteration 12/25 | Loss: 0.00079458
Iteration 13/25 | Loss: 0.00079458
Iteration 14/25 | Loss: 0.00079458
Iteration 15/25 | Loss: 0.00079458
Iteration 16/25 | Loss: 0.00079458
Iteration 17/25 | Loss: 0.00079458
Iteration 18/25 | Loss: 0.00079458
Iteration 19/25 | Loss: 0.00079458
Iteration 20/25 | Loss: 0.00079458
Iteration 21/25 | Loss: 0.00079458
Iteration 22/25 | Loss: 0.00079458
Iteration 23/25 | Loss: 0.00079458
Iteration 24/25 | Loss: 0.00079458
Iteration 25/25 | Loss: 0.00079458

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00079458
Iteration 2/1000 | Loss: 0.00003347
Iteration 3/1000 | Loss: 0.00002166
Iteration 4/1000 | Loss: 0.00001847
Iteration 5/1000 | Loss: 0.00001751
Iteration 6/1000 | Loss: 0.00001678
Iteration 7/1000 | Loss: 0.00001626
Iteration 8/1000 | Loss: 0.00001583
Iteration 9/1000 | Loss: 0.00001569
Iteration 10/1000 | Loss: 0.00001549
Iteration 11/1000 | Loss: 0.00001549
Iteration 12/1000 | Loss: 0.00001538
Iteration 13/1000 | Loss: 0.00001535
Iteration 14/1000 | Loss: 0.00001532
Iteration 15/1000 | Loss: 0.00001530
Iteration 16/1000 | Loss: 0.00001529
Iteration 17/1000 | Loss: 0.00001521
Iteration 18/1000 | Loss: 0.00001519
Iteration 19/1000 | Loss: 0.00001516
Iteration 20/1000 | Loss: 0.00001515
Iteration 21/1000 | Loss: 0.00001515
Iteration 22/1000 | Loss: 0.00001514
Iteration 23/1000 | Loss: 0.00001514
Iteration 24/1000 | Loss: 0.00001511
Iteration 25/1000 | Loss: 0.00001508
Iteration 26/1000 | Loss: 0.00001508
Iteration 27/1000 | Loss: 0.00001503
Iteration 28/1000 | Loss: 0.00001502
Iteration 29/1000 | Loss: 0.00001501
Iteration 30/1000 | Loss: 0.00001500
Iteration 31/1000 | Loss: 0.00001500
Iteration 32/1000 | Loss: 0.00001500
Iteration 33/1000 | Loss: 0.00001500
Iteration 34/1000 | Loss: 0.00001499
Iteration 35/1000 | Loss: 0.00001498
Iteration 36/1000 | Loss: 0.00001495
Iteration 37/1000 | Loss: 0.00001489
Iteration 38/1000 | Loss: 0.00001489
Iteration 39/1000 | Loss: 0.00001487
Iteration 40/1000 | Loss: 0.00001487
Iteration 41/1000 | Loss: 0.00001486
Iteration 42/1000 | Loss: 0.00001486
Iteration 43/1000 | Loss: 0.00001486
Iteration 44/1000 | Loss: 0.00001486
Iteration 45/1000 | Loss: 0.00001486
Iteration 46/1000 | Loss: 0.00001486
Iteration 47/1000 | Loss: 0.00001486
Iteration 48/1000 | Loss: 0.00001486
Iteration 49/1000 | Loss: 0.00001486
Iteration 50/1000 | Loss: 0.00001486
Iteration 51/1000 | Loss: 0.00001485
Iteration 52/1000 | Loss: 0.00001485
Iteration 53/1000 | Loss: 0.00001485
Iteration 54/1000 | Loss: 0.00001485
Iteration 55/1000 | Loss: 0.00001485
Iteration 56/1000 | Loss: 0.00001484
Iteration 57/1000 | Loss: 0.00001483
Iteration 58/1000 | Loss: 0.00001482
Iteration 59/1000 | Loss: 0.00001482
Iteration 60/1000 | Loss: 0.00001482
Iteration 61/1000 | Loss: 0.00001482
Iteration 62/1000 | Loss: 0.00001482
Iteration 63/1000 | Loss: 0.00001482
Iteration 64/1000 | Loss: 0.00001481
Iteration 65/1000 | Loss: 0.00001481
Iteration 66/1000 | Loss: 0.00001479
Iteration 67/1000 | Loss: 0.00001478
Iteration 68/1000 | Loss: 0.00001478
Iteration 69/1000 | Loss: 0.00001478
Iteration 70/1000 | Loss: 0.00001478
Iteration 71/1000 | Loss: 0.00001477
Iteration 72/1000 | Loss: 0.00001477
Iteration 73/1000 | Loss: 0.00001477
Iteration 74/1000 | Loss: 0.00001477
Iteration 75/1000 | Loss: 0.00001477
Iteration 76/1000 | Loss: 0.00001476
Iteration 77/1000 | Loss: 0.00001476
Iteration 78/1000 | Loss: 0.00001476
Iteration 79/1000 | Loss: 0.00001476
Iteration 80/1000 | Loss: 0.00001476
Iteration 81/1000 | Loss: 0.00001475
Iteration 82/1000 | Loss: 0.00001475
Iteration 83/1000 | Loss: 0.00001475
Iteration 84/1000 | Loss: 0.00001475
Iteration 85/1000 | Loss: 0.00001473
Iteration 86/1000 | Loss: 0.00001473
Iteration 87/1000 | Loss: 0.00001473
Iteration 88/1000 | Loss: 0.00001472
Iteration 89/1000 | Loss: 0.00001472
Iteration 90/1000 | Loss: 0.00001470
Iteration 91/1000 | Loss: 0.00001469
Iteration 92/1000 | Loss: 0.00001469
Iteration 93/1000 | Loss: 0.00001469
Iteration 94/1000 | Loss: 0.00001469
Iteration 95/1000 | Loss: 0.00001468
Iteration 96/1000 | Loss: 0.00001468
Iteration 97/1000 | Loss: 0.00001467
Iteration 98/1000 | Loss: 0.00001467
Iteration 99/1000 | Loss: 0.00001466
Iteration 100/1000 | Loss: 0.00001466
Iteration 101/1000 | Loss: 0.00001466
Iteration 102/1000 | Loss: 0.00001466
Iteration 103/1000 | Loss: 0.00001466
Iteration 104/1000 | Loss: 0.00001466
Iteration 105/1000 | Loss: 0.00001466
Iteration 106/1000 | Loss: 0.00001466
Iteration 107/1000 | Loss: 0.00001466
Iteration 108/1000 | Loss: 0.00001465
Iteration 109/1000 | Loss: 0.00001465
Iteration 110/1000 | Loss: 0.00001465
Iteration 111/1000 | Loss: 0.00001465
Iteration 112/1000 | Loss: 0.00001465
Iteration 113/1000 | Loss: 0.00001465
Iteration 114/1000 | Loss: 0.00001465
Iteration 115/1000 | Loss: 0.00001464
Iteration 116/1000 | Loss: 0.00001464
Iteration 117/1000 | Loss: 0.00001463
Iteration 118/1000 | Loss: 0.00001463
Iteration 119/1000 | Loss: 0.00001463
Iteration 120/1000 | Loss: 0.00001462
Iteration 121/1000 | Loss: 0.00001462
Iteration 122/1000 | Loss: 0.00001461
Iteration 123/1000 | Loss: 0.00001461
Iteration 124/1000 | Loss: 0.00001461
Iteration 125/1000 | Loss: 0.00001461
Iteration 126/1000 | Loss: 0.00001460
Iteration 127/1000 | Loss: 0.00001460
Iteration 128/1000 | Loss: 0.00001460
Iteration 129/1000 | Loss: 0.00001460
Iteration 130/1000 | Loss: 0.00001460
Iteration 131/1000 | Loss: 0.00001460
Iteration 132/1000 | Loss: 0.00001459
Iteration 133/1000 | Loss: 0.00001459
Iteration 134/1000 | Loss: 0.00001459
Iteration 135/1000 | Loss: 0.00001459
Iteration 136/1000 | Loss: 0.00001459
Iteration 137/1000 | Loss: 0.00001458
Iteration 138/1000 | Loss: 0.00001458
Iteration 139/1000 | Loss: 0.00001458
Iteration 140/1000 | Loss: 0.00001458
Iteration 141/1000 | Loss: 0.00001458
Iteration 142/1000 | Loss: 0.00001457
Iteration 143/1000 | Loss: 0.00001457
Iteration 144/1000 | Loss: 0.00001457
Iteration 145/1000 | Loss: 0.00001457
Iteration 146/1000 | Loss: 0.00001457
Iteration 147/1000 | Loss: 0.00001457
Iteration 148/1000 | Loss: 0.00001457
Iteration 149/1000 | Loss: 0.00001457
Iteration 150/1000 | Loss: 0.00001457
Iteration 151/1000 | Loss: 0.00001457
Iteration 152/1000 | Loss: 0.00001457
Iteration 153/1000 | Loss: 0.00001457
Iteration 154/1000 | Loss: 0.00001457
Iteration 155/1000 | Loss: 0.00001457
Iteration 156/1000 | Loss: 0.00001456
Iteration 157/1000 | Loss: 0.00001456
Iteration 158/1000 | Loss: 0.00001456
Iteration 159/1000 | Loss: 0.00001456
Iteration 160/1000 | Loss: 0.00001456
Iteration 161/1000 | Loss: 0.00001456
Iteration 162/1000 | Loss: 0.00001455
Iteration 163/1000 | Loss: 0.00001455
Iteration 164/1000 | Loss: 0.00001455
Iteration 165/1000 | Loss: 0.00001455
Iteration 166/1000 | Loss: 0.00001455
Iteration 167/1000 | Loss: 0.00001455
Iteration 168/1000 | Loss: 0.00001455
Iteration 169/1000 | Loss: 0.00001455
Iteration 170/1000 | Loss: 0.00001455
Iteration 171/1000 | Loss: 0.00001455
Iteration 172/1000 | Loss: 0.00001455
Iteration 173/1000 | Loss: 0.00001455
Iteration 174/1000 | Loss: 0.00001455
Iteration 175/1000 | Loss: 0.00001455
Iteration 176/1000 | Loss: 0.00001455
Iteration 177/1000 | Loss: 0.00001455
Iteration 178/1000 | Loss: 0.00001455
Iteration 179/1000 | Loss: 0.00001455
Iteration 180/1000 | Loss: 0.00001455
Iteration 181/1000 | Loss: 0.00001455
Iteration 182/1000 | Loss: 0.00001455
Iteration 183/1000 | Loss: 0.00001455
Iteration 184/1000 | Loss: 0.00001455
Iteration 185/1000 | Loss: 0.00001455
Iteration 186/1000 | Loss: 0.00001455
Iteration 187/1000 | Loss: 0.00001455
Iteration 188/1000 | Loss: 0.00001455
Iteration 189/1000 | Loss: 0.00001455
Iteration 190/1000 | Loss: 0.00001455
Iteration 191/1000 | Loss: 0.00001455
Iteration 192/1000 | Loss: 0.00001455
Iteration 193/1000 | Loss: 0.00001455
Iteration 194/1000 | Loss: 0.00001455
Iteration 195/1000 | Loss: 0.00001455
Iteration 196/1000 | Loss: 0.00001455
Iteration 197/1000 | Loss: 0.00001455
Iteration 198/1000 | Loss: 0.00001455
Iteration 199/1000 | Loss: 0.00001455
Iteration 200/1000 | Loss: 0.00001455
Iteration 201/1000 | Loss: 0.00001455
Iteration 202/1000 | Loss: 0.00001455
Iteration 203/1000 | Loss: 0.00001455
Iteration 204/1000 | Loss: 0.00001455
Iteration 205/1000 | Loss: 0.00001455
Iteration 206/1000 | Loss: 0.00001455
Iteration 207/1000 | Loss: 0.00001455
Iteration 208/1000 | Loss: 0.00001455
Iteration 209/1000 | Loss: 0.00001455
Iteration 210/1000 | Loss: 0.00001455
Iteration 211/1000 | Loss: 0.00001455
Iteration 212/1000 | Loss: 0.00001455
Iteration 213/1000 | Loss: 0.00001455
Iteration 214/1000 | Loss: 0.00001455
Iteration 215/1000 | Loss: 0.00001455
Iteration 216/1000 | Loss: 0.00001455
Iteration 217/1000 | Loss: 0.00001455
Iteration 218/1000 | Loss: 0.00001455
Iteration 219/1000 | Loss: 0.00001455
Iteration 220/1000 | Loss: 0.00001455
Iteration 221/1000 | Loss: 0.00001455
Iteration 222/1000 | Loss: 0.00001455
Iteration 223/1000 | Loss: 0.00001455
Iteration 224/1000 | Loss: 0.00001455
Iteration 225/1000 | Loss: 0.00001455
Iteration 226/1000 | Loss: 0.00001455
Iteration 227/1000 | Loss: 0.00001455
Iteration 228/1000 | Loss: 0.00001455
Iteration 229/1000 | Loss: 0.00001455
Iteration 230/1000 | Loss: 0.00001455
Iteration 231/1000 | Loss: 0.00001455
Iteration 232/1000 | Loss: 0.00001455
Iteration 233/1000 | Loss: 0.00001455
Iteration 234/1000 | Loss: 0.00001455
Iteration 235/1000 | Loss: 0.00001455
Iteration 236/1000 | Loss: 0.00001455
Iteration 237/1000 | Loss: 0.00001455
Iteration 238/1000 | Loss: 0.00001455
Iteration 239/1000 | Loss: 0.00001455
Iteration 240/1000 | Loss: 0.00001455
Iteration 241/1000 | Loss: 0.00001455
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 241. Stopping optimization.
Last 5 losses: [1.4545382327924017e-05, 1.4545382327924017e-05, 1.4545382327924017e-05, 1.4545382327924017e-05, 1.4545382327924017e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4545382327924017e-05

Optimization complete. Final v2v error: 3.267742156982422 mm

Highest mean error: 3.616509437561035 mm for frame 108

Lowest mean error: 3.094217538833618 mm for frame 6

Saving results

Total time: 41.073500871658325
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alison_posed_001/1051/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alison_posed_001/1051.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alison_posed_001/1051
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00838000
Iteration 2/25 | Loss: 0.00135981
Iteration 3/25 | Loss: 0.00124424
Iteration 4/25 | Loss: 0.00123357
Iteration 5/25 | Loss: 0.00123181
Iteration 6/25 | Loss: 0.00123146
Iteration 7/25 | Loss: 0.00123146
Iteration 8/25 | Loss: 0.00123146
Iteration 9/25 | Loss: 0.00123146
Iteration 10/25 | Loss: 0.00123146
Iteration 11/25 | Loss: 0.00123146
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012314561754465103, 0.0012314561754465103, 0.0012314561754465103, 0.0012314561754465103, 0.0012314561754465103]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012314561754465103

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45804441
Iteration 2/25 | Loss: 0.00073559
Iteration 3/25 | Loss: 0.00073556
Iteration 4/25 | Loss: 0.00073555
Iteration 5/25 | Loss: 0.00073555
Iteration 6/25 | Loss: 0.00073555
Iteration 7/25 | Loss: 0.00073555
Iteration 8/25 | Loss: 0.00073555
Iteration 9/25 | Loss: 0.00073555
Iteration 10/25 | Loss: 0.00073555
Iteration 11/25 | Loss: 0.00073555
Iteration 12/25 | Loss: 0.00073555
Iteration 13/25 | Loss: 0.00073555
Iteration 14/25 | Loss: 0.00073555
Iteration 15/25 | Loss: 0.00073555
Iteration 16/25 | Loss: 0.00073555
Iteration 17/25 | Loss: 0.00073555
Iteration 18/25 | Loss: 0.00073555
Iteration 19/25 | Loss: 0.00073555
Iteration 20/25 | Loss: 0.00073555
Iteration 21/25 | Loss: 0.00073555
Iteration 22/25 | Loss: 0.00073555
Iteration 23/25 | Loss: 0.00073555
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0007355516427196562, 0.0007355516427196562, 0.0007355516427196562, 0.0007355516427196562, 0.0007355516427196562]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007355516427196562

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00073555
Iteration 2/1000 | Loss: 0.00002410
Iteration 3/1000 | Loss: 0.00001635
Iteration 4/1000 | Loss: 0.00001450
Iteration 5/1000 | Loss: 0.00001345
Iteration 6/1000 | Loss: 0.00001277
Iteration 7/1000 | Loss: 0.00001226
Iteration 8/1000 | Loss: 0.00001191
Iteration 9/1000 | Loss: 0.00001181
Iteration 10/1000 | Loss: 0.00001176
Iteration 11/1000 | Loss: 0.00001171
Iteration 12/1000 | Loss: 0.00001170
Iteration 13/1000 | Loss: 0.00001169
Iteration 14/1000 | Loss: 0.00001158
Iteration 15/1000 | Loss: 0.00001156
Iteration 16/1000 | Loss: 0.00001152
Iteration 17/1000 | Loss: 0.00001151
Iteration 18/1000 | Loss: 0.00001142
Iteration 19/1000 | Loss: 0.00001140
Iteration 20/1000 | Loss: 0.00001139
Iteration 21/1000 | Loss: 0.00001136
Iteration 22/1000 | Loss: 0.00001135
Iteration 23/1000 | Loss: 0.00001134
Iteration 24/1000 | Loss: 0.00001133
Iteration 25/1000 | Loss: 0.00001133
Iteration 26/1000 | Loss: 0.00001131
Iteration 27/1000 | Loss: 0.00001130
Iteration 28/1000 | Loss: 0.00001130
Iteration 29/1000 | Loss: 0.00001130
Iteration 30/1000 | Loss: 0.00001129
Iteration 31/1000 | Loss: 0.00001129
Iteration 32/1000 | Loss: 0.00001128
Iteration 33/1000 | Loss: 0.00001128
Iteration 34/1000 | Loss: 0.00001127
Iteration 35/1000 | Loss: 0.00001127
Iteration 36/1000 | Loss: 0.00001126
Iteration 37/1000 | Loss: 0.00001124
Iteration 38/1000 | Loss: 0.00001124
Iteration 39/1000 | Loss: 0.00001124
Iteration 40/1000 | Loss: 0.00001123
Iteration 41/1000 | Loss: 0.00001123
Iteration 42/1000 | Loss: 0.00001123
Iteration 43/1000 | Loss: 0.00001122
Iteration 44/1000 | Loss: 0.00001122
Iteration 45/1000 | Loss: 0.00001122
Iteration 46/1000 | Loss: 0.00001121
Iteration 47/1000 | Loss: 0.00001120
Iteration 48/1000 | Loss: 0.00001120
Iteration 49/1000 | Loss: 0.00001120
Iteration 50/1000 | Loss: 0.00001119
Iteration 51/1000 | Loss: 0.00001119
Iteration 52/1000 | Loss: 0.00001119
Iteration 53/1000 | Loss: 0.00001119
Iteration 54/1000 | Loss: 0.00001119
Iteration 55/1000 | Loss: 0.00001119
Iteration 56/1000 | Loss: 0.00001119
Iteration 57/1000 | Loss: 0.00001119
Iteration 58/1000 | Loss: 0.00001119
Iteration 59/1000 | Loss: 0.00001119
Iteration 60/1000 | Loss: 0.00001119
Iteration 61/1000 | Loss: 0.00001119
Iteration 62/1000 | Loss: 0.00001119
Iteration 63/1000 | Loss: 0.00001119
Iteration 64/1000 | Loss: 0.00001119
Iteration 65/1000 | Loss: 0.00001119
Iteration 66/1000 | Loss: 0.00001119
Iteration 67/1000 | Loss: 0.00001119
Iteration 68/1000 | Loss: 0.00001119
Iteration 69/1000 | Loss: 0.00001119
Iteration 70/1000 | Loss: 0.00001119
Iteration 71/1000 | Loss: 0.00001119
Iteration 72/1000 | Loss: 0.00001119
Iteration 73/1000 | Loss: 0.00001119
Iteration 74/1000 | Loss: 0.00001119
Iteration 75/1000 | Loss: 0.00001119
Iteration 76/1000 | Loss: 0.00001119
Iteration 77/1000 | Loss: 0.00001119
Iteration 78/1000 | Loss: 0.00001119
Iteration 79/1000 | Loss: 0.00001119
Iteration 80/1000 | Loss: 0.00001119
Iteration 81/1000 | Loss: 0.00001119
Iteration 82/1000 | Loss: 0.00001119
Iteration 83/1000 | Loss: 0.00001119
Iteration 84/1000 | Loss: 0.00001119
Iteration 85/1000 | Loss: 0.00001119
Iteration 86/1000 | Loss: 0.00001119
Iteration 87/1000 | Loss: 0.00001119
Iteration 88/1000 | Loss: 0.00001119
Iteration 89/1000 | Loss: 0.00001119
Iteration 90/1000 | Loss: 0.00001119
Iteration 91/1000 | Loss: 0.00001119
Iteration 92/1000 | Loss: 0.00001119
Iteration 93/1000 | Loss: 0.00001119
Iteration 94/1000 | Loss: 0.00001119
Iteration 95/1000 | Loss: 0.00001119
Iteration 96/1000 | Loss: 0.00001119
Iteration 97/1000 | Loss: 0.00001119
Iteration 98/1000 | Loss: 0.00001119
Iteration 99/1000 | Loss: 0.00001119
Iteration 100/1000 | Loss: 0.00001119
Iteration 101/1000 | Loss: 0.00001119
Iteration 102/1000 | Loss: 0.00001119
Iteration 103/1000 | Loss: 0.00001119
Iteration 104/1000 | Loss: 0.00001119
Iteration 105/1000 | Loss: 0.00001119
Iteration 106/1000 | Loss: 0.00001119
Iteration 107/1000 | Loss: 0.00001119
Iteration 108/1000 | Loss: 0.00001119
Iteration 109/1000 | Loss: 0.00001119
Iteration 110/1000 | Loss: 0.00001119
Iteration 111/1000 | Loss: 0.00001119
Iteration 112/1000 | Loss: 0.00001119
Iteration 113/1000 | Loss: 0.00001119
Iteration 114/1000 | Loss: 0.00001119
Iteration 115/1000 | Loss: 0.00001119
Iteration 116/1000 | Loss: 0.00001119
Iteration 117/1000 | Loss: 0.00001119
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 117. Stopping optimization.
Last 5 losses: [1.1188978533027694e-05, 1.1188978533027694e-05, 1.1188978533027694e-05, 1.1188978533027694e-05, 1.1188978533027694e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1188978533027694e-05

Optimization complete. Final v2v error: 2.8563389778137207 mm

Highest mean error: 3.1598639488220215 mm for frame 4

Lowest mean error: 2.753030776977539 mm for frame 91

Saving results

Total time: 29.93877911567688
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_30_nl_6559/0008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_nl_6559/0008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_nl_6559/0008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00859740
Iteration 2/25 | Loss: 0.00127983
Iteration 3/25 | Loss: 0.00102280
Iteration 4/25 | Loss: 0.00096694
Iteration 5/25 | Loss: 0.00094763
Iteration 6/25 | Loss: 0.00094111
Iteration 7/25 | Loss: 0.00093799
Iteration 8/25 | Loss: 0.00094353
Iteration 9/25 | Loss: 0.00094469
Iteration 10/25 | Loss: 0.00094521
Iteration 11/25 | Loss: 0.00094082
Iteration 12/25 | Loss: 0.00092481
Iteration 13/25 | Loss: 0.00092110
Iteration 14/25 | Loss: 0.00091346
Iteration 15/25 | Loss: 0.00091240
Iteration 16/25 | Loss: 0.00090770
Iteration 17/25 | Loss: 0.00090668
Iteration 18/25 | Loss: 0.00091306
Iteration 19/25 | Loss: 0.00090484
Iteration 20/25 | Loss: 0.00090362
Iteration 21/25 | Loss: 0.00090689
Iteration 22/25 | Loss: 0.00090595
Iteration 23/25 | Loss: 0.00090439
Iteration 24/25 | Loss: 0.00090135
Iteration 25/25 | Loss: 0.00090035

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.59472477
Iteration 2/25 | Loss: 0.00132459
Iteration 3/25 | Loss: 0.00132458
Iteration 4/25 | Loss: 0.00132458
Iteration 5/25 | Loss: 0.00132458
Iteration 6/25 | Loss: 0.00132458
Iteration 7/25 | Loss: 0.00132457
Iteration 8/25 | Loss: 0.00132457
Iteration 9/25 | Loss: 0.00132457
Iteration 10/25 | Loss: 0.00132457
Iteration 11/25 | Loss: 0.00132457
Iteration 12/25 | Loss: 0.00132457
Iteration 13/25 | Loss: 0.00132457
Iteration 14/25 | Loss: 0.00132457
Iteration 15/25 | Loss: 0.00132457
Iteration 16/25 | Loss: 0.00132457
Iteration 17/25 | Loss: 0.00132457
Iteration 18/25 | Loss: 0.00132457
Iteration 19/25 | Loss: 0.00132457
Iteration 20/25 | Loss: 0.00132457
Iteration 21/25 | Loss: 0.00132457
Iteration 22/25 | Loss: 0.00132457
Iteration 23/25 | Loss: 0.00132457
Iteration 24/25 | Loss: 0.00132457
Iteration 25/25 | Loss: 0.00132457

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00132457
Iteration 2/1000 | Loss: 0.00028801
Iteration 3/1000 | Loss: 0.00005278
Iteration 4/1000 | Loss: 0.00005148
Iteration 5/1000 | Loss: 0.00004393
Iteration 6/1000 | Loss: 0.00004346
Iteration 7/1000 | Loss: 0.00018504
Iteration 8/1000 | Loss: 0.00017454
Iteration 9/1000 | Loss: 0.00020647
Iteration 10/1000 | Loss: 0.00011457
Iteration 11/1000 | Loss: 0.00017162
Iteration 12/1000 | Loss: 0.00008224
Iteration 13/1000 | Loss: 0.00004296
Iteration 14/1000 | Loss: 0.00003336
Iteration 15/1000 | Loss: 0.00017106
Iteration 16/1000 | Loss: 0.00017987
Iteration 17/1000 | Loss: 0.00014884
Iteration 18/1000 | Loss: 0.00018385
Iteration 19/1000 | Loss: 0.00005154
Iteration 20/1000 | Loss: 0.00004916
Iteration 21/1000 | Loss: 0.00004723
Iteration 22/1000 | Loss: 0.00003497
Iteration 23/1000 | Loss: 0.00004588
Iteration 24/1000 | Loss: 0.00004456
Iteration 25/1000 | Loss: 0.00004520
Iteration 26/1000 | Loss: 0.00004173
Iteration 27/1000 | Loss: 0.00031297
Iteration 28/1000 | Loss: 0.00003579
Iteration 29/1000 | Loss: 0.00002944
Iteration 30/1000 | Loss: 0.00002762
Iteration 31/1000 | Loss: 0.00002692
Iteration 32/1000 | Loss: 0.00002640
Iteration 33/1000 | Loss: 0.00002602
Iteration 34/1000 | Loss: 0.00002585
Iteration 35/1000 | Loss: 0.00002574
Iteration 36/1000 | Loss: 0.00002573
Iteration 37/1000 | Loss: 0.00002568
Iteration 38/1000 | Loss: 0.00002567
Iteration 39/1000 | Loss: 0.00002566
Iteration 40/1000 | Loss: 0.00002566
Iteration 41/1000 | Loss: 0.00002565
Iteration 42/1000 | Loss: 0.00002565
Iteration 43/1000 | Loss: 0.00002564
Iteration 44/1000 | Loss: 0.00002564
Iteration 45/1000 | Loss: 0.00002563
Iteration 46/1000 | Loss: 0.00002562
Iteration 47/1000 | Loss: 0.00002555
Iteration 48/1000 | Loss: 0.00002553
Iteration 49/1000 | Loss: 0.00002548
Iteration 50/1000 | Loss: 0.00002544
Iteration 51/1000 | Loss: 0.00002543
Iteration 52/1000 | Loss: 0.00002542
Iteration 53/1000 | Loss: 0.00002540
Iteration 54/1000 | Loss: 0.00002538
Iteration 55/1000 | Loss: 0.00002528
Iteration 56/1000 | Loss: 0.00002525
Iteration 57/1000 | Loss: 0.00002523
Iteration 58/1000 | Loss: 0.00002515
Iteration 59/1000 | Loss: 0.00002514
Iteration 60/1000 | Loss: 0.00002514
Iteration 61/1000 | Loss: 0.00002512
Iteration 62/1000 | Loss: 0.00002511
Iteration 63/1000 | Loss: 0.00002510
Iteration 64/1000 | Loss: 0.00002509
Iteration 65/1000 | Loss: 0.00002509
Iteration 66/1000 | Loss: 0.00002508
Iteration 67/1000 | Loss: 0.00002508
Iteration 68/1000 | Loss: 0.00002505
Iteration 69/1000 | Loss: 0.00002504
Iteration 70/1000 | Loss: 0.00002503
Iteration 71/1000 | Loss: 0.00002503
Iteration 72/1000 | Loss: 0.00002503
Iteration 73/1000 | Loss: 0.00002502
Iteration 74/1000 | Loss: 0.00002502
Iteration 75/1000 | Loss: 0.00002502
Iteration 76/1000 | Loss: 0.00002502
Iteration 77/1000 | Loss: 0.00002502
Iteration 78/1000 | Loss: 0.00002502
Iteration 79/1000 | Loss: 0.00002501
Iteration 80/1000 | Loss: 0.00002501
Iteration 81/1000 | Loss: 0.00002501
Iteration 82/1000 | Loss: 0.00002501
Iteration 83/1000 | Loss: 0.00002501
Iteration 84/1000 | Loss: 0.00002501
Iteration 85/1000 | Loss: 0.00002501
Iteration 86/1000 | Loss: 0.00002501
Iteration 87/1000 | Loss: 0.00002500
Iteration 88/1000 | Loss: 0.00002500
Iteration 89/1000 | Loss: 0.00002500
Iteration 90/1000 | Loss: 0.00025671
Iteration 91/1000 | Loss: 0.00003464
Iteration 92/1000 | Loss: 0.00002993
Iteration 93/1000 | Loss: 0.00002809
Iteration 94/1000 | Loss: 0.00002703
Iteration 95/1000 | Loss: 0.00021545
Iteration 96/1000 | Loss: 0.00072060
Iteration 97/1000 | Loss: 0.00015447
Iteration 98/1000 | Loss: 0.00004475
Iteration 99/1000 | Loss: 0.00003833
Iteration 100/1000 | Loss: 0.00003319
Iteration 101/1000 | Loss: 0.00003113
Iteration 102/1000 | Loss: 0.00018948
Iteration 103/1000 | Loss: 0.00034546
Iteration 104/1000 | Loss: 0.00013507
Iteration 105/1000 | Loss: 0.00022337
Iteration 106/1000 | Loss: 0.00013198
Iteration 107/1000 | Loss: 0.00003626
Iteration 108/1000 | Loss: 0.00003304
Iteration 109/1000 | Loss: 0.00026934
Iteration 110/1000 | Loss: 0.00020212
Iteration 111/1000 | Loss: 0.00032847
Iteration 112/1000 | Loss: 0.00029493
Iteration 113/1000 | Loss: 0.00009239
Iteration 114/1000 | Loss: 0.00003898
Iteration 115/1000 | Loss: 0.00003973
Iteration 116/1000 | Loss: 0.00005459
Iteration 117/1000 | Loss: 0.00004106
Iteration 118/1000 | Loss: 0.00003259
Iteration 119/1000 | Loss: 0.00002951
Iteration 120/1000 | Loss: 0.00025952
Iteration 121/1000 | Loss: 0.00003540
Iteration 122/1000 | Loss: 0.00003424
Iteration 123/1000 | Loss: 0.00003019
Iteration 124/1000 | Loss: 0.00002838
Iteration 125/1000 | Loss: 0.00002716
Iteration 126/1000 | Loss: 0.00002645
Iteration 127/1000 | Loss: 0.00002608
Iteration 128/1000 | Loss: 0.00002563
Iteration 129/1000 | Loss: 0.00002527
Iteration 130/1000 | Loss: 0.00002497
Iteration 131/1000 | Loss: 0.00002467
Iteration 132/1000 | Loss: 0.00002466
Iteration 133/1000 | Loss: 0.00002464
Iteration 134/1000 | Loss: 0.00002455
Iteration 135/1000 | Loss: 0.00002452
Iteration 136/1000 | Loss: 0.00002447
Iteration 137/1000 | Loss: 0.00002433
Iteration 138/1000 | Loss: 0.00002425
Iteration 139/1000 | Loss: 0.00002425
Iteration 140/1000 | Loss: 0.00002425
Iteration 141/1000 | Loss: 0.00002424
Iteration 142/1000 | Loss: 0.00002424
Iteration 143/1000 | Loss: 0.00002424
Iteration 144/1000 | Loss: 0.00002424
Iteration 145/1000 | Loss: 0.00002424
Iteration 146/1000 | Loss: 0.00002424
Iteration 147/1000 | Loss: 0.00002424
Iteration 148/1000 | Loss: 0.00002424
Iteration 149/1000 | Loss: 0.00002423
Iteration 150/1000 | Loss: 0.00002423
Iteration 151/1000 | Loss: 0.00002423
Iteration 152/1000 | Loss: 0.00002422
Iteration 153/1000 | Loss: 0.00002422
Iteration 154/1000 | Loss: 0.00002422
Iteration 155/1000 | Loss: 0.00002422
Iteration 156/1000 | Loss: 0.00002421
Iteration 157/1000 | Loss: 0.00002421
Iteration 158/1000 | Loss: 0.00002421
Iteration 159/1000 | Loss: 0.00002421
Iteration 160/1000 | Loss: 0.00002421
Iteration 161/1000 | Loss: 0.00002420
Iteration 162/1000 | Loss: 0.00002420
Iteration 163/1000 | Loss: 0.00002420
Iteration 164/1000 | Loss: 0.00002420
Iteration 165/1000 | Loss: 0.00002419
Iteration 166/1000 | Loss: 0.00002419
Iteration 167/1000 | Loss: 0.00002419
Iteration 168/1000 | Loss: 0.00002419
Iteration 169/1000 | Loss: 0.00002418
Iteration 170/1000 | Loss: 0.00002418
Iteration 171/1000 | Loss: 0.00002418
Iteration 172/1000 | Loss: 0.00002418
Iteration 173/1000 | Loss: 0.00002418
Iteration 174/1000 | Loss: 0.00002418
Iteration 175/1000 | Loss: 0.00002418
Iteration 176/1000 | Loss: 0.00002418
Iteration 177/1000 | Loss: 0.00002418
Iteration 178/1000 | Loss: 0.00002417
Iteration 179/1000 | Loss: 0.00002417
Iteration 180/1000 | Loss: 0.00002417
Iteration 181/1000 | Loss: 0.00002417
Iteration 182/1000 | Loss: 0.00002417
Iteration 183/1000 | Loss: 0.00002417
Iteration 184/1000 | Loss: 0.00002417
Iteration 185/1000 | Loss: 0.00002417
Iteration 186/1000 | Loss: 0.00002417
Iteration 187/1000 | Loss: 0.00002417
Iteration 188/1000 | Loss: 0.00002417
Iteration 189/1000 | Loss: 0.00002417
Iteration 190/1000 | Loss: 0.00002417
Iteration 191/1000 | Loss: 0.00002416
Iteration 192/1000 | Loss: 0.00002416
Iteration 193/1000 | Loss: 0.00002416
Iteration 194/1000 | Loss: 0.00002416
Iteration 195/1000 | Loss: 0.00002416
Iteration 196/1000 | Loss: 0.00002416
Iteration 197/1000 | Loss: 0.00002416
Iteration 198/1000 | Loss: 0.00002416
Iteration 199/1000 | Loss: 0.00002416
Iteration 200/1000 | Loss: 0.00002416
Iteration 201/1000 | Loss: 0.00002416
Iteration 202/1000 | Loss: 0.00002415
Iteration 203/1000 | Loss: 0.00002415
Iteration 204/1000 | Loss: 0.00002415
Iteration 205/1000 | Loss: 0.00002415
Iteration 206/1000 | Loss: 0.00002415
Iteration 207/1000 | Loss: 0.00002415
Iteration 208/1000 | Loss: 0.00002415
Iteration 209/1000 | Loss: 0.00002415
Iteration 210/1000 | Loss: 0.00002415
Iteration 211/1000 | Loss: 0.00002415
Iteration 212/1000 | Loss: 0.00002415
Iteration 213/1000 | Loss: 0.00002415
Iteration 214/1000 | Loss: 0.00002415
Iteration 215/1000 | Loss: 0.00002415
Iteration 216/1000 | Loss: 0.00002415
Iteration 217/1000 | Loss: 0.00002415
Iteration 218/1000 | Loss: 0.00002415
Iteration 219/1000 | Loss: 0.00002415
Iteration 220/1000 | Loss: 0.00002415
Iteration 221/1000 | Loss: 0.00002415
Iteration 222/1000 | Loss: 0.00002415
Iteration 223/1000 | Loss: 0.00002414
Iteration 224/1000 | Loss: 0.00002414
Iteration 225/1000 | Loss: 0.00002414
Iteration 226/1000 | Loss: 0.00002414
Iteration 227/1000 | Loss: 0.00002414
Iteration 228/1000 | Loss: 0.00002414
Iteration 229/1000 | Loss: 0.00002414
Iteration 230/1000 | Loss: 0.00002414
Iteration 231/1000 | Loss: 0.00002414
Iteration 232/1000 | Loss: 0.00002414
Iteration 233/1000 | Loss: 0.00002414
Iteration 234/1000 | Loss: 0.00002414
Iteration 235/1000 | Loss: 0.00002414
Iteration 236/1000 | Loss: 0.00002414
Iteration 237/1000 | Loss: 0.00002414
Iteration 238/1000 | Loss: 0.00002414
Iteration 239/1000 | Loss: 0.00002414
Iteration 240/1000 | Loss: 0.00002414
Iteration 241/1000 | Loss: 0.00002414
Iteration 242/1000 | Loss: 0.00002414
Iteration 243/1000 | Loss: 0.00002414
Iteration 244/1000 | Loss: 0.00002414
Iteration 245/1000 | Loss: 0.00002414
Iteration 246/1000 | Loss: 0.00002414
Iteration 247/1000 | Loss: 0.00002413
Iteration 248/1000 | Loss: 0.00002413
Iteration 249/1000 | Loss: 0.00002413
Iteration 250/1000 | Loss: 0.00002413
Iteration 251/1000 | Loss: 0.00002413
Iteration 252/1000 | Loss: 0.00002413
Iteration 253/1000 | Loss: 0.00002413
Iteration 254/1000 | Loss: 0.00002413
Iteration 255/1000 | Loss: 0.00002413
Iteration 256/1000 | Loss: 0.00002413
Iteration 257/1000 | Loss: 0.00002413
Iteration 258/1000 | Loss: 0.00002413
Iteration 259/1000 | Loss: 0.00002413
Iteration 260/1000 | Loss: 0.00002413
Iteration 261/1000 | Loss: 0.00002413
Iteration 262/1000 | Loss: 0.00002413
Iteration 263/1000 | Loss: 0.00002413
Iteration 264/1000 | Loss: 0.00002413
Iteration 265/1000 | Loss: 0.00002413
Iteration 266/1000 | Loss: 0.00002413
Iteration 267/1000 | Loss: 0.00002412
Iteration 268/1000 | Loss: 0.00002412
Iteration 269/1000 | Loss: 0.00002412
Iteration 270/1000 | Loss: 0.00002412
Iteration 271/1000 | Loss: 0.00002412
Iteration 272/1000 | Loss: 0.00002412
Iteration 273/1000 | Loss: 0.00002412
Iteration 274/1000 | Loss: 0.00002412
Iteration 275/1000 | Loss: 0.00002412
Iteration 276/1000 | Loss: 0.00002412
Iteration 277/1000 | Loss: 0.00002412
Iteration 278/1000 | Loss: 0.00002412
Iteration 279/1000 | Loss: 0.00002412
Iteration 280/1000 | Loss: 0.00002412
Iteration 281/1000 | Loss: 0.00002412
Iteration 282/1000 | Loss: 0.00002412
Iteration 283/1000 | Loss: 0.00002412
Iteration 284/1000 | Loss: 0.00002412
Iteration 285/1000 | Loss: 0.00002412
Iteration 286/1000 | Loss: 0.00002412
Iteration 287/1000 | Loss: 0.00002412
Iteration 288/1000 | Loss: 0.00002412
Iteration 289/1000 | Loss: 0.00002412
Iteration 290/1000 | Loss: 0.00002412
Iteration 291/1000 | Loss: 0.00002412
Iteration 292/1000 | Loss: 0.00002412
Iteration 293/1000 | Loss: 0.00002412
Iteration 294/1000 | Loss: 0.00002412
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 294. Stopping optimization.
Last 5 losses: [2.4117105567711405e-05, 2.4117105567711405e-05, 2.4117105567711405e-05, 2.4117105567711405e-05, 2.4117105567711405e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4117105567711405e-05

Optimization complete. Final v2v error: 4.034886837005615 mm

Highest mean error: 5.908819198608398 mm for frame 222

Lowest mean error: 3.396967649459839 mm for frame 117

Saving results

Total time: 199.88829851150513
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_30_nl_6559/0014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_nl_6559/0014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_nl_6559/0014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00473902
Iteration 2/25 | Loss: 0.00095913
Iteration 3/25 | Loss: 0.00085924
Iteration 4/25 | Loss: 0.00084043
Iteration 5/25 | Loss: 0.00083577
Iteration 6/25 | Loss: 0.00083426
Iteration 7/25 | Loss: 0.00083406
Iteration 8/25 | Loss: 0.00083406
Iteration 9/25 | Loss: 0.00083406
Iteration 10/25 | Loss: 0.00083406
Iteration 11/25 | Loss: 0.00083406
Iteration 12/25 | Loss: 0.00083406
Iteration 13/25 | Loss: 0.00083406
Iteration 14/25 | Loss: 0.00083406
Iteration 15/25 | Loss: 0.00083406
Iteration 16/25 | Loss: 0.00083406
Iteration 17/25 | Loss: 0.00083406
Iteration 18/25 | Loss: 0.00083406
Iteration 19/25 | Loss: 0.00083406
Iteration 20/25 | Loss: 0.00083406
Iteration 21/25 | Loss: 0.00083406
Iteration 22/25 | Loss: 0.00083406
Iteration 23/25 | Loss: 0.00083406
Iteration 24/25 | Loss: 0.00083406
Iteration 25/25 | Loss: 0.00083406

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.67403460
Iteration 2/25 | Loss: 0.00104798
Iteration 3/25 | Loss: 0.00104798
Iteration 4/25 | Loss: 0.00104798
Iteration 5/25 | Loss: 0.00104798
Iteration 6/25 | Loss: 0.00104798
Iteration 7/25 | Loss: 0.00104798
Iteration 8/25 | Loss: 0.00104798
Iteration 9/25 | Loss: 0.00104798
Iteration 10/25 | Loss: 0.00104798
Iteration 11/25 | Loss: 0.00104798
Iteration 12/25 | Loss: 0.00104798
Iteration 13/25 | Loss: 0.00104798
Iteration 14/25 | Loss: 0.00104798
Iteration 15/25 | Loss: 0.00104798
Iteration 16/25 | Loss: 0.00104798
Iteration 17/25 | Loss: 0.00104798
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0010479766642674804, 0.0010479766642674804, 0.0010479766642674804, 0.0010479766642674804, 0.0010479766642674804]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010479766642674804

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00104798
Iteration 2/1000 | Loss: 0.00003220
Iteration 3/1000 | Loss: 0.00002107
Iteration 4/1000 | Loss: 0.00001898
Iteration 5/1000 | Loss: 0.00001781
Iteration 6/1000 | Loss: 0.00001729
Iteration 7/1000 | Loss: 0.00001677
Iteration 8/1000 | Loss: 0.00001657
Iteration 9/1000 | Loss: 0.00001650
Iteration 10/1000 | Loss: 0.00001650
Iteration 11/1000 | Loss: 0.00001648
Iteration 12/1000 | Loss: 0.00001646
Iteration 13/1000 | Loss: 0.00001645
Iteration 14/1000 | Loss: 0.00001645
Iteration 15/1000 | Loss: 0.00001645
Iteration 16/1000 | Loss: 0.00001645
Iteration 17/1000 | Loss: 0.00001645
Iteration 18/1000 | Loss: 0.00001645
Iteration 19/1000 | Loss: 0.00001645
Iteration 20/1000 | Loss: 0.00001644
Iteration 21/1000 | Loss: 0.00001644
Iteration 22/1000 | Loss: 0.00001643
Iteration 23/1000 | Loss: 0.00001642
Iteration 24/1000 | Loss: 0.00001641
Iteration 25/1000 | Loss: 0.00001641
Iteration 26/1000 | Loss: 0.00001641
Iteration 27/1000 | Loss: 0.00001641
Iteration 28/1000 | Loss: 0.00001641
Iteration 29/1000 | Loss: 0.00001641
Iteration 30/1000 | Loss: 0.00001640
Iteration 31/1000 | Loss: 0.00001640
Iteration 32/1000 | Loss: 0.00001640
Iteration 33/1000 | Loss: 0.00001640
Iteration 34/1000 | Loss: 0.00001640
Iteration 35/1000 | Loss: 0.00001640
Iteration 36/1000 | Loss: 0.00001640
Iteration 37/1000 | Loss: 0.00001640
Iteration 38/1000 | Loss: 0.00001640
Iteration 39/1000 | Loss: 0.00001640
Iteration 40/1000 | Loss: 0.00001640
Iteration 41/1000 | Loss: 0.00001639
Iteration 42/1000 | Loss: 0.00001639
Iteration 43/1000 | Loss: 0.00001639
Iteration 44/1000 | Loss: 0.00001639
Iteration 45/1000 | Loss: 0.00001638
Iteration 46/1000 | Loss: 0.00001638
Iteration 47/1000 | Loss: 0.00001637
Iteration 48/1000 | Loss: 0.00001637
Iteration 49/1000 | Loss: 0.00001637
Iteration 50/1000 | Loss: 0.00001637
Iteration 51/1000 | Loss: 0.00001637
Iteration 52/1000 | Loss: 0.00001637
Iteration 53/1000 | Loss: 0.00001637
Iteration 54/1000 | Loss: 0.00001637
Iteration 55/1000 | Loss: 0.00001637
Iteration 56/1000 | Loss: 0.00001637
Iteration 57/1000 | Loss: 0.00001637
Iteration 58/1000 | Loss: 0.00001637
Iteration 59/1000 | Loss: 0.00001637
Iteration 60/1000 | Loss: 0.00001637
Iteration 61/1000 | Loss: 0.00001637
Iteration 62/1000 | Loss: 0.00001637
Iteration 63/1000 | Loss: 0.00001637
Iteration 64/1000 | Loss: 0.00001637
Iteration 65/1000 | Loss: 0.00001637
Iteration 66/1000 | Loss: 0.00001637
Iteration 67/1000 | Loss: 0.00001637
Iteration 68/1000 | Loss: 0.00001637
Iteration 69/1000 | Loss: 0.00001637
Iteration 70/1000 | Loss: 0.00001637
Iteration 71/1000 | Loss: 0.00001637
Iteration 72/1000 | Loss: 0.00001637
Iteration 73/1000 | Loss: 0.00001637
Iteration 74/1000 | Loss: 0.00001637
Iteration 75/1000 | Loss: 0.00001637
Iteration 76/1000 | Loss: 0.00001637
Iteration 77/1000 | Loss: 0.00001637
Iteration 78/1000 | Loss: 0.00001637
Iteration 79/1000 | Loss: 0.00001637
Iteration 80/1000 | Loss: 0.00001637
Iteration 81/1000 | Loss: 0.00001637
Iteration 82/1000 | Loss: 0.00001637
Iteration 83/1000 | Loss: 0.00001637
Iteration 84/1000 | Loss: 0.00001637
Iteration 85/1000 | Loss: 0.00001637
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 85. Stopping optimization.
Last 5 losses: [1.637062268855516e-05, 1.637062268855516e-05, 1.637062268855516e-05, 1.637062268855516e-05, 1.637062268855516e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.637062268855516e-05

Optimization complete. Final v2v error: 3.387082815170288 mm

Highest mean error: 3.7495319843292236 mm for frame 4

Lowest mean error: 3.107264280319214 mm for frame 21

Saving results

Total time: 25.55861735343933
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_30_nl_6559/0001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_nl_6559/0001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_nl_6559/0001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01130420
Iteration 2/25 | Loss: 0.00164435
Iteration 3/25 | Loss: 0.00112322
Iteration 4/25 | Loss: 0.00103419
Iteration 5/25 | Loss: 0.00101143
Iteration 6/25 | Loss: 0.00100583
Iteration 7/25 | Loss: 0.00100388
Iteration 8/25 | Loss: 0.00100380
Iteration 9/25 | Loss: 0.00100380
Iteration 10/25 | Loss: 0.00100380
Iteration 11/25 | Loss: 0.00100380
Iteration 12/25 | Loss: 0.00100380
Iteration 13/25 | Loss: 0.00100380
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0010038027539849281, 0.0010038027539849281, 0.0010038027539849281, 0.0010038027539849281, 0.0010038027539849281]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010038027539849281

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.06304038
Iteration 2/25 | Loss: 0.00179155
Iteration 3/25 | Loss: 0.00179152
Iteration 4/25 | Loss: 0.00179152
Iteration 5/25 | Loss: 0.00179152
Iteration 6/25 | Loss: 0.00179152
Iteration 7/25 | Loss: 0.00179152
Iteration 8/25 | Loss: 0.00179152
Iteration 9/25 | Loss: 0.00179152
Iteration 10/25 | Loss: 0.00179152
Iteration 11/25 | Loss: 0.00179152
Iteration 12/25 | Loss: 0.00179152
Iteration 13/25 | Loss: 0.00179152
Iteration 14/25 | Loss: 0.00179152
Iteration 15/25 | Loss: 0.00179152
Iteration 16/25 | Loss: 0.00179152
Iteration 17/25 | Loss: 0.00179152
Iteration 18/25 | Loss: 0.00179152
Iteration 19/25 | Loss: 0.00179152
Iteration 20/25 | Loss: 0.00179152
Iteration 21/25 | Loss: 0.00179152
Iteration 22/25 | Loss: 0.00179152
Iteration 23/25 | Loss: 0.00179152
Iteration 24/25 | Loss: 0.00179152
Iteration 25/25 | Loss: 0.00179152
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0017915170174092054, 0.0017915170174092054, 0.0017915170174092054, 0.0017915170174092054, 0.0017915170174092054]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0017915170174092054

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00179152
Iteration 2/1000 | Loss: 0.00003560
Iteration 3/1000 | Loss: 0.00002787
Iteration 4/1000 | Loss: 0.00002561
Iteration 5/1000 | Loss: 0.00002491
Iteration 6/1000 | Loss: 0.00002431
Iteration 7/1000 | Loss: 0.00002361
Iteration 8/1000 | Loss: 0.00002325
Iteration 9/1000 | Loss: 0.00002303
Iteration 10/1000 | Loss: 0.00002279
Iteration 11/1000 | Loss: 0.00002258
Iteration 12/1000 | Loss: 0.00002242
Iteration 13/1000 | Loss: 0.00002239
Iteration 14/1000 | Loss: 0.00002231
Iteration 15/1000 | Loss: 0.00002225
Iteration 16/1000 | Loss: 0.00002221
Iteration 17/1000 | Loss: 0.00002221
Iteration 18/1000 | Loss: 0.00002221
Iteration 19/1000 | Loss: 0.00002220
Iteration 20/1000 | Loss: 0.00002220
Iteration 21/1000 | Loss: 0.00002220
Iteration 22/1000 | Loss: 0.00002220
Iteration 23/1000 | Loss: 0.00002220
Iteration 24/1000 | Loss: 0.00002220
Iteration 25/1000 | Loss: 0.00002219
Iteration 26/1000 | Loss: 0.00002218
Iteration 27/1000 | Loss: 0.00002218
Iteration 28/1000 | Loss: 0.00002217
Iteration 29/1000 | Loss: 0.00002216
Iteration 30/1000 | Loss: 0.00002216
Iteration 31/1000 | Loss: 0.00002216
Iteration 32/1000 | Loss: 0.00002216
Iteration 33/1000 | Loss: 0.00002215
Iteration 34/1000 | Loss: 0.00002214
Iteration 35/1000 | Loss: 0.00002214
Iteration 36/1000 | Loss: 0.00002214
Iteration 37/1000 | Loss: 0.00002214
Iteration 38/1000 | Loss: 0.00002214
Iteration 39/1000 | Loss: 0.00002213
Iteration 40/1000 | Loss: 0.00002213
Iteration 41/1000 | Loss: 0.00002213
Iteration 42/1000 | Loss: 0.00002213
Iteration 43/1000 | Loss: 0.00002213
Iteration 44/1000 | Loss: 0.00002213
Iteration 45/1000 | Loss: 0.00002213
Iteration 46/1000 | Loss: 0.00002212
Iteration 47/1000 | Loss: 0.00002212
Iteration 48/1000 | Loss: 0.00002211
Iteration 49/1000 | Loss: 0.00002211
Iteration 50/1000 | Loss: 0.00002211
Iteration 51/1000 | Loss: 0.00002211
Iteration 52/1000 | Loss: 0.00002211
Iteration 53/1000 | Loss: 0.00002210
Iteration 54/1000 | Loss: 0.00002210
Iteration 55/1000 | Loss: 0.00002210
Iteration 56/1000 | Loss: 0.00002210
Iteration 57/1000 | Loss: 0.00002210
Iteration 58/1000 | Loss: 0.00002210
Iteration 59/1000 | Loss: 0.00002209
Iteration 60/1000 | Loss: 0.00002209
Iteration 61/1000 | Loss: 0.00002208
Iteration 62/1000 | Loss: 0.00002208
Iteration 63/1000 | Loss: 0.00002208
Iteration 64/1000 | Loss: 0.00002208
Iteration 65/1000 | Loss: 0.00002208
Iteration 66/1000 | Loss: 0.00002208
Iteration 67/1000 | Loss: 0.00002208
Iteration 68/1000 | Loss: 0.00002207
Iteration 69/1000 | Loss: 0.00002207
Iteration 70/1000 | Loss: 0.00002207
Iteration 71/1000 | Loss: 0.00002207
Iteration 72/1000 | Loss: 0.00002207
Iteration 73/1000 | Loss: 0.00002207
Iteration 74/1000 | Loss: 0.00002207
Iteration 75/1000 | Loss: 0.00002207
Iteration 76/1000 | Loss: 0.00002207
Iteration 77/1000 | Loss: 0.00002207
Iteration 78/1000 | Loss: 0.00002207
Iteration 79/1000 | Loss: 0.00002207
Iteration 80/1000 | Loss: 0.00002207
Iteration 81/1000 | Loss: 0.00002207
Iteration 82/1000 | Loss: 0.00002207
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 82. Stopping optimization.
Last 5 losses: [2.2073656509746797e-05, 2.2073656509746797e-05, 2.2073656509746797e-05, 2.2073656509746797e-05, 2.2073656509746797e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2073656509746797e-05

Optimization complete. Final v2v error: 3.953160285949707 mm

Highest mean error: 4.876669883728027 mm for frame 157

Lowest mean error: 3.2091257572174072 mm for frame 239

Saving results

Total time: 39.27766728401184
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_30_nl_6559/0011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_nl_6559/0011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_nl_6559/0011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01117616
Iteration 2/25 | Loss: 0.01117616
Iteration 3/25 | Loss: 0.01117616
Iteration 4/25 | Loss: 0.01117616
Iteration 5/25 | Loss: 0.01117616
Iteration 6/25 | Loss: 0.01117615
Iteration 7/25 | Loss: 0.01117615
Iteration 8/25 | Loss: 0.01117615
Iteration 9/25 | Loss: 0.01117615
Iteration 10/25 | Loss: 0.01117615
Iteration 11/25 | Loss: 0.01117615
Iteration 12/25 | Loss: 0.01117615
Iteration 13/25 | Loss: 0.01117615
Iteration 14/25 | Loss: 0.01117615
Iteration 15/25 | Loss: 0.01117615
Iteration 16/25 | Loss: 0.01117615
Iteration 17/25 | Loss: 0.01117615
Iteration 18/25 | Loss: 0.01117615
Iteration 19/25 | Loss: 0.01117615
Iteration 20/25 | Loss: 0.01117614
Iteration 21/25 | Loss: 0.01117614
Iteration 22/25 | Loss: 0.01117614
Iteration 23/25 | Loss: 0.01117614
Iteration 24/25 | Loss: 0.01117614
Iteration 25/25 | Loss: 0.01117614

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.65594363
Iteration 2/25 | Loss: 0.12070473
Iteration 3/25 | Loss: 0.12067525
Iteration 4/25 | Loss: 0.12067383
Iteration 5/25 | Loss: 0.12066608
Iteration 6/25 | Loss: 0.12066604
Iteration 7/25 | Loss: 0.12066603
Iteration 8/25 | Loss: 0.12066603
Iteration 9/25 | Loss: 0.12066600
Iteration 10/25 | Loss: 0.12066600
Iteration 11/25 | Loss: 0.12066600
Iteration 12/25 | Loss: 0.12066600
Iteration 13/25 | Loss: 0.12066600
Iteration 14/25 | Loss: 0.12066600
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.12066600471735, 0.12066600471735, 0.12066600471735, 0.12066600471735, 0.12066600471735]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.12066600471735

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.12066600
Iteration 2/1000 | Loss: 0.00187455
Iteration 3/1000 | Loss: 0.00113133
Iteration 4/1000 | Loss: 0.00044230
Iteration 5/1000 | Loss: 0.00021942
Iteration 6/1000 | Loss: 0.00030096
Iteration 7/1000 | Loss: 0.00209062
Iteration 8/1000 | Loss: 0.00270619
Iteration 9/1000 | Loss: 0.00169230
Iteration 10/1000 | Loss: 0.00009722
Iteration 11/1000 | Loss: 0.00008779
Iteration 12/1000 | Loss: 0.00012748
Iteration 13/1000 | Loss: 0.00010777
Iteration 14/1000 | Loss: 0.00004382
Iteration 15/1000 | Loss: 0.00008079
Iteration 16/1000 | Loss: 0.00036885
Iteration 17/1000 | Loss: 0.00010572
Iteration 18/1000 | Loss: 0.00005172
Iteration 19/1000 | Loss: 0.00010659
Iteration 20/1000 | Loss: 0.00004719
Iteration 21/1000 | Loss: 0.00012880
Iteration 22/1000 | Loss: 0.00003680
Iteration 23/1000 | Loss: 0.00005175
Iteration 24/1000 | Loss: 0.00004251
Iteration 25/1000 | Loss: 0.00003431
Iteration 26/1000 | Loss: 0.00003209
Iteration 27/1000 | Loss: 0.00002493
Iteration 28/1000 | Loss: 0.00008500
Iteration 29/1000 | Loss: 0.00002045
Iteration 30/1000 | Loss: 0.00007023
Iteration 31/1000 | Loss: 0.00001808
Iteration 32/1000 | Loss: 0.00002528
Iteration 33/1000 | Loss: 0.00002787
Iteration 34/1000 | Loss: 0.00003456
Iteration 35/1000 | Loss: 0.00001865
Iteration 36/1000 | Loss: 0.00001752
Iteration 37/1000 | Loss: 0.00002579
Iteration 38/1000 | Loss: 0.00002357
Iteration 39/1000 | Loss: 0.00001936
Iteration 40/1000 | Loss: 0.00001898
Iteration 41/1000 | Loss: 0.00010077
Iteration 42/1000 | Loss: 0.00001686
Iteration 43/1000 | Loss: 0.00001449
Iteration 44/1000 | Loss: 0.00001709
Iteration 45/1000 | Loss: 0.00001419
Iteration 46/1000 | Loss: 0.00003344
Iteration 47/1000 | Loss: 0.00001378
Iteration 48/1000 | Loss: 0.00001453
Iteration 49/1000 | Loss: 0.00002107
Iteration 50/1000 | Loss: 0.00001382
Iteration 51/1000 | Loss: 0.00002599
Iteration 52/1000 | Loss: 0.00001336
Iteration 53/1000 | Loss: 0.00001403
Iteration 54/1000 | Loss: 0.00001823
Iteration 55/1000 | Loss: 0.00001367
Iteration 56/1000 | Loss: 0.00001318
Iteration 57/1000 | Loss: 0.00001303
Iteration 58/1000 | Loss: 0.00001299
Iteration 59/1000 | Loss: 0.00002834
Iteration 60/1000 | Loss: 0.00001323
Iteration 61/1000 | Loss: 0.00001289
Iteration 62/1000 | Loss: 0.00001289
Iteration 63/1000 | Loss: 0.00001289
Iteration 64/1000 | Loss: 0.00001289
Iteration 65/1000 | Loss: 0.00001289
Iteration 66/1000 | Loss: 0.00001289
Iteration 67/1000 | Loss: 0.00001289
Iteration 68/1000 | Loss: 0.00001288
Iteration 69/1000 | Loss: 0.00001288
Iteration 70/1000 | Loss: 0.00001288
Iteration 71/1000 | Loss: 0.00001287
Iteration 72/1000 | Loss: 0.00001287
Iteration 73/1000 | Loss: 0.00001287
Iteration 74/1000 | Loss: 0.00001287
Iteration 75/1000 | Loss: 0.00001287
Iteration 76/1000 | Loss: 0.00001287
Iteration 77/1000 | Loss: 0.00001286
Iteration 78/1000 | Loss: 0.00001286
Iteration 79/1000 | Loss: 0.00001286
Iteration 80/1000 | Loss: 0.00001286
Iteration 81/1000 | Loss: 0.00001286
Iteration 82/1000 | Loss: 0.00001286
Iteration 83/1000 | Loss: 0.00001286
Iteration 84/1000 | Loss: 0.00001286
Iteration 85/1000 | Loss: 0.00001286
Iteration 86/1000 | Loss: 0.00001285
Iteration 87/1000 | Loss: 0.00001285
Iteration 88/1000 | Loss: 0.00001285
Iteration 89/1000 | Loss: 0.00001285
Iteration 90/1000 | Loss: 0.00001285
Iteration 91/1000 | Loss: 0.00001285
Iteration 92/1000 | Loss: 0.00001285
Iteration 93/1000 | Loss: 0.00001285
Iteration 94/1000 | Loss: 0.00001285
Iteration 95/1000 | Loss: 0.00001285
Iteration 96/1000 | Loss: 0.00001372
Iteration 97/1000 | Loss: 0.00001766
Iteration 98/1000 | Loss: 0.00004176
Iteration 99/1000 | Loss: 0.00001623
Iteration 100/1000 | Loss: 0.00001364
Iteration 101/1000 | Loss: 0.00001271
Iteration 102/1000 | Loss: 0.00001271
Iteration 103/1000 | Loss: 0.00001271
Iteration 104/1000 | Loss: 0.00001271
Iteration 105/1000 | Loss: 0.00001271
Iteration 106/1000 | Loss: 0.00001271
Iteration 107/1000 | Loss: 0.00001271
Iteration 108/1000 | Loss: 0.00001271
Iteration 109/1000 | Loss: 0.00001271
Iteration 110/1000 | Loss: 0.00001270
Iteration 111/1000 | Loss: 0.00001270
Iteration 112/1000 | Loss: 0.00001270
Iteration 113/1000 | Loss: 0.00001270
Iteration 114/1000 | Loss: 0.00001269
Iteration 115/1000 | Loss: 0.00001269
Iteration 116/1000 | Loss: 0.00001269
Iteration 117/1000 | Loss: 0.00001269
Iteration 118/1000 | Loss: 0.00001269
Iteration 119/1000 | Loss: 0.00001269
Iteration 120/1000 | Loss: 0.00001269
Iteration 121/1000 | Loss: 0.00001269
Iteration 122/1000 | Loss: 0.00001269
Iteration 123/1000 | Loss: 0.00001269
Iteration 124/1000 | Loss: 0.00001269
Iteration 125/1000 | Loss: 0.00001269
Iteration 126/1000 | Loss: 0.00001269
Iteration 127/1000 | Loss: 0.00001269
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 127. Stopping optimization.
Last 5 losses: [1.2690023140748963e-05, 1.2690023140748963e-05, 1.2690023140748963e-05, 1.2690023140748963e-05, 1.2690023140748963e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2690023140748963e-05

Optimization complete. Final v2v error: 2.6937570571899414 mm

Highest mean error: 10.238258361816406 mm for frame 159

Lowest mean error: 2.095154285430908 mm for frame 88

Saving results

Total time: 106.58166837692261
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_30_nl_6559/0000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_nl_6559/0000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_nl_6559/0000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00421145
Iteration 2/25 | Loss: 0.00100928
Iteration 3/25 | Loss: 0.00090899
Iteration 4/25 | Loss: 0.00089498
Iteration 5/25 | Loss: 0.00088562
Iteration 6/25 | Loss: 0.00088416
Iteration 7/25 | Loss: 0.00088416
Iteration 8/25 | Loss: 0.00088416
Iteration 9/25 | Loss: 0.00088416
Iteration 10/25 | Loss: 0.00088416
Iteration 11/25 | Loss: 0.00088416
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0008841602830216289, 0.0008841602830216289, 0.0008841602830216289, 0.0008841602830216289, 0.0008841602830216289]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008841602830216289

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.84099448
Iteration 2/25 | Loss: 0.00141324
Iteration 3/25 | Loss: 0.00141324
Iteration 4/25 | Loss: 0.00141324
Iteration 5/25 | Loss: 0.00141324
Iteration 6/25 | Loss: 0.00141324
Iteration 7/25 | Loss: 0.00141324
Iteration 8/25 | Loss: 0.00141324
Iteration 9/25 | Loss: 0.00141323
Iteration 10/25 | Loss: 0.00141323
Iteration 11/25 | Loss: 0.00141323
Iteration 12/25 | Loss: 0.00141323
Iteration 13/25 | Loss: 0.00141323
Iteration 14/25 | Loss: 0.00141323
Iteration 15/25 | Loss: 0.00141323
Iteration 16/25 | Loss: 0.00141323
Iteration 17/25 | Loss: 0.00141323
Iteration 18/25 | Loss: 0.00141323
Iteration 19/25 | Loss: 0.00141323
Iteration 20/25 | Loss: 0.00141323
Iteration 21/25 | Loss: 0.00141323
Iteration 22/25 | Loss: 0.00141323
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.001413234625943005, 0.001413234625943005, 0.001413234625943005, 0.001413234625943005, 0.001413234625943005]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001413234625943005

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00141323
Iteration 2/1000 | Loss: 0.00002533
Iteration 3/1000 | Loss: 0.00001917
Iteration 4/1000 | Loss: 0.00001733
Iteration 5/1000 | Loss: 0.00001649
Iteration 6/1000 | Loss: 0.00001600
Iteration 7/1000 | Loss: 0.00001574
Iteration 8/1000 | Loss: 0.00001554
Iteration 9/1000 | Loss: 0.00001554
Iteration 10/1000 | Loss: 0.00001530
Iteration 11/1000 | Loss: 0.00001528
Iteration 12/1000 | Loss: 0.00001522
Iteration 13/1000 | Loss: 0.00001517
Iteration 14/1000 | Loss: 0.00001517
Iteration 15/1000 | Loss: 0.00001517
Iteration 16/1000 | Loss: 0.00001516
Iteration 17/1000 | Loss: 0.00001510
Iteration 18/1000 | Loss: 0.00001508
Iteration 19/1000 | Loss: 0.00001508
Iteration 20/1000 | Loss: 0.00001507
Iteration 21/1000 | Loss: 0.00001507
Iteration 22/1000 | Loss: 0.00001507
Iteration 23/1000 | Loss: 0.00001506
Iteration 24/1000 | Loss: 0.00001506
Iteration 25/1000 | Loss: 0.00001504
Iteration 26/1000 | Loss: 0.00001504
Iteration 27/1000 | Loss: 0.00001504
Iteration 28/1000 | Loss: 0.00001504
Iteration 29/1000 | Loss: 0.00001504
Iteration 30/1000 | Loss: 0.00001504
Iteration 31/1000 | Loss: 0.00001504
Iteration 32/1000 | Loss: 0.00001504
Iteration 33/1000 | Loss: 0.00001504
Iteration 34/1000 | Loss: 0.00001503
Iteration 35/1000 | Loss: 0.00001503
Iteration 36/1000 | Loss: 0.00001503
Iteration 37/1000 | Loss: 0.00001503
Iteration 38/1000 | Loss: 0.00001503
Iteration 39/1000 | Loss: 0.00001503
Iteration 40/1000 | Loss: 0.00001503
Iteration 41/1000 | Loss: 0.00001503
Iteration 42/1000 | Loss: 0.00001503
Iteration 43/1000 | Loss: 0.00001503
Iteration 44/1000 | Loss: 0.00001503
Iteration 45/1000 | Loss: 0.00001502
Iteration 46/1000 | Loss: 0.00001502
Iteration 47/1000 | Loss: 0.00001501
Iteration 48/1000 | Loss: 0.00001497
Iteration 49/1000 | Loss: 0.00001496
Iteration 50/1000 | Loss: 0.00001496
Iteration 51/1000 | Loss: 0.00001495
Iteration 52/1000 | Loss: 0.00001495
Iteration 53/1000 | Loss: 0.00001495
Iteration 54/1000 | Loss: 0.00001495
Iteration 55/1000 | Loss: 0.00001495
Iteration 56/1000 | Loss: 0.00001495
Iteration 57/1000 | Loss: 0.00001495
Iteration 58/1000 | Loss: 0.00001495
Iteration 59/1000 | Loss: 0.00001494
Iteration 60/1000 | Loss: 0.00001494
Iteration 61/1000 | Loss: 0.00001493
Iteration 62/1000 | Loss: 0.00001493
Iteration 63/1000 | Loss: 0.00001493
Iteration 64/1000 | Loss: 0.00001493
Iteration 65/1000 | Loss: 0.00001493
Iteration 66/1000 | Loss: 0.00001493
Iteration 67/1000 | Loss: 0.00001493
Iteration 68/1000 | Loss: 0.00001492
Iteration 69/1000 | Loss: 0.00001492
Iteration 70/1000 | Loss: 0.00001492
Iteration 71/1000 | Loss: 0.00001492
Iteration 72/1000 | Loss: 0.00001492
Iteration 73/1000 | Loss: 0.00001492
Iteration 74/1000 | Loss: 0.00001491
Iteration 75/1000 | Loss: 0.00001491
Iteration 76/1000 | Loss: 0.00001491
Iteration 77/1000 | Loss: 0.00001491
Iteration 78/1000 | Loss: 0.00001491
Iteration 79/1000 | Loss: 0.00001491
Iteration 80/1000 | Loss: 0.00001491
Iteration 81/1000 | Loss: 0.00001491
Iteration 82/1000 | Loss: 0.00001491
Iteration 83/1000 | Loss: 0.00001491
Iteration 84/1000 | Loss: 0.00001491
Iteration 85/1000 | Loss: 0.00001491
Iteration 86/1000 | Loss: 0.00001490
Iteration 87/1000 | Loss: 0.00001490
Iteration 88/1000 | Loss: 0.00001490
Iteration 89/1000 | Loss: 0.00001490
Iteration 90/1000 | Loss: 0.00001490
Iteration 91/1000 | Loss: 0.00001489
Iteration 92/1000 | Loss: 0.00001489
Iteration 93/1000 | Loss: 0.00001489
Iteration 94/1000 | Loss: 0.00001489
Iteration 95/1000 | Loss: 0.00001489
Iteration 96/1000 | Loss: 0.00001489
Iteration 97/1000 | Loss: 0.00001489
Iteration 98/1000 | Loss: 0.00001489
Iteration 99/1000 | Loss: 0.00001489
Iteration 100/1000 | Loss: 0.00001488
Iteration 101/1000 | Loss: 0.00001488
Iteration 102/1000 | Loss: 0.00001488
Iteration 103/1000 | Loss: 0.00001488
Iteration 104/1000 | Loss: 0.00001488
Iteration 105/1000 | Loss: 0.00001488
Iteration 106/1000 | Loss: 0.00001488
Iteration 107/1000 | Loss: 0.00001488
Iteration 108/1000 | Loss: 0.00001488
Iteration 109/1000 | Loss: 0.00001488
Iteration 110/1000 | Loss: 0.00001488
Iteration 111/1000 | Loss: 0.00001487
Iteration 112/1000 | Loss: 0.00001487
Iteration 113/1000 | Loss: 0.00001487
Iteration 114/1000 | Loss: 0.00001487
Iteration 115/1000 | Loss: 0.00001487
Iteration 116/1000 | Loss: 0.00001487
Iteration 117/1000 | Loss: 0.00001486
Iteration 118/1000 | Loss: 0.00001486
Iteration 119/1000 | Loss: 0.00001486
Iteration 120/1000 | Loss: 0.00001486
Iteration 121/1000 | Loss: 0.00001486
Iteration 122/1000 | Loss: 0.00001486
Iteration 123/1000 | Loss: 0.00001486
Iteration 124/1000 | Loss: 0.00001486
Iteration 125/1000 | Loss: 0.00001486
Iteration 126/1000 | Loss: 0.00001486
Iteration 127/1000 | Loss: 0.00001486
Iteration 128/1000 | Loss: 0.00001486
Iteration 129/1000 | Loss: 0.00001486
Iteration 130/1000 | Loss: 0.00001486
Iteration 131/1000 | Loss: 0.00001486
Iteration 132/1000 | Loss: 0.00001486
Iteration 133/1000 | Loss: 0.00001486
Iteration 134/1000 | Loss: 0.00001485
Iteration 135/1000 | Loss: 0.00001485
Iteration 136/1000 | Loss: 0.00001485
Iteration 137/1000 | Loss: 0.00001485
Iteration 138/1000 | Loss: 0.00001485
Iteration 139/1000 | Loss: 0.00001485
Iteration 140/1000 | Loss: 0.00001485
Iteration 141/1000 | Loss: 0.00001485
Iteration 142/1000 | Loss: 0.00001485
Iteration 143/1000 | Loss: 0.00001485
Iteration 144/1000 | Loss: 0.00001485
Iteration 145/1000 | Loss: 0.00001485
Iteration 146/1000 | Loss: 0.00001485
Iteration 147/1000 | Loss: 0.00001484
Iteration 148/1000 | Loss: 0.00001484
Iteration 149/1000 | Loss: 0.00001484
Iteration 150/1000 | Loss: 0.00001484
Iteration 151/1000 | Loss: 0.00001484
Iteration 152/1000 | Loss: 0.00001484
Iteration 153/1000 | Loss: 0.00001484
Iteration 154/1000 | Loss: 0.00001484
Iteration 155/1000 | Loss: 0.00001484
Iteration 156/1000 | Loss: 0.00001484
Iteration 157/1000 | Loss: 0.00001484
Iteration 158/1000 | Loss: 0.00001484
Iteration 159/1000 | Loss: 0.00001484
Iteration 160/1000 | Loss: 0.00001484
Iteration 161/1000 | Loss: 0.00001484
Iteration 162/1000 | Loss: 0.00001484
Iteration 163/1000 | Loss: 0.00001484
Iteration 164/1000 | Loss: 0.00001484
Iteration 165/1000 | Loss: 0.00001484
Iteration 166/1000 | Loss: 0.00001484
Iteration 167/1000 | Loss: 0.00001484
Iteration 168/1000 | Loss: 0.00001484
Iteration 169/1000 | Loss: 0.00001483
Iteration 170/1000 | Loss: 0.00001483
Iteration 171/1000 | Loss: 0.00001483
Iteration 172/1000 | Loss: 0.00001483
Iteration 173/1000 | Loss: 0.00001483
Iteration 174/1000 | Loss: 0.00001483
Iteration 175/1000 | Loss: 0.00001483
Iteration 176/1000 | Loss: 0.00001483
Iteration 177/1000 | Loss: 0.00001483
Iteration 178/1000 | Loss: 0.00001483
Iteration 179/1000 | Loss: 0.00001483
Iteration 180/1000 | Loss: 0.00001483
Iteration 181/1000 | Loss: 0.00001483
Iteration 182/1000 | Loss: 0.00001483
Iteration 183/1000 | Loss: 0.00001483
Iteration 184/1000 | Loss: 0.00001483
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 184. Stopping optimization.
Last 5 losses: [1.4832734450465068e-05, 1.4832734450465068e-05, 1.4832734450465068e-05, 1.4832734450465068e-05, 1.4832734450465068e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4832734450465068e-05

Optimization complete. Final v2v error: 3.3170039653778076 mm

Highest mean error: 3.6481118202209473 mm for frame 25

Lowest mean error: 3.0370566844940186 mm for frame 162

Saving results

Total time: 38.264607191085815
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_30_nl_6559/0002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_nl_6559/0002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_nl_6559/0002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00893935
Iteration 2/25 | Loss: 0.00110892
Iteration 3/25 | Loss: 0.00092217
Iteration 4/25 | Loss: 0.00088992
Iteration 5/25 | Loss: 0.00087945
Iteration 6/25 | Loss: 0.00087705
Iteration 7/25 | Loss: 0.00087630
Iteration 8/25 | Loss: 0.00087627
Iteration 9/25 | Loss: 0.00087627
Iteration 10/25 | Loss: 0.00087627
Iteration 11/25 | Loss: 0.00087627
Iteration 12/25 | Loss: 0.00087627
Iteration 13/25 | Loss: 0.00087627
Iteration 14/25 | Loss: 0.00087627
Iteration 15/25 | Loss: 0.00087627
Iteration 16/25 | Loss: 0.00087627
Iteration 17/25 | Loss: 0.00087627
Iteration 18/25 | Loss: 0.00087627
Iteration 19/25 | Loss: 0.00087627
Iteration 20/25 | Loss: 0.00087627
Iteration 21/25 | Loss: 0.00087627
Iteration 22/25 | Loss: 0.00087627
Iteration 23/25 | Loss: 0.00087627
Iteration 24/25 | Loss: 0.00087627
Iteration 25/25 | Loss: 0.00087627

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.67688346
Iteration 2/25 | Loss: 0.00121906
Iteration 3/25 | Loss: 0.00121905
Iteration 4/25 | Loss: 0.00121905
Iteration 5/25 | Loss: 0.00121905
Iteration 6/25 | Loss: 0.00121905
Iteration 7/25 | Loss: 0.00121905
Iteration 8/25 | Loss: 0.00121905
Iteration 9/25 | Loss: 0.00121905
Iteration 10/25 | Loss: 0.00121905
Iteration 11/25 | Loss: 0.00121905
Iteration 12/25 | Loss: 0.00121905
Iteration 13/25 | Loss: 0.00121905
Iteration 14/25 | Loss: 0.00121905
Iteration 15/25 | Loss: 0.00121905
Iteration 16/25 | Loss: 0.00121905
Iteration 17/25 | Loss: 0.00121905
Iteration 18/25 | Loss: 0.00121905
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0012190494453534484, 0.0012190494453534484, 0.0012190494453534484, 0.0012190494453534484, 0.0012190494453534484]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012190494453534484

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00121905
Iteration 2/1000 | Loss: 0.00003038
Iteration 3/1000 | Loss: 0.00002397
Iteration 4/1000 | Loss: 0.00002106
Iteration 5/1000 | Loss: 0.00001998
Iteration 6/1000 | Loss: 0.00001917
Iteration 7/1000 | Loss: 0.00001865
Iteration 8/1000 | Loss: 0.00001818
Iteration 9/1000 | Loss: 0.00001793
Iteration 10/1000 | Loss: 0.00001786
Iteration 11/1000 | Loss: 0.00001770
Iteration 12/1000 | Loss: 0.00001769
Iteration 13/1000 | Loss: 0.00001766
Iteration 14/1000 | Loss: 0.00001763
Iteration 15/1000 | Loss: 0.00001763
Iteration 16/1000 | Loss: 0.00001762
Iteration 17/1000 | Loss: 0.00001758
Iteration 18/1000 | Loss: 0.00001754
Iteration 19/1000 | Loss: 0.00001754
Iteration 20/1000 | Loss: 0.00001752
Iteration 21/1000 | Loss: 0.00001751
Iteration 22/1000 | Loss: 0.00001751
Iteration 23/1000 | Loss: 0.00001750
Iteration 24/1000 | Loss: 0.00001749
Iteration 25/1000 | Loss: 0.00001749
Iteration 26/1000 | Loss: 0.00001748
Iteration 27/1000 | Loss: 0.00001747
Iteration 28/1000 | Loss: 0.00001747
Iteration 29/1000 | Loss: 0.00001746
Iteration 30/1000 | Loss: 0.00001746
Iteration 31/1000 | Loss: 0.00001746
Iteration 32/1000 | Loss: 0.00001746
Iteration 33/1000 | Loss: 0.00001746
Iteration 34/1000 | Loss: 0.00001746
Iteration 35/1000 | Loss: 0.00001746
Iteration 36/1000 | Loss: 0.00001746
Iteration 37/1000 | Loss: 0.00001746
Iteration 38/1000 | Loss: 0.00001746
Iteration 39/1000 | Loss: 0.00001745
Iteration 40/1000 | Loss: 0.00001745
Iteration 41/1000 | Loss: 0.00001744
Iteration 42/1000 | Loss: 0.00001742
Iteration 43/1000 | Loss: 0.00001742
Iteration 44/1000 | Loss: 0.00001741
Iteration 45/1000 | Loss: 0.00001741
Iteration 46/1000 | Loss: 0.00001739
Iteration 47/1000 | Loss: 0.00001739
Iteration 48/1000 | Loss: 0.00001739
Iteration 49/1000 | Loss: 0.00001739
Iteration 50/1000 | Loss: 0.00001739
Iteration 51/1000 | Loss: 0.00001739
Iteration 52/1000 | Loss: 0.00001739
Iteration 53/1000 | Loss: 0.00001739
Iteration 54/1000 | Loss: 0.00001738
Iteration 55/1000 | Loss: 0.00001738
Iteration 56/1000 | Loss: 0.00001738
Iteration 57/1000 | Loss: 0.00001738
Iteration 58/1000 | Loss: 0.00001737
Iteration 59/1000 | Loss: 0.00001737
Iteration 60/1000 | Loss: 0.00001736
Iteration 61/1000 | Loss: 0.00001736
Iteration 62/1000 | Loss: 0.00001736
Iteration 63/1000 | Loss: 0.00001736
Iteration 64/1000 | Loss: 0.00001736
Iteration 65/1000 | Loss: 0.00001736
Iteration 66/1000 | Loss: 0.00001736
Iteration 67/1000 | Loss: 0.00001736
Iteration 68/1000 | Loss: 0.00001735
Iteration 69/1000 | Loss: 0.00001735
Iteration 70/1000 | Loss: 0.00001735
Iteration 71/1000 | Loss: 0.00001735
Iteration 72/1000 | Loss: 0.00001735
Iteration 73/1000 | Loss: 0.00001735
Iteration 74/1000 | Loss: 0.00001735
Iteration 75/1000 | Loss: 0.00001735
Iteration 76/1000 | Loss: 0.00001735
Iteration 77/1000 | Loss: 0.00001735
Iteration 78/1000 | Loss: 0.00001735
Iteration 79/1000 | Loss: 0.00001734
Iteration 80/1000 | Loss: 0.00001734
Iteration 81/1000 | Loss: 0.00001734
Iteration 82/1000 | Loss: 0.00001734
Iteration 83/1000 | Loss: 0.00001734
Iteration 84/1000 | Loss: 0.00001734
Iteration 85/1000 | Loss: 0.00001734
Iteration 86/1000 | Loss: 0.00001734
Iteration 87/1000 | Loss: 0.00001733
Iteration 88/1000 | Loss: 0.00001733
Iteration 89/1000 | Loss: 0.00001733
Iteration 90/1000 | Loss: 0.00001733
Iteration 91/1000 | Loss: 0.00001733
Iteration 92/1000 | Loss: 0.00001733
Iteration 93/1000 | Loss: 0.00001733
Iteration 94/1000 | Loss: 0.00001733
Iteration 95/1000 | Loss: 0.00001733
Iteration 96/1000 | Loss: 0.00001733
Iteration 97/1000 | Loss: 0.00001733
Iteration 98/1000 | Loss: 0.00001733
Iteration 99/1000 | Loss: 0.00001733
Iteration 100/1000 | Loss: 0.00001733
Iteration 101/1000 | Loss: 0.00001732
Iteration 102/1000 | Loss: 0.00001732
Iteration 103/1000 | Loss: 0.00001732
Iteration 104/1000 | Loss: 0.00001732
Iteration 105/1000 | Loss: 0.00001732
Iteration 106/1000 | Loss: 0.00001732
Iteration 107/1000 | Loss: 0.00001732
Iteration 108/1000 | Loss: 0.00001732
Iteration 109/1000 | Loss: 0.00001731
Iteration 110/1000 | Loss: 0.00001731
Iteration 111/1000 | Loss: 0.00001731
Iteration 112/1000 | Loss: 0.00001731
Iteration 113/1000 | Loss: 0.00001731
Iteration 114/1000 | Loss: 0.00001731
Iteration 115/1000 | Loss: 0.00001731
Iteration 116/1000 | Loss: 0.00001731
Iteration 117/1000 | Loss: 0.00001731
Iteration 118/1000 | Loss: 0.00001731
Iteration 119/1000 | Loss: 0.00001731
Iteration 120/1000 | Loss: 0.00001731
Iteration 121/1000 | Loss: 0.00001730
Iteration 122/1000 | Loss: 0.00001730
Iteration 123/1000 | Loss: 0.00001730
Iteration 124/1000 | Loss: 0.00001730
Iteration 125/1000 | Loss: 0.00001730
Iteration 126/1000 | Loss: 0.00001730
Iteration 127/1000 | Loss: 0.00001730
Iteration 128/1000 | Loss: 0.00001729
Iteration 129/1000 | Loss: 0.00001729
Iteration 130/1000 | Loss: 0.00001729
Iteration 131/1000 | Loss: 0.00001729
Iteration 132/1000 | Loss: 0.00001729
Iteration 133/1000 | Loss: 0.00001729
Iteration 134/1000 | Loss: 0.00001728
Iteration 135/1000 | Loss: 0.00001728
Iteration 136/1000 | Loss: 0.00001728
Iteration 137/1000 | Loss: 0.00001728
Iteration 138/1000 | Loss: 0.00001728
Iteration 139/1000 | Loss: 0.00001728
Iteration 140/1000 | Loss: 0.00001728
Iteration 141/1000 | Loss: 0.00001727
Iteration 142/1000 | Loss: 0.00001727
Iteration 143/1000 | Loss: 0.00001727
Iteration 144/1000 | Loss: 0.00001727
Iteration 145/1000 | Loss: 0.00001727
Iteration 146/1000 | Loss: 0.00001727
Iteration 147/1000 | Loss: 0.00001727
Iteration 148/1000 | Loss: 0.00001727
Iteration 149/1000 | Loss: 0.00001727
Iteration 150/1000 | Loss: 0.00001727
Iteration 151/1000 | Loss: 0.00001727
Iteration 152/1000 | Loss: 0.00001727
Iteration 153/1000 | Loss: 0.00001727
Iteration 154/1000 | Loss: 0.00001727
Iteration 155/1000 | Loss: 0.00001727
Iteration 156/1000 | Loss: 0.00001727
Iteration 157/1000 | Loss: 0.00001727
Iteration 158/1000 | Loss: 0.00001727
Iteration 159/1000 | Loss: 0.00001727
Iteration 160/1000 | Loss: 0.00001727
Iteration 161/1000 | Loss: 0.00001727
Iteration 162/1000 | Loss: 0.00001727
Iteration 163/1000 | Loss: 0.00001727
Iteration 164/1000 | Loss: 0.00001727
Iteration 165/1000 | Loss: 0.00001727
Iteration 166/1000 | Loss: 0.00001727
Iteration 167/1000 | Loss: 0.00001727
Iteration 168/1000 | Loss: 0.00001727
Iteration 169/1000 | Loss: 0.00001727
Iteration 170/1000 | Loss: 0.00001727
Iteration 171/1000 | Loss: 0.00001727
Iteration 172/1000 | Loss: 0.00001727
Iteration 173/1000 | Loss: 0.00001727
Iteration 174/1000 | Loss: 0.00001727
Iteration 175/1000 | Loss: 0.00001727
Iteration 176/1000 | Loss: 0.00001727
Iteration 177/1000 | Loss: 0.00001727
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 177. Stopping optimization.
Last 5 losses: [1.726723712636158e-05, 1.726723712636158e-05, 1.726723712636158e-05, 1.726723712636158e-05, 1.726723712636158e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.726723712636158e-05

Optimization complete. Final v2v error: 3.4961564540863037 mm

Highest mean error: 4.177336692810059 mm for frame 158

Lowest mean error: 2.8356940746307373 mm for frame 146

Saving results

Total time: 37.12478709220886
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_30_nl_6559/0022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_nl_6559/0022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_nl_6559/0022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01107227
Iteration 2/25 | Loss: 0.00414288
Iteration 3/25 | Loss: 0.00236024
Iteration 4/25 | Loss: 0.00158851
Iteration 5/25 | Loss: 0.00146587
Iteration 6/25 | Loss: 0.00144068
Iteration 7/25 | Loss: 0.00144623
Iteration 8/25 | Loss: 0.00141772
Iteration 9/25 | Loss: 0.00139852
Iteration 10/25 | Loss: 0.00138477
Iteration 11/25 | Loss: 0.00139535
Iteration 12/25 | Loss: 0.00138299
Iteration 13/25 | Loss: 0.00136965
Iteration 14/25 | Loss: 0.00136070
Iteration 15/25 | Loss: 0.00135797
Iteration 16/25 | Loss: 0.00135660
Iteration 17/25 | Loss: 0.00135563
Iteration 18/25 | Loss: 0.00136474
Iteration 19/25 | Loss: 0.00135023
Iteration 20/25 | Loss: 0.00134618
Iteration 21/25 | Loss: 0.00134471
Iteration 22/25 | Loss: 0.00134415
Iteration 23/25 | Loss: 0.00134377
Iteration 24/25 | Loss: 0.00134352
Iteration 25/25 | Loss: 0.00134342

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 6.16483688
Iteration 2/25 | Loss: 0.00582221
Iteration 3/25 | Loss: 0.00582221
Iteration 4/25 | Loss: 0.00582221
Iteration 5/25 | Loss: 0.00582221
Iteration 6/25 | Loss: 0.00582221
Iteration 7/25 | Loss: 0.00582221
Iteration 8/25 | Loss: 0.00582221
Iteration 9/25 | Loss: 0.00582221
Iteration 10/25 | Loss: 0.00582221
Iteration 11/25 | Loss: 0.00582221
Iteration 12/25 | Loss: 0.00582221
Iteration 13/25 | Loss: 0.00582221
Iteration 14/25 | Loss: 0.00582221
Iteration 15/25 | Loss: 0.00582221
Iteration 16/25 | Loss: 0.00582221
Iteration 17/25 | Loss: 0.00582221
Iteration 18/25 | Loss: 0.00582221
Iteration 19/25 | Loss: 0.00582221
Iteration 20/25 | Loss: 0.00582221
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0058222063817083836, 0.0058222063817083836, 0.0058222063817083836, 0.0058222063817083836, 0.0058222063817083836]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0058222063817083836

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00582221
Iteration 2/1000 | Loss: 0.00278944
Iteration 3/1000 | Loss: 0.00980478
Iteration 4/1000 | Loss: 0.00177471
Iteration 5/1000 | Loss: 0.00758829
Iteration 6/1000 | Loss: 0.01088131
Iteration 7/1000 | Loss: 0.00171994
Iteration 8/1000 | Loss: 0.00638633
Iteration 9/1000 | Loss: 0.00457904
Iteration 10/1000 | Loss: 0.00199971
Iteration 11/1000 | Loss: 0.00586765
Iteration 12/1000 | Loss: 0.00106741
Iteration 13/1000 | Loss: 0.00139506
Iteration 14/1000 | Loss: 0.00172451
Iteration 15/1000 | Loss: 0.00109363
Iteration 16/1000 | Loss: 0.00158062
Iteration 17/1000 | Loss: 0.00049382
Iteration 18/1000 | Loss: 0.00014285
Iteration 19/1000 | Loss: 0.00115026
Iteration 20/1000 | Loss: 0.00476843
Iteration 21/1000 | Loss: 0.00030486
Iteration 22/1000 | Loss: 0.00331488
Iteration 23/1000 | Loss: 0.00025572
Iteration 24/1000 | Loss: 0.00206711
Iteration 25/1000 | Loss: 0.00078896
Iteration 26/1000 | Loss: 0.00112969
Iteration 27/1000 | Loss: 0.00045733
Iteration 28/1000 | Loss: 0.00029419
Iteration 29/1000 | Loss: 0.00009525
Iteration 30/1000 | Loss: 0.00008021
Iteration 31/1000 | Loss: 0.00033900
Iteration 32/1000 | Loss: 0.00030025
Iteration 33/1000 | Loss: 0.00079983
Iteration 34/1000 | Loss: 0.00045976
Iteration 35/1000 | Loss: 0.00043734
Iteration 36/1000 | Loss: 0.00311347
Iteration 37/1000 | Loss: 0.00114453
Iteration 38/1000 | Loss: 0.00029668
Iteration 39/1000 | Loss: 0.00023299
Iteration 40/1000 | Loss: 0.00012119
Iteration 41/1000 | Loss: 0.00005645
Iteration 42/1000 | Loss: 0.00047368
Iteration 43/1000 | Loss: 0.00005698
Iteration 44/1000 | Loss: 0.00004688
Iteration 45/1000 | Loss: 0.00101393
Iteration 46/1000 | Loss: 0.00111388
Iteration 47/1000 | Loss: 0.00101234
Iteration 48/1000 | Loss: 0.00010299
Iteration 49/1000 | Loss: 0.00005947
Iteration 50/1000 | Loss: 0.00004798
Iteration 51/1000 | Loss: 0.00021643
Iteration 52/1000 | Loss: 0.00005442
Iteration 53/1000 | Loss: 0.00011651
Iteration 54/1000 | Loss: 0.00012072
Iteration 55/1000 | Loss: 0.00003330
Iteration 56/1000 | Loss: 0.00003121
Iteration 57/1000 | Loss: 0.00003018
Iteration 58/1000 | Loss: 0.00002944
Iteration 59/1000 | Loss: 0.00002874
Iteration 60/1000 | Loss: 0.00002830
Iteration 61/1000 | Loss: 0.00002799
Iteration 62/1000 | Loss: 0.00002762
Iteration 63/1000 | Loss: 0.00002725
Iteration 64/1000 | Loss: 0.00002704
Iteration 65/1000 | Loss: 0.00002683
Iteration 66/1000 | Loss: 0.00002669
Iteration 67/1000 | Loss: 0.00002668
Iteration 68/1000 | Loss: 0.00002665
Iteration 69/1000 | Loss: 0.00002664
Iteration 70/1000 | Loss: 0.00002664
Iteration 71/1000 | Loss: 0.00002663
Iteration 72/1000 | Loss: 0.00002663
Iteration 73/1000 | Loss: 0.00002662
Iteration 74/1000 | Loss: 0.00002660
Iteration 75/1000 | Loss: 0.00002659
Iteration 76/1000 | Loss: 0.00002659
Iteration 77/1000 | Loss: 0.00002659
Iteration 78/1000 | Loss: 0.00002659
Iteration 79/1000 | Loss: 0.00002659
Iteration 80/1000 | Loss: 0.00002658
Iteration 81/1000 | Loss: 0.00002658
Iteration 82/1000 | Loss: 0.00002657
Iteration 83/1000 | Loss: 0.00002656
Iteration 84/1000 | Loss: 0.00002656
Iteration 85/1000 | Loss: 0.00002654
Iteration 86/1000 | Loss: 0.00002653
Iteration 87/1000 | Loss: 0.00002653
Iteration 88/1000 | Loss: 0.00002653
Iteration 89/1000 | Loss: 0.00002652
Iteration 90/1000 | Loss: 0.00002652
Iteration 91/1000 | Loss: 0.00002652
Iteration 92/1000 | Loss: 0.00002652
Iteration 93/1000 | Loss: 0.00002651
Iteration 94/1000 | Loss: 0.00002651
Iteration 95/1000 | Loss: 0.00002650
Iteration 96/1000 | Loss: 0.00002650
Iteration 97/1000 | Loss: 0.00002650
Iteration 98/1000 | Loss: 0.00002649
Iteration 99/1000 | Loss: 0.00002649
Iteration 100/1000 | Loss: 0.00002649
Iteration 101/1000 | Loss: 0.00002649
Iteration 102/1000 | Loss: 0.00002649
Iteration 103/1000 | Loss: 0.00002649
Iteration 104/1000 | Loss: 0.00002648
Iteration 105/1000 | Loss: 0.00002648
Iteration 106/1000 | Loss: 0.00002648
Iteration 107/1000 | Loss: 0.00002648
Iteration 108/1000 | Loss: 0.00002648
Iteration 109/1000 | Loss: 0.00002647
Iteration 110/1000 | Loss: 0.00002647
Iteration 111/1000 | Loss: 0.00002647
Iteration 112/1000 | Loss: 0.00002647
Iteration 113/1000 | Loss: 0.00002646
Iteration 114/1000 | Loss: 0.00002646
Iteration 115/1000 | Loss: 0.00002646
Iteration 116/1000 | Loss: 0.00002646
Iteration 117/1000 | Loss: 0.00002646
Iteration 118/1000 | Loss: 0.00002646
Iteration 119/1000 | Loss: 0.00002646
Iteration 120/1000 | Loss: 0.00002646
Iteration 121/1000 | Loss: 0.00002645
Iteration 122/1000 | Loss: 0.00002645
Iteration 123/1000 | Loss: 0.00002645
Iteration 124/1000 | Loss: 0.00002645
Iteration 125/1000 | Loss: 0.00002645
Iteration 126/1000 | Loss: 0.00002645
Iteration 127/1000 | Loss: 0.00002645
Iteration 128/1000 | Loss: 0.00002645
Iteration 129/1000 | Loss: 0.00002645
Iteration 130/1000 | Loss: 0.00002645
Iteration 131/1000 | Loss: 0.00002644
Iteration 132/1000 | Loss: 0.00002644
Iteration 133/1000 | Loss: 0.00002644
Iteration 134/1000 | Loss: 0.00002644
Iteration 135/1000 | Loss: 0.00002644
Iteration 136/1000 | Loss: 0.00002644
Iteration 137/1000 | Loss: 0.00002644
Iteration 138/1000 | Loss: 0.00002644
Iteration 139/1000 | Loss: 0.00002644
Iteration 140/1000 | Loss: 0.00002644
Iteration 141/1000 | Loss: 0.00002644
Iteration 142/1000 | Loss: 0.00002643
Iteration 143/1000 | Loss: 0.00002643
Iteration 144/1000 | Loss: 0.00002643
Iteration 145/1000 | Loss: 0.00002643
Iteration 146/1000 | Loss: 0.00002643
Iteration 147/1000 | Loss: 0.00002643
Iteration 148/1000 | Loss: 0.00002643
Iteration 149/1000 | Loss: 0.00002643
Iteration 150/1000 | Loss: 0.00002642
Iteration 151/1000 | Loss: 0.00002642
Iteration 152/1000 | Loss: 0.00002642
Iteration 153/1000 | Loss: 0.00002642
Iteration 154/1000 | Loss: 0.00002642
Iteration 155/1000 | Loss: 0.00002642
Iteration 156/1000 | Loss: 0.00002642
Iteration 157/1000 | Loss: 0.00002642
Iteration 158/1000 | Loss: 0.00002641
Iteration 159/1000 | Loss: 0.00002641
Iteration 160/1000 | Loss: 0.00002641
Iteration 161/1000 | Loss: 0.00002641
Iteration 162/1000 | Loss: 0.00002641
Iteration 163/1000 | Loss: 0.00002641
Iteration 164/1000 | Loss: 0.00002641
Iteration 165/1000 | Loss: 0.00002641
Iteration 166/1000 | Loss: 0.00002641
Iteration 167/1000 | Loss: 0.00002641
Iteration 168/1000 | Loss: 0.00002641
Iteration 169/1000 | Loss: 0.00002641
Iteration 170/1000 | Loss: 0.00002640
Iteration 171/1000 | Loss: 0.00002640
Iteration 172/1000 | Loss: 0.00002640
Iteration 173/1000 | Loss: 0.00002640
Iteration 174/1000 | Loss: 0.00002640
Iteration 175/1000 | Loss: 0.00002640
Iteration 176/1000 | Loss: 0.00002640
Iteration 177/1000 | Loss: 0.00002640
Iteration 178/1000 | Loss: 0.00002640
Iteration 179/1000 | Loss: 0.00002640
Iteration 180/1000 | Loss: 0.00002640
Iteration 181/1000 | Loss: 0.00002639
Iteration 182/1000 | Loss: 0.00002639
Iteration 183/1000 | Loss: 0.00002639
Iteration 184/1000 | Loss: 0.00002639
Iteration 185/1000 | Loss: 0.00002639
Iteration 186/1000 | Loss: 0.00002639
Iteration 187/1000 | Loss: 0.00002639
Iteration 188/1000 | Loss: 0.00002639
Iteration 189/1000 | Loss: 0.00002639
Iteration 190/1000 | Loss: 0.00002639
Iteration 191/1000 | Loss: 0.00002639
Iteration 192/1000 | Loss: 0.00002639
Iteration 193/1000 | Loss: 0.00002638
Iteration 194/1000 | Loss: 0.00002638
Iteration 195/1000 | Loss: 0.00002638
Iteration 196/1000 | Loss: 0.00002638
Iteration 197/1000 | Loss: 0.00002638
Iteration 198/1000 | Loss: 0.00002638
Iteration 199/1000 | Loss: 0.00002637
Iteration 200/1000 | Loss: 0.00002637
Iteration 201/1000 | Loss: 0.00002637
Iteration 202/1000 | Loss: 0.00002637
Iteration 203/1000 | Loss: 0.00002637
Iteration 204/1000 | Loss: 0.00002637
Iteration 205/1000 | Loss: 0.00002637
Iteration 206/1000 | Loss: 0.00002637
Iteration 207/1000 | Loss: 0.00002637
Iteration 208/1000 | Loss: 0.00002636
Iteration 209/1000 | Loss: 0.00002636
Iteration 210/1000 | Loss: 0.00002636
Iteration 211/1000 | Loss: 0.00002636
Iteration 212/1000 | Loss: 0.00002636
Iteration 213/1000 | Loss: 0.00002636
Iteration 214/1000 | Loss: 0.00002636
Iteration 215/1000 | Loss: 0.00002636
Iteration 216/1000 | Loss: 0.00002636
Iteration 217/1000 | Loss: 0.00002636
Iteration 218/1000 | Loss: 0.00002636
Iteration 219/1000 | Loss: 0.00002636
Iteration 220/1000 | Loss: 0.00002636
Iteration 221/1000 | Loss: 0.00002636
Iteration 222/1000 | Loss: 0.00002636
Iteration 223/1000 | Loss: 0.00002636
Iteration 224/1000 | Loss: 0.00002636
Iteration 225/1000 | Loss: 0.00002636
Iteration 226/1000 | Loss: 0.00002636
Iteration 227/1000 | Loss: 0.00002636
Iteration 228/1000 | Loss: 0.00002636
Iteration 229/1000 | Loss: 0.00002636
Iteration 230/1000 | Loss: 0.00002636
Iteration 231/1000 | Loss: 0.00002636
Iteration 232/1000 | Loss: 0.00002636
Iteration 233/1000 | Loss: 0.00002636
Iteration 234/1000 | Loss: 0.00002636
Iteration 235/1000 | Loss: 0.00002636
Iteration 236/1000 | Loss: 0.00002636
Iteration 237/1000 | Loss: 0.00002636
Iteration 238/1000 | Loss: 0.00002636
Iteration 239/1000 | Loss: 0.00002636
Iteration 240/1000 | Loss: 0.00002636
Iteration 241/1000 | Loss: 0.00002636
Iteration 242/1000 | Loss: 0.00002636
Iteration 243/1000 | Loss: 0.00002636
Iteration 244/1000 | Loss: 0.00002636
Iteration 245/1000 | Loss: 0.00002636
Iteration 246/1000 | Loss: 0.00002636
Iteration 247/1000 | Loss: 0.00002636
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 247. Stopping optimization.
Last 5 losses: [2.635752571222838e-05, 2.635752571222838e-05, 2.635752571222838e-05, 2.635752571222838e-05, 2.635752571222838e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.635752571222838e-05

Optimization complete. Final v2v error: 4.232778072357178 mm

Highest mean error: 5.964015960693359 mm for frame 81

Lowest mean error: 3.0984232425689697 mm for frame 0

Saving results

Total time: 148.516695022583
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_30_nl_6559/0021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_nl_6559/0021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_nl_6559/0021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00930013
Iteration 2/25 | Loss: 0.00151771
Iteration 3/25 | Loss: 0.00130443
Iteration 4/25 | Loss: 0.00120674
Iteration 5/25 | Loss: 0.00118541
Iteration 6/25 | Loss: 0.00120705
Iteration 7/25 | Loss: 0.00116861
Iteration 8/25 | Loss: 0.00116426
Iteration 9/25 | Loss: 0.00115348
Iteration 10/25 | Loss: 0.00114583
Iteration 11/25 | Loss: 0.00113670
Iteration 12/25 | Loss: 0.00113157
Iteration 13/25 | Loss: 0.00112245
Iteration 14/25 | Loss: 0.00112541
Iteration 15/25 | Loss: 0.00112150
Iteration 16/25 | Loss: 0.00111693
Iteration 17/25 | Loss: 0.00111567
Iteration 18/25 | Loss: 0.00111403
Iteration 19/25 | Loss: 0.00112219
Iteration 20/25 | Loss: 0.00111807
Iteration 21/25 | Loss: 0.00111180
Iteration 22/25 | Loss: 0.00111361
Iteration 23/25 | Loss: 0.00111275
Iteration 24/25 | Loss: 0.00111258
Iteration 25/25 | Loss: 0.00111283

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.55547357
Iteration 2/25 | Loss: 0.00336648
Iteration 3/25 | Loss: 0.00336645
Iteration 4/25 | Loss: 0.00336645
Iteration 5/25 | Loss: 0.00336645
Iteration 6/25 | Loss: 0.00336645
Iteration 7/25 | Loss: 0.00336645
Iteration 8/25 | Loss: 0.00336645
Iteration 9/25 | Loss: 0.00336645
Iteration 10/25 | Loss: 0.00336645
Iteration 11/25 | Loss: 0.00336645
Iteration 12/25 | Loss: 0.00336645
Iteration 13/25 | Loss: 0.00336645
Iteration 14/25 | Loss: 0.00336645
Iteration 15/25 | Loss: 0.00336645
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.00336644914932549, 0.00336644914932549, 0.00336644914932549, 0.00336644914932549, 0.00336644914932549]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00336644914932549

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00336645
Iteration 2/1000 | Loss: 0.00061149
Iteration 3/1000 | Loss: 0.00034739
Iteration 4/1000 | Loss: 0.00048593
Iteration 5/1000 | Loss: 0.00032509
Iteration 6/1000 | Loss: 0.00024982
Iteration 7/1000 | Loss: 0.00058457
Iteration 8/1000 | Loss: 0.00219203
Iteration 9/1000 | Loss: 0.00138089
Iteration 10/1000 | Loss: 0.00411139
Iteration 11/1000 | Loss: 0.00382065
Iteration 12/1000 | Loss: 0.00222281
Iteration 13/1000 | Loss: 0.00076296
Iteration 14/1000 | Loss: 0.00021215
Iteration 15/1000 | Loss: 0.00052110
Iteration 16/1000 | Loss: 0.00015109
Iteration 17/1000 | Loss: 0.00089607
Iteration 18/1000 | Loss: 0.00086177
Iteration 19/1000 | Loss: 0.00138787
Iteration 20/1000 | Loss: 0.00124152
Iteration 21/1000 | Loss: 0.00021349
Iteration 22/1000 | Loss: 0.00127680
Iteration 23/1000 | Loss: 0.00021402
Iteration 24/1000 | Loss: 0.00048676
Iteration 25/1000 | Loss: 0.00056384
Iteration 26/1000 | Loss: 0.00009320
Iteration 27/1000 | Loss: 0.00049623
Iteration 28/1000 | Loss: 0.00008496
Iteration 29/1000 | Loss: 0.00007279
Iteration 30/1000 | Loss: 0.00099124
Iteration 31/1000 | Loss: 0.00047814
Iteration 32/1000 | Loss: 0.00199978
Iteration 33/1000 | Loss: 0.00061043
Iteration 34/1000 | Loss: 0.00007793
Iteration 35/1000 | Loss: 0.00006736
Iteration 36/1000 | Loss: 0.00034376
Iteration 37/1000 | Loss: 0.00006632
Iteration 38/1000 | Loss: 0.00054588
Iteration 39/1000 | Loss: 0.00018688
Iteration 40/1000 | Loss: 0.00050462
Iteration 41/1000 | Loss: 0.00006176
Iteration 42/1000 | Loss: 0.00060834
Iteration 43/1000 | Loss: 0.00022096
Iteration 44/1000 | Loss: 0.00067207
Iteration 45/1000 | Loss: 0.00030487
Iteration 46/1000 | Loss: 0.00024717
Iteration 47/1000 | Loss: 0.00113390
Iteration 48/1000 | Loss: 0.00032747
Iteration 49/1000 | Loss: 0.00018421
Iteration 50/1000 | Loss: 0.00006303
Iteration 51/1000 | Loss: 0.00059092
Iteration 52/1000 | Loss: 0.00065252
Iteration 53/1000 | Loss: 0.00025191
Iteration 54/1000 | Loss: 0.00012483
Iteration 55/1000 | Loss: 0.00006681
Iteration 56/1000 | Loss: 0.00005457
Iteration 57/1000 | Loss: 0.00052215
Iteration 58/1000 | Loss: 0.00014723
Iteration 59/1000 | Loss: 0.00040546
Iteration 60/1000 | Loss: 0.00016109
Iteration 61/1000 | Loss: 0.00013781
Iteration 62/1000 | Loss: 0.00004862
Iteration 63/1000 | Loss: 0.00004335
Iteration 64/1000 | Loss: 0.00004040
Iteration 65/1000 | Loss: 0.00003852
Iteration 66/1000 | Loss: 0.00003706
Iteration 67/1000 | Loss: 0.00003635
Iteration 68/1000 | Loss: 0.00003577
Iteration 69/1000 | Loss: 0.00003530
Iteration 70/1000 | Loss: 0.00003487
Iteration 71/1000 | Loss: 0.00003461
Iteration 72/1000 | Loss: 0.00003434
Iteration 73/1000 | Loss: 0.00003416
Iteration 74/1000 | Loss: 0.00003415
Iteration 75/1000 | Loss: 0.00003404
Iteration 76/1000 | Loss: 0.00003402
Iteration 77/1000 | Loss: 0.00003390
Iteration 78/1000 | Loss: 0.00003388
Iteration 79/1000 | Loss: 0.00003387
Iteration 80/1000 | Loss: 0.00003385
Iteration 81/1000 | Loss: 0.00003382
Iteration 82/1000 | Loss: 0.00003382
Iteration 83/1000 | Loss: 0.00003380
Iteration 84/1000 | Loss: 0.00003379
Iteration 85/1000 | Loss: 0.00003379
Iteration 86/1000 | Loss: 0.00003378
Iteration 87/1000 | Loss: 0.00003378
Iteration 88/1000 | Loss: 0.00003377
Iteration 89/1000 | Loss: 0.00003377
Iteration 90/1000 | Loss: 0.00003376
Iteration 91/1000 | Loss: 0.00003376
Iteration 92/1000 | Loss: 0.00003375
Iteration 93/1000 | Loss: 0.00003375
Iteration 94/1000 | Loss: 0.00003374
Iteration 95/1000 | Loss: 0.00003374
Iteration 96/1000 | Loss: 0.00003374
Iteration 97/1000 | Loss: 0.00003373
Iteration 98/1000 | Loss: 0.00003373
Iteration 99/1000 | Loss: 0.00003372
Iteration 100/1000 | Loss: 0.00003372
Iteration 101/1000 | Loss: 0.00003372
Iteration 102/1000 | Loss: 0.00003371
Iteration 103/1000 | Loss: 0.00003371
Iteration 104/1000 | Loss: 0.00003371
Iteration 105/1000 | Loss: 0.00003371
Iteration 106/1000 | Loss: 0.00003371
Iteration 107/1000 | Loss: 0.00003371
Iteration 108/1000 | Loss: 0.00003371
Iteration 109/1000 | Loss: 0.00003371
Iteration 110/1000 | Loss: 0.00003370
Iteration 111/1000 | Loss: 0.00003370
Iteration 112/1000 | Loss: 0.00003370
Iteration 113/1000 | Loss: 0.00003370
Iteration 114/1000 | Loss: 0.00003370
Iteration 115/1000 | Loss: 0.00003370
Iteration 116/1000 | Loss: 0.00003370
Iteration 117/1000 | Loss: 0.00003370
Iteration 118/1000 | Loss: 0.00003370
Iteration 119/1000 | Loss: 0.00003370
Iteration 120/1000 | Loss: 0.00003369
Iteration 121/1000 | Loss: 0.00003369
Iteration 122/1000 | Loss: 0.00003369
Iteration 123/1000 | Loss: 0.00003369
Iteration 124/1000 | Loss: 0.00003369
Iteration 125/1000 | Loss: 0.00003369
Iteration 126/1000 | Loss: 0.00003369
Iteration 127/1000 | Loss: 0.00003368
Iteration 128/1000 | Loss: 0.00003368
Iteration 129/1000 | Loss: 0.00003368
Iteration 130/1000 | Loss: 0.00003368
Iteration 131/1000 | Loss: 0.00003368
Iteration 132/1000 | Loss: 0.00003368
Iteration 133/1000 | Loss: 0.00003367
Iteration 134/1000 | Loss: 0.00003367
Iteration 135/1000 | Loss: 0.00003367
Iteration 136/1000 | Loss: 0.00003367
Iteration 137/1000 | Loss: 0.00003367
Iteration 138/1000 | Loss: 0.00003367
Iteration 139/1000 | Loss: 0.00003366
Iteration 140/1000 | Loss: 0.00003366
Iteration 141/1000 | Loss: 0.00003366
Iteration 142/1000 | Loss: 0.00003366
Iteration 143/1000 | Loss: 0.00003366
Iteration 144/1000 | Loss: 0.00003366
Iteration 145/1000 | Loss: 0.00003366
Iteration 146/1000 | Loss: 0.00003366
Iteration 147/1000 | Loss: 0.00003366
Iteration 148/1000 | Loss: 0.00003366
Iteration 149/1000 | Loss: 0.00003365
Iteration 150/1000 | Loss: 0.00003365
Iteration 151/1000 | Loss: 0.00003365
Iteration 152/1000 | Loss: 0.00003365
Iteration 153/1000 | Loss: 0.00003365
Iteration 154/1000 | Loss: 0.00003365
Iteration 155/1000 | Loss: 0.00003365
Iteration 156/1000 | Loss: 0.00003364
Iteration 157/1000 | Loss: 0.00003364
Iteration 158/1000 | Loss: 0.00003364
Iteration 159/1000 | Loss: 0.00003364
Iteration 160/1000 | Loss: 0.00003364
Iteration 161/1000 | Loss: 0.00003364
Iteration 162/1000 | Loss: 0.00003364
Iteration 163/1000 | Loss: 0.00003363
Iteration 164/1000 | Loss: 0.00003363
Iteration 165/1000 | Loss: 0.00003363
Iteration 166/1000 | Loss: 0.00003363
Iteration 167/1000 | Loss: 0.00003363
Iteration 168/1000 | Loss: 0.00003363
Iteration 169/1000 | Loss: 0.00003363
Iteration 170/1000 | Loss: 0.00003363
Iteration 171/1000 | Loss: 0.00003363
Iteration 172/1000 | Loss: 0.00003363
Iteration 173/1000 | Loss: 0.00003363
Iteration 174/1000 | Loss: 0.00003363
Iteration 175/1000 | Loss: 0.00003362
Iteration 176/1000 | Loss: 0.00003362
Iteration 177/1000 | Loss: 0.00003362
Iteration 178/1000 | Loss: 0.00003362
Iteration 179/1000 | Loss: 0.00003362
Iteration 180/1000 | Loss: 0.00003362
Iteration 181/1000 | Loss: 0.00003362
Iteration 182/1000 | Loss: 0.00003362
Iteration 183/1000 | Loss: 0.00003362
Iteration 184/1000 | Loss: 0.00003362
Iteration 185/1000 | Loss: 0.00003362
Iteration 186/1000 | Loss: 0.00003361
Iteration 187/1000 | Loss: 0.00003361
Iteration 188/1000 | Loss: 0.00003361
Iteration 189/1000 | Loss: 0.00003361
Iteration 190/1000 | Loss: 0.00003361
Iteration 191/1000 | Loss: 0.00003361
Iteration 192/1000 | Loss: 0.00003361
Iteration 193/1000 | Loss: 0.00003361
Iteration 194/1000 | Loss: 0.00003360
Iteration 195/1000 | Loss: 0.00003360
Iteration 196/1000 | Loss: 0.00003360
Iteration 197/1000 | Loss: 0.00003360
Iteration 198/1000 | Loss: 0.00003360
Iteration 199/1000 | Loss: 0.00003360
Iteration 200/1000 | Loss: 0.00003360
Iteration 201/1000 | Loss: 0.00003359
Iteration 202/1000 | Loss: 0.00003359
Iteration 203/1000 | Loss: 0.00003359
Iteration 204/1000 | Loss: 0.00003359
Iteration 205/1000 | Loss: 0.00003359
Iteration 206/1000 | Loss: 0.00003359
Iteration 207/1000 | Loss: 0.00003359
Iteration 208/1000 | Loss: 0.00003359
Iteration 209/1000 | Loss: 0.00003359
Iteration 210/1000 | Loss: 0.00003359
Iteration 211/1000 | Loss: 0.00003358
Iteration 212/1000 | Loss: 0.00003358
Iteration 213/1000 | Loss: 0.00003358
Iteration 214/1000 | Loss: 0.00003358
Iteration 215/1000 | Loss: 0.00003358
Iteration 216/1000 | Loss: 0.00003358
Iteration 217/1000 | Loss: 0.00003358
Iteration 218/1000 | Loss: 0.00003358
Iteration 219/1000 | Loss: 0.00003358
Iteration 220/1000 | Loss: 0.00003358
Iteration 221/1000 | Loss: 0.00003358
Iteration 222/1000 | Loss: 0.00003358
Iteration 223/1000 | Loss: 0.00003358
Iteration 224/1000 | Loss: 0.00003358
Iteration 225/1000 | Loss: 0.00003358
Iteration 226/1000 | Loss: 0.00003358
Iteration 227/1000 | Loss: 0.00003358
Iteration 228/1000 | Loss: 0.00003358
Iteration 229/1000 | Loss: 0.00003358
Iteration 230/1000 | Loss: 0.00003358
Iteration 231/1000 | Loss: 0.00003358
Iteration 232/1000 | Loss: 0.00003358
Iteration 233/1000 | Loss: 0.00003358
Iteration 234/1000 | Loss: 0.00003358
Iteration 235/1000 | Loss: 0.00003358
Iteration 236/1000 | Loss: 0.00003358
Iteration 237/1000 | Loss: 0.00003358
Iteration 238/1000 | Loss: 0.00003358
Iteration 239/1000 | Loss: 0.00003358
Iteration 240/1000 | Loss: 0.00003358
Iteration 241/1000 | Loss: 0.00003358
Iteration 242/1000 | Loss: 0.00003358
Iteration 243/1000 | Loss: 0.00003358
Iteration 244/1000 | Loss: 0.00003358
Iteration 245/1000 | Loss: 0.00003358
Iteration 246/1000 | Loss: 0.00003358
Iteration 247/1000 | Loss: 0.00003358
Iteration 248/1000 | Loss: 0.00003358
Iteration 249/1000 | Loss: 0.00003358
Iteration 250/1000 | Loss: 0.00003358
Iteration 251/1000 | Loss: 0.00003358
Iteration 252/1000 | Loss: 0.00003358
Iteration 253/1000 | Loss: 0.00003358
Iteration 254/1000 | Loss: 0.00003358
Iteration 255/1000 | Loss: 0.00003358
Iteration 256/1000 | Loss: 0.00003358
Iteration 257/1000 | Loss: 0.00003358
Iteration 258/1000 | Loss: 0.00003358
Iteration 259/1000 | Loss: 0.00003358
Iteration 260/1000 | Loss: 0.00003358
Iteration 261/1000 | Loss: 0.00003358
Iteration 262/1000 | Loss: 0.00003358
Iteration 263/1000 | Loss: 0.00003358
Iteration 264/1000 | Loss: 0.00003358
Iteration 265/1000 | Loss: 0.00003358
Iteration 266/1000 | Loss: 0.00003358
Iteration 267/1000 | Loss: 0.00003358
Iteration 268/1000 | Loss: 0.00003358
Iteration 269/1000 | Loss: 0.00003358
Iteration 270/1000 | Loss: 0.00003358
Iteration 271/1000 | Loss: 0.00003358
Iteration 272/1000 | Loss: 0.00003358
Iteration 273/1000 | Loss: 0.00003358
Iteration 274/1000 | Loss: 0.00003358
Iteration 275/1000 | Loss: 0.00003358
Iteration 276/1000 | Loss: 0.00003358
Iteration 277/1000 | Loss: 0.00003358
Iteration 278/1000 | Loss: 0.00003358
Iteration 279/1000 | Loss: 0.00003358
Iteration 280/1000 | Loss: 0.00003358
Iteration 281/1000 | Loss: 0.00003358
Iteration 282/1000 | Loss: 0.00003358
Iteration 283/1000 | Loss: 0.00003358
Iteration 284/1000 | Loss: 0.00003358
Iteration 285/1000 | Loss: 0.00003358
Iteration 286/1000 | Loss: 0.00003358
Iteration 287/1000 | Loss: 0.00003358
Iteration 288/1000 | Loss: 0.00003358
Iteration 289/1000 | Loss: 0.00003358
Iteration 290/1000 | Loss: 0.00003358
Iteration 291/1000 | Loss: 0.00003358
Iteration 292/1000 | Loss: 0.00003358
Iteration 293/1000 | Loss: 0.00003358
Iteration 294/1000 | Loss: 0.00003358
Iteration 295/1000 | Loss: 0.00003358
Iteration 296/1000 | Loss: 0.00003358
Iteration 297/1000 | Loss: 0.00003358
Iteration 298/1000 | Loss: 0.00003358
Iteration 299/1000 | Loss: 0.00003358
Iteration 300/1000 | Loss: 0.00003358
Iteration 301/1000 | Loss: 0.00003358
Iteration 302/1000 | Loss: 0.00003358
Iteration 303/1000 | Loss: 0.00003358
Iteration 304/1000 | Loss: 0.00003358
Iteration 305/1000 | Loss: 0.00003358
Iteration 306/1000 | Loss: 0.00003358
Iteration 307/1000 | Loss: 0.00003358
Iteration 308/1000 | Loss: 0.00003358
Iteration 309/1000 | Loss: 0.00003358
Iteration 310/1000 | Loss: 0.00003358
Iteration 311/1000 | Loss: 0.00003358
Iteration 312/1000 | Loss: 0.00003358
Iteration 313/1000 | Loss: 0.00003358
Iteration 314/1000 | Loss: 0.00003358
Iteration 315/1000 | Loss: 0.00003358
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 315. Stopping optimization.
Last 5 losses: [3.358224421390332e-05, 3.358224421390332e-05, 3.358224421390332e-05, 3.358224421390332e-05, 3.358224421390332e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.358224421390332e-05

Optimization complete. Final v2v error: 4.495119571685791 mm

Highest mean error: 13.630742073059082 mm for frame 111

Lowest mean error: 3.918111801147461 mm for frame 43

Saving results

Total time: 168.41862726211548
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_30_nl_6559/0013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_nl_6559/0013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_nl_6559/0013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00881758
Iteration 2/25 | Loss: 0.00117718
Iteration 3/25 | Loss: 0.00102156
Iteration 4/25 | Loss: 0.00097638
Iteration 5/25 | Loss: 0.00096808
Iteration 6/25 | Loss: 0.00096528
Iteration 7/25 | Loss: 0.00096407
Iteration 8/25 | Loss: 0.00096399
Iteration 9/25 | Loss: 0.00096399
Iteration 10/25 | Loss: 0.00096399
Iteration 11/25 | Loss: 0.00096399
Iteration 12/25 | Loss: 0.00096399
Iteration 13/25 | Loss: 0.00096399
Iteration 14/25 | Loss: 0.00096399
Iteration 15/25 | Loss: 0.00096399
Iteration 16/25 | Loss: 0.00096399
Iteration 17/25 | Loss: 0.00096399
Iteration 18/25 | Loss: 0.00096399
Iteration 19/25 | Loss: 0.00096399
Iteration 20/25 | Loss: 0.00096399
Iteration 21/25 | Loss: 0.00096399
Iteration 22/25 | Loss: 0.00096399
Iteration 23/25 | Loss: 0.00096399
Iteration 24/25 | Loss: 0.00096399
Iteration 25/25 | Loss: 0.00096399

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35764039
Iteration 2/25 | Loss: 0.00160088
Iteration 3/25 | Loss: 0.00160088
Iteration 4/25 | Loss: 0.00160088
Iteration 5/25 | Loss: 0.00160088
Iteration 6/25 | Loss: 0.00160088
Iteration 7/25 | Loss: 0.00160088
Iteration 8/25 | Loss: 0.00160087
Iteration 9/25 | Loss: 0.00160087
Iteration 10/25 | Loss: 0.00160087
Iteration 11/25 | Loss: 0.00160087
Iteration 12/25 | Loss: 0.00160087
Iteration 13/25 | Loss: 0.00160087
Iteration 14/25 | Loss: 0.00160087
Iteration 15/25 | Loss: 0.00160087
Iteration 16/25 | Loss: 0.00160087
Iteration 17/25 | Loss: 0.00160087
Iteration 18/25 | Loss: 0.00160087
Iteration 19/25 | Loss: 0.00160087
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0016008743550628424, 0.0016008743550628424, 0.0016008743550628424, 0.0016008743550628424, 0.0016008743550628424]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0016008743550628424

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00160087
Iteration 2/1000 | Loss: 0.00004287
Iteration 3/1000 | Loss: 0.00003303
Iteration 4/1000 | Loss: 0.00003049
Iteration 5/1000 | Loss: 0.00002978
Iteration 6/1000 | Loss: 0.00002878
Iteration 7/1000 | Loss: 0.00002827
Iteration 8/1000 | Loss: 0.00002765
Iteration 9/1000 | Loss: 0.00002728
Iteration 10/1000 | Loss: 0.00002704
Iteration 11/1000 | Loss: 0.00002690
Iteration 12/1000 | Loss: 0.00002674
Iteration 13/1000 | Loss: 0.00002670
Iteration 14/1000 | Loss: 0.00002669
Iteration 15/1000 | Loss: 0.00002664
Iteration 16/1000 | Loss: 0.00002664
Iteration 17/1000 | Loss: 0.00002663
Iteration 18/1000 | Loss: 0.00002663
Iteration 19/1000 | Loss: 0.00002659
Iteration 20/1000 | Loss: 0.00002658
Iteration 21/1000 | Loss: 0.00002658
Iteration 22/1000 | Loss: 0.00002657
Iteration 23/1000 | Loss: 0.00002657
Iteration 24/1000 | Loss: 0.00002657
Iteration 25/1000 | Loss: 0.00002657
Iteration 26/1000 | Loss: 0.00002656
Iteration 27/1000 | Loss: 0.00002656
Iteration 28/1000 | Loss: 0.00002654
Iteration 29/1000 | Loss: 0.00002654
Iteration 30/1000 | Loss: 0.00002654
Iteration 31/1000 | Loss: 0.00002654
Iteration 32/1000 | Loss: 0.00002653
Iteration 33/1000 | Loss: 0.00002653
Iteration 34/1000 | Loss: 0.00002653
Iteration 35/1000 | Loss: 0.00002653
Iteration 36/1000 | Loss: 0.00002653
Iteration 37/1000 | Loss: 0.00002653
Iteration 38/1000 | Loss: 0.00002652
Iteration 39/1000 | Loss: 0.00002652
Iteration 40/1000 | Loss: 0.00002652
Iteration 41/1000 | Loss: 0.00002652
Iteration 42/1000 | Loss: 0.00002652
Iteration 43/1000 | Loss: 0.00002652
Iteration 44/1000 | Loss: 0.00002652
Iteration 45/1000 | Loss: 0.00002651
Iteration 46/1000 | Loss: 0.00002651
Iteration 47/1000 | Loss: 0.00002651
Iteration 48/1000 | Loss: 0.00002651
Iteration 49/1000 | Loss: 0.00002651
Iteration 50/1000 | Loss: 0.00002651
Iteration 51/1000 | Loss: 0.00002651
Iteration 52/1000 | Loss: 0.00002651
Iteration 53/1000 | Loss: 0.00002651
Iteration 54/1000 | Loss: 0.00002650
Iteration 55/1000 | Loss: 0.00002650
Iteration 56/1000 | Loss: 0.00002650
Iteration 57/1000 | Loss: 0.00002650
Iteration 58/1000 | Loss: 0.00002650
Iteration 59/1000 | Loss: 0.00002650
Iteration 60/1000 | Loss: 0.00002650
Iteration 61/1000 | Loss: 0.00002650
Iteration 62/1000 | Loss: 0.00002650
Iteration 63/1000 | Loss: 0.00002650
Iteration 64/1000 | Loss: 0.00002650
Iteration 65/1000 | Loss: 0.00002649
Iteration 66/1000 | Loss: 0.00002649
Iteration 67/1000 | Loss: 0.00002649
Iteration 68/1000 | Loss: 0.00002649
Iteration 69/1000 | Loss: 0.00002649
Iteration 70/1000 | Loss: 0.00002649
Iteration 71/1000 | Loss: 0.00002649
Iteration 72/1000 | Loss: 0.00002649
Iteration 73/1000 | Loss: 0.00002649
Iteration 74/1000 | Loss: 0.00002648
Iteration 75/1000 | Loss: 0.00002648
Iteration 76/1000 | Loss: 0.00002648
Iteration 77/1000 | Loss: 0.00002648
Iteration 78/1000 | Loss: 0.00002648
Iteration 79/1000 | Loss: 0.00002648
Iteration 80/1000 | Loss: 0.00002648
Iteration 81/1000 | Loss: 0.00002648
Iteration 82/1000 | Loss: 0.00002648
Iteration 83/1000 | Loss: 0.00002648
Iteration 84/1000 | Loss: 0.00002648
Iteration 85/1000 | Loss: 0.00002647
Iteration 86/1000 | Loss: 0.00002647
Iteration 87/1000 | Loss: 0.00002647
Iteration 88/1000 | Loss: 0.00002647
Iteration 89/1000 | Loss: 0.00002647
Iteration 90/1000 | Loss: 0.00002647
Iteration 91/1000 | Loss: 0.00002647
Iteration 92/1000 | Loss: 0.00002647
Iteration 93/1000 | Loss: 0.00002647
Iteration 94/1000 | Loss: 0.00002647
Iteration 95/1000 | Loss: 0.00002647
Iteration 96/1000 | Loss: 0.00002647
Iteration 97/1000 | Loss: 0.00002647
Iteration 98/1000 | Loss: 0.00002647
Iteration 99/1000 | Loss: 0.00002647
Iteration 100/1000 | Loss: 0.00002647
Iteration 101/1000 | Loss: 0.00002647
Iteration 102/1000 | Loss: 0.00002647
Iteration 103/1000 | Loss: 0.00002647
Iteration 104/1000 | Loss: 0.00002647
Iteration 105/1000 | Loss: 0.00002647
Iteration 106/1000 | Loss: 0.00002647
Iteration 107/1000 | Loss: 0.00002647
Iteration 108/1000 | Loss: 0.00002647
Iteration 109/1000 | Loss: 0.00002647
Iteration 110/1000 | Loss: 0.00002647
Iteration 111/1000 | Loss: 0.00002647
Iteration 112/1000 | Loss: 0.00002647
Iteration 113/1000 | Loss: 0.00002647
Iteration 114/1000 | Loss: 0.00002647
Iteration 115/1000 | Loss: 0.00002647
Iteration 116/1000 | Loss: 0.00002647
Iteration 117/1000 | Loss: 0.00002647
Iteration 118/1000 | Loss: 0.00002647
Iteration 119/1000 | Loss: 0.00002647
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 119. Stopping optimization.
Last 5 losses: [2.6474368496565148e-05, 2.6474368496565148e-05, 2.6474368496565148e-05, 2.6474368496565148e-05, 2.6474368496565148e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.6474368496565148e-05

Optimization complete. Final v2v error: 4.26729679107666 mm

Highest mean error: 4.5242767333984375 mm for frame 27

Lowest mean error: 3.763556480407715 mm for frame 192

Saving results

Total time: 36.54330372810364
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_30_nl_6559/0005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_nl_6559/0005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_nl_6559/0005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00801309
Iteration 2/25 | Loss: 0.00111269
Iteration 3/25 | Loss: 0.00090935
Iteration 4/25 | Loss: 0.00088525
Iteration 5/25 | Loss: 0.00087930
Iteration 6/25 | Loss: 0.00087776
Iteration 7/25 | Loss: 0.00087735
Iteration 8/25 | Loss: 0.00087735
Iteration 9/25 | Loss: 0.00087735
Iteration 10/25 | Loss: 0.00087735
Iteration 11/25 | Loss: 0.00087735
Iteration 12/25 | Loss: 0.00087735
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0008773506269790232, 0.0008773506269790232, 0.0008773506269790232, 0.0008773506269790232, 0.0008773506269790232]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008773506269790232

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.61377859
Iteration 2/25 | Loss: 0.00119235
Iteration 3/25 | Loss: 0.00119234
Iteration 4/25 | Loss: 0.00119234
Iteration 5/25 | Loss: 0.00119234
Iteration 6/25 | Loss: 0.00119234
Iteration 7/25 | Loss: 0.00119234
Iteration 8/25 | Loss: 0.00119234
Iteration 9/25 | Loss: 0.00119234
Iteration 10/25 | Loss: 0.00119234
Iteration 11/25 | Loss: 0.00119234
Iteration 12/25 | Loss: 0.00119234
Iteration 13/25 | Loss: 0.00119234
Iteration 14/25 | Loss: 0.00119234
Iteration 15/25 | Loss: 0.00119234
Iteration 16/25 | Loss: 0.00119234
Iteration 17/25 | Loss: 0.00119234
Iteration 18/25 | Loss: 0.00119234
Iteration 19/25 | Loss: 0.00119234
Iteration 20/25 | Loss: 0.00119234
Iteration 21/25 | Loss: 0.00119234
Iteration 22/25 | Loss: 0.00119234
Iteration 23/25 | Loss: 0.00119234
Iteration 24/25 | Loss: 0.00119234
Iteration 25/25 | Loss: 0.00119234

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00119234
Iteration 2/1000 | Loss: 0.00003538
Iteration 3/1000 | Loss: 0.00002337
Iteration 4/1000 | Loss: 0.00002095
Iteration 5/1000 | Loss: 0.00001937
Iteration 6/1000 | Loss: 0.00001845
Iteration 7/1000 | Loss: 0.00001802
Iteration 8/1000 | Loss: 0.00001786
Iteration 9/1000 | Loss: 0.00001785
Iteration 10/1000 | Loss: 0.00001774
Iteration 11/1000 | Loss: 0.00001772
Iteration 12/1000 | Loss: 0.00001772
Iteration 13/1000 | Loss: 0.00001771
Iteration 14/1000 | Loss: 0.00001771
Iteration 15/1000 | Loss: 0.00001762
Iteration 16/1000 | Loss: 0.00001758
Iteration 17/1000 | Loss: 0.00001756
Iteration 18/1000 | Loss: 0.00001756
Iteration 19/1000 | Loss: 0.00001755
Iteration 20/1000 | Loss: 0.00001755
Iteration 21/1000 | Loss: 0.00001754
Iteration 22/1000 | Loss: 0.00001754
Iteration 23/1000 | Loss: 0.00001753
Iteration 24/1000 | Loss: 0.00001752
Iteration 25/1000 | Loss: 0.00001752
Iteration 26/1000 | Loss: 0.00001751
Iteration 27/1000 | Loss: 0.00001750
Iteration 28/1000 | Loss: 0.00001750
Iteration 29/1000 | Loss: 0.00001749
Iteration 30/1000 | Loss: 0.00001749
Iteration 31/1000 | Loss: 0.00001749
Iteration 32/1000 | Loss: 0.00001748
Iteration 33/1000 | Loss: 0.00001748
Iteration 34/1000 | Loss: 0.00001748
Iteration 35/1000 | Loss: 0.00001747
Iteration 36/1000 | Loss: 0.00001747
Iteration 37/1000 | Loss: 0.00001747
Iteration 38/1000 | Loss: 0.00001746
Iteration 39/1000 | Loss: 0.00001746
Iteration 40/1000 | Loss: 0.00001746
Iteration 41/1000 | Loss: 0.00001746
Iteration 42/1000 | Loss: 0.00001745
Iteration 43/1000 | Loss: 0.00001745
Iteration 44/1000 | Loss: 0.00001745
Iteration 45/1000 | Loss: 0.00001745
Iteration 46/1000 | Loss: 0.00001745
Iteration 47/1000 | Loss: 0.00001744
Iteration 48/1000 | Loss: 0.00001744
Iteration 49/1000 | Loss: 0.00001744
Iteration 50/1000 | Loss: 0.00001744
Iteration 51/1000 | Loss: 0.00001744
Iteration 52/1000 | Loss: 0.00001743
Iteration 53/1000 | Loss: 0.00001743
Iteration 54/1000 | Loss: 0.00001743
Iteration 55/1000 | Loss: 0.00001742
Iteration 56/1000 | Loss: 0.00001742
Iteration 57/1000 | Loss: 0.00001742
Iteration 58/1000 | Loss: 0.00001742
Iteration 59/1000 | Loss: 0.00001742
Iteration 60/1000 | Loss: 0.00001742
Iteration 61/1000 | Loss: 0.00001741
Iteration 62/1000 | Loss: 0.00001741
Iteration 63/1000 | Loss: 0.00001741
Iteration 64/1000 | Loss: 0.00001741
Iteration 65/1000 | Loss: 0.00001741
Iteration 66/1000 | Loss: 0.00001741
Iteration 67/1000 | Loss: 0.00001741
Iteration 68/1000 | Loss: 0.00001741
Iteration 69/1000 | Loss: 0.00001740
Iteration 70/1000 | Loss: 0.00001740
Iteration 71/1000 | Loss: 0.00001740
Iteration 72/1000 | Loss: 0.00001740
Iteration 73/1000 | Loss: 0.00001740
Iteration 74/1000 | Loss: 0.00001740
Iteration 75/1000 | Loss: 0.00001739
Iteration 76/1000 | Loss: 0.00001739
Iteration 77/1000 | Loss: 0.00001739
Iteration 78/1000 | Loss: 0.00001739
Iteration 79/1000 | Loss: 0.00001739
Iteration 80/1000 | Loss: 0.00001739
Iteration 81/1000 | Loss: 0.00001739
Iteration 82/1000 | Loss: 0.00001739
Iteration 83/1000 | Loss: 0.00001739
Iteration 84/1000 | Loss: 0.00001739
Iteration 85/1000 | Loss: 0.00001739
Iteration 86/1000 | Loss: 0.00001739
Iteration 87/1000 | Loss: 0.00001739
Iteration 88/1000 | Loss: 0.00001739
Iteration 89/1000 | Loss: 0.00001739
Iteration 90/1000 | Loss: 0.00001739
Iteration 91/1000 | Loss: 0.00001739
Iteration 92/1000 | Loss: 0.00001739
Iteration 93/1000 | Loss: 0.00001738
Iteration 94/1000 | Loss: 0.00001738
Iteration 95/1000 | Loss: 0.00001738
Iteration 96/1000 | Loss: 0.00001738
Iteration 97/1000 | Loss: 0.00001738
Iteration 98/1000 | Loss: 0.00001738
Iteration 99/1000 | Loss: 0.00001738
Iteration 100/1000 | Loss: 0.00001738
Iteration 101/1000 | Loss: 0.00001738
Iteration 102/1000 | Loss: 0.00001737
Iteration 103/1000 | Loss: 0.00001737
Iteration 104/1000 | Loss: 0.00001737
Iteration 105/1000 | Loss: 0.00001737
Iteration 106/1000 | Loss: 0.00001737
Iteration 107/1000 | Loss: 0.00001737
Iteration 108/1000 | Loss: 0.00001737
Iteration 109/1000 | Loss: 0.00001737
Iteration 110/1000 | Loss: 0.00001737
Iteration 111/1000 | Loss: 0.00001737
Iteration 112/1000 | Loss: 0.00001737
Iteration 113/1000 | Loss: 0.00001737
Iteration 114/1000 | Loss: 0.00001737
Iteration 115/1000 | Loss: 0.00001737
Iteration 116/1000 | Loss: 0.00001737
Iteration 117/1000 | Loss: 0.00001737
Iteration 118/1000 | Loss: 0.00001737
Iteration 119/1000 | Loss: 0.00001737
Iteration 120/1000 | Loss: 0.00001737
Iteration 121/1000 | Loss: 0.00001736
Iteration 122/1000 | Loss: 0.00001736
Iteration 123/1000 | Loss: 0.00001736
Iteration 124/1000 | Loss: 0.00001736
Iteration 125/1000 | Loss: 0.00001736
Iteration 126/1000 | Loss: 0.00001736
Iteration 127/1000 | Loss: 0.00001736
Iteration 128/1000 | Loss: 0.00001736
Iteration 129/1000 | Loss: 0.00001736
Iteration 130/1000 | Loss: 0.00001736
Iteration 131/1000 | Loss: 0.00001736
Iteration 132/1000 | Loss: 0.00001736
Iteration 133/1000 | Loss: 0.00001736
Iteration 134/1000 | Loss: 0.00001735
Iteration 135/1000 | Loss: 0.00001735
Iteration 136/1000 | Loss: 0.00001735
Iteration 137/1000 | Loss: 0.00001735
Iteration 138/1000 | Loss: 0.00001735
Iteration 139/1000 | Loss: 0.00001735
Iteration 140/1000 | Loss: 0.00001735
Iteration 141/1000 | Loss: 0.00001735
Iteration 142/1000 | Loss: 0.00001735
Iteration 143/1000 | Loss: 0.00001735
Iteration 144/1000 | Loss: 0.00001735
Iteration 145/1000 | Loss: 0.00001735
Iteration 146/1000 | Loss: 0.00001735
Iteration 147/1000 | Loss: 0.00001735
Iteration 148/1000 | Loss: 0.00001735
Iteration 149/1000 | Loss: 0.00001735
Iteration 150/1000 | Loss: 0.00001735
Iteration 151/1000 | Loss: 0.00001735
Iteration 152/1000 | Loss: 0.00001735
Iteration 153/1000 | Loss: 0.00001735
Iteration 154/1000 | Loss: 0.00001735
Iteration 155/1000 | Loss: 0.00001735
Iteration 156/1000 | Loss: 0.00001735
Iteration 157/1000 | Loss: 0.00001735
Iteration 158/1000 | Loss: 0.00001735
Iteration 159/1000 | Loss: 0.00001735
Iteration 160/1000 | Loss: 0.00001735
Iteration 161/1000 | Loss: 0.00001735
Iteration 162/1000 | Loss: 0.00001735
Iteration 163/1000 | Loss: 0.00001735
Iteration 164/1000 | Loss: 0.00001735
Iteration 165/1000 | Loss: 0.00001735
Iteration 166/1000 | Loss: 0.00001735
Iteration 167/1000 | Loss: 0.00001735
Iteration 168/1000 | Loss: 0.00001735
Iteration 169/1000 | Loss: 0.00001735
Iteration 170/1000 | Loss: 0.00001735
Iteration 171/1000 | Loss: 0.00001735
Iteration 172/1000 | Loss: 0.00001735
Iteration 173/1000 | Loss: 0.00001735
Iteration 174/1000 | Loss: 0.00001735
Iteration 175/1000 | Loss: 0.00001735
Iteration 176/1000 | Loss: 0.00001735
Iteration 177/1000 | Loss: 0.00001735
Iteration 178/1000 | Loss: 0.00001735
Iteration 179/1000 | Loss: 0.00001735
Iteration 180/1000 | Loss: 0.00001735
Iteration 181/1000 | Loss: 0.00001735
Iteration 182/1000 | Loss: 0.00001735
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 182. Stopping optimization.
Last 5 losses: [1.7348062101518735e-05, 1.7348062101518735e-05, 1.7348062101518735e-05, 1.7348062101518735e-05, 1.7348062101518735e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7348062101518735e-05

Optimization complete. Final v2v error: 3.5908138751983643 mm

Highest mean error: 4.151561260223389 mm for frame 34

Lowest mean error: 3.221508264541626 mm for frame 44

Saving results

Total time: 33.08534002304077
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_30_nl_6559/0010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_nl_6559/0010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_nl_6559/0010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01051125
Iteration 2/25 | Loss: 0.00208859
Iteration 3/25 | Loss: 0.00138190
Iteration 4/25 | Loss: 0.00120220
Iteration 5/25 | Loss: 0.00110352
Iteration 6/25 | Loss: 0.00106809
Iteration 7/25 | Loss: 0.00106152
Iteration 8/25 | Loss: 0.00104391
Iteration 9/25 | Loss: 0.00101268
Iteration 10/25 | Loss: 0.00097908
Iteration 11/25 | Loss: 0.00096735
Iteration 12/25 | Loss: 0.00096781
Iteration 13/25 | Loss: 0.00095853
Iteration 14/25 | Loss: 0.00094571
Iteration 15/25 | Loss: 0.00094013
Iteration 16/25 | Loss: 0.00093655
Iteration 17/25 | Loss: 0.00094324
Iteration 18/25 | Loss: 0.00093476
Iteration 19/25 | Loss: 0.00093265
Iteration 20/25 | Loss: 0.00092465
Iteration 21/25 | Loss: 0.00092268
Iteration 22/25 | Loss: 0.00092954
Iteration 23/25 | Loss: 0.00092168
Iteration 24/25 | Loss: 0.00092053
Iteration 25/25 | Loss: 0.00091933

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.47806823
Iteration 2/25 | Loss: 0.00147851
Iteration 3/25 | Loss: 0.00147851
Iteration 4/25 | Loss: 0.00147851
Iteration 5/25 | Loss: 0.00147851
Iteration 6/25 | Loss: 0.00147851
Iteration 7/25 | Loss: 0.00147851
Iteration 8/25 | Loss: 0.00147851
Iteration 9/25 | Loss: 0.00147851
Iteration 10/25 | Loss: 0.00147851
Iteration 11/25 | Loss: 0.00147851
Iteration 12/25 | Loss: 0.00147851
Iteration 13/25 | Loss: 0.00147851
Iteration 14/25 | Loss: 0.00147851
Iteration 15/25 | Loss: 0.00147851
Iteration 16/25 | Loss: 0.00147851
Iteration 17/25 | Loss: 0.00147851
Iteration 18/25 | Loss: 0.00147851
Iteration 19/25 | Loss: 0.00147851
Iteration 20/25 | Loss: 0.00147851
Iteration 21/25 | Loss: 0.00147851
Iteration 22/25 | Loss: 0.00147851
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0014785118401050568, 0.0014785118401050568, 0.0014785118401050568, 0.0014785118401050568, 0.0014785118401050568]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014785118401050568

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00147851
Iteration 2/1000 | Loss: 0.00010886
Iteration 3/1000 | Loss: 0.00006788
Iteration 4/1000 | Loss: 0.00012980
Iteration 5/1000 | Loss: 0.00003859
Iteration 6/1000 | Loss: 0.00003508
Iteration 7/1000 | Loss: 0.00013333
Iteration 8/1000 | Loss: 0.00009221
Iteration 9/1000 | Loss: 0.00006094
Iteration 10/1000 | Loss: 0.00011842
Iteration 11/1000 | Loss: 0.00014174
Iteration 12/1000 | Loss: 0.00014973
Iteration 13/1000 | Loss: 0.00015166
Iteration 14/1000 | Loss: 0.00014886
Iteration 15/1000 | Loss: 0.00014546
Iteration 16/1000 | Loss: 0.00012874
Iteration 17/1000 | Loss: 0.00014678
Iteration 18/1000 | Loss: 0.00016044
Iteration 19/1000 | Loss: 0.00015221
Iteration 20/1000 | Loss: 0.00014921
Iteration 21/1000 | Loss: 0.00031132
Iteration 22/1000 | Loss: 0.00015234
Iteration 23/1000 | Loss: 0.00043604
Iteration 24/1000 | Loss: 0.00090363
Iteration 25/1000 | Loss: 0.00014994
Iteration 26/1000 | Loss: 0.00013597
Iteration 27/1000 | Loss: 0.00020275
Iteration 28/1000 | Loss: 0.00010269
Iteration 29/1000 | Loss: 0.00014692
Iteration 30/1000 | Loss: 0.00009521
Iteration 31/1000 | Loss: 0.00020705
Iteration 32/1000 | Loss: 0.00022891
Iteration 33/1000 | Loss: 0.00015065
Iteration 34/1000 | Loss: 0.00004861
Iteration 35/1000 | Loss: 0.00007746
Iteration 36/1000 | Loss: 0.00054473
Iteration 37/1000 | Loss: 0.00013364
Iteration 38/1000 | Loss: 0.00064627
Iteration 39/1000 | Loss: 0.00042072
Iteration 40/1000 | Loss: 0.00026094
Iteration 41/1000 | Loss: 0.00031531
Iteration 42/1000 | Loss: 0.00005653
Iteration 43/1000 | Loss: 0.00006271
Iteration 44/1000 | Loss: 0.00005888
Iteration 45/1000 | Loss: 0.00005363
Iteration 46/1000 | Loss: 0.00003782
Iteration 47/1000 | Loss: 0.00003605
Iteration 48/1000 | Loss: 0.00003473
Iteration 49/1000 | Loss: 0.00003356
Iteration 50/1000 | Loss: 0.00017769
Iteration 51/1000 | Loss: 0.00017232
Iteration 52/1000 | Loss: 0.00010764
Iteration 53/1000 | Loss: 0.00004487
Iteration 54/1000 | Loss: 0.00003440
Iteration 55/1000 | Loss: 0.00003350
Iteration 56/1000 | Loss: 0.00003297
Iteration 57/1000 | Loss: 0.00003266
Iteration 58/1000 | Loss: 0.00003233
Iteration 59/1000 | Loss: 0.00003923
Iteration 60/1000 | Loss: 0.00003518
Iteration 61/1000 | Loss: 0.00003783
Iteration 62/1000 | Loss: 0.00003262
Iteration 63/1000 | Loss: 0.00003166
Iteration 64/1000 | Loss: 0.00021186
Iteration 65/1000 | Loss: 0.00025553
Iteration 66/1000 | Loss: 0.00025301
Iteration 67/1000 | Loss: 0.00008494
Iteration 68/1000 | Loss: 0.00003490
Iteration 69/1000 | Loss: 0.00008569
Iteration 70/1000 | Loss: 0.00003394
Iteration 71/1000 | Loss: 0.00003256
Iteration 72/1000 | Loss: 0.00003187
Iteration 73/1000 | Loss: 0.00003142
Iteration 74/1000 | Loss: 0.00003101
Iteration 75/1000 | Loss: 0.00003060
Iteration 76/1000 | Loss: 0.00029468
Iteration 77/1000 | Loss: 0.00003418
Iteration 78/1000 | Loss: 0.00005850
Iteration 79/1000 | Loss: 0.00002944
Iteration 80/1000 | Loss: 0.00002834
Iteration 81/1000 | Loss: 0.00002768
Iteration 82/1000 | Loss: 0.00002740
Iteration 83/1000 | Loss: 0.00002711
Iteration 84/1000 | Loss: 0.00002689
Iteration 85/1000 | Loss: 0.00002685
Iteration 86/1000 | Loss: 0.00002683
Iteration 87/1000 | Loss: 0.00002682
Iteration 88/1000 | Loss: 0.00002681
Iteration 89/1000 | Loss: 0.00002680
Iteration 90/1000 | Loss: 0.00002680
Iteration 91/1000 | Loss: 0.00002680
Iteration 92/1000 | Loss: 0.00002679
Iteration 93/1000 | Loss: 0.00002679
Iteration 94/1000 | Loss: 0.00002676
Iteration 95/1000 | Loss: 0.00002676
Iteration 96/1000 | Loss: 0.00002675
Iteration 97/1000 | Loss: 0.00002675
Iteration 98/1000 | Loss: 0.00002674
Iteration 99/1000 | Loss: 0.00002674
Iteration 100/1000 | Loss: 0.00002673
Iteration 101/1000 | Loss: 0.00002673
Iteration 102/1000 | Loss: 0.00002672
Iteration 103/1000 | Loss: 0.00002672
Iteration 104/1000 | Loss: 0.00002672
Iteration 105/1000 | Loss: 0.00002672
Iteration 106/1000 | Loss: 0.00002671
Iteration 107/1000 | Loss: 0.00002671
Iteration 108/1000 | Loss: 0.00002671
Iteration 109/1000 | Loss: 0.00002671
Iteration 110/1000 | Loss: 0.00002671
Iteration 111/1000 | Loss: 0.00002670
Iteration 112/1000 | Loss: 0.00002670
Iteration 113/1000 | Loss: 0.00002670
Iteration 114/1000 | Loss: 0.00002670
Iteration 115/1000 | Loss: 0.00002670
Iteration 116/1000 | Loss: 0.00002670
Iteration 117/1000 | Loss: 0.00002670
Iteration 118/1000 | Loss: 0.00002670
Iteration 119/1000 | Loss: 0.00002669
Iteration 120/1000 | Loss: 0.00002669
Iteration 121/1000 | Loss: 0.00002669
Iteration 122/1000 | Loss: 0.00002669
Iteration 123/1000 | Loss: 0.00002669
Iteration 124/1000 | Loss: 0.00002669
Iteration 125/1000 | Loss: 0.00002669
Iteration 126/1000 | Loss: 0.00002669
Iteration 127/1000 | Loss: 0.00002669
Iteration 128/1000 | Loss: 0.00002669
Iteration 129/1000 | Loss: 0.00002668
Iteration 130/1000 | Loss: 0.00002668
Iteration 131/1000 | Loss: 0.00002668
Iteration 132/1000 | Loss: 0.00002668
Iteration 133/1000 | Loss: 0.00002667
Iteration 134/1000 | Loss: 0.00002667
Iteration 135/1000 | Loss: 0.00002667
Iteration 136/1000 | Loss: 0.00002666
Iteration 137/1000 | Loss: 0.00002666
Iteration 138/1000 | Loss: 0.00002666
Iteration 139/1000 | Loss: 0.00002666
Iteration 140/1000 | Loss: 0.00002665
Iteration 141/1000 | Loss: 0.00002665
Iteration 142/1000 | Loss: 0.00002665
Iteration 143/1000 | Loss: 0.00002665
Iteration 144/1000 | Loss: 0.00002665
Iteration 145/1000 | Loss: 0.00002665
Iteration 146/1000 | Loss: 0.00002665
Iteration 147/1000 | Loss: 0.00002665
Iteration 148/1000 | Loss: 0.00002665
Iteration 149/1000 | Loss: 0.00002665
Iteration 150/1000 | Loss: 0.00002665
Iteration 151/1000 | Loss: 0.00002665
Iteration 152/1000 | Loss: 0.00002665
Iteration 153/1000 | Loss: 0.00002665
Iteration 154/1000 | Loss: 0.00002665
Iteration 155/1000 | Loss: 0.00002665
Iteration 156/1000 | Loss: 0.00002664
Iteration 157/1000 | Loss: 0.00002664
Iteration 158/1000 | Loss: 0.00002664
Iteration 159/1000 | Loss: 0.00002664
Iteration 160/1000 | Loss: 0.00002664
Iteration 161/1000 | Loss: 0.00002664
Iteration 162/1000 | Loss: 0.00002664
Iteration 163/1000 | Loss: 0.00002664
Iteration 164/1000 | Loss: 0.00002664
Iteration 165/1000 | Loss: 0.00002664
Iteration 166/1000 | Loss: 0.00002664
Iteration 167/1000 | Loss: 0.00002664
Iteration 168/1000 | Loss: 0.00002664
Iteration 169/1000 | Loss: 0.00002664
Iteration 170/1000 | Loss: 0.00002664
Iteration 171/1000 | Loss: 0.00002664
Iteration 172/1000 | Loss: 0.00002664
Iteration 173/1000 | Loss: 0.00002664
Iteration 174/1000 | Loss: 0.00002664
Iteration 175/1000 | Loss: 0.00002664
Iteration 176/1000 | Loss: 0.00002664
Iteration 177/1000 | Loss: 0.00002664
Iteration 178/1000 | Loss: 0.00002664
Iteration 179/1000 | Loss: 0.00002664
Iteration 180/1000 | Loss: 0.00002664
Iteration 181/1000 | Loss: 0.00002664
Iteration 182/1000 | Loss: 0.00002664
Iteration 183/1000 | Loss: 0.00002664
Iteration 184/1000 | Loss: 0.00002664
Iteration 185/1000 | Loss: 0.00002664
Iteration 186/1000 | Loss: 0.00002664
Iteration 187/1000 | Loss: 0.00002664
Iteration 188/1000 | Loss: 0.00002664
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 188. Stopping optimization.
Last 5 losses: [2.6637442715582438e-05, 2.6637442715582438e-05, 2.6637442715582438e-05, 2.6637442715582438e-05, 2.6637442715582438e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.6637442715582438e-05

Optimization complete. Final v2v error: 3.940445899963379 mm

Highest mean error: 20.395843505859375 mm for frame 214

Lowest mean error: 3.057851552963257 mm for frame 256

Saving results

Total time: 199.64353895187378
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_30_nl_6559/0004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_nl_6559/0004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_nl_6559/0004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01146940
Iteration 2/25 | Loss: 0.01146940
Iteration 3/25 | Loss: 0.01146940
Iteration 4/25 | Loss: 0.01146940
Iteration 5/25 | Loss: 0.01146940
Iteration 6/25 | Loss: 0.01146940
Iteration 7/25 | Loss: 0.01146940
Iteration 8/25 | Loss: 0.01146939
Iteration 9/25 | Loss: 0.01146939
Iteration 10/25 | Loss: 0.01146939
Iteration 11/25 | Loss: 0.01146939
Iteration 12/25 | Loss: 0.01146939
Iteration 13/25 | Loss: 0.01146939
Iteration 14/25 | Loss: 0.01146939
Iteration 15/25 | Loss: 0.01146938
Iteration 16/25 | Loss: 0.01146938
Iteration 17/25 | Loss: 0.01146938
Iteration 18/25 | Loss: 0.01146938
Iteration 19/25 | Loss: 0.01146938
Iteration 20/25 | Loss: 0.01146938
Iteration 21/25 | Loss: 0.01146937
Iteration 22/25 | Loss: 0.01146937
Iteration 23/25 | Loss: 0.01146937
Iteration 24/25 | Loss: 0.01146937
Iteration 25/25 | Loss: 0.01146937

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.69874549
Iteration 2/25 | Loss: 0.11398888
Iteration 3/25 | Loss: 0.10741392
Iteration 4/25 | Loss: 0.10702356
Iteration 5/25 | Loss: 0.10702356
Iteration 6/25 | Loss: 0.10711459
Iteration 7/25 | Loss: 0.10709377
Iteration 8/25 | Loss: 0.10709374
Iteration 9/25 | Loss: 0.10709374
Iteration 10/25 | Loss: 0.10709374
Iteration 11/25 | Loss: 0.10709374
Iteration 12/25 | Loss: 0.10709374
Iteration 13/25 | Loss: 0.10709374
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.10709374397993088, 0.10709374397993088, 0.10709374397993088, 0.10709374397993088, 0.10709374397993088]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.10709374397993088

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.10709374
Iteration 2/1000 | Loss: 0.00188611
Iteration 3/1000 | Loss: 0.00083037
Iteration 4/1000 | Loss: 0.00018368
Iteration 5/1000 | Loss: 0.00007714
Iteration 6/1000 | Loss: 0.00004486
Iteration 7/1000 | Loss: 0.00003290
Iteration 8/1000 | Loss: 0.00002582
Iteration 9/1000 | Loss: 0.00002161
Iteration 10/1000 | Loss: 0.00001926
Iteration 11/1000 | Loss: 0.00001756
Iteration 12/1000 | Loss: 0.00001623
Iteration 13/1000 | Loss: 0.00001540
Iteration 14/1000 | Loss: 0.00001463
Iteration 15/1000 | Loss: 0.00001387
Iteration 16/1000 | Loss: 0.00001345
Iteration 17/1000 | Loss: 0.00001316
Iteration 18/1000 | Loss: 0.00001281
Iteration 19/1000 | Loss: 0.00001250
Iteration 20/1000 | Loss: 0.00001223
Iteration 21/1000 | Loss: 0.00001216
Iteration 22/1000 | Loss: 0.00001210
Iteration 23/1000 | Loss: 0.00001204
Iteration 24/1000 | Loss: 0.00001196
Iteration 25/1000 | Loss: 0.00001187
Iteration 26/1000 | Loss: 0.00001182
Iteration 27/1000 | Loss: 0.00001182
Iteration 28/1000 | Loss: 0.00001179
Iteration 29/1000 | Loss: 0.00001178
Iteration 30/1000 | Loss: 0.00001177
Iteration 31/1000 | Loss: 0.00001177
Iteration 32/1000 | Loss: 0.00001175
Iteration 33/1000 | Loss: 0.00001175
Iteration 34/1000 | Loss: 0.00001174
Iteration 35/1000 | Loss: 0.00001174
Iteration 36/1000 | Loss: 0.00001174
Iteration 37/1000 | Loss: 0.00001173
Iteration 38/1000 | Loss: 0.00001173
Iteration 39/1000 | Loss: 0.00001172
Iteration 40/1000 | Loss: 0.00001170
Iteration 41/1000 | Loss: 0.00001170
Iteration 42/1000 | Loss: 0.00001170
Iteration 43/1000 | Loss: 0.00001169
Iteration 44/1000 | Loss: 0.00001169
Iteration 45/1000 | Loss: 0.00001168
Iteration 46/1000 | Loss: 0.00001167
Iteration 47/1000 | Loss: 0.00001167
Iteration 48/1000 | Loss: 0.00001166
Iteration 49/1000 | Loss: 0.00001166
Iteration 50/1000 | Loss: 0.00001165
Iteration 51/1000 | Loss: 0.00001165
Iteration 52/1000 | Loss: 0.00001165
Iteration 53/1000 | Loss: 0.00001164
Iteration 54/1000 | Loss: 0.00001164
Iteration 55/1000 | Loss: 0.00001163
Iteration 56/1000 | Loss: 0.00001163
Iteration 57/1000 | Loss: 0.00001162
Iteration 58/1000 | Loss: 0.00001162
Iteration 59/1000 | Loss: 0.00001161
Iteration 60/1000 | Loss: 0.00001161
Iteration 61/1000 | Loss: 0.00001160
Iteration 62/1000 | Loss: 0.00001160
Iteration 63/1000 | Loss: 0.00001160
Iteration 64/1000 | Loss: 0.00001159
Iteration 65/1000 | Loss: 0.00001159
Iteration 66/1000 | Loss: 0.00001159
Iteration 67/1000 | Loss: 0.00001158
Iteration 68/1000 | Loss: 0.00001158
Iteration 69/1000 | Loss: 0.00001157
Iteration 70/1000 | Loss: 0.00001157
Iteration 71/1000 | Loss: 0.00001157
Iteration 72/1000 | Loss: 0.00001157
Iteration 73/1000 | Loss: 0.00001156
Iteration 74/1000 | Loss: 0.00001156
Iteration 75/1000 | Loss: 0.00001156
Iteration 76/1000 | Loss: 0.00001156
Iteration 77/1000 | Loss: 0.00001155
Iteration 78/1000 | Loss: 0.00001155
Iteration 79/1000 | Loss: 0.00001155
Iteration 80/1000 | Loss: 0.00001154
Iteration 81/1000 | Loss: 0.00001154
Iteration 82/1000 | Loss: 0.00001153
Iteration 83/1000 | Loss: 0.00001153
Iteration 84/1000 | Loss: 0.00001153
Iteration 85/1000 | Loss: 0.00001153
Iteration 86/1000 | Loss: 0.00001153
Iteration 87/1000 | Loss: 0.00001152
Iteration 88/1000 | Loss: 0.00001152
Iteration 89/1000 | Loss: 0.00001151
Iteration 90/1000 | Loss: 0.00001151
Iteration 91/1000 | Loss: 0.00001150
Iteration 92/1000 | Loss: 0.00001150
Iteration 93/1000 | Loss: 0.00001150
Iteration 94/1000 | Loss: 0.00001150
Iteration 95/1000 | Loss: 0.00001150
Iteration 96/1000 | Loss: 0.00001150
Iteration 97/1000 | Loss: 0.00001149
Iteration 98/1000 | Loss: 0.00001149
Iteration 99/1000 | Loss: 0.00001149
Iteration 100/1000 | Loss: 0.00001149
Iteration 101/1000 | Loss: 0.00001149
Iteration 102/1000 | Loss: 0.00001148
Iteration 103/1000 | Loss: 0.00001148
Iteration 104/1000 | Loss: 0.00001148
Iteration 105/1000 | Loss: 0.00001148
Iteration 106/1000 | Loss: 0.00001147
Iteration 107/1000 | Loss: 0.00001147
Iteration 108/1000 | Loss: 0.00001147
Iteration 109/1000 | Loss: 0.00001147
Iteration 110/1000 | Loss: 0.00001147
Iteration 111/1000 | Loss: 0.00001147
Iteration 112/1000 | Loss: 0.00001147
Iteration 113/1000 | Loss: 0.00001147
Iteration 114/1000 | Loss: 0.00001146
Iteration 115/1000 | Loss: 0.00001146
Iteration 116/1000 | Loss: 0.00001146
Iteration 117/1000 | Loss: 0.00001146
Iteration 118/1000 | Loss: 0.00001146
Iteration 119/1000 | Loss: 0.00001146
Iteration 120/1000 | Loss: 0.00001146
Iteration 121/1000 | Loss: 0.00001146
Iteration 122/1000 | Loss: 0.00001146
Iteration 123/1000 | Loss: 0.00001146
Iteration 124/1000 | Loss: 0.00001146
Iteration 125/1000 | Loss: 0.00001146
Iteration 126/1000 | Loss: 0.00001146
Iteration 127/1000 | Loss: 0.00001146
Iteration 128/1000 | Loss: 0.00001146
Iteration 129/1000 | Loss: 0.00001146
Iteration 130/1000 | Loss: 0.00001146
Iteration 131/1000 | Loss: 0.00001146
Iteration 132/1000 | Loss: 0.00001146
Iteration 133/1000 | Loss: 0.00001146
Iteration 134/1000 | Loss: 0.00001146
Iteration 135/1000 | Loss: 0.00001146
Iteration 136/1000 | Loss: 0.00001146
Iteration 137/1000 | Loss: 0.00001146
Iteration 138/1000 | Loss: 0.00001146
Iteration 139/1000 | Loss: 0.00001146
Iteration 140/1000 | Loss: 0.00001146
Iteration 141/1000 | Loss: 0.00001146
Iteration 142/1000 | Loss: 0.00001146
Iteration 143/1000 | Loss: 0.00001146
Iteration 144/1000 | Loss: 0.00001146
Iteration 145/1000 | Loss: 0.00001146
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 145. Stopping optimization.
Last 5 losses: [1.1458823792054318e-05, 1.1458823792054318e-05, 1.1458823792054318e-05, 1.1458823792054318e-05, 1.1458823792054318e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1458823792054318e-05

Optimization complete. Final v2v error: 2.959705352783203 mm

Highest mean error: 3.3365848064422607 mm for frame 50

Lowest mean error: 2.7152349948883057 mm for frame 85

Saving results

Total time: 55.65893340110779
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_30_nl_6559/0017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_nl_6559/0017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_nl_6559/0017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00658425
Iteration 2/25 | Loss: 0.00127356
Iteration 3/25 | Loss: 0.00107831
Iteration 4/25 | Loss: 0.00103960
Iteration 5/25 | Loss: 0.00103271
Iteration 6/25 | Loss: 0.00103166
Iteration 7/25 | Loss: 0.00103166
Iteration 8/25 | Loss: 0.00103166
Iteration 9/25 | Loss: 0.00103166
Iteration 10/25 | Loss: 0.00103166
Iteration 11/25 | Loss: 0.00103166
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010316604748368263, 0.0010316604748368263, 0.0010316604748368263, 0.0010316604748368263, 0.0010316604748368263]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010316604748368263

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.51864851
Iteration 2/25 | Loss: 0.00134775
Iteration 3/25 | Loss: 0.00134774
Iteration 4/25 | Loss: 0.00134774
Iteration 5/25 | Loss: 0.00134774
Iteration 6/25 | Loss: 0.00134774
Iteration 7/25 | Loss: 0.00134774
Iteration 8/25 | Loss: 0.00134774
Iteration 9/25 | Loss: 0.00134774
Iteration 10/25 | Loss: 0.00134774
Iteration 11/25 | Loss: 0.00134774
Iteration 12/25 | Loss: 0.00134774
Iteration 13/25 | Loss: 0.00134774
Iteration 14/25 | Loss: 0.00134774
Iteration 15/25 | Loss: 0.00134774
Iteration 16/25 | Loss: 0.00134774
Iteration 17/25 | Loss: 0.00134774
Iteration 18/25 | Loss: 0.00134774
Iteration 19/25 | Loss: 0.00134774
Iteration 20/25 | Loss: 0.00134774
Iteration 21/25 | Loss: 0.00134774
Iteration 22/25 | Loss: 0.00134774
Iteration 23/25 | Loss: 0.00134774
Iteration 24/25 | Loss: 0.00134774
Iteration 25/25 | Loss: 0.00134774

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00134774
Iteration 2/1000 | Loss: 0.00004971
Iteration 3/1000 | Loss: 0.00003899
Iteration 4/1000 | Loss: 0.00003704
Iteration 5/1000 | Loss: 0.00003560
Iteration 6/1000 | Loss: 0.00003412
Iteration 7/1000 | Loss: 0.00003337
Iteration 8/1000 | Loss: 0.00003299
Iteration 9/1000 | Loss: 0.00003281
Iteration 10/1000 | Loss: 0.00003272
Iteration 11/1000 | Loss: 0.00003258
Iteration 12/1000 | Loss: 0.00003257
Iteration 13/1000 | Loss: 0.00003257
Iteration 14/1000 | Loss: 0.00003254
Iteration 15/1000 | Loss: 0.00003254
Iteration 16/1000 | Loss: 0.00003252
Iteration 17/1000 | Loss: 0.00003251
Iteration 18/1000 | Loss: 0.00003247
Iteration 19/1000 | Loss: 0.00003246
Iteration 20/1000 | Loss: 0.00003244
Iteration 21/1000 | Loss: 0.00003244
Iteration 22/1000 | Loss: 0.00003244
Iteration 23/1000 | Loss: 0.00003244
Iteration 24/1000 | Loss: 0.00003244
Iteration 25/1000 | Loss: 0.00003244
Iteration 26/1000 | Loss: 0.00003243
Iteration 27/1000 | Loss: 0.00003243
Iteration 28/1000 | Loss: 0.00003243
Iteration 29/1000 | Loss: 0.00003243
Iteration 30/1000 | Loss: 0.00003242
Iteration 31/1000 | Loss: 0.00003242
Iteration 32/1000 | Loss: 0.00003242
Iteration 33/1000 | Loss: 0.00003241
Iteration 34/1000 | Loss: 0.00003240
Iteration 35/1000 | Loss: 0.00003239
Iteration 36/1000 | Loss: 0.00003239
Iteration 37/1000 | Loss: 0.00003239
Iteration 38/1000 | Loss: 0.00003239
Iteration 39/1000 | Loss: 0.00003239
Iteration 40/1000 | Loss: 0.00003239
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 40. Stopping optimization.
Last 5 losses: [3.2394775189459324e-05, 3.2394775189459324e-05, 3.2394775189459324e-05, 3.2394775189459324e-05, 3.2394775189459324e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.2394775189459324e-05

Optimization complete. Final v2v error: 4.696305274963379 mm

Highest mean error: 5.084178447723389 mm for frame 203

Lowest mean error: 4.088019371032715 mm for frame 13

Saving results

Total time: 30.744489431381226
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_30_nl_6559/0020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_nl_6559/0020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_nl_6559/0020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00375484
Iteration 2/25 | Loss: 0.00091165
Iteration 3/25 | Loss: 0.00081601
Iteration 4/25 | Loss: 0.00080017
Iteration 5/25 | Loss: 0.00079510
Iteration 6/25 | Loss: 0.00079337
Iteration 7/25 | Loss: 0.00079309
Iteration 8/25 | Loss: 0.00079309
Iteration 9/25 | Loss: 0.00079309
Iteration 10/25 | Loss: 0.00079309
Iteration 11/25 | Loss: 0.00079309
Iteration 12/25 | Loss: 0.00079309
Iteration 13/25 | Loss: 0.00079309
Iteration 14/25 | Loss: 0.00079309
Iteration 15/25 | Loss: 0.00079309
Iteration 16/25 | Loss: 0.00079309
Iteration 17/25 | Loss: 0.00079309
Iteration 18/25 | Loss: 0.00079309
Iteration 19/25 | Loss: 0.00079309
Iteration 20/25 | Loss: 0.00079309
Iteration 21/25 | Loss: 0.00079309
Iteration 22/25 | Loss: 0.00079309
Iteration 23/25 | Loss: 0.00079309
Iteration 24/25 | Loss: 0.00079309
Iteration 25/25 | Loss: 0.00079309

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.62381995
Iteration 2/25 | Loss: 0.00109213
Iteration 3/25 | Loss: 0.00109213
Iteration 4/25 | Loss: 0.00109213
Iteration 5/25 | Loss: 0.00109213
Iteration 6/25 | Loss: 0.00109213
Iteration 7/25 | Loss: 0.00109213
Iteration 8/25 | Loss: 0.00109213
Iteration 9/25 | Loss: 0.00109213
Iteration 10/25 | Loss: 0.00109213
Iteration 11/25 | Loss: 0.00109213
Iteration 12/25 | Loss: 0.00109213
Iteration 13/25 | Loss: 0.00109213
Iteration 14/25 | Loss: 0.00109213
Iteration 15/25 | Loss: 0.00109213
Iteration 16/25 | Loss: 0.00109213
Iteration 17/25 | Loss: 0.00109213
Iteration 18/25 | Loss: 0.00109213
Iteration 19/25 | Loss: 0.00109213
Iteration 20/25 | Loss: 0.00109213
Iteration 21/25 | Loss: 0.00109213
Iteration 22/25 | Loss: 0.00109213
Iteration 23/25 | Loss: 0.00109213
Iteration 24/25 | Loss: 0.00109213
Iteration 25/25 | Loss: 0.00109213

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00109213
Iteration 2/1000 | Loss: 0.00002460
Iteration 3/1000 | Loss: 0.00001641
Iteration 4/1000 | Loss: 0.00001453
Iteration 5/1000 | Loss: 0.00001377
Iteration 6/1000 | Loss: 0.00001341
Iteration 7/1000 | Loss: 0.00001297
Iteration 8/1000 | Loss: 0.00001279
Iteration 9/1000 | Loss: 0.00001270
Iteration 10/1000 | Loss: 0.00001270
Iteration 11/1000 | Loss: 0.00001270
Iteration 12/1000 | Loss: 0.00001270
Iteration 13/1000 | Loss: 0.00001270
Iteration 14/1000 | Loss: 0.00001270
Iteration 15/1000 | Loss: 0.00001270
Iteration 16/1000 | Loss: 0.00001270
Iteration 17/1000 | Loss: 0.00001260
Iteration 18/1000 | Loss: 0.00001259
Iteration 19/1000 | Loss: 0.00001259
Iteration 20/1000 | Loss: 0.00001258
Iteration 21/1000 | Loss: 0.00001258
Iteration 22/1000 | Loss: 0.00001258
Iteration 23/1000 | Loss: 0.00001258
Iteration 24/1000 | Loss: 0.00001257
Iteration 25/1000 | Loss: 0.00001257
Iteration 26/1000 | Loss: 0.00001257
Iteration 27/1000 | Loss: 0.00001256
Iteration 28/1000 | Loss: 0.00001256
Iteration 29/1000 | Loss: 0.00001256
Iteration 30/1000 | Loss: 0.00001255
Iteration 31/1000 | Loss: 0.00001255
Iteration 32/1000 | Loss: 0.00001255
Iteration 33/1000 | Loss: 0.00001254
Iteration 34/1000 | Loss: 0.00001254
Iteration 35/1000 | Loss: 0.00001253
Iteration 36/1000 | Loss: 0.00001253
Iteration 37/1000 | Loss: 0.00001253
Iteration 38/1000 | Loss: 0.00001253
Iteration 39/1000 | Loss: 0.00001253
Iteration 40/1000 | Loss: 0.00001253
Iteration 41/1000 | Loss: 0.00001253
Iteration 42/1000 | Loss: 0.00001253
Iteration 43/1000 | Loss: 0.00001253
Iteration 44/1000 | Loss: 0.00001253
Iteration 45/1000 | Loss: 0.00001252
Iteration 46/1000 | Loss: 0.00001252
Iteration 47/1000 | Loss: 0.00001252
Iteration 48/1000 | Loss: 0.00001252
Iteration 49/1000 | Loss: 0.00001252
Iteration 50/1000 | Loss: 0.00001252
Iteration 51/1000 | Loss: 0.00001251
Iteration 52/1000 | Loss: 0.00001251
Iteration 53/1000 | Loss: 0.00001251
Iteration 54/1000 | Loss: 0.00001251
Iteration 55/1000 | Loss: 0.00001251
Iteration 56/1000 | Loss: 0.00001251
Iteration 57/1000 | Loss: 0.00001251
Iteration 58/1000 | Loss: 0.00001251
Iteration 59/1000 | Loss: 0.00001251
Iteration 60/1000 | Loss: 0.00001251
Iteration 61/1000 | Loss: 0.00001251
Iteration 62/1000 | Loss: 0.00001251
Iteration 63/1000 | Loss: 0.00001251
Iteration 64/1000 | Loss: 0.00001251
Iteration 65/1000 | Loss: 0.00001251
Iteration 66/1000 | Loss: 0.00001251
Iteration 67/1000 | Loss: 0.00001251
Iteration 68/1000 | Loss: 0.00001250
Iteration 69/1000 | Loss: 0.00001250
Iteration 70/1000 | Loss: 0.00001250
Iteration 71/1000 | Loss: 0.00001250
Iteration 72/1000 | Loss: 0.00001250
Iteration 73/1000 | Loss: 0.00001249
Iteration 74/1000 | Loss: 0.00001249
Iteration 75/1000 | Loss: 0.00001249
Iteration 76/1000 | Loss: 0.00001249
Iteration 77/1000 | Loss: 0.00001249
Iteration 78/1000 | Loss: 0.00001249
Iteration 79/1000 | Loss: 0.00001249
Iteration 80/1000 | Loss: 0.00001249
Iteration 81/1000 | Loss: 0.00001248
Iteration 82/1000 | Loss: 0.00001248
Iteration 83/1000 | Loss: 0.00001248
Iteration 84/1000 | Loss: 0.00001248
Iteration 85/1000 | Loss: 0.00001248
Iteration 86/1000 | Loss: 0.00001248
Iteration 87/1000 | Loss: 0.00001248
Iteration 88/1000 | Loss: 0.00001247
Iteration 89/1000 | Loss: 0.00001247
Iteration 90/1000 | Loss: 0.00001247
Iteration 91/1000 | Loss: 0.00001247
Iteration 92/1000 | Loss: 0.00001247
Iteration 93/1000 | Loss: 0.00001247
Iteration 94/1000 | Loss: 0.00001247
Iteration 95/1000 | Loss: 0.00001246
Iteration 96/1000 | Loss: 0.00001246
Iteration 97/1000 | Loss: 0.00001246
Iteration 98/1000 | Loss: 0.00001246
Iteration 99/1000 | Loss: 0.00001246
Iteration 100/1000 | Loss: 0.00001246
Iteration 101/1000 | Loss: 0.00001246
Iteration 102/1000 | Loss: 0.00001246
Iteration 103/1000 | Loss: 0.00001246
Iteration 104/1000 | Loss: 0.00001246
Iteration 105/1000 | Loss: 0.00001246
Iteration 106/1000 | Loss: 0.00001246
Iteration 107/1000 | Loss: 0.00001246
Iteration 108/1000 | Loss: 0.00001246
Iteration 109/1000 | Loss: 0.00001246
Iteration 110/1000 | Loss: 0.00001246
Iteration 111/1000 | Loss: 0.00001246
Iteration 112/1000 | Loss: 0.00001246
Iteration 113/1000 | Loss: 0.00001246
Iteration 114/1000 | Loss: 0.00001246
Iteration 115/1000 | Loss: 0.00001246
Iteration 116/1000 | Loss: 0.00001246
Iteration 117/1000 | Loss: 0.00001246
Iteration 118/1000 | Loss: 0.00001246
Iteration 119/1000 | Loss: 0.00001246
Iteration 120/1000 | Loss: 0.00001246
Iteration 121/1000 | Loss: 0.00001246
Iteration 122/1000 | Loss: 0.00001246
Iteration 123/1000 | Loss: 0.00001246
Iteration 124/1000 | Loss: 0.00001246
Iteration 125/1000 | Loss: 0.00001246
Iteration 126/1000 | Loss: 0.00001246
Iteration 127/1000 | Loss: 0.00001246
Iteration 128/1000 | Loss: 0.00001246
Iteration 129/1000 | Loss: 0.00001246
Iteration 130/1000 | Loss: 0.00001246
Iteration 131/1000 | Loss: 0.00001246
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 131. Stopping optimization.
Last 5 losses: [1.2460323887353297e-05, 1.2460323887353297e-05, 1.2460323887353297e-05, 1.2460323887353297e-05, 1.2460323887353297e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2460323887353297e-05

Optimization complete. Final v2v error: 3.0096230506896973 mm

Highest mean error: 3.1665561199188232 mm for frame 114

Lowest mean error: 2.744256019592285 mm for frame 3

Saving results

Total time: 29.601933240890503
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_30_nl_6559/0015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_nl_6559/0015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_nl_6559/0015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01071422
Iteration 2/25 | Loss: 0.00281004
Iteration 3/25 | Loss: 0.00173741
Iteration 4/25 | Loss: 0.00154047
Iteration 5/25 | Loss: 0.00147264
Iteration 6/25 | Loss: 0.00143764
Iteration 7/25 | Loss: 0.00141788
Iteration 8/25 | Loss: 0.00146075
Iteration 9/25 | Loss: 0.00137717
Iteration 10/25 | Loss: 0.00134113
Iteration 11/25 | Loss: 0.00127169
Iteration 12/25 | Loss: 0.00113876
Iteration 13/25 | Loss: 0.00109901
Iteration 14/25 | Loss: 0.00108896
Iteration 15/25 | Loss: 0.00107903
Iteration 16/25 | Loss: 0.00107578
Iteration 17/25 | Loss: 0.00107490
Iteration 18/25 | Loss: 0.00107462
Iteration 19/25 | Loss: 0.00107444
Iteration 20/25 | Loss: 0.00107435
Iteration 21/25 | Loss: 0.00107435
Iteration 22/25 | Loss: 0.00107435
Iteration 23/25 | Loss: 0.00107435
Iteration 24/25 | Loss: 0.00107435
Iteration 25/25 | Loss: 0.00107434

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.56588304
Iteration 2/25 | Loss: 0.00168315
Iteration 3/25 | Loss: 0.00167768
Iteration 4/25 | Loss: 0.00167768
Iteration 5/25 | Loss: 0.00167768
Iteration 6/25 | Loss: 0.00167768
Iteration 7/25 | Loss: 0.00167768
Iteration 8/25 | Loss: 0.00167768
Iteration 9/25 | Loss: 0.00167768
Iteration 10/25 | Loss: 0.00167768
Iteration 11/25 | Loss: 0.00167768
Iteration 12/25 | Loss: 0.00167768
Iteration 13/25 | Loss: 0.00167768
Iteration 14/25 | Loss: 0.00167768
Iteration 15/25 | Loss: 0.00167768
Iteration 16/25 | Loss: 0.00167768
Iteration 17/25 | Loss: 0.00167768
Iteration 18/25 | Loss: 0.00167768
Iteration 19/25 | Loss: 0.00167768
Iteration 20/25 | Loss: 0.00167768
Iteration 21/25 | Loss: 0.00167768
Iteration 22/25 | Loss: 0.00167768
Iteration 23/25 | Loss: 0.00167768
Iteration 24/25 | Loss: 0.00167768
Iteration 25/25 | Loss: 0.00167768

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00167768
Iteration 2/1000 | Loss: 0.00012107
Iteration 3/1000 | Loss: 0.00014136
Iteration 4/1000 | Loss: 0.00007795
Iteration 5/1000 | Loss: 0.00092585
Iteration 6/1000 | Loss: 0.00143341
Iteration 7/1000 | Loss: 0.00058472
Iteration 8/1000 | Loss: 0.00015130
Iteration 9/1000 | Loss: 0.00099869
Iteration 10/1000 | Loss: 0.00007889
Iteration 11/1000 | Loss: 0.00005854
Iteration 12/1000 | Loss: 0.00006599
Iteration 13/1000 | Loss: 0.00005145
Iteration 14/1000 | Loss: 0.00005198
Iteration 15/1000 | Loss: 0.00004795
Iteration 16/1000 | Loss: 0.00004687
Iteration 17/1000 | Loss: 0.00006847
Iteration 18/1000 | Loss: 0.00004535
Iteration 19/1000 | Loss: 0.00004474
Iteration 20/1000 | Loss: 0.00004433
Iteration 21/1000 | Loss: 0.00004401
Iteration 22/1000 | Loss: 0.00004364
Iteration 23/1000 | Loss: 0.00004340
Iteration 24/1000 | Loss: 0.00004321
Iteration 25/1000 | Loss: 0.00004300
Iteration 26/1000 | Loss: 0.00004282
Iteration 27/1000 | Loss: 0.00004278
Iteration 28/1000 | Loss: 0.00004261
Iteration 29/1000 | Loss: 0.00004247
Iteration 30/1000 | Loss: 0.00004246
Iteration 31/1000 | Loss: 0.00005207
Iteration 32/1000 | Loss: 0.00004234
Iteration 33/1000 | Loss: 0.00004227
Iteration 34/1000 | Loss: 0.00004227
Iteration 35/1000 | Loss: 0.00004227
Iteration 36/1000 | Loss: 0.00004226
Iteration 37/1000 | Loss: 0.00004226
Iteration 38/1000 | Loss: 0.00004225
Iteration 39/1000 | Loss: 0.00004225
Iteration 40/1000 | Loss: 0.00004225
Iteration 41/1000 | Loss: 0.00004222
Iteration 42/1000 | Loss: 0.00004221
Iteration 43/1000 | Loss: 0.00004218
Iteration 44/1000 | Loss: 0.00004218
Iteration 45/1000 | Loss: 0.00004217
Iteration 46/1000 | Loss: 0.00004214
Iteration 47/1000 | Loss: 0.00004213
Iteration 48/1000 | Loss: 0.00004213
Iteration 49/1000 | Loss: 0.00004212
Iteration 50/1000 | Loss: 0.00004212
Iteration 51/1000 | Loss: 0.00004211
Iteration 52/1000 | Loss: 0.00004211
Iteration 53/1000 | Loss: 0.00004211
Iteration 54/1000 | Loss: 0.00004210
Iteration 55/1000 | Loss: 0.00004207
Iteration 56/1000 | Loss: 0.00004206
Iteration 57/1000 | Loss: 0.00004205
Iteration 58/1000 | Loss: 0.00004205
Iteration 59/1000 | Loss: 0.00004205
Iteration 60/1000 | Loss: 0.00004205
Iteration 61/1000 | Loss: 0.00004205
Iteration 62/1000 | Loss: 0.00004205
Iteration 63/1000 | Loss: 0.00004205
Iteration 64/1000 | Loss: 0.00004205
Iteration 65/1000 | Loss: 0.00004205
Iteration 66/1000 | Loss: 0.00004205
Iteration 67/1000 | Loss: 0.00004204
Iteration 68/1000 | Loss: 0.00004204
Iteration 69/1000 | Loss: 0.00004204
Iteration 70/1000 | Loss: 0.00004204
Iteration 71/1000 | Loss: 0.00004203
Iteration 72/1000 | Loss: 0.00004203
Iteration 73/1000 | Loss: 0.00004203
Iteration 74/1000 | Loss: 0.00004203
Iteration 75/1000 | Loss: 0.00004202
Iteration 76/1000 | Loss: 0.00004202
Iteration 77/1000 | Loss: 0.00005413
Iteration 78/1000 | Loss: 0.00004300
Iteration 79/1000 | Loss: 0.00004202
Iteration 80/1000 | Loss: 0.00004195
Iteration 81/1000 | Loss: 0.00004195
Iteration 82/1000 | Loss: 0.00004194
Iteration 83/1000 | Loss: 0.00004193
Iteration 84/1000 | Loss: 0.00004193
Iteration 85/1000 | Loss: 0.00004193
Iteration 86/1000 | Loss: 0.00004193
Iteration 87/1000 | Loss: 0.00004192
Iteration 88/1000 | Loss: 0.00004192
Iteration 89/1000 | Loss: 0.00004192
Iteration 90/1000 | Loss: 0.00004192
Iteration 91/1000 | Loss: 0.00004192
Iteration 92/1000 | Loss: 0.00004192
Iteration 93/1000 | Loss: 0.00004192
Iteration 94/1000 | Loss: 0.00004192
Iteration 95/1000 | Loss: 0.00004191
Iteration 96/1000 | Loss: 0.00004191
Iteration 97/1000 | Loss: 0.00004191
Iteration 98/1000 | Loss: 0.00004191
Iteration 99/1000 | Loss: 0.00004191
Iteration 100/1000 | Loss: 0.00004191
Iteration 101/1000 | Loss: 0.00004191
Iteration 102/1000 | Loss: 0.00004191
Iteration 103/1000 | Loss: 0.00004191
Iteration 104/1000 | Loss: 0.00004191
Iteration 105/1000 | Loss: 0.00004191
Iteration 106/1000 | Loss: 0.00004191
Iteration 107/1000 | Loss: 0.00004191
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 107. Stopping optimization.
Last 5 losses: [4.190645267954096e-05, 4.190645267954096e-05, 4.190645267954096e-05, 4.190645267954096e-05, 4.190645267954096e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.190645267954096e-05

Optimization complete. Final v2v error: 4.845870018005371 mm

Highest mean error: 12.593195915222168 mm for frame 47

Lowest mean error: 3.3978419303894043 mm for frame 30

Saving results

Total time: 86.04825282096863
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_30_nl_6559/0016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_nl_6559/0016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_nl_6559/0016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00538825
Iteration 2/25 | Loss: 0.00146462
Iteration 3/25 | Loss: 0.00104288
Iteration 4/25 | Loss: 0.00099179
Iteration 5/25 | Loss: 0.00097563
Iteration 6/25 | Loss: 0.00097098
Iteration 7/25 | Loss: 0.00097011
Iteration 8/25 | Loss: 0.00097011
Iteration 9/25 | Loss: 0.00097011
Iteration 10/25 | Loss: 0.00097011
Iteration 11/25 | Loss: 0.00097011
Iteration 12/25 | Loss: 0.00097011
Iteration 13/25 | Loss: 0.00097011
Iteration 14/25 | Loss: 0.00097011
Iteration 15/25 | Loss: 0.00097011
Iteration 16/25 | Loss: 0.00097011
Iteration 17/25 | Loss: 0.00097011
Iteration 18/25 | Loss: 0.00097011
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0009701058734208345, 0.0009701058734208345, 0.0009701058734208345, 0.0009701058734208345, 0.0009701058734208345]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009701058734208345

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.21265125
Iteration 2/25 | Loss: 0.00143853
Iteration 3/25 | Loss: 0.00143853
Iteration 4/25 | Loss: 0.00143853
Iteration 5/25 | Loss: 0.00143853
Iteration 6/25 | Loss: 0.00143853
Iteration 7/25 | Loss: 0.00143853
Iteration 8/25 | Loss: 0.00143853
Iteration 9/25 | Loss: 0.00143853
Iteration 10/25 | Loss: 0.00143853
Iteration 11/25 | Loss: 0.00143853
Iteration 12/25 | Loss: 0.00143853
Iteration 13/25 | Loss: 0.00143853
Iteration 14/25 | Loss: 0.00143853
Iteration 15/25 | Loss: 0.00143853
Iteration 16/25 | Loss: 0.00143853
Iteration 17/25 | Loss: 0.00143853
Iteration 18/25 | Loss: 0.00143853
Iteration 19/25 | Loss: 0.00143853
Iteration 20/25 | Loss: 0.00143853
Iteration 21/25 | Loss: 0.00143853
Iteration 22/25 | Loss: 0.00143853
Iteration 23/25 | Loss: 0.00143853
Iteration 24/25 | Loss: 0.00143853
Iteration 25/25 | Loss: 0.00143853

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00143853
Iteration 2/1000 | Loss: 0.00004777
Iteration 3/1000 | Loss: 0.00003732
Iteration 4/1000 | Loss: 0.00003146
Iteration 5/1000 | Loss: 0.00002852
Iteration 6/1000 | Loss: 0.00002681
Iteration 7/1000 | Loss: 0.00002592
Iteration 8/1000 | Loss: 0.00002502
Iteration 9/1000 | Loss: 0.00002446
Iteration 10/1000 | Loss: 0.00002404
Iteration 11/1000 | Loss: 0.00002360
Iteration 12/1000 | Loss: 0.00002332
Iteration 13/1000 | Loss: 0.00002309
Iteration 14/1000 | Loss: 0.00002290
Iteration 15/1000 | Loss: 0.00002281
Iteration 16/1000 | Loss: 0.00002279
Iteration 17/1000 | Loss: 0.00002270
Iteration 18/1000 | Loss: 0.00002269
Iteration 19/1000 | Loss: 0.00002262
Iteration 20/1000 | Loss: 0.00002262
Iteration 21/1000 | Loss: 0.00002261
Iteration 22/1000 | Loss: 0.00002260
Iteration 23/1000 | Loss: 0.00002259
Iteration 24/1000 | Loss: 0.00002259
Iteration 25/1000 | Loss: 0.00002259
Iteration 26/1000 | Loss: 0.00002258
Iteration 27/1000 | Loss: 0.00002258
Iteration 28/1000 | Loss: 0.00002257
Iteration 29/1000 | Loss: 0.00002256
Iteration 30/1000 | Loss: 0.00002253
Iteration 31/1000 | Loss: 0.00002253
Iteration 32/1000 | Loss: 0.00002247
Iteration 33/1000 | Loss: 0.00002247
Iteration 34/1000 | Loss: 0.00002244
Iteration 35/1000 | Loss: 0.00002244
Iteration 36/1000 | Loss: 0.00002241
Iteration 37/1000 | Loss: 0.00002241
Iteration 38/1000 | Loss: 0.00002240
Iteration 39/1000 | Loss: 0.00002239
Iteration 40/1000 | Loss: 0.00002238
Iteration 41/1000 | Loss: 0.00002238
Iteration 42/1000 | Loss: 0.00002235
Iteration 43/1000 | Loss: 0.00002235
Iteration 44/1000 | Loss: 0.00002234
Iteration 45/1000 | Loss: 0.00002234
Iteration 46/1000 | Loss: 0.00002234
Iteration 47/1000 | Loss: 0.00002233
Iteration 48/1000 | Loss: 0.00002233
Iteration 49/1000 | Loss: 0.00002232
Iteration 50/1000 | Loss: 0.00002232
Iteration 51/1000 | Loss: 0.00002231
Iteration 52/1000 | Loss: 0.00002231
Iteration 53/1000 | Loss: 0.00002231
Iteration 54/1000 | Loss: 0.00002231
Iteration 55/1000 | Loss: 0.00002231
Iteration 56/1000 | Loss: 0.00002231
Iteration 57/1000 | Loss: 0.00002231
Iteration 58/1000 | Loss: 0.00002231
Iteration 59/1000 | Loss: 0.00002230
Iteration 60/1000 | Loss: 0.00002230
Iteration 61/1000 | Loss: 0.00002230
Iteration 62/1000 | Loss: 0.00002230
Iteration 63/1000 | Loss: 0.00002230
Iteration 64/1000 | Loss: 0.00002230
Iteration 65/1000 | Loss: 0.00002230
Iteration 66/1000 | Loss: 0.00002230
Iteration 67/1000 | Loss: 0.00002230
Iteration 68/1000 | Loss: 0.00002229
Iteration 69/1000 | Loss: 0.00002229
Iteration 70/1000 | Loss: 0.00002228
Iteration 71/1000 | Loss: 0.00002228
Iteration 72/1000 | Loss: 0.00002228
Iteration 73/1000 | Loss: 0.00002228
Iteration 74/1000 | Loss: 0.00002228
Iteration 75/1000 | Loss: 0.00002228
Iteration 76/1000 | Loss: 0.00002227
Iteration 77/1000 | Loss: 0.00002227
Iteration 78/1000 | Loss: 0.00002227
Iteration 79/1000 | Loss: 0.00002227
Iteration 80/1000 | Loss: 0.00002227
Iteration 81/1000 | Loss: 0.00002227
Iteration 82/1000 | Loss: 0.00002227
Iteration 83/1000 | Loss: 0.00002226
Iteration 84/1000 | Loss: 0.00002226
Iteration 85/1000 | Loss: 0.00002225
Iteration 86/1000 | Loss: 0.00002225
Iteration 87/1000 | Loss: 0.00002225
Iteration 88/1000 | Loss: 0.00002225
Iteration 89/1000 | Loss: 0.00002225
Iteration 90/1000 | Loss: 0.00002224
Iteration 91/1000 | Loss: 0.00002224
Iteration 92/1000 | Loss: 0.00002224
Iteration 93/1000 | Loss: 0.00002223
Iteration 94/1000 | Loss: 0.00002223
Iteration 95/1000 | Loss: 0.00002223
Iteration 96/1000 | Loss: 0.00002222
Iteration 97/1000 | Loss: 0.00002222
Iteration 98/1000 | Loss: 0.00002222
Iteration 99/1000 | Loss: 0.00002222
Iteration 100/1000 | Loss: 0.00002222
Iteration 101/1000 | Loss: 0.00002221
Iteration 102/1000 | Loss: 0.00002221
Iteration 103/1000 | Loss: 0.00002221
Iteration 104/1000 | Loss: 0.00002221
Iteration 105/1000 | Loss: 0.00002221
Iteration 106/1000 | Loss: 0.00002221
Iteration 107/1000 | Loss: 0.00002220
Iteration 108/1000 | Loss: 0.00002220
Iteration 109/1000 | Loss: 0.00002220
Iteration 110/1000 | Loss: 0.00002220
Iteration 111/1000 | Loss: 0.00002219
Iteration 112/1000 | Loss: 0.00002219
Iteration 113/1000 | Loss: 0.00002219
Iteration 114/1000 | Loss: 0.00002219
Iteration 115/1000 | Loss: 0.00002218
Iteration 116/1000 | Loss: 0.00002218
Iteration 117/1000 | Loss: 0.00002218
Iteration 118/1000 | Loss: 0.00002218
Iteration 119/1000 | Loss: 0.00002218
Iteration 120/1000 | Loss: 0.00002218
Iteration 121/1000 | Loss: 0.00002218
Iteration 122/1000 | Loss: 0.00002218
Iteration 123/1000 | Loss: 0.00002217
Iteration 124/1000 | Loss: 0.00002217
Iteration 125/1000 | Loss: 0.00002217
Iteration 126/1000 | Loss: 0.00002217
Iteration 127/1000 | Loss: 0.00002216
Iteration 128/1000 | Loss: 0.00002216
Iteration 129/1000 | Loss: 0.00002216
Iteration 130/1000 | Loss: 0.00002216
Iteration 131/1000 | Loss: 0.00002216
Iteration 132/1000 | Loss: 0.00002216
Iteration 133/1000 | Loss: 0.00002215
Iteration 134/1000 | Loss: 0.00002215
Iteration 135/1000 | Loss: 0.00002215
Iteration 136/1000 | Loss: 0.00002215
Iteration 137/1000 | Loss: 0.00002215
Iteration 138/1000 | Loss: 0.00002215
Iteration 139/1000 | Loss: 0.00002215
Iteration 140/1000 | Loss: 0.00002214
Iteration 141/1000 | Loss: 0.00002214
Iteration 142/1000 | Loss: 0.00002214
Iteration 143/1000 | Loss: 0.00002214
Iteration 144/1000 | Loss: 0.00002214
Iteration 145/1000 | Loss: 0.00002214
Iteration 146/1000 | Loss: 0.00002214
Iteration 147/1000 | Loss: 0.00002213
Iteration 148/1000 | Loss: 0.00002213
Iteration 149/1000 | Loss: 0.00002212
Iteration 150/1000 | Loss: 0.00002212
Iteration 151/1000 | Loss: 0.00002212
Iteration 152/1000 | Loss: 0.00002212
Iteration 153/1000 | Loss: 0.00002212
Iteration 154/1000 | Loss: 0.00002212
Iteration 155/1000 | Loss: 0.00002212
Iteration 156/1000 | Loss: 0.00002212
Iteration 157/1000 | Loss: 0.00002212
Iteration 158/1000 | Loss: 0.00002212
Iteration 159/1000 | Loss: 0.00002212
Iteration 160/1000 | Loss: 0.00002211
Iteration 161/1000 | Loss: 0.00002211
Iteration 162/1000 | Loss: 0.00002211
Iteration 163/1000 | Loss: 0.00002211
Iteration 164/1000 | Loss: 0.00002211
Iteration 165/1000 | Loss: 0.00002211
Iteration 166/1000 | Loss: 0.00002211
Iteration 167/1000 | Loss: 0.00002211
Iteration 168/1000 | Loss: 0.00002211
Iteration 169/1000 | Loss: 0.00002210
Iteration 170/1000 | Loss: 0.00002210
Iteration 171/1000 | Loss: 0.00002210
Iteration 172/1000 | Loss: 0.00002210
Iteration 173/1000 | Loss: 0.00002210
Iteration 174/1000 | Loss: 0.00002209
Iteration 175/1000 | Loss: 0.00002209
Iteration 176/1000 | Loss: 0.00002209
Iteration 177/1000 | Loss: 0.00002209
Iteration 178/1000 | Loss: 0.00002208
Iteration 179/1000 | Loss: 0.00002208
Iteration 180/1000 | Loss: 0.00002208
Iteration 181/1000 | Loss: 0.00002207
Iteration 182/1000 | Loss: 0.00002207
Iteration 183/1000 | Loss: 0.00002207
Iteration 184/1000 | Loss: 0.00002207
Iteration 185/1000 | Loss: 0.00002206
Iteration 186/1000 | Loss: 0.00002206
Iteration 187/1000 | Loss: 0.00002206
Iteration 188/1000 | Loss: 0.00002206
Iteration 189/1000 | Loss: 0.00002206
Iteration 190/1000 | Loss: 0.00002205
Iteration 191/1000 | Loss: 0.00002205
Iteration 192/1000 | Loss: 0.00002205
Iteration 193/1000 | Loss: 0.00002205
Iteration 194/1000 | Loss: 0.00002205
Iteration 195/1000 | Loss: 0.00002204
Iteration 196/1000 | Loss: 0.00002204
Iteration 197/1000 | Loss: 0.00002204
Iteration 198/1000 | Loss: 0.00002204
Iteration 199/1000 | Loss: 0.00002204
Iteration 200/1000 | Loss: 0.00002204
Iteration 201/1000 | Loss: 0.00002204
Iteration 202/1000 | Loss: 0.00002203
Iteration 203/1000 | Loss: 0.00002203
Iteration 204/1000 | Loss: 0.00002203
Iteration 205/1000 | Loss: 0.00002203
Iteration 206/1000 | Loss: 0.00002203
Iteration 207/1000 | Loss: 0.00002203
Iteration 208/1000 | Loss: 0.00002203
Iteration 209/1000 | Loss: 0.00002203
Iteration 210/1000 | Loss: 0.00002203
Iteration 211/1000 | Loss: 0.00002203
Iteration 212/1000 | Loss: 0.00002203
Iteration 213/1000 | Loss: 0.00002203
Iteration 214/1000 | Loss: 0.00002202
Iteration 215/1000 | Loss: 0.00002202
Iteration 216/1000 | Loss: 0.00002202
Iteration 217/1000 | Loss: 0.00002202
Iteration 218/1000 | Loss: 0.00002202
Iteration 219/1000 | Loss: 0.00002202
Iteration 220/1000 | Loss: 0.00002202
Iteration 221/1000 | Loss: 0.00002202
Iteration 222/1000 | Loss: 0.00002202
Iteration 223/1000 | Loss: 0.00002202
Iteration 224/1000 | Loss: 0.00002202
Iteration 225/1000 | Loss: 0.00002202
Iteration 226/1000 | Loss: 0.00002202
Iteration 227/1000 | Loss: 0.00002202
Iteration 228/1000 | Loss: 0.00002202
Iteration 229/1000 | Loss: 0.00002202
Iteration 230/1000 | Loss: 0.00002202
Iteration 231/1000 | Loss: 0.00002202
Iteration 232/1000 | Loss: 0.00002202
Iteration 233/1000 | Loss: 0.00002202
Iteration 234/1000 | Loss: 0.00002202
Iteration 235/1000 | Loss: 0.00002202
Iteration 236/1000 | Loss: 0.00002202
Iteration 237/1000 | Loss: 0.00002202
Iteration 238/1000 | Loss: 0.00002202
Iteration 239/1000 | Loss: 0.00002202
Iteration 240/1000 | Loss: 0.00002202
Iteration 241/1000 | Loss: 0.00002202
Iteration 242/1000 | Loss: 0.00002202
Iteration 243/1000 | Loss: 0.00002202
Iteration 244/1000 | Loss: 0.00002202
Iteration 245/1000 | Loss: 0.00002202
Iteration 246/1000 | Loss: 0.00002202
Iteration 247/1000 | Loss: 0.00002202
Iteration 248/1000 | Loss: 0.00002202
Iteration 249/1000 | Loss: 0.00002202
Iteration 250/1000 | Loss: 0.00002202
Iteration 251/1000 | Loss: 0.00002202
Iteration 252/1000 | Loss: 0.00002202
Iteration 253/1000 | Loss: 0.00002202
Iteration 254/1000 | Loss: 0.00002202
Iteration 255/1000 | Loss: 0.00002202
Iteration 256/1000 | Loss: 0.00002202
Iteration 257/1000 | Loss: 0.00002202
Iteration 258/1000 | Loss: 0.00002202
Iteration 259/1000 | Loss: 0.00002202
Iteration 260/1000 | Loss: 0.00002202
Iteration 261/1000 | Loss: 0.00002202
Iteration 262/1000 | Loss: 0.00002202
Iteration 263/1000 | Loss: 0.00002202
Iteration 264/1000 | Loss: 0.00002202
Iteration 265/1000 | Loss: 0.00002202
Iteration 266/1000 | Loss: 0.00002202
Iteration 267/1000 | Loss: 0.00002202
Iteration 268/1000 | Loss: 0.00002202
Iteration 269/1000 | Loss: 0.00002202
Iteration 270/1000 | Loss: 0.00002202
Iteration 271/1000 | Loss: 0.00002202
Iteration 272/1000 | Loss: 0.00002202
Iteration 273/1000 | Loss: 0.00002202
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 273. Stopping optimization.
Last 5 losses: [2.2018795789335854e-05, 2.2018795789335854e-05, 2.2018795789335854e-05, 2.2018795789335854e-05, 2.2018795789335854e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2018795789335854e-05

Optimization complete. Final v2v error: 3.9263908863067627 mm

Highest mean error: 5.469796657562256 mm for frame 121

Lowest mean error: 2.9828672409057617 mm for frame 191

Saving results

Total time: 54.46248960494995
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_30_nl_6559/0006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_nl_6559/0006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_nl_6559/0006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01064119
Iteration 2/25 | Loss: 0.00423338
Iteration 3/25 | Loss: 0.00275415
Iteration 4/25 | Loss: 0.00218709
Iteration 5/25 | Loss: 0.00193015
Iteration 6/25 | Loss: 0.00183138
Iteration 7/25 | Loss: 0.00173663
Iteration 8/25 | Loss: 0.00164996
Iteration 9/25 | Loss: 0.00159756
Iteration 10/25 | Loss: 0.00157830
Iteration 11/25 | Loss: 0.00153007
Iteration 12/25 | Loss: 0.00146288
Iteration 13/25 | Loss: 0.00145274
Iteration 14/25 | Loss: 0.00142004
Iteration 15/25 | Loss: 0.00140500
Iteration 16/25 | Loss: 0.00141122
Iteration 17/25 | Loss: 0.00139979
Iteration 18/25 | Loss: 0.00138330
Iteration 19/25 | Loss: 0.00137141
Iteration 20/25 | Loss: 0.00137493
Iteration 21/25 | Loss: 0.00137244
Iteration 22/25 | Loss: 0.00136564
Iteration 23/25 | Loss: 0.00136003
Iteration 24/25 | Loss: 0.00135913
Iteration 25/25 | Loss: 0.00136619

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.16398454
Iteration 2/25 | Loss: 0.00525838
Iteration 3/25 | Loss: 0.00525836
Iteration 4/25 | Loss: 0.00525836
Iteration 5/25 | Loss: 0.00525836
Iteration 6/25 | Loss: 0.00525836
Iteration 7/25 | Loss: 0.00525836
Iteration 8/25 | Loss: 0.00525836
Iteration 9/25 | Loss: 0.00525836
Iteration 10/25 | Loss: 0.00525836
Iteration 11/25 | Loss: 0.00525836
Iteration 12/25 | Loss: 0.00525836
Iteration 13/25 | Loss: 0.00525836
Iteration 14/25 | Loss: 0.00525836
Iteration 15/25 | Loss: 0.00525836
Iteration 16/25 | Loss: 0.00525836
Iteration 17/25 | Loss: 0.00525836
Iteration 18/25 | Loss: 0.00525836
Iteration 19/25 | Loss: 0.00525836
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.005258355755358934, 0.005258355755358934, 0.005258355755358934, 0.005258355755358934, 0.005258355755358934]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.005258355755358934

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00525836
Iteration 2/1000 | Loss: 0.00079986
Iteration 3/1000 | Loss: 0.00128964
Iteration 4/1000 | Loss: 0.00842535
Iteration 5/1000 | Loss: 0.00703090
Iteration 6/1000 | Loss: 0.00639519
Iteration 7/1000 | Loss: 0.00166682
Iteration 8/1000 | Loss: 0.00338024
Iteration 9/1000 | Loss: 0.00319261
Iteration 10/1000 | Loss: 0.00187194
Iteration 11/1000 | Loss: 0.00162095
Iteration 12/1000 | Loss: 0.00044198
Iteration 13/1000 | Loss: 0.00063792
Iteration 14/1000 | Loss: 0.00353679
Iteration 15/1000 | Loss: 0.00032207
Iteration 16/1000 | Loss: 0.00451186
Iteration 17/1000 | Loss: 0.00056948
Iteration 18/1000 | Loss: 0.00020096
Iteration 19/1000 | Loss: 0.00518441
Iteration 20/1000 | Loss: 0.00255372
Iteration 21/1000 | Loss: 0.00212876
Iteration 22/1000 | Loss: 0.00346134
Iteration 23/1000 | Loss: 0.01965437
Iteration 24/1000 | Loss: 0.00358350
Iteration 25/1000 | Loss: 0.00087726
Iteration 26/1000 | Loss: 0.00105430
Iteration 27/1000 | Loss: 0.00016976
Iteration 28/1000 | Loss: 0.00191376
Iteration 29/1000 | Loss: 0.00155769
Iteration 30/1000 | Loss: 0.00515184
Iteration 31/1000 | Loss: 0.00734407
Iteration 32/1000 | Loss: 0.00211157
Iteration 33/1000 | Loss: 0.00173486
Iteration 34/1000 | Loss: 0.00146503
Iteration 35/1000 | Loss: 0.00106676
Iteration 36/1000 | Loss: 0.00038661
Iteration 37/1000 | Loss: 0.00074211
Iteration 38/1000 | Loss: 0.00063496
Iteration 39/1000 | Loss: 0.00057837
Iteration 40/1000 | Loss: 0.00131445
Iteration 41/1000 | Loss: 0.00074464
Iteration 42/1000 | Loss: 0.00098632
Iteration 43/1000 | Loss: 0.00202453
Iteration 44/1000 | Loss: 0.00083807
Iteration 45/1000 | Loss: 0.00006189
Iteration 46/1000 | Loss: 0.00005430
Iteration 47/1000 | Loss: 0.00004946
Iteration 48/1000 | Loss: 0.00004564
Iteration 49/1000 | Loss: 0.00004300
Iteration 50/1000 | Loss: 0.00004145
Iteration 51/1000 | Loss: 0.00004009
Iteration 52/1000 | Loss: 0.00003909
Iteration 53/1000 | Loss: 0.00003818
Iteration 54/1000 | Loss: 0.00003749
Iteration 55/1000 | Loss: 0.00169104
Iteration 56/1000 | Loss: 0.00064277
Iteration 57/1000 | Loss: 0.00004517
Iteration 58/1000 | Loss: 0.00003818
Iteration 59/1000 | Loss: 0.00167322
Iteration 60/1000 | Loss: 0.00051649
Iteration 61/1000 | Loss: 0.00003817
Iteration 62/1000 | Loss: 0.00003644
Iteration 63/1000 | Loss: 0.00003607
Iteration 64/1000 | Loss: 0.00003576
Iteration 65/1000 | Loss: 0.00166856
Iteration 66/1000 | Loss: 0.00088164
Iteration 67/1000 | Loss: 0.00234191
Iteration 68/1000 | Loss: 0.00191410
Iteration 69/1000 | Loss: 0.00006571
Iteration 70/1000 | Loss: 0.00007109
Iteration 71/1000 | Loss: 0.00088226
Iteration 72/1000 | Loss: 0.00090376
Iteration 73/1000 | Loss: 0.00043527
Iteration 74/1000 | Loss: 0.00081920
Iteration 75/1000 | Loss: 0.00078431
Iteration 76/1000 | Loss: 0.00018683
Iteration 77/1000 | Loss: 0.00047926
Iteration 78/1000 | Loss: 0.00141469
Iteration 79/1000 | Loss: 0.00034243
Iteration 80/1000 | Loss: 0.00101137
Iteration 81/1000 | Loss: 0.00088304
Iteration 82/1000 | Loss: 0.00069167
Iteration 83/1000 | Loss: 0.00066717
Iteration 84/1000 | Loss: 0.00006488
Iteration 85/1000 | Loss: 0.00004088
Iteration 86/1000 | Loss: 0.00003567
Iteration 87/1000 | Loss: 0.00069943
Iteration 88/1000 | Loss: 0.00019005
Iteration 89/1000 | Loss: 0.00003452
Iteration 90/1000 | Loss: 0.00003250
Iteration 91/1000 | Loss: 0.00070438
Iteration 92/1000 | Loss: 0.00007232
Iteration 93/1000 | Loss: 0.00004934
Iteration 94/1000 | Loss: 0.00147778
Iteration 95/1000 | Loss: 0.00048334
Iteration 96/1000 | Loss: 0.00007773
Iteration 97/1000 | Loss: 0.00059864
Iteration 98/1000 | Loss: 0.00050223
Iteration 99/1000 | Loss: 0.00070023
Iteration 100/1000 | Loss: 0.00091886
Iteration 101/1000 | Loss: 0.00048526
Iteration 102/1000 | Loss: 0.00056184
Iteration 103/1000 | Loss: 0.00045695
Iteration 104/1000 | Loss: 0.00014196
Iteration 105/1000 | Loss: 0.00089392
Iteration 106/1000 | Loss: 0.00050599
Iteration 107/1000 | Loss: 0.00035160
Iteration 108/1000 | Loss: 0.00068225
Iteration 109/1000 | Loss: 0.00034442
Iteration 110/1000 | Loss: 0.00009048
Iteration 111/1000 | Loss: 0.00005209
Iteration 112/1000 | Loss: 0.00010640
Iteration 113/1000 | Loss: 0.00078842
Iteration 114/1000 | Loss: 0.00041089
Iteration 115/1000 | Loss: 0.00052297
Iteration 116/1000 | Loss: 0.00087277
Iteration 117/1000 | Loss: 0.00006812
Iteration 118/1000 | Loss: 0.00042505
Iteration 119/1000 | Loss: 0.00071042
Iteration 120/1000 | Loss: 0.00016485
Iteration 121/1000 | Loss: 0.00025645
Iteration 122/1000 | Loss: 0.00039954
Iteration 123/1000 | Loss: 0.00095599
Iteration 124/1000 | Loss: 0.00078797
Iteration 125/1000 | Loss: 0.00014957
Iteration 126/1000 | Loss: 0.00003965
Iteration 127/1000 | Loss: 0.00003513
Iteration 128/1000 | Loss: 0.00003135
Iteration 129/1000 | Loss: 0.00034178
Iteration 130/1000 | Loss: 0.00021783
Iteration 131/1000 | Loss: 0.00030198
Iteration 132/1000 | Loss: 0.00041778
Iteration 133/1000 | Loss: 0.00003079
Iteration 134/1000 | Loss: 0.00002836
Iteration 135/1000 | Loss: 0.00030327
Iteration 136/1000 | Loss: 0.00003487
Iteration 137/1000 | Loss: 0.00002941
Iteration 138/1000 | Loss: 0.00002750
Iteration 139/1000 | Loss: 0.00002671
Iteration 140/1000 | Loss: 0.00002611
Iteration 141/1000 | Loss: 0.00002553
Iteration 142/1000 | Loss: 0.00002509
Iteration 143/1000 | Loss: 0.00002466
Iteration 144/1000 | Loss: 0.00002442
Iteration 145/1000 | Loss: 0.00002420
Iteration 146/1000 | Loss: 0.00002418
Iteration 147/1000 | Loss: 0.00002414
Iteration 148/1000 | Loss: 0.00002411
Iteration 149/1000 | Loss: 0.00002407
Iteration 150/1000 | Loss: 0.00002406
Iteration 151/1000 | Loss: 0.00002405
Iteration 152/1000 | Loss: 0.00002403
Iteration 153/1000 | Loss: 0.00002403
Iteration 154/1000 | Loss: 0.00002402
Iteration 155/1000 | Loss: 0.00002402
Iteration 156/1000 | Loss: 0.00002402
Iteration 157/1000 | Loss: 0.00002401
Iteration 158/1000 | Loss: 0.00002401
Iteration 159/1000 | Loss: 0.00002401
Iteration 160/1000 | Loss: 0.00002401
Iteration 161/1000 | Loss: 0.00002400
Iteration 162/1000 | Loss: 0.00002400
Iteration 163/1000 | Loss: 0.00002400
Iteration 164/1000 | Loss: 0.00002399
Iteration 165/1000 | Loss: 0.00002399
Iteration 166/1000 | Loss: 0.00002399
Iteration 167/1000 | Loss: 0.00002399
Iteration 168/1000 | Loss: 0.00002399
Iteration 169/1000 | Loss: 0.00002399
Iteration 170/1000 | Loss: 0.00002398
Iteration 171/1000 | Loss: 0.00002398
Iteration 172/1000 | Loss: 0.00002398
Iteration 173/1000 | Loss: 0.00002397
Iteration 174/1000 | Loss: 0.00002397
Iteration 175/1000 | Loss: 0.00002396
Iteration 176/1000 | Loss: 0.00002396
Iteration 177/1000 | Loss: 0.00002395
Iteration 178/1000 | Loss: 0.00002395
Iteration 179/1000 | Loss: 0.00002395
Iteration 180/1000 | Loss: 0.00002394
Iteration 181/1000 | Loss: 0.00002394
Iteration 182/1000 | Loss: 0.00002394
Iteration 183/1000 | Loss: 0.00002393
Iteration 184/1000 | Loss: 0.00002392
Iteration 185/1000 | Loss: 0.00002392
Iteration 186/1000 | Loss: 0.00002391
Iteration 187/1000 | Loss: 0.00002391
Iteration 188/1000 | Loss: 0.00002391
Iteration 189/1000 | Loss: 0.00002390
Iteration 190/1000 | Loss: 0.00002390
Iteration 191/1000 | Loss: 0.00002390
Iteration 192/1000 | Loss: 0.00002389
Iteration 193/1000 | Loss: 0.00002389
Iteration 194/1000 | Loss: 0.00002389
Iteration 195/1000 | Loss: 0.00002388
Iteration 196/1000 | Loss: 0.00002388
Iteration 197/1000 | Loss: 0.00002388
Iteration 198/1000 | Loss: 0.00002388
Iteration 199/1000 | Loss: 0.00002388
Iteration 200/1000 | Loss: 0.00002388
Iteration 201/1000 | Loss: 0.00002388
Iteration 202/1000 | Loss: 0.00002388
Iteration 203/1000 | Loss: 0.00002387
Iteration 204/1000 | Loss: 0.00002387
Iteration 205/1000 | Loss: 0.00002387
Iteration 206/1000 | Loss: 0.00002387
Iteration 207/1000 | Loss: 0.00002387
Iteration 208/1000 | Loss: 0.00002387
Iteration 209/1000 | Loss: 0.00002387
Iteration 210/1000 | Loss: 0.00002387
Iteration 211/1000 | Loss: 0.00002387
Iteration 212/1000 | Loss: 0.00002387
Iteration 213/1000 | Loss: 0.00002386
Iteration 214/1000 | Loss: 0.00002386
Iteration 215/1000 | Loss: 0.00002386
Iteration 216/1000 | Loss: 0.00002386
Iteration 217/1000 | Loss: 0.00002386
Iteration 218/1000 | Loss: 0.00002386
Iteration 219/1000 | Loss: 0.00002386
Iteration 220/1000 | Loss: 0.00002386
Iteration 221/1000 | Loss: 0.00002385
Iteration 222/1000 | Loss: 0.00002385
Iteration 223/1000 | Loss: 0.00002385
Iteration 224/1000 | Loss: 0.00002385
Iteration 225/1000 | Loss: 0.00002384
Iteration 226/1000 | Loss: 0.00002384
Iteration 227/1000 | Loss: 0.00002384
Iteration 228/1000 | Loss: 0.00002384
Iteration 229/1000 | Loss: 0.00002384
Iteration 230/1000 | Loss: 0.00002384
Iteration 231/1000 | Loss: 0.00002384
Iteration 232/1000 | Loss: 0.00002384
Iteration 233/1000 | Loss: 0.00002384
Iteration 234/1000 | Loss: 0.00002384
Iteration 235/1000 | Loss: 0.00002384
Iteration 236/1000 | Loss: 0.00002384
Iteration 237/1000 | Loss: 0.00002384
Iteration 238/1000 | Loss: 0.00002384
Iteration 239/1000 | Loss: 0.00002384
Iteration 240/1000 | Loss: 0.00002384
Iteration 241/1000 | Loss: 0.00002384
Iteration 242/1000 | Loss: 0.00002384
Iteration 243/1000 | Loss: 0.00002384
Iteration 244/1000 | Loss: 0.00002384
Iteration 245/1000 | Loss: 0.00002384
Iteration 246/1000 | Loss: 0.00002384
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 246. Stopping optimization.
Last 5 losses: [2.3839553250581957e-05, 2.3839553250581957e-05, 2.3839553250581957e-05, 2.3839553250581957e-05, 2.3839553250581957e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3839553250581957e-05

Optimization complete. Final v2v error: 4.009157180786133 mm

Highest mean error: 5.603549957275391 mm for frame 44

Lowest mean error: 3.0510613918304443 mm for frame 92

Saving results

Total time: 251.52741265296936
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_30_nl_6559/0023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_nl_6559/0023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_nl_6559/0023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00562501
Iteration 2/25 | Loss: 0.00101728
Iteration 3/25 | Loss: 0.00090284
Iteration 4/25 | Loss: 0.00088337
Iteration 5/25 | Loss: 0.00087634
Iteration 6/25 | Loss: 0.00087430
Iteration 7/25 | Loss: 0.00087405
Iteration 8/25 | Loss: 0.00087405
Iteration 9/25 | Loss: 0.00087405
Iteration 10/25 | Loss: 0.00087405
Iteration 11/25 | Loss: 0.00087405
Iteration 12/25 | Loss: 0.00087405
Iteration 13/25 | Loss: 0.00087405
Iteration 14/25 | Loss: 0.00087405
Iteration 15/25 | Loss: 0.00087405
Iteration 16/25 | Loss: 0.00087405
Iteration 17/25 | Loss: 0.00087405
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0008740477496758103, 0.0008740477496758103, 0.0008740477496758103, 0.0008740477496758103, 0.0008740477496758103]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008740477496758103

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.52841437
Iteration 2/25 | Loss: 0.00102126
Iteration 3/25 | Loss: 0.00102122
Iteration 4/25 | Loss: 0.00102122
Iteration 5/25 | Loss: 0.00102122
Iteration 6/25 | Loss: 0.00102121
Iteration 7/25 | Loss: 0.00102121
Iteration 8/25 | Loss: 0.00102121
Iteration 9/25 | Loss: 0.00102121
Iteration 10/25 | Loss: 0.00102121
Iteration 11/25 | Loss: 0.00102121
Iteration 12/25 | Loss: 0.00102121
Iteration 13/25 | Loss: 0.00102121
Iteration 14/25 | Loss: 0.00102121
Iteration 15/25 | Loss: 0.00102121
Iteration 16/25 | Loss: 0.00102121
Iteration 17/25 | Loss: 0.00102121
Iteration 18/25 | Loss: 0.00102121
Iteration 19/25 | Loss: 0.00102121
Iteration 20/25 | Loss: 0.00102121
Iteration 21/25 | Loss: 0.00102121
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0010212140623480082, 0.0010212140623480082, 0.0010212140623480082, 0.0010212140623480082, 0.0010212140623480082]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010212140623480082

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00102121
Iteration 2/1000 | Loss: 0.00003813
Iteration 3/1000 | Loss: 0.00002765
Iteration 4/1000 | Loss: 0.00002468
Iteration 5/1000 | Loss: 0.00002348
Iteration 6/1000 | Loss: 0.00002252
Iteration 7/1000 | Loss: 0.00002194
Iteration 8/1000 | Loss: 0.00002153
Iteration 9/1000 | Loss: 0.00002128
Iteration 10/1000 | Loss: 0.00002121
Iteration 11/1000 | Loss: 0.00002111
Iteration 12/1000 | Loss: 0.00002105
Iteration 13/1000 | Loss: 0.00002103
Iteration 14/1000 | Loss: 0.00002103
Iteration 15/1000 | Loss: 0.00002102
Iteration 16/1000 | Loss: 0.00002096
Iteration 17/1000 | Loss: 0.00002090
Iteration 18/1000 | Loss: 0.00002086
Iteration 19/1000 | Loss: 0.00002085
Iteration 20/1000 | Loss: 0.00002085
Iteration 21/1000 | Loss: 0.00002084
Iteration 22/1000 | Loss: 0.00002084
Iteration 23/1000 | Loss: 0.00002083
Iteration 24/1000 | Loss: 0.00002082
Iteration 25/1000 | Loss: 0.00002082
Iteration 26/1000 | Loss: 0.00002081
Iteration 27/1000 | Loss: 0.00002081
Iteration 28/1000 | Loss: 0.00002080
Iteration 29/1000 | Loss: 0.00002080
Iteration 30/1000 | Loss: 0.00002080
Iteration 31/1000 | Loss: 0.00002080
Iteration 32/1000 | Loss: 0.00002080
Iteration 33/1000 | Loss: 0.00002079
Iteration 34/1000 | Loss: 0.00002078
Iteration 35/1000 | Loss: 0.00002078
Iteration 36/1000 | Loss: 0.00002078
Iteration 37/1000 | Loss: 0.00002077
Iteration 38/1000 | Loss: 0.00002077
Iteration 39/1000 | Loss: 0.00002077
Iteration 40/1000 | Loss: 0.00002077
Iteration 41/1000 | Loss: 0.00002077
Iteration 42/1000 | Loss: 0.00002076
Iteration 43/1000 | Loss: 0.00002076
Iteration 44/1000 | Loss: 0.00002076
Iteration 45/1000 | Loss: 0.00002075
Iteration 46/1000 | Loss: 0.00002075
Iteration 47/1000 | Loss: 0.00002075
Iteration 48/1000 | Loss: 0.00002074
Iteration 49/1000 | Loss: 0.00002074
Iteration 50/1000 | Loss: 0.00002074
Iteration 51/1000 | Loss: 0.00002074
Iteration 52/1000 | Loss: 0.00002074
Iteration 53/1000 | Loss: 0.00002074
Iteration 54/1000 | Loss: 0.00002073
Iteration 55/1000 | Loss: 0.00002073
Iteration 56/1000 | Loss: 0.00002072
Iteration 57/1000 | Loss: 0.00002072
Iteration 58/1000 | Loss: 0.00002072
Iteration 59/1000 | Loss: 0.00002072
Iteration 60/1000 | Loss: 0.00002071
Iteration 61/1000 | Loss: 0.00002071
Iteration 62/1000 | Loss: 0.00002070
Iteration 63/1000 | Loss: 0.00002070
Iteration 64/1000 | Loss: 0.00002070
Iteration 65/1000 | Loss: 0.00002069
Iteration 66/1000 | Loss: 0.00002069
Iteration 67/1000 | Loss: 0.00002069
Iteration 68/1000 | Loss: 0.00002068
Iteration 69/1000 | Loss: 0.00002068
Iteration 70/1000 | Loss: 0.00002068
Iteration 71/1000 | Loss: 0.00002068
Iteration 72/1000 | Loss: 0.00002068
Iteration 73/1000 | Loss: 0.00002067
Iteration 74/1000 | Loss: 0.00002067
Iteration 75/1000 | Loss: 0.00002067
Iteration 76/1000 | Loss: 0.00002067
Iteration 77/1000 | Loss: 0.00002067
Iteration 78/1000 | Loss: 0.00002067
Iteration 79/1000 | Loss: 0.00002067
Iteration 80/1000 | Loss: 0.00002066
Iteration 81/1000 | Loss: 0.00002066
Iteration 82/1000 | Loss: 0.00002066
Iteration 83/1000 | Loss: 0.00002066
Iteration 84/1000 | Loss: 0.00002066
Iteration 85/1000 | Loss: 0.00002065
Iteration 86/1000 | Loss: 0.00002065
Iteration 87/1000 | Loss: 0.00002065
Iteration 88/1000 | Loss: 0.00002065
Iteration 89/1000 | Loss: 0.00002065
Iteration 90/1000 | Loss: 0.00002064
Iteration 91/1000 | Loss: 0.00002064
Iteration 92/1000 | Loss: 0.00002064
Iteration 93/1000 | Loss: 0.00002063
Iteration 94/1000 | Loss: 0.00002063
Iteration 95/1000 | Loss: 0.00002063
Iteration 96/1000 | Loss: 0.00002063
Iteration 97/1000 | Loss: 0.00002063
Iteration 98/1000 | Loss: 0.00002063
Iteration 99/1000 | Loss: 0.00002063
Iteration 100/1000 | Loss: 0.00002063
Iteration 101/1000 | Loss: 0.00002063
Iteration 102/1000 | Loss: 0.00002063
Iteration 103/1000 | Loss: 0.00002062
Iteration 104/1000 | Loss: 0.00002062
Iteration 105/1000 | Loss: 0.00002062
Iteration 106/1000 | Loss: 0.00002062
Iteration 107/1000 | Loss: 0.00002062
Iteration 108/1000 | Loss: 0.00002062
Iteration 109/1000 | Loss: 0.00002062
Iteration 110/1000 | Loss: 0.00002062
Iteration 111/1000 | Loss: 0.00002062
Iteration 112/1000 | Loss: 0.00002062
Iteration 113/1000 | Loss: 0.00002062
Iteration 114/1000 | Loss: 0.00002062
Iteration 115/1000 | Loss: 0.00002062
Iteration 116/1000 | Loss: 0.00002062
Iteration 117/1000 | Loss: 0.00002062
Iteration 118/1000 | Loss: 0.00002062
Iteration 119/1000 | Loss: 0.00002062
Iteration 120/1000 | Loss: 0.00002062
Iteration 121/1000 | Loss: 0.00002062
Iteration 122/1000 | Loss: 0.00002062
Iteration 123/1000 | Loss: 0.00002062
Iteration 124/1000 | Loss: 0.00002062
Iteration 125/1000 | Loss: 0.00002062
Iteration 126/1000 | Loss: 0.00002062
Iteration 127/1000 | Loss: 0.00002062
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 127. Stopping optimization.
Last 5 losses: [2.0619329006876796e-05, 2.0619329006876796e-05, 2.0619329006876796e-05, 2.0619329006876796e-05, 2.0619329006876796e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0619329006876796e-05

Optimization complete. Final v2v error: 3.88385009765625 mm

Highest mean error: 4.325995922088623 mm for frame 107

Lowest mean error: 3.249180316925049 mm for frame 38

Saving results

Total time: 38.84583520889282
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_30_nl_6559/0024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_nl_6559/0024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_nl_6559/0024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00836919
Iteration 2/25 | Loss: 0.00160342
Iteration 3/25 | Loss: 0.00116457
Iteration 4/25 | Loss: 0.00109749
Iteration 5/25 | Loss: 0.00106713
Iteration 6/25 | Loss: 0.00106745
Iteration 7/25 | Loss: 0.00105988
Iteration 8/25 | Loss: 0.00106574
Iteration 9/25 | Loss: 0.00105655
Iteration 10/25 | Loss: 0.00105263
Iteration 11/25 | Loss: 0.00105067
Iteration 12/25 | Loss: 0.00104328
Iteration 13/25 | Loss: 0.00104431
Iteration 14/25 | Loss: 0.00103397
Iteration 15/25 | Loss: 0.00102474
Iteration 16/25 | Loss: 0.00101843
Iteration 17/25 | Loss: 0.00101899
Iteration 18/25 | Loss: 0.00102062
Iteration 19/25 | Loss: 0.00101867
Iteration 20/25 | Loss: 0.00101662
Iteration 21/25 | Loss: 0.00101628
Iteration 22/25 | Loss: 0.00101583
Iteration 23/25 | Loss: 0.00101499
Iteration 24/25 | Loss: 0.00101505
Iteration 25/25 | Loss: 0.00101641

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.51436532
Iteration 2/25 | Loss: 0.00156734
Iteration 3/25 | Loss: 0.00156734
Iteration 4/25 | Loss: 0.00156734
Iteration 5/25 | Loss: 0.00156734
Iteration 6/25 | Loss: 0.00156734
Iteration 7/25 | Loss: 0.00156734
Iteration 8/25 | Loss: 0.00156734
Iteration 9/25 | Loss: 0.00156734
Iteration 10/25 | Loss: 0.00156734
Iteration 11/25 | Loss: 0.00156734
Iteration 12/25 | Loss: 0.00156734
Iteration 13/25 | Loss: 0.00156734
Iteration 14/25 | Loss: 0.00156734
Iteration 15/25 | Loss: 0.00156734
Iteration 16/25 | Loss: 0.00156734
Iteration 17/25 | Loss: 0.00156734
Iteration 18/25 | Loss: 0.00156734
Iteration 19/25 | Loss: 0.00156734
Iteration 20/25 | Loss: 0.00156734
Iteration 21/25 | Loss: 0.00156734
Iteration 22/25 | Loss: 0.00156734
Iteration 23/25 | Loss: 0.00156734
Iteration 24/25 | Loss: 0.00156734
Iteration 25/25 | Loss: 0.00156734

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00156734
Iteration 2/1000 | Loss: 0.00007766
Iteration 3/1000 | Loss: 0.00005328
Iteration 4/1000 | Loss: 0.00005095
Iteration 5/1000 | Loss: 0.00005160
Iteration 6/1000 | Loss: 0.00004309
Iteration 7/1000 | Loss: 0.00005061
Iteration 8/1000 | Loss: 0.00004790
Iteration 9/1000 | Loss: 0.00134399
Iteration 10/1000 | Loss: 0.00038522
Iteration 11/1000 | Loss: 0.00015383
Iteration 12/1000 | Loss: 0.00005024
Iteration 13/1000 | Loss: 0.00006240
Iteration 14/1000 | Loss: 0.00005891
Iteration 15/1000 | Loss: 0.00004881
Iteration 16/1000 | Loss: 0.00003580
Iteration 17/1000 | Loss: 0.00004594
Iteration 18/1000 | Loss: 0.00003924
Iteration 19/1000 | Loss: 0.00021759
Iteration 20/1000 | Loss: 0.00018090
Iteration 21/1000 | Loss: 0.00026442
Iteration 22/1000 | Loss: 0.00016257
Iteration 23/1000 | Loss: 0.00026473
Iteration 24/1000 | Loss: 0.00013452
Iteration 25/1000 | Loss: 0.00018744
Iteration 26/1000 | Loss: 0.00011851
Iteration 27/1000 | Loss: 0.00031022
Iteration 28/1000 | Loss: 0.00009162
Iteration 29/1000 | Loss: 0.00004762
Iteration 30/1000 | Loss: 0.00007492
Iteration 31/1000 | Loss: 0.00005767
Iteration 32/1000 | Loss: 0.00004822
Iteration 33/1000 | Loss: 0.00004962
Iteration 34/1000 | Loss: 0.00005543
Iteration 35/1000 | Loss: 0.00005058
Iteration 36/1000 | Loss: 0.00006546
Iteration 37/1000 | Loss: 0.00004272
Iteration 38/1000 | Loss: 0.00003558
Iteration 39/1000 | Loss: 0.00003771
Iteration 40/1000 | Loss: 0.00003184
Iteration 41/1000 | Loss: 0.00003110
Iteration 42/1000 | Loss: 0.00003022
Iteration 43/1000 | Loss: 0.00002964
Iteration 44/1000 | Loss: 0.00002920
Iteration 45/1000 | Loss: 0.00002857
Iteration 46/1000 | Loss: 0.00002778
Iteration 47/1000 | Loss: 0.00002718
Iteration 48/1000 | Loss: 0.00002681
Iteration 49/1000 | Loss: 0.00002661
Iteration 50/1000 | Loss: 0.00002649
Iteration 51/1000 | Loss: 0.00002642
Iteration 52/1000 | Loss: 0.00002639
Iteration 53/1000 | Loss: 0.00002639
Iteration 54/1000 | Loss: 0.00002639
Iteration 55/1000 | Loss: 0.00002639
Iteration 56/1000 | Loss: 0.00002639
Iteration 57/1000 | Loss: 0.00002639
Iteration 58/1000 | Loss: 0.00002639
Iteration 59/1000 | Loss: 0.00002639
Iteration 60/1000 | Loss: 0.00002639
Iteration 61/1000 | Loss: 0.00002639
Iteration 62/1000 | Loss: 0.00002639
Iteration 63/1000 | Loss: 0.00002638
Iteration 64/1000 | Loss: 0.00002638
Iteration 65/1000 | Loss: 0.00002638
Iteration 66/1000 | Loss: 0.00002637
Iteration 67/1000 | Loss: 0.00002637
Iteration 68/1000 | Loss: 0.00002636
Iteration 69/1000 | Loss: 0.00002636
Iteration 70/1000 | Loss: 0.00002636
Iteration 71/1000 | Loss: 0.00002636
Iteration 72/1000 | Loss: 0.00002636
Iteration 73/1000 | Loss: 0.00002636
Iteration 74/1000 | Loss: 0.00002636
Iteration 75/1000 | Loss: 0.00002635
Iteration 76/1000 | Loss: 0.00002635
Iteration 77/1000 | Loss: 0.00002635
Iteration 78/1000 | Loss: 0.00002635
Iteration 79/1000 | Loss: 0.00002635
Iteration 80/1000 | Loss: 0.00002635
Iteration 81/1000 | Loss: 0.00002635
Iteration 82/1000 | Loss: 0.00002635
Iteration 83/1000 | Loss: 0.00002635
Iteration 84/1000 | Loss: 0.00002634
Iteration 85/1000 | Loss: 0.00002634
Iteration 86/1000 | Loss: 0.00002634
Iteration 87/1000 | Loss: 0.00002634
Iteration 88/1000 | Loss: 0.00002634
Iteration 89/1000 | Loss: 0.00002634
Iteration 90/1000 | Loss: 0.00002633
Iteration 91/1000 | Loss: 0.00002633
Iteration 92/1000 | Loss: 0.00002633
Iteration 93/1000 | Loss: 0.00002633
Iteration 94/1000 | Loss: 0.00002633
Iteration 95/1000 | Loss: 0.00002633
Iteration 96/1000 | Loss: 0.00002633
Iteration 97/1000 | Loss: 0.00002633
Iteration 98/1000 | Loss: 0.00002633
Iteration 99/1000 | Loss: 0.00002633
Iteration 100/1000 | Loss: 0.00002633
Iteration 101/1000 | Loss: 0.00002632
Iteration 102/1000 | Loss: 0.00002632
Iteration 103/1000 | Loss: 0.00002632
Iteration 104/1000 | Loss: 0.00002632
Iteration 105/1000 | Loss: 0.00002632
Iteration 106/1000 | Loss: 0.00002631
Iteration 107/1000 | Loss: 0.00002631
Iteration 108/1000 | Loss: 0.00002631
Iteration 109/1000 | Loss: 0.00002631
Iteration 110/1000 | Loss: 0.00002631
Iteration 111/1000 | Loss: 0.00002631
Iteration 112/1000 | Loss: 0.00002631
Iteration 113/1000 | Loss: 0.00002631
Iteration 114/1000 | Loss: 0.00002631
Iteration 115/1000 | Loss: 0.00002631
Iteration 116/1000 | Loss: 0.00002631
Iteration 117/1000 | Loss: 0.00002630
Iteration 118/1000 | Loss: 0.00002630
Iteration 119/1000 | Loss: 0.00002630
Iteration 120/1000 | Loss: 0.00002630
Iteration 121/1000 | Loss: 0.00002630
Iteration 122/1000 | Loss: 0.00002630
Iteration 123/1000 | Loss: 0.00002629
Iteration 124/1000 | Loss: 0.00002629
Iteration 125/1000 | Loss: 0.00002629
Iteration 126/1000 | Loss: 0.00002629
Iteration 127/1000 | Loss: 0.00002628
Iteration 128/1000 | Loss: 0.00002628
Iteration 129/1000 | Loss: 0.00002628
Iteration 130/1000 | Loss: 0.00002628
Iteration 131/1000 | Loss: 0.00002628
Iteration 132/1000 | Loss: 0.00002627
Iteration 133/1000 | Loss: 0.00002627
Iteration 134/1000 | Loss: 0.00002627
Iteration 135/1000 | Loss: 0.00002627
Iteration 136/1000 | Loss: 0.00002627
Iteration 137/1000 | Loss: 0.00002627
Iteration 138/1000 | Loss: 0.00002626
Iteration 139/1000 | Loss: 0.00002626
Iteration 140/1000 | Loss: 0.00002626
Iteration 141/1000 | Loss: 0.00002626
Iteration 142/1000 | Loss: 0.00002626
Iteration 143/1000 | Loss: 0.00002626
Iteration 144/1000 | Loss: 0.00002626
Iteration 145/1000 | Loss: 0.00002626
Iteration 146/1000 | Loss: 0.00002625
Iteration 147/1000 | Loss: 0.00002625
Iteration 148/1000 | Loss: 0.00002625
Iteration 149/1000 | Loss: 0.00002625
Iteration 150/1000 | Loss: 0.00002624
Iteration 151/1000 | Loss: 0.00002624
Iteration 152/1000 | Loss: 0.00002624
Iteration 153/1000 | Loss: 0.00002623
Iteration 154/1000 | Loss: 0.00002623
Iteration 155/1000 | Loss: 0.00002623
Iteration 156/1000 | Loss: 0.00002622
Iteration 157/1000 | Loss: 0.00002621
Iteration 158/1000 | Loss: 0.00002621
Iteration 159/1000 | Loss: 0.00002621
Iteration 160/1000 | Loss: 0.00002621
Iteration 161/1000 | Loss: 0.00002621
Iteration 162/1000 | Loss: 0.00002621
Iteration 163/1000 | Loss: 0.00002621
Iteration 164/1000 | Loss: 0.00002621
Iteration 165/1000 | Loss: 0.00002621
Iteration 166/1000 | Loss: 0.00002621
Iteration 167/1000 | Loss: 0.00002621
Iteration 168/1000 | Loss: 0.00002621
Iteration 169/1000 | Loss: 0.00002620
Iteration 170/1000 | Loss: 0.00002620
Iteration 171/1000 | Loss: 0.00002620
Iteration 172/1000 | Loss: 0.00002620
Iteration 173/1000 | Loss: 0.00002620
Iteration 174/1000 | Loss: 0.00002620
Iteration 175/1000 | Loss: 0.00002620
Iteration 176/1000 | Loss: 0.00002620
Iteration 177/1000 | Loss: 0.00002619
Iteration 178/1000 | Loss: 0.00002619
Iteration 179/1000 | Loss: 0.00002619
Iteration 180/1000 | Loss: 0.00002619
Iteration 181/1000 | Loss: 0.00002619
Iteration 182/1000 | Loss: 0.00002618
Iteration 183/1000 | Loss: 0.00002618
Iteration 184/1000 | Loss: 0.00002618
Iteration 185/1000 | Loss: 0.00002618
Iteration 186/1000 | Loss: 0.00002618
Iteration 187/1000 | Loss: 0.00002618
Iteration 188/1000 | Loss: 0.00002617
Iteration 189/1000 | Loss: 0.00002617
Iteration 190/1000 | Loss: 0.00002617
Iteration 191/1000 | Loss: 0.00002617
Iteration 192/1000 | Loss: 0.00002617
Iteration 193/1000 | Loss: 0.00002617
Iteration 194/1000 | Loss: 0.00002617
Iteration 195/1000 | Loss: 0.00002616
Iteration 196/1000 | Loss: 0.00002616
Iteration 197/1000 | Loss: 0.00002616
Iteration 198/1000 | Loss: 0.00002616
Iteration 199/1000 | Loss: 0.00002615
Iteration 200/1000 | Loss: 0.00002615
Iteration 201/1000 | Loss: 0.00002615
Iteration 202/1000 | Loss: 0.00002614
Iteration 203/1000 | Loss: 0.00002614
Iteration 204/1000 | Loss: 0.00002614
Iteration 205/1000 | Loss: 0.00002614
Iteration 206/1000 | Loss: 0.00002614
Iteration 207/1000 | Loss: 0.00002614
Iteration 208/1000 | Loss: 0.00002613
Iteration 209/1000 | Loss: 0.00002613
Iteration 210/1000 | Loss: 0.00002613
Iteration 211/1000 | Loss: 0.00002613
Iteration 212/1000 | Loss: 0.00002613
Iteration 213/1000 | Loss: 0.00002613
Iteration 214/1000 | Loss: 0.00002613
Iteration 215/1000 | Loss: 0.00002613
Iteration 216/1000 | Loss: 0.00002612
Iteration 217/1000 | Loss: 0.00002612
Iteration 218/1000 | Loss: 0.00002612
Iteration 219/1000 | Loss: 0.00002612
Iteration 220/1000 | Loss: 0.00002612
Iteration 221/1000 | Loss: 0.00002612
Iteration 222/1000 | Loss: 0.00002612
Iteration 223/1000 | Loss: 0.00002612
Iteration 224/1000 | Loss: 0.00002612
Iteration 225/1000 | Loss: 0.00002611
Iteration 226/1000 | Loss: 0.00002611
Iteration 227/1000 | Loss: 0.00002611
Iteration 228/1000 | Loss: 0.00002611
Iteration 229/1000 | Loss: 0.00002611
Iteration 230/1000 | Loss: 0.00002611
Iteration 231/1000 | Loss: 0.00002610
Iteration 232/1000 | Loss: 0.00002610
Iteration 233/1000 | Loss: 0.00002610
Iteration 234/1000 | Loss: 0.00002610
Iteration 235/1000 | Loss: 0.00002610
Iteration 236/1000 | Loss: 0.00002610
Iteration 237/1000 | Loss: 0.00002610
Iteration 238/1000 | Loss: 0.00002610
Iteration 239/1000 | Loss: 0.00002610
Iteration 240/1000 | Loss: 0.00002610
Iteration 241/1000 | Loss: 0.00002610
Iteration 242/1000 | Loss: 0.00002610
Iteration 243/1000 | Loss: 0.00002610
Iteration 244/1000 | Loss: 0.00002610
Iteration 245/1000 | Loss: 0.00002609
Iteration 246/1000 | Loss: 0.00002609
Iteration 247/1000 | Loss: 0.00002609
Iteration 248/1000 | Loss: 0.00002609
Iteration 249/1000 | Loss: 0.00002609
Iteration 250/1000 | Loss: 0.00002609
Iteration 251/1000 | Loss: 0.00002609
Iteration 252/1000 | Loss: 0.00002609
Iteration 253/1000 | Loss: 0.00002609
Iteration 254/1000 | Loss: 0.00002608
Iteration 255/1000 | Loss: 0.00002608
Iteration 256/1000 | Loss: 0.00002608
Iteration 257/1000 | Loss: 0.00002608
Iteration 258/1000 | Loss: 0.00002608
Iteration 259/1000 | Loss: 0.00002608
Iteration 260/1000 | Loss: 0.00002608
Iteration 261/1000 | Loss: 0.00002608
Iteration 262/1000 | Loss: 0.00002608
Iteration 263/1000 | Loss: 0.00002608
Iteration 264/1000 | Loss: 0.00002608
Iteration 265/1000 | Loss: 0.00002608
Iteration 266/1000 | Loss: 0.00002608
Iteration 267/1000 | Loss: 0.00002608
Iteration 268/1000 | Loss: 0.00002608
Iteration 269/1000 | Loss: 0.00002608
Iteration 270/1000 | Loss: 0.00002608
Iteration 271/1000 | Loss: 0.00002608
Iteration 272/1000 | Loss: 0.00002608
Iteration 273/1000 | Loss: 0.00002608
Iteration 274/1000 | Loss: 0.00002608
Iteration 275/1000 | Loss: 0.00002608
Iteration 276/1000 | Loss: 0.00002608
Iteration 277/1000 | Loss: 0.00002608
Iteration 278/1000 | Loss: 0.00002608
Iteration 279/1000 | Loss: 0.00002608
Iteration 280/1000 | Loss: 0.00002608
Iteration 281/1000 | Loss: 0.00002608
Iteration 282/1000 | Loss: 0.00002608
Iteration 283/1000 | Loss: 0.00002608
Iteration 284/1000 | Loss: 0.00002608
Iteration 285/1000 | Loss: 0.00002608
Iteration 286/1000 | Loss: 0.00002608
Iteration 287/1000 | Loss: 0.00002608
Iteration 288/1000 | Loss: 0.00002608
Iteration 289/1000 | Loss: 0.00002608
Iteration 290/1000 | Loss: 0.00002608
Iteration 291/1000 | Loss: 0.00002608
Iteration 292/1000 | Loss: 0.00002608
Iteration 293/1000 | Loss: 0.00002608
Iteration 294/1000 | Loss: 0.00002608
Iteration 295/1000 | Loss: 0.00002608
Iteration 296/1000 | Loss: 0.00002608
Iteration 297/1000 | Loss: 0.00002608
Iteration 298/1000 | Loss: 0.00002608
Iteration 299/1000 | Loss: 0.00002608
Iteration 300/1000 | Loss: 0.00002608
Iteration 301/1000 | Loss: 0.00002608
Iteration 302/1000 | Loss: 0.00002608
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 302. Stopping optimization.
Last 5 losses: [2.6076537324115634e-05, 2.6076537324115634e-05, 2.6076537324115634e-05, 2.6076537324115634e-05, 2.6076537324115634e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.6076537324115634e-05

Optimization complete. Final v2v error: 4.3562331199646 mm

Highest mean error: 5.193174362182617 mm for frame 235

Lowest mean error: 3.1994733810424805 mm for frame 2

Saving results

Total time: 149.94968271255493
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_30_nl_6559/0012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_nl_6559/0012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_nl_6559/0012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00713155
Iteration 2/25 | Loss: 0.00111642
Iteration 3/25 | Loss: 0.00099471
Iteration 4/25 | Loss: 0.00095050
Iteration 5/25 | Loss: 0.00094133
Iteration 6/25 | Loss: 0.00093894
Iteration 7/25 | Loss: 0.00093762
Iteration 8/25 | Loss: 0.00093762
Iteration 9/25 | Loss: 0.00093762
Iteration 10/25 | Loss: 0.00093762
Iteration 11/25 | Loss: 0.00093762
Iteration 12/25 | Loss: 0.00093762
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0009376183734275401, 0.0009376183734275401, 0.0009376183734275401, 0.0009376183734275401, 0.0009376183734275401]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009376183734275401

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.53731012
Iteration 2/25 | Loss: 0.00142532
Iteration 3/25 | Loss: 0.00142531
Iteration 4/25 | Loss: 0.00142531
Iteration 5/25 | Loss: 0.00142531
Iteration 6/25 | Loss: 0.00142531
Iteration 7/25 | Loss: 0.00142531
Iteration 8/25 | Loss: 0.00142531
Iteration 9/25 | Loss: 0.00142531
Iteration 10/25 | Loss: 0.00142531
Iteration 11/25 | Loss: 0.00142531
Iteration 12/25 | Loss: 0.00142531
Iteration 13/25 | Loss: 0.00142531
Iteration 14/25 | Loss: 0.00142531
Iteration 15/25 | Loss: 0.00142531
Iteration 16/25 | Loss: 0.00142531
Iteration 17/25 | Loss: 0.00142531
Iteration 18/25 | Loss: 0.00142531
Iteration 19/25 | Loss: 0.00142531
Iteration 20/25 | Loss: 0.00142531
Iteration 21/25 | Loss: 0.00142531
Iteration 22/25 | Loss: 0.00142531
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0014253099216148257, 0.0014253099216148257, 0.0014253099216148257, 0.0014253099216148257, 0.0014253099216148257]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014253099216148257

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00142531
Iteration 2/1000 | Loss: 0.00006035
Iteration 3/1000 | Loss: 0.00004252
Iteration 4/1000 | Loss: 0.00003831
Iteration 5/1000 | Loss: 0.00003643
Iteration 6/1000 | Loss: 0.00003524
Iteration 7/1000 | Loss: 0.00003438
Iteration 8/1000 | Loss: 0.00003341
Iteration 9/1000 | Loss: 0.00003276
Iteration 10/1000 | Loss: 0.00003237
Iteration 11/1000 | Loss: 0.00003213
Iteration 12/1000 | Loss: 0.00003199
Iteration 13/1000 | Loss: 0.00003197
Iteration 14/1000 | Loss: 0.00003195
Iteration 15/1000 | Loss: 0.00003194
Iteration 16/1000 | Loss: 0.00003187
Iteration 17/1000 | Loss: 0.00003180
Iteration 18/1000 | Loss: 0.00003177
Iteration 19/1000 | Loss: 0.00003176
Iteration 20/1000 | Loss: 0.00003176
Iteration 21/1000 | Loss: 0.00003176
Iteration 22/1000 | Loss: 0.00003176
Iteration 23/1000 | Loss: 0.00003176
Iteration 24/1000 | Loss: 0.00003175
Iteration 25/1000 | Loss: 0.00003175
Iteration 26/1000 | Loss: 0.00003175
Iteration 27/1000 | Loss: 0.00003174
Iteration 28/1000 | Loss: 0.00003174
Iteration 29/1000 | Loss: 0.00003174
Iteration 30/1000 | Loss: 0.00003173
Iteration 31/1000 | Loss: 0.00003173
Iteration 32/1000 | Loss: 0.00003172
Iteration 33/1000 | Loss: 0.00003172
Iteration 34/1000 | Loss: 0.00003172
Iteration 35/1000 | Loss: 0.00003171
Iteration 36/1000 | Loss: 0.00003171
Iteration 37/1000 | Loss: 0.00003171
Iteration 38/1000 | Loss: 0.00003170
Iteration 39/1000 | Loss: 0.00003170
Iteration 40/1000 | Loss: 0.00003170
Iteration 41/1000 | Loss: 0.00003169
Iteration 42/1000 | Loss: 0.00003169
Iteration 43/1000 | Loss: 0.00003169
Iteration 44/1000 | Loss: 0.00003169
Iteration 45/1000 | Loss: 0.00003168
Iteration 46/1000 | Loss: 0.00003168
Iteration 47/1000 | Loss: 0.00003167
Iteration 48/1000 | Loss: 0.00003167
Iteration 49/1000 | Loss: 0.00003166
Iteration 50/1000 | Loss: 0.00003166
Iteration 51/1000 | Loss: 0.00003165
Iteration 52/1000 | Loss: 0.00003165
Iteration 53/1000 | Loss: 0.00003165
Iteration 54/1000 | Loss: 0.00003165
Iteration 55/1000 | Loss: 0.00003164
Iteration 56/1000 | Loss: 0.00003164
Iteration 57/1000 | Loss: 0.00003163
Iteration 58/1000 | Loss: 0.00003163
Iteration 59/1000 | Loss: 0.00003163
Iteration 60/1000 | Loss: 0.00003163
Iteration 61/1000 | Loss: 0.00003163
Iteration 62/1000 | Loss: 0.00003163
Iteration 63/1000 | Loss: 0.00003163
Iteration 64/1000 | Loss: 0.00003163
Iteration 65/1000 | Loss: 0.00003162
Iteration 66/1000 | Loss: 0.00003162
Iteration 67/1000 | Loss: 0.00003162
Iteration 68/1000 | Loss: 0.00003162
Iteration 69/1000 | Loss: 0.00003162
Iteration 70/1000 | Loss: 0.00003161
Iteration 71/1000 | Loss: 0.00003161
Iteration 72/1000 | Loss: 0.00003161
Iteration 73/1000 | Loss: 0.00003161
Iteration 74/1000 | Loss: 0.00003161
Iteration 75/1000 | Loss: 0.00003161
Iteration 76/1000 | Loss: 0.00003160
Iteration 77/1000 | Loss: 0.00003160
Iteration 78/1000 | Loss: 0.00003160
Iteration 79/1000 | Loss: 0.00003160
Iteration 80/1000 | Loss: 0.00003159
Iteration 81/1000 | Loss: 0.00003159
Iteration 82/1000 | Loss: 0.00003159
Iteration 83/1000 | Loss: 0.00003158
Iteration 84/1000 | Loss: 0.00003158
Iteration 85/1000 | Loss: 0.00003158
Iteration 86/1000 | Loss: 0.00003158
Iteration 87/1000 | Loss: 0.00003158
Iteration 88/1000 | Loss: 0.00003157
Iteration 89/1000 | Loss: 0.00003157
Iteration 90/1000 | Loss: 0.00003157
Iteration 91/1000 | Loss: 0.00003156
Iteration 92/1000 | Loss: 0.00003156
Iteration 93/1000 | Loss: 0.00003156
Iteration 94/1000 | Loss: 0.00003155
Iteration 95/1000 | Loss: 0.00003155
Iteration 96/1000 | Loss: 0.00003155
Iteration 97/1000 | Loss: 0.00003155
Iteration 98/1000 | Loss: 0.00003155
Iteration 99/1000 | Loss: 0.00003155
Iteration 100/1000 | Loss: 0.00003155
Iteration 101/1000 | Loss: 0.00003155
Iteration 102/1000 | Loss: 0.00003155
Iteration 103/1000 | Loss: 0.00003155
Iteration 104/1000 | Loss: 0.00003155
Iteration 105/1000 | Loss: 0.00003155
Iteration 106/1000 | Loss: 0.00003155
Iteration 107/1000 | Loss: 0.00003154
Iteration 108/1000 | Loss: 0.00003154
Iteration 109/1000 | Loss: 0.00003154
Iteration 110/1000 | Loss: 0.00003154
Iteration 111/1000 | Loss: 0.00003154
Iteration 112/1000 | Loss: 0.00003154
Iteration 113/1000 | Loss: 0.00003154
Iteration 114/1000 | Loss: 0.00003154
Iteration 115/1000 | Loss: 0.00003154
Iteration 116/1000 | Loss: 0.00003154
Iteration 117/1000 | Loss: 0.00003154
Iteration 118/1000 | Loss: 0.00003154
Iteration 119/1000 | Loss: 0.00003153
Iteration 120/1000 | Loss: 0.00003153
Iteration 121/1000 | Loss: 0.00003153
Iteration 122/1000 | Loss: 0.00003153
Iteration 123/1000 | Loss: 0.00003153
Iteration 124/1000 | Loss: 0.00003153
Iteration 125/1000 | Loss: 0.00003153
Iteration 126/1000 | Loss: 0.00003153
Iteration 127/1000 | Loss: 0.00003153
Iteration 128/1000 | Loss: 0.00003153
Iteration 129/1000 | Loss: 0.00003153
Iteration 130/1000 | Loss: 0.00003153
Iteration 131/1000 | Loss: 0.00003153
Iteration 132/1000 | Loss: 0.00003153
Iteration 133/1000 | Loss: 0.00003153
Iteration 134/1000 | Loss: 0.00003153
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 134. Stopping optimization.
Last 5 losses: [3.153003490297124e-05, 3.153003490297124e-05, 3.153003490297124e-05, 3.153003490297124e-05, 3.153003490297124e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.153003490297124e-05

Optimization complete. Final v2v error: 4.5766119956970215 mm

Highest mean error: 4.880069732666016 mm for frame 125

Lowest mean error: 4.039128303527832 mm for frame 234

Saving results

Total time: 44.76734471321106
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_30_nl_6559/0007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_nl_6559/0007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_nl_6559/0007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00877099
Iteration 2/25 | Loss: 0.00120509
Iteration 3/25 | Loss: 0.00093378
Iteration 4/25 | Loss: 0.00090758
Iteration 5/25 | Loss: 0.00089693
Iteration 6/25 | Loss: 0.00089876
Iteration 7/25 | Loss: 0.00089931
Iteration 8/25 | Loss: 0.00089469
Iteration 9/25 | Loss: 0.00088809
Iteration 10/25 | Loss: 0.00088634
Iteration 11/25 | Loss: 0.00088595
Iteration 12/25 | Loss: 0.00088592
Iteration 13/25 | Loss: 0.00088591
Iteration 14/25 | Loss: 0.00088591
Iteration 15/25 | Loss: 0.00088591
Iteration 16/25 | Loss: 0.00088591
Iteration 17/25 | Loss: 0.00088591
Iteration 18/25 | Loss: 0.00088591
Iteration 19/25 | Loss: 0.00088591
Iteration 20/25 | Loss: 0.00088591
Iteration 21/25 | Loss: 0.00088591
Iteration 22/25 | Loss: 0.00088591
Iteration 23/25 | Loss: 0.00088591
Iteration 24/25 | Loss: 0.00088591
Iteration 25/25 | Loss: 0.00088591

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.48777771
Iteration 2/25 | Loss: 0.00121404
Iteration 3/25 | Loss: 0.00121404
Iteration 4/25 | Loss: 0.00121404
Iteration 5/25 | Loss: 0.00121404
Iteration 6/25 | Loss: 0.00121404
Iteration 7/25 | Loss: 0.00121404
Iteration 8/25 | Loss: 0.00121404
Iteration 9/25 | Loss: 0.00121404
Iteration 10/25 | Loss: 0.00121404
Iteration 11/25 | Loss: 0.00121404
Iteration 12/25 | Loss: 0.00121404
Iteration 13/25 | Loss: 0.00121404
Iteration 14/25 | Loss: 0.00121404
Iteration 15/25 | Loss: 0.00121404
Iteration 16/25 | Loss: 0.00121404
Iteration 17/25 | Loss: 0.00121404
Iteration 18/25 | Loss: 0.00121404
Iteration 19/25 | Loss: 0.00121404
Iteration 20/25 | Loss: 0.00121404
Iteration 21/25 | Loss: 0.00121404
Iteration 22/25 | Loss: 0.00121404
Iteration 23/25 | Loss: 0.00121404
Iteration 24/25 | Loss: 0.00121404
Iteration 25/25 | Loss: 0.00121404

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00121404
Iteration 2/1000 | Loss: 0.00004258
Iteration 3/1000 | Loss: 0.00002717
Iteration 4/1000 | Loss: 0.00002459
Iteration 5/1000 | Loss: 0.00002374
Iteration 6/1000 | Loss: 0.00002319
Iteration 7/1000 | Loss: 0.00002273
Iteration 8/1000 | Loss: 0.00002234
Iteration 9/1000 | Loss: 0.00002207
Iteration 10/1000 | Loss: 0.00002188
Iteration 11/1000 | Loss: 0.00002183
Iteration 12/1000 | Loss: 0.00002182
Iteration 13/1000 | Loss: 0.00002181
Iteration 14/1000 | Loss: 0.00002177
Iteration 15/1000 | Loss: 0.00002176
Iteration 16/1000 | Loss: 0.00002176
Iteration 17/1000 | Loss: 0.00002175
Iteration 18/1000 | Loss: 0.00002174
Iteration 19/1000 | Loss: 0.00002172
Iteration 20/1000 | Loss: 0.00002172
Iteration 21/1000 | Loss: 0.00002171
Iteration 22/1000 | Loss: 0.00002169
Iteration 23/1000 | Loss: 0.00002169
Iteration 24/1000 | Loss: 0.00002168
Iteration 25/1000 | Loss: 0.00002167
Iteration 26/1000 | Loss: 0.00002166
Iteration 27/1000 | Loss: 0.00002165
Iteration 28/1000 | Loss: 0.00002165
Iteration 29/1000 | Loss: 0.00002164
Iteration 30/1000 | Loss: 0.00002164
Iteration 31/1000 | Loss: 0.00002164
Iteration 32/1000 | Loss: 0.00002163
Iteration 33/1000 | Loss: 0.00002163
Iteration 34/1000 | Loss: 0.00002163
Iteration 35/1000 | Loss: 0.00002162
Iteration 36/1000 | Loss: 0.00002162
Iteration 37/1000 | Loss: 0.00002162
Iteration 38/1000 | Loss: 0.00002162
Iteration 39/1000 | Loss: 0.00002161
Iteration 40/1000 | Loss: 0.00002161
Iteration 41/1000 | Loss: 0.00002161
Iteration 42/1000 | Loss: 0.00002160
Iteration 43/1000 | Loss: 0.00002160
Iteration 44/1000 | Loss: 0.00002160
Iteration 45/1000 | Loss: 0.00002160
Iteration 46/1000 | Loss: 0.00002160
Iteration 47/1000 | Loss: 0.00002159
Iteration 48/1000 | Loss: 0.00002159
Iteration 49/1000 | Loss: 0.00002159
Iteration 50/1000 | Loss: 0.00002159
Iteration 51/1000 | Loss: 0.00002159
Iteration 52/1000 | Loss: 0.00002159
Iteration 53/1000 | Loss: 0.00002158
Iteration 54/1000 | Loss: 0.00002158
Iteration 55/1000 | Loss: 0.00002158
Iteration 56/1000 | Loss: 0.00002158
Iteration 57/1000 | Loss: 0.00002157
Iteration 58/1000 | Loss: 0.00002157
Iteration 59/1000 | Loss: 0.00002157
Iteration 60/1000 | Loss: 0.00002157
Iteration 61/1000 | Loss: 0.00002157
Iteration 62/1000 | Loss: 0.00002157
Iteration 63/1000 | Loss: 0.00002157
Iteration 64/1000 | Loss: 0.00002157
Iteration 65/1000 | Loss: 0.00002156
Iteration 66/1000 | Loss: 0.00002156
Iteration 67/1000 | Loss: 0.00002156
Iteration 68/1000 | Loss: 0.00002156
Iteration 69/1000 | Loss: 0.00002156
Iteration 70/1000 | Loss: 0.00002156
Iteration 71/1000 | Loss: 0.00002155
Iteration 72/1000 | Loss: 0.00002155
Iteration 73/1000 | Loss: 0.00002155
Iteration 74/1000 | Loss: 0.00002154
Iteration 75/1000 | Loss: 0.00002154
Iteration 76/1000 | Loss: 0.00002154
Iteration 77/1000 | Loss: 0.00002154
Iteration 78/1000 | Loss: 0.00002153
Iteration 79/1000 | Loss: 0.00002153
Iteration 80/1000 | Loss: 0.00002153
Iteration 81/1000 | Loss: 0.00002153
Iteration 82/1000 | Loss: 0.00002153
Iteration 83/1000 | Loss: 0.00002153
Iteration 84/1000 | Loss: 0.00002152
Iteration 85/1000 | Loss: 0.00002152
Iteration 86/1000 | Loss: 0.00002152
Iteration 87/1000 | Loss: 0.00002152
Iteration 88/1000 | Loss: 0.00002152
Iteration 89/1000 | Loss: 0.00002152
Iteration 90/1000 | Loss: 0.00002152
Iteration 91/1000 | Loss: 0.00002151
Iteration 92/1000 | Loss: 0.00002151
Iteration 93/1000 | Loss: 0.00002151
Iteration 94/1000 | Loss: 0.00002151
Iteration 95/1000 | Loss: 0.00002151
Iteration 96/1000 | Loss: 0.00002151
Iteration 97/1000 | Loss: 0.00002151
Iteration 98/1000 | Loss: 0.00002150
Iteration 99/1000 | Loss: 0.00002150
Iteration 100/1000 | Loss: 0.00002150
Iteration 101/1000 | Loss: 0.00002150
Iteration 102/1000 | Loss: 0.00002150
Iteration 103/1000 | Loss: 0.00002150
Iteration 104/1000 | Loss: 0.00002150
Iteration 105/1000 | Loss: 0.00002149
Iteration 106/1000 | Loss: 0.00002149
Iteration 107/1000 | Loss: 0.00002149
Iteration 108/1000 | Loss: 0.00002149
Iteration 109/1000 | Loss: 0.00002149
Iteration 110/1000 | Loss: 0.00002149
Iteration 111/1000 | Loss: 0.00002149
Iteration 112/1000 | Loss: 0.00002149
Iteration 113/1000 | Loss: 0.00002149
Iteration 114/1000 | Loss: 0.00002149
Iteration 115/1000 | Loss: 0.00002149
Iteration 116/1000 | Loss: 0.00002148
Iteration 117/1000 | Loss: 0.00002148
Iteration 118/1000 | Loss: 0.00002148
Iteration 119/1000 | Loss: 0.00002148
Iteration 120/1000 | Loss: 0.00002148
Iteration 121/1000 | Loss: 0.00002148
Iteration 122/1000 | Loss: 0.00002148
Iteration 123/1000 | Loss: 0.00002148
Iteration 124/1000 | Loss: 0.00002148
Iteration 125/1000 | Loss: 0.00002148
Iteration 126/1000 | Loss: 0.00002148
Iteration 127/1000 | Loss: 0.00002148
Iteration 128/1000 | Loss: 0.00002148
Iteration 129/1000 | Loss: 0.00002148
Iteration 130/1000 | Loss: 0.00002148
Iteration 131/1000 | Loss: 0.00002148
Iteration 132/1000 | Loss: 0.00002148
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 132. Stopping optimization.
Last 5 losses: [2.1477673726622015e-05, 2.1477673726622015e-05, 2.1477673726622015e-05, 2.1477673726622015e-05, 2.1477673726622015e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1477673726622015e-05

Optimization complete. Final v2v error: 3.964369297027588 mm

Highest mean error: 4.631095886230469 mm for frame 95

Lowest mean error: 3.4068667888641357 mm for frame 47

Saving results

Total time: 41.41862630844116
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_30_nl_6559/0003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_nl_6559/0003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_nl_6559/0003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01080056
Iteration 2/25 | Loss: 0.00626320
Iteration 3/25 | Loss: 0.00283613
Iteration 4/25 | Loss: 0.00213935
Iteration 5/25 | Loss: 0.00192313
Iteration 6/25 | Loss: 0.00168397
Iteration 7/25 | Loss: 0.00158613
Iteration 8/25 | Loss: 0.00149255
Iteration 9/25 | Loss: 0.00149325
Iteration 10/25 | Loss: 0.00157736
Iteration 11/25 | Loss: 0.00145129
Iteration 12/25 | Loss: 0.00144927
Iteration 13/25 | Loss: 0.00142923
Iteration 14/25 | Loss: 0.00141297
Iteration 15/25 | Loss: 0.00139380
Iteration 16/25 | Loss: 0.00138658
Iteration 17/25 | Loss: 0.00136168
Iteration 18/25 | Loss: 0.00136367
Iteration 19/25 | Loss: 0.00134423
Iteration 20/25 | Loss: 0.00133265
Iteration 21/25 | Loss: 0.00132549
Iteration 22/25 | Loss: 0.00132332
Iteration 23/25 | Loss: 0.00131436
Iteration 24/25 | Loss: 0.00131057
Iteration 25/25 | Loss: 0.00133181

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.60393786
Iteration 2/25 | Loss: 0.01159020
Iteration 3/25 | Loss: 0.00611598
Iteration 4/25 | Loss: 0.00611598
Iteration 5/25 | Loss: 0.00611598
Iteration 6/25 | Loss: 0.00611598
Iteration 7/25 | Loss: 0.00611598
Iteration 8/25 | Loss: 0.00611598
Iteration 9/25 | Loss: 0.00611598
Iteration 10/25 | Loss: 0.00611598
Iteration 11/25 | Loss: 0.00611598
Iteration 12/25 | Loss: 0.00611598
Iteration 13/25 | Loss: 0.00611598
Iteration 14/25 | Loss: 0.00611598
Iteration 15/25 | Loss: 0.00611598
Iteration 16/25 | Loss: 0.00611598
Iteration 17/25 | Loss: 0.00611598
Iteration 18/25 | Loss: 0.00611598
Iteration 19/25 | Loss: 0.00611598
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.00611597765237093, 0.00611597765237093, 0.00611597765237093, 0.00611597765237093, 0.00611597765237093]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00611597765237093

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00611598
Iteration 2/1000 | Loss: 0.00645008
Iteration 3/1000 | Loss: 0.01558120
Iteration 4/1000 | Loss: 0.00349977
Iteration 5/1000 | Loss: 0.00277818
Iteration 6/1000 | Loss: 0.00180378
Iteration 7/1000 | Loss: 0.00190108
Iteration 8/1000 | Loss: 0.00230270
Iteration 9/1000 | Loss: 0.00144563
Iteration 10/1000 | Loss: 0.00128834
Iteration 11/1000 | Loss: 0.00152255
Iteration 12/1000 | Loss: 0.00178862
Iteration 13/1000 | Loss: 0.00141175
Iteration 14/1000 | Loss: 0.00147130
Iteration 15/1000 | Loss: 0.00147978
Iteration 16/1000 | Loss: 0.00389947
Iteration 17/1000 | Loss: 0.00102291
Iteration 18/1000 | Loss: 0.00137027
Iteration 19/1000 | Loss: 0.00034532
Iteration 20/1000 | Loss: 0.00056038
Iteration 21/1000 | Loss: 0.00051786
Iteration 22/1000 | Loss: 0.00115849
Iteration 23/1000 | Loss: 0.00041343
Iteration 24/1000 | Loss: 0.00108370
Iteration 25/1000 | Loss: 0.00059782
Iteration 26/1000 | Loss: 0.00039958
Iteration 27/1000 | Loss: 0.00027978
Iteration 28/1000 | Loss: 0.00042782
Iteration 29/1000 | Loss: 0.00019810
Iteration 30/1000 | Loss: 0.00064415
Iteration 31/1000 | Loss: 0.00056209
Iteration 32/1000 | Loss: 0.00034931
Iteration 33/1000 | Loss: 0.00081319
Iteration 34/1000 | Loss: 0.00030204
Iteration 35/1000 | Loss: 0.00172182
Iteration 36/1000 | Loss: 0.00038690
Iteration 37/1000 | Loss: 0.00054743
Iteration 38/1000 | Loss: 0.00041425
Iteration 39/1000 | Loss: 0.00037109
Iteration 40/1000 | Loss: 0.00038268
Iteration 41/1000 | Loss: 0.00041517
Iteration 42/1000 | Loss: 0.00044094
Iteration 43/1000 | Loss: 0.00045182
Iteration 44/1000 | Loss: 0.00021869
Iteration 45/1000 | Loss: 0.00062719
Iteration 46/1000 | Loss: 0.00029329
Iteration 47/1000 | Loss: 0.00008984
Iteration 48/1000 | Loss: 0.00063864
Iteration 49/1000 | Loss: 0.00034090
Iteration 50/1000 | Loss: 0.00022701
Iteration 51/1000 | Loss: 0.00022728
Iteration 52/1000 | Loss: 0.00053141
Iteration 53/1000 | Loss: 0.00022931
Iteration 54/1000 | Loss: 0.00068338
Iteration 55/1000 | Loss: 0.00090131
Iteration 56/1000 | Loss: 0.00108726
Iteration 57/1000 | Loss: 0.00161849
Iteration 58/1000 | Loss: 0.00037723
Iteration 59/1000 | Loss: 0.00022886
Iteration 60/1000 | Loss: 0.00045250
Iteration 61/1000 | Loss: 0.00054454
Iteration 62/1000 | Loss: 0.00037740
Iteration 63/1000 | Loss: 0.00034618
Iteration 64/1000 | Loss: 0.00031017
Iteration 65/1000 | Loss: 0.00046960
Iteration 66/1000 | Loss: 0.00068654
Iteration 67/1000 | Loss: 0.00025180
Iteration 68/1000 | Loss: 0.00013289
Iteration 69/1000 | Loss: 0.00009767
Iteration 70/1000 | Loss: 0.00007649
Iteration 71/1000 | Loss: 0.00093384
Iteration 72/1000 | Loss: 0.00075748
Iteration 73/1000 | Loss: 0.00008553
Iteration 74/1000 | Loss: 0.00035996
Iteration 75/1000 | Loss: 0.00050202
Iteration 76/1000 | Loss: 0.00038127
Iteration 77/1000 | Loss: 0.00010229
Iteration 78/1000 | Loss: 0.00007114
Iteration 79/1000 | Loss: 0.00009464
Iteration 80/1000 | Loss: 0.00076769
Iteration 81/1000 | Loss: 0.00026158
Iteration 82/1000 | Loss: 0.00074194
Iteration 83/1000 | Loss: 0.00061758
Iteration 84/1000 | Loss: 0.00014375
Iteration 85/1000 | Loss: 0.00020016
Iteration 86/1000 | Loss: 0.00006084
Iteration 87/1000 | Loss: 0.00082055
Iteration 88/1000 | Loss: 0.00032258
Iteration 89/1000 | Loss: 0.00020055
Iteration 90/1000 | Loss: 0.00008593
Iteration 91/1000 | Loss: 0.00042912
Iteration 92/1000 | Loss: 0.00037670
Iteration 93/1000 | Loss: 0.00009429
Iteration 94/1000 | Loss: 0.00050564
Iteration 95/1000 | Loss: 0.00059784
Iteration 96/1000 | Loss: 0.00050555
Iteration 97/1000 | Loss: 0.00074809
Iteration 98/1000 | Loss: 0.00054013
Iteration 99/1000 | Loss: 0.00043294
Iteration 100/1000 | Loss: 0.00062492
Iteration 101/1000 | Loss: 0.00045746
Iteration 102/1000 | Loss: 0.00008300
Iteration 103/1000 | Loss: 0.00062859
Iteration 104/1000 | Loss: 0.00052831
Iteration 105/1000 | Loss: 0.00006897
Iteration 106/1000 | Loss: 0.00061304
Iteration 107/1000 | Loss: 0.00052872
Iteration 108/1000 | Loss: 0.00013271
Iteration 109/1000 | Loss: 0.00005214
Iteration 110/1000 | Loss: 0.00042740
Iteration 111/1000 | Loss: 0.00007557
Iteration 112/1000 | Loss: 0.00009584
Iteration 113/1000 | Loss: 0.00021886
Iteration 114/1000 | Loss: 0.00004942
Iteration 115/1000 | Loss: 0.00013628
Iteration 116/1000 | Loss: 0.00005363
Iteration 117/1000 | Loss: 0.00005000
Iteration 118/1000 | Loss: 0.00005215
Iteration 119/1000 | Loss: 0.00095774
Iteration 120/1000 | Loss: 0.00104040
Iteration 121/1000 | Loss: 0.00109045
Iteration 122/1000 | Loss: 0.00012021
Iteration 123/1000 | Loss: 0.00007833
Iteration 124/1000 | Loss: 0.00005293
Iteration 125/1000 | Loss: 0.00013702
Iteration 126/1000 | Loss: 0.00004295
Iteration 127/1000 | Loss: 0.00006148
Iteration 128/1000 | Loss: 0.00005417
Iteration 129/1000 | Loss: 0.00003883
Iteration 130/1000 | Loss: 0.00018163
Iteration 131/1000 | Loss: 0.00013784
Iteration 132/1000 | Loss: 0.00036856
Iteration 133/1000 | Loss: 0.00036296
Iteration 134/1000 | Loss: 0.00038639
Iteration 135/1000 | Loss: 0.00005440
Iteration 136/1000 | Loss: 0.00057446
Iteration 137/1000 | Loss: 0.00003915
Iteration 138/1000 | Loss: 0.00006276
Iteration 139/1000 | Loss: 0.00003760
Iteration 140/1000 | Loss: 0.00003739
Iteration 141/1000 | Loss: 0.00007060
Iteration 142/1000 | Loss: 0.00013085
Iteration 143/1000 | Loss: 0.00004208
Iteration 144/1000 | Loss: 0.00004118
Iteration 145/1000 | Loss: 0.00003721
Iteration 146/1000 | Loss: 0.00003720
Iteration 147/1000 | Loss: 0.00003719
Iteration 148/1000 | Loss: 0.00003719
Iteration 149/1000 | Loss: 0.00003718
Iteration 150/1000 | Loss: 0.00003717
Iteration 151/1000 | Loss: 0.00003717
Iteration 152/1000 | Loss: 0.00003716
Iteration 153/1000 | Loss: 0.00003716
Iteration 154/1000 | Loss: 0.00003716
Iteration 155/1000 | Loss: 0.00003715
Iteration 156/1000 | Loss: 0.00003714
Iteration 157/1000 | Loss: 0.00003714
Iteration 158/1000 | Loss: 0.00003713
Iteration 159/1000 | Loss: 0.00003713
Iteration 160/1000 | Loss: 0.00003713
Iteration 161/1000 | Loss: 0.00003713
Iteration 162/1000 | Loss: 0.00003713
Iteration 163/1000 | Loss: 0.00006108
Iteration 164/1000 | Loss: 0.00008155
Iteration 165/1000 | Loss: 0.00005928
Iteration 166/1000 | Loss: 0.00014742
Iteration 167/1000 | Loss: 0.00005839
Iteration 168/1000 | Loss: 0.00003935
Iteration 169/1000 | Loss: 0.00011296
Iteration 170/1000 | Loss: 0.00004482
Iteration 171/1000 | Loss: 0.00005450
Iteration 172/1000 | Loss: 0.00004591
Iteration 173/1000 | Loss: 0.00003712
Iteration 174/1000 | Loss: 0.00004950
Iteration 175/1000 | Loss: 0.00006224
Iteration 176/1000 | Loss: 0.00004426
Iteration 177/1000 | Loss: 0.00004008
Iteration 178/1000 | Loss: 0.00003848
Iteration 179/1000 | Loss: 0.00003759
Iteration 180/1000 | Loss: 0.00005640
Iteration 181/1000 | Loss: 0.00003706
Iteration 182/1000 | Loss: 0.00003756
Iteration 183/1000 | Loss: 0.00005142
Iteration 184/1000 | Loss: 0.00003757
Iteration 185/1000 | Loss: 0.00003754
Iteration 186/1000 | Loss: 0.00003753
Iteration 187/1000 | Loss: 0.00004053
Iteration 188/1000 | Loss: 0.00003701
Iteration 189/1000 | Loss: 0.00003701
Iteration 190/1000 | Loss: 0.00003700
Iteration 191/1000 | Loss: 0.00003700
Iteration 192/1000 | Loss: 0.00003698
Iteration 193/1000 | Loss: 0.00003698
Iteration 194/1000 | Loss: 0.00003698
Iteration 195/1000 | Loss: 0.00003698
Iteration 196/1000 | Loss: 0.00003698
Iteration 197/1000 | Loss: 0.00003698
Iteration 198/1000 | Loss: 0.00003698
Iteration 199/1000 | Loss: 0.00003698
Iteration 200/1000 | Loss: 0.00003698
Iteration 201/1000 | Loss: 0.00003698
Iteration 202/1000 | Loss: 0.00003698
Iteration 203/1000 | Loss: 0.00003698
Iteration 204/1000 | Loss: 0.00003698
Iteration 205/1000 | Loss: 0.00003698
Iteration 206/1000 | Loss: 0.00003698
Iteration 207/1000 | Loss: 0.00003698
Iteration 208/1000 | Loss: 0.00003698
Iteration 209/1000 | Loss: 0.00003698
Iteration 210/1000 | Loss: 0.00003698
Iteration 211/1000 | Loss: 0.00003698
Iteration 212/1000 | Loss: 0.00003698
Iteration 213/1000 | Loss: 0.00003698
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 213. Stopping optimization.
Last 5 losses: [3.697758438647725e-05, 3.697758438647725e-05, 3.697758438647725e-05, 3.697758438647725e-05, 3.697758438647725e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.697758438647725e-05

Optimization complete. Final v2v error: 4.5201311111450195 mm

Highest mean error: 13.553918838500977 mm for frame 80

Lowest mean error: 3.8161423206329346 mm for frame 193

Saving results

Total time: 311.98142981529236
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_30_nl_6559/0018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_nl_6559/0018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_nl_6559/0018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00412761
Iteration 2/25 | Loss: 0.00092134
Iteration 3/25 | Loss: 0.00083752
Iteration 4/25 | Loss: 0.00081761
Iteration 5/25 | Loss: 0.00081045
Iteration 6/25 | Loss: 0.00080878
Iteration 7/25 | Loss: 0.00080847
Iteration 8/25 | Loss: 0.00080847
Iteration 9/25 | Loss: 0.00080847
Iteration 10/25 | Loss: 0.00080847
Iteration 11/25 | Loss: 0.00080847
Iteration 12/25 | Loss: 0.00080847
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0008084735018201172, 0.0008084735018201172, 0.0008084735018201172, 0.0008084735018201172, 0.0008084735018201172]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008084735018201172

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.53847599
Iteration 2/25 | Loss: 0.00101535
Iteration 3/25 | Loss: 0.00101535
Iteration 4/25 | Loss: 0.00101535
Iteration 5/25 | Loss: 0.00101535
Iteration 6/25 | Loss: 0.00101535
Iteration 7/25 | Loss: 0.00101535
Iteration 8/25 | Loss: 0.00101535
Iteration 9/25 | Loss: 0.00101535
Iteration 10/25 | Loss: 0.00101535
Iteration 11/25 | Loss: 0.00101535
Iteration 12/25 | Loss: 0.00101535
Iteration 13/25 | Loss: 0.00101535
Iteration 14/25 | Loss: 0.00101535
Iteration 15/25 | Loss: 0.00101535
Iteration 16/25 | Loss: 0.00101535
Iteration 17/25 | Loss: 0.00101535
Iteration 18/25 | Loss: 0.00101535
Iteration 19/25 | Loss: 0.00101535
Iteration 20/25 | Loss: 0.00101535
Iteration 21/25 | Loss: 0.00101535
Iteration 22/25 | Loss: 0.00101535
Iteration 23/25 | Loss: 0.00101535
Iteration 24/25 | Loss: 0.00101535
Iteration 25/25 | Loss: 0.00101535

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00101535
Iteration 2/1000 | Loss: 0.00002971
Iteration 3/1000 | Loss: 0.00001995
Iteration 4/1000 | Loss: 0.00001798
Iteration 5/1000 | Loss: 0.00001718
Iteration 6/1000 | Loss: 0.00001656
Iteration 7/1000 | Loss: 0.00001617
Iteration 8/1000 | Loss: 0.00001578
Iteration 9/1000 | Loss: 0.00001572
Iteration 10/1000 | Loss: 0.00001569
Iteration 11/1000 | Loss: 0.00001569
Iteration 12/1000 | Loss: 0.00001565
Iteration 13/1000 | Loss: 0.00001562
Iteration 14/1000 | Loss: 0.00001562
Iteration 15/1000 | Loss: 0.00001557
Iteration 16/1000 | Loss: 0.00001557
Iteration 17/1000 | Loss: 0.00001556
Iteration 18/1000 | Loss: 0.00001556
Iteration 19/1000 | Loss: 0.00001556
Iteration 20/1000 | Loss: 0.00001556
Iteration 21/1000 | Loss: 0.00001556
Iteration 22/1000 | Loss: 0.00001556
Iteration 23/1000 | Loss: 0.00001556
Iteration 24/1000 | Loss: 0.00001555
Iteration 25/1000 | Loss: 0.00001555
Iteration 26/1000 | Loss: 0.00001555
Iteration 27/1000 | Loss: 0.00001555
Iteration 28/1000 | Loss: 0.00001555
Iteration 29/1000 | Loss: 0.00001555
Iteration 30/1000 | Loss: 0.00001554
Iteration 31/1000 | Loss: 0.00001554
Iteration 32/1000 | Loss: 0.00001554
Iteration 33/1000 | Loss: 0.00001554
Iteration 34/1000 | Loss: 0.00001554
Iteration 35/1000 | Loss: 0.00001553
Iteration 36/1000 | Loss: 0.00001553
Iteration 37/1000 | Loss: 0.00001553
Iteration 38/1000 | Loss: 0.00001553
Iteration 39/1000 | Loss: 0.00001552
Iteration 40/1000 | Loss: 0.00001552
Iteration 41/1000 | Loss: 0.00001552
Iteration 42/1000 | Loss: 0.00001552
Iteration 43/1000 | Loss: 0.00001552
Iteration 44/1000 | Loss: 0.00001552
Iteration 45/1000 | Loss: 0.00001552
Iteration 46/1000 | Loss: 0.00001552
Iteration 47/1000 | Loss: 0.00001552
Iteration 48/1000 | Loss: 0.00001552
Iteration 49/1000 | Loss: 0.00001552
Iteration 50/1000 | Loss: 0.00001552
Iteration 51/1000 | Loss: 0.00001552
Iteration 52/1000 | Loss: 0.00001552
Iteration 53/1000 | Loss: 0.00001552
Iteration 54/1000 | Loss: 0.00001552
Iteration 55/1000 | Loss: 0.00001551
Iteration 56/1000 | Loss: 0.00001551
Iteration 57/1000 | Loss: 0.00001551
Iteration 58/1000 | Loss: 0.00001551
Iteration 59/1000 | Loss: 0.00001551
Iteration 60/1000 | Loss: 0.00001551
Iteration 61/1000 | Loss: 0.00001551
Iteration 62/1000 | Loss: 0.00001551
Iteration 63/1000 | Loss: 0.00001551
Iteration 64/1000 | Loss: 0.00001550
Iteration 65/1000 | Loss: 0.00001550
Iteration 66/1000 | Loss: 0.00001550
Iteration 67/1000 | Loss: 0.00001550
Iteration 68/1000 | Loss: 0.00001550
Iteration 69/1000 | Loss: 0.00001550
Iteration 70/1000 | Loss: 0.00001550
Iteration 71/1000 | Loss: 0.00001550
Iteration 72/1000 | Loss: 0.00001550
Iteration 73/1000 | Loss: 0.00001550
Iteration 74/1000 | Loss: 0.00001550
Iteration 75/1000 | Loss: 0.00001550
Iteration 76/1000 | Loss: 0.00001549
Iteration 77/1000 | Loss: 0.00001549
Iteration 78/1000 | Loss: 0.00001549
Iteration 79/1000 | Loss: 0.00001549
Iteration 80/1000 | Loss: 0.00001549
Iteration 81/1000 | Loss: 0.00001549
Iteration 82/1000 | Loss: 0.00001549
Iteration 83/1000 | Loss: 0.00001549
Iteration 84/1000 | Loss: 0.00001549
Iteration 85/1000 | Loss: 0.00001549
Iteration 86/1000 | Loss: 0.00001548
Iteration 87/1000 | Loss: 0.00001548
Iteration 88/1000 | Loss: 0.00001548
Iteration 89/1000 | Loss: 0.00001548
Iteration 90/1000 | Loss: 0.00001548
Iteration 91/1000 | Loss: 0.00001547
Iteration 92/1000 | Loss: 0.00001547
Iteration 93/1000 | Loss: 0.00001547
Iteration 94/1000 | Loss: 0.00001547
Iteration 95/1000 | Loss: 0.00001547
Iteration 96/1000 | Loss: 0.00001546
Iteration 97/1000 | Loss: 0.00001546
Iteration 98/1000 | Loss: 0.00001546
Iteration 99/1000 | Loss: 0.00001546
Iteration 100/1000 | Loss: 0.00001545
Iteration 101/1000 | Loss: 0.00001545
Iteration 102/1000 | Loss: 0.00001545
Iteration 103/1000 | Loss: 0.00001545
Iteration 104/1000 | Loss: 0.00001545
Iteration 105/1000 | Loss: 0.00001545
Iteration 106/1000 | Loss: 0.00001545
Iteration 107/1000 | Loss: 0.00001544
Iteration 108/1000 | Loss: 0.00001544
Iteration 109/1000 | Loss: 0.00001544
Iteration 110/1000 | Loss: 0.00001544
Iteration 111/1000 | Loss: 0.00001544
Iteration 112/1000 | Loss: 0.00001544
Iteration 113/1000 | Loss: 0.00001544
Iteration 114/1000 | Loss: 0.00001544
Iteration 115/1000 | Loss: 0.00001544
Iteration 116/1000 | Loss: 0.00001544
Iteration 117/1000 | Loss: 0.00001544
Iteration 118/1000 | Loss: 0.00001544
Iteration 119/1000 | Loss: 0.00001544
Iteration 120/1000 | Loss: 0.00001544
Iteration 121/1000 | Loss: 0.00001544
Iteration 122/1000 | Loss: 0.00001544
Iteration 123/1000 | Loss: 0.00001544
Iteration 124/1000 | Loss: 0.00001544
Iteration 125/1000 | Loss: 0.00001544
Iteration 126/1000 | Loss: 0.00001544
Iteration 127/1000 | Loss: 0.00001544
Iteration 128/1000 | Loss: 0.00001544
Iteration 129/1000 | Loss: 0.00001544
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 129. Stopping optimization.
Last 5 losses: [1.544063889014069e-05, 1.544063889014069e-05, 1.544063889014069e-05, 1.544063889014069e-05, 1.544063889014069e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.544063889014069e-05

Optimization complete. Final v2v error: 3.3454596996307373 mm

Highest mean error: 3.516131639480591 mm for frame 28

Lowest mean error: 3.110607147216797 mm for frame 0

Saving results

Total time: 30.29027485847473
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_30_nl_6559/0019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_nl_6559/0019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_nl_6559/0019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01101000
Iteration 2/25 | Loss: 0.00288665
Iteration 3/25 | Loss: 0.00179031
Iteration 4/25 | Loss: 0.00157913
Iteration 5/25 | Loss: 0.00152331
Iteration 6/25 | Loss: 0.00136885
Iteration 7/25 | Loss: 0.00112538
Iteration 8/25 | Loss: 0.00099047
Iteration 9/25 | Loss: 0.00097811
Iteration 10/25 | Loss: 0.00097804
Iteration 11/25 | Loss: 0.00096832
Iteration 12/25 | Loss: 0.00096683
Iteration 13/25 | Loss: 0.00097547
Iteration 14/25 | Loss: 0.00096335
Iteration 15/25 | Loss: 0.00095934
Iteration 16/25 | Loss: 0.00096357
Iteration 17/25 | Loss: 0.00095898
Iteration 18/25 | Loss: 0.00095908
Iteration 19/25 | Loss: 0.00096036
Iteration 20/25 | Loss: 0.00096100
Iteration 21/25 | Loss: 0.00096091
Iteration 22/25 | Loss: 0.00096542
Iteration 23/25 | Loss: 0.00096069
Iteration 24/25 | Loss: 0.00096145
Iteration 25/25 | Loss: 0.00095630

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.55659783
Iteration 2/25 | Loss: 0.00157041
Iteration 3/25 | Loss: 0.00146427
Iteration 4/25 | Loss: 0.00146427
Iteration 5/25 | Loss: 0.00146427
Iteration 6/25 | Loss: 0.00146427
Iteration 7/25 | Loss: 0.00146427
Iteration 8/25 | Loss: 0.00146427
Iteration 9/25 | Loss: 0.00146427
Iteration 10/25 | Loss: 0.00146427
Iteration 11/25 | Loss: 0.00146427
Iteration 12/25 | Loss: 0.00146427
Iteration 13/25 | Loss: 0.00146427
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0014642681926488876, 0.0014642681926488876, 0.0014642681926488876, 0.0014642681926488876, 0.0014642681926488876]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014642681926488876

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00146427
Iteration 2/1000 | Loss: 0.00006257
Iteration 3/1000 | Loss: 0.00016124
Iteration 4/1000 | Loss: 0.00029991
Iteration 5/1000 | Loss: 0.00009145
Iteration 6/1000 | Loss: 0.00008692
Iteration 7/1000 | Loss: 0.00003919
Iteration 8/1000 | Loss: 0.00011406
Iteration 9/1000 | Loss: 0.00061002
Iteration 10/1000 | Loss: 0.00004920
Iteration 11/1000 | Loss: 0.00005049
Iteration 12/1000 | Loss: 0.00003333
Iteration 13/1000 | Loss: 0.00004831
Iteration 14/1000 | Loss: 0.00006065
Iteration 15/1000 | Loss: 0.00010062
Iteration 16/1000 | Loss: 0.00004586
Iteration 17/1000 | Loss: 0.00005238
Iteration 18/1000 | Loss: 0.00005185
Iteration 19/1000 | Loss: 0.00003316
Iteration 20/1000 | Loss: 0.00004889
Iteration 21/1000 | Loss: 0.00009217
Iteration 22/1000 | Loss: 0.00010037
Iteration 23/1000 | Loss: 0.00003602
Iteration 24/1000 | Loss: 0.00004364
Iteration 25/1000 | Loss: 0.00009977
Iteration 26/1000 | Loss: 0.00005626
Iteration 27/1000 | Loss: 0.00004055
Iteration 28/1000 | Loss: 0.00005536
Iteration 29/1000 | Loss: 0.00004875
Iteration 30/1000 | Loss: 0.00017220
Iteration 31/1000 | Loss: 0.00005577
Iteration 32/1000 | Loss: 0.00012539
Iteration 33/1000 | Loss: 0.00027672
Iteration 34/1000 | Loss: 0.00003959
Iteration 35/1000 | Loss: 0.00003221
Iteration 36/1000 | Loss: 0.00003217
Iteration 37/1000 | Loss: 0.00003215
Iteration 38/1000 | Loss: 0.00003215
Iteration 39/1000 | Loss: 0.00003215
Iteration 40/1000 | Loss: 0.00003215
Iteration 41/1000 | Loss: 0.00003215
Iteration 42/1000 | Loss: 0.00003215
Iteration 43/1000 | Loss: 0.00003215
Iteration 44/1000 | Loss: 0.00003215
Iteration 45/1000 | Loss: 0.00003214
Iteration 46/1000 | Loss: 0.00003214
Iteration 47/1000 | Loss: 0.00003214
Iteration 48/1000 | Loss: 0.00003214
Iteration 49/1000 | Loss: 0.00003839
Iteration 50/1000 | Loss: 0.00003323
Iteration 51/1000 | Loss: 0.00003214
Iteration 52/1000 | Loss: 0.00003214
Iteration 53/1000 | Loss: 0.00003213
Iteration 54/1000 | Loss: 0.00003213
Iteration 55/1000 | Loss: 0.00003213
Iteration 56/1000 | Loss: 0.00003213
Iteration 57/1000 | Loss: 0.00003213
Iteration 58/1000 | Loss: 0.00003213
Iteration 59/1000 | Loss: 0.00003213
Iteration 60/1000 | Loss: 0.00003212
Iteration 61/1000 | Loss: 0.00003212
Iteration 62/1000 | Loss: 0.00003212
Iteration 63/1000 | Loss: 0.00003212
Iteration 64/1000 | Loss: 0.00003212
Iteration 65/1000 | Loss: 0.00003212
Iteration 66/1000 | Loss: 0.00003212
Iteration 67/1000 | Loss: 0.00003212
Iteration 68/1000 | Loss: 0.00003211
Iteration 69/1000 | Loss: 0.00003211
Iteration 70/1000 | Loss: 0.00003211
Iteration 71/1000 | Loss: 0.00003211
Iteration 72/1000 | Loss: 0.00003211
Iteration 73/1000 | Loss: 0.00003210
Iteration 74/1000 | Loss: 0.00003210
Iteration 75/1000 | Loss: 0.00003210
Iteration 76/1000 | Loss: 0.00003210
Iteration 77/1000 | Loss: 0.00003210
Iteration 78/1000 | Loss: 0.00003210
Iteration 79/1000 | Loss: 0.00003209
Iteration 80/1000 | Loss: 0.00004004
Iteration 81/1000 | Loss: 0.00004476
Iteration 82/1000 | Loss: 0.00004764
Iteration 83/1000 | Loss: 0.00004763
Iteration 84/1000 | Loss: 0.00004763
Iteration 85/1000 | Loss: 0.00004763
Iteration 86/1000 | Loss: 0.00004763
Iteration 87/1000 | Loss: 0.00004763
Iteration 88/1000 | Loss: 0.00004762
Iteration 89/1000 | Loss: 0.00004762
Iteration 90/1000 | Loss: 0.00005458
Iteration 91/1000 | Loss: 0.00009091
Iteration 92/1000 | Loss: 0.00003450
Iteration 93/1000 | Loss: 0.00005139
Iteration 94/1000 | Loss: 0.00004797
Iteration 95/1000 | Loss: 0.00005494
Iteration 96/1000 | Loss: 0.00060486
Iteration 97/1000 | Loss: 0.00007363
Iteration 98/1000 | Loss: 0.00004699
Iteration 99/1000 | Loss: 0.00006364
Iteration 100/1000 | Loss: 0.00003408
Iteration 101/1000 | Loss: 0.00003278
Iteration 102/1000 | Loss: 0.00003215
Iteration 103/1000 | Loss: 0.00003235
Iteration 104/1000 | Loss: 0.00003208
Iteration 105/1000 | Loss: 0.00003208
Iteration 106/1000 | Loss: 0.00003208
Iteration 107/1000 | Loss: 0.00003208
Iteration 108/1000 | Loss: 0.00003208
Iteration 109/1000 | Loss: 0.00003208
Iteration 110/1000 | Loss: 0.00003208
Iteration 111/1000 | Loss: 0.00003208
Iteration 112/1000 | Loss: 0.00003208
Iteration 113/1000 | Loss: 0.00003208
Iteration 114/1000 | Loss: 0.00003208
Iteration 115/1000 | Loss: 0.00003208
Iteration 116/1000 | Loss: 0.00003208
Iteration 117/1000 | Loss: 0.00003208
Iteration 118/1000 | Loss: 0.00003208
Iteration 119/1000 | Loss: 0.00003208
Iteration 120/1000 | Loss: 0.00003208
Iteration 121/1000 | Loss: 0.00003208
Iteration 122/1000 | Loss: 0.00003208
Iteration 123/1000 | Loss: 0.00003208
Iteration 124/1000 | Loss: 0.00003208
Iteration 125/1000 | Loss: 0.00003208
Iteration 126/1000 | Loss: 0.00003208
Iteration 127/1000 | Loss: 0.00003208
Iteration 128/1000 | Loss: 0.00003208
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 128. Stopping optimization.
Last 5 losses: [3.207529880455695e-05, 3.207529880455695e-05, 3.207529880455695e-05, 3.207529880455695e-05, 3.207529880455695e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.207529880455695e-05

Optimization complete. Final v2v error: 4.418846130371094 mm

Highest mean error: 22.380809783935547 mm for frame 151

Lowest mean error: 3.9603254795074463 mm for frame 95

Saving results

Total time: 134.34713768959045
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_30_nl_6559/0009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_nl_6559/0009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_nl_6559/0009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01099988
Iteration 2/25 | Loss: 0.00207603
Iteration 3/25 | Loss: 0.00145117
Iteration 4/25 | Loss: 0.00137352
Iteration 5/25 | Loss: 0.00146965
Iteration 6/25 | Loss: 0.00129921
Iteration 7/25 | Loss: 0.00120837
Iteration 8/25 | Loss: 0.00117232
Iteration 9/25 | Loss: 0.00116060
Iteration 10/25 | Loss: 0.00115866
Iteration 11/25 | Loss: 0.00114571
Iteration 12/25 | Loss: 0.00113111
Iteration 13/25 | Loss: 0.00113283
Iteration 14/25 | Loss: 0.00113703
Iteration 15/25 | Loss: 0.00112594
Iteration 16/25 | Loss: 0.00112170
Iteration 17/25 | Loss: 0.00112015
Iteration 18/25 | Loss: 0.00112389
Iteration 19/25 | Loss: 0.00112341
Iteration 20/25 | Loss: 0.00112181
Iteration 21/25 | Loss: 0.00111658
Iteration 22/25 | Loss: 0.00111702
Iteration 23/25 | Loss: 0.00112567
Iteration 24/25 | Loss: 0.00111513
Iteration 25/25 | Loss: 0.00112308

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.06083214
Iteration 2/25 | Loss: 0.00185238
Iteration 3/25 | Loss: 0.00185238
Iteration 4/25 | Loss: 0.00185238
Iteration 5/25 | Loss: 0.00185238
Iteration 6/25 | Loss: 0.00185238
Iteration 7/25 | Loss: 0.00185238
Iteration 8/25 | Loss: 0.00185238
Iteration 9/25 | Loss: 0.00185238
Iteration 10/25 | Loss: 0.00185238
Iteration 11/25 | Loss: 0.00185238
Iteration 12/25 | Loss: 0.00185238
Iteration 13/25 | Loss: 0.00185238
Iteration 14/25 | Loss: 0.00185238
Iteration 15/25 | Loss: 0.00185238
Iteration 16/25 | Loss: 0.00185238
Iteration 17/25 | Loss: 0.00185238
Iteration 18/25 | Loss: 0.00185238
Iteration 19/25 | Loss: 0.00185238
Iteration 20/25 | Loss: 0.00185238
Iteration 21/25 | Loss: 0.00185238
Iteration 22/25 | Loss: 0.00185238
Iteration 23/25 | Loss: 0.00185238
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0018523820908740163, 0.0018523820908740163, 0.0018523820908740163, 0.0018523820908740163, 0.0018523820908740163]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0018523820908740163

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00185238
Iteration 2/1000 | Loss: 0.00032497
Iteration 3/1000 | Loss: 0.00035460
Iteration 4/1000 | Loss: 0.00026721
Iteration 5/1000 | Loss: 0.00020461
Iteration 6/1000 | Loss: 0.00023510
Iteration 7/1000 | Loss: 0.00024950
Iteration 8/1000 | Loss: 0.00025761
Iteration 9/1000 | Loss: 0.00017235
Iteration 10/1000 | Loss: 0.00028827
Iteration 11/1000 | Loss: 0.00024851
Iteration 12/1000 | Loss: 0.00017139
Iteration 13/1000 | Loss: 0.00022888
Iteration 14/1000 | Loss: 0.00016532
Iteration 15/1000 | Loss: 0.00015385
Iteration 16/1000 | Loss: 0.00021331
Iteration 17/1000 | Loss: 0.00017120
Iteration 18/1000 | Loss: 0.00012871
Iteration 19/1000 | Loss: 0.00014845
Iteration 20/1000 | Loss: 0.00015004
Iteration 21/1000 | Loss: 0.00017815
Iteration 22/1000 | Loss: 0.00016390
Iteration 23/1000 | Loss: 0.00015694
Iteration 24/1000 | Loss: 0.00013210
Iteration 25/1000 | Loss: 0.00028486
Iteration 26/1000 | Loss: 0.00020822
Iteration 27/1000 | Loss: 0.00021260
Iteration 28/1000 | Loss: 0.00019013
Iteration 29/1000 | Loss: 0.00020216
Iteration 30/1000 | Loss: 0.00021666
Iteration 31/1000 | Loss: 0.00020537
Iteration 32/1000 | Loss: 0.00018450
Iteration 33/1000 | Loss: 0.00014510
Iteration 34/1000 | Loss: 0.00017595
Iteration 35/1000 | Loss: 0.00020174
Iteration 36/1000 | Loss: 0.00020819
Iteration 37/1000 | Loss: 0.00045324
Iteration 38/1000 | Loss: 0.00029394
Iteration 39/1000 | Loss: 0.00023127
Iteration 40/1000 | Loss: 0.00021968
Iteration 41/1000 | Loss: 0.00017937
Iteration 42/1000 | Loss: 0.00014017
Iteration 43/1000 | Loss: 0.00011319
Iteration 44/1000 | Loss: 0.00009578
Iteration 45/1000 | Loss: 0.00011681
Iteration 46/1000 | Loss: 0.00012196
Iteration 47/1000 | Loss: 0.00010471
Iteration 48/1000 | Loss: 0.00011022
Iteration 49/1000 | Loss: 0.00013592
Iteration 50/1000 | Loss: 0.00013562
Iteration 51/1000 | Loss: 0.00014321
Iteration 52/1000 | Loss: 0.00012504
Iteration 53/1000 | Loss: 0.00011634
Iteration 54/1000 | Loss: 0.00010134
Iteration 55/1000 | Loss: 0.00007652
Iteration 56/1000 | Loss: 0.00005048
Iteration 57/1000 | Loss: 0.00010238
Iteration 58/1000 | Loss: 0.00008742
Iteration 59/1000 | Loss: 0.00008496
Iteration 60/1000 | Loss: 0.00009226
Iteration 61/1000 | Loss: 0.00009368
Iteration 62/1000 | Loss: 0.00007590
Iteration 63/1000 | Loss: 0.00007465
Iteration 64/1000 | Loss: 0.00005706
Iteration 65/1000 | Loss: 0.00007945
Iteration 66/1000 | Loss: 0.00006455
Iteration 67/1000 | Loss: 0.00008199
Iteration 68/1000 | Loss: 0.00006380
Iteration 69/1000 | Loss: 0.00005840
Iteration 70/1000 | Loss: 0.00006859
Iteration 71/1000 | Loss: 0.00007496
Iteration 72/1000 | Loss: 0.00008236
Iteration 73/1000 | Loss: 0.00008133
Iteration 74/1000 | Loss: 0.00006657
Iteration 75/1000 | Loss: 0.00005565
Iteration 76/1000 | Loss: 0.00005615
Iteration 77/1000 | Loss: 0.00006747
Iteration 78/1000 | Loss: 0.00007125
Iteration 79/1000 | Loss: 0.00005560
Iteration 80/1000 | Loss: 0.00007745
Iteration 81/1000 | Loss: 0.00005625
Iteration 82/1000 | Loss: 0.00005386
Iteration 83/1000 | Loss: 0.00006340
Iteration 84/1000 | Loss: 0.00005035
Iteration 85/1000 | Loss: 0.00007410
Iteration 86/1000 | Loss: 0.00004830
Iteration 87/1000 | Loss: 0.00005781
Iteration 88/1000 | Loss: 0.00004046
Iteration 89/1000 | Loss: 0.00003584
Iteration 90/1000 | Loss: 0.00003890
Iteration 91/1000 | Loss: 0.00003650
Iteration 92/1000 | Loss: 0.00003672
Iteration 93/1000 | Loss: 0.00005265
Iteration 94/1000 | Loss: 0.00008844
Iteration 95/1000 | Loss: 0.00007869
Iteration 96/1000 | Loss: 0.00008431
Iteration 97/1000 | Loss: 0.00007353
Iteration 98/1000 | Loss: 0.00007335
Iteration 99/1000 | Loss: 0.00007596
Iteration 100/1000 | Loss: 0.00007506
Iteration 101/1000 | Loss: 0.00008128
Iteration 102/1000 | Loss: 0.00007108
Iteration 103/1000 | Loss: 0.00007252
Iteration 104/1000 | Loss: 0.00005300
Iteration 105/1000 | Loss: 0.00008143
Iteration 106/1000 | Loss: 0.00005543
Iteration 107/1000 | Loss: 0.00004761
Iteration 108/1000 | Loss: 0.00005363
Iteration 109/1000 | Loss: 0.00003691
Iteration 110/1000 | Loss: 0.00003908
Iteration 111/1000 | Loss: 0.00003715
Iteration 112/1000 | Loss: 0.00003839
Iteration 113/1000 | Loss: 0.00004537
Iteration 114/1000 | Loss: 0.00004152
Iteration 115/1000 | Loss: 0.00004679
Iteration 116/1000 | Loss: 0.00004492
Iteration 117/1000 | Loss: 0.00004631
Iteration 118/1000 | Loss: 0.00003920
Iteration 119/1000 | Loss: 0.00004180
Iteration 120/1000 | Loss: 0.00003771
Iteration 121/1000 | Loss: 0.00004523
Iteration 122/1000 | Loss: 0.00003887
Iteration 123/1000 | Loss: 0.00003415
Iteration 124/1000 | Loss: 0.00003169
Iteration 125/1000 | Loss: 0.00003017
Iteration 126/1000 | Loss: 0.00002951
Iteration 127/1000 | Loss: 0.00002889
Iteration 128/1000 | Loss: 0.00002864
Iteration 129/1000 | Loss: 0.00002854
Iteration 130/1000 | Loss: 0.00002852
Iteration 131/1000 | Loss: 0.00002851
Iteration 132/1000 | Loss: 0.00002851
Iteration 133/1000 | Loss: 0.00002844
Iteration 134/1000 | Loss: 0.00002838
Iteration 135/1000 | Loss: 0.00002830
Iteration 136/1000 | Loss: 0.00002829
Iteration 137/1000 | Loss: 0.00002829
Iteration 138/1000 | Loss: 0.00002827
Iteration 139/1000 | Loss: 0.00002827
Iteration 140/1000 | Loss: 0.00002825
Iteration 141/1000 | Loss: 0.00002825
Iteration 142/1000 | Loss: 0.00002824
Iteration 143/1000 | Loss: 0.00002824
Iteration 144/1000 | Loss: 0.00002823
Iteration 145/1000 | Loss: 0.00002823
Iteration 146/1000 | Loss: 0.00002821
Iteration 147/1000 | Loss: 0.00002820
Iteration 148/1000 | Loss: 0.00002820
Iteration 149/1000 | Loss: 0.00002820
Iteration 150/1000 | Loss: 0.00002820
Iteration 151/1000 | Loss: 0.00002820
Iteration 152/1000 | Loss: 0.00002820
Iteration 153/1000 | Loss: 0.00002819
Iteration 154/1000 | Loss: 0.00002819
Iteration 155/1000 | Loss: 0.00002818
Iteration 156/1000 | Loss: 0.00002818
Iteration 157/1000 | Loss: 0.00002818
Iteration 158/1000 | Loss: 0.00002818
Iteration 159/1000 | Loss: 0.00002818
Iteration 160/1000 | Loss: 0.00002818
Iteration 161/1000 | Loss: 0.00002818
Iteration 162/1000 | Loss: 0.00002818
Iteration 163/1000 | Loss: 0.00002818
Iteration 164/1000 | Loss: 0.00002817
Iteration 165/1000 | Loss: 0.00002817
Iteration 166/1000 | Loss: 0.00002817
Iteration 167/1000 | Loss: 0.00002817
Iteration 168/1000 | Loss: 0.00002817
Iteration 169/1000 | Loss: 0.00002817
Iteration 170/1000 | Loss: 0.00002816
Iteration 171/1000 | Loss: 0.00002816
Iteration 172/1000 | Loss: 0.00002816
Iteration 173/1000 | Loss: 0.00002816
Iteration 174/1000 | Loss: 0.00002815
Iteration 175/1000 | Loss: 0.00002815
Iteration 176/1000 | Loss: 0.00002815
Iteration 177/1000 | Loss: 0.00002815
Iteration 178/1000 | Loss: 0.00002815
Iteration 179/1000 | Loss: 0.00002814
Iteration 180/1000 | Loss: 0.00002814
Iteration 181/1000 | Loss: 0.00002814
Iteration 182/1000 | Loss: 0.00002813
Iteration 183/1000 | Loss: 0.00002813
Iteration 184/1000 | Loss: 0.00002813
Iteration 185/1000 | Loss: 0.00002813
Iteration 186/1000 | Loss: 0.00002813
Iteration 187/1000 | Loss: 0.00002813
Iteration 188/1000 | Loss: 0.00002813
Iteration 189/1000 | Loss: 0.00002813
Iteration 190/1000 | Loss: 0.00002812
Iteration 191/1000 | Loss: 0.00002812
Iteration 192/1000 | Loss: 0.00002812
Iteration 193/1000 | Loss: 0.00002812
Iteration 194/1000 | Loss: 0.00002812
Iteration 195/1000 | Loss: 0.00002812
Iteration 196/1000 | Loss: 0.00002812
Iteration 197/1000 | Loss: 0.00002812
Iteration 198/1000 | Loss: 0.00002812
Iteration 199/1000 | Loss: 0.00002812
Iteration 200/1000 | Loss: 0.00002812
Iteration 201/1000 | Loss: 0.00002812
Iteration 202/1000 | Loss: 0.00002812
Iteration 203/1000 | Loss: 0.00002812
Iteration 204/1000 | Loss: 0.00002812
Iteration 205/1000 | Loss: 0.00002812
Iteration 206/1000 | Loss: 0.00002812
Iteration 207/1000 | Loss: 0.00002812
Iteration 208/1000 | Loss: 0.00002812
Iteration 209/1000 | Loss: 0.00002812
Iteration 210/1000 | Loss: 0.00002812
Iteration 211/1000 | Loss: 0.00002812
Iteration 212/1000 | Loss: 0.00002812
Iteration 213/1000 | Loss: 0.00002812
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 213. Stopping optimization.
Last 5 losses: [2.8115138775319792e-05, 2.8115138775319792e-05, 2.8115138775319792e-05, 2.8115138775319792e-05, 2.8115138775319792e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.8115138775319792e-05

Optimization complete. Final v2v error: 4.463105201721191 mm

Highest mean error: 5.0743303298950195 mm for frame 194

Lowest mean error: 3.9754393100738525 mm for frame 1

Saving results

Total time: 267.00246691703796
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_32_us_2373/0008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_us_2373/0008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_us_2373/0008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00893120
Iteration 2/25 | Loss: 0.00100192
Iteration 3/25 | Loss: 0.00088164
Iteration 4/25 | Loss: 0.00085421
Iteration 5/25 | Loss: 0.00084569
Iteration 6/25 | Loss: 0.00084430
Iteration 7/25 | Loss: 0.00084424
Iteration 8/25 | Loss: 0.00084424
Iteration 9/25 | Loss: 0.00084424
Iteration 10/25 | Loss: 0.00084424
Iteration 11/25 | Loss: 0.00084424
Iteration 12/25 | Loss: 0.00084424
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0008442361722700298, 0.0008442361722700298, 0.0008442361722700298, 0.0008442361722700298, 0.0008442361722700298]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008442361722700298

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38131201
Iteration 2/25 | Loss: 0.00037563
Iteration 3/25 | Loss: 0.00037557
Iteration 4/25 | Loss: 0.00037557
Iteration 5/25 | Loss: 0.00037557
Iteration 6/25 | Loss: 0.00037557
Iteration 7/25 | Loss: 0.00037557
Iteration 8/25 | Loss: 0.00037557
Iteration 9/25 | Loss: 0.00037557
Iteration 10/25 | Loss: 0.00037557
Iteration 11/25 | Loss: 0.00037557
Iteration 12/25 | Loss: 0.00037557
Iteration 13/25 | Loss: 0.00037557
Iteration 14/25 | Loss: 0.00037557
Iteration 15/25 | Loss: 0.00037557
Iteration 16/25 | Loss: 0.00037557
Iteration 17/25 | Loss: 0.00037557
Iteration 18/25 | Loss: 0.00037557
Iteration 19/25 | Loss: 0.00037557
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.000375566742150113, 0.000375566742150113, 0.000375566742150113, 0.000375566742150113, 0.000375566742150113]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000375566742150113

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00037557
Iteration 2/1000 | Loss: 0.00004747
Iteration 3/1000 | Loss: 0.00003005
Iteration 4/1000 | Loss: 0.00002346
Iteration 5/1000 | Loss: 0.00002086
Iteration 6/1000 | Loss: 0.00001961
Iteration 7/1000 | Loss: 0.00001879
Iteration 8/1000 | Loss: 0.00001832
Iteration 9/1000 | Loss: 0.00001794
Iteration 10/1000 | Loss: 0.00001770
Iteration 11/1000 | Loss: 0.00001753
Iteration 12/1000 | Loss: 0.00001752
Iteration 13/1000 | Loss: 0.00001742
Iteration 14/1000 | Loss: 0.00001726
Iteration 15/1000 | Loss: 0.00001726
Iteration 16/1000 | Loss: 0.00001725
Iteration 17/1000 | Loss: 0.00001725
Iteration 18/1000 | Loss: 0.00001725
Iteration 19/1000 | Loss: 0.00001725
Iteration 20/1000 | Loss: 0.00001725
Iteration 21/1000 | Loss: 0.00001725
Iteration 22/1000 | Loss: 0.00001725
Iteration 23/1000 | Loss: 0.00001725
Iteration 24/1000 | Loss: 0.00001725
Iteration 25/1000 | Loss: 0.00001725
Iteration 26/1000 | Loss: 0.00001724
Iteration 27/1000 | Loss: 0.00001724
Iteration 28/1000 | Loss: 0.00001724
Iteration 29/1000 | Loss: 0.00001724
Iteration 30/1000 | Loss: 0.00001724
Iteration 31/1000 | Loss: 0.00001724
Iteration 32/1000 | Loss: 0.00001724
Iteration 33/1000 | Loss: 0.00001724
Iteration 34/1000 | Loss: 0.00001723
Iteration 35/1000 | Loss: 0.00001723
Iteration 36/1000 | Loss: 0.00001723
Iteration 37/1000 | Loss: 0.00001722
Iteration 38/1000 | Loss: 0.00001722
Iteration 39/1000 | Loss: 0.00001722
Iteration 40/1000 | Loss: 0.00001721
Iteration 41/1000 | Loss: 0.00001721
Iteration 42/1000 | Loss: 0.00001721
Iteration 43/1000 | Loss: 0.00001720
Iteration 44/1000 | Loss: 0.00001719
Iteration 45/1000 | Loss: 0.00001718
Iteration 46/1000 | Loss: 0.00001718
Iteration 47/1000 | Loss: 0.00001717
Iteration 48/1000 | Loss: 0.00001717
Iteration 49/1000 | Loss: 0.00001717
Iteration 50/1000 | Loss: 0.00001716
Iteration 51/1000 | Loss: 0.00001716
Iteration 52/1000 | Loss: 0.00001715
Iteration 53/1000 | Loss: 0.00001715
Iteration 54/1000 | Loss: 0.00001715
Iteration 55/1000 | Loss: 0.00001715
Iteration 56/1000 | Loss: 0.00001715
Iteration 57/1000 | Loss: 0.00001715
Iteration 58/1000 | Loss: 0.00001715
Iteration 59/1000 | Loss: 0.00001714
Iteration 60/1000 | Loss: 0.00001714
Iteration 61/1000 | Loss: 0.00001714
Iteration 62/1000 | Loss: 0.00001714
Iteration 63/1000 | Loss: 0.00001714
Iteration 64/1000 | Loss: 0.00001714
Iteration 65/1000 | Loss: 0.00001714
Iteration 66/1000 | Loss: 0.00001714
Iteration 67/1000 | Loss: 0.00001713
Iteration 68/1000 | Loss: 0.00001713
Iteration 69/1000 | Loss: 0.00001713
Iteration 70/1000 | Loss: 0.00001713
Iteration 71/1000 | Loss: 0.00001713
Iteration 72/1000 | Loss: 0.00001713
Iteration 73/1000 | Loss: 0.00001713
Iteration 74/1000 | Loss: 0.00001713
Iteration 75/1000 | Loss: 0.00001712
Iteration 76/1000 | Loss: 0.00001712
Iteration 77/1000 | Loss: 0.00001712
Iteration 78/1000 | Loss: 0.00001712
Iteration 79/1000 | Loss: 0.00001712
Iteration 80/1000 | Loss: 0.00001711
Iteration 81/1000 | Loss: 0.00001711
Iteration 82/1000 | Loss: 0.00001711
Iteration 83/1000 | Loss: 0.00001711
Iteration 84/1000 | Loss: 0.00001711
Iteration 85/1000 | Loss: 0.00001711
Iteration 86/1000 | Loss: 0.00001711
Iteration 87/1000 | Loss: 0.00001711
Iteration 88/1000 | Loss: 0.00001710
Iteration 89/1000 | Loss: 0.00001710
Iteration 90/1000 | Loss: 0.00001710
Iteration 91/1000 | Loss: 0.00001710
Iteration 92/1000 | Loss: 0.00001710
Iteration 93/1000 | Loss: 0.00001710
Iteration 94/1000 | Loss: 0.00001710
Iteration 95/1000 | Loss: 0.00001710
Iteration 96/1000 | Loss: 0.00001710
Iteration 97/1000 | Loss: 0.00001710
Iteration 98/1000 | Loss: 0.00001710
Iteration 99/1000 | Loss: 0.00001710
Iteration 100/1000 | Loss: 0.00001710
Iteration 101/1000 | Loss: 0.00001710
Iteration 102/1000 | Loss: 0.00001710
Iteration 103/1000 | Loss: 0.00001710
Iteration 104/1000 | Loss: 0.00001710
Iteration 105/1000 | Loss: 0.00001710
Iteration 106/1000 | Loss: 0.00001710
Iteration 107/1000 | Loss: 0.00001710
Iteration 108/1000 | Loss: 0.00001710
Iteration 109/1000 | Loss: 0.00001710
Iteration 110/1000 | Loss: 0.00001710
Iteration 111/1000 | Loss: 0.00001710
Iteration 112/1000 | Loss: 0.00001710
Iteration 113/1000 | Loss: 0.00001710
Iteration 114/1000 | Loss: 0.00001710
Iteration 115/1000 | Loss: 0.00001710
Iteration 116/1000 | Loss: 0.00001710
Iteration 117/1000 | Loss: 0.00001710
Iteration 118/1000 | Loss: 0.00001710
Iteration 119/1000 | Loss: 0.00001710
Iteration 120/1000 | Loss: 0.00001710
Iteration 121/1000 | Loss: 0.00001710
Iteration 122/1000 | Loss: 0.00001710
Iteration 123/1000 | Loss: 0.00001710
Iteration 124/1000 | Loss: 0.00001710
Iteration 125/1000 | Loss: 0.00001710
Iteration 126/1000 | Loss: 0.00001710
Iteration 127/1000 | Loss: 0.00001710
Iteration 128/1000 | Loss: 0.00001710
Iteration 129/1000 | Loss: 0.00001710
Iteration 130/1000 | Loss: 0.00001710
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 130. Stopping optimization.
Last 5 losses: [1.70981147675775e-05, 1.70981147675775e-05, 1.70981147675775e-05, 1.70981147675775e-05, 1.70981147675775e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.70981147675775e-05

Optimization complete. Final v2v error: 3.5608716011047363 mm

Highest mean error: 4.13133430480957 mm for frame 125

Lowest mean error: 3.142904043197632 mm for frame 79

Saving results

Total time: 32.40239953994751
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_32_us_2373/0014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_us_2373/0014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_us_2373/0014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00432607
Iteration 2/25 | Loss: 0.00094432
Iteration 3/25 | Loss: 0.00086062
Iteration 4/25 | Loss: 0.00083369
Iteration 5/25 | Loss: 0.00082313
Iteration 6/25 | Loss: 0.00082104
Iteration 7/25 | Loss: 0.00082067
Iteration 8/25 | Loss: 0.00082067
Iteration 9/25 | Loss: 0.00082067
Iteration 10/25 | Loss: 0.00082067
Iteration 11/25 | Loss: 0.00082067
Iteration 12/25 | Loss: 0.00082067
Iteration 13/25 | Loss: 0.00082067
Iteration 14/25 | Loss: 0.00082067
Iteration 15/25 | Loss: 0.00082067
Iteration 16/25 | Loss: 0.00082067
Iteration 17/25 | Loss: 0.00082067
Iteration 18/25 | Loss: 0.00082067
Iteration 19/25 | Loss: 0.00082067
Iteration 20/25 | Loss: 0.00082067
Iteration 21/25 | Loss: 0.00082067
Iteration 22/25 | Loss: 0.00082067
Iteration 23/25 | Loss: 0.00082067
Iteration 24/25 | Loss: 0.00082067
Iteration 25/25 | Loss: 0.00082067
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0008206714992411435, 0.0008206714992411435, 0.0008206714992411435, 0.0008206714992411435, 0.0008206714992411435]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008206714992411435

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.54561353
Iteration 2/25 | Loss: 0.00032748
Iteration 3/25 | Loss: 0.00032748
Iteration 4/25 | Loss: 0.00032748
Iteration 5/25 | Loss: 0.00032747
Iteration 6/25 | Loss: 0.00032747
Iteration 7/25 | Loss: 0.00032747
Iteration 8/25 | Loss: 0.00032747
Iteration 9/25 | Loss: 0.00032747
Iteration 10/25 | Loss: 0.00032747
Iteration 11/25 | Loss: 0.00032747
Iteration 12/25 | Loss: 0.00032747
Iteration 13/25 | Loss: 0.00032747
Iteration 14/25 | Loss: 0.00032747
Iteration 15/25 | Loss: 0.00032747
Iteration 16/25 | Loss: 0.00032747
Iteration 17/25 | Loss: 0.00032747
Iteration 18/25 | Loss: 0.00032747
Iteration 19/25 | Loss: 0.00032747
Iteration 20/25 | Loss: 0.00032747
Iteration 21/25 | Loss: 0.00032747
Iteration 22/25 | Loss: 0.00032747
Iteration 23/25 | Loss: 0.00032747
Iteration 24/25 | Loss: 0.00032747
Iteration 25/25 | Loss: 0.00032747
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.00032747277873568237, 0.00032747277873568237, 0.00032747277873568237, 0.00032747277873568237, 0.00032747277873568237]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00032747277873568237

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00032747
Iteration 2/1000 | Loss: 0.00004734
Iteration 3/1000 | Loss: 0.00003233
Iteration 4/1000 | Loss: 0.00002936
Iteration 5/1000 | Loss: 0.00002724
Iteration 6/1000 | Loss: 0.00002628
Iteration 7/1000 | Loss: 0.00002555
Iteration 8/1000 | Loss: 0.00002520
Iteration 9/1000 | Loss: 0.00002486
Iteration 10/1000 | Loss: 0.00002464
Iteration 11/1000 | Loss: 0.00002461
Iteration 12/1000 | Loss: 0.00002460
Iteration 13/1000 | Loss: 0.00002458
Iteration 14/1000 | Loss: 0.00002458
Iteration 15/1000 | Loss: 0.00002458
Iteration 16/1000 | Loss: 0.00002458
Iteration 17/1000 | Loss: 0.00002455
Iteration 18/1000 | Loss: 0.00002455
Iteration 19/1000 | Loss: 0.00002455
Iteration 20/1000 | Loss: 0.00002453
Iteration 21/1000 | Loss: 0.00002453
Iteration 22/1000 | Loss: 0.00002453
Iteration 23/1000 | Loss: 0.00002453
Iteration 24/1000 | Loss: 0.00002453
Iteration 25/1000 | Loss: 0.00002453
Iteration 26/1000 | Loss: 0.00002453
Iteration 27/1000 | Loss: 0.00002453
Iteration 28/1000 | Loss: 0.00002453
Iteration 29/1000 | Loss: 0.00002453
Iteration 30/1000 | Loss: 0.00002453
Iteration 31/1000 | Loss: 0.00002453
Iteration 32/1000 | Loss: 0.00002453
Iteration 33/1000 | Loss: 0.00002453
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 33. Stopping optimization.
Last 5 losses: [2.453003980917856e-05, 2.453003980917856e-05, 2.453003980917856e-05, 2.453003980917856e-05, 2.453003980917856e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.453003980917856e-05

Optimization complete. Final v2v error: 4.1541266441345215 mm

Highest mean error: 4.5006818771362305 mm for frame 80

Lowest mean error: 3.746297836303711 mm for frame 155

Saving results

Total time: 26.174753665924072
