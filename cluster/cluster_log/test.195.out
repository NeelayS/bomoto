Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=195, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 10920-10975
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_47_us_1850/0011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_47_us_1850/0011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_47_us_1850/0011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00890071
Iteration 2/25 | Loss: 0.00180445
Iteration 3/25 | Loss: 0.00159532
Iteration 4/25 | Loss: 0.00157580
Iteration 5/25 | Loss: 0.00157029
Iteration 6/25 | Loss: 0.00156800
Iteration 7/25 | Loss: 0.00156796
Iteration 8/25 | Loss: 0.00156796
Iteration 9/25 | Loss: 0.00156796
Iteration 10/25 | Loss: 0.00156796
Iteration 11/25 | Loss: 0.00156796
Iteration 12/25 | Loss: 0.00156796
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0015679558273404837, 0.0015679558273404837, 0.0015679558273404837, 0.0015679558273404837, 0.0015679558273404837]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015679558273404837

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33897281
Iteration 2/25 | Loss: 0.00137542
Iteration 3/25 | Loss: 0.00137542
Iteration 4/25 | Loss: 0.00137542
Iteration 5/25 | Loss: 0.00137542
Iteration 6/25 | Loss: 0.00137542
Iteration 7/25 | Loss: 0.00137542
Iteration 8/25 | Loss: 0.00137542
Iteration 9/25 | Loss: 0.00137542
Iteration 10/25 | Loss: 0.00137542
Iteration 11/25 | Loss: 0.00137542
Iteration 12/25 | Loss: 0.00137542
Iteration 13/25 | Loss: 0.00137542
Iteration 14/25 | Loss: 0.00137542
Iteration 15/25 | Loss: 0.00137542
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0013754190877079964, 0.0013754190877079964, 0.0013754190877079964, 0.0013754190877079964, 0.0013754190877079964]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013754190877079964

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00137542
Iteration 2/1000 | Loss: 0.00007762
Iteration 3/1000 | Loss: 0.00005322
Iteration 4/1000 | Loss: 0.00004385
Iteration 5/1000 | Loss: 0.00003739
Iteration 6/1000 | Loss: 0.00003506
Iteration 7/1000 | Loss: 0.00003341
Iteration 8/1000 | Loss: 0.00003243
Iteration 9/1000 | Loss: 0.00003166
Iteration 10/1000 | Loss: 0.00003103
Iteration 11/1000 | Loss: 0.00003054
Iteration 12/1000 | Loss: 0.00003019
Iteration 13/1000 | Loss: 0.00002997
Iteration 14/1000 | Loss: 0.00002979
Iteration 15/1000 | Loss: 0.00002978
Iteration 16/1000 | Loss: 0.00002977
Iteration 17/1000 | Loss: 0.00002977
Iteration 18/1000 | Loss: 0.00002967
Iteration 19/1000 | Loss: 0.00002967
Iteration 20/1000 | Loss: 0.00002967
Iteration 21/1000 | Loss: 0.00002967
Iteration 22/1000 | Loss: 0.00002967
Iteration 23/1000 | Loss: 0.00002967
Iteration 24/1000 | Loss: 0.00002967
Iteration 25/1000 | Loss: 0.00002966
Iteration 26/1000 | Loss: 0.00002964
Iteration 27/1000 | Loss: 0.00002963
Iteration 28/1000 | Loss: 0.00002963
Iteration 29/1000 | Loss: 0.00002963
Iteration 30/1000 | Loss: 0.00002963
Iteration 31/1000 | Loss: 0.00002963
Iteration 32/1000 | Loss: 0.00002962
Iteration 33/1000 | Loss: 0.00002962
Iteration 34/1000 | Loss: 0.00002962
Iteration 35/1000 | Loss: 0.00002961
Iteration 36/1000 | Loss: 0.00002961
Iteration 37/1000 | Loss: 0.00002961
Iteration 38/1000 | Loss: 0.00002961
Iteration 39/1000 | Loss: 0.00002961
Iteration 40/1000 | Loss: 0.00002961
Iteration 41/1000 | Loss: 0.00002961
Iteration 42/1000 | Loss: 0.00002961
Iteration 43/1000 | Loss: 0.00002960
Iteration 44/1000 | Loss: 0.00002960
Iteration 45/1000 | Loss: 0.00002960
Iteration 46/1000 | Loss: 0.00002960
Iteration 47/1000 | Loss: 0.00002960
Iteration 48/1000 | Loss: 0.00002960
Iteration 49/1000 | Loss: 0.00002960
Iteration 50/1000 | Loss: 0.00002959
Iteration 51/1000 | Loss: 0.00002959
Iteration 52/1000 | Loss: 0.00002959
Iteration 53/1000 | Loss: 0.00002959
Iteration 54/1000 | Loss: 0.00002959
Iteration 55/1000 | Loss: 0.00002959
Iteration 56/1000 | Loss: 0.00002958
Iteration 57/1000 | Loss: 0.00002958
Iteration 58/1000 | Loss: 0.00002958
Iteration 59/1000 | Loss: 0.00002958
Iteration 60/1000 | Loss: 0.00002958
Iteration 61/1000 | Loss: 0.00002958
Iteration 62/1000 | Loss: 0.00002958
Iteration 63/1000 | Loss: 0.00002957
Iteration 64/1000 | Loss: 0.00002957
Iteration 65/1000 | Loss: 0.00002957
Iteration 66/1000 | Loss: 0.00002957
Iteration 67/1000 | Loss: 0.00002957
Iteration 68/1000 | Loss: 0.00002957
Iteration 69/1000 | Loss: 0.00002956
Iteration 70/1000 | Loss: 0.00002956
Iteration 71/1000 | Loss: 0.00002956
Iteration 72/1000 | Loss: 0.00002956
Iteration 73/1000 | Loss: 0.00002956
Iteration 74/1000 | Loss: 0.00002955
Iteration 75/1000 | Loss: 0.00002955
Iteration 76/1000 | Loss: 0.00002955
Iteration 77/1000 | Loss: 0.00002955
Iteration 78/1000 | Loss: 0.00002955
Iteration 79/1000 | Loss: 0.00002955
Iteration 80/1000 | Loss: 0.00002955
Iteration 81/1000 | Loss: 0.00002954
Iteration 82/1000 | Loss: 0.00002954
Iteration 83/1000 | Loss: 0.00002954
Iteration 84/1000 | Loss: 0.00002954
Iteration 85/1000 | Loss: 0.00002953
Iteration 86/1000 | Loss: 0.00002953
Iteration 87/1000 | Loss: 0.00002953
Iteration 88/1000 | Loss: 0.00002953
Iteration 89/1000 | Loss: 0.00002953
Iteration 90/1000 | Loss: 0.00002953
Iteration 91/1000 | Loss: 0.00002953
Iteration 92/1000 | Loss: 0.00002953
Iteration 93/1000 | Loss: 0.00002953
Iteration 94/1000 | Loss: 0.00002953
Iteration 95/1000 | Loss: 0.00002953
Iteration 96/1000 | Loss: 0.00002953
Iteration 97/1000 | Loss: 0.00002953
Iteration 98/1000 | Loss: 0.00002953
Iteration 99/1000 | Loss: 0.00002953
Iteration 100/1000 | Loss: 0.00002953
Iteration 101/1000 | Loss: 0.00002953
Iteration 102/1000 | Loss: 0.00002953
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 102. Stopping optimization.
Last 5 losses: [2.9532675398513675e-05, 2.9532675398513675e-05, 2.9532675398513675e-05, 2.9532675398513675e-05, 2.9532675398513675e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.9532675398513675e-05

Optimization complete. Final v2v error: 4.805199146270752 mm

Highest mean error: 5.074557781219482 mm for frame 163

Lowest mean error: 4.549483299255371 mm for frame 187

Saving results

Total time: 41.664966106414795
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_47_us_1850/0020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_47_us_1850/0020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_47_us_1850/0020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01156984
Iteration 2/25 | Loss: 0.00307545
Iteration 3/25 | Loss: 0.00231173
Iteration 4/25 | Loss: 0.00204162
Iteration 5/25 | Loss: 0.00193767
Iteration 6/25 | Loss: 0.00179786
Iteration 7/25 | Loss: 0.00183853
Iteration 8/25 | Loss: 0.00178471
Iteration 9/25 | Loss: 0.00174578
Iteration 10/25 | Loss: 0.00180324
Iteration 11/25 | Loss: 0.00173306
Iteration 12/25 | Loss: 0.00172467
Iteration 13/25 | Loss: 0.00171749
Iteration 14/25 | Loss: 0.00171205
Iteration 15/25 | Loss: 0.00170985
Iteration 16/25 | Loss: 0.00170914
Iteration 17/25 | Loss: 0.00170292
Iteration 18/25 | Loss: 0.00169578
Iteration 19/25 | Loss: 0.00170319
Iteration 20/25 | Loss: 0.00170419
Iteration 21/25 | Loss: 0.00169933
Iteration 22/25 | Loss: 0.00169927
Iteration 23/25 | Loss: 0.00170361
Iteration 24/25 | Loss: 0.00170108
Iteration 25/25 | Loss: 0.00169421

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37377906
Iteration 2/25 | Loss: 0.00300773
Iteration 3/25 | Loss: 0.00223873
Iteration 4/25 | Loss: 0.00223873
Iteration 5/25 | Loss: 0.00223873
Iteration 6/25 | Loss: 0.00223873
Iteration 7/25 | Loss: 0.00223873
Iteration 8/25 | Loss: 0.00223873
Iteration 9/25 | Loss: 0.00223873
Iteration 10/25 | Loss: 0.00223873
Iteration 11/25 | Loss: 0.00223873
Iteration 12/25 | Loss: 0.00223873
Iteration 13/25 | Loss: 0.00223873
Iteration 14/25 | Loss: 0.00223873
Iteration 15/25 | Loss: 0.00223873
Iteration 16/25 | Loss: 0.00223873
Iteration 17/25 | Loss: 0.00223873
Iteration 18/25 | Loss: 0.00223873
Iteration 19/25 | Loss: 0.00223873
Iteration 20/25 | Loss: 0.00223873
Iteration 21/25 | Loss: 0.00223873
Iteration 22/25 | Loss: 0.00223873
Iteration 23/25 | Loss: 0.00223873
Iteration 24/25 | Loss: 0.00223873
Iteration 25/25 | Loss: 0.00223873

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00223873
Iteration 2/1000 | Loss: 0.00062858
Iteration 3/1000 | Loss: 0.00029449
Iteration 4/1000 | Loss: 0.00075783
Iteration 5/1000 | Loss: 0.00015538
Iteration 6/1000 | Loss: 0.00091272
Iteration 7/1000 | Loss: 0.00036682
Iteration 8/1000 | Loss: 0.00012699
Iteration 9/1000 | Loss: 0.00011797
Iteration 10/1000 | Loss: 0.00018830
Iteration 11/1000 | Loss: 0.00639621
Iteration 12/1000 | Loss: 0.00173521
Iteration 13/1000 | Loss: 0.00262518
Iteration 14/1000 | Loss: 0.00046987
Iteration 15/1000 | Loss: 0.00038844
Iteration 16/1000 | Loss: 0.00055018
Iteration 17/1000 | Loss: 0.00011349
Iteration 18/1000 | Loss: 0.00065389
Iteration 19/1000 | Loss: 0.00050323
Iteration 20/1000 | Loss: 0.00018917
Iteration 21/1000 | Loss: 0.00019497
Iteration 22/1000 | Loss: 0.00043894
Iteration 23/1000 | Loss: 0.00009571
Iteration 24/1000 | Loss: 0.00053830
Iteration 25/1000 | Loss: 0.00008951
Iteration 26/1000 | Loss: 0.00020254
Iteration 27/1000 | Loss: 0.00005115
Iteration 28/1000 | Loss: 0.00024860
Iteration 29/1000 | Loss: 0.00004868
Iteration 30/1000 | Loss: 0.00004690
Iteration 31/1000 | Loss: 0.00015592
Iteration 32/1000 | Loss: 0.00005175
Iteration 33/1000 | Loss: 0.00004468
Iteration 34/1000 | Loss: 0.00004407
Iteration 35/1000 | Loss: 0.00004375
Iteration 36/1000 | Loss: 0.00013941
Iteration 37/1000 | Loss: 0.00004344
Iteration 38/1000 | Loss: 0.00004307
Iteration 39/1000 | Loss: 0.00004301
Iteration 40/1000 | Loss: 0.00004300
Iteration 41/1000 | Loss: 0.00004292
Iteration 42/1000 | Loss: 0.00004281
Iteration 43/1000 | Loss: 0.00004274
Iteration 44/1000 | Loss: 0.00004274
Iteration 45/1000 | Loss: 0.00004272
Iteration 46/1000 | Loss: 0.00004272
Iteration 47/1000 | Loss: 0.00004271
Iteration 48/1000 | Loss: 0.00059291
Iteration 49/1000 | Loss: 0.00004904
Iteration 50/1000 | Loss: 0.00024889
Iteration 51/1000 | Loss: 0.00004190
Iteration 52/1000 | Loss: 0.00004051
Iteration 53/1000 | Loss: 0.00016036
Iteration 54/1000 | Loss: 0.00003932
Iteration 55/1000 | Loss: 0.00012596
Iteration 56/1000 | Loss: 0.00003875
Iteration 57/1000 | Loss: 0.00015527
Iteration 58/1000 | Loss: 0.00003895
Iteration 59/1000 | Loss: 0.00003828
Iteration 60/1000 | Loss: 0.00003823
Iteration 61/1000 | Loss: 0.00003818
Iteration 62/1000 | Loss: 0.00003818
Iteration 63/1000 | Loss: 0.00003815
Iteration 64/1000 | Loss: 0.00003814
Iteration 65/1000 | Loss: 0.00003812
Iteration 66/1000 | Loss: 0.00003811
Iteration 67/1000 | Loss: 0.00003809
Iteration 68/1000 | Loss: 0.00003807
Iteration 69/1000 | Loss: 0.00003807
Iteration 70/1000 | Loss: 0.00003806
Iteration 71/1000 | Loss: 0.00003806
Iteration 72/1000 | Loss: 0.00003803
Iteration 73/1000 | Loss: 0.00003803
Iteration 74/1000 | Loss: 0.00003803
Iteration 75/1000 | Loss: 0.00003802
Iteration 76/1000 | Loss: 0.00003802
Iteration 77/1000 | Loss: 0.00003802
Iteration 78/1000 | Loss: 0.00003800
Iteration 79/1000 | Loss: 0.00003800
Iteration 80/1000 | Loss: 0.00003799
Iteration 81/1000 | Loss: 0.00003799
Iteration 82/1000 | Loss: 0.00003799
Iteration 83/1000 | Loss: 0.00003799
Iteration 84/1000 | Loss: 0.00003799
Iteration 85/1000 | Loss: 0.00003798
Iteration 86/1000 | Loss: 0.00003798
Iteration 87/1000 | Loss: 0.00003798
Iteration 88/1000 | Loss: 0.00003798
Iteration 89/1000 | Loss: 0.00003798
Iteration 90/1000 | Loss: 0.00003798
Iteration 91/1000 | Loss: 0.00003798
Iteration 92/1000 | Loss: 0.00003797
Iteration 93/1000 | Loss: 0.00003797
Iteration 94/1000 | Loss: 0.00003797
Iteration 95/1000 | Loss: 0.00003797
Iteration 96/1000 | Loss: 0.00003796
Iteration 97/1000 | Loss: 0.00003796
Iteration 98/1000 | Loss: 0.00003796
Iteration 99/1000 | Loss: 0.00003796
Iteration 100/1000 | Loss: 0.00003796
Iteration 101/1000 | Loss: 0.00003795
Iteration 102/1000 | Loss: 0.00003795
Iteration 103/1000 | Loss: 0.00003794
Iteration 104/1000 | Loss: 0.00003794
Iteration 105/1000 | Loss: 0.00003794
Iteration 106/1000 | Loss: 0.00003794
Iteration 107/1000 | Loss: 0.00003794
Iteration 108/1000 | Loss: 0.00003793
Iteration 109/1000 | Loss: 0.00003793
Iteration 110/1000 | Loss: 0.00003793
Iteration 111/1000 | Loss: 0.00003793
Iteration 112/1000 | Loss: 0.00003793
Iteration 113/1000 | Loss: 0.00003793
Iteration 114/1000 | Loss: 0.00003793
Iteration 115/1000 | Loss: 0.00003793
Iteration 116/1000 | Loss: 0.00003793
Iteration 117/1000 | Loss: 0.00003793
Iteration 118/1000 | Loss: 0.00003793
Iteration 119/1000 | Loss: 0.00003793
Iteration 120/1000 | Loss: 0.00003793
Iteration 121/1000 | Loss: 0.00003792
Iteration 122/1000 | Loss: 0.00003792
Iteration 123/1000 | Loss: 0.00003792
Iteration 124/1000 | Loss: 0.00003792
Iteration 125/1000 | Loss: 0.00003792
Iteration 126/1000 | Loss: 0.00003792
Iteration 127/1000 | Loss: 0.00003792
Iteration 128/1000 | Loss: 0.00003792
Iteration 129/1000 | Loss: 0.00003791
Iteration 130/1000 | Loss: 0.00003791
Iteration 131/1000 | Loss: 0.00003791
Iteration 132/1000 | Loss: 0.00003791
Iteration 133/1000 | Loss: 0.00003791
Iteration 134/1000 | Loss: 0.00003791
Iteration 135/1000 | Loss: 0.00003791
Iteration 136/1000 | Loss: 0.00003791
Iteration 137/1000 | Loss: 0.00003791
Iteration 138/1000 | Loss: 0.00003791
Iteration 139/1000 | Loss: 0.00003791
Iteration 140/1000 | Loss: 0.00003791
Iteration 141/1000 | Loss: 0.00003791
Iteration 142/1000 | Loss: 0.00003791
Iteration 143/1000 | Loss: 0.00003791
Iteration 144/1000 | Loss: 0.00003791
Iteration 145/1000 | Loss: 0.00003791
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 145. Stopping optimization.
Last 5 losses: [3.7912635889369994e-05, 3.7912635889369994e-05, 3.7912635889369994e-05, 3.7912635889369994e-05, 3.7912635889369994e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.7912635889369994e-05

Optimization complete. Final v2v error: 5.31117057800293 mm

Highest mean error: 11.452495574951172 mm for frame 34

Lowest mean error: 5.0711188316345215 mm for frame 135

Saving results

Total time: 125.44864821434021
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_47_us_1850/0014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_47_us_1850/0014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_47_us_1850/0014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01043905
Iteration 2/25 | Loss: 0.00207149
Iteration 3/25 | Loss: 0.00178871
Iteration 4/25 | Loss: 0.00175777
Iteration 5/25 | Loss: 0.00174034
Iteration 6/25 | Loss: 0.00173641
Iteration 7/25 | Loss: 0.00172378
Iteration 8/25 | Loss: 0.00171802
Iteration 9/25 | Loss: 0.00171599
Iteration 10/25 | Loss: 0.00171421
Iteration 11/25 | Loss: 0.00171779
Iteration 12/25 | Loss: 0.00171984
Iteration 13/25 | Loss: 0.00171562
Iteration 14/25 | Loss: 0.00171014
Iteration 15/25 | Loss: 0.00171340
Iteration 16/25 | Loss: 0.00171113
Iteration 17/25 | Loss: 0.00171209
Iteration 18/25 | Loss: 0.00170968
Iteration 19/25 | Loss: 0.00170825
Iteration 20/25 | Loss: 0.00171041
Iteration 21/25 | Loss: 0.00170744
Iteration 22/25 | Loss: 0.00170786
Iteration 23/25 | Loss: 0.00170627
Iteration 24/25 | Loss: 0.00170695
Iteration 25/25 | Loss: 0.00170676

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.73751807
Iteration 2/25 | Loss: 0.00218261
Iteration 3/25 | Loss: 0.00216499
Iteration 4/25 | Loss: 0.00216499
Iteration 5/25 | Loss: 0.00216498
Iteration 6/25 | Loss: 0.00216498
Iteration 7/25 | Loss: 0.00216498
Iteration 8/25 | Loss: 0.00216498
Iteration 9/25 | Loss: 0.00216498
Iteration 10/25 | Loss: 0.00216498
Iteration 11/25 | Loss: 0.00216498
Iteration 12/25 | Loss: 0.00216498
Iteration 13/25 | Loss: 0.00216498
Iteration 14/25 | Loss: 0.00216498
Iteration 15/25 | Loss: 0.00216498
Iteration 16/25 | Loss: 0.00216498
Iteration 17/25 | Loss: 0.00216498
Iteration 18/25 | Loss: 0.00216498
Iteration 19/25 | Loss: 0.00216498
Iteration 20/25 | Loss: 0.00216498
Iteration 21/25 | Loss: 0.00216498
Iteration 22/25 | Loss: 0.00216498
Iteration 23/25 | Loss: 0.00216498
Iteration 24/25 | Loss: 0.00216498
Iteration 25/25 | Loss: 0.00216498

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00216498
Iteration 2/1000 | Loss: 0.00020296
Iteration 3/1000 | Loss: 0.00012413
Iteration 4/1000 | Loss: 0.00138093
Iteration 5/1000 | Loss: 0.00283523
Iteration 6/1000 | Loss: 0.00304892
Iteration 7/1000 | Loss: 0.00420967
Iteration 8/1000 | Loss: 0.00316609
Iteration 9/1000 | Loss: 0.00098989
Iteration 10/1000 | Loss: 0.00292236
Iteration 11/1000 | Loss: 0.00109538
Iteration 12/1000 | Loss: 0.00179013
Iteration 13/1000 | Loss: 0.00395679
Iteration 14/1000 | Loss: 0.00223957
Iteration 15/1000 | Loss: 0.00190360
Iteration 16/1000 | Loss: 0.00011764
Iteration 17/1000 | Loss: 0.00080870
Iteration 18/1000 | Loss: 0.00030688
Iteration 19/1000 | Loss: 0.00180733
Iteration 20/1000 | Loss: 0.00101042
Iteration 21/1000 | Loss: 0.00152181
Iteration 22/1000 | Loss: 0.00033452
Iteration 23/1000 | Loss: 0.00097809
Iteration 24/1000 | Loss: 0.00138401
Iteration 25/1000 | Loss: 0.00064168
Iteration 26/1000 | Loss: 0.00090716
Iteration 27/1000 | Loss: 0.00044740
Iteration 28/1000 | Loss: 0.00087884
Iteration 29/1000 | Loss: 0.00019316
Iteration 30/1000 | Loss: 0.00043530
Iteration 31/1000 | Loss: 0.00018275
Iteration 32/1000 | Loss: 0.00047245
Iteration 33/1000 | Loss: 0.00083897
Iteration 34/1000 | Loss: 0.00061115
Iteration 35/1000 | Loss: 0.00068983
Iteration 36/1000 | Loss: 0.00068415
Iteration 37/1000 | Loss: 0.00085405
Iteration 38/1000 | Loss: 0.00035615
Iteration 39/1000 | Loss: 0.00037215
Iteration 40/1000 | Loss: 0.00021356
Iteration 41/1000 | Loss: 0.00029358
Iteration 42/1000 | Loss: 0.00013436
Iteration 43/1000 | Loss: 0.00029722
Iteration 44/1000 | Loss: 0.00045923
Iteration 45/1000 | Loss: 0.00066820
Iteration 46/1000 | Loss: 0.00065759
Iteration 47/1000 | Loss: 0.00034240
Iteration 48/1000 | Loss: 0.00054000
Iteration 49/1000 | Loss: 0.00034489
Iteration 50/1000 | Loss: 0.00018207
Iteration 51/1000 | Loss: 0.00011666
Iteration 52/1000 | Loss: 0.00016308
Iteration 53/1000 | Loss: 0.00015764
Iteration 54/1000 | Loss: 0.00013145
Iteration 55/1000 | Loss: 0.00046723
Iteration 56/1000 | Loss: 0.00024514
Iteration 57/1000 | Loss: 0.00043206
Iteration 58/1000 | Loss: 0.00007072
Iteration 59/1000 | Loss: 0.00013193
Iteration 60/1000 | Loss: 0.00006186
Iteration 61/1000 | Loss: 0.00106158
Iteration 62/1000 | Loss: 0.00044226
Iteration 63/1000 | Loss: 0.00099454
Iteration 64/1000 | Loss: 0.00043551
Iteration 65/1000 | Loss: 0.00105014
Iteration 66/1000 | Loss: 0.00039042
Iteration 67/1000 | Loss: 0.00024141
Iteration 68/1000 | Loss: 0.00007535
Iteration 69/1000 | Loss: 0.00011000
Iteration 70/1000 | Loss: 0.00006177
Iteration 71/1000 | Loss: 0.00051798
Iteration 72/1000 | Loss: 0.00006527
Iteration 73/1000 | Loss: 0.00124500
Iteration 74/1000 | Loss: 0.00041562
Iteration 75/1000 | Loss: 0.00008719
Iteration 76/1000 | Loss: 0.00052672
Iteration 77/1000 | Loss: 0.00021407
Iteration 78/1000 | Loss: 0.00044419
Iteration 79/1000 | Loss: 0.00029705
Iteration 80/1000 | Loss: 0.00005519
Iteration 81/1000 | Loss: 0.00004995
Iteration 82/1000 | Loss: 0.00009936
Iteration 83/1000 | Loss: 0.00005278
Iteration 84/1000 | Loss: 0.00012090
Iteration 85/1000 | Loss: 0.00007754
Iteration 86/1000 | Loss: 0.00032567
Iteration 87/1000 | Loss: 0.00009678
Iteration 88/1000 | Loss: 0.00012173
Iteration 89/1000 | Loss: 0.00018106
Iteration 90/1000 | Loss: 0.00028191
Iteration 91/1000 | Loss: 0.00038339
Iteration 92/1000 | Loss: 0.00005828
Iteration 93/1000 | Loss: 0.00005222
Iteration 94/1000 | Loss: 0.00043732
Iteration 95/1000 | Loss: 0.00040385
Iteration 96/1000 | Loss: 0.00005395
Iteration 97/1000 | Loss: 0.00005026
Iteration 98/1000 | Loss: 0.00004792
Iteration 99/1000 | Loss: 0.00004693
Iteration 100/1000 | Loss: 0.00004663
Iteration 101/1000 | Loss: 0.00004632
Iteration 102/1000 | Loss: 0.00004630
Iteration 103/1000 | Loss: 0.00004607
Iteration 104/1000 | Loss: 0.00022422
Iteration 105/1000 | Loss: 0.00004915
Iteration 106/1000 | Loss: 0.00034644
Iteration 107/1000 | Loss: 0.00039799
Iteration 108/1000 | Loss: 0.00022382
Iteration 109/1000 | Loss: 0.00028946
Iteration 110/1000 | Loss: 0.00015667
Iteration 111/1000 | Loss: 0.00020726
Iteration 112/1000 | Loss: 0.00018188
Iteration 113/1000 | Loss: 0.00019850
Iteration 114/1000 | Loss: 0.00056128
Iteration 115/1000 | Loss: 0.00005905
Iteration 116/1000 | Loss: 0.00005095
Iteration 117/1000 | Loss: 0.00004777
Iteration 118/1000 | Loss: 0.00004662
Iteration 119/1000 | Loss: 0.00004547
Iteration 120/1000 | Loss: 0.00018826
Iteration 121/1000 | Loss: 0.00027462
Iteration 122/1000 | Loss: 0.00005384
Iteration 123/1000 | Loss: 0.00006349
Iteration 124/1000 | Loss: 0.00004687
Iteration 125/1000 | Loss: 0.00006899
Iteration 126/1000 | Loss: 0.00006070
Iteration 127/1000 | Loss: 0.00006915
Iteration 128/1000 | Loss: 0.00005673
Iteration 129/1000 | Loss: 0.00005755
Iteration 130/1000 | Loss: 0.00010354
Iteration 131/1000 | Loss: 0.00006055
Iteration 132/1000 | Loss: 0.00014719
Iteration 133/1000 | Loss: 0.00008278
Iteration 134/1000 | Loss: 0.00017686
Iteration 135/1000 | Loss: 0.00012276
Iteration 136/1000 | Loss: 0.00017437
Iteration 137/1000 | Loss: 0.00009901
Iteration 138/1000 | Loss: 0.00006194
Iteration 139/1000 | Loss: 0.00018742
Iteration 140/1000 | Loss: 0.00012257
Iteration 141/1000 | Loss: 0.00004415
Iteration 142/1000 | Loss: 0.00012342
Iteration 143/1000 | Loss: 0.00011917
Iteration 144/1000 | Loss: 0.00004608
Iteration 145/1000 | Loss: 0.00004457
Iteration 146/1000 | Loss: 0.00004387
Iteration 147/1000 | Loss: 0.00004357
Iteration 148/1000 | Loss: 0.00004345
Iteration 149/1000 | Loss: 0.00004342
Iteration 150/1000 | Loss: 0.00004314
Iteration 151/1000 | Loss: 0.00004314
Iteration 152/1000 | Loss: 0.00004309
Iteration 153/1000 | Loss: 0.00004308
Iteration 154/1000 | Loss: 0.00004308
Iteration 155/1000 | Loss: 0.00004303
Iteration 156/1000 | Loss: 0.00004302
Iteration 157/1000 | Loss: 0.00004301
Iteration 158/1000 | Loss: 0.00004301
Iteration 159/1000 | Loss: 0.00004300
Iteration 160/1000 | Loss: 0.00004300
Iteration 161/1000 | Loss: 0.00004299
Iteration 162/1000 | Loss: 0.00004299
Iteration 163/1000 | Loss: 0.00004296
Iteration 164/1000 | Loss: 0.00004296
Iteration 165/1000 | Loss: 0.00004296
Iteration 166/1000 | Loss: 0.00004296
Iteration 167/1000 | Loss: 0.00004296
Iteration 168/1000 | Loss: 0.00004296
Iteration 169/1000 | Loss: 0.00004296
Iteration 170/1000 | Loss: 0.00004295
Iteration 171/1000 | Loss: 0.00004295
Iteration 172/1000 | Loss: 0.00004295
Iteration 173/1000 | Loss: 0.00004295
Iteration 174/1000 | Loss: 0.00004295
Iteration 175/1000 | Loss: 0.00004295
Iteration 176/1000 | Loss: 0.00004295
Iteration 177/1000 | Loss: 0.00004295
Iteration 178/1000 | Loss: 0.00004295
Iteration 179/1000 | Loss: 0.00004295
Iteration 180/1000 | Loss: 0.00004295
Iteration 181/1000 | Loss: 0.00004295
Iteration 182/1000 | Loss: 0.00004294
Iteration 183/1000 | Loss: 0.00004294
Iteration 184/1000 | Loss: 0.00004294
Iteration 185/1000 | Loss: 0.00004294
Iteration 186/1000 | Loss: 0.00004294
Iteration 187/1000 | Loss: 0.00004294
Iteration 188/1000 | Loss: 0.00004294
Iteration 189/1000 | Loss: 0.00004294
Iteration 190/1000 | Loss: 0.00004294
Iteration 191/1000 | Loss: 0.00004294
Iteration 192/1000 | Loss: 0.00004293
Iteration 193/1000 | Loss: 0.00004293
Iteration 194/1000 | Loss: 0.00004293
Iteration 195/1000 | Loss: 0.00004293
Iteration 196/1000 | Loss: 0.00004293
Iteration 197/1000 | Loss: 0.00004293
Iteration 198/1000 | Loss: 0.00004293
Iteration 199/1000 | Loss: 0.00004293
Iteration 200/1000 | Loss: 0.00004293
Iteration 201/1000 | Loss: 0.00004293
Iteration 202/1000 | Loss: 0.00004293
Iteration 203/1000 | Loss: 0.00004293
Iteration 204/1000 | Loss: 0.00004293
Iteration 205/1000 | Loss: 0.00004292
Iteration 206/1000 | Loss: 0.00004292
Iteration 207/1000 | Loss: 0.00004292
Iteration 208/1000 | Loss: 0.00004292
Iteration 209/1000 | Loss: 0.00004292
Iteration 210/1000 | Loss: 0.00004292
Iteration 211/1000 | Loss: 0.00004292
Iteration 212/1000 | Loss: 0.00004292
Iteration 213/1000 | Loss: 0.00004292
Iteration 214/1000 | Loss: 0.00004292
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 214. Stopping optimization.
Last 5 losses: [4.2923529690597206e-05, 4.2923529690597206e-05, 4.2923529690597206e-05, 4.2923529690597206e-05, 4.2923529690597206e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.2923529690597206e-05

Optimization complete. Final v2v error: 5.7162699699401855 mm

Highest mean error: 6.974314212799072 mm for frame 140

Lowest mean error: 5.048568248748779 mm for frame 187

Saving results

Total time: 292.12854862213135
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_47_us_1850/0015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_47_us_1850/0015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_47_us_1850/0015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00986264
Iteration 2/25 | Loss: 0.00319052
Iteration 3/25 | Loss: 0.00232807
Iteration 4/25 | Loss: 0.00197502
Iteration 5/25 | Loss: 0.00195491
Iteration 6/25 | Loss: 0.00190511
Iteration 7/25 | Loss: 0.00186307
Iteration 8/25 | Loss: 0.00184802
Iteration 9/25 | Loss: 0.00182350
Iteration 10/25 | Loss: 0.00181993
Iteration 11/25 | Loss: 0.00179183
Iteration 12/25 | Loss: 0.00179156
Iteration 13/25 | Loss: 0.00178494
Iteration 14/25 | Loss: 0.00179661
Iteration 15/25 | Loss: 0.00178707
Iteration 16/25 | Loss: 0.00176781
Iteration 17/25 | Loss: 0.00176716
Iteration 18/25 | Loss: 0.00175407
Iteration 19/25 | Loss: 0.00174482
Iteration 20/25 | Loss: 0.00175386
Iteration 21/25 | Loss: 0.00175082
Iteration 22/25 | Loss: 0.00174930
Iteration 23/25 | Loss: 0.00175292
Iteration 24/25 | Loss: 0.00174717
Iteration 25/25 | Loss: 0.00174832

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.23249578
Iteration 2/25 | Loss: 0.00420737
Iteration 3/25 | Loss: 0.00397748
Iteration 4/25 | Loss: 0.00397747
Iteration 5/25 | Loss: 0.00397747
Iteration 6/25 | Loss: 0.00397746
Iteration 7/25 | Loss: 0.00397746
Iteration 8/25 | Loss: 0.00397746
Iteration 9/25 | Loss: 0.00397746
Iteration 10/25 | Loss: 0.00397746
Iteration 11/25 | Loss: 0.00397746
Iteration 12/25 | Loss: 0.00397746
Iteration 13/25 | Loss: 0.00397746
Iteration 14/25 | Loss: 0.00397746
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.00397746404632926, 0.00397746404632926, 0.00397746404632926, 0.00397746404632926, 0.00397746404632926]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00397746404632926

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00397746
Iteration 2/1000 | Loss: 0.00239757
Iteration 3/1000 | Loss: 0.01041655
Iteration 4/1000 | Loss: 0.00061655
Iteration 5/1000 | Loss: 0.00531170
Iteration 6/1000 | Loss: 0.00046929
Iteration 7/1000 | Loss: 0.00344363
Iteration 8/1000 | Loss: 0.00080553
Iteration 9/1000 | Loss: 0.00094474
Iteration 10/1000 | Loss: 0.00411272
Iteration 11/1000 | Loss: 0.00083277
Iteration 12/1000 | Loss: 0.00020825
Iteration 13/1000 | Loss: 0.00040932
Iteration 14/1000 | Loss: 0.00025027
Iteration 15/1000 | Loss: 0.00048936
Iteration 16/1000 | Loss: 0.00017476
Iteration 17/1000 | Loss: 0.00050582
Iteration 18/1000 | Loss: 0.00033220
Iteration 19/1000 | Loss: 0.00036509
Iteration 20/1000 | Loss: 0.00112430
Iteration 21/1000 | Loss: 0.00039580
Iteration 22/1000 | Loss: 0.00131224
Iteration 23/1000 | Loss: 0.00027807
Iteration 24/1000 | Loss: 0.00029524
Iteration 25/1000 | Loss: 0.00024214
Iteration 26/1000 | Loss: 0.00043238
Iteration 27/1000 | Loss: 0.00024244
Iteration 28/1000 | Loss: 0.00025172
Iteration 29/1000 | Loss: 0.00029164
Iteration 30/1000 | Loss: 0.00032570
Iteration 31/1000 | Loss: 0.00030166
Iteration 32/1000 | Loss: 0.00031073
Iteration 33/1000 | Loss: 0.00015762
Iteration 34/1000 | Loss: 0.00044650
Iteration 35/1000 | Loss: 0.00028467
Iteration 36/1000 | Loss: 0.00028528
Iteration 37/1000 | Loss: 0.00026721
Iteration 38/1000 | Loss: 0.00029578
Iteration 39/1000 | Loss: 0.00026967
Iteration 40/1000 | Loss: 0.00038297
Iteration 41/1000 | Loss: 0.00008146
Iteration 42/1000 | Loss: 0.00143475
Iteration 43/1000 | Loss: 0.00009656
Iteration 44/1000 | Loss: 0.00007651
Iteration 45/1000 | Loss: 0.00013197
Iteration 46/1000 | Loss: 0.00034450
Iteration 47/1000 | Loss: 0.00006925
Iteration 48/1000 | Loss: 0.00013747
Iteration 49/1000 | Loss: 0.00006801
Iteration 50/1000 | Loss: 0.00006475
Iteration 51/1000 | Loss: 0.00006306
Iteration 52/1000 | Loss: 0.00006208
Iteration 53/1000 | Loss: 0.00024398
Iteration 54/1000 | Loss: 0.00009410
Iteration 55/1000 | Loss: 0.00012568
Iteration 56/1000 | Loss: 0.00133143
Iteration 57/1000 | Loss: 0.00260206
Iteration 58/1000 | Loss: 0.00176631
Iteration 59/1000 | Loss: 0.00126700
Iteration 60/1000 | Loss: 0.00019722
Iteration 61/1000 | Loss: 0.00024584
Iteration 62/1000 | Loss: 0.00059050
Iteration 63/1000 | Loss: 0.00015240
Iteration 64/1000 | Loss: 0.00056827
Iteration 65/1000 | Loss: 0.00056964
Iteration 66/1000 | Loss: 0.00035271
Iteration 67/1000 | Loss: 0.00010494
Iteration 68/1000 | Loss: 0.00008965
Iteration 69/1000 | Loss: 0.00017385
Iteration 70/1000 | Loss: 0.00005835
Iteration 71/1000 | Loss: 0.00023472
Iteration 72/1000 | Loss: 0.00006353
Iteration 73/1000 | Loss: 0.00005512
Iteration 74/1000 | Loss: 0.00005430
Iteration 75/1000 | Loss: 0.00005379
Iteration 76/1000 | Loss: 0.00019201
Iteration 77/1000 | Loss: 0.00010288
Iteration 78/1000 | Loss: 0.00006291
Iteration 79/1000 | Loss: 0.00005269
Iteration 80/1000 | Loss: 0.00012541
Iteration 81/1000 | Loss: 0.00007689
Iteration 82/1000 | Loss: 0.00005511
Iteration 83/1000 | Loss: 0.00005269
Iteration 84/1000 | Loss: 0.00007300
Iteration 85/1000 | Loss: 0.00005212
Iteration 86/1000 | Loss: 0.00016513
Iteration 87/1000 | Loss: 0.00007537
Iteration 88/1000 | Loss: 0.00005217
Iteration 89/1000 | Loss: 0.00012325
Iteration 90/1000 | Loss: 0.00016994
Iteration 91/1000 | Loss: 0.00005208
Iteration 92/1000 | Loss: 0.00010766
Iteration 93/1000 | Loss: 0.00008250
Iteration 94/1000 | Loss: 0.00007043
Iteration 95/1000 | Loss: 0.00005159
Iteration 96/1000 | Loss: 0.00005140
Iteration 97/1000 | Loss: 0.00005136
Iteration 98/1000 | Loss: 0.00005133
Iteration 99/1000 | Loss: 0.00005129
Iteration 100/1000 | Loss: 0.00005124
Iteration 101/1000 | Loss: 0.00005122
Iteration 102/1000 | Loss: 0.00126798
Iteration 103/1000 | Loss: 0.00008296
Iteration 104/1000 | Loss: 0.00008083
Iteration 105/1000 | Loss: 0.00005290
Iteration 106/1000 | Loss: 0.00005017
Iteration 107/1000 | Loss: 0.00005855
Iteration 108/1000 | Loss: 0.00004793
Iteration 109/1000 | Loss: 0.00006584
Iteration 110/1000 | Loss: 0.00004737
Iteration 111/1000 | Loss: 0.00004702
Iteration 112/1000 | Loss: 0.00004686
Iteration 113/1000 | Loss: 0.00004686
Iteration 114/1000 | Loss: 0.00004686
Iteration 115/1000 | Loss: 0.00004685
Iteration 116/1000 | Loss: 0.00004685
Iteration 117/1000 | Loss: 0.00004685
Iteration 118/1000 | Loss: 0.00004685
Iteration 119/1000 | Loss: 0.00004685
Iteration 120/1000 | Loss: 0.00004685
Iteration 121/1000 | Loss: 0.00004685
Iteration 122/1000 | Loss: 0.00004685
Iteration 123/1000 | Loss: 0.00004685
Iteration 124/1000 | Loss: 0.00004685
Iteration 125/1000 | Loss: 0.00004684
Iteration 126/1000 | Loss: 0.00004683
Iteration 127/1000 | Loss: 0.00004682
Iteration 128/1000 | Loss: 0.00004682
Iteration 129/1000 | Loss: 0.00004682
Iteration 130/1000 | Loss: 0.00004682
Iteration 131/1000 | Loss: 0.00004681
Iteration 132/1000 | Loss: 0.00004681
Iteration 133/1000 | Loss: 0.00004680
Iteration 134/1000 | Loss: 0.00004679
Iteration 135/1000 | Loss: 0.00004679
Iteration 136/1000 | Loss: 0.00004679
Iteration 137/1000 | Loss: 0.00004679
Iteration 138/1000 | Loss: 0.00004678
Iteration 139/1000 | Loss: 0.00004678
Iteration 140/1000 | Loss: 0.00009195
Iteration 141/1000 | Loss: 0.00004681
Iteration 142/1000 | Loss: 0.00004676
Iteration 143/1000 | Loss: 0.00004676
Iteration 144/1000 | Loss: 0.00004676
Iteration 145/1000 | Loss: 0.00004676
Iteration 146/1000 | Loss: 0.00004676
Iteration 147/1000 | Loss: 0.00004676
Iteration 148/1000 | Loss: 0.00004676
Iteration 149/1000 | Loss: 0.00004676
Iteration 150/1000 | Loss: 0.00004676
Iteration 151/1000 | Loss: 0.00004676
Iteration 152/1000 | Loss: 0.00004676
Iteration 153/1000 | Loss: 0.00004676
Iteration 154/1000 | Loss: 0.00004675
Iteration 155/1000 | Loss: 0.00004675
Iteration 156/1000 | Loss: 0.00004675
Iteration 157/1000 | Loss: 0.00004675
Iteration 158/1000 | Loss: 0.00004675
Iteration 159/1000 | Loss: 0.00004675
Iteration 160/1000 | Loss: 0.00004675
Iteration 161/1000 | Loss: 0.00004675
Iteration 162/1000 | Loss: 0.00004675
Iteration 163/1000 | Loss: 0.00004675
Iteration 164/1000 | Loss: 0.00004674
Iteration 165/1000 | Loss: 0.00004674
Iteration 166/1000 | Loss: 0.00004674
Iteration 167/1000 | Loss: 0.00004674
Iteration 168/1000 | Loss: 0.00004674
Iteration 169/1000 | Loss: 0.00004674
Iteration 170/1000 | Loss: 0.00004674
Iteration 171/1000 | Loss: 0.00004674
Iteration 172/1000 | Loss: 0.00004674
Iteration 173/1000 | Loss: 0.00004674
Iteration 174/1000 | Loss: 0.00004674
Iteration 175/1000 | Loss: 0.00004674
Iteration 176/1000 | Loss: 0.00004674
Iteration 177/1000 | Loss: 0.00004674
Iteration 178/1000 | Loss: 0.00004674
Iteration 179/1000 | Loss: 0.00004674
Iteration 180/1000 | Loss: 0.00004674
Iteration 181/1000 | Loss: 0.00004674
Iteration 182/1000 | Loss: 0.00004674
Iteration 183/1000 | Loss: 0.00004674
Iteration 184/1000 | Loss: 0.00004674
Iteration 185/1000 | Loss: 0.00004674
Iteration 186/1000 | Loss: 0.00004674
Iteration 187/1000 | Loss: 0.00004674
Iteration 188/1000 | Loss: 0.00004674
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 188. Stopping optimization.
Last 5 losses: [4.6741282858420163e-05, 4.6741282858420163e-05, 4.6741282858420163e-05, 4.6741282858420163e-05, 4.6741282858420163e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.6741282858420163e-05

Optimization complete. Final v2v error: 5.718520164489746 mm

Highest mean error: 10.319975852966309 mm for frame 26

Lowest mean error: 4.6802592277526855 mm for frame 1

Saving results

Total time: 201.56011176109314
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_47_us_1850/0010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_47_us_1850/0010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_47_us_1850/0010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01121344
Iteration 2/25 | Loss: 0.00290099
Iteration 3/25 | Loss: 0.00216566
Iteration 4/25 | Loss: 0.00238064
Iteration 5/25 | Loss: 0.00244355
Iteration 6/25 | Loss: 0.00199566
Iteration 7/25 | Loss: 0.00185188
Iteration 8/25 | Loss: 0.00181611
Iteration 9/25 | Loss: 0.00180679
Iteration 10/25 | Loss: 0.00176328
Iteration 11/25 | Loss: 0.00181045
Iteration 12/25 | Loss: 0.00180925
Iteration 13/25 | Loss: 0.00177277
Iteration 14/25 | Loss: 0.00177380
Iteration 15/25 | Loss: 0.00176220
Iteration 16/25 | Loss: 0.00176125
Iteration 17/25 | Loss: 0.00175682
Iteration 18/25 | Loss: 0.00175677
Iteration 19/25 | Loss: 0.00176083
Iteration 20/25 | Loss: 0.00175652
Iteration 21/25 | Loss: 0.00176084
Iteration 22/25 | Loss: 0.00175653
Iteration 23/25 | Loss: 0.00173968
Iteration 24/25 | Loss: 0.00175371
Iteration 25/25 | Loss: 0.00172801

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.26666760
Iteration 2/25 | Loss: 0.00183156
Iteration 3/25 | Loss: 0.00183156
Iteration 4/25 | Loss: 0.00183156
Iteration 5/25 | Loss: 0.00183156
Iteration 6/25 | Loss: 0.00183156
Iteration 7/25 | Loss: 0.00183156
Iteration 8/25 | Loss: 0.00183156
Iteration 9/25 | Loss: 0.00183156
Iteration 10/25 | Loss: 0.00183156
Iteration 11/25 | Loss: 0.00183156
Iteration 12/25 | Loss: 0.00183156
Iteration 13/25 | Loss: 0.00183156
Iteration 14/25 | Loss: 0.00183156
Iteration 15/25 | Loss: 0.00183156
Iteration 16/25 | Loss: 0.00183156
Iteration 17/25 | Loss: 0.00183156
Iteration 18/25 | Loss: 0.00183156
Iteration 19/25 | Loss: 0.00183156
Iteration 20/25 | Loss: 0.00183156
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0018315606284886599, 0.0018315606284886599, 0.0018315606284886599, 0.0018315606284886599, 0.0018315606284886599]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0018315606284886599

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00183156
Iteration 2/1000 | Loss: 0.00019519
Iteration 3/1000 | Loss: 0.00013753
Iteration 4/1000 | Loss: 0.00010983
Iteration 5/1000 | Loss: 0.00013704
Iteration 6/1000 | Loss: 0.00009678
Iteration 7/1000 | Loss: 0.00008149
Iteration 8/1000 | Loss: 0.00007804
Iteration 9/1000 | Loss: 0.00007280
Iteration 10/1000 | Loss: 0.00006927
Iteration 11/1000 | Loss: 0.00006718
Iteration 12/1000 | Loss: 0.00064306
Iteration 13/1000 | Loss: 0.00133768
Iteration 14/1000 | Loss: 0.00026114
Iteration 15/1000 | Loss: 0.00012058
Iteration 16/1000 | Loss: 0.00008434
Iteration 17/1000 | Loss: 0.00006991
Iteration 18/1000 | Loss: 0.00005937
Iteration 19/1000 | Loss: 0.00005024
Iteration 20/1000 | Loss: 0.00004511
Iteration 21/1000 | Loss: 0.00004259
Iteration 22/1000 | Loss: 0.00004090
Iteration 23/1000 | Loss: 0.00004003
Iteration 24/1000 | Loss: 0.00003934
Iteration 25/1000 | Loss: 0.00003880
Iteration 26/1000 | Loss: 0.00003836
Iteration 27/1000 | Loss: 0.00003808
Iteration 28/1000 | Loss: 0.00003788
Iteration 29/1000 | Loss: 0.00003787
Iteration 30/1000 | Loss: 0.00003787
Iteration 31/1000 | Loss: 0.00003786
Iteration 32/1000 | Loss: 0.00003786
Iteration 33/1000 | Loss: 0.00003785
Iteration 34/1000 | Loss: 0.00003784
Iteration 35/1000 | Loss: 0.00003783
Iteration 36/1000 | Loss: 0.00003783
Iteration 37/1000 | Loss: 0.00003783
Iteration 38/1000 | Loss: 0.00003783
Iteration 39/1000 | Loss: 0.00003783
Iteration 40/1000 | Loss: 0.00003783
Iteration 41/1000 | Loss: 0.00003783
Iteration 42/1000 | Loss: 0.00003783
Iteration 43/1000 | Loss: 0.00003783
Iteration 44/1000 | Loss: 0.00003782
Iteration 45/1000 | Loss: 0.00003782
Iteration 46/1000 | Loss: 0.00003782
Iteration 47/1000 | Loss: 0.00003782
Iteration 48/1000 | Loss: 0.00003781
Iteration 49/1000 | Loss: 0.00003781
Iteration 50/1000 | Loss: 0.00003780
Iteration 51/1000 | Loss: 0.00003780
Iteration 52/1000 | Loss: 0.00003780
Iteration 53/1000 | Loss: 0.00003780
Iteration 54/1000 | Loss: 0.00003780
Iteration 55/1000 | Loss: 0.00003780
Iteration 56/1000 | Loss: 0.00003779
Iteration 57/1000 | Loss: 0.00003779
Iteration 58/1000 | Loss: 0.00003778
Iteration 59/1000 | Loss: 0.00003778
Iteration 60/1000 | Loss: 0.00003778
Iteration 61/1000 | Loss: 0.00003778
Iteration 62/1000 | Loss: 0.00003778
Iteration 63/1000 | Loss: 0.00003778
Iteration 64/1000 | Loss: 0.00003778
Iteration 65/1000 | Loss: 0.00003778
Iteration 66/1000 | Loss: 0.00003778
Iteration 67/1000 | Loss: 0.00003778
Iteration 68/1000 | Loss: 0.00003778
Iteration 69/1000 | Loss: 0.00003777
Iteration 70/1000 | Loss: 0.00003777
Iteration 71/1000 | Loss: 0.00003777
Iteration 72/1000 | Loss: 0.00003777
Iteration 73/1000 | Loss: 0.00003777
Iteration 74/1000 | Loss: 0.00003776
Iteration 75/1000 | Loss: 0.00003776
Iteration 76/1000 | Loss: 0.00003776
Iteration 77/1000 | Loss: 0.00003776
Iteration 78/1000 | Loss: 0.00003776
Iteration 79/1000 | Loss: 0.00003776
Iteration 80/1000 | Loss: 0.00003776
Iteration 81/1000 | Loss: 0.00003776
Iteration 82/1000 | Loss: 0.00003776
Iteration 83/1000 | Loss: 0.00003776
Iteration 84/1000 | Loss: 0.00003776
Iteration 85/1000 | Loss: 0.00003776
Iteration 86/1000 | Loss: 0.00003776
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 86. Stopping optimization.
Last 5 losses: [3.775668301386759e-05, 3.775668301386759e-05, 3.775668301386759e-05, 3.775668301386759e-05, 3.775668301386759e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.775668301386759e-05

Optimization complete. Final v2v error: 5.395036220550537 mm

Highest mean error: 5.648108959197998 mm for frame 141

Lowest mean error: 5.149487018585205 mm for frame 22

Saving results

Total time: 101.26895833015442
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_56_us_1894/0008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_56_us_1894/0008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_56_us_1894/0008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01179176
Iteration 2/25 | Loss: 0.00281494
Iteration 3/25 | Loss: 0.00227551
Iteration 4/25 | Loss: 0.00218197
Iteration 5/25 | Loss: 0.00209797
Iteration 6/25 | Loss: 0.00204111
Iteration 7/25 | Loss: 0.00198176
Iteration 8/25 | Loss: 0.00194177
Iteration 9/25 | Loss: 0.00194778
Iteration 10/25 | Loss: 0.00195082
Iteration 11/25 | Loss: 0.00190704
Iteration 12/25 | Loss: 0.00189600
Iteration 13/25 | Loss: 0.00188549
Iteration 14/25 | Loss: 0.00187624
Iteration 15/25 | Loss: 0.00187371
Iteration 16/25 | Loss: 0.00188173
Iteration 17/25 | Loss: 0.00188475
Iteration 18/25 | Loss: 0.00186499
Iteration 19/25 | Loss: 0.00185759
Iteration 20/25 | Loss: 0.00185736
Iteration 21/25 | Loss: 0.00184897
Iteration 22/25 | Loss: 0.00184668
Iteration 23/25 | Loss: 0.00184756
Iteration 24/25 | Loss: 0.00184168
Iteration 25/25 | Loss: 0.00183505

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33093131
Iteration 2/25 | Loss: 0.00368720
Iteration 3/25 | Loss: 0.00258636
Iteration 4/25 | Loss: 0.00258635
Iteration 5/25 | Loss: 0.00258635
Iteration 6/25 | Loss: 0.00258635
Iteration 7/25 | Loss: 0.00258635
Iteration 8/25 | Loss: 0.00258635
Iteration 9/25 | Loss: 0.00258635
Iteration 10/25 | Loss: 0.00258635
Iteration 11/25 | Loss: 0.00258635
Iteration 12/25 | Loss: 0.00258635
Iteration 13/25 | Loss: 0.00258635
Iteration 14/25 | Loss: 0.00258635
Iteration 15/25 | Loss: 0.00258635
Iteration 16/25 | Loss: 0.00258635
Iteration 17/25 | Loss: 0.00258635
Iteration 18/25 | Loss: 0.00258635
Iteration 19/25 | Loss: 0.00258635
Iteration 20/25 | Loss: 0.00258635
Iteration 21/25 | Loss: 0.00258635
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0025863470509648323, 0.0025863470509648323, 0.0025863470509648323, 0.0025863470509648323, 0.0025863470509648323]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0025863470509648323

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00258635
Iteration 2/1000 | Loss: 0.00047427
Iteration 3/1000 | Loss: 0.00059804
Iteration 4/1000 | Loss: 0.00045282
Iteration 5/1000 | Loss: 0.00042436
Iteration 6/1000 | Loss: 0.00044968
Iteration 7/1000 | Loss: 0.00043453
Iteration 8/1000 | Loss: 0.00056301
Iteration 9/1000 | Loss: 0.00053957
Iteration 10/1000 | Loss: 0.00042185
Iteration 11/1000 | Loss: 0.00067753
Iteration 12/1000 | Loss: 0.00053044
Iteration 13/1000 | Loss: 0.00043210
Iteration 14/1000 | Loss: 0.00041036
Iteration 15/1000 | Loss: 0.00028693
Iteration 16/1000 | Loss: 0.00038170
Iteration 17/1000 | Loss: 0.00051071
Iteration 18/1000 | Loss: 0.00057077
Iteration 19/1000 | Loss: 0.00035873
Iteration 20/1000 | Loss: 0.00073399
Iteration 21/1000 | Loss: 0.00072031
Iteration 22/1000 | Loss: 0.00029112
Iteration 23/1000 | Loss: 0.00056615
Iteration 24/1000 | Loss: 0.00075662
Iteration 25/1000 | Loss: 0.00077520
Iteration 26/1000 | Loss: 0.00079595
Iteration 27/1000 | Loss: 0.00049614
Iteration 28/1000 | Loss: 0.00045981
Iteration 29/1000 | Loss: 0.00089506
Iteration 30/1000 | Loss: 0.00063916
Iteration 31/1000 | Loss: 0.00059367
Iteration 32/1000 | Loss: 0.00041901
Iteration 33/1000 | Loss: 0.00046344
Iteration 34/1000 | Loss: 0.00036585
Iteration 35/1000 | Loss: 0.00048715
Iteration 36/1000 | Loss: 0.00037917
Iteration 37/1000 | Loss: 0.00042425
Iteration 38/1000 | Loss: 0.00035651
Iteration 39/1000 | Loss: 0.00026882
Iteration 40/1000 | Loss: 0.00063873
Iteration 41/1000 | Loss: 0.00043479
Iteration 42/1000 | Loss: 0.00025038
Iteration 43/1000 | Loss: 0.00026180
Iteration 44/1000 | Loss: 0.00026050
Iteration 45/1000 | Loss: 0.00029335
Iteration 46/1000 | Loss: 0.00042752
Iteration 47/1000 | Loss: 0.00029110
Iteration 48/1000 | Loss: 0.00016542
Iteration 49/1000 | Loss: 0.00026243
Iteration 50/1000 | Loss: 0.00025246
Iteration 51/1000 | Loss: 0.00026423
Iteration 52/1000 | Loss: 0.00016210
Iteration 53/1000 | Loss: 0.00023844
Iteration 54/1000 | Loss: 0.00032104
Iteration 55/1000 | Loss: 0.00035065
Iteration 56/1000 | Loss: 0.00034350
Iteration 57/1000 | Loss: 0.00025869
Iteration 58/1000 | Loss: 0.00019014
Iteration 59/1000 | Loss: 0.00030764
Iteration 60/1000 | Loss: 0.00025988
Iteration 61/1000 | Loss: 0.00041381
Iteration 62/1000 | Loss: 0.00045743
Iteration 63/1000 | Loss: 0.00042320
Iteration 64/1000 | Loss: 0.00039114
Iteration 65/1000 | Loss: 0.00037472
Iteration 66/1000 | Loss: 0.00060630
Iteration 67/1000 | Loss: 0.00053458
Iteration 68/1000 | Loss: 0.00054996
Iteration 69/1000 | Loss: 0.00060986
Iteration 70/1000 | Loss: 0.00078838
Iteration 71/1000 | Loss: 0.00100155
Iteration 72/1000 | Loss: 0.00041692
Iteration 73/1000 | Loss: 0.00040041
Iteration 74/1000 | Loss: 0.00018596
Iteration 75/1000 | Loss: 0.00023034
Iteration 76/1000 | Loss: 0.00017999
Iteration 77/1000 | Loss: 0.00025962
Iteration 78/1000 | Loss: 0.00018220
Iteration 79/1000 | Loss: 0.00017472
Iteration 80/1000 | Loss: 0.00012534
Iteration 81/1000 | Loss: 0.00045469
Iteration 82/1000 | Loss: 0.00008198
Iteration 83/1000 | Loss: 0.00006603
Iteration 84/1000 | Loss: 0.00019722
Iteration 85/1000 | Loss: 0.00008230
Iteration 86/1000 | Loss: 0.00005825
Iteration 87/1000 | Loss: 0.00005332
Iteration 88/1000 | Loss: 0.00005038
Iteration 89/1000 | Loss: 0.00008170
Iteration 90/1000 | Loss: 0.00004838
Iteration 91/1000 | Loss: 0.00004742
Iteration 92/1000 | Loss: 0.00004530
Iteration 93/1000 | Loss: 0.00004367
Iteration 94/1000 | Loss: 0.00004292
Iteration 95/1000 | Loss: 0.00004263
Iteration 96/1000 | Loss: 0.00004592
Iteration 97/1000 | Loss: 0.00004259
Iteration 98/1000 | Loss: 0.00004166
Iteration 99/1000 | Loss: 0.00004133
Iteration 100/1000 | Loss: 0.00004106
Iteration 101/1000 | Loss: 0.00004102
Iteration 102/1000 | Loss: 0.00004089
Iteration 103/1000 | Loss: 0.00004084
Iteration 104/1000 | Loss: 0.00004077
Iteration 105/1000 | Loss: 0.00004077
Iteration 106/1000 | Loss: 0.00004077
Iteration 107/1000 | Loss: 0.00004081
Iteration 108/1000 | Loss: 0.00004081
Iteration 109/1000 | Loss: 0.00004081
Iteration 110/1000 | Loss: 0.00004081
Iteration 111/1000 | Loss: 0.00004081
Iteration 112/1000 | Loss: 0.00004080
Iteration 113/1000 | Loss: 0.00004073
Iteration 114/1000 | Loss: 0.00004068
Iteration 115/1000 | Loss: 0.00004064
Iteration 116/1000 | Loss: 0.00004064
Iteration 117/1000 | Loss: 0.00004060
Iteration 118/1000 | Loss: 0.00004057
Iteration 119/1000 | Loss: 0.00004055
Iteration 120/1000 | Loss: 0.00004052
Iteration 121/1000 | Loss: 0.00004060
Iteration 122/1000 | Loss: 0.00004059
Iteration 123/1000 | Loss: 0.00004058
Iteration 124/1000 | Loss: 0.00004046
Iteration 125/1000 | Loss: 0.00004045
Iteration 126/1000 | Loss: 0.00004044
Iteration 127/1000 | Loss: 0.00004044
Iteration 128/1000 | Loss: 0.00004043
Iteration 129/1000 | Loss: 0.00004042
Iteration 130/1000 | Loss: 0.00004041
Iteration 131/1000 | Loss: 0.00004041
Iteration 132/1000 | Loss: 0.00004041
Iteration 133/1000 | Loss: 0.00004041
Iteration 134/1000 | Loss: 0.00004041
Iteration 135/1000 | Loss: 0.00004040
Iteration 136/1000 | Loss: 0.00004040
Iteration 137/1000 | Loss: 0.00004039
Iteration 138/1000 | Loss: 0.00004039
Iteration 139/1000 | Loss: 0.00004039
Iteration 140/1000 | Loss: 0.00004038
Iteration 141/1000 | Loss: 0.00004038
Iteration 142/1000 | Loss: 0.00004037
Iteration 143/1000 | Loss: 0.00004037
Iteration 144/1000 | Loss: 0.00004037
Iteration 145/1000 | Loss: 0.00004037
Iteration 146/1000 | Loss: 0.00004037
Iteration 147/1000 | Loss: 0.00004036
Iteration 148/1000 | Loss: 0.00004036
Iteration 149/1000 | Loss: 0.00004036
Iteration 150/1000 | Loss: 0.00004035
Iteration 151/1000 | Loss: 0.00004035
Iteration 152/1000 | Loss: 0.00004035
Iteration 153/1000 | Loss: 0.00004035
Iteration 154/1000 | Loss: 0.00004035
Iteration 155/1000 | Loss: 0.00004034
Iteration 156/1000 | Loss: 0.00004034
Iteration 157/1000 | Loss: 0.00004033
Iteration 158/1000 | Loss: 0.00004033
Iteration 159/1000 | Loss: 0.00004033
Iteration 160/1000 | Loss: 0.00004033
Iteration 161/1000 | Loss: 0.00004033
Iteration 162/1000 | Loss: 0.00004033
Iteration 163/1000 | Loss: 0.00004032
Iteration 164/1000 | Loss: 0.00004032
Iteration 165/1000 | Loss: 0.00004032
Iteration 166/1000 | Loss: 0.00004032
Iteration 167/1000 | Loss: 0.00004031
Iteration 168/1000 | Loss: 0.00004031
Iteration 169/1000 | Loss: 0.00004031
Iteration 170/1000 | Loss: 0.00004030
Iteration 171/1000 | Loss: 0.00004030
Iteration 172/1000 | Loss: 0.00004029
Iteration 173/1000 | Loss: 0.00004029
Iteration 174/1000 | Loss: 0.00004029
Iteration 175/1000 | Loss: 0.00004029
Iteration 176/1000 | Loss: 0.00004028
Iteration 177/1000 | Loss: 0.00004028
Iteration 178/1000 | Loss: 0.00004028
Iteration 179/1000 | Loss: 0.00004028
Iteration 180/1000 | Loss: 0.00004028
Iteration 181/1000 | Loss: 0.00004028
Iteration 182/1000 | Loss: 0.00004027
Iteration 183/1000 | Loss: 0.00004026
Iteration 184/1000 | Loss: 0.00004026
Iteration 185/1000 | Loss: 0.00004025
Iteration 186/1000 | Loss: 0.00004024
Iteration 187/1000 | Loss: 0.00004024
Iteration 188/1000 | Loss: 0.00004024
Iteration 189/1000 | Loss: 0.00004023
Iteration 190/1000 | Loss: 0.00004023
Iteration 191/1000 | Loss: 0.00004023
Iteration 192/1000 | Loss: 0.00004022
Iteration 193/1000 | Loss: 0.00004022
Iteration 194/1000 | Loss: 0.00004022
Iteration 195/1000 | Loss: 0.00004022
Iteration 196/1000 | Loss: 0.00004022
Iteration 197/1000 | Loss: 0.00004021
Iteration 198/1000 | Loss: 0.00004021
Iteration 199/1000 | Loss: 0.00004021
Iteration 200/1000 | Loss: 0.00004021
Iteration 201/1000 | Loss: 0.00004021
Iteration 202/1000 | Loss: 0.00004021
Iteration 203/1000 | Loss: 0.00004021
Iteration 204/1000 | Loss: 0.00004021
Iteration 205/1000 | Loss: 0.00004021
Iteration 206/1000 | Loss: 0.00004021
Iteration 207/1000 | Loss: 0.00004020
Iteration 208/1000 | Loss: 0.00004020
Iteration 209/1000 | Loss: 0.00004020
Iteration 210/1000 | Loss: 0.00004020
Iteration 211/1000 | Loss: 0.00004020
Iteration 212/1000 | Loss: 0.00004020
Iteration 213/1000 | Loss: 0.00004020
Iteration 214/1000 | Loss: 0.00004020
Iteration 215/1000 | Loss: 0.00004020
Iteration 216/1000 | Loss: 0.00004020
Iteration 217/1000 | Loss: 0.00004020
Iteration 218/1000 | Loss: 0.00004019
Iteration 219/1000 | Loss: 0.00004019
Iteration 220/1000 | Loss: 0.00004019
Iteration 221/1000 | Loss: 0.00004019
Iteration 222/1000 | Loss: 0.00004019
Iteration 223/1000 | Loss: 0.00004019
Iteration 224/1000 | Loss: 0.00004019
Iteration 225/1000 | Loss: 0.00004019
Iteration 226/1000 | Loss: 0.00004019
Iteration 227/1000 | Loss: 0.00004019
Iteration 228/1000 | Loss: 0.00004019
Iteration 229/1000 | Loss: 0.00004018
Iteration 230/1000 | Loss: 0.00004018
Iteration 231/1000 | Loss: 0.00004018
Iteration 232/1000 | Loss: 0.00004018
Iteration 233/1000 | Loss: 0.00004018
Iteration 234/1000 | Loss: 0.00004018
Iteration 235/1000 | Loss: 0.00004018
Iteration 236/1000 | Loss: 0.00004018
Iteration 237/1000 | Loss: 0.00004018
Iteration 238/1000 | Loss: 0.00004018
Iteration 239/1000 | Loss: 0.00004018
Iteration 240/1000 | Loss: 0.00004018
Iteration 241/1000 | Loss: 0.00004018
Iteration 242/1000 | Loss: 0.00004018
Iteration 243/1000 | Loss: 0.00004018
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 243. Stopping optimization.
Last 5 losses: [4.017825631308369e-05, 4.017825631308369e-05, 4.017825631308369e-05, 4.017825631308369e-05, 4.017825631308369e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.017825631308369e-05

Optimization complete. Final v2v error: 5.26449728012085 mm

Highest mean error: 14.131500244140625 mm for frame 97

Lowest mean error: 4.724184036254883 mm for frame 229

Saving results

Total time: 231.12509083747864
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_56_us_1894/0014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_56_us_1894/0014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_56_us_1894/0014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00934683
Iteration 2/25 | Loss: 0.00232522
Iteration 3/25 | Loss: 0.00220329
Iteration 4/25 | Loss: 0.00217630
Iteration 5/25 | Loss: 0.00216848
Iteration 6/25 | Loss: 0.00216743
Iteration 7/25 | Loss: 0.00216743
Iteration 8/25 | Loss: 0.00216743
Iteration 9/25 | Loss: 0.00216743
Iteration 10/25 | Loss: 0.00216743
Iteration 11/25 | Loss: 0.00216743
Iteration 12/25 | Loss: 0.00216743
Iteration 13/25 | Loss: 0.00216743
Iteration 14/25 | Loss: 0.00216743
Iteration 15/25 | Loss: 0.00216743
Iteration 16/25 | Loss: 0.00216743
Iteration 17/25 | Loss: 0.00216743
Iteration 18/25 | Loss: 0.00216743
Iteration 19/25 | Loss: 0.00216743
Iteration 20/25 | Loss: 0.00216743
Iteration 21/25 | Loss: 0.00216743
Iteration 22/25 | Loss: 0.00216743
Iteration 23/25 | Loss: 0.00216743
Iteration 24/25 | Loss: 0.00216743
Iteration 25/25 | Loss: 0.00216743

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.14974201
Iteration 2/25 | Loss: 0.00252889
Iteration 3/25 | Loss: 0.00252884
Iteration 4/25 | Loss: 0.00252884
Iteration 5/25 | Loss: 0.00252884
Iteration 6/25 | Loss: 0.00252884
Iteration 7/25 | Loss: 0.00252884
Iteration 8/25 | Loss: 0.00252884
Iteration 9/25 | Loss: 0.00252884
Iteration 10/25 | Loss: 0.00252884
Iteration 11/25 | Loss: 0.00252884
Iteration 12/25 | Loss: 0.00252884
Iteration 13/25 | Loss: 0.00252884
Iteration 14/25 | Loss: 0.00252884
Iteration 15/25 | Loss: 0.00252884
Iteration 16/25 | Loss: 0.00252884
Iteration 17/25 | Loss: 0.00252884
Iteration 18/25 | Loss: 0.00252884
Iteration 19/25 | Loss: 0.00252884
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0025288390461355448, 0.0025288390461355448, 0.0025288390461355448, 0.0025288390461355448, 0.0025288390461355448]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0025288390461355448

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00252884
Iteration 2/1000 | Loss: 0.00010103
Iteration 3/1000 | Loss: 0.00006554
Iteration 4/1000 | Loss: 0.00005908
Iteration 5/1000 | Loss: 0.00005476
Iteration 6/1000 | Loss: 0.00005086
Iteration 7/1000 | Loss: 0.00004827
Iteration 8/1000 | Loss: 0.00004628
Iteration 9/1000 | Loss: 0.00004534
Iteration 10/1000 | Loss: 0.00004451
Iteration 11/1000 | Loss: 0.00004383
Iteration 12/1000 | Loss: 0.00004322
Iteration 13/1000 | Loss: 0.00004261
Iteration 14/1000 | Loss: 0.00004214
Iteration 15/1000 | Loss: 0.00004189
Iteration 16/1000 | Loss: 0.00004165
Iteration 17/1000 | Loss: 0.00004148
Iteration 18/1000 | Loss: 0.00004144
Iteration 19/1000 | Loss: 0.00004142
Iteration 20/1000 | Loss: 0.00004141
Iteration 21/1000 | Loss: 0.00004141
Iteration 22/1000 | Loss: 0.00004140
Iteration 23/1000 | Loss: 0.00004140
Iteration 24/1000 | Loss: 0.00004139
Iteration 25/1000 | Loss: 0.00004138
Iteration 26/1000 | Loss: 0.00004138
Iteration 27/1000 | Loss: 0.00004138
Iteration 28/1000 | Loss: 0.00004138
Iteration 29/1000 | Loss: 0.00004137
Iteration 30/1000 | Loss: 0.00004137
Iteration 31/1000 | Loss: 0.00004137
Iteration 32/1000 | Loss: 0.00004137
Iteration 33/1000 | Loss: 0.00004137
Iteration 34/1000 | Loss: 0.00004136
Iteration 35/1000 | Loss: 0.00004136
Iteration 36/1000 | Loss: 0.00004136
Iteration 37/1000 | Loss: 0.00004136
Iteration 38/1000 | Loss: 0.00004135
Iteration 39/1000 | Loss: 0.00004134
Iteration 40/1000 | Loss: 0.00004134
Iteration 41/1000 | Loss: 0.00004134
Iteration 42/1000 | Loss: 0.00004134
Iteration 43/1000 | Loss: 0.00004134
Iteration 44/1000 | Loss: 0.00004134
Iteration 45/1000 | Loss: 0.00004134
Iteration 46/1000 | Loss: 0.00004134
Iteration 47/1000 | Loss: 0.00004134
Iteration 48/1000 | Loss: 0.00004134
Iteration 49/1000 | Loss: 0.00004133
Iteration 50/1000 | Loss: 0.00004132
Iteration 51/1000 | Loss: 0.00004132
Iteration 52/1000 | Loss: 0.00004131
Iteration 53/1000 | Loss: 0.00004131
Iteration 54/1000 | Loss: 0.00004131
Iteration 55/1000 | Loss: 0.00004131
Iteration 56/1000 | Loss: 0.00004131
Iteration 57/1000 | Loss: 0.00004130
Iteration 58/1000 | Loss: 0.00004129
Iteration 59/1000 | Loss: 0.00004129
Iteration 60/1000 | Loss: 0.00004128
Iteration 61/1000 | Loss: 0.00004128
Iteration 62/1000 | Loss: 0.00004128
Iteration 63/1000 | Loss: 0.00004128
Iteration 64/1000 | Loss: 0.00004128
Iteration 65/1000 | Loss: 0.00004128
Iteration 66/1000 | Loss: 0.00004128
Iteration 67/1000 | Loss: 0.00004128
Iteration 68/1000 | Loss: 0.00004128
Iteration 69/1000 | Loss: 0.00004128
Iteration 70/1000 | Loss: 0.00004128
Iteration 71/1000 | Loss: 0.00004128
Iteration 72/1000 | Loss: 0.00004127
Iteration 73/1000 | Loss: 0.00004127
Iteration 74/1000 | Loss: 0.00004127
Iteration 75/1000 | Loss: 0.00004127
Iteration 76/1000 | Loss: 0.00004127
Iteration 77/1000 | Loss: 0.00004126
Iteration 78/1000 | Loss: 0.00004126
Iteration 79/1000 | Loss: 0.00004126
Iteration 80/1000 | Loss: 0.00004126
Iteration 81/1000 | Loss: 0.00004126
Iteration 82/1000 | Loss: 0.00004126
Iteration 83/1000 | Loss: 0.00004126
Iteration 84/1000 | Loss: 0.00004126
Iteration 85/1000 | Loss: 0.00004126
Iteration 86/1000 | Loss: 0.00004126
Iteration 87/1000 | Loss: 0.00004126
Iteration 88/1000 | Loss: 0.00004126
Iteration 89/1000 | Loss: 0.00004126
Iteration 90/1000 | Loss: 0.00004126
Iteration 91/1000 | Loss: 0.00004126
Iteration 92/1000 | Loss: 0.00004126
Iteration 93/1000 | Loss: 0.00004126
Iteration 94/1000 | Loss: 0.00004126
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 94. Stopping optimization.
Last 5 losses: [4.126053681829944e-05, 4.126053681829944e-05, 4.126053681829944e-05, 4.126053681829944e-05, 4.126053681829944e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.126053681829944e-05

Optimization complete. Final v2v error: 5.534501075744629 mm

Highest mean error: 5.981692314147949 mm for frame 7

Lowest mean error: 5.125916004180908 mm for frame 146

Saving results

Total time: 43.194830656051636
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_56_us_1894/0001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_56_us_1894/0001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_56_us_1894/0001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00477406
Iteration 2/25 | Loss: 0.00216559
Iteration 3/25 | Loss: 0.00206327
Iteration 4/25 | Loss: 0.00204929
Iteration 5/25 | Loss: 0.00204462
Iteration 6/25 | Loss: 0.00204362
Iteration 7/25 | Loss: 0.00204362
Iteration 8/25 | Loss: 0.00204362
Iteration 9/25 | Loss: 0.00204362
Iteration 10/25 | Loss: 0.00204362
Iteration 11/25 | Loss: 0.00204362
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.002043616259470582, 0.002043616259470582, 0.002043616259470582, 0.002043616259470582, 0.002043616259470582]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002043616259470582

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.22359943
Iteration 2/25 | Loss: 0.00275415
Iteration 3/25 | Loss: 0.00275415
Iteration 4/25 | Loss: 0.00275415
Iteration 5/25 | Loss: 0.00275415
Iteration 6/25 | Loss: 0.00275414
Iteration 7/25 | Loss: 0.00275414
Iteration 8/25 | Loss: 0.00275414
Iteration 9/25 | Loss: 0.00275414
Iteration 10/25 | Loss: 0.00275414
Iteration 11/25 | Loss: 0.00275414
Iteration 12/25 | Loss: 0.00275414
Iteration 13/25 | Loss: 0.00275414
Iteration 14/25 | Loss: 0.00275414
Iteration 15/25 | Loss: 0.00275414
Iteration 16/25 | Loss: 0.00275414
Iteration 17/25 | Loss: 0.00275414
Iteration 18/25 | Loss: 0.00275414
Iteration 19/25 | Loss: 0.00275414
Iteration 20/25 | Loss: 0.00275414
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.002754143439233303, 0.002754143439233303, 0.002754143439233303, 0.002754143439233303, 0.002754143439233303]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002754143439233303

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00275414
Iteration 2/1000 | Loss: 0.00008624
Iteration 3/1000 | Loss: 0.00005518
Iteration 4/1000 | Loss: 0.00005032
Iteration 5/1000 | Loss: 0.00004602
Iteration 6/1000 | Loss: 0.00004300
Iteration 7/1000 | Loss: 0.00004154
Iteration 8/1000 | Loss: 0.00004073
Iteration 9/1000 | Loss: 0.00003994
Iteration 10/1000 | Loss: 0.00003914
Iteration 11/1000 | Loss: 0.00003847
Iteration 12/1000 | Loss: 0.00003805
Iteration 13/1000 | Loss: 0.00003784
Iteration 14/1000 | Loss: 0.00003777
Iteration 15/1000 | Loss: 0.00003770
Iteration 16/1000 | Loss: 0.00003768
Iteration 17/1000 | Loss: 0.00003768
Iteration 18/1000 | Loss: 0.00003767
Iteration 19/1000 | Loss: 0.00003767
Iteration 20/1000 | Loss: 0.00003764
Iteration 21/1000 | Loss: 0.00003763
Iteration 22/1000 | Loss: 0.00003761
Iteration 23/1000 | Loss: 0.00003760
Iteration 24/1000 | Loss: 0.00003759
Iteration 25/1000 | Loss: 0.00003759
Iteration 26/1000 | Loss: 0.00003759
Iteration 27/1000 | Loss: 0.00003759
Iteration 28/1000 | Loss: 0.00003759
Iteration 29/1000 | Loss: 0.00003758
Iteration 30/1000 | Loss: 0.00003758
Iteration 31/1000 | Loss: 0.00003758
Iteration 32/1000 | Loss: 0.00003758
Iteration 33/1000 | Loss: 0.00003758
Iteration 34/1000 | Loss: 0.00003758
Iteration 35/1000 | Loss: 0.00003758
Iteration 36/1000 | Loss: 0.00003757
Iteration 37/1000 | Loss: 0.00003756
Iteration 38/1000 | Loss: 0.00003756
Iteration 39/1000 | Loss: 0.00003755
Iteration 40/1000 | Loss: 0.00003753
Iteration 41/1000 | Loss: 0.00003753
Iteration 42/1000 | Loss: 0.00003752
Iteration 43/1000 | Loss: 0.00003751
Iteration 44/1000 | Loss: 0.00003751
Iteration 45/1000 | Loss: 0.00003750
Iteration 46/1000 | Loss: 0.00003749
Iteration 47/1000 | Loss: 0.00003748
Iteration 48/1000 | Loss: 0.00003747
Iteration 49/1000 | Loss: 0.00003747
Iteration 50/1000 | Loss: 0.00003747
Iteration 51/1000 | Loss: 0.00003747
Iteration 52/1000 | Loss: 0.00003747
Iteration 53/1000 | Loss: 0.00003747
Iteration 54/1000 | Loss: 0.00003746
Iteration 55/1000 | Loss: 0.00003745
Iteration 56/1000 | Loss: 0.00003745
Iteration 57/1000 | Loss: 0.00003745
Iteration 58/1000 | Loss: 0.00003744
Iteration 59/1000 | Loss: 0.00003744
Iteration 60/1000 | Loss: 0.00003744
Iteration 61/1000 | Loss: 0.00003744
Iteration 62/1000 | Loss: 0.00003743
Iteration 63/1000 | Loss: 0.00003743
Iteration 64/1000 | Loss: 0.00003743
Iteration 65/1000 | Loss: 0.00003743
Iteration 66/1000 | Loss: 0.00003743
Iteration 67/1000 | Loss: 0.00003742
Iteration 68/1000 | Loss: 0.00003742
Iteration 69/1000 | Loss: 0.00003742
Iteration 70/1000 | Loss: 0.00003742
Iteration 71/1000 | Loss: 0.00003742
Iteration 72/1000 | Loss: 0.00003742
Iteration 73/1000 | Loss: 0.00003742
Iteration 74/1000 | Loss: 0.00003741
Iteration 75/1000 | Loss: 0.00003741
Iteration 76/1000 | Loss: 0.00003741
Iteration 77/1000 | Loss: 0.00003741
Iteration 78/1000 | Loss: 0.00003741
Iteration 79/1000 | Loss: 0.00003741
Iteration 80/1000 | Loss: 0.00003741
Iteration 81/1000 | Loss: 0.00003740
Iteration 82/1000 | Loss: 0.00003740
Iteration 83/1000 | Loss: 0.00003740
Iteration 84/1000 | Loss: 0.00003740
Iteration 85/1000 | Loss: 0.00003740
Iteration 86/1000 | Loss: 0.00003740
Iteration 87/1000 | Loss: 0.00003740
Iteration 88/1000 | Loss: 0.00003740
Iteration 89/1000 | Loss: 0.00003740
Iteration 90/1000 | Loss: 0.00003740
Iteration 91/1000 | Loss: 0.00003740
Iteration 92/1000 | Loss: 0.00003740
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 92. Stopping optimization.
Last 5 losses: [3.739978637895547e-05, 3.739978637895547e-05, 3.739978637895547e-05, 3.739978637895547e-05, 3.739978637895547e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.739978637895547e-05

Optimization complete. Final v2v error: 5.2185773849487305 mm

Highest mean error: 5.711306095123291 mm for frame 101

Lowest mean error: 4.7319746017456055 mm for frame 0

Saving results

Total time: 34.60636854171753
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_56_us_1894/0011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_56_us_1894/0011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_56_us_1894/0011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00477200
Iteration 2/25 | Loss: 0.00208574
Iteration 3/25 | Loss: 0.00202339
Iteration 4/25 | Loss: 0.00201592
Iteration 5/25 | Loss: 0.00201262
Iteration 6/25 | Loss: 0.00201213
Iteration 7/25 | Loss: 0.00201213
Iteration 8/25 | Loss: 0.00201213
Iteration 9/25 | Loss: 0.00201213
Iteration 10/25 | Loss: 0.00201213
Iteration 11/25 | Loss: 0.00201213
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0020121338311582804, 0.0020121338311582804, 0.0020121338311582804, 0.0020121338311582804, 0.0020121338311582804]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0020121338311582804

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.80019617
Iteration 2/25 | Loss: 0.00241919
Iteration 3/25 | Loss: 0.00241918
Iteration 4/25 | Loss: 0.00241918
Iteration 5/25 | Loss: 0.00241918
Iteration 6/25 | Loss: 0.00241918
Iteration 7/25 | Loss: 0.00241917
Iteration 8/25 | Loss: 0.00241917
Iteration 9/25 | Loss: 0.00241917
Iteration 10/25 | Loss: 0.00241917
Iteration 11/25 | Loss: 0.00241917
Iteration 12/25 | Loss: 0.00241917
Iteration 13/25 | Loss: 0.00241917
Iteration 14/25 | Loss: 0.00241917
Iteration 15/25 | Loss: 0.00241917
Iteration 16/25 | Loss: 0.00241917
Iteration 17/25 | Loss: 0.00241917
Iteration 18/25 | Loss: 0.00241917
Iteration 19/25 | Loss: 0.00241917
Iteration 20/25 | Loss: 0.00241917
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0024191737174987793, 0.0024191737174987793, 0.0024191737174987793, 0.0024191737174987793, 0.0024191737174987793]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0024191737174987793

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00241917
Iteration 2/1000 | Loss: 0.00008423
Iteration 3/1000 | Loss: 0.00005085
Iteration 4/1000 | Loss: 0.00004495
Iteration 5/1000 | Loss: 0.00004198
Iteration 6/1000 | Loss: 0.00003986
Iteration 7/1000 | Loss: 0.00003873
Iteration 8/1000 | Loss: 0.00003780
Iteration 9/1000 | Loss: 0.00003701
Iteration 10/1000 | Loss: 0.00003660
Iteration 11/1000 | Loss: 0.00003630
Iteration 12/1000 | Loss: 0.00003602
Iteration 13/1000 | Loss: 0.00003584
Iteration 14/1000 | Loss: 0.00003573
Iteration 15/1000 | Loss: 0.00003564
Iteration 16/1000 | Loss: 0.00003558
Iteration 17/1000 | Loss: 0.00003558
Iteration 18/1000 | Loss: 0.00003558
Iteration 19/1000 | Loss: 0.00003558
Iteration 20/1000 | Loss: 0.00003558
Iteration 21/1000 | Loss: 0.00003558
Iteration 22/1000 | Loss: 0.00003557
Iteration 23/1000 | Loss: 0.00003557
Iteration 24/1000 | Loss: 0.00003557
Iteration 25/1000 | Loss: 0.00003557
Iteration 26/1000 | Loss: 0.00003556
Iteration 27/1000 | Loss: 0.00003553
Iteration 28/1000 | Loss: 0.00003553
Iteration 29/1000 | Loss: 0.00003553
Iteration 30/1000 | Loss: 0.00003553
Iteration 31/1000 | Loss: 0.00003553
Iteration 32/1000 | Loss: 0.00003553
Iteration 33/1000 | Loss: 0.00003553
Iteration 34/1000 | Loss: 0.00003552
Iteration 35/1000 | Loss: 0.00003552
Iteration 36/1000 | Loss: 0.00003552
Iteration 37/1000 | Loss: 0.00003552
Iteration 38/1000 | Loss: 0.00003552
Iteration 39/1000 | Loss: 0.00003552
Iteration 40/1000 | Loss: 0.00003551
Iteration 41/1000 | Loss: 0.00003551
Iteration 42/1000 | Loss: 0.00003551
Iteration 43/1000 | Loss: 0.00003550
Iteration 44/1000 | Loss: 0.00003550
Iteration 45/1000 | Loss: 0.00003550
Iteration 46/1000 | Loss: 0.00003550
Iteration 47/1000 | Loss: 0.00003550
Iteration 48/1000 | Loss: 0.00003550
Iteration 49/1000 | Loss: 0.00003549
Iteration 50/1000 | Loss: 0.00003549
Iteration 51/1000 | Loss: 0.00003549
Iteration 52/1000 | Loss: 0.00003549
Iteration 53/1000 | Loss: 0.00003548
Iteration 54/1000 | Loss: 0.00003547
Iteration 55/1000 | Loss: 0.00003547
Iteration 56/1000 | Loss: 0.00003547
Iteration 57/1000 | Loss: 0.00003547
Iteration 58/1000 | Loss: 0.00003547
Iteration 59/1000 | Loss: 0.00003547
Iteration 60/1000 | Loss: 0.00003547
Iteration 61/1000 | Loss: 0.00003547
Iteration 62/1000 | Loss: 0.00003547
Iteration 63/1000 | Loss: 0.00003546
Iteration 64/1000 | Loss: 0.00003546
Iteration 65/1000 | Loss: 0.00003546
Iteration 66/1000 | Loss: 0.00003546
Iteration 67/1000 | Loss: 0.00003546
Iteration 68/1000 | Loss: 0.00003546
Iteration 69/1000 | Loss: 0.00003546
Iteration 70/1000 | Loss: 0.00003546
Iteration 71/1000 | Loss: 0.00003545
Iteration 72/1000 | Loss: 0.00003545
Iteration 73/1000 | Loss: 0.00003544
Iteration 74/1000 | Loss: 0.00003544
Iteration 75/1000 | Loss: 0.00003544
Iteration 76/1000 | Loss: 0.00003544
Iteration 77/1000 | Loss: 0.00003544
Iteration 78/1000 | Loss: 0.00003544
Iteration 79/1000 | Loss: 0.00003543
Iteration 80/1000 | Loss: 0.00003543
Iteration 81/1000 | Loss: 0.00003541
Iteration 82/1000 | Loss: 0.00003541
Iteration 83/1000 | Loss: 0.00003541
Iteration 84/1000 | Loss: 0.00003541
Iteration 85/1000 | Loss: 0.00003541
Iteration 86/1000 | Loss: 0.00003541
Iteration 87/1000 | Loss: 0.00003541
Iteration 88/1000 | Loss: 0.00003540
Iteration 89/1000 | Loss: 0.00003540
Iteration 90/1000 | Loss: 0.00003540
Iteration 91/1000 | Loss: 0.00003539
Iteration 92/1000 | Loss: 0.00003538
Iteration 93/1000 | Loss: 0.00003537
Iteration 94/1000 | Loss: 0.00003537
Iteration 95/1000 | Loss: 0.00003537
Iteration 96/1000 | Loss: 0.00003536
Iteration 97/1000 | Loss: 0.00003536
Iteration 98/1000 | Loss: 0.00003536
Iteration 99/1000 | Loss: 0.00003536
Iteration 100/1000 | Loss: 0.00003535
Iteration 101/1000 | Loss: 0.00003535
Iteration 102/1000 | Loss: 0.00003535
Iteration 103/1000 | Loss: 0.00003534
Iteration 104/1000 | Loss: 0.00003534
Iteration 105/1000 | Loss: 0.00003534
Iteration 106/1000 | Loss: 0.00003534
Iteration 107/1000 | Loss: 0.00003534
Iteration 108/1000 | Loss: 0.00003534
Iteration 109/1000 | Loss: 0.00003534
Iteration 110/1000 | Loss: 0.00003534
Iteration 111/1000 | Loss: 0.00003534
Iteration 112/1000 | Loss: 0.00003534
Iteration 113/1000 | Loss: 0.00003533
Iteration 114/1000 | Loss: 0.00003533
Iteration 115/1000 | Loss: 0.00003533
Iteration 116/1000 | Loss: 0.00003533
Iteration 117/1000 | Loss: 0.00003533
Iteration 118/1000 | Loss: 0.00003533
Iteration 119/1000 | Loss: 0.00003533
Iteration 120/1000 | Loss: 0.00003533
Iteration 121/1000 | Loss: 0.00003533
Iteration 122/1000 | Loss: 0.00003533
Iteration 123/1000 | Loss: 0.00003533
Iteration 124/1000 | Loss: 0.00003533
Iteration 125/1000 | Loss: 0.00003533
Iteration 126/1000 | Loss: 0.00003533
Iteration 127/1000 | Loss: 0.00003533
Iteration 128/1000 | Loss: 0.00003532
Iteration 129/1000 | Loss: 0.00003532
Iteration 130/1000 | Loss: 0.00003532
Iteration 131/1000 | Loss: 0.00003532
Iteration 132/1000 | Loss: 0.00003532
Iteration 133/1000 | Loss: 0.00003532
Iteration 134/1000 | Loss: 0.00003532
Iteration 135/1000 | Loss: 0.00003532
Iteration 136/1000 | Loss: 0.00003532
Iteration 137/1000 | Loss: 0.00003532
Iteration 138/1000 | Loss: 0.00003532
Iteration 139/1000 | Loss: 0.00003532
Iteration 140/1000 | Loss: 0.00003532
Iteration 141/1000 | Loss: 0.00003532
Iteration 142/1000 | Loss: 0.00003532
Iteration 143/1000 | Loss: 0.00003532
Iteration 144/1000 | Loss: 0.00003532
Iteration 145/1000 | Loss: 0.00003532
Iteration 146/1000 | Loss: 0.00003532
Iteration 147/1000 | Loss: 0.00003532
Iteration 148/1000 | Loss: 0.00003532
Iteration 149/1000 | Loss: 0.00003532
Iteration 150/1000 | Loss: 0.00003532
Iteration 151/1000 | Loss: 0.00003532
Iteration 152/1000 | Loss: 0.00003532
Iteration 153/1000 | Loss: 0.00003532
Iteration 154/1000 | Loss: 0.00003532
Iteration 155/1000 | Loss: 0.00003532
Iteration 156/1000 | Loss: 0.00003532
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 156. Stopping optimization.
Last 5 losses: [3.531832408043556e-05, 3.531832408043556e-05, 3.531832408043556e-05, 3.531832408043556e-05, 3.531832408043556e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.531832408043556e-05

Optimization complete. Final v2v error: 5.060919284820557 mm

Highest mean error: 5.313551425933838 mm for frame 76

Lowest mean error: 4.880256652832031 mm for frame 151

Saving results

Total time: 38.58926749229431
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_56_us_1894/0000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_56_us_1894/0000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_56_us_1894/0000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00959923
Iteration 2/25 | Loss: 0.00227175
Iteration 3/25 | Loss: 0.00208882
Iteration 4/25 | Loss: 0.00206281
Iteration 5/25 | Loss: 0.00206068
Iteration 6/25 | Loss: 0.00206180
Iteration 7/25 | Loss: 0.00205233
Iteration 8/25 | Loss: 0.00205009
Iteration 9/25 | Loss: 0.00204956
Iteration 10/25 | Loss: 0.00204905
Iteration 11/25 | Loss: 0.00204878
Iteration 12/25 | Loss: 0.00204872
Iteration 13/25 | Loss: 0.00204871
Iteration 14/25 | Loss: 0.00204871
Iteration 15/25 | Loss: 0.00204871
Iteration 16/25 | Loss: 0.00204870
Iteration 17/25 | Loss: 0.00204870
Iteration 18/25 | Loss: 0.00204870
Iteration 19/25 | Loss: 0.00204870
Iteration 20/25 | Loss: 0.00204870
Iteration 21/25 | Loss: 0.00204870
Iteration 22/25 | Loss: 0.00204870
Iteration 23/25 | Loss: 0.00204869
Iteration 24/25 | Loss: 0.00204869
Iteration 25/25 | Loss: 0.00204869

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.66764641
Iteration 2/25 | Loss: 0.00225644
Iteration 3/25 | Loss: 0.00225644
Iteration 4/25 | Loss: 0.00225644
Iteration 5/25 | Loss: 0.00225644
Iteration 6/25 | Loss: 0.00225644
Iteration 7/25 | Loss: 0.00225644
Iteration 8/25 | Loss: 0.00225644
Iteration 9/25 | Loss: 0.00225644
Iteration 10/25 | Loss: 0.00225644
Iteration 11/25 | Loss: 0.00225644
Iteration 12/25 | Loss: 0.00225644
Iteration 13/25 | Loss: 0.00225644
Iteration 14/25 | Loss: 0.00225644
Iteration 15/25 | Loss: 0.00225643
Iteration 16/25 | Loss: 0.00225644
Iteration 17/25 | Loss: 0.00225644
Iteration 18/25 | Loss: 0.00225644
Iteration 19/25 | Loss: 0.00225644
Iteration 20/25 | Loss: 0.00225644
Iteration 21/25 | Loss: 0.00225643
Iteration 22/25 | Loss: 0.00225644
Iteration 23/25 | Loss: 0.00225644
Iteration 24/25 | Loss: 0.00225644
Iteration 25/25 | Loss: 0.00225644

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00225644
Iteration 2/1000 | Loss: 0.00011158
Iteration 3/1000 | Loss: 0.00006373
Iteration 4/1000 | Loss: 0.00005432
Iteration 5/1000 | Loss: 0.00004871
Iteration 6/1000 | Loss: 0.00004545
Iteration 7/1000 | Loss: 0.00004406
Iteration 8/1000 | Loss: 0.00004269
Iteration 9/1000 | Loss: 0.00034347
Iteration 10/1000 | Loss: 0.00005098
Iteration 11/1000 | Loss: 0.00004259
Iteration 12/1000 | Loss: 0.00004014
Iteration 13/1000 | Loss: 0.00003901
Iteration 14/1000 | Loss: 0.00003801
Iteration 15/1000 | Loss: 0.00003747
Iteration 16/1000 | Loss: 0.00003704
Iteration 17/1000 | Loss: 0.00003680
Iteration 18/1000 | Loss: 0.00003655
Iteration 19/1000 | Loss: 0.00003635
Iteration 20/1000 | Loss: 0.00003616
Iteration 21/1000 | Loss: 0.00003608
Iteration 22/1000 | Loss: 0.00003603
Iteration 23/1000 | Loss: 0.00003602
Iteration 24/1000 | Loss: 0.00003601
Iteration 25/1000 | Loss: 0.00003600
Iteration 26/1000 | Loss: 0.00003598
Iteration 27/1000 | Loss: 0.00003597
Iteration 28/1000 | Loss: 0.00003597
Iteration 29/1000 | Loss: 0.00003597
Iteration 30/1000 | Loss: 0.00003597
Iteration 31/1000 | Loss: 0.00003597
Iteration 32/1000 | Loss: 0.00003597
Iteration 33/1000 | Loss: 0.00003597
Iteration 34/1000 | Loss: 0.00003596
Iteration 35/1000 | Loss: 0.00003596
Iteration 36/1000 | Loss: 0.00003595
Iteration 37/1000 | Loss: 0.00003595
Iteration 38/1000 | Loss: 0.00003595
Iteration 39/1000 | Loss: 0.00003594
Iteration 40/1000 | Loss: 0.00003594
Iteration 41/1000 | Loss: 0.00003594
Iteration 42/1000 | Loss: 0.00003593
Iteration 43/1000 | Loss: 0.00003593
Iteration 44/1000 | Loss: 0.00003593
Iteration 45/1000 | Loss: 0.00003593
Iteration 46/1000 | Loss: 0.00003592
Iteration 47/1000 | Loss: 0.00003592
Iteration 48/1000 | Loss: 0.00003592
Iteration 49/1000 | Loss: 0.00003592
Iteration 50/1000 | Loss: 0.00003591
Iteration 51/1000 | Loss: 0.00003591
Iteration 52/1000 | Loss: 0.00003591
Iteration 53/1000 | Loss: 0.00003591
Iteration 54/1000 | Loss: 0.00003591
Iteration 55/1000 | Loss: 0.00003590
Iteration 56/1000 | Loss: 0.00003590
Iteration 57/1000 | Loss: 0.00003590
Iteration 58/1000 | Loss: 0.00003590
Iteration 59/1000 | Loss: 0.00003590
Iteration 60/1000 | Loss: 0.00003590
Iteration 61/1000 | Loss: 0.00003590
Iteration 62/1000 | Loss: 0.00003590
Iteration 63/1000 | Loss: 0.00003589
Iteration 64/1000 | Loss: 0.00003589
Iteration 65/1000 | Loss: 0.00003589
Iteration 66/1000 | Loss: 0.00003589
Iteration 67/1000 | Loss: 0.00003588
Iteration 68/1000 | Loss: 0.00003588
Iteration 69/1000 | Loss: 0.00003588
Iteration 70/1000 | Loss: 0.00003588
Iteration 71/1000 | Loss: 0.00003588
Iteration 72/1000 | Loss: 0.00003587
Iteration 73/1000 | Loss: 0.00003587
Iteration 74/1000 | Loss: 0.00003587
Iteration 75/1000 | Loss: 0.00003587
Iteration 76/1000 | Loss: 0.00003587
Iteration 77/1000 | Loss: 0.00003587
Iteration 78/1000 | Loss: 0.00003586
Iteration 79/1000 | Loss: 0.00003586
Iteration 80/1000 | Loss: 0.00003586
Iteration 81/1000 | Loss: 0.00003586
Iteration 82/1000 | Loss: 0.00003586
Iteration 83/1000 | Loss: 0.00003586
Iteration 84/1000 | Loss: 0.00003586
Iteration 85/1000 | Loss: 0.00003586
Iteration 86/1000 | Loss: 0.00003586
Iteration 87/1000 | Loss: 0.00003586
Iteration 88/1000 | Loss: 0.00003586
Iteration 89/1000 | Loss: 0.00003585
Iteration 90/1000 | Loss: 0.00003585
Iteration 91/1000 | Loss: 0.00003585
Iteration 92/1000 | Loss: 0.00003585
Iteration 93/1000 | Loss: 0.00003585
Iteration 94/1000 | Loss: 0.00003585
Iteration 95/1000 | Loss: 0.00003585
Iteration 96/1000 | Loss: 0.00003584
Iteration 97/1000 | Loss: 0.00003584
Iteration 98/1000 | Loss: 0.00003584
Iteration 99/1000 | Loss: 0.00003584
Iteration 100/1000 | Loss: 0.00003584
Iteration 101/1000 | Loss: 0.00003584
Iteration 102/1000 | Loss: 0.00003584
Iteration 103/1000 | Loss: 0.00003584
Iteration 104/1000 | Loss: 0.00003583
Iteration 105/1000 | Loss: 0.00003583
Iteration 106/1000 | Loss: 0.00003583
Iteration 107/1000 | Loss: 0.00003583
Iteration 108/1000 | Loss: 0.00003583
Iteration 109/1000 | Loss: 0.00003583
Iteration 110/1000 | Loss: 0.00003583
Iteration 111/1000 | Loss: 0.00003583
Iteration 112/1000 | Loss: 0.00003583
Iteration 113/1000 | Loss: 0.00003582
Iteration 114/1000 | Loss: 0.00003582
Iteration 115/1000 | Loss: 0.00003582
Iteration 116/1000 | Loss: 0.00003582
Iteration 117/1000 | Loss: 0.00003582
Iteration 118/1000 | Loss: 0.00003582
Iteration 119/1000 | Loss: 0.00003582
Iteration 120/1000 | Loss: 0.00003582
Iteration 121/1000 | Loss: 0.00003582
Iteration 122/1000 | Loss: 0.00003582
Iteration 123/1000 | Loss: 0.00003582
Iteration 124/1000 | Loss: 0.00003582
Iteration 125/1000 | Loss: 0.00003582
Iteration 126/1000 | Loss: 0.00003582
Iteration 127/1000 | Loss: 0.00003582
Iteration 128/1000 | Loss: 0.00003582
Iteration 129/1000 | Loss: 0.00003581
Iteration 130/1000 | Loss: 0.00003581
Iteration 131/1000 | Loss: 0.00003581
Iteration 132/1000 | Loss: 0.00003581
Iteration 133/1000 | Loss: 0.00003581
Iteration 134/1000 | Loss: 0.00003581
Iteration 135/1000 | Loss: 0.00003581
Iteration 136/1000 | Loss: 0.00003581
Iteration 137/1000 | Loss: 0.00003580
Iteration 138/1000 | Loss: 0.00003580
Iteration 139/1000 | Loss: 0.00003580
Iteration 140/1000 | Loss: 0.00003580
Iteration 141/1000 | Loss: 0.00003580
Iteration 142/1000 | Loss: 0.00003580
Iteration 143/1000 | Loss: 0.00003580
Iteration 144/1000 | Loss: 0.00003580
Iteration 145/1000 | Loss: 0.00003580
Iteration 146/1000 | Loss: 0.00003580
Iteration 147/1000 | Loss: 0.00003580
Iteration 148/1000 | Loss: 0.00003580
Iteration 149/1000 | Loss: 0.00003580
Iteration 150/1000 | Loss: 0.00003580
Iteration 151/1000 | Loss: 0.00003580
Iteration 152/1000 | Loss: 0.00003580
Iteration 153/1000 | Loss: 0.00003580
Iteration 154/1000 | Loss: 0.00003580
Iteration 155/1000 | Loss: 0.00003580
Iteration 156/1000 | Loss: 0.00003580
Iteration 157/1000 | Loss: 0.00003580
Iteration 158/1000 | Loss: 0.00003580
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 158. Stopping optimization.
Last 5 losses: [3.579912299755961e-05, 3.579912299755961e-05, 3.579912299755961e-05, 3.579912299755961e-05, 3.579912299755961e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.579912299755961e-05

Optimization complete. Final v2v error: 5.114802360534668 mm

Highest mean error: 6.07127571105957 mm for frame 192

Lowest mean error: 4.681822776794434 mm for frame 8

Saving results

Total time: 66.57140445709229
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_56_us_1894/0002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_56_us_1894/0002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_56_us_1894/0002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01161606
Iteration 2/25 | Loss: 0.00387994
Iteration 3/25 | Loss: 0.00336281
Iteration 4/25 | Loss: 0.00306419
Iteration 5/25 | Loss: 0.00289317
Iteration 6/25 | Loss: 0.00262580
Iteration 7/25 | Loss: 0.00240991
Iteration 8/25 | Loss: 0.00235853
Iteration 9/25 | Loss: 0.00229622
Iteration 10/25 | Loss: 0.00225466
Iteration 11/25 | Loss: 0.00223268
Iteration 12/25 | Loss: 0.00222624
Iteration 13/25 | Loss: 0.00222394
Iteration 14/25 | Loss: 0.00222353
Iteration 15/25 | Loss: 0.00222429
Iteration 16/25 | Loss: 0.00222458
Iteration 17/25 | Loss: 0.00222422
Iteration 18/25 | Loss: 0.00222447
Iteration 19/25 | Loss: 0.00222383
Iteration 20/25 | Loss: 0.00222403
Iteration 21/25 | Loss: 0.00222417
Iteration 22/25 | Loss: 0.00222410
Iteration 23/25 | Loss: 0.00222183
Iteration 24/25 | Loss: 0.00222314
Iteration 25/25 | Loss: 0.00222118

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.23591781
Iteration 2/25 | Loss: 0.00512919
Iteration 3/25 | Loss: 0.00512916
Iteration 4/25 | Loss: 0.00512916
Iteration 5/25 | Loss: 0.00512916
Iteration 6/25 | Loss: 0.00512916
Iteration 7/25 | Loss: 0.00512916
Iteration 8/25 | Loss: 0.00512916
Iteration 9/25 | Loss: 0.00512916
Iteration 10/25 | Loss: 0.00512916
Iteration 11/25 | Loss: 0.00512916
Iteration 12/25 | Loss: 0.00512916
Iteration 13/25 | Loss: 0.00512916
Iteration 14/25 | Loss: 0.00512916
Iteration 15/25 | Loss: 0.00512916
Iteration 16/25 | Loss: 0.00512916
Iteration 17/25 | Loss: 0.00512916
Iteration 18/25 | Loss: 0.00512916
Iteration 19/25 | Loss: 0.00512916
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.005129158962517977, 0.005129158962517977, 0.005129158962517977, 0.005129158962517977, 0.005129158962517977]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.005129158962517977

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00512916
Iteration 2/1000 | Loss: 0.00061703
Iteration 3/1000 | Loss: 0.00138605
Iteration 4/1000 | Loss: 0.00468103
Iteration 5/1000 | Loss: 0.00109578
Iteration 6/1000 | Loss: 0.00044801
Iteration 7/1000 | Loss: 0.00037080
Iteration 8/1000 | Loss: 0.00031456
Iteration 9/1000 | Loss: 0.00029070
Iteration 10/1000 | Loss: 0.00023151
Iteration 11/1000 | Loss: 0.00021038
Iteration 12/1000 | Loss: 0.00019908
Iteration 13/1000 | Loss: 0.00026138
Iteration 14/1000 | Loss: 0.00020342
Iteration 15/1000 | Loss: 0.00018353
Iteration 16/1000 | Loss: 0.00018727
Iteration 17/1000 | Loss: 0.00019236
Iteration 18/1000 | Loss: 0.00017289
Iteration 19/1000 | Loss: 0.00019036
Iteration 20/1000 | Loss: 0.00016607
Iteration 21/1000 | Loss: 0.00019939
Iteration 22/1000 | Loss: 0.00016991
Iteration 23/1000 | Loss: 0.00016836
Iteration 24/1000 | Loss: 0.00016044
Iteration 25/1000 | Loss: 0.00015859
Iteration 26/1000 | Loss: 0.00017918
Iteration 27/1000 | Loss: 0.00016875
Iteration 28/1000 | Loss: 0.00018174
Iteration 29/1000 | Loss: 0.00015736
Iteration 30/1000 | Loss: 0.00015405
Iteration 31/1000 | Loss: 0.00017674
Iteration 32/1000 | Loss: 0.00016127
Iteration 33/1000 | Loss: 0.00016078
Iteration 34/1000 | Loss: 0.00015376
Iteration 35/1000 | Loss: 0.00015389
Iteration 36/1000 | Loss: 0.00015466
Iteration 37/1000 | Loss: 0.00015216
Iteration 38/1000 | Loss: 0.00015685
Iteration 39/1000 | Loss: 0.00015536
Iteration 40/1000 | Loss: 0.00015933
Iteration 41/1000 | Loss: 0.00015635
Iteration 42/1000 | Loss: 0.00015909
Iteration 43/1000 | Loss: 0.00015565
Iteration 44/1000 | Loss: 0.00016243
Iteration 45/1000 | Loss: 0.00016893
Iteration 46/1000 | Loss: 0.00015797
Iteration 47/1000 | Loss: 0.00015124
Iteration 48/1000 | Loss: 0.00015966
Iteration 49/1000 | Loss: 0.00015516
Iteration 50/1000 | Loss: 0.00016783
Iteration 51/1000 | Loss: 0.00015798
Iteration 52/1000 | Loss: 0.00018018
Iteration 53/1000 | Loss: 0.00016316
Iteration 54/1000 | Loss: 0.00015142
Iteration 55/1000 | Loss: 0.00015090
Iteration 56/1000 | Loss: 0.00015393
Iteration 57/1000 | Loss: 0.00016325
Iteration 58/1000 | Loss: 0.00016916
Iteration 59/1000 | Loss: 0.00015840
Iteration 60/1000 | Loss: 0.00016797
Iteration 61/1000 | Loss: 0.00016076
Iteration 62/1000 | Loss: 0.00017217
Iteration 63/1000 | Loss: 0.00016834
Iteration 64/1000 | Loss: 0.00015456
Iteration 65/1000 | Loss: 0.00015070
Iteration 66/1000 | Loss: 0.00015795
Iteration 67/1000 | Loss: 0.00015066
Iteration 68/1000 | Loss: 0.00015119
Iteration 69/1000 | Loss: 0.00015007
Iteration 70/1000 | Loss: 0.00014991
Iteration 71/1000 | Loss: 0.00015126
Iteration 72/1000 | Loss: 0.00015309
Iteration 73/1000 | Loss: 0.00015242
Iteration 74/1000 | Loss: 0.00015363
Iteration 75/1000 | Loss: 0.00015297
Iteration 76/1000 | Loss: 0.00015649
Iteration 77/1000 | Loss: 0.00015184
Iteration 78/1000 | Loss: 0.00016979
Iteration 79/1000 | Loss: 0.00015420
Iteration 80/1000 | Loss: 0.00017323
Iteration 81/1000 | Loss: 0.00015494
Iteration 82/1000 | Loss: 0.00017373
Iteration 83/1000 | Loss: 0.00015808
Iteration 84/1000 | Loss: 0.00017369
Iteration 85/1000 | Loss: 0.00018368
Iteration 86/1000 | Loss: 0.00018659
Iteration 87/1000 | Loss: 0.00017745
Iteration 88/1000 | Loss: 0.00017228
Iteration 89/1000 | Loss: 0.00015469
Iteration 90/1000 | Loss: 0.00015153
Iteration 91/1000 | Loss: 0.00015044
Iteration 92/1000 | Loss: 0.00015508
Iteration 93/1000 | Loss: 0.00015193
Iteration 94/1000 | Loss: 0.00015027
Iteration 95/1000 | Loss: 0.00015447
Iteration 96/1000 | Loss: 0.00015485
Iteration 97/1000 | Loss: 0.00015100
Iteration 98/1000 | Loss: 0.00015123
Iteration 99/1000 | Loss: 0.00015299
Iteration 100/1000 | Loss: 0.00015079
Iteration 101/1000 | Loss: 0.00015077
Iteration 102/1000 | Loss: 0.00015037
Iteration 103/1000 | Loss: 0.00015691
Iteration 104/1000 | Loss: 0.00015137
Iteration 105/1000 | Loss: 0.00015918
Iteration 106/1000 | Loss: 0.00015224
Iteration 107/1000 | Loss: 0.00015502
Iteration 108/1000 | Loss: 0.00015564
Iteration 109/1000 | Loss: 0.00015280
Iteration 110/1000 | Loss: 0.00016037
Iteration 111/1000 | Loss: 0.00015070
Iteration 112/1000 | Loss: 0.00015921
Iteration 113/1000 | Loss: 0.00015169
Iteration 114/1000 | Loss: 0.00015362
Iteration 115/1000 | Loss: 0.00016488
Iteration 116/1000 | Loss: 0.00015816
Iteration 117/1000 | Loss: 0.00017830
Iteration 118/1000 | Loss: 0.00018970
Iteration 119/1000 | Loss: 0.00018323
Iteration 120/1000 | Loss: 0.00015618
Iteration 121/1000 | Loss: 0.00016160
Iteration 122/1000 | Loss: 0.00017248
Iteration 123/1000 | Loss: 0.00015223
Iteration 124/1000 | Loss: 0.00015153
Iteration 125/1000 | Loss: 0.00016109
Iteration 126/1000 | Loss: 0.00016060
Iteration 127/1000 | Loss: 0.00016558
Iteration 128/1000 | Loss: 0.00016399
Iteration 129/1000 | Loss: 0.00016148
Iteration 130/1000 | Loss: 0.00016335
Iteration 131/1000 | Loss: 0.00015774
Iteration 132/1000 | Loss: 0.00015336
Iteration 133/1000 | Loss: 0.00016112
Iteration 134/1000 | Loss: 0.00015974
Iteration 135/1000 | Loss: 0.00016466
Iteration 136/1000 | Loss: 0.00016812
Iteration 137/1000 | Loss: 0.00016373
Iteration 138/1000 | Loss: 0.00017348
Iteration 139/1000 | Loss: 0.00016259
Iteration 140/1000 | Loss: 0.00017521
Iteration 141/1000 | Loss: 0.00015851
Iteration 142/1000 | Loss: 0.00015378
Iteration 143/1000 | Loss: 0.00016252
Iteration 144/1000 | Loss: 0.00017104
Iteration 145/1000 | Loss: 0.00016194
Iteration 146/1000 | Loss: 0.00016894
Iteration 147/1000 | Loss: 0.00016179
Iteration 148/1000 | Loss: 0.00016987
Iteration 149/1000 | Loss: 0.00015470
Iteration 150/1000 | Loss: 0.00016403
Iteration 151/1000 | Loss: 0.00015470
Iteration 152/1000 | Loss: 0.00015326
Iteration 153/1000 | Loss: 0.00015042
Iteration 154/1000 | Loss: 0.00015017
Iteration 155/1000 | Loss: 0.00014997
Iteration 156/1000 | Loss: 0.00015044
Iteration 157/1000 | Loss: 0.00015036
Iteration 158/1000 | Loss: 0.00015024
Iteration 159/1000 | Loss: 0.00016676
Iteration 160/1000 | Loss: 0.00015198
Iteration 161/1000 | Loss: 0.00015065
Iteration 162/1000 | Loss: 0.00015047
Iteration 163/1000 | Loss: 0.00017327
Iteration 164/1000 | Loss: 0.00015472
Iteration 165/1000 | Loss: 0.00015611
Iteration 166/1000 | Loss: 0.00015211
Iteration 167/1000 | Loss: 0.00015038
Iteration 168/1000 | Loss: 0.00016940
Iteration 169/1000 | Loss: 0.00015065
Iteration 170/1000 | Loss: 0.00017208
Iteration 171/1000 | Loss: 0.00015152
Iteration 172/1000 | Loss: 0.00015028
Iteration 173/1000 | Loss: 0.00017368
Iteration 174/1000 | Loss: 0.00015140
Iteration 175/1000 | Loss: 0.00017606
Iteration 176/1000 | Loss: 0.00015142
Iteration 177/1000 | Loss: 0.00017537
Iteration 178/1000 | Loss: 0.00015153
Iteration 179/1000 | Loss: 0.00016393
Iteration 180/1000 | Loss: 0.00015233
Iteration 181/1000 | Loss: 0.00015111
Iteration 182/1000 | Loss: 0.00015045
Iteration 183/1000 | Loss: 0.00016972
Iteration 184/1000 | Loss: 0.00015396
Iteration 185/1000 | Loss: 0.00019437
Iteration 186/1000 | Loss: 0.00016963
Iteration 187/1000 | Loss: 0.00015959
Iteration 188/1000 | Loss: 0.00015226
Iteration 189/1000 | Loss: 0.00015105
Iteration 190/1000 | Loss: 0.00015418
Iteration 191/1000 | Loss: 0.00015135
Iteration 192/1000 | Loss: 0.00016359
Iteration 193/1000 | Loss: 0.00015970
Iteration 194/1000 | Loss: 0.00016480
Iteration 195/1000 | Loss: 0.00017064
Iteration 196/1000 | Loss: 0.00016731
Iteration 197/1000 | Loss: 0.00016015
Iteration 198/1000 | Loss: 0.00016518
Iteration 199/1000 | Loss: 0.00015565
Iteration 200/1000 | Loss: 0.00016028
Iteration 201/1000 | Loss: 0.00017196
Iteration 202/1000 | Loss: 0.00016193
Iteration 203/1000 | Loss: 0.00015956
Iteration 204/1000 | Loss: 0.00016227
Iteration 205/1000 | Loss: 0.00016019
Iteration 206/1000 | Loss: 0.00016570
Iteration 207/1000 | Loss: 0.00015839
Iteration 208/1000 | Loss: 0.00017925
Iteration 209/1000 | Loss: 0.00015373
Iteration 210/1000 | Loss: 0.00016366
Iteration 211/1000 | Loss: 0.00016231
Iteration 212/1000 | Loss: 0.00017182
Iteration 213/1000 | Loss: 0.00015684
Iteration 214/1000 | Loss: 0.00017543
Iteration 215/1000 | Loss: 0.00016082
Iteration 216/1000 | Loss: 0.00017337
Iteration 217/1000 | Loss: 0.00016125
Iteration 218/1000 | Loss: 0.00017449
Iteration 219/1000 | Loss: 0.00015933
Iteration 220/1000 | Loss: 0.00017941
Iteration 221/1000 | Loss: 0.00016087
Iteration 222/1000 | Loss: 0.00019619
Iteration 223/1000 | Loss: 0.00016315
Iteration 224/1000 | Loss: 0.00017943
Iteration 225/1000 | Loss: 0.00016961
Iteration 226/1000 | Loss: 0.00018136
Iteration 227/1000 | Loss: 0.00016539
Iteration 228/1000 | Loss: 0.00018749
Iteration 229/1000 | Loss: 0.00016554
Iteration 230/1000 | Loss: 0.00017511
Iteration 231/1000 | Loss: 0.00017140
Iteration 232/1000 | Loss: 0.00018779
Iteration 233/1000 | Loss: 0.00016100
Iteration 234/1000 | Loss: 0.00017148
Iteration 235/1000 | Loss: 0.00015412
Iteration 236/1000 | Loss: 0.00015700
Iteration 237/1000 | Loss: 0.00015565
Iteration 238/1000 | Loss: 0.00015133
Iteration 239/1000 | Loss: 0.00016681
Iteration 240/1000 | Loss: 0.00015684
Iteration 241/1000 | Loss: 0.00017355
Iteration 242/1000 | Loss: 0.00015940
Iteration 243/1000 | Loss: 0.00016734
Iteration 244/1000 | Loss: 0.00015965
Iteration 245/1000 | Loss: 0.00016313
Iteration 246/1000 | Loss: 0.00015126
Iteration 247/1000 | Loss: 0.00015418
Iteration 248/1000 | Loss: 0.00015090
Iteration 249/1000 | Loss: 0.00016194
Iteration 250/1000 | Loss: 0.00017143
Iteration 251/1000 | Loss: 0.00016347
Iteration 252/1000 | Loss: 0.00015407
Iteration 253/1000 | Loss: 0.00015670
Iteration 254/1000 | Loss: 0.00016241
Iteration 255/1000 | Loss: 0.00015801
Iteration 256/1000 | Loss: 0.00015087
Iteration 257/1000 | Loss: 0.00016149
Iteration 258/1000 | Loss: 0.00015413
Iteration 259/1000 | Loss: 0.00015613
Iteration 260/1000 | Loss: 0.00016680
Iteration 261/1000 | Loss: 0.00015616
Iteration 262/1000 | Loss: 0.00015655
Iteration 263/1000 | Loss: 0.00016642
Iteration 264/1000 | Loss: 0.00015721
Iteration 265/1000 | Loss: 0.00015712
Iteration 266/1000 | Loss: 0.00015820
Iteration 267/1000 | Loss: 0.00016385
Iteration 268/1000 | Loss: 0.00016019
Iteration 269/1000 | Loss: 0.00016197
Iteration 270/1000 | Loss: 0.00015791
Iteration 271/1000 | Loss: 0.00016128
Iteration 272/1000 | Loss: 0.00015752
Iteration 273/1000 | Loss: 0.00016755
Iteration 274/1000 | Loss: 0.00015580
Iteration 275/1000 | Loss: 0.00016317
Iteration 276/1000 | Loss: 0.00016199
Iteration 277/1000 | Loss: 0.00016223
Iteration 278/1000 | Loss: 0.00015428
Iteration 279/1000 | Loss: 0.00016248
Iteration 280/1000 | Loss: 0.00016101
Iteration 281/1000 | Loss: 0.00015225
Iteration 282/1000 | Loss: 0.00016072
Iteration 283/1000 | Loss: 0.00016797
Iteration 284/1000 | Loss: 0.00015311
Iteration 285/1000 | Loss: 0.00015851
Iteration 286/1000 | Loss: 0.00015254
Iteration 287/1000 | Loss: 0.00015086
Iteration 288/1000 | Loss: 0.00015007
Iteration 289/1000 | Loss: 0.00016638
Iteration 290/1000 | Loss: 0.00015648
Iteration 291/1000 | Loss: 0.00015163
Iteration 292/1000 | Loss: 0.00015317
Iteration 293/1000 | Loss: 0.00015150
Iteration 294/1000 | Loss: 0.00015021
Iteration 295/1000 | Loss: 0.00015021
Iteration 296/1000 | Loss: 0.00015021
Iteration 297/1000 | Loss: 0.00015021
Iteration 298/1000 | Loss: 0.00015020
Iteration 299/1000 | Loss: 0.00015020
Iteration 300/1000 | Loss: 0.00015020
Iteration 301/1000 | Loss: 0.00015020
Iteration 302/1000 | Loss: 0.00015020
Iteration 303/1000 | Loss: 0.00015020
Iteration 304/1000 | Loss: 0.00015020
Iteration 305/1000 | Loss: 0.00015020
Iteration 306/1000 | Loss: 0.00015020
Iteration 307/1000 | Loss: 0.00015020
Iteration 308/1000 | Loss: 0.00015020
Iteration 309/1000 | Loss: 0.00015018
Iteration 310/1000 | Loss: 0.00016746
Iteration 311/1000 | Loss: 0.00015355
Iteration 312/1000 | Loss: 0.00016554
Iteration 313/1000 | Loss: 0.00016432
Iteration 314/1000 | Loss: 0.00015106
Iteration 315/1000 | Loss: 0.00015057
Iteration 316/1000 | Loss: 0.00016524
Iteration 317/1000 | Loss: 0.00015111
Iteration 318/1000 | Loss: 0.00017652
Iteration 319/1000 | Loss: 0.00015199
Iteration 320/1000 | Loss: 0.00015024
Iteration 321/1000 | Loss: 0.00017519
Iteration 322/1000 | Loss: 0.00015327
Iteration 323/1000 | Loss: 0.00015150
Iteration 324/1000 | Loss: 0.00016601
Iteration 325/1000 | Loss: 0.00015074
Iteration 326/1000 | Loss: 0.00015034
Iteration 327/1000 | Loss: 0.00016812
Iteration 328/1000 | Loss: 0.00015520
Iteration 329/1000 | Loss: 0.00015062
Iteration 330/1000 | Loss: 0.00018803
Iteration 331/1000 | Loss: 0.00015840
Iteration 332/1000 | Loss: 0.00016616
Iteration 333/1000 | Loss: 0.00015185
Iteration 334/1000 | Loss: 0.00015026
Iteration 335/1000 | Loss: 0.00017326
Iteration 336/1000 | Loss: 0.00015203
Iteration 337/1000 | Loss: 0.00017187
Iteration 338/1000 | Loss: 0.00015181
Iteration 339/1000 | Loss: 0.00015714
Iteration 340/1000 | Loss: 0.00015270
Iteration 341/1000 | Loss: 0.00015027
Iteration 342/1000 | Loss: 0.00017294
Iteration 343/1000 | Loss: 0.00015427
Iteration 344/1000 | Loss: 0.00015014
Iteration 345/1000 | Loss: 0.00018134
Iteration 346/1000 | Loss: 0.00015476
Iteration 347/1000 | Loss: 0.00015016
Iteration 348/1000 | Loss: 0.00018517
Iteration 349/1000 | Loss: 0.00015869
Iteration 350/1000 | Loss: 0.00018379
Iteration 351/1000 | Loss: 0.00016447
Iteration 352/1000 | Loss: 0.00017867
Iteration 353/1000 | Loss: 0.00017028
Iteration 354/1000 | Loss: 0.00016447
Iteration 355/1000 | Loss: 0.00015324
Iteration 356/1000 | Loss: 0.00015293
Iteration 357/1000 | Loss: 0.00018422
Iteration 358/1000 | Loss: 0.00015707
Iteration 359/1000 | Loss: 0.00015031
Iteration 360/1000 | Loss: 0.00018092
Iteration 361/1000 | Loss: 0.00016141
Iteration 362/1000 | Loss: 0.00015019
Iteration 363/1000 | Loss: 0.00018463
Iteration 364/1000 | Loss: 0.00016990
Iteration 365/1000 | Loss: 0.00016291
Iteration 366/1000 | Loss: 0.00015165
Iteration 367/1000 | Loss: 0.00018815
Iteration 368/1000 | Loss: 0.00015632
Iteration 369/1000 | Loss: 0.00017127
Iteration 370/1000 | Loss: 0.00015495
Iteration 371/1000 | Loss: 0.00017202
Iteration 372/1000 | Loss: 0.00015171
Iteration 373/1000 | Loss: 0.00016747
Iteration 374/1000 | Loss: 0.00015171
Iteration 375/1000 | Loss: 0.00016721
Iteration 376/1000 | Loss: 0.00015126
Iteration 377/1000 | Loss: 0.00015082
Iteration 378/1000 | Loss: 0.00016508
Iteration 379/1000 | Loss: 0.00015637
Iteration 380/1000 | Loss: 0.00015732
Iteration 381/1000 | Loss: 0.00016692
Iteration 382/1000 | Loss: 0.00015407
Iteration 383/1000 | Loss: 0.00016428
Iteration 384/1000 | Loss: 0.00015540
Iteration 385/1000 | Loss: 0.00015190
Iteration 386/1000 | Loss: 0.00015062
Iteration 387/1000 | Loss: 0.00016637
Iteration 388/1000 | Loss: 0.00015414
Iteration 389/1000 | Loss: 0.00016417
Iteration 390/1000 | Loss: 0.00015375
Iteration 391/1000 | Loss: 0.00015277
Iteration 392/1000 | Loss: 0.00015116
Iteration 393/1000 | Loss: 0.00015207
Iteration 394/1000 | Loss: 0.00016421
Iteration 395/1000 | Loss: 0.00015536
Iteration 396/1000 | Loss: 0.00016387
Iteration 397/1000 | Loss: 0.00015427
Iteration 398/1000 | Loss: 0.00015057
Iteration 399/1000 | Loss: 0.00015020
Iteration 400/1000 | Loss: 0.00015356
Iteration 401/1000 | Loss: 0.00015036
Iteration 402/1000 | Loss: 0.00016331
Iteration 403/1000 | Loss: 0.00015060
Iteration 404/1000 | Loss: 0.00015586
Iteration 405/1000 | Loss: 0.00016324
Iteration 406/1000 | Loss: 0.00015109
Iteration 407/1000 | Loss: 0.00015694
Iteration 408/1000 | Loss: 0.00015897
Iteration 409/1000 | Loss: 0.00015156
Iteration 410/1000 | Loss: 0.00016267
Iteration 411/1000 | Loss: 0.00016098
Iteration 412/1000 | Loss: 0.00015533
Iteration 413/1000 | Loss: 0.00016505
Iteration 414/1000 | Loss: 0.00015660
Iteration 415/1000 | Loss: 0.00015754
Iteration 416/1000 | Loss: 0.00016794
Iteration 417/1000 | Loss: 0.00015627
Iteration 418/1000 | Loss: 0.00015850
Iteration 419/1000 | Loss: 0.00015592
Iteration 420/1000 | Loss: 0.00015816
Iteration 421/1000 | Loss: 0.00016298
Iteration 422/1000 | Loss: 0.00015153
Iteration 423/1000 | Loss: 0.00016747
Iteration 424/1000 | Loss: 0.00015711
Iteration 425/1000 | Loss: 0.00016732
Iteration 426/1000 | Loss: 0.00015128
Iteration 427/1000 | Loss: 0.00016092
Iteration 428/1000 | Loss: 0.00015103
Iteration 429/1000 | Loss: 0.00016611
Iteration 430/1000 | Loss: 0.00015646
Iteration 431/1000 | Loss: 0.00016780
Iteration 432/1000 | Loss: 0.00015809
Iteration 433/1000 | Loss: 0.00016383
Iteration 434/1000 | Loss: 0.00016098
Iteration 435/1000 | Loss: 0.00015734
Iteration 436/1000 | Loss: 0.00016125
Iteration 437/1000 | Loss: 0.00015352
Iteration 438/1000 | Loss: 0.00016337
Iteration 439/1000 | Loss: 0.00015797
Iteration 440/1000 | Loss: 0.00015906
Iteration 441/1000 | Loss: 0.00015822
Iteration 442/1000 | Loss: 0.00015365
Iteration 443/1000 | Loss: 0.00016578
Iteration 444/1000 | Loss: 0.00016091
Iteration 445/1000 | Loss: 0.00016033
Iteration 446/1000 | Loss: 0.00016308
Iteration 447/1000 | Loss: 0.00015772
Iteration 448/1000 | Loss: 0.00015934
Iteration 449/1000 | Loss: 0.00015819
Iteration 450/1000 | Loss: 0.00016201
Iteration 451/1000 | Loss: 0.00015825
Iteration 452/1000 | Loss: 0.00016082
Iteration 453/1000 | Loss: 0.00016032
Iteration 454/1000 | Loss: 0.00015313
Iteration 455/1000 | Loss: 0.00015487
Iteration 456/1000 | Loss: 0.00015804
Iteration 457/1000 | Loss: 0.00015995
Iteration 458/1000 | Loss: 0.00015438
Iteration 459/1000 | Loss: 0.00015880
Iteration 460/1000 | Loss: 0.00016087
Iteration 461/1000 | Loss: 0.00015551
Iteration 462/1000 | Loss: 0.00015764
Iteration 463/1000 | Loss: 0.00016023
Iteration 464/1000 | Loss: 0.00015814
Iteration 465/1000 | Loss: 0.00015856
Iteration 466/1000 | Loss: 0.00017823
Iteration 467/1000 | Loss: 0.00015520
Iteration 468/1000 | Loss: 0.00015728
Iteration 469/1000 | Loss: 0.00015554
Iteration 470/1000 | Loss: 0.00016139
Iteration 471/1000 | Loss: 0.00015640
Iteration 472/1000 | Loss: 0.00015907
Iteration 473/1000 | Loss: 0.00015939
Iteration 474/1000 | Loss: 0.00015076
Iteration 475/1000 | Loss: 0.00017471
Iteration 476/1000 | Loss: 0.00015680
Iteration 477/1000 | Loss: 0.00016202
Iteration 478/1000 | Loss: 0.00015864
Iteration 479/1000 | Loss: 0.00016040
Iteration 480/1000 | Loss: 0.00015945
Iteration 481/1000 | Loss: 0.00016322
Iteration 482/1000 | Loss: 0.00015874
Iteration 483/1000 | Loss: 0.00016069
Iteration 484/1000 | Loss: 0.00015515
Iteration 485/1000 | Loss: 0.00016617
Iteration 486/1000 | Loss: 0.00015699
Iteration 487/1000 | Loss: 0.00015371
Iteration 488/1000 | Loss: 0.00015500
Iteration 489/1000 | Loss: 0.00015612
Iteration 490/1000 | Loss: 0.00015764
Iteration 491/1000 | Loss: 0.00015870
Iteration 492/1000 | Loss: 0.00015785
Iteration 493/1000 | Loss: 0.00015576
Iteration 494/1000 | Loss: 0.00015722
Iteration 495/1000 | Loss: 0.00015889
Iteration 496/1000 | Loss: 0.00015128
Iteration 497/1000 | Loss: 0.00015176
Iteration 498/1000 | Loss: 0.00015046
Iteration 499/1000 | Loss: 0.00015031
Iteration 500/1000 | Loss: 0.00015030
Iteration 501/1000 | Loss: 0.00015029
Iteration 502/1000 | Loss: 0.00015298
Iteration 503/1000 | Loss: 0.00015056
Iteration 504/1000 | Loss: 0.00015353
Iteration 505/1000 | Loss: 0.00015066
Iteration 506/1000 | Loss: 0.00015291
Iteration 507/1000 | Loss: 0.00015096
Iteration 508/1000 | Loss: 0.00015576
Iteration 509/1000 | Loss: 0.00015115
Iteration 510/1000 | Loss: 0.00015631
Iteration 511/1000 | Loss: 0.00015115
Iteration 512/1000 | Loss: 0.00015649
Iteration 513/1000 | Loss: 0.00016613
Iteration 514/1000 | Loss: 0.00015624
Iteration 515/1000 | Loss: 0.00016569
Iteration 516/1000 | Loss: 0.00015600
Iteration 517/1000 | Loss: 0.00015600
Iteration 518/1000 | Loss: 0.00015052
Iteration 519/1000 | Loss: 0.00016256
Iteration 520/1000 | Loss: 0.00015435
Iteration 521/1000 | Loss: 0.00015312
Iteration 522/1000 | Loss: 0.00015409
Iteration 523/1000 | Loss: 0.00015025
Iteration 524/1000 | Loss: 0.00017063
Iteration 525/1000 | Loss: 0.00018003
Iteration 526/1000 | Loss: 0.00019154
Iteration 527/1000 | Loss: 0.00015645
Iteration 528/1000 | Loss: 0.00015745
Iteration 529/1000 | Loss: 0.00015131
Iteration 530/1000 | Loss: 0.00015561
Iteration 531/1000 | Loss: 0.00015241
Iteration 532/1000 | Loss: 0.00015488
Iteration 533/1000 | Loss: 0.00015463
Iteration 534/1000 | Loss: 0.00015310
Iteration 535/1000 | Loss: 0.00015354
Iteration 536/1000 | Loss: 0.00016748
Iteration 537/1000 | Loss: 0.00015657
Iteration 538/1000 | Loss: 0.00016477
Iteration 539/1000 | Loss: 0.00017321
Iteration 540/1000 | Loss: 0.00015384
Iteration 541/1000 | Loss: 0.00015121
Iteration 542/1000 | Loss: 0.00015359
Iteration 543/1000 | Loss: 0.00015133
Iteration 544/1000 | Loss: 0.00015585
Iteration 545/1000 | Loss: 0.00015200
Iteration 546/1000 | Loss: 0.00017338
Iteration 547/1000 | Loss: 0.00015250
Iteration 548/1000 | Loss: 0.00015170
Iteration 549/1000 | Loss: 0.00016553
Iteration 550/1000 | Loss: 0.00015347
Iteration 551/1000 | Loss: 0.00016860
Iteration 552/1000 | Loss: 0.00015340
Iteration 553/1000 | Loss: 0.00015006
Iteration 554/1000 | Loss: 0.00016867
Iteration 555/1000 | Loss: 0.00015514
Iteration 556/1000 | Loss: 0.00014995
Iteration 557/1000 | Loss: 0.00017149
Iteration 558/1000 | Loss: 0.00015721
Iteration 559/1000 | Loss: 0.00016978
Iteration 560/1000 | Loss: 0.00015546
Iteration 561/1000 | Loss: 0.00015796
Iteration 562/1000 | Loss: 0.00015463
Iteration 563/1000 | Loss: 0.00016953
Iteration 564/1000 | Loss: 0.00015430
Iteration 565/1000 | Loss: 0.00015802
Iteration 566/1000 | Loss: 0.00015217
Iteration 567/1000 | Loss: 0.00017258
Iteration 568/1000 | Loss: 0.00015264
Iteration 569/1000 | Loss: 0.00016795
Iteration 570/1000 | Loss: 0.00015384
Iteration 571/1000 | Loss: 0.00019504
Iteration 572/1000 | Loss: 0.00015410
Iteration 573/1000 | Loss: 0.00016720
Iteration 574/1000 | Loss: 0.00015340
Iteration 575/1000 | Loss: 0.00017589
Iteration 576/1000 | Loss: 0.00017486
Iteration 577/1000 | Loss: 0.00015217
Iteration 578/1000 | Loss: 0.00015097
Iteration 579/1000 | Loss: 0.00016949
Iteration 580/1000 | Loss: 0.00015353
Iteration 581/1000 | Loss: 0.00017087
Iteration 582/1000 | Loss: 0.00015245
Iteration 583/1000 | Loss: 0.00019718
Iteration 584/1000 | Loss: 0.00015329
Iteration 585/1000 | Loss: 0.00016341
Iteration 586/1000 | Loss: 0.00015119
Iteration 587/1000 | Loss: 0.00017603
Iteration 588/1000 | Loss: 0.00015233
Iteration 589/1000 | Loss: 0.00017858
Iteration 590/1000 | Loss: 0.00015351
Iteration 591/1000 | Loss: 0.00017976
Iteration 592/1000 | Loss: 0.00015348
Iteration 593/1000 | Loss: 0.00016387
Iteration 594/1000 | Loss: 0.00015124
Iteration 595/1000 | Loss: 0.00017466
Iteration 596/1000 | Loss: 0.00015295
Iteration 597/1000 | Loss: 0.00017386
Iteration 598/1000 | Loss: 0.00015434
Iteration 599/1000 | Loss: 0.00015727
Iteration 600/1000 | Loss: 0.00015091
Iteration 601/1000 | Loss: 0.00017973
Iteration 602/1000 | Loss: 0.00015488
Iteration 603/1000 | Loss: 0.00015753
Iteration 604/1000 | Loss: 0.00015081
Iteration 605/1000 | Loss: 0.00015055
Iteration 606/1000 | Loss: 0.00016508
Iteration 607/1000 | Loss: 0.00015329
Iteration 608/1000 | Loss: 0.00017180
Iteration 609/1000 | Loss: 0.00015403
Iteration 610/1000 | Loss: 0.00018289
Iteration 611/1000 | Loss: 0.00015334
Iteration 612/1000 | Loss: 0.00015015
Iteration 613/1000 | Loss: 0.00017084
Iteration 614/1000 | Loss: 0.00015669
Iteration 615/1000 | Loss: 0.00015222
Iteration 616/1000 | Loss: 0.00017010
Iteration 617/1000 | Loss: 0.00015211
Iteration 618/1000 | Loss: 0.00014987
Iteration 619/1000 | Loss: 0.00017728
Iteration 620/1000 | Loss: 0.00015457
Iteration 621/1000 | Loss: 0.00015102
Iteration 622/1000 | Loss: 0.00015030
Iteration 623/1000 | Loss: 0.00018500
Iteration 624/1000 | Loss: 0.00016397
Iteration 625/1000 | Loss: 0.00018230
Iteration 626/1000 | Loss: 0.00016028
Iteration 627/1000 | Loss: 0.00015247
Iteration 628/1000 | Loss: 0.00015474
Iteration 629/1000 | Loss: 0.00015091
Iteration 630/1000 | Loss: 0.00015028
Iteration 631/1000 | Loss: 0.00016575
Iteration 632/1000 | Loss: 0.00015236
Iteration 633/1000 | Loss: 0.00015135
Iteration 634/1000 | Loss: 0.00015030
Iteration 635/1000 | Loss: 0.00015001
Iteration 636/1000 | Loss: 0.00016600
Iteration 637/1000 | Loss: 0.00015191
Iteration 638/1000 | Loss: 0.00017578
Iteration 639/1000 | Loss: 0.00015287
Iteration 640/1000 | Loss: 0.00015112
Iteration 641/1000 | Loss: 0.00016422
Iteration 642/1000 | Loss: 0.00015880
Iteration 643/1000 | Loss: 0.00015402
Iteration 644/1000 | Loss: 0.00015503
Iteration 645/1000 | Loss: 0.00017644
Iteration 646/1000 | Loss: 0.00015200
Iteration 647/1000 | Loss: 0.00015177
Iteration 648/1000 | Loss: 0.00016046
Iteration 649/1000 | Loss: 0.00015112
Iteration 650/1000 | Loss: 0.00017805
Iteration 651/1000 | Loss: 0.00015221
Iteration 652/1000 | Loss: 0.00016102
Iteration 653/1000 | Loss: 0.00015103
Iteration 654/1000 | Loss: 0.00015097
Iteration 655/1000 | Loss: 0.00015415
Iteration 656/1000 | Loss: 0.00015064
Iteration 657/1000 | Loss: 0.00016020
Iteration 658/1000 | Loss: 0.00015119
Iteration 659/1000 | Loss: 0.00015058
Iteration 660/1000 | Loss: 0.00015023
Iteration 661/1000 | Loss: 0.00015995
Iteration 662/1000 | Loss: 0.00015155
Iteration 663/1000 | Loss: 0.00015859
Iteration 664/1000 | Loss: 0.00015188
Iteration 665/1000 | Loss: 0.00016196
Iteration 666/1000 | Loss: 0.00015442
Iteration 667/1000 | Loss: 0.00018450
Iteration 668/1000 | Loss: 0.00015774
Iteration 669/1000 | Loss: 0.00015409
Iteration 670/1000 | Loss: 0.00015182
Iteration 671/1000 | Loss: 0.00015896
Iteration 672/1000 | Loss: 0.00015143
Iteration 673/1000 | Loss: 0.00015930
Iteration 674/1000 | Loss: 0.00015077
Iteration 675/1000 | Loss: 0.00015333
Iteration 676/1000 | Loss: 0.00016916
Iteration 677/1000 | Loss: 0.00018783
Iteration 678/1000 | Loss: 0.00017562
Iteration 679/1000 | Loss: 0.00016153
Iteration 680/1000 | Loss: 0.00015522
Iteration 681/1000 | Loss: 0.00016313
Iteration 682/1000 | Loss: 0.00015778
Iteration 683/1000 | Loss: 0.00016372
Iteration 684/1000 | Loss: 0.00016320
Iteration 685/1000 | Loss: 0.00015391
Iteration 686/1000 | Loss: 0.00016446
Iteration 687/1000 | Loss: 0.00015506
Iteration 688/1000 | Loss: 0.00015335
Iteration 689/1000 | Loss: 0.00018091
Iteration 690/1000 | Loss: 0.00015313
Iteration 691/1000 | Loss: 0.00015896
Iteration 692/1000 | Loss: 0.00015157
Iteration 693/1000 | Loss: 0.00016094
Iteration 694/1000 | Loss: 0.00015878
Iteration 695/1000 | Loss: 0.00016167
Iteration 696/1000 | Loss: 0.00015689
Iteration 697/1000 | Loss: 0.00017343
Iteration 698/1000 | Loss: 0.00015545
Iteration 699/1000 | Loss: 0.00015858
Iteration 700/1000 | Loss: 0.00015332
Iteration 701/1000 | Loss: 0.00016155
Iteration 702/1000 | Loss: 0.00015181
Iteration 703/1000 | Loss: 0.00015995
Iteration 704/1000 | Loss: 0.00015309
Iteration 705/1000 | Loss: 0.00015987
Iteration 706/1000 | Loss: 0.00015248
Iteration 707/1000 | Loss: 0.00016129
Iteration 708/1000 | Loss: 0.00015233
Iteration 709/1000 | Loss: 0.00016032
Iteration 710/1000 | Loss: 0.00015230
Iteration 711/1000 | Loss: 0.00015759
Iteration 712/1000 | Loss: 0.00015207
Iteration 713/1000 | Loss: 0.00015699
Iteration 714/1000 | Loss: 0.00015202
Iteration 715/1000 | Loss: 0.00015660
Iteration 716/1000 | Loss: 0.00015331
Iteration 717/1000 | Loss: 0.00015611
Iteration 718/1000 | Loss: 0.00015313
Iteration 719/1000 | Loss: 0.00015563
Iteration 720/1000 | Loss: 0.00015275
Iteration 721/1000 | Loss: 0.00015217
Iteration 722/1000 | Loss: 0.00016680
Iteration 723/1000 | Loss: 0.00016005
Iteration 724/1000 | Loss: 0.00015401
Iteration 725/1000 | Loss: 0.00015954
Iteration 726/1000 | Loss: 0.00015648
Iteration 727/1000 | Loss: 0.00016052
Iteration 728/1000 | Loss: 0.00015580
Iteration 729/1000 | Loss: 0.00015331
Iteration 730/1000 | Loss: 0.00015558
Iteration 731/1000 | Loss: 0.00016105
Iteration 732/1000 | Loss: 0.00015541
Iteration 733/1000 | Loss: 0.00015228
Iteration 734/1000 | Loss: 0.00015121
Iteration 735/1000 | Loss: 0.00015087
Iteration 736/1000 | Loss: 0.00015233
Iteration 737/1000 | Loss: 0.00015929
Iteration 738/1000 | Loss: 0.00015991
Iteration 739/1000 | Loss: 0.00017617
Iteration 740/1000 | Loss: 0.00016356
Iteration 741/1000 | Loss: 0.00015907
Iteration 742/1000 | Loss: 0.00015466
Iteration 743/1000 | Loss: 0.00017496
Iteration 744/1000 | Loss: 0.00015472
Iteration 745/1000 | Loss: 0.00016231
Iteration 746/1000 | Loss: 0.00015433
Iteration 747/1000 | Loss: 0.00016478
Iteration 748/1000 | Loss: 0.00015382
Iteration 749/1000 | Loss: 0.00015633
Iteration 750/1000 | Loss: 0.00015281
Iteration 751/1000 | Loss: 0.00015745
Iteration 752/1000 | Loss: 0.00015264
Iteration 753/1000 | Loss: 0.00017008
Iteration 754/1000 | Loss: 0.00015264
Iteration 755/1000 | Loss: 0.00015347
Iteration 756/1000 | Loss: 0.00015293
Iteration 757/1000 | Loss: 0.00015680
Iteration 758/1000 | Loss: 0.00017487
Iteration 759/1000 | Loss: 0.00015314
Iteration 760/1000 | Loss: 0.00016714
Iteration 761/1000 | Loss: 0.00015686
Iteration 762/1000 | Loss: 0.00016183
Iteration 763/1000 | Loss: 0.00015684
Iteration 764/1000 | Loss: 0.00015805
Iteration 765/1000 | Loss: 0.00015957
Iteration 766/1000 | Loss: 0.00015581
Iteration 767/1000 | Loss: 0.00015435
Iteration 768/1000 | Loss: 0.00015254
Iteration 769/1000 | Loss: 0.00015430
Iteration 770/1000 | Loss: 0.00015220
Iteration 771/1000 | Loss: 0.00018873
Iteration 772/1000 | Loss: 0.00015421
Iteration 773/1000 | Loss: 0.00016351
Iteration 774/1000 | Loss: 0.00015316
Iteration 775/1000 | Loss: 0.00016728
Iteration 776/1000 | Loss: 0.00015370
Iteration 777/1000 | Loss: 0.00017806
Iteration 778/1000 | Loss: 0.00015874
Iteration 779/1000 | Loss: 0.00015137
Iteration 780/1000 | Loss: 0.00015135
Iteration 781/1000 | Loss: 0.00016825
Iteration 782/1000 | Loss: 0.00015252
Iteration 783/1000 | Loss: 0.00016960
Iteration 784/1000 | Loss: 0.00015467
Iteration 785/1000 | Loss: 0.00016835
Iteration 786/1000 | Loss: 0.00015215
Iteration 787/1000 | Loss: 0.00018761
Iteration 788/1000 | Loss: 0.00015612
Iteration 789/1000 | Loss: 0.00015190
Iteration 790/1000 | Loss: 0.00015090
Iteration 791/1000 | Loss: 0.00015005
Iteration 792/1000 | Loss: 0.00017615
Iteration 793/1000 | Loss: 0.00015478
Iteration 794/1000 | Loss: 0.00018701
Iteration 795/1000 | Loss: 0.00015326
Iteration 796/1000 | Loss: 0.00015835
Iteration 797/1000 | Loss: 0.00015150
Iteration 798/1000 | Loss: 0.00017414
Iteration 799/1000 | Loss: 0.00015160
Iteration 800/1000 | Loss: 0.00019154
Iteration 801/1000 | Loss: 0.00015409
Iteration 802/1000 | Loss: 0.00016944
Iteration 803/1000 | Loss: 0.00015278
Iteration 804/1000 | Loss: 0.00019330
Iteration 805/1000 | Loss: 0.00017870
Iteration 806/1000 | Loss: 0.00015182
Iteration 807/1000 | Loss: 0.00015057
Iteration 808/1000 | Loss: 0.00015111
Iteration 809/1000 | Loss: 0.00018532
Iteration 810/1000 | Loss: 0.00015866
Iteration 811/1000 | Loss: 0.00017867
Iteration 812/1000 | Loss: 0.00015994
Iteration 813/1000 | Loss: 0.00018018
Iteration 814/1000 | Loss: 0.00016135
Iteration 815/1000 | Loss: 0.00017825
Iteration 816/1000 | Loss: 0.00016587
Iteration 817/1000 | Loss: 0.00017596
Iteration 818/1000 | Loss: 0.00016371
Iteration 819/1000 | Loss: 0.00017825
Iteration 820/1000 | Loss: 0.00016371
Iteration 821/1000 | Loss: 0.00017865
Iteration 822/1000 | Loss: 0.00016440
Iteration 823/1000 | Loss: 0.00017948
Iteration 824/1000 | Loss: 0.00016392
Iteration 825/1000 | Loss: 0.00018494
Iteration 826/1000 | Loss: 0.00016791
Iteration 827/1000 | Loss: 0.00017967
Iteration 828/1000 | Loss: 0.00016984
Iteration 829/1000 | Loss: 0.00018011
Iteration 830/1000 | Loss: 0.00016785
Iteration 831/1000 | Loss: 0.00018597
Iteration 832/1000 | Loss: 0.00016873
Iteration 833/1000 | Loss: 0.00020233
Iteration 834/1000 | Loss: 0.00016525
Iteration 835/1000 | Loss: 0.00018106
Iteration 836/1000 | Loss: 0.00016341
Iteration 837/1000 | Loss: 0.00015230
Iteration 838/1000 | Loss: 0.00017771
Iteration 839/1000 | Loss: 0.00016024
Iteration 840/1000 | Loss: 0.00017342
Iteration 841/1000 | Loss: 0.00016763
Iteration 842/1000 | Loss: 0.00017678
Iteration 843/1000 | Loss: 0.00016605
Iteration 844/1000 | Loss: 0.00017926
Iteration 845/1000 | Loss: 0.00016365
Iteration 846/1000 | Loss: 0.00017261
Iteration 847/1000 | Loss: 0.00016222
Iteration 848/1000 | Loss: 0.00018097
Iteration 849/1000 | Loss: 0.00016703
Iteration 850/1000 | Loss: 0.00017595
Iteration 851/1000 | Loss: 0.00016576
Iteration 852/1000 | Loss: 0.00018113
Iteration 853/1000 | Loss: 0.00016426
Iteration 854/1000 | Loss: 0.00018098
Iteration 855/1000 | Loss: 0.00016284
Iteration 856/1000 | Loss: 0.00017387
Iteration 857/1000 | Loss: 0.00016971
Iteration 858/1000 | Loss: 0.00017490
Iteration 859/1000 | Loss: 0.00016647
Iteration 860/1000 | Loss: 0.00017580
Iteration 861/1000 | Loss: 0.00016512
Iteration 862/1000 | Loss: 0.00018435
Iteration 863/1000 | Loss: 0.00016145
Iteration 864/1000 | Loss: 0.00015526
Iteration 865/1000 | Loss: 0.00016966
Iteration 866/1000 | Loss: 0.00017189
Iteration 867/1000 | Loss: 0.00015323
Iteration 868/1000 | Loss: 0.00015105
Iteration 869/1000 | Loss: 0.00015039
Iteration 870/1000 | Loss: 0.00015575
Iteration 871/1000 | Loss: 0.00015330
Iteration 872/1000 | Loss: 0.00015081
Iteration 873/1000 | Loss: 0.00016916
Iteration 874/1000 | Loss: 0.00015471
Iteration 875/1000 | Loss: 0.00017775
Iteration 876/1000 | Loss: 0.00015718
Iteration 877/1000 | Loss: 0.00017113
Iteration 878/1000 | Loss: 0.00015680
Iteration 879/1000 | Loss: 0.00016709
Iteration 880/1000 | Loss: 0.00015745
Iteration 881/1000 | Loss: 0.00017237
Iteration 882/1000 | Loss: 0.00015698
Iteration 883/1000 | Loss: 0.00017553
Iteration 884/1000 | Loss: 0.00015693
Iteration 885/1000 | Loss: 0.00017032
Iteration 886/1000 | Loss: 0.00015683
Iteration 887/1000 | Loss: 0.00015683
Iteration 888/1000 | Loss: 0.00015275
Iteration 889/1000 | Loss: 0.00015446
Iteration 890/1000 | Loss: 0.00015519
Iteration 891/1000 | Loss: 0.00015268
Iteration 892/1000 | Loss: 0.00015107
Iteration 893/1000 | Loss: 0.00018094
Iteration 894/1000 | Loss: 0.00015318
Iteration 895/1000 | Loss: 0.00016906
Iteration 896/1000 | Loss: 0.00015234
Iteration 897/1000 | Loss: 0.00015681
Iteration 898/1000 | Loss: 0.00015571
Iteration 899/1000 | Loss: 0.00018116
Iteration 900/1000 | Loss: 0.00015836
Iteration 901/1000 | Loss: 0.00017871
Iteration 902/1000 | Loss: 0.00016065
Iteration 903/1000 | Loss: 0.00017116
Iteration 904/1000 | Loss: 0.00016010
Iteration 905/1000 | Loss: 0.00017365
Iteration 906/1000 | Loss: 0.00016018
Iteration 907/1000 | Loss: 0.00015450
Iteration 908/1000 | Loss: 0.00017011
Iteration 909/1000 | Loss: 0.00016078
Iteration 910/1000 | Loss: 0.00016733
Iteration 911/1000 | Loss: 0.00016094
Iteration 912/1000 | Loss: 0.00017270
Iteration 913/1000 | Loss: 0.00015977
Iteration 914/1000 | Loss: 0.00015976
Iteration 915/1000 | Loss: 0.00018039
Iteration 916/1000 | Loss: 0.00015962
Iteration 917/1000 | Loss: 0.00015961
Iteration 918/1000 | Loss: 0.00016974
Iteration 919/1000 | Loss: 0.00015948
Iteration 920/1000 | Loss: 0.00016979
Iteration 921/1000 | Loss: 0.00015905
Iteration 922/1000 | Loss: 0.00017085
Iteration 923/1000 | Loss: 0.00015902
Iteration 924/1000 | Loss: 0.00015901
Iteration 925/1000 | Loss: 0.00015901
Iteration 926/1000 | Loss: 0.00015570
Iteration 927/1000 | Loss: 0.00015623
Iteration 928/1000 | Loss: 0.00015078
Iteration 929/1000 | Loss: 0.00015048
Iteration 930/1000 | Loss: 0.00015875
Iteration 931/1000 | Loss: 0.00015708
Iteration 932/1000 | Loss: 0.00016228
Iteration 933/1000 | Loss: 0.00015728
Iteration 934/1000 | Loss: 0.00015833
Iteration 935/1000 | Loss: 0.00015503
Iteration 936/1000 | Loss: 0.00017764
Iteration 937/1000 | Loss: 0.00015953
Iteration 938/1000 | Loss: 0.00017388
Iteration 939/1000 | Loss: 0.00015838
Iteration 940/1000 | Loss: 0.00017426
Iteration 941/1000 | Loss: 0.00015746
Iteration 942/1000 | Loss: 0.00017226
Iteration 943/1000 | Loss: 0.00015977
Iteration 944/1000 | Loss: 0.00017670
Iteration 945/1000 | Loss: 0.00016574
Iteration 946/1000 | Loss: 0.00017684
Iteration 947/1000 | Loss: 0.00016777
Iteration 948/1000 | Loss: 0.00017102
Iteration 949/1000 | Loss: 0.00016159
Iteration 950/1000 | Loss: 0.00017687
Iteration 951/1000 | Loss: 0.00017139
Iteration 952/1000 | Loss: 0.00017809
Iteration 953/1000 | Loss: 0.00016815
Iteration 954/1000 | Loss: 0.00015900
Iteration 955/1000 | Loss: 0.00016338
Iteration 956/1000 | Loss: 0.00015221
Iteration 957/1000 | Loss: 0.00015067
Iteration 958/1000 | Loss: 0.00016375
Iteration 959/1000 | Loss: 0.00016974
Iteration 960/1000 | Loss: 0.00015981
Iteration 961/1000 | Loss: 0.00015116
Iteration 962/1000 | Loss: 0.00016634
Iteration 963/1000 | Loss: 0.00015798
Iteration 964/1000 | Loss: 0.00016367
Iteration 965/1000 | Loss: 0.00015954
Iteration 966/1000 | Loss: 0.00017418
Iteration 967/1000 | Loss: 0.00015945
Iteration 968/1000 | Loss: 0.00016711
Iteration 969/1000 | Loss: 0.00015930
Iteration 970/1000 | Loss: 0.00016524
Iteration 971/1000 | Loss: 0.00016266
Iteration 972/1000 | Loss: 0.00015798
Iteration 973/1000 | Loss: 0.00016614
Iteration 974/1000 | Loss: 0.00015344
Iteration 975/1000 | Loss: 0.00015878
Iteration 976/1000 | Loss: 0.00015110
Iteration 977/1000 | Loss: 0.00015028
Iteration 978/1000 | Loss: 0.00016896
Iteration 979/1000 | Loss: 0.00015389
Iteration 980/1000 | Loss: 0.00016610
Iteration 981/1000 | Loss: 0.00015568
Iteration 982/1000 | Loss: 0.00016746
Iteration 983/1000 | Loss: 0.00015158
Iteration 984/1000 | Loss: 0.00015095
Iteration 985/1000 | Loss: 0.00015043
Iteration 986/1000 | Loss: 0.00015024
Iteration 987/1000 | Loss: 0.00015117
Iteration 988/1000 | Loss: 0.00016269
Iteration 989/1000 | Loss: 0.00015230
Iteration 990/1000 | Loss: 0.00015162
Iteration 991/1000 | Loss: 0.00018396
Iteration 992/1000 | Loss: 0.00015148
Iteration 993/1000 | Loss: 0.00016299
Iteration 994/1000 | Loss: 0.00015825
Iteration 995/1000 | Loss: 0.00015232
Iteration 996/1000 | Loss: 0.00015085
Iteration 997/1000 | Loss: 0.00015456
Iteration 998/1000 | Loss: 0.00015057
Iteration 999/1000 | Loss: 0.00015789
Iteration 1000/1000 | Loss: 0.00015880

Optimization complete. Final v2v error: 7.304093360900879 mm

Highest mean error: 16.87971305847168 mm for frame 9

Lowest mean error: 4.976731300354004 mm for frame 12

Saving results

Total time: 1396.3618619441986
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_56_us_1894/0022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_56_us_1894/0022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_56_us_1894/0022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00475762
Iteration 2/25 | Loss: 0.00226093
Iteration 3/25 | Loss: 0.00214014
Iteration 4/25 | Loss: 0.00212323
Iteration 5/25 | Loss: 0.00211815
Iteration 6/25 | Loss: 0.00211727
Iteration 7/25 | Loss: 0.00211727
Iteration 8/25 | Loss: 0.00211727
Iteration 9/25 | Loss: 0.00211727
Iteration 10/25 | Loss: 0.00211727
Iteration 11/25 | Loss: 0.00211727
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0021172661799937487, 0.0021172661799937487, 0.0021172661799937487, 0.0021172661799937487, 0.0021172661799937487]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0021172661799937487

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.21231413
Iteration 2/25 | Loss: 0.00321336
Iteration 3/25 | Loss: 0.00321336
Iteration 4/25 | Loss: 0.00321336
Iteration 5/25 | Loss: 0.00321336
Iteration 6/25 | Loss: 0.00321336
Iteration 7/25 | Loss: 0.00321336
Iteration 8/25 | Loss: 0.00321336
Iteration 9/25 | Loss: 0.00321336
Iteration 10/25 | Loss: 0.00321336
Iteration 11/25 | Loss: 0.00321336
Iteration 12/25 | Loss: 0.00321336
Iteration 13/25 | Loss: 0.00321336
Iteration 14/25 | Loss: 0.00321336
Iteration 15/25 | Loss: 0.00321336
Iteration 16/25 | Loss: 0.00321336
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.003213359508663416, 0.003213359508663416, 0.003213359508663416, 0.003213359508663416, 0.003213359508663416]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.003213359508663416

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00321336
Iteration 2/1000 | Loss: 0.00010893
Iteration 3/1000 | Loss: 0.00006914
Iteration 4/1000 | Loss: 0.00005799
Iteration 5/1000 | Loss: 0.00005259
Iteration 6/1000 | Loss: 0.00004970
Iteration 7/1000 | Loss: 0.00004747
Iteration 8/1000 | Loss: 0.00004628
Iteration 9/1000 | Loss: 0.00004486
Iteration 10/1000 | Loss: 0.00004379
Iteration 11/1000 | Loss: 0.00004299
Iteration 12/1000 | Loss: 0.00004241
Iteration 13/1000 | Loss: 0.00004196
Iteration 14/1000 | Loss: 0.00004161
Iteration 15/1000 | Loss: 0.00004129
Iteration 16/1000 | Loss: 0.00004109
Iteration 17/1000 | Loss: 0.00004109
Iteration 18/1000 | Loss: 0.00004099
Iteration 19/1000 | Loss: 0.00004097
Iteration 20/1000 | Loss: 0.00004091
Iteration 21/1000 | Loss: 0.00004090
Iteration 22/1000 | Loss: 0.00004089
Iteration 23/1000 | Loss: 0.00004089
Iteration 24/1000 | Loss: 0.00004089
Iteration 25/1000 | Loss: 0.00004089
Iteration 26/1000 | Loss: 0.00004083
Iteration 27/1000 | Loss: 0.00004083
Iteration 28/1000 | Loss: 0.00004082
Iteration 29/1000 | Loss: 0.00004082
Iteration 30/1000 | Loss: 0.00004081
Iteration 31/1000 | Loss: 0.00004081
Iteration 32/1000 | Loss: 0.00004079
Iteration 33/1000 | Loss: 0.00004079
Iteration 34/1000 | Loss: 0.00004078
Iteration 35/1000 | Loss: 0.00004078
Iteration 36/1000 | Loss: 0.00004078
Iteration 37/1000 | Loss: 0.00004078
Iteration 38/1000 | Loss: 0.00004078
Iteration 39/1000 | Loss: 0.00004078
Iteration 40/1000 | Loss: 0.00004078
Iteration 41/1000 | Loss: 0.00004078
Iteration 42/1000 | Loss: 0.00004078
Iteration 43/1000 | Loss: 0.00004078
Iteration 44/1000 | Loss: 0.00004078
Iteration 45/1000 | Loss: 0.00004078
Iteration 46/1000 | Loss: 0.00004077
Iteration 47/1000 | Loss: 0.00004077
Iteration 48/1000 | Loss: 0.00004077
Iteration 49/1000 | Loss: 0.00004077
Iteration 50/1000 | Loss: 0.00004077
Iteration 51/1000 | Loss: 0.00004077
Iteration 52/1000 | Loss: 0.00004077
Iteration 53/1000 | Loss: 0.00004077
Iteration 54/1000 | Loss: 0.00004077
Iteration 55/1000 | Loss: 0.00004077
Iteration 56/1000 | Loss: 0.00004077
Iteration 57/1000 | Loss: 0.00004076
Iteration 58/1000 | Loss: 0.00004076
Iteration 59/1000 | Loss: 0.00004076
Iteration 60/1000 | Loss: 0.00004076
Iteration 61/1000 | Loss: 0.00004076
Iteration 62/1000 | Loss: 0.00004076
Iteration 63/1000 | Loss: 0.00004076
Iteration 64/1000 | Loss: 0.00004076
Iteration 65/1000 | Loss: 0.00004076
Iteration 66/1000 | Loss: 0.00004076
Iteration 67/1000 | Loss: 0.00004075
Iteration 68/1000 | Loss: 0.00004075
Iteration 69/1000 | Loss: 0.00004073
Iteration 70/1000 | Loss: 0.00004073
Iteration 71/1000 | Loss: 0.00004073
Iteration 72/1000 | Loss: 0.00004073
Iteration 73/1000 | Loss: 0.00004073
Iteration 74/1000 | Loss: 0.00004073
Iteration 75/1000 | Loss: 0.00004073
Iteration 76/1000 | Loss: 0.00004073
Iteration 77/1000 | Loss: 0.00004073
Iteration 78/1000 | Loss: 0.00004073
Iteration 79/1000 | Loss: 0.00004073
Iteration 80/1000 | Loss: 0.00004072
Iteration 81/1000 | Loss: 0.00004072
Iteration 82/1000 | Loss: 0.00004072
Iteration 83/1000 | Loss: 0.00004071
Iteration 84/1000 | Loss: 0.00004071
Iteration 85/1000 | Loss: 0.00004071
Iteration 86/1000 | Loss: 0.00004070
Iteration 87/1000 | Loss: 0.00004070
Iteration 88/1000 | Loss: 0.00004070
Iteration 89/1000 | Loss: 0.00004069
Iteration 90/1000 | Loss: 0.00004069
Iteration 91/1000 | Loss: 0.00004068
Iteration 92/1000 | Loss: 0.00004068
Iteration 93/1000 | Loss: 0.00004068
Iteration 94/1000 | Loss: 0.00004068
Iteration 95/1000 | Loss: 0.00004067
Iteration 96/1000 | Loss: 0.00004067
Iteration 97/1000 | Loss: 0.00004067
Iteration 98/1000 | Loss: 0.00004066
Iteration 99/1000 | Loss: 0.00004066
Iteration 100/1000 | Loss: 0.00004065
Iteration 101/1000 | Loss: 0.00004065
Iteration 102/1000 | Loss: 0.00004065
Iteration 103/1000 | Loss: 0.00004065
Iteration 104/1000 | Loss: 0.00004065
Iteration 105/1000 | Loss: 0.00004065
Iteration 106/1000 | Loss: 0.00004065
Iteration 107/1000 | Loss: 0.00004065
Iteration 108/1000 | Loss: 0.00004064
Iteration 109/1000 | Loss: 0.00004064
Iteration 110/1000 | Loss: 0.00004064
Iteration 111/1000 | Loss: 0.00004064
Iteration 112/1000 | Loss: 0.00004064
Iteration 113/1000 | Loss: 0.00004064
Iteration 114/1000 | Loss: 0.00004064
Iteration 115/1000 | Loss: 0.00004064
Iteration 116/1000 | Loss: 0.00004064
Iteration 117/1000 | Loss: 0.00004064
Iteration 118/1000 | Loss: 0.00004064
Iteration 119/1000 | Loss: 0.00004064
Iteration 120/1000 | Loss: 0.00004063
Iteration 121/1000 | Loss: 0.00004063
Iteration 122/1000 | Loss: 0.00004063
Iteration 123/1000 | Loss: 0.00004063
Iteration 124/1000 | Loss: 0.00004063
Iteration 125/1000 | Loss: 0.00004062
Iteration 126/1000 | Loss: 0.00004062
Iteration 127/1000 | Loss: 0.00004062
Iteration 128/1000 | Loss: 0.00004062
Iteration 129/1000 | Loss: 0.00004061
Iteration 130/1000 | Loss: 0.00004061
Iteration 131/1000 | Loss: 0.00004061
Iteration 132/1000 | Loss: 0.00004061
Iteration 133/1000 | Loss: 0.00004061
Iteration 134/1000 | Loss: 0.00004061
Iteration 135/1000 | Loss: 0.00004061
Iteration 136/1000 | Loss: 0.00004060
Iteration 137/1000 | Loss: 0.00004060
Iteration 138/1000 | Loss: 0.00004060
Iteration 139/1000 | Loss: 0.00004060
Iteration 140/1000 | Loss: 0.00004060
Iteration 141/1000 | Loss: 0.00004060
Iteration 142/1000 | Loss: 0.00004059
Iteration 143/1000 | Loss: 0.00004059
Iteration 144/1000 | Loss: 0.00004059
Iteration 145/1000 | Loss: 0.00004059
Iteration 146/1000 | Loss: 0.00004059
Iteration 147/1000 | Loss: 0.00004058
Iteration 148/1000 | Loss: 0.00004058
Iteration 149/1000 | Loss: 0.00004058
Iteration 150/1000 | Loss: 0.00004058
Iteration 151/1000 | Loss: 0.00004058
Iteration 152/1000 | Loss: 0.00004058
Iteration 153/1000 | Loss: 0.00004058
Iteration 154/1000 | Loss: 0.00004058
Iteration 155/1000 | Loss: 0.00004058
Iteration 156/1000 | Loss: 0.00004058
Iteration 157/1000 | Loss: 0.00004058
Iteration 158/1000 | Loss: 0.00004058
Iteration 159/1000 | Loss: 0.00004058
Iteration 160/1000 | Loss: 0.00004058
Iteration 161/1000 | Loss: 0.00004058
Iteration 162/1000 | Loss: 0.00004058
Iteration 163/1000 | Loss: 0.00004058
Iteration 164/1000 | Loss: 0.00004057
Iteration 165/1000 | Loss: 0.00004057
Iteration 166/1000 | Loss: 0.00004057
Iteration 167/1000 | Loss: 0.00004057
Iteration 168/1000 | Loss: 0.00004057
Iteration 169/1000 | Loss: 0.00004057
Iteration 170/1000 | Loss: 0.00004056
Iteration 171/1000 | Loss: 0.00004056
Iteration 172/1000 | Loss: 0.00004056
Iteration 173/1000 | Loss: 0.00004056
Iteration 174/1000 | Loss: 0.00004056
Iteration 175/1000 | Loss: 0.00004056
Iteration 176/1000 | Loss: 0.00004056
Iteration 177/1000 | Loss: 0.00004056
Iteration 178/1000 | Loss: 0.00004056
Iteration 179/1000 | Loss: 0.00004056
Iteration 180/1000 | Loss: 0.00004056
Iteration 181/1000 | Loss: 0.00004056
Iteration 182/1000 | Loss: 0.00004056
Iteration 183/1000 | Loss: 0.00004056
Iteration 184/1000 | Loss: 0.00004056
Iteration 185/1000 | Loss: 0.00004056
Iteration 186/1000 | Loss: 0.00004056
Iteration 187/1000 | Loss: 0.00004056
Iteration 188/1000 | Loss: 0.00004056
Iteration 189/1000 | Loss: 0.00004056
Iteration 190/1000 | Loss: 0.00004056
Iteration 191/1000 | Loss: 0.00004056
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 191. Stopping optimization.
Last 5 losses: [4.055907629663125e-05, 4.055907629663125e-05, 4.055907629663125e-05, 4.055907629663125e-05, 4.055907629663125e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.055907629663125e-05

Optimization complete. Final v2v error: 5.484663486480713 mm

Highest mean error: 6.0193867683410645 mm for frame 63

Lowest mean error: 5.0056986808776855 mm for frame 102

Saving results

Total time: 48.699288845062256
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_56_us_1894/0021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_56_us_1894/0021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_56_us_1894/0021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00713171
Iteration 2/25 | Loss: 0.00232151
Iteration 3/25 | Loss: 0.00215929
Iteration 4/25 | Loss: 0.00214618
Iteration 5/25 | Loss: 0.00214259
Iteration 6/25 | Loss: 0.00214259
Iteration 7/25 | Loss: 0.00214259
Iteration 8/25 | Loss: 0.00214259
Iteration 9/25 | Loss: 0.00214259
Iteration 10/25 | Loss: 0.00214259
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.002142589772120118, 0.002142589772120118, 0.002142589772120118, 0.002142589772120118, 0.002142589772120118]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002142589772120118

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.60895181
Iteration 2/25 | Loss: 0.00201112
Iteration 3/25 | Loss: 0.00201112
Iteration 4/25 | Loss: 0.00201112
Iteration 5/25 | Loss: 0.00201112
Iteration 6/25 | Loss: 0.00201112
Iteration 7/25 | Loss: 0.00201112
Iteration 8/25 | Loss: 0.00201112
Iteration 9/25 | Loss: 0.00201112
Iteration 10/25 | Loss: 0.00201112
Iteration 11/25 | Loss: 0.00201112
Iteration 12/25 | Loss: 0.00201112
Iteration 13/25 | Loss: 0.00201112
Iteration 14/25 | Loss: 0.00201112
Iteration 15/25 | Loss: 0.00201112
Iteration 16/25 | Loss: 0.00201112
Iteration 17/25 | Loss: 0.00201112
Iteration 18/25 | Loss: 0.00201112
Iteration 19/25 | Loss: 0.00201112
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0020111172925680876, 0.0020111172925680876, 0.0020111172925680876, 0.0020111172925680876, 0.0020111172925680876]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0020111172925680876

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00201112
Iteration 2/1000 | Loss: 0.00011396
Iteration 3/1000 | Loss: 0.00008441
Iteration 4/1000 | Loss: 0.00007550
Iteration 5/1000 | Loss: 0.00007180
Iteration 6/1000 | Loss: 0.00006916
Iteration 7/1000 | Loss: 0.00006715
Iteration 8/1000 | Loss: 0.00006561
Iteration 9/1000 | Loss: 0.00006453
Iteration 10/1000 | Loss: 0.00006397
Iteration 11/1000 | Loss: 0.00006347
Iteration 12/1000 | Loss: 0.00006301
Iteration 13/1000 | Loss: 0.00006241
Iteration 14/1000 | Loss: 0.00006215
Iteration 15/1000 | Loss: 0.00006195
Iteration 16/1000 | Loss: 0.00006181
Iteration 17/1000 | Loss: 0.00006160
Iteration 18/1000 | Loss: 0.00006141
Iteration 19/1000 | Loss: 0.00006122
Iteration 20/1000 | Loss: 0.00006105
Iteration 21/1000 | Loss: 0.00006099
Iteration 22/1000 | Loss: 0.00006090
Iteration 23/1000 | Loss: 0.00006082
Iteration 24/1000 | Loss: 0.00006067
Iteration 25/1000 | Loss: 0.00006064
Iteration 26/1000 | Loss: 0.00006061
Iteration 27/1000 | Loss: 0.00006046
Iteration 28/1000 | Loss: 0.00006037
Iteration 29/1000 | Loss: 0.00006032
Iteration 30/1000 | Loss: 0.00006029
Iteration 31/1000 | Loss: 0.00006029
Iteration 32/1000 | Loss: 0.00006028
Iteration 33/1000 | Loss: 0.00006027
Iteration 34/1000 | Loss: 0.00006027
Iteration 35/1000 | Loss: 0.00006027
Iteration 36/1000 | Loss: 0.00006027
Iteration 37/1000 | Loss: 0.00006027
Iteration 38/1000 | Loss: 0.00006026
Iteration 39/1000 | Loss: 0.00006026
Iteration 40/1000 | Loss: 0.00006026
Iteration 41/1000 | Loss: 0.00006026
Iteration 42/1000 | Loss: 0.00006026
Iteration 43/1000 | Loss: 0.00006026
Iteration 44/1000 | Loss: 0.00006026
Iteration 45/1000 | Loss: 0.00006026
Iteration 46/1000 | Loss: 0.00006026
Iteration 47/1000 | Loss: 0.00006026
Iteration 48/1000 | Loss: 0.00006026
Iteration 49/1000 | Loss: 0.00006026
Iteration 50/1000 | Loss: 0.00006026
Iteration 51/1000 | Loss: 0.00006026
Iteration 52/1000 | Loss: 0.00006025
Iteration 53/1000 | Loss: 0.00006024
Iteration 54/1000 | Loss: 0.00006023
Iteration 55/1000 | Loss: 0.00006023
Iteration 56/1000 | Loss: 0.00006023
Iteration 57/1000 | Loss: 0.00006023
Iteration 58/1000 | Loss: 0.00006023
Iteration 59/1000 | Loss: 0.00006022
Iteration 60/1000 | Loss: 0.00006022
Iteration 61/1000 | Loss: 0.00006022
Iteration 62/1000 | Loss: 0.00006022
Iteration 63/1000 | Loss: 0.00006022
Iteration 64/1000 | Loss: 0.00006022
Iteration 65/1000 | Loss: 0.00006021
Iteration 66/1000 | Loss: 0.00006021
Iteration 67/1000 | Loss: 0.00006021
Iteration 68/1000 | Loss: 0.00006021
Iteration 69/1000 | Loss: 0.00006020
Iteration 70/1000 | Loss: 0.00006020
Iteration 71/1000 | Loss: 0.00006020
Iteration 72/1000 | Loss: 0.00006020
Iteration 73/1000 | Loss: 0.00006020
Iteration 74/1000 | Loss: 0.00006020
Iteration 75/1000 | Loss: 0.00006020
Iteration 76/1000 | Loss: 0.00006020
Iteration 77/1000 | Loss: 0.00006020
Iteration 78/1000 | Loss: 0.00006020
Iteration 79/1000 | Loss: 0.00006020
Iteration 80/1000 | Loss: 0.00006020
Iteration 81/1000 | Loss: 0.00006020
Iteration 82/1000 | Loss: 0.00006020
Iteration 83/1000 | Loss: 0.00006020
Iteration 84/1000 | Loss: 0.00006020
Iteration 85/1000 | Loss: 0.00006020
Iteration 86/1000 | Loss: 0.00006020
Iteration 87/1000 | Loss: 0.00006020
Iteration 88/1000 | Loss: 0.00006020
Iteration 89/1000 | Loss: 0.00006020
Iteration 90/1000 | Loss: 0.00006020
Iteration 91/1000 | Loss: 0.00006020
Iteration 92/1000 | Loss: 0.00006020
Iteration 93/1000 | Loss: 0.00006020
Iteration 94/1000 | Loss: 0.00006020
Iteration 95/1000 | Loss: 0.00006020
Iteration 96/1000 | Loss: 0.00006020
Iteration 97/1000 | Loss: 0.00006020
Iteration 98/1000 | Loss: 0.00006020
Iteration 99/1000 | Loss: 0.00006020
Iteration 100/1000 | Loss: 0.00006020
Iteration 101/1000 | Loss: 0.00006020
Iteration 102/1000 | Loss: 0.00006020
Iteration 103/1000 | Loss: 0.00006020
Iteration 104/1000 | Loss: 0.00006020
Iteration 105/1000 | Loss: 0.00006020
Iteration 106/1000 | Loss: 0.00006020
Iteration 107/1000 | Loss: 0.00006020
Iteration 108/1000 | Loss: 0.00006020
Iteration 109/1000 | Loss: 0.00006020
Iteration 110/1000 | Loss: 0.00006020
Iteration 111/1000 | Loss: 0.00006020
Iteration 112/1000 | Loss: 0.00006020
Iteration 113/1000 | Loss: 0.00006020
Iteration 114/1000 | Loss: 0.00006020
Iteration 115/1000 | Loss: 0.00006020
Iteration 116/1000 | Loss: 0.00006020
Iteration 117/1000 | Loss: 0.00006020
Iteration 118/1000 | Loss: 0.00006020
Iteration 119/1000 | Loss: 0.00006020
Iteration 120/1000 | Loss: 0.00006020
Iteration 121/1000 | Loss: 0.00006020
Iteration 122/1000 | Loss: 0.00006020
Iteration 123/1000 | Loss: 0.00006020
Iteration 124/1000 | Loss: 0.00006020
Iteration 125/1000 | Loss: 0.00006020
Iteration 126/1000 | Loss: 0.00006020
Iteration 127/1000 | Loss: 0.00006020
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 127. Stopping optimization.
Last 5 losses: [6.019966895109974e-05, 6.019966895109974e-05, 6.019966895109974e-05, 6.019966895109974e-05, 6.019966895109974e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 6.019966895109974e-05

Optimization complete. Final v2v error: 6.544672966003418 mm

Highest mean error: 7.333837509155273 mm for frame 174

Lowest mean error: 6.299680709838867 mm for frame 0

Saving results

Total time: 54.42583441734314
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_56_us_1894/0013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_56_us_1894/0013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_56_us_1894/0013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00964563
Iteration 2/25 | Loss: 0.00245264
Iteration 3/25 | Loss: 0.00217230
Iteration 4/25 | Loss: 0.00213068
Iteration 5/25 | Loss: 0.00211249
Iteration 6/25 | Loss: 0.00210737
Iteration 7/25 | Loss: 0.00210663
Iteration 8/25 | Loss: 0.00210347
Iteration 9/25 | Loss: 0.00210302
Iteration 10/25 | Loss: 0.00210294
Iteration 11/25 | Loss: 0.00210294
Iteration 12/25 | Loss: 0.00210294
Iteration 13/25 | Loss: 0.00210294
Iteration 14/25 | Loss: 0.00210294
Iteration 15/25 | Loss: 0.00210294
Iteration 16/25 | Loss: 0.00210294
Iteration 17/25 | Loss: 0.00210294
Iteration 18/25 | Loss: 0.00210294
Iteration 19/25 | Loss: 0.00210294
Iteration 20/25 | Loss: 0.00210294
Iteration 21/25 | Loss: 0.00210294
Iteration 22/25 | Loss: 0.00210294
Iteration 23/25 | Loss: 0.00210294
Iteration 24/25 | Loss: 0.00210294
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0021029377821832895, 0.0021029377821832895, 0.0021029377821832895, 0.0021029377821832895, 0.0021029377821832895]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0021029377821832895

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.83172089
Iteration 2/25 | Loss: 0.00195139
Iteration 3/25 | Loss: 0.00195138
Iteration 4/25 | Loss: 0.00195138
Iteration 5/25 | Loss: 0.00195138
Iteration 6/25 | Loss: 0.00195138
Iteration 7/25 | Loss: 0.00195138
Iteration 8/25 | Loss: 0.00195138
Iteration 9/25 | Loss: 0.00195138
Iteration 10/25 | Loss: 0.00195138
Iteration 11/25 | Loss: 0.00195138
Iteration 12/25 | Loss: 0.00195138
Iteration 13/25 | Loss: 0.00195138
Iteration 14/25 | Loss: 0.00195138
Iteration 15/25 | Loss: 0.00195138
Iteration 16/25 | Loss: 0.00195138
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0019513769075274467, 0.0019513769075274467, 0.0019513769075274467, 0.0019513769075274467, 0.0019513769075274467]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0019513769075274467

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00195138
Iteration 2/1000 | Loss: 0.00009098
Iteration 3/1000 | Loss: 0.00006829
Iteration 4/1000 | Loss: 0.00005653
Iteration 5/1000 | Loss: 0.00005027
Iteration 6/1000 | Loss: 0.00004693
Iteration 7/1000 | Loss: 0.00004485
Iteration 8/1000 | Loss: 0.00004325
Iteration 9/1000 | Loss: 0.00004240
Iteration 10/1000 | Loss: 0.00004174
Iteration 11/1000 | Loss: 0.00004120
Iteration 12/1000 | Loss: 0.00004093
Iteration 13/1000 | Loss: 0.00004069
Iteration 14/1000 | Loss: 0.00004048
Iteration 15/1000 | Loss: 0.00004575
Iteration 16/1000 | Loss: 0.00004098
Iteration 17/1000 | Loss: 0.00004033
Iteration 18/1000 | Loss: 0.00003963
Iteration 19/1000 | Loss: 0.00003905
Iteration 20/1000 | Loss: 0.00003856
Iteration 21/1000 | Loss: 0.00003823
Iteration 22/1000 | Loss: 0.00003815
Iteration 23/1000 | Loss: 0.00003806
Iteration 24/1000 | Loss: 0.00003805
Iteration 25/1000 | Loss: 0.00003805
Iteration 26/1000 | Loss: 0.00003805
Iteration 27/1000 | Loss: 0.00003801
Iteration 28/1000 | Loss: 0.00003801
Iteration 29/1000 | Loss: 0.00003800
Iteration 30/1000 | Loss: 0.00003799
Iteration 31/1000 | Loss: 0.00003798
Iteration 32/1000 | Loss: 0.00003797
Iteration 33/1000 | Loss: 0.00003797
Iteration 34/1000 | Loss: 0.00003797
Iteration 35/1000 | Loss: 0.00003797
Iteration 36/1000 | Loss: 0.00003796
Iteration 37/1000 | Loss: 0.00003796
Iteration 38/1000 | Loss: 0.00003796
Iteration 39/1000 | Loss: 0.00003796
Iteration 40/1000 | Loss: 0.00003796
Iteration 41/1000 | Loss: 0.00003796
Iteration 42/1000 | Loss: 0.00003796
Iteration 43/1000 | Loss: 0.00003796
Iteration 44/1000 | Loss: 0.00003796
Iteration 45/1000 | Loss: 0.00003796
Iteration 46/1000 | Loss: 0.00003795
Iteration 47/1000 | Loss: 0.00003795
Iteration 48/1000 | Loss: 0.00003795
Iteration 49/1000 | Loss: 0.00003795
Iteration 50/1000 | Loss: 0.00003795
Iteration 51/1000 | Loss: 0.00003795
Iteration 52/1000 | Loss: 0.00003795
Iteration 53/1000 | Loss: 0.00003795
Iteration 54/1000 | Loss: 0.00003795
Iteration 55/1000 | Loss: 0.00003794
Iteration 56/1000 | Loss: 0.00003794
Iteration 57/1000 | Loss: 0.00003794
Iteration 58/1000 | Loss: 0.00003794
Iteration 59/1000 | Loss: 0.00003794
Iteration 60/1000 | Loss: 0.00003794
Iteration 61/1000 | Loss: 0.00003794
Iteration 62/1000 | Loss: 0.00003794
Iteration 63/1000 | Loss: 0.00003794
Iteration 64/1000 | Loss: 0.00003794
Iteration 65/1000 | Loss: 0.00003793
Iteration 66/1000 | Loss: 0.00003793
Iteration 67/1000 | Loss: 0.00003793
Iteration 68/1000 | Loss: 0.00003793
Iteration 69/1000 | Loss: 0.00003793
Iteration 70/1000 | Loss: 0.00003793
Iteration 71/1000 | Loss: 0.00003792
Iteration 72/1000 | Loss: 0.00003792
Iteration 73/1000 | Loss: 0.00003792
Iteration 74/1000 | Loss: 0.00003792
Iteration 75/1000 | Loss: 0.00003792
Iteration 76/1000 | Loss: 0.00003792
Iteration 77/1000 | Loss: 0.00003791
Iteration 78/1000 | Loss: 0.00003791
Iteration 79/1000 | Loss: 0.00003791
Iteration 80/1000 | Loss: 0.00003791
Iteration 81/1000 | Loss: 0.00003791
Iteration 82/1000 | Loss: 0.00003791
Iteration 83/1000 | Loss: 0.00003790
Iteration 84/1000 | Loss: 0.00003790
Iteration 85/1000 | Loss: 0.00003790
Iteration 86/1000 | Loss: 0.00003790
Iteration 87/1000 | Loss: 0.00003790
Iteration 88/1000 | Loss: 0.00003790
Iteration 89/1000 | Loss: 0.00003789
Iteration 90/1000 | Loss: 0.00003789
Iteration 91/1000 | Loss: 0.00003789
Iteration 92/1000 | Loss: 0.00003789
Iteration 93/1000 | Loss: 0.00003789
Iteration 94/1000 | Loss: 0.00003789
Iteration 95/1000 | Loss: 0.00003789
Iteration 96/1000 | Loss: 0.00003789
Iteration 97/1000 | Loss: 0.00003789
Iteration 98/1000 | Loss: 0.00003788
Iteration 99/1000 | Loss: 0.00003788
Iteration 100/1000 | Loss: 0.00003788
Iteration 101/1000 | Loss: 0.00003788
Iteration 102/1000 | Loss: 0.00003787
Iteration 103/1000 | Loss: 0.00003787
Iteration 104/1000 | Loss: 0.00003787
Iteration 105/1000 | Loss: 0.00003787
Iteration 106/1000 | Loss: 0.00003787
Iteration 107/1000 | Loss: 0.00003787
Iteration 108/1000 | Loss: 0.00003787
Iteration 109/1000 | Loss: 0.00003787
Iteration 110/1000 | Loss: 0.00003787
Iteration 111/1000 | Loss: 0.00003787
Iteration 112/1000 | Loss: 0.00003787
Iteration 113/1000 | Loss: 0.00003787
Iteration 114/1000 | Loss: 0.00003787
Iteration 115/1000 | Loss: 0.00003787
Iteration 116/1000 | Loss: 0.00003787
Iteration 117/1000 | Loss: 0.00003787
Iteration 118/1000 | Loss: 0.00003787
Iteration 119/1000 | Loss: 0.00003787
Iteration 120/1000 | Loss: 0.00003787
Iteration 121/1000 | Loss: 0.00003787
Iteration 122/1000 | Loss: 0.00003786
Iteration 123/1000 | Loss: 0.00003786
Iteration 124/1000 | Loss: 0.00003786
Iteration 125/1000 | Loss: 0.00003786
Iteration 126/1000 | Loss: 0.00003786
Iteration 127/1000 | Loss: 0.00003786
Iteration 128/1000 | Loss: 0.00003786
Iteration 129/1000 | Loss: 0.00003786
Iteration 130/1000 | Loss: 0.00003786
Iteration 131/1000 | Loss: 0.00003786
Iteration 132/1000 | Loss: 0.00003786
Iteration 133/1000 | Loss: 0.00003786
Iteration 134/1000 | Loss: 0.00003786
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 134. Stopping optimization.
Last 5 losses: [3.7861409509787336e-05, 3.7861409509787336e-05, 3.7861409509787336e-05, 3.7861409509787336e-05, 3.7861409509787336e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.7861409509787336e-05

Optimization complete. Final v2v error: 5.32858943939209 mm

Highest mean error: 5.968080043792725 mm for frame 146

Lowest mean error: 5.137990474700928 mm for frame 21

Saving results

Total time: 54.84887456893921
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_56_us_1894/0005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_56_us_1894/0005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_56_us_1894/0005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01263729
Iteration 2/25 | Loss: 0.00285196
Iteration 3/25 | Loss: 0.00232043
Iteration 4/25 | Loss: 0.00218589
Iteration 5/25 | Loss: 0.00216021
Iteration 6/25 | Loss: 0.00213382
Iteration 7/25 | Loss: 0.00207386
Iteration 8/25 | Loss: 0.00206856
Iteration 9/25 | Loss: 0.00206660
Iteration 10/25 | Loss: 0.00206591
Iteration 11/25 | Loss: 0.00206581
Iteration 12/25 | Loss: 0.00206580
Iteration 13/25 | Loss: 0.00206580
Iteration 14/25 | Loss: 0.00206580
Iteration 15/25 | Loss: 0.00206580
Iteration 16/25 | Loss: 0.00206580
Iteration 17/25 | Loss: 0.00206580
Iteration 18/25 | Loss: 0.00206580
Iteration 19/25 | Loss: 0.00206580
Iteration 20/25 | Loss: 0.00206580
Iteration 21/25 | Loss: 0.00206580
Iteration 22/25 | Loss: 0.00206580
Iteration 23/25 | Loss: 0.00206580
Iteration 24/25 | Loss: 0.00206580
Iteration 25/25 | Loss: 0.00206580

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.84777880
Iteration 2/25 | Loss: 0.00263459
Iteration 3/25 | Loss: 0.00263459
Iteration 4/25 | Loss: 0.00263459
Iteration 5/25 | Loss: 0.00263459
Iteration 6/25 | Loss: 0.00263459
Iteration 7/25 | Loss: 0.00263459
Iteration 8/25 | Loss: 0.00263459
Iteration 9/25 | Loss: 0.00263459
Iteration 10/25 | Loss: 0.00263459
Iteration 11/25 | Loss: 0.00263459
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.002634590957313776, 0.002634590957313776, 0.002634590957313776, 0.002634590957313776, 0.002634590957313776]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002634590957313776

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00263459
Iteration 2/1000 | Loss: 0.00018155
Iteration 3/1000 | Loss: 0.00012719
Iteration 4/1000 | Loss: 0.00012445
Iteration 5/1000 | Loss: 0.00014248
Iteration 6/1000 | Loss: 0.00009330
Iteration 7/1000 | Loss: 0.00009113
Iteration 8/1000 | Loss: 0.00036286
Iteration 9/1000 | Loss: 0.00034574
Iteration 10/1000 | Loss: 0.00009666
Iteration 11/1000 | Loss: 0.00008910
Iteration 12/1000 | Loss: 0.00008526
Iteration 13/1000 | Loss: 0.00008216
Iteration 14/1000 | Loss: 0.00010047
Iteration 15/1000 | Loss: 0.00008533
Iteration 16/1000 | Loss: 0.00008001
Iteration 17/1000 | Loss: 0.00007954
Iteration 18/1000 | Loss: 0.00024473
Iteration 19/1000 | Loss: 0.00018171
Iteration 20/1000 | Loss: 0.00020138
Iteration 21/1000 | Loss: 0.00017438
Iteration 22/1000 | Loss: 0.00016483
Iteration 23/1000 | Loss: 0.00020478
Iteration 24/1000 | Loss: 0.00016054
Iteration 25/1000 | Loss: 0.00008426
Iteration 26/1000 | Loss: 0.00008155
Iteration 27/1000 | Loss: 0.00020636
Iteration 28/1000 | Loss: 0.00008243
Iteration 29/1000 | Loss: 0.00020428
Iteration 30/1000 | Loss: 0.00022605
Iteration 31/1000 | Loss: 0.00008236
Iteration 32/1000 | Loss: 0.00011909
Iteration 33/1000 | Loss: 0.00007727
Iteration 34/1000 | Loss: 0.00007633
Iteration 35/1000 | Loss: 0.00007584
Iteration 36/1000 | Loss: 0.00007535
Iteration 37/1000 | Loss: 0.00007516
Iteration 38/1000 | Loss: 0.00007502
Iteration 39/1000 | Loss: 0.00007491
Iteration 40/1000 | Loss: 0.00007478
Iteration 41/1000 | Loss: 0.00007464
Iteration 42/1000 | Loss: 0.00007449
Iteration 43/1000 | Loss: 0.00007445
Iteration 44/1000 | Loss: 0.00007441
Iteration 45/1000 | Loss: 0.00007441
Iteration 46/1000 | Loss: 0.00007437
Iteration 47/1000 | Loss: 0.00007423
Iteration 48/1000 | Loss: 0.00009446
Iteration 49/1000 | Loss: 0.00007404
Iteration 50/1000 | Loss: 0.00007386
Iteration 51/1000 | Loss: 0.00010254
Iteration 52/1000 | Loss: 0.00007416
Iteration 53/1000 | Loss: 0.00007367
Iteration 54/1000 | Loss: 0.00007365
Iteration 55/1000 | Loss: 0.00007365
Iteration 56/1000 | Loss: 0.00007365
Iteration 57/1000 | Loss: 0.00007365
Iteration 58/1000 | Loss: 0.00007365
Iteration 59/1000 | Loss: 0.00007364
Iteration 60/1000 | Loss: 0.00007364
Iteration 61/1000 | Loss: 0.00007364
Iteration 62/1000 | Loss: 0.00007364
Iteration 63/1000 | Loss: 0.00007364
Iteration 64/1000 | Loss: 0.00007364
Iteration 65/1000 | Loss: 0.00007364
Iteration 66/1000 | Loss: 0.00007364
Iteration 67/1000 | Loss: 0.00007364
Iteration 68/1000 | Loss: 0.00007363
Iteration 69/1000 | Loss: 0.00007363
Iteration 70/1000 | Loss: 0.00007363
Iteration 71/1000 | Loss: 0.00007363
Iteration 72/1000 | Loss: 0.00007362
Iteration 73/1000 | Loss: 0.00007362
Iteration 74/1000 | Loss: 0.00007362
Iteration 75/1000 | Loss: 0.00007362
Iteration 76/1000 | Loss: 0.00007362
Iteration 77/1000 | Loss: 0.00007362
Iteration 78/1000 | Loss: 0.00007362
Iteration 79/1000 | Loss: 0.00007362
Iteration 80/1000 | Loss: 0.00007362
Iteration 81/1000 | Loss: 0.00007362
Iteration 82/1000 | Loss: 0.00007362
Iteration 83/1000 | Loss: 0.00007362
Iteration 84/1000 | Loss: 0.00007362
Iteration 85/1000 | Loss: 0.00007361
Iteration 86/1000 | Loss: 0.00007361
Iteration 87/1000 | Loss: 0.00007361
Iteration 88/1000 | Loss: 0.00007361
Iteration 89/1000 | Loss: 0.00007361
Iteration 90/1000 | Loss: 0.00007361
Iteration 91/1000 | Loss: 0.00007361
Iteration 92/1000 | Loss: 0.00007361
Iteration 93/1000 | Loss: 0.00007360
Iteration 94/1000 | Loss: 0.00007360
Iteration 95/1000 | Loss: 0.00007360
Iteration 96/1000 | Loss: 0.00007360
Iteration 97/1000 | Loss: 0.00007360
Iteration 98/1000 | Loss: 0.00007360
Iteration 99/1000 | Loss: 0.00007360
Iteration 100/1000 | Loss: 0.00007360
Iteration 101/1000 | Loss: 0.00007360
Iteration 102/1000 | Loss: 0.00007359
Iteration 103/1000 | Loss: 0.00007359
Iteration 104/1000 | Loss: 0.00007359
Iteration 105/1000 | Loss: 0.00007359
Iteration 106/1000 | Loss: 0.00007359
Iteration 107/1000 | Loss: 0.00007359
Iteration 108/1000 | Loss: 0.00007359
Iteration 109/1000 | Loss: 0.00007359
Iteration 110/1000 | Loss: 0.00007359
Iteration 111/1000 | Loss: 0.00007358
Iteration 112/1000 | Loss: 0.00007358
Iteration 113/1000 | Loss: 0.00007358
Iteration 114/1000 | Loss: 0.00007358
Iteration 115/1000 | Loss: 0.00007358
Iteration 116/1000 | Loss: 0.00007357
Iteration 117/1000 | Loss: 0.00007357
Iteration 118/1000 | Loss: 0.00007357
Iteration 119/1000 | Loss: 0.00007357
Iteration 120/1000 | Loss: 0.00007357
Iteration 121/1000 | Loss: 0.00007357
Iteration 122/1000 | Loss: 0.00007357
Iteration 123/1000 | Loss: 0.00007357
Iteration 124/1000 | Loss: 0.00007357
Iteration 125/1000 | Loss: 0.00007357
Iteration 126/1000 | Loss: 0.00007357
Iteration 127/1000 | Loss: 0.00007357
Iteration 128/1000 | Loss: 0.00007357
Iteration 129/1000 | Loss: 0.00007357
Iteration 130/1000 | Loss: 0.00007357
Iteration 131/1000 | Loss: 0.00007357
Iteration 132/1000 | Loss: 0.00007357
Iteration 133/1000 | Loss: 0.00007357
Iteration 134/1000 | Loss: 0.00007357
Iteration 135/1000 | Loss: 0.00007357
Iteration 136/1000 | Loss: 0.00007357
Iteration 137/1000 | Loss: 0.00007357
Iteration 138/1000 | Loss: 0.00007357
Iteration 139/1000 | Loss: 0.00007357
Iteration 140/1000 | Loss: 0.00007357
Iteration 141/1000 | Loss: 0.00007357
Iteration 142/1000 | Loss: 0.00007357
Iteration 143/1000 | Loss: 0.00007357
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 143. Stopping optimization.
Last 5 losses: [7.356933929258958e-05, 7.356933929258958e-05, 7.356933929258958e-05, 7.356933929258958e-05, 7.356933929258958e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 7.356933929258958e-05

Optimization complete. Final v2v error: 7.038547992706299 mm

Highest mean error: 13.366085052490234 mm for frame 117

Lowest mean error: 5.959591388702393 mm for frame 135

Saving results

Total time: 105.91212725639343
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_56_us_1894/0010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_56_us_1894/0010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_56_us_1894/0010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00727677
Iteration 2/25 | Loss: 0.00231922
Iteration 3/25 | Loss: 0.00216365
Iteration 4/25 | Loss: 0.00214181
Iteration 5/25 | Loss: 0.00214210
Iteration 6/25 | Loss: 0.00213739
Iteration 7/25 | Loss: 0.00212193
Iteration 8/25 | Loss: 0.00211330
Iteration 9/25 | Loss: 0.00211214
Iteration 10/25 | Loss: 0.00211190
Iteration 11/25 | Loss: 0.00211171
Iteration 12/25 | Loss: 0.00211156
Iteration 13/25 | Loss: 0.00211357
Iteration 14/25 | Loss: 0.00211042
Iteration 15/25 | Loss: 0.00210968
Iteration 16/25 | Loss: 0.00210959
Iteration 17/25 | Loss: 0.00210959
Iteration 18/25 | Loss: 0.00210959
Iteration 19/25 | Loss: 0.00210959
Iteration 20/25 | Loss: 0.00210959
Iteration 21/25 | Loss: 0.00210959
Iteration 22/25 | Loss: 0.00210959
Iteration 23/25 | Loss: 0.00210959
Iteration 24/25 | Loss: 0.00210959
Iteration 25/25 | Loss: 0.00210958

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.65058231
Iteration 2/25 | Loss: 0.00306211
Iteration 3/25 | Loss: 0.00284141
Iteration 4/25 | Loss: 0.00284141
Iteration 5/25 | Loss: 0.00284141
Iteration 6/25 | Loss: 0.00284141
Iteration 7/25 | Loss: 0.00284141
Iteration 8/25 | Loss: 0.00284141
Iteration 9/25 | Loss: 0.00284141
Iteration 10/25 | Loss: 0.00284141
Iteration 11/25 | Loss: 0.00284141
Iteration 12/25 | Loss: 0.00284140
Iteration 13/25 | Loss: 0.00284140
Iteration 14/25 | Loss: 0.00284140
Iteration 15/25 | Loss: 0.00284140
Iteration 16/25 | Loss: 0.00284140
Iteration 17/25 | Loss: 0.00284140
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0028414048720151186, 0.0028414048720151186, 0.0028414048720151186, 0.0028414048720151186, 0.0028414048720151186]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0028414048720151186

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00284140
Iteration 2/1000 | Loss: 0.00023736
Iteration 3/1000 | Loss: 0.00008606
Iteration 4/1000 | Loss: 0.00013814
Iteration 5/1000 | Loss: 0.00005912
Iteration 6/1000 | Loss: 0.00012477
Iteration 7/1000 | Loss: 0.00005529
Iteration 8/1000 | Loss: 0.00004913
Iteration 9/1000 | Loss: 0.00004811
Iteration 10/1000 | Loss: 0.00004709
Iteration 11/1000 | Loss: 0.00047788
Iteration 12/1000 | Loss: 0.00025479
Iteration 13/1000 | Loss: 0.00006278
Iteration 14/1000 | Loss: 0.00004928
Iteration 15/1000 | Loss: 0.00043803
Iteration 16/1000 | Loss: 0.00005121
Iteration 17/1000 | Loss: 0.00006344
Iteration 18/1000 | Loss: 0.00005482
Iteration 19/1000 | Loss: 0.00004471
Iteration 20/1000 | Loss: 0.00004358
Iteration 21/1000 | Loss: 0.00006464
Iteration 22/1000 | Loss: 0.00004274
Iteration 23/1000 | Loss: 0.00004238
Iteration 24/1000 | Loss: 0.00004218
Iteration 25/1000 | Loss: 0.00004214
Iteration 26/1000 | Loss: 0.00004194
Iteration 27/1000 | Loss: 0.00004194
Iteration 28/1000 | Loss: 0.00004192
Iteration 29/1000 | Loss: 0.00004191
Iteration 30/1000 | Loss: 0.00004182
Iteration 31/1000 | Loss: 0.00004169
Iteration 32/1000 | Loss: 0.00004155
Iteration 33/1000 | Loss: 0.00004147
Iteration 34/1000 | Loss: 0.00004146
Iteration 35/1000 | Loss: 0.00004146
Iteration 36/1000 | Loss: 0.00004146
Iteration 37/1000 | Loss: 0.00004145
Iteration 38/1000 | Loss: 0.00004143
Iteration 39/1000 | Loss: 0.00004142
Iteration 40/1000 | Loss: 0.00006693
Iteration 41/1000 | Loss: 0.00005009
Iteration 42/1000 | Loss: 0.00005297
Iteration 43/1000 | Loss: 0.00004125
Iteration 44/1000 | Loss: 0.00004125
Iteration 45/1000 | Loss: 0.00004125
Iteration 46/1000 | Loss: 0.00004125
Iteration 47/1000 | Loss: 0.00004125
Iteration 48/1000 | Loss: 0.00004125
Iteration 49/1000 | Loss: 0.00004125
Iteration 50/1000 | Loss: 0.00004125
Iteration 51/1000 | Loss: 0.00004125
Iteration 52/1000 | Loss: 0.00004125
Iteration 53/1000 | Loss: 0.00004125
Iteration 54/1000 | Loss: 0.00004125
Iteration 55/1000 | Loss: 0.00004124
Iteration 56/1000 | Loss: 0.00004124
Iteration 57/1000 | Loss: 0.00004124
Iteration 58/1000 | Loss: 0.00004124
Iteration 59/1000 | Loss: 0.00004124
Iteration 60/1000 | Loss: 0.00004122
Iteration 61/1000 | Loss: 0.00004121
Iteration 62/1000 | Loss: 0.00004121
Iteration 63/1000 | Loss: 0.00004120
Iteration 64/1000 | Loss: 0.00004120
Iteration 65/1000 | Loss: 0.00004120
Iteration 66/1000 | Loss: 0.00004119
Iteration 67/1000 | Loss: 0.00004119
Iteration 68/1000 | Loss: 0.00004118
Iteration 69/1000 | Loss: 0.00004118
Iteration 70/1000 | Loss: 0.00004118
Iteration 71/1000 | Loss: 0.00004117
Iteration 72/1000 | Loss: 0.00004117
Iteration 73/1000 | Loss: 0.00004117
Iteration 74/1000 | Loss: 0.00004116
Iteration 75/1000 | Loss: 0.00004116
Iteration 76/1000 | Loss: 0.00004116
Iteration 77/1000 | Loss: 0.00004115
Iteration 78/1000 | Loss: 0.00004115
Iteration 79/1000 | Loss: 0.00004115
Iteration 80/1000 | Loss: 0.00004115
Iteration 81/1000 | Loss: 0.00004115
Iteration 82/1000 | Loss: 0.00004115
Iteration 83/1000 | Loss: 0.00004115
Iteration 84/1000 | Loss: 0.00004114
Iteration 85/1000 | Loss: 0.00004114
Iteration 86/1000 | Loss: 0.00004114
Iteration 87/1000 | Loss: 0.00004114
Iteration 88/1000 | Loss: 0.00004114
Iteration 89/1000 | Loss: 0.00004114
Iteration 90/1000 | Loss: 0.00004114
Iteration 91/1000 | Loss: 0.00004113
Iteration 92/1000 | Loss: 0.00004113
Iteration 93/1000 | Loss: 0.00004113
Iteration 94/1000 | Loss: 0.00004113
Iteration 95/1000 | Loss: 0.00004113
Iteration 96/1000 | Loss: 0.00004113
Iteration 97/1000 | Loss: 0.00004112
Iteration 98/1000 | Loss: 0.00004112
Iteration 99/1000 | Loss: 0.00004112
Iteration 100/1000 | Loss: 0.00004112
Iteration 101/1000 | Loss: 0.00004111
Iteration 102/1000 | Loss: 0.00004111
Iteration 103/1000 | Loss: 0.00004111
Iteration 104/1000 | Loss: 0.00004111
Iteration 105/1000 | Loss: 0.00004111
Iteration 106/1000 | Loss: 0.00004111
Iteration 107/1000 | Loss: 0.00004111
Iteration 108/1000 | Loss: 0.00004111
Iteration 109/1000 | Loss: 0.00004111
Iteration 110/1000 | Loss: 0.00004111
Iteration 111/1000 | Loss: 0.00004111
Iteration 112/1000 | Loss: 0.00004111
Iteration 113/1000 | Loss: 0.00004111
Iteration 114/1000 | Loss: 0.00004110
Iteration 115/1000 | Loss: 0.00004110
Iteration 116/1000 | Loss: 0.00004110
Iteration 117/1000 | Loss: 0.00004110
Iteration 118/1000 | Loss: 0.00004110
Iteration 119/1000 | Loss: 0.00004110
Iteration 120/1000 | Loss: 0.00004110
Iteration 121/1000 | Loss: 0.00004110
Iteration 122/1000 | Loss: 0.00004109
Iteration 123/1000 | Loss: 0.00004109
Iteration 124/1000 | Loss: 0.00004109
Iteration 125/1000 | Loss: 0.00004109
Iteration 126/1000 | Loss: 0.00004109
Iteration 127/1000 | Loss: 0.00004109
Iteration 128/1000 | Loss: 0.00004109
Iteration 129/1000 | Loss: 0.00004109
Iteration 130/1000 | Loss: 0.00004109
Iteration 131/1000 | Loss: 0.00004109
Iteration 132/1000 | Loss: 0.00004109
Iteration 133/1000 | Loss: 0.00004109
Iteration 134/1000 | Loss: 0.00004109
Iteration 135/1000 | Loss: 0.00004109
Iteration 136/1000 | Loss: 0.00004109
Iteration 137/1000 | Loss: 0.00004109
Iteration 138/1000 | Loss: 0.00004109
Iteration 139/1000 | Loss: 0.00004109
Iteration 140/1000 | Loss: 0.00004109
Iteration 141/1000 | Loss: 0.00004109
Iteration 142/1000 | Loss: 0.00004109
Iteration 143/1000 | Loss: 0.00004109
Iteration 144/1000 | Loss: 0.00004109
Iteration 145/1000 | Loss: 0.00004109
Iteration 146/1000 | Loss: 0.00004109
Iteration 147/1000 | Loss: 0.00004109
Iteration 148/1000 | Loss: 0.00004109
Iteration 149/1000 | Loss: 0.00004109
Iteration 150/1000 | Loss: 0.00004109
Iteration 151/1000 | Loss: 0.00004109
Iteration 152/1000 | Loss: 0.00004109
Iteration 153/1000 | Loss: 0.00004109
Iteration 154/1000 | Loss: 0.00004109
Iteration 155/1000 | Loss: 0.00004109
Iteration 156/1000 | Loss: 0.00004109
Iteration 157/1000 | Loss: 0.00004109
Iteration 158/1000 | Loss: 0.00004109
Iteration 159/1000 | Loss: 0.00004109
Iteration 160/1000 | Loss: 0.00004109
Iteration 161/1000 | Loss: 0.00004109
Iteration 162/1000 | Loss: 0.00004109
Iteration 163/1000 | Loss: 0.00004109
Iteration 164/1000 | Loss: 0.00004109
Iteration 165/1000 | Loss: 0.00004109
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 165. Stopping optimization.
Last 5 losses: [4.108791472390294e-05, 4.108791472390294e-05, 4.108791472390294e-05, 4.108791472390294e-05, 4.108791472390294e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.108791472390294e-05

Optimization complete. Final v2v error: 5.613803863525391 mm

Highest mean error: 6.163432598114014 mm for frame 64

Lowest mean error: 5.197223663330078 mm for frame 23

Saving results

Total time: 91.9539725780487
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_56_us_1894/0004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_56_us_1894/0004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_56_us_1894/0004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00533670
Iteration 2/25 | Loss: 0.00212591
Iteration 3/25 | Loss: 0.00203814
Iteration 4/25 | Loss: 0.00202896
Iteration 5/25 | Loss: 0.00202528
Iteration 6/25 | Loss: 0.00202491
Iteration 7/25 | Loss: 0.00202491
Iteration 8/25 | Loss: 0.00202491
Iteration 9/25 | Loss: 0.00202491
Iteration 10/25 | Loss: 0.00202491
Iteration 11/25 | Loss: 0.00202491
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.002024914138019085, 0.002024914138019085, 0.002024914138019085, 0.002024914138019085, 0.002024914138019085]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002024914138019085

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.23645329
Iteration 2/25 | Loss: 0.00219769
Iteration 3/25 | Loss: 0.00219769
Iteration 4/25 | Loss: 0.00219769
Iteration 5/25 | Loss: 0.00219769
Iteration 6/25 | Loss: 0.00219769
Iteration 7/25 | Loss: 0.00219769
Iteration 8/25 | Loss: 0.00219769
Iteration 9/25 | Loss: 0.00219768
Iteration 10/25 | Loss: 0.00219768
Iteration 11/25 | Loss: 0.00219768
Iteration 12/25 | Loss: 0.00219768
Iteration 13/25 | Loss: 0.00219768
Iteration 14/25 | Loss: 0.00219768
Iteration 15/25 | Loss: 0.00219768
Iteration 16/25 | Loss: 0.00219768
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0021976844873279333, 0.0021976844873279333, 0.0021976844873279333, 0.0021976844873279333, 0.0021976844873279333]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0021976844873279333

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00219768
Iteration 2/1000 | Loss: 0.00010865
Iteration 3/1000 | Loss: 0.00006937
Iteration 4/1000 | Loss: 0.00005919
Iteration 5/1000 | Loss: 0.00005477
Iteration 6/1000 | Loss: 0.00005235
Iteration 7/1000 | Loss: 0.00004972
Iteration 8/1000 | Loss: 0.00004796
Iteration 9/1000 | Loss: 0.00004686
Iteration 10/1000 | Loss: 0.00004614
Iteration 11/1000 | Loss: 0.00004554
Iteration 12/1000 | Loss: 0.00004506
Iteration 13/1000 | Loss: 0.00004468
Iteration 14/1000 | Loss: 0.00004438
Iteration 15/1000 | Loss: 0.00004420
Iteration 16/1000 | Loss: 0.00004420
Iteration 17/1000 | Loss: 0.00004411
Iteration 18/1000 | Loss: 0.00004405
Iteration 19/1000 | Loss: 0.00004401
Iteration 20/1000 | Loss: 0.00004395
Iteration 21/1000 | Loss: 0.00004389
Iteration 22/1000 | Loss: 0.00004388
Iteration 23/1000 | Loss: 0.00004377
Iteration 24/1000 | Loss: 0.00004373
Iteration 25/1000 | Loss: 0.00004372
Iteration 26/1000 | Loss: 0.00004372
Iteration 27/1000 | Loss: 0.00004371
Iteration 28/1000 | Loss: 0.00004371
Iteration 29/1000 | Loss: 0.00004371
Iteration 30/1000 | Loss: 0.00004371
Iteration 31/1000 | Loss: 0.00004370
Iteration 32/1000 | Loss: 0.00004370
Iteration 33/1000 | Loss: 0.00004369
Iteration 34/1000 | Loss: 0.00004369
Iteration 35/1000 | Loss: 0.00004369
Iteration 36/1000 | Loss: 0.00004369
Iteration 37/1000 | Loss: 0.00004369
Iteration 38/1000 | Loss: 0.00004369
Iteration 39/1000 | Loss: 0.00004369
Iteration 40/1000 | Loss: 0.00004369
Iteration 41/1000 | Loss: 0.00004369
Iteration 42/1000 | Loss: 0.00004369
Iteration 43/1000 | Loss: 0.00004369
Iteration 44/1000 | Loss: 0.00004369
Iteration 45/1000 | Loss: 0.00004368
Iteration 46/1000 | Loss: 0.00004368
Iteration 47/1000 | Loss: 0.00004368
Iteration 48/1000 | Loss: 0.00004368
Iteration 49/1000 | Loss: 0.00004368
Iteration 50/1000 | Loss: 0.00004367
Iteration 51/1000 | Loss: 0.00004367
Iteration 52/1000 | Loss: 0.00004367
Iteration 53/1000 | Loss: 0.00004367
Iteration 54/1000 | Loss: 0.00004366
Iteration 55/1000 | Loss: 0.00004366
Iteration 56/1000 | Loss: 0.00004366
Iteration 57/1000 | Loss: 0.00004365
Iteration 58/1000 | Loss: 0.00004365
Iteration 59/1000 | Loss: 0.00004365
Iteration 60/1000 | Loss: 0.00004365
Iteration 61/1000 | Loss: 0.00004365
Iteration 62/1000 | Loss: 0.00004364
Iteration 63/1000 | Loss: 0.00004364
Iteration 64/1000 | Loss: 0.00004364
Iteration 65/1000 | Loss: 0.00004364
Iteration 66/1000 | Loss: 0.00004364
Iteration 67/1000 | Loss: 0.00004364
Iteration 68/1000 | Loss: 0.00004364
Iteration 69/1000 | Loss: 0.00004364
Iteration 70/1000 | Loss: 0.00004364
Iteration 71/1000 | Loss: 0.00004364
Iteration 72/1000 | Loss: 0.00004364
Iteration 73/1000 | Loss: 0.00004363
Iteration 74/1000 | Loss: 0.00004363
Iteration 75/1000 | Loss: 0.00004363
Iteration 76/1000 | Loss: 0.00004363
Iteration 77/1000 | Loss: 0.00004363
Iteration 78/1000 | Loss: 0.00004363
Iteration 79/1000 | Loss: 0.00004363
Iteration 80/1000 | Loss: 0.00004362
Iteration 81/1000 | Loss: 0.00004362
Iteration 82/1000 | Loss: 0.00004362
Iteration 83/1000 | Loss: 0.00004362
Iteration 84/1000 | Loss: 0.00004362
Iteration 85/1000 | Loss: 0.00004361
Iteration 86/1000 | Loss: 0.00004361
Iteration 87/1000 | Loss: 0.00004361
Iteration 88/1000 | Loss: 0.00004361
Iteration 89/1000 | Loss: 0.00004361
Iteration 90/1000 | Loss: 0.00004361
Iteration 91/1000 | Loss: 0.00004361
Iteration 92/1000 | Loss: 0.00004361
Iteration 93/1000 | Loss: 0.00004361
Iteration 94/1000 | Loss: 0.00004361
Iteration 95/1000 | Loss: 0.00004361
Iteration 96/1000 | Loss: 0.00004361
Iteration 97/1000 | Loss: 0.00004360
Iteration 98/1000 | Loss: 0.00004360
Iteration 99/1000 | Loss: 0.00004360
Iteration 100/1000 | Loss: 0.00004360
Iteration 101/1000 | Loss: 0.00004360
Iteration 102/1000 | Loss: 0.00004360
Iteration 103/1000 | Loss: 0.00004360
Iteration 104/1000 | Loss: 0.00004360
Iteration 105/1000 | Loss: 0.00004360
Iteration 106/1000 | Loss: 0.00004360
Iteration 107/1000 | Loss: 0.00004359
Iteration 108/1000 | Loss: 0.00004359
Iteration 109/1000 | Loss: 0.00004359
Iteration 110/1000 | Loss: 0.00004359
Iteration 111/1000 | Loss: 0.00004359
Iteration 112/1000 | Loss: 0.00004359
Iteration 113/1000 | Loss: 0.00004359
Iteration 114/1000 | Loss: 0.00004359
Iteration 115/1000 | Loss: 0.00004359
Iteration 116/1000 | Loss: 0.00004359
Iteration 117/1000 | Loss: 0.00004359
Iteration 118/1000 | Loss: 0.00004359
Iteration 119/1000 | Loss: 0.00004359
Iteration 120/1000 | Loss: 0.00004358
Iteration 121/1000 | Loss: 0.00004358
Iteration 122/1000 | Loss: 0.00004358
Iteration 123/1000 | Loss: 0.00004358
Iteration 124/1000 | Loss: 0.00004358
Iteration 125/1000 | Loss: 0.00004358
Iteration 126/1000 | Loss: 0.00004358
Iteration 127/1000 | Loss: 0.00004358
Iteration 128/1000 | Loss: 0.00004358
Iteration 129/1000 | Loss: 0.00004358
Iteration 130/1000 | Loss: 0.00004358
Iteration 131/1000 | Loss: 0.00004358
Iteration 132/1000 | Loss: 0.00004358
Iteration 133/1000 | Loss: 0.00004358
Iteration 134/1000 | Loss: 0.00004358
Iteration 135/1000 | Loss: 0.00004358
Iteration 136/1000 | Loss: 0.00004358
Iteration 137/1000 | Loss: 0.00004358
Iteration 138/1000 | Loss: 0.00004358
Iteration 139/1000 | Loss: 0.00004358
Iteration 140/1000 | Loss: 0.00004358
Iteration 141/1000 | Loss: 0.00004358
Iteration 142/1000 | Loss: 0.00004358
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 142. Stopping optimization.
Last 5 losses: [4.35771798947826e-05, 4.35771798947826e-05, 4.35771798947826e-05, 4.35771798947826e-05, 4.35771798947826e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.35771798947826e-05

Optimization complete. Final v2v error: 5.610847473144531 mm

Highest mean error: 6.180467128753662 mm for frame 23

Lowest mean error: 5.1173200607299805 mm for frame 228

Saving results

Total time: 47.17463421821594
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_56_us_1894/0017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_56_us_1894/0017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_56_us_1894/0017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00787286
Iteration 2/25 | Loss: 0.00262424
Iteration 3/25 | Loss: 0.00248167
Iteration 4/25 | Loss: 0.00215947
Iteration 5/25 | Loss: 0.00211889
Iteration 6/25 | Loss: 0.00210947
Iteration 7/25 | Loss: 0.00210308
Iteration 8/25 | Loss: 0.00210057
Iteration 9/25 | Loss: 0.00210020
Iteration 10/25 | Loss: 0.00210000
Iteration 11/25 | Loss: 0.00209986
Iteration 12/25 | Loss: 0.00209980
Iteration 13/25 | Loss: 0.00209980
Iteration 14/25 | Loss: 0.00209980
Iteration 15/25 | Loss: 0.00209980
Iteration 16/25 | Loss: 0.00209980
Iteration 17/25 | Loss: 0.00209980
Iteration 18/25 | Loss: 0.00209980
Iteration 19/25 | Loss: 0.00209980
Iteration 20/25 | Loss: 0.00209980
Iteration 21/25 | Loss: 0.00209980
Iteration 22/25 | Loss: 0.00209980
Iteration 23/25 | Loss: 0.00209980
Iteration 24/25 | Loss: 0.00209980
Iteration 25/25 | Loss: 0.00209980

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.04322600
Iteration 2/25 | Loss: 0.00287625
Iteration 3/25 | Loss: 0.00253105
Iteration 4/25 | Loss: 0.00253105
Iteration 5/25 | Loss: 0.00253105
Iteration 6/25 | Loss: 0.00253105
Iteration 7/25 | Loss: 0.00253105
Iteration 8/25 | Loss: 0.00253105
Iteration 9/25 | Loss: 0.00253105
Iteration 10/25 | Loss: 0.00253105
Iteration 11/25 | Loss: 0.00253105
Iteration 12/25 | Loss: 0.00253105
Iteration 13/25 | Loss: 0.00253105
Iteration 14/25 | Loss: 0.00253105
Iteration 15/25 | Loss: 0.00253105
Iteration 16/25 | Loss: 0.00253105
Iteration 17/25 | Loss: 0.00253105
Iteration 18/25 | Loss: 0.00253105
Iteration 19/25 | Loss: 0.00253105
Iteration 20/25 | Loss: 0.00253105
Iteration 21/25 | Loss: 0.00253105
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0025310455821454525, 0.0025310455821454525, 0.0025310455821454525, 0.0025310455821454525, 0.0025310455821454525]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0025310455821454525

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00253105
Iteration 2/1000 | Loss: 0.00013650
Iteration 3/1000 | Loss: 0.00008696
Iteration 4/1000 | Loss: 0.00007527
Iteration 5/1000 | Loss: 0.00006884
Iteration 6/1000 | Loss: 0.00006555
Iteration 7/1000 | Loss: 0.00006325
Iteration 8/1000 | Loss: 0.00006167
Iteration 9/1000 | Loss: 0.00006035
Iteration 10/1000 | Loss: 0.00005915
Iteration 11/1000 | Loss: 0.00005809
Iteration 12/1000 | Loss: 0.00005735
Iteration 13/1000 | Loss: 0.00005678
Iteration 14/1000 | Loss: 0.00005632
Iteration 15/1000 | Loss: 0.00005590
Iteration 16/1000 | Loss: 0.00005557
Iteration 17/1000 | Loss: 0.00005527
Iteration 18/1000 | Loss: 0.00005507
Iteration 19/1000 | Loss: 0.00005497
Iteration 20/1000 | Loss: 0.00005488
Iteration 21/1000 | Loss: 0.00005483
Iteration 22/1000 | Loss: 0.00005477
Iteration 23/1000 | Loss: 0.00005467
Iteration 24/1000 | Loss: 0.00005460
Iteration 25/1000 | Loss: 0.00005459
Iteration 26/1000 | Loss: 0.00005458
Iteration 27/1000 | Loss: 0.00005458
Iteration 28/1000 | Loss: 0.00005455
Iteration 29/1000 | Loss: 0.00005454
Iteration 30/1000 | Loss: 0.00005451
Iteration 31/1000 | Loss: 0.00005450
Iteration 32/1000 | Loss: 0.00005449
Iteration 33/1000 | Loss: 0.00005449
Iteration 34/1000 | Loss: 0.00005449
Iteration 35/1000 | Loss: 0.00005448
Iteration 36/1000 | Loss: 0.00005447
Iteration 37/1000 | Loss: 0.00005447
Iteration 38/1000 | Loss: 0.00005446
Iteration 39/1000 | Loss: 0.00005445
Iteration 40/1000 | Loss: 0.00005444
Iteration 41/1000 | Loss: 0.00005443
Iteration 42/1000 | Loss: 0.00005442
Iteration 43/1000 | Loss: 0.00005442
Iteration 44/1000 | Loss: 0.00005442
Iteration 45/1000 | Loss: 0.00005441
Iteration 46/1000 | Loss: 0.00005441
Iteration 47/1000 | Loss: 0.00005441
Iteration 48/1000 | Loss: 0.00005439
Iteration 49/1000 | Loss: 0.00005439
Iteration 50/1000 | Loss: 0.00005438
Iteration 51/1000 | Loss: 0.00005438
Iteration 52/1000 | Loss: 0.00005436
Iteration 53/1000 | Loss: 0.00005436
Iteration 54/1000 | Loss: 0.00005436
Iteration 55/1000 | Loss: 0.00005435
Iteration 56/1000 | Loss: 0.00005435
Iteration 57/1000 | Loss: 0.00005435
Iteration 58/1000 | Loss: 0.00005435
Iteration 59/1000 | Loss: 0.00005434
Iteration 60/1000 | Loss: 0.00005434
Iteration 61/1000 | Loss: 0.00005434
Iteration 62/1000 | Loss: 0.00005434
Iteration 63/1000 | Loss: 0.00005433
Iteration 64/1000 | Loss: 0.00005433
Iteration 65/1000 | Loss: 0.00005433
Iteration 66/1000 | Loss: 0.00005433
Iteration 67/1000 | Loss: 0.00005432
Iteration 68/1000 | Loss: 0.00005431
Iteration 69/1000 | Loss: 0.00005431
Iteration 70/1000 | Loss: 0.00005431
Iteration 71/1000 | Loss: 0.00005431
Iteration 72/1000 | Loss: 0.00005431
Iteration 73/1000 | Loss: 0.00005431
Iteration 74/1000 | Loss: 0.00005430
Iteration 75/1000 | Loss: 0.00005430
Iteration 76/1000 | Loss: 0.00005430
Iteration 77/1000 | Loss: 0.00005430
Iteration 78/1000 | Loss: 0.00005430
Iteration 79/1000 | Loss: 0.00005430
Iteration 80/1000 | Loss: 0.00005430
Iteration 81/1000 | Loss: 0.00005429
Iteration 82/1000 | Loss: 0.00005429
Iteration 83/1000 | Loss: 0.00005429
Iteration 84/1000 | Loss: 0.00005429
Iteration 85/1000 | Loss: 0.00005429
Iteration 86/1000 | Loss: 0.00005429
Iteration 87/1000 | Loss: 0.00005429
Iteration 88/1000 | Loss: 0.00005428
Iteration 89/1000 | Loss: 0.00005428
Iteration 90/1000 | Loss: 0.00005428
Iteration 91/1000 | Loss: 0.00005428
Iteration 92/1000 | Loss: 0.00005428
Iteration 93/1000 | Loss: 0.00005428
Iteration 94/1000 | Loss: 0.00005428
Iteration 95/1000 | Loss: 0.00005428
Iteration 96/1000 | Loss: 0.00005428
Iteration 97/1000 | Loss: 0.00005427
Iteration 98/1000 | Loss: 0.00005427
Iteration 99/1000 | Loss: 0.00005427
Iteration 100/1000 | Loss: 0.00005427
Iteration 101/1000 | Loss: 0.00005426
Iteration 102/1000 | Loss: 0.00005426
Iteration 103/1000 | Loss: 0.00005426
Iteration 104/1000 | Loss: 0.00005426
Iteration 105/1000 | Loss: 0.00005426
Iteration 106/1000 | Loss: 0.00005426
Iteration 107/1000 | Loss: 0.00005426
Iteration 108/1000 | Loss: 0.00005426
Iteration 109/1000 | Loss: 0.00005426
Iteration 110/1000 | Loss: 0.00005425
Iteration 111/1000 | Loss: 0.00005425
Iteration 112/1000 | Loss: 0.00005425
Iteration 113/1000 | Loss: 0.00005425
Iteration 114/1000 | Loss: 0.00005425
Iteration 115/1000 | Loss: 0.00005425
Iteration 116/1000 | Loss: 0.00005425
Iteration 117/1000 | Loss: 0.00005425
Iteration 118/1000 | Loss: 0.00005424
Iteration 119/1000 | Loss: 0.00005424
Iteration 120/1000 | Loss: 0.00005424
Iteration 121/1000 | Loss: 0.00005424
Iteration 122/1000 | Loss: 0.00005424
Iteration 123/1000 | Loss: 0.00005423
Iteration 124/1000 | Loss: 0.00005423
Iteration 125/1000 | Loss: 0.00005423
Iteration 126/1000 | Loss: 0.00005423
Iteration 127/1000 | Loss: 0.00005423
Iteration 128/1000 | Loss: 0.00005423
Iteration 129/1000 | Loss: 0.00005423
Iteration 130/1000 | Loss: 0.00005423
Iteration 131/1000 | Loss: 0.00005423
Iteration 132/1000 | Loss: 0.00005423
Iteration 133/1000 | Loss: 0.00005423
Iteration 134/1000 | Loss: 0.00005422
Iteration 135/1000 | Loss: 0.00005422
Iteration 136/1000 | Loss: 0.00005422
Iteration 137/1000 | Loss: 0.00005422
Iteration 138/1000 | Loss: 0.00005422
Iteration 139/1000 | Loss: 0.00005422
Iteration 140/1000 | Loss: 0.00005422
Iteration 141/1000 | Loss: 0.00005421
Iteration 142/1000 | Loss: 0.00005421
Iteration 143/1000 | Loss: 0.00005421
Iteration 144/1000 | Loss: 0.00005421
Iteration 145/1000 | Loss: 0.00005421
Iteration 146/1000 | Loss: 0.00005421
Iteration 147/1000 | Loss: 0.00005421
Iteration 148/1000 | Loss: 0.00005421
Iteration 149/1000 | Loss: 0.00005421
Iteration 150/1000 | Loss: 0.00005421
Iteration 151/1000 | Loss: 0.00005420
Iteration 152/1000 | Loss: 0.00005420
Iteration 153/1000 | Loss: 0.00005420
Iteration 154/1000 | Loss: 0.00005420
Iteration 155/1000 | Loss: 0.00005420
Iteration 156/1000 | Loss: 0.00005419
Iteration 157/1000 | Loss: 0.00005419
Iteration 158/1000 | Loss: 0.00005419
Iteration 159/1000 | Loss: 0.00005419
Iteration 160/1000 | Loss: 0.00005419
Iteration 161/1000 | Loss: 0.00005418
Iteration 162/1000 | Loss: 0.00005418
Iteration 163/1000 | Loss: 0.00005418
Iteration 164/1000 | Loss: 0.00005418
Iteration 165/1000 | Loss: 0.00005418
Iteration 166/1000 | Loss: 0.00005417
Iteration 167/1000 | Loss: 0.00005417
Iteration 168/1000 | Loss: 0.00005417
Iteration 169/1000 | Loss: 0.00005417
Iteration 170/1000 | Loss: 0.00005417
Iteration 171/1000 | Loss: 0.00005417
Iteration 172/1000 | Loss: 0.00005417
Iteration 173/1000 | Loss: 0.00005417
Iteration 174/1000 | Loss: 0.00005416
Iteration 175/1000 | Loss: 0.00005416
Iteration 176/1000 | Loss: 0.00005416
Iteration 177/1000 | Loss: 0.00005416
Iteration 178/1000 | Loss: 0.00005416
Iteration 179/1000 | Loss: 0.00005416
Iteration 180/1000 | Loss: 0.00005416
Iteration 181/1000 | Loss: 0.00005416
Iteration 182/1000 | Loss: 0.00005416
Iteration 183/1000 | Loss: 0.00005416
Iteration 184/1000 | Loss: 0.00005416
Iteration 185/1000 | Loss: 0.00005416
Iteration 186/1000 | Loss: 0.00005416
Iteration 187/1000 | Loss: 0.00005416
Iteration 188/1000 | Loss: 0.00005416
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 188. Stopping optimization.
Last 5 losses: [5.415541090769693e-05, 5.415541090769693e-05, 5.415541090769693e-05, 5.415541090769693e-05, 5.415541090769693e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 5.415541090769693e-05

Optimization complete. Final v2v error: 6.127091884613037 mm

Highest mean error: 9.072266578674316 mm for frame 12

Lowest mean error: 5.216610908508301 mm for frame 207

Saving results

Total time: 73.75613522529602
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_56_us_1894/0020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_56_us_1894/0020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_56_us_1894/0020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01142546
Iteration 2/25 | Loss: 0.00255405
Iteration 3/25 | Loss: 0.00243222
Iteration 4/25 | Loss: 0.00211726
Iteration 5/25 | Loss: 0.00206021
Iteration 6/25 | Loss: 0.00203290
Iteration 7/25 | Loss: 0.00199291
Iteration 8/25 | Loss: 0.00195456
Iteration 9/25 | Loss: 0.00193022
Iteration 10/25 | Loss: 0.00192352
Iteration 11/25 | Loss: 0.00191863
Iteration 12/25 | Loss: 0.00191093
Iteration 13/25 | Loss: 0.00190556
Iteration 14/25 | Loss: 0.00189645
Iteration 15/25 | Loss: 0.00189461
Iteration 16/25 | Loss: 0.00189414
Iteration 17/25 | Loss: 0.00189392
Iteration 18/25 | Loss: 0.00189376
Iteration 19/25 | Loss: 0.00189375
Iteration 20/25 | Loss: 0.00189375
Iteration 21/25 | Loss: 0.00189375
Iteration 22/25 | Loss: 0.00189375
Iteration 23/25 | Loss: 0.00189375
Iteration 24/25 | Loss: 0.00189375
Iteration 25/25 | Loss: 0.00189374

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.24363518
Iteration 2/25 | Loss: 0.00232801
Iteration 3/25 | Loss: 0.00232801
Iteration 4/25 | Loss: 0.00232801
Iteration 5/25 | Loss: 0.00232801
Iteration 6/25 | Loss: 0.00232801
Iteration 7/25 | Loss: 0.00232801
Iteration 8/25 | Loss: 0.00232801
Iteration 9/25 | Loss: 0.00232801
Iteration 10/25 | Loss: 0.00232801
Iteration 11/25 | Loss: 0.00232801
Iteration 12/25 | Loss: 0.00232801
Iteration 13/25 | Loss: 0.00232801
Iteration 14/25 | Loss: 0.00232801
Iteration 15/25 | Loss: 0.00232801
Iteration 16/25 | Loss: 0.00232801
Iteration 17/25 | Loss: 0.00232801
Iteration 18/25 | Loss: 0.00232801
Iteration 19/25 | Loss: 0.00232801
Iteration 20/25 | Loss: 0.00232801
Iteration 21/25 | Loss: 0.00232801
Iteration 22/25 | Loss: 0.00232801
Iteration 23/25 | Loss: 0.00232801
Iteration 24/25 | Loss: 0.00232801
Iteration 25/25 | Loss: 0.00232801

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00232801
Iteration 2/1000 | Loss: 0.00010944
Iteration 3/1000 | Loss: 0.00008234
Iteration 4/1000 | Loss: 0.00007255
Iteration 5/1000 | Loss: 0.00005213
Iteration 6/1000 | Loss: 0.00017074
Iteration 7/1000 | Loss: 0.00033750
Iteration 8/1000 | Loss: 0.00004273
Iteration 9/1000 | Loss: 0.00004102
Iteration 10/1000 | Loss: 0.00004550
Iteration 11/1000 | Loss: 0.00003910
Iteration 12/1000 | Loss: 0.00003832
Iteration 13/1000 | Loss: 0.00003748
Iteration 14/1000 | Loss: 0.00003683
Iteration 15/1000 | Loss: 0.00003637
Iteration 16/1000 | Loss: 0.00003615
Iteration 17/1000 | Loss: 0.00003595
Iteration 18/1000 | Loss: 0.00003576
Iteration 19/1000 | Loss: 0.00003569
Iteration 20/1000 | Loss: 0.00003566
Iteration 21/1000 | Loss: 0.00003566
Iteration 22/1000 | Loss: 0.00003553
Iteration 23/1000 | Loss: 0.00003552
Iteration 24/1000 | Loss: 0.00003545
Iteration 25/1000 | Loss: 0.00003543
Iteration 26/1000 | Loss: 0.00003542
Iteration 27/1000 | Loss: 0.00003541
Iteration 28/1000 | Loss: 0.00003541
Iteration 29/1000 | Loss: 0.00003541
Iteration 30/1000 | Loss: 0.00003540
Iteration 31/1000 | Loss: 0.00003540
Iteration 32/1000 | Loss: 0.00003540
Iteration 33/1000 | Loss: 0.00003540
Iteration 34/1000 | Loss: 0.00003540
Iteration 35/1000 | Loss: 0.00003540
Iteration 36/1000 | Loss: 0.00003539
Iteration 37/1000 | Loss: 0.00003538
Iteration 38/1000 | Loss: 0.00003538
Iteration 39/1000 | Loss: 0.00003537
Iteration 40/1000 | Loss: 0.00003537
Iteration 41/1000 | Loss: 0.00003537
Iteration 42/1000 | Loss: 0.00003537
Iteration 43/1000 | Loss: 0.00003536
Iteration 44/1000 | Loss: 0.00003536
Iteration 45/1000 | Loss: 0.00003536
Iteration 46/1000 | Loss: 0.00003535
Iteration 47/1000 | Loss: 0.00003535
Iteration 48/1000 | Loss: 0.00003535
Iteration 49/1000 | Loss: 0.00003534
Iteration 50/1000 | Loss: 0.00003534
Iteration 51/1000 | Loss: 0.00003534
Iteration 52/1000 | Loss: 0.00003534
Iteration 53/1000 | Loss: 0.00003534
Iteration 54/1000 | Loss: 0.00003533
Iteration 55/1000 | Loss: 0.00003533
Iteration 56/1000 | Loss: 0.00003533
Iteration 57/1000 | Loss: 0.00003533
Iteration 58/1000 | Loss: 0.00003533
Iteration 59/1000 | Loss: 0.00003533
Iteration 60/1000 | Loss: 0.00003533
Iteration 61/1000 | Loss: 0.00003533
Iteration 62/1000 | Loss: 0.00003533
Iteration 63/1000 | Loss: 0.00003533
Iteration 64/1000 | Loss: 0.00003533
Iteration 65/1000 | Loss: 0.00003533
Iteration 66/1000 | Loss: 0.00003533
Iteration 67/1000 | Loss: 0.00003533
Iteration 68/1000 | Loss: 0.00003533
Iteration 69/1000 | Loss: 0.00003533
Iteration 70/1000 | Loss: 0.00003533
Iteration 71/1000 | Loss: 0.00003533
Iteration 72/1000 | Loss: 0.00003533
Iteration 73/1000 | Loss: 0.00003533
Iteration 74/1000 | Loss: 0.00003533
Iteration 75/1000 | Loss: 0.00003533
Iteration 76/1000 | Loss: 0.00003533
Iteration 77/1000 | Loss: 0.00003533
Iteration 78/1000 | Loss: 0.00003533
Iteration 79/1000 | Loss: 0.00003533
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 79. Stopping optimization.
Last 5 losses: [3.532910704961978e-05, 3.532910704961978e-05, 3.532910704961978e-05, 3.532910704961978e-05, 3.532910704961978e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.532910704961978e-05

Optimization complete. Final v2v error: 5.0369157791137695 mm

Highest mean error: 10.761585235595703 mm for frame 115

Lowest mean error: 4.674455642700195 mm for frame 35

Saving results

Total time: 61.25236201286316
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_56_us_1894/0015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_56_us_1894/0015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_56_us_1894/0015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00594494
Iteration 2/25 | Loss: 0.00218583
Iteration 3/25 | Loss: 0.00207657
Iteration 4/25 | Loss: 0.00206035
Iteration 5/25 | Loss: 0.00205514
Iteration 6/25 | Loss: 0.00205427
Iteration 7/25 | Loss: 0.00205427
Iteration 8/25 | Loss: 0.00205427
Iteration 9/25 | Loss: 0.00205427
Iteration 10/25 | Loss: 0.00205427
Iteration 11/25 | Loss: 0.00205427
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0020542703568935394, 0.0020542703568935394, 0.0020542703568935394, 0.0020542703568935394, 0.0020542703568935394]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0020542703568935394

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.26987267
Iteration 2/25 | Loss: 0.00216714
Iteration 3/25 | Loss: 0.00216712
Iteration 4/25 | Loss: 0.00216712
Iteration 5/25 | Loss: 0.00216712
Iteration 6/25 | Loss: 0.00216712
Iteration 7/25 | Loss: 0.00216712
Iteration 8/25 | Loss: 0.00216712
Iteration 9/25 | Loss: 0.00216712
Iteration 10/25 | Loss: 0.00216712
Iteration 11/25 | Loss: 0.00216712
Iteration 12/25 | Loss: 0.00216712
Iteration 13/25 | Loss: 0.00216712
Iteration 14/25 | Loss: 0.00216712
Iteration 15/25 | Loss: 0.00216712
Iteration 16/25 | Loss: 0.00216712
Iteration 17/25 | Loss: 0.00216712
Iteration 18/25 | Loss: 0.00216712
Iteration 19/25 | Loss: 0.00216712
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0021671184804290533, 0.0021671184804290533, 0.0021671184804290533, 0.0021671184804290533, 0.0021671184804290533]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0021671184804290533

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00216712
Iteration 2/1000 | Loss: 0.00009041
Iteration 3/1000 | Loss: 0.00005712
Iteration 4/1000 | Loss: 0.00004874
Iteration 5/1000 | Loss: 0.00004351
Iteration 6/1000 | Loss: 0.00004075
Iteration 7/1000 | Loss: 0.00003911
Iteration 8/1000 | Loss: 0.00003821
Iteration 9/1000 | Loss: 0.00003725
Iteration 10/1000 | Loss: 0.00003645
Iteration 11/1000 | Loss: 0.00003610
Iteration 12/1000 | Loss: 0.00003588
Iteration 13/1000 | Loss: 0.00003572
Iteration 14/1000 | Loss: 0.00003570
Iteration 15/1000 | Loss: 0.00003564
Iteration 16/1000 | Loss: 0.00003560
Iteration 17/1000 | Loss: 0.00003553
Iteration 18/1000 | Loss: 0.00003553
Iteration 19/1000 | Loss: 0.00003548
Iteration 20/1000 | Loss: 0.00003542
Iteration 21/1000 | Loss: 0.00003542
Iteration 22/1000 | Loss: 0.00003542
Iteration 23/1000 | Loss: 0.00003542
Iteration 24/1000 | Loss: 0.00003541
Iteration 25/1000 | Loss: 0.00003541
Iteration 26/1000 | Loss: 0.00003541
Iteration 27/1000 | Loss: 0.00003539
Iteration 28/1000 | Loss: 0.00003538
Iteration 29/1000 | Loss: 0.00003538
Iteration 30/1000 | Loss: 0.00003538
Iteration 31/1000 | Loss: 0.00003538
Iteration 32/1000 | Loss: 0.00003538
Iteration 33/1000 | Loss: 0.00003538
Iteration 34/1000 | Loss: 0.00003538
Iteration 35/1000 | Loss: 0.00003538
Iteration 36/1000 | Loss: 0.00003538
Iteration 37/1000 | Loss: 0.00003538
Iteration 38/1000 | Loss: 0.00003537
Iteration 39/1000 | Loss: 0.00003537
Iteration 40/1000 | Loss: 0.00003537
Iteration 41/1000 | Loss: 0.00003537
Iteration 42/1000 | Loss: 0.00003536
Iteration 43/1000 | Loss: 0.00003536
Iteration 44/1000 | Loss: 0.00003535
Iteration 45/1000 | Loss: 0.00003535
Iteration 46/1000 | Loss: 0.00003535
Iteration 47/1000 | Loss: 0.00003535
Iteration 48/1000 | Loss: 0.00003534
Iteration 49/1000 | Loss: 0.00003534
Iteration 50/1000 | Loss: 0.00003534
Iteration 51/1000 | Loss: 0.00003534
Iteration 52/1000 | Loss: 0.00003533
Iteration 53/1000 | Loss: 0.00003533
Iteration 54/1000 | Loss: 0.00003533
Iteration 55/1000 | Loss: 0.00003532
Iteration 56/1000 | Loss: 0.00003532
Iteration 57/1000 | Loss: 0.00003532
Iteration 58/1000 | Loss: 0.00003532
Iteration 59/1000 | Loss: 0.00003532
Iteration 60/1000 | Loss: 0.00003532
Iteration 61/1000 | Loss: 0.00003532
Iteration 62/1000 | Loss: 0.00003532
Iteration 63/1000 | Loss: 0.00003532
Iteration 64/1000 | Loss: 0.00003532
Iteration 65/1000 | Loss: 0.00003532
Iteration 66/1000 | Loss: 0.00003532
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 66. Stopping optimization.
Last 5 losses: [3.5315366403665394e-05, 3.5315366403665394e-05, 3.5315366403665394e-05, 3.5315366403665394e-05, 3.5315366403665394e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.5315366403665394e-05

Optimization complete. Final v2v error: 5.104181289672852 mm

Highest mean error: 5.718498229980469 mm for frame 47

Lowest mean error: 4.739157676696777 mm for frame 116

Saving results

Total time: 33.20860052108765
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_56_us_1894/0016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_56_us_1894/0016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_56_us_1894/0016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00853306
Iteration 2/25 | Loss: 0.00227485
Iteration 3/25 | Loss: 0.00189676
Iteration 4/25 | Loss: 0.00176314
Iteration 5/25 | Loss: 0.00177891
Iteration 6/25 | Loss: 0.00174329
Iteration 7/25 | Loss: 0.00171121
Iteration 8/25 | Loss: 0.00169970
Iteration 9/25 | Loss: 0.00168729
Iteration 10/25 | Loss: 0.00168263
Iteration 11/25 | Loss: 0.00168136
Iteration 12/25 | Loss: 0.00168056
Iteration 13/25 | Loss: 0.00168107
Iteration 14/25 | Loss: 0.00168074
Iteration 15/25 | Loss: 0.00167987
Iteration 16/25 | Loss: 0.00168002
Iteration 17/25 | Loss: 0.00167973
Iteration 18/25 | Loss: 0.00167934
Iteration 19/25 | Loss: 0.00167892
Iteration 20/25 | Loss: 0.00167871
Iteration 21/25 | Loss: 0.00167863
Iteration 22/25 | Loss: 0.00167862
Iteration 23/25 | Loss: 0.00167862
Iteration 24/25 | Loss: 0.00167862
Iteration 25/25 | Loss: 0.00167862

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.82914102
Iteration 2/25 | Loss: 0.00209935
Iteration 3/25 | Loss: 0.00209935
Iteration 4/25 | Loss: 0.00209935
Iteration 5/25 | Loss: 0.00209935
Iteration 6/25 | Loss: 0.00209935
Iteration 7/25 | Loss: 0.00209935
Iteration 8/25 | Loss: 0.00209935
Iteration 9/25 | Loss: 0.00209935
Iteration 10/25 | Loss: 0.00209935
Iteration 11/25 | Loss: 0.00209935
Iteration 12/25 | Loss: 0.00209935
Iteration 13/25 | Loss: 0.00209935
Iteration 14/25 | Loss: 0.00209935
Iteration 15/25 | Loss: 0.00209935
Iteration 16/25 | Loss: 0.00209935
Iteration 17/25 | Loss: 0.00209935
Iteration 18/25 | Loss: 0.00209935
Iteration 19/25 | Loss: 0.00209935
Iteration 20/25 | Loss: 0.00209935
Iteration 21/25 | Loss: 0.00209935
Iteration 22/25 | Loss: 0.00209935
Iteration 23/25 | Loss: 0.00209935
Iteration 24/25 | Loss: 0.00209935
Iteration 25/25 | Loss: 0.00209935

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00209935
Iteration 2/1000 | Loss: 0.00008544
Iteration 3/1000 | Loss: 0.00005848
Iteration 4/1000 | Loss: 0.00005076
Iteration 5/1000 | Loss: 0.00008803
Iteration 6/1000 | Loss: 0.00004302
Iteration 7/1000 | Loss: 0.00004166
Iteration 8/1000 | Loss: 0.00008948
Iteration 9/1000 | Loss: 0.00004032
Iteration 10/1000 | Loss: 0.00003959
Iteration 11/1000 | Loss: 0.00014230
Iteration 12/1000 | Loss: 0.00003850
Iteration 13/1000 | Loss: 0.00003815
Iteration 14/1000 | Loss: 0.00009389
Iteration 15/1000 | Loss: 0.00003769
Iteration 16/1000 | Loss: 0.00003766
Iteration 17/1000 | Loss: 0.00006529
Iteration 18/1000 | Loss: 0.00003770
Iteration 19/1000 | Loss: 0.00003748
Iteration 20/1000 | Loss: 0.00003745
Iteration 21/1000 | Loss: 0.00003726
Iteration 22/1000 | Loss: 0.00003726
Iteration 23/1000 | Loss: 0.00003720
Iteration 24/1000 | Loss: 0.00003719
Iteration 25/1000 | Loss: 0.00003718
Iteration 26/1000 | Loss: 0.00003718
Iteration 27/1000 | Loss: 0.00003712
Iteration 28/1000 | Loss: 0.00003707
Iteration 29/1000 | Loss: 0.00003707
Iteration 30/1000 | Loss: 0.00003707
Iteration 31/1000 | Loss: 0.00003706
Iteration 32/1000 | Loss: 0.00003706
Iteration 33/1000 | Loss: 0.00003706
Iteration 34/1000 | Loss: 0.00003705
Iteration 35/1000 | Loss: 0.00003704
Iteration 36/1000 | Loss: 0.00003704
Iteration 37/1000 | Loss: 0.00003703
Iteration 38/1000 | Loss: 0.00003703
Iteration 39/1000 | Loss: 0.00003703
Iteration 40/1000 | Loss: 0.00003702
Iteration 41/1000 | Loss: 0.00003702
Iteration 42/1000 | Loss: 0.00003701
Iteration 43/1000 | Loss: 0.00003701
Iteration 44/1000 | Loss: 0.00003701
Iteration 45/1000 | Loss: 0.00003700
Iteration 46/1000 | Loss: 0.00003699
Iteration 47/1000 | Loss: 0.00003699
Iteration 48/1000 | Loss: 0.00003698
Iteration 49/1000 | Loss: 0.00003698
Iteration 50/1000 | Loss: 0.00003698
Iteration 51/1000 | Loss: 0.00003697
Iteration 52/1000 | Loss: 0.00003697
Iteration 53/1000 | Loss: 0.00003695
Iteration 54/1000 | Loss: 0.00003695
Iteration 55/1000 | Loss: 0.00003691
Iteration 56/1000 | Loss: 0.00003689
Iteration 57/1000 | Loss: 0.00003689
Iteration 58/1000 | Loss: 0.00003689
Iteration 59/1000 | Loss: 0.00003689
Iteration 60/1000 | Loss: 0.00003683
Iteration 61/1000 | Loss: 0.00003683
Iteration 62/1000 | Loss: 0.00003683
Iteration 63/1000 | Loss: 0.00003683
Iteration 64/1000 | Loss: 0.00003683
Iteration 65/1000 | Loss: 0.00003683
Iteration 66/1000 | Loss: 0.00003683
Iteration 67/1000 | Loss: 0.00003683
Iteration 68/1000 | Loss: 0.00003683
Iteration 69/1000 | Loss: 0.00003683
Iteration 70/1000 | Loss: 0.00003682
Iteration 71/1000 | Loss: 0.00003682
Iteration 72/1000 | Loss: 0.00003681
Iteration 73/1000 | Loss: 0.00003681
Iteration 74/1000 | Loss: 0.00003681
Iteration 75/1000 | Loss: 0.00003680
Iteration 76/1000 | Loss: 0.00003680
Iteration 77/1000 | Loss: 0.00003680
Iteration 78/1000 | Loss: 0.00003680
Iteration 79/1000 | Loss: 0.00003680
Iteration 80/1000 | Loss: 0.00003680
Iteration 81/1000 | Loss: 0.00003680
Iteration 82/1000 | Loss: 0.00003679
Iteration 83/1000 | Loss: 0.00003679
Iteration 84/1000 | Loss: 0.00003679
Iteration 85/1000 | Loss: 0.00003679
Iteration 86/1000 | Loss: 0.00003679
Iteration 87/1000 | Loss: 0.00003678
Iteration 88/1000 | Loss: 0.00003678
Iteration 89/1000 | Loss: 0.00003678
Iteration 90/1000 | Loss: 0.00003678
Iteration 91/1000 | Loss: 0.00003678
Iteration 92/1000 | Loss: 0.00003678
Iteration 93/1000 | Loss: 0.00003677
Iteration 94/1000 | Loss: 0.00003677
Iteration 95/1000 | Loss: 0.00003677
Iteration 96/1000 | Loss: 0.00003677
Iteration 97/1000 | Loss: 0.00003677
Iteration 98/1000 | Loss: 0.00003677
Iteration 99/1000 | Loss: 0.00003677
Iteration 100/1000 | Loss: 0.00003677
Iteration 101/1000 | Loss: 0.00003676
Iteration 102/1000 | Loss: 0.00003676
Iteration 103/1000 | Loss: 0.00003676
Iteration 104/1000 | Loss: 0.00003676
Iteration 105/1000 | Loss: 0.00003675
Iteration 106/1000 | Loss: 0.00003675
Iteration 107/1000 | Loss: 0.00003675
Iteration 108/1000 | Loss: 0.00003675
Iteration 109/1000 | Loss: 0.00003675
Iteration 110/1000 | Loss: 0.00003675
Iteration 111/1000 | Loss: 0.00003675
Iteration 112/1000 | Loss: 0.00003675
Iteration 113/1000 | Loss: 0.00003675
Iteration 114/1000 | Loss: 0.00003675
Iteration 115/1000 | Loss: 0.00003675
Iteration 116/1000 | Loss: 0.00003675
Iteration 117/1000 | Loss: 0.00003675
Iteration 118/1000 | Loss: 0.00003675
Iteration 119/1000 | Loss: 0.00003675
Iteration 120/1000 | Loss: 0.00003675
Iteration 121/1000 | Loss: 0.00003675
Iteration 122/1000 | Loss: 0.00003675
Iteration 123/1000 | Loss: 0.00003675
Iteration 124/1000 | Loss: 0.00003675
Iteration 125/1000 | Loss: 0.00003675
Iteration 126/1000 | Loss: 0.00003675
Iteration 127/1000 | Loss: 0.00003675
Iteration 128/1000 | Loss: 0.00003675
Iteration 129/1000 | Loss: 0.00003675
Iteration 130/1000 | Loss: 0.00003675
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 130. Stopping optimization.
Last 5 losses: [3.6750061553902924e-05, 3.6750061553902924e-05, 3.6750061553902924e-05, 3.6750061553902924e-05, 3.6750061553902924e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.6750061553902924e-05

Optimization complete. Final v2v error: 5.080108642578125 mm

Highest mean error: 11.075492858886719 mm for frame 115

Lowest mean error: 4.517726421356201 mm for frame 11

Saving results

Total time: 70.94177222251892
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_56_us_1894/0006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_56_us_1894/0006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_56_us_1894/0006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01084953
Iteration 2/25 | Loss: 0.00244969
Iteration 3/25 | Loss: 0.00231218
Iteration 4/25 | Loss: 0.00215819
Iteration 5/25 | Loss: 0.00213319
Iteration 6/25 | Loss: 0.00212934
Iteration 7/25 | Loss: 0.00213029
Iteration 8/25 | Loss: 0.00213421
Iteration 9/25 | Loss: 0.00212455
Iteration 10/25 | Loss: 0.00210913
Iteration 11/25 | Loss: 0.00210839
Iteration 12/25 | Loss: 0.00210825
Iteration 13/25 | Loss: 0.00210825
Iteration 14/25 | Loss: 0.00210825
Iteration 15/25 | Loss: 0.00210825
Iteration 16/25 | Loss: 0.00210825
Iteration 17/25 | Loss: 0.00210825
Iteration 18/25 | Loss: 0.00210825
Iteration 19/25 | Loss: 0.00210825
Iteration 20/25 | Loss: 0.00210824
Iteration 21/25 | Loss: 0.00210824
Iteration 22/25 | Loss: 0.00210824
Iteration 23/25 | Loss: 0.00210824
Iteration 24/25 | Loss: 0.00210824
Iteration 25/25 | Loss: 0.00210824

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39299691
Iteration 2/25 | Loss: 0.00238182
Iteration 3/25 | Loss: 0.00237013
Iteration 4/25 | Loss: 0.00237013
Iteration 5/25 | Loss: 0.00237013
Iteration 6/25 | Loss: 0.00237013
Iteration 7/25 | Loss: 0.00237013
Iteration 8/25 | Loss: 0.00237013
Iteration 9/25 | Loss: 0.00237013
Iteration 10/25 | Loss: 0.00237013
Iteration 11/25 | Loss: 0.00237013
Iteration 12/25 | Loss: 0.00237013
Iteration 13/25 | Loss: 0.00237013
Iteration 14/25 | Loss: 0.00237013
Iteration 15/25 | Loss: 0.00237013
Iteration 16/25 | Loss: 0.00237013
Iteration 17/25 | Loss: 0.00237013
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.002370125148445368, 0.002370125148445368, 0.002370125148445368, 0.002370125148445368, 0.002370125148445368]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002370125148445368

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00237013
Iteration 2/1000 | Loss: 0.00012248
Iteration 3/1000 | Loss: 0.00008006
Iteration 4/1000 | Loss: 0.00006317
Iteration 5/1000 | Loss: 0.00005466
Iteration 6/1000 | Loss: 0.00005114
Iteration 7/1000 | Loss: 0.00008870
Iteration 8/1000 | Loss: 0.00006359
Iteration 9/1000 | Loss: 0.00004730
Iteration 10/1000 | Loss: 0.00004607
Iteration 11/1000 | Loss: 0.00005628
Iteration 12/1000 | Loss: 0.00005965
Iteration 13/1000 | Loss: 0.00008065
Iteration 14/1000 | Loss: 0.00004486
Iteration 15/1000 | Loss: 0.00004370
Iteration 16/1000 | Loss: 0.00004329
Iteration 17/1000 | Loss: 0.00008286
Iteration 18/1000 | Loss: 0.00004936
Iteration 19/1000 | Loss: 0.00008144
Iteration 20/1000 | Loss: 0.00004754
Iteration 21/1000 | Loss: 0.00004505
Iteration 22/1000 | Loss: 0.00004303
Iteration 23/1000 | Loss: 0.00004282
Iteration 24/1000 | Loss: 0.00004280
Iteration 25/1000 | Loss: 0.00004279
Iteration 26/1000 | Loss: 0.00004278
Iteration 27/1000 | Loss: 0.00004278
Iteration 28/1000 | Loss: 0.00004278
Iteration 29/1000 | Loss: 0.00004277
Iteration 30/1000 | Loss: 0.00004277
Iteration 31/1000 | Loss: 0.00004277
Iteration 32/1000 | Loss: 0.00004277
Iteration 33/1000 | Loss: 0.00004275
Iteration 34/1000 | Loss: 0.00004274
Iteration 35/1000 | Loss: 0.00004272
Iteration 36/1000 | Loss: 0.00004271
Iteration 37/1000 | Loss: 0.00004269
Iteration 38/1000 | Loss: 0.00004269
Iteration 39/1000 | Loss: 0.00004259
Iteration 40/1000 | Loss: 0.00004258
Iteration 41/1000 | Loss: 0.00004258
Iteration 42/1000 | Loss: 0.00004258
Iteration 43/1000 | Loss: 0.00004258
Iteration 44/1000 | Loss: 0.00004258
Iteration 45/1000 | Loss: 0.00004257
Iteration 46/1000 | Loss: 0.00004256
Iteration 47/1000 | Loss: 0.00004256
Iteration 48/1000 | Loss: 0.00004256
Iteration 49/1000 | Loss: 0.00004255
Iteration 50/1000 | Loss: 0.00004255
Iteration 51/1000 | Loss: 0.00004255
Iteration 52/1000 | Loss: 0.00004255
Iteration 53/1000 | Loss: 0.00004255
Iteration 54/1000 | Loss: 0.00004254
Iteration 55/1000 | Loss: 0.00004254
Iteration 56/1000 | Loss: 0.00004254
Iteration 57/1000 | Loss: 0.00004254
Iteration 58/1000 | Loss: 0.00004253
Iteration 59/1000 | Loss: 0.00004253
Iteration 60/1000 | Loss: 0.00004253
Iteration 61/1000 | Loss: 0.00004253
Iteration 62/1000 | Loss: 0.00004252
Iteration 63/1000 | Loss: 0.00004252
Iteration 64/1000 | Loss: 0.00004252
Iteration 65/1000 | Loss: 0.00004252
Iteration 66/1000 | Loss: 0.00004252
Iteration 67/1000 | Loss: 0.00004251
Iteration 68/1000 | Loss: 0.00004251
Iteration 69/1000 | Loss: 0.00004251
Iteration 70/1000 | Loss: 0.00004251
Iteration 71/1000 | Loss: 0.00004251
Iteration 72/1000 | Loss: 0.00004251
Iteration 73/1000 | Loss: 0.00004251
Iteration 74/1000 | Loss: 0.00004251
Iteration 75/1000 | Loss: 0.00004250
Iteration 76/1000 | Loss: 0.00004250
Iteration 77/1000 | Loss: 0.00004250
Iteration 78/1000 | Loss: 0.00004250
Iteration 79/1000 | Loss: 0.00004250
Iteration 80/1000 | Loss: 0.00004250
Iteration 81/1000 | Loss: 0.00004249
Iteration 82/1000 | Loss: 0.00004249
Iteration 83/1000 | Loss: 0.00005769
Iteration 84/1000 | Loss: 0.00004481
Iteration 85/1000 | Loss: 0.00004491
Iteration 86/1000 | Loss: 0.00004248
Iteration 87/1000 | Loss: 0.00004248
Iteration 88/1000 | Loss: 0.00004248
Iteration 89/1000 | Loss: 0.00004248
Iteration 90/1000 | Loss: 0.00004248
Iteration 91/1000 | Loss: 0.00004248
Iteration 92/1000 | Loss: 0.00004248
Iteration 93/1000 | Loss: 0.00004248
Iteration 94/1000 | Loss: 0.00004400
Iteration 95/1000 | Loss: 0.00004419
Iteration 96/1000 | Loss: 0.00004249
Iteration 97/1000 | Loss: 0.00004249
Iteration 98/1000 | Loss: 0.00004249
Iteration 99/1000 | Loss: 0.00004249
Iteration 100/1000 | Loss: 0.00004249
Iteration 101/1000 | Loss: 0.00004249
Iteration 102/1000 | Loss: 0.00004249
Iteration 103/1000 | Loss: 0.00004249
Iteration 104/1000 | Loss: 0.00004248
Iteration 105/1000 | Loss: 0.00004248
Iteration 106/1000 | Loss: 0.00004248
Iteration 107/1000 | Loss: 0.00004248
Iteration 108/1000 | Loss: 0.00004248
Iteration 109/1000 | Loss: 0.00004248
Iteration 110/1000 | Loss: 0.00004248
Iteration 111/1000 | Loss: 0.00004248
Iteration 112/1000 | Loss: 0.00004248
Iteration 113/1000 | Loss: 0.00004248
Iteration 114/1000 | Loss: 0.00004248
Iteration 115/1000 | Loss: 0.00004248
Iteration 116/1000 | Loss: 0.00004248
Iteration 117/1000 | Loss: 0.00004248
Iteration 118/1000 | Loss: 0.00004248
Iteration 119/1000 | Loss: 0.00004247
Iteration 120/1000 | Loss: 0.00004247
Iteration 121/1000 | Loss: 0.00004247
Iteration 122/1000 | Loss: 0.00004247
Iteration 123/1000 | Loss: 0.00004247
Iteration 124/1000 | Loss: 0.00004247
Iteration 125/1000 | Loss: 0.00004247
Iteration 126/1000 | Loss: 0.00004247
Iteration 127/1000 | Loss: 0.00004247
Iteration 128/1000 | Loss: 0.00004247
Iteration 129/1000 | Loss: 0.00004247
Iteration 130/1000 | Loss: 0.00004247
Iteration 131/1000 | Loss: 0.00004247
Iteration 132/1000 | Loss: 0.00004247
Iteration 133/1000 | Loss: 0.00004247
Iteration 134/1000 | Loss: 0.00004247
Iteration 135/1000 | Loss: 0.00004247
Iteration 136/1000 | Loss: 0.00004247
Iteration 137/1000 | Loss: 0.00004247
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 137. Stopping optimization.
Last 5 losses: [4.247393007972278e-05, 4.247393007972278e-05, 4.247393007972278e-05, 4.247393007972278e-05, 4.247393007972278e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.247393007972278e-05

Optimization complete. Final v2v error: 5.718843460083008 mm

Highest mean error: 6.296430587768555 mm for frame 187

Lowest mean error: 5.261180400848389 mm for frame 0

Saving results

Total time: 69.0727608203888
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_56_us_1894/0023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_56_us_1894/0023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_56_us_1894/0023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00657986
Iteration 2/25 | Loss: 0.00229280
Iteration 3/25 | Loss: 0.00213861
Iteration 4/25 | Loss: 0.00211926
Iteration 5/25 | Loss: 0.00211816
Iteration 6/25 | Loss: 0.00211816
Iteration 7/25 | Loss: 0.00211816
Iteration 8/25 | Loss: 0.00211816
Iteration 9/25 | Loss: 0.00211816
Iteration 10/25 | Loss: 0.00211816
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0021181588526815176, 0.0021181588526815176, 0.0021181588526815176, 0.0021181588526815176, 0.0021181588526815176]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0021181588526815176

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.29115772
Iteration 2/25 | Loss: 0.00261372
Iteration 3/25 | Loss: 0.00261371
Iteration 4/25 | Loss: 0.00261370
Iteration 5/25 | Loss: 0.00261370
Iteration 6/25 | Loss: 0.00261370
Iteration 7/25 | Loss: 0.00261370
Iteration 8/25 | Loss: 0.00261370
Iteration 9/25 | Loss: 0.00261370
Iteration 10/25 | Loss: 0.00261370
Iteration 11/25 | Loss: 0.00261370
Iteration 12/25 | Loss: 0.00261370
Iteration 13/25 | Loss: 0.00261370
Iteration 14/25 | Loss: 0.00261370
Iteration 15/25 | Loss: 0.00261370
Iteration 16/25 | Loss: 0.00261370
Iteration 17/25 | Loss: 0.00261370
Iteration 18/25 | Loss: 0.00261370
Iteration 19/25 | Loss: 0.00261370
Iteration 20/25 | Loss: 0.00261370
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.002613701159134507, 0.002613701159134507, 0.002613701159134507, 0.002613701159134507, 0.002613701159134507]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002613701159134507

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00261370
Iteration 2/1000 | Loss: 0.00013277
Iteration 3/1000 | Loss: 0.00007751
Iteration 4/1000 | Loss: 0.00006636
Iteration 5/1000 | Loss: 0.00006121
Iteration 6/1000 | Loss: 0.00005781
Iteration 7/1000 | Loss: 0.00005575
Iteration 8/1000 | Loss: 0.00005401
Iteration 9/1000 | Loss: 0.00005311
Iteration 10/1000 | Loss: 0.00005263
Iteration 11/1000 | Loss: 0.00005230
Iteration 12/1000 | Loss: 0.00005192
Iteration 13/1000 | Loss: 0.00005158
Iteration 14/1000 | Loss: 0.00005139
Iteration 15/1000 | Loss: 0.00005129
Iteration 16/1000 | Loss: 0.00005126
Iteration 17/1000 | Loss: 0.00005125
Iteration 18/1000 | Loss: 0.00005125
Iteration 19/1000 | Loss: 0.00005124
Iteration 20/1000 | Loss: 0.00005124
Iteration 21/1000 | Loss: 0.00005124
Iteration 22/1000 | Loss: 0.00005123
Iteration 23/1000 | Loss: 0.00005122
Iteration 24/1000 | Loss: 0.00005122
Iteration 25/1000 | Loss: 0.00005122
Iteration 26/1000 | Loss: 0.00005121
Iteration 27/1000 | Loss: 0.00005121
Iteration 28/1000 | Loss: 0.00005120
Iteration 29/1000 | Loss: 0.00005120
Iteration 30/1000 | Loss: 0.00005120
Iteration 31/1000 | Loss: 0.00005120
Iteration 32/1000 | Loss: 0.00005120
Iteration 33/1000 | Loss: 0.00005119
Iteration 34/1000 | Loss: 0.00005119
Iteration 35/1000 | Loss: 0.00005119
Iteration 36/1000 | Loss: 0.00005119
Iteration 37/1000 | Loss: 0.00005118
Iteration 38/1000 | Loss: 0.00005118
Iteration 39/1000 | Loss: 0.00005118
Iteration 40/1000 | Loss: 0.00005117
Iteration 41/1000 | Loss: 0.00005117
Iteration 42/1000 | Loss: 0.00005116
Iteration 43/1000 | Loss: 0.00005116
Iteration 44/1000 | Loss: 0.00005115
Iteration 45/1000 | Loss: 0.00005115
Iteration 46/1000 | Loss: 0.00005115
Iteration 47/1000 | Loss: 0.00005115
Iteration 48/1000 | Loss: 0.00005114
Iteration 49/1000 | Loss: 0.00005114
Iteration 50/1000 | Loss: 0.00005113
Iteration 51/1000 | Loss: 0.00005113
Iteration 52/1000 | Loss: 0.00005113
Iteration 53/1000 | Loss: 0.00005113
Iteration 54/1000 | Loss: 0.00005112
Iteration 55/1000 | Loss: 0.00005112
Iteration 56/1000 | Loss: 0.00005112
Iteration 57/1000 | Loss: 0.00005111
Iteration 58/1000 | Loss: 0.00005111
Iteration 59/1000 | Loss: 0.00005110
Iteration 60/1000 | Loss: 0.00005110
Iteration 61/1000 | Loss: 0.00005110
Iteration 62/1000 | Loss: 0.00005110
Iteration 63/1000 | Loss: 0.00005109
Iteration 64/1000 | Loss: 0.00005109
Iteration 65/1000 | Loss: 0.00005109
Iteration 66/1000 | Loss: 0.00005109
Iteration 67/1000 | Loss: 0.00005108
Iteration 68/1000 | Loss: 0.00005108
Iteration 69/1000 | Loss: 0.00005108
Iteration 70/1000 | Loss: 0.00005108
Iteration 71/1000 | Loss: 0.00005108
Iteration 72/1000 | Loss: 0.00005108
Iteration 73/1000 | Loss: 0.00005108
Iteration 74/1000 | Loss: 0.00005108
Iteration 75/1000 | Loss: 0.00005107
Iteration 76/1000 | Loss: 0.00005107
Iteration 77/1000 | Loss: 0.00005107
Iteration 78/1000 | Loss: 0.00005107
Iteration 79/1000 | Loss: 0.00005107
Iteration 80/1000 | Loss: 0.00005107
Iteration 81/1000 | Loss: 0.00005107
Iteration 82/1000 | Loss: 0.00005107
Iteration 83/1000 | Loss: 0.00005107
Iteration 84/1000 | Loss: 0.00005107
Iteration 85/1000 | Loss: 0.00005107
Iteration 86/1000 | Loss: 0.00005107
Iteration 87/1000 | Loss: 0.00005107
Iteration 88/1000 | Loss: 0.00005107
Iteration 89/1000 | Loss: 0.00005107
Iteration 90/1000 | Loss: 0.00005107
Iteration 91/1000 | Loss: 0.00005107
Iteration 92/1000 | Loss: 0.00005107
Iteration 93/1000 | Loss: 0.00005107
Iteration 94/1000 | Loss: 0.00005107
Iteration 95/1000 | Loss: 0.00005107
Iteration 96/1000 | Loss: 0.00005107
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 96. Stopping optimization.
Last 5 losses: [5.1072089263470843e-05, 5.1072089263470843e-05, 5.1072089263470843e-05, 5.1072089263470843e-05, 5.1072089263470843e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 5.1072089263470843e-05

Optimization complete. Final v2v error: 6.185476779937744 mm

Highest mean error: 6.4881911277771 mm for frame 65

Lowest mean error: 5.840780735015869 mm for frame 230

Saving results

Total time: 38.160409450531006
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_56_us_1894/0024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_56_us_1894/0024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_56_us_1894/0024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01054400
Iteration 2/25 | Loss: 0.00257946
Iteration 3/25 | Loss: 0.00224540
Iteration 4/25 | Loss: 0.00222855
Iteration 5/25 | Loss: 0.00222503
Iteration 6/25 | Loss: 0.00222433
Iteration 7/25 | Loss: 0.00222433
Iteration 8/25 | Loss: 0.00222433
Iteration 9/25 | Loss: 0.00222433
Iteration 10/25 | Loss: 0.00222433
Iteration 11/25 | Loss: 0.00222433
Iteration 12/25 | Loss: 0.00222433
Iteration 13/25 | Loss: 0.00222433
Iteration 14/25 | Loss: 0.00222433
Iteration 15/25 | Loss: 0.00222433
Iteration 16/25 | Loss: 0.00222433
Iteration 17/25 | Loss: 0.00222433
Iteration 18/25 | Loss: 0.00222433
Iteration 19/25 | Loss: 0.00222433
Iteration 20/25 | Loss: 0.00222433
Iteration 21/25 | Loss: 0.00222433
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.002224330324679613, 0.002224330324679613, 0.002224330324679613, 0.002224330324679613, 0.002224330324679613]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002224330324679613

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.17227077
Iteration 2/25 | Loss: 0.00235357
Iteration 3/25 | Loss: 0.00235357
Iteration 4/25 | Loss: 0.00235357
Iteration 5/25 | Loss: 0.00235357
Iteration 6/25 | Loss: 0.00235357
Iteration 7/25 | Loss: 0.00235357
Iteration 8/25 | Loss: 0.00235357
Iteration 9/25 | Loss: 0.00235356
Iteration 10/25 | Loss: 0.00235356
Iteration 11/25 | Loss: 0.00235356
Iteration 12/25 | Loss: 0.00235356
Iteration 13/25 | Loss: 0.00235356
Iteration 14/25 | Loss: 0.00235356
Iteration 15/25 | Loss: 0.00235356
Iteration 16/25 | Loss: 0.00235356
Iteration 17/25 | Loss: 0.00235356
Iteration 18/25 | Loss: 0.00235356
Iteration 19/25 | Loss: 0.00235356
Iteration 20/25 | Loss: 0.00235356
Iteration 21/25 | Loss: 0.00235356
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.002353564603254199, 0.002353564603254199, 0.002353564603254199, 0.002353564603254199, 0.002353564603254199]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002353564603254199

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00235356
Iteration 2/1000 | Loss: 0.00015246
Iteration 3/1000 | Loss: 0.00008793
Iteration 4/1000 | Loss: 0.00006976
Iteration 5/1000 | Loss: 0.00005944
Iteration 6/1000 | Loss: 0.00005471
Iteration 7/1000 | Loss: 0.00005172
Iteration 8/1000 | Loss: 0.00004899
Iteration 9/1000 | Loss: 0.00004612
Iteration 10/1000 | Loss: 0.00004472
Iteration 11/1000 | Loss: 0.00004356
Iteration 12/1000 | Loss: 0.00004278
Iteration 13/1000 | Loss: 0.00004225
Iteration 14/1000 | Loss: 0.00004199
Iteration 15/1000 | Loss: 0.00004177
Iteration 16/1000 | Loss: 0.00004158
Iteration 17/1000 | Loss: 0.00004140
Iteration 18/1000 | Loss: 0.00004127
Iteration 19/1000 | Loss: 0.00004126
Iteration 20/1000 | Loss: 0.00004126
Iteration 21/1000 | Loss: 0.00004125
Iteration 22/1000 | Loss: 0.00004118
Iteration 23/1000 | Loss: 0.00004109
Iteration 24/1000 | Loss: 0.00004107
Iteration 25/1000 | Loss: 0.00004107
Iteration 26/1000 | Loss: 0.00004104
Iteration 27/1000 | Loss: 0.00004096
Iteration 28/1000 | Loss: 0.00004095
Iteration 29/1000 | Loss: 0.00004095
Iteration 30/1000 | Loss: 0.00004092
Iteration 31/1000 | Loss: 0.00004092
Iteration 32/1000 | Loss: 0.00004091
Iteration 33/1000 | Loss: 0.00004091
Iteration 34/1000 | Loss: 0.00004090
Iteration 35/1000 | Loss: 0.00004090
Iteration 36/1000 | Loss: 0.00004090
Iteration 37/1000 | Loss: 0.00004090
Iteration 38/1000 | Loss: 0.00004089
Iteration 39/1000 | Loss: 0.00004089
Iteration 40/1000 | Loss: 0.00004089
Iteration 41/1000 | Loss: 0.00004089
Iteration 42/1000 | Loss: 0.00004088
Iteration 43/1000 | Loss: 0.00004088
Iteration 44/1000 | Loss: 0.00004088
Iteration 45/1000 | Loss: 0.00004088
Iteration 46/1000 | Loss: 0.00004088
Iteration 47/1000 | Loss: 0.00004088
Iteration 48/1000 | Loss: 0.00004087
Iteration 49/1000 | Loss: 0.00004087
Iteration 50/1000 | Loss: 0.00004087
Iteration 51/1000 | Loss: 0.00004087
Iteration 52/1000 | Loss: 0.00004087
Iteration 53/1000 | Loss: 0.00004087
Iteration 54/1000 | Loss: 0.00004087
Iteration 55/1000 | Loss: 0.00004087
Iteration 56/1000 | Loss: 0.00004087
Iteration 57/1000 | Loss: 0.00004087
Iteration 58/1000 | Loss: 0.00004087
Iteration 59/1000 | Loss: 0.00004087
Iteration 60/1000 | Loss: 0.00004087
Iteration 61/1000 | Loss: 0.00004087
Iteration 62/1000 | Loss: 0.00004086
Iteration 63/1000 | Loss: 0.00004086
Iteration 64/1000 | Loss: 0.00004086
Iteration 65/1000 | Loss: 0.00004086
Iteration 66/1000 | Loss: 0.00004085
Iteration 67/1000 | Loss: 0.00004085
Iteration 68/1000 | Loss: 0.00004085
Iteration 69/1000 | Loss: 0.00004085
Iteration 70/1000 | Loss: 0.00004085
Iteration 71/1000 | Loss: 0.00004085
Iteration 72/1000 | Loss: 0.00004085
Iteration 73/1000 | Loss: 0.00004085
Iteration 74/1000 | Loss: 0.00004085
Iteration 75/1000 | Loss: 0.00004084
Iteration 76/1000 | Loss: 0.00004084
Iteration 77/1000 | Loss: 0.00004084
Iteration 78/1000 | Loss: 0.00004084
Iteration 79/1000 | Loss: 0.00004084
Iteration 80/1000 | Loss: 0.00004084
Iteration 81/1000 | Loss: 0.00004084
Iteration 82/1000 | Loss: 0.00004084
Iteration 83/1000 | Loss: 0.00004084
Iteration 84/1000 | Loss: 0.00004084
Iteration 85/1000 | Loss: 0.00004084
Iteration 86/1000 | Loss: 0.00004084
Iteration 87/1000 | Loss: 0.00004084
Iteration 88/1000 | Loss: 0.00004084
Iteration 89/1000 | Loss: 0.00004084
Iteration 90/1000 | Loss: 0.00004083
Iteration 91/1000 | Loss: 0.00004083
Iteration 92/1000 | Loss: 0.00004083
Iteration 93/1000 | Loss: 0.00004083
Iteration 94/1000 | Loss: 0.00004083
Iteration 95/1000 | Loss: 0.00004083
Iteration 96/1000 | Loss: 0.00004083
Iteration 97/1000 | Loss: 0.00004083
Iteration 98/1000 | Loss: 0.00004083
Iteration 99/1000 | Loss: 0.00004083
Iteration 100/1000 | Loss: 0.00004083
Iteration 101/1000 | Loss: 0.00004083
Iteration 102/1000 | Loss: 0.00004083
Iteration 103/1000 | Loss: 0.00004083
Iteration 104/1000 | Loss: 0.00004083
Iteration 105/1000 | Loss: 0.00004083
Iteration 106/1000 | Loss: 0.00004083
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 106. Stopping optimization.
Last 5 losses: [4.0834889659890905e-05, 4.0834889659890905e-05, 4.0834889659890905e-05, 4.0834889659890905e-05, 4.0834889659890905e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.0834889659890905e-05

Optimization complete. Final v2v error: 5.498107433319092 mm

Highest mean error: 5.798344135284424 mm for frame 61

Lowest mean error: 5.337896347045898 mm for frame 6

Saving results

Total time: 42.75658345222473
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_56_us_1894/0012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_56_us_1894/0012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_56_us_1894/0012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00921745
Iteration 2/25 | Loss: 0.00217015
Iteration 3/25 | Loss: 0.00206462
Iteration 4/25 | Loss: 0.00205354
Iteration 5/25 | Loss: 0.00204964
Iteration 6/25 | Loss: 0.00204919
Iteration 7/25 | Loss: 0.00204919
Iteration 8/25 | Loss: 0.00204919
Iteration 9/25 | Loss: 0.00204919
Iteration 10/25 | Loss: 0.00204919
Iteration 11/25 | Loss: 0.00204919
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.002049192786216736, 0.002049192786216736, 0.002049192786216736, 0.002049192786216736, 0.002049192786216736]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002049192786216736

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.25147700
Iteration 2/25 | Loss: 0.00251306
Iteration 3/25 | Loss: 0.00251306
Iteration 4/25 | Loss: 0.00251306
Iteration 5/25 | Loss: 0.00251306
Iteration 6/25 | Loss: 0.00251306
Iteration 7/25 | Loss: 0.00251306
Iteration 8/25 | Loss: 0.00251306
Iteration 9/25 | Loss: 0.00251306
Iteration 10/25 | Loss: 0.00251306
Iteration 11/25 | Loss: 0.00251306
Iteration 12/25 | Loss: 0.00251306
Iteration 13/25 | Loss: 0.00251306
Iteration 14/25 | Loss: 0.00251306
Iteration 15/25 | Loss: 0.00251306
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.002513055456802249, 0.002513055456802249, 0.002513055456802249, 0.002513055456802249, 0.002513055456802249]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002513055456802249

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00251306
Iteration 2/1000 | Loss: 0.00011391
Iteration 3/1000 | Loss: 0.00006410
Iteration 4/1000 | Loss: 0.00005568
Iteration 5/1000 | Loss: 0.00004992
Iteration 6/1000 | Loss: 0.00004618
Iteration 7/1000 | Loss: 0.00004377
Iteration 8/1000 | Loss: 0.00004224
Iteration 9/1000 | Loss: 0.00004113
Iteration 10/1000 | Loss: 0.00003999
Iteration 11/1000 | Loss: 0.00003930
Iteration 12/1000 | Loss: 0.00003890
Iteration 13/1000 | Loss: 0.00003883
Iteration 14/1000 | Loss: 0.00003874
Iteration 15/1000 | Loss: 0.00003857
Iteration 16/1000 | Loss: 0.00003852
Iteration 17/1000 | Loss: 0.00003850
Iteration 18/1000 | Loss: 0.00003840
Iteration 19/1000 | Loss: 0.00003840
Iteration 20/1000 | Loss: 0.00003839
Iteration 21/1000 | Loss: 0.00003839
Iteration 22/1000 | Loss: 0.00003839
Iteration 23/1000 | Loss: 0.00003834
Iteration 24/1000 | Loss: 0.00003830
Iteration 25/1000 | Loss: 0.00003829
Iteration 26/1000 | Loss: 0.00003829
Iteration 27/1000 | Loss: 0.00003829
Iteration 28/1000 | Loss: 0.00003820
Iteration 29/1000 | Loss: 0.00003814
Iteration 30/1000 | Loss: 0.00003809
Iteration 31/1000 | Loss: 0.00003808
Iteration 32/1000 | Loss: 0.00003808
Iteration 33/1000 | Loss: 0.00003808
Iteration 34/1000 | Loss: 0.00003808
Iteration 35/1000 | Loss: 0.00003808
Iteration 36/1000 | Loss: 0.00003808
Iteration 37/1000 | Loss: 0.00003808
Iteration 38/1000 | Loss: 0.00003808
Iteration 39/1000 | Loss: 0.00003808
Iteration 40/1000 | Loss: 0.00003808
Iteration 41/1000 | Loss: 0.00003807
Iteration 42/1000 | Loss: 0.00003807
Iteration 43/1000 | Loss: 0.00003805
Iteration 44/1000 | Loss: 0.00003805
Iteration 45/1000 | Loss: 0.00003805
Iteration 46/1000 | Loss: 0.00003805
Iteration 47/1000 | Loss: 0.00003804
Iteration 48/1000 | Loss: 0.00003804
Iteration 49/1000 | Loss: 0.00003804
Iteration 50/1000 | Loss: 0.00003804
Iteration 51/1000 | Loss: 0.00003803
Iteration 52/1000 | Loss: 0.00003803
Iteration 53/1000 | Loss: 0.00003803
Iteration 54/1000 | Loss: 0.00003803
Iteration 55/1000 | Loss: 0.00003803
Iteration 56/1000 | Loss: 0.00003802
Iteration 57/1000 | Loss: 0.00003802
Iteration 58/1000 | Loss: 0.00003802
Iteration 59/1000 | Loss: 0.00003802
Iteration 60/1000 | Loss: 0.00003802
Iteration 61/1000 | Loss: 0.00003802
Iteration 62/1000 | Loss: 0.00003802
Iteration 63/1000 | Loss: 0.00003802
Iteration 64/1000 | Loss: 0.00003802
Iteration 65/1000 | Loss: 0.00003802
Iteration 66/1000 | Loss: 0.00003802
Iteration 67/1000 | Loss: 0.00003802
Iteration 68/1000 | Loss: 0.00003802
Iteration 69/1000 | Loss: 0.00003801
Iteration 70/1000 | Loss: 0.00003801
Iteration 71/1000 | Loss: 0.00003801
Iteration 72/1000 | Loss: 0.00003801
Iteration 73/1000 | Loss: 0.00003800
Iteration 74/1000 | Loss: 0.00003800
Iteration 75/1000 | Loss: 0.00003800
Iteration 76/1000 | Loss: 0.00003800
Iteration 77/1000 | Loss: 0.00003799
Iteration 78/1000 | Loss: 0.00003799
Iteration 79/1000 | Loss: 0.00003799
Iteration 80/1000 | Loss: 0.00003798
Iteration 81/1000 | Loss: 0.00003798
Iteration 82/1000 | Loss: 0.00003798
Iteration 83/1000 | Loss: 0.00003798
Iteration 84/1000 | Loss: 0.00003798
Iteration 85/1000 | Loss: 0.00003797
Iteration 86/1000 | Loss: 0.00003797
Iteration 87/1000 | Loss: 0.00003797
Iteration 88/1000 | Loss: 0.00003797
Iteration 89/1000 | Loss: 0.00003797
Iteration 90/1000 | Loss: 0.00003797
Iteration 91/1000 | Loss: 0.00003797
Iteration 92/1000 | Loss: 0.00003796
Iteration 93/1000 | Loss: 0.00003796
Iteration 94/1000 | Loss: 0.00003796
Iteration 95/1000 | Loss: 0.00003796
Iteration 96/1000 | Loss: 0.00003796
Iteration 97/1000 | Loss: 0.00003796
Iteration 98/1000 | Loss: 0.00003796
Iteration 99/1000 | Loss: 0.00003796
Iteration 100/1000 | Loss: 0.00003796
Iteration 101/1000 | Loss: 0.00003796
Iteration 102/1000 | Loss: 0.00003796
Iteration 103/1000 | Loss: 0.00003796
Iteration 104/1000 | Loss: 0.00003796
Iteration 105/1000 | Loss: 0.00003795
Iteration 106/1000 | Loss: 0.00003795
Iteration 107/1000 | Loss: 0.00003795
Iteration 108/1000 | Loss: 0.00003795
Iteration 109/1000 | Loss: 0.00003795
Iteration 110/1000 | Loss: 0.00003795
Iteration 111/1000 | Loss: 0.00003795
Iteration 112/1000 | Loss: 0.00003795
Iteration 113/1000 | Loss: 0.00003794
Iteration 114/1000 | Loss: 0.00003794
Iteration 115/1000 | Loss: 0.00003794
Iteration 116/1000 | Loss: 0.00003794
Iteration 117/1000 | Loss: 0.00003793
Iteration 118/1000 | Loss: 0.00003793
Iteration 119/1000 | Loss: 0.00003793
Iteration 120/1000 | Loss: 0.00003793
Iteration 121/1000 | Loss: 0.00003793
Iteration 122/1000 | Loss: 0.00003793
Iteration 123/1000 | Loss: 0.00003793
Iteration 124/1000 | Loss: 0.00003793
Iteration 125/1000 | Loss: 0.00003792
Iteration 126/1000 | Loss: 0.00003792
Iteration 127/1000 | Loss: 0.00003792
Iteration 128/1000 | Loss: 0.00003792
Iteration 129/1000 | Loss: 0.00003792
Iteration 130/1000 | Loss: 0.00003792
Iteration 131/1000 | Loss: 0.00003792
Iteration 132/1000 | Loss: 0.00003792
Iteration 133/1000 | Loss: 0.00003792
Iteration 134/1000 | Loss: 0.00003792
Iteration 135/1000 | Loss: 0.00003792
Iteration 136/1000 | Loss: 0.00003792
Iteration 137/1000 | Loss: 0.00003792
Iteration 138/1000 | Loss: 0.00003792
Iteration 139/1000 | Loss: 0.00003792
Iteration 140/1000 | Loss: 0.00003792
Iteration 141/1000 | Loss: 0.00003792
Iteration 142/1000 | Loss: 0.00003792
Iteration 143/1000 | Loss: 0.00003792
Iteration 144/1000 | Loss: 0.00003792
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 144. Stopping optimization.
Last 5 losses: [3.791580820688978e-05, 3.791580820688978e-05, 3.791580820688978e-05, 3.791580820688978e-05, 3.791580820688978e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.791580820688978e-05

Optimization complete. Final v2v error: 5.2804412841796875 mm

Highest mean error: 5.746189594268799 mm for frame 100

Lowest mean error: 5.000489711761475 mm for frame 157

Saving results

Total time: 44.64860510826111
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_56_us_1894/0007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_56_us_1894/0007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_56_us_1894/0007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00954748
Iteration 2/25 | Loss: 0.00229923
Iteration 3/25 | Loss: 0.00214474
Iteration 4/25 | Loss: 0.00212577
Iteration 5/25 | Loss: 0.00211963
Iteration 6/25 | Loss: 0.00211785
Iteration 7/25 | Loss: 0.00211785
Iteration 8/25 | Loss: 0.00211785
Iteration 9/25 | Loss: 0.00211785
Iteration 10/25 | Loss: 0.00211785
Iteration 11/25 | Loss: 0.00211785
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0021178529132157564, 0.0021178529132157564, 0.0021178529132157564, 0.0021178529132157564, 0.0021178529132157564]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0021178529132157564

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.09423149
Iteration 2/25 | Loss: 0.00260614
Iteration 3/25 | Loss: 0.00260614
Iteration 4/25 | Loss: 0.00260614
Iteration 5/25 | Loss: 0.00260614
Iteration 6/25 | Loss: 0.00260614
Iteration 7/25 | Loss: 0.00260614
Iteration 8/25 | Loss: 0.00260614
Iteration 9/25 | Loss: 0.00260614
Iteration 10/25 | Loss: 0.00260614
Iteration 11/25 | Loss: 0.00260614
Iteration 12/25 | Loss: 0.00260614
Iteration 13/25 | Loss: 0.00260614
Iteration 14/25 | Loss: 0.00260614
Iteration 15/25 | Loss: 0.00260614
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0026061409153044224, 0.0026061409153044224, 0.0026061409153044224, 0.0026061409153044224, 0.0026061409153044224]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0026061409153044224

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00260614
Iteration 2/1000 | Loss: 0.00012344
Iteration 3/1000 | Loss: 0.00007939
Iteration 4/1000 | Loss: 0.00006655
Iteration 5/1000 | Loss: 0.00005929
Iteration 6/1000 | Loss: 0.00005509
Iteration 7/1000 | Loss: 0.00005267
Iteration 8/1000 | Loss: 0.00005140
Iteration 9/1000 | Loss: 0.00004976
Iteration 10/1000 | Loss: 0.00004878
Iteration 11/1000 | Loss: 0.00004752
Iteration 12/1000 | Loss: 0.00004670
Iteration 13/1000 | Loss: 0.00004611
Iteration 14/1000 | Loss: 0.00004552
Iteration 15/1000 | Loss: 0.00004522
Iteration 16/1000 | Loss: 0.00004494
Iteration 17/1000 | Loss: 0.00004483
Iteration 18/1000 | Loss: 0.00004467
Iteration 19/1000 | Loss: 0.00004467
Iteration 20/1000 | Loss: 0.00004466
Iteration 21/1000 | Loss: 0.00004466
Iteration 22/1000 | Loss: 0.00004461
Iteration 23/1000 | Loss: 0.00004459
Iteration 24/1000 | Loss: 0.00004459
Iteration 25/1000 | Loss: 0.00004458
Iteration 26/1000 | Loss: 0.00004458
Iteration 27/1000 | Loss: 0.00004457
Iteration 28/1000 | Loss: 0.00004455
Iteration 29/1000 | Loss: 0.00004455
Iteration 30/1000 | Loss: 0.00004455
Iteration 31/1000 | Loss: 0.00004454
Iteration 32/1000 | Loss: 0.00004454
Iteration 33/1000 | Loss: 0.00004453
Iteration 34/1000 | Loss: 0.00004453
Iteration 35/1000 | Loss: 0.00004452
Iteration 36/1000 | Loss: 0.00004452
Iteration 37/1000 | Loss: 0.00004451
Iteration 38/1000 | Loss: 0.00004451
Iteration 39/1000 | Loss: 0.00004450
Iteration 40/1000 | Loss: 0.00004450
Iteration 41/1000 | Loss: 0.00004450
Iteration 42/1000 | Loss: 0.00004447
Iteration 43/1000 | Loss: 0.00004447
Iteration 44/1000 | Loss: 0.00004447
Iteration 45/1000 | Loss: 0.00004447
Iteration 46/1000 | Loss: 0.00004447
Iteration 47/1000 | Loss: 0.00004446
Iteration 48/1000 | Loss: 0.00004446
Iteration 49/1000 | Loss: 0.00004446
Iteration 50/1000 | Loss: 0.00004444
Iteration 51/1000 | Loss: 0.00004444
Iteration 52/1000 | Loss: 0.00004444
Iteration 53/1000 | Loss: 0.00004444
Iteration 54/1000 | Loss: 0.00004444
Iteration 55/1000 | Loss: 0.00004444
Iteration 56/1000 | Loss: 0.00004444
Iteration 57/1000 | Loss: 0.00004444
Iteration 58/1000 | Loss: 0.00004443
Iteration 59/1000 | Loss: 0.00004443
Iteration 60/1000 | Loss: 0.00004443
Iteration 61/1000 | Loss: 0.00004443
Iteration 62/1000 | Loss: 0.00004443
Iteration 63/1000 | Loss: 0.00004442
Iteration 64/1000 | Loss: 0.00004442
Iteration 65/1000 | Loss: 0.00004442
Iteration 66/1000 | Loss: 0.00004441
Iteration 67/1000 | Loss: 0.00004441
Iteration 68/1000 | Loss: 0.00004441
Iteration 69/1000 | Loss: 0.00004440
Iteration 70/1000 | Loss: 0.00004440
Iteration 71/1000 | Loss: 0.00004440
Iteration 72/1000 | Loss: 0.00004440
Iteration 73/1000 | Loss: 0.00004440
Iteration 74/1000 | Loss: 0.00004440
Iteration 75/1000 | Loss: 0.00004440
Iteration 76/1000 | Loss: 0.00004440
Iteration 77/1000 | Loss: 0.00004440
Iteration 78/1000 | Loss: 0.00004440
Iteration 79/1000 | Loss: 0.00004440
Iteration 80/1000 | Loss: 0.00004440
Iteration 81/1000 | Loss: 0.00004440
Iteration 82/1000 | Loss: 0.00004440
Iteration 83/1000 | Loss: 0.00004440
Iteration 84/1000 | Loss: 0.00004440
Iteration 85/1000 | Loss: 0.00004440
Iteration 86/1000 | Loss: 0.00004440
Iteration 87/1000 | Loss: 0.00004440
Iteration 88/1000 | Loss: 0.00004440
Iteration 89/1000 | Loss: 0.00004440
Iteration 90/1000 | Loss: 0.00004440
Iteration 91/1000 | Loss: 0.00004440
Iteration 92/1000 | Loss: 0.00004440
Iteration 93/1000 | Loss: 0.00004440
Iteration 94/1000 | Loss: 0.00004440
Iteration 95/1000 | Loss: 0.00004440
Iteration 96/1000 | Loss: 0.00004440
Iteration 97/1000 | Loss: 0.00004440
Iteration 98/1000 | Loss: 0.00004440
Iteration 99/1000 | Loss: 0.00004440
Iteration 100/1000 | Loss: 0.00004440
Iteration 101/1000 | Loss: 0.00004440
Iteration 102/1000 | Loss: 0.00004440
Iteration 103/1000 | Loss: 0.00004440
Iteration 104/1000 | Loss: 0.00004440
Iteration 105/1000 | Loss: 0.00004440
Iteration 106/1000 | Loss: 0.00004440
Iteration 107/1000 | Loss: 0.00004440
Iteration 108/1000 | Loss: 0.00004440
Iteration 109/1000 | Loss: 0.00004440
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 109. Stopping optimization.
Last 5 losses: [4.440117845661007e-05, 4.440117845661007e-05, 4.440117845661007e-05, 4.440117845661007e-05, 4.440117845661007e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.440117845661007e-05

Optimization complete. Final v2v error: 5.780210494995117 mm

Highest mean error: 6.356271743774414 mm for frame 125

Lowest mean error: 4.982000827789307 mm for frame 0

Saving results

Total time: 45.46101522445679
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_56_us_1894/0003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_56_us_1894/0003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_56_us_1894/0003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00718038
Iteration 2/25 | Loss: 0.00217632
Iteration 3/25 | Loss: 0.00208144
Iteration 4/25 | Loss: 0.00206745
Iteration 5/25 | Loss: 0.00206257
Iteration 6/25 | Loss: 0.00206170
Iteration 7/25 | Loss: 0.00206170
Iteration 8/25 | Loss: 0.00206170
Iteration 9/25 | Loss: 0.00206170
Iteration 10/25 | Loss: 0.00206170
Iteration 11/25 | Loss: 0.00206170
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0020616971887648106, 0.0020616971887648106, 0.0020616971887648106, 0.0020616971887648106, 0.0020616971887648106]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0020616971887648106

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.46392941
Iteration 2/25 | Loss: 0.00247258
Iteration 3/25 | Loss: 0.00247258
Iteration 4/25 | Loss: 0.00247258
Iteration 5/25 | Loss: 0.00247258
Iteration 6/25 | Loss: 0.00247258
Iteration 7/25 | Loss: 0.00247258
Iteration 8/25 | Loss: 0.00247258
Iteration 9/25 | Loss: 0.00247258
Iteration 10/25 | Loss: 0.00247257
Iteration 11/25 | Loss: 0.00247257
Iteration 12/25 | Loss: 0.00247257
Iteration 13/25 | Loss: 0.00247257
Iteration 14/25 | Loss: 0.00247257
Iteration 15/25 | Loss: 0.00247257
Iteration 16/25 | Loss: 0.00247257
Iteration 17/25 | Loss: 0.00247257
Iteration 18/25 | Loss: 0.00247257
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.002472574356943369, 0.002472574356943369, 0.002472574356943369, 0.002472574356943369, 0.002472574356943369]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002472574356943369

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00247257
Iteration 2/1000 | Loss: 0.00009285
Iteration 3/1000 | Loss: 0.00006457
Iteration 4/1000 | Loss: 0.00005835
Iteration 5/1000 | Loss: 0.00005382
Iteration 6/1000 | Loss: 0.00005089
Iteration 7/1000 | Loss: 0.00004913
Iteration 8/1000 | Loss: 0.00004814
Iteration 9/1000 | Loss: 0.00004711
Iteration 10/1000 | Loss: 0.00004631
Iteration 11/1000 | Loss: 0.00004587
Iteration 12/1000 | Loss: 0.00004552
Iteration 13/1000 | Loss: 0.00004525
Iteration 14/1000 | Loss: 0.00004504
Iteration 15/1000 | Loss: 0.00004490
Iteration 16/1000 | Loss: 0.00004482
Iteration 17/1000 | Loss: 0.00004482
Iteration 18/1000 | Loss: 0.00004482
Iteration 19/1000 | Loss: 0.00004482
Iteration 20/1000 | Loss: 0.00004481
Iteration 21/1000 | Loss: 0.00004481
Iteration 22/1000 | Loss: 0.00004481
Iteration 23/1000 | Loss: 0.00004478
Iteration 24/1000 | Loss: 0.00004478
Iteration 25/1000 | Loss: 0.00004477
Iteration 26/1000 | Loss: 0.00004477
Iteration 27/1000 | Loss: 0.00004476
Iteration 28/1000 | Loss: 0.00004474
Iteration 29/1000 | Loss: 0.00004474
Iteration 30/1000 | Loss: 0.00004474
Iteration 31/1000 | Loss: 0.00004474
Iteration 32/1000 | Loss: 0.00004474
Iteration 33/1000 | Loss: 0.00004474
Iteration 34/1000 | Loss: 0.00004474
Iteration 35/1000 | Loss: 0.00004474
Iteration 36/1000 | Loss: 0.00004474
Iteration 37/1000 | Loss: 0.00004474
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 37. Stopping optimization.
Last 5 losses: [4.4739063014276326e-05, 4.4739063014276326e-05, 4.4739063014276326e-05, 4.4739063014276326e-05, 4.4739063014276326e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.4739063014276326e-05

Optimization complete. Final v2v error: 5.699747085571289 mm

Highest mean error: 6.365926742553711 mm for frame 103

Lowest mean error: 5.296970844268799 mm for frame 2

Saving results

Total time: 32.64837884902954
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_56_us_1894/0018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_56_us_1894/0018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_56_us_1894/0018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00516397
Iteration 2/25 | Loss: 0.00215290
Iteration 3/25 | Loss: 0.00203871
Iteration 4/25 | Loss: 0.00202942
Iteration 5/25 | Loss: 0.00202513
Iteration 6/25 | Loss: 0.00202402
Iteration 7/25 | Loss: 0.00202402
Iteration 8/25 | Loss: 0.00202402
Iteration 9/25 | Loss: 0.00202402
Iteration 10/25 | Loss: 0.00202402
Iteration 11/25 | Loss: 0.00202402
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.002024017507210374, 0.002024017507210374, 0.002024017507210374, 0.002024017507210374, 0.002024017507210374]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002024017507210374

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.25073040
Iteration 2/25 | Loss: 0.00220113
Iteration 3/25 | Loss: 0.00220113
Iteration 4/25 | Loss: 0.00220113
Iteration 5/25 | Loss: 0.00220113
Iteration 6/25 | Loss: 0.00220113
Iteration 7/25 | Loss: 0.00220113
Iteration 8/25 | Loss: 0.00220113
Iteration 9/25 | Loss: 0.00220113
Iteration 10/25 | Loss: 0.00220113
Iteration 11/25 | Loss: 0.00220113
Iteration 12/25 | Loss: 0.00220113
Iteration 13/25 | Loss: 0.00220113
Iteration 14/25 | Loss: 0.00220113
Iteration 15/25 | Loss: 0.00220113
Iteration 16/25 | Loss: 0.00220113
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0022011289838701487, 0.0022011289838701487, 0.0022011289838701487, 0.0022011289838701487, 0.0022011289838701487]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0022011289838701487

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00220113
Iteration 2/1000 | Loss: 0.00008376
Iteration 3/1000 | Loss: 0.00006455
Iteration 4/1000 | Loss: 0.00005792
Iteration 5/1000 | Loss: 0.00005439
Iteration 6/1000 | Loss: 0.00005193
Iteration 7/1000 | Loss: 0.00004971
Iteration 8/1000 | Loss: 0.00004807
Iteration 9/1000 | Loss: 0.00004725
Iteration 10/1000 | Loss: 0.00004693
Iteration 11/1000 | Loss: 0.00004653
Iteration 12/1000 | Loss: 0.00004614
Iteration 13/1000 | Loss: 0.00004574
Iteration 14/1000 | Loss: 0.00004548
Iteration 15/1000 | Loss: 0.00004528
Iteration 16/1000 | Loss: 0.00004528
Iteration 17/1000 | Loss: 0.00004526
Iteration 18/1000 | Loss: 0.00004519
Iteration 19/1000 | Loss: 0.00004513
Iteration 20/1000 | Loss: 0.00004506
Iteration 21/1000 | Loss: 0.00004506
Iteration 22/1000 | Loss: 0.00004505
Iteration 23/1000 | Loss: 0.00004504
Iteration 24/1000 | Loss: 0.00004503
Iteration 25/1000 | Loss: 0.00004503
Iteration 26/1000 | Loss: 0.00004502
Iteration 27/1000 | Loss: 0.00004502
Iteration 28/1000 | Loss: 0.00004502
Iteration 29/1000 | Loss: 0.00004500
Iteration 30/1000 | Loss: 0.00004500
Iteration 31/1000 | Loss: 0.00004500
Iteration 32/1000 | Loss: 0.00004500
Iteration 33/1000 | Loss: 0.00004500
Iteration 34/1000 | Loss: 0.00004500
Iteration 35/1000 | Loss: 0.00004500
Iteration 36/1000 | Loss: 0.00004500
Iteration 37/1000 | Loss: 0.00004500
Iteration 38/1000 | Loss: 0.00004500
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 38. Stopping optimization.
Last 5 losses: [4.4997988879913464e-05, 4.4997988879913464e-05, 4.4997988879913464e-05, 4.4997988879913464e-05, 4.4997988879913464e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.4997988879913464e-05

Optimization complete. Final v2v error: 5.730050086975098 mm

Highest mean error: 5.942244052886963 mm for frame 38

Lowest mean error: 5.348992347717285 mm for frame 231

Saving results

Total time: 38.70843315124512
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_56_us_1894/0019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_56_us_1894/0019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_56_us_1894/0019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00934331
Iteration 2/25 | Loss: 0.00248334
Iteration 3/25 | Loss: 0.00220747
Iteration 4/25 | Loss: 0.00218238
Iteration 5/25 | Loss: 0.00217275
Iteration 6/25 | Loss: 0.00217010
Iteration 7/25 | Loss: 0.00216967
Iteration 8/25 | Loss: 0.00216967
Iteration 9/25 | Loss: 0.00216967
Iteration 10/25 | Loss: 0.00216967
Iteration 11/25 | Loss: 0.00216967
Iteration 12/25 | Loss: 0.00216967
Iteration 13/25 | Loss: 0.00216967
Iteration 14/25 | Loss: 0.00216967
Iteration 15/25 | Loss: 0.00216967
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0021696733310818672, 0.0021696733310818672, 0.0021696733310818672, 0.0021696733310818672, 0.0021696733310818672]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0021696733310818672

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.20335591
Iteration 2/25 | Loss: 0.00275528
Iteration 3/25 | Loss: 0.00275527
Iteration 4/25 | Loss: 0.00275526
Iteration 5/25 | Loss: 0.00275526
Iteration 6/25 | Loss: 0.00275526
Iteration 7/25 | Loss: 0.00275526
Iteration 8/25 | Loss: 0.00275526
Iteration 9/25 | Loss: 0.00275526
Iteration 10/25 | Loss: 0.00275526
Iteration 11/25 | Loss: 0.00275526
Iteration 12/25 | Loss: 0.00275526
Iteration 13/25 | Loss: 0.00275526
Iteration 14/25 | Loss: 0.00275526
Iteration 15/25 | Loss: 0.00275526
Iteration 16/25 | Loss: 0.00275526
Iteration 17/25 | Loss: 0.00275526
Iteration 18/25 | Loss: 0.00275526
Iteration 19/25 | Loss: 0.00275526
Iteration 20/25 | Loss: 0.00275526
Iteration 21/25 | Loss: 0.00275526
Iteration 22/25 | Loss: 0.00275526
Iteration 23/25 | Loss: 0.00275526
Iteration 24/25 | Loss: 0.00275526
Iteration 25/25 | Loss: 0.00275526

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00275526
Iteration 2/1000 | Loss: 0.00014910
Iteration 3/1000 | Loss: 0.00010622
Iteration 4/1000 | Loss: 0.00009027
Iteration 5/1000 | Loss: 0.00008210
Iteration 6/1000 | Loss: 0.00007516
Iteration 7/1000 | Loss: 0.00007072
Iteration 8/1000 | Loss: 0.00006762
Iteration 9/1000 | Loss: 0.00006515
Iteration 10/1000 | Loss: 0.00006256
Iteration 11/1000 | Loss: 0.00006065
Iteration 12/1000 | Loss: 0.00005899
Iteration 13/1000 | Loss: 0.00005794
Iteration 14/1000 | Loss: 0.00005715
Iteration 15/1000 | Loss: 0.00005635
Iteration 16/1000 | Loss: 0.00005588
Iteration 17/1000 | Loss: 0.00005558
Iteration 18/1000 | Loss: 0.00005533
Iteration 19/1000 | Loss: 0.00005504
Iteration 20/1000 | Loss: 0.00005482
Iteration 21/1000 | Loss: 0.00005462
Iteration 22/1000 | Loss: 0.00005446
Iteration 23/1000 | Loss: 0.00005430
Iteration 24/1000 | Loss: 0.00005428
Iteration 25/1000 | Loss: 0.00005428
Iteration 26/1000 | Loss: 0.00005421
Iteration 27/1000 | Loss: 0.00005415
Iteration 28/1000 | Loss: 0.00005411
Iteration 29/1000 | Loss: 0.00005401
Iteration 30/1000 | Loss: 0.00005399
Iteration 31/1000 | Loss: 0.00005395
Iteration 32/1000 | Loss: 0.00005394
Iteration 33/1000 | Loss: 0.00005389
Iteration 34/1000 | Loss: 0.00005389
Iteration 35/1000 | Loss: 0.00005387
Iteration 36/1000 | Loss: 0.00005386
Iteration 37/1000 | Loss: 0.00005385
Iteration 38/1000 | Loss: 0.00005382
Iteration 39/1000 | Loss: 0.00005381
Iteration 40/1000 | Loss: 0.00005379
Iteration 41/1000 | Loss: 0.00005379
Iteration 42/1000 | Loss: 0.00005379
Iteration 43/1000 | Loss: 0.00005379
Iteration 44/1000 | Loss: 0.00005379
Iteration 45/1000 | Loss: 0.00005379
Iteration 46/1000 | Loss: 0.00005379
Iteration 47/1000 | Loss: 0.00005379
Iteration 48/1000 | Loss: 0.00005379
Iteration 49/1000 | Loss: 0.00005378
Iteration 50/1000 | Loss: 0.00005378
Iteration 51/1000 | Loss: 0.00005378
Iteration 52/1000 | Loss: 0.00005378
Iteration 53/1000 | Loss: 0.00005378
Iteration 54/1000 | Loss: 0.00005378
Iteration 55/1000 | Loss: 0.00005378
Iteration 56/1000 | Loss: 0.00005378
Iteration 57/1000 | Loss: 0.00005378
Iteration 58/1000 | Loss: 0.00005378
Iteration 59/1000 | Loss: 0.00005377
Iteration 60/1000 | Loss: 0.00005377
Iteration 61/1000 | Loss: 0.00005376
Iteration 62/1000 | Loss: 0.00005376
Iteration 63/1000 | Loss: 0.00005376
Iteration 64/1000 | Loss: 0.00005376
Iteration 65/1000 | Loss: 0.00005376
Iteration 66/1000 | Loss: 0.00005376
Iteration 67/1000 | Loss: 0.00005376
Iteration 68/1000 | Loss: 0.00005376
Iteration 69/1000 | Loss: 0.00005376
Iteration 70/1000 | Loss: 0.00005376
Iteration 71/1000 | Loss: 0.00005375
Iteration 72/1000 | Loss: 0.00005375
Iteration 73/1000 | Loss: 0.00005375
Iteration 74/1000 | Loss: 0.00005375
Iteration 75/1000 | Loss: 0.00005375
Iteration 76/1000 | Loss: 0.00005374
Iteration 77/1000 | Loss: 0.00005374
Iteration 78/1000 | Loss: 0.00005374
Iteration 79/1000 | Loss: 0.00005374
Iteration 80/1000 | Loss: 0.00005374
Iteration 81/1000 | Loss: 0.00005374
Iteration 82/1000 | Loss: 0.00005374
Iteration 83/1000 | Loss: 0.00005373
Iteration 84/1000 | Loss: 0.00005373
Iteration 85/1000 | Loss: 0.00005373
Iteration 86/1000 | Loss: 0.00005373
Iteration 87/1000 | Loss: 0.00005372
Iteration 88/1000 | Loss: 0.00005372
Iteration 89/1000 | Loss: 0.00005372
Iteration 90/1000 | Loss: 0.00005372
Iteration 91/1000 | Loss: 0.00005372
Iteration 92/1000 | Loss: 0.00005372
Iteration 93/1000 | Loss: 0.00005372
Iteration 94/1000 | Loss: 0.00005372
Iteration 95/1000 | Loss: 0.00005372
Iteration 96/1000 | Loss: 0.00005371
Iteration 97/1000 | Loss: 0.00005371
Iteration 98/1000 | Loss: 0.00005371
Iteration 99/1000 | Loss: 0.00005371
Iteration 100/1000 | Loss: 0.00005371
Iteration 101/1000 | Loss: 0.00005371
Iteration 102/1000 | Loss: 0.00005371
Iteration 103/1000 | Loss: 0.00005371
Iteration 104/1000 | Loss: 0.00005370
Iteration 105/1000 | Loss: 0.00005370
Iteration 106/1000 | Loss: 0.00005370
Iteration 107/1000 | Loss: 0.00005370
Iteration 108/1000 | Loss: 0.00005370
Iteration 109/1000 | Loss: 0.00005370
Iteration 110/1000 | Loss: 0.00005369
Iteration 111/1000 | Loss: 0.00005369
Iteration 112/1000 | Loss: 0.00005369
Iteration 113/1000 | Loss: 0.00005369
Iteration 114/1000 | Loss: 0.00005369
Iteration 115/1000 | Loss: 0.00005369
Iteration 116/1000 | Loss: 0.00005369
Iteration 117/1000 | Loss: 0.00005369
Iteration 118/1000 | Loss: 0.00005369
Iteration 119/1000 | Loss: 0.00005369
Iteration 120/1000 | Loss: 0.00005369
Iteration 121/1000 | Loss: 0.00005368
Iteration 122/1000 | Loss: 0.00005368
Iteration 123/1000 | Loss: 0.00005368
Iteration 124/1000 | Loss: 0.00005368
Iteration 125/1000 | Loss: 0.00005368
Iteration 126/1000 | Loss: 0.00005368
Iteration 127/1000 | Loss: 0.00005368
Iteration 128/1000 | Loss: 0.00005368
Iteration 129/1000 | Loss: 0.00005368
Iteration 130/1000 | Loss: 0.00005368
Iteration 131/1000 | Loss: 0.00005368
Iteration 132/1000 | Loss: 0.00005367
Iteration 133/1000 | Loss: 0.00005367
Iteration 134/1000 | Loss: 0.00005367
Iteration 135/1000 | Loss: 0.00005367
Iteration 136/1000 | Loss: 0.00005367
Iteration 137/1000 | Loss: 0.00005367
Iteration 138/1000 | Loss: 0.00005367
Iteration 139/1000 | Loss: 0.00005367
Iteration 140/1000 | Loss: 0.00005367
Iteration 141/1000 | Loss: 0.00005367
Iteration 142/1000 | Loss: 0.00005367
Iteration 143/1000 | Loss: 0.00005366
Iteration 144/1000 | Loss: 0.00005366
Iteration 145/1000 | Loss: 0.00005366
Iteration 146/1000 | Loss: 0.00005366
Iteration 147/1000 | Loss: 0.00005366
Iteration 148/1000 | Loss: 0.00005366
Iteration 149/1000 | Loss: 0.00005366
Iteration 150/1000 | Loss: 0.00005366
Iteration 151/1000 | Loss: 0.00005366
Iteration 152/1000 | Loss: 0.00005366
Iteration 153/1000 | Loss: 0.00005366
Iteration 154/1000 | Loss: 0.00005366
Iteration 155/1000 | Loss: 0.00005366
Iteration 156/1000 | Loss: 0.00005366
Iteration 157/1000 | Loss: 0.00005366
Iteration 158/1000 | Loss: 0.00005366
Iteration 159/1000 | Loss: 0.00005366
Iteration 160/1000 | Loss: 0.00005366
Iteration 161/1000 | Loss: 0.00005366
Iteration 162/1000 | Loss: 0.00005366
Iteration 163/1000 | Loss: 0.00005366
Iteration 164/1000 | Loss: 0.00005366
Iteration 165/1000 | Loss: 0.00005366
Iteration 166/1000 | Loss: 0.00005366
Iteration 167/1000 | Loss: 0.00005366
Iteration 168/1000 | Loss: 0.00005366
Iteration 169/1000 | Loss: 0.00005366
Iteration 170/1000 | Loss: 0.00005366
Iteration 171/1000 | Loss: 0.00005366
Iteration 172/1000 | Loss: 0.00005366
Iteration 173/1000 | Loss: 0.00005366
Iteration 174/1000 | Loss: 0.00005366
Iteration 175/1000 | Loss: 0.00005366
Iteration 176/1000 | Loss: 0.00005366
Iteration 177/1000 | Loss: 0.00005366
Iteration 178/1000 | Loss: 0.00005366
Iteration 179/1000 | Loss: 0.00005366
Iteration 180/1000 | Loss: 0.00005366
Iteration 181/1000 | Loss: 0.00005366
Iteration 182/1000 | Loss: 0.00005366
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 182. Stopping optimization.
Last 5 losses: [5.366229743231088e-05, 5.366229743231088e-05, 5.366229743231088e-05, 5.366229743231088e-05, 5.366229743231088e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 5.366229743231088e-05

Optimization complete. Final v2v error: 5.997718811035156 mm

Highest mean error: 7.9404144287109375 mm for frame 159

Lowest mean error: 5.205932140350342 mm for frame 96

Saving results

Total time: 60.841015577316284
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_56_us_1894/0009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_56_us_1894/0009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_56_us_1894/0009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00931752
Iteration 2/25 | Loss: 0.00229617
Iteration 3/25 | Loss: 0.00209810
Iteration 4/25 | Loss: 0.00208548
Iteration 5/25 | Loss: 0.00208497
Iteration 6/25 | Loss: 0.00208497
Iteration 7/25 | Loss: 0.00208497
Iteration 8/25 | Loss: 0.00208497
Iteration 9/25 | Loss: 0.00208497
Iteration 10/25 | Loss: 0.00208497
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0020849741995334625, 0.0020849741995334625, 0.0020849741995334625, 0.0020849741995334625, 0.0020849741995334625]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0020849741995334625

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.79965723
Iteration 2/25 | Loss: 0.00173945
Iteration 3/25 | Loss: 0.00173945
Iteration 4/25 | Loss: 0.00173945
Iteration 5/25 | Loss: 0.00173945
Iteration 6/25 | Loss: 0.00173945
Iteration 7/25 | Loss: 0.00173945
Iteration 8/25 | Loss: 0.00173945
Iteration 9/25 | Loss: 0.00173945
Iteration 10/25 | Loss: 0.00173945
Iteration 11/25 | Loss: 0.00173945
Iteration 12/25 | Loss: 0.00173945
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0017394520109519362, 0.0017394520109519362, 0.0017394520109519362, 0.0017394520109519362, 0.0017394520109519362]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0017394520109519362

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00173945
Iteration 2/1000 | Loss: 0.00007377
Iteration 3/1000 | Loss: 0.00004880
Iteration 4/1000 | Loss: 0.00004253
Iteration 5/1000 | Loss: 0.00004059
Iteration 6/1000 | Loss: 0.00003930
Iteration 7/1000 | Loss: 0.00003825
Iteration 8/1000 | Loss: 0.00003764
Iteration 9/1000 | Loss: 0.00003718
Iteration 10/1000 | Loss: 0.00003696
Iteration 11/1000 | Loss: 0.00003679
Iteration 12/1000 | Loss: 0.00003677
Iteration 13/1000 | Loss: 0.00003664
Iteration 14/1000 | Loss: 0.00003648
Iteration 15/1000 | Loss: 0.00003645
Iteration 16/1000 | Loss: 0.00003644
Iteration 17/1000 | Loss: 0.00003644
Iteration 18/1000 | Loss: 0.00003642
Iteration 19/1000 | Loss: 0.00003641
Iteration 20/1000 | Loss: 0.00003641
Iteration 21/1000 | Loss: 0.00003641
Iteration 22/1000 | Loss: 0.00003639
Iteration 23/1000 | Loss: 0.00003639
Iteration 24/1000 | Loss: 0.00003638
Iteration 25/1000 | Loss: 0.00003638
Iteration 26/1000 | Loss: 0.00003638
Iteration 27/1000 | Loss: 0.00003638
Iteration 28/1000 | Loss: 0.00003638
Iteration 29/1000 | Loss: 0.00003638
Iteration 30/1000 | Loss: 0.00003638
Iteration 31/1000 | Loss: 0.00003638
Iteration 32/1000 | Loss: 0.00003638
Iteration 33/1000 | Loss: 0.00003638
Iteration 34/1000 | Loss: 0.00003637
Iteration 35/1000 | Loss: 0.00003637
Iteration 36/1000 | Loss: 0.00003637
Iteration 37/1000 | Loss: 0.00003636
Iteration 38/1000 | Loss: 0.00003636
Iteration 39/1000 | Loss: 0.00003636
Iteration 40/1000 | Loss: 0.00003636
Iteration 41/1000 | Loss: 0.00003635
Iteration 42/1000 | Loss: 0.00003635
Iteration 43/1000 | Loss: 0.00003635
Iteration 44/1000 | Loss: 0.00003635
Iteration 45/1000 | Loss: 0.00003635
Iteration 46/1000 | Loss: 0.00003635
Iteration 47/1000 | Loss: 0.00003635
Iteration 48/1000 | Loss: 0.00003635
Iteration 49/1000 | Loss: 0.00003635
Iteration 50/1000 | Loss: 0.00003635
Iteration 51/1000 | Loss: 0.00003635
Iteration 52/1000 | Loss: 0.00003634
Iteration 53/1000 | Loss: 0.00003634
Iteration 54/1000 | Loss: 0.00003634
Iteration 55/1000 | Loss: 0.00003634
Iteration 56/1000 | Loss: 0.00003634
Iteration 57/1000 | Loss: 0.00003634
Iteration 58/1000 | Loss: 0.00003633
Iteration 59/1000 | Loss: 0.00003633
Iteration 60/1000 | Loss: 0.00003633
Iteration 61/1000 | Loss: 0.00003633
Iteration 62/1000 | Loss: 0.00003633
Iteration 63/1000 | Loss: 0.00003633
Iteration 64/1000 | Loss: 0.00003633
Iteration 65/1000 | Loss: 0.00003633
Iteration 66/1000 | Loss: 0.00003632
Iteration 67/1000 | Loss: 0.00003632
Iteration 68/1000 | Loss: 0.00003632
Iteration 69/1000 | Loss: 0.00003632
Iteration 70/1000 | Loss: 0.00003632
Iteration 71/1000 | Loss: 0.00003632
Iteration 72/1000 | Loss: 0.00003632
Iteration 73/1000 | Loss: 0.00003632
Iteration 74/1000 | Loss: 0.00003632
Iteration 75/1000 | Loss: 0.00003632
Iteration 76/1000 | Loss: 0.00003631
Iteration 77/1000 | Loss: 0.00003631
Iteration 78/1000 | Loss: 0.00003631
Iteration 79/1000 | Loss: 0.00003631
Iteration 80/1000 | Loss: 0.00003631
Iteration 81/1000 | Loss: 0.00003631
Iteration 82/1000 | Loss: 0.00003631
Iteration 83/1000 | Loss: 0.00003631
Iteration 84/1000 | Loss: 0.00003631
Iteration 85/1000 | Loss: 0.00003631
Iteration 86/1000 | Loss: 0.00003631
Iteration 87/1000 | Loss: 0.00003631
Iteration 88/1000 | Loss: 0.00003631
Iteration 89/1000 | Loss: 0.00003631
Iteration 90/1000 | Loss: 0.00003631
Iteration 91/1000 | Loss: 0.00003631
Iteration 92/1000 | Loss: 0.00003631
Iteration 93/1000 | Loss: 0.00003631
Iteration 94/1000 | Loss: 0.00003631
Iteration 95/1000 | Loss: 0.00003631
Iteration 96/1000 | Loss: 0.00003631
Iteration 97/1000 | Loss: 0.00003631
Iteration 98/1000 | Loss: 0.00003631
Iteration 99/1000 | Loss: 0.00003631
Iteration 100/1000 | Loss: 0.00003631
Iteration 101/1000 | Loss: 0.00003631
Iteration 102/1000 | Loss: 0.00003631
Iteration 103/1000 | Loss: 0.00003631
Iteration 104/1000 | Loss: 0.00003631
Iteration 105/1000 | Loss: 0.00003631
Iteration 106/1000 | Loss: 0.00003631
Iteration 107/1000 | Loss: 0.00003631
Iteration 108/1000 | Loss: 0.00003631
Iteration 109/1000 | Loss: 0.00003631
Iteration 110/1000 | Loss: 0.00003631
Iteration 111/1000 | Loss: 0.00003631
Iteration 112/1000 | Loss: 0.00003631
Iteration 113/1000 | Loss: 0.00003631
Iteration 114/1000 | Loss: 0.00003631
Iteration 115/1000 | Loss: 0.00003631
Iteration 116/1000 | Loss: 0.00003631
Iteration 117/1000 | Loss: 0.00003631
Iteration 118/1000 | Loss: 0.00003631
Iteration 119/1000 | Loss: 0.00003631
Iteration 120/1000 | Loss: 0.00003631
Iteration 121/1000 | Loss: 0.00003631
Iteration 122/1000 | Loss: 0.00003631
Iteration 123/1000 | Loss: 0.00003631
Iteration 124/1000 | Loss: 0.00003631
Iteration 125/1000 | Loss: 0.00003631
Iteration 126/1000 | Loss: 0.00003631
Iteration 127/1000 | Loss: 0.00003631
Iteration 128/1000 | Loss: 0.00003631
Iteration 129/1000 | Loss: 0.00003631
Iteration 130/1000 | Loss: 0.00003631
Iteration 131/1000 | Loss: 0.00003631
Iteration 132/1000 | Loss: 0.00003631
Iteration 133/1000 | Loss: 0.00003631
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 133. Stopping optimization.
Last 5 losses: [3.630694482126273e-05, 3.630694482126273e-05, 3.630694482126273e-05, 3.630694482126273e-05, 3.630694482126273e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.630694482126273e-05

Optimization complete. Final v2v error: 5.183143138885498 mm

Highest mean error: 5.258833885192871 mm for frame 239

Lowest mean error: 5.117264270782471 mm for frame 27

Saving results

Total time: 35.394352436065674
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_011/1042/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_011/1042.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_011/1042
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00903405
Iteration 2/25 | Loss: 0.00137707
Iteration 3/25 | Loss: 0.00128255
Iteration 4/25 | Loss: 0.00126075
Iteration 5/25 | Loss: 0.00125530
Iteration 6/25 | Loss: 0.00125423
Iteration 7/25 | Loss: 0.00125423
Iteration 8/25 | Loss: 0.00125423
Iteration 9/25 | Loss: 0.00125423
Iteration 10/25 | Loss: 0.00125423
Iteration 11/25 | Loss: 0.00125423
Iteration 12/25 | Loss: 0.00125423
Iteration 13/25 | Loss: 0.00125423
Iteration 14/25 | Loss: 0.00125423
Iteration 15/25 | Loss: 0.00125423
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0012542330659925938, 0.0012542330659925938, 0.0012542330659925938, 0.0012542330659925938, 0.0012542330659925938]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012542330659925938

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.38063383
Iteration 2/25 | Loss: 0.00140282
Iteration 3/25 | Loss: 0.00140278
Iteration 4/25 | Loss: 0.00140277
Iteration 5/25 | Loss: 0.00140277
Iteration 6/25 | Loss: 0.00140277
Iteration 7/25 | Loss: 0.00140277
Iteration 8/25 | Loss: 0.00140277
Iteration 9/25 | Loss: 0.00140277
Iteration 10/25 | Loss: 0.00140277
Iteration 11/25 | Loss: 0.00140277
Iteration 12/25 | Loss: 0.00140277
Iteration 13/25 | Loss: 0.00140277
Iteration 14/25 | Loss: 0.00140277
Iteration 15/25 | Loss: 0.00140277
Iteration 16/25 | Loss: 0.00140277
Iteration 17/25 | Loss: 0.00140277
Iteration 18/25 | Loss: 0.00140277
Iteration 19/25 | Loss: 0.00140277
Iteration 20/25 | Loss: 0.00140277
Iteration 21/25 | Loss: 0.00140277
Iteration 22/25 | Loss: 0.00140277
Iteration 23/25 | Loss: 0.00140277
Iteration 24/25 | Loss: 0.00140277
Iteration 25/25 | Loss: 0.00140277

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00140277
Iteration 2/1000 | Loss: 0.00003904
Iteration 3/1000 | Loss: 0.00002685
Iteration 4/1000 | Loss: 0.00002175
Iteration 5/1000 | Loss: 0.00002061
Iteration 6/1000 | Loss: 0.00001981
Iteration 7/1000 | Loss: 0.00001936
Iteration 8/1000 | Loss: 0.00001889
Iteration 9/1000 | Loss: 0.00001852
Iteration 10/1000 | Loss: 0.00001815
Iteration 11/1000 | Loss: 0.00001784
Iteration 12/1000 | Loss: 0.00001765
Iteration 13/1000 | Loss: 0.00001752
Iteration 14/1000 | Loss: 0.00001744
Iteration 15/1000 | Loss: 0.00001735
Iteration 16/1000 | Loss: 0.00001730
Iteration 17/1000 | Loss: 0.00001730
Iteration 18/1000 | Loss: 0.00001729
Iteration 19/1000 | Loss: 0.00001729
Iteration 20/1000 | Loss: 0.00001728
Iteration 21/1000 | Loss: 0.00001728
Iteration 22/1000 | Loss: 0.00001727
Iteration 23/1000 | Loss: 0.00001726
Iteration 24/1000 | Loss: 0.00001726
Iteration 25/1000 | Loss: 0.00001726
Iteration 26/1000 | Loss: 0.00001726
Iteration 27/1000 | Loss: 0.00001725
Iteration 28/1000 | Loss: 0.00001725
Iteration 29/1000 | Loss: 0.00001725
Iteration 30/1000 | Loss: 0.00001725
Iteration 31/1000 | Loss: 0.00001725
Iteration 32/1000 | Loss: 0.00001725
Iteration 33/1000 | Loss: 0.00001725
Iteration 34/1000 | Loss: 0.00001725
Iteration 35/1000 | Loss: 0.00001725
Iteration 36/1000 | Loss: 0.00001725
Iteration 37/1000 | Loss: 0.00001725
Iteration 38/1000 | Loss: 0.00001725
Iteration 39/1000 | Loss: 0.00001724
Iteration 40/1000 | Loss: 0.00001724
Iteration 41/1000 | Loss: 0.00001724
Iteration 42/1000 | Loss: 0.00001724
Iteration 43/1000 | Loss: 0.00001723
Iteration 44/1000 | Loss: 0.00001722
Iteration 45/1000 | Loss: 0.00001722
Iteration 46/1000 | Loss: 0.00001721
Iteration 47/1000 | Loss: 0.00001721
Iteration 48/1000 | Loss: 0.00001721
Iteration 49/1000 | Loss: 0.00001720
Iteration 50/1000 | Loss: 0.00001720
Iteration 51/1000 | Loss: 0.00001720
Iteration 52/1000 | Loss: 0.00001719
Iteration 53/1000 | Loss: 0.00001719
Iteration 54/1000 | Loss: 0.00001719
Iteration 55/1000 | Loss: 0.00001719
Iteration 56/1000 | Loss: 0.00001719
Iteration 57/1000 | Loss: 0.00001719
Iteration 58/1000 | Loss: 0.00001719
Iteration 59/1000 | Loss: 0.00001719
Iteration 60/1000 | Loss: 0.00001719
Iteration 61/1000 | Loss: 0.00001719
Iteration 62/1000 | Loss: 0.00001719
Iteration 63/1000 | Loss: 0.00001719
Iteration 64/1000 | Loss: 0.00001719
Iteration 65/1000 | Loss: 0.00001719
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 65. Stopping optimization.
Last 5 losses: [1.718884777801577e-05, 1.718884777801577e-05, 1.718884777801577e-05, 1.718884777801577e-05, 1.718884777801577e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.718884777801577e-05

Optimization complete. Final v2v error: 3.552684783935547 mm

Highest mean error: 4.041794300079346 mm for frame 65

Lowest mean error: 3.175692319869995 mm for frame 47

Saving results

Total time: 33.55530571937561
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_011/1022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_011/1022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_011/1022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00952177
Iteration 2/25 | Loss: 0.00370158
Iteration 3/25 | Loss: 0.00255784
Iteration 4/25 | Loss: 0.00238162
Iteration 5/25 | Loss: 0.00236238
Iteration 6/25 | Loss: 0.00228954
Iteration 7/25 | Loss: 0.00218848
Iteration 8/25 | Loss: 0.00206137
Iteration 9/25 | Loss: 0.00202358
Iteration 10/25 | Loss: 0.00200575
Iteration 11/25 | Loss: 0.00191995
Iteration 12/25 | Loss: 0.00189021
Iteration 13/25 | Loss: 0.00187870
Iteration 14/25 | Loss: 0.00187411
Iteration 15/25 | Loss: 0.00186340
Iteration 16/25 | Loss: 0.00185899
Iteration 17/25 | Loss: 0.00186324
Iteration 18/25 | Loss: 0.00186124
Iteration 19/25 | Loss: 0.00185646
Iteration 20/25 | Loss: 0.00186002
Iteration 21/25 | Loss: 0.00185638
Iteration 22/25 | Loss: 0.00185604
Iteration 23/25 | Loss: 0.00185603
Iteration 24/25 | Loss: 0.00185603
Iteration 25/25 | Loss: 0.00185603

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.30109239
Iteration 2/25 | Loss: 0.00480016
Iteration 3/25 | Loss: 0.00374450
Iteration 4/25 | Loss: 0.00374450
Iteration 5/25 | Loss: 0.00374450
Iteration 6/25 | Loss: 0.00374450
Iteration 7/25 | Loss: 0.00374449
Iteration 8/25 | Loss: 0.00374449
Iteration 9/25 | Loss: 0.00374449
Iteration 10/25 | Loss: 0.00374449
Iteration 11/25 | Loss: 0.00374449
Iteration 12/25 | Loss: 0.00374449
Iteration 13/25 | Loss: 0.00374449
Iteration 14/25 | Loss: 0.00374449
Iteration 15/25 | Loss: 0.00374449
Iteration 16/25 | Loss: 0.00374449
Iteration 17/25 | Loss: 0.00374449
Iteration 18/25 | Loss: 0.00374449
Iteration 19/25 | Loss: 0.00374449
Iteration 20/25 | Loss: 0.00374449
Iteration 21/25 | Loss: 0.00374449
Iteration 22/25 | Loss: 0.00374449
Iteration 23/25 | Loss: 0.00374449
Iteration 24/25 | Loss: 0.00374449
Iteration 25/25 | Loss: 0.00374449

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00374449
Iteration 2/1000 | Loss: 0.00108520
Iteration 3/1000 | Loss: 0.00065330
Iteration 4/1000 | Loss: 0.00223356
Iteration 5/1000 | Loss: 0.00173537
Iteration 6/1000 | Loss: 0.00034686
Iteration 7/1000 | Loss: 0.00032140
Iteration 8/1000 | Loss: 0.00041438
Iteration 9/1000 | Loss: 0.00083795
Iteration 10/1000 | Loss: 0.00028679
Iteration 11/1000 | Loss: 0.00048476
Iteration 12/1000 | Loss: 0.00164732
Iteration 13/1000 | Loss: 0.00926782
Iteration 14/1000 | Loss: 0.00230524
Iteration 15/1000 | Loss: 0.00089324
Iteration 16/1000 | Loss: 0.00079327
Iteration 17/1000 | Loss: 0.00118476
Iteration 18/1000 | Loss: 0.00046034
Iteration 19/1000 | Loss: 0.00069145
Iteration 20/1000 | Loss: 0.00094344
Iteration 21/1000 | Loss: 0.00060789
Iteration 22/1000 | Loss: 0.00187120
Iteration 23/1000 | Loss: 0.00183293
Iteration 24/1000 | Loss: 0.00113101
Iteration 25/1000 | Loss: 0.00049401
Iteration 26/1000 | Loss: 0.00018964
Iteration 27/1000 | Loss: 0.00048204
Iteration 28/1000 | Loss: 0.00041678
Iteration 29/1000 | Loss: 0.00007384
Iteration 30/1000 | Loss: 0.00061888
Iteration 31/1000 | Loss: 0.00014589
Iteration 32/1000 | Loss: 0.00056631
Iteration 33/1000 | Loss: 0.00028848
Iteration 34/1000 | Loss: 0.00053942
Iteration 35/1000 | Loss: 0.00004271
Iteration 36/1000 | Loss: 0.00029287
Iteration 37/1000 | Loss: 0.00013890
Iteration 38/1000 | Loss: 0.00154238
Iteration 39/1000 | Loss: 0.00011300
Iteration 40/1000 | Loss: 0.00014575
Iteration 41/1000 | Loss: 0.00013667
Iteration 42/1000 | Loss: 0.00009660
Iteration 43/1000 | Loss: 0.00037909
Iteration 44/1000 | Loss: 0.00208059
Iteration 45/1000 | Loss: 0.00013121
Iteration 46/1000 | Loss: 0.00018093
Iteration 47/1000 | Loss: 0.00060565
Iteration 48/1000 | Loss: 0.00291541
Iteration 49/1000 | Loss: 0.00060906
Iteration 50/1000 | Loss: 0.00017703
Iteration 51/1000 | Loss: 0.00004394
Iteration 52/1000 | Loss: 0.00004453
Iteration 53/1000 | Loss: 0.00001756
Iteration 54/1000 | Loss: 0.00010451
Iteration 55/1000 | Loss: 0.00003973
Iteration 56/1000 | Loss: 0.00069039
Iteration 57/1000 | Loss: 0.00013836
Iteration 58/1000 | Loss: 0.00003432
Iteration 59/1000 | Loss: 0.00003112
Iteration 60/1000 | Loss: 0.00001996
Iteration 61/1000 | Loss: 0.00012909
Iteration 62/1000 | Loss: 0.00019170
Iteration 63/1000 | Loss: 0.00001787
Iteration 64/1000 | Loss: 0.00003352
Iteration 65/1000 | Loss: 0.00001450
Iteration 66/1000 | Loss: 0.00006136
Iteration 67/1000 | Loss: 0.00002229
Iteration 68/1000 | Loss: 0.00005683
Iteration 69/1000 | Loss: 0.00103732
Iteration 70/1000 | Loss: 0.00002644
Iteration 71/1000 | Loss: 0.00004374
Iteration 72/1000 | Loss: 0.00001443
Iteration 73/1000 | Loss: 0.00002801
Iteration 74/1000 | Loss: 0.00001413
Iteration 75/1000 | Loss: 0.00001411
Iteration 76/1000 | Loss: 0.00001407
Iteration 77/1000 | Loss: 0.00002117
Iteration 78/1000 | Loss: 0.00001405
Iteration 79/1000 | Loss: 0.00001404
Iteration 80/1000 | Loss: 0.00001403
Iteration 81/1000 | Loss: 0.00001402
Iteration 82/1000 | Loss: 0.00001402
Iteration 83/1000 | Loss: 0.00001402
Iteration 84/1000 | Loss: 0.00001402
Iteration 85/1000 | Loss: 0.00001402
Iteration 86/1000 | Loss: 0.00001402
Iteration 87/1000 | Loss: 0.00001400
Iteration 88/1000 | Loss: 0.00005102
Iteration 89/1000 | Loss: 0.00001485
Iteration 90/1000 | Loss: 0.00001988
Iteration 91/1000 | Loss: 0.00001393
Iteration 92/1000 | Loss: 0.00001388
Iteration 93/1000 | Loss: 0.00001386
Iteration 94/1000 | Loss: 0.00001383
Iteration 95/1000 | Loss: 0.00001383
Iteration 96/1000 | Loss: 0.00002892
Iteration 97/1000 | Loss: 0.00001383
Iteration 98/1000 | Loss: 0.00001382
Iteration 99/1000 | Loss: 0.00001382
Iteration 100/1000 | Loss: 0.00001382
Iteration 101/1000 | Loss: 0.00001382
Iteration 102/1000 | Loss: 0.00001382
Iteration 103/1000 | Loss: 0.00001382
Iteration 104/1000 | Loss: 0.00001381
Iteration 105/1000 | Loss: 0.00001381
Iteration 106/1000 | Loss: 0.00001381
Iteration 107/1000 | Loss: 0.00001381
Iteration 108/1000 | Loss: 0.00001381
Iteration 109/1000 | Loss: 0.00003563
Iteration 110/1000 | Loss: 0.00001384
Iteration 111/1000 | Loss: 0.00007307
Iteration 112/1000 | Loss: 0.00006858
Iteration 113/1000 | Loss: 0.00002731
Iteration 114/1000 | Loss: 0.00002078
Iteration 115/1000 | Loss: 0.00004353
Iteration 116/1000 | Loss: 0.00001418
Iteration 117/1000 | Loss: 0.00005987
Iteration 118/1000 | Loss: 0.00002009
Iteration 119/1000 | Loss: 0.00001545
Iteration 120/1000 | Loss: 0.00001371
Iteration 121/1000 | Loss: 0.00001371
Iteration 122/1000 | Loss: 0.00001371
Iteration 123/1000 | Loss: 0.00001370
Iteration 124/1000 | Loss: 0.00001370
Iteration 125/1000 | Loss: 0.00001370
Iteration 126/1000 | Loss: 0.00001370
Iteration 127/1000 | Loss: 0.00001370
Iteration 128/1000 | Loss: 0.00001370
Iteration 129/1000 | Loss: 0.00001370
Iteration 130/1000 | Loss: 0.00001370
Iteration 131/1000 | Loss: 0.00001370
Iteration 132/1000 | Loss: 0.00001370
Iteration 133/1000 | Loss: 0.00001370
Iteration 134/1000 | Loss: 0.00001370
Iteration 135/1000 | Loss: 0.00001370
Iteration 136/1000 | Loss: 0.00001370
Iteration 137/1000 | Loss: 0.00001370
Iteration 138/1000 | Loss: 0.00001370
Iteration 139/1000 | Loss: 0.00001370
Iteration 140/1000 | Loss: 0.00001370
Iteration 141/1000 | Loss: 0.00001370
Iteration 142/1000 | Loss: 0.00001370
Iteration 143/1000 | Loss: 0.00001370
Iteration 144/1000 | Loss: 0.00001370
Iteration 145/1000 | Loss: 0.00001370
Iteration 146/1000 | Loss: 0.00001370
Iteration 147/1000 | Loss: 0.00001370
Iteration 148/1000 | Loss: 0.00001370
Iteration 149/1000 | Loss: 0.00001370
Iteration 150/1000 | Loss: 0.00001370
Iteration 151/1000 | Loss: 0.00001370
Iteration 152/1000 | Loss: 0.00001370
Iteration 153/1000 | Loss: 0.00001370
Iteration 154/1000 | Loss: 0.00001370
Iteration 155/1000 | Loss: 0.00001370
Iteration 156/1000 | Loss: 0.00001370
Iteration 157/1000 | Loss: 0.00001370
Iteration 158/1000 | Loss: 0.00001370
Iteration 159/1000 | Loss: 0.00001370
Iteration 160/1000 | Loss: 0.00001370
Iteration 161/1000 | Loss: 0.00001370
Iteration 162/1000 | Loss: 0.00001370
Iteration 163/1000 | Loss: 0.00001370
Iteration 164/1000 | Loss: 0.00001370
Iteration 165/1000 | Loss: 0.00001370
Iteration 166/1000 | Loss: 0.00001369
Iteration 167/1000 | Loss: 0.00001369
Iteration 168/1000 | Loss: 0.00001369
Iteration 169/1000 | Loss: 0.00001369
Iteration 170/1000 | Loss: 0.00001369
Iteration 171/1000 | Loss: 0.00001369
Iteration 172/1000 | Loss: 0.00001369
Iteration 173/1000 | Loss: 0.00001369
Iteration 174/1000 | Loss: 0.00001369
Iteration 175/1000 | Loss: 0.00001369
Iteration 176/1000 | Loss: 0.00001369
Iteration 177/1000 | Loss: 0.00001369
Iteration 178/1000 | Loss: 0.00001369
Iteration 179/1000 | Loss: 0.00001369
Iteration 180/1000 | Loss: 0.00001369
Iteration 181/1000 | Loss: 0.00001369
Iteration 182/1000 | Loss: 0.00001369
Iteration 183/1000 | Loss: 0.00001369
Iteration 184/1000 | Loss: 0.00001369
Iteration 185/1000 | Loss: 0.00001369
Iteration 186/1000 | Loss: 0.00001369
Iteration 187/1000 | Loss: 0.00001369
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 187. Stopping optimization.
Last 5 losses: [1.369418441754533e-05, 1.369418441754533e-05, 1.369418441754533e-05, 1.369418441754533e-05, 1.369418441754533e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.369418441754533e-05

Optimization complete. Final v2v error: 3.1695945262908936 mm

Highest mean error: 3.7799806594848633 mm for frame 141

Lowest mean error: 2.904599189758301 mm for frame 46

Saving results

Total time: 167.77161359786987
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_011/1000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_011/1000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_011/1000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01001852
Iteration 2/25 | Loss: 0.00260417
Iteration 3/25 | Loss: 0.00250860
Iteration 4/25 | Loss: 0.00214922
Iteration 5/25 | Loss: 0.00200319
Iteration 6/25 | Loss: 0.00184596
Iteration 7/25 | Loss: 0.00163288
Iteration 8/25 | Loss: 0.00153330
Iteration 9/25 | Loss: 0.00149002
Iteration 10/25 | Loss: 0.00148205
Iteration 11/25 | Loss: 0.00146391
Iteration 12/25 | Loss: 0.00142750
Iteration 13/25 | Loss: 0.00141394
Iteration 14/25 | Loss: 0.00140877
Iteration 15/25 | Loss: 0.00140112
Iteration 16/25 | Loss: 0.00139912
Iteration 17/25 | Loss: 0.00140456
Iteration 18/25 | Loss: 0.00140399
Iteration 19/25 | Loss: 0.00140081
Iteration 20/25 | Loss: 0.00139855
Iteration 21/25 | Loss: 0.00139408
Iteration 22/25 | Loss: 0.00139299
Iteration 23/25 | Loss: 0.00139266
Iteration 24/25 | Loss: 0.00139260
Iteration 25/25 | Loss: 0.00139260

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36985278
Iteration 2/25 | Loss: 0.00156147
Iteration 3/25 | Loss: 0.00134019
Iteration 4/25 | Loss: 0.00115345
Iteration 5/25 | Loss: 0.00115345
Iteration 6/25 | Loss: 0.00115345
Iteration 7/25 | Loss: 0.00115344
Iteration 8/25 | Loss: 0.00115344
Iteration 9/25 | Loss: 0.00115344
Iteration 10/25 | Loss: 0.00115344
Iteration 11/25 | Loss: 0.00115344
Iteration 12/25 | Loss: 0.00115344
Iteration 13/25 | Loss: 0.00115344
Iteration 14/25 | Loss: 0.00115344
Iteration 15/25 | Loss: 0.00115344
Iteration 16/25 | Loss: 0.00115344
Iteration 17/25 | Loss: 0.00115344
Iteration 18/25 | Loss: 0.00115344
Iteration 19/25 | Loss: 0.00115344
Iteration 20/25 | Loss: 0.00115344
Iteration 21/25 | Loss: 0.00115344
Iteration 22/25 | Loss: 0.00115344
Iteration 23/25 | Loss: 0.00115344
Iteration 24/25 | Loss: 0.00115344
Iteration 25/25 | Loss: 0.00115344

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00115344
Iteration 2/1000 | Loss: 0.00188223
Iteration 3/1000 | Loss: 0.00020361
Iteration 4/1000 | Loss: 0.00043177
Iteration 5/1000 | Loss: 0.00011124
Iteration 6/1000 | Loss: 0.00024217
Iteration 7/1000 | Loss: 0.00007943
Iteration 8/1000 | Loss: 0.00076806
Iteration 9/1000 | Loss: 0.00006108
Iteration 10/1000 | Loss: 0.00007610
Iteration 11/1000 | Loss: 0.00005490
Iteration 12/1000 | Loss: 0.00005329
Iteration 13/1000 | Loss: 0.00006628
Iteration 14/1000 | Loss: 0.00005140
Iteration 15/1000 | Loss: 0.00005023
Iteration 16/1000 | Loss: 0.00029659
Iteration 17/1000 | Loss: 0.00257009
Iteration 18/1000 | Loss: 0.00358076
Iteration 19/1000 | Loss: 0.00150574
Iteration 20/1000 | Loss: 0.00024311
Iteration 21/1000 | Loss: 0.00016080
Iteration 22/1000 | Loss: 0.00011265
Iteration 23/1000 | Loss: 0.00136649
Iteration 24/1000 | Loss: 0.00064990
Iteration 25/1000 | Loss: 0.00009411
Iteration 26/1000 | Loss: 0.00005443
Iteration 27/1000 | Loss: 0.00004443
Iteration 28/1000 | Loss: 0.00045186
Iteration 29/1000 | Loss: 0.00007943
Iteration 30/1000 | Loss: 0.00004366
Iteration 31/1000 | Loss: 0.00003917
Iteration 32/1000 | Loss: 0.00003099
Iteration 33/1000 | Loss: 0.00005528
Iteration 34/1000 | Loss: 0.00002972
Iteration 35/1000 | Loss: 0.00002650
Iteration 36/1000 | Loss: 0.00032330
Iteration 37/1000 | Loss: 0.00038239
Iteration 38/1000 | Loss: 0.00002760
Iteration 39/1000 | Loss: 0.00002513
Iteration 40/1000 | Loss: 0.00007206
Iteration 41/1000 | Loss: 0.00002457
Iteration 42/1000 | Loss: 0.00002403
Iteration 43/1000 | Loss: 0.00035002
Iteration 44/1000 | Loss: 0.00002386
Iteration 45/1000 | Loss: 0.00010685
Iteration 46/1000 | Loss: 0.00002966
Iteration 47/1000 | Loss: 0.00002498
Iteration 48/1000 | Loss: 0.00020435
Iteration 49/1000 | Loss: 0.00002339
Iteration 50/1000 | Loss: 0.00002275
Iteration 51/1000 | Loss: 0.00002255
Iteration 52/1000 | Loss: 0.00002231
Iteration 53/1000 | Loss: 0.00002207
Iteration 54/1000 | Loss: 0.00002189
Iteration 55/1000 | Loss: 0.00002183
Iteration 56/1000 | Loss: 0.00002182
Iteration 57/1000 | Loss: 0.00002177
Iteration 58/1000 | Loss: 0.00002175
Iteration 59/1000 | Loss: 0.00002174
Iteration 60/1000 | Loss: 0.00002174
Iteration 61/1000 | Loss: 0.00002173
Iteration 62/1000 | Loss: 0.00002173
Iteration 63/1000 | Loss: 0.00002172
Iteration 64/1000 | Loss: 0.00002172
Iteration 65/1000 | Loss: 0.00002172
Iteration 66/1000 | Loss: 0.00002171
Iteration 67/1000 | Loss: 0.00002171
Iteration 68/1000 | Loss: 0.00002171
Iteration 69/1000 | Loss: 0.00002170
Iteration 70/1000 | Loss: 0.00002170
Iteration 71/1000 | Loss: 0.00002169
Iteration 72/1000 | Loss: 0.00002169
Iteration 73/1000 | Loss: 0.00002168
Iteration 74/1000 | Loss: 0.00002168
Iteration 75/1000 | Loss: 0.00002167
Iteration 76/1000 | Loss: 0.00002158
Iteration 77/1000 | Loss: 0.00002158
Iteration 78/1000 | Loss: 0.00002157
Iteration 79/1000 | Loss: 0.00002156
Iteration 80/1000 | Loss: 0.00002155
Iteration 81/1000 | Loss: 0.00002154
Iteration 82/1000 | Loss: 0.00002154
Iteration 83/1000 | Loss: 0.00002153
Iteration 84/1000 | Loss: 0.00002152
Iteration 85/1000 | Loss: 0.00002151
Iteration 86/1000 | Loss: 0.00002147
Iteration 87/1000 | Loss: 0.00002147
Iteration 88/1000 | Loss: 0.00002147
Iteration 89/1000 | Loss: 0.00002147
Iteration 90/1000 | Loss: 0.00002146
Iteration 91/1000 | Loss: 0.00002146
Iteration 92/1000 | Loss: 0.00002146
Iteration 93/1000 | Loss: 0.00002146
Iteration 94/1000 | Loss: 0.00002146
Iteration 95/1000 | Loss: 0.00002145
Iteration 96/1000 | Loss: 0.00002145
Iteration 97/1000 | Loss: 0.00002145
Iteration 98/1000 | Loss: 0.00002145
Iteration 99/1000 | Loss: 0.00002144
Iteration 100/1000 | Loss: 0.00002144
Iteration 101/1000 | Loss: 0.00002144
Iteration 102/1000 | Loss: 0.00002143
Iteration 103/1000 | Loss: 0.00002143
Iteration 104/1000 | Loss: 0.00002143
Iteration 105/1000 | Loss: 0.00002143
Iteration 106/1000 | Loss: 0.00002143
Iteration 107/1000 | Loss: 0.00002143
Iteration 108/1000 | Loss: 0.00002143
Iteration 109/1000 | Loss: 0.00002143
Iteration 110/1000 | Loss: 0.00002143
Iteration 111/1000 | Loss: 0.00002142
Iteration 112/1000 | Loss: 0.00002142
Iteration 113/1000 | Loss: 0.00002142
Iteration 114/1000 | Loss: 0.00002142
Iteration 115/1000 | Loss: 0.00002142
Iteration 116/1000 | Loss: 0.00002142
Iteration 117/1000 | Loss: 0.00002142
Iteration 118/1000 | Loss: 0.00002142
Iteration 119/1000 | Loss: 0.00002141
Iteration 120/1000 | Loss: 0.00002141
Iteration 121/1000 | Loss: 0.00002141
Iteration 122/1000 | Loss: 0.00002141
Iteration 123/1000 | Loss: 0.00002141
Iteration 124/1000 | Loss: 0.00002141
Iteration 125/1000 | Loss: 0.00002141
Iteration 126/1000 | Loss: 0.00002141
Iteration 127/1000 | Loss: 0.00002141
Iteration 128/1000 | Loss: 0.00002141
Iteration 129/1000 | Loss: 0.00002141
Iteration 130/1000 | Loss: 0.00002141
Iteration 131/1000 | Loss: 0.00002141
Iteration 132/1000 | Loss: 0.00002141
Iteration 133/1000 | Loss: 0.00002140
Iteration 134/1000 | Loss: 0.00002140
Iteration 135/1000 | Loss: 0.00002140
Iteration 136/1000 | Loss: 0.00002140
Iteration 137/1000 | Loss: 0.00002140
Iteration 138/1000 | Loss: 0.00002140
Iteration 139/1000 | Loss: 0.00002140
Iteration 140/1000 | Loss: 0.00002140
Iteration 141/1000 | Loss: 0.00002140
Iteration 142/1000 | Loss: 0.00002139
Iteration 143/1000 | Loss: 0.00002139
Iteration 144/1000 | Loss: 0.00002139
Iteration 145/1000 | Loss: 0.00002139
Iteration 146/1000 | Loss: 0.00002138
Iteration 147/1000 | Loss: 0.00002138
Iteration 148/1000 | Loss: 0.00002138
Iteration 149/1000 | Loss: 0.00002138
Iteration 150/1000 | Loss: 0.00002138
Iteration 151/1000 | Loss: 0.00002138
Iteration 152/1000 | Loss: 0.00002137
Iteration 153/1000 | Loss: 0.00002137
Iteration 154/1000 | Loss: 0.00002137
Iteration 155/1000 | Loss: 0.00002137
Iteration 156/1000 | Loss: 0.00002137
Iteration 157/1000 | Loss: 0.00002137
Iteration 158/1000 | Loss: 0.00002137
Iteration 159/1000 | Loss: 0.00002136
Iteration 160/1000 | Loss: 0.00002136
Iteration 161/1000 | Loss: 0.00002135
Iteration 162/1000 | Loss: 0.00002135
Iteration 163/1000 | Loss: 0.00002135
Iteration 164/1000 | Loss: 0.00002135
Iteration 165/1000 | Loss: 0.00002134
Iteration 166/1000 | Loss: 0.00002134
Iteration 167/1000 | Loss: 0.00002134
Iteration 168/1000 | Loss: 0.00002134
Iteration 169/1000 | Loss: 0.00002133
Iteration 170/1000 | Loss: 0.00002132
Iteration 171/1000 | Loss: 0.00002132
Iteration 172/1000 | Loss: 0.00002132
Iteration 173/1000 | Loss: 0.00002132
Iteration 174/1000 | Loss: 0.00002132
Iteration 175/1000 | Loss: 0.00002131
Iteration 176/1000 | Loss: 0.00002131
Iteration 177/1000 | Loss: 0.00002131
Iteration 178/1000 | Loss: 0.00002131
Iteration 179/1000 | Loss: 0.00002131
Iteration 180/1000 | Loss: 0.00002131
Iteration 181/1000 | Loss: 0.00002131
Iteration 182/1000 | Loss: 0.00002131
Iteration 183/1000 | Loss: 0.00002131
Iteration 184/1000 | Loss: 0.00002131
Iteration 185/1000 | Loss: 0.00002131
Iteration 186/1000 | Loss: 0.00002130
Iteration 187/1000 | Loss: 0.00002130
Iteration 188/1000 | Loss: 0.00002130
Iteration 189/1000 | Loss: 0.00002130
Iteration 190/1000 | Loss: 0.00002130
Iteration 191/1000 | Loss: 0.00002130
Iteration 192/1000 | Loss: 0.00002129
Iteration 193/1000 | Loss: 0.00002129
Iteration 194/1000 | Loss: 0.00002129
Iteration 195/1000 | Loss: 0.00002129
Iteration 196/1000 | Loss: 0.00002129
Iteration 197/1000 | Loss: 0.00002129
Iteration 198/1000 | Loss: 0.00002129
Iteration 199/1000 | Loss: 0.00002129
Iteration 200/1000 | Loss: 0.00002129
Iteration 201/1000 | Loss: 0.00002129
Iteration 202/1000 | Loss: 0.00002129
Iteration 203/1000 | Loss: 0.00002129
Iteration 204/1000 | Loss: 0.00002129
Iteration 205/1000 | Loss: 0.00002129
Iteration 206/1000 | Loss: 0.00002129
Iteration 207/1000 | Loss: 0.00002129
Iteration 208/1000 | Loss: 0.00002129
Iteration 209/1000 | Loss: 0.00002129
Iteration 210/1000 | Loss: 0.00002129
Iteration 211/1000 | Loss: 0.00002129
Iteration 212/1000 | Loss: 0.00002129
Iteration 213/1000 | Loss: 0.00002129
Iteration 214/1000 | Loss: 0.00002129
Iteration 215/1000 | Loss: 0.00002129
Iteration 216/1000 | Loss: 0.00002129
Iteration 217/1000 | Loss: 0.00002129
Iteration 218/1000 | Loss: 0.00002129
Iteration 219/1000 | Loss: 0.00002129
Iteration 220/1000 | Loss: 0.00002129
Iteration 221/1000 | Loss: 0.00002129
Iteration 222/1000 | Loss: 0.00002129
Iteration 223/1000 | Loss: 0.00002129
Iteration 224/1000 | Loss: 0.00002129
Iteration 225/1000 | Loss: 0.00002129
Iteration 226/1000 | Loss: 0.00002129
Iteration 227/1000 | Loss: 0.00002129
Iteration 228/1000 | Loss: 0.00002129
Iteration 229/1000 | Loss: 0.00002129
Iteration 230/1000 | Loss: 0.00002129
Iteration 231/1000 | Loss: 0.00002129
Iteration 232/1000 | Loss: 0.00002129
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 232. Stopping optimization.
Last 5 losses: [2.129120366589632e-05, 2.129120366589632e-05, 2.129120366589632e-05, 2.129120366589632e-05, 2.129120366589632e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.129120366589632e-05

Optimization complete. Final v2v error: 3.839897394180298 mm

Highest mean error: 5.043939590454102 mm for frame 20

Lowest mean error: 3.502584457397461 mm for frame 12

Saving results

Total time: 141.0765242576599
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_011/1014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_011/1014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_011/1014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00510839
Iteration 2/25 | Loss: 0.00132321
Iteration 3/25 | Loss: 0.00124790
Iteration 4/25 | Loss: 0.00123132
Iteration 5/25 | Loss: 0.00122597
Iteration 6/25 | Loss: 0.00122472
Iteration 7/25 | Loss: 0.00122472
Iteration 8/25 | Loss: 0.00122472
Iteration 9/25 | Loss: 0.00122472
Iteration 10/25 | Loss: 0.00122472
Iteration 11/25 | Loss: 0.00122472
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012247180566191673, 0.0012247180566191673, 0.0012247180566191673, 0.0012247180566191673, 0.0012247180566191673]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012247180566191673

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 6.01810026
Iteration 2/25 | Loss: 0.00104601
Iteration 3/25 | Loss: 0.00104601
Iteration 4/25 | Loss: 0.00104601
Iteration 5/25 | Loss: 0.00104601
Iteration 6/25 | Loss: 0.00104601
Iteration 7/25 | Loss: 0.00104601
Iteration 8/25 | Loss: 0.00104601
Iteration 9/25 | Loss: 0.00104601
Iteration 10/25 | Loss: 0.00104601
Iteration 11/25 | Loss: 0.00104601
Iteration 12/25 | Loss: 0.00104601
Iteration 13/25 | Loss: 0.00104601
Iteration 14/25 | Loss: 0.00104601
Iteration 15/25 | Loss: 0.00104601
Iteration 16/25 | Loss: 0.00104601
Iteration 17/25 | Loss: 0.00104601
Iteration 18/25 | Loss: 0.00104601
Iteration 19/25 | Loss: 0.00104601
Iteration 20/25 | Loss: 0.00104601
Iteration 21/25 | Loss: 0.00104601
Iteration 22/25 | Loss: 0.00104601
Iteration 23/25 | Loss: 0.00104601
Iteration 24/25 | Loss: 0.00104601
Iteration 25/25 | Loss: 0.00104601

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00104601
Iteration 2/1000 | Loss: 0.00004201
Iteration 3/1000 | Loss: 0.00002779
Iteration 4/1000 | Loss: 0.00002308
Iteration 5/1000 | Loss: 0.00002175
Iteration 6/1000 | Loss: 0.00002095
Iteration 7/1000 | Loss: 0.00002044
Iteration 8/1000 | Loss: 0.00002003
Iteration 9/1000 | Loss: 0.00001963
Iteration 10/1000 | Loss: 0.00001949
Iteration 11/1000 | Loss: 0.00001923
Iteration 12/1000 | Loss: 0.00001898
Iteration 13/1000 | Loss: 0.00001891
Iteration 14/1000 | Loss: 0.00001885
Iteration 15/1000 | Loss: 0.00001878
Iteration 16/1000 | Loss: 0.00001876
Iteration 17/1000 | Loss: 0.00001873
Iteration 18/1000 | Loss: 0.00001867
Iteration 19/1000 | Loss: 0.00001848
Iteration 20/1000 | Loss: 0.00001844
Iteration 21/1000 | Loss: 0.00001843
Iteration 22/1000 | Loss: 0.00001841
Iteration 23/1000 | Loss: 0.00001840
Iteration 24/1000 | Loss: 0.00001834
Iteration 25/1000 | Loss: 0.00001833
Iteration 26/1000 | Loss: 0.00001831
Iteration 27/1000 | Loss: 0.00001829
Iteration 28/1000 | Loss: 0.00001828
Iteration 29/1000 | Loss: 0.00001828
Iteration 30/1000 | Loss: 0.00001825
Iteration 31/1000 | Loss: 0.00001824
Iteration 32/1000 | Loss: 0.00001824
Iteration 33/1000 | Loss: 0.00001824
Iteration 34/1000 | Loss: 0.00001823
Iteration 35/1000 | Loss: 0.00001823
Iteration 36/1000 | Loss: 0.00001822
Iteration 37/1000 | Loss: 0.00001821
Iteration 38/1000 | Loss: 0.00001821
Iteration 39/1000 | Loss: 0.00001820
Iteration 40/1000 | Loss: 0.00001820
Iteration 41/1000 | Loss: 0.00001820
Iteration 42/1000 | Loss: 0.00001819
Iteration 43/1000 | Loss: 0.00001819
Iteration 44/1000 | Loss: 0.00001819
Iteration 45/1000 | Loss: 0.00001818
Iteration 46/1000 | Loss: 0.00001818
Iteration 47/1000 | Loss: 0.00001818
Iteration 48/1000 | Loss: 0.00001818
Iteration 49/1000 | Loss: 0.00001817
Iteration 50/1000 | Loss: 0.00001817
Iteration 51/1000 | Loss: 0.00001817
Iteration 52/1000 | Loss: 0.00001816
Iteration 53/1000 | Loss: 0.00001816
Iteration 54/1000 | Loss: 0.00001815
Iteration 55/1000 | Loss: 0.00001815
Iteration 56/1000 | Loss: 0.00001815
Iteration 57/1000 | Loss: 0.00001815
Iteration 58/1000 | Loss: 0.00001814
Iteration 59/1000 | Loss: 0.00001814
Iteration 60/1000 | Loss: 0.00001814
Iteration 61/1000 | Loss: 0.00001814
Iteration 62/1000 | Loss: 0.00001813
Iteration 63/1000 | Loss: 0.00001813
Iteration 64/1000 | Loss: 0.00001813
Iteration 65/1000 | Loss: 0.00001812
Iteration 66/1000 | Loss: 0.00001812
Iteration 67/1000 | Loss: 0.00001811
Iteration 68/1000 | Loss: 0.00001811
Iteration 69/1000 | Loss: 0.00001811
Iteration 70/1000 | Loss: 0.00001811
Iteration 71/1000 | Loss: 0.00001810
Iteration 72/1000 | Loss: 0.00001810
Iteration 73/1000 | Loss: 0.00001810
Iteration 74/1000 | Loss: 0.00001810
Iteration 75/1000 | Loss: 0.00001809
Iteration 76/1000 | Loss: 0.00001808
Iteration 77/1000 | Loss: 0.00001808
Iteration 78/1000 | Loss: 0.00001808
Iteration 79/1000 | Loss: 0.00001808
Iteration 80/1000 | Loss: 0.00001808
Iteration 81/1000 | Loss: 0.00001808
Iteration 82/1000 | Loss: 0.00001808
Iteration 83/1000 | Loss: 0.00001808
Iteration 84/1000 | Loss: 0.00001808
Iteration 85/1000 | Loss: 0.00001807
Iteration 86/1000 | Loss: 0.00001807
Iteration 87/1000 | Loss: 0.00001806
Iteration 88/1000 | Loss: 0.00001806
Iteration 89/1000 | Loss: 0.00001806
Iteration 90/1000 | Loss: 0.00001805
Iteration 91/1000 | Loss: 0.00001805
Iteration 92/1000 | Loss: 0.00001805
Iteration 93/1000 | Loss: 0.00001805
Iteration 94/1000 | Loss: 0.00001804
Iteration 95/1000 | Loss: 0.00001804
Iteration 96/1000 | Loss: 0.00001804
Iteration 97/1000 | Loss: 0.00001804
Iteration 98/1000 | Loss: 0.00001804
Iteration 99/1000 | Loss: 0.00001804
Iteration 100/1000 | Loss: 0.00001804
Iteration 101/1000 | Loss: 0.00001804
Iteration 102/1000 | Loss: 0.00001804
Iteration 103/1000 | Loss: 0.00001804
Iteration 104/1000 | Loss: 0.00001803
Iteration 105/1000 | Loss: 0.00001803
Iteration 106/1000 | Loss: 0.00001803
Iteration 107/1000 | Loss: 0.00001803
Iteration 108/1000 | Loss: 0.00001803
Iteration 109/1000 | Loss: 0.00001802
Iteration 110/1000 | Loss: 0.00001802
Iteration 111/1000 | Loss: 0.00001802
Iteration 112/1000 | Loss: 0.00001801
Iteration 113/1000 | Loss: 0.00001801
Iteration 114/1000 | Loss: 0.00001801
Iteration 115/1000 | Loss: 0.00001801
Iteration 116/1000 | Loss: 0.00001801
Iteration 117/1000 | Loss: 0.00001801
Iteration 118/1000 | Loss: 0.00001801
Iteration 119/1000 | Loss: 0.00001801
Iteration 120/1000 | Loss: 0.00001800
Iteration 121/1000 | Loss: 0.00001800
Iteration 122/1000 | Loss: 0.00001800
Iteration 123/1000 | Loss: 0.00001800
Iteration 124/1000 | Loss: 0.00001800
Iteration 125/1000 | Loss: 0.00001800
Iteration 126/1000 | Loss: 0.00001799
Iteration 127/1000 | Loss: 0.00001799
Iteration 128/1000 | Loss: 0.00001799
Iteration 129/1000 | Loss: 0.00001798
Iteration 130/1000 | Loss: 0.00001798
Iteration 131/1000 | Loss: 0.00001798
Iteration 132/1000 | Loss: 0.00001798
Iteration 133/1000 | Loss: 0.00001797
Iteration 134/1000 | Loss: 0.00001797
Iteration 135/1000 | Loss: 0.00001797
Iteration 136/1000 | Loss: 0.00001796
Iteration 137/1000 | Loss: 0.00001796
Iteration 138/1000 | Loss: 0.00001796
Iteration 139/1000 | Loss: 0.00001796
Iteration 140/1000 | Loss: 0.00001795
Iteration 141/1000 | Loss: 0.00001795
Iteration 142/1000 | Loss: 0.00001795
Iteration 143/1000 | Loss: 0.00001795
Iteration 144/1000 | Loss: 0.00001795
Iteration 145/1000 | Loss: 0.00001794
Iteration 146/1000 | Loss: 0.00001794
Iteration 147/1000 | Loss: 0.00001794
Iteration 148/1000 | Loss: 0.00001794
Iteration 149/1000 | Loss: 0.00001794
Iteration 150/1000 | Loss: 0.00001794
Iteration 151/1000 | Loss: 0.00001793
Iteration 152/1000 | Loss: 0.00001793
Iteration 153/1000 | Loss: 0.00001793
Iteration 154/1000 | Loss: 0.00001793
Iteration 155/1000 | Loss: 0.00001793
Iteration 156/1000 | Loss: 0.00001792
Iteration 157/1000 | Loss: 0.00001792
Iteration 158/1000 | Loss: 0.00001792
Iteration 159/1000 | Loss: 0.00001792
Iteration 160/1000 | Loss: 0.00001792
Iteration 161/1000 | Loss: 0.00001792
Iteration 162/1000 | Loss: 0.00001792
Iteration 163/1000 | Loss: 0.00001792
Iteration 164/1000 | Loss: 0.00001792
Iteration 165/1000 | Loss: 0.00001792
Iteration 166/1000 | Loss: 0.00001792
Iteration 167/1000 | Loss: 0.00001792
Iteration 168/1000 | Loss: 0.00001792
Iteration 169/1000 | Loss: 0.00001792
Iteration 170/1000 | Loss: 0.00001792
Iteration 171/1000 | Loss: 0.00001792
Iteration 172/1000 | Loss: 0.00001792
Iteration 173/1000 | Loss: 0.00001791
Iteration 174/1000 | Loss: 0.00001791
Iteration 175/1000 | Loss: 0.00001791
Iteration 176/1000 | Loss: 0.00001791
Iteration 177/1000 | Loss: 0.00001791
Iteration 178/1000 | Loss: 0.00001791
Iteration 179/1000 | Loss: 0.00001791
Iteration 180/1000 | Loss: 0.00001791
Iteration 181/1000 | Loss: 0.00001791
Iteration 182/1000 | Loss: 0.00001791
Iteration 183/1000 | Loss: 0.00001791
Iteration 184/1000 | Loss: 0.00001791
Iteration 185/1000 | Loss: 0.00001791
Iteration 186/1000 | Loss: 0.00001791
Iteration 187/1000 | Loss: 0.00001791
Iteration 188/1000 | Loss: 0.00001791
Iteration 189/1000 | Loss: 0.00001791
Iteration 190/1000 | Loss: 0.00001791
Iteration 191/1000 | Loss: 0.00001791
Iteration 192/1000 | Loss: 0.00001790
Iteration 193/1000 | Loss: 0.00001790
Iteration 194/1000 | Loss: 0.00001790
Iteration 195/1000 | Loss: 0.00001790
Iteration 196/1000 | Loss: 0.00001790
Iteration 197/1000 | Loss: 0.00001790
Iteration 198/1000 | Loss: 0.00001790
Iteration 199/1000 | Loss: 0.00001790
Iteration 200/1000 | Loss: 0.00001790
Iteration 201/1000 | Loss: 0.00001790
Iteration 202/1000 | Loss: 0.00001790
Iteration 203/1000 | Loss: 0.00001790
Iteration 204/1000 | Loss: 0.00001790
Iteration 205/1000 | Loss: 0.00001790
Iteration 206/1000 | Loss: 0.00001790
Iteration 207/1000 | Loss: 0.00001790
Iteration 208/1000 | Loss: 0.00001790
Iteration 209/1000 | Loss: 0.00001790
Iteration 210/1000 | Loss: 0.00001789
Iteration 211/1000 | Loss: 0.00001789
Iteration 212/1000 | Loss: 0.00001789
Iteration 213/1000 | Loss: 0.00001789
Iteration 214/1000 | Loss: 0.00001789
Iteration 215/1000 | Loss: 0.00001789
Iteration 216/1000 | Loss: 0.00001789
Iteration 217/1000 | Loss: 0.00001789
Iteration 218/1000 | Loss: 0.00001789
Iteration 219/1000 | Loss: 0.00001789
Iteration 220/1000 | Loss: 0.00001789
Iteration 221/1000 | Loss: 0.00001789
Iteration 222/1000 | Loss: 0.00001789
Iteration 223/1000 | Loss: 0.00001789
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 223. Stopping optimization.
Last 5 losses: [1.788997178664431e-05, 1.788997178664431e-05, 1.788997178664431e-05, 1.788997178664431e-05, 1.788997178664431e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.788997178664431e-05

Optimization complete. Final v2v error: 3.5491745471954346 mm

Highest mean error: 4.667250633239746 mm for frame 67

Lowest mean error: 3.0077054500579834 mm for frame 18

Saving results

Total time: 44.74520421028137
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_011/1084/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_011/1084.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_011/1084
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01028440
Iteration 2/25 | Loss: 0.00238624
Iteration 3/25 | Loss: 0.00189541
Iteration 4/25 | Loss: 0.00175534
Iteration 5/25 | Loss: 0.00148577
Iteration 6/25 | Loss: 0.00144240
Iteration 7/25 | Loss: 0.00143477
Iteration 8/25 | Loss: 0.00142134
Iteration 9/25 | Loss: 0.00140466
Iteration 10/25 | Loss: 0.00138895
Iteration 11/25 | Loss: 0.00139659
Iteration 12/25 | Loss: 0.00138614
Iteration 13/25 | Loss: 0.00141631
Iteration 14/25 | Loss: 0.00142347
Iteration 15/25 | Loss: 0.00136566
Iteration 16/25 | Loss: 0.00136090
Iteration 17/25 | Loss: 0.00135629
Iteration 18/25 | Loss: 0.00134658
Iteration 19/25 | Loss: 0.00134241
Iteration 20/25 | Loss: 0.00134078
Iteration 21/25 | Loss: 0.00134026
Iteration 22/25 | Loss: 0.00134008
Iteration 23/25 | Loss: 0.00134006
Iteration 24/25 | Loss: 0.00134006
Iteration 25/25 | Loss: 0.00134006

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36473513
Iteration 2/25 | Loss: 0.00275461
Iteration 3/25 | Loss: 0.00217299
Iteration 4/25 | Loss: 0.00217298
Iteration 5/25 | Loss: 0.00217298
Iteration 6/25 | Loss: 0.00217298
Iteration 7/25 | Loss: 0.00217298
Iteration 8/25 | Loss: 0.00217298
Iteration 9/25 | Loss: 0.00217298
Iteration 10/25 | Loss: 0.00217298
Iteration 11/25 | Loss: 0.00217298
Iteration 12/25 | Loss: 0.00217298
Iteration 13/25 | Loss: 0.00217298
Iteration 14/25 | Loss: 0.00217298
Iteration 15/25 | Loss: 0.00217298
Iteration 16/25 | Loss: 0.00217298
Iteration 17/25 | Loss: 0.00217298
Iteration 18/25 | Loss: 0.00217298
Iteration 19/25 | Loss: 0.00217298
Iteration 20/25 | Loss: 0.00217298
Iteration 21/25 | Loss: 0.00217298
Iteration 22/25 | Loss: 0.00217298
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0021729825530201197, 0.0021729825530201197, 0.0021729825530201197, 0.0021729825530201197, 0.0021729825530201197]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0021729825530201197

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00217298
Iteration 2/1000 | Loss: 0.00019028
Iteration 3/1000 | Loss: 0.00100808
Iteration 4/1000 | Loss: 0.00033399
Iteration 5/1000 | Loss: 0.00011155
Iteration 6/1000 | Loss: 0.00068170
Iteration 7/1000 | Loss: 0.00051322
Iteration 8/1000 | Loss: 0.00065886
Iteration 9/1000 | Loss: 0.00043201
Iteration 10/1000 | Loss: 0.00061872
Iteration 11/1000 | Loss: 0.00014332
Iteration 12/1000 | Loss: 0.00026648
Iteration 13/1000 | Loss: 0.00009822
Iteration 14/1000 | Loss: 0.00020078
Iteration 15/1000 | Loss: 0.00032325
Iteration 16/1000 | Loss: 0.00033375
Iteration 17/1000 | Loss: 0.00040692
Iteration 18/1000 | Loss: 0.00072290
Iteration 19/1000 | Loss: 0.00043113
Iteration 20/1000 | Loss: 0.00032398
Iteration 21/1000 | Loss: 0.00010884
Iteration 22/1000 | Loss: 0.00026275
Iteration 23/1000 | Loss: 0.00033615
Iteration 24/1000 | Loss: 0.00022256
Iteration 25/1000 | Loss: 0.00040215
Iteration 26/1000 | Loss: 0.00020629
Iteration 27/1000 | Loss: 0.00019655
Iteration 28/1000 | Loss: 0.00014874
Iteration 29/1000 | Loss: 0.00015618
Iteration 30/1000 | Loss: 0.00013089
Iteration 31/1000 | Loss: 0.00022910
Iteration 32/1000 | Loss: 0.00016679
Iteration 33/1000 | Loss: 0.00014624
Iteration 34/1000 | Loss: 0.00019424
Iteration 35/1000 | Loss: 0.00017755
Iteration 36/1000 | Loss: 0.00056043
Iteration 37/1000 | Loss: 0.00053977
Iteration 38/1000 | Loss: 0.00057248
Iteration 39/1000 | Loss: 0.00034143
Iteration 40/1000 | Loss: 0.00030680
Iteration 41/1000 | Loss: 0.00009178
Iteration 42/1000 | Loss: 0.00014369
Iteration 43/1000 | Loss: 0.00010608
Iteration 44/1000 | Loss: 0.00011641
Iteration 45/1000 | Loss: 0.00022372
Iteration 46/1000 | Loss: 0.00021576
Iteration 47/1000 | Loss: 0.00023118
Iteration 48/1000 | Loss: 0.00017172
Iteration 49/1000 | Loss: 0.00008950
Iteration 50/1000 | Loss: 0.00010387
Iteration 51/1000 | Loss: 0.00046617
Iteration 52/1000 | Loss: 0.00076311
Iteration 53/1000 | Loss: 0.00096826
Iteration 54/1000 | Loss: 0.00084866
Iteration 55/1000 | Loss: 0.00079534
Iteration 56/1000 | Loss: 0.00076090
Iteration 57/1000 | Loss: 0.00068302
Iteration 58/1000 | Loss: 0.00077391
Iteration 59/1000 | Loss: 0.00056345
Iteration 60/1000 | Loss: 0.00049555
Iteration 61/1000 | Loss: 0.00036182
Iteration 62/1000 | Loss: 0.00031697
Iteration 63/1000 | Loss: 0.00032219
Iteration 64/1000 | Loss: 0.00026895
Iteration 65/1000 | Loss: 0.00028249
Iteration 66/1000 | Loss: 0.00035118
Iteration 67/1000 | Loss: 0.00021286
Iteration 68/1000 | Loss: 0.00017925
Iteration 69/1000 | Loss: 0.00025713
Iteration 70/1000 | Loss: 0.00028148
Iteration 71/1000 | Loss: 0.00029171
Iteration 72/1000 | Loss: 0.00038247
Iteration 73/1000 | Loss: 0.00019153
Iteration 74/1000 | Loss: 0.00024788
Iteration 75/1000 | Loss: 0.00037649
Iteration 76/1000 | Loss: 0.00021751
Iteration 77/1000 | Loss: 0.00034799
Iteration 78/1000 | Loss: 0.00024939
Iteration 79/1000 | Loss: 0.00017893
Iteration 80/1000 | Loss: 0.00021246
Iteration 81/1000 | Loss: 0.00030566
Iteration 82/1000 | Loss: 0.00030410
Iteration 83/1000 | Loss: 0.00024134
Iteration 84/1000 | Loss: 0.00031626
Iteration 85/1000 | Loss: 0.00016786
Iteration 86/1000 | Loss: 0.00043464
Iteration 87/1000 | Loss: 0.00032586
Iteration 88/1000 | Loss: 0.00025485
Iteration 89/1000 | Loss: 0.00020902
Iteration 90/1000 | Loss: 0.00025393
Iteration 91/1000 | Loss: 0.00029831
Iteration 92/1000 | Loss: 0.00033124
Iteration 93/1000 | Loss: 0.00035371
Iteration 94/1000 | Loss: 0.00023793
Iteration 95/1000 | Loss: 0.00031198
Iteration 96/1000 | Loss: 0.00021252
Iteration 97/1000 | Loss: 0.00029159
Iteration 98/1000 | Loss: 0.00017819
Iteration 99/1000 | Loss: 0.00026206
Iteration 100/1000 | Loss: 0.00051232
Iteration 101/1000 | Loss: 0.00059147
Iteration 102/1000 | Loss: 0.00033653
Iteration 103/1000 | Loss: 0.00035164
Iteration 104/1000 | Loss: 0.00027112
Iteration 105/1000 | Loss: 0.00026711
Iteration 106/1000 | Loss: 0.00018426
Iteration 107/1000 | Loss: 0.00020317
Iteration 108/1000 | Loss: 0.00021969
Iteration 109/1000 | Loss: 0.00014241
Iteration 110/1000 | Loss: 0.00017495
Iteration 111/1000 | Loss: 0.00019153
Iteration 112/1000 | Loss: 0.00022101
Iteration 113/1000 | Loss: 0.00020408
Iteration 114/1000 | Loss: 0.00021097
Iteration 115/1000 | Loss: 0.00019556
Iteration 116/1000 | Loss: 0.00014537
Iteration 117/1000 | Loss: 0.00016605
Iteration 118/1000 | Loss: 0.00014842
Iteration 119/1000 | Loss: 0.00016820
Iteration 120/1000 | Loss: 0.00015874
Iteration 121/1000 | Loss: 0.00012462
Iteration 122/1000 | Loss: 0.00015714
Iteration 123/1000 | Loss: 0.00016959
Iteration 124/1000 | Loss: 0.00007606
Iteration 125/1000 | Loss: 0.00012080
Iteration 126/1000 | Loss: 0.00007032
Iteration 127/1000 | Loss: 0.00011796
Iteration 128/1000 | Loss: 0.00015928
Iteration 129/1000 | Loss: 0.00016359
Iteration 130/1000 | Loss: 0.00043305
Iteration 131/1000 | Loss: 0.00027855
Iteration 132/1000 | Loss: 0.00037106
Iteration 133/1000 | Loss: 0.00010608
Iteration 134/1000 | Loss: 0.00018976
Iteration 135/1000 | Loss: 0.00019349
Iteration 136/1000 | Loss: 0.00021318
Iteration 137/1000 | Loss: 0.00013808
Iteration 138/1000 | Loss: 0.00021165
Iteration 139/1000 | Loss: 0.00028262
Iteration 140/1000 | Loss: 0.00008764
Iteration 141/1000 | Loss: 0.00013345
Iteration 142/1000 | Loss: 0.00012054
Iteration 143/1000 | Loss: 0.00011433
Iteration 144/1000 | Loss: 0.00018010
Iteration 145/1000 | Loss: 0.00022720
Iteration 146/1000 | Loss: 0.00008730
Iteration 147/1000 | Loss: 0.00035600
Iteration 148/1000 | Loss: 0.00027628
Iteration 149/1000 | Loss: 0.00027974
Iteration 150/1000 | Loss: 0.00040273
Iteration 151/1000 | Loss: 0.00018290
Iteration 152/1000 | Loss: 0.00037704
Iteration 153/1000 | Loss: 0.00006336
Iteration 154/1000 | Loss: 0.00039544
Iteration 155/1000 | Loss: 0.00035870
Iteration 156/1000 | Loss: 0.00054492
Iteration 157/1000 | Loss: 0.00036122
Iteration 158/1000 | Loss: 0.00052274
Iteration 159/1000 | Loss: 0.00011031
Iteration 160/1000 | Loss: 0.00008841
Iteration 161/1000 | Loss: 0.00005591
Iteration 162/1000 | Loss: 0.00011474
Iteration 163/1000 | Loss: 0.00004964
Iteration 164/1000 | Loss: 0.00039045
Iteration 165/1000 | Loss: 0.00020072
Iteration 166/1000 | Loss: 0.00036033
Iteration 167/1000 | Loss: 0.00004695
Iteration 168/1000 | Loss: 0.00039898
Iteration 169/1000 | Loss: 0.00055574
Iteration 170/1000 | Loss: 0.00122543
Iteration 171/1000 | Loss: 0.00058786
Iteration 172/1000 | Loss: 0.00081909
Iteration 173/1000 | Loss: 0.00087800
Iteration 174/1000 | Loss: 0.00102885
Iteration 175/1000 | Loss: 0.00026227
Iteration 176/1000 | Loss: 0.00005948
Iteration 177/1000 | Loss: 0.00004770
Iteration 178/1000 | Loss: 0.00017436
Iteration 179/1000 | Loss: 0.00064598
Iteration 180/1000 | Loss: 0.00032507
Iteration 181/1000 | Loss: 0.00021379
Iteration 182/1000 | Loss: 0.00031410
Iteration 183/1000 | Loss: 0.00021977
Iteration 184/1000 | Loss: 0.00004643
Iteration 185/1000 | Loss: 0.00003550
Iteration 186/1000 | Loss: 0.00003022
Iteration 187/1000 | Loss: 0.00002689
Iteration 188/1000 | Loss: 0.00002482
Iteration 189/1000 | Loss: 0.00002310
Iteration 190/1000 | Loss: 0.00002245
Iteration 191/1000 | Loss: 0.00013880
Iteration 192/1000 | Loss: 0.00003191
Iteration 193/1000 | Loss: 0.00010582
Iteration 194/1000 | Loss: 0.00016817
Iteration 195/1000 | Loss: 0.00062684
Iteration 196/1000 | Loss: 0.00027602
Iteration 197/1000 | Loss: 0.00031988
Iteration 198/1000 | Loss: 0.00032097
Iteration 199/1000 | Loss: 0.00050018
Iteration 200/1000 | Loss: 0.00022185
Iteration 201/1000 | Loss: 0.00041418
Iteration 202/1000 | Loss: 0.00103343
Iteration 203/1000 | Loss: 0.00006877
Iteration 204/1000 | Loss: 0.00019916
Iteration 205/1000 | Loss: 0.00021720
Iteration 206/1000 | Loss: 0.00005218
Iteration 207/1000 | Loss: 0.00003177
Iteration 208/1000 | Loss: 0.00002721
Iteration 209/1000 | Loss: 0.00002480
Iteration 210/1000 | Loss: 0.00002289
Iteration 211/1000 | Loss: 0.00002154
Iteration 212/1000 | Loss: 0.00002060
Iteration 213/1000 | Loss: 0.00001989
Iteration 214/1000 | Loss: 0.00005464
Iteration 215/1000 | Loss: 0.00003265
Iteration 216/1000 | Loss: 0.00002562
Iteration 217/1000 | Loss: 0.00002256
Iteration 218/1000 | Loss: 0.00002038
Iteration 219/1000 | Loss: 0.00001882
Iteration 220/1000 | Loss: 0.00001777
Iteration 221/1000 | Loss: 0.00001725
Iteration 222/1000 | Loss: 0.00001703
Iteration 223/1000 | Loss: 0.00001702
Iteration 224/1000 | Loss: 0.00001699
Iteration 225/1000 | Loss: 0.00001690
Iteration 226/1000 | Loss: 0.00035449
Iteration 227/1000 | Loss: 0.00086063
Iteration 228/1000 | Loss: 0.00035433
Iteration 229/1000 | Loss: 0.00002901
Iteration 230/1000 | Loss: 0.00002088
Iteration 231/1000 | Loss: 0.00001857
Iteration 232/1000 | Loss: 0.00001664
Iteration 233/1000 | Loss: 0.00001569
Iteration 234/1000 | Loss: 0.00001489
Iteration 235/1000 | Loss: 0.00001449
Iteration 236/1000 | Loss: 0.00001426
Iteration 237/1000 | Loss: 0.00001408
Iteration 238/1000 | Loss: 0.00001403
Iteration 239/1000 | Loss: 0.00001403
Iteration 240/1000 | Loss: 0.00001400
Iteration 241/1000 | Loss: 0.00001395
Iteration 242/1000 | Loss: 0.00001395
Iteration 243/1000 | Loss: 0.00001395
Iteration 244/1000 | Loss: 0.00001394
Iteration 245/1000 | Loss: 0.00001393
Iteration 246/1000 | Loss: 0.00001393
Iteration 247/1000 | Loss: 0.00001392
Iteration 248/1000 | Loss: 0.00001392
Iteration 249/1000 | Loss: 0.00001392
Iteration 250/1000 | Loss: 0.00001391
Iteration 251/1000 | Loss: 0.00001391
Iteration 252/1000 | Loss: 0.00001390
Iteration 253/1000 | Loss: 0.00001390
Iteration 254/1000 | Loss: 0.00001390
Iteration 255/1000 | Loss: 0.00001390
Iteration 256/1000 | Loss: 0.00001390
Iteration 257/1000 | Loss: 0.00001390
Iteration 258/1000 | Loss: 0.00001390
Iteration 259/1000 | Loss: 0.00001390
Iteration 260/1000 | Loss: 0.00001389
Iteration 261/1000 | Loss: 0.00001389
Iteration 262/1000 | Loss: 0.00001389
Iteration 263/1000 | Loss: 0.00001389
Iteration 264/1000 | Loss: 0.00001389
Iteration 265/1000 | Loss: 0.00001389
Iteration 266/1000 | Loss: 0.00001389
Iteration 267/1000 | Loss: 0.00001388
Iteration 268/1000 | Loss: 0.00001388
Iteration 269/1000 | Loss: 0.00001388
Iteration 270/1000 | Loss: 0.00001388
Iteration 271/1000 | Loss: 0.00001387
Iteration 272/1000 | Loss: 0.00001387
Iteration 273/1000 | Loss: 0.00001387
Iteration 274/1000 | Loss: 0.00001387
Iteration 275/1000 | Loss: 0.00001386
Iteration 276/1000 | Loss: 0.00001386
Iteration 277/1000 | Loss: 0.00001386
Iteration 278/1000 | Loss: 0.00001386
Iteration 279/1000 | Loss: 0.00001386
Iteration 280/1000 | Loss: 0.00001386
Iteration 281/1000 | Loss: 0.00001386
Iteration 282/1000 | Loss: 0.00001386
Iteration 283/1000 | Loss: 0.00001385
Iteration 284/1000 | Loss: 0.00001385
Iteration 285/1000 | Loss: 0.00001385
Iteration 286/1000 | Loss: 0.00001385
Iteration 287/1000 | Loss: 0.00001385
Iteration 288/1000 | Loss: 0.00001385
Iteration 289/1000 | Loss: 0.00001385
Iteration 290/1000 | Loss: 0.00001385
Iteration 291/1000 | Loss: 0.00001385
Iteration 292/1000 | Loss: 0.00001384
Iteration 293/1000 | Loss: 0.00001384
Iteration 294/1000 | Loss: 0.00001384
Iteration 295/1000 | Loss: 0.00001384
Iteration 296/1000 | Loss: 0.00001384
Iteration 297/1000 | Loss: 0.00001384
Iteration 298/1000 | Loss: 0.00001384
Iteration 299/1000 | Loss: 0.00001384
Iteration 300/1000 | Loss: 0.00001384
Iteration 301/1000 | Loss: 0.00001383
Iteration 302/1000 | Loss: 0.00001383
Iteration 303/1000 | Loss: 0.00001383
Iteration 304/1000 | Loss: 0.00001383
Iteration 305/1000 | Loss: 0.00001382
Iteration 306/1000 | Loss: 0.00001382
Iteration 307/1000 | Loss: 0.00001382
Iteration 308/1000 | Loss: 0.00001382
Iteration 309/1000 | Loss: 0.00001381
Iteration 310/1000 | Loss: 0.00001381
Iteration 311/1000 | Loss: 0.00001381
Iteration 312/1000 | Loss: 0.00001381
Iteration 313/1000 | Loss: 0.00001381
Iteration 314/1000 | Loss: 0.00001380
Iteration 315/1000 | Loss: 0.00001380
Iteration 316/1000 | Loss: 0.00001380
Iteration 317/1000 | Loss: 0.00001380
Iteration 318/1000 | Loss: 0.00001380
Iteration 319/1000 | Loss: 0.00001380
Iteration 320/1000 | Loss: 0.00001380
Iteration 321/1000 | Loss: 0.00001380
Iteration 322/1000 | Loss: 0.00001380
Iteration 323/1000 | Loss: 0.00001380
Iteration 324/1000 | Loss: 0.00001380
Iteration 325/1000 | Loss: 0.00001380
Iteration 326/1000 | Loss: 0.00001379
Iteration 327/1000 | Loss: 0.00001379
Iteration 328/1000 | Loss: 0.00001379
Iteration 329/1000 | Loss: 0.00001379
Iteration 330/1000 | Loss: 0.00001379
Iteration 331/1000 | Loss: 0.00001379
Iteration 332/1000 | Loss: 0.00001379
Iteration 333/1000 | Loss: 0.00001379
Iteration 334/1000 | Loss: 0.00001378
Iteration 335/1000 | Loss: 0.00001378
Iteration 336/1000 | Loss: 0.00001378
Iteration 337/1000 | Loss: 0.00001378
Iteration 338/1000 | Loss: 0.00001378
Iteration 339/1000 | Loss: 0.00001378
Iteration 340/1000 | Loss: 0.00001378
Iteration 341/1000 | Loss: 0.00001378
Iteration 342/1000 | Loss: 0.00001378
Iteration 343/1000 | Loss: 0.00001378
Iteration 344/1000 | Loss: 0.00001378
Iteration 345/1000 | Loss: 0.00001378
Iteration 346/1000 | Loss: 0.00001378
Iteration 347/1000 | Loss: 0.00001378
Iteration 348/1000 | Loss: 0.00001378
Iteration 349/1000 | Loss: 0.00001378
Iteration 350/1000 | Loss: 0.00001378
Iteration 351/1000 | Loss: 0.00001378
Iteration 352/1000 | Loss: 0.00001378
Iteration 353/1000 | Loss: 0.00001378
Iteration 354/1000 | Loss: 0.00001378
Iteration 355/1000 | Loss: 0.00001377
Iteration 356/1000 | Loss: 0.00001377
Iteration 357/1000 | Loss: 0.00001377
Iteration 358/1000 | Loss: 0.00001377
Iteration 359/1000 | Loss: 0.00001377
Iteration 360/1000 | Loss: 0.00001377
Iteration 361/1000 | Loss: 0.00001377
Iteration 362/1000 | Loss: 0.00001377
Iteration 363/1000 | Loss: 0.00001377
Iteration 364/1000 | Loss: 0.00001377
Iteration 365/1000 | Loss: 0.00001377
Iteration 366/1000 | Loss: 0.00001377
Iteration 367/1000 | Loss: 0.00001377
Iteration 368/1000 | Loss: 0.00001377
Iteration 369/1000 | Loss: 0.00001377
Iteration 370/1000 | Loss: 0.00001377
Iteration 371/1000 | Loss: 0.00001377
Iteration 372/1000 | Loss: 0.00001377
Iteration 373/1000 | Loss: 0.00001377
Iteration 374/1000 | Loss: 0.00001377
Iteration 375/1000 | Loss: 0.00001377
Iteration 376/1000 | Loss: 0.00001377
Iteration 377/1000 | Loss: 0.00001377
Iteration 378/1000 | Loss: 0.00001377
Iteration 379/1000 | Loss: 0.00001377
Iteration 380/1000 | Loss: 0.00001377
Iteration 381/1000 | Loss: 0.00001377
Iteration 382/1000 | Loss: 0.00001377
Iteration 383/1000 | Loss: 0.00001377
Iteration 384/1000 | Loss: 0.00001377
Iteration 385/1000 | Loss: 0.00001377
Iteration 386/1000 | Loss: 0.00001377
Iteration 387/1000 | Loss: 0.00001377
Iteration 388/1000 | Loss: 0.00001377
Iteration 389/1000 | Loss: 0.00001377
Iteration 390/1000 | Loss: 0.00001377
Iteration 391/1000 | Loss: 0.00001377
Iteration 392/1000 | Loss: 0.00001377
Iteration 393/1000 | Loss: 0.00001377
Iteration 394/1000 | Loss: 0.00001377
Iteration 395/1000 | Loss: 0.00001377
Iteration 396/1000 | Loss: 0.00001377
Iteration 397/1000 | Loss: 0.00001377
Iteration 398/1000 | Loss: 0.00001377
Iteration 399/1000 | Loss: 0.00001377
Iteration 400/1000 | Loss: 0.00001377
Iteration 401/1000 | Loss: 0.00001377
Iteration 402/1000 | Loss: 0.00001377
Iteration 403/1000 | Loss: 0.00001377
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 403. Stopping optimization.
Last 5 losses: [1.3774895705864765e-05, 1.3774895705864765e-05, 1.3774895705864765e-05, 1.3774895705864765e-05, 1.3774895705864765e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3774895705864765e-05

Optimization complete. Final v2v error: 3.183082103729248 mm

Highest mean error: 3.9055745601654053 mm for frame 105

Lowest mean error: 2.829023599624634 mm for frame 103

Saving results

Total time: 367.5153605937958
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_011/1048/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_011/1048.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_011/1048
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00835328
Iteration 2/25 | Loss: 0.00129291
Iteration 3/25 | Loss: 0.00120826
Iteration 4/25 | Loss: 0.00119900
Iteration 5/25 | Loss: 0.00119675
Iteration 6/25 | Loss: 0.00119675
Iteration 7/25 | Loss: 0.00119675
Iteration 8/25 | Loss: 0.00119675
Iteration 9/25 | Loss: 0.00119675
Iteration 10/25 | Loss: 0.00119675
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0011967532336711884, 0.0011967532336711884, 0.0011967532336711884, 0.0011967532336711884, 0.0011967532336711884]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011967532336711884

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 6.78941774
Iteration 2/25 | Loss: 0.00100390
Iteration 3/25 | Loss: 0.00100390
Iteration 4/25 | Loss: 0.00100389
Iteration 5/25 | Loss: 0.00100389
Iteration 6/25 | Loss: 0.00100389
Iteration 7/25 | Loss: 0.00100389
Iteration 8/25 | Loss: 0.00100389
Iteration 9/25 | Loss: 0.00100389
Iteration 10/25 | Loss: 0.00100389
Iteration 11/25 | Loss: 0.00100389
Iteration 12/25 | Loss: 0.00100389
Iteration 13/25 | Loss: 0.00100389
Iteration 14/25 | Loss: 0.00100389
Iteration 15/25 | Loss: 0.00100389
Iteration 16/25 | Loss: 0.00100389
Iteration 17/25 | Loss: 0.00100389
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0010038913460448384, 0.0010038913460448384, 0.0010038913460448384, 0.0010038913460448384, 0.0010038913460448384]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010038913460448384

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00100389
Iteration 2/1000 | Loss: 0.00002218
Iteration 3/1000 | Loss: 0.00001546
Iteration 4/1000 | Loss: 0.00001338
Iteration 5/1000 | Loss: 0.00001246
Iteration 6/1000 | Loss: 0.00001190
Iteration 7/1000 | Loss: 0.00001155
Iteration 8/1000 | Loss: 0.00001139
Iteration 9/1000 | Loss: 0.00001108
Iteration 10/1000 | Loss: 0.00001081
Iteration 11/1000 | Loss: 0.00001080
Iteration 12/1000 | Loss: 0.00001076
Iteration 13/1000 | Loss: 0.00001074
Iteration 14/1000 | Loss: 0.00001062
Iteration 15/1000 | Loss: 0.00001060
Iteration 16/1000 | Loss: 0.00001052
Iteration 17/1000 | Loss: 0.00001052
Iteration 18/1000 | Loss: 0.00001052
Iteration 19/1000 | Loss: 0.00001052
Iteration 20/1000 | Loss: 0.00001052
Iteration 21/1000 | Loss: 0.00001052
Iteration 22/1000 | Loss: 0.00001052
Iteration 23/1000 | Loss: 0.00001052
Iteration 24/1000 | Loss: 0.00001052
Iteration 25/1000 | Loss: 0.00001051
Iteration 26/1000 | Loss: 0.00001050
Iteration 27/1000 | Loss: 0.00001046
Iteration 28/1000 | Loss: 0.00001045
Iteration 29/1000 | Loss: 0.00001045
Iteration 30/1000 | Loss: 0.00001044
Iteration 31/1000 | Loss: 0.00001044
Iteration 32/1000 | Loss: 0.00001037
Iteration 33/1000 | Loss: 0.00001036
Iteration 34/1000 | Loss: 0.00001035
Iteration 35/1000 | Loss: 0.00001029
Iteration 36/1000 | Loss: 0.00001028
Iteration 37/1000 | Loss: 0.00001027
Iteration 38/1000 | Loss: 0.00001025
Iteration 39/1000 | Loss: 0.00001019
Iteration 40/1000 | Loss: 0.00001017
Iteration 41/1000 | Loss: 0.00001017
Iteration 42/1000 | Loss: 0.00001016
Iteration 43/1000 | Loss: 0.00001015
Iteration 44/1000 | Loss: 0.00001014
Iteration 45/1000 | Loss: 0.00001014
Iteration 46/1000 | Loss: 0.00001012
Iteration 47/1000 | Loss: 0.00001012
Iteration 48/1000 | Loss: 0.00001011
Iteration 49/1000 | Loss: 0.00001010
Iteration 50/1000 | Loss: 0.00001008
Iteration 51/1000 | Loss: 0.00001007
Iteration 52/1000 | Loss: 0.00001006
Iteration 53/1000 | Loss: 0.00001006
Iteration 54/1000 | Loss: 0.00001006
Iteration 55/1000 | Loss: 0.00001006
Iteration 56/1000 | Loss: 0.00001006
Iteration 57/1000 | Loss: 0.00001006
Iteration 58/1000 | Loss: 0.00001006
Iteration 59/1000 | Loss: 0.00001005
Iteration 60/1000 | Loss: 0.00001005
Iteration 61/1000 | Loss: 0.00001003
Iteration 62/1000 | Loss: 0.00001002
Iteration 63/1000 | Loss: 0.00001001
Iteration 64/1000 | Loss: 0.00001001
Iteration 65/1000 | Loss: 0.00001000
Iteration 66/1000 | Loss: 0.00000999
Iteration 67/1000 | Loss: 0.00000999
Iteration 68/1000 | Loss: 0.00000998
Iteration 69/1000 | Loss: 0.00000997
Iteration 70/1000 | Loss: 0.00000996
Iteration 71/1000 | Loss: 0.00000996
Iteration 72/1000 | Loss: 0.00000996
Iteration 73/1000 | Loss: 0.00000995
Iteration 74/1000 | Loss: 0.00000995
Iteration 75/1000 | Loss: 0.00000995
Iteration 76/1000 | Loss: 0.00000991
Iteration 77/1000 | Loss: 0.00000991
Iteration 78/1000 | Loss: 0.00000991
Iteration 79/1000 | Loss: 0.00000991
Iteration 80/1000 | Loss: 0.00000991
Iteration 81/1000 | Loss: 0.00000990
Iteration 82/1000 | Loss: 0.00000990
Iteration 83/1000 | Loss: 0.00000990
Iteration 84/1000 | Loss: 0.00000990
Iteration 85/1000 | Loss: 0.00000990
Iteration 86/1000 | Loss: 0.00000990
Iteration 87/1000 | Loss: 0.00000990
Iteration 88/1000 | Loss: 0.00000989
Iteration 89/1000 | Loss: 0.00000988
Iteration 90/1000 | Loss: 0.00000988
Iteration 91/1000 | Loss: 0.00000988
Iteration 92/1000 | Loss: 0.00000988
Iteration 93/1000 | Loss: 0.00000988
Iteration 94/1000 | Loss: 0.00000988
Iteration 95/1000 | Loss: 0.00000988
Iteration 96/1000 | Loss: 0.00000988
Iteration 97/1000 | Loss: 0.00000987
Iteration 98/1000 | Loss: 0.00000987
Iteration 99/1000 | Loss: 0.00000986
Iteration 100/1000 | Loss: 0.00000985
Iteration 101/1000 | Loss: 0.00000985
Iteration 102/1000 | Loss: 0.00000985
Iteration 103/1000 | Loss: 0.00000985
Iteration 104/1000 | Loss: 0.00000985
Iteration 105/1000 | Loss: 0.00000984
Iteration 106/1000 | Loss: 0.00000984
Iteration 107/1000 | Loss: 0.00000984
Iteration 108/1000 | Loss: 0.00000984
Iteration 109/1000 | Loss: 0.00000983
Iteration 110/1000 | Loss: 0.00000983
Iteration 111/1000 | Loss: 0.00000983
Iteration 112/1000 | Loss: 0.00000983
Iteration 113/1000 | Loss: 0.00000983
Iteration 114/1000 | Loss: 0.00000983
Iteration 115/1000 | Loss: 0.00000983
Iteration 116/1000 | Loss: 0.00000983
Iteration 117/1000 | Loss: 0.00000983
Iteration 118/1000 | Loss: 0.00000982
Iteration 119/1000 | Loss: 0.00000982
Iteration 120/1000 | Loss: 0.00000982
Iteration 121/1000 | Loss: 0.00000982
Iteration 122/1000 | Loss: 0.00000982
Iteration 123/1000 | Loss: 0.00000982
Iteration 124/1000 | Loss: 0.00000982
Iteration 125/1000 | Loss: 0.00000981
Iteration 126/1000 | Loss: 0.00000981
Iteration 127/1000 | Loss: 0.00000981
Iteration 128/1000 | Loss: 0.00000981
Iteration 129/1000 | Loss: 0.00000981
Iteration 130/1000 | Loss: 0.00000981
Iteration 131/1000 | Loss: 0.00000981
Iteration 132/1000 | Loss: 0.00000981
Iteration 133/1000 | Loss: 0.00000981
Iteration 134/1000 | Loss: 0.00000980
Iteration 135/1000 | Loss: 0.00000980
Iteration 136/1000 | Loss: 0.00000980
Iteration 137/1000 | Loss: 0.00000980
Iteration 138/1000 | Loss: 0.00000980
Iteration 139/1000 | Loss: 0.00000980
Iteration 140/1000 | Loss: 0.00000980
Iteration 141/1000 | Loss: 0.00000980
Iteration 142/1000 | Loss: 0.00000979
Iteration 143/1000 | Loss: 0.00000979
Iteration 144/1000 | Loss: 0.00000979
Iteration 145/1000 | Loss: 0.00000979
Iteration 146/1000 | Loss: 0.00000979
Iteration 147/1000 | Loss: 0.00000979
Iteration 148/1000 | Loss: 0.00000978
Iteration 149/1000 | Loss: 0.00000978
Iteration 150/1000 | Loss: 0.00000978
Iteration 151/1000 | Loss: 0.00000978
Iteration 152/1000 | Loss: 0.00000977
Iteration 153/1000 | Loss: 0.00000977
Iteration 154/1000 | Loss: 0.00000977
Iteration 155/1000 | Loss: 0.00000977
Iteration 156/1000 | Loss: 0.00000977
Iteration 157/1000 | Loss: 0.00000977
Iteration 158/1000 | Loss: 0.00000977
Iteration 159/1000 | Loss: 0.00000977
Iteration 160/1000 | Loss: 0.00000977
Iteration 161/1000 | Loss: 0.00000977
Iteration 162/1000 | Loss: 0.00000977
Iteration 163/1000 | Loss: 0.00000977
Iteration 164/1000 | Loss: 0.00000977
Iteration 165/1000 | Loss: 0.00000977
Iteration 166/1000 | Loss: 0.00000976
Iteration 167/1000 | Loss: 0.00000976
Iteration 168/1000 | Loss: 0.00000976
Iteration 169/1000 | Loss: 0.00000976
Iteration 170/1000 | Loss: 0.00000976
Iteration 171/1000 | Loss: 0.00000976
Iteration 172/1000 | Loss: 0.00000976
Iteration 173/1000 | Loss: 0.00000976
Iteration 174/1000 | Loss: 0.00000976
Iteration 175/1000 | Loss: 0.00000976
Iteration 176/1000 | Loss: 0.00000975
Iteration 177/1000 | Loss: 0.00000975
Iteration 178/1000 | Loss: 0.00000975
Iteration 179/1000 | Loss: 0.00000975
Iteration 180/1000 | Loss: 0.00000975
Iteration 181/1000 | Loss: 0.00000975
Iteration 182/1000 | Loss: 0.00000975
Iteration 183/1000 | Loss: 0.00000975
Iteration 184/1000 | Loss: 0.00000975
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 184. Stopping optimization.
Last 5 losses: [9.751844118000008e-06, 9.751844118000008e-06, 9.751844118000008e-06, 9.751844118000008e-06, 9.751844118000008e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.751844118000008e-06

Optimization complete. Final v2v error: 2.6854162216186523 mm

Highest mean error: 2.9215433597564697 mm for frame 238

Lowest mean error: 2.4929637908935547 mm for frame 134

Saving results

Total time: 45.81361269950867
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_011/1016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_011/1016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_011/1016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00804629
Iteration 2/25 | Loss: 0.00143569
Iteration 3/25 | Loss: 0.00129894
Iteration 4/25 | Loss: 0.00128218
Iteration 5/25 | Loss: 0.00127798
Iteration 6/25 | Loss: 0.00127690
Iteration 7/25 | Loss: 0.00127673
Iteration 8/25 | Loss: 0.00127673
Iteration 9/25 | Loss: 0.00127673
Iteration 10/25 | Loss: 0.00127673
Iteration 11/25 | Loss: 0.00127673
Iteration 12/25 | Loss: 0.00127673
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.001276728231459856, 0.001276728231459856, 0.001276728231459856, 0.001276728231459856, 0.001276728231459856]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001276728231459856

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35050201
Iteration 2/25 | Loss: 0.00208250
Iteration 3/25 | Loss: 0.00208249
Iteration 4/25 | Loss: 0.00208249
Iteration 5/25 | Loss: 0.00208249
Iteration 6/25 | Loss: 0.00208249
Iteration 7/25 | Loss: 0.00208249
Iteration 8/25 | Loss: 0.00208249
Iteration 9/25 | Loss: 0.00208249
Iteration 10/25 | Loss: 0.00208249
Iteration 11/25 | Loss: 0.00208249
Iteration 12/25 | Loss: 0.00208249
Iteration 13/25 | Loss: 0.00208249
Iteration 14/25 | Loss: 0.00208249
Iteration 15/25 | Loss: 0.00208249
Iteration 16/25 | Loss: 0.00208249
Iteration 17/25 | Loss: 0.00208249
Iteration 18/25 | Loss: 0.00208249
Iteration 19/25 | Loss: 0.00208249
Iteration 20/25 | Loss: 0.00208249
Iteration 21/25 | Loss: 0.00208249
Iteration 22/25 | Loss: 0.00208249
Iteration 23/25 | Loss: 0.00208249
Iteration 24/25 | Loss: 0.00208249
Iteration 25/25 | Loss: 0.00208249

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00208249
Iteration 2/1000 | Loss: 0.00015161
Iteration 3/1000 | Loss: 0.00009478
Iteration 4/1000 | Loss: 0.00007757
Iteration 5/1000 | Loss: 0.00013556
Iteration 6/1000 | Loss: 0.00025113
Iteration 7/1000 | Loss: 0.00006402
Iteration 8/1000 | Loss: 0.00006134
Iteration 9/1000 | Loss: 0.00005901
Iteration 10/1000 | Loss: 0.00012562
Iteration 11/1000 | Loss: 0.00011358
Iteration 12/1000 | Loss: 0.00012154
Iteration 13/1000 | Loss: 0.00009921
Iteration 14/1000 | Loss: 0.00010321
Iteration 15/1000 | Loss: 0.00018262
Iteration 16/1000 | Loss: 0.00019503
Iteration 17/1000 | Loss: 0.00025465
Iteration 18/1000 | Loss: 0.00012462
Iteration 19/1000 | Loss: 0.00015562
Iteration 20/1000 | Loss: 0.00017215
Iteration 21/1000 | Loss: 0.00020069
Iteration 22/1000 | Loss: 0.00014752
Iteration 23/1000 | Loss: 0.00010564
Iteration 24/1000 | Loss: 0.00012412
Iteration 25/1000 | Loss: 0.00010025
Iteration 26/1000 | Loss: 0.00005676
Iteration 27/1000 | Loss: 0.00005523
Iteration 28/1000 | Loss: 0.00006653
Iteration 29/1000 | Loss: 0.00008272
Iteration 30/1000 | Loss: 0.00017984
Iteration 31/1000 | Loss: 0.00019338
Iteration 32/1000 | Loss: 0.00016782
Iteration 33/1000 | Loss: 0.00006978
Iteration 34/1000 | Loss: 0.00016159
Iteration 35/1000 | Loss: 0.00011319
Iteration 36/1000 | Loss: 0.00012277
Iteration 37/1000 | Loss: 0.00014669
Iteration 38/1000 | Loss: 0.00006007
Iteration 39/1000 | Loss: 0.00005558
Iteration 40/1000 | Loss: 0.00005403
Iteration 41/1000 | Loss: 0.00005332
Iteration 42/1000 | Loss: 0.00005278
Iteration 43/1000 | Loss: 0.00005239
Iteration 44/1000 | Loss: 0.00005211
Iteration 45/1000 | Loss: 0.00005532
Iteration 46/1000 | Loss: 0.00005175
Iteration 47/1000 | Loss: 0.00005113
Iteration 48/1000 | Loss: 0.00005088
Iteration 49/1000 | Loss: 0.00005067
Iteration 50/1000 | Loss: 0.00005043
Iteration 51/1000 | Loss: 0.00005020
Iteration 52/1000 | Loss: 0.00004984
Iteration 53/1000 | Loss: 0.00004942
Iteration 54/1000 | Loss: 0.00004899
Iteration 55/1000 | Loss: 0.00036830
Iteration 56/1000 | Loss: 0.00016916
Iteration 57/1000 | Loss: 0.00004942
Iteration 58/1000 | Loss: 0.00004828
Iteration 59/1000 | Loss: 0.00126444
Iteration 60/1000 | Loss: 0.00355284
Iteration 61/1000 | Loss: 0.00194712
Iteration 62/1000 | Loss: 0.00216277
Iteration 63/1000 | Loss: 0.00065027
Iteration 64/1000 | Loss: 0.00073139
Iteration 65/1000 | Loss: 0.00084391
Iteration 66/1000 | Loss: 0.00068838
Iteration 67/1000 | Loss: 0.00047927
Iteration 68/1000 | Loss: 0.00043683
Iteration 69/1000 | Loss: 0.00029286
Iteration 70/1000 | Loss: 0.00040401
Iteration 71/1000 | Loss: 0.00010294
Iteration 72/1000 | Loss: 0.00054320
Iteration 73/1000 | Loss: 0.00025712
Iteration 74/1000 | Loss: 0.00020768
Iteration 75/1000 | Loss: 0.00004398
Iteration 76/1000 | Loss: 0.00003992
Iteration 77/1000 | Loss: 0.00003693
Iteration 78/1000 | Loss: 0.00003519
Iteration 79/1000 | Loss: 0.00015821
Iteration 80/1000 | Loss: 0.00003419
Iteration 81/1000 | Loss: 0.00022671
Iteration 82/1000 | Loss: 0.00004394
Iteration 83/1000 | Loss: 0.00003398
Iteration 84/1000 | Loss: 0.00003129
Iteration 85/1000 | Loss: 0.00003008
Iteration 86/1000 | Loss: 0.00002937
Iteration 87/1000 | Loss: 0.00002874
Iteration 88/1000 | Loss: 0.00002819
Iteration 89/1000 | Loss: 0.00002788
Iteration 90/1000 | Loss: 0.00002739
Iteration 91/1000 | Loss: 0.00002708
Iteration 92/1000 | Loss: 0.00002684
Iteration 93/1000 | Loss: 0.00002679
Iteration 94/1000 | Loss: 0.00002653
Iteration 95/1000 | Loss: 0.00002633
Iteration 96/1000 | Loss: 0.00002614
Iteration 97/1000 | Loss: 0.00002612
Iteration 98/1000 | Loss: 0.00002609
Iteration 99/1000 | Loss: 0.00002609
Iteration 100/1000 | Loss: 0.00002608
Iteration 101/1000 | Loss: 0.00002593
Iteration 102/1000 | Loss: 0.00002591
Iteration 103/1000 | Loss: 0.00002586
Iteration 104/1000 | Loss: 0.00002586
Iteration 105/1000 | Loss: 0.00002584
Iteration 106/1000 | Loss: 0.00002583
Iteration 107/1000 | Loss: 0.00002583
Iteration 108/1000 | Loss: 0.00002582
Iteration 109/1000 | Loss: 0.00002582
Iteration 110/1000 | Loss: 0.00002581
Iteration 111/1000 | Loss: 0.00002581
Iteration 112/1000 | Loss: 0.00002581
Iteration 113/1000 | Loss: 0.00002581
Iteration 114/1000 | Loss: 0.00002580
Iteration 115/1000 | Loss: 0.00002580
Iteration 116/1000 | Loss: 0.00002579
Iteration 117/1000 | Loss: 0.00002579
Iteration 118/1000 | Loss: 0.00002579
Iteration 119/1000 | Loss: 0.00002578
Iteration 120/1000 | Loss: 0.00002578
Iteration 121/1000 | Loss: 0.00002578
Iteration 122/1000 | Loss: 0.00002578
Iteration 123/1000 | Loss: 0.00002577
Iteration 124/1000 | Loss: 0.00002577
Iteration 125/1000 | Loss: 0.00002576
Iteration 126/1000 | Loss: 0.00002576
Iteration 127/1000 | Loss: 0.00002576
Iteration 128/1000 | Loss: 0.00002575
Iteration 129/1000 | Loss: 0.00002575
Iteration 130/1000 | Loss: 0.00002575
Iteration 131/1000 | Loss: 0.00002575
Iteration 132/1000 | Loss: 0.00002574
Iteration 133/1000 | Loss: 0.00002574
Iteration 134/1000 | Loss: 0.00002574
Iteration 135/1000 | Loss: 0.00002573
Iteration 136/1000 | Loss: 0.00002573
Iteration 137/1000 | Loss: 0.00002573
Iteration 138/1000 | Loss: 0.00002572
Iteration 139/1000 | Loss: 0.00002572
Iteration 140/1000 | Loss: 0.00002572
Iteration 141/1000 | Loss: 0.00002571
Iteration 142/1000 | Loss: 0.00002571
Iteration 143/1000 | Loss: 0.00002571
Iteration 144/1000 | Loss: 0.00002571
Iteration 145/1000 | Loss: 0.00002571
Iteration 146/1000 | Loss: 0.00002571
Iteration 147/1000 | Loss: 0.00002571
Iteration 148/1000 | Loss: 0.00002571
Iteration 149/1000 | Loss: 0.00002571
Iteration 150/1000 | Loss: 0.00002570
Iteration 151/1000 | Loss: 0.00002570
Iteration 152/1000 | Loss: 0.00002570
Iteration 153/1000 | Loss: 0.00002570
Iteration 154/1000 | Loss: 0.00002570
Iteration 155/1000 | Loss: 0.00002570
Iteration 156/1000 | Loss: 0.00002570
Iteration 157/1000 | Loss: 0.00002569
Iteration 158/1000 | Loss: 0.00002569
Iteration 159/1000 | Loss: 0.00002569
Iteration 160/1000 | Loss: 0.00002568
Iteration 161/1000 | Loss: 0.00002568
Iteration 162/1000 | Loss: 0.00002568
Iteration 163/1000 | Loss: 0.00002567
Iteration 164/1000 | Loss: 0.00002567
Iteration 165/1000 | Loss: 0.00002566
Iteration 166/1000 | Loss: 0.00002566
Iteration 167/1000 | Loss: 0.00002565
Iteration 168/1000 | Loss: 0.00002565
Iteration 169/1000 | Loss: 0.00002565
Iteration 170/1000 | Loss: 0.00002564
Iteration 171/1000 | Loss: 0.00002564
Iteration 172/1000 | Loss: 0.00002564
Iteration 173/1000 | Loss: 0.00002563
Iteration 174/1000 | Loss: 0.00002563
Iteration 175/1000 | Loss: 0.00002563
Iteration 176/1000 | Loss: 0.00002562
Iteration 177/1000 | Loss: 0.00002562
Iteration 178/1000 | Loss: 0.00002562
Iteration 179/1000 | Loss: 0.00002562
Iteration 180/1000 | Loss: 0.00002562
Iteration 181/1000 | Loss: 0.00002561
Iteration 182/1000 | Loss: 0.00002561
Iteration 183/1000 | Loss: 0.00002561
Iteration 184/1000 | Loss: 0.00002561
Iteration 185/1000 | Loss: 0.00002561
Iteration 186/1000 | Loss: 0.00002560
Iteration 187/1000 | Loss: 0.00002560
Iteration 188/1000 | Loss: 0.00002560
Iteration 189/1000 | Loss: 0.00002560
Iteration 190/1000 | Loss: 0.00002560
Iteration 191/1000 | Loss: 0.00002560
Iteration 192/1000 | Loss: 0.00002560
Iteration 193/1000 | Loss: 0.00002560
Iteration 194/1000 | Loss: 0.00002560
Iteration 195/1000 | Loss: 0.00002560
Iteration 196/1000 | Loss: 0.00002560
Iteration 197/1000 | Loss: 0.00002560
Iteration 198/1000 | Loss: 0.00002559
Iteration 199/1000 | Loss: 0.00002559
Iteration 200/1000 | Loss: 0.00002559
Iteration 201/1000 | Loss: 0.00002559
Iteration 202/1000 | Loss: 0.00002559
Iteration 203/1000 | Loss: 0.00002559
Iteration 204/1000 | Loss: 0.00002559
Iteration 205/1000 | Loss: 0.00002559
Iteration 206/1000 | Loss: 0.00002559
Iteration 207/1000 | Loss: 0.00002559
Iteration 208/1000 | Loss: 0.00002559
Iteration 209/1000 | Loss: 0.00002559
Iteration 210/1000 | Loss: 0.00002559
Iteration 211/1000 | Loss: 0.00002559
Iteration 212/1000 | Loss: 0.00002559
Iteration 213/1000 | Loss: 0.00002559
Iteration 214/1000 | Loss: 0.00002559
Iteration 215/1000 | Loss: 0.00002559
Iteration 216/1000 | Loss: 0.00002559
Iteration 217/1000 | Loss: 0.00002559
Iteration 218/1000 | Loss: 0.00002559
Iteration 219/1000 | Loss: 0.00002559
Iteration 220/1000 | Loss: 0.00002559
Iteration 221/1000 | Loss: 0.00002559
Iteration 222/1000 | Loss: 0.00002559
Iteration 223/1000 | Loss: 0.00002559
Iteration 224/1000 | Loss: 0.00002559
Iteration 225/1000 | Loss: 0.00002559
Iteration 226/1000 | Loss: 0.00002558
Iteration 227/1000 | Loss: 0.00002558
Iteration 228/1000 | Loss: 0.00002558
Iteration 229/1000 | Loss: 0.00002558
Iteration 230/1000 | Loss: 0.00002558
Iteration 231/1000 | Loss: 0.00002558
Iteration 232/1000 | Loss: 0.00002558
Iteration 233/1000 | Loss: 0.00002558
Iteration 234/1000 | Loss: 0.00002558
Iteration 235/1000 | Loss: 0.00002558
Iteration 236/1000 | Loss: 0.00002558
Iteration 237/1000 | Loss: 0.00002558
Iteration 238/1000 | Loss: 0.00002558
Iteration 239/1000 | Loss: 0.00002558
Iteration 240/1000 | Loss: 0.00002558
Iteration 241/1000 | Loss: 0.00002558
Iteration 242/1000 | Loss: 0.00002558
Iteration 243/1000 | Loss: 0.00002558
Iteration 244/1000 | Loss: 0.00002558
Iteration 245/1000 | Loss: 0.00002558
Iteration 246/1000 | Loss: 0.00002558
Iteration 247/1000 | Loss: 0.00002558
Iteration 248/1000 | Loss: 0.00002558
Iteration 249/1000 | Loss: 0.00002558
Iteration 250/1000 | Loss: 0.00002558
Iteration 251/1000 | Loss: 0.00002558
Iteration 252/1000 | Loss: 0.00002558
Iteration 253/1000 | Loss: 0.00002558
Iteration 254/1000 | Loss: 0.00002558
Iteration 255/1000 | Loss: 0.00002558
Iteration 256/1000 | Loss: 0.00002558
Iteration 257/1000 | Loss: 0.00002558
Iteration 258/1000 | Loss: 0.00002558
Iteration 259/1000 | Loss: 0.00002558
Iteration 260/1000 | Loss: 0.00002558
Iteration 261/1000 | Loss: 0.00002558
Iteration 262/1000 | Loss: 0.00002558
Iteration 263/1000 | Loss: 0.00002558
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 263. Stopping optimization.
Last 5 losses: [2.5583840397302993e-05, 2.5583840397302993e-05, 2.5583840397302993e-05, 2.5583840397302993e-05, 2.5583840397302993e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.5583840397302993e-05

Optimization complete. Final v2v error: 3.064838409423828 mm

Highest mean error: 11.600276947021484 mm for frame 89

Lowest mean error: 2.4136266708374023 mm for frame 36

Saving results

Total time: 166.07404589653015
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_011/1073/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_011/1073.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_011/1073
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00705449
Iteration 2/25 | Loss: 0.00167533
Iteration 3/25 | Loss: 0.00135850
Iteration 4/25 | Loss: 0.00132279
Iteration 5/25 | Loss: 0.00131274
Iteration 6/25 | Loss: 0.00130822
Iteration 7/25 | Loss: 0.00130250
Iteration 8/25 | Loss: 0.00129611
Iteration 9/25 | Loss: 0.00129224
Iteration 10/25 | Loss: 0.00129099
Iteration 11/25 | Loss: 0.00129058
Iteration 12/25 | Loss: 0.00129043
Iteration 13/25 | Loss: 0.00129040
Iteration 14/25 | Loss: 0.00129040
Iteration 15/25 | Loss: 0.00129039
Iteration 16/25 | Loss: 0.00129039
Iteration 17/25 | Loss: 0.00129039
Iteration 18/25 | Loss: 0.00129039
Iteration 19/25 | Loss: 0.00129038
Iteration 20/25 | Loss: 0.00129038
Iteration 21/25 | Loss: 0.00129038
Iteration 22/25 | Loss: 0.00129038
Iteration 23/25 | Loss: 0.00129038
Iteration 24/25 | Loss: 0.00129038
Iteration 25/25 | Loss: 0.00129038

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33494127
Iteration 2/25 | Loss: 0.00074035
Iteration 3/25 | Loss: 0.00074032
Iteration 4/25 | Loss: 0.00074032
Iteration 5/25 | Loss: 0.00074032
Iteration 6/25 | Loss: 0.00074032
Iteration 7/25 | Loss: 0.00074032
Iteration 8/25 | Loss: 0.00074032
Iteration 9/25 | Loss: 0.00074032
Iteration 10/25 | Loss: 0.00074032
Iteration 11/25 | Loss: 0.00074032
Iteration 12/25 | Loss: 0.00074031
Iteration 13/25 | Loss: 0.00074031
Iteration 14/25 | Loss: 0.00074031
Iteration 15/25 | Loss: 0.00074031
Iteration 16/25 | Loss: 0.00074031
Iteration 17/25 | Loss: 0.00074031
Iteration 18/25 | Loss: 0.00074031
Iteration 19/25 | Loss: 0.00074031
Iteration 20/25 | Loss: 0.00074031
Iteration 21/25 | Loss: 0.00074031
Iteration 22/25 | Loss: 0.00074031
Iteration 23/25 | Loss: 0.00074031
Iteration 24/25 | Loss: 0.00074031
Iteration 25/25 | Loss: 0.00074031

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00074031
Iteration 2/1000 | Loss: 0.00003817
Iteration 3/1000 | Loss: 0.00002536
Iteration 4/1000 | Loss: 0.00002268
Iteration 5/1000 | Loss: 0.00002143
Iteration 6/1000 | Loss: 0.00002065
Iteration 7/1000 | Loss: 0.00002015
Iteration 8/1000 | Loss: 0.00001983
Iteration 9/1000 | Loss: 0.00001971
Iteration 10/1000 | Loss: 0.00001950
Iteration 11/1000 | Loss: 0.00001935
Iteration 12/1000 | Loss: 0.00001932
Iteration 13/1000 | Loss: 0.00001922
Iteration 14/1000 | Loss: 0.00001912
Iteration 15/1000 | Loss: 0.00001903
Iteration 16/1000 | Loss: 0.00001902
Iteration 17/1000 | Loss: 0.00001900
Iteration 18/1000 | Loss: 0.00001899
Iteration 19/1000 | Loss: 0.00001896
Iteration 20/1000 | Loss: 0.00001896
Iteration 21/1000 | Loss: 0.00001896
Iteration 22/1000 | Loss: 0.00001896
Iteration 23/1000 | Loss: 0.00001895
Iteration 24/1000 | Loss: 0.00001895
Iteration 25/1000 | Loss: 0.00001895
Iteration 26/1000 | Loss: 0.00001894
Iteration 27/1000 | Loss: 0.00001894
Iteration 28/1000 | Loss: 0.00001893
Iteration 29/1000 | Loss: 0.00001893
Iteration 30/1000 | Loss: 0.00001893
Iteration 31/1000 | Loss: 0.00001893
Iteration 32/1000 | Loss: 0.00001892
Iteration 33/1000 | Loss: 0.00001892
Iteration 34/1000 | Loss: 0.00001892
Iteration 35/1000 | Loss: 0.00001892
Iteration 36/1000 | Loss: 0.00001891
Iteration 37/1000 | Loss: 0.00001891
Iteration 38/1000 | Loss: 0.00001891
Iteration 39/1000 | Loss: 0.00001891
Iteration 40/1000 | Loss: 0.00001891
Iteration 41/1000 | Loss: 0.00001891
Iteration 42/1000 | Loss: 0.00001890
Iteration 43/1000 | Loss: 0.00001890
Iteration 44/1000 | Loss: 0.00001890
Iteration 45/1000 | Loss: 0.00001890
Iteration 46/1000 | Loss: 0.00001890
Iteration 47/1000 | Loss: 0.00001890
Iteration 48/1000 | Loss: 0.00001889
Iteration 49/1000 | Loss: 0.00001889
Iteration 50/1000 | Loss: 0.00001889
Iteration 51/1000 | Loss: 0.00001889
Iteration 52/1000 | Loss: 0.00001888
Iteration 53/1000 | Loss: 0.00001888
Iteration 54/1000 | Loss: 0.00001888
Iteration 55/1000 | Loss: 0.00001888
Iteration 56/1000 | Loss: 0.00001888
Iteration 57/1000 | Loss: 0.00001888
Iteration 58/1000 | Loss: 0.00001887
Iteration 59/1000 | Loss: 0.00001887
Iteration 60/1000 | Loss: 0.00001887
Iteration 61/1000 | Loss: 0.00001887
Iteration 62/1000 | Loss: 0.00001887
Iteration 63/1000 | Loss: 0.00001887
Iteration 64/1000 | Loss: 0.00001887
Iteration 65/1000 | Loss: 0.00001887
Iteration 66/1000 | Loss: 0.00001886
Iteration 67/1000 | Loss: 0.00001886
Iteration 68/1000 | Loss: 0.00001886
Iteration 69/1000 | Loss: 0.00001885
Iteration 70/1000 | Loss: 0.00001885
Iteration 71/1000 | Loss: 0.00001885
Iteration 72/1000 | Loss: 0.00001885
Iteration 73/1000 | Loss: 0.00001884
Iteration 74/1000 | Loss: 0.00001884
Iteration 75/1000 | Loss: 0.00001884
Iteration 76/1000 | Loss: 0.00001884
Iteration 77/1000 | Loss: 0.00001884
Iteration 78/1000 | Loss: 0.00001883
Iteration 79/1000 | Loss: 0.00001883
Iteration 80/1000 | Loss: 0.00001883
Iteration 81/1000 | Loss: 0.00001883
Iteration 82/1000 | Loss: 0.00001882
Iteration 83/1000 | Loss: 0.00001882
Iteration 84/1000 | Loss: 0.00001882
Iteration 85/1000 | Loss: 0.00001882
Iteration 86/1000 | Loss: 0.00001882
Iteration 87/1000 | Loss: 0.00001882
Iteration 88/1000 | Loss: 0.00001881
Iteration 89/1000 | Loss: 0.00001881
Iteration 90/1000 | Loss: 0.00001881
Iteration 91/1000 | Loss: 0.00001881
Iteration 92/1000 | Loss: 0.00001880
Iteration 93/1000 | Loss: 0.00001880
Iteration 94/1000 | Loss: 0.00001880
Iteration 95/1000 | Loss: 0.00001879
Iteration 96/1000 | Loss: 0.00001879
Iteration 97/1000 | Loss: 0.00001879
Iteration 98/1000 | Loss: 0.00001879
Iteration 99/1000 | Loss: 0.00001879
Iteration 100/1000 | Loss: 0.00001878
Iteration 101/1000 | Loss: 0.00001878
Iteration 102/1000 | Loss: 0.00001878
Iteration 103/1000 | Loss: 0.00001878
Iteration 104/1000 | Loss: 0.00001877
Iteration 105/1000 | Loss: 0.00001877
Iteration 106/1000 | Loss: 0.00001877
Iteration 107/1000 | Loss: 0.00001876
Iteration 108/1000 | Loss: 0.00001876
Iteration 109/1000 | Loss: 0.00001876
Iteration 110/1000 | Loss: 0.00001875
Iteration 111/1000 | Loss: 0.00001875
Iteration 112/1000 | Loss: 0.00001875
Iteration 113/1000 | Loss: 0.00001875
Iteration 114/1000 | Loss: 0.00001875
Iteration 115/1000 | Loss: 0.00001875
Iteration 116/1000 | Loss: 0.00001874
Iteration 117/1000 | Loss: 0.00001874
Iteration 118/1000 | Loss: 0.00001874
Iteration 119/1000 | Loss: 0.00001874
Iteration 120/1000 | Loss: 0.00001874
Iteration 121/1000 | Loss: 0.00001874
Iteration 122/1000 | Loss: 0.00001874
Iteration 123/1000 | Loss: 0.00001873
Iteration 124/1000 | Loss: 0.00001873
Iteration 125/1000 | Loss: 0.00001873
Iteration 126/1000 | Loss: 0.00001873
Iteration 127/1000 | Loss: 0.00001873
Iteration 128/1000 | Loss: 0.00001873
Iteration 129/1000 | Loss: 0.00001873
Iteration 130/1000 | Loss: 0.00001873
Iteration 131/1000 | Loss: 0.00001873
Iteration 132/1000 | Loss: 0.00001872
Iteration 133/1000 | Loss: 0.00001872
Iteration 134/1000 | Loss: 0.00001872
Iteration 135/1000 | Loss: 0.00001872
Iteration 136/1000 | Loss: 0.00001872
Iteration 137/1000 | Loss: 0.00001871
Iteration 138/1000 | Loss: 0.00001871
Iteration 139/1000 | Loss: 0.00001871
Iteration 140/1000 | Loss: 0.00001871
Iteration 141/1000 | Loss: 0.00001871
Iteration 142/1000 | Loss: 0.00001871
Iteration 143/1000 | Loss: 0.00001871
Iteration 144/1000 | Loss: 0.00001871
Iteration 145/1000 | Loss: 0.00001871
Iteration 146/1000 | Loss: 0.00001871
Iteration 147/1000 | Loss: 0.00001871
Iteration 148/1000 | Loss: 0.00001870
Iteration 149/1000 | Loss: 0.00001870
Iteration 150/1000 | Loss: 0.00001870
Iteration 151/1000 | Loss: 0.00001870
Iteration 152/1000 | Loss: 0.00001869
Iteration 153/1000 | Loss: 0.00001869
Iteration 154/1000 | Loss: 0.00001869
Iteration 155/1000 | Loss: 0.00001869
Iteration 156/1000 | Loss: 0.00001869
Iteration 157/1000 | Loss: 0.00001869
Iteration 158/1000 | Loss: 0.00001869
Iteration 159/1000 | Loss: 0.00001869
Iteration 160/1000 | Loss: 0.00001869
Iteration 161/1000 | Loss: 0.00001869
Iteration 162/1000 | Loss: 0.00001869
Iteration 163/1000 | Loss: 0.00001869
Iteration 164/1000 | Loss: 0.00001869
Iteration 165/1000 | Loss: 0.00001869
Iteration 166/1000 | Loss: 0.00001869
Iteration 167/1000 | Loss: 0.00001869
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 167. Stopping optimization.
Last 5 losses: [1.8686085240915418e-05, 1.8686085240915418e-05, 1.8686085240915418e-05, 1.8686085240915418e-05, 1.8686085240915418e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8686085240915418e-05

Optimization complete. Final v2v error: 3.5629916191101074 mm

Highest mean error: 4.033146858215332 mm for frame 119

Lowest mean error: 2.919529914855957 mm for frame 15

Saving results

Total time: 58.99489688873291
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_011/1081/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_011/1081.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_011/1081
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00576067
Iteration 2/25 | Loss: 0.00168007
Iteration 3/25 | Loss: 0.00140979
Iteration 4/25 | Loss: 0.00138130
Iteration 5/25 | Loss: 0.00138517
Iteration 6/25 | Loss: 0.00137099
Iteration 7/25 | Loss: 0.00136434
Iteration 8/25 | Loss: 0.00136288
Iteration 9/25 | Loss: 0.00136269
Iteration 10/25 | Loss: 0.00136259
Iteration 11/25 | Loss: 0.00136259
Iteration 12/25 | Loss: 0.00136259
Iteration 13/25 | Loss: 0.00136258
Iteration 14/25 | Loss: 0.00136258
Iteration 15/25 | Loss: 0.00136257
Iteration 16/25 | Loss: 0.00136257
Iteration 17/25 | Loss: 0.00136257
Iteration 18/25 | Loss: 0.00136257
Iteration 19/25 | Loss: 0.00136257
Iteration 20/25 | Loss: 0.00136256
Iteration 21/25 | Loss: 0.00136256
Iteration 22/25 | Loss: 0.00136256
Iteration 23/25 | Loss: 0.00136256
Iteration 24/25 | Loss: 0.00136256
Iteration 25/25 | Loss: 0.00136256

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.24493825
Iteration 2/25 | Loss: 0.00138998
Iteration 3/25 | Loss: 0.00138995
Iteration 4/25 | Loss: 0.00138995
Iteration 5/25 | Loss: 0.00138995
Iteration 6/25 | Loss: 0.00138995
Iteration 7/25 | Loss: 0.00138995
Iteration 8/25 | Loss: 0.00138995
Iteration 9/25 | Loss: 0.00138995
Iteration 10/25 | Loss: 0.00138995
Iteration 11/25 | Loss: 0.00138995
Iteration 12/25 | Loss: 0.00138995
Iteration 13/25 | Loss: 0.00138995
Iteration 14/25 | Loss: 0.00138995
Iteration 15/25 | Loss: 0.00138995
Iteration 16/25 | Loss: 0.00138995
Iteration 17/25 | Loss: 0.00138995
Iteration 18/25 | Loss: 0.00138995
Iteration 19/25 | Loss: 0.00138995
Iteration 20/25 | Loss: 0.00138995
Iteration 21/25 | Loss: 0.00138995
Iteration 22/25 | Loss: 0.00138995
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.001389945624396205, 0.001389945624396205, 0.001389945624396205, 0.001389945624396205, 0.001389945624396205]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001389945624396205

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00138995
Iteration 2/1000 | Loss: 0.00012134
Iteration 3/1000 | Loss: 0.00008712
Iteration 4/1000 | Loss: 0.00007631
Iteration 5/1000 | Loss: 0.00007012
Iteration 6/1000 | Loss: 0.00006593
Iteration 7/1000 | Loss: 0.00039913
Iteration 8/1000 | Loss: 0.00025678
Iteration 9/1000 | Loss: 0.00006315
Iteration 10/1000 | Loss: 0.00038673
Iteration 11/1000 | Loss: 0.00006641
Iteration 12/1000 | Loss: 0.00038847
Iteration 13/1000 | Loss: 0.00018634
Iteration 14/1000 | Loss: 0.00034347
Iteration 15/1000 | Loss: 0.00016325
Iteration 16/1000 | Loss: 0.00005891
Iteration 17/1000 | Loss: 0.00017653
Iteration 18/1000 | Loss: 0.00005865
Iteration 19/1000 | Loss: 0.00005539
Iteration 20/1000 | Loss: 0.00005432
Iteration 21/1000 | Loss: 0.00005336
Iteration 22/1000 | Loss: 0.00066585
Iteration 23/1000 | Loss: 0.00037541
Iteration 24/1000 | Loss: 0.00043556
Iteration 25/1000 | Loss: 0.00006051
Iteration 26/1000 | Loss: 0.00026442
Iteration 27/1000 | Loss: 0.00018029
Iteration 28/1000 | Loss: 0.00005516
Iteration 29/1000 | Loss: 0.00060822
Iteration 30/1000 | Loss: 0.00024814
Iteration 31/1000 | Loss: 0.00034399
Iteration 32/1000 | Loss: 0.00024248
Iteration 33/1000 | Loss: 0.00005188
Iteration 34/1000 | Loss: 0.00033928
Iteration 35/1000 | Loss: 0.00026841
Iteration 36/1000 | Loss: 0.00033147
Iteration 37/1000 | Loss: 0.00031009
Iteration 38/1000 | Loss: 0.00006448
Iteration 39/1000 | Loss: 0.00005321
Iteration 40/1000 | Loss: 0.00038348
Iteration 41/1000 | Loss: 0.00073182
Iteration 42/1000 | Loss: 0.00012572
Iteration 43/1000 | Loss: 0.00037309
Iteration 44/1000 | Loss: 0.00012198
Iteration 45/1000 | Loss: 0.00036421
Iteration 46/1000 | Loss: 0.00013336
Iteration 47/1000 | Loss: 0.00029767
Iteration 48/1000 | Loss: 0.00014960
Iteration 49/1000 | Loss: 0.00028079
Iteration 50/1000 | Loss: 0.00013237
Iteration 51/1000 | Loss: 0.00021202
Iteration 52/1000 | Loss: 0.00006602
Iteration 53/1000 | Loss: 0.00005135
Iteration 54/1000 | Loss: 0.00004805
Iteration 55/1000 | Loss: 0.00004642
Iteration 56/1000 | Loss: 0.00004521
Iteration 57/1000 | Loss: 0.00004437
Iteration 58/1000 | Loss: 0.00022382
Iteration 59/1000 | Loss: 0.00007352
Iteration 60/1000 | Loss: 0.00019379
Iteration 61/1000 | Loss: 0.00007606
Iteration 62/1000 | Loss: 0.00017247
Iteration 63/1000 | Loss: 0.00004770
Iteration 64/1000 | Loss: 0.00004331
Iteration 65/1000 | Loss: 0.00004195
Iteration 66/1000 | Loss: 0.00004143
Iteration 67/1000 | Loss: 0.00004102
Iteration 68/1000 | Loss: 0.00004074
Iteration 69/1000 | Loss: 0.00004044
Iteration 70/1000 | Loss: 0.00004025
Iteration 71/1000 | Loss: 0.00004023
Iteration 72/1000 | Loss: 0.00004017
Iteration 73/1000 | Loss: 0.00004008
Iteration 74/1000 | Loss: 0.00004006
Iteration 75/1000 | Loss: 0.00004005
Iteration 76/1000 | Loss: 0.00004005
Iteration 77/1000 | Loss: 0.00004003
Iteration 78/1000 | Loss: 0.00004001
Iteration 79/1000 | Loss: 0.00003997
Iteration 80/1000 | Loss: 0.00003997
Iteration 81/1000 | Loss: 0.00003996
Iteration 82/1000 | Loss: 0.00003996
Iteration 83/1000 | Loss: 0.00003995
Iteration 84/1000 | Loss: 0.00003994
Iteration 85/1000 | Loss: 0.00003994
Iteration 86/1000 | Loss: 0.00003993
Iteration 87/1000 | Loss: 0.00003993
Iteration 88/1000 | Loss: 0.00003992
Iteration 89/1000 | Loss: 0.00003992
Iteration 90/1000 | Loss: 0.00003992
Iteration 91/1000 | Loss: 0.00003991
Iteration 92/1000 | Loss: 0.00003991
Iteration 93/1000 | Loss: 0.00003991
Iteration 94/1000 | Loss: 0.00003991
Iteration 95/1000 | Loss: 0.00003991
Iteration 96/1000 | Loss: 0.00003990
Iteration 97/1000 | Loss: 0.00003990
Iteration 98/1000 | Loss: 0.00003990
Iteration 99/1000 | Loss: 0.00003990
Iteration 100/1000 | Loss: 0.00003990
Iteration 101/1000 | Loss: 0.00003989
Iteration 102/1000 | Loss: 0.00003989
Iteration 103/1000 | Loss: 0.00003988
Iteration 104/1000 | Loss: 0.00003988
Iteration 105/1000 | Loss: 0.00003987
Iteration 106/1000 | Loss: 0.00003985
Iteration 107/1000 | Loss: 0.00003985
Iteration 108/1000 | Loss: 0.00003985
Iteration 109/1000 | Loss: 0.00003985
Iteration 110/1000 | Loss: 0.00003984
Iteration 111/1000 | Loss: 0.00003981
Iteration 112/1000 | Loss: 0.00003981
Iteration 113/1000 | Loss: 0.00003980
Iteration 114/1000 | Loss: 0.00003980
Iteration 115/1000 | Loss: 0.00003978
Iteration 116/1000 | Loss: 0.00003977
Iteration 117/1000 | Loss: 0.00003975
Iteration 118/1000 | Loss: 0.00003975
Iteration 119/1000 | Loss: 0.00003975
Iteration 120/1000 | Loss: 0.00003975
Iteration 121/1000 | Loss: 0.00003974
Iteration 122/1000 | Loss: 0.00003974
Iteration 123/1000 | Loss: 0.00003974
Iteration 124/1000 | Loss: 0.00003974
Iteration 125/1000 | Loss: 0.00003973
Iteration 126/1000 | Loss: 0.00003973
Iteration 127/1000 | Loss: 0.00003973
Iteration 128/1000 | Loss: 0.00003973
Iteration 129/1000 | Loss: 0.00003973
Iteration 130/1000 | Loss: 0.00003973
Iteration 131/1000 | Loss: 0.00003972
Iteration 132/1000 | Loss: 0.00003972
Iteration 133/1000 | Loss: 0.00003972
Iteration 134/1000 | Loss: 0.00003971
Iteration 135/1000 | Loss: 0.00003971
Iteration 136/1000 | Loss: 0.00003971
Iteration 137/1000 | Loss: 0.00003971
Iteration 138/1000 | Loss: 0.00003971
Iteration 139/1000 | Loss: 0.00003970
Iteration 140/1000 | Loss: 0.00003970
Iteration 141/1000 | Loss: 0.00003970
Iteration 142/1000 | Loss: 0.00003970
Iteration 143/1000 | Loss: 0.00003968
Iteration 144/1000 | Loss: 0.00003968
Iteration 145/1000 | Loss: 0.00003968
Iteration 146/1000 | Loss: 0.00003967
Iteration 147/1000 | Loss: 0.00003967
Iteration 148/1000 | Loss: 0.00003967
Iteration 149/1000 | Loss: 0.00003967
Iteration 150/1000 | Loss: 0.00003966
Iteration 151/1000 | Loss: 0.00003966
Iteration 152/1000 | Loss: 0.00003966
Iteration 153/1000 | Loss: 0.00003966
Iteration 154/1000 | Loss: 0.00003966
Iteration 155/1000 | Loss: 0.00003965
Iteration 156/1000 | Loss: 0.00003965
Iteration 157/1000 | Loss: 0.00003965
Iteration 158/1000 | Loss: 0.00003965
Iteration 159/1000 | Loss: 0.00003965
Iteration 160/1000 | Loss: 0.00003965
Iteration 161/1000 | Loss: 0.00003965
Iteration 162/1000 | Loss: 0.00003964
Iteration 163/1000 | Loss: 0.00003964
Iteration 164/1000 | Loss: 0.00003964
Iteration 165/1000 | Loss: 0.00023654
Iteration 166/1000 | Loss: 0.00003921
Iteration 167/1000 | Loss: 0.00003861
Iteration 168/1000 | Loss: 0.00003838
Iteration 169/1000 | Loss: 0.00003818
Iteration 170/1000 | Loss: 0.00003799
Iteration 171/1000 | Loss: 0.00003785
Iteration 172/1000 | Loss: 0.00003781
Iteration 173/1000 | Loss: 0.00003764
Iteration 174/1000 | Loss: 0.00003759
Iteration 175/1000 | Loss: 0.00003756
Iteration 176/1000 | Loss: 0.00003755
Iteration 177/1000 | Loss: 0.00003755
Iteration 178/1000 | Loss: 0.00003755
Iteration 179/1000 | Loss: 0.00003754
Iteration 180/1000 | Loss: 0.00003754
Iteration 181/1000 | Loss: 0.00003754
Iteration 182/1000 | Loss: 0.00003754
Iteration 183/1000 | Loss: 0.00003754
Iteration 184/1000 | Loss: 0.00003754
Iteration 185/1000 | Loss: 0.00003753
Iteration 186/1000 | Loss: 0.00003753
Iteration 187/1000 | Loss: 0.00003753
Iteration 188/1000 | Loss: 0.00003753
Iteration 189/1000 | Loss: 0.00003753
Iteration 190/1000 | Loss: 0.00003753
Iteration 191/1000 | Loss: 0.00003753
Iteration 192/1000 | Loss: 0.00003753
Iteration 193/1000 | Loss: 0.00003753
Iteration 194/1000 | Loss: 0.00003753
Iteration 195/1000 | Loss: 0.00003753
Iteration 196/1000 | Loss: 0.00003753
Iteration 197/1000 | Loss: 0.00003753
Iteration 198/1000 | Loss: 0.00003752
Iteration 199/1000 | Loss: 0.00003752
Iteration 200/1000 | Loss: 0.00003752
Iteration 201/1000 | Loss: 0.00003752
Iteration 202/1000 | Loss: 0.00003752
Iteration 203/1000 | Loss: 0.00003752
Iteration 204/1000 | Loss: 0.00003752
Iteration 205/1000 | Loss: 0.00003752
Iteration 206/1000 | Loss: 0.00003752
Iteration 207/1000 | Loss: 0.00003752
Iteration 208/1000 | Loss: 0.00003752
Iteration 209/1000 | Loss: 0.00003752
Iteration 210/1000 | Loss: 0.00003751
Iteration 211/1000 | Loss: 0.00003751
Iteration 212/1000 | Loss: 0.00003751
Iteration 213/1000 | Loss: 0.00003751
Iteration 214/1000 | Loss: 0.00003751
Iteration 215/1000 | Loss: 0.00003751
Iteration 216/1000 | Loss: 0.00003751
Iteration 217/1000 | Loss: 0.00003751
Iteration 218/1000 | Loss: 0.00003751
Iteration 219/1000 | Loss: 0.00003751
Iteration 220/1000 | Loss: 0.00003751
Iteration 221/1000 | Loss: 0.00003751
Iteration 222/1000 | Loss: 0.00003751
Iteration 223/1000 | Loss: 0.00003751
Iteration 224/1000 | Loss: 0.00003750
Iteration 225/1000 | Loss: 0.00003750
Iteration 226/1000 | Loss: 0.00003750
Iteration 227/1000 | Loss: 0.00003750
Iteration 228/1000 | Loss: 0.00003750
Iteration 229/1000 | Loss: 0.00003750
Iteration 230/1000 | Loss: 0.00003750
Iteration 231/1000 | Loss: 0.00003749
Iteration 232/1000 | Loss: 0.00003749
Iteration 233/1000 | Loss: 0.00003749
Iteration 234/1000 | Loss: 0.00003749
Iteration 235/1000 | Loss: 0.00003749
Iteration 236/1000 | Loss: 0.00003749
Iteration 237/1000 | Loss: 0.00003749
Iteration 238/1000 | Loss: 0.00003749
Iteration 239/1000 | Loss: 0.00003749
Iteration 240/1000 | Loss: 0.00003749
Iteration 241/1000 | Loss: 0.00003749
Iteration 242/1000 | Loss: 0.00003749
Iteration 243/1000 | Loss: 0.00003749
Iteration 244/1000 | Loss: 0.00003749
Iteration 245/1000 | Loss: 0.00003749
Iteration 246/1000 | Loss: 0.00003749
Iteration 247/1000 | Loss: 0.00003749
Iteration 248/1000 | Loss: 0.00003749
Iteration 249/1000 | Loss: 0.00003749
Iteration 250/1000 | Loss: 0.00003749
Iteration 251/1000 | Loss: 0.00003749
Iteration 252/1000 | Loss: 0.00003749
Iteration 253/1000 | Loss: 0.00003748
Iteration 254/1000 | Loss: 0.00003748
Iteration 255/1000 | Loss: 0.00003748
Iteration 256/1000 | Loss: 0.00003748
Iteration 257/1000 | Loss: 0.00003748
Iteration 258/1000 | Loss: 0.00003748
Iteration 259/1000 | Loss: 0.00003748
Iteration 260/1000 | Loss: 0.00003748
Iteration 261/1000 | Loss: 0.00003748
Iteration 262/1000 | Loss: 0.00003748
Iteration 263/1000 | Loss: 0.00003748
Iteration 264/1000 | Loss: 0.00003748
Iteration 265/1000 | Loss: 0.00003748
Iteration 266/1000 | Loss: 0.00003748
Iteration 267/1000 | Loss: 0.00003748
Iteration 268/1000 | Loss: 0.00003748
Iteration 269/1000 | Loss: 0.00003748
Iteration 270/1000 | Loss: 0.00003748
Iteration 271/1000 | Loss: 0.00003748
Iteration 272/1000 | Loss: 0.00003748
Iteration 273/1000 | Loss: 0.00003748
Iteration 274/1000 | Loss: 0.00003748
Iteration 275/1000 | Loss: 0.00003748
Iteration 276/1000 | Loss: 0.00003747
Iteration 277/1000 | Loss: 0.00003747
Iteration 278/1000 | Loss: 0.00003747
Iteration 279/1000 | Loss: 0.00003747
Iteration 280/1000 | Loss: 0.00003747
Iteration 281/1000 | Loss: 0.00003747
Iteration 282/1000 | Loss: 0.00003747
Iteration 283/1000 | Loss: 0.00003747
Iteration 284/1000 | Loss: 0.00003747
Iteration 285/1000 | Loss: 0.00003747
Iteration 286/1000 | Loss: 0.00003747
Iteration 287/1000 | Loss: 0.00003747
Iteration 288/1000 | Loss: 0.00003747
Iteration 289/1000 | Loss: 0.00003747
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 289. Stopping optimization.
Last 5 losses: [3.747419032151811e-05, 3.747419032151811e-05, 3.747419032151811e-05, 3.747419032151811e-05, 3.747419032151811e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.747419032151811e-05

Optimization complete. Final v2v error: 3.940437078475952 mm

Highest mean error: 11.034494400024414 mm for frame 92

Lowest mean error: 2.7092621326446533 mm for frame 126

Saving results

Total time: 165.46861457824707
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_011/1038/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_011/1038.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_011/1038
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00593286
Iteration 2/25 | Loss: 0.00137126
Iteration 3/25 | Loss: 0.00130372
Iteration 4/25 | Loss: 0.00129548
Iteration 5/25 | Loss: 0.00129302
Iteration 6/25 | Loss: 0.00129302
Iteration 7/25 | Loss: 0.00129302
Iteration 8/25 | Loss: 0.00129302
Iteration 9/25 | Loss: 0.00129302
Iteration 10/25 | Loss: 0.00129302
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.001293018227443099, 0.001293018227443099, 0.001293018227443099, 0.001293018227443099, 0.001293018227443099]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001293018227443099

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36509883
Iteration 2/25 | Loss: 0.00091961
Iteration 3/25 | Loss: 0.00091958
Iteration 4/25 | Loss: 0.00091958
Iteration 5/25 | Loss: 0.00091958
Iteration 6/25 | Loss: 0.00091958
Iteration 7/25 | Loss: 0.00091958
Iteration 8/25 | Loss: 0.00091958
Iteration 9/25 | Loss: 0.00091958
Iteration 10/25 | Loss: 0.00091958
Iteration 11/25 | Loss: 0.00091958
Iteration 12/25 | Loss: 0.00091958
Iteration 13/25 | Loss: 0.00091958
Iteration 14/25 | Loss: 0.00091958
Iteration 15/25 | Loss: 0.00091958
Iteration 16/25 | Loss: 0.00091958
Iteration 17/25 | Loss: 0.00091958
Iteration 18/25 | Loss: 0.00091958
Iteration 19/25 | Loss: 0.00091958
Iteration 20/25 | Loss: 0.00091958
Iteration 21/25 | Loss: 0.00091958
Iteration 22/25 | Loss: 0.00091958
Iteration 23/25 | Loss: 0.00091958
Iteration 24/25 | Loss: 0.00091958
Iteration 25/25 | Loss: 0.00091958

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00091958
Iteration 2/1000 | Loss: 0.00003127
Iteration 3/1000 | Loss: 0.00002275
Iteration 4/1000 | Loss: 0.00002098
Iteration 5/1000 | Loss: 0.00002017
Iteration 6/1000 | Loss: 0.00001958
Iteration 7/1000 | Loss: 0.00001913
Iteration 8/1000 | Loss: 0.00001879
Iteration 9/1000 | Loss: 0.00001842
Iteration 10/1000 | Loss: 0.00001816
Iteration 11/1000 | Loss: 0.00001802
Iteration 12/1000 | Loss: 0.00001790
Iteration 13/1000 | Loss: 0.00001788
Iteration 14/1000 | Loss: 0.00001785
Iteration 15/1000 | Loss: 0.00001778
Iteration 16/1000 | Loss: 0.00001770
Iteration 17/1000 | Loss: 0.00001762
Iteration 18/1000 | Loss: 0.00001758
Iteration 19/1000 | Loss: 0.00001756
Iteration 20/1000 | Loss: 0.00001753
Iteration 21/1000 | Loss: 0.00001753
Iteration 22/1000 | Loss: 0.00001752
Iteration 23/1000 | Loss: 0.00001750
Iteration 24/1000 | Loss: 0.00001742
Iteration 25/1000 | Loss: 0.00001739
Iteration 26/1000 | Loss: 0.00001738
Iteration 27/1000 | Loss: 0.00001736
Iteration 28/1000 | Loss: 0.00001736
Iteration 29/1000 | Loss: 0.00001736
Iteration 30/1000 | Loss: 0.00001735
Iteration 31/1000 | Loss: 0.00001735
Iteration 32/1000 | Loss: 0.00001734
Iteration 33/1000 | Loss: 0.00001729
Iteration 34/1000 | Loss: 0.00001729
Iteration 35/1000 | Loss: 0.00001727
Iteration 36/1000 | Loss: 0.00001726
Iteration 37/1000 | Loss: 0.00001724
Iteration 38/1000 | Loss: 0.00001723
Iteration 39/1000 | Loss: 0.00001723
Iteration 40/1000 | Loss: 0.00001721
Iteration 41/1000 | Loss: 0.00001721
Iteration 42/1000 | Loss: 0.00001720
Iteration 43/1000 | Loss: 0.00001719
Iteration 44/1000 | Loss: 0.00001719
Iteration 45/1000 | Loss: 0.00001719
Iteration 46/1000 | Loss: 0.00001719
Iteration 47/1000 | Loss: 0.00001718
Iteration 48/1000 | Loss: 0.00001718
Iteration 49/1000 | Loss: 0.00001718
Iteration 50/1000 | Loss: 0.00001718
Iteration 51/1000 | Loss: 0.00001718
Iteration 52/1000 | Loss: 0.00001718
Iteration 53/1000 | Loss: 0.00001718
Iteration 54/1000 | Loss: 0.00001718
Iteration 55/1000 | Loss: 0.00001718
Iteration 56/1000 | Loss: 0.00001718
Iteration 57/1000 | Loss: 0.00001717
Iteration 58/1000 | Loss: 0.00001717
Iteration 59/1000 | Loss: 0.00001716
Iteration 60/1000 | Loss: 0.00001716
Iteration 61/1000 | Loss: 0.00001715
Iteration 62/1000 | Loss: 0.00001714
Iteration 63/1000 | Loss: 0.00001714
Iteration 64/1000 | Loss: 0.00001714
Iteration 65/1000 | Loss: 0.00001713
Iteration 66/1000 | Loss: 0.00001713
Iteration 67/1000 | Loss: 0.00001712
Iteration 68/1000 | Loss: 0.00001712
Iteration 69/1000 | Loss: 0.00001710
Iteration 70/1000 | Loss: 0.00001710
Iteration 71/1000 | Loss: 0.00001710
Iteration 72/1000 | Loss: 0.00001710
Iteration 73/1000 | Loss: 0.00001710
Iteration 74/1000 | Loss: 0.00001710
Iteration 75/1000 | Loss: 0.00001710
Iteration 76/1000 | Loss: 0.00001710
Iteration 77/1000 | Loss: 0.00001710
Iteration 78/1000 | Loss: 0.00001709
Iteration 79/1000 | Loss: 0.00001708
Iteration 80/1000 | Loss: 0.00001708
Iteration 81/1000 | Loss: 0.00001708
Iteration 82/1000 | Loss: 0.00001707
Iteration 83/1000 | Loss: 0.00001707
Iteration 84/1000 | Loss: 0.00001706
Iteration 85/1000 | Loss: 0.00001706
Iteration 86/1000 | Loss: 0.00001706
Iteration 87/1000 | Loss: 0.00001705
Iteration 88/1000 | Loss: 0.00001705
Iteration 89/1000 | Loss: 0.00001705
Iteration 90/1000 | Loss: 0.00001705
Iteration 91/1000 | Loss: 0.00001705
Iteration 92/1000 | Loss: 0.00001705
Iteration 93/1000 | Loss: 0.00001705
Iteration 94/1000 | Loss: 0.00001705
Iteration 95/1000 | Loss: 0.00001705
Iteration 96/1000 | Loss: 0.00001705
Iteration 97/1000 | Loss: 0.00001705
Iteration 98/1000 | Loss: 0.00001704
Iteration 99/1000 | Loss: 0.00001704
Iteration 100/1000 | Loss: 0.00001704
Iteration 101/1000 | Loss: 0.00001703
Iteration 102/1000 | Loss: 0.00001703
Iteration 103/1000 | Loss: 0.00001702
Iteration 104/1000 | Loss: 0.00001702
Iteration 105/1000 | Loss: 0.00001702
Iteration 106/1000 | Loss: 0.00001701
Iteration 107/1000 | Loss: 0.00001701
Iteration 108/1000 | Loss: 0.00001701
Iteration 109/1000 | Loss: 0.00001701
Iteration 110/1000 | Loss: 0.00001700
Iteration 111/1000 | Loss: 0.00001700
Iteration 112/1000 | Loss: 0.00001700
Iteration 113/1000 | Loss: 0.00001700
Iteration 114/1000 | Loss: 0.00001700
Iteration 115/1000 | Loss: 0.00001700
Iteration 116/1000 | Loss: 0.00001699
Iteration 117/1000 | Loss: 0.00001699
Iteration 118/1000 | Loss: 0.00001699
Iteration 119/1000 | Loss: 0.00001698
Iteration 120/1000 | Loss: 0.00001698
Iteration 121/1000 | Loss: 0.00001698
Iteration 122/1000 | Loss: 0.00001698
Iteration 123/1000 | Loss: 0.00001698
Iteration 124/1000 | Loss: 0.00001697
Iteration 125/1000 | Loss: 0.00001697
Iteration 126/1000 | Loss: 0.00001696
Iteration 127/1000 | Loss: 0.00001696
Iteration 128/1000 | Loss: 0.00001696
Iteration 129/1000 | Loss: 0.00001696
Iteration 130/1000 | Loss: 0.00001696
Iteration 131/1000 | Loss: 0.00001695
Iteration 132/1000 | Loss: 0.00001695
Iteration 133/1000 | Loss: 0.00001695
Iteration 134/1000 | Loss: 0.00001695
Iteration 135/1000 | Loss: 0.00001695
Iteration 136/1000 | Loss: 0.00001695
Iteration 137/1000 | Loss: 0.00001694
Iteration 138/1000 | Loss: 0.00001694
Iteration 139/1000 | Loss: 0.00001694
Iteration 140/1000 | Loss: 0.00001694
Iteration 141/1000 | Loss: 0.00001694
Iteration 142/1000 | Loss: 0.00001694
Iteration 143/1000 | Loss: 0.00001694
Iteration 144/1000 | Loss: 0.00001694
Iteration 145/1000 | Loss: 0.00001694
Iteration 146/1000 | Loss: 0.00001694
Iteration 147/1000 | Loss: 0.00001694
Iteration 148/1000 | Loss: 0.00001694
Iteration 149/1000 | Loss: 0.00001694
Iteration 150/1000 | Loss: 0.00001693
Iteration 151/1000 | Loss: 0.00001693
Iteration 152/1000 | Loss: 0.00001693
Iteration 153/1000 | Loss: 0.00001693
Iteration 154/1000 | Loss: 0.00001693
Iteration 155/1000 | Loss: 0.00001693
Iteration 156/1000 | Loss: 0.00001693
Iteration 157/1000 | Loss: 0.00001693
Iteration 158/1000 | Loss: 0.00001693
Iteration 159/1000 | Loss: 0.00001693
Iteration 160/1000 | Loss: 0.00001693
Iteration 161/1000 | Loss: 0.00001693
Iteration 162/1000 | Loss: 0.00001693
Iteration 163/1000 | Loss: 0.00001692
Iteration 164/1000 | Loss: 0.00001692
Iteration 165/1000 | Loss: 0.00001692
Iteration 166/1000 | Loss: 0.00001692
Iteration 167/1000 | Loss: 0.00001692
Iteration 168/1000 | Loss: 0.00001692
Iteration 169/1000 | Loss: 0.00001691
Iteration 170/1000 | Loss: 0.00001691
Iteration 171/1000 | Loss: 0.00001691
Iteration 172/1000 | Loss: 0.00001691
Iteration 173/1000 | Loss: 0.00001691
Iteration 174/1000 | Loss: 0.00001691
Iteration 175/1000 | Loss: 0.00001691
Iteration 176/1000 | Loss: 0.00001691
Iteration 177/1000 | Loss: 0.00001690
Iteration 178/1000 | Loss: 0.00001690
Iteration 179/1000 | Loss: 0.00001690
Iteration 180/1000 | Loss: 0.00001690
Iteration 181/1000 | Loss: 0.00001690
Iteration 182/1000 | Loss: 0.00001690
Iteration 183/1000 | Loss: 0.00001689
Iteration 184/1000 | Loss: 0.00001689
Iteration 185/1000 | Loss: 0.00001689
Iteration 186/1000 | Loss: 0.00001689
Iteration 187/1000 | Loss: 0.00001689
Iteration 188/1000 | Loss: 0.00001689
Iteration 189/1000 | Loss: 0.00001689
Iteration 190/1000 | Loss: 0.00001689
Iteration 191/1000 | Loss: 0.00001689
Iteration 192/1000 | Loss: 0.00001689
Iteration 193/1000 | Loss: 0.00001689
Iteration 194/1000 | Loss: 0.00001689
Iteration 195/1000 | Loss: 0.00001689
Iteration 196/1000 | Loss: 0.00001688
Iteration 197/1000 | Loss: 0.00001688
Iteration 198/1000 | Loss: 0.00001688
Iteration 199/1000 | Loss: 0.00001688
Iteration 200/1000 | Loss: 0.00001688
Iteration 201/1000 | Loss: 0.00001688
Iteration 202/1000 | Loss: 0.00001688
Iteration 203/1000 | Loss: 0.00001688
Iteration 204/1000 | Loss: 0.00001688
Iteration 205/1000 | Loss: 0.00001688
Iteration 206/1000 | Loss: 0.00001688
Iteration 207/1000 | Loss: 0.00001688
Iteration 208/1000 | Loss: 0.00001688
Iteration 209/1000 | Loss: 0.00001688
Iteration 210/1000 | Loss: 0.00001688
Iteration 211/1000 | Loss: 0.00001688
Iteration 212/1000 | Loss: 0.00001688
Iteration 213/1000 | Loss: 0.00001688
Iteration 214/1000 | Loss: 0.00001688
Iteration 215/1000 | Loss: 0.00001688
Iteration 216/1000 | Loss: 0.00001688
Iteration 217/1000 | Loss: 0.00001688
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 217. Stopping optimization.
Last 5 losses: [1.6877716916496865e-05, 1.6877716916496865e-05, 1.6877716916496865e-05, 1.6877716916496865e-05, 1.6877716916496865e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6877716916496865e-05

Optimization complete. Final v2v error: 3.420941114425659 mm

Highest mean error: 3.824976682662964 mm for frame 157

Lowest mean error: 2.925421714782715 mm for frame 239

Saving results

Total time: 48.60239863395691
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_011/1001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_011/1001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_011/1001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00626704
Iteration 2/25 | Loss: 0.00138034
Iteration 3/25 | Loss: 0.00124357
Iteration 4/25 | Loss: 0.00121772
Iteration 5/25 | Loss: 0.00120999
Iteration 6/25 | Loss: 0.00120803
Iteration 7/25 | Loss: 0.00120803
Iteration 8/25 | Loss: 0.00120803
Iteration 9/25 | Loss: 0.00120803
Iteration 10/25 | Loss: 0.00120803
Iteration 11/25 | Loss: 0.00120803
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012080261949449778, 0.0012080261949449778, 0.0012080261949449778, 0.0012080261949449778, 0.0012080261949449778]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012080261949449778

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37620616
Iteration 2/25 | Loss: 0.00130605
Iteration 3/25 | Loss: 0.00130605
Iteration 4/25 | Loss: 0.00130605
Iteration 5/25 | Loss: 0.00130605
Iteration 6/25 | Loss: 0.00130605
Iteration 7/25 | Loss: 0.00130605
Iteration 8/25 | Loss: 0.00130605
Iteration 9/25 | Loss: 0.00130605
Iteration 10/25 | Loss: 0.00130605
Iteration 11/25 | Loss: 0.00130605
Iteration 12/25 | Loss: 0.00130605
Iteration 13/25 | Loss: 0.00130605
Iteration 14/25 | Loss: 0.00130605
Iteration 15/25 | Loss: 0.00130605
Iteration 16/25 | Loss: 0.00130605
Iteration 17/25 | Loss: 0.00130605
Iteration 18/25 | Loss: 0.00130605
Iteration 19/25 | Loss: 0.00130605
Iteration 20/25 | Loss: 0.00130605
Iteration 21/25 | Loss: 0.00130605
Iteration 22/25 | Loss: 0.00130605
Iteration 23/25 | Loss: 0.00130605
Iteration 24/25 | Loss: 0.00130605
Iteration 25/25 | Loss: 0.00130605

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00130605
Iteration 2/1000 | Loss: 0.00004173
Iteration 3/1000 | Loss: 0.00002640
Iteration 4/1000 | Loss: 0.00001963
Iteration 5/1000 | Loss: 0.00001716
Iteration 6/1000 | Loss: 0.00001585
Iteration 7/1000 | Loss: 0.00001473
Iteration 8/1000 | Loss: 0.00001407
Iteration 9/1000 | Loss: 0.00001358
Iteration 10/1000 | Loss: 0.00001327
Iteration 11/1000 | Loss: 0.00001326
Iteration 12/1000 | Loss: 0.00001301
Iteration 13/1000 | Loss: 0.00001284
Iteration 14/1000 | Loss: 0.00001282
Iteration 15/1000 | Loss: 0.00001281
Iteration 16/1000 | Loss: 0.00001279
Iteration 17/1000 | Loss: 0.00001274
Iteration 18/1000 | Loss: 0.00001273
Iteration 19/1000 | Loss: 0.00001272
Iteration 20/1000 | Loss: 0.00001271
Iteration 21/1000 | Loss: 0.00001271
Iteration 22/1000 | Loss: 0.00001271
Iteration 23/1000 | Loss: 0.00001270
Iteration 24/1000 | Loss: 0.00001270
Iteration 25/1000 | Loss: 0.00001266
Iteration 26/1000 | Loss: 0.00001264
Iteration 27/1000 | Loss: 0.00001264
Iteration 28/1000 | Loss: 0.00001261
Iteration 29/1000 | Loss: 0.00001258
Iteration 30/1000 | Loss: 0.00001257
Iteration 31/1000 | Loss: 0.00001257
Iteration 32/1000 | Loss: 0.00001256
Iteration 33/1000 | Loss: 0.00001255
Iteration 34/1000 | Loss: 0.00001254
Iteration 35/1000 | Loss: 0.00001253
Iteration 36/1000 | Loss: 0.00001253
Iteration 37/1000 | Loss: 0.00001252
Iteration 38/1000 | Loss: 0.00001251
Iteration 39/1000 | Loss: 0.00001251
Iteration 40/1000 | Loss: 0.00001250
Iteration 41/1000 | Loss: 0.00001250
Iteration 42/1000 | Loss: 0.00001249
Iteration 43/1000 | Loss: 0.00001249
Iteration 44/1000 | Loss: 0.00001248
Iteration 45/1000 | Loss: 0.00001248
Iteration 46/1000 | Loss: 0.00001247
Iteration 47/1000 | Loss: 0.00001247
Iteration 48/1000 | Loss: 0.00001246
Iteration 49/1000 | Loss: 0.00001246
Iteration 50/1000 | Loss: 0.00001246
Iteration 51/1000 | Loss: 0.00001245
Iteration 52/1000 | Loss: 0.00001245
Iteration 53/1000 | Loss: 0.00001244
Iteration 54/1000 | Loss: 0.00001243
Iteration 55/1000 | Loss: 0.00001243
Iteration 56/1000 | Loss: 0.00001243
Iteration 57/1000 | Loss: 0.00001243
Iteration 58/1000 | Loss: 0.00001242
Iteration 59/1000 | Loss: 0.00001242
Iteration 60/1000 | Loss: 0.00001241
Iteration 61/1000 | Loss: 0.00001241
Iteration 62/1000 | Loss: 0.00001241
Iteration 63/1000 | Loss: 0.00001241
Iteration 64/1000 | Loss: 0.00001241
Iteration 65/1000 | Loss: 0.00001241
Iteration 66/1000 | Loss: 0.00001241
Iteration 67/1000 | Loss: 0.00001241
Iteration 68/1000 | Loss: 0.00001241
Iteration 69/1000 | Loss: 0.00001241
Iteration 70/1000 | Loss: 0.00001240
Iteration 71/1000 | Loss: 0.00001240
Iteration 72/1000 | Loss: 0.00001240
Iteration 73/1000 | Loss: 0.00001240
Iteration 74/1000 | Loss: 0.00001239
Iteration 75/1000 | Loss: 0.00001239
Iteration 76/1000 | Loss: 0.00001239
Iteration 77/1000 | Loss: 0.00001239
Iteration 78/1000 | Loss: 0.00001239
Iteration 79/1000 | Loss: 0.00001239
Iteration 80/1000 | Loss: 0.00001239
Iteration 81/1000 | Loss: 0.00001239
Iteration 82/1000 | Loss: 0.00001239
Iteration 83/1000 | Loss: 0.00001239
Iteration 84/1000 | Loss: 0.00001238
Iteration 85/1000 | Loss: 0.00001238
Iteration 86/1000 | Loss: 0.00001238
Iteration 87/1000 | Loss: 0.00001238
Iteration 88/1000 | Loss: 0.00001237
Iteration 89/1000 | Loss: 0.00001237
Iteration 90/1000 | Loss: 0.00001237
Iteration 91/1000 | Loss: 0.00001237
Iteration 92/1000 | Loss: 0.00001237
Iteration 93/1000 | Loss: 0.00001237
Iteration 94/1000 | Loss: 0.00001237
Iteration 95/1000 | Loss: 0.00001237
Iteration 96/1000 | Loss: 0.00001237
Iteration 97/1000 | Loss: 0.00001236
Iteration 98/1000 | Loss: 0.00001236
Iteration 99/1000 | Loss: 0.00001236
Iteration 100/1000 | Loss: 0.00001235
Iteration 101/1000 | Loss: 0.00001235
Iteration 102/1000 | Loss: 0.00001235
Iteration 103/1000 | Loss: 0.00001235
Iteration 104/1000 | Loss: 0.00001235
Iteration 105/1000 | Loss: 0.00001234
Iteration 106/1000 | Loss: 0.00001234
Iteration 107/1000 | Loss: 0.00001234
Iteration 108/1000 | Loss: 0.00001234
Iteration 109/1000 | Loss: 0.00001233
Iteration 110/1000 | Loss: 0.00001233
Iteration 111/1000 | Loss: 0.00001233
Iteration 112/1000 | Loss: 0.00001233
Iteration 113/1000 | Loss: 0.00001233
Iteration 114/1000 | Loss: 0.00001233
Iteration 115/1000 | Loss: 0.00001232
Iteration 116/1000 | Loss: 0.00001232
Iteration 117/1000 | Loss: 0.00001232
Iteration 118/1000 | Loss: 0.00001232
Iteration 119/1000 | Loss: 0.00001232
Iteration 120/1000 | Loss: 0.00001232
Iteration 121/1000 | Loss: 0.00001232
Iteration 122/1000 | Loss: 0.00001232
Iteration 123/1000 | Loss: 0.00001231
Iteration 124/1000 | Loss: 0.00001231
Iteration 125/1000 | Loss: 0.00001231
Iteration 126/1000 | Loss: 0.00001231
Iteration 127/1000 | Loss: 0.00001230
Iteration 128/1000 | Loss: 0.00001230
Iteration 129/1000 | Loss: 0.00001230
Iteration 130/1000 | Loss: 0.00001230
Iteration 131/1000 | Loss: 0.00001229
Iteration 132/1000 | Loss: 0.00001229
Iteration 133/1000 | Loss: 0.00001229
Iteration 134/1000 | Loss: 0.00001229
Iteration 135/1000 | Loss: 0.00001229
Iteration 136/1000 | Loss: 0.00001229
Iteration 137/1000 | Loss: 0.00001229
Iteration 138/1000 | Loss: 0.00001229
Iteration 139/1000 | Loss: 0.00001228
Iteration 140/1000 | Loss: 0.00001228
Iteration 141/1000 | Loss: 0.00001228
Iteration 142/1000 | Loss: 0.00001228
Iteration 143/1000 | Loss: 0.00001228
Iteration 144/1000 | Loss: 0.00001228
Iteration 145/1000 | Loss: 0.00001228
Iteration 146/1000 | Loss: 0.00001228
Iteration 147/1000 | Loss: 0.00001228
Iteration 148/1000 | Loss: 0.00001227
Iteration 149/1000 | Loss: 0.00001227
Iteration 150/1000 | Loss: 0.00001227
Iteration 151/1000 | Loss: 0.00001227
Iteration 152/1000 | Loss: 0.00001227
Iteration 153/1000 | Loss: 0.00001227
Iteration 154/1000 | Loss: 0.00001227
Iteration 155/1000 | Loss: 0.00001227
Iteration 156/1000 | Loss: 0.00001227
Iteration 157/1000 | Loss: 0.00001227
Iteration 158/1000 | Loss: 0.00001227
Iteration 159/1000 | Loss: 0.00001227
Iteration 160/1000 | Loss: 0.00001227
Iteration 161/1000 | Loss: 0.00001227
Iteration 162/1000 | Loss: 0.00001226
Iteration 163/1000 | Loss: 0.00001226
Iteration 164/1000 | Loss: 0.00001226
Iteration 165/1000 | Loss: 0.00001226
Iteration 166/1000 | Loss: 0.00001226
Iteration 167/1000 | Loss: 0.00001226
Iteration 168/1000 | Loss: 0.00001226
Iteration 169/1000 | Loss: 0.00001226
Iteration 170/1000 | Loss: 0.00001226
Iteration 171/1000 | Loss: 0.00001225
Iteration 172/1000 | Loss: 0.00001225
Iteration 173/1000 | Loss: 0.00001225
Iteration 174/1000 | Loss: 0.00001225
Iteration 175/1000 | Loss: 0.00001225
Iteration 176/1000 | Loss: 0.00001225
Iteration 177/1000 | Loss: 0.00001225
Iteration 178/1000 | Loss: 0.00001224
Iteration 179/1000 | Loss: 0.00001224
Iteration 180/1000 | Loss: 0.00001224
Iteration 181/1000 | Loss: 0.00001224
Iteration 182/1000 | Loss: 0.00001224
Iteration 183/1000 | Loss: 0.00001223
Iteration 184/1000 | Loss: 0.00001223
Iteration 185/1000 | Loss: 0.00001223
Iteration 186/1000 | Loss: 0.00001223
Iteration 187/1000 | Loss: 0.00001223
Iteration 188/1000 | Loss: 0.00001223
Iteration 189/1000 | Loss: 0.00001223
Iteration 190/1000 | Loss: 0.00001223
Iteration 191/1000 | Loss: 0.00001223
Iteration 192/1000 | Loss: 0.00001223
Iteration 193/1000 | Loss: 0.00001222
Iteration 194/1000 | Loss: 0.00001222
Iteration 195/1000 | Loss: 0.00001222
Iteration 196/1000 | Loss: 0.00001222
Iteration 197/1000 | Loss: 0.00001222
Iteration 198/1000 | Loss: 0.00001222
Iteration 199/1000 | Loss: 0.00001222
Iteration 200/1000 | Loss: 0.00001222
Iteration 201/1000 | Loss: 0.00001221
Iteration 202/1000 | Loss: 0.00001221
Iteration 203/1000 | Loss: 0.00001221
Iteration 204/1000 | Loss: 0.00001221
Iteration 205/1000 | Loss: 0.00001221
Iteration 206/1000 | Loss: 0.00001221
Iteration 207/1000 | Loss: 0.00001221
Iteration 208/1000 | Loss: 0.00001221
Iteration 209/1000 | Loss: 0.00001221
Iteration 210/1000 | Loss: 0.00001221
Iteration 211/1000 | Loss: 0.00001220
Iteration 212/1000 | Loss: 0.00001220
Iteration 213/1000 | Loss: 0.00001220
Iteration 214/1000 | Loss: 0.00001220
Iteration 215/1000 | Loss: 0.00001220
Iteration 216/1000 | Loss: 0.00001220
Iteration 217/1000 | Loss: 0.00001220
Iteration 218/1000 | Loss: 0.00001220
Iteration 219/1000 | Loss: 0.00001220
Iteration 220/1000 | Loss: 0.00001220
Iteration 221/1000 | Loss: 0.00001219
Iteration 222/1000 | Loss: 0.00001219
Iteration 223/1000 | Loss: 0.00001219
Iteration 224/1000 | Loss: 0.00001219
Iteration 225/1000 | Loss: 0.00001219
Iteration 226/1000 | Loss: 0.00001219
Iteration 227/1000 | Loss: 0.00001219
Iteration 228/1000 | Loss: 0.00001219
Iteration 229/1000 | Loss: 0.00001218
Iteration 230/1000 | Loss: 0.00001218
Iteration 231/1000 | Loss: 0.00001218
Iteration 232/1000 | Loss: 0.00001218
Iteration 233/1000 | Loss: 0.00001218
Iteration 234/1000 | Loss: 0.00001218
Iteration 235/1000 | Loss: 0.00001218
Iteration 236/1000 | Loss: 0.00001218
Iteration 237/1000 | Loss: 0.00001218
Iteration 238/1000 | Loss: 0.00001217
Iteration 239/1000 | Loss: 0.00001217
Iteration 240/1000 | Loss: 0.00001217
Iteration 241/1000 | Loss: 0.00001217
Iteration 242/1000 | Loss: 0.00001216
Iteration 243/1000 | Loss: 0.00001216
Iteration 244/1000 | Loss: 0.00001216
Iteration 245/1000 | Loss: 0.00001216
Iteration 246/1000 | Loss: 0.00001215
Iteration 247/1000 | Loss: 0.00001215
Iteration 248/1000 | Loss: 0.00001215
Iteration 249/1000 | Loss: 0.00001215
Iteration 250/1000 | Loss: 0.00001215
Iteration 251/1000 | Loss: 0.00001215
Iteration 252/1000 | Loss: 0.00001215
Iteration 253/1000 | Loss: 0.00001215
Iteration 254/1000 | Loss: 0.00001215
Iteration 255/1000 | Loss: 0.00001215
Iteration 256/1000 | Loss: 0.00001215
Iteration 257/1000 | Loss: 0.00001215
Iteration 258/1000 | Loss: 0.00001215
Iteration 259/1000 | Loss: 0.00001214
Iteration 260/1000 | Loss: 0.00001214
Iteration 261/1000 | Loss: 0.00001214
Iteration 262/1000 | Loss: 0.00001214
Iteration 263/1000 | Loss: 0.00001214
Iteration 264/1000 | Loss: 0.00001214
Iteration 265/1000 | Loss: 0.00001214
Iteration 266/1000 | Loss: 0.00001213
Iteration 267/1000 | Loss: 0.00001213
Iteration 268/1000 | Loss: 0.00001213
Iteration 269/1000 | Loss: 0.00001213
Iteration 270/1000 | Loss: 0.00001213
Iteration 271/1000 | Loss: 0.00001212
Iteration 272/1000 | Loss: 0.00001212
Iteration 273/1000 | Loss: 0.00001212
Iteration 274/1000 | Loss: 0.00001212
Iteration 275/1000 | Loss: 0.00001212
Iteration 276/1000 | Loss: 0.00001212
Iteration 277/1000 | Loss: 0.00001212
Iteration 278/1000 | Loss: 0.00001212
Iteration 279/1000 | Loss: 0.00001212
Iteration 280/1000 | Loss: 0.00001212
Iteration 281/1000 | Loss: 0.00001212
Iteration 282/1000 | Loss: 0.00001211
Iteration 283/1000 | Loss: 0.00001211
Iteration 284/1000 | Loss: 0.00001211
Iteration 285/1000 | Loss: 0.00001211
Iteration 286/1000 | Loss: 0.00001211
Iteration 287/1000 | Loss: 0.00001211
Iteration 288/1000 | Loss: 0.00001211
Iteration 289/1000 | Loss: 0.00001211
Iteration 290/1000 | Loss: 0.00001211
Iteration 291/1000 | Loss: 0.00001211
Iteration 292/1000 | Loss: 0.00001211
Iteration 293/1000 | Loss: 0.00001211
Iteration 294/1000 | Loss: 0.00001211
Iteration 295/1000 | Loss: 0.00001211
Iteration 296/1000 | Loss: 0.00001211
Iteration 297/1000 | Loss: 0.00001211
Iteration 298/1000 | Loss: 0.00001210
Iteration 299/1000 | Loss: 0.00001210
Iteration 300/1000 | Loss: 0.00001210
Iteration 301/1000 | Loss: 0.00001210
Iteration 302/1000 | Loss: 0.00001210
Iteration 303/1000 | Loss: 0.00001210
Iteration 304/1000 | Loss: 0.00001210
Iteration 305/1000 | Loss: 0.00001210
Iteration 306/1000 | Loss: 0.00001210
Iteration 307/1000 | Loss: 0.00001210
Iteration 308/1000 | Loss: 0.00001210
Iteration 309/1000 | Loss: 0.00001210
Iteration 310/1000 | Loss: 0.00001210
Iteration 311/1000 | Loss: 0.00001210
Iteration 312/1000 | Loss: 0.00001210
Iteration 313/1000 | Loss: 0.00001210
Iteration 314/1000 | Loss: 0.00001210
Iteration 315/1000 | Loss: 0.00001210
Iteration 316/1000 | Loss: 0.00001210
Iteration 317/1000 | Loss: 0.00001210
Iteration 318/1000 | Loss: 0.00001210
Iteration 319/1000 | Loss: 0.00001210
Iteration 320/1000 | Loss: 0.00001210
Iteration 321/1000 | Loss: 0.00001210
Iteration 322/1000 | Loss: 0.00001210
Iteration 323/1000 | Loss: 0.00001210
Iteration 324/1000 | Loss: 0.00001210
Iteration 325/1000 | Loss: 0.00001210
Iteration 326/1000 | Loss: 0.00001210
Iteration 327/1000 | Loss: 0.00001210
Iteration 328/1000 | Loss: 0.00001210
Iteration 329/1000 | Loss: 0.00001210
Iteration 330/1000 | Loss: 0.00001210
Iteration 331/1000 | Loss: 0.00001210
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 331. Stopping optimization.
Last 5 losses: [1.21009243230219e-05, 1.21009243230219e-05, 1.21009243230219e-05, 1.21009243230219e-05, 1.21009243230219e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.21009243230219e-05

Optimization complete. Final v2v error: 2.9486618041992188 mm

Highest mean error: 3.587514877319336 mm for frame 74

Lowest mean error: 2.5610735416412354 mm for frame 125

Saving results

Total time: 49.64734601974487
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_011/1002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_011/1002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_011/1002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00487563
Iteration 2/25 | Loss: 0.00144271
Iteration 3/25 | Loss: 0.00129874
Iteration 4/25 | Loss: 0.00128443
Iteration 5/25 | Loss: 0.00128168
Iteration 6/25 | Loss: 0.00128117
Iteration 7/25 | Loss: 0.00128117
Iteration 8/25 | Loss: 0.00128117
Iteration 9/25 | Loss: 0.00128117
Iteration 10/25 | Loss: 0.00128117
Iteration 11/25 | Loss: 0.00128117
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012811683118343353, 0.0012811683118343353, 0.0012811683118343353, 0.0012811683118343353, 0.0012811683118343353]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012811683118343353

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45934153
Iteration 2/25 | Loss: 0.00108336
Iteration 3/25 | Loss: 0.00108332
Iteration 4/25 | Loss: 0.00108332
Iteration 5/25 | Loss: 0.00108332
Iteration 6/25 | Loss: 0.00108332
Iteration 7/25 | Loss: 0.00108332
Iteration 8/25 | Loss: 0.00108332
Iteration 9/25 | Loss: 0.00108332
Iteration 10/25 | Loss: 0.00108332
Iteration 11/25 | Loss: 0.00108332
Iteration 12/25 | Loss: 0.00108332
Iteration 13/25 | Loss: 0.00108332
Iteration 14/25 | Loss: 0.00108332
Iteration 15/25 | Loss: 0.00108332
Iteration 16/25 | Loss: 0.00108332
Iteration 17/25 | Loss: 0.00108332
Iteration 18/25 | Loss: 0.00108332
Iteration 19/25 | Loss: 0.00108332
Iteration 20/25 | Loss: 0.00108332
Iteration 21/25 | Loss: 0.00108332
Iteration 22/25 | Loss: 0.00108332
Iteration 23/25 | Loss: 0.00108332
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0010833193082362413, 0.0010833193082362413, 0.0010833193082362413, 0.0010833193082362413, 0.0010833193082362413]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010833193082362413

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00108332
Iteration 2/1000 | Loss: 0.00004490
Iteration 3/1000 | Loss: 0.00003008
Iteration 4/1000 | Loss: 0.00002219
Iteration 5/1000 | Loss: 0.00002046
Iteration 6/1000 | Loss: 0.00001963
Iteration 7/1000 | Loss: 0.00001865
Iteration 8/1000 | Loss: 0.00001808
Iteration 9/1000 | Loss: 0.00001752
Iteration 10/1000 | Loss: 0.00001708
Iteration 11/1000 | Loss: 0.00001671
Iteration 12/1000 | Loss: 0.00001647
Iteration 13/1000 | Loss: 0.00001628
Iteration 14/1000 | Loss: 0.00001623
Iteration 15/1000 | Loss: 0.00001619
Iteration 16/1000 | Loss: 0.00001615
Iteration 17/1000 | Loss: 0.00001613
Iteration 18/1000 | Loss: 0.00001610
Iteration 19/1000 | Loss: 0.00001610
Iteration 20/1000 | Loss: 0.00001609
Iteration 21/1000 | Loss: 0.00001604
Iteration 22/1000 | Loss: 0.00001603
Iteration 23/1000 | Loss: 0.00001602
Iteration 24/1000 | Loss: 0.00001602
Iteration 25/1000 | Loss: 0.00001601
Iteration 26/1000 | Loss: 0.00001598
Iteration 27/1000 | Loss: 0.00001598
Iteration 28/1000 | Loss: 0.00001596
Iteration 29/1000 | Loss: 0.00001596
Iteration 30/1000 | Loss: 0.00001595
Iteration 31/1000 | Loss: 0.00001595
Iteration 32/1000 | Loss: 0.00001595
Iteration 33/1000 | Loss: 0.00001595
Iteration 34/1000 | Loss: 0.00001595
Iteration 35/1000 | Loss: 0.00001595
Iteration 36/1000 | Loss: 0.00001594
Iteration 37/1000 | Loss: 0.00001594
Iteration 38/1000 | Loss: 0.00001593
Iteration 39/1000 | Loss: 0.00001593
Iteration 40/1000 | Loss: 0.00001592
Iteration 41/1000 | Loss: 0.00001592
Iteration 42/1000 | Loss: 0.00001592
Iteration 43/1000 | Loss: 0.00001592
Iteration 44/1000 | Loss: 0.00001591
Iteration 45/1000 | Loss: 0.00001590
Iteration 46/1000 | Loss: 0.00001590
Iteration 47/1000 | Loss: 0.00001590
Iteration 48/1000 | Loss: 0.00001590
Iteration 49/1000 | Loss: 0.00001590
Iteration 50/1000 | Loss: 0.00001590
Iteration 51/1000 | Loss: 0.00001590
Iteration 52/1000 | Loss: 0.00001590
Iteration 53/1000 | Loss: 0.00001590
Iteration 54/1000 | Loss: 0.00001590
Iteration 55/1000 | Loss: 0.00001590
Iteration 56/1000 | Loss: 0.00001590
Iteration 57/1000 | Loss: 0.00001589
Iteration 58/1000 | Loss: 0.00001589
Iteration 59/1000 | Loss: 0.00001589
Iteration 60/1000 | Loss: 0.00001589
Iteration 61/1000 | Loss: 0.00001588
Iteration 62/1000 | Loss: 0.00001588
Iteration 63/1000 | Loss: 0.00001588
Iteration 64/1000 | Loss: 0.00001588
Iteration 65/1000 | Loss: 0.00001587
Iteration 66/1000 | Loss: 0.00001587
Iteration 67/1000 | Loss: 0.00001587
Iteration 68/1000 | Loss: 0.00001587
Iteration 69/1000 | Loss: 0.00001587
Iteration 70/1000 | Loss: 0.00001587
Iteration 71/1000 | Loss: 0.00001587
Iteration 72/1000 | Loss: 0.00001587
Iteration 73/1000 | Loss: 0.00001587
Iteration 74/1000 | Loss: 0.00001587
Iteration 75/1000 | Loss: 0.00001587
Iteration 76/1000 | Loss: 0.00001587
Iteration 77/1000 | Loss: 0.00001587
Iteration 78/1000 | Loss: 0.00001587
Iteration 79/1000 | Loss: 0.00001587
Iteration 80/1000 | Loss: 0.00001587
Iteration 81/1000 | Loss: 0.00001586
Iteration 82/1000 | Loss: 0.00001586
Iteration 83/1000 | Loss: 0.00001586
Iteration 84/1000 | Loss: 0.00001586
Iteration 85/1000 | Loss: 0.00001586
Iteration 86/1000 | Loss: 0.00001585
Iteration 87/1000 | Loss: 0.00001585
Iteration 88/1000 | Loss: 0.00001585
Iteration 89/1000 | Loss: 0.00001585
Iteration 90/1000 | Loss: 0.00001585
Iteration 91/1000 | Loss: 0.00001585
Iteration 92/1000 | Loss: 0.00001585
Iteration 93/1000 | Loss: 0.00001584
Iteration 94/1000 | Loss: 0.00001584
Iteration 95/1000 | Loss: 0.00001584
Iteration 96/1000 | Loss: 0.00001584
Iteration 97/1000 | Loss: 0.00001584
Iteration 98/1000 | Loss: 0.00001584
Iteration 99/1000 | Loss: 0.00001583
Iteration 100/1000 | Loss: 0.00001583
Iteration 101/1000 | Loss: 0.00001583
Iteration 102/1000 | Loss: 0.00001583
Iteration 103/1000 | Loss: 0.00001583
Iteration 104/1000 | Loss: 0.00001583
Iteration 105/1000 | Loss: 0.00001583
Iteration 106/1000 | Loss: 0.00001582
Iteration 107/1000 | Loss: 0.00001582
Iteration 108/1000 | Loss: 0.00001582
Iteration 109/1000 | Loss: 0.00001582
Iteration 110/1000 | Loss: 0.00001582
Iteration 111/1000 | Loss: 0.00001582
Iteration 112/1000 | Loss: 0.00001582
Iteration 113/1000 | Loss: 0.00001582
Iteration 114/1000 | Loss: 0.00001582
Iteration 115/1000 | Loss: 0.00001581
Iteration 116/1000 | Loss: 0.00001581
Iteration 117/1000 | Loss: 0.00001581
Iteration 118/1000 | Loss: 0.00001581
Iteration 119/1000 | Loss: 0.00001581
Iteration 120/1000 | Loss: 0.00001581
Iteration 121/1000 | Loss: 0.00001581
Iteration 122/1000 | Loss: 0.00001581
Iteration 123/1000 | Loss: 0.00001581
Iteration 124/1000 | Loss: 0.00001581
Iteration 125/1000 | Loss: 0.00001581
Iteration 126/1000 | Loss: 0.00001581
Iteration 127/1000 | Loss: 0.00001580
Iteration 128/1000 | Loss: 0.00001580
Iteration 129/1000 | Loss: 0.00001580
Iteration 130/1000 | Loss: 0.00001580
Iteration 131/1000 | Loss: 0.00001580
Iteration 132/1000 | Loss: 0.00001579
Iteration 133/1000 | Loss: 0.00001579
Iteration 134/1000 | Loss: 0.00001579
Iteration 135/1000 | Loss: 0.00001579
Iteration 136/1000 | Loss: 0.00001579
Iteration 137/1000 | Loss: 0.00001579
Iteration 138/1000 | Loss: 0.00001579
Iteration 139/1000 | Loss: 0.00001579
Iteration 140/1000 | Loss: 0.00001579
Iteration 141/1000 | Loss: 0.00001579
Iteration 142/1000 | Loss: 0.00001579
Iteration 143/1000 | Loss: 0.00001579
Iteration 144/1000 | Loss: 0.00001579
Iteration 145/1000 | Loss: 0.00001579
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 145. Stopping optimization.
Last 5 losses: [1.5785039067850448e-05, 1.5785039067850448e-05, 1.5785039067850448e-05, 1.5785039067850448e-05, 1.5785039067850448e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5785039067850448e-05

Optimization complete. Final v2v error: 3.3330657482147217 mm

Highest mean error: 3.9133434295654297 mm for frame 69

Lowest mean error: 2.807250738143921 mm for frame 93

Saving results

Total time: 36.778631925582886
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_011/1031/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_011/1031.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_011/1031
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00880983
Iteration 2/25 | Loss: 0.00134972
Iteration 3/25 | Loss: 0.00124306
Iteration 4/25 | Loss: 0.00121665
Iteration 5/25 | Loss: 0.00121014
Iteration 6/25 | Loss: 0.00120871
Iteration 7/25 | Loss: 0.00120871
Iteration 8/25 | Loss: 0.00120871
Iteration 9/25 | Loss: 0.00120871
Iteration 10/25 | Loss: 0.00120871
Iteration 11/25 | Loss: 0.00120871
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012087138602510095, 0.0012087138602510095, 0.0012087138602510095, 0.0012087138602510095, 0.0012087138602510095]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012087138602510095

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.28980410
Iteration 2/25 | Loss: 0.00109444
Iteration 3/25 | Loss: 0.00109444
Iteration 4/25 | Loss: 0.00109444
Iteration 5/25 | Loss: 0.00109444
Iteration 6/25 | Loss: 0.00109444
Iteration 7/25 | Loss: 0.00109444
Iteration 8/25 | Loss: 0.00109444
Iteration 9/25 | Loss: 0.00109444
Iteration 10/25 | Loss: 0.00109444
Iteration 11/25 | Loss: 0.00109444
Iteration 12/25 | Loss: 0.00109444
Iteration 13/25 | Loss: 0.00109444
Iteration 14/25 | Loss: 0.00109444
Iteration 15/25 | Loss: 0.00109444
Iteration 16/25 | Loss: 0.00109444
Iteration 17/25 | Loss: 0.00109444
Iteration 18/25 | Loss: 0.00109444
Iteration 19/25 | Loss: 0.00109444
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0010944384848698974, 0.0010944384848698974, 0.0010944384848698974, 0.0010944384848698974, 0.0010944384848698974]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010944384848698974

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00109444
Iteration 2/1000 | Loss: 0.00003733
Iteration 3/1000 | Loss: 0.00002739
Iteration 4/1000 | Loss: 0.00002362
Iteration 5/1000 | Loss: 0.00002144
Iteration 6/1000 | Loss: 0.00001983
Iteration 7/1000 | Loss: 0.00001862
Iteration 8/1000 | Loss: 0.00001780
Iteration 9/1000 | Loss: 0.00001734
Iteration 10/1000 | Loss: 0.00001710
Iteration 11/1000 | Loss: 0.00001689
Iteration 12/1000 | Loss: 0.00001686
Iteration 13/1000 | Loss: 0.00001684
Iteration 14/1000 | Loss: 0.00001683
Iteration 15/1000 | Loss: 0.00001682
Iteration 16/1000 | Loss: 0.00001679
Iteration 17/1000 | Loss: 0.00001673
Iteration 18/1000 | Loss: 0.00001673
Iteration 19/1000 | Loss: 0.00001668
Iteration 20/1000 | Loss: 0.00001666
Iteration 21/1000 | Loss: 0.00001665
Iteration 22/1000 | Loss: 0.00001660
Iteration 23/1000 | Loss: 0.00001660
Iteration 24/1000 | Loss: 0.00001658
Iteration 25/1000 | Loss: 0.00001658
Iteration 26/1000 | Loss: 0.00001658
Iteration 27/1000 | Loss: 0.00001657
Iteration 28/1000 | Loss: 0.00001657
Iteration 29/1000 | Loss: 0.00001656
Iteration 30/1000 | Loss: 0.00001656
Iteration 31/1000 | Loss: 0.00001655
Iteration 32/1000 | Loss: 0.00001655
Iteration 33/1000 | Loss: 0.00001655
Iteration 34/1000 | Loss: 0.00001654
Iteration 35/1000 | Loss: 0.00001654
Iteration 36/1000 | Loss: 0.00001653
Iteration 37/1000 | Loss: 0.00001653
Iteration 38/1000 | Loss: 0.00001653
Iteration 39/1000 | Loss: 0.00001652
Iteration 40/1000 | Loss: 0.00001652
Iteration 41/1000 | Loss: 0.00001648
Iteration 42/1000 | Loss: 0.00001648
Iteration 43/1000 | Loss: 0.00001647
Iteration 44/1000 | Loss: 0.00001646
Iteration 45/1000 | Loss: 0.00001646
Iteration 46/1000 | Loss: 0.00001646
Iteration 47/1000 | Loss: 0.00001645
Iteration 48/1000 | Loss: 0.00001645
Iteration 49/1000 | Loss: 0.00001644
Iteration 50/1000 | Loss: 0.00001644
Iteration 51/1000 | Loss: 0.00001644
Iteration 52/1000 | Loss: 0.00001643
Iteration 53/1000 | Loss: 0.00001643
Iteration 54/1000 | Loss: 0.00001643
Iteration 55/1000 | Loss: 0.00001643
Iteration 56/1000 | Loss: 0.00001642
Iteration 57/1000 | Loss: 0.00001642
Iteration 58/1000 | Loss: 0.00001641
Iteration 59/1000 | Loss: 0.00001641
Iteration 60/1000 | Loss: 0.00001641
Iteration 61/1000 | Loss: 0.00001641
Iteration 62/1000 | Loss: 0.00001641
Iteration 63/1000 | Loss: 0.00001641
Iteration 64/1000 | Loss: 0.00001641
Iteration 65/1000 | Loss: 0.00001641
Iteration 66/1000 | Loss: 0.00001641
Iteration 67/1000 | Loss: 0.00001641
Iteration 68/1000 | Loss: 0.00001641
Iteration 69/1000 | Loss: 0.00001641
Iteration 70/1000 | Loss: 0.00001640
Iteration 71/1000 | Loss: 0.00001639
Iteration 72/1000 | Loss: 0.00001639
Iteration 73/1000 | Loss: 0.00001638
Iteration 74/1000 | Loss: 0.00001638
Iteration 75/1000 | Loss: 0.00001638
Iteration 76/1000 | Loss: 0.00001638
Iteration 77/1000 | Loss: 0.00001637
Iteration 78/1000 | Loss: 0.00001637
Iteration 79/1000 | Loss: 0.00001637
Iteration 80/1000 | Loss: 0.00001637
Iteration 81/1000 | Loss: 0.00001636
Iteration 82/1000 | Loss: 0.00001636
Iteration 83/1000 | Loss: 0.00001636
Iteration 84/1000 | Loss: 0.00001635
Iteration 85/1000 | Loss: 0.00001635
Iteration 86/1000 | Loss: 0.00001635
Iteration 87/1000 | Loss: 0.00001634
Iteration 88/1000 | Loss: 0.00001634
Iteration 89/1000 | Loss: 0.00001634
Iteration 90/1000 | Loss: 0.00001634
Iteration 91/1000 | Loss: 0.00001634
Iteration 92/1000 | Loss: 0.00001633
Iteration 93/1000 | Loss: 0.00001633
Iteration 94/1000 | Loss: 0.00001633
Iteration 95/1000 | Loss: 0.00001633
Iteration 96/1000 | Loss: 0.00001633
Iteration 97/1000 | Loss: 0.00001633
Iteration 98/1000 | Loss: 0.00001633
Iteration 99/1000 | Loss: 0.00001633
Iteration 100/1000 | Loss: 0.00001633
Iteration 101/1000 | Loss: 0.00001633
Iteration 102/1000 | Loss: 0.00001633
Iteration 103/1000 | Loss: 0.00001633
Iteration 104/1000 | Loss: 0.00001633
Iteration 105/1000 | Loss: 0.00001632
Iteration 106/1000 | Loss: 0.00001632
Iteration 107/1000 | Loss: 0.00001632
Iteration 108/1000 | Loss: 0.00001632
Iteration 109/1000 | Loss: 0.00001632
Iteration 110/1000 | Loss: 0.00001632
Iteration 111/1000 | Loss: 0.00001632
Iteration 112/1000 | Loss: 0.00001632
Iteration 113/1000 | Loss: 0.00001632
Iteration 114/1000 | Loss: 0.00001632
Iteration 115/1000 | Loss: 0.00001632
Iteration 116/1000 | Loss: 0.00001632
Iteration 117/1000 | Loss: 0.00001632
Iteration 118/1000 | Loss: 0.00001632
Iteration 119/1000 | Loss: 0.00001632
Iteration 120/1000 | Loss: 0.00001631
Iteration 121/1000 | Loss: 0.00001631
Iteration 122/1000 | Loss: 0.00001631
Iteration 123/1000 | Loss: 0.00001631
Iteration 124/1000 | Loss: 0.00001631
Iteration 125/1000 | Loss: 0.00001631
Iteration 126/1000 | Loss: 0.00001631
Iteration 127/1000 | Loss: 0.00001631
Iteration 128/1000 | Loss: 0.00001631
Iteration 129/1000 | Loss: 0.00001631
Iteration 130/1000 | Loss: 0.00001630
Iteration 131/1000 | Loss: 0.00001630
Iteration 132/1000 | Loss: 0.00001630
Iteration 133/1000 | Loss: 0.00001630
Iteration 134/1000 | Loss: 0.00001630
Iteration 135/1000 | Loss: 0.00001630
Iteration 136/1000 | Loss: 0.00001630
Iteration 137/1000 | Loss: 0.00001629
Iteration 138/1000 | Loss: 0.00001629
Iteration 139/1000 | Loss: 0.00001629
Iteration 140/1000 | Loss: 0.00001629
Iteration 141/1000 | Loss: 0.00001629
Iteration 142/1000 | Loss: 0.00001629
Iteration 143/1000 | Loss: 0.00001629
Iteration 144/1000 | Loss: 0.00001629
Iteration 145/1000 | Loss: 0.00001629
Iteration 146/1000 | Loss: 0.00001629
Iteration 147/1000 | Loss: 0.00001629
Iteration 148/1000 | Loss: 0.00001629
Iteration 149/1000 | Loss: 0.00001629
Iteration 150/1000 | Loss: 0.00001629
Iteration 151/1000 | Loss: 0.00001629
Iteration 152/1000 | Loss: 0.00001629
Iteration 153/1000 | Loss: 0.00001629
Iteration 154/1000 | Loss: 0.00001629
Iteration 155/1000 | Loss: 0.00001629
Iteration 156/1000 | Loss: 0.00001629
Iteration 157/1000 | Loss: 0.00001629
Iteration 158/1000 | Loss: 0.00001629
Iteration 159/1000 | Loss: 0.00001629
Iteration 160/1000 | Loss: 0.00001629
Iteration 161/1000 | Loss: 0.00001629
Iteration 162/1000 | Loss: 0.00001629
Iteration 163/1000 | Loss: 0.00001629
Iteration 164/1000 | Loss: 0.00001629
Iteration 165/1000 | Loss: 0.00001629
Iteration 166/1000 | Loss: 0.00001629
Iteration 167/1000 | Loss: 0.00001629
Iteration 168/1000 | Loss: 0.00001629
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 168. Stopping optimization.
Last 5 losses: [1.628723657631781e-05, 1.628723657631781e-05, 1.628723657631781e-05, 1.628723657631781e-05, 1.628723657631781e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.628723657631781e-05

Optimization complete. Final v2v error: 3.457881212234497 mm

Highest mean error: 3.9173760414123535 mm for frame 167

Lowest mean error: 2.921708106994629 mm for frame 237

Saving results

Total time: 42.2224223613739
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_011/1039/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_011/1039.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_011/1039
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00969732
Iteration 2/25 | Loss: 0.00198873
Iteration 3/25 | Loss: 0.00137544
Iteration 4/25 | Loss: 0.00129647
Iteration 5/25 | Loss: 0.00128670
Iteration 6/25 | Loss: 0.00128593
Iteration 7/25 | Loss: 0.00128593
Iteration 8/25 | Loss: 0.00128593
Iteration 9/25 | Loss: 0.00128593
Iteration 10/25 | Loss: 0.00128593
Iteration 11/25 | Loss: 0.00128593
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012859258567914367, 0.0012859258567914367, 0.0012859258567914367, 0.0012859258567914367, 0.0012859258567914367]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012859258567914367

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34380043
Iteration 2/25 | Loss: 0.00082104
Iteration 3/25 | Loss: 0.00082104
Iteration 4/25 | Loss: 0.00082104
Iteration 5/25 | Loss: 0.00082104
Iteration 6/25 | Loss: 0.00082104
Iteration 7/25 | Loss: 0.00082104
Iteration 8/25 | Loss: 0.00082104
Iteration 9/25 | Loss: 0.00082104
Iteration 10/25 | Loss: 0.00082104
Iteration 11/25 | Loss: 0.00082104
Iteration 12/25 | Loss: 0.00082104
Iteration 13/25 | Loss: 0.00082104
Iteration 14/25 | Loss: 0.00082104
Iteration 15/25 | Loss: 0.00082104
Iteration 16/25 | Loss: 0.00082104
Iteration 17/25 | Loss: 0.00082104
Iteration 18/25 | Loss: 0.00082104
Iteration 19/25 | Loss: 0.00082104
Iteration 20/25 | Loss: 0.00082104
Iteration 21/25 | Loss: 0.00082104
Iteration 22/25 | Loss: 0.00082104
Iteration 23/25 | Loss: 0.00082104
Iteration 24/25 | Loss: 0.00082104
Iteration 25/25 | Loss: 0.00082104

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00082104
Iteration 2/1000 | Loss: 0.00002953
Iteration 3/1000 | Loss: 0.00002403
Iteration 4/1000 | Loss: 0.00002304
Iteration 5/1000 | Loss: 0.00002231
Iteration 6/1000 | Loss: 0.00002183
Iteration 7/1000 | Loss: 0.00002145
Iteration 8/1000 | Loss: 0.00002111
Iteration 9/1000 | Loss: 0.00002081
Iteration 10/1000 | Loss: 0.00002061
Iteration 11/1000 | Loss: 0.00002051
Iteration 12/1000 | Loss: 0.00002051
Iteration 13/1000 | Loss: 0.00002050
Iteration 14/1000 | Loss: 0.00002042
Iteration 15/1000 | Loss: 0.00002037
Iteration 16/1000 | Loss: 0.00002028
Iteration 17/1000 | Loss: 0.00002026
Iteration 18/1000 | Loss: 0.00002026
Iteration 19/1000 | Loss: 0.00002026
Iteration 20/1000 | Loss: 0.00002026
Iteration 21/1000 | Loss: 0.00002025
Iteration 22/1000 | Loss: 0.00002023
Iteration 23/1000 | Loss: 0.00002021
Iteration 24/1000 | Loss: 0.00002021
Iteration 25/1000 | Loss: 0.00002021
Iteration 26/1000 | Loss: 0.00002020
Iteration 27/1000 | Loss: 0.00002020
Iteration 28/1000 | Loss: 0.00002017
Iteration 29/1000 | Loss: 0.00002016
Iteration 30/1000 | Loss: 0.00002016
Iteration 31/1000 | Loss: 0.00002016
Iteration 32/1000 | Loss: 0.00002016
Iteration 33/1000 | Loss: 0.00002016
Iteration 34/1000 | Loss: 0.00002015
Iteration 35/1000 | Loss: 0.00002015
Iteration 36/1000 | Loss: 0.00002015
Iteration 37/1000 | Loss: 0.00002015
Iteration 38/1000 | Loss: 0.00002014
Iteration 39/1000 | Loss: 0.00002013
Iteration 40/1000 | Loss: 0.00002013
Iteration 41/1000 | Loss: 0.00002013
Iteration 42/1000 | Loss: 0.00002013
Iteration 43/1000 | Loss: 0.00002012
Iteration 44/1000 | Loss: 0.00002012
Iteration 45/1000 | Loss: 0.00002012
Iteration 46/1000 | Loss: 0.00002012
Iteration 47/1000 | Loss: 0.00002012
Iteration 48/1000 | Loss: 0.00002012
Iteration 49/1000 | Loss: 0.00002012
Iteration 50/1000 | Loss: 0.00002011
Iteration 51/1000 | Loss: 0.00002011
Iteration 52/1000 | Loss: 0.00002011
Iteration 53/1000 | Loss: 0.00002011
Iteration 54/1000 | Loss: 0.00002011
Iteration 55/1000 | Loss: 0.00002011
Iteration 56/1000 | Loss: 0.00002011
Iteration 57/1000 | Loss: 0.00002009
Iteration 58/1000 | Loss: 0.00002009
Iteration 59/1000 | Loss: 0.00002009
Iteration 60/1000 | Loss: 0.00002009
Iteration 61/1000 | Loss: 0.00002009
Iteration 62/1000 | Loss: 0.00002009
Iteration 63/1000 | Loss: 0.00002008
Iteration 64/1000 | Loss: 0.00002008
Iteration 65/1000 | Loss: 0.00002008
Iteration 66/1000 | Loss: 0.00002007
Iteration 67/1000 | Loss: 0.00002007
Iteration 68/1000 | Loss: 0.00002007
Iteration 69/1000 | Loss: 0.00002007
Iteration 70/1000 | Loss: 0.00002007
Iteration 71/1000 | Loss: 0.00002007
Iteration 72/1000 | Loss: 0.00002007
Iteration 73/1000 | Loss: 0.00002006
Iteration 74/1000 | Loss: 0.00002006
Iteration 75/1000 | Loss: 0.00002006
Iteration 76/1000 | Loss: 0.00002006
Iteration 77/1000 | Loss: 0.00002006
Iteration 78/1000 | Loss: 0.00002006
Iteration 79/1000 | Loss: 0.00002006
Iteration 80/1000 | Loss: 0.00002006
Iteration 81/1000 | Loss: 0.00002006
Iteration 82/1000 | Loss: 0.00002006
Iteration 83/1000 | Loss: 0.00002006
Iteration 84/1000 | Loss: 0.00002005
Iteration 85/1000 | Loss: 0.00002005
Iteration 86/1000 | Loss: 0.00002005
Iteration 87/1000 | Loss: 0.00002005
Iteration 88/1000 | Loss: 0.00002005
Iteration 89/1000 | Loss: 0.00002005
Iteration 90/1000 | Loss: 0.00002005
Iteration 91/1000 | Loss: 0.00002005
Iteration 92/1000 | Loss: 0.00002004
Iteration 93/1000 | Loss: 0.00002004
Iteration 94/1000 | Loss: 0.00002004
Iteration 95/1000 | Loss: 0.00002003
Iteration 96/1000 | Loss: 0.00002003
Iteration 97/1000 | Loss: 0.00002003
Iteration 98/1000 | Loss: 0.00002003
Iteration 99/1000 | Loss: 0.00002003
Iteration 100/1000 | Loss: 0.00002002
Iteration 101/1000 | Loss: 0.00002002
Iteration 102/1000 | Loss: 0.00002002
Iteration 103/1000 | Loss: 0.00002002
Iteration 104/1000 | Loss: 0.00002002
Iteration 105/1000 | Loss: 0.00002002
Iteration 106/1000 | Loss: 0.00002002
Iteration 107/1000 | Loss: 0.00002002
Iteration 108/1000 | Loss: 0.00002002
Iteration 109/1000 | Loss: 0.00002001
Iteration 110/1000 | Loss: 0.00002001
Iteration 111/1000 | Loss: 0.00002001
Iteration 112/1000 | Loss: 0.00002001
Iteration 113/1000 | Loss: 0.00002001
Iteration 114/1000 | Loss: 0.00002001
Iteration 115/1000 | Loss: 0.00002001
Iteration 116/1000 | Loss: 0.00002001
Iteration 117/1000 | Loss: 0.00002001
Iteration 118/1000 | Loss: 0.00002001
Iteration 119/1000 | Loss: 0.00002001
Iteration 120/1000 | Loss: 0.00002001
Iteration 121/1000 | Loss: 0.00002001
Iteration 122/1000 | Loss: 0.00002001
Iteration 123/1000 | Loss: 0.00002001
Iteration 124/1000 | Loss: 0.00002001
Iteration 125/1000 | Loss: 0.00002001
Iteration 126/1000 | Loss: 0.00002001
Iteration 127/1000 | Loss: 0.00002001
Iteration 128/1000 | Loss: 0.00002001
Iteration 129/1000 | Loss: 0.00002001
Iteration 130/1000 | Loss: 0.00002001
Iteration 131/1000 | Loss: 0.00002001
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 131. Stopping optimization.
Last 5 losses: [2.0005041733384132e-05, 2.0005041733384132e-05, 2.0005041733384132e-05, 2.0005041733384132e-05, 2.0005041733384132e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0005041733384132e-05

Optimization complete. Final v2v error: 3.7377097606658936 mm

Highest mean error: 3.7827208042144775 mm for frame 174

Lowest mean error: 3.46069598197937 mm for frame 0

Saving results

Total time: 35.7054226398468
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_011/1092/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_011/1092.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_011/1092
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00966828
Iteration 2/25 | Loss: 0.00966828
Iteration 3/25 | Loss: 0.00966828
Iteration 4/25 | Loss: 0.00966828
Iteration 5/25 | Loss: 0.00966828
Iteration 6/25 | Loss: 0.00966828
Iteration 7/25 | Loss: 0.00966827
Iteration 8/25 | Loss: 0.00966827
Iteration 9/25 | Loss: 0.00966827
Iteration 10/25 | Loss: 0.00966827
Iteration 11/25 | Loss: 0.00966827
Iteration 12/25 | Loss: 0.00966827
Iteration 13/25 | Loss: 0.00966827
Iteration 14/25 | Loss: 0.00966826
Iteration 15/25 | Loss: 0.00966826
Iteration 16/25 | Loss: 0.00966826
Iteration 17/25 | Loss: 0.00966826
Iteration 18/25 | Loss: 0.00966826
Iteration 19/25 | Loss: 0.00966826
Iteration 20/25 | Loss: 0.00966826
Iteration 21/25 | Loss: 0.00966826
Iteration 22/25 | Loss: 0.00966826
Iteration 23/25 | Loss: 0.00966826
Iteration 24/25 | Loss: 0.00966825
Iteration 25/25 | Loss: 0.00966825

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.50885701
Iteration 2/25 | Loss: 0.18779869
Iteration 3/25 | Loss: 0.18779352
Iteration 4/25 | Loss: 0.18779348
Iteration 5/25 | Loss: 0.18779346
Iteration 6/25 | Loss: 0.18779345
Iteration 7/25 | Loss: 0.18779345
Iteration 8/25 | Loss: 0.18779345
Iteration 9/25 | Loss: 0.18779345
Iteration 10/25 | Loss: 0.18779345
Iteration 11/25 | Loss: 0.18779343
Iteration 12/25 | Loss: 0.18779343
Iteration 13/25 | Loss: 0.18779343
Iteration 14/25 | Loss: 0.18779345
Iteration 15/25 | Loss: 0.18779343
Iteration 16/25 | Loss: 0.18779340
Iteration 17/25 | Loss: 0.18779343
Iteration 18/25 | Loss: 0.18779343
Iteration 19/25 | Loss: 0.18779343
Iteration 20/25 | Loss: 0.18779343
Iteration 21/25 | Loss: 0.18779343
Iteration 22/25 | Loss: 0.18779343
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.18779343366622925, 0.18779343366622925, 0.18779343366622925, 0.18779343366622925, 0.18779343366622925]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.18779343366622925

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.18779343
Iteration 2/1000 | Loss: 0.01178628
Iteration 3/1000 | Loss: 0.00468961
Iteration 4/1000 | Loss: 0.00652207
Iteration 5/1000 | Loss: 0.00221119
Iteration 6/1000 | Loss: 0.00112193
Iteration 7/1000 | Loss: 0.00097402
Iteration 8/1000 | Loss: 0.00073604
Iteration 9/1000 | Loss: 0.00076707
Iteration 10/1000 | Loss: 0.00424997
Iteration 11/1000 | Loss: 0.00208685
Iteration 12/1000 | Loss: 0.00191090
Iteration 13/1000 | Loss: 0.00013665
Iteration 14/1000 | Loss: 0.00087412
Iteration 15/1000 | Loss: 0.00035590
Iteration 16/1000 | Loss: 0.00058285
Iteration 17/1000 | Loss: 0.00045242
Iteration 18/1000 | Loss: 0.00026650
Iteration 19/1000 | Loss: 0.00297387
Iteration 20/1000 | Loss: 0.00013244
Iteration 21/1000 | Loss: 0.00032337
Iteration 22/1000 | Loss: 0.00010992
Iteration 23/1000 | Loss: 0.00004961
Iteration 24/1000 | Loss: 0.00031795
Iteration 25/1000 | Loss: 0.00017599
Iteration 26/1000 | Loss: 0.00029442
Iteration 27/1000 | Loss: 0.00012279
Iteration 28/1000 | Loss: 0.00012184
Iteration 29/1000 | Loss: 0.00017328
Iteration 30/1000 | Loss: 0.00023896
Iteration 31/1000 | Loss: 0.00036513
Iteration 32/1000 | Loss: 0.00144362
Iteration 33/1000 | Loss: 0.00006665
Iteration 34/1000 | Loss: 0.00012690
Iteration 35/1000 | Loss: 0.00024976
Iteration 36/1000 | Loss: 0.00007715
Iteration 37/1000 | Loss: 0.00002563
Iteration 38/1000 | Loss: 0.00005369
Iteration 39/1000 | Loss: 0.00002416
Iteration 40/1000 | Loss: 0.00010467
Iteration 41/1000 | Loss: 0.00030983
Iteration 42/1000 | Loss: 0.00003740
Iteration 43/1000 | Loss: 0.00002948
Iteration 44/1000 | Loss: 0.00014920
Iteration 45/1000 | Loss: 0.00059784
Iteration 46/1000 | Loss: 0.00011025
Iteration 47/1000 | Loss: 0.00007483
Iteration 48/1000 | Loss: 0.00002310
Iteration 49/1000 | Loss: 0.00002932
Iteration 50/1000 | Loss: 0.00002254
Iteration 51/1000 | Loss: 0.00016096
Iteration 52/1000 | Loss: 0.00008719
Iteration 53/1000 | Loss: 0.00004466
Iteration 54/1000 | Loss: 0.00004087
Iteration 55/1000 | Loss: 0.00014430
Iteration 56/1000 | Loss: 0.00017664
Iteration 57/1000 | Loss: 0.00014768
Iteration 58/1000 | Loss: 0.00023151
Iteration 59/1000 | Loss: 0.00018550
Iteration 60/1000 | Loss: 0.00002182
Iteration 61/1000 | Loss: 0.00003523
Iteration 62/1000 | Loss: 0.00002136
Iteration 63/1000 | Loss: 0.00002123
Iteration 64/1000 | Loss: 0.00016979
Iteration 65/1000 | Loss: 0.00002481
Iteration 66/1000 | Loss: 0.00010452
Iteration 67/1000 | Loss: 0.00005740
Iteration 68/1000 | Loss: 0.00002212
Iteration 69/1000 | Loss: 0.00006199
Iteration 70/1000 | Loss: 0.00053314
Iteration 71/1000 | Loss: 0.00003735
Iteration 72/1000 | Loss: 0.00006342
Iteration 73/1000 | Loss: 0.00003714
Iteration 74/1000 | Loss: 0.00002629
Iteration 75/1000 | Loss: 0.00002267
Iteration 76/1000 | Loss: 0.00002096
Iteration 77/1000 | Loss: 0.00002971
Iteration 78/1000 | Loss: 0.00006341
Iteration 79/1000 | Loss: 0.00052542
Iteration 80/1000 | Loss: 0.00008853
Iteration 81/1000 | Loss: 0.00002928
Iteration 82/1000 | Loss: 0.00002091
Iteration 83/1000 | Loss: 0.00002089
Iteration 84/1000 | Loss: 0.00002086
Iteration 85/1000 | Loss: 0.00002085
Iteration 86/1000 | Loss: 0.00002085
Iteration 87/1000 | Loss: 0.00002084
Iteration 88/1000 | Loss: 0.00002084
Iteration 89/1000 | Loss: 0.00002084
Iteration 90/1000 | Loss: 0.00002084
Iteration 91/1000 | Loss: 0.00002084
Iteration 92/1000 | Loss: 0.00002084
Iteration 93/1000 | Loss: 0.00002083
Iteration 94/1000 | Loss: 0.00002083
Iteration 95/1000 | Loss: 0.00002083
Iteration 96/1000 | Loss: 0.00002083
Iteration 97/1000 | Loss: 0.00002083
Iteration 98/1000 | Loss: 0.00002083
Iteration 99/1000 | Loss: 0.00002083
Iteration 100/1000 | Loss: 0.00002083
Iteration 101/1000 | Loss: 0.00002083
Iteration 102/1000 | Loss: 0.00002082
Iteration 103/1000 | Loss: 0.00002082
Iteration 104/1000 | Loss: 0.00002082
Iteration 105/1000 | Loss: 0.00002082
Iteration 106/1000 | Loss: 0.00002082
Iteration 107/1000 | Loss: 0.00002082
Iteration 108/1000 | Loss: 0.00002082
Iteration 109/1000 | Loss: 0.00002081
Iteration 110/1000 | Loss: 0.00002081
Iteration 111/1000 | Loss: 0.00002081
Iteration 112/1000 | Loss: 0.00002081
Iteration 113/1000 | Loss: 0.00002081
Iteration 114/1000 | Loss: 0.00002081
Iteration 115/1000 | Loss: 0.00002081
Iteration 116/1000 | Loss: 0.00002081
Iteration 117/1000 | Loss: 0.00002081
Iteration 118/1000 | Loss: 0.00002081
Iteration 119/1000 | Loss: 0.00002081
Iteration 120/1000 | Loss: 0.00002080
Iteration 121/1000 | Loss: 0.00002080
Iteration 122/1000 | Loss: 0.00002080
Iteration 123/1000 | Loss: 0.00002080
Iteration 124/1000 | Loss: 0.00002080
Iteration 125/1000 | Loss: 0.00002080
Iteration 126/1000 | Loss: 0.00002079
Iteration 127/1000 | Loss: 0.00002079
Iteration 128/1000 | Loss: 0.00002079
Iteration 129/1000 | Loss: 0.00002079
Iteration 130/1000 | Loss: 0.00002079
Iteration 131/1000 | Loss: 0.00002079
Iteration 132/1000 | Loss: 0.00002079
Iteration 133/1000 | Loss: 0.00002078
Iteration 134/1000 | Loss: 0.00002078
Iteration 135/1000 | Loss: 0.00002078
Iteration 136/1000 | Loss: 0.00002078
Iteration 137/1000 | Loss: 0.00002078
Iteration 138/1000 | Loss: 0.00002078
Iteration 139/1000 | Loss: 0.00002078
Iteration 140/1000 | Loss: 0.00002077
Iteration 141/1000 | Loss: 0.00002077
Iteration 142/1000 | Loss: 0.00002077
Iteration 143/1000 | Loss: 0.00002077
Iteration 144/1000 | Loss: 0.00002077
Iteration 145/1000 | Loss: 0.00002077
Iteration 146/1000 | Loss: 0.00002077
Iteration 147/1000 | Loss: 0.00002077
Iteration 148/1000 | Loss: 0.00002077
Iteration 149/1000 | Loss: 0.00002077
Iteration 150/1000 | Loss: 0.00002076
Iteration 151/1000 | Loss: 0.00002076
Iteration 152/1000 | Loss: 0.00002076
Iteration 153/1000 | Loss: 0.00002076
Iteration 154/1000 | Loss: 0.00002076
Iteration 155/1000 | Loss: 0.00002076
Iteration 156/1000 | Loss: 0.00002076
Iteration 157/1000 | Loss: 0.00002076
Iteration 158/1000 | Loss: 0.00002076
Iteration 159/1000 | Loss: 0.00002076
Iteration 160/1000 | Loss: 0.00002076
Iteration 161/1000 | Loss: 0.00002076
Iteration 162/1000 | Loss: 0.00002076
Iteration 163/1000 | Loss: 0.00002076
Iteration 164/1000 | Loss: 0.00002075
Iteration 165/1000 | Loss: 0.00002075
Iteration 166/1000 | Loss: 0.00002075
Iteration 167/1000 | Loss: 0.00002075
Iteration 168/1000 | Loss: 0.00002075
Iteration 169/1000 | Loss: 0.00002075
Iteration 170/1000 | Loss: 0.00002075
Iteration 171/1000 | Loss: 0.00002075
Iteration 172/1000 | Loss: 0.00002075
Iteration 173/1000 | Loss: 0.00002075
Iteration 174/1000 | Loss: 0.00002075
Iteration 175/1000 | Loss: 0.00002075
Iteration 176/1000 | Loss: 0.00002075
Iteration 177/1000 | Loss: 0.00002075
Iteration 178/1000 | Loss: 0.00002074
Iteration 179/1000 | Loss: 0.00002074
Iteration 180/1000 | Loss: 0.00002074
Iteration 181/1000 | Loss: 0.00002074
Iteration 182/1000 | Loss: 0.00002074
Iteration 183/1000 | Loss: 0.00002074
Iteration 184/1000 | Loss: 0.00002074
Iteration 185/1000 | Loss: 0.00002074
Iteration 186/1000 | Loss: 0.00002074
Iteration 187/1000 | Loss: 0.00002074
Iteration 188/1000 | Loss: 0.00002074
Iteration 189/1000 | Loss: 0.00002074
Iteration 190/1000 | Loss: 0.00002074
Iteration 191/1000 | Loss: 0.00002074
Iteration 192/1000 | Loss: 0.00002074
Iteration 193/1000 | Loss: 0.00002074
Iteration 194/1000 | Loss: 0.00002074
Iteration 195/1000 | Loss: 0.00002074
Iteration 196/1000 | Loss: 0.00002073
Iteration 197/1000 | Loss: 0.00002073
Iteration 198/1000 | Loss: 0.00002073
Iteration 199/1000 | Loss: 0.00002073
Iteration 200/1000 | Loss: 0.00002073
Iteration 201/1000 | Loss: 0.00002073
Iteration 202/1000 | Loss: 0.00002073
Iteration 203/1000 | Loss: 0.00002073
Iteration 204/1000 | Loss: 0.00002073
Iteration 205/1000 | Loss: 0.00002073
Iteration 206/1000 | Loss: 0.00002073
Iteration 207/1000 | Loss: 0.00002072
Iteration 208/1000 | Loss: 0.00002072
Iteration 209/1000 | Loss: 0.00002072
Iteration 210/1000 | Loss: 0.00002072
Iteration 211/1000 | Loss: 0.00002072
Iteration 212/1000 | Loss: 0.00002072
Iteration 213/1000 | Loss: 0.00002072
Iteration 214/1000 | Loss: 0.00002072
Iteration 215/1000 | Loss: 0.00002072
Iteration 216/1000 | Loss: 0.00002072
Iteration 217/1000 | Loss: 0.00002072
Iteration 218/1000 | Loss: 0.00002072
Iteration 219/1000 | Loss: 0.00002072
Iteration 220/1000 | Loss: 0.00002072
Iteration 221/1000 | Loss: 0.00002072
Iteration 222/1000 | Loss: 0.00002072
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 222. Stopping optimization.
Last 5 losses: [2.072318056889344e-05, 2.072318056889344e-05, 2.072318056889344e-05, 2.072318056889344e-05, 2.072318056889344e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.072318056889344e-05

Optimization complete. Final v2v error: 3.9317803382873535 mm

Highest mean error: 4.098880290985107 mm for frame 157

Lowest mean error: 3.679417610168457 mm for frame 18

Saving results

Total time: 145.0098536014557
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_011/1091/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_011/1091.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_011/1091
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00424939
Iteration 2/25 | Loss: 0.00130398
Iteration 3/25 | Loss: 0.00122236
Iteration 4/25 | Loss: 0.00120710
Iteration 5/25 | Loss: 0.00120301
Iteration 6/25 | Loss: 0.00120243
Iteration 7/25 | Loss: 0.00120243
Iteration 8/25 | Loss: 0.00120243
Iteration 9/25 | Loss: 0.00120243
Iteration 10/25 | Loss: 0.00120243
Iteration 11/25 | Loss: 0.00120243
Iteration 12/25 | Loss: 0.00120243
Iteration 13/25 | Loss: 0.00120243
Iteration 14/25 | Loss: 0.00120243
Iteration 15/25 | Loss: 0.00120243
Iteration 16/25 | Loss: 0.00120243
Iteration 17/25 | Loss: 0.00120243
Iteration 18/25 | Loss: 0.00120243
Iteration 19/25 | Loss: 0.00120243
Iteration 20/25 | Loss: 0.00120243
Iteration 21/25 | Loss: 0.00120243
Iteration 22/25 | Loss: 0.00120243
Iteration 23/25 | Loss: 0.00120243
Iteration 24/25 | Loss: 0.00120243
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0012024325551465154, 0.0012024325551465154, 0.0012024325551465154, 0.0012024325551465154, 0.0012024325551465154]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012024325551465154

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44694972
Iteration 2/25 | Loss: 0.00091250
Iteration 3/25 | Loss: 0.00091250
Iteration 4/25 | Loss: 0.00091250
Iteration 5/25 | Loss: 0.00091250
Iteration 6/25 | Loss: 0.00091250
Iteration 7/25 | Loss: 0.00091250
Iteration 8/25 | Loss: 0.00091250
Iteration 9/25 | Loss: 0.00091250
Iteration 10/25 | Loss: 0.00091250
Iteration 11/25 | Loss: 0.00091250
Iteration 12/25 | Loss: 0.00091250
Iteration 13/25 | Loss: 0.00091250
Iteration 14/25 | Loss: 0.00091250
Iteration 15/25 | Loss: 0.00091250
Iteration 16/25 | Loss: 0.00091250
Iteration 17/25 | Loss: 0.00091250
Iteration 18/25 | Loss: 0.00091250
Iteration 19/25 | Loss: 0.00091250
Iteration 20/25 | Loss: 0.00091250
Iteration 21/25 | Loss: 0.00091250
Iteration 22/25 | Loss: 0.00091250
Iteration 23/25 | Loss: 0.00091250
Iteration 24/25 | Loss: 0.00091250
Iteration 25/25 | Loss: 0.00091250

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00091250
Iteration 2/1000 | Loss: 0.00002784
Iteration 3/1000 | Loss: 0.00001948
Iteration 4/1000 | Loss: 0.00001743
Iteration 5/1000 | Loss: 0.00001654
Iteration 6/1000 | Loss: 0.00001607
Iteration 7/1000 | Loss: 0.00001557
Iteration 8/1000 | Loss: 0.00001520
Iteration 9/1000 | Loss: 0.00001490
Iteration 10/1000 | Loss: 0.00001457
Iteration 11/1000 | Loss: 0.00001455
Iteration 12/1000 | Loss: 0.00001454
Iteration 13/1000 | Loss: 0.00001448
Iteration 14/1000 | Loss: 0.00001429
Iteration 15/1000 | Loss: 0.00001422
Iteration 16/1000 | Loss: 0.00001422
Iteration 17/1000 | Loss: 0.00001421
Iteration 18/1000 | Loss: 0.00001421
Iteration 19/1000 | Loss: 0.00001421
Iteration 20/1000 | Loss: 0.00001421
Iteration 21/1000 | Loss: 0.00001421
Iteration 22/1000 | Loss: 0.00001421
Iteration 23/1000 | Loss: 0.00001421
Iteration 24/1000 | Loss: 0.00001417
Iteration 25/1000 | Loss: 0.00001416
Iteration 26/1000 | Loss: 0.00001414
Iteration 27/1000 | Loss: 0.00001410
Iteration 28/1000 | Loss: 0.00001406
Iteration 29/1000 | Loss: 0.00001401
Iteration 30/1000 | Loss: 0.00001397
Iteration 31/1000 | Loss: 0.00001393
Iteration 32/1000 | Loss: 0.00001393
Iteration 33/1000 | Loss: 0.00001393
Iteration 34/1000 | Loss: 0.00001393
Iteration 35/1000 | Loss: 0.00001393
Iteration 36/1000 | Loss: 0.00001393
Iteration 37/1000 | Loss: 0.00001393
Iteration 38/1000 | Loss: 0.00001393
Iteration 39/1000 | Loss: 0.00001393
Iteration 40/1000 | Loss: 0.00001391
Iteration 41/1000 | Loss: 0.00001389
Iteration 42/1000 | Loss: 0.00001388
Iteration 43/1000 | Loss: 0.00001387
Iteration 44/1000 | Loss: 0.00001387
Iteration 45/1000 | Loss: 0.00001383
Iteration 46/1000 | Loss: 0.00001381
Iteration 47/1000 | Loss: 0.00001380
Iteration 48/1000 | Loss: 0.00001380
Iteration 49/1000 | Loss: 0.00001380
Iteration 50/1000 | Loss: 0.00001380
Iteration 51/1000 | Loss: 0.00001379
Iteration 52/1000 | Loss: 0.00001378
Iteration 53/1000 | Loss: 0.00001378
Iteration 54/1000 | Loss: 0.00001377
Iteration 55/1000 | Loss: 0.00001375
Iteration 56/1000 | Loss: 0.00001375
Iteration 57/1000 | Loss: 0.00001374
Iteration 58/1000 | Loss: 0.00001374
Iteration 59/1000 | Loss: 0.00001374
Iteration 60/1000 | Loss: 0.00001373
Iteration 61/1000 | Loss: 0.00001373
Iteration 62/1000 | Loss: 0.00001373
Iteration 63/1000 | Loss: 0.00001372
Iteration 64/1000 | Loss: 0.00001372
Iteration 65/1000 | Loss: 0.00001371
Iteration 66/1000 | Loss: 0.00001371
Iteration 67/1000 | Loss: 0.00001371
Iteration 68/1000 | Loss: 0.00001370
Iteration 69/1000 | Loss: 0.00001370
Iteration 70/1000 | Loss: 0.00001370
Iteration 71/1000 | Loss: 0.00001369
Iteration 72/1000 | Loss: 0.00001369
Iteration 73/1000 | Loss: 0.00001369
Iteration 74/1000 | Loss: 0.00001369
Iteration 75/1000 | Loss: 0.00001369
Iteration 76/1000 | Loss: 0.00001368
Iteration 77/1000 | Loss: 0.00001368
Iteration 78/1000 | Loss: 0.00001368
Iteration 79/1000 | Loss: 0.00001368
Iteration 80/1000 | Loss: 0.00001368
Iteration 81/1000 | Loss: 0.00001367
Iteration 82/1000 | Loss: 0.00001367
Iteration 83/1000 | Loss: 0.00001367
Iteration 84/1000 | Loss: 0.00001366
Iteration 85/1000 | Loss: 0.00001366
Iteration 86/1000 | Loss: 0.00001366
Iteration 87/1000 | Loss: 0.00001366
Iteration 88/1000 | Loss: 0.00001366
Iteration 89/1000 | Loss: 0.00001366
Iteration 90/1000 | Loss: 0.00001366
Iteration 91/1000 | Loss: 0.00001366
Iteration 92/1000 | Loss: 0.00001365
Iteration 93/1000 | Loss: 0.00001365
Iteration 94/1000 | Loss: 0.00001365
Iteration 95/1000 | Loss: 0.00001364
Iteration 96/1000 | Loss: 0.00001364
Iteration 97/1000 | Loss: 0.00001363
Iteration 98/1000 | Loss: 0.00001363
Iteration 99/1000 | Loss: 0.00001363
Iteration 100/1000 | Loss: 0.00001363
Iteration 101/1000 | Loss: 0.00001363
Iteration 102/1000 | Loss: 0.00001362
Iteration 103/1000 | Loss: 0.00001362
Iteration 104/1000 | Loss: 0.00001362
Iteration 105/1000 | Loss: 0.00001361
Iteration 106/1000 | Loss: 0.00001361
Iteration 107/1000 | Loss: 0.00001361
Iteration 108/1000 | Loss: 0.00001361
Iteration 109/1000 | Loss: 0.00001361
Iteration 110/1000 | Loss: 0.00001360
Iteration 111/1000 | Loss: 0.00001360
Iteration 112/1000 | Loss: 0.00001360
Iteration 113/1000 | Loss: 0.00001360
Iteration 114/1000 | Loss: 0.00001360
Iteration 115/1000 | Loss: 0.00001359
Iteration 116/1000 | Loss: 0.00001359
Iteration 117/1000 | Loss: 0.00001359
Iteration 118/1000 | Loss: 0.00001359
Iteration 119/1000 | Loss: 0.00001359
Iteration 120/1000 | Loss: 0.00001359
Iteration 121/1000 | Loss: 0.00001359
Iteration 122/1000 | Loss: 0.00001359
Iteration 123/1000 | Loss: 0.00001359
Iteration 124/1000 | Loss: 0.00001359
Iteration 125/1000 | Loss: 0.00001359
Iteration 126/1000 | Loss: 0.00001359
Iteration 127/1000 | Loss: 0.00001359
Iteration 128/1000 | Loss: 0.00001359
Iteration 129/1000 | Loss: 0.00001359
Iteration 130/1000 | Loss: 0.00001359
Iteration 131/1000 | Loss: 0.00001359
Iteration 132/1000 | Loss: 0.00001359
Iteration 133/1000 | Loss: 0.00001358
Iteration 134/1000 | Loss: 0.00001358
Iteration 135/1000 | Loss: 0.00001358
Iteration 136/1000 | Loss: 0.00001358
Iteration 137/1000 | Loss: 0.00001358
Iteration 138/1000 | Loss: 0.00001358
Iteration 139/1000 | Loss: 0.00001358
Iteration 140/1000 | Loss: 0.00001358
Iteration 141/1000 | Loss: 0.00001358
Iteration 142/1000 | Loss: 0.00001358
Iteration 143/1000 | Loss: 0.00001358
Iteration 144/1000 | Loss: 0.00001358
Iteration 145/1000 | Loss: 0.00001358
Iteration 146/1000 | Loss: 0.00001358
Iteration 147/1000 | Loss: 0.00001358
Iteration 148/1000 | Loss: 0.00001358
Iteration 149/1000 | Loss: 0.00001358
Iteration 150/1000 | Loss: 0.00001357
Iteration 151/1000 | Loss: 0.00001357
Iteration 152/1000 | Loss: 0.00001357
Iteration 153/1000 | Loss: 0.00001357
Iteration 154/1000 | Loss: 0.00001357
Iteration 155/1000 | Loss: 0.00001357
Iteration 156/1000 | Loss: 0.00001357
Iteration 157/1000 | Loss: 0.00001357
Iteration 158/1000 | Loss: 0.00001357
Iteration 159/1000 | Loss: 0.00001357
Iteration 160/1000 | Loss: 0.00001357
Iteration 161/1000 | Loss: 0.00001357
Iteration 162/1000 | Loss: 0.00001357
Iteration 163/1000 | Loss: 0.00001357
Iteration 164/1000 | Loss: 0.00001357
Iteration 165/1000 | Loss: 0.00001357
Iteration 166/1000 | Loss: 0.00001357
Iteration 167/1000 | Loss: 0.00001357
Iteration 168/1000 | Loss: 0.00001357
Iteration 169/1000 | Loss: 0.00001357
Iteration 170/1000 | Loss: 0.00001357
Iteration 171/1000 | Loss: 0.00001357
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 171. Stopping optimization.
Last 5 losses: [1.3571516319643706e-05, 1.3571516319643706e-05, 1.3571516319643706e-05, 1.3571516319643706e-05, 1.3571516319643706e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3571516319643706e-05

Optimization complete. Final v2v error: 3.1570115089416504 mm

Highest mean error: 3.639071226119995 mm for frame 116

Lowest mean error: 3.01863694190979 mm for frame 40

Saving results

Total time: 42.40046668052673
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_011/1040/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_011/1040.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_011/1040
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00531769
Iteration 2/25 | Loss: 0.00142863
Iteration 3/25 | Loss: 0.00135162
Iteration 4/25 | Loss: 0.00133778
Iteration 5/25 | Loss: 0.00133595
Iteration 6/25 | Loss: 0.00133595
Iteration 7/25 | Loss: 0.00133595
Iteration 8/25 | Loss: 0.00133595
Iteration 9/25 | Loss: 0.00133595
Iteration 10/25 | Loss: 0.00133595
Iteration 11/25 | Loss: 0.00133595
Iteration 12/25 | Loss: 0.00133595
Iteration 13/25 | Loss: 0.00133595
Iteration 14/25 | Loss: 0.00133595
Iteration 15/25 | Loss: 0.00133595
Iteration 16/25 | Loss: 0.00133595
Iteration 17/25 | Loss: 0.00133595
Iteration 18/25 | Loss: 0.00133595
Iteration 19/25 | Loss: 0.00133595
Iteration 20/25 | Loss: 0.00133595
Iteration 21/25 | Loss: 0.00133595
Iteration 22/25 | Loss: 0.00133595
Iteration 23/25 | Loss: 0.00133595
Iteration 24/25 | Loss: 0.00133595
Iteration 25/25 | Loss: 0.00133595
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0013359501026570797, 0.0013359501026570797, 0.0013359501026570797, 0.0013359501026570797, 0.0013359501026570797]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013359501026570797

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.74642426
Iteration 2/25 | Loss: 0.00085256
Iteration 3/25 | Loss: 0.00085255
Iteration 4/25 | Loss: 0.00085255
Iteration 5/25 | Loss: 0.00085255
Iteration 6/25 | Loss: 0.00085255
Iteration 7/25 | Loss: 0.00085255
Iteration 8/25 | Loss: 0.00085255
Iteration 9/25 | Loss: 0.00085255
Iteration 10/25 | Loss: 0.00085255
Iteration 11/25 | Loss: 0.00085255
Iteration 12/25 | Loss: 0.00085255
Iteration 13/25 | Loss: 0.00085255
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0008525514276698232, 0.0008525514276698232, 0.0008525514276698232, 0.0008525514276698232, 0.0008525514276698232]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008525514276698232

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00085255
Iteration 2/1000 | Loss: 0.00004493
Iteration 3/1000 | Loss: 0.00004044
Iteration 4/1000 | Loss: 0.00003819
Iteration 5/1000 | Loss: 0.00003706
Iteration 6/1000 | Loss: 0.00003678
Iteration 7/1000 | Loss: 0.00003626
Iteration 8/1000 | Loss: 0.00003561
Iteration 9/1000 | Loss: 0.00003510
Iteration 10/1000 | Loss: 0.00003472
Iteration 11/1000 | Loss: 0.00003441
Iteration 12/1000 | Loss: 0.00003394
Iteration 13/1000 | Loss: 0.00003367
Iteration 14/1000 | Loss: 0.00003344
Iteration 15/1000 | Loss: 0.00003340
Iteration 16/1000 | Loss: 0.00003322
Iteration 17/1000 | Loss: 0.00003313
Iteration 18/1000 | Loss: 0.00003305
Iteration 19/1000 | Loss: 0.00003302
Iteration 20/1000 | Loss: 0.00003301
Iteration 21/1000 | Loss: 0.00003301
Iteration 22/1000 | Loss: 0.00003300
Iteration 23/1000 | Loss: 0.00003300
Iteration 24/1000 | Loss: 0.00003299
Iteration 25/1000 | Loss: 0.00003299
Iteration 26/1000 | Loss: 0.00003299
Iteration 27/1000 | Loss: 0.00003299
Iteration 28/1000 | Loss: 0.00003298
Iteration 29/1000 | Loss: 0.00003298
Iteration 30/1000 | Loss: 0.00003297
Iteration 31/1000 | Loss: 0.00003293
Iteration 32/1000 | Loss: 0.00003292
Iteration 33/1000 | Loss: 0.00003292
Iteration 34/1000 | Loss: 0.00003291
Iteration 35/1000 | Loss: 0.00003291
Iteration 36/1000 | Loss: 0.00003291
Iteration 37/1000 | Loss: 0.00003291
Iteration 38/1000 | Loss: 0.00003290
Iteration 39/1000 | Loss: 0.00003289
Iteration 40/1000 | Loss: 0.00003289
Iteration 41/1000 | Loss: 0.00003289
Iteration 42/1000 | Loss: 0.00003289
Iteration 43/1000 | Loss: 0.00003289
Iteration 44/1000 | Loss: 0.00003288
Iteration 45/1000 | Loss: 0.00003288
Iteration 46/1000 | Loss: 0.00003288
Iteration 47/1000 | Loss: 0.00003288
Iteration 48/1000 | Loss: 0.00003288
Iteration 49/1000 | Loss: 0.00003288
Iteration 50/1000 | Loss: 0.00003288
Iteration 51/1000 | Loss: 0.00003288
Iteration 52/1000 | Loss: 0.00003288
Iteration 53/1000 | Loss: 0.00003288
Iteration 54/1000 | Loss: 0.00003286
Iteration 55/1000 | Loss: 0.00003286
Iteration 56/1000 | Loss: 0.00003285
Iteration 57/1000 | Loss: 0.00003285
Iteration 58/1000 | Loss: 0.00003285
Iteration 59/1000 | Loss: 0.00003285
Iteration 60/1000 | Loss: 0.00003284
Iteration 61/1000 | Loss: 0.00003284
Iteration 62/1000 | Loss: 0.00003283
Iteration 63/1000 | Loss: 0.00003282
Iteration 64/1000 | Loss: 0.00003282
Iteration 65/1000 | Loss: 0.00003282
Iteration 66/1000 | Loss: 0.00003282
Iteration 67/1000 | Loss: 0.00003282
Iteration 68/1000 | Loss: 0.00003282
Iteration 69/1000 | Loss: 0.00003282
Iteration 70/1000 | Loss: 0.00003281
Iteration 71/1000 | Loss: 0.00003281
Iteration 72/1000 | Loss: 0.00003281
Iteration 73/1000 | Loss: 0.00003281
Iteration 74/1000 | Loss: 0.00003281
Iteration 75/1000 | Loss: 0.00003281
Iteration 76/1000 | Loss: 0.00003280
Iteration 77/1000 | Loss: 0.00003280
Iteration 78/1000 | Loss: 0.00003279
Iteration 79/1000 | Loss: 0.00003279
Iteration 80/1000 | Loss: 0.00003279
Iteration 81/1000 | Loss: 0.00003278
Iteration 82/1000 | Loss: 0.00003278
Iteration 83/1000 | Loss: 0.00003278
Iteration 84/1000 | Loss: 0.00003278
Iteration 85/1000 | Loss: 0.00003278
Iteration 86/1000 | Loss: 0.00003278
Iteration 87/1000 | Loss: 0.00003278
Iteration 88/1000 | Loss: 0.00003277
Iteration 89/1000 | Loss: 0.00003277
Iteration 90/1000 | Loss: 0.00003277
Iteration 91/1000 | Loss: 0.00003277
Iteration 92/1000 | Loss: 0.00003277
Iteration 93/1000 | Loss: 0.00003275
Iteration 94/1000 | Loss: 0.00003275
Iteration 95/1000 | Loss: 0.00003275
Iteration 96/1000 | Loss: 0.00003275
Iteration 97/1000 | Loss: 0.00003275
Iteration 98/1000 | Loss: 0.00003275
Iteration 99/1000 | Loss: 0.00003274
Iteration 100/1000 | Loss: 0.00003274
Iteration 101/1000 | Loss: 0.00003274
Iteration 102/1000 | Loss: 0.00003273
Iteration 103/1000 | Loss: 0.00003273
Iteration 104/1000 | Loss: 0.00003273
Iteration 105/1000 | Loss: 0.00003272
Iteration 106/1000 | Loss: 0.00003272
Iteration 107/1000 | Loss: 0.00003272
Iteration 108/1000 | Loss: 0.00003272
Iteration 109/1000 | Loss: 0.00003271
Iteration 110/1000 | Loss: 0.00003271
Iteration 111/1000 | Loss: 0.00003271
Iteration 112/1000 | Loss: 0.00003271
Iteration 113/1000 | Loss: 0.00003271
Iteration 114/1000 | Loss: 0.00003271
Iteration 115/1000 | Loss: 0.00003271
Iteration 116/1000 | Loss: 0.00003271
Iteration 117/1000 | Loss: 0.00003271
Iteration 118/1000 | Loss: 0.00003270
Iteration 119/1000 | Loss: 0.00003270
Iteration 120/1000 | Loss: 0.00003270
Iteration 121/1000 | Loss: 0.00003269
Iteration 122/1000 | Loss: 0.00003269
Iteration 123/1000 | Loss: 0.00003268
Iteration 124/1000 | Loss: 0.00003268
Iteration 125/1000 | Loss: 0.00003268
Iteration 126/1000 | Loss: 0.00003268
Iteration 127/1000 | Loss: 0.00003268
Iteration 128/1000 | Loss: 0.00003268
Iteration 129/1000 | Loss: 0.00003268
Iteration 130/1000 | Loss: 0.00003267
Iteration 131/1000 | Loss: 0.00003267
Iteration 132/1000 | Loss: 0.00003267
Iteration 133/1000 | Loss: 0.00003267
Iteration 134/1000 | Loss: 0.00003267
Iteration 135/1000 | Loss: 0.00003267
Iteration 136/1000 | Loss: 0.00003267
Iteration 137/1000 | Loss: 0.00003267
Iteration 138/1000 | Loss: 0.00003267
Iteration 139/1000 | Loss: 0.00003267
Iteration 140/1000 | Loss: 0.00003267
Iteration 141/1000 | Loss: 0.00003267
Iteration 142/1000 | Loss: 0.00003267
Iteration 143/1000 | Loss: 0.00003267
Iteration 144/1000 | Loss: 0.00003267
Iteration 145/1000 | Loss: 0.00003267
Iteration 146/1000 | Loss: 0.00003267
Iteration 147/1000 | Loss: 0.00003267
Iteration 148/1000 | Loss: 0.00003267
Iteration 149/1000 | Loss: 0.00003267
Iteration 150/1000 | Loss: 0.00003267
Iteration 151/1000 | Loss: 0.00003267
Iteration 152/1000 | Loss: 0.00003267
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 152. Stopping optimization.
Last 5 losses: [3.266586281824857e-05, 3.266586281824857e-05, 3.266586281824857e-05, 3.266586281824857e-05, 3.266586281824857e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.266586281824857e-05

Optimization complete. Final v2v error: 4.530071258544922 mm

Highest mean error: 4.582046985626221 mm for frame 85

Lowest mean error: 4.492363929748535 mm for frame 91

Saving results

Total time: 47.15604639053345
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_011/1060/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_011/1060.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_011/1060
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00955354
Iteration 2/25 | Loss: 0.00226207
Iteration 3/25 | Loss: 0.00243805
Iteration 4/25 | Loss: 0.00159937
Iteration 5/25 | Loss: 0.00147866
Iteration 6/25 | Loss: 0.00144742
Iteration 7/25 | Loss: 0.00141115
Iteration 8/25 | Loss: 0.00133436
Iteration 9/25 | Loss: 0.00128971
Iteration 10/25 | Loss: 0.00126262
Iteration 11/25 | Loss: 0.00125750
Iteration 12/25 | Loss: 0.00125280
Iteration 13/25 | Loss: 0.00125139
Iteration 14/25 | Loss: 0.00125083
Iteration 15/25 | Loss: 0.00125053
Iteration 16/25 | Loss: 0.00125039
Iteration 17/25 | Loss: 0.00125025
Iteration 18/25 | Loss: 0.00137622
Iteration 19/25 | Loss: 0.00123073
Iteration 20/25 | Loss: 0.00122650
Iteration 21/25 | Loss: 0.00122553
Iteration 22/25 | Loss: 0.00122512
Iteration 23/25 | Loss: 0.00122506
Iteration 24/25 | Loss: 0.00122506
Iteration 25/25 | Loss: 0.00122506

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35026860
Iteration 2/25 | Loss: 0.00113882
Iteration 3/25 | Loss: 0.00113882
Iteration 4/25 | Loss: 0.00113882
Iteration 5/25 | Loss: 0.00113882
Iteration 6/25 | Loss: 0.00113882
Iteration 7/25 | Loss: 0.00113882
Iteration 8/25 | Loss: 0.00113882
Iteration 9/25 | Loss: 0.00113882
Iteration 10/25 | Loss: 0.00113882
Iteration 11/25 | Loss: 0.00113882
Iteration 12/25 | Loss: 0.00113882
Iteration 13/25 | Loss: 0.00113882
Iteration 14/25 | Loss: 0.00113882
Iteration 15/25 | Loss: 0.00113882
Iteration 16/25 | Loss: 0.00113882
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.001138816587626934, 0.001138816587626934, 0.001138816587626934, 0.001138816587626934, 0.001138816587626934]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001138816587626934

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00113882
Iteration 2/1000 | Loss: 0.00034477
Iteration 3/1000 | Loss: 0.00005867
Iteration 4/1000 | Loss: 0.00004521
Iteration 5/1000 | Loss: 0.00038509
Iteration 6/1000 | Loss: 0.00022963
Iteration 7/1000 | Loss: 0.00004295
Iteration 8/1000 | Loss: 0.00003470
Iteration 9/1000 | Loss: 0.00003189
Iteration 10/1000 | Loss: 0.00003013
Iteration 11/1000 | Loss: 0.00002793
Iteration 12/1000 | Loss: 0.00096324
Iteration 13/1000 | Loss: 0.00057513
Iteration 14/1000 | Loss: 0.00072709
Iteration 15/1000 | Loss: 0.00101792
Iteration 16/1000 | Loss: 0.00113471
Iteration 17/1000 | Loss: 0.00007117
Iteration 18/1000 | Loss: 0.00004374
Iteration 19/1000 | Loss: 0.00060117
Iteration 20/1000 | Loss: 0.00004666
Iteration 21/1000 | Loss: 0.00003220
Iteration 22/1000 | Loss: 0.00002769
Iteration 23/1000 | Loss: 0.00033730
Iteration 24/1000 | Loss: 0.00034860
Iteration 25/1000 | Loss: 0.00003696
Iteration 26/1000 | Loss: 0.00003026
Iteration 27/1000 | Loss: 0.00034673
Iteration 28/1000 | Loss: 0.00003308
Iteration 29/1000 | Loss: 0.00002859
Iteration 30/1000 | Loss: 0.00020534
Iteration 31/1000 | Loss: 0.00002437
Iteration 32/1000 | Loss: 0.00002204
Iteration 33/1000 | Loss: 0.00001944
Iteration 34/1000 | Loss: 0.00001791
Iteration 35/1000 | Loss: 0.00001702
Iteration 36/1000 | Loss: 0.00001641
Iteration 37/1000 | Loss: 0.00001590
Iteration 38/1000 | Loss: 0.00001561
Iteration 39/1000 | Loss: 0.00001536
Iteration 40/1000 | Loss: 0.00001529
Iteration 41/1000 | Loss: 0.00001513
Iteration 42/1000 | Loss: 0.00001499
Iteration 43/1000 | Loss: 0.00001495
Iteration 44/1000 | Loss: 0.00001492
Iteration 45/1000 | Loss: 0.00001490
Iteration 46/1000 | Loss: 0.00001490
Iteration 47/1000 | Loss: 0.00001490
Iteration 48/1000 | Loss: 0.00001488
Iteration 49/1000 | Loss: 0.00001487
Iteration 50/1000 | Loss: 0.00001486
Iteration 51/1000 | Loss: 0.00001479
Iteration 52/1000 | Loss: 0.00001479
Iteration 53/1000 | Loss: 0.00013688
Iteration 54/1000 | Loss: 0.00021253
Iteration 55/1000 | Loss: 0.00006000
Iteration 56/1000 | Loss: 0.00013378
Iteration 57/1000 | Loss: 0.00018744
Iteration 58/1000 | Loss: 0.00016690
Iteration 59/1000 | Loss: 0.00024944
Iteration 60/1000 | Loss: 0.00001840
Iteration 61/1000 | Loss: 0.00001668
Iteration 62/1000 | Loss: 0.00001522
Iteration 63/1000 | Loss: 0.00001448
Iteration 64/1000 | Loss: 0.00001422
Iteration 65/1000 | Loss: 0.00001411
Iteration 66/1000 | Loss: 0.00001409
Iteration 67/1000 | Loss: 0.00001404
Iteration 68/1000 | Loss: 0.00001386
Iteration 69/1000 | Loss: 0.00001369
Iteration 70/1000 | Loss: 0.00001363
Iteration 71/1000 | Loss: 0.00001362
Iteration 72/1000 | Loss: 0.00001360
Iteration 73/1000 | Loss: 0.00001359
Iteration 74/1000 | Loss: 0.00001358
Iteration 75/1000 | Loss: 0.00001357
Iteration 76/1000 | Loss: 0.00001356
Iteration 77/1000 | Loss: 0.00001356
Iteration 78/1000 | Loss: 0.00001354
Iteration 79/1000 | Loss: 0.00001352
Iteration 80/1000 | Loss: 0.00001351
Iteration 81/1000 | Loss: 0.00001351
Iteration 82/1000 | Loss: 0.00001350
Iteration 83/1000 | Loss: 0.00001350
Iteration 84/1000 | Loss: 0.00001350
Iteration 85/1000 | Loss: 0.00001350
Iteration 86/1000 | Loss: 0.00001350
Iteration 87/1000 | Loss: 0.00001350
Iteration 88/1000 | Loss: 0.00001349
Iteration 89/1000 | Loss: 0.00001349
Iteration 90/1000 | Loss: 0.00001349
Iteration 91/1000 | Loss: 0.00001349
Iteration 92/1000 | Loss: 0.00001349
Iteration 93/1000 | Loss: 0.00001349
Iteration 94/1000 | Loss: 0.00001349
Iteration 95/1000 | Loss: 0.00001349
Iteration 96/1000 | Loss: 0.00001349
Iteration 97/1000 | Loss: 0.00001348
Iteration 98/1000 | Loss: 0.00001348
Iteration 99/1000 | Loss: 0.00001348
Iteration 100/1000 | Loss: 0.00001348
Iteration 101/1000 | Loss: 0.00001348
Iteration 102/1000 | Loss: 0.00001347
Iteration 103/1000 | Loss: 0.00001347
Iteration 104/1000 | Loss: 0.00001347
Iteration 105/1000 | Loss: 0.00001347
Iteration 106/1000 | Loss: 0.00001346
Iteration 107/1000 | Loss: 0.00001346
Iteration 108/1000 | Loss: 0.00001346
Iteration 109/1000 | Loss: 0.00001346
Iteration 110/1000 | Loss: 0.00001345
Iteration 111/1000 | Loss: 0.00001345
Iteration 112/1000 | Loss: 0.00001345
Iteration 113/1000 | Loss: 0.00001345
Iteration 114/1000 | Loss: 0.00001345
Iteration 115/1000 | Loss: 0.00001345
Iteration 116/1000 | Loss: 0.00001345
Iteration 117/1000 | Loss: 0.00001345
Iteration 118/1000 | Loss: 0.00001345
Iteration 119/1000 | Loss: 0.00001345
Iteration 120/1000 | Loss: 0.00001345
Iteration 121/1000 | Loss: 0.00001345
Iteration 122/1000 | Loss: 0.00001345
Iteration 123/1000 | Loss: 0.00001345
Iteration 124/1000 | Loss: 0.00001345
Iteration 125/1000 | Loss: 0.00001345
Iteration 126/1000 | Loss: 0.00001345
Iteration 127/1000 | Loss: 0.00001345
Iteration 128/1000 | Loss: 0.00001345
Iteration 129/1000 | Loss: 0.00001345
Iteration 130/1000 | Loss: 0.00001345
Iteration 131/1000 | Loss: 0.00001345
Iteration 132/1000 | Loss: 0.00001345
Iteration 133/1000 | Loss: 0.00001345
Iteration 134/1000 | Loss: 0.00001345
Iteration 135/1000 | Loss: 0.00001345
Iteration 136/1000 | Loss: 0.00001345
Iteration 137/1000 | Loss: 0.00001345
Iteration 138/1000 | Loss: 0.00001345
Iteration 139/1000 | Loss: 0.00001345
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 139. Stopping optimization.
Last 5 losses: [1.3449307516566478e-05, 1.3449307516566478e-05, 1.3449307516566478e-05, 1.3449307516566478e-05, 1.3449307516566478e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3449307516566478e-05

Optimization complete. Final v2v error: 2.980621337890625 mm

Highest mean error: 10.370546340942383 mm for frame 130

Lowest mean error: 2.7509639263153076 mm for frame 165

Saving results

Total time: 146.06895971298218
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_011/1003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_011/1003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_011/1003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00714839
Iteration 2/25 | Loss: 0.00182982
Iteration 3/25 | Loss: 0.00137462
Iteration 4/25 | Loss: 0.00130836
Iteration 5/25 | Loss: 0.00127884
Iteration 6/25 | Loss: 0.00125328
Iteration 7/25 | Loss: 0.00122985
Iteration 8/25 | Loss: 0.00122957
Iteration 9/25 | Loss: 0.00122571
Iteration 10/25 | Loss: 0.00122219
Iteration 11/25 | Loss: 0.00122132
Iteration 12/25 | Loss: 0.00122124
Iteration 13/25 | Loss: 0.00122115
Iteration 14/25 | Loss: 0.00122113
Iteration 15/25 | Loss: 0.00122113
Iteration 16/25 | Loss: 0.00122113
Iteration 17/25 | Loss: 0.00122112
Iteration 18/25 | Loss: 0.00122112
Iteration 19/25 | Loss: 0.00122111
Iteration 20/25 | Loss: 0.00122111
Iteration 21/25 | Loss: 0.00122111
Iteration 22/25 | Loss: 0.00122111
Iteration 23/25 | Loss: 0.00122110
Iteration 24/25 | Loss: 0.00122110
Iteration 25/25 | Loss: 0.00122110

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.31228936
Iteration 2/25 | Loss: 0.00109455
Iteration 3/25 | Loss: 0.00109454
Iteration 4/25 | Loss: 0.00109454
Iteration 5/25 | Loss: 0.00109454
Iteration 6/25 | Loss: 0.00109454
Iteration 7/25 | Loss: 0.00109453
Iteration 8/25 | Loss: 0.00109453
Iteration 9/25 | Loss: 0.00109453
Iteration 10/25 | Loss: 0.00109453
Iteration 11/25 | Loss: 0.00109453
Iteration 12/25 | Loss: 0.00109453
Iteration 13/25 | Loss: 0.00109453
Iteration 14/25 | Loss: 0.00109453
Iteration 15/25 | Loss: 0.00109453
Iteration 16/25 | Loss: 0.00109453
Iteration 17/25 | Loss: 0.00109453
Iteration 18/25 | Loss: 0.00109453
Iteration 19/25 | Loss: 0.00109453
Iteration 20/25 | Loss: 0.00109453
Iteration 21/25 | Loss: 0.00109453
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0010945334797725081, 0.0010945334797725081, 0.0010945334797725081, 0.0010945334797725081, 0.0010945334797725081]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010945334797725081

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00109453
Iteration 2/1000 | Loss: 0.00006425
Iteration 3/1000 | Loss: 0.00006121
Iteration 4/1000 | Loss: 0.00002925
Iteration 5/1000 | Loss: 0.00002446
Iteration 6/1000 | Loss: 0.00002159
Iteration 7/1000 | Loss: 0.00002017
Iteration 8/1000 | Loss: 0.00001948
Iteration 9/1000 | Loss: 0.00002189
Iteration 10/1000 | Loss: 0.00001905
Iteration 11/1000 | Loss: 0.00001838
Iteration 12/1000 | Loss: 0.00001798
Iteration 13/1000 | Loss: 0.00001786
Iteration 14/1000 | Loss: 0.00001761
Iteration 15/1000 | Loss: 0.00001747
Iteration 16/1000 | Loss: 0.00001715
Iteration 17/1000 | Loss: 0.00001677
Iteration 18/1000 | Loss: 0.00001655
Iteration 19/1000 | Loss: 0.00001652
Iteration 20/1000 | Loss: 0.00001645
Iteration 21/1000 | Loss: 0.00001643
Iteration 22/1000 | Loss: 0.00001641
Iteration 23/1000 | Loss: 0.00001640
Iteration 24/1000 | Loss: 0.00001632
Iteration 25/1000 | Loss: 0.00001626
Iteration 26/1000 | Loss: 0.00001622
Iteration 27/1000 | Loss: 0.00001617
Iteration 28/1000 | Loss: 0.00001609
Iteration 29/1000 | Loss: 0.00001608
Iteration 30/1000 | Loss: 0.00001606
Iteration 31/1000 | Loss: 0.00001606
Iteration 32/1000 | Loss: 0.00001606
Iteration 33/1000 | Loss: 0.00001606
Iteration 34/1000 | Loss: 0.00001605
Iteration 35/1000 | Loss: 0.00001605
Iteration 36/1000 | Loss: 0.00001605
Iteration 37/1000 | Loss: 0.00001605
Iteration 38/1000 | Loss: 0.00001605
Iteration 39/1000 | Loss: 0.00001605
Iteration 40/1000 | Loss: 0.00001605
Iteration 41/1000 | Loss: 0.00001604
Iteration 42/1000 | Loss: 0.00001602
Iteration 43/1000 | Loss: 0.00001601
Iteration 44/1000 | Loss: 0.00001601
Iteration 45/1000 | Loss: 0.00001600
Iteration 46/1000 | Loss: 0.00001600
Iteration 47/1000 | Loss: 0.00001600
Iteration 48/1000 | Loss: 0.00001599
Iteration 49/1000 | Loss: 0.00001599
Iteration 50/1000 | Loss: 0.00001598
Iteration 51/1000 | Loss: 0.00001598
Iteration 52/1000 | Loss: 0.00001597
Iteration 53/1000 | Loss: 0.00001596
Iteration 54/1000 | Loss: 0.00001596
Iteration 55/1000 | Loss: 0.00001595
Iteration 56/1000 | Loss: 0.00001593
Iteration 57/1000 | Loss: 0.00001593
Iteration 58/1000 | Loss: 0.00001592
Iteration 59/1000 | Loss: 0.00001592
Iteration 60/1000 | Loss: 0.00001584
Iteration 61/1000 | Loss: 0.00001583
Iteration 62/1000 | Loss: 0.00001583
Iteration 63/1000 | Loss: 0.00001583
Iteration 64/1000 | Loss: 0.00001583
Iteration 65/1000 | Loss: 0.00001583
Iteration 66/1000 | Loss: 0.00001583
Iteration 67/1000 | Loss: 0.00001583
Iteration 68/1000 | Loss: 0.00001583
Iteration 69/1000 | Loss: 0.00001582
Iteration 70/1000 | Loss: 0.00001582
Iteration 71/1000 | Loss: 0.00001582
Iteration 72/1000 | Loss: 0.00001582
Iteration 73/1000 | Loss: 0.00001582
Iteration 74/1000 | Loss: 0.00001582
Iteration 75/1000 | Loss: 0.00001582
Iteration 76/1000 | Loss: 0.00001582
Iteration 77/1000 | Loss: 0.00001582
Iteration 78/1000 | Loss: 0.00001582
Iteration 79/1000 | Loss: 0.00001581
Iteration 80/1000 | Loss: 0.00001581
Iteration 81/1000 | Loss: 0.00001581
Iteration 82/1000 | Loss: 0.00001580
Iteration 83/1000 | Loss: 0.00001580
Iteration 84/1000 | Loss: 0.00001579
Iteration 85/1000 | Loss: 0.00001579
Iteration 86/1000 | Loss: 0.00001579
Iteration 87/1000 | Loss: 0.00001579
Iteration 88/1000 | Loss: 0.00001579
Iteration 89/1000 | Loss: 0.00001579
Iteration 90/1000 | Loss: 0.00001579
Iteration 91/1000 | Loss: 0.00001579
Iteration 92/1000 | Loss: 0.00001579
Iteration 93/1000 | Loss: 0.00001579
Iteration 94/1000 | Loss: 0.00001578
Iteration 95/1000 | Loss: 0.00001578
Iteration 96/1000 | Loss: 0.00001578
Iteration 97/1000 | Loss: 0.00001578
Iteration 98/1000 | Loss: 0.00001578
Iteration 99/1000 | Loss: 0.00001577
Iteration 100/1000 | Loss: 0.00001577
Iteration 101/1000 | Loss: 0.00001577
Iteration 102/1000 | Loss: 0.00001577
Iteration 103/1000 | Loss: 0.00001577
Iteration 104/1000 | Loss: 0.00001577
Iteration 105/1000 | Loss: 0.00001577
Iteration 106/1000 | Loss: 0.00001577
Iteration 107/1000 | Loss: 0.00001577
Iteration 108/1000 | Loss: 0.00001577
Iteration 109/1000 | Loss: 0.00001577
Iteration 110/1000 | Loss: 0.00001577
Iteration 111/1000 | Loss: 0.00001577
Iteration 112/1000 | Loss: 0.00001577
Iteration 113/1000 | Loss: 0.00001577
Iteration 114/1000 | Loss: 0.00001577
Iteration 115/1000 | Loss: 0.00001577
Iteration 116/1000 | Loss: 0.00001577
Iteration 117/1000 | Loss: 0.00001577
Iteration 118/1000 | Loss: 0.00001577
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 118. Stopping optimization.
Last 5 losses: [1.5768440789543092e-05, 1.5768440789543092e-05, 1.5768440789543092e-05, 1.5768440789543092e-05, 1.5768440789543092e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5768440789543092e-05

Optimization complete. Final v2v error: 3.3673458099365234 mm

Highest mean error: 4.186225414276123 mm for frame 234

Lowest mean error: 3.1401498317718506 mm for frame 174

Saving results

Total time: 67.53315854072571
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_011/1072/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_011/1072.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_011/1072
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00987798
Iteration 2/25 | Loss: 0.00182163
Iteration 3/25 | Loss: 0.00149915
Iteration 4/25 | Loss: 0.00145584
Iteration 5/25 | Loss: 0.00140076
Iteration 6/25 | Loss: 0.00139590
Iteration 7/25 | Loss: 0.00141822
Iteration 8/25 | Loss: 0.00136866
Iteration 9/25 | Loss: 0.00134045
Iteration 10/25 | Loss: 0.00132313
Iteration 11/25 | Loss: 0.00131815
Iteration 12/25 | Loss: 0.00129887
Iteration 13/25 | Loss: 0.00129656
Iteration 14/25 | Loss: 0.00129530
Iteration 15/25 | Loss: 0.00129467
Iteration 16/25 | Loss: 0.00129432
Iteration 17/25 | Loss: 0.00129412
Iteration 18/25 | Loss: 0.00129334
Iteration 19/25 | Loss: 0.00129304
Iteration 20/25 | Loss: 0.00129295
Iteration 21/25 | Loss: 0.00129199
Iteration 22/25 | Loss: 0.00129095
Iteration 23/25 | Loss: 0.00129052
Iteration 24/25 | Loss: 0.00129038
Iteration 25/25 | Loss: 0.00129034

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.91167688
Iteration 2/25 | Loss: 0.00073190
Iteration 3/25 | Loss: 0.00073188
Iteration 4/25 | Loss: 0.00073188
Iteration 5/25 | Loss: 0.00073188
Iteration 6/25 | Loss: 0.00073188
Iteration 7/25 | Loss: 0.00073188
Iteration 8/25 | Loss: 0.00073188
Iteration 9/25 | Loss: 0.00073188
Iteration 10/25 | Loss: 0.00073188
Iteration 11/25 | Loss: 0.00073188
Iteration 12/25 | Loss: 0.00073188
Iteration 13/25 | Loss: 0.00073188
Iteration 14/25 | Loss: 0.00073188
Iteration 15/25 | Loss: 0.00073188
Iteration 16/25 | Loss: 0.00073188
Iteration 17/25 | Loss: 0.00073188
Iteration 18/25 | Loss: 0.00073188
Iteration 19/25 | Loss: 0.00073188
Iteration 20/25 | Loss: 0.00073188
Iteration 21/25 | Loss: 0.00073188
Iteration 22/25 | Loss: 0.00073188
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0007318767602555454, 0.0007318767602555454, 0.0007318767602555454, 0.0007318767602555454, 0.0007318767602555454]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007318767602555454

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00073188
Iteration 2/1000 | Loss: 0.00002915
Iteration 3/1000 | Loss: 0.00002367
Iteration 4/1000 | Loss: 0.00002183
Iteration 5/1000 | Loss: 0.00002079
Iteration 6/1000 | Loss: 0.00001999
Iteration 7/1000 | Loss: 0.00001952
Iteration 8/1000 | Loss: 0.00001919
Iteration 9/1000 | Loss: 0.00001886
Iteration 10/1000 | Loss: 0.00001879
Iteration 11/1000 | Loss: 0.00001856
Iteration 12/1000 | Loss: 0.00001839
Iteration 13/1000 | Loss: 0.00001835
Iteration 14/1000 | Loss: 0.00001835
Iteration 15/1000 | Loss: 0.00001817
Iteration 16/1000 | Loss: 0.00001797
Iteration 17/1000 | Loss: 0.00001792
Iteration 18/1000 | Loss: 0.00001789
Iteration 19/1000 | Loss: 0.00001787
Iteration 20/1000 | Loss: 0.00001785
Iteration 21/1000 | Loss: 0.00001785
Iteration 22/1000 | Loss: 0.00001784
Iteration 23/1000 | Loss: 0.00001784
Iteration 24/1000 | Loss: 0.00001784
Iteration 25/1000 | Loss: 0.00001784
Iteration 26/1000 | Loss: 0.00001783
Iteration 27/1000 | Loss: 0.00001783
Iteration 28/1000 | Loss: 0.00001780
Iteration 29/1000 | Loss: 0.00001779
Iteration 30/1000 | Loss: 0.00001779
Iteration 31/1000 | Loss: 0.00001778
Iteration 32/1000 | Loss: 0.00001777
Iteration 33/1000 | Loss: 0.00001776
Iteration 34/1000 | Loss: 0.00001776
Iteration 35/1000 | Loss: 0.00001776
Iteration 36/1000 | Loss: 0.00001776
Iteration 37/1000 | Loss: 0.00001776
Iteration 38/1000 | Loss: 0.00001775
Iteration 39/1000 | Loss: 0.00001775
Iteration 40/1000 | Loss: 0.00001774
Iteration 41/1000 | Loss: 0.00001771
Iteration 42/1000 | Loss: 0.00001771
Iteration 43/1000 | Loss: 0.00001771
Iteration 44/1000 | Loss: 0.00001770
Iteration 45/1000 | Loss: 0.00001770
Iteration 46/1000 | Loss: 0.00001770
Iteration 47/1000 | Loss: 0.00001770
Iteration 48/1000 | Loss: 0.00001770
Iteration 49/1000 | Loss: 0.00001770
Iteration 50/1000 | Loss: 0.00001770
Iteration 51/1000 | Loss: 0.00001769
Iteration 52/1000 | Loss: 0.00001767
Iteration 53/1000 | Loss: 0.00001766
Iteration 54/1000 | Loss: 0.00001766
Iteration 55/1000 | Loss: 0.00001766
Iteration 56/1000 | Loss: 0.00001766
Iteration 57/1000 | Loss: 0.00001766
Iteration 58/1000 | Loss: 0.00001766
Iteration 59/1000 | Loss: 0.00001765
Iteration 60/1000 | Loss: 0.00001765
Iteration 61/1000 | Loss: 0.00001765
Iteration 62/1000 | Loss: 0.00001764
Iteration 63/1000 | Loss: 0.00001764
Iteration 64/1000 | Loss: 0.00001763
Iteration 65/1000 | Loss: 0.00001763
Iteration 66/1000 | Loss: 0.00001763
Iteration 67/1000 | Loss: 0.00001763
Iteration 68/1000 | Loss: 0.00001763
Iteration 69/1000 | Loss: 0.00001763
Iteration 70/1000 | Loss: 0.00001763
Iteration 71/1000 | Loss: 0.00001763
Iteration 72/1000 | Loss: 0.00001762
Iteration 73/1000 | Loss: 0.00001762
Iteration 74/1000 | Loss: 0.00001762
Iteration 75/1000 | Loss: 0.00001762
Iteration 76/1000 | Loss: 0.00001761
Iteration 77/1000 | Loss: 0.00001761
Iteration 78/1000 | Loss: 0.00001760
Iteration 79/1000 | Loss: 0.00001759
Iteration 80/1000 | Loss: 0.00001759
Iteration 81/1000 | Loss: 0.00001759
Iteration 82/1000 | Loss: 0.00001758
Iteration 83/1000 | Loss: 0.00001758
Iteration 84/1000 | Loss: 0.00001758
Iteration 85/1000 | Loss: 0.00001757
Iteration 86/1000 | Loss: 0.00001757
Iteration 87/1000 | Loss: 0.00001756
Iteration 88/1000 | Loss: 0.00001756
Iteration 89/1000 | Loss: 0.00001755
Iteration 90/1000 | Loss: 0.00001755
Iteration 91/1000 | Loss: 0.00001755
Iteration 92/1000 | Loss: 0.00001755
Iteration 93/1000 | Loss: 0.00001755
Iteration 94/1000 | Loss: 0.00001755
Iteration 95/1000 | Loss: 0.00001754
Iteration 96/1000 | Loss: 0.00001754
Iteration 97/1000 | Loss: 0.00001754
Iteration 98/1000 | Loss: 0.00001754
Iteration 99/1000 | Loss: 0.00001754
Iteration 100/1000 | Loss: 0.00001754
Iteration 101/1000 | Loss: 0.00001754
Iteration 102/1000 | Loss: 0.00001754
Iteration 103/1000 | Loss: 0.00001754
Iteration 104/1000 | Loss: 0.00001754
Iteration 105/1000 | Loss: 0.00001754
Iteration 106/1000 | Loss: 0.00001754
Iteration 107/1000 | Loss: 0.00001754
Iteration 108/1000 | Loss: 0.00001753
Iteration 109/1000 | Loss: 0.00001753
Iteration 110/1000 | Loss: 0.00001753
Iteration 111/1000 | Loss: 0.00001753
Iteration 112/1000 | Loss: 0.00001753
Iteration 113/1000 | Loss: 0.00001753
Iteration 114/1000 | Loss: 0.00001753
Iteration 115/1000 | Loss: 0.00001753
Iteration 116/1000 | Loss: 0.00001753
Iteration 117/1000 | Loss: 0.00001753
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 117. Stopping optimization.
Last 5 losses: [1.7532529454911128e-05, 1.7532529454911128e-05, 1.7532529454911128e-05, 1.7532529454911128e-05, 1.7532529454911128e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7532529454911128e-05

Optimization complete. Final v2v error: 3.5435245037078857 mm

Highest mean error: 4.294334888458252 mm for frame 94

Lowest mean error: 3.217101812362671 mm for frame 16

Saving results

Total time: 79.86328959465027
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_011/1030/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_011/1030.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_011/1030
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00976342
Iteration 2/25 | Loss: 0.00200253
Iteration 3/25 | Loss: 0.00150992
Iteration 4/25 | Loss: 0.00145734
Iteration 5/25 | Loss: 0.00140894
Iteration 6/25 | Loss: 0.00138180
Iteration 7/25 | Loss: 0.00136075
Iteration 8/25 | Loss: 0.00132608
Iteration 9/25 | Loss: 0.00131729
Iteration 10/25 | Loss: 0.00129863
Iteration 11/25 | Loss: 0.00129080
Iteration 12/25 | Loss: 0.00128820
Iteration 13/25 | Loss: 0.00128906
Iteration 14/25 | Loss: 0.00128967
Iteration 15/25 | Loss: 0.00128997
Iteration 16/25 | Loss: 0.00128619
Iteration 17/25 | Loss: 0.00128675
Iteration 18/25 | Loss: 0.00129232
Iteration 19/25 | Loss: 0.00128616
Iteration 20/25 | Loss: 0.00128401
Iteration 21/25 | Loss: 0.00128346
Iteration 22/25 | Loss: 0.00128335
Iteration 23/25 | Loss: 0.00128335
Iteration 24/25 | Loss: 0.00128335
Iteration 25/25 | Loss: 0.00128335

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33325648
Iteration 2/25 | Loss: 0.00097362
Iteration 3/25 | Loss: 0.00097362
Iteration 4/25 | Loss: 0.00097362
Iteration 5/25 | Loss: 0.00097362
Iteration 6/25 | Loss: 0.00097362
Iteration 7/25 | Loss: 0.00097362
Iteration 8/25 | Loss: 0.00097362
Iteration 9/25 | Loss: 0.00097362
Iteration 10/25 | Loss: 0.00097362
Iteration 11/25 | Loss: 0.00097362
Iteration 12/25 | Loss: 0.00097362
Iteration 13/25 | Loss: 0.00097362
Iteration 14/25 | Loss: 0.00097362
Iteration 15/25 | Loss: 0.00097362
Iteration 16/25 | Loss: 0.00097362
Iteration 17/25 | Loss: 0.00097362
Iteration 18/25 | Loss: 0.00097362
Iteration 19/25 | Loss: 0.00097362
Iteration 20/25 | Loss: 0.00097362
Iteration 21/25 | Loss: 0.00097362
Iteration 22/25 | Loss: 0.00097362
Iteration 23/25 | Loss: 0.00097362
Iteration 24/25 | Loss: 0.00097362
Iteration 25/25 | Loss: 0.00097362

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00097362
Iteration 2/1000 | Loss: 0.00005227
Iteration 3/1000 | Loss: 0.00017071
Iteration 4/1000 | Loss: 0.00003409
Iteration 5/1000 | Loss: 0.00007145
Iteration 6/1000 | Loss: 0.00004670
Iteration 7/1000 | Loss: 0.00004705
Iteration 8/1000 | Loss: 0.00006710
Iteration 9/1000 | Loss: 0.00003779
Iteration 10/1000 | Loss: 0.00028891
Iteration 11/1000 | Loss: 0.00025672
Iteration 12/1000 | Loss: 0.00016441
Iteration 13/1000 | Loss: 0.00002948
Iteration 14/1000 | Loss: 0.00002782
Iteration 15/1000 | Loss: 0.00002691
Iteration 16/1000 | Loss: 0.00002621
Iteration 17/1000 | Loss: 0.00002560
Iteration 18/1000 | Loss: 0.00002526
Iteration 19/1000 | Loss: 0.00002501
Iteration 20/1000 | Loss: 0.00002473
Iteration 21/1000 | Loss: 0.00002444
Iteration 22/1000 | Loss: 0.00002416
Iteration 23/1000 | Loss: 0.00002396
Iteration 24/1000 | Loss: 0.00002378
Iteration 25/1000 | Loss: 0.00002363
Iteration 26/1000 | Loss: 0.00002360
Iteration 27/1000 | Loss: 0.00002359
Iteration 28/1000 | Loss: 0.00002358
Iteration 29/1000 | Loss: 0.00002357
Iteration 30/1000 | Loss: 0.00002356
Iteration 31/1000 | Loss: 0.00002356
Iteration 32/1000 | Loss: 0.00002355
Iteration 33/1000 | Loss: 0.00002354
Iteration 34/1000 | Loss: 0.00002353
Iteration 35/1000 | Loss: 0.00002353
Iteration 36/1000 | Loss: 0.00002353
Iteration 37/1000 | Loss: 0.00002352
Iteration 38/1000 | Loss: 0.00002351
Iteration 39/1000 | Loss: 0.00002351
Iteration 40/1000 | Loss: 0.00002350
Iteration 41/1000 | Loss: 0.00002349
Iteration 42/1000 | Loss: 0.00002348
Iteration 43/1000 | Loss: 0.00002348
Iteration 44/1000 | Loss: 0.00002347
Iteration 45/1000 | Loss: 0.00002347
Iteration 46/1000 | Loss: 0.00002338
Iteration 47/1000 | Loss: 0.00002336
Iteration 48/1000 | Loss: 0.00002336
Iteration 49/1000 | Loss: 0.00002335
Iteration 50/1000 | Loss: 0.00002335
Iteration 51/1000 | Loss: 0.00002335
Iteration 52/1000 | Loss: 0.00002334
Iteration 53/1000 | Loss: 0.00002333
Iteration 54/1000 | Loss: 0.00002332
Iteration 55/1000 | Loss: 0.00002331
Iteration 56/1000 | Loss: 0.00002331
Iteration 57/1000 | Loss: 0.00002331
Iteration 58/1000 | Loss: 0.00002330
Iteration 59/1000 | Loss: 0.00002330
Iteration 60/1000 | Loss: 0.00002329
Iteration 61/1000 | Loss: 0.00002329
Iteration 62/1000 | Loss: 0.00002329
Iteration 63/1000 | Loss: 0.00002328
Iteration 64/1000 | Loss: 0.00002328
Iteration 65/1000 | Loss: 0.00002328
Iteration 66/1000 | Loss: 0.00002327
Iteration 67/1000 | Loss: 0.00002327
Iteration 68/1000 | Loss: 0.00002325
Iteration 69/1000 | Loss: 0.00002325
Iteration 70/1000 | Loss: 0.00002324
Iteration 71/1000 | Loss: 0.00002324
Iteration 72/1000 | Loss: 0.00002324
Iteration 73/1000 | Loss: 0.00002324
Iteration 74/1000 | Loss: 0.00002324
Iteration 75/1000 | Loss: 0.00002324
Iteration 76/1000 | Loss: 0.00002323
Iteration 77/1000 | Loss: 0.00002323
Iteration 78/1000 | Loss: 0.00002323
Iteration 79/1000 | Loss: 0.00002323
Iteration 80/1000 | Loss: 0.00002323
Iteration 81/1000 | Loss: 0.00002323
Iteration 82/1000 | Loss: 0.00002323
Iteration 83/1000 | Loss: 0.00002323
Iteration 84/1000 | Loss: 0.00002323
Iteration 85/1000 | Loss: 0.00002322
Iteration 86/1000 | Loss: 0.00002322
Iteration 87/1000 | Loss: 0.00002322
Iteration 88/1000 | Loss: 0.00002322
Iteration 89/1000 | Loss: 0.00002322
Iteration 90/1000 | Loss: 0.00002322
Iteration 91/1000 | Loss: 0.00002322
Iteration 92/1000 | Loss: 0.00002322
Iteration 93/1000 | Loss: 0.00002322
Iteration 94/1000 | Loss: 0.00002321
Iteration 95/1000 | Loss: 0.00002321
Iteration 96/1000 | Loss: 0.00002321
Iteration 97/1000 | Loss: 0.00002321
Iteration 98/1000 | Loss: 0.00002321
Iteration 99/1000 | Loss: 0.00002321
Iteration 100/1000 | Loss: 0.00002321
Iteration 101/1000 | Loss: 0.00002321
Iteration 102/1000 | Loss: 0.00002321
Iteration 103/1000 | Loss: 0.00002321
Iteration 104/1000 | Loss: 0.00002321
Iteration 105/1000 | Loss: 0.00002320
Iteration 106/1000 | Loss: 0.00002320
Iteration 107/1000 | Loss: 0.00002320
Iteration 108/1000 | Loss: 0.00002320
Iteration 109/1000 | Loss: 0.00002320
Iteration 110/1000 | Loss: 0.00002320
Iteration 111/1000 | Loss: 0.00002319
Iteration 112/1000 | Loss: 0.00002319
Iteration 113/1000 | Loss: 0.00002319
Iteration 114/1000 | Loss: 0.00002319
Iteration 115/1000 | Loss: 0.00002319
Iteration 116/1000 | Loss: 0.00002319
Iteration 117/1000 | Loss: 0.00002319
Iteration 118/1000 | Loss: 0.00002319
Iteration 119/1000 | Loss: 0.00002319
Iteration 120/1000 | Loss: 0.00002319
Iteration 121/1000 | Loss: 0.00002319
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 121. Stopping optimization.
Last 5 losses: [2.3194728782982565e-05, 2.3194728782982565e-05, 2.3194728782982565e-05, 2.3194728782982565e-05, 2.3194728782982565e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3194728782982565e-05

Optimization complete. Final v2v error: 4.069112300872803 mm

Highest mean error: 5.05437707901001 mm for frame 90

Lowest mean error: 3.452786445617676 mm for frame 24

Saving results

Total time: 93.67792749404907
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_011/1061/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_011/1061.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_011/1061
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00826158
Iteration 2/25 | Loss: 0.00180171
Iteration 3/25 | Loss: 0.00143445
Iteration 4/25 | Loss: 0.00139796
Iteration 5/25 | Loss: 0.00138942
Iteration 6/25 | Loss: 0.00137900
Iteration 7/25 | Loss: 0.00135121
Iteration 8/25 | Loss: 0.00134006
Iteration 9/25 | Loss: 0.00132334
Iteration 10/25 | Loss: 0.00132297
Iteration 11/25 | Loss: 0.00131922
Iteration 12/25 | Loss: 0.00132238
Iteration 13/25 | Loss: 0.00132075
Iteration 14/25 | Loss: 0.00130867
Iteration 15/25 | Loss: 0.00131088
Iteration 16/25 | Loss: 0.00130775
Iteration 17/25 | Loss: 0.00130985
Iteration 18/25 | Loss: 0.00131059
Iteration 19/25 | Loss: 0.00130588
Iteration 20/25 | Loss: 0.00130535
Iteration 21/25 | Loss: 0.00130530
Iteration 22/25 | Loss: 0.00130530
Iteration 23/25 | Loss: 0.00130530
Iteration 24/25 | Loss: 0.00130530
Iteration 25/25 | Loss: 0.00130529

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.63688827
Iteration 2/25 | Loss: 0.00085659
Iteration 3/25 | Loss: 0.00081380
Iteration 4/25 | Loss: 0.00081379
Iteration 5/25 | Loss: 0.00081379
Iteration 6/25 | Loss: 0.00081379
Iteration 7/25 | Loss: 0.00081379
Iteration 8/25 | Loss: 0.00081379
Iteration 9/25 | Loss: 0.00081379
Iteration 10/25 | Loss: 0.00081379
Iteration 11/25 | Loss: 0.00081379
Iteration 12/25 | Loss: 0.00081379
Iteration 13/25 | Loss: 0.00081379
Iteration 14/25 | Loss: 0.00081379
Iteration 15/25 | Loss: 0.00081379
Iteration 16/25 | Loss: 0.00081379
Iteration 17/25 | Loss: 0.00081379
Iteration 18/25 | Loss: 0.00081379
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0008137921104207635, 0.0008137921104207635, 0.0008137921104207635, 0.0008137921104207635, 0.0008137921104207635]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008137921104207635

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00081379
Iteration 2/1000 | Loss: 0.00007324
Iteration 3/1000 | Loss: 0.00002499
Iteration 4/1000 | Loss: 0.00004504
Iteration 5/1000 | Loss: 0.00003136
Iteration 6/1000 | Loss: 0.00002842
Iteration 7/1000 | Loss: 0.00002046
Iteration 8/1000 | Loss: 0.00001996
Iteration 9/1000 | Loss: 0.00001942
Iteration 10/1000 | Loss: 0.00001910
Iteration 11/1000 | Loss: 0.00001888
Iteration 12/1000 | Loss: 0.00001879
Iteration 13/1000 | Loss: 0.00001862
Iteration 14/1000 | Loss: 0.00001856
Iteration 15/1000 | Loss: 0.00001856
Iteration 16/1000 | Loss: 0.00001855
Iteration 17/1000 | Loss: 0.00001851
Iteration 18/1000 | Loss: 0.00001845
Iteration 19/1000 | Loss: 0.00001844
Iteration 20/1000 | Loss: 0.00001839
Iteration 21/1000 | Loss: 0.00001837
Iteration 22/1000 | Loss: 0.00001837
Iteration 23/1000 | Loss: 0.00001837
Iteration 24/1000 | Loss: 0.00001837
Iteration 25/1000 | Loss: 0.00001837
Iteration 26/1000 | Loss: 0.00001837
Iteration 27/1000 | Loss: 0.00001837
Iteration 28/1000 | Loss: 0.00001836
Iteration 29/1000 | Loss: 0.00001836
Iteration 30/1000 | Loss: 0.00001836
Iteration 31/1000 | Loss: 0.00001836
Iteration 32/1000 | Loss: 0.00001835
Iteration 33/1000 | Loss: 0.00001834
Iteration 34/1000 | Loss: 0.00001834
Iteration 35/1000 | Loss: 0.00001834
Iteration 36/1000 | Loss: 0.00001834
Iteration 37/1000 | Loss: 0.00001834
Iteration 38/1000 | Loss: 0.00001834
Iteration 39/1000 | Loss: 0.00001834
Iteration 40/1000 | Loss: 0.00001834
Iteration 41/1000 | Loss: 0.00001833
Iteration 42/1000 | Loss: 0.00001833
Iteration 43/1000 | Loss: 0.00001831
Iteration 44/1000 | Loss: 0.00001831
Iteration 45/1000 | Loss: 0.00001831
Iteration 46/1000 | Loss: 0.00001831
Iteration 47/1000 | Loss: 0.00001831
Iteration 48/1000 | Loss: 0.00001831
Iteration 49/1000 | Loss: 0.00001831
Iteration 50/1000 | Loss: 0.00001831
Iteration 51/1000 | Loss: 0.00001830
Iteration 52/1000 | Loss: 0.00001830
Iteration 53/1000 | Loss: 0.00001830
Iteration 54/1000 | Loss: 0.00001830
Iteration 55/1000 | Loss: 0.00001830
Iteration 56/1000 | Loss: 0.00001830
Iteration 57/1000 | Loss: 0.00001830
Iteration 58/1000 | Loss: 0.00001830
Iteration 59/1000 | Loss: 0.00001830
Iteration 60/1000 | Loss: 0.00001830
Iteration 61/1000 | Loss: 0.00001830
Iteration 62/1000 | Loss: 0.00001830
Iteration 63/1000 | Loss: 0.00001829
Iteration 64/1000 | Loss: 0.00001829
Iteration 65/1000 | Loss: 0.00001829
Iteration 66/1000 | Loss: 0.00001829
Iteration 67/1000 | Loss: 0.00001829
Iteration 68/1000 | Loss: 0.00001829
Iteration 69/1000 | Loss: 0.00001829
Iteration 70/1000 | Loss: 0.00001828
Iteration 71/1000 | Loss: 0.00001828
Iteration 72/1000 | Loss: 0.00001828
Iteration 73/1000 | Loss: 0.00001827
Iteration 74/1000 | Loss: 0.00001827
Iteration 75/1000 | Loss: 0.00001827
Iteration 76/1000 | Loss: 0.00001826
Iteration 77/1000 | Loss: 0.00001825
Iteration 78/1000 | Loss: 0.00001825
Iteration 79/1000 | Loss: 0.00001825
Iteration 80/1000 | Loss: 0.00001824
Iteration 81/1000 | Loss: 0.00001824
Iteration 82/1000 | Loss: 0.00001824
Iteration 83/1000 | Loss: 0.00001824
Iteration 84/1000 | Loss: 0.00001824
Iteration 85/1000 | Loss: 0.00001824
Iteration 86/1000 | Loss: 0.00001824
Iteration 87/1000 | Loss: 0.00001824
Iteration 88/1000 | Loss: 0.00001823
Iteration 89/1000 | Loss: 0.00001823
Iteration 90/1000 | Loss: 0.00001823
Iteration 91/1000 | Loss: 0.00001823
Iteration 92/1000 | Loss: 0.00001823
Iteration 93/1000 | Loss: 0.00001823
Iteration 94/1000 | Loss: 0.00001823
Iteration 95/1000 | Loss: 0.00001822
Iteration 96/1000 | Loss: 0.00001821
Iteration 97/1000 | Loss: 0.00001821
Iteration 98/1000 | Loss: 0.00001821
Iteration 99/1000 | Loss: 0.00001821
Iteration 100/1000 | Loss: 0.00001821
Iteration 101/1000 | Loss: 0.00001821
Iteration 102/1000 | Loss: 0.00001820
Iteration 103/1000 | Loss: 0.00001820
Iteration 104/1000 | Loss: 0.00001820
Iteration 105/1000 | Loss: 0.00001820
Iteration 106/1000 | Loss: 0.00001820
Iteration 107/1000 | Loss: 0.00001820
Iteration 108/1000 | Loss: 0.00001820
Iteration 109/1000 | Loss: 0.00001820
Iteration 110/1000 | Loss: 0.00001820
Iteration 111/1000 | Loss: 0.00001820
Iteration 112/1000 | Loss: 0.00001820
Iteration 113/1000 | Loss: 0.00001820
Iteration 114/1000 | Loss: 0.00001820
Iteration 115/1000 | Loss: 0.00001820
Iteration 116/1000 | Loss: 0.00001820
Iteration 117/1000 | Loss: 0.00001820
Iteration 118/1000 | Loss: 0.00001820
Iteration 119/1000 | Loss: 0.00001820
Iteration 120/1000 | Loss: 0.00001820
Iteration 121/1000 | Loss: 0.00001820
Iteration 122/1000 | Loss: 0.00001820
Iteration 123/1000 | Loss: 0.00001820
Iteration 124/1000 | Loss: 0.00001820
Iteration 125/1000 | Loss: 0.00001820
Iteration 126/1000 | Loss: 0.00001820
Iteration 127/1000 | Loss: 0.00001820
Iteration 128/1000 | Loss: 0.00001820
Iteration 129/1000 | Loss: 0.00001820
Iteration 130/1000 | Loss: 0.00001820
Iteration 131/1000 | Loss: 0.00001820
Iteration 132/1000 | Loss: 0.00001820
Iteration 133/1000 | Loss: 0.00001820
Iteration 134/1000 | Loss: 0.00001820
Iteration 135/1000 | Loss: 0.00001820
Iteration 136/1000 | Loss: 0.00001820
Iteration 137/1000 | Loss: 0.00001820
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 137. Stopping optimization.
Last 5 losses: [1.819678072934039e-05, 1.819678072934039e-05, 1.819678072934039e-05, 1.819678072934039e-05, 1.819678072934039e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.819678072934039e-05

Optimization complete. Final v2v error: 3.574218273162842 mm

Highest mean error: 3.942594051361084 mm for frame 19

Lowest mean error: 3.1902759075164795 mm for frame 238

Saving results

Total time: 73.2022054195404
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_011/1082/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_011/1082.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_011/1082
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00806481
Iteration 2/25 | Loss: 0.00132810
Iteration 3/25 | Loss: 0.00123316
Iteration 4/25 | Loss: 0.00122359
Iteration 5/25 | Loss: 0.00122065
Iteration 6/25 | Loss: 0.00122028
Iteration 7/25 | Loss: 0.00122028
Iteration 8/25 | Loss: 0.00122028
Iteration 9/25 | Loss: 0.00122028
Iteration 10/25 | Loss: 0.00122028
Iteration 11/25 | Loss: 0.00122028
Iteration 12/25 | Loss: 0.00122028
Iteration 13/25 | Loss: 0.00122028
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0012202768120914698, 0.0012202768120914698, 0.0012202768120914698, 0.0012202768120914698, 0.0012202768120914698]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012202768120914698

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.37069225
Iteration 2/25 | Loss: 0.00103348
Iteration 3/25 | Loss: 0.00103347
Iteration 4/25 | Loss: 0.00103347
Iteration 5/25 | Loss: 0.00103347
Iteration 6/25 | Loss: 0.00103347
Iteration 7/25 | Loss: 0.00103347
Iteration 8/25 | Loss: 0.00103347
Iteration 9/25 | Loss: 0.00103347
Iteration 10/25 | Loss: 0.00103347
Iteration 11/25 | Loss: 0.00103347
Iteration 12/25 | Loss: 0.00103347
Iteration 13/25 | Loss: 0.00103347
Iteration 14/25 | Loss: 0.00103347
Iteration 15/25 | Loss: 0.00103347
Iteration 16/25 | Loss: 0.00103347
Iteration 17/25 | Loss: 0.00103347
Iteration 18/25 | Loss: 0.00103347
Iteration 19/25 | Loss: 0.00103347
Iteration 20/25 | Loss: 0.00103347
Iteration 21/25 | Loss: 0.00103347
Iteration 22/25 | Loss: 0.00103347
Iteration 23/25 | Loss: 0.00103347
Iteration 24/25 | Loss: 0.00103347
Iteration 25/25 | Loss: 0.00103347

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00103347
Iteration 2/1000 | Loss: 0.00003098
Iteration 3/1000 | Loss: 0.00002074
Iteration 4/1000 | Loss: 0.00001698
Iteration 5/1000 | Loss: 0.00001571
Iteration 6/1000 | Loss: 0.00001484
Iteration 7/1000 | Loss: 0.00001428
Iteration 8/1000 | Loss: 0.00001386
Iteration 9/1000 | Loss: 0.00001382
Iteration 10/1000 | Loss: 0.00001361
Iteration 11/1000 | Loss: 0.00001333
Iteration 12/1000 | Loss: 0.00001311
Iteration 13/1000 | Loss: 0.00001298
Iteration 14/1000 | Loss: 0.00001297
Iteration 15/1000 | Loss: 0.00001292
Iteration 16/1000 | Loss: 0.00001283
Iteration 17/1000 | Loss: 0.00001283
Iteration 18/1000 | Loss: 0.00001278
Iteration 19/1000 | Loss: 0.00001278
Iteration 20/1000 | Loss: 0.00001278
Iteration 21/1000 | Loss: 0.00001278
Iteration 22/1000 | Loss: 0.00001278
Iteration 23/1000 | Loss: 0.00001278
Iteration 24/1000 | Loss: 0.00001278
Iteration 25/1000 | Loss: 0.00001278
Iteration 26/1000 | Loss: 0.00001271
Iteration 27/1000 | Loss: 0.00001268
Iteration 28/1000 | Loss: 0.00001267
Iteration 29/1000 | Loss: 0.00001267
Iteration 30/1000 | Loss: 0.00001267
Iteration 31/1000 | Loss: 0.00001266
Iteration 32/1000 | Loss: 0.00001266
Iteration 33/1000 | Loss: 0.00001266
Iteration 34/1000 | Loss: 0.00001265
Iteration 35/1000 | Loss: 0.00001265
Iteration 36/1000 | Loss: 0.00001265
Iteration 37/1000 | Loss: 0.00001264
Iteration 38/1000 | Loss: 0.00001263
Iteration 39/1000 | Loss: 0.00001263
Iteration 40/1000 | Loss: 0.00001263
Iteration 41/1000 | Loss: 0.00001263
Iteration 42/1000 | Loss: 0.00001262
Iteration 43/1000 | Loss: 0.00001262
Iteration 44/1000 | Loss: 0.00001262
Iteration 45/1000 | Loss: 0.00001261
Iteration 46/1000 | Loss: 0.00001260
Iteration 47/1000 | Loss: 0.00001260
Iteration 48/1000 | Loss: 0.00001260
Iteration 49/1000 | Loss: 0.00001259
Iteration 50/1000 | Loss: 0.00001259
Iteration 51/1000 | Loss: 0.00001259
Iteration 52/1000 | Loss: 0.00001258
Iteration 53/1000 | Loss: 0.00001258
Iteration 54/1000 | Loss: 0.00001257
Iteration 55/1000 | Loss: 0.00001257
Iteration 56/1000 | Loss: 0.00001256
Iteration 57/1000 | Loss: 0.00001256
Iteration 58/1000 | Loss: 0.00001254
Iteration 59/1000 | Loss: 0.00001254
Iteration 60/1000 | Loss: 0.00001253
Iteration 61/1000 | Loss: 0.00001251
Iteration 62/1000 | Loss: 0.00001251
Iteration 63/1000 | Loss: 0.00001250
Iteration 64/1000 | Loss: 0.00001250
Iteration 65/1000 | Loss: 0.00001250
Iteration 66/1000 | Loss: 0.00001249
Iteration 67/1000 | Loss: 0.00001249
Iteration 68/1000 | Loss: 0.00001248
Iteration 69/1000 | Loss: 0.00001248
Iteration 70/1000 | Loss: 0.00001246
Iteration 71/1000 | Loss: 0.00001246
Iteration 72/1000 | Loss: 0.00001245
Iteration 73/1000 | Loss: 0.00001245
Iteration 74/1000 | Loss: 0.00001245
Iteration 75/1000 | Loss: 0.00001245
Iteration 76/1000 | Loss: 0.00001244
Iteration 77/1000 | Loss: 0.00001244
Iteration 78/1000 | Loss: 0.00001244
Iteration 79/1000 | Loss: 0.00001243
Iteration 80/1000 | Loss: 0.00001243
Iteration 81/1000 | Loss: 0.00001242
Iteration 82/1000 | Loss: 0.00001242
Iteration 83/1000 | Loss: 0.00001242
Iteration 84/1000 | Loss: 0.00001241
Iteration 85/1000 | Loss: 0.00001241
Iteration 86/1000 | Loss: 0.00001241
Iteration 87/1000 | Loss: 0.00001240
Iteration 88/1000 | Loss: 0.00001240
Iteration 89/1000 | Loss: 0.00001240
Iteration 90/1000 | Loss: 0.00001239
Iteration 91/1000 | Loss: 0.00001239
Iteration 92/1000 | Loss: 0.00001239
Iteration 93/1000 | Loss: 0.00001239
Iteration 94/1000 | Loss: 0.00001239
Iteration 95/1000 | Loss: 0.00001239
Iteration 96/1000 | Loss: 0.00001238
Iteration 97/1000 | Loss: 0.00001238
Iteration 98/1000 | Loss: 0.00001238
Iteration 99/1000 | Loss: 0.00001238
Iteration 100/1000 | Loss: 0.00001238
Iteration 101/1000 | Loss: 0.00001238
Iteration 102/1000 | Loss: 0.00001238
Iteration 103/1000 | Loss: 0.00001238
Iteration 104/1000 | Loss: 0.00001238
Iteration 105/1000 | Loss: 0.00001237
Iteration 106/1000 | Loss: 0.00001237
Iteration 107/1000 | Loss: 0.00001237
Iteration 108/1000 | Loss: 0.00001237
Iteration 109/1000 | Loss: 0.00001237
Iteration 110/1000 | Loss: 0.00001237
Iteration 111/1000 | Loss: 0.00001237
Iteration 112/1000 | Loss: 0.00001237
Iteration 113/1000 | Loss: 0.00001237
Iteration 114/1000 | Loss: 0.00001237
Iteration 115/1000 | Loss: 0.00001237
Iteration 116/1000 | Loss: 0.00001237
Iteration 117/1000 | Loss: 0.00001237
Iteration 118/1000 | Loss: 0.00001237
Iteration 119/1000 | Loss: 0.00001237
Iteration 120/1000 | Loss: 0.00001237
Iteration 121/1000 | Loss: 0.00001237
Iteration 122/1000 | Loss: 0.00001237
Iteration 123/1000 | Loss: 0.00001237
Iteration 124/1000 | Loss: 0.00001237
Iteration 125/1000 | Loss: 0.00001237
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 125. Stopping optimization.
Last 5 losses: [1.2372031960694585e-05, 1.2372031960694585e-05, 1.2372031960694585e-05, 1.2372031960694585e-05, 1.2372031960694585e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2372031960694585e-05

Optimization complete. Final v2v error: 2.9982645511627197 mm

Highest mean error: 4.009810924530029 mm for frame 86

Lowest mean error: 2.710592746734619 mm for frame 116

Saving results

Total time: 37.96598172187805
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_011/1067/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_011/1067.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_011/1067
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00904403
Iteration 2/25 | Loss: 0.00169692
Iteration 3/25 | Loss: 0.00144427
Iteration 4/25 | Loss: 0.00141835
Iteration 5/25 | Loss: 0.00141359
Iteration 6/25 | Loss: 0.00141222
Iteration 7/25 | Loss: 0.00141213
Iteration 8/25 | Loss: 0.00141202
Iteration 9/25 | Loss: 0.00141202
Iteration 10/25 | Loss: 0.00141202
Iteration 11/25 | Loss: 0.00141202
Iteration 12/25 | Loss: 0.00141202
Iteration 13/25 | Loss: 0.00141202
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0014120230916887522, 0.0014120230916887522, 0.0014120230916887522, 0.0014120230916887522, 0.0014120230916887522]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014120230916887522

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.55193579
Iteration 2/25 | Loss: 0.00116496
Iteration 3/25 | Loss: 0.00116496
Iteration 4/25 | Loss: 0.00116496
Iteration 5/25 | Loss: 0.00116496
Iteration 6/25 | Loss: 0.00116496
Iteration 7/25 | Loss: 0.00116496
Iteration 8/25 | Loss: 0.00116495
Iteration 9/25 | Loss: 0.00116495
Iteration 10/25 | Loss: 0.00116495
Iteration 11/25 | Loss: 0.00116495
Iteration 12/25 | Loss: 0.00116495
Iteration 13/25 | Loss: 0.00116495
Iteration 14/25 | Loss: 0.00116495
Iteration 15/25 | Loss: 0.00116495
Iteration 16/25 | Loss: 0.00116495
Iteration 17/25 | Loss: 0.00116495
Iteration 18/25 | Loss: 0.00116495
Iteration 19/25 | Loss: 0.00116495
Iteration 20/25 | Loss: 0.00116495
Iteration 21/25 | Loss: 0.00116495
Iteration 22/25 | Loss: 0.00116495
Iteration 23/25 | Loss: 0.00116495
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0011649533407762647, 0.0011649533407762647, 0.0011649533407762647, 0.0011649533407762647, 0.0011649533407762647]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011649533407762647

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00116495
Iteration 2/1000 | Loss: 0.00007144
Iteration 3/1000 | Loss: 0.00004015
Iteration 4/1000 | Loss: 0.00003130
Iteration 5/1000 | Loss: 0.00002908
Iteration 6/1000 | Loss: 0.00002797
Iteration 7/1000 | Loss: 0.00002714
Iteration 8/1000 | Loss: 0.00002657
Iteration 9/1000 | Loss: 0.00002606
Iteration 10/1000 | Loss: 0.00002578
Iteration 11/1000 | Loss: 0.00002553
Iteration 12/1000 | Loss: 0.00002534
Iteration 13/1000 | Loss: 0.00002522
Iteration 14/1000 | Loss: 0.00002507
Iteration 15/1000 | Loss: 0.00002495
Iteration 16/1000 | Loss: 0.00002492
Iteration 17/1000 | Loss: 0.00002489
Iteration 18/1000 | Loss: 0.00002487
Iteration 19/1000 | Loss: 0.00002482
Iteration 20/1000 | Loss: 0.00002481
Iteration 21/1000 | Loss: 0.00002478
Iteration 22/1000 | Loss: 0.00002478
Iteration 23/1000 | Loss: 0.00002478
Iteration 24/1000 | Loss: 0.00002478
Iteration 25/1000 | Loss: 0.00002478
Iteration 26/1000 | Loss: 0.00002478
Iteration 27/1000 | Loss: 0.00002478
Iteration 28/1000 | Loss: 0.00002478
Iteration 29/1000 | Loss: 0.00002478
Iteration 30/1000 | Loss: 0.00002478
Iteration 31/1000 | Loss: 0.00002478
Iteration 32/1000 | Loss: 0.00002477
Iteration 33/1000 | Loss: 0.00002477
Iteration 34/1000 | Loss: 0.00002476
Iteration 35/1000 | Loss: 0.00002476
Iteration 36/1000 | Loss: 0.00002475
Iteration 37/1000 | Loss: 0.00002474
Iteration 38/1000 | Loss: 0.00002474
Iteration 39/1000 | Loss: 0.00002474
Iteration 40/1000 | Loss: 0.00002474
Iteration 41/1000 | Loss: 0.00002474
Iteration 42/1000 | Loss: 0.00002473
Iteration 43/1000 | Loss: 0.00002473
Iteration 44/1000 | Loss: 0.00002472
Iteration 45/1000 | Loss: 0.00002472
Iteration 46/1000 | Loss: 0.00002472
Iteration 47/1000 | Loss: 0.00002471
Iteration 48/1000 | Loss: 0.00002471
Iteration 49/1000 | Loss: 0.00002471
Iteration 50/1000 | Loss: 0.00002471
Iteration 51/1000 | Loss: 0.00002471
Iteration 52/1000 | Loss: 0.00002471
Iteration 53/1000 | Loss: 0.00002470
Iteration 54/1000 | Loss: 0.00002470
Iteration 55/1000 | Loss: 0.00002470
Iteration 56/1000 | Loss: 0.00002470
Iteration 57/1000 | Loss: 0.00002470
Iteration 58/1000 | Loss: 0.00002470
Iteration 59/1000 | Loss: 0.00002470
Iteration 60/1000 | Loss: 0.00002469
Iteration 61/1000 | Loss: 0.00002469
Iteration 62/1000 | Loss: 0.00002469
Iteration 63/1000 | Loss: 0.00002469
Iteration 64/1000 | Loss: 0.00002469
Iteration 65/1000 | Loss: 0.00002469
Iteration 66/1000 | Loss: 0.00002469
Iteration 67/1000 | Loss: 0.00002469
Iteration 68/1000 | Loss: 0.00002469
Iteration 69/1000 | Loss: 0.00002469
Iteration 70/1000 | Loss: 0.00002469
Iteration 71/1000 | Loss: 0.00002468
Iteration 72/1000 | Loss: 0.00002468
Iteration 73/1000 | Loss: 0.00002468
Iteration 74/1000 | Loss: 0.00002468
Iteration 75/1000 | Loss: 0.00002467
Iteration 76/1000 | Loss: 0.00002467
Iteration 77/1000 | Loss: 0.00002467
Iteration 78/1000 | Loss: 0.00002467
Iteration 79/1000 | Loss: 0.00002466
Iteration 80/1000 | Loss: 0.00002466
Iteration 81/1000 | Loss: 0.00002466
Iteration 82/1000 | Loss: 0.00002466
Iteration 83/1000 | Loss: 0.00002466
Iteration 84/1000 | Loss: 0.00002466
Iteration 85/1000 | Loss: 0.00002465
Iteration 86/1000 | Loss: 0.00002465
Iteration 87/1000 | Loss: 0.00002465
Iteration 88/1000 | Loss: 0.00002465
Iteration 89/1000 | Loss: 0.00002465
Iteration 90/1000 | Loss: 0.00002464
Iteration 91/1000 | Loss: 0.00002464
Iteration 92/1000 | Loss: 0.00002464
Iteration 93/1000 | Loss: 0.00002464
Iteration 94/1000 | Loss: 0.00002464
Iteration 95/1000 | Loss: 0.00002464
Iteration 96/1000 | Loss: 0.00002464
Iteration 97/1000 | Loss: 0.00002464
Iteration 98/1000 | Loss: 0.00002464
Iteration 99/1000 | Loss: 0.00002464
Iteration 100/1000 | Loss: 0.00002464
Iteration 101/1000 | Loss: 0.00002464
Iteration 102/1000 | Loss: 0.00002464
Iteration 103/1000 | Loss: 0.00002464
Iteration 104/1000 | Loss: 0.00002464
Iteration 105/1000 | Loss: 0.00002464
Iteration 106/1000 | Loss: 0.00002464
Iteration 107/1000 | Loss: 0.00002464
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 107. Stopping optimization.
Last 5 losses: [2.4639326511533e-05, 2.4639326511533e-05, 2.4639326511533e-05, 2.4639326511533e-05, 2.4639326511533e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4639326511533e-05

Optimization complete. Final v2v error: 4.1869001388549805 mm

Highest mean error: 4.640313625335693 mm for frame 19

Lowest mean error: 3.785804510116577 mm for frame 42

Saving results

Total time: 37.55208969116211
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_011/1057/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_011/1057.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_011/1057
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00359647
Iteration 2/25 | Loss: 0.00135695
Iteration 3/25 | Loss: 0.00122682
Iteration 4/25 | Loss: 0.00120759
Iteration 5/25 | Loss: 0.00120071
Iteration 6/25 | Loss: 0.00119914
Iteration 7/25 | Loss: 0.00119914
Iteration 8/25 | Loss: 0.00119914
Iteration 9/25 | Loss: 0.00119914
Iteration 10/25 | Loss: 0.00119914
Iteration 11/25 | Loss: 0.00119914
Iteration 12/25 | Loss: 0.00119914
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0011991433566436172, 0.0011991433566436172, 0.0011991433566436172, 0.0011991433566436172, 0.0011991433566436172]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011991433566436172

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33889103
Iteration 2/25 | Loss: 0.00129313
Iteration 3/25 | Loss: 0.00129313
Iteration 4/25 | Loss: 0.00129313
Iteration 5/25 | Loss: 0.00129313
Iteration 6/25 | Loss: 0.00129313
Iteration 7/25 | Loss: 0.00129313
Iteration 8/25 | Loss: 0.00129313
Iteration 9/25 | Loss: 0.00129313
Iteration 10/25 | Loss: 0.00129313
Iteration 11/25 | Loss: 0.00129313
Iteration 12/25 | Loss: 0.00129313
Iteration 13/25 | Loss: 0.00129313
Iteration 14/25 | Loss: 0.00129313
Iteration 15/25 | Loss: 0.00129313
Iteration 16/25 | Loss: 0.00129313
Iteration 17/25 | Loss: 0.00129313
Iteration 18/25 | Loss: 0.00129313
Iteration 19/25 | Loss: 0.00129313
Iteration 20/25 | Loss: 0.00129313
Iteration 21/25 | Loss: 0.00129313
Iteration 22/25 | Loss: 0.00129313
Iteration 23/25 | Loss: 0.00129313
Iteration 24/25 | Loss: 0.00129313
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0012931257952004671, 0.0012931257952004671, 0.0012931257952004671, 0.0012931257952004671, 0.0012931257952004671]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012931257952004671

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00129313
Iteration 2/1000 | Loss: 0.00005315
Iteration 3/1000 | Loss: 0.00003461
Iteration 4/1000 | Loss: 0.00002742
Iteration 5/1000 | Loss: 0.00002574
Iteration 6/1000 | Loss: 0.00002447
Iteration 7/1000 | Loss: 0.00002334
Iteration 8/1000 | Loss: 0.00002276
Iteration 9/1000 | Loss: 0.00002237
Iteration 10/1000 | Loss: 0.00002207
Iteration 11/1000 | Loss: 0.00002175
Iteration 12/1000 | Loss: 0.00002146
Iteration 13/1000 | Loss: 0.00002126
Iteration 14/1000 | Loss: 0.00002115
Iteration 15/1000 | Loss: 0.00002115
Iteration 16/1000 | Loss: 0.00002110
Iteration 17/1000 | Loss: 0.00002109
Iteration 18/1000 | Loss: 0.00002108
Iteration 19/1000 | Loss: 0.00002108
Iteration 20/1000 | Loss: 0.00002107
Iteration 21/1000 | Loss: 0.00002105
Iteration 22/1000 | Loss: 0.00002101
Iteration 23/1000 | Loss: 0.00002100
Iteration 24/1000 | Loss: 0.00002100
Iteration 25/1000 | Loss: 0.00002099
Iteration 26/1000 | Loss: 0.00002094
Iteration 27/1000 | Loss: 0.00002091
Iteration 28/1000 | Loss: 0.00002086
Iteration 29/1000 | Loss: 0.00002085
Iteration 30/1000 | Loss: 0.00002084
Iteration 31/1000 | Loss: 0.00002083
Iteration 32/1000 | Loss: 0.00002082
Iteration 33/1000 | Loss: 0.00002080
Iteration 34/1000 | Loss: 0.00002080
Iteration 35/1000 | Loss: 0.00002079
Iteration 36/1000 | Loss: 0.00002076
Iteration 37/1000 | Loss: 0.00002076
Iteration 38/1000 | Loss: 0.00002074
Iteration 39/1000 | Loss: 0.00002074
Iteration 40/1000 | Loss: 0.00002073
Iteration 41/1000 | Loss: 0.00002073
Iteration 42/1000 | Loss: 0.00002072
Iteration 43/1000 | Loss: 0.00002072
Iteration 44/1000 | Loss: 0.00002069
Iteration 45/1000 | Loss: 0.00002066
Iteration 46/1000 | Loss: 0.00002066
Iteration 47/1000 | Loss: 0.00002065
Iteration 48/1000 | Loss: 0.00002065
Iteration 49/1000 | Loss: 0.00002064
Iteration 50/1000 | Loss: 0.00002064
Iteration 51/1000 | Loss: 0.00002064
Iteration 52/1000 | Loss: 0.00002064
Iteration 53/1000 | Loss: 0.00002064
Iteration 54/1000 | Loss: 0.00002064
Iteration 55/1000 | Loss: 0.00002063
Iteration 56/1000 | Loss: 0.00002062
Iteration 57/1000 | Loss: 0.00002061
Iteration 58/1000 | Loss: 0.00002060
Iteration 59/1000 | Loss: 0.00002060
Iteration 60/1000 | Loss: 0.00002059
Iteration 61/1000 | Loss: 0.00002059
Iteration 62/1000 | Loss: 0.00002059
Iteration 63/1000 | Loss: 0.00002058
Iteration 64/1000 | Loss: 0.00002056
Iteration 65/1000 | Loss: 0.00002056
Iteration 66/1000 | Loss: 0.00002055
Iteration 67/1000 | Loss: 0.00002055
Iteration 68/1000 | Loss: 0.00002054
Iteration 69/1000 | Loss: 0.00002054
Iteration 70/1000 | Loss: 0.00002054
Iteration 71/1000 | Loss: 0.00002053
Iteration 72/1000 | Loss: 0.00002053
Iteration 73/1000 | Loss: 0.00002052
Iteration 74/1000 | Loss: 0.00002052
Iteration 75/1000 | Loss: 0.00002052
Iteration 76/1000 | Loss: 0.00002051
Iteration 77/1000 | Loss: 0.00002051
Iteration 78/1000 | Loss: 0.00002050
Iteration 79/1000 | Loss: 0.00002049
Iteration 80/1000 | Loss: 0.00002049
Iteration 81/1000 | Loss: 0.00002048
Iteration 82/1000 | Loss: 0.00002047
Iteration 83/1000 | Loss: 0.00002047
Iteration 84/1000 | Loss: 0.00002046
Iteration 85/1000 | Loss: 0.00002046
Iteration 86/1000 | Loss: 0.00002046
Iteration 87/1000 | Loss: 0.00002045
Iteration 88/1000 | Loss: 0.00002045
Iteration 89/1000 | Loss: 0.00002045
Iteration 90/1000 | Loss: 0.00002044
Iteration 91/1000 | Loss: 0.00002044
Iteration 92/1000 | Loss: 0.00002044
Iteration 93/1000 | Loss: 0.00002043
Iteration 94/1000 | Loss: 0.00002043
Iteration 95/1000 | Loss: 0.00002043
Iteration 96/1000 | Loss: 0.00002043
Iteration 97/1000 | Loss: 0.00002043
Iteration 98/1000 | Loss: 0.00002043
Iteration 99/1000 | Loss: 0.00002043
Iteration 100/1000 | Loss: 0.00002042
Iteration 101/1000 | Loss: 0.00002042
Iteration 102/1000 | Loss: 0.00002042
Iteration 103/1000 | Loss: 0.00002042
Iteration 104/1000 | Loss: 0.00002042
Iteration 105/1000 | Loss: 0.00002042
Iteration 106/1000 | Loss: 0.00002042
Iteration 107/1000 | Loss: 0.00002042
Iteration 108/1000 | Loss: 0.00002042
Iteration 109/1000 | Loss: 0.00002042
Iteration 110/1000 | Loss: 0.00002041
Iteration 111/1000 | Loss: 0.00002041
Iteration 112/1000 | Loss: 0.00002041
Iteration 113/1000 | Loss: 0.00002041
Iteration 114/1000 | Loss: 0.00002041
Iteration 115/1000 | Loss: 0.00002041
Iteration 116/1000 | Loss: 0.00002041
Iteration 117/1000 | Loss: 0.00002040
Iteration 118/1000 | Loss: 0.00002040
Iteration 119/1000 | Loss: 0.00002040
Iteration 120/1000 | Loss: 0.00002040
Iteration 121/1000 | Loss: 0.00002040
Iteration 122/1000 | Loss: 0.00002040
Iteration 123/1000 | Loss: 0.00002040
Iteration 124/1000 | Loss: 0.00002040
Iteration 125/1000 | Loss: 0.00002040
Iteration 126/1000 | Loss: 0.00002040
Iteration 127/1000 | Loss: 0.00002040
Iteration 128/1000 | Loss: 0.00002040
Iteration 129/1000 | Loss: 0.00002040
Iteration 130/1000 | Loss: 0.00002039
Iteration 131/1000 | Loss: 0.00002039
Iteration 132/1000 | Loss: 0.00002039
Iteration 133/1000 | Loss: 0.00002039
Iteration 134/1000 | Loss: 0.00002039
Iteration 135/1000 | Loss: 0.00002039
Iteration 136/1000 | Loss: 0.00002039
Iteration 137/1000 | Loss: 0.00002039
Iteration 138/1000 | Loss: 0.00002039
Iteration 139/1000 | Loss: 0.00002039
Iteration 140/1000 | Loss: 0.00002039
Iteration 141/1000 | Loss: 0.00002039
Iteration 142/1000 | Loss: 0.00002039
Iteration 143/1000 | Loss: 0.00002039
Iteration 144/1000 | Loss: 0.00002038
Iteration 145/1000 | Loss: 0.00002038
Iteration 146/1000 | Loss: 0.00002038
Iteration 147/1000 | Loss: 0.00002038
Iteration 148/1000 | Loss: 0.00002038
Iteration 149/1000 | Loss: 0.00002038
Iteration 150/1000 | Loss: 0.00002038
Iteration 151/1000 | Loss: 0.00002038
Iteration 152/1000 | Loss: 0.00002038
Iteration 153/1000 | Loss: 0.00002038
Iteration 154/1000 | Loss: 0.00002038
Iteration 155/1000 | Loss: 0.00002038
Iteration 156/1000 | Loss: 0.00002038
Iteration 157/1000 | Loss: 0.00002038
Iteration 158/1000 | Loss: 0.00002038
Iteration 159/1000 | Loss: 0.00002038
Iteration 160/1000 | Loss: 0.00002037
Iteration 161/1000 | Loss: 0.00002037
Iteration 162/1000 | Loss: 0.00002037
Iteration 163/1000 | Loss: 0.00002037
Iteration 164/1000 | Loss: 0.00002037
Iteration 165/1000 | Loss: 0.00002037
Iteration 166/1000 | Loss: 0.00002037
Iteration 167/1000 | Loss: 0.00002037
Iteration 168/1000 | Loss: 0.00002037
Iteration 169/1000 | Loss: 0.00002037
Iteration 170/1000 | Loss: 0.00002036
Iteration 171/1000 | Loss: 0.00002036
Iteration 172/1000 | Loss: 0.00002036
Iteration 173/1000 | Loss: 0.00002036
Iteration 174/1000 | Loss: 0.00002036
Iteration 175/1000 | Loss: 0.00002036
Iteration 176/1000 | Loss: 0.00002036
Iteration 177/1000 | Loss: 0.00002035
Iteration 178/1000 | Loss: 0.00002035
Iteration 179/1000 | Loss: 0.00002035
Iteration 180/1000 | Loss: 0.00002035
Iteration 181/1000 | Loss: 0.00002035
Iteration 182/1000 | Loss: 0.00002035
Iteration 183/1000 | Loss: 0.00002035
Iteration 184/1000 | Loss: 0.00002035
Iteration 185/1000 | Loss: 0.00002035
Iteration 186/1000 | Loss: 0.00002035
Iteration 187/1000 | Loss: 0.00002035
Iteration 188/1000 | Loss: 0.00002035
Iteration 189/1000 | Loss: 0.00002034
Iteration 190/1000 | Loss: 0.00002034
Iteration 191/1000 | Loss: 0.00002034
Iteration 192/1000 | Loss: 0.00002034
Iteration 193/1000 | Loss: 0.00002034
Iteration 194/1000 | Loss: 0.00002034
Iteration 195/1000 | Loss: 0.00002034
Iteration 196/1000 | Loss: 0.00002034
Iteration 197/1000 | Loss: 0.00002034
Iteration 198/1000 | Loss: 0.00002034
Iteration 199/1000 | Loss: 0.00002034
Iteration 200/1000 | Loss: 0.00002034
Iteration 201/1000 | Loss: 0.00002033
Iteration 202/1000 | Loss: 0.00002033
Iteration 203/1000 | Loss: 0.00002033
Iteration 204/1000 | Loss: 0.00002033
Iteration 205/1000 | Loss: 0.00002033
Iteration 206/1000 | Loss: 0.00002033
Iteration 207/1000 | Loss: 0.00002033
Iteration 208/1000 | Loss: 0.00002032
Iteration 209/1000 | Loss: 0.00002032
Iteration 210/1000 | Loss: 0.00002032
Iteration 211/1000 | Loss: 0.00002032
Iteration 212/1000 | Loss: 0.00002032
Iteration 213/1000 | Loss: 0.00002032
Iteration 214/1000 | Loss: 0.00002032
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 214. Stopping optimization.
Last 5 losses: [2.0321136616985314e-05, 2.0321136616985314e-05, 2.0321136616985314e-05, 2.0321136616985314e-05, 2.0321136616985314e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0321136616985314e-05

Optimization complete. Final v2v error: 3.7276391983032227 mm

Highest mean error: 4.864986896514893 mm for frame 193

Lowest mean error: 2.6625943183898926 mm for frame 220

Saving results

Total time: 53.2129442691803
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_011/1025/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_011/1025.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_011/1025
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00958866
Iteration 2/25 | Loss: 0.00200643
Iteration 3/25 | Loss: 0.00142171
Iteration 4/25 | Loss: 0.00133837
Iteration 5/25 | Loss: 0.00131471
Iteration 6/25 | Loss: 0.00133359
Iteration 7/25 | Loss: 0.00132945
Iteration 8/25 | Loss: 0.00129132
Iteration 9/25 | Loss: 0.00129019
Iteration 10/25 | Loss: 0.00128681
Iteration 11/25 | Loss: 0.00128409
Iteration 12/25 | Loss: 0.00128039
Iteration 13/25 | Loss: 0.00127931
Iteration 14/25 | Loss: 0.00127910
Iteration 15/25 | Loss: 0.00127907
Iteration 16/25 | Loss: 0.00127907
Iteration 17/25 | Loss: 0.00127907
Iteration 18/25 | Loss: 0.00127907
Iteration 19/25 | Loss: 0.00127907
Iteration 20/25 | Loss: 0.00127907
Iteration 21/25 | Loss: 0.00127907
Iteration 22/25 | Loss: 0.00127907
Iteration 23/25 | Loss: 0.00127907
Iteration 24/25 | Loss: 0.00127906
Iteration 25/25 | Loss: 0.00127906

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34273171
Iteration 2/25 | Loss: 0.00100949
Iteration 3/25 | Loss: 0.00100948
Iteration 4/25 | Loss: 0.00100948
Iteration 5/25 | Loss: 0.00100948
Iteration 6/25 | Loss: 0.00099177
Iteration 7/25 | Loss: 0.00099176
Iteration 8/25 | Loss: 0.00099176
Iteration 9/25 | Loss: 0.00099176
Iteration 10/25 | Loss: 0.00099176
Iteration 11/25 | Loss: 0.00099176
Iteration 12/25 | Loss: 0.00099176
Iteration 13/25 | Loss: 0.00099176
Iteration 14/25 | Loss: 0.00099176
Iteration 15/25 | Loss: 0.00099176
Iteration 16/25 | Loss: 0.00099176
Iteration 17/25 | Loss: 0.00099176
Iteration 18/25 | Loss: 0.00099176
Iteration 19/25 | Loss: 0.00099176
Iteration 20/25 | Loss: 0.00099176
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0009917622664943337, 0.0009917622664943337, 0.0009917622664943337, 0.0009917622664943337, 0.0009917622664943337]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009917622664943337

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00099176
Iteration 2/1000 | Loss: 0.00028148
Iteration 3/1000 | Loss: 0.00033480
Iteration 4/1000 | Loss: 0.00019460
Iteration 5/1000 | Loss: 0.00006615
Iteration 6/1000 | Loss: 0.00004384
Iteration 7/1000 | Loss: 0.00026105
Iteration 8/1000 | Loss: 0.00004994
Iteration 9/1000 | Loss: 0.00002621
Iteration 10/1000 | Loss: 0.00004136
Iteration 11/1000 | Loss: 0.00032767
Iteration 12/1000 | Loss: 0.00043255
Iteration 13/1000 | Loss: 0.00009886
Iteration 14/1000 | Loss: 0.00029847
Iteration 15/1000 | Loss: 0.00005170
Iteration 16/1000 | Loss: 0.00002581
Iteration 17/1000 | Loss: 0.00002552
Iteration 18/1000 | Loss: 0.00007032
Iteration 19/1000 | Loss: 0.00015234
Iteration 20/1000 | Loss: 0.00007321
Iteration 21/1000 | Loss: 0.00002250
Iteration 22/1000 | Loss: 0.00003979
Iteration 23/1000 | Loss: 0.00014579
Iteration 24/1000 | Loss: 0.00006490
Iteration 25/1000 | Loss: 0.00016940
Iteration 26/1000 | Loss: 0.00005905
Iteration 27/1000 | Loss: 0.00019333
Iteration 28/1000 | Loss: 0.00011045
Iteration 29/1000 | Loss: 0.00022328
Iteration 30/1000 | Loss: 0.00012734
Iteration 31/1000 | Loss: 0.00005542
Iteration 32/1000 | Loss: 0.00002540
Iteration 33/1000 | Loss: 0.00006924
Iteration 34/1000 | Loss: 0.00002029
Iteration 35/1000 | Loss: 0.00014208
Iteration 36/1000 | Loss: 0.00004044
Iteration 37/1000 | Loss: 0.00002015
Iteration 38/1000 | Loss: 0.00001916
Iteration 39/1000 | Loss: 0.00001896
Iteration 40/1000 | Loss: 0.00001949
Iteration 41/1000 | Loss: 0.00012735
Iteration 42/1000 | Loss: 0.00013349
Iteration 43/1000 | Loss: 0.00002967
Iteration 44/1000 | Loss: 0.00002002
Iteration 45/1000 | Loss: 0.00001876
Iteration 46/1000 | Loss: 0.00001864
Iteration 47/1000 | Loss: 0.00001853
Iteration 48/1000 | Loss: 0.00001850
Iteration 49/1000 | Loss: 0.00001848
Iteration 50/1000 | Loss: 0.00001848
Iteration 51/1000 | Loss: 0.00001840
Iteration 52/1000 | Loss: 0.00001839
Iteration 53/1000 | Loss: 0.00002060
Iteration 54/1000 | Loss: 0.00001833
Iteration 55/1000 | Loss: 0.00001827
Iteration 56/1000 | Loss: 0.00001826
Iteration 57/1000 | Loss: 0.00001826
Iteration 58/1000 | Loss: 0.00001826
Iteration 59/1000 | Loss: 0.00001825
Iteration 60/1000 | Loss: 0.00001825
Iteration 61/1000 | Loss: 0.00001825
Iteration 62/1000 | Loss: 0.00001825
Iteration 63/1000 | Loss: 0.00001824
Iteration 64/1000 | Loss: 0.00001824
Iteration 65/1000 | Loss: 0.00001824
Iteration 66/1000 | Loss: 0.00001824
Iteration 67/1000 | Loss: 0.00001824
Iteration 68/1000 | Loss: 0.00001824
Iteration 69/1000 | Loss: 0.00001824
Iteration 70/1000 | Loss: 0.00001824
Iteration 71/1000 | Loss: 0.00001824
Iteration 72/1000 | Loss: 0.00001823
Iteration 73/1000 | Loss: 0.00001823
Iteration 74/1000 | Loss: 0.00001823
Iteration 75/1000 | Loss: 0.00001823
Iteration 76/1000 | Loss: 0.00001822
Iteration 77/1000 | Loss: 0.00001821
Iteration 78/1000 | Loss: 0.00001820
Iteration 79/1000 | Loss: 0.00006562
Iteration 80/1000 | Loss: 0.00008160
Iteration 81/1000 | Loss: 0.00003011
Iteration 82/1000 | Loss: 0.00001805
Iteration 83/1000 | Loss: 0.00001799
Iteration 84/1000 | Loss: 0.00001798
Iteration 85/1000 | Loss: 0.00001798
Iteration 86/1000 | Loss: 0.00001798
Iteration 87/1000 | Loss: 0.00001797
Iteration 88/1000 | Loss: 0.00001797
Iteration 89/1000 | Loss: 0.00001797
Iteration 90/1000 | Loss: 0.00001797
Iteration 91/1000 | Loss: 0.00001796
Iteration 92/1000 | Loss: 0.00001796
Iteration 93/1000 | Loss: 0.00001796
Iteration 94/1000 | Loss: 0.00001796
Iteration 95/1000 | Loss: 0.00001795
Iteration 96/1000 | Loss: 0.00001795
Iteration 97/1000 | Loss: 0.00001795
Iteration 98/1000 | Loss: 0.00001795
Iteration 99/1000 | Loss: 0.00001795
Iteration 100/1000 | Loss: 0.00001795
Iteration 101/1000 | Loss: 0.00001795
Iteration 102/1000 | Loss: 0.00001795
Iteration 103/1000 | Loss: 0.00001795
Iteration 104/1000 | Loss: 0.00001795
Iteration 105/1000 | Loss: 0.00001795
Iteration 106/1000 | Loss: 0.00001795
Iteration 107/1000 | Loss: 0.00001795
Iteration 108/1000 | Loss: 0.00001795
Iteration 109/1000 | Loss: 0.00001795
Iteration 110/1000 | Loss: 0.00001795
Iteration 111/1000 | Loss: 0.00001795
Iteration 112/1000 | Loss: 0.00001795
Iteration 113/1000 | Loss: 0.00001794
Iteration 114/1000 | Loss: 0.00001794
Iteration 115/1000 | Loss: 0.00001794
Iteration 116/1000 | Loss: 0.00001794
Iteration 117/1000 | Loss: 0.00001794
Iteration 118/1000 | Loss: 0.00001794
Iteration 119/1000 | Loss: 0.00001794
Iteration 120/1000 | Loss: 0.00001794
Iteration 121/1000 | Loss: 0.00001794
Iteration 122/1000 | Loss: 0.00001794
Iteration 123/1000 | Loss: 0.00001794
Iteration 124/1000 | Loss: 0.00001794
Iteration 125/1000 | Loss: 0.00001794
Iteration 126/1000 | Loss: 0.00001794
Iteration 127/1000 | Loss: 0.00001794
Iteration 128/1000 | Loss: 0.00001794
Iteration 129/1000 | Loss: 0.00001794
Iteration 130/1000 | Loss: 0.00001794
Iteration 131/1000 | Loss: 0.00001794
Iteration 132/1000 | Loss: 0.00001793
Iteration 133/1000 | Loss: 0.00001793
Iteration 134/1000 | Loss: 0.00001793
Iteration 135/1000 | Loss: 0.00001793
Iteration 136/1000 | Loss: 0.00001793
Iteration 137/1000 | Loss: 0.00001793
Iteration 138/1000 | Loss: 0.00001793
Iteration 139/1000 | Loss: 0.00001793
Iteration 140/1000 | Loss: 0.00001793
Iteration 141/1000 | Loss: 0.00001793
Iteration 142/1000 | Loss: 0.00001793
Iteration 143/1000 | Loss: 0.00001793
Iteration 144/1000 | Loss: 0.00001793
Iteration 145/1000 | Loss: 0.00001793
Iteration 146/1000 | Loss: 0.00001792
Iteration 147/1000 | Loss: 0.00001792
Iteration 148/1000 | Loss: 0.00001792
Iteration 149/1000 | Loss: 0.00001792
Iteration 150/1000 | Loss: 0.00001792
Iteration 151/1000 | Loss: 0.00001792
Iteration 152/1000 | Loss: 0.00001792
Iteration 153/1000 | Loss: 0.00001792
Iteration 154/1000 | Loss: 0.00001792
Iteration 155/1000 | Loss: 0.00001792
Iteration 156/1000 | Loss: 0.00001792
Iteration 157/1000 | Loss: 0.00001792
Iteration 158/1000 | Loss: 0.00001791
Iteration 159/1000 | Loss: 0.00001791
Iteration 160/1000 | Loss: 0.00001791
Iteration 161/1000 | Loss: 0.00001791
Iteration 162/1000 | Loss: 0.00001791
Iteration 163/1000 | Loss: 0.00001791
Iteration 164/1000 | Loss: 0.00001791
Iteration 165/1000 | Loss: 0.00001791
Iteration 166/1000 | Loss: 0.00001791
Iteration 167/1000 | Loss: 0.00002327
Iteration 168/1000 | Loss: 0.00002327
Iteration 169/1000 | Loss: 0.00001789
Iteration 170/1000 | Loss: 0.00001789
Iteration 171/1000 | Loss: 0.00001788
Iteration 172/1000 | Loss: 0.00001788
Iteration 173/1000 | Loss: 0.00001788
Iteration 174/1000 | Loss: 0.00001788
Iteration 175/1000 | Loss: 0.00001788
Iteration 176/1000 | Loss: 0.00001843
Iteration 177/1000 | Loss: 0.00001839
Iteration 178/1000 | Loss: 0.00001787
Iteration 179/1000 | Loss: 0.00001787
Iteration 180/1000 | Loss: 0.00001787
Iteration 181/1000 | Loss: 0.00001787
Iteration 182/1000 | Loss: 0.00001787
Iteration 183/1000 | Loss: 0.00001787
Iteration 184/1000 | Loss: 0.00001787
Iteration 185/1000 | Loss: 0.00001787
Iteration 186/1000 | Loss: 0.00001787
Iteration 187/1000 | Loss: 0.00001787
Iteration 188/1000 | Loss: 0.00001787
Iteration 189/1000 | Loss: 0.00001787
Iteration 190/1000 | Loss: 0.00001787
Iteration 191/1000 | Loss: 0.00001787
Iteration 192/1000 | Loss: 0.00001787
Iteration 193/1000 | Loss: 0.00001787
Iteration 194/1000 | Loss: 0.00001787
Iteration 195/1000 | Loss: 0.00001787
Iteration 196/1000 | Loss: 0.00001787
Iteration 197/1000 | Loss: 0.00001787
Iteration 198/1000 | Loss: 0.00001787
Iteration 199/1000 | Loss: 0.00001787
Iteration 200/1000 | Loss: 0.00001787
Iteration 201/1000 | Loss: 0.00001787
Iteration 202/1000 | Loss: 0.00001787
Iteration 203/1000 | Loss: 0.00001787
Iteration 204/1000 | Loss: 0.00001787
Iteration 205/1000 | Loss: 0.00001787
Iteration 206/1000 | Loss: 0.00001787
Iteration 207/1000 | Loss: 0.00001787
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 207. Stopping optimization.
Last 5 losses: [1.7867076167021878e-05, 1.7867076167021878e-05, 1.7867076167021878e-05, 1.7867076167021878e-05, 1.7867076167021878e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7867076167021878e-05

Optimization complete. Final v2v error: 3.553389310836792 mm

Highest mean error: 5.688511848449707 mm for frame 68

Lowest mean error: 2.917875051498413 mm for frame 136

Saving results

Total time: 128.0168867111206
