Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=292, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 16352-16407
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_31_nl_1440/0001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_31_nl_1440/0001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_31_nl_1440/0001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01112280
Iteration 2/25 | Loss: 0.00268407
Iteration 3/25 | Loss: 0.00211546
Iteration 4/25 | Loss: 0.00197220
Iteration 5/25 | Loss: 0.00188170
Iteration 6/25 | Loss: 0.00167331
Iteration 7/25 | Loss: 0.00160224
Iteration 8/25 | Loss: 0.00158128
Iteration 9/25 | Loss: 0.00157382
Iteration 10/25 | Loss: 0.00156974
Iteration 11/25 | Loss: 0.00156615
Iteration 12/25 | Loss: 0.00156485
Iteration 13/25 | Loss: 0.00156395
Iteration 14/25 | Loss: 0.00156329
Iteration 15/25 | Loss: 0.00156291
Iteration 16/25 | Loss: 0.00156277
Iteration 17/25 | Loss: 0.00156275
Iteration 18/25 | Loss: 0.00156274
Iteration 19/25 | Loss: 0.00156274
Iteration 20/25 | Loss: 0.00156273
Iteration 21/25 | Loss: 0.00156273
Iteration 22/25 | Loss: 0.00156273
Iteration 23/25 | Loss: 0.00156273
Iteration 24/25 | Loss: 0.00156273
Iteration 25/25 | Loss: 0.00156273

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.12983346
Iteration 2/25 | Loss: 0.00316224
Iteration 3/25 | Loss: 0.00316224
Iteration 4/25 | Loss: 0.00316224
Iteration 5/25 | Loss: 0.00316224
Iteration 6/25 | Loss: 0.00316224
Iteration 7/25 | Loss: 0.00316224
Iteration 8/25 | Loss: 0.00316224
Iteration 9/25 | Loss: 0.00316224
Iteration 10/25 | Loss: 0.00316224
Iteration 11/25 | Loss: 0.00316224
Iteration 12/25 | Loss: 0.00316224
Iteration 13/25 | Loss: 0.00316224
Iteration 14/25 | Loss: 0.00316224
Iteration 15/25 | Loss: 0.00316224
Iteration 16/25 | Loss: 0.00316224
Iteration 17/25 | Loss: 0.00316224
Iteration 18/25 | Loss: 0.00316224
Iteration 19/25 | Loss: 0.00316224
Iteration 20/25 | Loss: 0.00316224
Iteration 21/25 | Loss: 0.00316224
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0031622357200831175, 0.0031622357200831175, 0.0031622357200831175, 0.0031622357200831175, 0.0031622357200831175]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0031622357200831175

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00316224
Iteration 2/1000 | Loss: 0.00039363
Iteration 3/1000 | Loss: 0.00032029
Iteration 4/1000 | Loss: 0.00027558
Iteration 5/1000 | Loss: 0.00025353
Iteration 6/1000 | Loss: 0.00023371
Iteration 7/1000 | Loss: 0.00022166
Iteration 8/1000 | Loss: 0.00021199
Iteration 9/1000 | Loss: 0.00020586
Iteration 10/1000 | Loss: 0.00020072
Iteration 11/1000 | Loss: 0.00019709
Iteration 12/1000 | Loss: 0.00019349
Iteration 13/1000 | Loss: 0.00019138
Iteration 14/1000 | Loss: 0.00018996
Iteration 15/1000 | Loss: 0.00018854
Iteration 16/1000 | Loss: 0.00018715
Iteration 17/1000 | Loss: 0.00018618
Iteration 18/1000 | Loss: 0.00018560
Iteration 19/1000 | Loss: 0.00018506
Iteration 20/1000 | Loss: 0.00018471
Iteration 21/1000 | Loss: 0.00018443
Iteration 22/1000 | Loss: 0.00018410
Iteration 23/1000 | Loss: 0.00018399
Iteration 24/1000 | Loss: 0.00018392
Iteration 25/1000 | Loss: 0.00018389
Iteration 26/1000 | Loss: 0.00018389
Iteration 27/1000 | Loss: 0.00018388
Iteration 28/1000 | Loss: 0.00018386
Iteration 29/1000 | Loss: 0.00018381
Iteration 30/1000 | Loss: 0.00018369
Iteration 31/1000 | Loss: 0.00018365
Iteration 32/1000 | Loss: 0.00018358
Iteration 33/1000 | Loss: 0.00018353
Iteration 34/1000 | Loss: 0.00018352
Iteration 35/1000 | Loss: 0.00018339
Iteration 36/1000 | Loss: 0.00018331
Iteration 37/1000 | Loss: 0.00018331
Iteration 38/1000 | Loss: 0.00018330
Iteration 39/1000 | Loss: 0.00018329
Iteration 40/1000 | Loss: 0.00018329
Iteration 41/1000 | Loss: 0.00018329
Iteration 42/1000 | Loss: 0.00018329
Iteration 43/1000 | Loss: 0.00018328
Iteration 44/1000 | Loss: 0.00018320
Iteration 45/1000 | Loss: 0.00018319
Iteration 46/1000 | Loss: 0.00018319
Iteration 47/1000 | Loss: 0.00018319
Iteration 48/1000 | Loss: 0.00018318
Iteration 49/1000 | Loss: 0.00018318
Iteration 50/1000 | Loss: 0.00018317
Iteration 51/1000 | Loss: 0.00018317
Iteration 52/1000 | Loss: 0.00018317
Iteration 53/1000 | Loss: 0.00018317
Iteration 54/1000 | Loss: 0.00018316
Iteration 55/1000 | Loss: 0.00018316
Iteration 56/1000 | Loss: 0.00018316
Iteration 57/1000 | Loss: 0.00018316
Iteration 58/1000 | Loss: 0.00018316
Iteration 59/1000 | Loss: 0.00018316
Iteration 60/1000 | Loss: 0.00018316
Iteration 61/1000 | Loss: 0.00018316
Iteration 62/1000 | Loss: 0.00018315
Iteration 63/1000 | Loss: 0.00018314
Iteration 64/1000 | Loss: 0.00018313
Iteration 65/1000 | Loss: 0.00018313
Iteration 66/1000 | Loss: 0.00018313
Iteration 67/1000 | Loss: 0.00018313
Iteration 68/1000 | Loss: 0.00018313
Iteration 69/1000 | Loss: 0.00018313
Iteration 70/1000 | Loss: 0.00018313
Iteration 71/1000 | Loss: 0.00018313
Iteration 72/1000 | Loss: 0.00018313
Iteration 73/1000 | Loss: 0.00018313
Iteration 74/1000 | Loss: 0.00018312
Iteration 75/1000 | Loss: 0.00018312
Iteration 76/1000 | Loss: 0.00018311
Iteration 77/1000 | Loss: 0.00018311
Iteration 78/1000 | Loss: 0.00018311
Iteration 79/1000 | Loss: 0.00018310
Iteration 80/1000 | Loss: 0.00018310
Iteration 81/1000 | Loss: 0.00018310
Iteration 82/1000 | Loss: 0.00018310
Iteration 83/1000 | Loss: 0.00018310
Iteration 84/1000 | Loss: 0.00018310
Iteration 85/1000 | Loss: 0.00018310
Iteration 86/1000 | Loss: 0.00018309
Iteration 87/1000 | Loss: 0.00018308
Iteration 88/1000 | Loss: 0.00018308
Iteration 89/1000 | Loss: 0.00018308
Iteration 90/1000 | Loss: 0.00018308
Iteration 91/1000 | Loss: 0.00018308
Iteration 92/1000 | Loss: 0.00018307
Iteration 93/1000 | Loss: 0.00018307
Iteration 94/1000 | Loss: 0.00018307
Iteration 95/1000 | Loss: 0.00018307
Iteration 96/1000 | Loss: 0.00018307
Iteration 97/1000 | Loss: 0.00018307
Iteration 98/1000 | Loss: 0.00018307
Iteration 99/1000 | Loss: 0.00018307
Iteration 100/1000 | Loss: 0.00018307
Iteration 101/1000 | Loss: 0.00018306
Iteration 102/1000 | Loss: 0.00018305
Iteration 103/1000 | Loss: 0.00018305
Iteration 104/1000 | Loss: 0.00018304
Iteration 105/1000 | Loss: 0.00018304
Iteration 106/1000 | Loss: 0.00018304
Iteration 107/1000 | Loss: 0.00018304
Iteration 108/1000 | Loss: 0.00018304
Iteration 109/1000 | Loss: 0.00018304
Iteration 110/1000 | Loss: 0.00018304
Iteration 111/1000 | Loss: 0.00018304
Iteration 112/1000 | Loss: 0.00018304
Iteration 113/1000 | Loss: 0.00018304
Iteration 114/1000 | Loss: 0.00018304
Iteration 115/1000 | Loss: 0.00018304
Iteration 116/1000 | Loss: 0.00018304
Iteration 117/1000 | Loss: 0.00018304
Iteration 118/1000 | Loss: 0.00018304
Iteration 119/1000 | Loss: 0.00018304
Iteration 120/1000 | Loss: 0.00018304
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 120. Stopping optimization.
Last 5 losses: [0.0001830412948038429, 0.0001830412948038429, 0.0001830412948038429, 0.0001830412948038429, 0.0001830412948038429]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0001830412948038429

Optimization complete. Final v2v error: 7.697248935699463 mm

Highest mean error: 21.46988296508789 mm for frame 8

Lowest mean error: 4.761895179748535 mm for frame 11

Saving results

Total time: 89.09888648986816
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_31_nl_1440/0011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_31_nl_1440/0011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_31_nl_1440/0011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00279424
Iteration 2/25 | Loss: 0.00128780
Iteration 3/25 | Loss: 0.00105109
Iteration 4/25 | Loss: 0.00102010
Iteration 5/25 | Loss: 0.00101426
Iteration 6/25 | Loss: 0.00101231
Iteration 7/25 | Loss: 0.00101189
Iteration 8/25 | Loss: 0.00101189
Iteration 9/25 | Loss: 0.00101189
Iteration 10/25 | Loss: 0.00101189
Iteration 11/25 | Loss: 0.00101189
Iteration 12/25 | Loss: 0.00101189
Iteration 13/25 | Loss: 0.00101189
Iteration 14/25 | Loss: 0.00101189
Iteration 15/25 | Loss: 0.00101189
Iteration 16/25 | Loss: 0.00101189
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0010118938516825438, 0.0010118938516825438, 0.0010118938516825438, 0.0010118938516825438, 0.0010118938516825438]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010118938516825438

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.20754194
Iteration 2/25 | Loss: 0.00251589
Iteration 3/25 | Loss: 0.00251589
Iteration 4/25 | Loss: 0.00251589
Iteration 5/25 | Loss: 0.00251589
Iteration 6/25 | Loss: 0.00251589
Iteration 7/25 | Loss: 0.00251589
Iteration 8/25 | Loss: 0.00251589
Iteration 9/25 | Loss: 0.00251589
Iteration 10/25 | Loss: 0.00251589
Iteration 11/25 | Loss: 0.00251589
Iteration 12/25 | Loss: 0.00251589
Iteration 13/25 | Loss: 0.00251589
Iteration 14/25 | Loss: 0.00251589
Iteration 15/25 | Loss: 0.00251589
Iteration 16/25 | Loss: 0.00251589
Iteration 17/25 | Loss: 0.00251589
Iteration 18/25 | Loss: 0.00251589
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0025158878415822983, 0.0025158878415822983, 0.0025158878415822983, 0.0025158878415822983, 0.0025158878415822983]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0025158878415822983

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00251589
Iteration 2/1000 | Loss: 0.00004107
Iteration 3/1000 | Loss: 0.00001822
Iteration 4/1000 | Loss: 0.00001371
Iteration 5/1000 | Loss: 0.00001217
Iteration 6/1000 | Loss: 0.00001151
Iteration 7/1000 | Loss: 0.00001118
Iteration 8/1000 | Loss: 0.00001100
Iteration 9/1000 | Loss: 0.00001096
Iteration 10/1000 | Loss: 0.00001077
Iteration 11/1000 | Loss: 0.00001071
Iteration 12/1000 | Loss: 0.00001059
Iteration 13/1000 | Loss: 0.00001057
Iteration 14/1000 | Loss: 0.00001056
Iteration 15/1000 | Loss: 0.00001052
Iteration 16/1000 | Loss: 0.00001052
Iteration 17/1000 | Loss: 0.00001051
Iteration 18/1000 | Loss: 0.00001050
Iteration 19/1000 | Loss: 0.00001050
Iteration 20/1000 | Loss: 0.00001049
Iteration 21/1000 | Loss: 0.00001045
Iteration 22/1000 | Loss: 0.00001039
Iteration 23/1000 | Loss: 0.00001039
Iteration 24/1000 | Loss: 0.00001038
Iteration 25/1000 | Loss: 0.00001037
Iteration 26/1000 | Loss: 0.00001036
Iteration 27/1000 | Loss: 0.00001034
Iteration 28/1000 | Loss: 0.00001034
Iteration 29/1000 | Loss: 0.00001033
Iteration 30/1000 | Loss: 0.00001033
Iteration 31/1000 | Loss: 0.00001032
Iteration 32/1000 | Loss: 0.00001032
Iteration 33/1000 | Loss: 0.00001032
Iteration 34/1000 | Loss: 0.00001032
Iteration 35/1000 | Loss: 0.00001031
Iteration 36/1000 | Loss: 0.00001031
Iteration 37/1000 | Loss: 0.00001030
Iteration 38/1000 | Loss: 0.00001028
Iteration 39/1000 | Loss: 0.00001028
Iteration 40/1000 | Loss: 0.00001026
Iteration 41/1000 | Loss: 0.00001026
Iteration 42/1000 | Loss: 0.00001025
Iteration 43/1000 | Loss: 0.00001025
Iteration 44/1000 | Loss: 0.00001025
Iteration 45/1000 | Loss: 0.00001024
Iteration 46/1000 | Loss: 0.00001024
Iteration 47/1000 | Loss: 0.00001024
Iteration 48/1000 | Loss: 0.00001023
Iteration 49/1000 | Loss: 0.00001023
Iteration 50/1000 | Loss: 0.00001023
Iteration 51/1000 | Loss: 0.00001022
Iteration 52/1000 | Loss: 0.00001021
Iteration 53/1000 | Loss: 0.00001020
Iteration 54/1000 | Loss: 0.00001020
Iteration 55/1000 | Loss: 0.00001019
Iteration 56/1000 | Loss: 0.00001019
Iteration 57/1000 | Loss: 0.00001018
Iteration 58/1000 | Loss: 0.00001018
Iteration 59/1000 | Loss: 0.00001017
Iteration 60/1000 | Loss: 0.00001017
Iteration 61/1000 | Loss: 0.00001016
Iteration 62/1000 | Loss: 0.00001016
Iteration 63/1000 | Loss: 0.00001015
Iteration 64/1000 | Loss: 0.00001015
Iteration 65/1000 | Loss: 0.00001015
Iteration 66/1000 | Loss: 0.00001015
Iteration 67/1000 | Loss: 0.00001014
Iteration 68/1000 | Loss: 0.00001014
Iteration 69/1000 | Loss: 0.00001014
Iteration 70/1000 | Loss: 0.00001013
Iteration 71/1000 | Loss: 0.00001013
Iteration 72/1000 | Loss: 0.00001012
Iteration 73/1000 | Loss: 0.00001012
Iteration 74/1000 | Loss: 0.00001011
Iteration 75/1000 | Loss: 0.00001011
Iteration 76/1000 | Loss: 0.00001010
Iteration 77/1000 | Loss: 0.00001010
Iteration 78/1000 | Loss: 0.00001010
Iteration 79/1000 | Loss: 0.00001009
Iteration 80/1000 | Loss: 0.00001009
Iteration 81/1000 | Loss: 0.00001009
Iteration 82/1000 | Loss: 0.00001009
Iteration 83/1000 | Loss: 0.00001009
Iteration 84/1000 | Loss: 0.00001009
Iteration 85/1000 | Loss: 0.00001009
Iteration 86/1000 | Loss: 0.00001009
Iteration 87/1000 | Loss: 0.00001009
Iteration 88/1000 | Loss: 0.00001009
Iteration 89/1000 | Loss: 0.00001009
Iteration 90/1000 | Loss: 0.00001008
Iteration 91/1000 | Loss: 0.00001008
Iteration 92/1000 | Loss: 0.00001008
Iteration 93/1000 | Loss: 0.00001007
Iteration 94/1000 | Loss: 0.00001007
Iteration 95/1000 | Loss: 0.00001006
Iteration 96/1000 | Loss: 0.00001006
Iteration 97/1000 | Loss: 0.00001006
Iteration 98/1000 | Loss: 0.00001005
Iteration 99/1000 | Loss: 0.00001005
Iteration 100/1000 | Loss: 0.00001005
Iteration 101/1000 | Loss: 0.00001005
Iteration 102/1000 | Loss: 0.00001005
Iteration 103/1000 | Loss: 0.00001005
Iteration 104/1000 | Loss: 0.00001005
Iteration 105/1000 | Loss: 0.00001005
Iteration 106/1000 | Loss: 0.00001004
Iteration 107/1000 | Loss: 0.00001004
Iteration 108/1000 | Loss: 0.00001004
Iteration 109/1000 | Loss: 0.00001004
Iteration 110/1000 | Loss: 0.00001004
Iteration 111/1000 | Loss: 0.00001003
Iteration 112/1000 | Loss: 0.00001003
Iteration 113/1000 | Loss: 0.00001003
Iteration 114/1000 | Loss: 0.00001002
Iteration 115/1000 | Loss: 0.00001002
Iteration 116/1000 | Loss: 0.00001002
Iteration 117/1000 | Loss: 0.00001001
Iteration 118/1000 | Loss: 0.00001001
Iteration 119/1000 | Loss: 0.00001001
Iteration 120/1000 | Loss: 0.00001001
Iteration 121/1000 | Loss: 0.00001001
Iteration 122/1000 | Loss: 0.00001001
Iteration 123/1000 | Loss: 0.00001001
Iteration 124/1000 | Loss: 0.00001001
Iteration 125/1000 | Loss: 0.00001001
Iteration 126/1000 | Loss: 0.00001001
Iteration 127/1000 | Loss: 0.00001001
Iteration 128/1000 | Loss: 0.00001001
Iteration 129/1000 | Loss: 0.00001001
Iteration 130/1000 | Loss: 0.00001001
Iteration 131/1000 | Loss: 0.00001000
Iteration 132/1000 | Loss: 0.00001000
Iteration 133/1000 | Loss: 0.00001000
Iteration 134/1000 | Loss: 0.00001000
Iteration 135/1000 | Loss: 0.00001000
Iteration 136/1000 | Loss: 0.00001000
Iteration 137/1000 | Loss: 0.00001000
Iteration 138/1000 | Loss: 0.00001000
Iteration 139/1000 | Loss: 0.00001000
Iteration 140/1000 | Loss: 0.00001000
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 140. Stopping optimization.
Last 5 losses: [1.0001973350881599e-05, 1.0001973350881599e-05, 1.0001973350881599e-05, 1.0001973350881599e-05, 1.0001973350881599e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0001973350881599e-05

Optimization complete. Final v2v error: 2.6916041374206543 mm

Highest mean error: 3.176743745803833 mm for frame 98

Lowest mean error: 2.3374195098876953 mm for frame 78

Saving results

Total time: 39.42153358459473
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_31_nl_1440/0000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_31_nl_1440/0000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_31_nl_1440/0000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00796079
Iteration 2/25 | Loss: 0.00140235
Iteration 3/25 | Loss: 0.00116760
Iteration 4/25 | Loss: 0.00114378
Iteration 5/25 | Loss: 0.00114071
Iteration 6/25 | Loss: 0.00114037
Iteration 7/25 | Loss: 0.00114037
Iteration 8/25 | Loss: 0.00114037
Iteration 9/25 | Loss: 0.00114037
Iteration 10/25 | Loss: 0.00114037
Iteration 11/25 | Loss: 0.00114037
Iteration 12/25 | Loss: 0.00114037
Iteration 13/25 | Loss: 0.00114037
Iteration 14/25 | Loss: 0.00114037
Iteration 15/25 | Loss: 0.00114037
Iteration 16/25 | Loss: 0.00114037
Iteration 17/25 | Loss: 0.00114037
Iteration 18/25 | Loss: 0.00114037
Iteration 19/25 | Loss: 0.00114037
Iteration 20/25 | Loss: 0.00114037
Iteration 21/25 | Loss: 0.00114037
Iteration 22/25 | Loss: 0.00114037
Iteration 23/25 | Loss: 0.00114037
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0011403708485886455, 0.0011403708485886455, 0.0011403708485886455, 0.0011403708485886455, 0.0011403708485886455]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011403708485886455

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.06615806
Iteration 2/25 | Loss: 0.00127720
Iteration 3/25 | Loss: 0.00127716
Iteration 4/25 | Loss: 0.00127716
Iteration 5/25 | Loss: 0.00127716
Iteration 6/25 | Loss: 0.00127716
Iteration 7/25 | Loss: 0.00127716
Iteration 8/25 | Loss: 0.00127716
Iteration 9/25 | Loss: 0.00127716
Iteration 10/25 | Loss: 0.00127716
Iteration 11/25 | Loss: 0.00127716
Iteration 12/25 | Loss: 0.00127716
Iteration 13/25 | Loss: 0.00127716
Iteration 14/25 | Loss: 0.00127716
Iteration 15/25 | Loss: 0.00127716
Iteration 16/25 | Loss: 0.00127716
Iteration 17/25 | Loss: 0.00127716
Iteration 18/25 | Loss: 0.00127716
Iteration 19/25 | Loss: 0.00127716
Iteration 20/25 | Loss: 0.00127716
Iteration 21/25 | Loss: 0.00127716
Iteration 22/25 | Loss: 0.00127716
Iteration 23/25 | Loss: 0.00127716
Iteration 24/25 | Loss: 0.00127716
Iteration 25/25 | Loss: 0.00127716
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0012771619949489832, 0.0012771619949489832, 0.0012771619949489832, 0.0012771619949489832, 0.0012771619949489832]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012771619949489832

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00127716
Iteration 2/1000 | Loss: 0.00004726
Iteration 3/1000 | Loss: 0.00003310
Iteration 4/1000 | Loss: 0.00002947
Iteration 5/1000 | Loss: 0.00002788
Iteration 6/1000 | Loss: 0.00002719
Iteration 7/1000 | Loss: 0.00002663
Iteration 8/1000 | Loss: 0.00002623
Iteration 9/1000 | Loss: 0.00002602
Iteration 10/1000 | Loss: 0.00002574
Iteration 11/1000 | Loss: 0.00002552
Iteration 12/1000 | Loss: 0.00002535
Iteration 13/1000 | Loss: 0.00002516
Iteration 14/1000 | Loss: 0.00002511
Iteration 15/1000 | Loss: 0.00002506
Iteration 16/1000 | Loss: 0.00002506
Iteration 17/1000 | Loss: 0.00002491
Iteration 18/1000 | Loss: 0.00002491
Iteration 19/1000 | Loss: 0.00002476
Iteration 20/1000 | Loss: 0.00002474
Iteration 21/1000 | Loss: 0.00002471
Iteration 22/1000 | Loss: 0.00002470
Iteration 23/1000 | Loss: 0.00002469
Iteration 24/1000 | Loss: 0.00002468
Iteration 25/1000 | Loss: 0.00002467
Iteration 26/1000 | Loss: 0.00002467
Iteration 27/1000 | Loss: 0.00002466
Iteration 28/1000 | Loss: 0.00002466
Iteration 29/1000 | Loss: 0.00002464
Iteration 30/1000 | Loss: 0.00002464
Iteration 31/1000 | Loss: 0.00002464
Iteration 32/1000 | Loss: 0.00002463
Iteration 33/1000 | Loss: 0.00002461
Iteration 34/1000 | Loss: 0.00002461
Iteration 35/1000 | Loss: 0.00002461
Iteration 36/1000 | Loss: 0.00002460
Iteration 37/1000 | Loss: 0.00002459
Iteration 38/1000 | Loss: 0.00002458
Iteration 39/1000 | Loss: 0.00002458
Iteration 40/1000 | Loss: 0.00002458
Iteration 41/1000 | Loss: 0.00002457
Iteration 42/1000 | Loss: 0.00002457
Iteration 43/1000 | Loss: 0.00002457
Iteration 44/1000 | Loss: 0.00002457
Iteration 45/1000 | Loss: 0.00002455
Iteration 46/1000 | Loss: 0.00002454
Iteration 47/1000 | Loss: 0.00002451
Iteration 48/1000 | Loss: 0.00002450
Iteration 49/1000 | Loss: 0.00002450
Iteration 50/1000 | Loss: 0.00002448
Iteration 51/1000 | Loss: 0.00002447
Iteration 52/1000 | Loss: 0.00002447
Iteration 53/1000 | Loss: 0.00002446
Iteration 54/1000 | Loss: 0.00002446
Iteration 55/1000 | Loss: 0.00002445
Iteration 56/1000 | Loss: 0.00002445
Iteration 57/1000 | Loss: 0.00002443
Iteration 58/1000 | Loss: 0.00002443
Iteration 59/1000 | Loss: 0.00002442
Iteration 60/1000 | Loss: 0.00002442
Iteration 61/1000 | Loss: 0.00002442
Iteration 62/1000 | Loss: 0.00002441
Iteration 63/1000 | Loss: 0.00002441
Iteration 64/1000 | Loss: 0.00002440
Iteration 65/1000 | Loss: 0.00002440
Iteration 66/1000 | Loss: 0.00002440
Iteration 67/1000 | Loss: 0.00002440
Iteration 68/1000 | Loss: 0.00002440
Iteration 69/1000 | Loss: 0.00002440
Iteration 70/1000 | Loss: 0.00002440
Iteration 71/1000 | Loss: 0.00002439
Iteration 72/1000 | Loss: 0.00002439
Iteration 73/1000 | Loss: 0.00002439
Iteration 74/1000 | Loss: 0.00002439
Iteration 75/1000 | Loss: 0.00002439
Iteration 76/1000 | Loss: 0.00002438
Iteration 77/1000 | Loss: 0.00002438
Iteration 78/1000 | Loss: 0.00002438
Iteration 79/1000 | Loss: 0.00002437
Iteration 80/1000 | Loss: 0.00002437
Iteration 81/1000 | Loss: 0.00002437
Iteration 82/1000 | Loss: 0.00002437
Iteration 83/1000 | Loss: 0.00002436
Iteration 84/1000 | Loss: 0.00002436
Iteration 85/1000 | Loss: 0.00002435
Iteration 86/1000 | Loss: 0.00002435
Iteration 87/1000 | Loss: 0.00002435
Iteration 88/1000 | Loss: 0.00002435
Iteration 89/1000 | Loss: 0.00002435
Iteration 90/1000 | Loss: 0.00002435
Iteration 91/1000 | Loss: 0.00002434
Iteration 92/1000 | Loss: 0.00002434
Iteration 93/1000 | Loss: 0.00002434
Iteration 94/1000 | Loss: 0.00002434
Iteration 95/1000 | Loss: 0.00002434
Iteration 96/1000 | Loss: 0.00002433
Iteration 97/1000 | Loss: 0.00002432
Iteration 98/1000 | Loss: 0.00002432
Iteration 99/1000 | Loss: 0.00002432
Iteration 100/1000 | Loss: 0.00002432
Iteration 101/1000 | Loss: 0.00002431
Iteration 102/1000 | Loss: 0.00002431
Iteration 103/1000 | Loss: 0.00002431
Iteration 104/1000 | Loss: 0.00002430
Iteration 105/1000 | Loss: 0.00002430
Iteration 106/1000 | Loss: 0.00002429
Iteration 107/1000 | Loss: 0.00002429
Iteration 108/1000 | Loss: 0.00002429
Iteration 109/1000 | Loss: 0.00002429
Iteration 110/1000 | Loss: 0.00002428
Iteration 111/1000 | Loss: 0.00002428
Iteration 112/1000 | Loss: 0.00002427
Iteration 113/1000 | Loss: 0.00002427
Iteration 114/1000 | Loss: 0.00002427
Iteration 115/1000 | Loss: 0.00002427
Iteration 116/1000 | Loss: 0.00002427
Iteration 117/1000 | Loss: 0.00002427
Iteration 118/1000 | Loss: 0.00002427
Iteration 119/1000 | Loss: 0.00002426
Iteration 120/1000 | Loss: 0.00002426
Iteration 121/1000 | Loss: 0.00002426
Iteration 122/1000 | Loss: 0.00002425
Iteration 123/1000 | Loss: 0.00002425
Iteration 124/1000 | Loss: 0.00002425
Iteration 125/1000 | Loss: 0.00002425
Iteration 126/1000 | Loss: 0.00002425
Iteration 127/1000 | Loss: 0.00002425
Iteration 128/1000 | Loss: 0.00002425
Iteration 129/1000 | Loss: 0.00002425
Iteration 130/1000 | Loss: 0.00002425
Iteration 131/1000 | Loss: 0.00002425
Iteration 132/1000 | Loss: 0.00002425
Iteration 133/1000 | Loss: 0.00002424
Iteration 134/1000 | Loss: 0.00002424
Iteration 135/1000 | Loss: 0.00002424
Iteration 136/1000 | Loss: 0.00002424
Iteration 137/1000 | Loss: 0.00002424
Iteration 138/1000 | Loss: 0.00002424
Iteration 139/1000 | Loss: 0.00002424
Iteration 140/1000 | Loss: 0.00002424
Iteration 141/1000 | Loss: 0.00002424
Iteration 142/1000 | Loss: 0.00002424
Iteration 143/1000 | Loss: 0.00002424
Iteration 144/1000 | Loss: 0.00002424
Iteration 145/1000 | Loss: 0.00002424
Iteration 146/1000 | Loss: 0.00002423
Iteration 147/1000 | Loss: 0.00002423
Iteration 148/1000 | Loss: 0.00002423
Iteration 149/1000 | Loss: 0.00002423
Iteration 150/1000 | Loss: 0.00002423
Iteration 151/1000 | Loss: 0.00002423
Iteration 152/1000 | Loss: 0.00002423
Iteration 153/1000 | Loss: 0.00002423
Iteration 154/1000 | Loss: 0.00002423
Iteration 155/1000 | Loss: 0.00002422
Iteration 156/1000 | Loss: 0.00002422
Iteration 157/1000 | Loss: 0.00002422
Iteration 158/1000 | Loss: 0.00002422
Iteration 159/1000 | Loss: 0.00002422
Iteration 160/1000 | Loss: 0.00002422
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 160. Stopping optimization.
Last 5 losses: [2.4224844310083427e-05, 2.4224844310083427e-05, 2.4224844310083427e-05, 2.4224844310083427e-05, 2.4224844310083427e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4224844310083427e-05

Optimization complete. Final v2v error: 3.8899521827697754 mm

Highest mean error: 5.466151714324951 mm for frame 207

Lowest mean error: 3.2237651348114014 mm for frame 231

Saving results

Total time: 50.139668464660645
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_31_nl_1440/0002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_31_nl_1440/0002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_31_nl_1440/0002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01102248
Iteration 2/25 | Loss: 0.01102248
Iteration 3/25 | Loss: 0.01102247
Iteration 4/25 | Loss: 0.01102247
Iteration 5/25 | Loss: 0.01102247
Iteration 6/25 | Loss: 0.01102247
Iteration 7/25 | Loss: 0.01102247
Iteration 8/25 | Loss: 0.01102246
Iteration 9/25 | Loss: 0.00142823
Iteration 10/25 | Loss: 0.00115125
Iteration 11/25 | Loss: 0.00103051
Iteration 12/25 | Loss: 0.00103059
Iteration 13/25 | Loss: 0.00098304
Iteration 14/25 | Loss: 0.00098387
Iteration 15/25 | Loss: 0.00098014
Iteration 16/25 | Loss: 0.00097989
Iteration 17/25 | Loss: 0.00097665
Iteration 18/25 | Loss: 0.00098052
Iteration 19/25 | Loss: 0.00098197
Iteration 20/25 | Loss: 0.00098630
Iteration 21/25 | Loss: 0.00097709
Iteration 22/25 | Loss: 0.00097472
Iteration 23/25 | Loss: 0.00097347
Iteration 24/25 | Loss: 0.00097371
Iteration 25/25 | Loss: 0.00097342

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 8.27166462
Iteration 2/25 | Loss: 0.00151884
Iteration 3/25 | Loss: 0.00151884
Iteration 4/25 | Loss: 0.00151884
Iteration 5/25 | Loss: 0.00151883
Iteration 6/25 | Loss: 0.00151883
Iteration 7/25 | Loss: 0.00151883
Iteration 8/25 | Loss: 0.00151883
Iteration 9/25 | Loss: 0.00151883
Iteration 10/25 | Loss: 0.00151883
Iteration 11/25 | Loss: 0.00151883
Iteration 12/25 | Loss: 0.00151883
Iteration 13/25 | Loss: 0.00151883
Iteration 14/25 | Loss: 0.00151883
Iteration 15/25 | Loss: 0.00151883
Iteration 16/25 | Loss: 0.00151883
Iteration 17/25 | Loss: 0.00151883
Iteration 18/25 | Loss: 0.00151883
Iteration 19/25 | Loss: 0.00151883
Iteration 20/25 | Loss: 0.00151883
Iteration 21/25 | Loss: 0.00151883
Iteration 22/25 | Loss: 0.00151883
Iteration 23/25 | Loss: 0.00151883
Iteration 24/25 | Loss: 0.00151883
Iteration 25/25 | Loss: 0.00151883

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00151883
Iteration 2/1000 | Loss: 0.00003024
Iteration 3/1000 | Loss: 0.00004940
Iteration 4/1000 | Loss: 0.00001368
Iteration 5/1000 | Loss: 0.00001179
Iteration 6/1000 | Loss: 0.00001102
Iteration 7/1000 | Loss: 0.00006143
Iteration 8/1000 | Loss: 0.00001051
Iteration 9/1000 | Loss: 0.00001009
Iteration 10/1000 | Loss: 0.00004093
Iteration 11/1000 | Loss: 0.00000989
Iteration 12/1000 | Loss: 0.00000972
Iteration 13/1000 | Loss: 0.00000969
Iteration 14/1000 | Loss: 0.00000968
Iteration 15/1000 | Loss: 0.00000968
Iteration 16/1000 | Loss: 0.00000960
Iteration 17/1000 | Loss: 0.00000959
Iteration 18/1000 | Loss: 0.00000957
Iteration 19/1000 | Loss: 0.00000956
Iteration 20/1000 | Loss: 0.00000956
Iteration 21/1000 | Loss: 0.00000955
Iteration 22/1000 | Loss: 0.00000955
Iteration 23/1000 | Loss: 0.00000954
Iteration 24/1000 | Loss: 0.00000954
Iteration 25/1000 | Loss: 0.00004518
Iteration 26/1000 | Loss: 0.00002176
Iteration 27/1000 | Loss: 0.00006014
Iteration 28/1000 | Loss: 0.00002128
Iteration 29/1000 | Loss: 0.00002251
Iteration 30/1000 | Loss: 0.00001704
Iteration 31/1000 | Loss: 0.00005312
Iteration 32/1000 | Loss: 0.00001678
Iteration 33/1000 | Loss: 0.00005844
Iteration 34/1000 | Loss: 0.00000942
Iteration 35/1000 | Loss: 0.00000942
Iteration 36/1000 | Loss: 0.00001803
Iteration 37/1000 | Loss: 0.00001046
Iteration 38/1000 | Loss: 0.00001007
Iteration 39/1000 | Loss: 0.00001007
Iteration 40/1000 | Loss: 0.00001007
Iteration 41/1000 | Loss: 0.00001005
Iteration 42/1000 | Loss: 0.00000940
Iteration 43/1000 | Loss: 0.00000940
Iteration 44/1000 | Loss: 0.00000940
Iteration 45/1000 | Loss: 0.00000940
Iteration 46/1000 | Loss: 0.00000940
Iteration 47/1000 | Loss: 0.00000940
Iteration 48/1000 | Loss: 0.00000940
Iteration 49/1000 | Loss: 0.00000940
Iteration 50/1000 | Loss: 0.00000940
Iteration 51/1000 | Loss: 0.00000939
Iteration 52/1000 | Loss: 0.00001805
Iteration 53/1000 | Loss: 0.00001069
Iteration 54/1000 | Loss: 0.00001070
Iteration 55/1000 | Loss: 0.00000961
Iteration 56/1000 | Loss: 0.00000961
Iteration 57/1000 | Loss: 0.00000950
Iteration 58/1000 | Loss: 0.00000940
Iteration 59/1000 | Loss: 0.00001021
Iteration 60/1000 | Loss: 0.00000941
Iteration 61/1000 | Loss: 0.00000940
Iteration 62/1000 | Loss: 0.00000940
Iteration 63/1000 | Loss: 0.00000940
Iteration 64/1000 | Loss: 0.00000940
Iteration 65/1000 | Loss: 0.00000940
Iteration 66/1000 | Loss: 0.00000940
Iteration 67/1000 | Loss: 0.00000940
Iteration 68/1000 | Loss: 0.00000940
Iteration 69/1000 | Loss: 0.00000940
Iteration 70/1000 | Loss: 0.00000940
Iteration 71/1000 | Loss: 0.00000940
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 71. Stopping optimization.
Last 5 losses: [9.396112545800861e-06, 9.396112545800861e-06, 9.396112545800861e-06, 9.396112545800861e-06, 9.396112545800861e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.396112545800861e-06

Optimization complete. Final v2v error: 2.4967710971832275 mm

Highest mean error: 8.126995086669922 mm for frame 77

Lowest mean error: 2.176724910736084 mm for frame 205

Saving results

Total time: 84.61813521385193
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_31_nl_1440/0022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_31_nl_1440/0022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_31_nl_1440/0022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00783428
Iteration 2/25 | Loss: 0.00197409
Iteration 3/25 | Loss: 0.00123378
Iteration 4/25 | Loss: 0.00111534
Iteration 5/25 | Loss: 0.00112969
Iteration 6/25 | Loss: 0.00111125
Iteration 7/25 | Loss: 0.00108934
Iteration 8/25 | Loss: 0.00101270
Iteration 9/25 | Loss: 0.00101182
Iteration 10/25 | Loss: 0.00102352
Iteration 11/25 | Loss: 0.00103539
Iteration 12/25 | Loss: 0.00102890
Iteration 13/25 | Loss: 0.00102499
Iteration 14/25 | Loss: 0.00102019
Iteration 15/25 | Loss: 0.00101691
Iteration 16/25 | Loss: 0.00101429
Iteration 17/25 | Loss: 0.00101411
Iteration 18/25 | Loss: 0.00101343
Iteration 19/25 | Loss: 0.00100887
Iteration 20/25 | Loss: 0.00100943
Iteration 21/25 | Loss: 0.00100779
Iteration 22/25 | Loss: 0.00100694
Iteration 23/25 | Loss: 0.00100474
Iteration 24/25 | Loss: 0.00100414
Iteration 25/25 | Loss: 0.00100371

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.08447504
Iteration 2/25 | Loss: 0.00179833
Iteration 3/25 | Loss: 0.00179833
Iteration 4/25 | Loss: 0.00176027
Iteration 5/25 | Loss: 0.00176027
Iteration 6/25 | Loss: 0.00176027
Iteration 7/25 | Loss: 0.00176027
Iteration 8/25 | Loss: 0.00176027
Iteration 9/25 | Loss: 0.00176027
Iteration 10/25 | Loss: 0.00176027
Iteration 11/25 | Loss: 0.00176027
Iteration 12/25 | Loss: 0.00176027
Iteration 13/25 | Loss: 0.00176027
Iteration 14/25 | Loss: 0.00176027
Iteration 15/25 | Loss: 0.00176027
Iteration 16/25 | Loss: 0.00176027
Iteration 17/25 | Loss: 0.00176027
Iteration 18/25 | Loss: 0.00176027
Iteration 19/25 | Loss: 0.00176027
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0017602695152163506, 0.0017602695152163506, 0.0017602695152163506, 0.0017602695152163506, 0.0017602695152163506]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0017602695152163506

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00176027
Iteration 2/1000 | Loss: 0.00006197
Iteration 3/1000 | Loss: 0.00003357
Iteration 4/1000 | Loss: 0.00001840
Iteration 5/1000 | Loss: 0.00002503
Iteration 6/1000 | Loss: 0.00001502
Iteration 7/1000 | Loss: 0.00001433
Iteration 8/1000 | Loss: 0.00004582
Iteration 9/1000 | Loss: 0.00001377
Iteration 10/1000 | Loss: 0.00001335
Iteration 11/1000 | Loss: 0.00005061
Iteration 12/1000 | Loss: 0.00005582
Iteration 13/1000 | Loss: 0.00007719
Iteration 14/1000 | Loss: 0.00003791
Iteration 15/1000 | Loss: 0.00009462
Iteration 16/1000 | Loss: 0.00003968
Iteration 17/1000 | Loss: 0.00001307
Iteration 18/1000 | Loss: 0.00005772
Iteration 19/1000 | Loss: 0.00001386
Iteration 20/1000 | Loss: 0.00001274
Iteration 21/1000 | Loss: 0.00001273
Iteration 22/1000 | Loss: 0.00001263
Iteration 23/1000 | Loss: 0.00001252
Iteration 24/1000 | Loss: 0.00001445
Iteration 25/1000 | Loss: 0.00001234
Iteration 26/1000 | Loss: 0.00001231
Iteration 27/1000 | Loss: 0.00001205
Iteration 28/1000 | Loss: 0.00001200
Iteration 29/1000 | Loss: 0.00001190
Iteration 30/1000 | Loss: 0.00001186
Iteration 31/1000 | Loss: 0.00001186
Iteration 32/1000 | Loss: 0.00001186
Iteration 33/1000 | Loss: 0.00001185
Iteration 34/1000 | Loss: 0.00001184
Iteration 35/1000 | Loss: 0.00001183
Iteration 36/1000 | Loss: 0.00001182
Iteration 37/1000 | Loss: 0.00001182
Iteration 38/1000 | Loss: 0.00001182
Iteration 39/1000 | Loss: 0.00001182
Iteration 40/1000 | Loss: 0.00001181
Iteration 41/1000 | Loss: 0.00001181
Iteration 42/1000 | Loss: 0.00001180
Iteration 43/1000 | Loss: 0.00001180
Iteration 44/1000 | Loss: 0.00001179
Iteration 45/1000 | Loss: 0.00001179
Iteration 46/1000 | Loss: 0.00001179
Iteration 47/1000 | Loss: 0.00001178
Iteration 48/1000 | Loss: 0.00001176
Iteration 49/1000 | Loss: 0.00001176
Iteration 50/1000 | Loss: 0.00001175
Iteration 51/1000 | Loss: 0.00001175
Iteration 52/1000 | Loss: 0.00001175
Iteration 53/1000 | Loss: 0.00001174
Iteration 54/1000 | Loss: 0.00001174
Iteration 55/1000 | Loss: 0.00001174
Iteration 56/1000 | Loss: 0.00001173
Iteration 57/1000 | Loss: 0.00001173
Iteration 58/1000 | Loss: 0.00001173
Iteration 59/1000 | Loss: 0.00001173
Iteration 60/1000 | Loss: 0.00001172
Iteration 61/1000 | Loss: 0.00001172
Iteration 62/1000 | Loss: 0.00001172
Iteration 63/1000 | Loss: 0.00001172
Iteration 64/1000 | Loss: 0.00001172
Iteration 65/1000 | Loss: 0.00001172
Iteration 66/1000 | Loss: 0.00001172
Iteration 67/1000 | Loss: 0.00001172
Iteration 68/1000 | Loss: 0.00001172
Iteration 69/1000 | Loss: 0.00001171
Iteration 70/1000 | Loss: 0.00001171
Iteration 71/1000 | Loss: 0.00001171
Iteration 72/1000 | Loss: 0.00001171
Iteration 73/1000 | Loss: 0.00001170
Iteration 74/1000 | Loss: 0.00001170
Iteration 75/1000 | Loss: 0.00001170
Iteration 76/1000 | Loss: 0.00001170
Iteration 77/1000 | Loss: 0.00001170
Iteration 78/1000 | Loss: 0.00001170
Iteration 79/1000 | Loss: 0.00001169
Iteration 80/1000 | Loss: 0.00001169
Iteration 81/1000 | Loss: 0.00001169
Iteration 82/1000 | Loss: 0.00001169
Iteration 83/1000 | Loss: 0.00001169
Iteration 84/1000 | Loss: 0.00001169
Iteration 85/1000 | Loss: 0.00001169
Iteration 86/1000 | Loss: 0.00001169
Iteration 87/1000 | Loss: 0.00001168
Iteration 88/1000 | Loss: 0.00001168
Iteration 89/1000 | Loss: 0.00001168
Iteration 90/1000 | Loss: 0.00001168
Iteration 91/1000 | Loss: 0.00001168
Iteration 92/1000 | Loss: 0.00001168
Iteration 93/1000 | Loss: 0.00001168
Iteration 94/1000 | Loss: 0.00001168
Iteration 95/1000 | Loss: 0.00001168
Iteration 96/1000 | Loss: 0.00001168
Iteration 97/1000 | Loss: 0.00001167
Iteration 98/1000 | Loss: 0.00001167
Iteration 99/1000 | Loss: 0.00001167
Iteration 100/1000 | Loss: 0.00001167
Iteration 101/1000 | Loss: 0.00001167
Iteration 102/1000 | Loss: 0.00001167
Iteration 103/1000 | Loss: 0.00001167
Iteration 104/1000 | Loss: 0.00001167
Iteration 105/1000 | Loss: 0.00001167
Iteration 106/1000 | Loss: 0.00001167
Iteration 107/1000 | Loss: 0.00001167
Iteration 108/1000 | Loss: 0.00001167
Iteration 109/1000 | Loss: 0.00001167
Iteration 110/1000 | Loss: 0.00001167
Iteration 111/1000 | Loss: 0.00001167
Iteration 112/1000 | Loss: 0.00001167
Iteration 113/1000 | Loss: 0.00001166
Iteration 114/1000 | Loss: 0.00001166
Iteration 115/1000 | Loss: 0.00001166
Iteration 116/1000 | Loss: 0.00001166
Iteration 117/1000 | Loss: 0.00001166
Iteration 118/1000 | Loss: 0.00001166
Iteration 119/1000 | Loss: 0.00001166
Iteration 120/1000 | Loss: 0.00001166
Iteration 121/1000 | Loss: 0.00001166
Iteration 122/1000 | Loss: 0.00001166
Iteration 123/1000 | Loss: 0.00001166
Iteration 124/1000 | Loss: 0.00001166
Iteration 125/1000 | Loss: 0.00001166
Iteration 126/1000 | Loss: 0.00001166
Iteration 127/1000 | Loss: 0.00001166
Iteration 128/1000 | Loss: 0.00001166
Iteration 129/1000 | Loss: 0.00001166
Iteration 130/1000 | Loss: 0.00001165
Iteration 131/1000 | Loss: 0.00001165
Iteration 132/1000 | Loss: 0.00001165
Iteration 133/1000 | Loss: 0.00001165
Iteration 134/1000 | Loss: 0.00001165
Iteration 135/1000 | Loss: 0.00001165
Iteration 136/1000 | Loss: 0.00001165
Iteration 137/1000 | Loss: 0.00001165
Iteration 138/1000 | Loss: 0.00001165
Iteration 139/1000 | Loss: 0.00001165
Iteration 140/1000 | Loss: 0.00001165
Iteration 141/1000 | Loss: 0.00001165
Iteration 142/1000 | Loss: 0.00001164
Iteration 143/1000 | Loss: 0.00001164
Iteration 144/1000 | Loss: 0.00001164
Iteration 145/1000 | Loss: 0.00001164
Iteration 146/1000 | Loss: 0.00001164
Iteration 147/1000 | Loss: 0.00001164
Iteration 148/1000 | Loss: 0.00001164
Iteration 149/1000 | Loss: 0.00001163
Iteration 150/1000 | Loss: 0.00001163
Iteration 151/1000 | Loss: 0.00001163
Iteration 152/1000 | Loss: 0.00001163
Iteration 153/1000 | Loss: 0.00001163
Iteration 154/1000 | Loss: 0.00001163
Iteration 155/1000 | Loss: 0.00001163
Iteration 156/1000 | Loss: 0.00001163
Iteration 157/1000 | Loss: 0.00001162
Iteration 158/1000 | Loss: 0.00001162
Iteration 159/1000 | Loss: 0.00001162
Iteration 160/1000 | Loss: 0.00001162
Iteration 161/1000 | Loss: 0.00001162
Iteration 162/1000 | Loss: 0.00001162
Iteration 163/1000 | Loss: 0.00001162
Iteration 164/1000 | Loss: 0.00001162
Iteration 165/1000 | Loss: 0.00001162
Iteration 166/1000 | Loss: 0.00001162
Iteration 167/1000 | Loss: 0.00001162
Iteration 168/1000 | Loss: 0.00001161
Iteration 169/1000 | Loss: 0.00001161
Iteration 170/1000 | Loss: 0.00001161
Iteration 171/1000 | Loss: 0.00001161
Iteration 172/1000 | Loss: 0.00001161
Iteration 173/1000 | Loss: 0.00001161
Iteration 174/1000 | Loss: 0.00001161
Iteration 175/1000 | Loss: 0.00001161
Iteration 176/1000 | Loss: 0.00001161
Iteration 177/1000 | Loss: 0.00001161
Iteration 178/1000 | Loss: 0.00001160
Iteration 179/1000 | Loss: 0.00001160
Iteration 180/1000 | Loss: 0.00001160
Iteration 181/1000 | Loss: 0.00001160
Iteration 182/1000 | Loss: 0.00001160
Iteration 183/1000 | Loss: 0.00001160
Iteration 184/1000 | Loss: 0.00001160
Iteration 185/1000 | Loss: 0.00001160
Iteration 186/1000 | Loss: 0.00001160
Iteration 187/1000 | Loss: 0.00001160
Iteration 188/1000 | Loss: 0.00001160
Iteration 189/1000 | Loss: 0.00001160
Iteration 190/1000 | Loss: 0.00001160
Iteration 191/1000 | Loss: 0.00001160
Iteration 192/1000 | Loss: 0.00001160
Iteration 193/1000 | Loss: 0.00001160
Iteration 194/1000 | Loss: 0.00001160
Iteration 195/1000 | Loss: 0.00001160
Iteration 196/1000 | Loss: 0.00001160
Iteration 197/1000 | Loss: 0.00001160
Iteration 198/1000 | Loss: 0.00001160
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 198. Stopping optimization.
Last 5 losses: [1.1595665455388371e-05, 1.1595665455388371e-05, 1.1595665455388371e-05, 1.1595665455388371e-05, 1.1595665455388371e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1595665455388371e-05

Optimization complete. Final v2v error: 2.853590488433838 mm

Highest mean error: 3.898043394088745 mm for frame 124

Lowest mean error: 2.3979501724243164 mm for frame 237

Saving results

Total time: 107.74364113807678
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_31_nl_1440/0021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_31_nl_1440/0021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_31_nl_1440/0021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00862968
Iteration 2/25 | Loss: 0.00138479
Iteration 3/25 | Loss: 0.00109912
Iteration 4/25 | Loss: 0.00106656
Iteration 5/25 | Loss: 0.00106144
Iteration 6/25 | Loss: 0.00106042
Iteration 7/25 | Loss: 0.00106042
Iteration 8/25 | Loss: 0.00106042
Iteration 9/25 | Loss: 0.00106042
Iteration 10/25 | Loss: 0.00106042
Iteration 11/25 | Loss: 0.00106042
Iteration 12/25 | Loss: 0.00106042
Iteration 13/25 | Loss: 0.00106042
Iteration 14/25 | Loss: 0.00106042
Iteration 15/25 | Loss: 0.00106042
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0010604182025417686, 0.0010604182025417686, 0.0010604182025417686, 0.0010604182025417686, 0.0010604182025417686]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010604182025417686

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.23453891
Iteration 2/25 | Loss: 0.00157643
Iteration 3/25 | Loss: 0.00157640
Iteration 4/25 | Loss: 0.00157640
Iteration 5/25 | Loss: 0.00157640
Iteration 6/25 | Loss: 0.00157640
Iteration 7/25 | Loss: 0.00157640
Iteration 8/25 | Loss: 0.00157640
Iteration 9/25 | Loss: 0.00157640
Iteration 10/25 | Loss: 0.00157640
Iteration 11/25 | Loss: 0.00157640
Iteration 12/25 | Loss: 0.00157640
Iteration 13/25 | Loss: 0.00157640
Iteration 14/25 | Loss: 0.00157640
Iteration 15/25 | Loss: 0.00157640
Iteration 16/25 | Loss: 0.00157640
Iteration 17/25 | Loss: 0.00157640
Iteration 18/25 | Loss: 0.00157640
Iteration 19/25 | Loss: 0.00157640
Iteration 20/25 | Loss: 0.00157640
Iteration 21/25 | Loss: 0.00157640
Iteration 22/25 | Loss: 0.00157640
Iteration 23/25 | Loss: 0.00157640
Iteration 24/25 | Loss: 0.00157640
Iteration 25/25 | Loss: 0.00157640

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00157640
Iteration 2/1000 | Loss: 0.00003230
Iteration 3/1000 | Loss: 0.00002356
Iteration 4/1000 | Loss: 0.00002049
Iteration 5/1000 | Loss: 0.00001919
Iteration 6/1000 | Loss: 0.00001820
Iteration 7/1000 | Loss: 0.00001775
Iteration 8/1000 | Loss: 0.00001729
Iteration 9/1000 | Loss: 0.00001691
Iteration 10/1000 | Loss: 0.00001671
Iteration 11/1000 | Loss: 0.00001657
Iteration 12/1000 | Loss: 0.00001652
Iteration 13/1000 | Loss: 0.00001647
Iteration 14/1000 | Loss: 0.00001637
Iteration 15/1000 | Loss: 0.00001636
Iteration 16/1000 | Loss: 0.00001630
Iteration 17/1000 | Loss: 0.00001627
Iteration 18/1000 | Loss: 0.00001626
Iteration 19/1000 | Loss: 0.00001625
Iteration 20/1000 | Loss: 0.00001618
Iteration 21/1000 | Loss: 0.00001617
Iteration 22/1000 | Loss: 0.00001615
Iteration 23/1000 | Loss: 0.00001615
Iteration 24/1000 | Loss: 0.00001611
Iteration 25/1000 | Loss: 0.00001610
Iteration 26/1000 | Loss: 0.00001609
Iteration 27/1000 | Loss: 0.00001608
Iteration 28/1000 | Loss: 0.00001608
Iteration 29/1000 | Loss: 0.00001607
Iteration 30/1000 | Loss: 0.00001607
Iteration 31/1000 | Loss: 0.00001607
Iteration 32/1000 | Loss: 0.00001606
Iteration 33/1000 | Loss: 0.00001606
Iteration 34/1000 | Loss: 0.00001606
Iteration 35/1000 | Loss: 0.00001605
Iteration 36/1000 | Loss: 0.00001605
Iteration 37/1000 | Loss: 0.00001604
Iteration 38/1000 | Loss: 0.00001604
Iteration 39/1000 | Loss: 0.00001604
Iteration 40/1000 | Loss: 0.00001603
Iteration 41/1000 | Loss: 0.00001603
Iteration 42/1000 | Loss: 0.00001602
Iteration 43/1000 | Loss: 0.00001602
Iteration 44/1000 | Loss: 0.00001602
Iteration 45/1000 | Loss: 0.00001602
Iteration 46/1000 | Loss: 0.00001601
Iteration 47/1000 | Loss: 0.00001601
Iteration 48/1000 | Loss: 0.00001600
Iteration 49/1000 | Loss: 0.00001599
Iteration 50/1000 | Loss: 0.00001599
Iteration 51/1000 | Loss: 0.00001599
Iteration 52/1000 | Loss: 0.00001598
Iteration 53/1000 | Loss: 0.00001598
Iteration 54/1000 | Loss: 0.00001598
Iteration 55/1000 | Loss: 0.00001598
Iteration 56/1000 | Loss: 0.00001598
Iteration 57/1000 | Loss: 0.00001598
Iteration 58/1000 | Loss: 0.00001598
Iteration 59/1000 | Loss: 0.00001598
Iteration 60/1000 | Loss: 0.00001598
Iteration 61/1000 | Loss: 0.00001598
Iteration 62/1000 | Loss: 0.00001598
Iteration 63/1000 | Loss: 0.00001598
Iteration 64/1000 | Loss: 0.00001598
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 64. Stopping optimization.
Last 5 losses: [1.5981226169969887e-05, 1.5981226169969887e-05, 1.5981226169969887e-05, 1.5981226169969887e-05, 1.5981226169969887e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5981226169969887e-05

Optimization complete. Final v2v error: 3.2926454544067383 mm

Highest mean error: 4.143020153045654 mm for frame 176

Lowest mean error: 2.6968040466308594 mm for frame 23

Saving results

Total time: 36.98444938659668
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_31_nl_1440/0013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_31_nl_1440/0013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_31_nl_1440/0013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00884571
Iteration 2/25 | Loss: 0.00146885
Iteration 3/25 | Loss: 0.00113727
Iteration 4/25 | Loss: 0.00103314
Iteration 5/25 | Loss: 0.00101923
Iteration 6/25 | Loss: 0.00101559
Iteration 7/25 | Loss: 0.00101504
Iteration 8/25 | Loss: 0.00101471
Iteration 9/25 | Loss: 0.00101449
Iteration 10/25 | Loss: 0.00101394
Iteration 11/25 | Loss: 0.00101705
Iteration 12/25 | Loss: 0.00101434
Iteration 13/25 | Loss: 0.00101317
Iteration 14/25 | Loss: 0.00100994
Iteration 15/25 | Loss: 0.00100955
Iteration 16/25 | Loss: 0.00100942
Iteration 17/25 | Loss: 0.00100941
Iteration 18/25 | Loss: 0.00100941
Iteration 19/25 | Loss: 0.00100941
Iteration 20/25 | Loss: 0.00100941
Iteration 21/25 | Loss: 0.00100941
Iteration 22/25 | Loss: 0.00100941
Iteration 23/25 | Loss: 0.00100941
Iteration 24/25 | Loss: 0.00100941
Iteration 25/25 | Loss: 0.00100941

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.19670749
Iteration 2/25 | Loss: 0.00148690
Iteration 3/25 | Loss: 0.00148689
Iteration 4/25 | Loss: 0.00148689
Iteration 5/25 | Loss: 0.00148689
Iteration 6/25 | Loss: 0.00148689
Iteration 7/25 | Loss: 0.00148689
Iteration 8/25 | Loss: 0.00148689
Iteration 9/25 | Loss: 0.00148689
Iteration 10/25 | Loss: 0.00148689
Iteration 11/25 | Loss: 0.00148689
Iteration 12/25 | Loss: 0.00148689
Iteration 13/25 | Loss: 0.00148689
Iteration 14/25 | Loss: 0.00148689
Iteration 15/25 | Loss: 0.00148689
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0014868893194943666, 0.0014868893194943666, 0.0014868893194943666, 0.0014868893194943666, 0.0014868893194943666]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014868893194943666

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00148689
Iteration 2/1000 | Loss: 0.00003854
Iteration 3/1000 | Loss: 0.00002364
Iteration 4/1000 | Loss: 0.00001694
Iteration 5/1000 | Loss: 0.00001474
Iteration 6/1000 | Loss: 0.00001383
Iteration 7/1000 | Loss: 0.00001334
Iteration 8/1000 | Loss: 0.00001299
Iteration 9/1000 | Loss: 0.00001271
Iteration 10/1000 | Loss: 0.00001245
Iteration 11/1000 | Loss: 0.00001223
Iteration 12/1000 | Loss: 0.00001210
Iteration 13/1000 | Loss: 0.00001199
Iteration 14/1000 | Loss: 0.00001196
Iteration 15/1000 | Loss: 0.00001192
Iteration 16/1000 | Loss: 0.00001192
Iteration 17/1000 | Loss: 0.00001192
Iteration 18/1000 | Loss: 0.00001191
Iteration 19/1000 | Loss: 0.00001189
Iteration 20/1000 | Loss: 0.00001188
Iteration 21/1000 | Loss: 0.00001188
Iteration 22/1000 | Loss: 0.00001187
Iteration 23/1000 | Loss: 0.00001186
Iteration 24/1000 | Loss: 0.00001186
Iteration 25/1000 | Loss: 0.00001184
Iteration 26/1000 | Loss: 0.00001181
Iteration 27/1000 | Loss: 0.00001178
Iteration 28/1000 | Loss: 0.00001177
Iteration 29/1000 | Loss: 0.00001176
Iteration 30/1000 | Loss: 0.00001173
Iteration 31/1000 | Loss: 0.00001173
Iteration 32/1000 | Loss: 0.00001173
Iteration 33/1000 | Loss: 0.00001172
Iteration 34/1000 | Loss: 0.00001172
Iteration 35/1000 | Loss: 0.00001171
Iteration 36/1000 | Loss: 0.00001171
Iteration 37/1000 | Loss: 0.00001170
Iteration 38/1000 | Loss: 0.00001170
Iteration 39/1000 | Loss: 0.00001170
Iteration 40/1000 | Loss: 0.00001169
Iteration 41/1000 | Loss: 0.00001169
Iteration 42/1000 | Loss: 0.00001168
Iteration 43/1000 | Loss: 0.00001168
Iteration 44/1000 | Loss: 0.00001168
Iteration 45/1000 | Loss: 0.00001168
Iteration 46/1000 | Loss: 0.00001168
Iteration 47/1000 | Loss: 0.00001168
Iteration 48/1000 | Loss: 0.00001168
Iteration 49/1000 | Loss: 0.00001167
Iteration 50/1000 | Loss: 0.00001167
Iteration 51/1000 | Loss: 0.00001167
Iteration 52/1000 | Loss: 0.00001167
Iteration 53/1000 | Loss: 0.00001167
Iteration 54/1000 | Loss: 0.00001167
Iteration 55/1000 | Loss: 0.00001166
Iteration 56/1000 | Loss: 0.00001166
Iteration 57/1000 | Loss: 0.00001166
Iteration 58/1000 | Loss: 0.00001165
Iteration 59/1000 | Loss: 0.00001165
Iteration 60/1000 | Loss: 0.00001165
Iteration 61/1000 | Loss: 0.00001165
Iteration 62/1000 | Loss: 0.00001165
Iteration 63/1000 | Loss: 0.00001164
Iteration 64/1000 | Loss: 0.00001164
Iteration 65/1000 | Loss: 0.00001164
Iteration 66/1000 | Loss: 0.00001164
Iteration 67/1000 | Loss: 0.00001164
Iteration 68/1000 | Loss: 0.00001164
Iteration 69/1000 | Loss: 0.00001164
Iteration 70/1000 | Loss: 0.00001164
Iteration 71/1000 | Loss: 0.00001164
Iteration 72/1000 | Loss: 0.00001164
Iteration 73/1000 | Loss: 0.00001164
Iteration 74/1000 | Loss: 0.00001164
Iteration 75/1000 | Loss: 0.00001164
Iteration 76/1000 | Loss: 0.00001164
Iteration 77/1000 | Loss: 0.00001164
Iteration 78/1000 | Loss: 0.00001164
Iteration 79/1000 | Loss: 0.00001164
Iteration 80/1000 | Loss: 0.00001164
Iteration 81/1000 | Loss: 0.00001164
Iteration 82/1000 | Loss: 0.00001164
Iteration 83/1000 | Loss: 0.00001164
Iteration 84/1000 | Loss: 0.00001164
Iteration 85/1000 | Loss: 0.00001164
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 85. Stopping optimization.
Last 5 losses: [1.164377863460686e-05, 1.164377863460686e-05, 1.164377863460686e-05, 1.164377863460686e-05, 1.164377863460686e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.164377863460686e-05

Optimization complete. Final v2v error: 2.829713821411133 mm

Highest mean error: 4.096750259399414 mm for frame 0

Lowest mean error: 2.294116258621216 mm for frame 239

Saving results

Total time: 59.45900344848633
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_31_nl_1440/0005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_31_nl_1440/0005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_31_nl_1440/0005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01084975
Iteration 2/25 | Loss: 0.01084975
Iteration 3/25 | Loss: 0.01084975
Iteration 4/25 | Loss: 0.01084975
Iteration 5/25 | Loss: 0.01084975
Iteration 6/25 | Loss: 0.01084975
Iteration 7/25 | Loss: 0.01084975
Iteration 8/25 | Loss: 0.01084975
Iteration 9/25 | Loss: 0.01084975
Iteration 10/25 | Loss: 0.01084975
Iteration 11/25 | Loss: 0.01084975
Iteration 12/25 | Loss: 0.01084975
Iteration 13/25 | Loss: 0.01084975
Iteration 14/25 | Loss: 0.01084975
Iteration 15/25 | Loss: 0.01084975
Iteration 16/25 | Loss: 0.01084975
Iteration 17/25 | Loss: 0.01084974
Iteration 18/25 | Loss: 0.01084974
Iteration 19/25 | Loss: 0.01084974
Iteration 20/25 | Loss: 0.01084974
Iteration 21/25 | Loss: 0.01084974
Iteration 22/25 | Loss: 0.01084974
Iteration 23/25 | Loss: 0.01084974
Iteration 24/25 | Loss: 0.01084974
Iteration 25/25 | Loss: 0.01084974

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35439980
Iteration 2/25 | Loss: 0.12614353
Iteration 3/25 | Loss: 0.12608455
Iteration 4/25 | Loss: 0.12579107
Iteration 5/25 | Loss: 0.12579104
Iteration 6/25 | Loss: 0.12579104
Iteration 7/25 | Loss: 0.12579104
Iteration 8/25 | Loss: 0.12579101
Iteration 9/25 | Loss: 0.12579103
Iteration 10/25 | Loss: 0.12579103
Iteration 11/25 | Loss: 0.12579104
Iteration 12/25 | Loss: 0.12579101
Iteration 13/25 | Loss: 0.12579101
Iteration 14/25 | Loss: 0.12579101
Iteration 15/25 | Loss: 0.12579101
Iteration 16/25 | Loss: 0.12579101
Iteration 17/25 | Loss: 0.12579101
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.1257910132408142, 0.1257910132408142, 0.1257910132408142, 0.1257910132408142, 0.1257910132408142]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.1257910132408142

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.12579101
Iteration 2/1000 | Loss: 0.00094553
Iteration 3/1000 | Loss: 0.00061254
Iteration 4/1000 | Loss: 0.00028628
Iteration 5/1000 | Loss: 0.00009400
Iteration 6/1000 | Loss: 0.00024975
Iteration 7/1000 | Loss: 0.00010392
Iteration 8/1000 | Loss: 0.00007460
Iteration 9/1000 | Loss: 0.00004471
Iteration 10/1000 | Loss: 0.00007612
Iteration 11/1000 | Loss: 0.00005677
Iteration 12/1000 | Loss: 0.00015133
Iteration 13/1000 | Loss: 0.00003864
Iteration 14/1000 | Loss: 0.00006821
Iteration 15/1000 | Loss: 0.00003358
Iteration 16/1000 | Loss: 0.00004502
Iteration 17/1000 | Loss: 0.00004852
Iteration 18/1000 | Loss: 0.00007130
Iteration 19/1000 | Loss: 0.00003007
Iteration 20/1000 | Loss: 0.00002249
Iteration 21/1000 | Loss: 0.00009060
Iteration 22/1000 | Loss: 0.00008239
Iteration 23/1000 | Loss: 0.00002687
Iteration 24/1000 | Loss: 0.00009735
Iteration 25/1000 | Loss: 0.00002266
Iteration 26/1000 | Loss: 0.00011149
Iteration 27/1000 | Loss: 0.00001721
Iteration 28/1000 | Loss: 0.00002492
Iteration 29/1000 | Loss: 0.00004268
Iteration 30/1000 | Loss: 0.00001745
Iteration 31/1000 | Loss: 0.00003318
Iteration 32/1000 | Loss: 0.00038304
Iteration 33/1000 | Loss: 0.00003016
Iteration 34/1000 | Loss: 0.00007253
Iteration 35/1000 | Loss: 0.00002595
Iteration 36/1000 | Loss: 0.00001473
Iteration 37/1000 | Loss: 0.00003798
Iteration 38/1000 | Loss: 0.00006060
Iteration 39/1000 | Loss: 0.00001851
Iteration 40/1000 | Loss: 0.00002138
Iteration 41/1000 | Loss: 0.00002136
Iteration 42/1000 | Loss: 0.00001817
Iteration 43/1000 | Loss: 0.00001655
Iteration 44/1000 | Loss: 0.00009455
Iteration 45/1000 | Loss: 0.00001390
Iteration 46/1000 | Loss: 0.00001312
Iteration 47/1000 | Loss: 0.00001905
Iteration 48/1000 | Loss: 0.00001307
Iteration 49/1000 | Loss: 0.00001290
Iteration 50/1000 | Loss: 0.00001289
Iteration 51/1000 | Loss: 0.00001289
Iteration 52/1000 | Loss: 0.00001288
Iteration 53/1000 | Loss: 0.00001288
Iteration 54/1000 | Loss: 0.00001288
Iteration 55/1000 | Loss: 0.00001288
Iteration 56/1000 | Loss: 0.00001288
Iteration 57/1000 | Loss: 0.00001288
Iteration 58/1000 | Loss: 0.00001288
Iteration 59/1000 | Loss: 0.00001288
Iteration 60/1000 | Loss: 0.00001290
Iteration 61/1000 | Loss: 0.00001290
Iteration 62/1000 | Loss: 0.00001288
Iteration 63/1000 | Loss: 0.00001288
Iteration 64/1000 | Loss: 0.00001288
Iteration 65/1000 | Loss: 0.00001287
Iteration 66/1000 | Loss: 0.00001287
Iteration 67/1000 | Loss: 0.00001287
Iteration 68/1000 | Loss: 0.00001287
Iteration 69/1000 | Loss: 0.00001287
Iteration 70/1000 | Loss: 0.00001287
Iteration 71/1000 | Loss: 0.00001287
Iteration 72/1000 | Loss: 0.00001285
Iteration 73/1000 | Loss: 0.00001284
Iteration 74/1000 | Loss: 0.00001284
Iteration 75/1000 | Loss: 0.00001283
Iteration 76/1000 | Loss: 0.00001283
Iteration 77/1000 | Loss: 0.00001282
Iteration 78/1000 | Loss: 0.00001282
Iteration 79/1000 | Loss: 0.00001845
Iteration 80/1000 | Loss: 0.00002474
Iteration 81/1000 | Loss: 0.00002762
Iteration 82/1000 | Loss: 0.00001291
Iteration 83/1000 | Loss: 0.00001284
Iteration 84/1000 | Loss: 0.00001275
Iteration 85/1000 | Loss: 0.00001274
Iteration 86/1000 | Loss: 0.00001274
Iteration 87/1000 | Loss: 0.00001274
Iteration 88/1000 | Loss: 0.00001274
Iteration 89/1000 | Loss: 0.00001274
Iteration 90/1000 | Loss: 0.00001274
Iteration 91/1000 | Loss: 0.00001279
Iteration 92/1000 | Loss: 0.00001279
Iteration 93/1000 | Loss: 0.00001275
Iteration 94/1000 | Loss: 0.00001731
Iteration 95/1000 | Loss: 0.00001325
Iteration 96/1000 | Loss: 0.00001270
Iteration 97/1000 | Loss: 0.00001269
Iteration 98/1000 | Loss: 0.00001269
Iteration 99/1000 | Loss: 0.00001269
Iteration 100/1000 | Loss: 0.00001269
Iteration 101/1000 | Loss: 0.00001269
Iteration 102/1000 | Loss: 0.00001304
Iteration 103/1000 | Loss: 0.00001267
Iteration 104/1000 | Loss: 0.00001267
Iteration 105/1000 | Loss: 0.00001267
Iteration 106/1000 | Loss: 0.00001267
Iteration 107/1000 | Loss: 0.00001266
Iteration 108/1000 | Loss: 0.00001266
Iteration 109/1000 | Loss: 0.00001266
Iteration 110/1000 | Loss: 0.00001266
Iteration 111/1000 | Loss: 0.00001266
Iteration 112/1000 | Loss: 0.00001266
Iteration 113/1000 | Loss: 0.00001266
Iteration 114/1000 | Loss: 0.00001266
Iteration 115/1000 | Loss: 0.00001266
Iteration 116/1000 | Loss: 0.00001266
Iteration 117/1000 | Loss: 0.00001442
Iteration 118/1000 | Loss: 0.00001895
Iteration 119/1000 | Loss: 0.00001264
Iteration 120/1000 | Loss: 0.00001264
Iteration 121/1000 | Loss: 0.00001263
Iteration 122/1000 | Loss: 0.00001263
Iteration 123/1000 | Loss: 0.00001263
Iteration 124/1000 | Loss: 0.00001496
Iteration 125/1000 | Loss: 0.00001264
Iteration 126/1000 | Loss: 0.00001264
Iteration 127/1000 | Loss: 0.00001263
Iteration 128/1000 | Loss: 0.00001360
Iteration 129/1000 | Loss: 0.00001261
Iteration 130/1000 | Loss: 0.00001261
Iteration 131/1000 | Loss: 0.00001261
Iteration 132/1000 | Loss: 0.00001261
Iteration 133/1000 | Loss: 0.00001261
Iteration 134/1000 | Loss: 0.00001261
Iteration 135/1000 | Loss: 0.00001261
Iteration 136/1000 | Loss: 0.00001260
Iteration 137/1000 | Loss: 0.00001260
Iteration 138/1000 | Loss: 0.00001259
Iteration 139/1000 | Loss: 0.00001259
Iteration 140/1000 | Loss: 0.00001259
Iteration 141/1000 | Loss: 0.00001259
Iteration 142/1000 | Loss: 0.00001259
Iteration 143/1000 | Loss: 0.00001274
Iteration 144/1000 | Loss: 0.00001274
Iteration 145/1000 | Loss: 0.00001859
Iteration 146/1000 | Loss: 0.00001302
Iteration 147/1000 | Loss: 0.00001364
Iteration 148/1000 | Loss: 0.00001272
Iteration 149/1000 | Loss: 0.00001258
Iteration 150/1000 | Loss: 0.00001336
Iteration 151/1000 | Loss: 0.00001380
Iteration 152/1000 | Loss: 0.00001258
Iteration 153/1000 | Loss: 0.00001265
Iteration 154/1000 | Loss: 0.00001264
Iteration 155/1000 | Loss: 0.00002413
Iteration 156/1000 | Loss: 0.00002102
Iteration 157/1000 | Loss: 0.00001278
Iteration 158/1000 | Loss: 0.00001571
Iteration 159/1000 | Loss: 0.00001251
Iteration 160/1000 | Loss: 0.00001241
Iteration 161/1000 | Loss: 0.00001241
Iteration 162/1000 | Loss: 0.00001241
Iteration 163/1000 | Loss: 0.00001240
Iteration 164/1000 | Loss: 0.00001240
Iteration 165/1000 | Loss: 0.00001240
Iteration 166/1000 | Loss: 0.00001240
Iteration 167/1000 | Loss: 0.00001240
Iteration 168/1000 | Loss: 0.00001240
Iteration 169/1000 | Loss: 0.00001240
Iteration 170/1000 | Loss: 0.00001240
Iteration 171/1000 | Loss: 0.00001252
Iteration 172/1000 | Loss: 0.00001245
Iteration 173/1000 | Loss: 0.00001239
Iteration 174/1000 | Loss: 0.00001239
Iteration 175/1000 | Loss: 0.00001239
Iteration 176/1000 | Loss: 0.00001239
Iteration 177/1000 | Loss: 0.00001239
Iteration 178/1000 | Loss: 0.00001239
Iteration 179/1000 | Loss: 0.00001250
Iteration 180/1000 | Loss: 0.00001245
Iteration 181/1000 | Loss: 0.00001245
Iteration 182/1000 | Loss: 0.00001245
Iteration 183/1000 | Loss: 0.00001244
Iteration 184/1000 | Loss: 0.00001242
Iteration 185/1000 | Loss: 0.00001242
Iteration 186/1000 | Loss: 0.00001241
Iteration 187/1000 | Loss: 0.00001241
Iteration 188/1000 | Loss: 0.00001240
Iteration 189/1000 | Loss: 0.00001240
Iteration 190/1000 | Loss: 0.00001240
Iteration 191/1000 | Loss: 0.00001240
Iteration 192/1000 | Loss: 0.00001239
Iteration 193/1000 | Loss: 0.00001239
Iteration 194/1000 | Loss: 0.00001239
Iteration 195/1000 | Loss: 0.00001238
Iteration 196/1000 | Loss: 0.00001238
Iteration 197/1000 | Loss: 0.00001238
Iteration 198/1000 | Loss: 0.00001238
Iteration 199/1000 | Loss: 0.00001237
Iteration 200/1000 | Loss: 0.00001237
Iteration 201/1000 | Loss: 0.00001237
Iteration 202/1000 | Loss: 0.00001237
Iteration 203/1000 | Loss: 0.00001237
Iteration 204/1000 | Loss: 0.00001237
Iteration 205/1000 | Loss: 0.00001247
Iteration 206/1000 | Loss: 0.00001240
Iteration 207/1000 | Loss: 0.00001239
Iteration 208/1000 | Loss: 0.00001239
Iteration 209/1000 | Loss: 0.00001238
Iteration 210/1000 | Loss: 0.00001238
Iteration 211/1000 | Loss: 0.00001237
Iteration 212/1000 | Loss: 0.00001237
Iteration 213/1000 | Loss: 0.00001244
Iteration 214/1000 | Loss: 0.00001244
Iteration 215/1000 | Loss: 0.00001242
Iteration 216/1000 | Loss: 0.00001237
Iteration 217/1000 | Loss: 0.00001237
Iteration 218/1000 | Loss: 0.00001237
Iteration 219/1000 | Loss: 0.00001236
Iteration 220/1000 | Loss: 0.00001236
Iteration 221/1000 | Loss: 0.00001236
Iteration 222/1000 | Loss: 0.00001236
Iteration 223/1000 | Loss: 0.00001236
Iteration 224/1000 | Loss: 0.00001236
Iteration 225/1000 | Loss: 0.00001236
Iteration 226/1000 | Loss: 0.00001242
Iteration 227/1000 | Loss: 0.00001242
Iteration 228/1000 | Loss: 0.00001236
Iteration 229/1000 | Loss: 0.00001236
Iteration 230/1000 | Loss: 0.00001236
Iteration 231/1000 | Loss: 0.00001236
Iteration 232/1000 | Loss: 0.00001236
Iteration 233/1000 | Loss: 0.00001236
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 233. Stopping optimization.
Last 5 losses: [1.2359607353573665e-05, 1.2359607353573665e-05, 1.2359607353573665e-05, 1.2359607353573665e-05, 1.2359607353573665e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2359607353573665e-05

Optimization complete. Final v2v error: 2.3702311515808105 mm

Highest mean error: 21.448062896728516 mm for frame 54

Lowest mean error: 2.010099411010742 mm for frame 8

Saving results

Total time: 131.68128275871277
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_31_nl_1440/0010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_31_nl_1440/0010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_31_nl_1440/0010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01119783
Iteration 2/25 | Loss: 0.00415810
Iteration 3/25 | Loss: 0.00229822
Iteration 4/25 | Loss: 0.00197652
Iteration 5/25 | Loss: 0.00252627
Iteration 6/25 | Loss: 0.00240790
Iteration 7/25 | Loss: 0.00147041
Iteration 8/25 | Loss: 0.00134696
Iteration 9/25 | Loss: 0.00132641
Iteration 10/25 | Loss: 0.00131993
Iteration 11/25 | Loss: 0.00131731
Iteration 12/25 | Loss: 0.00131669
Iteration 13/25 | Loss: 0.00131786
Iteration 14/25 | Loss: 0.00131816
Iteration 15/25 | Loss: 0.00131734
Iteration 16/25 | Loss: 0.00131752
Iteration 17/25 | Loss: 0.00131715
Iteration 18/25 | Loss: 0.00131736
Iteration 19/25 | Loss: 0.00131699
Iteration 20/25 | Loss: 0.00131637
Iteration 21/25 | Loss: 0.00131703
Iteration 22/25 | Loss: 0.00131513
Iteration 23/25 | Loss: 0.00131446
Iteration 24/25 | Loss: 0.00131387
Iteration 25/25 | Loss: 0.00131386

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.62768650
Iteration 2/25 | Loss: 0.00131015
Iteration 3/25 | Loss: 0.00131015
Iteration 4/25 | Loss: 0.00131015
Iteration 5/25 | Loss: 0.00131015
Iteration 6/25 | Loss: 0.00131015
Iteration 7/25 | Loss: 0.00131015
Iteration 8/25 | Loss: 0.00131015
Iteration 9/25 | Loss: 0.00131015
Iteration 10/25 | Loss: 0.00131015
Iteration 11/25 | Loss: 0.00131015
Iteration 12/25 | Loss: 0.00131015
Iteration 13/25 | Loss: 0.00131015
Iteration 14/25 | Loss: 0.00131015
Iteration 15/25 | Loss: 0.00131015
Iteration 16/25 | Loss: 0.00131015
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.001310145715251565, 0.001310145715251565, 0.001310145715251565, 0.001310145715251565, 0.001310145715251565]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001310145715251565

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00131015
Iteration 2/1000 | Loss: 0.00012634
Iteration 3/1000 | Loss: 0.00018240
Iteration 4/1000 | Loss: 0.00014234
Iteration 5/1000 | Loss: 0.00016151
Iteration 6/1000 | Loss: 0.00012571
Iteration 7/1000 | Loss: 0.00015901
Iteration 8/1000 | Loss: 0.00012439
Iteration 9/1000 | Loss: 0.00018850
Iteration 10/1000 | Loss: 0.00015146
Iteration 11/1000 | Loss: 0.00023849
Iteration 12/1000 | Loss: 0.00008766
Iteration 13/1000 | Loss: 0.00008908
Iteration 14/1000 | Loss: 0.00008987
Iteration 15/1000 | Loss: 0.00007919
Iteration 16/1000 | Loss: 0.00008353
Iteration 17/1000 | Loss: 0.00009340
Iteration 18/1000 | Loss: 0.00007654
Iteration 19/1000 | Loss: 0.00008904
Iteration 20/1000 | Loss: 0.00008959
Iteration 21/1000 | Loss: 0.00009824
Iteration 22/1000 | Loss: 0.00008172
Iteration 23/1000 | Loss: 0.00008561
Iteration 24/1000 | Loss: 0.00008007
Iteration 25/1000 | Loss: 0.00008656
Iteration 26/1000 | Loss: 0.00008580
Iteration 27/1000 | Loss: 0.00008643
Iteration 28/1000 | Loss: 0.00009023
Iteration 29/1000 | Loss: 0.00008370
Iteration 30/1000 | Loss: 0.00009266
Iteration 31/1000 | Loss: 0.00008295
Iteration 32/1000 | Loss: 0.00008647
Iteration 33/1000 | Loss: 0.00008726
Iteration 34/1000 | Loss: 0.00008502
Iteration 35/1000 | Loss: 0.00009098
Iteration 36/1000 | Loss: 0.00009355
Iteration 37/1000 | Loss: 0.00007824
Iteration 38/1000 | Loss: 0.00008720
Iteration 39/1000 | Loss: 0.00008190
Iteration 40/1000 | Loss: 0.00008081
Iteration 41/1000 | Loss: 0.00008002
Iteration 42/1000 | Loss: 0.00010390
Iteration 43/1000 | Loss: 0.00008896
Iteration 44/1000 | Loss: 0.00008858
Iteration 45/1000 | Loss: 0.00008668
Iteration 46/1000 | Loss: 0.00009538
Iteration 47/1000 | Loss: 0.00008661
Iteration 48/1000 | Loss: 0.00008724
Iteration 49/1000 | Loss: 0.00009039
Iteration 50/1000 | Loss: 0.00008219
Iteration 51/1000 | Loss: 0.00008369
Iteration 52/1000 | Loss: 0.00008983
Iteration 53/1000 | Loss: 0.00008296
Iteration 54/1000 | Loss: 0.00008575
Iteration 55/1000 | Loss: 0.00008577
Iteration 56/1000 | Loss: 0.00009290
Iteration 57/1000 | Loss: 0.00007845
Iteration 58/1000 | Loss: 0.00008208
Iteration 59/1000 | Loss: 0.00007808
Iteration 60/1000 | Loss: 0.00007796
Iteration 61/1000 | Loss: 0.00007419
Iteration 62/1000 | Loss: 0.00007486
Iteration 63/1000 | Loss: 0.00007232
Iteration 64/1000 | Loss: 0.00006828
Iteration 65/1000 | Loss: 0.00007503
Iteration 66/1000 | Loss: 0.00007697
Iteration 67/1000 | Loss: 0.00007241
Iteration 68/1000 | Loss: 0.00007425
Iteration 69/1000 | Loss: 0.00007451
Iteration 70/1000 | Loss: 0.00007650
Iteration 71/1000 | Loss: 0.00007041
Iteration 72/1000 | Loss: 0.00007528
Iteration 73/1000 | Loss: 0.00007657
Iteration 74/1000 | Loss: 0.00007084
Iteration 75/1000 | Loss: 0.00007431
Iteration 76/1000 | Loss: 0.00007579
Iteration 77/1000 | Loss: 0.00007476
Iteration 78/1000 | Loss: 0.00006802
Iteration 79/1000 | Loss: 0.00006921
Iteration 80/1000 | Loss: 0.00006779
Iteration 81/1000 | Loss: 0.00006852
Iteration 82/1000 | Loss: 0.00006922
Iteration 83/1000 | Loss: 0.00006971
Iteration 84/1000 | Loss: 0.00006711
Iteration 85/1000 | Loss: 0.00007014
Iteration 86/1000 | Loss: 0.00006915
Iteration 87/1000 | Loss: 0.00006928
Iteration 88/1000 | Loss: 0.00007348
Iteration 89/1000 | Loss: 0.00007348
Iteration 90/1000 | Loss: 0.00007330
Iteration 91/1000 | Loss: 0.00007097
Iteration 92/1000 | Loss: 0.00007078
Iteration 93/1000 | Loss: 0.00007426
Iteration 94/1000 | Loss: 0.00007167
Iteration 95/1000 | Loss: 0.00007125
Iteration 96/1000 | Loss: 0.00007338
Iteration 97/1000 | Loss: 0.00007406
Iteration 98/1000 | Loss: 0.00007305
Iteration 99/1000 | Loss: 0.00007381
Iteration 100/1000 | Loss: 0.00007441
Iteration 101/1000 | Loss: 0.00007733
Iteration 102/1000 | Loss: 0.00007843
Iteration 103/1000 | Loss: 0.00007604
Iteration 104/1000 | Loss: 0.00008033
Iteration 105/1000 | Loss: 0.00007456
Iteration 106/1000 | Loss: 0.00007610
Iteration 107/1000 | Loss: 0.00007380
Iteration 108/1000 | Loss: 0.00007133
Iteration 109/1000 | Loss: 0.00006721
Iteration 110/1000 | Loss: 0.00007341
Iteration 111/1000 | Loss: 0.00007499
Iteration 112/1000 | Loss: 0.00007280
Iteration 113/1000 | Loss: 0.00007362
Iteration 114/1000 | Loss: 0.00006663
Iteration 115/1000 | Loss: 0.00007477
Iteration 116/1000 | Loss: 0.00006716
Iteration 117/1000 | Loss: 0.00007304
Iteration 118/1000 | Loss: 0.00007407
Iteration 119/1000 | Loss: 0.00007607
Iteration 120/1000 | Loss: 0.00007400
Iteration 121/1000 | Loss: 0.00007339
Iteration 122/1000 | Loss: 0.00007886
Iteration 123/1000 | Loss: 0.00007469
Iteration 124/1000 | Loss: 0.00007358
Iteration 125/1000 | Loss: 0.00007328
Iteration 126/1000 | Loss: 0.00007336
Iteration 127/1000 | Loss: 0.00007348
Iteration 128/1000 | Loss: 0.00007600
Iteration 129/1000 | Loss: 0.00007860
Iteration 130/1000 | Loss: 0.00007388
Iteration 131/1000 | Loss: 0.00007539
Iteration 132/1000 | Loss: 0.00007360
Iteration 133/1000 | Loss: 0.00007437
Iteration 134/1000 | Loss: 0.00007455
Iteration 135/1000 | Loss: 0.00007305
Iteration 136/1000 | Loss: 0.00007416
Iteration 137/1000 | Loss: 0.00007334
Iteration 138/1000 | Loss: 0.00007342
Iteration 139/1000 | Loss: 0.00007424
Iteration 140/1000 | Loss: 0.00007414
Iteration 141/1000 | Loss: 0.00007410
Iteration 142/1000 | Loss: 0.00007502
Iteration 143/1000 | Loss: 0.00007151
Iteration 144/1000 | Loss: 0.00006868
Iteration 145/1000 | Loss: 0.00007046
Iteration 146/1000 | Loss: 0.00007353
Iteration 147/1000 | Loss: 0.00007412
Iteration 148/1000 | Loss: 0.00007412
Iteration 149/1000 | Loss: 0.00007451
Iteration 150/1000 | Loss: 0.00007366
Iteration 151/1000 | Loss: 0.00007164
Iteration 152/1000 | Loss: 0.00006873
Iteration 153/1000 | Loss: 0.00007343
Iteration 154/1000 | Loss: 0.00007342
Iteration 155/1000 | Loss: 0.00006854
Iteration 156/1000 | Loss: 0.00007525
Iteration 157/1000 | Loss: 0.00007401
Iteration 158/1000 | Loss: 0.00006561
Iteration 159/1000 | Loss: 0.00006737
Iteration 160/1000 | Loss: 0.00007318
Iteration 161/1000 | Loss: 0.00007516
Iteration 162/1000 | Loss: 0.00007305
Iteration 163/1000 | Loss: 0.00007401
Iteration 164/1000 | Loss: 0.00007305
Iteration 165/1000 | Loss: 0.00007277
Iteration 166/1000 | Loss: 0.00007109
Iteration 167/1000 | Loss: 0.00007283
Iteration 168/1000 | Loss: 0.00007323
Iteration 169/1000 | Loss: 0.00007310
Iteration 170/1000 | Loss: 0.00006999
Iteration 171/1000 | Loss: 0.00007015
Iteration 172/1000 | Loss: 0.00006901
Iteration 173/1000 | Loss: 0.00006939
Iteration 174/1000 | Loss: 0.00007362
Iteration 175/1000 | Loss: 0.00007197
Iteration 176/1000 | Loss: 0.00007680
Iteration 177/1000 | Loss: 0.00007299
Iteration 178/1000 | Loss: 0.00007476
Iteration 179/1000 | Loss: 0.00007128
Iteration 180/1000 | Loss: 0.00007437
Iteration 181/1000 | Loss: 0.00006797
Iteration 182/1000 | Loss: 0.00007291
Iteration 183/1000 | Loss: 0.00006814
Iteration 184/1000 | Loss: 0.00008417
Iteration 185/1000 | Loss: 0.00007913
Iteration 186/1000 | Loss: 0.00007661
Iteration 187/1000 | Loss: 0.00007405
Iteration 188/1000 | Loss: 0.00007320
Iteration 189/1000 | Loss: 0.00007378
Iteration 190/1000 | Loss: 0.00007373
Iteration 191/1000 | Loss: 0.00007334
Iteration 192/1000 | Loss: 0.00007492
Iteration 193/1000 | Loss: 0.00007242
Iteration 194/1000 | Loss: 0.00007336
Iteration 195/1000 | Loss: 0.00007348
Iteration 196/1000 | Loss: 0.00007323
Iteration 197/1000 | Loss: 0.00007991
Iteration 198/1000 | Loss: 0.00007414
Iteration 199/1000 | Loss: 0.00007836
Iteration 200/1000 | Loss: 0.00006807
Iteration 201/1000 | Loss: 0.00006597
Iteration 202/1000 | Loss: 0.00006485
Iteration 203/1000 | Loss: 0.00006422
Iteration 204/1000 | Loss: 0.00006380
Iteration 205/1000 | Loss: 0.00006354
Iteration 206/1000 | Loss: 0.00006330
Iteration 207/1000 | Loss: 0.00006314
Iteration 208/1000 | Loss: 0.00006303
Iteration 209/1000 | Loss: 0.00006297
Iteration 210/1000 | Loss: 0.00006297
Iteration 211/1000 | Loss: 0.00006296
Iteration 212/1000 | Loss: 0.00006296
Iteration 213/1000 | Loss: 0.00006296
Iteration 214/1000 | Loss: 0.00006296
Iteration 215/1000 | Loss: 0.00006296
Iteration 216/1000 | Loss: 0.00006296
Iteration 217/1000 | Loss: 0.00006296
Iteration 218/1000 | Loss: 0.00006296
Iteration 219/1000 | Loss: 0.00006295
Iteration 220/1000 | Loss: 0.00006295
Iteration 221/1000 | Loss: 0.00006294
Iteration 222/1000 | Loss: 0.00006294
Iteration 223/1000 | Loss: 0.00006294
Iteration 224/1000 | Loss: 0.00006294
Iteration 225/1000 | Loss: 0.00006294
Iteration 226/1000 | Loss: 0.00006293
Iteration 227/1000 | Loss: 0.00006292
Iteration 228/1000 | Loss: 0.00006291
Iteration 229/1000 | Loss: 0.00006291
Iteration 230/1000 | Loss: 0.00006288
Iteration 231/1000 | Loss: 0.00006286
Iteration 232/1000 | Loss: 0.00006285
Iteration 233/1000 | Loss: 0.00006285
Iteration 234/1000 | Loss: 0.00006285
Iteration 235/1000 | Loss: 0.00006285
Iteration 236/1000 | Loss: 0.00006285
Iteration 237/1000 | Loss: 0.00006285
Iteration 238/1000 | Loss: 0.00006285
Iteration 239/1000 | Loss: 0.00006285
Iteration 240/1000 | Loss: 0.00006284
Iteration 241/1000 | Loss: 0.00006284
Iteration 242/1000 | Loss: 0.00006284
Iteration 243/1000 | Loss: 0.00006284
Iteration 244/1000 | Loss: 0.00006284
Iteration 245/1000 | Loss: 0.00006284
Iteration 246/1000 | Loss: 0.00006283
Iteration 247/1000 | Loss: 0.00006283
Iteration 248/1000 | Loss: 0.00006283
Iteration 249/1000 | Loss: 0.00006283
Iteration 250/1000 | Loss: 0.00006283
Iteration 251/1000 | Loss: 0.00006283
Iteration 252/1000 | Loss: 0.00006283
Iteration 253/1000 | Loss: 0.00006283
Iteration 254/1000 | Loss: 0.00006283
Iteration 255/1000 | Loss: 0.00006283
Iteration 256/1000 | Loss: 0.00006282
Iteration 257/1000 | Loss: 0.00006282
Iteration 258/1000 | Loss: 0.00006281
Iteration 259/1000 | Loss: 0.00006281
Iteration 260/1000 | Loss: 0.00006281
Iteration 261/1000 | Loss: 0.00006281
Iteration 262/1000 | Loss: 0.00006281
Iteration 263/1000 | Loss: 0.00006281
Iteration 264/1000 | Loss: 0.00006281
Iteration 265/1000 | Loss: 0.00006281
Iteration 266/1000 | Loss: 0.00006281
Iteration 267/1000 | Loss: 0.00006281
Iteration 268/1000 | Loss: 0.00006281
Iteration 269/1000 | Loss: 0.00006281
Iteration 270/1000 | Loss: 0.00006281
Iteration 271/1000 | Loss: 0.00006281
Iteration 272/1000 | Loss: 0.00006281
Iteration 273/1000 | Loss: 0.00006281
Iteration 274/1000 | Loss: 0.00006281
Iteration 275/1000 | Loss: 0.00006281
Iteration 276/1000 | Loss: 0.00006281
Iteration 277/1000 | Loss: 0.00006280
Iteration 278/1000 | Loss: 0.00006280
Iteration 279/1000 | Loss: 0.00006280
Iteration 280/1000 | Loss: 0.00006280
Iteration 281/1000 | Loss: 0.00006280
Iteration 282/1000 | Loss: 0.00006280
Iteration 283/1000 | Loss: 0.00006280
Iteration 284/1000 | Loss: 0.00006280
Iteration 285/1000 | Loss: 0.00006280
Iteration 286/1000 | Loss: 0.00006280
Iteration 287/1000 | Loss: 0.00006280
Iteration 288/1000 | Loss: 0.00006280
Iteration 289/1000 | Loss: 0.00006280
Iteration 290/1000 | Loss: 0.00006280
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 290. Stopping optimization.
Last 5 losses: [6.279938679654151e-05, 6.279938679654151e-05, 6.279938679654151e-05, 6.279938679654151e-05, 6.279938679654151e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 6.279938679654151e-05

Optimization complete. Final v2v error: 6.159453392028809 mm

Highest mean error: 7.1562886238098145 mm for frame 246

Lowest mean error: 5.893350601196289 mm for frame 249

Saving results

Total time: 400.81410813331604
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_31_nl_1440/0004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_31_nl_1440/0004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_31_nl_1440/0004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01019303
Iteration 2/25 | Loss: 0.00277325
Iteration 3/25 | Loss: 0.00183107
Iteration 4/25 | Loss: 0.00157537
Iteration 5/25 | Loss: 0.00138356
Iteration 6/25 | Loss: 0.00146776
Iteration 7/25 | Loss: 0.00140089
Iteration 8/25 | Loss: 0.00141149
Iteration 9/25 | Loss: 0.00128988
Iteration 10/25 | Loss: 0.00117407
Iteration 11/25 | Loss: 0.00113882
Iteration 12/25 | Loss: 0.00111721
Iteration 13/25 | Loss: 0.00112151
Iteration 14/25 | Loss: 0.00112059
Iteration 15/25 | Loss: 0.00110988
Iteration 16/25 | Loss: 0.00112682
Iteration 17/25 | Loss: 0.00111965
Iteration 18/25 | Loss: 0.00109262
Iteration 19/25 | Loss: 0.00107454
Iteration 20/25 | Loss: 0.00107176
Iteration 21/25 | Loss: 0.00106517
Iteration 22/25 | Loss: 0.00106183
Iteration 23/25 | Loss: 0.00106101
Iteration 24/25 | Loss: 0.00106015
Iteration 25/25 | Loss: 0.00105943

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.25845778
Iteration 2/25 | Loss: 0.00203231
Iteration 3/25 | Loss: 0.00199723
Iteration 4/25 | Loss: 0.00199722
Iteration 5/25 | Loss: 0.00199722
Iteration 6/25 | Loss: 0.00199722
Iteration 7/25 | Loss: 0.00199722
Iteration 8/25 | Loss: 0.00199722
Iteration 9/25 | Loss: 0.00199722
Iteration 10/25 | Loss: 0.00187329
Iteration 11/25 | Loss: 0.00187320
Iteration 12/25 | Loss: 0.00187320
Iteration 13/25 | Loss: 0.00187320
Iteration 14/25 | Loss: 0.00187320
Iteration 15/25 | Loss: 0.00187320
Iteration 16/25 | Loss: 0.00187320
Iteration 17/25 | Loss: 0.00187320
Iteration 18/25 | Loss: 0.00187320
Iteration 19/25 | Loss: 0.00187320
Iteration 20/25 | Loss: 0.00187320
Iteration 21/25 | Loss: 0.00187320
Iteration 22/25 | Loss: 0.00187320
Iteration 23/25 | Loss: 0.00187320
Iteration 24/25 | Loss: 0.00187320
Iteration 25/25 | Loss: 0.00187320

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00187320
Iteration 2/1000 | Loss: 0.00029400
Iteration 3/1000 | Loss: 0.00036672
Iteration 4/1000 | Loss: 0.00003867
Iteration 5/1000 | Loss: 0.00013117
Iteration 6/1000 | Loss: 0.00003144
Iteration 7/1000 | Loss: 0.00011607
Iteration 8/1000 | Loss: 0.00002857
Iteration 9/1000 | Loss: 0.00002779
Iteration 10/1000 | Loss: 0.00032351
Iteration 11/1000 | Loss: 0.00041070
Iteration 12/1000 | Loss: 0.00007312
Iteration 13/1000 | Loss: 0.00007042
Iteration 14/1000 | Loss: 0.00002499
Iteration 15/1000 | Loss: 0.00002263
Iteration 16/1000 | Loss: 0.00002157
Iteration 17/1000 | Loss: 0.00002101
Iteration 18/1000 | Loss: 0.00002056
Iteration 19/1000 | Loss: 0.00002026
Iteration 20/1000 | Loss: 0.00002002
Iteration 21/1000 | Loss: 0.00015653
Iteration 22/1000 | Loss: 0.00002004
Iteration 23/1000 | Loss: 0.00001987
Iteration 24/1000 | Loss: 0.00001982
Iteration 25/1000 | Loss: 0.00001978
Iteration 26/1000 | Loss: 0.00001978
Iteration 27/1000 | Loss: 0.00001976
Iteration 28/1000 | Loss: 0.00001975
Iteration 29/1000 | Loss: 0.00001975
Iteration 30/1000 | Loss: 0.00001975
Iteration 31/1000 | Loss: 0.00001974
Iteration 32/1000 | Loss: 0.00001972
Iteration 33/1000 | Loss: 0.00001971
Iteration 34/1000 | Loss: 0.00001971
Iteration 35/1000 | Loss: 0.00001970
Iteration 36/1000 | Loss: 0.00001970
Iteration 37/1000 | Loss: 0.00001969
Iteration 38/1000 | Loss: 0.00001969
Iteration 39/1000 | Loss: 0.00001968
Iteration 40/1000 | Loss: 0.00001968
Iteration 41/1000 | Loss: 0.00001967
Iteration 42/1000 | Loss: 0.00001963
Iteration 43/1000 | Loss: 0.00001956
Iteration 44/1000 | Loss: 0.00001956
Iteration 45/1000 | Loss: 0.00001955
Iteration 46/1000 | Loss: 0.00001955
Iteration 47/1000 | Loss: 0.00001955
Iteration 48/1000 | Loss: 0.00001954
Iteration 49/1000 | Loss: 0.00001954
Iteration 50/1000 | Loss: 0.00001954
Iteration 51/1000 | Loss: 0.00001953
Iteration 52/1000 | Loss: 0.00001952
Iteration 53/1000 | Loss: 0.00001952
Iteration 54/1000 | Loss: 0.00001952
Iteration 55/1000 | Loss: 0.00001952
Iteration 56/1000 | Loss: 0.00001951
Iteration 57/1000 | Loss: 0.00001951
Iteration 58/1000 | Loss: 0.00001950
Iteration 59/1000 | Loss: 0.00001950
Iteration 60/1000 | Loss: 0.00001950
Iteration 61/1000 | Loss: 0.00001950
Iteration 62/1000 | Loss: 0.00001950
Iteration 63/1000 | Loss: 0.00001950
Iteration 64/1000 | Loss: 0.00001949
Iteration 65/1000 | Loss: 0.00001949
Iteration 66/1000 | Loss: 0.00001949
Iteration 67/1000 | Loss: 0.00001949
Iteration 68/1000 | Loss: 0.00001949
Iteration 69/1000 | Loss: 0.00001949
Iteration 70/1000 | Loss: 0.00001949
Iteration 71/1000 | Loss: 0.00001949
Iteration 72/1000 | Loss: 0.00001949
Iteration 73/1000 | Loss: 0.00001948
Iteration 74/1000 | Loss: 0.00001948
Iteration 75/1000 | Loss: 0.00001948
Iteration 76/1000 | Loss: 0.00001948
Iteration 77/1000 | Loss: 0.00001948
Iteration 78/1000 | Loss: 0.00001948
Iteration 79/1000 | Loss: 0.00001948
Iteration 80/1000 | Loss: 0.00001948
Iteration 81/1000 | Loss: 0.00001948
Iteration 82/1000 | Loss: 0.00001948
Iteration 83/1000 | Loss: 0.00001948
Iteration 84/1000 | Loss: 0.00001948
Iteration 85/1000 | Loss: 0.00001948
Iteration 86/1000 | Loss: 0.00001948
Iteration 87/1000 | Loss: 0.00001948
Iteration 88/1000 | Loss: 0.00001948
Iteration 89/1000 | Loss: 0.00001948
Iteration 90/1000 | Loss: 0.00001948
Iteration 91/1000 | Loss: 0.00001948
Iteration 92/1000 | Loss: 0.00001948
Iteration 93/1000 | Loss: 0.00001948
Iteration 94/1000 | Loss: 0.00001948
Iteration 95/1000 | Loss: 0.00001948
Iteration 96/1000 | Loss: 0.00001948
Iteration 97/1000 | Loss: 0.00001948
Iteration 98/1000 | Loss: 0.00001948
Iteration 99/1000 | Loss: 0.00001948
Iteration 100/1000 | Loss: 0.00001948
Iteration 101/1000 | Loss: 0.00001948
Iteration 102/1000 | Loss: 0.00001948
Iteration 103/1000 | Loss: 0.00001948
Iteration 104/1000 | Loss: 0.00001948
Iteration 105/1000 | Loss: 0.00001948
Iteration 106/1000 | Loss: 0.00001948
Iteration 107/1000 | Loss: 0.00001948
Iteration 108/1000 | Loss: 0.00001948
Iteration 109/1000 | Loss: 0.00001948
Iteration 110/1000 | Loss: 0.00001948
Iteration 111/1000 | Loss: 0.00001948
Iteration 112/1000 | Loss: 0.00001948
Iteration 113/1000 | Loss: 0.00001948
Iteration 114/1000 | Loss: 0.00001948
Iteration 115/1000 | Loss: 0.00001948
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 115. Stopping optimization.
Last 5 losses: [1.9475262888590805e-05, 1.9475262888590805e-05, 1.9475262888590805e-05, 1.9475262888590805e-05, 1.9475262888590805e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9475262888590805e-05

Optimization complete. Final v2v error: 3.4877994060516357 mm

Highest mean error: 14.475372314453125 mm for frame 36

Lowest mean error: 2.8547067642211914 mm for frame 108

Saving results

Total time: 88.69533848762512
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_31_nl_1440/0017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_31_nl_1440/0017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_31_nl_1440/0017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01049469
Iteration 2/25 | Loss: 0.00225900
Iteration 3/25 | Loss: 0.00155547
Iteration 4/25 | Loss: 0.00137370
Iteration 5/25 | Loss: 0.00132259
Iteration 6/25 | Loss: 0.00115498
Iteration 7/25 | Loss: 0.00111873
Iteration 8/25 | Loss: 0.00109432
Iteration 9/25 | Loss: 0.00108821
Iteration 10/25 | Loss: 0.00108218
Iteration 11/25 | Loss: 0.00106964
Iteration 12/25 | Loss: 0.00106756
Iteration 13/25 | Loss: 0.00106824
Iteration 14/25 | Loss: 0.00106874
Iteration 15/25 | Loss: 0.00106680
Iteration 16/25 | Loss: 0.00106554
Iteration 17/25 | Loss: 0.00106837
Iteration 18/25 | Loss: 0.00106862
Iteration 19/25 | Loss: 0.00106448
Iteration 20/25 | Loss: 0.00106638
Iteration 21/25 | Loss: 0.00106318
Iteration 22/25 | Loss: 0.00106472
Iteration 23/25 | Loss: 0.00106443
Iteration 24/25 | Loss: 0.00105990
Iteration 25/25 | Loss: 0.00106073

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36845899
Iteration 2/25 | Loss: 0.00194328
Iteration 3/25 | Loss: 0.00191074
Iteration 4/25 | Loss: 0.00191073
Iteration 5/25 | Loss: 0.00191073
Iteration 6/25 | Loss: 0.00191073
Iteration 7/25 | Loss: 0.00191073
Iteration 8/25 | Loss: 0.00191073
Iteration 9/25 | Loss: 0.00191073
Iteration 10/25 | Loss: 0.00191073
Iteration 11/25 | Loss: 0.00191073
Iteration 12/25 | Loss: 0.00191073
Iteration 13/25 | Loss: 0.00191073
Iteration 14/25 | Loss: 0.00191073
Iteration 15/25 | Loss: 0.00191073
Iteration 16/25 | Loss: 0.00191073
Iteration 17/25 | Loss: 0.00191073
Iteration 18/25 | Loss: 0.00191073
Iteration 19/25 | Loss: 0.00191073
Iteration 20/25 | Loss: 0.00191073
Iteration 21/25 | Loss: 0.00191073
Iteration 22/25 | Loss: 0.00191073
Iteration 23/25 | Loss: 0.00191073
Iteration 24/25 | Loss: 0.00191073
Iteration 25/25 | Loss: 0.00191073

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00191073
Iteration 2/1000 | Loss: 0.00028324
Iteration 3/1000 | Loss: 0.00014927
Iteration 4/1000 | Loss: 0.00018210
Iteration 5/1000 | Loss: 0.00018404
Iteration 6/1000 | Loss: 0.00016621
Iteration 7/1000 | Loss: 0.00020432
Iteration 8/1000 | Loss: 0.00011362
Iteration 9/1000 | Loss: 0.00010577
Iteration 10/1000 | Loss: 0.00022633
Iteration 11/1000 | Loss: 0.00024932
Iteration 12/1000 | Loss: 0.00028252
Iteration 13/1000 | Loss: 0.00016941
Iteration 14/1000 | Loss: 0.00024095
Iteration 15/1000 | Loss: 0.00012594
Iteration 16/1000 | Loss: 0.00006736
Iteration 17/1000 | Loss: 0.00046374
Iteration 18/1000 | Loss: 0.00034003
Iteration 19/1000 | Loss: 0.00008716
Iteration 20/1000 | Loss: 0.00007753
Iteration 21/1000 | Loss: 0.00006320
Iteration 22/1000 | Loss: 0.00004890
Iteration 23/1000 | Loss: 0.00006492
Iteration 24/1000 | Loss: 0.00017473
Iteration 25/1000 | Loss: 0.00054631
Iteration 26/1000 | Loss: 0.00006744
Iteration 27/1000 | Loss: 0.00006532
Iteration 28/1000 | Loss: 0.00006321
Iteration 29/1000 | Loss: 0.00014527
Iteration 30/1000 | Loss: 0.00005986
Iteration 31/1000 | Loss: 0.00005855
Iteration 32/1000 | Loss: 0.00005948
Iteration 33/1000 | Loss: 0.00006574
Iteration 34/1000 | Loss: 0.00007969
Iteration 35/1000 | Loss: 0.00006739
Iteration 36/1000 | Loss: 0.00007115
Iteration 37/1000 | Loss: 0.00020947
Iteration 38/1000 | Loss: 0.00007196
Iteration 39/1000 | Loss: 0.00005277
Iteration 40/1000 | Loss: 0.00006006
Iteration 41/1000 | Loss: 0.00021729
Iteration 42/1000 | Loss: 0.00005844
Iteration 43/1000 | Loss: 0.00004125
Iteration 44/1000 | Loss: 0.00004518
Iteration 45/1000 | Loss: 0.00021844
Iteration 46/1000 | Loss: 0.00005136
Iteration 47/1000 | Loss: 0.00005114
Iteration 48/1000 | Loss: 0.00003912
Iteration 49/1000 | Loss: 0.00004878
Iteration 50/1000 | Loss: 0.00004348
Iteration 51/1000 | Loss: 0.00003941
Iteration 52/1000 | Loss: 0.00003696
Iteration 53/1000 | Loss: 0.00002860
Iteration 54/1000 | Loss: 0.00003996
Iteration 55/1000 | Loss: 0.00014747
Iteration 56/1000 | Loss: 0.00007217
Iteration 57/1000 | Loss: 0.00004566
Iteration 58/1000 | Loss: 0.00004492
Iteration 59/1000 | Loss: 0.00003712
Iteration 60/1000 | Loss: 0.00004685
Iteration 61/1000 | Loss: 0.00004771
Iteration 62/1000 | Loss: 0.00004743
Iteration 63/1000 | Loss: 0.00004491
Iteration 64/1000 | Loss: 0.00003883
Iteration 65/1000 | Loss: 0.00004175
Iteration 66/1000 | Loss: 0.00004629
Iteration 67/1000 | Loss: 0.00004235
Iteration 68/1000 | Loss: 0.00004724
Iteration 69/1000 | Loss: 0.00004188
Iteration 70/1000 | Loss: 0.00003728
Iteration 71/1000 | Loss: 0.00004701
Iteration 72/1000 | Loss: 0.00004972
Iteration 73/1000 | Loss: 0.00004729
Iteration 74/1000 | Loss: 0.00004923
Iteration 75/1000 | Loss: 0.00004699
Iteration 76/1000 | Loss: 0.00004869
Iteration 77/1000 | Loss: 0.00003510
Iteration 78/1000 | Loss: 0.00003837
Iteration 79/1000 | Loss: 0.00005288
Iteration 80/1000 | Loss: 0.00004491
Iteration 81/1000 | Loss: 0.00004620
Iteration 82/1000 | Loss: 0.00005114
Iteration 83/1000 | Loss: 0.00004687
Iteration 84/1000 | Loss: 0.00019990
Iteration 85/1000 | Loss: 0.00036330
Iteration 86/1000 | Loss: 0.00005802
Iteration 87/1000 | Loss: 0.00017067
Iteration 88/1000 | Loss: 0.00015531
Iteration 89/1000 | Loss: 0.00003688
Iteration 90/1000 | Loss: 0.00003431
Iteration 91/1000 | Loss: 0.00013950
Iteration 92/1000 | Loss: 0.00012602
Iteration 93/1000 | Loss: 0.00002985
Iteration 94/1000 | Loss: 0.00014408
Iteration 95/1000 | Loss: 0.00002800
Iteration 96/1000 | Loss: 0.00002469
Iteration 97/1000 | Loss: 0.00002280
Iteration 98/1000 | Loss: 0.00013577
Iteration 99/1000 | Loss: 0.00011245
Iteration 100/1000 | Loss: 0.00037961
Iteration 101/1000 | Loss: 0.00010211
Iteration 102/1000 | Loss: 0.00014840
Iteration 103/1000 | Loss: 0.00025749
Iteration 104/1000 | Loss: 0.00008374
Iteration 105/1000 | Loss: 0.00012068
Iteration 106/1000 | Loss: 0.00003299
Iteration 107/1000 | Loss: 0.00023768
Iteration 108/1000 | Loss: 0.00015324
Iteration 109/1000 | Loss: 0.00002117
Iteration 110/1000 | Loss: 0.00002055
Iteration 111/1000 | Loss: 0.00014982
Iteration 112/1000 | Loss: 0.00009172
Iteration 113/1000 | Loss: 0.00009545
Iteration 114/1000 | Loss: 0.00002847
Iteration 115/1000 | Loss: 0.00002237
Iteration 116/1000 | Loss: 0.00002108
Iteration 117/1000 | Loss: 0.00002043
Iteration 118/1000 | Loss: 0.00003949
Iteration 119/1000 | Loss: 0.00002123
Iteration 120/1000 | Loss: 0.00038994
Iteration 121/1000 | Loss: 0.00016864
Iteration 122/1000 | Loss: 0.00003990
Iteration 123/1000 | Loss: 0.00010486
Iteration 124/1000 | Loss: 0.00003754
Iteration 125/1000 | Loss: 0.00003239
Iteration 126/1000 | Loss: 0.00002230
Iteration 127/1000 | Loss: 0.00012174
Iteration 128/1000 | Loss: 0.00008397
Iteration 129/1000 | Loss: 0.00011382
Iteration 130/1000 | Loss: 0.00015308
Iteration 131/1000 | Loss: 0.00012413
Iteration 132/1000 | Loss: 0.00007597
Iteration 133/1000 | Loss: 0.00002951
Iteration 134/1000 | Loss: 0.00002476
Iteration 135/1000 | Loss: 0.00004188
Iteration 136/1000 | Loss: 0.00002137
Iteration 137/1000 | Loss: 0.00002471
Iteration 138/1000 | Loss: 0.00001618
Iteration 139/1000 | Loss: 0.00013811
Iteration 140/1000 | Loss: 0.00036047
Iteration 141/1000 | Loss: 0.00016405
Iteration 142/1000 | Loss: 0.00010907
Iteration 143/1000 | Loss: 0.00011173
Iteration 144/1000 | Loss: 0.00010644
Iteration 145/1000 | Loss: 0.00003925
Iteration 146/1000 | Loss: 0.00001703
Iteration 147/1000 | Loss: 0.00001540
Iteration 148/1000 | Loss: 0.00020039
Iteration 149/1000 | Loss: 0.00010147
Iteration 150/1000 | Loss: 0.00019102
Iteration 151/1000 | Loss: 0.00003473
Iteration 152/1000 | Loss: 0.00002252
Iteration 153/1000 | Loss: 0.00002447
Iteration 154/1000 | Loss: 0.00001698
Iteration 155/1000 | Loss: 0.00001656
Iteration 156/1000 | Loss: 0.00002672
Iteration 157/1000 | Loss: 0.00002444
Iteration 158/1000 | Loss: 0.00002788
Iteration 159/1000 | Loss: 0.00001338
Iteration 160/1000 | Loss: 0.00001229
Iteration 161/1000 | Loss: 0.00001165
Iteration 162/1000 | Loss: 0.00001130
Iteration 163/1000 | Loss: 0.00001108
Iteration 164/1000 | Loss: 0.00001418
Iteration 165/1000 | Loss: 0.00001103
Iteration 166/1000 | Loss: 0.00001086
Iteration 167/1000 | Loss: 0.00001086
Iteration 168/1000 | Loss: 0.00001085
Iteration 169/1000 | Loss: 0.00001085
Iteration 170/1000 | Loss: 0.00001085
Iteration 171/1000 | Loss: 0.00001085
Iteration 172/1000 | Loss: 0.00001085
Iteration 173/1000 | Loss: 0.00001085
Iteration 174/1000 | Loss: 0.00001085
Iteration 175/1000 | Loss: 0.00001085
Iteration 176/1000 | Loss: 0.00001085
Iteration 177/1000 | Loss: 0.00001085
Iteration 178/1000 | Loss: 0.00001084
Iteration 179/1000 | Loss: 0.00001084
Iteration 180/1000 | Loss: 0.00001084
Iteration 181/1000 | Loss: 0.00001084
Iteration 182/1000 | Loss: 0.00001083
Iteration 183/1000 | Loss: 0.00001083
Iteration 184/1000 | Loss: 0.00001083
Iteration 185/1000 | Loss: 0.00001083
Iteration 186/1000 | Loss: 0.00001083
Iteration 187/1000 | Loss: 0.00001083
Iteration 188/1000 | Loss: 0.00001083
Iteration 189/1000 | Loss: 0.00001083
Iteration 190/1000 | Loss: 0.00001083
Iteration 191/1000 | Loss: 0.00001083
Iteration 192/1000 | Loss: 0.00001083
Iteration 193/1000 | Loss: 0.00001082
Iteration 194/1000 | Loss: 0.00001082
Iteration 195/1000 | Loss: 0.00001082
Iteration 196/1000 | Loss: 0.00001082
Iteration 197/1000 | Loss: 0.00001082
Iteration 198/1000 | Loss: 0.00001082
Iteration 199/1000 | Loss: 0.00001082
Iteration 200/1000 | Loss: 0.00001082
Iteration 201/1000 | Loss: 0.00001082
Iteration 202/1000 | Loss: 0.00001082
Iteration 203/1000 | Loss: 0.00001082
Iteration 204/1000 | Loss: 0.00001082
Iteration 205/1000 | Loss: 0.00001081
Iteration 206/1000 | Loss: 0.00001081
Iteration 207/1000 | Loss: 0.00001081
Iteration 208/1000 | Loss: 0.00001081
Iteration 209/1000 | Loss: 0.00001081
Iteration 210/1000 | Loss: 0.00001081
Iteration 211/1000 | Loss: 0.00001081
Iteration 212/1000 | Loss: 0.00001081
Iteration 213/1000 | Loss: 0.00001081
Iteration 214/1000 | Loss: 0.00001081
Iteration 215/1000 | Loss: 0.00001081
Iteration 216/1000 | Loss: 0.00001081
Iteration 217/1000 | Loss: 0.00001081
Iteration 218/1000 | Loss: 0.00001081
Iteration 219/1000 | Loss: 0.00001081
Iteration 220/1000 | Loss: 0.00001081
Iteration 221/1000 | Loss: 0.00001081
Iteration 222/1000 | Loss: 0.00001081
Iteration 223/1000 | Loss: 0.00001080
Iteration 224/1000 | Loss: 0.00001080
Iteration 225/1000 | Loss: 0.00001080
Iteration 226/1000 | Loss: 0.00001080
Iteration 227/1000 | Loss: 0.00001080
Iteration 228/1000 | Loss: 0.00001080
Iteration 229/1000 | Loss: 0.00001080
Iteration 230/1000 | Loss: 0.00001080
Iteration 231/1000 | Loss: 0.00001080
Iteration 232/1000 | Loss: 0.00001080
Iteration 233/1000 | Loss: 0.00001080
Iteration 234/1000 | Loss: 0.00001080
Iteration 235/1000 | Loss: 0.00001079
Iteration 236/1000 | Loss: 0.00001079
Iteration 237/1000 | Loss: 0.00001078
Iteration 238/1000 | Loss: 0.00001078
Iteration 239/1000 | Loss: 0.00001078
Iteration 240/1000 | Loss: 0.00001077
Iteration 241/1000 | Loss: 0.00001077
Iteration 242/1000 | Loss: 0.00001077
Iteration 243/1000 | Loss: 0.00001077
Iteration 244/1000 | Loss: 0.00001076
Iteration 245/1000 | Loss: 0.00001076
Iteration 246/1000 | Loss: 0.00001076
Iteration 247/1000 | Loss: 0.00001076
Iteration 248/1000 | Loss: 0.00001076
Iteration 249/1000 | Loss: 0.00001076
Iteration 250/1000 | Loss: 0.00001076
Iteration 251/1000 | Loss: 0.00001075
Iteration 252/1000 | Loss: 0.00001075
Iteration 253/1000 | Loss: 0.00001120
Iteration 254/1000 | Loss: 0.00001074
Iteration 255/1000 | Loss: 0.00001074
Iteration 256/1000 | Loss: 0.00001074
Iteration 257/1000 | Loss: 0.00001074
Iteration 258/1000 | Loss: 0.00001074
Iteration 259/1000 | Loss: 0.00001074
Iteration 260/1000 | Loss: 0.00001074
Iteration 261/1000 | Loss: 0.00001074
Iteration 262/1000 | Loss: 0.00001074
Iteration 263/1000 | Loss: 0.00001074
Iteration 264/1000 | Loss: 0.00001074
Iteration 265/1000 | Loss: 0.00001074
Iteration 266/1000 | Loss: 0.00001074
Iteration 267/1000 | Loss: 0.00001074
Iteration 268/1000 | Loss: 0.00001074
Iteration 269/1000 | Loss: 0.00001074
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 269. Stopping optimization.
Last 5 losses: [1.0736580406955909e-05, 1.0736580406955909e-05, 1.0736580406955909e-05, 1.0736580406955909e-05, 1.0736580406955909e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0736580406955909e-05

Optimization complete. Final v2v error: 2.746368408203125 mm

Highest mean error: 8.88456916809082 mm for frame 13

Lowest mean error: 2.1934943199157715 mm for frame 18

Saving results

Total time: 330.0359239578247
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_31_nl_1440/0020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_31_nl_1440/0020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_31_nl_1440/0020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00651639
Iteration 2/25 | Loss: 0.00131385
Iteration 3/25 | Loss: 0.00115780
Iteration 4/25 | Loss: 0.00113747
Iteration 5/25 | Loss: 0.00110915
Iteration 6/25 | Loss: 0.00111213
Iteration 7/25 | Loss: 0.00110225
Iteration 8/25 | Loss: 0.00110965
Iteration 9/25 | Loss: 0.00110125
Iteration 10/25 | Loss: 0.00110111
Iteration 11/25 | Loss: 0.00110109
Iteration 12/25 | Loss: 0.00110105
Iteration 13/25 | Loss: 0.00110105
Iteration 14/25 | Loss: 0.00110105
Iteration 15/25 | Loss: 0.00110105
Iteration 16/25 | Loss: 0.00110105
Iteration 17/25 | Loss: 0.00110105
Iteration 18/25 | Loss: 0.00110105
Iteration 19/25 | Loss: 0.00110104
Iteration 20/25 | Loss: 0.00110104
Iteration 21/25 | Loss: 0.00110104
Iteration 22/25 | Loss: 0.00110104
Iteration 23/25 | Loss: 0.00110104
Iteration 24/25 | Loss: 0.00110104
Iteration 25/25 | Loss: 0.00110104

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.55169177
Iteration 2/25 | Loss: 0.00275594
Iteration 3/25 | Loss: 0.00275594
Iteration 4/25 | Loss: 0.00275594
Iteration 5/25 | Loss: 0.00275593
Iteration 6/25 | Loss: 0.00275593
Iteration 7/25 | Loss: 0.00275593
Iteration 8/25 | Loss: 0.00275593
Iteration 9/25 | Loss: 0.00275593
Iteration 10/25 | Loss: 0.00275593
Iteration 11/25 | Loss: 0.00275593
Iteration 12/25 | Loss: 0.00275593
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0027559329755604267, 0.0027559329755604267, 0.0027559329755604267, 0.0027559329755604267, 0.0027559329755604267]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0027559329755604267

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00275593
Iteration 2/1000 | Loss: 0.00008400
Iteration 3/1000 | Loss: 0.00004753
Iteration 4/1000 | Loss: 0.00003353
Iteration 5/1000 | Loss: 0.00002891
Iteration 6/1000 | Loss: 0.00002669
Iteration 7/1000 | Loss: 0.00002508
Iteration 8/1000 | Loss: 0.00002414
Iteration 9/1000 | Loss: 0.00002354
Iteration 10/1000 | Loss: 0.00002307
Iteration 11/1000 | Loss: 0.00057417
Iteration 12/1000 | Loss: 0.00002790
Iteration 13/1000 | Loss: 0.00002334
Iteration 14/1000 | Loss: 0.00002188
Iteration 15/1000 | Loss: 0.00002098
Iteration 16/1000 | Loss: 0.00002029
Iteration 17/1000 | Loss: 0.00001971
Iteration 18/1000 | Loss: 0.00001934
Iteration 19/1000 | Loss: 0.00001909
Iteration 20/1000 | Loss: 0.00001892
Iteration 21/1000 | Loss: 0.00001892
Iteration 22/1000 | Loss: 0.00001891
Iteration 23/1000 | Loss: 0.00001889
Iteration 24/1000 | Loss: 0.00001887
Iteration 25/1000 | Loss: 0.00001887
Iteration 26/1000 | Loss: 0.00001886
Iteration 27/1000 | Loss: 0.00001885
Iteration 28/1000 | Loss: 0.00001885
Iteration 29/1000 | Loss: 0.00001885
Iteration 30/1000 | Loss: 0.00001884
Iteration 31/1000 | Loss: 0.00001884
Iteration 32/1000 | Loss: 0.00001884
Iteration 33/1000 | Loss: 0.00001883
Iteration 34/1000 | Loss: 0.00001883
Iteration 35/1000 | Loss: 0.00001883
Iteration 36/1000 | Loss: 0.00001882
Iteration 37/1000 | Loss: 0.00001882
Iteration 38/1000 | Loss: 0.00001881
Iteration 39/1000 | Loss: 0.00001881
Iteration 40/1000 | Loss: 0.00001881
Iteration 41/1000 | Loss: 0.00001880
Iteration 42/1000 | Loss: 0.00001880
Iteration 43/1000 | Loss: 0.00001880
Iteration 44/1000 | Loss: 0.00001879
Iteration 45/1000 | Loss: 0.00001879
Iteration 46/1000 | Loss: 0.00001879
Iteration 47/1000 | Loss: 0.00001878
Iteration 48/1000 | Loss: 0.00001878
Iteration 49/1000 | Loss: 0.00001877
Iteration 50/1000 | Loss: 0.00001877
Iteration 51/1000 | Loss: 0.00001877
Iteration 52/1000 | Loss: 0.00001876
Iteration 53/1000 | Loss: 0.00001876
Iteration 54/1000 | Loss: 0.00001876
Iteration 55/1000 | Loss: 0.00001875
Iteration 56/1000 | Loss: 0.00001875
Iteration 57/1000 | Loss: 0.00001875
Iteration 58/1000 | Loss: 0.00001875
Iteration 59/1000 | Loss: 0.00001874
Iteration 60/1000 | Loss: 0.00001874
Iteration 61/1000 | Loss: 0.00001874
Iteration 62/1000 | Loss: 0.00001873
Iteration 63/1000 | Loss: 0.00001873
Iteration 64/1000 | Loss: 0.00001873
Iteration 65/1000 | Loss: 0.00001873
Iteration 66/1000 | Loss: 0.00001872
Iteration 67/1000 | Loss: 0.00001872
Iteration 68/1000 | Loss: 0.00001872
Iteration 69/1000 | Loss: 0.00001872
Iteration 70/1000 | Loss: 0.00001871
Iteration 71/1000 | Loss: 0.00001871
Iteration 72/1000 | Loss: 0.00001871
Iteration 73/1000 | Loss: 0.00001871
Iteration 74/1000 | Loss: 0.00001870
Iteration 75/1000 | Loss: 0.00001870
Iteration 76/1000 | Loss: 0.00001870
Iteration 77/1000 | Loss: 0.00001869
Iteration 78/1000 | Loss: 0.00001869
Iteration 79/1000 | Loss: 0.00001869
Iteration 80/1000 | Loss: 0.00001869
Iteration 81/1000 | Loss: 0.00001869
Iteration 82/1000 | Loss: 0.00001869
Iteration 83/1000 | Loss: 0.00001869
Iteration 84/1000 | Loss: 0.00001868
Iteration 85/1000 | Loss: 0.00001868
Iteration 86/1000 | Loss: 0.00001868
Iteration 87/1000 | Loss: 0.00001868
Iteration 88/1000 | Loss: 0.00001868
Iteration 89/1000 | Loss: 0.00001868
Iteration 90/1000 | Loss: 0.00001867
Iteration 91/1000 | Loss: 0.00001867
Iteration 92/1000 | Loss: 0.00001867
Iteration 93/1000 | Loss: 0.00001867
Iteration 94/1000 | Loss: 0.00001866
Iteration 95/1000 | Loss: 0.00001866
Iteration 96/1000 | Loss: 0.00001866
Iteration 97/1000 | Loss: 0.00001866
Iteration 98/1000 | Loss: 0.00001866
Iteration 99/1000 | Loss: 0.00001866
Iteration 100/1000 | Loss: 0.00001866
Iteration 101/1000 | Loss: 0.00001866
Iteration 102/1000 | Loss: 0.00001866
Iteration 103/1000 | Loss: 0.00001866
Iteration 104/1000 | Loss: 0.00001866
Iteration 105/1000 | Loss: 0.00001866
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 105. Stopping optimization.
Last 5 losses: [1.866409002104774e-05, 1.866409002104774e-05, 1.866409002104774e-05, 1.866409002104774e-05, 1.866409002104774e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.866409002104774e-05

Optimization complete. Final v2v error: 3.4533495903015137 mm

Highest mean error: 5.118563652038574 mm for frame 61

Lowest mean error: 2.6897265911102295 mm for frame 1

Saving results

Total time: 54.85087966918945
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_31_nl_1440/0015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_31_nl_1440/0015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_31_nl_1440/0015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00473855
Iteration 2/25 | Loss: 0.00113707
Iteration 3/25 | Loss: 0.00102023
Iteration 4/25 | Loss: 0.00101529
Iteration 5/25 | Loss: 0.00101483
Iteration 6/25 | Loss: 0.00101483
Iteration 7/25 | Loss: 0.00101483
Iteration 8/25 | Loss: 0.00101483
Iteration 9/25 | Loss: 0.00101483
Iteration 10/25 | Loss: 0.00101483
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0010148296132683754, 0.0010148296132683754, 0.0010148296132683754, 0.0010148296132683754, 0.0010148296132683754]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010148296132683754

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.22856641
Iteration 2/25 | Loss: 0.00169112
Iteration 3/25 | Loss: 0.00169112
Iteration 4/25 | Loss: 0.00169112
Iteration 5/25 | Loss: 0.00169112
Iteration 6/25 | Loss: 0.00169112
Iteration 7/25 | Loss: 0.00169112
Iteration 8/25 | Loss: 0.00169112
Iteration 9/25 | Loss: 0.00169112
Iteration 10/25 | Loss: 0.00169112
Iteration 11/25 | Loss: 0.00169112
Iteration 12/25 | Loss: 0.00169112
Iteration 13/25 | Loss: 0.00169112
Iteration 14/25 | Loss: 0.00169112
Iteration 15/25 | Loss: 0.00169112
Iteration 16/25 | Loss: 0.00169112
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0016911200946196914, 0.0016911200946196914, 0.0016911200946196914, 0.0016911200946196914, 0.0016911200946196914]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0016911200946196914

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00169112
Iteration 2/1000 | Loss: 0.00002747
Iteration 3/1000 | Loss: 0.00001745
Iteration 4/1000 | Loss: 0.00001577
Iteration 5/1000 | Loss: 0.00001485
Iteration 6/1000 | Loss: 0.00001434
Iteration 7/1000 | Loss: 0.00001393
Iteration 8/1000 | Loss: 0.00001370
Iteration 9/1000 | Loss: 0.00001350
Iteration 10/1000 | Loss: 0.00001340
Iteration 11/1000 | Loss: 0.00001329
Iteration 12/1000 | Loss: 0.00001325
Iteration 13/1000 | Loss: 0.00001324
Iteration 14/1000 | Loss: 0.00001324
Iteration 15/1000 | Loss: 0.00001323
Iteration 16/1000 | Loss: 0.00001320
Iteration 17/1000 | Loss: 0.00001319
Iteration 18/1000 | Loss: 0.00001318
Iteration 19/1000 | Loss: 0.00001314
Iteration 20/1000 | Loss: 0.00001313
Iteration 21/1000 | Loss: 0.00001313
Iteration 22/1000 | Loss: 0.00001313
Iteration 23/1000 | Loss: 0.00001313
Iteration 24/1000 | Loss: 0.00001313
Iteration 25/1000 | Loss: 0.00001313
Iteration 26/1000 | Loss: 0.00001313
Iteration 27/1000 | Loss: 0.00001313
Iteration 28/1000 | Loss: 0.00001312
Iteration 29/1000 | Loss: 0.00001312
Iteration 30/1000 | Loss: 0.00001312
Iteration 31/1000 | Loss: 0.00001311
Iteration 32/1000 | Loss: 0.00001311
Iteration 33/1000 | Loss: 0.00001308
Iteration 34/1000 | Loss: 0.00001308
Iteration 35/1000 | Loss: 0.00001308
Iteration 36/1000 | Loss: 0.00001306
Iteration 37/1000 | Loss: 0.00001305
Iteration 38/1000 | Loss: 0.00001303
Iteration 39/1000 | Loss: 0.00001303
Iteration 40/1000 | Loss: 0.00001302
Iteration 41/1000 | Loss: 0.00001302
Iteration 42/1000 | Loss: 0.00001302
Iteration 43/1000 | Loss: 0.00001301
Iteration 44/1000 | Loss: 0.00001300
Iteration 45/1000 | Loss: 0.00001298
Iteration 46/1000 | Loss: 0.00001298
Iteration 47/1000 | Loss: 0.00001298
Iteration 48/1000 | Loss: 0.00001297
Iteration 49/1000 | Loss: 0.00001297
Iteration 50/1000 | Loss: 0.00001297
Iteration 51/1000 | Loss: 0.00001297
Iteration 52/1000 | Loss: 0.00001297
Iteration 53/1000 | Loss: 0.00001297
Iteration 54/1000 | Loss: 0.00001297
Iteration 55/1000 | Loss: 0.00001297
Iteration 56/1000 | Loss: 0.00001297
Iteration 57/1000 | Loss: 0.00001297
Iteration 58/1000 | Loss: 0.00001297
Iteration 59/1000 | Loss: 0.00001297
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 59. Stopping optimization.
Last 5 losses: [1.2971614523848984e-05, 1.2971614523848984e-05, 1.2971614523848984e-05, 1.2971614523848984e-05, 1.2971614523848984e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2971614523848984e-05

Optimization complete. Final v2v error: 2.907978057861328 mm

Highest mean error: 3.121845006942749 mm for frame 60

Lowest mean error: 2.738919258117676 mm for frame 189

Saving results

Total time: 27.563751935958862
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_31_nl_1440/0016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_31_nl_1440/0016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_31_nl_1440/0016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00357368
Iteration 2/25 | Loss: 0.00118773
Iteration 3/25 | Loss: 0.00103673
Iteration 4/25 | Loss: 0.00102347
Iteration 5/25 | Loss: 0.00102065
Iteration 6/25 | Loss: 0.00102028
Iteration 7/25 | Loss: 0.00102028
Iteration 8/25 | Loss: 0.00102028
Iteration 9/25 | Loss: 0.00102028
Iteration 10/25 | Loss: 0.00102028
Iteration 11/25 | Loss: 0.00102028
Iteration 12/25 | Loss: 0.00102028
Iteration 13/25 | Loss: 0.00102028
Iteration 14/25 | Loss: 0.00102028
Iteration 15/25 | Loss: 0.00102028
Iteration 16/25 | Loss: 0.00102028
Iteration 17/25 | Loss: 0.00102028
Iteration 18/25 | Loss: 0.00102028
Iteration 19/25 | Loss: 0.00102028
Iteration 20/25 | Loss: 0.00102028
Iteration 21/25 | Loss: 0.00102028
Iteration 22/25 | Loss: 0.00102028
Iteration 23/25 | Loss: 0.00102028
Iteration 24/25 | Loss: 0.00102028
Iteration 25/25 | Loss: 0.00102028

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.24642384
Iteration 2/25 | Loss: 0.00211857
Iteration 3/25 | Loss: 0.00211857
Iteration 4/25 | Loss: 0.00211857
Iteration 5/25 | Loss: 0.00211857
Iteration 6/25 | Loss: 0.00211857
Iteration 7/25 | Loss: 0.00211857
Iteration 8/25 | Loss: 0.00211857
Iteration 9/25 | Loss: 0.00211857
Iteration 10/25 | Loss: 0.00211857
Iteration 11/25 | Loss: 0.00211857
Iteration 12/25 | Loss: 0.00211857
Iteration 13/25 | Loss: 0.00211857
Iteration 14/25 | Loss: 0.00211857
Iteration 15/25 | Loss: 0.00211857
Iteration 16/25 | Loss: 0.00211857
Iteration 17/25 | Loss: 0.00211857
Iteration 18/25 | Loss: 0.00211857
Iteration 19/25 | Loss: 0.00211857
Iteration 20/25 | Loss: 0.00211857
Iteration 21/25 | Loss: 0.00211857
Iteration 22/25 | Loss: 0.00211857
Iteration 23/25 | Loss: 0.00211857
Iteration 24/25 | Loss: 0.00211857
Iteration 25/25 | Loss: 0.00211857

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00211857
Iteration 2/1000 | Loss: 0.00004695
Iteration 3/1000 | Loss: 0.00002547
Iteration 4/1000 | Loss: 0.00001671
Iteration 5/1000 | Loss: 0.00001545
Iteration 6/1000 | Loss: 0.00001448
Iteration 7/1000 | Loss: 0.00001387
Iteration 8/1000 | Loss: 0.00001342
Iteration 9/1000 | Loss: 0.00001308
Iteration 10/1000 | Loss: 0.00001280
Iteration 11/1000 | Loss: 0.00001257
Iteration 12/1000 | Loss: 0.00001240
Iteration 13/1000 | Loss: 0.00001223
Iteration 14/1000 | Loss: 0.00001221
Iteration 15/1000 | Loss: 0.00001217
Iteration 16/1000 | Loss: 0.00001216
Iteration 17/1000 | Loss: 0.00001215
Iteration 18/1000 | Loss: 0.00001215
Iteration 19/1000 | Loss: 0.00001214
Iteration 20/1000 | Loss: 0.00001213
Iteration 21/1000 | Loss: 0.00001210
Iteration 22/1000 | Loss: 0.00001207
Iteration 23/1000 | Loss: 0.00001207
Iteration 24/1000 | Loss: 0.00001206
Iteration 25/1000 | Loss: 0.00001206
Iteration 26/1000 | Loss: 0.00001206
Iteration 27/1000 | Loss: 0.00001205
Iteration 28/1000 | Loss: 0.00001205
Iteration 29/1000 | Loss: 0.00001204
Iteration 30/1000 | Loss: 0.00001204
Iteration 31/1000 | Loss: 0.00001202
Iteration 32/1000 | Loss: 0.00001201
Iteration 33/1000 | Loss: 0.00001201
Iteration 34/1000 | Loss: 0.00001201
Iteration 35/1000 | Loss: 0.00001200
Iteration 36/1000 | Loss: 0.00001200
Iteration 37/1000 | Loss: 0.00001200
Iteration 38/1000 | Loss: 0.00001199
Iteration 39/1000 | Loss: 0.00001199
Iteration 40/1000 | Loss: 0.00001199
Iteration 41/1000 | Loss: 0.00001199
Iteration 42/1000 | Loss: 0.00001199
Iteration 43/1000 | Loss: 0.00001199
Iteration 44/1000 | Loss: 0.00001199
Iteration 45/1000 | Loss: 0.00001199
Iteration 46/1000 | Loss: 0.00001199
Iteration 47/1000 | Loss: 0.00001198
Iteration 48/1000 | Loss: 0.00001198
Iteration 49/1000 | Loss: 0.00001198
Iteration 50/1000 | Loss: 0.00001198
Iteration 51/1000 | Loss: 0.00001198
Iteration 52/1000 | Loss: 0.00001198
Iteration 53/1000 | Loss: 0.00001198
Iteration 54/1000 | Loss: 0.00001198
Iteration 55/1000 | Loss: 0.00001198
Iteration 56/1000 | Loss: 0.00001198
Iteration 57/1000 | Loss: 0.00001197
Iteration 58/1000 | Loss: 0.00001197
Iteration 59/1000 | Loss: 0.00001197
Iteration 60/1000 | Loss: 0.00001197
Iteration 61/1000 | Loss: 0.00001196
Iteration 62/1000 | Loss: 0.00001196
Iteration 63/1000 | Loss: 0.00001196
Iteration 64/1000 | Loss: 0.00001196
Iteration 65/1000 | Loss: 0.00001195
Iteration 66/1000 | Loss: 0.00001195
Iteration 67/1000 | Loss: 0.00001195
Iteration 68/1000 | Loss: 0.00001195
Iteration 69/1000 | Loss: 0.00001195
Iteration 70/1000 | Loss: 0.00001195
Iteration 71/1000 | Loss: 0.00001194
Iteration 72/1000 | Loss: 0.00001194
Iteration 73/1000 | Loss: 0.00001194
Iteration 74/1000 | Loss: 0.00001194
Iteration 75/1000 | Loss: 0.00001193
Iteration 76/1000 | Loss: 0.00001193
Iteration 77/1000 | Loss: 0.00001193
Iteration 78/1000 | Loss: 0.00001193
Iteration 79/1000 | Loss: 0.00001193
Iteration 80/1000 | Loss: 0.00001192
Iteration 81/1000 | Loss: 0.00001192
Iteration 82/1000 | Loss: 0.00001192
Iteration 83/1000 | Loss: 0.00001191
Iteration 84/1000 | Loss: 0.00001191
Iteration 85/1000 | Loss: 0.00001191
Iteration 86/1000 | Loss: 0.00001191
Iteration 87/1000 | Loss: 0.00001191
Iteration 88/1000 | Loss: 0.00001190
Iteration 89/1000 | Loss: 0.00001190
Iteration 90/1000 | Loss: 0.00001189
Iteration 91/1000 | Loss: 0.00001189
Iteration 92/1000 | Loss: 0.00001189
Iteration 93/1000 | Loss: 0.00001189
Iteration 94/1000 | Loss: 0.00001189
Iteration 95/1000 | Loss: 0.00001188
Iteration 96/1000 | Loss: 0.00001188
Iteration 97/1000 | Loss: 0.00001188
Iteration 98/1000 | Loss: 0.00001188
Iteration 99/1000 | Loss: 0.00001188
Iteration 100/1000 | Loss: 0.00001188
Iteration 101/1000 | Loss: 0.00001188
Iteration 102/1000 | Loss: 0.00001188
Iteration 103/1000 | Loss: 0.00001187
Iteration 104/1000 | Loss: 0.00001187
Iteration 105/1000 | Loss: 0.00001187
Iteration 106/1000 | Loss: 0.00001187
Iteration 107/1000 | Loss: 0.00001187
Iteration 108/1000 | Loss: 0.00001187
Iteration 109/1000 | Loss: 0.00001187
Iteration 110/1000 | Loss: 0.00001187
Iteration 111/1000 | Loss: 0.00001187
Iteration 112/1000 | Loss: 0.00001187
Iteration 113/1000 | Loss: 0.00001187
Iteration 114/1000 | Loss: 0.00001187
Iteration 115/1000 | Loss: 0.00001187
Iteration 116/1000 | Loss: 0.00001187
Iteration 117/1000 | Loss: 0.00001187
Iteration 118/1000 | Loss: 0.00001187
Iteration 119/1000 | Loss: 0.00001187
Iteration 120/1000 | Loss: 0.00001186
Iteration 121/1000 | Loss: 0.00001186
Iteration 122/1000 | Loss: 0.00001186
Iteration 123/1000 | Loss: 0.00001186
Iteration 124/1000 | Loss: 0.00001186
Iteration 125/1000 | Loss: 0.00001186
Iteration 126/1000 | Loss: 0.00001186
Iteration 127/1000 | Loss: 0.00001186
Iteration 128/1000 | Loss: 0.00001186
Iteration 129/1000 | Loss: 0.00001185
Iteration 130/1000 | Loss: 0.00001185
Iteration 131/1000 | Loss: 0.00001185
Iteration 132/1000 | Loss: 0.00001185
Iteration 133/1000 | Loss: 0.00001185
Iteration 134/1000 | Loss: 0.00001185
Iteration 135/1000 | Loss: 0.00001185
Iteration 136/1000 | Loss: 0.00001185
Iteration 137/1000 | Loss: 0.00001185
Iteration 138/1000 | Loss: 0.00001185
Iteration 139/1000 | Loss: 0.00001184
Iteration 140/1000 | Loss: 0.00001184
Iteration 141/1000 | Loss: 0.00001184
Iteration 142/1000 | Loss: 0.00001184
Iteration 143/1000 | Loss: 0.00001184
Iteration 144/1000 | Loss: 0.00001184
Iteration 145/1000 | Loss: 0.00001184
Iteration 146/1000 | Loss: 0.00001184
Iteration 147/1000 | Loss: 0.00001184
Iteration 148/1000 | Loss: 0.00001184
Iteration 149/1000 | Loss: 0.00001183
Iteration 150/1000 | Loss: 0.00001183
Iteration 151/1000 | Loss: 0.00001183
Iteration 152/1000 | Loss: 0.00001183
Iteration 153/1000 | Loss: 0.00001183
Iteration 154/1000 | Loss: 0.00001183
Iteration 155/1000 | Loss: 0.00001183
Iteration 156/1000 | Loss: 0.00001183
Iteration 157/1000 | Loss: 0.00001183
Iteration 158/1000 | Loss: 0.00001183
Iteration 159/1000 | Loss: 0.00001183
Iteration 160/1000 | Loss: 0.00001183
Iteration 161/1000 | Loss: 0.00001183
Iteration 162/1000 | Loss: 0.00001183
Iteration 163/1000 | Loss: 0.00001183
Iteration 164/1000 | Loss: 0.00001183
Iteration 165/1000 | Loss: 0.00001183
Iteration 166/1000 | Loss: 0.00001183
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 166. Stopping optimization.
Last 5 losses: [1.1831341907964088e-05, 1.1831341907964088e-05, 1.1831341907964088e-05, 1.1831341907964088e-05, 1.1831341907964088e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1831341907964088e-05

Optimization complete. Final v2v error: 2.8958137035369873 mm

Highest mean error: 3.254540205001831 mm for frame 11

Lowest mean error: 2.702009677886963 mm for frame 173

Saving results

Total time: 39.06448006629944
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_31_nl_1440/0006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_31_nl_1440/0006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_31_nl_1440/0006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00735099
Iteration 2/25 | Loss: 0.00131578
Iteration 3/25 | Loss: 0.00112481
Iteration 4/25 | Loss: 0.00110394
Iteration 5/25 | Loss: 0.00109611
Iteration 6/25 | Loss: 0.00109444
Iteration 7/25 | Loss: 0.00109444
Iteration 8/25 | Loss: 0.00109444
Iteration 9/25 | Loss: 0.00109444
Iteration 10/25 | Loss: 0.00109444
Iteration 11/25 | Loss: 0.00109444
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010944448877125978, 0.0010944448877125978, 0.0010944448877125978, 0.0010944448877125978, 0.0010944448877125978]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010944448877125978

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.96493220
Iteration 2/25 | Loss: 0.00179763
Iteration 3/25 | Loss: 0.00179763
Iteration 4/25 | Loss: 0.00179763
Iteration 5/25 | Loss: 0.00179763
Iteration 6/25 | Loss: 0.00179763
Iteration 7/25 | Loss: 0.00179763
Iteration 8/25 | Loss: 0.00179763
Iteration 9/25 | Loss: 0.00179763
Iteration 10/25 | Loss: 0.00179763
Iteration 11/25 | Loss: 0.00179763
Iteration 12/25 | Loss: 0.00179763
Iteration 13/25 | Loss: 0.00179763
Iteration 14/25 | Loss: 0.00179763
Iteration 15/25 | Loss: 0.00179763
Iteration 16/25 | Loss: 0.00179763
Iteration 17/25 | Loss: 0.00179763
Iteration 18/25 | Loss: 0.00179763
Iteration 19/25 | Loss: 0.00179763
Iteration 20/25 | Loss: 0.00179763
Iteration 21/25 | Loss: 0.00179763
Iteration 22/25 | Loss: 0.00179763
Iteration 23/25 | Loss: 0.00179763
Iteration 24/25 | Loss: 0.00179763
Iteration 25/25 | Loss: 0.00179763

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00179763
Iteration 2/1000 | Loss: 0.00003310
Iteration 3/1000 | Loss: 0.00002318
Iteration 4/1000 | Loss: 0.00002102
Iteration 5/1000 | Loss: 0.00001987
Iteration 6/1000 | Loss: 0.00001924
Iteration 7/1000 | Loss: 0.00001884
Iteration 8/1000 | Loss: 0.00001845
Iteration 9/1000 | Loss: 0.00001817
Iteration 10/1000 | Loss: 0.00001798
Iteration 11/1000 | Loss: 0.00001789
Iteration 12/1000 | Loss: 0.00001788
Iteration 13/1000 | Loss: 0.00001769
Iteration 14/1000 | Loss: 0.00001768
Iteration 15/1000 | Loss: 0.00001767
Iteration 16/1000 | Loss: 0.00001766
Iteration 17/1000 | Loss: 0.00001760
Iteration 18/1000 | Loss: 0.00001756
Iteration 19/1000 | Loss: 0.00001755
Iteration 20/1000 | Loss: 0.00001754
Iteration 21/1000 | Loss: 0.00001754
Iteration 22/1000 | Loss: 0.00001754
Iteration 23/1000 | Loss: 0.00001754
Iteration 24/1000 | Loss: 0.00001754
Iteration 25/1000 | Loss: 0.00001754
Iteration 26/1000 | Loss: 0.00001753
Iteration 27/1000 | Loss: 0.00001751
Iteration 28/1000 | Loss: 0.00001750
Iteration 29/1000 | Loss: 0.00001750
Iteration 30/1000 | Loss: 0.00001750
Iteration 31/1000 | Loss: 0.00001749
Iteration 32/1000 | Loss: 0.00001749
Iteration 33/1000 | Loss: 0.00001749
Iteration 34/1000 | Loss: 0.00001746
Iteration 35/1000 | Loss: 0.00001746
Iteration 36/1000 | Loss: 0.00001746
Iteration 37/1000 | Loss: 0.00001746
Iteration 38/1000 | Loss: 0.00001745
Iteration 39/1000 | Loss: 0.00001745
Iteration 40/1000 | Loss: 0.00001744
Iteration 41/1000 | Loss: 0.00001744
Iteration 42/1000 | Loss: 0.00001744
Iteration 43/1000 | Loss: 0.00001744
Iteration 44/1000 | Loss: 0.00001743
Iteration 45/1000 | Loss: 0.00001743
Iteration 46/1000 | Loss: 0.00001743
Iteration 47/1000 | Loss: 0.00001743
Iteration 48/1000 | Loss: 0.00001743
Iteration 49/1000 | Loss: 0.00001742
Iteration 50/1000 | Loss: 0.00001742
Iteration 51/1000 | Loss: 0.00001742
Iteration 52/1000 | Loss: 0.00001742
Iteration 53/1000 | Loss: 0.00001742
Iteration 54/1000 | Loss: 0.00001742
Iteration 55/1000 | Loss: 0.00001741
Iteration 56/1000 | Loss: 0.00001741
Iteration 57/1000 | Loss: 0.00001741
Iteration 58/1000 | Loss: 0.00001741
Iteration 59/1000 | Loss: 0.00001741
Iteration 60/1000 | Loss: 0.00001740
Iteration 61/1000 | Loss: 0.00001740
Iteration 62/1000 | Loss: 0.00001740
Iteration 63/1000 | Loss: 0.00001739
Iteration 64/1000 | Loss: 0.00001739
Iteration 65/1000 | Loss: 0.00001739
Iteration 66/1000 | Loss: 0.00001739
Iteration 67/1000 | Loss: 0.00001739
Iteration 68/1000 | Loss: 0.00001739
Iteration 69/1000 | Loss: 0.00001739
Iteration 70/1000 | Loss: 0.00001739
Iteration 71/1000 | Loss: 0.00001739
Iteration 72/1000 | Loss: 0.00001739
Iteration 73/1000 | Loss: 0.00001739
Iteration 74/1000 | Loss: 0.00001739
Iteration 75/1000 | Loss: 0.00001739
Iteration 76/1000 | Loss: 0.00001739
Iteration 77/1000 | Loss: 0.00001739
Iteration 78/1000 | Loss: 0.00001739
Iteration 79/1000 | Loss: 0.00001739
Iteration 80/1000 | Loss: 0.00001739
Iteration 81/1000 | Loss: 0.00001739
Iteration 82/1000 | Loss: 0.00001739
Iteration 83/1000 | Loss: 0.00001739
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 83. Stopping optimization.
Last 5 losses: [1.738869832479395e-05, 1.738869832479395e-05, 1.738869832479395e-05, 1.738869832479395e-05, 1.738869832479395e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.738869832479395e-05

Optimization complete. Final v2v error: 3.5104663372039795 mm

Highest mean error: 4.167784214019775 mm for frame 180

Lowest mean error: 2.6351890563964844 mm for frame 13

Saving results

Total time: 34.78605341911316
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_31_nl_1440/0023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_31_nl_1440/0023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_31_nl_1440/0023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00458543
Iteration 2/25 | Loss: 0.00111273
Iteration 3/25 | Loss: 0.00101536
Iteration 4/25 | Loss: 0.00101004
Iteration 5/25 | Loss: 0.00100949
Iteration 6/25 | Loss: 0.00100949
Iteration 7/25 | Loss: 0.00100949
Iteration 8/25 | Loss: 0.00100949
Iteration 9/25 | Loss: 0.00100949
Iteration 10/25 | Loss: 0.00100949
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0010094894096255302, 0.0010094894096255302, 0.0010094894096255302, 0.0010094894096255302, 0.0010094894096255302]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010094894096255302

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.27617145
Iteration 2/25 | Loss: 0.00164827
Iteration 3/25 | Loss: 0.00164826
Iteration 4/25 | Loss: 0.00164826
Iteration 5/25 | Loss: 0.00164826
Iteration 6/25 | Loss: 0.00164826
Iteration 7/25 | Loss: 0.00164826
Iteration 8/25 | Loss: 0.00164826
Iteration 9/25 | Loss: 0.00164825
Iteration 10/25 | Loss: 0.00164825
Iteration 11/25 | Loss: 0.00164825
Iteration 12/25 | Loss: 0.00164825
Iteration 13/25 | Loss: 0.00164825
Iteration 14/25 | Loss: 0.00164825
Iteration 15/25 | Loss: 0.00164825
Iteration 16/25 | Loss: 0.00164825
Iteration 17/25 | Loss: 0.00164825
Iteration 18/25 | Loss: 0.00164825
Iteration 19/25 | Loss: 0.00164825
Iteration 20/25 | Loss: 0.00164825
Iteration 21/25 | Loss: 0.00164825
Iteration 22/25 | Loss: 0.00164825
Iteration 23/25 | Loss: 0.00164825
Iteration 24/25 | Loss: 0.00164825
Iteration 25/25 | Loss: 0.00164825

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00164825
Iteration 2/1000 | Loss: 0.00002986
Iteration 3/1000 | Loss: 0.00001445
Iteration 4/1000 | Loss: 0.00001267
Iteration 5/1000 | Loss: 0.00001167
Iteration 6/1000 | Loss: 0.00001127
Iteration 7/1000 | Loss: 0.00001078
Iteration 8/1000 | Loss: 0.00001041
Iteration 9/1000 | Loss: 0.00001029
Iteration 10/1000 | Loss: 0.00001028
Iteration 11/1000 | Loss: 0.00001023
Iteration 12/1000 | Loss: 0.00001021
Iteration 13/1000 | Loss: 0.00001018
Iteration 14/1000 | Loss: 0.00001016
Iteration 15/1000 | Loss: 0.00001014
Iteration 16/1000 | Loss: 0.00001013
Iteration 17/1000 | Loss: 0.00001007
Iteration 18/1000 | Loss: 0.00001006
Iteration 19/1000 | Loss: 0.00001006
Iteration 20/1000 | Loss: 0.00001005
Iteration 21/1000 | Loss: 0.00001005
Iteration 22/1000 | Loss: 0.00001005
Iteration 23/1000 | Loss: 0.00001005
Iteration 24/1000 | Loss: 0.00001005
Iteration 25/1000 | Loss: 0.00001004
Iteration 26/1000 | Loss: 0.00001003
Iteration 27/1000 | Loss: 0.00001003
Iteration 28/1000 | Loss: 0.00001002
Iteration 29/1000 | Loss: 0.00001002
Iteration 30/1000 | Loss: 0.00001002
Iteration 31/1000 | Loss: 0.00001002
Iteration 32/1000 | Loss: 0.00001000
Iteration 33/1000 | Loss: 0.00001000
Iteration 34/1000 | Loss: 0.00000999
Iteration 35/1000 | Loss: 0.00000999
Iteration 36/1000 | Loss: 0.00000998
Iteration 37/1000 | Loss: 0.00000998
Iteration 38/1000 | Loss: 0.00000998
Iteration 39/1000 | Loss: 0.00000998
Iteration 40/1000 | Loss: 0.00000997
Iteration 41/1000 | Loss: 0.00000996
Iteration 42/1000 | Loss: 0.00000995
Iteration 43/1000 | Loss: 0.00000994
Iteration 44/1000 | Loss: 0.00000994
Iteration 45/1000 | Loss: 0.00000994
Iteration 46/1000 | Loss: 0.00000993
Iteration 47/1000 | Loss: 0.00000993
Iteration 48/1000 | Loss: 0.00000993
Iteration 49/1000 | Loss: 0.00000993
Iteration 50/1000 | Loss: 0.00000993
Iteration 51/1000 | Loss: 0.00000993
Iteration 52/1000 | Loss: 0.00000992
Iteration 53/1000 | Loss: 0.00000991
Iteration 54/1000 | Loss: 0.00000991
Iteration 55/1000 | Loss: 0.00000990
Iteration 56/1000 | Loss: 0.00000990
Iteration 57/1000 | Loss: 0.00000989
Iteration 58/1000 | Loss: 0.00000989
Iteration 59/1000 | Loss: 0.00000989
Iteration 60/1000 | Loss: 0.00000988
Iteration 61/1000 | Loss: 0.00000988
Iteration 62/1000 | Loss: 0.00000987
Iteration 63/1000 | Loss: 0.00000987
Iteration 64/1000 | Loss: 0.00000986
Iteration 65/1000 | Loss: 0.00000986
Iteration 66/1000 | Loss: 0.00000986
Iteration 67/1000 | Loss: 0.00000985
Iteration 68/1000 | Loss: 0.00000985
Iteration 69/1000 | Loss: 0.00000985
Iteration 70/1000 | Loss: 0.00000985
Iteration 71/1000 | Loss: 0.00000985
Iteration 72/1000 | Loss: 0.00000985
Iteration 73/1000 | Loss: 0.00000985
Iteration 74/1000 | Loss: 0.00000984
Iteration 75/1000 | Loss: 0.00000984
Iteration 76/1000 | Loss: 0.00000984
Iteration 77/1000 | Loss: 0.00000984
Iteration 78/1000 | Loss: 0.00000984
Iteration 79/1000 | Loss: 0.00000983
Iteration 80/1000 | Loss: 0.00000983
Iteration 81/1000 | Loss: 0.00000983
Iteration 82/1000 | Loss: 0.00000983
Iteration 83/1000 | Loss: 0.00000983
Iteration 84/1000 | Loss: 0.00000983
Iteration 85/1000 | Loss: 0.00000982
Iteration 86/1000 | Loss: 0.00000982
Iteration 87/1000 | Loss: 0.00000982
Iteration 88/1000 | Loss: 0.00000982
Iteration 89/1000 | Loss: 0.00000982
Iteration 90/1000 | Loss: 0.00000982
Iteration 91/1000 | Loss: 0.00000982
Iteration 92/1000 | Loss: 0.00000981
Iteration 93/1000 | Loss: 0.00000981
Iteration 94/1000 | Loss: 0.00000981
Iteration 95/1000 | Loss: 0.00000981
Iteration 96/1000 | Loss: 0.00000981
Iteration 97/1000 | Loss: 0.00000981
Iteration 98/1000 | Loss: 0.00000981
Iteration 99/1000 | Loss: 0.00000981
Iteration 100/1000 | Loss: 0.00000981
Iteration 101/1000 | Loss: 0.00000981
Iteration 102/1000 | Loss: 0.00000981
Iteration 103/1000 | Loss: 0.00000981
Iteration 104/1000 | Loss: 0.00000981
Iteration 105/1000 | Loss: 0.00000981
Iteration 106/1000 | Loss: 0.00000981
Iteration 107/1000 | Loss: 0.00000980
Iteration 108/1000 | Loss: 0.00000980
Iteration 109/1000 | Loss: 0.00000980
Iteration 110/1000 | Loss: 0.00000980
Iteration 111/1000 | Loss: 0.00000980
Iteration 112/1000 | Loss: 0.00000980
Iteration 113/1000 | Loss: 0.00000980
Iteration 114/1000 | Loss: 0.00000980
Iteration 115/1000 | Loss: 0.00000980
Iteration 116/1000 | Loss: 0.00000980
Iteration 117/1000 | Loss: 0.00000980
Iteration 118/1000 | Loss: 0.00000980
Iteration 119/1000 | Loss: 0.00000980
Iteration 120/1000 | Loss: 0.00000980
Iteration 121/1000 | Loss: 0.00000980
Iteration 122/1000 | Loss: 0.00000980
Iteration 123/1000 | Loss: 0.00000980
Iteration 124/1000 | Loss: 0.00000980
Iteration 125/1000 | Loss: 0.00000980
Iteration 126/1000 | Loss: 0.00000979
Iteration 127/1000 | Loss: 0.00000979
Iteration 128/1000 | Loss: 0.00000979
Iteration 129/1000 | Loss: 0.00000979
Iteration 130/1000 | Loss: 0.00000979
Iteration 131/1000 | Loss: 0.00000979
Iteration 132/1000 | Loss: 0.00000979
Iteration 133/1000 | Loss: 0.00000979
Iteration 134/1000 | Loss: 0.00000979
Iteration 135/1000 | Loss: 0.00000979
Iteration 136/1000 | Loss: 0.00000979
Iteration 137/1000 | Loss: 0.00000979
Iteration 138/1000 | Loss: 0.00000979
Iteration 139/1000 | Loss: 0.00000979
Iteration 140/1000 | Loss: 0.00000979
Iteration 141/1000 | Loss: 0.00000979
Iteration 142/1000 | Loss: 0.00000979
Iteration 143/1000 | Loss: 0.00000978
Iteration 144/1000 | Loss: 0.00000978
Iteration 145/1000 | Loss: 0.00000978
Iteration 146/1000 | Loss: 0.00000978
Iteration 147/1000 | Loss: 0.00000978
Iteration 148/1000 | Loss: 0.00000978
Iteration 149/1000 | Loss: 0.00000978
Iteration 150/1000 | Loss: 0.00000978
Iteration 151/1000 | Loss: 0.00000978
Iteration 152/1000 | Loss: 0.00000978
Iteration 153/1000 | Loss: 0.00000978
Iteration 154/1000 | Loss: 0.00000978
Iteration 155/1000 | Loss: 0.00000978
Iteration 156/1000 | Loss: 0.00000978
Iteration 157/1000 | Loss: 0.00000978
Iteration 158/1000 | Loss: 0.00000978
Iteration 159/1000 | Loss: 0.00000978
Iteration 160/1000 | Loss: 0.00000978
Iteration 161/1000 | Loss: 0.00000978
Iteration 162/1000 | Loss: 0.00000978
Iteration 163/1000 | Loss: 0.00000978
Iteration 164/1000 | Loss: 0.00000978
Iteration 165/1000 | Loss: 0.00000978
Iteration 166/1000 | Loss: 0.00000978
Iteration 167/1000 | Loss: 0.00000978
Iteration 168/1000 | Loss: 0.00000978
Iteration 169/1000 | Loss: 0.00000978
Iteration 170/1000 | Loss: 0.00000978
Iteration 171/1000 | Loss: 0.00000978
Iteration 172/1000 | Loss: 0.00000978
Iteration 173/1000 | Loss: 0.00000978
Iteration 174/1000 | Loss: 0.00000978
Iteration 175/1000 | Loss: 0.00000978
Iteration 176/1000 | Loss: 0.00000978
Iteration 177/1000 | Loss: 0.00000978
Iteration 178/1000 | Loss: 0.00000978
Iteration 179/1000 | Loss: 0.00000978
Iteration 180/1000 | Loss: 0.00000978
Iteration 181/1000 | Loss: 0.00000978
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 181. Stopping optimization.
Last 5 losses: [9.777931154530961e-06, 9.777931154530961e-06, 9.777931154530961e-06, 9.777931154530961e-06, 9.777931154530961e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.777931154530961e-06

Optimization complete. Final v2v error: 2.54449200630188 mm

Highest mean error: 2.9413928985595703 mm for frame 63

Lowest mean error: 2.203122138977051 mm for frame 132

Saving results

Total time: 31.42959189414978
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_31_nl_1440/0024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_31_nl_1440/0024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_31_nl_1440/0024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00384681
Iteration 2/25 | Loss: 0.00112708
Iteration 3/25 | Loss: 0.00100305
Iteration 4/25 | Loss: 0.00099042
Iteration 5/25 | Loss: 0.00098690
Iteration 6/25 | Loss: 0.00098645
Iteration 7/25 | Loss: 0.00098645
Iteration 8/25 | Loss: 0.00098645
Iteration 9/25 | Loss: 0.00098645
Iteration 10/25 | Loss: 0.00098645
Iteration 11/25 | Loss: 0.00098645
Iteration 12/25 | Loss: 0.00098645
Iteration 13/25 | Loss: 0.00098645
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0009864518651738763, 0.0009864518651738763, 0.0009864518651738763, 0.0009864518651738763, 0.0009864518651738763]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009864518651738763

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36228704
Iteration 2/25 | Loss: 0.00165524
Iteration 3/25 | Loss: 0.00165524
Iteration 4/25 | Loss: 0.00165524
Iteration 5/25 | Loss: 0.00165524
Iteration 6/25 | Loss: 0.00165524
Iteration 7/25 | Loss: 0.00165524
Iteration 8/25 | Loss: 0.00165524
Iteration 9/25 | Loss: 0.00165524
Iteration 10/25 | Loss: 0.00165524
Iteration 11/25 | Loss: 0.00165523
Iteration 12/25 | Loss: 0.00165524
Iteration 13/25 | Loss: 0.00165523
Iteration 14/25 | Loss: 0.00165524
Iteration 15/25 | Loss: 0.00165523
Iteration 16/25 | Loss: 0.00165523
Iteration 17/25 | Loss: 0.00165523
Iteration 18/25 | Loss: 0.00165523
Iteration 19/25 | Loss: 0.00165523
Iteration 20/25 | Loss: 0.00165523
Iteration 21/25 | Loss: 0.00165523
Iteration 22/25 | Loss: 0.00165523
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.001655234838835895, 0.001655234838835895, 0.001655234838835895, 0.001655234838835895, 0.001655234838835895]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001655234838835895

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00165523
Iteration 2/1000 | Loss: 0.00002085
Iteration 3/1000 | Loss: 0.00001217
Iteration 4/1000 | Loss: 0.00001066
Iteration 5/1000 | Loss: 0.00000983
Iteration 6/1000 | Loss: 0.00000939
Iteration 7/1000 | Loss: 0.00000912
Iteration 8/1000 | Loss: 0.00000883
Iteration 9/1000 | Loss: 0.00000861
Iteration 10/1000 | Loss: 0.00000853
Iteration 11/1000 | Loss: 0.00000847
Iteration 12/1000 | Loss: 0.00000846
Iteration 13/1000 | Loss: 0.00000846
Iteration 14/1000 | Loss: 0.00000838
Iteration 15/1000 | Loss: 0.00000836
Iteration 16/1000 | Loss: 0.00000835
Iteration 17/1000 | Loss: 0.00000830
Iteration 18/1000 | Loss: 0.00000823
Iteration 19/1000 | Loss: 0.00000823
Iteration 20/1000 | Loss: 0.00000821
Iteration 21/1000 | Loss: 0.00000819
Iteration 22/1000 | Loss: 0.00000819
Iteration 23/1000 | Loss: 0.00000819
Iteration 24/1000 | Loss: 0.00000818
Iteration 25/1000 | Loss: 0.00000818
Iteration 26/1000 | Loss: 0.00000818
Iteration 27/1000 | Loss: 0.00000818
Iteration 28/1000 | Loss: 0.00000817
Iteration 29/1000 | Loss: 0.00000817
Iteration 30/1000 | Loss: 0.00000817
Iteration 31/1000 | Loss: 0.00000816
Iteration 32/1000 | Loss: 0.00000814
Iteration 33/1000 | Loss: 0.00000813
Iteration 34/1000 | Loss: 0.00000813
Iteration 35/1000 | Loss: 0.00000812
Iteration 36/1000 | Loss: 0.00000811
Iteration 37/1000 | Loss: 0.00000810
Iteration 38/1000 | Loss: 0.00000810
Iteration 39/1000 | Loss: 0.00000810
Iteration 40/1000 | Loss: 0.00000810
Iteration 41/1000 | Loss: 0.00000810
Iteration 42/1000 | Loss: 0.00000810
Iteration 43/1000 | Loss: 0.00000810
Iteration 44/1000 | Loss: 0.00000810
Iteration 45/1000 | Loss: 0.00000809
Iteration 46/1000 | Loss: 0.00000809
Iteration 47/1000 | Loss: 0.00000809
Iteration 48/1000 | Loss: 0.00000808
Iteration 49/1000 | Loss: 0.00000808
Iteration 50/1000 | Loss: 0.00000808
Iteration 51/1000 | Loss: 0.00000808
Iteration 52/1000 | Loss: 0.00000808
Iteration 53/1000 | Loss: 0.00000808
Iteration 54/1000 | Loss: 0.00000808
Iteration 55/1000 | Loss: 0.00000808
Iteration 56/1000 | Loss: 0.00000807
Iteration 57/1000 | Loss: 0.00000807
Iteration 58/1000 | Loss: 0.00000803
Iteration 59/1000 | Loss: 0.00000802
Iteration 60/1000 | Loss: 0.00000801
Iteration 61/1000 | Loss: 0.00000800
Iteration 62/1000 | Loss: 0.00000800
Iteration 63/1000 | Loss: 0.00000800
Iteration 64/1000 | Loss: 0.00000800
Iteration 65/1000 | Loss: 0.00000800
Iteration 66/1000 | Loss: 0.00000800
Iteration 67/1000 | Loss: 0.00000800
Iteration 68/1000 | Loss: 0.00000800
Iteration 69/1000 | Loss: 0.00000799
Iteration 70/1000 | Loss: 0.00000799
Iteration 71/1000 | Loss: 0.00000799
Iteration 72/1000 | Loss: 0.00000799
Iteration 73/1000 | Loss: 0.00000799
Iteration 74/1000 | Loss: 0.00000798
Iteration 75/1000 | Loss: 0.00000798
Iteration 76/1000 | Loss: 0.00000798
Iteration 77/1000 | Loss: 0.00000797
Iteration 78/1000 | Loss: 0.00000797
Iteration 79/1000 | Loss: 0.00000796
Iteration 80/1000 | Loss: 0.00000796
Iteration 81/1000 | Loss: 0.00000795
Iteration 82/1000 | Loss: 0.00000795
Iteration 83/1000 | Loss: 0.00000795
Iteration 84/1000 | Loss: 0.00000795
Iteration 85/1000 | Loss: 0.00000795
Iteration 86/1000 | Loss: 0.00000795
Iteration 87/1000 | Loss: 0.00000795
Iteration 88/1000 | Loss: 0.00000794
Iteration 89/1000 | Loss: 0.00000794
Iteration 90/1000 | Loss: 0.00000794
Iteration 91/1000 | Loss: 0.00000794
Iteration 92/1000 | Loss: 0.00000794
Iteration 93/1000 | Loss: 0.00000793
Iteration 94/1000 | Loss: 0.00000793
Iteration 95/1000 | Loss: 0.00000793
Iteration 96/1000 | Loss: 0.00000793
Iteration 97/1000 | Loss: 0.00000793
Iteration 98/1000 | Loss: 0.00000792
Iteration 99/1000 | Loss: 0.00000792
Iteration 100/1000 | Loss: 0.00000792
Iteration 101/1000 | Loss: 0.00000792
Iteration 102/1000 | Loss: 0.00000792
Iteration 103/1000 | Loss: 0.00000792
Iteration 104/1000 | Loss: 0.00000792
Iteration 105/1000 | Loss: 0.00000791
Iteration 106/1000 | Loss: 0.00000791
Iteration 107/1000 | Loss: 0.00000791
Iteration 108/1000 | Loss: 0.00000791
Iteration 109/1000 | Loss: 0.00000791
Iteration 110/1000 | Loss: 0.00000791
Iteration 111/1000 | Loss: 0.00000791
Iteration 112/1000 | Loss: 0.00000791
Iteration 113/1000 | Loss: 0.00000791
Iteration 114/1000 | Loss: 0.00000791
Iteration 115/1000 | Loss: 0.00000791
Iteration 116/1000 | Loss: 0.00000791
Iteration 117/1000 | Loss: 0.00000791
Iteration 118/1000 | Loss: 0.00000790
Iteration 119/1000 | Loss: 0.00000790
Iteration 120/1000 | Loss: 0.00000790
Iteration 121/1000 | Loss: 0.00000790
Iteration 122/1000 | Loss: 0.00000790
Iteration 123/1000 | Loss: 0.00000790
Iteration 124/1000 | Loss: 0.00000790
Iteration 125/1000 | Loss: 0.00000790
Iteration 126/1000 | Loss: 0.00000790
Iteration 127/1000 | Loss: 0.00000790
Iteration 128/1000 | Loss: 0.00000790
Iteration 129/1000 | Loss: 0.00000790
Iteration 130/1000 | Loss: 0.00000790
Iteration 131/1000 | Loss: 0.00000789
Iteration 132/1000 | Loss: 0.00000789
Iteration 133/1000 | Loss: 0.00000789
Iteration 134/1000 | Loss: 0.00000789
Iteration 135/1000 | Loss: 0.00000789
Iteration 136/1000 | Loss: 0.00000789
Iteration 137/1000 | Loss: 0.00000789
Iteration 138/1000 | Loss: 0.00000789
Iteration 139/1000 | Loss: 0.00000789
Iteration 140/1000 | Loss: 0.00000789
Iteration 141/1000 | Loss: 0.00000789
Iteration 142/1000 | Loss: 0.00000789
Iteration 143/1000 | Loss: 0.00000789
Iteration 144/1000 | Loss: 0.00000789
Iteration 145/1000 | Loss: 0.00000789
Iteration 146/1000 | Loss: 0.00000789
Iteration 147/1000 | Loss: 0.00000789
Iteration 148/1000 | Loss: 0.00000789
Iteration 149/1000 | Loss: 0.00000789
Iteration 150/1000 | Loss: 0.00000789
Iteration 151/1000 | Loss: 0.00000789
Iteration 152/1000 | Loss: 0.00000789
Iteration 153/1000 | Loss: 0.00000789
Iteration 154/1000 | Loss: 0.00000789
Iteration 155/1000 | Loss: 0.00000789
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 155. Stopping optimization.
Last 5 losses: [7.885136255936231e-06, 7.885136255936231e-06, 7.885136255936231e-06, 7.885136255936231e-06, 7.885136255936231e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 7.885136255936231e-06

Optimization complete. Final v2v error: 2.397454023361206 mm

Highest mean error: 2.8325908184051514 mm for frame 24

Lowest mean error: 2.1357054710388184 mm for frame 196

Saving results

Total time: 40.4677414894104
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_31_nl_1440/0012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_31_nl_1440/0012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_31_nl_1440/0012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00445955
Iteration 2/25 | Loss: 0.00108326
Iteration 3/25 | Loss: 0.00100581
Iteration 4/25 | Loss: 0.00099644
Iteration 5/25 | Loss: 0.00099409
Iteration 6/25 | Loss: 0.00099409
Iteration 7/25 | Loss: 0.00099409
Iteration 8/25 | Loss: 0.00099409
Iteration 9/25 | Loss: 0.00099409
Iteration 10/25 | Loss: 0.00099409
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0009940946474671364, 0.0009940946474671364, 0.0009940946474671364, 0.0009940946474671364, 0.0009940946474671364]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009940946474671364

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.24242544
Iteration 2/25 | Loss: 0.00174343
Iteration 3/25 | Loss: 0.00174342
Iteration 4/25 | Loss: 0.00174342
Iteration 5/25 | Loss: 0.00174342
Iteration 6/25 | Loss: 0.00174342
Iteration 7/25 | Loss: 0.00174342
Iteration 8/25 | Loss: 0.00174342
Iteration 9/25 | Loss: 0.00174342
Iteration 10/25 | Loss: 0.00174342
Iteration 11/25 | Loss: 0.00174342
Iteration 12/25 | Loss: 0.00174342
Iteration 13/25 | Loss: 0.00174342
Iteration 14/25 | Loss: 0.00174342
Iteration 15/25 | Loss: 0.00174342
Iteration 16/25 | Loss: 0.00174342
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.001743421540595591, 0.001743421540595591, 0.001743421540595591, 0.001743421540595591, 0.001743421540595591]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001743421540595591

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00174342
Iteration 2/1000 | Loss: 0.00003062
Iteration 3/1000 | Loss: 0.00001809
Iteration 4/1000 | Loss: 0.00001525
Iteration 5/1000 | Loss: 0.00001412
Iteration 6/1000 | Loss: 0.00001353
Iteration 7/1000 | Loss: 0.00001294
Iteration 8/1000 | Loss: 0.00001265
Iteration 9/1000 | Loss: 0.00001250
Iteration 10/1000 | Loss: 0.00001250
Iteration 11/1000 | Loss: 0.00001249
Iteration 12/1000 | Loss: 0.00001234
Iteration 13/1000 | Loss: 0.00001231
Iteration 14/1000 | Loss: 0.00001229
Iteration 15/1000 | Loss: 0.00001229
Iteration 16/1000 | Loss: 0.00001228
Iteration 17/1000 | Loss: 0.00001224
Iteration 18/1000 | Loss: 0.00001224
Iteration 19/1000 | Loss: 0.00001223
Iteration 20/1000 | Loss: 0.00001223
Iteration 21/1000 | Loss: 0.00001223
Iteration 22/1000 | Loss: 0.00001222
Iteration 23/1000 | Loss: 0.00001222
Iteration 24/1000 | Loss: 0.00001222
Iteration 25/1000 | Loss: 0.00001221
Iteration 26/1000 | Loss: 0.00001221
Iteration 27/1000 | Loss: 0.00001221
Iteration 28/1000 | Loss: 0.00001220
Iteration 29/1000 | Loss: 0.00001219
Iteration 30/1000 | Loss: 0.00001219
Iteration 31/1000 | Loss: 0.00001219
Iteration 32/1000 | Loss: 0.00001218
Iteration 33/1000 | Loss: 0.00001218
Iteration 34/1000 | Loss: 0.00001217
Iteration 35/1000 | Loss: 0.00001217
Iteration 36/1000 | Loss: 0.00001210
Iteration 37/1000 | Loss: 0.00001210
Iteration 38/1000 | Loss: 0.00001210
Iteration 39/1000 | Loss: 0.00001210
Iteration 40/1000 | Loss: 0.00001210
Iteration 41/1000 | Loss: 0.00001207
Iteration 42/1000 | Loss: 0.00001206
Iteration 43/1000 | Loss: 0.00001206
Iteration 44/1000 | Loss: 0.00001205
Iteration 45/1000 | Loss: 0.00001204
Iteration 46/1000 | Loss: 0.00001204
Iteration 47/1000 | Loss: 0.00001203
Iteration 48/1000 | Loss: 0.00001203
Iteration 49/1000 | Loss: 0.00001200
Iteration 50/1000 | Loss: 0.00001197
Iteration 51/1000 | Loss: 0.00001197
Iteration 52/1000 | Loss: 0.00001196
Iteration 53/1000 | Loss: 0.00001196
Iteration 54/1000 | Loss: 0.00001196
Iteration 55/1000 | Loss: 0.00001195
Iteration 56/1000 | Loss: 0.00001195
Iteration 57/1000 | Loss: 0.00001195
Iteration 58/1000 | Loss: 0.00001194
Iteration 59/1000 | Loss: 0.00001194
Iteration 60/1000 | Loss: 0.00001194
Iteration 61/1000 | Loss: 0.00001193
Iteration 62/1000 | Loss: 0.00001193
Iteration 63/1000 | Loss: 0.00001193
Iteration 64/1000 | Loss: 0.00001192
Iteration 65/1000 | Loss: 0.00001192
Iteration 66/1000 | Loss: 0.00001192
Iteration 67/1000 | Loss: 0.00001192
Iteration 68/1000 | Loss: 0.00001191
Iteration 69/1000 | Loss: 0.00001191
Iteration 70/1000 | Loss: 0.00001191
Iteration 71/1000 | Loss: 0.00001190
Iteration 72/1000 | Loss: 0.00001190
Iteration 73/1000 | Loss: 0.00001190
Iteration 74/1000 | Loss: 0.00001189
Iteration 75/1000 | Loss: 0.00001189
Iteration 76/1000 | Loss: 0.00001188
Iteration 77/1000 | Loss: 0.00001188
Iteration 78/1000 | Loss: 0.00001188
Iteration 79/1000 | Loss: 0.00001187
Iteration 80/1000 | Loss: 0.00001185
Iteration 81/1000 | Loss: 0.00001185
Iteration 82/1000 | Loss: 0.00001185
Iteration 83/1000 | Loss: 0.00001185
Iteration 84/1000 | Loss: 0.00001185
Iteration 85/1000 | Loss: 0.00001184
Iteration 86/1000 | Loss: 0.00001184
Iteration 87/1000 | Loss: 0.00001184
Iteration 88/1000 | Loss: 0.00001184
Iteration 89/1000 | Loss: 0.00001184
Iteration 90/1000 | Loss: 0.00001183
Iteration 91/1000 | Loss: 0.00001182
Iteration 92/1000 | Loss: 0.00001181
Iteration 93/1000 | Loss: 0.00001181
Iteration 94/1000 | Loss: 0.00001181
Iteration 95/1000 | Loss: 0.00001180
Iteration 96/1000 | Loss: 0.00001180
Iteration 97/1000 | Loss: 0.00001180
Iteration 98/1000 | Loss: 0.00001180
Iteration 99/1000 | Loss: 0.00001179
Iteration 100/1000 | Loss: 0.00001179
Iteration 101/1000 | Loss: 0.00001178
Iteration 102/1000 | Loss: 0.00001178
Iteration 103/1000 | Loss: 0.00001178
Iteration 104/1000 | Loss: 0.00001178
Iteration 105/1000 | Loss: 0.00001178
Iteration 106/1000 | Loss: 0.00001177
Iteration 107/1000 | Loss: 0.00001177
Iteration 108/1000 | Loss: 0.00001177
Iteration 109/1000 | Loss: 0.00001177
Iteration 110/1000 | Loss: 0.00001177
Iteration 111/1000 | Loss: 0.00001177
Iteration 112/1000 | Loss: 0.00001177
Iteration 113/1000 | Loss: 0.00001177
Iteration 114/1000 | Loss: 0.00001176
Iteration 115/1000 | Loss: 0.00001176
Iteration 116/1000 | Loss: 0.00001176
Iteration 117/1000 | Loss: 0.00001175
Iteration 118/1000 | Loss: 0.00001175
Iteration 119/1000 | Loss: 0.00001175
Iteration 120/1000 | Loss: 0.00001175
Iteration 121/1000 | Loss: 0.00001175
Iteration 122/1000 | Loss: 0.00001175
Iteration 123/1000 | Loss: 0.00001174
Iteration 124/1000 | Loss: 0.00001174
Iteration 125/1000 | Loss: 0.00001174
Iteration 126/1000 | Loss: 0.00001173
Iteration 127/1000 | Loss: 0.00001173
Iteration 128/1000 | Loss: 0.00001173
Iteration 129/1000 | Loss: 0.00001173
Iteration 130/1000 | Loss: 0.00001172
Iteration 131/1000 | Loss: 0.00001172
Iteration 132/1000 | Loss: 0.00001172
Iteration 133/1000 | Loss: 0.00001172
Iteration 134/1000 | Loss: 0.00001172
Iteration 135/1000 | Loss: 0.00001172
Iteration 136/1000 | Loss: 0.00001172
Iteration 137/1000 | Loss: 0.00001172
Iteration 138/1000 | Loss: 0.00001172
Iteration 139/1000 | Loss: 0.00001172
Iteration 140/1000 | Loss: 0.00001172
Iteration 141/1000 | Loss: 0.00001172
Iteration 142/1000 | Loss: 0.00001172
Iteration 143/1000 | Loss: 0.00001172
Iteration 144/1000 | Loss: 0.00001171
Iteration 145/1000 | Loss: 0.00001171
Iteration 146/1000 | Loss: 0.00001171
Iteration 147/1000 | Loss: 0.00001171
Iteration 148/1000 | Loss: 0.00001171
Iteration 149/1000 | Loss: 0.00001171
Iteration 150/1000 | Loss: 0.00001170
Iteration 151/1000 | Loss: 0.00001170
Iteration 152/1000 | Loss: 0.00001170
Iteration 153/1000 | Loss: 0.00001170
Iteration 154/1000 | Loss: 0.00001170
Iteration 155/1000 | Loss: 0.00001170
Iteration 156/1000 | Loss: 0.00001169
Iteration 157/1000 | Loss: 0.00001169
Iteration 158/1000 | Loss: 0.00001169
Iteration 159/1000 | Loss: 0.00001169
Iteration 160/1000 | Loss: 0.00001169
Iteration 161/1000 | Loss: 0.00001169
Iteration 162/1000 | Loss: 0.00001169
Iteration 163/1000 | Loss: 0.00001169
Iteration 164/1000 | Loss: 0.00001169
Iteration 165/1000 | Loss: 0.00001169
Iteration 166/1000 | Loss: 0.00001169
Iteration 167/1000 | Loss: 0.00001169
Iteration 168/1000 | Loss: 0.00001169
Iteration 169/1000 | Loss: 0.00001169
Iteration 170/1000 | Loss: 0.00001169
Iteration 171/1000 | Loss: 0.00001169
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 171. Stopping optimization.
Last 5 losses: [1.1694556633301545e-05, 1.1694556633301545e-05, 1.1694556633301545e-05, 1.1694556633301545e-05, 1.1694556633301545e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1694556633301545e-05

Optimization complete. Final v2v error: 2.8354034423828125 mm

Highest mean error: 3.1946470737457275 mm for frame 98

Lowest mean error: 2.0451786518096924 mm for frame 8

Saving results

Total time: 40.89880323410034
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_31_nl_1440/0007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_31_nl_1440/0007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_31_nl_1440/0007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01056079
Iteration 2/25 | Loss: 0.00194817
Iteration 3/25 | Loss: 0.00136465
Iteration 4/25 | Loss: 0.00121672
Iteration 5/25 | Loss: 0.00115299
Iteration 6/25 | Loss: 0.00111073
Iteration 7/25 | Loss: 0.00109027
Iteration 8/25 | Loss: 0.00108041
Iteration 9/25 | Loss: 0.00106041
Iteration 10/25 | Loss: 0.00102815
Iteration 11/25 | Loss: 0.00102786
Iteration 12/25 | Loss: 0.00101685
Iteration 13/25 | Loss: 0.00100616
Iteration 14/25 | Loss: 0.00100473
Iteration 15/25 | Loss: 0.00099851
Iteration 16/25 | Loss: 0.00100181
Iteration 17/25 | Loss: 0.00100002
Iteration 18/25 | Loss: 0.00099484
Iteration 19/25 | Loss: 0.00099415
Iteration 20/25 | Loss: 0.00099434
Iteration 21/25 | Loss: 0.00099319
Iteration 22/25 | Loss: 0.00099256
Iteration 23/25 | Loss: 0.00099349
Iteration 24/25 | Loss: 0.00099218
Iteration 25/25 | Loss: 0.00099204

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.27249992
Iteration 2/25 | Loss: 0.00168751
Iteration 3/25 | Loss: 0.00168750
Iteration 4/25 | Loss: 0.00168750
Iteration 5/25 | Loss: 0.00168750
Iteration 6/25 | Loss: 0.00168750
Iteration 7/25 | Loss: 0.00168750
Iteration 8/25 | Loss: 0.00168750
Iteration 9/25 | Loss: 0.00168750
Iteration 10/25 | Loss: 0.00168750
Iteration 11/25 | Loss: 0.00168750
Iteration 12/25 | Loss: 0.00168750
Iteration 13/25 | Loss: 0.00168750
Iteration 14/25 | Loss: 0.00168750
Iteration 15/25 | Loss: 0.00168750
Iteration 16/25 | Loss: 0.00168750
Iteration 17/25 | Loss: 0.00168750
Iteration 18/25 | Loss: 0.00168750
Iteration 19/25 | Loss: 0.00168750
Iteration 20/25 | Loss: 0.00168750
Iteration 21/25 | Loss: 0.00168750
Iteration 22/25 | Loss: 0.00168750
Iteration 23/25 | Loss: 0.00168750
Iteration 24/25 | Loss: 0.00168750
Iteration 25/25 | Loss: 0.00168750

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00168750
Iteration 2/1000 | Loss: 0.00003823
Iteration 3/1000 | Loss: 0.00007487
Iteration 4/1000 | Loss: 0.00011289
Iteration 5/1000 | Loss: 0.00002656
Iteration 6/1000 | Loss: 0.00002328
Iteration 7/1000 | Loss: 0.00012519
Iteration 8/1000 | Loss: 0.00002749
Iteration 9/1000 | Loss: 0.00016507
Iteration 10/1000 | Loss: 0.00018485
Iteration 11/1000 | Loss: 0.00019873
Iteration 12/1000 | Loss: 0.00002950
Iteration 13/1000 | Loss: 0.00002564
Iteration 14/1000 | Loss: 0.00002365
Iteration 15/1000 | Loss: 0.00054919
Iteration 16/1000 | Loss: 0.00003794
Iteration 17/1000 | Loss: 0.00005662
Iteration 18/1000 | Loss: 0.00003795
Iteration 19/1000 | Loss: 0.00006401
Iteration 20/1000 | Loss: 0.00001397
Iteration 21/1000 | Loss: 0.00005539
Iteration 22/1000 | Loss: 0.00001348
Iteration 23/1000 | Loss: 0.00001130
Iteration 24/1000 | Loss: 0.00001080
Iteration 25/1000 | Loss: 0.00001039
Iteration 26/1000 | Loss: 0.00001004
Iteration 27/1000 | Loss: 0.00000973
Iteration 28/1000 | Loss: 0.00000950
Iteration 29/1000 | Loss: 0.00004015
Iteration 30/1000 | Loss: 0.00000923
Iteration 31/1000 | Loss: 0.00000912
Iteration 32/1000 | Loss: 0.00000911
Iteration 33/1000 | Loss: 0.00000906
Iteration 34/1000 | Loss: 0.00000905
Iteration 35/1000 | Loss: 0.00000904
Iteration 36/1000 | Loss: 0.00000904
Iteration 37/1000 | Loss: 0.00000904
Iteration 38/1000 | Loss: 0.00000903
Iteration 39/1000 | Loss: 0.00004123
Iteration 40/1000 | Loss: 0.00001126
Iteration 41/1000 | Loss: 0.00002134
Iteration 42/1000 | Loss: 0.00000895
Iteration 43/1000 | Loss: 0.00000895
Iteration 44/1000 | Loss: 0.00000894
Iteration 45/1000 | Loss: 0.00000894
Iteration 46/1000 | Loss: 0.00000894
Iteration 47/1000 | Loss: 0.00000894
Iteration 48/1000 | Loss: 0.00000894
Iteration 49/1000 | Loss: 0.00000894
Iteration 50/1000 | Loss: 0.00000894
Iteration 51/1000 | Loss: 0.00000894
Iteration 52/1000 | Loss: 0.00000894
Iteration 53/1000 | Loss: 0.00000894
Iteration 54/1000 | Loss: 0.00000894
Iteration 55/1000 | Loss: 0.00000893
Iteration 56/1000 | Loss: 0.00000893
Iteration 57/1000 | Loss: 0.00000893
Iteration 58/1000 | Loss: 0.00000893
Iteration 59/1000 | Loss: 0.00000892
Iteration 60/1000 | Loss: 0.00000892
Iteration 61/1000 | Loss: 0.00000892
Iteration 62/1000 | Loss: 0.00000892
Iteration 63/1000 | Loss: 0.00000892
Iteration 64/1000 | Loss: 0.00000892
Iteration 65/1000 | Loss: 0.00000892
Iteration 66/1000 | Loss: 0.00000891
Iteration 67/1000 | Loss: 0.00000891
Iteration 68/1000 | Loss: 0.00000891
Iteration 69/1000 | Loss: 0.00000891
Iteration 70/1000 | Loss: 0.00000891
Iteration 71/1000 | Loss: 0.00000891
Iteration 72/1000 | Loss: 0.00000890
Iteration 73/1000 | Loss: 0.00000890
Iteration 74/1000 | Loss: 0.00000890
Iteration 75/1000 | Loss: 0.00000890
Iteration 76/1000 | Loss: 0.00000890
Iteration 77/1000 | Loss: 0.00000890
Iteration 78/1000 | Loss: 0.00000889
Iteration 79/1000 | Loss: 0.00000889
Iteration 80/1000 | Loss: 0.00000889
Iteration 81/1000 | Loss: 0.00000889
Iteration 82/1000 | Loss: 0.00000889
Iteration 83/1000 | Loss: 0.00000889
Iteration 84/1000 | Loss: 0.00000889
Iteration 85/1000 | Loss: 0.00000889
Iteration 86/1000 | Loss: 0.00000889
Iteration 87/1000 | Loss: 0.00000889
Iteration 88/1000 | Loss: 0.00000889
Iteration 89/1000 | Loss: 0.00000889
Iteration 90/1000 | Loss: 0.00000888
Iteration 91/1000 | Loss: 0.00000888
Iteration 92/1000 | Loss: 0.00000888
Iteration 93/1000 | Loss: 0.00000888
Iteration 94/1000 | Loss: 0.00000888
Iteration 95/1000 | Loss: 0.00000888
Iteration 96/1000 | Loss: 0.00000888
Iteration 97/1000 | Loss: 0.00000888
Iteration 98/1000 | Loss: 0.00000888
Iteration 99/1000 | Loss: 0.00000888
Iteration 100/1000 | Loss: 0.00000888
Iteration 101/1000 | Loss: 0.00000888
Iteration 102/1000 | Loss: 0.00000888
Iteration 103/1000 | Loss: 0.00000887
Iteration 104/1000 | Loss: 0.00000887
Iteration 105/1000 | Loss: 0.00000887
Iteration 106/1000 | Loss: 0.00000887
Iteration 107/1000 | Loss: 0.00000887
Iteration 108/1000 | Loss: 0.00000887
Iteration 109/1000 | Loss: 0.00000887
Iteration 110/1000 | Loss: 0.00000887
Iteration 111/1000 | Loss: 0.00000887
Iteration 112/1000 | Loss: 0.00000887
Iteration 113/1000 | Loss: 0.00000887
Iteration 114/1000 | Loss: 0.00000887
Iteration 115/1000 | Loss: 0.00000887
Iteration 116/1000 | Loss: 0.00000886
Iteration 117/1000 | Loss: 0.00000886
Iteration 118/1000 | Loss: 0.00000886
Iteration 119/1000 | Loss: 0.00003301
Iteration 120/1000 | Loss: 0.00000976
Iteration 121/1000 | Loss: 0.00001024
Iteration 122/1000 | Loss: 0.00000889
Iteration 123/1000 | Loss: 0.00000886
Iteration 124/1000 | Loss: 0.00000886
Iteration 125/1000 | Loss: 0.00000886
Iteration 126/1000 | Loss: 0.00000886
Iteration 127/1000 | Loss: 0.00000886
Iteration 128/1000 | Loss: 0.00001740
Iteration 129/1000 | Loss: 0.00000891
Iteration 130/1000 | Loss: 0.00000888
Iteration 131/1000 | Loss: 0.00000888
Iteration 132/1000 | Loss: 0.00000887
Iteration 133/1000 | Loss: 0.00000887
Iteration 134/1000 | Loss: 0.00000887
Iteration 135/1000 | Loss: 0.00000887
Iteration 136/1000 | Loss: 0.00000886
Iteration 137/1000 | Loss: 0.00000886
Iteration 138/1000 | Loss: 0.00000886
Iteration 139/1000 | Loss: 0.00000886
Iteration 140/1000 | Loss: 0.00000886
Iteration 141/1000 | Loss: 0.00000886
Iteration 142/1000 | Loss: 0.00000886
Iteration 143/1000 | Loss: 0.00000886
Iteration 144/1000 | Loss: 0.00000886
Iteration 145/1000 | Loss: 0.00000886
Iteration 146/1000 | Loss: 0.00000886
Iteration 147/1000 | Loss: 0.00000886
Iteration 148/1000 | Loss: 0.00000886
Iteration 149/1000 | Loss: 0.00000885
Iteration 150/1000 | Loss: 0.00000885
Iteration 151/1000 | Loss: 0.00000885
Iteration 152/1000 | Loss: 0.00000884
Iteration 153/1000 | Loss: 0.00000884
Iteration 154/1000 | Loss: 0.00000884
Iteration 155/1000 | Loss: 0.00000884
Iteration 156/1000 | Loss: 0.00000884
Iteration 157/1000 | Loss: 0.00000884
Iteration 158/1000 | Loss: 0.00000883
Iteration 159/1000 | Loss: 0.00000883
Iteration 160/1000 | Loss: 0.00000883
Iteration 161/1000 | Loss: 0.00000883
Iteration 162/1000 | Loss: 0.00000883
Iteration 163/1000 | Loss: 0.00000883
Iteration 164/1000 | Loss: 0.00000883
Iteration 165/1000 | Loss: 0.00000883
Iteration 166/1000 | Loss: 0.00000882
Iteration 167/1000 | Loss: 0.00000882
Iteration 168/1000 | Loss: 0.00000882
Iteration 169/1000 | Loss: 0.00000882
Iteration 170/1000 | Loss: 0.00000882
Iteration 171/1000 | Loss: 0.00000882
Iteration 172/1000 | Loss: 0.00000882
Iteration 173/1000 | Loss: 0.00000882
Iteration 174/1000 | Loss: 0.00000882
Iteration 175/1000 | Loss: 0.00000881
Iteration 176/1000 | Loss: 0.00000881
Iteration 177/1000 | Loss: 0.00000881
Iteration 178/1000 | Loss: 0.00000881
Iteration 179/1000 | Loss: 0.00000881
Iteration 180/1000 | Loss: 0.00000880
Iteration 181/1000 | Loss: 0.00000880
Iteration 182/1000 | Loss: 0.00000880
Iteration 183/1000 | Loss: 0.00000880
Iteration 184/1000 | Loss: 0.00000880
Iteration 185/1000 | Loss: 0.00000880
Iteration 186/1000 | Loss: 0.00000880
Iteration 187/1000 | Loss: 0.00000880
Iteration 188/1000 | Loss: 0.00000880
Iteration 189/1000 | Loss: 0.00000880
Iteration 190/1000 | Loss: 0.00000880
Iteration 191/1000 | Loss: 0.00000880
Iteration 192/1000 | Loss: 0.00000880
Iteration 193/1000 | Loss: 0.00000880
Iteration 194/1000 | Loss: 0.00000880
Iteration 195/1000 | Loss: 0.00000880
Iteration 196/1000 | Loss: 0.00000880
Iteration 197/1000 | Loss: 0.00000880
Iteration 198/1000 | Loss: 0.00000880
Iteration 199/1000 | Loss: 0.00000880
Iteration 200/1000 | Loss: 0.00000880
Iteration 201/1000 | Loss: 0.00000880
Iteration 202/1000 | Loss: 0.00000880
Iteration 203/1000 | Loss: 0.00000880
Iteration 204/1000 | Loss: 0.00000880
Iteration 205/1000 | Loss: 0.00000880
Iteration 206/1000 | Loss: 0.00000880
Iteration 207/1000 | Loss: 0.00000880
Iteration 208/1000 | Loss: 0.00000880
Iteration 209/1000 | Loss: 0.00000880
Iteration 210/1000 | Loss: 0.00000880
Iteration 211/1000 | Loss: 0.00000880
Iteration 212/1000 | Loss: 0.00000880
Iteration 213/1000 | Loss: 0.00000880
Iteration 214/1000 | Loss: 0.00000880
Iteration 215/1000 | Loss: 0.00000880
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 215. Stopping optimization.
Last 5 losses: [8.803884156804997e-06, 8.803884156804997e-06, 8.803884156804997e-06, 8.803884156804997e-06, 8.803884156804997e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.803884156804997e-06

Optimization complete. Final v2v error: 2.4546475410461426 mm

Highest mean error: 3.5808706283569336 mm for frame 22

Lowest mean error: 2.0147249698638916 mm for frame 124

Saving results

Total time: 109.66732692718506
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_31_nl_1440/0003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_31_nl_1440/0003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_31_nl_1440/0003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00409909
Iteration 2/25 | Loss: 0.00110036
Iteration 3/25 | Loss: 0.00099806
Iteration 4/25 | Loss: 0.00098366
Iteration 5/25 | Loss: 0.00097882
Iteration 6/25 | Loss: 0.00097708
Iteration 7/25 | Loss: 0.00097664
Iteration 8/25 | Loss: 0.00097661
Iteration 9/25 | Loss: 0.00097661
Iteration 10/25 | Loss: 0.00097661
Iteration 11/25 | Loss: 0.00097661
Iteration 12/25 | Loss: 0.00097661
Iteration 13/25 | Loss: 0.00097661
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0009766137227416039, 0.0009766137227416039, 0.0009766137227416039, 0.0009766137227416039, 0.0009766137227416039]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009766137227416039

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.23102510
Iteration 2/25 | Loss: 0.00185157
Iteration 3/25 | Loss: 0.00185157
Iteration 4/25 | Loss: 0.00185157
Iteration 5/25 | Loss: 0.00185157
Iteration 6/25 | Loss: 0.00185157
Iteration 7/25 | Loss: 0.00185157
Iteration 8/25 | Loss: 0.00185157
Iteration 9/25 | Loss: 0.00185156
Iteration 10/25 | Loss: 0.00185156
Iteration 11/25 | Loss: 0.00185156
Iteration 12/25 | Loss: 0.00185156
Iteration 13/25 | Loss: 0.00185156
Iteration 14/25 | Loss: 0.00185156
Iteration 15/25 | Loss: 0.00185156
Iteration 16/25 | Loss: 0.00185156
Iteration 17/25 | Loss: 0.00185156
Iteration 18/25 | Loss: 0.00185156
Iteration 19/25 | Loss: 0.00185156
Iteration 20/25 | Loss: 0.00185156
Iteration 21/25 | Loss: 0.00185156
Iteration 22/25 | Loss: 0.00185156
Iteration 23/25 | Loss: 0.00185156
Iteration 24/25 | Loss: 0.00185156
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.001851564273238182, 0.001851564273238182, 0.001851564273238182, 0.001851564273238182, 0.001851564273238182]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001851564273238182

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00185156
Iteration 2/1000 | Loss: 0.00004104
Iteration 3/1000 | Loss: 0.00002107
Iteration 4/1000 | Loss: 0.00001403
Iteration 5/1000 | Loss: 0.00001219
Iteration 6/1000 | Loss: 0.00001132
Iteration 7/1000 | Loss: 0.00001064
Iteration 8/1000 | Loss: 0.00001030
Iteration 9/1000 | Loss: 0.00000990
Iteration 10/1000 | Loss: 0.00000972
Iteration 11/1000 | Loss: 0.00000960
Iteration 12/1000 | Loss: 0.00000956
Iteration 13/1000 | Loss: 0.00000950
Iteration 14/1000 | Loss: 0.00000948
Iteration 15/1000 | Loss: 0.00000945
Iteration 16/1000 | Loss: 0.00000942
Iteration 17/1000 | Loss: 0.00000940
Iteration 18/1000 | Loss: 0.00000939
Iteration 19/1000 | Loss: 0.00000939
Iteration 20/1000 | Loss: 0.00000937
Iteration 21/1000 | Loss: 0.00000935
Iteration 22/1000 | Loss: 0.00000935
Iteration 23/1000 | Loss: 0.00000934
Iteration 24/1000 | Loss: 0.00000934
Iteration 25/1000 | Loss: 0.00000933
Iteration 26/1000 | Loss: 0.00000932
Iteration 27/1000 | Loss: 0.00000931
Iteration 28/1000 | Loss: 0.00000930
Iteration 29/1000 | Loss: 0.00000930
Iteration 30/1000 | Loss: 0.00000929
Iteration 31/1000 | Loss: 0.00000929
Iteration 32/1000 | Loss: 0.00000928
Iteration 33/1000 | Loss: 0.00000928
Iteration 34/1000 | Loss: 0.00000927
Iteration 35/1000 | Loss: 0.00000926
Iteration 36/1000 | Loss: 0.00000926
Iteration 37/1000 | Loss: 0.00000926
Iteration 38/1000 | Loss: 0.00000925
Iteration 39/1000 | Loss: 0.00000925
Iteration 40/1000 | Loss: 0.00000925
Iteration 41/1000 | Loss: 0.00000925
Iteration 42/1000 | Loss: 0.00000924
Iteration 43/1000 | Loss: 0.00000924
Iteration 44/1000 | Loss: 0.00000924
Iteration 45/1000 | Loss: 0.00000924
Iteration 46/1000 | Loss: 0.00000924
Iteration 47/1000 | Loss: 0.00000923
Iteration 48/1000 | Loss: 0.00000923
Iteration 49/1000 | Loss: 0.00000922
Iteration 50/1000 | Loss: 0.00000922
Iteration 51/1000 | Loss: 0.00000921
Iteration 52/1000 | Loss: 0.00000921
Iteration 53/1000 | Loss: 0.00000920
Iteration 54/1000 | Loss: 0.00000920
Iteration 55/1000 | Loss: 0.00000920
Iteration 56/1000 | Loss: 0.00000919
Iteration 57/1000 | Loss: 0.00000919
Iteration 58/1000 | Loss: 0.00000919
Iteration 59/1000 | Loss: 0.00000919
Iteration 60/1000 | Loss: 0.00000917
Iteration 61/1000 | Loss: 0.00000917
Iteration 62/1000 | Loss: 0.00000916
Iteration 63/1000 | Loss: 0.00000916
Iteration 64/1000 | Loss: 0.00000915
Iteration 65/1000 | Loss: 0.00000915
Iteration 66/1000 | Loss: 0.00000915
Iteration 67/1000 | Loss: 0.00000915
Iteration 68/1000 | Loss: 0.00000915
Iteration 69/1000 | Loss: 0.00000915
Iteration 70/1000 | Loss: 0.00000915
Iteration 71/1000 | Loss: 0.00000915
Iteration 72/1000 | Loss: 0.00000914
Iteration 73/1000 | Loss: 0.00000914
Iteration 74/1000 | Loss: 0.00000914
Iteration 75/1000 | Loss: 0.00000914
Iteration 76/1000 | Loss: 0.00000914
Iteration 77/1000 | Loss: 0.00000914
Iteration 78/1000 | Loss: 0.00000913
Iteration 79/1000 | Loss: 0.00000913
Iteration 80/1000 | Loss: 0.00000913
Iteration 81/1000 | Loss: 0.00000913
Iteration 82/1000 | Loss: 0.00000913
Iteration 83/1000 | Loss: 0.00000913
Iteration 84/1000 | Loss: 0.00000913
Iteration 85/1000 | Loss: 0.00000913
Iteration 86/1000 | Loss: 0.00000913
Iteration 87/1000 | Loss: 0.00000912
Iteration 88/1000 | Loss: 0.00000912
Iteration 89/1000 | Loss: 0.00000912
Iteration 90/1000 | Loss: 0.00000912
Iteration 91/1000 | Loss: 0.00000912
Iteration 92/1000 | Loss: 0.00000911
Iteration 93/1000 | Loss: 0.00000911
Iteration 94/1000 | Loss: 0.00000911
Iteration 95/1000 | Loss: 0.00000911
Iteration 96/1000 | Loss: 0.00000911
Iteration 97/1000 | Loss: 0.00000911
Iteration 98/1000 | Loss: 0.00000911
Iteration 99/1000 | Loss: 0.00000911
Iteration 100/1000 | Loss: 0.00000911
Iteration 101/1000 | Loss: 0.00000911
Iteration 102/1000 | Loss: 0.00000911
Iteration 103/1000 | Loss: 0.00000911
Iteration 104/1000 | Loss: 0.00000911
Iteration 105/1000 | Loss: 0.00000910
Iteration 106/1000 | Loss: 0.00000910
Iteration 107/1000 | Loss: 0.00000910
Iteration 108/1000 | Loss: 0.00000910
Iteration 109/1000 | Loss: 0.00000909
Iteration 110/1000 | Loss: 0.00000909
Iteration 111/1000 | Loss: 0.00000909
Iteration 112/1000 | Loss: 0.00000909
Iteration 113/1000 | Loss: 0.00000909
Iteration 114/1000 | Loss: 0.00000909
Iteration 115/1000 | Loss: 0.00000908
Iteration 116/1000 | Loss: 0.00000908
Iteration 117/1000 | Loss: 0.00000908
Iteration 118/1000 | Loss: 0.00000908
Iteration 119/1000 | Loss: 0.00000908
Iteration 120/1000 | Loss: 0.00000908
Iteration 121/1000 | Loss: 0.00000908
Iteration 122/1000 | Loss: 0.00000907
Iteration 123/1000 | Loss: 0.00000907
Iteration 124/1000 | Loss: 0.00000907
Iteration 125/1000 | Loss: 0.00000907
Iteration 126/1000 | Loss: 0.00000907
Iteration 127/1000 | Loss: 0.00000907
Iteration 128/1000 | Loss: 0.00000907
Iteration 129/1000 | Loss: 0.00000907
Iteration 130/1000 | Loss: 0.00000907
Iteration 131/1000 | Loss: 0.00000907
Iteration 132/1000 | Loss: 0.00000906
Iteration 133/1000 | Loss: 0.00000906
Iteration 134/1000 | Loss: 0.00000906
Iteration 135/1000 | Loss: 0.00000906
Iteration 136/1000 | Loss: 0.00000906
Iteration 137/1000 | Loss: 0.00000906
Iteration 138/1000 | Loss: 0.00000906
Iteration 139/1000 | Loss: 0.00000905
Iteration 140/1000 | Loss: 0.00000905
Iteration 141/1000 | Loss: 0.00000905
Iteration 142/1000 | Loss: 0.00000905
Iteration 143/1000 | Loss: 0.00000905
Iteration 144/1000 | Loss: 0.00000905
Iteration 145/1000 | Loss: 0.00000905
Iteration 146/1000 | Loss: 0.00000905
Iteration 147/1000 | Loss: 0.00000905
Iteration 148/1000 | Loss: 0.00000905
Iteration 149/1000 | Loss: 0.00000904
Iteration 150/1000 | Loss: 0.00000904
Iteration 151/1000 | Loss: 0.00000904
Iteration 152/1000 | Loss: 0.00000904
Iteration 153/1000 | Loss: 0.00000904
Iteration 154/1000 | Loss: 0.00000904
Iteration 155/1000 | Loss: 0.00000903
Iteration 156/1000 | Loss: 0.00000903
Iteration 157/1000 | Loss: 0.00000903
Iteration 158/1000 | Loss: 0.00000903
Iteration 159/1000 | Loss: 0.00000903
Iteration 160/1000 | Loss: 0.00000902
Iteration 161/1000 | Loss: 0.00000902
Iteration 162/1000 | Loss: 0.00000902
Iteration 163/1000 | Loss: 0.00000902
Iteration 164/1000 | Loss: 0.00000902
Iteration 165/1000 | Loss: 0.00000902
Iteration 166/1000 | Loss: 0.00000902
Iteration 167/1000 | Loss: 0.00000902
Iteration 168/1000 | Loss: 0.00000902
Iteration 169/1000 | Loss: 0.00000902
Iteration 170/1000 | Loss: 0.00000902
Iteration 171/1000 | Loss: 0.00000902
Iteration 172/1000 | Loss: 0.00000902
Iteration 173/1000 | Loss: 0.00000902
Iteration 174/1000 | Loss: 0.00000902
Iteration 175/1000 | Loss: 0.00000901
Iteration 176/1000 | Loss: 0.00000901
Iteration 177/1000 | Loss: 0.00000901
Iteration 178/1000 | Loss: 0.00000901
Iteration 179/1000 | Loss: 0.00000901
Iteration 180/1000 | Loss: 0.00000901
Iteration 181/1000 | Loss: 0.00000901
Iteration 182/1000 | Loss: 0.00000901
Iteration 183/1000 | Loss: 0.00000901
Iteration 184/1000 | Loss: 0.00000901
Iteration 185/1000 | Loss: 0.00000901
Iteration 186/1000 | Loss: 0.00000901
Iteration 187/1000 | Loss: 0.00000901
Iteration 188/1000 | Loss: 0.00000901
Iteration 189/1000 | Loss: 0.00000901
Iteration 190/1000 | Loss: 0.00000900
Iteration 191/1000 | Loss: 0.00000900
Iteration 192/1000 | Loss: 0.00000900
Iteration 193/1000 | Loss: 0.00000900
Iteration 194/1000 | Loss: 0.00000900
Iteration 195/1000 | Loss: 0.00000900
Iteration 196/1000 | Loss: 0.00000900
Iteration 197/1000 | Loss: 0.00000900
Iteration 198/1000 | Loss: 0.00000900
Iteration 199/1000 | Loss: 0.00000900
Iteration 200/1000 | Loss: 0.00000900
Iteration 201/1000 | Loss: 0.00000900
Iteration 202/1000 | Loss: 0.00000900
Iteration 203/1000 | Loss: 0.00000900
Iteration 204/1000 | Loss: 0.00000900
Iteration 205/1000 | Loss: 0.00000900
Iteration 206/1000 | Loss: 0.00000900
Iteration 207/1000 | Loss: 0.00000900
Iteration 208/1000 | Loss: 0.00000900
Iteration 209/1000 | Loss: 0.00000900
Iteration 210/1000 | Loss: 0.00000900
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 210. Stopping optimization.
Last 5 losses: [9.002666956803296e-06, 9.002666956803296e-06, 9.002666956803296e-06, 9.002666956803296e-06, 9.002666956803296e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.002666956803296e-06

Optimization complete. Final v2v error: 2.494969367980957 mm

Highest mean error: 3.481657028198242 mm for frame 27

Lowest mean error: 2.001631021499634 mm for frame 85

Saving results

Total time: 40.52932405471802
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_31_nl_1440/0018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_31_nl_1440/0018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_31_nl_1440/0018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01086550
Iteration 2/25 | Loss: 0.01086550
Iteration 3/25 | Loss: 0.01086549
Iteration 4/25 | Loss: 0.01086549
Iteration 5/25 | Loss: 0.01086549
Iteration 6/25 | Loss: 0.01086549
Iteration 7/25 | Loss: 0.01086549
Iteration 8/25 | Loss: 0.01086549
Iteration 9/25 | Loss: 0.01086549
Iteration 10/25 | Loss: 0.01086549
Iteration 11/25 | Loss: 0.01086549
Iteration 12/25 | Loss: 0.01086549
Iteration 13/25 | Loss: 0.01086549
Iteration 14/25 | Loss: 0.01086549
Iteration 15/25 | Loss: 0.01086549
Iteration 16/25 | Loss: 0.01086549
Iteration 17/25 | Loss: 0.01086549
Iteration 18/25 | Loss: 0.01086549
Iteration 19/25 | Loss: 0.01086549
Iteration 20/25 | Loss: 0.01086549
Iteration 21/25 | Loss: 0.01086549
Iteration 22/25 | Loss: 0.01086549
Iteration 23/25 | Loss: 0.01086549
Iteration 24/25 | Loss: 0.01086549
Iteration 25/25 | Loss: 0.01086548

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.70646000
Iteration 2/25 | Loss: 0.08100506
Iteration 3/25 | Loss: 0.08097275
Iteration 4/25 | Loss: 0.08097275
Iteration 5/25 | Loss: 0.08097275
Iteration 6/25 | Loss: 0.08097273
Iteration 7/25 | Loss: 0.08097272
Iteration 8/25 | Loss: 0.08097272
Iteration 9/25 | Loss: 0.08097273
Iteration 10/25 | Loss: 0.08097272
Iteration 11/25 | Loss: 0.08097272
Iteration 12/25 | Loss: 0.08097272
Iteration 13/25 | Loss: 0.08097272
Iteration 14/25 | Loss: 0.08097272
Iteration 15/25 | Loss: 0.08097272
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.08097272366285324, 0.08097272366285324, 0.08097272366285324, 0.08097272366285324, 0.08097272366285324]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.08097272366285324

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.08097272
Iteration 2/1000 | Loss: 0.01132443
Iteration 3/1000 | Loss: 0.00248975
Iteration 4/1000 | Loss: 0.00063672
Iteration 5/1000 | Loss: 0.00137659
Iteration 6/1000 | Loss: 0.00050477
Iteration 7/1000 | Loss: 0.00071347
Iteration 8/1000 | Loss: 0.00190249
Iteration 9/1000 | Loss: 0.00170860
Iteration 10/1000 | Loss: 0.00207757
Iteration 11/1000 | Loss: 0.00215617
Iteration 12/1000 | Loss: 0.00246636
Iteration 13/1000 | Loss: 0.00193686
Iteration 14/1000 | Loss: 0.00106816
Iteration 15/1000 | Loss: 0.00246145
Iteration 16/1000 | Loss: 0.00012516
Iteration 17/1000 | Loss: 0.00007549
Iteration 18/1000 | Loss: 0.00005709
Iteration 19/1000 | Loss: 0.00004714
Iteration 20/1000 | Loss: 0.00034684
Iteration 21/1000 | Loss: 0.00025249
Iteration 22/1000 | Loss: 0.00006462
Iteration 23/1000 | Loss: 0.00003585
Iteration 24/1000 | Loss: 0.00013419
Iteration 25/1000 | Loss: 0.00021324
Iteration 26/1000 | Loss: 0.00028763
Iteration 27/1000 | Loss: 0.00057533
Iteration 28/1000 | Loss: 0.00003282
Iteration 29/1000 | Loss: 0.00003033
Iteration 30/1000 | Loss: 0.00016550
Iteration 31/1000 | Loss: 0.00026487
Iteration 32/1000 | Loss: 0.00002869
Iteration 33/1000 | Loss: 0.00002747
Iteration 34/1000 | Loss: 0.00023615
Iteration 35/1000 | Loss: 0.00003028
Iteration 36/1000 | Loss: 0.00003227
Iteration 37/1000 | Loss: 0.00002741
Iteration 38/1000 | Loss: 0.00002574
Iteration 39/1000 | Loss: 0.00019306
Iteration 40/1000 | Loss: 0.00012594
Iteration 41/1000 | Loss: 0.00002485
Iteration 42/1000 | Loss: 0.00007927
Iteration 43/1000 | Loss: 0.00002403
Iteration 44/1000 | Loss: 0.00002345
Iteration 45/1000 | Loss: 0.00004155
Iteration 46/1000 | Loss: 0.00013496
Iteration 47/1000 | Loss: 0.00002608
Iteration 48/1000 | Loss: 0.00014015
Iteration 49/1000 | Loss: 0.00023584
Iteration 50/1000 | Loss: 0.00008556
Iteration 51/1000 | Loss: 0.00004947
Iteration 52/1000 | Loss: 0.00004161
Iteration 53/1000 | Loss: 0.00003360
Iteration 54/1000 | Loss: 0.00004136
Iteration 55/1000 | Loss: 0.00003017
Iteration 56/1000 | Loss: 0.00008724
Iteration 57/1000 | Loss: 0.00002261
Iteration 58/1000 | Loss: 0.00008964
Iteration 59/1000 | Loss: 0.00005133
Iteration 60/1000 | Loss: 0.00002253
Iteration 61/1000 | Loss: 0.00007782
Iteration 62/1000 | Loss: 0.00004653
Iteration 63/1000 | Loss: 0.00007260
Iteration 64/1000 | Loss: 0.00006663
Iteration 65/1000 | Loss: 0.00007392
Iteration 66/1000 | Loss: 0.00005900
Iteration 67/1000 | Loss: 0.00005296
Iteration 68/1000 | Loss: 0.00002459
Iteration 69/1000 | Loss: 0.00004114
Iteration 70/1000 | Loss: 0.00003022
Iteration 71/1000 | Loss: 0.00004374
Iteration 72/1000 | Loss: 0.00002870
Iteration 73/1000 | Loss: 0.00003026
Iteration 74/1000 | Loss: 0.00002206
Iteration 75/1000 | Loss: 0.00002199
Iteration 76/1000 | Loss: 0.00003990
Iteration 77/1000 | Loss: 0.00006341
Iteration 78/1000 | Loss: 0.00005749
Iteration 79/1000 | Loss: 0.00002831
Iteration 80/1000 | Loss: 0.00002650
Iteration 81/1000 | Loss: 0.00002205
Iteration 82/1000 | Loss: 0.00002173
Iteration 83/1000 | Loss: 0.00002152
Iteration 84/1000 | Loss: 0.00002148
Iteration 85/1000 | Loss: 0.00002198
Iteration 86/1000 | Loss: 0.00002212
Iteration 87/1000 | Loss: 0.00005771
Iteration 88/1000 | Loss: 0.00002669
Iteration 89/1000 | Loss: 0.00005260
Iteration 90/1000 | Loss: 0.00007830
Iteration 91/1000 | Loss: 0.00006456
Iteration 92/1000 | Loss: 0.00003586
Iteration 93/1000 | Loss: 0.00004389
Iteration 94/1000 | Loss: 0.00002677
Iteration 95/1000 | Loss: 0.00003248
Iteration 96/1000 | Loss: 0.00002276
Iteration 97/1000 | Loss: 0.00004225
Iteration 98/1000 | Loss: 0.00002715
Iteration 99/1000 | Loss: 0.00003865
Iteration 100/1000 | Loss: 0.00002526
Iteration 101/1000 | Loss: 0.00005701
Iteration 102/1000 | Loss: 0.00007104
Iteration 103/1000 | Loss: 0.00003776
Iteration 104/1000 | Loss: 0.00002902
Iteration 105/1000 | Loss: 0.00005334
Iteration 106/1000 | Loss: 0.00002860
Iteration 107/1000 | Loss: 0.00003746
Iteration 108/1000 | Loss: 0.00002211
Iteration 109/1000 | Loss: 0.00002179
Iteration 110/1000 | Loss: 0.00002207
Iteration 111/1000 | Loss: 0.00002529
Iteration 112/1000 | Loss: 0.00002290
Iteration 113/1000 | Loss: 0.00002151
Iteration 114/1000 | Loss: 0.00002139
Iteration 115/1000 | Loss: 0.00002139
Iteration 116/1000 | Loss: 0.00002138
Iteration 117/1000 | Loss: 0.00002135
Iteration 118/1000 | Loss: 0.00002148
Iteration 119/1000 | Loss: 0.00003988
Iteration 120/1000 | Loss: 0.00004676
Iteration 121/1000 | Loss: 0.00002224
Iteration 122/1000 | Loss: 0.00005680
Iteration 123/1000 | Loss: 0.00005049
Iteration 124/1000 | Loss: 0.00005720
Iteration 125/1000 | Loss: 0.00005865
Iteration 126/1000 | Loss: 0.00003723
Iteration 127/1000 | Loss: 0.00002218
Iteration 128/1000 | Loss: 0.00003700
Iteration 129/1000 | Loss: 0.00002132
Iteration 130/1000 | Loss: 0.00002110
Iteration 131/1000 | Loss: 0.00002135
Iteration 132/1000 | Loss: 0.00002088
Iteration 133/1000 | Loss: 0.00002069
Iteration 134/1000 | Loss: 0.00002061
Iteration 135/1000 | Loss: 0.00002061
Iteration 136/1000 | Loss: 0.00002060
Iteration 137/1000 | Loss: 0.00002060
Iteration 138/1000 | Loss: 0.00002060
Iteration 139/1000 | Loss: 0.00002060
Iteration 140/1000 | Loss: 0.00002060
Iteration 141/1000 | Loss: 0.00002060
Iteration 142/1000 | Loss: 0.00002053
Iteration 143/1000 | Loss: 0.00002051
Iteration 144/1000 | Loss: 0.00002051
Iteration 145/1000 | Loss: 0.00002050
Iteration 146/1000 | Loss: 0.00002049
Iteration 147/1000 | Loss: 0.00002074
Iteration 148/1000 | Loss: 0.00002073
Iteration 149/1000 | Loss: 0.00002055
Iteration 150/1000 | Loss: 0.00002062
Iteration 151/1000 | Loss: 0.00002099
Iteration 152/1000 | Loss: 0.00002075
Iteration 153/1000 | Loss: 0.00002090
Iteration 154/1000 | Loss: 0.00002064
Iteration 155/1000 | Loss: 0.00002048
Iteration 156/1000 | Loss: 0.00002036
Iteration 157/1000 | Loss: 0.00002036
Iteration 158/1000 | Loss: 0.00002035
Iteration 159/1000 | Loss: 0.00002035
Iteration 160/1000 | Loss: 0.00002035
Iteration 161/1000 | Loss: 0.00002035
Iteration 162/1000 | Loss: 0.00002035
Iteration 163/1000 | Loss: 0.00002035
Iteration 164/1000 | Loss: 0.00002035
Iteration 165/1000 | Loss: 0.00002035
Iteration 166/1000 | Loss: 0.00002035
Iteration 167/1000 | Loss: 0.00002035
Iteration 168/1000 | Loss: 0.00002035
Iteration 169/1000 | Loss: 0.00002034
Iteration 170/1000 | Loss: 0.00002034
Iteration 171/1000 | Loss: 0.00002034
Iteration 172/1000 | Loss: 0.00002034
Iteration 173/1000 | Loss: 0.00002034
Iteration 174/1000 | Loss: 0.00002034
Iteration 175/1000 | Loss: 0.00002034
Iteration 176/1000 | Loss: 0.00002034
Iteration 177/1000 | Loss: 0.00002034
Iteration 178/1000 | Loss: 0.00002034
Iteration 179/1000 | Loss: 0.00002034
Iteration 180/1000 | Loss: 0.00002034
Iteration 181/1000 | Loss: 0.00002034
Iteration 182/1000 | Loss: 0.00002034
Iteration 183/1000 | Loss: 0.00002034
Iteration 184/1000 | Loss: 0.00002034
Iteration 185/1000 | Loss: 0.00002034
Iteration 186/1000 | Loss: 0.00002034
Iteration 187/1000 | Loss: 0.00002034
Iteration 188/1000 | Loss: 0.00002034
Iteration 189/1000 | Loss: 0.00002034
Iteration 190/1000 | Loss: 0.00002034
Iteration 191/1000 | Loss: 0.00002034
Iteration 192/1000 | Loss: 0.00002034
Iteration 193/1000 | Loss: 0.00002034
Iteration 194/1000 | Loss: 0.00002034
Iteration 195/1000 | Loss: 0.00002034
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 195. Stopping optimization.
Last 5 losses: [2.0344934455351904e-05, 2.0344934455351904e-05, 2.0344934455351904e-05, 2.0344934455351904e-05, 2.0344934455351904e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0344934455351904e-05

Optimization complete. Final v2v error: 3.387751817703247 mm

Highest mean error: 20.04217529296875 mm for frame 18

Lowest mean error: 2.8608953952789307 mm for frame 110

Saving results

Total time: 222.87237405776978
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_31_nl_1440/0019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_31_nl_1440/0019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_31_nl_1440/0019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01043153
Iteration 2/25 | Loss: 0.00241410
Iteration 3/25 | Loss: 0.00175450
Iteration 4/25 | Loss: 0.00156860
Iteration 5/25 | Loss: 0.00162012
Iteration 6/25 | Loss: 0.00133801
Iteration 7/25 | Loss: 0.00124295
Iteration 8/25 | Loss: 0.00120772
Iteration 9/25 | Loss: 0.00121172
Iteration 10/25 | Loss: 0.00118554
Iteration 11/25 | Loss: 0.00117820
Iteration 12/25 | Loss: 0.00117696
Iteration 13/25 | Loss: 0.00117207
Iteration 14/25 | Loss: 0.00117060
Iteration 15/25 | Loss: 0.00116992
Iteration 16/25 | Loss: 0.00117169
Iteration 17/25 | Loss: 0.00117125
Iteration 18/25 | Loss: 0.00117088
Iteration 19/25 | Loss: 0.00116962
Iteration 20/25 | Loss: 0.00116894
Iteration 21/25 | Loss: 0.00116863
Iteration 22/25 | Loss: 0.00116845
Iteration 23/25 | Loss: 0.00116845
Iteration 24/25 | Loss: 0.00116845
Iteration 25/25 | Loss: 0.00116845

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.17678702
Iteration 2/25 | Loss: 0.00403313
Iteration 3/25 | Loss: 0.00224863
Iteration 4/25 | Loss: 0.00224861
Iteration 5/25 | Loss: 0.00224861
Iteration 6/25 | Loss: 0.00224861
Iteration 7/25 | Loss: 0.00224861
Iteration 8/25 | Loss: 0.00224861
Iteration 9/25 | Loss: 0.00224861
Iteration 10/25 | Loss: 0.00224861
Iteration 11/25 | Loss: 0.00224861
Iteration 12/25 | Loss: 0.00224861
Iteration 13/25 | Loss: 0.00224861
Iteration 14/25 | Loss: 0.00224861
Iteration 15/25 | Loss: 0.00224861
Iteration 16/25 | Loss: 0.00224861
Iteration 17/25 | Loss: 0.00224861
Iteration 18/25 | Loss: 0.00224861
Iteration 19/25 | Loss: 0.00224861
Iteration 20/25 | Loss: 0.00224861
Iteration 21/25 | Loss: 0.00224861
Iteration 22/25 | Loss: 0.00224861
Iteration 23/25 | Loss: 0.00224861
Iteration 24/25 | Loss: 0.00224861
Iteration 25/25 | Loss: 0.00224861

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00224861
Iteration 2/1000 | Loss: 0.00151084
Iteration 3/1000 | Loss: 0.00015119
Iteration 4/1000 | Loss: 0.00012237
Iteration 5/1000 | Loss: 0.00010574
Iteration 6/1000 | Loss: 0.00009679
Iteration 7/1000 | Loss: 0.00009261
Iteration 8/1000 | Loss: 0.00008865
Iteration 9/1000 | Loss: 0.00008629
Iteration 10/1000 | Loss: 0.00019365
Iteration 11/1000 | Loss: 0.00113663
Iteration 12/1000 | Loss: 0.00034363
Iteration 13/1000 | Loss: 0.00123264
Iteration 14/1000 | Loss: 0.00160757
Iteration 15/1000 | Loss: 0.00021373
Iteration 16/1000 | Loss: 0.00011873
Iteration 17/1000 | Loss: 0.00013375
Iteration 18/1000 | Loss: 0.00011885
Iteration 19/1000 | Loss: 0.00009998
Iteration 20/1000 | Loss: 0.00005273
Iteration 21/1000 | Loss: 0.00009755
Iteration 22/1000 | Loss: 0.00009542
Iteration 23/1000 | Loss: 0.00004576
Iteration 24/1000 | Loss: 0.00004139
Iteration 25/1000 | Loss: 0.00004366
Iteration 26/1000 | Loss: 0.00003539
Iteration 27/1000 | Loss: 0.00003324
Iteration 28/1000 | Loss: 0.00005631
Iteration 29/1000 | Loss: 0.00003000
Iteration 30/1000 | Loss: 0.00011211
Iteration 31/1000 | Loss: 0.00003566
Iteration 32/1000 | Loss: 0.00002920
Iteration 33/1000 | Loss: 0.00002740
Iteration 34/1000 | Loss: 0.00002627
Iteration 35/1000 | Loss: 0.00002564
Iteration 36/1000 | Loss: 0.00002531
Iteration 37/1000 | Loss: 0.00002509
Iteration 38/1000 | Loss: 0.00002492
Iteration 39/1000 | Loss: 0.00002480
Iteration 40/1000 | Loss: 0.00002473
Iteration 41/1000 | Loss: 0.00002472
Iteration 42/1000 | Loss: 0.00002472
Iteration 43/1000 | Loss: 0.00002472
Iteration 44/1000 | Loss: 0.00002471
Iteration 45/1000 | Loss: 0.00002470
Iteration 46/1000 | Loss: 0.00002470
Iteration 47/1000 | Loss: 0.00002470
Iteration 48/1000 | Loss: 0.00002470
Iteration 49/1000 | Loss: 0.00002469
Iteration 50/1000 | Loss: 0.00002469
Iteration 51/1000 | Loss: 0.00002467
Iteration 52/1000 | Loss: 0.00002466
Iteration 53/1000 | Loss: 0.00002466
Iteration 54/1000 | Loss: 0.00002466
Iteration 55/1000 | Loss: 0.00002466
Iteration 56/1000 | Loss: 0.00002466
Iteration 57/1000 | Loss: 0.00002465
Iteration 58/1000 | Loss: 0.00002464
Iteration 59/1000 | Loss: 0.00002464
Iteration 60/1000 | Loss: 0.00002463
Iteration 61/1000 | Loss: 0.00002461
Iteration 62/1000 | Loss: 0.00002461
Iteration 63/1000 | Loss: 0.00002460
Iteration 64/1000 | Loss: 0.00002460
Iteration 65/1000 | Loss: 0.00002459
Iteration 66/1000 | Loss: 0.00002459
Iteration 67/1000 | Loss: 0.00002458
Iteration 68/1000 | Loss: 0.00002458
Iteration 69/1000 | Loss: 0.00002458
Iteration 70/1000 | Loss: 0.00002458
Iteration 71/1000 | Loss: 0.00002457
Iteration 72/1000 | Loss: 0.00002457
Iteration 73/1000 | Loss: 0.00002457
Iteration 74/1000 | Loss: 0.00002457
Iteration 75/1000 | Loss: 0.00002457
Iteration 76/1000 | Loss: 0.00002456
Iteration 77/1000 | Loss: 0.00002456
Iteration 78/1000 | Loss: 0.00002456
Iteration 79/1000 | Loss: 0.00002456
Iteration 80/1000 | Loss: 0.00002456
Iteration 81/1000 | Loss: 0.00002455
Iteration 82/1000 | Loss: 0.00002455
Iteration 83/1000 | Loss: 0.00002455
Iteration 84/1000 | Loss: 0.00002455
Iteration 85/1000 | Loss: 0.00002455
Iteration 86/1000 | Loss: 0.00002455
Iteration 87/1000 | Loss: 0.00002455
Iteration 88/1000 | Loss: 0.00002455
Iteration 89/1000 | Loss: 0.00002454
Iteration 90/1000 | Loss: 0.00002454
Iteration 91/1000 | Loss: 0.00002454
Iteration 92/1000 | Loss: 0.00002454
Iteration 93/1000 | Loss: 0.00002454
Iteration 94/1000 | Loss: 0.00002454
Iteration 95/1000 | Loss: 0.00002453
Iteration 96/1000 | Loss: 0.00002453
Iteration 97/1000 | Loss: 0.00002453
Iteration 98/1000 | Loss: 0.00002453
Iteration 99/1000 | Loss: 0.00002452
Iteration 100/1000 | Loss: 0.00002452
Iteration 101/1000 | Loss: 0.00002452
Iteration 102/1000 | Loss: 0.00002452
Iteration 103/1000 | Loss: 0.00002452
Iteration 104/1000 | Loss: 0.00002452
Iteration 105/1000 | Loss: 0.00002452
Iteration 106/1000 | Loss: 0.00002452
Iteration 107/1000 | Loss: 0.00002452
Iteration 108/1000 | Loss: 0.00002452
Iteration 109/1000 | Loss: 0.00002452
Iteration 110/1000 | Loss: 0.00002452
Iteration 111/1000 | Loss: 0.00002452
Iteration 112/1000 | Loss: 0.00002452
Iteration 113/1000 | Loss: 0.00002452
Iteration 114/1000 | Loss: 0.00002452
Iteration 115/1000 | Loss: 0.00002452
Iteration 116/1000 | Loss: 0.00002451
Iteration 117/1000 | Loss: 0.00002451
Iteration 118/1000 | Loss: 0.00002451
Iteration 119/1000 | Loss: 0.00002451
Iteration 120/1000 | Loss: 0.00002451
Iteration 121/1000 | Loss: 0.00002451
Iteration 122/1000 | Loss: 0.00002450
Iteration 123/1000 | Loss: 0.00002450
Iteration 124/1000 | Loss: 0.00002450
Iteration 125/1000 | Loss: 0.00002450
Iteration 126/1000 | Loss: 0.00002450
Iteration 127/1000 | Loss: 0.00002450
Iteration 128/1000 | Loss: 0.00002449
Iteration 129/1000 | Loss: 0.00002449
Iteration 130/1000 | Loss: 0.00002449
Iteration 131/1000 | Loss: 0.00002449
Iteration 132/1000 | Loss: 0.00002449
Iteration 133/1000 | Loss: 0.00002449
Iteration 134/1000 | Loss: 0.00002449
Iteration 135/1000 | Loss: 0.00002449
Iteration 136/1000 | Loss: 0.00002449
Iteration 137/1000 | Loss: 0.00002449
Iteration 138/1000 | Loss: 0.00002449
Iteration 139/1000 | Loss: 0.00002449
Iteration 140/1000 | Loss: 0.00002449
Iteration 141/1000 | Loss: 0.00002449
Iteration 142/1000 | Loss: 0.00002449
Iteration 143/1000 | Loss: 0.00002449
Iteration 144/1000 | Loss: 0.00002449
Iteration 145/1000 | Loss: 0.00002448
Iteration 146/1000 | Loss: 0.00002448
Iteration 147/1000 | Loss: 0.00002448
Iteration 148/1000 | Loss: 0.00002448
Iteration 149/1000 | Loss: 0.00002448
Iteration 150/1000 | Loss: 0.00002448
Iteration 151/1000 | Loss: 0.00002447
Iteration 152/1000 | Loss: 0.00002447
Iteration 153/1000 | Loss: 0.00002447
Iteration 154/1000 | Loss: 0.00002447
Iteration 155/1000 | Loss: 0.00002447
Iteration 156/1000 | Loss: 0.00002447
Iteration 157/1000 | Loss: 0.00002447
Iteration 158/1000 | Loss: 0.00002447
Iteration 159/1000 | Loss: 0.00002447
Iteration 160/1000 | Loss: 0.00002447
Iteration 161/1000 | Loss: 0.00002447
Iteration 162/1000 | Loss: 0.00002447
Iteration 163/1000 | Loss: 0.00002447
Iteration 164/1000 | Loss: 0.00002447
Iteration 165/1000 | Loss: 0.00002446
Iteration 166/1000 | Loss: 0.00002446
Iteration 167/1000 | Loss: 0.00002446
Iteration 168/1000 | Loss: 0.00002446
Iteration 169/1000 | Loss: 0.00002446
Iteration 170/1000 | Loss: 0.00002446
Iteration 171/1000 | Loss: 0.00002446
Iteration 172/1000 | Loss: 0.00002446
Iteration 173/1000 | Loss: 0.00002446
Iteration 174/1000 | Loss: 0.00002446
Iteration 175/1000 | Loss: 0.00002446
Iteration 176/1000 | Loss: 0.00002446
Iteration 177/1000 | Loss: 0.00002446
Iteration 178/1000 | Loss: 0.00002446
Iteration 179/1000 | Loss: 0.00002446
Iteration 180/1000 | Loss: 0.00002446
Iteration 181/1000 | Loss: 0.00002446
Iteration 182/1000 | Loss: 0.00002446
Iteration 183/1000 | Loss: 0.00002446
Iteration 184/1000 | Loss: 0.00002446
Iteration 185/1000 | Loss: 0.00002446
Iteration 186/1000 | Loss: 0.00002446
Iteration 187/1000 | Loss: 0.00002446
Iteration 188/1000 | Loss: 0.00002446
Iteration 189/1000 | Loss: 0.00002446
Iteration 190/1000 | Loss: 0.00002446
Iteration 191/1000 | Loss: 0.00002446
Iteration 192/1000 | Loss: 0.00002446
Iteration 193/1000 | Loss: 0.00002446
Iteration 194/1000 | Loss: 0.00002446
Iteration 195/1000 | Loss: 0.00002446
Iteration 196/1000 | Loss: 0.00002446
Iteration 197/1000 | Loss: 0.00002446
Iteration 198/1000 | Loss: 0.00002446
Iteration 199/1000 | Loss: 0.00002446
Iteration 200/1000 | Loss: 0.00002446
Iteration 201/1000 | Loss: 0.00002446
Iteration 202/1000 | Loss: 0.00002446
Iteration 203/1000 | Loss: 0.00002446
Iteration 204/1000 | Loss: 0.00002446
Iteration 205/1000 | Loss: 0.00002446
Iteration 206/1000 | Loss: 0.00002446
Iteration 207/1000 | Loss: 0.00002446
Iteration 208/1000 | Loss: 0.00002446
Iteration 209/1000 | Loss: 0.00002446
Iteration 210/1000 | Loss: 0.00002446
Iteration 211/1000 | Loss: 0.00002446
Iteration 212/1000 | Loss: 0.00002446
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 212. Stopping optimization.
Last 5 losses: [2.4461882276227698e-05, 2.4461882276227698e-05, 2.4461882276227698e-05, 2.4461882276227698e-05, 2.4461882276227698e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4461882276227698e-05

Optimization complete. Final v2v error: 3.4868898391723633 mm

Highest mean error: 21.24187660217285 mm for frame 40

Lowest mean error: 2.8385047912597656 mm for frame 221

Saving results

Total time: 121.71149277687073
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_31_nl_1440/0009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_31_nl_1440/0009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_31_nl_1440/0009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01057785
Iteration 2/25 | Loss: 0.00140908
Iteration 3/25 | Loss: 0.00119944
Iteration 4/25 | Loss: 0.00113891
Iteration 5/25 | Loss: 0.00111972
Iteration 6/25 | Loss: 0.00109901
Iteration 7/25 | Loss: 0.00106187
Iteration 8/25 | Loss: 0.00106067
Iteration 9/25 | Loss: 0.00105231
Iteration 10/25 | Loss: 0.00105124
Iteration 11/25 | Loss: 0.00105064
Iteration 12/25 | Loss: 0.00105041
Iteration 13/25 | Loss: 0.00105038
Iteration 14/25 | Loss: 0.00105038
Iteration 15/25 | Loss: 0.00105037
Iteration 16/25 | Loss: 0.00105037
Iteration 17/25 | Loss: 0.00105037
Iteration 18/25 | Loss: 0.00105037
Iteration 19/25 | Loss: 0.00105037
Iteration 20/25 | Loss: 0.00105037
Iteration 21/25 | Loss: 0.00105037
Iteration 22/25 | Loss: 0.00105036
Iteration 23/25 | Loss: 0.00105036
Iteration 24/25 | Loss: 0.00105036
Iteration 25/25 | Loss: 0.00105036

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.79196239
Iteration 2/25 | Loss: 0.00180885
Iteration 3/25 | Loss: 0.00180885
Iteration 4/25 | Loss: 0.00180885
Iteration 5/25 | Loss: 0.00180884
Iteration 6/25 | Loss: 0.00180884
Iteration 7/25 | Loss: 0.00180884
Iteration 8/25 | Loss: 0.00180884
Iteration 9/25 | Loss: 0.00180884
Iteration 10/25 | Loss: 0.00180884
Iteration 11/25 | Loss: 0.00180884
Iteration 12/25 | Loss: 0.00180884
Iteration 13/25 | Loss: 0.00180884
Iteration 14/25 | Loss: 0.00180884
Iteration 15/25 | Loss: 0.00180884
Iteration 16/25 | Loss: 0.00180884
Iteration 17/25 | Loss: 0.00180884
Iteration 18/25 | Loss: 0.00180884
Iteration 19/25 | Loss: 0.00180884
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.001808842527680099, 0.001808842527680099, 0.001808842527680099, 0.001808842527680099, 0.001808842527680099]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001808842527680099

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00180884
Iteration 2/1000 | Loss: 0.00002540
Iteration 3/1000 | Loss: 0.00001650
Iteration 4/1000 | Loss: 0.00001416
Iteration 5/1000 | Loss: 0.00009023
Iteration 6/1000 | Loss: 0.00001298
Iteration 7/1000 | Loss: 0.00001262
Iteration 8/1000 | Loss: 0.00002886
Iteration 9/1000 | Loss: 0.00002840
Iteration 10/1000 | Loss: 0.00001770
Iteration 11/1000 | Loss: 0.00001312
Iteration 12/1000 | Loss: 0.00001217
Iteration 13/1000 | Loss: 0.00001172
Iteration 14/1000 | Loss: 0.00001153
Iteration 15/1000 | Loss: 0.00001150
Iteration 16/1000 | Loss: 0.00001150
Iteration 17/1000 | Loss: 0.00001150
Iteration 18/1000 | Loss: 0.00001150
Iteration 19/1000 | Loss: 0.00001150
Iteration 20/1000 | Loss: 0.00001150
Iteration 21/1000 | Loss: 0.00001150
Iteration 22/1000 | Loss: 0.00001150
Iteration 23/1000 | Loss: 0.00001149
Iteration 24/1000 | Loss: 0.00001149
Iteration 25/1000 | Loss: 0.00001149
Iteration 26/1000 | Loss: 0.00001149
Iteration 27/1000 | Loss: 0.00001149
Iteration 28/1000 | Loss: 0.00001149
Iteration 29/1000 | Loss: 0.00001148
Iteration 30/1000 | Loss: 0.00001148
Iteration 31/1000 | Loss: 0.00001148
Iteration 32/1000 | Loss: 0.00001147
Iteration 33/1000 | Loss: 0.00001147
Iteration 34/1000 | Loss: 0.00001147
Iteration 35/1000 | Loss: 0.00001147
Iteration 36/1000 | Loss: 0.00001147
Iteration 37/1000 | Loss: 0.00001147
Iteration 38/1000 | Loss: 0.00001147
Iteration 39/1000 | Loss: 0.00001147
Iteration 40/1000 | Loss: 0.00001147
Iteration 41/1000 | Loss: 0.00001147
Iteration 42/1000 | Loss: 0.00001146
Iteration 43/1000 | Loss: 0.00001146
Iteration 44/1000 | Loss: 0.00001146
Iteration 45/1000 | Loss: 0.00001146
Iteration 46/1000 | Loss: 0.00001146
Iteration 47/1000 | Loss: 0.00001146
Iteration 48/1000 | Loss: 0.00001146
Iteration 49/1000 | Loss: 0.00001145
Iteration 50/1000 | Loss: 0.00001145
Iteration 51/1000 | Loss: 0.00001144
Iteration 52/1000 | Loss: 0.00001144
Iteration 53/1000 | Loss: 0.00001144
Iteration 54/1000 | Loss: 0.00001144
Iteration 55/1000 | Loss: 0.00001144
Iteration 56/1000 | Loss: 0.00001144
Iteration 57/1000 | Loss: 0.00001144
Iteration 58/1000 | Loss: 0.00001144
Iteration 59/1000 | Loss: 0.00001144
Iteration 60/1000 | Loss: 0.00001144
Iteration 61/1000 | Loss: 0.00001144
Iteration 62/1000 | Loss: 0.00001144
Iteration 63/1000 | Loss: 0.00001144
Iteration 64/1000 | Loss: 0.00001144
Iteration 65/1000 | Loss: 0.00001144
Iteration 66/1000 | Loss: 0.00001144
Iteration 67/1000 | Loss: 0.00001144
Iteration 68/1000 | Loss: 0.00001144
Iteration 69/1000 | Loss: 0.00001144
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 69. Stopping optimization.
Last 5 losses: [1.1438696674304083e-05, 1.1438696674304083e-05, 1.1438696674304083e-05, 1.1438696674304083e-05, 1.1438696674304083e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1438696674304083e-05

Optimization complete. Final v2v error: 2.7965879440307617 mm

Highest mean error: 8.932198524475098 mm for frame 22

Lowest mean error: 2.501447916030884 mm for frame 36

Saving results

Total time: 51.06841731071472
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_36_nl_6641/0023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_nl_6641/0023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_nl_6641/0023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01090204
Iteration 2/25 | Loss: 0.01090204
Iteration 3/25 | Loss: 0.01090204
Iteration 4/25 | Loss: 0.01090203
Iteration 5/25 | Loss: 0.01090203
Iteration 6/25 | Loss: 0.01090203
Iteration 7/25 | Loss: 0.01090203
Iteration 8/25 | Loss: 0.01090202
Iteration 9/25 | Loss: 0.01090202
Iteration 10/25 | Loss: 0.01090202
Iteration 11/25 | Loss: 0.01090202
Iteration 12/25 | Loss: 0.01090201
Iteration 13/25 | Loss: 0.01090201
Iteration 14/25 | Loss: 0.01090201
Iteration 15/25 | Loss: 0.01090201
Iteration 16/25 | Loss: 0.01090201
Iteration 17/25 | Loss: 0.01090201
Iteration 18/25 | Loss: 0.01090200
Iteration 19/25 | Loss: 0.01090200
Iteration 20/25 | Loss: 0.01090200
Iteration 21/25 | Loss: 0.01090200
Iteration 22/25 | Loss: 0.01090200
Iteration 23/25 | Loss: 0.01090200
Iteration 24/25 | Loss: 0.01090199
Iteration 25/25 | Loss: 0.01090199

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.69407845
Iteration 2/25 | Loss: 0.07908037
Iteration 3/25 | Loss: 0.06961996
Iteration 4/25 | Loss: 0.06851482
Iteration 5/25 | Loss: 0.06850623
Iteration 6/25 | Loss: 0.06850623
Iteration 7/25 | Loss: 0.06850623
Iteration 8/25 | Loss: 0.06850623
Iteration 9/25 | Loss: 0.06850623
Iteration 10/25 | Loss: 0.06850623
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.06850623339414597, 0.06850623339414597, 0.06850623339414597, 0.06850623339414597, 0.06850623339414597]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.06850623339414597

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.06850623
Iteration 2/1000 | Loss: 0.01804851
Iteration 3/1000 | Loss: 0.01449359
Iteration 4/1000 | Loss: 0.00676922
Iteration 5/1000 | Loss: 0.00469697
Iteration 6/1000 | Loss: 0.00442134
Iteration 7/1000 | Loss: 0.00494668
Iteration 8/1000 | Loss: 0.00380336
Iteration 9/1000 | Loss: 0.00240874
Iteration 10/1000 | Loss: 0.00354649
Iteration 11/1000 | Loss: 0.00125273
Iteration 12/1000 | Loss: 0.00046819
Iteration 13/1000 | Loss: 0.00077817
Iteration 14/1000 | Loss: 0.00114270
Iteration 15/1000 | Loss: 0.00143958
Iteration 16/1000 | Loss: 0.00064789
Iteration 17/1000 | Loss: 0.00038382
Iteration 18/1000 | Loss: 0.00028775
Iteration 19/1000 | Loss: 0.00092162
Iteration 20/1000 | Loss: 0.00112098
Iteration 21/1000 | Loss: 0.00166546
Iteration 22/1000 | Loss: 0.00047724
Iteration 23/1000 | Loss: 0.00102334
Iteration 24/1000 | Loss: 0.00021786
Iteration 25/1000 | Loss: 0.00046919
Iteration 26/1000 | Loss: 0.00022074
Iteration 27/1000 | Loss: 0.00026423
Iteration 28/1000 | Loss: 0.00033433
Iteration 29/1000 | Loss: 0.00020573
Iteration 30/1000 | Loss: 0.00073855
Iteration 31/1000 | Loss: 0.00060011
Iteration 32/1000 | Loss: 0.00092348
Iteration 33/1000 | Loss: 0.00071869
Iteration 34/1000 | Loss: 0.00052941
Iteration 35/1000 | Loss: 0.00052427
Iteration 36/1000 | Loss: 0.00014584
Iteration 37/1000 | Loss: 0.00037554
Iteration 38/1000 | Loss: 0.00020912
Iteration 39/1000 | Loss: 0.00066047
Iteration 40/1000 | Loss: 0.00023359
Iteration 41/1000 | Loss: 0.00029593
Iteration 42/1000 | Loss: 0.00020609
Iteration 43/1000 | Loss: 0.00034851
Iteration 44/1000 | Loss: 0.00061290
Iteration 45/1000 | Loss: 0.00033077
Iteration 46/1000 | Loss: 0.00054931
Iteration 47/1000 | Loss: 0.00056225
Iteration 48/1000 | Loss: 0.00030108
Iteration 49/1000 | Loss: 0.00011818
Iteration 50/1000 | Loss: 0.00010938
Iteration 51/1000 | Loss: 0.00026769
Iteration 52/1000 | Loss: 0.00073296
Iteration 53/1000 | Loss: 0.00073612
Iteration 54/1000 | Loss: 0.00062552
Iteration 55/1000 | Loss: 0.00032073
Iteration 56/1000 | Loss: 0.00023389
Iteration 57/1000 | Loss: 0.00022343
Iteration 58/1000 | Loss: 0.00010220
Iteration 59/1000 | Loss: 0.00009487
Iteration 60/1000 | Loss: 0.00031878
Iteration 61/1000 | Loss: 0.00035533
Iteration 62/1000 | Loss: 0.00017946
Iteration 63/1000 | Loss: 0.00061071
Iteration 64/1000 | Loss: 0.00039915
Iteration 65/1000 | Loss: 0.00030561
Iteration 66/1000 | Loss: 0.00035229
Iteration 67/1000 | Loss: 0.00057700
Iteration 68/1000 | Loss: 0.00080894
Iteration 69/1000 | Loss: 0.00063614
Iteration 70/1000 | Loss: 0.00060086
Iteration 71/1000 | Loss: 0.00059446
Iteration 72/1000 | Loss: 0.00077291
Iteration 73/1000 | Loss: 0.00062030
Iteration 74/1000 | Loss: 0.00021131
Iteration 75/1000 | Loss: 0.00029811
Iteration 76/1000 | Loss: 0.00018838
Iteration 77/1000 | Loss: 0.00021979
Iteration 78/1000 | Loss: 0.00032458
Iteration 79/1000 | Loss: 0.00025478
Iteration 80/1000 | Loss: 0.00047481
Iteration 81/1000 | Loss: 0.00011536
Iteration 82/1000 | Loss: 0.00020880
Iteration 83/1000 | Loss: 0.00018586
Iteration 84/1000 | Loss: 0.00035846
Iteration 85/1000 | Loss: 0.00024557
Iteration 86/1000 | Loss: 0.00028659
Iteration 87/1000 | Loss: 0.00025894
Iteration 88/1000 | Loss: 0.00025135
Iteration 89/1000 | Loss: 0.00009065
Iteration 90/1000 | Loss: 0.00023239
Iteration 91/1000 | Loss: 0.00031868
Iteration 92/1000 | Loss: 0.00041544
Iteration 93/1000 | Loss: 0.00032417
Iteration 94/1000 | Loss: 0.00023667
Iteration 95/1000 | Loss: 0.00025842
Iteration 96/1000 | Loss: 0.00035056
Iteration 97/1000 | Loss: 0.00045895
Iteration 98/1000 | Loss: 0.00017732
Iteration 99/1000 | Loss: 0.00020716
Iteration 100/1000 | Loss: 0.00016470
Iteration 101/1000 | Loss: 0.00008986
Iteration 102/1000 | Loss: 0.00008360
Iteration 103/1000 | Loss: 0.00027354
Iteration 104/1000 | Loss: 0.00062017
Iteration 105/1000 | Loss: 0.00044977
Iteration 106/1000 | Loss: 0.00044459
Iteration 107/1000 | Loss: 0.00029976
Iteration 108/1000 | Loss: 0.00018734
Iteration 109/1000 | Loss: 0.00035809
Iteration 110/1000 | Loss: 0.00081758
Iteration 111/1000 | Loss: 0.00046074
Iteration 112/1000 | Loss: 0.00021857
Iteration 113/1000 | Loss: 0.00032972
Iteration 114/1000 | Loss: 0.00035607
Iteration 115/1000 | Loss: 0.00031575
Iteration 116/1000 | Loss: 0.00026784
Iteration 117/1000 | Loss: 0.00048092
Iteration 118/1000 | Loss: 0.00030835
Iteration 119/1000 | Loss: 0.00022363
Iteration 120/1000 | Loss: 0.00015003
Iteration 121/1000 | Loss: 0.00008305
Iteration 122/1000 | Loss: 0.00025439
Iteration 123/1000 | Loss: 0.00019894
Iteration 124/1000 | Loss: 0.00015434
Iteration 125/1000 | Loss: 0.00007441
Iteration 126/1000 | Loss: 0.00007289
Iteration 127/1000 | Loss: 0.00032520
Iteration 128/1000 | Loss: 0.00036122
Iteration 129/1000 | Loss: 0.00025944
Iteration 130/1000 | Loss: 0.00022826
Iteration 131/1000 | Loss: 0.00007249
Iteration 132/1000 | Loss: 0.00016830
Iteration 133/1000 | Loss: 0.00035384
Iteration 134/1000 | Loss: 0.00023498
Iteration 135/1000 | Loss: 0.00025364
Iteration 136/1000 | Loss: 0.00015518
Iteration 137/1000 | Loss: 0.00018269
Iteration 138/1000 | Loss: 0.00025648
Iteration 139/1000 | Loss: 0.00044049
Iteration 140/1000 | Loss: 0.00032162
Iteration 141/1000 | Loss: 0.00037582
Iteration 142/1000 | Loss: 0.00025832
Iteration 143/1000 | Loss: 0.00022839
Iteration 144/1000 | Loss: 0.00011037
Iteration 145/1000 | Loss: 0.00007543
Iteration 146/1000 | Loss: 0.00012555
Iteration 147/1000 | Loss: 0.00013173
Iteration 148/1000 | Loss: 0.00013545
Iteration 149/1000 | Loss: 0.00017210
Iteration 150/1000 | Loss: 0.00007664
Iteration 151/1000 | Loss: 0.00007092
Iteration 152/1000 | Loss: 0.00006954
Iteration 153/1000 | Loss: 0.00008585
Iteration 154/1000 | Loss: 0.00066647
Iteration 155/1000 | Loss: 0.00056594
Iteration 156/1000 | Loss: 0.00057806
Iteration 157/1000 | Loss: 0.00051483
Iteration 158/1000 | Loss: 0.00027234
Iteration 159/1000 | Loss: 0.00032326
Iteration 160/1000 | Loss: 0.00024421
Iteration 161/1000 | Loss: 0.00032951
Iteration 162/1000 | Loss: 0.00031679
Iteration 163/1000 | Loss: 0.00027128
Iteration 164/1000 | Loss: 0.00012129
Iteration 165/1000 | Loss: 0.00021161
Iteration 166/1000 | Loss: 0.00017354
Iteration 167/1000 | Loss: 0.00007998
Iteration 168/1000 | Loss: 0.00018859
Iteration 169/1000 | Loss: 0.00024620
Iteration 170/1000 | Loss: 0.00027025
Iteration 171/1000 | Loss: 0.00023567
Iteration 172/1000 | Loss: 0.00012085
Iteration 173/1000 | Loss: 0.00015601
Iteration 174/1000 | Loss: 0.00014850
Iteration 175/1000 | Loss: 0.00014079
Iteration 176/1000 | Loss: 0.00041717
Iteration 177/1000 | Loss: 0.00020516
Iteration 178/1000 | Loss: 0.00011718
Iteration 179/1000 | Loss: 0.00030222
Iteration 180/1000 | Loss: 0.00021164
Iteration 181/1000 | Loss: 0.00016168
Iteration 182/1000 | Loss: 0.00020571
Iteration 183/1000 | Loss: 0.00022108
Iteration 184/1000 | Loss: 0.00011222
Iteration 185/1000 | Loss: 0.00009342
Iteration 186/1000 | Loss: 0.00010677
Iteration 187/1000 | Loss: 0.00030561
Iteration 188/1000 | Loss: 0.00016657
Iteration 189/1000 | Loss: 0.00012770
Iteration 190/1000 | Loss: 0.00014338
Iteration 191/1000 | Loss: 0.00010901
Iteration 192/1000 | Loss: 0.00007528
Iteration 193/1000 | Loss: 0.00007229
Iteration 194/1000 | Loss: 0.00006890
Iteration 195/1000 | Loss: 0.00027612
Iteration 196/1000 | Loss: 0.00032414
Iteration 197/1000 | Loss: 0.00015774
Iteration 198/1000 | Loss: 0.00020571
Iteration 199/1000 | Loss: 0.00036645
Iteration 200/1000 | Loss: 0.00022272
Iteration 201/1000 | Loss: 0.00041415
Iteration 202/1000 | Loss: 0.00026547
Iteration 203/1000 | Loss: 0.00014502
Iteration 204/1000 | Loss: 0.00007842
Iteration 205/1000 | Loss: 0.00006850
Iteration 206/1000 | Loss: 0.00010448
Iteration 207/1000 | Loss: 0.00006625
Iteration 208/1000 | Loss: 0.00017143
Iteration 209/1000 | Loss: 0.00052170
Iteration 210/1000 | Loss: 0.00048067
Iteration 211/1000 | Loss: 0.00019482
Iteration 212/1000 | Loss: 0.00007611
Iteration 213/1000 | Loss: 0.00006862
Iteration 214/1000 | Loss: 0.00012612
Iteration 215/1000 | Loss: 0.00006377
Iteration 216/1000 | Loss: 0.00016725
Iteration 217/1000 | Loss: 0.00011773
Iteration 218/1000 | Loss: 0.00006239
Iteration 219/1000 | Loss: 0.00006133
Iteration 220/1000 | Loss: 0.00015104
Iteration 221/1000 | Loss: 0.00007049
Iteration 222/1000 | Loss: 0.00006242
Iteration 223/1000 | Loss: 0.00019050
Iteration 224/1000 | Loss: 0.00010684
Iteration 225/1000 | Loss: 0.00026355
Iteration 226/1000 | Loss: 0.00009128
Iteration 227/1000 | Loss: 0.00009043
Iteration 228/1000 | Loss: 0.00009078
Iteration 229/1000 | Loss: 0.00008872
Iteration 230/1000 | Loss: 0.00006552
Iteration 231/1000 | Loss: 0.00007340
Iteration 232/1000 | Loss: 0.00007900
Iteration 233/1000 | Loss: 0.00007093
Iteration 234/1000 | Loss: 0.00006682
Iteration 235/1000 | Loss: 0.00006180
Iteration 236/1000 | Loss: 0.00014963
Iteration 237/1000 | Loss: 0.00024485
Iteration 238/1000 | Loss: 0.00014481
Iteration 239/1000 | Loss: 0.00017107
Iteration 240/1000 | Loss: 0.00006174
Iteration 241/1000 | Loss: 0.00006028
Iteration 242/1000 | Loss: 0.00009672
Iteration 243/1000 | Loss: 0.00029174
Iteration 244/1000 | Loss: 0.00024699
Iteration 245/1000 | Loss: 0.00021214
Iteration 246/1000 | Loss: 0.00009141
Iteration 247/1000 | Loss: 0.00006853
Iteration 248/1000 | Loss: 0.00006515
Iteration 249/1000 | Loss: 0.00021406
Iteration 250/1000 | Loss: 0.00008249
Iteration 251/1000 | Loss: 0.00006361
Iteration 252/1000 | Loss: 0.00006089
Iteration 253/1000 | Loss: 0.00006014
Iteration 254/1000 | Loss: 0.00023006
Iteration 255/1000 | Loss: 0.00027304
Iteration 256/1000 | Loss: 0.00021515
Iteration 257/1000 | Loss: 0.00029109
Iteration 258/1000 | Loss: 0.00019616
Iteration 259/1000 | Loss: 0.00027020
Iteration 260/1000 | Loss: 0.00021011
Iteration 261/1000 | Loss: 0.00007067
Iteration 262/1000 | Loss: 0.00006459
Iteration 263/1000 | Loss: 0.00007754
Iteration 264/1000 | Loss: 0.00018651
Iteration 265/1000 | Loss: 0.00006324
Iteration 266/1000 | Loss: 0.00006078
Iteration 267/1000 | Loss: 0.00018194
Iteration 268/1000 | Loss: 0.00020314
Iteration 269/1000 | Loss: 0.00014699
Iteration 270/1000 | Loss: 0.00006889
Iteration 271/1000 | Loss: 0.00007674
Iteration 272/1000 | Loss: 0.00006443
Iteration 273/1000 | Loss: 0.00008116
Iteration 274/1000 | Loss: 0.00007290
Iteration 275/1000 | Loss: 0.00006043
Iteration 276/1000 | Loss: 0.00005936
Iteration 277/1000 | Loss: 0.00005879
Iteration 278/1000 | Loss: 0.00005823
Iteration 279/1000 | Loss: 0.00005791
Iteration 280/1000 | Loss: 0.00005776
Iteration 281/1000 | Loss: 0.00005774
Iteration 282/1000 | Loss: 0.00005774
Iteration 283/1000 | Loss: 0.00005773
Iteration 284/1000 | Loss: 0.00005773
Iteration 285/1000 | Loss: 0.00005773
Iteration 286/1000 | Loss: 0.00005772
Iteration 287/1000 | Loss: 0.00005773
Iteration 288/1000 | Loss: 0.00005768
Iteration 289/1000 | Loss: 0.00017296
Iteration 290/1000 | Loss: 0.00017898
Iteration 291/1000 | Loss: 0.00007525
Iteration 292/1000 | Loss: 0.00006778
Iteration 293/1000 | Loss: 0.00008658
Iteration 294/1000 | Loss: 0.00007050
Iteration 295/1000 | Loss: 0.00006271
Iteration 296/1000 | Loss: 0.00005980
Iteration 297/1000 | Loss: 0.00007138
Iteration 298/1000 | Loss: 0.00005958
Iteration 299/1000 | Loss: 0.00008679
Iteration 300/1000 | Loss: 0.00006767
Iteration 301/1000 | Loss: 0.00008431
Iteration 302/1000 | Loss: 0.00007378
Iteration 303/1000 | Loss: 0.00006045
Iteration 304/1000 | Loss: 0.00006765
Iteration 305/1000 | Loss: 0.00009930
Iteration 306/1000 | Loss: 0.00007436
Iteration 307/1000 | Loss: 0.00008018
Iteration 308/1000 | Loss: 0.00006625
Iteration 309/1000 | Loss: 0.00008234
Iteration 310/1000 | Loss: 0.00006559
Iteration 311/1000 | Loss: 0.00008233
Iteration 312/1000 | Loss: 0.00013760
Iteration 313/1000 | Loss: 0.00011156
Iteration 314/1000 | Loss: 0.00010148
Iteration 315/1000 | Loss: 0.00008938
Iteration 316/1000 | Loss: 0.00009336
Iteration 317/1000 | Loss: 0.00008258
Iteration 318/1000 | Loss: 0.00006300
Iteration 319/1000 | Loss: 0.00007427
Iteration 320/1000 | Loss: 0.00006620
Iteration 321/1000 | Loss: 0.00007639
Iteration 322/1000 | Loss: 0.00007097
Iteration 323/1000 | Loss: 0.00005918
Iteration 324/1000 | Loss: 0.00007599
Iteration 325/1000 | Loss: 0.00007665
Iteration 326/1000 | Loss: 0.00008805
Iteration 327/1000 | Loss: 0.00007615
Iteration 328/1000 | Loss: 0.00008343
Iteration 329/1000 | Loss: 0.00008635
Iteration 330/1000 | Loss: 0.00006707
Iteration 331/1000 | Loss: 0.00005819
Iteration 332/1000 | Loss: 0.00005762
Iteration 333/1000 | Loss: 0.00005719
Iteration 334/1000 | Loss: 0.00005701
Iteration 335/1000 | Loss: 0.00007604
Iteration 336/1000 | Loss: 0.00016319
Iteration 337/1000 | Loss: 0.00013962
Iteration 338/1000 | Loss: 0.00016287
Iteration 339/1000 | Loss: 0.00010896
Iteration 340/1000 | Loss: 0.00016938
Iteration 341/1000 | Loss: 0.00010799
Iteration 342/1000 | Loss: 0.00006461
Iteration 343/1000 | Loss: 0.00005990
Iteration 344/1000 | Loss: 0.00005722
Iteration 345/1000 | Loss: 0.00005681
Iteration 346/1000 | Loss: 0.00005664
Iteration 347/1000 | Loss: 0.00005654
Iteration 348/1000 | Loss: 0.00005650
Iteration 349/1000 | Loss: 0.00005647
Iteration 350/1000 | Loss: 0.00005643
Iteration 351/1000 | Loss: 0.00005639
Iteration 352/1000 | Loss: 0.00005637
Iteration 353/1000 | Loss: 0.00005637
Iteration 354/1000 | Loss: 0.00005636
Iteration 355/1000 | Loss: 0.00005636
Iteration 356/1000 | Loss: 0.00005635
Iteration 357/1000 | Loss: 0.00005635
Iteration 358/1000 | Loss: 0.00005635
Iteration 359/1000 | Loss: 0.00005634
Iteration 360/1000 | Loss: 0.00005634
Iteration 361/1000 | Loss: 0.00015057
Iteration 362/1000 | Loss: 0.00010186
Iteration 363/1000 | Loss: 0.00006285
Iteration 364/1000 | Loss: 0.00005959
Iteration 365/1000 | Loss: 0.00005852
Iteration 366/1000 | Loss: 0.00005789
Iteration 367/1000 | Loss: 0.00007541
Iteration 368/1000 | Loss: 0.00006637
Iteration 369/1000 | Loss: 0.00007429
Iteration 370/1000 | Loss: 0.00006655
Iteration 371/1000 | Loss: 0.00015356
Iteration 372/1000 | Loss: 0.00010273
Iteration 373/1000 | Loss: 0.00021610
Iteration 374/1000 | Loss: 0.00011704
Iteration 375/1000 | Loss: 0.00009974
Iteration 376/1000 | Loss: 0.00006773
Iteration 377/1000 | Loss: 0.00005930
Iteration 378/1000 | Loss: 0.00005687
Iteration 379/1000 | Loss: 0.00005640
Iteration 380/1000 | Loss: 0.00016819
Iteration 381/1000 | Loss: 0.00012177
Iteration 382/1000 | Loss: 0.00019741
Iteration 383/1000 | Loss: 0.00023671
Iteration 384/1000 | Loss: 0.00020452
Iteration 385/1000 | Loss: 0.00008785
Iteration 386/1000 | Loss: 0.00006661
Iteration 387/1000 | Loss: 0.00005996
Iteration 388/1000 | Loss: 0.00005683
Iteration 389/1000 | Loss: 0.00005637
Iteration 390/1000 | Loss: 0.00007538
Iteration 391/1000 | Loss: 0.00023153
Iteration 392/1000 | Loss: 0.00016860
Iteration 393/1000 | Loss: 0.00025175
Iteration 394/1000 | Loss: 0.00020555
Iteration 395/1000 | Loss: 0.00022686
Iteration 396/1000 | Loss: 0.00013473
Iteration 397/1000 | Loss: 0.00009902
Iteration 398/1000 | Loss: 0.00009594
Iteration 399/1000 | Loss: 0.00008241
Iteration 400/1000 | Loss: 0.00009756
Iteration 401/1000 | Loss: 0.00006587
Iteration 402/1000 | Loss: 0.00007236
Iteration 403/1000 | Loss: 0.00006505
Iteration 404/1000 | Loss: 0.00007117
Iteration 405/1000 | Loss: 0.00006575
Iteration 406/1000 | Loss: 0.00007191
Iteration 407/1000 | Loss: 0.00006901
Iteration 408/1000 | Loss: 0.00006687
Iteration 409/1000 | Loss: 0.00006760
Iteration 410/1000 | Loss: 0.00009459
Iteration 411/1000 | Loss: 0.00007064
Iteration 412/1000 | Loss: 0.00006937
Iteration 413/1000 | Loss: 0.00008130
Iteration 414/1000 | Loss: 0.00007685
Iteration 415/1000 | Loss: 0.00007110
Iteration 416/1000 | Loss: 0.00006897
Iteration 417/1000 | Loss: 0.00007918
Iteration 418/1000 | Loss: 0.00008157
Iteration 419/1000 | Loss: 0.00007720
Iteration 420/1000 | Loss: 0.00007099
Iteration 421/1000 | Loss: 0.00006710
Iteration 422/1000 | Loss: 0.00006057
Iteration 423/1000 | Loss: 0.00005777
Iteration 424/1000 | Loss: 0.00005706
Iteration 425/1000 | Loss: 0.00005816
Iteration 426/1000 | Loss: 0.00005667
Iteration 427/1000 | Loss: 0.00005654
Iteration 428/1000 | Loss: 0.00005647
Iteration 429/1000 | Loss: 0.00005647
Iteration 430/1000 | Loss: 0.00005643
Iteration 431/1000 | Loss: 0.00005641
Iteration 432/1000 | Loss: 0.00005641
Iteration 433/1000 | Loss: 0.00005641
Iteration 434/1000 | Loss: 0.00005640
Iteration 435/1000 | Loss: 0.00005640
Iteration 436/1000 | Loss: 0.00005640
Iteration 437/1000 | Loss: 0.00005639
Iteration 438/1000 | Loss: 0.00005639
Iteration 439/1000 | Loss: 0.00005639
Iteration 440/1000 | Loss: 0.00005638
Iteration 441/1000 | Loss: 0.00005636
Iteration 442/1000 | Loss: 0.00005635
Iteration 443/1000 | Loss: 0.00005634
Iteration 444/1000 | Loss: 0.00005634
Iteration 445/1000 | Loss: 0.00005632
Iteration 446/1000 | Loss: 0.00005631
Iteration 447/1000 | Loss: 0.00005631
Iteration 448/1000 | Loss: 0.00005630
Iteration 449/1000 | Loss: 0.00005630
Iteration 450/1000 | Loss: 0.00005630
Iteration 451/1000 | Loss: 0.00005629
Iteration 452/1000 | Loss: 0.00005627
Iteration 453/1000 | Loss: 0.00018851
Iteration 454/1000 | Loss: 0.00008987
Iteration 455/1000 | Loss: 0.00005648
Iteration 456/1000 | Loss: 0.00005624
Iteration 457/1000 | Loss: 0.00005624
Iteration 458/1000 | Loss: 0.00005626
Iteration 459/1000 | Loss: 0.00005625
Iteration 460/1000 | Loss: 0.00005625
Iteration 461/1000 | Loss: 0.00005624
Iteration 462/1000 | Loss: 0.00005624
Iteration 463/1000 | Loss: 0.00005624
Iteration 464/1000 | Loss: 0.00005624
Iteration 465/1000 | Loss: 0.00005621
Iteration 466/1000 | Loss: 0.00005621
Iteration 467/1000 | Loss: 0.00018861
Iteration 468/1000 | Loss: 0.00008644
Iteration 469/1000 | Loss: 0.00006197
Iteration 470/1000 | Loss: 0.00006031
Iteration 471/1000 | Loss: 0.00005766
Iteration 472/1000 | Loss: 0.00005706
Iteration 473/1000 | Loss: 0.00005638
Iteration 474/1000 | Loss: 0.00018657
Iteration 475/1000 | Loss: 0.00011687
Iteration 476/1000 | Loss: 0.00005628
Iteration 477/1000 | Loss: 0.00005614
Iteration 478/1000 | Loss: 0.00005614
Iteration 479/1000 | Loss: 0.00005614
Iteration 480/1000 | Loss: 0.00005613
Iteration 481/1000 | Loss: 0.00005613
Iteration 482/1000 | Loss: 0.00005612
Iteration 483/1000 | Loss: 0.00005612
Iteration 484/1000 | Loss: 0.00005612
Iteration 485/1000 | Loss: 0.00005611
Iteration 486/1000 | Loss: 0.00005611
Iteration 487/1000 | Loss: 0.00005611
Iteration 488/1000 | Loss: 0.00005611
Iteration 489/1000 | Loss: 0.00005611
Iteration 490/1000 | Loss: 0.00005611
Iteration 491/1000 | Loss: 0.00005611
Iteration 492/1000 | Loss: 0.00018545
Iteration 493/1000 | Loss: 0.00011550
Iteration 494/1000 | Loss: 0.00005861
Iteration 495/1000 | Loss: 0.00005645
Iteration 496/1000 | Loss: 0.00005617
Iteration 497/1000 | Loss: 0.00005614
Iteration 498/1000 | Loss: 0.00005610
Iteration 499/1000 | Loss: 0.00005614
Iteration 500/1000 | Loss: 0.00005612
Iteration 501/1000 | Loss: 0.00005610
Iteration 502/1000 | Loss: 0.00005610
Iteration 503/1000 | Loss: 0.00005610
Iteration 504/1000 | Loss: 0.00005610
Iteration 505/1000 | Loss: 0.00005610
Iteration 506/1000 | Loss: 0.00005610
Iteration 507/1000 | Loss: 0.00005609
Iteration 508/1000 | Loss: 0.00005609
Iteration 509/1000 | Loss: 0.00005609
Iteration 510/1000 | Loss: 0.00005609
Iteration 511/1000 | Loss: 0.00005609
Iteration 512/1000 | Loss: 0.00005608
Iteration 513/1000 | Loss: 0.00018845
Iteration 514/1000 | Loss: 0.00010411
Iteration 515/1000 | Loss: 0.00006355
Iteration 516/1000 | Loss: 0.00005917
Iteration 517/1000 | Loss: 0.00005737
Iteration 518/1000 | Loss: 0.00005650
Iteration 519/1000 | Loss: 0.00005637
Iteration 520/1000 | Loss: 0.00005631
Iteration 521/1000 | Loss: 0.00005622
Iteration 522/1000 | Loss: 0.00005619
Iteration 523/1000 | Loss: 0.00005614
Iteration 524/1000 | Loss: 0.00005611
Iteration 525/1000 | Loss: 0.00005606
Iteration 526/1000 | Loss: 0.00005604
Iteration 527/1000 | Loss: 0.00005602
Iteration 528/1000 | Loss: 0.00005600
Iteration 529/1000 | Loss: 0.00005600
Iteration 530/1000 | Loss: 0.00005599
Iteration 531/1000 | Loss: 0.00005599
Iteration 532/1000 | Loss: 0.00005599
Iteration 533/1000 | Loss: 0.00005598
Iteration 534/1000 | Loss: 0.00005598
Iteration 535/1000 | Loss: 0.00005598
Iteration 536/1000 | Loss: 0.00005598
Iteration 537/1000 | Loss: 0.00005598
Iteration 538/1000 | Loss: 0.00005598
Iteration 539/1000 | Loss: 0.00005598
Iteration 540/1000 | Loss: 0.00005598
Iteration 541/1000 | Loss: 0.00005598
Iteration 542/1000 | Loss: 0.00005598
Iteration 543/1000 | Loss: 0.00005598
Iteration 544/1000 | Loss: 0.00005596
Iteration 545/1000 | Loss: 0.00005595
Iteration 546/1000 | Loss: 0.00005595
Iteration 547/1000 | Loss: 0.00007424
Iteration 548/1000 | Loss: 0.00010797
Iteration 549/1000 | Loss: 0.00009130
Iteration 550/1000 | Loss: 0.00006143
Iteration 551/1000 | Loss: 0.00005794
Iteration 552/1000 | Loss: 0.00005903
Iteration 553/1000 | Loss: 0.00007151
Iteration 554/1000 | Loss: 0.00005929
Iteration 555/1000 | Loss: 0.00007143
Iteration 556/1000 | Loss: 0.00005655
Iteration 557/1000 | Loss: 0.00005598
Iteration 558/1000 | Loss: 0.00005587
Iteration 559/1000 | Loss: 0.00005586
Iteration 560/1000 | Loss: 0.00005575
Iteration 561/1000 | Loss: 0.00005579
Iteration 562/1000 | Loss: 0.00005579
Iteration 563/1000 | Loss: 0.00005573
Iteration 564/1000 | Loss: 0.00005566
Iteration 565/1000 | Loss: 0.00005566
Iteration 566/1000 | Loss: 0.00005565
Iteration 567/1000 | Loss: 0.00005562
Iteration 568/1000 | Loss: 0.00005562
Iteration 569/1000 | Loss: 0.00005562
Iteration 570/1000 | Loss: 0.00005562
Iteration 571/1000 | Loss: 0.00005562
Iteration 572/1000 | Loss: 0.00005562
Iteration 573/1000 | Loss: 0.00005562
Iteration 574/1000 | Loss: 0.00005561
Iteration 575/1000 | Loss: 0.00005561
Iteration 576/1000 | Loss: 0.00005560
Iteration 577/1000 | Loss: 0.00005560
Iteration 578/1000 | Loss: 0.00005558
Iteration 579/1000 | Loss: 0.00005558
Iteration 580/1000 | Loss: 0.00005558
Iteration 581/1000 | Loss: 0.00005557
Iteration 582/1000 | Loss: 0.00005557
Iteration 583/1000 | Loss: 0.00005557
Iteration 584/1000 | Loss: 0.00005557
Iteration 585/1000 | Loss: 0.00005556
Iteration 586/1000 | Loss: 0.00005556
Iteration 587/1000 | Loss: 0.00005556
Iteration 588/1000 | Loss: 0.00005556
Iteration 589/1000 | Loss: 0.00005556
Iteration 590/1000 | Loss: 0.00005556
Iteration 591/1000 | Loss: 0.00005556
Iteration 592/1000 | Loss: 0.00005556
Iteration 593/1000 | Loss: 0.00005556
Iteration 594/1000 | Loss: 0.00005556
Iteration 595/1000 | Loss: 0.00005556
Iteration 596/1000 | Loss: 0.00005554
Iteration 597/1000 | Loss: 0.00005554
Iteration 598/1000 | Loss: 0.00005554
Iteration 599/1000 | Loss: 0.00005560
Iteration 600/1000 | Loss: 0.00005560
Iteration 601/1000 | Loss: 0.00005560
Iteration 602/1000 | Loss: 0.00005560
Iteration 603/1000 | Loss: 0.00005557
Iteration 604/1000 | Loss: 0.00005557
Iteration 605/1000 | Loss: 0.00005556
Iteration 606/1000 | Loss: 0.00005556
Iteration 607/1000 | Loss: 0.00005556
Iteration 608/1000 | Loss: 0.00005556
Iteration 609/1000 | Loss: 0.00005556
Iteration 610/1000 | Loss: 0.00005555
Iteration 611/1000 | Loss: 0.00005555
Iteration 612/1000 | Loss: 0.00005555
Iteration 613/1000 | Loss: 0.00005555
Iteration 614/1000 | Loss: 0.00005555
Iteration 615/1000 | Loss: 0.00005555
Iteration 616/1000 | Loss: 0.00005555
Iteration 617/1000 | Loss: 0.00005555
Iteration 618/1000 | Loss: 0.00005554
Iteration 619/1000 | Loss: 0.00005554
Iteration 620/1000 | Loss: 0.00005554
Iteration 621/1000 | Loss: 0.00005554
Iteration 622/1000 | Loss: 0.00005554
Iteration 623/1000 | Loss: 0.00005553
Iteration 624/1000 | Loss: 0.00005553
Iteration 625/1000 | Loss: 0.00005551
Iteration 626/1000 | Loss: 0.00005550
Iteration 627/1000 | Loss: 0.00005550
Iteration 628/1000 | Loss: 0.00005550
Iteration 629/1000 | Loss: 0.00005555
Iteration 630/1000 | Loss: 0.00005555
Iteration 631/1000 | Loss: 0.00005555
Iteration 632/1000 | Loss: 0.00005550
Iteration 633/1000 | Loss: 0.00005550
Iteration 634/1000 | Loss: 0.00005550
Iteration 635/1000 | Loss: 0.00005550
Iteration 636/1000 | Loss: 0.00005550
Iteration 637/1000 | Loss: 0.00005550
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 637. Stopping optimization.
Last 5 losses: [5.5495773267466575e-05, 5.5495773267466575e-05, 5.5495773267466575e-05, 5.5495773267466575e-05, 5.5495773267466575e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 5.5495773267466575e-05

Optimization complete. Final v2v error: 4.764304161071777 mm

Highest mean error: 19.80889892578125 mm for frame 36

Lowest mean error: 3.284170627593994 mm for frame 85

Saving results

Total time: 754.0813417434692
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_36_nl_6641/0022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_nl_6641/0022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_nl_6641/0022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00469504
Iteration 2/25 | Loss: 0.00130320
Iteration 3/25 | Loss: 0.00119044
Iteration 4/25 | Loss: 0.00117131
Iteration 5/25 | Loss: 0.00116764
Iteration 6/25 | Loss: 0.00116644
Iteration 7/25 | Loss: 0.00116638
Iteration 8/25 | Loss: 0.00116638
Iteration 9/25 | Loss: 0.00116638
Iteration 10/25 | Loss: 0.00116638
Iteration 11/25 | Loss: 0.00116638
Iteration 12/25 | Loss: 0.00116638
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0011663768673315644, 0.0011663768673315644, 0.0011663768673315644, 0.0011663768673315644, 0.0011663768673315644]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011663768673315644

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36783731
Iteration 2/25 | Loss: 0.00046592
Iteration 3/25 | Loss: 0.00046591
Iteration 4/25 | Loss: 0.00046590
Iteration 5/25 | Loss: 0.00046590
Iteration 6/25 | Loss: 0.00046590
Iteration 7/25 | Loss: 0.00046590
Iteration 8/25 | Loss: 0.00046590
Iteration 9/25 | Loss: 0.00046590
Iteration 10/25 | Loss: 0.00046590
Iteration 11/25 | Loss: 0.00046590
Iteration 12/25 | Loss: 0.00046590
Iteration 13/25 | Loss: 0.00046590
Iteration 14/25 | Loss: 0.00046590
Iteration 15/25 | Loss: 0.00046590
Iteration 16/25 | Loss: 0.00046590
Iteration 17/25 | Loss: 0.00046590
Iteration 18/25 | Loss: 0.00046590
Iteration 19/25 | Loss: 0.00046590
Iteration 20/25 | Loss: 0.00046590
Iteration 21/25 | Loss: 0.00046590
Iteration 22/25 | Loss: 0.00046590
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.00046590244164690375, 0.00046590244164690375, 0.00046590244164690375, 0.00046590244164690375, 0.00046590244164690375]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00046590244164690375

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00046590
Iteration 2/1000 | Loss: 0.00005236
Iteration 3/1000 | Loss: 0.00003357
Iteration 4/1000 | Loss: 0.00002694
Iteration 5/1000 | Loss: 0.00002401
Iteration 6/1000 | Loss: 0.00002251
Iteration 7/1000 | Loss: 0.00002147
Iteration 8/1000 | Loss: 0.00002072
Iteration 9/1000 | Loss: 0.00002005
Iteration 10/1000 | Loss: 0.00001973
Iteration 11/1000 | Loss: 0.00001948
Iteration 12/1000 | Loss: 0.00001947
Iteration 13/1000 | Loss: 0.00001945
Iteration 14/1000 | Loss: 0.00001936
Iteration 15/1000 | Loss: 0.00001936
Iteration 16/1000 | Loss: 0.00001935
Iteration 17/1000 | Loss: 0.00001934
Iteration 18/1000 | Loss: 0.00001931
Iteration 19/1000 | Loss: 0.00001931
Iteration 20/1000 | Loss: 0.00001931
Iteration 21/1000 | Loss: 0.00001931
Iteration 22/1000 | Loss: 0.00001931
Iteration 23/1000 | Loss: 0.00001931
Iteration 24/1000 | Loss: 0.00001930
Iteration 25/1000 | Loss: 0.00001930
Iteration 26/1000 | Loss: 0.00001930
Iteration 27/1000 | Loss: 0.00001929
Iteration 28/1000 | Loss: 0.00001929
Iteration 29/1000 | Loss: 0.00001929
Iteration 30/1000 | Loss: 0.00001928
Iteration 31/1000 | Loss: 0.00001928
Iteration 32/1000 | Loss: 0.00001927
Iteration 33/1000 | Loss: 0.00001927
Iteration 34/1000 | Loss: 0.00001927
Iteration 35/1000 | Loss: 0.00001927
Iteration 36/1000 | Loss: 0.00001927
Iteration 37/1000 | Loss: 0.00001927
Iteration 38/1000 | Loss: 0.00001926
Iteration 39/1000 | Loss: 0.00001925
Iteration 40/1000 | Loss: 0.00001924
Iteration 41/1000 | Loss: 0.00001924
Iteration 42/1000 | Loss: 0.00001924
Iteration 43/1000 | Loss: 0.00001924
Iteration 44/1000 | Loss: 0.00001924
Iteration 45/1000 | Loss: 0.00001924
Iteration 46/1000 | Loss: 0.00001924
Iteration 47/1000 | Loss: 0.00001924
Iteration 48/1000 | Loss: 0.00001924
Iteration 49/1000 | Loss: 0.00001924
Iteration 50/1000 | Loss: 0.00001924
Iteration 51/1000 | Loss: 0.00001924
Iteration 52/1000 | Loss: 0.00001923
Iteration 53/1000 | Loss: 0.00001923
Iteration 54/1000 | Loss: 0.00001923
Iteration 55/1000 | Loss: 0.00001923
Iteration 56/1000 | Loss: 0.00001923
Iteration 57/1000 | Loss: 0.00001923
Iteration 58/1000 | Loss: 0.00001922
Iteration 59/1000 | Loss: 0.00001922
Iteration 60/1000 | Loss: 0.00001922
Iteration 61/1000 | Loss: 0.00001922
Iteration 62/1000 | Loss: 0.00001922
Iteration 63/1000 | Loss: 0.00001922
Iteration 64/1000 | Loss: 0.00001922
Iteration 65/1000 | Loss: 0.00001922
Iteration 66/1000 | Loss: 0.00001922
Iteration 67/1000 | Loss: 0.00001922
Iteration 68/1000 | Loss: 0.00001922
Iteration 69/1000 | Loss: 0.00001922
Iteration 70/1000 | Loss: 0.00001922
Iteration 71/1000 | Loss: 0.00001922
Iteration 72/1000 | Loss: 0.00001922
Iteration 73/1000 | Loss: 0.00001922
Iteration 74/1000 | Loss: 0.00001921
Iteration 75/1000 | Loss: 0.00001921
Iteration 76/1000 | Loss: 0.00001921
Iteration 77/1000 | Loss: 0.00001921
Iteration 78/1000 | Loss: 0.00001920
Iteration 79/1000 | Loss: 0.00001920
Iteration 80/1000 | Loss: 0.00001920
Iteration 81/1000 | Loss: 0.00001920
Iteration 82/1000 | Loss: 0.00001920
Iteration 83/1000 | Loss: 0.00001919
Iteration 84/1000 | Loss: 0.00001919
Iteration 85/1000 | Loss: 0.00001919
Iteration 86/1000 | Loss: 0.00001919
Iteration 87/1000 | Loss: 0.00001919
Iteration 88/1000 | Loss: 0.00001919
Iteration 89/1000 | Loss: 0.00001919
Iteration 90/1000 | Loss: 0.00001919
Iteration 91/1000 | Loss: 0.00001919
Iteration 92/1000 | Loss: 0.00001919
Iteration 93/1000 | Loss: 0.00001919
Iteration 94/1000 | Loss: 0.00001918
Iteration 95/1000 | Loss: 0.00001918
Iteration 96/1000 | Loss: 0.00001917
Iteration 97/1000 | Loss: 0.00001917
Iteration 98/1000 | Loss: 0.00001917
Iteration 99/1000 | Loss: 0.00001917
Iteration 100/1000 | Loss: 0.00001917
Iteration 101/1000 | Loss: 0.00001917
Iteration 102/1000 | Loss: 0.00001916
Iteration 103/1000 | Loss: 0.00001916
Iteration 104/1000 | Loss: 0.00001916
Iteration 105/1000 | Loss: 0.00001916
Iteration 106/1000 | Loss: 0.00001916
Iteration 107/1000 | Loss: 0.00001915
Iteration 108/1000 | Loss: 0.00001915
Iteration 109/1000 | Loss: 0.00001915
Iteration 110/1000 | Loss: 0.00001915
Iteration 111/1000 | Loss: 0.00001915
Iteration 112/1000 | Loss: 0.00001915
Iteration 113/1000 | Loss: 0.00001915
Iteration 114/1000 | Loss: 0.00001915
Iteration 115/1000 | Loss: 0.00001915
Iteration 116/1000 | Loss: 0.00001915
Iteration 117/1000 | Loss: 0.00001915
Iteration 118/1000 | Loss: 0.00001915
Iteration 119/1000 | Loss: 0.00001915
Iteration 120/1000 | Loss: 0.00001915
Iteration 121/1000 | Loss: 0.00001915
Iteration 122/1000 | Loss: 0.00001915
Iteration 123/1000 | Loss: 0.00001915
Iteration 124/1000 | Loss: 0.00001915
Iteration 125/1000 | Loss: 0.00001915
Iteration 126/1000 | Loss: 0.00001915
Iteration 127/1000 | Loss: 0.00001915
Iteration 128/1000 | Loss: 0.00001915
Iteration 129/1000 | Loss: 0.00001915
Iteration 130/1000 | Loss: 0.00001915
Iteration 131/1000 | Loss: 0.00001915
Iteration 132/1000 | Loss: 0.00001915
Iteration 133/1000 | Loss: 0.00001915
Iteration 134/1000 | Loss: 0.00001915
Iteration 135/1000 | Loss: 0.00001915
Iteration 136/1000 | Loss: 0.00001915
Iteration 137/1000 | Loss: 0.00001915
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 137. Stopping optimization.
Last 5 losses: [1.9145902115269564e-05, 1.9145902115269564e-05, 1.9145902115269564e-05, 1.9145902115269564e-05, 1.9145902115269564e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9145902115269564e-05

Optimization complete. Final v2v error: 3.7310409545898438 mm

Highest mean error: 3.921492099761963 mm for frame 25

Lowest mean error: 3.5994181632995605 mm for frame 17

Saving results

Total time: 34.06156635284424
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_36_nl_6641/0021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_nl_6641/0021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_nl_6641/0021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00388631
Iteration 2/25 | Loss: 0.00131180
Iteration 3/25 | Loss: 0.00121805
Iteration 4/25 | Loss: 0.00120319
Iteration 5/25 | Loss: 0.00119691
Iteration 6/25 | Loss: 0.00119563
Iteration 7/25 | Loss: 0.00119563
Iteration 8/25 | Loss: 0.00119563
Iteration 9/25 | Loss: 0.00119563
Iteration 10/25 | Loss: 0.00119563
Iteration 11/25 | Loss: 0.00119561
Iteration 12/25 | Loss: 0.00119561
Iteration 13/25 | Loss: 0.00119561
Iteration 14/25 | Loss: 0.00119561
Iteration 15/25 | Loss: 0.00119561
Iteration 16/25 | Loss: 0.00119561
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.001195611315779388, 0.001195611315779388, 0.001195611315779388, 0.001195611315779388, 0.001195611315779388]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001195611315779388

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37637866
Iteration 2/25 | Loss: 0.00051091
Iteration 3/25 | Loss: 0.00051091
Iteration 4/25 | Loss: 0.00051091
Iteration 5/25 | Loss: 0.00051091
Iteration 6/25 | Loss: 0.00051091
Iteration 7/25 | Loss: 0.00051091
Iteration 8/25 | Loss: 0.00051091
Iteration 9/25 | Loss: 0.00051091
Iteration 10/25 | Loss: 0.00051091
Iteration 11/25 | Loss: 0.00051091
Iteration 12/25 | Loss: 0.00051091
Iteration 13/25 | Loss: 0.00051091
Iteration 14/25 | Loss: 0.00051091
Iteration 15/25 | Loss: 0.00051091
Iteration 16/25 | Loss: 0.00051091
Iteration 17/25 | Loss: 0.00051091
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0005109071498736739, 0.0005109071498736739, 0.0005109071498736739, 0.0005109071498736739, 0.0005109071498736739]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005109071498736739

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00051091
Iteration 2/1000 | Loss: 0.00005469
Iteration 3/1000 | Loss: 0.00003275
Iteration 4/1000 | Loss: 0.00002722
Iteration 5/1000 | Loss: 0.00002436
Iteration 6/1000 | Loss: 0.00002359
Iteration 7/1000 | Loss: 0.00002281
Iteration 8/1000 | Loss: 0.00002257
Iteration 9/1000 | Loss: 0.00002230
Iteration 10/1000 | Loss: 0.00002203
Iteration 11/1000 | Loss: 0.00002202
Iteration 12/1000 | Loss: 0.00002195
Iteration 13/1000 | Loss: 0.00002194
Iteration 14/1000 | Loss: 0.00002192
Iteration 15/1000 | Loss: 0.00002191
Iteration 16/1000 | Loss: 0.00002185
Iteration 17/1000 | Loss: 0.00002183
Iteration 18/1000 | Loss: 0.00002182
Iteration 19/1000 | Loss: 0.00002182
Iteration 20/1000 | Loss: 0.00002182
Iteration 21/1000 | Loss: 0.00002182
Iteration 22/1000 | Loss: 0.00002181
Iteration 23/1000 | Loss: 0.00002180
Iteration 24/1000 | Loss: 0.00002180
Iteration 25/1000 | Loss: 0.00002180
Iteration 26/1000 | Loss: 0.00002180
Iteration 27/1000 | Loss: 0.00002179
Iteration 28/1000 | Loss: 0.00002179
Iteration 29/1000 | Loss: 0.00002178
Iteration 30/1000 | Loss: 0.00002178
Iteration 31/1000 | Loss: 0.00002177
Iteration 32/1000 | Loss: 0.00002177
Iteration 33/1000 | Loss: 0.00002177
Iteration 34/1000 | Loss: 0.00002177
Iteration 35/1000 | Loss: 0.00002177
Iteration 36/1000 | Loss: 0.00002177
Iteration 37/1000 | Loss: 0.00002176
Iteration 38/1000 | Loss: 0.00002176
Iteration 39/1000 | Loss: 0.00002176
Iteration 40/1000 | Loss: 0.00002176
Iteration 41/1000 | Loss: 0.00002176
Iteration 42/1000 | Loss: 0.00002176
Iteration 43/1000 | Loss: 0.00002176
Iteration 44/1000 | Loss: 0.00002176
Iteration 45/1000 | Loss: 0.00002176
Iteration 46/1000 | Loss: 0.00002176
Iteration 47/1000 | Loss: 0.00002176
Iteration 48/1000 | Loss: 0.00002176
Iteration 49/1000 | Loss: 0.00002176
Iteration 50/1000 | Loss: 0.00002176
Iteration 51/1000 | Loss: 0.00002176
Iteration 52/1000 | Loss: 0.00002176
Iteration 53/1000 | Loss: 0.00002176
Iteration 54/1000 | Loss: 0.00002176
Iteration 55/1000 | Loss: 0.00002176
Iteration 56/1000 | Loss: 0.00002176
Iteration 57/1000 | Loss: 0.00002176
Iteration 58/1000 | Loss: 0.00002176
Iteration 59/1000 | Loss: 0.00002176
Iteration 60/1000 | Loss: 0.00002176
Iteration 61/1000 | Loss: 0.00002176
Iteration 62/1000 | Loss: 0.00002176
Iteration 63/1000 | Loss: 0.00002176
Iteration 64/1000 | Loss: 0.00002176
Iteration 65/1000 | Loss: 0.00002176
Iteration 66/1000 | Loss: 0.00002176
Iteration 67/1000 | Loss: 0.00002176
Iteration 68/1000 | Loss: 0.00002176
Iteration 69/1000 | Loss: 0.00002176
Iteration 70/1000 | Loss: 0.00002176
Iteration 71/1000 | Loss: 0.00002176
Iteration 72/1000 | Loss: 0.00002176
Iteration 73/1000 | Loss: 0.00002176
Iteration 74/1000 | Loss: 0.00002176
Iteration 75/1000 | Loss: 0.00002176
Iteration 76/1000 | Loss: 0.00002176
Iteration 77/1000 | Loss: 0.00002176
Iteration 78/1000 | Loss: 0.00002176
Iteration 79/1000 | Loss: 0.00002176
Iteration 80/1000 | Loss: 0.00002176
Iteration 81/1000 | Loss: 0.00002176
Iteration 82/1000 | Loss: 0.00002176
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 82. Stopping optimization.
Last 5 losses: [2.175561894546263e-05, 2.175561894546263e-05, 2.175561894546263e-05, 2.175561894546263e-05, 2.175561894546263e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.175561894546263e-05

Optimization complete. Final v2v error: 4.041925430297852 mm

Highest mean error: 4.234156608581543 mm for frame 37

Lowest mean error: 3.8903231620788574 mm for frame 103

Saving results

Total time: 27.109752655029297
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_36_nl_6641/0013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_nl_6641/0013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_nl_6641/0013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00911056
Iteration 2/25 | Loss: 0.00134152
Iteration 3/25 | Loss: 0.00120958
Iteration 4/25 | Loss: 0.00119435
Iteration 5/25 | Loss: 0.00118865
Iteration 6/25 | Loss: 0.00118733
Iteration 7/25 | Loss: 0.00118713
Iteration 8/25 | Loss: 0.00118713
Iteration 9/25 | Loss: 0.00118713
Iteration 10/25 | Loss: 0.00118713
Iteration 11/25 | Loss: 0.00118713
Iteration 12/25 | Loss: 0.00118713
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0011871258029714227, 0.0011871258029714227, 0.0011871258029714227, 0.0011871258029714227, 0.0011871258029714227]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011871258029714227

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.19831657
Iteration 2/25 | Loss: 0.00049121
Iteration 3/25 | Loss: 0.00049121
Iteration 4/25 | Loss: 0.00049121
Iteration 5/25 | Loss: 0.00049121
Iteration 6/25 | Loss: 0.00049121
Iteration 7/25 | Loss: 0.00049121
Iteration 8/25 | Loss: 0.00049121
Iteration 9/25 | Loss: 0.00049121
Iteration 10/25 | Loss: 0.00049121
Iteration 11/25 | Loss: 0.00049121
Iteration 12/25 | Loss: 0.00049121
Iteration 13/25 | Loss: 0.00049121
Iteration 14/25 | Loss: 0.00049121
Iteration 15/25 | Loss: 0.00049121
Iteration 16/25 | Loss: 0.00049121
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0004912052536383271, 0.0004912052536383271, 0.0004912052536383271, 0.0004912052536383271, 0.0004912052536383271]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0004912052536383271

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00049121
Iteration 2/1000 | Loss: 0.00005360
Iteration 3/1000 | Loss: 0.00003286
Iteration 4/1000 | Loss: 0.00002782
Iteration 5/1000 | Loss: 0.00002481
Iteration 6/1000 | Loss: 0.00002361
Iteration 7/1000 | Loss: 0.00002292
Iteration 8/1000 | Loss: 0.00002260
Iteration 9/1000 | Loss: 0.00002227
Iteration 10/1000 | Loss: 0.00002200
Iteration 11/1000 | Loss: 0.00002195
Iteration 12/1000 | Loss: 0.00002194
Iteration 13/1000 | Loss: 0.00002193
Iteration 14/1000 | Loss: 0.00002193
Iteration 15/1000 | Loss: 0.00002193
Iteration 16/1000 | Loss: 0.00002192
Iteration 17/1000 | Loss: 0.00002189
Iteration 18/1000 | Loss: 0.00002188
Iteration 19/1000 | Loss: 0.00002188
Iteration 20/1000 | Loss: 0.00002188
Iteration 21/1000 | Loss: 0.00002187
Iteration 22/1000 | Loss: 0.00002187
Iteration 23/1000 | Loss: 0.00002187
Iteration 24/1000 | Loss: 0.00002181
Iteration 25/1000 | Loss: 0.00002180
Iteration 26/1000 | Loss: 0.00002179
Iteration 27/1000 | Loss: 0.00002178
Iteration 28/1000 | Loss: 0.00002176
Iteration 29/1000 | Loss: 0.00002176
Iteration 30/1000 | Loss: 0.00002175
Iteration 31/1000 | Loss: 0.00002175
Iteration 32/1000 | Loss: 0.00002174
Iteration 33/1000 | Loss: 0.00002174
Iteration 34/1000 | Loss: 0.00002174
Iteration 35/1000 | Loss: 0.00002173
Iteration 36/1000 | Loss: 0.00002173
Iteration 37/1000 | Loss: 0.00002172
Iteration 38/1000 | Loss: 0.00002171
Iteration 39/1000 | Loss: 0.00002171
Iteration 40/1000 | Loss: 0.00002171
Iteration 41/1000 | Loss: 0.00002171
Iteration 42/1000 | Loss: 0.00002171
Iteration 43/1000 | Loss: 0.00002171
Iteration 44/1000 | Loss: 0.00002171
Iteration 45/1000 | Loss: 0.00002170
Iteration 46/1000 | Loss: 0.00002170
Iteration 47/1000 | Loss: 0.00002170
Iteration 48/1000 | Loss: 0.00002169
Iteration 49/1000 | Loss: 0.00002169
Iteration 50/1000 | Loss: 0.00002169
Iteration 51/1000 | Loss: 0.00002169
Iteration 52/1000 | Loss: 0.00002169
Iteration 53/1000 | Loss: 0.00002168
Iteration 54/1000 | Loss: 0.00002168
Iteration 55/1000 | Loss: 0.00002168
Iteration 56/1000 | Loss: 0.00002168
Iteration 57/1000 | Loss: 0.00002168
Iteration 58/1000 | Loss: 0.00002168
Iteration 59/1000 | Loss: 0.00002168
Iteration 60/1000 | Loss: 0.00002167
Iteration 61/1000 | Loss: 0.00002167
Iteration 62/1000 | Loss: 0.00002167
Iteration 63/1000 | Loss: 0.00002167
Iteration 64/1000 | Loss: 0.00002167
Iteration 65/1000 | Loss: 0.00002167
Iteration 66/1000 | Loss: 0.00002167
Iteration 67/1000 | Loss: 0.00002167
Iteration 68/1000 | Loss: 0.00002167
Iteration 69/1000 | Loss: 0.00002167
Iteration 70/1000 | Loss: 0.00002166
Iteration 71/1000 | Loss: 0.00002166
Iteration 72/1000 | Loss: 0.00002166
Iteration 73/1000 | Loss: 0.00002166
Iteration 74/1000 | Loss: 0.00002165
Iteration 75/1000 | Loss: 0.00002165
Iteration 76/1000 | Loss: 0.00002165
Iteration 77/1000 | Loss: 0.00002165
Iteration 78/1000 | Loss: 0.00002165
Iteration 79/1000 | Loss: 0.00002165
Iteration 80/1000 | Loss: 0.00002165
Iteration 81/1000 | Loss: 0.00002165
Iteration 82/1000 | Loss: 0.00002165
Iteration 83/1000 | Loss: 0.00002165
Iteration 84/1000 | Loss: 0.00002165
Iteration 85/1000 | Loss: 0.00002165
Iteration 86/1000 | Loss: 0.00002165
Iteration 87/1000 | Loss: 0.00002165
Iteration 88/1000 | Loss: 0.00002165
Iteration 89/1000 | Loss: 0.00002165
Iteration 90/1000 | Loss: 0.00002165
Iteration 91/1000 | Loss: 0.00002165
Iteration 92/1000 | Loss: 0.00002165
Iteration 93/1000 | Loss: 0.00002164
Iteration 94/1000 | Loss: 0.00002164
Iteration 95/1000 | Loss: 0.00002164
Iteration 96/1000 | Loss: 0.00002164
Iteration 97/1000 | Loss: 0.00002164
Iteration 98/1000 | Loss: 0.00002164
Iteration 99/1000 | Loss: 0.00002164
Iteration 100/1000 | Loss: 0.00002164
Iteration 101/1000 | Loss: 0.00002164
Iteration 102/1000 | Loss: 0.00002164
Iteration 103/1000 | Loss: 0.00002164
Iteration 104/1000 | Loss: 0.00002164
Iteration 105/1000 | Loss: 0.00002164
Iteration 106/1000 | Loss: 0.00002164
Iteration 107/1000 | Loss: 0.00002163
Iteration 108/1000 | Loss: 0.00002163
Iteration 109/1000 | Loss: 0.00002163
Iteration 110/1000 | Loss: 0.00002163
Iteration 111/1000 | Loss: 0.00002163
Iteration 112/1000 | Loss: 0.00002163
Iteration 113/1000 | Loss: 0.00002163
Iteration 114/1000 | Loss: 0.00002163
Iteration 115/1000 | Loss: 0.00002163
Iteration 116/1000 | Loss: 0.00002163
Iteration 117/1000 | Loss: 0.00002163
Iteration 118/1000 | Loss: 0.00002163
Iteration 119/1000 | Loss: 0.00002163
Iteration 120/1000 | Loss: 0.00002163
Iteration 121/1000 | Loss: 0.00002163
Iteration 122/1000 | Loss: 0.00002163
Iteration 123/1000 | Loss: 0.00002163
Iteration 124/1000 | Loss: 0.00002163
Iteration 125/1000 | Loss: 0.00002163
Iteration 126/1000 | Loss: 0.00002163
Iteration 127/1000 | Loss: 0.00002163
Iteration 128/1000 | Loss: 0.00002163
Iteration 129/1000 | Loss: 0.00002162
Iteration 130/1000 | Loss: 0.00002162
Iteration 131/1000 | Loss: 0.00002162
Iteration 132/1000 | Loss: 0.00002162
Iteration 133/1000 | Loss: 0.00002162
Iteration 134/1000 | Loss: 0.00002162
Iteration 135/1000 | Loss: 0.00002162
Iteration 136/1000 | Loss: 0.00002162
Iteration 137/1000 | Loss: 0.00002162
Iteration 138/1000 | Loss: 0.00002162
Iteration 139/1000 | Loss: 0.00002162
Iteration 140/1000 | Loss: 0.00002162
Iteration 141/1000 | Loss: 0.00002162
Iteration 142/1000 | Loss: 0.00002162
Iteration 143/1000 | Loss: 0.00002162
Iteration 144/1000 | Loss: 0.00002162
Iteration 145/1000 | Loss: 0.00002162
Iteration 146/1000 | Loss: 0.00002162
Iteration 147/1000 | Loss: 0.00002162
Iteration 148/1000 | Loss: 0.00002162
Iteration 149/1000 | Loss: 0.00002162
Iteration 150/1000 | Loss: 0.00002162
Iteration 151/1000 | Loss: 0.00002162
Iteration 152/1000 | Loss: 0.00002162
Iteration 153/1000 | Loss: 0.00002162
Iteration 154/1000 | Loss: 0.00002162
Iteration 155/1000 | Loss: 0.00002162
Iteration 156/1000 | Loss: 0.00002162
Iteration 157/1000 | Loss: 0.00002162
Iteration 158/1000 | Loss: 0.00002162
Iteration 159/1000 | Loss: 0.00002162
Iteration 160/1000 | Loss: 0.00002162
Iteration 161/1000 | Loss: 0.00002162
Iteration 162/1000 | Loss: 0.00002162
Iteration 163/1000 | Loss: 0.00002162
Iteration 164/1000 | Loss: 0.00002162
Iteration 165/1000 | Loss: 0.00002162
Iteration 166/1000 | Loss: 0.00002162
Iteration 167/1000 | Loss: 0.00002162
Iteration 168/1000 | Loss: 0.00002162
Iteration 169/1000 | Loss: 0.00002162
Iteration 170/1000 | Loss: 0.00002162
Iteration 171/1000 | Loss: 0.00002162
Iteration 172/1000 | Loss: 0.00002162
Iteration 173/1000 | Loss: 0.00002162
Iteration 174/1000 | Loss: 0.00002162
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 174. Stopping optimization.
Last 5 losses: [2.1622907297569327e-05, 2.1622907297569327e-05, 2.1622907297569327e-05, 2.1622907297569327e-05, 2.1622907297569327e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1622907297569327e-05

Optimization complete. Final v2v error: 3.905177593231201 mm

Highest mean error: 4.615351676940918 mm for frame 88

Lowest mean error: 3.515453815460205 mm for frame 14

Saving results

Total time: 33.18684267997742
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_36_nl_6641/0016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_nl_6641/0016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_nl_6641/0016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00877836
Iteration 2/25 | Loss: 0.00131511
Iteration 3/25 | Loss: 0.00117910
Iteration 4/25 | Loss: 0.00116929
Iteration 5/25 | Loss: 0.00116739
Iteration 6/25 | Loss: 0.00116739
Iteration 7/25 | Loss: 0.00116739
Iteration 8/25 | Loss: 0.00116739
Iteration 9/25 | Loss: 0.00116739
Iteration 10/25 | Loss: 0.00116739
Iteration 11/25 | Loss: 0.00116739
Iteration 12/25 | Loss: 0.00116739
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0011673921253532171, 0.0011673921253532171, 0.0011673921253532171, 0.0011673921253532171, 0.0011673921253532171]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011673921253532171

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38394630
Iteration 2/25 | Loss: 0.00049600
Iteration 3/25 | Loss: 0.00049600
Iteration 4/25 | Loss: 0.00049600
Iteration 5/25 | Loss: 0.00049600
Iteration 6/25 | Loss: 0.00049600
Iteration 7/25 | Loss: 0.00049600
Iteration 8/25 | Loss: 0.00049600
Iteration 9/25 | Loss: 0.00049600
Iteration 10/25 | Loss: 0.00049600
Iteration 11/25 | Loss: 0.00049600
Iteration 12/25 | Loss: 0.00049600
Iteration 13/25 | Loss: 0.00049600
Iteration 14/25 | Loss: 0.00049600
Iteration 15/25 | Loss: 0.00049600
Iteration 16/25 | Loss: 0.00049600
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.000496000109706074, 0.000496000109706074, 0.000496000109706074, 0.000496000109706074, 0.000496000109706074]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000496000109706074

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00049600
Iteration 2/1000 | Loss: 0.00005061
Iteration 3/1000 | Loss: 0.00002986
Iteration 4/1000 | Loss: 0.00002420
Iteration 5/1000 | Loss: 0.00002151
Iteration 6/1000 | Loss: 0.00002007
Iteration 7/1000 | Loss: 0.00001944
Iteration 8/1000 | Loss: 0.00001897
Iteration 9/1000 | Loss: 0.00001849
Iteration 10/1000 | Loss: 0.00001827
Iteration 11/1000 | Loss: 0.00001820
Iteration 12/1000 | Loss: 0.00001816
Iteration 13/1000 | Loss: 0.00001816
Iteration 14/1000 | Loss: 0.00001816
Iteration 15/1000 | Loss: 0.00001812
Iteration 16/1000 | Loss: 0.00001812
Iteration 17/1000 | Loss: 0.00001812
Iteration 18/1000 | Loss: 0.00001812
Iteration 19/1000 | Loss: 0.00001812
Iteration 20/1000 | Loss: 0.00001812
Iteration 21/1000 | Loss: 0.00001812
Iteration 22/1000 | Loss: 0.00001812
Iteration 23/1000 | Loss: 0.00001811
Iteration 24/1000 | Loss: 0.00001811
Iteration 25/1000 | Loss: 0.00001811
Iteration 26/1000 | Loss: 0.00001811
Iteration 27/1000 | Loss: 0.00001811
Iteration 28/1000 | Loss: 0.00001811
Iteration 29/1000 | Loss: 0.00001811
Iteration 30/1000 | Loss: 0.00001810
Iteration 31/1000 | Loss: 0.00001809
Iteration 32/1000 | Loss: 0.00001809
Iteration 33/1000 | Loss: 0.00001809
Iteration 34/1000 | Loss: 0.00001809
Iteration 35/1000 | Loss: 0.00001808
Iteration 36/1000 | Loss: 0.00001808
Iteration 37/1000 | Loss: 0.00001808
Iteration 38/1000 | Loss: 0.00001808
Iteration 39/1000 | Loss: 0.00001808
Iteration 40/1000 | Loss: 0.00001808
Iteration 41/1000 | Loss: 0.00001808
Iteration 42/1000 | Loss: 0.00001807
Iteration 43/1000 | Loss: 0.00001807
Iteration 44/1000 | Loss: 0.00001807
Iteration 45/1000 | Loss: 0.00001807
Iteration 46/1000 | Loss: 0.00001807
Iteration 47/1000 | Loss: 0.00001807
Iteration 48/1000 | Loss: 0.00001807
Iteration 49/1000 | Loss: 0.00001807
Iteration 50/1000 | Loss: 0.00001806
Iteration 51/1000 | Loss: 0.00001806
Iteration 52/1000 | Loss: 0.00001806
Iteration 53/1000 | Loss: 0.00001806
Iteration 54/1000 | Loss: 0.00001806
Iteration 55/1000 | Loss: 0.00001806
Iteration 56/1000 | Loss: 0.00001805
Iteration 57/1000 | Loss: 0.00001805
Iteration 58/1000 | Loss: 0.00001804
Iteration 59/1000 | Loss: 0.00001804
Iteration 60/1000 | Loss: 0.00001804
Iteration 61/1000 | Loss: 0.00001804
Iteration 62/1000 | Loss: 0.00001804
Iteration 63/1000 | Loss: 0.00001804
Iteration 64/1000 | Loss: 0.00001804
Iteration 65/1000 | Loss: 0.00001804
Iteration 66/1000 | Loss: 0.00001804
Iteration 67/1000 | Loss: 0.00001804
Iteration 68/1000 | Loss: 0.00001804
Iteration 69/1000 | Loss: 0.00001804
Iteration 70/1000 | Loss: 0.00001804
Iteration 71/1000 | Loss: 0.00001804
Iteration 72/1000 | Loss: 0.00001804
Iteration 73/1000 | Loss: 0.00001804
Iteration 74/1000 | Loss: 0.00001804
Iteration 75/1000 | Loss: 0.00001804
Iteration 76/1000 | Loss: 0.00001804
Iteration 77/1000 | Loss: 0.00001804
Iteration 78/1000 | Loss: 0.00001804
Iteration 79/1000 | Loss: 0.00001804
Iteration 80/1000 | Loss: 0.00001804
Iteration 81/1000 | Loss: 0.00001804
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 81. Stopping optimization.
Last 5 losses: [1.8038628695649095e-05, 1.8038628695649095e-05, 1.8038628695649095e-05, 1.8038628695649095e-05, 1.8038628695649095e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8038628695649095e-05

Optimization complete. Final v2v error: 3.6422417163848877 mm

Highest mean error: 3.760118246078491 mm for frame 141

Lowest mean error: 3.5126945972442627 mm for frame 85

Saving results

Total time: 27.153571605682373
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_36_nl_6641/0001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_nl_6641/0001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_nl_6641/0001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00851969
Iteration 2/25 | Loss: 0.00148047
Iteration 3/25 | Loss: 0.00121896
Iteration 4/25 | Loss: 0.00118077
Iteration 5/25 | Loss: 0.00117718
Iteration 6/25 | Loss: 0.00117659
Iteration 7/25 | Loss: 0.00117659
Iteration 8/25 | Loss: 0.00117659
Iteration 9/25 | Loss: 0.00117659
Iteration 10/25 | Loss: 0.00117659
Iteration 11/25 | Loss: 0.00117659
Iteration 12/25 | Loss: 0.00117659
Iteration 13/25 | Loss: 0.00117659
Iteration 14/25 | Loss: 0.00117659
Iteration 15/25 | Loss: 0.00117659
Iteration 16/25 | Loss: 0.00117659
Iteration 17/25 | Loss: 0.00117659
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.001176589634269476, 0.001176589634269476, 0.001176589634269476, 0.001176589634269476, 0.001176589634269476]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001176589634269476

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36933851
Iteration 2/25 | Loss: 0.00053583
Iteration 3/25 | Loss: 0.00053582
Iteration 4/25 | Loss: 0.00053581
Iteration 5/25 | Loss: 0.00053581
Iteration 6/25 | Loss: 0.00053581
Iteration 7/25 | Loss: 0.00053581
Iteration 8/25 | Loss: 0.00053581
Iteration 9/25 | Loss: 0.00053581
Iteration 10/25 | Loss: 0.00053581
Iteration 11/25 | Loss: 0.00053581
Iteration 12/25 | Loss: 0.00053581
Iteration 13/25 | Loss: 0.00053581
Iteration 14/25 | Loss: 0.00053581
Iteration 15/25 | Loss: 0.00053581
Iteration 16/25 | Loss: 0.00053581
Iteration 17/25 | Loss: 0.00053581
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0005358125199563801, 0.0005358125199563801, 0.0005358125199563801, 0.0005358125199563801, 0.0005358125199563801]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005358125199563801

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00053581
Iteration 2/1000 | Loss: 0.00005886
Iteration 3/1000 | Loss: 0.00003245
Iteration 4/1000 | Loss: 0.00002717
Iteration 5/1000 | Loss: 0.00002470
Iteration 6/1000 | Loss: 0.00002322
Iteration 7/1000 | Loss: 0.00002257
Iteration 8/1000 | Loss: 0.00002212
Iteration 9/1000 | Loss: 0.00002174
Iteration 10/1000 | Loss: 0.00002144
Iteration 11/1000 | Loss: 0.00002122
Iteration 12/1000 | Loss: 0.00002114
Iteration 13/1000 | Loss: 0.00002109
Iteration 14/1000 | Loss: 0.00002108
Iteration 15/1000 | Loss: 0.00002108
Iteration 16/1000 | Loss: 0.00002108
Iteration 17/1000 | Loss: 0.00002107
Iteration 18/1000 | Loss: 0.00002107
Iteration 19/1000 | Loss: 0.00002106
Iteration 20/1000 | Loss: 0.00002106
Iteration 21/1000 | Loss: 0.00002106
Iteration 22/1000 | Loss: 0.00002106
Iteration 23/1000 | Loss: 0.00002106
Iteration 24/1000 | Loss: 0.00002105
Iteration 25/1000 | Loss: 0.00002105
Iteration 26/1000 | Loss: 0.00002105
Iteration 27/1000 | Loss: 0.00002104
Iteration 28/1000 | Loss: 0.00002104
Iteration 29/1000 | Loss: 0.00002103
Iteration 30/1000 | Loss: 0.00002103
Iteration 31/1000 | Loss: 0.00002103
Iteration 32/1000 | Loss: 0.00002103
Iteration 33/1000 | Loss: 0.00002102
Iteration 34/1000 | Loss: 0.00002102
Iteration 35/1000 | Loss: 0.00002101
Iteration 36/1000 | Loss: 0.00002101
Iteration 37/1000 | Loss: 0.00002101
Iteration 38/1000 | Loss: 0.00002101
Iteration 39/1000 | Loss: 0.00002100
Iteration 40/1000 | Loss: 0.00002100
Iteration 41/1000 | Loss: 0.00002100
Iteration 42/1000 | Loss: 0.00002100
Iteration 43/1000 | Loss: 0.00002099
Iteration 44/1000 | Loss: 0.00002099
Iteration 45/1000 | Loss: 0.00002099
Iteration 46/1000 | Loss: 0.00002098
Iteration 47/1000 | Loss: 0.00002098
Iteration 48/1000 | Loss: 0.00002098
Iteration 49/1000 | Loss: 0.00002098
Iteration 50/1000 | Loss: 0.00002098
Iteration 51/1000 | Loss: 0.00002098
Iteration 52/1000 | Loss: 0.00002098
Iteration 53/1000 | Loss: 0.00002098
Iteration 54/1000 | Loss: 0.00002098
Iteration 55/1000 | Loss: 0.00002098
Iteration 56/1000 | Loss: 0.00002097
Iteration 57/1000 | Loss: 0.00002097
Iteration 58/1000 | Loss: 0.00002097
Iteration 59/1000 | Loss: 0.00002097
Iteration 60/1000 | Loss: 0.00002097
Iteration 61/1000 | Loss: 0.00002097
Iteration 62/1000 | Loss: 0.00002097
Iteration 63/1000 | Loss: 0.00002097
Iteration 64/1000 | Loss: 0.00002096
Iteration 65/1000 | Loss: 0.00002096
Iteration 66/1000 | Loss: 0.00002096
Iteration 67/1000 | Loss: 0.00002096
Iteration 68/1000 | Loss: 0.00002096
Iteration 69/1000 | Loss: 0.00002096
Iteration 70/1000 | Loss: 0.00002096
Iteration 71/1000 | Loss: 0.00002096
Iteration 72/1000 | Loss: 0.00002096
Iteration 73/1000 | Loss: 0.00002096
Iteration 74/1000 | Loss: 0.00002096
Iteration 75/1000 | Loss: 0.00002096
Iteration 76/1000 | Loss: 0.00002096
Iteration 77/1000 | Loss: 0.00002096
Iteration 78/1000 | Loss: 0.00002096
Iteration 79/1000 | Loss: 0.00002096
Iteration 80/1000 | Loss: 0.00002096
Iteration 81/1000 | Loss: 0.00002096
Iteration 82/1000 | Loss: 0.00002096
Iteration 83/1000 | Loss: 0.00002096
Iteration 84/1000 | Loss: 0.00002096
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 84. Stopping optimization.
Last 5 losses: [2.0958046661689878e-05, 2.0958046661689878e-05, 2.0958046661689878e-05, 2.0958046661689878e-05, 2.0958046661689878e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0958046661689878e-05

Optimization complete. Final v2v error: 3.8585047721862793 mm

Highest mean error: 4.378409385681152 mm for frame 234

Lowest mean error: 3.4726827144622803 mm for frame 19

Saving results

Total time: 33.40288996696472
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_36_nl_6641/0017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_nl_6641/0017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_nl_6641/0017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00439180
Iteration 2/25 | Loss: 0.00137392
Iteration 3/25 | Loss: 0.00124587
Iteration 4/25 | Loss: 0.00122422
Iteration 5/25 | Loss: 0.00121707
Iteration 6/25 | Loss: 0.00121569
Iteration 7/25 | Loss: 0.00121539
Iteration 8/25 | Loss: 0.00121539
Iteration 9/25 | Loss: 0.00121539
Iteration 10/25 | Loss: 0.00121539
Iteration 11/25 | Loss: 0.00121539
Iteration 12/25 | Loss: 0.00121539
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0012153915595263243, 0.0012153915595263243, 0.0012153915595263243, 0.0012153915595263243, 0.0012153915595263243]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012153915595263243

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.00482631
Iteration 2/25 | Loss: 0.00049248
Iteration 3/25 | Loss: 0.00049248
Iteration 4/25 | Loss: 0.00049248
Iteration 5/25 | Loss: 0.00049248
Iteration 6/25 | Loss: 0.00049248
Iteration 7/25 | Loss: 0.00049248
Iteration 8/25 | Loss: 0.00049248
Iteration 9/25 | Loss: 0.00049248
Iteration 10/25 | Loss: 0.00049248
Iteration 11/25 | Loss: 0.00049248
Iteration 12/25 | Loss: 0.00049248
Iteration 13/25 | Loss: 0.00049248
Iteration 14/25 | Loss: 0.00049248
Iteration 15/25 | Loss: 0.00049248
Iteration 16/25 | Loss: 0.00049248
Iteration 17/25 | Loss: 0.00049248
Iteration 18/25 | Loss: 0.00049248
Iteration 19/25 | Loss: 0.00049248
Iteration 20/25 | Loss: 0.00049248
Iteration 21/25 | Loss: 0.00049248
Iteration 22/25 | Loss: 0.00049248
Iteration 23/25 | Loss: 0.00049248
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0004924761014990509, 0.0004924761014990509, 0.0004924761014990509, 0.0004924761014990509, 0.0004924761014990509]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0004924761014990509

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00049248
Iteration 2/1000 | Loss: 0.00007245
Iteration 3/1000 | Loss: 0.00004759
Iteration 4/1000 | Loss: 0.00003676
Iteration 5/1000 | Loss: 0.00003284
Iteration 6/1000 | Loss: 0.00003143
Iteration 7/1000 | Loss: 0.00003049
Iteration 8/1000 | Loss: 0.00002997
Iteration 9/1000 | Loss: 0.00002950
Iteration 10/1000 | Loss: 0.00002916
Iteration 11/1000 | Loss: 0.00002902
Iteration 12/1000 | Loss: 0.00002899
Iteration 13/1000 | Loss: 0.00002892
Iteration 14/1000 | Loss: 0.00002888
Iteration 15/1000 | Loss: 0.00002888
Iteration 16/1000 | Loss: 0.00002888
Iteration 17/1000 | Loss: 0.00002887
Iteration 18/1000 | Loss: 0.00002887
Iteration 19/1000 | Loss: 0.00002883
Iteration 20/1000 | Loss: 0.00002881
Iteration 21/1000 | Loss: 0.00002880
Iteration 22/1000 | Loss: 0.00002879
Iteration 23/1000 | Loss: 0.00002879
Iteration 24/1000 | Loss: 0.00002879
Iteration 25/1000 | Loss: 0.00002879
Iteration 26/1000 | Loss: 0.00002879
Iteration 27/1000 | Loss: 0.00002879
Iteration 28/1000 | Loss: 0.00002878
Iteration 29/1000 | Loss: 0.00002878
Iteration 30/1000 | Loss: 0.00002878
Iteration 31/1000 | Loss: 0.00002878
Iteration 32/1000 | Loss: 0.00002878
Iteration 33/1000 | Loss: 0.00002878
Iteration 34/1000 | Loss: 0.00002878
Iteration 35/1000 | Loss: 0.00002878
Iteration 36/1000 | Loss: 0.00002877
Iteration 37/1000 | Loss: 0.00002877
Iteration 38/1000 | Loss: 0.00002877
Iteration 39/1000 | Loss: 0.00002877
Iteration 40/1000 | Loss: 0.00002877
Iteration 41/1000 | Loss: 0.00002877
Iteration 42/1000 | Loss: 0.00002876
Iteration 43/1000 | Loss: 0.00002876
Iteration 44/1000 | Loss: 0.00002876
Iteration 45/1000 | Loss: 0.00002876
Iteration 46/1000 | Loss: 0.00002876
Iteration 47/1000 | Loss: 0.00002876
Iteration 48/1000 | Loss: 0.00002876
Iteration 49/1000 | Loss: 0.00002875
Iteration 50/1000 | Loss: 0.00002875
Iteration 51/1000 | Loss: 0.00002875
Iteration 52/1000 | Loss: 0.00002875
Iteration 53/1000 | Loss: 0.00002875
Iteration 54/1000 | Loss: 0.00002875
Iteration 55/1000 | Loss: 0.00002875
Iteration 56/1000 | Loss: 0.00002875
Iteration 57/1000 | Loss: 0.00002874
Iteration 58/1000 | Loss: 0.00002874
Iteration 59/1000 | Loss: 0.00002874
Iteration 60/1000 | Loss: 0.00002874
Iteration 61/1000 | Loss: 0.00002874
Iteration 62/1000 | Loss: 0.00002874
Iteration 63/1000 | Loss: 0.00002874
Iteration 64/1000 | Loss: 0.00002874
Iteration 65/1000 | Loss: 0.00002874
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 65. Stopping optimization.
Last 5 losses: [2.8743897928507067e-05, 2.8743897928507067e-05, 2.8743897928507067e-05, 2.8743897928507067e-05, 2.8743897928507067e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.8743897928507067e-05

Optimization complete. Final v2v error: 4.5137619972229 mm

Highest mean error: 4.775949954986572 mm for frame 29

Lowest mean error: 4.1796979904174805 mm for frame 43

Saving results

Total time: 30.7339346408844
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_36_nl_6641/0012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_nl_6641/0012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_nl_6641/0012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00436264
Iteration 2/25 | Loss: 0.00137356
Iteration 3/25 | Loss: 0.00121617
Iteration 4/25 | Loss: 0.00120083
Iteration 5/25 | Loss: 0.00119638
Iteration 6/25 | Loss: 0.00119493
Iteration 7/25 | Loss: 0.00119473
Iteration 8/25 | Loss: 0.00119473
Iteration 9/25 | Loss: 0.00119473
Iteration 10/25 | Loss: 0.00119473
Iteration 11/25 | Loss: 0.00119473
Iteration 12/25 | Loss: 0.00119473
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0011947324965149164, 0.0011947324965149164, 0.0011947324965149164, 0.0011947324965149164, 0.0011947324965149164]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011947324965149164

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39947033
Iteration 2/25 | Loss: 0.00050099
Iteration 3/25 | Loss: 0.00050097
Iteration 4/25 | Loss: 0.00050097
Iteration 5/25 | Loss: 0.00050097
Iteration 6/25 | Loss: 0.00050096
Iteration 7/25 | Loss: 0.00050096
Iteration 8/25 | Loss: 0.00050096
Iteration 9/25 | Loss: 0.00050096
Iteration 10/25 | Loss: 0.00050096
Iteration 11/25 | Loss: 0.00050096
Iteration 12/25 | Loss: 0.00050096
Iteration 13/25 | Loss: 0.00050096
Iteration 14/25 | Loss: 0.00050096
Iteration 15/25 | Loss: 0.00050096
Iteration 16/25 | Loss: 0.00050096
Iteration 17/25 | Loss: 0.00050096
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0005009636515751481, 0.0005009636515751481, 0.0005009636515751481, 0.0005009636515751481, 0.0005009636515751481]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005009636515751481

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00050096
Iteration 2/1000 | Loss: 0.00007005
Iteration 3/1000 | Loss: 0.00004260
Iteration 4/1000 | Loss: 0.00003210
Iteration 5/1000 | Loss: 0.00002812
Iteration 6/1000 | Loss: 0.00002567
Iteration 7/1000 | Loss: 0.00002450
Iteration 8/1000 | Loss: 0.00002340
Iteration 9/1000 | Loss: 0.00002280
Iteration 10/1000 | Loss: 0.00002243
Iteration 11/1000 | Loss: 0.00002205
Iteration 12/1000 | Loss: 0.00002169
Iteration 13/1000 | Loss: 0.00002165
Iteration 14/1000 | Loss: 0.00002163
Iteration 15/1000 | Loss: 0.00002162
Iteration 16/1000 | Loss: 0.00002152
Iteration 17/1000 | Loss: 0.00002147
Iteration 18/1000 | Loss: 0.00002140
Iteration 19/1000 | Loss: 0.00002140
Iteration 20/1000 | Loss: 0.00002140
Iteration 21/1000 | Loss: 0.00002140
Iteration 22/1000 | Loss: 0.00002139
Iteration 23/1000 | Loss: 0.00002139
Iteration 24/1000 | Loss: 0.00002139
Iteration 25/1000 | Loss: 0.00002139
Iteration 26/1000 | Loss: 0.00002139
Iteration 27/1000 | Loss: 0.00002139
Iteration 28/1000 | Loss: 0.00002139
Iteration 29/1000 | Loss: 0.00002134
Iteration 30/1000 | Loss: 0.00002133
Iteration 31/1000 | Loss: 0.00002133
Iteration 32/1000 | Loss: 0.00002132
Iteration 33/1000 | Loss: 0.00002132
Iteration 34/1000 | Loss: 0.00002130
Iteration 35/1000 | Loss: 0.00002130
Iteration 36/1000 | Loss: 0.00002128
Iteration 37/1000 | Loss: 0.00002128
Iteration 38/1000 | Loss: 0.00002128
Iteration 39/1000 | Loss: 0.00002128
Iteration 40/1000 | Loss: 0.00002128
Iteration 41/1000 | Loss: 0.00002128
Iteration 42/1000 | Loss: 0.00002127
Iteration 43/1000 | Loss: 0.00002127
Iteration 44/1000 | Loss: 0.00002127
Iteration 45/1000 | Loss: 0.00002127
Iteration 46/1000 | Loss: 0.00002127
Iteration 47/1000 | Loss: 0.00002127
Iteration 48/1000 | Loss: 0.00002127
Iteration 49/1000 | Loss: 0.00002127
Iteration 50/1000 | Loss: 0.00002127
Iteration 51/1000 | Loss: 0.00002127
Iteration 52/1000 | Loss: 0.00002127
Iteration 53/1000 | Loss: 0.00002127
Iteration 54/1000 | Loss: 0.00002126
Iteration 55/1000 | Loss: 0.00002126
Iteration 56/1000 | Loss: 0.00002126
Iteration 57/1000 | Loss: 0.00002126
Iteration 58/1000 | Loss: 0.00002125
Iteration 59/1000 | Loss: 0.00002125
Iteration 60/1000 | Loss: 0.00002125
Iteration 61/1000 | Loss: 0.00002125
Iteration 62/1000 | Loss: 0.00002125
Iteration 63/1000 | Loss: 0.00002124
Iteration 64/1000 | Loss: 0.00002124
Iteration 65/1000 | Loss: 0.00002124
Iteration 66/1000 | Loss: 0.00002124
Iteration 67/1000 | Loss: 0.00002124
Iteration 68/1000 | Loss: 0.00002124
Iteration 69/1000 | Loss: 0.00002124
Iteration 70/1000 | Loss: 0.00002123
Iteration 71/1000 | Loss: 0.00002123
Iteration 72/1000 | Loss: 0.00002123
Iteration 73/1000 | Loss: 0.00002123
Iteration 74/1000 | Loss: 0.00002123
Iteration 75/1000 | Loss: 0.00002123
Iteration 76/1000 | Loss: 0.00002123
Iteration 77/1000 | Loss: 0.00002123
Iteration 78/1000 | Loss: 0.00002123
Iteration 79/1000 | Loss: 0.00002123
Iteration 80/1000 | Loss: 0.00002123
Iteration 81/1000 | Loss: 0.00002123
Iteration 82/1000 | Loss: 0.00002123
Iteration 83/1000 | Loss: 0.00002123
Iteration 84/1000 | Loss: 0.00002122
Iteration 85/1000 | Loss: 0.00002122
Iteration 86/1000 | Loss: 0.00002122
Iteration 87/1000 | Loss: 0.00002122
Iteration 88/1000 | Loss: 0.00002122
Iteration 89/1000 | Loss: 0.00002121
Iteration 90/1000 | Loss: 0.00002121
Iteration 91/1000 | Loss: 0.00002121
Iteration 92/1000 | Loss: 0.00002121
Iteration 93/1000 | Loss: 0.00002121
Iteration 94/1000 | Loss: 0.00002121
Iteration 95/1000 | Loss: 0.00002121
Iteration 96/1000 | Loss: 0.00002121
Iteration 97/1000 | Loss: 0.00002121
Iteration 98/1000 | Loss: 0.00002121
Iteration 99/1000 | Loss: 0.00002121
Iteration 100/1000 | Loss: 0.00002121
Iteration 101/1000 | Loss: 0.00002121
Iteration 102/1000 | Loss: 0.00002121
Iteration 103/1000 | Loss: 0.00002121
Iteration 104/1000 | Loss: 0.00002120
Iteration 105/1000 | Loss: 0.00002120
Iteration 106/1000 | Loss: 0.00002120
Iteration 107/1000 | Loss: 0.00002120
Iteration 108/1000 | Loss: 0.00002120
Iteration 109/1000 | Loss: 0.00002120
Iteration 110/1000 | Loss: 0.00002120
Iteration 111/1000 | Loss: 0.00002120
Iteration 112/1000 | Loss: 0.00002120
Iteration 113/1000 | Loss: 0.00002120
Iteration 114/1000 | Loss: 0.00002120
Iteration 115/1000 | Loss: 0.00002120
Iteration 116/1000 | Loss: 0.00002120
Iteration 117/1000 | Loss: 0.00002120
Iteration 118/1000 | Loss: 0.00002119
Iteration 119/1000 | Loss: 0.00002119
Iteration 120/1000 | Loss: 0.00002119
Iteration 121/1000 | Loss: 0.00002119
Iteration 122/1000 | Loss: 0.00002119
Iteration 123/1000 | Loss: 0.00002119
Iteration 124/1000 | Loss: 0.00002119
Iteration 125/1000 | Loss: 0.00002119
Iteration 126/1000 | Loss: 0.00002119
Iteration 127/1000 | Loss: 0.00002119
Iteration 128/1000 | Loss: 0.00002119
Iteration 129/1000 | Loss: 0.00002119
Iteration 130/1000 | Loss: 0.00002119
Iteration 131/1000 | Loss: 0.00002119
Iteration 132/1000 | Loss: 0.00002119
Iteration 133/1000 | Loss: 0.00002119
Iteration 134/1000 | Loss: 0.00002119
Iteration 135/1000 | Loss: 0.00002119
Iteration 136/1000 | Loss: 0.00002119
Iteration 137/1000 | Loss: 0.00002119
Iteration 138/1000 | Loss: 0.00002119
Iteration 139/1000 | Loss: 0.00002119
Iteration 140/1000 | Loss: 0.00002119
Iteration 141/1000 | Loss: 0.00002119
Iteration 142/1000 | Loss: 0.00002119
Iteration 143/1000 | Loss: 0.00002119
Iteration 144/1000 | Loss: 0.00002119
Iteration 145/1000 | Loss: 0.00002119
Iteration 146/1000 | Loss: 0.00002119
Iteration 147/1000 | Loss: 0.00002119
Iteration 148/1000 | Loss: 0.00002119
Iteration 149/1000 | Loss: 0.00002119
Iteration 150/1000 | Loss: 0.00002119
Iteration 151/1000 | Loss: 0.00002119
Iteration 152/1000 | Loss: 0.00002119
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 152. Stopping optimization.
Last 5 losses: [2.118662632710766e-05, 2.118662632710766e-05, 2.118662632710766e-05, 2.118662632710766e-05, 2.118662632710766e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.118662632710766e-05

Optimization complete. Final v2v error: 3.8677098751068115 mm

Highest mean error: 5.244983196258545 mm for frame 83

Lowest mean error: 3.415127754211426 mm for frame 119

Saving results

Total time: 37.8054633140564
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_36_nl_6641/0006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_nl_6641/0006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_nl_6641/0006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00504846
Iteration 2/25 | Loss: 0.00142504
Iteration 3/25 | Loss: 0.00118349
Iteration 4/25 | Loss: 0.00116132
Iteration 5/25 | Loss: 0.00115752
Iteration 6/25 | Loss: 0.00115739
Iteration 7/25 | Loss: 0.00115739
Iteration 8/25 | Loss: 0.00115739
Iteration 9/25 | Loss: 0.00115739
Iteration 10/25 | Loss: 0.00115739
Iteration 11/25 | Loss: 0.00115739
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001157389022409916, 0.001157389022409916, 0.001157389022409916, 0.001157389022409916, 0.001157389022409916]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001157389022409916

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37837338
Iteration 2/25 | Loss: 0.00051923
Iteration 3/25 | Loss: 0.00051923
Iteration 4/25 | Loss: 0.00051923
Iteration 5/25 | Loss: 0.00051923
Iteration 6/25 | Loss: 0.00051923
Iteration 7/25 | Loss: 0.00051923
Iteration 8/25 | Loss: 0.00051923
Iteration 9/25 | Loss: 0.00051923
Iteration 10/25 | Loss: 0.00051923
Iteration 11/25 | Loss: 0.00051923
Iteration 12/25 | Loss: 0.00051923
Iteration 13/25 | Loss: 0.00051923
Iteration 14/25 | Loss: 0.00051923
Iteration 15/25 | Loss: 0.00051923
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0005192267126403749, 0.0005192267126403749, 0.0005192267126403749, 0.0005192267126403749, 0.0005192267126403749]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005192267126403749

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00051923
Iteration 2/1000 | Loss: 0.00006256
Iteration 3/1000 | Loss: 0.00003217
Iteration 4/1000 | Loss: 0.00002676
Iteration 5/1000 | Loss: 0.00002349
Iteration 6/1000 | Loss: 0.00002165
Iteration 7/1000 | Loss: 0.00002059
Iteration 8/1000 | Loss: 0.00002010
Iteration 9/1000 | Loss: 0.00001970
Iteration 10/1000 | Loss: 0.00001935
Iteration 11/1000 | Loss: 0.00001921
Iteration 12/1000 | Loss: 0.00001921
Iteration 13/1000 | Loss: 0.00001916
Iteration 14/1000 | Loss: 0.00001916
Iteration 15/1000 | Loss: 0.00001915
Iteration 16/1000 | Loss: 0.00001915
Iteration 17/1000 | Loss: 0.00001911
Iteration 18/1000 | Loss: 0.00001910
Iteration 19/1000 | Loss: 0.00001906
Iteration 20/1000 | Loss: 0.00001906
Iteration 21/1000 | Loss: 0.00001905
Iteration 22/1000 | Loss: 0.00001904
Iteration 23/1000 | Loss: 0.00001904
Iteration 24/1000 | Loss: 0.00001904
Iteration 25/1000 | Loss: 0.00001903
Iteration 26/1000 | Loss: 0.00001903
Iteration 27/1000 | Loss: 0.00001903
Iteration 28/1000 | Loss: 0.00001902
Iteration 29/1000 | Loss: 0.00001902
Iteration 30/1000 | Loss: 0.00001902
Iteration 31/1000 | Loss: 0.00001902
Iteration 32/1000 | Loss: 0.00001902
Iteration 33/1000 | Loss: 0.00001902
Iteration 34/1000 | Loss: 0.00001901
Iteration 35/1000 | Loss: 0.00001901
Iteration 36/1000 | Loss: 0.00001901
Iteration 37/1000 | Loss: 0.00001901
Iteration 38/1000 | Loss: 0.00001901
Iteration 39/1000 | Loss: 0.00001901
Iteration 40/1000 | Loss: 0.00001901
Iteration 41/1000 | Loss: 0.00001901
Iteration 42/1000 | Loss: 0.00001900
Iteration 43/1000 | Loss: 0.00001900
Iteration 44/1000 | Loss: 0.00001900
Iteration 45/1000 | Loss: 0.00001900
Iteration 46/1000 | Loss: 0.00001900
Iteration 47/1000 | Loss: 0.00001900
Iteration 48/1000 | Loss: 0.00001900
Iteration 49/1000 | Loss: 0.00001899
Iteration 50/1000 | Loss: 0.00001899
Iteration 51/1000 | Loss: 0.00001899
Iteration 52/1000 | Loss: 0.00001899
Iteration 53/1000 | Loss: 0.00001899
Iteration 54/1000 | Loss: 0.00001899
Iteration 55/1000 | Loss: 0.00001899
Iteration 56/1000 | Loss: 0.00001899
Iteration 57/1000 | Loss: 0.00001899
Iteration 58/1000 | Loss: 0.00001899
Iteration 59/1000 | Loss: 0.00001899
Iteration 60/1000 | Loss: 0.00001899
Iteration 61/1000 | Loss: 0.00001898
Iteration 62/1000 | Loss: 0.00001898
Iteration 63/1000 | Loss: 0.00001898
Iteration 64/1000 | Loss: 0.00001898
Iteration 65/1000 | Loss: 0.00001898
Iteration 66/1000 | Loss: 0.00001897
Iteration 67/1000 | Loss: 0.00001897
Iteration 68/1000 | Loss: 0.00001897
Iteration 69/1000 | Loss: 0.00001897
Iteration 70/1000 | Loss: 0.00001897
Iteration 71/1000 | Loss: 0.00001897
Iteration 72/1000 | Loss: 0.00001897
Iteration 73/1000 | Loss: 0.00001896
Iteration 74/1000 | Loss: 0.00001896
Iteration 75/1000 | Loss: 0.00001896
Iteration 76/1000 | Loss: 0.00001896
Iteration 77/1000 | Loss: 0.00001896
Iteration 78/1000 | Loss: 0.00001896
Iteration 79/1000 | Loss: 0.00001896
Iteration 80/1000 | Loss: 0.00001896
Iteration 81/1000 | Loss: 0.00001896
Iteration 82/1000 | Loss: 0.00001896
Iteration 83/1000 | Loss: 0.00001896
Iteration 84/1000 | Loss: 0.00001896
Iteration 85/1000 | Loss: 0.00001895
Iteration 86/1000 | Loss: 0.00001895
Iteration 87/1000 | Loss: 0.00001895
Iteration 88/1000 | Loss: 0.00001895
Iteration 89/1000 | Loss: 0.00001895
Iteration 90/1000 | Loss: 0.00001895
Iteration 91/1000 | Loss: 0.00001895
Iteration 92/1000 | Loss: 0.00001895
Iteration 93/1000 | Loss: 0.00001895
Iteration 94/1000 | Loss: 0.00001895
Iteration 95/1000 | Loss: 0.00001895
Iteration 96/1000 | Loss: 0.00001895
Iteration 97/1000 | Loss: 0.00001895
Iteration 98/1000 | Loss: 0.00001895
Iteration 99/1000 | Loss: 0.00001895
Iteration 100/1000 | Loss: 0.00001895
Iteration 101/1000 | Loss: 0.00001895
Iteration 102/1000 | Loss: 0.00001895
Iteration 103/1000 | Loss: 0.00001895
Iteration 104/1000 | Loss: 0.00001895
Iteration 105/1000 | Loss: 0.00001895
Iteration 106/1000 | Loss: 0.00001895
Iteration 107/1000 | Loss: 0.00001895
Iteration 108/1000 | Loss: 0.00001895
Iteration 109/1000 | Loss: 0.00001895
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 109. Stopping optimization.
Last 5 losses: [1.8951155652757734e-05, 1.8951155652757734e-05, 1.8951155652757734e-05, 1.8951155652757734e-05, 1.8951155652757734e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8951155652757734e-05

Optimization complete. Final v2v error: 3.6586813926696777 mm

Highest mean error: 4.631586074829102 mm for frame 85

Lowest mean error: 3.3376457691192627 mm for frame 15

Saving results

Total time: 30.33926773071289
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_36_nl_6641/0024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_nl_6641/0024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_nl_6641/0024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00531297
Iteration 2/25 | Loss: 0.00130008
Iteration 3/25 | Loss: 0.00118724
Iteration 4/25 | Loss: 0.00117599
Iteration 5/25 | Loss: 0.00117296
Iteration 6/25 | Loss: 0.00117205
Iteration 7/25 | Loss: 0.00117205
Iteration 8/25 | Loss: 0.00117205
Iteration 9/25 | Loss: 0.00117205
Iteration 10/25 | Loss: 0.00117205
Iteration 11/25 | Loss: 0.00117205
Iteration 12/25 | Loss: 0.00117205
Iteration 13/25 | Loss: 0.00117205
Iteration 14/25 | Loss: 0.00117205
Iteration 15/25 | Loss: 0.00117205
Iteration 16/25 | Loss: 0.00117205
Iteration 17/25 | Loss: 0.00117205
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0011720519978553057, 0.0011720519978553057, 0.0011720519978553057, 0.0011720519978553057, 0.0011720519978553057]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011720519978553057

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.98743629
Iteration 2/25 | Loss: 0.00050829
Iteration 3/25 | Loss: 0.00050829
Iteration 4/25 | Loss: 0.00050829
Iteration 5/25 | Loss: 0.00050829
Iteration 6/25 | Loss: 0.00050829
Iteration 7/25 | Loss: 0.00050828
Iteration 8/25 | Loss: 0.00050828
Iteration 9/25 | Loss: 0.00050828
Iteration 10/25 | Loss: 0.00050828
Iteration 11/25 | Loss: 0.00050828
Iteration 12/25 | Loss: 0.00050828
Iteration 13/25 | Loss: 0.00050828
Iteration 14/25 | Loss: 0.00050828
Iteration 15/25 | Loss: 0.00050828
Iteration 16/25 | Loss: 0.00050828
Iteration 17/25 | Loss: 0.00050828
Iteration 18/25 | Loss: 0.00050828
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.000508283672388643, 0.000508283672388643, 0.000508283672388643, 0.000508283672388643, 0.000508283672388643]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000508283672388643

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00050828
Iteration 2/1000 | Loss: 0.00005937
Iteration 3/1000 | Loss: 0.00003724
Iteration 4/1000 | Loss: 0.00002656
Iteration 5/1000 | Loss: 0.00002285
Iteration 6/1000 | Loss: 0.00002150
Iteration 7/1000 | Loss: 0.00002069
Iteration 8/1000 | Loss: 0.00002009
Iteration 9/1000 | Loss: 0.00001968
Iteration 10/1000 | Loss: 0.00001927
Iteration 11/1000 | Loss: 0.00001906
Iteration 12/1000 | Loss: 0.00001904
Iteration 13/1000 | Loss: 0.00001900
Iteration 14/1000 | Loss: 0.00001900
Iteration 15/1000 | Loss: 0.00001899
Iteration 16/1000 | Loss: 0.00001899
Iteration 17/1000 | Loss: 0.00001899
Iteration 18/1000 | Loss: 0.00001899
Iteration 19/1000 | Loss: 0.00001897
Iteration 20/1000 | Loss: 0.00001897
Iteration 21/1000 | Loss: 0.00001895
Iteration 22/1000 | Loss: 0.00001895
Iteration 23/1000 | Loss: 0.00001894
Iteration 24/1000 | Loss: 0.00001894
Iteration 25/1000 | Loss: 0.00001893
Iteration 26/1000 | Loss: 0.00001892
Iteration 27/1000 | Loss: 0.00001892
Iteration 28/1000 | Loss: 0.00001891
Iteration 29/1000 | Loss: 0.00001891
Iteration 30/1000 | Loss: 0.00001891
Iteration 31/1000 | Loss: 0.00001889
Iteration 32/1000 | Loss: 0.00001888
Iteration 33/1000 | Loss: 0.00001888
Iteration 34/1000 | Loss: 0.00001888
Iteration 35/1000 | Loss: 0.00001888
Iteration 36/1000 | Loss: 0.00001888
Iteration 37/1000 | Loss: 0.00001888
Iteration 38/1000 | Loss: 0.00001888
Iteration 39/1000 | Loss: 0.00001888
Iteration 40/1000 | Loss: 0.00001888
Iteration 41/1000 | Loss: 0.00001888
Iteration 42/1000 | Loss: 0.00001888
Iteration 43/1000 | Loss: 0.00001887
Iteration 44/1000 | Loss: 0.00001887
Iteration 45/1000 | Loss: 0.00001887
Iteration 46/1000 | Loss: 0.00001887
Iteration 47/1000 | Loss: 0.00001886
Iteration 48/1000 | Loss: 0.00001886
Iteration 49/1000 | Loss: 0.00001886
Iteration 50/1000 | Loss: 0.00001886
Iteration 51/1000 | Loss: 0.00001885
Iteration 52/1000 | Loss: 0.00001885
Iteration 53/1000 | Loss: 0.00001885
Iteration 54/1000 | Loss: 0.00001885
Iteration 55/1000 | Loss: 0.00001885
Iteration 56/1000 | Loss: 0.00001885
Iteration 57/1000 | Loss: 0.00001885
Iteration 58/1000 | Loss: 0.00001885
Iteration 59/1000 | Loss: 0.00001885
Iteration 60/1000 | Loss: 0.00001884
Iteration 61/1000 | Loss: 0.00001884
Iteration 62/1000 | Loss: 0.00001884
Iteration 63/1000 | Loss: 0.00001884
Iteration 64/1000 | Loss: 0.00001884
Iteration 65/1000 | Loss: 0.00001884
Iteration 66/1000 | Loss: 0.00001884
Iteration 67/1000 | Loss: 0.00001884
Iteration 68/1000 | Loss: 0.00001884
Iteration 69/1000 | Loss: 0.00001884
Iteration 70/1000 | Loss: 0.00001883
Iteration 71/1000 | Loss: 0.00001883
Iteration 72/1000 | Loss: 0.00001883
Iteration 73/1000 | Loss: 0.00001883
Iteration 74/1000 | Loss: 0.00001883
Iteration 75/1000 | Loss: 0.00001883
Iteration 76/1000 | Loss: 0.00001883
Iteration 77/1000 | Loss: 0.00001883
Iteration 78/1000 | Loss: 0.00001883
Iteration 79/1000 | Loss: 0.00001883
Iteration 80/1000 | Loss: 0.00001882
Iteration 81/1000 | Loss: 0.00001882
Iteration 82/1000 | Loss: 0.00001882
Iteration 83/1000 | Loss: 0.00001882
Iteration 84/1000 | Loss: 0.00001882
Iteration 85/1000 | Loss: 0.00001882
Iteration 86/1000 | Loss: 0.00001882
Iteration 87/1000 | Loss: 0.00001881
Iteration 88/1000 | Loss: 0.00001881
Iteration 89/1000 | Loss: 0.00001881
Iteration 90/1000 | Loss: 0.00001881
Iteration 91/1000 | Loss: 0.00001881
Iteration 92/1000 | Loss: 0.00001881
Iteration 93/1000 | Loss: 0.00001881
Iteration 94/1000 | Loss: 0.00001881
Iteration 95/1000 | Loss: 0.00001881
Iteration 96/1000 | Loss: 0.00001881
Iteration 97/1000 | Loss: 0.00001881
Iteration 98/1000 | Loss: 0.00001881
Iteration 99/1000 | Loss: 0.00001881
Iteration 100/1000 | Loss: 0.00001881
Iteration 101/1000 | Loss: 0.00001881
Iteration 102/1000 | Loss: 0.00001881
Iteration 103/1000 | Loss: 0.00001881
Iteration 104/1000 | Loss: 0.00001881
Iteration 105/1000 | Loss: 0.00001881
Iteration 106/1000 | Loss: 0.00001881
Iteration 107/1000 | Loss: 0.00001881
Iteration 108/1000 | Loss: 0.00001881
Iteration 109/1000 | Loss: 0.00001881
Iteration 110/1000 | Loss: 0.00001881
Iteration 111/1000 | Loss: 0.00001881
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 111. Stopping optimization.
Last 5 losses: [1.8809545508702286e-05, 1.8809545508702286e-05, 1.8809545508702286e-05, 1.8809545508702286e-05, 1.8809545508702286e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8809545508702286e-05

Optimization complete. Final v2v error: 3.7454516887664795 mm

Highest mean error: 4.283425331115723 mm for frame 47

Lowest mean error: 3.3714027404785156 mm for frame 0

Saving results

Total time: 31.418773651123047
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_36_nl_6641/0002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_nl_6641/0002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_nl_6641/0002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01142946
Iteration 2/25 | Loss: 0.01142946
Iteration 3/25 | Loss: 0.01142946
Iteration 4/25 | Loss: 0.01142946
Iteration 5/25 | Loss: 0.01142946
Iteration 6/25 | Loss: 0.01142946
Iteration 7/25 | Loss: 0.01142945
Iteration 8/25 | Loss: 0.01142945
Iteration 9/25 | Loss: 0.01142945
Iteration 10/25 | Loss: 0.00357368
Iteration 11/25 | Loss: 0.00209408
Iteration 12/25 | Loss: 0.00237731
Iteration 13/25 | Loss: 0.00200856
Iteration 14/25 | Loss: 0.00183205
Iteration 15/25 | Loss: 0.00130743
Iteration 16/25 | Loss: 0.00119205
Iteration 17/25 | Loss: 0.00117837
Iteration 18/25 | Loss: 0.00117291
Iteration 19/25 | Loss: 0.00116256
Iteration 20/25 | Loss: 0.00116715
Iteration 21/25 | Loss: 0.00115970
Iteration 22/25 | Loss: 0.00116022
Iteration 23/25 | Loss: 0.00115998
Iteration 24/25 | Loss: 0.00115880
Iteration 25/25 | Loss: 0.00115788

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.54880953
Iteration 2/25 | Loss: 0.00053064
Iteration 3/25 | Loss: 0.00039301
Iteration 4/25 | Loss: 0.00039301
Iteration 5/25 | Loss: 0.00039301
Iteration 6/25 | Loss: 0.00039301
Iteration 7/25 | Loss: 0.00039301
Iteration 8/25 | Loss: 0.00039301
Iteration 9/25 | Loss: 0.00039301
Iteration 10/25 | Loss: 0.00039301
Iteration 11/25 | Loss: 0.00039301
Iteration 12/25 | Loss: 0.00039301
Iteration 13/25 | Loss: 0.00039301
Iteration 14/25 | Loss: 0.00039301
Iteration 15/25 | Loss: 0.00039301
Iteration 16/25 | Loss: 0.00039301
Iteration 17/25 | Loss: 0.00039301
Iteration 18/25 | Loss: 0.00039301
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.00039300660137087107, 0.00039300660137087107, 0.00039300660137087107, 0.00039300660137087107, 0.00039300660137087107]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00039300660137087107

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00039301
Iteration 2/1000 | Loss: 0.00037387
Iteration 3/1000 | Loss: 0.00007947
Iteration 4/1000 | Loss: 0.00008555
Iteration 5/1000 | Loss: 0.00008201
Iteration 6/1000 | Loss: 0.00008417
Iteration 7/1000 | Loss: 0.00005207
Iteration 8/1000 | Loss: 0.00004731
Iteration 9/1000 | Loss: 0.00004826
Iteration 10/1000 | Loss: 0.00004552
Iteration 11/1000 | Loss: 0.00004252
Iteration 12/1000 | Loss: 0.00004183
Iteration 13/1000 | Loss: 0.00004174
Iteration 14/1000 | Loss: 0.00004192
Iteration 15/1000 | Loss: 0.00004102
Iteration 16/1000 | Loss: 0.00004532
Iteration 17/1000 | Loss: 0.00004115
Iteration 18/1000 | Loss: 0.00004099
Iteration 19/1000 | Loss: 0.00004121
Iteration 20/1000 | Loss: 0.00004118
Iteration 21/1000 | Loss: 0.00004118
Iteration 22/1000 | Loss: 0.00004185
Iteration 23/1000 | Loss: 0.00004080
Iteration 24/1000 | Loss: 0.00004079
Iteration 25/1000 | Loss: 0.00004079
Iteration 26/1000 | Loss: 0.00004079
Iteration 27/1000 | Loss: 0.00004079
Iteration 28/1000 | Loss: 0.00004079
Iteration 29/1000 | Loss: 0.00004078
Iteration 30/1000 | Loss: 0.00004078
Iteration 31/1000 | Loss: 0.00004078
Iteration 32/1000 | Loss: 0.00004077
Iteration 33/1000 | Loss: 0.00004076
Iteration 34/1000 | Loss: 0.00004076
Iteration 35/1000 | Loss: 0.00004076
Iteration 36/1000 | Loss: 0.00004076
Iteration 37/1000 | Loss: 0.00004076
Iteration 38/1000 | Loss: 0.00004076
Iteration 39/1000 | Loss: 0.00004076
Iteration 40/1000 | Loss: 0.00004076
Iteration 41/1000 | Loss: 0.00004076
Iteration 42/1000 | Loss: 0.00004074
Iteration 43/1000 | Loss: 0.00004074
Iteration 44/1000 | Loss: 0.00004074
Iteration 45/1000 | Loss: 0.00004074
Iteration 46/1000 | Loss: 0.00004074
Iteration 47/1000 | Loss: 0.00004074
Iteration 48/1000 | Loss: 0.00004074
Iteration 49/1000 | Loss: 0.00004074
Iteration 50/1000 | Loss: 0.00004074
Iteration 51/1000 | Loss: 0.00004074
Iteration 52/1000 | Loss: 0.00004073
Iteration 53/1000 | Loss: 0.00004073
Iteration 54/1000 | Loss: 0.00004073
Iteration 55/1000 | Loss: 0.00004073
Iteration 56/1000 | Loss: 0.00004073
Iteration 57/1000 | Loss: 0.00004097
Iteration 58/1000 | Loss: 0.00004071
Iteration 59/1000 | Loss: 0.00004070
Iteration 60/1000 | Loss: 0.00004070
Iteration 61/1000 | Loss: 0.00004070
Iteration 62/1000 | Loss: 0.00004070
Iteration 63/1000 | Loss: 0.00004070
Iteration 64/1000 | Loss: 0.00004070
Iteration 65/1000 | Loss: 0.00004070
Iteration 66/1000 | Loss: 0.00004070
Iteration 67/1000 | Loss: 0.00004070
Iteration 68/1000 | Loss: 0.00004070
Iteration 69/1000 | Loss: 0.00004070
Iteration 70/1000 | Loss: 0.00004070
Iteration 71/1000 | Loss: 0.00004070
Iteration 72/1000 | Loss: 0.00004070
Iteration 73/1000 | Loss: 0.00004070
Iteration 74/1000 | Loss: 0.00004070
Iteration 75/1000 | Loss: 0.00004069
Iteration 76/1000 | Loss: 0.00004069
Iteration 77/1000 | Loss: 0.00004069
Iteration 78/1000 | Loss: 0.00004068
Iteration 79/1000 | Loss: 0.00004068
Iteration 80/1000 | Loss: 0.00004068
Iteration 81/1000 | Loss: 0.00004067
Iteration 82/1000 | Loss: 0.00004067
Iteration 83/1000 | Loss: 0.00004067
Iteration 84/1000 | Loss: 0.00004066
Iteration 85/1000 | Loss: 0.00004066
Iteration 86/1000 | Loss: 0.00004066
Iteration 87/1000 | Loss: 0.00004066
Iteration 88/1000 | Loss: 0.00004066
Iteration 89/1000 | Loss: 0.00004066
Iteration 90/1000 | Loss: 0.00004065
Iteration 91/1000 | Loss: 0.00004065
Iteration 92/1000 | Loss: 0.00004065
Iteration 93/1000 | Loss: 0.00004065
Iteration 94/1000 | Loss: 0.00004065
Iteration 95/1000 | Loss: 0.00004065
Iteration 96/1000 | Loss: 0.00004065
Iteration 97/1000 | Loss: 0.00004065
Iteration 98/1000 | Loss: 0.00004065
Iteration 99/1000 | Loss: 0.00004064
Iteration 100/1000 | Loss: 0.00004064
Iteration 101/1000 | Loss: 0.00004064
Iteration 102/1000 | Loss: 0.00004064
Iteration 103/1000 | Loss: 0.00004064
Iteration 104/1000 | Loss: 0.00004063
Iteration 105/1000 | Loss: 0.00004063
Iteration 106/1000 | Loss: 0.00004206
Iteration 107/1000 | Loss: 0.00004062
Iteration 108/1000 | Loss: 0.00004062
Iteration 109/1000 | Loss: 0.00004062
Iteration 110/1000 | Loss: 0.00004062
Iteration 111/1000 | Loss: 0.00004062
Iteration 112/1000 | Loss: 0.00004062
Iteration 113/1000 | Loss: 0.00004062
Iteration 114/1000 | Loss: 0.00004061
Iteration 115/1000 | Loss: 0.00004061
Iteration 116/1000 | Loss: 0.00004061
Iteration 117/1000 | Loss: 0.00004061
Iteration 118/1000 | Loss: 0.00004061
Iteration 119/1000 | Loss: 0.00004061
Iteration 120/1000 | Loss: 0.00004061
Iteration 121/1000 | Loss: 0.00004061
Iteration 122/1000 | Loss: 0.00004061
Iteration 123/1000 | Loss: 0.00004061
Iteration 124/1000 | Loss: 0.00004061
Iteration 125/1000 | Loss: 0.00004061
Iteration 126/1000 | Loss: 0.00004061
Iteration 127/1000 | Loss: 0.00004061
Iteration 128/1000 | Loss: 0.00004061
Iteration 129/1000 | Loss: 0.00004061
Iteration 130/1000 | Loss: 0.00004061
Iteration 131/1000 | Loss: 0.00004061
Iteration 132/1000 | Loss: 0.00004061
Iteration 133/1000 | Loss: 0.00004061
Iteration 134/1000 | Loss: 0.00004061
Iteration 135/1000 | Loss: 0.00004061
Iteration 136/1000 | Loss: 0.00004061
Iteration 137/1000 | Loss: 0.00004061
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 137. Stopping optimization.
Last 5 losses: [4.060976789332926e-05, 4.060976789332926e-05, 4.060976789332926e-05, 4.060976789332926e-05, 4.060976789332926e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.060976789332926e-05

Optimization complete. Final v2v error: 4.445895195007324 mm

Highest mean error: 20.397449493408203 mm for frame 209

Lowest mean error: 3.8647427558898926 mm for frame 1

Saving results

Total time: 76.67389750480652
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_36_nl_6641/0005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_nl_6641/0005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_nl_6641/0005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00416602
Iteration 2/25 | Loss: 0.00125572
Iteration 3/25 | Loss: 0.00116053
Iteration 4/25 | Loss: 0.00115187
Iteration 5/25 | Loss: 0.00115016
Iteration 6/25 | Loss: 0.00114981
Iteration 7/25 | Loss: 0.00114981
Iteration 8/25 | Loss: 0.00114981
Iteration 9/25 | Loss: 0.00114981
Iteration 10/25 | Loss: 0.00114981
Iteration 11/25 | Loss: 0.00114981
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011498068924993277, 0.0011498068924993277, 0.0011498068924993277, 0.0011498068924993277, 0.0011498068924993277]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011498068924993277

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38963509
Iteration 2/25 | Loss: 0.00048529
Iteration 3/25 | Loss: 0.00048528
Iteration 4/25 | Loss: 0.00048528
Iteration 5/25 | Loss: 0.00048528
Iteration 6/25 | Loss: 0.00048528
Iteration 7/25 | Loss: 0.00048528
Iteration 8/25 | Loss: 0.00048528
Iteration 9/25 | Loss: 0.00048528
Iteration 10/25 | Loss: 0.00048528
Iteration 11/25 | Loss: 0.00048528
Iteration 12/25 | Loss: 0.00048528
Iteration 13/25 | Loss: 0.00048528
Iteration 14/25 | Loss: 0.00048528
Iteration 15/25 | Loss: 0.00048528
Iteration 16/25 | Loss: 0.00048528
Iteration 17/25 | Loss: 0.00048528
Iteration 18/25 | Loss: 0.00048528
Iteration 19/25 | Loss: 0.00048528
Iteration 20/25 | Loss: 0.00048528
Iteration 21/25 | Loss: 0.00048528
Iteration 22/25 | Loss: 0.00048528
Iteration 23/25 | Loss: 0.00048528
Iteration 24/25 | Loss: 0.00048528
Iteration 25/25 | Loss: 0.00048528

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00048528
Iteration 2/1000 | Loss: 0.00005807
Iteration 3/1000 | Loss: 0.00003005
Iteration 4/1000 | Loss: 0.00002357
Iteration 5/1000 | Loss: 0.00002063
Iteration 6/1000 | Loss: 0.00001958
Iteration 7/1000 | Loss: 0.00001878
Iteration 8/1000 | Loss: 0.00001835
Iteration 9/1000 | Loss: 0.00001793
Iteration 10/1000 | Loss: 0.00001769
Iteration 11/1000 | Loss: 0.00001766
Iteration 12/1000 | Loss: 0.00001766
Iteration 13/1000 | Loss: 0.00001762
Iteration 14/1000 | Loss: 0.00001762
Iteration 15/1000 | Loss: 0.00001762
Iteration 16/1000 | Loss: 0.00001760
Iteration 17/1000 | Loss: 0.00001760
Iteration 18/1000 | Loss: 0.00001757
Iteration 19/1000 | Loss: 0.00001757
Iteration 20/1000 | Loss: 0.00001757
Iteration 21/1000 | Loss: 0.00001757
Iteration 22/1000 | Loss: 0.00001756
Iteration 23/1000 | Loss: 0.00001756
Iteration 24/1000 | Loss: 0.00001756
Iteration 25/1000 | Loss: 0.00001756
Iteration 26/1000 | Loss: 0.00001756
Iteration 27/1000 | Loss: 0.00001755
Iteration 28/1000 | Loss: 0.00001755
Iteration 29/1000 | Loss: 0.00001755
Iteration 30/1000 | Loss: 0.00001755
Iteration 31/1000 | Loss: 0.00001755
Iteration 32/1000 | Loss: 0.00001754
Iteration 33/1000 | Loss: 0.00001754
Iteration 34/1000 | Loss: 0.00001754
Iteration 35/1000 | Loss: 0.00001754
Iteration 36/1000 | Loss: 0.00001753
Iteration 37/1000 | Loss: 0.00001753
Iteration 38/1000 | Loss: 0.00001750
Iteration 39/1000 | Loss: 0.00001750
Iteration 40/1000 | Loss: 0.00001750
Iteration 41/1000 | Loss: 0.00001750
Iteration 42/1000 | Loss: 0.00001750
Iteration 43/1000 | Loss: 0.00001750
Iteration 44/1000 | Loss: 0.00001750
Iteration 45/1000 | Loss: 0.00001750
Iteration 46/1000 | Loss: 0.00001750
Iteration 47/1000 | Loss: 0.00001750
Iteration 48/1000 | Loss: 0.00001750
Iteration 49/1000 | Loss: 0.00001750
Iteration 50/1000 | Loss: 0.00001750
Iteration 51/1000 | Loss: 0.00001750
Iteration 52/1000 | Loss: 0.00001750
Iteration 53/1000 | Loss: 0.00001750
Iteration 54/1000 | Loss: 0.00001750
Iteration 55/1000 | Loss: 0.00001750
Iteration 56/1000 | Loss: 0.00001750
Iteration 57/1000 | Loss: 0.00001750
Iteration 58/1000 | Loss: 0.00001750
Iteration 59/1000 | Loss: 0.00001750
Iteration 60/1000 | Loss: 0.00001750
Iteration 61/1000 | Loss: 0.00001750
Iteration 62/1000 | Loss: 0.00001750
Iteration 63/1000 | Loss: 0.00001750
Iteration 64/1000 | Loss: 0.00001750
Iteration 65/1000 | Loss: 0.00001750
Iteration 66/1000 | Loss: 0.00001750
Iteration 67/1000 | Loss: 0.00001750
Iteration 68/1000 | Loss: 0.00001750
Iteration 69/1000 | Loss: 0.00001750
Iteration 70/1000 | Loss: 0.00001750
Iteration 71/1000 | Loss: 0.00001750
Iteration 72/1000 | Loss: 0.00001750
Iteration 73/1000 | Loss: 0.00001750
Iteration 74/1000 | Loss: 0.00001750
Iteration 75/1000 | Loss: 0.00001750
Iteration 76/1000 | Loss: 0.00001750
Iteration 77/1000 | Loss: 0.00001750
Iteration 78/1000 | Loss: 0.00001750
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 78. Stopping optimization.
Last 5 losses: [1.7495915017207153e-05, 1.7495915017207153e-05, 1.7495915017207153e-05, 1.7495915017207153e-05, 1.7495915017207153e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7495915017207153e-05

Optimization complete. Final v2v error: 3.5715322494506836 mm

Highest mean error: 3.694953680038452 mm for frame 29

Lowest mean error: 3.404266357421875 mm for frame 132

Saving results

Total time: 26.78967261314392
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_36_nl_6641/0003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_nl_6641/0003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_nl_6641/0003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00429138
Iteration 2/25 | Loss: 0.00133800
Iteration 3/25 | Loss: 0.00117646
Iteration 4/25 | Loss: 0.00115721
Iteration 5/25 | Loss: 0.00115425
Iteration 6/25 | Loss: 0.00115342
Iteration 7/25 | Loss: 0.00115340
Iteration 8/25 | Loss: 0.00115340
Iteration 9/25 | Loss: 0.00115340
Iteration 10/25 | Loss: 0.00115340
Iteration 11/25 | Loss: 0.00115340
Iteration 12/25 | Loss: 0.00115340
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.001153399352915585, 0.001153399352915585, 0.001153399352915585, 0.001153399352915585, 0.001153399352915585]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001153399352915585

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38941097
Iteration 2/25 | Loss: 0.00048464
Iteration 3/25 | Loss: 0.00048464
Iteration 4/25 | Loss: 0.00048464
Iteration 5/25 | Loss: 0.00048464
Iteration 6/25 | Loss: 0.00048464
Iteration 7/25 | Loss: 0.00048464
Iteration 8/25 | Loss: 0.00048464
Iteration 9/25 | Loss: 0.00048464
Iteration 10/25 | Loss: 0.00048464
Iteration 11/25 | Loss: 0.00048464
Iteration 12/25 | Loss: 0.00048464
Iteration 13/25 | Loss: 0.00048464
Iteration 14/25 | Loss: 0.00048464
Iteration 15/25 | Loss: 0.00048464
Iteration 16/25 | Loss: 0.00048464
Iteration 17/25 | Loss: 0.00048464
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.00048463832354173064, 0.00048463832354173064, 0.00048463832354173064, 0.00048463832354173064, 0.00048463832354173064]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00048463832354173064

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00048464
Iteration 2/1000 | Loss: 0.00005569
Iteration 3/1000 | Loss: 0.00003187
Iteration 4/1000 | Loss: 0.00002591
Iteration 5/1000 | Loss: 0.00002309
Iteration 6/1000 | Loss: 0.00002181
Iteration 7/1000 | Loss: 0.00002092
Iteration 8/1000 | Loss: 0.00002035
Iteration 9/1000 | Loss: 0.00001988
Iteration 10/1000 | Loss: 0.00001957
Iteration 11/1000 | Loss: 0.00001952
Iteration 12/1000 | Loss: 0.00001944
Iteration 13/1000 | Loss: 0.00001944
Iteration 14/1000 | Loss: 0.00001944
Iteration 15/1000 | Loss: 0.00001943
Iteration 16/1000 | Loss: 0.00001943
Iteration 17/1000 | Loss: 0.00001938
Iteration 18/1000 | Loss: 0.00001938
Iteration 19/1000 | Loss: 0.00001938
Iteration 20/1000 | Loss: 0.00001937
Iteration 21/1000 | Loss: 0.00001935
Iteration 22/1000 | Loss: 0.00001935
Iteration 23/1000 | Loss: 0.00001934
Iteration 24/1000 | Loss: 0.00001934
Iteration 25/1000 | Loss: 0.00001934
Iteration 26/1000 | Loss: 0.00001933
Iteration 27/1000 | Loss: 0.00001933
Iteration 28/1000 | Loss: 0.00001933
Iteration 29/1000 | Loss: 0.00001933
Iteration 30/1000 | Loss: 0.00001933
Iteration 31/1000 | Loss: 0.00001932
Iteration 32/1000 | Loss: 0.00001932
Iteration 33/1000 | Loss: 0.00001931
Iteration 34/1000 | Loss: 0.00001931
Iteration 35/1000 | Loss: 0.00001931
Iteration 36/1000 | Loss: 0.00001931
Iteration 37/1000 | Loss: 0.00001931
Iteration 38/1000 | Loss: 0.00001931
Iteration 39/1000 | Loss: 0.00001931
Iteration 40/1000 | Loss: 0.00001930
Iteration 41/1000 | Loss: 0.00001930
Iteration 42/1000 | Loss: 0.00001930
Iteration 43/1000 | Loss: 0.00001930
Iteration 44/1000 | Loss: 0.00001929
Iteration 45/1000 | Loss: 0.00001929
Iteration 46/1000 | Loss: 0.00001929
Iteration 47/1000 | Loss: 0.00001929
Iteration 48/1000 | Loss: 0.00001929
Iteration 49/1000 | Loss: 0.00001928
Iteration 50/1000 | Loss: 0.00001928
Iteration 51/1000 | Loss: 0.00001928
Iteration 52/1000 | Loss: 0.00001928
Iteration 53/1000 | Loss: 0.00001927
Iteration 54/1000 | Loss: 0.00001927
Iteration 55/1000 | Loss: 0.00001927
Iteration 56/1000 | Loss: 0.00001927
Iteration 57/1000 | Loss: 0.00001926
Iteration 58/1000 | Loss: 0.00001926
Iteration 59/1000 | Loss: 0.00001925
Iteration 60/1000 | Loss: 0.00001925
Iteration 61/1000 | Loss: 0.00001925
Iteration 62/1000 | Loss: 0.00001925
Iteration 63/1000 | Loss: 0.00001925
Iteration 64/1000 | Loss: 0.00001924
Iteration 65/1000 | Loss: 0.00001924
Iteration 66/1000 | Loss: 0.00001924
Iteration 67/1000 | Loss: 0.00001924
Iteration 68/1000 | Loss: 0.00001924
Iteration 69/1000 | Loss: 0.00001923
Iteration 70/1000 | Loss: 0.00001923
Iteration 71/1000 | Loss: 0.00001923
Iteration 72/1000 | Loss: 0.00001923
Iteration 73/1000 | Loss: 0.00001923
Iteration 74/1000 | Loss: 0.00001923
Iteration 75/1000 | Loss: 0.00001923
Iteration 76/1000 | Loss: 0.00001923
Iteration 77/1000 | Loss: 0.00001922
Iteration 78/1000 | Loss: 0.00001922
Iteration 79/1000 | Loss: 0.00001922
Iteration 80/1000 | Loss: 0.00001922
Iteration 81/1000 | Loss: 0.00001922
Iteration 82/1000 | Loss: 0.00001922
Iteration 83/1000 | Loss: 0.00001922
Iteration 84/1000 | Loss: 0.00001921
Iteration 85/1000 | Loss: 0.00001921
Iteration 86/1000 | Loss: 0.00001921
Iteration 87/1000 | Loss: 0.00001921
Iteration 88/1000 | Loss: 0.00001921
Iteration 89/1000 | Loss: 0.00001921
Iteration 90/1000 | Loss: 0.00001921
Iteration 91/1000 | Loss: 0.00001921
Iteration 92/1000 | Loss: 0.00001921
Iteration 93/1000 | Loss: 0.00001921
Iteration 94/1000 | Loss: 0.00001921
Iteration 95/1000 | Loss: 0.00001921
Iteration 96/1000 | Loss: 0.00001921
Iteration 97/1000 | Loss: 0.00001921
Iteration 98/1000 | Loss: 0.00001921
Iteration 99/1000 | Loss: 0.00001921
Iteration 100/1000 | Loss: 0.00001920
Iteration 101/1000 | Loss: 0.00001920
Iteration 102/1000 | Loss: 0.00001920
Iteration 103/1000 | Loss: 0.00001920
Iteration 104/1000 | Loss: 0.00001920
Iteration 105/1000 | Loss: 0.00001920
Iteration 106/1000 | Loss: 0.00001920
Iteration 107/1000 | Loss: 0.00001920
Iteration 108/1000 | Loss: 0.00001920
Iteration 109/1000 | Loss: 0.00001920
Iteration 110/1000 | Loss: 0.00001919
Iteration 111/1000 | Loss: 0.00001919
Iteration 112/1000 | Loss: 0.00001919
Iteration 113/1000 | Loss: 0.00001919
Iteration 114/1000 | Loss: 0.00001919
Iteration 115/1000 | Loss: 0.00001919
Iteration 116/1000 | Loss: 0.00001919
Iteration 117/1000 | Loss: 0.00001919
Iteration 118/1000 | Loss: 0.00001919
Iteration 119/1000 | Loss: 0.00001919
Iteration 120/1000 | Loss: 0.00001919
Iteration 121/1000 | Loss: 0.00001919
Iteration 122/1000 | Loss: 0.00001919
Iteration 123/1000 | Loss: 0.00001919
Iteration 124/1000 | Loss: 0.00001919
Iteration 125/1000 | Loss: 0.00001919
Iteration 126/1000 | Loss: 0.00001919
Iteration 127/1000 | Loss: 0.00001919
Iteration 128/1000 | Loss: 0.00001919
Iteration 129/1000 | Loss: 0.00001919
Iteration 130/1000 | Loss: 0.00001919
Iteration 131/1000 | Loss: 0.00001919
Iteration 132/1000 | Loss: 0.00001919
Iteration 133/1000 | Loss: 0.00001919
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 133. Stopping optimization.
Last 5 losses: [1.919267015182413e-05, 1.919267015182413e-05, 1.919267015182413e-05, 1.919267015182413e-05, 1.919267015182413e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.919267015182413e-05

Optimization complete. Final v2v error: 3.725104808807373 mm

Highest mean error: 4.410882472991943 mm for frame 66

Lowest mean error: 3.433535575866699 mm for frame 0

Saving results

Total time: 33.012444257736206
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_36_nl_6641/0019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_nl_6641/0019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_nl_6641/0019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00481004
Iteration 2/25 | Loss: 0.00155009
Iteration 3/25 | Loss: 0.00128705
Iteration 4/25 | Loss: 0.00125601
Iteration 5/25 | Loss: 0.00124951
Iteration 6/25 | Loss: 0.00124747
Iteration 7/25 | Loss: 0.00124689
Iteration 8/25 | Loss: 0.00124689
Iteration 9/25 | Loss: 0.00124689
Iteration 10/25 | Loss: 0.00124689
Iteration 11/25 | Loss: 0.00124689
Iteration 12/25 | Loss: 0.00124689
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0012468938948586583, 0.0012468938948586583, 0.0012468938948586583, 0.0012468938948586583, 0.0012468938948586583]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012468938948586583

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.23668373
Iteration 2/25 | Loss: 0.00054846
Iteration 3/25 | Loss: 0.00054844
Iteration 4/25 | Loss: 0.00054844
Iteration 5/25 | Loss: 0.00054844
Iteration 6/25 | Loss: 0.00054844
Iteration 7/25 | Loss: 0.00054844
Iteration 8/25 | Loss: 0.00054844
Iteration 9/25 | Loss: 0.00054844
Iteration 10/25 | Loss: 0.00054844
Iteration 11/25 | Loss: 0.00054844
Iteration 12/25 | Loss: 0.00054844
Iteration 13/25 | Loss: 0.00054844
Iteration 14/25 | Loss: 0.00054844
Iteration 15/25 | Loss: 0.00054844
Iteration 16/25 | Loss: 0.00054844
Iteration 17/25 | Loss: 0.00054844
Iteration 18/25 | Loss: 0.00054844
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0005484411376528442, 0.0005484411376528442, 0.0005484411376528442, 0.0005484411376528442, 0.0005484411376528442]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005484411376528442

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00054844
Iteration 2/1000 | Loss: 0.00008555
Iteration 3/1000 | Loss: 0.00006228
Iteration 4/1000 | Loss: 0.00004847
Iteration 5/1000 | Loss: 0.00004179
Iteration 6/1000 | Loss: 0.00003921
Iteration 7/1000 | Loss: 0.00003728
Iteration 8/1000 | Loss: 0.00003607
Iteration 9/1000 | Loss: 0.00003484
Iteration 10/1000 | Loss: 0.00003441
Iteration 11/1000 | Loss: 0.00003405
Iteration 12/1000 | Loss: 0.00003357
Iteration 13/1000 | Loss: 0.00003327
Iteration 14/1000 | Loss: 0.00003300
Iteration 15/1000 | Loss: 0.00003297
Iteration 16/1000 | Loss: 0.00003283
Iteration 17/1000 | Loss: 0.00003265
Iteration 18/1000 | Loss: 0.00003260
Iteration 19/1000 | Loss: 0.00003260
Iteration 20/1000 | Loss: 0.00003260
Iteration 21/1000 | Loss: 0.00003260
Iteration 22/1000 | Loss: 0.00003259
Iteration 23/1000 | Loss: 0.00003258
Iteration 24/1000 | Loss: 0.00003253
Iteration 25/1000 | Loss: 0.00003253
Iteration 26/1000 | Loss: 0.00003252
Iteration 27/1000 | Loss: 0.00003245
Iteration 28/1000 | Loss: 0.00003245
Iteration 29/1000 | Loss: 0.00003245
Iteration 30/1000 | Loss: 0.00003241
Iteration 31/1000 | Loss: 0.00003241
Iteration 32/1000 | Loss: 0.00003241
Iteration 33/1000 | Loss: 0.00003241
Iteration 34/1000 | Loss: 0.00003241
Iteration 35/1000 | Loss: 0.00003240
Iteration 36/1000 | Loss: 0.00003240
Iteration 37/1000 | Loss: 0.00003240
Iteration 38/1000 | Loss: 0.00003240
Iteration 39/1000 | Loss: 0.00003240
Iteration 40/1000 | Loss: 0.00003240
Iteration 41/1000 | Loss: 0.00003240
Iteration 42/1000 | Loss: 0.00003240
Iteration 43/1000 | Loss: 0.00003240
Iteration 44/1000 | Loss: 0.00003237
Iteration 45/1000 | Loss: 0.00003237
Iteration 46/1000 | Loss: 0.00003236
Iteration 47/1000 | Loss: 0.00003236
Iteration 48/1000 | Loss: 0.00003236
Iteration 49/1000 | Loss: 0.00003236
Iteration 50/1000 | Loss: 0.00003236
Iteration 51/1000 | Loss: 0.00003235
Iteration 52/1000 | Loss: 0.00003235
Iteration 53/1000 | Loss: 0.00003235
Iteration 54/1000 | Loss: 0.00003235
Iteration 55/1000 | Loss: 0.00003235
Iteration 56/1000 | Loss: 0.00003235
Iteration 57/1000 | Loss: 0.00003235
Iteration 58/1000 | Loss: 0.00003234
Iteration 59/1000 | Loss: 0.00003234
Iteration 60/1000 | Loss: 0.00003234
Iteration 61/1000 | Loss: 0.00003234
Iteration 62/1000 | Loss: 0.00003232
Iteration 63/1000 | Loss: 0.00003232
Iteration 64/1000 | Loss: 0.00003232
Iteration 65/1000 | Loss: 0.00003232
Iteration 66/1000 | Loss: 0.00003232
Iteration 67/1000 | Loss: 0.00003232
Iteration 68/1000 | Loss: 0.00003231
Iteration 69/1000 | Loss: 0.00003231
Iteration 70/1000 | Loss: 0.00003231
Iteration 71/1000 | Loss: 0.00003231
Iteration 72/1000 | Loss: 0.00003231
Iteration 73/1000 | Loss: 0.00003231
Iteration 74/1000 | Loss: 0.00003230
Iteration 75/1000 | Loss: 0.00003230
Iteration 76/1000 | Loss: 0.00003230
Iteration 77/1000 | Loss: 0.00003230
Iteration 78/1000 | Loss: 0.00003230
Iteration 79/1000 | Loss: 0.00003230
Iteration 80/1000 | Loss: 0.00003230
Iteration 81/1000 | Loss: 0.00003230
Iteration 82/1000 | Loss: 0.00003230
Iteration 83/1000 | Loss: 0.00003229
Iteration 84/1000 | Loss: 0.00003229
Iteration 85/1000 | Loss: 0.00003227
Iteration 86/1000 | Loss: 0.00003227
Iteration 87/1000 | Loss: 0.00003227
Iteration 88/1000 | Loss: 0.00003227
Iteration 89/1000 | Loss: 0.00003227
Iteration 90/1000 | Loss: 0.00003227
Iteration 91/1000 | Loss: 0.00003227
Iteration 92/1000 | Loss: 0.00003226
Iteration 93/1000 | Loss: 0.00003226
Iteration 94/1000 | Loss: 0.00003226
Iteration 95/1000 | Loss: 0.00003226
Iteration 96/1000 | Loss: 0.00003225
Iteration 97/1000 | Loss: 0.00003225
Iteration 98/1000 | Loss: 0.00003224
Iteration 99/1000 | Loss: 0.00003223
Iteration 100/1000 | Loss: 0.00003223
Iteration 101/1000 | Loss: 0.00003223
Iteration 102/1000 | Loss: 0.00003223
Iteration 103/1000 | Loss: 0.00003223
Iteration 104/1000 | Loss: 0.00003222
Iteration 105/1000 | Loss: 0.00003222
Iteration 106/1000 | Loss: 0.00003222
Iteration 107/1000 | Loss: 0.00003222
Iteration 108/1000 | Loss: 0.00003222
Iteration 109/1000 | Loss: 0.00003221
Iteration 110/1000 | Loss: 0.00003221
Iteration 111/1000 | Loss: 0.00003221
Iteration 112/1000 | Loss: 0.00003220
Iteration 113/1000 | Loss: 0.00003220
Iteration 114/1000 | Loss: 0.00003220
Iteration 115/1000 | Loss: 0.00003220
Iteration 116/1000 | Loss: 0.00003219
Iteration 117/1000 | Loss: 0.00003218
Iteration 118/1000 | Loss: 0.00003218
Iteration 119/1000 | Loss: 0.00003218
Iteration 120/1000 | Loss: 0.00003217
Iteration 121/1000 | Loss: 0.00003217
Iteration 122/1000 | Loss: 0.00003217
Iteration 123/1000 | Loss: 0.00003217
Iteration 124/1000 | Loss: 0.00003217
Iteration 125/1000 | Loss: 0.00003217
Iteration 126/1000 | Loss: 0.00003216
Iteration 127/1000 | Loss: 0.00003216
Iteration 128/1000 | Loss: 0.00003216
Iteration 129/1000 | Loss: 0.00003216
Iteration 130/1000 | Loss: 0.00003216
Iteration 131/1000 | Loss: 0.00003216
Iteration 132/1000 | Loss: 0.00003216
Iteration 133/1000 | Loss: 0.00003216
Iteration 134/1000 | Loss: 0.00003215
Iteration 135/1000 | Loss: 0.00003215
Iteration 136/1000 | Loss: 0.00003215
Iteration 137/1000 | Loss: 0.00003213
Iteration 138/1000 | Loss: 0.00003213
Iteration 139/1000 | Loss: 0.00003213
Iteration 140/1000 | Loss: 0.00003213
Iteration 141/1000 | Loss: 0.00003213
Iteration 142/1000 | Loss: 0.00003212
Iteration 143/1000 | Loss: 0.00003212
Iteration 144/1000 | Loss: 0.00003212
Iteration 145/1000 | Loss: 0.00003212
Iteration 146/1000 | Loss: 0.00003212
Iteration 147/1000 | Loss: 0.00003212
Iteration 148/1000 | Loss: 0.00003212
Iteration 149/1000 | Loss: 0.00003212
Iteration 150/1000 | Loss: 0.00003212
Iteration 151/1000 | Loss: 0.00003212
Iteration 152/1000 | Loss: 0.00003212
Iteration 153/1000 | Loss: 0.00003212
Iteration 154/1000 | Loss: 0.00003212
Iteration 155/1000 | Loss: 0.00003211
Iteration 156/1000 | Loss: 0.00003211
Iteration 157/1000 | Loss: 0.00003211
Iteration 158/1000 | Loss: 0.00003211
Iteration 159/1000 | Loss: 0.00003211
Iteration 160/1000 | Loss: 0.00003210
Iteration 161/1000 | Loss: 0.00003210
Iteration 162/1000 | Loss: 0.00003210
Iteration 163/1000 | Loss: 0.00003210
Iteration 164/1000 | Loss: 0.00003210
Iteration 165/1000 | Loss: 0.00003210
Iteration 166/1000 | Loss: 0.00003209
Iteration 167/1000 | Loss: 0.00003209
Iteration 168/1000 | Loss: 0.00003209
Iteration 169/1000 | Loss: 0.00003209
Iteration 170/1000 | Loss: 0.00003209
Iteration 171/1000 | Loss: 0.00003209
Iteration 172/1000 | Loss: 0.00003209
Iteration 173/1000 | Loss: 0.00003209
Iteration 174/1000 | Loss: 0.00003209
Iteration 175/1000 | Loss: 0.00003209
Iteration 176/1000 | Loss: 0.00003208
Iteration 177/1000 | Loss: 0.00003208
Iteration 178/1000 | Loss: 0.00003208
Iteration 179/1000 | Loss: 0.00003208
Iteration 180/1000 | Loss: 0.00003208
Iteration 181/1000 | Loss: 0.00003208
Iteration 182/1000 | Loss: 0.00003208
Iteration 183/1000 | Loss: 0.00003207
Iteration 184/1000 | Loss: 0.00003207
Iteration 185/1000 | Loss: 0.00003207
Iteration 186/1000 | Loss: 0.00003207
Iteration 187/1000 | Loss: 0.00003207
Iteration 188/1000 | Loss: 0.00003207
Iteration 189/1000 | Loss: 0.00003207
Iteration 190/1000 | Loss: 0.00003206
Iteration 191/1000 | Loss: 0.00003206
Iteration 192/1000 | Loss: 0.00003206
Iteration 193/1000 | Loss: 0.00003206
Iteration 194/1000 | Loss: 0.00003205
Iteration 195/1000 | Loss: 0.00003205
Iteration 196/1000 | Loss: 0.00003205
Iteration 197/1000 | Loss: 0.00003205
Iteration 198/1000 | Loss: 0.00003205
Iteration 199/1000 | Loss: 0.00003205
Iteration 200/1000 | Loss: 0.00003205
Iteration 201/1000 | Loss: 0.00003205
Iteration 202/1000 | Loss: 0.00003205
Iteration 203/1000 | Loss: 0.00003205
Iteration 204/1000 | Loss: 0.00003205
Iteration 205/1000 | Loss: 0.00003205
Iteration 206/1000 | Loss: 0.00003204
Iteration 207/1000 | Loss: 0.00003204
Iteration 208/1000 | Loss: 0.00003204
Iteration 209/1000 | Loss: 0.00003204
Iteration 210/1000 | Loss: 0.00003204
Iteration 211/1000 | Loss: 0.00003204
Iteration 212/1000 | Loss: 0.00003204
Iteration 213/1000 | Loss: 0.00003203
Iteration 214/1000 | Loss: 0.00003203
Iteration 215/1000 | Loss: 0.00003203
Iteration 216/1000 | Loss: 0.00003203
Iteration 217/1000 | Loss: 0.00003203
Iteration 218/1000 | Loss: 0.00003203
Iteration 219/1000 | Loss: 0.00003203
Iteration 220/1000 | Loss: 0.00003203
Iteration 221/1000 | Loss: 0.00003203
Iteration 222/1000 | Loss: 0.00003203
Iteration 223/1000 | Loss: 0.00003203
Iteration 224/1000 | Loss: 0.00003202
Iteration 225/1000 | Loss: 0.00003202
Iteration 226/1000 | Loss: 0.00003202
Iteration 227/1000 | Loss: 0.00003202
Iteration 228/1000 | Loss: 0.00003202
Iteration 229/1000 | Loss: 0.00003202
Iteration 230/1000 | Loss: 0.00003202
Iteration 231/1000 | Loss: 0.00003201
Iteration 232/1000 | Loss: 0.00003201
Iteration 233/1000 | Loss: 0.00003201
Iteration 234/1000 | Loss: 0.00003201
Iteration 235/1000 | Loss: 0.00003201
Iteration 236/1000 | Loss: 0.00003201
Iteration 237/1000 | Loss: 0.00003201
Iteration 238/1000 | Loss: 0.00003201
Iteration 239/1000 | Loss: 0.00003201
Iteration 240/1000 | Loss: 0.00003201
Iteration 241/1000 | Loss: 0.00003201
Iteration 242/1000 | Loss: 0.00003201
Iteration 243/1000 | Loss: 0.00003201
Iteration 244/1000 | Loss: 0.00003201
Iteration 245/1000 | Loss: 0.00003200
Iteration 246/1000 | Loss: 0.00003200
Iteration 247/1000 | Loss: 0.00003200
Iteration 248/1000 | Loss: 0.00003200
Iteration 249/1000 | Loss: 0.00003200
Iteration 250/1000 | Loss: 0.00003200
Iteration 251/1000 | Loss: 0.00003200
Iteration 252/1000 | Loss: 0.00003200
Iteration 253/1000 | Loss: 0.00003200
Iteration 254/1000 | Loss: 0.00003200
Iteration 255/1000 | Loss: 0.00003200
Iteration 256/1000 | Loss: 0.00003200
Iteration 257/1000 | Loss: 0.00003200
Iteration 258/1000 | Loss: 0.00003200
Iteration 259/1000 | Loss: 0.00003200
Iteration 260/1000 | Loss: 0.00003200
Iteration 261/1000 | Loss: 0.00003200
Iteration 262/1000 | Loss: 0.00003200
Iteration 263/1000 | Loss: 0.00003200
Iteration 264/1000 | Loss: 0.00003200
Iteration 265/1000 | Loss: 0.00003200
Iteration 266/1000 | Loss: 0.00003200
Iteration 267/1000 | Loss: 0.00003200
Iteration 268/1000 | Loss: 0.00003200
Iteration 269/1000 | Loss: 0.00003200
Iteration 270/1000 | Loss: 0.00003200
Iteration 271/1000 | Loss: 0.00003200
Iteration 272/1000 | Loss: 0.00003200
Iteration 273/1000 | Loss: 0.00003200
Iteration 274/1000 | Loss: 0.00003200
Iteration 275/1000 | Loss: 0.00003200
Iteration 276/1000 | Loss: 0.00003200
Iteration 277/1000 | Loss: 0.00003200
Iteration 278/1000 | Loss: 0.00003200
Iteration 279/1000 | Loss: 0.00003200
Iteration 280/1000 | Loss: 0.00003200
Iteration 281/1000 | Loss: 0.00003200
Iteration 282/1000 | Loss: 0.00003200
Iteration 283/1000 | Loss: 0.00003200
Iteration 284/1000 | Loss: 0.00003200
Iteration 285/1000 | Loss: 0.00003200
Iteration 286/1000 | Loss: 0.00003200
Iteration 287/1000 | Loss: 0.00003200
Iteration 288/1000 | Loss: 0.00003200
Iteration 289/1000 | Loss: 0.00003200
Iteration 290/1000 | Loss: 0.00003200
Iteration 291/1000 | Loss: 0.00003200
Iteration 292/1000 | Loss: 0.00003200
Iteration 293/1000 | Loss: 0.00003200
Iteration 294/1000 | Loss: 0.00003200
Iteration 295/1000 | Loss: 0.00003200
Iteration 296/1000 | Loss: 0.00003200
Iteration 297/1000 | Loss: 0.00003200
Iteration 298/1000 | Loss: 0.00003200
Iteration 299/1000 | Loss: 0.00003200
Iteration 300/1000 | Loss: 0.00003200
Iteration 301/1000 | Loss: 0.00003200
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 301. Stopping optimization.
Last 5 losses: [3.1999363272916526e-05, 3.1999363272916526e-05, 3.1999363272916526e-05, 3.1999363272916526e-05, 3.1999363272916526e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.1999363272916526e-05

Optimization complete. Final v2v error: 4.57190465927124 mm

Highest mean error: 5.986494541168213 mm for frame 80

Lowest mean error: 3.7667839527130127 mm for frame 49

Saving results

Total time: 52.90632367134094
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_36_nl_6641/0008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_nl_6641/0008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_nl_6641/0008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00323586
Iteration 2/25 | Loss: 0.00146050
Iteration 3/25 | Loss: 0.00130663
Iteration 4/25 | Loss: 0.00128915
Iteration 5/25 | Loss: 0.00128329
Iteration 6/25 | Loss: 0.00128142
Iteration 7/25 | Loss: 0.00128062
Iteration 8/25 | Loss: 0.00128036
Iteration 9/25 | Loss: 0.00128036
Iteration 10/25 | Loss: 0.00128036
Iteration 11/25 | Loss: 0.00128036
Iteration 12/25 | Loss: 0.00128036
Iteration 13/25 | Loss: 0.00128036
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0012803622521460056, 0.0012803622521460056, 0.0012803622521460056, 0.0012803622521460056, 0.0012803622521460056]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012803622521460056

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35776937
Iteration 2/25 | Loss: 0.00068635
Iteration 3/25 | Loss: 0.00068635
Iteration 4/25 | Loss: 0.00068635
Iteration 5/25 | Loss: 0.00068635
Iteration 6/25 | Loss: 0.00068635
Iteration 7/25 | Loss: 0.00068635
Iteration 8/25 | Loss: 0.00068635
Iteration 9/25 | Loss: 0.00068635
Iteration 10/25 | Loss: 0.00068635
Iteration 11/25 | Loss: 0.00068635
Iteration 12/25 | Loss: 0.00068634
Iteration 13/25 | Loss: 0.00068634
Iteration 14/25 | Loss: 0.00068634
Iteration 15/25 | Loss: 0.00068634
Iteration 16/25 | Loss: 0.00068634
Iteration 17/25 | Loss: 0.00068634
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0006863449816592038, 0.0006863449816592038, 0.0006863449816592038, 0.0006863449816592038, 0.0006863449816592038]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006863449816592038

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00068634
Iteration 2/1000 | Loss: 0.00008313
Iteration 3/1000 | Loss: 0.00005328
Iteration 4/1000 | Loss: 0.00004154
Iteration 5/1000 | Loss: 0.00003557
Iteration 6/1000 | Loss: 0.00003253
Iteration 7/1000 | Loss: 0.00003088
Iteration 8/1000 | Loss: 0.00002933
Iteration 9/1000 | Loss: 0.00002843
Iteration 10/1000 | Loss: 0.00002773
Iteration 11/1000 | Loss: 0.00002709
Iteration 12/1000 | Loss: 0.00002663
Iteration 13/1000 | Loss: 0.00002618
Iteration 14/1000 | Loss: 0.00002590
Iteration 15/1000 | Loss: 0.00002566
Iteration 16/1000 | Loss: 0.00002545
Iteration 17/1000 | Loss: 0.00002536
Iteration 18/1000 | Loss: 0.00002531
Iteration 19/1000 | Loss: 0.00002531
Iteration 20/1000 | Loss: 0.00002529
Iteration 21/1000 | Loss: 0.00002529
Iteration 22/1000 | Loss: 0.00002528
Iteration 23/1000 | Loss: 0.00002527
Iteration 24/1000 | Loss: 0.00002527
Iteration 25/1000 | Loss: 0.00002526
Iteration 26/1000 | Loss: 0.00002526
Iteration 27/1000 | Loss: 0.00002526
Iteration 28/1000 | Loss: 0.00002526
Iteration 29/1000 | Loss: 0.00002525
Iteration 30/1000 | Loss: 0.00002525
Iteration 31/1000 | Loss: 0.00002525
Iteration 32/1000 | Loss: 0.00002525
Iteration 33/1000 | Loss: 0.00002525
Iteration 34/1000 | Loss: 0.00002525
Iteration 35/1000 | Loss: 0.00002524
Iteration 36/1000 | Loss: 0.00002524
Iteration 37/1000 | Loss: 0.00002524
Iteration 38/1000 | Loss: 0.00002524
Iteration 39/1000 | Loss: 0.00002524
Iteration 40/1000 | Loss: 0.00002524
Iteration 41/1000 | Loss: 0.00002523
Iteration 42/1000 | Loss: 0.00002523
Iteration 43/1000 | Loss: 0.00002523
Iteration 44/1000 | Loss: 0.00002522
Iteration 45/1000 | Loss: 0.00002522
Iteration 46/1000 | Loss: 0.00002522
Iteration 47/1000 | Loss: 0.00002521
Iteration 48/1000 | Loss: 0.00002521
Iteration 49/1000 | Loss: 0.00002521
Iteration 50/1000 | Loss: 0.00002520
Iteration 51/1000 | Loss: 0.00002520
Iteration 52/1000 | Loss: 0.00002520
Iteration 53/1000 | Loss: 0.00002520
Iteration 54/1000 | Loss: 0.00002519
Iteration 55/1000 | Loss: 0.00002519
Iteration 56/1000 | Loss: 0.00002519
Iteration 57/1000 | Loss: 0.00002518
Iteration 58/1000 | Loss: 0.00002518
Iteration 59/1000 | Loss: 0.00002518
Iteration 60/1000 | Loss: 0.00002517
Iteration 61/1000 | Loss: 0.00002517
Iteration 62/1000 | Loss: 0.00002516
Iteration 63/1000 | Loss: 0.00002516
Iteration 64/1000 | Loss: 0.00002515
Iteration 65/1000 | Loss: 0.00002515
Iteration 66/1000 | Loss: 0.00002515
Iteration 67/1000 | Loss: 0.00002514
Iteration 68/1000 | Loss: 0.00002514
Iteration 69/1000 | Loss: 0.00002514
Iteration 70/1000 | Loss: 0.00002513
Iteration 71/1000 | Loss: 0.00002513
Iteration 72/1000 | Loss: 0.00002512
Iteration 73/1000 | Loss: 0.00002512
Iteration 74/1000 | Loss: 0.00002512
Iteration 75/1000 | Loss: 0.00002512
Iteration 76/1000 | Loss: 0.00002511
Iteration 77/1000 | Loss: 0.00002511
Iteration 78/1000 | Loss: 0.00002511
Iteration 79/1000 | Loss: 0.00002510
Iteration 80/1000 | Loss: 0.00002510
Iteration 81/1000 | Loss: 0.00002510
Iteration 82/1000 | Loss: 0.00002509
Iteration 83/1000 | Loss: 0.00002509
Iteration 84/1000 | Loss: 0.00002509
Iteration 85/1000 | Loss: 0.00002508
Iteration 86/1000 | Loss: 0.00002508
Iteration 87/1000 | Loss: 0.00002508
Iteration 88/1000 | Loss: 0.00002507
Iteration 89/1000 | Loss: 0.00002507
Iteration 90/1000 | Loss: 0.00002507
Iteration 91/1000 | Loss: 0.00002507
Iteration 92/1000 | Loss: 0.00002507
Iteration 93/1000 | Loss: 0.00002507
Iteration 94/1000 | Loss: 0.00002506
Iteration 95/1000 | Loss: 0.00002506
Iteration 96/1000 | Loss: 0.00002506
Iteration 97/1000 | Loss: 0.00002506
Iteration 98/1000 | Loss: 0.00002506
Iteration 99/1000 | Loss: 0.00002505
Iteration 100/1000 | Loss: 0.00002505
Iteration 101/1000 | Loss: 0.00002505
Iteration 102/1000 | Loss: 0.00002505
Iteration 103/1000 | Loss: 0.00002505
Iteration 104/1000 | Loss: 0.00002504
Iteration 105/1000 | Loss: 0.00002504
Iteration 106/1000 | Loss: 0.00002504
Iteration 107/1000 | Loss: 0.00002503
Iteration 108/1000 | Loss: 0.00002503
Iteration 109/1000 | Loss: 0.00002503
Iteration 110/1000 | Loss: 0.00002502
Iteration 111/1000 | Loss: 0.00002502
Iteration 112/1000 | Loss: 0.00002501
Iteration 113/1000 | Loss: 0.00002501
Iteration 114/1000 | Loss: 0.00002501
Iteration 115/1000 | Loss: 0.00002501
Iteration 116/1000 | Loss: 0.00002501
Iteration 117/1000 | Loss: 0.00002501
Iteration 118/1000 | Loss: 0.00002501
Iteration 119/1000 | Loss: 0.00002501
Iteration 120/1000 | Loss: 0.00002501
Iteration 121/1000 | Loss: 0.00002501
Iteration 122/1000 | Loss: 0.00002500
Iteration 123/1000 | Loss: 0.00002500
Iteration 124/1000 | Loss: 0.00002500
Iteration 125/1000 | Loss: 0.00002499
Iteration 126/1000 | Loss: 0.00002499
Iteration 127/1000 | Loss: 0.00002499
Iteration 128/1000 | Loss: 0.00002499
Iteration 129/1000 | Loss: 0.00002499
Iteration 130/1000 | Loss: 0.00002499
Iteration 131/1000 | Loss: 0.00002498
Iteration 132/1000 | Loss: 0.00002498
Iteration 133/1000 | Loss: 0.00002498
Iteration 134/1000 | Loss: 0.00002498
Iteration 135/1000 | Loss: 0.00002498
Iteration 136/1000 | Loss: 0.00002498
Iteration 137/1000 | Loss: 0.00002498
Iteration 138/1000 | Loss: 0.00002498
Iteration 139/1000 | Loss: 0.00002497
Iteration 140/1000 | Loss: 0.00002497
Iteration 141/1000 | Loss: 0.00002497
Iteration 142/1000 | Loss: 0.00002497
Iteration 143/1000 | Loss: 0.00002497
Iteration 144/1000 | Loss: 0.00002497
Iteration 145/1000 | Loss: 0.00002497
Iteration 146/1000 | Loss: 0.00002496
Iteration 147/1000 | Loss: 0.00002496
Iteration 148/1000 | Loss: 0.00002496
Iteration 149/1000 | Loss: 0.00002496
Iteration 150/1000 | Loss: 0.00002496
Iteration 151/1000 | Loss: 0.00002496
Iteration 152/1000 | Loss: 0.00002496
Iteration 153/1000 | Loss: 0.00002496
Iteration 154/1000 | Loss: 0.00002496
Iteration 155/1000 | Loss: 0.00002495
Iteration 156/1000 | Loss: 0.00002495
Iteration 157/1000 | Loss: 0.00002495
Iteration 158/1000 | Loss: 0.00002495
Iteration 159/1000 | Loss: 0.00002495
Iteration 160/1000 | Loss: 0.00002495
Iteration 161/1000 | Loss: 0.00002495
Iteration 162/1000 | Loss: 0.00002494
Iteration 163/1000 | Loss: 0.00002494
Iteration 164/1000 | Loss: 0.00002494
Iteration 165/1000 | Loss: 0.00002494
Iteration 166/1000 | Loss: 0.00002494
Iteration 167/1000 | Loss: 0.00002494
Iteration 168/1000 | Loss: 0.00002494
Iteration 169/1000 | Loss: 0.00002494
Iteration 170/1000 | Loss: 0.00002493
Iteration 171/1000 | Loss: 0.00002493
Iteration 172/1000 | Loss: 0.00002493
Iteration 173/1000 | Loss: 0.00002493
Iteration 174/1000 | Loss: 0.00002493
Iteration 175/1000 | Loss: 0.00002493
Iteration 176/1000 | Loss: 0.00002493
Iteration 177/1000 | Loss: 0.00002493
Iteration 178/1000 | Loss: 0.00002492
Iteration 179/1000 | Loss: 0.00002492
Iteration 180/1000 | Loss: 0.00002492
Iteration 181/1000 | Loss: 0.00002492
Iteration 182/1000 | Loss: 0.00002492
Iteration 183/1000 | Loss: 0.00002492
Iteration 184/1000 | Loss: 0.00002492
Iteration 185/1000 | Loss: 0.00002492
Iteration 186/1000 | Loss: 0.00002492
Iteration 187/1000 | Loss: 0.00002492
Iteration 188/1000 | Loss: 0.00002492
Iteration 189/1000 | Loss: 0.00002492
Iteration 190/1000 | Loss: 0.00002492
Iteration 191/1000 | Loss: 0.00002492
Iteration 192/1000 | Loss: 0.00002491
Iteration 193/1000 | Loss: 0.00002491
Iteration 194/1000 | Loss: 0.00002491
Iteration 195/1000 | Loss: 0.00002491
Iteration 196/1000 | Loss: 0.00002491
Iteration 197/1000 | Loss: 0.00002491
Iteration 198/1000 | Loss: 0.00002491
Iteration 199/1000 | Loss: 0.00002491
Iteration 200/1000 | Loss: 0.00002491
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 200. Stopping optimization.
Last 5 losses: [2.491237137292046e-05, 2.491237137292046e-05, 2.491237137292046e-05, 2.491237137292046e-05, 2.491237137292046e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.491237137292046e-05

Optimization complete. Final v2v error: 4.365710735321045 mm

Highest mean error: 4.947601318359375 mm for frame 106

Lowest mean error: 3.962484121322632 mm for frame 86

Saving results

Total time: 48.45645236968994
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_36_nl_6641/0018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_nl_6641/0018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_nl_6641/0018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00759794
Iteration 2/25 | Loss: 0.00152728
Iteration 3/25 | Loss: 0.00134761
Iteration 4/25 | Loss: 0.00131135
Iteration 5/25 | Loss: 0.00129833
Iteration 6/25 | Loss: 0.00130059
Iteration 7/25 | Loss: 0.00131624
Iteration 8/25 | Loss: 0.00128639
Iteration 9/25 | Loss: 0.00127922
Iteration 10/25 | Loss: 0.00127753
Iteration 11/25 | Loss: 0.00130459
Iteration 12/25 | Loss: 0.00127596
Iteration 13/25 | Loss: 0.00127558
Iteration 14/25 | Loss: 0.00127548
Iteration 15/25 | Loss: 0.00127539
Iteration 16/25 | Loss: 0.00129660
Iteration 17/25 | Loss: 0.00127644
Iteration 18/25 | Loss: 0.00129562
Iteration 19/25 | Loss: 0.00129808
Iteration 20/25 | Loss: 0.00129561
Iteration 21/25 | Loss: 0.00127702
Iteration 22/25 | Loss: 0.00127649
Iteration 23/25 | Loss: 0.00127617
Iteration 24/25 | Loss: 0.00127600
Iteration 25/25 | Loss: 0.00127588

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38890791
Iteration 2/25 | Loss: 0.00073024
Iteration 3/25 | Loss: 0.00073024
Iteration 4/25 | Loss: 0.00073024
Iteration 5/25 | Loss: 0.00073024
Iteration 6/25 | Loss: 0.00073024
Iteration 7/25 | Loss: 0.00073024
Iteration 8/25 | Loss: 0.00073024
Iteration 9/25 | Loss: 0.00073024
Iteration 10/25 | Loss: 0.00073024
Iteration 11/25 | Loss: 0.00073024
Iteration 12/25 | Loss: 0.00073024
Iteration 13/25 | Loss: 0.00073024
Iteration 14/25 | Loss: 0.00073024
Iteration 15/25 | Loss: 0.00073024
Iteration 16/25 | Loss: 0.00073024
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0007302358280867338, 0.0007302358280867338, 0.0007302358280867338, 0.0007302358280867338, 0.0007302358280867338]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007302358280867338

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00073024
Iteration 2/1000 | Loss: 0.00355682
Iteration 3/1000 | Loss: 0.00288337
Iteration 4/1000 | Loss: 0.00430511
Iteration 5/1000 | Loss: 0.00530505
Iteration 6/1000 | Loss: 0.00185213
Iteration 7/1000 | Loss: 0.00028004
Iteration 8/1000 | Loss: 0.00036368
Iteration 9/1000 | Loss: 0.00053165
Iteration 10/1000 | Loss: 0.00053274
Iteration 11/1000 | Loss: 0.00033710
Iteration 12/1000 | Loss: 0.00043940
Iteration 13/1000 | Loss: 0.00033637
Iteration 14/1000 | Loss: 0.00110857
Iteration 15/1000 | Loss: 0.00109613
Iteration 16/1000 | Loss: 0.00047882
Iteration 17/1000 | Loss: 0.00114144
Iteration 18/1000 | Loss: 0.00153814
Iteration 19/1000 | Loss: 0.00047091
Iteration 20/1000 | Loss: 0.00086474
Iteration 21/1000 | Loss: 0.00083982
Iteration 22/1000 | Loss: 0.00174453
Iteration 23/1000 | Loss: 0.00141525
Iteration 24/1000 | Loss: 0.00208974
Iteration 25/1000 | Loss: 0.00044370
Iteration 26/1000 | Loss: 0.00102711
Iteration 27/1000 | Loss: 0.00069261
Iteration 28/1000 | Loss: 0.00072583
Iteration 29/1000 | Loss: 0.00036345
Iteration 30/1000 | Loss: 0.00032648
Iteration 31/1000 | Loss: 0.00079307
Iteration 32/1000 | Loss: 0.00074123
Iteration 33/1000 | Loss: 0.00069855
Iteration 34/1000 | Loss: 0.00081982
Iteration 35/1000 | Loss: 0.00114628
Iteration 36/1000 | Loss: 0.00057958
Iteration 37/1000 | Loss: 0.00028179
Iteration 38/1000 | Loss: 0.00075300
Iteration 39/1000 | Loss: 0.00057025
Iteration 40/1000 | Loss: 0.00070459
Iteration 41/1000 | Loss: 0.00076610
Iteration 42/1000 | Loss: 0.00062538
Iteration 43/1000 | Loss: 0.00064913
Iteration 44/1000 | Loss: 0.00050524
Iteration 45/1000 | Loss: 0.00056024
Iteration 46/1000 | Loss: 0.00046659
Iteration 47/1000 | Loss: 0.00070474
Iteration 48/1000 | Loss: 0.00064078
Iteration 49/1000 | Loss: 0.00092599
Iteration 50/1000 | Loss: 0.00045723
Iteration 51/1000 | Loss: 0.00069752
Iteration 52/1000 | Loss: 0.00065982
Iteration 53/1000 | Loss: 0.00040182
Iteration 54/1000 | Loss: 0.00006886
Iteration 55/1000 | Loss: 0.00005067
Iteration 56/1000 | Loss: 0.00004114
Iteration 57/1000 | Loss: 0.00003618
Iteration 58/1000 | Loss: 0.00003408
Iteration 59/1000 | Loss: 0.00003186
Iteration 60/1000 | Loss: 0.00003058
Iteration 61/1000 | Loss: 0.00002936
Iteration 62/1000 | Loss: 0.00038756
Iteration 63/1000 | Loss: 0.00005290
Iteration 64/1000 | Loss: 0.00028125
Iteration 65/1000 | Loss: 0.00058932
Iteration 66/1000 | Loss: 0.00003873
Iteration 67/1000 | Loss: 0.00003105
Iteration 68/1000 | Loss: 0.00024514
Iteration 69/1000 | Loss: 0.00016777
Iteration 70/1000 | Loss: 0.00014232
Iteration 71/1000 | Loss: 0.00031559
Iteration 72/1000 | Loss: 0.00038656
Iteration 73/1000 | Loss: 0.00039466
Iteration 74/1000 | Loss: 0.00055394
Iteration 75/1000 | Loss: 0.00030958
Iteration 76/1000 | Loss: 0.00003239
Iteration 77/1000 | Loss: 0.00016811
Iteration 78/1000 | Loss: 0.00003772
Iteration 79/1000 | Loss: 0.00012637
Iteration 80/1000 | Loss: 0.00010288
Iteration 81/1000 | Loss: 0.00003042
Iteration 82/1000 | Loss: 0.00002882
Iteration 83/1000 | Loss: 0.00002841
Iteration 84/1000 | Loss: 0.00002804
Iteration 85/1000 | Loss: 0.00002781
Iteration 86/1000 | Loss: 0.00002763
Iteration 87/1000 | Loss: 0.00002759
Iteration 88/1000 | Loss: 0.00002759
Iteration 89/1000 | Loss: 0.00002758
Iteration 90/1000 | Loss: 0.00002758
Iteration 91/1000 | Loss: 0.00002756
Iteration 92/1000 | Loss: 0.00002755
Iteration 93/1000 | Loss: 0.00002755
Iteration 94/1000 | Loss: 0.00002754
Iteration 95/1000 | Loss: 0.00002754
Iteration 96/1000 | Loss: 0.00002753
Iteration 97/1000 | Loss: 0.00002752
Iteration 98/1000 | Loss: 0.00002751
Iteration 99/1000 | Loss: 0.00002751
Iteration 100/1000 | Loss: 0.00032176
Iteration 101/1000 | Loss: 0.00005761
Iteration 102/1000 | Loss: 0.00030326
Iteration 103/1000 | Loss: 0.00004032
Iteration 104/1000 | Loss: 0.00003617
Iteration 105/1000 | Loss: 0.00009156
Iteration 106/1000 | Loss: 0.00049557
Iteration 107/1000 | Loss: 0.00031707
Iteration 108/1000 | Loss: 0.00037379
Iteration 109/1000 | Loss: 0.00032023
Iteration 110/1000 | Loss: 0.00051996
Iteration 111/1000 | Loss: 0.00005837
Iteration 112/1000 | Loss: 0.00003331
Iteration 113/1000 | Loss: 0.00003002
Iteration 114/1000 | Loss: 0.00002861
Iteration 115/1000 | Loss: 0.00002787
Iteration 116/1000 | Loss: 0.00028852
Iteration 117/1000 | Loss: 0.00004717
Iteration 118/1000 | Loss: 0.00004622
Iteration 119/1000 | Loss: 0.00038322
Iteration 120/1000 | Loss: 0.00044799
Iteration 121/1000 | Loss: 0.00023862
Iteration 122/1000 | Loss: 0.00026278
Iteration 123/1000 | Loss: 0.00033036
Iteration 124/1000 | Loss: 0.00004945
Iteration 125/1000 | Loss: 0.00043863
Iteration 126/1000 | Loss: 0.00014642
Iteration 127/1000 | Loss: 0.00004154
Iteration 128/1000 | Loss: 0.00051408
Iteration 129/1000 | Loss: 0.00004417
Iteration 130/1000 | Loss: 0.00003221
Iteration 131/1000 | Loss: 0.00002817
Iteration 132/1000 | Loss: 0.00002685
Iteration 133/1000 | Loss: 0.00002608
Iteration 134/1000 | Loss: 0.00002569
Iteration 135/1000 | Loss: 0.00002546
Iteration 136/1000 | Loss: 0.00005466
Iteration 137/1000 | Loss: 0.00002960
Iteration 138/1000 | Loss: 0.00002668
Iteration 139/1000 | Loss: 0.00002522
Iteration 140/1000 | Loss: 0.00002488
Iteration 141/1000 | Loss: 0.00002485
Iteration 142/1000 | Loss: 0.00002480
Iteration 143/1000 | Loss: 0.00002457
Iteration 144/1000 | Loss: 0.00002438
Iteration 145/1000 | Loss: 0.00002437
Iteration 146/1000 | Loss: 0.00002437
Iteration 147/1000 | Loss: 0.00002436
Iteration 148/1000 | Loss: 0.00002435
Iteration 149/1000 | Loss: 0.00002434
Iteration 150/1000 | Loss: 0.00002434
Iteration 151/1000 | Loss: 0.00002434
Iteration 152/1000 | Loss: 0.00002434
Iteration 153/1000 | Loss: 0.00002433
Iteration 154/1000 | Loss: 0.00002433
Iteration 155/1000 | Loss: 0.00002433
Iteration 156/1000 | Loss: 0.00002433
Iteration 157/1000 | Loss: 0.00002433
Iteration 158/1000 | Loss: 0.00002432
Iteration 159/1000 | Loss: 0.00002432
Iteration 160/1000 | Loss: 0.00002431
Iteration 161/1000 | Loss: 0.00002431
Iteration 162/1000 | Loss: 0.00002430
Iteration 163/1000 | Loss: 0.00002429
Iteration 164/1000 | Loss: 0.00002429
Iteration 165/1000 | Loss: 0.00002426
Iteration 166/1000 | Loss: 0.00002425
Iteration 167/1000 | Loss: 0.00002422
Iteration 168/1000 | Loss: 0.00002420
Iteration 169/1000 | Loss: 0.00002413
Iteration 170/1000 | Loss: 0.00002413
Iteration 171/1000 | Loss: 0.00002413
Iteration 172/1000 | Loss: 0.00002412
Iteration 173/1000 | Loss: 0.00002412
Iteration 174/1000 | Loss: 0.00002411
Iteration 175/1000 | Loss: 0.00002409
Iteration 176/1000 | Loss: 0.00002409
Iteration 177/1000 | Loss: 0.00002406
Iteration 178/1000 | Loss: 0.00002406
Iteration 179/1000 | Loss: 0.00002406
Iteration 180/1000 | Loss: 0.00002406
Iteration 181/1000 | Loss: 0.00002406
Iteration 182/1000 | Loss: 0.00002405
Iteration 183/1000 | Loss: 0.00002405
Iteration 184/1000 | Loss: 0.00002405
Iteration 185/1000 | Loss: 0.00002405
Iteration 186/1000 | Loss: 0.00002404
Iteration 187/1000 | Loss: 0.00002404
Iteration 188/1000 | Loss: 0.00002404
Iteration 189/1000 | Loss: 0.00002404
Iteration 190/1000 | Loss: 0.00002404
Iteration 191/1000 | Loss: 0.00002404
Iteration 192/1000 | Loss: 0.00002403
Iteration 193/1000 | Loss: 0.00002403
Iteration 194/1000 | Loss: 0.00002403
Iteration 195/1000 | Loss: 0.00002403
Iteration 196/1000 | Loss: 0.00002403
Iteration 197/1000 | Loss: 0.00002403
Iteration 198/1000 | Loss: 0.00002403
Iteration 199/1000 | Loss: 0.00002403
Iteration 200/1000 | Loss: 0.00002403
Iteration 201/1000 | Loss: 0.00002402
Iteration 202/1000 | Loss: 0.00002402
Iteration 203/1000 | Loss: 0.00002402
Iteration 204/1000 | Loss: 0.00002402
Iteration 205/1000 | Loss: 0.00002402
Iteration 206/1000 | Loss: 0.00002402
Iteration 207/1000 | Loss: 0.00002402
Iteration 208/1000 | Loss: 0.00002402
Iteration 209/1000 | Loss: 0.00002402
Iteration 210/1000 | Loss: 0.00002402
Iteration 211/1000 | Loss: 0.00002402
Iteration 212/1000 | Loss: 0.00002401
Iteration 213/1000 | Loss: 0.00002401
Iteration 214/1000 | Loss: 0.00002401
Iteration 215/1000 | Loss: 0.00002401
Iteration 216/1000 | Loss: 0.00002400
Iteration 217/1000 | Loss: 0.00002400
Iteration 218/1000 | Loss: 0.00002400
Iteration 219/1000 | Loss: 0.00002400
Iteration 220/1000 | Loss: 0.00002400
Iteration 221/1000 | Loss: 0.00002400
Iteration 222/1000 | Loss: 0.00002400
Iteration 223/1000 | Loss: 0.00002400
Iteration 224/1000 | Loss: 0.00002400
Iteration 225/1000 | Loss: 0.00002400
Iteration 226/1000 | Loss: 0.00002400
Iteration 227/1000 | Loss: 0.00002400
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 227. Stopping optimization.
Last 5 losses: [2.4002751160878688e-05, 2.4002751160878688e-05, 2.4002751160878688e-05, 2.4002751160878688e-05, 2.4002751160878688e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4002751160878688e-05

Optimization complete. Final v2v error: 4.206105709075928 mm

Highest mean error: 5.500134468078613 mm for frame 134

Lowest mean error: 3.825040340423584 mm for frame 208

Saving results

Total time: 267.4287326335907
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_36_nl_6641/0000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_nl_6641/0000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_nl_6641/0000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00504072
Iteration 2/25 | Loss: 0.00156154
Iteration 3/25 | Loss: 0.00123468
Iteration 4/25 | Loss: 0.00117988
Iteration 5/25 | Loss: 0.00117095
Iteration 6/25 | Loss: 0.00116888
Iteration 7/25 | Loss: 0.00116882
Iteration 8/25 | Loss: 0.00116872
Iteration 9/25 | Loss: 0.00116872
Iteration 10/25 | Loss: 0.00116872
Iteration 11/25 | Loss: 0.00116872
Iteration 12/25 | Loss: 0.00116872
Iteration 13/25 | Loss: 0.00116872
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0011687196092680097, 0.0011687196092680097, 0.0011687196092680097, 0.0011687196092680097, 0.0011687196092680097]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011687196092680097

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39483285
Iteration 2/25 | Loss: 0.00051995
Iteration 3/25 | Loss: 0.00051995
Iteration 4/25 | Loss: 0.00051995
Iteration 5/25 | Loss: 0.00051995
Iteration 6/25 | Loss: 0.00051995
Iteration 7/25 | Loss: 0.00051995
Iteration 8/25 | Loss: 0.00051995
Iteration 9/25 | Loss: 0.00051995
Iteration 10/25 | Loss: 0.00051995
Iteration 11/25 | Loss: 0.00051995
Iteration 12/25 | Loss: 0.00051995
Iteration 13/25 | Loss: 0.00051995
Iteration 14/25 | Loss: 0.00051995
Iteration 15/25 | Loss: 0.00051995
Iteration 16/25 | Loss: 0.00051995
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.00051994709065184, 0.00051994709065184, 0.00051994709065184, 0.00051994709065184, 0.00051994709065184]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00051994709065184

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00051995
Iteration 2/1000 | Loss: 0.00005851
Iteration 3/1000 | Loss: 0.00003415
Iteration 4/1000 | Loss: 0.00002732
Iteration 5/1000 | Loss: 0.00002424
Iteration 6/1000 | Loss: 0.00002264
Iteration 7/1000 | Loss: 0.00002165
Iteration 8/1000 | Loss: 0.00002113
Iteration 9/1000 | Loss: 0.00002075
Iteration 10/1000 | Loss: 0.00002033
Iteration 11/1000 | Loss: 0.00002031
Iteration 12/1000 | Loss: 0.00002013
Iteration 13/1000 | Loss: 0.00002009
Iteration 14/1000 | Loss: 0.00002008
Iteration 15/1000 | Loss: 0.00002007
Iteration 16/1000 | Loss: 0.00002007
Iteration 17/1000 | Loss: 0.00002006
Iteration 18/1000 | Loss: 0.00002006
Iteration 19/1000 | Loss: 0.00002003
Iteration 20/1000 | Loss: 0.00002003
Iteration 21/1000 | Loss: 0.00002002
Iteration 22/1000 | Loss: 0.00002001
Iteration 23/1000 | Loss: 0.00002000
Iteration 24/1000 | Loss: 0.00002000
Iteration 25/1000 | Loss: 0.00002000
Iteration 26/1000 | Loss: 0.00001999
Iteration 27/1000 | Loss: 0.00001999
Iteration 28/1000 | Loss: 0.00001999
Iteration 29/1000 | Loss: 0.00001999
Iteration 30/1000 | Loss: 0.00001998
Iteration 31/1000 | Loss: 0.00001998
Iteration 32/1000 | Loss: 0.00001998
Iteration 33/1000 | Loss: 0.00001998
Iteration 34/1000 | Loss: 0.00001998
Iteration 35/1000 | Loss: 0.00001998
Iteration 36/1000 | Loss: 0.00001998
Iteration 37/1000 | Loss: 0.00001998
Iteration 38/1000 | Loss: 0.00001998
Iteration 39/1000 | Loss: 0.00001998
Iteration 40/1000 | Loss: 0.00001998
Iteration 41/1000 | Loss: 0.00001998
Iteration 42/1000 | Loss: 0.00001998
Iteration 43/1000 | Loss: 0.00001998
Iteration 44/1000 | Loss: 0.00001997
Iteration 45/1000 | Loss: 0.00001996
Iteration 46/1000 | Loss: 0.00001996
Iteration 47/1000 | Loss: 0.00001996
Iteration 48/1000 | Loss: 0.00001995
Iteration 49/1000 | Loss: 0.00001995
Iteration 50/1000 | Loss: 0.00001995
Iteration 51/1000 | Loss: 0.00001995
Iteration 52/1000 | Loss: 0.00001995
Iteration 53/1000 | Loss: 0.00001995
Iteration 54/1000 | Loss: 0.00001994
Iteration 55/1000 | Loss: 0.00001994
Iteration 56/1000 | Loss: 0.00001994
Iteration 57/1000 | Loss: 0.00001993
Iteration 58/1000 | Loss: 0.00001993
Iteration 59/1000 | Loss: 0.00001993
Iteration 60/1000 | Loss: 0.00001993
Iteration 61/1000 | Loss: 0.00001993
Iteration 62/1000 | Loss: 0.00001993
Iteration 63/1000 | Loss: 0.00001992
Iteration 64/1000 | Loss: 0.00001992
Iteration 65/1000 | Loss: 0.00001992
Iteration 66/1000 | Loss: 0.00001992
Iteration 67/1000 | Loss: 0.00001992
Iteration 68/1000 | Loss: 0.00001991
Iteration 69/1000 | Loss: 0.00001991
Iteration 70/1000 | Loss: 0.00001990
Iteration 71/1000 | Loss: 0.00001990
Iteration 72/1000 | Loss: 0.00001990
Iteration 73/1000 | Loss: 0.00001990
Iteration 74/1000 | Loss: 0.00001990
Iteration 75/1000 | Loss: 0.00001990
Iteration 76/1000 | Loss: 0.00001989
Iteration 77/1000 | Loss: 0.00001989
Iteration 78/1000 | Loss: 0.00001989
Iteration 79/1000 | Loss: 0.00001989
Iteration 80/1000 | Loss: 0.00001989
Iteration 81/1000 | Loss: 0.00001988
Iteration 82/1000 | Loss: 0.00001988
Iteration 83/1000 | Loss: 0.00001988
Iteration 84/1000 | Loss: 0.00001988
Iteration 85/1000 | Loss: 0.00001988
Iteration 86/1000 | Loss: 0.00001988
Iteration 87/1000 | Loss: 0.00001988
Iteration 88/1000 | Loss: 0.00001988
Iteration 89/1000 | Loss: 0.00001988
Iteration 90/1000 | Loss: 0.00001988
Iteration 91/1000 | Loss: 0.00001988
Iteration 92/1000 | Loss: 0.00001988
Iteration 93/1000 | Loss: 0.00001987
Iteration 94/1000 | Loss: 0.00001987
Iteration 95/1000 | Loss: 0.00001987
Iteration 96/1000 | Loss: 0.00001987
Iteration 97/1000 | Loss: 0.00001987
Iteration 98/1000 | Loss: 0.00001987
Iteration 99/1000 | Loss: 0.00001987
Iteration 100/1000 | Loss: 0.00001987
Iteration 101/1000 | Loss: 0.00001987
Iteration 102/1000 | Loss: 0.00001987
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 102. Stopping optimization.
Last 5 losses: [1.9868757590302266e-05, 1.9868757590302266e-05, 1.9868757590302266e-05, 1.9868757590302266e-05, 1.9868757590302266e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9868757590302266e-05

Optimization complete. Final v2v error: 3.768317222595215 mm

Highest mean error: 4.628808498382568 mm for frame 70

Lowest mean error: 3.3721351623535156 mm for frame 168

Saving results

Total time: 32.80799174308777
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_36_nl_6641/0007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_nl_6641/0007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_nl_6641/0007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00838633
Iteration 2/25 | Loss: 0.00181053
Iteration 3/25 | Loss: 0.00130406
Iteration 4/25 | Loss: 0.00126173
Iteration 5/25 | Loss: 0.00125345
Iteration 6/25 | Loss: 0.00125038
Iteration 7/25 | Loss: 0.00124883
Iteration 8/25 | Loss: 0.00124873
Iteration 9/25 | Loss: 0.00124873
Iteration 10/25 | Loss: 0.00124873
Iteration 11/25 | Loss: 0.00124873
Iteration 12/25 | Loss: 0.00124873
Iteration 13/25 | Loss: 0.00124873
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0012487294152379036, 0.0012487294152379036, 0.0012487294152379036, 0.0012487294152379036, 0.0012487294152379036]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012487294152379036

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.27721334
Iteration 2/25 | Loss: 0.00060198
Iteration 3/25 | Loss: 0.00060198
Iteration 4/25 | Loss: 0.00060198
Iteration 5/25 | Loss: 0.00060198
Iteration 6/25 | Loss: 0.00060198
Iteration 7/25 | Loss: 0.00060198
Iteration 8/25 | Loss: 0.00060198
Iteration 9/25 | Loss: 0.00060198
Iteration 10/25 | Loss: 0.00060198
Iteration 11/25 | Loss: 0.00060198
Iteration 12/25 | Loss: 0.00060198
Iteration 13/25 | Loss: 0.00060198
Iteration 14/25 | Loss: 0.00060198
Iteration 15/25 | Loss: 0.00060198
Iteration 16/25 | Loss: 0.00060198
Iteration 17/25 | Loss: 0.00060198
Iteration 18/25 | Loss: 0.00060198
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0006019786233082414, 0.0006019786233082414, 0.0006019786233082414, 0.0006019786233082414, 0.0006019786233082414]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006019786233082414

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00060198
Iteration 2/1000 | Loss: 0.00009018
Iteration 3/1000 | Loss: 0.00004553
Iteration 4/1000 | Loss: 0.00003788
Iteration 5/1000 | Loss: 0.00003300
Iteration 6/1000 | Loss: 0.00003110
Iteration 7/1000 | Loss: 0.00002975
Iteration 8/1000 | Loss: 0.00002907
Iteration 9/1000 | Loss: 0.00002854
Iteration 10/1000 | Loss: 0.00002816
Iteration 11/1000 | Loss: 0.00002782
Iteration 12/1000 | Loss: 0.00002766
Iteration 13/1000 | Loss: 0.00002763
Iteration 14/1000 | Loss: 0.00002760
Iteration 15/1000 | Loss: 0.00002745
Iteration 16/1000 | Loss: 0.00002742
Iteration 17/1000 | Loss: 0.00002741
Iteration 18/1000 | Loss: 0.00002740
Iteration 19/1000 | Loss: 0.00002739
Iteration 20/1000 | Loss: 0.00002738
Iteration 21/1000 | Loss: 0.00002735
Iteration 22/1000 | Loss: 0.00002735
Iteration 23/1000 | Loss: 0.00002735
Iteration 24/1000 | Loss: 0.00002735
Iteration 25/1000 | Loss: 0.00002735
Iteration 26/1000 | Loss: 0.00002735
Iteration 27/1000 | Loss: 0.00002735
Iteration 28/1000 | Loss: 0.00002734
Iteration 29/1000 | Loss: 0.00002734
Iteration 30/1000 | Loss: 0.00002733
Iteration 31/1000 | Loss: 0.00002732
Iteration 32/1000 | Loss: 0.00002732
Iteration 33/1000 | Loss: 0.00002732
Iteration 34/1000 | Loss: 0.00002731
Iteration 35/1000 | Loss: 0.00002731
Iteration 36/1000 | Loss: 0.00002731
Iteration 37/1000 | Loss: 0.00002731
Iteration 38/1000 | Loss: 0.00002730
Iteration 39/1000 | Loss: 0.00002730
Iteration 40/1000 | Loss: 0.00002730
Iteration 41/1000 | Loss: 0.00002730
Iteration 42/1000 | Loss: 0.00002729
Iteration 43/1000 | Loss: 0.00002729
Iteration 44/1000 | Loss: 0.00002729
Iteration 45/1000 | Loss: 0.00002729
Iteration 46/1000 | Loss: 0.00002728
Iteration 47/1000 | Loss: 0.00002728
Iteration 48/1000 | Loss: 0.00002728
Iteration 49/1000 | Loss: 0.00002728
Iteration 50/1000 | Loss: 0.00002728
Iteration 51/1000 | Loss: 0.00002728
Iteration 52/1000 | Loss: 0.00002728
Iteration 53/1000 | Loss: 0.00002728
Iteration 54/1000 | Loss: 0.00002728
Iteration 55/1000 | Loss: 0.00002728
Iteration 56/1000 | Loss: 0.00002727
Iteration 57/1000 | Loss: 0.00002727
Iteration 58/1000 | Loss: 0.00002727
Iteration 59/1000 | Loss: 0.00002727
Iteration 60/1000 | Loss: 0.00002727
Iteration 61/1000 | Loss: 0.00002727
Iteration 62/1000 | Loss: 0.00002727
Iteration 63/1000 | Loss: 0.00002727
Iteration 64/1000 | Loss: 0.00002727
Iteration 65/1000 | Loss: 0.00002727
Iteration 66/1000 | Loss: 0.00002727
Iteration 67/1000 | Loss: 0.00002727
Iteration 68/1000 | Loss: 0.00002727
Iteration 69/1000 | Loss: 0.00002726
Iteration 70/1000 | Loss: 0.00002726
Iteration 71/1000 | Loss: 0.00002726
Iteration 72/1000 | Loss: 0.00002726
Iteration 73/1000 | Loss: 0.00002726
Iteration 74/1000 | Loss: 0.00002726
Iteration 75/1000 | Loss: 0.00002726
Iteration 76/1000 | Loss: 0.00002726
Iteration 77/1000 | Loss: 0.00002725
Iteration 78/1000 | Loss: 0.00002725
Iteration 79/1000 | Loss: 0.00002725
Iteration 80/1000 | Loss: 0.00002725
Iteration 81/1000 | Loss: 0.00002725
Iteration 82/1000 | Loss: 0.00002725
Iteration 83/1000 | Loss: 0.00002725
Iteration 84/1000 | Loss: 0.00002724
Iteration 85/1000 | Loss: 0.00002724
Iteration 86/1000 | Loss: 0.00002724
Iteration 87/1000 | Loss: 0.00002724
Iteration 88/1000 | Loss: 0.00002724
Iteration 89/1000 | Loss: 0.00002724
Iteration 90/1000 | Loss: 0.00002724
Iteration 91/1000 | Loss: 0.00002724
Iteration 92/1000 | Loss: 0.00002724
Iteration 93/1000 | Loss: 0.00002723
Iteration 94/1000 | Loss: 0.00002723
Iteration 95/1000 | Loss: 0.00002723
Iteration 96/1000 | Loss: 0.00002723
Iteration 97/1000 | Loss: 0.00002723
Iteration 98/1000 | Loss: 0.00002723
Iteration 99/1000 | Loss: 0.00002723
Iteration 100/1000 | Loss: 0.00002723
Iteration 101/1000 | Loss: 0.00002723
Iteration 102/1000 | Loss: 0.00002723
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 102. Stopping optimization.
Last 5 losses: [2.723100988077931e-05, 2.723100988077931e-05, 2.723100988077931e-05, 2.723100988077931e-05, 2.723100988077931e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.723100988077931e-05

Optimization complete. Final v2v error: 4.387330055236816 mm

Highest mean error: 5.316953659057617 mm for frame 163

Lowest mean error: 3.86061429977417 mm for frame 74

Saving results

Total time: 38.335535526275635
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_36_nl_6641/0004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_nl_6641/0004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_nl_6641/0004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00471033
Iteration 2/25 | Loss: 0.00131375
Iteration 3/25 | Loss: 0.00121979
Iteration 4/25 | Loss: 0.00120552
Iteration 5/25 | Loss: 0.00120206
Iteration 6/25 | Loss: 0.00120144
Iteration 7/25 | Loss: 0.00120144
Iteration 8/25 | Loss: 0.00120144
Iteration 9/25 | Loss: 0.00120144
Iteration 10/25 | Loss: 0.00120144
Iteration 11/25 | Loss: 0.00120144
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001201436622068286, 0.001201436622068286, 0.001201436622068286, 0.001201436622068286, 0.001201436622068286]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001201436622068286

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36713409
Iteration 2/25 | Loss: 0.00047998
Iteration 3/25 | Loss: 0.00047998
Iteration 4/25 | Loss: 0.00047998
Iteration 5/25 | Loss: 0.00047998
Iteration 6/25 | Loss: 0.00047998
Iteration 7/25 | Loss: 0.00047998
Iteration 8/25 | Loss: 0.00047998
Iteration 9/25 | Loss: 0.00047998
Iteration 10/25 | Loss: 0.00047998
Iteration 11/25 | Loss: 0.00047998
Iteration 12/25 | Loss: 0.00047998
Iteration 13/25 | Loss: 0.00047998
Iteration 14/25 | Loss: 0.00047998
Iteration 15/25 | Loss: 0.00047998
Iteration 16/25 | Loss: 0.00047998
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.00047998098307289183, 0.00047998098307289183, 0.00047998098307289183, 0.00047998098307289183, 0.00047998098307289183]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00047998098307289183

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00047998
Iteration 2/1000 | Loss: 0.00006473
Iteration 3/1000 | Loss: 0.00003615
Iteration 4/1000 | Loss: 0.00003135
Iteration 5/1000 | Loss: 0.00002890
Iteration 6/1000 | Loss: 0.00002785
Iteration 7/1000 | Loss: 0.00002743
Iteration 8/1000 | Loss: 0.00002711
Iteration 9/1000 | Loss: 0.00002674
Iteration 10/1000 | Loss: 0.00002653
Iteration 11/1000 | Loss: 0.00002643
Iteration 12/1000 | Loss: 0.00002643
Iteration 13/1000 | Loss: 0.00002641
Iteration 14/1000 | Loss: 0.00002641
Iteration 15/1000 | Loss: 0.00002641
Iteration 16/1000 | Loss: 0.00002640
Iteration 17/1000 | Loss: 0.00002640
Iteration 18/1000 | Loss: 0.00002640
Iteration 19/1000 | Loss: 0.00002640
Iteration 20/1000 | Loss: 0.00002640
Iteration 21/1000 | Loss: 0.00002640
Iteration 22/1000 | Loss: 0.00002640
Iteration 23/1000 | Loss: 0.00002640
Iteration 24/1000 | Loss: 0.00002640
Iteration 25/1000 | Loss: 0.00002640
Iteration 26/1000 | Loss: 0.00002639
Iteration 27/1000 | Loss: 0.00002639
Iteration 28/1000 | Loss: 0.00002637
Iteration 29/1000 | Loss: 0.00002637
Iteration 30/1000 | Loss: 0.00002637
Iteration 31/1000 | Loss: 0.00002637
Iteration 32/1000 | Loss: 0.00002637
Iteration 33/1000 | Loss: 0.00002637
Iteration 34/1000 | Loss: 0.00002637
Iteration 35/1000 | Loss: 0.00002637
Iteration 36/1000 | Loss: 0.00002636
Iteration 37/1000 | Loss: 0.00002636
Iteration 38/1000 | Loss: 0.00002636
Iteration 39/1000 | Loss: 0.00002636
Iteration 40/1000 | Loss: 0.00002636
Iteration 41/1000 | Loss: 0.00002636
Iteration 42/1000 | Loss: 0.00002636
Iteration 43/1000 | Loss: 0.00002636
Iteration 44/1000 | Loss: 0.00002636
Iteration 45/1000 | Loss: 0.00002636
Iteration 46/1000 | Loss: 0.00002632
Iteration 47/1000 | Loss: 0.00002632
Iteration 48/1000 | Loss: 0.00002632
Iteration 49/1000 | Loss: 0.00002632
Iteration 50/1000 | Loss: 0.00002632
Iteration 51/1000 | Loss: 0.00002632
Iteration 52/1000 | Loss: 0.00002631
Iteration 53/1000 | Loss: 0.00002631
Iteration 54/1000 | Loss: 0.00002631
Iteration 55/1000 | Loss: 0.00002630
Iteration 56/1000 | Loss: 0.00002629
Iteration 57/1000 | Loss: 0.00002628
Iteration 58/1000 | Loss: 0.00002628
Iteration 59/1000 | Loss: 0.00002628
Iteration 60/1000 | Loss: 0.00002628
Iteration 61/1000 | Loss: 0.00002628
Iteration 62/1000 | Loss: 0.00002628
Iteration 63/1000 | Loss: 0.00002628
Iteration 64/1000 | Loss: 0.00002628
Iteration 65/1000 | Loss: 0.00002628
Iteration 66/1000 | Loss: 0.00002628
Iteration 67/1000 | Loss: 0.00002628
Iteration 68/1000 | Loss: 0.00002627
Iteration 69/1000 | Loss: 0.00002627
Iteration 70/1000 | Loss: 0.00002626
Iteration 71/1000 | Loss: 0.00002626
Iteration 72/1000 | Loss: 0.00002625
Iteration 73/1000 | Loss: 0.00002625
Iteration 74/1000 | Loss: 0.00002625
Iteration 75/1000 | Loss: 0.00002625
Iteration 76/1000 | Loss: 0.00002625
Iteration 77/1000 | Loss: 0.00002625
Iteration 78/1000 | Loss: 0.00002625
Iteration 79/1000 | Loss: 0.00002624
Iteration 80/1000 | Loss: 0.00002624
Iteration 81/1000 | Loss: 0.00002624
Iteration 82/1000 | Loss: 0.00002624
Iteration 83/1000 | Loss: 0.00002623
Iteration 84/1000 | Loss: 0.00002623
Iteration 85/1000 | Loss: 0.00002623
Iteration 86/1000 | Loss: 0.00002623
Iteration 87/1000 | Loss: 0.00002622
Iteration 88/1000 | Loss: 0.00002622
Iteration 89/1000 | Loss: 0.00002622
Iteration 90/1000 | Loss: 0.00002621
Iteration 91/1000 | Loss: 0.00002621
Iteration 92/1000 | Loss: 0.00002621
Iteration 93/1000 | Loss: 0.00002620
Iteration 94/1000 | Loss: 0.00002620
Iteration 95/1000 | Loss: 0.00002620
Iteration 96/1000 | Loss: 0.00002619
Iteration 97/1000 | Loss: 0.00002619
Iteration 98/1000 | Loss: 0.00002618
Iteration 99/1000 | Loss: 0.00002618
Iteration 100/1000 | Loss: 0.00002617
Iteration 101/1000 | Loss: 0.00002617
Iteration 102/1000 | Loss: 0.00002617
Iteration 103/1000 | Loss: 0.00002617
Iteration 104/1000 | Loss: 0.00002617
Iteration 105/1000 | Loss: 0.00002617
Iteration 106/1000 | Loss: 0.00002617
Iteration 107/1000 | Loss: 0.00002617
Iteration 108/1000 | Loss: 0.00002617
Iteration 109/1000 | Loss: 0.00002617
Iteration 110/1000 | Loss: 0.00002617
Iteration 111/1000 | Loss: 0.00002617
Iteration 112/1000 | Loss: 0.00002616
Iteration 113/1000 | Loss: 0.00002616
Iteration 114/1000 | Loss: 0.00002616
Iteration 115/1000 | Loss: 0.00002616
Iteration 116/1000 | Loss: 0.00002616
Iteration 117/1000 | Loss: 0.00002616
Iteration 118/1000 | Loss: 0.00002616
Iteration 119/1000 | Loss: 0.00002616
Iteration 120/1000 | Loss: 0.00002615
Iteration 121/1000 | Loss: 0.00002615
Iteration 122/1000 | Loss: 0.00002615
Iteration 123/1000 | Loss: 0.00002615
Iteration 124/1000 | Loss: 0.00002615
Iteration 125/1000 | Loss: 0.00002614
Iteration 126/1000 | Loss: 0.00002614
Iteration 127/1000 | Loss: 0.00002614
Iteration 128/1000 | Loss: 0.00002614
Iteration 129/1000 | Loss: 0.00002613
Iteration 130/1000 | Loss: 0.00002613
Iteration 131/1000 | Loss: 0.00002613
Iteration 132/1000 | Loss: 0.00002613
Iteration 133/1000 | Loss: 0.00002613
Iteration 134/1000 | Loss: 0.00002613
Iteration 135/1000 | Loss: 0.00002613
Iteration 136/1000 | Loss: 0.00002612
Iteration 137/1000 | Loss: 0.00002612
Iteration 138/1000 | Loss: 0.00002612
Iteration 139/1000 | Loss: 0.00002612
Iteration 140/1000 | Loss: 0.00002612
Iteration 141/1000 | Loss: 0.00002612
Iteration 142/1000 | Loss: 0.00002612
Iteration 143/1000 | Loss: 0.00002612
Iteration 144/1000 | Loss: 0.00002612
Iteration 145/1000 | Loss: 0.00002612
Iteration 146/1000 | Loss: 0.00002612
Iteration 147/1000 | Loss: 0.00002612
Iteration 148/1000 | Loss: 0.00002612
Iteration 149/1000 | Loss: 0.00002612
Iteration 150/1000 | Loss: 0.00002612
Iteration 151/1000 | Loss: 0.00002611
Iteration 152/1000 | Loss: 0.00002611
Iteration 153/1000 | Loss: 0.00002611
Iteration 154/1000 | Loss: 0.00002611
Iteration 155/1000 | Loss: 0.00002611
Iteration 156/1000 | Loss: 0.00002611
Iteration 157/1000 | Loss: 0.00002610
Iteration 158/1000 | Loss: 0.00002610
Iteration 159/1000 | Loss: 0.00002610
Iteration 160/1000 | Loss: 0.00002610
Iteration 161/1000 | Loss: 0.00002610
Iteration 162/1000 | Loss: 0.00002610
Iteration 163/1000 | Loss: 0.00002610
Iteration 164/1000 | Loss: 0.00002610
Iteration 165/1000 | Loss: 0.00002609
Iteration 166/1000 | Loss: 0.00002609
Iteration 167/1000 | Loss: 0.00002609
Iteration 168/1000 | Loss: 0.00002609
Iteration 169/1000 | Loss: 0.00002609
Iteration 170/1000 | Loss: 0.00002609
Iteration 171/1000 | Loss: 0.00002609
Iteration 172/1000 | Loss: 0.00002609
Iteration 173/1000 | Loss: 0.00002609
Iteration 174/1000 | Loss: 0.00002608
Iteration 175/1000 | Loss: 0.00002608
Iteration 176/1000 | Loss: 0.00002608
Iteration 177/1000 | Loss: 0.00002608
Iteration 178/1000 | Loss: 0.00002608
Iteration 179/1000 | Loss: 0.00002608
Iteration 180/1000 | Loss: 0.00002608
Iteration 181/1000 | Loss: 0.00002608
Iteration 182/1000 | Loss: 0.00002608
Iteration 183/1000 | Loss: 0.00002608
Iteration 184/1000 | Loss: 0.00002608
Iteration 185/1000 | Loss: 0.00002608
Iteration 186/1000 | Loss: 0.00002608
Iteration 187/1000 | Loss: 0.00002607
Iteration 188/1000 | Loss: 0.00002607
Iteration 189/1000 | Loss: 0.00002607
Iteration 190/1000 | Loss: 0.00002607
Iteration 191/1000 | Loss: 0.00002607
Iteration 192/1000 | Loss: 0.00002607
Iteration 193/1000 | Loss: 0.00002607
Iteration 194/1000 | Loss: 0.00002607
Iteration 195/1000 | Loss: 0.00002607
Iteration 196/1000 | Loss: 0.00002607
Iteration 197/1000 | Loss: 0.00002607
Iteration 198/1000 | Loss: 0.00002607
Iteration 199/1000 | Loss: 0.00002607
Iteration 200/1000 | Loss: 0.00002607
Iteration 201/1000 | Loss: 0.00002606
Iteration 202/1000 | Loss: 0.00002606
Iteration 203/1000 | Loss: 0.00002606
Iteration 204/1000 | Loss: 0.00002606
Iteration 205/1000 | Loss: 0.00002606
Iteration 206/1000 | Loss: 0.00002606
Iteration 207/1000 | Loss: 0.00002606
Iteration 208/1000 | Loss: 0.00002606
Iteration 209/1000 | Loss: 0.00002606
Iteration 210/1000 | Loss: 0.00002606
Iteration 211/1000 | Loss: 0.00002606
Iteration 212/1000 | Loss: 0.00002606
Iteration 213/1000 | Loss: 0.00002606
Iteration 214/1000 | Loss: 0.00002606
Iteration 215/1000 | Loss: 0.00002606
Iteration 216/1000 | Loss: 0.00002606
Iteration 217/1000 | Loss: 0.00002606
Iteration 218/1000 | Loss: 0.00002606
Iteration 219/1000 | Loss: 0.00002606
Iteration 220/1000 | Loss: 0.00002606
Iteration 221/1000 | Loss: 0.00002606
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 221. Stopping optimization.
Last 5 losses: [2.6063917175633833e-05, 2.6063917175633833e-05, 2.6063917175633833e-05, 2.6063917175633833e-05, 2.6063917175633833e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.6063917175633833e-05

Optimization complete. Final v2v error: 4.334212303161621 mm

Highest mean error: 4.588078498840332 mm for frame 36

Lowest mean error: 4.10737419128418 mm for frame 48

Saving results

Total time: 39.319613456726074
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_36_nl_6641/0009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_nl_6641/0009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_nl_6641/0009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01146717
Iteration 2/25 | Loss: 0.00217715
Iteration 3/25 | Loss: 0.00144709
Iteration 4/25 | Loss: 0.00130743
Iteration 5/25 | Loss: 0.00123627
Iteration 6/25 | Loss: 0.00121307
Iteration 7/25 | Loss: 0.00119719
Iteration 8/25 | Loss: 0.00118760
Iteration 9/25 | Loss: 0.00116881
Iteration 10/25 | Loss: 0.00116174
Iteration 11/25 | Loss: 0.00115341
Iteration 12/25 | Loss: 0.00114895
Iteration 13/25 | Loss: 0.00114415
Iteration 14/25 | Loss: 0.00113804
Iteration 15/25 | Loss: 0.00113583
Iteration 16/25 | Loss: 0.00113486
Iteration 17/25 | Loss: 0.00113461
Iteration 18/25 | Loss: 0.00113452
Iteration 19/25 | Loss: 0.00113451
Iteration 20/25 | Loss: 0.00113451
Iteration 21/25 | Loss: 0.00113451
Iteration 22/25 | Loss: 0.00113451
Iteration 23/25 | Loss: 0.00113451
Iteration 24/25 | Loss: 0.00113450
Iteration 25/25 | Loss: 0.00113997

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34393668
Iteration 2/25 | Loss: 0.00046475
Iteration 3/25 | Loss: 0.00046474
Iteration 4/25 | Loss: 0.00046474
Iteration 5/25 | Loss: 0.00046474
Iteration 6/25 | Loss: 0.00046474
Iteration 7/25 | Loss: 0.00046474
Iteration 8/25 | Loss: 0.00046474
Iteration 9/25 | Loss: 0.00046474
Iteration 10/25 | Loss: 0.00046474
Iteration 11/25 | Loss: 0.00046474
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0004647411697078496, 0.0004647411697078496, 0.0004647411697078496, 0.0004647411697078496, 0.0004647411697078496]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0004647411697078496

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00046474
Iteration 2/1000 | Loss: 0.00007381
Iteration 3/1000 | Loss: 0.00009572
Iteration 4/1000 | Loss: 0.00004358
Iteration 5/1000 | Loss: 0.00007995
Iteration 6/1000 | Loss: 0.00003560
Iteration 7/1000 | Loss: 0.00011646
Iteration 8/1000 | Loss: 0.00003205
Iteration 9/1000 | Loss: 0.00003098
Iteration 10/1000 | Loss: 0.00007815
Iteration 11/1000 | Loss: 0.00018804
Iteration 12/1000 | Loss: 0.00005013
Iteration 13/1000 | Loss: 0.00004245
Iteration 14/1000 | Loss: 0.00003568
Iteration 15/1000 | Loss: 0.00003012
Iteration 16/1000 | Loss: 0.00002879
Iteration 17/1000 | Loss: 0.00002858
Iteration 18/1000 | Loss: 0.00002835
Iteration 19/1000 | Loss: 0.00002821
Iteration 20/1000 | Loss: 0.00002817
Iteration 21/1000 | Loss: 0.00011798
Iteration 22/1000 | Loss: 0.00002815
Iteration 23/1000 | Loss: 0.00002793
Iteration 24/1000 | Loss: 0.00002792
Iteration 25/1000 | Loss: 0.00002792
Iteration 26/1000 | Loss: 0.00002792
Iteration 27/1000 | Loss: 0.00002792
Iteration 28/1000 | Loss: 0.00002791
Iteration 29/1000 | Loss: 0.00002791
Iteration 30/1000 | Loss: 0.00002791
Iteration 31/1000 | Loss: 0.00002791
Iteration 32/1000 | Loss: 0.00002791
Iteration 33/1000 | Loss: 0.00002790
Iteration 34/1000 | Loss: 0.00002790
Iteration 35/1000 | Loss: 0.00002790
Iteration 36/1000 | Loss: 0.00002790
Iteration 37/1000 | Loss: 0.00002790
Iteration 38/1000 | Loss: 0.00002790
Iteration 39/1000 | Loss: 0.00002790
Iteration 40/1000 | Loss: 0.00002790
Iteration 41/1000 | Loss: 0.00002790
Iteration 42/1000 | Loss: 0.00002790
Iteration 43/1000 | Loss: 0.00002789
Iteration 44/1000 | Loss: 0.00002789
Iteration 45/1000 | Loss: 0.00002789
Iteration 46/1000 | Loss: 0.00002789
Iteration 47/1000 | Loss: 0.00002789
Iteration 48/1000 | Loss: 0.00002788
Iteration 49/1000 | Loss: 0.00002788
Iteration 50/1000 | Loss: 0.00002787
Iteration 51/1000 | Loss: 0.00002787
Iteration 52/1000 | Loss: 0.00002785
Iteration 53/1000 | Loss: 0.00002784
Iteration 54/1000 | Loss: 0.00002784
Iteration 55/1000 | Loss: 0.00002783
Iteration 56/1000 | Loss: 0.00002783
Iteration 57/1000 | Loss: 0.00002781
Iteration 58/1000 | Loss: 0.00002780
Iteration 59/1000 | Loss: 0.00002780
Iteration 60/1000 | Loss: 0.00002780
Iteration 61/1000 | Loss: 0.00002780
Iteration 62/1000 | Loss: 0.00002780
Iteration 63/1000 | Loss: 0.00002779
Iteration 64/1000 | Loss: 0.00002779
Iteration 65/1000 | Loss: 0.00002778
Iteration 66/1000 | Loss: 0.00002778
Iteration 67/1000 | Loss: 0.00002778
Iteration 68/1000 | Loss: 0.00002777
Iteration 69/1000 | Loss: 0.00002777
Iteration 70/1000 | Loss: 0.00002777
Iteration 71/1000 | Loss: 0.00002775
Iteration 72/1000 | Loss: 0.00002775
Iteration 73/1000 | Loss: 0.00002775
Iteration 74/1000 | Loss: 0.00002774
Iteration 75/1000 | Loss: 0.00002773
Iteration 76/1000 | Loss: 0.00002773
Iteration 77/1000 | Loss: 0.00002772
Iteration 78/1000 | Loss: 0.00002771
Iteration 79/1000 | Loss: 0.00002771
Iteration 80/1000 | Loss: 0.00002771
Iteration 81/1000 | Loss: 0.00002771
Iteration 82/1000 | Loss: 0.00002771
Iteration 83/1000 | Loss: 0.00002770
Iteration 84/1000 | Loss: 0.00002770
Iteration 85/1000 | Loss: 0.00002770
Iteration 86/1000 | Loss: 0.00002770
Iteration 87/1000 | Loss: 0.00002770
Iteration 88/1000 | Loss: 0.00002770
Iteration 89/1000 | Loss: 0.00002770
Iteration 90/1000 | Loss: 0.00002770
Iteration 91/1000 | Loss: 0.00002770
Iteration 92/1000 | Loss: 0.00002770
Iteration 93/1000 | Loss: 0.00002770
Iteration 94/1000 | Loss: 0.00002770
Iteration 95/1000 | Loss: 0.00002770
Iteration 96/1000 | Loss: 0.00002770
Iteration 97/1000 | Loss: 0.00002770
Iteration 98/1000 | Loss: 0.00002769
Iteration 99/1000 | Loss: 0.00002769
Iteration 100/1000 | Loss: 0.00002769
Iteration 101/1000 | Loss: 0.00002769
Iteration 102/1000 | Loss: 0.00002769
Iteration 103/1000 | Loss: 0.00002769
Iteration 104/1000 | Loss: 0.00002769
Iteration 105/1000 | Loss: 0.00002769
Iteration 106/1000 | Loss: 0.00002769
Iteration 107/1000 | Loss: 0.00002769
Iteration 108/1000 | Loss: 0.00002769
Iteration 109/1000 | Loss: 0.00002768
Iteration 110/1000 | Loss: 0.00002768
Iteration 111/1000 | Loss: 0.00002768
Iteration 112/1000 | Loss: 0.00002768
Iteration 113/1000 | Loss: 0.00002768
Iteration 114/1000 | Loss: 0.00002768
Iteration 115/1000 | Loss: 0.00002768
Iteration 116/1000 | Loss: 0.00002768
Iteration 117/1000 | Loss: 0.00002767
Iteration 118/1000 | Loss: 0.00002767
Iteration 119/1000 | Loss: 0.00002767
Iteration 120/1000 | Loss: 0.00002767
Iteration 121/1000 | Loss: 0.00002767
Iteration 122/1000 | Loss: 0.00002766
Iteration 123/1000 | Loss: 0.00002766
Iteration 124/1000 | Loss: 0.00002766
Iteration 125/1000 | Loss: 0.00003475
Iteration 126/1000 | Loss: 0.00003475
Iteration 127/1000 | Loss: 0.00003474
Iteration 128/1000 | Loss: 0.00003474
Iteration 129/1000 | Loss: 0.00003328
Iteration 130/1000 | Loss: 0.00007437
Iteration 131/1000 | Loss: 0.00003311
Iteration 132/1000 | Loss: 0.00002812
Iteration 133/1000 | Loss: 0.00003504
Iteration 134/1000 | Loss: 0.00002768
Iteration 135/1000 | Loss: 0.00002766
Iteration 136/1000 | Loss: 0.00002766
Iteration 137/1000 | Loss: 0.00002766
Iteration 138/1000 | Loss: 0.00002766
Iteration 139/1000 | Loss: 0.00002766
Iteration 140/1000 | Loss: 0.00002766
Iteration 141/1000 | Loss: 0.00002766
Iteration 142/1000 | Loss: 0.00002766
Iteration 143/1000 | Loss: 0.00002766
Iteration 144/1000 | Loss: 0.00002766
Iteration 145/1000 | Loss: 0.00002765
Iteration 146/1000 | Loss: 0.00004021
Iteration 147/1000 | Loss: 0.00003150
Iteration 148/1000 | Loss: 0.00002849
Iteration 149/1000 | Loss: 0.00003290
Iteration 150/1000 | Loss: 0.00002898
Iteration 151/1000 | Loss: 0.00002840
Iteration 152/1000 | Loss: 0.00002828
Iteration 153/1000 | Loss: 0.00002818
Iteration 154/1000 | Loss: 0.00003375
Iteration 155/1000 | Loss: 0.00005276
Iteration 156/1000 | Loss: 0.00005557
Iteration 157/1000 | Loss: 0.00002781
Iteration 158/1000 | Loss: 0.00004071
Iteration 159/1000 | Loss: 0.00002764
Iteration 160/1000 | Loss: 0.00002760
Iteration 161/1000 | Loss: 0.00002760
Iteration 162/1000 | Loss: 0.00002759
Iteration 163/1000 | Loss: 0.00002758
Iteration 164/1000 | Loss: 0.00002758
Iteration 165/1000 | Loss: 0.00002758
Iteration 166/1000 | Loss: 0.00002757
Iteration 167/1000 | Loss: 0.00002757
Iteration 168/1000 | Loss: 0.00002757
Iteration 169/1000 | Loss: 0.00002756
Iteration 170/1000 | Loss: 0.00002825
Iteration 171/1000 | Loss: 0.00002784
Iteration 172/1000 | Loss: 0.00002783
Iteration 173/1000 | Loss: 0.00002783
Iteration 174/1000 | Loss: 0.00002782
Iteration 175/1000 | Loss: 0.00002782
Iteration 176/1000 | Loss: 0.00002780
Iteration 177/1000 | Loss: 0.00002780
Iteration 178/1000 | Loss: 0.00002779
Iteration 179/1000 | Loss: 0.00002779
Iteration 180/1000 | Loss: 0.00002779
Iteration 181/1000 | Loss: 0.00002767
Iteration 182/1000 | Loss: 0.00002767
Iteration 183/1000 | Loss: 0.00002766
Iteration 184/1000 | Loss: 0.00002765
Iteration 185/1000 | Loss: 0.00002763
Iteration 186/1000 | Loss: 0.00002762
Iteration 187/1000 | Loss: 0.00002762
Iteration 188/1000 | Loss: 0.00002762
Iteration 189/1000 | Loss: 0.00002762
Iteration 190/1000 | Loss: 0.00002762
Iteration 191/1000 | Loss: 0.00002762
Iteration 192/1000 | Loss: 0.00002762
Iteration 193/1000 | Loss: 0.00002762
Iteration 194/1000 | Loss: 0.00002762
Iteration 195/1000 | Loss: 0.00002761
Iteration 196/1000 | Loss: 0.00004129
Iteration 197/1000 | Loss: 0.00005271
Iteration 198/1000 | Loss: 0.00002961
Iteration 199/1000 | Loss: 0.00002800
Iteration 200/1000 | Loss: 0.00002756
Iteration 201/1000 | Loss: 0.00002755
Iteration 202/1000 | Loss: 0.00002755
Iteration 203/1000 | Loss: 0.00002755
Iteration 204/1000 | Loss: 0.00002755
Iteration 205/1000 | Loss: 0.00002755
Iteration 206/1000 | Loss: 0.00002754
Iteration 207/1000 | Loss: 0.00002754
Iteration 208/1000 | Loss: 0.00002754
Iteration 209/1000 | Loss: 0.00002754
Iteration 210/1000 | Loss: 0.00002753
Iteration 211/1000 | Loss: 0.00002753
Iteration 212/1000 | Loss: 0.00002753
Iteration 213/1000 | Loss: 0.00002753
Iteration 214/1000 | Loss: 0.00002753
Iteration 215/1000 | Loss: 0.00002752
Iteration 216/1000 | Loss: 0.00002752
Iteration 217/1000 | Loss: 0.00002752
Iteration 218/1000 | Loss: 0.00002752
Iteration 219/1000 | Loss: 0.00002752
Iteration 220/1000 | Loss: 0.00002751
Iteration 221/1000 | Loss: 0.00002751
Iteration 222/1000 | Loss: 0.00002751
Iteration 223/1000 | Loss: 0.00002750
Iteration 224/1000 | Loss: 0.00002750
Iteration 225/1000 | Loss: 0.00002750
Iteration 226/1000 | Loss: 0.00002750
Iteration 227/1000 | Loss: 0.00002750
Iteration 228/1000 | Loss: 0.00002750
Iteration 229/1000 | Loss: 0.00002750
Iteration 230/1000 | Loss: 0.00002750
Iteration 231/1000 | Loss: 0.00002749
Iteration 232/1000 | Loss: 0.00002749
Iteration 233/1000 | Loss: 0.00002749
Iteration 234/1000 | Loss: 0.00002748
Iteration 235/1000 | Loss: 0.00002748
Iteration 236/1000 | Loss: 0.00003479
Iteration 237/1000 | Loss: 0.00006123
Iteration 238/1000 | Loss: 0.00051132
Iteration 239/1000 | Loss: 0.00023946
Iteration 240/1000 | Loss: 0.00003335
Iteration 241/1000 | Loss: 0.00002754
Iteration 242/1000 | Loss: 0.00003565
Iteration 243/1000 | Loss: 0.00002760
Iteration 244/1000 | Loss: 0.00002752
Iteration 245/1000 | Loss: 0.00002751
Iteration 246/1000 | Loss: 0.00002751
Iteration 247/1000 | Loss: 0.00002751
Iteration 248/1000 | Loss: 0.00002750
Iteration 249/1000 | Loss: 0.00002750
Iteration 250/1000 | Loss: 0.00002750
Iteration 251/1000 | Loss: 0.00002749
Iteration 252/1000 | Loss: 0.00002749
Iteration 253/1000 | Loss: 0.00002748
Iteration 254/1000 | Loss: 0.00002748
Iteration 255/1000 | Loss: 0.00002748
Iteration 256/1000 | Loss: 0.00002748
Iteration 257/1000 | Loss: 0.00002748
Iteration 258/1000 | Loss: 0.00002748
Iteration 259/1000 | Loss: 0.00002748
Iteration 260/1000 | Loss: 0.00002748
Iteration 261/1000 | Loss: 0.00002748
Iteration 262/1000 | Loss: 0.00002748
Iteration 263/1000 | Loss: 0.00002748
Iteration 264/1000 | Loss: 0.00002748
Iteration 265/1000 | Loss: 0.00002748
Iteration 266/1000 | Loss: 0.00002748
Iteration 267/1000 | Loss: 0.00002748
Iteration 268/1000 | Loss: 0.00002748
Iteration 269/1000 | Loss: 0.00002748
Iteration 270/1000 | Loss: 0.00002748
Iteration 271/1000 | Loss: 0.00002748
Iteration 272/1000 | Loss: 0.00002748
Iteration 273/1000 | Loss: 0.00002748
Iteration 274/1000 | Loss: 0.00002748
Iteration 275/1000 | Loss: 0.00002748
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 275. Stopping optimization.
Last 5 losses: [2.7475873139337637e-05, 2.7475873139337637e-05, 2.7475873139337637e-05, 2.7475873139337637e-05, 2.7475873139337637e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.7475873139337637e-05

Optimization complete. Final v2v error: 4.207118034362793 mm

Highest mean error: 11.322696685791016 mm for frame 118

Lowest mean error: 3.180732488632202 mm for frame 2

Saving results

Total time: 130.3013367652893
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_36_nl_6641/0011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_nl_6641/0011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_nl_6641/0011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00937701
Iteration 2/25 | Loss: 0.00190322
Iteration 3/25 | Loss: 0.00140867
Iteration 4/25 | Loss: 0.00135327
Iteration 5/25 | Loss: 0.00134828
Iteration 6/25 | Loss: 0.00134770
Iteration 7/25 | Loss: 0.00134770
Iteration 8/25 | Loss: 0.00134770
Iteration 9/25 | Loss: 0.00134770
Iteration 10/25 | Loss: 0.00134770
Iteration 11/25 | Loss: 0.00134770
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013477042084559798, 0.0013477042084559798, 0.0013477042084559798, 0.0013477042084559798, 0.0013477042084559798]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013477042084559798

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.22871411
Iteration 2/25 | Loss: 0.00050728
Iteration 3/25 | Loss: 0.00050728
Iteration 4/25 | Loss: 0.00050728
Iteration 5/25 | Loss: 0.00050728
Iteration 6/25 | Loss: 0.00050728
Iteration 7/25 | Loss: 0.00050728
Iteration 8/25 | Loss: 0.00050728
Iteration 9/25 | Loss: 0.00050728
Iteration 10/25 | Loss: 0.00050728
Iteration 11/25 | Loss: 0.00050728
Iteration 12/25 | Loss: 0.00050728
Iteration 13/25 | Loss: 0.00050728
Iteration 14/25 | Loss: 0.00050728
Iteration 15/25 | Loss: 0.00050728
Iteration 16/25 | Loss: 0.00050728
Iteration 17/25 | Loss: 0.00050728
Iteration 18/25 | Loss: 0.00050728
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0005072775529697537, 0.0005072775529697537, 0.0005072775529697537, 0.0005072775529697537, 0.0005072775529697537]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005072775529697537

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00050728
Iteration 2/1000 | Loss: 0.00007851
Iteration 3/1000 | Loss: 0.00005030
Iteration 4/1000 | Loss: 0.00003905
Iteration 5/1000 | Loss: 0.00003523
Iteration 6/1000 | Loss: 0.00003304
Iteration 7/1000 | Loss: 0.00003181
Iteration 8/1000 | Loss: 0.00003054
Iteration 9/1000 | Loss: 0.00002958
Iteration 10/1000 | Loss: 0.00002916
Iteration 11/1000 | Loss: 0.00002864
Iteration 12/1000 | Loss: 0.00002829
Iteration 13/1000 | Loss: 0.00002809
Iteration 14/1000 | Loss: 0.00002805
Iteration 15/1000 | Loss: 0.00002795
Iteration 16/1000 | Loss: 0.00002791
Iteration 17/1000 | Loss: 0.00002790
Iteration 18/1000 | Loss: 0.00002790
Iteration 19/1000 | Loss: 0.00002790
Iteration 20/1000 | Loss: 0.00002790
Iteration 21/1000 | Loss: 0.00002790
Iteration 22/1000 | Loss: 0.00002789
Iteration 23/1000 | Loss: 0.00002789
Iteration 24/1000 | Loss: 0.00002789
Iteration 25/1000 | Loss: 0.00002789
Iteration 26/1000 | Loss: 0.00002789
Iteration 27/1000 | Loss: 0.00002789
Iteration 28/1000 | Loss: 0.00002789
Iteration 29/1000 | Loss: 0.00002789
Iteration 30/1000 | Loss: 0.00002789
Iteration 31/1000 | Loss: 0.00002788
Iteration 32/1000 | Loss: 0.00002788
Iteration 33/1000 | Loss: 0.00002788
Iteration 34/1000 | Loss: 0.00002787
Iteration 35/1000 | Loss: 0.00002787
Iteration 36/1000 | Loss: 0.00002787
Iteration 37/1000 | Loss: 0.00002787
Iteration 38/1000 | Loss: 0.00002787
Iteration 39/1000 | Loss: 0.00002786
Iteration 40/1000 | Loss: 0.00002786
Iteration 41/1000 | Loss: 0.00002786
Iteration 42/1000 | Loss: 0.00002786
Iteration 43/1000 | Loss: 0.00002786
Iteration 44/1000 | Loss: 0.00002786
Iteration 45/1000 | Loss: 0.00002786
Iteration 46/1000 | Loss: 0.00002786
Iteration 47/1000 | Loss: 0.00002786
Iteration 48/1000 | Loss: 0.00002786
Iteration 49/1000 | Loss: 0.00002786
Iteration 50/1000 | Loss: 0.00002786
Iteration 51/1000 | Loss: 0.00002785
Iteration 52/1000 | Loss: 0.00002785
Iteration 53/1000 | Loss: 0.00002785
Iteration 54/1000 | Loss: 0.00002785
Iteration 55/1000 | Loss: 0.00002784
Iteration 56/1000 | Loss: 0.00002784
Iteration 57/1000 | Loss: 0.00002784
Iteration 58/1000 | Loss: 0.00002784
Iteration 59/1000 | Loss: 0.00002784
Iteration 60/1000 | Loss: 0.00002784
Iteration 61/1000 | Loss: 0.00002784
Iteration 62/1000 | Loss: 0.00002783
Iteration 63/1000 | Loss: 0.00002783
Iteration 64/1000 | Loss: 0.00002783
Iteration 65/1000 | Loss: 0.00002782
Iteration 66/1000 | Loss: 0.00002782
Iteration 67/1000 | Loss: 0.00002781
Iteration 68/1000 | Loss: 0.00002781
Iteration 69/1000 | Loss: 0.00002780
Iteration 70/1000 | Loss: 0.00002780
Iteration 71/1000 | Loss: 0.00002780
Iteration 72/1000 | Loss: 0.00002779
Iteration 73/1000 | Loss: 0.00002779
Iteration 74/1000 | Loss: 0.00002779
Iteration 75/1000 | Loss: 0.00002779
Iteration 76/1000 | Loss: 0.00002778
Iteration 77/1000 | Loss: 0.00002778
Iteration 78/1000 | Loss: 0.00002778
Iteration 79/1000 | Loss: 0.00002778
Iteration 80/1000 | Loss: 0.00002778
Iteration 81/1000 | Loss: 0.00002778
Iteration 82/1000 | Loss: 0.00002777
Iteration 83/1000 | Loss: 0.00002777
Iteration 84/1000 | Loss: 0.00002777
Iteration 85/1000 | Loss: 0.00002777
Iteration 86/1000 | Loss: 0.00002777
Iteration 87/1000 | Loss: 0.00002777
Iteration 88/1000 | Loss: 0.00002777
Iteration 89/1000 | Loss: 0.00002777
Iteration 90/1000 | Loss: 0.00002777
Iteration 91/1000 | Loss: 0.00002777
Iteration 92/1000 | Loss: 0.00002777
Iteration 93/1000 | Loss: 0.00002777
Iteration 94/1000 | Loss: 0.00002777
Iteration 95/1000 | Loss: 0.00002777
Iteration 96/1000 | Loss: 0.00002777
Iteration 97/1000 | Loss: 0.00002777
Iteration 98/1000 | Loss: 0.00002777
Iteration 99/1000 | Loss: 0.00002777
Iteration 100/1000 | Loss: 0.00002777
Iteration 101/1000 | Loss: 0.00002777
Iteration 102/1000 | Loss: 0.00002777
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 102. Stopping optimization.
Last 5 losses: [2.7774520276580006e-05, 2.7774520276580006e-05, 2.7774520276580006e-05, 2.7774520276580006e-05, 2.7774520276580006e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.7774520276580006e-05

Optimization complete. Final v2v error: 4.534501075744629 mm

Highest mean error: 4.798444747924805 mm for frame 109

Lowest mean error: 4.23627233505249 mm for frame 48

Saving results

Total time: 33.16553807258606
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_36_nl_6641/0020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_nl_6641/0020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_nl_6641/0020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00991992
Iteration 2/25 | Loss: 0.00147783
Iteration 3/25 | Loss: 0.00129184
Iteration 4/25 | Loss: 0.00125304
Iteration 5/25 | Loss: 0.00123574
Iteration 6/25 | Loss: 0.00123241
Iteration 7/25 | Loss: 0.00123183
Iteration 8/25 | Loss: 0.00123183
Iteration 9/25 | Loss: 0.00123183
Iteration 10/25 | Loss: 0.00123183
Iteration 11/25 | Loss: 0.00123183
Iteration 12/25 | Loss: 0.00123183
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0012318323133513331, 0.0012318323133513331, 0.0012318323133513331, 0.0012318323133513331, 0.0012318323133513331]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012318323133513331

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42333412
Iteration 2/25 | Loss: 0.00055789
Iteration 3/25 | Loss: 0.00055787
Iteration 4/25 | Loss: 0.00055787
Iteration 5/25 | Loss: 0.00055787
Iteration 6/25 | Loss: 0.00055787
Iteration 7/25 | Loss: 0.00055787
Iteration 8/25 | Loss: 0.00055787
Iteration 9/25 | Loss: 0.00055787
Iteration 10/25 | Loss: 0.00055787
Iteration 11/25 | Loss: 0.00055787
Iteration 12/25 | Loss: 0.00055787
Iteration 13/25 | Loss: 0.00055787
Iteration 14/25 | Loss: 0.00055787
Iteration 15/25 | Loss: 0.00055787
Iteration 16/25 | Loss: 0.00055787
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0005578650161623955, 0.0005578650161623955, 0.0005578650161623955, 0.0005578650161623955, 0.0005578650161623955]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005578650161623955

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00055787
Iteration 2/1000 | Loss: 0.00007998
Iteration 3/1000 | Loss: 0.00005320
Iteration 4/1000 | Loss: 0.00004170
Iteration 5/1000 | Loss: 0.00003696
Iteration 6/1000 | Loss: 0.00003447
Iteration 7/1000 | Loss: 0.00003343
Iteration 8/1000 | Loss: 0.00003239
Iteration 9/1000 | Loss: 0.00003170
Iteration 10/1000 | Loss: 0.00003124
Iteration 11/1000 | Loss: 0.00003085
Iteration 12/1000 | Loss: 0.00003060
Iteration 13/1000 | Loss: 0.00003059
Iteration 14/1000 | Loss: 0.00003048
Iteration 15/1000 | Loss: 0.00003035
Iteration 16/1000 | Loss: 0.00003034
Iteration 17/1000 | Loss: 0.00003028
Iteration 18/1000 | Loss: 0.00003028
Iteration 19/1000 | Loss: 0.00003027
Iteration 20/1000 | Loss: 0.00003026
Iteration 21/1000 | Loss: 0.00003025
Iteration 22/1000 | Loss: 0.00003025
Iteration 23/1000 | Loss: 0.00003024
Iteration 24/1000 | Loss: 0.00003023
Iteration 25/1000 | Loss: 0.00003022
Iteration 26/1000 | Loss: 0.00003022
Iteration 27/1000 | Loss: 0.00003021
Iteration 28/1000 | Loss: 0.00003021
Iteration 29/1000 | Loss: 0.00003021
Iteration 30/1000 | Loss: 0.00003020
Iteration 31/1000 | Loss: 0.00003020
Iteration 32/1000 | Loss: 0.00003019
Iteration 33/1000 | Loss: 0.00003019
Iteration 34/1000 | Loss: 0.00003019
Iteration 35/1000 | Loss: 0.00003019
Iteration 36/1000 | Loss: 0.00003019
Iteration 37/1000 | Loss: 0.00003019
Iteration 38/1000 | Loss: 0.00003019
Iteration 39/1000 | Loss: 0.00003019
Iteration 40/1000 | Loss: 0.00003019
Iteration 41/1000 | Loss: 0.00003018
Iteration 42/1000 | Loss: 0.00003018
Iteration 43/1000 | Loss: 0.00003018
Iteration 44/1000 | Loss: 0.00003018
Iteration 45/1000 | Loss: 0.00003018
Iteration 46/1000 | Loss: 0.00003018
Iteration 47/1000 | Loss: 0.00003018
Iteration 48/1000 | Loss: 0.00003018
Iteration 49/1000 | Loss: 0.00003017
Iteration 50/1000 | Loss: 0.00003017
Iteration 51/1000 | Loss: 0.00003017
Iteration 52/1000 | Loss: 0.00003017
Iteration 53/1000 | Loss: 0.00003017
Iteration 54/1000 | Loss: 0.00003017
Iteration 55/1000 | Loss: 0.00003016
Iteration 56/1000 | Loss: 0.00003016
Iteration 57/1000 | Loss: 0.00003016
Iteration 58/1000 | Loss: 0.00003015
Iteration 59/1000 | Loss: 0.00003015
Iteration 60/1000 | Loss: 0.00003015
Iteration 61/1000 | Loss: 0.00003014
Iteration 62/1000 | Loss: 0.00003014
Iteration 63/1000 | Loss: 0.00003014
Iteration 64/1000 | Loss: 0.00003014
Iteration 65/1000 | Loss: 0.00003014
Iteration 66/1000 | Loss: 0.00003014
Iteration 67/1000 | Loss: 0.00003013
Iteration 68/1000 | Loss: 0.00003013
Iteration 69/1000 | Loss: 0.00003013
Iteration 70/1000 | Loss: 0.00003013
Iteration 71/1000 | Loss: 0.00003013
Iteration 72/1000 | Loss: 0.00003013
Iteration 73/1000 | Loss: 0.00003013
Iteration 74/1000 | Loss: 0.00003013
Iteration 75/1000 | Loss: 0.00003013
Iteration 76/1000 | Loss: 0.00003013
Iteration 77/1000 | Loss: 0.00003012
Iteration 78/1000 | Loss: 0.00003012
Iteration 79/1000 | Loss: 0.00003012
Iteration 80/1000 | Loss: 0.00003011
Iteration 81/1000 | Loss: 0.00003011
Iteration 82/1000 | Loss: 0.00003011
Iteration 83/1000 | Loss: 0.00003011
Iteration 84/1000 | Loss: 0.00003011
Iteration 85/1000 | Loss: 0.00003011
Iteration 86/1000 | Loss: 0.00003010
Iteration 87/1000 | Loss: 0.00003010
Iteration 88/1000 | Loss: 0.00003009
Iteration 89/1000 | Loss: 0.00003008
Iteration 90/1000 | Loss: 0.00003008
Iteration 91/1000 | Loss: 0.00003008
Iteration 92/1000 | Loss: 0.00003008
Iteration 93/1000 | Loss: 0.00003008
Iteration 94/1000 | Loss: 0.00003008
Iteration 95/1000 | Loss: 0.00003007
Iteration 96/1000 | Loss: 0.00003007
Iteration 97/1000 | Loss: 0.00003007
Iteration 98/1000 | Loss: 0.00003007
Iteration 99/1000 | Loss: 0.00003007
Iteration 100/1000 | Loss: 0.00003007
Iteration 101/1000 | Loss: 0.00003007
Iteration 102/1000 | Loss: 0.00003006
Iteration 103/1000 | Loss: 0.00003006
Iteration 104/1000 | Loss: 0.00003006
Iteration 105/1000 | Loss: 0.00003006
Iteration 106/1000 | Loss: 0.00003006
Iteration 107/1000 | Loss: 0.00003006
Iteration 108/1000 | Loss: 0.00003006
Iteration 109/1000 | Loss: 0.00003006
Iteration 110/1000 | Loss: 0.00003006
Iteration 111/1000 | Loss: 0.00003006
Iteration 112/1000 | Loss: 0.00003006
Iteration 113/1000 | Loss: 0.00003005
Iteration 114/1000 | Loss: 0.00003005
Iteration 115/1000 | Loss: 0.00003005
Iteration 116/1000 | Loss: 0.00003005
Iteration 117/1000 | Loss: 0.00003005
Iteration 118/1000 | Loss: 0.00003005
Iteration 119/1000 | Loss: 0.00003005
Iteration 120/1000 | Loss: 0.00003005
Iteration 121/1000 | Loss: 0.00003005
Iteration 122/1000 | Loss: 0.00003005
Iteration 123/1000 | Loss: 0.00003005
Iteration 124/1000 | Loss: 0.00003005
Iteration 125/1000 | Loss: 0.00003005
Iteration 126/1000 | Loss: 0.00003005
Iteration 127/1000 | Loss: 0.00003005
Iteration 128/1000 | Loss: 0.00003005
Iteration 129/1000 | Loss: 0.00003005
Iteration 130/1000 | Loss: 0.00003005
Iteration 131/1000 | Loss: 0.00003005
Iteration 132/1000 | Loss: 0.00003005
Iteration 133/1000 | Loss: 0.00003005
Iteration 134/1000 | Loss: 0.00003005
Iteration 135/1000 | Loss: 0.00003005
Iteration 136/1000 | Loss: 0.00003005
Iteration 137/1000 | Loss: 0.00003005
Iteration 138/1000 | Loss: 0.00003005
Iteration 139/1000 | Loss: 0.00003005
Iteration 140/1000 | Loss: 0.00003005
Iteration 141/1000 | Loss: 0.00003005
Iteration 142/1000 | Loss: 0.00003005
Iteration 143/1000 | Loss: 0.00003005
Iteration 144/1000 | Loss: 0.00003005
Iteration 145/1000 | Loss: 0.00003005
Iteration 146/1000 | Loss: 0.00003005
Iteration 147/1000 | Loss: 0.00003005
Iteration 148/1000 | Loss: 0.00003005
Iteration 149/1000 | Loss: 0.00003005
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 149. Stopping optimization.
Last 5 losses: [3.0048600820009597e-05, 3.0048600820009597e-05, 3.0048600820009597e-05, 3.0048600820009597e-05, 3.0048600820009597e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.0048600820009597e-05

Optimization complete. Final v2v error: 4.604791164398193 mm

Highest mean error: 5.886663913726807 mm for frame 66

Lowest mean error: 4.123425006866455 mm for frame 101

Saving results

Total time: 39.316664934158325
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_36_nl_6641/0014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_nl_6641/0014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_nl_6641/0014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00676788
Iteration 2/25 | Loss: 0.00150681
Iteration 3/25 | Loss: 0.00119586
Iteration 4/25 | Loss: 0.00117073
Iteration 5/25 | Loss: 0.00116743
Iteration 6/25 | Loss: 0.00116743
Iteration 7/25 | Loss: 0.00116743
Iteration 8/25 | Loss: 0.00116743
Iteration 9/25 | Loss: 0.00116743
Iteration 10/25 | Loss: 0.00116743
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.001167434616945684, 0.001167434616945684, 0.001167434616945684, 0.001167434616945684, 0.001167434616945684]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001167434616945684

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.09179521
Iteration 2/25 | Loss: 0.00052095
Iteration 3/25 | Loss: 0.00052093
Iteration 4/25 | Loss: 0.00052093
Iteration 5/25 | Loss: 0.00052093
Iteration 6/25 | Loss: 0.00052093
Iteration 7/25 | Loss: 0.00052093
Iteration 8/25 | Loss: 0.00052093
Iteration 9/25 | Loss: 0.00052093
Iteration 10/25 | Loss: 0.00052093
Iteration 11/25 | Loss: 0.00052093
Iteration 12/25 | Loss: 0.00052093
Iteration 13/25 | Loss: 0.00052093
Iteration 14/25 | Loss: 0.00052093
Iteration 15/25 | Loss: 0.00052093
Iteration 16/25 | Loss: 0.00052093
Iteration 17/25 | Loss: 0.00052093
Iteration 18/25 | Loss: 0.00052093
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0005209276569075882, 0.0005209276569075882, 0.0005209276569075882, 0.0005209276569075882, 0.0005209276569075882]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005209276569075882

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00052093
Iteration 2/1000 | Loss: 0.00005174
Iteration 3/1000 | Loss: 0.00003320
Iteration 4/1000 | Loss: 0.00002831
Iteration 5/1000 | Loss: 0.00002535
Iteration 6/1000 | Loss: 0.00002339
Iteration 7/1000 | Loss: 0.00002227
Iteration 8/1000 | Loss: 0.00002139
Iteration 9/1000 | Loss: 0.00002069
Iteration 10/1000 | Loss: 0.00002031
Iteration 11/1000 | Loss: 0.00002007
Iteration 12/1000 | Loss: 0.00002007
Iteration 13/1000 | Loss: 0.00001998
Iteration 14/1000 | Loss: 0.00001995
Iteration 15/1000 | Loss: 0.00001994
Iteration 16/1000 | Loss: 0.00001994
Iteration 17/1000 | Loss: 0.00001992
Iteration 18/1000 | Loss: 0.00001992
Iteration 19/1000 | Loss: 0.00001992
Iteration 20/1000 | Loss: 0.00001992
Iteration 21/1000 | Loss: 0.00001992
Iteration 22/1000 | Loss: 0.00001992
Iteration 23/1000 | Loss: 0.00001992
Iteration 24/1000 | Loss: 0.00001992
Iteration 25/1000 | Loss: 0.00001991
Iteration 26/1000 | Loss: 0.00001991
Iteration 27/1000 | Loss: 0.00001990
Iteration 28/1000 | Loss: 0.00001990
Iteration 29/1000 | Loss: 0.00001990
Iteration 30/1000 | Loss: 0.00001989
Iteration 31/1000 | Loss: 0.00001989
Iteration 32/1000 | Loss: 0.00001989
Iteration 33/1000 | Loss: 0.00001989
Iteration 34/1000 | Loss: 0.00001989
Iteration 35/1000 | Loss: 0.00001988
Iteration 36/1000 | Loss: 0.00001988
Iteration 37/1000 | Loss: 0.00001988
Iteration 38/1000 | Loss: 0.00001988
Iteration 39/1000 | Loss: 0.00001987
Iteration 40/1000 | Loss: 0.00001987
Iteration 41/1000 | Loss: 0.00001987
Iteration 42/1000 | Loss: 0.00001987
Iteration 43/1000 | Loss: 0.00001986
Iteration 44/1000 | Loss: 0.00001986
Iteration 45/1000 | Loss: 0.00001986
Iteration 46/1000 | Loss: 0.00001986
Iteration 47/1000 | Loss: 0.00001986
Iteration 48/1000 | Loss: 0.00001986
Iteration 49/1000 | Loss: 0.00001985
Iteration 50/1000 | Loss: 0.00001985
Iteration 51/1000 | Loss: 0.00001985
Iteration 52/1000 | Loss: 0.00001984
Iteration 53/1000 | Loss: 0.00001984
Iteration 54/1000 | Loss: 0.00001984
Iteration 55/1000 | Loss: 0.00001984
Iteration 56/1000 | Loss: 0.00001984
Iteration 57/1000 | Loss: 0.00001983
Iteration 58/1000 | Loss: 0.00001983
Iteration 59/1000 | Loss: 0.00001983
Iteration 60/1000 | Loss: 0.00001983
Iteration 61/1000 | Loss: 0.00001983
Iteration 62/1000 | Loss: 0.00001983
Iteration 63/1000 | Loss: 0.00001983
Iteration 64/1000 | Loss: 0.00001983
Iteration 65/1000 | Loss: 0.00001982
Iteration 66/1000 | Loss: 0.00001982
Iteration 67/1000 | Loss: 0.00001982
Iteration 68/1000 | Loss: 0.00001982
Iteration 69/1000 | Loss: 0.00001982
Iteration 70/1000 | Loss: 0.00001982
Iteration 71/1000 | Loss: 0.00001982
Iteration 72/1000 | Loss: 0.00001982
Iteration 73/1000 | Loss: 0.00001982
Iteration 74/1000 | Loss: 0.00001982
Iteration 75/1000 | Loss: 0.00001982
Iteration 76/1000 | Loss: 0.00001982
Iteration 77/1000 | Loss: 0.00001982
Iteration 78/1000 | Loss: 0.00001982
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 78. Stopping optimization.
Last 5 losses: [1.9823897673632018e-05, 1.9823897673632018e-05, 1.9823897673632018e-05, 1.9823897673632018e-05, 1.9823897673632018e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9823897673632018e-05

Optimization complete. Final v2v error: 3.7424073219299316 mm

Highest mean error: 4.400872230529785 mm for frame 230

Lowest mean error: 3.4482104778289795 mm for frame 91

Saving results

Total time: 34.21622061729431
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_36_nl_6641/0015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_nl_6641/0015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_nl_6641/0015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00800482
Iteration 2/25 | Loss: 0.00157092
Iteration 3/25 | Loss: 0.00132031
Iteration 4/25 | Loss: 0.00127247
Iteration 5/25 | Loss: 0.00128295
Iteration 6/25 | Loss: 0.00125961
Iteration 7/25 | Loss: 0.00125616
Iteration 8/25 | Loss: 0.00127065
Iteration 9/25 | Loss: 0.00125448
Iteration 10/25 | Loss: 0.00125370
Iteration 11/25 | Loss: 0.00125366
Iteration 12/25 | Loss: 0.00125366
Iteration 13/25 | Loss: 0.00125366
Iteration 14/25 | Loss: 0.00125366
Iteration 15/25 | Loss: 0.00125366
Iteration 16/25 | Loss: 0.00125366
Iteration 17/25 | Loss: 0.00125366
Iteration 18/25 | Loss: 0.00125366
Iteration 19/25 | Loss: 0.00125366
Iteration 20/25 | Loss: 0.00125366
Iteration 21/25 | Loss: 0.00125366
Iteration 22/25 | Loss: 0.00125366
Iteration 23/25 | Loss: 0.00125366
Iteration 24/25 | Loss: 0.00125366
Iteration 25/25 | Loss: 0.00125366

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.33476734
Iteration 2/25 | Loss: 0.00056247
Iteration 3/25 | Loss: 0.00056247
Iteration 4/25 | Loss: 0.00056247
Iteration 5/25 | Loss: 0.00056247
Iteration 6/25 | Loss: 0.00056247
Iteration 7/25 | Loss: 0.00056247
Iteration 8/25 | Loss: 0.00056247
Iteration 9/25 | Loss: 0.00056247
Iteration 10/25 | Loss: 0.00056247
Iteration 11/25 | Loss: 0.00056247
Iteration 12/25 | Loss: 0.00056247
Iteration 13/25 | Loss: 0.00056247
Iteration 14/25 | Loss: 0.00056247
Iteration 15/25 | Loss: 0.00056247
Iteration 16/25 | Loss: 0.00056247
Iteration 17/25 | Loss: 0.00056247
Iteration 18/25 | Loss: 0.00056247
Iteration 19/25 | Loss: 0.00056247
Iteration 20/25 | Loss: 0.00056247
Iteration 21/25 | Loss: 0.00056247
Iteration 22/25 | Loss: 0.00056247
Iteration 23/25 | Loss: 0.00056247
Iteration 24/25 | Loss: 0.00056247
Iteration 25/25 | Loss: 0.00056247

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00056247
Iteration 2/1000 | Loss: 0.00005544
Iteration 3/1000 | Loss: 0.00003799
Iteration 4/1000 | Loss: 0.00006186
Iteration 5/1000 | Loss: 0.00003106
Iteration 6/1000 | Loss: 0.00002951
Iteration 7/1000 | Loss: 0.00002874
Iteration 8/1000 | Loss: 0.00002778
Iteration 9/1000 | Loss: 0.00002729
Iteration 10/1000 | Loss: 0.00002703
Iteration 11/1000 | Loss: 0.00002693
Iteration 12/1000 | Loss: 0.00002687
Iteration 13/1000 | Loss: 0.00002685
Iteration 14/1000 | Loss: 0.00002685
Iteration 15/1000 | Loss: 0.00002684
Iteration 16/1000 | Loss: 0.00002680
Iteration 17/1000 | Loss: 0.00002680
Iteration 18/1000 | Loss: 0.00002680
Iteration 19/1000 | Loss: 0.00002679
Iteration 20/1000 | Loss: 0.00002679
Iteration 21/1000 | Loss: 0.00002678
Iteration 22/1000 | Loss: 0.00002678
Iteration 23/1000 | Loss: 0.00002678
Iteration 24/1000 | Loss: 0.00002678
Iteration 25/1000 | Loss: 0.00002677
Iteration 26/1000 | Loss: 0.00002676
Iteration 27/1000 | Loss: 0.00002676
Iteration 28/1000 | Loss: 0.00002676
Iteration 29/1000 | Loss: 0.00002676
Iteration 30/1000 | Loss: 0.00002676
Iteration 31/1000 | Loss: 0.00002676
Iteration 32/1000 | Loss: 0.00002676
Iteration 33/1000 | Loss: 0.00002676
Iteration 34/1000 | Loss: 0.00002676
Iteration 35/1000 | Loss: 0.00002676
Iteration 36/1000 | Loss: 0.00002676
Iteration 37/1000 | Loss: 0.00002676
Iteration 38/1000 | Loss: 0.00002674
Iteration 39/1000 | Loss: 0.00002674
Iteration 40/1000 | Loss: 0.00002674
Iteration 41/1000 | Loss: 0.00002674
Iteration 42/1000 | Loss: 0.00002674
Iteration 43/1000 | Loss: 0.00002673
Iteration 44/1000 | Loss: 0.00002673
Iteration 45/1000 | Loss: 0.00002673
Iteration 46/1000 | Loss: 0.00002673
Iteration 47/1000 | Loss: 0.00002673
Iteration 48/1000 | Loss: 0.00002673
Iteration 49/1000 | Loss: 0.00002672
Iteration 50/1000 | Loss: 0.00002672
Iteration 51/1000 | Loss: 0.00002672
Iteration 52/1000 | Loss: 0.00002671
Iteration 53/1000 | Loss: 0.00002671
Iteration 54/1000 | Loss: 0.00002670
Iteration 55/1000 | Loss: 0.00002670
Iteration 56/1000 | Loss: 0.00002670
Iteration 57/1000 | Loss: 0.00002670
Iteration 58/1000 | Loss: 0.00002670
Iteration 59/1000 | Loss: 0.00002670
Iteration 60/1000 | Loss: 0.00002669
Iteration 61/1000 | Loss: 0.00002669
Iteration 62/1000 | Loss: 0.00002669
Iteration 63/1000 | Loss: 0.00002669
Iteration 64/1000 | Loss: 0.00002669
Iteration 65/1000 | Loss: 0.00002669
Iteration 66/1000 | Loss: 0.00002669
Iteration 67/1000 | Loss: 0.00002669
Iteration 68/1000 | Loss: 0.00002669
Iteration 69/1000 | Loss: 0.00002668
Iteration 70/1000 | Loss: 0.00002668
Iteration 71/1000 | Loss: 0.00002668
Iteration 72/1000 | Loss: 0.00002668
Iteration 73/1000 | Loss: 0.00002668
Iteration 74/1000 | Loss: 0.00002668
Iteration 75/1000 | Loss: 0.00002668
Iteration 76/1000 | Loss: 0.00002668
Iteration 77/1000 | Loss: 0.00002668
Iteration 78/1000 | Loss: 0.00002668
Iteration 79/1000 | Loss: 0.00002667
Iteration 80/1000 | Loss: 0.00002667
Iteration 81/1000 | Loss: 0.00002667
Iteration 82/1000 | Loss: 0.00002667
Iteration 83/1000 | Loss: 0.00002667
Iteration 84/1000 | Loss: 0.00002667
Iteration 85/1000 | Loss: 0.00002666
Iteration 86/1000 | Loss: 0.00002666
Iteration 87/1000 | Loss: 0.00002666
Iteration 88/1000 | Loss: 0.00002666
Iteration 89/1000 | Loss: 0.00002666
Iteration 90/1000 | Loss: 0.00002666
Iteration 91/1000 | Loss: 0.00002666
Iteration 92/1000 | Loss: 0.00002666
Iteration 93/1000 | Loss: 0.00002666
Iteration 94/1000 | Loss: 0.00002666
Iteration 95/1000 | Loss: 0.00002666
Iteration 96/1000 | Loss: 0.00002666
Iteration 97/1000 | Loss: 0.00002666
Iteration 98/1000 | Loss: 0.00002666
Iteration 99/1000 | Loss: 0.00002666
Iteration 100/1000 | Loss: 0.00002666
Iteration 101/1000 | Loss: 0.00002666
Iteration 102/1000 | Loss: 0.00002666
Iteration 103/1000 | Loss: 0.00002666
Iteration 104/1000 | Loss: 0.00002666
Iteration 105/1000 | Loss: 0.00002666
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 105. Stopping optimization.
Last 5 losses: [2.6660201910999604e-05, 2.6660201910999604e-05, 2.6660201910999604e-05, 2.6660201910999604e-05, 2.6660201910999604e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.6660201910999604e-05

Optimization complete. Final v2v error: 4.388016223907471 mm

Highest mean error: 4.748308181762695 mm for frame 6

Lowest mean error: 3.865103244781494 mm for frame 143

Saving results

Total time: 46.01552414894104
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_36_nl_6641/0010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_nl_6641/0010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_nl_6641/0010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00513529
Iteration 2/25 | Loss: 0.00140619
Iteration 3/25 | Loss: 0.00125636
Iteration 4/25 | Loss: 0.00122730
Iteration 5/25 | Loss: 0.00121846
Iteration 6/25 | Loss: 0.00121685
Iteration 7/25 | Loss: 0.00121685
Iteration 8/25 | Loss: 0.00121685
Iteration 9/25 | Loss: 0.00121685
Iteration 10/25 | Loss: 0.00121685
Iteration 11/25 | Loss: 0.00121685
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012168490793555975, 0.0012168490793555975, 0.0012168490793555975, 0.0012168490793555975, 0.0012168490793555975]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012168490793555975

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39445138
Iteration 2/25 | Loss: 0.00048961
Iteration 3/25 | Loss: 0.00048959
Iteration 4/25 | Loss: 0.00048959
Iteration 5/25 | Loss: 0.00048959
Iteration 6/25 | Loss: 0.00048959
Iteration 7/25 | Loss: 0.00048959
Iteration 8/25 | Loss: 0.00048959
Iteration 9/25 | Loss: 0.00048959
Iteration 10/25 | Loss: 0.00048959
Iteration 11/25 | Loss: 0.00048959
Iteration 12/25 | Loss: 0.00048959
Iteration 13/25 | Loss: 0.00048959
Iteration 14/25 | Loss: 0.00048959
Iteration 15/25 | Loss: 0.00048959
Iteration 16/25 | Loss: 0.00048959
Iteration 17/25 | Loss: 0.00048959
Iteration 18/25 | Loss: 0.00048959
Iteration 19/25 | Loss: 0.00048959
Iteration 20/25 | Loss: 0.00048959
Iteration 21/25 | Loss: 0.00048959
Iteration 22/25 | Loss: 0.00048959
Iteration 23/25 | Loss: 0.00048959
Iteration 24/25 | Loss: 0.00048959
Iteration 25/25 | Loss: 0.00048959

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00048959
Iteration 2/1000 | Loss: 0.00006337
Iteration 3/1000 | Loss: 0.00003852
Iteration 4/1000 | Loss: 0.00003337
Iteration 5/1000 | Loss: 0.00003050
Iteration 6/1000 | Loss: 0.00002922
Iteration 7/1000 | Loss: 0.00002828
Iteration 8/1000 | Loss: 0.00002782
Iteration 9/1000 | Loss: 0.00002712
Iteration 10/1000 | Loss: 0.00002665
Iteration 11/1000 | Loss: 0.00002636
Iteration 12/1000 | Loss: 0.00002620
Iteration 13/1000 | Loss: 0.00002618
Iteration 14/1000 | Loss: 0.00002617
Iteration 15/1000 | Loss: 0.00002617
Iteration 16/1000 | Loss: 0.00002616
Iteration 17/1000 | Loss: 0.00002616
Iteration 18/1000 | Loss: 0.00002614
Iteration 19/1000 | Loss: 0.00002614
Iteration 20/1000 | Loss: 0.00002612
Iteration 21/1000 | Loss: 0.00002611
Iteration 22/1000 | Loss: 0.00002611
Iteration 23/1000 | Loss: 0.00002610
Iteration 24/1000 | Loss: 0.00002610
Iteration 25/1000 | Loss: 0.00002609
Iteration 26/1000 | Loss: 0.00002607
Iteration 27/1000 | Loss: 0.00002607
Iteration 28/1000 | Loss: 0.00002607
Iteration 29/1000 | Loss: 0.00002607
Iteration 30/1000 | Loss: 0.00002606
Iteration 31/1000 | Loss: 0.00002606
Iteration 32/1000 | Loss: 0.00002606
Iteration 33/1000 | Loss: 0.00002606
Iteration 34/1000 | Loss: 0.00002606
Iteration 35/1000 | Loss: 0.00002604
Iteration 36/1000 | Loss: 0.00002604
Iteration 37/1000 | Loss: 0.00002604
Iteration 38/1000 | Loss: 0.00002604
Iteration 39/1000 | Loss: 0.00002603
Iteration 40/1000 | Loss: 0.00002603
Iteration 41/1000 | Loss: 0.00002603
Iteration 42/1000 | Loss: 0.00002603
Iteration 43/1000 | Loss: 0.00002603
Iteration 44/1000 | Loss: 0.00002603
Iteration 45/1000 | Loss: 0.00002602
Iteration 46/1000 | Loss: 0.00002602
Iteration 47/1000 | Loss: 0.00002601
Iteration 48/1000 | Loss: 0.00002601
Iteration 49/1000 | Loss: 0.00002601
Iteration 50/1000 | Loss: 0.00002600
Iteration 51/1000 | Loss: 0.00002600
Iteration 52/1000 | Loss: 0.00002599
Iteration 53/1000 | Loss: 0.00002599
Iteration 54/1000 | Loss: 0.00002598
Iteration 55/1000 | Loss: 0.00002598
Iteration 56/1000 | Loss: 0.00002598
Iteration 57/1000 | Loss: 0.00002597
Iteration 58/1000 | Loss: 0.00002597
Iteration 59/1000 | Loss: 0.00002596
Iteration 60/1000 | Loss: 0.00002596
Iteration 61/1000 | Loss: 0.00002596
Iteration 62/1000 | Loss: 0.00002596
Iteration 63/1000 | Loss: 0.00002596
Iteration 64/1000 | Loss: 0.00002595
Iteration 65/1000 | Loss: 0.00002595
Iteration 66/1000 | Loss: 0.00002595
Iteration 67/1000 | Loss: 0.00002595
Iteration 68/1000 | Loss: 0.00002595
Iteration 69/1000 | Loss: 0.00002594
Iteration 70/1000 | Loss: 0.00002594
Iteration 71/1000 | Loss: 0.00002594
Iteration 72/1000 | Loss: 0.00002594
Iteration 73/1000 | Loss: 0.00002593
Iteration 74/1000 | Loss: 0.00002593
Iteration 75/1000 | Loss: 0.00002592
Iteration 76/1000 | Loss: 0.00002592
Iteration 77/1000 | Loss: 0.00002592
Iteration 78/1000 | Loss: 0.00002591
Iteration 79/1000 | Loss: 0.00002591
Iteration 80/1000 | Loss: 0.00002591
Iteration 81/1000 | Loss: 0.00002591
Iteration 82/1000 | Loss: 0.00002590
Iteration 83/1000 | Loss: 0.00002590
Iteration 84/1000 | Loss: 0.00002590
Iteration 85/1000 | Loss: 0.00002590
Iteration 86/1000 | Loss: 0.00002590
Iteration 87/1000 | Loss: 0.00002589
Iteration 88/1000 | Loss: 0.00002589
Iteration 89/1000 | Loss: 0.00002589
Iteration 90/1000 | Loss: 0.00002589
Iteration 91/1000 | Loss: 0.00002588
Iteration 92/1000 | Loss: 0.00002588
Iteration 93/1000 | Loss: 0.00002588
Iteration 94/1000 | Loss: 0.00002588
Iteration 95/1000 | Loss: 0.00002588
Iteration 96/1000 | Loss: 0.00002588
Iteration 97/1000 | Loss: 0.00002587
Iteration 98/1000 | Loss: 0.00002587
Iteration 99/1000 | Loss: 0.00002587
Iteration 100/1000 | Loss: 0.00002587
Iteration 101/1000 | Loss: 0.00002587
Iteration 102/1000 | Loss: 0.00002587
Iteration 103/1000 | Loss: 0.00002586
Iteration 104/1000 | Loss: 0.00002586
Iteration 105/1000 | Loss: 0.00002586
Iteration 106/1000 | Loss: 0.00002586
Iteration 107/1000 | Loss: 0.00002586
Iteration 108/1000 | Loss: 0.00002586
Iteration 109/1000 | Loss: 0.00002586
Iteration 110/1000 | Loss: 0.00002585
Iteration 111/1000 | Loss: 0.00002585
Iteration 112/1000 | Loss: 0.00002585
Iteration 113/1000 | Loss: 0.00002585
Iteration 114/1000 | Loss: 0.00002585
Iteration 115/1000 | Loss: 0.00002585
Iteration 116/1000 | Loss: 0.00002585
Iteration 117/1000 | Loss: 0.00002585
Iteration 118/1000 | Loss: 0.00002585
Iteration 119/1000 | Loss: 0.00002585
Iteration 120/1000 | Loss: 0.00002585
Iteration 121/1000 | Loss: 0.00002585
Iteration 122/1000 | Loss: 0.00002585
Iteration 123/1000 | Loss: 0.00002585
Iteration 124/1000 | Loss: 0.00002585
Iteration 125/1000 | Loss: 0.00002585
Iteration 126/1000 | Loss: 0.00002585
Iteration 127/1000 | Loss: 0.00002585
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 127. Stopping optimization.
Last 5 losses: [2.5854100385913625e-05, 2.5854100385913625e-05, 2.5854100385913625e-05, 2.5854100385913625e-05, 2.5854100385913625e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.5854100385913625e-05

Optimization complete. Final v2v error: 4.246540069580078 mm

Highest mean error: 5.673827648162842 mm for frame 214

Lowest mean error: 3.461806535720825 mm for frame 186

Saving results

Total time: 40.550973892211914
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_39_us_1277/0008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_39_us_1277/0008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_39_us_1277/0008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01043173
Iteration 2/25 | Loss: 0.00207584
Iteration 3/25 | Loss: 0.00149288
Iteration 4/25 | Loss: 0.00138306
Iteration 5/25 | Loss: 0.00134147
Iteration 6/25 | Loss: 0.00132945
Iteration 7/25 | Loss: 0.00132149
Iteration 8/25 | Loss: 0.00131508
Iteration 9/25 | Loss: 0.00131426
Iteration 10/25 | Loss: 0.00131371
Iteration 11/25 | Loss: 0.00131490
Iteration 12/25 | Loss: 0.00131340
Iteration 13/25 | Loss: 0.00131297
Iteration 14/25 | Loss: 0.00131245
Iteration 15/25 | Loss: 0.00131228
Iteration 16/25 | Loss: 0.00131227
Iteration 17/25 | Loss: 0.00131227
Iteration 18/25 | Loss: 0.00131227
Iteration 19/25 | Loss: 0.00131226
Iteration 20/25 | Loss: 0.00131226
Iteration 21/25 | Loss: 0.00131226
Iteration 22/25 | Loss: 0.00131226
Iteration 23/25 | Loss: 0.00131226
Iteration 24/25 | Loss: 0.00131226
Iteration 25/25 | Loss: 0.00131226

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.79522216
Iteration 2/25 | Loss: 0.00129149
Iteration 3/25 | Loss: 0.00129149
Iteration 4/25 | Loss: 0.00129149
Iteration 5/25 | Loss: 0.00129149
Iteration 6/25 | Loss: 0.00129149
Iteration 7/25 | Loss: 0.00129149
Iteration 8/25 | Loss: 0.00129149
Iteration 9/25 | Loss: 0.00129149
Iteration 10/25 | Loss: 0.00129149
Iteration 11/25 | Loss: 0.00129149
Iteration 12/25 | Loss: 0.00129149
Iteration 13/25 | Loss: 0.00129149
Iteration 14/25 | Loss: 0.00129149
Iteration 15/25 | Loss: 0.00129149
Iteration 16/25 | Loss: 0.00129149
Iteration 17/25 | Loss: 0.00129149
Iteration 18/25 | Loss: 0.00129149
Iteration 19/25 | Loss: 0.00129149
Iteration 20/25 | Loss: 0.00129149
Iteration 21/25 | Loss: 0.00129149
Iteration 22/25 | Loss: 0.00129149
Iteration 23/25 | Loss: 0.00129149
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0012914894614368677, 0.0012914894614368677, 0.0012914894614368677, 0.0012914894614368677, 0.0012914894614368677]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012914894614368677

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00129149
Iteration 2/1000 | Loss: 0.00007619
Iteration 3/1000 | Loss: 0.00004385
Iteration 4/1000 | Loss: 0.00003481
Iteration 5/1000 | Loss: 0.00003161
Iteration 6/1000 | Loss: 0.00003033
Iteration 7/1000 | Loss: 0.00002921
Iteration 8/1000 | Loss: 0.00002857
Iteration 9/1000 | Loss: 0.00002802
Iteration 10/1000 | Loss: 0.00002747
Iteration 11/1000 | Loss: 0.00002721
Iteration 12/1000 | Loss: 0.00002701
Iteration 13/1000 | Loss: 0.00002699
Iteration 14/1000 | Loss: 0.00002699
Iteration 15/1000 | Loss: 0.00002697
Iteration 16/1000 | Loss: 0.00002685
Iteration 17/1000 | Loss: 0.00002684
Iteration 18/1000 | Loss: 0.00002681
Iteration 19/1000 | Loss: 0.00002680
Iteration 20/1000 | Loss: 0.00002679
Iteration 21/1000 | Loss: 0.00002679
Iteration 22/1000 | Loss: 0.00002675
Iteration 23/1000 | Loss: 0.00002675
Iteration 24/1000 | Loss: 0.00002675
Iteration 25/1000 | Loss: 0.00002674
Iteration 26/1000 | Loss: 0.00002674
Iteration 27/1000 | Loss: 0.00002674
Iteration 28/1000 | Loss: 0.00002674
Iteration 29/1000 | Loss: 0.00002674
Iteration 30/1000 | Loss: 0.00002674
Iteration 31/1000 | Loss: 0.00002674
Iteration 32/1000 | Loss: 0.00002674
Iteration 33/1000 | Loss: 0.00002674
Iteration 34/1000 | Loss: 0.00002674
Iteration 35/1000 | Loss: 0.00002673
Iteration 36/1000 | Loss: 0.00002672
Iteration 37/1000 | Loss: 0.00002672
Iteration 38/1000 | Loss: 0.00002672
Iteration 39/1000 | Loss: 0.00002671
Iteration 40/1000 | Loss: 0.00002671
Iteration 41/1000 | Loss: 0.00002671
Iteration 42/1000 | Loss: 0.00002670
Iteration 43/1000 | Loss: 0.00002670
Iteration 44/1000 | Loss: 0.00002669
Iteration 45/1000 | Loss: 0.00002669
Iteration 46/1000 | Loss: 0.00002669
Iteration 47/1000 | Loss: 0.00002668
Iteration 48/1000 | Loss: 0.00002668
Iteration 49/1000 | Loss: 0.00002667
Iteration 50/1000 | Loss: 0.00002666
Iteration 51/1000 | Loss: 0.00002666
Iteration 52/1000 | Loss: 0.00002666
Iteration 53/1000 | Loss: 0.00002666
Iteration 54/1000 | Loss: 0.00002665
Iteration 55/1000 | Loss: 0.00002665
Iteration 56/1000 | Loss: 0.00002665
Iteration 57/1000 | Loss: 0.00002664
Iteration 58/1000 | Loss: 0.00002663
Iteration 59/1000 | Loss: 0.00002663
Iteration 60/1000 | Loss: 0.00002661
Iteration 61/1000 | Loss: 0.00002661
Iteration 62/1000 | Loss: 0.00002660
Iteration 63/1000 | Loss: 0.00002660
Iteration 64/1000 | Loss: 0.00002660
Iteration 65/1000 | Loss: 0.00002659
Iteration 66/1000 | Loss: 0.00002659
Iteration 67/1000 | Loss: 0.00002658
Iteration 68/1000 | Loss: 0.00002658
Iteration 69/1000 | Loss: 0.00002657
Iteration 70/1000 | Loss: 0.00002657
Iteration 71/1000 | Loss: 0.00002656
Iteration 72/1000 | Loss: 0.00002656
Iteration 73/1000 | Loss: 0.00002656
Iteration 74/1000 | Loss: 0.00002655
Iteration 75/1000 | Loss: 0.00002655
Iteration 76/1000 | Loss: 0.00002655
Iteration 77/1000 | Loss: 0.00002655
Iteration 78/1000 | Loss: 0.00002655
Iteration 79/1000 | Loss: 0.00002654
Iteration 80/1000 | Loss: 0.00002654
Iteration 81/1000 | Loss: 0.00002654
Iteration 82/1000 | Loss: 0.00002654
Iteration 83/1000 | Loss: 0.00002653
Iteration 84/1000 | Loss: 0.00002653
Iteration 85/1000 | Loss: 0.00002653
Iteration 86/1000 | Loss: 0.00002652
Iteration 87/1000 | Loss: 0.00002652
Iteration 88/1000 | Loss: 0.00002652
Iteration 89/1000 | Loss: 0.00002652
Iteration 90/1000 | Loss: 0.00002651
Iteration 91/1000 | Loss: 0.00002651
Iteration 92/1000 | Loss: 0.00002651
Iteration 93/1000 | Loss: 0.00002651
Iteration 94/1000 | Loss: 0.00002651
Iteration 95/1000 | Loss: 0.00002651
Iteration 96/1000 | Loss: 0.00002651
Iteration 97/1000 | Loss: 0.00002651
Iteration 98/1000 | Loss: 0.00002651
Iteration 99/1000 | Loss: 0.00002650
Iteration 100/1000 | Loss: 0.00002650
Iteration 101/1000 | Loss: 0.00002650
Iteration 102/1000 | Loss: 0.00002650
Iteration 103/1000 | Loss: 0.00002650
Iteration 104/1000 | Loss: 0.00002650
Iteration 105/1000 | Loss: 0.00002650
Iteration 106/1000 | Loss: 0.00002650
Iteration 107/1000 | Loss: 0.00002650
Iteration 108/1000 | Loss: 0.00002650
Iteration 109/1000 | Loss: 0.00002650
Iteration 110/1000 | Loss: 0.00002649
Iteration 111/1000 | Loss: 0.00002649
Iteration 112/1000 | Loss: 0.00002649
Iteration 113/1000 | Loss: 0.00002649
Iteration 114/1000 | Loss: 0.00002649
Iteration 115/1000 | Loss: 0.00002649
Iteration 116/1000 | Loss: 0.00002649
Iteration 117/1000 | Loss: 0.00002649
Iteration 118/1000 | Loss: 0.00002649
Iteration 119/1000 | Loss: 0.00002649
Iteration 120/1000 | Loss: 0.00002649
Iteration 121/1000 | Loss: 0.00002649
Iteration 122/1000 | Loss: 0.00002648
Iteration 123/1000 | Loss: 0.00002648
Iteration 124/1000 | Loss: 0.00002648
Iteration 125/1000 | Loss: 0.00002648
Iteration 126/1000 | Loss: 0.00002648
Iteration 127/1000 | Loss: 0.00002648
Iteration 128/1000 | Loss: 0.00002648
Iteration 129/1000 | Loss: 0.00002648
Iteration 130/1000 | Loss: 0.00002648
Iteration 131/1000 | Loss: 0.00002648
Iteration 132/1000 | Loss: 0.00002647
Iteration 133/1000 | Loss: 0.00002647
Iteration 134/1000 | Loss: 0.00002647
Iteration 135/1000 | Loss: 0.00002647
Iteration 136/1000 | Loss: 0.00002647
Iteration 137/1000 | Loss: 0.00002647
Iteration 138/1000 | Loss: 0.00002647
Iteration 139/1000 | Loss: 0.00002647
Iteration 140/1000 | Loss: 0.00002647
Iteration 141/1000 | Loss: 0.00002647
Iteration 142/1000 | Loss: 0.00002647
Iteration 143/1000 | Loss: 0.00002646
Iteration 144/1000 | Loss: 0.00002646
Iteration 145/1000 | Loss: 0.00002646
Iteration 146/1000 | Loss: 0.00002646
Iteration 147/1000 | Loss: 0.00002645
Iteration 148/1000 | Loss: 0.00002645
Iteration 149/1000 | Loss: 0.00002645
Iteration 150/1000 | Loss: 0.00002645
Iteration 151/1000 | Loss: 0.00002645
Iteration 152/1000 | Loss: 0.00002645
Iteration 153/1000 | Loss: 0.00002645
Iteration 154/1000 | Loss: 0.00002645
Iteration 155/1000 | Loss: 0.00002645
Iteration 156/1000 | Loss: 0.00002645
Iteration 157/1000 | Loss: 0.00002645
Iteration 158/1000 | Loss: 0.00002645
Iteration 159/1000 | Loss: 0.00002645
Iteration 160/1000 | Loss: 0.00002644
Iteration 161/1000 | Loss: 0.00002644
Iteration 162/1000 | Loss: 0.00002644
Iteration 163/1000 | Loss: 0.00002644
Iteration 164/1000 | Loss: 0.00002644
Iteration 165/1000 | Loss: 0.00002644
Iteration 166/1000 | Loss: 0.00002644
Iteration 167/1000 | Loss: 0.00002644
Iteration 168/1000 | Loss: 0.00002643
Iteration 169/1000 | Loss: 0.00002643
Iteration 170/1000 | Loss: 0.00002643
Iteration 171/1000 | Loss: 0.00002643
Iteration 172/1000 | Loss: 0.00002643
Iteration 173/1000 | Loss: 0.00002643
Iteration 174/1000 | Loss: 0.00002643
Iteration 175/1000 | Loss: 0.00002643
Iteration 176/1000 | Loss: 0.00002643
Iteration 177/1000 | Loss: 0.00002643
Iteration 178/1000 | Loss: 0.00002643
Iteration 179/1000 | Loss: 0.00002643
Iteration 180/1000 | Loss: 0.00002643
Iteration 181/1000 | Loss: 0.00002643
Iteration 182/1000 | Loss: 0.00002643
Iteration 183/1000 | Loss: 0.00002643
Iteration 184/1000 | Loss: 0.00002643
Iteration 185/1000 | Loss: 0.00002642
Iteration 186/1000 | Loss: 0.00002642
Iteration 187/1000 | Loss: 0.00002642
Iteration 188/1000 | Loss: 0.00002642
Iteration 189/1000 | Loss: 0.00002642
Iteration 190/1000 | Loss: 0.00002642
Iteration 191/1000 | Loss: 0.00002642
Iteration 192/1000 | Loss: 0.00002642
Iteration 193/1000 | Loss: 0.00002642
Iteration 194/1000 | Loss: 0.00002642
Iteration 195/1000 | Loss: 0.00002642
Iteration 196/1000 | Loss: 0.00002642
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 196. Stopping optimization.
Last 5 losses: [2.6421945221954957e-05, 2.6421945221954957e-05, 2.6421945221954957e-05, 2.6421945221954957e-05, 2.6421945221954957e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.6421945221954957e-05

Optimization complete. Final v2v error: 4.368648052215576 mm

Highest mean error: 4.797235488891602 mm for frame 78

Lowest mean error: 3.9118523597717285 mm for frame 98

Saving results

Total time: 67.8289532661438
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_39_us_1277/0014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_39_us_1277/0014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_39_us_1277/0014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00413911
Iteration 2/25 | Loss: 0.00133210
Iteration 3/25 | Loss: 0.00125029
Iteration 4/25 | Loss: 0.00124282
Iteration 5/25 | Loss: 0.00123977
Iteration 6/25 | Loss: 0.00123950
Iteration 7/25 | Loss: 0.00123950
Iteration 8/25 | Loss: 0.00123950
Iteration 9/25 | Loss: 0.00123950
Iteration 10/25 | Loss: 0.00123950
Iteration 11/25 | Loss: 0.00123950
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012394965160638094, 0.0012394965160638094, 0.0012394965160638094, 0.0012394965160638094, 0.0012394965160638094]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012394965160638094

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.66464305
Iteration 2/25 | Loss: 0.00113467
Iteration 3/25 | Loss: 0.00113467
Iteration 4/25 | Loss: 0.00113467
Iteration 5/25 | Loss: 0.00113467
Iteration 6/25 | Loss: 0.00113466
Iteration 7/25 | Loss: 0.00113466
Iteration 8/25 | Loss: 0.00113466
Iteration 9/25 | Loss: 0.00113466
Iteration 10/25 | Loss: 0.00113466
Iteration 11/25 | Loss: 0.00113466
Iteration 12/25 | Loss: 0.00113466
Iteration 13/25 | Loss: 0.00113466
Iteration 14/25 | Loss: 0.00113466
Iteration 15/25 | Loss: 0.00113466
Iteration 16/25 | Loss: 0.00113466
Iteration 17/25 | Loss: 0.00113466
Iteration 18/25 | Loss: 0.00113466
Iteration 19/25 | Loss: 0.00113466
Iteration 20/25 | Loss: 0.00113466
Iteration 21/25 | Loss: 0.00113466
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.00113466358743608, 0.00113466358743608, 0.00113466358743608, 0.00113466358743608, 0.00113466358743608]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00113466358743608

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00113466
Iteration 2/1000 | Loss: 0.00004910
Iteration 3/1000 | Loss: 0.00002879
Iteration 4/1000 | Loss: 0.00002480
Iteration 5/1000 | Loss: 0.00002326
Iteration 6/1000 | Loss: 0.00002239
Iteration 7/1000 | Loss: 0.00002175
Iteration 8/1000 | Loss: 0.00002131
Iteration 9/1000 | Loss: 0.00002107
Iteration 10/1000 | Loss: 0.00002095
Iteration 11/1000 | Loss: 0.00002082
Iteration 12/1000 | Loss: 0.00002079
Iteration 13/1000 | Loss: 0.00002071
Iteration 14/1000 | Loss: 0.00002069
Iteration 15/1000 | Loss: 0.00002068
Iteration 16/1000 | Loss: 0.00002066
Iteration 17/1000 | Loss: 0.00002062
Iteration 18/1000 | Loss: 0.00002061
Iteration 19/1000 | Loss: 0.00002060
Iteration 20/1000 | Loss: 0.00002060
Iteration 21/1000 | Loss: 0.00002059
Iteration 22/1000 | Loss: 0.00002057
Iteration 23/1000 | Loss: 0.00002056
Iteration 24/1000 | Loss: 0.00002056
Iteration 25/1000 | Loss: 0.00002056
Iteration 26/1000 | Loss: 0.00002053
Iteration 27/1000 | Loss: 0.00002049
Iteration 28/1000 | Loss: 0.00002049
Iteration 29/1000 | Loss: 0.00002049
Iteration 30/1000 | Loss: 0.00002048
Iteration 31/1000 | Loss: 0.00002048
Iteration 32/1000 | Loss: 0.00002046
Iteration 33/1000 | Loss: 0.00002045
Iteration 34/1000 | Loss: 0.00002044
Iteration 35/1000 | Loss: 0.00002044
Iteration 36/1000 | Loss: 0.00002043
Iteration 37/1000 | Loss: 0.00002043
Iteration 38/1000 | Loss: 0.00002043
Iteration 39/1000 | Loss: 0.00002042
Iteration 40/1000 | Loss: 0.00002041
Iteration 41/1000 | Loss: 0.00002041
Iteration 42/1000 | Loss: 0.00002038
Iteration 43/1000 | Loss: 0.00002038
Iteration 44/1000 | Loss: 0.00002038
Iteration 45/1000 | Loss: 0.00002038
Iteration 46/1000 | Loss: 0.00002038
Iteration 47/1000 | Loss: 0.00002038
Iteration 48/1000 | Loss: 0.00002038
Iteration 49/1000 | Loss: 0.00002035
Iteration 50/1000 | Loss: 0.00002035
Iteration 51/1000 | Loss: 0.00002034
Iteration 52/1000 | Loss: 0.00002034
Iteration 53/1000 | Loss: 0.00002034
Iteration 54/1000 | Loss: 0.00002034
Iteration 55/1000 | Loss: 0.00002033
Iteration 56/1000 | Loss: 0.00002033
Iteration 57/1000 | Loss: 0.00002033
Iteration 58/1000 | Loss: 0.00002033
Iteration 59/1000 | Loss: 0.00002033
Iteration 60/1000 | Loss: 0.00002033
Iteration 61/1000 | Loss: 0.00002033
Iteration 62/1000 | Loss: 0.00002033
Iteration 63/1000 | Loss: 0.00002033
Iteration 64/1000 | Loss: 0.00002032
Iteration 65/1000 | Loss: 0.00002032
Iteration 66/1000 | Loss: 0.00002032
Iteration 67/1000 | Loss: 0.00002032
Iteration 68/1000 | Loss: 0.00002032
Iteration 69/1000 | Loss: 0.00002031
Iteration 70/1000 | Loss: 0.00002031
Iteration 71/1000 | Loss: 0.00002031
Iteration 72/1000 | Loss: 0.00002031
Iteration 73/1000 | Loss: 0.00002031
Iteration 74/1000 | Loss: 0.00002031
Iteration 75/1000 | Loss: 0.00002030
Iteration 76/1000 | Loss: 0.00002030
Iteration 77/1000 | Loss: 0.00002030
Iteration 78/1000 | Loss: 0.00002030
Iteration 79/1000 | Loss: 0.00002029
Iteration 80/1000 | Loss: 0.00002029
Iteration 81/1000 | Loss: 0.00002029
Iteration 82/1000 | Loss: 0.00002029
Iteration 83/1000 | Loss: 0.00002028
Iteration 84/1000 | Loss: 0.00002028
Iteration 85/1000 | Loss: 0.00002028
Iteration 86/1000 | Loss: 0.00002027
Iteration 87/1000 | Loss: 0.00002027
Iteration 88/1000 | Loss: 0.00002027
Iteration 89/1000 | Loss: 0.00002027
Iteration 90/1000 | Loss: 0.00002027
Iteration 91/1000 | Loss: 0.00002027
Iteration 92/1000 | Loss: 0.00002027
Iteration 93/1000 | Loss: 0.00002027
Iteration 94/1000 | Loss: 0.00002027
Iteration 95/1000 | Loss: 0.00002027
Iteration 96/1000 | Loss: 0.00002027
Iteration 97/1000 | Loss: 0.00002027
Iteration 98/1000 | Loss: 0.00002026
Iteration 99/1000 | Loss: 0.00002026
Iteration 100/1000 | Loss: 0.00002026
Iteration 101/1000 | Loss: 0.00002025
Iteration 102/1000 | Loss: 0.00002025
Iteration 103/1000 | Loss: 0.00002025
Iteration 104/1000 | Loss: 0.00002025
Iteration 105/1000 | Loss: 0.00002025
Iteration 106/1000 | Loss: 0.00002025
Iteration 107/1000 | Loss: 0.00002024
Iteration 108/1000 | Loss: 0.00002024
Iteration 109/1000 | Loss: 0.00002024
Iteration 110/1000 | Loss: 0.00002024
Iteration 111/1000 | Loss: 0.00002024
Iteration 112/1000 | Loss: 0.00002024
Iteration 113/1000 | Loss: 0.00002024
Iteration 114/1000 | Loss: 0.00002024
Iteration 115/1000 | Loss: 0.00002024
Iteration 116/1000 | Loss: 0.00002024
Iteration 117/1000 | Loss: 0.00002024
Iteration 118/1000 | Loss: 0.00002024
Iteration 119/1000 | Loss: 0.00002024
Iteration 120/1000 | Loss: 0.00002024
Iteration 121/1000 | Loss: 0.00002024
Iteration 122/1000 | Loss: 0.00002024
Iteration 123/1000 | Loss: 0.00002024
Iteration 124/1000 | Loss: 0.00002024
Iteration 125/1000 | Loss: 0.00002024
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 125. Stopping optimization.
Last 5 losses: [2.0240053345332853e-05, 2.0240053345332853e-05, 2.0240053345332853e-05, 2.0240053345332853e-05, 2.0240053345332853e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0240053345332853e-05

Optimization complete. Final v2v error: 3.978166341781616 mm

Highest mean error: 4.2722930908203125 mm for frame 65

Lowest mean error: 3.576260566711426 mm for frame 6

Saving results

Total time: 36.42620873451233
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_39_us_1277/0001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_39_us_1277/0001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_39_us_1277/0001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00507833
Iteration 2/25 | Loss: 0.00129206
Iteration 3/25 | Loss: 0.00121363
Iteration 4/25 | Loss: 0.00120564
Iteration 5/25 | Loss: 0.00120294
Iteration 6/25 | Loss: 0.00120236
Iteration 7/25 | Loss: 0.00120236
Iteration 8/25 | Loss: 0.00120236
Iteration 9/25 | Loss: 0.00120236
Iteration 10/25 | Loss: 0.00120236
Iteration 11/25 | Loss: 0.00120236
Iteration 12/25 | Loss: 0.00120236
Iteration 13/25 | Loss: 0.00120236
Iteration 14/25 | Loss: 0.00120236
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0012023609597235918, 0.0012023609597235918, 0.0012023609597235918, 0.0012023609597235918, 0.0012023609597235918]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012023609597235918

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.52360344
Iteration 2/25 | Loss: 0.00110330
Iteration 3/25 | Loss: 0.00110329
Iteration 4/25 | Loss: 0.00110329
Iteration 5/25 | Loss: 0.00110329
Iteration 6/25 | Loss: 0.00110329
Iteration 7/25 | Loss: 0.00110329
Iteration 8/25 | Loss: 0.00110329
Iteration 9/25 | Loss: 0.00110329
Iteration 10/25 | Loss: 0.00110329
Iteration 11/25 | Loss: 0.00110329
Iteration 12/25 | Loss: 0.00110329
Iteration 13/25 | Loss: 0.00110329
Iteration 14/25 | Loss: 0.00110329
Iteration 15/25 | Loss: 0.00110329
Iteration 16/25 | Loss: 0.00110329
Iteration 17/25 | Loss: 0.00110329
Iteration 18/25 | Loss: 0.00110329
Iteration 19/25 | Loss: 0.00110329
Iteration 20/25 | Loss: 0.00110329
Iteration 21/25 | Loss: 0.00110329
Iteration 22/25 | Loss: 0.00110329
Iteration 23/25 | Loss: 0.00110329
Iteration 24/25 | Loss: 0.00110329
Iteration 25/25 | Loss: 0.00110329

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00110329
Iteration 2/1000 | Loss: 0.00005000
Iteration 3/1000 | Loss: 0.00002994
Iteration 4/1000 | Loss: 0.00002440
Iteration 5/1000 | Loss: 0.00002295
Iteration 6/1000 | Loss: 0.00002190
Iteration 7/1000 | Loss: 0.00002117
Iteration 8/1000 | Loss: 0.00002077
Iteration 9/1000 | Loss: 0.00002055
Iteration 10/1000 | Loss: 0.00002032
Iteration 11/1000 | Loss: 0.00002031
Iteration 12/1000 | Loss: 0.00002015
Iteration 13/1000 | Loss: 0.00002011
Iteration 14/1000 | Loss: 0.00002009
Iteration 15/1000 | Loss: 0.00002007
Iteration 16/1000 | Loss: 0.00002006
Iteration 17/1000 | Loss: 0.00001998
Iteration 18/1000 | Loss: 0.00001989
Iteration 19/1000 | Loss: 0.00001983
Iteration 20/1000 | Loss: 0.00001982
Iteration 21/1000 | Loss: 0.00001982
Iteration 22/1000 | Loss: 0.00001982
Iteration 23/1000 | Loss: 0.00001982
Iteration 24/1000 | Loss: 0.00001982
Iteration 25/1000 | Loss: 0.00001982
Iteration 26/1000 | Loss: 0.00001978
Iteration 27/1000 | Loss: 0.00001978
Iteration 28/1000 | Loss: 0.00001977
Iteration 29/1000 | Loss: 0.00001977
Iteration 30/1000 | Loss: 0.00001977
Iteration 31/1000 | Loss: 0.00001977
Iteration 32/1000 | Loss: 0.00001977
Iteration 33/1000 | Loss: 0.00001977
Iteration 34/1000 | Loss: 0.00001975
Iteration 35/1000 | Loss: 0.00001974
Iteration 36/1000 | Loss: 0.00001974
Iteration 37/1000 | Loss: 0.00001974
Iteration 38/1000 | Loss: 0.00001973
Iteration 39/1000 | Loss: 0.00001973
Iteration 40/1000 | Loss: 0.00001972
Iteration 41/1000 | Loss: 0.00001972
Iteration 42/1000 | Loss: 0.00001972
Iteration 43/1000 | Loss: 0.00001971
Iteration 44/1000 | Loss: 0.00001970
Iteration 45/1000 | Loss: 0.00001970
Iteration 46/1000 | Loss: 0.00001969
Iteration 47/1000 | Loss: 0.00001969
Iteration 48/1000 | Loss: 0.00001969
Iteration 49/1000 | Loss: 0.00001968
Iteration 50/1000 | Loss: 0.00001967
Iteration 51/1000 | Loss: 0.00001967
Iteration 52/1000 | Loss: 0.00001967
Iteration 53/1000 | Loss: 0.00001967
Iteration 54/1000 | Loss: 0.00001967
Iteration 55/1000 | Loss: 0.00001967
Iteration 56/1000 | Loss: 0.00001967
Iteration 57/1000 | Loss: 0.00001966
Iteration 58/1000 | Loss: 0.00001966
Iteration 59/1000 | Loss: 0.00001966
Iteration 60/1000 | Loss: 0.00001966
Iteration 61/1000 | Loss: 0.00001966
Iteration 62/1000 | Loss: 0.00001965
Iteration 63/1000 | Loss: 0.00001965
Iteration 64/1000 | Loss: 0.00001965
Iteration 65/1000 | Loss: 0.00001965
Iteration 66/1000 | Loss: 0.00001964
Iteration 67/1000 | Loss: 0.00001964
Iteration 68/1000 | Loss: 0.00001964
Iteration 69/1000 | Loss: 0.00001963
Iteration 70/1000 | Loss: 0.00001963
Iteration 71/1000 | Loss: 0.00001963
Iteration 72/1000 | Loss: 0.00001963
Iteration 73/1000 | Loss: 0.00001963
Iteration 74/1000 | Loss: 0.00001963
Iteration 75/1000 | Loss: 0.00001963
Iteration 76/1000 | Loss: 0.00001963
Iteration 77/1000 | Loss: 0.00001962
Iteration 78/1000 | Loss: 0.00001962
Iteration 79/1000 | Loss: 0.00001962
Iteration 80/1000 | Loss: 0.00001961
Iteration 81/1000 | Loss: 0.00001960
Iteration 82/1000 | Loss: 0.00001960
Iteration 83/1000 | Loss: 0.00001960
Iteration 84/1000 | Loss: 0.00001959
Iteration 85/1000 | Loss: 0.00001959
Iteration 86/1000 | Loss: 0.00001958
Iteration 87/1000 | Loss: 0.00001958
Iteration 88/1000 | Loss: 0.00001957
Iteration 89/1000 | Loss: 0.00001957
Iteration 90/1000 | Loss: 0.00001957
Iteration 91/1000 | Loss: 0.00001957
Iteration 92/1000 | Loss: 0.00001956
Iteration 93/1000 | Loss: 0.00001956
Iteration 94/1000 | Loss: 0.00001956
Iteration 95/1000 | Loss: 0.00001956
Iteration 96/1000 | Loss: 0.00001956
Iteration 97/1000 | Loss: 0.00001955
Iteration 98/1000 | Loss: 0.00001955
Iteration 99/1000 | Loss: 0.00001955
Iteration 100/1000 | Loss: 0.00001954
Iteration 101/1000 | Loss: 0.00001954
Iteration 102/1000 | Loss: 0.00001954
Iteration 103/1000 | Loss: 0.00001954
Iteration 104/1000 | Loss: 0.00001954
Iteration 105/1000 | Loss: 0.00001954
Iteration 106/1000 | Loss: 0.00001954
Iteration 107/1000 | Loss: 0.00001954
Iteration 108/1000 | Loss: 0.00001954
Iteration 109/1000 | Loss: 0.00001954
Iteration 110/1000 | Loss: 0.00001953
Iteration 111/1000 | Loss: 0.00001953
Iteration 112/1000 | Loss: 0.00001953
Iteration 113/1000 | Loss: 0.00001953
Iteration 114/1000 | Loss: 0.00001952
Iteration 115/1000 | Loss: 0.00001952
Iteration 116/1000 | Loss: 0.00001952
Iteration 117/1000 | Loss: 0.00001952
Iteration 118/1000 | Loss: 0.00001952
Iteration 119/1000 | Loss: 0.00001952
Iteration 120/1000 | Loss: 0.00001952
Iteration 121/1000 | Loss: 0.00001952
Iteration 122/1000 | Loss: 0.00001952
Iteration 123/1000 | Loss: 0.00001951
Iteration 124/1000 | Loss: 0.00001951
Iteration 125/1000 | Loss: 0.00001951
Iteration 126/1000 | Loss: 0.00001951
Iteration 127/1000 | Loss: 0.00001951
Iteration 128/1000 | Loss: 0.00001951
Iteration 129/1000 | Loss: 0.00001951
Iteration 130/1000 | Loss: 0.00001951
Iteration 131/1000 | Loss: 0.00001951
Iteration 132/1000 | Loss: 0.00001951
Iteration 133/1000 | Loss: 0.00001951
Iteration 134/1000 | Loss: 0.00001951
Iteration 135/1000 | Loss: 0.00001951
Iteration 136/1000 | Loss: 0.00001951
Iteration 137/1000 | Loss: 0.00001950
Iteration 138/1000 | Loss: 0.00001950
Iteration 139/1000 | Loss: 0.00001950
Iteration 140/1000 | Loss: 0.00001950
Iteration 141/1000 | Loss: 0.00001950
Iteration 142/1000 | Loss: 0.00001949
Iteration 143/1000 | Loss: 0.00001949
Iteration 144/1000 | Loss: 0.00001949
Iteration 145/1000 | Loss: 0.00001949
Iteration 146/1000 | Loss: 0.00001949
Iteration 147/1000 | Loss: 0.00001949
Iteration 148/1000 | Loss: 0.00001949
Iteration 149/1000 | Loss: 0.00001949
Iteration 150/1000 | Loss: 0.00001948
Iteration 151/1000 | Loss: 0.00001948
Iteration 152/1000 | Loss: 0.00001948
Iteration 153/1000 | Loss: 0.00001948
Iteration 154/1000 | Loss: 0.00001948
Iteration 155/1000 | Loss: 0.00001948
Iteration 156/1000 | Loss: 0.00001948
Iteration 157/1000 | Loss: 0.00001948
Iteration 158/1000 | Loss: 0.00001948
Iteration 159/1000 | Loss: 0.00001948
Iteration 160/1000 | Loss: 0.00001948
Iteration 161/1000 | Loss: 0.00001947
Iteration 162/1000 | Loss: 0.00001947
Iteration 163/1000 | Loss: 0.00001947
Iteration 164/1000 | Loss: 0.00001947
Iteration 165/1000 | Loss: 0.00001947
Iteration 166/1000 | Loss: 0.00001947
Iteration 167/1000 | Loss: 0.00001947
Iteration 168/1000 | Loss: 0.00001947
Iteration 169/1000 | Loss: 0.00001947
Iteration 170/1000 | Loss: 0.00001947
Iteration 171/1000 | Loss: 0.00001947
Iteration 172/1000 | Loss: 0.00001947
Iteration 173/1000 | Loss: 0.00001947
Iteration 174/1000 | Loss: 0.00001947
Iteration 175/1000 | Loss: 0.00001947
Iteration 176/1000 | Loss: 0.00001947
Iteration 177/1000 | Loss: 0.00001947
Iteration 178/1000 | Loss: 0.00001947
Iteration 179/1000 | Loss: 0.00001947
Iteration 180/1000 | Loss: 0.00001947
Iteration 181/1000 | Loss: 0.00001947
Iteration 182/1000 | Loss: 0.00001947
Iteration 183/1000 | Loss: 0.00001947
Iteration 184/1000 | Loss: 0.00001947
Iteration 185/1000 | Loss: 0.00001947
Iteration 186/1000 | Loss: 0.00001947
Iteration 187/1000 | Loss: 0.00001947
Iteration 188/1000 | Loss: 0.00001947
Iteration 189/1000 | Loss: 0.00001947
Iteration 190/1000 | Loss: 0.00001947
Iteration 191/1000 | Loss: 0.00001947
Iteration 192/1000 | Loss: 0.00001947
Iteration 193/1000 | Loss: 0.00001947
Iteration 194/1000 | Loss: 0.00001947
Iteration 195/1000 | Loss: 0.00001947
Iteration 196/1000 | Loss: 0.00001947
Iteration 197/1000 | Loss: 0.00001947
Iteration 198/1000 | Loss: 0.00001947
Iteration 199/1000 | Loss: 0.00001947
Iteration 200/1000 | Loss: 0.00001947
Iteration 201/1000 | Loss: 0.00001947
Iteration 202/1000 | Loss: 0.00001947
Iteration 203/1000 | Loss: 0.00001947
Iteration 204/1000 | Loss: 0.00001947
Iteration 205/1000 | Loss: 0.00001947
Iteration 206/1000 | Loss: 0.00001947
Iteration 207/1000 | Loss: 0.00001947
Iteration 208/1000 | Loss: 0.00001947
Iteration 209/1000 | Loss: 0.00001947
Iteration 210/1000 | Loss: 0.00001947
Iteration 211/1000 | Loss: 0.00001947
Iteration 212/1000 | Loss: 0.00001947
Iteration 213/1000 | Loss: 0.00001947
Iteration 214/1000 | Loss: 0.00001947
Iteration 215/1000 | Loss: 0.00001947
Iteration 216/1000 | Loss: 0.00001947
Iteration 217/1000 | Loss: 0.00001947
Iteration 218/1000 | Loss: 0.00001947
Iteration 219/1000 | Loss: 0.00001947
Iteration 220/1000 | Loss: 0.00001947
Iteration 221/1000 | Loss: 0.00001947
Iteration 222/1000 | Loss: 0.00001947
Iteration 223/1000 | Loss: 0.00001947
Iteration 224/1000 | Loss: 0.00001947
Iteration 225/1000 | Loss: 0.00001947
Iteration 226/1000 | Loss: 0.00001947
Iteration 227/1000 | Loss: 0.00001947
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 227. Stopping optimization.
Last 5 losses: [1.9469845938147046e-05, 1.9469845938147046e-05, 1.9469845938147046e-05, 1.9469845938147046e-05, 1.9469845938147046e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9469845938147046e-05

Optimization complete. Final v2v error: 3.740785598754883 mm

Highest mean error: 4.105917930603027 mm for frame 37

Lowest mean error: 3.5186939239501953 mm for frame 25

Saving results

Total time: 40.447946071624756
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_39_us_1277/0011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_39_us_1277/0011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_39_us_1277/0011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00782266
Iteration 2/25 | Loss: 0.00192693
Iteration 3/25 | Loss: 0.00146534
Iteration 4/25 | Loss: 0.00133807
Iteration 5/25 | Loss: 0.00132516
Iteration 6/25 | Loss: 0.00130239
Iteration 7/25 | Loss: 0.00129056
Iteration 8/25 | Loss: 0.00129116
Iteration 9/25 | Loss: 0.00129827
Iteration 10/25 | Loss: 0.00127608
Iteration 11/25 | Loss: 0.00127152
Iteration 12/25 | Loss: 0.00126558
Iteration 13/25 | Loss: 0.00126382
Iteration 14/25 | Loss: 0.00126319
Iteration 15/25 | Loss: 0.00126290
Iteration 16/25 | Loss: 0.00126281
Iteration 17/25 | Loss: 0.00126278
Iteration 18/25 | Loss: 0.00126278
Iteration 19/25 | Loss: 0.00126278
Iteration 20/25 | Loss: 0.00126278
Iteration 21/25 | Loss: 0.00126278
Iteration 22/25 | Loss: 0.00126278
Iteration 23/25 | Loss: 0.00126278
Iteration 24/25 | Loss: 0.00126278
Iteration 25/25 | Loss: 0.00126278

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.01906061
Iteration 2/25 | Loss: 0.00121335
Iteration 3/25 | Loss: 0.00121335
Iteration 4/25 | Loss: 0.00121335
Iteration 5/25 | Loss: 0.00121334
Iteration 6/25 | Loss: 0.00121334
Iteration 7/25 | Loss: 0.00121334
Iteration 8/25 | Loss: 0.00121334
Iteration 9/25 | Loss: 0.00121334
Iteration 10/25 | Loss: 0.00121334
Iteration 11/25 | Loss: 0.00121334
Iteration 12/25 | Loss: 0.00121334
Iteration 13/25 | Loss: 0.00121334
Iteration 14/25 | Loss: 0.00121334
Iteration 15/25 | Loss: 0.00121334
Iteration 16/25 | Loss: 0.00121334
Iteration 17/25 | Loss: 0.00121334
Iteration 18/25 | Loss: 0.00121334
Iteration 19/25 | Loss: 0.00121334
Iteration 20/25 | Loss: 0.00121334
Iteration 21/25 | Loss: 0.00121334
Iteration 22/25 | Loss: 0.00121334
Iteration 23/25 | Loss: 0.00121334
Iteration 24/25 | Loss: 0.00121334
Iteration 25/25 | Loss: 0.00121334

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00121334
Iteration 2/1000 | Loss: 0.00012439
Iteration 3/1000 | Loss: 0.00016715
Iteration 4/1000 | Loss: 0.00006971
Iteration 5/1000 | Loss: 0.00005791
Iteration 6/1000 | Loss: 0.00017144
Iteration 7/1000 | Loss: 0.00010374
Iteration 8/1000 | Loss: 0.00014860
Iteration 9/1000 | Loss: 0.00004865
Iteration 10/1000 | Loss: 0.00004515
Iteration 11/1000 | Loss: 0.00004237
Iteration 12/1000 | Loss: 0.00004099
Iteration 13/1000 | Loss: 0.00003985
Iteration 14/1000 | Loss: 0.00003910
Iteration 15/1000 | Loss: 0.00003849
Iteration 16/1000 | Loss: 0.00003795
Iteration 17/1000 | Loss: 0.00003757
Iteration 18/1000 | Loss: 0.00003724
Iteration 19/1000 | Loss: 0.00028194
Iteration 20/1000 | Loss: 0.00041073
Iteration 21/1000 | Loss: 0.00034885
Iteration 22/1000 | Loss: 0.00034492
Iteration 23/1000 | Loss: 0.00007939
Iteration 24/1000 | Loss: 0.00006145
Iteration 25/1000 | Loss: 0.00005008
Iteration 26/1000 | Loss: 0.00004197
Iteration 27/1000 | Loss: 0.00003632
Iteration 28/1000 | Loss: 0.00003212
Iteration 29/1000 | Loss: 0.00002933
Iteration 30/1000 | Loss: 0.00002771
Iteration 31/1000 | Loss: 0.00002642
Iteration 32/1000 | Loss: 0.00002558
Iteration 33/1000 | Loss: 0.00002492
Iteration 34/1000 | Loss: 0.00002453
Iteration 35/1000 | Loss: 0.00002420
Iteration 36/1000 | Loss: 0.00002394
Iteration 37/1000 | Loss: 0.00002365
Iteration 38/1000 | Loss: 0.00002338
Iteration 39/1000 | Loss: 0.00002317
Iteration 40/1000 | Loss: 0.00002304
Iteration 41/1000 | Loss: 0.00002298
Iteration 42/1000 | Loss: 0.00002296
Iteration 43/1000 | Loss: 0.00002295
Iteration 44/1000 | Loss: 0.00002294
Iteration 45/1000 | Loss: 0.00002294
Iteration 46/1000 | Loss: 0.00002288
Iteration 47/1000 | Loss: 0.00002287
Iteration 48/1000 | Loss: 0.00002286
Iteration 49/1000 | Loss: 0.00002285
Iteration 50/1000 | Loss: 0.00002283
Iteration 51/1000 | Loss: 0.00002283
Iteration 52/1000 | Loss: 0.00002283
Iteration 53/1000 | Loss: 0.00002282
Iteration 54/1000 | Loss: 0.00002281
Iteration 55/1000 | Loss: 0.00002281
Iteration 56/1000 | Loss: 0.00002281
Iteration 57/1000 | Loss: 0.00002281
Iteration 58/1000 | Loss: 0.00002281
Iteration 59/1000 | Loss: 0.00002280
Iteration 60/1000 | Loss: 0.00002280
Iteration 61/1000 | Loss: 0.00002280
Iteration 62/1000 | Loss: 0.00002280
Iteration 63/1000 | Loss: 0.00002279
Iteration 64/1000 | Loss: 0.00002279
Iteration 65/1000 | Loss: 0.00002279
Iteration 66/1000 | Loss: 0.00002279
Iteration 67/1000 | Loss: 0.00002279
Iteration 68/1000 | Loss: 0.00002279
Iteration 69/1000 | Loss: 0.00002279
Iteration 70/1000 | Loss: 0.00002278
Iteration 71/1000 | Loss: 0.00002278
Iteration 72/1000 | Loss: 0.00002278
Iteration 73/1000 | Loss: 0.00002278
Iteration 74/1000 | Loss: 0.00002277
Iteration 75/1000 | Loss: 0.00002277
Iteration 76/1000 | Loss: 0.00002277
Iteration 77/1000 | Loss: 0.00002277
Iteration 78/1000 | Loss: 0.00002277
Iteration 79/1000 | Loss: 0.00002277
Iteration 80/1000 | Loss: 0.00002277
Iteration 81/1000 | Loss: 0.00002277
Iteration 82/1000 | Loss: 0.00002277
Iteration 83/1000 | Loss: 0.00002277
Iteration 84/1000 | Loss: 0.00002276
Iteration 85/1000 | Loss: 0.00002276
Iteration 86/1000 | Loss: 0.00002276
Iteration 87/1000 | Loss: 0.00002275
Iteration 88/1000 | Loss: 0.00002275
Iteration 89/1000 | Loss: 0.00002275
Iteration 90/1000 | Loss: 0.00002275
Iteration 91/1000 | Loss: 0.00002275
Iteration 92/1000 | Loss: 0.00002274
Iteration 93/1000 | Loss: 0.00002274
Iteration 94/1000 | Loss: 0.00002274
Iteration 95/1000 | Loss: 0.00002274
Iteration 96/1000 | Loss: 0.00002273
Iteration 97/1000 | Loss: 0.00002273
Iteration 98/1000 | Loss: 0.00002273
Iteration 99/1000 | Loss: 0.00002273
Iteration 100/1000 | Loss: 0.00002273
Iteration 101/1000 | Loss: 0.00002273
Iteration 102/1000 | Loss: 0.00002273
Iteration 103/1000 | Loss: 0.00002273
Iteration 104/1000 | Loss: 0.00002273
Iteration 105/1000 | Loss: 0.00002272
Iteration 106/1000 | Loss: 0.00002272
Iteration 107/1000 | Loss: 0.00002272
Iteration 108/1000 | Loss: 0.00002272
Iteration 109/1000 | Loss: 0.00002272
Iteration 110/1000 | Loss: 0.00002272
Iteration 111/1000 | Loss: 0.00002272
Iteration 112/1000 | Loss: 0.00002272
Iteration 113/1000 | Loss: 0.00002272
Iteration 114/1000 | Loss: 0.00002272
Iteration 115/1000 | Loss: 0.00002272
Iteration 116/1000 | Loss: 0.00002272
Iteration 117/1000 | Loss: 0.00002272
Iteration 118/1000 | Loss: 0.00002272
Iteration 119/1000 | Loss: 0.00002272
Iteration 120/1000 | Loss: 0.00002271
Iteration 121/1000 | Loss: 0.00002271
Iteration 122/1000 | Loss: 0.00002271
Iteration 123/1000 | Loss: 0.00002271
Iteration 124/1000 | Loss: 0.00002271
Iteration 125/1000 | Loss: 0.00002271
Iteration 126/1000 | Loss: 0.00002271
Iteration 127/1000 | Loss: 0.00002271
Iteration 128/1000 | Loss: 0.00002271
Iteration 129/1000 | Loss: 0.00002271
Iteration 130/1000 | Loss: 0.00002271
Iteration 131/1000 | Loss: 0.00002270
Iteration 132/1000 | Loss: 0.00002270
Iteration 133/1000 | Loss: 0.00002270
Iteration 134/1000 | Loss: 0.00002270
Iteration 135/1000 | Loss: 0.00002270
Iteration 136/1000 | Loss: 0.00002270
Iteration 137/1000 | Loss: 0.00002270
Iteration 138/1000 | Loss: 0.00002270
Iteration 139/1000 | Loss: 0.00002270
Iteration 140/1000 | Loss: 0.00002270
Iteration 141/1000 | Loss: 0.00002269
Iteration 142/1000 | Loss: 0.00002269
Iteration 143/1000 | Loss: 0.00002269
Iteration 144/1000 | Loss: 0.00002269
Iteration 145/1000 | Loss: 0.00002269
Iteration 146/1000 | Loss: 0.00002269
Iteration 147/1000 | Loss: 0.00002269
Iteration 148/1000 | Loss: 0.00002269
Iteration 149/1000 | Loss: 0.00002269
Iteration 150/1000 | Loss: 0.00002269
Iteration 151/1000 | Loss: 0.00002269
Iteration 152/1000 | Loss: 0.00002269
Iteration 153/1000 | Loss: 0.00002269
Iteration 154/1000 | Loss: 0.00002269
Iteration 155/1000 | Loss: 0.00002269
Iteration 156/1000 | Loss: 0.00002269
Iteration 157/1000 | Loss: 0.00002269
Iteration 158/1000 | Loss: 0.00002269
Iteration 159/1000 | Loss: 0.00002269
Iteration 160/1000 | Loss: 0.00002269
Iteration 161/1000 | Loss: 0.00002269
Iteration 162/1000 | Loss: 0.00002269
Iteration 163/1000 | Loss: 0.00002269
Iteration 164/1000 | Loss: 0.00002268
Iteration 165/1000 | Loss: 0.00002268
Iteration 166/1000 | Loss: 0.00002268
Iteration 167/1000 | Loss: 0.00002268
Iteration 168/1000 | Loss: 0.00002268
Iteration 169/1000 | Loss: 0.00002268
Iteration 170/1000 | Loss: 0.00002268
Iteration 171/1000 | Loss: 0.00002268
Iteration 172/1000 | Loss: 0.00002268
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 172. Stopping optimization.
Last 5 losses: [2.2683841962134466e-05, 2.2683841962134466e-05, 2.2683841962134466e-05, 2.2683841962134466e-05, 2.2683841962134466e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2683841962134466e-05

Optimization complete. Final v2v error: 4.019509315490723 mm

Highest mean error: 4.763370037078857 mm for frame 239

Lowest mean error: 3.4536261558532715 mm for frame 150

Saving results

Total time: 110.80418038368225
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_39_us_1277/0000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_39_us_1277/0000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_39_us_1277/0000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00828698
Iteration 2/25 | Loss: 0.00177725
Iteration 3/25 | Loss: 0.00142180
Iteration 4/25 | Loss: 0.00136691
Iteration 5/25 | Loss: 0.00134649
Iteration 6/25 | Loss: 0.00134977
Iteration 7/25 | Loss: 0.00134934
Iteration 8/25 | Loss: 0.00133626
Iteration 9/25 | Loss: 0.00132968
Iteration 10/25 | Loss: 0.00132409
Iteration 11/25 | Loss: 0.00132700
Iteration 12/25 | Loss: 0.00133085
Iteration 13/25 | Loss: 0.00132138
Iteration 14/25 | Loss: 0.00131283
Iteration 15/25 | Loss: 0.00131005
Iteration 16/25 | Loss: 0.00130909
Iteration 17/25 | Loss: 0.00130730
Iteration 18/25 | Loss: 0.00130682
Iteration 19/25 | Loss: 0.00130734
Iteration 20/25 | Loss: 0.00130693
Iteration 21/25 | Loss: 0.00130696
Iteration 22/25 | Loss: 0.00130661
Iteration 23/25 | Loss: 0.00130658
Iteration 24/25 | Loss: 0.00130582
Iteration 25/25 | Loss: 0.00130703

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.29001749
Iteration 2/25 | Loss: 0.00092446
Iteration 3/25 | Loss: 0.00092446
Iteration 4/25 | Loss: 0.00092446
Iteration 5/25 | Loss: 0.00092446
Iteration 6/25 | Loss: 0.00092446
Iteration 7/25 | Loss: 0.00092446
Iteration 8/25 | Loss: 0.00092446
Iteration 9/25 | Loss: 0.00092446
Iteration 10/25 | Loss: 0.00092446
Iteration 11/25 | Loss: 0.00092446
Iteration 12/25 | Loss: 0.00092446
Iteration 13/25 | Loss: 0.00092446
Iteration 14/25 | Loss: 0.00092446
Iteration 15/25 | Loss: 0.00092446
Iteration 16/25 | Loss: 0.00092446
Iteration 17/25 | Loss: 0.00092446
Iteration 18/25 | Loss: 0.00092446
Iteration 19/25 | Loss: 0.00092446
Iteration 20/25 | Loss: 0.00092446
Iteration 21/25 | Loss: 0.00092446
Iteration 22/25 | Loss: 0.00092446
Iteration 23/25 | Loss: 0.00092446
Iteration 24/25 | Loss: 0.00092446
Iteration 25/25 | Loss: 0.00092446

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00092446
Iteration 2/1000 | Loss: 0.00010050
Iteration 3/1000 | Loss: 0.00007209
Iteration 4/1000 | Loss: 0.00006335
Iteration 5/1000 | Loss: 0.00004310
Iteration 6/1000 | Loss: 0.00005682
Iteration 7/1000 | Loss: 0.00004106
Iteration 8/1000 | Loss: 0.00004279
Iteration 9/1000 | Loss: 0.00004483
Iteration 10/1000 | Loss: 0.00004485
Iteration 11/1000 | Loss: 0.00005433
Iteration 12/1000 | Loss: 0.00005762
Iteration 13/1000 | Loss: 0.00093002
Iteration 14/1000 | Loss: 0.00053270
Iteration 15/1000 | Loss: 0.00070900
Iteration 16/1000 | Loss: 0.00066769
Iteration 17/1000 | Loss: 0.00055592
Iteration 18/1000 | Loss: 0.00018548
Iteration 19/1000 | Loss: 0.00030072
Iteration 20/1000 | Loss: 0.00006709
Iteration 21/1000 | Loss: 0.00006116
Iteration 22/1000 | Loss: 0.00004555
Iteration 23/1000 | Loss: 0.00004678
Iteration 24/1000 | Loss: 0.00023978
Iteration 25/1000 | Loss: 0.00008650
Iteration 26/1000 | Loss: 0.00006083
Iteration 27/1000 | Loss: 0.00004321
Iteration 28/1000 | Loss: 0.00005255
Iteration 29/1000 | Loss: 0.00003931
Iteration 30/1000 | Loss: 0.00004040
Iteration 31/1000 | Loss: 0.00005509
Iteration 32/1000 | Loss: 0.00004604
Iteration 33/1000 | Loss: 0.00003845
Iteration 34/1000 | Loss: 0.00005114
Iteration 35/1000 | Loss: 0.00003574
Iteration 36/1000 | Loss: 0.00004218
Iteration 37/1000 | Loss: 0.00005149
Iteration 38/1000 | Loss: 0.00005174
Iteration 39/1000 | Loss: 0.00004968
Iteration 40/1000 | Loss: 0.00004583
Iteration 41/1000 | Loss: 0.00004478
Iteration 42/1000 | Loss: 0.00005943
Iteration 43/1000 | Loss: 0.00023812
Iteration 44/1000 | Loss: 0.00036854
Iteration 45/1000 | Loss: 0.00012403
Iteration 46/1000 | Loss: 0.00047352
Iteration 47/1000 | Loss: 0.00012554
Iteration 48/1000 | Loss: 0.00020648
Iteration 49/1000 | Loss: 0.00003715
Iteration 50/1000 | Loss: 0.00022994
Iteration 51/1000 | Loss: 0.00003555
Iteration 52/1000 | Loss: 0.00003166
Iteration 53/1000 | Loss: 0.00032184
Iteration 54/1000 | Loss: 0.00004096
Iteration 55/1000 | Loss: 0.00003412
Iteration 56/1000 | Loss: 0.00003165
Iteration 57/1000 | Loss: 0.00003029
Iteration 58/1000 | Loss: 0.00002914
Iteration 59/1000 | Loss: 0.00002848
Iteration 60/1000 | Loss: 0.00003968
Iteration 61/1000 | Loss: 0.00003749
Iteration 62/1000 | Loss: 0.00003105
Iteration 63/1000 | Loss: 0.00003756
Iteration 64/1000 | Loss: 0.00003977
Iteration 65/1000 | Loss: 0.00002827
Iteration 66/1000 | Loss: 0.00002775
Iteration 67/1000 | Loss: 0.00002751
Iteration 68/1000 | Loss: 0.00002737
Iteration 69/1000 | Loss: 0.00002735
Iteration 70/1000 | Loss: 0.00002727
Iteration 71/1000 | Loss: 0.00002718
Iteration 72/1000 | Loss: 0.00002696
Iteration 73/1000 | Loss: 0.00002692
Iteration 74/1000 | Loss: 0.00002692
Iteration 75/1000 | Loss: 0.00002692
Iteration 76/1000 | Loss: 0.00002691
Iteration 77/1000 | Loss: 0.00002691
Iteration 78/1000 | Loss: 0.00002683
Iteration 79/1000 | Loss: 0.00002679
Iteration 80/1000 | Loss: 0.00002679
Iteration 81/1000 | Loss: 0.00002678
Iteration 82/1000 | Loss: 0.00002678
Iteration 83/1000 | Loss: 0.00002677
Iteration 84/1000 | Loss: 0.00002677
Iteration 85/1000 | Loss: 0.00002677
Iteration 86/1000 | Loss: 0.00002677
Iteration 87/1000 | Loss: 0.00002677
Iteration 88/1000 | Loss: 0.00002677
Iteration 89/1000 | Loss: 0.00002677
Iteration 90/1000 | Loss: 0.00002677
Iteration 91/1000 | Loss: 0.00002677
Iteration 92/1000 | Loss: 0.00002677
Iteration 93/1000 | Loss: 0.00002677
Iteration 94/1000 | Loss: 0.00002676
Iteration 95/1000 | Loss: 0.00002676
Iteration 96/1000 | Loss: 0.00002675
Iteration 97/1000 | Loss: 0.00002675
Iteration 98/1000 | Loss: 0.00002674
Iteration 99/1000 | Loss: 0.00002674
Iteration 100/1000 | Loss: 0.00002673
Iteration 101/1000 | Loss: 0.00002673
Iteration 102/1000 | Loss: 0.00002673
Iteration 103/1000 | Loss: 0.00002673
Iteration 104/1000 | Loss: 0.00002673
Iteration 105/1000 | Loss: 0.00002673
Iteration 106/1000 | Loss: 0.00002673
Iteration 107/1000 | Loss: 0.00002672
Iteration 108/1000 | Loss: 0.00002672
Iteration 109/1000 | Loss: 0.00002672
Iteration 110/1000 | Loss: 0.00002672
Iteration 111/1000 | Loss: 0.00002672
Iteration 112/1000 | Loss: 0.00002671
Iteration 113/1000 | Loss: 0.00002671
Iteration 114/1000 | Loss: 0.00002671
Iteration 115/1000 | Loss: 0.00002670
Iteration 116/1000 | Loss: 0.00002670
Iteration 117/1000 | Loss: 0.00002670
Iteration 118/1000 | Loss: 0.00002670
Iteration 119/1000 | Loss: 0.00002669
Iteration 120/1000 | Loss: 0.00002669
Iteration 121/1000 | Loss: 0.00002669
Iteration 122/1000 | Loss: 0.00002669
Iteration 123/1000 | Loss: 0.00002669
Iteration 124/1000 | Loss: 0.00002669
Iteration 125/1000 | Loss: 0.00002669
Iteration 126/1000 | Loss: 0.00002669
Iteration 127/1000 | Loss: 0.00002668
Iteration 128/1000 | Loss: 0.00002668
Iteration 129/1000 | Loss: 0.00002668
Iteration 130/1000 | Loss: 0.00002668
Iteration 131/1000 | Loss: 0.00002667
Iteration 132/1000 | Loss: 0.00002667
Iteration 133/1000 | Loss: 0.00002666
Iteration 134/1000 | Loss: 0.00002666
Iteration 135/1000 | Loss: 0.00002666
Iteration 136/1000 | Loss: 0.00002666
Iteration 137/1000 | Loss: 0.00002666
Iteration 138/1000 | Loss: 0.00002665
Iteration 139/1000 | Loss: 0.00002665
Iteration 140/1000 | Loss: 0.00002665
Iteration 141/1000 | Loss: 0.00002665
Iteration 142/1000 | Loss: 0.00002665
Iteration 143/1000 | Loss: 0.00002665
Iteration 144/1000 | Loss: 0.00002665
Iteration 145/1000 | Loss: 0.00002664
Iteration 146/1000 | Loss: 0.00002664
Iteration 147/1000 | Loss: 0.00002664
Iteration 148/1000 | Loss: 0.00002663
Iteration 149/1000 | Loss: 0.00002663
Iteration 150/1000 | Loss: 0.00002663
Iteration 151/1000 | Loss: 0.00002663
Iteration 152/1000 | Loss: 0.00002663
Iteration 153/1000 | Loss: 0.00002663
Iteration 154/1000 | Loss: 0.00002663
Iteration 155/1000 | Loss: 0.00002662
Iteration 156/1000 | Loss: 0.00002661
Iteration 157/1000 | Loss: 0.00002661
Iteration 158/1000 | Loss: 0.00002660
Iteration 159/1000 | Loss: 0.00002660
Iteration 160/1000 | Loss: 0.00002660
Iteration 161/1000 | Loss: 0.00002660
Iteration 162/1000 | Loss: 0.00002660
Iteration 163/1000 | Loss: 0.00002660
Iteration 164/1000 | Loss: 0.00002660
Iteration 165/1000 | Loss: 0.00002660
Iteration 166/1000 | Loss: 0.00002660
Iteration 167/1000 | Loss: 0.00002659
Iteration 168/1000 | Loss: 0.00002659
Iteration 169/1000 | Loss: 0.00002659
Iteration 170/1000 | Loss: 0.00002659
Iteration 171/1000 | Loss: 0.00002659
Iteration 172/1000 | Loss: 0.00002659
Iteration 173/1000 | Loss: 0.00002658
Iteration 174/1000 | Loss: 0.00002658
Iteration 175/1000 | Loss: 0.00002658
Iteration 176/1000 | Loss: 0.00002658
Iteration 177/1000 | Loss: 0.00002658
Iteration 178/1000 | Loss: 0.00002658
Iteration 179/1000 | Loss: 0.00002658
Iteration 180/1000 | Loss: 0.00002658
Iteration 181/1000 | Loss: 0.00002658
Iteration 182/1000 | Loss: 0.00002657
Iteration 183/1000 | Loss: 0.00002657
Iteration 184/1000 | Loss: 0.00002657
Iteration 185/1000 | Loss: 0.00002657
Iteration 186/1000 | Loss: 0.00002656
Iteration 187/1000 | Loss: 0.00002656
Iteration 188/1000 | Loss: 0.00002656
Iteration 189/1000 | Loss: 0.00002656
Iteration 190/1000 | Loss: 0.00002656
Iteration 191/1000 | Loss: 0.00002656
Iteration 192/1000 | Loss: 0.00002655
Iteration 193/1000 | Loss: 0.00002655
Iteration 194/1000 | Loss: 0.00002655
Iteration 195/1000 | Loss: 0.00002655
Iteration 196/1000 | Loss: 0.00002655
Iteration 197/1000 | Loss: 0.00002654
Iteration 198/1000 | Loss: 0.00002654
Iteration 199/1000 | Loss: 0.00002654
Iteration 200/1000 | Loss: 0.00002654
Iteration 201/1000 | Loss: 0.00002654
Iteration 202/1000 | Loss: 0.00002654
Iteration 203/1000 | Loss: 0.00002654
Iteration 204/1000 | Loss: 0.00002654
Iteration 205/1000 | Loss: 0.00002654
Iteration 206/1000 | Loss: 0.00002653
Iteration 207/1000 | Loss: 0.00002653
Iteration 208/1000 | Loss: 0.00002653
Iteration 209/1000 | Loss: 0.00002653
Iteration 210/1000 | Loss: 0.00002653
Iteration 211/1000 | Loss: 0.00002653
Iteration 212/1000 | Loss: 0.00002653
Iteration 213/1000 | Loss: 0.00002653
Iteration 214/1000 | Loss: 0.00002653
Iteration 215/1000 | Loss: 0.00002653
Iteration 216/1000 | Loss: 0.00002653
Iteration 217/1000 | Loss: 0.00002653
Iteration 218/1000 | Loss: 0.00002653
Iteration 219/1000 | Loss: 0.00002653
Iteration 220/1000 | Loss: 0.00002653
Iteration 221/1000 | Loss: 0.00002653
Iteration 222/1000 | Loss: 0.00002653
Iteration 223/1000 | Loss: 0.00002653
Iteration 224/1000 | Loss: 0.00002653
Iteration 225/1000 | Loss: 0.00002653
Iteration 226/1000 | Loss: 0.00002653
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 226. Stopping optimization.
Last 5 losses: [2.6527513909968548e-05, 2.6527513909968548e-05, 2.6527513909968548e-05, 2.6527513909968548e-05, 2.6527513909968548e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.6527513909968548e-05

Optimization complete. Final v2v error: 4.313992500305176 mm

Highest mean error: 5.495379447937012 mm for frame 102

Lowest mean error: 3.6983439922332764 mm for frame 2

Saving results

Total time: 181.2674708366394
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_39_us_1277/0002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_39_us_1277/0002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_39_us_1277/0002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00752296
Iteration 2/25 | Loss: 0.00162775
Iteration 3/25 | Loss: 0.00132687
Iteration 4/25 | Loss: 0.00128140
Iteration 5/25 | Loss: 0.00126145
Iteration 6/25 | Loss: 0.00125924
Iteration 7/25 | Loss: 0.00125678
Iteration 8/25 | Loss: 0.00125707
Iteration 9/25 | Loss: 0.00125313
Iteration 10/25 | Loss: 0.00125280
Iteration 11/25 | Loss: 0.00125535
Iteration 12/25 | Loss: 0.00125291
Iteration 13/25 | Loss: 0.00125267
Iteration 14/25 | Loss: 0.00125266
Iteration 15/25 | Loss: 0.00125266
Iteration 16/25 | Loss: 0.00125266
Iteration 17/25 | Loss: 0.00125266
Iteration 18/25 | Loss: 0.00125266
Iteration 19/25 | Loss: 0.00125266
Iteration 20/25 | Loss: 0.00125266
Iteration 21/25 | Loss: 0.00125266
Iteration 22/25 | Loss: 0.00125266
Iteration 23/25 | Loss: 0.00125266
Iteration 24/25 | Loss: 0.00125266
Iteration 25/25 | Loss: 0.00125266

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.44777727
Iteration 2/25 | Loss: 0.00101293
Iteration 3/25 | Loss: 0.00101291
Iteration 4/25 | Loss: 0.00101291
Iteration 5/25 | Loss: 0.00101291
Iteration 6/25 | Loss: 0.00101291
Iteration 7/25 | Loss: 0.00101291
Iteration 8/25 | Loss: 0.00101291
Iteration 9/25 | Loss: 0.00101291
Iteration 10/25 | Loss: 0.00101291
Iteration 11/25 | Loss: 0.00101291
Iteration 12/25 | Loss: 0.00101291
Iteration 13/25 | Loss: 0.00101291
Iteration 14/25 | Loss: 0.00101291
Iteration 15/25 | Loss: 0.00101291
Iteration 16/25 | Loss: 0.00101291
Iteration 17/25 | Loss: 0.00101291
Iteration 18/25 | Loss: 0.00101291
Iteration 19/25 | Loss: 0.00101291
Iteration 20/25 | Loss: 0.00101291
Iteration 21/25 | Loss: 0.00101291
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0010129079455509782, 0.0010129079455509782, 0.0010129079455509782, 0.0010129079455509782, 0.0010129079455509782]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010129079455509782

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00101291
Iteration 2/1000 | Loss: 0.00006517
Iteration 3/1000 | Loss: 0.00003993
Iteration 4/1000 | Loss: 0.00002785
Iteration 5/1000 | Loss: 0.00002554
Iteration 6/1000 | Loss: 0.00002451
Iteration 7/1000 | Loss: 0.00002382
Iteration 8/1000 | Loss: 0.00008354
Iteration 9/1000 | Loss: 0.00003740
Iteration 10/1000 | Loss: 0.00002303
Iteration 11/1000 | Loss: 0.00002303
Iteration 12/1000 | Loss: 0.00007943
Iteration 13/1000 | Loss: 0.00002958
Iteration 14/1000 | Loss: 0.00002414
Iteration 15/1000 | Loss: 0.00002235
Iteration 16/1000 | Loss: 0.00002226
Iteration 17/1000 | Loss: 0.00002225
Iteration 18/1000 | Loss: 0.00002209
Iteration 19/1000 | Loss: 0.00002205
Iteration 20/1000 | Loss: 0.00002805
Iteration 21/1000 | Loss: 0.00008585
Iteration 22/1000 | Loss: 0.00002752
Iteration 23/1000 | Loss: 0.00003285
Iteration 24/1000 | Loss: 0.00002221
Iteration 25/1000 | Loss: 0.00002189
Iteration 26/1000 | Loss: 0.00002186
Iteration 27/1000 | Loss: 0.00002180
Iteration 28/1000 | Loss: 0.00002477
Iteration 29/1000 | Loss: 0.00002175
Iteration 30/1000 | Loss: 0.00002175
Iteration 31/1000 | Loss: 0.00002174
Iteration 32/1000 | Loss: 0.00002174
Iteration 33/1000 | Loss: 0.00002174
Iteration 34/1000 | Loss: 0.00002174
Iteration 35/1000 | Loss: 0.00002174
Iteration 36/1000 | Loss: 0.00002174
Iteration 37/1000 | Loss: 0.00002174
Iteration 38/1000 | Loss: 0.00002174
Iteration 39/1000 | Loss: 0.00002174
Iteration 40/1000 | Loss: 0.00002174
Iteration 41/1000 | Loss: 0.00002323
Iteration 42/1000 | Loss: 0.00002170
Iteration 43/1000 | Loss: 0.00002169
Iteration 44/1000 | Loss: 0.00002169
Iteration 45/1000 | Loss: 0.00002169
Iteration 46/1000 | Loss: 0.00002169
Iteration 47/1000 | Loss: 0.00002169
Iteration 48/1000 | Loss: 0.00002169
Iteration 49/1000 | Loss: 0.00002169
Iteration 50/1000 | Loss: 0.00002169
Iteration 51/1000 | Loss: 0.00002194
Iteration 52/1000 | Loss: 0.00002167
Iteration 53/1000 | Loss: 0.00002167
Iteration 54/1000 | Loss: 0.00002167
Iteration 55/1000 | Loss: 0.00002167
Iteration 56/1000 | Loss: 0.00002167
Iteration 57/1000 | Loss: 0.00002167
Iteration 58/1000 | Loss: 0.00002167
Iteration 59/1000 | Loss: 0.00002167
Iteration 60/1000 | Loss: 0.00002166
Iteration 61/1000 | Loss: 0.00002165
Iteration 62/1000 | Loss: 0.00002165
Iteration 63/1000 | Loss: 0.00002165
Iteration 64/1000 | Loss: 0.00002165
Iteration 65/1000 | Loss: 0.00002165
Iteration 66/1000 | Loss: 0.00002165
Iteration 67/1000 | Loss: 0.00002165
Iteration 68/1000 | Loss: 0.00002165
Iteration 69/1000 | Loss: 0.00002164
Iteration 70/1000 | Loss: 0.00002164
Iteration 71/1000 | Loss: 0.00002164
Iteration 72/1000 | Loss: 0.00002164
Iteration 73/1000 | Loss: 0.00002163
Iteration 74/1000 | Loss: 0.00002163
Iteration 75/1000 | Loss: 0.00002163
Iteration 76/1000 | Loss: 0.00002163
Iteration 77/1000 | Loss: 0.00002163
Iteration 78/1000 | Loss: 0.00002163
Iteration 79/1000 | Loss: 0.00002163
Iteration 80/1000 | Loss: 0.00002162
Iteration 81/1000 | Loss: 0.00002162
Iteration 82/1000 | Loss: 0.00002162
Iteration 83/1000 | Loss: 0.00002162
Iteration 84/1000 | Loss: 0.00002162
Iteration 85/1000 | Loss: 0.00008429
Iteration 86/1000 | Loss: 0.00002818
Iteration 87/1000 | Loss: 0.00002171
Iteration 88/1000 | Loss: 0.00002168
Iteration 89/1000 | Loss: 0.00004265
Iteration 90/1000 | Loss: 0.00002163
Iteration 91/1000 | Loss: 0.00002160
Iteration 92/1000 | Loss: 0.00002156
Iteration 93/1000 | Loss: 0.00002156
Iteration 94/1000 | Loss: 0.00002156
Iteration 95/1000 | Loss: 0.00002156
Iteration 96/1000 | Loss: 0.00002155
Iteration 97/1000 | Loss: 0.00002155
Iteration 98/1000 | Loss: 0.00002155
Iteration 99/1000 | Loss: 0.00002155
Iteration 100/1000 | Loss: 0.00002155
Iteration 101/1000 | Loss: 0.00002153
Iteration 102/1000 | Loss: 0.00002153
Iteration 103/1000 | Loss: 0.00002153
Iteration 104/1000 | Loss: 0.00002153
Iteration 105/1000 | Loss: 0.00002153
Iteration 106/1000 | Loss: 0.00002152
Iteration 107/1000 | Loss: 0.00002152
Iteration 108/1000 | Loss: 0.00002152
Iteration 109/1000 | Loss: 0.00002152
Iteration 110/1000 | Loss: 0.00002152
Iteration 111/1000 | Loss: 0.00002152
Iteration 112/1000 | Loss: 0.00002151
Iteration 113/1000 | Loss: 0.00002151
Iteration 114/1000 | Loss: 0.00002151
Iteration 115/1000 | Loss: 0.00002151
Iteration 116/1000 | Loss: 0.00002151
Iteration 117/1000 | Loss: 0.00002150
Iteration 118/1000 | Loss: 0.00002150
Iteration 119/1000 | Loss: 0.00002150
Iteration 120/1000 | Loss: 0.00002150
Iteration 121/1000 | Loss: 0.00002149
Iteration 122/1000 | Loss: 0.00002286
Iteration 123/1000 | Loss: 0.00002148
Iteration 124/1000 | Loss: 0.00002147
Iteration 125/1000 | Loss: 0.00002145
Iteration 126/1000 | Loss: 0.00002145
Iteration 127/1000 | Loss: 0.00002145
Iteration 128/1000 | Loss: 0.00002145
Iteration 129/1000 | Loss: 0.00002144
Iteration 130/1000 | Loss: 0.00002144
Iteration 131/1000 | Loss: 0.00002144
Iteration 132/1000 | Loss: 0.00002143
Iteration 133/1000 | Loss: 0.00002143
Iteration 134/1000 | Loss: 0.00002142
Iteration 135/1000 | Loss: 0.00002142
Iteration 136/1000 | Loss: 0.00002394
Iteration 137/1000 | Loss: 0.00002141
Iteration 138/1000 | Loss: 0.00002140
Iteration 139/1000 | Loss: 0.00002140
Iteration 140/1000 | Loss: 0.00002139
Iteration 141/1000 | Loss: 0.00002139
Iteration 142/1000 | Loss: 0.00002139
Iteration 143/1000 | Loss: 0.00002139
Iteration 144/1000 | Loss: 0.00002139
Iteration 145/1000 | Loss: 0.00002139
Iteration 146/1000 | Loss: 0.00002139
Iteration 147/1000 | Loss: 0.00002139
Iteration 148/1000 | Loss: 0.00002139
Iteration 149/1000 | Loss: 0.00002139
Iteration 150/1000 | Loss: 0.00002139
Iteration 151/1000 | Loss: 0.00002139
Iteration 152/1000 | Loss: 0.00002139
Iteration 153/1000 | Loss: 0.00002139
Iteration 154/1000 | Loss: 0.00002139
Iteration 155/1000 | Loss: 0.00002139
Iteration 156/1000 | Loss: 0.00002139
Iteration 157/1000 | Loss: 0.00002139
Iteration 158/1000 | Loss: 0.00002139
Iteration 159/1000 | Loss: 0.00002139
Iteration 160/1000 | Loss: 0.00002139
Iteration 161/1000 | Loss: 0.00002139
Iteration 162/1000 | Loss: 0.00002139
Iteration 163/1000 | Loss: 0.00002139
Iteration 164/1000 | Loss: 0.00002139
Iteration 165/1000 | Loss: 0.00002139
Iteration 166/1000 | Loss: 0.00002139
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 166. Stopping optimization.
Last 5 losses: [2.138714444299694e-05, 2.138714444299694e-05, 2.138714444299694e-05, 2.138714444299694e-05, 2.138714444299694e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.138714444299694e-05

Optimization complete. Final v2v error: 3.950082302093506 mm

Highest mean error: 5.041125774383545 mm for frame 67

Lowest mean error: 3.2781565189361572 mm for frame 238

Saving results

Total time: 85.37580800056458
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_39_us_1277/0022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_39_us_1277/0022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_39_us_1277/0022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00988230
Iteration 2/25 | Loss: 0.00138896
Iteration 3/25 | Loss: 0.00126088
Iteration 4/25 | Loss: 0.00125332
Iteration 5/25 | Loss: 0.00125063
Iteration 6/25 | Loss: 0.00124990
Iteration 7/25 | Loss: 0.00124990
Iteration 8/25 | Loss: 0.00124990
Iteration 9/25 | Loss: 0.00124990
Iteration 10/25 | Loss: 0.00124990
Iteration 11/25 | Loss: 0.00124990
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012498997384682298, 0.0012498997384682298, 0.0012498997384682298, 0.0012498997384682298, 0.0012498997384682298]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012498997384682298

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.17164159
Iteration 2/25 | Loss: 0.00088605
Iteration 3/25 | Loss: 0.00088604
Iteration 4/25 | Loss: 0.00088603
Iteration 5/25 | Loss: 0.00088603
Iteration 6/25 | Loss: 0.00088603
Iteration 7/25 | Loss: 0.00088603
Iteration 8/25 | Loss: 0.00088603
Iteration 9/25 | Loss: 0.00088603
Iteration 10/25 | Loss: 0.00088603
Iteration 11/25 | Loss: 0.00088603
Iteration 12/25 | Loss: 0.00088603
Iteration 13/25 | Loss: 0.00088603
Iteration 14/25 | Loss: 0.00088603
Iteration 15/25 | Loss: 0.00088603
Iteration 16/25 | Loss: 0.00088603
Iteration 17/25 | Loss: 0.00088603
Iteration 18/25 | Loss: 0.00088603
Iteration 19/25 | Loss: 0.00088603
Iteration 20/25 | Loss: 0.00088603
Iteration 21/25 | Loss: 0.00088603
Iteration 22/25 | Loss: 0.00088603
Iteration 23/25 | Loss: 0.00088603
Iteration 24/25 | Loss: 0.00088603
Iteration 25/25 | Loss: 0.00088603

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00088603
Iteration 2/1000 | Loss: 0.00005493
Iteration 3/1000 | Loss: 0.00003356
Iteration 4/1000 | Loss: 0.00002700
Iteration 5/1000 | Loss: 0.00002524
Iteration 6/1000 | Loss: 0.00002436
Iteration 7/1000 | Loss: 0.00002366
Iteration 8/1000 | Loss: 0.00002328
Iteration 9/1000 | Loss: 0.00002283
Iteration 10/1000 | Loss: 0.00002257
Iteration 11/1000 | Loss: 0.00002255
Iteration 12/1000 | Loss: 0.00002244
Iteration 13/1000 | Loss: 0.00002243
Iteration 14/1000 | Loss: 0.00002237
Iteration 15/1000 | Loss: 0.00002222
Iteration 16/1000 | Loss: 0.00002207
Iteration 17/1000 | Loss: 0.00002207
Iteration 18/1000 | Loss: 0.00002207
Iteration 19/1000 | Loss: 0.00002206
Iteration 20/1000 | Loss: 0.00002203
Iteration 21/1000 | Loss: 0.00002202
Iteration 22/1000 | Loss: 0.00002200
Iteration 23/1000 | Loss: 0.00002186
Iteration 24/1000 | Loss: 0.00002184
Iteration 25/1000 | Loss: 0.00002183
Iteration 26/1000 | Loss: 0.00002181
Iteration 27/1000 | Loss: 0.00002181
Iteration 28/1000 | Loss: 0.00002180
Iteration 29/1000 | Loss: 0.00002180
Iteration 30/1000 | Loss: 0.00002179
Iteration 31/1000 | Loss: 0.00002179
Iteration 32/1000 | Loss: 0.00002178
Iteration 33/1000 | Loss: 0.00002178
Iteration 34/1000 | Loss: 0.00002177
Iteration 35/1000 | Loss: 0.00002177
Iteration 36/1000 | Loss: 0.00002177
Iteration 37/1000 | Loss: 0.00002177
Iteration 38/1000 | Loss: 0.00002176
Iteration 39/1000 | Loss: 0.00002176
Iteration 40/1000 | Loss: 0.00002172
Iteration 41/1000 | Loss: 0.00002171
Iteration 42/1000 | Loss: 0.00002171
Iteration 43/1000 | Loss: 0.00002169
Iteration 44/1000 | Loss: 0.00002168
Iteration 45/1000 | Loss: 0.00002168
Iteration 46/1000 | Loss: 0.00002167
Iteration 47/1000 | Loss: 0.00002167
Iteration 48/1000 | Loss: 0.00002166
Iteration 49/1000 | Loss: 0.00002166
Iteration 50/1000 | Loss: 0.00002166
Iteration 51/1000 | Loss: 0.00002165
Iteration 52/1000 | Loss: 0.00002164
Iteration 53/1000 | Loss: 0.00002163
Iteration 54/1000 | Loss: 0.00002163
Iteration 55/1000 | Loss: 0.00002162
Iteration 56/1000 | Loss: 0.00002162
Iteration 57/1000 | Loss: 0.00002162
Iteration 58/1000 | Loss: 0.00002161
Iteration 59/1000 | Loss: 0.00002161
Iteration 60/1000 | Loss: 0.00002161
Iteration 61/1000 | Loss: 0.00002160
Iteration 62/1000 | Loss: 0.00002160
Iteration 63/1000 | Loss: 0.00002160
Iteration 64/1000 | Loss: 0.00002159
Iteration 65/1000 | Loss: 0.00002159
Iteration 66/1000 | Loss: 0.00002159
Iteration 67/1000 | Loss: 0.00002159
Iteration 68/1000 | Loss: 0.00002159
Iteration 69/1000 | Loss: 0.00002159
Iteration 70/1000 | Loss: 0.00002159
Iteration 71/1000 | Loss: 0.00002159
Iteration 72/1000 | Loss: 0.00002158
Iteration 73/1000 | Loss: 0.00002158
Iteration 74/1000 | Loss: 0.00002158
Iteration 75/1000 | Loss: 0.00002158
Iteration 76/1000 | Loss: 0.00002158
Iteration 77/1000 | Loss: 0.00002158
Iteration 78/1000 | Loss: 0.00002158
Iteration 79/1000 | Loss: 0.00002158
Iteration 80/1000 | Loss: 0.00002158
Iteration 81/1000 | Loss: 0.00002157
Iteration 82/1000 | Loss: 0.00002157
Iteration 83/1000 | Loss: 0.00002157
Iteration 84/1000 | Loss: 0.00002157
Iteration 85/1000 | Loss: 0.00002157
Iteration 86/1000 | Loss: 0.00002157
Iteration 87/1000 | Loss: 0.00002156
Iteration 88/1000 | Loss: 0.00002156
Iteration 89/1000 | Loss: 0.00002156
Iteration 90/1000 | Loss: 0.00002156
Iteration 91/1000 | Loss: 0.00002156
Iteration 92/1000 | Loss: 0.00002156
Iteration 93/1000 | Loss: 0.00002156
Iteration 94/1000 | Loss: 0.00002156
Iteration 95/1000 | Loss: 0.00002155
Iteration 96/1000 | Loss: 0.00002155
Iteration 97/1000 | Loss: 0.00002155
Iteration 98/1000 | Loss: 0.00002154
Iteration 99/1000 | Loss: 0.00002154
Iteration 100/1000 | Loss: 0.00002154
Iteration 101/1000 | Loss: 0.00002154
Iteration 102/1000 | Loss: 0.00002154
Iteration 103/1000 | Loss: 0.00002154
Iteration 104/1000 | Loss: 0.00002154
Iteration 105/1000 | Loss: 0.00002154
Iteration 106/1000 | Loss: 0.00002154
Iteration 107/1000 | Loss: 0.00002154
Iteration 108/1000 | Loss: 0.00002154
Iteration 109/1000 | Loss: 0.00002153
Iteration 110/1000 | Loss: 0.00002153
Iteration 111/1000 | Loss: 0.00002153
Iteration 112/1000 | Loss: 0.00002153
Iteration 113/1000 | Loss: 0.00002153
Iteration 114/1000 | Loss: 0.00002153
Iteration 115/1000 | Loss: 0.00002153
Iteration 116/1000 | Loss: 0.00002153
Iteration 117/1000 | Loss: 0.00002153
Iteration 118/1000 | Loss: 0.00002153
Iteration 119/1000 | Loss: 0.00002152
Iteration 120/1000 | Loss: 0.00002152
Iteration 121/1000 | Loss: 0.00002152
Iteration 122/1000 | Loss: 0.00002152
Iteration 123/1000 | Loss: 0.00002152
Iteration 124/1000 | Loss: 0.00002151
Iteration 125/1000 | Loss: 0.00002151
Iteration 126/1000 | Loss: 0.00002151
Iteration 127/1000 | Loss: 0.00002151
Iteration 128/1000 | Loss: 0.00002151
Iteration 129/1000 | Loss: 0.00002151
Iteration 130/1000 | Loss: 0.00002151
Iteration 131/1000 | Loss: 0.00002151
Iteration 132/1000 | Loss: 0.00002151
Iteration 133/1000 | Loss: 0.00002151
Iteration 134/1000 | Loss: 0.00002150
Iteration 135/1000 | Loss: 0.00002150
Iteration 136/1000 | Loss: 0.00002150
Iteration 137/1000 | Loss: 0.00002150
Iteration 138/1000 | Loss: 0.00002150
Iteration 139/1000 | Loss: 0.00002150
Iteration 140/1000 | Loss: 0.00002149
Iteration 141/1000 | Loss: 0.00002149
Iteration 142/1000 | Loss: 0.00002149
Iteration 143/1000 | Loss: 0.00002149
Iteration 144/1000 | Loss: 0.00002149
Iteration 145/1000 | Loss: 0.00002149
Iteration 146/1000 | Loss: 0.00002149
Iteration 147/1000 | Loss: 0.00002149
Iteration 148/1000 | Loss: 0.00002149
Iteration 149/1000 | Loss: 0.00002149
Iteration 150/1000 | Loss: 0.00002149
Iteration 151/1000 | Loss: 0.00002148
Iteration 152/1000 | Loss: 0.00002148
Iteration 153/1000 | Loss: 0.00002148
Iteration 154/1000 | Loss: 0.00002148
Iteration 155/1000 | Loss: 0.00002148
Iteration 156/1000 | Loss: 0.00002148
Iteration 157/1000 | Loss: 0.00002148
Iteration 158/1000 | Loss: 0.00002148
Iteration 159/1000 | Loss: 0.00002148
Iteration 160/1000 | Loss: 0.00002148
Iteration 161/1000 | Loss: 0.00002148
Iteration 162/1000 | Loss: 0.00002148
Iteration 163/1000 | Loss: 0.00002148
Iteration 164/1000 | Loss: 0.00002147
Iteration 165/1000 | Loss: 0.00002147
Iteration 166/1000 | Loss: 0.00002147
Iteration 167/1000 | Loss: 0.00002147
Iteration 168/1000 | Loss: 0.00002147
Iteration 169/1000 | Loss: 0.00002147
Iteration 170/1000 | Loss: 0.00002147
Iteration 171/1000 | Loss: 0.00002147
Iteration 172/1000 | Loss: 0.00002147
Iteration 173/1000 | Loss: 0.00002147
Iteration 174/1000 | Loss: 0.00002147
Iteration 175/1000 | Loss: 0.00002147
Iteration 176/1000 | Loss: 0.00002147
Iteration 177/1000 | Loss: 0.00002147
Iteration 178/1000 | Loss: 0.00002147
Iteration 179/1000 | Loss: 0.00002147
Iteration 180/1000 | Loss: 0.00002147
Iteration 181/1000 | Loss: 0.00002147
Iteration 182/1000 | Loss: 0.00002147
Iteration 183/1000 | Loss: 0.00002147
Iteration 184/1000 | Loss: 0.00002147
Iteration 185/1000 | Loss: 0.00002147
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 185. Stopping optimization.
Last 5 losses: [2.147078157577198e-05, 2.147078157577198e-05, 2.147078157577198e-05, 2.147078157577198e-05, 2.147078157577198e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.147078157577198e-05

Optimization complete. Final v2v error: 3.941411256790161 mm

Highest mean error: 4.10560417175293 mm for frame 16

Lowest mean error: 3.746349334716797 mm for frame 97

Saving results

Total time: 42.473644495010376
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_39_us_1277/0021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_39_us_1277/0021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_39_us_1277/0021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01095372
Iteration 2/25 | Loss: 0.01095372
Iteration 3/25 | Loss: 0.01095371
Iteration 4/25 | Loss: 0.00348962
Iteration 5/25 | Loss: 0.00224554
Iteration 6/25 | Loss: 0.00177319
Iteration 7/25 | Loss: 0.00188861
Iteration 8/25 | Loss: 0.00166273
Iteration 9/25 | Loss: 0.00146080
Iteration 10/25 | Loss: 0.00124102
Iteration 11/25 | Loss: 0.00121225
Iteration 12/25 | Loss: 0.00115935
Iteration 13/25 | Loss: 0.00117437
Iteration 14/25 | Loss: 0.00116470
Iteration 15/25 | Loss: 0.00112488
Iteration 16/25 | Loss: 0.00111768
Iteration 17/25 | Loss: 0.00111562
Iteration 18/25 | Loss: 0.00111517
Iteration 19/25 | Loss: 0.00115286
Iteration 20/25 | Loss: 0.00111966
Iteration 21/25 | Loss: 0.00111686
Iteration 22/25 | Loss: 0.00111625
Iteration 23/25 | Loss: 0.00111612
Iteration 24/25 | Loss: 0.00111604
Iteration 25/25 | Loss: 0.00114348

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.28404164
Iteration 2/25 | Loss: 0.00152566
Iteration 3/25 | Loss: 0.00152566
Iteration 4/25 | Loss: 0.00152566
Iteration 5/25 | Loss: 0.00152566
Iteration 6/25 | Loss: 0.00152566
Iteration 7/25 | Loss: 0.00152566
Iteration 8/25 | Loss: 0.00152566
Iteration 9/25 | Loss: 0.00152566
Iteration 10/25 | Loss: 0.00152566
Iteration 11/25 | Loss: 0.00152566
Iteration 12/25 | Loss: 0.00152566
Iteration 13/25 | Loss: 0.00152566
Iteration 14/25 | Loss: 0.00152566
Iteration 15/25 | Loss: 0.00152566
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0015256557380780578, 0.0015256557380780578, 0.0015256557380780578, 0.0015256557380780578, 0.0015256557380780578]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015256557380780578

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00152566
Iteration 2/1000 | Loss: 0.00066551
Iteration 3/1000 | Loss: 0.00074754
Iteration 4/1000 | Loss: 0.00116181
Iteration 5/1000 | Loss: 0.00073295
Iteration 6/1000 | Loss: 0.00109890
Iteration 7/1000 | Loss: 0.00103564
Iteration 8/1000 | Loss: 0.00135010
Iteration 9/1000 | Loss: 0.00051247
Iteration 10/1000 | Loss: 0.00028615
Iteration 11/1000 | Loss: 0.00009039
Iteration 12/1000 | Loss: 0.00009641
Iteration 13/1000 | Loss: 0.00046619
Iteration 14/1000 | Loss: 0.00055973
Iteration 15/1000 | Loss: 0.00007709
Iteration 16/1000 | Loss: 0.00039665
Iteration 17/1000 | Loss: 0.00006683
Iteration 18/1000 | Loss: 0.00007158
Iteration 19/1000 | Loss: 0.00068014
Iteration 20/1000 | Loss: 0.00088804
Iteration 21/1000 | Loss: 0.00054131
Iteration 22/1000 | Loss: 0.00005583
Iteration 23/1000 | Loss: 0.00088930
Iteration 24/1000 | Loss: 0.00069024
Iteration 25/1000 | Loss: 0.00006288
Iteration 26/1000 | Loss: 0.00009527
Iteration 27/1000 | Loss: 0.00006111
Iteration 28/1000 | Loss: 0.00004945
Iteration 29/1000 | Loss: 0.00004792
Iteration 30/1000 | Loss: 0.00004634
Iteration 31/1000 | Loss: 0.00004547
Iteration 32/1000 | Loss: 0.00004492
Iteration 33/1000 | Loss: 0.00351635
Iteration 34/1000 | Loss: 0.00183067
Iteration 35/1000 | Loss: 0.00096707
Iteration 36/1000 | Loss: 0.00017159
Iteration 37/1000 | Loss: 0.00008091
Iteration 38/1000 | Loss: 0.00006004
Iteration 39/1000 | Loss: 0.00004328
Iteration 40/1000 | Loss: 0.00003701
Iteration 41/1000 | Loss: 0.00003414
Iteration 42/1000 | Loss: 0.00091631
Iteration 43/1000 | Loss: 0.00003382
Iteration 44/1000 | Loss: 0.00003153
Iteration 45/1000 | Loss: 0.00035286
Iteration 46/1000 | Loss: 0.00005807
Iteration 47/1000 | Loss: 0.00006887
Iteration 48/1000 | Loss: 0.00002924
Iteration 49/1000 | Loss: 0.00002871
Iteration 50/1000 | Loss: 0.00002820
Iteration 51/1000 | Loss: 0.00020917
Iteration 52/1000 | Loss: 0.00003011
Iteration 53/1000 | Loss: 0.00002789
Iteration 54/1000 | Loss: 0.00002745
Iteration 55/1000 | Loss: 0.00002709
Iteration 56/1000 | Loss: 0.00002686
Iteration 57/1000 | Loss: 0.00002665
Iteration 58/1000 | Loss: 0.00002659
Iteration 59/1000 | Loss: 0.00002658
Iteration 60/1000 | Loss: 0.00002647
Iteration 61/1000 | Loss: 0.00002638
Iteration 62/1000 | Loss: 0.00002637
Iteration 63/1000 | Loss: 0.00002635
Iteration 64/1000 | Loss: 0.00002635
Iteration 65/1000 | Loss: 0.00002635
Iteration 66/1000 | Loss: 0.00002634
Iteration 67/1000 | Loss: 0.00002634
Iteration 68/1000 | Loss: 0.00002634
Iteration 69/1000 | Loss: 0.00002630
Iteration 70/1000 | Loss: 0.00002630
Iteration 71/1000 | Loss: 0.00002630
Iteration 72/1000 | Loss: 0.00002630
Iteration 73/1000 | Loss: 0.00002629
Iteration 74/1000 | Loss: 0.00002629
Iteration 75/1000 | Loss: 0.00002629
Iteration 76/1000 | Loss: 0.00002629
Iteration 77/1000 | Loss: 0.00002629
Iteration 78/1000 | Loss: 0.00002629
Iteration 79/1000 | Loss: 0.00002629
Iteration 80/1000 | Loss: 0.00002629
Iteration 81/1000 | Loss: 0.00002628
Iteration 82/1000 | Loss: 0.00002627
Iteration 83/1000 | Loss: 0.00002627
Iteration 84/1000 | Loss: 0.00002627
Iteration 85/1000 | Loss: 0.00002627
Iteration 86/1000 | Loss: 0.00002627
Iteration 87/1000 | Loss: 0.00002626
Iteration 88/1000 | Loss: 0.00002626
Iteration 89/1000 | Loss: 0.00002626
Iteration 90/1000 | Loss: 0.00002626
Iteration 91/1000 | Loss: 0.00002626
Iteration 92/1000 | Loss: 0.00002626
Iteration 93/1000 | Loss: 0.00002626
Iteration 94/1000 | Loss: 0.00002626
Iteration 95/1000 | Loss: 0.00002626
Iteration 96/1000 | Loss: 0.00002626
Iteration 97/1000 | Loss: 0.00002626
Iteration 98/1000 | Loss: 0.00002625
Iteration 99/1000 | Loss: 0.00002625
Iteration 100/1000 | Loss: 0.00002625
Iteration 101/1000 | Loss: 0.00002625
Iteration 102/1000 | Loss: 0.00002625
Iteration 103/1000 | Loss: 0.00002625
Iteration 104/1000 | Loss: 0.00002625
Iteration 105/1000 | Loss: 0.00002625
Iteration 106/1000 | Loss: 0.00002625
Iteration 107/1000 | Loss: 0.00002625
Iteration 108/1000 | Loss: 0.00002625
Iteration 109/1000 | Loss: 0.00002625
Iteration 110/1000 | Loss: 0.00002625
Iteration 111/1000 | Loss: 0.00002624
Iteration 112/1000 | Loss: 0.00002624
Iteration 113/1000 | Loss: 0.00002624
Iteration 114/1000 | Loss: 0.00002624
Iteration 115/1000 | Loss: 0.00002624
Iteration 116/1000 | Loss: 0.00002624
Iteration 117/1000 | Loss: 0.00002624
Iteration 118/1000 | Loss: 0.00002624
Iteration 119/1000 | Loss: 0.00002624
Iteration 120/1000 | Loss: 0.00002624
Iteration 121/1000 | Loss: 0.00002624
Iteration 122/1000 | Loss: 0.00002624
Iteration 123/1000 | Loss: 0.00002624
Iteration 124/1000 | Loss: 0.00002624
Iteration 125/1000 | Loss: 0.00002624
Iteration 126/1000 | Loss: 0.00002624
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 126. Stopping optimization.
Last 5 losses: [2.624322041810956e-05, 2.624322041810956e-05, 2.624322041810956e-05, 2.624322041810956e-05, 2.624322041810956e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.624322041810956e-05

Optimization complete. Final v2v error: 3.7312045097351074 mm

Highest mean error: 20.922273635864258 mm for frame 123

Lowest mean error: 3.1001052856445312 mm for frame 54

Saving results

Total time: 133.613422870636
