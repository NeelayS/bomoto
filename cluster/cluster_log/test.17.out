Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=17, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 952-1007
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_006/1000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_006/1000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_006/1000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01038610
Iteration 2/25 | Loss: 0.00511290
Iteration 3/25 | Loss: 0.00407575
Iteration 4/25 | Loss: 0.00401035
Iteration 5/25 | Loss: 0.00253551
Iteration 6/25 | Loss: 0.00216345
Iteration 7/25 | Loss: 0.00200533
Iteration 8/25 | Loss: 0.00187805
Iteration 9/25 | Loss: 0.00174193
Iteration 10/25 | Loss: 0.00171327
Iteration 11/25 | Loss: 0.00171221
Iteration 12/25 | Loss: 0.00169173
Iteration 13/25 | Loss: 0.00168380
Iteration 14/25 | Loss: 0.00166599
Iteration 15/25 | Loss: 0.00164558
Iteration 16/25 | Loss: 0.00162578
Iteration 17/25 | Loss: 0.00161727
Iteration 18/25 | Loss: 0.00161064
Iteration 19/25 | Loss: 0.00160420
Iteration 20/25 | Loss: 0.00160074
Iteration 21/25 | Loss: 0.00160132
Iteration 22/25 | Loss: 0.00160532
Iteration 23/25 | Loss: 0.00159782
Iteration 24/25 | Loss: 0.00159441
Iteration 25/25 | Loss: 0.00159393

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.41517818
Iteration 2/25 | Loss: 0.00863986
Iteration 3/25 | Loss: 0.00820893
Iteration 4/25 | Loss: 0.00820893
Iteration 5/25 | Loss: 0.00820893
Iteration 6/25 | Loss: 0.00820892
Iteration 7/25 | Loss: 0.00820892
Iteration 8/25 | Loss: 0.00820892
Iteration 9/25 | Loss: 0.00820892
Iteration 10/25 | Loss: 0.00820892
Iteration 11/25 | Loss: 0.00820892
Iteration 12/25 | Loss: 0.00820892
Iteration 13/25 | Loss: 0.00820892
Iteration 14/25 | Loss: 0.00820892
Iteration 15/25 | Loss: 0.00820892
Iteration 16/25 | Loss: 0.00820892
Iteration 17/25 | Loss: 0.00820892
Iteration 18/25 | Loss: 0.00820892
Iteration 19/25 | Loss: 0.00820892
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.008208923041820526, 0.008208923041820526, 0.008208923041820526, 0.008208923041820526, 0.008208923041820526]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.008208923041820526

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00820892
Iteration 2/1000 | Loss: 0.00407172
Iteration 3/1000 | Loss: 0.00349096
Iteration 4/1000 | Loss: 0.00191831
Iteration 5/1000 | Loss: 0.00126386
Iteration 6/1000 | Loss: 0.00065999
Iteration 7/1000 | Loss: 0.00051258
Iteration 8/1000 | Loss: 0.00038902
Iteration 9/1000 | Loss: 0.00031519
Iteration 10/1000 | Loss: 0.00026447
Iteration 11/1000 | Loss: 0.00022645
Iteration 12/1000 | Loss: 0.00047750
Iteration 13/1000 | Loss: 0.00045316
Iteration 14/1000 | Loss: 0.00066636
Iteration 15/1000 | Loss: 0.00420921
Iteration 16/1000 | Loss: 0.00367096
Iteration 17/1000 | Loss: 0.00229660
Iteration 18/1000 | Loss: 0.01470752
Iteration 19/1000 | Loss: 0.00588261
Iteration 20/1000 | Loss: 0.00103206
Iteration 21/1000 | Loss: 0.00118927
Iteration 22/1000 | Loss: 0.00115843
Iteration 23/1000 | Loss: 0.00059306
Iteration 24/1000 | Loss: 0.00030768
Iteration 25/1000 | Loss: 0.00053701
Iteration 26/1000 | Loss: 0.00261209
Iteration 27/1000 | Loss: 0.00084645
Iteration 28/1000 | Loss: 0.00151934
Iteration 29/1000 | Loss: 0.00077448
Iteration 30/1000 | Loss: 0.00041524
Iteration 31/1000 | Loss: 0.00073719
Iteration 32/1000 | Loss: 0.00047260
Iteration 33/1000 | Loss: 0.00064618
Iteration 34/1000 | Loss: 0.00081890
Iteration 35/1000 | Loss: 0.00098274
Iteration 36/1000 | Loss: 0.00086037
Iteration 37/1000 | Loss: 0.00071011
Iteration 38/1000 | Loss: 0.00134803
Iteration 39/1000 | Loss: 0.00092395
Iteration 40/1000 | Loss: 0.00050407
Iteration 41/1000 | Loss: 0.00081714
Iteration 42/1000 | Loss: 0.00047087
Iteration 43/1000 | Loss: 0.00054966
Iteration 44/1000 | Loss: 0.00077157
Iteration 45/1000 | Loss: 0.00046480
Iteration 46/1000 | Loss: 0.00055495
Iteration 47/1000 | Loss: 0.00047188
Iteration 48/1000 | Loss: 0.00008599
Iteration 49/1000 | Loss: 0.00023842
Iteration 50/1000 | Loss: 0.00045096
Iteration 51/1000 | Loss: 0.00060504
Iteration 52/1000 | Loss: 0.00019634
Iteration 53/1000 | Loss: 0.00027209
Iteration 54/1000 | Loss: 0.00064921
Iteration 55/1000 | Loss: 0.00047305
Iteration 56/1000 | Loss: 0.00009171
Iteration 57/1000 | Loss: 0.00007350
Iteration 58/1000 | Loss: 0.00060609
Iteration 59/1000 | Loss: 0.00048836
Iteration 60/1000 | Loss: 0.00053195
Iteration 61/1000 | Loss: 0.00026076
Iteration 62/1000 | Loss: 0.00018548
Iteration 63/1000 | Loss: 0.00006007
Iteration 64/1000 | Loss: 0.00005372
Iteration 65/1000 | Loss: 0.00011757
Iteration 66/1000 | Loss: 0.00004444
Iteration 67/1000 | Loss: 0.00004010
Iteration 68/1000 | Loss: 0.00003696
Iteration 69/1000 | Loss: 0.00034201
Iteration 70/1000 | Loss: 0.00017809
Iteration 71/1000 | Loss: 0.00015847
Iteration 72/1000 | Loss: 0.00063422
Iteration 73/1000 | Loss: 0.00043760
Iteration 74/1000 | Loss: 0.00034481
Iteration 75/1000 | Loss: 0.00003805
Iteration 76/1000 | Loss: 0.00003228
Iteration 77/1000 | Loss: 0.00009349
Iteration 78/1000 | Loss: 0.00011572
Iteration 79/1000 | Loss: 0.00022114
Iteration 80/1000 | Loss: 0.00015324
Iteration 81/1000 | Loss: 0.00002927
Iteration 82/1000 | Loss: 0.00002856
Iteration 83/1000 | Loss: 0.00022284
Iteration 84/1000 | Loss: 0.00092187
Iteration 85/1000 | Loss: 0.00041464
Iteration 86/1000 | Loss: 0.00054249
Iteration 87/1000 | Loss: 0.00004577
Iteration 88/1000 | Loss: 0.00003328
Iteration 89/1000 | Loss: 0.00003013
Iteration 90/1000 | Loss: 0.00031816
Iteration 91/1000 | Loss: 0.00013632
Iteration 92/1000 | Loss: 0.00004902
Iteration 93/1000 | Loss: 0.00003603
Iteration 94/1000 | Loss: 0.00003150
Iteration 95/1000 | Loss: 0.00062156
Iteration 96/1000 | Loss: 0.00056118
Iteration 97/1000 | Loss: 0.00021923
Iteration 98/1000 | Loss: 0.00028951
Iteration 99/1000 | Loss: 0.00005185
Iteration 100/1000 | Loss: 0.00005718
Iteration 101/1000 | Loss: 0.00003001
Iteration 102/1000 | Loss: 0.00002959
Iteration 103/1000 | Loss: 0.00002723
Iteration 104/1000 | Loss: 0.00046677
Iteration 105/1000 | Loss: 0.00033602
Iteration 106/1000 | Loss: 0.00030294
Iteration 107/1000 | Loss: 0.00004647
Iteration 108/1000 | Loss: 0.00003385
Iteration 109/1000 | Loss: 0.00002843
Iteration 110/1000 | Loss: 0.00002498
Iteration 111/1000 | Loss: 0.00002168
Iteration 112/1000 | Loss: 0.00002016
Iteration 113/1000 | Loss: 0.00001944
Iteration 114/1000 | Loss: 0.00001889
Iteration 115/1000 | Loss: 0.00001843
Iteration 116/1000 | Loss: 0.00001771
Iteration 117/1000 | Loss: 0.00001720
Iteration 118/1000 | Loss: 0.00001695
Iteration 119/1000 | Loss: 0.00001689
Iteration 120/1000 | Loss: 0.00001676
Iteration 121/1000 | Loss: 0.00001673
Iteration 122/1000 | Loss: 0.00001672
Iteration 123/1000 | Loss: 0.00001671
Iteration 124/1000 | Loss: 0.00001665
Iteration 125/1000 | Loss: 0.00001658
Iteration 126/1000 | Loss: 0.00001658
Iteration 127/1000 | Loss: 0.00001658
Iteration 128/1000 | Loss: 0.00001657
Iteration 129/1000 | Loss: 0.00001657
Iteration 130/1000 | Loss: 0.00001657
Iteration 131/1000 | Loss: 0.00001656
Iteration 132/1000 | Loss: 0.00001653
Iteration 133/1000 | Loss: 0.00001652
Iteration 134/1000 | Loss: 0.00001652
Iteration 135/1000 | Loss: 0.00001651
Iteration 136/1000 | Loss: 0.00001650
Iteration 137/1000 | Loss: 0.00001649
Iteration 138/1000 | Loss: 0.00001649
Iteration 139/1000 | Loss: 0.00001648
Iteration 140/1000 | Loss: 0.00001648
Iteration 141/1000 | Loss: 0.00001648
Iteration 142/1000 | Loss: 0.00001647
Iteration 143/1000 | Loss: 0.00001647
Iteration 144/1000 | Loss: 0.00001647
Iteration 145/1000 | Loss: 0.00001647
Iteration 146/1000 | Loss: 0.00001647
Iteration 147/1000 | Loss: 0.00001646
Iteration 148/1000 | Loss: 0.00001646
Iteration 149/1000 | Loss: 0.00001646
Iteration 150/1000 | Loss: 0.00001646
Iteration 151/1000 | Loss: 0.00001646
Iteration 152/1000 | Loss: 0.00001646
Iteration 153/1000 | Loss: 0.00001646
Iteration 154/1000 | Loss: 0.00001646
Iteration 155/1000 | Loss: 0.00001646
Iteration 156/1000 | Loss: 0.00001646
Iteration 157/1000 | Loss: 0.00001646
Iteration 158/1000 | Loss: 0.00001646
Iteration 159/1000 | Loss: 0.00001645
Iteration 160/1000 | Loss: 0.00001645
Iteration 161/1000 | Loss: 0.00001645
Iteration 162/1000 | Loss: 0.00001645
Iteration 163/1000 | Loss: 0.00001645
Iteration 164/1000 | Loss: 0.00001645
Iteration 165/1000 | Loss: 0.00001645
Iteration 166/1000 | Loss: 0.00001645
Iteration 167/1000 | Loss: 0.00001644
Iteration 168/1000 | Loss: 0.00001644
Iteration 169/1000 | Loss: 0.00001644
Iteration 170/1000 | Loss: 0.00001644
Iteration 171/1000 | Loss: 0.00001644
Iteration 172/1000 | Loss: 0.00001644
Iteration 173/1000 | Loss: 0.00001644
Iteration 174/1000 | Loss: 0.00001644
Iteration 175/1000 | Loss: 0.00001644
Iteration 176/1000 | Loss: 0.00001644
Iteration 177/1000 | Loss: 0.00001644
Iteration 178/1000 | Loss: 0.00001644
Iteration 179/1000 | Loss: 0.00001644
Iteration 180/1000 | Loss: 0.00001644
Iteration 181/1000 | Loss: 0.00001644
Iteration 182/1000 | Loss: 0.00001644
Iteration 183/1000 | Loss: 0.00001644
Iteration 184/1000 | Loss: 0.00001644
Iteration 185/1000 | Loss: 0.00001644
Iteration 186/1000 | Loss: 0.00001643
Iteration 187/1000 | Loss: 0.00001643
Iteration 188/1000 | Loss: 0.00001643
Iteration 189/1000 | Loss: 0.00001643
Iteration 190/1000 | Loss: 0.00001643
Iteration 191/1000 | Loss: 0.00001643
Iteration 192/1000 | Loss: 0.00001643
Iteration 193/1000 | Loss: 0.00001643
Iteration 194/1000 | Loss: 0.00001643
Iteration 195/1000 | Loss: 0.00001643
Iteration 196/1000 | Loss: 0.00001643
Iteration 197/1000 | Loss: 0.00001643
Iteration 198/1000 | Loss: 0.00001643
Iteration 199/1000 | Loss: 0.00001643
Iteration 200/1000 | Loss: 0.00001643
Iteration 201/1000 | Loss: 0.00001643
Iteration 202/1000 | Loss: 0.00001643
Iteration 203/1000 | Loss: 0.00001643
Iteration 204/1000 | Loss: 0.00001643
Iteration 205/1000 | Loss: 0.00001643
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 205. Stopping optimization.
Last 5 losses: [1.6427311493316665e-05, 1.6427311493316665e-05, 1.6427311493316665e-05, 1.6427311493316665e-05, 1.6427311493316665e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6427311493316665e-05

Optimization complete. Final v2v error: 3.4578933715820312 mm

Highest mean error: 4.59684944152832 mm for frame 149

Lowest mean error: 3.245435953140259 mm for frame 40

Saving results

Total time: 256.05101346969604
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_006/1014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_006/1014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_006/1014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01072401
Iteration 2/25 | Loss: 0.00180249
Iteration 3/25 | Loss: 0.00175139
Iteration 4/25 | Loss: 0.00145318
Iteration 5/25 | Loss: 0.00117460
Iteration 6/25 | Loss: 0.00095810
Iteration 7/25 | Loss: 0.00095702
Iteration 8/25 | Loss: 0.00094525
Iteration 9/25 | Loss: 0.00087439
Iteration 10/25 | Loss: 0.00082598
Iteration 11/25 | Loss: 0.00078042
Iteration 12/25 | Loss: 0.00078333
Iteration 13/25 | Loss: 0.00076993
Iteration 14/25 | Loss: 0.00076405
Iteration 15/25 | Loss: 0.00075797
Iteration 16/25 | Loss: 0.00075722
Iteration 17/25 | Loss: 0.00075651
Iteration 18/25 | Loss: 0.00075466
Iteration 19/25 | Loss: 0.00075598
Iteration 20/25 | Loss: 0.00075388
Iteration 21/25 | Loss: 0.00075190
Iteration 22/25 | Loss: 0.00075067
Iteration 23/25 | Loss: 0.00075187
Iteration 24/25 | Loss: 0.00074977
Iteration 25/25 | Loss: 0.00074574

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.52599037
Iteration 2/25 | Loss: 0.00091069
Iteration 3/25 | Loss: 0.00091069
Iteration 4/25 | Loss: 0.00091069
Iteration 5/25 | Loss: 0.00082899
Iteration 6/25 | Loss: 0.00082899
Iteration 7/25 | Loss: 0.00082899
Iteration 8/25 | Loss: 0.00082899
Iteration 9/25 | Loss: 0.00082899
Iteration 10/25 | Loss: 0.00082899
Iteration 11/25 | Loss: 0.00082899
Iteration 12/25 | Loss: 0.00082899
Iteration 13/25 | Loss: 0.00082899
Iteration 14/25 | Loss: 0.00082899
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0008289904217235744, 0.0008289904217235744, 0.0008289904217235744, 0.0008289904217235744, 0.0008289904217235744]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008289904217235744

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00082899
Iteration 2/1000 | Loss: 0.00039432
Iteration 3/1000 | Loss: 0.00031633
Iteration 4/1000 | Loss: 0.00077858
Iteration 5/1000 | Loss: 0.00038198
Iteration 6/1000 | Loss: 0.00042921
Iteration 7/1000 | Loss: 0.00047111
Iteration 8/1000 | Loss: 0.00066992
Iteration 9/1000 | Loss: 0.00083982
Iteration 10/1000 | Loss: 0.00044320
Iteration 11/1000 | Loss: 0.00042393
Iteration 12/1000 | Loss: 0.00037855
Iteration 13/1000 | Loss: 0.00005744
Iteration 14/1000 | Loss: 0.00004292
Iteration 15/1000 | Loss: 0.00003190
Iteration 16/1000 | Loss: 0.00002421
Iteration 17/1000 | Loss: 0.00002166
Iteration 18/1000 | Loss: 0.00043215
Iteration 19/1000 | Loss: 0.00035330
Iteration 20/1000 | Loss: 0.00035139
Iteration 21/1000 | Loss: 0.00033601
Iteration 22/1000 | Loss: 0.00007770
Iteration 23/1000 | Loss: 0.00022435
Iteration 24/1000 | Loss: 0.00002144
Iteration 25/1000 | Loss: 0.00006830
Iteration 26/1000 | Loss: 0.00010025
Iteration 27/1000 | Loss: 0.00010627
Iteration 28/1000 | Loss: 0.00001508
Iteration 29/1000 | Loss: 0.00001438
Iteration 30/1000 | Loss: 0.00001355
Iteration 31/1000 | Loss: 0.00001320
Iteration 32/1000 | Loss: 0.00001292
Iteration 33/1000 | Loss: 0.00024730
Iteration 34/1000 | Loss: 0.00018830
Iteration 35/1000 | Loss: 0.00009845
Iteration 36/1000 | Loss: 0.00015350
Iteration 37/1000 | Loss: 0.00030849
Iteration 38/1000 | Loss: 0.00015264
Iteration 39/1000 | Loss: 0.00020195
Iteration 40/1000 | Loss: 0.00005228
Iteration 41/1000 | Loss: 0.00030847
Iteration 42/1000 | Loss: 0.00025607
Iteration 43/1000 | Loss: 0.00003723
Iteration 44/1000 | Loss: 0.00006456
Iteration 45/1000 | Loss: 0.00004937
Iteration 46/1000 | Loss: 0.00021414
Iteration 47/1000 | Loss: 0.00021607
Iteration 48/1000 | Loss: 0.00021087
Iteration 49/1000 | Loss: 0.00016708
Iteration 50/1000 | Loss: 0.00008824
Iteration 51/1000 | Loss: 0.00011959
Iteration 52/1000 | Loss: 0.00019916
Iteration 53/1000 | Loss: 0.00004157
Iteration 54/1000 | Loss: 0.00005747
Iteration 55/1000 | Loss: 0.00016164
Iteration 56/1000 | Loss: 0.00021484
Iteration 57/1000 | Loss: 0.00036540
Iteration 58/1000 | Loss: 0.00004241
Iteration 59/1000 | Loss: 0.00002416
Iteration 60/1000 | Loss: 0.00001684
Iteration 61/1000 | Loss: 0.00001414
Iteration 62/1000 | Loss: 0.00001281
Iteration 63/1000 | Loss: 0.00001226
Iteration 64/1000 | Loss: 0.00001180
Iteration 65/1000 | Loss: 0.00001152
Iteration 66/1000 | Loss: 0.00001144
Iteration 67/1000 | Loss: 0.00001143
Iteration 68/1000 | Loss: 0.00001142
Iteration 69/1000 | Loss: 0.00001142
Iteration 70/1000 | Loss: 0.00001142
Iteration 71/1000 | Loss: 0.00001141
Iteration 72/1000 | Loss: 0.00001141
Iteration 73/1000 | Loss: 0.00001141
Iteration 74/1000 | Loss: 0.00001140
Iteration 75/1000 | Loss: 0.00001140
Iteration 76/1000 | Loss: 0.00001140
Iteration 77/1000 | Loss: 0.00001140
Iteration 78/1000 | Loss: 0.00001139
Iteration 79/1000 | Loss: 0.00001139
Iteration 80/1000 | Loss: 0.00001139
Iteration 81/1000 | Loss: 0.00001139
Iteration 82/1000 | Loss: 0.00001139
Iteration 83/1000 | Loss: 0.00001139
Iteration 84/1000 | Loss: 0.00001138
Iteration 85/1000 | Loss: 0.00001138
Iteration 86/1000 | Loss: 0.00001138
Iteration 87/1000 | Loss: 0.00001138
Iteration 88/1000 | Loss: 0.00001138
Iteration 89/1000 | Loss: 0.00001138
Iteration 90/1000 | Loss: 0.00001137
Iteration 91/1000 | Loss: 0.00001137
Iteration 92/1000 | Loss: 0.00001137
Iteration 93/1000 | Loss: 0.00001137
Iteration 94/1000 | Loss: 0.00001137
Iteration 95/1000 | Loss: 0.00001136
Iteration 96/1000 | Loss: 0.00001136
Iteration 97/1000 | Loss: 0.00001135
Iteration 98/1000 | Loss: 0.00001135
Iteration 99/1000 | Loss: 0.00001135
Iteration 100/1000 | Loss: 0.00001134
Iteration 101/1000 | Loss: 0.00001134
Iteration 102/1000 | Loss: 0.00001134
Iteration 103/1000 | Loss: 0.00001133
Iteration 104/1000 | Loss: 0.00001132
Iteration 105/1000 | Loss: 0.00001132
Iteration 106/1000 | Loss: 0.00001132
Iteration 107/1000 | Loss: 0.00001131
Iteration 108/1000 | Loss: 0.00001131
Iteration 109/1000 | Loss: 0.00001131
Iteration 110/1000 | Loss: 0.00001131
Iteration 111/1000 | Loss: 0.00001131
Iteration 112/1000 | Loss: 0.00001130
Iteration 113/1000 | Loss: 0.00001130
Iteration 114/1000 | Loss: 0.00001130
Iteration 115/1000 | Loss: 0.00001130
Iteration 116/1000 | Loss: 0.00001130
Iteration 117/1000 | Loss: 0.00001130
Iteration 118/1000 | Loss: 0.00001130
Iteration 119/1000 | Loss: 0.00001129
Iteration 120/1000 | Loss: 0.00001129
Iteration 121/1000 | Loss: 0.00001129
Iteration 122/1000 | Loss: 0.00001129
Iteration 123/1000 | Loss: 0.00001129
Iteration 124/1000 | Loss: 0.00001129
Iteration 125/1000 | Loss: 0.00001129
Iteration 126/1000 | Loss: 0.00001129
Iteration 127/1000 | Loss: 0.00001128
Iteration 128/1000 | Loss: 0.00001128
Iteration 129/1000 | Loss: 0.00001128
Iteration 130/1000 | Loss: 0.00001128
Iteration 131/1000 | Loss: 0.00001128
Iteration 132/1000 | Loss: 0.00001128
Iteration 133/1000 | Loss: 0.00001128
Iteration 134/1000 | Loss: 0.00001128
Iteration 135/1000 | Loss: 0.00001128
Iteration 136/1000 | Loss: 0.00001128
Iteration 137/1000 | Loss: 0.00001128
Iteration 138/1000 | Loss: 0.00001128
Iteration 139/1000 | Loss: 0.00001127
Iteration 140/1000 | Loss: 0.00001127
Iteration 141/1000 | Loss: 0.00001127
Iteration 142/1000 | Loss: 0.00001127
Iteration 143/1000 | Loss: 0.00001127
Iteration 144/1000 | Loss: 0.00001127
Iteration 145/1000 | Loss: 0.00001127
Iteration 146/1000 | Loss: 0.00001127
Iteration 147/1000 | Loss: 0.00001127
Iteration 148/1000 | Loss: 0.00001126
Iteration 149/1000 | Loss: 0.00001126
Iteration 150/1000 | Loss: 0.00001126
Iteration 151/1000 | Loss: 0.00001126
Iteration 152/1000 | Loss: 0.00001126
Iteration 153/1000 | Loss: 0.00001126
Iteration 154/1000 | Loss: 0.00001126
Iteration 155/1000 | Loss: 0.00001126
Iteration 156/1000 | Loss: 0.00001126
Iteration 157/1000 | Loss: 0.00001126
Iteration 158/1000 | Loss: 0.00001126
Iteration 159/1000 | Loss: 0.00001126
Iteration 160/1000 | Loss: 0.00001126
Iteration 161/1000 | Loss: 0.00001126
Iteration 162/1000 | Loss: 0.00001126
Iteration 163/1000 | Loss: 0.00001126
Iteration 164/1000 | Loss: 0.00001125
Iteration 165/1000 | Loss: 0.00001125
Iteration 166/1000 | Loss: 0.00001125
Iteration 167/1000 | Loss: 0.00001125
Iteration 168/1000 | Loss: 0.00001125
Iteration 169/1000 | Loss: 0.00001125
Iteration 170/1000 | Loss: 0.00001125
Iteration 171/1000 | Loss: 0.00001125
Iteration 172/1000 | Loss: 0.00001125
Iteration 173/1000 | Loss: 0.00001124
Iteration 174/1000 | Loss: 0.00001124
Iteration 175/1000 | Loss: 0.00001124
Iteration 176/1000 | Loss: 0.00001124
Iteration 177/1000 | Loss: 0.00001124
Iteration 178/1000 | Loss: 0.00001124
Iteration 179/1000 | Loss: 0.00001124
Iteration 180/1000 | Loss: 0.00001124
Iteration 181/1000 | Loss: 0.00001123
Iteration 182/1000 | Loss: 0.00001123
Iteration 183/1000 | Loss: 0.00001123
Iteration 184/1000 | Loss: 0.00001123
Iteration 185/1000 | Loss: 0.00001123
Iteration 186/1000 | Loss: 0.00001123
Iteration 187/1000 | Loss: 0.00001123
Iteration 188/1000 | Loss: 0.00001123
Iteration 189/1000 | Loss: 0.00001123
Iteration 190/1000 | Loss: 0.00001122
Iteration 191/1000 | Loss: 0.00001122
Iteration 192/1000 | Loss: 0.00001122
Iteration 193/1000 | Loss: 0.00001122
Iteration 194/1000 | Loss: 0.00001122
Iteration 195/1000 | Loss: 0.00001122
Iteration 196/1000 | Loss: 0.00001121
Iteration 197/1000 | Loss: 0.00001121
Iteration 198/1000 | Loss: 0.00001121
Iteration 199/1000 | Loss: 0.00001121
Iteration 200/1000 | Loss: 0.00001121
Iteration 201/1000 | Loss: 0.00001121
Iteration 202/1000 | Loss: 0.00001121
Iteration 203/1000 | Loss: 0.00001121
Iteration 204/1000 | Loss: 0.00001120
Iteration 205/1000 | Loss: 0.00001120
Iteration 206/1000 | Loss: 0.00001120
Iteration 207/1000 | Loss: 0.00001120
Iteration 208/1000 | Loss: 0.00001120
Iteration 209/1000 | Loss: 0.00001120
Iteration 210/1000 | Loss: 0.00001120
Iteration 211/1000 | Loss: 0.00001120
Iteration 212/1000 | Loss: 0.00001120
Iteration 213/1000 | Loss: 0.00001120
Iteration 214/1000 | Loss: 0.00001120
Iteration 215/1000 | Loss: 0.00001120
Iteration 216/1000 | Loss: 0.00001120
Iteration 217/1000 | Loss: 0.00001120
Iteration 218/1000 | Loss: 0.00001120
Iteration 219/1000 | Loss: 0.00001119
Iteration 220/1000 | Loss: 0.00001119
Iteration 221/1000 | Loss: 0.00001119
Iteration 222/1000 | Loss: 0.00001119
Iteration 223/1000 | Loss: 0.00001119
Iteration 224/1000 | Loss: 0.00001119
Iteration 225/1000 | Loss: 0.00001119
Iteration 226/1000 | Loss: 0.00001119
Iteration 227/1000 | Loss: 0.00001119
Iteration 228/1000 | Loss: 0.00001119
Iteration 229/1000 | Loss: 0.00001119
Iteration 230/1000 | Loss: 0.00001118
Iteration 231/1000 | Loss: 0.00001118
Iteration 232/1000 | Loss: 0.00001118
Iteration 233/1000 | Loss: 0.00001118
Iteration 234/1000 | Loss: 0.00001118
Iteration 235/1000 | Loss: 0.00001118
Iteration 236/1000 | Loss: 0.00001118
Iteration 237/1000 | Loss: 0.00001118
Iteration 238/1000 | Loss: 0.00001118
Iteration 239/1000 | Loss: 0.00001118
Iteration 240/1000 | Loss: 0.00001118
Iteration 241/1000 | Loss: 0.00001118
Iteration 242/1000 | Loss: 0.00001118
Iteration 243/1000 | Loss: 0.00001118
Iteration 244/1000 | Loss: 0.00001118
Iteration 245/1000 | Loss: 0.00001118
Iteration 246/1000 | Loss: 0.00001118
Iteration 247/1000 | Loss: 0.00001118
Iteration 248/1000 | Loss: 0.00001118
Iteration 249/1000 | Loss: 0.00001118
Iteration 250/1000 | Loss: 0.00001118
Iteration 251/1000 | Loss: 0.00001118
Iteration 252/1000 | Loss: 0.00001118
Iteration 253/1000 | Loss: 0.00001118
Iteration 254/1000 | Loss: 0.00001118
Iteration 255/1000 | Loss: 0.00001118
Iteration 256/1000 | Loss: 0.00001118
Iteration 257/1000 | Loss: 0.00001118
Iteration 258/1000 | Loss: 0.00001118
Iteration 259/1000 | Loss: 0.00001118
Iteration 260/1000 | Loss: 0.00001118
Iteration 261/1000 | Loss: 0.00001118
Iteration 262/1000 | Loss: 0.00001118
Iteration 263/1000 | Loss: 0.00001118
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 263. Stopping optimization.
Last 5 losses: [1.1181819900230039e-05, 1.1181819900230039e-05, 1.1181819900230039e-05, 1.1181819900230039e-05, 1.1181819900230039e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1181819900230039e-05

Optimization complete. Final v2v error: 2.810547351837158 mm

Highest mean error: 3.8094961643218994 mm for frame 72

Lowest mean error: 2.4412641525268555 mm for frame 93

Saving results

Total time: 149.07451486587524
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_006/1084/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_006/1084.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_006/1084
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00790851
Iteration 2/25 | Loss: 0.00134668
Iteration 3/25 | Loss: 0.00096532
Iteration 4/25 | Loss: 0.00087055
Iteration 5/25 | Loss: 0.00085841
Iteration 6/25 | Loss: 0.00085569
Iteration 7/25 | Loss: 0.00085491
Iteration 8/25 | Loss: 0.00085491
Iteration 9/25 | Loss: 0.00085491
Iteration 10/25 | Loss: 0.00085491
Iteration 11/25 | Loss: 0.00085491
Iteration 12/25 | Loss: 0.00085491
Iteration 13/25 | Loss: 0.00085491
Iteration 14/25 | Loss: 0.00085491
Iteration 15/25 | Loss: 0.00085491
Iteration 16/25 | Loss: 0.00085491
Iteration 17/25 | Loss: 0.00085491
Iteration 18/25 | Loss: 0.00085491
Iteration 19/25 | Loss: 0.00085491
Iteration 20/25 | Loss: 0.00085491
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0008549134363420308, 0.0008549134363420308, 0.0008549134363420308, 0.0008549134363420308, 0.0008549134363420308]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008549134363420308

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.54459298
Iteration 2/25 | Loss: 0.00073425
Iteration 3/25 | Loss: 0.00073425
Iteration 4/25 | Loss: 0.00073425
Iteration 5/25 | Loss: 0.00073425
Iteration 6/25 | Loss: 0.00073425
Iteration 7/25 | Loss: 0.00073425
Iteration 8/25 | Loss: 0.00073425
Iteration 9/25 | Loss: 0.00073425
Iteration 10/25 | Loss: 0.00073425
Iteration 11/25 | Loss: 0.00073425
Iteration 12/25 | Loss: 0.00073425
Iteration 13/25 | Loss: 0.00073425
Iteration 14/25 | Loss: 0.00073425
Iteration 15/25 | Loss: 0.00073425
Iteration 16/25 | Loss: 0.00073425
Iteration 17/25 | Loss: 0.00073425
Iteration 18/25 | Loss: 0.00073425
Iteration 19/25 | Loss: 0.00073425
Iteration 20/25 | Loss: 0.00073425
Iteration 21/25 | Loss: 0.00073425
Iteration 22/25 | Loss: 0.00073425
Iteration 23/25 | Loss: 0.00073425
Iteration 24/25 | Loss: 0.00073425
Iteration 25/25 | Loss: 0.00073425

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00073425
Iteration 2/1000 | Loss: 0.00004602
Iteration 3/1000 | Loss: 0.00003363
Iteration 4/1000 | Loss: 0.00002837
Iteration 5/1000 | Loss: 0.00002663
Iteration 6/1000 | Loss: 0.00002558
Iteration 7/1000 | Loss: 0.00002489
Iteration 8/1000 | Loss: 0.00002445
Iteration 9/1000 | Loss: 0.00002403
Iteration 10/1000 | Loss: 0.00002376
Iteration 11/1000 | Loss: 0.00002355
Iteration 12/1000 | Loss: 0.00002337
Iteration 13/1000 | Loss: 0.00002327
Iteration 14/1000 | Loss: 0.00002324
Iteration 15/1000 | Loss: 0.00002324
Iteration 16/1000 | Loss: 0.00002324
Iteration 17/1000 | Loss: 0.00002324
Iteration 18/1000 | Loss: 0.00002323
Iteration 19/1000 | Loss: 0.00002323
Iteration 20/1000 | Loss: 0.00002322
Iteration 21/1000 | Loss: 0.00002322
Iteration 22/1000 | Loss: 0.00002318
Iteration 23/1000 | Loss: 0.00002318
Iteration 24/1000 | Loss: 0.00002318
Iteration 25/1000 | Loss: 0.00002317
Iteration 26/1000 | Loss: 0.00002316
Iteration 27/1000 | Loss: 0.00002315
Iteration 28/1000 | Loss: 0.00002315
Iteration 29/1000 | Loss: 0.00002313
Iteration 30/1000 | Loss: 0.00002313
Iteration 31/1000 | Loss: 0.00002312
Iteration 32/1000 | Loss: 0.00002312
Iteration 33/1000 | Loss: 0.00002312
Iteration 34/1000 | Loss: 0.00002311
Iteration 35/1000 | Loss: 0.00002310
Iteration 36/1000 | Loss: 0.00002310
Iteration 37/1000 | Loss: 0.00002310
Iteration 38/1000 | Loss: 0.00002309
Iteration 39/1000 | Loss: 0.00002309
Iteration 40/1000 | Loss: 0.00002308
Iteration 41/1000 | Loss: 0.00002304
Iteration 42/1000 | Loss: 0.00002304
Iteration 43/1000 | Loss: 0.00002303
Iteration 44/1000 | Loss: 0.00002302
Iteration 45/1000 | Loss: 0.00002302
Iteration 46/1000 | Loss: 0.00002301
Iteration 47/1000 | Loss: 0.00002301
Iteration 48/1000 | Loss: 0.00002301
Iteration 49/1000 | Loss: 0.00002300
Iteration 50/1000 | Loss: 0.00002300
Iteration 51/1000 | Loss: 0.00002299
Iteration 52/1000 | Loss: 0.00002299
Iteration 53/1000 | Loss: 0.00002299
Iteration 54/1000 | Loss: 0.00002299
Iteration 55/1000 | Loss: 0.00002299
Iteration 56/1000 | Loss: 0.00002299
Iteration 57/1000 | Loss: 0.00002299
Iteration 58/1000 | Loss: 0.00002298
Iteration 59/1000 | Loss: 0.00002298
Iteration 60/1000 | Loss: 0.00002298
Iteration 61/1000 | Loss: 0.00002298
Iteration 62/1000 | Loss: 0.00002297
Iteration 63/1000 | Loss: 0.00002297
Iteration 64/1000 | Loss: 0.00002297
Iteration 65/1000 | Loss: 0.00002297
Iteration 66/1000 | Loss: 0.00002296
Iteration 67/1000 | Loss: 0.00002296
Iteration 68/1000 | Loss: 0.00002296
Iteration 69/1000 | Loss: 0.00002296
Iteration 70/1000 | Loss: 0.00002296
Iteration 71/1000 | Loss: 0.00002296
Iteration 72/1000 | Loss: 0.00002295
Iteration 73/1000 | Loss: 0.00002295
Iteration 74/1000 | Loss: 0.00002295
Iteration 75/1000 | Loss: 0.00002294
Iteration 76/1000 | Loss: 0.00002294
Iteration 77/1000 | Loss: 0.00002294
Iteration 78/1000 | Loss: 0.00002294
Iteration 79/1000 | Loss: 0.00002293
Iteration 80/1000 | Loss: 0.00002293
Iteration 81/1000 | Loss: 0.00002293
Iteration 82/1000 | Loss: 0.00002293
Iteration 83/1000 | Loss: 0.00002293
Iteration 84/1000 | Loss: 0.00002293
Iteration 85/1000 | Loss: 0.00002293
Iteration 86/1000 | Loss: 0.00002293
Iteration 87/1000 | Loss: 0.00002293
Iteration 88/1000 | Loss: 0.00002293
Iteration 89/1000 | Loss: 0.00002293
Iteration 90/1000 | Loss: 0.00002293
Iteration 91/1000 | Loss: 0.00002293
Iteration 92/1000 | Loss: 0.00002293
Iteration 93/1000 | Loss: 0.00002293
Iteration 94/1000 | Loss: 0.00002293
Iteration 95/1000 | Loss: 0.00002293
Iteration 96/1000 | Loss: 0.00002293
Iteration 97/1000 | Loss: 0.00002293
Iteration 98/1000 | Loss: 0.00002293
Iteration 99/1000 | Loss: 0.00002293
Iteration 100/1000 | Loss: 0.00002293
Iteration 101/1000 | Loss: 0.00002293
Iteration 102/1000 | Loss: 0.00002293
Iteration 103/1000 | Loss: 0.00002293
Iteration 104/1000 | Loss: 0.00002293
Iteration 105/1000 | Loss: 0.00002293
Iteration 106/1000 | Loss: 0.00002293
Iteration 107/1000 | Loss: 0.00002293
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 107. Stopping optimization.
Last 5 losses: [2.2927055397303775e-05, 2.2927055397303775e-05, 2.2927055397303775e-05, 2.2927055397303775e-05, 2.2927055397303775e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2927055397303775e-05

Optimization complete. Final v2v error: 3.944791316986084 mm

Highest mean error: 5.311094760894775 mm for frame 26

Lowest mean error: 3.37951397895813 mm for frame 180

Saving results

Total time: 37.64949059486389
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_006/1048/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_006/1048.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_006/1048
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00546703
Iteration 2/25 | Loss: 0.00106367
Iteration 3/25 | Loss: 0.00086294
Iteration 4/25 | Loss: 0.00083598
Iteration 5/25 | Loss: 0.00082907
Iteration 6/25 | Loss: 0.00083031
Iteration 7/25 | Loss: 0.00082535
Iteration 8/25 | Loss: 0.00082243
Iteration 9/25 | Loss: 0.00082136
Iteration 10/25 | Loss: 0.00082072
Iteration 11/25 | Loss: 0.00082292
Iteration 12/25 | Loss: 0.00082045
Iteration 13/25 | Loss: 0.00081890
Iteration 14/25 | Loss: 0.00081861
Iteration 15/25 | Loss: 0.00081851
Iteration 16/25 | Loss: 0.00081851
Iteration 17/25 | Loss: 0.00081851
Iteration 18/25 | Loss: 0.00081851
Iteration 19/25 | Loss: 0.00081851
Iteration 20/25 | Loss: 0.00081851
Iteration 21/25 | Loss: 0.00081850
Iteration 22/25 | Loss: 0.00081850
Iteration 23/25 | Loss: 0.00081850
Iteration 24/25 | Loss: 0.00081850
Iteration 25/25 | Loss: 0.00081850

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.53063536
Iteration 2/25 | Loss: 0.00050977
Iteration 3/25 | Loss: 0.00050975
Iteration 4/25 | Loss: 0.00050975
Iteration 5/25 | Loss: 0.00050975
Iteration 6/25 | Loss: 0.00050975
Iteration 7/25 | Loss: 0.00050975
Iteration 8/25 | Loss: 0.00050975
Iteration 9/25 | Loss: 0.00050975
Iteration 10/25 | Loss: 0.00050975
Iteration 11/25 | Loss: 0.00050975
Iteration 12/25 | Loss: 0.00050975
Iteration 13/25 | Loss: 0.00050975
Iteration 14/25 | Loss: 0.00050975
Iteration 15/25 | Loss: 0.00050975
Iteration 16/25 | Loss: 0.00050975
Iteration 17/25 | Loss: 0.00050975
Iteration 18/25 | Loss: 0.00050975
Iteration 19/25 | Loss: 0.00050975
Iteration 20/25 | Loss: 0.00050975
Iteration 21/25 | Loss: 0.00050975
Iteration 22/25 | Loss: 0.00050975
Iteration 23/25 | Loss: 0.00050975
Iteration 24/25 | Loss: 0.00050975
Iteration 25/25 | Loss: 0.00050975

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00050975
Iteration 2/1000 | Loss: 0.00004215
Iteration 3/1000 | Loss: 0.00002878
Iteration 4/1000 | Loss: 0.00002660
Iteration 5/1000 | Loss: 0.00002514
Iteration 6/1000 | Loss: 0.00002409
Iteration 7/1000 | Loss: 0.00002359
Iteration 8/1000 | Loss: 0.00002296
Iteration 9/1000 | Loss: 0.00002256
Iteration 10/1000 | Loss: 0.00002224
Iteration 11/1000 | Loss: 0.00002200
Iteration 12/1000 | Loss: 0.00002181
Iteration 13/1000 | Loss: 0.00002163
Iteration 14/1000 | Loss: 0.00002157
Iteration 15/1000 | Loss: 0.00002157
Iteration 16/1000 | Loss: 0.00002156
Iteration 17/1000 | Loss: 0.00002156
Iteration 18/1000 | Loss: 0.00002155
Iteration 19/1000 | Loss: 0.00002154
Iteration 20/1000 | Loss: 0.00002154
Iteration 21/1000 | Loss: 0.00002153
Iteration 22/1000 | Loss: 0.00002153
Iteration 23/1000 | Loss: 0.00002153
Iteration 24/1000 | Loss: 0.00002153
Iteration 25/1000 | Loss: 0.00002152
Iteration 26/1000 | Loss: 0.00002152
Iteration 27/1000 | Loss: 0.00002152
Iteration 28/1000 | Loss: 0.00002152
Iteration 29/1000 | Loss: 0.00002152
Iteration 30/1000 | Loss: 0.00002152
Iteration 31/1000 | Loss: 0.00002152
Iteration 32/1000 | Loss: 0.00002152
Iteration 33/1000 | Loss: 0.00002152
Iteration 34/1000 | Loss: 0.00002152
Iteration 35/1000 | Loss: 0.00002152
Iteration 36/1000 | Loss: 0.00002152
Iteration 37/1000 | Loss: 0.00002151
Iteration 38/1000 | Loss: 0.00002151
Iteration 39/1000 | Loss: 0.00002151
Iteration 40/1000 | Loss: 0.00002151
Iteration 41/1000 | Loss: 0.00002151
Iteration 42/1000 | Loss: 0.00002151
Iteration 43/1000 | Loss: 0.00002151
Iteration 44/1000 | Loss: 0.00002151
Iteration 45/1000 | Loss: 0.00002151
Iteration 46/1000 | Loss: 0.00002151
Iteration 47/1000 | Loss: 0.00002151
Iteration 48/1000 | Loss: 0.00002150
Iteration 49/1000 | Loss: 0.00002150
Iteration 50/1000 | Loss: 0.00002150
Iteration 51/1000 | Loss: 0.00002150
Iteration 52/1000 | Loss: 0.00002149
Iteration 53/1000 | Loss: 0.00002149
Iteration 54/1000 | Loss: 0.00002149
Iteration 55/1000 | Loss: 0.00002149
Iteration 56/1000 | Loss: 0.00002149
Iteration 57/1000 | Loss: 0.00002149
Iteration 58/1000 | Loss: 0.00002149
Iteration 59/1000 | Loss: 0.00002149
Iteration 60/1000 | Loss: 0.00002149
Iteration 61/1000 | Loss: 0.00002149
Iteration 62/1000 | Loss: 0.00002148
Iteration 63/1000 | Loss: 0.00002148
Iteration 64/1000 | Loss: 0.00002148
Iteration 65/1000 | Loss: 0.00002148
Iteration 66/1000 | Loss: 0.00002148
Iteration 67/1000 | Loss: 0.00002148
Iteration 68/1000 | Loss: 0.00002148
Iteration 69/1000 | Loss: 0.00002148
Iteration 70/1000 | Loss: 0.00002148
Iteration 71/1000 | Loss: 0.00002148
Iteration 72/1000 | Loss: 0.00002148
Iteration 73/1000 | Loss: 0.00002147
Iteration 74/1000 | Loss: 0.00002147
Iteration 75/1000 | Loss: 0.00002147
Iteration 76/1000 | Loss: 0.00002147
Iteration 77/1000 | Loss: 0.00002147
Iteration 78/1000 | Loss: 0.00002147
Iteration 79/1000 | Loss: 0.00002147
Iteration 80/1000 | Loss: 0.00002147
Iteration 81/1000 | Loss: 0.00002147
Iteration 82/1000 | Loss: 0.00002147
Iteration 83/1000 | Loss: 0.00002147
Iteration 84/1000 | Loss: 0.00002146
Iteration 85/1000 | Loss: 0.00002146
Iteration 86/1000 | Loss: 0.00002146
Iteration 87/1000 | Loss: 0.00002146
Iteration 88/1000 | Loss: 0.00002146
Iteration 89/1000 | Loss: 0.00002146
Iteration 90/1000 | Loss: 0.00002145
Iteration 91/1000 | Loss: 0.00002145
Iteration 92/1000 | Loss: 0.00002145
Iteration 93/1000 | Loss: 0.00002144
Iteration 94/1000 | Loss: 0.00002143
Iteration 95/1000 | Loss: 0.00002143
Iteration 96/1000 | Loss: 0.00002143
Iteration 97/1000 | Loss: 0.00002143
Iteration 98/1000 | Loss: 0.00002142
Iteration 99/1000 | Loss: 0.00002142
Iteration 100/1000 | Loss: 0.00002142
Iteration 101/1000 | Loss: 0.00002141
Iteration 102/1000 | Loss: 0.00002141
Iteration 103/1000 | Loss: 0.00002141
Iteration 104/1000 | Loss: 0.00002141
Iteration 105/1000 | Loss: 0.00002141
Iteration 106/1000 | Loss: 0.00002141
Iteration 107/1000 | Loss: 0.00002140
Iteration 108/1000 | Loss: 0.00002139
Iteration 109/1000 | Loss: 0.00002139
Iteration 110/1000 | Loss: 0.00002139
Iteration 111/1000 | Loss: 0.00002139
Iteration 112/1000 | Loss: 0.00002139
Iteration 113/1000 | Loss: 0.00002139
Iteration 114/1000 | Loss: 0.00002139
Iteration 115/1000 | Loss: 0.00002139
Iteration 116/1000 | Loss: 0.00002138
Iteration 117/1000 | Loss: 0.00002138
Iteration 118/1000 | Loss: 0.00002137
Iteration 119/1000 | Loss: 0.00002137
Iteration 120/1000 | Loss: 0.00002137
Iteration 121/1000 | Loss: 0.00002136
Iteration 122/1000 | Loss: 0.00002136
Iteration 123/1000 | Loss: 0.00002136
Iteration 124/1000 | Loss: 0.00002135
Iteration 125/1000 | Loss: 0.00002135
Iteration 126/1000 | Loss: 0.00002135
Iteration 127/1000 | Loss: 0.00002135
Iteration 128/1000 | Loss: 0.00002134
Iteration 129/1000 | Loss: 0.00002134
Iteration 130/1000 | Loss: 0.00002134
Iteration 131/1000 | Loss: 0.00002134
Iteration 132/1000 | Loss: 0.00002133
Iteration 133/1000 | Loss: 0.00002133
Iteration 134/1000 | Loss: 0.00002133
Iteration 135/1000 | Loss: 0.00002133
Iteration 136/1000 | Loss: 0.00002132
Iteration 137/1000 | Loss: 0.00002132
Iteration 138/1000 | Loss: 0.00002132
Iteration 139/1000 | Loss: 0.00002132
Iteration 140/1000 | Loss: 0.00002132
Iteration 141/1000 | Loss: 0.00002132
Iteration 142/1000 | Loss: 0.00002131
Iteration 143/1000 | Loss: 0.00002131
Iteration 144/1000 | Loss: 0.00002131
Iteration 145/1000 | Loss: 0.00002131
Iteration 146/1000 | Loss: 0.00002131
Iteration 147/1000 | Loss: 0.00002131
Iteration 148/1000 | Loss: 0.00002131
Iteration 149/1000 | Loss: 0.00002131
Iteration 150/1000 | Loss: 0.00002131
Iteration 151/1000 | Loss: 0.00002131
Iteration 152/1000 | Loss: 0.00002131
Iteration 153/1000 | Loss: 0.00002131
Iteration 154/1000 | Loss: 0.00002131
Iteration 155/1000 | Loss: 0.00002131
Iteration 156/1000 | Loss: 0.00002131
Iteration 157/1000 | Loss: 0.00002130
Iteration 158/1000 | Loss: 0.00002130
Iteration 159/1000 | Loss: 0.00002130
Iteration 160/1000 | Loss: 0.00002130
Iteration 161/1000 | Loss: 0.00002129
Iteration 162/1000 | Loss: 0.00002129
Iteration 163/1000 | Loss: 0.00002129
Iteration 164/1000 | Loss: 0.00002129
Iteration 165/1000 | Loss: 0.00002129
Iteration 166/1000 | Loss: 0.00002128
Iteration 167/1000 | Loss: 0.00002128
Iteration 168/1000 | Loss: 0.00002128
Iteration 169/1000 | Loss: 0.00002128
Iteration 170/1000 | Loss: 0.00002128
Iteration 171/1000 | Loss: 0.00002128
Iteration 172/1000 | Loss: 0.00002128
Iteration 173/1000 | Loss: 0.00002128
Iteration 174/1000 | Loss: 0.00002128
Iteration 175/1000 | Loss: 0.00002127
Iteration 176/1000 | Loss: 0.00002127
Iteration 177/1000 | Loss: 0.00002127
Iteration 178/1000 | Loss: 0.00002127
Iteration 179/1000 | Loss: 0.00002127
Iteration 180/1000 | Loss: 0.00002127
Iteration 181/1000 | Loss: 0.00002127
Iteration 182/1000 | Loss: 0.00002126
Iteration 183/1000 | Loss: 0.00002126
Iteration 184/1000 | Loss: 0.00002126
Iteration 185/1000 | Loss: 0.00002126
Iteration 186/1000 | Loss: 0.00002126
Iteration 187/1000 | Loss: 0.00002126
Iteration 188/1000 | Loss: 0.00002126
Iteration 189/1000 | Loss: 0.00002126
Iteration 190/1000 | Loss: 0.00002126
Iteration 191/1000 | Loss: 0.00002126
Iteration 192/1000 | Loss: 0.00002126
Iteration 193/1000 | Loss: 0.00002125
Iteration 194/1000 | Loss: 0.00002125
Iteration 195/1000 | Loss: 0.00002125
Iteration 196/1000 | Loss: 0.00002125
Iteration 197/1000 | Loss: 0.00002125
Iteration 198/1000 | Loss: 0.00002125
Iteration 199/1000 | Loss: 0.00002125
Iteration 200/1000 | Loss: 0.00002125
Iteration 201/1000 | Loss: 0.00002124
Iteration 202/1000 | Loss: 0.00002124
Iteration 203/1000 | Loss: 0.00002124
Iteration 204/1000 | Loss: 0.00002124
Iteration 205/1000 | Loss: 0.00002124
Iteration 206/1000 | Loss: 0.00002124
Iteration 207/1000 | Loss: 0.00002124
Iteration 208/1000 | Loss: 0.00002124
Iteration 209/1000 | Loss: 0.00002124
Iteration 210/1000 | Loss: 0.00002124
Iteration 211/1000 | Loss: 0.00002123
Iteration 212/1000 | Loss: 0.00002123
Iteration 213/1000 | Loss: 0.00002123
Iteration 214/1000 | Loss: 0.00002123
Iteration 215/1000 | Loss: 0.00002123
Iteration 216/1000 | Loss: 0.00002123
Iteration 217/1000 | Loss: 0.00002123
Iteration 218/1000 | Loss: 0.00002123
Iteration 219/1000 | Loss: 0.00002123
Iteration 220/1000 | Loss: 0.00002123
Iteration 221/1000 | Loss: 0.00002123
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 221. Stopping optimization.
Last 5 losses: [2.123423109878786e-05, 2.123423109878786e-05, 2.123423109878786e-05, 2.123423109878786e-05, 2.123423109878786e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.123423109878786e-05

Optimization complete. Final v2v error: 3.786210775375366 mm

Highest mean error: 4.582457542419434 mm for frame 71

Lowest mean error: 3.0577902793884277 mm for frame 212

Saving results

Total time: 62.106722593307495
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_006/1016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_006/1016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_006/1016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00997924
Iteration 2/25 | Loss: 0.00213031
Iteration 3/25 | Loss: 0.00132077
Iteration 4/25 | Loss: 0.00112069
Iteration 5/25 | Loss: 0.00105849
Iteration 6/25 | Loss: 0.00103783
Iteration 7/25 | Loss: 0.00093930
Iteration 8/25 | Loss: 0.00089354
Iteration 9/25 | Loss: 0.00086647
Iteration 10/25 | Loss: 0.00084705
Iteration 11/25 | Loss: 0.00081453
Iteration 12/25 | Loss: 0.00080951
Iteration 13/25 | Loss: 0.00080205
Iteration 14/25 | Loss: 0.00079387
Iteration 15/25 | Loss: 0.00079070
Iteration 16/25 | Loss: 0.00078966
Iteration 17/25 | Loss: 0.00078888
Iteration 18/25 | Loss: 0.00078808
Iteration 19/25 | Loss: 0.00079409
Iteration 20/25 | Loss: 0.00079155
Iteration 21/25 | Loss: 0.00078315
Iteration 22/25 | Loss: 0.00078135
Iteration 23/25 | Loss: 0.00078096
Iteration 24/25 | Loss: 0.00078077
Iteration 25/25 | Loss: 0.00078065

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.63703465
Iteration 2/25 | Loss: 0.00084396
Iteration 3/25 | Loss: 0.00084396
Iteration 4/25 | Loss: 0.00066887
Iteration 5/25 | Loss: 0.00066887
Iteration 6/25 | Loss: 0.00066887
Iteration 7/25 | Loss: 0.00066887
Iteration 8/25 | Loss: 0.00066887
Iteration 9/25 | Loss: 0.00066887
Iteration 10/25 | Loss: 0.00066887
Iteration 11/25 | Loss: 0.00066887
Iteration 12/25 | Loss: 0.00066887
Iteration 13/25 | Loss: 0.00066887
Iteration 14/25 | Loss: 0.00066887
Iteration 15/25 | Loss: 0.00066887
Iteration 16/25 | Loss: 0.00066887
Iteration 17/25 | Loss: 0.00066887
Iteration 18/25 | Loss: 0.00066887
Iteration 19/25 | Loss: 0.00066887
Iteration 20/25 | Loss: 0.00066887
Iteration 21/25 | Loss: 0.00066887
Iteration 22/25 | Loss: 0.00066887
Iteration 23/25 | Loss: 0.00066887
Iteration 24/25 | Loss: 0.00066887
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0006688712746836245, 0.0006688712746836245, 0.0006688712746836245, 0.0006688712746836245, 0.0006688712746836245]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006688712746836245

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00066887
Iteration 2/1000 | Loss: 0.00037807
Iteration 3/1000 | Loss: 0.00007819
Iteration 4/1000 | Loss: 0.00023172
Iteration 5/1000 | Loss: 0.00018179
Iteration 6/1000 | Loss: 0.00002897
Iteration 7/1000 | Loss: 0.00025587
Iteration 8/1000 | Loss: 0.00002225
Iteration 9/1000 | Loss: 0.00010240
Iteration 10/1000 | Loss: 0.00015499
Iteration 11/1000 | Loss: 0.00034938
Iteration 12/1000 | Loss: 0.00001996
Iteration 13/1000 | Loss: 0.00001931
Iteration 14/1000 | Loss: 0.00001904
Iteration 15/1000 | Loss: 0.00001872
Iteration 16/1000 | Loss: 0.00001847
Iteration 17/1000 | Loss: 0.00001822
Iteration 18/1000 | Loss: 0.00001795
Iteration 19/1000 | Loss: 0.00001772
Iteration 20/1000 | Loss: 0.00001770
Iteration 21/1000 | Loss: 0.00001767
Iteration 22/1000 | Loss: 0.00007197
Iteration 23/1000 | Loss: 0.00004590
Iteration 24/1000 | Loss: 0.00002223
Iteration 25/1000 | Loss: 0.00014745
Iteration 26/1000 | Loss: 0.00005284
Iteration 27/1000 | Loss: 0.00007925
Iteration 28/1000 | Loss: 0.00004824
Iteration 29/1000 | Loss: 0.00004024
Iteration 30/1000 | Loss: 0.00001803
Iteration 31/1000 | Loss: 0.00006421
Iteration 32/1000 | Loss: 0.00010626
Iteration 33/1000 | Loss: 0.00002592
Iteration 34/1000 | Loss: 0.00001786
Iteration 35/1000 | Loss: 0.00006508
Iteration 36/1000 | Loss: 0.00002934
Iteration 37/1000 | Loss: 0.00006182
Iteration 38/1000 | Loss: 0.00003557
Iteration 39/1000 | Loss: 0.00005462
Iteration 40/1000 | Loss: 0.00002998
Iteration 41/1000 | Loss: 0.00002159
Iteration 42/1000 | Loss: 0.00001828
Iteration 43/1000 | Loss: 0.00001645
Iteration 44/1000 | Loss: 0.00001553
Iteration 45/1000 | Loss: 0.00001502
Iteration 46/1000 | Loss: 0.00001478
Iteration 47/1000 | Loss: 0.00001454
Iteration 48/1000 | Loss: 0.00001443
Iteration 49/1000 | Loss: 0.00001442
Iteration 50/1000 | Loss: 0.00001442
Iteration 51/1000 | Loss: 0.00001440
Iteration 52/1000 | Loss: 0.00001438
Iteration 53/1000 | Loss: 0.00001438
Iteration 54/1000 | Loss: 0.00001438
Iteration 55/1000 | Loss: 0.00001438
Iteration 56/1000 | Loss: 0.00001438
Iteration 57/1000 | Loss: 0.00001438
Iteration 58/1000 | Loss: 0.00001438
Iteration 59/1000 | Loss: 0.00001438
Iteration 60/1000 | Loss: 0.00001438
Iteration 61/1000 | Loss: 0.00001437
Iteration 62/1000 | Loss: 0.00001437
Iteration 63/1000 | Loss: 0.00001437
Iteration 64/1000 | Loss: 0.00001436
Iteration 65/1000 | Loss: 0.00001436
Iteration 66/1000 | Loss: 0.00001435
Iteration 67/1000 | Loss: 0.00001435
Iteration 68/1000 | Loss: 0.00001435
Iteration 69/1000 | Loss: 0.00001435
Iteration 70/1000 | Loss: 0.00001433
Iteration 71/1000 | Loss: 0.00001433
Iteration 72/1000 | Loss: 0.00001432
Iteration 73/1000 | Loss: 0.00001432
Iteration 74/1000 | Loss: 0.00001430
Iteration 75/1000 | Loss: 0.00001428
Iteration 76/1000 | Loss: 0.00001428
Iteration 77/1000 | Loss: 0.00001428
Iteration 78/1000 | Loss: 0.00001427
Iteration 79/1000 | Loss: 0.00001427
Iteration 80/1000 | Loss: 0.00001426
Iteration 81/1000 | Loss: 0.00001426
Iteration 82/1000 | Loss: 0.00001425
Iteration 83/1000 | Loss: 0.00001425
Iteration 84/1000 | Loss: 0.00001424
Iteration 85/1000 | Loss: 0.00001424
Iteration 86/1000 | Loss: 0.00001424
Iteration 87/1000 | Loss: 0.00001424
Iteration 88/1000 | Loss: 0.00001424
Iteration 89/1000 | Loss: 0.00001424
Iteration 90/1000 | Loss: 0.00001424
Iteration 91/1000 | Loss: 0.00001424
Iteration 92/1000 | Loss: 0.00001424
Iteration 93/1000 | Loss: 0.00001423
Iteration 94/1000 | Loss: 0.00001423
Iteration 95/1000 | Loss: 0.00001423
Iteration 96/1000 | Loss: 0.00001423
Iteration 97/1000 | Loss: 0.00001423
Iteration 98/1000 | Loss: 0.00001423
Iteration 99/1000 | Loss: 0.00001423
Iteration 100/1000 | Loss: 0.00001423
Iteration 101/1000 | Loss: 0.00001422
Iteration 102/1000 | Loss: 0.00001422
Iteration 103/1000 | Loss: 0.00001422
Iteration 104/1000 | Loss: 0.00001422
Iteration 105/1000 | Loss: 0.00001422
Iteration 106/1000 | Loss: 0.00001422
Iteration 107/1000 | Loss: 0.00001422
Iteration 108/1000 | Loss: 0.00001421
Iteration 109/1000 | Loss: 0.00001421
Iteration 110/1000 | Loss: 0.00001421
Iteration 111/1000 | Loss: 0.00001421
Iteration 112/1000 | Loss: 0.00001421
Iteration 113/1000 | Loss: 0.00001420
Iteration 114/1000 | Loss: 0.00001420
Iteration 115/1000 | Loss: 0.00001420
Iteration 116/1000 | Loss: 0.00001420
Iteration 117/1000 | Loss: 0.00001420
Iteration 118/1000 | Loss: 0.00001420
Iteration 119/1000 | Loss: 0.00001420
Iteration 120/1000 | Loss: 0.00001419
Iteration 121/1000 | Loss: 0.00001419
Iteration 122/1000 | Loss: 0.00001419
Iteration 123/1000 | Loss: 0.00001419
Iteration 124/1000 | Loss: 0.00001419
Iteration 125/1000 | Loss: 0.00001419
Iteration 126/1000 | Loss: 0.00001419
Iteration 127/1000 | Loss: 0.00001419
Iteration 128/1000 | Loss: 0.00001419
Iteration 129/1000 | Loss: 0.00001419
Iteration 130/1000 | Loss: 0.00001419
Iteration 131/1000 | Loss: 0.00001419
Iteration 132/1000 | Loss: 0.00001419
Iteration 133/1000 | Loss: 0.00001419
Iteration 134/1000 | Loss: 0.00001419
Iteration 135/1000 | Loss: 0.00001419
Iteration 136/1000 | Loss: 0.00001418
Iteration 137/1000 | Loss: 0.00001418
Iteration 138/1000 | Loss: 0.00001418
Iteration 139/1000 | Loss: 0.00001418
Iteration 140/1000 | Loss: 0.00001418
Iteration 141/1000 | Loss: 0.00001418
Iteration 142/1000 | Loss: 0.00001418
Iteration 143/1000 | Loss: 0.00001418
Iteration 144/1000 | Loss: 0.00001418
Iteration 145/1000 | Loss: 0.00001418
Iteration 146/1000 | Loss: 0.00001418
Iteration 147/1000 | Loss: 0.00001418
Iteration 148/1000 | Loss: 0.00001417
Iteration 149/1000 | Loss: 0.00001417
Iteration 150/1000 | Loss: 0.00001417
Iteration 151/1000 | Loss: 0.00001417
Iteration 152/1000 | Loss: 0.00001417
Iteration 153/1000 | Loss: 0.00001417
Iteration 154/1000 | Loss: 0.00001417
Iteration 155/1000 | Loss: 0.00001417
Iteration 156/1000 | Loss: 0.00001417
Iteration 157/1000 | Loss: 0.00001417
Iteration 158/1000 | Loss: 0.00001417
Iteration 159/1000 | Loss: 0.00001417
Iteration 160/1000 | Loss: 0.00001417
Iteration 161/1000 | Loss: 0.00001417
Iteration 162/1000 | Loss: 0.00001417
Iteration 163/1000 | Loss: 0.00001417
Iteration 164/1000 | Loss: 0.00001417
Iteration 165/1000 | Loss: 0.00001417
Iteration 166/1000 | Loss: 0.00001417
Iteration 167/1000 | Loss: 0.00001417
Iteration 168/1000 | Loss: 0.00001416
Iteration 169/1000 | Loss: 0.00001416
Iteration 170/1000 | Loss: 0.00001416
Iteration 171/1000 | Loss: 0.00001416
Iteration 172/1000 | Loss: 0.00001416
Iteration 173/1000 | Loss: 0.00001416
Iteration 174/1000 | Loss: 0.00001416
Iteration 175/1000 | Loss: 0.00001416
Iteration 176/1000 | Loss: 0.00001416
Iteration 177/1000 | Loss: 0.00001416
Iteration 178/1000 | Loss: 0.00001416
Iteration 179/1000 | Loss: 0.00001416
Iteration 180/1000 | Loss: 0.00001416
Iteration 181/1000 | Loss: 0.00001416
Iteration 182/1000 | Loss: 0.00001416
Iteration 183/1000 | Loss: 0.00001416
Iteration 184/1000 | Loss: 0.00001416
Iteration 185/1000 | Loss: 0.00001416
Iteration 186/1000 | Loss: 0.00001416
Iteration 187/1000 | Loss: 0.00001416
Iteration 188/1000 | Loss: 0.00001416
Iteration 189/1000 | Loss: 0.00001416
Iteration 190/1000 | Loss: 0.00001416
Iteration 191/1000 | Loss: 0.00001415
Iteration 192/1000 | Loss: 0.00001415
Iteration 193/1000 | Loss: 0.00001415
Iteration 194/1000 | Loss: 0.00001415
Iteration 195/1000 | Loss: 0.00001415
Iteration 196/1000 | Loss: 0.00001415
Iteration 197/1000 | Loss: 0.00001415
Iteration 198/1000 | Loss: 0.00001415
Iteration 199/1000 | Loss: 0.00001415
Iteration 200/1000 | Loss: 0.00001415
Iteration 201/1000 | Loss: 0.00001415
Iteration 202/1000 | Loss: 0.00001415
Iteration 203/1000 | Loss: 0.00001415
Iteration 204/1000 | Loss: 0.00001415
Iteration 205/1000 | Loss: 0.00001415
Iteration 206/1000 | Loss: 0.00001415
Iteration 207/1000 | Loss: 0.00001415
Iteration 208/1000 | Loss: 0.00001415
Iteration 209/1000 | Loss: 0.00001415
Iteration 210/1000 | Loss: 0.00001415
Iteration 211/1000 | Loss: 0.00001415
Iteration 212/1000 | Loss: 0.00001415
Iteration 213/1000 | Loss: 0.00001415
Iteration 214/1000 | Loss: 0.00001415
Iteration 215/1000 | Loss: 0.00001415
Iteration 216/1000 | Loss: 0.00001415
Iteration 217/1000 | Loss: 0.00001415
Iteration 218/1000 | Loss: 0.00001415
Iteration 219/1000 | Loss: 0.00001415
Iteration 220/1000 | Loss: 0.00001415
Iteration 221/1000 | Loss: 0.00001415
Iteration 222/1000 | Loss: 0.00001415
Iteration 223/1000 | Loss: 0.00001415
Iteration 224/1000 | Loss: 0.00001415
Iteration 225/1000 | Loss: 0.00001415
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 225. Stopping optimization.
Last 5 losses: [1.4152928088151384e-05, 1.4152928088151384e-05, 1.4152928088151384e-05, 1.4152928088151384e-05, 1.4152928088151384e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4152928088151384e-05

Optimization complete. Final v2v error: 3.1011438369750977 mm

Highest mean error: 3.859745502471924 mm for frame 26

Lowest mean error: 2.582534074783325 mm for frame 123

Saving results

Total time: 117.79324007034302
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_006/1073/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_006/1073.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_006/1073
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01154401
Iteration 2/25 | Loss: 0.01154401
Iteration 3/25 | Loss: 0.01154401
Iteration 4/25 | Loss: 0.00231282
Iteration 5/25 | Loss: 0.00105634
Iteration 6/25 | Loss: 0.00093587
Iteration 7/25 | Loss: 0.00093408
Iteration 8/25 | Loss: 0.00096155
Iteration 9/25 | Loss: 0.00095134
Iteration 10/25 | Loss: 0.00091896
Iteration 11/25 | Loss: 0.00091054
Iteration 12/25 | Loss: 0.00090636
Iteration 13/25 | Loss: 0.00092691
Iteration 14/25 | Loss: 0.00090466
Iteration 15/25 | Loss: 0.00089893
Iteration 16/25 | Loss: 0.00089088
Iteration 17/25 | Loss: 0.00088808
Iteration 18/25 | Loss: 0.00088796
Iteration 19/25 | Loss: 0.00088796
Iteration 20/25 | Loss: 0.00088796
Iteration 21/25 | Loss: 0.00088796
Iteration 22/25 | Loss: 0.00088796
Iteration 23/25 | Loss: 0.00088796
Iteration 24/25 | Loss: 0.00088795
Iteration 25/25 | Loss: 0.00088795

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.53955352
Iteration 2/25 | Loss: 0.00057467
Iteration 3/25 | Loss: 0.00057467
Iteration 4/25 | Loss: 0.00057467
Iteration 5/25 | Loss: 0.00057467
Iteration 6/25 | Loss: 0.00057467
Iteration 7/25 | Loss: 0.00057467
Iteration 8/25 | Loss: 0.00057467
Iteration 9/25 | Loss: 0.00057467
Iteration 10/25 | Loss: 0.00057467
Iteration 11/25 | Loss: 0.00057467
Iteration 12/25 | Loss: 0.00057467
Iteration 13/25 | Loss: 0.00057467
Iteration 14/25 | Loss: 0.00057467
Iteration 15/25 | Loss: 0.00057467
Iteration 16/25 | Loss: 0.00057467
Iteration 17/25 | Loss: 0.00057467
Iteration 18/25 | Loss: 0.00057467
Iteration 19/25 | Loss: 0.00057467
Iteration 20/25 | Loss: 0.00057467
Iteration 21/25 | Loss: 0.00057467
Iteration 22/25 | Loss: 0.00057467
Iteration 23/25 | Loss: 0.00057467
Iteration 24/25 | Loss: 0.00057467
Iteration 25/25 | Loss: 0.00057467

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00057467
Iteration 2/1000 | Loss: 0.00003927
Iteration 3/1000 | Loss: 0.00002613
Iteration 4/1000 | Loss: 0.00002406
Iteration 5/1000 | Loss: 0.00002266
Iteration 6/1000 | Loss: 0.00002208
Iteration 7/1000 | Loss: 0.00002161
Iteration 8/1000 | Loss: 0.00002123
Iteration 9/1000 | Loss: 0.00002101
Iteration 10/1000 | Loss: 0.00002096
Iteration 11/1000 | Loss: 0.00002091
Iteration 12/1000 | Loss: 0.00002089
Iteration 13/1000 | Loss: 0.00002076
Iteration 14/1000 | Loss: 0.00002075
Iteration 15/1000 | Loss: 0.00002075
Iteration 16/1000 | Loss: 0.00002065
Iteration 17/1000 | Loss: 0.00002064
Iteration 18/1000 | Loss: 0.00002062
Iteration 19/1000 | Loss: 0.00002061
Iteration 20/1000 | Loss: 0.00002061
Iteration 21/1000 | Loss: 0.00002060
Iteration 22/1000 | Loss: 0.00002058
Iteration 23/1000 | Loss: 0.00002057
Iteration 24/1000 | Loss: 0.00002057
Iteration 25/1000 | Loss: 0.00002056
Iteration 26/1000 | Loss: 0.00002056
Iteration 27/1000 | Loss: 0.00002055
Iteration 28/1000 | Loss: 0.00002055
Iteration 29/1000 | Loss: 0.00002054
Iteration 30/1000 | Loss: 0.00002054
Iteration 31/1000 | Loss: 0.00002053
Iteration 32/1000 | Loss: 0.00002052
Iteration 33/1000 | Loss: 0.00002052
Iteration 34/1000 | Loss: 0.00002051
Iteration 35/1000 | Loss: 0.00002051
Iteration 36/1000 | Loss: 0.00002051
Iteration 37/1000 | Loss: 0.00002050
Iteration 38/1000 | Loss: 0.00002050
Iteration 39/1000 | Loss: 0.00002050
Iteration 40/1000 | Loss: 0.00002050
Iteration 41/1000 | Loss: 0.00002050
Iteration 42/1000 | Loss: 0.00002049
Iteration 43/1000 | Loss: 0.00002049
Iteration 44/1000 | Loss: 0.00002049
Iteration 45/1000 | Loss: 0.00002049
Iteration 46/1000 | Loss: 0.00002049
Iteration 47/1000 | Loss: 0.00002048
Iteration 48/1000 | Loss: 0.00002047
Iteration 49/1000 | Loss: 0.00002047
Iteration 50/1000 | Loss: 0.00002047
Iteration 51/1000 | Loss: 0.00002047
Iteration 52/1000 | Loss: 0.00002047
Iteration 53/1000 | Loss: 0.00002047
Iteration 54/1000 | Loss: 0.00002047
Iteration 55/1000 | Loss: 0.00002047
Iteration 56/1000 | Loss: 0.00002047
Iteration 57/1000 | Loss: 0.00002046
Iteration 58/1000 | Loss: 0.00002046
Iteration 59/1000 | Loss: 0.00002046
Iteration 60/1000 | Loss: 0.00002046
Iteration 61/1000 | Loss: 0.00002046
Iteration 62/1000 | Loss: 0.00002046
Iteration 63/1000 | Loss: 0.00002046
Iteration 64/1000 | Loss: 0.00002045
Iteration 65/1000 | Loss: 0.00002045
Iteration 66/1000 | Loss: 0.00002045
Iteration 67/1000 | Loss: 0.00002045
Iteration 68/1000 | Loss: 0.00002045
Iteration 69/1000 | Loss: 0.00002045
Iteration 70/1000 | Loss: 0.00002045
Iteration 71/1000 | Loss: 0.00002044
Iteration 72/1000 | Loss: 0.00002044
Iteration 73/1000 | Loss: 0.00002044
Iteration 74/1000 | Loss: 0.00002044
Iteration 75/1000 | Loss: 0.00002044
Iteration 76/1000 | Loss: 0.00002044
Iteration 77/1000 | Loss: 0.00002044
Iteration 78/1000 | Loss: 0.00002043
Iteration 79/1000 | Loss: 0.00002043
Iteration 80/1000 | Loss: 0.00002043
Iteration 81/1000 | Loss: 0.00002043
Iteration 82/1000 | Loss: 0.00002043
Iteration 83/1000 | Loss: 0.00002043
Iteration 84/1000 | Loss: 0.00002043
Iteration 85/1000 | Loss: 0.00002042
Iteration 86/1000 | Loss: 0.00002042
Iteration 87/1000 | Loss: 0.00002042
Iteration 88/1000 | Loss: 0.00002042
Iteration 89/1000 | Loss: 0.00002042
Iteration 90/1000 | Loss: 0.00002042
Iteration 91/1000 | Loss: 0.00002042
Iteration 92/1000 | Loss: 0.00002042
Iteration 93/1000 | Loss: 0.00002042
Iteration 94/1000 | Loss: 0.00002042
Iteration 95/1000 | Loss: 0.00002041
Iteration 96/1000 | Loss: 0.00002041
Iteration 97/1000 | Loss: 0.00002041
Iteration 98/1000 | Loss: 0.00002041
Iteration 99/1000 | Loss: 0.00002041
Iteration 100/1000 | Loss: 0.00002041
Iteration 101/1000 | Loss: 0.00002041
Iteration 102/1000 | Loss: 0.00002041
Iteration 103/1000 | Loss: 0.00002041
Iteration 104/1000 | Loss: 0.00002041
Iteration 105/1000 | Loss: 0.00002041
Iteration 106/1000 | Loss: 0.00002041
Iteration 107/1000 | Loss: 0.00002041
Iteration 108/1000 | Loss: 0.00002041
Iteration 109/1000 | Loss: 0.00002041
Iteration 110/1000 | Loss: 0.00002041
Iteration 111/1000 | Loss: 0.00002041
Iteration 112/1000 | Loss: 0.00002041
Iteration 113/1000 | Loss: 0.00002041
Iteration 114/1000 | Loss: 0.00002041
Iteration 115/1000 | Loss: 0.00002041
Iteration 116/1000 | Loss: 0.00002041
Iteration 117/1000 | Loss: 0.00002041
Iteration 118/1000 | Loss: 0.00002041
Iteration 119/1000 | Loss: 0.00002041
Iteration 120/1000 | Loss: 0.00002041
Iteration 121/1000 | Loss: 0.00002041
Iteration 122/1000 | Loss: 0.00002041
Iteration 123/1000 | Loss: 0.00002041
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 123. Stopping optimization.
Last 5 losses: [2.0408197087817825e-05, 2.0408197087817825e-05, 2.0408197087817825e-05, 2.0408197087817825e-05, 2.0408197087817825e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0408197087817825e-05

Optimization complete. Final v2v error: 3.5687670707702637 mm

Highest mean error: 4.329311847686768 mm for frame 224

Lowest mean error: 3.036715269088745 mm for frame 22

Saving results

Total time: 58.51979994773865
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_006/1081/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_006/1081.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_006/1081
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00983779
Iteration 2/25 | Loss: 0.00155363
Iteration 3/25 | Loss: 0.00102287
Iteration 4/25 | Loss: 0.00097915
Iteration 5/25 | Loss: 0.00096904
Iteration 6/25 | Loss: 0.00096675
Iteration 7/25 | Loss: 0.00096672
Iteration 8/25 | Loss: 0.00096672
Iteration 9/25 | Loss: 0.00096672
Iteration 10/25 | Loss: 0.00096672
Iteration 11/25 | Loss: 0.00096672
Iteration 12/25 | Loss: 0.00096672
Iteration 13/25 | Loss: 0.00096672
Iteration 14/25 | Loss: 0.00096672
Iteration 15/25 | Loss: 0.00096672
Iteration 16/25 | Loss: 0.00096672
Iteration 17/25 | Loss: 0.00096672
Iteration 18/25 | Loss: 0.00096672
Iteration 19/25 | Loss: 0.00096672
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0009667210397310555, 0.0009667210397310555, 0.0009667210397310555, 0.0009667210397310555, 0.0009667210397310555]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009667210397310555

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.91253293
Iteration 2/25 | Loss: 0.00039344
Iteration 3/25 | Loss: 0.00039344
Iteration 4/25 | Loss: 0.00039344
Iteration 5/25 | Loss: 0.00039344
Iteration 6/25 | Loss: 0.00039344
Iteration 7/25 | Loss: 0.00039344
Iteration 8/25 | Loss: 0.00039344
Iteration 9/25 | Loss: 0.00039344
Iteration 10/25 | Loss: 0.00039344
Iteration 11/25 | Loss: 0.00039344
Iteration 12/25 | Loss: 0.00039344
Iteration 13/25 | Loss: 0.00039344
Iteration 14/25 | Loss: 0.00039344
Iteration 15/25 | Loss: 0.00039344
Iteration 16/25 | Loss: 0.00039344
Iteration 17/25 | Loss: 0.00039344
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.00039343908429145813, 0.00039343908429145813, 0.00039343908429145813, 0.00039343908429145813, 0.00039343908429145813]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00039343908429145813

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00039344
Iteration 2/1000 | Loss: 0.00007167
Iteration 3/1000 | Loss: 0.00004674
Iteration 4/1000 | Loss: 0.00004330
Iteration 5/1000 | Loss: 0.00004157
Iteration 6/1000 | Loss: 0.00004044
Iteration 7/1000 | Loss: 0.00003962
Iteration 8/1000 | Loss: 0.00003905
Iteration 9/1000 | Loss: 0.00003857
Iteration 10/1000 | Loss: 0.00003820
Iteration 11/1000 | Loss: 0.00003788
Iteration 12/1000 | Loss: 0.00003756
Iteration 13/1000 | Loss: 0.00003734
Iteration 14/1000 | Loss: 0.00003713
Iteration 15/1000 | Loss: 0.00003696
Iteration 16/1000 | Loss: 0.00003681
Iteration 17/1000 | Loss: 0.00003678
Iteration 18/1000 | Loss: 0.00003676
Iteration 19/1000 | Loss: 0.00003675
Iteration 20/1000 | Loss: 0.00003675
Iteration 21/1000 | Loss: 0.00003675
Iteration 22/1000 | Loss: 0.00003674
Iteration 23/1000 | Loss: 0.00003674
Iteration 24/1000 | Loss: 0.00003673
Iteration 25/1000 | Loss: 0.00003672
Iteration 26/1000 | Loss: 0.00003672
Iteration 27/1000 | Loss: 0.00003672
Iteration 28/1000 | Loss: 0.00003672
Iteration 29/1000 | Loss: 0.00003672
Iteration 30/1000 | Loss: 0.00003671
Iteration 31/1000 | Loss: 0.00003670
Iteration 32/1000 | Loss: 0.00003669
Iteration 33/1000 | Loss: 0.00003667
Iteration 34/1000 | Loss: 0.00003666
Iteration 35/1000 | Loss: 0.00003664
Iteration 36/1000 | Loss: 0.00003664
Iteration 37/1000 | Loss: 0.00003663
Iteration 38/1000 | Loss: 0.00003663
Iteration 39/1000 | Loss: 0.00003662
Iteration 40/1000 | Loss: 0.00003662
Iteration 41/1000 | Loss: 0.00003662
Iteration 42/1000 | Loss: 0.00003660
Iteration 43/1000 | Loss: 0.00003659
Iteration 44/1000 | Loss: 0.00003657
Iteration 45/1000 | Loss: 0.00003657
Iteration 46/1000 | Loss: 0.00003657
Iteration 47/1000 | Loss: 0.00003657
Iteration 48/1000 | Loss: 0.00003656
Iteration 49/1000 | Loss: 0.00003656
Iteration 50/1000 | Loss: 0.00003656
Iteration 51/1000 | Loss: 0.00003656
Iteration 52/1000 | Loss: 0.00003656
Iteration 53/1000 | Loss: 0.00003656
Iteration 54/1000 | Loss: 0.00003656
Iteration 55/1000 | Loss: 0.00003655
Iteration 56/1000 | Loss: 0.00003655
Iteration 57/1000 | Loss: 0.00003653
Iteration 58/1000 | Loss: 0.00003652
Iteration 59/1000 | Loss: 0.00003651
Iteration 60/1000 | Loss: 0.00003651
Iteration 61/1000 | Loss: 0.00003650
Iteration 62/1000 | Loss: 0.00003650
Iteration 63/1000 | Loss: 0.00003649
Iteration 64/1000 | Loss: 0.00003649
Iteration 65/1000 | Loss: 0.00003649
Iteration 66/1000 | Loss: 0.00003648
Iteration 67/1000 | Loss: 0.00003648
Iteration 68/1000 | Loss: 0.00003648
Iteration 69/1000 | Loss: 0.00003648
Iteration 70/1000 | Loss: 0.00003648
Iteration 71/1000 | Loss: 0.00003648
Iteration 72/1000 | Loss: 0.00003648
Iteration 73/1000 | Loss: 0.00003648
Iteration 74/1000 | Loss: 0.00003648
Iteration 75/1000 | Loss: 0.00003648
Iteration 76/1000 | Loss: 0.00003647
Iteration 77/1000 | Loss: 0.00003647
Iteration 78/1000 | Loss: 0.00003647
Iteration 79/1000 | Loss: 0.00003646
Iteration 80/1000 | Loss: 0.00003646
Iteration 81/1000 | Loss: 0.00003646
Iteration 82/1000 | Loss: 0.00003646
Iteration 83/1000 | Loss: 0.00003646
Iteration 84/1000 | Loss: 0.00003646
Iteration 85/1000 | Loss: 0.00003646
Iteration 86/1000 | Loss: 0.00003646
Iteration 87/1000 | Loss: 0.00003646
Iteration 88/1000 | Loss: 0.00003646
Iteration 89/1000 | Loss: 0.00003646
Iteration 90/1000 | Loss: 0.00003646
Iteration 91/1000 | Loss: 0.00003645
Iteration 92/1000 | Loss: 0.00003645
Iteration 93/1000 | Loss: 0.00003645
Iteration 94/1000 | Loss: 0.00003645
Iteration 95/1000 | Loss: 0.00003645
Iteration 96/1000 | Loss: 0.00003644
Iteration 97/1000 | Loss: 0.00003644
Iteration 98/1000 | Loss: 0.00003644
Iteration 99/1000 | Loss: 0.00003644
Iteration 100/1000 | Loss: 0.00003643
Iteration 101/1000 | Loss: 0.00003643
Iteration 102/1000 | Loss: 0.00003643
Iteration 103/1000 | Loss: 0.00003643
Iteration 104/1000 | Loss: 0.00003642
Iteration 105/1000 | Loss: 0.00003642
Iteration 106/1000 | Loss: 0.00003642
Iteration 107/1000 | Loss: 0.00003642
Iteration 108/1000 | Loss: 0.00003642
Iteration 109/1000 | Loss: 0.00003641
Iteration 110/1000 | Loss: 0.00003641
Iteration 111/1000 | Loss: 0.00003641
Iteration 112/1000 | Loss: 0.00003641
Iteration 113/1000 | Loss: 0.00003641
Iteration 114/1000 | Loss: 0.00003641
Iteration 115/1000 | Loss: 0.00003641
Iteration 116/1000 | Loss: 0.00003640
Iteration 117/1000 | Loss: 0.00003640
Iteration 118/1000 | Loss: 0.00003640
Iteration 119/1000 | Loss: 0.00003640
Iteration 120/1000 | Loss: 0.00003640
Iteration 121/1000 | Loss: 0.00003640
Iteration 122/1000 | Loss: 0.00003640
Iteration 123/1000 | Loss: 0.00003640
Iteration 124/1000 | Loss: 0.00003640
Iteration 125/1000 | Loss: 0.00003639
Iteration 126/1000 | Loss: 0.00003639
Iteration 127/1000 | Loss: 0.00003639
Iteration 128/1000 | Loss: 0.00003639
Iteration 129/1000 | Loss: 0.00003639
Iteration 130/1000 | Loss: 0.00003639
Iteration 131/1000 | Loss: 0.00003639
Iteration 132/1000 | Loss: 0.00003639
Iteration 133/1000 | Loss: 0.00003638
Iteration 134/1000 | Loss: 0.00003638
Iteration 135/1000 | Loss: 0.00003638
Iteration 136/1000 | Loss: 0.00003638
Iteration 137/1000 | Loss: 0.00003638
Iteration 138/1000 | Loss: 0.00003638
Iteration 139/1000 | Loss: 0.00003637
Iteration 140/1000 | Loss: 0.00003637
Iteration 141/1000 | Loss: 0.00003637
Iteration 142/1000 | Loss: 0.00003637
Iteration 143/1000 | Loss: 0.00003637
Iteration 144/1000 | Loss: 0.00003637
Iteration 145/1000 | Loss: 0.00003637
Iteration 146/1000 | Loss: 0.00003637
Iteration 147/1000 | Loss: 0.00003637
Iteration 148/1000 | Loss: 0.00003637
Iteration 149/1000 | Loss: 0.00003637
Iteration 150/1000 | Loss: 0.00003637
Iteration 151/1000 | Loss: 0.00003636
Iteration 152/1000 | Loss: 0.00003636
Iteration 153/1000 | Loss: 0.00003636
Iteration 154/1000 | Loss: 0.00003636
Iteration 155/1000 | Loss: 0.00003636
Iteration 156/1000 | Loss: 0.00003636
Iteration 157/1000 | Loss: 0.00003636
Iteration 158/1000 | Loss: 0.00003636
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 158. Stopping optimization.
Last 5 losses: [3.635985194705427e-05, 3.635985194705427e-05, 3.635985194705427e-05, 3.635985194705427e-05, 3.635985194705427e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.635985194705427e-05

Optimization complete. Final v2v error: 4.988321304321289 mm

Highest mean error: 6.090163707733154 mm for frame 94

Lowest mean error: 4.055642127990723 mm for frame 58

Saving results

Total time: 50.896676778793335
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_006/1038/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_006/1038.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_006/1038
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00402725
Iteration 2/25 | Loss: 0.00095736
Iteration 3/25 | Loss: 0.00078588
Iteration 4/25 | Loss: 0.00075018
Iteration 5/25 | Loss: 0.00074228
Iteration 6/25 | Loss: 0.00074060
Iteration 7/25 | Loss: 0.00074024
Iteration 8/25 | Loss: 0.00074024
Iteration 9/25 | Loss: 0.00074024
Iteration 10/25 | Loss: 0.00074024
Iteration 11/25 | Loss: 0.00074024
Iteration 12/25 | Loss: 0.00074024
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0007402401533909142, 0.0007402401533909142, 0.0007402401533909142, 0.0007402401533909142, 0.0007402401533909142]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007402401533909142

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.55980158
Iteration 2/25 | Loss: 0.00052298
Iteration 3/25 | Loss: 0.00052298
Iteration 4/25 | Loss: 0.00052298
Iteration 5/25 | Loss: 0.00052298
Iteration 6/25 | Loss: 0.00052298
Iteration 7/25 | Loss: 0.00052298
Iteration 8/25 | Loss: 0.00052298
Iteration 9/25 | Loss: 0.00052298
Iteration 10/25 | Loss: 0.00052298
Iteration 11/25 | Loss: 0.00052298
Iteration 12/25 | Loss: 0.00052298
Iteration 13/25 | Loss: 0.00052298
Iteration 14/25 | Loss: 0.00052298
Iteration 15/25 | Loss: 0.00052298
Iteration 16/25 | Loss: 0.00052298
Iteration 17/25 | Loss: 0.00052298
Iteration 18/25 | Loss: 0.00052298
Iteration 19/25 | Loss: 0.00052298
Iteration 20/25 | Loss: 0.00052298
Iteration 21/25 | Loss: 0.00052298
Iteration 22/25 | Loss: 0.00052298
Iteration 23/25 | Loss: 0.00052298
Iteration 24/25 | Loss: 0.00052298
Iteration 25/25 | Loss: 0.00052298

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00052298
Iteration 2/1000 | Loss: 0.00003137
Iteration 3/1000 | Loss: 0.00002199
Iteration 4/1000 | Loss: 0.00001676
Iteration 5/1000 | Loss: 0.00001553
Iteration 6/1000 | Loss: 0.00001480
Iteration 7/1000 | Loss: 0.00001413
Iteration 8/1000 | Loss: 0.00001393
Iteration 9/1000 | Loss: 0.00001360
Iteration 10/1000 | Loss: 0.00001335
Iteration 11/1000 | Loss: 0.00001332
Iteration 12/1000 | Loss: 0.00001317
Iteration 13/1000 | Loss: 0.00001316
Iteration 14/1000 | Loss: 0.00001310
Iteration 15/1000 | Loss: 0.00001306
Iteration 16/1000 | Loss: 0.00001305
Iteration 17/1000 | Loss: 0.00001305
Iteration 18/1000 | Loss: 0.00001304
Iteration 19/1000 | Loss: 0.00001303
Iteration 20/1000 | Loss: 0.00001301
Iteration 21/1000 | Loss: 0.00001300
Iteration 22/1000 | Loss: 0.00001295
Iteration 23/1000 | Loss: 0.00001293
Iteration 24/1000 | Loss: 0.00001292
Iteration 25/1000 | Loss: 0.00001288
Iteration 26/1000 | Loss: 0.00001288
Iteration 27/1000 | Loss: 0.00001284
Iteration 28/1000 | Loss: 0.00001284
Iteration 29/1000 | Loss: 0.00001283
Iteration 30/1000 | Loss: 0.00001283
Iteration 31/1000 | Loss: 0.00001283
Iteration 32/1000 | Loss: 0.00001282
Iteration 33/1000 | Loss: 0.00001282
Iteration 34/1000 | Loss: 0.00001281
Iteration 35/1000 | Loss: 0.00001281
Iteration 36/1000 | Loss: 0.00001281
Iteration 37/1000 | Loss: 0.00001280
Iteration 38/1000 | Loss: 0.00001280
Iteration 39/1000 | Loss: 0.00001280
Iteration 40/1000 | Loss: 0.00001279
Iteration 41/1000 | Loss: 0.00001279
Iteration 42/1000 | Loss: 0.00001279
Iteration 43/1000 | Loss: 0.00001278
Iteration 44/1000 | Loss: 0.00001278
Iteration 45/1000 | Loss: 0.00001278
Iteration 46/1000 | Loss: 0.00001278
Iteration 47/1000 | Loss: 0.00001278
Iteration 48/1000 | Loss: 0.00001278
Iteration 49/1000 | Loss: 0.00001278
Iteration 50/1000 | Loss: 0.00001278
Iteration 51/1000 | Loss: 0.00001277
Iteration 52/1000 | Loss: 0.00001277
Iteration 53/1000 | Loss: 0.00001277
Iteration 54/1000 | Loss: 0.00001277
Iteration 55/1000 | Loss: 0.00001277
Iteration 56/1000 | Loss: 0.00001277
Iteration 57/1000 | Loss: 0.00001277
Iteration 58/1000 | Loss: 0.00001277
Iteration 59/1000 | Loss: 0.00001277
Iteration 60/1000 | Loss: 0.00001276
Iteration 61/1000 | Loss: 0.00001276
Iteration 62/1000 | Loss: 0.00001276
Iteration 63/1000 | Loss: 0.00001276
Iteration 64/1000 | Loss: 0.00001276
Iteration 65/1000 | Loss: 0.00001275
Iteration 66/1000 | Loss: 0.00001275
Iteration 67/1000 | Loss: 0.00001275
Iteration 68/1000 | Loss: 0.00001274
Iteration 69/1000 | Loss: 0.00001274
Iteration 70/1000 | Loss: 0.00001274
Iteration 71/1000 | Loss: 0.00001273
Iteration 72/1000 | Loss: 0.00001273
Iteration 73/1000 | Loss: 0.00001273
Iteration 74/1000 | Loss: 0.00001273
Iteration 75/1000 | Loss: 0.00001272
Iteration 76/1000 | Loss: 0.00001272
Iteration 77/1000 | Loss: 0.00001272
Iteration 78/1000 | Loss: 0.00001272
Iteration 79/1000 | Loss: 0.00001272
Iteration 80/1000 | Loss: 0.00001271
Iteration 81/1000 | Loss: 0.00001271
Iteration 82/1000 | Loss: 0.00001271
Iteration 83/1000 | Loss: 0.00001271
Iteration 84/1000 | Loss: 0.00001271
Iteration 85/1000 | Loss: 0.00001271
Iteration 86/1000 | Loss: 0.00001271
Iteration 87/1000 | Loss: 0.00001271
Iteration 88/1000 | Loss: 0.00001270
Iteration 89/1000 | Loss: 0.00001270
Iteration 90/1000 | Loss: 0.00001270
Iteration 91/1000 | Loss: 0.00001270
Iteration 92/1000 | Loss: 0.00001270
Iteration 93/1000 | Loss: 0.00001270
Iteration 94/1000 | Loss: 0.00001270
Iteration 95/1000 | Loss: 0.00001270
Iteration 96/1000 | Loss: 0.00001270
Iteration 97/1000 | Loss: 0.00001270
Iteration 98/1000 | Loss: 0.00001270
Iteration 99/1000 | Loss: 0.00001270
Iteration 100/1000 | Loss: 0.00001270
Iteration 101/1000 | Loss: 0.00001270
Iteration 102/1000 | Loss: 0.00001269
Iteration 103/1000 | Loss: 0.00001269
Iteration 104/1000 | Loss: 0.00001269
Iteration 105/1000 | Loss: 0.00001269
Iteration 106/1000 | Loss: 0.00001269
Iteration 107/1000 | Loss: 0.00001269
Iteration 108/1000 | Loss: 0.00001269
Iteration 109/1000 | Loss: 0.00001269
Iteration 110/1000 | Loss: 0.00001269
Iteration 111/1000 | Loss: 0.00001269
Iteration 112/1000 | Loss: 0.00001269
Iteration 113/1000 | Loss: 0.00001269
Iteration 114/1000 | Loss: 0.00001269
Iteration 115/1000 | Loss: 0.00001269
Iteration 116/1000 | Loss: 0.00001269
Iteration 117/1000 | Loss: 0.00001269
Iteration 118/1000 | Loss: 0.00001269
Iteration 119/1000 | Loss: 0.00001269
Iteration 120/1000 | Loss: 0.00001269
Iteration 121/1000 | Loss: 0.00001269
Iteration 122/1000 | Loss: 0.00001269
Iteration 123/1000 | Loss: 0.00001269
Iteration 124/1000 | Loss: 0.00001269
Iteration 125/1000 | Loss: 0.00001269
Iteration 126/1000 | Loss: 0.00001269
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 126. Stopping optimization.
Last 5 losses: [1.2690079529420473e-05, 1.2690079529420473e-05, 1.2690079529420473e-05, 1.2690079529420473e-05, 1.2690079529420473e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2690079529420473e-05

Optimization complete. Final v2v error: 3.027170419692993 mm

Highest mean error: 3.58882474899292 mm for frame 132

Lowest mean error: 2.7023162841796875 mm for frame 152

Saving results

Total time: 36.87744975090027
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_006/1001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_006/1001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_006/1001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01094850
Iteration 2/25 | Loss: 0.00165626
Iteration 3/25 | Loss: 0.00105040
Iteration 4/25 | Loss: 0.00098834
Iteration 5/25 | Loss: 0.00096462
Iteration 6/25 | Loss: 0.00095571
Iteration 7/25 | Loss: 0.00095440
Iteration 8/25 | Loss: 0.00095440
Iteration 9/25 | Loss: 0.00095440
Iteration 10/25 | Loss: 0.00095440
Iteration 11/25 | Loss: 0.00095440
Iteration 12/25 | Loss: 0.00095440
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0009544036001898348, 0.0009544036001898348, 0.0009544036001898348, 0.0009544036001898348, 0.0009544036001898348]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009544036001898348

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36774397
Iteration 2/25 | Loss: 0.00046831
Iteration 3/25 | Loss: 0.00046831
Iteration 4/25 | Loss: 0.00046830
Iteration 5/25 | Loss: 0.00046830
Iteration 6/25 | Loss: 0.00046830
Iteration 7/25 | Loss: 0.00046830
Iteration 8/25 | Loss: 0.00046830
Iteration 9/25 | Loss: 0.00046830
Iteration 10/25 | Loss: 0.00046830
Iteration 11/25 | Loss: 0.00046830
Iteration 12/25 | Loss: 0.00046830
Iteration 13/25 | Loss: 0.00046830
Iteration 14/25 | Loss: 0.00046830
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.00046830240171402693, 0.00046830240171402693, 0.00046830240171402693, 0.00046830240171402693, 0.00046830240171402693]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00046830240171402693

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00046830
Iteration 2/1000 | Loss: 0.00004000
Iteration 3/1000 | Loss: 0.00003151
Iteration 4/1000 | Loss: 0.00002909
Iteration 5/1000 | Loss: 0.00002792
Iteration 6/1000 | Loss: 0.00002738
Iteration 7/1000 | Loss: 0.00002706
Iteration 8/1000 | Loss: 0.00002682
Iteration 9/1000 | Loss: 0.00002670
Iteration 10/1000 | Loss: 0.00002663
Iteration 11/1000 | Loss: 0.00002663
Iteration 12/1000 | Loss: 0.00002663
Iteration 13/1000 | Loss: 0.00002663
Iteration 14/1000 | Loss: 0.00002662
Iteration 15/1000 | Loss: 0.00002662
Iteration 16/1000 | Loss: 0.00002662
Iteration 17/1000 | Loss: 0.00002662
Iteration 18/1000 | Loss: 0.00002662
Iteration 19/1000 | Loss: 0.00002662
Iteration 20/1000 | Loss: 0.00002662
Iteration 21/1000 | Loss: 0.00002660
Iteration 22/1000 | Loss: 0.00002660
Iteration 23/1000 | Loss: 0.00002660
Iteration 24/1000 | Loss: 0.00002660
Iteration 25/1000 | Loss: 0.00002660
Iteration 26/1000 | Loss: 0.00002659
Iteration 27/1000 | Loss: 0.00002659
Iteration 28/1000 | Loss: 0.00002659
Iteration 29/1000 | Loss: 0.00002659
Iteration 30/1000 | Loss: 0.00002659
Iteration 31/1000 | Loss: 0.00002659
Iteration 32/1000 | Loss: 0.00002659
Iteration 33/1000 | Loss: 0.00002658
Iteration 34/1000 | Loss: 0.00002658
Iteration 35/1000 | Loss: 0.00002658
Iteration 36/1000 | Loss: 0.00002657
Iteration 37/1000 | Loss: 0.00002657
Iteration 38/1000 | Loss: 0.00002657
Iteration 39/1000 | Loss: 0.00002656
Iteration 40/1000 | Loss: 0.00002656
Iteration 41/1000 | Loss: 0.00002656
Iteration 42/1000 | Loss: 0.00002656
Iteration 43/1000 | Loss: 0.00002656
Iteration 44/1000 | Loss: 0.00002656
Iteration 45/1000 | Loss: 0.00002656
Iteration 46/1000 | Loss: 0.00002656
Iteration 47/1000 | Loss: 0.00002656
Iteration 48/1000 | Loss: 0.00002656
Iteration 49/1000 | Loss: 0.00002655
Iteration 50/1000 | Loss: 0.00002655
Iteration 51/1000 | Loss: 0.00002654
Iteration 52/1000 | Loss: 0.00002654
Iteration 53/1000 | Loss: 0.00002654
Iteration 54/1000 | Loss: 0.00002654
Iteration 55/1000 | Loss: 0.00002654
Iteration 56/1000 | Loss: 0.00002654
Iteration 57/1000 | Loss: 0.00002654
Iteration 58/1000 | Loss: 0.00002653
Iteration 59/1000 | Loss: 0.00002653
Iteration 60/1000 | Loss: 0.00002652
Iteration 61/1000 | Loss: 0.00002652
Iteration 62/1000 | Loss: 0.00002652
Iteration 63/1000 | Loss: 0.00002652
Iteration 64/1000 | Loss: 0.00002652
Iteration 65/1000 | Loss: 0.00002652
Iteration 66/1000 | Loss: 0.00002652
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 66. Stopping optimization.
Last 5 losses: [2.6524949134909548e-05, 2.6524949134909548e-05, 2.6524949134909548e-05, 2.6524949134909548e-05, 2.6524949134909548e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.6524949134909548e-05

Optimization complete. Final v2v error: 4.023422718048096 mm

Highest mean error: 4.832016944885254 mm for frame 151

Lowest mean error: 3.3910927772521973 mm for frame 8

Saving results

Total time: 29.672771692276
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_006/1002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_006/1002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_006/1002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00422816
Iteration 2/25 | Loss: 0.00084726
Iteration 3/25 | Loss: 0.00075638
Iteration 4/25 | Loss: 0.00074026
Iteration 5/25 | Loss: 0.00073423
Iteration 6/25 | Loss: 0.00073293
Iteration 7/25 | Loss: 0.00073272
Iteration 8/25 | Loss: 0.00073271
Iteration 9/25 | Loss: 0.00073271
Iteration 10/25 | Loss: 0.00073271
Iteration 11/25 | Loss: 0.00073271
Iteration 12/25 | Loss: 0.00073271
Iteration 13/25 | Loss: 0.00073271
Iteration 14/25 | Loss: 0.00073271
Iteration 15/25 | Loss: 0.00073271
Iteration 16/25 | Loss: 0.00073271
Iteration 17/25 | Loss: 0.00073271
Iteration 18/25 | Loss: 0.00073271
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.000732713786419481, 0.000732713786419481, 0.000732713786419481, 0.000732713786419481, 0.000732713786419481]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000732713786419481

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.50073445
Iteration 2/25 | Loss: 0.00050524
Iteration 3/25 | Loss: 0.00050524
Iteration 4/25 | Loss: 0.00050524
Iteration 5/25 | Loss: 0.00050524
Iteration 6/25 | Loss: 0.00050524
Iteration 7/25 | Loss: 0.00050524
Iteration 8/25 | Loss: 0.00050524
Iteration 9/25 | Loss: 0.00050524
Iteration 10/25 | Loss: 0.00050524
Iteration 11/25 | Loss: 0.00050524
Iteration 12/25 | Loss: 0.00050524
Iteration 13/25 | Loss: 0.00050524
Iteration 14/25 | Loss: 0.00050524
Iteration 15/25 | Loss: 0.00050524
Iteration 16/25 | Loss: 0.00050524
Iteration 17/25 | Loss: 0.00050524
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0005052379565313458, 0.0005052379565313458, 0.0005052379565313458, 0.0005052379565313458, 0.0005052379565313458]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005052379565313458

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00050524
Iteration 2/1000 | Loss: 0.00002596
Iteration 3/1000 | Loss: 0.00001684
Iteration 4/1000 | Loss: 0.00001547
Iteration 5/1000 | Loss: 0.00001441
Iteration 6/1000 | Loss: 0.00001395
Iteration 7/1000 | Loss: 0.00001362
Iteration 8/1000 | Loss: 0.00001339
Iteration 9/1000 | Loss: 0.00001338
Iteration 10/1000 | Loss: 0.00001323
Iteration 11/1000 | Loss: 0.00001314
Iteration 12/1000 | Loss: 0.00001313
Iteration 13/1000 | Loss: 0.00001306
Iteration 14/1000 | Loss: 0.00001304
Iteration 15/1000 | Loss: 0.00001302
Iteration 16/1000 | Loss: 0.00001299
Iteration 17/1000 | Loss: 0.00001299
Iteration 18/1000 | Loss: 0.00001299
Iteration 19/1000 | Loss: 0.00001299
Iteration 20/1000 | Loss: 0.00001299
Iteration 21/1000 | Loss: 0.00001299
Iteration 22/1000 | Loss: 0.00001299
Iteration 23/1000 | Loss: 0.00001299
Iteration 24/1000 | Loss: 0.00001299
Iteration 25/1000 | Loss: 0.00001299
Iteration 26/1000 | Loss: 0.00001299
Iteration 27/1000 | Loss: 0.00001298
Iteration 28/1000 | Loss: 0.00001298
Iteration 29/1000 | Loss: 0.00001298
Iteration 30/1000 | Loss: 0.00001295
Iteration 31/1000 | Loss: 0.00001295
Iteration 32/1000 | Loss: 0.00001294
Iteration 33/1000 | Loss: 0.00001294
Iteration 34/1000 | Loss: 0.00001294
Iteration 35/1000 | Loss: 0.00001294
Iteration 36/1000 | Loss: 0.00001294
Iteration 37/1000 | Loss: 0.00001293
Iteration 38/1000 | Loss: 0.00001292
Iteration 39/1000 | Loss: 0.00001292
Iteration 40/1000 | Loss: 0.00001292
Iteration 41/1000 | Loss: 0.00001291
Iteration 42/1000 | Loss: 0.00001291
Iteration 43/1000 | Loss: 0.00001291
Iteration 44/1000 | Loss: 0.00001291
Iteration 45/1000 | Loss: 0.00001291
Iteration 46/1000 | Loss: 0.00001291
Iteration 47/1000 | Loss: 0.00001290
Iteration 48/1000 | Loss: 0.00001290
Iteration 49/1000 | Loss: 0.00001290
Iteration 50/1000 | Loss: 0.00001289
Iteration 51/1000 | Loss: 0.00001289
Iteration 52/1000 | Loss: 0.00001289
Iteration 53/1000 | Loss: 0.00001289
Iteration 54/1000 | Loss: 0.00001289
Iteration 55/1000 | Loss: 0.00001289
Iteration 56/1000 | Loss: 0.00001289
Iteration 57/1000 | Loss: 0.00001289
Iteration 58/1000 | Loss: 0.00001289
Iteration 59/1000 | Loss: 0.00001289
Iteration 60/1000 | Loss: 0.00001289
Iteration 61/1000 | Loss: 0.00001289
Iteration 62/1000 | Loss: 0.00001289
Iteration 63/1000 | Loss: 0.00001289
Iteration 64/1000 | Loss: 0.00001289
Iteration 65/1000 | Loss: 0.00001289
Iteration 66/1000 | Loss: 0.00001289
Iteration 67/1000 | Loss: 0.00001289
Iteration 68/1000 | Loss: 0.00001289
Iteration 69/1000 | Loss: 0.00001289
Iteration 70/1000 | Loss: 0.00001289
Iteration 71/1000 | Loss: 0.00001289
Iteration 72/1000 | Loss: 0.00001289
Iteration 73/1000 | Loss: 0.00001289
Iteration 74/1000 | Loss: 0.00001289
Iteration 75/1000 | Loss: 0.00001289
Iteration 76/1000 | Loss: 0.00001289
Iteration 77/1000 | Loss: 0.00001289
Iteration 78/1000 | Loss: 0.00001289
Iteration 79/1000 | Loss: 0.00001289
Iteration 80/1000 | Loss: 0.00001289
Iteration 81/1000 | Loss: 0.00001289
Iteration 82/1000 | Loss: 0.00001289
Iteration 83/1000 | Loss: 0.00001289
Iteration 84/1000 | Loss: 0.00001289
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 84. Stopping optimization.
Last 5 losses: [1.28874407891999e-05, 1.28874407891999e-05, 1.28874407891999e-05, 1.28874407891999e-05, 1.28874407891999e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.28874407891999e-05

Optimization complete. Final v2v error: 3.040311813354492 mm

Highest mean error: 3.2346627712249756 mm for frame 173

Lowest mean error: 2.8015291690826416 mm for frame 0

Saving results

Total time: 28.86716341972351
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_006/1031/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_006/1031.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_006/1031
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00398900
Iteration 2/25 | Loss: 0.00084355
Iteration 3/25 | Loss: 0.00075113
Iteration 4/25 | Loss: 0.00073228
Iteration 5/25 | Loss: 0.00072609
Iteration 6/25 | Loss: 0.00072471
Iteration 7/25 | Loss: 0.00072426
Iteration 8/25 | Loss: 0.00072426
Iteration 9/25 | Loss: 0.00072426
Iteration 10/25 | Loss: 0.00072426
Iteration 11/25 | Loss: 0.00072426
Iteration 12/25 | Loss: 0.00072426
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0007242588326334953, 0.0007242588326334953, 0.0007242588326334953, 0.0007242588326334953, 0.0007242588326334953]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007242588326334953

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.50415468
Iteration 2/25 | Loss: 0.00051142
Iteration 3/25 | Loss: 0.00051142
Iteration 4/25 | Loss: 0.00051142
Iteration 5/25 | Loss: 0.00051142
Iteration 6/25 | Loss: 0.00051142
Iteration 7/25 | Loss: 0.00051142
Iteration 8/25 | Loss: 0.00051142
Iteration 9/25 | Loss: 0.00051142
Iteration 10/25 | Loss: 0.00051142
Iteration 11/25 | Loss: 0.00051142
Iteration 12/25 | Loss: 0.00051142
Iteration 13/25 | Loss: 0.00051142
Iteration 14/25 | Loss: 0.00051142
Iteration 15/25 | Loss: 0.00051142
Iteration 16/25 | Loss: 0.00051142
Iteration 17/25 | Loss: 0.00051142
Iteration 18/25 | Loss: 0.00051142
Iteration 19/25 | Loss: 0.00051142
Iteration 20/25 | Loss: 0.00051142
Iteration 21/25 | Loss: 0.00051142
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0005114195519126952, 0.0005114195519126952, 0.0005114195519126952, 0.0005114195519126952, 0.0005114195519126952]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005114195519126952

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00051142
Iteration 2/1000 | Loss: 0.00002147
Iteration 3/1000 | Loss: 0.00001455
Iteration 4/1000 | Loss: 0.00001372
Iteration 5/1000 | Loss: 0.00001314
Iteration 6/1000 | Loss: 0.00001283
Iteration 7/1000 | Loss: 0.00001257
Iteration 8/1000 | Loss: 0.00001240
Iteration 9/1000 | Loss: 0.00001238
Iteration 10/1000 | Loss: 0.00001230
Iteration 11/1000 | Loss: 0.00001226
Iteration 12/1000 | Loss: 0.00001225
Iteration 13/1000 | Loss: 0.00001225
Iteration 14/1000 | Loss: 0.00001225
Iteration 15/1000 | Loss: 0.00001224
Iteration 16/1000 | Loss: 0.00001222
Iteration 17/1000 | Loss: 0.00001222
Iteration 18/1000 | Loss: 0.00001219
Iteration 19/1000 | Loss: 0.00001219
Iteration 20/1000 | Loss: 0.00001215
Iteration 21/1000 | Loss: 0.00001213
Iteration 22/1000 | Loss: 0.00001212
Iteration 23/1000 | Loss: 0.00001212
Iteration 24/1000 | Loss: 0.00001211
Iteration 25/1000 | Loss: 0.00001211
Iteration 26/1000 | Loss: 0.00001211
Iteration 27/1000 | Loss: 0.00001210
Iteration 28/1000 | Loss: 0.00001210
Iteration 29/1000 | Loss: 0.00001210
Iteration 30/1000 | Loss: 0.00001210
Iteration 31/1000 | Loss: 0.00001210
Iteration 32/1000 | Loss: 0.00001209
Iteration 33/1000 | Loss: 0.00001208
Iteration 34/1000 | Loss: 0.00001207
Iteration 35/1000 | Loss: 0.00001207
Iteration 36/1000 | Loss: 0.00001207
Iteration 37/1000 | Loss: 0.00001206
Iteration 38/1000 | Loss: 0.00001206
Iteration 39/1000 | Loss: 0.00001206
Iteration 40/1000 | Loss: 0.00001206
Iteration 41/1000 | Loss: 0.00001205
Iteration 42/1000 | Loss: 0.00001205
Iteration 43/1000 | Loss: 0.00001205
Iteration 44/1000 | Loss: 0.00001204
Iteration 45/1000 | Loss: 0.00001204
Iteration 46/1000 | Loss: 0.00001204
Iteration 47/1000 | Loss: 0.00001203
Iteration 48/1000 | Loss: 0.00001203
Iteration 49/1000 | Loss: 0.00001203
Iteration 50/1000 | Loss: 0.00001203
Iteration 51/1000 | Loss: 0.00001201
Iteration 52/1000 | Loss: 0.00001201
Iteration 53/1000 | Loss: 0.00001201
Iteration 54/1000 | Loss: 0.00001200
Iteration 55/1000 | Loss: 0.00001200
Iteration 56/1000 | Loss: 0.00001200
Iteration 57/1000 | Loss: 0.00001200
Iteration 58/1000 | Loss: 0.00001200
Iteration 59/1000 | Loss: 0.00001199
Iteration 60/1000 | Loss: 0.00001199
Iteration 61/1000 | Loss: 0.00001199
Iteration 62/1000 | Loss: 0.00001199
Iteration 63/1000 | Loss: 0.00001199
Iteration 64/1000 | Loss: 0.00001198
Iteration 65/1000 | Loss: 0.00001198
Iteration 66/1000 | Loss: 0.00001197
Iteration 67/1000 | Loss: 0.00001197
Iteration 68/1000 | Loss: 0.00001197
Iteration 69/1000 | Loss: 0.00001197
Iteration 70/1000 | Loss: 0.00001197
Iteration 71/1000 | Loss: 0.00001196
Iteration 72/1000 | Loss: 0.00001196
Iteration 73/1000 | Loss: 0.00001196
Iteration 74/1000 | Loss: 0.00001195
Iteration 75/1000 | Loss: 0.00001195
Iteration 76/1000 | Loss: 0.00001195
Iteration 77/1000 | Loss: 0.00001195
Iteration 78/1000 | Loss: 0.00001194
Iteration 79/1000 | Loss: 0.00001194
Iteration 80/1000 | Loss: 0.00001194
Iteration 81/1000 | Loss: 0.00001194
Iteration 82/1000 | Loss: 0.00001194
Iteration 83/1000 | Loss: 0.00001193
Iteration 84/1000 | Loss: 0.00001193
Iteration 85/1000 | Loss: 0.00001193
Iteration 86/1000 | Loss: 0.00001193
Iteration 87/1000 | Loss: 0.00001193
Iteration 88/1000 | Loss: 0.00001193
Iteration 89/1000 | Loss: 0.00001193
Iteration 90/1000 | Loss: 0.00001193
Iteration 91/1000 | Loss: 0.00001193
Iteration 92/1000 | Loss: 0.00001193
Iteration 93/1000 | Loss: 0.00001192
Iteration 94/1000 | Loss: 0.00001192
Iteration 95/1000 | Loss: 0.00001192
Iteration 96/1000 | Loss: 0.00001192
Iteration 97/1000 | Loss: 0.00001192
Iteration 98/1000 | Loss: 0.00001192
Iteration 99/1000 | Loss: 0.00001191
Iteration 100/1000 | Loss: 0.00001191
Iteration 101/1000 | Loss: 0.00001191
Iteration 102/1000 | Loss: 0.00001190
Iteration 103/1000 | Loss: 0.00001190
Iteration 104/1000 | Loss: 0.00001190
Iteration 105/1000 | Loss: 0.00001189
Iteration 106/1000 | Loss: 0.00001189
Iteration 107/1000 | Loss: 0.00001188
Iteration 108/1000 | Loss: 0.00001188
Iteration 109/1000 | Loss: 0.00001187
Iteration 110/1000 | Loss: 0.00001187
Iteration 111/1000 | Loss: 0.00001187
Iteration 112/1000 | Loss: 0.00001187
Iteration 113/1000 | Loss: 0.00001186
Iteration 114/1000 | Loss: 0.00001186
Iteration 115/1000 | Loss: 0.00001185
Iteration 116/1000 | Loss: 0.00001185
Iteration 117/1000 | Loss: 0.00001185
Iteration 118/1000 | Loss: 0.00001185
Iteration 119/1000 | Loss: 0.00001184
Iteration 120/1000 | Loss: 0.00001184
Iteration 121/1000 | Loss: 0.00001184
Iteration 122/1000 | Loss: 0.00001184
Iteration 123/1000 | Loss: 0.00001184
Iteration 124/1000 | Loss: 0.00001184
Iteration 125/1000 | Loss: 0.00001184
Iteration 126/1000 | Loss: 0.00001183
Iteration 127/1000 | Loss: 0.00001183
Iteration 128/1000 | Loss: 0.00001183
Iteration 129/1000 | Loss: 0.00001183
Iteration 130/1000 | Loss: 0.00001183
Iteration 131/1000 | Loss: 0.00001183
Iteration 132/1000 | Loss: 0.00001183
Iteration 133/1000 | Loss: 0.00001183
Iteration 134/1000 | Loss: 0.00001183
Iteration 135/1000 | Loss: 0.00001183
Iteration 136/1000 | Loss: 0.00001183
Iteration 137/1000 | Loss: 0.00001183
Iteration 138/1000 | Loss: 0.00001183
Iteration 139/1000 | Loss: 0.00001183
Iteration 140/1000 | Loss: 0.00001183
Iteration 141/1000 | Loss: 0.00001183
Iteration 142/1000 | Loss: 0.00001183
Iteration 143/1000 | Loss: 0.00001183
Iteration 144/1000 | Loss: 0.00001183
Iteration 145/1000 | Loss: 0.00001183
Iteration 146/1000 | Loss: 0.00001183
Iteration 147/1000 | Loss: 0.00001183
Iteration 148/1000 | Loss: 0.00001183
Iteration 149/1000 | Loss: 0.00001183
Iteration 150/1000 | Loss: 0.00001183
Iteration 151/1000 | Loss: 0.00001183
Iteration 152/1000 | Loss: 0.00001183
Iteration 153/1000 | Loss: 0.00001183
Iteration 154/1000 | Loss: 0.00001183
Iteration 155/1000 | Loss: 0.00001183
Iteration 156/1000 | Loss: 0.00001183
Iteration 157/1000 | Loss: 0.00001183
Iteration 158/1000 | Loss: 0.00001183
Iteration 159/1000 | Loss: 0.00001183
Iteration 160/1000 | Loss: 0.00001183
Iteration 161/1000 | Loss: 0.00001183
Iteration 162/1000 | Loss: 0.00001183
Iteration 163/1000 | Loss: 0.00001183
Iteration 164/1000 | Loss: 0.00001183
Iteration 165/1000 | Loss: 0.00001183
Iteration 166/1000 | Loss: 0.00001183
Iteration 167/1000 | Loss: 0.00001183
Iteration 168/1000 | Loss: 0.00001183
Iteration 169/1000 | Loss: 0.00001183
Iteration 170/1000 | Loss: 0.00001183
Iteration 171/1000 | Loss: 0.00001183
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 171. Stopping optimization.
Last 5 losses: [1.1832597920147236e-05, 1.1832597920147236e-05, 1.1832597920147236e-05, 1.1832597920147236e-05, 1.1832597920147236e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1832597920147236e-05

Optimization complete. Final v2v error: 2.924870729446411 mm

Highest mean error: 3.1402974128723145 mm for frame 15

Lowest mean error: 2.709763765335083 mm for frame 46

Saving results

Total time: 33.107866048812866
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_006/1039/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_006/1039.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_006/1039
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01061873
Iteration 2/25 | Loss: 0.00228450
Iteration 3/25 | Loss: 0.00147549
Iteration 4/25 | Loss: 0.00123148
Iteration 5/25 | Loss: 0.00111966
Iteration 6/25 | Loss: 0.00105026
Iteration 7/25 | Loss: 0.00102633
Iteration 8/25 | Loss: 0.00099957
Iteration 9/25 | Loss: 0.00096440
Iteration 10/25 | Loss: 0.00095409
Iteration 11/25 | Loss: 0.00094361
Iteration 12/25 | Loss: 0.00093083
Iteration 13/25 | Loss: 0.00092140
Iteration 14/25 | Loss: 0.00091900
Iteration 15/25 | Loss: 0.00091219
Iteration 16/25 | Loss: 0.00090945
Iteration 17/25 | Loss: 0.00090784
Iteration 18/25 | Loss: 0.00090757
Iteration 19/25 | Loss: 0.00090747
Iteration 20/25 | Loss: 0.00091169
Iteration 21/25 | Loss: 0.00090628
Iteration 22/25 | Loss: 0.00090365
Iteration 23/25 | Loss: 0.00090253
Iteration 24/25 | Loss: 0.00090511
Iteration 25/25 | Loss: 0.00090196

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.49036694
Iteration 2/25 | Loss: 0.00093569
Iteration 3/25 | Loss: 0.00093568
Iteration 4/25 | Loss: 0.00093568
Iteration 5/25 | Loss: 0.00093568
Iteration 6/25 | Loss: 0.00093568
Iteration 7/25 | Loss: 0.00093568
Iteration 8/25 | Loss: 0.00093568
Iteration 9/25 | Loss: 0.00093568
Iteration 10/25 | Loss: 0.00093568
Iteration 11/25 | Loss: 0.00093568
Iteration 12/25 | Loss: 0.00093568
Iteration 13/25 | Loss: 0.00093568
Iteration 14/25 | Loss: 0.00093568
Iteration 15/25 | Loss: 0.00093568
Iteration 16/25 | Loss: 0.00093568
Iteration 17/25 | Loss: 0.00093568
Iteration 18/25 | Loss: 0.00093568
Iteration 19/25 | Loss: 0.00093568
Iteration 20/25 | Loss: 0.00093568
Iteration 21/25 | Loss: 0.00093568
Iteration 22/25 | Loss: 0.00093568
Iteration 23/25 | Loss: 0.00093568
Iteration 24/25 | Loss: 0.00093568
Iteration 25/25 | Loss: 0.00093568

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00093568
Iteration 2/1000 | Loss: 0.00109730
Iteration 3/1000 | Loss: 0.00015134
Iteration 4/1000 | Loss: 0.00008921
Iteration 5/1000 | Loss: 0.00006079
Iteration 6/1000 | Loss: 0.00004768
Iteration 7/1000 | Loss: 0.00004123
Iteration 8/1000 | Loss: 0.00003795
Iteration 9/1000 | Loss: 0.00017707
Iteration 10/1000 | Loss: 0.00037218
Iteration 11/1000 | Loss: 0.00004515
Iteration 12/1000 | Loss: 0.00003400
Iteration 13/1000 | Loss: 0.00002788
Iteration 14/1000 | Loss: 0.00002336
Iteration 15/1000 | Loss: 0.00002144
Iteration 16/1000 | Loss: 0.00002054
Iteration 17/1000 | Loss: 0.00001972
Iteration 18/1000 | Loss: 0.00001918
Iteration 19/1000 | Loss: 0.00001869
Iteration 20/1000 | Loss: 0.00001835
Iteration 21/1000 | Loss: 0.00001812
Iteration 22/1000 | Loss: 0.00001794
Iteration 23/1000 | Loss: 0.00001792
Iteration 24/1000 | Loss: 0.00001790
Iteration 25/1000 | Loss: 0.00001783
Iteration 26/1000 | Loss: 0.00001778
Iteration 27/1000 | Loss: 0.00001778
Iteration 28/1000 | Loss: 0.00001777
Iteration 29/1000 | Loss: 0.00001776
Iteration 30/1000 | Loss: 0.00001776
Iteration 31/1000 | Loss: 0.00001774
Iteration 32/1000 | Loss: 0.00001774
Iteration 33/1000 | Loss: 0.00001773
Iteration 34/1000 | Loss: 0.00001773
Iteration 35/1000 | Loss: 0.00001773
Iteration 36/1000 | Loss: 0.00001772
Iteration 37/1000 | Loss: 0.00001772
Iteration 38/1000 | Loss: 0.00001771
Iteration 39/1000 | Loss: 0.00001771
Iteration 40/1000 | Loss: 0.00001771
Iteration 41/1000 | Loss: 0.00001771
Iteration 42/1000 | Loss: 0.00001770
Iteration 43/1000 | Loss: 0.00001770
Iteration 44/1000 | Loss: 0.00001770
Iteration 45/1000 | Loss: 0.00001770
Iteration 46/1000 | Loss: 0.00001769
Iteration 47/1000 | Loss: 0.00001769
Iteration 48/1000 | Loss: 0.00001769
Iteration 49/1000 | Loss: 0.00001768
Iteration 50/1000 | Loss: 0.00001768
Iteration 51/1000 | Loss: 0.00001768
Iteration 52/1000 | Loss: 0.00001767
Iteration 53/1000 | Loss: 0.00001767
Iteration 54/1000 | Loss: 0.00001767
Iteration 55/1000 | Loss: 0.00001767
Iteration 56/1000 | Loss: 0.00001767
Iteration 57/1000 | Loss: 0.00001766
Iteration 58/1000 | Loss: 0.00001766
Iteration 59/1000 | Loss: 0.00001766
Iteration 60/1000 | Loss: 0.00001766
Iteration 61/1000 | Loss: 0.00001766
Iteration 62/1000 | Loss: 0.00001765
Iteration 63/1000 | Loss: 0.00001765
Iteration 64/1000 | Loss: 0.00001765
Iteration 65/1000 | Loss: 0.00001765
Iteration 66/1000 | Loss: 0.00001764
Iteration 67/1000 | Loss: 0.00001764
Iteration 68/1000 | Loss: 0.00001764
Iteration 69/1000 | Loss: 0.00001764
Iteration 70/1000 | Loss: 0.00001764
Iteration 71/1000 | Loss: 0.00001764
Iteration 72/1000 | Loss: 0.00001764
Iteration 73/1000 | Loss: 0.00001763
Iteration 74/1000 | Loss: 0.00001763
Iteration 75/1000 | Loss: 0.00001763
Iteration 76/1000 | Loss: 0.00001763
Iteration 77/1000 | Loss: 0.00001763
Iteration 78/1000 | Loss: 0.00001763
Iteration 79/1000 | Loss: 0.00001763
Iteration 80/1000 | Loss: 0.00001763
Iteration 81/1000 | Loss: 0.00001763
Iteration 82/1000 | Loss: 0.00001763
Iteration 83/1000 | Loss: 0.00001763
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 83. Stopping optimization.
Last 5 losses: [1.762665488058701e-05, 1.762665488058701e-05, 1.762665488058701e-05, 1.762665488058701e-05, 1.762665488058701e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.762665488058701e-05

Optimization complete. Final v2v error: 3.541766881942749 mm

Highest mean error: 3.8261637687683105 mm for frame 27

Lowest mean error: 3.328129529953003 mm for frame 0

Saving results

Total time: 92.179771900177
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_006/1092/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_006/1092.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_006/1092
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00937251
Iteration 2/25 | Loss: 0.00152491
Iteration 3/25 | Loss: 0.00095706
Iteration 4/25 | Loss: 0.00089437
Iteration 5/25 | Loss: 0.00087263
Iteration 6/25 | Loss: 0.00086573
Iteration 7/25 | Loss: 0.00086475
Iteration 8/25 | Loss: 0.00086471
Iteration 9/25 | Loss: 0.00086471
Iteration 10/25 | Loss: 0.00086471
Iteration 11/25 | Loss: 0.00086471
Iteration 12/25 | Loss: 0.00086471
Iteration 13/25 | Loss: 0.00086471
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0008647057111375034, 0.0008647057111375034, 0.0008647057111375034, 0.0008647057111375034, 0.0008647057111375034]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008647057111375034

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.12004423
Iteration 2/25 | Loss: 0.00040132
Iteration 3/25 | Loss: 0.00040130
Iteration 4/25 | Loss: 0.00040130
Iteration 5/25 | Loss: 0.00040130
Iteration 6/25 | Loss: 0.00040130
Iteration 7/25 | Loss: 0.00040130
Iteration 8/25 | Loss: 0.00040130
Iteration 9/25 | Loss: 0.00040130
Iteration 10/25 | Loss: 0.00040130
Iteration 11/25 | Loss: 0.00040130
Iteration 12/25 | Loss: 0.00040130
Iteration 13/25 | Loss: 0.00040130
Iteration 14/25 | Loss: 0.00040130
Iteration 15/25 | Loss: 0.00040130
Iteration 16/25 | Loss: 0.00040130
Iteration 17/25 | Loss: 0.00040130
Iteration 18/25 | Loss: 0.00040130
Iteration 19/25 | Loss: 0.00040130
Iteration 20/25 | Loss: 0.00040130
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.00040129938861355186, 0.00040129938861355186, 0.00040129938861355186, 0.00040129938861355186, 0.00040129938861355186]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00040129938861355186

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00040130
Iteration 2/1000 | Loss: 0.00005083
Iteration 3/1000 | Loss: 0.00003723
Iteration 4/1000 | Loss: 0.00003324
Iteration 5/1000 | Loss: 0.00003141
Iteration 6/1000 | Loss: 0.00003010
Iteration 7/1000 | Loss: 0.00002901
Iteration 8/1000 | Loss: 0.00002814
Iteration 9/1000 | Loss: 0.00002768
Iteration 10/1000 | Loss: 0.00002740
Iteration 11/1000 | Loss: 0.00002715
Iteration 12/1000 | Loss: 0.00002699
Iteration 13/1000 | Loss: 0.00002687
Iteration 14/1000 | Loss: 0.00002679
Iteration 15/1000 | Loss: 0.00002675
Iteration 16/1000 | Loss: 0.00002674
Iteration 17/1000 | Loss: 0.00002673
Iteration 18/1000 | Loss: 0.00002671
Iteration 19/1000 | Loss: 0.00002670
Iteration 20/1000 | Loss: 0.00002670
Iteration 21/1000 | Loss: 0.00002669
Iteration 22/1000 | Loss: 0.00002669
Iteration 23/1000 | Loss: 0.00002665
Iteration 24/1000 | Loss: 0.00002665
Iteration 25/1000 | Loss: 0.00002660
Iteration 26/1000 | Loss: 0.00002660
Iteration 27/1000 | Loss: 0.00002660
Iteration 28/1000 | Loss: 0.00002659
Iteration 29/1000 | Loss: 0.00002659
Iteration 30/1000 | Loss: 0.00002659
Iteration 31/1000 | Loss: 0.00002653
Iteration 32/1000 | Loss: 0.00002646
Iteration 33/1000 | Loss: 0.00002644
Iteration 34/1000 | Loss: 0.00002644
Iteration 35/1000 | Loss: 0.00002643
Iteration 36/1000 | Loss: 0.00002640
Iteration 37/1000 | Loss: 0.00002639
Iteration 38/1000 | Loss: 0.00002639
Iteration 39/1000 | Loss: 0.00002633
Iteration 40/1000 | Loss: 0.00002626
Iteration 41/1000 | Loss: 0.00002622
Iteration 42/1000 | Loss: 0.00002622
Iteration 43/1000 | Loss: 0.00002621
Iteration 44/1000 | Loss: 0.00002621
Iteration 45/1000 | Loss: 0.00002621
Iteration 46/1000 | Loss: 0.00002619
Iteration 47/1000 | Loss: 0.00002618
Iteration 48/1000 | Loss: 0.00002618
Iteration 49/1000 | Loss: 0.00002617
Iteration 50/1000 | Loss: 0.00002617
Iteration 51/1000 | Loss: 0.00002617
Iteration 52/1000 | Loss: 0.00002617
Iteration 53/1000 | Loss: 0.00002617
Iteration 54/1000 | Loss: 0.00002616
Iteration 55/1000 | Loss: 0.00002616
Iteration 56/1000 | Loss: 0.00002616
Iteration 57/1000 | Loss: 0.00002616
Iteration 58/1000 | Loss: 0.00002616
Iteration 59/1000 | Loss: 0.00002616
Iteration 60/1000 | Loss: 0.00002616
Iteration 61/1000 | Loss: 0.00002615
Iteration 62/1000 | Loss: 0.00002615
Iteration 63/1000 | Loss: 0.00002615
Iteration 64/1000 | Loss: 0.00002615
Iteration 65/1000 | Loss: 0.00002614
Iteration 66/1000 | Loss: 0.00002614
Iteration 67/1000 | Loss: 0.00002614
Iteration 68/1000 | Loss: 0.00002614
Iteration 69/1000 | Loss: 0.00002614
Iteration 70/1000 | Loss: 0.00002614
Iteration 71/1000 | Loss: 0.00002614
Iteration 72/1000 | Loss: 0.00002613
Iteration 73/1000 | Loss: 0.00002613
Iteration 74/1000 | Loss: 0.00002613
Iteration 75/1000 | Loss: 0.00002613
Iteration 76/1000 | Loss: 0.00002613
Iteration 77/1000 | Loss: 0.00002612
Iteration 78/1000 | Loss: 0.00002612
Iteration 79/1000 | Loss: 0.00002612
Iteration 80/1000 | Loss: 0.00002612
Iteration 81/1000 | Loss: 0.00002612
Iteration 82/1000 | Loss: 0.00002611
Iteration 83/1000 | Loss: 0.00002611
Iteration 84/1000 | Loss: 0.00002611
Iteration 85/1000 | Loss: 0.00002611
Iteration 86/1000 | Loss: 0.00002611
Iteration 87/1000 | Loss: 0.00002610
Iteration 88/1000 | Loss: 0.00002610
Iteration 89/1000 | Loss: 0.00002610
Iteration 90/1000 | Loss: 0.00002610
Iteration 91/1000 | Loss: 0.00002609
Iteration 92/1000 | Loss: 0.00002609
Iteration 93/1000 | Loss: 0.00002609
Iteration 94/1000 | Loss: 0.00002609
Iteration 95/1000 | Loss: 0.00002609
Iteration 96/1000 | Loss: 0.00002609
Iteration 97/1000 | Loss: 0.00002609
Iteration 98/1000 | Loss: 0.00002608
Iteration 99/1000 | Loss: 0.00002608
Iteration 100/1000 | Loss: 0.00002608
Iteration 101/1000 | Loss: 0.00002608
Iteration 102/1000 | Loss: 0.00002608
Iteration 103/1000 | Loss: 0.00002608
Iteration 104/1000 | Loss: 0.00002608
Iteration 105/1000 | Loss: 0.00002607
Iteration 106/1000 | Loss: 0.00002607
Iteration 107/1000 | Loss: 0.00002607
Iteration 108/1000 | Loss: 0.00002607
Iteration 109/1000 | Loss: 0.00002607
Iteration 110/1000 | Loss: 0.00002607
Iteration 111/1000 | Loss: 0.00002607
Iteration 112/1000 | Loss: 0.00002607
Iteration 113/1000 | Loss: 0.00002607
Iteration 114/1000 | Loss: 0.00002607
Iteration 115/1000 | Loss: 0.00002607
Iteration 116/1000 | Loss: 0.00002607
Iteration 117/1000 | Loss: 0.00002607
Iteration 118/1000 | Loss: 0.00002606
Iteration 119/1000 | Loss: 0.00002606
Iteration 120/1000 | Loss: 0.00002606
Iteration 121/1000 | Loss: 0.00002606
Iteration 122/1000 | Loss: 0.00002606
Iteration 123/1000 | Loss: 0.00002606
Iteration 124/1000 | Loss: 0.00002606
Iteration 125/1000 | Loss: 0.00002606
Iteration 126/1000 | Loss: 0.00002606
Iteration 127/1000 | Loss: 0.00002606
Iteration 128/1000 | Loss: 0.00002606
Iteration 129/1000 | Loss: 0.00002605
Iteration 130/1000 | Loss: 0.00002605
Iteration 131/1000 | Loss: 0.00002605
Iteration 132/1000 | Loss: 0.00002605
Iteration 133/1000 | Loss: 0.00002605
Iteration 134/1000 | Loss: 0.00002605
Iteration 135/1000 | Loss: 0.00002605
Iteration 136/1000 | Loss: 0.00002605
Iteration 137/1000 | Loss: 0.00002605
Iteration 138/1000 | Loss: 0.00002605
Iteration 139/1000 | Loss: 0.00002605
Iteration 140/1000 | Loss: 0.00002605
Iteration 141/1000 | Loss: 0.00002605
Iteration 142/1000 | Loss: 0.00002604
Iteration 143/1000 | Loss: 0.00002604
Iteration 144/1000 | Loss: 0.00002604
Iteration 145/1000 | Loss: 0.00002604
Iteration 146/1000 | Loss: 0.00002604
Iteration 147/1000 | Loss: 0.00002604
Iteration 148/1000 | Loss: 0.00002604
Iteration 149/1000 | Loss: 0.00002604
Iteration 150/1000 | Loss: 0.00002604
Iteration 151/1000 | Loss: 0.00002604
Iteration 152/1000 | Loss: 0.00002604
Iteration 153/1000 | Loss: 0.00002604
Iteration 154/1000 | Loss: 0.00002604
Iteration 155/1000 | Loss: 0.00002603
Iteration 156/1000 | Loss: 0.00002603
Iteration 157/1000 | Loss: 0.00002603
Iteration 158/1000 | Loss: 0.00002603
Iteration 159/1000 | Loss: 0.00002603
Iteration 160/1000 | Loss: 0.00002603
Iteration 161/1000 | Loss: 0.00002603
Iteration 162/1000 | Loss: 0.00002603
Iteration 163/1000 | Loss: 0.00002603
Iteration 164/1000 | Loss: 0.00002603
Iteration 165/1000 | Loss: 0.00002603
Iteration 166/1000 | Loss: 0.00002603
Iteration 167/1000 | Loss: 0.00002603
Iteration 168/1000 | Loss: 0.00002602
Iteration 169/1000 | Loss: 0.00002602
Iteration 170/1000 | Loss: 0.00002602
Iteration 171/1000 | Loss: 0.00002602
Iteration 172/1000 | Loss: 0.00002602
Iteration 173/1000 | Loss: 0.00002602
Iteration 174/1000 | Loss: 0.00002602
Iteration 175/1000 | Loss: 0.00002602
Iteration 176/1000 | Loss: 0.00002602
Iteration 177/1000 | Loss: 0.00002602
Iteration 178/1000 | Loss: 0.00002601
Iteration 179/1000 | Loss: 0.00002601
Iteration 180/1000 | Loss: 0.00002601
Iteration 181/1000 | Loss: 0.00002601
Iteration 182/1000 | Loss: 0.00002601
Iteration 183/1000 | Loss: 0.00002601
Iteration 184/1000 | Loss: 0.00002601
Iteration 185/1000 | Loss: 0.00002601
Iteration 186/1000 | Loss: 0.00002601
Iteration 187/1000 | Loss: 0.00002601
Iteration 188/1000 | Loss: 0.00002601
Iteration 189/1000 | Loss: 0.00002601
Iteration 190/1000 | Loss: 0.00002601
Iteration 191/1000 | Loss: 0.00002601
Iteration 192/1000 | Loss: 0.00002600
Iteration 193/1000 | Loss: 0.00002600
Iteration 194/1000 | Loss: 0.00002600
Iteration 195/1000 | Loss: 0.00002600
Iteration 196/1000 | Loss: 0.00002600
Iteration 197/1000 | Loss: 0.00002600
Iteration 198/1000 | Loss: 0.00002600
Iteration 199/1000 | Loss: 0.00002600
Iteration 200/1000 | Loss: 0.00002600
Iteration 201/1000 | Loss: 0.00002600
Iteration 202/1000 | Loss: 0.00002600
Iteration 203/1000 | Loss: 0.00002600
Iteration 204/1000 | Loss: 0.00002600
Iteration 205/1000 | Loss: 0.00002600
Iteration 206/1000 | Loss: 0.00002600
Iteration 207/1000 | Loss: 0.00002600
Iteration 208/1000 | Loss: 0.00002600
Iteration 209/1000 | Loss: 0.00002600
Iteration 210/1000 | Loss: 0.00002600
Iteration 211/1000 | Loss: 0.00002599
Iteration 212/1000 | Loss: 0.00002599
Iteration 213/1000 | Loss: 0.00002599
Iteration 214/1000 | Loss: 0.00002599
Iteration 215/1000 | Loss: 0.00002599
Iteration 216/1000 | Loss: 0.00002599
Iteration 217/1000 | Loss: 0.00002599
Iteration 218/1000 | Loss: 0.00002599
Iteration 219/1000 | Loss: 0.00002599
Iteration 220/1000 | Loss: 0.00002599
Iteration 221/1000 | Loss: 0.00002599
Iteration 222/1000 | Loss: 0.00002599
Iteration 223/1000 | Loss: 0.00002599
Iteration 224/1000 | Loss: 0.00002599
Iteration 225/1000 | Loss: 0.00002599
Iteration 226/1000 | Loss: 0.00002599
Iteration 227/1000 | Loss: 0.00002599
Iteration 228/1000 | Loss: 0.00002599
Iteration 229/1000 | Loss: 0.00002599
Iteration 230/1000 | Loss: 0.00002599
Iteration 231/1000 | Loss: 0.00002599
Iteration 232/1000 | Loss: 0.00002599
Iteration 233/1000 | Loss: 0.00002599
Iteration 234/1000 | Loss: 0.00002599
Iteration 235/1000 | Loss: 0.00002599
Iteration 236/1000 | Loss: 0.00002599
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 236. Stopping optimization.
Last 5 losses: [2.598687933641486e-05, 2.598687933641486e-05, 2.598687933641486e-05, 2.598687933641486e-05, 2.598687933641486e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.598687933641486e-05

Optimization complete. Final v2v error: 4.146581172943115 mm

Highest mean error: 5.342396259307861 mm for frame 103

Lowest mean error: 3.369596481323242 mm for frame 41

Saving results

Total time: 50.074066400527954
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_006/1091/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_006/1091.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_006/1091
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00824489
Iteration 2/25 | Loss: 0.00091127
Iteration 3/25 | Loss: 0.00078390
Iteration 4/25 | Loss: 0.00075411
Iteration 5/25 | Loss: 0.00075016
Iteration 6/25 | Loss: 0.00074879
Iteration 7/25 | Loss: 0.00074879
Iteration 8/25 | Loss: 0.00074879
Iteration 9/25 | Loss: 0.00074879
Iteration 10/25 | Loss: 0.00074879
Iteration 11/25 | Loss: 0.00074879
Iteration 12/25 | Loss: 0.00074879
Iteration 13/25 | Loss: 0.00074879
Iteration 14/25 | Loss: 0.00074879
Iteration 15/25 | Loss: 0.00074879
Iteration 16/25 | Loss: 0.00074879
Iteration 17/25 | Loss: 0.00074879
Iteration 18/25 | Loss: 0.00074879
Iteration 19/25 | Loss: 0.00074879
Iteration 20/25 | Loss: 0.00074879
Iteration 21/25 | Loss: 0.00074879
Iteration 22/25 | Loss: 0.00074879
Iteration 23/25 | Loss: 0.00074879
Iteration 24/25 | Loss: 0.00074879
Iteration 25/25 | Loss: 0.00074879

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.53866255
Iteration 2/25 | Loss: 0.00042935
Iteration 3/25 | Loss: 0.00042934
Iteration 4/25 | Loss: 0.00042934
Iteration 5/25 | Loss: 0.00042934
Iteration 6/25 | Loss: 0.00042934
Iteration 7/25 | Loss: 0.00042934
Iteration 8/25 | Loss: 0.00042934
Iteration 9/25 | Loss: 0.00042934
Iteration 10/25 | Loss: 0.00042934
Iteration 11/25 | Loss: 0.00042934
Iteration 12/25 | Loss: 0.00042934
Iteration 13/25 | Loss: 0.00042934
Iteration 14/25 | Loss: 0.00042934
Iteration 15/25 | Loss: 0.00042934
Iteration 16/25 | Loss: 0.00042934
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.00042934040538966656, 0.00042934040538966656, 0.00042934040538966656, 0.00042934040538966656, 0.00042934040538966656]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00042934040538966656

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00042934
Iteration 2/1000 | Loss: 0.00003606
Iteration 3/1000 | Loss: 0.00002403
Iteration 4/1000 | Loss: 0.00002212
Iteration 5/1000 | Loss: 0.00002050
Iteration 6/1000 | Loss: 0.00001986
Iteration 7/1000 | Loss: 0.00001912
Iteration 8/1000 | Loss: 0.00001871
Iteration 9/1000 | Loss: 0.00001840
Iteration 10/1000 | Loss: 0.00001819
Iteration 11/1000 | Loss: 0.00001803
Iteration 12/1000 | Loss: 0.00001801
Iteration 13/1000 | Loss: 0.00001801
Iteration 14/1000 | Loss: 0.00001801
Iteration 15/1000 | Loss: 0.00001799
Iteration 16/1000 | Loss: 0.00001792
Iteration 17/1000 | Loss: 0.00001789
Iteration 18/1000 | Loss: 0.00001788
Iteration 19/1000 | Loss: 0.00001788
Iteration 20/1000 | Loss: 0.00001787
Iteration 21/1000 | Loss: 0.00001787
Iteration 22/1000 | Loss: 0.00001787
Iteration 23/1000 | Loss: 0.00001786
Iteration 24/1000 | Loss: 0.00001785
Iteration 25/1000 | Loss: 0.00001785
Iteration 26/1000 | Loss: 0.00001785
Iteration 27/1000 | Loss: 0.00001784
Iteration 28/1000 | Loss: 0.00001784
Iteration 29/1000 | Loss: 0.00001784
Iteration 30/1000 | Loss: 0.00001784
Iteration 31/1000 | Loss: 0.00001783
Iteration 32/1000 | Loss: 0.00001783
Iteration 33/1000 | Loss: 0.00001783
Iteration 34/1000 | Loss: 0.00001783
Iteration 35/1000 | Loss: 0.00001783
Iteration 36/1000 | Loss: 0.00001783
Iteration 37/1000 | Loss: 0.00001782
Iteration 38/1000 | Loss: 0.00001782
Iteration 39/1000 | Loss: 0.00001782
Iteration 40/1000 | Loss: 0.00001782
Iteration 41/1000 | Loss: 0.00001782
Iteration 42/1000 | Loss: 0.00001782
Iteration 43/1000 | Loss: 0.00001782
Iteration 44/1000 | Loss: 0.00001782
Iteration 45/1000 | Loss: 0.00001782
Iteration 46/1000 | Loss: 0.00001782
Iteration 47/1000 | Loss: 0.00001782
Iteration 48/1000 | Loss: 0.00001782
Iteration 49/1000 | Loss: 0.00001782
Iteration 50/1000 | Loss: 0.00001782
Iteration 51/1000 | Loss: 0.00001782
Iteration 52/1000 | Loss: 0.00001782
Iteration 53/1000 | Loss: 0.00001782
Iteration 54/1000 | Loss: 0.00001782
Iteration 55/1000 | Loss: 0.00001782
Iteration 56/1000 | Loss: 0.00001782
Iteration 57/1000 | Loss: 0.00001782
Iteration 58/1000 | Loss: 0.00001782
Iteration 59/1000 | Loss: 0.00001782
Iteration 60/1000 | Loss: 0.00001782
Iteration 61/1000 | Loss: 0.00001782
Iteration 62/1000 | Loss: 0.00001782
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 62. Stopping optimization.
Last 5 losses: [1.781687569746282e-05, 1.781687569746282e-05, 1.781687569746282e-05, 1.781687569746282e-05, 1.781687569746282e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.781687569746282e-05

Optimization complete. Final v2v error: 3.5577962398529053 mm

Highest mean error: 3.896646738052368 mm for frame 73

Lowest mean error: 3.3982009887695312 mm for frame 57

Saving results

Total time: 32.861955404281616
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_006/1040/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_006/1040.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_006/1040
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01118860
Iteration 2/25 | Loss: 0.00201083
Iteration 3/25 | Loss: 0.00112563
Iteration 4/25 | Loss: 0.00101632
Iteration 5/25 | Loss: 0.00097081
Iteration 6/25 | Loss: 0.00095541
Iteration 7/25 | Loss: 0.00098207
Iteration 8/25 | Loss: 0.00094759
Iteration 9/25 | Loss: 0.00093363
Iteration 10/25 | Loss: 0.00091857
Iteration 11/25 | Loss: 0.00090853
Iteration 12/25 | Loss: 0.00090431
Iteration 13/25 | Loss: 0.00090183
Iteration 14/25 | Loss: 0.00089992
Iteration 15/25 | Loss: 0.00089888
Iteration 16/25 | Loss: 0.00089766
Iteration 17/25 | Loss: 0.00089725
Iteration 18/25 | Loss: 0.00089715
Iteration 19/25 | Loss: 0.00089709
Iteration 20/25 | Loss: 0.00089709
Iteration 21/25 | Loss: 0.00089709
Iteration 22/25 | Loss: 0.00089709
Iteration 23/25 | Loss: 0.00089709
Iteration 24/25 | Loss: 0.00089709
Iteration 25/25 | Loss: 0.00089708

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.74381793
Iteration 2/25 | Loss: 0.00050211
Iteration 3/25 | Loss: 0.00044267
Iteration 4/25 | Loss: 0.00044261
Iteration 5/25 | Loss: 0.00044261
Iteration 6/25 | Loss: 0.00044261
Iteration 7/25 | Loss: 0.00044260
Iteration 8/25 | Loss: 0.00044260
Iteration 9/25 | Loss: 0.00044260
Iteration 10/25 | Loss: 0.00044260
Iteration 11/25 | Loss: 0.00044260
Iteration 12/25 | Loss: 0.00044260
Iteration 13/25 | Loss: 0.00044260
Iteration 14/25 | Loss: 0.00044260
Iteration 15/25 | Loss: 0.00044260
Iteration 16/25 | Loss: 0.00044260
Iteration 17/25 | Loss: 0.00044260
Iteration 18/25 | Loss: 0.00044260
Iteration 19/25 | Loss: 0.00044260
Iteration 20/25 | Loss: 0.00044260
Iteration 21/25 | Loss: 0.00044260
Iteration 22/25 | Loss: 0.00044260
Iteration 23/25 | Loss: 0.00044260
Iteration 24/25 | Loss: 0.00044260
Iteration 25/25 | Loss: 0.00044260

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00044260
Iteration 2/1000 | Loss: 0.00005545
Iteration 3/1000 | Loss: 0.00011124
Iteration 4/1000 | Loss: 0.00005108
Iteration 5/1000 | Loss: 0.00004067
Iteration 6/1000 | Loss: 0.00003855
Iteration 7/1000 | Loss: 0.00015962
Iteration 8/1000 | Loss: 0.00003609
Iteration 9/1000 | Loss: 0.00003516
Iteration 10/1000 | Loss: 0.00003446
Iteration 11/1000 | Loss: 0.00003393
Iteration 12/1000 | Loss: 0.00003357
Iteration 13/1000 | Loss: 0.00003322
Iteration 14/1000 | Loss: 0.00003294
Iteration 15/1000 | Loss: 0.00003268
Iteration 16/1000 | Loss: 0.00003250
Iteration 17/1000 | Loss: 0.00003246
Iteration 18/1000 | Loss: 0.00003243
Iteration 19/1000 | Loss: 0.00003242
Iteration 20/1000 | Loss: 0.00003242
Iteration 21/1000 | Loss: 0.00003237
Iteration 22/1000 | Loss: 0.00003235
Iteration 23/1000 | Loss: 0.00003233
Iteration 24/1000 | Loss: 0.00003225
Iteration 25/1000 | Loss: 0.00003224
Iteration 26/1000 | Loss: 0.00003223
Iteration 27/1000 | Loss: 0.00003223
Iteration 28/1000 | Loss: 0.00003223
Iteration 29/1000 | Loss: 0.00003222
Iteration 30/1000 | Loss: 0.00003222
Iteration 31/1000 | Loss: 0.00003222
Iteration 32/1000 | Loss: 0.00003221
Iteration 33/1000 | Loss: 0.00003220
Iteration 34/1000 | Loss: 0.00008249
Iteration 35/1000 | Loss: 0.00003216
Iteration 36/1000 | Loss: 0.00003214
Iteration 37/1000 | Loss: 0.00003214
Iteration 38/1000 | Loss: 0.00003213
Iteration 39/1000 | Loss: 0.00003213
Iteration 40/1000 | Loss: 0.00003213
Iteration 41/1000 | Loss: 0.00003213
Iteration 42/1000 | Loss: 0.00003213
Iteration 43/1000 | Loss: 0.00003212
Iteration 44/1000 | Loss: 0.00003212
Iteration 45/1000 | Loss: 0.00003211
Iteration 46/1000 | Loss: 0.00003211
Iteration 47/1000 | Loss: 0.00003211
Iteration 48/1000 | Loss: 0.00003210
Iteration 49/1000 | Loss: 0.00003210
Iteration 50/1000 | Loss: 0.00003210
Iteration 51/1000 | Loss: 0.00003209
Iteration 52/1000 | Loss: 0.00003209
Iteration 53/1000 | Loss: 0.00003209
Iteration 54/1000 | Loss: 0.00003209
Iteration 55/1000 | Loss: 0.00003209
Iteration 56/1000 | Loss: 0.00003209
Iteration 57/1000 | Loss: 0.00003208
Iteration 58/1000 | Loss: 0.00003208
Iteration 59/1000 | Loss: 0.00003208
Iteration 60/1000 | Loss: 0.00003208
Iteration 61/1000 | Loss: 0.00003208
Iteration 62/1000 | Loss: 0.00003208
Iteration 63/1000 | Loss: 0.00003208
Iteration 64/1000 | Loss: 0.00003208
Iteration 65/1000 | Loss: 0.00003208
Iteration 66/1000 | Loss: 0.00003208
Iteration 67/1000 | Loss: 0.00003208
Iteration 68/1000 | Loss: 0.00003208
Iteration 69/1000 | Loss: 0.00003208
Iteration 70/1000 | Loss: 0.00003208
Iteration 71/1000 | Loss: 0.00003207
Iteration 72/1000 | Loss: 0.00003207
Iteration 73/1000 | Loss: 0.00003207
Iteration 74/1000 | Loss: 0.00003207
Iteration 75/1000 | Loss: 0.00003207
Iteration 76/1000 | Loss: 0.00003207
Iteration 77/1000 | Loss: 0.00003207
Iteration 78/1000 | Loss: 0.00003207
Iteration 79/1000 | Loss: 0.00003207
Iteration 80/1000 | Loss: 0.00003207
Iteration 81/1000 | Loss: 0.00003207
Iteration 82/1000 | Loss: 0.00003206
Iteration 83/1000 | Loss: 0.00003206
Iteration 84/1000 | Loss: 0.00003206
Iteration 85/1000 | Loss: 0.00003206
Iteration 86/1000 | Loss: 0.00003205
Iteration 87/1000 | Loss: 0.00003204
Iteration 88/1000 | Loss: 0.00003204
Iteration 89/1000 | Loss: 0.00003204
Iteration 90/1000 | Loss: 0.00003203
Iteration 91/1000 | Loss: 0.00003203
Iteration 92/1000 | Loss: 0.00003203
Iteration 93/1000 | Loss: 0.00003202
Iteration 94/1000 | Loss: 0.00003202
Iteration 95/1000 | Loss: 0.00003202
Iteration 96/1000 | Loss: 0.00003202
Iteration 97/1000 | Loss: 0.00003202
Iteration 98/1000 | Loss: 0.00003201
Iteration 99/1000 | Loss: 0.00003201
Iteration 100/1000 | Loss: 0.00003201
Iteration 101/1000 | Loss: 0.00003201
Iteration 102/1000 | Loss: 0.00003201
Iteration 103/1000 | Loss: 0.00003200
Iteration 104/1000 | Loss: 0.00003200
Iteration 105/1000 | Loss: 0.00003200
Iteration 106/1000 | Loss: 0.00003200
Iteration 107/1000 | Loss: 0.00003200
Iteration 108/1000 | Loss: 0.00003200
Iteration 109/1000 | Loss: 0.00003200
Iteration 110/1000 | Loss: 0.00003200
Iteration 111/1000 | Loss: 0.00003199
Iteration 112/1000 | Loss: 0.00003199
Iteration 113/1000 | Loss: 0.00003199
Iteration 114/1000 | Loss: 0.00003199
Iteration 115/1000 | Loss: 0.00003199
Iteration 116/1000 | Loss: 0.00003199
Iteration 117/1000 | Loss: 0.00003199
Iteration 118/1000 | Loss: 0.00003199
Iteration 119/1000 | Loss: 0.00003199
Iteration 120/1000 | Loss: 0.00003199
Iteration 121/1000 | Loss: 0.00003199
Iteration 122/1000 | Loss: 0.00003199
Iteration 123/1000 | Loss: 0.00003199
Iteration 124/1000 | Loss: 0.00003199
Iteration 125/1000 | Loss: 0.00003199
Iteration 126/1000 | Loss: 0.00003198
Iteration 127/1000 | Loss: 0.00003198
Iteration 128/1000 | Loss: 0.00003198
Iteration 129/1000 | Loss: 0.00003198
Iteration 130/1000 | Loss: 0.00003198
Iteration 131/1000 | Loss: 0.00003198
Iteration 132/1000 | Loss: 0.00003198
Iteration 133/1000 | Loss: 0.00003198
Iteration 134/1000 | Loss: 0.00003198
Iteration 135/1000 | Loss: 0.00003198
Iteration 136/1000 | Loss: 0.00003198
Iteration 137/1000 | Loss: 0.00003198
Iteration 138/1000 | Loss: 0.00003198
Iteration 139/1000 | Loss: 0.00003198
Iteration 140/1000 | Loss: 0.00003198
Iteration 141/1000 | Loss: 0.00003198
Iteration 142/1000 | Loss: 0.00003198
Iteration 143/1000 | Loss: 0.00003198
Iteration 144/1000 | Loss: 0.00003198
Iteration 145/1000 | Loss: 0.00003198
Iteration 146/1000 | Loss: 0.00003198
Iteration 147/1000 | Loss: 0.00003198
Iteration 148/1000 | Loss: 0.00003198
Iteration 149/1000 | Loss: 0.00003198
Iteration 150/1000 | Loss: 0.00003198
Iteration 151/1000 | Loss: 0.00003198
Iteration 152/1000 | Loss: 0.00003198
Iteration 153/1000 | Loss: 0.00003198
Iteration 154/1000 | Loss: 0.00003198
Iteration 155/1000 | Loss: 0.00003198
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 155. Stopping optimization.
Last 5 losses: [3.1982413929654285e-05, 3.1982413929654285e-05, 3.1982413929654285e-05, 3.1982413929654285e-05, 3.1982413929654285e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.1982413929654285e-05

Optimization complete. Final v2v error: 4.708777904510498 mm

Highest mean error: 5.945266246795654 mm for frame 171

Lowest mean error: 3.9113612174987793 mm for frame 205

Saving results

Total time: 79.56709742546082
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_006/1060/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_006/1060.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_006/1060
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00893750
Iteration 2/25 | Loss: 0.00129806
Iteration 3/25 | Loss: 0.00099868
Iteration 4/25 | Loss: 0.00093991
Iteration 5/25 | Loss: 0.00091385
Iteration 6/25 | Loss: 0.00090216
Iteration 7/25 | Loss: 0.00089629
Iteration 8/25 | Loss: 0.00089617
Iteration 9/25 | Loss: 0.00089612
Iteration 10/25 | Loss: 0.00089611
Iteration 11/25 | Loss: 0.00089549
Iteration 12/25 | Loss: 0.00089529
Iteration 13/25 | Loss: 0.00089199
Iteration 14/25 | Loss: 0.00089026
Iteration 15/25 | Loss: 0.00089001
Iteration 16/25 | Loss: 0.00088983
Iteration 17/25 | Loss: 0.00088975
Iteration 18/25 | Loss: 0.00088975
Iteration 19/25 | Loss: 0.00088975
Iteration 20/25 | Loss: 0.00088975
Iteration 21/25 | Loss: 0.00088975
Iteration 22/25 | Loss: 0.00088975
Iteration 23/25 | Loss: 0.00088975
Iteration 24/25 | Loss: 0.00088975
Iteration 25/25 | Loss: 0.00088975

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42573547
Iteration 2/25 | Loss: 0.00051677
Iteration 3/25 | Loss: 0.00051672
Iteration 4/25 | Loss: 0.00051672
Iteration 5/25 | Loss: 0.00051672
Iteration 6/25 | Loss: 0.00051672
Iteration 7/25 | Loss: 0.00051672
Iteration 8/25 | Loss: 0.00051672
Iteration 9/25 | Loss: 0.00051672
Iteration 10/25 | Loss: 0.00051672
Iteration 11/25 | Loss: 0.00051672
Iteration 12/25 | Loss: 0.00051672
Iteration 13/25 | Loss: 0.00051672
Iteration 14/25 | Loss: 0.00051672
Iteration 15/25 | Loss: 0.00051672
Iteration 16/25 | Loss: 0.00051672
Iteration 17/25 | Loss: 0.00051672
Iteration 18/25 | Loss: 0.00051672
Iteration 19/25 | Loss: 0.00051672
Iteration 20/25 | Loss: 0.00051672
Iteration 21/25 | Loss: 0.00051672
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0005167202325537801, 0.0005167202325537801, 0.0005167202325537801, 0.0005167202325537801, 0.0005167202325537801]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005167202325537801

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00051672
Iteration 2/1000 | Loss: 0.00004421
Iteration 3/1000 | Loss: 0.00003191
Iteration 4/1000 | Loss: 0.00002867
Iteration 5/1000 | Loss: 0.00002754
Iteration 6/1000 | Loss: 0.00002649
Iteration 7/1000 | Loss: 0.00002582
Iteration 8/1000 | Loss: 0.00002526
Iteration 9/1000 | Loss: 0.00002474
Iteration 10/1000 | Loss: 0.00002443
Iteration 11/1000 | Loss: 0.00002419
Iteration 12/1000 | Loss: 0.00002410
Iteration 13/1000 | Loss: 0.00002394
Iteration 14/1000 | Loss: 0.00002392
Iteration 15/1000 | Loss: 0.00002388
Iteration 16/1000 | Loss: 0.00002388
Iteration 17/1000 | Loss: 0.00002386
Iteration 18/1000 | Loss: 0.00002385
Iteration 19/1000 | Loss: 0.00002382
Iteration 20/1000 | Loss: 0.00002381
Iteration 21/1000 | Loss: 0.00002374
Iteration 22/1000 | Loss: 0.00002370
Iteration 23/1000 | Loss: 0.00002363
Iteration 24/1000 | Loss: 0.00002360
Iteration 25/1000 | Loss: 0.00002359
Iteration 26/1000 | Loss: 0.00002359
Iteration 27/1000 | Loss: 0.00002358
Iteration 28/1000 | Loss: 0.00002357
Iteration 29/1000 | Loss: 0.00002357
Iteration 30/1000 | Loss: 0.00002357
Iteration 31/1000 | Loss: 0.00002353
Iteration 32/1000 | Loss: 0.00002353
Iteration 33/1000 | Loss: 0.00002351
Iteration 34/1000 | Loss: 0.00002351
Iteration 35/1000 | Loss: 0.00002350
Iteration 36/1000 | Loss: 0.00002350
Iteration 37/1000 | Loss: 0.00002349
Iteration 38/1000 | Loss: 0.00002349
Iteration 39/1000 | Loss: 0.00002348
Iteration 40/1000 | Loss: 0.00002348
Iteration 41/1000 | Loss: 0.00002347
Iteration 42/1000 | Loss: 0.00002347
Iteration 43/1000 | Loss: 0.00002347
Iteration 44/1000 | Loss: 0.00002346
Iteration 45/1000 | Loss: 0.00002346
Iteration 46/1000 | Loss: 0.00002346
Iteration 47/1000 | Loss: 0.00002345
Iteration 48/1000 | Loss: 0.00002344
Iteration 49/1000 | Loss: 0.00002344
Iteration 50/1000 | Loss: 0.00002344
Iteration 51/1000 | Loss: 0.00002343
Iteration 52/1000 | Loss: 0.00002343
Iteration 53/1000 | Loss: 0.00002342
Iteration 54/1000 | Loss: 0.00002342
Iteration 55/1000 | Loss: 0.00002342
Iteration 56/1000 | Loss: 0.00002342
Iteration 57/1000 | Loss: 0.00002342
Iteration 58/1000 | Loss: 0.00002342
Iteration 59/1000 | Loss: 0.00002342
Iteration 60/1000 | Loss: 0.00002342
Iteration 61/1000 | Loss: 0.00002342
Iteration 62/1000 | Loss: 0.00002341
Iteration 63/1000 | Loss: 0.00002341
Iteration 64/1000 | Loss: 0.00002341
Iteration 65/1000 | Loss: 0.00002340
Iteration 66/1000 | Loss: 0.00002340
Iteration 67/1000 | Loss: 0.00002340
Iteration 68/1000 | Loss: 0.00002340
Iteration 69/1000 | Loss: 0.00002340
Iteration 70/1000 | Loss: 0.00002339
Iteration 71/1000 | Loss: 0.00002339
Iteration 72/1000 | Loss: 0.00002339
Iteration 73/1000 | Loss: 0.00002338
Iteration 74/1000 | Loss: 0.00002338
Iteration 75/1000 | Loss: 0.00002338
Iteration 76/1000 | Loss: 0.00002338
Iteration 77/1000 | Loss: 0.00002338
Iteration 78/1000 | Loss: 0.00002338
Iteration 79/1000 | Loss: 0.00002338
Iteration 80/1000 | Loss: 0.00002338
Iteration 81/1000 | Loss: 0.00002338
Iteration 82/1000 | Loss: 0.00002338
Iteration 83/1000 | Loss: 0.00002337
Iteration 84/1000 | Loss: 0.00002337
Iteration 85/1000 | Loss: 0.00002337
Iteration 86/1000 | Loss: 0.00002337
Iteration 87/1000 | Loss: 0.00002337
Iteration 88/1000 | Loss: 0.00002337
Iteration 89/1000 | Loss: 0.00002336
Iteration 90/1000 | Loss: 0.00002336
Iteration 91/1000 | Loss: 0.00002336
Iteration 92/1000 | Loss: 0.00002336
Iteration 93/1000 | Loss: 0.00002336
Iteration 94/1000 | Loss: 0.00002336
Iteration 95/1000 | Loss: 0.00002336
Iteration 96/1000 | Loss: 0.00002336
Iteration 97/1000 | Loss: 0.00002335
Iteration 98/1000 | Loss: 0.00002335
Iteration 99/1000 | Loss: 0.00002335
Iteration 100/1000 | Loss: 0.00002335
Iteration 101/1000 | Loss: 0.00002335
Iteration 102/1000 | Loss: 0.00002334
Iteration 103/1000 | Loss: 0.00002334
Iteration 104/1000 | Loss: 0.00002334
Iteration 105/1000 | Loss: 0.00002333
Iteration 106/1000 | Loss: 0.00002333
Iteration 107/1000 | Loss: 0.00002333
Iteration 108/1000 | Loss: 0.00002333
Iteration 109/1000 | Loss: 0.00002332
Iteration 110/1000 | Loss: 0.00002332
Iteration 111/1000 | Loss: 0.00002332
Iteration 112/1000 | Loss: 0.00002332
Iteration 113/1000 | Loss: 0.00002332
Iteration 114/1000 | Loss: 0.00002332
Iteration 115/1000 | Loss: 0.00002332
Iteration 116/1000 | Loss: 0.00002331
Iteration 117/1000 | Loss: 0.00002331
Iteration 118/1000 | Loss: 0.00002331
Iteration 119/1000 | Loss: 0.00002331
Iteration 120/1000 | Loss: 0.00002331
Iteration 121/1000 | Loss: 0.00002331
Iteration 122/1000 | Loss: 0.00002331
Iteration 123/1000 | Loss: 0.00002331
Iteration 124/1000 | Loss: 0.00002331
Iteration 125/1000 | Loss: 0.00002330
Iteration 126/1000 | Loss: 0.00002330
Iteration 127/1000 | Loss: 0.00002330
Iteration 128/1000 | Loss: 0.00002330
Iteration 129/1000 | Loss: 0.00002330
Iteration 130/1000 | Loss: 0.00002330
Iteration 131/1000 | Loss: 0.00002330
Iteration 132/1000 | Loss: 0.00002330
Iteration 133/1000 | Loss: 0.00002330
Iteration 134/1000 | Loss: 0.00002329
Iteration 135/1000 | Loss: 0.00002329
Iteration 136/1000 | Loss: 0.00002329
Iteration 137/1000 | Loss: 0.00002329
Iteration 138/1000 | Loss: 0.00002329
Iteration 139/1000 | Loss: 0.00002329
Iteration 140/1000 | Loss: 0.00002329
Iteration 141/1000 | Loss: 0.00002329
Iteration 142/1000 | Loss: 0.00002329
Iteration 143/1000 | Loss: 0.00002329
Iteration 144/1000 | Loss: 0.00002329
Iteration 145/1000 | Loss: 0.00002329
Iteration 146/1000 | Loss: 0.00002329
Iteration 147/1000 | Loss: 0.00002329
Iteration 148/1000 | Loss: 0.00002329
Iteration 149/1000 | Loss: 0.00002328
Iteration 150/1000 | Loss: 0.00002328
Iteration 151/1000 | Loss: 0.00002328
Iteration 152/1000 | Loss: 0.00002328
Iteration 153/1000 | Loss: 0.00002327
Iteration 154/1000 | Loss: 0.00002327
Iteration 155/1000 | Loss: 0.00002327
Iteration 156/1000 | Loss: 0.00002327
Iteration 157/1000 | Loss: 0.00002327
Iteration 158/1000 | Loss: 0.00002327
Iteration 159/1000 | Loss: 0.00002327
Iteration 160/1000 | Loss: 0.00002327
Iteration 161/1000 | Loss: 0.00002327
Iteration 162/1000 | Loss: 0.00002327
Iteration 163/1000 | Loss: 0.00002327
Iteration 164/1000 | Loss: 0.00002327
Iteration 165/1000 | Loss: 0.00002326
Iteration 166/1000 | Loss: 0.00002326
Iteration 167/1000 | Loss: 0.00002326
Iteration 168/1000 | Loss: 0.00002326
Iteration 169/1000 | Loss: 0.00002326
Iteration 170/1000 | Loss: 0.00002326
Iteration 171/1000 | Loss: 0.00002326
Iteration 172/1000 | Loss: 0.00002326
Iteration 173/1000 | Loss: 0.00002326
Iteration 174/1000 | Loss: 0.00002326
Iteration 175/1000 | Loss: 0.00002326
Iteration 176/1000 | Loss: 0.00002326
Iteration 177/1000 | Loss: 0.00002326
Iteration 178/1000 | Loss: 0.00002326
Iteration 179/1000 | Loss: 0.00002325
Iteration 180/1000 | Loss: 0.00002325
Iteration 181/1000 | Loss: 0.00002325
Iteration 182/1000 | Loss: 0.00002325
Iteration 183/1000 | Loss: 0.00002325
Iteration 184/1000 | Loss: 0.00002325
Iteration 185/1000 | Loss: 0.00002325
Iteration 186/1000 | Loss: 0.00002324
Iteration 187/1000 | Loss: 0.00002324
Iteration 188/1000 | Loss: 0.00002324
Iteration 189/1000 | Loss: 0.00002324
Iteration 190/1000 | Loss: 0.00002324
Iteration 191/1000 | Loss: 0.00002324
Iteration 192/1000 | Loss: 0.00002324
Iteration 193/1000 | Loss: 0.00002324
Iteration 194/1000 | Loss: 0.00002324
Iteration 195/1000 | Loss: 0.00002324
Iteration 196/1000 | Loss: 0.00002324
Iteration 197/1000 | Loss: 0.00002324
Iteration 198/1000 | Loss: 0.00002324
Iteration 199/1000 | Loss: 0.00002324
Iteration 200/1000 | Loss: 0.00002324
Iteration 201/1000 | Loss: 0.00002323
Iteration 202/1000 | Loss: 0.00002323
Iteration 203/1000 | Loss: 0.00002323
Iteration 204/1000 | Loss: 0.00002323
Iteration 205/1000 | Loss: 0.00002323
Iteration 206/1000 | Loss: 0.00002323
Iteration 207/1000 | Loss: 0.00002323
Iteration 208/1000 | Loss: 0.00002323
Iteration 209/1000 | Loss: 0.00002323
Iteration 210/1000 | Loss: 0.00002323
Iteration 211/1000 | Loss: 0.00002323
Iteration 212/1000 | Loss: 0.00002323
Iteration 213/1000 | Loss: 0.00002323
Iteration 214/1000 | Loss: 0.00002323
Iteration 215/1000 | Loss: 0.00002323
Iteration 216/1000 | Loss: 0.00002323
Iteration 217/1000 | Loss: 0.00002323
Iteration 218/1000 | Loss: 0.00002323
Iteration 219/1000 | Loss: 0.00002323
Iteration 220/1000 | Loss: 0.00002323
Iteration 221/1000 | Loss: 0.00002322
Iteration 222/1000 | Loss: 0.00002322
Iteration 223/1000 | Loss: 0.00002322
Iteration 224/1000 | Loss: 0.00002322
Iteration 225/1000 | Loss: 0.00002322
Iteration 226/1000 | Loss: 0.00002322
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 226. Stopping optimization.
Last 5 losses: [2.3224687538458966e-05, 2.3224687538458966e-05, 2.3224687538458966e-05, 2.3224687538458966e-05, 2.3224687538458966e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3224687538458966e-05

Optimization complete. Final v2v error: 3.912757158279419 mm

Highest mean error: 4.757037162780762 mm for frame 209

Lowest mean error: 3.351717472076416 mm for frame 103

Saving results

Total time: 72.78215146064758
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_006/1003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_006/1003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_006/1003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00823299
Iteration 2/25 | Loss: 0.00101738
Iteration 3/25 | Loss: 0.00084240
Iteration 4/25 | Loss: 0.00082440
Iteration 5/25 | Loss: 0.00081714
Iteration 6/25 | Loss: 0.00081515
Iteration 7/25 | Loss: 0.00081489
Iteration 8/25 | Loss: 0.00081489
Iteration 9/25 | Loss: 0.00081489
Iteration 10/25 | Loss: 0.00081489
Iteration 11/25 | Loss: 0.00081489
Iteration 12/25 | Loss: 0.00081489
Iteration 13/25 | Loss: 0.00081489
Iteration 14/25 | Loss: 0.00081489
Iteration 15/25 | Loss: 0.00081489
Iteration 16/25 | Loss: 0.00081489
Iteration 17/25 | Loss: 0.00081489
Iteration 18/25 | Loss: 0.00081489
Iteration 19/25 | Loss: 0.00081489
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0008148872293531895, 0.0008148872293531895, 0.0008148872293531895, 0.0008148872293531895, 0.0008148872293531895]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008148872293531895

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34353554
Iteration 2/25 | Loss: 0.00044603
Iteration 3/25 | Loss: 0.00044600
Iteration 4/25 | Loss: 0.00044600
Iteration 5/25 | Loss: 0.00044600
Iteration 6/25 | Loss: 0.00044600
Iteration 7/25 | Loss: 0.00044600
Iteration 8/25 | Loss: 0.00044600
Iteration 9/25 | Loss: 0.00044600
Iteration 10/25 | Loss: 0.00044600
Iteration 11/25 | Loss: 0.00044600
Iteration 12/25 | Loss: 0.00044600
Iteration 13/25 | Loss: 0.00044600
Iteration 14/25 | Loss: 0.00044600
Iteration 15/25 | Loss: 0.00044600
Iteration 16/25 | Loss: 0.00044600
Iteration 17/25 | Loss: 0.00044600
Iteration 18/25 | Loss: 0.00044600
Iteration 19/25 | Loss: 0.00044600
Iteration 20/25 | Loss: 0.00044600
Iteration 21/25 | Loss: 0.00044600
Iteration 22/25 | Loss: 0.00044600
Iteration 23/25 | Loss: 0.00044600
Iteration 24/25 | Loss: 0.00044600
Iteration 25/25 | Loss: 0.00044600

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00044600
Iteration 2/1000 | Loss: 0.00003171
Iteration 3/1000 | Loss: 0.00002199
Iteration 4/1000 | Loss: 0.00002051
Iteration 5/1000 | Loss: 0.00001959
Iteration 6/1000 | Loss: 0.00001914
Iteration 7/1000 | Loss: 0.00001877
Iteration 8/1000 | Loss: 0.00001855
Iteration 9/1000 | Loss: 0.00001831
Iteration 10/1000 | Loss: 0.00001815
Iteration 11/1000 | Loss: 0.00001814
Iteration 12/1000 | Loss: 0.00001809
Iteration 13/1000 | Loss: 0.00001801
Iteration 14/1000 | Loss: 0.00001796
Iteration 15/1000 | Loss: 0.00001789
Iteration 16/1000 | Loss: 0.00001787
Iteration 17/1000 | Loss: 0.00001787
Iteration 18/1000 | Loss: 0.00001785
Iteration 19/1000 | Loss: 0.00001784
Iteration 20/1000 | Loss: 0.00001784
Iteration 21/1000 | Loss: 0.00001783
Iteration 22/1000 | Loss: 0.00001783
Iteration 23/1000 | Loss: 0.00001783
Iteration 24/1000 | Loss: 0.00001783
Iteration 25/1000 | Loss: 0.00001783
Iteration 26/1000 | Loss: 0.00001783
Iteration 27/1000 | Loss: 0.00001783
Iteration 28/1000 | Loss: 0.00001783
Iteration 29/1000 | Loss: 0.00001783
Iteration 30/1000 | Loss: 0.00001783
Iteration 31/1000 | Loss: 0.00001782
Iteration 32/1000 | Loss: 0.00001782
Iteration 33/1000 | Loss: 0.00001781
Iteration 34/1000 | Loss: 0.00001781
Iteration 35/1000 | Loss: 0.00001780
Iteration 36/1000 | Loss: 0.00001780
Iteration 37/1000 | Loss: 0.00001780
Iteration 38/1000 | Loss: 0.00001780
Iteration 39/1000 | Loss: 0.00001780
Iteration 40/1000 | Loss: 0.00001780
Iteration 41/1000 | Loss: 0.00001779
Iteration 42/1000 | Loss: 0.00001779
Iteration 43/1000 | Loss: 0.00001779
Iteration 44/1000 | Loss: 0.00001778
Iteration 45/1000 | Loss: 0.00001778
Iteration 46/1000 | Loss: 0.00001778
Iteration 47/1000 | Loss: 0.00001778
Iteration 48/1000 | Loss: 0.00001778
Iteration 49/1000 | Loss: 0.00001778
Iteration 50/1000 | Loss: 0.00001777
Iteration 51/1000 | Loss: 0.00001777
Iteration 52/1000 | Loss: 0.00001777
Iteration 53/1000 | Loss: 0.00001777
Iteration 54/1000 | Loss: 0.00001777
Iteration 55/1000 | Loss: 0.00001776
Iteration 56/1000 | Loss: 0.00001776
Iteration 57/1000 | Loss: 0.00001776
Iteration 58/1000 | Loss: 0.00001776
Iteration 59/1000 | Loss: 0.00001776
Iteration 60/1000 | Loss: 0.00001776
Iteration 61/1000 | Loss: 0.00001775
Iteration 62/1000 | Loss: 0.00001775
Iteration 63/1000 | Loss: 0.00001775
Iteration 64/1000 | Loss: 0.00001775
Iteration 65/1000 | Loss: 0.00001775
Iteration 66/1000 | Loss: 0.00001774
Iteration 67/1000 | Loss: 0.00001774
Iteration 68/1000 | Loss: 0.00001774
Iteration 69/1000 | Loss: 0.00001774
Iteration 70/1000 | Loss: 0.00001774
Iteration 71/1000 | Loss: 0.00001773
Iteration 72/1000 | Loss: 0.00001773
Iteration 73/1000 | Loss: 0.00001773
Iteration 74/1000 | Loss: 0.00001773
Iteration 75/1000 | Loss: 0.00001773
Iteration 76/1000 | Loss: 0.00001772
Iteration 77/1000 | Loss: 0.00001772
Iteration 78/1000 | Loss: 0.00001772
Iteration 79/1000 | Loss: 0.00001772
Iteration 80/1000 | Loss: 0.00001772
Iteration 81/1000 | Loss: 0.00001771
Iteration 82/1000 | Loss: 0.00001771
Iteration 83/1000 | Loss: 0.00001771
Iteration 84/1000 | Loss: 0.00001771
Iteration 85/1000 | Loss: 0.00001771
Iteration 86/1000 | Loss: 0.00001771
Iteration 87/1000 | Loss: 0.00001771
Iteration 88/1000 | Loss: 0.00001771
Iteration 89/1000 | Loss: 0.00001770
Iteration 90/1000 | Loss: 0.00001769
Iteration 91/1000 | Loss: 0.00001769
Iteration 92/1000 | Loss: 0.00001769
Iteration 93/1000 | Loss: 0.00001769
Iteration 94/1000 | Loss: 0.00001769
Iteration 95/1000 | Loss: 0.00001769
Iteration 96/1000 | Loss: 0.00001769
Iteration 97/1000 | Loss: 0.00001769
Iteration 98/1000 | Loss: 0.00001768
Iteration 99/1000 | Loss: 0.00001768
Iteration 100/1000 | Loss: 0.00001768
Iteration 101/1000 | Loss: 0.00001768
Iteration 102/1000 | Loss: 0.00001768
Iteration 103/1000 | Loss: 0.00001768
Iteration 104/1000 | Loss: 0.00001768
Iteration 105/1000 | Loss: 0.00001767
Iteration 106/1000 | Loss: 0.00001767
Iteration 107/1000 | Loss: 0.00001767
Iteration 108/1000 | Loss: 0.00001767
Iteration 109/1000 | Loss: 0.00001767
Iteration 110/1000 | Loss: 0.00001767
Iteration 111/1000 | Loss: 0.00001767
Iteration 112/1000 | Loss: 0.00001766
Iteration 113/1000 | Loss: 0.00001766
Iteration 114/1000 | Loss: 0.00001766
Iteration 115/1000 | Loss: 0.00001766
Iteration 116/1000 | Loss: 0.00001766
Iteration 117/1000 | Loss: 0.00001765
Iteration 118/1000 | Loss: 0.00001765
Iteration 119/1000 | Loss: 0.00001765
Iteration 120/1000 | Loss: 0.00001765
Iteration 121/1000 | Loss: 0.00001765
Iteration 122/1000 | Loss: 0.00001765
Iteration 123/1000 | Loss: 0.00001765
Iteration 124/1000 | Loss: 0.00001765
Iteration 125/1000 | Loss: 0.00001764
Iteration 126/1000 | Loss: 0.00001764
Iteration 127/1000 | Loss: 0.00001764
Iteration 128/1000 | Loss: 0.00001764
Iteration 129/1000 | Loss: 0.00001764
Iteration 130/1000 | Loss: 0.00001764
Iteration 131/1000 | Loss: 0.00001763
Iteration 132/1000 | Loss: 0.00001763
Iteration 133/1000 | Loss: 0.00001763
Iteration 134/1000 | Loss: 0.00001763
Iteration 135/1000 | Loss: 0.00001763
Iteration 136/1000 | Loss: 0.00001763
Iteration 137/1000 | Loss: 0.00001762
Iteration 138/1000 | Loss: 0.00001762
Iteration 139/1000 | Loss: 0.00001762
Iteration 140/1000 | Loss: 0.00001762
Iteration 141/1000 | Loss: 0.00001762
Iteration 142/1000 | Loss: 0.00001762
Iteration 143/1000 | Loss: 0.00001762
Iteration 144/1000 | Loss: 0.00001762
Iteration 145/1000 | Loss: 0.00001762
Iteration 146/1000 | Loss: 0.00001762
Iteration 147/1000 | Loss: 0.00001762
Iteration 148/1000 | Loss: 0.00001761
Iteration 149/1000 | Loss: 0.00001761
Iteration 150/1000 | Loss: 0.00001761
Iteration 151/1000 | Loss: 0.00001761
Iteration 152/1000 | Loss: 0.00001761
Iteration 153/1000 | Loss: 0.00001761
Iteration 154/1000 | Loss: 0.00001761
Iteration 155/1000 | Loss: 0.00001761
Iteration 156/1000 | Loss: 0.00001761
Iteration 157/1000 | Loss: 0.00001761
Iteration 158/1000 | Loss: 0.00001761
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 158. Stopping optimization.
Last 5 losses: [1.761039311531931e-05, 1.761039311531931e-05, 1.761039311531931e-05, 1.761039311531931e-05, 1.761039311531931e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.761039311531931e-05

Optimization complete. Final v2v error: 3.5520341396331787 mm

Highest mean error: 4.402572154998779 mm for frame 53

Lowest mean error: 3.155644655227661 mm for frame 17

Saving results

Total time: 41.974132776260376
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_006/1072/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_006/1072.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_006/1072
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01037936
Iteration 2/25 | Loss: 0.00156903
Iteration 3/25 | Loss: 0.00103127
Iteration 4/25 | Loss: 0.00095768
Iteration 5/25 | Loss: 0.00094155
Iteration 6/25 | Loss: 0.00092731
Iteration 7/25 | Loss: 0.00090530
Iteration 8/25 | Loss: 0.00087209
Iteration 9/25 | Loss: 0.00086023
Iteration 10/25 | Loss: 0.00085776
Iteration 11/25 | Loss: 0.00084288
Iteration 12/25 | Loss: 0.00085178
Iteration 13/25 | Loss: 0.00083946
Iteration 14/25 | Loss: 0.00084115
Iteration 15/25 | Loss: 0.00083858
Iteration 16/25 | Loss: 0.00083639
Iteration 17/25 | Loss: 0.00082431
Iteration 18/25 | Loss: 0.00081989
Iteration 19/25 | Loss: 0.00081761
Iteration 20/25 | Loss: 0.00081469
Iteration 21/25 | Loss: 0.00081595
Iteration 22/25 | Loss: 0.00081055
Iteration 23/25 | Loss: 0.00080878
Iteration 24/25 | Loss: 0.00080666
Iteration 25/25 | Loss: 0.00080694

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.48014522
Iteration 2/25 | Loss: 0.00093148
Iteration 3/25 | Loss: 0.00075642
Iteration 4/25 | Loss: 0.00075642
Iteration 5/25 | Loss: 0.00075642
Iteration 6/25 | Loss: 0.00075642
Iteration 7/25 | Loss: 0.00075642
Iteration 8/25 | Loss: 0.00075642
Iteration 9/25 | Loss: 0.00075642
Iteration 10/25 | Loss: 0.00075642
Iteration 11/25 | Loss: 0.00075642
Iteration 12/25 | Loss: 0.00075642
Iteration 13/25 | Loss: 0.00075642
Iteration 14/25 | Loss: 0.00075642
Iteration 15/25 | Loss: 0.00075642
Iteration 16/25 | Loss: 0.00075642
Iteration 17/25 | Loss: 0.00075642
Iteration 18/25 | Loss: 0.00075642
Iteration 19/25 | Loss: 0.00075642
Iteration 20/25 | Loss: 0.00075642
Iteration 21/25 | Loss: 0.00075642
Iteration 22/25 | Loss: 0.00075642
Iteration 23/25 | Loss: 0.00075642
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0007564153056591749, 0.0007564153056591749, 0.0007564153056591749, 0.0007564153056591749, 0.0007564153056591749]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007564153056591749

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00075642
Iteration 2/1000 | Loss: 0.00079679
Iteration 3/1000 | Loss: 0.00091710
Iteration 4/1000 | Loss: 0.00032208
Iteration 5/1000 | Loss: 0.00030669
Iteration 6/1000 | Loss: 0.00006796
Iteration 7/1000 | Loss: 0.00033831
Iteration 8/1000 | Loss: 0.00067557
Iteration 9/1000 | Loss: 0.00072901
Iteration 10/1000 | Loss: 0.00022985
Iteration 11/1000 | Loss: 0.00028553
Iteration 12/1000 | Loss: 0.00005592
Iteration 13/1000 | Loss: 0.00010208
Iteration 14/1000 | Loss: 0.00013505
Iteration 15/1000 | Loss: 0.00003974
Iteration 16/1000 | Loss: 0.00050461
Iteration 17/1000 | Loss: 0.00070442
Iteration 18/1000 | Loss: 0.00005761
Iteration 19/1000 | Loss: 0.00003652
Iteration 20/1000 | Loss: 0.00023608
Iteration 21/1000 | Loss: 0.00003158
Iteration 22/1000 | Loss: 0.00002717
Iteration 23/1000 | Loss: 0.00006939
Iteration 24/1000 | Loss: 0.00003097
Iteration 25/1000 | Loss: 0.00015788
Iteration 26/1000 | Loss: 0.00061575
Iteration 27/1000 | Loss: 0.00006847
Iteration 28/1000 | Loss: 0.00028203
Iteration 29/1000 | Loss: 0.00046106
Iteration 30/1000 | Loss: 0.00005652
Iteration 31/1000 | Loss: 0.00015949
Iteration 32/1000 | Loss: 0.00007293
Iteration 33/1000 | Loss: 0.00005117
Iteration 34/1000 | Loss: 0.00017593
Iteration 35/1000 | Loss: 0.00046314
Iteration 36/1000 | Loss: 0.00002776
Iteration 37/1000 | Loss: 0.00016921
Iteration 38/1000 | Loss: 0.00025349
Iteration 39/1000 | Loss: 0.00020678
Iteration 40/1000 | Loss: 0.00019799
Iteration 41/1000 | Loss: 0.00025977
Iteration 42/1000 | Loss: 0.00020803
Iteration 43/1000 | Loss: 0.00002041
Iteration 44/1000 | Loss: 0.00031346
Iteration 45/1000 | Loss: 0.00020490
Iteration 46/1000 | Loss: 0.00014664
Iteration 47/1000 | Loss: 0.00031483
Iteration 48/1000 | Loss: 0.00032402
Iteration 49/1000 | Loss: 0.00032656
Iteration 50/1000 | Loss: 0.00036712
Iteration 51/1000 | Loss: 0.00031634
Iteration 52/1000 | Loss: 0.00019370
Iteration 53/1000 | Loss: 0.00008894
Iteration 54/1000 | Loss: 0.00009605
Iteration 55/1000 | Loss: 0.00006088
Iteration 56/1000 | Loss: 0.00006326
Iteration 57/1000 | Loss: 0.00002330
Iteration 58/1000 | Loss: 0.00002127
Iteration 59/1000 | Loss: 0.00019020
Iteration 60/1000 | Loss: 0.00029265
Iteration 61/1000 | Loss: 0.00002041
Iteration 62/1000 | Loss: 0.00019186
Iteration 63/1000 | Loss: 0.00003386
Iteration 64/1000 | Loss: 0.00004804
Iteration 65/1000 | Loss: 0.00003083
Iteration 66/1000 | Loss: 0.00003130
Iteration 67/1000 | Loss: 0.00003664
Iteration 68/1000 | Loss: 0.00003043
Iteration 69/1000 | Loss: 0.00003267
Iteration 70/1000 | Loss: 0.00003039
Iteration 71/1000 | Loss: 0.00003329
Iteration 72/1000 | Loss: 0.00003236
Iteration 73/1000 | Loss: 0.00003040
Iteration 74/1000 | Loss: 0.00003162
Iteration 75/1000 | Loss: 0.00003886
Iteration 76/1000 | Loss: 0.00026525
Iteration 77/1000 | Loss: 0.00021332
Iteration 78/1000 | Loss: 0.00024753
Iteration 79/1000 | Loss: 0.00048441
Iteration 80/1000 | Loss: 0.00020234
Iteration 81/1000 | Loss: 0.00016799
Iteration 82/1000 | Loss: 0.00011492
Iteration 83/1000 | Loss: 0.00002257
Iteration 84/1000 | Loss: 0.00001968
Iteration 85/1000 | Loss: 0.00002332
Iteration 86/1000 | Loss: 0.00001713
Iteration 87/1000 | Loss: 0.00001703
Iteration 88/1000 | Loss: 0.00001715
Iteration 89/1000 | Loss: 0.00001591
Iteration 90/1000 | Loss: 0.00001797
Iteration 91/1000 | Loss: 0.00001579
Iteration 92/1000 | Loss: 0.00001578
Iteration 93/1000 | Loss: 0.00001577
Iteration 94/1000 | Loss: 0.00001551
Iteration 95/1000 | Loss: 0.00005279
Iteration 96/1000 | Loss: 0.00001646
Iteration 97/1000 | Loss: 0.00001563
Iteration 98/1000 | Loss: 0.00001910
Iteration 99/1000 | Loss: 0.00001492
Iteration 100/1000 | Loss: 0.00001955
Iteration 101/1000 | Loss: 0.00001483
Iteration 102/1000 | Loss: 0.00001477
Iteration 103/1000 | Loss: 0.00001471
Iteration 104/1000 | Loss: 0.00002554
Iteration 105/1000 | Loss: 0.00001458
Iteration 106/1000 | Loss: 0.00001454
Iteration 107/1000 | Loss: 0.00001454
Iteration 108/1000 | Loss: 0.00001454
Iteration 109/1000 | Loss: 0.00001454
Iteration 110/1000 | Loss: 0.00001454
Iteration 111/1000 | Loss: 0.00001454
Iteration 112/1000 | Loss: 0.00001454
Iteration 113/1000 | Loss: 0.00001454
Iteration 114/1000 | Loss: 0.00001453
Iteration 115/1000 | Loss: 0.00001453
Iteration 116/1000 | Loss: 0.00001453
Iteration 117/1000 | Loss: 0.00001453
Iteration 118/1000 | Loss: 0.00001453
Iteration 119/1000 | Loss: 0.00001453
Iteration 120/1000 | Loss: 0.00001453
Iteration 121/1000 | Loss: 0.00001453
Iteration 122/1000 | Loss: 0.00001452
Iteration 123/1000 | Loss: 0.00001452
Iteration 124/1000 | Loss: 0.00001693
Iteration 125/1000 | Loss: 0.00001452
Iteration 126/1000 | Loss: 0.00001452
Iteration 127/1000 | Loss: 0.00001452
Iteration 128/1000 | Loss: 0.00001452
Iteration 129/1000 | Loss: 0.00001452
Iteration 130/1000 | Loss: 0.00001452
Iteration 131/1000 | Loss: 0.00001451
Iteration 132/1000 | Loss: 0.00001451
Iteration 133/1000 | Loss: 0.00001450
Iteration 134/1000 | Loss: 0.00001450
Iteration 135/1000 | Loss: 0.00001450
Iteration 136/1000 | Loss: 0.00001449
Iteration 137/1000 | Loss: 0.00001449
Iteration 138/1000 | Loss: 0.00001448
Iteration 139/1000 | Loss: 0.00001448
Iteration 140/1000 | Loss: 0.00001448
Iteration 141/1000 | Loss: 0.00001448
Iteration 142/1000 | Loss: 0.00001447
Iteration 143/1000 | Loss: 0.00001447
Iteration 144/1000 | Loss: 0.00001447
Iteration 145/1000 | Loss: 0.00001447
Iteration 146/1000 | Loss: 0.00001447
Iteration 147/1000 | Loss: 0.00001446
Iteration 148/1000 | Loss: 0.00001446
Iteration 149/1000 | Loss: 0.00001446
Iteration 150/1000 | Loss: 0.00001446
Iteration 151/1000 | Loss: 0.00001446
Iteration 152/1000 | Loss: 0.00001446
Iteration 153/1000 | Loss: 0.00001446
Iteration 154/1000 | Loss: 0.00001446
Iteration 155/1000 | Loss: 0.00001446
Iteration 156/1000 | Loss: 0.00001446
Iteration 157/1000 | Loss: 0.00001446
Iteration 158/1000 | Loss: 0.00001446
Iteration 159/1000 | Loss: 0.00001446
Iteration 160/1000 | Loss: 0.00001445
Iteration 161/1000 | Loss: 0.00001445
Iteration 162/1000 | Loss: 0.00001445
Iteration 163/1000 | Loss: 0.00001445
Iteration 164/1000 | Loss: 0.00001445
Iteration 165/1000 | Loss: 0.00001445
Iteration 166/1000 | Loss: 0.00001445
Iteration 167/1000 | Loss: 0.00001445
Iteration 168/1000 | Loss: 0.00001445
Iteration 169/1000 | Loss: 0.00001444
Iteration 170/1000 | Loss: 0.00001444
Iteration 171/1000 | Loss: 0.00001444
Iteration 172/1000 | Loss: 0.00001444
Iteration 173/1000 | Loss: 0.00001444
Iteration 174/1000 | Loss: 0.00001444
Iteration 175/1000 | Loss: 0.00001444
Iteration 176/1000 | Loss: 0.00001444
Iteration 177/1000 | Loss: 0.00001444
Iteration 178/1000 | Loss: 0.00001444
Iteration 179/1000 | Loss: 0.00001444
Iteration 180/1000 | Loss: 0.00001444
Iteration 181/1000 | Loss: 0.00001444
Iteration 182/1000 | Loss: 0.00001444
Iteration 183/1000 | Loss: 0.00001444
Iteration 184/1000 | Loss: 0.00001444
Iteration 185/1000 | Loss: 0.00001444
Iteration 186/1000 | Loss: 0.00001444
Iteration 187/1000 | Loss: 0.00001444
Iteration 188/1000 | Loss: 0.00001444
Iteration 189/1000 | Loss: 0.00001443
Iteration 190/1000 | Loss: 0.00001443
Iteration 191/1000 | Loss: 0.00001443
Iteration 192/1000 | Loss: 0.00001443
Iteration 193/1000 | Loss: 0.00001443
Iteration 194/1000 | Loss: 0.00001443
Iteration 195/1000 | Loss: 0.00001443
Iteration 196/1000 | Loss: 0.00001443
Iteration 197/1000 | Loss: 0.00001443
Iteration 198/1000 | Loss: 0.00001443
Iteration 199/1000 | Loss: 0.00001443
Iteration 200/1000 | Loss: 0.00001443
Iteration 201/1000 | Loss: 0.00001443
Iteration 202/1000 | Loss: 0.00001443
Iteration 203/1000 | Loss: 0.00001443
Iteration 204/1000 | Loss: 0.00001710
Iteration 205/1000 | Loss: 0.00001499
Iteration 206/1000 | Loss: 0.00001540
Iteration 207/1000 | Loss: 0.00001485
Iteration 208/1000 | Loss: 0.00001445
Iteration 209/1000 | Loss: 0.00001444
Iteration 210/1000 | Loss: 0.00001444
Iteration 211/1000 | Loss: 0.00001444
Iteration 212/1000 | Loss: 0.00001447
Iteration 213/1000 | Loss: 0.00001452
Iteration 214/1000 | Loss: 0.00001440
Iteration 215/1000 | Loss: 0.00001440
Iteration 216/1000 | Loss: 0.00001440
Iteration 217/1000 | Loss: 0.00001440
Iteration 218/1000 | Loss: 0.00001440
Iteration 219/1000 | Loss: 0.00001440
Iteration 220/1000 | Loss: 0.00001440
Iteration 221/1000 | Loss: 0.00001440
Iteration 222/1000 | Loss: 0.00001440
Iteration 223/1000 | Loss: 0.00001440
Iteration 224/1000 | Loss: 0.00001440
Iteration 225/1000 | Loss: 0.00001440
Iteration 226/1000 | Loss: 0.00001440
Iteration 227/1000 | Loss: 0.00001440
Iteration 228/1000 | Loss: 0.00001440
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 228. Stopping optimization.
Last 5 losses: [1.4402472515939735e-05, 1.4402472515939735e-05, 1.4402472515939735e-05, 1.4402472515939735e-05, 1.4402472515939735e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4402472515939735e-05

Optimization complete. Final v2v error: 3.1554219722747803 mm

Highest mean error: 6.172360897064209 mm for frame 41

Lowest mean error: 2.889613628387451 mm for frame 213

Saving results

Total time: 227.46828603744507
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_006/1030/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_006/1030.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_006/1030
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00565377
Iteration 2/25 | Loss: 0.00105290
Iteration 3/25 | Loss: 0.00091547
Iteration 4/25 | Loss: 0.00089152
Iteration 5/25 | Loss: 0.00088313
Iteration 6/25 | Loss: 0.00088099
Iteration 7/25 | Loss: 0.00088092
Iteration 8/25 | Loss: 0.00088092
Iteration 9/25 | Loss: 0.00088092
Iteration 10/25 | Loss: 0.00088092
Iteration 11/25 | Loss: 0.00088092
Iteration 12/25 | Loss: 0.00088092
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0008809211431071162, 0.0008809211431071162, 0.0008809211431071162, 0.0008809211431071162, 0.0008809211431071162]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008809211431071162

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.53069305
Iteration 2/25 | Loss: 0.00060685
Iteration 3/25 | Loss: 0.00060682
Iteration 4/25 | Loss: 0.00060682
Iteration 5/25 | Loss: 0.00060682
Iteration 6/25 | Loss: 0.00060682
Iteration 7/25 | Loss: 0.00060682
Iteration 8/25 | Loss: 0.00060682
Iteration 9/25 | Loss: 0.00060682
Iteration 10/25 | Loss: 0.00060682
Iteration 11/25 | Loss: 0.00060682
Iteration 12/25 | Loss: 0.00060682
Iteration 13/25 | Loss: 0.00060682
Iteration 14/25 | Loss: 0.00060682
Iteration 15/25 | Loss: 0.00060682
Iteration 16/25 | Loss: 0.00060682
Iteration 17/25 | Loss: 0.00060682
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0006068214424885809, 0.0006068214424885809, 0.0006068214424885809, 0.0006068214424885809, 0.0006068214424885809]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006068214424885809

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00060682
Iteration 2/1000 | Loss: 0.00003812
Iteration 3/1000 | Loss: 0.00002587
Iteration 4/1000 | Loss: 0.00002365
Iteration 5/1000 | Loss: 0.00002283
Iteration 6/1000 | Loss: 0.00002233
Iteration 7/1000 | Loss: 0.00002177
Iteration 8/1000 | Loss: 0.00002135
Iteration 9/1000 | Loss: 0.00002111
Iteration 10/1000 | Loss: 0.00002094
Iteration 11/1000 | Loss: 0.00002094
Iteration 12/1000 | Loss: 0.00002093
Iteration 13/1000 | Loss: 0.00002093
Iteration 14/1000 | Loss: 0.00002093
Iteration 15/1000 | Loss: 0.00002092
Iteration 16/1000 | Loss: 0.00002086
Iteration 17/1000 | Loss: 0.00002085
Iteration 18/1000 | Loss: 0.00002082
Iteration 19/1000 | Loss: 0.00002079
Iteration 20/1000 | Loss: 0.00002079
Iteration 21/1000 | Loss: 0.00002078
Iteration 22/1000 | Loss: 0.00002075
Iteration 23/1000 | Loss: 0.00002074
Iteration 24/1000 | Loss: 0.00002073
Iteration 25/1000 | Loss: 0.00002072
Iteration 26/1000 | Loss: 0.00002072
Iteration 27/1000 | Loss: 0.00002071
Iteration 28/1000 | Loss: 0.00002071
Iteration 29/1000 | Loss: 0.00002070
Iteration 30/1000 | Loss: 0.00002067
Iteration 31/1000 | Loss: 0.00002066
Iteration 32/1000 | Loss: 0.00002065
Iteration 33/1000 | Loss: 0.00002065
Iteration 34/1000 | Loss: 0.00002064
Iteration 35/1000 | Loss: 0.00002064
Iteration 36/1000 | Loss: 0.00002063
Iteration 37/1000 | Loss: 0.00002063
Iteration 38/1000 | Loss: 0.00002062
Iteration 39/1000 | Loss: 0.00002061
Iteration 40/1000 | Loss: 0.00002061
Iteration 41/1000 | Loss: 0.00002061
Iteration 42/1000 | Loss: 0.00002061
Iteration 43/1000 | Loss: 0.00002060
Iteration 44/1000 | Loss: 0.00002060
Iteration 45/1000 | Loss: 0.00002060
Iteration 46/1000 | Loss: 0.00002060
Iteration 47/1000 | Loss: 0.00002060
Iteration 48/1000 | Loss: 0.00002060
Iteration 49/1000 | Loss: 0.00002060
Iteration 50/1000 | Loss: 0.00002060
Iteration 51/1000 | Loss: 0.00002060
Iteration 52/1000 | Loss: 0.00002060
Iteration 53/1000 | Loss: 0.00002060
Iteration 54/1000 | Loss: 0.00002060
Iteration 55/1000 | Loss: 0.00002060
Iteration 56/1000 | Loss: 0.00002060
Iteration 57/1000 | Loss: 0.00002060
Iteration 58/1000 | Loss: 0.00002060
Iteration 59/1000 | Loss: 0.00002060
Iteration 60/1000 | Loss: 0.00002060
Iteration 61/1000 | Loss: 0.00002060
Iteration 62/1000 | Loss: 0.00002060
Iteration 63/1000 | Loss: 0.00002060
Iteration 64/1000 | Loss: 0.00002060
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 64. Stopping optimization.
Last 5 losses: [2.0604000383173116e-05, 2.0604000383173116e-05, 2.0604000383173116e-05, 2.0604000383173116e-05, 2.0604000383173116e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0604000383173116e-05

Optimization complete. Final v2v error: 3.6903789043426514 mm

Highest mean error: 4.017560005187988 mm for frame 166

Lowest mean error: 3.3526275157928467 mm for frame 54

Saving results

Total time: 32.63065528869629
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_006/1061/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_006/1061.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_006/1061
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00465200
Iteration 2/25 | Loss: 0.00098730
Iteration 3/25 | Loss: 0.00084953
Iteration 4/25 | Loss: 0.00081605
Iteration 5/25 | Loss: 0.00081077
Iteration 6/25 | Loss: 0.00080846
Iteration 7/25 | Loss: 0.00080838
Iteration 8/25 | Loss: 0.00080838
Iteration 9/25 | Loss: 0.00080838
Iteration 10/25 | Loss: 0.00080838
Iteration 11/25 | Loss: 0.00080838
Iteration 12/25 | Loss: 0.00080838
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0008083791472017765, 0.0008083791472017765, 0.0008083791472017765, 0.0008083791472017765, 0.0008083791472017765]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008083791472017765

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.89052820
Iteration 2/25 | Loss: 0.00048319
Iteration 3/25 | Loss: 0.00048318
Iteration 4/25 | Loss: 0.00048318
Iteration 5/25 | Loss: 0.00048318
Iteration 6/25 | Loss: 0.00048318
Iteration 7/25 | Loss: 0.00048318
Iteration 8/25 | Loss: 0.00048318
Iteration 9/25 | Loss: 0.00048318
Iteration 10/25 | Loss: 0.00048318
Iteration 11/25 | Loss: 0.00048318
Iteration 12/25 | Loss: 0.00048318
Iteration 13/25 | Loss: 0.00048318
Iteration 14/25 | Loss: 0.00048318
Iteration 15/25 | Loss: 0.00048318
Iteration 16/25 | Loss: 0.00048318
Iteration 17/25 | Loss: 0.00048318
Iteration 18/25 | Loss: 0.00048318
Iteration 19/25 | Loss: 0.00048318
Iteration 20/25 | Loss: 0.00048318
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0004831754486076534, 0.0004831754486076534, 0.0004831754486076534, 0.0004831754486076534, 0.0004831754486076534]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0004831754486076534

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00048318
Iteration 2/1000 | Loss: 0.00003597
Iteration 3/1000 | Loss: 0.00002662
Iteration 4/1000 | Loss: 0.00002499
Iteration 5/1000 | Loss: 0.00002391
Iteration 6/1000 | Loss: 0.00002324
Iteration 7/1000 | Loss: 0.00002277
Iteration 8/1000 | Loss: 0.00002243
Iteration 9/1000 | Loss: 0.00002218
Iteration 10/1000 | Loss: 0.00002214
Iteration 11/1000 | Loss: 0.00002197
Iteration 12/1000 | Loss: 0.00002186
Iteration 13/1000 | Loss: 0.00002181
Iteration 14/1000 | Loss: 0.00002180
Iteration 15/1000 | Loss: 0.00002179
Iteration 16/1000 | Loss: 0.00002179
Iteration 17/1000 | Loss: 0.00002176
Iteration 18/1000 | Loss: 0.00002173
Iteration 19/1000 | Loss: 0.00002171
Iteration 20/1000 | Loss: 0.00002171
Iteration 21/1000 | Loss: 0.00002170
Iteration 22/1000 | Loss: 0.00002167
Iteration 23/1000 | Loss: 0.00002167
Iteration 24/1000 | Loss: 0.00002167
Iteration 25/1000 | Loss: 0.00002167
Iteration 26/1000 | Loss: 0.00002167
Iteration 27/1000 | Loss: 0.00002166
Iteration 28/1000 | Loss: 0.00002164
Iteration 29/1000 | Loss: 0.00002164
Iteration 30/1000 | Loss: 0.00002164
Iteration 31/1000 | Loss: 0.00002163
Iteration 32/1000 | Loss: 0.00002163
Iteration 33/1000 | Loss: 0.00002162
Iteration 34/1000 | Loss: 0.00002162
Iteration 35/1000 | Loss: 0.00002162
Iteration 36/1000 | Loss: 0.00002161
Iteration 37/1000 | Loss: 0.00002161
Iteration 38/1000 | Loss: 0.00002161
Iteration 39/1000 | Loss: 0.00002160
Iteration 40/1000 | Loss: 0.00002160
Iteration 41/1000 | Loss: 0.00002159
Iteration 42/1000 | Loss: 0.00002159
Iteration 43/1000 | Loss: 0.00002158
Iteration 44/1000 | Loss: 0.00002158
Iteration 45/1000 | Loss: 0.00002157
Iteration 46/1000 | Loss: 0.00002157
Iteration 47/1000 | Loss: 0.00002157
Iteration 48/1000 | Loss: 0.00002157
Iteration 49/1000 | Loss: 0.00002157
Iteration 50/1000 | Loss: 0.00002157
Iteration 51/1000 | Loss: 0.00002157
Iteration 52/1000 | Loss: 0.00002157
Iteration 53/1000 | Loss: 0.00002156
Iteration 54/1000 | Loss: 0.00002156
Iteration 55/1000 | Loss: 0.00002156
Iteration 56/1000 | Loss: 0.00002155
Iteration 57/1000 | Loss: 0.00002155
Iteration 58/1000 | Loss: 0.00002153
Iteration 59/1000 | Loss: 0.00002153
Iteration 60/1000 | Loss: 0.00002152
Iteration 61/1000 | Loss: 0.00002152
Iteration 62/1000 | Loss: 0.00002152
Iteration 63/1000 | Loss: 0.00002152
Iteration 64/1000 | Loss: 0.00002152
Iteration 65/1000 | Loss: 0.00002152
Iteration 66/1000 | Loss: 0.00002152
Iteration 67/1000 | Loss: 0.00002151
Iteration 68/1000 | Loss: 0.00002151
Iteration 69/1000 | Loss: 0.00002150
Iteration 70/1000 | Loss: 0.00002150
Iteration 71/1000 | Loss: 0.00002149
Iteration 72/1000 | Loss: 0.00002149
Iteration 73/1000 | Loss: 0.00002148
Iteration 74/1000 | Loss: 0.00002148
Iteration 75/1000 | Loss: 0.00002148
Iteration 76/1000 | Loss: 0.00002148
Iteration 77/1000 | Loss: 0.00002148
Iteration 78/1000 | Loss: 0.00002148
Iteration 79/1000 | Loss: 0.00002148
Iteration 80/1000 | Loss: 0.00002147
Iteration 81/1000 | Loss: 0.00002147
Iteration 82/1000 | Loss: 0.00002146
Iteration 83/1000 | Loss: 0.00002146
Iteration 84/1000 | Loss: 0.00002146
Iteration 85/1000 | Loss: 0.00002146
Iteration 86/1000 | Loss: 0.00002146
Iteration 87/1000 | Loss: 0.00002146
Iteration 88/1000 | Loss: 0.00002146
Iteration 89/1000 | Loss: 0.00002146
Iteration 90/1000 | Loss: 0.00002145
Iteration 91/1000 | Loss: 0.00002145
Iteration 92/1000 | Loss: 0.00002145
Iteration 93/1000 | Loss: 0.00002145
Iteration 94/1000 | Loss: 0.00002145
Iteration 95/1000 | Loss: 0.00002145
Iteration 96/1000 | Loss: 0.00002145
Iteration 97/1000 | Loss: 0.00002145
Iteration 98/1000 | Loss: 0.00002145
Iteration 99/1000 | Loss: 0.00002144
Iteration 100/1000 | Loss: 0.00002144
Iteration 101/1000 | Loss: 0.00002144
Iteration 102/1000 | Loss: 0.00002144
Iteration 103/1000 | Loss: 0.00002143
Iteration 104/1000 | Loss: 0.00002143
Iteration 105/1000 | Loss: 0.00002143
Iteration 106/1000 | Loss: 0.00002143
Iteration 107/1000 | Loss: 0.00002143
Iteration 108/1000 | Loss: 0.00002143
Iteration 109/1000 | Loss: 0.00002143
Iteration 110/1000 | Loss: 0.00002143
Iteration 111/1000 | Loss: 0.00002143
Iteration 112/1000 | Loss: 0.00002143
Iteration 113/1000 | Loss: 0.00002143
Iteration 114/1000 | Loss: 0.00002142
Iteration 115/1000 | Loss: 0.00002142
Iteration 116/1000 | Loss: 0.00002142
Iteration 117/1000 | Loss: 0.00002142
Iteration 118/1000 | Loss: 0.00002142
Iteration 119/1000 | Loss: 0.00002142
Iteration 120/1000 | Loss: 0.00002142
Iteration 121/1000 | Loss: 0.00002142
Iteration 122/1000 | Loss: 0.00002142
Iteration 123/1000 | Loss: 0.00002142
Iteration 124/1000 | Loss: 0.00002142
Iteration 125/1000 | Loss: 0.00002142
Iteration 126/1000 | Loss: 0.00002142
Iteration 127/1000 | Loss: 0.00002142
Iteration 128/1000 | Loss: 0.00002142
Iteration 129/1000 | Loss: 0.00002141
Iteration 130/1000 | Loss: 0.00002141
Iteration 131/1000 | Loss: 0.00002141
Iteration 132/1000 | Loss: 0.00002141
Iteration 133/1000 | Loss: 0.00002141
Iteration 134/1000 | Loss: 0.00002141
Iteration 135/1000 | Loss: 0.00002141
Iteration 136/1000 | Loss: 0.00002141
Iteration 137/1000 | Loss: 0.00002141
Iteration 138/1000 | Loss: 0.00002141
Iteration 139/1000 | Loss: 0.00002141
Iteration 140/1000 | Loss: 0.00002141
Iteration 141/1000 | Loss: 0.00002141
Iteration 142/1000 | Loss: 0.00002140
Iteration 143/1000 | Loss: 0.00002140
Iteration 144/1000 | Loss: 0.00002140
Iteration 145/1000 | Loss: 0.00002140
Iteration 146/1000 | Loss: 0.00002140
Iteration 147/1000 | Loss: 0.00002140
Iteration 148/1000 | Loss: 0.00002140
Iteration 149/1000 | Loss: 0.00002140
Iteration 150/1000 | Loss: 0.00002139
Iteration 151/1000 | Loss: 0.00002139
Iteration 152/1000 | Loss: 0.00002139
Iteration 153/1000 | Loss: 0.00002139
Iteration 154/1000 | Loss: 0.00002139
Iteration 155/1000 | Loss: 0.00002139
Iteration 156/1000 | Loss: 0.00002139
Iteration 157/1000 | Loss: 0.00002139
Iteration 158/1000 | Loss: 0.00002139
Iteration 159/1000 | Loss: 0.00002139
Iteration 160/1000 | Loss: 0.00002139
Iteration 161/1000 | Loss: 0.00002139
Iteration 162/1000 | Loss: 0.00002139
Iteration 163/1000 | Loss: 0.00002139
Iteration 164/1000 | Loss: 0.00002138
Iteration 165/1000 | Loss: 0.00002138
Iteration 166/1000 | Loss: 0.00002138
Iteration 167/1000 | Loss: 0.00002138
Iteration 168/1000 | Loss: 0.00002137
Iteration 169/1000 | Loss: 0.00002137
Iteration 170/1000 | Loss: 0.00002137
Iteration 171/1000 | Loss: 0.00002137
Iteration 172/1000 | Loss: 0.00002137
Iteration 173/1000 | Loss: 0.00002137
Iteration 174/1000 | Loss: 0.00002137
Iteration 175/1000 | Loss: 0.00002137
Iteration 176/1000 | Loss: 0.00002137
Iteration 177/1000 | Loss: 0.00002137
Iteration 178/1000 | Loss: 0.00002137
Iteration 179/1000 | Loss: 0.00002137
Iteration 180/1000 | Loss: 0.00002137
Iteration 181/1000 | Loss: 0.00002136
Iteration 182/1000 | Loss: 0.00002136
Iteration 183/1000 | Loss: 0.00002136
Iteration 184/1000 | Loss: 0.00002136
Iteration 185/1000 | Loss: 0.00002136
Iteration 186/1000 | Loss: 0.00002136
Iteration 187/1000 | Loss: 0.00002136
Iteration 188/1000 | Loss: 0.00002136
Iteration 189/1000 | Loss: 0.00002136
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 189. Stopping optimization.
Last 5 losses: [2.1359153834055178e-05, 2.1359153834055178e-05, 2.1359153834055178e-05, 2.1359153834055178e-05, 2.1359153834055178e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1359153834055178e-05

Optimization complete. Final v2v error: 3.8585867881774902 mm

Highest mean error: 4.374680042266846 mm for frame 227

Lowest mean error: 3.480719566345215 mm for frame 71

Saving results

Total time: 45.020214557647705
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_006/1082/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_006/1082.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_006/1082
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01062339
Iteration 2/25 | Loss: 0.00237372
Iteration 3/25 | Loss: 0.00167978
Iteration 4/25 | Loss: 0.00152510
Iteration 5/25 | Loss: 0.00140460
Iteration 6/25 | Loss: 0.00124139
Iteration 7/25 | Loss: 0.00119880
Iteration 8/25 | Loss: 0.00114304
Iteration 9/25 | Loss: 0.00111964
Iteration 10/25 | Loss: 0.00109749
Iteration 11/25 | Loss: 0.00108108
Iteration 12/25 | Loss: 0.00107267
Iteration 13/25 | Loss: 0.00106463
Iteration 14/25 | Loss: 0.00105626
Iteration 15/25 | Loss: 0.00105477
Iteration 16/25 | Loss: 0.00105647
Iteration 17/25 | Loss: 0.00105113
Iteration 18/25 | Loss: 0.00104100
Iteration 19/25 | Loss: 0.00104362
Iteration 20/25 | Loss: 0.00103955
Iteration 21/25 | Loss: 0.00103616
Iteration 22/25 | Loss: 0.00103481
Iteration 23/25 | Loss: 0.00103439
Iteration 24/25 | Loss: 0.00103428
Iteration 25/25 | Loss: 0.00103427

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.94197559
Iteration 2/25 | Loss: 0.00266459
Iteration 3/25 | Loss: 0.00266459
Iteration 4/25 | Loss: 0.00266459
Iteration 5/25 | Loss: 0.00266459
Iteration 6/25 | Loss: 0.00266459
Iteration 7/25 | Loss: 0.00266459
Iteration 8/25 | Loss: 0.00266459
Iteration 9/25 | Loss: 0.00266459
Iteration 10/25 | Loss: 0.00266459
Iteration 11/25 | Loss: 0.00266459
Iteration 12/25 | Loss: 0.00266459
Iteration 13/25 | Loss: 0.00266459
Iteration 14/25 | Loss: 0.00266459
Iteration 15/25 | Loss: 0.00266459
Iteration 16/25 | Loss: 0.00266459
Iteration 17/25 | Loss: 0.00266459
Iteration 18/25 | Loss: 0.00266459
Iteration 19/25 | Loss: 0.00266459
Iteration 20/25 | Loss: 0.00266459
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.002664587926119566, 0.002664587926119566, 0.002664587926119566, 0.002664587926119566, 0.002664587926119566]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002664587926119566

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00266459
Iteration 2/1000 | Loss: 0.00116375
Iteration 3/1000 | Loss: 0.00087391
Iteration 4/1000 | Loss: 0.00100253
Iteration 5/1000 | Loss: 0.00034953
Iteration 6/1000 | Loss: 0.00021233
Iteration 7/1000 | Loss: 0.00074227
Iteration 8/1000 | Loss: 0.00032522
Iteration 9/1000 | Loss: 0.00070036
Iteration 10/1000 | Loss: 0.00075729
Iteration 11/1000 | Loss: 0.00017668
Iteration 12/1000 | Loss: 0.00015076
Iteration 13/1000 | Loss: 0.00013584
Iteration 14/1000 | Loss: 0.00012697
Iteration 15/1000 | Loss: 0.00019700
Iteration 16/1000 | Loss: 0.00012590
Iteration 17/1000 | Loss: 0.00011922
Iteration 18/1000 | Loss: 0.00011579
Iteration 19/1000 | Loss: 0.00011203
Iteration 20/1000 | Loss: 0.00010920
Iteration 21/1000 | Loss: 0.00010689
Iteration 22/1000 | Loss: 0.00046928
Iteration 23/1000 | Loss: 0.00027313
Iteration 24/1000 | Loss: 0.00011614
Iteration 25/1000 | Loss: 0.00010718
Iteration 26/1000 | Loss: 0.00036007
Iteration 27/1000 | Loss: 0.00026230
Iteration 28/1000 | Loss: 0.00022774
Iteration 29/1000 | Loss: 0.00010798
Iteration 30/1000 | Loss: 0.00078710
Iteration 31/1000 | Loss: 0.00015991
Iteration 32/1000 | Loss: 0.00011850
Iteration 33/1000 | Loss: 0.00011005
Iteration 34/1000 | Loss: 0.00010398
Iteration 35/1000 | Loss: 0.00009869
Iteration 36/1000 | Loss: 0.00009482
Iteration 37/1000 | Loss: 0.00009228
Iteration 38/1000 | Loss: 0.00009028
Iteration 39/1000 | Loss: 0.00008898
Iteration 40/1000 | Loss: 0.00008795
Iteration 41/1000 | Loss: 0.00008700
Iteration 42/1000 | Loss: 0.00008611
Iteration 43/1000 | Loss: 0.00008528
Iteration 44/1000 | Loss: 0.00008478
Iteration 45/1000 | Loss: 0.00008424
Iteration 46/1000 | Loss: 0.00008381
Iteration 47/1000 | Loss: 0.00110857
Iteration 48/1000 | Loss: 0.00760371
Iteration 49/1000 | Loss: 0.00820765
Iteration 50/1000 | Loss: 0.00876046
Iteration 51/1000 | Loss: 0.00917925
Iteration 52/1000 | Loss: 0.00647555
Iteration 53/1000 | Loss: 0.00683718
Iteration 54/1000 | Loss: 0.00411580
Iteration 55/1000 | Loss: 0.00327928
Iteration 56/1000 | Loss: 0.00201487
Iteration 57/1000 | Loss: 0.00288389
Iteration 58/1000 | Loss: 0.00377763
Iteration 59/1000 | Loss: 0.00190630
Iteration 60/1000 | Loss: 0.00289581
Iteration 61/1000 | Loss: 0.00325259
Iteration 62/1000 | Loss: 0.00249225
Iteration 63/1000 | Loss: 0.00135883
Iteration 64/1000 | Loss: 0.00167216
Iteration 65/1000 | Loss: 0.00230651
Iteration 66/1000 | Loss: 0.00212215
Iteration 67/1000 | Loss: 0.00158317
Iteration 68/1000 | Loss: 0.00183948
Iteration 69/1000 | Loss: 0.00117235
Iteration 70/1000 | Loss: 0.00155612
Iteration 71/1000 | Loss: 0.00212626
Iteration 72/1000 | Loss: 0.00162475
Iteration 73/1000 | Loss: 0.00211820
Iteration 74/1000 | Loss: 0.00213809
Iteration 75/1000 | Loss: 0.00360566
Iteration 76/1000 | Loss: 0.00592007
Iteration 77/1000 | Loss: 0.00283614
Iteration 78/1000 | Loss: 0.00154348
Iteration 79/1000 | Loss: 0.00194375
Iteration 80/1000 | Loss: 0.00353882
Iteration 81/1000 | Loss: 0.00196297
Iteration 82/1000 | Loss: 0.00213817
Iteration 83/1000 | Loss: 0.00149734
Iteration 84/1000 | Loss: 0.00317646
Iteration 85/1000 | Loss: 0.00311076
Iteration 86/1000 | Loss: 0.00324190
Iteration 87/1000 | Loss: 0.00199010
Iteration 88/1000 | Loss: 0.00111910
Iteration 89/1000 | Loss: 0.00083526
Iteration 90/1000 | Loss: 0.00434789
Iteration 91/1000 | Loss: 0.00224320
Iteration 92/1000 | Loss: 0.00152076
Iteration 93/1000 | Loss: 0.00115358
Iteration 94/1000 | Loss: 0.00063076
Iteration 95/1000 | Loss: 0.00066804
Iteration 96/1000 | Loss: 0.00122042
Iteration 97/1000 | Loss: 0.00050111
Iteration 98/1000 | Loss: 0.00117481
Iteration 99/1000 | Loss: 0.00179336
Iteration 100/1000 | Loss: 0.00118926
Iteration 101/1000 | Loss: 0.00127153
Iteration 102/1000 | Loss: 0.00099181
Iteration 103/1000 | Loss: 0.00078116
Iteration 104/1000 | Loss: 0.00279835
Iteration 105/1000 | Loss: 0.00078708
Iteration 106/1000 | Loss: 0.00155413
Iteration 107/1000 | Loss: 0.00121466
Iteration 108/1000 | Loss: 0.00131815
Iteration 109/1000 | Loss: 0.00155596
Iteration 110/1000 | Loss: 0.00148251
Iteration 111/1000 | Loss: 0.00235172
Iteration 112/1000 | Loss: 0.00161424
Iteration 113/1000 | Loss: 0.00094565
Iteration 114/1000 | Loss: 0.00128502
Iteration 115/1000 | Loss: 0.00061105
Iteration 116/1000 | Loss: 0.00093797
Iteration 117/1000 | Loss: 0.00119115
Iteration 118/1000 | Loss: 0.00100398
Iteration 119/1000 | Loss: 0.00117755
Iteration 120/1000 | Loss: 0.00068095
Iteration 121/1000 | Loss: 0.00039650
Iteration 122/1000 | Loss: 0.00047444
Iteration 123/1000 | Loss: 0.00194891
Iteration 124/1000 | Loss: 0.00249074
Iteration 125/1000 | Loss: 0.00020862
Iteration 126/1000 | Loss: 0.00053012
Iteration 127/1000 | Loss: 0.00023562
Iteration 128/1000 | Loss: 0.00029509
Iteration 129/1000 | Loss: 0.00031859
Iteration 130/1000 | Loss: 0.00009906
Iteration 131/1000 | Loss: 0.00039186
Iteration 132/1000 | Loss: 0.00072270
Iteration 133/1000 | Loss: 0.00086820
Iteration 134/1000 | Loss: 0.00008659
Iteration 135/1000 | Loss: 0.00035313
Iteration 136/1000 | Loss: 0.00019564
Iteration 137/1000 | Loss: 0.00068152
Iteration 138/1000 | Loss: 0.00100992
Iteration 139/1000 | Loss: 0.00093977
Iteration 140/1000 | Loss: 0.00008713
Iteration 141/1000 | Loss: 0.00058198
Iteration 142/1000 | Loss: 0.00021197
Iteration 143/1000 | Loss: 0.00060905
Iteration 144/1000 | Loss: 0.00043191
Iteration 145/1000 | Loss: 0.00037954
Iteration 146/1000 | Loss: 0.00047026
Iteration 147/1000 | Loss: 0.00033345
Iteration 148/1000 | Loss: 0.00045201
Iteration 149/1000 | Loss: 0.00044559
Iteration 150/1000 | Loss: 0.00005893
Iteration 151/1000 | Loss: 0.00005137
Iteration 152/1000 | Loss: 0.00060786
Iteration 153/1000 | Loss: 0.00079286
Iteration 154/1000 | Loss: 0.00075604
Iteration 155/1000 | Loss: 0.00006874
Iteration 156/1000 | Loss: 0.00087951
Iteration 157/1000 | Loss: 0.00099214
Iteration 158/1000 | Loss: 0.00125784
Iteration 159/1000 | Loss: 0.00036388
Iteration 160/1000 | Loss: 0.00007741
Iteration 161/1000 | Loss: 0.00057851
Iteration 162/1000 | Loss: 0.00024711
Iteration 163/1000 | Loss: 0.00004212
Iteration 164/1000 | Loss: 0.00048801
Iteration 165/1000 | Loss: 0.00003566
Iteration 166/1000 | Loss: 0.00003604
Iteration 167/1000 | Loss: 0.00003221
Iteration 168/1000 | Loss: 0.00003129
Iteration 169/1000 | Loss: 0.00003132
Iteration 170/1000 | Loss: 0.00002983
Iteration 171/1000 | Loss: 0.00002956
Iteration 172/1000 | Loss: 0.00002965
Iteration 173/1000 | Loss: 0.00002909
Iteration 174/1000 | Loss: 0.00002866
Iteration 175/1000 | Loss: 0.00002908
Iteration 176/1000 | Loss: 0.00002865
Iteration 177/1000 | Loss: 0.00002847
Iteration 178/1000 | Loss: 0.00002842
Iteration 179/1000 | Loss: 0.00002826
Iteration 180/1000 | Loss: 0.00002825
Iteration 181/1000 | Loss: 0.00002823
Iteration 182/1000 | Loss: 0.00002803
Iteration 183/1000 | Loss: 0.00002801
Iteration 184/1000 | Loss: 0.00111662
Iteration 185/1000 | Loss: 0.00003545
Iteration 186/1000 | Loss: 0.00003093
Iteration 187/1000 | Loss: 0.00002915
Iteration 188/1000 | Loss: 0.00002733
Iteration 189/1000 | Loss: 0.00002615
Iteration 190/1000 | Loss: 0.00002492
Iteration 191/1000 | Loss: 0.00002433
Iteration 192/1000 | Loss: 0.00002389
Iteration 193/1000 | Loss: 0.00002354
Iteration 194/1000 | Loss: 0.00002316
Iteration 195/1000 | Loss: 0.00002288
Iteration 196/1000 | Loss: 0.00002266
Iteration 197/1000 | Loss: 0.00002256
Iteration 198/1000 | Loss: 0.00002249
Iteration 199/1000 | Loss: 0.00002247
Iteration 200/1000 | Loss: 0.00002247
Iteration 201/1000 | Loss: 0.00002246
Iteration 202/1000 | Loss: 0.00002244
Iteration 203/1000 | Loss: 0.00002244
Iteration 204/1000 | Loss: 0.00002244
Iteration 205/1000 | Loss: 0.00002244
Iteration 206/1000 | Loss: 0.00002244
Iteration 207/1000 | Loss: 0.00002244
Iteration 208/1000 | Loss: 0.00002244
Iteration 209/1000 | Loss: 0.00002243
Iteration 210/1000 | Loss: 0.00002243
Iteration 211/1000 | Loss: 0.00002243
Iteration 212/1000 | Loss: 0.00002243
Iteration 213/1000 | Loss: 0.00002243
Iteration 214/1000 | Loss: 0.00002242
Iteration 215/1000 | Loss: 0.00002242
Iteration 216/1000 | Loss: 0.00002242
Iteration 217/1000 | Loss: 0.00002242
Iteration 218/1000 | Loss: 0.00002241
Iteration 219/1000 | Loss: 0.00002240
Iteration 220/1000 | Loss: 0.00002240
Iteration 221/1000 | Loss: 0.00002238
Iteration 222/1000 | Loss: 0.00002238
Iteration 223/1000 | Loss: 0.00002238
Iteration 224/1000 | Loss: 0.00002238
Iteration 225/1000 | Loss: 0.00002238
Iteration 226/1000 | Loss: 0.00002238
Iteration 227/1000 | Loss: 0.00002238
Iteration 228/1000 | Loss: 0.00002238
Iteration 229/1000 | Loss: 0.00002237
Iteration 230/1000 | Loss: 0.00002237
Iteration 231/1000 | Loss: 0.00002237
Iteration 232/1000 | Loss: 0.00002237
Iteration 233/1000 | Loss: 0.00002237
Iteration 234/1000 | Loss: 0.00002237
Iteration 235/1000 | Loss: 0.00002236
Iteration 236/1000 | Loss: 0.00002236
Iteration 237/1000 | Loss: 0.00002235
Iteration 238/1000 | Loss: 0.00002235
Iteration 239/1000 | Loss: 0.00002235
Iteration 240/1000 | Loss: 0.00002235
Iteration 241/1000 | Loss: 0.00002234
Iteration 242/1000 | Loss: 0.00002234
Iteration 243/1000 | Loss: 0.00002233
Iteration 244/1000 | Loss: 0.00002233
Iteration 245/1000 | Loss: 0.00002233
Iteration 246/1000 | Loss: 0.00002233
Iteration 247/1000 | Loss: 0.00002233
Iteration 248/1000 | Loss: 0.00002233
Iteration 249/1000 | Loss: 0.00002233
Iteration 250/1000 | Loss: 0.00002233
Iteration 251/1000 | Loss: 0.00002232
Iteration 252/1000 | Loss: 0.00002232
Iteration 253/1000 | Loss: 0.00002232
Iteration 254/1000 | Loss: 0.00002231
Iteration 255/1000 | Loss: 0.00002231
Iteration 256/1000 | Loss: 0.00002231
Iteration 257/1000 | Loss: 0.00002231
Iteration 258/1000 | Loss: 0.00002230
Iteration 259/1000 | Loss: 0.00002230
Iteration 260/1000 | Loss: 0.00002230
Iteration 261/1000 | Loss: 0.00002230
Iteration 262/1000 | Loss: 0.00002230
Iteration 263/1000 | Loss: 0.00002230
Iteration 264/1000 | Loss: 0.00002230
Iteration 265/1000 | Loss: 0.00002229
Iteration 266/1000 | Loss: 0.00002229
Iteration 267/1000 | Loss: 0.00002229
Iteration 268/1000 | Loss: 0.00002229
Iteration 269/1000 | Loss: 0.00002229
Iteration 270/1000 | Loss: 0.00002228
Iteration 271/1000 | Loss: 0.00002228
Iteration 272/1000 | Loss: 0.00002227
Iteration 273/1000 | Loss: 0.00002227
Iteration 274/1000 | Loss: 0.00002226
Iteration 275/1000 | Loss: 0.00002226
Iteration 276/1000 | Loss: 0.00002225
Iteration 277/1000 | Loss: 0.00002225
Iteration 278/1000 | Loss: 0.00002225
Iteration 279/1000 | Loss: 0.00002225
Iteration 280/1000 | Loss: 0.00002225
Iteration 281/1000 | Loss: 0.00002224
Iteration 282/1000 | Loss: 0.00002224
Iteration 283/1000 | Loss: 0.00002224
Iteration 284/1000 | Loss: 0.00002223
Iteration 285/1000 | Loss: 0.00002223
Iteration 286/1000 | Loss: 0.00002223
Iteration 287/1000 | Loss: 0.00002223
Iteration 288/1000 | Loss: 0.00002223
Iteration 289/1000 | Loss: 0.00002223
Iteration 290/1000 | Loss: 0.00002222
Iteration 291/1000 | Loss: 0.00002222
Iteration 292/1000 | Loss: 0.00002222
Iteration 293/1000 | Loss: 0.00002222
Iteration 294/1000 | Loss: 0.00002222
Iteration 295/1000 | Loss: 0.00002222
Iteration 296/1000 | Loss: 0.00002222
Iteration 297/1000 | Loss: 0.00002222
Iteration 298/1000 | Loss: 0.00002222
Iteration 299/1000 | Loss: 0.00002222
Iteration 300/1000 | Loss: 0.00002222
Iteration 301/1000 | Loss: 0.00002222
Iteration 302/1000 | Loss: 0.00002222
Iteration 303/1000 | Loss: 0.00002222
Iteration 304/1000 | Loss: 0.00002222
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 304. Stopping optimization.
Last 5 losses: [2.2218262529349886e-05, 2.2218262529349886e-05, 2.2218262529349886e-05, 2.2218262529349886e-05, 2.2218262529349886e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2218262529349886e-05

Optimization complete. Final v2v error: 3.683170795440674 mm

Highest mean error: 12.094281196594238 mm for frame 48

Lowest mean error: 3.0847630500793457 mm for frame 4

Saving results

Total time: 311.9072427749634
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_006/1067/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_006/1067.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_006/1067
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01107488
Iteration 2/25 | Loss: 0.00246387
Iteration 3/25 | Loss: 0.00165179
Iteration 4/25 | Loss: 0.00147442
Iteration 5/25 | Loss: 0.00161764
Iteration 6/25 | Loss: 0.00187187
Iteration 7/25 | Loss: 0.00157138
Iteration 8/25 | Loss: 0.00134199
Iteration 9/25 | Loss: 0.00124178
Iteration 10/25 | Loss: 0.00117512
Iteration 11/25 | Loss: 0.00111970
Iteration 12/25 | Loss: 0.00106112
Iteration 13/25 | Loss: 0.00102972
Iteration 14/25 | Loss: 0.00102355
Iteration 15/25 | Loss: 0.00100058
Iteration 16/25 | Loss: 0.00095012
Iteration 17/25 | Loss: 0.00092819
Iteration 18/25 | Loss: 0.00090665
Iteration 19/25 | Loss: 0.00088495
Iteration 20/25 | Loss: 0.00087211
Iteration 21/25 | Loss: 0.00087244
Iteration 22/25 | Loss: 0.00086850
Iteration 23/25 | Loss: 0.00086257
Iteration 24/25 | Loss: 0.00086379
Iteration 25/25 | Loss: 0.00086629

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.09154272
Iteration 2/25 | Loss: 0.00057774
Iteration 3/25 | Loss: 0.00057774
Iteration 4/25 | Loss: 0.00057774
Iteration 5/25 | Loss: 0.00057774
Iteration 6/25 | Loss: 0.00057773
Iteration 7/25 | Loss: 0.00057773
Iteration 8/25 | Loss: 0.00057773
Iteration 9/25 | Loss: 0.00057773
Iteration 10/25 | Loss: 0.00057773
Iteration 11/25 | Loss: 0.00057773
Iteration 12/25 | Loss: 0.00057773
Iteration 13/25 | Loss: 0.00057773
Iteration 14/25 | Loss: 0.00057773
Iteration 15/25 | Loss: 0.00057773
Iteration 16/25 | Loss: 0.00057773
Iteration 17/25 | Loss: 0.00057773
Iteration 18/25 | Loss: 0.00057773
Iteration 19/25 | Loss: 0.00057773
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0005777336191385984, 0.0005777336191385984, 0.0005777336191385984, 0.0005777336191385984, 0.0005777336191385984]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005777336191385984

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00057773
Iteration 2/1000 | Loss: 0.00050873
Iteration 3/1000 | Loss: 0.00023046
Iteration 4/1000 | Loss: 0.00021766
Iteration 5/1000 | Loss: 0.00046577
Iteration 6/1000 | Loss: 0.00021264
Iteration 7/1000 | Loss: 0.00054337
Iteration 8/1000 | Loss: 0.00040751
Iteration 9/1000 | Loss: 0.00055176
Iteration 10/1000 | Loss: 0.00024577
Iteration 11/1000 | Loss: 0.00055135
Iteration 12/1000 | Loss: 0.00056286
Iteration 13/1000 | Loss: 0.00108005
Iteration 14/1000 | Loss: 0.00035994
Iteration 15/1000 | Loss: 0.00028349
Iteration 16/1000 | Loss: 0.00007017
Iteration 17/1000 | Loss: 0.00012558
Iteration 18/1000 | Loss: 0.00010777
Iteration 19/1000 | Loss: 0.00009831
Iteration 20/1000 | Loss: 0.00009064
Iteration 21/1000 | Loss: 0.00004608
Iteration 22/1000 | Loss: 0.00005898
Iteration 23/1000 | Loss: 0.00010402
Iteration 24/1000 | Loss: 0.00007552
Iteration 25/1000 | Loss: 0.00007375
Iteration 26/1000 | Loss: 0.00030067
Iteration 27/1000 | Loss: 0.00047847
Iteration 28/1000 | Loss: 0.00034682
Iteration 29/1000 | Loss: 0.00051811
Iteration 30/1000 | Loss: 0.00012254
Iteration 31/1000 | Loss: 0.00032111
Iteration 32/1000 | Loss: 0.00007784
Iteration 33/1000 | Loss: 0.00007438
Iteration 34/1000 | Loss: 0.00006644
Iteration 35/1000 | Loss: 0.00004436
Iteration 36/1000 | Loss: 0.00005980
Iteration 37/1000 | Loss: 0.00004761
Iteration 38/1000 | Loss: 0.00026187
Iteration 39/1000 | Loss: 0.00022056
Iteration 40/1000 | Loss: 0.00019854
Iteration 41/1000 | Loss: 0.00037625
Iteration 42/1000 | Loss: 0.00007148
Iteration 43/1000 | Loss: 0.00006930
Iteration 44/1000 | Loss: 0.00006922
Iteration 45/1000 | Loss: 0.00003928
Iteration 46/1000 | Loss: 0.00003960
Iteration 47/1000 | Loss: 0.00005149
Iteration 48/1000 | Loss: 0.00004912
Iteration 49/1000 | Loss: 0.00006058
Iteration 50/1000 | Loss: 0.00003681
Iteration 51/1000 | Loss: 0.00004663
Iteration 52/1000 | Loss: 0.00025103
Iteration 53/1000 | Loss: 0.00025947
Iteration 54/1000 | Loss: 0.00025663
Iteration 55/1000 | Loss: 0.00021616
Iteration 56/1000 | Loss: 0.00024178
Iteration 57/1000 | Loss: 0.00039779
Iteration 58/1000 | Loss: 0.00006619
Iteration 59/1000 | Loss: 0.00004339
Iteration 60/1000 | Loss: 0.00003393
Iteration 61/1000 | Loss: 0.00004044
Iteration 62/1000 | Loss: 0.00003183
Iteration 63/1000 | Loss: 0.00002868
Iteration 64/1000 | Loss: 0.00005616
Iteration 65/1000 | Loss: 0.00005212
Iteration 66/1000 | Loss: 0.00011239
Iteration 67/1000 | Loss: 0.00004057
Iteration 68/1000 | Loss: 0.00003170
Iteration 69/1000 | Loss: 0.00003456
Iteration 70/1000 | Loss: 0.00002808
Iteration 71/1000 | Loss: 0.00002364
Iteration 72/1000 | Loss: 0.00002096
Iteration 73/1000 | Loss: 0.00001917
Iteration 74/1000 | Loss: 0.00003418
Iteration 75/1000 | Loss: 0.00003287
Iteration 76/1000 | Loss: 0.00003489
Iteration 77/1000 | Loss: 0.00002402
Iteration 78/1000 | Loss: 0.00002039
Iteration 79/1000 | Loss: 0.00001882
Iteration 80/1000 | Loss: 0.00001783
Iteration 81/1000 | Loss: 0.00001743
Iteration 82/1000 | Loss: 0.00001702
Iteration 83/1000 | Loss: 0.00001663
Iteration 84/1000 | Loss: 0.00001639
Iteration 85/1000 | Loss: 0.00001638
Iteration 86/1000 | Loss: 0.00001637
Iteration 87/1000 | Loss: 0.00001629
Iteration 88/1000 | Loss: 0.00001623
Iteration 89/1000 | Loss: 0.00001623
Iteration 90/1000 | Loss: 0.00001622
Iteration 91/1000 | Loss: 0.00001612
Iteration 92/1000 | Loss: 0.00001605
Iteration 93/1000 | Loss: 0.00003880
Iteration 94/1000 | Loss: 0.00002140
Iteration 95/1000 | Loss: 0.00001899
Iteration 96/1000 | Loss: 0.00001727
Iteration 97/1000 | Loss: 0.00001626
Iteration 98/1000 | Loss: 0.00001585
Iteration 99/1000 | Loss: 0.00001558
Iteration 100/1000 | Loss: 0.00001541
Iteration 101/1000 | Loss: 0.00001527
Iteration 102/1000 | Loss: 0.00001524
Iteration 103/1000 | Loss: 0.00001518
Iteration 104/1000 | Loss: 0.00001518
Iteration 105/1000 | Loss: 0.00001517
Iteration 106/1000 | Loss: 0.00001516
Iteration 107/1000 | Loss: 0.00001516
Iteration 108/1000 | Loss: 0.00001516
Iteration 109/1000 | Loss: 0.00001516
Iteration 110/1000 | Loss: 0.00001516
Iteration 111/1000 | Loss: 0.00001516
Iteration 112/1000 | Loss: 0.00001516
Iteration 113/1000 | Loss: 0.00001516
Iteration 114/1000 | Loss: 0.00001516
Iteration 115/1000 | Loss: 0.00001515
Iteration 116/1000 | Loss: 0.00001515
Iteration 117/1000 | Loss: 0.00001515
Iteration 118/1000 | Loss: 0.00001514
Iteration 119/1000 | Loss: 0.00001514
Iteration 120/1000 | Loss: 0.00001514
Iteration 121/1000 | Loss: 0.00001514
Iteration 122/1000 | Loss: 0.00001513
Iteration 123/1000 | Loss: 0.00001513
Iteration 124/1000 | Loss: 0.00001513
Iteration 125/1000 | Loss: 0.00001513
Iteration 126/1000 | Loss: 0.00001512
Iteration 127/1000 | Loss: 0.00001512
Iteration 128/1000 | Loss: 0.00001512
Iteration 129/1000 | Loss: 0.00001512
Iteration 130/1000 | Loss: 0.00001512
Iteration 131/1000 | Loss: 0.00001512
Iteration 132/1000 | Loss: 0.00001512
Iteration 133/1000 | Loss: 0.00001512
Iteration 134/1000 | Loss: 0.00001512
Iteration 135/1000 | Loss: 0.00001512
Iteration 136/1000 | Loss: 0.00001512
Iteration 137/1000 | Loss: 0.00001512
Iteration 138/1000 | Loss: 0.00001512
Iteration 139/1000 | Loss: 0.00001512
Iteration 140/1000 | Loss: 0.00001512
Iteration 141/1000 | Loss: 0.00001512
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 141. Stopping optimization.
Last 5 losses: [1.511630580353085e-05, 1.511630580353085e-05, 1.511630580353085e-05, 1.511630580353085e-05, 1.511630580353085e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.511630580353085e-05

Optimization complete. Final v2v error: 3.313844919204712 mm

Highest mean error: 3.8710789680480957 mm for frame 1

Lowest mean error: 3.0932412147521973 mm for frame 136

Saving results

Total time: 186.66114163398743
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_006/1057/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_006/1057.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_006/1057
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00904507
Iteration 2/25 | Loss: 0.00104325
Iteration 3/25 | Loss: 0.00088763
Iteration 4/25 | Loss: 0.00084437
Iteration 5/25 | Loss: 0.00082747
Iteration 6/25 | Loss: 0.00082369
Iteration 7/25 | Loss: 0.00082271
Iteration 8/25 | Loss: 0.00082257
Iteration 9/25 | Loss: 0.00082256
Iteration 10/25 | Loss: 0.00082256
Iteration 11/25 | Loss: 0.00082256
Iteration 12/25 | Loss: 0.00082256
Iteration 13/25 | Loss: 0.00082256
Iteration 14/25 | Loss: 0.00082256
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0008225638302974403, 0.0008225638302974403, 0.0008225638302974403, 0.0008225638302974403, 0.0008225638302974403]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008225638302974403

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.48080671
Iteration 2/25 | Loss: 0.00046464
Iteration 3/25 | Loss: 0.00046461
Iteration 4/25 | Loss: 0.00046461
Iteration 5/25 | Loss: 0.00046461
Iteration 6/25 | Loss: 0.00046461
Iteration 7/25 | Loss: 0.00046461
Iteration 8/25 | Loss: 0.00046461
Iteration 9/25 | Loss: 0.00046461
Iteration 10/25 | Loss: 0.00046461
Iteration 11/25 | Loss: 0.00046461
Iteration 12/25 | Loss: 0.00046461
Iteration 13/25 | Loss: 0.00046461
Iteration 14/25 | Loss: 0.00046461
Iteration 15/25 | Loss: 0.00046461
Iteration 16/25 | Loss: 0.00046461
Iteration 17/25 | Loss: 0.00046461
Iteration 18/25 | Loss: 0.00046461
Iteration 19/25 | Loss: 0.00046461
Iteration 20/25 | Loss: 0.00046461
Iteration 21/25 | Loss: 0.00046461
Iteration 22/25 | Loss: 0.00046461
Iteration 23/25 | Loss: 0.00046461
Iteration 24/25 | Loss: 0.00046461
Iteration 25/25 | Loss: 0.00046461

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00046461
Iteration 2/1000 | Loss: 0.00004942
Iteration 3/1000 | Loss: 0.00003941
Iteration 4/1000 | Loss: 0.00003486
Iteration 5/1000 | Loss: 0.00003341
Iteration 6/1000 | Loss: 0.00003168
Iteration 7/1000 | Loss: 0.00003040
Iteration 8/1000 | Loss: 0.00002974
Iteration 9/1000 | Loss: 0.00002912
Iteration 10/1000 | Loss: 0.00002879
Iteration 11/1000 | Loss: 0.00002849
Iteration 12/1000 | Loss: 0.00002820
Iteration 13/1000 | Loss: 0.00002801
Iteration 14/1000 | Loss: 0.00002797
Iteration 15/1000 | Loss: 0.00002785
Iteration 16/1000 | Loss: 0.00002784
Iteration 17/1000 | Loss: 0.00002782
Iteration 18/1000 | Loss: 0.00002781
Iteration 19/1000 | Loss: 0.00002781
Iteration 20/1000 | Loss: 0.00002780
Iteration 21/1000 | Loss: 0.00002780
Iteration 22/1000 | Loss: 0.00002780
Iteration 23/1000 | Loss: 0.00002776
Iteration 24/1000 | Loss: 0.00002775
Iteration 25/1000 | Loss: 0.00002775
Iteration 26/1000 | Loss: 0.00002775
Iteration 27/1000 | Loss: 0.00002775
Iteration 28/1000 | Loss: 0.00002775
Iteration 29/1000 | Loss: 0.00002774
Iteration 30/1000 | Loss: 0.00002774
Iteration 31/1000 | Loss: 0.00002774
Iteration 32/1000 | Loss: 0.00002773
Iteration 33/1000 | Loss: 0.00002772
Iteration 34/1000 | Loss: 0.00002772
Iteration 35/1000 | Loss: 0.00002772
Iteration 36/1000 | Loss: 0.00002772
Iteration 37/1000 | Loss: 0.00002772
Iteration 38/1000 | Loss: 0.00002772
Iteration 39/1000 | Loss: 0.00002771
Iteration 40/1000 | Loss: 0.00002771
Iteration 41/1000 | Loss: 0.00002771
Iteration 42/1000 | Loss: 0.00002771
Iteration 43/1000 | Loss: 0.00002771
Iteration 44/1000 | Loss: 0.00002770
Iteration 45/1000 | Loss: 0.00002770
Iteration 46/1000 | Loss: 0.00002770
Iteration 47/1000 | Loss: 0.00002769
Iteration 48/1000 | Loss: 0.00002769
Iteration 49/1000 | Loss: 0.00002768
Iteration 50/1000 | Loss: 0.00002768
Iteration 51/1000 | Loss: 0.00002768
Iteration 52/1000 | Loss: 0.00002767
Iteration 53/1000 | Loss: 0.00002767
Iteration 54/1000 | Loss: 0.00002767
Iteration 55/1000 | Loss: 0.00002767
Iteration 56/1000 | Loss: 0.00002767
Iteration 57/1000 | Loss: 0.00002767
Iteration 58/1000 | Loss: 0.00002767
Iteration 59/1000 | Loss: 0.00002767
Iteration 60/1000 | Loss: 0.00002767
Iteration 61/1000 | Loss: 0.00002767
Iteration 62/1000 | Loss: 0.00002767
Iteration 63/1000 | Loss: 0.00002767
Iteration 64/1000 | Loss: 0.00002766
Iteration 65/1000 | Loss: 0.00002766
Iteration 66/1000 | Loss: 0.00002766
Iteration 67/1000 | Loss: 0.00002766
Iteration 68/1000 | Loss: 0.00002766
Iteration 69/1000 | Loss: 0.00002766
Iteration 70/1000 | Loss: 0.00002765
Iteration 71/1000 | Loss: 0.00002765
Iteration 72/1000 | Loss: 0.00002765
Iteration 73/1000 | Loss: 0.00002764
Iteration 74/1000 | Loss: 0.00002764
Iteration 75/1000 | Loss: 0.00002764
Iteration 76/1000 | Loss: 0.00002764
Iteration 77/1000 | Loss: 0.00002763
Iteration 78/1000 | Loss: 0.00002763
Iteration 79/1000 | Loss: 0.00002763
Iteration 80/1000 | Loss: 0.00002763
Iteration 81/1000 | Loss: 0.00002763
Iteration 82/1000 | Loss: 0.00002763
Iteration 83/1000 | Loss: 0.00002763
Iteration 84/1000 | Loss: 0.00002762
Iteration 85/1000 | Loss: 0.00002762
Iteration 86/1000 | Loss: 0.00002762
Iteration 87/1000 | Loss: 0.00002762
Iteration 88/1000 | Loss: 0.00002762
Iteration 89/1000 | Loss: 0.00002762
Iteration 90/1000 | Loss: 0.00002762
Iteration 91/1000 | Loss: 0.00002762
Iteration 92/1000 | Loss: 0.00002762
Iteration 93/1000 | Loss: 0.00002762
Iteration 94/1000 | Loss: 0.00002761
Iteration 95/1000 | Loss: 0.00002761
Iteration 96/1000 | Loss: 0.00002761
Iteration 97/1000 | Loss: 0.00002761
Iteration 98/1000 | Loss: 0.00002761
Iteration 99/1000 | Loss: 0.00002761
Iteration 100/1000 | Loss: 0.00002761
Iteration 101/1000 | Loss: 0.00002761
Iteration 102/1000 | Loss: 0.00002760
Iteration 103/1000 | Loss: 0.00002760
Iteration 104/1000 | Loss: 0.00002760
Iteration 105/1000 | Loss: 0.00002760
Iteration 106/1000 | Loss: 0.00002760
Iteration 107/1000 | Loss: 0.00002760
Iteration 108/1000 | Loss: 0.00002760
Iteration 109/1000 | Loss: 0.00002759
Iteration 110/1000 | Loss: 0.00002759
Iteration 111/1000 | Loss: 0.00002759
Iteration 112/1000 | Loss: 0.00002759
Iteration 113/1000 | Loss: 0.00002759
Iteration 114/1000 | Loss: 0.00002758
Iteration 115/1000 | Loss: 0.00002758
Iteration 116/1000 | Loss: 0.00002758
Iteration 117/1000 | Loss: 0.00002758
Iteration 118/1000 | Loss: 0.00002757
Iteration 119/1000 | Loss: 0.00002757
Iteration 120/1000 | Loss: 0.00002757
Iteration 121/1000 | Loss: 0.00002757
Iteration 122/1000 | Loss: 0.00002757
Iteration 123/1000 | Loss: 0.00002757
Iteration 124/1000 | Loss: 0.00002757
Iteration 125/1000 | Loss: 0.00002756
Iteration 126/1000 | Loss: 0.00002756
Iteration 127/1000 | Loss: 0.00002756
Iteration 128/1000 | Loss: 0.00002756
Iteration 129/1000 | Loss: 0.00002756
Iteration 130/1000 | Loss: 0.00002756
Iteration 131/1000 | Loss: 0.00002755
Iteration 132/1000 | Loss: 0.00002755
Iteration 133/1000 | Loss: 0.00002755
Iteration 134/1000 | Loss: 0.00002755
Iteration 135/1000 | Loss: 0.00002755
Iteration 136/1000 | Loss: 0.00002755
Iteration 137/1000 | Loss: 0.00002754
Iteration 138/1000 | Loss: 0.00002754
Iteration 139/1000 | Loss: 0.00002754
Iteration 140/1000 | Loss: 0.00002754
Iteration 141/1000 | Loss: 0.00002754
Iteration 142/1000 | Loss: 0.00002754
Iteration 143/1000 | Loss: 0.00002753
Iteration 144/1000 | Loss: 0.00002753
Iteration 145/1000 | Loss: 0.00002753
Iteration 146/1000 | Loss: 0.00002753
Iteration 147/1000 | Loss: 0.00002753
Iteration 148/1000 | Loss: 0.00002752
Iteration 149/1000 | Loss: 0.00002752
Iteration 150/1000 | Loss: 0.00002752
Iteration 151/1000 | Loss: 0.00002752
Iteration 152/1000 | Loss: 0.00002752
Iteration 153/1000 | Loss: 0.00002752
Iteration 154/1000 | Loss: 0.00002752
Iteration 155/1000 | Loss: 0.00002752
Iteration 156/1000 | Loss: 0.00002752
Iteration 157/1000 | Loss: 0.00002752
Iteration 158/1000 | Loss: 0.00002751
Iteration 159/1000 | Loss: 0.00002751
Iteration 160/1000 | Loss: 0.00002751
Iteration 161/1000 | Loss: 0.00002751
Iteration 162/1000 | Loss: 0.00002751
Iteration 163/1000 | Loss: 0.00002751
Iteration 164/1000 | Loss: 0.00002750
Iteration 165/1000 | Loss: 0.00002750
Iteration 166/1000 | Loss: 0.00002750
Iteration 167/1000 | Loss: 0.00002750
Iteration 168/1000 | Loss: 0.00002750
Iteration 169/1000 | Loss: 0.00002750
Iteration 170/1000 | Loss: 0.00002750
Iteration 171/1000 | Loss: 0.00002750
Iteration 172/1000 | Loss: 0.00002749
Iteration 173/1000 | Loss: 0.00002749
Iteration 174/1000 | Loss: 0.00002749
Iteration 175/1000 | Loss: 0.00002749
Iteration 176/1000 | Loss: 0.00002749
Iteration 177/1000 | Loss: 0.00002749
Iteration 178/1000 | Loss: 0.00002749
Iteration 179/1000 | Loss: 0.00002749
Iteration 180/1000 | Loss: 0.00002749
Iteration 181/1000 | Loss: 0.00002748
Iteration 182/1000 | Loss: 0.00002748
Iteration 183/1000 | Loss: 0.00002748
Iteration 184/1000 | Loss: 0.00002748
Iteration 185/1000 | Loss: 0.00002748
Iteration 186/1000 | Loss: 0.00002748
Iteration 187/1000 | Loss: 0.00002748
Iteration 188/1000 | Loss: 0.00002748
Iteration 189/1000 | Loss: 0.00002747
Iteration 190/1000 | Loss: 0.00002747
Iteration 191/1000 | Loss: 0.00002747
Iteration 192/1000 | Loss: 0.00002747
Iteration 193/1000 | Loss: 0.00002747
Iteration 194/1000 | Loss: 0.00002747
Iteration 195/1000 | Loss: 0.00002747
Iteration 196/1000 | Loss: 0.00002747
Iteration 197/1000 | Loss: 0.00002747
Iteration 198/1000 | Loss: 0.00002747
Iteration 199/1000 | Loss: 0.00002747
Iteration 200/1000 | Loss: 0.00002747
Iteration 201/1000 | Loss: 0.00002747
Iteration 202/1000 | Loss: 0.00002747
Iteration 203/1000 | Loss: 0.00002747
Iteration 204/1000 | Loss: 0.00002747
Iteration 205/1000 | Loss: 0.00002747
Iteration 206/1000 | Loss: 0.00002747
Iteration 207/1000 | Loss: 0.00002747
Iteration 208/1000 | Loss: 0.00002747
Iteration 209/1000 | Loss: 0.00002747
Iteration 210/1000 | Loss: 0.00002747
Iteration 211/1000 | Loss: 0.00002747
Iteration 212/1000 | Loss: 0.00002747
Iteration 213/1000 | Loss: 0.00002747
Iteration 214/1000 | Loss: 0.00002747
Iteration 215/1000 | Loss: 0.00002747
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 215. Stopping optimization.
Last 5 losses: [2.746542486420367e-05, 2.746542486420367e-05, 2.746542486420367e-05, 2.746542486420367e-05, 2.746542486420367e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.746542486420367e-05

Optimization complete. Final v2v error: 4.29578971862793 mm

Highest mean error: 5.86467981338501 mm for frame 67

Lowest mean error: 3.5264408588409424 mm for frame 98

Saving results

Total time: 43.25207257270813
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_006/1025/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_006/1025.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_006/1025
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00637324
Iteration 2/25 | Loss: 0.00085384
Iteration 3/25 | Loss: 0.00075524
Iteration 4/25 | Loss: 0.00073530
Iteration 5/25 | Loss: 0.00073014
Iteration 6/25 | Loss: 0.00072880
Iteration 7/25 | Loss: 0.00072861
Iteration 8/25 | Loss: 0.00072861
Iteration 9/25 | Loss: 0.00072861
Iteration 10/25 | Loss: 0.00072861
Iteration 11/25 | Loss: 0.00072861
Iteration 12/25 | Loss: 0.00072861
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.000728606537450105, 0.000728606537450105, 0.000728606537450105, 0.000728606537450105, 0.000728606537450105]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000728606537450105

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.06131220
Iteration 2/25 | Loss: 0.00043998
Iteration 3/25 | Loss: 0.00043997
Iteration 4/25 | Loss: 0.00043997
Iteration 5/25 | Loss: 0.00043997
Iteration 6/25 | Loss: 0.00043997
Iteration 7/25 | Loss: 0.00043997
Iteration 8/25 | Loss: 0.00043997
Iteration 9/25 | Loss: 0.00043997
Iteration 10/25 | Loss: 0.00043997
Iteration 11/25 | Loss: 0.00043997
Iteration 12/25 | Loss: 0.00043997
Iteration 13/25 | Loss: 0.00043997
Iteration 14/25 | Loss: 0.00043997
Iteration 15/25 | Loss: 0.00043997
Iteration 16/25 | Loss: 0.00043997
Iteration 17/25 | Loss: 0.00043997
Iteration 18/25 | Loss: 0.00043997
Iteration 19/25 | Loss: 0.00043997
Iteration 20/25 | Loss: 0.00043997
Iteration 21/25 | Loss: 0.00043997
Iteration 22/25 | Loss: 0.00043997
Iteration 23/25 | Loss: 0.00043997
Iteration 24/25 | Loss: 0.00043997
Iteration 25/25 | Loss: 0.00043997

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00043997
Iteration 2/1000 | Loss: 0.00003096
Iteration 3/1000 | Loss: 0.00001976
Iteration 4/1000 | Loss: 0.00001836
Iteration 5/1000 | Loss: 0.00001725
Iteration 6/1000 | Loss: 0.00001665
Iteration 7/1000 | Loss: 0.00001613
Iteration 8/1000 | Loss: 0.00001583
Iteration 9/1000 | Loss: 0.00001578
Iteration 10/1000 | Loss: 0.00001557
Iteration 11/1000 | Loss: 0.00001543
Iteration 12/1000 | Loss: 0.00001543
Iteration 13/1000 | Loss: 0.00001538
Iteration 14/1000 | Loss: 0.00001533
Iteration 15/1000 | Loss: 0.00001532
Iteration 16/1000 | Loss: 0.00001532
Iteration 17/1000 | Loss: 0.00001528
Iteration 18/1000 | Loss: 0.00001523
Iteration 19/1000 | Loss: 0.00001522
Iteration 20/1000 | Loss: 0.00001519
Iteration 21/1000 | Loss: 0.00001519
Iteration 22/1000 | Loss: 0.00001518
Iteration 23/1000 | Loss: 0.00001517
Iteration 24/1000 | Loss: 0.00001517
Iteration 25/1000 | Loss: 0.00001516
Iteration 26/1000 | Loss: 0.00001516
Iteration 27/1000 | Loss: 0.00001515
Iteration 28/1000 | Loss: 0.00001515
Iteration 29/1000 | Loss: 0.00001514
Iteration 30/1000 | Loss: 0.00001514
Iteration 31/1000 | Loss: 0.00001514
Iteration 32/1000 | Loss: 0.00001513
Iteration 33/1000 | Loss: 0.00001513
Iteration 34/1000 | Loss: 0.00001513
Iteration 35/1000 | Loss: 0.00001513
Iteration 36/1000 | Loss: 0.00001513
Iteration 37/1000 | Loss: 0.00001512
Iteration 38/1000 | Loss: 0.00001512
Iteration 39/1000 | Loss: 0.00001512
Iteration 40/1000 | Loss: 0.00001511
Iteration 41/1000 | Loss: 0.00001511
Iteration 42/1000 | Loss: 0.00001510
Iteration 43/1000 | Loss: 0.00001509
Iteration 44/1000 | Loss: 0.00001508
Iteration 45/1000 | Loss: 0.00001508
Iteration 46/1000 | Loss: 0.00001507
Iteration 47/1000 | Loss: 0.00001505
Iteration 48/1000 | Loss: 0.00001504
Iteration 49/1000 | Loss: 0.00001503
Iteration 50/1000 | Loss: 0.00001502
Iteration 51/1000 | Loss: 0.00001499
Iteration 52/1000 | Loss: 0.00001491
Iteration 53/1000 | Loss: 0.00001491
Iteration 54/1000 | Loss: 0.00001490
Iteration 55/1000 | Loss: 0.00001488
Iteration 56/1000 | Loss: 0.00001488
Iteration 57/1000 | Loss: 0.00001488
Iteration 58/1000 | Loss: 0.00001488
Iteration 59/1000 | Loss: 0.00001488
Iteration 60/1000 | Loss: 0.00001487
Iteration 61/1000 | Loss: 0.00001487
Iteration 62/1000 | Loss: 0.00001487
Iteration 63/1000 | Loss: 0.00001487
Iteration 64/1000 | Loss: 0.00001487
Iteration 65/1000 | Loss: 0.00001487
Iteration 66/1000 | Loss: 0.00001487
Iteration 67/1000 | Loss: 0.00001487
Iteration 68/1000 | Loss: 0.00001487
Iteration 69/1000 | Loss: 0.00001487
Iteration 70/1000 | Loss: 0.00001487
Iteration 71/1000 | Loss: 0.00001487
Iteration 72/1000 | Loss: 0.00001486
Iteration 73/1000 | Loss: 0.00001486
Iteration 74/1000 | Loss: 0.00001486
Iteration 75/1000 | Loss: 0.00001485
Iteration 76/1000 | Loss: 0.00001485
Iteration 77/1000 | Loss: 0.00001485
Iteration 78/1000 | Loss: 0.00001485
Iteration 79/1000 | Loss: 0.00001485
Iteration 80/1000 | Loss: 0.00001485
Iteration 81/1000 | Loss: 0.00001485
Iteration 82/1000 | Loss: 0.00001485
Iteration 83/1000 | Loss: 0.00001485
Iteration 84/1000 | Loss: 0.00001485
Iteration 85/1000 | Loss: 0.00001485
Iteration 86/1000 | Loss: 0.00001485
Iteration 87/1000 | Loss: 0.00001485
Iteration 88/1000 | Loss: 0.00001485
Iteration 89/1000 | Loss: 0.00001485
Iteration 90/1000 | Loss: 0.00001485
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 90. Stopping optimization.
Last 5 losses: [1.4846587873762473e-05, 1.4846587873762473e-05, 1.4846587873762473e-05, 1.4846587873762473e-05, 1.4846587873762473e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4846587873762473e-05

Optimization complete. Final v2v error: 3.3065173625946045 mm

Highest mean error: 3.7452392578125 mm for frame 106

Lowest mean error: 3.0882725715637207 mm for frame 142

Saving results

Total time: 33.35255718231201
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_006/1055/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_006/1055.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_006/1055
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00827011
Iteration 2/25 | Loss: 0.00101945
Iteration 3/25 | Loss: 0.00081529
Iteration 4/25 | Loss: 0.00078202
Iteration 5/25 | Loss: 0.00077185
Iteration 6/25 | Loss: 0.00076947
Iteration 7/25 | Loss: 0.00076881
Iteration 8/25 | Loss: 0.00076881
Iteration 9/25 | Loss: 0.00076881
Iteration 10/25 | Loss: 0.00076881
Iteration 11/25 | Loss: 0.00076881
Iteration 12/25 | Loss: 0.00076881
Iteration 13/25 | Loss: 0.00076881
Iteration 14/25 | Loss: 0.00076881
Iteration 15/25 | Loss: 0.00076881
Iteration 16/25 | Loss: 0.00076881
Iteration 17/25 | Loss: 0.00076881
Iteration 18/25 | Loss: 0.00076881
Iteration 19/25 | Loss: 0.00076881
Iteration 20/25 | Loss: 0.00076881
Iteration 21/25 | Loss: 0.00076881
Iteration 22/25 | Loss: 0.00076881
Iteration 23/25 | Loss: 0.00076881
Iteration 24/25 | Loss: 0.00076881
Iteration 25/25 | Loss: 0.00076881

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.53629780
Iteration 2/25 | Loss: 0.00060212
Iteration 3/25 | Loss: 0.00060212
Iteration 4/25 | Loss: 0.00060212
Iteration 5/25 | Loss: 0.00060212
Iteration 6/25 | Loss: 0.00060212
Iteration 7/25 | Loss: 0.00060212
Iteration 8/25 | Loss: 0.00060212
Iteration 9/25 | Loss: 0.00060212
Iteration 10/25 | Loss: 0.00060212
Iteration 11/25 | Loss: 0.00060212
Iteration 12/25 | Loss: 0.00060212
Iteration 13/25 | Loss: 0.00060212
Iteration 14/25 | Loss: 0.00060212
Iteration 15/25 | Loss: 0.00060212
Iteration 16/25 | Loss: 0.00060212
Iteration 17/25 | Loss: 0.00060212
Iteration 18/25 | Loss: 0.00060212
Iteration 19/25 | Loss: 0.00060212
Iteration 20/25 | Loss: 0.00060212
Iteration 21/25 | Loss: 0.00060212
Iteration 22/25 | Loss: 0.00060212
Iteration 23/25 | Loss: 0.00060212
Iteration 24/25 | Loss: 0.00060212
Iteration 25/25 | Loss: 0.00060212

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00060212
Iteration 2/1000 | Loss: 0.00003643
Iteration 3/1000 | Loss: 0.00002323
Iteration 4/1000 | Loss: 0.00001998
Iteration 5/1000 | Loss: 0.00001901
Iteration 6/1000 | Loss: 0.00001827
Iteration 7/1000 | Loss: 0.00001781
Iteration 8/1000 | Loss: 0.00001750
Iteration 9/1000 | Loss: 0.00001726
Iteration 10/1000 | Loss: 0.00001702
Iteration 11/1000 | Loss: 0.00001693
Iteration 12/1000 | Loss: 0.00001687
Iteration 13/1000 | Loss: 0.00001687
Iteration 14/1000 | Loss: 0.00001680
Iteration 15/1000 | Loss: 0.00001679
Iteration 16/1000 | Loss: 0.00001672
Iteration 17/1000 | Loss: 0.00001671
Iteration 18/1000 | Loss: 0.00001663
Iteration 19/1000 | Loss: 0.00001653
Iteration 20/1000 | Loss: 0.00001648
Iteration 21/1000 | Loss: 0.00001646
Iteration 22/1000 | Loss: 0.00001644
Iteration 23/1000 | Loss: 0.00001642
Iteration 24/1000 | Loss: 0.00001641
Iteration 25/1000 | Loss: 0.00001640
Iteration 26/1000 | Loss: 0.00001640
Iteration 27/1000 | Loss: 0.00001637
Iteration 28/1000 | Loss: 0.00001637
Iteration 29/1000 | Loss: 0.00001636
Iteration 30/1000 | Loss: 0.00001636
Iteration 31/1000 | Loss: 0.00001635
Iteration 32/1000 | Loss: 0.00001635
Iteration 33/1000 | Loss: 0.00001632
Iteration 34/1000 | Loss: 0.00001630
Iteration 35/1000 | Loss: 0.00001630
Iteration 36/1000 | Loss: 0.00001629
Iteration 37/1000 | Loss: 0.00001629
Iteration 38/1000 | Loss: 0.00001628
Iteration 39/1000 | Loss: 0.00001627
Iteration 40/1000 | Loss: 0.00001627
Iteration 41/1000 | Loss: 0.00001627
Iteration 42/1000 | Loss: 0.00001627
Iteration 43/1000 | Loss: 0.00001627
Iteration 44/1000 | Loss: 0.00001626
Iteration 45/1000 | Loss: 0.00001626
Iteration 46/1000 | Loss: 0.00001626
Iteration 47/1000 | Loss: 0.00001626
Iteration 48/1000 | Loss: 0.00001626
Iteration 49/1000 | Loss: 0.00001626
Iteration 50/1000 | Loss: 0.00001625
Iteration 51/1000 | Loss: 0.00001625
Iteration 52/1000 | Loss: 0.00001624
Iteration 53/1000 | Loss: 0.00001624
Iteration 54/1000 | Loss: 0.00001624
Iteration 55/1000 | Loss: 0.00001624
Iteration 56/1000 | Loss: 0.00001623
Iteration 57/1000 | Loss: 0.00001623
Iteration 58/1000 | Loss: 0.00001623
Iteration 59/1000 | Loss: 0.00001623
Iteration 60/1000 | Loss: 0.00001622
Iteration 61/1000 | Loss: 0.00001622
Iteration 62/1000 | Loss: 0.00001622
Iteration 63/1000 | Loss: 0.00001622
Iteration 64/1000 | Loss: 0.00001621
Iteration 65/1000 | Loss: 0.00001621
Iteration 66/1000 | Loss: 0.00001621
Iteration 67/1000 | Loss: 0.00001621
Iteration 68/1000 | Loss: 0.00001621
Iteration 69/1000 | Loss: 0.00001621
Iteration 70/1000 | Loss: 0.00001621
Iteration 71/1000 | Loss: 0.00001621
Iteration 72/1000 | Loss: 0.00001621
Iteration 73/1000 | Loss: 0.00001620
Iteration 74/1000 | Loss: 0.00001620
Iteration 75/1000 | Loss: 0.00001620
Iteration 76/1000 | Loss: 0.00001620
Iteration 77/1000 | Loss: 0.00001620
Iteration 78/1000 | Loss: 0.00001620
Iteration 79/1000 | Loss: 0.00001620
Iteration 80/1000 | Loss: 0.00001620
Iteration 81/1000 | Loss: 0.00001620
Iteration 82/1000 | Loss: 0.00001620
Iteration 83/1000 | Loss: 0.00001619
Iteration 84/1000 | Loss: 0.00001619
Iteration 85/1000 | Loss: 0.00001619
Iteration 86/1000 | Loss: 0.00001619
Iteration 87/1000 | Loss: 0.00001619
Iteration 88/1000 | Loss: 0.00001618
Iteration 89/1000 | Loss: 0.00001618
Iteration 90/1000 | Loss: 0.00001618
Iteration 91/1000 | Loss: 0.00001618
Iteration 92/1000 | Loss: 0.00001617
Iteration 93/1000 | Loss: 0.00001617
Iteration 94/1000 | Loss: 0.00001617
Iteration 95/1000 | Loss: 0.00001617
Iteration 96/1000 | Loss: 0.00001617
Iteration 97/1000 | Loss: 0.00001617
Iteration 98/1000 | Loss: 0.00001617
Iteration 99/1000 | Loss: 0.00001617
Iteration 100/1000 | Loss: 0.00001617
Iteration 101/1000 | Loss: 0.00001616
Iteration 102/1000 | Loss: 0.00001616
Iteration 103/1000 | Loss: 0.00001616
Iteration 104/1000 | Loss: 0.00001616
Iteration 105/1000 | Loss: 0.00001616
Iteration 106/1000 | Loss: 0.00001616
Iteration 107/1000 | Loss: 0.00001616
Iteration 108/1000 | Loss: 0.00001616
Iteration 109/1000 | Loss: 0.00001615
Iteration 110/1000 | Loss: 0.00001615
Iteration 111/1000 | Loss: 0.00001615
Iteration 112/1000 | Loss: 0.00001615
Iteration 113/1000 | Loss: 0.00001615
Iteration 114/1000 | Loss: 0.00001615
Iteration 115/1000 | Loss: 0.00001615
Iteration 116/1000 | Loss: 0.00001615
Iteration 117/1000 | Loss: 0.00001615
Iteration 118/1000 | Loss: 0.00001615
Iteration 119/1000 | Loss: 0.00001615
Iteration 120/1000 | Loss: 0.00001615
Iteration 121/1000 | Loss: 0.00001615
Iteration 122/1000 | Loss: 0.00001615
Iteration 123/1000 | Loss: 0.00001615
Iteration 124/1000 | Loss: 0.00001615
Iteration 125/1000 | Loss: 0.00001614
Iteration 126/1000 | Loss: 0.00001614
Iteration 127/1000 | Loss: 0.00001614
Iteration 128/1000 | Loss: 0.00001614
Iteration 129/1000 | Loss: 0.00001614
Iteration 130/1000 | Loss: 0.00001614
Iteration 131/1000 | Loss: 0.00001613
Iteration 132/1000 | Loss: 0.00001613
Iteration 133/1000 | Loss: 0.00001613
Iteration 134/1000 | Loss: 0.00001613
Iteration 135/1000 | Loss: 0.00001613
Iteration 136/1000 | Loss: 0.00001613
Iteration 137/1000 | Loss: 0.00001613
Iteration 138/1000 | Loss: 0.00001613
Iteration 139/1000 | Loss: 0.00001613
Iteration 140/1000 | Loss: 0.00001613
Iteration 141/1000 | Loss: 0.00001613
Iteration 142/1000 | Loss: 0.00001613
Iteration 143/1000 | Loss: 0.00001612
Iteration 144/1000 | Loss: 0.00001612
Iteration 145/1000 | Loss: 0.00001612
Iteration 146/1000 | Loss: 0.00001612
Iteration 147/1000 | Loss: 0.00001612
Iteration 148/1000 | Loss: 0.00001612
Iteration 149/1000 | Loss: 0.00001612
Iteration 150/1000 | Loss: 0.00001612
Iteration 151/1000 | Loss: 0.00001612
Iteration 152/1000 | Loss: 0.00001612
Iteration 153/1000 | Loss: 0.00001612
Iteration 154/1000 | Loss: 0.00001612
Iteration 155/1000 | Loss: 0.00001612
Iteration 156/1000 | Loss: 0.00001612
Iteration 157/1000 | Loss: 0.00001612
Iteration 158/1000 | Loss: 0.00001612
Iteration 159/1000 | Loss: 0.00001612
Iteration 160/1000 | Loss: 0.00001612
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 160. Stopping optimization.
Last 5 losses: [1.611505649634637e-05, 1.611505649634637e-05, 1.611505649634637e-05, 1.611505649634637e-05, 1.611505649634637e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.611505649634637e-05

Optimization complete. Final v2v error: 3.4045770168304443 mm

Highest mean error: 4.15338659286499 mm for frame 114

Lowest mean error: 2.893805503845215 mm for frame 104

Saving results

Total time: 38.83282136917114
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_006/1013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_006/1013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_006/1013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00824485
Iteration 2/25 | Loss: 0.00091218
Iteration 3/25 | Loss: 0.00079554
Iteration 4/25 | Loss: 0.00076132
Iteration 5/25 | Loss: 0.00075142
Iteration 6/25 | Loss: 0.00074934
Iteration 7/25 | Loss: 0.00074894
Iteration 8/25 | Loss: 0.00074894
Iteration 9/25 | Loss: 0.00074894
Iteration 10/25 | Loss: 0.00074894
Iteration 11/25 | Loss: 0.00074894
Iteration 12/25 | Loss: 0.00074894
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0007489448762498796, 0.0007489448762498796, 0.0007489448762498796, 0.0007489448762498796, 0.0007489448762498796]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007489448762498796

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.53866303
Iteration 2/25 | Loss: 0.00042838
Iteration 3/25 | Loss: 0.00042837
Iteration 4/25 | Loss: 0.00042837
Iteration 5/25 | Loss: 0.00042837
Iteration 6/25 | Loss: 0.00042837
Iteration 7/25 | Loss: 0.00042837
Iteration 8/25 | Loss: 0.00042837
Iteration 9/25 | Loss: 0.00042837
Iteration 10/25 | Loss: 0.00042837
Iteration 11/25 | Loss: 0.00042837
Iteration 12/25 | Loss: 0.00042837
Iteration 13/25 | Loss: 0.00042837
Iteration 14/25 | Loss: 0.00042837
Iteration 15/25 | Loss: 0.00042837
Iteration 16/25 | Loss: 0.00042837
Iteration 17/25 | Loss: 0.00042837
Iteration 18/25 | Loss: 0.00042837
Iteration 19/25 | Loss: 0.00042837
Iteration 20/25 | Loss: 0.00042837
Iteration 21/25 | Loss: 0.00042837
Iteration 22/25 | Loss: 0.00042837
Iteration 23/25 | Loss: 0.00042837
Iteration 24/25 | Loss: 0.00042837
Iteration 25/25 | Loss: 0.00042837

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00042837
Iteration 2/1000 | Loss: 0.00003614
Iteration 3/1000 | Loss: 0.00002414
Iteration 4/1000 | Loss: 0.00002221
Iteration 5/1000 | Loss: 0.00002056
Iteration 6/1000 | Loss: 0.00001983
Iteration 7/1000 | Loss: 0.00001920
Iteration 8/1000 | Loss: 0.00001875
Iteration 9/1000 | Loss: 0.00001841
Iteration 10/1000 | Loss: 0.00001821
Iteration 11/1000 | Loss: 0.00001814
Iteration 12/1000 | Loss: 0.00001804
Iteration 13/1000 | Loss: 0.00001797
Iteration 14/1000 | Loss: 0.00001795
Iteration 15/1000 | Loss: 0.00001791
Iteration 16/1000 | Loss: 0.00001787
Iteration 17/1000 | Loss: 0.00001786
Iteration 18/1000 | Loss: 0.00001786
Iteration 19/1000 | Loss: 0.00001785
Iteration 20/1000 | Loss: 0.00001785
Iteration 21/1000 | Loss: 0.00001784
Iteration 22/1000 | Loss: 0.00001784
Iteration 23/1000 | Loss: 0.00001784
Iteration 24/1000 | Loss: 0.00001783
Iteration 25/1000 | Loss: 0.00001783
Iteration 26/1000 | Loss: 0.00001782
Iteration 27/1000 | Loss: 0.00001782
Iteration 28/1000 | Loss: 0.00001782
Iteration 29/1000 | Loss: 0.00001782
Iteration 30/1000 | Loss: 0.00001782
Iteration 31/1000 | Loss: 0.00001782
Iteration 32/1000 | Loss: 0.00001782
Iteration 33/1000 | Loss: 0.00001781
Iteration 34/1000 | Loss: 0.00001781
Iteration 35/1000 | Loss: 0.00001781
Iteration 36/1000 | Loss: 0.00001781
Iteration 37/1000 | Loss: 0.00001781
Iteration 38/1000 | Loss: 0.00001781
Iteration 39/1000 | Loss: 0.00001780
Iteration 40/1000 | Loss: 0.00001780
Iteration 41/1000 | Loss: 0.00001780
Iteration 42/1000 | Loss: 0.00001780
Iteration 43/1000 | Loss: 0.00001780
Iteration 44/1000 | Loss: 0.00001780
Iteration 45/1000 | Loss: 0.00001780
Iteration 46/1000 | Loss: 0.00001780
Iteration 47/1000 | Loss: 0.00001780
Iteration 48/1000 | Loss: 0.00001780
Iteration 49/1000 | Loss: 0.00001780
Iteration 50/1000 | Loss: 0.00001780
Iteration 51/1000 | Loss: 0.00001779
Iteration 52/1000 | Loss: 0.00001779
Iteration 53/1000 | Loss: 0.00001779
Iteration 54/1000 | Loss: 0.00001779
Iteration 55/1000 | Loss: 0.00001779
Iteration 56/1000 | Loss: 0.00001779
Iteration 57/1000 | Loss: 0.00001779
Iteration 58/1000 | Loss: 0.00001779
Iteration 59/1000 | Loss: 0.00001779
Iteration 60/1000 | Loss: 0.00001779
Iteration 61/1000 | Loss: 0.00001778
Iteration 62/1000 | Loss: 0.00001778
Iteration 63/1000 | Loss: 0.00001778
Iteration 64/1000 | Loss: 0.00001778
Iteration 65/1000 | Loss: 0.00001778
Iteration 66/1000 | Loss: 0.00001778
Iteration 67/1000 | Loss: 0.00001778
Iteration 68/1000 | Loss: 0.00001777
Iteration 69/1000 | Loss: 0.00001777
Iteration 70/1000 | Loss: 0.00001777
Iteration 71/1000 | Loss: 0.00001777
Iteration 72/1000 | Loss: 0.00001777
Iteration 73/1000 | Loss: 0.00001777
Iteration 74/1000 | Loss: 0.00001777
Iteration 75/1000 | Loss: 0.00001777
Iteration 76/1000 | Loss: 0.00001777
Iteration 77/1000 | Loss: 0.00001776
Iteration 78/1000 | Loss: 0.00001776
Iteration 79/1000 | Loss: 0.00001776
Iteration 80/1000 | Loss: 0.00001776
Iteration 81/1000 | Loss: 0.00001775
Iteration 82/1000 | Loss: 0.00001775
Iteration 83/1000 | Loss: 0.00001775
Iteration 84/1000 | Loss: 0.00001774
Iteration 85/1000 | Loss: 0.00001774
Iteration 86/1000 | Loss: 0.00001774
Iteration 87/1000 | Loss: 0.00001774
Iteration 88/1000 | Loss: 0.00001774
Iteration 89/1000 | Loss: 0.00001773
Iteration 90/1000 | Loss: 0.00001773
Iteration 91/1000 | Loss: 0.00001773
Iteration 92/1000 | Loss: 0.00001773
Iteration 93/1000 | Loss: 0.00001773
Iteration 94/1000 | Loss: 0.00001773
Iteration 95/1000 | Loss: 0.00001773
Iteration 96/1000 | Loss: 0.00001773
Iteration 97/1000 | Loss: 0.00001773
Iteration 98/1000 | Loss: 0.00001773
Iteration 99/1000 | Loss: 0.00001772
Iteration 100/1000 | Loss: 0.00001772
Iteration 101/1000 | Loss: 0.00001772
Iteration 102/1000 | Loss: 0.00001772
Iteration 103/1000 | Loss: 0.00001771
Iteration 104/1000 | Loss: 0.00001771
Iteration 105/1000 | Loss: 0.00001771
Iteration 106/1000 | Loss: 0.00001770
Iteration 107/1000 | Loss: 0.00001770
Iteration 108/1000 | Loss: 0.00001770
Iteration 109/1000 | Loss: 0.00001770
Iteration 110/1000 | Loss: 0.00001770
Iteration 111/1000 | Loss: 0.00001770
Iteration 112/1000 | Loss: 0.00001770
Iteration 113/1000 | Loss: 0.00001770
Iteration 114/1000 | Loss: 0.00001769
Iteration 115/1000 | Loss: 0.00001769
Iteration 116/1000 | Loss: 0.00001769
Iteration 117/1000 | Loss: 0.00001769
Iteration 118/1000 | Loss: 0.00001769
Iteration 119/1000 | Loss: 0.00001769
Iteration 120/1000 | Loss: 0.00001769
Iteration 121/1000 | Loss: 0.00001769
Iteration 122/1000 | Loss: 0.00001769
Iteration 123/1000 | Loss: 0.00001769
Iteration 124/1000 | Loss: 0.00001769
Iteration 125/1000 | Loss: 0.00001768
Iteration 126/1000 | Loss: 0.00001768
Iteration 127/1000 | Loss: 0.00001768
Iteration 128/1000 | Loss: 0.00001768
Iteration 129/1000 | Loss: 0.00001767
Iteration 130/1000 | Loss: 0.00001767
Iteration 131/1000 | Loss: 0.00001767
Iteration 132/1000 | Loss: 0.00001767
Iteration 133/1000 | Loss: 0.00001767
Iteration 134/1000 | Loss: 0.00001766
Iteration 135/1000 | Loss: 0.00001766
Iteration 136/1000 | Loss: 0.00001766
Iteration 137/1000 | Loss: 0.00001765
Iteration 138/1000 | Loss: 0.00001765
Iteration 139/1000 | Loss: 0.00001765
Iteration 140/1000 | Loss: 0.00001765
Iteration 141/1000 | Loss: 0.00001765
Iteration 142/1000 | Loss: 0.00001764
Iteration 143/1000 | Loss: 0.00001764
Iteration 144/1000 | Loss: 0.00001764
Iteration 145/1000 | Loss: 0.00001764
Iteration 146/1000 | Loss: 0.00001764
Iteration 147/1000 | Loss: 0.00001764
Iteration 148/1000 | Loss: 0.00001764
Iteration 149/1000 | Loss: 0.00001764
Iteration 150/1000 | Loss: 0.00001764
Iteration 151/1000 | Loss: 0.00001764
Iteration 152/1000 | Loss: 0.00001764
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 152. Stopping optimization.
Last 5 losses: [1.7643034880165942e-05, 1.7643034880165942e-05, 1.7643034880165942e-05, 1.7643034880165942e-05, 1.7643034880165942e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7643034880165942e-05

Optimization complete. Final v2v error: 3.5402324199676514 mm

Highest mean error: 3.8567326068878174 mm for frame 75

Lowest mean error: 3.3759593963623047 mm for frame 57

Saving results

Total time: 41.23256206512451
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_006/1056/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_006/1056.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_006/1056
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01056220
Iteration 2/25 | Loss: 0.00220429
Iteration 3/25 | Loss: 0.00160173
Iteration 4/25 | Loss: 0.00146129
Iteration 5/25 | Loss: 0.00135301
Iteration 6/25 | Loss: 0.00131344
Iteration 7/25 | Loss: 0.00105453
Iteration 8/25 | Loss: 0.00095006
Iteration 9/25 | Loss: 0.00094585
Iteration 10/25 | Loss: 0.00093388
Iteration 11/25 | Loss: 0.00090901
Iteration 12/25 | Loss: 0.00090187
Iteration 13/25 | Loss: 0.00089935
Iteration 14/25 | Loss: 0.00089887
Iteration 15/25 | Loss: 0.00089843
Iteration 16/25 | Loss: 0.00089836
Iteration 17/25 | Loss: 0.00089836
Iteration 18/25 | Loss: 0.00089836
Iteration 19/25 | Loss: 0.00089836
Iteration 20/25 | Loss: 0.00089836
Iteration 21/25 | Loss: 0.00089836
Iteration 22/25 | Loss: 0.00089836
Iteration 23/25 | Loss: 0.00089836
Iteration 24/25 | Loss: 0.00089836
Iteration 25/25 | Loss: 0.00089836

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.49106634
Iteration 2/25 | Loss: 0.00044041
Iteration 3/25 | Loss: 0.00044040
Iteration 4/25 | Loss: 0.00044040
Iteration 5/25 | Loss: 0.00044040
Iteration 6/25 | Loss: 0.00044040
Iteration 7/25 | Loss: 0.00044040
Iteration 8/25 | Loss: 0.00044040
Iteration 9/25 | Loss: 0.00044040
Iteration 10/25 | Loss: 0.00044040
Iteration 11/25 | Loss: 0.00044040
Iteration 12/25 | Loss: 0.00044040
Iteration 13/25 | Loss: 0.00044040
Iteration 14/25 | Loss: 0.00044040
Iteration 15/25 | Loss: 0.00044040
Iteration 16/25 | Loss: 0.00044040
Iteration 17/25 | Loss: 0.00044040
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.00044039965723641217, 0.00044039965723641217, 0.00044039965723641217, 0.00044039965723641217, 0.00044039965723641217]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00044039965723641217

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00044040
Iteration 2/1000 | Loss: 0.00003896
Iteration 3/1000 | Loss: 0.00003054
Iteration 4/1000 | Loss: 0.00002755
Iteration 5/1000 | Loss: 0.00002564
Iteration 6/1000 | Loss: 0.00002486
Iteration 7/1000 | Loss: 0.00002421
Iteration 8/1000 | Loss: 0.00002383
Iteration 9/1000 | Loss: 0.00002348
Iteration 10/1000 | Loss: 0.00002317
Iteration 11/1000 | Loss: 0.00002310
Iteration 12/1000 | Loss: 0.00002308
Iteration 13/1000 | Loss: 0.00002299
Iteration 14/1000 | Loss: 0.00002293
Iteration 15/1000 | Loss: 0.00002290
Iteration 16/1000 | Loss: 0.00002290
Iteration 17/1000 | Loss: 0.00002289
Iteration 18/1000 | Loss: 0.00002287
Iteration 19/1000 | Loss: 0.00002287
Iteration 20/1000 | Loss: 0.00002286
Iteration 21/1000 | Loss: 0.00002285
Iteration 22/1000 | Loss: 0.00002285
Iteration 23/1000 | Loss: 0.00002284
Iteration 24/1000 | Loss: 0.00002284
Iteration 25/1000 | Loss: 0.00002282
Iteration 26/1000 | Loss: 0.00002282
Iteration 27/1000 | Loss: 0.00002281
Iteration 28/1000 | Loss: 0.00002281
Iteration 29/1000 | Loss: 0.00002280
Iteration 30/1000 | Loss: 0.00002277
Iteration 31/1000 | Loss: 0.00002277
Iteration 32/1000 | Loss: 0.00002277
Iteration 33/1000 | Loss: 0.00002277
Iteration 34/1000 | Loss: 0.00002277
Iteration 35/1000 | Loss: 0.00002277
Iteration 36/1000 | Loss: 0.00002277
Iteration 37/1000 | Loss: 0.00002277
Iteration 38/1000 | Loss: 0.00002277
Iteration 39/1000 | Loss: 0.00002277
Iteration 40/1000 | Loss: 0.00002277
Iteration 41/1000 | Loss: 0.00002277
Iteration 42/1000 | Loss: 0.00002275
Iteration 43/1000 | Loss: 0.00002275
Iteration 44/1000 | Loss: 0.00002275
Iteration 45/1000 | Loss: 0.00002274
Iteration 46/1000 | Loss: 0.00002274
Iteration 47/1000 | Loss: 0.00002274
Iteration 48/1000 | Loss: 0.00002274
Iteration 49/1000 | Loss: 0.00002274
Iteration 50/1000 | Loss: 0.00002274
Iteration 51/1000 | Loss: 0.00002274
Iteration 52/1000 | Loss: 0.00002274
Iteration 53/1000 | Loss: 0.00002274
Iteration 54/1000 | Loss: 0.00002274
Iteration 55/1000 | Loss: 0.00002274
Iteration 56/1000 | Loss: 0.00002273
Iteration 57/1000 | Loss: 0.00002273
Iteration 58/1000 | Loss: 0.00002273
Iteration 59/1000 | Loss: 0.00002273
Iteration 60/1000 | Loss: 0.00002273
Iteration 61/1000 | Loss: 0.00002273
Iteration 62/1000 | Loss: 0.00002273
Iteration 63/1000 | Loss: 0.00002273
Iteration 64/1000 | Loss: 0.00002272
Iteration 65/1000 | Loss: 0.00002272
Iteration 66/1000 | Loss: 0.00002272
Iteration 67/1000 | Loss: 0.00002272
Iteration 68/1000 | Loss: 0.00002271
Iteration 69/1000 | Loss: 0.00002271
Iteration 70/1000 | Loss: 0.00002271
Iteration 71/1000 | Loss: 0.00002271
Iteration 72/1000 | Loss: 0.00002270
Iteration 73/1000 | Loss: 0.00002270
Iteration 74/1000 | Loss: 0.00002270
Iteration 75/1000 | Loss: 0.00002270
Iteration 76/1000 | Loss: 0.00002270
Iteration 77/1000 | Loss: 0.00002270
Iteration 78/1000 | Loss: 0.00002270
Iteration 79/1000 | Loss: 0.00002270
Iteration 80/1000 | Loss: 0.00002270
Iteration 81/1000 | Loss: 0.00002269
Iteration 82/1000 | Loss: 0.00002269
Iteration 83/1000 | Loss: 0.00002269
Iteration 84/1000 | Loss: 0.00002269
Iteration 85/1000 | Loss: 0.00002269
Iteration 86/1000 | Loss: 0.00002269
Iteration 87/1000 | Loss: 0.00002269
Iteration 88/1000 | Loss: 0.00002269
Iteration 89/1000 | Loss: 0.00002269
Iteration 90/1000 | Loss: 0.00002269
Iteration 91/1000 | Loss: 0.00002269
Iteration 92/1000 | Loss: 0.00002269
Iteration 93/1000 | Loss: 0.00002269
Iteration 94/1000 | Loss: 0.00002269
Iteration 95/1000 | Loss: 0.00002269
Iteration 96/1000 | Loss: 0.00002269
Iteration 97/1000 | Loss: 0.00002269
Iteration 98/1000 | Loss: 0.00002269
Iteration 99/1000 | Loss: 0.00002269
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 99. Stopping optimization.
Last 5 losses: [2.2688078388455324e-05, 2.2688078388455324e-05, 2.2688078388455324e-05, 2.2688078388455324e-05, 2.2688078388455324e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2688078388455324e-05

Optimization complete. Final v2v error: 3.90570330619812 mm

Highest mean error: 9.896830558776855 mm for frame 7

Lowest mean error: 3.3803939819335938 mm for frame 201

Saving results

Total time: 56.383023500442505
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_006/1099/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_006/1099.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_006/1099
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01030526
Iteration 2/25 | Loss: 0.00239383
Iteration 3/25 | Loss: 0.00168646
Iteration 4/25 | Loss: 0.00155152
Iteration 5/25 | Loss: 0.00139180
Iteration 6/25 | Loss: 0.00121538
Iteration 7/25 | Loss: 0.00107565
Iteration 8/25 | Loss: 0.00102073
Iteration 9/25 | Loss: 0.00098331
Iteration 10/25 | Loss: 0.00093806
Iteration 11/25 | Loss: 0.00093450
Iteration 12/25 | Loss: 0.00093170
Iteration 13/25 | Loss: 0.00091462
Iteration 14/25 | Loss: 0.00089335
Iteration 15/25 | Loss: 0.00089610
Iteration 16/25 | Loss: 0.00087898
Iteration 17/25 | Loss: 0.00086450
Iteration 18/25 | Loss: 0.00085681
Iteration 19/25 | Loss: 0.00085304
Iteration 20/25 | Loss: 0.00085109
Iteration 21/25 | Loss: 0.00083933
Iteration 22/25 | Loss: 0.00083530
Iteration 23/25 | Loss: 0.00083423
Iteration 24/25 | Loss: 0.00083389
Iteration 25/25 | Loss: 0.00083363

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39202416
Iteration 2/25 | Loss: 0.00075854
Iteration 3/25 | Loss: 0.00075852
Iteration 4/25 | Loss: 0.00075852
Iteration 5/25 | Loss: 0.00075852
Iteration 6/25 | Loss: 0.00075852
Iteration 7/25 | Loss: 0.00075852
Iteration 8/25 | Loss: 0.00075852
Iteration 9/25 | Loss: 0.00075852
Iteration 10/25 | Loss: 0.00075852
Iteration 11/25 | Loss: 0.00075852
Iteration 12/25 | Loss: 0.00075852
Iteration 13/25 | Loss: 0.00075852
Iteration 14/25 | Loss: 0.00075852
Iteration 15/25 | Loss: 0.00075852
Iteration 16/25 | Loss: 0.00075852
Iteration 17/25 | Loss: 0.00075852
Iteration 18/25 | Loss: 0.00075852
Iteration 19/25 | Loss: 0.00075852
Iteration 20/25 | Loss: 0.00075852
Iteration 21/25 | Loss: 0.00075852
Iteration 22/25 | Loss: 0.00075852
Iteration 23/25 | Loss: 0.00075852
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0007585199200548232, 0.0007585199200548232, 0.0007585199200548232, 0.0007585199200548232, 0.0007585199200548232]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007585199200548232

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00075852
Iteration 2/1000 | Loss: 0.00006993
Iteration 3/1000 | Loss: 0.00021119
Iteration 4/1000 | Loss: 0.00022451
Iteration 5/1000 | Loss: 0.00004586
Iteration 6/1000 | Loss: 0.00003749
Iteration 7/1000 | Loss: 0.00003280
Iteration 8/1000 | Loss: 0.00002827
Iteration 9/1000 | Loss: 0.00002565
Iteration 10/1000 | Loss: 0.00002421
Iteration 11/1000 | Loss: 0.00002332
Iteration 12/1000 | Loss: 0.00038383
Iteration 13/1000 | Loss: 0.00003299
Iteration 14/1000 | Loss: 0.00002414
Iteration 15/1000 | Loss: 0.00002219
Iteration 16/1000 | Loss: 0.00002061
Iteration 17/1000 | Loss: 0.00001950
Iteration 18/1000 | Loss: 0.00001883
Iteration 19/1000 | Loss: 0.00001850
Iteration 20/1000 | Loss: 0.00001823
Iteration 21/1000 | Loss: 0.00001818
Iteration 22/1000 | Loss: 0.00001816
Iteration 23/1000 | Loss: 0.00001815
Iteration 24/1000 | Loss: 0.00001808
Iteration 25/1000 | Loss: 0.00001802
Iteration 26/1000 | Loss: 0.00001801
Iteration 27/1000 | Loss: 0.00001796
Iteration 28/1000 | Loss: 0.00001782
Iteration 29/1000 | Loss: 0.00001778
Iteration 30/1000 | Loss: 0.00001772
Iteration 31/1000 | Loss: 0.00001768
Iteration 32/1000 | Loss: 0.00001768
Iteration 33/1000 | Loss: 0.00001768
Iteration 34/1000 | Loss: 0.00001768
Iteration 35/1000 | Loss: 0.00001768
Iteration 36/1000 | Loss: 0.00001768
Iteration 37/1000 | Loss: 0.00001767
Iteration 38/1000 | Loss: 0.00001767
Iteration 39/1000 | Loss: 0.00001767
Iteration 40/1000 | Loss: 0.00001767
Iteration 41/1000 | Loss: 0.00001766
Iteration 42/1000 | Loss: 0.00001765
Iteration 43/1000 | Loss: 0.00001765
Iteration 44/1000 | Loss: 0.00001765
Iteration 45/1000 | Loss: 0.00001765
Iteration 46/1000 | Loss: 0.00001764
Iteration 47/1000 | Loss: 0.00001764
Iteration 48/1000 | Loss: 0.00001763
Iteration 49/1000 | Loss: 0.00001763
Iteration 50/1000 | Loss: 0.00001763
Iteration 51/1000 | Loss: 0.00001763
Iteration 52/1000 | Loss: 0.00001762
Iteration 53/1000 | Loss: 0.00001762
Iteration 54/1000 | Loss: 0.00001761
Iteration 55/1000 | Loss: 0.00001761
Iteration 56/1000 | Loss: 0.00001761
Iteration 57/1000 | Loss: 0.00001760
Iteration 58/1000 | Loss: 0.00001760
Iteration 59/1000 | Loss: 0.00001759
Iteration 60/1000 | Loss: 0.00001759
Iteration 61/1000 | Loss: 0.00001759
Iteration 62/1000 | Loss: 0.00001758
Iteration 63/1000 | Loss: 0.00001756
Iteration 64/1000 | Loss: 0.00001756
Iteration 65/1000 | Loss: 0.00001755
Iteration 66/1000 | Loss: 0.00001755
Iteration 67/1000 | Loss: 0.00001755
Iteration 68/1000 | Loss: 0.00001754
Iteration 69/1000 | Loss: 0.00001754
Iteration 70/1000 | Loss: 0.00001754
Iteration 71/1000 | Loss: 0.00001754
Iteration 72/1000 | Loss: 0.00001753
Iteration 73/1000 | Loss: 0.00001753
Iteration 74/1000 | Loss: 0.00001753
Iteration 75/1000 | Loss: 0.00001752
Iteration 76/1000 | Loss: 0.00001752
Iteration 77/1000 | Loss: 0.00001752
Iteration 78/1000 | Loss: 0.00001752
Iteration 79/1000 | Loss: 0.00001752
Iteration 80/1000 | Loss: 0.00001752
Iteration 81/1000 | Loss: 0.00001751
Iteration 82/1000 | Loss: 0.00001751
Iteration 83/1000 | Loss: 0.00001751
Iteration 84/1000 | Loss: 0.00001750
Iteration 85/1000 | Loss: 0.00001750
Iteration 86/1000 | Loss: 0.00001750
Iteration 87/1000 | Loss: 0.00001750
Iteration 88/1000 | Loss: 0.00001750
Iteration 89/1000 | Loss: 0.00001749
Iteration 90/1000 | Loss: 0.00001749
Iteration 91/1000 | Loss: 0.00001749
Iteration 92/1000 | Loss: 0.00001749
Iteration 93/1000 | Loss: 0.00001748
Iteration 94/1000 | Loss: 0.00001748
Iteration 95/1000 | Loss: 0.00001748
Iteration 96/1000 | Loss: 0.00001747
Iteration 97/1000 | Loss: 0.00001747
Iteration 98/1000 | Loss: 0.00001747
Iteration 99/1000 | Loss: 0.00001747
Iteration 100/1000 | Loss: 0.00001747
Iteration 101/1000 | Loss: 0.00001747
Iteration 102/1000 | Loss: 0.00001747
Iteration 103/1000 | Loss: 0.00001747
Iteration 104/1000 | Loss: 0.00001747
Iteration 105/1000 | Loss: 0.00001747
Iteration 106/1000 | Loss: 0.00001747
Iteration 107/1000 | Loss: 0.00001746
Iteration 108/1000 | Loss: 0.00001746
Iteration 109/1000 | Loss: 0.00001746
Iteration 110/1000 | Loss: 0.00001746
Iteration 111/1000 | Loss: 0.00001746
Iteration 112/1000 | Loss: 0.00001746
Iteration 113/1000 | Loss: 0.00001746
Iteration 114/1000 | Loss: 0.00001746
Iteration 115/1000 | Loss: 0.00001746
Iteration 116/1000 | Loss: 0.00001746
Iteration 117/1000 | Loss: 0.00001746
Iteration 118/1000 | Loss: 0.00001745
Iteration 119/1000 | Loss: 0.00001745
Iteration 120/1000 | Loss: 0.00001745
Iteration 121/1000 | Loss: 0.00001745
Iteration 122/1000 | Loss: 0.00001745
Iteration 123/1000 | Loss: 0.00001745
Iteration 124/1000 | Loss: 0.00001745
Iteration 125/1000 | Loss: 0.00001745
Iteration 126/1000 | Loss: 0.00001745
Iteration 127/1000 | Loss: 0.00001745
Iteration 128/1000 | Loss: 0.00001745
Iteration 129/1000 | Loss: 0.00001745
Iteration 130/1000 | Loss: 0.00001745
Iteration 131/1000 | Loss: 0.00001745
Iteration 132/1000 | Loss: 0.00001745
Iteration 133/1000 | Loss: 0.00001745
Iteration 134/1000 | Loss: 0.00001745
Iteration 135/1000 | Loss: 0.00001745
Iteration 136/1000 | Loss: 0.00001745
Iteration 137/1000 | Loss: 0.00001745
Iteration 138/1000 | Loss: 0.00001745
Iteration 139/1000 | Loss: 0.00001745
Iteration 140/1000 | Loss: 0.00001745
Iteration 141/1000 | Loss: 0.00001745
Iteration 142/1000 | Loss: 0.00001745
Iteration 143/1000 | Loss: 0.00001745
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 143. Stopping optimization.
Last 5 losses: [1.7451926396461204e-05, 1.7451926396461204e-05, 1.7451926396461204e-05, 1.7451926396461204e-05, 1.7451926396461204e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7451926396461204e-05

Optimization complete. Final v2v error: 3.4386606216430664 mm

Highest mean error: 6.287770748138428 mm for frame 1

Lowest mean error: 3.0764424800872803 mm for frame 111

Saving results

Total time: 85.32006430625916
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_006/1083/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_006/1083.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_006/1083
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01157937
Iteration 2/25 | Loss: 0.00168552
Iteration 3/25 | Loss: 0.00101660
Iteration 4/25 | Loss: 0.00094384
Iteration 5/25 | Loss: 0.00092810
Iteration 6/25 | Loss: 0.00092423
Iteration 7/25 | Loss: 0.00092359
Iteration 8/25 | Loss: 0.00092359
Iteration 9/25 | Loss: 0.00092359
Iteration 10/25 | Loss: 0.00092359
Iteration 11/25 | Loss: 0.00092359
Iteration 12/25 | Loss: 0.00092359
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0009235949837602675, 0.0009235949837602675, 0.0009235949837602675, 0.0009235949837602675, 0.0009235949837602675]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009235949837602675

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.06686187
Iteration 2/25 | Loss: 0.00034428
Iteration 3/25 | Loss: 0.00034426
Iteration 4/25 | Loss: 0.00034426
Iteration 5/25 | Loss: 0.00034426
Iteration 6/25 | Loss: 0.00034426
Iteration 7/25 | Loss: 0.00034426
Iteration 8/25 | Loss: 0.00034426
Iteration 9/25 | Loss: 0.00034425
Iteration 10/25 | Loss: 0.00034425
Iteration 11/25 | Loss: 0.00034425
Iteration 12/25 | Loss: 0.00034425
Iteration 13/25 | Loss: 0.00034425
Iteration 14/25 | Loss: 0.00034425
Iteration 15/25 | Loss: 0.00034425
Iteration 16/25 | Loss: 0.00034425
Iteration 17/25 | Loss: 0.00034425
Iteration 18/25 | Loss: 0.00034425
Iteration 19/25 | Loss: 0.00034425
Iteration 20/25 | Loss: 0.00034425
Iteration 21/25 | Loss: 0.00034425
Iteration 22/25 | Loss: 0.00034425
Iteration 23/25 | Loss: 0.00034425
Iteration 24/25 | Loss: 0.00034425
Iteration 25/25 | Loss: 0.00034425

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00034425
Iteration 2/1000 | Loss: 0.00005068
Iteration 3/1000 | Loss: 0.00003707
Iteration 4/1000 | Loss: 0.00003391
Iteration 5/1000 | Loss: 0.00003253
Iteration 6/1000 | Loss: 0.00003148
Iteration 7/1000 | Loss: 0.00003081
Iteration 8/1000 | Loss: 0.00003038
Iteration 9/1000 | Loss: 0.00003009
Iteration 10/1000 | Loss: 0.00002983
Iteration 11/1000 | Loss: 0.00002967
Iteration 12/1000 | Loss: 0.00002952
Iteration 13/1000 | Loss: 0.00002944
Iteration 14/1000 | Loss: 0.00002941
Iteration 15/1000 | Loss: 0.00002925
Iteration 16/1000 | Loss: 0.00002921
Iteration 17/1000 | Loss: 0.00002921
Iteration 18/1000 | Loss: 0.00002919
Iteration 19/1000 | Loss: 0.00002919
Iteration 20/1000 | Loss: 0.00002916
Iteration 21/1000 | Loss: 0.00002909
Iteration 22/1000 | Loss: 0.00002906
Iteration 23/1000 | Loss: 0.00002906
Iteration 24/1000 | Loss: 0.00002905
Iteration 25/1000 | Loss: 0.00002905
Iteration 26/1000 | Loss: 0.00002905
Iteration 27/1000 | Loss: 0.00002901
Iteration 28/1000 | Loss: 0.00002901
Iteration 29/1000 | Loss: 0.00002900
Iteration 30/1000 | Loss: 0.00002900
Iteration 31/1000 | Loss: 0.00002899
Iteration 32/1000 | Loss: 0.00002899
Iteration 33/1000 | Loss: 0.00002899
Iteration 34/1000 | Loss: 0.00002899
Iteration 35/1000 | Loss: 0.00002898
Iteration 36/1000 | Loss: 0.00002898
Iteration 37/1000 | Loss: 0.00002898
Iteration 38/1000 | Loss: 0.00002898
Iteration 39/1000 | Loss: 0.00002898
Iteration 40/1000 | Loss: 0.00002898
Iteration 41/1000 | Loss: 0.00002898
Iteration 42/1000 | Loss: 0.00002898
Iteration 43/1000 | Loss: 0.00002898
Iteration 44/1000 | Loss: 0.00002897
Iteration 45/1000 | Loss: 0.00002897
Iteration 46/1000 | Loss: 0.00002897
Iteration 47/1000 | Loss: 0.00002897
Iteration 48/1000 | Loss: 0.00002897
Iteration 49/1000 | Loss: 0.00002896
Iteration 50/1000 | Loss: 0.00002896
Iteration 51/1000 | Loss: 0.00002896
Iteration 52/1000 | Loss: 0.00002895
Iteration 53/1000 | Loss: 0.00002895
Iteration 54/1000 | Loss: 0.00002895
Iteration 55/1000 | Loss: 0.00002895
Iteration 56/1000 | Loss: 0.00002894
Iteration 57/1000 | Loss: 0.00002894
Iteration 58/1000 | Loss: 0.00002893
Iteration 59/1000 | Loss: 0.00002893
Iteration 60/1000 | Loss: 0.00002892
Iteration 61/1000 | Loss: 0.00002892
Iteration 62/1000 | Loss: 0.00002892
Iteration 63/1000 | Loss: 0.00002891
Iteration 64/1000 | Loss: 0.00002891
Iteration 65/1000 | Loss: 0.00002889
Iteration 66/1000 | Loss: 0.00002889
Iteration 67/1000 | Loss: 0.00002889
Iteration 68/1000 | Loss: 0.00002889
Iteration 69/1000 | Loss: 0.00002887
Iteration 70/1000 | Loss: 0.00002887
Iteration 71/1000 | Loss: 0.00002886
Iteration 72/1000 | Loss: 0.00002886
Iteration 73/1000 | Loss: 0.00002886
Iteration 74/1000 | Loss: 0.00002886
Iteration 75/1000 | Loss: 0.00002885
Iteration 76/1000 | Loss: 0.00002885
Iteration 77/1000 | Loss: 0.00002885
Iteration 78/1000 | Loss: 0.00002884
Iteration 79/1000 | Loss: 0.00002884
Iteration 80/1000 | Loss: 0.00002884
Iteration 81/1000 | Loss: 0.00002884
Iteration 82/1000 | Loss: 0.00002884
Iteration 83/1000 | Loss: 0.00002884
Iteration 84/1000 | Loss: 0.00002884
Iteration 85/1000 | Loss: 0.00002884
Iteration 86/1000 | Loss: 0.00002884
Iteration 87/1000 | Loss: 0.00002884
Iteration 88/1000 | Loss: 0.00002884
Iteration 89/1000 | Loss: 0.00002884
Iteration 90/1000 | Loss: 0.00002884
Iteration 91/1000 | Loss: 0.00002884
Iteration 92/1000 | Loss: 0.00002884
Iteration 93/1000 | Loss: 0.00002884
Iteration 94/1000 | Loss: 0.00002884
Iteration 95/1000 | Loss: 0.00002884
Iteration 96/1000 | Loss: 0.00002884
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 96. Stopping optimization.
Last 5 losses: [2.884093191823922e-05, 2.884093191823922e-05, 2.884093191823922e-05, 2.884093191823922e-05, 2.884093191823922e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.884093191823922e-05

Optimization complete. Final v2v error: 4.338498115539551 mm

Highest mean error: 5.601857662200928 mm for frame 73

Lowest mean error: 3.6718053817749023 mm for frame 33

Saving results

Total time: 44.598368644714355
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_006/1078/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_006/1078.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_006/1078
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01064475
Iteration 2/25 | Loss: 0.00219638
Iteration 3/25 | Loss: 0.00196054
Iteration 4/25 | Loss: 0.00101132
Iteration 5/25 | Loss: 0.00091445
Iteration 6/25 | Loss: 0.00091273
Iteration 7/25 | Loss: 0.00097163
Iteration 8/25 | Loss: 0.00088568
Iteration 9/25 | Loss: 0.00087350
Iteration 10/25 | Loss: 0.00087122
Iteration 11/25 | Loss: 0.00090215
Iteration 12/25 | Loss: 0.00088478
Iteration 13/25 | Loss: 0.00086556
Iteration 14/25 | Loss: 0.00085737
Iteration 15/25 | Loss: 0.00084784
Iteration 16/25 | Loss: 0.00084452
Iteration 17/25 | Loss: 0.00084325
Iteration 18/25 | Loss: 0.00084237
Iteration 19/25 | Loss: 0.00084528
Iteration 20/25 | Loss: 0.00084182
Iteration 21/25 | Loss: 0.00084001
Iteration 22/25 | Loss: 0.00083907
Iteration 23/25 | Loss: 0.00083873
Iteration 24/25 | Loss: 0.00083860
Iteration 25/25 | Loss: 0.00083858

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.73126936
Iteration 2/25 | Loss: 0.00061635
Iteration 3/25 | Loss: 0.00061635
Iteration 4/25 | Loss: 0.00061635
Iteration 5/25 | Loss: 0.00061635
Iteration 6/25 | Loss: 0.00061635
Iteration 7/25 | Loss: 0.00061635
Iteration 8/25 | Loss: 0.00061635
Iteration 9/25 | Loss: 0.00061635
Iteration 10/25 | Loss: 0.00061635
Iteration 11/25 | Loss: 0.00061635
Iteration 12/25 | Loss: 0.00061635
Iteration 13/25 | Loss: 0.00061635
Iteration 14/25 | Loss: 0.00061635
Iteration 15/25 | Loss: 0.00061635
Iteration 16/25 | Loss: 0.00061635
Iteration 17/25 | Loss: 0.00061635
Iteration 18/25 | Loss: 0.00061635
Iteration 19/25 | Loss: 0.00061635
Iteration 20/25 | Loss: 0.00061635
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0006163496291264892, 0.0006163496291264892, 0.0006163496291264892, 0.0006163496291264892, 0.0006163496291264892]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006163496291264892

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00061635
Iteration 2/1000 | Loss: 0.00003759
Iteration 3/1000 | Loss: 0.00050538
Iteration 4/1000 | Loss: 0.00003751
Iteration 5/1000 | Loss: 0.00002853
Iteration 6/1000 | Loss: 0.00002581
Iteration 7/1000 | Loss: 0.00002339
Iteration 8/1000 | Loss: 0.00002226
Iteration 9/1000 | Loss: 0.00002143
Iteration 10/1000 | Loss: 0.00002090
Iteration 11/1000 | Loss: 0.00002028
Iteration 12/1000 | Loss: 0.00001992
Iteration 13/1000 | Loss: 0.00001973
Iteration 14/1000 | Loss: 0.00001953
Iteration 15/1000 | Loss: 0.00001949
Iteration 16/1000 | Loss: 0.00001944
Iteration 17/1000 | Loss: 0.00001944
Iteration 18/1000 | Loss: 0.00001944
Iteration 19/1000 | Loss: 0.00001943
Iteration 20/1000 | Loss: 0.00001939
Iteration 21/1000 | Loss: 0.00001932
Iteration 22/1000 | Loss: 0.00001930
Iteration 23/1000 | Loss: 0.00001928
Iteration 24/1000 | Loss: 0.00001928
Iteration 25/1000 | Loss: 0.00001928
Iteration 26/1000 | Loss: 0.00001928
Iteration 27/1000 | Loss: 0.00001928
Iteration 28/1000 | Loss: 0.00001924
Iteration 29/1000 | Loss: 0.00001922
Iteration 30/1000 | Loss: 0.00001921
Iteration 31/1000 | Loss: 0.00001921
Iteration 32/1000 | Loss: 0.00001920
Iteration 33/1000 | Loss: 0.00001920
Iteration 34/1000 | Loss: 0.00001920
Iteration 35/1000 | Loss: 0.00001919
Iteration 36/1000 | Loss: 0.00001919
Iteration 37/1000 | Loss: 0.00001919
Iteration 38/1000 | Loss: 0.00001919
Iteration 39/1000 | Loss: 0.00001919
Iteration 40/1000 | Loss: 0.00001919
Iteration 41/1000 | Loss: 0.00001919
Iteration 42/1000 | Loss: 0.00001919
Iteration 43/1000 | Loss: 0.00001919
Iteration 44/1000 | Loss: 0.00001919
Iteration 45/1000 | Loss: 0.00001919
Iteration 46/1000 | Loss: 0.00001919
Iteration 47/1000 | Loss: 0.00001919
Iteration 48/1000 | Loss: 0.00001919
Iteration 49/1000 | Loss: 0.00001919
Iteration 50/1000 | Loss: 0.00001919
Iteration 51/1000 | Loss: 0.00001919
Iteration 52/1000 | Loss: 0.00001919
Iteration 53/1000 | Loss: 0.00001918
Iteration 54/1000 | Loss: 0.00001918
Iteration 55/1000 | Loss: 0.00001918
Iteration 56/1000 | Loss: 0.00001917
Iteration 57/1000 | Loss: 0.00001917
Iteration 58/1000 | Loss: 0.00001917
Iteration 59/1000 | Loss: 0.00001917
Iteration 60/1000 | Loss: 0.00001917
Iteration 61/1000 | Loss: 0.00001917
Iteration 62/1000 | Loss: 0.00001917
Iteration 63/1000 | Loss: 0.00001917
Iteration 64/1000 | Loss: 0.00001917
Iteration 65/1000 | Loss: 0.00001916
Iteration 66/1000 | Loss: 0.00001916
Iteration 67/1000 | Loss: 0.00001916
Iteration 68/1000 | Loss: 0.00001916
Iteration 69/1000 | Loss: 0.00001916
Iteration 70/1000 | Loss: 0.00001916
Iteration 71/1000 | Loss: 0.00001916
Iteration 72/1000 | Loss: 0.00001916
Iteration 73/1000 | Loss: 0.00001916
Iteration 74/1000 | Loss: 0.00001916
Iteration 75/1000 | Loss: 0.00001915
Iteration 76/1000 | Loss: 0.00001915
Iteration 77/1000 | Loss: 0.00001915
Iteration 78/1000 | Loss: 0.00001915
Iteration 79/1000 | Loss: 0.00001915
Iteration 80/1000 | Loss: 0.00001915
Iteration 81/1000 | Loss: 0.00001915
Iteration 82/1000 | Loss: 0.00001915
Iteration 83/1000 | Loss: 0.00001915
Iteration 84/1000 | Loss: 0.00001915
Iteration 85/1000 | Loss: 0.00001914
Iteration 86/1000 | Loss: 0.00001914
Iteration 87/1000 | Loss: 0.00001914
Iteration 88/1000 | Loss: 0.00001914
Iteration 89/1000 | Loss: 0.00001914
Iteration 90/1000 | Loss: 0.00001914
Iteration 91/1000 | Loss: 0.00001913
Iteration 92/1000 | Loss: 0.00001913
Iteration 93/1000 | Loss: 0.00001913
Iteration 94/1000 | Loss: 0.00001913
Iteration 95/1000 | Loss: 0.00001913
Iteration 96/1000 | Loss: 0.00001913
Iteration 97/1000 | Loss: 0.00001913
Iteration 98/1000 | Loss: 0.00001912
Iteration 99/1000 | Loss: 0.00001912
Iteration 100/1000 | Loss: 0.00001912
Iteration 101/1000 | Loss: 0.00001912
Iteration 102/1000 | Loss: 0.00001912
Iteration 103/1000 | Loss: 0.00001912
Iteration 104/1000 | Loss: 0.00001911
Iteration 105/1000 | Loss: 0.00001911
Iteration 106/1000 | Loss: 0.00001911
Iteration 107/1000 | Loss: 0.00001910
Iteration 108/1000 | Loss: 0.00001910
Iteration 109/1000 | Loss: 0.00001910
Iteration 110/1000 | Loss: 0.00001910
Iteration 111/1000 | Loss: 0.00001910
Iteration 112/1000 | Loss: 0.00001910
Iteration 113/1000 | Loss: 0.00001910
Iteration 114/1000 | Loss: 0.00001910
Iteration 115/1000 | Loss: 0.00001910
Iteration 116/1000 | Loss: 0.00001910
Iteration 117/1000 | Loss: 0.00001910
Iteration 118/1000 | Loss: 0.00001910
Iteration 119/1000 | Loss: 0.00001910
Iteration 120/1000 | Loss: 0.00001910
Iteration 121/1000 | Loss: 0.00001909
Iteration 122/1000 | Loss: 0.00001909
Iteration 123/1000 | Loss: 0.00001908
Iteration 124/1000 | Loss: 0.00001908
Iteration 125/1000 | Loss: 0.00001908
Iteration 126/1000 | Loss: 0.00001908
Iteration 127/1000 | Loss: 0.00001908
Iteration 128/1000 | Loss: 0.00001908
Iteration 129/1000 | Loss: 0.00001908
Iteration 130/1000 | Loss: 0.00001908
Iteration 131/1000 | Loss: 0.00001908
Iteration 132/1000 | Loss: 0.00001908
Iteration 133/1000 | Loss: 0.00001908
Iteration 134/1000 | Loss: 0.00001908
Iteration 135/1000 | Loss: 0.00001908
Iteration 136/1000 | Loss: 0.00001908
Iteration 137/1000 | Loss: 0.00001907
Iteration 138/1000 | Loss: 0.00001907
Iteration 139/1000 | Loss: 0.00001907
Iteration 140/1000 | Loss: 0.00001907
Iteration 141/1000 | Loss: 0.00001907
Iteration 142/1000 | Loss: 0.00001907
Iteration 143/1000 | Loss: 0.00001907
Iteration 144/1000 | Loss: 0.00001907
Iteration 145/1000 | Loss: 0.00001907
Iteration 146/1000 | Loss: 0.00001906
Iteration 147/1000 | Loss: 0.00001906
Iteration 148/1000 | Loss: 0.00001906
Iteration 149/1000 | Loss: 0.00001906
Iteration 150/1000 | Loss: 0.00001906
Iteration 151/1000 | Loss: 0.00001906
Iteration 152/1000 | Loss: 0.00001906
Iteration 153/1000 | Loss: 0.00001906
Iteration 154/1000 | Loss: 0.00001906
Iteration 155/1000 | Loss: 0.00001906
Iteration 156/1000 | Loss: 0.00001905
Iteration 157/1000 | Loss: 0.00001905
Iteration 158/1000 | Loss: 0.00001905
Iteration 159/1000 | Loss: 0.00001905
Iteration 160/1000 | Loss: 0.00001905
Iteration 161/1000 | Loss: 0.00001905
Iteration 162/1000 | Loss: 0.00001904
Iteration 163/1000 | Loss: 0.00001904
Iteration 164/1000 | Loss: 0.00001904
Iteration 165/1000 | Loss: 0.00001904
Iteration 166/1000 | Loss: 0.00001904
Iteration 167/1000 | Loss: 0.00001904
Iteration 168/1000 | Loss: 0.00001904
Iteration 169/1000 | Loss: 0.00001904
Iteration 170/1000 | Loss: 0.00001904
Iteration 171/1000 | Loss: 0.00001904
Iteration 172/1000 | Loss: 0.00001904
Iteration 173/1000 | Loss: 0.00001904
Iteration 174/1000 | Loss: 0.00001903
Iteration 175/1000 | Loss: 0.00001903
Iteration 176/1000 | Loss: 0.00001903
Iteration 177/1000 | Loss: 0.00001903
Iteration 178/1000 | Loss: 0.00001903
Iteration 179/1000 | Loss: 0.00001903
Iteration 180/1000 | Loss: 0.00001903
Iteration 181/1000 | Loss: 0.00001903
Iteration 182/1000 | Loss: 0.00001903
Iteration 183/1000 | Loss: 0.00001903
Iteration 184/1000 | Loss: 0.00001903
Iteration 185/1000 | Loss: 0.00001902
Iteration 186/1000 | Loss: 0.00001902
Iteration 187/1000 | Loss: 0.00001902
Iteration 188/1000 | Loss: 0.00001902
Iteration 189/1000 | Loss: 0.00001902
Iteration 190/1000 | Loss: 0.00001902
Iteration 191/1000 | Loss: 0.00001902
Iteration 192/1000 | Loss: 0.00001902
Iteration 193/1000 | Loss: 0.00001902
Iteration 194/1000 | Loss: 0.00001902
Iteration 195/1000 | Loss: 0.00001902
Iteration 196/1000 | Loss: 0.00001902
Iteration 197/1000 | Loss: 0.00001902
Iteration 198/1000 | Loss: 0.00001902
Iteration 199/1000 | Loss: 0.00001901
Iteration 200/1000 | Loss: 0.00001901
Iteration 201/1000 | Loss: 0.00001901
Iteration 202/1000 | Loss: 0.00001901
Iteration 203/1000 | Loss: 0.00001901
Iteration 204/1000 | Loss: 0.00001901
Iteration 205/1000 | Loss: 0.00001901
Iteration 206/1000 | Loss: 0.00001901
Iteration 207/1000 | Loss: 0.00001901
Iteration 208/1000 | Loss: 0.00001901
Iteration 209/1000 | Loss: 0.00001901
Iteration 210/1000 | Loss: 0.00001901
Iteration 211/1000 | Loss: 0.00001901
Iteration 212/1000 | Loss: 0.00001901
Iteration 213/1000 | Loss: 0.00001901
Iteration 214/1000 | Loss: 0.00001900
Iteration 215/1000 | Loss: 0.00001900
Iteration 216/1000 | Loss: 0.00001900
Iteration 217/1000 | Loss: 0.00001900
Iteration 218/1000 | Loss: 0.00001900
Iteration 219/1000 | Loss: 0.00001900
Iteration 220/1000 | Loss: 0.00001900
Iteration 221/1000 | Loss: 0.00001900
Iteration 222/1000 | Loss: 0.00001900
Iteration 223/1000 | Loss: 0.00001900
Iteration 224/1000 | Loss: 0.00001900
Iteration 225/1000 | Loss: 0.00001900
Iteration 226/1000 | Loss: 0.00001900
Iteration 227/1000 | Loss: 0.00001900
Iteration 228/1000 | Loss: 0.00001900
Iteration 229/1000 | Loss: 0.00001900
Iteration 230/1000 | Loss: 0.00001900
Iteration 231/1000 | Loss: 0.00001900
Iteration 232/1000 | Loss: 0.00001900
Iteration 233/1000 | Loss: 0.00001900
Iteration 234/1000 | Loss: 0.00001900
Iteration 235/1000 | Loss: 0.00001900
Iteration 236/1000 | Loss: 0.00001900
Iteration 237/1000 | Loss: 0.00001900
Iteration 238/1000 | Loss: 0.00001900
Iteration 239/1000 | Loss: 0.00001900
Iteration 240/1000 | Loss: 0.00001900
Iteration 241/1000 | Loss: 0.00001900
Iteration 242/1000 | Loss: 0.00001900
Iteration 243/1000 | Loss: 0.00001900
Iteration 244/1000 | Loss: 0.00001900
Iteration 245/1000 | Loss: 0.00001900
Iteration 246/1000 | Loss: 0.00001900
Iteration 247/1000 | Loss: 0.00001900
Iteration 248/1000 | Loss: 0.00001900
Iteration 249/1000 | Loss: 0.00001900
Iteration 250/1000 | Loss: 0.00001900
Iteration 251/1000 | Loss: 0.00001900
Iteration 252/1000 | Loss: 0.00001900
Iteration 253/1000 | Loss: 0.00001900
Iteration 254/1000 | Loss: 0.00001900
Iteration 255/1000 | Loss: 0.00001900
Iteration 256/1000 | Loss: 0.00001900
Iteration 257/1000 | Loss: 0.00001900
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 257. Stopping optimization.
Last 5 losses: [1.899615381262265e-05, 1.899615381262265e-05, 1.899615381262265e-05, 1.899615381262265e-05, 1.899615381262265e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.899615381262265e-05

Optimization complete. Final v2v error: 3.7112717628479004 mm

Highest mean error: 4.393795490264893 mm for frame 95

Lowest mean error: 3.0825893878936768 mm for frame 169

Saving results

Total time: 78.94083094596863
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_006/1028/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_006/1028.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_006/1028
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00455423
Iteration 2/25 | Loss: 0.00104669
Iteration 3/25 | Loss: 0.00082175
Iteration 4/25 | Loss: 0.00079262
Iteration 5/25 | Loss: 0.00078086
Iteration 6/25 | Loss: 0.00077871
Iteration 7/25 | Loss: 0.00077836
Iteration 8/25 | Loss: 0.00077836
Iteration 9/25 | Loss: 0.00077836
Iteration 10/25 | Loss: 0.00077836
Iteration 11/25 | Loss: 0.00077836
Iteration 12/25 | Loss: 0.00077836
Iteration 13/25 | Loss: 0.00077836
Iteration 14/25 | Loss: 0.00077836
Iteration 15/25 | Loss: 0.00077836
Iteration 16/25 | Loss: 0.00077836
Iteration 17/25 | Loss: 0.00077836
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0007783558685332537, 0.0007783558685332537, 0.0007783558685332537, 0.0007783558685332537, 0.0007783558685332537]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007783558685332537

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.49618697
Iteration 2/25 | Loss: 0.00046810
Iteration 3/25 | Loss: 0.00046810
Iteration 4/25 | Loss: 0.00046809
Iteration 5/25 | Loss: 0.00046809
Iteration 6/25 | Loss: 0.00046809
Iteration 7/25 | Loss: 0.00046809
Iteration 8/25 | Loss: 0.00046809
Iteration 9/25 | Loss: 0.00046809
Iteration 10/25 | Loss: 0.00046809
Iteration 11/25 | Loss: 0.00046809
Iteration 12/25 | Loss: 0.00046809
Iteration 13/25 | Loss: 0.00046809
Iteration 14/25 | Loss: 0.00046809
Iteration 15/25 | Loss: 0.00046809
Iteration 16/25 | Loss: 0.00046809
Iteration 17/25 | Loss: 0.00046809
Iteration 18/25 | Loss: 0.00046809
Iteration 19/25 | Loss: 0.00046809
Iteration 20/25 | Loss: 0.00046809
Iteration 21/25 | Loss: 0.00046809
Iteration 22/25 | Loss: 0.00046809
Iteration 23/25 | Loss: 0.00046809
Iteration 24/25 | Loss: 0.00046809
Iteration 25/25 | Loss: 0.00046809

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00046809
Iteration 2/1000 | Loss: 0.00002633
Iteration 3/1000 | Loss: 0.00002090
Iteration 4/1000 | Loss: 0.00001972
Iteration 5/1000 | Loss: 0.00001888
Iteration 6/1000 | Loss: 0.00001839
Iteration 7/1000 | Loss: 0.00001807
Iteration 8/1000 | Loss: 0.00001797
Iteration 9/1000 | Loss: 0.00001776
Iteration 10/1000 | Loss: 0.00001761
Iteration 11/1000 | Loss: 0.00001760
Iteration 12/1000 | Loss: 0.00001758
Iteration 13/1000 | Loss: 0.00001754
Iteration 14/1000 | Loss: 0.00001751
Iteration 15/1000 | Loss: 0.00001749
Iteration 16/1000 | Loss: 0.00001747
Iteration 17/1000 | Loss: 0.00001741
Iteration 18/1000 | Loss: 0.00001741
Iteration 19/1000 | Loss: 0.00001741
Iteration 20/1000 | Loss: 0.00001741
Iteration 21/1000 | Loss: 0.00001740
Iteration 22/1000 | Loss: 0.00001739
Iteration 23/1000 | Loss: 0.00001738
Iteration 24/1000 | Loss: 0.00001738
Iteration 25/1000 | Loss: 0.00001738
Iteration 26/1000 | Loss: 0.00001738
Iteration 27/1000 | Loss: 0.00001738
Iteration 28/1000 | Loss: 0.00001738
Iteration 29/1000 | Loss: 0.00001737
Iteration 30/1000 | Loss: 0.00001737
Iteration 31/1000 | Loss: 0.00001737
Iteration 32/1000 | Loss: 0.00001737
Iteration 33/1000 | Loss: 0.00001737
Iteration 34/1000 | Loss: 0.00001736
Iteration 35/1000 | Loss: 0.00001736
Iteration 36/1000 | Loss: 0.00001736
Iteration 37/1000 | Loss: 0.00001735
Iteration 38/1000 | Loss: 0.00001735
Iteration 39/1000 | Loss: 0.00001735
Iteration 40/1000 | Loss: 0.00001735
Iteration 41/1000 | Loss: 0.00001735
Iteration 42/1000 | Loss: 0.00001735
Iteration 43/1000 | Loss: 0.00001735
Iteration 44/1000 | Loss: 0.00001734
Iteration 45/1000 | Loss: 0.00001734
Iteration 46/1000 | Loss: 0.00001734
Iteration 47/1000 | Loss: 0.00001734
Iteration 48/1000 | Loss: 0.00001733
Iteration 49/1000 | Loss: 0.00001733
Iteration 50/1000 | Loss: 0.00001732
Iteration 51/1000 | Loss: 0.00001732
Iteration 52/1000 | Loss: 0.00001732
Iteration 53/1000 | Loss: 0.00001732
Iteration 54/1000 | Loss: 0.00001732
Iteration 55/1000 | Loss: 0.00001732
Iteration 56/1000 | Loss: 0.00001732
Iteration 57/1000 | Loss: 0.00001732
Iteration 58/1000 | Loss: 0.00001732
Iteration 59/1000 | Loss: 0.00001732
Iteration 60/1000 | Loss: 0.00001731
Iteration 61/1000 | Loss: 0.00001730
Iteration 62/1000 | Loss: 0.00001730
Iteration 63/1000 | Loss: 0.00001730
Iteration 64/1000 | Loss: 0.00001729
Iteration 65/1000 | Loss: 0.00001729
Iteration 66/1000 | Loss: 0.00001729
Iteration 67/1000 | Loss: 0.00001728
Iteration 68/1000 | Loss: 0.00001727
Iteration 69/1000 | Loss: 0.00001726
Iteration 70/1000 | Loss: 0.00001726
Iteration 71/1000 | Loss: 0.00001725
Iteration 72/1000 | Loss: 0.00001725
Iteration 73/1000 | Loss: 0.00001725
Iteration 74/1000 | Loss: 0.00001725
Iteration 75/1000 | Loss: 0.00001725
Iteration 76/1000 | Loss: 0.00001725
Iteration 77/1000 | Loss: 0.00001724
Iteration 78/1000 | Loss: 0.00001724
Iteration 79/1000 | Loss: 0.00001724
Iteration 80/1000 | Loss: 0.00001724
Iteration 81/1000 | Loss: 0.00001724
Iteration 82/1000 | Loss: 0.00001724
Iteration 83/1000 | Loss: 0.00001724
Iteration 84/1000 | Loss: 0.00001724
Iteration 85/1000 | Loss: 0.00001724
Iteration 86/1000 | Loss: 0.00001723
Iteration 87/1000 | Loss: 0.00001723
Iteration 88/1000 | Loss: 0.00001723
Iteration 89/1000 | Loss: 0.00001722
Iteration 90/1000 | Loss: 0.00001722
Iteration 91/1000 | Loss: 0.00001721
Iteration 92/1000 | Loss: 0.00001721
Iteration 93/1000 | Loss: 0.00001721
Iteration 94/1000 | Loss: 0.00001721
Iteration 95/1000 | Loss: 0.00001720
Iteration 96/1000 | Loss: 0.00001720
Iteration 97/1000 | Loss: 0.00001720
Iteration 98/1000 | Loss: 0.00001719
Iteration 99/1000 | Loss: 0.00001719
Iteration 100/1000 | Loss: 0.00001719
Iteration 101/1000 | Loss: 0.00001719
Iteration 102/1000 | Loss: 0.00001719
Iteration 103/1000 | Loss: 0.00001718
Iteration 104/1000 | Loss: 0.00001718
Iteration 105/1000 | Loss: 0.00001718
Iteration 106/1000 | Loss: 0.00001718
Iteration 107/1000 | Loss: 0.00001718
Iteration 108/1000 | Loss: 0.00001718
Iteration 109/1000 | Loss: 0.00001718
Iteration 110/1000 | Loss: 0.00001718
Iteration 111/1000 | Loss: 0.00001718
Iteration 112/1000 | Loss: 0.00001718
Iteration 113/1000 | Loss: 0.00001718
Iteration 114/1000 | Loss: 0.00001718
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 114. Stopping optimization.
Last 5 losses: [1.7179943824885413e-05, 1.7179943824885413e-05, 1.7179943824885413e-05, 1.7179943824885413e-05, 1.7179943824885413e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7179943824885413e-05

Optimization complete. Final v2v error: 3.4862661361694336 mm

Highest mean error: 3.895453453063965 mm for frame 23

Lowest mean error: 3.163452386856079 mm for frame 108

Saving results

Total time: 37.24798798561096
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_006/1071/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_006/1071.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_006/1071
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00837530
Iteration 2/25 | Loss: 0.00106485
Iteration 3/25 | Loss: 0.00086940
Iteration 4/25 | Loss: 0.00083142
Iteration 5/25 | Loss: 0.00082087
Iteration 6/25 | Loss: 0.00081702
Iteration 7/25 | Loss: 0.00081569
Iteration 8/25 | Loss: 0.00081892
Iteration 9/25 | Loss: 0.00082200
Iteration 10/25 | Loss: 0.00081721
Iteration 11/25 | Loss: 0.00081680
Iteration 12/25 | Loss: 0.00081488
Iteration 13/25 | Loss: 0.00081348
Iteration 14/25 | Loss: 0.00081328
Iteration 15/25 | Loss: 0.00081344
Iteration 16/25 | Loss: 0.00081360
Iteration 17/25 | Loss: 0.00081189
Iteration 18/25 | Loss: 0.00081140
Iteration 19/25 | Loss: 0.00081212
Iteration 20/25 | Loss: 0.00081163
Iteration 21/25 | Loss: 0.00080975
Iteration 22/25 | Loss: 0.00081265
Iteration 23/25 | Loss: 0.00081311
Iteration 24/25 | Loss: 0.00081263
Iteration 25/25 | Loss: 0.00081268

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.50515604
Iteration 2/25 | Loss: 0.00156231
Iteration 3/25 | Loss: 0.00156230
Iteration 4/25 | Loss: 0.00156230
Iteration 5/25 | Loss: 0.00156230
Iteration 6/25 | Loss: 0.00156230
Iteration 7/25 | Loss: 0.00156230
Iteration 8/25 | Loss: 0.00156230
Iteration 9/25 | Loss: 0.00156230
Iteration 10/25 | Loss: 0.00156230
Iteration 11/25 | Loss: 0.00156230
Iteration 12/25 | Loss: 0.00156230
Iteration 13/25 | Loss: 0.00156230
Iteration 14/25 | Loss: 0.00156230
Iteration 15/25 | Loss: 0.00156230
Iteration 16/25 | Loss: 0.00156230
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0015622994396835566, 0.0015622994396835566, 0.0015622994396835566, 0.0015622994396835566, 0.0015622994396835566]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015622994396835566

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00156230
Iteration 2/1000 | Loss: 0.00044243
Iteration 3/1000 | Loss: 0.00033138
Iteration 4/1000 | Loss: 0.00023241
Iteration 5/1000 | Loss: 0.00021654
Iteration 6/1000 | Loss: 0.00032433
Iteration 7/1000 | Loss: 0.00017032
Iteration 8/1000 | Loss: 0.00015228
Iteration 9/1000 | Loss: 0.00009095
Iteration 10/1000 | Loss: 0.00017619
Iteration 11/1000 | Loss: 0.00010379
Iteration 12/1000 | Loss: 0.00012520
Iteration 13/1000 | Loss: 0.00018033
Iteration 14/1000 | Loss: 0.00011146
Iteration 15/1000 | Loss: 0.00007078
Iteration 16/1000 | Loss: 0.00032685
Iteration 17/1000 | Loss: 0.00028132
Iteration 18/1000 | Loss: 0.00036206
Iteration 19/1000 | Loss: 0.00007302
Iteration 20/1000 | Loss: 0.00030810
Iteration 21/1000 | Loss: 0.00017847
Iteration 22/1000 | Loss: 0.00029714
Iteration 23/1000 | Loss: 0.00016755
Iteration 24/1000 | Loss: 0.00030320
Iteration 25/1000 | Loss: 0.00025991
Iteration 26/1000 | Loss: 0.00027815
Iteration 27/1000 | Loss: 0.00026421
Iteration 28/1000 | Loss: 0.00028089
Iteration 29/1000 | Loss: 0.00020490
Iteration 30/1000 | Loss: 0.00024004
Iteration 31/1000 | Loss: 0.00007161
Iteration 32/1000 | Loss: 0.00019704
Iteration 33/1000 | Loss: 0.00006453
Iteration 34/1000 | Loss: 0.00006061
Iteration 35/1000 | Loss: 0.00005934
Iteration 36/1000 | Loss: 0.00005855
Iteration 37/1000 | Loss: 0.00005802
Iteration 38/1000 | Loss: 0.00005726
Iteration 39/1000 | Loss: 0.00005660
Iteration 40/1000 | Loss: 0.00005606
Iteration 41/1000 | Loss: 0.00005810
Iteration 42/1000 | Loss: 0.00005541
Iteration 43/1000 | Loss: 0.00005494
Iteration 44/1000 | Loss: 0.00005448
Iteration 45/1000 | Loss: 0.00127108
Iteration 46/1000 | Loss: 0.00062853
Iteration 47/1000 | Loss: 0.00005719
Iteration 48/1000 | Loss: 0.00005327
Iteration 49/1000 | Loss: 0.00005133
Iteration 50/1000 | Loss: 0.00005002
Iteration 51/1000 | Loss: 0.00004901
Iteration 52/1000 | Loss: 0.00004827
Iteration 53/1000 | Loss: 0.00004780
Iteration 54/1000 | Loss: 0.00004743
Iteration 55/1000 | Loss: 0.00004713
Iteration 56/1000 | Loss: 0.00004687
Iteration 57/1000 | Loss: 0.00004663
Iteration 58/1000 | Loss: 0.00004648
Iteration 59/1000 | Loss: 0.00004635
Iteration 60/1000 | Loss: 0.00004632
Iteration 61/1000 | Loss: 0.00004622
Iteration 62/1000 | Loss: 0.00004615
Iteration 63/1000 | Loss: 0.00004611
Iteration 64/1000 | Loss: 0.00004610
Iteration 65/1000 | Loss: 0.00004609
Iteration 66/1000 | Loss: 0.00004607
Iteration 67/1000 | Loss: 0.00004607
Iteration 68/1000 | Loss: 0.00004606
Iteration 69/1000 | Loss: 0.00004604
Iteration 70/1000 | Loss: 0.00004604
Iteration 71/1000 | Loss: 0.00004604
Iteration 72/1000 | Loss: 0.00004604
Iteration 73/1000 | Loss: 0.00004604
Iteration 74/1000 | Loss: 0.00004604
Iteration 75/1000 | Loss: 0.00004604
Iteration 76/1000 | Loss: 0.00004604
Iteration 77/1000 | Loss: 0.00004604
Iteration 78/1000 | Loss: 0.00004604
Iteration 79/1000 | Loss: 0.00004602
Iteration 80/1000 | Loss: 0.00004602
Iteration 81/1000 | Loss: 0.00004602
Iteration 82/1000 | Loss: 0.00004601
Iteration 83/1000 | Loss: 0.00004601
Iteration 84/1000 | Loss: 0.00004600
Iteration 85/1000 | Loss: 0.00004599
Iteration 86/1000 | Loss: 0.00004599
Iteration 87/1000 | Loss: 0.00004599
Iteration 88/1000 | Loss: 0.00004598
Iteration 89/1000 | Loss: 0.00004597
Iteration 90/1000 | Loss: 0.00004597
Iteration 91/1000 | Loss: 0.00004597
Iteration 92/1000 | Loss: 0.00004597
Iteration 93/1000 | Loss: 0.00004596
Iteration 94/1000 | Loss: 0.00004596
Iteration 95/1000 | Loss: 0.00004596
Iteration 96/1000 | Loss: 0.00004595
Iteration 97/1000 | Loss: 0.00004595
Iteration 98/1000 | Loss: 0.00004594
Iteration 99/1000 | Loss: 0.00004594
Iteration 100/1000 | Loss: 0.00004594
Iteration 101/1000 | Loss: 0.00004594
Iteration 102/1000 | Loss: 0.00004594
Iteration 103/1000 | Loss: 0.00004594
Iteration 104/1000 | Loss: 0.00004593
Iteration 105/1000 | Loss: 0.00004593
Iteration 106/1000 | Loss: 0.00004593
Iteration 107/1000 | Loss: 0.00004593
Iteration 108/1000 | Loss: 0.00004592
Iteration 109/1000 | Loss: 0.00004592
Iteration 110/1000 | Loss: 0.00004592
Iteration 111/1000 | Loss: 0.00004592
Iteration 112/1000 | Loss: 0.00004592
Iteration 113/1000 | Loss: 0.00004592
Iteration 114/1000 | Loss: 0.00004591
Iteration 115/1000 | Loss: 0.00004591
Iteration 116/1000 | Loss: 0.00004591
Iteration 117/1000 | Loss: 0.00004590
Iteration 118/1000 | Loss: 0.00004590
Iteration 119/1000 | Loss: 0.00004590
Iteration 120/1000 | Loss: 0.00004589
Iteration 121/1000 | Loss: 0.00004589
Iteration 122/1000 | Loss: 0.00004589
Iteration 123/1000 | Loss: 0.00004588
Iteration 124/1000 | Loss: 0.00004588
Iteration 125/1000 | Loss: 0.00004588
Iteration 126/1000 | Loss: 0.00004588
Iteration 127/1000 | Loss: 0.00004587
Iteration 128/1000 | Loss: 0.00004587
Iteration 129/1000 | Loss: 0.00004587
Iteration 130/1000 | Loss: 0.00004587
Iteration 131/1000 | Loss: 0.00004587
Iteration 132/1000 | Loss: 0.00004587
Iteration 133/1000 | Loss: 0.00004587
Iteration 134/1000 | Loss: 0.00004587
Iteration 135/1000 | Loss: 0.00004587
Iteration 136/1000 | Loss: 0.00004587
Iteration 137/1000 | Loss: 0.00004587
Iteration 138/1000 | Loss: 0.00004587
Iteration 139/1000 | Loss: 0.00004587
Iteration 140/1000 | Loss: 0.00004586
Iteration 141/1000 | Loss: 0.00004585
Iteration 142/1000 | Loss: 0.00004585
Iteration 143/1000 | Loss: 0.00004585
Iteration 144/1000 | Loss: 0.00004584
Iteration 145/1000 | Loss: 0.00004584
Iteration 146/1000 | Loss: 0.00004584
Iteration 147/1000 | Loss: 0.00004583
Iteration 148/1000 | Loss: 0.00004583
Iteration 149/1000 | Loss: 0.00004583
Iteration 150/1000 | Loss: 0.00004583
Iteration 151/1000 | Loss: 0.00004583
Iteration 152/1000 | Loss: 0.00004583
Iteration 153/1000 | Loss: 0.00004582
Iteration 154/1000 | Loss: 0.00004582
Iteration 155/1000 | Loss: 0.00004582
Iteration 156/1000 | Loss: 0.00004581
Iteration 157/1000 | Loss: 0.00004581
Iteration 158/1000 | Loss: 0.00004581
Iteration 159/1000 | Loss: 0.00004581
Iteration 160/1000 | Loss: 0.00004581
Iteration 161/1000 | Loss: 0.00004581
Iteration 162/1000 | Loss: 0.00004581
Iteration 163/1000 | Loss: 0.00004581
Iteration 164/1000 | Loss: 0.00004581
Iteration 165/1000 | Loss: 0.00004581
Iteration 166/1000 | Loss: 0.00004580
Iteration 167/1000 | Loss: 0.00004580
Iteration 168/1000 | Loss: 0.00004580
Iteration 169/1000 | Loss: 0.00004580
Iteration 170/1000 | Loss: 0.00004580
Iteration 171/1000 | Loss: 0.00004580
Iteration 172/1000 | Loss: 0.00004580
Iteration 173/1000 | Loss: 0.00004580
Iteration 174/1000 | Loss: 0.00004580
Iteration 175/1000 | Loss: 0.00004580
Iteration 176/1000 | Loss: 0.00004580
Iteration 177/1000 | Loss: 0.00004580
Iteration 178/1000 | Loss: 0.00004580
Iteration 179/1000 | Loss: 0.00004580
Iteration 180/1000 | Loss: 0.00004580
Iteration 181/1000 | Loss: 0.00004580
Iteration 182/1000 | Loss: 0.00004579
Iteration 183/1000 | Loss: 0.00004579
Iteration 184/1000 | Loss: 0.00004579
Iteration 185/1000 | Loss: 0.00004579
Iteration 186/1000 | Loss: 0.00004579
Iteration 187/1000 | Loss: 0.00004579
Iteration 188/1000 | Loss: 0.00004579
Iteration 189/1000 | Loss: 0.00004579
Iteration 190/1000 | Loss: 0.00004579
Iteration 191/1000 | Loss: 0.00004579
Iteration 192/1000 | Loss: 0.00004579
Iteration 193/1000 | Loss: 0.00004579
Iteration 194/1000 | Loss: 0.00004579
Iteration 195/1000 | Loss: 0.00004579
Iteration 196/1000 | Loss: 0.00004579
Iteration 197/1000 | Loss: 0.00004579
Iteration 198/1000 | Loss: 0.00004579
Iteration 199/1000 | Loss: 0.00004579
Iteration 200/1000 | Loss: 0.00004579
Iteration 201/1000 | Loss: 0.00004579
Iteration 202/1000 | Loss: 0.00004579
Iteration 203/1000 | Loss: 0.00004579
Iteration 204/1000 | Loss: 0.00004579
Iteration 205/1000 | Loss: 0.00004579
Iteration 206/1000 | Loss: 0.00004579
Iteration 207/1000 | Loss: 0.00004579
Iteration 208/1000 | Loss: 0.00004579
Iteration 209/1000 | Loss: 0.00004579
Iteration 210/1000 | Loss: 0.00004579
Iteration 211/1000 | Loss: 0.00004579
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 211. Stopping optimization.
Last 5 losses: [4.5793760364176705e-05, 4.5793760364176705e-05, 4.5793760364176705e-05, 4.5793760364176705e-05, 4.5793760364176705e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.5793760364176705e-05

Optimization complete. Final v2v error: 3.3518943786621094 mm

Highest mean error: 11.53426456451416 mm for frame 90

Lowest mean error: 2.4584310054779053 mm for frame 52

Saving results

Total time: 148.52154397964478
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_006/1085/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_006/1085.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_006/1085
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00565547
Iteration 2/25 | Loss: 0.00130451
Iteration 3/25 | Loss: 0.00083852
Iteration 4/25 | Loss: 0.00078834
Iteration 5/25 | Loss: 0.00077823
Iteration 6/25 | Loss: 0.00077518
Iteration 7/25 | Loss: 0.00077414
Iteration 8/25 | Loss: 0.00077388
Iteration 9/25 | Loss: 0.00077388
Iteration 10/25 | Loss: 0.00077388
Iteration 11/25 | Loss: 0.00077388
Iteration 12/25 | Loss: 0.00077388
Iteration 13/25 | Loss: 0.00077388
Iteration 14/25 | Loss: 0.00077388
Iteration 15/25 | Loss: 0.00077388
Iteration 16/25 | Loss: 0.00077388
Iteration 17/25 | Loss: 0.00077388
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0007738778367638588, 0.0007738778367638588, 0.0007738778367638588, 0.0007738778367638588, 0.0007738778367638588]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007738778367638588

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.54018259
Iteration 2/25 | Loss: 0.00051016
Iteration 3/25 | Loss: 0.00051005
Iteration 4/25 | Loss: 0.00051005
Iteration 5/25 | Loss: 0.00051005
Iteration 6/25 | Loss: 0.00051005
Iteration 7/25 | Loss: 0.00051005
Iteration 8/25 | Loss: 0.00051005
Iteration 9/25 | Loss: 0.00051005
Iteration 10/25 | Loss: 0.00051005
Iteration 11/25 | Loss: 0.00051005
Iteration 12/25 | Loss: 0.00051005
Iteration 13/25 | Loss: 0.00051005
Iteration 14/25 | Loss: 0.00051005
Iteration 15/25 | Loss: 0.00051005
Iteration 16/25 | Loss: 0.00051005
Iteration 17/25 | Loss: 0.00051005
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0005100509733892977, 0.0005100509733892977, 0.0005100509733892977, 0.0005100509733892977, 0.0005100509733892977]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005100509733892977

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00051005
Iteration 2/1000 | Loss: 0.00002770
Iteration 3/1000 | Loss: 0.00001896
Iteration 4/1000 | Loss: 0.00001746
Iteration 5/1000 | Loss: 0.00001669
Iteration 6/1000 | Loss: 0.00001604
Iteration 7/1000 | Loss: 0.00001563
Iteration 8/1000 | Loss: 0.00001532
Iteration 9/1000 | Loss: 0.00001504
Iteration 10/1000 | Loss: 0.00001494
Iteration 11/1000 | Loss: 0.00001492
Iteration 12/1000 | Loss: 0.00001483
Iteration 13/1000 | Loss: 0.00001482
Iteration 14/1000 | Loss: 0.00001481
Iteration 15/1000 | Loss: 0.00001480
Iteration 16/1000 | Loss: 0.00001477
Iteration 17/1000 | Loss: 0.00001474
Iteration 18/1000 | Loss: 0.00001474
Iteration 19/1000 | Loss: 0.00001474
Iteration 20/1000 | Loss: 0.00001473
Iteration 21/1000 | Loss: 0.00001472
Iteration 22/1000 | Loss: 0.00001472
Iteration 23/1000 | Loss: 0.00001471
Iteration 24/1000 | Loss: 0.00001471
Iteration 25/1000 | Loss: 0.00001470
Iteration 26/1000 | Loss: 0.00001470
Iteration 27/1000 | Loss: 0.00001470
Iteration 28/1000 | Loss: 0.00001470
Iteration 29/1000 | Loss: 0.00001469
Iteration 30/1000 | Loss: 0.00001469
Iteration 31/1000 | Loss: 0.00001469
Iteration 32/1000 | Loss: 0.00001467
Iteration 33/1000 | Loss: 0.00001466
Iteration 34/1000 | Loss: 0.00001466
Iteration 35/1000 | Loss: 0.00001465
Iteration 36/1000 | Loss: 0.00001464
Iteration 37/1000 | Loss: 0.00001463
Iteration 38/1000 | Loss: 0.00001463
Iteration 39/1000 | Loss: 0.00001463
Iteration 40/1000 | Loss: 0.00001461
Iteration 41/1000 | Loss: 0.00001461
Iteration 42/1000 | Loss: 0.00001461
Iteration 43/1000 | Loss: 0.00001461
Iteration 44/1000 | Loss: 0.00001461
Iteration 45/1000 | Loss: 0.00001460
Iteration 46/1000 | Loss: 0.00001460
Iteration 47/1000 | Loss: 0.00001459
Iteration 48/1000 | Loss: 0.00001459
Iteration 49/1000 | Loss: 0.00001459
Iteration 50/1000 | Loss: 0.00001459
Iteration 51/1000 | Loss: 0.00001459
Iteration 52/1000 | Loss: 0.00001459
Iteration 53/1000 | Loss: 0.00001459
Iteration 54/1000 | Loss: 0.00001459
Iteration 55/1000 | Loss: 0.00001459
Iteration 56/1000 | Loss: 0.00001458
Iteration 57/1000 | Loss: 0.00001458
Iteration 58/1000 | Loss: 0.00001458
Iteration 59/1000 | Loss: 0.00001458
Iteration 60/1000 | Loss: 0.00001458
Iteration 61/1000 | Loss: 0.00001458
Iteration 62/1000 | Loss: 0.00001457
Iteration 63/1000 | Loss: 0.00001457
Iteration 64/1000 | Loss: 0.00001457
Iteration 65/1000 | Loss: 0.00001456
Iteration 66/1000 | Loss: 0.00001456
Iteration 67/1000 | Loss: 0.00001456
Iteration 68/1000 | Loss: 0.00001456
Iteration 69/1000 | Loss: 0.00001456
Iteration 70/1000 | Loss: 0.00001456
Iteration 71/1000 | Loss: 0.00001456
Iteration 72/1000 | Loss: 0.00001456
Iteration 73/1000 | Loss: 0.00001456
Iteration 74/1000 | Loss: 0.00001456
Iteration 75/1000 | Loss: 0.00001456
Iteration 76/1000 | Loss: 0.00001456
Iteration 77/1000 | Loss: 0.00001455
Iteration 78/1000 | Loss: 0.00001455
Iteration 79/1000 | Loss: 0.00001455
Iteration 80/1000 | Loss: 0.00001455
Iteration 81/1000 | Loss: 0.00001455
Iteration 82/1000 | Loss: 0.00001455
Iteration 83/1000 | Loss: 0.00001455
Iteration 84/1000 | Loss: 0.00001455
Iteration 85/1000 | Loss: 0.00001455
Iteration 86/1000 | Loss: 0.00001455
Iteration 87/1000 | Loss: 0.00001455
Iteration 88/1000 | Loss: 0.00001454
Iteration 89/1000 | Loss: 0.00001454
Iteration 90/1000 | Loss: 0.00001454
Iteration 91/1000 | Loss: 0.00001454
Iteration 92/1000 | Loss: 0.00001454
Iteration 93/1000 | Loss: 0.00001454
Iteration 94/1000 | Loss: 0.00001454
Iteration 95/1000 | Loss: 0.00001454
Iteration 96/1000 | Loss: 0.00001454
Iteration 97/1000 | Loss: 0.00001454
Iteration 98/1000 | Loss: 0.00001454
Iteration 99/1000 | Loss: 0.00001453
Iteration 100/1000 | Loss: 0.00001453
Iteration 101/1000 | Loss: 0.00001453
Iteration 102/1000 | Loss: 0.00001453
Iteration 103/1000 | Loss: 0.00001453
Iteration 104/1000 | Loss: 0.00001453
Iteration 105/1000 | Loss: 0.00001453
Iteration 106/1000 | Loss: 0.00001453
Iteration 107/1000 | Loss: 0.00001453
Iteration 108/1000 | Loss: 0.00001453
Iteration 109/1000 | Loss: 0.00001453
Iteration 110/1000 | Loss: 0.00001453
Iteration 111/1000 | Loss: 0.00001453
Iteration 112/1000 | Loss: 0.00001453
Iteration 113/1000 | Loss: 0.00001453
Iteration 114/1000 | Loss: 0.00001452
Iteration 115/1000 | Loss: 0.00001452
Iteration 116/1000 | Loss: 0.00001452
Iteration 117/1000 | Loss: 0.00001452
Iteration 118/1000 | Loss: 0.00001452
Iteration 119/1000 | Loss: 0.00001452
Iteration 120/1000 | Loss: 0.00001452
Iteration 121/1000 | Loss: 0.00001452
Iteration 122/1000 | Loss: 0.00001452
Iteration 123/1000 | Loss: 0.00001452
Iteration 124/1000 | Loss: 0.00001452
Iteration 125/1000 | Loss: 0.00001451
Iteration 126/1000 | Loss: 0.00001451
Iteration 127/1000 | Loss: 0.00001451
Iteration 128/1000 | Loss: 0.00001451
Iteration 129/1000 | Loss: 0.00001451
Iteration 130/1000 | Loss: 0.00001451
Iteration 131/1000 | Loss: 0.00001451
Iteration 132/1000 | Loss: 0.00001451
Iteration 133/1000 | Loss: 0.00001451
Iteration 134/1000 | Loss: 0.00001451
Iteration 135/1000 | Loss: 0.00001451
Iteration 136/1000 | Loss: 0.00001451
Iteration 137/1000 | Loss: 0.00001451
Iteration 138/1000 | Loss: 0.00001451
Iteration 139/1000 | Loss: 0.00001451
Iteration 140/1000 | Loss: 0.00001451
Iteration 141/1000 | Loss: 0.00001450
Iteration 142/1000 | Loss: 0.00001450
Iteration 143/1000 | Loss: 0.00001450
Iteration 144/1000 | Loss: 0.00001450
Iteration 145/1000 | Loss: 0.00001450
Iteration 146/1000 | Loss: 0.00001450
Iteration 147/1000 | Loss: 0.00001450
Iteration 148/1000 | Loss: 0.00001450
Iteration 149/1000 | Loss: 0.00001450
Iteration 150/1000 | Loss: 0.00001450
Iteration 151/1000 | Loss: 0.00001450
Iteration 152/1000 | Loss: 0.00001450
Iteration 153/1000 | Loss: 0.00001450
Iteration 154/1000 | Loss: 0.00001450
Iteration 155/1000 | Loss: 0.00001450
Iteration 156/1000 | Loss: 0.00001450
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 156. Stopping optimization.
Last 5 losses: [1.4497074516839348e-05, 1.4497074516839348e-05, 1.4497074516839348e-05, 1.4497074516839348e-05, 1.4497074516839348e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4497074516839348e-05

Optimization complete. Final v2v error: 3.1477227210998535 mm

Highest mean error: 4.79826021194458 mm for frame 185

Lowest mean error: 2.5845181941986084 mm for frame 147

Saving results

Total time: 42.12317776679993
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_006/1045/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_006/1045.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_006/1045
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01227546
Iteration 2/25 | Loss: 0.00322064
Iteration 3/25 | Loss: 0.00183292
Iteration 4/25 | Loss: 0.00165867
Iteration 5/25 | Loss: 0.00161425
Iteration 6/25 | Loss: 0.00194738
Iteration 7/25 | Loss: 0.00154121
Iteration 8/25 | Loss: 0.00146879
Iteration 9/25 | Loss: 0.00147161
Iteration 10/25 | Loss: 0.00149383
Iteration 11/25 | Loss: 0.00144064
Iteration 12/25 | Loss: 0.00135411
Iteration 13/25 | Loss: 0.00132273
Iteration 14/25 | Loss: 0.00131687
Iteration 15/25 | Loss: 0.00131465
Iteration 16/25 | Loss: 0.00131327
Iteration 17/25 | Loss: 0.00131257
Iteration 18/25 | Loss: 0.00131228
Iteration 19/25 | Loss: 0.00131206
Iteration 20/25 | Loss: 0.00134506
Iteration 21/25 | Loss: 0.00134074
Iteration 22/25 | Loss: 0.00133220
Iteration 23/25 | Loss: 0.00134981
Iteration 24/25 | Loss: 0.00132552
Iteration 25/25 | Loss: 0.00131149

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.78417629
Iteration 2/25 | Loss: 0.00264441
Iteration 3/25 | Loss: 0.00264439
Iteration 4/25 | Loss: 0.00264439
Iteration 5/25 | Loss: 0.00264439
Iteration 6/25 | Loss: 0.00264439
Iteration 7/25 | Loss: 0.00264439
Iteration 8/25 | Loss: 0.00264439
Iteration 9/25 | Loss: 0.00264439
Iteration 10/25 | Loss: 0.00264439
Iteration 11/25 | Loss: 0.00264439
Iteration 12/25 | Loss: 0.00264439
Iteration 13/25 | Loss: 0.00264439
Iteration 14/25 | Loss: 0.00264439
Iteration 15/25 | Loss: 0.00264439
Iteration 16/25 | Loss: 0.00264439
Iteration 17/25 | Loss: 0.00264439
Iteration 18/25 | Loss: 0.00264439
Iteration 19/25 | Loss: 0.00264439
Iteration 20/25 | Loss: 0.00264439
Iteration 21/25 | Loss: 0.00264439
Iteration 22/25 | Loss: 0.00264439
Iteration 23/25 | Loss: 0.00264439
Iteration 24/25 | Loss: 0.00264439
Iteration 25/25 | Loss: 0.00264439

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00264439
Iteration 2/1000 | Loss: 0.00119368
Iteration 3/1000 | Loss: 0.00030959
Iteration 4/1000 | Loss: 0.00025946
Iteration 5/1000 | Loss: 0.00345620
Iteration 6/1000 | Loss: 0.00346641
Iteration 7/1000 | Loss: 0.00022895
Iteration 8/1000 | Loss: 0.00021316
Iteration 9/1000 | Loss: 0.00694286
Iteration 10/1000 | Loss: 0.00208643
Iteration 11/1000 | Loss: 0.01083671
Iteration 12/1000 | Loss: 0.00376752
Iteration 13/1000 | Loss: 0.00084491
Iteration 14/1000 | Loss: 0.00350131
Iteration 15/1000 | Loss: 0.00204063
Iteration 16/1000 | Loss: 0.00352306
Iteration 17/1000 | Loss: 0.00495029
Iteration 18/1000 | Loss: 0.00502269
Iteration 19/1000 | Loss: 0.00265966
Iteration 20/1000 | Loss: 0.00265236
Iteration 21/1000 | Loss: 0.00256413
Iteration 22/1000 | Loss: 0.00522795
Iteration 23/1000 | Loss: 0.00354853
Iteration 24/1000 | Loss: 0.00267579
Iteration 25/1000 | Loss: 0.00067485
Iteration 26/1000 | Loss: 0.00061773
Iteration 27/1000 | Loss: 0.00155379
Iteration 28/1000 | Loss: 0.00210602
Iteration 29/1000 | Loss: 0.00496733
Iteration 30/1000 | Loss: 0.00330033
Iteration 31/1000 | Loss: 0.00424662
Iteration 32/1000 | Loss: 0.00524641
Iteration 33/1000 | Loss: 0.00643771
Iteration 34/1000 | Loss: 0.00286233
Iteration 35/1000 | Loss: 0.00295132
Iteration 36/1000 | Loss: 0.00204917
Iteration 37/1000 | Loss: 0.00114732
Iteration 38/1000 | Loss: 0.00653738
Iteration 39/1000 | Loss: 0.00194760
Iteration 40/1000 | Loss: 0.00337472
Iteration 41/1000 | Loss: 0.00178402
Iteration 42/1000 | Loss: 0.00134235
Iteration 43/1000 | Loss: 0.00191098
Iteration 44/1000 | Loss: 0.00422501
Iteration 45/1000 | Loss: 0.00223628
Iteration 46/1000 | Loss: 0.00368264
Iteration 47/1000 | Loss: 0.00091518
Iteration 48/1000 | Loss: 0.00204864
Iteration 49/1000 | Loss: 0.00269688
Iteration 50/1000 | Loss: 0.00050520
Iteration 51/1000 | Loss: 0.00076493
Iteration 52/1000 | Loss: 0.00023378
Iteration 53/1000 | Loss: 0.00053649
Iteration 54/1000 | Loss: 0.00107526
Iteration 55/1000 | Loss: 0.00100968
Iteration 56/1000 | Loss: 0.00117945
Iteration 57/1000 | Loss: 0.00096391
Iteration 58/1000 | Loss: 0.00058836
Iteration 59/1000 | Loss: 0.00192280
Iteration 60/1000 | Loss: 0.00111863
Iteration 61/1000 | Loss: 0.00189039
Iteration 62/1000 | Loss: 0.00257433
Iteration 63/1000 | Loss: 0.00267820
Iteration 64/1000 | Loss: 0.00343358
Iteration 65/1000 | Loss: 0.00233579
Iteration 66/1000 | Loss: 0.00239682
Iteration 67/1000 | Loss: 0.00300206
Iteration 68/1000 | Loss: 0.00113922
Iteration 69/1000 | Loss: 0.00281610
Iteration 70/1000 | Loss: 0.00195757
Iteration 71/1000 | Loss: 0.00129510
Iteration 72/1000 | Loss: 0.00177601
Iteration 73/1000 | Loss: 0.00167421
Iteration 74/1000 | Loss: 0.00362543
Iteration 75/1000 | Loss: 0.00245760
Iteration 76/1000 | Loss: 0.00171300
Iteration 77/1000 | Loss: 0.00208469
Iteration 78/1000 | Loss: 0.00207757
Iteration 79/1000 | Loss: 0.00090978
Iteration 80/1000 | Loss: 0.00199219
Iteration 81/1000 | Loss: 0.00192342
Iteration 82/1000 | Loss: 0.00150329
Iteration 83/1000 | Loss: 0.00120080
Iteration 84/1000 | Loss: 0.00174065
Iteration 85/1000 | Loss: 0.00122821
Iteration 86/1000 | Loss: 0.00191671
Iteration 87/1000 | Loss: 0.00101138
Iteration 88/1000 | Loss: 0.00222987
Iteration 89/1000 | Loss: 0.00180415
Iteration 90/1000 | Loss: 0.00164124
Iteration 91/1000 | Loss: 0.00201565
Iteration 92/1000 | Loss: 0.00112936
Iteration 93/1000 | Loss: 0.00147799
Iteration 94/1000 | Loss: 0.00082402
Iteration 95/1000 | Loss: 0.00182953
Iteration 96/1000 | Loss: 0.00137556
Iteration 97/1000 | Loss: 0.00184818
Iteration 98/1000 | Loss: 0.00154134
Iteration 99/1000 | Loss: 0.00149228
Iteration 100/1000 | Loss: 0.00186831
Iteration 101/1000 | Loss: 0.00243743
Iteration 102/1000 | Loss: 0.00119952
Iteration 103/1000 | Loss: 0.00099489
Iteration 104/1000 | Loss: 0.00105099
Iteration 105/1000 | Loss: 0.00185096
Iteration 106/1000 | Loss: 0.00242880
Iteration 107/1000 | Loss: 0.00316730
Iteration 108/1000 | Loss: 0.00240827
Iteration 109/1000 | Loss: 0.00261691
Iteration 110/1000 | Loss: 0.00152873
Iteration 111/1000 | Loss: 0.00107742
Iteration 112/1000 | Loss: 0.00086021
Iteration 113/1000 | Loss: 0.00125185
Iteration 114/1000 | Loss: 0.00111335
Iteration 115/1000 | Loss: 0.00114914
Iteration 116/1000 | Loss: 0.00136719
Iteration 117/1000 | Loss: 0.00205976
Iteration 118/1000 | Loss: 0.00076503
Iteration 119/1000 | Loss: 0.00125197
Iteration 120/1000 | Loss: 0.00116577
Iteration 121/1000 | Loss: 0.00187566
Iteration 122/1000 | Loss: 0.00263065
Iteration 123/1000 | Loss: 0.00032061
Iteration 124/1000 | Loss: 0.00165231
Iteration 125/1000 | Loss: 0.00141277
Iteration 126/1000 | Loss: 0.00178161
Iteration 127/1000 | Loss: 0.00159328
Iteration 128/1000 | Loss: 0.00188840
Iteration 129/1000 | Loss: 0.00217093
Iteration 130/1000 | Loss: 0.00105405
Iteration 131/1000 | Loss: 0.00072335
Iteration 132/1000 | Loss: 0.00064620
Iteration 133/1000 | Loss: 0.00074766
Iteration 134/1000 | Loss: 0.00092754
Iteration 135/1000 | Loss: 0.00073599
Iteration 136/1000 | Loss: 0.00097913
Iteration 137/1000 | Loss: 0.00088347
Iteration 138/1000 | Loss: 0.00101047
Iteration 139/1000 | Loss: 0.00085582
Iteration 140/1000 | Loss: 0.00081836
Iteration 141/1000 | Loss: 0.00032721
Iteration 142/1000 | Loss: 0.00212575
Iteration 143/1000 | Loss: 0.00133535
Iteration 144/1000 | Loss: 0.00104465
Iteration 145/1000 | Loss: 0.00081916
Iteration 146/1000 | Loss: 0.00199941
Iteration 147/1000 | Loss: 0.00090314
Iteration 148/1000 | Loss: 0.00088935
Iteration 149/1000 | Loss: 0.00059648
Iteration 150/1000 | Loss: 0.00059185
Iteration 151/1000 | Loss: 0.00077954
Iteration 152/1000 | Loss: 0.00050728
Iteration 153/1000 | Loss: 0.00172184
Iteration 154/1000 | Loss: 0.00102538
Iteration 155/1000 | Loss: 0.00098019
Iteration 156/1000 | Loss: 0.00039960
Iteration 157/1000 | Loss: 0.00051152
Iteration 158/1000 | Loss: 0.00051435
Iteration 159/1000 | Loss: 0.00069686
Iteration 160/1000 | Loss: 0.00135275
Iteration 161/1000 | Loss: 0.00094664
Iteration 162/1000 | Loss: 0.00128073
Iteration 163/1000 | Loss: 0.00155960
Iteration 164/1000 | Loss: 0.00056484
Iteration 165/1000 | Loss: 0.00053322
Iteration 166/1000 | Loss: 0.00258245
Iteration 167/1000 | Loss: 0.00132473
Iteration 168/1000 | Loss: 0.00100330
Iteration 169/1000 | Loss: 0.00121369
Iteration 170/1000 | Loss: 0.00087975
Iteration 171/1000 | Loss: 0.00066536
Iteration 172/1000 | Loss: 0.00263501
Iteration 173/1000 | Loss: 0.00161092
Iteration 174/1000 | Loss: 0.00035945
Iteration 175/1000 | Loss: 0.00054754
Iteration 176/1000 | Loss: 0.00234565
Iteration 177/1000 | Loss: 0.00164916
Iteration 178/1000 | Loss: 0.00122935
Iteration 179/1000 | Loss: 0.00014614
Iteration 180/1000 | Loss: 0.00013331
Iteration 181/1000 | Loss: 0.00253152
Iteration 182/1000 | Loss: 0.00086932
Iteration 183/1000 | Loss: 0.00041732
Iteration 184/1000 | Loss: 0.00158621
Iteration 185/1000 | Loss: 0.00340303
Iteration 186/1000 | Loss: 0.00212254
Iteration 187/1000 | Loss: 0.00072221
Iteration 188/1000 | Loss: 0.00091710
Iteration 189/1000 | Loss: 0.00104945
Iteration 190/1000 | Loss: 0.00013908
Iteration 191/1000 | Loss: 0.00260856
Iteration 192/1000 | Loss: 0.00153614
Iteration 193/1000 | Loss: 0.00019774
Iteration 194/1000 | Loss: 0.00213137
Iteration 195/1000 | Loss: 0.00154297
Iteration 196/1000 | Loss: 0.00202791
Iteration 197/1000 | Loss: 0.00255453
Iteration 198/1000 | Loss: 0.00237403
Iteration 199/1000 | Loss: 0.00069334
Iteration 200/1000 | Loss: 0.00080073
Iteration 201/1000 | Loss: 0.00059647
Iteration 202/1000 | Loss: 0.00021553
Iteration 203/1000 | Loss: 0.00027024
Iteration 204/1000 | Loss: 0.00164125
Iteration 205/1000 | Loss: 0.00109279
Iteration 206/1000 | Loss: 0.00013094
Iteration 207/1000 | Loss: 0.00154814
Iteration 208/1000 | Loss: 0.00028413
Iteration 209/1000 | Loss: 0.00072657
Iteration 210/1000 | Loss: 0.00050954
Iteration 211/1000 | Loss: 0.00012345
Iteration 212/1000 | Loss: 0.00010629
Iteration 213/1000 | Loss: 0.00053202
Iteration 214/1000 | Loss: 0.00053683
Iteration 215/1000 | Loss: 0.00073424
Iteration 216/1000 | Loss: 0.00021367
Iteration 217/1000 | Loss: 0.00024466
Iteration 218/1000 | Loss: 0.00073176
Iteration 219/1000 | Loss: 0.00088500
Iteration 220/1000 | Loss: 0.00060399
Iteration 221/1000 | Loss: 0.00078149
Iteration 222/1000 | Loss: 0.00056048
Iteration 223/1000 | Loss: 0.00010817
Iteration 224/1000 | Loss: 0.00009835
Iteration 225/1000 | Loss: 0.00008879
Iteration 226/1000 | Loss: 0.00008529
Iteration 227/1000 | Loss: 0.00093020
Iteration 228/1000 | Loss: 0.00047856
Iteration 229/1000 | Loss: 0.00008821
Iteration 230/1000 | Loss: 0.00055285
Iteration 231/1000 | Loss: 0.00030912
Iteration 232/1000 | Loss: 0.00013379
Iteration 233/1000 | Loss: 0.00008132
Iteration 234/1000 | Loss: 0.00149583
Iteration 235/1000 | Loss: 0.00077266
Iteration 236/1000 | Loss: 0.00145002
Iteration 237/1000 | Loss: 0.00028501
Iteration 238/1000 | Loss: 0.00043328
Iteration 239/1000 | Loss: 0.00044481
Iteration 240/1000 | Loss: 0.00008466
Iteration 241/1000 | Loss: 0.00007952
Iteration 242/1000 | Loss: 0.00007768
Iteration 243/1000 | Loss: 0.00088119
Iteration 244/1000 | Loss: 0.00020421
Iteration 245/1000 | Loss: 0.00007776
Iteration 246/1000 | Loss: 0.00128941
Iteration 247/1000 | Loss: 0.00029434
Iteration 248/1000 | Loss: 0.00007683
Iteration 249/1000 | Loss: 0.00046453
Iteration 250/1000 | Loss: 0.00010435
Iteration 251/1000 | Loss: 0.00086775
Iteration 252/1000 | Loss: 0.00016889
Iteration 253/1000 | Loss: 0.00008594
Iteration 254/1000 | Loss: 0.00007742
Iteration 255/1000 | Loss: 0.00007438
Iteration 256/1000 | Loss: 0.00007344
Iteration 257/1000 | Loss: 0.00068528
Iteration 258/1000 | Loss: 0.00013873
Iteration 259/1000 | Loss: 0.00022441
Iteration 260/1000 | Loss: 0.00048573
Iteration 261/1000 | Loss: 0.00057020
Iteration 262/1000 | Loss: 0.00076844
Iteration 263/1000 | Loss: 0.00081123
Iteration 264/1000 | Loss: 0.00074203
Iteration 265/1000 | Loss: 0.00100966
Iteration 266/1000 | Loss: 0.00047180
Iteration 267/1000 | Loss: 0.00117193
Iteration 268/1000 | Loss: 0.00056067
Iteration 269/1000 | Loss: 0.00047955
Iteration 270/1000 | Loss: 0.00008364
Iteration 271/1000 | Loss: 0.00007597
Iteration 272/1000 | Loss: 0.00007440
Iteration 273/1000 | Loss: 0.00007293
Iteration 274/1000 | Loss: 0.00015406
Iteration 275/1000 | Loss: 0.00007528
Iteration 276/1000 | Loss: 0.00033934
Iteration 277/1000 | Loss: 0.00083454
Iteration 278/1000 | Loss: 0.00025081
Iteration 279/1000 | Loss: 0.00068463
Iteration 280/1000 | Loss: 0.00064873
Iteration 281/1000 | Loss: 0.00012565
Iteration 282/1000 | Loss: 0.00072353
Iteration 283/1000 | Loss: 0.00045980
Iteration 284/1000 | Loss: 0.00040794
Iteration 285/1000 | Loss: 0.00031410
Iteration 286/1000 | Loss: 0.00030450
Iteration 287/1000 | Loss: 0.00118896
Iteration 288/1000 | Loss: 0.00067792
Iteration 289/1000 | Loss: 0.00066095
Iteration 290/1000 | Loss: 0.00049114
Iteration 291/1000 | Loss: 0.00022191
Iteration 292/1000 | Loss: 0.00024282
Iteration 293/1000 | Loss: 0.00019800
Iteration 294/1000 | Loss: 0.00117449
Iteration 295/1000 | Loss: 0.00015937
Iteration 296/1000 | Loss: 0.00013118
Iteration 297/1000 | Loss: 0.00046240
Iteration 298/1000 | Loss: 0.00020981
Iteration 299/1000 | Loss: 0.00009513
Iteration 300/1000 | Loss: 0.00007858
Iteration 301/1000 | Loss: 0.00009742
Iteration 302/1000 | Loss: 0.00009859
Iteration 303/1000 | Loss: 0.00009811
Iteration 304/1000 | Loss: 0.00038337
Iteration 305/1000 | Loss: 0.00022822
Iteration 306/1000 | Loss: 0.00118840
Iteration 307/1000 | Loss: 0.00020974
Iteration 308/1000 | Loss: 0.00010159
Iteration 309/1000 | Loss: 0.00008107
Iteration 310/1000 | Loss: 0.00007496
Iteration 311/1000 | Loss: 0.00007279
Iteration 312/1000 | Loss: 0.00007067
Iteration 313/1000 | Loss: 0.00006929
Iteration 314/1000 | Loss: 0.00049314
Iteration 315/1000 | Loss: 0.00035520
Iteration 316/1000 | Loss: 0.00025988
Iteration 317/1000 | Loss: 0.00006916
Iteration 318/1000 | Loss: 0.00006774
Iteration 319/1000 | Loss: 0.00006550
Iteration 320/1000 | Loss: 0.00006392
Iteration 321/1000 | Loss: 0.00006294
Iteration 322/1000 | Loss: 0.00006230
Iteration 323/1000 | Loss: 0.00006188
Iteration 324/1000 | Loss: 0.00006157
Iteration 325/1000 | Loss: 0.00006135
Iteration 326/1000 | Loss: 0.00006117
Iteration 327/1000 | Loss: 0.00006111
Iteration 328/1000 | Loss: 0.00006111
Iteration 329/1000 | Loss: 0.00006106
Iteration 330/1000 | Loss: 0.00006094
Iteration 331/1000 | Loss: 0.00006094
Iteration 332/1000 | Loss: 0.00006090
Iteration 333/1000 | Loss: 0.00006089
Iteration 334/1000 | Loss: 0.00006087
Iteration 335/1000 | Loss: 0.00006087
Iteration 336/1000 | Loss: 0.00006086
Iteration 337/1000 | Loss: 0.00006086
Iteration 338/1000 | Loss: 0.00006086
Iteration 339/1000 | Loss: 0.00006086
Iteration 340/1000 | Loss: 0.00006086
Iteration 341/1000 | Loss: 0.00006086
Iteration 342/1000 | Loss: 0.00006086
Iteration 343/1000 | Loss: 0.00006086
Iteration 344/1000 | Loss: 0.00006085
Iteration 345/1000 | Loss: 0.00006085
Iteration 346/1000 | Loss: 0.00006085
Iteration 347/1000 | Loss: 0.00006085
Iteration 348/1000 | Loss: 0.00006085
Iteration 349/1000 | Loss: 0.00006085
Iteration 350/1000 | Loss: 0.00006085
Iteration 351/1000 | Loss: 0.00006084
Iteration 352/1000 | Loss: 0.00006084
Iteration 353/1000 | Loss: 0.00006084
Iteration 354/1000 | Loss: 0.00006083
Iteration 355/1000 | Loss: 0.00006083
Iteration 356/1000 | Loss: 0.00006083
Iteration 357/1000 | Loss: 0.00006083
Iteration 358/1000 | Loss: 0.00006083
Iteration 359/1000 | Loss: 0.00006083
Iteration 360/1000 | Loss: 0.00006083
Iteration 361/1000 | Loss: 0.00006083
Iteration 362/1000 | Loss: 0.00006082
Iteration 363/1000 | Loss: 0.00006082
Iteration 364/1000 | Loss: 0.00006082
Iteration 365/1000 | Loss: 0.00006082
Iteration 366/1000 | Loss: 0.00006082
Iteration 367/1000 | Loss: 0.00006081
Iteration 368/1000 | Loss: 0.00006081
Iteration 369/1000 | Loss: 0.00006081
Iteration 370/1000 | Loss: 0.00006081
Iteration 371/1000 | Loss: 0.00006080
Iteration 372/1000 | Loss: 0.00006080
Iteration 373/1000 | Loss: 0.00006080
Iteration 374/1000 | Loss: 0.00006080
Iteration 375/1000 | Loss: 0.00006080
Iteration 376/1000 | Loss: 0.00006080
Iteration 377/1000 | Loss: 0.00006080
Iteration 378/1000 | Loss: 0.00006080
Iteration 379/1000 | Loss: 0.00006080
Iteration 380/1000 | Loss: 0.00006080
Iteration 381/1000 | Loss: 0.00006080
Iteration 382/1000 | Loss: 0.00006079
Iteration 383/1000 | Loss: 0.00006079
Iteration 384/1000 | Loss: 0.00006079
Iteration 385/1000 | Loss: 0.00006079
Iteration 386/1000 | Loss: 0.00006079
Iteration 387/1000 | Loss: 0.00006079
Iteration 388/1000 | Loss: 0.00006079
Iteration 389/1000 | Loss: 0.00006079
Iteration 390/1000 | Loss: 0.00006079
Iteration 391/1000 | Loss: 0.00006079
Iteration 392/1000 | Loss: 0.00006079
Iteration 393/1000 | Loss: 0.00006079
Iteration 394/1000 | Loss: 0.00006079
Iteration 395/1000 | Loss: 0.00006079
Iteration 396/1000 | Loss: 0.00006078
Iteration 397/1000 | Loss: 0.00006078
Iteration 398/1000 | Loss: 0.00006078
Iteration 399/1000 | Loss: 0.00006078
Iteration 400/1000 | Loss: 0.00006078
Iteration 401/1000 | Loss: 0.00006078
Iteration 402/1000 | Loss: 0.00006078
Iteration 403/1000 | Loss: 0.00006078
Iteration 404/1000 | Loss: 0.00006077
Iteration 405/1000 | Loss: 0.00006077
Iteration 406/1000 | Loss: 0.00006077
Iteration 407/1000 | Loss: 0.00006077
Iteration 408/1000 | Loss: 0.00006077
Iteration 409/1000 | Loss: 0.00006077
Iteration 410/1000 | Loss: 0.00006077
Iteration 411/1000 | Loss: 0.00006077
Iteration 412/1000 | Loss: 0.00006077
Iteration 413/1000 | Loss: 0.00006077
Iteration 414/1000 | Loss: 0.00006077
Iteration 415/1000 | Loss: 0.00006077
Iteration 416/1000 | Loss: 0.00006077
Iteration 417/1000 | Loss: 0.00006077
Iteration 418/1000 | Loss: 0.00006077
Iteration 419/1000 | Loss: 0.00006077
Iteration 420/1000 | Loss: 0.00006077
Iteration 421/1000 | Loss: 0.00006077
Iteration 422/1000 | Loss: 0.00006077
Iteration 423/1000 | Loss: 0.00006077
Iteration 424/1000 | Loss: 0.00006077
Iteration 425/1000 | Loss: 0.00006077
Iteration 426/1000 | Loss: 0.00006077
Iteration 427/1000 | Loss: 0.00006077
Iteration 428/1000 | Loss: 0.00006077
Iteration 429/1000 | Loss: 0.00006077
Iteration 430/1000 | Loss: 0.00006077
Iteration 431/1000 | Loss: 0.00006077
Iteration 432/1000 | Loss: 0.00006077
Iteration 433/1000 | Loss: 0.00006077
Iteration 434/1000 | Loss: 0.00006077
Iteration 435/1000 | Loss: 0.00006077
Iteration 436/1000 | Loss: 0.00006077
Iteration 437/1000 | Loss: 0.00006077
Iteration 438/1000 | Loss: 0.00006077
Iteration 439/1000 | Loss: 0.00006077
Iteration 440/1000 | Loss: 0.00006077
Iteration 441/1000 | Loss: 0.00006077
Iteration 442/1000 | Loss: 0.00006077
Iteration 443/1000 | Loss: 0.00006077
Iteration 444/1000 | Loss: 0.00006077
Iteration 445/1000 | Loss: 0.00006077
Iteration 446/1000 | Loss: 0.00006077
Iteration 447/1000 | Loss: 0.00006077
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 447. Stopping optimization.
Last 5 losses: [6.0772599681513384e-05, 6.0772599681513384e-05, 6.0772599681513384e-05, 6.0772599681513384e-05, 6.0772599681513384e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 6.0772599681513384e-05

Optimization complete. Final v2v error: 5.488194465637207 mm

Highest mean error: 12.268314361572266 mm for frame 37

Lowest mean error: 4.6421685218811035 mm for frame 134

Saving results

Total time: 496.7587978839874
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_006/1070/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_006/1070.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_006/1070
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00891515
Iteration 2/25 | Loss: 0.00103562
Iteration 3/25 | Loss: 0.00092084
Iteration 4/25 | Loss: 0.00087995
Iteration 5/25 | Loss: 0.00086746
Iteration 6/25 | Loss: 0.00086571
Iteration 7/25 | Loss: 0.00086480
Iteration 8/25 | Loss: 0.00086471
Iteration 9/25 | Loss: 0.00086471
Iteration 10/25 | Loss: 0.00086471
Iteration 11/25 | Loss: 0.00086471
Iteration 12/25 | Loss: 0.00086471
Iteration 13/25 | Loss: 0.00086471
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0008647054200991988, 0.0008647054200991988, 0.0008647054200991988, 0.0008647054200991988, 0.0008647054200991988]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008647054200991988

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43483329
Iteration 2/25 | Loss: 0.00049175
Iteration 3/25 | Loss: 0.00049167
Iteration 4/25 | Loss: 0.00049167
Iteration 5/25 | Loss: 0.00049167
Iteration 6/25 | Loss: 0.00049167
Iteration 7/25 | Loss: 0.00049167
Iteration 8/25 | Loss: 0.00049167
Iteration 9/25 | Loss: 0.00049167
Iteration 10/25 | Loss: 0.00049167
Iteration 11/25 | Loss: 0.00049167
Iteration 12/25 | Loss: 0.00049167
Iteration 13/25 | Loss: 0.00049167
Iteration 14/25 | Loss: 0.00049167
Iteration 15/25 | Loss: 0.00049167
Iteration 16/25 | Loss: 0.00049167
Iteration 17/25 | Loss: 0.00049167
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0004916652687825263, 0.0004916652687825263, 0.0004916652687825263, 0.0004916652687825263, 0.0004916652687825263]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0004916652687825263

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00049167
Iteration 2/1000 | Loss: 0.00005966
Iteration 3/1000 | Loss: 0.00003744
Iteration 4/1000 | Loss: 0.00003212
Iteration 5/1000 | Loss: 0.00003021
Iteration 6/1000 | Loss: 0.00002881
Iteration 7/1000 | Loss: 0.00002809
Iteration 8/1000 | Loss: 0.00002733
Iteration 9/1000 | Loss: 0.00002690
Iteration 10/1000 | Loss: 0.00002652
Iteration 11/1000 | Loss: 0.00002635
Iteration 12/1000 | Loss: 0.00002613
Iteration 13/1000 | Loss: 0.00002595
Iteration 14/1000 | Loss: 0.00002589
Iteration 15/1000 | Loss: 0.00002588
Iteration 16/1000 | Loss: 0.00002578
Iteration 17/1000 | Loss: 0.00002577
Iteration 18/1000 | Loss: 0.00002573
Iteration 19/1000 | Loss: 0.00002569
Iteration 20/1000 | Loss: 0.00002568
Iteration 21/1000 | Loss: 0.00002567
Iteration 22/1000 | Loss: 0.00002566
Iteration 23/1000 | Loss: 0.00002566
Iteration 24/1000 | Loss: 0.00002565
Iteration 25/1000 | Loss: 0.00002565
Iteration 26/1000 | Loss: 0.00002564
Iteration 27/1000 | Loss: 0.00002564
Iteration 28/1000 | Loss: 0.00002563
Iteration 29/1000 | Loss: 0.00002563
Iteration 30/1000 | Loss: 0.00002562
Iteration 31/1000 | Loss: 0.00002562
Iteration 32/1000 | Loss: 0.00002562
Iteration 33/1000 | Loss: 0.00002562
Iteration 34/1000 | Loss: 0.00002562
Iteration 35/1000 | Loss: 0.00002562
Iteration 36/1000 | Loss: 0.00002561
Iteration 37/1000 | Loss: 0.00002561
Iteration 38/1000 | Loss: 0.00002560
Iteration 39/1000 | Loss: 0.00002560
Iteration 40/1000 | Loss: 0.00002559
Iteration 41/1000 | Loss: 0.00002559
Iteration 42/1000 | Loss: 0.00002559
Iteration 43/1000 | Loss: 0.00002558
Iteration 44/1000 | Loss: 0.00002558
Iteration 45/1000 | Loss: 0.00002558
Iteration 46/1000 | Loss: 0.00002557
Iteration 47/1000 | Loss: 0.00002557
Iteration 48/1000 | Loss: 0.00002556
Iteration 49/1000 | Loss: 0.00002556
Iteration 50/1000 | Loss: 0.00002555
Iteration 51/1000 | Loss: 0.00002555
Iteration 52/1000 | Loss: 0.00002555
Iteration 53/1000 | Loss: 0.00002554
Iteration 54/1000 | Loss: 0.00002554
Iteration 55/1000 | Loss: 0.00002554
Iteration 56/1000 | Loss: 0.00002553
Iteration 57/1000 | Loss: 0.00002553
Iteration 58/1000 | Loss: 0.00002553
Iteration 59/1000 | Loss: 0.00002553
Iteration 60/1000 | Loss: 0.00002553
Iteration 61/1000 | Loss: 0.00002552
Iteration 62/1000 | Loss: 0.00002552
Iteration 63/1000 | Loss: 0.00002552
Iteration 64/1000 | Loss: 0.00002551
Iteration 65/1000 | Loss: 0.00002551
Iteration 66/1000 | Loss: 0.00002550
Iteration 67/1000 | Loss: 0.00002550
Iteration 68/1000 | Loss: 0.00002549
Iteration 69/1000 | Loss: 0.00002549
Iteration 70/1000 | Loss: 0.00002548
Iteration 71/1000 | Loss: 0.00002548
Iteration 72/1000 | Loss: 0.00002548
Iteration 73/1000 | Loss: 0.00002548
Iteration 74/1000 | Loss: 0.00002547
Iteration 75/1000 | Loss: 0.00002547
Iteration 76/1000 | Loss: 0.00002547
Iteration 77/1000 | Loss: 0.00002547
Iteration 78/1000 | Loss: 0.00002547
Iteration 79/1000 | Loss: 0.00002547
Iteration 80/1000 | Loss: 0.00002547
Iteration 81/1000 | Loss: 0.00002547
Iteration 82/1000 | Loss: 0.00002546
Iteration 83/1000 | Loss: 0.00002546
Iteration 84/1000 | Loss: 0.00002546
Iteration 85/1000 | Loss: 0.00002546
Iteration 86/1000 | Loss: 0.00002546
Iteration 87/1000 | Loss: 0.00002546
Iteration 88/1000 | Loss: 0.00002545
Iteration 89/1000 | Loss: 0.00002545
Iteration 90/1000 | Loss: 0.00002545
Iteration 91/1000 | Loss: 0.00002545
Iteration 92/1000 | Loss: 0.00002545
Iteration 93/1000 | Loss: 0.00002545
Iteration 94/1000 | Loss: 0.00002544
Iteration 95/1000 | Loss: 0.00002544
Iteration 96/1000 | Loss: 0.00002544
Iteration 97/1000 | Loss: 0.00002544
Iteration 98/1000 | Loss: 0.00002544
Iteration 99/1000 | Loss: 0.00002544
Iteration 100/1000 | Loss: 0.00002544
Iteration 101/1000 | Loss: 0.00002544
Iteration 102/1000 | Loss: 0.00002544
Iteration 103/1000 | Loss: 0.00002543
Iteration 104/1000 | Loss: 0.00002543
Iteration 105/1000 | Loss: 0.00002543
Iteration 106/1000 | Loss: 0.00002542
Iteration 107/1000 | Loss: 0.00002542
Iteration 108/1000 | Loss: 0.00002542
Iteration 109/1000 | Loss: 0.00002542
Iteration 110/1000 | Loss: 0.00002541
Iteration 111/1000 | Loss: 0.00002541
Iteration 112/1000 | Loss: 0.00002541
Iteration 113/1000 | Loss: 0.00002541
Iteration 114/1000 | Loss: 0.00002540
Iteration 115/1000 | Loss: 0.00002540
Iteration 116/1000 | Loss: 0.00002540
Iteration 117/1000 | Loss: 0.00002540
Iteration 118/1000 | Loss: 0.00002540
Iteration 119/1000 | Loss: 0.00002540
Iteration 120/1000 | Loss: 0.00002540
Iteration 121/1000 | Loss: 0.00002540
Iteration 122/1000 | Loss: 0.00002540
Iteration 123/1000 | Loss: 0.00002540
Iteration 124/1000 | Loss: 0.00002540
Iteration 125/1000 | Loss: 0.00002539
Iteration 126/1000 | Loss: 0.00002539
Iteration 127/1000 | Loss: 0.00002539
Iteration 128/1000 | Loss: 0.00002539
Iteration 129/1000 | Loss: 0.00002539
Iteration 130/1000 | Loss: 0.00002539
Iteration 131/1000 | Loss: 0.00002539
Iteration 132/1000 | Loss: 0.00002539
Iteration 133/1000 | Loss: 0.00002539
Iteration 134/1000 | Loss: 0.00002539
Iteration 135/1000 | Loss: 0.00002539
Iteration 136/1000 | Loss: 0.00002539
Iteration 137/1000 | Loss: 0.00002538
Iteration 138/1000 | Loss: 0.00002538
Iteration 139/1000 | Loss: 0.00002538
Iteration 140/1000 | Loss: 0.00002538
Iteration 141/1000 | Loss: 0.00002538
Iteration 142/1000 | Loss: 0.00002538
Iteration 143/1000 | Loss: 0.00002538
Iteration 144/1000 | Loss: 0.00002538
Iteration 145/1000 | Loss: 0.00002538
Iteration 146/1000 | Loss: 0.00002538
Iteration 147/1000 | Loss: 0.00002538
Iteration 148/1000 | Loss: 0.00002538
Iteration 149/1000 | Loss: 0.00002538
Iteration 150/1000 | Loss: 0.00002538
Iteration 151/1000 | Loss: 0.00002538
Iteration 152/1000 | Loss: 0.00002537
Iteration 153/1000 | Loss: 0.00002537
Iteration 154/1000 | Loss: 0.00002537
Iteration 155/1000 | Loss: 0.00002537
Iteration 156/1000 | Loss: 0.00002537
Iteration 157/1000 | Loss: 0.00002537
Iteration 158/1000 | Loss: 0.00002537
Iteration 159/1000 | Loss: 0.00002537
Iteration 160/1000 | Loss: 0.00002537
Iteration 161/1000 | Loss: 0.00002536
Iteration 162/1000 | Loss: 0.00002536
Iteration 163/1000 | Loss: 0.00002536
Iteration 164/1000 | Loss: 0.00002536
Iteration 165/1000 | Loss: 0.00002536
Iteration 166/1000 | Loss: 0.00002536
Iteration 167/1000 | Loss: 0.00002535
Iteration 168/1000 | Loss: 0.00002535
Iteration 169/1000 | Loss: 0.00002535
Iteration 170/1000 | Loss: 0.00002535
Iteration 171/1000 | Loss: 0.00002535
Iteration 172/1000 | Loss: 0.00002535
Iteration 173/1000 | Loss: 0.00002535
Iteration 174/1000 | Loss: 0.00002535
Iteration 175/1000 | Loss: 0.00002535
Iteration 176/1000 | Loss: 0.00002535
Iteration 177/1000 | Loss: 0.00002535
Iteration 178/1000 | Loss: 0.00002535
Iteration 179/1000 | Loss: 0.00002535
Iteration 180/1000 | Loss: 0.00002535
Iteration 181/1000 | Loss: 0.00002534
Iteration 182/1000 | Loss: 0.00002534
Iteration 183/1000 | Loss: 0.00002534
Iteration 184/1000 | Loss: 0.00002534
Iteration 185/1000 | Loss: 0.00002534
Iteration 186/1000 | Loss: 0.00002533
Iteration 187/1000 | Loss: 0.00002533
Iteration 188/1000 | Loss: 0.00002533
Iteration 189/1000 | Loss: 0.00002533
Iteration 190/1000 | Loss: 0.00002533
Iteration 191/1000 | Loss: 0.00002533
Iteration 192/1000 | Loss: 0.00002533
Iteration 193/1000 | Loss: 0.00002533
Iteration 194/1000 | Loss: 0.00002533
Iteration 195/1000 | Loss: 0.00002533
Iteration 196/1000 | Loss: 0.00002532
Iteration 197/1000 | Loss: 0.00002532
Iteration 198/1000 | Loss: 0.00002532
Iteration 199/1000 | Loss: 0.00002532
Iteration 200/1000 | Loss: 0.00002531
Iteration 201/1000 | Loss: 0.00002531
Iteration 202/1000 | Loss: 0.00002531
Iteration 203/1000 | Loss: 0.00002531
Iteration 204/1000 | Loss: 0.00002531
Iteration 205/1000 | Loss: 0.00002531
Iteration 206/1000 | Loss: 0.00002531
Iteration 207/1000 | Loss: 0.00002530
Iteration 208/1000 | Loss: 0.00002530
Iteration 209/1000 | Loss: 0.00002530
Iteration 210/1000 | Loss: 0.00002530
Iteration 211/1000 | Loss: 0.00002530
Iteration 212/1000 | Loss: 0.00002530
Iteration 213/1000 | Loss: 0.00002530
Iteration 214/1000 | Loss: 0.00002530
Iteration 215/1000 | Loss: 0.00002530
Iteration 216/1000 | Loss: 0.00002530
Iteration 217/1000 | Loss: 0.00002530
Iteration 218/1000 | Loss: 0.00002529
Iteration 219/1000 | Loss: 0.00002529
Iteration 220/1000 | Loss: 0.00002529
Iteration 221/1000 | Loss: 0.00002529
Iteration 222/1000 | Loss: 0.00002529
Iteration 223/1000 | Loss: 0.00002529
Iteration 224/1000 | Loss: 0.00002529
Iteration 225/1000 | Loss: 0.00002529
Iteration 226/1000 | Loss: 0.00002529
Iteration 227/1000 | Loss: 0.00002528
Iteration 228/1000 | Loss: 0.00002528
Iteration 229/1000 | Loss: 0.00002528
Iteration 230/1000 | Loss: 0.00002528
Iteration 231/1000 | Loss: 0.00002528
Iteration 232/1000 | Loss: 0.00002528
Iteration 233/1000 | Loss: 0.00002528
Iteration 234/1000 | Loss: 0.00002528
Iteration 235/1000 | Loss: 0.00002528
Iteration 236/1000 | Loss: 0.00002528
Iteration 237/1000 | Loss: 0.00002528
Iteration 238/1000 | Loss: 0.00002528
Iteration 239/1000 | Loss: 0.00002528
Iteration 240/1000 | Loss: 0.00002528
Iteration 241/1000 | Loss: 0.00002528
Iteration 242/1000 | Loss: 0.00002528
Iteration 243/1000 | Loss: 0.00002528
Iteration 244/1000 | Loss: 0.00002528
Iteration 245/1000 | Loss: 0.00002528
Iteration 246/1000 | Loss: 0.00002528
Iteration 247/1000 | Loss: 0.00002528
Iteration 248/1000 | Loss: 0.00002528
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 248. Stopping optimization.
Last 5 losses: [2.5279721739934757e-05, 2.5279721739934757e-05, 2.5279721739934757e-05, 2.5279721739934757e-05, 2.5279721739934757e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.5279721739934757e-05

Optimization complete. Final v2v error: 4.167566776275635 mm

Highest mean error: 4.511348247528076 mm for frame 110

Lowest mean error: 3.4918625354766846 mm for frame 0

Saving results

Total time: 47.61152958869934
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_006/1049/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_006/1049.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_006/1049
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00783092
Iteration 2/25 | Loss: 0.00109724
Iteration 3/25 | Loss: 0.00090471
Iteration 4/25 | Loss: 0.00086216
Iteration 5/25 | Loss: 0.00084525
Iteration 6/25 | Loss: 0.00084235
Iteration 7/25 | Loss: 0.00084156
Iteration 8/25 | Loss: 0.00084156
Iteration 9/25 | Loss: 0.00084156
Iteration 10/25 | Loss: 0.00084156
Iteration 11/25 | Loss: 0.00084156
Iteration 12/25 | Loss: 0.00084156
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0008415635675191879, 0.0008415635675191879, 0.0008415635675191879, 0.0008415635675191879, 0.0008415635675191879]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008415635675191879

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.51933742
Iteration 2/25 | Loss: 0.00062950
Iteration 3/25 | Loss: 0.00062949
Iteration 4/25 | Loss: 0.00062949
Iteration 5/25 | Loss: 0.00062949
Iteration 6/25 | Loss: 0.00062949
Iteration 7/25 | Loss: 0.00062949
Iteration 8/25 | Loss: 0.00062949
Iteration 9/25 | Loss: 0.00062949
Iteration 10/25 | Loss: 0.00062949
Iteration 11/25 | Loss: 0.00062949
Iteration 12/25 | Loss: 0.00062949
Iteration 13/25 | Loss: 0.00062949
Iteration 14/25 | Loss: 0.00062949
Iteration 15/25 | Loss: 0.00062949
Iteration 16/25 | Loss: 0.00062949
Iteration 17/25 | Loss: 0.00062949
Iteration 18/25 | Loss: 0.00062949
Iteration 19/25 | Loss: 0.00062949
Iteration 20/25 | Loss: 0.00062949
Iteration 21/25 | Loss: 0.00062949
Iteration 22/25 | Loss: 0.00062949
Iteration 23/25 | Loss: 0.00062949
Iteration 24/25 | Loss: 0.00062949
Iteration 25/25 | Loss: 0.00062949

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00062949
Iteration 2/1000 | Loss: 0.00005653
Iteration 3/1000 | Loss: 0.00004154
Iteration 4/1000 | Loss: 0.00003503
Iteration 5/1000 | Loss: 0.00003302
Iteration 6/1000 | Loss: 0.00003149
Iteration 7/1000 | Loss: 0.00003054
Iteration 8/1000 | Loss: 0.00002973
Iteration 9/1000 | Loss: 0.00002916
Iteration 10/1000 | Loss: 0.00002886
Iteration 11/1000 | Loss: 0.00002858
Iteration 12/1000 | Loss: 0.00002832
Iteration 13/1000 | Loss: 0.00002813
Iteration 14/1000 | Loss: 0.00002803
Iteration 15/1000 | Loss: 0.00002802
Iteration 16/1000 | Loss: 0.00002789
Iteration 17/1000 | Loss: 0.00002788
Iteration 18/1000 | Loss: 0.00002787
Iteration 19/1000 | Loss: 0.00002786
Iteration 20/1000 | Loss: 0.00002786
Iteration 21/1000 | Loss: 0.00002785
Iteration 22/1000 | Loss: 0.00002785
Iteration 23/1000 | Loss: 0.00002785
Iteration 24/1000 | Loss: 0.00002783
Iteration 25/1000 | Loss: 0.00002782
Iteration 26/1000 | Loss: 0.00002782
Iteration 27/1000 | Loss: 0.00002781
Iteration 28/1000 | Loss: 0.00002781
Iteration 29/1000 | Loss: 0.00002781
Iteration 30/1000 | Loss: 0.00002780
Iteration 31/1000 | Loss: 0.00002780
Iteration 32/1000 | Loss: 0.00002780
Iteration 33/1000 | Loss: 0.00002779
Iteration 34/1000 | Loss: 0.00002779
Iteration 35/1000 | Loss: 0.00002778
Iteration 36/1000 | Loss: 0.00002778
Iteration 37/1000 | Loss: 0.00002778
Iteration 38/1000 | Loss: 0.00002777
Iteration 39/1000 | Loss: 0.00002777
Iteration 40/1000 | Loss: 0.00002777
Iteration 41/1000 | Loss: 0.00002777
Iteration 42/1000 | Loss: 0.00002777
Iteration 43/1000 | Loss: 0.00002777
Iteration 44/1000 | Loss: 0.00002777
Iteration 45/1000 | Loss: 0.00002776
Iteration 46/1000 | Loss: 0.00002776
Iteration 47/1000 | Loss: 0.00002776
Iteration 48/1000 | Loss: 0.00002776
Iteration 49/1000 | Loss: 0.00002776
Iteration 50/1000 | Loss: 0.00002776
Iteration 51/1000 | Loss: 0.00002776
Iteration 52/1000 | Loss: 0.00002776
Iteration 53/1000 | Loss: 0.00002776
Iteration 54/1000 | Loss: 0.00002776
Iteration 55/1000 | Loss: 0.00002776
Iteration 56/1000 | Loss: 0.00002776
Iteration 57/1000 | Loss: 0.00002776
Iteration 58/1000 | Loss: 0.00002775
Iteration 59/1000 | Loss: 0.00002775
Iteration 60/1000 | Loss: 0.00002775
Iteration 61/1000 | Loss: 0.00002775
Iteration 62/1000 | Loss: 0.00002775
Iteration 63/1000 | Loss: 0.00002774
Iteration 64/1000 | Loss: 0.00002774
Iteration 65/1000 | Loss: 0.00002774
Iteration 66/1000 | Loss: 0.00002774
Iteration 67/1000 | Loss: 0.00002774
Iteration 68/1000 | Loss: 0.00002774
Iteration 69/1000 | Loss: 0.00002774
Iteration 70/1000 | Loss: 0.00002773
Iteration 71/1000 | Loss: 0.00002773
Iteration 72/1000 | Loss: 0.00002773
Iteration 73/1000 | Loss: 0.00002773
Iteration 74/1000 | Loss: 0.00002773
Iteration 75/1000 | Loss: 0.00002772
Iteration 76/1000 | Loss: 0.00002772
Iteration 77/1000 | Loss: 0.00002772
Iteration 78/1000 | Loss: 0.00002772
Iteration 79/1000 | Loss: 0.00002772
Iteration 80/1000 | Loss: 0.00002771
Iteration 81/1000 | Loss: 0.00002771
Iteration 82/1000 | Loss: 0.00002771
Iteration 83/1000 | Loss: 0.00002771
Iteration 84/1000 | Loss: 0.00002771
Iteration 85/1000 | Loss: 0.00002771
Iteration 86/1000 | Loss: 0.00002771
Iteration 87/1000 | Loss: 0.00002771
Iteration 88/1000 | Loss: 0.00002770
Iteration 89/1000 | Loss: 0.00002770
Iteration 90/1000 | Loss: 0.00002770
Iteration 91/1000 | Loss: 0.00002770
Iteration 92/1000 | Loss: 0.00002770
Iteration 93/1000 | Loss: 0.00002770
Iteration 94/1000 | Loss: 0.00002770
Iteration 95/1000 | Loss: 0.00002770
Iteration 96/1000 | Loss: 0.00002770
Iteration 97/1000 | Loss: 0.00002770
Iteration 98/1000 | Loss: 0.00002770
Iteration 99/1000 | Loss: 0.00002770
Iteration 100/1000 | Loss: 0.00002770
Iteration 101/1000 | Loss: 0.00002769
Iteration 102/1000 | Loss: 0.00002769
Iteration 103/1000 | Loss: 0.00002769
Iteration 104/1000 | Loss: 0.00002769
Iteration 105/1000 | Loss: 0.00002769
Iteration 106/1000 | Loss: 0.00002768
Iteration 107/1000 | Loss: 0.00002768
Iteration 108/1000 | Loss: 0.00002768
Iteration 109/1000 | Loss: 0.00002768
Iteration 110/1000 | Loss: 0.00002768
Iteration 111/1000 | Loss: 0.00002768
Iteration 112/1000 | Loss: 0.00002767
Iteration 113/1000 | Loss: 0.00002767
Iteration 114/1000 | Loss: 0.00002767
Iteration 115/1000 | Loss: 0.00002767
Iteration 116/1000 | Loss: 0.00002767
Iteration 117/1000 | Loss: 0.00002766
Iteration 118/1000 | Loss: 0.00002766
Iteration 119/1000 | Loss: 0.00002766
Iteration 120/1000 | Loss: 0.00002766
Iteration 121/1000 | Loss: 0.00002766
Iteration 122/1000 | Loss: 0.00002766
Iteration 123/1000 | Loss: 0.00002766
Iteration 124/1000 | Loss: 0.00002766
Iteration 125/1000 | Loss: 0.00002766
Iteration 126/1000 | Loss: 0.00002766
Iteration 127/1000 | Loss: 0.00002766
Iteration 128/1000 | Loss: 0.00002766
Iteration 129/1000 | Loss: 0.00002766
Iteration 130/1000 | Loss: 0.00002766
Iteration 131/1000 | Loss: 0.00002766
Iteration 132/1000 | Loss: 0.00002766
Iteration 133/1000 | Loss: 0.00002766
Iteration 134/1000 | Loss: 0.00002765
Iteration 135/1000 | Loss: 0.00002765
Iteration 136/1000 | Loss: 0.00002765
Iteration 137/1000 | Loss: 0.00002765
Iteration 138/1000 | Loss: 0.00002765
Iteration 139/1000 | Loss: 0.00002765
Iteration 140/1000 | Loss: 0.00002765
Iteration 141/1000 | Loss: 0.00002765
Iteration 142/1000 | Loss: 0.00002765
Iteration 143/1000 | Loss: 0.00002765
Iteration 144/1000 | Loss: 0.00002765
Iteration 145/1000 | Loss: 0.00002765
Iteration 146/1000 | Loss: 0.00002765
Iteration 147/1000 | Loss: 0.00002765
Iteration 148/1000 | Loss: 0.00002765
Iteration 149/1000 | Loss: 0.00002765
Iteration 150/1000 | Loss: 0.00002765
Iteration 151/1000 | Loss: 0.00002765
Iteration 152/1000 | Loss: 0.00002765
Iteration 153/1000 | Loss: 0.00002765
Iteration 154/1000 | Loss: 0.00002765
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 154. Stopping optimization.
Last 5 losses: [2.7645031877909787e-05, 2.7645031877909787e-05, 2.7645031877909787e-05, 2.7645031877909787e-05, 2.7645031877909787e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.7645031877909787e-05

Optimization complete. Final v2v error: 4.304283142089844 mm

Highest mean error: 5.053520202636719 mm for frame 40

Lowest mean error: 3.777745008468628 mm for frame 153

Saving results

Total time: 40.21029758453369
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_006/1068/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_006/1068.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_006/1068
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00797249
Iteration 2/25 | Loss: 0.00134403
Iteration 3/25 | Loss: 0.00089334
Iteration 4/25 | Loss: 0.00086268
Iteration 5/25 | Loss: 0.00085974
Iteration 6/25 | Loss: 0.00085918
Iteration 7/25 | Loss: 0.00085917
Iteration 8/25 | Loss: 0.00085917
Iteration 9/25 | Loss: 0.00085917
Iteration 10/25 | Loss: 0.00085917
Iteration 11/25 | Loss: 0.00085917
Iteration 12/25 | Loss: 0.00085917
Iteration 13/25 | Loss: 0.00085917
Iteration 14/25 | Loss: 0.00085917
Iteration 15/25 | Loss: 0.00085917
Iteration 16/25 | Loss: 0.00085917
Iteration 17/25 | Loss: 0.00085917
Iteration 18/25 | Loss: 0.00085917
Iteration 19/25 | Loss: 0.00085917
Iteration 20/25 | Loss: 0.00085917
Iteration 21/25 | Loss: 0.00085917
Iteration 22/25 | Loss: 0.00085917
Iteration 23/25 | Loss: 0.00085917
Iteration 24/25 | Loss: 0.00085917
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0008591676596552134, 0.0008591676596552134, 0.0008591676596552134, 0.0008591676596552134, 0.0008591676596552134]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008591676596552134

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.51160264
Iteration 2/25 | Loss: 0.00053404
Iteration 3/25 | Loss: 0.00053402
Iteration 4/25 | Loss: 0.00053402
Iteration 5/25 | Loss: 0.00053402
Iteration 6/25 | Loss: 0.00053402
Iteration 7/25 | Loss: 0.00053402
Iteration 8/25 | Loss: 0.00053402
Iteration 9/25 | Loss: 0.00053402
Iteration 10/25 | Loss: 0.00053402
Iteration 11/25 | Loss: 0.00053402
Iteration 12/25 | Loss: 0.00053402
Iteration 13/25 | Loss: 0.00053402
Iteration 14/25 | Loss: 0.00053402
Iteration 15/25 | Loss: 0.00053402
Iteration 16/25 | Loss: 0.00053402
Iteration 17/25 | Loss: 0.00053402
Iteration 18/25 | Loss: 0.00053402
Iteration 19/25 | Loss: 0.00053402
Iteration 20/25 | Loss: 0.00053402
Iteration 21/25 | Loss: 0.00053402
Iteration 22/25 | Loss: 0.00053402
Iteration 23/25 | Loss: 0.00053402
Iteration 24/25 | Loss: 0.00053402
Iteration 25/25 | Loss: 0.00053402

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00053402
Iteration 2/1000 | Loss: 0.00003001
Iteration 3/1000 | Loss: 0.00002370
Iteration 4/1000 | Loss: 0.00002121
Iteration 5/1000 | Loss: 0.00002026
Iteration 6/1000 | Loss: 0.00001967
Iteration 7/1000 | Loss: 0.00001938
Iteration 8/1000 | Loss: 0.00001925
Iteration 9/1000 | Loss: 0.00001897
Iteration 10/1000 | Loss: 0.00001876
Iteration 11/1000 | Loss: 0.00001861
Iteration 12/1000 | Loss: 0.00001860
Iteration 13/1000 | Loss: 0.00001860
Iteration 14/1000 | Loss: 0.00001848
Iteration 15/1000 | Loss: 0.00001848
Iteration 16/1000 | Loss: 0.00001847
Iteration 17/1000 | Loss: 0.00001846
Iteration 18/1000 | Loss: 0.00001846
Iteration 19/1000 | Loss: 0.00001845
Iteration 20/1000 | Loss: 0.00001845
Iteration 21/1000 | Loss: 0.00001844
Iteration 22/1000 | Loss: 0.00001844
Iteration 23/1000 | Loss: 0.00001844
Iteration 24/1000 | Loss: 0.00001844
Iteration 25/1000 | Loss: 0.00001844
Iteration 26/1000 | Loss: 0.00001844
Iteration 27/1000 | Loss: 0.00001843
Iteration 28/1000 | Loss: 0.00001843
Iteration 29/1000 | Loss: 0.00001843
Iteration 30/1000 | Loss: 0.00001842
Iteration 31/1000 | Loss: 0.00001842
Iteration 32/1000 | Loss: 0.00001842
Iteration 33/1000 | Loss: 0.00001842
Iteration 34/1000 | Loss: 0.00001842
Iteration 35/1000 | Loss: 0.00001842
Iteration 36/1000 | Loss: 0.00001842
Iteration 37/1000 | Loss: 0.00001842
Iteration 38/1000 | Loss: 0.00001842
Iteration 39/1000 | Loss: 0.00001840
Iteration 40/1000 | Loss: 0.00001838
Iteration 41/1000 | Loss: 0.00001838
Iteration 42/1000 | Loss: 0.00001838
Iteration 43/1000 | Loss: 0.00001838
Iteration 44/1000 | Loss: 0.00001838
Iteration 45/1000 | Loss: 0.00001838
Iteration 46/1000 | Loss: 0.00001838
Iteration 47/1000 | Loss: 0.00001838
Iteration 48/1000 | Loss: 0.00001838
Iteration 49/1000 | Loss: 0.00001838
Iteration 50/1000 | Loss: 0.00001837
Iteration 51/1000 | Loss: 0.00001837
Iteration 52/1000 | Loss: 0.00001837
Iteration 53/1000 | Loss: 0.00001836
Iteration 54/1000 | Loss: 0.00001835
Iteration 55/1000 | Loss: 0.00001835
Iteration 56/1000 | Loss: 0.00001834
Iteration 57/1000 | Loss: 0.00001834
Iteration 58/1000 | Loss: 0.00001834
Iteration 59/1000 | Loss: 0.00001834
Iteration 60/1000 | Loss: 0.00001833
Iteration 61/1000 | Loss: 0.00001833
Iteration 62/1000 | Loss: 0.00001832
Iteration 63/1000 | Loss: 0.00001831
Iteration 64/1000 | Loss: 0.00001831
Iteration 65/1000 | Loss: 0.00001831
Iteration 66/1000 | Loss: 0.00001831
Iteration 67/1000 | Loss: 0.00001831
Iteration 68/1000 | Loss: 0.00001830
Iteration 69/1000 | Loss: 0.00001830
Iteration 70/1000 | Loss: 0.00001830
Iteration 71/1000 | Loss: 0.00001830
Iteration 72/1000 | Loss: 0.00001830
Iteration 73/1000 | Loss: 0.00001830
Iteration 74/1000 | Loss: 0.00001830
Iteration 75/1000 | Loss: 0.00001829
Iteration 76/1000 | Loss: 0.00001829
Iteration 77/1000 | Loss: 0.00001829
Iteration 78/1000 | Loss: 0.00001829
Iteration 79/1000 | Loss: 0.00001829
Iteration 80/1000 | Loss: 0.00001829
Iteration 81/1000 | Loss: 0.00001829
Iteration 82/1000 | Loss: 0.00001829
Iteration 83/1000 | Loss: 0.00001829
Iteration 84/1000 | Loss: 0.00001829
Iteration 85/1000 | Loss: 0.00001829
Iteration 86/1000 | Loss: 0.00001829
Iteration 87/1000 | Loss: 0.00001829
Iteration 88/1000 | Loss: 0.00001829
Iteration 89/1000 | Loss: 0.00001829
Iteration 90/1000 | Loss: 0.00001829
Iteration 91/1000 | Loss: 0.00001829
Iteration 92/1000 | Loss: 0.00001829
Iteration 93/1000 | Loss: 0.00001829
Iteration 94/1000 | Loss: 0.00001829
Iteration 95/1000 | Loss: 0.00001829
Iteration 96/1000 | Loss: 0.00001829
Iteration 97/1000 | Loss: 0.00001829
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 97. Stopping optimization.
Last 5 losses: [1.828643689805176e-05, 1.828643689805176e-05, 1.828643689805176e-05, 1.828643689805176e-05, 1.828643689805176e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.828643689805176e-05

Optimization complete. Final v2v error: 3.634654998779297 mm

Highest mean error: 3.862837314605713 mm for frame 60

Lowest mean error: 3.4091084003448486 mm for frame 82

Saving results

Total time: 30.346195936203003
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_006/1079/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_006/1079.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_006/1079
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00989700
Iteration 2/25 | Loss: 0.00500627
Iteration 3/25 | Loss: 0.00356025
Iteration 4/25 | Loss: 0.00295381
Iteration 5/25 | Loss: 0.00238989
Iteration 6/25 | Loss: 0.00209233
Iteration 7/25 | Loss: 0.00198884
Iteration 8/25 | Loss: 0.00186699
Iteration 9/25 | Loss: 0.00175021
Iteration 10/25 | Loss: 0.00172262
Iteration 11/25 | Loss: 0.00167366
Iteration 12/25 | Loss: 0.00163318
Iteration 13/25 | Loss: 0.00162182
Iteration 14/25 | Loss: 0.00157899
Iteration 15/25 | Loss: 0.00153230
Iteration 16/25 | Loss: 0.00148857
Iteration 17/25 | Loss: 0.00145495
Iteration 18/25 | Loss: 0.00144876
Iteration 19/25 | Loss: 0.00144809
Iteration 20/25 | Loss: 0.00144996
Iteration 21/25 | Loss: 0.00144367
Iteration 22/25 | Loss: 0.00144509
Iteration 23/25 | Loss: 0.00143767
Iteration 24/25 | Loss: 0.00143949
Iteration 25/25 | Loss: 0.00143455

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.48843551
Iteration 2/25 | Loss: 0.00702404
Iteration 3/25 | Loss: 0.00687957
Iteration 4/25 | Loss: 0.00687957
Iteration 5/25 | Loss: 0.00687957
Iteration 6/25 | Loss: 0.00687957
Iteration 7/25 | Loss: 0.00687957
Iteration 8/25 | Loss: 0.00687957
Iteration 9/25 | Loss: 0.00687957
Iteration 10/25 | Loss: 0.00687957
Iteration 11/25 | Loss: 0.00687957
Iteration 12/25 | Loss: 0.00687957
Iteration 13/25 | Loss: 0.00687957
Iteration 14/25 | Loss: 0.00687957
Iteration 15/25 | Loss: 0.00687957
Iteration 16/25 | Loss: 0.00687957
Iteration 17/25 | Loss: 0.00687957
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.006879569496959448, 0.006879569496959448, 0.006879569496959448, 0.006879569496959448, 0.006879569496959448]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.006879569496959448

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00687957
Iteration 2/1000 | Loss: 0.00179673
Iteration 3/1000 | Loss: 0.00264853
Iteration 4/1000 | Loss: 0.00201676
Iteration 5/1000 | Loss: 0.00206294
Iteration 6/1000 | Loss: 0.00058890
Iteration 7/1000 | Loss: 0.00138937
Iteration 8/1000 | Loss: 0.00043139
Iteration 9/1000 | Loss: 0.00069720
Iteration 10/1000 | Loss: 0.00032987
Iteration 11/1000 | Loss: 0.00041628
Iteration 12/1000 | Loss: 0.00026178
Iteration 13/1000 | Loss: 0.00023819
Iteration 14/1000 | Loss: 0.00024647
Iteration 15/1000 | Loss: 0.00047060
Iteration 16/1000 | Loss: 0.00038773
Iteration 17/1000 | Loss: 0.00160027
Iteration 18/1000 | Loss: 0.01675894
Iteration 19/1000 | Loss: 0.00705349
Iteration 20/1000 | Loss: 0.00322683
Iteration 21/1000 | Loss: 0.00192675
Iteration 22/1000 | Loss: 0.00487141
Iteration 23/1000 | Loss: 0.00261065
Iteration 24/1000 | Loss: 0.00329858
Iteration 25/1000 | Loss: 0.00181181
Iteration 26/1000 | Loss: 0.00126795
Iteration 27/1000 | Loss: 0.00097973
Iteration 28/1000 | Loss: 0.00130910
Iteration 29/1000 | Loss: 0.00164831
Iteration 30/1000 | Loss: 0.00080401
Iteration 31/1000 | Loss: 0.00066350
Iteration 32/1000 | Loss: 0.00099885
Iteration 33/1000 | Loss: 0.00037470
Iteration 34/1000 | Loss: 0.00119082
Iteration 35/1000 | Loss: 0.00053317
Iteration 36/1000 | Loss: 0.00052568
Iteration 37/1000 | Loss: 0.00028541
Iteration 38/1000 | Loss: 0.00076403
Iteration 39/1000 | Loss: 0.00022957
Iteration 40/1000 | Loss: 0.00022997
Iteration 41/1000 | Loss: 0.00021281
Iteration 42/1000 | Loss: 0.00042875
Iteration 43/1000 | Loss: 0.00070497
Iteration 44/1000 | Loss: 0.00020335
Iteration 45/1000 | Loss: 0.00029116
Iteration 46/1000 | Loss: 0.00032070
Iteration 47/1000 | Loss: 0.00027813
Iteration 48/1000 | Loss: 0.00026027
Iteration 49/1000 | Loss: 0.00013382
Iteration 50/1000 | Loss: 0.00021179
Iteration 51/1000 | Loss: 0.00008419
Iteration 52/1000 | Loss: 0.00008735
Iteration 53/1000 | Loss: 0.00057845
Iteration 54/1000 | Loss: 0.00014037
Iteration 55/1000 | Loss: 0.00067066
Iteration 56/1000 | Loss: 0.00088674
Iteration 57/1000 | Loss: 0.00051915
Iteration 58/1000 | Loss: 0.00045951
Iteration 59/1000 | Loss: 0.00037475
Iteration 60/1000 | Loss: 0.00054700
Iteration 61/1000 | Loss: 0.00025477
Iteration 62/1000 | Loss: 0.00007426
Iteration 63/1000 | Loss: 0.00006984
Iteration 64/1000 | Loss: 0.00030252
Iteration 65/1000 | Loss: 0.00007247
Iteration 66/1000 | Loss: 0.00006742
Iteration 67/1000 | Loss: 0.00006536
Iteration 68/1000 | Loss: 0.00006374
Iteration 69/1000 | Loss: 0.00006168
Iteration 70/1000 | Loss: 0.00005979
Iteration 71/1000 | Loss: 0.00005881
Iteration 72/1000 | Loss: 0.00069868
Iteration 73/1000 | Loss: 0.00006897
Iteration 74/1000 | Loss: 0.00005739
Iteration 75/1000 | Loss: 0.00005535
Iteration 76/1000 | Loss: 0.00005445
Iteration 77/1000 | Loss: 0.00005388
Iteration 78/1000 | Loss: 0.00005333
Iteration 79/1000 | Loss: 0.00005828
Iteration 80/1000 | Loss: 0.00005317
Iteration 81/1000 | Loss: 0.00005233
Iteration 82/1000 | Loss: 0.00021833
Iteration 83/1000 | Loss: 0.00005715
Iteration 84/1000 | Loss: 0.00005473
Iteration 85/1000 | Loss: 0.00005291
Iteration 86/1000 | Loss: 0.00005238
Iteration 87/1000 | Loss: 0.00005213
Iteration 88/1000 | Loss: 0.00026836
Iteration 89/1000 | Loss: 0.00025798
Iteration 90/1000 | Loss: 0.00005702
Iteration 91/1000 | Loss: 0.00005337
Iteration 92/1000 | Loss: 0.00005114
Iteration 93/1000 | Loss: 0.00004894
Iteration 94/1000 | Loss: 0.00004809
Iteration 95/1000 | Loss: 0.00004765
Iteration 96/1000 | Loss: 0.00004740
Iteration 97/1000 | Loss: 0.00006916
Iteration 98/1000 | Loss: 0.00006916
Iteration 99/1000 | Loss: 0.00006520
Iteration 100/1000 | Loss: 0.00007799
Iteration 101/1000 | Loss: 0.00023160
Iteration 102/1000 | Loss: 0.00018149
Iteration 103/1000 | Loss: 0.00020852
Iteration 104/1000 | Loss: 0.00075216
Iteration 105/1000 | Loss: 0.00006945
Iteration 106/1000 | Loss: 0.00005796
Iteration 107/1000 | Loss: 0.00021809
Iteration 108/1000 | Loss: 0.00006982
Iteration 109/1000 | Loss: 0.00055943
Iteration 110/1000 | Loss: 0.00009254
Iteration 111/1000 | Loss: 0.00034525
Iteration 112/1000 | Loss: 0.00009594
Iteration 113/1000 | Loss: 0.00017657
Iteration 114/1000 | Loss: 0.00005536
Iteration 115/1000 | Loss: 0.00005251
Iteration 116/1000 | Loss: 0.00060963
Iteration 117/1000 | Loss: 0.00008177
Iteration 118/1000 | Loss: 0.00005964
Iteration 119/1000 | Loss: 0.00071925
Iteration 120/1000 | Loss: 0.00023240
Iteration 121/1000 | Loss: 0.00005108
Iteration 122/1000 | Loss: 0.00004923
Iteration 123/1000 | Loss: 0.00004867
Iteration 124/1000 | Loss: 0.00004824
Iteration 125/1000 | Loss: 0.00004795
Iteration 126/1000 | Loss: 0.00004772
Iteration 127/1000 | Loss: 0.00004764
Iteration 128/1000 | Loss: 0.00004745
Iteration 129/1000 | Loss: 0.00071195
Iteration 130/1000 | Loss: 0.00023365
Iteration 131/1000 | Loss: 0.00004823
Iteration 132/1000 | Loss: 0.00004745
Iteration 133/1000 | Loss: 0.00136523
Iteration 134/1000 | Loss: 0.00110324
Iteration 135/1000 | Loss: 0.00122277
Iteration 136/1000 | Loss: 0.00040762
Iteration 137/1000 | Loss: 0.00007806
Iteration 138/1000 | Loss: 0.00010928
Iteration 139/1000 | Loss: 0.00006887
Iteration 140/1000 | Loss: 0.00005188
Iteration 141/1000 | Loss: 0.00004918
Iteration 142/1000 | Loss: 0.00004861
Iteration 143/1000 | Loss: 0.00004824
Iteration 144/1000 | Loss: 0.00004794
Iteration 145/1000 | Loss: 0.00004778
Iteration 146/1000 | Loss: 0.00004760
Iteration 147/1000 | Loss: 0.00017109
Iteration 148/1000 | Loss: 0.00090039
Iteration 149/1000 | Loss: 0.00131210
Iteration 150/1000 | Loss: 0.00026932
Iteration 151/1000 | Loss: 0.00006339
Iteration 152/1000 | Loss: 0.00005347
Iteration 153/1000 | Loss: 0.00004921
Iteration 154/1000 | Loss: 0.00004722
Iteration 155/1000 | Loss: 0.00004583
Iteration 156/1000 | Loss: 0.00004494
Iteration 157/1000 | Loss: 0.00073095
Iteration 158/1000 | Loss: 0.00070748
Iteration 159/1000 | Loss: 0.00025632
Iteration 160/1000 | Loss: 0.00005322
Iteration 161/1000 | Loss: 0.00028376
Iteration 162/1000 | Loss: 0.00005489
Iteration 163/1000 | Loss: 0.00004948
Iteration 164/1000 | Loss: 0.00051351
Iteration 165/1000 | Loss: 0.00027089
Iteration 166/1000 | Loss: 0.00016122
Iteration 167/1000 | Loss: 0.00022442
Iteration 168/1000 | Loss: 0.00006772
Iteration 169/1000 | Loss: 0.00006400
Iteration 170/1000 | Loss: 0.00074236
Iteration 171/1000 | Loss: 0.00080699
Iteration 172/1000 | Loss: 0.00028272
Iteration 173/1000 | Loss: 0.00006046
Iteration 174/1000 | Loss: 0.00005397
Iteration 175/1000 | Loss: 0.00004115
Iteration 176/1000 | Loss: 0.00003908
Iteration 177/1000 | Loss: 0.00003769
Iteration 178/1000 | Loss: 0.00003691
Iteration 179/1000 | Loss: 0.00003647
Iteration 180/1000 | Loss: 0.00003612
Iteration 181/1000 | Loss: 0.00003586
Iteration 182/1000 | Loss: 0.00095531
Iteration 183/1000 | Loss: 0.00027067
Iteration 184/1000 | Loss: 0.00057560
Iteration 185/1000 | Loss: 0.00005343
Iteration 186/1000 | Loss: 0.00004129
Iteration 187/1000 | Loss: 0.00027211
Iteration 188/1000 | Loss: 0.00037167
Iteration 189/1000 | Loss: 0.00083868
Iteration 190/1000 | Loss: 0.00039690
Iteration 191/1000 | Loss: 0.00060725
Iteration 192/1000 | Loss: 0.00061728
Iteration 193/1000 | Loss: 0.00078388
Iteration 194/1000 | Loss: 0.00068363
Iteration 195/1000 | Loss: 0.00044626
Iteration 196/1000 | Loss: 0.00005093
Iteration 197/1000 | Loss: 0.00023393
Iteration 198/1000 | Loss: 0.00004310
Iteration 199/1000 | Loss: 0.00003761
Iteration 200/1000 | Loss: 0.00003528
Iteration 201/1000 | Loss: 0.00003414
Iteration 202/1000 | Loss: 0.00003324
Iteration 203/1000 | Loss: 0.00003272
Iteration 204/1000 | Loss: 0.00003242
Iteration 205/1000 | Loss: 0.00003226
Iteration 206/1000 | Loss: 0.00003223
Iteration 207/1000 | Loss: 0.00003221
Iteration 208/1000 | Loss: 0.00003216
Iteration 209/1000 | Loss: 0.00003213
Iteration 210/1000 | Loss: 0.00003212
Iteration 211/1000 | Loss: 0.00003207
Iteration 212/1000 | Loss: 0.00003207
Iteration 213/1000 | Loss: 0.00003199
Iteration 214/1000 | Loss: 0.00003195
Iteration 215/1000 | Loss: 0.00003194
Iteration 216/1000 | Loss: 0.00003191
Iteration 217/1000 | Loss: 0.00003191
Iteration 218/1000 | Loss: 0.00003191
Iteration 219/1000 | Loss: 0.00003191
Iteration 220/1000 | Loss: 0.00003191
Iteration 221/1000 | Loss: 0.00003191
Iteration 222/1000 | Loss: 0.00003190
Iteration 223/1000 | Loss: 0.00003190
Iteration 224/1000 | Loss: 0.00003189
Iteration 225/1000 | Loss: 0.00003188
Iteration 226/1000 | Loss: 0.00003187
Iteration 227/1000 | Loss: 0.00003187
Iteration 228/1000 | Loss: 0.00003186
Iteration 229/1000 | Loss: 0.00003186
Iteration 230/1000 | Loss: 0.00003186
Iteration 231/1000 | Loss: 0.00003186
Iteration 232/1000 | Loss: 0.00003185
Iteration 233/1000 | Loss: 0.00003185
Iteration 234/1000 | Loss: 0.00003185
Iteration 235/1000 | Loss: 0.00003184
Iteration 236/1000 | Loss: 0.00003184
Iteration 237/1000 | Loss: 0.00003184
Iteration 238/1000 | Loss: 0.00003184
Iteration 239/1000 | Loss: 0.00003184
Iteration 240/1000 | Loss: 0.00003184
Iteration 241/1000 | Loss: 0.00003184
Iteration 242/1000 | Loss: 0.00003184
Iteration 243/1000 | Loss: 0.00003184
Iteration 244/1000 | Loss: 0.00003183
Iteration 245/1000 | Loss: 0.00003183
Iteration 246/1000 | Loss: 0.00003183
Iteration 247/1000 | Loss: 0.00003183
Iteration 248/1000 | Loss: 0.00003183
Iteration 249/1000 | Loss: 0.00003182
Iteration 250/1000 | Loss: 0.00003182
Iteration 251/1000 | Loss: 0.00003181
Iteration 252/1000 | Loss: 0.00003181
Iteration 253/1000 | Loss: 0.00003181
Iteration 254/1000 | Loss: 0.00003180
Iteration 255/1000 | Loss: 0.00003180
Iteration 256/1000 | Loss: 0.00003179
Iteration 257/1000 | Loss: 0.00003178
Iteration 258/1000 | Loss: 0.00003177
Iteration 259/1000 | Loss: 0.00003177
Iteration 260/1000 | Loss: 0.00003177
Iteration 261/1000 | Loss: 0.00003177
Iteration 262/1000 | Loss: 0.00003177
Iteration 263/1000 | Loss: 0.00003177
Iteration 264/1000 | Loss: 0.00003177
Iteration 265/1000 | Loss: 0.00003177
Iteration 266/1000 | Loss: 0.00003176
Iteration 267/1000 | Loss: 0.00003176
Iteration 268/1000 | Loss: 0.00003175
Iteration 269/1000 | Loss: 0.00003175
Iteration 270/1000 | Loss: 0.00003175
Iteration 271/1000 | Loss: 0.00003175
Iteration 272/1000 | Loss: 0.00003174
Iteration 273/1000 | Loss: 0.00003174
Iteration 274/1000 | Loss: 0.00003174
Iteration 275/1000 | Loss: 0.00003173
Iteration 276/1000 | Loss: 0.00003173
Iteration 277/1000 | Loss: 0.00003172
Iteration 278/1000 | Loss: 0.00003172
Iteration 279/1000 | Loss: 0.00003172
Iteration 280/1000 | Loss: 0.00003171
Iteration 281/1000 | Loss: 0.00003171
Iteration 282/1000 | Loss: 0.00003170
Iteration 283/1000 | Loss: 0.00003170
Iteration 284/1000 | Loss: 0.00003169
Iteration 285/1000 | Loss: 0.00003169
Iteration 286/1000 | Loss: 0.00003169
Iteration 287/1000 | Loss: 0.00003167
Iteration 288/1000 | Loss: 0.00003166
Iteration 289/1000 | Loss: 0.00003166
Iteration 290/1000 | Loss: 0.00003166
Iteration 291/1000 | Loss: 0.00003166
Iteration 292/1000 | Loss: 0.00003166
Iteration 293/1000 | Loss: 0.00003165
Iteration 294/1000 | Loss: 0.00003163
Iteration 295/1000 | Loss: 0.00003163
Iteration 296/1000 | Loss: 0.00003158
Iteration 297/1000 | Loss: 0.00003137
Iteration 298/1000 | Loss: 0.00141420
Iteration 299/1000 | Loss: 0.00040688
Iteration 300/1000 | Loss: 0.00007236
Iteration 301/1000 | Loss: 0.00008807
Iteration 302/1000 | Loss: 0.00010736
Iteration 303/1000 | Loss: 0.00007360
Iteration 304/1000 | Loss: 0.00007631
Iteration 305/1000 | Loss: 0.00004820
Iteration 306/1000 | Loss: 0.00003334
Iteration 307/1000 | Loss: 0.00003177
Iteration 308/1000 | Loss: 0.00003079
Iteration 309/1000 | Loss: 0.00003025
Iteration 310/1000 | Loss: 0.00002998
Iteration 311/1000 | Loss: 0.00002957
Iteration 312/1000 | Loss: 0.00002932
Iteration 313/1000 | Loss: 0.00002926
Iteration 314/1000 | Loss: 0.00002913
Iteration 315/1000 | Loss: 0.00002908
Iteration 316/1000 | Loss: 0.00002908
Iteration 317/1000 | Loss: 0.00002907
Iteration 318/1000 | Loss: 0.00002902
Iteration 319/1000 | Loss: 0.00002901
Iteration 320/1000 | Loss: 0.00002901
Iteration 321/1000 | Loss: 0.00002900
Iteration 322/1000 | Loss: 0.00002900
Iteration 323/1000 | Loss: 0.00002893
Iteration 324/1000 | Loss: 0.00002889
Iteration 325/1000 | Loss: 0.00002888
Iteration 326/1000 | Loss: 0.00002881
Iteration 327/1000 | Loss: 0.00002868
Iteration 328/1000 | Loss: 0.00002846
Iteration 329/1000 | Loss: 0.00002838
Iteration 330/1000 | Loss: 0.00002821
Iteration 331/1000 | Loss: 0.00002816
Iteration 332/1000 | Loss: 0.00002816
Iteration 333/1000 | Loss: 0.00002813
Iteration 334/1000 | Loss: 0.00002812
Iteration 335/1000 | Loss: 0.00002812
Iteration 336/1000 | Loss: 0.00002811
Iteration 337/1000 | Loss: 0.00002810
Iteration 338/1000 | Loss: 0.00002808
Iteration 339/1000 | Loss: 0.00002807
Iteration 340/1000 | Loss: 0.00002804
Iteration 341/1000 | Loss: 0.00002798
Iteration 342/1000 | Loss: 0.00002796
Iteration 343/1000 | Loss: 0.00002796
Iteration 344/1000 | Loss: 0.00002796
Iteration 345/1000 | Loss: 0.00002796
Iteration 346/1000 | Loss: 0.00002795
Iteration 347/1000 | Loss: 0.00002795
Iteration 348/1000 | Loss: 0.00002795
Iteration 349/1000 | Loss: 0.00002795
Iteration 350/1000 | Loss: 0.00002795
Iteration 351/1000 | Loss: 0.00002795
Iteration 352/1000 | Loss: 0.00002795
Iteration 353/1000 | Loss: 0.00002795
Iteration 354/1000 | Loss: 0.00002795
Iteration 355/1000 | Loss: 0.00002795
Iteration 356/1000 | Loss: 0.00002795
Iteration 357/1000 | Loss: 0.00002795
Iteration 358/1000 | Loss: 0.00002794
Iteration 359/1000 | Loss: 0.00002794
Iteration 360/1000 | Loss: 0.00002794
Iteration 361/1000 | Loss: 0.00002794
Iteration 362/1000 | Loss: 0.00002793
Iteration 363/1000 | Loss: 0.00002792
Iteration 364/1000 | Loss: 0.00002792
Iteration 365/1000 | Loss: 0.00002792
Iteration 366/1000 | Loss: 0.00002792
Iteration 367/1000 | Loss: 0.00002791
Iteration 368/1000 | Loss: 0.00002791
Iteration 369/1000 | Loss: 0.00002791
Iteration 370/1000 | Loss: 0.00002790
Iteration 371/1000 | Loss: 0.00032260
Iteration 372/1000 | Loss: 0.00002872
Iteration 373/1000 | Loss: 0.00002771
Iteration 374/1000 | Loss: 0.00002708
Iteration 375/1000 | Loss: 0.00002651
Iteration 376/1000 | Loss: 0.00002607
Iteration 377/1000 | Loss: 0.00002586
Iteration 378/1000 | Loss: 0.00002567
Iteration 379/1000 | Loss: 0.00002548
Iteration 380/1000 | Loss: 0.00002545
Iteration 381/1000 | Loss: 0.00002540
Iteration 382/1000 | Loss: 0.00002538
Iteration 383/1000 | Loss: 0.00002535
Iteration 384/1000 | Loss: 0.00002530
Iteration 385/1000 | Loss: 0.00002524
Iteration 386/1000 | Loss: 0.00002524
Iteration 387/1000 | Loss: 0.00002518
Iteration 388/1000 | Loss: 0.00002517
Iteration 389/1000 | Loss: 0.00002517
Iteration 390/1000 | Loss: 0.00002516
Iteration 391/1000 | Loss: 0.00002515
Iteration 392/1000 | Loss: 0.00002514
Iteration 393/1000 | Loss: 0.00002514
Iteration 394/1000 | Loss: 0.00002514
Iteration 395/1000 | Loss: 0.00002514
Iteration 396/1000 | Loss: 0.00002514
Iteration 397/1000 | Loss: 0.00002514
Iteration 398/1000 | Loss: 0.00002514
Iteration 399/1000 | Loss: 0.00002514
Iteration 400/1000 | Loss: 0.00002514
Iteration 401/1000 | Loss: 0.00002514
Iteration 402/1000 | Loss: 0.00002514
Iteration 403/1000 | Loss: 0.00002514
Iteration 404/1000 | Loss: 0.00002514
Iteration 405/1000 | Loss: 0.00002514
Iteration 406/1000 | Loss: 0.00002514
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 406. Stopping optimization.
Last 5 losses: [2.5135057512670755e-05, 2.5135057512670755e-05, 2.5135057512670755e-05, 2.5135057512670755e-05, 2.5135057512670755e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.5135057512670755e-05

Optimization complete. Final v2v error: 3.823167562484741 mm

Highest mean error: 12.557453155517578 mm for frame 193

Lowest mean error: 3.540640354156494 mm for frame 235

Saving results

Total time: 450.09660363197327
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_006/1020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_006/1020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_006/1020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00995938
Iteration 2/25 | Loss: 0.00995938
Iteration 3/25 | Loss: 0.00415889
Iteration 4/25 | Loss: 0.00253778
Iteration 5/25 | Loss: 0.00229796
Iteration 6/25 | Loss: 0.00173688
Iteration 7/25 | Loss: 0.00189180
Iteration 8/25 | Loss: 0.00176335
Iteration 9/25 | Loss: 0.00157702
Iteration 10/25 | Loss: 0.00155126
Iteration 11/25 | Loss: 0.00154120
Iteration 12/25 | Loss: 0.00153186
Iteration 13/25 | Loss: 0.00151901
Iteration 14/25 | Loss: 0.00151700
Iteration 15/25 | Loss: 0.00151538
Iteration 16/25 | Loss: 0.00151269
Iteration 17/25 | Loss: 0.00151073
Iteration 18/25 | Loss: 0.00150668
Iteration 19/25 | Loss: 0.00150726
Iteration 20/25 | Loss: 0.00150453
Iteration 21/25 | Loss: 0.00150425
Iteration 22/25 | Loss: 0.00150593
Iteration 23/25 | Loss: 0.00150682
Iteration 24/25 | Loss: 0.00150349
Iteration 25/25 | Loss: 0.00150283

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.49259734
Iteration 2/25 | Loss: 0.01419717
Iteration 3/25 | Loss: 0.00874020
Iteration 4/25 | Loss: 0.00874019
Iteration 5/25 | Loss: 0.00874019
Iteration 6/25 | Loss: 0.00874019
Iteration 7/25 | Loss: 0.00874019
Iteration 8/25 | Loss: 0.00874019
Iteration 9/25 | Loss: 0.00874019
Iteration 10/25 | Loss: 0.00874019
Iteration 11/25 | Loss: 0.00874019
Iteration 12/25 | Loss: 0.00874019
Iteration 13/25 | Loss: 0.00874019
Iteration 14/25 | Loss: 0.00874019
Iteration 15/25 | Loss: 0.00874019
Iteration 16/25 | Loss: 0.00874019
Iteration 17/25 | Loss: 0.00874019
Iteration 18/25 | Loss: 0.00874019
Iteration 19/25 | Loss: 0.00874019
Iteration 20/25 | Loss: 0.00874019
Iteration 21/25 | Loss: 0.00874019
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.008740190416574478, 0.008740190416574478, 0.008740190416574478, 0.008740190416574478, 0.008740190416574478]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.008740190416574478

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00874019
Iteration 2/1000 | Loss: 0.00617238
Iteration 3/1000 | Loss: 0.01077594
Iteration 4/1000 | Loss: 0.00992032
Iteration 5/1000 | Loss: 0.00273158
Iteration 6/1000 | Loss: 0.00093582
Iteration 7/1000 | Loss: 0.00109212
Iteration 8/1000 | Loss: 0.00112302
Iteration 9/1000 | Loss: 0.00113459
Iteration 10/1000 | Loss: 0.00253010
Iteration 11/1000 | Loss: 0.00053977
Iteration 12/1000 | Loss: 0.00073272
Iteration 13/1000 | Loss: 0.00132689
Iteration 14/1000 | Loss: 0.00067854
Iteration 15/1000 | Loss: 0.00046949
Iteration 16/1000 | Loss: 0.00042663
Iteration 17/1000 | Loss: 0.00070191
Iteration 18/1000 | Loss: 0.00057011
Iteration 19/1000 | Loss: 0.00067219
Iteration 20/1000 | Loss: 0.00061660
Iteration 21/1000 | Loss: 0.00064969
Iteration 22/1000 | Loss: 0.00050716
Iteration 23/1000 | Loss: 0.00051234
Iteration 24/1000 | Loss: 0.00044758
Iteration 25/1000 | Loss: 0.00045415
Iteration 26/1000 | Loss: 0.00035259
Iteration 27/1000 | Loss: 0.00137320
Iteration 28/1000 | Loss: 0.00069294
Iteration 29/1000 | Loss: 0.00230654
Iteration 30/1000 | Loss: 0.01764105
Iteration 31/1000 | Loss: 0.00373443
Iteration 32/1000 | Loss: 0.00285053
Iteration 33/1000 | Loss: 0.00200913
Iteration 34/1000 | Loss: 0.00152018
Iteration 35/1000 | Loss: 0.00443536
Iteration 36/1000 | Loss: 0.00041515
Iteration 37/1000 | Loss: 0.00105721
Iteration 38/1000 | Loss: 0.00241583
Iteration 39/1000 | Loss: 0.00090583
Iteration 40/1000 | Loss: 0.00086627
Iteration 41/1000 | Loss: 0.00535355
Iteration 42/1000 | Loss: 0.00038541
Iteration 43/1000 | Loss: 0.00250356
Iteration 44/1000 | Loss: 0.00297291
Iteration 45/1000 | Loss: 0.00047617
Iteration 46/1000 | Loss: 0.00103182
Iteration 47/1000 | Loss: 0.00088743
Iteration 48/1000 | Loss: 0.00041035
Iteration 49/1000 | Loss: 0.00055459
Iteration 50/1000 | Loss: 0.00059355
Iteration 51/1000 | Loss: 0.00202333
Iteration 52/1000 | Loss: 0.00052298
Iteration 53/1000 | Loss: 0.00105592
Iteration 54/1000 | Loss: 0.00054081
Iteration 55/1000 | Loss: 0.00046458
Iteration 56/1000 | Loss: 0.00030490
Iteration 57/1000 | Loss: 0.00044266
Iteration 58/1000 | Loss: 0.00062097
Iteration 59/1000 | Loss: 0.00047450
Iteration 60/1000 | Loss: 0.00039391
Iteration 61/1000 | Loss: 0.00052649
Iteration 62/1000 | Loss: 0.00038796
Iteration 63/1000 | Loss: 0.00042320
Iteration 64/1000 | Loss: 0.00166375
Iteration 65/1000 | Loss: 0.00435838
Iteration 66/1000 | Loss: 0.00201055
Iteration 67/1000 | Loss: 0.00387024
Iteration 68/1000 | Loss: 0.00191989
Iteration 69/1000 | Loss: 0.00046962
Iteration 70/1000 | Loss: 0.00029117
Iteration 71/1000 | Loss: 0.00037702
Iteration 72/1000 | Loss: 0.00037336
Iteration 73/1000 | Loss: 0.00025523
Iteration 74/1000 | Loss: 0.00032051
Iteration 75/1000 | Loss: 0.00024867
Iteration 76/1000 | Loss: 0.00046948
Iteration 77/1000 | Loss: 0.00321186
Iteration 78/1000 | Loss: 0.00056718
Iteration 79/1000 | Loss: 0.00029992
Iteration 80/1000 | Loss: 0.00088341
Iteration 81/1000 | Loss: 0.00043867
Iteration 82/1000 | Loss: 0.00040116
Iteration 83/1000 | Loss: 0.00074907
Iteration 84/1000 | Loss: 0.00049955
Iteration 85/1000 | Loss: 0.00043206
Iteration 86/1000 | Loss: 0.00039818
Iteration 87/1000 | Loss: 0.00064846
Iteration 88/1000 | Loss: 0.00039438
Iteration 89/1000 | Loss: 0.00043845
Iteration 90/1000 | Loss: 0.00042145
Iteration 91/1000 | Loss: 0.00057577
Iteration 92/1000 | Loss: 0.00029146
Iteration 93/1000 | Loss: 0.00027290
Iteration 94/1000 | Loss: 0.00058588
Iteration 95/1000 | Loss: 0.00029134
Iteration 96/1000 | Loss: 0.00036426
Iteration 97/1000 | Loss: 0.00031332
Iteration 98/1000 | Loss: 0.00037217
Iteration 99/1000 | Loss: 0.00040745
Iteration 100/1000 | Loss: 0.00078796
Iteration 101/1000 | Loss: 0.00032844
Iteration 102/1000 | Loss: 0.00040215
Iteration 103/1000 | Loss: 0.00025632
Iteration 104/1000 | Loss: 0.00028386
Iteration 105/1000 | Loss: 0.00037030
Iteration 106/1000 | Loss: 0.00039441
Iteration 107/1000 | Loss: 0.00039851
Iteration 108/1000 | Loss: 0.00040300
Iteration 109/1000 | Loss: 0.00036964
Iteration 110/1000 | Loss: 0.00043693
Iteration 111/1000 | Loss: 0.00035492
Iteration 112/1000 | Loss: 0.00041991
Iteration 113/1000 | Loss: 0.00040493
Iteration 114/1000 | Loss: 0.00060780
Iteration 115/1000 | Loss: 0.00043548
Iteration 116/1000 | Loss: 0.00050216
Iteration 117/1000 | Loss: 0.00035656
Iteration 118/1000 | Loss: 0.00037537
Iteration 119/1000 | Loss: 0.00034409
Iteration 120/1000 | Loss: 0.00047849
Iteration 121/1000 | Loss: 0.00033712
Iteration 122/1000 | Loss: 0.00033157
Iteration 123/1000 | Loss: 0.00028603
Iteration 124/1000 | Loss: 0.00058612
Iteration 125/1000 | Loss: 0.00059641
Iteration 126/1000 | Loss: 0.00026504
Iteration 127/1000 | Loss: 0.00026781
Iteration 128/1000 | Loss: 0.00024194
Iteration 129/1000 | Loss: 0.00073172
Iteration 130/1000 | Loss: 0.00036950
Iteration 131/1000 | Loss: 0.00024659
Iteration 132/1000 | Loss: 0.00064570
Iteration 133/1000 | Loss: 0.00045237
Iteration 134/1000 | Loss: 0.00025021
Iteration 135/1000 | Loss: 0.00057851
Iteration 136/1000 | Loss: 0.00078771
Iteration 137/1000 | Loss: 0.00395261
Iteration 138/1000 | Loss: 0.00691948
Iteration 139/1000 | Loss: 0.00541811
Iteration 140/1000 | Loss: 0.00313621
Iteration 141/1000 | Loss: 0.00084386
Iteration 142/1000 | Loss: 0.00155666
Iteration 143/1000 | Loss: 0.00047129
Iteration 144/1000 | Loss: 0.00419909
Iteration 145/1000 | Loss: 0.01576518
Iteration 146/1000 | Loss: 0.00288840
Iteration 147/1000 | Loss: 0.00195693
Iteration 148/1000 | Loss: 0.00079574
Iteration 149/1000 | Loss: 0.00074091
Iteration 150/1000 | Loss: 0.00120193
Iteration 151/1000 | Loss: 0.01109302
Iteration 152/1000 | Loss: 0.00550993
Iteration 153/1000 | Loss: 0.00126497
Iteration 154/1000 | Loss: 0.00134181
Iteration 155/1000 | Loss: 0.00116899
Iteration 156/1000 | Loss: 0.00300285
Iteration 157/1000 | Loss: 0.00084918
Iteration 158/1000 | Loss: 0.00085857
Iteration 159/1000 | Loss: 0.00177557
Iteration 160/1000 | Loss: 0.00054553
Iteration 161/1000 | Loss: 0.00028490
Iteration 162/1000 | Loss: 0.00022749
Iteration 163/1000 | Loss: 0.00041169
Iteration 164/1000 | Loss: 0.00049745
Iteration 165/1000 | Loss: 0.00030143
Iteration 166/1000 | Loss: 0.00021930
Iteration 167/1000 | Loss: 0.00023793
Iteration 168/1000 | Loss: 0.00071052
Iteration 169/1000 | Loss: 0.00048571
Iteration 170/1000 | Loss: 0.00083948
Iteration 171/1000 | Loss: 0.00081920
Iteration 172/1000 | Loss: 0.00046939
Iteration 173/1000 | Loss: 0.00005341
Iteration 174/1000 | Loss: 0.00009688
Iteration 175/1000 | Loss: 0.00023406
Iteration 176/1000 | Loss: 0.00007233
Iteration 177/1000 | Loss: 0.00004461
Iteration 178/1000 | Loss: 0.00004333
Iteration 179/1000 | Loss: 0.00011563
Iteration 180/1000 | Loss: 0.00009973
Iteration 181/1000 | Loss: 0.00004163
Iteration 182/1000 | Loss: 0.00028391
Iteration 183/1000 | Loss: 0.00004111
Iteration 184/1000 | Loss: 0.00004053
Iteration 185/1000 | Loss: 0.00013483
Iteration 186/1000 | Loss: 0.00004012
Iteration 187/1000 | Loss: 0.00003974
Iteration 188/1000 | Loss: 0.00003944
Iteration 189/1000 | Loss: 0.00015492
Iteration 190/1000 | Loss: 0.00012213
Iteration 191/1000 | Loss: 0.00003947
Iteration 192/1000 | Loss: 0.00003908
Iteration 193/1000 | Loss: 0.00003902
Iteration 194/1000 | Loss: 0.00003899
Iteration 195/1000 | Loss: 0.00003898
Iteration 196/1000 | Loss: 0.00003898
Iteration 197/1000 | Loss: 0.00003898
Iteration 198/1000 | Loss: 0.00003898
Iteration 199/1000 | Loss: 0.00003898
Iteration 200/1000 | Loss: 0.00003898
Iteration 201/1000 | Loss: 0.00003898
Iteration 202/1000 | Loss: 0.00003898
Iteration 203/1000 | Loss: 0.00003897
Iteration 204/1000 | Loss: 0.00003897
Iteration 205/1000 | Loss: 0.00003897
Iteration 206/1000 | Loss: 0.00003897
Iteration 207/1000 | Loss: 0.00003893
Iteration 208/1000 | Loss: 0.00003890
Iteration 209/1000 | Loss: 0.00003889
Iteration 210/1000 | Loss: 0.00003889
Iteration 211/1000 | Loss: 0.00003889
Iteration 212/1000 | Loss: 0.00003889
Iteration 213/1000 | Loss: 0.00003889
Iteration 214/1000 | Loss: 0.00014555
Iteration 215/1000 | Loss: 0.00003910
Iteration 216/1000 | Loss: 0.00003886
Iteration 217/1000 | Loss: 0.00003886
Iteration 218/1000 | Loss: 0.00003885
Iteration 219/1000 | Loss: 0.00003884
Iteration 220/1000 | Loss: 0.00003884
Iteration 221/1000 | Loss: 0.00003883
Iteration 222/1000 | Loss: 0.00003883
Iteration 223/1000 | Loss: 0.00003883
Iteration 224/1000 | Loss: 0.00003883
Iteration 225/1000 | Loss: 0.00003883
Iteration 226/1000 | Loss: 0.00003883
Iteration 227/1000 | Loss: 0.00003883
Iteration 228/1000 | Loss: 0.00003882
Iteration 229/1000 | Loss: 0.00003882
Iteration 230/1000 | Loss: 0.00003882
Iteration 231/1000 | Loss: 0.00003882
Iteration 232/1000 | Loss: 0.00003882
Iteration 233/1000 | Loss: 0.00003882
Iteration 234/1000 | Loss: 0.00003882
Iteration 235/1000 | Loss: 0.00003882
Iteration 236/1000 | Loss: 0.00003882
Iteration 237/1000 | Loss: 0.00003881
Iteration 238/1000 | Loss: 0.00003881
Iteration 239/1000 | Loss: 0.00003881
Iteration 240/1000 | Loss: 0.00003881
Iteration 241/1000 | Loss: 0.00003881
Iteration 242/1000 | Loss: 0.00003881
Iteration 243/1000 | Loss: 0.00003881
Iteration 244/1000 | Loss: 0.00003881
Iteration 245/1000 | Loss: 0.00003881
Iteration 246/1000 | Loss: 0.00003881
Iteration 247/1000 | Loss: 0.00003880
Iteration 248/1000 | Loss: 0.00003879
Iteration 249/1000 | Loss: 0.00003879
Iteration 250/1000 | Loss: 0.00003878
Iteration 251/1000 | Loss: 0.00003878
Iteration 252/1000 | Loss: 0.00003877
Iteration 253/1000 | Loss: 0.00003877
Iteration 254/1000 | Loss: 0.00003877
Iteration 255/1000 | Loss: 0.00003877
Iteration 256/1000 | Loss: 0.00003876
Iteration 257/1000 | Loss: 0.00003876
Iteration 258/1000 | Loss: 0.00003876
Iteration 259/1000 | Loss: 0.00003876
Iteration 260/1000 | Loss: 0.00003875
Iteration 261/1000 | Loss: 0.00003875
Iteration 262/1000 | Loss: 0.00003875
Iteration 263/1000 | Loss: 0.00003875
Iteration 264/1000 | Loss: 0.00003874
Iteration 265/1000 | Loss: 0.00003874
Iteration 266/1000 | Loss: 0.00003874
Iteration 267/1000 | Loss: 0.00003874
Iteration 268/1000 | Loss: 0.00003874
Iteration 269/1000 | Loss: 0.00003874
Iteration 270/1000 | Loss: 0.00003874
Iteration 271/1000 | Loss: 0.00003874
Iteration 272/1000 | Loss: 0.00003874
Iteration 273/1000 | Loss: 0.00003874
Iteration 274/1000 | Loss: 0.00003874
Iteration 275/1000 | Loss: 0.00003874
Iteration 276/1000 | Loss: 0.00003873
Iteration 277/1000 | Loss: 0.00003873
Iteration 278/1000 | Loss: 0.00003873
Iteration 279/1000 | Loss: 0.00003873
Iteration 280/1000 | Loss: 0.00003873
Iteration 281/1000 | Loss: 0.00003873
Iteration 282/1000 | Loss: 0.00003873
Iteration 283/1000 | Loss: 0.00003873
Iteration 284/1000 | Loss: 0.00003873
Iteration 285/1000 | Loss: 0.00003872
Iteration 286/1000 | Loss: 0.00003872
Iteration 287/1000 | Loss: 0.00003872
Iteration 288/1000 | Loss: 0.00003872
Iteration 289/1000 | Loss: 0.00003872
Iteration 290/1000 | Loss: 0.00003872
Iteration 291/1000 | Loss: 0.00003871
Iteration 292/1000 | Loss: 0.00003871
Iteration 293/1000 | Loss: 0.00003871
Iteration 294/1000 | Loss: 0.00003871
Iteration 295/1000 | Loss: 0.00003871
Iteration 296/1000 | Loss: 0.00003870
Iteration 297/1000 | Loss: 0.00003870
Iteration 298/1000 | Loss: 0.00003870
Iteration 299/1000 | Loss: 0.00003869
Iteration 300/1000 | Loss: 0.00003869
Iteration 301/1000 | Loss: 0.00003869
Iteration 302/1000 | Loss: 0.00003869
Iteration 303/1000 | Loss: 0.00003869
Iteration 304/1000 | Loss: 0.00003869
Iteration 305/1000 | Loss: 0.00003869
Iteration 306/1000 | Loss: 0.00003868
Iteration 307/1000 | Loss: 0.00003868
Iteration 308/1000 | Loss: 0.00003868
Iteration 309/1000 | Loss: 0.00003868
Iteration 310/1000 | Loss: 0.00003868
Iteration 311/1000 | Loss: 0.00003868
Iteration 312/1000 | Loss: 0.00003868
Iteration 313/1000 | Loss: 0.00003868
Iteration 314/1000 | Loss: 0.00003868
Iteration 315/1000 | Loss: 0.00003868
Iteration 316/1000 | Loss: 0.00003868
Iteration 317/1000 | Loss: 0.00003868
Iteration 318/1000 | Loss: 0.00003867
Iteration 319/1000 | Loss: 0.00003867
Iteration 320/1000 | Loss: 0.00003867
Iteration 321/1000 | Loss: 0.00003867
Iteration 322/1000 | Loss: 0.00003867
Iteration 323/1000 | Loss: 0.00003867
Iteration 324/1000 | Loss: 0.00003867
Iteration 325/1000 | Loss: 0.00003867
Iteration 326/1000 | Loss: 0.00003867
Iteration 327/1000 | Loss: 0.00003867
Iteration 328/1000 | Loss: 0.00003867
Iteration 329/1000 | Loss: 0.00003866
Iteration 330/1000 | Loss: 0.00003866
Iteration 331/1000 | Loss: 0.00003866
Iteration 332/1000 | Loss: 0.00003866
Iteration 333/1000 | Loss: 0.00003866
Iteration 334/1000 | Loss: 0.00003866
Iteration 335/1000 | Loss: 0.00003866
Iteration 336/1000 | Loss: 0.00003866
Iteration 337/1000 | Loss: 0.00003866
Iteration 338/1000 | Loss: 0.00003865
Iteration 339/1000 | Loss: 0.00003865
Iteration 340/1000 | Loss: 0.00003865
Iteration 341/1000 | Loss: 0.00003865
Iteration 342/1000 | Loss: 0.00003865
Iteration 343/1000 | Loss: 0.00003865
Iteration 344/1000 | Loss: 0.00003865
Iteration 345/1000 | Loss: 0.00003865
Iteration 346/1000 | Loss: 0.00003865
Iteration 347/1000 | Loss: 0.00003864
Iteration 348/1000 | Loss: 0.00003864
Iteration 349/1000 | Loss: 0.00003864
Iteration 350/1000 | Loss: 0.00003864
Iteration 351/1000 | Loss: 0.00003864
Iteration 352/1000 | Loss: 0.00003864
Iteration 353/1000 | Loss: 0.00003864
Iteration 354/1000 | Loss: 0.00003864
Iteration 355/1000 | Loss: 0.00003864
Iteration 356/1000 | Loss: 0.00003864
Iteration 357/1000 | Loss: 0.00003864
Iteration 358/1000 | Loss: 0.00003863
Iteration 359/1000 | Loss: 0.00003863
Iteration 360/1000 | Loss: 0.00003863
Iteration 361/1000 | Loss: 0.00003863
Iteration 362/1000 | Loss: 0.00003863
Iteration 363/1000 | Loss: 0.00003863
Iteration 364/1000 | Loss: 0.00003863
Iteration 365/1000 | Loss: 0.00003863
Iteration 366/1000 | Loss: 0.00003862
Iteration 367/1000 | Loss: 0.00003862
Iteration 368/1000 | Loss: 0.00003862
Iteration 369/1000 | Loss: 0.00003862
Iteration 370/1000 | Loss: 0.00003862
Iteration 371/1000 | Loss: 0.00003862
Iteration 372/1000 | Loss: 0.00003862
Iteration 373/1000 | Loss: 0.00003862
Iteration 374/1000 | Loss: 0.00003862
Iteration 375/1000 | Loss: 0.00003862
Iteration 376/1000 | Loss: 0.00003862
Iteration 377/1000 | Loss: 0.00003862
Iteration 378/1000 | Loss: 0.00003862
Iteration 379/1000 | Loss: 0.00003862
Iteration 380/1000 | Loss: 0.00003862
Iteration 381/1000 | Loss: 0.00003862
Iteration 382/1000 | Loss: 0.00003862
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 382. Stopping optimization.
Last 5 losses: [3.86187712138053e-05, 3.86187712138053e-05, 3.86187712138053e-05, 3.86187712138053e-05, 3.86187712138053e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.86187712138053e-05

Optimization complete. Final v2v error: 4.3231120109558105 mm

Highest mean error: 11.149523735046387 mm for frame 43

Lowest mean error: 3.6232569217681885 mm for frame 17

Saving results

Total time: 371.4297273159027
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_006/1090/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_006/1090.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_006/1090
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00772747
Iteration 2/25 | Loss: 0.00098782
Iteration 3/25 | Loss: 0.00082423
Iteration 4/25 | Loss: 0.00077151
Iteration 5/25 | Loss: 0.00076280
Iteration 6/25 | Loss: 0.00076141
Iteration 7/25 | Loss: 0.00076141
Iteration 8/25 | Loss: 0.00076141
Iteration 9/25 | Loss: 0.00076141
Iteration 10/25 | Loss: 0.00076141
Iteration 11/25 | Loss: 0.00076141
Iteration 12/25 | Loss: 0.00076141
Iteration 13/25 | Loss: 0.00076141
Iteration 14/25 | Loss: 0.00076141
Iteration 15/25 | Loss: 0.00076141
Iteration 16/25 | Loss: 0.00076141
Iteration 17/25 | Loss: 0.00076141
Iteration 18/25 | Loss: 0.00076141
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0007614114438183606, 0.0007614114438183606, 0.0007614114438183606, 0.0007614114438183606, 0.0007614114438183606]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007614114438183606

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45667183
Iteration 2/25 | Loss: 0.00045918
Iteration 3/25 | Loss: 0.00045916
Iteration 4/25 | Loss: 0.00045916
Iteration 5/25 | Loss: 0.00045915
Iteration 6/25 | Loss: 0.00045915
Iteration 7/25 | Loss: 0.00045915
Iteration 8/25 | Loss: 0.00045915
Iteration 9/25 | Loss: 0.00045915
Iteration 10/25 | Loss: 0.00045915
Iteration 11/25 | Loss: 0.00045915
Iteration 12/25 | Loss: 0.00045915
Iteration 13/25 | Loss: 0.00045915
Iteration 14/25 | Loss: 0.00045915
Iteration 15/25 | Loss: 0.00045915
Iteration 16/25 | Loss: 0.00045915
Iteration 17/25 | Loss: 0.00045915
Iteration 18/25 | Loss: 0.00045915
Iteration 19/25 | Loss: 0.00045915
Iteration 20/25 | Loss: 0.00045915
Iteration 21/25 | Loss: 0.00045915
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0004591533506754786, 0.0004591533506754786, 0.0004591533506754786, 0.0004591533506754786, 0.0004591533506754786]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0004591533506754786

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00045915
Iteration 2/1000 | Loss: 0.00002825
Iteration 3/1000 | Loss: 0.00002281
Iteration 4/1000 | Loss: 0.00002114
Iteration 5/1000 | Loss: 0.00001989
Iteration 6/1000 | Loss: 0.00001915
Iteration 7/1000 | Loss: 0.00001863
Iteration 8/1000 | Loss: 0.00001833
Iteration 9/1000 | Loss: 0.00001814
Iteration 10/1000 | Loss: 0.00001811
Iteration 11/1000 | Loss: 0.00001797
Iteration 12/1000 | Loss: 0.00001793
Iteration 13/1000 | Loss: 0.00001793
Iteration 14/1000 | Loss: 0.00001791
Iteration 15/1000 | Loss: 0.00001789
Iteration 16/1000 | Loss: 0.00001787
Iteration 17/1000 | Loss: 0.00001785
Iteration 18/1000 | Loss: 0.00001784
Iteration 19/1000 | Loss: 0.00001783
Iteration 20/1000 | Loss: 0.00001780
Iteration 21/1000 | Loss: 0.00001779
Iteration 22/1000 | Loss: 0.00001776
Iteration 23/1000 | Loss: 0.00001770
Iteration 24/1000 | Loss: 0.00001770
Iteration 25/1000 | Loss: 0.00001769
Iteration 26/1000 | Loss: 0.00001767
Iteration 27/1000 | Loss: 0.00001766
Iteration 28/1000 | Loss: 0.00001761
Iteration 29/1000 | Loss: 0.00001760
Iteration 30/1000 | Loss: 0.00001758
Iteration 31/1000 | Loss: 0.00001758
Iteration 32/1000 | Loss: 0.00001758
Iteration 33/1000 | Loss: 0.00001758
Iteration 34/1000 | Loss: 0.00001758
Iteration 35/1000 | Loss: 0.00001758
Iteration 36/1000 | Loss: 0.00001758
Iteration 37/1000 | Loss: 0.00001758
Iteration 38/1000 | Loss: 0.00001758
Iteration 39/1000 | Loss: 0.00001757
Iteration 40/1000 | Loss: 0.00001757
Iteration 41/1000 | Loss: 0.00001757
Iteration 42/1000 | Loss: 0.00001757
Iteration 43/1000 | Loss: 0.00001756
Iteration 44/1000 | Loss: 0.00001756
Iteration 45/1000 | Loss: 0.00001756
Iteration 46/1000 | Loss: 0.00001755
Iteration 47/1000 | Loss: 0.00001755
Iteration 48/1000 | Loss: 0.00001755
Iteration 49/1000 | Loss: 0.00001755
Iteration 50/1000 | Loss: 0.00001755
Iteration 51/1000 | Loss: 0.00001755
Iteration 52/1000 | Loss: 0.00001755
Iteration 53/1000 | Loss: 0.00001755
Iteration 54/1000 | Loss: 0.00001754
Iteration 55/1000 | Loss: 0.00001754
Iteration 56/1000 | Loss: 0.00001754
Iteration 57/1000 | Loss: 0.00001754
Iteration 58/1000 | Loss: 0.00001754
Iteration 59/1000 | Loss: 0.00001754
Iteration 60/1000 | Loss: 0.00001754
Iteration 61/1000 | Loss: 0.00001754
Iteration 62/1000 | Loss: 0.00001754
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 62. Stopping optimization.
Last 5 losses: [1.754456570779439e-05, 1.754456570779439e-05, 1.754456570779439e-05, 1.754456570779439e-05, 1.754456570779439e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.754456570779439e-05

Optimization complete. Final v2v error: 3.5552000999450684 mm

Highest mean error: 4.021835803985596 mm for frame 211

Lowest mean error: 3.3167645931243896 mm for frame 63

Saving results

Total time: 34.285218954086304
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_006/1018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_006/1018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_006/1018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00813780
Iteration 2/25 | Loss: 0.00127340
Iteration 3/25 | Loss: 0.00091394
Iteration 4/25 | Loss: 0.00085974
Iteration 5/25 | Loss: 0.00084600
Iteration 6/25 | Loss: 0.00084208
Iteration 7/25 | Loss: 0.00084133
Iteration 8/25 | Loss: 0.00084133
Iteration 9/25 | Loss: 0.00084133
Iteration 10/25 | Loss: 0.00084133
Iteration 11/25 | Loss: 0.00084133
Iteration 12/25 | Loss: 0.00084133
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0008413334144279361, 0.0008413334144279361, 0.0008413334144279361, 0.0008413334144279361, 0.0008413334144279361]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008413334144279361

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.53677380
Iteration 2/25 | Loss: 0.00058234
Iteration 3/25 | Loss: 0.00058234
Iteration 4/25 | Loss: 0.00058233
Iteration 5/25 | Loss: 0.00058233
Iteration 6/25 | Loss: 0.00058233
Iteration 7/25 | Loss: 0.00058233
Iteration 8/25 | Loss: 0.00058233
Iteration 9/25 | Loss: 0.00058233
Iteration 10/25 | Loss: 0.00058233
Iteration 11/25 | Loss: 0.00058233
Iteration 12/25 | Loss: 0.00058233
Iteration 13/25 | Loss: 0.00058233
Iteration 14/25 | Loss: 0.00058233
Iteration 15/25 | Loss: 0.00058233
Iteration 16/25 | Loss: 0.00058233
Iteration 17/25 | Loss: 0.00058233
Iteration 18/25 | Loss: 0.00058233
Iteration 19/25 | Loss: 0.00058233
Iteration 20/25 | Loss: 0.00058233
Iteration 21/25 | Loss: 0.00058233
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.000582331616897136, 0.000582331616897136, 0.000582331616897136, 0.000582331616897136, 0.000582331616897136]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000582331616897136

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00058233
Iteration 2/1000 | Loss: 0.00004104
Iteration 3/1000 | Loss: 0.00003019
Iteration 4/1000 | Loss: 0.00002673
Iteration 5/1000 | Loss: 0.00002508
Iteration 6/1000 | Loss: 0.00002402
Iteration 7/1000 | Loss: 0.00002334
Iteration 8/1000 | Loss: 0.00002280
Iteration 9/1000 | Loss: 0.00002244
Iteration 10/1000 | Loss: 0.00002212
Iteration 11/1000 | Loss: 0.00002187
Iteration 12/1000 | Loss: 0.00002166
Iteration 13/1000 | Loss: 0.00002149
Iteration 14/1000 | Loss: 0.00002138
Iteration 15/1000 | Loss: 0.00002129
Iteration 16/1000 | Loss: 0.00002125
Iteration 17/1000 | Loss: 0.00002125
Iteration 18/1000 | Loss: 0.00002123
Iteration 19/1000 | Loss: 0.00002123
Iteration 20/1000 | Loss: 0.00002122
Iteration 21/1000 | Loss: 0.00002122
Iteration 22/1000 | Loss: 0.00002121
Iteration 23/1000 | Loss: 0.00002121
Iteration 24/1000 | Loss: 0.00002120
Iteration 25/1000 | Loss: 0.00002120
Iteration 26/1000 | Loss: 0.00002120
Iteration 27/1000 | Loss: 0.00002120
Iteration 28/1000 | Loss: 0.00002120
Iteration 29/1000 | Loss: 0.00002119
Iteration 30/1000 | Loss: 0.00002119
Iteration 31/1000 | Loss: 0.00002119
Iteration 32/1000 | Loss: 0.00002119
Iteration 33/1000 | Loss: 0.00002118
Iteration 34/1000 | Loss: 0.00002117
Iteration 35/1000 | Loss: 0.00002117
Iteration 36/1000 | Loss: 0.00002117
Iteration 37/1000 | Loss: 0.00002117
Iteration 38/1000 | Loss: 0.00002116
Iteration 39/1000 | Loss: 0.00002115
Iteration 40/1000 | Loss: 0.00002115
Iteration 41/1000 | Loss: 0.00002115
Iteration 42/1000 | Loss: 0.00002115
Iteration 43/1000 | Loss: 0.00002114
Iteration 44/1000 | Loss: 0.00002114
Iteration 45/1000 | Loss: 0.00002114
Iteration 46/1000 | Loss: 0.00002113
Iteration 47/1000 | Loss: 0.00002113
Iteration 48/1000 | Loss: 0.00002113
Iteration 49/1000 | Loss: 0.00002113
Iteration 50/1000 | Loss: 0.00002112
Iteration 51/1000 | Loss: 0.00002112
Iteration 52/1000 | Loss: 0.00002112
Iteration 53/1000 | Loss: 0.00002112
Iteration 54/1000 | Loss: 0.00002112
Iteration 55/1000 | Loss: 0.00002112
Iteration 56/1000 | Loss: 0.00002112
Iteration 57/1000 | Loss: 0.00002112
Iteration 58/1000 | Loss: 0.00002112
Iteration 59/1000 | Loss: 0.00002112
Iteration 60/1000 | Loss: 0.00002112
Iteration 61/1000 | Loss: 0.00002111
Iteration 62/1000 | Loss: 0.00002111
Iteration 63/1000 | Loss: 0.00002111
Iteration 64/1000 | Loss: 0.00002111
Iteration 65/1000 | Loss: 0.00002111
Iteration 66/1000 | Loss: 0.00002111
Iteration 67/1000 | Loss: 0.00002110
Iteration 68/1000 | Loss: 0.00002110
Iteration 69/1000 | Loss: 0.00002110
Iteration 70/1000 | Loss: 0.00002110
Iteration 71/1000 | Loss: 0.00002110
Iteration 72/1000 | Loss: 0.00002110
Iteration 73/1000 | Loss: 0.00002110
Iteration 74/1000 | Loss: 0.00002110
Iteration 75/1000 | Loss: 0.00002109
Iteration 76/1000 | Loss: 0.00002109
Iteration 77/1000 | Loss: 0.00002109
Iteration 78/1000 | Loss: 0.00002108
Iteration 79/1000 | Loss: 0.00002108
Iteration 80/1000 | Loss: 0.00002108
Iteration 81/1000 | Loss: 0.00002108
Iteration 82/1000 | Loss: 0.00002108
Iteration 83/1000 | Loss: 0.00002107
Iteration 84/1000 | Loss: 0.00002107
Iteration 85/1000 | Loss: 0.00002107
Iteration 86/1000 | Loss: 0.00002106
Iteration 87/1000 | Loss: 0.00002106
Iteration 88/1000 | Loss: 0.00002106
Iteration 89/1000 | Loss: 0.00002106
Iteration 90/1000 | Loss: 0.00002105
Iteration 91/1000 | Loss: 0.00002105
Iteration 92/1000 | Loss: 0.00002105
Iteration 93/1000 | Loss: 0.00002105
Iteration 94/1000 | Loss: 0.00002105
Iteration 95/1000 | Loss: 0.00002105
Iteration 96/1000 | Loss: 0.00002105
Iteration 97/1000 | Loss: 0.00002105
Iteration 98/1000 | Loss: 0.00002104
Iteration 99/1000 | Loss: 0.00002104
Iteration 100/1000 | Loss: 0.00002104
Iteration 101/1000 | Loss: 0.00002104
Iteration 102/1000 | Loss: 0.00002104
Iteration 103/1000 | Loss: 0.00002104
Iteration 104/1000 | Loss: 0.00002104
Iteration 105/1000 | Loss: 0.00002104
Iteration 106/1000 | Loss: 0.00002104
Iteration 107/1000 | Loss: 0.00002103
Iteration 108/1000 | Loss: 0.00002103
Iteration 109/1000 | Loss: 0.00002103
Iteration 110/1000 | Loss: 0.00002103
Iteration 111/1000 | Loss: 0.00002103
Iteration 112/1000 | Loss: 0.00002103
Iteration 113/1000 | Loss: 0.00002102
Iteration 114/1000 | Loss: 0.00002102
Iteration 115/1000 | Loss: 0.00002102
Iteration 116/1000 | Loss: 0.00002102
Iteration 117/1000 | Loss: 0.00002101
Iteration 118/1000 | Loss: 0.00002101
Iteration 119/1000 | Loss: 0.00002101
Iteration 120/1000 | Loss: 0.00002101
Iteration 121/1000 | Loss: 0.00002101
Iteration 122/1000 | Loss: 0.00002100
Iteration 123/1000 | Loss: 0.00002100
Iteration 124/1000 | Loss: 0.00002100
Iteration 125/1000 | Loss: 0.00002100
Iteration 126/1000 | Loss: 0.00002100
Iteration 127/1000 | Loss: 0.00002100
Iteration 128/1000 | Loss: 0.00002100
Iteration 129/1000 | Loss: 0.00002100
Iteration 130/1000 | Loss: 0.00002100
Iteration 131/1000 | Loss: 0.00002100
Iteration 132/1000 | Loss: 0.00002100
Iteration 133/1000 | Loss: 0.00002100
Iteration 134/1000 | Loss: 0.00002100
Iteration 135/1000 | Loss: 0.00002100
Iteration 136/1000 | Loss: 0.00002100
Iteration 137/1000 | Loss: 0.00002100
Iteration 138/1000 | Loss: 0.00002100
Iteration 139/1000 | Loss: 0.00002100
Iteration 140/1000 | Loss: 0.00002100
Iteration 141/1000 | Loss: 0.00002100
Iteration 142/1000 | Loss: 0.00002100
Iteration 143/1000 | Loss: 0.00002100
Iteration 144/1000 | Loss: 0.00002100
Iteration 145/1000 | Loss: 0.00002100
Iteration 146/1000 | Loss: 0.00002100
Iteration 147/1000 | Loss: 0.00002100
Iteration 148/1000 | Loss: 0.00002100
Iteration 149/1000 | Loss: 0.00002100
Iteration 150/1000 | Loss: 0.00002100
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 150. Stopping optimization.
Last 5 losses: [2.0998852050979622e-05, 2.0998852050979622e-05, 2.0998852050979622e-05, 2.0998852050979622e-05, 2.0998852050979622e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0998852050979622e-05

Optimization complete. Final v2v error: 3.8214271068573 mm

Highest mean error: 4.395748138427734 mm for frame 169

Lowest mean error: 3.1414504051208496 mm for frame 230

Saving results

Total time: 45.99443507194519
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_006/1034/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_006/1034.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_006/1034
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00911622
Iteration 2/25 | Loss: 0.00153122
Iteration 3/25 | Loss: 0.00103839
Iteration 4/25 | Loss: 0.00091420
Iteration 5/25 | Loss: 0.00088269
Iteration 6/25 | Loss: 0.00087322
Iteration 7/25 | Loss: 0.00086116
Iteration 8/25 | Loss: 0.00084207
Iteration 9/25 | Loss: 0.00083877
Iteration 10/25 | Loss: 0.00084139
Iteration 11/25 | Loss: 0.00083661
Iteration 12/25 | Loss: 0.00083100
Iteration 13/25 | Loss: 0.00083438
Iteration 14/25 | Loss: 0.00083571
Iteration 15/25 | Loss: 0.00083211
Iteration 16/25 | Loss: 0.00083073
Iteration 17/25 | Loss: 0.00082845
Iteration 18/25 | Loss: 0.00082422
Iteration 19/25 | Loss: 0.00082344
Iteration 20/25 | Loss: 0.00082320
Iteration 21/25 | Loss: 0.00082318
Iteration 22/25 | Loss: 0.00082317
Iteration 23/25 | Loss: 0.00082317
Iteration 24/25 | Loss: 0.00082317
Iteration 25/25 | Loss: 0.00082317

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 9.83169937
Iteration 2/25 | Loss: 0.00059951
Iteration 3/25 | Loss: 0.00059929
Iteration 4/25 | Loss: 0.00059929
Iteration 5/25 | Loss: 0.00059929
Iteration 6/25 | Loss: 0.00059929
Iteration 7/25 | Loss: 0.00059929
Iteration 8/25 | Loss: 0.00059929
Iteration 9/25 | Loss: 0.00059929
Iteration 10/25 | Loss: 0.00059929
Iteration 11/25 | Loss: 0.00059929
Iteration 12/25 | Loss: 0.00059929
Iteration 13/25 | Loss: 0.00059929
Iteration 14/25 | Loss: 0.00059929
Iteration 15/25 | Loss: 0.00059929
Iteration 16/25 | Loss: 0.00059929
Iteration 17/25 | Loss: 0.00059929
Iteration 18/25 | Loss: 0.00059929
Iteration 19/25 | Loss: 0.00059929
Iteration 20/25 | Loss: 0.00059929
Iteration 21/25 | Loss: 0.00059929
Iteration 22/25 | Loss: 0.00059929
Iteration 23/25 | Loss: 0.00059929
Iteration 24/25 | Loss: 0.00059929
Iteration 25/25 | Loss: 0.00059929

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00059929
Iteration 2/1000 | Loss: 0.00037133
Iteration 3/1000 | Loss: 0.00022623
Iteration 4/1000 | Loss: 0.00019791
Iteration 5/1000 | Loss: 0.00004480
Iteration 6/1000 | Loss: 0.00003295
Iteration 7/1000 | Loss: 0.00002901
Iteration 8/1000 | Loss: 0.00009853
Iteration 9/1000 | Loss: 0.00007031
Iteration 10/1000 | Loss: 0.00038301
Iteration 11/1000 | Loss: 0.00015122
Iteration 12/1000 | Loss: 0.00003671
Iteration 13/1000 | Loss: 0.00035091
Iteration 14/1000 | Loss: 0.00111941
Iteration 15/1000 | Loss: 0.00077934
Iteration 16/1000 | Loss: 0.00020827
Iteration 17/1000 | Loss: 0.00004663
Iteration 18/1000 | Loss: 0.00005261
Iteration 19/1000 | Loss: 0.00002747
Iteration 20/1000 | Loss: 0.00002584
Iteration 21/1000 | Loss: 0.00002501
Iteration 22/1000 | Loss: 0.00006369
Iteration 23/1000 | Loss: 0.00002457
Iteration 24/1000 | Loss: 0.00002426
Iteration 25/1000 | Loss: 0.00002394
Iteration 26/1000 | Loss: 0.00002360
Iteration 27/1000 | Loss: 0.00020492
Iteration 28/1000 | Loss: 0.00028561
Iteration 29/1000 | Loss: 0.00012458
Iteration 30/1000 | Loss: 0.00022372
Iteration 31/1000 | Loss: 0.00021801
Iteration 32/1000 | Loss: 0.00030290
Iteration 33/1000 | Loss: 0.00040625
Iteration 34/1000 | Loss: 0.00002887
Iteration 35/1000 | Loss: 0.00007786
Iteration 36/1000 | Loss: 0.00002386
Iteration 37/1000 | Loss: 0.00002305
Iteration 38/1000 | Loss: 0.00002265
Iteration 39/1000 | Loss: 0.00002212
Iteration 40/1000 | Loss: 0.00002167
Iteration 41/1000 | Loss: 0.00010832
Iteration 42/1000 | Loss: 0.00002120
Iteration 43/1000 | Loss: 0.00002107
Iteration 44/1000 | Loss: 0.00002106
Iteration 45/1000 | Loss: 0.00002102
Iteration 46/1000 | Loss: 0.00002101
Iteration 47/1000 | Loss: 0.00002100
Iteration 48/1000 | Loss: 0.00002100
Iteration 49/1000 | Loss: 0.00002097
Iteration 50/1000 | Loss: 0.00002096
Iteration 51/1000 | Loss: 0.00002095
Iteration 52/1000 | Loss: 0.00002094
Iteration 53/1000 | Loss: 0.00002093
Iteration 54/1000 | Loss: 0.00002093
Iteration 55/1000 | Loss: 0.00002091
Iteration 56/1000 | Loss: 0.00002091
Iteration 57/1000 | Loss: 0.00002090
Iteration 58/1000 | Loss: 0.00002090
Iteration 59/1000 | Loss: 0.00002087
Iteration 60/1000 | Loss: 0.00002086
Iteration 61/1000 | Loss: 0.00002085
Iteration 62/1000 | Loss: 0.00002085
Iteration 63/1000 | Loss: 0.00002085
Iteration 64/1000 | Loss: 0.00002085
Iteration 65/1000 | Loss: 0.00002084
Iteration 66/1000 | Loss: 0.00002084
Iteration 67/1000 | Loss: 0.00002084
Iteration 68/1000 | Loss: 0.00002080
Iteration 69/1000 | Loss: 0.00002079
Iteration 70/1000 | Loss: 0.00002079
Iteration 71/1000 | Loss: 0.00002078
Iteration 72/1000 | Loss: 0.00010738
Iteration 73/1000 | Loss: 0.00002100
Iteration 74/1000 | Loss: 0.00002075
Iteration 75/1000 | Loss: 0.00002075
Iteration 76/1000 | Loss: 0.00002075
Iteration 77/1000 | Loss: 0.00002074
Iteration 78/1000 | Loss: 0.00002074
Iteration 79/1000 | Loss: 0.00002073
Iteration 80/1000 | Loss: 0.00002073
Iteration 81/1000 | Loss: 0.00002073
Iteration 82/1000 | Loss: 0.00002073
Iteration 83/1000 | Loss: 0.00002073
Iteration 84/1000 | Loss: 0.00002073
Iteration 85/1000 | Loss: 0.00002073
Iteration 86/1000 | Loss: 0.00002073
Iteration 87/1000 | Loss: 0.00002073
Iteration 88/1000 | Loss: 0.00002073
Iteration 89/1000 | Loss: 0.00002073
Iteration 90/1000 | Loss: 0.00002073
Iteration 91/1000 | Loss: 0.00002073
Iteration 92/1000 | Loss: 0.00002073
Iteration 93/1000 | Loss: 0.00002073
Iteration 94/1000 | Loss: 0.00002073
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 94. Stopping optimization.
Last 5 losses: [2.0725512513308786e-05, 2.0725512513308786e-05, 2.0725512513308786e-05, 2.0725512513308786e-05, 2.0725512513308786e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0725512513308786e-05

Optimization complete. Final v2v error: 3.8388917446136475 mm

Highest mean error: 6.034427642822266 mm for frame 93

Lowest mean error: 3.093444347381592 mm for frame 1

Saving results

Total time: 104.04530692100525
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_006/1008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_006/1008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_006/1008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01018638
Iteration 2/25 | Loss: 0.00217672
Iteration 3/25 | Loss: 0.00124653
Iteration 4/25 | Loss: 0.00098097
Iteration 5/25 | Loss: 0.00089657
Iteration 6/25 | Loss: 0.00089412
Iteration 7/25 | Loss: 0.00089345
Iteration 8/25 | Loss: 0.00089540
Iteration 9/25 | Loss: 0.00093224
Iteration 10/25 | Loss: 0.00088505
Iteration 11/25 | Loss: 0.00083986
Iteration 12/25 | Loss: 0.00084945
Iteration 13/25 | Loss: 0.00086691
Iteration 14/25 | Loss: 0.00085296
Iteration 15/25 | Loss: 0.00083095
Iteration 16/25 | Loss: 0.00081306
Iteration 17/25 | Loss: 0.00080714
Iteration 18/25 | Loss: 0.00080557
Iteration 19/25 | Loss: 0.00080447
Iteration 20/25 | Loss: 0.00080411
Iteration 21/25 | Loss: 0.00080395
Iteration 22/25 | Loss: 0.00080375
Iteration 23/25 | Loss: 0.00080345
Iteration 24/25 | Loss: 0.00080750
Iteration 25/25 | Loss: 0.00079919

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.79237592
Iteration 2/25 | Loss: 0.00053226
Iteration 3/25 | Loss: 0.00050832
Iteration 4/25 | Loss: 0.00050831
Iteration 5/25 | Loss: 0.00050831
Iteration 6/25 | Loss: 0.00050831
Iteration 7/25 | Loss: 0.00050831
Iteration 8/25 | Loss: 0.00050831
Iteration 9/25 | Loss: 0.00050831
Iteration 10/25 | Loss: 0.00050831
Iteration 11/25 | Loss: 0.00050831
Iteration 12/25 | Loss: 0.00050831
Iteration 13/25 | Loss: 0.00050831
Iteration 14/25 | Loss: 0.00050831
Iteration 15/25 | Loss: 0.00050831
Iteration 16/25 | Loss: 0.00050831
Iteration 17/25 | Loss: 0.00050831
Iteration 18/25 | Loss: 0.00050831
Iteration 19/25 | Loss: 0.00050831
Iteration 20/25 | Loss: 0.00050831
Iteration 21/25 | Loss: 0.00050831
Iteration 22/25 | Loss: 0.00050831
Iteration 23/25 | Loss: 0.00050831
Iteration 24/25 | Loss: 0.00050831
Iteration 25/25 | Loss: 0.00050831

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00050831
Iteration 2/1000 | Loss: 0.00007534
Iteration 3/1000 | Loss: 0.00010685
Iteration 4/1000 | Loss: 0.00004015
Iteration 5/1000 | Loss: 0.00002296
Iteration 6/1000 | Loss: 0.00018853
Iteration 7/1000 | Loss: 0.00002168
Iteration 8/1000 | Loss: 0.00002107
Iteration 9/1000 | Loss: 0.00002050
Iteration 10/1000 | Loss: 0.00002357
Iteration 11/1000 | Loss: 0.00001996
Iteration 12/1000 | Loss: 0.00010822
Iteration 13/1000 | Loss: 0.00006928
Iteration 14/1000 | Loss: 0.00003328
Iteration 15/1000 | Loss: 0.00004311
Iteration 16/1000 | Loss: 0.00001850
Iteration 17/1000 | Loss: 0.00001804
Iteration 18/1000 | Loss: 0.00001766
Iteration 19/1000 | Loss: 0.00001743
Iteration 20/1000 | Loss: 0.00001721
Iteration 21/1000 | Loss: 0.00001697
Iteration 22/1000 | Loss: 0.00001682
Iteration 23/1000 | Loss: 0.00033459
Iteration 24/1000 | Loss: 0.00128770
Iteration 25/1000 | Loss: 0.00046622
Iteration 26/1000 | Loss: 0.00018941
Iteration 27/1000 | Loss: 0.00008092
Iteration 28/1000 | Loss: 0.00011979
Iteration 29/1000 | Loss: 0.00013353
Iteration 30/1000 | Loss: 0.00038582
Iteration 31/1000 | Loss: 0.00019942
Iteration 32/1000 | Loss: 0.00047792
Iteration 33/1000 | Loss: 0.00019623
Iteration 34/1000 | Loss: 0.00016319
Iteration 35/1000 | Loss: 0.00003808
Iteration 36/1000 | Loss: 0.00012386
Iteration 37/1000 | Loss: 0.00041917
Iteration 38/1000 | Loss: 0.00011080
Iteration 39/1000 | Loss: 0.00018248
Iteration 40/1000 | Loss: 0.00026636
Iteration 41/1000 | Loss: 0.00003491
Iteration 42/1000 | Loss: 0.00006350
Iteration 43/1000 | Loss: 0.00002993
Iteration 44/1000 | Loss: 0.00011955
Iteration 45/1000 | Loss: 0.00009545
Iteration 46/1000 | Loss: 0.00003842
Iteration 47/1000 | Loss: 0.00010009
Iteration 48/1000 | Loss: 0.00002581
Iteration 49/1000 | Loss: 0.00002445
Iteration 50/1000 | Loss: 0.00016230
Iteration 51/1000 | Loss: 0.00006421
Iteration 52/1000 | Loss: 0.00002360
Iteration 53/1000 | Loss: 0.00004323
Iteration 54/1000 | Loss: 0.00008091
Iteration 55/1000 | Loss: 0.00002122
Iteration 56/1000 | Loss: 0.00004261
Iteration 57/1000 | Loss: 0.00002016
Iteration 58/1000 | Loss: 0.00005903
Iteration 59/1000 | Loss: 0.00002016
Iteration 60/1000 | Loss: 0.00001969
Iteration 61/1000 | Loss: 0.00001937
Iteration 62/1000 | Loss: 0.00001908
Iteration 63/1000 | Loss: 0.00001880
Iteration 64/1000 | Loss: 0.00001849
Iteration 65/1000 | Loss: 0.00005593
Iteration 66/1000 | Loss: 0.00002972
Iteration 67/1000 | Loss: 0.00001787
Iteration 68/1000 | Loss: 0.00032232
Iteration 69/1000 | Loss: 0.00038520
Iteration 70/1000 | Loss: 0.00051597
Iteration 71/1000 | Loss: 0.00039298
Iteration 72/1000 | Loss: 0.00008315
Iteration 73/1000 | Loss: 0.00003394
Iteration 74/1000 | Loss: 0.00005973
Iteration 75/1000 | Loss: 0.00003845
Iteration 76/1000 | Loss: 0.00003361
Iteration 77/1000 | Loss: 0.00002354
Iteration 78/1000 | Loss: 0.00053141
Iteration 79/1000 | Loss: 0.00020645
Iteration 80/1000 | Loss: 0.00016560
Iteration 81/1000 | Loss: 0.00062973
Iteration 82/1000 | Loss: 0.00106618
Iteration 83/1000 | Loss: 0.00002519
Iteration 84/1000 | Loss: 0.00002049
Iteration 85/1000 | Loss: 0.00001818
Iteration 86/1000 | Loss: 0.00001715
Iteration 87/1000 | Loss: 0.00001653
Iteration 88/1000 | Loss: 0.00001613
Iteration 89/1000 | Loss: 0.00001607
Iteration 90/1000 | Loss: 0.00001584
Iteration 91/1000 | Loss: 0.00010442
Iteration 92/1000 | Loss: 0.00001566
Iteration 93/1000 | Loss: 0.00001549
Iteration 94/1000 | Loss: 0.00001534
Iteration 95/1000 | Loss: 0.00001529
Iteration 96/1000 | Loss: 0.00007526
Iteration 97/1000 | Loss: 0.00002635
Iteration 98/1000 | Loss: 0.00001518
Iteration 99/1000 | Loss: 0.00003201
Iteration 100/1000 | Loss: 0.00001505
Iteration 101/1000 | Loss: 0.00001499
Iteration 102/1000 | Loss: 0.00001495
Iteration 103/1000 | Loss: 0.00001493
Iteration 104/1000 | Loss: 0.00001492
Iteration 105/1000 | Loss: 0.00001486
Iteration 106/1000 | Loss: 0.00001485
Iteration 107/1000 | Loss: 0.00001485
Iteration 108/1000 | Loss: 0.00001482
Iteration 109/1000 | Loss: 0.00001481
Iteration 110/1000 | Loss: 0.00001481
Iteration 111/1000 | Loss: 0.00001480
Iteration 112/1000 | Loss: 0.00001480
Iteration 113/1000 | Loss: 0.00001479
Iteration 114/1000 | Loss: 0.00001478
Iteration 115/1000 | Loss: 0.00001475
Iteration 116/1000 | Loss: 0.00001475
Iteration 117/1000 | Loss: 0.00001475
Iteration 118/1000 | Loss: 0.00001474
Iteration 119/1000 | Loss: 0.00001473
Iteration 120/1000 | Loss: 0.00001473
Iteration 121/1000 | Loss: 0.00001472
Iteration 122/1000 | Loss: 0.00001472
Iteration 123/1000 | Loss: 0.00001472
Iteration 124/1000 | Loss: 0.00001472
Iteration 125/1000 | Loss: 0.00001472
Iteration 126/1000 | Loss: 0.00001472
Iteration 127/1000 | Loss: 0.00001472
Iteration 128/1000 | Loss: 0.00001472
Iteration 129/1000 | Loss: 0.00001472
Iteration 130/1000 | Loss: 0.00001471
Iteration 131/1000 | Loss: 0.00001471
Iteration 132/1000 | Loss: 0.00001471
Iteration 133/1000 | Loss: 0.00001470
Iteration 134/1000 | Loss: 0.00001470
Iteration 135/1000 | Loss: 0.00001469
Iteration 136/1000 | Loss: 0.00001469
Iteration 137/1000 | Loss: 0.00001469
Iteration 138/1000 | Loss: 0.00001468
Iteration 139/1000 | Loss: 0.00001468
Iteration 140/1000 | Loss: 0.00001468
Iteration 141/1000 | Loss: 0.00001468
Iteration 142/1000 | Loss: 0.00001467
Iteration 143/1000 | Loss: 0.00001467
Iteration 144/1000 | Loss: 0.00001467
Iteration 145/1000 | Loss: 0.00001467
Iteration 146/1000 | Loss: 0.00001467
Iteration 147/1000 | Loss: 0.00001467
Iteration 148/1000 | Loss: 0.00001467
Iteration 149/1000 | Loss: 0.00001467
Iteration 150/1000 | Loss: 0.00001466
Iteration 151/1000 | Loss: 0.00001466
Iteration 152/1000 | Loss: 0.00001466
Iteration 153/1000 | Loss: 0.00001466
Iteration 154/1000 | Loss: 0.00001466
Iteration 155/1000 | Loss: 0.00001466
Iteration 156/1000 | Loss: 0.00001465
Iteration 157/1000 | Loss: 0.00001464
Iteration 158/1000 | Loss: 0.00001464
Iteration 159/1000 | Loss: 0.00001464
Iteration 160/1000 | Loss: 0.00001463
Iteration 161/1000 | Loss: 0.00001463
Iteration 162/1000 | Loss: 0.00001463
Iteration 163/1000 | Loss: 0.00001463
Iteration 164/1000 | Loss: 0.00001463
Iteration 165/1000 | Loss: 0.00001463
Iteration 166/1000 | Loss: 0.00001463
Iteration 167/1000 | Loss: 0.00001463
Iteration 168/1000 | Loss: 0.00001463
Iteration 169/1000 | Loss: 0.00001463
Iteration 170/1000 | Loss: 0.00001463
Iteration 171/1000 | Loss: 0.00001463
Iteration 172/1000 | Loss: 0.00001463
Iteration 173/1000 | Loss: 0.00001463
Iteration 174/1000 | Loss: 0.00001463
Iteration 175/1000 | Loss: 0.00001463
Iteration 176/1000 | Loss: 0.00001463
Iteration 177/1000 | Loss: 0.00001463
Iteration 178/1000 | Loss: 0.00001463
Iteration 179/1000 | Loss: 0.00001463
Iteration 180/1000 | Loss: 0.00001463
Iteration 181/1000 | Loss: 0.00001463
Iteration 182/1000 | Loss: 0.00001463
Iteration 183/1000 | Loss: 0.00001463
Iteration 184/1000 | Loss: 0.00001463
Iteration 185/1000 | Loss: 0.00001463
Iteration 186/1000 | Loss: 0.00001463
Iteration 187/1000 | Loss: 0.00001463
Iteration 188/1000 | Loss: 0.00001463
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 188. Stopping optimization.
Last 5 losses: [1.462899581383681e-05, 1.462899581383681e-05, 1.462899581383681e-05, 1.462899581383681e-05, 1.462899581383681e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.462899581383681e-05

Optimization complete. Final v2v error: 3.165919065475464 mm

Highest mean error: 5.0931525230407715 mm for frame 81

Lowest mean error: 2.679412841796875 mm for frame 128

Saving results

Total time: 182.5724856853485
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_006/1052/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_006/1052.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_006/1052
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00909979
Iteration 2/25 | Loss: 0.00208899
Iteration 3/25 | Loss: 0.00107698
Iteration 4/25 | Loss: 0.00102234
Iteration 5/25 | Loss: 0.00100532
Iteration 6/25 | Loss: 0.00100097
Iteration 7/25 | Loss: 0.00100006
Iteration 8/25 | Loss: 0.00100006
Iteration 9/25 | Loss: 0.00100006
Iteration 10/25 | Loss: 0.00100006
Iteration 11/25 | Loss: 0.00100006
Iteration 12/25 | Loss: 0.00100006
Iteration 13/25 | Loss: 0.00100006
Iteration 14/25 | Loss: 0.00100006
Iteration 15/25 | Loss: 0.00100006
Iteration 16/25 | Loss: 0.00100006
Iteration 17/25 | Loss: 0.00100006
Iteration 18/25 | Loss: 0.00100006
Iteration 19/25 | Loss: 0.00100006
Iteration 20/25 | Loss: 0.00100006
Iteration 21/25 | Loss: 0.00100006
Iteration 22/25 | Loss: 0.00100006
Iteration 23/25 | Loss: 0.00100006
Iteration 24/25 | Loss: 0.00100006
Iteration 25/25 | Loss: 0.00100006

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.93061090
Iteration 2/25 | Loss: 0.00028061
Iteration 3/25 | Loss: 0.00028061
Iteration 4/25 | Loss: 0.00028061
Iteration 5/25 | Loss: 0.00028061
Iteration 6/25 | Loss: 0.00028061
Iteration 7/25 | Loss: 0.00028061
Iteration 8/25 | Loss: 0.00028061
Iteration 9/25 | Loss: 0.00028061
Iteration 10/25 | Loss: 0.00028061
Iteration 11/25 | Loss: 0.00028061
Iteration 12/25 | Loss: 0.00028061
Iteration 13/25 | Loss: 0.00028061
Iteration 14/25 | Loss: 0.00028061
Iteration 15/25 | Loss: 0.00028061
Iteration 16/25 | Loss: 0.00028061
Iteration 17/25 | Loss: 0.00028061
Iteration 18/25 | Loss: 0.00028061
Iteration 19/25 | Loss: 0.00028061
Iteration 20/25 | Loss: 0.00028061
Iteration 21/25 | Loss: 0.00028061
Iteration 22/25 | Loss: 0.00028061
Iteration 23/25 | Loss: 0.00028061
Iteration 24/25 | Loss: 0.00028061
Iteration 25/25 | Loss: 0.00028061

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00028061
Iteration 2/1000 | Loss: 0.00006498
Iteration 3/1000 | Loss: 0.00004686
Iteration 4/1000 | Loss: 0.00004128
Iteration 5/1000 | Loss: 0.00003924
Iteration 6/1000 | Loss: 0.00003793
Iteration 7/1000 | Loss: 0.00003669
Iteration 8/1000 | Loss: 0.00003569
Iteration 9/1000 | Loss: 0.00003510
Iteration 10/1000 | Loss: 0.00003452
Iteration 11/1000 | Loss: 0.00003418
Iteration 12/1000 | Loss: 0.00003388
Iteration 13/1000 | Loss: 0.00003360
Iteration 14/1000 | Loss: 0.00003343
Iteration 15/1000 | Loss: 0.00003327
Iteration 16/1000 | Loss: 0.00003307
Iteration 17/1000 | Loss: 0.00003288
Iteration 18/1000 | Loss: 0.00003270
Iteration 19/1000 | Loss: 0.00003256
Iteration 20/1000 | Loss: 0.00003247
Iteration 21/1000 | Loss: 0.00003236
Iteration 22/1000 | Loss: 0.00003233
Iteration 23/1000 | Loss: 0.00003230
Iteration 24/1000 | Loss: 0.00003223
Iteration 25/1000 | Loss: 0.00003222
Iteration 26/1000 | Loss: 0.00003220
Iteration 27/1000 | Loss: 0.00003220
Iteration 28/1000 | Loss: 0.00003219
Iteration 29/1000 | Loss: 0.00003219
Iteration 30/1000 | Loss: 0.00003218
Iteration 31/1000 | Loss: 0.00003218
Iteration 32/1000 | Loss: 0.00003217
Iteration 33/1000 | Loss: 0.00003217
Iteration 34/1000 | Loss: 0.00003216
Iteration 35/1000 | Loss: 0.00003216
Iteration 36/1000 | Loss: 0.00003215
Iteration 37/1000 | Loss: 0.00003215
Iteration 38/1000 | Loss: 0.00003215
Iteration 39/1000 | Loss: 0.00003214
Iteration 40/1000 | Loss: 0.00003214
Iteration 41/1000 | Loss: 0.00003214
Iteration 42/1000 | Loss: 0.00003214
Iteration 43/1000 | Loss: 0.00003214
Iteration 44/1000 | Loss: 0.00003214
Iteration 45/1000 | Loss: 0.00003213
Iteration 46/1000 | Loss: 0.00003211
Iteration 47/1000 | Loss: 0.00003211
Iteration 48/1000 | Loss: 0.00003211
Iteration 49/1000 | Loss: 0.00003211
Iteration 50/1000 | Loss: 0.00003211
Iteration 51/1000 | Loss: 0.00003211
Iteration 52/1000 | Loss: 0.00003210
Iteration 53/1000 | Loss: 0.00003210
Iteration 54/1000 | Loss: 0.00003210
Iteration 55/1000 | Loss: 0.00003210
Iteration 56/1000 | Loss: 0.00003209
Iteration 57/1000 | Loss: 0.00003208
Iteration 58/1000 | Loss: 0.00003208
Iteration 59/1000 | Loss: 0.00003208
Iteration 60/1000 | Loss: 0.00003208
Iteration 61/1000 | Loss: 0.00003208
Iteration 62/1000 | Loss: 0.00003208
Iteration 63/1000 | Loss: 0.00003208
Iteration 64/1000 | Loss: 0.00003208
Iteration 65/1000 | Loss: 0.00003208
Iteration 66/1000 | Loss: 0.00003208
Iteration 67/1000 | Loss: 0.00003208
Iteration 68/1000 | Loss: 0.00003207
Iteration 69/1000 | Loss: 0.00003207
Iteration 70/1000 | Loss: 0.00003206
Iteration 71/1000 | Loss: 0.00003206
Iteration 72/1000 | Loss: 0.00003206
Iteration 73/1000 | Loss: 0.00003206
Iteration 74/1000 | Loss: 0.00003206
Iteration 75/1000 | Loss: 0.00003206
Iteration 76/1000 | Loss: 0.00003206
Iteration 77/1000 | Loss: 0.00003206
Iteration 78/1000 | Loss: 0.00003206
Iteration 79/1000 | Loss: 0.00003205
Iteration 80/1000 | Loss: 0.00003205
Iteration 81/1000 | Loss: 0.00003205
Iteration 82/1000 | Loss: 0.00003205
Iteration 83/1000 | Loss: 0.00003205
Iteration 84/1000 | Loss: 0.00003205
Iteration 85/1000 | Loss: 0.00003205
Iteration 86/1000 | Loss: 0.00003205
Iteration 87/1000 | Loss: 0.00003205
Iteration 88/1000 | Loss: 0.00003205
Iteration 89/1000 | Loss: 0.00003205
Iteration 90/1000 | Loss: 0.00003205
Iteration 91/1000 | Loss: 0.00003205
Iteration 92/1000 | Loss: 0.00003205
Iteration 93/1000 | Loss: 0.00003205
Iteration 94/1000 | Loss: 0.00003204
Iteration 95/1000 | Loss: 0.00003204
Iteration 96/1000 | Loss: 0.00003204
Iteration 97/1000 | Loss: 0.00003204
Iteration 98/1000 | Loss: 0.00003204
Iteration 99/1000 | Loss: 0.00003203
Iteration 100/1000 | Loss: 0.00003203
Iteration 101/1000 | Loss: 0.00003203
Iteration 102/1000 | Loss: 0.00003203
Iteration 103/1000 | Loss: 0.00003203
Iteration 104/1000 | Loss: 0.00003203
Iteration 105/1000 | Loss: 0.00003202
Iteration 106/1000 | Loss: 0.00003202
Iteration 107/1000 | Loss: 0.00003202
Iteration 108/1000 | Loss: 0.00003202
Iteration 109/1000 | Loss: 0.00003201
Iteration 110/1000 | Loss: 0.00003201
Iteration 111/1000 | Loss: 0.00003201
Iteration 112/1000 | Loss: 0.00003201
Iteration 113/1000 | Loss: 0.00003201
Iteration 114/1000 | Loss: 0.00003200
Iteration 115/1000 | Loss: 0.00003200
Iteration 116/1000 | Loss: 0.00003200
Iteration 117/1000 | Loss: 0.00003200
Iteration 118/1000 | Loss: 0.00003199
Iteration 119/1000 | Loss: 0.00003199
Iteration 120/1000 | Loss: 0.00003199
Iteration 121/1000 | Loss: 0.00003199
Iteration 122/1000 | Loss: 0.00003199
Iteration 123/1000 | Loss: 0.00003199
Iteration 124/1000 | Loss: 0.00003199
Iteration 125/1000 | Loss: 0.00003199
Iteration 126/1000 | Loss: 0.00003199
Iteration 127/1000 | Loss: 0.00003198
Iteration 128/1000 | Loss: 0.00003198
Iteration 129/1000 | Loss: 0.00003198
Iteration 130/1000 | Loss: 0.00003198
Iteration 131/1000 | Loss: 0.00003198
Iteration 132/1000 | Loss: 0.00003198
Iteration 133/1000 | Loss: 0.00003198
Iteration 134/1000 | Loss: 0.00003198
Iteration 135/1000 | Loss: 0.00003198
Iteration 136/1000 | Loss: 0.00003198
Iteration 137/1000 | Loss: 0.00003198
Iteration 138/1000 | Loss: 0.00003198
Iteration 139/1000 | Loss: 0.00003198
Iteration 140/1000 | Loss: 0.00003198
Iteration 141/1000 | Loss: 0.00003198
Iteration 142/1000 | Loss: 0.00003197
Iteration 143/1000 | Loss: 0.00003197
Iteration 144/1000 | Loss: 0.00003197
Iteration 145/1000 | Loss: 0.00003197
Iteration 146/1000 | Loss: 0.00003196
Iteration 147/1000 | Loss: 0.00003196
Iteration 148/1000 | Loss: 0.00003196
Iteration 149/1000 | Loss: 0.00003196
Iteration 150/1000 | Loss: 0.00003195
Iteration 151/1000 | Loss: 0.00003195
Iteration 152/1000 | Loss: 0.00003195
Iteration 153/1000 | Loss: 0.00003194
Iteration 154/1000 | Loss: 0.00003194
Iteration 155/1000 | Loss: 0.00003194
Iteration 156/1000 | Loss: 0.00003194
Iteration 157/1000 | Loss: 0.00003194
Iteration 158/1000 | Loss: 0.00003193
Iteration 159/1000 | Loss: 0.00003193
Iteration 160/1000 | Loss: 0.00003193
Iteration 161/1000 | Loss: 0.00003193
Iteration 162/1000 | Loss: 0.00003193
Iteration 163/1000 | Loss: 0.00003193
Iteration 164/1000 | Loss: 0.00003192
Iteration 165/1000 | Loss: 0.00003192
Iteration 166/1000 | Loss: 0.00003192
Iteration 167/1000 | Loss: 0.00003192
Iteration 168/1000 | Loss: 0.00003192
Iteration 169/1000 | Loss: 0.00003192
Iteration 170/1000 | Loss: 0.00003192
Iteration 171/1000 | Loss: 0.00003192
Iteration 172/1000 | Loss: 0.00003192
Iteration 173/1000 | Loss: 0.00003191
Iteration 174/1000 | Loss: 0.00003191
Iteration 175/1000 | Loss: 0.00003191
Iteration 176/1000 | Loss: 0.00003191
Iteration 177/1000 | Loss: 0.00003190
Iteration 178/1000 | Loss: 0.00003190
Iteration 179/1000 | Loss: 0.00003190
Iteration 180/1000 | Loss: 0.00003190
Iteration 181/1000 | Loss: 0.00003190
Iteration 182/1000 | Loss: 0.00003189
Iteration 183/1000 | Loss: 0.00003189
Iteration 184/1000 | Loss: 0.00003189
Iteration 185/1000 | Loss: 0.00003188
Iteration 186/1000 | Loss: 0.00003188
Iteration 187/1000 | Loss: 0.00003188
Iteration 188/1000 | Loss: 0.00003188
Iteration 189/1000 | Loss: 0.00003188
Iteration 190/1000 | Loss: 0.00003187
Iteration 191/1000 | Loss: 0.00003187
Iteration 192/1000 | Loss: 0.00003187
Iteration 193/1000 | Loss: 0.00003187
Iteration 194/1000 | Loss: 0.00003187
Iteration 195/1000 | Loss: 0.00003186
Iteration 196/1000 | Loss: 0.00003186
Iteration 197/1000 | Loss: 0.00003186
Iteration 198/1000 | Loss: 0.00003186
Iteration 199/1000 | Loss: 0.00003186
Iteration 200/1000 | Loss: 0.00003186
Iteration 201/1000 | Loss: 0.00003186
Iteration 202/1000 | Loss: 0.00003186
Iteration 203/1000 | Loss: 0.00003185
Iteration 204/1000 | Loss: 0.00003185
Iteration 205/1000 | Loss: 0.00003185
Iteration 206/1000 | Loss: 0.00003185
Iteration 207/1000 | Loss: 0.00003185
Iteration 208/1000 | Loss: 0.00003185
Iteration 209/1000 | Loss: 0.00003185
Iteration 210/1000 | Loss: 0.00003185
Iteration 211/1000 | Loss: 0.00003184
Iteration 212/1000 | Loss: 0.00003184
Iteration 213/1000 | Loss: 0.00003184
Iteration 214/1000 | Loss: 0.00003184
Iteration 215/1000 | Loss: 0.00003184
Iteration 216/1000 | Loss: 0.00003184
Iteration 217/1000 | Loss: 0.00003184
Iteration 218/1000 | Loss: 0.00003184
Iteration 219/1000 | Loss: 0.00003184
Iteration 220/1000 | Loss: 0.00003183
Iteration 221/1000 | Loss: 0.00003183
Iteration 222/1000 | Loss: 0.00003183
Iteration 223/1000 | Loss: 0.00003183
Iteration 224/1000 | Loss: 0.00003183
Iteration 225/1000 | Loss: 0.00003183
Iteration 226/1000 | Loss: 0.00003183
Iteration 227/1000 | Loss: 0.00003182
Iteration 228/1000 | Loss: 0.00003182
Iteration 229/1000 | Loss: 0.00003182
Iteration 230/1000 | Loss: 0.00003182
Iteration 231/1000 | Loss: 0.00003182
Iteration 232/1000 | Loss: 0.00003182
Iteration 233/1000 | Loss: 0.00003182
Iteration 234/1000 | Loss: 0.00003182
Iteration 235/1000 | Loss: 0.00003182
Iteration 236/1000 | Loss: 0.00003181
Iteration 237/1000 | Loss: 0.00003181
Iteration 238/1000 | Loss: 0.00003181
Iteration 239/1000 | Loss: 0.00003181
Iteration 240/1000 | Loss: 0.00003181
Iteration 241/1000 | Loss: 0.00003180
Iteration 242/1000 | Loss: 0.00003180
Iteration 243/1000 | Loss: 0.00003180
Iteration 244/1000 | Loss: 0.00003180
Iteration 245/1000 | Loss: 0.00003180
Iteration 246/1000 | Loss: 0.00003180
Iteration 247/1000 | Loss: 0.00003180
Iteration 248/1000 | Loss: 0.00003180
Iteration 249/1000 | Loss: 0.00003180
Iteration 250/1000 | Loss: 0.00003180
Iteration 251/1000 | Loss: 0.00003180
Iteration 252/1000 | Loss: 0.00003180
Iteration 253/1000 | Loss: 0.00003179
Iteration 254/1000 | Loss: 0.00003179
Iteration 255/1000 | Loss: 0.00003179
Iteration 256/1000 | Loss: 0.00003179
Iteration 257/1000 | Loss: 0.00003179
Iteration 258/1000 | Loss: 0.00003179
Iteration 259/1000 | Loss: 0.00003179
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 259. Stopping optimization.
Last 5 losses: [3.179495979566127e-05, 3.179495979566127e-05, 3.179495979566127e-05, 3.179495979566127e-05, 3.179495979566127e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.179495979566127e-05

Optimization complete. Final v2v error: 4.736937999725342 mm

Highest mean error: 5.3983988761901855 mm for frame 39

Lowest mean error: 4.127079010009766 mm for frame 237

Saving results

Total time: 66.69868683815002
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_006/1029/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_006/1029.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_006/1029
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00893838
Iteration 2/25 | Loss: 0.00091746
Iteration 3/25 | Loss: 0.00078583
Iteration 4/25 | Loss: 0.00074518
Iteration 5/25 | Loss: 0.00073917
Iteration 6/25 | Loss: 0.00073741
Iteration 7/25 | Loss: 0.00073731
Iteration 8/25 | Loss: 0.00073731
Iteration 9/25 | Loss: 0.00073731
Iteration 10/25 | Loss: 0.00073731
Iteration 11/25 | Loss: 0.00073731
Iteration 12/25 | Loss: 0.00073731
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0007373105618171394, 0.0007373105618171394, 0.0007373105618171394, 0.0007373105618171394, 0.0007373105618171394]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007373105618171394

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42198503
Iteration 2/25 | Loss: 0.00034002
Iteration 3/25 | Loss: 0.00033992
Iteration 4/25 | Loss: 0.00033992
Iteration 5/25 | Loss: 0.00033992
Iteration 6/25 | Loss: 0.00033992
Iteration 7/25 | Loss: 0.00033992
Iteration 8/25 | Loss: 0.00033992
Iteration 9/25 | Loss: 0.00033992
Iteration 10/25 | Loss: 0.00033992
Iteration 11/25 | Loss: 0.00033992
Iteration 12/25 | Loss: 0.00033992
Iteration 13/25 | Loss: 0.00033992
Iteration 14/25 | Loss: 0.00033992
Iteration 15/25 | Loss: 0.00033992
Iteration 16/25 | Loss: 0.00033992
Iteration 17/25 | Loss: 0.00033992
Iteration 18/25 | Loss: 0.00033992
Iteration 19/25 | Loss: 0.00033992
Iteration 20/25 | Loss: 0.00033992
Iteration 21/25 | Loss: 0.00033992
Iteration 22/25 | Loss: 0.00033992
Iteration 23/25 | Loss: 0.00033992
Iteration 24/25 | Loss: 0.00033992
Iteration 25/25 | Loss: 0.00033992

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00033992
Iteration 2/1000 | Loss: 0.00002348
Iteration 3/1000 | Loss: 0.00001865
Iteration 4/1000 | Loss: 0.00001684
Iteration 5/1000 | Loss: 0.00001567
Iteration 6/1000 | Loss: 0.00001517
Iteration 7/1000 | Loss: 0.00001478
Iteration 8/1000 | Loss: 0.00001454
Iteration 9/1000 | Loss: 0.00001444
Iteration 10/1000 | Loss: 0.00001438
Iteration 11/1000 | Loss: 0.00001434
Iteration 12/1000 | Loss: 0.00001432
Iteration 13/1000 | Loss: 0.00001428
Iteration 14/1000 | Loss: 0.00001424
Iteration 15/1000 | Loss: 0.00001413
Iteration 16/1000 | Loss: 0.00001404
Iteration 17/1000 | Loss: 0.00001394
Iteration 18/1000 | Loss: 0.00001389
Iteration 19/1000 | Loss: 0.00001389
Iteration 20/1000 | Loss: 0.00001388
Iteration 21/1000 | Loss: 0.00001388
Iteration 22/1000 | Loss: 0.00001387
Iteration 23/1000 | Loss: 0.00001386
Iteration 24/1000 | Loss: 0.00001386
Iteration 25/1000 | Loss: 0.00001386
Iteration 26/1000 | Loss: 0.00001385
Iteration 27/1000 | Loss: 0.00001385
Iteration 28/1000 | Loss: 0.00001385
Iteration 29/1000 | Loss: 0.00001384
Iteration 30/1000 | Loss: 0.00001384
Iteration 31/1000 | Loss: 0.00001383
Iteration 32/1000 | Loss: 0.00001382
Iteration 33/1000 | Loss: 0.00001381
Iteration 34/1000 | Loss: 0.00001381
Iteration 35/1000 | Loss: 0.00001381
Iteration 36/1000 | Loss: 0.00001380
Iteration 37/1000 | Loss: 0.00001380
Iteration 38/1000 | Loss: 0.00001380
Iteration 39/1000 | Loss: 0.00001380
Iteration 40/1000 | Loss: 0.00001379
Iteration 41/1000 | Loss: 0.00001379
Iteration 42/1000 | Loss: 0.00001378
Iteration 43/1000 | Loss: 0.00001378
Iteration 44/1000 | Loss: 0.00001377
Iteration 45/1000 | Loss: 0.00001376
Iteration 46/1000 | Loss: 0.00001376
Iteration 47/1000 | Loss: 0.00001376
Iteration 48/1000 | Loss: 0.00001376
Iteration 49/1000 | Loss: 0.00001376
Iteration 50/1000 | Loss: 0.00001376
Iteration 51/1000 | Loss: 0.00001375
Iteration 52/1000 | Loss: 0.00001375
Iteration 53/1000 | Loss: 0.00001375
Iteration 54/1000 | Loss: 0.00001375
Iteration 55/1000 | Loss: 0.00001375
Iteration 56/1000 | Loss: 0.00001375
Iteration 57/1000 | Loss: 0.00001375
Iteration 58/1000 | Loss: 0.00001375
Iteration 59/1000 | Loss: 0.00001375
Iteration 60/1000 | Loss: 0.00001375
Iteration 61/1000 | Loss: 0.00001375
Iteration 62/1000 | Loss: 0.00001375
Iteration 63/1000 | Loss: 0.00001374
Iteration 64/1000 | Loss: 0.00001374
Iteration 65/1000 | Loss: 0.00001374
Iteration 66/1000 | Loss: 0.00001373
Iteration 67/1000 | Loss: 0.00001373
Iteration 68/1000 | Loss: 0.00001372
Iteration 69/1000 | Loss: 0.00001372
Iteration 70/1000 | Loss: 0.00001372
Iteration 71/1000 | Loss: 0.00001372
Iteration 72/1000 | Loss: 0.00001372
Iteration 73/1000 | Loss: 0.00001372
Iteration 74/1000 | Loss: 0.00001372
Iteration 75/1000 | Loss: 0.00001372
Iteration 76/1000 | Loss: 0.00001372
Iteration 77/1000 | Loss: 0.00001372
Iteration 78/1000 | Loss: 0.00001372
Iteration 79/1000 | Loss: 0.00001372
Iteration 80/1000 | Loss: 0.00001372
Iteration 81/1000 | Loss: 0.00001372
Iteration 82/1000 | Loss: 0.00001371
Iteration 83/1000 | Loss: 0.00001371
Iteration 84/1000 | Loss: 0.00001371
Iteration 85/1000 | Loss: 0.00001371
Iteration 86/1000 | Loss: 0.00001371
Iteration 87/1000 | Loss: 0.00001371
Iteration 88/1000 | Loss: 0.00001371
Iteration 89/1000 | Loss: 0.00001371
Iteration 90/1000 | Loss: 0.00001371
Iteration 91/1000 | Loss: 0.00001370
Iteration 92/1000 | Loss: 0.00001370
Iteration 93/1000 | Loss: 0.00001370
Iteration 94/1000 | Loss: 0.00001370
Iteration 95/1000 | Loss: 0.00001370
Iteration 96/1000 | Loss: 0.00001370
Iteration 97/1000 | Loss: 0.00001369
Iteration 98/1000 | Loss: 0.00001369
Iteration 99/1000 | Loss: 0.00001369
Iteration 100/1000 | Loss: 0.00001369
Iteration 101/1000 | Loss: 0.00001369
Iteration 102/1000 | Loss: 0.00001369
Iteration 103/1000 | Loss: 0.00001369
Iteration 104/1000 | Loss: 0.00001369
Iteration 105/1000 | Loss: 0.00001369
Iteration 106/1000 | Loss: 0.00001369
Iteration 107/1000 | Loss: 0.00001369
Iteration 108/1000 | Loss: 0.00001369
Iteration 109/1000 | Loss: 0.00001369
Iteration 110/1000 | Loss: 0.00001368
Iteration 111/1000 | Loss: 0.00001368
Iteration 112/1000 | Loss: 0.00001368
Iteration 113/1000 | Loss: 0.00001368
Iteration 114/1000 | Loss: 0.00001368
Iteration 115/1000 | Loss: 0.00001368
Iteration 116/1000 | Loss: 0.00001368
Iteration 117/1000 | Loss: 0.00001368
Iteration 118/1000 | Loss: 0.00001368
Iteration 119/1000 | Loss: 0.00001368
Iteration 120/1000 | Loss: 0.00001367
Iteration 121/1000 | Loss: 0.00001367
Iteration 122/1000 | Loss: 0.00001367
Iteration 123/1000 | Loss: 0.00001366
Iteration 124/1000 | Loss: 0.00001366
Iteration 125/1000 | Loss: 0.00001366
Iteration 126/1000 | Loss: 0.00001366
Iteration 127/1000 | Loss: 0.00001366
Iteration 128/1000 | Loss: 0.00001366
Iteration 129/1000 | Loss: 0.00001365
Iteration 130/1000 | Loss: 0.00001365
Iteration 131/1000 | Loss: 0.00001365
Iteration 132/1000 | Loss: 0.00001365
Iteration 133/1000 | Loss: 0.00001365
Iteration 134/1000 | Loss: 0.00001365
Iteration 135/1000 | Loss: 0.00001364
Iteration 136/1000 | Loss: 0.00001364
Iteration 137/1000 | Loss: 0.00001364
Iteration 138/1000 | Loss: 0.00001364
Iteration 139/1000 | Loss: 0.00001364
Iteration 140/1000 | Loss: 0.00001364
Iteration 141/1000 | Loss: 0.00001364
Iteration 142/1000 | Loss: 0.00001364
Iteration 143/1000 | Loss: 0.00001364
Iteration 144/1000 | Loss: 0.00001364
Iteration 145/1000 | Loss: 0.00001364
Iteration 146/1000 | Loss: 0.00001364
Iteration 147/1000 | Loss: 0.00001364
Iteration 148/1000 | Loss: 0.00001364
Iteration 149/1000 | Loss: 0.00001364
Iteration 150/1000 | Loss: 0.00001364
Iteration 151/1000 | Loss: 0.00001364
Iteration 152/1000 | Loss: 0.00001364
Iteration 153/1000 | Loss: 0.00001364
Iteration 154/1000 | Loss: 0.00001364
Iteration 155/1000 | Loss: 0.00001364
Iteration 156/1000 | Loss: 0.00001364
Iteration 157/1000 | Loss: 0.00001364
Iteration 158/1000 | Loss: 0.00001364
Iteration 159/1000 | Loss: 0.00001364
Iteration 160/1000 | Loss: 0.00001364
Iteration 161/1000 | Loss: 0.00001364
Iteration 162/1000 | Loss: 0.00001364
Iteration 163/1000 | Loss: 0.00001364
Iteration 164/1000 | Loss: 0.00001364
Iteration 165/1000 | Loss: 0.00001364
Iteration 166/1000 | Loss: 0.00001364
Iteration 167/1000 | Loss: 0.00001364
Iteration 168/1000 | Loss: 0.00001364
Iteration 169/1000 | Loss: 0.00001364
Iteration 170/1000 | Loss: 0.00001364
Iteration 171/1000 | Loss: 0.00001364
Iteration 172/1000 | Loss: 0.00001364
Iteration 173/1000 | Loss: 0.00001364
Iteration 174/1000 | Loss: 0.00001364
Iteration 175/1000 | Loss: 0.00001364
Iteration 176/1000 | Loss: 0.00001364
Iteration 177/1000 | Loss: 0.00001364
Iteration 178/1000 | Loss: 0.00001364
Iteration 179/1000 | Loss: 0.00001364
Iteration 180/1000 | Loss: 0.00001364
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 180. Stopping optimization.
Last 5 losses: [1.3637321899295785e-05, 1.3637321899295785e-05, 1.3637321899295785e-05, 1.3637321899295785e-05, 1.3637321899295785e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3637321899295785e-05

Optimization complete. Final v2v error: 3.1980137825012207 mm

Highest mean error: 3.505622148513794 mm for frame 220

Lowest mean error: 2.984895944595337 mm for frame 40

Saving results

Total time: 40.50965452194214
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_006/1006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_006/1006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_006/1006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00925773
Iteration 2/25 | Loss: 0.00326395
Iteration 3/25 | Loss: 0.00204901
Iteration 4/25 | Loss: 0.00165774
Iteration 5/25 | Loss: 0.00140047
Iteration 6/25 | Loss: 0.00133441
Iteration 7/25 | Loss: 0.00125953
Iteration 8/25 | Loss: 0.00122238
Iteration 9/25 | Loss: 0.00117237
Iteration 10/25 | Loss: 0.00111199
Iteration 11/25 | Loss: 0.00107176
Iteration 12/25 | Loss: 0.00104244
Iteration 13/25 | Loss: 0.00103326
Iteration 14/25 | Loss: 0.00103048
Iteration 15/25 | Loss: 0.00103317
Iteration 16/25 | Loss: 0.00102832
Iteration 17/25 | Loss: 0.00102626
Iteration 18/25 | Loss: 0.00102401
Iteration 19/25 | Loss: 0.00102254
Iteration 20/25 | Loss: 0.00101745
Iteration 21/25 | Loss: 0.00101553
Iteration 22/25 | Loss: 0.00101515
Iteration 23/25 | Loss: 0.00101505
Iteration 24/25 | Loss: 0.00101505
Iteration 25/25 | Loss: 0.00101504

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.70340824
Iteration 2/25 | Loss: 0.00357304
Iteration 3/25 | Loss: 0.00290634
Iteration 4/25 | Loss: 0.00290634
Iteration 5/25 | Loss: 0.00290634
Iteration 6/25 | Loss: 0.00290634
Iteration 7/25 | Loss: 0.00290633
Iteration 8/25 | Loss: 0.00290633
Iteration 9/25 | Loss: 0.00290633
Iteration 10/25 | Loss: 0.00290633
Iteration 11/25 | Loss: 0.00290633
Iteration 12/25 | Loss: 0.00290633
Iteration 13/25 | Loss: 0.00290633
Iteration 14/25 | Loss: 0.00290633
Iteration 15/25 | Loss: 0.00290633
Iteration 16/25 | Loss: 0.00290633
Iteration 17/25 | Loss: 0.00290633
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0029063336551189423, 0.0029063336551189423, 0.0029063336551189423, 0.0029063336551189423, 0.0029063336551189423]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0029063336551189423

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00290633
Iteration 2/1000 | Loss: 0.00068990
Iteration 3/1000 | Loss: 0.00154472
Iteration 4/1000 | Loss: 0.00098590
Iteration 5/1000 | Loss: 0.00609533
Iteration 6/1000 | Loss: 0.00105555
Iteration 7/1000 | Loss: 0.00188346
Iteration 8/1000 | Loss: 0.00311457
Iteration 9/1000 | Loss: 0.00076583
Iteration 10/1000 | Loss: 0.00218296
Iteration 11/1000 | Loss: 0.00108332
Iteration 12/1000 | Loss: 0.00332159
Iteration 13/1000 | Loss: 0.00051291
Iteration 14/1000 | Loss: 0.00017285
Iteration 15/1000 | Loss: 0.00013957
Iteration 16/1000 | Loss: 0.00036532
Iteration 17/1000 | Loss: 0.00166055
Iteration 18/1000 | Loss: 0.00039937
Iteration 19/1000 | Loss: 0.00057363
Iteration 20/1000 | Loss: 0.00164853
Iteration 21/1000 | Loss: 0.00099993
Iteration 22/1000 | Loss: 0.00073325
Iteration 23/1000 | Loss: 0.00030115
Iteration 24/1000 | Loss: 0.00081806
Iteration 25/1000 | Loss: 0.00583857
Iteration 26/1000 | Loss: 0.00092779
Iteration 27/1000 | Loss: 0.00139035
Iteration 28/1000 | Loss: 0.00178708
Iteration 29/1000 | Loss: 0.00102156
Iteration 30/1000 | Loss: 0.00135357
Iteration 31/1000 | Loss: 0.00203981
Iteration 32/1000 | Loss: 0.00337059
Iteration 33/1000 | Loss: 0.00208027
Iteration 34/1000 | Loss: 0.00305914
Iteration 35/1000 | Loss: 0.00254276
Iteration 36/1000 | Loss: 0.00174804
Iteration 37/1000 | Loss: 0.00139401
Iteration 38/1000 | Loss: 0.00221994
Iteration 39/1000 | Loss: 0.00261245
Iteration 40/1000 | Loss: 0.00262866
Iteration 41/1000 | Loss: 0.00243168
Iteration 42/1000 | Loss: 0.00180310
Iteration 43/1000 | Loss: 0.00160649
Iteration 44/1000 | Loss: 0.00066204
Iteration 45/1000 | Loss: 0.00075191
Iteration 46/1000 | Loss: 0.00070976
Iteration 47/1000 | Loss: 0.00105389
Iteration 48/1000 | Loss: 0.00065028
Iteration 49/1000 | Loss: 0.00103007
Iteration 50/1000 | Loss: 0.00104491
Iteration 51/1000 | Loss: 0.00176591
Iteration 52/1000 | Loss: 0.00104165
Iteration 53/1000 | Loss: 0.00064881
Iteration 54/1000 | Loss: 0.00055266
Iteration 55/1000 | Loss: 0.00117226
Iteration 56/1000 | Loss: 0.00068333
Iteration 57/1000 | Loss: 0.00041829
Iteration 58/1000 | Loss: 0.00087375
Iteration 59/1000 | Loss: 0.00006972
Iteration 60/1000 | Loss: 0.00030866
Iteration 61/1000 | Loss: 0.00066326
Iteration 62/1000 | Loss: 0.00055106
Iteration 63/1000 | Loss: 0.00039330
Iteration 64/1000 | Loss: 0.00041616
Iteration 65/1000 | Loss: 0.00180496
Iteration 66/1000 | Loss: 0.00017627
Iteration 67/1000 | Loss: 0.00038001
Iteration 68/1000 | Loss: 0.00017221
Iteration 69/1000 | Loss: 0.00021937
Iteration 70/1000 | Loss: 0.00017246
Iteration 71/1000 | Loss: 0.00295515
Iteration 72/1000 | Loss: 0.00071935
Iteration 73/1000 | Loss: 0.00080716
Iteration 74/1000 | Loss: 0.00007635
Iteration 75/1000 | Loss: 0.00018692
Iteration 76/1000 | Loss: 0.00038388
Iteration 77/1000 | Loss: 0.00079355
Iteration 78/1000 | Loss: 0.00118409
Iteration 79/1000 | Loss: 0.00051462
Iteration 80/1000 | Loss: 0.00025486
Iteration 81/1000 | Loss: 0.00040662
Iteration 82/1000 | Loss: 0.00006503
Iteration 83/1000 | Loss: 0.00090906
Iteration 84/1000 | Loss: 0.00170994
Iteration 85/1000 | Loss: 0.00107281
Iteration 86/1000 | Loss: 0.00073453
Iteration 87/1000 | Loss: 0.00075998
Iteration 88/1000 | Loss: 0.00060520
Iteration 89/1000 | Loss: 0.00108529
Iteration 90/1000 | Loss: 0.00172991
Iteration 91/1000 | Loss: 0.00062111
Iteration 92/1000 | Loss: 0.00069198
Iteration 93/1000 | Loss: 0.00063744
Iteration 94/1000 | Loss: 0.00004263
Iteration 95/1000 | Loss: 0.00003671
Iteration 96/1000 | Loss: 0.00003349
Iteration 97/1000 | Loss: 0.00082830
Iteration 98/1000 | Loss: 0.00025878
Iteration 99/1000 | Loss: 0.00007845
Iteration 100/1000 | Loss: 0.00003196
Iteration 101/1000 | Loss: 0.00002951
Iteration 102/1000 | Loss: 0.00002792
Iteration 103/1000 | Loss: 0.00044872
Iteration 104/1000 | Loss: 0.00008440
Iteration 105/1000 | Loss: 0.00034427
Iteration 106/1000 | Loss: 0.00047725
Iteration 107/1000 | Loss: 0.00048598
Iteration 108/1000 | Loss: 0.00242583
Iteration 109/1000 | Loss: 0.00087489
Iteration 110/1000 | Loss: 0.00007769
Iteration 111/1000 | Loss: 0.00004308
Iteration 112/1000 | Loss: 0.00002851
Iteration 113/1000 | Loss: 0.00002521
Iteration 114/1000 | Loss: 0.00002352
Iteration 115/1000 | Loss: 0.00002224
Iteration 116/1000 | Loss: 0.00002139
Iteration 117/1000 | Loss: 0.00082192
Iteration 118/1000 | Loss: 0.00006217
Iteration 119/1000 | Loss: 0.00007105
Iteration 120/1000 | Loss: 0.00002216
Iteration 121/1000 | Loss: 0.00002074
Iteration 122/1000 | Loss: 0.00002031
Iteration 123/1000 | Loss: 0.00002008
Iteration 124/1000 | Loss: 0.00001984
Iteration 125/1000 | Loss: 0.00001967
Iteration 126/1000 | Loss: 0.00001949
Iteration 127/1000 | Loss: 0.00001943
Iteration 128/1000 | Loss: 0.00001937
Iteration 129/1000 | Loss: 0.00001937
Iteration 130/1000 | Loss: 0.00001935
Iteration 131/1000 | Loss: 0.00001935
Iteration 132/1000 | Loss: 0.00001934
Iteration 133/1000 | Loss: 0.00001933
Iteration 134/1000 | Loss: 0.00001933
Iteration 135/1000 | Loss: 0.00001931
Iteration 136/1000 | Loss: 0.00001931
Iteration 137/1000 | Loss: 0.00001930
Iteration 138/1000 | Loss: 0.00001928
Iteration 139/1000 | Loss: 0.00001927
Iteration 140/1000 | Loss: 0.00001926
Iteration 141/1000 | Loss: 0.00001926
Iteration 142/1000 | Loss: 0.00001926
Iteration 143/1000 | Loss: 0.00001924
Iteration 144/1000 | Loss: 0.00001923
Iteration 145/1000 | Loss: 0.00001923
Iteration 146/1000 | Loss: 0.00001922
Iteration 147/1000 | Loss: 0.00001921
Iteration 148/1000 | Loss: 0.00001918
Iteration 149/1000 | Loss: 0.00001917
Iteration 150/1000 | Loss: 0.00001917
Iteration 151/1000 | Loss: 0.00001917
Iteration 152/1000 | Loss: 0.00001917
Iteration 153/1000 | Loss: 0.00001917
Iteration 154/1000 | Loss: 0.00001917
Iteration 155/1000 | Loss: 0.00001916
Iteration 156/1000 | Loss: 0.00001916
Iteration 157/1000 | Loss: 0.00001916
Iteration 158/1000 | Loss: 0.00001915
Iteration 159/1000 | Loss: 0.00001915
Iteration 160/1000 | Loss: 0.00001915
Iteration 161/1000 | Loss: 0.00001915
Iteration 162/1000 | Loss: 0.00001914
Iteration 163/1000 | Loss: 0.00001914
Iteration 164/1000 | Loss: 0.00001914
Iteration 165/1000 | Loss: 0.00001914
Iteration 166/1000 | Loss: 0.00001913
Iteration 167/1000 | Loss: 0.00001913
Iteration 168/1000 | Loss: 0.00001913
Iteration 169/1000 | Loss: 0.00001912
Iteration 170/1000 | Loss: 0.00001912
Iteration 171/1000 | Loss: 0.00001912
Iteration 172/1000 | Loss: 0.00001912
Iteration 173/1000 | Loss: 0.00001911
Iteration 174/1000 | Loss: 0.00001911
Iteration 175/1000 | Loss: 0.00001911
Iteration 176/1000 | Loss: 0.00001911
Iteration 177/1000 | Loss: 0.00001911
Iteration 178/1000 | Loss: 0.00001911
Iteration 179/1000 | Loss: 0.00001910
Iteration 180/1000 | Loss: 0.00001910
Iteration 181/1000 | Loss: 0.00001910
Iteration 182/1000 | Loss: 0.00001910
Iteration 183/1000 | Loss: 0.00001910
Iteration 184/1000 | Loss: 0.00001910
Iteration 185/1000 | Loss: 0.00001910
Iteration 186/1000 | Loss: 0.00001910
Iteration 187/1000 | Loss: 0.00001910
Iteration 188/1000 | Loss: 0.00001909
Iteration 189/1000 | Loss: 0.00001909
Iteration 190/1000 | Loss: 0.00001909
Iteration 191/1000 | Loss: 0.00001909
Iteration 192/1000 | Loss: 0.00001909
Iteration 193/1000 | Loss: 0.00001909
Iteration 194/1000 | Loss: 0.00001909
Iteration 195/1000 | Loss: 0.00001909
Iteration 196/1000 | Loss: 0.00001909
Iteration 197/1000 | Loss: 0.00001909
Iteration 198/1000 | Loss: 0.00001909
Iteration 199/1000 | Loss: 0.00001909
Iteration 200/1000 | Loss: 0.00001908
Iteration 201/1000 | Loss: 0.00001908
Iteration 202/1000 | Loss: 0.00001908
Iteration 203/1000 | Loss: 0.00001908
Iteration 204/1000 | Loss: 0.00001908
Iteration 205/1000 | Loss: 0.00001908
Iteration 206/1000 | Loss: 0.00001908
Iteration 207/1000 | Loss: 0.00001908
Iteration 208/1000 | Loss: 0.00001908
Iteration 209/1000 | Loss: 0.00001908
Iteration 210/1000 | Loss: 0.00001908
Iteration 211/1000 | Loss: 0.00001908
Iteration 212/1000 | Loss: 0.00001908
Iteration 213/1000 | Loss: 0.00001908
Iteration 214/1000 | Loss: 0.00001908
Iteration 215/1000 | Loss: 0.00001908
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 215. Stopping optimization.
Last 5 losses: [1.9078044715570286e-05, 1.9078044715570286e-05, 1.9078044715570286e-05, 1.9078044715570286e-05, 1.9078044715570286e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9078044715570286e-05

Optimization complete. Final v2v error: 3.647368907928467 mm

Highest mean error: 5.705748081207275 mm for frame 77

Lowest mean error: 3.0829360485076904 mm for frame 193

Saving results

Total time: 237.31633496284485
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_006/1033/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_006/1033.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_006/1033
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00710111
Iteration 2/25 | Loss: 0.00103034
Iteration 3/25 | Loss: 0.00080915
Iteration 4/25 | Loss: 0.00073841
Iteration 5/25 | Loss: 0.00072978
Iteration 6/25 | Loss: 0.00072847
Iteration 7/25 | Loss: 0.00072847
Iteration 8/25 | Loss: 0.00072847
Iteration 9/25 | Loss: 0.00072847
Iteration 10/25 | Loss: 0.00072847
Iteration 11/25 | Loss: 0.00072847
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0007284720777533948, 0.0007284720777533948, 0.0007284720777533948, 0.0007284720777533948, 0.0007284720777533948]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007284720777533948

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 6.19369888
Iteration 2/25 | Loss: 0.00051248
Iteration 3/25 | Loss: 0.00051242
Iteration 4/25 | Loss: 0.00051242
Iteration 5/25 | Loss: 0.00051242
Iteration 6/25 | Loss: 0.00051242
Iteration 7/25 | Loss: 0.00051241
Iteration 8/25 | Loss: 0.00051241
Iteration 9/25 | Loss: 0.00051241
Iteration 10/25 | Loss: 0.00051241
Iteration 11/25 | Loss: 0.00051241
Iteration 12/25 | Loss: 0.00051241
Iteration 13/25 | Loss: 0.00051241
Iteration 14/25 | Loss: 0.00051241
Iteration 15/25 | Loss: 0.00051241
Iteration 16/25 | Loss: 0.00051241
Iteration 17/25 | Loss: 0.00051241
Iteration 18/25 | Loss: 0.00051241
Iteration 19/25 | Loss: 0.00051241
Iteration 20/25 | Loss: 0.00051241
Iteration 21/25 | Loss: 0.00051241
Iteration 22/25 | Loss: 0.00051241
Iteration 23/25 | Loss: 0.00051241
Iteration 24/25 | Loss: 0.00051241
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0005124133313074708, 0.0005124133313074708, 0.0005124133313074708, 0.0005124133313074708, 0.0005124133313074708]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005124133313074708

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00051241
Iteration 2/1000 | Loss: 0.00003598
Iteration 3/1000 | Loss: 0.00002365
Iteration 4/1000 | Loss: 0.00002046
Iteration 5/1000 | Loss: 0.00001905
Iteration 6/1000 | Loss: 0.00001817
Iteration 7/1000 | Loss: 0.00001746
Iteration 8/1000 | Loss: 0.00001700
Iteration 9/1000 | Loss: 0.00001664
Iteration 10/1000 | Loss: 0.00001636
Iteration 11/1000 | Loss: 0.00001613
Iteration 12/1000 | Loss: 0.00001591
Iteration 13/1000 | Loss: 0.00001573
Iteration 14/1000 | Loss: 0.00001564
Iteration 15/1000 | Loss: 0.00001562
Iteration 16/1000 | Loss: 0.00001562
Iteration 17/1000 | Loss: 0.00001562
Iteration 18/1000 | Loss: 0.00001557
Iteration 19/1000 | Loss: 0.00001556
Iteration 20/1000 | Loss: 0.00001556
Iteration 21/1000 | Loss: 0.00001556
Iteration 22/1000 | Loss: 0.00001555
Iteration 23/1000 | Loss: 0.00001554
Iteration 24/1000 | Loss: 0.00001552
Iteration 25/1000 | Loss: 0.00001551
Iteration 26/1000 | Loss: 0.00001551
Iteration 27/1000 | Loss: 0.00001551
Iteration 28/1000 | Loss: 0.00001551
Iteration 29/1000 | Loss: 0.00001549
Iteration 30/1000 | Loss: 0.00001548
Iteration 31/1000 | Loss: 0.00001548
Iteration 32/1000 | Loss: 0.00001548
Iteration 33/1000 | Loss: 0.00001548
Iteration 34/1000 | Loss: 0.00001548
Iteration 35/1000 | Loss: 0.00001547
Iteration 36/1000 | Loss: 0.00001547
Iteration 37/1000 | Loss: 0.00001547
Iteration 38/1000 | Loss: 0.00001546
Iteration 39/1000 | Loss: 0.00001546
Iteration 40/1000 | Loss: 0.00001545
Iteration 41/1000 | Loss: 0.00001545
Iteration 42/1000 | Loss: 0.00001545
Iteration 43/1000 | Loss: 0.00001545
Iteration 44/1000 | Loss: 0.00001545
Iteration 45/1000 | Loss: 0.00001544
Iteration 46/1000 | Loss: 0.00001544
Iteration 47/1000 | Loss: 0.00001543
Iteration 48/1000 | Loss: 0.00001541
Iteration 49/1000 | Loss: 0.00001541
Iteration 50/1000 | Loss: 0.00001541
Iteration 51/1000 | Loss: 0.00001541
Iteration 52/1000 | Loss: 0.00001541
Iteration 53/1000 | Loss: 0.00001541
Iteration 54/1000 | Loss: 0.00001541
Iteration 55/1000 | Loss: 0.00001541
Iteration 56/1000 | Loss: 0.00001541
Iteration 57/1000 | Loss: 0.00001540
Iteration 58/1000 | Loss: 0.00001539
Iteration 59/1000 | Loss: 0.00001539
Iteration 60/1000 | Loss: 0.00001537
Iteration 61/1000 | Loss: 0.00001537
Iteration 62/1000 | Loss: 0.00001537
Iteration 63/1000 | Loss: 0.00001537
Iteration 64/1000 | Loss: 0.00001537
Iteration 65/1000 | Loss: 0.00001537
Iteration 66/1000 | Loss: 0.00001537
Iteration 67/1000 | Loss: 0.00001537
Iteration 68/1000 | Loss: 0.00001537
Iteration 69/1000 | Loss: 0.00001537
Iteration 70/1000 | Loss: 0.00001537
Iteration 71/1000 | Loss: 0.00001536
Iteration 72/1000 | Loss: 0.00001535
Iteration 73/1000 | Loss: 0.00001535
Iteration 74/1000 | Loss: 0.00001535
Iteration 75/1000 | Loss: 0.00001535
Iteration 76/1000 | Loss: 0.00001535
Iteration 77/1000 | Loss: 0.00001535
Iteration 78/1000 | Loss: 0.00001534
Iteration 79/1000 | Loss: 0.00001534
Iteration 80/1000 | Loss: 0.00001534
Iteration 81/1000 | Loss: 0.00001534
Iteration 82/1000 | Loss: 0.00001534
Iteration 83/1000 | Loss: 0.00001533
Iteration 84/1000 | Loss: 0.00001533
Iteration 85/1000 | Loss: 0.00001533
Iteration 86/1000 | Loss: 0.00001533
Iteration 87/1000 | Loss: 0.00001533
Iteration 88/1000 | Loss: 0.00001533
Iteration 89/1000 | Loss: 0.00001533
Iteration 90/1000 | Loss: 0.00001533
Iteration 91/1000 | Loss: 0.00001533
Iteration 92/1000 | Loss: 0.00001533
Iteration 93/1000 | Loss: 0.00001533
Iteration 94/1000 | Loss: 0.00001533
Iteration 95/1000 | Loss: 0.00001533
Iteration 96/1000 | Loss: 0.00001533
Iteration 97/1000 | Loss: 0.00001533
Iteration 98/1000 | Loss: 0.00001533
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 98. Stopping optimization.
Last 5 losses: [1.532921669422649e-05, 1.532921669422649e-05, 1.532921669422649e-05, 1.532921669422649e-05, 1.532921669422649e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.532921669422649e-05

Optimization complete. Final v2v error: 3.3651185035705566 mm

Highest mean error: 3.8142101764678955 mm for frame 22

Lowest mean error: 3.1474876403808594 mm for frame 108

Saving results

Total time: 40.484469175338745
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_006/1005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_006/1005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_006/1005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00499070
Iteration 2/25 | Loss: 0.00101879
Iteration 3/25 | Loss: 0.00079771
Iteration 4/25 | Loss: 0.00075990
Iteration 5/25 | Loss: 0.00074754
Iteration 6/25 | Loss: 0.00074509
Iteration 7/25 | Loss: 0.00074509
Iteration 8/25 | Loss: 0.00074509
Iteration 9/25 | Loss: 0.00074509
Iteration 10/25 | Loss: 0.00074509
Iteration 11/25 | Loss: 0.00074509
Iteration 12/25 | Loss: 0.00074509
Iteration 13/25 | Loss: 0.00074509
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.000745090190321207, 0.000745090190321207, 0.000745090190321207, 0.000745090190321207, 0.000745090190321207]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000745090190321207

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.84213346
Iteration 2/25 | Loss: 0.00035734
Iteration 3/25 | Loss: 0.00035734
Iteration 4/25 | Loss: 0.00035734
Iteration 5/25 | Loss: 0.00035734
Iteration 6/25 | Loss: 0.00035734
Iteration 7/25 | Loss: 0.00035734
Iteration 8/25 | Loss: 0.00035734
Iteration 9/25 | Loss: 0.00035734
Iteration 10/25 | Loss: 0.00035734
Iteration 11/25 | Loss: 0.00035734
Iteration 12/25 | Loss: 0.00035734
Iteration 13/25 | Loss: 0.00035734
Iteration 14/25 | Loss: 0.00035734
Iteration 15/25 | Loss: 0.00035734
Iteration 16/25 | Loss: 0.00035734
Iteration 17/25 | Loss: 0.00035734
Iteration 18/25 | Loss: 0.00035734
Iteration 19/25 | Loss: 0.00035734
Iteration 20/25 | Loss: 0.00035734
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0003573405556380749, 0.0003573405556380749, 0.0003573405556380749, 0.0003573405556380749, 0.0003573405556380749]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0003573405556380749

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00035734
Iteration 2/1000 | Loss: 0.00003469
Iteration 3/1000 | Loss: 0.00002331
Iteration 4/1000 | Loss: 0.00002182
Iteration 5/1000 | Loss: 0.00002102
Iteration 6/1000 | Loss: 0.00002005
Iteration 7/1000 | Loss: 0.00001949
Iteration 8/1000 | Loss: 0.00001895
Iteration 9/1000 | Loss: 0.00001854
Iteration 10/1000 | Loss: 0.00001834
Iteration 11/1000 | Loss: 0.00001814
Iteration 12/1000 | Loss: 0.00001797
Iteration 13/1000 | Loss: 0.00001778
Iteration 14/1000 | Loss: 0.00001761
Iteration 15/1000 | Loss: 0.00001751
Iteration 16/1000 | Loss: 0.00001747
Iteration 17/1000 | Loss: 0.00001747
Iteration 18/1000 | Loss: 0.00001742
Iteration 19/1000 | Loss: 0.00001742
Iteration 20/1000 | Loss: 0.00001738
Iteration 21/1000 | Loss: 0.00001736
Iteration 22/1000 | Loss: 0.00001736
Iteration 23/1000 | Loss: 0.00001733
Iteration 24/1000 | Loss: 0.00001733
Iteration 25/1000 | Loss: 0.00001732
Iteration 26/1000 | Loss: 0.00001726
Iteration 27/1000 | Loss: 0.00001726
Iteration 28/1000 | Loss: 0.00001726
Iteration 29/1000 | Loss: 0.00001722
Iteration 30/1000 | Loss: 0.00001721
Iteration 31/1000 | Loss: 0.00001720
Iteration 32/1000 | Loss: 0.00001720
Iteration 33/1000 | Loss: 0.00001719
Iteration 34/1000 | Loss: 0.00001719
Iteration 35/1000 | Loss: 0.00001719
Iteration 36/1000 | Loss: 0.00001718
Iteration 37/1000 | Loss: 0.00001718
Iteration 38/1000 | Loss: 0.00001718
Iteration 39/1000 | Loss: 0.00001717
Iteration 40/1000 | Loss: 0.00001717
Iteration 41/1000 | Loss: 0.00001717
Iteration 42/1000 | Loss: 0.00001716
Iteration 43/1000 | Loss: 0.00001716
Iteration 44/1000 | Loss: 0.00001716
Iteration 45/1000 | Loss: 0.00001716
Iteration 46/1000 | Loss: 0.00001715
Iteration 47/1000 | Loss: 0.00001715
Iteration 48/1000 | Loss: 0.00001714
Iteration 49/1000 | Loss: 0.00001714
Iteration 50/1000 | Loss: 0.00001714
Iteration 51/1000 | Loss: 0.00001714
Iteration 52/1000 | Loss: 0.00001714
Iteration 53/1000 | Loss: 0.00001714
Iteration 54/1000 | Loss: 0.00001714
Iteration 55/1000 | Loss: 0.00001713
Iteration 56/1000 | Loss: 0.00001713
Iteration 57/1000 | Loss: 0.00001712
Iteration 58/1000 | Loss: 0.00001712
Iteration 59/1000 | Loss: 0.00001711
Iteration 60/1000 | Loss: 0.00001711
Iteration 61/1000 | Loss: 0.00001711
Iteration 62/1000 | Loss: 0.00001711
Iteration 63/1000 | Loss: 0.00001711
Iteration 64/1000 | Loss: 0.00001711
Iteration 65/1000 | Loss: 0.00001711
Iteration 66/1000 | Loss: 0.00001711
Iteration 67/1000 | Loss: 0.00001711
Iteration 68/1000 | Loss: 0.00001711
Iteration 69/1000 | Loss: 0.00001711
Iteration 70/1000 | Loss: 0.00001711
Iteration 71/1000 | Loss: 0.00001711
Iteration 72/1000 | Loss: 0.00001710
Iteration 73/1000 | Loss: 0.00001710
Iteration 74/1000 | Loss: 0.00001710
Iteration 75/1000 | Loss: 0.00001709
Iteration 76/1000 | Loss: 0.00001709
Iteration 77/1000 | Loss: 0.00001709
Iteration 78/1000 | Loss: 0.00001708
Iteration 79/1000 | Loss: 0.00001708
Iteration 80/1000 | Loss: 0.00001708
Iteration 81/1000 | Loss: 0.00001707
Iteration 82/1000 | Loss: 0.00001707
Iteration 83/1000 | Loss: 0.00001707
Iteration 84/1000 | Loss: 0.00001707
Iteration 85/1000 | Loss: 0.00001707
Iteration 86/1000 | Loss: 0.00001707
Iteration 87/1000 | Loss: 0.00001707
Iteration 88/1000 | Loss: 0.00001707
Iteration 89/1000 | Loss: 0.00001707
Iteration 90/1000 | Loss: 0.00001707
Iteration 91/1000 | Loss: 0.00001707
Iteration 92/1000 | Loss: 0.00001707
Iteration 93/1000 | Loss: 0.00001706
Iteration 94/1000 | Loss: 0.00001706
Iteration 95/1000 | Loss: 0.00001706
Iteration 96/1000 | Loss: 0.00001706
Iteration 97/1000 | Loss: 0.00001706
Iteration 98/1000 | Loss: 0.00001706
Iteration 99/1000 | Loss: 0.00001706
Iteration 100/1000 | Loss: 0.00001706
Iteration 101/1000 | Loss: 0.00001706
Iteration 102/1000 | Loss: 0.00001706
Iteration 103/1000 | Loss: 0.00001706
Iteration 104/1000 | Loss: 0.00001706
Iteration 105/1000 | Loss: 0.00001705
Iteration 106/1000 | Loss: 0.00001705
Iteration 107/1000 | Loss: 0.00001705
Iteration 108/1000 | Loss: 0.00001705
Iteration 109/1000 | Loss: 0.00001705
Iteration 110/1000 | Loss: 0.00001705
Iteration 111/1000 | Loss: 0.00001705
Iteration 112/1000 | Loss: 0.00001705
Iteration 113/1000 | Loss: 0.00001705
Iteration 114/1000 | Loss: 0.00001705
Iteration 115/1000 | Loss: 0.00001705
Iteration 116/1000 | Loss: 0.00001704
Iteration 117/1000 | Loss: 0.00001704
Iteration 118/1000 | Loss: 0.00001704
Iteration 119/1000 | Loss: 0.00001704
Iteration 120/1000 | Loss: 0.00001704
Iteration 121/1000 | Loss: 0.00001704
Iteration 122/1000 | Loss: 0.00001704
Iteration 123/1000 | Loss: 0.00001704
Iteration 124/1000 | Loss: 0.00001704
Iteration 125/1000 | Loss: 0.00001703
Iteration 126/1000 | Loss: 0.00001703
Iteration 127/1000 | Loss: 0.00001703
Iteration 128/1000 | Loss: 0.00001703
Iteration 129/1000 | Loss: 0.00001703
Iteration 130/1000 | Loss: 0.00001703
Iteration 131/1000 | Loss: 0.00001703
Iteration 132/1000 | Loss: 0.00001703
Iteration 133/1000 | Loss: 0.00001702
Iteration 134/1000 | Loss: 0.00001702
Iteration 135/1000 | Loss: 0.00001702
Iteration 136/1000 | Loss: 0.00001702
Iteration 137/1000 | Loss: 0.00001702
Iteration 138/1000 | Loss: 0.00001702
Iteration 139/1000 | Loss: 0.00001702
Iteration 140/1000 | Loss: 0.00001702
Iteration 141/1000 | Loss: 0.00001702
Iteration 142/1000 | Loss: 0.00001701
Iteration 143/1000 | Loss: 0.00001701
Iteration 144/1000 | Loss: 0.00001701
Iteration 145/1000 | Loss: 0.00001701
Iteration 146/1000 | Loss: 0.00001701
Iteration 147/1000 | Loss: 0.00001700
Iteration 148/1000 | Loss: 0.00001700
Iteration 149/1000 | Loss: 0.00001700
Iteration 150/1000 | Loss: 0.00001700
Iteration 151/1000 | Loss: 0.00001700
Iteration 152/1000 | Loss: 0.00001700
Iteration 153/1000 | Loss: 0.00001700
Iteration 154/1000 | Loss: 0.00001700
Iteration 155/1000 | Loss: 0.00001700
Iteration 156/1000 | Loss: 0.00001700
Iteration 157/1000 | Loss: 0.00001699
Iteration 158/1000 | Loss: 0.00001699
Iteration 159/1000 | Loss: 0.00001699
Iteration 160/1000 | Loss: 0.00001699
Iteration 161/1000 | Loss: 0.00001699
Iteration 162/1000 | Loss: 0.00001699
Iteration 163/1000 | Loss: 0.00001699
Iteration 164/1000 | Loss: 0.00001699
Iteration 165/1000 | Loss: 0.00001699
Iteration 166/1000 | Loss: 0.00001699
Iteration 167/1000 | Loss: 0.00001699
Iteration 168/1000 | Loss: 0.00001699
Iteration 169/1000 | Loss: 0.00001699
Iteration 170/1000 | Loss: 0.00001699
Iteration 171/1000 | Loss: 0.00001699
Iteration 172/1000 | Loss: 0.00001699
Iteration 173/1000 | Loss: 0.00001699
Iteration 174/1000 | Loss: 0.00001699
Iteration 175/1000 | Loss: 0.00001699
Iteration 176/1000 | Loss: 0.00001699
Iteration 177/1000 | Loss: 0.00001699
Iteration 178/1000 | Loss: 0.00001699
Iteration 179/1000 | Loss: 0.00001699
Iteration 180/1000 | Loss: 0.00001699
Iteration 181/1000 | Loss: 0.00001699
Iteration 182/1000 | Loss: 0.00001699
Iteration 183/1000 | Loss: 0.00001699
Iteration 184/1000 | Loss: 0.00001699
Iteration 185/1000 | Loss: 0.00001699
Iteration 186/1000 | Loss: 0.00001699
Iteration 187/1000 | Loss: 0.00001699
Iteration 188/1000 | Loss: 0.00001699
Iteration 189/1000 | Loss: 0.00001699
Iteration 190/1000 | Loss: 0.00001699
Iteration 191/1000 | Loss: 0.00001699
Iteration 192/1000 | Loss: 0.00001699
Iteration 193/1000 | Loss: 0.00001699
Iteration 194/1000 | Loss: 0.00001699
Iteration 195/1000 | Loss: 0.00001699
Iteration 196/1000 | Loss: 0.00001699
Iteration 197/1000 | Loss: 0.00001699
Iteration 198/1000 | Loss: 0.00001699
Iteration 199/1000 | Loss: 0.00001699
Iteration 200/1000 | Loss: 0.00001699
Iteration 201/1000 | Loss: 0.00001699
Iteration 202/1000 | Loss: 0.00001699
Iteration 203/1000 | Loss: 0.00001699
Iteration 204/1000 | Loss: 0.00001699
Iteration 205/1000 | Loss: 0.00001699
Iteration 206/1000 | Loss: 0.00001699
Iteration 207/1000 | Loss: 0.00001699
Iteration 208/1000 | Loss: 0.00001699
Iteration 209/1000 | Loss: 0.00001699
Iteration 210/1000 | Loss: 0.00001699
Iteration 211/1000 | Loss: 0.00001699
Iteration 212/1000 | Loss: 0.00001699
Iteration 213/1000 | Loss: 0.00001699
Iteration 214/1000 | Loss: 0.00001699
Iteration 215/1000 | Loss: 0.00001699
Iteration 216/1000 | Loss: 0.00001699
Iteration 217/1000 | Loss: 0.00001699
Iteration 218/1000 | Loss: 0.00001699
Iteration 219/1000 | Loss: 0.00001699
Iteration 220/1000 | Loss: 0.00001699
Iteration 221/1000 | Loss: 0.00001699
Iteration 222/1000 | Loss: 0.00001699
Iteration 223/1000 | Loss: 0.00001699
Iteration 224/1000 | Loss: 0.00001699
Iteration 225/1000 | Loss: 0.00001699
Iteration 226/1000 | Loss: 0.00001699
Iteration 227/1000 | Loss: 0.00001699
Iteration 228/1000 | Loss: 0.00001699
Iteration 229/1000 | Loss: 0.00001699
Iteration 230/1000 | Loss: 0.00001699
Iteration 231/1000 | Loss: 0.00001699
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 231. Stopping optimization.
Last 5 losses: [1.698684172879439e-05, 1.698684172879439e-05, 1.698684172879439e-05, 1.698684172879439e-05, 1.698684172879439e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.698684172879439e-05

Optimization complete. Final v2v error: 3.4896039962768555 mm

Highest mean error: 4.387465000152588 mm for frame 2

Lowest mean error: 3.241309404373169 mm for frame 35

Saving results

Total time: 52.71758484840393
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_006/1043/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_006/1043.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_006/1043
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01068004
Iteration 2/25 | Loss: 0.00199996
Iteration 3/25 | Loss: 0.00145240
Iteration 4/25 | Loss: 0.00141240
Iteration 5/25 | Loss: 0.00112004
Iteration 6/25 | Loss: 0.00105152
Iteration 7/25 | Loss: 0.00099561
Iteration 8/25 | Loss: 0.00095348
Iteration 9/25 | Loss: 0.00092933
Iteration 10/25 | Loss: 0.00092910
Iteration 11/25 | Loss: 0.00089402
Iteration 12/25 | Loss: 0.00084811
Iteration 13/25 | Loss: 0.00083882
Iteration 14/25 | Loss: 0.00081729
Iteration 15/25 | Loss: 0.00080680
Iteration 16/25 | Loss: 0.00080229
Iteration 17/25 | Loss: 0.00080436
Iteration 18/25 | Loss: 0.00079912
Iteration 19/25 | Loss: 0.00079683
Iteration 20/25 | Loss: 0.00080242
Iteration 21/25 | Loss: 0.00079662
Iteration 22/25 | Loss: 0.00079515
Iteration 23/25 | Loss: 0.00079521
Iteration 24/25 | Loss: 0.00079428
Iteration 25/25 | Loss: 0.00079417

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.69576037
Iteration 2/25 | Loss: 0.00070353
Iteration 3/25 | Loss: 0.00070353
Iteration 4/25 | Loss: 0.00070353
Iteration 5/25 | Loss: 0.00070353
Iteration 6/25 | Loss: 0.00070353
Iteration 7/25 | Loss: 0.00070353
Iteration 8/25 | Loss: 0.00070353
Iteration 9/25 | Loss: 0.00070353
Iteration 10/25 | Loss: 0.00070353
Iteration 11/25 | Loss: 0.00070353
Iteration 12/25 | Loss: 0.00070353
Iteration 13/25 | Loss: 0.00070353
Iteration 14/25 | Loss: 0.00070353
Iteration 15/25 | Loss: 0.00070353
Iteration 16/25 | Loss: 0.00070353
Iteration 17/25 | Loss: 0.00070353
Iteration 18/25 | Loss: 0.00070353
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0007035277085378766, 0.0007035277085378766, 0.0007035277085378766, 0.0007035277085378766, 0.0007035277085378766]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007035277085378766

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00070353
Iteration 2/1000 | Loss: 0.00006819
Iteration 3/1000 | Loss: 0.00005629
Iteration 4/1000 | Loss: 0.00005934
Iteration 5/1000 | Loss: 0.00007316
Iteration 6/1000 | Loss: 0.00003910
Iteration 7/1000 | Loss: 0.00005964
Iteration 8/1000 | Loss: 0.00004719
Iteration 9/1000 | Loss: 0.00004245
Iteration 10/1000 | Loss: 0.00004209
Iteration 11/1000 | Loss: 0.00004201
Iteration 12/1000 | Loss: 0.00004456
Iteration 13/1000 | Loss: 0.00004217
Iteration 14/1000 | Loss: 0.00093301
Iteration 15/1000 | Loss: 0.00012259
Iteration 16/1000 | Loss: 0.00005109
Iteration 17/1000 | Loss: 0.00003296
Iteration 18/1000 | Loss: 0.00002452
Iteration 19/1000 | Loss: 0.00002075
Iteration 20/1000 | Loss: 0.00001838
Iteration 21/1000 | Loss: 0.00001742
Iteration 22/1000 | Loss: 0.00001681
Iteration 23/1000 | Loss: 0.00001642
Iteration 24/1000 | Loss: 0.00001609
Iteration 25/1000 | Loss: 0.00001577
Iteration 26/1000 | Loss: 0.00001554
Iteration 27/1000 | Loss: 0.00001543
Iteration 28/1000 | Loss: 0.00001534
Iteration 29/1000 | Loss: 0.00001530
Iteration 30/1000 | Loss: 0.00001528
Iteration 31/1000 | Loss: 0.00001523
Iteration 32/1000 | Loss: 0.00001523
Iteration 33/1000 | Loss: 0.00001519
Iteration 34/1000 | Loss: 0.00001519
Iteration 35/1000 | Loss: 0.00001519
Iteration 36/1000 | Loss: 0.00001519
Iteration 37/1000 | Loss: 0.00001519
Iteration 38/1000 | Loss: 0.00001519
Iteration 39/1000 | Loss: 0.00001518
Iteration 40/1000 | Loss: 0.00001518
Iteration 41/1000 | Loss: 0.00001517
Iteration 42/1000 | Loss: 0.00001517
Iteration 43/1000 | Loss: 0.00001516
Iteration 44/1000 | Loss: 0.00001516
Iteration 45/1000 | Loss: 0.00001515
Iteration 46/1000 | Loss: 0.00001515
Iteration 47/1000 | Loss: 0.00001515
Iteration 48/1000 | Loss: 0.00001515
Iteration 49/1000 | Loss: 0.00001515
Iteration 50/1000 | Loss: 0.00001515
Iteration 51/1000 | Loss: 0.00001515
Iteration 52/1000 | Loss: 0.00001514
Iteration 53/1000 | Loss: 0.00001514
Iteration 54/1000 | Loss: 0.00001513
Iteration 55/1000 | Loss: 0.00001513
Iteration 56/1000 | Loss: 0.00001513
Iteration 57/1000 | Loss: 0.00001512
Iteration 58/1000 | Loss: 0.00001512
Iteration 59/1000 | Loss: 0.00001512
Iteration 60/1000 | Loss: 0.00001512
Iteration 61/1000 | Loss: 0.00001512
Iteration 62/1000 | Loss: 0.00001511
Iteration 63/1000 | Loss: 0.00001511
Iteration 64/1000 | Loss: 0.00001511
Iteration 65/1000 | Loss: 0.00001511
Iteration 66/1000 | Loss: 0.00001511
Iteration 67/1000 | Loss: 0.00001511
Iteration 68/1000 | Loss: 0.00001511
Iteration 69/1000 | Loss: 0.00001511
Iteration 70/1000 | Loss: 0.00001511
Iteration 71/1000 | Loss: 0.00001511
Iteration 72/1000 | Loss: 0.00001511
Iteration 73/1000 | Loss: 0.00001511
Iteration 74/1000 | Loss: 0.00001511
Iteration 75/1000 | Loss: 0.00001511
Iteration 76/1000 | Loss: 0.00001511
Iteration 77/1000 | Loss: 0.00001511
Iteration 78/1000 | Loss: 0.00001511
Iteration 79/1000 | Loss: 0.00001511
Iteration 80/1000 | Loss: 0.00001511
Iteration 81/1000 | Loss: 0.00001511
Iteration 82/1000 | Loss: 0.00001511
Iteration 83/1000 | Loss: 0.00001511
Iteration 84/1000 | Loss: 0.00001511
Iteration 85/1000 | Loss: 0.00001511
Iteration 86/1000 | Loss: 0.00001511
Iteration 87/1000 | Loss: 0.00001511
Iteration 88/1000 | Loss: 0.00001511
Iteration 89/1000 | Loss: 0.00001511
Iteration 90/1000 | Loss: 0.00001511
Iteration 91/1000 | Loss: 0.00001511
Iteration 92/1000 | Loss: 0.00001511
Iteration 93/1000 | Loss: 0.00001511
Iteration 94/1000 | Loss: 0.00001511
Iteration 95/1000 | Loss: 0.00001511
Iteration 96/1000 | Loss: 0.00001511
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 96. Stopping optimization.
Last 5 losses: [1.5110141248442233e-05, 1.5110141248442233e-05, 1.5110141248442233e-05, 1.5110141248442233e-05, 1.5110141248442233e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5110141248442233e-05

Optimization complete. Final v2v error: 3.294029474258423 mm

Highest mean error: 3.8993585109710693 mm for frame 62

Lowest mean error: 2.875092029571533 mm for frame 89

Saving results

Total time: 87.15063238143921
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_006/1026/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_006/1026.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_006/1026
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00428108
Iteration 2/25 | Loss: 0.00091865
Iteration 3/25 | Loss: 0.00078011
Iteration 4/25 | Loss: 0.00076375
Iteration 5/25 | Loss: 0.00076059
Iteration 6/25 | Loss: 0.00076001
Iteration 7/25 | Loss: 0.00076001
Iteration 8/25 | Loss: 0.00076001
Iteration 9/25 | Loss: 0.00076001
Iteration 10/25 | Loss: 0.00076001
Iteration 11/25 | Loss: 0.00076001
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.00076001335401088, 0.00076001335401088, 0.00076001335401088, 0.00076001335401088, 0.00076001335401088]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00076001335401088

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 6.67610073
Iteration 2/25 | Loss: 0.00048361
Iteration 3/25 | Loss: 0.00048361
Iteration 4/25 | Loss: 0.00048361
Iteration 5/25 | Loss: 0.00048361
Iteration 6/25 | Loss: 0.00048361
Iteration 7/25 | Loss: 0.00048361
Iteration 8/25 | Loss: 0.00048361
Iteration 9/25 | Loss: 0.00048361
Iteration 10/25 | Loss: 0.00048361
Iteration 11/25 | Loss: 0.00048361
Iteration 12/25 | Loss: 0.00048361
Iteration 13/25 | Loss: 0.00048361
Iteration 14/25 | Loss: 0.00048361
Iteration 15/25 | Loss: 0.00048361
Iteration 16/25 | Loss: 0.00048361
Iteration 17/25 | Loss: 0.00048361
Iteration 18/25 | Loss: 0.00048361
Iteration 19/25 | Loss: 0.00048361
Iteration 20/25 | Loss: 0.00048361
Iteration 21/25 | Loss: 0.00048361
Iteration 22/25 | Loss: 0.00048361
Iteration 23/25 | Loss: 0.00048361
Iteration 24/25 | Loss: 0.00048361
Iteration 25/25 | Loss: 0.00048361

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00048361
Iteration 2/1000 | Loss: 0.00002303
Iteration 3/1000 | Loss: 0.00001709
Iteration 4/1000 | Loss: 0.00001604
Iteration 5/1000 | Loss: 0.00001511
Iteration 6/1000 | Loss: 0.00001476
Iteration 7/1000 | Loss: 0.00001450
Iteration 8/1000 | Loss: 0.00001444
Iteration 9/1000 | Loss: 0.00001442
Iteration 10/1000 | Loss: 0.00001427
Iteration 11/1000 | Loss: 0.00001425
Iteration 12/1000 | Loss: 0.00001416
Iteration 13/1000 | Loss: 0.00001416
Iteration 14/1000 | Loss: 0.00001407
Iteration 15/1000 | Loss: 0.00001404
Iteration 16/1000 | Loss: 0.00001403
Iteration 17/1000 | Loss: 0.00001399
Iteration 18/1000 | Loss: 0.00001399
Iteration 19/1000 | Loss: 0.00001398
Iteration 20/1000 | Loss: 0.00001396
Iteration 21/1000 | Loss: 0.00001392
Iteration 22/1000 | Loss: 0.00001391
Iteration 23/1000 | Loss: 0.00001389
Iteration 24/1000 | Loss: 0.00001389
Iteration 25/1000 | Loss: 0.00001389
Iteration 26/1000 | Loss: 0.00001389
Iteration 27/1000 | Loss: 0.00001389
Iteration 28/1000 | Loss: 0.00001389
Iteration 29/1000 | Loss: 0.00001389
Iteration 30/1000 | Loss: 0.00001389
Iteration 31/1000 | Loss: 0.00001389
Iteration 32/1000 | Loss: 0.00001389
Iteration 33/1000 | Loss: 0.00001389
Iteration 34/1000 | Loss: 0.00001388
Iteration 35/1000 | Loss: 0.00001388
Iteration 36/1000 | Loss: 0.00001388
Iteration 37/1000 | Loss: 0.00001388
Iteration 38/1000 | Loss: 0.00001386
Iteration 39/1000 | Loss: 0.00001386
Iteration 40/1000 | Loss: 0.00001386
Iteration 41/1000 | Loss: 0.00001386
Iteration 42/1000 | Loss: 0.00001386
Iteration 43/1000 | Loss: 0.00001386
Iteration 44/1000 | Loss: 0.00001386
Iteration 45/1000 | Loss: 0.00001386
Iteration 46/1000 | Loss: 0.00001386
Iteration 47/1000 | Loss: 0.00001385
Iteration 48/1000 | Loss: 0.00001385
Iteration 49/1000 | Loss: 0.00001384
Iteration 50/1000 | Loss: 0.00001383
Iteration 51/1000 | Loss: 0.00001383
Iteration 52/1000 | Loss: 0.00001383
Iteration 53/1000 | Loss: 0.00001382
Iteration 54/1000 | Loss: 0.00001382
Iteration 55/1000 | Loss: 0.00001382
Iteration 56/1000 | Loss: 0.00001380
Iteration 57/1000 | Loss: 0.00001380
Iteration 58/1000 | Loss: 0.00001379
Iteration 59/1000 | Loss: 0.00001379
Iteration 60/1000 | Loss: 0.00001379
Iteration 61/1000 | Loss: 0.00001378
Iteration 62/1000 | Loss: 0.00001378
Iteration 63/1000 | Loss: 0.00001378
Iteration 64/1000 | Loss: 0.00001378
Iteration 65/1000 | Loss: 0.00001377
Iteration 66/1000 | Loss: 0.00001377
Iteration 67/1000 | Loss: 0.00001376
Iteration 68/1000 | Loss: 0.00001375
Iteration 69/1000 | Loss: 0.00001375
Iteration 70/1000 | Loss: 0.00001374
Iteration 71/1000 | Loss: 0.00001374
Iteration 72/1000 | Loss: 0.00001373
Iteration 73/1000 | Loss: 0.00001373
Iteration 74/1000 | Loss: 0.00001372
Iteration 75/1000 | Loss: 0.00001370
Iteration 76/1000 | Loss: 0.00001370
Iteration 77/1000 | Loss: 0.00001370
Iteration 78/1000 | Loss: 0.00001370
Iteration 79/1000 | Loss: 0.00001370
Iteration 80/1000 | Loss: 0.00001368
Iteration 81/1000 | Loss: 0.00001368
Iteration 82/1000 | Loss: 0.00001368
Iteration 83/1000 | Loss: 0.00001368
Iteration 84/1000 | Loss: 0.00001368
Iteration 85/1000 | Loss: 0.00001368
Iteration 86/1000 | Loss: 0.00001368
Iteration 87/1000 | Loss: 0.00001368
Iteration 88/1000 | Loss: 0.00001367
Iteration 89/1000 | Loss: 0.00001367
Iteration 90/1000 | Loss: 0.00001367
Iteration 91/1000 | Loss: 0.00001367
Iteration 92/1000 | Loss: 0.00001367
Iteration 93/1000 | Loss: 0.00001367
Iteration 94/1000 | Loss: 0.00001367
Iteration 95/1000 | Loss: 0.00001366
Iteration 96/1000 | Loss: 0.00001366
Iteration 97/1000 | Loss: 0.00001366
Iteration 98/1000 | Loss: 0.00001366
Iteration 99/1000 | Loss: 0.00001366
Iteration 100/1000 | Loss: 0.00001366
Iteration 101/1000 | Loss: 0.00001366
Iteration 102/1000 | Loss: 0.00001365
Iteration 103/1000 | Loss: 0.00001365
Iteration 104/1000 | Loss: 0.00001365
Iteration 105/1000 | Loss: 0.00001365
Iteration 106/1000 | Loss: 0.00001365
Iteration 107/1000 | Loss: 0.00001365
Iteration 108/1000 | Loss: 0.00001364
Iteration 109/1000 | Loss: 0.00001364
Iteration 110/1000 | Loss: 0.00001364
Iteration 111/1000 | Loss: 0.00001364
Iteration 112/1000 | Loss: 0.00001364
Iteration 113/1000 | Loss: 0.00001364
Iteration 114/1000 | Loss: 0.00001364
Iteration 115/1000 | Loss: 0.00001364
Iteration 116/1000 | Loss: 0.00001364
Iteration 117/1000 | Loss: 0.00001364
Iteration 118/1000 | Loss: 0.00001364
Iteration 119/1000 | Loss: 0.00001364
Iteration 120/1000 | Loss: 0.00001363
Iteration 121/1000 | Loss: 0.00001363
Iteration 122/1000 | Loss: 0.00001363
Iteration 123/1000 | Loss: 0.00001363
Iteration 124/1000 | Loss: 0.00001362
Iteration 125/1000 | Loss: 0.00001362
Iteration 126/1000 | Loss: 0.00001362
Iteration 127/1000 | Loss: 0.00001362
Iteration 128/1000 | Loss: 0.00001362
Iteration 129/1000 | Loss: 0.00001362
Iteration 130/1000 | Loss: 0.00001362
Iteration 131/1000 | Loss: 0.00001362
Iteration 132/1000 | Loss: 0.00001362
Iteration 133/1000 | Loss: 0.00001362
Iteration 134/1000 | Loss: 0.00001361
Iteration 135/1000 | Loss: 0.00001361
Iteration 136/1000 | Loss: 0.00001361
Iteration 137/1000 | Loss: 0.00001361
Iteration 138/1000 | Loss: 0.00001361
Iteration 139/1000 | Loss: 0.00001361
Iteration 140/1000 | Loss: 0.00001360
Iteration 141/1000 | Loss: 0.00001360
Iteration 142/1000 | Loss: 0.00001360
Iteration 143/1000 | Loss: 0.00001360
Iteration 144/1000 | Loss: 0.00001360
Iteration 145/1000 | Loss: 0.00001360
Iteration 146/1000 | Loss: 0.00001360
Iteration 147/1000 | Loss: 0.00001360
Iteration 148/1000 | Loss: 0.00001360
Iteration 149/1000 | Loss: 0.00001360
Iteration 150/1000 | Loss: 0.00001359
Iteration 151/1000 | Loss: 0.00001359
Iteration 152/1000 | Loss: 0.00001359
Iteration 153/1000 | Loss: 0.00001359
Iteration 154/1000 | Loss: 0.00001359
Iteration 155/1000 | Loss: 0.00001359
Iteration 156/1000 | Loss: 0.00001359
Iteration 157/1000 | Loss: 0.00001359
Iteration 158/1000 | Loss: 0.00001359
Iteration 159/1000 | Loss: 0.00001359
Iteration 160/1000 | Loss: 0.00001359
Iteration 161/1000 | Loss: 0.00001359
Iteration 162/1000 | Loss: 0.00001359
Iteration 163/1000 | Loss: 0.00001359
Iteration 164/1000 | Loss: 0.00001359
Iteration 165/1000 | Loss: 0.00001359
Iteration 166/1000 | Loss: 0.00001359
Iteration 167/1000 | Loss: 0.00001359
Iteration 168/1000 | Loss: 0.00001359
Iteration 169/1000 | Loss: 0.00001359
Iteration 170/1000 | Loss: 0.00001359
Iteration 171/1000 | Loss: 0.00001359
Iteration 172/1000 | Loss: 0.00001358
Iteration 173/1000 | Loss: 0.00001358
Iteration 174/1000 | Loss: 0.00001358
Iteration 175/1000 | Loss: 0.00001358
Iteration 176/1000 | Loss: 0.00001358
Iteration 177/1000 | Loss: 0.00001358
Iteration 178/1000 | Loss: 0.00001358
Iteration 179/1000 | Loss: 0.00001358
Iteration 180/1000 | Loss: 0.00001358
Iteration 181/1000 | Loss: 0.00001358
Iteration 182/1000 | Loss: 0.00001358
Iteration 183/1000 | Loss: 0.00001358
Iteration 184/1000 | Loss: 0.00001358
Iteration 185/1000 | Loss: 0.00001358
Iteration 186/1000 | Loss: 0.00001358
Iteration 187/1000 | Loss: 0.00001358
Iteration 188/1000 | Loss: 0.00001358
Iteration 189/1000 | Loss: 0.00001358
Iteration 190/1000 | Loss: 0.00001358
Iteration 191/1000 | Loss: 0.00001358
Iteration 192/1000 | Loss: 0.00001358
Iteration 193/1000 | Loss: 0.00001358
Iteration 194/1000 | Loss: 0.00001358
Iteration 195/1000 | Loss: 0.00001358
Iteration 196/1000 | Loss: 0.00001358
Iteration 197/1000 | Loss: 0.00001358
Iteration 198/1000 | Loss: 0.00001358
Iteration 199/1000 | Loss: 0.00001358
Iteration 200/1000 | Loss: 0.00001358
Iteration 201/1000 | Loss: 0.00001358
Iteration 202/1000 | Loss: 0.00001358
Iteration 203/1000 | Loss: 0.00001358
Iteration 204/1000 | Loss: 0.00001358
Iteration 205/1000 | Loss: 0.00001358
Iteration 206/1000 | Loss: 0.00001358
Iteration 207/1000 | Loss: 0.00001358
Iteration 208/1000 | Loss: 0.00001358
Iteration 209/1000 | Loss: 0.00001358
Iteration 210/1000 | Loss: 0.00001358
Iteration 211/1000 | Loss: 0.00001358
Iteration 212/1000 | Loss: 0.00001358
Iteration 213/1000 | Loss: 0.00001358
Iteration 214/1000 | Loss: 0.00001358
Iteration 215/1000 | Loss: 0.00001358
Iteration 216/1000 | Loss: 0.00001358
Iteration 217/1000 | Loss: 0.00001358
Iteration 218/1000 | Loss: 0.00001358
Iteration 219/1000 | Loss: 0.00001358
Iteration 220/1000 | Loss: 0.00001358
Iteration 221/1000 | Loss: 0.00001358
Iteration 222/1000 | Loss: 0.00001358
Iteration 223/1000 | Loss: 0.00001358
Iteration 224/1000 | Loss: 0.00001358
Iteration 225/1000 | Loss: 0.00001358
Iteration 226/1000 | Loss: 0.00001358
Iteration 227/1000 | Loss: 0.00001358
Iteration 228/1000 | Loss: 0.00001358
Iteration 229/1000 | Loss: 0.00001358
Iteration 230/1000 | Loss: 0.00001358
Iteration 231/1000 | Loss: 0.00001358
Iteration 232/1000 | Loss: 0.00001358
Iteration 233/1000 | Loss: 0.00001358
Iteration 234/1000 | Loss: 0.00001358
Iteration 235/1000 | Loss: 0.00001358
Iteration 236/1000 | Loss: 0.00001358
Iteration 237/1000 | Loss: 0.00001358
Iteration 238/1000 | Loss: 0.00001358
Iteration 239/1000 | Loss: 0.00001358
Iteration 240/1000 | Loss: 0.00001358
Iteration 241/1000 | Loss: 0.00001358
Iteration 242/1000 | Loss: 0.00001358
Iteration 243/1000 | Loss: 0.00001358
Iteration 244/1000 | Loss: 0.00001358
Iteration 245/1000 | Loss: 0.00001358
Iteration 246/1000 | Loss: 0.00001358
Iteration 247/1000 | Loss: 0.00001358
Iteration 248/1000 | Loss: 0.00001358
Iteration 249/1000 | Loss: 0.00001358
Iteration 250/1000 | Loss: 0.00001358
Iteration 251/1000 | Loss: 0.00001358
Iteration 252/1000 | Loss: 0.00001358
Iteration 253/1000 | Loss: 0.00001358
Iteration 254/1000 | Loss: 0.00001358
Iteration 255/1000 | Loss: 0.00001358
Iteration 256/1000 | Loss: 0.00001358
Iteration 257/1000 | Loss: 0.00001358
Iteration 258/1000 | Loss: 0.00001358
Iteration 259/1000 | Loss: 0.00001358
Iteration 260/1000 | Loss: 0.00001358
Iteration 261/1000 | Loss: 0.00001358
Iteration 262/1000 | Loss: 0.00001358
Iteration 263/1000 | Loss: 0.00001358
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 263. Stopping optimization.
Last 5 losses: [1.3582144674728625e-05, 1.3582144674728625e-05, 1.3582144674728625e-05, 1.3582144674728625e-05, 1.3582144674728625e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3582144674728625e-05

Optimization complete. Final v2v error: 3.1243574619293213 mm

Highest mean error: 3.4410009384155273 mm for frame 82

Lowest mean error: 2.952875852584839 mm for frame 27

Saving results

Total time: 37.13334250450134
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_006/1063/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_006/1063.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_006/1063
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00398413
Iteration 2/25 | Loss: 0.00089266
Iteration 3/25 | Loss: 0.00073716
Iteration 4/25 | Loss: 0.00071140
Iteration 5/25 | Loss: 0.00070621
Iteration 6/25 | Loss: 0.00070448
Iteration 7/25 | Loss: 0.00070393
Iteration 8/25 | Loss: 0.00070389
Iteration 9/25 | Loss: 0.00070389
Iteration 10/25 | Loss: 0.00070389
Iteration 11/25 | Loss: 0.00070389
Iteration 12/25 | Loss: 0.00070389
Iteration 13/25 | Loss: 0.00070389
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.000703892670571804, 0.000703892670571804, 0.000703892670571804, 0.000703892670571804, 0.000703892670571804]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000703892670571804

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.50315034
Iteration 2/25 | Loss: 0.00040278
Iteration 3/25 | Loss: 0.00040278
Iteration 4/25 | Loss: 0.00040278
Iteration 5/25 | Loss: 0.00040277
Iteration 6/25 | Loss: 0.00040277
Iteration 7/25 | Loss: 0.00040277
Iteration 8/25 | Loss: 0.00040277
Iteration 9/25 | Loss: 0.00040277
Iteration 10/25 | Loss: 0.00040277
Iteration 11/25 | Loss: 0.00040277
Iteration 12/25 | Loss: 0.00040277
Iteration 13/25 | Loss: 0.00040277
Iteration 14/25 | Loss: 0.00040277
Iteration 15/25 | Loss: 0.00040277
Iteration 16/25 | Loss: 0.00040277
Iteration 17/25 | Loss: 0.00040277
Iteration 18/25 | Loss: 0.00040277
Iteration 19/25 | Loss: 0.00040277
Iteration 20/25 | Loss: 0.00040277
Iteration 21/25 | Loss: 0.00040277
Iteration 22/25 | Loss: 0.00040277
Iteration 23/25 | Loss: 0.00040277
Iteration 24/25 | Loss: 0.00040277
Iteration 25/25 | Loss: 0.00040277

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00040277
Iteration 2/1000 | Loss: 0.00002040
Iteration 3/1000 | Loss: 0.00001254
Iteration 4/1000 | Loss: 0.00001128
Iteration 5/1000 | Loss: 0.00001074
Iteration 6/1000 | Loss: 0.00001033
Iteration 7/1000 | Loss: 0.00001008
Iteration 8/1000 | Loss: 0.00001000
Iteration 9/1000 | Loss: 0.00000989
Iteration 10/1000 | Loss: 0.00000985
Iteration 11/1000 | Loss: 0.00000985
Iteration 12/1000 | Loss: 0.00000984
Iteration 13/1000 | Loss: 0.00000984
Iteration 14/1000 | Loss: 0.00000983
Iteration 15/1000 | Loss: 0.00000983
Iteration 16/1000 | Loss: 0.00000983
Iteration 17/1000 | Loss: 0.00000982
Iteration 18/1000 | Loss: 0.00000982
Iteration 19/1000 | Loss: 0.00000981
Iteration 20/1000 | Loss: 0.00000980
Iteration 21/1000 | Loss: 0.00000980
Iteration 22/1000 | Loss: 0.00000979
Iteration 23/1000 | Loss: 0.00000979
Iteration 24/1000 | Loss: 0.00000979
Iteration 25/1000 | Loss: 0.00000979
Iteration 26/1000 | Loss: 0.00000979
Iteration 27/1000 | Loss: 0.00000979
Iteration 28/1000 | Loss: 0.00000978
Iteration 29/1000 | Loss: 0.00000978
Iteration 30/1000 | Loss: 0.00000977
Iteration 31/1000 | Loss: 0.00000975
Iteration 32/1000 | Loss: 0.00000975
Iteration 33/1000 | Loss: 0.00000974
Iteration 34/1000 | Loss: 0.00000974
Iteration 35/1000 | Loss: 0.00000972
Iteration 36/1000 | Loss: 0.00000971
Iteration 37/1000 | Loss: 0.00000970
Iteration 38/1000 | Loss: 0.00000970
Iteration 39/1000 | Loss: 0.00000970
Iteration 40/1000 | Loss: 0.00000970
Iteration 41/1000 | Loss: 0.00000970
Iteration 42/1000 | Loss: 0.00000969
Iteration 43/1000 | Loss: 0.00000969
Iteration 44/1000 | Loss: 0.00000968
Iteration 45/1000 | Loss: 0.00000968
Iteration 46/1000 | Loss: 0.00000968
Iteration 47/1000 | Loss: 0.00000967
Iteration 48/1000 | Loss: 0.00000967
Iteration 49/1000 | Loss: 0.00000967
Iteration 50/1000 | Loss: 0.00000966
Iteration 51/1000 | Loss: 0.00000966
Iteration 52/1000 | Loss: 0.00000965
Iteration 53/1000 | Loss: 0.00000965
Iteration 54/1000 | Loss: 0.00000965
Iteration 55/1000 | Loss: 0.00000964
Iteration 56/1000 | Loss: 0.00000964
Iteration 57/1000 | Loss: 0.00000964
Iteration 58/1000 | Loss: 0.00000963
Iteration 59/1000 | Loss: 0.00000963
Iteration 60/1000 | Loss: 0.00000962
Iteration 61/1000 | Loss: 0.00000962
Iteration 62/1000 | Loss: 0.00000962
Iteration 63/1000 | Loss: 0.00000961
Iteration 64/1000 | Loss: 0.00000960
Iteration 65/1000 | Loss: 0.00000960
Iteration 66/1000 | Loss: 0.00000959
Iteration 67/1000 | Loss: 0.00000959
Iteration 68/1000 | Loss: 0.00000958
Iteration 69/1000 | Loss: 0.00000958
Iteration 70/1000 | Loss: 0.00000958
Iteration 71/1000 | Loss: 0.00000957
Iteration 72/1000 | Loss: 0.00000957
Iteration 73/1000 | Loss: 0.00000956
Iteration 74/1000 | Loss: 0.00000956
Iteration 75/1000 | Loss: 0.00000955
Iteration 76/1000 | Loss: 0.00000955
Iteration 77/1000 | Loss: 0.00000954
Iteration 78/1000 | Loss: 0.00000954
Iteration 79/1000 | Loss: 0.00000954
Iteration 80/1000 | Loss: 0.00000954
Iteration 81/1000 | Loss: 0.00000953
Iteration 82/1000 | Loss: 0.00000953
Iteration 83/1000 | Loss: 0.00000952
Iteration 84/1000 | Loss: 0.00000952
Iteration 85/1000 | Loss: 0.00000951
Iteration 86/1000 | Loss: 0.00000951
Iteration 87/1000 | Loss: 0.00000951
Iteration 88/1000 | Loss: 0.00000951
Iteration 89/1000 | Loss: 0.00000951
Iteration 90/1000 | Loss: 0.00000951
Iteration 91/1000 | Loss: 0.00000951
Iteration 92/1000 | Loss: 0.00000950
Iteration 93/1000 | Loss: 0.00000950
Iteration 94/1000 | Loss: 0.00000950
Iteration 95/1000 | Loss: 0.00000950
Iteration 96/1000 | Loss: 0.00000950
Iteration 97/1000 | Loss: 0.00000950
Iteration 98/1000 | Loss: 0.00000950
Iteration 99/1000 | Loss: 0.00000950
Iteration 100/1000 | Loss: 0.00000950
Iteration 101/1000 | Loss: 0.00000950
Iteration 102/1000 | Loss: 0.00000950
Iteration 103/1000 | Loss: 0.00000950
Iteration 104/1000 | Loss: 0.00000950
Iteration 105/1000 | Loss: 0.00000950
Iteration 106/1000 | Loss: 0.00000950
Iteration 107/1000 | Loss: 0.00000950
Iteration 108/1000 | Loss: 0.00000950
Iteration 109/1000 | Loss: 0.00000950
Iteration 110/1000 | Loss: 0.00000950
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 110. Stopping optimization.
Last 5 losses: [9.502853572485037e-06, 9.502853572485037e-06, 9.502853572485037e-06, 9.502853572485037e-06, 9.502853572485037e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.502853572485037e-06

Optimization complete. Final v2v error: 2.6090211868286133 mm

Highest mean error: 3.4333536624908447 mm for frame 61

Lowest mean error: 2.435822010040283 mm for frame 84

Saving results

Total time: 30.33544683456421
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_006/1095/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_006/1095.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_006/1095
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00388743
Iteration 2/25 | Loss: 0.00103342
Iteration 3/25 | Loss: 0.00087181
Iteration 4/25 | Loss: 0.00083518
Iteration 5/25 | Loss: 0.00082052
Iteration 6/25 | Loss: 0.00081598
Iteration 7/25 | Loss: 0.00081453
Iteration 8/25 | Loss: 0.00081400
Iteration 9/25 | Loss: 0.00081395
Iteration 10/25 | Loss: 0.00081395
Iteration 11/25 | Loss: 0.00081395
Iteration 12/25 | Loss: 0.00081395
Iteration 13/25 | Loss: 0.00081395
Iteration 14/25 | Loss: 0.00081395
Iteration 15/25 | Loss: 0.00081395
Iteration 16/25 | Loss: 0.00081395
Iteration 17/25 | Loss: 0.00081395
Iteration 18/25 | Loss: 0.00081395
Iteration 19/25 | Loss: 0.00081395
Iteration 20/25 | Loss: 0.00081395
Iteration 21/25 | Loss: 0.00081395
Iteration 22/25 | Loss: 0.00081395
Iteration 23/25 | Loss: 0.00081395
Iteration 24/25 | Loss: 0.00081395
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0008139451965689659, 0.0008139451965689659, 0.0008139451965689659, 0.0008139451965689659, 0.0008139451965689659]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008139451965689659

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.58357954
Iteration 2/25 | Loss: 0.00067979
Iteration 3/25 | Loss: 0.00067979
Iteration 4/25 | Loss: 0.00067979
Iteration 5/25 | Loss: 0.00067979
Iteration 6/25 | Loss: 0.00067979
Iteration 7/25 | Loss: 0.00067979
Iteration 8/25 | Loss: 0.00067979
Iteration 9/25 | Loss: 0.00067979
Iteration 10/25 | Loss: 0.00067979
Iteration 11/25 | Loss: 0.00067979
Iteration 12/25 | Loss: 0.00067979
Iteration 13/25 | Loss: 0.00067979
Iteration 14/25 | Loss: 0.00067979
Iteration 15/25 | Loss: 0.00067979
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0006797854439355433, 0.0006797854439355433, 0.0006797854439355433, 0.0006797854439355433, 0.0006797854439355433]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006797854439355433

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00067979
Iteration 2/1000 | Loss: 0.00004632
Iteration 3/1000 | Loss: 0.00003485
Iteration 4/1000 | Loss: 0.00003076
Iteration 5/1000 | Loss: 0.00002839
Iteration 6/1000 | Loss: 0.00002698
Iteration 7/1000 | Loss: 0.00002606
Iteration 8/1000 | Loss: 0.00002543
Iteration 9/1000 | Loss: 0.00002478
Iteration 10/1000 | Loss: 0.00002442
Iteration 11/1000 | Loss: 0.00002415
Iteration 12/1000 | Loss: 0.00002395
Iteration 13/1000 | Loss: 0.00002374
Iteration 14/1000 | Loss: 0.00002359
Iteration 15/1000 | Loss: 0.00002351
Iteration 16/1000 | Loss: 0.00002336
Iteration 17/1000 | Loss: 0.00002336
Iteration 18/1000 | Loss: 0.00002331
Iteration 19/1000 | Loss: 0.00002330
Iteration 20/1000 | Loss: 0.00002329
Iteration 21/1000 | Loss: 0.00002328
Iteration 22/1000 | Loss: 0.00002328
Iteration 23/1000 | Loss: 0.00002328
Iteration 24/1000 | Loss: 0.00002327
Iteration 25/1000 | Loss: 0.00002326
Iteration 26/1000 | Loss: 0.00002326
Iteration 27/1000 | Loss: 0.00002325
Iteration 28/1000 | Loss: 0.00002325
Iteration 29/1000 | Loss: 0.00002324
Iteration 30/1000 | Loss: 0.00002324
Iteration 31/1000 | Loss: 0.00002323
Iteration 32/1000 | Loss: 0.00002323
Iteration 33/1000 | Loss: 0.00002323
Iteration 34/1000 | Loss: 0.00002322
Iteration 35/1000 | Loss: 0.00002322
Iteration 36/1000 | Loss: 0.00002322
Iteration 37/1000 | Loss: 0.00002321
Iteration 38/1000 | Loss: 0.00002321
Iteration 39/1000 | Loss: 0.00002320
Iteration 40/1000 | Loss: 0.00002320
Iteration 41/1000 | Loss: 0.00002319
Iteration 42/1000 | Loss: 0.00002318
Iteration 43/1000 | Loss: 0.00002318
Iteration 44/1000 | Loss: 0.00002318
Iteration 45/1000 | Loss: 0.00002317
Iteration 46/1000 | Loss: 0.00002317
Iteration 47/1000 | Loss: 0.00002317
Iteration 48/1000 | Loss: 0.00002316
Iteration 49/1000 | Loss: 0.00002316
Iteration 50/1000 | Loss: 0.00002316
Iteration 51/1000 | Loss: 0.00002315
Iteration 52/1000 | Loss: 0.00002315
Iteration 53/1000 | Loss: 0.00002315
Iteration 54/1000 | Loss: 0.00002314
Iteration 55/1000 | Loss: 0.00002314
Iteration 56/1000 | Loss: 0.00002314
Iteration 57/1000 | Loss: 0.00002313
Iteration 58/1000 | Loss: 0.00002313
Iteration 59/1000 | Loss: 0.00002313
Iteration 60/1000 | Loss: 0.00002313
Iteration 61/1000 | Loss: 0.00002313
Iteration 62/1000 | Loss: 0.00002313
Iteration 63/1000 | Loss: 0.00002313
Iteration 64/1000 | Loss: 0.00002312
Iteration 65/1000 | Loss: 0.00002312
Iteration 66/1000 | Loss: 0.00002312
Iteration 67/1000 | Loss: 0.00002311
Iteration 68/1000 | Loss: 0.00002311
Iteration 69/1000 | Loss: 0.00002311
Iteration 70/1000 | Loss: 0.00002311
Iteration 71/1000 | Loss: 0.00002310
Iteration 72/1000 | Loss: 0.00002310
Iteration 73/1000 | Loss: 0.00002310
Iteration 74/1000 | Loss: 0.00002310
Iteration 75/1000 | Loss: 0.00002310
Iteration 76/1000 | Loss: 0.00002309
Iteration 77/1000 | Loss: 0.00002309
Iteration 78/1000 | Loss: 0.00002309
Iteration 79/1000 | Loss: 0.00002309
Iteration 80/1000 | Loss: 0.00002309
Iteration 81/1000 | Loss: 0.00002309
Iteration 82/1000 | Loss: 0.00002308
Iteration 83/1000 | Loss: 0.00002308
Iteration 84/1000 | Loss: 0.00002308
Iteration 85/1000 | Loss: 0.00002308
Iteration 86/1000 | Loss: 0.00002308
Iteration 87/1000 | Loss: 0.00002307
Iteration 88/1000 | Loss: 0.00002307
Iteration 89/1000 | Loss: 0.00002307
Iteration 90/1000 | Loss: 0.00002307
Iteration 91/1000 | Loss: 0.00002307
Iteration 92/1000 | Loss: 0.00002307
Iteration 93/1000 | Loss: 0.00002307
Iteration 94/1000 | Loss: 0.00002307
Iteration 95/1000 | Loss: 0.00002307
Iteration 96/1000 | Loss: 0.00002307
Iteration 97/1000 | Loss: 0.00002307
Iteration 98/1000 | Loss: 0.00002306
Iteration 99/1000 | Loss: 0.00002306
Iteration 100/1000 | Loss: 0.00002306
Iteration 101/1000 | Loss: 0.00002306
Iteration 102/1000 | Loss: 0.00002306
Iteration 103/1000 | Loss: 0.00002306
Iteration 104/1000 | Loss: 0.00002306
Iteration 105/1000 | Loss: 0.00002306
Iteration 106/1000 | Loss: 0.00002306
Iteration 107/1000 | Loss: 0.00002306
Iteration 108/1000 | Loss: 0.00002306
Iteration 109/1000 | Loss: 0.00002306
Iteration 110/1000 | Loss: 0.00002306
Iteration 111/1000 | Loss: 0.00002306
Iteration 112/1000 | Loss: 0.00002306
Iteration 113/1000 | Loss: 0.00002306
Iteration 114/1000 | Loss: 0.00002306
Iteration 115/1000 | Loss: 0.00002306
Iteration 116/1000 | Loss: 0.00002306
Iteration 117/1000 | Loss: 0.00002306
Iteration 118/1000 | Loss: 0.00002306
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 118. Stopping optimization.
Last 5 losses: [2.3058308215695433e-05, 2.3058308215695433e-05, 2.3058308215695433e-05, 2.3058308215695433e-05, 2.3058308215695433e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3058308215695433e-05

Optimization complete. Final v2v error: 3.872915267944336 mm

Highest mean error: 5.300227642059326 mm for frame 106

Lowest mean error: 2.6994621753692627 mm for frame 33

Saving results

Total time: 40.44938015937805
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_006/1088/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_006/1088.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_006/1088
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00967152
Iteration 2/25 | Loss: 0.00298711
Iteration 3/25 | Loss: 0.00204738
Iteration 4/25 | Loss: 0.00183376
Iteration 5/25 | Loss: 0.00177691
Iteration 6/25 | Loss: 0.00151609
Iteration 7/25 | Loss: 0.00133463
Iteration 8/25 | Loss: 0.00118921
Iteration 9/25 | Loss: 0.00113397
Iteration 10/25 | Loss: 0.00101919
Iteration 11/25 | Loss: 0.00098083
Iteration 12/25 | Loss: 0.00095824
Iteration 13/25 | Loss: 0.00094861
Iteration 14/25 | Loss: 0.00094614
Iteration 15/25 | Loss: 0.00094491
Iteration 16/25 | Loss: 0.00094397
Iteration 17/25 | Loss: 0.00094316
Iteration 18/25 | Loss: 0.00094260
Iteration 19/25 | Loss: 0.00094232
Iteration 20/25 | Loss: 0.00094220
Iteration 21/25 | Loss: 0.00094218
Iteration 22/25 | Loss: 0.00094217
Iteration 23/25 | Loss: 0.00094217
Iteration 24/25 | Loss: 0.00094217
Iteration 25/25 | Loss: 0.00094217

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.48762703
Iteration 2/25 | Loss: 0.00153419
Iteration 3/25 | Loss: 0.00153419
Iteration 4/25 | Loss: 0.00153419
Iteration 5/25 | Loss: 0.00153419
Iteration 6/25 | Loss: 0.00153419
Iteration 7/25 | Loss: 0.00153419
Iteration 8/25 | Loss: 0.00153419
Iteration 9/25 | Loss: 0.00153419
Iteration 10/25 | Loss: 0.00153419
Iteration 11/25 | Loss: 0.00153419
Iteration 12/25 | Loss: 0.00153419
Iteration 13/25 | Loss: 0.00153419
Iteration 14/25 | Loss: 0.00153419
Iteration 15/25 | Loss: 0.00153419
Iteration 16/25 | Loss: 0.00153419
Iteration 17/25 | Loss: 0.00153419
Iteration 18/25 | Loss: 0.00153419
Iteration 19/25 | Loss: 0.00153419
Iteration 20/25 | Loss: 0.00153419
Iteration 21/25 | Loss: 0.00153419
Iteration 22/25 | Loss: 0.00153419
Iteration 23/25 | Loss: 0.00153419
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.001534185721538961, 0.001534185721538961, 0.001534185721538961, 0.001534185721538961, 0.001534185721538961]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001534185721538961

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00153419
Iteration 2/1000 | Loss: 0.00050310
Iteration 3/1000 | Loss: 0.00014351
Iteration 4/1000 | Loss: 0.00009103
Iteration 5/1000 | Loss: 0.00005573
Iteration 6/1000 | Loss: 0.00004147
Iteration 7/1000 | Loss: 0.00003495
Iteration 8/1000 | Loss: 0.00003085
Iteration 9/1000 | Loss: 0.00002780
Iteration 10/1000 | Loss: 0.00028796
Iteration 11/1000 | Loss: 0.00003185
Iteration 12/1000 | Loss: 0.00002535
Iteration 13/1000 | Loss: 0.00002333
Iteration 14/1000 | Loss: 0.00002157
Iteration 15/1000 | Loss: 0.00002063
Iteration 16/1000 | Loss: 0.00002021
Iteration 17/1000 | Loss: 0.00001985
Iteration 18/1000 | Loss: 0.00001965
Iteration 19/1000 | Loss: 0.00001943
Iteration 20/1000 | Loss: 0.00001941
Iteration 21/1000 | Loss: 0.00001933
Iteration 22/1000 | Loss: 0.00001931
Iteration 23/1000 | Loss: 0.00001929
Iteration 24/1000 | Loss: 0.00001929
Iteration 25/1000 | Loss: 0.00001929
Iteration 26/1000 | Loss: 0.00001929
Iteration 27/1000 | Loss: 0.00001929
Iteration 28/1000 | Loss: 0.00001929
Iteration 29/1000 | Loss: 0.00001929
Iteration 30/1000 | Loss: 0.00001929
Iteration 31/1000 | Loss: 0.00001928
Iteration 32/1000 | Loss: 0.00001927
Iteration 33/1000 | Loss: 0.00001927
Iteration 34/1000 | Loss: 0.00001925
Iteration 35/1000 | Loss: 0.00001924
Iteration 36/1000 | Loss: 0.00001924
Iteration 37/1000 | Loss: 0.00001923
Iteration 38/1000 | Loss: 0.00001923
Iteration 39/1000 | Loss: 0.00001923
Iteration 40/1000 | Loss: 0.00001923
Iteration 41/1000 | Loss: 0.00001923
Iteration 42/1000 | Loss: 0.00001923
Iteration 43/1000 | Loss: 0.00001923
Iteration 44/1000 | Loss: 0.00001922
Iteration 45/1000 | Loss: 0.00001922
Iteration 46/1000 | Loss: 0.00001922
Iteration 47/1000 | Loss: 0.00001922
Iteration 48/1000 | Loss: 0.00001922
Iteration 49/1000 | Loss: 0.00001922
Iteration 50/1000 | Loss: 0.00001922
Iteration 51/1000 | Loss: 0.00001921
Iteration 52/1000 | Loss: 0.00001921
Iteration 53/1000 | Loss: 0.00001921
Iteration 54/1000 | Loss: 0.00001921
Iteration 55/1000 | Loss: 0.00001921
Iteration 56/1000 | Loss: 0.00001921
Iteration 57/1000 | Loss: 0.00001921
Iteration 58/1000 | Loss: 0.00001920
Iteration 59/1000 | Loss: 0.00001920
Iteration 60/1000 | Loss: 0.00001920
Iteration 61/1000 | Loss: 0.00001919
Iteration 62/1000 | Loss: 0.00001919
Iteration 63/1000 | Loss: 0.00001919
Iteration 64/1000 | Loss: 0.00001919
Iteration 65/1000 | Loss: 0.00001919
Iteration 66/1000 | Loss: 0.00001918
Iteration 67/1000 | Loss: 0.00001918
Iteration 68/1000 | Loss: 0.00001918
Iteration 69/1000 | Loss: 0.00001918
Iteration 70/1000 | Loss: 0.00001918
Iteration 71/1000 | Loss: 0.00001918
Iteration 72/1000 | Loss: 0.00001918
Iteration 73/1000 | Loss: 0.00001918
Iteration 74/1000 | Loss: 0.00001917
Iteration 75/1000 | Loss: 0.00001917
Iteration 76/1000 | Loss: 0.00001917
Iteration 77/1000 | Loss: 0.00001916
Iteration 78/1000 | Loss: 0.00001916
Iteration 79/1000 | Loss: 0.00001916
Iteration 80/1000 | Loss: 0.00001916
Iteration 81/1000 | Loss: 0.00001916
Iteration 82/1000 | Loss: 0.00001915
Iteration 83/1000 | Loss: 0.00001915
Iteration 84/1000 | Loss: 0.00001915
Iteration 85/1000 | Loss: 0.00001915
Iteration 86/1000 | Loss: 0.00001915
Iteration 87/1000 | Loss: 0.00001914
Iteration 88/1000 | Loss: 0.00001914
Iteration 89/1000 | Loss: 0.00001914
Iteration 90/1000 | Loss: 0.00001914
Iteration 91/1000 | Loss: 0.00001914
Iteration 92/1000 | Loss: 0.00001914
Iteration 93/1000 | Loss: 0.00001914
Iteration 94/1000 | Loss: 0.00001914
Iteration 95/1000 | Loss: 0.00001914
Iteration 96/1000 | Loss: 0.00001914
Iteration 97/1000 | Loss: 0.00001914
Iteration 98/1000 | Loss: 0.00001914
Iteration 99/1000 | Loss: 0.00001914
Iteration 100/1000 | Loss: 0.00001913
Iteration 101/1000 | Loss: 0.00001913
Iteration 102/1000 | Loss: 0.00001913
Iteration 103/1000 | Loss: 0.00001913
Iteration 104/1000 | Loss: 0.00001913
Iteration 105/1000 | Loss: 0.00001913
Iteration 106/1000 | Loss: 0.00001913
Iteration 107/1000 | Loss: 0.00001913
Iteration 108/1000 | Loss: 0.00001913
Iteration 109/1000 | Loss: 0.00001913
Iteration 110/1000 | Loss: 0.00001913
Iteration 111/1000 | Loss: 0.00001913
Iteration 112/1000 | Loss: 0.00001913
Iteration 113/1000 | Loss: 0.00001913
Iteration 114/1000 | Loss: 0.00001913
Iteration 115/1000 | Loss: 0.00001913
Iteration 116/1000 | Loss: 0.00001913
Iteration 117/1000 | Loss: 0.00001913
Iteration 118/1000 | Loss: 0.00001913
Iteration 119/1000 | Loss: 0.00001913
Iteration 120/1000 | Loss: 0.00001913
Iteration 121/1000 | Loss: 0.00001913
Iteration 122/1000 | Loss: 0.00001913
Iteration 123/1000 | Loss: 0.00001913
Iteration 124/1000 | Loss: 0.00001913
Iteration 125/1000 | Loss: 0.00001913
Iteration 126/1000 | Loss: 0.00001913
Iteration 127/1000 | Loss: 0.00001913
Iteration 128/1000 | Loss: 0.00001913
Iteration 129/1000 | Loss: 0.00001913
Iteration 130/1000 | Loss: 0.00001913
Iteration 131/1000 | Loss: 0.00001913
Iteration 132/1000 | Loss: 0.00001913
Iteration 133/1000 | Loss: 0.00001913
Iteration 134/1000 | Loss: 0.00001913
Iteration 135/1000 | Loss: 0.00001913
Iteration 136/1000 | Loss: 0.00001913
Iteration 137/1000 | Loss: 0.00001913
Iteration 138/1000 | Loss: 0.00001913
Iteration 139/1000 | Loss: 0.00001913
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 139. Stopping optimization.
Last 5 losses: [1.913208507176023e-05, 1.913208507176023e-05, 1.913208507176023e-05, 1.913208507176023e-05, 1.913208507176023e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.913208507176023e-05

Optimization complete. Final v2v error: 3.747586727142334 mm

Highest mean error: 4.519040584564209 mm for frame 3

Lowest mean error: 3.173128366470337 mm for frame 165

Saving results

Total time: 73.10384273529053
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_006/1096/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_006/1096.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_006/1096
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00664638
Iteration 2/25 | Loss: 0.00114279
Iteration 3/25 | Loss: 0.00091934
Iteration 4/25 | Loss: 0.00085456
Iteration 5/25 | Loss: 0.00083088
Iteration 6/25 | Loss: 0.00082478
Iteration 7/25 | Loss: 0.00082280
Iteration 8/25 | Loss: 0.00081921
Iteration 9/25 | Loss: 0.00081704
Iteration 10/25 | Loss: 0.00081364
Iteration 11/25 | Loss: 0.00081118
Iteration 12/25 | Loss: 0.00081051
Iteration 13/25 | Loss: 0.00081044
Iteration 14/25 | Loss: 0.00081043
Iteration 15/25 | Loss: 0.00081043
Iteration 16/25 | Loss: 0.00081043
Iteration 17/25 | Loss: 0.00081043
Iteration 18/25 | Loss: 0.00081043
Iteration 19/25 | Loss: 0.00081043
Iteration 20/25 | Loss: 0.00081043
Iteration 21/25 | Loss: 0.00081043
Iteration 22/25 | Loss: 0.00081043
Iteration 23/25 | Loss: 0.00081043
Iteration 24/25 | Loss: 0.00081043
Iteration 25/25 | Loss: 0.00081043

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.56021917
Iteration 2/25 | Loss: 0.00052988
Iteration 3/25 | Loss: 0.00052988
Iteration 4/25 | Loss: 0.00052988
Iteration 5/25 | Loss: 0.00052988
Iteration 6/25 | Loss: 0.00052988
Iteration 7/25 | Loss: 0.00052988
Iteration 8/25 | Loss: 0.00052988
Iteration 9/25 | Loss: 0.00052988
Iteration 10/25 | Loss: 0.00052988
Iteration 11/25 | Loss: 0.00052988
Iteration 12/25 | Loss: 0.00052988
Iteration 13/25 | Loss: 0.00052988
Iteration 14/25 | Loss: 0.00052988
Iteration 15/25 | Loss: 0.00052988
Iteration 16/25 | Loss: 0.00052988
Iteration 17/25 | Loss: 0.00052988
Iteration 18/25 | Loss: 0.00052988
Iteration 19/25 | Loss: 0.00052988
Iteration 20/25 | Loss: 0.00052988
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0005298787727952003, 0.0005298787727952003, 0.0005298787727952003, 0.0005298787727952003, 0.0005298787727952003]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005298787727952003

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00052988
Iteration 2/1000 | Loss: 0.00005231
Iteration 3/1000 | Loss: 0.00003686
Iteration 4/1000 | Loss: 0.00003250
Iteration 5/1000 | Loss: 0.00003084
Iteration 6/1000 | Loss: 0.00002947
Iteration 7/1000 | Loss: 0.00002859
Iteration 8/1000 | Loss: 0.00002784
Iteration 9/1000 | Loss: 0.00002735
Iteration 10/1000 | Loss: 0.00002701
Iteration 11/1000 | Loss: 0.00002671
Iteration 12/1000 | Loss: 0.00002646
Iteration 13/1000 | Loss: 0.00002628
Iteration 14/1000 | Loss: 0.00002610
Iteration 15/1000 | Loss: 0.00002598
Iteration 16/1000 | Loss: 0.00002597
Iteration 17/1000 | Loss: 0.00002594
Iteration 18/1000 | Loss: 0.00002593
Iteration 19/1000 | Loss: 0.00002592
Iteration 20/1000 | Loss: 0.00002591
Iteration 21/1000 | Loss: 0.00002591
Iteration 22/1000 | Loss: 0.00002590
Iteration 23/1000 | Loss: 0.00002589
Iteration 24/1000 | Loss: 0.00002589
Iteration 25/1000 | Loss: 0.00002589
Iteration 26/1000 | Loss: 0.00002589
Iteration 27/1000 | Loss: 0.00002588
Iteration 28/1000 | Loss: 0.00002588
Iteration 29/1000 | Loss: 0.00002588
Iteration 30/1000 | Loss: 0.00002588
Iteration 31/1000 | Loss: 0.00002588
Iteration 32/1000 | Loss: 0.00002588
Iteration 33/1000 | Loss: 0.00002588
Iteration 34/1000 | Loss: 0.00002587
Iteration 35/1000 | Loss: 0.00002587
Iteration 36/1000 | Loss: 0.00002586
Iteration 37/1000 | Loss: 0.00002585
Iteration 38/1000 | Loss: 0.00002585
Iteration 39/1000 | Loss: 0.00002585
Iteration 40/1000 | Loss: 0.00002584
Iteration 41/1000 | Loss: 0.00002584
Iteration 42/1000 | Loss: 0.00002583
Iteration 43/1000 | Loss: 0.00002583
Iteration 44/1000 | Loss: 0.00002582
Iteration 45/1000 | Loss: 0.00002582
Iteration 46/1000 | Loss: 0.00002581
Iteration 47/1000 | Loss: 0.00002581
Iteration 48/1000 | Loss: 0.00002581
Iteration 49/1000 | Loss: 0.00002580
Iteration 50/1000 | Loss: 0.00002579
Iteration 51/1000 | Loss: 0.00002577
Iteration 52/1000 | Loss: 0.00002577
Iteration 53/1000 | Loss: 0.00002576
Iteration 54/1000 | Loss: 0.00002576
Iteration 55/1000 | Loss: 0.00002575
Iteration 56/1000 | Loss: 0.00002575
Iteration 57/1000 | Loss: 0.00002574
Iteration 58/1000 | Loss: 0.00002574
Iteration 59/1000 | Loss: 0.00002574
Iteration 60/1000 | Loss: 0.00002574
Iteration 61/1000 | Loss: 0.00002574
Iteration 62/1000 | Loss: 0.00002573
Iteration 63/1000 | Loss: 0.00002573
Iteration 64/1000 | Loss: 0.00002573
Iteration 65/1000 | Loss: 0.00002572
Iteration 66/1000 | Loss: 0.00002572
Iteration 67/1000 | Loss: 0.00002571
Iteration 68/1000 | Loss: 0.00002571
Iteration 69/1000 | Loss: 0.00002571
Iteration 70/1000 | Loss: 0.00002570
Iteration 71/1000 | Loss: 0.00002570
Iteration 72/1000 | Loss: 0.00002570
Iteration 73/1000 | Loss: 0.00002570
Iteration 74/1000 | Loss: 0.00002570
Iteration 75/1000 | Loss: 0.00002570
Iteration 76/1000 | Loss: 0.00002569
Iteration 77/1000 | Loss: 0.00002569
Iteration 78/1000 | Loss: 0.00002569
Iteration 79/1000 | Loss: 0.00002569
Iteration 80/1000 | Loss: 0.00002569
Iteration 81/1000 | Loss: 0.00002569
Iteration 82/1000 | Loss: 0.00002568
Iteration 83/1000 | Loss: 0.00002568
Iteration 84/1000 | Loss: 0.00002568
Iteration 85/1000 | Loss: 0.00002567
Iteration 86/1000 | Loss: 0.00002567
Iteration 87/1000 | Loss: 0.00002567
Iteration 88/1000 | Loss: 0.00002567
Iteration 89/1000 | Loss: 0.00002566
Iteration 90/1000 | Loss: 0.00002566
Iteration 91/1000 | Loss: 0.00002566
Iteration 92/1000 | Loss: 0.00002566
Iteration 93/1000 | Loss: 0.00002566
Iteration 94/1000 | Loss: 0.00002566
Iteration 95/1000 | Loss: 0.00002566
Iteration 96/1000 | Loss: 0.00002566
Iteration 97/1000 | Loss: 0.00002566
Iteration 98/1000 | Loss: 0.00002566
Iteration 99/1000 | Loss: 0.00002566
Iteration 100/1000 | Loss: 0.00002566
Iteration 101/1000 | Loss: 0.00002565
Iteration 102/1000 | Loss: 0.00002565
Iteration 103/1000 | Loss: 0.00002565
Iteration 104/1000 | Loss: 0.00002565
Iteration 105/1000 | Loss: 0.00002565
Iteration 106/1000 | Loss: 0.00002565
Iteration 107/1000 | Loss: 0.00002565
Iteration 108/1000 | Loss: 0.00002565
Iteration 109/1000 | Loss: 0.00002565
Iteration 110/1000 | Loss: 0.00002565
Iteration 111/1000 | Loss: 0.00002565
Iteration 112/1000 | Loss: 0.00002564
Iteration 113/1000 | Loss: 0.00002564
Iteration 114/1000 | Loss: 0.00002564
Iteration 115/1000 | Loss: 0.00002564
Iteration 116/1000 | Loss: 0.00002564
Iteration 117/1000 | Loss: 0.00002564
Iteration 118/1000 | Loss: 0.00002564
Iteration 119/1000 | Loss: 0.00002564
Iteration 120/1000 | Loss: 0.00002564
Iteration 121/1000 | Loss: 0.00002564
Iteration 122/1000 | Loss: 0.00002564
Iteration 123/1000 | Loss: 0.00002564
Iteration 124/1000 | Loss: 0.00002564
Iteration 125/1000 | Loss: 0.00002564
Iteration 126/1000 | Loss: 0.00002564
Iteration 127/1000 | Loss: 0.00002564
Iteration 128/1000 | Loss: 0.00002563
Iteration 129/1000 | Loss: 0.00002563
Iteration 130/1000 | Loss: 0.00002563
Iteration 131/1000 | Loss: 0.00002563
Iteration 132/1000 | Loss: 0.00002563
Iteration 133/1000 | Loss: 0.00002563
Iteration 134/1000 | Loss: 0.00002563
Iteration 135/1000 | Loss: 0.00002563
Iteration 136/1000 | Loss: 0.00002562
Iteration 137/1000 | Loss: 0.00002562
Iteration 138/1000 | Loss: 0.00002562
Iteration 139/1000 | Loss: 0.00002562
Iteration 140/1000 | Loss: 0.00002562
Iteration 141/1000 | Loss: 0.00002562
Iteration 142/1000 | Loss: 0.00002562
Iteration 143/1000 | Loss: 0.00002562
Iteration 144/1000 | Loss: 0.00002562
Iteration 145/1000 | Loss: 0.00002562
Iteration 146/1000 | Loss: 0.00002562
Iteration 147/1000 | Loss: 0.00002562
Iteration 148/1000 | Loss: 0.00002562
Iteration 149/1000 | Loss: 0.00002562
Iteration 150/1000 | Loss: 0.00002562
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 150. Stopping optimization.
Last 5 losses: [2.562248300819192e-05, 2.562248300819192e-05, 2.562248300819192e-05, 2.562248300819192e-05, 2.562248300819192e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.562248300819192e-05

Optimization complete. Final v2v error: 4.269993305206299 mm

Highest mean error: 5.023797512054443 mm for frame 169

Lowest mean error: 3.6307950019836426 mm for frame 133

Saving results

Total time: 59.232192039489746
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_006/1076/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_006/1076.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_006/1076
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00442153
Iteration 2/25 | Loss: 0.00097003
Iteration 3/25 | Loss: 0.00080032
Iteration 4/25 | Loss: 0.00077472
Iteration 5/25 | Loss: 0.00076700
Iteration 6/25 | Loss: 0.00076552
Iteration 7/25 | Loss: 0.00076532
Iteration 8/25 | Loss: 0.00076532
Iteration 9/25 | Loss: 0.00076532
Iteration 10/25 | Loss: 0.00076532
Iteration 11/25 | Loss: 0.00076532
Iteration 12/25 | Loss: 0.00076532
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.000765319331549108, 0.000765319331549108, 0.000765319331549108, 0.000765319331549108, 0.000765319331549108]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000765319331549108

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.08690691
Iteration 2/25 | Loss: 0.00051263
Iteration 3/25 | Loss: 0.00051263
Iteration 4/25 | Loss: 0.00051263
Iteration 5/25 | Loss: 0.00051263
Iteration 6/25 | Loss: 0.00051263
Iteration 7/25 | Loss: 0.00051263
Iteration 8/25 | Loss: 0.00051263
Iteration 9/25 | Loss: 0.00051263
Iteration 10/25 | Loss: 0.00051263
Iteration 11/25 | Loss: 0.00051263
Iteration 12/25 | Loss: 0.00051263
Iteration 13/25 | Loss: 0.00051263
Iteration 14/25 | Loss: 0.00051263
Iteration 15/25 | Loss: 0.00051263
Iteration 16/25 | Loss: 0.00051263
Iteration 17/25 | Loss: 0.00051263
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0005126254400238395, 0.0005126254400238395, 0.0005126254400238395, 0.0005126254400238395, 0.0005126254400238395]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005126254400238395

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00051263
Iteration 2/1000 | Loss: 0.00003427
Iteration 3/1000 | Loss: 0.00002391
Iteration 4/1000 | Loss: 0.00002133
Iteration 5/1000 | Loss: 0.00001997
Iteration 6/1000 | Loss: 0.00001931
Iteration 7/1000 | Loss: 0.00001864
Iteration 8/1000 | Loss: 0.00001842
Iteration 9/1000 | Loss: 0.00001814
Iteration 10/1000 | Loss: 0.00001796
Iteration 11/1000 | Loss: 0.00001792
Iteration 12/1000 | Loss: 0.00001774
Iteration 13/1000 | Loss: 0.00001772
Iteration 14/1000 | Loss: 0.00001762
Iteration 15/1000 | Loss: 0.00001762
Iteration 16/1000 | Loss: 0.00001762
Iteration 17/1000 | Loss: 0.00001762
Iteration 18/1000 | Loss: 0.00001762
Iteration 19/1000 | Loss: 0.00001762
Iteration 20/1000 | Loss: 0.00001762
Iteration 21/1000 | Loss: 0.00001762
Iteration 22/1000 | Loss: 0.00001762
Iteration 23/1000 | Loss: 0.00001761
Iteration 24/1000 | Loss: 0.00001761
Iteration 25/1000 | Loss: 0.00001761
Iteration 26/1000 | Loss: 0.00001759
Iteration 27/1000 | Loss: 0.00001757
Iteration 28/1000 | Loss: 0.00001757
Iteration 29/1000 | Loss: 0.00001757
Iteration 30/1000 | Loss: 0.00001757
Iteration 31/1000 | Loss: 0.00001757
Iteration 32/1000 | Loss: 0.00001757
Iteration 33/1000 | Loss: 0.00001756
Iteration 34/1000 | Loss: 0.00001756
Iteration 35/1000 | Loss: 0.00001756
Iteration 36/1000 | Loss: 0.00001755
Iteration 37/1000 | Loss: 0.00001755
Iteration 38/1000 | Loss: 0.00001754
Iteration 39/1000 | Loss: 0.00001750
Iteration 40/1000 | Loss: 0.00001750
Iteration 41/1000 | Loss: 0.00001750
Iteration 42/1000 | Loss: 0.00001750
Iteration 43/1000 | Loss: 0.00001750
Iteration 44/1000 | Loss: 0.00001750
Iteration 45/1000 | Loss: 0.00001750
Iteration 46/1000 | Loss: 0.00001750
Iteration 47/1000 | Loss: 0.00001750
Iteration 48/1000 | Loss: 0.00001750
Iteration 49/1000 | Loss: 0.00001750
Iteration 50/1000 | Loss: 0.00001750
Iteration 51/1000 | Loss: 0.00001750
Iteration 52/1000 | Loss: 0.00001750
Iteration 53/1000 | Loss: 0.00001750
Iteration 54/1000 | Loss: 0.00001750
Iteration 55/1000 | Loss: 0.00001750
Iteration 56/1000 | Loss: 0.00001750
Iteration 57/1000 | Loss: 0.00001750
Iteration 58/1000 | Loss: 0.00001750
Iteration 59/1000 | Loss: 0.00001750
Iteration 60/1000 | Loss: 0.00001750
Iteration 61/1000 | Loss: 0.00001750
Iteration 62/1000 | Loss: 0.00001750
Iteration 63/1000 | Loss: 0.00001750
Iteration 64/1000 | Loss: 0.00001750
Iteration 65/1000 | Loss: 0.00001750
Iteration 66/1000 | Loss: 0.00001750
Iteration 67/1000 | Loss: 0.00001750
Iteration 68/1000 | Loss: 0.00001750
Iteration 69/1000 | Loss: 0.00001750
Iteration 70/1000 | Loss: 0.00001750
Iteration 71/1000 | Loss: 0.00001750
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 71. Stopping optimization.
Last 5 losses: [1.7495409338152967e-05, 1.7495409338152967e-05, 1.7495409338152967e-05, 1.7495409338152967e-05, 1.7495409338152967e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7495409338152967e-05

Optimization complete. Final v2v error: 3.527022123336792 mm

Highest mean error: 3.551856756210327 mm for frame 83

Lowest mean error: 3.4931864738464355 mm for frame 100

Saving results

Total time: 27.391557931900024
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_006/1075/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_006/1075.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_006/1075
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00938124
Iteration 2/25 | Loss: 0.00117506
Iteration 3/25 | Loss: 0.00094578
Iteration 4/25 | Loss: 0.00090828
Iteration 5/25 | Loss: 0.00089638
Iteration 6/25 | Loss: 0.00089452
Iteration 7/25 | Loss: 0.00089445
Iteration 8/25 | Loss: 0.00089445
Iteration 9/25 | Loss: 0.00089445
Iteration 10/25 | Loss: 0.00089445
Iteration 11/25 | Loss: 0.00089445
Iteration 12/25 | Loss: 0.00089445
Iteration 13/25 | Loss: 0.00089445
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0008944537257775664, 0.0008944537257775664, 0.0008944537257775664, 0.0008944537257775664, 0.0008944537257775664]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008944537257775664

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43862379
Iteration 2/25 | Loss: 0.00058822
Iteration 3/25 | Loss: 0.00058822
Iteration 4/25 | Loss: 0.00058822
Iteration 5/25 | Loss: 0.00058822
Iteration 6/25 | Loss: 0.00058822
Iteration 7/25 | Loss: 0.00058822
Iteration 8/25 | Loss: 0.00058822
Iteration 9/25 | Loss: 0.00058822
Iteration 10/25 | Loss: 0.00058821
Iteration 11/25 | Loss: 0.00058821
Iteration 12/25 | Loss: 0.00058821
Iteration 13/25 | Loss: 0.00058821
Iteration 14/25 | Loss: 0.00058821
Iteration 15/25 | Loss: 0.00058821
Iteration 16/25 | Loss: 0.00058821
Iteration 17/25 | Loss: 0.00058821
Iteration 18/25 | Loss: 0.00058821
Iteration 19/25 | Loss: 0.00058821
Iteration 20/25 | Loss: 0.00058821
Iteration 21/25 | Loss: 0.00058821
Iteration 22/25 | Loss: 0.00058821
Iteration 23/25 | Loss: 0.00058821
Iteration 24/25 | Loss: 0.00058821
Iteration 25/25 | Loss: 0.00058821

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00058821
Iteration 2/1000 | Loss: 0.00004581
Iteration 3/1000 | Loss: 0.00003693
Iteration 4/1000 | Loss: 0.00003364
Iteration 5/1000 | Loss: 0.00003236
Iteration 6/1000 | Loss: 0.00003113
Iteration 7/1000 | Loss: 0.00003054
Iteration 8/1000 | Loss: 0.00003008
Iteration 9/1000 | Loss: 0.00002975
Iteration 10/1000 | Loss: 0.00002952
Iteration 11/1000 | Loss: 0.00002943
Iteration 12/1000 | Loss: 0.00002935
Iteration 13/1000 | Loss: 0.00002926
Iteration 14/1000 | Loss: 0.00002922
Iteration 15/1000 | Loss: 0.00002921
Iteration 16/1000 | Loss: 0.00002921
Iteration 17/1000 | Loss: 0.00002919
Iteration 18/1000 | Loss: 0.00002919
Iteration 19/1000 | Loss: 0.00002918
Iteration 20/1000 | Loss: 0.00002915
Iteration 21/1000 | Loss: 0.00002915
Iteration 22/1000 | Loss: 0.00002915
Iteration 23/1000 | Loss: 0.00002915
Iteration 24/1000 | Loss: 0.00002915
Iteration 25/1000 | Loss: 0.00002915
Iteration 26/1000 | Loss: 0.00002915
Iteration 27/1000 | Loss: 0.00002914
Iteration 28/1000 | Loss: 0.00002912
Iteration 29/1000 | Loss: 0.00002910
Iteration 30/1000 | Loss: 0.00002910
Iteration 31/1000 | Loss: 0.00002910
Iteration 32/1000 | Loss: 0.00002910
Iteration 33/1000 | Loss: 0.00002910
Iteration 34/1000 | Loss: 0.00002910
Iteration 35/1000 | Loss: 0.00002910
Iteration 36/1000 | Loss: 0.00002910
Iteration 37/1000 | Loss: 0.00002909
Iteration 38/1000 | Loss: 0.00002909
Iteration 39/1000 | Loss: 0.00002909
Iteration 40/1000 | Loss: 0.00002908
Iteration 41/1000 | Loss: 0.00002908
Iteration 42/1000 | Loss: 0.00002908
Iteration 43/1000 | Loss: 0.00002907
Iteration 44/1000 | Loss: 0.00002907
Iteration 45/1000 | Loss: 0.00002907
Iteration 46/1000 | Loss: 0.00002907
Iteration 47/1000 | Loss: 0.00002907
Iteration 48/1000 | Loss: 0.00002907
Iteration 49/1000 | Loss: 0.00002906
Iteration 50/1000 | Loss: 0.00002906
Iteration 51/1000 | Loss: 0.00002906
Iteration 52/1000 | Loss: 0.00002906
Iteration 53/1000 | Loss: 0.00002906
Iteration 54/1000 | Loss: 0.00002906
Iteration 55/1000 | Loss: 0.00002906
Iteration 56/1000 | Loss: 0.00002906
Iteration 57/1000 | Loss: 0.00002906
Iteration 58/1000 | Loss: 0.00002906
Iteration 59/1000 | Loss: 0.00002906
Iteration 60/1000 | Loss: 0.00002905
Iteration 61/1000 | Loss: 0.00002905
Iteration 62/1000 | Loss: 0.00002905
Iteration 63/1000 | Loss: 0.00002905
Iteration 64/1000 | Loss: 0.00002905
Iteration 65/1000 | Loss: 0.00002905
Iteration 66/1000 | Loss: 0.00002905
Iteration 67/1000 | Loss: 0.00002905
Iteration 68/1000 | Loss: 0.00002905
Iteration 69/1000 | Loss: 0.00002905
Iteration 70/1000 | Loss: 0.00002905
Iteration 71/1000 | Loss: 0.00002905
Iteration 72/1000 | Loss: 0.00002904
Iteration 73/1000 | Loss: 0.00002904
Iteration 74/1000 | Loss: 0.00002904
Iteration 75/1000 | Loss: 0.00002904
Iteration 76/1000 | Loss: 0.00002903
Iteration 77/1000 | Loss: 0.00002903
Iteration 78/1000 | Loss: 0.00002903
Iteration 79/1000 | Loss: 0.00002902
Iteration 80/1000 | Loss: 0.00002902
Iteration 81/1000 | Loss: 0.00002902
Iteration 82/1000 | Loss: 0.00002902
Iteration 83/1000 | Loss: 0.00002902
Iteration 84/1000 | Loss: 0.00002902
Iteration 85/1000 | Loss: 0.00002902
Iteration 86/1000 | Loss: 0.00002902
Iteration 87/1000 | Loss: 0.00002901
Iteration 88/1000 | Loss: 0.00002901
Iteration 89/1000 | Loss: 0.00002901
Iteration 90/1000 | Loss: 0.00002901
Iteration 91/1000 | Loss: 0.00002900
Iteration 92/1000 | Loss: 0.00002900
Iteration 93/1000 | Loss: 0.00002900
Iteration 94/1000 | Loss: 0.00002900
Iteration 95/1000 | Loss: 0.00002900
Iteration 96/1000 | Loss: 0.00002900
Iteration 97/1000 | Loss: 0.00002900
Iteration 98/1000 | Loss: 0.00002899
Iteration 99/1000 | Loss: 0.00002899
Iteration 100/1000 | Loss: 0.00002899
Iteration 101/1000 | Loss: 0.00002899
Iteration 102/1000 | Loss: 0.00002899
Iteration 103/1000 | Loss: 0.00002899
Iteration 104/1000 | Loss: 0.00002899
Iteration 105/1000 | Loss: 0.00002898
Iteration 106/1000 | Loss: 0.00002898
Iteration 107/1000 | Loss: 0.00002898
Iteration 108/1000 | Loss: 0.00002898
Iteration 109/1000 | Loss: 0.00002898
Iteration 110/1000 | Loss: 0.00002898
Iteration 111/1000 | Loss: 0.00002898
Iteration 112/1000 | Loss: 0.00002898
Iteration 113/1000 | Loss: 0.00002898
Iteration 114/1000 | Loss: 0.00002898
Iteration 115/1000 | Loss: 0.00002897
Iteration 116/1000 | Loss: 0.00002897
Iteration 117/1000 | Loss: 0.00002897
Iteration 118/1000 | Loss: 0.00002897
Iteration 119/1000 | Loss: 0.00002897
Iteration 120/1000 | Loss: 0.00002897
Iteration 121/1000 | Loss: 0.00002896
Iteration 122/1000 | Loss: 0.00002896
Iteration 123/1000 | Loss: 0.00002896
Iteration 124/1000 | Loss: 0.00002896
Iteration 125/1000 | Loss: 0.00002896
Iteration 126/1000 | Loss: 0.00002896
Iteration 127/1000 | Loss: 0.00002896
Iteration 128/1000 | Loss: 0.00002896
Iteration 129/1000 | Loss: 0.00002896
Iteration 130/1000 | Loss: 0.00002896
Iteration 131/1000 | Loss: 0.00002896
Iteration 132/1000 | Loss: 0.00002896
Iteration 133/1000 | Loss: 0.00002895
Iteration 134/1000 | Loss: 0.00002895
Iteration 135/1000 | Loss: 0.00002895
Iteration 136/1000 | Loss: 0.00002895
Iteration 137/1000 | Loss: 0.00002895
Iteration 138/1000 | Loss: 0.00002895
Iteration 139/1000 | Loss: 0.00002895
Iteration 140/1000 | Loss: 0.00002895
Iteration 141/1000 | Loss: 0.00002894
Iteration 142/1000 | Loss: 0.00002894
Iteration 143/1000 | Loss: 0.00002894
Iteration 144/1000 | Loss: 0.00002894
Iteration 145/1000 | Loss: 0.00002894
Iteration 146/1000 | Loss: 0.00002894
Iteration 147/1000 | Loss: 0.00002894
Iteration 148/1000 | Loss: 0.00002894
Iteration 149/1000 | Loss: 0.00002894
Iteration 150/1000 | Loss: 0.00002894
Iteration 151/1000 | Loss: 0.00002894
Iteration 152/1000 | Loss: 0.00002894
Iteration 153/1000 | Loss: 0.00002894
Iteration 154/1000 | Loss: 0.00002894
Iteration 155/1000 | Loss: 0.00002894
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 155. Stopping optimization.
Last 5 losses: [2.8935935915797018e-05, 2.8935935915797018e-05, 2.8935935915797018e-05, 2.8935935915797018e-05, 2.8935935915797018e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.8935935915797018e-05

Optimization complete. Final v2v error: 4.501919269561768 mm

Highest mean error: 5.178469657897949 mm for frame 0

Lowest mean error: 3.9374935626983643 mm for frame 54

Saving results

Total time: 41.5510892868042
