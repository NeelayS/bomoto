Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=128, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 7168-7223
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ben_posed_008/1003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_008/1003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_008/1003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00824478
Iteration 2/25 | Loss: 0.00098028
Iteration 3/25 | Loss: 0.00065999
Iteration 4/25 | Loss: 0.00060709
Iteration 5/25 | Loss: 0.00059209
Iteration 6/25 | Loss: 0.00058854
Iteration 7/25 | Loss: 0.00058775
Iteration 8/25 | Loss: 0.00058773
Iteration 9/25 | Loss: 0.00058773
Iteration 10/25 | Loss: 0.00058773
Iteration 11/25 | Loss: 0.00058773
Iteration 12/25 | Loss: 0.00058773
Iteration 13/25 | Loss: 0.00058773
Iteration 14/25 | Loss: 0.00058773
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0005877280491404235, 0.0005877280491404235, 0.0005877280491404235, 0.0005877280491404235, 0.0005877280491404235]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005877280491404235

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.26208520
Iteration 2/25 | Loss: 0.00014263
Iteration 3/25 | Loss: 0.00014263
Iteration 4/25 | Loss: 0.00014263
Iteration 5/25 | Loss: 0.00014263
Iteration 6/25 | Loss: 0.00014263
Iteration 7/25 | Loss: 0.00014263
Iteration 8/25 | Loss: 0.00014263
Iteration 9/25 | Loss: 0.00014263
Iteration 10/25 | Loss: 0.00014263
Iteration 11/25 | Loss: 0.00014263
Iteration 12/25 | Loss: 0.00014263
Iteration 13/25 | Loss: 0.00014263
Iteration 14/25 | Loss: 0.00014263
Iteration 15/25 | Loss: 0.00014263
Iteration 16/25 | Loss: 0.00014263
Iteration 17/25 | Loss: 0.00014263
Iteration 18/25 | Loss: 0.00014263
Iteration 19/25 | Loss: 0.00014263
Iteration 20/25 | Loss: 0.00014263
Iteration 21/25 | Loss: 0.00014263
Iteration 22/25 | Loss: 0.00014263
Iteration 23/25 | Loss: 0.00014263
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.00014262559125199914, 0.00014262559125199914, 0.00014262559125199914, 0.00014262559125199914, 0.00014262559125199914]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00014262559125199914

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00014263
Iteration 2/1000 | Loss: 0.00002941
Iteration 3/1000 | Loss: 0.00002209
Iteration 4/1000 | Loss: 0.00001985
Iteration 5/1000 | Loss: 0.00001899
Iteration 6/1000 | Loss: 0.00001814
Iteration 7/1000 | Loss: 0.00001759
Iteration 8/1000 | Loss: 0.00001719
Iteration 9/1000 | Loss: 0.00001695
Iteration 10/1000 | Loss: 0.00001677
Iteration 11/1000 | Loss: 0.00001674
Iteration 12/1000 | Loss: 0.00001666
Iteration 13/1000 | Loss: 0.00001666
Iteration 14/1000 | Loss: 0.00001665
Iteration 15/1000 | Loss: 0.00001659
Iteration 16/1000 | Loss: 0.00001657
Iteration 17/1000 | Loss: 0.00001653
Iteration 18/1000 | Loss: 0.00001652
Iteration 19/1000 | Loss: 0.00001652
Iteration 20/1000 | Loss: 0.00001651
Iteration 21/1000 | Loss: 0.00001650
Iteration 22/1000 | Loss: 0.00001644
Iteration 23/1000 | Loss: 0.00001644
Iteration 24/1000 | Loss: 0.00001643
Iteration 25/1000 | Loss: 0.00001641
Iteration 26/1000 | Loss: 0.00001639
Iteration 27/1000 | Loss: 0.00001638
Iteration 28/1000 | Loss: 0.00001637
Iteration 29/1000 | Loss: 0.00001637
Iteration 30/1000 | Loss: 0.00001637
Iteration 31/1000 | Loss: 0.00001637
Iteration 32/1000 | Loss: 0.00001636
Iteration 33/1000 | Loss: 0.00001636
Iteration 34/1000 | Loss: 0.00001634
Iteration 35/1000 | Loss: 0.00001632
Iteration 36/1000 | Loss: 0.00001632
Iteration 37/1000 | Loss: 0.00001630
Iteration 38/1000 | Loss: 0.00001630
Iteration 39/1000 | Loss: 0.00001629
Iteration 40/1000 | Loss: 0.00001629
Iteration 41/1000 | Loss: 0.00001628
Iteration 42/1000 | Loss: 0.00001626
Iteration 43/1000 | Loss: 0.00001626
Iteration 44/1000 | Loss: 0.00001626
Iteration 45/1000 | Loss: 0.00001626
Iteration 46/1000 | Loss: 0.00001626
Iteration 47/1000 | Loss: 0.00001626
Iteration 48/1000 | Loss: 0.00001626
Iteration 49/1000 | Loss: 0.00001626
Iteration 50/1000 | Loss: 0.00001625
Iteration 51/1000 | Loss: 0.00001625
Iteration 52/1000 | Loss: 0.00001625
Iteration 53/1000 | Loss: 0.00001624
Iteration 54/1000 | Loss: 0.00001624
Iteration 55/1000 | Loss: 0.00001624
Iteration 56/1000 | Loss: 0.00001624
Iteration 57/1000 | Loss: 0.00001624
Iteration 58/1000 | Loss: 0.00001624
Iteration 59/1000 | Loss: 0.00001624
Iteration 60/1000 | Loss: 0.00001623
Iteration 61/1000 | Loss: 0.00001623
Iteration 62/1000 | Loss: 0.00001623
Iteration 63/1000 | Loss: 0.00001622
Iteration 64/1000 | Loss: 0.00001622
Iteration 65/1000 | Loss: 0.00001622
Iteration 66/1000 | Loss: 0.00001622
Iteration 67/1000 | Loss: 0.00001621
Iteration 68/1000 | Loss: 0.00001621
Iteration 69/1000 | Loss: 0.00001621
Iteration 70/1000 | Loss: 0.00001621
Iteration 71/1000 | Loss: 0.00001621
Iteration 72/1000 | Loss: 0.00001621
Iteration 73/1000 | Loss: 0.00001621
Iteration 74/1000 | Loss: 0.00001621
Iteration 75/1000 | Loss: 0.00001621
Iteration 76/1000 | Loss: 0.00001621
Iteration 77/1000 | Loss: 0.00001621
Iteration 78/1000 | Loss: 0.00001621
Iteration 79/1000 | Loss: 0.00001621
Iteration 80/1000 | Loss: 0.00001621
Iteration 81/1000 | Loss: 0.00001620
Iteration 82/1000 | Loss: 0.00001620
Iteration 83/1000 | Loss: 0.00001620
Iteration 84/1000 | Loss: 0.00001620
Iteration 85/1000 | Loss: 0.00001620
Iteration 86/1000 | Loss: 0.00001620
Iteration 87/1000 | Loss: 0.00001620
Iteration 88/1000 | Loss: 0.00001620
Iteration 89/1000 | Loss: 0.00001620
Iteration 90/1000 | Loss: 0.00001620
Iteration 91/1000 | Loss: 0.00001620
Iteration 92/1000 | Loss: 0.00001620
Iteration 93/1000 | Loss: 0.00001620
Iteration 94/1000 | Loss: 0.00001620
Iteration 95/1000 | Loss: 0.00001620
Iteration 96/1000 | Loss: 0.00001620
Iteration 97/1000 | Loss: 0.00001619
Iteration 98/1000 | Loss: 0.00001619
Iteration 99/1000 | Loss: 0.00001619
Iteration 100/1000 | Loss: 0.00001619
Iteration 101/1000 | Loss: 0.00001619
Iteration 102/1000 | Loss: 0.00001619
Iteration 103/1000 | Loss: 0.00001618
Iteration 104/1000 | Loss: 0.00001618
Iteration 105/1000 | Loss: 0.00001618
Iteration 106/1000 | Loss: 0.00001618
Iteration 107/1000 | Loss: 0.00001618
Iteration 108/1000 | Loss: 0.00001618
Iteration 109/1000 | Loss: 0.00001618
Iteration 110/1000 | Loss: 0.00001617
Iteration 111/1000 | Loss: 0.00001617
Iteration 112/1000 | Loss: 0.00001617
Iteration 113/1000 | Loss: 0.00001617
Iteration 114/1000 | Loss: 0.00001616
Iteration 115/1000 | Loss: 0.00001616
Iteration 116/1000 | Loss: 0.00001616
Iteration 117/1000 | Loss: 0.00001615
Iteration 118/1000 | Loss: 0.00001615
Iteration 119/1000 | Loss: 0.00001615
Iteration 120/1000 | Loss: 0.00001614
Iteration 121/1000 | Loss: 0.00001614
Iteration 122/1000 | Loss: 0.00001614
Iteration 123/1000 | Loss: 0.00001613
Iteration 124/1000 | Loss: 0.00001613
Iteration 125/1000 | Loss: 0.00001613
Iteration 126/1000 | Loss: 0.00001613
Iteration 127/1000 | Loss: 0.00001613
Iteration 128/1000 | Loss: 0.00001612
Iteration 129/1000 | Loss: 0.00001612
Iteration 130/1000 | Loss: 0.00001612
Iteration 131/1000 | Loss: 0.00001611
Iteration 132/1000 | Loss: 0.00001611
Iteration 133/1000 | Loss: 0.00001611
Iteration 134/1000 | Loss: 0.00001611
Iteration 135/1000 | Loss: 0.00001610
Iteration 136/1000 | Loss: 0.00001610
Iteration 137/1000 | Loss: 0.00001610
Iteration 138/1000 | Loss: 0.00001610
Iteration 139/1000 | Loss: 0.00001610
Iteration 140/1000 | Loss: 0.00001610
Iteration 141/1000 | Loss: 0.00001610
Iteration 142/1000 | Loss: 0.00001610
Iteration 143/1000 | Loss: 0.00001610
Iteration 144/1000 | Loss: 0.00001610
Iteration 145/1000 | Loss: 0.00001610
Iteration 146/1000 | Loss: 0.00001610
Iteration 147/1000 | Loss: 0.00001610
Iteration 148/1000 | Loss: 0.00001610
Iteration 149/1000 | Loss: 0.00001610
Iteration 150/1000 | Loss: 0.00001610
Iteration 151/1000 | Loss: 0.00001610
Iteration 152/1000 | Loss: 0.00001610
Iteration 153/1000 | Loss: 0.00001610
Iteration 154/1000 | Loss: 0.00001610
Iteration 155/1000 | Loss: 0.00001610
Iteration 156/1000 | Loss: 0.00001610
Iteration 157/1000 | Loss: 0.00001610
Iteration 158/1000 | Loss: 0.00001610
Iteration 159/1000 | Loss: 0.00001610
Iteration 160/1000 | Loss: 0.00001610
Iteration 161/1000 | Loss: 0.00001610
Iteration 162/1000 | Loss: 0.00001610
Iteration 163/1000 | Loss: 0.00001610
Iteration 164/1000 | Loss: 0.00001610
Iteration 165/1000 | Loss: 0.00001610
Iteration 166/1000 | Loss: 0.00001610
Iteration 167/1000 | Loss: 0.00001610
Iteration 168/1000 | Loss: 0.00001610
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 168. Stopping optimization.
Last 5 losses: [1.6100218999781646e-05, 1.6100218999781646e-05, 1.6100218999781646e-05, 1.6100218999781646e-05, 1.6100218999781646e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6100218999781646e-05

Optimization complete. Final v2v error: 3.317708969116211 mm

Highest mean error: 5.017700672149658 mm for frame 77

Lowest mean error: 2.6867072582244873 mm for frame 107

Saving results

Total time: 41.75683784484863
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ben_posed_008/1072/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_008/1072.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_008/1072
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01078266
Iteration 2/25 | Loss: 0.00286480
Iteration 3/25 | Loss: 0.00157188
Iteration 4/25 | Loss: 0.00143564
Iteration 5/25 | Loss: 0.00107279
Iteration 6/25 | Loss: 0.00108942
Iteration 7/25 | Loss: 0.00098002
Iteration 8/25 | Loss: 0.00101868
Iteration 9/25 | Loss: 0.00091097
Iteration 10/25 | Loss: 0.00084087
Iteration 11/25 | Loss: 0.00083230
Iteration 12/25 | Loss: 0.00076576
Iteration 13/25 | Loss: 0.00076464
Iteration 14/25 | Loss: 0.00074566
Iteration 15/25 | Loss: 0.00071264
Iteration 16/25 | Loss: 0.00069924
Iteration 17/25 | Loss: 0.00068485
Iteration 18/25 | Loss: 0.00067727
Iteration 19/25 | Loss: 0.00067627
Iteration 20/25 | Loss: 0.00067005
Iteration 21/25 | Loss: 0.00067443
Iteration 22/25 | Loss: 0.00067417
Iteration 23/25 | Loss: 0.00066646
Iteration 24/25 | Loss: 0.00066936
Iteration 25/25 | Loss: 0.00066859

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.52632940
Iteration 2/25 | Loss: 0.00102028
Iteration 3/25 | Loss: 0.00056678
Iteration 4/25 | Loss: 0.00056678
Iteration 5/25 | Loss: 0.00056678
Iteration 6/25 | Loss: 0.00056678
Iteration 7/25 | Loss: 0.00056678
Iteration 8/25 | Loss: 0.00056678
Iteration 9/25 | Loss: 0.00056678
Iteration 10/25 | Loss: 0.00056678
Iteration 11/25 | Loss: 0.00056678
Iteration 12/25 | Loss: 0.00056678
Iteration 13/25 | Loss: 0.00056678
Iteration 14/25 | Loss: 0.00056678
Iteration 15/25 | Loss: 0.00056678
Iteration 16/25 | Loss: 0.00056678
Iteration 17/25 | Loss: 0.00056678
Iteration 18/25 | Loss: 0.00056678
Iteration 19/25 | Loss: 0.00056678
Iteration 20/25 | Loss: 0.00056678
Iteration 21/25 | Loss: 0.00056678
Iteration 22/25 | Loss: 0.00056678
Iteration 23/25 | Loss: 0.00056678
Iteration 24/25 | Loss: 0.00056678
Iteration 25/25 | Loss: 0.00056678

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00056678
Iteration 2/1000 | Loss: 0.00083827
Iteration 3/1000 | Loss: 0.00053636
Iteration 4/1000 | Loss: 0.00134094
Iteration 5/1000 | Loss: 0.00040798
Iteration 6/1000 | Loss: 0.00056275
Iteration 7/1000 | Loss: 0.00032094
Iteration 8/1000 | Loss: 0.00029140
Iteration 9/1000 | Loss: 0.00050029
Iteration 10/1000 | Loss: 0.00078609
Iteration 11/1000 | Loss: 0.00026641
Iteration 12/1000 | Loss: 0.00067368
Iteration 13/1000 | Loss: 0.00036964
Iteration 14/1000 | Loss: 0.00041886
Iteration 15/1000 | Loss: 0.00026260
Iteration 16/1000 | Loss: 0.00028976
Iteration 17/1000 | Loss: 0.00035740
Iteration 18/1000 | Loss: 0.00017495
Iteration 19/1000 | Loss: 0.00101190
Iteration 20/1000 | Loss: 0.00393871
Iteration 21/1000 | Loss: 0.00171495
Iteration 22/1000 | Loss: 0.00262500
Iteration 23/1000 | Loss: 0.00158359
Iteration 24/1000 | Loss: 0.00283374
Iteration 25/1000 | Loss: 0.00128624
Iteration 26/1000 | Loss: 0.00305322
Iteration 27/1000 | Loss: 0.00089936
Iteration 28/1000 | Loss: 0.00068613
Iteration 29/1000 | Loss: 0.00055788
Iteration 30/1000 | Loss: 0.00049154
Iteration 31/1000 | Loss: 0.00043647
Iteration 32/1000 | Loss: 0.00020201
Iteration 33/1000 | Loss: 0.00024513
Iteration 34/1000 | Loss: 0.00083479
Iteration 35/1000 | Loss: 0.00056468
Iteration 36/1000 | Loss: 0.00030357
Iteration 37/1000 | Loss: 0.00023048
Iteration 38/1000 | Loss: 0.00019164
Iteration 39/1000 | Loss: 0.00099778
Iteration 40/1000 | Loss: 0.00105652
Iteration 41/1000 | Loss: 0.00164368
Iteration 42/1000 | Loss: 0.00040625
Iteration 43/1000 | Loss: 0.00015667
Iteration 44/1000 | Loss: 0.00033253
Iteration 45/1000 | Loss: 0.00033744
Iteration 46/1000 | Loss: 0.00029264
Iteration 47/1000 | Loss: 0.00021553
Iteration 48/1000 | Loss: 0.00029615
Iteration 49/1000 | Loss: 0.00032657
Iteration 50/1000 | Loss: 0.00133260
Iteration 51/1000 | Loss: 0.00067582
Iteration 52/1000 | Loss: 0.00034173
Iteration 53/1000 | Loss: 0.00012301
Iteration 54/1000 | Loss: 0.00046758
Iteration 55/1000 | Loss: 0.00060227
Iteration 56/1000 | Loss: 0.00027696
Iteration 57/1000 | Loss: 0.00070233
Iteration 58/1000 | Loss: 0.00004948
Iteration 59/1000 | Loss: 0.00160715
Iteration 60/1000 | Loss: 0.00162296
Iteration 61/1000 | Loss: 0.00029104
Iteration 62/1000 | Loss: 0.00005132
Iteration 63/1000 | Loss: 0.00004172
Iteration 64/1000 | Loss: 0.00048615
Iteration 65/1000 | Loss: 0.00038811
Iteration 66/1000 | Loss: 0.00058302
Iteration 67/1000 | Loss: 0.00035580
Iteration 68/1000 | Loss: 0.00058358
Iteration 69/1000 | Loss: 0.00024070
Iteration 70/1000 | Loss: 0.00046495
Iteration 71/1000 | Loss: 0.00039779
Iteration 72/1000 | Loss: 0.00038484
Iteration 73/1000 | Loss: 0.00135749
Iteration 74/1000 | Loss: 0.00080577
Iteration 75/1000 | Loss: 0.00115075
Iteration 76/1000 | Loss: 0.00081942
Iteration 77/1000 | Loss: 0.00041133
Iteration 78/1000 | Loss: 0.00028390
Iteration 79/1000 | Loss: 0.00007055
Iteration 80/1000 | Loss: 0.00004756
Iteration 81/1000 | Loss: 0.00005986
Iteration 82/1000 | Loss: 0.00026606
Iteration 83/1000 | Loss: 0.00020714
Iteration 84/1000 | Loss: 0.00002879
Iteration 85/1000 | Loss: 0.00002435
Iteration 86/1000 | Loss: 0.00016751
Iteration 87/1000 | Loss: 0.00024123
Iteration 88/1000 | Loss: 0.00032246
Iteration 89/1000 | Loss: 0.00031266
Iteration 90/1000 | Loss: 0.00045045
Iteration 91/1000 | Loss: 0.00049650
Iteration 92/1000 | Loss: 0.00121335
Iteration 93/1000 | Loss: 0.00030044
Iteration 94/1000 | Loss: 0.00002831
Iteration 95/1000 | Loss: 0.00002370
Iteration 96/1000 | Loss: 0.00002011
Iteration 97/1000 | Loss: 0.00001800
Iteration 98/1000 | Loss: 0.00001659
Iteration 99/1000 | Loss: 0.00001598
Iteration 100/1000 | Loss: 0.00001542
Iteration 101/1000 | Loss: 0.00001501
Iteration 102/1000 | Loss: 0.00001469
Iteration 103/1000 | Loss: 0.00001449
Iteration 104/1000 | Loss: 0.00001436
Iteration 105/1000 | Loss: 0.00001428
Iteration 106/1000 | Loss: 0.00001424
Iteration 107/1000 | Loss: 0.00001422
Iteration 108/1000 | Loss: 0.00001420
Iteration 109/1000 | Loss: 0.00001420
Iteration 110/1000 | Loss: 0.00001420
Iteration 111/1000 | Loss: 0.00001419
Iteration 112/1000 | Loss: 0.00001418
Iteration 113/1000 | Loss: 0.00001418
Iteration 114/1000 | Loss: 0.00001416
Iteration 115/1000 | Loss: 0.00001416
Iteration 116/1000 | Loss: 0.00001415
Iteration 117/1000 | Loss: 0.00001415
Iteration 118/1000 | Loss: 0.00001414
Iteration 119/1000 | Loss: 0.00001413
Iteration 120/1000 | Loss: 0.00001412
Iteration 121/1000 | Loss: 0.00001412
Iteration 122/1000 | Loss: 0.00001411
Iteration 123/1000 | Loss: 0.00001411
Iteration 124/1000 | Loss: 0.00001411
Iteration 125/1000 | Loss: 0.00001410
Iteration 126/1000 | Loss: 0.00001410
Iteration 127/1000 | Loss: 0.00001410
Iteration 128/1000 | Loss: 0.00001409
Iteration 129/1000 | Loss: 0.00001409
Iteration 130/1000 | Loss: 0.00001409
Iteration 131/1000 | Loss: 0.00001409
Iteration 132/1000 | Loss: 0.00001408
Iteration 133/1000 | Loss: 0.00001408
Iteration 134/1000 | Loss: 0.00001408
Iteration 135/1000 | Loss: 0.00001408
Iteration 136/1000 | Loss: 0.00001407
Iteration 137/1000 | Loss: 0.00001407
Iteration 138/1000 | Loss: 0.00001407
Iteration 139/1000 | Loss: 0.00001407
Iteration 140/1000 | Loss: 0.00001407
Iteration 141/1000 | Loss: 0.00001407
Iteration 142/1000 | Loss: 0.00001407
Iteration 143/1000 | Loss: 0.00001406
Iteration 144/1000 | Loss: 0.00001406
Iteration 145/1000 | Loss: 0.00001406
Iteration 146/1000 | Loss: 0.00001406
Iteration 147/1000 | Loss: 0.00001406
Iteration 148/1000 | Loss: 0.00001406
Iteration 149/1000 | Loss: 0.00001405
Iteration 150/1000 | Loss: 0.00001405
Iteration 151/1000 | Loss: 0.00001405
Iteration 152/1000 | Loss: 0.00001405
Iteration 153/1000 | Loss: 0.00001405
Iteration 154/1000 | Loss: 0.00001405
Iteration 155/1000 | Loss: 0.00001405
Iteration 156/1000 | Loss: 0.00001405
Iteration 157/1000 | Loss: 0.00001404
Iteration 158/1000 | Loss: 0.00001404
Iteration 159/1000 | Loss: 0.00001404
Iteration 160/1000 | Loss: 0.00001404
Iteration 161/1000 | Loss: 0.00001403
Iteration 162/1000 | Loss: 0.00001403
Iteration 163/1000 | Loss: 0.00001403
Iteration 164/1000 | Loss: 0.00001403
Iteration 165/1000 | Loss: 0.00001402
Iteration 166/1000 | Loss: 0.00001402
Iteration 167/1000 | Loss: 0.00001402
Iteration 168/1000 | Loss: 0.00001402
Iteration 169/1000 | Loss: 0.00001402
Iteration 170/1000 | Loss: 0.00001402
Iteration 171/1000 | Loss: 0.00001402
Iteration 172/1000 | Loss: 0.00001402
Iteration 173/1000 | Loss: 0.00001402
Iteration 174/1000 | Loss: 0.00001402
Iteration 175/1000 | Loss: 0.00001402
Iteration 176/1000 | Loss: 0.00001402
Iteration 177/1000 | Loss: 0.00001402
Iteration 178/1000 | Loss: 0.00001402
Iteration 179/1000 | Loss: 0.00001402
Iteration 180/1000 | Loss: 0.00001402
Iteration 181/1000 | Loss: 0.00001402
Iteration 182/1000 | Loss: 0.00001402
Iteration 183/1000 | Loss: 0.00001402
Iteration 184/1000 | Loss: 0.00001402
Iteration 185/1000 | Loss: 0.00001402
Iteration 186/1000 | Loss: 0.00001402
Iteration 187/1000 | Loss: 0.00001402
Iteration 188/1000 | Loss: 0.00001402
Iteration 189/1000 | Loss: 0.00001402
Iteration 190/1000 | Loss: 0.00001402
Iteration 191/1000 | Loss: 0.00001402
Iteration 192/1000 | Loss: 0.00001402
Iteration 193/1000 | Loss: 0.00001402
Iteration 194/1000 | Loss: 0.00001402
Iteration 195/1000 | Loss: 0.00001402
Iteration 196/1000 | Loss: 0.00001402
Iteration 197/1000 | Loss: 0.00001402
Iteration 198/1000 | Loss: 0.00001402
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 198. Stopping optimization.
Last 5 losses: [1.4015567103342619e-05, 1.4015567103342619e-05, 1.4015567103342619e-05, 1.4015567103342619e-05, 1.4015567103342619e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4015567103342619e-05

Optimization complete. Final v2v error: 3.1392099857330322 mm

Highest mean error: 4.137589931488037 mm for frame 60

Lowest mean error: 2.7996716499328613 mm for frame 107

Saving results

Total time: 209.3392379283905
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ben_posed_008/1030/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_008/1030.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_008/1030
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01047838
Iteration 2/25 | Loss: 0.00190364
Iteration 3/25 | Loss: 0.00110184
Iteration 4/25 | Loss: 0.00123255
Iteration 5/25 | Loss: 0.00100551
Iteration 6/25 | Loss: 0.00089867
Iteration 7/25 | Loss: 0.00095151
Iteration 8/25 | Loss: 0.00083549
Iteration 9/25 | Loss: 0.00075473
Iteration 10/25 | Loss: 0.00070433
Iteration 11/25 | Loss: 0.00069297
Iteration 12/25 | Loss: 0.00068273
Iteration 13/25 | Loss: 0.00066748
Iteration 14/25 | Loss: 0.00066588
Iteration 15/25 | Loss: 0.00066114
Iteration 16/25 | Loss: 0.00065895
Iteration 17/25 | Loss: 0.00067012
Iteration 18/25 | Loss: 0.00065619
Iteration 19/25 | Loss: 0.00067393
Iteration 20/25 | Loss: 0.00066749
Iteration 21/25 | Loss: 0.00063727
Iteration 22/25 | Loss: 0.00062913
Iteration 23/25 | Loss: 0.00067078
Iteration 24/25 | Loss: 0.00063886
Iteration 25/25 | Loss: 0.00062765

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.53026628
Iteration 2/25 | Loss: 0.00023092
Iteration 3/25 | Loss: 0.00023092
Iteration 4/25 | Loss: 0.00023092
Iteration 5/25 | Loss: 0.00023092
Iteration 6/25 | Loss: 0.00023092
Iteration 7/25 | Loss: 0.00023092
Iteration 8/25 | Loss: 0.00023092
Iteration 9/25 | Loss: 0.00023092
Iteration 10/25 | Loss: 0.00023092
Iteration 11/25 | Loss: 0.00023092
Iteration 12/25 | Loss: 0.00023092
Iteration 13/25 | Loss: 0.00023092
Iteration 14/25 | Loss: 0.00023092
Iteration 15/25 | Loss: 0.00023092
Iteration 16/25 | Loss: 0.00023092
Iteration 17/25 | Loss: 0.00023092
Iteration 18/25 | Loss: 0.00023092
Iteration 19/25 | Loss: 0.00023092
Iteration 20/25 | Loss: 0.00023092
Iteration 21/25 | Loss: 0.00023092
Iteration 22/25 | Loss: 0.00023092
Iteration 23/25 | Loss: 0.00023092
Iteration 24/25 | Loss: 0.00023092
Iteration 25/25 | Loss: 0.00023092

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00023092
Iteration 2/1000 | Loss: 0.00004554
Iteration 3/1000 | Loss: 0.00004421
Iteration 4/1000 | Loss: 0.00005192
Iteration 5/1000 | Loss: 0.00003584
Iteration 6/1000 | Loss: 0.00002265
Iteration 7/1000 | Loss: 0.00003559
Iteration 8/1000 | Loss: 0.00004521
Iteration 9/1000 | Loss: 0.00011897
Iteration 10/1000 | Loss: 0.00005264
Iteration 11/1000 | Loss: 0.00003805
Iteration 12/1000 | Loss: 0.00005133
Iteration 13/1000 | Loss: 0.00003942
Iteration 14/1000 | Loss: 0.00004319
Iteration 15/1000 | Loss: 0.00005391
Iteration 16/1000 | Loss: 0.00019197
Iteration 17/1000 | Loss: 0.00003027
Iteration 18/1000 | Loss: 0.00002459
Iteration 19/1000 | Loss: 0.00001928
Iteration 20/1000 | Loss: 0.00017709
Iteration 21/1000 | Loss: 0.00001700
Iteration 22/1000 | Loss: 0.00001644
Iteration 23/1000 | Loss: 0.00001585
Iteration 24/1000 | Loss: 0.00001552
Iteration 25/1000 | Loss: 0.00001531
Iteration 26/1000 | Loss: 0.00001515
Iteration 27/1000 | Loss: 0.00001507
Iteration 28/1000 | Loss: 0.00001505
Iteration 29/1000 | Loss: 0.00001497
Iteration 30/1000 | Loss: 0.00001486
Iteration 31/1000 | Loss: 0.00001484
Iteration 32/1000 | Loss: 0.00001484
Iteration 33/1000 | Loss: 0.00001483
Iteration 34/1000 | Loss: 0.00001482
Iteration 35/1000 | Loss: 0.00001481
Iteration 36/1000 | Loss: 0.00001478
Iteration 37/1000 | Loss: 0.00001478
Iteration 38/1000 | Loss: 0.00001477
Iteration 39/1000 | Loss: 0.00001476
Iteration 40/1000 | Loss: 0.00001476
Iteration 41/1000 | Loss: 0.00001474
Iteration 42/1000 | Loss: 0.00001474
Iteration 43/1000 | Loss: 0.00001474
Iteration 44/1000 | Loss: 0.00001474
Iteration 45/1000 | Loss: 0.00001474
Iteration 46/1000 | Loss: 0.00001474
Iteration 47/1000 | Loss: 0.00001474
Iteration 48/1000 | Loss: 0.00001473
Iteration 49/1000 | Loss: 0.00001473
Iteration 50/1000 | Loss: 0.00001471
Iteration 51/1000 | Loss: 0.00001471
Iteration 52/1000 | Loss: 0.00001471
Iteration 53/1000 | Loss: 0.00001470
Iteration 54/1000 | Loss: 0.00001470
Iteration 55/1000 | Loss: 0.00001470
Iteration 56/1000 | Loss: 0.00001469
Iteration 57/1000 | Loss: 0.00001469
Iteration 58/1000 | Loss: 0.00001468
Iteration 59/1000 | Loss: 0.00001468
Iteration 60/1000 | Loss: 0.00001468
Iteration 61/1000 | Loss: 0.00001468
Iteration 62/1000 | Loss: 0.00001468
Iteration 63/1000 | Loss: 0.00001468
Iteration 64/1000 | Loss: 0.00001468
Iteration 65/1000 | Loss: 0.00001467
Iteration 66/1000 | Loss: 0.00001467
Iteration 67/1000 | Loss: 0.00001467
Iteration 68/1000 | Loss: 0.00001467
Iteration 69/1000 | Loss: 0.00001467
Iteration 70/1000 | Loss: 0.00001467
Iteration 71/1000 | Loss: 0.00001467
Iteration 72/1000 | Loss: 0.00001467
Iteration 73/1000 | Loss: 0.00005417
Iteration 74/1000 | Loss: 0.00001468
Iteration 75/1000 | Loss: 0.00001467
Iteration 76/1000 | Loss: 0.00001467
Iteration 77/1000 | Loss: 0.00001466
Iteration 78/1000 | Loss: 0.00001466
Iteration 79/1000 | Loss: 0.00001466
Iteration 80/1000 | Loss: 0.00001466
Iteration 81/1000 | Loss: 0.00001466
Iteration 82/1000 | Loss: 0.00001466
Iteration 83/1000 | Loss: 0.00001465
Iteration 84/1000 | Loss: 0.00001465
Iteration 85/1000 | Loss: 0.00001464
Iteration 86/1000 | Loss: 0.00001464
Iteration 87/1000 | Loss: 0.00001463
Iteration 88/1000 | Loss: 0.00001463
Iteration 89/1000 | Loss: 0.00001463
Iteration 90/1000 | Loss: 0.00001463
Iteration 91/1000 | Loss: 0.00001463
Iteration 92/1000 | Loss: 0.00001462
Iteration 93/1000 | Loss: 0.00001462
Iteration 94/1000 | Loss: 0.00001462
Iteration 95/1000 | Loss: 0.00001462
Iteration 96/1000 | Loss: 0.00001462
Iteration 97/1000 | Loss: 0.00001462
Iteration 98/1000 | Loss: 0.00001462
Iteration 99/1000 | Loss: 0.00001462
Iteration 100/1000 | Loss: 0.00001462
Iteration 101/1000 | Loss: 0.00001462
Iteration 102/1000 | Loss: 0.00001461
Iteration 103/1000 | Loss: 0.00001461
Iteration 104/1000 | Loss: 0.00001461
Iteration 105/1000 | Loss: 0.00001461
Iteration 106/1000 | Loss: 0.00001461
Iteration 107/1000 | Loss: 0.00001461
Iteration 108/1000 | Loss: 0.00001461
Iteration 109/1000 | Loss: 0.00001461
Iteration 110/1000 | Loss: 0.00001461
Iteration 111/1000 | Loss: 0.00001461
Iteration 112/1000 | Loss: 0.00001461
Iteration 113/1000 | Loss: 0.00001461
Iteration 114/1000 | Loss: 0.00001461
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 114. Stopping optimization.
Last 5 losses: [1.461109786760062e-05, 1.461109786760062e-05, 1.461109786760062e-05, 1.461109786760062e-05, 1.461109786760062e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.461109786760062e-05

Optimization complete. Final v2v error: 3.1686627864837646 mm

Highest mean error: 4.199926376342773 mm for frame 61

Lowest mean error: 2.5989303588867188 mm for frame 14

Saving results

Total time: 95.57538366317749
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ben_posed_008/1061/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_008/1061.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_008/1061
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00424287
Iteration 2/25 | Loss: 0.00100448
Iteration 3/25 | Loss: 0.00073132
Iteration 4/25 | Loss: 0.00068849
Iteration 5/25 | Loss: 0.00067941
Iteration 6/25 | Loss: 0.00067723
Iteration 7/25 | Loss: 0.00067638
Iteration 8/25 | Loss: 0.00067631
Iteration 9/25 | Loss: 0.00067631
Iteration 10/25 | Loss: 0.00067631
Iteration 11/25 | Loss: 0.00067631
Iteration 12/25 | Loss: 0.00067631
Iteration 13/25 | Loss: 0.00067631
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0006763132987543941, 0.0006763132987543941, 0.0006763132987543941, 0.0006763132987543941, 0.0006763132987543941]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006763132987543941

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 6.03046083
Iteration 2/25 | Loss: 0.00022281
Iteration 3/25 | Loss: 0.00022277
Iteration 4/25 | Loss: 0.00022277
Iteration 5/25 | Loss: 0.00022277
Iteration 6/25 | Loss: 0.00022277
Iteration 7/25 | Loss: 0.00022277
Iteration 8/25 | Loss: 0.00022277
Iteration 9/25 | Loss: 0.00022277
Iteration 10/25 | Loss: 0.00022277
Iteration 11/25 | Loss: 0.00022277
Iteration 12/25 | Loss: 0.00022277
Iteration 13/25 | Loss: 0.00022277
Iteration 14/25 | Loss: 0.00022277
Iteration 15/25 | Loss: 0.00022277
Iteration 16/25 | Loss: 0.00022277
Iteration 17/25 | Loss: 0.00022277
Iteration 18/25 | Loss: 0.00022277
Iteration 19/25 | Loss: 0.00022277
Iteration 20/25 | Loss: 0.00022277
Iteration 21/25 | Loss: 0.00022277
Iteration 22/25 | Loss: 0.00022277
Iteration 23/25 | Loss: 0.00022277
Iteration 24/25 | Loss: 0.00022277
Iteration 25/25 | Loss: 0.00022277

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00022277
Iteration 2/1000 | Loss: 0.00003824
Iteration 3/1000 | Loss: 0.00002759
Iteration 4/1000 | Loss: 0.00002588
Iteration 5/1000 | Loss: 0.00002463
Iteration 6/1000 | Loss: 0.00002360
Iteration 7/1000 | Loss: 0.00002296
Iteration 8/1000 | Loss: 0.00002263
Iteration 9/1000 | Loss: 0.00002236
Iteration 10/1000 | Loss: 0.00002215
Iteration 11/1000 | Loss: 0.00002203
Iteration 12/1000 | Loss: 0.00002197
Iteration 13/1000 | Loss: 0.00002193
Iteration 14/1000 | Loss: 0.00002192
Iteration 15/1000 | Loss: 0.00002192
Iteration 16/1000 | Loss: 0.00002191
Iteration 17/1000 | Loss: 0.00002190
Iteration 18/1000 | Loss: 0.00002190
Iteration 19/1000 | Loss: 0.00002189
Iteration 20/1000 | Loss: 0.00002188
Iteration 21/1000 | Loss: 0.00002187
Iteration 22/1000 | Loss: 0.00002187
Iteration 23/1000 | Loss: 0.00002187
Iteration 24/1000 | Loss: 0.00002186
Iteration 25/1000 | Loss: 0.00002186
Iteration 26/1000 | Loss: 0.00002184
Iteration 27/1000 | Loss: 0.00002184
Iteration 28/1000 | Loss: 0.00002184
Iteration 29/1000 | Loss: 0.00002183
Iteration 30/1000 | Loss: 0.00002183
Iteration 31/1000 | Loss: 0.00002183
Iteration 32/1000 | Loss: 0.00002183
Iteration 33/1000 | Loss: 0.00002183
Iteration 34/1000 | Loss: 0.00002183
Iteration 35/1000 | Loss: 0.00002183
Iteration 36/1000 | Loss: 0.00002183
Iteration 37/1000 | Loss: 0.00002182
Iteration 38/1000 | Loss: 0.00002182
Iteration 39/1000 | Loss: 0.00002182
Iteration 40/1000 | Loss: 0.00002182
Iteration 41/1000 | Loss: 0.00002182
Iteration 42/1000 | Loss: 0.00002182
Iteration 43/1000 | Loss: 0.00002182
Iteration 44/1000 | Loss: 0.00002182
Iteration 45/1000 | Loss: 0.00002181
Iteration 46/1000 | Loss: 0.00002181
Iteration 47/1000 | Loss: 0.00002180
Iteration 48/1000 | Loss: 0.00002180
Iteration 49/1000 | Loss: 0.00002179
Iteration 50/1000 | Loss: 0.00002179
Iteration 51/1000 | Loss: 0.00002179
Iteration 52/1000 | Loss: 0.00002178
Iteration 53/1000 | Loss: 0.00002178
Iteration 54/1000 | Loss: 0.00002178
Iteration 55/1000 | Loss: 0.00002177
Iteration 56/1000 | Loss: 0.00002177
Iteration 57/1000 | Loss: 0.00002177
Iteration 58/1000 | Loss: 0.00002177
Iteration 59/1000 | Loss: 0.00002176
Iteration 60/1000 | Loss: 0.00002176
Iteration 61/1000 | Loss: 0.00002176
Iteration 62/1000 | Loss: 0.00002175
Iteration 63/1000 | Loss: 0.00002175
Iteration 64/1000 | Loss: 0.00002175
Iteration 65/1000 | Loss: 0.00002175
Iteration 66/1000 | Loss: 0.00002175
Iteration 67/1000 | Loss: 0.00002175
Iteration 68/1000 | Loss: 0.00002175
Iteration 69/1000 | Loss: 0.00002175
Iteration 70/1000 | Loss: 0.00002175
Iteration 71/1000 | Loss: 0.00002175
Iteration 72/1000 | Loss: 0.00002175
Iteration 73/1000 | Loss: 0.00002175
Iteration 74/1000 | Loss: 0.00002175
Iteration 75/1000 | Loss: 0.00002175
Iteration 76/1000 | Loss: 0.00002175
Iteration 77/1000 | Loss: 0.00002175
Iteration 78/1000 | Loss: 0.00002174
Iteration 79/1000 | Loss: 0.00002174
Iteration 80/1000 | Loss: 0.00002174
Iteration 81/1000 | Loss: 0.00002173
Iteration 82/1000 | Loss: 0.00002173
Iteration 83/1000 | Loss: 0.00002173
Iteration 84/1000 | Loss: 0.00002173
Iteration 85/1000 | Loss: 0.00002173
Iteration 86/1000 | Loss: 0.00002173
Iteration 87/1000 | Loss: 0.00002173
Iteration 88/1000 | Loss: 0.00002172
Iteration 89/1000 | Loss: 0.00002172
Iteration 90/1000 | Loss: 0.00002172
Iteration 91/1000 | Loss: 0.00002172
Iteration 92/1000 | Loss: 0.00002171
Iteration 93/1000 | Loss: 0.00002171
Iteration 94/1000 | Loss: 0.00002171
Iteration 95/1000 | Loss: 0.00002171
Iteration 96/1000 | Loss: 0.00002170
Iteration 97/1000 | Loss: 0.00002170
Iteration 98/1000 | Loss: 0.00002170
Iteration 99/1000 | Loss: 0.00002169
Iteration 100/1000 | Loss: 0.00002169
Iteration 101/1000 | Loss: 0.00002169
Iteration 102/1000 | Loss: 0.00002169
Iteration 103/1000 | Loss: 0.00002169
Iteration 104/1000 | Loss: 0.00002168
Iteration 105/1000 | Loss: 0.00002168
Iteration 106/1000 | Loss: 0.00002168
Iteration 107/1000 | Loss: 0.00002168
Iteration 108/1000 | Loss: 0.00002168
Iteration 109/1000 | Loss: 0.00002168
Iteration 110/1000 | Loss: 0.00002168
Iteration 111/1000 | Loss: 0.00002168
Iteration 112/1000 | Loss: 0.00002168
Iteration 113/1000 | Loss: 0.00002168
Iteration 114/1000 | Loss: 0.00002168
Iteration 115/1000 | Loss: 0.00002168
Iteration 116/1000 | Loss: 0.00002168
Iteration 117/1000 | Loss: 0.00002168
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 117. Stopping optimization.
Last 5 losses: [2.167507955164183e-05, 2.167507955164183e-05, 2.167507955164183e-05, 2.167507955164183e-05, 2.167507955164183e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.167507955164183e-05

Optimization complete. Final v2v error: 3.900179862976074 mm

Highest mean error: 4.421302318572998 mm for frame 59

Lowest mean error: 3.5470786094665527 mm for frame 65

Saving results

Total time: 36.14819931983948
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ben_posed_008/1082/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_008/1082.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_008/1082
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00602864
Iteration 2/25 | Loss: 0.00076525
Iteration 3/25 | Loss: 0.00061989
Iteration 4/25 | Loss: 0.00059705
Iteration 5/25 | Loss: 0.00058739
Iteration 6/25 | Loss: 0.00058530
Iteration 7/25 | Loss: 0.00058463
Iteration 8/25 | Loss: 0.00058458
Iteration 9/25 | Loss: 0.00058458
Iteration 10/25 | Loss: 0.00058458
Iteration 11/25 | Loss: 0.00058458
Iteration 12/25 | Loss: 0.00058458
Iteration 13/25 | Loss: 0.00058458
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0005845837295055389, 0.0005845837295055389, 0.0005845837295055389, 0.0005845837295055389, 0.0005845837295055389]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005845837295055389

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.54305243
Iteration 2/25 | Loss: 0.00021690
Iteration 3/25 | Loss: 0.00021689
Iteration 4/25 | Loss: 0.00021689
Iteration 5/25 | Loss: 0.00021689
Iteration 6/25 | Loss: 0.00021689
Iteration 7/25 | Loss: 0.00021689
Iteration 8/25 | Loss: 0.00021689
Iteration 9/25 | Loss: 0.00021689
Iteration 10/25 | Loss: 0.00021689
Iteration 11/25 | Loss: 0.00021689
Iteration 12/25 | Loss: 0.00021689
Iteration 13/25 | Loss: 0.00021689
Iteration 14/25 | Loss: 0.00021689
Iteration 15/25 | Loss: 0.00021689
Iteration 16/25 | Loss: 0.00021689
Iteration 17/25 | Loss: 0.00021689
Iteration 18/25 | Loss: 0.00021689
Iteration 19/25 | Loss: 0.00021689
Iteration 20/25 | Loss: 0.00021689
Iteration 21/25 | Loss: 0.00021689
Iteration 22/25 | Loss: 0.00021689
Iteration 23/25 | Loss: 0.00021689
Iteration 24/25 | Loss: 0.00021689
Iteration 25/25 | Loss: 0.00021689

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00021689
Iteration 2/1000 | Loss: 0.00001964
Iteration 3/1000 | Loss: 0.00001436
Iteration 4/1000 | Loss: 0.00001343
Iteration 5/1000 | Loss: 0.00001295
Iteration 6/1000 | Loss: 0.00001276
Iteration 7/1000 | Loss: 0.00001255
Iteration 8/1000 | Loss: 0.00001242
Iteration 9/1000 | Loss: 0.00001239
Iteration 10/1000 | Loss: 0.00001238
Iteration 11/1000 | Loss: 0.00001225
Iteration 12/1000 | Loss: 0.00001213
Iteration 13/1000 | Loss: 0.00001212
Iteration 14/1000 | Loss: 0.00001212
Iteration 15/1000 | Loss: 0.00001210
Iteration 16/1000 | Loss: 0.00001209
Iteration 17/1000 | Loss: 0.00001209
Iteration 18/1000 | Loss: 0.00001209
Iteration 19/1000 | Loss: 0.00001207
Iteration 20/1000 | Loss: 0.00001207
Iteration 21/1000 | Loss: 0.00001205
Iteration 22/1000 | Loss: 0.00001205
Iteration 23/1000 | Loss: 0.00001204
Iteration 24/1000 | Loss: 0.00001204
Iteration 25/1000 | Loss: 0.00001204
Iteration 26/1000 | Loss: 0.00001201
Iteration 27/1000 | Loss: 0.00001200
Iteration 28/1000 | Loss: 0.00001200
Iteration 29/1000 | Loss: 0.00001197
Iteration 30/1000 | Loss: 0.00001196
Iteration 31/1000 | Loss: 0.00001196
Iteration 32/1000 | Loss: 0.00001196
Iteration 33/1000 | Loss: 0.00001196
Iteration 34/1000 | Loss: 0.00001196
Iteration 35/1000 | Loss: 0.00001196
Iteration 36/1000 | Loss: 0.00001196
Iteration 37/1000 | Loss: 0.00001194
Iteration 38/1000 | Loss: 0.00001194
Iteration 39/1000 | Loss: 0.00001194
Iteration 40/1000 | Loss: 0.00001194
Iteration 41/1000 | Loss: 0.00001193
Iteration 42/1000 | Loss: 0.00001193
Iteration 43/1000 | Loss: 0.00001193
Iteration 44/1000 | Loss: 0.00001193
Iteration 45/1000 | Loss: 0.00001193
Iteration 46/1000 | Loss: 0.00001193
Iteration 47/1000 | Loss: 0.00001193
Iteration 48/1000 | Loss: 0.00001193
Iteration 49/1000 | Loss: 0.00001192
Iteration 50/1000 | Loss: 0.00001192
Iteration 51/1000 | Loss: 0.00001192
Iteration 52/1000 | Loss: 0.00001191
Iteration 53/1000 | Loss: 0.00001191
Iteration 54/1000 | Loss: 0.00001190
Iteration 55/1000 | Loss: 0.00001190
Iteration 56/1000 | Loss: 0.00001190
Iteration 57/1000 | Loss: 0.00001190
Iteration 58/1000 | Loss: 0.00001190
Iteration 59/1000 | Loss: 0.00001190
Iteration 60/1000 | Loss: 0.00001190
Iteration 61/1000 | Loss: 0.00001190
Iteration 62/1000 | Loss: 0.00001190
Iteration 63/1000 | Loss: 0.00001190
Iteration 64/1000 | Loss: 0.00001189
Iteration 65/1000 | Loss: 0.00001189
Iteration 66/1000 | Loss: 0.00001189
Iteration 67/1000 | Loss: 0.00001189
Iteration 68/1000 | Loss: 0.00001188
Iteration 69/1000 | Loss: 0.00001188
Iteration 70/1000 | Loss: 0.00001188
Iteration 71/1000 | Loss: 0.00001188
Iteration 72/1000 | Loss: 0.00001188
Iteration 73/1000 | Loss: 0.00001187
Iteration 74/1000 | Loss: 0.00001187
Iteration 75/1000 | Loss: 0.00001187
Iteration 76/1000 | Loss: 0.00001186
Iteration 77/1000 | Loss: 0.00001186
Iteration 78/1000 | Loss: 0.00001186
Iteration 79/1000 | Loss: 0.00001185
Iteration 80/1000 | Loss: 0.00001185
Iteration 81/1000 | Loss: 0.00001185
Iteration 82/1000 | Loss: 0.00001185
Iteration 83/1000 | Loss: 0.00001184
Iteration 84/1000 | Loss: 0.00001184
Iteration 85/1000 | Loss: 0.00001184
Iteration 86/1000 | Loss: 0.00001184
Iteration 87/1000 | Loss: 0.00001184
Iteration 88/1000 | Loss: 0.00001184
Iteration 89/1000 | Loss: 0.00001184
Iteration 90/1000 | Loss: 0.00001184
Iteration 91/1000 | Loss: 0.00001183
Iteration 92/1000 | Loss: 0.00001183
Iteration 93/1000 | Loss: 0.00001183
Iteration 94/1000 | Loss: 0.00001183
Iteration 95/1000 | Loss: 0.00001183
Iteration 96/1000 | Loss: 0.00001183
Iteration 97/1000 | Loss: 0.00001182
Iteration 98/1000 | Loss: 0.00001182
Iteration 99/1000 | Loss: 0.00001182
Iteration 100/1000 | Loss: 0.00001182
Iteration 101/1000 | Loss: 0.00001182
Iteration 102/1000 | Loss: 0.00001182
Iteration 103/1000 | Loss: 0.00001182
Iteration 104/1000 | Loss: 0.00001182
Iteration 105/1000 | Loss: 0.00001182
Iteration 106/1000 | Loss: 0.00001181
Iteration 107/1000 | Loss: 0.00001181
Iteration 108/1000 | Loss: 0.00001181
Iteration 109/1000 | Loss: 0.00001180
Iteration 110/1000 | Loss: 0.00001180
Iteration 111/1000 | Loss: 0.00001180
Iteration 112/1000 | Loss: 0.00001180
Iteration 113/1000 | Loss: 0.00001180
Iteration 114/1000 | Loss: 0.00001180
Iteration 115/1000 | Loss: 0.00001180
Iteration 116/1000 | Loss: 0.00001180
Iteration 117/1000 | Loss: 0.00001180
Iteration 118/1000 | Loss: 0.00001180
Iteration 119/1000 | Loss: 0.00001180
Iteration 120/1000 | Loss: 0.00001180
Iteration 121/1000 | Loss: 0.00001179
Iteration 122/1000 | Loss: 0.00001179
Iteration 123/1000 | Loss: 0.00001179
Iteration 124/1000 | Loss: 0.00001179
Iteration 125/1000 | Loss: 0.00001179
Iteration 126/1000 | Loss: 0.00001179
Iteration 127/1000 | Loss: 0.00001179
Iteration 128/1000 | Loss: 0.00001179
Iteration 129/1000 | Loss: 0.00001179
Iteration 130/1000 | Loss: 0.00001179
Iteration 131/1000 | Loss: 0.00001179
Iteration 132/1000 | Loss: 0.00001179
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 132. Stopping optimization.
Last 5 losses: [1.1792974873969797e-05, 1.1792974873969797e-05, 1.1792974873969797e-05, 1.1792974873969797e-05, 1.1792974873969797e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1792974873969797e-05

Optimization complete. Final v2v error: 2.89315128326416 mm

Highest mean error: 3.344452381134033 mm for frame 87

Lowest mean error: 2.709256649017334 mm for frame 7

Saving results

Total time: 35.74915051460266
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ben_posed_008/1067/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_008/1067.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_008/1067
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00397733
Iteration 2/25 | Loss: 0.00069414
Iteration 3/25 | Loss: 0.00057561
Iteration 4/25 | Loss: 0.00056037
Iteration 5/25 | Loss: 0.00055489
Iteration 6/25 | Loss: 0.00055399
Iteration 7/25 | Loss: 0.00055378
Iteration 8/25 | Loss: 0.00055378
Iteration 9/25 | Loss: 0.00055378
Iteration 10/25 | Loss: 0.00055378
Iteration 11/25 | Loss: 0.00055378
Iteration 12/25 | Loss: 0.00055378
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0005537790711969137, 0.0005537790711969137, 0.0005537790711969137, 0.0005537790711969137, 0.0005537790711969137]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005537790711969137

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46145666
Iteration 2/25 | Loss: 0.00020574
Iteration 3/25 | Loss: 0.00020574
Iteration 4/25 | Loss: 0.00020574
Iteration 5/25 | Loss: 0.00020574
Iteration 6/25 | Loss: 0.00020574
Iteration 7/25 | Loss: 0.00020574
Iteration 8/25 | Loss: 0.00020574
Iteration 9/25 | Loss: 0.00020574
Iteration 10/25 | Loss: 0.00020574
Iteration 11/25 | Loss: 0.00020574
Iteration 12/25 | Loss: 0.00020574
Iteration 13/25 | Loss: 0.00020574
Iteration 14/25 | Loss: 0.00020574
Iteration 15/25 | Loss: 0.00020574
Iteration 16/25 | Loss: 0.00020574
Iteration 17/25 | Loss: 0.00020574
Iteration 18/25 | Loss: 0.00020574
Iteration 19/25 | Loss: 0.00020574
Iteration 20/25 | Loss: 0.00020574
Iteration 21/25 | Loss: 0.00020574
Iteration 22/25 | Loss: 0.00020574
Iteration 23/25 | Loss: 0.00020574
Iteration 24/25 | Loss: 0.00020574
Iteration 25/25 | Loss: 0.00020574

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00020574
Iteration 2/1000 | Loss: 0.00001829
Iteration 3/1000 | Loss: 0.00001248
Iteration 4/1000 | Loss: 0.00001160
Iteration 5/1000 | Loss: 0.00001110
Iteration 6/1000 | Loss: 0.00001081
Iteration 7/1000 | Loss: 0.00001080
Iteration 8/1000 | Loss: 0.00001077
Iteration 9/1000 | Loss: 0.00001071
Iteration 10/1000 | Loss: 0.00001069
Iteration 11/1000 | Loss: 0.00001068
Iteration 12/1000 | Loss: 0.00001062
Iteration 13/1000 | Loss: 0.00001056
Iteration 14/1000 | Loss: 0.00001056
Iteration 15/1000 | Loss: 0.00001054
Iteration 16/1000 | Loss: 0.00001052
Iteration 17/1000 | Loss: 0.00001051
Iteration 18/1000 | Loss: 0.00001051
Iteration 19/1000 | Loss: 0.00001050
Iteration 20/1000 | Loss: 0.00001050
Iteration 21/1000 | Loss: 0.00001049
Iteration 22/1000 | Loss: 0.00001047
Iteration 23/1000 | Loss: 0.00001047
Iteration 24/1000 | Loss: 0.00001039
Iteration 25/1000 | Loss: 0.00001033
Iteration 26/1000 | Loss: 0.00001033
Iteration 27/1000 | Loss: 0.00001032
Iteration 28/1000 | Loss: 0.00001029
Iteration 29/1000 | Loss: 0.00001028
Iteration 30/1000 | Loss: 0.00001028
Iteration 31/1000 | Loss: 0.00001027
Iteration 32/1000 | Loss: 0.00001025
Iteration 33/1000 | Loss: 0.00001024
Iteration 34/1000 | Loss: 0.00001024
Iteration 35/1000 | Loss: 0.00001024
Iteration 36/1000 | Loss: 0.00001024
Iteration 37/1000 | Loss: 0.00001024
Iteration 38/1000 | Loss: 0.00001024
Iteration 39/1000 | Loss: 0.00001024
Iteration 40/1000 | Loss: 0.00001023
Iteration 41/1000 | Loss: 0.00001023
Iteration 42/1000 | Loss: 0.00001023
Iteration 43/1000 | Loss: 0.00001022
Iteration 44/1000 | Loss: 0.00001021
Iteration 45/1000 | Loss: 0.00001020
Iteration 46/1000 | Loss: 0.00001020
Iteration 47/1000 | Loss: 0.00001020
Iteration 48/1000 | Loss: 0.00001020
Iteration 49/1000 | Loss: 0.00001019
Iteration 50/1000 | Loss: 0.00001019
Iteration 51/1000 | Loss: 0.00001019
Iteration 52/1000 | Loss: 0.00001019
Iteration 53/1000 | Loss: 0.00001019
Iteration 54/1000 | Loss: 0.00001018
Iteration 55/1000 | Loss: 0.00001018
Iteration 56/1000 | Loss: 0.00001017
Iteration 57/1000 | Loss: 0.00001017
Iteration 58/1000 | Loss: 0.00001017
Iteration 59/1000 | Loss: 0.00001017
Iteration 60/1000 | Loss: 0.00001016
Iteration 61/1000 | Loss: 0.00001016
Iteration 62/1000 | Loss: 0.00001016
Iteration 63/1000 | Loss: 0.00001016
Iteration 64/1000 | Loss: 0.00001016
Iteration 65/1000 | Loss: 0.00001016
Iteration 66/1000 | Loss: 0.00001015
Iteration 67/1000 | Loss: 0.00001015
Iteration 68/1000 | Loss: 0.00001015
Iteration 69/1000 | Loss: 0.00001015
Iteration 70/1000 | Loss: 0.00001015
Iteration 71/1000 | Loss: 0.00001015
Iteration 72/1000 | Loss: 0.00001015
Iteration 73/1000 | Loss: 0.00001015
Iteration 74/1000 | Loss: 0.00001015
Iteration 75/1000 | Loss: 0.00001015
Iteration 76/1000 | Loss: 0.00001015
Iteration 77/1000 | Loss: 0.00001014
Iteration 78/1000 | Loss: 0.00001014
Iteration 79/1000 | Loss: 0.00001014
Iteration 80/1000 | Loss: 0.00001014
Iteration 81/1000 | Loss: 0.00001014
Iteration 82/1000 | Loss: 0.00001013
Iteration 83/1000 | Loss: 0.00001012
Iteration 84/1000 | Loss: 0.00001012
Iteration 85/1000 | Loss: 0.00001012
Iteration 86/1000 | Loss: 0.00001012
Iteration 87/1000 | Loss: 0.00001012
Iteration 88/1000 | Loss: 0.00001012
Iteration 89/1000 | Loss: 0.00001012
Iteration 90/1000 | Loss: 0.00001012
Iteration 91/1000 | Loss: 0.00001011
Iteration 92/1000 | Loss: 0.00001011
Iteration 93/1000 | Loss: 0.00001011
Iteration 94/1000 | Loss: 0.00001011
Iteration 95/1000 | Loss: 0.00001011
Iteration 96/1000 | Loss: 0.00001011
Iteration 97/1000 | Loss: 0.00001011
Iteration 98/1000 | Loss: 0.00001011
Iteration 99/1000 | Loss: 0.00001011
Iteration 100/1000 | Loss: 0.00001011
Iteration 101/1000 | Loss: 0.00001011
Iteration 102/1000 | Loss: 0.00001011
Iteration 103/1000 | Loss: 0.00001011
Iteration 104/1000 | Loss: 0.00001011
Iteration 105/1000 | Loss: 0.00001011
Iteration 106/1000 | Loss: 0.00001011
Iteration 107/1000 | Loss: 0.00001011
Iteration 108/1000 | Loss: 0.00001011
Iteration 109/1000 | Loss: 0.00001011
Iteration 110/1000 | Loss: 0.00001011
Iteration 111/1000 | Loss: 0.00001011
Iteration 112/1000 | Loss: 0.00001011
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 112. Stopping optimization.
Last 5 losses: [1.010599316941807e-05, 1.010599316941807e-05, 1.010599316941807e-05, 1.010599316941807e-05, 1.010599316941807e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.010599316941807e-05

Optimization complete. Final v2v error: 2.7337987422943115 mm

Highest mean error: 2.8318445682525635 mm for frame 34

Lowest mean error: 2.6210827827453613 mm for frame 57

Saving results

Total time: 30.427467584609985
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ben_posed_008/1057/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_008/1057.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_008/1057
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01059650
Iteration 2/25 | Loss: 0.00197721
Iteration 3/25 | Loss: 0.00151695
Iteration 4/25 | Loss: 0.00142335
Iteration 5/25 | Loss: 0.00127156
Iteration 6/25 | Loss: 0.00092143
Iteration 7/25 | Loss: 0.00080750
Iteration 8/25 | Loss: 0.00077584
Iteration 9/25 | Loss: 0.00077320
Iteration 10/25 | Loss: 0.00077320
Iteration 11/25 | Loss: 0.00077320
Iteration 12/25 | Loss: 0.00077320
Iteration 13/25 | Loss: 0.00077320
Iteration 14/25 | Loss: 0.00077320
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0007731954683549702, 0.0007731954683549702, 0.0007731954683549702, 0.0007731954683549702, 0.0007731954683549702]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007731954683549702

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43846154
Iteration 2/25 | Loss: 0.00015321
Iteration 3/25 | Loss: 0.00015321
Iteration 4/25 | Loss: 0.00015321
Iteration 5/25 | Loss: 0.00015321
Iteration 6/25 | Loss: 0.00015321
Iteration 7/25 | Loss: 0.00015321
Iteration 8/25 | Loss: 0.00015321
Iteration 9/25 | Loss: 0.00015321
Iteration 10/25 | Loss: 0.00015321
Iteration 11/25 | Loss: 0.00015321
Iteration 12/25 | Loss: 0.00015321
Iteration 13/25 | Loss: 0.00015321
Iteration 14/25 | Loss: 0.00015321
Iteration 15/25 | Loss: 0.00015321
Iteration 16/25 | Loss: 0.00015321
Iteration 17/25 | Loss: 0.00015321
Iteration 18/25 | Loss: 0.00015321
Iteration 19/25 | Loss: 0.00015321
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.00015320652164518833, 0.00015320652164518833, 0.00015320652164518833, 0.00015320652164518833, 0.00015320652164518833]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00015320652164518833

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00015321
Iteration 2/1000 | Loss: 0.00003034
Iteration 3/1000 | Loss: 0.00002623
Iteration 4/1000 | Loss: 0.00002497
Iteration 5/1000 | Loss: 0.00002399
Iteration 6/1000 | Loss: 0.00002346
Iteration 7/1000 | Loss: 0.00002325
Iteration 8/1000 | Loss: 0.00002303
Iteration 9/1000 | Loss: 0.00002289
Iteration 10/1000 | Loss: 0.00002289
Iteration 11/1000 | Loss: 0.00002288
Iteration 12/1000 | Loss: 0.00002285
Iteration 13/1000 | Loss: 0.00002283
Iteration 14/1000 | Loss: 0.00002283
Iteration 15/1000 | Loss: 0.00002283
Iteration 16/1000 | Loss: 0.00002282
Iteration 17/1000 | Loss: 0.00002281
Iteration 18/1000 | Loss: 0.00002281
Iteration 19/1000 | Loss: 0.00002280
Iteration 20/1000 | Loss: 0.00002279
Iteration 21/1000 | Loss: 0.00002278
Iteration 22/1000 | Loss: 0.00002278
Iteration 23/1000 | Loss: 0.00002278
Iteration 24/1000 | Loss: 0.00002278
Iteration 25/1000 | Loss: 0.00002278
Iteration 26/1000 | Loss: 0.00002278
Iteration 27/1000 | Loss: 0.00002278
Iteration 28/1000 | Loss: 0.00002278
Iteration 29/1000 | Loss: 0.00002278
Iteration 30/1000 | Loss: 0.00002278
Iteration 31/1000 | Loss: 0.00002278
Iteration 32/1000 | Loss: 0.00002277
Iteration 33/1000 | Loss: 0.00002277
Iteration 34/1000 | Loss: 0.00002277
Iteration 35/1000 | Loss: 0.00002277
Iteration 36/1000 | Loss: 0.00002277
Iteration 37/1000 | Loss: 0.00002277
Iteration 38/1000 | Loss: 0.00002276
Iteration 39/1000 | Loss: 0.00002275
Iteration 40/1000 | Loss: 0.00002274
Iteration 41/1000 | Loss: 0.00002273
Iteration 42/1000 | Loss: 0.00002273
Iteration 43/1000 | Loss: 0.00002273
Iteration 44/1000 | Loss: 0.00002273
Iteration 45/1000 | Loss: 0.00002273
Iteration 46/1000 | Loss: 0.00002273
Iteration 47/1000 | Loss: 0.00002272
Iteration 48/1000 | Loss: 0.00002272
Iteration 49/1000 | Loss: 0.00002272
Iteration 50/1000 | Loss: 0.00002271
Iteration 51/1000 | Loss: 0.00002271
Iteration 52/1000 | Loss: 0.00002271
Iteration 53/1000 | Loss: 0.00002271
Iteration 54/1000 | Loss: 0.00002270
Iteration 55/1000 | Loss: 0.00002270
Iteration 56/1000 | Loss: 0.00002270
Iteration 57/1000 | Loss: 0.00002270
Iteration 58/1000 | Loss: 0.00002270
Iteration 59/1000 | Loss: 0.00002270
Iteration 60/1000 | Loss: 0.00002270
Iteration 61/1000 | Loss: 0.00002269
Iteration 62/1000 | Loss: 0.00002269
Iteration 63/1000 | Loss: 0.00002269
Iteration 64/1000 | Loss: 0.00002269
Iteration 65/1000 | Loss: 0.00002269
Iteration 66/1000 | Loss: 0.00002269
Iteration 67/1000 | Loss: 0.00002269
Iteration 68/1000 | Loss: 0.00002269
Iteration 69/1000 | Loss: 0.00002269
Iteration 70/1000 | Loss: 0.00002269
Iteration 71/1000 | Loss: 0.00002269
Iteration 72/1000 | Loss: 0.00002269
Iteration 73/1000 | Loss: 0.00002269
Iteration 74/1000 | Loss: 0.00002269
Iteration 75/1000 | Loss: 0.00002269
Iteration 76/1000 | Loss: 0.00002269
Iteration 77/1000 | Loss: 0.00002268
Iteration 78/1000 | Loss: 0.00002268
Iteration 79/1000 | Loss: 0.00002268
Iteration 80/1000 | Loss: 0.00002268
Iteration 81/1000 | Loss: 0.00002268
Iteration 82/1000 | Loss: 0.00002268
Iteration 83/1000 | Loss: 0.00002268
Iteration 84/1000 | Loss: 0.00002268
Iteration 85/1000 | Loss: 0.00002268
Iteration 86/1000 | Loss: 0.00002268
Iteration 87/1000 | Loss: 0.00002268
Iteration 88/1000 | Loss: 0.00002268
Iteration 89/1000 | Loss: 0.00002268
Iteration 90/1000 | Loss: 0.00002268
Iteration 91/1000 | Loss: 0.00002268
Iteration 92/1000 | Loss: 0.00002267
Iteration 93/1000 | Loss: 0.00002267
Iteration 94/1000 | Loss: 0.00002267
Iteration 95/1000 | Loss: 0.00002267
Iteration 96/1000 | Loss: 0.00002266
Iteration 97/1000 | Loss: 0.00002266
Iteration 98/1000 | Loss: 0.00002266
Iteration 99/1000 | Loss: 0.00002266
Iteration 100/1000 | Loss: 0.00002266
Iteration 101/1000 | Loss: 0.00002266
Iteration 102/1000 | Loss: 0.00002266
Iteration 103/1000 | Loss: 0.00002266
Iteration 104/1000 | Loss: 0.00002266
Iteration 105/1000 | Loss: 0.00002266
Iteration 106/1000 | Loss: 0.00002266
Iteration 107/1000 | Loss: 0.00002266
Iteration 108/1000 | Loss: 0.00002266
Iteration 109/1000 | Loss: 0.00002266
Iteration 110/1000 | Loss: 0.00002266
Iteration 111/1000 | Loss: 0.00002266
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 111. Stopping optimization.
Last 5 losses: [2.265614784846548e-05, 2.265614784846548e-05, 2.265614784846548e-05, 2.265614784846548e-05, 2.265614784846548e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.265614784846548e-05

Optimization complete. Final v2v error: 3.9959652423858643 mm

Highest mean error: 4.365927696228027 mm for frame 28

Lowest mean error: 3.866243600845337 mm for frame 8

Saving results

Total time: 38.89904570579529
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ben_posed_008/1025/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_008/1025.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_008/1025
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00792191
Iteration 2/25 | Loss: 0.00202750
Iteration 3/25 | Loss: 0.00108193
Iteration 4/25 | Loss: 0.00085615
Iteration 5/25 | Loss: 0.00080494
Iteration 6/25 | Loss: 0.00072322
Iteration 7/25 | Loss: 0.00073610
Iteration 8/25 | Loss: 0.00066781
Iteration 9/25 | Loss: 0.00062969
Iteration 10/25 | Loss: 0.00062956
Iteration 11/25 | Loss: 0.00060336
Iteration 12/25 | Loss: 0.00059070
Iteration 13/25 | Loss: 0.00058493
Iteration 14/25 | Loss: 0.00059161
Iteration 15/25 | Loss: 0.00058298
Iteration 16/25 | Loss: 0.00058188
Iteration 17/25 | Loss: 0.00058153
Iteration 18/25 | Loss: 0.00058096
Iteration 19/25 | Loss: 0.00058506
Iteration 20/25 | Loss: 0.00057973
Iteration 21/25 | Loss: 0.00057874
Iteration 22/25 | Loss: 0.00057678
Iteration 23/25 | Loss: 0.00057591
Iteration 24/25 | Loss: 0.00057674
Iteration 25/25 | Loss: 0.00057425

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.10846019
Iteration 2/25 | Loss: 0.00019354
Iteration 3/25 | Loss: 0.00019354
Iteration 4/25 | Loss: 0.00019354
Iteration 5/25 | Loss: 0.00019354
Iteration 6/25 | Loss: 0.00019354
Iteration 7/25 | Loss: 0.00019354
Iteration 8/25 | Loss: 0.00019354
Iteration 9/25 | Loss: 0.00019354
Iteration 10/25 | Loss: 0.00019354
Iteration 11/25 | Loss: 0.00019354
Iteration 12/25 | Loss: 0.00019354
Iteration 13/25 | Loss: 0.00019354
Iteration 14/25 | Loss: 0.00019354
Iteration 15/25 | Loss: 0.00019354
Iteration 16/25 | Loss: 0.00019354
Iteration 17/25 | Loss: 0.00019354
Iteration 18/25 | Loss: 0.00019354
Iteration 19/25 | Loss: 0.00019354
Iteration 20/25 | Loss: 0.00019354
Iteration 21/25 | Loss: 0.00019354
Iteration 22/25 | Loss: 0.00019354
Iteration 23/25 | Loss: 0.00019354
Iteration 24/25 | Loss: 0.00019354
Iteration 25/25 | Loss: 0.00019354

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00019354
Iteration 2/1000 | Loss: 0.00003720
Iteration 3/1000 | Loss: 0.00002977
Iteration 4/1000 | Loss: 0.00003629
Iteration 5/1000 | Loss: 0.00003952
Iteration 6/1000 | Loss: 0.00002114
Iteration 7/1000 | Loss: 0.00005106
Iteration 8/1000 | Loss: 0.00011014
Iteration 9/1000 | Loss: 0.00004811
Iteration 10/1000 | Loss: 0.00001900
Iteration 11/1000 | Loss: 0.00153645
Iteration 12/1000 | Loss: 0.00009554
Iteration 13/1000 | Loss: 0.00007348
Iteration 14/1000 | Loss: 0.00005374
Iteration 15/1000 | Loss: 0.00001676
Iteration 16/1000 | Loss: 0.00001559
Iteration 17/1000 | Loss: 0.00001408
Iteration 18/1000 | Loss: 0.00002146
Iteration 19/1000 | Loss: 0.00002327
Iteration 20/1000 | Loss: 0.00005458
Iteration 21/1000 | Loss: 0.00001948
Iteration 22/1000 | Loss: 0.00001535
Iteration 23/1000 | Loss: 0.00001230
Iteration 24/1000 | Loss: 0.00001396
Iteration 25/1000 | Loss: 0.00001396
Iteration 26/1000 | Loss: 0.00002547
Iteration 27/1000 | Loss: 0.00001208
Iteration 28/1000 | Loss: 0.00001191
Iteration 29/1000 | Loss: 0.00001190
Iteration 30/1000 | Loss: 0.00001190
Iteration 31/1000 | Loss: 0.00001188
Iteration 32/1000 | Loss: 0.00001187
Iteration 33/1000 | Loss: 0.00001187
Iteration 34/1000 | Loss: 0.00001884
Iteration 35/1000 | Loss: 0.00001180
Iteration 36/1000 | Loss: 0.00001179
Iteration 37/1000 | Loss: 0.00001177
Iteration 38/1000 | Loss: 0.00001177
Iteration 39/1000 | Loss: 0.00001177
Iteration 40/1000 | Loss: 0.00001177
Iteration 41/1000 | Loss: 0.00001177
Iteration 42/1000 | Loss: 0.00001176
Iteration 43/1000 | Loss: 0.00001176
Iteration 44/1000 | Loss: 0.00001176
Iteration 45/1000 | Loss: 0.00003608
Iteration 46/1000 | Loss: 0.00001180
Iteration 47/1000 | Loss: 0.00003467
Iteration 48/1000 | Loss: 0.00003063
Iteration 49/1000 | Loss: 0.00001169
Iteration 50/1000 | Loss: 0.00001394
Iteration 51/1000 | Loss: 0.00002567
Iteration 52/1000 | Loss: 0.00001164
Iteration 53/1000 | Loss: 0.00001163
Iteration 54/1000 | Loss: 0.00001163
Iteration 55/1000 | Loss: 0.00001163
Iteration 56/1000 | Loss: 0.00001163
Iteration 57/1000 | Loss: 0.00001163
Iteration 58/1000 | Loss: 0.00001163
Iteration 59/1000 | Loss: 0.00001163
Iteration 60/1000 | Loss: 0.00001163
Iteration 61/1000 | Loss: 0.00001163
Iteration 62/1000 | Loss: 0.00001163
Iteration 63/1000 | Loss: 0.00001163
Iteration 64/1000 | Loss: 0.00001163
Iteration 65/1000 | Loss: 0.00001163
Iteration 66/1000 | Loss: 0.00001163
Iteration 67/1000 | Loss: 0.00001163
Iteration 68/1000 | Loss: 0.00001163
Iteration 69/1000 | Loss: 0.00001163
Iteration 70/1000 | Loss: 0.00001163
Iteration 71/1000 | Loss: 0.00001163
Iteration 72/1000 | Loss: 0.00001163
Iteration 73/1000 | Loss: 0.00001163
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 73. Stopping optimization.
Last 5 losses: [1.162589069281239e-05, 1.162589069281239e-05, 1.162589069281239e-05, 1.162589069281239e-05, 1.162589069281239e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.162589069281239e-05

Optimization complete. Final v2v error: 2.8944590091705322 mm

Highest mean error: 3.60962176322937 mm for frame 181

Lowest mean error: 2.677898645401001 mm for frame 148

Saving results

Total time: 99.92132043838501
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ben_posed_008/1055/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_008/1055.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_008/1055
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00782016
Iteration 2/25 | Loss: 0.00099572
Iteration 3/25 | Loss: 0.00074445
Iteration 4/25 | Loss: 0.00069021
Iteration 5/25 | Loss: 0.00068203
Iteration 6/25 | Loss: 0.00068105
Iteration 7/25 | Loss: 0.00068105
Iteration 8/25 | Loss: 0.00068105
Iteration 9/25 | Loss: 0.00068105
Iteration 10/25 | Loss: 0.00068105
Iteration 11/25 | Loss: 0.00068105
Iteration 12/25 | Loss: 0.00068105
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0006810497725382447, 0.0006810497725382447, 0.0006810497725382447, 0.0006810497725382447, 0.0006810497725382447]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006810497725382447

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44357789
Iteration 2/25 | Loss: 0.00022266
Iteration 3/25 | Loss: 0.00022261
Iteration 4/25 | Loss: 0.00022261
Iteration 5/25 | Loss: 0.00022261
Iteration 6/25 | Loss: 0.00022261
Iteration 7/25 | Loss: 0.00022261
Iteration 8/25 | Loss: 0.00022261
Iteration 9/25 | Loss: 0.00022261
Iteration 10/25 | Loss: 0.00022261
Iteration 11/25 | Loss: 0.00022261
Iteration 12/25 | Loss: 0.00022261
Iteration 13/25 | Loss: 0.00022261
Iteration 14/25 | Loss: 0.00022261
Iteration 15/25 | Loss: 0.00022261
Iteration 16/25 | Loss: 0.00022261
Iteration 17/25 | Loss: 0.00022261
Iteration 18/25 | Loss: 0.00022261
Iteration 19/25 | Loss: 0.00022261
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.00022260952391661704, 0.00022260952391661704, 0.00022260952391661704, 0.00022260952391661704, 0.00022260952391661704]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00022260952391661704

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00022261
Iteration 2/1000 | Loss: 0.00005317
Iteration 3/1000 | Loss: 0.00003733
Iteration 4/1000 | Loss: 0.00003239
Iteration 5/1000 | Loss: 0.00002985
Iteration 6/1000 | Loss: 0.00002840
Iteration 7/1000 | Loss: 0.00002724
Iteration 8/1000 | Loss: 0.00002653
Iteration 9/1000 | Loss: 0.00002613
Iteration 10/1000 | Loss: 0.00002583
Iteration 11/1000 | Loss: 0.00002552
Iteration 12/1000 | Loss: 0.00002533
Iteration 13/1000 | Loss: 0.00002532
Iteration 14/1000 | Loss: 0.00002530
Iteration 15/1000 | Loss: 0.00002525
Iteration 16/1000 | Loss: 0.00002523
Iteration 17/1000 | Loss: 0.00002509
Iteration 18/1000 | Loss: 0.00002507
Iteration 19/1000 | Loss: 0.00002507
Iteration 20/1000 | Loss: 0.00002506
Iteration 21/1000 | Loss: 0.00002505
Iteration 22/1000 | Loss: 0.00002503
Iteration 23/1000 | Loss: 0.00002503
Iteration 24/1000 | Loss: 0.00002503
Iteration 25/1000 | Loss: 0.00002503
Iteration 26/1000 | Loss: 0.00002503
Iteration 27/1000 | Loss: 0.00002503
Iteration 28/1000 | Loss: 0.00002503
Iteration 29/1000 | Loss: 0.00002503
Iteration 30/1000 | Loss: 0.00002502
Iteration 31/1000 | Loss: 0.00002502
Iteration 32/1000 | Loss: 0.00002502
Iteration 33/1000 | Loss: 0.00002502
Iteration 34/1000 | Loss: 0.00002501
Iteration 35/1000 | Loss: 0.00002501
Iteration 36/1000 | Loss: 0.00002501
Iteration 37/1000 | Loss: 0.00002501
Iteration 38/1000 | Loss: 0.00002501
Iteration 39/1000 | Loss: 0.00002501
Iteration 40/1000 | Loss: 0.00002501
Iteration 41/1000 | Loss: 0.00002500
Iteration 42/1000 | Loss: 0.00002500
Iteration 43/1000 | Loss: 0.00002500
Iteration 44/1000 | Loss: 0.00002500
Iteration 45/1000 | Loss: 0.00002500
Iteration 46/1000 | Loss: 0.00002500
Iteration 47/1000 | Loss: 0.00002500
Iteration 48/1000 | Loss: 0.00002500
Iteration 49/1000 | Loss: 0.00002500
Iteration 50/1000 | Loss: 0.00002500
Iteration 51/1000 | Loss: 0.00002500
Iteration 52/1000 | Loss: 0.00002499
Iteration 53/1000 | Loss: 0.00002499
Iteration 54/1000 | Loss: 0.00002499
Iteration 55/1000 | Loss: 0.00002499
Iteration 56/1000 | Loss: 0.00002499
Iteration 57/1000 | Loss: 0.00002499
Iteration 58/1000 | Loss: 0.00002499
Iteration 59/1000 | Loss: 0.00002498
Iteration 60/1000 | Loss: 0.00002498
Iteration 61/1000 | Loss: 0.00002498
Iteration 62/1000 | Loss: 0.00002498
Iteration 63/1000 | Loss: 0.00002498
Iteration 64/1000 | Loss: 0.00002497
Iteration 65/1000 | Loss: 0.00002497
Iteration 66/1000 | Loss: 0.00002497
Iteration 67/1000 | Loss: 0.00002497
Iteration 68/1000 | Loss: 0.00002497
Iteration 69/1000 | Loss: 0.00002497
Iteration 70/1000 | Loss: 0.00002497
Iteration 71/1000 | Loss: 0.00002497
Iteration 72/1000 | Loss: 0.00002497
Iteration 73/1000 | Loss: 0.00002497
Iteration 74/1000 | Loss: 0.00002497
Iteration 75/1000 | Loss: 0.00002497
Iteration 76/1000 | Loss: 0.00002496
Iteration 77/1000 | Loss: 0.00002496
Iteration 78/1000 | Loss: 0.00002496
Iteration 79/1000 | Loss: 0.00002496
Iteration 80/1000 | Loss: 0.00002496
Iteration 81/1000 | Loss: 0.00002496
Iteration 82/1000 | Loss: 0.00002496
Iteration 83/1000 | Loss: 0.00002496
Iteration 84/1000 | Loss: 0.00002496
Iteration 85/1000 | Loss: 0.00002496
Iteration 86/1000 | Loss: 0.00002496
Iteration 87/1000 | Loss: 0.00002496
Iteration 88/1000 | Loss: 0.00002496
Iteration 89/1000 | Loss: 0.00002496
Iteration 90/1000 | Loss: 0.00002495
Iteration 91/1000 | Loss: 0.00002495
Iteration 92/1000 | Loss: 0.00002495
Iteration 93/1000 | Loss: 0.00002495
Iteration 94/1000 | Loss: 0.00002495
Iteration 95/1000 | Loss: 0.00002495
Iteration 96/1000 | Loss: 0.00002495
Iteration 97/1000 | Loss: 0.00002495
Iteration 98/1000 | Loss: 0.00002495
Iteration 99/1000 | Loss: 0.00002495
Iteration 100/1000 | Loss: 0.00002495
Iteration 101/1000 | Loss: 0.00002495
Iteration 102/1000 | Loss: 0.00002495
Iteration 103/1000 | Loss: 0.00002495
Iteration 104/1000 | Loss: 0.00002495
Iteration 105/1000 | Loss: 0.00002495
Iteration 106/1000 | Loss: 0.00002495
Iteration 107/1000 | Loss: 0.00002495
Iteration 108/1000 | Loss: 0.00002495
Iteration 109/1000 | Loss: 0.00002495
Iteration 110/1000 | Loss: 0.00002495
Iteration 111/1000 | Loss: 0.00002494
Iteration 112/1000 | Loss: 0.00002494
Iteration 113/1000 | Loss: 0.00002494
Iteration 114/1000 | Loss: 0.00002494
Iteration 115/1000 | Loss: 0.00002494
Iteration 116/1000 | Loss: 0.00002494
Iteration 117/1000 | Loss: 0.00002494
Iteration 118/1000 | Loss: 0.00002494
Iteration 119/1000 | Loss: 0.00002494
Iteration 120/1000 | Loss: 0.00002494
Iteration 121/1000 | Loss: 0.00002494
Iteration 122/1000 | Loss: 0.00002494
Iteration 123/1000 | Loss: 0.00002494
Iteration 124/1000 | Loss: 0.00002494
Iteration 125/1000 | Loss: 0.00002494
Iteration 126/1000 | Loss: 0.00002494
Iteration 127/1000 | Loss: 0.00002494
Iteration 128/1000 | Loss: 0.00002494
Iteration 129/1000 | Loss: 0.00002494
Iteration 130/1000 | Loss: 0.00002494
Iteration 131/1000 | Loss: 0.00002494
Iteration 132/1000 | Loss: 0.00002494
Iteration 133/1000 | Loss: 0.00002494
Iteration 134/1000 | Loss: 0.00002494
Iteration 135/1000 | Loss: 0.00002494
Iteration 136/1000 | Loss: 0.00002494
Iteration 137/1000 | Loss: 0.00002494
Iteration 138/1000 | Loss: 0.00002494
Iteration 139/1000 | Loss: 0.00002494
Iteration 140/1000 | Loss: 0.00002494
Iteration 141/1000 | Loss: 0.00002494
Iteration 142/1000 | Loss: 0.00002494
Iteration 143/1000 | Loss: 0.00002494
Iteration 144/1000 | Loss: 0.00002494
Iteration 145/1000 | Loss: 0.00002494
Iteration 146/1000 | Loss: 0.00002494
Iteration 147/1000 | Loss: 0.00002494
Iteration 148/1000 | Loss: 0.00002494
Iteration 149/1000 | Loss: 0.00002494
Iteration 150/1000 | Loss: 0.00002494
Iteration 151/1000 | Loss: 0.00002494
Iteration 152/1000 | Loss: 0.00002494
Iteration 153/1000 | Loss: 0.00002494
Iteration 154/1000 | Loss: 0.00002494
Iteration 155/1000 | Loss: 0.00002494
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 155. Stopping optimization.
Last 5 losses: [2.4942721211118624e-05, 2.4942721211118624e-05, 2.4942721211118624e-05, 2.4942721211118624e-05, 2.4942721211118624e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4942721211118624e-05

Optimization complete. Final v2v error: 4.1255364418029785 mm

Highest mean error: 4.394223213195801 mm for frame 182

Lowest mean error: 3.7306642532348633 mm for frame 209

Saving results

Total time: 41.18896818161011
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ben_posed_008/1013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_008/1013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_008/1013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00528475
Iteration 2/25 | Loss: 0.00084045
Iteration 3/25 | Loss: 0.00065658
Iteration 4/25 | Loss: 0.00061949
Iteration 5/25 | Loss: 0.00061187
Iteration 6/25 | Loss: 0.00061028
Iteration 7/25 | Loss: 0.00060988
Iteration 8/25 | Loss: 0.00060988
Iteration 9/25 | Loss: 0.00060988
Iteration 10/25 | Loss: 0.00060988
Iteration 11/25 | Loss: 0.00060988
Iteration 12/25 | Loss: 0.00060988
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0006098797312006354, 0.0006098797312006354, 0.0006098797312006354, 0.0006098797312006354, 0.0006098797312006354]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006098797312006354

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.22118592
Iteration 2/25 | Loss: 0.00021715
Iteration 3/25 | Loss: 0.00021715
Iteration 4/25 | Loss: 0.00021715
Iteration 5/25 | Loss: 0.00021715
Iteration 6/25 | Loss: 0.00021715
Iteration 7/25 | Loss: 0.00021714
Iteration 8/25 | Loss: 0.00021714
Iteration 9/25 | Loss: 0.00021714
Iteration 10/25 | Loss: 0.00021714
Iteration 11/25 | Loss: 0.00021714
Iteration 12/25 | Loss: 0.00021714
Iteration 13/25 | Loss: 0.00021714
Iteration 14/25 | Loss: 0.00021714
Iteration 15/25 | Loss: 0.00021714
Iteration 16/25 | Loss: 0.00021714
Iteration 17/25 | Loss: 0.00021714
Iteration 18/25 | Loss: 0.00021714
Iteration 19/25 | Loss: 0.00021714
Iteration 20/25 | Loss: 0.00021714
Iteration 21/25 | Loss: 0.00021714
Iteration 22/25 | Loss: 0.00021714
Iteration 23/25 | Loss: 0.00021714
Iteration 24/25 | Loss: 0.00021714
Iteration 25/25 | Loss: 0.00021714

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00021714
Iteration 2/1000 | Loss: 0.00001929
Iteration 3/1000 | Loss: 0.00001292
Iteration 4/1000 | Loss: 0.00001175
Iteration 5/1000 | Loss: 0.00001114
Iteration 6/1000 | Loss: 0.00001092
Iteration 7/1000 | Loss: 0.00001090
Iteration 8/1000 | Loss: 0.00001087
Iteration 9/1000 | Loss: 0.00001081
Iteration 10/1000 | Loss: 0.00001062
Iteration 11/1000 | Loss: 0.00001060
Iteration 12/1000 | Loss: 0.00001060
Iteration 13/1000 | Loss: 0.00001059
Iteration 14/1000 | Loss: 0.00001054
Iteration 15/1000 | Loss: 0.00001053
Iteration 16/1000 | Loss: 0.00001052
Iteration 17/1000 | Loss: 0.00001052
Iteration 18/1000 | Loss: 0.00001052
Iteration 19/1000 | Loss: 0.00001047
Iteration 20/1000 | Loss: 0.00001042
Iteration 21/1000 | Loss: 0.00001038
Iteration 22/1000 | Loss: 0.00001038
Iteration 23/1000 | Loss: 0.00001035
Iteration 24/1000 | Loss: 0.00001035
Iteration 25/1000 | Loss: 0.00001034
Iteration 26/1000 | Loss: 0.00001034
Iteration 27/1000 | Loss: 0.00001034
Iteration 28/1000 | Loss: 0.00001033
Iteration 29/1000 | Loss: 0.00001033
Iteration 30/1000 | Loss: 0.00001032
Iteration 31/1000 | Loss: 0.00001032
Iteration 32/1000 | Loss: 0.00001031
Iteration 33/1000 | Loss: 0.00001031
Iteration 34/1000 | Loss: 0.00001030
Iteration 35/1000 | Loss: 0.00001030
Iteration 36/1000 | Loss: 0.00001029
Iteration 37/1000 | Loss: 0.00001029
Iteration 38/1000 | Loss: 0.00001028
Iteration 39/1000 | Loss: 0.00001028
Iteration 40/1000 | Loss: 0.00001028
Iteration 41/1000 | Loss: 0.00001028
Iteration 42/1000 | Loss: 0.00001028
Iteration 43/1000 | Loss: 0.00001027
Iteration 44/1000 | Loss: 0.00001027
Iteration 45/1000 | Loss: 0.00001027
Iteration 46/1000 | Loss: 0.00001027
Iteration 47/1000 | Loss: 0.00001027
Iteration 48/1000 | Loss: 0.00001027
Iteration 49/1000 | Loss: 0.00001027
Iteration 50/1000 | Loss: 0.00001027
Iteration 51/1000 | Loss: 0.00001027
Iteration 52/1000 | Loss: 0.00001027
Iteration 53/1000 | Loss: 0.00001026
Iteration 54/1000 | Loss: 0.00001026
Iteration 55/1000 | Loss: 0.00001026
Iteration 56/1000 | Loss: 0.00001026
Iteration 57/1000 | Loss: 0.00001026
Iteration 58/1000 | Loss: 0.00001026
Iteration 59/1000 | Loss: 0.00001025
Iteration 60/1000 | Loss: 0.00001025
Iteration 61/1000 | Loss: 0.00001025
Iteration 62/1000 | Loss: 0.00001025
Iteration 63/1000 | Loss: 0.00001025
Iteration 64/1000 | Loss: 0.00001025
Iteration 65/1000 | Loss: 0.00001025
Iteration 66/1000 | Loss: 0.00001025
Iteration 67/1000 | Loss: 0.00001025
Iteration 68/1000 | Loss: 0.00001025
Iteration 69/1000 | Loss: 0.00001024
Iteration 70/1000 | Loss: 0.00001024
Iteration 71/1000 | Loss: 0.00001024
Iteration 72/1000 | Loss: 0.00001024
Iteration 73/1000 | Loss: 0.00001024
Iteration 74/1000 | Loss: 0.00001023
Iteration 75/1000 | Loss: 0.00001023
Iteration 76/1000 | Loss: 0.00001023
Iteration 77/1000 | Loss: 0.00001023
Iteration 78/1000 | Loss: 0.00001022
Iteration 79/1000 | Loss: 0.00001022
Iteration 80/1000 | Loss: 0.00001022
Iteration 81/1000 | Loss: 0.00001022
Iteration 82/1000 | Loss: 0.00001021
Iteration 83/1000 | Loss: 0.00001021
Iteration 84/1000 | Loss: 0.00001021
Iteration 85/1000 | Loss: 0.00001021
Iteration 86/1000 | Loss: 0.00001021
Iteration 87/1000 | Loss: 0.00001021
Iteration 88/1000 | Loss: 0.00001021
Iteration 89/1000 | Loss: 0.00001021
Iteration 90/1000 | Loss: 0.00001021
Iteration 91/1000 | Loss: 0.00001021
Iteration 92/1000 | Loss: 0.00001021
Iteration 93/1000 | Loss: 0.00001021
Iteration 94/1000 | Loss: 0.00001021
Iteration 95/1000 | Loss: 0.00001020
Iteration 96/1000 | Loss: 0.00001020
Iteration 97/1000 | Loss: 0.00001020
Iteration 98/1000 | Loss: 0.00001020
Iteration 99/1000 | Loss: 0.00001019
Iteration 100/1000 | Loss: 0.00001019
Iteration 101/1000 | Loss: 0.00001019
Iteration 102/1000 | Loss: 0.00001019
Iteration 103/1000 | Loss: 0.00001019
Iteration 104/1000 | Loss: 0.00001019
Iteration 105/1000 | Loss: 0.00001019
Iteration 106/1000 | Loss: 0.00001019
Iteration 107/1000 | Loss: 0.00001019
Iteration 108/1000 | Loss: 0.00001018
Iteration 109/1000 | Loss: 0.00001018
Iteration 110/1000 | Loss: 0.00001018
Iteration 111/1000 | Loss: 0.00001018
Iteration 112/1000 | Loss: 0.00001018
Iteration 113/1000 | Loss: 0.00001018
Iteration 114/1000 | Loss: 0.00001018
Iteration 115/1000 | Loss: 0.00001018
Iteration 116/1000 | Loss: 0.00001018
Iteration 117/1000 | Loss: 0.00001018
Iteration 118/1000 | Loss: 0.00001018
Iteration 119/1000 | Loss: 0.00001017
Iteration 120/1000 | Loss: 0.00001017
Iteration 121/1000 | Loss: 0.00001017
Iteration 122/1000 | Loss: 0.00001017
Iteration 123/1000 | Loss: 0.00001017
Iteration 124/1000 | Loss: 0.00001017
Iteration 125/1000 | Loss: 0.00001017
Iteration 126/1000 | Loss: 0.00001017
Iteration 127/1000 | Loss: 0.00001017
Iteration 128/1000 | Loss: 0.00001017
Iteration 129/1000 | Loss: 0.00001017
Iteration 130/1000 | Loss: 0.00001017
Iteration 131/1000 | Loss: 0.00001017
Iteration 132/1000 | Loss: 0.00001016
Iteration 133/1000 | Loss: 0.00001016
Iteration 134/1000 | Loss: 0.00001016
Iteration 135/1000 | Loss: 0.00001016
Iteration 136/1000 | Loss: 0.00001016
Iteration 137/1000 | Loss: 0.00001016
Iteration 138/1000 | Loss: 0.00001016
Iteration 139/1000 | Loss: 0.00001016
Iteration 140/1000 | Loss: 0.00001015
Iteration 141/1000 | Loss: 0.00001015
Iteration 142/1000 | Loss: 0.00001015
Iteration 143/1000 | Loss: 0.00001015
Iteration 144/1000 | Loss: 0.00001015
Iteration 145/1000 | Loss: 0.00001015
Iteration 146/1000 | Loss: 0.00001015
Iteration 147/1000 | Loss: 0.00001015
Iteration 148/1000 | Loss: 0.00001015
Iteration 149/1000 | Loss: 0.00001015
Iteration 150/1000 | Loss: 0.00001014
Iteration 151/1000 | Loss: 0.00001014
Iteration 152/1000 | Loss: 0.00001014
Iteration 153/1000 | Loss: 0.00001014
Iteration 154/1000 | Loss: 0.00001014
Iteration 155/1000 | Loss: 0.00001014
Iteration 156/1000 | Loss: 0.00001014
Iteration 157/1000 | Loss: 0.00001014
Iteration 158/1000 | Loss: 0.00001014
Iteration 159/1000 | Loss: 0.00001014
Iteration 160/1000 | Loss: 0.00001014
Iteration 161/1000 | Loss: 0.00001014
Iteration 162/1000 | Loss: 0.00001014
Iteration 163/1000 | Loss: 0.00001014
Iteration 164/1000 | Loss: 0.00001014
Iteration 165/1000 | Loss: 0.00001014
Iteration 166/1000 | Loss: 0.00001014
Iteration 167/1000 | Loss: 0.00001014
Iteration 168/1000 | Loss: 0.00001014
Iteration 169/1000 | Loss: 0.00001013
Iteration 170/1000 | Loss: 0.00001013
Iteration 171/1000 | Loss: 0.00001013
Iteration 172/1000 | Loss: 0.00001013
Iteration 173/1000 | Loss: 0.00001013
Iteration 174/1000 | Loss: 0.00001013
Iteration 175/1000 | Loss: 0.00001013
Iteration 176/1000 | Loss: 0.00001013
Iteration 177/1000 | Loss: 0.00001013
Iteration 178/1000 | Loss: 0.00001013
Iteration 179/1000 | Loss: 0.00001013
Iteration 180/1000 | Loss: 0.00001013
Iteration 181/1000 | Loss: 0.00001013
Iteration 182/1000 | Loss: 0.00001013
Iteration 183/1000 | Loss: 0.00001013
Iteration 184/1000 | Loss: 0.00001013
Iteration 185/1000 | Loss: 0.00001013
Iteration 186/1000 | Loss: 0.00001013
Iteration 187/1000 | Loss: 0.00001013
Iteration 188/1000 | Loss: 0.00001013
Iteration 189/1000 | Loss: 0.00001013
Iteration 190/1000 | Loss: 0.00001013
Iteration 191/1000 | Loss: 0.00001013
Iteration 192/1000 | Loss: 0.00001012
Iteration 193/1000 | Loss: 0.00001012
Iteration 194/1000 | Loss: 0.00001012
Iteration 195/1000 | Loss: 0.00001012
Iteration 196/1000 | Loss: 0.00001012
Iteration 197/1000 | Loss: 0.00001012
Iteration 198/1000 | Loss: 0.00001012
Iteration 199/1000 | Loss: 0.00001012
Iteration 200/1000 | Loss: 0.00001012
Iteration 201/1000 | Loss: 0.00001012
Iteration 202/1000 | Loss: 0.00001012
Iteration 203/1000 | Loss: 0.00001012
Iteration 204/1000 | Loss: 0.00001012
Iteration 205/1000 | Loss: 0.00001012
Iteration 206/1000 | Loss: 0.00001012
Iteration 207/1000 | Loss: 0.00001012
Iteration 208/1000 | Loss: 0.00001012
Iteration 209/1000 | Loss: 0.00001012
Iteration 210/1000 | Loss: 0.00001012
Iteration 211/1000 | Loss: 0.00001012
Iteration 212/1000 | Loss: 0.00001012
Iteration 213/1000 | Loss: 0.00001012
Iteration 214/1000 | Loss: 0.00001012
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 214. Stopping optimization.
Last 5 losses: [1.0120321348949801e-05, 1.0120321348949801e-05, 1.0120321348949801e-05, 1.0120321348949801e-05, 1.0120321348949801e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0120321348949801e-05

Optimization complete. Final v2v error: 2.698612928390503 mm

Highest mean error: 3.1653194427490234 mm for frame 0

Lowest mean error: 2.598865270614624 mm for frame 113

Saving results

Total time: 36.46752452850342
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ben_posed_008/1056/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_008/1056.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_008/1056
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00512846
Iteration 2/25 | Loss: 0.00088421
Iteration 3/25 | Loss: 0.00067088
Iteration 4/25 | Loss: 0.00062705
Iteration 5/25 | Loss: 0.00062001
Iteration 6/25 | Loss: 0.00061903
Iteration 7/25 | Loss: 0.00061860
Iteration 8/25 | Loss: 0.00061860
Iteration 9/25 | Loss: 0.00061860
Iteration 10/25 | Loss: 0.00061860
Iteration 11/25 | Loss: 0.00061860
Iteration 12/25 | Loss: 0.00061860
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0006185986567288637, 0.0006185986567288637, 0.0006185986567288637, 0.0006185986567288637, 0.0006185986567288637]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006185986567288637

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.80333215
Iteration 2/25 | Loss: 0.00016986
Iteration 3/25 | Loss: 0.00016986
Iteration 4/25 | Loss: 0.00016986
Iteration 5/25 | Loss: 0.00016986
Iteration 6/25 | Loss: 0.00016986
Iteration 7/25 | Loss: 0.00016986
Iteration 8/25 | Loss: 0.00016986
Iteration 9/25 | Loss: 0.00016986
Iteration 10/25 | Loss: 0.00016986
Iteration 11/25 | Loss: 0.00016986
Iteration 12/25 | Loss: 0.00016986
Iteration 13/25 | Loss: 0.00016986
Iteration 14/25 | Loss: 0.00016986
Iteration 15/25 | Loss: 0.00016986
Iteration 16/25 | Loss: 0.00016986
Iteration 17/25 | Loss: 0.00016986
Iteration 18/25 | Loss: 0.00016986
Iteration 19/25 | Loss: 0.00016986
Iteration 20/25 | Loss: 0.00016986
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0001698577543720603, 0.0001698577543720603, 0.0001698577543720603, 0.0001698577543720603, 0.0001698577543720603]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0001698577543720603

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00016986
Iteration 2/1000 | Loss: 0.00003124
Iteration 3/1000 | Loss: 0.00002373
Iteration 4/1000 | Loss: 0.00002227
Iteration 5/1000 | Loss: 0.00002116
Iteration 6/1000 | Loss: 0.00002061
Iteration 7/1000 | Loss: 0.00001997
Iteration 8/1000 | Loss: 0.00001957
Iteration 9/1000 | Loss: 0.00001932
Iteration 10/1000 | Loss: 0.00001911
Iteration 11/1000 | Loss: 0.00001891
Iteration 12/1000 | Loss: 0.00001890
Iteration 13/1000 | Loss: 0.00001887
Iteration 14/1000 | Loss: 0.00001869
Iteration 15/1000 | Loss: 0.00001865
Iteration 16/1000 | Loss: 0.00001861
Iteration 17/1000 | Loss: 0.00001859
Iteration 18/1000 | Loss: 0.00001858
Iteration 19/1000 | Loss: 0.00001858
Iteration 20/1000 | Loss: 0.00001856
Iteration 21/1000 | Loss: 0.00001854
Iteration 22/1000 | Loss: 0.00001848
Iteration 23/1000 | Loss: 0.00001845
Iteration 24/1000 | Loss: 0.00001844
Iteration 25/1000 | Loss: 0.00001844
Iteration 26/1000 | Loss: 0.00001844
Iteration 27/1000 | Loss: 0.00001843
Iteration 28/1000 | Loss: 0.00001843
Iteration 29/1000 | Loss: 0.00001843
Iteration 30/1000 | Loss: 0.00001842
Iteration 31/1000 | Loss: 0.00001841
Iteration 32/1000 | Loss: 0.00001841
Iteration 33/1000 | Loss: 0.00001840
Iteration 34/1000 | Loss: 0.00001840
Iteration 35/1000 | Loss: 0.00001839
Iteration 36/1000 | Loss: 0.00001839
Iteration 37/1000 | Loss: 0.00001838
Iteration 38/1000 | Loss: 0.00001838
Iteration 39/1000 | Loss: 0.00001838
Iteration 40/1000 | Loss: 0.00001837
Iteration 41/1000 | Loss: 0.00001837
Iteration 42/1000 | Loss: 0.00001837
Iteration 43/1000 | Loss: 0.00001836
Iteration 44/1000 | Loss: 0.00001836
Iteration 45/1000 | Loss: 0.00001836
Iteration 46/1000 | Loss: 0.00001835
Iteration 47/1000 | Loss: 0.00001835
Iteration 48/1000 | Loss: 0.00001834
Iteration 49/1000 | Loss: 0.00001834
Iteration 50/1000 | Loss: 0.00001834
Iteration 51/1000 | Loss: 0.00001833
Iteration 52/1000 | Loss: 0.00001833
Iteration 53/1000 | Loss: 0.00001832
Iteration 54/1000 | Loss: 0.00001832
Iteration 55/1000 | Loss: 0.00001832
Iteration 56/1000 | Loss: 0.00001832
Iteration 57/1000 | Loss: 0.00001832
Iteration 58/1000 | Loss: 0.00001832
Iteration 59/1000 | Loss: 0.00001831
Iteration 60/1000 | Loss: 0.00001831
Iteration 61/1000 | Loss: 0.00001831
Iteration 62/1000 | Loss: 0.00001831
Iteration 63/1000 | Loss: 0.00001831
Iteration 64/1000 | Loss: 0.00001831
Iteration 65/1000 | Loss: 0.00001830
Iteration 66/1000 | Loss: 0.00001830
Iteration 67/1000 | Loss: 0.00001830
Iteration 68/1000 | Loss: 0.00001829
Iteration 69/1000 | Loss: 0.00001829
Iteration 70/1000 | Loss: 0.00001829
Iteration 71/1000 | Loss: 0.00001829
Iteration 72/1000 | Loss: 0.00001828
Iteration 73/1000 | Loss: 0.00001828
Iteration 74/1000 | Loss: 0.00001828
Iteration 75/1000 | Loss: 0.00001828
Iteration 76/1000 | Loss: 0.00001828
Iteration 77/1000 | Loss: 0.00001828
Iteration 78/1000 | Loss: 0.00001828
Iteration 79/1000 | Loss: 0.00001828
Iteration 80/1000 | Loss: 0.00001828
Iteration 81/1000 | Loss: 0.00001828
Iteration 82/1000 | Loss: 0.00001826
Iteration 83/1000 | Loss: 0.00001826
Iteration 84/1000 | Loss: 0.00001826
Iteration 85/1000 | Loss: 0.00001826
Iteration 86/1000 | Loss: 0.00001826
Iteration 87/1000 | Loss: 0.00001826
Iteration 88/1000 | Loss: 0.00001825
Iteration 89/1000 | Loss: 0.00001825
Iteration 90/1000 | Loss: 0.00001825
Iteration 91/1000 | Loss: 0.00001825
Iteration 92/1000 | Loss: 0.00001822
Iteration 93/1000 | Loss: 0.00001821
Iteration 94/1000 | Loss: 0.00001821
Iteration 95/1000 | Loss: 0.00001821
Iteration 96/1000 | Loss: 0.00001821
Iteration 97/1000 | Loss: 0.00001821
Iteration 98/1000 | Loss: 0.00001821
Iteration 99/1000 | Loss: 0.00001821
Iteration 100/1000 | Loss: 0.00001821
Iteration 101/1000 | Loss: 0.00001821
Iteration 102/1000 | Loss: 0.00001821
Iteration 103/1000 | Loss: 0.00001821
Iteration 104/1000 | Loss: 0.00001821
Iteration 105/1000 | Loss: 0.00001820
Iteration 106/1000 | Loss: 0.00001820
Iteration 107/1000 | Loss: 0.00001820
Iteration 108/1000 | Loss: 0.00001820
Iteration 109/1000 | Loss: 0.00001820
Iteration 110/1000 | Loss: 0.00001820
Iteration 111/1000 | Loss: 0.00001820
Iteration 112/1000 | Loss: 0.00001820
Iteration 113/1000 | Loss: 0.00001820
Iteration 114/1000 | Loss: 0.00001819
Iteration 115/1000 | Loss: 0.00001819
Iteration 116/1000 | Loss: 0.00001819
Iteration 117/1000 | Loss: 0.00001819
Iteration 118/1000 | Loss: 0.00001819
Iteration 119/1000 | Loss: 0.00001819
Iteration 120/1000 | Loss: 0.00001819
Iteration 121/1000 | Loss: 0.00001819
Iteration 122/1000 | Loss: 0.00001819
Iteration 123/1000 | Loss: 0.00001819
Iteration 124/1000 | Loss: 0.00001819
Iteration 125/1000 | Loss: 0.00001819
Iteration 126/1000 | Loss: 0.00001819
Iteration 127/1000 | Loss: 0.00001819
Iteration 128/1000 | Loss: 0.00001818
Iteration 129/1000 | Loss: 0.00001818
Iteration 130/1000 | Loss: 0.00001818
Iteration 131/1000 | Loss: 0.00001818
Iteration 132/1000 | Loss: 0.00001818
Iteration 133/1000 | Loss: 0.00001818
Iteration 134/1000 | Loss: 0.00001818
Iteration 135/1000 | Loss: 0.00001818
Iteration 136/1000 | Loss: 0.00001818
Iteration 137/1000 | Loss: 0.00001818
Iteration 138/1000 | Loss: 0.00001818
Iteration 139/1000 | Loss: 0.00001818
Iteration 140/1000 | Loss: 0.00001818
Iteration 141/1000 | Loss: 0.00001818
Iteration 142/1000 | Loss: 0.00001817
Iteration 143/1000 | Loss: 0.00001817
Iteration 144/1000 | Loss: 0.00001817
Iteration 145/1000 | Loss: 0.00001817
Iteration 146/1000 | Loss: 0.00001817
Iteration 147/1000 | Loss: 0.00001817
Iteration 148/1000 | Loss: 0.00001817
Iteration 149/1000 | Loss: 0.00001817
Iteration 150/1000 | Loss: 0.00001817
Iteration 151/1000 | Loss: 0.00001816
Iteration 152/1000 | Loss: 0.00001816
Iteration 153/1000 | Loss: 0.00001816
Iteration 154/1000 | Loss: 0.00001816
Iteration 155/1000 | Loss: 0.00001816
Iteration 156/1000 | Loss: 0.00001816
Iteration 157/1000 | Loss: 0.00001816
Iteration 158/1000 | Loss: 0.00001816
Iteration 159/1000 | Loss: 0.00001816
Iteration 160/1000 | Loss: 0.00001816
Iteration 161/1000 | Loss: 0.00001816
Iteration 162/1000 | Loss: 0.00001816
Iteration 163/1000 | Loss: 0.00001816
Iteration 164/1000 | Loss: 0.00001816
Iteration 165/1000 | Loss: 0.00001815
Iteration 166/1000 | Loss: 0.00001815
Iteration 167/1000 | Loss: 0.00001815
Iteration 168/1000 | Loss: 0.00001815
Iteration 169/1000 | Loss: 0.00001815
Iteration 170/1000 | Loss: 0.00001815
Iteration 171/1000 | Loss: 0.00001815
Iteration 172/1000 | Loss: 0.00001815
Iteration 173/1000 | Loss: 0.00001815
Iteration 174/1000 | Loss: 0.00001815
Iteration 175/1000 | Loss: 0.00001815
Iteration 176/1000 | Loss: 0.00001815
Iteration 177/1000 | Loss: 0.00001815
Iteration 178/1000 | Loss: 0.00001814
Iteration 179/1000 | Loss: 0.00001814
Iteration 180/1000 | Loss: 0.00001814
Iteration 181/1000 | Loss: 0.00001814
Iteration 182/1000 | Loss: 0.00001814
Iteration 183/1000 | Loss: 0.00001814
Iteration 184/1000 | Loss: 0.00001814
Iteration 185/1000 | Loss: 0.00001814
Iteration 186/1000 | Loss: 0.00001814
Iteration 187/1000 | Loss: 0.00001814
Iteration 188/1000 | Loss: 0.00001814
Iteration 189/1000 | Loss: 0.00001814
Iteration 190/1000 | Loss: 0.00001814
Iteration 191/1000 | Loss: 0.00001814
Iteration 192/1000 | Loss: 0.00001814
Iteration 193/1000 | Loss: 0.00001813
Iteration 194/1000 | Loss: 0.00001813
Iteration 195/1000 | Loss: 0.00001813
Iteration 196/1000 | Loss: 0.00001813
Iteration 197/1000 | Loss: 0.00001813
Iteration 198/1000 | Loss: 0.00001813
Iteration 199/1000 | Loss: 0.00001813
Iteration 200/1000 | Loss: 0.00001813
Iteration 201/1000 | Loss: 0.00001813
Iteration 202/1000 | Loss: 0.00001813
Iteration 203/1000 | Loss: 0.00001813
Iteration 204/1000 | Loss: 0.00001813
Iteration 205/1000 | Loss: 0.00001813
Iteration 206/1000 | Loss: 0.00001813
Iteration 207/1000 | Loss: 0.00001813
Iteration 208/1000 | Loss: 0.00001813
Iteration 209/1000 | Loss: 0.00001813
Iteration 210/1000 | Loss: 0.00001813
Iteration 211/1000 | Loss: 0.00001813
Iteration 212/1000 | Loss: 0.00001813
Iteration 213/1000 | Loss: 0.00001813
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 213. Stopping optimization.
Last 5 losses: [1.812766640796326e-05, 1.812766640796326e-05, 1.812766640796326e-05, 1.812766640796326e-05, 1.812766640796326e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.812766640796326e-05

Optimization complete. Final v2v error: 3.584141492843628 mm

Highest mean error: 4.487811088562012 mm for frame 265

Lowest mean error: 3.4865710735321045 mm for frame 54

Saving results

Total time: 51.98142671585083
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ben_posed_008/1099/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_008/1099.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_008/1099
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00773971
Iteration 2/25 | Loss: 0.00095834
Iteration 3/25 | Loss: 0.00071450
Iteration 4/25 | Loss: 0.00066702
Iteration 5/25 | Loss: 0.00066169
Iteration 6/25 | Loss: 0.00066108
Iteration 7/25 | Loss: 0.00066108
Iteration 8/25 | Loss: 0.00066108
Iteration 9/25 | Loss: 0.00066108
Iteration 10/25 | Loss: 0.00066108
Iteration 11/25 | Loss: 0.00066108
Iteration 12/25 | Loss: 0.00066108
Iteration 13/25 | Loss: 0.00066108
Iteration 14/25 | Loss: 0.00066108
Iteration 15/25 | Loss: 0.00066108
Iteration 16/25 | Loss: 0.00066108
Iteration 17/25 | Loss: 0.00066108
Iteration 18/25 | Loss: 0.00066108
Iteration 19/25 | Loss: 0.00066108
Iteration 20/25 | Loss: 0.00066108
Iteration 21/25 | Loss: 0.00066108
Iteration 22/25 | Loss: 0.00066108
Iteration 23/25 | Loss: 0.00066108
Iteration 24/25 | Loss: 0.00066108
Iteration 25/25 | Loss: 0.00066108

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.41666269
Iteration 2/25 | Loss: 0.00018287
Iteration 3/25 | Loss: 0.00018283
Iteration 4/25 | Loss: 0.00018283
Iteration 5/25 | Loss: 0.00018283
Iteration 6/25 | Loss: 0.00018283
Iteration 7/25 | Loss: 0.00018283
Iteration 8/25 | Loss: 0.00018283
Iteration 9/25 | Loss: 0.00018283
Iteration 10/25 | Loss: 0.00018283
Iteration 11/25 | Loss: 0.00018283
Iteration 12/25 | Loss: 0.00018283
Iteration 13/25 | Loss: 0.00018283
Iteration 14/25 | Loss: 0.00018283
Iteration 15/25 | Loss: 0.00018283
Iteration 16/25 | Loss: 0.00018283
Iteration 17/25 | Loss: 0.00018283
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0001828271197155118, 0.0001828271197155118, 0.0001828271197155118, 0.0001828271197155118, 0.0001828271197155118]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0001828271197155118

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00018283
Iteration 2/1000 | Loss: 0.00003686
Iteration 3/1000 | Loss: 0.00002803
Iteration 4/1000 | Loss: 0.00002476
Iteration 5/1000 | Loss: 0.00002296
Iteration 6/1000 | Loss: 0.00002194
Iteration 7/1000 | Loss: 0.00002116
Iteration 8/1000 | Loss: 0.00002050
Iteration 9/1000 | Loss: 0.00002017
Iteration 10/1000 | Loss: 0.00002002
Iteration 11/1000 | Loss: 0.00001998
Iteration 12/1000 | Loss: 0.00001984
Iteration 13/1000 | Loss: 0.00001976
Iteration 14/1000 | Loss: 0.00001969
Iteration 15/1000 | Loss: 0.00001968
Iteration 16/1000 | Loss: 0.00001967
Iteration 17/1000 | Loss: 0.00001965
Iteration 18/1000 | Loss: 0.00001965
Iteration 19/1000 | Loss: 0.00001965
Iteration 20/1000 | Loss: 0.00001961
Iteration 21/1000 | Loss: 0.00001961
Iteration 22/1000 | Loss: 0.00001961
Iteration 23/1000 | Loss: 0.00001961
Iteration 24/1000 | Loss: 0.00001961
Iteration 25/1000 | Loss: 0.00001960
Iteration 26/1000 | Loss: 0.00001960
Iteration 27/1000 | Loss: 0.00001959
Iteration 28/1000 | Loss: 0.00001959
Iteration 29/1000 | Loss: 0.00001959
Iteration 30/1000 | Loss: 0.00001959
Iteration 31/1000 | Loss: 0.00001959
Iteration 32/1000 | Loss: 0.00001959
Iteration 33/1000 | Loss: 0.00001958
Iteration 34/1000 | Loss: 0.00001958
Iteration 35/1000 | Loss: 0.00001958
Iteration 36/1000 | Loss: 0.00001958
Iteration 37/1000 | Loss: 0.00001958
Iteration 38/1000 | Loss: 0.00001958
Iteration 39/1000 | Loss: 0.00001958
Iteration 40/1000 | Loss: 0.00001958
Iteration 41/1000 | Loss: 0.00001958
Iteration 42/1000 | Loss: 0.00001958
Iteration 43/1000 | Loss: 0.00001957
Iteration 44/1000 | Loss: 0.00001957
Iteration 45/1000 | Loss: 0.00001957
Iteration 46/1000 | Loss: 0.00001957
Iteration 47/1000 | Loss: 0.00001957
Iteration 48/1000 | Loss: 0.00001957
Iteration 49/1000 | Loss: 0.00001957
Iteration 50/1000 | Loss: 0.00001957
Iteration 51/1000 | Loss: 0.00001956
Iteration 52/1000 | Loss: 0.00001956
Iteration 53/1000 | Loss: 0.00001956
Iteration 54/1000 | Loss: 0.00001956
Iteration 55/1000 | Loss: 0.00001956
Iteration 56/1000 | Loss: 0.00001956
Iteration 57/1000 | Loss: 0.00001956
Iteration 58/1000 | Loss: 0.00001956
Iteration 59/1000 | Loss: 0.00001956
Iteration 60/1000 | Loss: 0.00001956
Iteration 61/1000 | Loss: 0.00001956
Iteration 62/1000 | Loss: 0.00001956
Iteration 63/1000 | Loss: 0.00001956
Iteration 64/1000 | Loss: 0.00001956
Iteration 65/1000 | Loss: 0.00001956
Iteration 66/1000 | Loss: 0.00001956
Iteration 67/1000 | Loss: 0.00001956
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 67. Stopping optimization.
Last 5 losses: [1.9559896827558987e-05, 1.9559896827558987e-05, 1.9559896827558987e-05, 1.9559896827558987e-05, 1.9559896827558987e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9559896827558987e-05

Optimization complete. Final v2v error: 3.7415990829467773 mm

Highest mean error: 4.037274360656738 mm for frame 230

Lowest mean error: 3.5282154083251953 mm for frame 29

Saving results

Total time: 34.79833793640137
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ben_posed_008/1083/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_008/1083.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_008/1083
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00835131
Iteration 2/25 | Loss: 0.00095103
Iteration 3/25 | Loss: 0.00067029
Iteration 4/25 | Loss: 0.00059839
Iteration 5/25 | Loss: 0.00057815
Iteration 6/25 | Loss: 0.00057202
Iteration 7/25 | Loss: 0.00057045
Iteration 8/25 | Loss: 0.00057001
Iteration 9/25 | Loss: 0.00057001
Iteration 10/25 | Loss: 0.00057001
Iteration 11/25 | Loss: 0.00057001
Iteration 12/25 | Loss: 0.00057001
Iteration 13/25 | Loss: 0.00057001
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0005700102774426341, 0.0005700102774426341, 0.0005700102774426341, 0.0005700102774426341, 0.0005700102774426341]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005700102774426341

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.50086296
Iteration 2/25 | Loss: 0.00022672
Iteration 3/25 | Loss: 0.00022672
Iteration 4/25 | Loss: 0.00022672
Iteration 5/25 | Loss: 0.00022672
Iteration 6/25 | Loss: 0.00022672
Iteration 7/25 | Loss: 0.00022672
Iteration 8/25 | Loss: 0.00022672
Iteration 9/25 | Loss: 0.00022672
Iteration 10/25 | Loss: 0.00022672
Iteration 11/25 | Loss: 0.00022672
Iteration 12/25 | Loss: 0.00022672
Iteration 13/25 | Loss: 0.00022672
Iteration 14/25 | Loss: 0.00022672
Iteration 15/25 | Loss: 0.00022672
Iteration 16/25 | Loss: 0.00022672
Iteration 17/25 | Loss: 0.00022672
Iteration 18/25 | Loss: 0.00022672
Iteration 19/25 | Loss: 0.00022672
Iteration 20/25 | Loss: 0.00022672
Iteration 21/25 | Loss: 0.00022672
Iteration 22/25 | Loss: 0.00022672
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.00022671780607197434, 0.00022671780607197434, 0.00022671780607197434, 0.00022671780607197434, 0.00022671780607197434]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00022671780607197434

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00022672
Iteration 2/1000 | Loss: 0.00002631
Iteration 3/1000 | Loss: 0.00001647
Iteration 4/1000 | Loss: 0.00001359
Iteration 5/1000 | Loss: 0.00001264
Iteration 6/1000 | Loss: 0.00001198
Iteration 7/1000 | Loss: 0.00001163
Iteration 8/1000 | Loss: 0.00001135
Iteration 9/1000 | Loss: 0.00001106
Iteration 10/1000 | Loss: 0.00001089
Iteration 11/1000 | Loss: 0.00001075
Iteration 12/1000 | Loss: 0.00001073
Iteration 13/1000 | Loss: 0.00001072
Iteration 14/1000 | Loss: 0.00001071
Iteration 15/1000 | Loss: 0.00001070
Iteration 16/1000 | Loss: 0.00001058
Iteration 17/1000 | Loss: 0.00001056
Iteration 18/1000 | Loss: 0.00001051
Iteration 19/1000 | Loss: 0.00001049
Iteration 20/1000 | Loss: 0.00001049
Iteration 21/1000 | Loss: 0.00001047
Iteration 22/1000 | Loss: 0.00001046
Iteration 23/1000 | Loss: 0.00001046
Iteration 24/1000 | Loss: 0.00001045
Iteration 25/1000 | Loss: 0.00001045
Iteration 26/1000 | Loss: 0.00001044
Iteration 27/1000 | Loss: 0.00001043
Iteration 28/1000 | Loss: 0.00001043
Iteration 29/1000 | Loss: 0.00001043
Iteration 30/1000 | Loss: 0.00001042
Iteration 31/1000 | Loss: 0.00001042
Iteration 32/1000 | Loss: 0.00001042
Iteration 33/1000 | Loss: 0.00001041
Iteration 34/1000 | Loss: 0.00001041
Iteration 35/1000 | Loss: 0.00001041
Iteration 36/1000 | Loss: 0.00001041
Iteration 37/1000 | Loss: 0.00001041
Iteration 38/1000 | Loss: 0.00001041
Iteration 39/1000 | Loss: 0.00001041
Iteration 40/1000 | Loss: 0.00001041
Iteration 41/1000 | Loss: 0.00001041
Iteration 42/1000 | Loss: 0.00001041
Iteration 43/1000 | Loss: 0.00001040
Iteration 44/1000 | Loss: 0.00001040
Iteration 45/1000 | Loss: 0.00001040
Iteration 46/1000 | Loss: 0.00001040
Iteration 47/1000 | Loss: 0.00001040
Iteration 48/1000 | Loss: 0.00001040
Iteration 49/1000 | Loss: 0.00001039
Iteration 50/1000 | Loss: 0.00001039
Iteration 51/1000 | Loss: 0.00001039
Iteration 52/1000 | Loss: 0.00001038
Iteration 53/1000 | Loss: 0.00001038
Iteration 54/1000 | Loss: 0.00001038
Iteration 55/1000 | Loss: 0.00001037
Iteration 56/1000 | Loss: 0.00001037
Iteration 57/1000 | Loss: 0.00001037
Iteration 58/1000 | Loss: 0.00001037
Iteration 59/1000 | Loss: 0.00001036
Iteration 60/1000 | Loss: 0.00001036
Iteration 61/1000 | Loss: 0.00001036
Iteration 62/1000 | Loss: 0.00001035
Iteration 63/1000 | Loss: 0.00001035
Iteration 64/1000 | Loss: 0.00001035
Iteration 65/1000 | Loss: 0.00001035
Iteration 66/1000 | Loss: 0.00001035
Iteration 67/1000 | Loss: 0.00001035
Iteration 68/1000 | Loss: 0.00001034
Iteration 69/1000 | Loss: 0.00001034
Iteration 70/1000 | Loss: 0.00001034
Iteration 71/1000 | Loss: 0.00001034
Iteration 72/1000 | Loss: 0.00001034
Iteration 73/1000 | Loss: 0.00001034
Iteration 74/1000 | Loss: 0.00001034
Iteration 75/1000 | Loss: 0.00001034
Iteration 76/1000 | Loss: 0.00001034
Iteration 77/1000 | Loss: 0.00001033
Iteration 78/1000 | Loss: 0.00001033
Iteration 79/1000 | Loss: 0.00001033
Iteration 80/1000 | Loss: 0.00001033
Iteration 81/1000 | Loss: 0.00001032
Iteration 82/1000 | Loss: 0.00001032
Iteration 83/1000 | Loss: 0.00001032
Iteration 84/1000 | Loss: 0.00001031
Iteration 85/1000 | Loss: 0.00001031
Iteration 86/1000 | Loss: 0.00001031
Iteration 87/1000 | Loss: 0.00001031
Iteration 88/1000 | Loss: 0.00001031
Iteration 89/1000 | Loss: 0.00001031
Iteration 90/1000 | Loss: 0.00001031
Iteration 91/1000 | Loss: 0.00001031
Iteration 92/1000 | Loss: 0.00001031
Iteration 93/1000 | Loss: 0.00001031
Iteration 94/1000 | Loss: 0.00001031
Iteration 95/1000 | Loss: 0.00001031
Iteration 96/1000 | Loss: 0.00001031
Iteration 97/1000 | Loss: 0.00001031
Iteration 98/1000 | Loss: 0.00001030
Iteration 99/1000 | Loss: 0.00001030
Iteration 100/1000 | Loss: 0.00001030
Iteration 101/1000 | Loss: 0.00001030
Iteration 102/1000 | Loss: 0.00001030
Iteration 103/1000 | Loss: 0.00001030
Iteration 104/1000 | Loss: 0.00001030
Iteration 105/1000 | Loss: 0.00001030
Iteration 106/1000 | Loss: 0.00001030
Iteration 107/1000 | Loss: 0.00001030
Iteration 108/1000 | Loss: 0.00001030
Iteration 109/1000 | Loss: 0.00001030
Iteration 110/1000 | Loss: 0.00001029
Iteration 111/1000 | Loss: 0.00001029
Iteration 112/1000 | Loss: 0.00001029
Iteration 113/1000 | Loss: 0.00001029
Iteration 114/1000 | Loss: 0.00001029
Iteration 115/1000 | Loss: 0.00001029
Iteration 116/1000 | Loss: 0.00001029
Iteration 117/1000 | Loss: 0.00001029
Iteration 118/1000 | Loss: 0.00001029
Iteration 119/1000 | Loss: 0.00001029
Iteration 120/1000 | Loss: 0.00001029
Iteration 121/1000 | Loss: 0.00001029
Iteration 122/1000 | Loss: 0.00001029
Iteration 123/1000 | Loss: 0.00001029
Iteration 124/1000 | Loss: 0.00001029
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 124. Stopping optimization.
Last 5 losses: [1.0292826118529774e-05, 1.0292826118529774e-05, 1.0292826118529774e-05, 1.0292826118529774e-05, 1.0292826118529774e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0292826118529774e-05

Optimization complete. Final v2v error: 2.7367513179779053 mm

Highest mean error: 3.3218038082122803 mm for frame 48

Lowest mean error: 2.48580002784729 mm for frame 176

Saving results

Total time: 38.81299424171448
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ben_posed_008/1078/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_008/1078.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_008/1078
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00609507
Iteration 2/25 | Loss: 0.00095143
Iteration 3/25 | Loss: 0.00076293
Iteration 4/25 | Loss: 0.00073414
Iteration 5/25 | Loss: 0.00072915
Iteration 6/25 | Loss: 0.00072789
Iteration 7/25 | Loss: 0.00072741
Iteration 8/25 | Loss: 0.00072741
Iteration 9/25 | Loss: 0.00072741
Iteration 10/25 | Loss: 0.00072741
Iteration 11/25 | Loss: 0.00072741
Iteration 12/25 | Loss: 0.00072741
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0007274114177562296, 0.0007274114177562296, 0.0007274114177562296, 0.0007274114177562296, 0.0007274114177562296]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007274114177562296

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.73626691
Iteration 2/25 | Loss: 0.00045468
Iteration 3/25 | Loss: 0.00045466
Iteration 4/25 | Loss: 0.00045466
Iteration 5/25 | Loss: 0.00045466
Iteration 6/25 | Loss: 0.00045466
Iteration 7/25 | Loss: 0.00045466
Iteration 8/25 | Loss: 0.00045466
Iteration 9/25 | Loss: 0.00045466
Iteration 10/25 | Loss: 0.00045466
Iteration 11/25 | Loss: 0.00045466
Iteration 12/25 | Loss: 0.00045466
Iteration 13/25 | Loss: 0.00045466
Iteration 14/25 | Loss: 0.00045466
Iteration 15/25 | Loss: 0.00045466
Iteration 16/25 | Loss: 0.00045466
Iteration 17/25 | Loss: 0.00045466
Iteration 18/25 | Loss: 0.00045466
Iteration 19/25 | Loss: 0.00045466
Iteration 20/25 | Loss: 0.00045466
Iteration 21/25 | Loss: 0.00045466
Iteration 22/25 | Loss: 0.00045466
Iteration 23/25 | Loss: 0.00045466
Iteration 24/25 | Loss: 0.00045466
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0004546575655695051, 0.0004546575655695051, 0.0004546575655695051, 0.0004546575655695051, 0.0004546575655695051]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0004546575655695051

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00045466
Iteration 2/1000 | Loss: 0.00004696
Iteration 3/1000 | Loss: 0.00002492
Iteration 4/1000 | Loss: 0.00002001
Iteration 5/1000 | Loss: 0.00001780
Iteration 6/1000 | Loss: 0.00001649
Iteration 7/1000 | Loss: 0.00001595
Iteration 8/1000 | Loss: 0.00001567
Iteration 9/1000 | Loss: 0.00001543
Iteration 10/1000 | Loss: 0.00001541
Iteration 11/1000 | Loss: 0.00001527
Iteration 12/1000 | Loss: 0.00001526
Iteration 13/1000 | Loss: 0.00001517
Iteration 14/1000 | Loss: 0.00001516
Iteration 15/1000 | Loss: 0.00001516
Iteration 16/1000 | Loss: 0.00001516
Iteration 17/1000 | Loss: 0.00001516
Iteration 18/1000 | Loss: 0.00001515
Iteration 19/1000 | Loss: 0.00001515
Iteration 20/1000 | Loss: 0.00001514
Iteration 21/1000 | Loss: 0.00001514
Iteration 22/1000 | Loss: 0.00001514
Iteration 23/1000 | Loss: 0.00001514
Iteration 24/1000 | Loss: 0.00001513
Iteration 25/1000 | Loss: 0.00001513
Iteration 26/1000 | Loss: 0.00001513
Iteration 27/1000 | Loss: 0.00001512
Iteration 28/1000 | Loss: 0.00001512
Iteration 29/1000 | Loss: 0.00001512
Iteration 30/1000 | Loss: 0.00001511
Iteration 31/1000 | Loss: 0.00001511
Iteration 32/1000 | Loss: 0.00001510
Iteration 33/1000 | Loss: 0.00001510
Iteration 34/1000 | Loss: 0.00001510
Iteration 35/1000 | Loss: 0.00001510
Iteration 36/1000 | Loss: 0.00001510
Iteration 37/1000 | Loss: 0.00001509
Iteration 38/1000 | Loss: 0.00001509
Iteration 39/1000 | Loss: 0.00001509
Iteration 40/1000 | Loss: 0.00001507
Iteration 41/1000 | Loss: 0.00001506
Iteration 42/1000 | Loss: 0.00001504
Iteration 43/1000 | Loss: 0.00001503
Iteration 44/1000 | Loss: 0.00001502
Iteration 45/1000 | Loss: 0.00001502
Iteration 46/1000 | Loss: 0.00001501
Iteration 47/1000 | Loss: 0.00001500
Iteration 48/1000 | Loss: 0.00001499
Iteration 49/1000 | Loss: 0.00001499
Iteration 50/1000 | Loss: 0.00001499
Iteration 51/1000 | Loss: 0.00001499
Iteration 52/1000 | Loss: 0.00001498
Iteration 53/1000 | Loss: 0.00001498
Iteration 54/1000 | Loss: 0.00001497
Iteration 55/1000 | Loss: 0.00001496
Iteration 56/1000 | Loss: 0.00001495
Iteration 57/1000 | Loss: 0.00001495
Iteration 58/1000 | Loss: 0.00001494
Iteration 59/1000 | Loss: 0.00001494
Iteration 60/1000 | Loss: 0.00001493
Iteration 61/1000 | Loss: 0.00001493
Iteration 62/1000 | Loss: 0.00001493
Iteration 63/1000 | Loss: 0.00001493
Iteration 64/1000 | Loss: 0.00001493
Iteration 65/1000 | Loss: 0.00001493
Iteration 66/1000 | Loss: 0.00001493
Iteration 67/1000 | Loss: 0.00001493
Iteration 68/1000 | Loss: 0.00001493
Iteration 69/1000 | Loss: 0.00001492
Iteration 70/1000 | Loss: 0.00001492
Iteration 71/1000 | Loss: 0.00001492
Iteration 72/1000 | Loss: 0.00001492
Iteration 73/1000 | Loss: 0.00001492
Iteration 74/1000 | Loss: 0.00001492
Iteration 75/1000 | Loss: 0.00001492
Iteration 76/1000 | Loss: 0.00001492
Iteration 77/1000 | Loss: 0.00001491
Iteration 78/1000 | Loss: 0.00001491
Iteration 79/1000 | Loss: 0.00001491
Iteration 80/1000 | Loss: 0.00001491
Iteration 81/1000 | Loss: 0.00001491
Iteration 82/1000 | Loss: 0.00001490
Iteration 83/1000 | Loss: 0.00001490
Iteration 84/1000 | Loss: 0.00001490
Iteration 85/1000 | Loss: 0.00001490
Iteration 86/1000 | Loss: 0.00001490
Iteration 87/1000 | Loss: 0.00001490
Iteration 88/1000 | Loss: 0.00001490
Iteration 89/1000 | Loss: 0.00001490
Iteration 90/1000 | Loss: 0.00001490
Iteration 91/1000 | Loss: 0.00001490
Iteration 92/1000 | Loss: 0.00001490
Iteration 93/1000 | Loss: 0.00001489
Iteration 94/1000 | Loss: 0.00001489
Iteration 95/1000 | Loss: 0.00001489
Iteration 96/1000 | Loss: 0.00001489
Iteration 97/1000 | Loss: 0.00001489
Iteration 98/1000 | Loss: 0.00001489
Iteration 99/1000 | Loss: 0.00001489
Iteration 100/1000 | Loss: 0.00001489
Iteration 101/1000 | Loss: 0.00001489
Iteration 102/1000 | Loss: 0.00001489
Iteration 103/1000 | Loss: 0.00001489
Iteration 104/1000 | Loss: 0.00001489
Iteration 105/1000 | Loss: 0.00001489
Iteration 106/1000 | Loss: 0.00001489
Iteration 107/1000 | Loss: 0.00001489
Iteration 108/1000 | Loss: 0.00001489
Iteration 109/1000 | Loss: 0.00001489
Iteration 110/1000 | Loss: 0.00001489
Iteration 111/1000 | Loss: 0.00001489
Iteration 112/1000 | Loss: 0.00001489
Iteration 113/1000 | Loss: 0.00001489
Iteration 114/1000 | Loss: 0.00001489
Iteration 115/1000 | Loss: 0.00001489
Iteration 116/1000 | Loss: 0.00001489
Iteration 117/1000 | Loss: 0.00001489
Iteration 118/1000 | Loss: 0.00001489
Iteration 119/1000 | Loss: 0.00001489
Iteration 120/1000 | Loss: 0.00001489
Iteration 121/1000 | Loss: 0.00001489
Iteration 122/1000 | Loss: 0.00001489
Iteration 123/1000 | Loss: 0.00001489
Iteration 124/1000 | Loss: 0.00001489
Iteration 125/1000 | Loss: 0.00001489
Iteration 126/1000 | Loss: 0.00001489
Iteration 127/1000 | Loss: 0.00001489
Iteration 128/1000 | Loss: 0.00001489
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 128. Stopping optimization.
Last 5 losses: [1.4889500562276226e-05, 1.4889500562276226e-05, 1.4889500562276226e-05, 1.4889500562276226e-05, 1.4889500562276226e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4889500562276226e-05

Optimization complete. Final v2v error: 3.2012898921966553 mm

Highest mean error: 4.411561012268066 mm for frame 0

Lowest mean error: 3.0367813110351562 mm for frame 29

Saving results

Total time: 33.81684398651123
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ben_posed_008/1028/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_008/1028.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_008/1028
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00380608
Iteration 2/25 | Loss: 0.00077699
Iteration 3/25 | Loss: 0.00060174
Iteration 4/25 | Loss: 0.00056488
Iteration 5/25 | Loss: 0.00055559
Iteration 6/25 | Loss: 0.00055209
Iteration 7/25 | Loss: 0.00055122
Iteration 8/25 | Loss: 0.00055109
Iteration 9/25 | Loss: 0.00055109
Iteration 10/25 | Loss: 0.00055109
Iteration 11/25 | Loss: 0.00055109
Iteration 12/25 | Loss: 0.00055109
Iteration 13/25 | Loss: 0.00055109
Iteration 14/25 | Loss: 0.00055109
Iteration 15/25 | Loss: 0.00055109
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0005510899354703724, 0.0005510899354703724, 0.0005510899354703724, 0.0005510899354703724, 0.0005510899354703724]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005510899354703724

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43460643
Iteration 2/25 | Loss: 0.00012259
Iteration 3/25 | Loss: 0.00012259
Iteration 4/25 | Loss: 0.00012259
Iteration 5/25 | Loss: 0.00012258
Iteration 6/25 | Loss: 0.00012258
Iteration 7/25 | Loss: 0.00012258
Iteration 8/25 | Loss: 0.00012258
Iteration 9/25 | Loss: 0.00012258
Iteration 10/25 | Loss: 0.00012258
Iteration 11/25 | Loss: 0.00012258
Iteration 12/25 | Loss: 0.00012258
Iteration 13/25 | Loss: 0.00012258
Iteration 14/25 | Loss: 0.00012258
Iteration 15/25 | Loss: 0.00012258
Iteration 16/25 | Loss: 0.00012258
Iteration 17/25 | Loss: 0.00012258
Iteration 18/25 | Loss: 0.00012258
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.00012258269998710603, 0.00012258269998710603, 0.00012258269998710603, 0.00012258269998710603, 0.00012258269998710603]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00012258269998710603

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00012258
Iteration 2/1000 | Loss: 0.00001962
Iteration 3/1000 | Loss: 0.00001593
Iteration 4/1000 | Loss: 0.00001469
Iteration 5/1000 | Loss: 0.00001390
Iteration 6/1000 | Loss: 0.00001354
Iteration 7/1000 | Loss: 0.00001319
Iteration 8/1000 | Loss: 0.00001313
Iteration 9/1000 | Loss: 0.00001307
Iteration 10/1000 | Loss: 0.00001307
Iteration 11/1000 | Loss: 0.00001306
Iteration 12/1000 | Loss: 0.00001306
Iteration 13/1000 | Loss: 0.00001300
Iteration 14/1000 | Loss: 0.00001290
Iteration 15/1000 | Loss: 0.00001289
Iteration 16/1000 | Loss: 0.00001280
Iteration 17/1000 | Loss: 0.00001274
Iteration 18/1000 | Loss: 0.00001270
Iteration 19/1000 | Loss: 0.00001270
Iteration 20/1000 | Loss: 0.00001270
Iteration 21/1000 | Loss: 0.00001270
Iteration 22/1000 | Loss: 0.00001270
Iteration 23/1000 | Loss: 0.00001268
Iteration 24/1000 | Loss: 0.00001265
Iteration 25/1000 | Loss: 0.00001260
Iteration 26/1000 | Loss: 0.00001258
Iteration 27/1000 | Loss: 0.00001258
Iteration 28/1000 | Loss: 0.00001254
Iteration 29/1000 | Loss: 0.00001253
Iteration 30/1000 | Loss: 0.00001253
Iteration 31/1000 | Loss: 0.00001252
Iteration 32/1000 | Loss: 0.00001252
Iteration 33/1000 | Loss: 0.00001252
Iteration 34/1000 | Loss: 0.00001251
Iteration 35/1000 | Loss: 0.00001250
Iteration 36/1000 | Loss: 0.00001250
Iteration 37/1000 | Loss: 0.00001250
Iteration 38/1000 | Loss: 0.00001250
Iteration 39/1000 | Loss: 0.00001250
Iteration 40/1000 | Loss: 0.00001249
Iteration 41/1000 | Loss: 0.00001249
Iteration 42/1000 | Loss: 0.00001249
Iteration 43/1000 | Loss: 0.00001249
Iteration 44/1000 | Loss: 0.00001249
Iteration 45/1000 | Loss: 0.00001249
Iteration 46/1000 | Loss: 0.00001249
Iteration 47/1000 | Loss: 0.00001248
Iteration 48/1000 | Loss: 0.00001248
Iteration 49/1000 | Loss: 0.00001248
Iteration 50/1000 | Loss: 0.00001247
Iteration 51/1000 | Loss: 0.00001247
Iteration 52/1000 | Loss: 0.00001247
Iteration 53/1000 | Loss: 0.00001246
Iteration 54/1000 | Loss: 0.00001246
Iteration 55/1000 | Loss: 0.00001246
Iteration 56/1000 | Loss: 0.00001246
Iteration 57/1000 | Loss: 0.00001246
Iteration 58/1000 | Loss: 0.00001246
Iteration 59/1000 | Loss: 0.00001245
Iteration 60/1000 | Loss: 0.00001245
Iteration 61/1000 | Loss: 0.00001245
Iteration 62/1000 | Loss: 0.00001245
Iteration 63/1000 | Loss: 0.00001245
Iteration 64/1000 | Loss: 0.00001245
Iteration 65/1000 | Loss: 0.00001244
Iteration 66/1000 | Loss: 0.00001243
Iteration 67/1000 | Loss: 0.00001243
Iteration 68/1000 | Loss: 0.00001242
Iteration 69/1000 | Loss: 0.00001242
Iteration 70/1000 | Loss: 0.00001242
Iteration 71/1000 | Loss: 0.00001242
Iteration 72/1000 | Loss: 0.00001242
Iteration 73/1000 | Loss: 0.00001241
Iteration 74/1000 | Loss: 0.00001241
Iteration 75/1000 | Loss: 0.00001241
Iteration 76/1000 | Loss: 0.00001241
Iteration 77/1000 | Loss: 0.00001240
Iteration 78/1000 | Loss: 0.00001239
Iteration 79/1000 | Loss: 0.00001239
Iteration 80/1000 | Loss: 0.00001238
Iteration 81/1000 | Loss: 0.00001238
Iteration 82/1000 | Loss: 0.00001237
Iteration 83/1000 | Loss: 0.00001236
Iteration 84/1000 | Loss: 0.00001236
Iteration 85/1000 | Loss: 0.00001236
Iteration 86/1000 | Loss: 0.00001235
Iteration 87/1000 | Loss: 0.00001235
Iteration 88/1000 | Loss: 0.00001235
Iteration 89/1000 | Loss: 0.00001235
Iteration 90/1000 | Loss: 0.00001235
Iteration 91/1000 | Loss: 0.00001234
Iteration 92/1000 | Loss: 0.00001234
Iteration 93/1000 | Loss: 0.00001234
Iteration 94/1000 | Loss: 0.00001233
Iteration 95/1000 | Loss: 0.00001233
Iteration 96/1000 | Loss: 0.00001233
Iteration 97/1000 | Loss: 0.00001232
Iteration 98/1000 | Loss: 0.00001232
Iteration 99/1000 | Loss: 0.00001231
Iteration 100/1000 | Loss: 0.00001231
Iteration 101/1000 | Loss: 0.00001231
Iteration 102/1000 | Loss: 0.00001231
Iteration 103/1000 | Loss: 0.00001230
Iteration 104/1000 | Loss: 0.00001230
Iteration 105/1000 | Loss: 0.00001230
Iteration 106/1000 | Loss: 0.00001230
Iteration 107/1000 | Loss: 0.00001230
Iteration 108/1000 | Loss: 0.00001229
Iteration 109/1000 | Loss: 0.00001229
Iteration 110/1000 | Loss: 0.00001229
Iteration 111/1000 | Loss: 0.00001229
Iteration 112/1000 | Loss: 0.00001228
Iteration 113/1000 | Loss: 0.00001228
Iteration 114/1000 | Loss: 0.00001228
Iteration 115/1000 | Loss: 0.00001228
Iteration 116/1000 | Loss: 0.00001228
Iteration 117/1000 | Loss: 0.00001227
Iteration 118/1000 | Loss: 0.00001227
Iteration 119/1000 | Loss: 0.00001227
Iteration 120/1000 | Loss: 0.00001227
Iteration 121/1000 | Loss: 0.00001227
Iteration 122/1000 | Loss: 0.00001227
Iteration 123/1000 | Loss: 0.00001227
Iteration 124/1000 | Loss: 0.00001227
Iteration 125/1000 | Loss: 0.00001227
Iteration 126/1000 | Loss: 0.00001226
Iteration 127/1000 | Loss: 0.00001226
Iteration 128/1000 | Loss: 0.00001226
Iteration 129/1000 | Loss: 0.00001226
Iteration 130/1000 | Loss: 0.00001226
Iteration 131/1000 | Loss: 0.00001226
Iteration 132/1000 | Loss: 0.00001226
Iteration 133/1000 | Loss: 0.00001226
Iteration 134/1000 | Loss: 0.00001226
Iteration 135/1000 | Loss: 0.00001226
Iteration 136/1000 | Loss: 0.00001225
Iteration 137/1000 | Loss: 0.00001225
Iteration 138/1000 | Loss: 0.00001225
Iteration 139/1000 | Loss: 0.00001225
Iteration 140/1000 | Loss: 0.00001225
Iteration 141/1000 | Loss: 0.00001225
Iteration 142/1000 | Loss: 0.00001225
Iteration 143/1000 | Loss: 0.00001225
Iteration 144/1000 | Loss: 0.00001225
Iteration 145/1000 | Loss: 0.00001225
Iteration 146/1000 | Loss: 0.00001225
Iteration 147/1000 | Loss: 0.00001225
Iteration 148/1000 | Loss: 0.00001225
Iteration 149/1000 | Loss: 0.00001225
Iteration 150/1000 | Loss: 0.00001225
Iteration 151/1000 | Loss: 0.00001225
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 151. Stopping optimization.
Last 5 losses: [1.2248299753991887e-05, 1.2248299753991887e-05, 1.2248299753991887e-05, 1.2248299753991887e-05, 1.2248299753991887e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2248299753991887e-05

Optimization complete. Final v2v error: 2.9429163932800293 mm

Highest mean error: 3.349364757537842 mm for frame 76

Lowest mean error: 2.707671880722046 mm for frame 13

Saving results

Total time: 38.824137687683105
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ben_posed_008/1071/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_008/1071.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_008/1071
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00576555
Iteration 2/25 | Loss: 0.00069555
Iteration 3/25 | Loss: 0.00057097
Iteration 4/25 | Loss: 0.00055487
Iteration 5/25 | Loss: 0.00054838
Iteration 6/25 | Loss: 0.00054703
Iteration 7/25 | Loss: 0.00054693
Iteration 8/25 | Loss: 0.00054693
Iteration 9/25 | Loss: 0.00054693
Iteration 10/25 | Loss: 0.00054693
Iteration 11/25 | Loss: 0.00054693
Iteration 12/25 | Loss: 0.00054693
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0005469288444146514, 0.0005469288444146514, 0.0005469288444146514, 0.0005469288444146514, 0.0005469288444146514]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005469288444146514

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.92736387
Iteration 2/25 | Loss: 0.00016393
Iteration 3/25 | Loss: 0.00016393
Iteration 4/25 | Loss: 0.00016393
Iteration 5/25 | Loss: 0.00016393
Iteration 6/25 | Loss: 0.00016393
Iteration 7/25 | Loss: 0.00016393
Iteration 8/25 | Loss: 0.00016393
Iteration 9/25 | Loss: 0.00016393
Iteration 10/25 | Loss: 0.00016393
Iteration 11/25 | Loss: 0.00016393
Iteration 12/25 | Loss: 0.00016393
Iteration 13/25 | Loss: 0.00016393
Iteration 14/25 | Loss: 0.00016393
Iteration 15/25 | Loss: 0.00016393
Iteration 16/25 | Loss: 0.00016393
Iteration 17/25 | Loss: 0.00016393
Iteration 18/25 | Loss: 0.00016393
Iteration 19/25 | Loss: 0.00016393
Iteration 20/25 | Loss: 0.00016393
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0001639264664845541, 0.0001639264664845541, 0.0001639264664845541, 0.0001639264664845541, 0.0001639264664845541]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0001639264664845541

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00016393
Iteration 2/1000 | Loss: 0.00001693
Iteration 3/1000 | Loss: 0.00001292
Iteration 4/1000 | Loss: 0.00001211
Iteration 5/1000 | Loss: 0.00001162
Iteration 6/1000 | Loss: 0.00001126
Iteration 7/1000 | Loss: 0.00001124
Iteration 8/1000 | Loss: 0.00001123
Iteration 9/1000 | Loss: 0.00001106
Iteration 10/1000 | Loss: 0.00001105
Iteration 11/1000 | Loss: 0.00001105
Iteration 12/1000 | Loss: 0.00001103
Iteration 13/1000 | Loss: 0.00001098
Iteration 14/1000 | Loss: 0.00001089
Iteration 15/1000 | Loss: 0.00001088
Iteration 16/1000 | Loss: 0.00001088
Iteration 17/1000 | Loss: 0.00001086
Iteration 18/1000 | Loss: 0.00001085
Iteration 19/1000 | Loss: 0.00001085
Iteration 20/1000 | Loss: 0.00001085
Iteration 21/1000 | Loss: 0.00001084
Iteration 22/1000 | Loss: 0.00001083
Iteration 23/1000 | Loss: 0.00001083
Iteration 24/1000 | Loss: 0.00001083
Iteration 25/1000 | Loss: 0.00001082
Iteration 26/1000 | Loss: 0.00001081
Iteration 27/1000 | Loss: 0.00001080
Iteration 28/1000 | Loss: 0.00001080
Iteration 29/1000 | Loss: 0.00001079
Iteration 30/1000 | Loss: 0.00001077
Iteration 31/1000 | Loss: 0.00001077
Iteration 32/1000 | Loss: 0.00001076
Iteration 33/1000 | Loss: 0.00001076
Iteration 34/1000 | Loss: 0.00001073
Iteration 35/1000 | Loss: 0.00001073
Iteration 36/1000 | Loss: 0.00001073
Iteration 37/1000 | Loss: 0.00001073
Iteration 38/1000 | Loss: 0.00001072
Iteration 39/1000 | Loss: 0.00001072
Iteration 40/1000 | Loss: 0.00001072
Iteration 41/1000 | Loss: 0.00001072
Iteration 42/1000 | Loss: 0.00001070
Iteration 43/1000 | Loss: 0.00001069
Iteration 44/1000 | Loss: 0.00001068
Iteration 45/1000 | Loss: 0.00001068
Iteration 46/1000 | Loss: 0.00001067
Iteration 47/1000 | Loss: 0.00001067
Iteration 48/1000 | Loss: 0.00001066
Iteration 49/1000 | Loss: 0.00001064
Iteration 50/1000 | Loss: 0.00001064
Iteration 51/1000 | Loss: 0.00001063
Iteration 52/1000 | Loss: 0.00001063
Iteration 53/1000 | Loss: 0.00001062
Iteration 54/1000 | Loss: 0.00001062
Iteration 55/1000 | Loss: 0.00001061
Iteration 56/1000 | Loss: 0.00001061
Iteration 57/1000 | Loss: 0.00001060
Iteration 58/1000 | Loss: 0.00001060
Iteration 59/1000 | Loss: 0.00001060
Iteration 60/1000 | Loss: 0.00001060
Iteration 61/1000 | Loss: 0.00001060
Iteration 62/1000 | Loss: 0.00001060
Iteration 63/1000 | Loss: 0.00001060
Iteration 64/1000 | Loss: 0.00001060
Iteration 65/1000 | Loss: 0.00001059
Iteration 66/1000 | Loss: 0.00001059
Iteration 67/1000 | Loss: 0.00001059
Iteration 68/1000 | Loss: 0.00001059
Iteration 69/1000 | Loss: 0.00001059
Iteration 70/1000 | Loss: 0.00001058
Iteration 71/1000 | Loss: 0.00001058
Iteration 72/1000 | Loss: 0.00001058
Iteration 73/1000 | Loss: 0.00001057
Iteration 74/1000 | Loss: 0.00001057
Iteration 75/1000 | Loss: 0.00001057
Iteration 76/1000 | Loss: 0.00001056
Iteration 77/1000 | Loss: 0.00001056
Iteration 78/1000 | Loss: 0.00001056
Iteration 79/1000 | Loss: 0.00001056
Iteration 80/1000 | Loss: 0.00001056
Iteration 81/1000 | Loss: 0.00001056
Iteration 82/1000 | Loss: 0.00001056
Iteration 83/1000 | Loss: 0.00001056
Iteration 84/1000 | Loss: 0.00001055
Iteration 85/1000 | Loss: 0.00001055
Iteration 86/1000 | Loss: 0.00001055
Iteration 87/1000 | Loss: 0.00001054
Iteration 88/1000 | Loss: 0.00001054
Iteration 89/1000 | Loss: 0.00001054
Iteration 90/1000 | Loss: 0.00001053
Iteration 91/1000 | Loss: 0.00001052
Iteration 92/1000 | Loss: 0.00001051
Iteration 93/1000 | Loss: 0.00001051
Iteration 94/1000 | Loss: 0.00001050
Iteration 95/1000 | Loss: 0.00001050
Iteration 96/1000 | Loss: 0.00001048
Iteration 97/1000 | Loss: 0.00001048
Iteration 98/1000 | Loss: 0.00001048
Iteration 99/1000 | Loss: 0.00001048
Iteration 100/1000 | Loss: 0.00001048
Iteration 101/1000 | Loss: 0.00001047
Iteration 102/1000 | Loss: 0.00001047
Iteration 103/1000 | Loss: 0.00001047
Iteration 104/1000 | Loss: 0.00001047
Iteration 105/1000 | Loss: 0.00001046
Iteration 106/1000 | Loss: 0.00001046
Iteration 107/1000 | Loss: 0.00001046
Iteration 108/1000 | Loss: 0.00001046
Iteration 109/1000 | Loss: 0.00001046
Iteration 110/1000 | Loss: 0.00001046
Iteration 111/1000 | Loss: 0.00001045
Iteration 112/1000 | Loss: 0.00001045
Iteration 113/1000 | Loss: 0.00001045
Iteration 114/1000 | Loss: 0.00001045
Iteration 115/1000 | Loss: 0.00001045
Iteration 116/1000 | Loss: 0.00001045
Iteration 117/1000 | Loss: 0.00001045
Iteration 118/1000 | Loss: 0.00001045
Iteration 119/1000 | Loss: 0.00001044
Iteration 120/1000 | Loss: 0.00001044
Iteration 121/1000 | Loss: 0.00001044
Iteration 122/1000 | Loss: 0.00001044
Iteration 123/1000 | Loss: 0.00001044
Iteration 124/1000 | Loss: 0.00001044
Iteration 125/1000 | Loss: 0.00001044
Iteration 126/1000 | Loss: 0.00001044
Iteration 127/1000 | Loss: 0.00001044
Iteration 128/1000 | Loss: 0.00001044
Iteration 129/1000 | Loss: 0.00001044
Iteration 130/1000 | Loss: 0.00001044
Iteration 131/1000 | Loss: 0.00001044
Iteration 132/1000 | Loss: 0.00001044
Iteration 133/1000 | Loss: 0.00001044
Iteration 134/1000 | Loss: 0.00001044
Iteration 135/1000 | Loss: 0.00001044
Iteration 136/1000 | Loss: 0.00001044
Iteration 137/1000 | Loss: 0.00001044
Iteration 138/1000 | Loss: 0.00001044
Iteration 139/1000 | Loss: 0.00001044
Iteration 140/1000 | Loss: 0.00001044
Iteration 141/1000 | Loss: 0.00001044
Iteration 142/1000 | Loss: 0.00001043
Iteration 143/1000 | Loss: 0.00001043
Iteration 144/1000 | Loss: 0.00001043
Iteration 145/1000 | Loss: 0.00001043
Iteration 146/1000 | Loss: 0.00001043
Iteration 147/1000 | Loss: 0.00001043
Iteration 148/1000 | Loss: 0.00001043
Iteration 149/1000 | Loss: 0.00001043
Iteration 150/1000 | Loss: 0.00001043
Iteration 151/1000 | Loss: 0.00001043
Iteration 152/1000 | Loss: 0.00001043
Iteration 153/1000 | Loss: 0.00001043
Iteration 154/1000 | Loss: 0.00001043
Iteration 155/1000 | Loss: 0.00001043
Iteration 156/1000 | Loss: 0.00001043
Iteration 157/1000 | Loss: 0.00001043
Iteration 158/1000 | Loss: 0.00001043
Iteration 159/1000 | Loss: 0.00001043
Iteration 160/1000 | Loss: 0.00001043
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 160. Stopping optimization.
Last 5 losses: [1.042835083353566e-05, 1.042835083353566e-05, 1.042835083353566e-05, 1.042835083353566e-05, 1.042835083353566e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.042835083353566e-05

Optimization complete. Final v2v error: 2.752140522003174 mm

Highest mean error: 3.0082039833068848 mm for frame 137

Lowest mean error: 2.6346418857574463 mm for frame 118

Saving results

Total time: 34.382936239242554
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ben_posed_008/1085/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_008/1085.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_008/1085
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00886037
Iteration 2/25 | Loss: 0.00077754
Iteration 3/25 | Loss: 0.00061535
Iteration 4/25 | Loss: 0.00058498
Iteration 5/25 | Loss: 0.00057595
Iteration 6/25 | Loss: 0.00057455
Iteration 7/25 | Loss: 0.00057447
Iteration 8/25 | Loss: 0.00057447
Iteration 9/25 | Loss: 0.00057447
Iteration 10/25 | Loss: 0.00057447
Iteration 11/25 | Loss: 0.00057447
Iteration 12/25 | Loss: 0.00057447
Iteration 13/25 | Loss: 0.00057447
Iteration 14/25 | Loss: 0.00057447
Iteration 15/25 | Loss: 0.00057447
Iteration 16/25 | Loss: 0.00057447
Iteration 17/25 | Loss: 0.00057447
Iteration 18/25 | Loss: 0.00057447
Iteration 19/25 | Loss: 0.00057447
Iteration 20/25 | Loss: 0.00057447
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0005744728841818869, 0.0005744728841818869, 0.0005744728841818869, 0.0005744728841818869, 0.0005744728841818869]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005744728841818869

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 6.95862246
Iteration 2/25 | Loss: 0.00012841
Iteration 3/25 | Loss: 0.00012841
Iteration 4/25 | Loss: 0.00012841
Iteration 5/25 | Loss: 0.00012841
Iteration 6/25 | Loss: 0.00012841
Iteration 7/25 | Loss: 0.00012841
Iteration 8/25 | Loss: 0.00012841
Iteration 9/25 | Loss: 0.00012841
Iteration 10/25 | Loss: 0.00012841
Iteration 11/25 | Loss: 0.00012841
Iteration 12/25 | Loss: 0.00012841
Iteration 13/25 | Loss: 0.00012840
Iteration 14/25 | Loss: 0.00012840
Iteration 15/25 | Loss: 0.00012840
Iteration 16/25 | Loss: 0.00012840
Iteration 17/25 | Loss: 0.00012840
Iteration 18/25 | Loss: 0.00012840
Iteration 19/25 | Loss: 0.00012840
Iteration 20/25 | Loss: 0.00012840
Iteration 21/25 | Loss: 0.00012840
Iteration 22/25 | Loss: 0.00012840
Iteration 23/25 | Loss: 0.00012840
Iteration 24/25 | Loss: 0.00012840
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.00012840490671806037, 0.00012840490671806037, 0.00012840490671806037, 0.00012840490671806037, 0.00012840490671806037]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00012840490671806037

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00012840
Iteration 2/1000 | Loss: 0.00002119
Iteration 3/1000 | Loss: 0.00001569
Iteration 4/1000 | Loss: 0.00001438
Iteration 5/1000 | Loss: 0.00001333
Iteration 6/1000 | Loss: 0.00001288
Iteration 7/1000 | Loss: 0.00001257
Iteration 8/1000 | Loss: 0.00001249
Iteration 9/1000 | Loss: 0.00001248
Iteration 10/1000 | Loss: 0.00001247
Iteration 11/1000 | Loss: 0.00001236
Iteration 12/1000 | Loss: 0.00001236
Iteration 13/1000 | Loss: 0.00001233
Iteration 14/1000 | Loss: 0.00001232
Iteration 15/1000 | Loss: 0.00001232
Iteration 16/1000 | Loss: 0.00001232
Iteration 17/1000 | Loss: 0.00001231
Iteration 18/1000 | Loss: 0.00001231
Iteration 19/1000 | Loss: 0.00001230
Iteration 20/1000 | Loss: 0.00001228
Iteration 21/1000 | Loss: 0.00001228
Iteration 22/1000 | Loss: 0.00001228
Iteration 23/1000 | Loss: 0.00001227
Iteration 24/1000 | Loss: 0.00001227
Iteration 25/1000 | Loss: 0.00001227
Iteration 26/1000 | Loss: 0.00001227
Iteration 27/1000 | Loss: 0.00001227
Iteration 28/1000 | Loss: 0.00001225
Iteration 29/1000 | Loss: 0.00001223
Iteration 30/1000 | Loss: 0.00001223
Iteration 31/1000 | Loss: 0.00001222
Iteration 32/1000 | Loss: 0.00001222
Iteration 33/1000 | Loss: 0.00001221
Iteration 34/1000 | Loss: 0.00001221
Iteration 35/1000 | Loss: 0.00001220
Iteration 36/1000 | Loss: 0.00001220
Iteration 37/1000 | Loss: 0.00001220
Iteration 38/1000 | Loss: 0.00001220
Iteration 39/1000 | Loss: 0.00001219
Iteration 40/1000 | Loss: 0.00001219
Iteration 41/1000 | Loss: 0.00001219
Iteration 42/1000 | Loss: 0.00001219
Iteration 43/1000 | Loss: 0.00001219
Iteration 44/1000 | Loss: 0.00001219
Iteration 45/1000 | Loss: 0.00001219
Iteration 46/1000 | Loss: 0.00001219
Iteration 47/1000 | Loss: 0.00001218
Iteration 48/1000 | Loss: 0.00001217
Iteration 49/1000 | Loss: 0.00001217
Iteration 50/1000 | Loss: 0.00001217
Iteration 51/1000 | Loss: 0.00001217
Iteration 52/1000 | Loss: 0.00001216
Iteration 53/1000 | Loss: 0.00001216
Iteration 54/1000 | Loss: 0.00001216
Iteration 55/1000 | Loss: 0.00001216
Iteration 56/1000 | Loss: 0.00001216
Iteration 57/1000 | Loss: 0.00001215
Iteration 58/1000 | Loss: 0.00001215
Iteration 59/1000 | Loss: 0.00001215
Iteration 60/1000 | Loss: 0.00001215
Iteration 61/1000 | Loss: 0.00001215
Iteration 62/1000 | Loss: 0.00001215
Iteration 63/1000 | Loss: 0.00001214
Iteration 64/1000 | Loss: 0.00001214
Iteration 65/1000 | Loss: 0.00001214
Iteration 66/1000 | Loss: 0.00001214
Iteration 67/1000 | Loss: 0.00001214
Iteration 68/1000 | Loss: 0.00001214
Iteration 69/1000 | Loss: 0.00001214
Iteration 70/1000 | Loss: 0.00001213
Iteration 71/1000 | Loss: 0.00001213
Iteration 72/1000 | Loss: 0.00001213
Iteration 73/1000 | Loss: 0.00001213
Iteration 74/1000 | Loss: 0.00001213
Iteration 75/1000 | Loss: 0.00001213
Iteration 76/1000 | Loss: 0.00001213
Iteration 77/1000 | Loss: 0.00001213
Iteration 78/1000 | Loss: 0.00001213
Iteration 79/1000 | Loss: 0.00001213
Iteration 80/1000 | Loss: 0.00001212
Iteration 81/1000 | Loss: 0.00001212
Iteration 82/1000 | Loss: 0.00001212
Iteration 83/1000 | Loss: 0.00001212
Iteration 84/1000 | Loss: 0.00001212
Iteration 85/1000 | Loss: 0.00001212
Iteration 86/1000 | Loss: 0.00001212
Iteration 87/1000 | Loss: 0.00001212
Iteration 88/1000 | Loss: 0.00001212
Iteration 89/1000 | Loss: 0.00001212
Iteration 90/1000 | Loss: 0.00001212
Iteration 91/1000 | Loss: 0.00001212
Iteration 92/1000 | Loss: 0.00001211
Iteration 93/1000 | Loss: 0.00001211
Iteration 94/1000 | Loss: 0.00001211
Iteration 95/1000 | Loss: 0.00001211
Iteration 96/1000 | Loss: 0.00001211
Iteration 97/1000 | Loss: 0.00001211
Iteration 98/1000 | Loss: 0.00001211
Iteration 99/1000 | Loss: 0.00001211
Iteration 100/1000 | Loss: 0.00001211
Iteration 101/1000 | Loss: 0.00001211
Iteration 102/1000 | Loss: 0.00001210
Iteration 103/1000 | Loss: 0.00001210
Iteration 104/1000 | Loss: 0.00001210
Iteration 105/1000 | Loss: 0.00001210
Iteration 106/1000 | Loss: 0.00001210
Iteration 107/1000 | Loss: 0.00001210
Iteration 108/1000 | Loss: 0.00001210
Iteration 109/1000 | Loss: 0.00001210
Iteration 110/1000 | Loss: 0.00001210
Iteration 111/1000 | Loss: 0.00001210
Iteration 112/1000 | Loss: 0.00001210
Iteration 113/1000 | Loss: 0.00001210
Iteration 114/1000 | Loss: 0.00001210
Iteration 115/1000 | Loss: 0.00001210
Iteration 116/1000 | Loss: 0.00001210
Iteration 117/1000 | Loss: 0.00001210
Iteration 118/1000 | Loss: 0.00001209
Iteration 119/1000 | Loss: 0.00001209
Iteration 120/1000 | Loss: 0.00001209
Iteration 121/1000 | Loss: 0.00001209
Iteration 122/1000 | Loss: 0.00001209
Iteration 123/1000 | Loss: 0.00001209
Iteration 124/1000 | Loss: 0.00001209
Iteration 125/1000 | Loss: 0.00001209
Iteration 126/1000 | Loss: 0.00001209
Iteration 127/1000 | Loss: 0.00001209
Iteration 128/1000 | Loss: 0.00001209
Iteration 129/1000 | Loss: 0.00001209
Iteration 130/1000 | Loss: 0.00001209
Iteration 131/1000 | Loss: 0.00001209
Iteration 132/1000 | Loss: 0.00001208
Iteration 133/1000 | Loss: 0.00001208
Iteration 134/1000 | Loss: 0.00001208
Iteration 135/1000 | Loss: 0.00001208
Iteration 136/1000 | Loss: 0.00001208
Iteration 137/1000 | Loss: 0.00001208
Iteration 138/1000 | Loss: 0.00001207
Iteration 139/1000 | Loss: 0.00001207
Iteration 140/1000 | Loss: 0.00001207
Iteration 141/1000 | Loss: 0.00001207
Iteration 142/1000 | Loss: 0.00001207
Iteration 143/1000 | Loss: 0.00001207
Iteration 144/1000 | Loss: 0.00001207
Iteration 145/1000 | Loss: 0.00001207
Iteration 146/1000 | Loss: 0.00001207
Iteration 147/1000 | Loss: 0.00001207
Iteration 148/1000 | Loss: 0.00001207
Iteration 149/1000 | Loss: 0.00001207
Iteration 150/1000 | Loss: 0.00001206
Iteration 151/1000 | Loss: 0.00001206
Iteration 152/1000 | Loss: 0.00001206
Iteration 153/1000 | Loss: 0.00001206
Iteration 154/1000 | Loss: 0.00001206
Iteration 155/1000 | Loss: 0.00001206
Iteration 156/1000 | Loss: 0.00001206
Iteration 157/1000 | Loss: 0.00001206
Iteration 158/1000 | Loss: 0.00001206
Iteration 159/1000 | Loss: 0.00001206
Iteration 160/1000 | Loss: 0.00001206
Iteration 161/1000 | Loss: 0.00001205
Iteration 162/1000 | Loss: 0.00001205
Iteration 163/1000 | Loss: 0.00001205
Iteration 164/1000 | Loss: 0.00001205
Iteration 165/1000 | Loss: 0.00001205
Iteration 166/1000 | Loss: 0.00001205
Iteration 167/1000 | Loss: 0.00001205
Iteration 168/1000 | Loss: 0.00001205
Iteration 169/1000 | Loss: 0.00001205
Iteration 170/1000 | Loss: 0.00001205
Iteration 171/1000 | Loss: 0.00001205
Iteration 172/1000 | Loss: 0.00001205
Iteration 173/1000 | Loss: 0.00001205
Iteration 174/1000 | Loss: 0.00001205
Iteration 175/1000 | Loss: 0.00001205
Iteration 176/1000 | Loss: 0.00001205
Iteration 177/1000 | Loss: 0.00001205
Iteration 178/1000 | Loss: 0.00001205
Iteration 179/1000 | Loss: 0.00001205
Iteration 180/1000 | Loss: 0.00001205
Iteration 181/1000 | Loss: 0.00001205
Iteration 182/1000 | Loss: 0.00001205
Iteration 183/1000 | Loss: 0.00001205
Iteration 184/1000 | Loss: 0.00001205
Iteration 185/1000 | Loss: 0.00001205
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 185. Stopping optimization.
Last 5 losses: [1.2050597433699295e-05, 1.2050597433699295e-05, 1.2050597433699295e-05, 1.2050597433699295e-05, 1.2050597433699295e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2050597433699295e-05

Optimization complete. Final v2v error: 2.9520328044891357 mm

Highest mean error: 3.146540880203247 mm for frame 237

Lowest mean error: 2.8272199630737305 mm for frame 41

Saving results

Total time: 37.56691288948059
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ben_posed_008/1045/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_008/1045.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_008/1045
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00777764
Iteration 2/25 | Loss: 0.00089557
Iteration 3/25 | Loss: 0.00069361
Iteration 4/25 | Loss: 0.00064614
Iteration 5/25 | Loss: 0.00063769
Iteration 6/25 | Loss: 0.00063631
Iteration 7/25 | Loss: 0.00063584
Iteration 8/25 | Loss: 0.00063584
Iteration 9/25 | Loss: 0.00063584
Iteration 10/25 | Loss: 0.00063584
Iteration 11/25 | Loss: 0.00063584
Iteration 12/25 | Loss: 0.00063584
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0006358444225043058, 0.0006358444225043058, 0.0006358444225043058, 0.0006358444225043058, 0.0006358444225043058]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006358444225043058

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38964832
Iteration 2/25 | Loss: 0.00015819
Iteration 3/25 | Loss: 0.00015816
Iteration 4/25 | Loss: 0.00015816
Iteration 5/25 | Loss: 0.00015816
Iteration 6/25 | Loss: 0.00015816
Iteration 7/25 | Loss: 0.00015816
Iteration 8/25 | Loss: 0.00015816
Iteration 9/25 | Loss: 0.00015816
Iteration 10/25 | Loss: 0.00015816
Iteration 11/25 | Loss: 0.00015816
Iteration 12/25 | Loss: 0.00015816
Iteration 13/25 | Loss: 0.00015816
Iteration 14/25 | Loss: 0.00015816
Iteration 15/25 | Loss: 0.00015816
Iteration 16/25 | Loss: 0.00015816
Iteration 17/25 | Loss: 0.00015816
Iteration 18/25 | Loss: 0.00015816
Iteration 19/25 | Loss: 0.00015816
Iteration 20/25 | Loss: 0.00015816
Iteration 21/25 | Loss: 0.00015816
Iteration 22/25 | Loss: 0.00015816
Iteration 23/25 | Loss: 0.00015816
Iteration 24/25 | Loss: 0.00015816
Iteration 25/25 | Loss: 0.00015816

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00015816
Iteration 2/1000 | Loss: 0.00002813
Iteration 3/1000 | Loss: 0.00002207
Iteration 4/1000 | Loss: 0.00001950
Iteration 5/1000 | Loss: 0.00001852
Iteration 6/1000 | Loss: 0.00001800
Iteration 7/1000 | Loss: 0.00001761
Iteration 8/1000 | Loss: 0.00001730
Iteration 9/1000 | Loss: 0.00001718
Iteration 10/1000 | Loss: 0.00001717
Iteration 11/1000 | Loss: 0.00001717
Iteration 12/1000 | Loss: 0.00001705
Iteration 13/1000 | Loss: 0.00001700
Iteration 14/1000 | Loss: 0.00001696
Iteration 15/1000 | Loss: 0.00001696
Iteration 16/1000 | Loss: 0.00001695
Iteration 17/1000 | Loss: 0.00001695
Iteration 18/1000 | Loss: 0.00001693
Iteration 19/1000 | Loss: 0.00001692
Iteration 20/1000 | Loss: 0.00001692
Iteration 21/1000 | Loss: 0.00001691
Iteration 22/1000 | Loss: 0.00001691
Iteration 23/1000 | Loss: 0.00001689
Iteration 24/1000 | Loss: 0.00001689
Iteration 25/1000 | Loss: 0.00001688
Iteration 26/1000 | Loss: 0.00001688
Iteration 27/1000 | Loss: 0.00001688
Iteration 28/1000 | Loss: 0.00001688
Iteration 29/1000 | Loss: 0.00001688
Iteration 30/1000 | Loss: 0.00001688
Iteration 31/1000 | Loss: 0.00001687
Iteration 32/1000 | Loss: 0.00001687
Iteration 33/1000 | Loss: 0.00001687
Iteration 34/1000 | Loss: 0.00001687
Iteration 35/1000 | Loss: 0.00001687
Iteration 36/1000 | Loss: 0.00001687
Iteration 37/1000 | Loss: 0.00001686
Iteration 38/1000 | Loss: 0.00001683
Iteration 39/1000 | Loss: 0.00001683
Iteration 40/1000 | Loss: 0.00001683
Iteration 41/1000 | Loss: 0.00001683
Iteration 42/1000 | Loss: 0.00001683
Iteration 43/1000 | Loss: 0.00001683
Iteration 44/1000 | Loss: 0.00001683
Iteration 45/1000 | Loss: 0.00001683
Iteration 46/1000 | Loss: 0.00001682
Iteration 47/1000 | Loss: 0.00001680
Iteration 48/1000 | Loss: 0.00001680
Iteration 49/1000 | Loss: 0.00001680
Iteration 50/1000 | Loss: 0.00001680
Iteration 51/1000 | Loss: 0.00001680
Iteration 52/1000 | Loss: 0.00001679
Iteration 53/1000 | Loss: 0.00001679
Iteration 54/1000 | Loss: 0.00001679
Iteration 55/1000 | Loss: 0.00001676
Iteration 56/1000 | Loss: 0.00001675
Iteration 57/1000 | Loss: 0.00001675
Iteration 58/1000 | Loss: 0.00001675
Iteration 59/1000 | Loss: 0.00001675
Iteration 60/1000 | Loss: 0.00001674
Iteration 61/1000 | Loss: 0.00001674
Iteration 62/1000 | Loss: 0.00001673
Iteration 63/1000 | Loss: 0.00001673
Iteration 64/1000 | Loss: 0.00001673
Iteration 65/1000 | Loss: 0.00001672
Iteration 66/1000 | Loss: 0.00001672
Iteration 67/1000 | Loss: 0.00001671
Iteration 68/1000 | Loss: 0.00001670
Iteration 69/1000 | Loss: 0.00001670
Iteration 70/1000 | Loss: 0.00001670
Iteration 71/1000 | Loss: 0.00001669
Iteration 72/1000 | Loss: 0.00001669
Iteration 73/1000 | Loss: 0.00001669
Iteration 74/1000 | Loss: 0.00001669
Iteration 75/1000 | Loss: 0.00001669
Iteration 76/1000 | Loss: 0.00001669
Iteration 77/1000 | Loss: 0.00001669
Iteration 78/1000 | Loss: 0.00001669
Iteration 79/1000 | Loss: 0.00001669
Iteration 80/1000 | Loss: 0.00001668
Iteration 81/1000 | Loss: 0.00001668
Iteration 82/1000 | Loss: 0.00001668
Iteration 83/1000 | Loss: 0.00001667
Iteration 84/1000 | Loss: 0.00001667
Iteration 85/1000 | Loss: 0.00001667
Iteration 86/1000 | Loss: 0.00001666
Iteration 87/1000 | Loss: 0.00001666
Iteration 88/1000 | Loss: 0.00001666
Iteration 89/1000 | Loss: 0.00001666
Iteration 90/1000 | Loss: 0.00001665
Iteration 91/1000 | Loss: 0.00001665
Iteration 92/1000 | Loss: 0.00001665
Iteration 93/1000 | Loss: 0.00001664
Iteration 94/1000 | Loss: 0.00001664
Iteration 95/1000 | Loss: 0.00001664
Iteration 96/1000 | Loss: 0.00001664
Iteration 97/1000 | Loss: 0.00001664
Iteration 98/1000 | Loss: 0.00001664
Iteration 99/1000 | Loss: 0.00001663
Iteration 100/1000 | Loss: 0.00001663
Iteration 101/1000 | Loss: 0.00001663
Iteration 102/1000 | Loss: 0.00001663
Iteration 103/1000 | Loss: 0.00001663
Iteration 104/1000 | Loss: 0.00001663
Iteration 105/1000 | Loss: 0.00001662
Iteration 106/1000 | Loss: 0.00001662
Iteration 107/1000 | Loss: 0.00001662
Iteration 108/1000 | Loss: 0.00001662
Iteration 109/1000 | Loss: 0.00001661
Iteration 110/1000 | Loss: 0.00001661
Iteration 111/1000 | Loss: 0.00001661
Iteration 112/1000 | Loss: 0.00001661
Iteration 113/1000 | Loss: 0.00001660
Iteration 114/1000 | Loss: 0.00001660
Iteration 115/1000 | Loss: 0.00001660
Iteration 116/1000 | Loss: 0.00001660
Iteration 117/1000 | Loss: 0.00001660
Iteration 118/1000 | Loss: 0.00001660
Iteration 119/1000 | Loss: 0.00001660
Iteration 120/1000 | Loss: 0.00001660
Iteration 121/1000 | Loss: 0.00001660
Iteration 122/1000 | Loss: 0.00001660
Iteration 123/1000 | Loss: 0.00001660
Iteration 124/1000 | Loss: 0.00001660
Iteration 125/1000 | Loss: 0.00001660
Iteration 126/1000 | Loss: 0.00001660
Iteration 127/1000 | Loss: 0.00001660
Iteration 128/1000 | Loss: 0.00001660
Iteration 129/1000 | Loss: 0.00001660
Iteration 130/1000 | Loss: 0.00001660
Iteration 131/1000 | Loss: 0.00001660
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 131. Stopping optimization.
Last 5 losses: [1.6598232832620852e-05, 1.6598232832620852e-05, 1.6598232832620852e-05, 1.6598232832620852e-05, 1.6598232832620852e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6598232832620852e-05

Optimization complete. Final v2v error: 3.4787862300872803 mm

Highest mean error: 3.9270617961883545 mm for frame 4

Lowest mean error: 3.211228370666504 mm for frame 239

Saving results

Total time: 40.759663581848145
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ben_posed_008/1070/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_008/1070.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_008/1070
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00636016
Iteration 2/25 | Loss: 0.00090888
Iteration 3/25 | Loss: 0.00073238
Iteration 4/25 | Loss: 0.00070015
Iteration 5/25 | Loss: 0.00068864
Iteration 6/25 | Loss: 0.00068739
Iteration 7/25 | Loss: 0.00068739
Iteration 8/25 | Loss: 0.00068739
Iteration 9/25 | Loss: 0.00068739
Iteration 10/25 | Loss: 0.00068739
Iteration 11/25 | Loss: 0.00068739
Iteration 12/25 | Loss: 0.00068739
Iteration 13/25 | Loss: 0.00068739
Iteration 14/25 | Loss: 0.00068739
Iteration 15/25 | Loss: 0.00068739
Iteration 16/25 | Loss: 0.00068739
Iteration 17/25 | Loss: 0.00068739
Iteration 18/25 | Loss: 0.00068739
Iteration 19/25 | Loss: 0.00068739
Iteration 20/25 | Loss: 0.00068739
Iteration 21/25 | Loss: 0.00068739
Iteration 22/25 | Loss: 0.00068739
Iteration 23/25 | Loss: 0.00068739
Iteration 24/25 | Loss: 0.00068739
Iteration 25/25 | Loss: 0.00068739

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.20241725
Iteration 2/25 | Loss: 0.00019158
Iteration 3/25 | Loss: 0.00019155
Iteration 4/25 | Loss: 0.00019155
Iteration 5/25 | Loss: 0.00019155
Iteration 6/25 | Loss: 0.00019155
Iteration 7/25 | Loss: 0.00019155
Iteration 8/25 | Loss: 0.00019155
Iteration 9/25 | Loss: 0.00019155
Iteration 10/25 | Loss: 0.00019155
Iteration 11/25 | Loss: 0.00019155
Iteration 12/25 | Loss: 0.00019155
Iteration 13/25 | Loss: 0.00019155
Iteration 14/25 | Loss: 0.00019155
Iteration 15/25 | Loss: 0.00019155
Iteration 16/25 | Loss: 0.00019155
Iteration 17/25 | Loss: 0.00019155
Iteration 18/25 | Loss: 0.00019155
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.00019155150221195072, 0.00019155150221195072, 0.00019155150221195072, 0.00019155150221195072, 0.00019155150221195072]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00019155150221195072

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00019155
Iteration 2/1000 | Loss: 0.00003196
Iteration 3/1000 | Loss: 0.00002672
Iteration 4/1000 | Loss: 0.00002512
Iteration 5/1000 | Loss: 0.00002385
Iteration 6/1000 | Loss: 0.00002307
Iteration 7/1000 | Loss: 0.00002241
Iteration 8/1000 | Loss: 0.00002203
Iteration 9/1000 | Loss: 0.00002184
Iteration 10/1000 | Loss: 0.00002174
Iteration 11/1000 | Loss: 0.00002169
Iteration 12/1000 | Loss: 0.00002168
Iteration 13/1000 | Loss: 0.00002163
Iteration 14/1000 | Loss: 0.00002163
Iteration 15/1000 | Loss: 0.00002162
Iteration 16/1000 | Loss: 0.00002161
Iteration 17/1000 | Loss: 0.00002156
Iteration 18/1000 | Loss: 0.00002152
Iteration 19/1000 | Loss: 0.00002152
Iteration 20/1000 | Loss: 0.00002152
Iteration 21/1000 | Loss: 0.00002151
Iteration 22/1000 | Loss: 0.00002150
Iteration 23/1000 | Loss: 0.00002150
Iteration 24/1000 | Loss: 0.00002150
Iteration 25/1000 | Loss: 0.00002149
Iteration 26/1000 | Loss: 0.00002149
Iteration 27/1000 | Loss: 0.00002149
Iteration 28/1000 | Loss: 0.00002148
Iteration 29/1000 | Loss: 0.00002148
Iteration 30/1000 | Loss: 0.00002148
Iteration 31/1000 | Loss: 0.00002147
Iteration 32/1000 | Loss: 0.00002147
Iteration 33/1000 | Loss: 0.00002147
Iteration 34/1000 | Loss: 0.00002147
Iteration 35/1000 | Loss: 0.00002147
Iteration 36/1000 | Loss: 0.00002147
Iteration 37/1000 | Loss: 0.00002146
Iteration 38/1000 | Loss: 0.00002146
Iteration 39/1000 | Loss: 0.00002146
Iteration 40/1000 | Loss: 0.00002146
Iteration 41/1000 | Loss: 0.00002146
Iteration 42/1000 | Loss: 0.00002146
Iteration 43/1000 | Loss: 0.00002145
Iteration 44/1000 | Loss: 0.00002145
Iteration 45/1000 | Loss: 0.00002145
Iteration 46/1000 | Loss: 0.00002145
Iteration 47/1000 | Loss: 0.00002145
Iteration 48/1000 | Loss: 0.00002145
Iteration 49/1000 | Loss: 0.00002145
Iteration 50/1000 | Loss: 0.00002145
Iteration 51/1000 | Loss: 0.00002145
Iteration 52/1000 | Loss: 0.00002145
Iteration 53/1000 | Loss: 0.00002145
Iteration 54/1000 | Loss: 0.00002144
Iteration 55/1000 | Loss: 0.00002144
Iteration 56/1000 | Loss: 0.00002144
Iteration 57/1000 | Loss: 0.00002144
Iteration 58/1000 | Loss: 0.00002144
Iteration 59/1000 | Loss: 0.00002144
Iteration 60/1000 | Loss: 0.00002144
Iteration 61/1000 | Loss: 0.00002144
Iteration 62/1000 | Loss: 0.00002144
Iteration 63/1000 | Loss: 0.00002144
Iteration 64/1000 | Loss: 0.00002144
Iteration 65/1000 | Loss: 0.00002144
Iteration 66/1000 | Loss: 0.00002144
Iteration 67/1000 | Loss: 0.00002144
Iteration 68/1000 | Loss: 0.00002144
Iteration 69/1000 | Loss: 0.00002144
Iteration 70/1000 | Loss: 0.00002144
Iteration 71/1000 | Loss: 0.00002144
Iteration 72/1000 | Loss: 0.00002144
Iteration 73/1000 | Loss: 0.00002143
Iteration 74/1000 | Loss: 0.00002143
Iteration 75/1000 | Loss: 0.00002143
Iteration 76/1000 | Loss: 0.00002143
Iteration 77/1000 | Loss: 0.00002143
Iteration 78/1000 | Loss: 0.00002143
Iteration 79/1000 | Loss: 0.00002143
Iteration 80/1000 | Loss: 0.00002143
Iteration 81/1000 | Loss: 0.00002143
Iteration 82/1000 | Loss: 0.00002143
Iteration 83/1000 | Loss: 0.00002143
Iteration 84/1000 | Loss: 0.00002143
Iteration 85/1000 | Loss: 0.00002143
Iteration 86/1000 | Loss: 0.00002143
Iteration 87/1000 | Loss: 0.00002143
Iteration 88/1000 | Loss: 0.00002143
Iteration 89/1000 | Loss: 0.00002143
Iteration 90/1000 | Loss: 0.00002143
Iteration 91/1000 | Loss: 0.00002143
Iteration 92/1000 | Loss: 0.00002143
Iteration 93/1000 | Loss: 0.00002142
Iteration 94/1000 | Loss: 0.00002142
Iteration 95/1000 | Loss: 0.00002142
Iteration 96/1000 | Loss: 0.00002142
Iteration 97/1000 | Loss: 0.00002142
Iteration 98/1000 | Loss: 0.00002142
Iteration 99/1000 | Loss: 0.00002142
Iteration 100/1000 | Loss: 0.00002142
Iteration 101/1000 | Loss: 0.00002142
Iteration 102/1000 | Loss: 0.00002142
Iteration 103/1000 | Loss: 0.00002142
Iteration 104/1000 | Loss: 0.00002142
Iteration 105/1000 | Loss: 0.00002142
Iteration 106/1000 | Loss: 0.00002142
Iteration 107/1000 | Loss: 0.00002142
Iteration 108/1000 | Loss: 0.00002142
Iteration 109/1000 | Loss: 0.00002142
Iteration 110/1000 | Loss: 0.00002142
Iteration 111/1000 | Loss: 0.00002142
Iteration 112/1000 | Loss: 0.00002141
Iteration 113/1000 | Loss: 0.00002141
Iteration 114/1000 | Loss: 0.00002141
Iteration 115/1000 | Loss: 0.00002141
Iteration 116/1000 | Loss: 0.00002141
Iteration 117/1000 | Loss: 0.00002141
Iteration 118/1000 | Loss: 0.00002141
Iteration 119/1000 | Loss: 0.00002141
Iteration 120/1000 | Loss: 0.00002141
Iteration 121/1000 | Loss: 0.00002141
Iteration 122/1000 | Loss: 0.00002141
Iteration 123/1000 | Loss: 0.00002141
Iteration 124/1000 | Loss: 0.00002141
Iteration 125/1000 | Loss: 0.00002141
Iteration 126/1000 | Loss: 0.00002141
Iteration 127/1000 | Loss: 0.00002141
Iteration 128/1000 | Loss: 0.00002141
Iteration 129/1000 | Loss: 0.00002141
Iteration 130/1000 | Loss: 0.00002141
Iteration 131/1000 | Loss: 0.00002141
Iteration 132/1000 | Loss: 0.00002141
Iteration 133/1000 | Loss: 0.00002141
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 133. Stopping optimization.
Last 5 losses: [2.1405208826763555e-05, 2.1405208826763555e-05, 2.1405208826763555e-05, 2.1405208826763555e-05, 2.1405208826763555e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1405208826763555e-05

Optimization complete. Final v2v error: 3.912729263305664 mm

Highest mean error: 4.468482971191406 mm for frame 185

Lowest mean error: 3.558170795440674 mm for frame 135

Saving results

Total time: 36.83722996711731
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ben_posed_008/1049/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_008/1049.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_008/1049
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01166289
Iteration 2/25 | Loss: 0.00203601
Iteration 3/25 | Loss: 0.00110972
Iteration 4/25 | Loss: 0.00096800
Iteration 5/25 | Loss: 0.00095122
Iteration 6/25 | Loss: 0.00093015
Iteration 7/25 | Loss: 0.00089439
Iteration 8/25 | Loss: 0.00090043
Iteration 9/25 | Loss: 0.00089034
Iteration 10/25 | Loss: 0.00086959
Iteration 11/25 | Loss: 0.00086178
Iteration 12/25 | Loss: 0.00086439
Iteration 13/25 | Loss: 0.00086110
Iteration 14/25 | Loss: 0.00085649
Iteration 15/25 | Loss: 0.00085486
Iteration 16/25 | Loss: 0.00084172
Iteration 17/25 | Loss: 0.00084221
Iteration 18/25 | Loss: 0.00084085
Iteration 19/25 | Loss: 0.00083825
Iteration 20/25 | Loss: 0.00083269
Iteration 21/25 | Loss: 0.00083117
Iteration 22/25 | Loss: 0.00082544
Iteration 23/25 | Loss: 0.00082510
Iteration 24/25 | Loss: 0.00082243
Iteration 25/25 | Loss: 0.00082288

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.98898309
Iteration 2/25 | Loss: 0.00114226
Iteration 3/25 | Loss: 0.00114226
Iteration 4/25 | Loss: 0.00114226
Iteration 5/25 | Loss: 0.00114226
Iteration 6/25 | Loss: 0.00114226
Iteration 7/25 | Loss: 0.00114226
Iteration 8/25 | Loss: 0.00114226
Iteration 9/25 | Loss: 0.00114226
Iteration 10/25 | Loss: 0.00114226
Iteration 11/25 | Loss: 0.00114226
Iteration 12/25 | Loss: 0.00114226
Iteration 13/25 | Loss: 0.00114226
Iteration 14/25 | Loss: 0.00114226
Iteration 15/25 | Loss: 0.00114226
Iteration 16/25 | Loss: 0.00114226
Iteration 17/25 | Loss: 0.00114226
Iteration 18/25 | Loss: 0.00114226
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.001142258639447391, 0.001142258639447391, 0.001142258639447391, 0.001142258639447391, 0.001142258639447391]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001142258639447391

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00114226
Iteration 2/1000 | Loss: 0.00044675
Iteration 3/1000 | Loss: 0.00032375
Iteration 4/1000 | Loss: 0.00025210
Iteration 5/1000 | Loss: 0.00025961
Iteration 6/1000 | Loss: 0.00115283
Iteration 7/1000 | Loss: 0.00020494
Iteration 8/1000 | Loss: 0.00023918
Iteration 9/1000 | Loss: 0.00041569
Iteration 10/1000 | Loss: 0.00085814
Iteration 11/1000 | Loss: 0.00080275
Iteration 12/1000 | Loss: 0.00027932
Iteration 13/1000 | Loss: 0.00025242
Iteration 14/1000 | Loss: 0.00007193
Iteration 15/1000 | Loss: 0.00024215
Iteration 16/1000 | Loss: 0.00007884
Iteration 17/1000 | Loss: 0.00096423
Iteration 18/1000 | Loss: 0.00025484
Iteration 19/1000 | Loss: 0.00020097
Iteration 20/1000 | Loss: 0.00017282
Iteration 21/1000 | Loss: 0.00036531
Iteration 22/1000 | Loss: 0.00016490
Iteration 23/1000 | Loss: 0.00079903
Iteration 24/1000 | Loss: 0.00079697
Iteration 25/1000 | Loss: 0.00014560
Iteration 26/1000 | Loss: 0.00086743
Iteration 27/1000 | Loss: 0.00141253
Iteration 28/1000 | Loss: 0.00046091
Iteration 29/1000 | Loss: 0.00017870
Iteration 30/1000 | Loss: 0.00055759
Iteration 31/1000 | Loss: 0.00055821
Iteration 32/1000 | Loss: 0.00098186
Iteration 33/1000 | Loss: 0.00063894
Iteration 34/1000 | Loss: 0.00018531
Iteration 35/1000 | Loss: 0.00113017
Iteration 36/1000 | Loss: 0.00016829
Iteration 37/1000 | Loss: 0.00026381
Iteration 38/1000 | Loss: 0.00041983
Iteration 39/1000 | Loss: 0.00027359
Iteration 40/1000 | Loss: 0.00016367
Iteration 41/1000 | Loss: 0.00040901
Iteration 42/1000 | Loss: 0.00114928
Iteration 43/1000 | Loss: 0.00009782
Iteration 44/1000 | Loss: 0.00045216
Iteration 45/1000 | Loss: 0.00035775
Iteration 46/1000 | Loss: 0.00044936
Iteration 47/1000 | Loss: 0.00017780
Iteration 48/1000 | Loss: 0.00017969
Iteration 49/1000 | Loss: 0.00012723
Iteration 50/1000 | Loss: 0.00061867
Iteration 51/1000 | Loss: 0.00025131
Iteration 52/1000 | Loss: 0.00018805
Iteration 53/1000 | Loss: 0.00016788
Iteration 54/1000 | Loss: 0.00016713
Iteration 55/1000 | Loss: 0.00014588
Iteration 56/1000 | Loss: 0.00017088
Iteration 57/1000 | Loss: 0.00018801
Iteration 58/1000 | Loss: 0.00018320
Iteration 59/1000 | Loss: 0.00018185
Iteration 60/1000 | Loss: 0.00022758
Iteration 61/1000 | Loss: 0.00022393
Iteration 62/1000 | Loss: 0.00020068
Iteration 63/1000 | Loss: 0.00036351
Iteration 64/1000 | Loss: 0.00014475
Iteration 65/1000 | Loss: 0.00022390
Iteration 66/1000 | Loss: 0.00005205
Iteration 67/1000 | Loss: 0.00018780
Iteration 68/1000 | Loss: 0.00014179
Iteration 69/1000 | Loss: 0.00012789
Iteration 70/1000 | Loss: 0.00008440
Iteration 71/1000 | Loss: 0.00005460
Iteration 72/1000 | Loss: 0.00003998
Iteration 73/1000 | Loss: 0.00014256
Iteration 74/1000 | Loss: 0.00014079
Iteration 75/1000 | Loss: 0.00010778
Iteration 76/1000 | Loss: 0.00013653
Iteration 77/1000 | Loss: 0.00003697
Iteration 78/1000 | Loss: 0.00003357
Iteration 79/1000 | Loss: 0.00014394
Iteration 80/1000 | Loss: 0.00014505
Iteration 81/1000 | Loss: 0.00012816
Iteration 82/1000 | Loss: 0.00012761
Iteration 83/1000 | Loss: 0.00017058
Iteration 84/1000 | Loss: 0.00003808
Iteration 85/1000 | Loss: 0.00003427
Iteration 86/1000 | Loss: 0.00003308
Iteration 87/1000 | Loss: 0.00003245
Iteration 88/1000 | Loss: 0.00017780
Iteration 89/1000 | Loss: 0.00016608
Iteration 90/1000 | Loss: 0.00028536
Iteration 91/1000 | Loss: 0.00018246
Iteration 92/1000 | Loss: 0.00019212
Iteration 93/1000 | Loss: 0.00014510
Iteration 94/1000 | Loss: 0.00003449
Iteration 95/1000 | Loss: 0.00003319
Iteration 96/1000 | Loss: 0.00017386
Iteration 97/1000 | Loss: 0.00014702
Iteration 98/1000 | Loss: 0.00028854
Iteration 99/1000 | Loss: 0.00014949
Iteration 100/1000 | Loss: 0.00019972
Iteration 101/1000 | Loss: 0.00013673
Iteration 102/1000 | Loss: 0.00019673
Iteration 103/1000 | Loss: 0.00005633
Iteration 104/1000 | Loss: 0.00003367
Iteration 105/1000 | Loss: 0.00003164
Iteration 106/1000 | Loss: 0.00003100
Iteration 107/1000 | Loss: 0.00003064
Iteration 108/1000 | Loss: 0.00003034
Iteration 109/1000 | Loss: 0.00003011
Iteration 110/1000 | Loss: 0.00020388
Iteration 111/1000 | Loss: 0.00053423
Iteration 112/1000 | Loss: 0.00077730
Iteration 113/1000 | Loss: 0.00021967
Iteration 114/1000 | Loss: 0.00020727
Iteration 115/1000 | Loss: 0.00015297
Iteration 116/1000 | Loss: 0.00003354
Iteration 117/1000 | Loss: 0.00003044
Iteration 118/1000 | Loss: 0.00002982
Iteration 119/1000 | Loss: 0.00002958
Iteration 120/1000 | Loss: 0.00002955
Iteration 121/1000 | Loss: 0.00002943
Iteration 122/1000 | Loss: 0.00002943
Iteration 123/1000 | Loss: 0.00002943
Iteration 124/1000 | Loss: 0.00002943
Iteration 125/1000 | Loss: 0.00002942
Iteration 126/1000 | Loss: 0.00002942
Iteration 127/1000 | Loss: 0.00002942
Iteration 128/1000 | Loss: 0.00002942
Iteration 129/1000 | Loss: 0.00002942
Iteration 130/1000 | Loss: 0.00002942
Iteration 131/1000 | Loss: 0.00002942
Iteration 132/1000 | Loss: 0.00002942
Iteration 133/1000 | Loss: 0.00002942
Iteration 134/1000 | Loss: 0.00002942
Iteration 135/1000 | Loss: 0.00002941
Iteration 136/1000 | Loss: 0.00002941
Iteration 137/1000 | Loss: 0.00002941
Iteration 138/1000 | Loss: 0.00002941
Iteration 139/1000 | Loss: 0.00002941
Iteration 140/1000 | Loss: 0.00002940
Iteration 141/1000 | Loss: 0.00002940
Iteration 142/1000 | Loss: 0.00002939
Iteration 143/1000 | Loss: 0.00002939
Iteration 144/1000 | Loss: 0.00002939
Iteration 145/1000 | Loss: 0.00002939
Iteration 146/1000 | Loss: 0.00002939
Iteration 147/1000 | Loss: 0.00002939
Iteration 148/1000 | Loss: 0.00002939
Iteration 149/1000 | Loss: 0.00002938
Iteration 150/1000 | Loss: 0.00002938
Iteration 151/1000 | Loss: 0.00002938
Iteration 152/1000 | Loss: 0.00002938
Iteration 153/1000 | Loss: 0.00002938
Iteration 154/1000 | Loss: 0.00002938
Iteration 155/1000 | Loss: 0.00002938
Iteration 156/1000 | Loss: 0.00002937
Iteration 157/1000 | Loss: 0.00002937
Iteration 158/1000 | Loss: 0.00002937
Iteration 159/1000 | Loss: 0.00002937
Iteration 160/1000 | Loss: 0.00002937
Iteration 161/1000 | Loss: 0.00002937
Iteration 162/1000 | Loss: 0.00002937
Iteration 163/1000 | Loss: 0.00002937
Iteration 164/1000 | Loss: 0.00002937
Iteration 165/1000 | Loss: 0.00007658
Iteration 166/1000 | Loss: 0.00020298
Iteration 167/1000 | Loss: 0.00014976
Iteration 168/1000 | Loss: 0.00003096
Iteration 169/1000 | Loss: 0.00003009
Iteration 170/1000 | Loss: 0.00013426
Iteration 171/1000 | Loss: 0.00003502
Iteration 172/1000 | Loss: 0.00003188
Iteration 173/1000 | Loss: 0.00003101
Iteration 174/1000 | Loss: 0.00003077
Iteration 175/1000 | Loss: 0.00007150
Iteration 176/1000 | Loss: 0.00018890
Iteration 177/1000 | Loss: 0.00033444
Iteration 178/1000 | Loss: 0.00011932
Iteration 179/1000 | Loss: 0.00013285
Iteration 180/1000 | Loss: 0.00004318
Iteration 181/1000 | Loss: 0.00005832
Iteration 182/1000 | Loss: 0.00011303
Iteration 183/1000 | Loss: 0.00003921
Iteration 184/1000 | Loss: 0.00010493
Iteration 185/1000 | Loss: 0.00003612
Iteration 186/1000 | Loss: 0.00010501
Iteration 187/1000 | Loss: 0.00010588
Iteration 188/1000 | Loss: 0.00011123
Iteration 189/1000 | Loss: 0.00007583
Iteration 190/1000 | Loss: 0.00028101
Iteration 191/1000 | Loss: 0.00008560
Iteration 192/1000 | Loss: 0.00010068
Iteration 193/1000 | Loss: 0.00010497
Iteration 194/1000 | Loss: 0.00004144
Iteration 195/1000 | Loss: 0.00003758
Iteration 196/1000 | Loss: 0.00012392
Iteration 197/1000 | Loss: 0.00024286
Iteration 198/1000 | Loss: 0.00009127
Iteration 199/1000 | Loss: 0.00013757
Iteration 200/1000 | Loss: 0.00015893
Iteration 201/1000 | Loss: 0.00027309
Iteration 202/1000 | Loss: 0.00013189
Iteration 203/1000 | Loss: 0.00040893
Iteration 204/1000 | Loss: 0.00024979
Iteration 205/1000 | Loss: 0.00051760
Iteration 206/1000 | Loss: 0.00045411
Iteration 207/1000 | Loss: 0.00005946
Iteration 208/1000 | Loss: 0.00009433
Iteration 209/1000 | Loss: 0.00022102
Iteration 210/1000 | Loss: 0.00009868
Iteration 211/1000 | Loss: 0.00009798
Iteration 212/1000 | Loss: 0.00015045
Iteration 213/1000 | Loss: 0.00015255
Iteration 214/1000 | Loss: 0.00009842
Iteration 215/1000 | Loss: 0.00016336
Iteration 216/1000 | Loss: 0.00016677
Iteration 217/1000 | Loss: 0.00014258
Iteration 218/1000 | Loss: 0.00020472
Iteration 219/1000 | Loss: 0.00010779
Iteration 220/1000 | Loss: 0.00010444
Iteration 221/1000 | Loss: 0.00003268
Iteration 222/1000 | Loss: 0.00016675
Iteration 223/1000 | Loss: 0.00006311
Iteration 224/1000 | Loss: 0.00006927
Iteration 225/1000 | Loss: 0.00003316
Iteration 226/1000 | Loss: 0.00034865
Iteration 227/1000 | Loss: 0.00021918
Iteration 228/1000 | Loss: 0.00003755
Iteration 229/1000 | Loss: 0.00008078
Iteration 230/1000 | Loss: 0.00003314
Iteration 231/1000 | Loss: 0.00038469
Iteration 232/1000 | Loss: 0.00020299
Iteration 233/1000 | Loss: 0.00016410
Iteration 234/1000 | Loss: 0.00008620
Iteration 235/1000 | Loss: 0.00008860
Iteration 236/1000 | Loss: 0.00012517
Iteration 237/1000 | Loss: 0.00028754
Iteration 238/1000 | Loss: 0.00020533
Iteration 239/1000 | Loss: 0.00026746
Iteration 240/1000 | Loss: 0.00007838
Iteration 241/1000 | Loss: 0.00006004
Iteration 242/1000 | Loss: 0.00003626
Iteration 243/1000 | Loss: 0.00014662
Iteration 244/1000 | Loss: 0.00035815
Iteration 245/1000 | Loss: 0.00019889
Iteration 246/1000 | Loss: 0.00032481
Iteration 247/1000 | Loss: 0.00038276
Iteration 248/1000 | Loss: 0.00030391
Iteration 249/1000 | Loss: 0.00029703
Iteration 250/1000 | Loss: 0.00025151
Iteration 251/1000 | Loss: 0.00009998
Iteration 252/1000 | Loss: 0.00014765
Iteration 253/1000 | Loss: 0.00016633
Iteration 254/1000 | Loss: 0.00014525
Iteration 255/1000 | Loss: 0.00008208
Iteration 256/1000 | Loss: 0.00017813
Iteration 257/1000 | Loss: 0.00015532
Iteration 258/1000 | Loss: 0.00013817
Iteration 259/1000 | Loss: 0.00014698
Iteration 260/1000 | Loss: 0.00018827
Iteration 261/1000 | Loss: 0.00023175
Iteration 262/1000 | Loss: 0.00011914
Iteration 263/1000 | Loss: 0.00016001
Iteration 264/1000 | Loss: 0.00008600
Iteration 265/1000 | Loss: 0.00015131
Iteration 266/1000 | Loss: 0.00019420
Iteration 267/1000 | Loss: 0.00008112
Iteration 268/1000 | Loss: 0.00011786
Iteration 269/1000 | Loss: 0.00007720
Iteration 270/1000 | Loss: 0.00012369
Iteration 271/1000 | Loss: 0.00007633
Iteration 272/1000 | Loss: 0.00008842
Iteration 273/1000 | Loss: 0.00007612
Iteration 274/1000 | Loss: 0.00010750
Iteration 275/1000 | Loss: 0.00010962
Iteration 276/1000 | Loss: 0.00010975
Iteration 277/1000 | Loss: 0.00011842
Iteration 278/1000 | Loss: 0.00010965
Iteration 279/1000 | Loss: 0.00009779
Iteration 280/1000 | Loss: 0.00008448
Iteration 281/1000 | Loss: 0.00018214
Iteration 282/1000 | Loss: 0.00013078
Iteration 283/1000 | Loss: 0.00006043
Iteration 284/1000 | Loss: 0.00014449
Iteration 285/1000 | Loss: 0.00004004
Iteration 286/1000 | Loss: 0.00003206
Iteration 287/1000 | Loss: 0.00003007
Iteration 288/1000 | Loss: 0.00015132
Iteration 289/1000 | Loss: 0.00003405
Iteration 290/1000 | Loss: 0.00003202
Iteration 291/1000 | Loss: 0.00002912
Iteration 292/1000 | Loss: 0.00007412
Iteration 293/1000 | Loss: 0.00003081
Iteration 294/1000 | Loss: 0.00002961
Iteration 295/1000 | Loss: 0.00002850
Iteration 296/1000 | Loss: 0.00002839
Iteration 297/1000 | Loss: 0.00002838
Iteration 298/1000 | Loss: 0.00002831
Iteration 299/1000 | Loss: 0.00002830
Iteration 300/1000 | Loss: 0.00002829
Iteration 301/1000 | Loss: 0.00002827
Iteration 302/1000 | Loss: 0.00002826
Iteration 303/1000 | Loss: 0.00002826
Iteration 304/1000 | Loss: 0.00002818
Iteration 305/1000 | Loss: 0.00002808
Iteration 306/1000 | Loss: 0.00002808
Iteration 307/1000 | Loss: 0.00002808
Iteration 308/1000 | Loss: 0.00002808
Iteration 309/1000 | Loss: 0.00002808
Iteration 310/1000 | Loss: 0.00002808
Iteration 311/1000 | Loss: 0.00002808
Iteration 312/1000 | Loss: 0.00002808
Iteration 313/1000 | Loss: 0.00002808
Iteration 314/1000 | Loss: 0.00002807
Iteration 315/1000 | Loss: 0.00002807
Iteration 316/1000 | Loss: 0.00002807
Iteration 317/1000 | Loss: 0.00002807
Iteration 318/1000 | Loss: 0.00002807
Iteration 319/1000 | Loss: 0.00002807
Iteration 320/1000 | Loss: 0.00002806
Iteration 321/1000 | Loss: 0.00002806
Iteration 322/1000 | Loss: 0.00002806
Iteration 323/1000 | Loss: 0.00002806
Iteration 324/1000 | Loss: 0.00002806
Iteration 325/1000 | Loss: 0.00002806
Iteration 326/1000 | Loss: 0.00002806
Iteration 327/1000 | Loss: 0.00002806
Iteration 328/1000 | Loss: 0.00002806
Iteration 329/1000 | Loss: 0.00002806
Iteration 330/1000 | Loss: 0.00002806
Iteration 331/1000 | Loss: 0.00002805
Iteration 332/1000 | Loss: 0.00002805
Iteration 333/1000 | Loss: 0.00002805
Iteration 334/1000 | Loss: 0.00002805
Iteration 335/1000 | Loss: 0.00002805
Iteration 336/1000 | Loss: 0.00002805
Iteration 337/1000 | Loss: 0.00002805
Iteration 338/1000 | Loss: 0.00002805
Iteration 339/1000 | Loss: 0.00002805
Iteration 340/1000 | Loss: 0.00002804
Iteration 341/1000 | Loss: 0.00002804
Iteration 342/1000 | Loss: 0.00002804
Iteration 343/1000 | Loss: 0.00002804
Iteration 344/1000 | Loss: 0.00002804
Iteration 345/1000 | Loss: 0.00002804
Iteration 346/1000 | Loss: 0.00002804
Iteration 347/1000 | Loss: 0.00002803
Iteration 348/1000 | Loss: 0.00002803
Iteration 349/1000 | Loss: 0.00002803
Iteration 350/1000 | Loss: 0.00002803
Iteration 351/1000 | Loss: 0.00002803
Iteration 352/1000 | Loss: 0.00002803
Iteration 353/1000 | Loss: 0.00002802
Iteration 354/1000 | Loss: 0.00002802
Iteration 355/1000 | Loss: 0.00002802
Iteration 356/1000 | Loss: 0.00002802
Iteration 357/1000 | Loss: 0.00002802
Iteration 358/1000 | Loss: 0.00002802
Iteration 359/1000 | Loss: 0.00002802
Iteration 360/1000 | Loss: 0.00002802
Iteration 361/1000 | Loss: 0.00002802
Iteration 362/1000 | Loss: 0.00002802
Iteration 363/1000 | Loss: 0.00002802
Iteration 364/1000 | Loss: 0.00002802
Iteration 365/1000 | Loss: 0.00002801
Iteration 366/1000 | Loss: 0.00002801
Iteration 367/1000 | Loss: 0.00002801
Iteration 368/1000 | Loss: 0.00002801
Iteration 369/1000 | Loss: 0.00002801
Iteration 370/1000 | Loss: 0.00002801
Iteration 371/1000 | Loss: 0.00002801
Iteration 372/1000 | Loss: 0.00002801
Iteration 373/1000 | Loss: 0.00002801
Iteration 374/1000 | Loss: 0.00002801
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 374. Stopping optimization.
Last 5 losses: [2.8012680559186265e-05, 2.8012680559186265e-05, 2.8012680559186265e-05, 2.8012680559186265e-05, 2.8012680559186265e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.8012680559186265e-05

Optimization complete. Final v2v error: 4.391519069671631 mm

Highest mean error: 5.432570934295654 mm for frame 152

Lowest mean error: 3.946898937225342 mm for frame 238

Saving results

Total time: 467.54799222946167
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ben_posed_008/1068/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_008/1068.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_008/1068
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00826274
Iteration 2/25 | Loss: 0.00079779
Iteration 3/25 | Loss: 0.00059991
Iteration 4/25 | Loss: 0.00057370
Iteration 5/25 | Loss: 0.00056721
Iteration 6/25 | Loss: 0.00056468
Iteration 7/25 | Loss: 0.00056391
Iteration 8/25 | Loss: 0.00056391
Iteration 9/25 | Loss: 0.00056391
Iteration 10/25 | Loss: 0.00056391
Iteration 11/25 | Loss: 0.00056391
Iteration 12/25 | Loss: 0.00056391
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.00056390993995592, 0.00056390993995592, 0.00056390993995592, 0.00056390993995592, 0.00056390993995592]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00056390993995592

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45272171
Iteration 2/25 | Loss: 0.00017338
Iteration 3/25 | Loss: 0.00017338
Iteration 4/25 | Loss: 0.00017338
Iteration 5/25 | Loss: 0.00017338
Iteration 6/25 | Loss: 0.00017338
Iteration 7/25 | Loss: 0.00017338
Iteration 8/25 | Loss: 0.00017338
Iteration 9/25 | Loss: 0.00017338
Iteration 10/25 | Loss: 0.00017338
Iteration 11/25 | Loss: 0.00017338
Iteration 12/25 | Loss: 0.00017338
Iteration 13/25 | Loss: 0.00017338
Iteration 14/25 | Loss: 0.00017338
Iteration 15/25 | Loss: 0.00017338
Iteration 16/25 | Loss: 0.00017338
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.00017338180623482913, 0.00017338180623482913, 0.00017338180623482913, 0.00017338180623482913, 0.00017338180623482913]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00017338180623482913

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00017338
Iteration 2/1000 | Loss: 0.00001834
Iteration 3/1000 | Loss: 0.00001313
Iteration 4/1000 | Loss: 0.00001245
Iteration 5/1000 | Loss: 0.00001189
Iteration 6/1000 | Loss: 0.00001139
Iteration 7/1000 | Loss: 0.00001107
Iteration 8/1000 | Loss: 0.00001094
Iteration 9/1000 | Loss: 0.00001085
Iteration 10/1000 | Loss: 0.00001083
Iteration 11/1000 | Loss: 0.00001076
Iteration 12/1000 | Loss: 0.00001075
Iteration 13/1000 | Loss: 0.00001060
Iteration 14/1000 | Loss: 0.00001056
Iteration 15/1000 | Loss: 0.00001055
Iteration 16/1000 | Loss: 0.00001055
Iteration 17/1000 | Loss: 0.00001054
Iteration 18/1000 | Loss: 0.00001053
Iteration 19/1000 | Loss: 0.00001053
Iteration 20/1000 | Loss: 0.00001049
Iteration 21/1000 | Loss: 0.00001047
Iteration 22/1000 | Loss: 0.00001044
Iteration 23/1000 | Loss: 0.00001044
Iteration 24/1000 | Loss: 0.00001040
Iteration 25/1000 | Loss: 0.00001040
Iteration 26/1000 | Loss: 0.00001040
Iteration 27/1000 | Loss: 0.00001040
Iteration 28/1000 | Loss: 0.00001040
Iteration 29/1000 | Loss: 0.00001040
Iteration 30/1000 | Loss: 0.00001039
Iteration 31/1000 | Loss: 0.00001039
Iteration 32/1000 | Loss: 0.00001039
Iteration 33/1000 | Loss: 0.00001039
Iteration 34/1000 | Loss: 0.00001039
Iteration 35/1000 | Loss: 0.00001039
Iteration 36/1000 | Loss: 0.00001039
Iteration 37/1000 | Loss: 0.00001039
Iteration 38/1000 | Loss: 0.00001039
Iteration 39/1000 | Loss: 0.00001038
Iteration 40/1000 | Loss: 0.00001038
Iteration 41/1000 | Loss: 0.00001038
Iteration 42/1000 | Loss: 0.00001037
Iteration 43/1000 | Loss: 0.00001037
Iteration 44/1000 | Loss: 0.00001037
Iteration 45/1000 | Loss: 0.00001037
Iteration 46/1000 | Loss: 0.00001036
Iteration 47/1000 | Loss: 0.00001036
Iteration 48/1000 | Loss: 0.00001036
Iteration 49/1000 | Loss: 0.00001035
Iteration 50/1000 | Loss: 0.00001034
Iteration 51/1000 | Loss: 0.00001034
Iteration 52/1000 | Loss: 0.00001034
Iteration 53/1000 | Loss: 0.00001033
Iteration 54/1000 | Loss: 0.00001033
Iteration 55/1000 | Loss: 0.00001033
Iteration 56/1000 | Loss: 0.00001033
Iteration 57/1000 | Loss: 0.00001032
Iteration 58/1000 | Loss: 0.00001032
Iteration 59/1000 | Loss: 0.00001032
Iteration 60/1000 | Loss: 0.00001031
Iteration 61/1000 | Loss: 0.00001031
Iteration 62/1000 | Loss: 0.00001031
Iteration 63/1000 | Loss: 0.00001030
Iteration 64/1000 | Loss: 0.00001030
Iteration 65/1000 | Loss: 0.00001030
Iteration 66/1000 | Loss: 0.00001029
Iteration 67/1000 | Loss: 0.00001029
Iteration 68/1000 | Loss: 0.00001029
Iteration 69/1000 | Loss: 0.00001029
Iteration 70/1000 | Loss: 0.00001029
Iteration 71/1000 | Loss: 0.00001028
Iteration 72/1000 | Loss: 0.00001028
Iteration 73/1000 | Loss: 0.00001028
Iteration 74/1000 | Loss: 0.00001028
Iteration 75/1000 | Loss: 0.00001027
Iteration 76/1000 | Loss: 0.00001027
Iteration 77/1000 | Loss: 0.00001027
Iteration 78/1000 | Loss: 0.00001027
Iteration 79/1000 | Loss: 0.00001027
Iteration 80/1000 | Loss: 0.00001027
Iteration 81/1000 | Loss: 0.00001027
Iteration 82/1000 | Loss: 0.00001027
Iteration 83/1000 | Loss: 0.00001027
Iteration 84/1000 | Loss: 0.00001027
Iteration 85/1000 | Loss: 0.00001026
Iteration 86/1000 | Loss: 0.00001026
Iteration 87/1000 | Loss: 0.00001026
Iteration 88/1000 | Loss: 0.00001026
Iteration 89/1000 | Loss: 0.00001026
Iteration 90/1000 | Loss: 0.00001026
Iteration 91/1000 | Loss: 0.00001026
Iteration 92/1000 | Loss: 0.00001026
Iteration 93/1000 | Loss: 0.00001026
Iteration 94/1000 | Loss: 0.00001026
Iteration 95/1000 | Loss: 0.00001026
Iteration 96/1000 | Loss: 0.00001026
Iteration 97/1000 | Loss: 0.00001026
Iteration 98/1000 | Loss: 0.00001026
Iteration 99/1000 | Loss: 0.00001026
Iteration 100/1000 | Loss: 0.00001025
Iteration 101/1000 | Loss: 0.00001025
Iteration 102/1000 | Loss: 0.00001025
Iteration 103/1000 | Loss: 0.00001025
Iteration 104/1000 | Loss: 0.00001025
Iteration 105/1000 | Loss: 0.00001025
Iteration 106/1000 | Loss: 0.00001025
Iteration 107/1000 | Loss: 0.00001025
Iteration 108/1000 | Loss: 0.00001025
Iteration 109/1000 | Loss: 0.00001025
Iteration 110/1000 | Loss: 0.00001025
Iteration 111/1000 | Loss: 0.00001025
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 111. Stopping optimization.
Last 5 losses: [1.0251874300593045e-05, 1.0251874300593045e-05, 1.0251874300593045e-05, 1.0251874300593045e-05, 1.0251874300593045e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0251874300593045e-05

Optimization complete. Final v2v error: 2.6874372959136963 mm

Highest mean error: 3.0992767810821533 mm for frame 92

Lowest mean error: 2.53916597366333 mm for frame 26

Saving results

Total time: 32.948776721954346
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ben_posed_008/1079/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_008/1079.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_008/1079
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01025567
Iteration 2/25 | Loss: 0.00307490
Iteration 3/25 | Loss: 0.00173125
Iteration 4/25 | Loss: 0.00126338
Iteration 5/25 | Loss: 0.00111051
Iteration 6/25 | Loss: 0.00107003
Iteration 7/25 | Loss: 0.00105604
Iteration 8/25 | Loss: 0.00100636
Iteration 9/25 | Loss: 0.00098799
Iteration 10/25 | Loss: 0.00095042
Iteration 11/25 | Loss: 0.00094252
Iteration 12/25 | Loss: 0.00094117
Iteration 13/25 | Loss: 0.00092356
Iteration 14/25 | Loss: 0.00090762
Iteration 15/25 | Loss: 0.00090400
Iteration 16/25 | Loss: 0.00089423
Iteration 17/25 | Loss: 0.00087782
Iteration 18/25 | Loss: 0.00088000
Iteration 19/25 | Loss: 0.00087573
Iteration 20/25 | Loss: 0.00087193
Iteration 21/25 | Loss: 0.00087086
Iteration 22/25 | Loss: 0.00087060
Iteration 23/25 | Loss: 0.00087427
Iteration 24/25 | Loss: 0.00086961
Iteration 25/25 | Loss: 0.00087494

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45164549
Iteration 2/25 | Loss: 0.00209242
Iteration 3/25 | Loss: 0.00185208
Iteration 4/25 | Loss: 0.00185208
Iteration 5/25 | Loss: 0.00185208
Iteration 6/25 | Loss: 0.00185207
Iteration 7/25 | Loss: 0.00185207
Iteration 8/25 | Loss: 0.00185207
Iteration 9/25 | Loss: 0.00185207
Iteration 10/25 | Loss: 0.00185207
Iteration 11/25 | Loss: 0.00185207
Iteration 12/25 | Loss: 0.00185207
Iteration 13/25 | Loss: 0.00185207
Iteration 14/25 | Loss: 0.00185207
Iteration 15/25 | Loss: 0.00185207
Iteration 16/25 | Loss: 0.00185207
Iteration 17/25 | Loss: 0.00185207
Iteration 18/25 | Loss: 0.00185207
Iteration 19/25 | Loss: 0.00185207
Iteration 20/25 | Loss: 0.00185207
Iteration 21/25 | Loss: 0.00185207
Iteration 22/25 | Loss: 0.00185207
Iteration 23/25 | Loss: 0.00185207
Iteration 24/25 | Loss: 0.00185207
Iteration 25/25 | Loss: 0.00185207

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00185207
Iteration 2/1000 | Loss: 0.00078779
Iteration 3/1000 | Loss: 0.00079274
Iteration 4/1000 | Loss: 0.00054481
Iteration 5/1000 | Loss: 0.00037103
Iteration 6/1000 | Loss: 0.00048721
Iteration 7/1000 | Loss: 0.00054283
Iteration 8/1000 | Loss: 0.00044872
Iteration 9/1000 | Loss: 0.00019839
Iteration 10/1000 | Loss: 0.00022285
Iteration 11/1000 | Loss: 0.00081820
Iteration 12/1000 | Loss: 0.00011373
Iteration 13/1000 | Loss: 0.00012160
Iteration 14/1000 | Loss: 0.00011405
Iteration 15/1000 | Loss: 0.00030250
Iteration 16/1000 | Loss: 0.00038855
Iteration 17/1000 | Loss: 0.00081205
Iteration 18/1000 | Loss: 0.00085257
Iteration 19/1000 | Loss: 0.00262554
Iteration 20/1000 | Loss: 0.00152603
Iteration 21/1000 | Loss: 0.00138102
Iteration 22/1000 | Loss: 0.00022909
Iteration 23/1000 | Loss: 0.00010312
Iteration 24/1000 | Loss: 0.00010035
Iteration 25/1000 | Loss: 0.00008749
Iteration 26/1000 | Loss: 0.00018330
Iteration 27/1000 | Loss: 0.00030732
Iteration 28/1000 | Loss: 0.00015215
Iteration 29/1000 | Loss: 0.00062040
Iteration 30/1000 | Loss: 0.00008489
Iteration 31/1000 | Loss: 0.00008125
Iteration 32/1000 | Loss: 0.00023046
Iteration 33/1000 | Loss: 0.00020672
Iteration 34/1000 | Loss: 0.00057401
Iteration 35/1000 | Loss: 0.00007386
Iteration 36/1000 | Loss: 0.00007073
Iteration 37/1000 | Loss: 0.00006156
Iteration 38/1000 | Loss: 0.00006268
Iteration 39/1000 | Loss: 0.00005586
Iteration 40/1000 | Loss: 0.00006395
Iteration 41/1000 | Loss: 0.00004812
Iteration 42/1000 | Loss: 0.00004756
Iteration 43/1000 | Loss: 0.00006472
Iteration 44/1000 | Loss: 0.00026723
Iteration 45/1000 | Loss: 0.00032217
Iteration 46/1000 | Loss: 0.00007976
Iteration 47/1000 | Loss: 0.00068245
Iteration 48/1000 | Loss: 0.00018210
Iteration 49/1000 | Loss: 0.00018323
Iteration 50/1000 | Loss: 0.00013742
Iteration 51/1000 | Loss: 0.00003907
Iteration 52/1000 | Loss: 0.00010732
Iteration 53/1000 | Loss: 0.00007209
Iteration 54/1000 | Loss: 0.00003181
Iteration 55/1000 | Loss: 0.00006886
Iteration 56/1000 | Loss: 0.00003893
Iteration 57/1000 | Loss: 0.00004806
Iteration 58/1000 | Loss: 0.00004083
Iteration 59/1000 | Loss: 0.00002970
Iteration 60/1000 | Loss: 0.00002826
Iteration 61/1000 | Loss: 0.00004578
Iteration 62/1000 | Loss: 0.00005991
Iteration 63/1000 | Loss: 0.00012074
Iteration 64/1000 | Loss: 0.00003135
Iteration 65/1000 | Loss: 0.00002793
Iteration 66/1000 | Loss: 0.00002793
Iteration 67/1000 | Loss: 0.00002793
Iteration 68/1000 | Loss: 0.00002793
Iteration 69/1000 | Loss: 0.00002793
Iteration 70/1000 | Loss: 0.00002793
Iteration 71/1000 | Loss: 0.00002790
Iteration 72/1000 | Loss: 0.00005312
Iteration 73/1000 | Loss: 0.00003605
Iteration 74/1000 | Loss: 0.00002762
Iteration 75/1000 | Loss: 0.00002752
Iteration 76/1000 | Loss: 0.00002749
Iteration 77/1000 | Loss: 0.00002859
Iteration 78/1000 | Loss: 0.00004370
Iteration 79/1000 | Loss: 0.00031023
Iteration 80/1000 | Loss: 0.00002902
Iteration 81/1000 | Loss: 0.00004338
Iteration 82/1000 | Loss: 0.00004048
Iteration 83/1000 | Loss: 0.00002785
Iteration 84/1000 | Loss: 0.00002724
Iteration 85/1000 | Loss: 0.00002739
Iteration 86/1000 | Loss: 0.00002724
Iteration 87/1000 | Loss: 0.00002722
Iteration 88/1000 | Loss: 0.00002721
Iteration 89/1000 | Loss: 0.00002721
Iteration 90/1000 | Loss: 0.00002721
Iteration 91/1000 | Loss: 0.00002721
Iteration 92/1000 | Loss: 0.00002721
Iteration 93/1000 | Loss: 0.00002721
Iteration 94/1000 | Loss: 0.00002723
Iteration 95/1000 | Loss: 0.00002723
Iteration 96/1000 | Loss: 0.00002723
Iteration 97/1000 | Loss: 0.00002723
Iteration 98/1000 | Loss: 0.00002723
Iteration 99/1000 | Loss: 0.00002723
Iteration 100/1000 | Loss: 0.00002723
Iteration 101/1000 | Loss: 0.00002723
Iteration 102/1000 | Loss: 0.00002723
Iteration 103/1000 | Loss: 0.00002723
Iteration 104/1000 | Loss: 0.00002723
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 104. Stopping optimization.
Last 5 losses: [2.7226313250139356e-05, 2.7226313250139356e-05, 2.7226313250139356e-05, 2.7226313250139356e-05, 2.7226313250139356e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.7226313250139356e-05

Optimization complete. Final v2v error: 3.384614944458008 mm

Highest mean error: 14.933490753173828 mm for frame 59

Lowest mean error: 2.551265239715576 mm for frame 134

Saving results

Total time: 175.4000072479248
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ben_posed_008/1020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_008/1020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_008/1020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00602857
Iteration 2/25 | Loss: 0.00108474
Iteration 3/25 | Loss: 0.00071870
Iteration 4/25 | Loss: 0.00063728
Iteration 5/25 | Loss: 0.00061361
Iteration 6/25 | Loss: 0.00060567
Iteration 7/25 | Loss: 0.00060374
Iteration 8/25 | Loss: 0.00060775
Iteration 9/25 | Loss: 0.00060681
Iteration 10/25 | Loss: 0.00060138
Iteration 11/25 | Loss: 0.00059836
Iteration 12/25 | Loss: 0.00059658
Iteration 13/25 | Loss: 0.00059572
Iteration 14/25 | Loss: 0.00059537
Iteration 15/25 | Loss: 0.00059521
Iteration 16/25 | Loss: 0.00059510
Iteration 17/25 | Loss: 0.00059508
Iteration 18/25 | Loss: 0.00059507
Iteration 19/25 | Loss: 0.00059507
Iteration 20/25 | Loss: 0.00059506
Iteration 21/25 | Loss: 0.00059506
Iteration 22/25 | Loss: 0.00059506
Iteration 23/25 | Loss: 0.00059506
Iteration 24/25 | Loss: 0.00059506
Iteration 25/25 | Loss: 0.00059506

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 7.69758749
Iteration 2/25 | Loss: 0.00020430
Iteration 3/25 | Loss: 0.00020426
Iteration 4/25 | Loss: 0.00020426
Iteration 5/25 | Loss: 0.00020426
Iteration 6/25 | Loss: 0.00020426
Iteration 7/25 | Loss: 0.00020426
Iteration 8/25 | Loss: 0.00020426
Iteration 9/25 | Loss: 0.00020426
Iteration 10/25 | Loss: 0.00020426
Iteration 11/25 | Loss: 0.00020426
Iteration 12/25 | Loss: 0.00020425
Iteration 13/25 | Loss: 0.00020425
Iteration 14/25 | Loss: 0.00020425
Iteration 15/25 | Loss: 0.00020425
Iteration 16/25 | Loss: 0.00020425
Iteration 17/25 | Loss: 0.00020425
Iteration 18/25 | Loss: 0.00020425
Iteration 19/25 | Loss: 0.00020425
Iteration 20/25 | Loss: 0.00020425
Iteration 21/25 | Loss: 0.00020425
Iteration 22/25 | Loss: 0.00020425
Iteration 23/25 | Loss: 0.00020425
Iteration 24/25 | Loss: 0.00020425
Iteration 25/25 | Loss: 0.00020425

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00020425
Iteration 2/1000 | Loss: 0.00002376
Iteration 3/1000 | Loss: 0.00001618
Iteration 4/1000 | Loss: 0.00001507
Iteration 5/1000 | Loss: 0.00001434
Iteration 6/1000 | Loss: 0.00001397
Iteration 7/1000 | Loss: 0.00001366
Iteration 8/1000 | Loss: 0.00001341
Iteration 9/1000 | Loss: 0.00001331
Iteration 10/1000 | Loss: 0.00001331
Iteration 11/1000 | Loss: 0.00001327
Iteration 12/1000 | Loss: 0.00001326
Iteration 13/1000 | Loss: 0.00001324
Iteration 14/1000 | Loss: 0.00001323
Iteration 15/1000 | Loss: 0.00001323
Iteration 16/1000 | Loss: 0.00001316
Iteration 17/1000 | Loss: 0.00001310
Iteration 18/1000 | Loss: 0.00001307
Iteration 19/1000 | Loss: 0.00001307
Iteration 20/1000 | Loss: 0.00001307
Iteration 21/1000 | Loss: 0.00001305
Iteration 22/1000 | Loss: 0.00001305
Iteration 23/1000 | Loss: 0.00001304
Iteration 24/1000 | Loss: 0.00001303
Iteration 25/1000 | Loss: 0.00001303
Iteration 26/1000 | Loss: 0.00001303
Iteration 27/1000 | Loss: 0.00001303
Iteration 28/1000 | Loss: 0.00001303
Iteration 29/1000 | Loss: 0.00001302
Iteration 30/1000 | Loss: 0.00001301
Iteration 31/1000 | Loss: 0.00001301
Iteration 32/1000 | Loss: 0.00001300
Iteration 33/1000 | Loss: 0.00001300
Iteration 34/1000 | Loss: 0.00001300
Iteration 35/1000 | Loss: 0.00001300
Iteration 36/1000 | Loss: 0.00001300
Iteration 37/1000 | Loss: 0.00001299
Iteration 38/1000 | Loss: 0.00001299
Iteration 39/1000 | Loss: 0.00001299
Iteration 40/1000 | Loss: 0.00001299
Iteration 41/1000 | Loss: 0.00001299
Iteration 42/1000 | Loss: 0.00001298
Iteration 43/1000 | Loss: 0.00001298
Iteration 44/1000 | Loss: 0.00001298
Iteration 45/1000 | Loss: 0.00001298
Iteration 46/1000 | Loss: 0.00001298
Iteration 47/1000 | Loss: 0.00001297
Iteration 48/1000 | Loss: 0.00001297
Iteration 49/1000 | Loss: 0.00001297
Iteration 50/1000 | Loss: 0.00001297
Iteration 51/1000 | Loss: 0.00001296
Iteration 52/1000 | Loss: 0.00001296
Iteration 53/1000 | Loss: 0.00001296
Iteration 54/1000 | Loss: 0.00001295
Iteration 55/1000 | Loss: 0.00001295
Iteration 56/1000 | Loss: 0.00001295
Iteration 57/1000 | Loss: 0.00001294
Iteration 58/1000 | Loss: 0.00001294
Iteration 59/1000 | Loss: 0.00001294
Iteration 60/1000 | Loss: 0.00001294
Iteration 61/1000 | Loss: 0.00001294
Iteration 62/1000 | Loss: 0.00001294
Iteration 63/1000 | Loss: 0.00001294
Iteration 64/1000 | Loss: 0.00001293
Iteration 65/1000 | Loss: 0.00001293
Iteration 66/1000 | Loss: 0.00001293
Iteration 67/1000 | Loss: 0.00001293
Iteration 68/1000 | Loss: 0.00001293
Iteration 69/1000 | Loss: 0.00001293
Iteration 70/1000 | Loss: 0.00001292
Iteration 71/1000 | Loss: 0.00001292
Iteration 72/1000 | Loss: 0.00001292
Iteration 73/1000 | Loss: 0.00001291
Iteration 74/1000 | Loss: 0.00001291
Iteration 75/1000 | Loss: 0.00001291
Iteration 76/1000 | Loss: 0.00001291
Iteration 77/1000 | Loss: 0.00001291
Iteration 78/1000 | Loss: 0.00001291
Iteration 79/1000 | Loss: 0.00001291
Iteration 80/1000 | Loss: 0.00001291
Iteration 81/1000 | Loss: 0.00001291
Iteration 82/1000 | Loss: 0.00001291
Iteration 83/1000 | Loss: 0.00001291
Iteration 84/1000 | Loss: 0.00001291
Iteration 85/1000 | Loss: 0.00001290
Iteration 86/1000 | Loss: 0.00001290
Iteration 87/1000 | Loss: 0.00001290
Iteration 88/1000 | Loss: 0.00001290
Iteration 89/1000 | Loss: 0.00001290
Iteration 90/1000 | Loss: 0.00001290
Iteration 91/1000 | Loss: 0.00001290
Iteration 92/1000 | Loss: 0.00001290
Iteration 93/1000 | Loss: 0.00001289
Iteration 94/1000 | Loss: 0.00001289
Iteration 95/1000 | Loss: 0.00001289
Iteration 96/1000 | Loss: 0.00001288
Iteration 97/1000 | Loss: 0.00001288
Iteration 98/1000 | Loss: 0.00001288
Iteration 99/1000 | Loss: 0.00001288
Iteration 100/1000 | Loss: 0.00001288
Iteration 101/1000 | Loss: 0.00001287
Iteration 102/1000 | Loss: 0.00001287
Iteration 103/1000 | Loss: 0.00001287
Iteration 104/1000 | Loss: 0.00001287
Iteration 105/1000 | Loss: 0.00001287
Iteration 106/1000 | Loss: 0.00001287
Iteration 107/1000 | Loss: 0.00001286
Iteration 108/1000 | Loss: 0.00001286
Iteration 109/1000 | Loss: 0.00001286
Iteration 110/1000 | Loss: 0.00001286
Iteration 111/1000 | Loss: 0.00001286
Iteration 112/1000 | Loss: 0.00001286
Iteration 113/1000 | Loss: 0.00001286
Iteration 114/1000 | Loss: 0.00001286
Iteration 115/1000 | Loss: 0.00001286
Iteration 116/1000 | Loss: 0.00001285
Iteration 117/1000 | Loss: 0.00001285
Iteration 118/1000 | Loss: 0.00001285
Iteration 119/1000 | Loss: 0.00001285
Iteration 120/1000 | Loss: 0.00001285
Iteration 121/1000 | Loss: 0.00001285
Iteration 122/1000 | Loss: 0.00001285
Iteration 123/1000 | Loss: 0.00001284
Iteration 124/1000 | Loss: 0.00001284
Iteration 125/1000 | Loss: 0.00001284
Iteration 126/1000 | Loss: 0.00001284
Iteration 127/1000 | Loss: 0.00001284
Iteration 128/1000 | Loss: 0.00001284
Iteration 129/1000 | Loss: 0.00001283
Iteration 130/1000 | Loss: 0.00001283
Iteration 131/1000 | Loss: 0.00001283
Iteration 132/1000 | Loss: 0.00001283
Iteration 133/1000 | Loss: 0.00001283
Iteration 134/1000 | Loss: 0.00001283
Iteration 135/1000 | Loss: 0.00001283
Iteration 136/1000 | Loss: 0.00001283
Iteration 137/1000 | Loss: 0.00001283
Iteration 138/1000 | Loss: 0.00001283
Iteration 139/1000 | Loss: 0.00001282
Iteration 140/1000 | Loss: 0.00001282
Iteration 141/1000 | Loss: 0.00001281
Iteration 142/1000 | Loss: 0.00001281
Iteration 143/1000 | Loss: 0.00001281
Iteration 144/1000 | Loss: 0.00001280
Iteration 145/1000 | Loss: 0.00001280
Iteration 146/1000 | Loss: 0.00001280
Iteration 147/1000 | Loss: 0.00001280
Iteration 148/1000 | Loss: 0.00001280
Iteration 149/1000 | Loss: 0.00001279
Iteration 150/1000 | Loss: 0.00001279
Iteration 151/1000 | Loss: 0.00001279
Iteration 152/1000 | Loss: 0.00001279
Iteration 153/1000 | Loss: 0.00001279
Iteration 154/1000 | Loss: 0.00001279
Iteration 155/1000 | Loss: 0.00001279
Iteration 156/1000 | Loss: 0.00001279
Iteration 157/1000 | Loss: 0.00001279
Iteration 158/1000 | Loss: 0.00001279
Iteration 159/1000 | Loss: 0.00001279
Iteration 160/1000 | Loss: 0.00001279
Iteration 161/1000 | Loss: 0.00001279
Iteration 162/1000 | Loss: 0.00001278
Iteration 163/1000 | Loss: 0.00001278
Iteration 164/1000 | Loss: 0.00001278
Iteration 165/1000 | Loss: 0.00001278
Iteration 166/1000 | Loss: 0.00001278
Iteration 167/1000 | Loss: 0.00001278
Iteration 168/1000 | Loss: 0.00001278
Iteration 169/1000 | Loss: 0.00001278
Iteration 170/1000 | Loss: 0.00001278
Iteration 171/1000 | Loss: 0.00001278
Iteration 172/1000 | Loss: 0.00001277
Iteration 173/1000 | Loss: 0.00001277
Iteration 174/1000 | Loss: 0.00001277
Iteration 175/1000 | Loss: 0.00001277
Iteration 176/1000 | Loss: 0.00001277
Iteration 177/1000 | Loss: 0.00001277
Iteration 178/1000 | Loss: 0.00001277
Iteration 179/1000 | Loss: 0.00001277
Iteration 180/1000 | Loss: 0.00001277
Iteration 181/1000 | Loss: 0.00001277
Iteration 182/1000 | Loss: 0.00001277
Iteration 183/1000 | Loss: 0.00001277
Iteration 184/1000 | Loss: 0.00001277
Iteration 185/1000 | Loss: 0.00001277
Iteration 186/1000 | Loss: 0.00001277
Iteration 187/1000 | Loss: 0.00001277
Iteration 188/1000 | Loss: 0.00001277
Iteration 189/1000 | Loss: 0.00001277
Iteration 190/1000 | Loss: 0.00001277
Iteration 191/1000 | Loss: 0.00001277
Iteration 192/1000 | Loss: 0.00001277
Iteration 193/1000 | Loss: 0.00001277
Iteration 194/1000 | Loss: 0.00001277
Iteration 195/1000 | Loss: 0.00001277
Iteration 196/1000 | Loss: 0.00001277
Iteration 197/1000 | Loss: 0.00001277
Iteration 198/1000 | Loss: 0.00001277
Iteration 199/1000 | Loss: 0.00001277
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 199. Stopping optimization.
Last 5 losses: [1.2773825801559724e-05, 1.2773825801559724e-05, 1.2773825801559724e-05, 1.2773825801559724e-05, 1.2773825801559724e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2773825801559724e-05

Optimization complete. Final v2v error: 3.046847343444824 mm

Highest mean error: 3.533405065536499 mm for frame 173

Lowest mean error: 2.650275945663452 mm for frame 13

Saving results

Total time: 63.386584758758545
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ben_posed_008/1090/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_008/1090.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_008/1090
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00839442
Iteration 2/25 | Loss: 0.00076552
Iteration 3/25 | Loss: 0.00058845
Iteration 4/25 | Loss: 0.00055566
Iteration 5/25 | Loss: 0.00054869
Iteration 6/25 | Loss: 0.00054670
Iteration 7/25 | Loss: 0.00054619
Iteration 8/25 | Loss: 0.00054619
Iteration 9/25 | Loss: 0.00054619
Iteration 10/25 | Loss: 0.00054619
Iteration 11/25 | Loss: 0.00054619
Iteration 12/25 | Loss: 0.00054619
Iteration 13/25 | Loss: 0.00054619
Iteration 14/25 | Loss: 0.00054619
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0005461901891976595, 0.0005461901891976595, 0.0005461901891976595, 0.0005461901891976595, 0.0005461901891976595]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005461901891976595

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44802368
Iteration 2/25 | Loss: 0.00013336
Iteration 3/25 | Loss: 0.00013335
Iteration 4/25 | Loss: 0.00013335
Iteration 5/25 | Loss: 0.00013335
Iteration 6/25 | Loss: 0.00013335
Iteration 7/25 | Loss: 0.00013335
Iteration 8/25 | Loss: 0.00013335
Iteration 9/25 | Loss: 0.00013335
Iteration 10/25 | Loss: 0.00013335
Iteration 11/25 | Loss: 0.00013335
Iteration 12/25 | Loss: 0.00013335
Iteration 13/25 | Loss: 0.00013335
Iteration 14/25 | Loss: 0.00013335
Iteration 15/25 | Loss: 0.00013335
Iteration 16/25 | Loss: 0.00013335
Iteration 17/25 | Loss: 0.00013335
Iteration 18/25 | Loss: 0.00013335
Iteration 19/25 | Loss: 0.00013335
Iteration 20/25 | Loss: 0.00013335
Iteration 21/25 | Loss: 0.00013335
Iteration 22/25 | Loss: 0.00013335
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.00013335123367141932, 0.00013335123367141932, 0.00013335123367141932, 0.00013335123367141932, 0.00013335123367141932]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00013335123367141932

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00013335
Iteration 2/1000 | Loss: 0.00001848
Iteration 3/1000 | Loss: 0.00001268
Iteration 4/1000 | Loss: 0.00001184
Iteration 5/1000 | Loss: 0.00001133
Iteration 6/1000 | Loss: 0.00001081
Iteration 7/1000 | Loss: 0.00001055
Iteration 8/1000 | Loss: 0.00001037
Iteration 9/1000 | Loss: 0.00001036
Iteration 10/1000 | Loss: 0.00001036
Iteration 11/1000 | Loss: 0.00001035
Iteration 12/1000 | Loss: 0.00001033
Iteration 13/1000 | Loss: 0.00001032
Iteration 14/1000 | Loss: 0.00001029
Iteration 15/1000 | Loss: 0.00001029
Iteration 16/1000 | Loss: 0.00001028
Iteration 17/1000 | Loss: 0.00001028
Iteration 18/1000 | Loss: 0.00001027
Iteration 19/1000 | Loss: 0.00001027
Iteration 20/1000 | Loss: 0.00001027
Iteration 21/1000 | Loss: 0.00001026
Iteration 22/1000 | Loss: 0.00001021
Iteration 23/1000 | Loss: 0.00001018
Iteration 24/1000 | Loss: 0.00001015
Iteration 25/1000 | Loss: 0.00001015
Iteration 26/1000 | Loss: 0.00001013
Iteration 27/1000 | Loss: 0.00001013
Iteration 28/1000 | Loss: 0.00001013
Iteration 29/1000 | Loss: 0.00001012
Iteration 30/1000 | Loss: 0.00001011
Iteration 31/1000 | Loss: 0.00001011
Iteration 32/1000 | Loss: 0.00001011
Iteration 33/1000 | Loss: 0.00001010
Iteration 34/1000 | Loss: 0.00001010
Iteration 35/1000 | Loss: 0.00001010
Iteration 36/1000 | Loss: 0.00001010
Iteration 37/1000 | Loss: 0.00001010
Iteration 38/1000 | Loss: 0.00001010
Iteration 39/1000 | Loss: 0.00001009
Iteration 40/1000 | Loss: 0.00001007
Iteration 41/1000 | Loss: 0.00001005
Iteration 42/1000 | Loss: 0.00001004
Iteration 43/1000 | Loss: 0.00001004
Iteration 44/1000 | Loss: 0.00001004
Iteration 45/1000 | Loss: 0.00001004
Iteration 46/1000 | Loss: 0.00001003
Iteration 47/1000 | Loss: 0.00001003
Iteration 48/1000 | Loss: 0.00001002
Iteration 49/1000 | Loss: 0.00001001
Iteration 50/1000 | Loss: 0.00001001
Iteration 51/1000 | Loss: 0.00001000
Iteration 52/1000 | Loss: 0.00001000
Iteration 53/1000 | Loss: 0.00001000
Iteration 54/1000 | Loss: 0.00000999
Iteration 55/1000 | Loss: 0.00000997
Iteration 56/1000 | Loss: 0.00000995
Iteration 57/1000 | Loss: 0.00000995
Iteration 58/1000 | Loss: 0.00000995
Iteration 59/1000 | Loss: 0.00000994
Iteration 60/1000 | Loss: 0.00000994
Iteration 61/1000 | Loss: 0.00000993
Iteration 62/1000 | Loss: 0.00000992
Iteration 63/1000 | Loss: 0.00000992
Iteration 64/1000 | Loss: 0.00000991
Iteration 65/1000 | Loss: 0.00000990
Iteration 66/1000 | Loss: 0.00000990
Iteration 67/1000 | Loss: 0.00000989
Iteration 68/1000 | Loss: 0.00000989
Iteration 69/1000 | Loss: 0.00000988
Iteration 70/1000 | Loss: 0.00000988
Iteration 71/1000 | Loss: 0.00000988
Iteration 72/1000 | Loss: 0.00000988
Iteration 73/1000 | Loss: 0.00000988
Iteration 74/1000 | Loss: 0.00000988
Iteration 75/1000 | Loss: 0.00000988
Iteration 76/1000 | Loss: 0.00000988
Iteration 77/1000 | Loss: 0.00000987
Iteration 78/1000 | Loss: 0.00000987
Iteration 79/1000 | Loss: 0.00000987
Iteration 80/1000 | Loss: 0.00000987
Iteration 81/1000 | Loss: 0.00000986
Iteration 82/1000 | Loss: 0.00000986
Iteration 83/1000 | Loss: 0.00000986
Iteration 84/1000 | Loss: 0.00000985
Iteration 85/1000 | Loss: 0.00000985
Iteration 86/1000 | Loss: 0.00000985
Iteration 87/1000 | Loss: 0.00000984
Iteration 88/1000 | Loss: 0.00000984
Iteration 89/1000 | Loss: 0.00000984
Iteration 90/1000 | Loss: 0.00000984
Iteration 91/1000 | Loss: 0.00000984
Iteration 92/1000 | Loss: 0.00000983
Iteration 93/1000 | Loss: 0.00000983
Iteration 94/1000 | Loss: 0.00000983
Iteration 95/1000 | Loss: 0.00000983
Iteration 96/1000 | Loss: 0.00000983
Iteration 97/1000 | Loss: 0.00000982
Iteration 98/1000 | Loss: 0.00000982
Iteration 99/1000 | Loss: 0.00000982
Iteration 100/1000 | Loss: 0.00000982
Iteration 101/1000 | Loss: 0.00000981
Iteration 102/1000 | Loss: 0.00000981
Iteration 103/1000 | Loss: 0.00000981
Iteration 104/1000 | Loss: 0.00000981
Iteration 105/1000 | Loss: 0.00000981
Iteration 106/1000 | Loss: 0.00000981
Iteration 107/1000 | Loss: 0.00000981
Iteration 108/1000 | Loss: 0.00000981
Iteration 109/1000 | Loss: 0.00000981
Iteration 110/1000 | Loss: 0.00000981
Iteration 111/1000 | Loss: 0.00000980
Iteration 112/1000 | Loss: 0.00000980
Iteration 113/1000 | Loss: 0.00000980
Iteration 114/1000 | Loss: 0.00000980
Iteration 115/1000 | Loss: 0.00000980
Iteration 116/1000 | Loss: 0.00000980
Iteration 117/1000 | Loss: 0.00000979
Iteration 118/1000 | Loss: 0.00000978
Iteration 119/1000 | Loss: 0.00000978
Iteration 120/1000 | Loss: 0.00000978
Iteration 121/1000 | Loss: 0.00000978
Iteration 122/1000 | Loss: 0.00000978
Iteration 123/1000 | Loss: 0.00000978
Iteration 124/1000 | Loss: 0.00000978
Iteration 125/1000 | Loss: 0.00000977
Iteration 126/1000 | Loss: 0.00000977
Iteration 127/1000 | Loss: 0.00000977
Iteration 128/1000 | Loss: 0.00000977
Iteration 129/1000 | Loss: 0.00000977
Iteration 130/1000 | Loss: 0.00000977
Iteration 131/1000 | Loss: 0.00000977
Iteration 132/1000 | Loss: 0.00000977
Iteration 133/1000 | Loss: 0.00000976
Iteration 134/1000 | Loss: 0.00000976
Iteration 135/1000 | Loss: 0.00000976
Iteration 136/1000 | Loss: 0.00000976
Iteration 137/1000 | Loss: 0.00000975
Iteration 138/1000 | Loss: 0.00000975
Iteration 139/1000 | Loss: 0.00000975
Iteration 140/1000 | Loss: 0.00000975
Iteration 141/1000 | Loss: 0.00000975
Iteration 142/1000 | Loss: 0.00000975
Iteration 143/1000 | Loss: 0.00000975
Iteration 144/1000 | Loss: 0.00000975
Iteration 145/1000 | Loss: 0.00000975
Iteration 146/1000 | Loss: 0.00000975
Iteration 147/1000 | Loss: 0.00000974
Iteration 148/1000 | Loss: 0.00000974
Iteration 149/1000 | Loss: 0.00000974
Iteration 150/1000 | Loss: 0.00000973
Iteration 151/1000 | Loss: 0.00000973
Iteration 152/1000 | Loss: 0.00000973
Iteration 153/1000 | Loss: 0.00000973
Iteration 154/1000 | Loss: 0.00000973
Iteration 155/1000 | Loss: 0.00000973
Iteration 156/1000 | Loss: 0.00000973
Iteration 157/1000 | Loss: 0.00000973
Iteration 158/1000 | Loss: 0.00000973
Iteration 159/1000 | Loss: 0.00000972
Iteration 160/1000 | Loss: 0.00000972
Iteration 161/1000 | Loss: 0.00000972
Iteration 162/1000 | Loss: 0.00000972
Iteration 163/1000 | Loss: 0.00000972
Iteration 164/1000 | Loss: 0.00000972
Iteration 165/1000 | Loss: 0.00000972
Iteration 166/1000 | Loss: 0.00000972
Iteration 167/1000 | Loss: 0.00000972
Iteration 168/1000 | Loss: 0.00000972
Iteration 169/1000 | Loss: 0.00000972
Iteration 170/1000 | Loss: 0.00000972
Iteration 171/1000 | Loss: 0.00000972
Iteration 172/1000 | Loss: 0.00000972
Iteration 173/1000 | Loss: 0.00000972
Iteration 174/1000 | Loss: 0.00000972
Iteration 175/1000 | Loss: 0.00000972
Iteration 176/1000 | Loss: 0.00000971
Iteration 177/1000 | Loss: 0.00000971
Iteration 178/1000 | Loss: 0.00000971
Iteration 179/1000 | Loss: 0.00000971
Iteration 180/1000 | Loss: 0.00000971
Iteration 181/1000 | Loss: 0.00000971
Iteration 182/1000 | Loss: 0.00000971
Iteration 183/1000 | Loss: 0.00000971
Iteration 184/1000 | Loss: 0.00000971
Iteration 185/1000 | Loss: 0.00000971
Iteration 186/1000 | Loss: 0.00000971
Iteration 187/1000 | Loss: 0.00000971
Iteration 188/1000 | Loss: 0.00000971
Iteration 189/1000 | Loss: 0.00000971
Iteration 190/1000 | Loss: 0.00000971
Iteration 191/1000 | Loss: 0.00000971
Iteration 192/1000 | Loss: 0.00000971
Iteration 193/1000 | Loss: 0.00000971
Iteration 194/1000 | Loss: 0.00000971
Iteration 195/1000 | Loss: 0.00000971
Iteration 196/1000 | Loss: 0.00000971
Iteration 197/1000 | Loss: 0.00000971
Iteration 198/1000 | Loss: 0.00000971
Iteration 199/1000 | Loss: 0.00000971
Iteration 200/1000 | Loss: 0.00000971
Iteration 201/1000 | Loss: 0.00000971
Iteration 202/1000 | Loss: 0.00000971
Iteration 203/1000 | Loss: 0.00000971
Iteration 204/1000 | Loss: 0.00000971
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 204. Stopping optimization.
Last 5 losses: [9.710222911962774e-06, 9.710222911962774e-06, 9.710222911962774e-06, 9.710222911962774e-06, 9.710222911962774e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.710222911962774e-06

Optimization complete. Final v2v error: 2.6241238117218018 mm

Highest mean error: 3.0133750438690186 mm for frame 47

Lowest mean error: 2.4443628787994385 mm for frame 164

Saving results

Total time: 39.055384397506714
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ben_posed_008/1018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_008/1018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_008/1018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01041253
Iteration 2/25 | Loss: 0.00159034
Iteration 3/25 | Loss: 0.00101449
Iteration 4/25 | Loss: 0.00087649
Iteration 5/25 | Loss: 0.00086099
Iteration 6/25 | Loss: 0.00086012
Iteration 7/25 | Loss: 0.00083208
Iteration 8/25 | Loss: 0.00077662
Iteration 9/25 | Loss: 0.00075160
Iteration 10/25 | Loss: 0.00074273
Iteration 11/25 | Loss: 0.00074772
Iteration 12/25 | Loss: 0.00073362
Iteration 13/25 | Loss: 0.00073112
Iteration 14/25 | Loss: 0.00073263
Iteration 15/25 | Loss: 0.00072777
Iteration 16/25 | Loss: 0.00073702
Iteration 17/25 | Loss: 0.00073153
Iteration 18/25 | Loss: 0.00072468
Iteration 19/25 | Loss: 0.00072028
Iteration 20/25 | Loss: 0.00071848
Iteration 21/25 | Loss: 0.00071737
Iteration 22/25 | Loss: 0.00071664
Iteration 23/25 | Loss: 0.00071581
Iteration 24/25 | Loss: 0.00072065
Iteration 25/25 | Loss: 0.00072638

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.96523607
Iteration 2/25 | Loss: 0.00049539
Iteration 3/25 | Loss: 0.00049538
Iteration 4/25 | Loss: 0.00049538
Iteration 5/25 | Loss: 0.00049538
Iteration 6/25 | Loss: 0.00049538
Iteration 7/25 | Loss: 0.00049538
Iteration 8/25 | Loss: 0.00049538
Iteration 9/25 | Loss: 0.00049538
Iteration 10/25 | Loss: 0.00049538
Iteration 11/25 | Loss: 0.00049538
Iteration 12/25 | Loss: 0.00049538
Iteration 13/25 | Loss: 0.00049538
Iteration 14/25 | Loss: 0.00049538
Iteration 15/25 | Loss: 0.00049538
Iteration 16/25 | Loss: 0.00049538
Iteration 17/25 | Loss: 0.00049538
Iteration 18/25 | Loss: 0.00049538
Iteration 19/25 | Loss: 0.00049538
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0004953824100084603, 0.0004953824100084603, 0.0004953824100084603, 0.0004953824100084603, 0.0004953824100084603]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0004953824100084603

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00049538
Iteration 2/1000 | Loss: 0.00005913
Iteration 3/1000 | Loss: 0.00004438
Iteration 4/1000 | Loss: 0.00003764
Iteration 5/1000 | Loss: 0.00003371
Iteration 6/1000 | Loss: 0.00003136
Iteration 7/1000 | Loss: 0.00002968
Iteration 8/1000 | Loss: 0.00002822
Iteration 9/1000 | Loss: 0.00002747
Iteration 10/1000 | Loss: 0.00002680
Iteration 11/1000 | Loss: 0.00002635
Iteration 12/1000 | Loss: 0.00002598
Iteration 13/1000 | Loss: 0.00002564
Iteration 14/1000 | Loss: 0.00002527
Iteration 15/1000 | Loss: 0.00002501
Iteration 16/1000 | Loss: 0.00002484
Iteration 17/1000 | Loss: 0.00005130
Iteration 18/1000 | Loss: 0.00004477
Iteration 19/1000 | Loss: 0.00002735
Iteration 20/1000 | Loss: 0.00002543
Iteration 21/1000 | Loss: 0.00002480
Iteration 22/1000 | Loss: 0.00005037
Iteration 23/1000 | Loss: 0.00004330
Iteration 24/1000 | Loss: 0.00002630
Iteration 25/1000 | Loss: 0.00002541
Iteration 26/1000 | Loss: 0.00002456
Iteration 27/1000 | Loss: 0.00005132
Iteration 28/1000 | Loss: 0.00004036
Iteration 29/1000 | Loss: 0.00005010
Iteration 30/1000 | Loss: 0.00004073
Iteration 31/1000 | Loss: 0.00002457
Iteration 32/1000 | Loss: 0.00005119
Iteration 33/1000 | Loss: 0.00002916
Iteration 34/1000 | Loss: 0.00002633
Iteration 35/1000 | Loss: 0.00002841
Iteration 36/1000 | Loss: 0.00004817
Iteration 37/1000 | Loss: 0.00002896
Iteration 38/1000 | Loss: 0.00003015
Iteration 39/1000 | Loss: 0.00005060
Iteration 40/1000 | Loss: 0.00002585
Iteration 41/1000 | Loss: 0.00002509
Iteration 42/1000 | Loss: 0.00002466
Iteration 43/1000 | Loss: 0.00002431
Iteration 44/1000 | Loss: 0.00002404
Iteration 45/1000 | Loss: 0.00002387
Iteration 46/1000 | Loss: 0.00002382
Iteration 47/1000 | Loss: 0.00002379
Iteration 48/1000 | Loss: 0.00002378
Iteration 49/1000 | Loss: 0.00002377
Iteration 50/1000 | Loss: 0.00002377
Iteration 51/1000 | Loss: 0.00002377
Iteration 52/1000 | Loss: 0.00002377
Iteration 53/1000 | Loss: 0.00002376
Iteration 54/1000 | Loss: 0.00002376
Iteration 55/1000 | Loss: 0.00002376
Iteration 56/1000 | Loss: 0.00002376
Iteration 57/1000 | Loss: 0.00002375
Iteration 58/1000 | Loss: 0.00002375
Iteration 59/1000 | Loss: 0.00002375
Iteration 60/1000 | Loss: 0.00002375
Iteration 61/1000 | Loss: 0.00002375
Iteration 62/1000 | Loss: 0.00002374
Iteration 63/1000 | Loss: 0.00002374
Iteration 64/1000 | Loss: 0.00002374
Iteration 65/1000 | Loss: 0.00002374
Iteration 66/1000 | Loss: 0.00002374
Iteration 67/1000 | Loss: 0.00002373
Iteration 68/1000 | Loss: 0.00002373
Iteration 69/1000 | Loss: 0.00002373
Iteration 70/1000 | Loss: 0.00002373
Iteration 71/1000 | Loss: 0.00002372
Iteration 72/1000 | Loss: 0.00002372
Iteration 73/1000 | Loss: 0.00002372
Iteration 74/1000 | Loss: 0.00002372
Iteration 75/1000 | Loss: 0.00002371
Iteration 76/1000 | Loss: 0.00002371
Iteration 77/1000 | Loss: 0.00002370
Iteration 78/1000 | Loss: 0.00002370
Iteration 79/1000 | Loss: 0.00002370
Iteration 80/1000 | Loss: 0.00002370
Iteration 81/1000 | Loss: 0.00002369
Iteration 82/1000 | Loss: 0.00002369
Iteration 83/1000 | Loss: 0.00002369
Iteration 84/1000 | Loss: 0.00002369
Iteration 85/1000 | Loss: 0.00002369
Iteration 86/1000 | Loss: 0.00002368
Iteration 87/1000 | Loss: 0.00002368
Iteration 88/1000 | Loss: 0.00002368
Iteration 89/1000 | Loss: 0.00002368
Iteration 90/1000 | Loss: 0.00002367
Iteration 91/1000 | Loss: 0.00002367
Iteration 92/1000 | Loss: 0.00002367
Iteration 93/1000 | Loss: 0.00002367
Iteration 94/1000 | Loss: 0.00002367
Iteration 95/1000 | Loss: 0.00002367
Iteration 96/1000 | Loss: 0.00002367
Iteration 97/1000 | Loss: 0.00002366
Iteration 98/1000 | Loss: 0.00002366
Iteration 99/1000 | Loss: 0.00002366
Iteration 100/1000 | Loss: 0.00002366
Iteration 101/1000 | Loss: 0.00002366
Iteration 102/1000 | Loss: 0.00002366
Iteration 103/1000 | Loss: 0.00002366
Iteration 104/1000 | Loss: 0.00002365
Iteration 105/1000 | Loss: 0.00002365
Iteration 106/1000 | Loss: 0.00002365
Iteration 107/1000 | Loss: 0.00002365
Iteration 108/1000 | Loss: 0.00002365
Iteration 109/1000 | Loss: 0.00002365
Iteration 110/1000 | Loss: 0.00002365
Iteration 111/1000 | Loss: 0.00002365
Iteration 112/1000 | Loss: 0.00002365
Iteration 113/1000 | Loss: 0.00002365
Iteration 114/1000 | Loss: 0.00002364
Iteration 115/1000 | Loss: 0.00002364
Iteration 116/1000 | Loss: 0.00002364
Iteration 117/1000 | Loss: 0.00002364
Iteration 118/1000 | Loss: 0.00002363
Iteration 119/1000 | Loss: 0.00002363
Iteration 120/1000 | Loss: 0.00002363
Iteration 121/1000 | Loss: 0.00002363
Iteration 122/1000 | Loss: 0.00002363
Iteration 123/1000 | Loss: 0.00002363
Iteration 124/1000 | Loss: 0.00002363
Iteration 125/1000 | Loss: 0.00002363
Iteration 126/1000 | Loss: 0.00002363
Iteration 127/1000 | Loss: 0.00002363
Iteration 128/1000 | Loss: 0.00002363
Iteration 129/1000 | Loss: 0.00002363
Iteration 130/1000 | Loss: 0.00002362
Iteration 131/1000 | Loss: 0.00002362
Iteration 132/1000 | Loss: 0.00002362
Iteration 133/1000 | Loss: 0.00002362
Iteration 134/1000 | Loss: 0.00002361
Iteration 135/1000 | Loss: 0.00002361
Iteration 136/1000 | Loss: 0.00002361
Iteration 137/1000 | Loss: 0.00002361
Iteration 138/1000 | Loss: 0.00002361
Iteration 139/1000 | Loss: 0.00002361
Iteration 140/1000 | Loss: 0.00002361
Iteration 141/1000 | Loss: 0.00002361
Iteration 142/1000 | Loss: 0.00002361
Iteration 143/1000 | Loss: 0.00002361
Iteration 144/1000 | Loss: 0.00002361
Iteration 145/1000 | Loss: 0.00002361
Iteration 146/1000 | Loss: 0.00002361
Iteration 147/1000 | Loss: 0.00002361
Iteration 148/1000 | Loss: 0.00002360
Iteration 149/1000 | Loss: 0.00002360
Iteration 150/1000 | Loss: 0.00002360
Iteration 151/1000 | Loss: 0.00002360
Iteration 152/1000 | Loss: 0.00002360
Iteration 153/1000 | Loss: 0.00002360
Iteration 154/1000 | Loss: 0.00002360
Iteration 155/1000 | Loss: 0.00002360
Iteration 156/1000 | Loss: 0.00002360
Iteration 157/1000 | Loss: 0.00002360
Iteration 158/1000 | Loss: 0.00002360
Iteration 159/1000 | Loss: 0.00002360
Iteration 160/1000 | Loss: 0.00002360
Iteration 161/1000 | Loss: 0.00002360
Iteration 162/1000 | Loss: 0.00002360
Iteration 163/1000 | Loss: 0.00002360
Iteration 164/1000 | Loss: 0.00002360
Iteration 165/1000 | Loss: 0.00002360
Iteration 166/1000 | Loss: 0.00002359
Iteration 167/1000 | Loss: 0.00002359
Iteration 168/1000 | Loss: 0.00002359
Iteration 169/1000 | Loss: 0.00002359
Iteration 170/1000 | Loss: 0.00002359
Iteration 171/1000 | Loss: 0.00002359
Iteration 172/1000 | Loss: 0.00002359
Iteration 173/1000 | Loss: 0.00002359
Iteration 174/1000 | Loss: 0.00002359
Iteration 175/1000 | Loss: 0.00002359
Iteration 176/1000 | Loss: 0.00002359
Iteration 177/1000 | Loss: 0.00002359
Iteration 178/1000 | Loss: 0.00002359
Iteration 179/1000 | Loss: 0.00002359
Iteration 180/1000 | Loss: 0.00002359
Iteration 181/1000 | Loss: 0.00002359
Iteration 182/1000 | Loss: 0.00002359
Iteration 183/1000 | Loss: 0.00002359
Iteration 184/1000 | Loss: 0.00002359
Iteration 185/1000 | Loss: 0.00002359
Iteration 186/1000 | Loss: 0.00002359
Iteration 187/1000 | Loss: 0.00002359
Iteration 188/1000 | Loss: 0.00002359
Iteration 189/1000 | Loss: 0.00002359
Iteration 190/1000 | Loss: 0.00002359
Iteration 191/1000 | Loss: 0.00002359
Iteration 192/1000 | Loss: 0.00002359
Iteration 193/1000 | Loss: 0.00002359
Iteration 194/1000 | Loss: 0.00002359
Iteration 195/1000 | Loss: 0.00002359
Iteration 196/1000 | Loss: 0.00002359
Iteration 197/1000 | Loss: 0.00002359
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 197. Stopping optimization.
Last 5 losses: [2.3589434931636788e-05, 2.3589434931636788e-05, 2.3589434931636788e-05, 2.3589434931636788e-05, 2.3589434931636788e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3589434931636788e-05

Optimization complete. Final v2v error: 4.001547336578369 mm

Highest mean error: 5.917936325073242 mm for frame 114

Lowest mean error: 3.661808490753174 mm for frame 192

Saving results

Total time: 124.66985368728638
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ben_posed_008/1034/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_008/1034.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_008/1034
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00495006
Iteration 2/25 | Loss: 0.00088580
Iteration 3/25 | Loss: 0.00068772
Iteration 4/25 | Loss: 0.00065507
Iteration 5/25 | Loss: 0.00064202
Iteration 6/25 | Loss: 0.00063942
Iteration 7/25 | Loss: 0.00063898
Iteration 8/25 | Loss: 0.00063898
Iteration 9/25 | Loss: 0.00063898
Iteration 10/25 | Loss: 0.00063898
Iteration 11/25 | Loss: 0.00063898
Iteration 12/25 | Loss: 0.00063898
Iteration 13/25 | Loss: 0.00063898
Iteration 14/25 | Loss: 0.00063898
Iteration 15/25 | Loss: 0.00063898
Iteration 16/25 | Loss: 0.00063898
Iteration 17/25 | Loss: 0.00063898
Iteration 18/25 | Loss: 0.00063898
Iteration 19/25 | Loss: 0.00063898
Iteration 20/25 | Loss: 0.00063898
Iteration 21/25 | Loss: 0.00063898
Iteration 22/25 | Loss: 0.00063898
Iteration 23/25 | Loss: 0.00063898
Iteration 24/25 | Loss: 0.00063898
Iteration 25/25 | Loss: 0.00063898

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46461296
Iteration 2/25 | Loss: 0.00021984
Iteration 3/25 | Loss: 0.00021980
Iteration 4/25 | Loss: 0.00021980
Iteration 5/25 | Loss: 0.00021980
Iteration 6/25 | Loss: 0.00021980
Iteration 7/25 | Loss: 0.00021980
Iteration 8/25 | Loss: 0.00021980
Iteration 9/25 | Loss: 0.00021980
Iteration 10/25 | Loss: 0.00021980
Iteration 11/25 | Loss: 0.00021980
Iteration 12/25 | Loss: 0.00021980
Iteration 13/25 | Loss: 0.00021980
Iteration 14/25 | Loss: 0.00021980
Iteration 15/25 | Loss: 0.00021980
Iteration 16/25 | Loss: 0.00021980
Iteration 17/25 | Loss: 0.00021980
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.00021980133897159249, 0.00021980133897159249, 0.00021980133897159249, 0.00021980133897159249, 0.00021980133897159249]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00021980133897159249

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00021980
Iteration 2/1000 | Loss: 0.00002731
Iteration 3/1000 | Loss: 0.00002009
Iteration 4/1000 | Loss: 0.00001853
Iteration 5/1000 | Loss: 0.00001782
Iteration 6/1000 | Loss: 0.00001726
Iteration 7/1000 | Loss: 0.00001695
Iteration 8/1000 | Loss: 0.00001671
Iteration 9/1000 | Loss: 0.00001650
Iteration 10/1000 | Loss: 0.00001648
Iteration 11/1000 | Loss: 0.00001639
Iteration 12/1000 | Loss: 0.00001632
Iteration 13/1000 | Loss: 0.00001624
Iteration 14/1000 | Loss: 0.00001621
Iteration 15/1000 | Loss: 0.00001620
Iteration 16/1000 | Loss: 0.00001619
Iteration 17/1000 | Loss: 0.00001619
Iteration 18/1000 | Loss: 0.00001614
Iteration 19/1000 | Loss: 0.00001614
Iteration 20/1000 | Loss: 0.00001613
Iteration 21/1000 | Loss: 0.00001613
Iteration 22/1000 | Loss: 0.00001612
Iteration 23/1000 | Loss: 0.00001612
Iteration 24/1000 | Loss: 0.00001610
Iteration 25/1000 | Loss: 0.00001609
Iteration 26/1000 | Loss: 0.00001608
Iteration 27/1000 | Loss: 0.00001608
Iteration 28/1000 | Loss: 0.00001603
Iteration 29/1000 | Loss: 0.00001603
Iteration 30/1000 | Loss: 0.00001600
Iteration 31/1000 | Loss: 0.00001599
Iteration 32/1000 | Loss: 0.00001599
Iteration 33/1000 | Loss: 0.00001598
Iteration 34/1000 | Loss: 0.00001598
Iteration 35/1000 | Loss: 0.00001598
Iteration 36/1000 | Loss: 0.00001597
Iteration 37/1000 | Loss: 0.00001597
Iteration 38/1000 | Loss: 0.00001596
Iteration 39/1000 | Loss: 0.00001596
Iteration 40/1000 | Loss: 0.00001596
Iteration 41/1000 | Loss: 0.00001595
Iteration 42/1000 | Loss: 0.00001595
Iteration 43/1000 | Loss: 0.00001595
Iteration 44/1000 | Loss: 0.00001595
Iteration 45/1000 | Loss: 0.00001594
Iteration 46/1000 | Loss: 0.00001594
Iteration 47/1000 | Loss: 0.00001594
Iteration 48/1000 | Loss: 0.00001594
Iteration 49/1000 | Loss: 0.00001593
Iteration 50/1000 | Loss: 0.00001593
Iteration 51/1000 | Loss: 0.00001593
Iteration 52/1000 | Loss: 0.00001592
Iteration 53/1000 | Loss: 0.00001592
Iteration 54/1000 | Loss: 0.00001592
Iteration 55/1000 | Loss: 0.00001592
Iteration 56/1000 | Loss: 0.00001592
Iteration 57/1000 | Loss: 0.00001591
Iteration 58/1000 | Loss: 0.00001591
Iteration 59/1000 | Loss: 0.00001591
Iteration 60/1000 | Loss: 0.00001591
Iteration 61/1000 | Loss: 0.00001590
Iteration 62/1000 | Loss: 0.00001590
Iteration 63/1000 | Loss: 0.00001590
Iteration 64/1000 | Loss: 0.00001590
Iteration 65/1000 | Loss: 0.00001590
Iteration 66/1000 | Loss: 0.00001590
Iteration 67/1000 | Loss: 0.00001589
Iteration 68/1000 | Loss: 0.00001589
Iteration 69/1000 | Loss: 0.00001589
Iteration 70/1000 | Loss: 0.00001589
Iteration 71/1000 | Loss: 0.00001589
Iteration 72/1000 | Loss: 0.00001589
Iteration 73/1000 | Loss: 0.00001588
Iteration 74/1000 | Loss: 0.00001588
Iteration 75/1000 | Loss: 0.00001588
Iteration 76/1000 | Loss: 0.00001588
Iteration 77/1000 | Loss: 0.00001588
Iteration 78/1000 | Loss: 0.00001588
Iteration 79/1000 | Loss: 0.00001588
Iteration 80/1000 | Loss: 0.00001587
Iteration 81/1000 | Loss: 0.00001587
Iteration 82/1000 | Loss: 0.00001587
Iteration 83/1000 | Loss: 0.00001586
Iteration 84/1000 | Loss: 0.00001586
Iteration 85/1000 | Loss: 0.00001586
Iteration 86/1000 | Loss: 0.00001586
Iteration 87/1000 | Loss: 0.00001586
Iteration 88/1000 | Loss: 0.00001586
Iteration 89/1000 | Loss: 0.00001585
Iteration 90/1000 | Loss: 0.00001585
Iteration 91/1000 | Loss: 0.00001585
Iteration 92/1000 | Loss: 0.00001585
Iteration 93/1000 | Loss: 0.00001585
Iteration 94/1000 | Loss: 0.00001585
Iteration 95/1000 | Loss: 0.00001585
Iteration 96/1000 | Loss: 0.00001585
Iteration 97/1000 | Loss: 0.00001585
Iteration 98/1000 | Loss: 0.00001585
Iteration 99/1000 | Loss: 0.00001585
Iteration 100/1000 | Loss: 0.00001585
Iteration 101/1000 | Loss: 0.00001585
Iteration 102/1000 | Loss: 0.00001585
Iteration 103/1000 | Loss: 0.00001585
Iteration 104/1000 | Loss: 0.00001585
Iteration 105/1000 | Loss: 0.00001585
Iteration 106/1000 | Loss: 0.00001584
Iteration 107/1000 | Loss: 0.00001584
Iteration 108/1000 | Loss: 0.00001584
Iteration 109/1000 | Loss: 0.00001584
Iteration 110/1000 | Loss: 0.00001584
Iteration 111/1000 | Loss: 0.00001584
Iteration 112/1000 | Loss: 0.00001584
Iteration 113/1000 | Loss: 0.00001584
Iteration 114/1000 | Loss: 0.00001583
Iteration 115/1000 | Loss: 0.00001583
Iteration 116/1000 | Loss: 0.00001583
Iteration 117/1000 | Loss: 0.00001583
Iteration 118/1000 | Loss: 0.00001583
Iteration 119/1000 | Loss: 0.00001583
Iteration 120/1000 | Loss: 0.00001583
Iteration 121/1000 | Loss: 0.00001583
Iteration 122/1000 | Loss: 0.00001583
Iteration 123/1000 | Loss: 0.00001583
Iteration 124/1000 | Loss: 0.00001583
Iteration 125/1000 | Loss: 0.00001583
Iteration 126/1000 | Loss: 0.00001583
Iteration 127/1000 | Loss: 0.00001583
Iteration 128/1000 | Loss: 0.00001583
Iteration 129/1000 | Loss: 0.00001583
Iteration 130/1000 | Loss: 0.00001582
Iteration 131/1000 | Loss: 0.00001582
Iteration 132/1000 | Loss: 0.00001582
Iteration 133/1000 | Loss: 0.00001582
Iteration 134/1000 | Loss: 0.00001581
Iteration 135/1000 | Loss: 0.00001581
Iteration 136/1000 | Loss: 0.00001581
Iteration 137/1000 | Loss: 0.00001581
Iteration 138/1000 | Loss: 0.00001581
Iteration 139/1000 | Loss: 0.00001581
Iteration 140/1000 | Loss: 0.00001581
Iteration 141/1000 | Loss: 0.00001581
Iteration 142/1000 | Loss: 0.00001581
Iteration 143/1000 | Loss: 0.00001581
Iteration 144/1000 | Loss: 0.00001581
Iteration 145/1000 | Loss: 0.00001581
Iteration 146/1000 | Loss: 0.00001581
Iteration 147/1000 | Loss: 0.00001581
Iteration 148/1000 | Loss: 0.00001581
Iteration 149/1000 | Loss: 0.00001581
Iteration 150/1000 | Loss: 0.00001581
Iteration 151/1000 | Loss: 0.00001581
Iteration 152/1000 | Loss: 0.00001581
Iteration 153/1000 | Loss: 0.00001581
Iteration 154/1000 | Loss: 0.00001581
Iteration 155/1000 | Loss: 0.00001581
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 155. Stopping optimization.
Last 5 losses: [1.5811050616321154e-05, 1.5811050616321154e-05, 1.5811050616321154e-05, 1.5811050616321154e-05, 1.5811050616321154e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5811050616321154e-05

Optimization complete. Final v2v error: 3.3188347816467285 mm

Highest mean error: 4.111875534057617 mm for frame 30

Lowest mean error: 2.779393434524536 mm for frame 146

Saving results

Total time: 44.3222131729126
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ben_posed_008/1008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_008/1008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_008/1008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00862937
Iteration 2/25 | Loss: 0.00074394
Iteration 3/25 | Loss: 0.00058231
Iteration 4/25 | Loss: 0.00055804
Iteration 5/25 | Loss: 0.00054881
Iteration 6/25 | Loss: 0.00054740
Iteration 7/25 | Loss: 0.00054699
Iteration 8/25 | Loss: 0.00054698
Iteration 9/25 | Loss: 0.00054698
Iteration 10/25 | Loss: 0.00054698
Iteration 11/25 | Loss: 0.00054698
Iteration 12/25 | Loss: 0.00054698
Iteration 13/25 | Loss: 0.00054698
Iteration 14/25 | Loss: 0.00054698
Iteration 15/25 | Loss: 0.00054698
Iteration 16/25 | Loss: 0.00054698
Iteration 17/25 | Loss: 0.00054698
Iteration 18/25 | Loss: 0.00054698
Iteration 19/25 | Loss: 0.00054698
Iteration 20/25 | Loss: 0.00054698
Iteration 21/25 | Loss: 0.00054698
Iteration 22/25 | Loss: 0.00054698
Iteration 23/25 | Loss: 0.00054698
Iteration 24/25 | Loss: 0.00054698
Iteration 25/25 | Loss: 0.00054698

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.65989625
Iteration 2/25 | Loss: 0.00015471
Iteration 3/25 | Loss: 0.00015471
Iteration 4/25 | Loss: 0.00015470
Iteration 5/25 | Loss: 0.00015470
Iteration 6/25 | Loss: 0.00015470
Iteration 7/25 | Loss: 0.00015470
Iteration 8/25 | Loss: 0.00015470
Iteration 9/25 | Loss: 0.00015470
Iteration 10/25 | Loss: 0.00015470
Iteration 11/25 | Loss: 0.00015470
Iteration 12/25 | Loss: 0.00015470
Iteration 13/25 | Loss: 0.00015470
Iteration 14/25 | Loss: 0.00015470
Iteration 15/25 | Loss: 0.00015470
Iteration 16/25 | Loss: 0.00015470
Iteration 17/25 | Loss: 0.00015470
Iteration 18/25 | Loss: 0.00015470
Iteration 19/25 | Loss: 0.00015470
Iteration 20/25 | Loss: 0.00015470
Iteration 21/25 | Loss: 0.00015470
Iteration 22/25 | Loss: 0.00015470
Iteration 23/25 | Loss: 0.00015470
Iteration 24/25 | Loss: 0.00015470
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.00015470250218641013, 0.00015470250218641013, 0.00015470250218641013, 0.00015470250218641013, 0.00015470250218641013]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00015470250218641013

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00015470
Iteration 2/1000 | Loss: 0.00001733
Iteration 3/1000 | Loss: 0.00001287
Iteration 4/1000 | Loss: 0.00001212
Iteration 5/1000 | Loss: 0.00001152
Iteration 6/1000 | Loss: 0.00001128
Iteration 7/1000 | Loss: 0.00001103
Iteration 8/1000 | Loss: 0.00001089
Iteration 9/1000 | Loss: 0.00001088
Iteration 10/1000 | Loss: 0.00001088
Iteration 11/1000 | Loss: 0.00001079
Iteration 12/1000 | Loss: 0.00001076
Iteration 13/1000 | Loss: 0.00001076
Iteration 14/1000 | Loss: 0.00001075
Iteration 15/1000 | Loss: 0.00001074
Iteration 16/1000 | Loss: 0.00001072
Iteration 17/1000 | Loss: 0.00001072
Iteration 18/1000 | Loss: 0.00001072
Iteration 19/1000 | Loss: 0.00001071
Iteration 20/1000 | Loss: 0.00001071
Iteration 21/1000 | Loss: 0.00001070
Iteration 22/1000 | Loss: 0.00001069
Iteration 23/1000 | Loss: 0.00001069
Iteration 24/1000 | Loss: 0.00001068
Iteration 25/1000 | Loss: 0.00001067
Iteration 26/1000 | Loss: 0.00001067
Iteration 27/1000 | Loss: 0.00001067
Iteration 28/1000 | Loss: 0.00001065
Iteration 29/1000 | Loss: 0.00001064
Iteration 30/1000 | Loss: 0.00001064
Iteration 31/1000 | Loss: 0.00001064
Iteration 32/1000 | Loss: 0.00001064
Iteration 33/1000 | Loss: 0.00001064
Iteration 34/1000 | Loss: 0.00001064
Iteration 35/1000 | Loss: 0.00001063
Iteration 36/1000 | Loss: 0.00001063
Iteration 37/1000 | Loss: 0.00001063
Iteration 38/1000 | Loss: 0.00001063
Iteration 39/1000 | Loss: 0.00001063
Iteration 40/1000 | Loss: 0.00001063
Iteration 41/1000 | Loss: 0.00001063
Iteration 42/1000 | Loss: 0.00001063
Iteration 43/1000 | Loss: 0.00001062
Iteration 44/1000 | Loss: 0.00001061
Iteration 45/1000 | Loss: 0.00001060
Iteration 46/1000 | Loss: 0.00001060
Iteration 47/1000 | Loss: 0.00001060
Iteration 48/1000 | Loss: 0.00001059
Iteration 49/1000 | Loss: 0.00001059
Iteration 50/1000 | Loss: 0.00001056
Iteration 51/1000 | Loss: 0.00001056
Iteration 52/1000 | Loss: 0.00001056
Iteration 53/1000 | Loss: 0.00001056
Iteration 54/1000 | Loss: 0.00001056
Iteration 55/1000 | Loss: 0.00001055
Iteration 56/1000 | Loss: 0.00001055
Iteration 57/1000 | Loss: 0.00001055
Iteration 58/1000 | Loss: 0.00001055
Iteration 59/1000 | Loss: 0.00001055
Iteration 60/1000 | Loss: 0.00001055
Iteration 61/1000 | Loss: 0.00001055
Iteration 62/1000 | Loss: 0.00001055
Iteration 63/1000 | Loss: 0.00001055
Iteration 64/1000 | Loss: 0.00001055
Iteration 65/1000 | Loss: 0.00001052
Iteration 66/1000 | Loss: 0.00001052
Iteration 67/1000 | Loss: 0.00001052
Iteration 68/1000 | Loss: 0.00001052
Iteration 69/1000 | Loss: 0.00001052
Iteration 70/1000 | Loss: 0.00001052
Iteration 71/1000 | Loss: 0.00001052
Iteration 72/1000 | Loss: 0.00001052
Iteration 73/1000 | Loss: 0.00001052
Iteration 74/1000 | Loss: 0.00001052
Iteration 75/1000 | Loss: 0.00001051
Iteration 76/1000 | Loss: 0.00001051
Iteration 77/1000 | Loss: 0.00001051
Iteration 78/1000 | Loss: 0.00001051
Iteration 79/1000 | Loss: 0.00001051
Iteration 80/1000 | Loss: 0.00001050
Iteration 81/1000 | Loss: 0.00001050
Iteration 82/1000 | Loss: 0.00001050
Iteration 83/1000 | Loss: 0.00001049
Iteration 84/1000 | Loss: 0.00001049
Iteration 85/1000 | Loss: 0.00001049
Iteration 86/1000 | Loss: 0.00001049
Iteration 87/1000 | Loss: 0.00001049
Iteration 88/1000 | Loss: 0.00001049
Iteration 89/1000 | Loss: 0.00001049
Iteration 90/1000 | Loss: 0.00001049
Iteration 91/1000 | Loss: 0.00001049
Iteration 92/1000 | Loss: 0.00001049
Iteration 93/1000 | Loss: 0.00001049
Iteration 94/1000 | Loss: 0.00001049
Iteration 95/1000 | Loss: 0.00001049
Iteration 96/1000 | Loss: 0.00001049
Iteration 97/1000 | Loss: 0.00001048
Iteration 98/1000 | Loss: 0.00001048
Iteration 99/1000 | Loss: 0.00001048
Iteration 100/1000 | Loss: 0.00001047
Iteration 101/1000 | Loss: 0.00001047
Iteration 102/1000 | Loss: 0.00001047
Iteration 103/1000 | Loss: 0.00001047
Iteration 104/1000 | Loss: 0.00001047
Iteration 105/1000 | Loss: 0.00001047
Iteration 106/1000 | Loss: 0.00001046
Iteration 107/1000 | Loss: 0.00001046
Iteration 108/1000 | Loss: 0.00001046
Iteration 109/1000 | Loss: 0.00001046
Iteration 110/1000 | Loss: 0.00001046
Iteration 111/1000 | Loss: 0.00001046
Iteration 112/1000 | Loss: 0.00001046
Iteration 113/1000 | Loss: 0.00001046
Iteration 114/1000 | Loss: 0.00001046
Iteration 115/1000 | Loss: 0.00001046
Iteration 116/1000 | Loss: 0.00001046
Iteration 117/1000 | Loss: 0.00001046
Iteration 118/1000 | Loss: 0.00001045
Iteration 119/1000 | Loss: 0.00001045
Iteration 120/1000 | Loss: 0.00001045
Iteration 121/1000 | Loss: 0.00001045
Iteration 122/1000 | Loss: 0.00001044
Iteration 123/1000 | Loss: 0.00001044
Iteration 124/1000 | Loss: 0.00001044
Iteration 125/1000 | Loss: 0.00001044
Iteration 126/1000 | Loss: 0.00001043
Iteration 127/1000 | Loss: 0.00001043
Iteration 128/1000 | Loss: 0.00001043
Iteration 129/1000 | Loss: 0.00001043
Iteration 130/1000 | Loss: 0.00001042
Iteration 131/1000 | Loss: 0.00001042
Iteration 132/1000 | Loss: 0.00001042
Iteration 133/1000 | Loss: 0.00001042
Iteration 134/1000 | Loss: 0.00001041
Iteration 135/1000 | Loss: 0.00001041
Iteration 136/1000 | Loss: 0.00001041
Iteration 137/1000 | Loss: 0.00001041
Iteration 138/1000 | Loss: 0.00001041
Iteration 139/1000 | Loss: 0.00001041
Iteration 140/1000 | Loss: 0.00001040
Iteration 141/1000 | Loss: 0.00001040
Iteration 142/1000 | Loss: 0.00001039
Iteration 143/1000 | Loss: 0.00001039
Iteration 144/1000 | Loss: 0.00001039
Iteration 145/1000 | Loss: 0.00001039
Iteration 146/1000 | Loss: 0.00001039
Iteration 147/1000 | Loss: 0.00001039
Iteration 148/1000 | Loss: 0.00001039
Iteration 149/1000 | Loss: 0.00001039
Iteration 150/1000 | Loss: 0.00001039
Iteration 151/1000 | Loss: 0.00001039
Iteration 152/1000 | Loss: 0.00001039
Iteration 153/1000 | Loss: 0.00001039
Iteration 154/1000 | Loss: 0.00001039
Iteration 155/1000 | Loss: 0.00001039
Iteration 156/1000 | Loss: 0.00001039
Iteration 157/1000 | Loss: 0.00001039
Iteration 158/1000 | Loss: 0.00001039
Iteration 159/1000 | Loss: 0.00001039
Iteration 160/1000 | Loss: 0.00001039
Iteration 161/1000 | Loss: 0.00001039
Iteration 162/1000 | Loss: 0.00001039
Iteration 163/1000 | Loss: 0.00001039
Iteration 164/1000 | Loss: 0.00001039
Iteration 165/1000 | Loss: 0.00001039
Iteration 166/1000 | Loss: 0.00001039
Iteration 167/1000 | Loss: 0.00001039
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 167. Stopping optimization.
Last 5 losses: [1.0387718248239253e-05, 1.0387718248239253e-05, 1.0387718248239253e-05, 1.0387718248239253e-05, 1.0387718248239253e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0387718248239253e-05

Optimization complete. Final v2v error: 2.7431111335754395 mm

Highest mean error: 2.906937599182129 mm for frame 63

Lowest mean error: 2.588336706161499 mm for frame 123

Saving results

Total time: 34.785195112228394
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ben_posed_008/1052/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_008/1052.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_008/1052
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00655461
Iteration 2/25 | Loss: 0.00109786
Iteration 3/25 | Loss: 0.00071028
Iteration 4/25 | Loss: 0.00063684
Iteration 5/25 | Loss: 0.00061507
Iteration 6/25 | Loss: 0.00060193
Iteration 7/25 | Loss: 0.00059871
Iteration 8/25 | Loss: 0.00060179
Iteration 9/25 | Loss: 0.00060751
Iteration 10/25 | Loss: 0.00059983
Iteration 11/25 | Loss: 0.00059365
Iteration 12/25 | Loss: 0.00058976
Iteration 13/25 | Loss: 0.00058659
Iteration 14/25 | Loss: 0.00058506
Iteration 15/25 | Loss: 0.00058452
Iteration 16/25 | Loss: 0.00058418
Iteration 17/25 | Loss: 0.00058408
Iteration 18/25 | Loss: 0.00058407
Iteration 19/25 | Loss: 0.00058406
Iteration 20/25 | Loss: 0.00058406
Iteration 21/25 | Loss: 0.00058406
Iteration 22/25 | Loss: 0.00058406
Iteration 23/25 | Loss: 0.00058406
Iteration 24/25 | Loss: 0.00058406
Iteration 25/25 | Loss: 0.00058406

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.74601054
Iteration 2/25 | Loss: 0.00016803
Iteration 3/25 | Loss: 0.00016803
Iteration 4/25 | Loss: 0.00016803
Iteration 5/25 | Loss: 0.00016802
Iteration 6/25 | Loss: 0.00016802
Iteration 7/25 | Loss: 0.00016802
Iteration 8/25 | Loss: 0.00016802
Iteration 9/25 | Loss: 0.00016802
Iteration 10/25 | Loss: 0.00016802
Iteration 11/25 | Loss: 0.00016802
Iteration 12/25 | Loss: 0.00016802
Iteration 13/25 | Loss: 0.00016802
Iteration 14/25 | Loss: 0.00016802
Iteration 15/25 | Loss: 0.00016802
Iteration 16/25 | Loss: 0.00016802
Iteration 17/25 | Loss: 0.00016802
Iteration 18/25 | Loss: 0.00016802
Iteration 19/25 | Loss: 0.00016802
Iteration 20/25 | Loss: 0.00016802
Iteration 21/25 | Loss: 0.00016802
Iteration 22/25 | Loss: 0.00016802
Iteration 23/25 | Loss: 0.00016802
Iteration 24/25 | Loss: 0.00016802
Iteration 25/25 | Loss: 0.00016802

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00016802
Iteration 2/1000 | Loss: 0.00002590
Iteration 3/1000 | Loss: 0.00001767
Iteration 4/1000 | Loss: 0.00001633
Iteration 5/1000 | Loss: 0.00003550
Iteration 6/1000 | Loss: 0.00003351
Iteration 7/1000 | Loss: 0.00006266
Iteration 8/1000 | Loss: 0.00034148
Iteration 9/1000 | Loss: 0.00001602
Iteration 10/1000 | Loss: 0.00001444
Iteration 11/1000 | Loss: 0.00001421
Iteration 12/1000 | Loss: 0.00001402
Iteration 13/1000 | Loss: 0.00003110
Iteration 14/1000 | Loss: 0.00001382
Iteration 15/1000 | Loss: 0.00001376
Iteration 16/1000 | Loss: 0.00001373
Iteration 17/1000 | Loss: 0.00001373
Iteration 18/1000 | Loss: 0.00001373
Iteration 19/1000 | Loss: 0.00001373
Iteration 20/1000 | Loss: 0.00001370
Iteration 21/1000 | Loss: 0.00001365
Iteration 22/1000 | Loss: 0.00001365
Iteration 23/1000 | Loss: 0.00001365
Iteration 24/1000 | Loss: 0.00001365
Iteration 25/1000 | Loss: 0.00001364
Iteration 26/1000 | Loss: 0.00001364
Iteration 27/1000 | Loss: 0.00001363
Iteration 28/1000 | Loss: 0.00001362
Iteration 29/1000 | Loss: 0.00003781
Iteration 30/1000 | Loss: 0.00003781
Iteration 31/1000 | Loss: 0.00004822
Iteration 32/1000 | Loss: 0.00001391
Iteration 33/1000 | Loss: 0.00001354
Iteration 34/1000 | Loss: 0.00001351
Iteration 35/1000 | Loss: 0.00001350
Iteration 36/1000 | Loss: 0.00001350
Iteration 37/1000 | Loss: 0.00001350
Iteration 38/1000 | Loss: 0.00001350
Iteration 39/1000 | Loss: 0.00001350
Iteration 40/1000 | Loss: 0.00001349
Iteration 41/1000 | Loss: 0.00001349
Iteration 42/1000 | Loss: 0.00001349
Iteration 43/1000 | Loss: 0.00001349
Iteration 44/1000 | Loss: 0.00001349
Iteration 45/1000 | Loss: 0.00001349
Iteration 46/1000 | Loss: 0.00001349
Iteration 47/1000 | Loss: 0.00001349
Iteration 48/1000 | Loss: 0.00001348
Iteration 49/1000 | Loss: 0.00001348
Iteration 50/1000 | Loss: 0.00001348
Iteration 51/1000 | Loss: 0.00001348
Iteration 52/1000 | Loss: 0.00001348
Iteration 53/1000 | Loss: 0.00001348
Iteration 54/1000 | Loss: 0.00001348
Iteration 55/1000 | Loss: 0.00001348
Iteration 56/1000 | Loss: 0.00001348
Iteration 57/1000 | Loss: 0.00001347
Iteration 58/1000 | Loss: 0.00001347
Iteration 59/1000 | Loss: 0.00001347
Iteration 60/1000 | Loss: 0.00001347
Iteration 61/1000 | Loss: 0.00001347
Iteration 62/1000 | Loss: 0.00001347
Iteration 63/1000 | Loss: 0.00001347
Iteration 64/1000 | Loss: 0.00001347
Iteration 65/1000 | Loss: 0.00001347
Iteration 66/1000 | Loss: 0.00001347
Iteration 67/1000 | Loss: 0.00001347
Iteration 68/1000 | Loss: 0.00001347
Iteration 69/1000 | Loss: 0.00001347
Iteration 70/1000 | Loss: 0.00001347
Iteration 71/1000 | Loss: 0.00001347
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 71. Stopping optimization.
Last 5 losses: [1.34695310407551e-05, 1.34695310407551e-05, 1.34695310407551e-05, 1.34695310407551e-05, 1.34695310407551e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.34695310407551e-05

Optimization complete. Final v2v error: 3.081207513809204 mm

Highest mean error: 4.304784774780273 mm for frame 73

Lowest mean error: 2.5114870071411133 mm for frame 51

Saving results

Total time: 66.60404992103577
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ben_posed_008/1029/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_008/1029.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_008/1029
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00841112
Iteration 2/25 | Loss: 0.00090564
Iteration 3/25 | Loss: 0.00073496
Iteration 4/25 | Loss: 0.00069082
Iteration 5/25 | Loss: 0.00066583
Iteration 6/25 | Loss: 0.00065893
Iteration 7/25 | Loss: 0.00065629
Iteration 8/25 | Loss: 0.00065472
Iteration 9/25 | Loss: 0.00065446
Iteration 10/25 | Loss: 0.00065446
Iteration 11/25 | Loss: 0.00065446
Iteration 12/25 | Loss: 0.00065446
Iteration 13/25 | Loss: 0.00065446
Iteration 14/25 | Loss: 0.00065446
Iteration 15/25 | Loss: 0.00065446
Iteration 16/25 | Loss: 0.00065446
Iteration 17/25 | Loss: 0.00065446
Iteration 18/25 | Loss: 0.00065446
Iteration 19/25 | Loss: 0.00065446
Iteration 20/25 | Loss: 0.00065446
Iteration 21/25 | Loss: 0.00065446
Iteration 22/25 | Loss: 0.00065446
Iteration 23/25 | Loss: 0.00065446
Iteration 24/25 | Loss: 0.00065446
Iteration 25/25 | Loss: 0.00065446
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0006544628413394094, 0.0006544628413394094, 0.0006544628413394094, 0.0006544628413394094, 0.0006544628413394094]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006544628413394094

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.66929317
Iteration 2/25 | Loss: 0.00028252
Iteration 3/25 | Loss: 0.00028252
Iteration 4/25 | Loss: 0.00028252
Iteration 5/25 | Loss: 0.00028252
Iteration 6/25 | Loss: 0.00028252
Iteration 7/25 | Loss: 0.00028252
Iteration 8/25 | Loss: 0.00028252
Iteration 9/25 | Loss: 0.00028252
Iteration 10/25 | Loss: 0.00028252
Iteration 11/25 | Loss: 0.00028252
Iteration 12/25 | Loss: 0.00028252
Iteration 13/25 | Loss: 0.00028252
Iteration 14/25 | Loss: 0.00028252
Iteration 15/25 | Loss: 0.00028252
Iteration 16/25 | Loss: 0.00028252
Iteration 17/25 | Loss: 0.00028252
Iteration 18/25 | Loss: 0.00028252
Iteration 19/25 | Loss: 0.00028252
Iteration 20/25 | Loss: 0.00028252
Iteration 21/25 | Loss: 0.00028252
Iteration 22/25 | Loss: 0.00028252
Iteration 23/25 | Loss: 0.00028252
Iteration 24/25 | Loss: 0.00028252
Iteration 25/25 | Loss: 0.00028252

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00028252
Iteration 2/1000 | Loss: 0.00004302
Iteration 3/1000 | Loss: 0.00003242
Iteration 4/1000 | Loss: 0.00002664
Iteration 5/1000 | Loss: 0.00002466
Iteration 6/1000 | Loss: 0.00002387
Iteration 7/1000 | Loss: 0.00002323
Iteration 8/1000 | Loss: 0.00002282
Iteration 9/1000 | Loss: 0.00002237
Iteration 10/1000 | Loss: 0.00002207
Iteration 11/1000 | Loss: 0.00002186
Iteration 12/1000 | Loss: 0.00002171
Iteration 13/1000 | Loss: 0.00002160
Iteration 14/1000 | Loss: 0.00002159
Iteration 15/1000 | Loss: 0.00002158
Iteration 16/1000 | Loss: 0.00002157
Iteration 17/1000 | Loss: 0.00002150
Iteration 18/1000 | Loss: 0.00002149
Iteration 19/1000 | Loss: 0.00002149
Iteration 20/1000 | Loss: 0.00002148
Iteration 21/1000 | Loss: 0.00002148
Iteration 22/1000 | Loss: 0.00002147
Iteration 23/1000 | Loss: 0.00002147
Iteration 24/1000 | Loss: 0.00002144
Iteration 25/1000 | Loss: 0.00002144
Iteration 26/1000 | Loss: 0.00002143
Iteration 27/1000 | Loss: 0.00002143
Iteration 28/1000 | Loss: 0.00002142
Iteration 29/1000 | Loss: 0.00002142
Iteration 30/1000 | Loss: 0.00002141
Iteration 31/1000 | Loss: 0.00002141
Iteration 32/1000 | Loss: 0.00002140
Iteration 33/1000 | Loss: 0.00002140
Iteration 34/1000 | Loss: 0.00002140
Iteration 35/1000 | Loss: 0.00002139
Iteration 36/1000 | Loss: 0.00002139
Iteration 37/1000 | Loss: 0.00002139
Iteration 38/1000 | Loss: 0.00002139
Iteration 39/1000 | Loss: 0.00002138
Iteration 40/1000 | Loss: 0.00002138
Iteration 41/1000 | Loss: 0.00002138
Iteration 42/1000 | Loss: 0.00002137
Iteration 43/1000 | Loss: 0.00002137
Iteration 44/1000 | Loss: 0.00002137
Iteration 45/1000 | Loss: 0.00002137
Iteration 46/1000 | Loss: 0.00002137
Iteration 47/1000 | Loss: 0.00002136
Iteration 48/1000 | Loss: 0.00002136
Iteration 49/1000 | Loss: 0.00002136
Iteration 50/1000 | Loss: 0.00002135
Iteration 51/1000 | Loss: 0.00002135
Iteration 52/1000 | Loss: 0.00002135
Iteration 53/1000 | Loss: 0.00002134
Iteration 54/1000 | Loss: 0.00002134
Iteration 55/1000 | Loss: 0.00002134
Iteration 56/1000 | Loss: 0.00002134
Iteration 57/1000 | Loss: 0.00002134
Iteration 58/1000 | Loss: 0.00002133
Iteration 59/1000 | Loss: 0.00002133
Iteration 60/1000 | Loss: 0.00002133
Iteration 61/1000 | Loss: 0.00002133
Iteration 62/1000 | Loss: 0.00002132
Iteration 63/1000 | Loss: 0.00002132
Iteration 64/1000 | Loss: 0.00002132
Iteration 65/1000 | Loss: 0.00002132
Iteration 66/1000 | Loss: 0.00002131
Iteration 67/1000 | Loss: 0.00002131
Iteration 68/1000 | Loss: 0.00002131
Iteration 69/1000 | Loss: 0.00002131
Iteration 70/1000 | Loss: 0.00002130
Iteration 71/1000 | Loss: 0.00002130
Iteration 72/1000 | Loss: 0.00002130
Iteration 73/1000 | Loss: 0.00002130
Iteration 74/1000 | Loss: 0.00002129
Iteration 75/1000 | Loss: 0.00002129
Iteration 76/1000 | Loss: 0.00002129
Iteration 77/1000 | Loss: 0.00002128
Iteration 78/1000 | Loss: 0.00002128
Iteration 79/1000 | Loss: 0.00002128
Iteration 80/1000 | Loss: 0.00002128
Iteration 81/1000 | Loss: 0.00002128
Iteration 82/1000 | Loss: 0.00002128
Iteration 83/1000 | Loss: 0.00002128
Iteration 84/1000 | Loss: 0.00002128
Iteration 85/1000 | Loss: 0.00002128
Iteration 86/1000 | Loss: 0.00002127
Iteration 87/1000 | Loss: 0.00002127
Iteration 88/1000 | Loss: 0.00002127
Iteration 89/1000 | Loss: 0.00002127
Iteration 90/1000 | Loss: 0.00002127
Iteration 91/1000 | Loss: 0.00002127
Iteration 92/1000 | Loss: 0.00002127
Iteration 93/1000 | Loss: 0.00002127
Iteration 94/1000 | Loss: 0.00002126
Iteration 95/1000 | Loss: 0.00002126
Iteration 96/1000 | Loss: 0.00002126
Iteration 97/1000 | Loss: 0.00002126
Iteration 98/1000 | Loss: 0.00002126
Iteration 99/1000 | Loss: 0.00002126
Iteration 100/1000 | Loss: 0.00002126
Iteration 101/1000 | Loss: 0.00002126
Iteration 102/1000 | Loss: 0.00002125
Iteration 103/1000 | Loss: 0.00002125
Iteration 104/1000 | Loss: 0.00002125
Iteration 105/1000 | Loss: 0.00002125
Iteration 106/1000 | Loss: 0.00002125
Iteration 107/1000 | Loss: 0.00002124
Iteration 108/1000 | Loss: 0.00002124
Iteration 109/1000 | Loss: 0.00002124
Iteration 110/1000 | Loss: 0.00002124
Iteration 111/1000 | Loss: 0.00002123
Iteration 112/1000 | Loss: 0.00002123
Iteration 113/1000 | Loss: 0.00002123
Iteration 114/1000 | Loss: 0.00002123
Iteration 115/1000 | Loss: 0.00002123
Iteration 116/1000 | Loss: 0.00002123
Iteration 117/1000 | Loss: 0.00002123
Iteration 118/1000 | Loss: 0.00002123
Iteration 119/1000 | Loss: 0.00002123
Iteration 120/1000 | Loss: 0.00002123
Iteration 121/1000 | Loss: 0.00002123
Iteration 122/1000 | Loss: 0.00002123
Iteration 123/1000 | Loss: 0.00002123
Iteration 124/1000 | Loss: 0.00002123
Iteration 125/1000 | Loss: 0.00002123
Iteration 126/1000 | Loss: 0.00002122
Iteration 127/1000 | Loss: 0.00002122
Iteration 128/1000 | Loss: 0.00002122
Iteration 129/1000 | Loss: 0.00002122
Iteration 130/1000 | Loss: 0.00002122
Iteration 131/1000 | Loss: 0.00002122
Iteration 132/1000 | Loss: 0.00002122
Iteration 133/1000 | Loss: 0.00002122
Iteration 134/1000 | Loss: 0.00002122
Iteration 135/1000 | Loss: 0.00002122
Iteration 136/1000 | Loss: 0.00002122
Iteration 137/1000 | Loss: 0.00002122
Iteration 138/1000 | Loss: 0.00002122
Iteration 139/1000 | Loss: 0.00002122
Iteration 140/1000 | Loss: 0.00002122
Iteration 141/1000 | Loss: 0.00002122
Iteration 142/1000 | Loss: 0.00002122
Iteration 143/1000 | Loss: 0.00002122
Iteration 144/1000 | Loss: 0.00002121
Iteration 145/1000 | Loss: 0.00002121
Iteration 146/1000 | Loss: 0.00002121
Iteration 147/1000 | Loss: 0.00002121
Iteration 148/1000 | Loss: 0.00002121
Iteration 149/1000 | Loss: 0.00002121
Iteration 150/1000 | Loss: 0.00002121
Iteration 151/1000 | Loss: 0.00002121
Iteration 152/1000 | Loss: 0.00002121
Iteration 153/1000 | Loss: 0.00002121
Iteration 154/1000 | Loss: 0.00002121
Iteration 155/1000 | Loss: 0.00002121
Iteration 156/1000 | Loss: 0.00002120
Iteration 157/1000 | Loss: 0.00002120
Iteration 158/1000 | Loss: 0.00002120
Iteration 159/1000 | Loss: 0.00002120
Iteration 160/1000 | Loss: 0.00002120
Iteration 161/1000 | Loss: 0.00002120
Iteration 162/1000 | Loss: 0.00002120
Iteration 163/1000 | Loss: 0.00002120
Iteration 164/1000 | Loss: 0.00002120
Iteration 165/1000 | Loss: 0.00002120
Iteration 166/1000 | Loss: 0.00002120
Iteration 167/1000 | Loss: 0.00002120
Iteration 168/1000 | Loss: 0.00002120
Iteration 169/1000 | Loss: 0.00002120
Iteration 170/1000 | Loss: 0.00002120
Iteration 171/1000 | Loss: 0.00002120
Iteration 172/1000 | Loss: 0.00002120
Iteration 173/1000 | Loss: 0.00002120
Iteration 174/1000 | Loss: 0.00002120
Iteration 175/1000 | Loss: 0.00002120
Iteration 176/1000 | Loss: 0.00002120
Iteration 177/1000 | Loss: 0.00002120
Iteration 178/1000 | Loss: 0.00002120
Iteration 179/1000 | Loss: 0.00002120
Iteration 180/1000 | Loss: 0.00002120
Iteration 181/1000 | Loss: 0.00002120
Iteration 182/1000 | Loss: 0.00002120
Iteration 183/1000 | Loss: 0.00002120
Iteration 184/1000 | Loss: 0.00002120
Iteration 185/1000 | Loss: 0.00002120
Iteration 186/1000 | Loss: 0.00002120
Iteration 187/1000 | Loss: 0.00002120
Iteration 188/1000 | Loss: 0.00002120
Iteration 189/1000 | Loss: 0.00002120
Iteration 190/1000 | Loss: 0.00002120
Iteration 191/1000 | Loss: 0.00002120
Iteration 192/1000 | Loss: 0.00002120
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 192. Stopping optimization.
Last 5 losses: [2.120032331731636e-05, 2.120032331731636e-05, 2.120032331731636e-05, 2.120032331731636e-05, 2.120032331731636e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.120032331731636e-05

Optimization complete. Final v2v error: 3.7353200912475586 mm

Highest mean error: 5.997557163238525 mm for frame 136

Lowest mean error: 2.7793993949890137 mm for frame 83

Saving results

Total time: 43.64139151573181
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ben_posed_008/1006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_008/1006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_008/1006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00849972
Iteration 2/25 | Loss: 0.00099605
Iteration 3/25 | Loss: 0.00069871
Iteration 4/25 | Loss: 0.00063751
Iteration 5/25 | Loss: 0.00062551
Iteration 6/25 | Loss: 0.00062315
Iteration 7/25 | Loss: 0.00062291
Iteration 8/25 | Loss: 0.00062291
Iteration 9/25 | Loss: 0.00062291
Iteration 10/25 | Loss: 0.00062291
Iteration 11/25 | Loss: 0.00062291
Iteration 12/25 | Loss: 0.00062291
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0006229127757251263, 0.0006229127757251263, 0.0006229127757251263, 0.0006229127757251263, 0.0006229127757251263]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006229127757251263

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.40115857
Iteration 2/25 | Loss: 0.00018797
Iteration 3/25 | Loss: 0.00018795
Iteration 4/25 | Loss: 0.00018795
Iteration 5/25 | Loss: 0.00018795
Iteration 6/25 | Loss: 0.00018795
Iteration 7/25 | Loss: 0.00018795
Iteration 8/25 | Loss: 0.00018795
Iteration 9/25 | Loss: 0.00018795
Iteration 10/25 | Loss: 0.00018795
Iteration 11/25 | Loss: 0.00018795
Iteration 12/25 | Loss: 0.00018795
Iteration 13/25 | Loss: 0.00018795
Iteration 14/25 | Loss: 0.00018795
Iteration 15/25 | Loss: 0.00018795
Iteration 16/25 | Loss: 0.00018795
Iteration 17/25 | Loss: 0.00018795
Iteration 18/25 | Loss: 0.00018795
Iteration 19/25 | Loss: 0.00018795
Iteration 20/25 | Loss: 0.00018795
Iteration 21/25 | Loss: 0.00018795
Iteration 22/25 | Loss: 0.00018795
Iteration 23/25 | Loss: 0.00018795
Iteration 24/25 | Loss: 0.00018795
Iteration 25/25 | Loss: 0.00018795

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00018795
Iteration 2/1000 | Loss: 0.00002785
Iteration 3/1000 | Loss: 0.00001990
Iteration 4/1000 | Loss: 0.00001760
Iteration 5/1000 | Loss: 0.00001673
Iteration 6/1000 | Loss: 0.00001601
Iteration 7/1000 | Loss: 0.00001561
Iteration 8/1000 | Loss: 0.00001526
Iteration 9/1000 | Loss: 0.00001499
Iteration 10/1000 | Loss: 0.00001480
Iteration 11/1000 | Loss: 0.00001479
Iteration 12/1000 | Loss: 0.00001470
Iteration 13/1000 | Loss: 0.00001462
Iteration 14/1000 | Loss: 0.00001454
Iteration 15/1000 | Loss: 0.00001452
Iteration 16/1000 | Loss: 0.00001451
Iteration 17/1000 | Loss: 0.00001450
Iteration 18/1000 | Loss: 0.00001449
Iteration 19/1000 | Loss: 0.00001449
Iteration 20/1000 | Loss: 0.00001448
Iteration 21/1000 | Loss: 0.00001447
Iteration 22/1000 | Loss: 0.00001447
Iteration 23/1000 | Loss: 0.00001446
Iteration 24/1000 | Loss: 0.00001445
Iteration 25/1000 | Loss: 0.00001445
Iteration 26/1000 | Loss: 0.00001444
Iteration 27/1000 | Loss: 0.00001443
Iteration 28/1000 | Loss: 0.00001443
Iteration 29/1000 | Loss: 0.00001442
Iteration 30/1000 | Loss: 0.00001441
Iteration 31/1000 | Loss: 0.00001440
Iteration 32/1000 | Loss: 0.00001438
Iteration 33/1000 | Loss: 0.00001436
Iteration 34/1000 | Loss: 0.00001435
Iteration 35/1000 | Loss: 0.00001434
Iteration 36/1000 | Loss: 0.00001433
Iteration 37/1000 | Loss: 0.00001433
Iteration 38/1000 | Loss: 0.00001433
Iteration 39/1000 | Loss: 0.00001432
Iteration 40/1000 | Loss: 0.00001431
Iteration 41/1000 | Loss: 0.00001431
Iteration 42/1000 | Loss: 0.00001430
Iteration 43/1000 | Loss: 0.00001430
Iteration 44/1000 | Loss: 0.00001430
Iteration 45/1000 | Loss: 0.00001430
Iteration 46/1000 | Loss: 0.00001430
Iteration 47/1000 | Loss: 0.00001430
Iteration 48/1000 | Loss: 0.00001430
Iteration 49/1000 | Loss: 0.00001430
Iteration 50/1000 | Loss: 0.00001430
Iteration 51/1000 | Loss: 0.00001430
Iteration 52/1000 | Loss: 0.00001430
Iteration 53/1000 | Loss: 0.00001429
Iteration 54/1000 | Loss: 0.00001429
Iteration 55/1000 | Loss: 0.00001428
Iteration 56/1000 | Loss: 0.00001428
Iteration 57/1000 | Loss: 0.00001428
Iteration 58/1000 | Loss: 0.00001428
Iteration 59/1000 | Loss: 0.00001428
Iteration 60/1000 | Loss: 0.00001427
Iteration 61/1000 | Loss: 0.00001427
Iteration 62/1000 | Loss: 0.00001427
Iteration 63/1000 | Loss: 0.00001426
Iteration 64/1000 | Loss: 0.00001426
Iteration 65/1000 | Loss: 0.00001426
Iteration 66/1000 | Loss: 0.00001426
Iteration 67/1000 | Loss: 0.00001426
Iteration 68/1000 | Loss: 0.00001426
Iteration 69/1000 | Loss: 0.00001425
Iteration 70/1000 | Loss: 0.00001425
Iteration 71/1000 | Loss: 0.00001425
Iteration 72/1000 | Loss: 0.00001425
Iteration 73/1000 | Loss: 0.00001425
Iteration 74/1000 | Loss: 0.00001425
Iteration 75/1000 | Loss: 0.00001425
Iteration 76/1000 | Loss: 0.00001425
Iteration 77/1000 | Loss: 0.00001424
Iteration 78/1000 | Loss: 0.00001424
Iteration 79/1000 | Loss: 0.00001424
Iteration 80/1000 | Loss: 0.00001424
Iteration 81/1000 | Loss: 0.00001424
Iteration 82/1000 | Loss: 0.00001424
Iteration 83/1000 | Loss: 0.00001424
Iteration 84/1000 | Loss: 0.00001424
Iteration 85/1000 | Loss: 0.00001424
Iteration 86/1000 | Loss: 0.00001424
Iteration 87/1000 | Loss: 0.00001423
Iteration 88/1000 | Loss: 0.00001423
Iteration 89/1000 | Loss: 0.00001423
Iteration 90/1000 | Loss: 0.00001423
Iteration 91/1000 | Loss: 0.00001423
Iteration 92/1000 | Loss: 0.00001423
Iteration 93/1000 | Loss: 0.00001423
Iteration 94/1000 | Loss: 0.00001423
Iteration 95/1000 | Loss: 0.00001423
Iteration 96/1000 | Loss: 0.00001423
Iteration 97/1000 | Loss: 0.00001423
Iteration 98/1000 | Loss: 0.00001423
Iteration 99/1000 | Loss: 0.00001423
Iteration 100/1000 | Loss: 0.00001422
Iteration 101/1000 | Loss: 0.00001422
Iteration 102/1000 | Loss: 0.00001422
Iteration 103/1000 | Loss: 0.00001422
Iteration 104/1000 | Loss: 0.00001422
Iteration 105/1000 | Loss: 0.00001422
Iteration 106/1000 | Loss: 0.00001422
Iteration 107/1000 | Loss: 0.00001421
Iteration 108/1000 | Loss: 0.00001421
Iteration 109/1000 | Loss: 0.00001421
Iteration 110/1000 | Loss: 0.00001421
Iteration 111/1000 | Loss: 0.00001421
Iteration 112/1000 | Loss: 0.00001421
Iteration 113/1000 | Loss: 0.00001421
Iteration 114/1000 | Loss: 0.00001420
Iteration 115/1000 | Loss: 0.00001420
Iteration 116/1000 | Loss: 0.00001420
Iteration 117/1000 | Loss: 0.00001420
Iteration 118/1000 | Loss: 0.00001420
Iteration 119/1000 | Loss: 0.00001420
Iteration 120/1000 | Loss: 0.00001420
Iteration 121/1000 | Loss: 0.00001420
Iteration 122/1000 | Loss: 0.00001420
Iteration 123/1000 | Loss: 0.00001419
Iteration 124/1000 | Loss: 0.00001419
Iteration 125/1000 | Loss: 0.00001419
Iteration 126/1000 | Loss: 0.00001419
Iteration 127/1000 | Loss: 0.00001419
Iteration 128/1000 | Loss: 0.00001419
Iteration 129/1000 | Loss: 0.00001419
Iteration 130/1000 | Loss: 0.00001418
Iteration 131/1000 | Loss: 0.00001418
Iteration 132/1000 | Loss: 0.00001418
Iteration 133/1000 | Loss: 0.00001418
Iteration 134/1000 | Loss: 0.00001418
Iteration 135/1000 | Loss: 0.00001418
Iteration 136/1000 | Loss: 0.00001418
Iteration 137/1000 | Loss: 0.00001418
Iteration 138/1000 | Loss: 0.00001418
Iteration 139/1000 | Loss: 0.00001418
Iteration 140/1000 | Loss: 0.00001418
Iteration 141/1000 | Loss: 0.00001418
Iteration 142/1000 | Loss: 0.00001418
Iteration 143/1000 | Loss: 0.00001418
Iteration 144/1000 | Loss: 0.00001418
Iteration 145/1000 | Loss: 0.00001418
Iteration 146/1000 | Loss: 0.00001418
Iteration 147/1000 | Loss: 0.00001418
Iteration 148/1000 | Loss: 0.00001418
Iteration 149/1000 | Loss: 0.00001418
Iteration 150/1000 | Loss: 0.00001418
Iteration 151/1000 | Loss: 0.00001418
Iteration 152/1000 | Loss: 0.00001418
Iteration 153/1000 | Loss: 0.00001418
Iteration 154/1000 | Loss: 0.00001418
Iteration 155/1000 | Loss: 0.00001418
Iteration 156/1000 | Loss: 0.00001418
Iteration 157/1000 | Loss: 0.00001418
Iteration 158/1000 | Loss: 0.00001418
Iteration 159/1000 | Loss: 0.00001418
Iteration 160/1000 | Loss: 0.00001418
Iteration 161/1000 | Loss: 0.00001418
Iteration 162/1000 | Loss: 0.00001418
Iteration 163/1000 | Loss: 0.00001418
Iteration 164/1000 | Loss: 0.00001418
Iteration 165/1000 | Loss: 0.00001418
Iteration 166/1000 | Loss: 0.00001418
Iteration 167/1000 | Loss: 0.00001418
Iteration 168/1000 | Loss: 0.00001418
Iteration 169/1000 | Loss: 0.00001418
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 169. Stopping optimization.
Last 5 losses: [1.417732346453704e-05, 1.417732346453704e-05, 1.417732346453704e-05, 1.417732346453704e-05, 1.417732346453704e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.417732346453704e-05

Optimization complete. Final v2v error: 3.2002623081207275 mm

Highest mean error: 4.085710525512695 mm for frame 2

Lowest mean error: 2.9115004539489746 mm for frame 169

Saving results

Total time: 39.09903836250305
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ben_posed_008/1033/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_008/1033.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_008/1033
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00896823
Iteration 2/25 | Loss: 0.00079787
Iteration 3/25 | Loss: 0.00065120
Iteration 4/25 | Loss: 0.00062445
Iteration 5/25 | Loss: 0.00062030
Iteration 6/25 | Loss: 0.00061921
Iteration 7/25 | Loss: 0.00061919
Iteration 8/25 | Loss: 0.00061919
Iteration 9/25 | Loss: 0.00061919
Iteration 10/25 | Loss: 0.00061919
Iteration 11/25 | Loss: 0.00061919
Iteration 12/25 | Loss: 0.00061919
Iteration 13/25 | Loss: 0.00061919
Iteration 14/25 | Loss: 0.00061919
Iteration 15/25 | Loss: 0.00061919
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0006191948777996004, 0.0006191948777996004, 0.0006191948777996004, 0.0006191948777996004, 0.0006191948777996004]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006191948777996004

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 6.56296110
Iteration 2/25 | Loss: 0.00015473
Iteration 3/25 | Loss: 0.00015472
Iteration 4/25 | Loss: 0.00015472
Iteration 5/25 | Loss: 0.00015472
Iteration 6/25 | Loss: 0.00015472
Iteration 7/25 | Loss: 0.00015472
Iteration 8/25 | Loss: 0.00015472
Iteration 9/25 | Loss: 0.00015472
Iteration 10/25 | Loss: 0.00015472
Iteration 11/25 | Loss: 0.00015472
Iteration 12/25 | Loss: 0.00015472
Iteration 13/25 | Loss: 0.00015472
Iteration 14/25 | Loss: 0.00015472
Iteration 15/25 | Loss: 0.00015472
Iteration 16/25 | Loss: 0.00015472
Iteration 17/25 | Loss: 0.00015472
Iteration 18/25 | Loss: 0.00015472
Iteration 19/25 | Loss: 0.00015472
Iteration 20/25 | Loss: 0.00015472
Iteration 21/25 | Loss: 0.00015472
Iteration 22/25 | Loss: 0.00015472
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.00015472169616259634, 0.00015472169616259634, 0.00015472169616259634, 0.00015472169616259634, 0.00015472169616259634]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00015472169616259634

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00015472
Iteration 2/1000 | Loss: 0.00002881
Iteration 3/1000 | Loss: 0.00002185
Iteration 4/1000 | Loss: 0.00001980
Iteration 5/1000 | Loss: 0.00001865
Iteration 6/1000 | Loss: 0.00001800
Iteration 7/1000 | Loss: 0.00001754
Iteration 8/1000 | Loss: 0.00001722
Iteration 9/1000 | Loss: 0.00001713
Iteration 10/1000 | Loss: 0.00001698
Iteration 11/1000 | Loss: 0.00001697
Iteration 12/1000 | Loss: 0.00001697
Iteration 13/1000 | Loss: 0.00001688
Iteration 14/1000 | Loss: 0.00001686
Iteration 15/1000 | Loss: 0.00001685
Iteration 16/1000 | Loss: 0.00001685
Iteration 17/1000 | Loss: 0.00001684
Iteration 18/1000 | Loss: 0.00001682
Iteration 19/1000 | Loss: 0.00001681
Iteration 20/1000 | Loss: 0.00001681
Iteration 21/1000 | Loss: 0.00001681
Iteration 22/1000 | Loss: 0.00001681
Iteration 23/1000 | Loss: 0.00001681
Iteration 24/1000 | Loss: 0.00001680
Iteration 25/1000 | Loss: 0.00001680
Iteration 26/1000 | Loss: 0.00001679
Iteration 27/1000 | Loss: 0.00001678
Iteration 28/1000 | Loss: 0.00001678
Iteration 29/1000 | Loss: 0.00001677
Iteration 30/1000 | Loss: 0.00001677
Iteration 31/1000 | Loss: 0.00001677
Iteration 32/1000 | Loss: 0.00001677
Iteration 33/1000 | Loss: 0.00001677
Iteration 34/1000 | Loss: 0.00001677
Iteration 35/1000 | Loss: 0.00001677
Iteration 36/1000 | Loss: 0.00001677
Iteration 37/1000 | Loss: 0.00001677
Iteration 38/1000 | Loss: 0.00001676
Iteration 39/1000 | Loss: 0.00001675
Iteration 40/1000 | Loss: 0.00001675
Iteration 41/1000 | Loss: 0.00001674
Iteration 42/1000 | Loss: 0.00001674
Iteration 43/1000 | Loss: 0.00001674
Iteration 44/1000 | Loss: 0.00001673
Iteration 45/1000 | Loss: 0.00001673
Iteration 46/1000 | Loss: 0.00001673
Iteration 47/1000 | Loss: 0.00001672
Iteration 48/1000 | Loss: 0.00001672
Iteration 49/1000 | Loss: 0.00001671
Iteration 50/1000 | Loss: 0.00001671
Iteration 51/1000 | Loss: 0.00001671
Iteration 52/1000 | Loss: 0.00001671
Iteration 53/1000 | Loss: 0.00001671
Iteration 54/1000 | Loss: 0.00001671
Iteration 55/1000 | Loss: 0.00001671
Iteration 56/1000 | Loss: 0.00001671
Iteration 57/1000 | Loss: 0.00001670
Iteration 58/1000 | Loss: 0.00001670
Iteration 59/1000 | Loss: 0.00001670
Iteration 60/1000 | Loss: 0.00001670
Iteration 61/1000 | Loss: 0.00001670
Iteration 62/1000 | Loss: 0.00001669
Iteration 63/1000 | Loss: 0.00001669
Iteration 64/1000 | Loss: 0.00001668
Iteration 65/1000 | Loss: 0.00001668
Iteration 66/1000 | Loss: 0.00001668
Iteration 67/1000 | Loss: 0.00001667
Iteration 68/1000 | Loss: 0.00001667
Iteration 69/1000 | Loss: 0.00001667
Iteration 70/1000 | Loss: 0.00001666
Iteration 71/1000 | Loss: 0.00001666
Iteration 72/1000 | Loss: 0.00001666
Iteration 73/1000 | Loss: 0.00001665
Iteration 74/1000 | Loss: 0.00001665
Iteration 75/1000 | Loss: 0.00001665
Iteration 76/1000 | Loss: 0.00001665
Iteration 77/1000 | Loss: 0.00001665
Iteration 78/1000 | Loss: 0.00001664
Iteration 79/1000 | Loss: 0.00001664
Iteration 80/1000 | Loss: 0.00001664
Iteration 81/1000 | Loss: 0.00001663
Iteration 82/1000 | Loss: 0.00001662
Iteration 83/1000 | Loss: 0.00001662
Iteration 84/1000 | Loss: 0.00001662
Iteration 85/1000 | Loss: 0.00001662
Iteration 86/1000 | Loss: 0.00001661
Iteration 87/1000 | Loss: 0.00001661
Iteration 88/1000 | Loss: 0.00001661
Iteration 89/1000 | Loss: 0.00001658
Iteration 90/1000 | Loss: 0.00001657
Iteration 91/1000 | Loss: 0.00001657
Iteration 92/1000 | Loss: 0.00001656
Iteration 93/1000 | Loss: 0.00001656
Iteration 94/1000 | Loss: 0.00001656
Iteration 95/1000 | Loss: 0.00001656
Iteration 96/1000 | Loss: 0.00001655
Iteration 97/1000 | Loss: 0.00001655
Iteration 98/1000 | Loss: 0.00001655
Iteration 99/1000 | Loss: 0.00001655
Iteration 100/1000 | Loss: 0.00001655
Iteration 101/1000 | Loss: 0.00001654
Iteration 102/1000 | Loss: 0.00001654
Iteration 103/1000 | Loss: 0.00001654
Iteration 104/1000 | Loss: 0.00001653
Iteration 105/1000 | Loss: 0.00001653
Iteration 106/1000 | Loss: 0.00001653
Iteration 107/1000 | Loss: 0.00001653
Iteration 108/1000 | Loss: 0.00001653
Iteration 109/1000 | Loss: 0.00001653
Iteration 110/1000 | Loss: 0.00001653
Iteration 111/1000 | Loss: 0.00001652
Iteration 112/1000 | Loss: 0.00001652
Iteration 113/1000 | Loss: 0.00001652
Iteration 114/1000 | Loss: 0.00001652
Iteration 115/1000 | Loss: 0.00001651
Iteration 116/1000 | Loss: 0.00001651
Iteration 117/1000 | Loss: 0.00001651
Iteration 118/1000 | Loss: 0.00001651
Iteration 119/1000 | Loss: 0.00001651
Iteration 120/1000 | Loss: 0.00001651
Iteration 121/1000 | Loss: 0.00001651
Iteration 122/1000 | Loss: 0.00001651
Iteration 123/1000 | Loss: 0.00001650
Iteration 124/1000 | Loss: 0.00001650
Iteration 125/1000 | Loss: 0.00001650
Iteration 126/1000 | Loss: 0.00001649
Iteration 127/1000 | Loss: 0.00001649
Iteration 128/1000 | Loss: 0.00001649
Iteration 129/1000 | Loss: 0.00001649
Iteration 130/1000 | Loss: 0.00001649
Iteration 131/1000 | Loss: 0.00001649
Iteration 132/1000 | Loss: 0.00001649
Iteration 133/1000 | Loss: 0.00001649
Iteration 134/1000 | Loss: 0.00001649
Iteration 135/1000 | Loss: 0.00001649
Iteration 136/1000 | Loss: 0.00001649
Iteration 137/1000 | Loss: 0.00001649
Iteration 138/1000 | Loss: 0.00001649
Iteration 139/1000 | Loss: 0.00001649
Iteration 140/1000 | Loss: 0.00001649
Iteration 141/1000 | Loss: 0.00001649
Iteration 142/1000 | Loss: 0.00001649
Iteration 143/1000 | Loss: 0.00001649
Iteration 144/1000 | Loss: 0.00001649
Iteration 145/1000 | Loss: 0.00001649
Iteration 146/1000 | Loss: 0.00001649
Iteration 147/1000 | Loss: 0.00001649
Iteration 148/1000 | Loss: 0.00001649
Iteration 149/1000 | Loss: 0.00001649
Iteration 150/1000 | Loss: 0.00001649
Iteration 151/1000 | Loss: 0.00001649
Iteration 152/1000 | Loss: 0.00001649
Iteration 153/1000 | Loss: 0.00001649
Iteration 154/1000 | Loss: 0.00001649
Iteration 155/1000 | Loss: 0.00001649
Iteration 156/1000 | Loss: 0.00001649
Iteration 157/1000 | Loss: 0.00001649
Iteration 158/1000 | Loss: 0.00001649
Iteration 159/1000 | Loss: 0.00001649
Iteration 160/1000 | Loss: 0.00001649
Iteration 161/1000 | Loss: 0.00001649
Iteration 162/1000 | Loss: 0.00001649
Iteration 163/1000 | Loss: 0.00001649
Iteration 164/1000 | Loss: 0.00001649
Iteration 165/1000 | Loss: 0.00001649
Iteration 166/1000 | Loss: 0.00001649
Iteration 167/1000 | Loss: 0.00001649
Iteration 168/1000 | Loss: 0.00001649
Iteration 169/1000 | Loss: 0.00001649
Iteration 170/1000 | Loss: 0.00001649
Iteration 171/1000 | Loss: 0.00001649
Iteration 172/1000 | Loss: 0.00001649
Iteration 173/1000 | Loss: 0.00001649
Iteration 174/1000 | Loss: 0.00001649
Iteration 175/1000 | Loss: 0.00001649
Iteration 176/1000 | Loss: 0.00001649
Iteration 177/1000 | Loss: 0.00001649
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 177. Stopping optimization.
Last 5 losses: [1.6493380826432258e-05, 1.6493380826432258e-05, 1.6493380826432258e-05, 1.6493380826432258e-05, 1.6493380826432258e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6493380826432258e-05

Optimization complete. Final v2v error: 3.437249183654785 mm

Highest mean error: 3.924301862716675 mm for frame 180

Lowest mean error: 3.159738779067993 mm for frame 11

Saving results

Total time: 36.868390798568726
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ben_posed_008/1005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_008/1005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_008/1005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00717108
Iteration 2/25 | Loss: 0.00114348
Iteration 3/25 | Loss: 0.00086573
Iteration 4/25 | Loss: 0.00077618
Iteration 5/25 | Loss: 0.00073547
Iteration 6/25 | Loss: 0.00070422
Iteration 7/25 | Loss: 0.00068573
Iteration 8/25 | Loss: 0.00067853
Iteration 9/25 | Loss: 0.00067676
Iteration 10/25 | Loss: 0.00067623
Iteration 11/25 | Loss: 0.00067574
Iteration 12/25 | Loss: 0.00067519
Iteration 13/25 | Loss: 0.00067485
Iteration 14/25 | Loss: 0.00067482
Iteration 15/25 | Loss: 0.00067481
Iteration 16/25 | Loss: 0.00067481
Iteration 17/25 | Loss: 0.00067481
Iteration 18/25 | Loss: 0.00067481
Iteration 19/25 | Loss: 0.00067481
Iteration 20/25 | Loss: 0.00067481
Iteration 21/25 | Loss: 0.00067481
Iteration 22/25 | Loss: 0.00067481
Iteration 23/25 | Loss: 0.00067480
Iteration 24/25 | Loss: 0.00067480
Iteration 25/25 | Loss: 0.00067480

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.47793210
Iteration 2/25 | Loss: 0.00040045
Iteration 3/25 | Loss: 0.00040045
Iteration 4/25 | Loss: 0.00040045
Iteration 5/25 | Loss: 0.00040045
Iteration 6/25 | Loss: 0.00040045
Iteration 7/25 | Loss: 0.00040045
Iteration 8/25 | Loss: 0.00040045
Iteration 9/25 | Loss: 0.00040045
Iteration 10/25 | Loss: 0.00040045
Iteration 11/25 | Loss: 0.00040045
Iteration 12/25 | Loss: 0.00040045
Iteration 13/25 | Loss: 0.00040045
Iteration 14/25 | Loss: 0.00040045
Iteration 15/25 | Loss: 0.00040045
Iteration 16/25 | Loss: 0.00040045
Iteration 17/25 | Loss: 0.00040045
Iteration 18/25 | Loss: 0.00040045
Iteration 19/25 | Loss: 0.00040045
Iteration 20/25 | Loss: 0.00040045
Iteration 21/25 | Loss: 0.00040045
Iteration 22/25 | Loss: 0.00040045
Iteration 23/25 | Loss: 0.00040045
Iteration 24/25 | Loss: 0.00040045
Iteration 25/25 | Loss: 0.00040045

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00040045
Iteration 2/1000 | Loss: 0.00041859
Iteration 3/1000 | Loss: 0.00180124
Iteration 4/1000 | Loss: 0.00024937
Iteration 5/1000 | Loss: 0.00004415
Iteration 6/1000 | Loss: 0.00007243
Iteration 7/1000 | Loss: 0.00035174
Iteration 8/1000 | Loss: 0.00022847
Iteration 9/1000 | Loss: 0.00030166
Iteration 10/1000 | Loss: 0.00022046
Iteration 11/1000 | Loss: 0.00029542
Iteration 12/1000 | Loss: 0.00038061
Iteration 13/1000 | Loss: 0.00048055
Iteration 14/1000 | Loss: 0.00027547
Iteration 15/1000 | Loss: 0.00031717
Iteration 16/1000 | Loss: 0.00021187
Iteration 17/1000 | Loss: 0.00015979
Iteration 18/1000 | Loss: 0.00014833
Iteration 19/1000 | Loss: 0.00039562
Iteration 20/1000 | Loss: 0.00021004
Iteration 21/1000 | Loss: 0.00037974
Iteration 22/1000 | Loss: 0.00013810
Iteration 23/1000 | Loss: 0.00033800
Iteration 24/1000 | Loss: 0.00031502
Iteration 25/1000 | Loss: 0.00049601
Iteration 26/1000 | Loss: 0.00023860
Iteration 27/1000 | Loss: 0.00002657
Iteration 28/1000 | Loss: 0.00002522
Iteration 29/1000 | Loss: 0.00008399
Iteration 30/1000 | Loss: 0.00031446
Iteration 31/1000 | Loss: 0.00016670
Iteration 32/1000 | Loss: 0.00025109
Iteration 33/1000 | Loss: 0.00023308
Iteration 34/1000 | Loss: 0.00030021
Iteration 35/1000 | Loss: 0.00031510
Iteration 36/1000 | Loss: 0.00040637
Iteration 37/1000 | Loss: 0.00024081
Iteration 38/1000 | Loss: 0.00002912
Iteration 39/1000 | Loss: 0.00002656
Iteration 40/1000 | Loss: 0.00005943
Iteration 41/1000 | Loss: 0.00002517
Iteration 42/1000 | Loss: 0.00002412
Iteration 43/1000 | Loss: 0.00002390
Iteration 44/1000 | Loss: 0.00002370
Iteration 45/1000 | Loss: 0.00012724
Iteration 46/1000 | Loss: 0.00037792
Iteration 47/1000 | Loss: 0.00025703
Iteration 48/1000 | Loss: 0.00033195
Iteration 49/1000 | Loss: 0.00003860
Iteration 50/1000 | Loss: 0.00002948
Iteration 51/1000 | Loss: 0.00002503
Iteration 52/1000 | Loss: 0.00002327
Iteration 53/1000 | Loss: 0.00002220
Iteration 54/1000 | Loss: 0.00002153
Iteration 55/1000 | Loss: 0.00002083
Iteration 56/1000 | Loss: 0.00002057
Iteration 57/1000 | Loss: 0.00002050
Iteration 58/1000 | Loss: 0.00002047
Iteration 59/1000 | Loss: 0.00002045
Iteration 60/1000 | Loss: 0.00002043
Iteration 61/1000 | Loss: 0.00002043
Iteration 62/1000 | Loss: 0.00002042
Iteration 63/1000 | Loss: 0.00002038
Iteration 64/1000 | Loss: 0.00002037
Iteration 65/1000 | Loss: 0.00002035
Iteration 66/1000 | Loss: 0.00002035
Iteration 67/1000 | Loss: 0.00002034
Iteration 68/1000 | Loss: 0.00002031
Iteration 69/1000 | Loss: 0.00002031
Iteration 70/1000 | Loss: 0.00002031
Iteration 71/1000 | Loss: 0.00002031
Iteration 72/1000 | Loss: 0.00002030
Iteration 73/1000 | Loss: 0.00002030
Iteration 74/1000 | Loss: 0.00002029
Iteration 75/1000 | Loss: 0.00002029
Iteration 76/1000 | Loss: 0.00002029
Iteration 77/1000 | Loss: 0.00002029
Iteration 78/1000 | Loss: 0.00002028
Iteration 79/1000 | Loss: 0.00002028
Iteration 80/1000 | Loss: 0.00002027
Iteration 81/1000 | Loss: 0.00002027
Iteration 82/1000 | Loss: 0.00002027
Iteration 83/1000 | Loss: 0.00002026
Iteration 84/1000 | Loss: 0.00002026
Iteration 85/1000 | Loss: 0.00002025
Iteration 86/1000 | Loss: 0.00002025
Iteration 87/1000 | Loss: 0.00002024
Iteration 88/1000 | Loss: 0.00002024
Iteration 89/1000 | Loss: 0.00002024
Iteration 90/1000 | Loss: 0.00002023
Iteration 91/1000 | Loss: 0.00002023
Iteration 92/1000 | Loss: 0.00002022
Iteration 93/1000 | Loss: 0.00002020
Iteration 94/1000 | Loss: 0.00002020
Iteration 95/1000 | Loss: 0.00002020
Iteration 96/1000 | Loss: 0.00002019
Iteration 97/1000 | Loss: 0.00002019
Iteration 98/1000 | Loss: 0.00002019
Iteration 99/1000 | Loss: 0.00002018
Iteration 100/1000 | Loss: 0.00002018
Iteration 101/1000 | Loss: 0.00002018
Iteration 102/1000 | Loss: 0.00002017
Iteration 103/1000 | Loss: 0.00002017
Iteration 104/1000 | Loss: 0.00002017
Iteration 105/1000 | Loss: 0.00002016
Iteration 106/1000 | Loss: 0.00002016
Iteration 107/1000 | Loss: 0.00002016
Iteration 108/1000 | Loss: 0.00002016
Iteration 109/1000 | Loss: 0.00002016
Iteration 110/1000 | Loss: 0.00002016
Iteration 111/1000 | Loss: 0.00002015
Iteration 112/1000 | Loss: 0.00002015
Iteration 113/1000 | Loss: 0.00002015
Iteration 114/1000 | Loss: 0.00002015
Iteration 115/1000 | Loss: 0.00002015
Iteration 116/1000 | Loss: 0.00002014
Iteration 117/1000 | Loss: 0.00002014
Iteration 118/1000 | Loss: 0.00002014
Iteration 119/1000 | Loss: 0.00002014
Iteration 120/1000 | Loss: 0.00002014
Iteration 121/1000 | Loss: 0.00002014
Iteration 122/1000 | Loss: 0.00002014
Iteration 123/1000 | Loss: 0.00002014
Iteration 124/1000 | Loss: 0.00002014
Iteration 125/1000 | Loss: 0.00002013
Iteration 126/1000 | Loss: 0.00002013
Iteration 127/1000 | Loss: 0.00002013
Iteration 128/1000 | Loss: 0.00002013
Iteration 129/1000 | Loss: 0.00002012
Iteration 130/1000 | Loss: 0.00002012
Iteration 131/1000 | Loss: 0.00002012
Iteration 132/1000 | Loss: 0.00002012
Iteration 133/1000 | Loss: 0.00002012
Iteration 134/1000 | Loss: 0.00002012
Iteration 135/1000 | Loss: 0.00002012
Iteration 136/1000 | Loss: 0.00002011
Iteration 137/1000 | Loss: 0.00002011
Iteration 138/1000 | Loss: 0.00002011
Iteration 139/1000 | Loss: 0.00002011
Iteration 140/1000 | Loss: 0.00002011
Iteration 141/1000 | Loss: 0.00002011
Iteration 142/1000 | Loss: 0.00002010
Iteration 143/1000 | Loss: 0.00002010
Iteration 144/1000 | Loss: 0.00002010
Iteration 145/1000 | Loss: 0.00002010
Iteration 146/1000 | Loss: 0.00002010
Iteration 147/1000 | Loss: 0.00002010
Iteration 148/1000 | Loss: 0.00002010
Iteration 149/1000 | Loss: 0.00002010
Iteration 150/1000 | Loss: 0.00002010
Iteration 151/1000 | Loss: 0.00002010
Iteration 152/1000 | Loss: 0.00002010
Iteration 153/1000 | Loss: 0.00002010
Iteration 154/1000 | Loss: 0.00002010
Iteration 155/1000 | Loss: 0.00002010
Iteration 156/1000 | Loss: 0.00002010
Iteration 157/1000 | Loss: 0.00002010
Iteration 158/1000 | Loss: 0.00002010
Iteration 159/1000 | Loss: 0.00002010
Iteration 160/1000 | Loss: 0.00002010
Iteration 161/1000 | Loss: 0.00002010
Iteration 162/1000 | Loss: 0.00002010
Iteration 163/1000 | Loss: 0.00002010
Iteration 164/1000 | Loss: 0.00002010
Iteration 165/1000 | Loss: 0.00002010
Iteration 166/1000 | Loss: 0.00002010
Iteration 167/1000 | Loss: 0.00002010
Iteration 168/1000 | Loss: 0.00002010
Iteration 169/1000 | Loss: 0.00002010
Iteration 170/1000 | Loss: 0.00002010
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 170. Stopping optimization.
Last 5 losses: [2.0096695152460597e-05, 2.0096695152460597e-05, 2.0096695152460597e-05, 2.0096695152460597e-05, 2.0096695152460597e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0096695152460597e-05

Optimization complete. Final v2v error: 3.749910593032837 mm

Highest mean error: 5.145566940307617 mm for frame 133

Lowest mean error: 3.282064437866211 mm for frame 207

Saving results

Total time: 126.64520454406738
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ben_posed_008/1043/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_008/1043.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_008/1043
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00404249
Iteration 2/25 | Loss: 0.00079503
Iteration 3/25 | Loss: 0.00061115
Iteration 4/25 | Loss: 0.00058795
Iteration 5/25 | Loss: 0.00058216
Iteration 6/25 | Loss: 0.00058125
Iteration 7/25 | Loss: 0.00058099
Iteration 8/25 | Loss: 0.00058099
Iteration 9/25 | Loss: 0.00058099
Iteration 10/25 | Loss: 0.00058099
Iteration 11/25 | Loss: 0.00058099
Iteration 12/25 | Loss: 0.00058099
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0005809940630570054, 0.0005809940630570054, 0.0005809940630570054, 0.0005809940630570054, 0.0005809940630570054]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005809940630570054

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.81298590
Iteration 2/25 | Loss: 0.00017099
Iteration 3/25 | Loss: 0.00017098
Iteration 4/25 | Loss: 0.00017098
Iteration 5/25 | Loss: 0.00017098
Iteration 6/25 | Loss: 0.00017098
Iteration 7/25 | Loss: 0.00017098
Iteration 8/25 | Loss: 0.00017098
Iteration 9/25 | Loss: 0.00017098
Iteration 10/25 | Loss: 0.00017098
Iteration 11/25 | Loss: 0.00017097
Iteration 12/25 | Loss: 0.00017097
Iteration 13/25 | Loss: 0.00017097
Iteration 14/25 | Loss: 0.00017097
Iteration 15/25 | Loss: 0.00017097
Iteration 16/25 | Loss: 0.00017097
Iteration 17/25 | Loss: 0.00017097
Iteration 18/25 | Loss: 0.00017097
Iteration 19/25 | Loss: 0.00017097
Iteration 20/25 | Loss: 0.00017097
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.00017097461386583745, 0.00017097461386583745, 0.00017097461386583745, 0.00017097461386583745, 0.00017097461386583745]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00017097461386583745

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00017097
Iteration 2/1000 | Loss: 0.00002336
Iteration 3/1000 | Loss: 0.00001628
Iteration 4/1000 | Loss: 0.00001525
Iteration 5/1000 | Loss: 0.00001447
Iteration 6/1000 | Loss: 0.00001406
Iteration 7/1000 | Loss: 0.00001377
Iteration 8/1000 | Loss: 0.00001357
Iteration 9/1000 | Loss: 0.00001353
Iteration 10/1000 | Loss: 0.00001348
Iteration 11/1000 | Loss: 0.00001346
Iteration 12/1000 | Loss: 0.00001345
Iteration 13/1000 | Loss: 0.00001344
Iteration 14/1000 | Loss: 0.00001329
Iteration 15/1000 | Loss: 0.00001328
Iteration 16/1000 | Loss: 0.00001327
Iteration 17/1000 | Loss: 0.00001323
Iteration 18/1000 | Loss: 0.00001323
Iteration 19/1000 | Loss: 0.00001321
Iteration 20/1000 | Loss: 0.00001318
Iteration 21/1000 | Loss: 0.00001318
Iteration 22/1000 | Loss: 0.00001317
Iteration 23/1000 | Loss: 0.00001315
Iteration 24/1000 | Loss: 0.00001313
Iteration 25/1000 | Loss: 0.00001313
Iteration 26/1000 | Loss: 0.00001311
Iteration 27/1000 | Loss: 0.00001311
Iteration 28/1000 | Loss: 0.00001310
Iteration 29/1000 | Loss: 0.00001307
Iteration 30/1000 | Loss: 0.00001307
Iteration 31/1000 | Loss: 0.00001307
Iteration 32/1000 | Loss: 0.00001307
Iteration 33/1000 | Loss: 0.00001306
Iteration 34/1000 | Loss: 0.00001306
Iteration 35/1000 | Loss: 0.00001306
Iteration 36/1000 | Loss: 0.00001305
Iteration 37/1000 | Loss: 0.00001304
Iteration 38/1000 | Loss: 0.00001304
Iteration 39/1000 | Loss: 0.00001303
Iteration 40/1000 | Loss: 0.00001303
Iteration 41/1000 | Loss: 0.00001303
Iteration 42/1000 | Loss: 0.00001302
Iteration 43/1000 | Loss: 0.00001302
Iteration 44/1000 | Loss: 0.00001302
Iteration 45/1000 | Loss: 0.00001302
Iteration 46/1000 | Loss: 0.00001301
Iteration 47/1000 | Loss: 0.00001301
Iteration 48/1000 | Loss: 0.00001301
Iteration 49/1000 | Loss: 0.00001301
Iteration 50/1000 | Loss: 0.00001301
Iteration 51/1000 | Loss: 0.00001301
Iteration 52/1000 | Loss: 0.00001300
Iteration 53/1000 | Loss: 0.00001300
Iteration 54/1000 | Loss: 0.00001300
Iteration 55/1000 | Loss: 0.00001300
Iteration 56/1000 | Loss: 0.00001300
Iteration 57/1000 | Loss: 0.00001299
Iteration 58/1000 | Loss: 0.00001299
Iteration 59/1000 | Loss: 0.00001299
Iteration 60/1000 | Loss: 0.00001298
Iteration 61/1000 | Loss: 0.00001298
Iteration 62/1000 | Loss: 0.00001298
Iteration 63/1000 | Loss: 0.00001298
Iteration 64/1000 | Loss: 0.00001298
Iteration 65/1000 | Loss: 0.00001298
Iteration 66/1000 | Loss: 0.00001298
Iteration 67/1000 | Loss: 0.00001297
Iteration 68/1000 | Loss: 0.00001297
Iteration 69/1000 | Loss: 0.00001297
Iteration 70/1000 | Loss: 0.00001297
Iteration 71/1000 | Loss: 0.00001297
Iteration 72/1000 | Loss: 0.00001297
Iteration 73/1000 | Loss: 0.00001296
Iteration 74/1000 | Loss: 0.00001296
Iteration 75/1000 | Loss: 0.00001295
Iteration 76/1000 | Loss: 0.00001295
Iteration 77/1000 | Loss: 0.00001295
Iteration 78/1000 | Loss: 0.00001294
Iteration 79/1000 | Loss: 0.00001294
Iteration 80/1000 | Loss: 0.00001294
Iteration 81/1000 | Loss: 0.00001294
Iteration 82/1000 | Loss: 0.00001294
Iteration 83/1000 | Loss: 0.00001294
Iteration 84/1000 | Loss: 0.00001294
Iteration 85/1000 | Loss: 0.00001293
Iteration 86/1000 | Loss: 0.00001293
Iteration 87/1000 | Loss: 0.00001293
Iteration 88/1000 | Loss: 0.00001292
Iteration 89/1000 | Loss: 0.00001292
Iteration 90/1000 | Loss: 0.00001292
Iteration 91/1000 | Loss: 0.00001292
Iteration 92/1000 | Loss: 0.00001292
Iteration 93/1000 | Loss: 0.00001292
Iteration 94/1000 | Loss: 0.00001291
Iteration 95/1000 | Loss: 0.00001291
Iteration 96/1000 | Loss: 0.00001291
Iteration 97/1000 | Loss: 0.00001291
Iteration 98/1000 | Loss: 0.00001291
Iteration 99/1000 | Loss: 0.00001291
Iteration 100/1000 | Loss: 0.00001291
Iteration 101/1000 | Loss: 0.00001290
Iteration 102/1000 | Loss: 0.00001290
Iteration 103/1000 | Loss: 0.00001289
Iteration 104/1000 | Loss: 0.00001289
Iteration 105/1000 | Loss: 0.00001289
Iteration 106/1000 | Loss: 0.00001289
Iteration 107/1000 | Loss: 0.00001289
Iteration 108/1000 | Loss: 0.00001289
Iteration 109/1000 | Loss: 0.00001289
Iteration 110/1000 | Loss: 0.00001289
Iteration 111/1000 | Loss: 0.00001289
Iteration 112/1000 | Loss: 0.00001289
Iteration 113/1000 | Loss: 0.00001289
Iteration 114/1000 | Loss: 0.00001289
Iteration 115/1000 | Loss: 0.00001288
Iteration 116/1000 | Loss: 0.00001288
Iteration 117/1000 | Loss: 0.00001288
Iteration 118/1000 | Loss: 0.00001288
Iteration 119/1000 | Loss: 0.00001288
Iteration 120/1000 | Loss: 0.00001288
Iteration 121/1000 | Loss: 0.00001288
Iteration 122/1000 | Loss: 0.00001288
Iteration 123/1000 | Loss: 0.00001288
Iteration 124/1000 | Loss: 0.00001288
Iteration 125/1000 | Loss: 0.00001288
Iteration 126/1000 | Loss: 0.00001287
Iteration 127/1000 | Loss: 0.00001287
Iteration 128/1000 | Loss: 0.00001287
Iteration 129/1000 | Loss: 0.00001287
Iteration 130/1000 | Loss: 0.00001287
Iteration 131/1000 | Loss: 0.00001287
Iteration 132/1000 | Loss: 0.00001287
Iteration 133/1000 | Loss: 0.00001287
Iteration 134/1000 | Loss: 0.00001287
Iteration 135/1000 | Loss: 0.00001287
Iteration 136/1000 | Loss: 0.00001287
Iteration 137/1000 | Loss: 0.00001287
Iteration 138/1000 | Loss: 0.00001287
Iteration 139/1000 | Loss: 0.00001287
Iteration 140/1000 | Loss: 0.00001287
Iteration 141/1000 | Loss: 0.00001287
Iteration 142/1000 | Loss: 0.00001287
Iteration 143/1000 | Loss: 0.00001287
Iteration 144/1000 | Loss: 0.00001287
Iteration 145/1000 | Loss: 0.00001287
Iteration 146/1000 | Loss: 0.00001287
Iteration 147/1000 | Loss: 0.00001287
Iteration 148/1000 | Loss: 0.00001287
Iteration 149/1000 | Loss: 0.00001286
Iteration 150/1000 | Loss: 0.00001286
Iteration 151/1000 | Loss: 0.00001286
Iteration 152/1000 | Loss: 0.00001286
Iteration 153/1000 | Loss: 0.00001286
Iteration 154/1000 | Loss: 0.00001286
Iteration 155/1000 | Loss: 0.00001286
Iteration 156/1000 | Loss: 0.00001286
Iteration 157/1000 | Loss: 0.00001286
Iteration 158/1000 | Loss: 0.00001285
Iteration 159/1000 | Loss: 0.00001285
Iteration 160/1000 | Loss: 0.00001285
Iteration 161/1000 | Loss: 0.00001285
Iteration 162/1000 | Loss: 0.00001285
Iteration 163/1000 | Loss: 0.00001285
Iteration 164/1000 | Loss: 0.00001285
Iteration 165/1000 | Loss: 0.00001285
Iteration 166/1000 | Loss: 0.00001285
Iteration 167/1000 | Loss: 0.00001285
Iteration 168/1000 | Loss: 0.00001285
Iteration 169/1000 | Loss: 0.00001285
Iteration 170/1000 | Loss: 0.00001285
Iteration 171/1000 | Loss: 0.00001285
Iteration 172/1000 | Loss: 0.00001285
Iteration 173/1000 | Loss: 0.00001285
Iteration 174/1000 | Loss: 0.00001285
Iteration 175/1000 | Loss: 0.00001285
Iteration 176/1000 | Loss: 0.00001285
Iteration 177/1000 | Loss: 0.00001285
Iteration 178/1000 | Loss: 0.00001285
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 178. Stopping optimization.
Last 5 losses: [1.2853553926106542e-05, 1.2853553926106542e-05, 1.2853553926106542e-05, 1.2853553926106542e-05, 1.2853553926106542e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2853553926106542e-05

Optimization complete. Final v2v error: 3.07143235206604 mm

Highest mean error: 3.551051378250122 mm for frame 70

Lowest mean error: 2.787736415863037 mm for frame 42

Saving results

Total time: 37.431851625442505
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ben_posed_008/1026/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_008/1026.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_008/1026
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01041536
Iteration 2/25 | Loss: 0.00135140
Iteration 3/25 | Loss: 0.00089259
Iteration 4/25 | Loss: 0.00084680
Iteration 5/25 | Loss: 0.00079636
Iteration 6/25 | Loss: 0.00079288
Iteration 7/25 | Loss: 0.00079125
Iteration 8/25 | Loss: 0.00078987
Iteration 9/25 | Loss: 0.00078753
Iteration 10/25 | Loss: 0.00078894
Iteration 11/25 | Loss: 0.00078916
Iteration 12/25 | Loss: 0.00079004
Iteration 13/25 | Loss: 0.00079034
Iteration 14/25 | Loss: 0.00078940
Iteration 15/25 | Loss: 0.00078906
Iteration 16/25 | Loss: 0.00078858
Iteration 17/25 | Loss: 0.00079008
Iteration 18/25 | Loss: 0.00079067
Iteration 19/25 | Loss: 0.00079000
Iteration 20/25 | Loss: 0.00078826
Iteration 21/25 | Loss: 0.00078947
Iteration 22/25 | Loss: 0.00079034
Iteration 23/25 | Loss: 0.00079011
Iteration 24/25 | Loss: 0.00078937
Iteration 25/25 | Loss: 0.00079048

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.09325242
Iteration 2/25 | Loss: 0.00029790
Iteration 3/25 | Loss: 0.00029788
Iteration 4/25 | Loss: 0.00029788
Iteration 5/25 | Loss: 0.00029788
Iteration 6/25 | Loss: 0.00029788
Iteration 7/25 | Loss: 0.00029788
Iteration 8/25 | Loss: 0.00029788
Iteration 9/25 | Loss: 0.00029788
Iteration 10/25 | Loss: 0.00029788
Iteration 11/25 | Loss: 0.00029788
Iteration 12/25 | Loss: 0.00029788
Iteration 13/25 | Loss: 0.00029788
Iteration 14/25 | Loss: 0.00029788
Iteration 15/25 | Loss: 0.00029788
Iteration 16/25 | Loss: 0.00029788
Iteration 17/25 | Loss: 0.00029788
Iteration 18/25 | Loss: 0.00029788
Iteration 19/25 | Loss: 0.00029788
Iteration 20/25 | Loss: 0.00029788
Iteration 21/25 | Loss: 0.00029788
Iteration 22/25 | Loss: 0.00029788
Iteration 23/25 | Loss: 0.00029788
Iteration 24/25 | Loss: 0.00029788
Iteration 25/25 | Loss: 0.00029788

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00029788
Iteration 2/1000 | Loss: 0.00010693
Iteration 3/1000 | Loss: 0.00015438
Iteration 4/1000 | Loss: 0.00009123
Iteration 5/1000 | Loss: 0.00017320
Iteration 6/1000 | Loss: 0.00012587
Iteration 7/1000 | Loss: 0.00016251
Iteration 8/1000 | Loss: 0.00014400
Iteration 9/1000 | Loss: 0.00024758
Iteration 10/1000 | Loss: 0.00014078
Iteration 11/1000 | Loss: 0.00024510
Iteration 12/1000 | Loss: 0.00004442
Iteration 13/1000 | Loss: 0.00003376
Iteration 14/1000 | Loss: 0.00003167
Iteration 15/1000 | Loss: 0.00003074
Iteration 16/1000 | Loss: 0.00002992
Iteration 17/1000 | Loss: 0.00002941
Iteration 18/1000 | Loss: 0.00002903
Iteration 19/1000 | Loss: 0.00002871
Iteration 20/1000 | Loss: 0.00002840
Iteration 21/1000 | Loss: 0.00002822
Iteration 22/1000 | Loss: 0.00002812
Iteration 23/1000 | Loss: 0.00002811
Iteration 24/1000 | Loss: 0.00002805
Iteration 25/1000 | Loss: 0.00002805
Iteration 26/1000 | Loss: 0.00002801
Iteration 27/1000 | Loss: 0.00002799
Iteration 28/1000 | Loss: 0.00002793
Iteration 29/1000 | Loss: 0.00002791
Iteration 30/1000 | Loss: 0.00002790
Iteration 31/1000 | Loss: 0.00002789
Iteration 32/1000 | Loss: 0.00002789
Iteration 33/1000 | Loss: 0.00002789
Iteration 34/1000 | Loss: 0.00002789
Iteration 35/1000 | Loss: 0.00002789
Iteration 36/1000 | Loss: 0.00002789
Iteration 37/1000 | Loss: 0.00002789
Iteration 38/1000 | Loss: 0.00002788
Iteration 39/1000 | Loss: 0.00002786
Iteration 40/1000 | Loss: 0.00002786
Iteration 41/1000 | Loss: 0.00002782
Iteration 42/1000 | Loss: 0.00002780
Iteration 43/1000 | Loss: 0.00002779
Iteration 44/1000 | Loss: 0.00002779
Iteration 45/1000 | Loss: 0.00002778
Iteration 46/1000 | Loss: 0.00002778
Iteration 47/1000 | Loss: 0.00002777
Iteration 48/1000 | Loss: 0.00002777
Iteration 49/1000 | Loss: 0.00002777
Iteration 50/1000 | Loss: 0.00002776
Iteration 51/1000 | Loss: 0.00002767
Iteration 52/1000 | Loss: 0.00002767
Iteration 53/1000 | Loss: 0.00002766
Iteration 54/1000 | Loss: 0.00002766
Iteration 55/1000 | Loss: 0.00002766
Iteration 56/1000 | Loss: 0.00002766
Iteration 57/1000 | Loss: 0.00002766
Iteration 58/1000 | Loss: 0.00002765
Iteration 59/1000 | Loss: 0.00002765
Iteration 60/1000 | Loss: 0.00002765
Iteration 61/1000 | Loss: 0.00002765
Iteration 62/1000 | Loss: 0.00002765
Iteration 63/1000 | Loss: 0.00002764
Iteration 64/1000 | Loss: 0.00002764
Iteration 65/1000 | Loss: 0.00002764
Iteration 66/1000 | Loss: 0.00002764
Iteration 67/1000 | Loss: 0.00002764
Iteration 68/1000 | Loss: 0.00002764
Iteration 69/1000 | Loss: 0.00002763
Iteration 70/1000 | Loss: 0.00002763
Iteration 71/1000 | Loss: 0.00002763
Iteration 72/1000 | Loss: 0.00002763
Iteration 73/1000 | Loss: 0.00002763
Iteration 74/1000 | Loss: 0.00002763
Iteration 75/1000 | Loss: 0.00002763
Iteration 76/1000 | Loss: 0.00002762
Iteration 77/1000 | Loss: 0.00002762
Iteration 78/1000 | Loss: 0.00002761
Iteration 79/1000 | Loss: 0.00002761
Iteration 80/1000 | Loss: 0.00002761
Iteration 81/1000 | Loss: 0.00002761
Iteration 82/1000 | Loss: 0.00002761
Iteration 83/1000 | Loss: 0.00002760
Iteration 84/1000 | Loss: 0.00002760
Iteration 85/1000 | Loss: 0.00002760
Iteration 86/1000 | Loss: 0.00002760
Iteration 87/1000 | Loss: 0.00002760
Iteration 88/1000 | Loss: 0.00002760
Iteration 89/1000 | Loss: 0.00002760
Iteration 90/1000 | Loss: 0.00002759
Iteration 91/1000 | Loss: 0.00002759
Iteration 92/1000 | Loss: 0.00002759
Iteration 93/1000 | Loss: 0.00002759
Iteration 94/1000 | Loss: 0.00002759
Iteration 95/1000 | Loss: 0.00002759
Iteration 96/1000 | Loss: 0.00002758
Iteration 97/1000 | Loss: 0.00002758
Iteration 98/1000 | Loss: 0.00002758
Iteration 99/1000 | Loss: 0.00002758
Iteration 100/1000 | Loss: 0.00002758
Iteration 101/1000 | Loss: 0.00002758
Iteration 102/1000 | Loss: 0.00002758
Iteration 103/1000 | Loss: 0.00002758
Iteration 104/1000 | Loss: 0.00002758
Iteration 105/1000 | Loss: 0.00002758
Iteration 106/1000 | Loss: 0.00002758
Iteration 107/1000 | Loss: 0.00002757
Iteration 108/1000 | Loss: 0.00002757
Iteration 109/1000 | Loss: 0.00002757
Iteration 110/1000 | Loss: 0.00002757
Iteration 111/1000 | Loss: 0.00002757
Iteration 112/1000 | Loss: 0.00002757
Iteration 113/1000 | Loss: 0.00002757
Iteration 114/1000 | Loss: 0.00002756
Iteration 115/1000 | Loss: 0.00002756
Iteration 116/1000 | Loss: 0.00002756
Iteration 117/1000 | Loss: 0.00002756
Iteration 118/1000 | Loss: 0.00002756
Iteration 119/1000 | Loss: 0.00002756
Iteration 120/1000 | Loss: 0.00002756
Iteration 121/1000 | Loss: 0.00002756
Iteration 122/1000 | Loss: 0.00002756
Iteration 123/1000 | Loss: 0.00002756
Iteration 124/1000 | Loss: 0.00002755
Iteration 125/1000 | Loss: 0.00002755
Iteration 126/1000 | Loss: 0.00002755
Iteration 127/1000 | Loss: 0.00002755
Iteration 128/1000 | Loss: 0.00002754
Iteration 129/1000 | Loss: 0.00002754
Iteration 130/1000 | Loss: 0.00002754
Iteration 131/1000 | Loss: 0.00002754
Iteration 132/1000 | Loss: 0.00002754
Iteration 133/1000 | Loss: 0.00002754
Iteration 134/1000 | Loss: 0.00002754
Iteration 135/1000 | Loss: 0.00002754
Iteration 136/1000 | Loss: 0.00002754
Iteration 137/1000 | Loss: 0.00002754
Iteration 138/1000 | Loss: 0.00002754
Iteration 139/1000 | Loss: 0.00002754
Iteration 140/1000 | Loss: 0.00002754
Iteration 141/1000 | Loss: 0.00002754
Iteration 142/1000 | Loss: 0.00002754
Iteration 143/1000 | Loss: 0.00002754
Iteration 144/1000 | Loss: 0.00002754
Iteration 145/1000 | Loss: 0.00002754
Iteration 146/1000 | Loss: 0.00002754
Iteration 147/1000 | Loss: 0.00002754
Iteration 148/1000 | Loss: 0.00002754
Iteration 149/1000 | Loss: 0.00002754
Iteration 150/1000 | Loss: 0.00002754
Iteration 151/1000 | Loss: 0.00002754
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 151. Stopping optimization.
Last 5 losses: [2.753665467025712e-05, 2.753665467025712e-05, 2.753665467025712e-05, 2.753665467025712e-05, 2.753665467025712e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.753665467025712e-05

Optimization complete. Final v2v error: 4.190366268157959 mm

Highest mean error: 4.9927897453308105 mm for frame 125

Lowest mean error: 3.684417724609375 mm for frame 103

Saving results

Total time: 94.46017694473267
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ben_posed_008/1063/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_008/1063.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_008/1063
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00890015
Iteration 2/25 | Loss: 0.00077174
Iteration 3/25 | Loss: 0.00059985
Iteration 4/25 | Loss: 0.00057504
Iteration 5/25 | Loss: 0.00056630
Iteration 6/25 | Loss: 0.00056482
Iteration 7/25 | Loss: 0.00056471
Iteration 8/25 | Loss: 0.00056471
Iteration 9/25 | Loss: 0.00056471
Iteration 10/25 | Loss: 0.00056471
Iteration 11/25 | Loss: 0.00056471
Iteration 12/25 | Loss: 0.00056471
Iteration 13/25 | Loss: 0.00056471
Iteration 14/25 | Loss: 0.00056471
Iteration 15/25 | Loss: 0.00056471
Iteration 16/25 | Loss: 0.00056471
Iteration 17/25 | Loss: 0.00056471
Iteration 18/25 | Loss: 0.00056471
Iteration 19/25 | Loss: 0.00056471
Iteration 20/25 | Loss: 0.00056471
Iteration 21/25 | Loss: 0.00056471
Iteration 22/25 | Loss: 0.00056471
Iteration 23/25 | Loss: 0.00056471
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0005647146026603878, 0.0005647146026603878, 0.0005647146026603878, 0.0005647146026603878, 0.0005647146026603878]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005647146026603878

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.81946182
Iteration 2/25 | Loss: 0.00017350
Iteration 3/25 | Loss: 0.00017350
Iteration 4/25 | Loss: 0.00017350
Iteration 5/25 | Loss: 0.00017350
Iteration 6/25 | Loss: 0.00017350
Iteration 7/25 | Loss: 0.00017350
Iteration 8/25 | Loss: 0.00017350
Iteration 9/25 | Loss: 0.00017350
Iteration 10/25 | Loss: 0.00017350
Iteration 11/25 | Loss: 0.00017350
Iteration 12/25 | Loss: 0.00017350
Iteration 13/25 | Loss: 0.00017350
Iteration 14/25 | Loss: 0.00017350
Iteration 15/25 | Loss: 0.00017350
Iteration 16/25 | Loss: 0.00017350
Iteration 17/25 | Loss: 0.00017350
Iteration 18/25 | Loss: 0.00017350
Iteration 19/25 | Loss: 0.00017350
Iteration 20/25 | Loss: 0.00017350
Iteration 21/25 | Loss: 0.00017350
Iteration 22/25 | Loss: 0.00017350
Iteration 23/25 | Loss: 0.00017350
Iteration 24/25 | Loss: 0.00017350
Iteration 25/25 | Loss: 0.00017350
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.00017349518020637333, 0.00017349518020637333, 0.00017349518020637333, 0.00017349518020637333, 0.00017349518020637333]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00017349518020637333

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00017350
Iteration 2/1000 | Loss: 0.00001630
Iteration 3/1000 | Loss: 0.00001348
Iteration 4/1000 | Loss: 0.00001271
Iteration 5/1000 | Loss: 0.00001222
Iteration 6/1000 | Loss: 0.00001186
Iteration 7/1000 | Loss: 0.00001184
Iteration 8/1000 | Loss: 0.00001166
Iteration 9/1000 | Loss: 0.00001164
Iteration 10/1000 | Loss: 0.00001163
Iteration 11/1000 | Loss: 0.00001160
Iteration 12/1000 | Loss: 0.00001160
Iteration 13/1000 | Loss: 0.00001158
Iteration 14/1000 | Loss: 0.00001157
Iteration 15/1000 | Loss: 0.00001152
Iteration 16/1000 | Loss: 0.00001145
Iteration 17/1000 | Loss: 0.00001144
Iteration 18/1000 | Loss: 0.00001144
Iteration 19/1000 | Loss: 0.00001143
Iteration 20/1000 | Loss: 0.00001142
Iteration 21/1000 | Loss: 0.00001142
Iteration 22/1000 | Loss: 0.00001139
Iteration 23/1000 | Loss: 0.00001138
Iteration 24/1000 | Loss: 0.00001137
Iteration 25/1000 | Loss: 0.00001137
Iteration 26/1000 | Loss: 0.00001136
Iteration 27/1000 | Loss: 0.00001136
Iteration 28/1000 | Loss: 0.00001135
Iteration 29/1000 | Loss: 0.00001134
Iteration 30/1000 | Loss: 0.00001133
Iteration 31/1000 | Loss: 0.00001133
Iteration 32/1000 | Loss: 0.00001133
Iteration 33/1000 | Loss: 0.00001133
Iteration 34/1000 | Loss: 0.00001132
Iteration 35/1000 | Loss: 0.00001132
Iteration 36/1000 | Loss: 0.00001132
Iteration 37/1000 | Loss: 0.00001131
Iteration 38/1000 | Loss: 0.00001131
Iteration 39/1000 | Loss: 0.00001130
Iteration 40/1000 | Loss: 0.00001128
Iteration 41/1000 | Loss: 0.00001128
Iteration 42/1000 | Loss: 0.00001128
Iteration 43/1000 | Loss: 0.00001127
Iteration 44/1000 | Loss: 0.00001127
Iteration 45/1000 | Loss: 0.00001125
Iteration 46/1000 | Loss: 0.00001125
Iteration 47/1000 | Loss: 0.00001124
Iteration 48/1000 | Loss: 0.00001124
Iteration 49/1000 | Loss: 0.00001124
Iteration 50/1000 | Loss: 0.00001124
Iteration 51/1000 | Loss: 0.00001123
Iteration 52/1000 | Loss: 0.00001123
Iteration 53/1000 | Loss: 0.00001122
Iteration 54/1000 | Loss: 0.00001121
Iteration 55/1000 | Loss: 0.00001121
Iteration 56/1000 | Loss: 0.00001121
Iteration 57/1000 | Loss: 0.00001121
Iteration 58/1000 | Loss: 0.00001121
Iteration 59/1000 | Loss: 0.00001121
Iteration 60/1000 | Loss: 0.00001121
Iteration 61/1000 | Loss: 0.00001121
Iteration 62/1000 | Loss: 0.00001121
Iteration 63/1000 | Loss: 0.00001121
Iteration 64/1000 | Loss: 0.00001121
Iteration 65/1000 | Loss: 0.00001121
Iteration 66/1000 | Loss: 0.00001121
Iteration 67/1000 | Loss: 0.00001121
Iteration 68/1000 | Loss: 0.00001121
Iteration 69/1000 | Loss: 0.00001121
Iteration 70/1000 | Loss: 0.00001121
Iteration 71/1000 | Loss: 0.00001120
Iteration 72/1000 | Loss: 0.00001119
Iteration 73/1000 | Loss: 0.00001119
Iteration 74/1000 | Loss: 0.00001118
Iteration 75/1000 | Loss: 0.00001118
Iteration 76/1000 | Loss: 0.00001118
Iteration 77/1000 | Loss: 0.00001118
Iteration 78/1000 | Loss: 0.00001117
Iteration 79/1000 | Loss: 0.00001117
Iteration 80/1000 | Loss: 0.00001117
Iteration 81/1000 | Loss: 0.00001115
Iteration 82/1000 | Loss: 0.00001115
Iteration 83/1000 | Loss: 0.00001115
Iteration 84/1000 | Loss: 0.00001114
Iteration 85/1000 | Loss: 0.00001114
Iteration 86/1000 | Loss: 0.00001114
Iteration 87/1000 | Loss: 0.00001114
Iteration 88/1000 | Loss: 0.00001114
Iteration 89/1000 | Loss: 0.00001113
Iteration 90/1000 | Loss: 0.00001113
Iteration 91/1000 | Loss: 0.00001112
Iteration 92/1000 | Loss: 0.00001112
Iteration 93/1000 | Loss: 0.00001112
Iteration 94/1000 | Loss: 0.00001112
Iteration 95/1000 | Loss: 0.00001112
Iteration 96/1000 | Loss: 0.00001112
Iteration 97/1000 | Loss: 0.00001112
Iteration 98/1000 | Loss: 0.00001112
Iteration 99/1000 | Loss: 0.00001111
Iteration 100/1000 | Loss: 0.00001111
Iteration 101/1000 | Loss: 0.00001111
Iteration 102/1000 | Loss: 0.00001111
Iteration 103/1000 | Loss: 0.00001111
Iteration 104/1000 | Loss: 0.00001111
Iteration 105/1000 | Loss: 0.00001111
Iteration 106/1000 | Loss: 0.00001111
Iteration 107/1000 | Loss: 0.00001111
Iteration 108/1000 | Loss: 0.00001111
Iteration 109/1000 | Loss: 0.00001110
Iteration 110/1000 | Loss: 0.00001110
Iteration 111/1000 | Loss: 0.00001110
Iteration 112/1000 | Loss: 0.00001110
Iteration 113/1000 | Loss: 0.00001110
Iteration 114/1000 | Loss: 0.00001110
Iteration 115/1000 | Loss: 0.00001109
Iteration 116/1000 | Loss: 0.00001109
Iteration 117/1000 | Loss: 0.00001109
Iteration 118/1000 | Loss: 0.00001109
Iteration 119/1000 | Loss: 0.00001109
Iteration 120/1000 | Loss: 0.00001109
Iteration 121/1000 | Loss: 0.00001109
Iteration 122/1000 | Loss: 0.00001109
Iteration 123/1000 | Loss: 0.00001109
Iteration 124/1000 | Loss: 0.00001108
Iteration 125/1000 | Loss: 0.00001108
Iteration 126/1000 | Loss: 0.00001108
Iteration 127/1000 | Loss: 0.00001108
Iteration 128/1000 | Loss: 0.00001108
Iteration 129/1000 | Loss: 0.00001108
Iteration 130/1000 | Loss: 0.00001108
Iteration 131/1000 | Loss: 0.00001108
Iteration 132/1000 | Loss: 0.00001108
Iteration 133/1000 | Loss: 0.00001107
Iteration 134/1000 | Loss: 0.00001107
Iteration 135/1000 | Loss: 0.00001107
Iteration 136/1000 | Loss: 0.00001107
Iteration 137/1000 | Loss: 0.00001107
Iteration 138/1000 | Loss: 0.00001107
Iteration 139/1000 | Loss: 0.00001107
Iteration 140/1000 | Loss: 0.00001107
Iteration 141/1000 | Loss: 0.00001107
Iteration 142/1000 | Loss: 0.00001107
Iteration 143/1000 | Loss: 0.00001107
Iteration 144/1000 | Loss: 0.00001107
Iteration 145/1000 | Loss: 0.00001107
Iteration 146/1000 | Loss: 0.00001107
Iteration 147/1000 | Loss: 0.00001107
Iteration 148/1000 | Loss: 0.00001107
Iteration 149/1000 | Loss: 0.00001107
Iteration 150/1000 | Loss: 0.00001107
Iteration 151/1000 | Loss: 0.00001107
Iteration 152/1000 | Loss: 0.00001107
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 152. Stopping optimization.
Last 5 losses: [1.1065570106438827e-05, 1.1065570106438827e-05, 1.1065570106438827e-05, 1.1065570106438827e-05, 1.1065570106438827e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1065570106438827e-05

Optimization complete. Final v2v error: 2.803881883621216 mm

Highest mean error: 3.2696473598480225 mm for frame 157

Lowest mean error: 2.620604991912842 mm for frame 217

Saving results

Total time: 37.02420926094055
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ben_posed_008/1095/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_008/1095.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_008/1095
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01021649
Iteration 2/25 | Loss: 0.00158468
Iteration 3/25 | Loss: 0.00096796
Iteration 4/25 | Loss: 0.00091875
Iteration 5/25 | Loss: 0.00090285
Iteration 6/25 | Loss: 0.00089922
Iteration 7/25 | Loss: 0.00089898
Iteration 8/25 | Loss: 0.00089898
Iteration 9/25 | Loss: 0.00089898
Iteration 10/25 | Loss: 0.00089898
Iteration 11/25 | Loss: 0.00089898
Iteration 12/25 | Loss: 0.00089898
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0008989829802885652, 0.0008989829802885652, 0.0008989829802885652, 0.0008989829802885652, 0.0008989829802885652]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008989829802885652

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.59545857
Iteration 2/25 | Loss: 0.00022221
Iteration 3/25 | Loss: 0.00022221
Iteration 4/25 | Loss: 0.00022221
Iteration 5/25 | Loss: 0.00022221
Iteration 6/25 | Loss: 0.00022221
Iteration 7/25 | Loss: 0.00022221
Iteration 8/25 | Loss: 0.00022221
Iteration 9/25 | Loss: 0.00022221
Iteration 10/25 | Loss: 0.00022221
Iteration 11/25 | Loss: 0.00022221
Iteration 12/25 | Loss: 0.00022221
Iteration 13/25 | Loss: 0.00022221
Iteration 14/25 | Loss: 0.00022221
Iteration 15/25 | Loss: 0.00022221
Iteration 16/25 | Loss: 0.00022221
Iteration 17/25 | Loss: 0.00022221
Iteration 18/25 | Loss: 0.00022221
Iteration 19/25 | Loss: 0.00022221
Iteration 20/25 | Loss: 0.00022221
Iteration 21/25 | Loss: 0.00022221
Iteration 22/25 | Loss: 0.00022221
Iteration 23/25 | Loss: 0.00022221
Iteration 24/25 | Loss: 0.00022221
Iteration 25/25 | Loss: 0.00022221

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00022221
Iteration 2/1000 | Loss: 0.00007155
Iteration 3/1000 | Loss: 0.00005107
Iteration 4/1000 | Loss: 0.00004768
Iteration 5/1000 | Loss: 0.00004583
Iteration 6/1000 | Loss: 0.00004461
Iteration 7/1000 | Loss: 0.00004363
Iteration 8/1000 | Loss: 0.00004293
Iteration 9/1000 | Loss: 0.00004246
Iteration 10/1000 | Loss: 0.00004210
Iteration 11/1000 | Loss: 0.00004174
Iteration 12/1000 | Loss: 0.00004144
Iteration 13/1000 | Loss: 0.00004117
Iteration 14/1000 | Loss: 0.00004103
Iteration 15/1000 | Loss: 0.00004085
Iteration 16/1000 | Loss: 0.00004072
Iteration 17/1000 | Loss: 0.00004063
Iteration 18/1000 | Loss: 0.00004062
Iteration 19/1000 | Loss: 0.00004062
Iteration 20/1000 | Loss: 0.00004062
Iteration 21/1000 | Loss: 0.00004059
Iteration 22/1000 | Loss: 0.00004059
Iteration 23/1000 | Loss: 0.00004058
Iteration 24/1000 | Loss: 0.00004058
Iteration 25/1000 | Loss: 0.00004058
Iteration 26/1000 | Loss: 0.00004058
Iteration 27/1000 | Loss: 0.00004057
Iteration 28/1000 | Loss: 0.00004057
Iteration 29/1000 | Loss: 0.00004049
Iteration 30/1000 | Loss: 0.00004048
Iteration 31/1000 | Loss: 0.00004041
Iteration 32/1000 | Loss: 0.00004039
Iteration 33/1000 | Loss: 0.00004039
Iteration 34/1000 | Loss: 0.00004038
Iteration 35/1000 | Loss: 0.00004038
Iteration 36/1000 | Loss: 0.00004038
Iteration 37/1000 | Loss: 0.00004038
Iteration 38/1000 | Loss: 0.00004037
Iteration 39/1000 | Loss: 0.00004037
Iteration 40/1000 | Loss: 0.00004036
Iteration 41/1000 | Loss: 0.00004036
Iteration 42/1000 | Loss: 0.00004036
Iteration 43/1000 | Loss: 0.00004035
Iteration 44/1000 | Loss: 0.00004035
Iteration 45/1000 | Loss: 0.00004035
Iteration 46/1000 | Loss: 0.00004035
Iteration 47/1000 | Loss: 0.00004035
Iteration 48/1000 | Loss: 0.00004035
Iteration 49/1000 | Loss: 0.00004035
Iteration 50/1000 | Loss: 0.00004035
Iteration 51/1000 | Loss: 0.00004034
Iteration 52/1000 | Loss: 0.00004032
Iteration 53/1000 | Loss: 0.00004031
Iteration 54/1000 | Loss: 0.00004031
Iteration 55/1000 | Loss: 0.00004029
Iteration 56/1000 | Loss: 0.00004029
Iteration 57/1000 | Loss: 0.00004028
Iteration 58/1000 | Loss: 0.00004028
Iteration 59/1000 | Loss: 0.00004028
Iteration 60/1000 | Loss: 0.00004028
Iteration 61/1000 | Loss: 0.00004028
Iteration 62/1000 | Loss: 0.00004028
Iteration 63/1000 | Loss: 0.00004028
Iteration 64/1000 | Loss: 0.00004028
Iteration 65/1000 | Loss: 0.00004028
Iteration 66/1000 | Loss: 0.00004028
Iteration 67/1000 | Loss: 0.00004028
Iteration 68/1000 | Loss: 0.00004028
Iteration 69/1000 | Loss: 0.00004028
Iteration 70/1000 | Loss: 0.00004027
Iteration 71/1000 | Loss: 0.00004027
Iteration 72/1000 | Loss: 0.00004027
Iteration 73/1000 | Loss: 0.00004027
Iteration 74/1000 | Loss: 0.00004027
Iteration 75/1000 | Loss: 0.00004027
Iteration 76/1000 | Loss: 0.00004027
Iteration 77/1000 | Loss: 0.00004027
Iteration 78/1000 | Loss: 0.00004027
Iteration 79/1000 | Loss: 0.00004026
Iteration 80/1000 | Loss: 0.00004026
Iteration 81/1000 | Loss: 0.00004026
Iteration 82/1000 | Loss: 0.00004026
Iteration 83/1000 | Loss: 0.00004026
Iteration 84/1000 | Loss: 0.00004026
Iteration 85/1000 | Loss: 0.00004025
Iteration 86/1000 | Loss: 0.00004025
Iteration 87/1000 | Loss: 0.00004025
Iteration 88/1000 | Loss: 0.00004025
Iteration 89/1000 | Loss: 0.00004025
Iteration 90/1000 | Loss: 0.00004025
Iteration 91/1000 | Loss: 0.00004025
Iteration 92/1000 | Loss: 0.00004024
Iteration 93/1000 | Loss: 0.00004024
Iteration 94/1000 | Loss: 0.00004024
Iteration 95/1000 | Loss: 0.00004024
Iteration 96/1000 | Loss: 0.00004024
Iteration 97/1000 | Loss: 0.00004024
Iteration 98/1000 | Loss: 0.00004023
Iteration 99/1000 | Loss: 0.00004023
Iteration 100/1000 | Loss: 0.00004023
Iteration 101/1000 | Loss: 0.00004023
Iteration 102/1000 | Loss: 0.00004023
Iteration 103/1000 | Loss: 0.00004023
Iteration 104/1000 | Loss: 0.00004023
Iteration 105/1000 | Loss: 0.00004023
Iteration 106/1000 | Loss: 0.00004023
Iteration 107/1000 | Loss: 0.00004023
Iteration 108/1000 | Loss: 0.00004023
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 108. Stopping optimization.
Last 5 losses: [4.023402652819641e-05, 4.023402652819641e-05, 4.023402652819641e-05, 4.023402652819641e-05, 4.023402652819641e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.023402652819641e-05

Optimization complete. Final v2v error: 5.20504903793335 mm

Highest mean error: 6.144737243652344 mm for frame 14

Lowest mean error: 4.764042854309082 mm for frame 44

Saving results

Total time: 44.695735454559326
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ben_posed_008/1088/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_008/1088.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_008/1088
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01072234
Iteration 2/25 | Loss: 0.00239410
Iteration 3/25 | Loss: 0.00145996
Iteration 4/25 | Loss: 0.00138268
Iteration 5/25 | Loss: 0.00134438
Iteration 6/25 | Loss: 0.00118637
Iteration 7/25 | Loss: 0.00106900
Iteration 8/25 | Loss: 0.00108277
Iteration 9/25 | Loss: 0.00097337
Iteration 10/25 | Loss: 0.00080053
Iteration 11/25 | Loss: 0.00074736
Iteration 12/25 | Loss: 0.00072860
Iteration 13/25 | Loss: 0.00070686
Iteration 14/25 | Loss: 0.00070161
Iteration 15/25 | Loss: 0.00068071
Iteration 16/25 | Loss: 0.00067182
Iteration 17/25 | Loss: 0.00065361
Iteration 18/25 | Loss: 0.00065482
Iteration 19/25 | Loss: 0.00065052
Iteration 20/25 | Loss: 0.00064132
Iteration 21/25 | Loss: 0.00065263
Iteration 22/25 | Loss: 0.00064997
Iteration 23/25 | Loss: 0.00064871
Iteration 24/25 | Loss: 0.00065172
Iteration 25/25 | Loss: 0.00065309

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46870208
Iteration 2/25 | Loss: 0.00072123
Iteration 3/25 | Loss: 0.00072122
Iteration 4/25 | Loss: 0.00072122
Iteration 5/25 | Loss: 0.00072122
Iteration 6/25 | Loss: 0.00072122
Iteration 7/25 | Loss: 0.00072122
Iteration 8/25 | Loss: 0.00072122
Iteration 9/25 | Loss: 0.00072122
Iteration 10/25 | Loss: 0.00072122
Iteration 11/25 | Loss: 0.00072122
Iteration 12/25 | Loss: 0.00072122
Iteration 13/25 | Loss: 0.00072122
Iteration 14/25 | Loss: 0.00072122
Iteration 15/25 | Loss: 0.00072122
Iteration 16/25 | Loss: 0.00072122
Iteration 17/25 | Loss: 0.00072122
Iteration 18/25 | Loss: 0.00072122
Iteration 19/25 | Loss: 0.00072122
Iteration 20/25 | Loss: 0.00072122
Iteration 21/25 | Loss: 0.00072122
Iteration 22/25 | Loss: 0.00072122
Iteration 23/25 | Loss: 0.00072122
Iteration 24/25 | Loss: 0.00072122
Iteration 25/25 | Loss: 0.00072122

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00072122
Iteration 2/1000 | Loss: 0.00062483
Iteration 3/1000 | Loss: 0.00033589
Iteration 4/1000 | Loss: 0.00039085
Iteration 5/1000 | Loss: 0.00043004
Iteration 6/1000 | Loss: 0.00074680
Iteration 7/1000 | Loss: 0.00045632
Iteration 8/1000 | Loss: 0.00027419
Iteration 9/1000 | Loss: 0.00021635
Iteration 10/1000 | Loss: 0.00035865
Iteration 11/1000 | Loss: 0.00048321
Iteration 12/1000 | Loss: 0.00051013
Iteration 13/1000 | Loss: 0.00035693
Iteration 14/1000 | Loss: 0.00051011
Iteration 15/1000 | Loss: 0.00056410
Iteration 16/1000 | Loss: 0.00037877
Iteration 17/1000 | Loss: 0.00037909
Iteration 18/1000 | Loss: 0.00066651
Iteration 19/1000 | Loss: 0.00022903
Iteration 20/1000 | Loss: 0.00034950
Iteration 21/1000 | Loss: 0.00130136
Iteration 22/1000 | Loss: 0.00074098
Iteration 23/1000 | Loss: 0.00061190
Iteration 24/1000 | Loss: 0.00034668
Iteration 25/1000 | Loss: 0.00040468
Iteration 26/1000 | Loss: 0.00027050
Iteration 27/1000 | Loss: 0.00025578
Iteration 28/1000 | Loss: 0.00012604
Iteration 29/1000 | Loss: 0.00023697
Iteration 30/1000 | Loss: 0.00080693
Iteration 31/1000 | Loss: 0.00053857
Iteration 32/1000 | Loss: 0.00059213
Iteration 33/1000 | Loss: 0.00038316
Iteration 34/1000 | Loss: 0.00009910
Iteration 35/1000 | Loss: 0.00019823
Iteration 36/1000 | Loss: 0.00050440
Iteration 37/1000 | Loss: 0.00021686
Iteration 38/1000 | Loss: 0.00037345
Iteration 39/1000 | Loss: 0.00012441
Iteration 40/1000 | Loss: 0.00033206
Iteration 41/1000 | Loss: 0.00112202
Iteration 42/1000 | Loss: 0.00088646
Iteration 43/1000 | Loss: 0.00033338
Iteration 44/1000 | Loss: 0.00035827
Iteration 45/1000 | Loss: 0.00117774
Iteration 46/1000 | Loss: 0.00100423
Iteration 47/1000 | Loss: 0.00125701
Iteration 48/1000 | Loss: 0.00044106
Iteration 49/1000 | Loss: 0.00016402
Iteration 50/1000 | Loss: 0.00020097
Iteration 51/1000 | Loss: 0.00036456
Iteration 52/1000 | Loss: 0.00020580
Iteration 53/1000 | Loss: 0.00027046
Iteration 54/1000 | Loss: 0.00022822
Iteration 55/1000 | Loss: 0.00023487
Iteration 56/1000 | Loss: 0.00042226
Iteration 57/1000 | Loss: 0.00018597
Iteration 58/1000 | Loss: 0.00084470
Iteration 59/1000 | Loss: 0.00012868
Iteration 60/1000 | Loss: 0.00003111
Iteration 61/1000 | Loss: 0.00031934
Iteration 62/1000 | Loss: 0.00040372
Iteration 63/1000 | Loss: 0.00026415
Iteration 64/1000 | Loss: 0.00022233
Iteration 65/1000 | Loss: 0.00022162
Iteration 66/1000 | Loss: 0.00015025
Iteration 67/1000 | Loss: 0.00015318
Iteration 68/1000 | Loss: 0.00013440
Iteration 69/1000 | Loss: 0.00015290
Iteration 70/1000 | Loss: 0.00036952
Iteration 71/1000 | Loss: 0.00048681
Iteration 72/1000 | Loss: 0.00037912
Iteration 73/1000 | Loss: 0.00033913
Iteration 74/1000 | Loss: 0.00005056
Iteration 75/1000 | Loss: 0.00014783
Iteration 76/1000 | Loss: 0.00009695
Iteration 77/1000 | Loss: 0.00046739
Iteration 78/1000 | Loss: 0.00016442
Iteration 79/1000 | Loss: 0.00025787
Iteration 80/1000 | Loss: 0.00003266
Iteration 81/1000 | Loss: 0.00002821
Iteration 82/1000 | Loss: 0.00002418
Iteration 83/1000 | Loss: 0.00002220
Iteration 84/1000 | Loss: 0.00002117
Iteration 85/1000 | Loss: 0.00002006
Iteration 86/1000 | Loss: 0.00001916
Iteration 87/1000 | Loss: 0.00002284
Iteration 88/1000 | Loss: 0.00001851
Iteration 89/1000 | Loss: 0.00001771
Iteration 90/1000 | Loss: 0.00001736
Iteration 91/1000 | Loss: 0.00001700
Iteration 92/1000 | Loss: 0.00001674
Iteration 93/1000 | Loss: 0.00001652
Iteration 94/1000 | Loss: 0.00001632
Iteration 95/1000 | Loss: 0.00001621
Iteration 96/1000 | Loss: 0.00001620
Iteration 97/1000 | Loss: 0.00001620
Iteration 98/1000 | Loss: 0.00001617
Iteration 99/1000 | Loss: 0.00001617
Iteration 100/1000 | Loss: 0.00001616
Iteration 101/1000 | Loss: 0.00001616
Iteration 102/1000 | Loss: 0.00001614
Iteration 103/1000 | Loss: 0.00001614
Iteration 104/1000 | Loss: 0.00001612
Iteration 105/1000 | Loss: 0.00001612
Iteration 106/1000 | Loss: 0.00001612
Iteration 107/1000 | Loss: 0.00001612
Iteration 108/1000 | Loss: 0.00001611
Iteration 109/1000 | Loss: 0.00001611
Iteration 110/1000 | Loss: 0.00001611
Iteration 111/1000 | Loss: 0.00001610
Iteration 112/1000 | Loss: 0.00001610
Iteration 113/1000 | Loss: 0.00001610
Iteration 114/1000 | Loss: 0.00001610
Iteration 115/1000 | Loss: 0.00001610
Iteration 116/1000 | Loss: 0.00001609
Iteration 117/1000 | Loss: 0.00001609
Iteration 118/1000 | Loss: 0.00001609
Iteration 119/1000 | Loss: 0.00001609
Iteration 120/1000 | Loss: 0.00001609
Iteration 121/1000 | Loss: 0.00001609
Iteration 122/1000 | Loss: 0.00001609
Iteration 123/1000 | Loss: 0.00001608
Iteration 124/1000 | Loss: 0.00001608
Iteration 125/1000 | Loss: 0.00001608
Iteration 126/1000 | Loss: 0.00001608
Iteration 127/1000 | Loss: 0.00001608
Iteration 128/1000 | Loss: 0.00001608
Iteration 129/1000 | Loss: 0.00001608
Iteration 130/1000 | Loss: 0.00001608
Iteration 131/1000 | Loss: 0.00001608
Iteration 132/1000 | Loss: 0.00001608
Iteration 133/1000 | Loss: 0.00001608
Iteration 134/1000 | Loss: 0.00001608
Iteration 135/1000 | Loss: 0.00001608
Iteration 136/1000 | Loss: 0.00001608
Iteration 137/1000 | Loss: 0.00001607
Iteration 138/1000 | Loss: 0.00001607
Iteration 139/1000 | Loss: 0.00001607
Iteration 140/1000 | Loss: 0.00001607
Iteration 141/1000 | Loss: 0.00001607
Iteration 142/1000 | Loss: 0.00001606
Iteration 143/1000 | Loss: 0.00001606
Iteration 144/1000 | Loss: 0.00001606
Iteration 145/1000 | Loss: 0.00001606
Iteration 146/1000 | Loss: 0.00001606
Iteration 147/1000 | Loss: 0.00001605
Iteration 148/1000 | Loss: 0.00001605
Iteration 149/1000 | Loss: 0.00001604
Iteration 150/1000 | Loss: 0.00001604
Iteration 151/1000 | Loss: 0.00001604
Iteration 152/1000 | Loss: 0.00001603
Iteration 153/1000 | Loss: 0.00001603
Iteration 154/1000 | Loss: 0.00001603
Iteration 155/1000 | Loss: 0.00001603
Iteration 156/1000 | Loss: 0.00001603
Iteration 157/1000 | Loss: 0.00001603
Iteration 158/1000 | Loss: 0.00001602
Iteration 159/1000 | Loss: 0.00001602
Iteration 160/1000 | Loss: 0.00001602
Iteration 161/1000 | Loss: 0.00001602
Iteration 162/1000 | Loss: 0.00001602
Iteration 163/1000 | Loss: 0.00001602
Iteration 164/1000 | Loss: 0.00001602
Iteration 165/1000 | Loss: 0.00001602
Iteration 166/1000 | Loss: 0.00001601
Iteration 167/1000 | Loss: 0.00001601
Iteration 168/1000 | Loss: 0.00001601
Iteration 169/1000 | Loss: 0.00001601
Iteration 170/1000 | Loss: 0.00001601
Iteration 171/1000 | Loss: 0.00001601
Iteration 172/1000 | Loss: 0.00001601
Iteration 173/1000 | Loss: 0.00001601
Iteration 174/1000 | Loss: 0.00001601
Iteration 175/1000 | Loss: 0.00001601
Iteration 176/1000 | Loss: 0.00001601
Iteration 177/1000 | Loss: 0.00001600
Iteration 178/1000 | Loss: 0.00001600
Iteration 179/1000 | Loss: 0.00001600
Iteration 180/1000 | Loss: 0.00001599
Iteration 181/1000 | Loss: 0.00001599
Iteration 182/1000 | Loss: 0.00001599
Iteration 183/1000 | Loss: 0.00001599
Iteration 184/1000 | Loss: 0.00001598
Iteration 185/1000 | Loss: 0.00001598
Iteration 186/1000 | Loss: 0.00001598
Iteration 187/1000 | Loss: 0.00001598
Iteration 188/1000 | Loss: 0.00001598
Iteration 189/1000 | Loss: 0.00001598
Iteration 190/1000 | Loss: 0.00001598
Iteration 191/1000 | Loss: 0.00001598
Iteration 192/1000 | Loss: 0.00001598
Iteration 193/1000 | Loss: 0.00001598
Iteration 194/1000 | Loss: 0.00001598
Iteration 195/1000 | Loss: 0.00001598
Iteration 196/1000 | Loss: 0.00001598
Iteration 197/1000 | Loss: 0.00001598
Iteration 198/1000 | Loss: 0.00001597
Iteration 199/1000 | Loss: 0.00001597
Iteration 200/1000 | Loss: 0.00001597
Iteration 201/1000 | Loss: 0.00001597
Iteration 202/1000 | Loss: 0.00001597
Iteration 203/1000 | Loss: 0.00001597
Iteration 204/1000 | Loss: 0.00001597
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 204. Stopping optimization.
Last 5 losses: [1.597421032784041e-05, 1.597421032784041e-05, 1.597421032784041e-05, 1.597421032784041e-05, 1.597421032784041e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.597421032784041e-05

Optimization complete. Final v2v error: 3.3328158855438232 mm

Highest mean error: 4.944741249084473 mm for frame 76

Lowest mean error: 2.8671200275421143 mm for frame 110

Saving results

Total time: 188.89108657836914
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ben_posed_008/1096/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_008/1096.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_008/1096
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00486154
Iteration 2/25 | Loss: 0.00084762
Iteration 3/25 | Loss: 0.00066311
Iteration 4/25 | Loss: 0.00063851
Iteration 5/25 | Loss: 0.00062944
Iteration 6/25 | Loss: 0.00062758
Iteration 7/25 | Loss: 0.00062748
Iteration 8/25 | Loss: 0.00062748
Iteration 9/25 | Loss: 0.00062748
Iteration 10/25 | Loss: 0.00062748
Iteration 11/25 | Loss: 0.00062748
Iteration 12/25 | Loss: 0.00062748
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0006274828338064253, 0.0006274828338064253, 0.0006274828338064253, 0.0006274828338064253, 0.0006274828338064253]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006274828338064253

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45839846
Iteration 2/25 | Loss: 0.00021360
Iteration 3/25 | Loss: 0.00021358
Iteration 4/25 | Loss: 0.00021358
Iteration 5/25 | Loss: 0.00021358
Iteration 6/25 | Loss: 0.00021358
Iteration 7/25 | Loss: 0.00021358
Iteration 8/25 | Loss: 0.00021358
Iteration 9/25 | Loss: 0.00021358
Iteration 10/25 | Loss: 0.00021358
Iteration 11/25 | Loss: 0.00021358
Iteration 12/25 | Loss: 0.00021358
Iteration 13/25 | Loss: 0.00021358
Iteration 14/25 | Loss: 0.00021358
Iteration 15/25 | Loss: 0.00021358
Iteration 16/25 | Loss: 0.00021358
Iteration 17/25 | Loss: 0.00021358
Iteration 18/25 | Loss: 0.00021358
Iteration 19/25 | Loss: 0.00021358
Iteration 20/25 | Loss: 0.00021358
Iteration 21/25 | Loss: 0.00021358
Iteration 22/25 | Loss: 0.00021358
Iteration 23/25 | Loss: 0.00021358
Iteration 24/25 | Loss: 0.00021358
Iteration 25/25 | Loss: 0.00021358

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00021358
Iteration 2/1000 | Loss: 0.00002586
Iteration 3/1000 | Loss: 0.00001831
Iteration 4/1000 | Loss: 0.00001693
Iteration 5/1000 | Loss: 0.00001620
Iteration 6/1000 | Loss: 0.00001582
Iteration 7/1000 | Loss: 0.00001550
Iteration 8/1000 | Loss: 0.00001529
Iteration 9/1000 | Loss: 0.00001522
Iteration 10/1000 | Loss: 0.00001518
Iteration 11/1000 | Loss: 0.00001511
Iteration 12/1000 | Loss: 0.00001506
Iteration 13/1000 | Loss: 0.00001503
Iteration 14/1000 | Loss: 0.00001499
Iteration 15/1000 | Loss: 0.00001499
Iteration 16/1000 | Loss: 0.00001499
Iteration 17/1000 | Loss: 0.00001499
Iteration 18/1000 | Loss: 0.00001499
Iteration 19/1000 | Loss: 0.00001499
Iteration 20/1000 | Loss: 0.00001499
Iteration 21/1000 | Loss: 0.00001499
Iteration 22/1000 | Loss: 0.00001498
Iteration 23/1000 | Loss: 0.00001498
Iteration 24/1000 | Loss: 0.00001498
Iteration 25/1000 | Loss: 0.00001498
Iteration 26/1000 | Loss: 0.00001497
Iteration 27/1000 | Loss: 0.00001495
Iteration 28/1000 | Loss: 0.00001495
Iteration 29/1000 | Loss: 0.00001494
Iteration 30/1000 | Loss: 0.00001491
Iteration 31/1000 | Loss: 0.00001491
Iteration 32/1000 | Loss: 0.00001490
Iteration 33/1000 | Loss: 0.00001489
Iteration 34/1000 | Loss: 0.00001488
Iteration 35/1000 | Loss: 0.00001488
Iteration 36/1000 | Loss: 0.00001488
Iteration 37/1000 | Loss: 0.00001488
Iteration 38/1000 | Loss: 0.00001488
Iteration 39/1000 | Loss: 0.00001488
Iteration 40/1000 | Loss: 0.00001487
Iteration 41/1000 | Loss: 0.00001487
Iteration 42/1000 | Loss: 0.00001487
Iteration 43/1000 | Loss: 0.00001486
Iteration 44/1000 | Loss: 0.00001486
Iteration 45/1000 | Loss: 0.00001485
Iteration 46/1000 | Loss: 0.00001485
Iteration 47/1000 | Loss: 0.00001485
Iteration 48/1000 | Loss: 0.00001484
Iteration 49/1000 | Loss: 0.00001483
Iteration 50/1000 | Loss: 0.00001483
Iteration 51/1000 | Loss: 0.00001483
Iteration 52/1000 | Loss: 0.00001483
Iteration 53/1000 | Loss: 0.00001483
Iteration 54/1000 | Loss: 0.00001483
Iteration 55/1000 | Loss: 0.00001482
Iteration 56/1000 | Loss: 0.00001482
Iteration 57/1000 | Loss: 0.00001481
Iteration 58/1000 | Loss: 0.00001481
Iteration 59/1000 | Loss: 0.00001480
Iteration 60/1000 | Loss: 0.00001480
Iteration 61/1000 | Loss: 0.00001479
Iteration 62/1000 | Loss: 0.00001479
Iteration 63/1000 | Loss: 0.00001479
Iteration 64/1000 | Loss: 0.00001478
Iteration 65/1000 | Loss: 0.00001478
Iteration 66/1000 | Loss: 0.00001478
Iteration 67/1000 | Loss: 0.00001476
Iteration 68/1000 | Loss: 0.00001475
Iteration 69/1000 | Loss: 0.00001475
Iteration 70/1000 | Loss: 0.00001475
Iteration 71/1000 | Loss: 0.00001475
Iteration 72/1000 | Loss: 0.00001475
Iteration 73/1000 | Loss: 0.00001475
Iteration 74/1000 | Loss: 0.00001474
Iteration 75/1000 | Loss: 0.00001474
Iteration 76/1000 | Loss: 0.00001474
Iteration 77/1000 | Loss: 0.00001474
Iteration 78/1000 | Loss: 0.00001474
Iteration 79/1000 | Loss: 0.00001474
Iteration 80/1000 | Loss: 0.00001473
Iteration 81/1000 | Loss: 0.00001473
Iteration 82/1000 | Loss: 0.00001473
Iteration 83/1000 | Loss: 0.00001473
Iteration 84/1000 | Loss: 0.00001472
Iteration 85/1000 | Loss: 0.00001472
Iteration 86/1000 | Loss: 0.00001472
Iteration 87/1000 | Loss: 0.00001472
Iteration 88/1000 | Loss: 0.00001472
Iteration 89/1000 | Loss: 0.00001472
Iteration 90/1000 | Loss: 0.00001472
Iteration 91/1000 | Loss: 0.00001472
Iteration 92/1000 | Loss: 0.00001472
Iteration 93/1000 | Loss: 0.00001471
Iteration 94/1000 | Loss: 0.00001471
Iteration 95/1000 | Loss: 0.00001471
Iteration 96/1000 | Loss: 0.00001471
Iteration 97/1000 | Loss: 0.00001471
Iteration 98/1000 | Loss: 0.00001471
Iteration 99/1000 | Loss: 0.00001471
Iteration 100/1000 | Loss: 0.00001471
Iteration 101/1000 | Loss: 0.00001470
Iteration 102/1000 | Loss: 0.00001470
Iteration 103/1000 | Loss: 0.00001470
Iteration 104/1000 | Loss: 0.00001470
Iteration 105/1000 | Loss: 0.00001470
Iteration 106/1000 | Loss: 0.00001470
Iteration 107/1000 | Loss: 0.00001470
Iteration 108/1000 | Loss: 0.00001470
Iteration 109/1000 | Loss: 0.00001470
Iteration 110/1000 | Loss: 0.00001470
Iteration 111/1000 | Loss: 0.00001470
Iteration 112/1000 | Loss: 0.00001470
Iteration 113/1000 | Loss: 0.00001470
Iteration 114/1000 | Loss: 0.00001470
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 114. Stopping optimization.
Last 5 losses: [1.4695742720505223e-05, 1.4695742720505223e-05, 1.4695742720505223e-05, 1.4695742720505223e-05, 1.4695742720505223e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4695742720505223e-05

Optimization complete. Final v2v error: 3.2021851539611816 mm

Highest mean error: 3.7029869556427 mm for frame 58

Lowest mean error: 2.7558064460754395 mm for frame 12

Saving results

Total time: 36.88759136199951
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ben_posed_008/1076/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_008/1076.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_008/1076
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00852764
Iteration 2/25 | Loss: 0.00109337
Iteration 3/25 | Loss: 0.00073437
Iteration 4/25 | Loss: 0.00068737
Iteration 5/25 | Loss: 0.00068072
Iteration 6/25 | Loss: 0.00067823
Iteration 7/25 | Loss: 0.00067793
Iteration 8/25 | Loss: 0.00067793
Iteration 9/25 | Loss: 0.00067793
Iteration 10/25 | Loss: 0.00067793
Iteration 11/25 | Loss: 0.00067793
Iteration 12/25 | Loss: 0.00067793
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0006779292016290128, 0.0006779292016290128, 0.0006779292016290128, 0.0006779292016290128, 0.0006779292016290128]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006779292016290128

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.03024018
Iteration 2/25 | Loss: 0.00019847
Iteration 3/25 | Loss: 0.00019846
Iteration 4/25 | Loss: 0.00019846
Iteration 5/25 | Loss: 0.00019846
Iteration 6/25 | Loss: 0.00019846
Iteration 7/25 | Loss: 0.00019846
Iteration 8/25 | Loss: 0.00019846
Iteration 9/25 | Loss: 0.00019846
Iteration 10/25 | Loss: 0.00019846
Iteration 11/25 | Loss: 0.00019846
Iteration 12/25 | Loss: 0.00019846
Iteration 13/25 | Loss: 0.00019846
Iteration 14/25 | Loss: 0.00019846
Iteration 15/25 | Loss: 0.00019846
Iteration 16/25 | Loss: 0.00019846
Iteration 17/25 | Loss: 0.00019846
Iteration 18/25 | Loss: 0.00019846
Iteration 19/25 | Loss: 0.00019846
Iteration 20/25 | Loss: 0.00019846
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0001984604896279052, 0.0001984604896279052, 0.0001984604896279052, 0.0001984604896279052, 0.0001984604896279052]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0001984604896279052

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00019846
Iteration 2/1000 | Loss: 0.00002924
Iteration 3/1000 | Loss: 0.00002489
Iteration 4/1000 | Loss: 0.00002337
Iteration 5/1000 | Loss: 0.00002218
Iteration 6/1000 | Loss: 0.00002139
Iteration 7/1000 | Loss: 0.00002095
Iteration 8/1000 | Loss: 0.00002075
Iteration 9/1000 | Loss: 0.00002064
Iteration 10/1000 | Loss: 0.00002048
Iteration 11/1000 | Loss: 0.00002047
Iteration 12/1000 | Loss: 0.00002046
Iteration 13/1000 | Loss: 0.00002045
Iteration 14/1000 | Loss: 0.00002043
Iteration 15/1000 | Loss: 0.00002043
Iteration 16/1000 | Loss: 0.00002042
Iteration 17/1000 | Loss: 0.00002042
Iteration 18/1000 | Loss: 0.00002042
Iteration 19/1000 | Loss: 0.00002042
Iteration 20/1000 | Loss: 0.00002042
Iteration 21/1000 | Loss: 0.00002040
Iteration 22/1000 | Loss: 0.00002038
Iteration 23/1000 | Loss: 0.00002037
Iteration 24/1000 | Loss: 0.00002037
Iteration 25/1000 | Loss: 0.00002037
Iteration 26/1000 | Loss: 0.00002035
Iteration 27/1000 | Loss: 0.00002035
Iteration 28/1000 | Loss: 0.00002035
Iteration 29/1000 | Loss: 0.00002035
Iteration 30/1000 | Loss: 0.00002035
Iteration 31/1000 | Loss: 0.00002035
Iteration 32/1000 | Loss: 0.00002035
Iteration 33/1000 | Loss: 0.00002035
Iteration 34/1000 | Loss: 0.00002035
Iteration 35/1000 | Loss: 0.00002035
Iteration 36/1000 | Loss: 0.00002035
Iteration 37/1000 | Loss: 0.00002035
Iteration 38/1000 | Loss: 0.00002034
Iteration 39/1000 | Loss: 0.00002034
Iteration 40/1000 | Loss: 0.00002033
Iteration 41/1000 | Loss: 0.00002033
Iteration 42/1000 | Loss: 0.00002033
Iteration 43/1000 | Loss: 0.00002032
Iteration 44/1000 | Loss: 0.00002032
Iteration 45/1000 | Loss: 0.00002032
Iteration 46/1000 | Loss: 0.00002031
Iteration 47/1000 | Loss: 0.00002031
Iteration 48/1000 | Loss: 0.00002031
Iteration 49/1000 | Loss: 0.00002031
Iteration 50/1000 | Loss: 0.00002030
Iteration 51/1000 | Loss: 0.00002030
Iteration 52/1000 | Loss: 0.00002030
Iteration 53/1000 | Loss: 0.00002030
Iteration 54/1000 | Loss: 0.00002030
Iteration 55/1000 | Loss: 0.00002030
Iteration 56/1000 | Loss: 0.00002029
Iteration 57/1000 | Loss: 0.00002028
Iteration 58/1000 | Loss: 0.00002028
Iteration 59/1000 | Loss: 0.00002028
Iteration 60/1000 | Loss: 0.00002027
Iteration 61/1000 | Loss: 0.00002027
Iteration 62/1000 | Loss: 0.00002027
Iteration 63/1000 | Loss: 0.00002027
Iteration 64/1000 | Loss: 0.00002027
Iteration 65/1000 | Loss: 0.00002027
Iteration 66/1000 | Loss: 0.00002027
Iteration 67/1000 | Loss: 0.00002027
Iteration 68/1000 | Loss: 0.00002027
Iteration 69/1000 | Loss: 0.00002027
Iteration 70/1000 | Loss: 0.00002026
Iteration 71/1000 | Loss: 0.00002026
Iteration 72/1000 | Loss: 0.00002026
Iteration 73/1000 | Loss: 0.00002026
Iteration 74/1000 | Loss: 0.00002025
Iteration 75/1000 | Loss: 0.00002025
Iteration 76/1000 | Loss: 0.00002025
Iteration 77/1000 | Loss: 0.00002025
Iteration 78/1000 | Loss: 0.00002025
Iteration 79/1000 | Loss: 0.00002025
Iteration 80/1000 | Loss: 0.00002025
Iteration 81/1000 | Loss: 0.00002025
Iteration 82/1000 | Loss: 0.00002025
Iteration 83/1000 | Loss: 0.00002025
Iteration 84/1000 | Loss: 0.00002025
Iteration 85/1000 | Loss: 0.00002025
Iteration 86/1000 | Loss: 0.00002024
Iteration 87/1000 | Loss: 0.00002024
Iteration 88/1000 | Loss: 0.00002024
Iteration 89/1000 | Loss: 0.00002024
Iteration 90/1000 | Loss: 0.00002024
Iteration 91/1000 | Loss: 0.00002024
Iteration 92/1000 | Loss: 0.00002024
Iteration 93/1000 | Loss: 0.00002024
Iteration 94/1000 | Loss: 0.00002024
Iteration 95/1000 | Loss: 0.00002024
Iteration 96/1000 | Loss: 0.00002024
Iteration 97/1000 | Loss: 0.00002023
Iteration 98/1000 | Loss: 0.00002023
Iteration 99/1000 | Loss: 0.00002023
Iteration 100/1000 | Loss: 0.00002023
Iteration 101/1000 | Loss: 0.00002023
Iteration 102/1000 | Loss: 0.00002023
Iteration 103/1000 | Loss: 0.00002023
Iteration 104/1000 | Loss: 0.00002023
Iteration 105/1000 | Loss: 0.00002023
Iteration 106/1000 | Loss: 0.00002023
Iteration 107/1000 | Loss: 0.00002022
Iteration 108/1000 | Loss: 0.00002022
Iteration 109/1000 | Loss: 0.00002022
Iteration 110/1000 | Loss: 0.00002022
Iteration 111/1000 | Loss: 0.00002022
Iteration 112/1000 | Loss: 0.00002022
Iteration 113/1000 | Loss: 0.00002022
Iteration 114/1000 | Loss: 0.00002021
Iteration 115/1000 | Loss: 0.00002021
Iteration 116/1000 | Loss: 0.00002021
Iteration 117/1000 | Loss: 0.00002021
Iteration 118/1000 | Loss: 0.00002021
Iteration 119/1000 | Loss: 0.00002021
Iteration 120/1000 | Loss: 0.00002021
Iteration 121/1000 | Loss: 0.00002021
Iteration 122/1000 | Loss: 0.00002021
Iteration 123/1000 | Loss: 0.00002021
Iteration 124/1000 | Loss: 0.00002020
Iteration 125/1000 | Loss: 0.00002020
Iteration 126/1000 | Loss: 0.00002020
Iteration 127/1000 | Loss: 0.00002020
Iteration 128/1000 | Loss: 0.00002020
Iteration 129/1000 | Loss: 0.00002020
Iteration 130/1000 | Loss: 0.00002020
Iteration 131/1000 | Loss: 0.00002020
Iteration 132/1000 | Loss: 0.00002020
Iteration 133/1000 | Loss: 0.00002020
Iteration 134/1000 | Loss: 0.00002020
Iteration 135/1000 | Loss: 0.00002020
Iteration 136/1000 | Loss: 0.00002020
Iteration 137/1000 | Loss: 0.00002020
Iteration 138/1000 | Loss: 0.00002020
Iteration 139/1000 | Loss: 0.00002020
Iteration 140/1000 | Loss: 0.00002020
Iteration 141/1000 | Loss: 0.00002020
Iteration 142/1000 | Loss: 0.00002020
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 142. Stopping optimization.
Last 5 losses: [2.0199327991576865e-05, 2.0199327991576865e-05, 2.0199327991576865e-05, 2.0199327991576865e-05, 2.0199327991576865e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0199327991576865e-05

Optimization complete. Final v2v error: 3.830482006072998 mm

Highest mean error: 4.027467250823975 mm for frame 14

Lowest mean error: 3.5431623458862305 mm for frame 78

Saving results

Total time: 33.06888127326965
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ben_posed_008/1075/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_008/1075.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_008/1075
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01052968
Iteration 2/25 | Loss: 0.01052968
Iteration 3/25 | Loss: 0.01052968
Iteration 4/25 | Loss: 0.01052967
Iteration 5/25 | Loss: 0.01052967
Iteration 6/25 | Loss: 0.01052967
Iteration 7/25 | Loss: 0.01052967
Iteration 8/25 | Loss: 0.01052967
Iteration 9/25 | Loss: 0.01052967
Iteration 10/25 | Loss: 0.01052967
Iteration 11/25 | Loss: 0.01052967
Iteration 12/25 | Loss: 0.01052967
Iteration 13/25 | Loss: 0.01052967
Iteration 14/25 | Loss: 0.01052967
Iteration 15/25 | Loss: 0.01052966
Iteration 16/25 | Loss: 0.01052966
Iteration 17/25 | Loss: 0.01052966
Iteration 18/25 | Loss: 0.01052966
Iteration 19/25 | Loss: 0.01052966
Iteration 20/25 | Loss: 0.01052966
Iteration 21/25 | Loss: 0.01052966
Iteration 22/25 | Loss: 0.01052966
Iteration 23/25 | Loss: 0.01052966
Iteration 24/25 | Loss: 0.01052966
Iteration 25/25 | Loss: 0.01052966

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.75360584
Iteration 2/25 | Loss: 0.11209228
Iteration 3/25 | Loss: 0.10586473
Iteration 4/25 | Loss: 0.10575360
Iteration 5/25 | Loss: 0.10575356
Iteration 6/25 | Loss: 0.10575354
Iteration 7/25 | Loss: 0.10575354
Iteration 8/25 | Loss: 0.10575354
Iteration 9/25 | Loss: 0.10575354
Iteration 10/25 | Loss: 0.10575354
Iteration 11/25 | Loss: 0.10575353
Iteration 12/25 | Loss: 0.10575353
Iteration 13/25 | Loss: 0.10575353
Iteration 14/25 | Loss: 0.10575353
Iteration 15/25 | Loss: 0.10575353
Iteration 16/25 | Loss: 0.10575353
Iteration 17/25 | Loss: 0.10575353
Iteration 18/25 | Loss: 0.10575353
Iteration 19/25 | Loss: 0.10575353
Iteration 20/25 | Loss: 0.10575353
Iteration 21/25 | Loss: 0.10575353
Iteration 22/25 | Loss: 0.10575353
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.10575352609157562, 0.10575352609157562, 0.10575352609157562, 0.10575352609157562, 0.10575352609157562]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.10575352609157562

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.10575353
Iteration 2/1000 | Loss: 0.00540474
Iteration 3/1000 | Loss: 0.00640591
Iteration 4/1000 | Loss: 0.00055208
Iteration 5/1000 | Loss: 0.00047521
Iteration 6/1000 | Loss: 0.00023124
Iteration 7/1000 | Loss: 0.00011807
Iteration 8/1000 | Loss: 0.00006295
Iteration 9/1000 | Loss: 0.00033179
Iteration 10/1000 | Loss: 0.00045865
Iteration 11/1000 | Loss: 0.00026645
Iteration 12/1000 | Loss: 0.00005102
Iteration 13/1000 | Loss: 0.00042942
Iteration 14/1000 | Loss: 0.00010108
Iteration 15/1000 | Loss: 0.00012225
Iteration 16/1000 | Loss: 0.00004389
Iteration 17/1000 | Loss: 0.00015892
Iteration 18/1000 | Loss: 0.00003848
Iteration 19/1000 | Loss: 0.00021865
Iteration 20/1000 | Loss: 0.00003454
Iteration 21/1000 | Loss: 0.00004018
Iteration 22/1000 | Loss: 0.00019158
Iteration 23/1000 | Loss: 0.00015579
Iteration 24/1000 | Loss: 0.00004801
Iteration 25/1000 | Loss: 0.00003810
Iteration 26/1000 | Loss: 0.00002505
Iteration 27/1000 | Loss: 0.00005512
Iteration 28/1000 | Loss: 0.00003616
Iteration 29/1000 | Loss: 0.00006365
Iteration 30/1000 | Loss: 0.00004298
Iteration 31/1000 | Loss: 0.00004565
Iteration 32/1000 | Loss: 0.00014778
Iteration 33/1000 | Loss: 0.00001925
Iteration 34/1000 | Loss: 0.00004419
Iteration 35/1000 | Loss: 0.00002444
Iteration 36/1000 | Loss: 0.00001966
Iteration 37/1000 | Loss: 0.00004768
Iteration 38/1000 | Loss: 0.00006679
Iteration 39/1000 | Loss: 0.00005822
Iteration 40/1000 | Loss: 0.00002204
Iteration 41/1000 | Loss: 0.00001819
Iteration 42/1000 | Loss: 0.00002513
Iteration 43/1000 | Loss: 0.00001819
Iteration 44/1000 | Loss: 0.00003396
Iteration 45/1000 | Loss: 0.00011909
Iteration 46/1000 | Loss: 0.00002854
Iteration 47/1000 | Loss: 0.00003162
Iteration 48/1000 | Loss: 0.00001855
Iteration 49/1000 | Loss: 0.00001883
Iteration 50/1000 | Loss: 0.00002863
Iteration 51/1000 | Loss: 0.00006137
Iteration 52/1000 | Loss: 0.00001907
Iteration 53/1000 | Loss: 0.00001884
Iteration 54/1000 | Loss: 0.00007651
Iteration 55/1000 | Loss: 0.00002074
Iteration 56/1000 | Loss: 0.00001861
Iteration 57/1000 | Loss: 0.00001667
Iteration 58/1000 | Loss: 0.00001819
Iteration 59/1000 | Loss: 0.00001772
Iteration 60/1000 | Loss: 0.00001771
Iteration 61/1000 | Loss: 0.00001770
Iteration 62/1000 | Loss: 0.00001769
Iteration 63/1000 | Loss: 0.00005646
Iteration 64/1000 | Loss: 0.00001963
Iteration 65/1000 | Loss: 0.00001870
Iteration 66/1000 | Loss: 0.00001865
Iteration 67/1000 | Loss: 0.00003026
Iteration 68/1000 | Loss: 0.00001733
Iteration 69/1000 | Loss: 0.00001666
Iteration 70/1000 | Loss: 0.00001735
Iteration 71/1000 | Loss: 0.00001682
Iteration 72/1000 | Loss: 0.00001659
Iteration 73/1000 | Loss: 0.00001659
Iteration 74/1000 | Loss: 0.00001658
Iteration 75/1000 | Loss: 0.00001658
Iteration 76/1000 | Loss: 0.00001658
Iteration 77/1000 | Loss: 0.00001658
Iteration 78/1000 | Loss: 0.00001658
Iteration 79/1000 | Loss: 0.00001659
Iteration 80/1000 | Loss: 0.00001658
Iteration 81/1000 | Loss: 0.00001658
Iteration 82/1000 | Loss: 0.00001658
Iteration 83/1000 | Loss: 0.00001658
Iteration 84/1000 | Loss: 0.00001658
Iteration 85/1000 | Loss: 0.00001658
Iteration 86/1000 | Loss: 0.00001658
Iteration 87/1000 | Loss: 0.00001658
Iteration 88/1000 | Loss: 0.00001658
Iteration 89/1000 | Loss: 0.00001658
Iteration 90/1000 | Loss: 0.00001658
Iteration 91/1000 | Loss: 0.00001658
Iteration 92/1000 | Loss: 0.00001658
Iteration 93/1000 | Loss: 0.00001658
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 93. Stopping optimization.
Last 5 losses: [1.658166183915455e-05, 1.658166183915455e-05, 1.658166183915455e-05, 1.658166183915455e-05, 1.658166183915455e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.658166183915455e-05

Optimization complete. Final v2v error: 3.4214420318603516 mm

Highest mean error: 3.6053903102874756 mm for frame 177

Lowest mean error: 3.3417932987213135 mm for frame 46

Saving results

Total time: 110.52704215049744
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ben_posed_008/1041/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_008/1041.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_008/1041
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00890998
Iteration 2/25 | Loss: 0.00110901
Iteration 3/25 | Loss: 0.00079583
Iteration 4/25 | Loss: 0.00075668
Iteration 5/25 | Loss: 0.00074443
Iteration 6/25 | Loss: 0.00074157
Iteration 7/25 | Loss: 0.00074104
Iteration 8/25 | Loss: 0.00074104
Iteration 9/25 | Loss: 0.00074104
Iteration 10/25 | Loss: 0.00074104
Iteration 11/25 | Loss: 0.00074104
Iteration 12/25 | Loss: 0.00074104
Iteration 13/25 | Loss: 0.00074104
Iteration 14/25 | Loss: 0.00074104
Iteration 15/25 | Loss: 0.00074104
Iteration 16/25 | Loss: 0.00074104
Iteration 17/25 | Loss: 0.00074104
Iteration 18/25 | Loss: 0.00074104
Iteration 19/25 | Loss: 0.00074104
Iteration 20/25 | Loss: 0.00074104
Iteration 21/25 | Loss: 0.00074104
Iteration 22/25 | Loss: 0.00074104
Iteration 23/25 | Loss: 0.00074104
Iteration 24/25 | Loss: 0.00074104
Iteration 25/25 | Loss: 0.00074104

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35177946
Iteration 2/25 | Loss: 0.00027579
Iteration 3/25 | Loss: 0.00027575
Iteration 4/25 | Loss: 0.00027575
Iteration 5/25 | Loss: 0.00027575
Iteration 6/25 | Loss: 0.00027575
Iteration 7/25 | Loss: 0.00027575
Iteration 8/25 | Loss: 0.00027575
Iteration 9/25 | Loss: 0.00027575
Iteration 10/25 | Loss: 0.00027575
Iteration 11/25 | Loss: 0.00027575
Iteration 12/25 | Loss: 0.00027575
Iteration 13/25 | Loss: 0.00027575
Iteration 14/25 | Loss: 0.00027575
Iteration 15/25 | Loss: 0.00027575
Iteration 16/25 | Loss: 0.00027575
Iteration 17/25 | Loss: 0.00027575
Iteration 18/25 | Loss: 0.00027575
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.00027574654086492956, 0.00027574654086492956, 0.00027574654086492956, 0.00027574654086492956, 0.00027574654086492956]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00027574654086492956

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00027575
Iteration 2/1000 | Loss: 0.00004251
Iteration 3/1000 | Loss: 0.00003341
Iteration 4/1000 | Loss: 0.00002951
Iteration 5/1000 | Loss: 0.00002793
Iteration 6/1000 | Loss: 0.00002691
Iteration 7/1000 | Loss: 0.00002629
Iteration 8/1000 | Loss: 0.00002581
Iteration 9/1000 | Loss: 0.00002550
Iteration 10/1000 | Loss: 0.00002537
Iteration 11/1000 | Loss: 0.00002532
Iteration 12/1000 | Loss: 0.00002512
Iteration 13/1000 | Loss: 0.00002497
Iteration 14/1000 | Loss: 0.00002495
Iteration 15/1000 | Loss: 0.00002489
Iteration 16/1000 | Loss: 0.00002484
Iteration 17/1000 | Loss: 0.00002484
Iteration 18/1000 | Loss: 0.00002482
Iteration 19/1000 | Loss: 0.00002481
Iteration 20/1000 | Loss: 0.00002480
Iteration 21/1000 | Loss: 0.00002473
Iteration 22/1000 | Loss: 0.00002468
Iteration 23/1000 | Loss: 0.00002467
Iteration 24/1000 | Loss: 0.00002467
Iteration 25/1000 | Loss: 0.00002464
Iteration 26/1000 | Loss: 0.00002464
Iteration 27/1000 | Loss: 0.00002463
Iteration 28/1000 | Loss: 0.00002463
Iteration 29/1000 | Loss: 0.00002462
Iteration 30/1000 | Loss: 0.00002461
Iteration 31/1000 | Loss: 0.00002461
Iteration 32/1000 | Loss: 0.00002461
Iteration 33/1000 | Loss: 0.00002461
Iteration 34/1000 | Loss: 0.00002461
Iteration 35/1000 | Loss: 0.00002461
Iteration 36/1000 | Loss: 0.00002460
Iteration 37/1000 | Loss: 0.00002460
Iteration 38/1000 | Loss: 0.00002460
Iteration 39/1000 | Loss: 0.00002460
Iteration 40/1000 | Loss: 0.00002460
Iteration 41/1000 | Loss: 0.00002459
Iteration 42/1000 | Loss: 0.00002459
Iteration 43/1000 | Loss: 0.00002459
Iteration 44/1000 | Loss: 0.00002459
Iteration 45/1000 | Loss: 0.00002458
Iteration 46/1000 | Loss: 0.00002458
Iteration 47/1000 | Loss: 0.00002458
Iteration 48/1000 | Loss: 0.00002458
Iteration 49/1000 | Loss: 0.00002458
Iteration 50/1000 | Loss: 0.00002457
Iteration 51/1000 | Loss: 0.00002457
Iteration 52/1000 | Loss: 0.00002457
Iteration 53/1000 | Loss: 0.00002457
Iteration 54/1000 | Loss: 0.00002456
Iteration 55/1000 | Loss: 0.00002456
Iteration 56/1000 | Loss: 0.00002456
Iteration 57/1000 | Loss: 0.00002456
Iteration 58/1000 | Loss: 0.00002456
Iteration 59/1000 | Loss: 0.00002455
Iteration 60/1000 | Loss: 0.00002455
Iteration 61/1000 | Loss: 0.00002455
Iteration 62/1000 | Loss: 0.00002454
Iteration 63/1000 | Loss: 0.00002454
Iteration 64/1000 | Loss: 0.00002454
Iteration 65/1000 | Loss: 0.00002453
Iteration 66/1000 | Loss: 0.00002453
Iteration 67/1000 | Loss: 0.00002453
Iteration 68/1000 | Loss: 0.00002453
Iteration 69/1000 | Loss: 0.00002453
Iteration 70/1000 | Loss: 0.00002453
Iteration 71/1000 | Loss: 0.00002453
Iteration 72/1000 | Loss: 0.00002452
Iteration 73/1000 | Loss: 0.00002452
Iteration 74/1000 | Loss: 0.00002452
Iteration 75/1000 | Loss: 0.00002452
Iteration 76/1000 | Loss: 0.00002452
Iteration 77/1000 | Loss: 0.00002452
Iteration 78/1000 | Loss: 0.00002452
Iteration 79/1000 | Loss: 0.00002451
Iteration 80/1000 | Loss: 0.00002451
Iteration 81/1000 | Loss: 0.00002451
Iteration 82/1000 | Loss: 0.00002451
Iteration 83/1000 | Loss: 0.00002451
Iteration 84/1000 | Loss: 0.00002450
Iteration 85/1000 | Loss: 0.00002450
Iteration 86/1000 | Loss: 0.00002450
Iteration 87/1000 | Loss: 0.00002449
Iteration 88/1000 | Loss: 0.00002449
Iteration 89/1000 | Loss: 0.00002449
Iteration 90/1000 | Loss: 0.00002448
Iteration 91/1000 | Loss: 0.00002448
Iteration 92/1000 | Loss: 0.00002448
Iteration 93/1000 | Loss: 0.00002448
Iteration 94/1000 | Loss: 0.00002447
Iteration 95/1000 | Loss: 0.00002447
Iteration 96/1000 | Loss: 0.00002447
Iteration 97/1000 | Loss: 0.00002447
Iteration 98/1000 | Loss: 0.00002447
Iteration 99/1000 | Loss: 0.00002447
Iteration 100/1000 | Loss: 0.00002446
Iteration 101/1000 | Loss: 0.00002446
Iteration 102/1000 | Loss: 0.00002446
Iteration 103/1000 | Loss: 0.00002445
Iteration 104/1000 | Loss: 0.00002445
Iteration 105/1000 | Loss: 0.00002445
Iteration 106/1000 | Loss: 0.00002445
Iteration 107/1000 | Loss: 0.00002444
Iteration 108/1000 | Loss: 0.00002444
Iteration 109/1000 | Loss: 0.00002444
Iteration 110/1000 | Loss: 0.00002444
Iteration 111/1000 | Loss: 0.00002444
Iteration 112/1000 | Loss: 0.00002444
Iteration 113/1000 | Loss: 0.00002443
Iteration 114/1000 | Loss: 0.00002443
Iteration 115/1000 | Loss: 0.00002443
Iteration 116/1000 | Loss: 0.00002443
Iteration 117/1000 | Loss: 0.00002443
Iteration 118/1000 | Loss: 0.00002442
Iteration 119/1000 | Loss: 0.00002442
Iteration 120/1000 | Loss: 0.00002442
Iteration 121/1000 | Loss: 0.00002442
Iteration 122/1000 | Loss: 0.00002441
Iteration 123/1000 | Loss: 0.00002441
Iteration 124/1000 | Loss: 0.00002441
Iteration 125/1000 | Loss: 0.00002441
Iteration 126/1000 | Loss: 0.00002441
Iteration 127/1000 | Loss: 0.00002441
Iteration 128/1000 | Loss: 0.00002441
Iteration 129/1000 | Loss: 0.00002441
Iteration 130/1000 | Loss: 0.00002440
Iteration 131/1000 | Loss: 0.00002440
Iteration 132/1000 | Loss: 0.00002440
Iteration 133/1000 | Loss: 0.00002440
Iteration 134/1000 | Loss: 0.00002440
Iteration 135/1000 | Loss: 0.00002440
Iteration 136/1000 | Loss: 0.00002440
Iteration 137/1000 | Loss: 0.00002440
Iteration 138/1000 | Loss: 0.00002440
Iteration 139/1000 | Loss: 0.00002440
Iteration 140/1000 | Loss: 0.00002440
Iteration 141/1000 | Loss: 0.00002440
Iteration 142/1000 | Loss: 0.00002440
Iteration 143/1000 | Loss: 0.00002439
Iteration 144/1000 | Loss: 0.00002439
Iteration 145/1000 | Loss: 0.00002439
Iteration 146/1000 | Loss: 0.00002439
Iteration 147/1000 | Loss: 0.00002439
Iteration 148/1000 | Loss: 0.00002439
Iteration 149/1000 | Loss: 0.00002439
Iteration 150/1000 | Loss: 0.00002439
Iteration 151/1000 | Loss: 0.00002439
Iteration 152/1000 | Loss: 0.00002439
Iteration 153/1000 | Loss: 0.00002439
Iteration 154/1000 | Loss: 0.00002439
Iteration 155/1000 | Loss: 0.00002439
Iteration 156/1000 | Loss: 0.00002439
Iteration 157/1000 | Loss: 0.00002438
Iteration 158/1000 | Loss: 0.00002438
Iteration 159/1000 | Loss: 0.00002438
Iteration 160/1000 | Loss: 0.00002438
Iteration 161/1000 | Loss: 0.00002438
Iteration 162/1000 | Loss: 0.00002438
Iteration 163/1000 | Loss: 0.00002438
Iteration 164/1000 | Loss: 0.00002438
Iteration 165/1000 | Loss: 0.00002438
Iteration 166/1000 | Loss: 0.00002438
Iteration 167/1000 | Loss: 0.00002438
Iteration 168/1000 | Loss: 0.00002438
Iteration 169/1000 | Loss: 0.00002438
Iteration 170/1000 | Loss: 0.00002438
Iteration 171/1000 | Loss: 0.00002438
Iteration 172/1000 | Loss: 0.00002438
Iteration 173/1000 | Loss: 0.00002438
Iteration 174/1000 | Loss: 0.00002438
Iteration 175/1000 | Loss: 0.00002438
Iteration 176/1000 | Loss: 0.00002438
Iteration 177/1000 | Loss: 0.00002437
Iteration 178/1000 | Loss: 0.00002437
Iteration 179/1000 | Loss: 0.00002437
Iteration 180/1000 | Loss: 0.00002437
Iteration 181/1000 | Loss: 0.00002437
Iteration 182/1000 | Loss: 0.00002437
Iteration 183/1000 | Loss: 0.00002437
Iteration 184/1000 | Loss: 0.00002437
Iteration 185/1000 | Loss: 0.00002437
Iteration 186/1000 | Loss: 0.00002437
Iteration 187/1000 | Loss: 0.00002437
Iteration 188/1000 | Loss: 0.00002437
Iteration 189/1000 | Loss: 0.00002437
Iteration 190/1000 | Loss: 0.00002437
Iteration 191/1000 | Loss: 0.00002437
Iteration 192/1000 | Loss: 0.00002437
Iteration 193/1000 | Loss: 0.00002437
Iteration 194/1000 | Loss: 0.00002437
Iteration 195/1000 | Loss: 0.00002437
Iteration 196/1000 | Loss: 0.00002437
Iteration 197/1000 | Loss: 0.00002437
Iteration 198/1000 | Loss: 0.00002436
Iteration 199/1000 | Loss: 0.00002436
Iteration 200/1000 | Loss: 0.00002436
Iteration 201/1000 | Loss: 0.00002436
Iteration 202/1000 | Loss: 0.00002436
Iteration 203/1000 | Loss: 0.00002436
Iteration 204/1000 | Loss: 0.00002436
Iteration 205/1000 | Loss: 0.00002436
Iteration 206/1000 | Loss: 0.00002436
Iteration 207/1000 | Loss: 0.00002436
Iteration 208/1000 | Loss: 0.00002436
Iteration 209/1000 | Loss: 0.00002436
Iteration 210/1000 | Loss: 0.00002436
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 210. Stopping optimization.
Last 5 losses: [2.43618505919585e-05, 2.43618505919585e-05, 2.43618505919585e-05, 2.43618505919585e-05, 2.43618505919585e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.43618505919585e-05

Optimization complete. Final v2v error: 3.943584442138672 mm

Highest mean error: 6.099279880523682 mm for frame 101

Lowest mean error: 2.9496164321899414 mm for frame 127

Saving results

Total time: 49.06587338447571
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ben_posed_008/1074/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_008/1074.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_008/1074
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00623322
Iteration 2/25 | Loss: 0.00121903
Iteration 3/25 | Loss: 0.00079836
Iteration 4/25 | Loss: 0.00071696
Iteration 5/25 | Loss: 0.00070139
Iteration 6/25 | Loss: 0.00068536
Iteration 7/25 | Loss: 0.00068228
Iteration 8/25 | Loss: 0.00068127
Iteration 9/25 | Loss: 0.00067696
Iteration 10/25 | Loss: 0.00067479
Iteration 11/25 | Loss: 0.00067343
Iteration 12/25 | Loss: 0.00067294
Iteration 13/25 | Loss: 0.00067260
Iteration 14/25 | Loss: 0.00067239
Iteration 15/25 | Loss: 0.00067235
Iteration 16/25 | Loss: 0.00067235
Iteration 17/25 | Loss: 0.00067235
Iteration 18/25 | Loss: 0.00067234
Iteration 19/25 | Loss: 0.00067234
Iteration 20/25 | Loss: 0.00067234
Iteration 21/25 | Loss: 0.00067234
Iteration 22/25 | Loss: 0.00067234
Iteration 23/25 | Loss: 0.00067234
Iteration 24/25 | Loss: 0.00067234
Iteration 25/25 | Loss: 0.00067233

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.47417593
Iteration 2/25 | Loss: 0.00022110
Iteration 3/25 | Loss: 0.00022110
Iteration 4/25 | Loss: 0.00022110
Iteration 5/25 | Loss: 0.00022110
Iteration 6/25 | Loss: 0.00022110
Iteration 7/25 | Loss: 0.00022110
Iteration 8/25 | Loss: 0.00022110
Iteration 9/25 | Loss: 0.00022110
Iteration 10/25 | Loss: 0.00022110
Iteration 11/25 | Loss: 0.00022110
Iteration 12/25 | Loss: 0.00022110
Iteration 13/25 | Loss: 0.00022110
Iteration 14/25 | Loss: 0.00022110
Iteration 15/25 | Loss: 0.00022110
Iteration 16/25 | Loss: 0.00022110
Iteration 17/25 | Loss: 0.00022110
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.00022109840938355774, 0.00022109840938355774, 0.00022109840938355774, 0.00022109840938355774, 0.00022109840938355774]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00022109840938355774

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00022110
Iteration 2/1000 | Loss: 0.00003852
Iteration 3/1000 | Loss: 0.00002580
Iteration 4/1000 | Loss: 0.00002318
Iteration 5/1000 | Loss: 0.00002238
Iteration 6/1000 | Loss: 0.00002157
Iteration 7/1000 | Loss: 0.00002100
Iteration 8/1000 | Loss: 0.00002057
Iteration 9/1000 | Loss: 0.00002021
Iteration 10/1000 | Loss: 0.00024454
Iteration 11/1000 | Loss: 0.00002615
Iteration 12/1000 | Loss: 0.00052086
Iteration 13/1000 | Loss: 0.00003987
Iteration 14/1000 | Loss: 0.00002555
Iteration 15/1000 | Loss: 0.00002201
Iteration 16/1000 | Loss: 0.00002073
Iteration 17/1000 | Loss: 0.00002007
Iteration 18/1000 | Loss: 0.00001956
Iteration 19/1000 | Loss: 0.00001914
Iteration 20/1000 | Loss: 0.00001881
Iteration 21/1000 | Loss: 0.00001857
Iteration 22/1000 | Loss: 0.00001849
Iteration 23/1000 | Loss: 0.00001843
Iteration 24/1000 | Loss: 0.00001842
Iteration 25/1000 | Loss: 0.00001839
Iteration 26/1000 | Loss: 0.00001838
Iteration 27/1000 | Loss: 0.00001838
Iteration 28/1000 | Loss: 0.00001837
Iteration 29/1000 | Loss: 0.00001835
Iteration 30/1000 | Loss: 0.00001832
Iteration 31/1000 | Loss: 0.00001828
Iteration 32/1000 | Loss: 0.00001828
Iteration 33/1000 | Loss: 0.00001828
Iteration 34/1000 | Loss: 0.00001827
Iteration 35/1000 | Loss: 0.00001827
Iteration 36/1000 | Loss: 0.00001826
Iteration 37/1000 | Loss: 0.00001826
Iteration 38/1000 | Loss: 0.00001826
Iteration 39/1000 | Loss: 0.00001826
Iteration 40/1000 | Loss: 0.00001826
Iteration 41/1000 | Loss: 0.00001826
Iteration 42/1000 | Loss: 0.00001825
Iteration 43/1000 | Loss: 0.00001825
Iteration 44/1000 | Loss: 0.00001825
Iteration 45/1000 | Loss: 0.00001824
Iteration 46/1000 | Loss: 0.00001824
Iteration 47/1000 | Loss: 0.00001824
Iteration 48/1000 | Loss: 0.00001824
Iteration 49/1000 | Loss: 0.00001824
Iteration 50/1000 | Loss: 0.00001823
Iteration 51/1000 | Loss: 0.00001823
Iteration 52/1000 | Loss: 0.00001823
Iteration 53/1000 | Loss: 0.00001822
Iteration 54/1000 | Loss: 0.00001822
Iteration 55/1000 | Loss: 0.00001822
Iteration 56/1000 | Loss: 0.00001821
Iteration 57/1000 | Loss: 0.00001821
Iteration 58/1000 | Loss: 0.00001821
Iteration 59/1000 | Loss: 0.00001820
Iteration 60/1000 | Loss: 0.00001820
Iteration 61/1000 | Loss: 0.00001819
Iteration 62/1000 | Loss: 0.00001819
Iteration 63/1000 | Loss: 0.00001819
Iteration 64/1000 | Loss: 0.00001817
Iteration 65/1000 | Loss: 0.00001815
Iteration 66/1000 | Loss: 0.00001813
Iteration 67/1000 | Loss: 0.00001810
Iteration 68/1000 | Loss: 0.00001808
Iteration 69/1000 | Loss: 0.00001806
Iteration 70/1000 | Loss: 0.00001806
Iteration 71/1000 | Loss: 0.00001806
Iteration 72/1000 | Loss: 0.00001806
Iteration 73/1000 | Loss: 0.00001806
Iteration 74/1000 | Loss: 0.00001806
Iteration 75/1000 | Loss: 0.00001805
Iteration 76/1000 | Loss: 0.00001805
Iteration 77/1000 | Loss: 0.00001805
Iteration 78/1000 | Loss: 0.00001804
Iteration 79/1000 | Loss: 0.00001803
Iteration 80/1000 | Loss: 0.00001803
Iteration 81/1000 | Loss: 0.00001803
Iteration 82/1000 | Loss: 0.00001802
Iteration 83/1000 | Loss: 0.00001802
Iteration 84/1000 | Loss: 0.00001801
Iteration 85/1000 | Loss: 0.00001801
Iteration 86/1000 | Loss: 0.00001800
Iteration 87/1000 | Loss: 0.00001800
Iteration 88/1000 | Loss: 0.00001799
Iteration 89/1000 | Loss: 0.00001791
Iteration 90/1000 | Loss: 0.00001788
Iteration 91/1000 | Loss: 0.00001787
Iteration 92/1000 | Loss: 0.00001787
Iteration 93/1000 | Loss: 0.00001786
Iteration 94/1000 | Loss: 0.00001785
Iteration 95/1000 | Loss: 0.00001784
Iteration 96/1000 | Loss: 0.00001783
Iteration 97/1000 | Loss: 0.00001781
Iteration 98/1000 | Loss: 0.00001780
Iteration 99/1000 | Loss: 0.00001779
Iteration 100/1000 | Loss: 0.00001779
Iteration 101/1000 | Loss: 0.00001778
Iteration 102/1000 | Loss: 0.00001778
Iteration 103/1000 | Loss: 0.00001777
Iteration 104/1000 | Loss: 0.00001773
Iteration 105/1000 | Loss: 0.00001772
Iteration 106/1000 | Loss: 0.00001771
Iteration 107/1000 | Loss: 0.00001771
Iteration 108/1000 | Loss: 0.00001770
Iteration 109/1000 | Loss: 0.00001770
Iteration 110/1000 | Loss: 0.00001769
Iteration 111/1000 | Loss: 0.00001768
Iteration 112/1000 | Loss: 0.00001768
Iteration 113/1000 | Loss: 0.00001768
Iteration 114/1000 | Loss: 0.00001767
Iteration 115/1000 | Loss: 0.00001767
Iteration 116/1000 | Loss: 0.00001767
Iteration 117/1000 | Loss: 0.00001767
Iteration 118/1000 | Loss: 0.00001766
Iteration 119/1000 | Loss: 0.00001766
Iteration 120/1000 | Loss: 0.00001766
Iteration 121/1000 | Loss: 0.00001766
Iteration 122/1000 | Loss: 0.00001765
Iteration 123/1000 | Loss: 0.00001765
Iteration 124/1000 | Loss: 0.00001765
Iteration 125/1000 | Loss: 0.00001764
Iteration 126/1000 | Loss: 0.00001764
Iteration 127/1000 | Loss: 0.00001764
Iteration 128/1000 | Loss: 0.00001763
Iteration 129/1000 | Loss: 0.00001763
Iteration 130/1000 | Loss: 0.00001763
Iteration 131/1000 | Loss: 0.00001762
Iteration 132/1000 | Loss: 0.00001761
Iteration 133/1000 | Loss: 0.00001761
Iteration 134/1000 | Loss: 0.00001761
Iteration 135/1000 | Loss: 0.00001761
Iteration 136/1000 | Loss: 0.00001761
Iteration 137/1000 | Loss: 0.00001761
Iteration 138/1000 | Loss: 0.00001761
Iteration 139/1000 | Loss: 0.00001761
Iteration 140/1000 | Loss: 0.00001761
Iteration 141/1000 | Loss: 0.00001761
Iteration 142/1000 | Loss: 0.00001761
Iteration 143/1000 | Loss: 0.00001761
Iteration 144/1000 | Loss: 0.00001761
Iteration 145/1000 | Loss: 0.00001760
Iteration 146/1000 | Loss: 0.00001760
Iteration 147/1000 | Loss: 0.00001760
Iteration 148/1000 | Loss: 0.00001759
Iteration 149/1000 | Loss: 0.00001759
Iteration 150/1000 | Loss: 0.00001759
Iteration 151/1000 | Loss: 0.00001758
Iteration 152/1000 | Loss: 0.00001758
Iteration 153/1000 | Loss: 0.00001758
Iteration 154/1000 | Loss: 0.00001758
Iteration 155/1000 | Loss: 0.00001758
Iteration 156/1000 | Loss: 0.00001757
Iteration 157/1000 | Loss: 0.00001757
Iteration 158/1000 | Loss: 0.00001757
Iteration 159/1000 | Loss: 0.00001757
Iteration 160/1000 | Loss: 0.00001757
Iteration 161/1000 | Loss: 0.00001757
Iteration 162/1000 | Loss: 0.00001757
Iteration 163/1000 | Loss: 0.00001757
Iteration 164/1000 | Loss: 0.00001756
Iteration 165/1000 | Loss: 0.00001756
Iteration 166/1000 | Loss: 0.00001756
Iteration 167/1000 | Loss: 0.00001756
Iteration 168/1000 | Loss: 0.00001756
Iteration 169/1000 | Loss: 0.00001756
Iteration 170/1000 | Loss: 0.00001756
Iteration 171/1000 | Loss: 0.00001756
Iteration 172/1000 | Loss: 0.00001756
Iteration 173/1000 | Loss: 0.00001756
Iteration 174/1000 | Loss: 0.00001756
Iteration 175/1000 | Loss: 0.00001756
Iteration 176/1000 | Loss: 0.00001756
Iteration 177/1000 | Loss: 0.00001756
Iteration 178/1000 | Loss: 0.00001756
Iteration 179/1000 | Loss: 0.00001756
Iteration 180/1000 | Loss: 0.00001755
Iteration 181/1000 | Loss: 0.00001755
Iteration 182/1000 | Loss: 0.00001755
Iteration 183/1000 | Loss: 0.00001755
Iteration 184/1000 | Loss: 0.00001755
Iteration 185/1000 | Loss: 0.00001755
Iteration 186/1000 | Loss: 0.00001755
Iteration 187/1000 | Loss: 0.00001755
Iteration 188/1000 | Loss: 0.00001755
Iteration 189/1000 | Loss: 0.00001755
Iteration 190/1000 | Loss: 0.00001755
Iteration 191/1000 | Loss: 0.00001755
Iteration 192/1000 | Loss: 0.00001755
Iteration 193/1000 | Loss: 0.00001755
Iteration 194/1000 | Loss: 0.00001755
Iteration 195/1000 | Loss: 0.00001755
Iteration 196/1000 | Loss: 0.00001755
Iteration 197/1000 | Loss: 0.00001755
Iteration 198/1000 | Loss: 0.00001755
Iteration 199/1000 | Loss: 0.00001755
Iteration 200/1000 | Loss: 0.00001755
Iteration 201/1000 | Loss: 0.00001755
Iteration 202/1000 | Loss: 0.00001755
Iteration 203/1000 | Loss: 0.00001755
Iteration 204/1000 | Loss: 0.00001755
Iteration 205/1000 | Loss: 0.00001755
Iteration 206/1000 | Loss: 0.00001755
Iteration 207/1000 | Loss: 0.00001755
Iteration 208/1000 | Loss: 0.00001755
Iteration 209/1000 | Loss: 0.00001755
Iteration 210/1000 | Loss: 0.00001755
Iteration 211/1000 | Loss: 0.00001755
Iteration 212/1000 | Loss: 0.00001755
Iteration 213/1000 | Loss: 0.00001755
Iteration 214/1000 | Loss: 0.00001755
Iteration 215/1000 | Loss: 0.00001755
Iteration 216/1000 | Loss: 0.00001755
Iteration 217/1000 | Loss: 0.00001755
Iteration 218/1000 | Loss: 0.00001755
Iteration 219/1000 | Loss: 0.00001755
Iteration 220/1000 | Loss: 0.00001755
Iteration 221/1000 | Loss: 0.00001755
Iteration 222/1000 | Loss: 0.00001755
Iteration 223/1000 | Loss: 0.00001755
Iteration 224/1000 | Loss: 0.00001755
Iteration 225/1000 | Loss: 0.00001755
Iteration 226/1000 | Loss: 0.00001755
Iteration 227/1000 | Loss: 0.00001755
Iteration 228/1000 | Loss: 0.00001755
Iteration 229/1000 | Loss: 0.00001755
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 229. Stopping optimization.
Last 5 losses: [1.755175799189601e-05, 1.755175799189601e-05, 1.755175799189601e-05, 1.755175799189601e-05, 1.755175799189601e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.755175799189601e-05

Optimization complete. Final v2v error: 3.490440845489502 mm

Highest mean error: 6.550034046173096 mm for frame 166

Lowest mean error: 2.6109209060668945 mm for frame 237

Saving results

Total time: 87.18324756622314
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ben_posed_008/1094/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_008/1094.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_008/1094
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00392687
Iteration 2/25 | Loss: 0.00067738
Iteration 3/25 | Loss: 0.00056891
Iteration 4/25 | Loss: 0.00054739
Iteration 5/25 | Loss: 0.00054218
Iteration 6/25 | Loss: 0.00054122
Iteration 7/25 | Loss: 0.00054117
Iteration 8/25 | Loss: 0.00054117
Iteration 9/25 | Loss: 0.00054117
Iteration 10/25 | Loss: 0.00054117
Iteration 11/25 | Loss: 0.00054117
Iteration 12/25 | Loss: 0.00054117
Iteration 13/25 | Loss: 0.00054117
Iteration 14/25 | Loss: 0.00054117
Iteration 15/25 | Loss: 0.00054117
Iteration 16/25 | Loss: 0.00054117
Iteration 17/25 | Loss: 0.00054117
Iteration 18/25 | Loss: 0.00054117
Iteration 19/25 | Loss: 0.00054117
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0005411687889136374, 0.0005411687889136374, 0.0005411687889136374, 0.0005411687889136374, 0.0005411687889136374]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005411687889136374

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39448655
Iteration 2/25 | Loss: 0.00014029
Iteration 3/25 | Loss: 0.00014029
Iteration 4/25 | Loss: 0.00014029
Iteration 5/25 | Loss: 0.00014028
Iteration 6/25 | Loss: 0.00014028
Iteration 7/25 | Loss: 0.00014028
Iteration 8/25 | Loss: 0.00014028
Iteration 9/25 | Loss: 0.00014028
Iteration 10/25 | Loss: 0.00014028
Iteration 11/25 | Loss: 0.00014028
Iteration 12/25 | Loss: 0.00014028
Iteration 13/25 | Loss: 0.00014028
Iteration 14/25 | Loss: 0.00014028
Iteration 15/25 | Loss: 0.00014028
Iteration 16/25 | Loss: 0.00014028
Iteration 17/25 | Loss: 0.00014028
Iteration 18/25 | Loss: 0.00014028
Iteration 19/25 | Loss: 0.00014028
Iteration 20/25 | Loss: 0.00014028
Iteration 21/25 | Loss: 0.00014028
Iteration 22/25 | Loss: 0.00014028
Iteration 23/25 | Loss: 0.00014028
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.00014028306759428233, 0.00014028306759428233, 0.00014028306759428233, 0.00014028306759428233, 0.00014028306759428233]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00014028306759428233

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00014028
Iteration 2/1000 | Loss: 0.00001796
Iteration 3/1000 | Loss: 0.00001370
Iteration 4/1000 | Loss: 0.00001272
Iteration 5/1000 | Loss: 0.00001200
Iteration 6/1000 | Loss: 0.00001149
Iteration 7/1000 | Loss: 0.00001126
Iteration 8/1000 | Loss: 0.00001114
Iteration 9/1000 | Loss: 0.00001112
Iteration 10/1000 | Loss: 0.00001106
Iteration 11/1000 | Loss: 0.00001106
Iteration 12/1000 | Loss: 0.00001103
Iteration 13/1000 | Loss: 0.00001101
Iteration 14/1000 | Loss: 0.00001092
Iteration 15/1000 | Loss: 0.00001092
Iteration 16/1000 | Loss: 0.00001092
Iteration 17/1000 | Loss: 0.00001091
Iteration 18/1000 | Loss: 0.00001090
Iteration 19/1000 | Loss: 0.00001087
Iteration 20/1000 | Loss: 0.00001084
Iteration 21/1000 | Loss: 0.00001084
Iteration 22/1000 | Loss: 0.00001083
Iteration 23/1000 | Loss: 0.00001083
Iteration 24/1000 | Loss: 0.00001082
Iteration 25/1000 | Loss: 0.00001082
Iteration 26/1000 | Loss: 0.00001082
Iteration 27/1000 | Loss: 0.00001082
Iteration 28/1000 | Loss: 0.00001082
Iteration 29/1000 | Loss: 0.00001082
Iteration 30/1000 | Loss: 0.00001079
Iteration 31/1000 | Loss: 0.00001079
Iteration 32/1000 | Loss: 0.00001078
Iteration 33/1000 | Loss: 0.00001078
Iteration 34/1000 | Loss: 0.00001077
Iteration 35/1000 | Loss: 0.00001077
Iteration 36/1000 | Loss: 0.00001077
Iteration 37/1000 | Loss: 0.00001077
Iteration 38/1000 | Loss: 0.00001076
Iteration 39/1000 | Loss: 0.00001076
Iteration 40/1000 | Loss: 0.00001075
Iteration 41/1000 | Loss: 0.00001075
Iteration 42/1000 | Loss: 0.00001075
Iteration 43/1000 | Loss: 0.00001074
Iteration 44/1000 | Loss: 0.00001074
Iteration 45/1000 | Loss: 0.00001073
Iteration 46/1000 | Loss: 0.00001073
Iteration 47/1000 | Loss: 0.00001073
Iteration 48/1000 | Loss: 0.00001073
Iteration 49/1000 | Loss: 0.00001073
Iteration 50/1000 | Loss: 0.00001073
Iteration 51/1000 | Loss: 0.00001072
Iteration 52/1000 | Loss: 0.00001071
Iteration 53/1000 | Loss: 0.00001071
Iteration 54/1000 | Loss: 0.00001071
Iteration 55/1000 | Loss: 0.00001070
Iteration 56/1000 | Loss: 0.00001070
Iteration 57/1000 | Loss: 0.00001070
Iteration 58/1000 | Loss: 0.00001069
Iteration 59/1000 | Loss: 0.00001069
Iteration 60/1000 | Loss: 0.00001068
Iteration 61/1000 | Loss: 0.00001068
Iteration 62/1000 | Loss: 0.00001068
Iteration 63/1000 | Loss: 0.00001068
Iteration 64/1000 | Loss: 0.00001067
Iteration 65/1000 | Loss: 0.00001067
Iteration 66/1000 | Loss: 0.00001067
Iteration 67/1000 | Loss: 0.00001066
Iteration 68/1000 | Loss: 0.00001066
Iteration 69/1000 | Loss: 0.00001066
Iteration 70/1000 | Loss: 0.00001066
Iteration 71/1000 | Loss: 0.00001066
Iteration 72/1000 | Loss: 0.00001066
Iteration 73/1000 | Loss: 0.00001066
Iteration 74/1000 | Loss: 0.00001065
Iteration 75/1000 | Loss: 0.00001065
Iteration 76/1000 | Loss: 0.00001064
Iteration 77/1000 | Loss: 0.00001064
Iteration 78/1000 | Loss: 0.00001064
Iteration 79/1000 | Loss: 0.00001064
Iteration 80/1000 | Loss: 0.00001064
Iteration 81/1000 | Loss: 0.00001064
Iteration 82/1000 | Loss: 0.00001064
Iteration 83/1000 | Loss: 0.00001063
Iteration 84/1000 | Loss: 0.00001063
Iteration 85/1000 | Loss: 0.00001063
Iteration 86/1000 | Loss: 0.00001063
Iteration 87/1000 | Loss: 0.00001063
Iteration 88/1000 | Loss: 0.00001063
Iteration 89/1000 | Loss: 0.00001062
Iteration 90/1000 | Loss: 0.00001062
Iteration 91/1000 | Loss: 0.00001062
Iteration 92/1000 | Loss: 0.00001062
Iteration 93/1000 | Loss: 0.00001062
Iteration 94/1000 | Loss: 0.00001062
Iteration 95/1000 | Loss: 0.00001062
Iteration 96/1000 | Loss: 0.00001062
Iteration 97/1000 | Loss: 0.00001062
Iteration 98/1000 | Loss: 0.00001061
Iteration 99/1000 | Loss: 0.00001061
Iteration 100/1000 | Loss: 0.00001061
Iteration 101/1000 | Loss: 0.00001060
Iteration 102/1000 | Loss: 0.00001060
Iteration 103/1000 | Loss: 0.00001060
Iteration 104/1000 | Loss: 0.00001060
Iteration 105/1000 | Loss: 0.00001060
Iteration 106/1000 | Loss: 0.00001060
Iteration 107/1000 | Loss: 0.00001059
Iteration 108/1000 | Loss: 0.00001056
Iteration 109/1000 | Loss: 0.00001056
Iteration 110/1000 | Loss: 0.00001056
Iteration 111/1000 | Loss: 0.00001056
Iteration 112/1000 | Loss: 0.00001056
Iteration 113/1000 | Loss: 0.00001056
Iteration 114/1000 | Loss: 0.00001056
Iteration 115/1000 | Loss: 0.00001056
Iteration 116/1000 | Loss: 0.00001056
Iteration 117/1000 | Loss: 0.00001056
Iteration 118/1000 | Loss: 0.00001056
Iteration 119/1000 | Loss: 0.00001056
Iteration 120/1000 | Loss: 0.00001056
Iteration 121/1000 | Loss: 0.00001056
Iteration 122/1000 | Loss: 0.00001056
Iteration 123/1000 | Loss: 0.00001055
Iteration 124/1000 | Loss: 0.00001055
Iteration 125/1000 | Loss: 0.00001055
Iteration 126/1000 | Loss: 0.00001054
Iteration 127/1000 | Loss: 0.00001054
Iteration 128/1000 | Loss: 0.00001054
Iteration 129/1000 | Loss: 0.00001053
Iteration 130/1000 | Loss: 0.00001053
Iteration 131/1000 | Loss: 0.00001053
Iteration 132/1000 | Loss: 0.00001053
Iteration 133/1000 | Loss: 0.00001053
Iteration 134/1000 | Loss: 0.00001053
Iteration 135/1000 | Loss: 0.00001053
Iteration 136/1000 | Loss: 0.00001053
Iteration 137/1000 | Loss: 0.00001053
Iteration 138/1000 | Loss: 0.00001053
Iteration 139/1000 | Loss: 0.00001053
Iteration 140/1000 | Loss: 0.00001053
Iteration 141/1000 | Loss: 0.00001053
Iteration 142/1000 | Loss: 0.00001053
Iteration 143/1000 | Loss: 0.00001053
Iteration 144/1000 | Loss: 0.00001053
Iteration 145/1000 | Loss: 0.00001053
Iteration 146/1000 | Loss: 0.00001053
Iteration 147/1000 | Loss: 0.00001053
Iteration 148/1000 | Loss: 0.00001053
Iteration 149/1000 | Loss: 0.00001053
Iteration 150/1000 | Loss: 0.00001053
Iteration 151/1000 | Loss: 0.00001053
Iteration 152/1000 | Loss: 0.00001053
Iteration 153/1000 | Loss: 0.00001053
Iteration 154/1000 | Loss: 0.00001053
Iteration 155/1000 | Loss: 0.00001053
Iteration 156/1000 | Loss: 0.00001053
Iteration 157/1000 | Loss: 0.00001053
Iteration 158/1000 | Loss: 0.00001053
Iteration 159/1000 | Loss: 0.00001053
Iteration 160/1000 | Loss: 0.00001053
Iteration 161/1000 | Loss: 0.00001053
Iteration 162/1000 | Loss: 0.00001053
Iteration 163/1000 | Loss: 0.00001053
Iteration 164/1000 | Loss: 0.00001053
Iteration 165/1000 | Loss: 0.00001053
Iteration 166/1000 | Loss: 0.00001053
Iteration 167/1000 | Loss: 0.00001053
Iteration 168/1000 | Loss: 0.00001053
Iteration 169/1000 | Loss: 0.00001053
Iteration 170/1000 | Loss: 0.00001053
Iteration 171/1000 | Loss: 0.00001053
Iteration 172/1000 | Loss: 0.00001053
Iteration 173/1000 | Loss: 0.00001053
Iteration 174/1000 | Loss: 0.00001053
Iteration 175/1000 | Loss: 0.00001053
Iteration 176/1000 | Loss: 0.00001053
Iteration 177/1000 | Loss: 0.00001053
Iteration 178/1000 | Loss: 0.00001053
Iteration 179/1000 | Loss: 0.00001053
Iteration 180/1000 | Loss: 0.00001053
Iteration 181/1000 | Loss: 0.00001053
Iteration 182/1000 | Loss: 0.00001053
Iteration 183/1000 | Loss: 0.00001053
Iteration 184/1000 | Loss: 0.00001053
Iteration 185/1000 | Loss: 0.00001053
Iteration 186/1000 | Loss: 0.00001053
Iteration 187/1000 | Loss: 0.00001053
Iteration 188/1000 | Loss: 0.00001053
Iteration 189/1000 | Loss: 0.00001053
Iteration 190/1000 | Loss: 0.00001053
Iteration 191/1000 | Loss: 0.00001053
Iteration 192/1000 | Loss: 0.00001053
Iteration 193/1000 | Loss: 0.00001053
Iteration 194/1000 | Loss: 0.00001053
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 194. Stopping optimization.
Last 5 losses: [1.053481173585169e-05, 1.053481173585169e-05, 1.053481173585169e-05, 1.053481173585169e-05, 1.053481173585169e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.053481173585169e-05

Optimization complete. Final v2v error: 2.7596044540405273 mm

Highest mean error: 2.8879430294036865 mm for frame 76

Lowest mean error: 2.6692802906036377 mm for frame 116

Saving results

Total time: 34.500497579574585
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ben_posed_008/1066/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_008/1066.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_008/1066
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00864182
Iteration 2/25 | Loss: 0.00095105
Iteration 3/25 | Loss: 0.00061244
Iteration 4/25 | Loss: 0.00057431
Iteration 5/25 | Loss: 0.00056386
Iteration 6/25 | Loss: 0.00056122
Iteration 7/25 | Loss: 0.00056042
Iteration 8/25 | Loss: 0.00056041
Iteration 9/25 | Loss: 0.00056041
Iteration 10/25 | Loss: 0.00056041
Iteration 11/25 | Loss: 0.00056041
Iteration 12/25 | Loss: 0.00056041
Iteration 13/25 | Loss: 0.00056041
Iteration 14/25 | Loss: 0.00056041
Iteration 15/25 | Loss: 0.00056041
Iteration 16/25 | Loss: 0.00056041
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0005604100879281759, 0.0005604100879281759, 0.0005604100879281759, 0.0005604100879281759, 0.0005604100879281759]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005604100879281759

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.92284751
Iteration 2/25 | Loss: 0.00018295
Iteration 3/25 | Loss: 0.00018294
Iteration 4/25 | Loss: 0.00018294
Iteration 5/25 | Loss: 0.00018294
Iteration 6/25 | Loss: 0.00018294
Iteration 7/25 | Loss: 0.00018294
Iteration 8/25 | Loss: 0.00018294
Iteration 9/25 | Loss: 0.00018294
Iteration 10/25 | Loss: 0.00018294
Iteration 11/25 | Loss: 0.00018294
Iteration 12/25 | Loss: 0.00018294
Iteration 13/25 | Loss: 0.00018294
Iteration 14/25 | Loss: 0.00018294
Iteration 15/25 | Loss: 0.00018294
Iteration 16/25 | Loss: 0.00018294
Iteration 17/25 | Loss: 0.00018294
Iteration 18/25 | Loss: 0.00018294
Iteration 19/25 | Loss: 0.00018294
Iteration 20/25 | Loss: 0.00018294
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.00018293883476871997, 0.00018293883476871997, 0.00018293883476871997, 0.00018293883476871997, 0.00018293883476871997]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00018293883476871997

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00018294
Iteration 2/1000 | Loss: 0.00002157
Iteration 3/1000 | Loss: 0.00001469
Iteration 4/1000 | Loss: 0.00001365
Iteration 5/1000 | Loss: 0.00001297
Iteration 6/1000 | Loss: 0.00001267
Iteration 7/1000 | Loss: 0.00001232
Iteration 8/1000 | Loss: 0.00001213
Iteration 9/1000 | Loss: 0.00001198
Iteration 10/1000 | Loss: 0.00001196
Iteration 11/1000 | Loss: 0.00001195
Iteration 12/1000 | Loss: 0.00001195
Iteration 13/1000 | Loss: 0.00001194
Iteration 14/1000 | Loss: 0.00001193
Iteration 15/1000 | Loss: 0.00001187
Iteration 16/1000 | Loss: 0.00001184
Iteration 17/1000 | Loss: 0.00001180
Iteration 18/1000 | Loss: 0.00001179
Iteration 19/1000 | Loss: 0.00001177
Iteration 20/1000 | Loss: 0.00001176
Iteration 21/1000 | Loss: 0.00001173
Iteration 22/1000 | Loss: 0.00001173
Iteration 23/1000 | Loss: 0.00001173
Iteration 24/1000 | Loss: 0.00001173
Iteration 25/1000 | Loss: 0.00001172
Iteration 26/1000 | Loss: 0.00001172
Iteration 27/1000 | Loss: 0.00001172
Iteration 28/1000 | Loss: 0.00001171
Iteration 29/1000 | Loss: 0.00001171
Iteration 30/1000 | Loss: 0.00001168
Iteration 31/1000 | Loss: 0.00001168
Iteration 32/1000 | Loss: 0.00001168
Iteration 33/1000 | Loss: 0.00001168
Iteration 34/1000 | Loss: 0.00001168
Iteration 35/1000 | Loss: 0.00001167
Iteration 36/1000 | Loss: 0.00001167
Iteration 37/1000 | Loss: 0.00001167
Iteration 38/1000 | Loss: 0.00001166
Iteration 39/1000 | Loss: 0.00001166
Iteration 40/1000 | Loss: 0.00001165
Iteration 41/1000 | Loss: 0.00001165
Iteration 42/1000 | Loss: 0.00001164
Iteration 43/1000 | Loss: 0.00001164
Iteration 44/1000 | Loss: 0.00001163
Iteration 45/1000 | Loss: 0.00001163
Iteration 46/1000 | Loss: 0.00001162
Iteration 47/1000 | Loss: 0.00001162
Iteration 48/1000 | Loss: 0.00001162
Iteration 49/1000 | Loss: 0.00001162
Iteration 50/1000 | Loss: 0.00001161
Iteration 51/1000 | Loss: 0.00001160
Iteration 52/1000 | Loss: 0.00001160
Iteration 53/1000 | Loss: 0.00001159
Iteration 54/1000 | Loss: 0.00001159
Iteration 55/1000 | Loss: 0.00001158
Iteration 56/1000 | Loss: 0.00001158
Iteration 57/1000 | Loss: 0.00001157
Iteration 58/1000 | Loss: 0.00001156
Iteration 59/1000 | Loss: 0.00001156
Iteration 60/1000 | Loss: 0.00001156
Iteration 61/1000 | Loss: 0.00001156
Iteration 62/1000 | Loss: 0.00001156
Iteration 63/1000 | Loss: 0.00001156
Iteration 64/1000 | Loss: 0.00001156
Iteration 65/1000 | Loss: 0.00001156
Iteration 66/1000 | Loss: 0.00001156
Iteration 67/1000 | Loss: 0.00001156
Iteration 68/1000 | Loss: 0.00001155
Iteration 69/1000 | Loss: 0.00001155
Iteration 70/1000 | Loss: 0.00001154
Iteration 71/1000 | Loss: 0.00001154
Iteration 72/1000 | Loss: 0.00001153
Iteration 73/1000 | Loss: 0.00001153
Iteration 74/1000 | Loss: 0.00001152
Iteration 75/1000 | Loss: 0.00001152
Iteration 76/1000 | Loss: 0.00001152
Iteration 77/1000 | Loss: 0.00001151
Iteration 78/1000 | Loss: 0.00001151
Iteration 79/1000 | Loss: 0.00001151
Iteration 80/1000 | Loss: 0.00001151
Iteration 81/1000 | Loss: 0.00001151
Iteration 82/1000 | Loss: 0.00001150
Iteration 83/1000 | Loss: 0.00001150
Iteration 84/1000 | Loss: 0.00001150
Iteration 85/1000 | Loss: 0.00001150
Iteration 86/1000 | Loss: 0.00001150
Iteration 87/1000 | Loss: 0.00001150
Iteration 88/1000 | Loss: 0.00001150
Iteration 89/1000 | Loss: 0.00001149
Iteration 90/1000 | Loss: 0.00001149
Iteration 91/1000 | Loss: 0.00001149
Iteration 92/1000 | Loss: 0.00001149
Iteration 93/1000 | Loss: 0.00001149
Iteration 94/1000 | Loss: 0.00001148
Iteration 95/1000 | Loss: 0.00001148
Iteration 96/1000 | Loss: 0.00001148
Iteration 97/1000 | Loss: 0.00001148
Iteration 98/1000 | Loss: 0.00001148
Iteration 99/1000 | Loss: 0.00001147
Iteration 100/1000 | Loss: 0.00001147
Iteration 101/1000 | Loss: 0.00001147
Iteration 102/1000 | Loss: 0.00001147
Iteration 103/1000 | Loss: 0.00001147
Iteration 104/1000 | Loss: 0.00001147
Iteration 105/1000 | Loss: 0.00001146
Iteration 106/1000 | Loss: 0.00001146
Iteration 107/1000 | Loss: 0.00001146
Iteration 108/1000 | Loss: 0.00001146
Iteration 109/1000 | Loss: 0.00001146
Iteration 110/1000 | Loss: 0.00001145
Iteration 111/1000 | Loss: 0.00001145
Iteration 112/1000 | Loss: 0.00001145
Iteration 113/1000 | Loss: 0.00001145
Iteration 114/1000 | Loss: 0.00001145
Iteration 115/1000 | Loss: 0.00001145
Iteration 116/1000 | Loss: 0.00001145
Iteration 117/1000 | Loss: 0.00001145
Iteration 118/1000 | Loss: 0.00001145
Iteration 119/1000 | Loss: 0.00001145
Iteration 120/1000 | Loss: 0.00001145
Iteration 121/1000 | Loss: 0.00001145
Iteration 122/1000 | Loss: 0.00001144
Iteration 123/1000 | Loss: 0.00001144
Iteration 124/1000 | Loss: 0.00001144
Iteration 125/1000 | Loss: 0.00001144
Iteration 126/1000 | Loss: 0.00001144
Iteration 127/1000 | Loss: 0.00001144
Iteration 128/1000 | Loss: 0.00001144
Iteration 129/1000 | Loss: 0.00001144
Iteration 130/1000 | Loss: 0.00001144
Iteration 131/1000 | Loss: 0.00001144
Iteration 132/1000 | Loss: 0.00001144
Iteration 133/1000 | Loss: 0.00001144
Iteration 134/1000 | Loss: 0.00001144
Iteration 135/1000 | Loss: 0.00001144
Iteration 136/1000 | Loss: 0.00001144
Iteration 137/1000 | Loss: 0.00001144
Iteration 138/1000 | Loss: 0.00001144
Iteration 139/1000 | Loss: 0.00001144
Iteration 140/1000 | Loss: 0.00001144
Iteration 141/1000 | Loss: 0.00001144
Iteration 142/1000 | Loss: 0.00001144
Iteration 143/1000 | Loss: 0.00001144
Iteration 144/1000 | Loss: 0.00001144
Iteration 145/1000 | Loss: 0.00001144
Iteration 146/1000 | Loss: 0.00001144
Iteration 147/1000 | Loss: 0.00001144
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 147. Stopping optimization.
Last 5 losses: [1.1444827578088734e-05, 1.1444827578088734e-05, 1.1444827578088734e-05, 1.1444827578088734e-05, 1.1444827578088734e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1444827578088734e-05

Optimization complete. Final v2v error: 2.8930323123931885 mm

Highest mean error: 3.3079886436462402 mm for frame 172

Lowest mean error: 2.6269803047180176 mm for frame 1

Saving results

Total time: 40.0892276763916
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ben_posed_008/1046/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_008/1046.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_008/1046
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01261132
Iteration 2/25 | Loss: 0.01261132
Iteration 3/25 | Loss: 0.00362751
Iteration 4/25 | Loss: 0.00248798
Iteration 5/25 | Loss: 0.00228374
Iteration 6/25 | Loss: 0.00197410
Iteration 7/25 | Loss: 0.00159789
Iteration 8/25 | Loss: 0.00145757
Iteration 9/25 | Loss: 0.00140315
Iteration 10/25 | Loss: 0.00136261
Iteration 11/25 | Loss: 0.00135744
Iteration 12/25 | Loss: 0.00134811
Iteration 13/25 | Loss: 0.00133473
Iteration 14/25 | Loss: 0.00132596
Iteration 15/25 | Loss: 0.00129852
Iteration 16/25 | Loss: 0.00130500
Iteration 17/25 | Loss: 0.00129704
Iteration 18/25 | Loss: 0.00130168
Iteration 19/25 | Loss: 0.00129343
Iteration 20/25 | Loss: 0.00129464
Iteration 21/25 | Loss: 0.00129902
Iteration 22/25 | Loss: 0.00129968
Iteration 23/25 | Loss: 0.00129882
Iteration 24/25 | Loss: 0.00129422
Iteration 25/25 | Loss: 0.00128526

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.60223490
Iteration 2/25 | Loss: 0.00484063
Iteration 3/25 | Loss: 0.00471286
Iteration 4/25 | Loss: 0.00471285
Iteration 5/25 | Loss: 0.00471285
Iteration 6/25 | Loss: 0.00471285
Iteration 7/25 | Loss: 0.00471285
Iteration 8/25 | Loss: 0.00471285
Iteration 9/25 | Loss: 0.00471285
Iteration 10/25 | Loss: 0.00471285
Iteration 11/25 | Loss: 0.00471285
Iteration 12/25 | Loss: 0.00471285
Iteration 13/25 | Loss: 0.00471285
Iteration 14/25 | Loss: 0.00471285
Iteration 15/25 | Loss: 0.00471285
Iteration 16/25 | Loss: 0.00471285
Iteration 17/25 | Loss: 0.00471285
Iteration 18/25 | Loss: 0.00471285
Iteration 19/25 | Loss: 0.00471285
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.004712849389761686, 0.004712849389761686, 0.004712849389761686, 0.004712849389761686, 0.004712849389761686]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.004712849389761686

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00471285
Iteration 2/1000 | Loss: 0.00193021
Iteration 3/1000 | Loss: 0.00291247
Iteration 4/1000 | Loss: 0.00249380
Iteration 5/1000 | Loss: 0.00265134
Iteration 6/1000 | Loss: 0.00203105
Iteration 7/1000 | Loss: 0.00099386
Iteration 8/1000 | Loss: 0.00161501
Iteration 9/1000 | Loss: 0.00177252
Iteration 10/1000 | Loss: 0.00164444
Iteration 11/1000 | Loss: 0.00373245
Iteration 12/1000 | Loss: 0.00149972
Iteration 13/1000 | Loss: 0.00305653
Iteration 14/1000 | Loss: 0.00124105
Iteration 15/1000 | Loss: 0.00506581
Iteration 16/1000 | Loss: 0.00292495
Iteration 17/1000 | Loss: 0.00496829
Iteration 18/1000 | Loss: 0.00233650
Iteration 19/1000 | Loss: 0.00157113
Iteration 20/1000 | Loss: 0.00139292
Iteration 21/1000 | Loss: 0.00133124
Iteration 22/1000 | Loss: 0.00076046
Iteration 23/1000 | Loss: 0.00109785
Iteration 24/1000 | Loss: 0.00234967
Iteration 25/1000 | Loss: 0.00149345
Iteration 26/1000 | Loss: 0.00110289
Iteration 27/1000 | Loss: 0.00097878
Iteration 28/1000 | Loss: 0.00245098
Iteration 29/1000 | Loss: 0.00155797
Iteration 30/1000 | Loss: 0.00054922
Iteration 31/1000 | Loss: 0.00117949
Iteration 32/1000 | Loss: 0.00322255
Iteration 33/1000 | Loss: 0.00246828
Iteration 34/1000 | Loss: 0.00117071
Iteration 35/1000 | Loss: 0.00081776
Iteration 36/1000 | Loss: 0.00058904
Iteration 37/1000 | Loss: 0.00113257
Iteration 38/1000 | Loss: 0.00055479
Iteration 39/1000 | Loss: 0.00057094
Iteration 40/1000 | Loss: 0.00057649
Iteration 41/1000 | Loss: 0.00066040
Iteration 42/1000 | Loss: 0.00068544
Iteration 43/1000 | Loss: 0.00150620
Iteration 44/1000 | Loss: 0.00075474
Iteration 45/1000 | Loss: 0.00185371
Iteration 46/1000 | Loss: 0.00137689
Iteration 47/1000 | Loss: 0.00096164
Iteration 48/1000 | Loss: 0.00194812
Iteration 49/1000 | Loss: 0.00162390
Iteration 50/1000 | Loss: 0.00130391
Iteration 51/1000 | Loss: 0.00144197
Iteration 52/1000 | Loss: 0.00120615
Iteration 53/1000 | Loss: 0.00119359
Iteration 54/1000 | Loss: 0.00131578
Iteration 55/1000 | Loss: 0.00201695
Iteration 56/1000 | Loss: 0.00099190
Iteration 57/1000 | Loss: 0.00053572
Iteration 58/1000 | Loss: 0.00044380
Iteration 59/1000 | Loss: 0.00084587
Iteration 60/1000 | Loss: 0.00093559
Iteration 61/1000 | Loss: 0.00105090
Iteration 62/1000 | Loss: 0.00114734
Iteration 63/1000 | Loss: 0.00101458
Iteration 64/1000 | Loss: 0.00099765
Iteration 65/1000 | Loss: 0.00084914
Iteration 66/1000 | Loss: 0.00089126
Iteration 67/1000 | Loss: 0.00086396
Iteration 68/1000 | Loss: 0.00096521
Iteration 69/1000 | Loss: 0.00092466
Iteration 70/1000 | Loss: 0.00239938
Iteration 71/1000 | Loss: 0.00076460
Iteration 72/1000 | Loss: 0.00101051
Iteration 73/1000 | Loss: 0.00082319
Iteration 74/1000 | Loss: 0.00076074
Iteration 75/1000 | Loss: 0.00091173
Iteration 76/1000 | Loss: 0.00164235
Iteration 77/1000 | Loss: 0.00076486
Iteration 78/1000 | Loss: 0.00053810
Iteration 79/1000 | Loss: 0.00072202
Iteration 80/1000 | Loss: 0.00090068
Iteration 81/1000 | Loss: 0.00070654
Iteration 82/1000 | Loss: 0.00072726
Iteration 83/1000 | Loss: 0.00219608
Iteration 84/1000 | Loss: 0.00067310
Iteration 85/1000 | Loss: 0.00132444
Iteration 86/1000 | Loss: 0.00059027
Iteration 87/1000 | Loss: 0.00041761
Iteration 88/1000 | Loss: 0.00029736
Iteration 89/1000 | Loss: 0.00049618
Iteration 90/1000 | Loss: 0.00041866
Iteration 91/1000 | Loss: 0.00060976
Iteration 92/1000 | Loss: 0.00053177
Iteration 93/1000 | Loss: 0.00056429
Iteration 94/1000 | Loss: 0.00054829
Iteration 95/1000 | Loss: 0.00057183
Iteration 96/1000 | Loss: 0.00064298
Iteration 97/1000 | Loss: 0.00114648
Iteration 98/1000 | Loss: 0.00052440
Iteration 99/1000 | Loss: 0.00067626
Iteration 100/1000 | Loss: 0.00042037
Iteration 101/1000 | Loss: 0.00033729
Iteration 102/1000 | Loss: 0.00038793
Iteration 103/1000 | Loss: 0.00026920
Iteration 104/1000 | Loss: 0.00113308
Iteration 105/1000 | Loss: 0.00044152
Iteration 106/1000 | Loss: 0.00177501
Iteration 107/1000 | Loss: 0.00123149
Iteration 108/1000 | Loss: 0.00099660
Iteration 109/1000 | Loss: 0.00082577
Iteration 110/1000 | Loss: 0.00037776
Iteration 111/1000 | Loss: 0.00033114
Iteration 112/1000 | Loss: 0.00037549
Iteration 113/1000 | Loss: 0.00052978
Iteration 114/1000 | Loss: 0.00033005
Iteration 115/1000 | Loss: 0.00039523
Iteration 116/1000 | Loss: 0.00043779
Iteration 117/1000 | Loss: 0.00034231
Iteration 118/1000 | Loss: 0.00026917
Iteration 119/1000 | Loss: 0.00052937
Iteration 120/1000 | Loss: 0.00037676
Iteration 121/1000 | Loss: 0.00032586
Iteration 122/1000 | Loss: 0.00030283
Iteration 123/1000 | Loss: 0.00030839
Iteration 124/1000 | Loss: 0.00032228
Iteration 125/1000 | Loss: 0.00031927
Iteration 126/1000 | Loss: 0.00033790
Iteration 127/1000 | Loss: 0.00028080
Iteration 128/1000 | Loss: 0.00023717
Iteration 129/1000 | Loss: 0.00026311
Iteration 130/1000 | Loss: 0.00018464
Iteration 131/1000 | Loss: 0.00020468
Iteration 132/1000 | Loss: 0.00047448
Iteration 133/1000 | Loss: 0.00037465
Iteration 134/1000 | Loss: 0.00032557
Iteration 135/1000 | Loss: 0.00019367
Iteration 136/1000 | Loss: 0.00017352
Iteration 137/1000 | Loss: 0.00026307
Iteration 138/1000 | Loss: 0.00030203
Iteration 139/1000 | Loss: 0.00036878
Iteration 140/1000 | Loss: 0.00052633
Iteration 141/1000 | Loss: 0.00033879
Iteration 142/1000 | Loss: 0.00059066
Iteration 143/1000 | Loss: 0.00051050
Iteration 144/1000 | Loss: 0.00049775
Iteration 145/1000 | Loss: 0.00052916
Iteration 146/1000 | Loss: 0.00050950
Iteration 147/1000 | Loss: 0.00054156
Iteration 148/1000 | Loss: 0.00076677
Iteration 149/1000 | Loss: 0.00040806
Iteration 150/1000 | Loss: 0.00025373
Iteration 151/1000 | Loss: 0.00027109
Iteration 152/1000 | Loss: 0.00039985
Iteration 153/1000 | Loss: 0.00041058
Iteration 154/1000 | Loss: 0.00039272
Iteration 155/1000 | Loss: 0.00033275
Iteration 156/1000 | Loss: 0.00021183
Iteration 157/1000 | Loss: 0.00023441
Iteration 158/1000 | Loss: 0.00019563
Iteration 159/1000 | Loss: 0.00035368
Iteration 160/1000 | Loss: 0.00054665
Iteration 161/1000 | Loss: 0.00035862
Iteration 162/1000 | Loss: 0.00031359
Iteration 163/1000 | Loss: 0.00093436
Iteration 164/1000 | Loss: 0.00049646
Iteration 165/1000 | Loss: 0.00034854
Iteration 166/1000 | Loss: 0.00035899
Iteration 167/1000 | Loss: 0.00038918
Iteration 168/1000 | Loss: 0.00035035
Iteration 169/1000 | Loss: 0.00037245
Iteration 170/1000 | Loss: 0.00024498
Iteration 171/1000 | Loss: 0.00034885
Iteration 172/1000 | Loss: 0.00019659
Iteration 173/1000 | Loss: 0.00052499
Iteration 174/1000 | Loss: 0.00034939
Iteration 175/1000 | Loss: 0.00010907
Iteration 176/1000 | Loss: 0.00018622
Iteration 177/1000 | Loss: 0.00023241
Iteration 178/1000 | Loss: 0.00037243
Iteration 179/1000 | Loss: 0.00041915
Iteration 180/1000 | Loss: 0.00033160
Iteration 181/1000 | Loss: 0.00037178
Iteration 182/1000 | Loss: 0.00223039
Iteration 183/1000 | Loss: 0.00109011
Iteration 184/1000 | Loss: 0.00029004
Iteration 185/1000 | Loss: 0.00015587
Iteration 186/1000 | Loss: 0.00041342
Iteration 187/1000 | Loss: 0.00088283
Iteration 188/1000 | Loss: 0.00067159
Iteration 189/1000 | Loss: 0.00062932
Iteration 190/1000 | Loss: 0.00033847
Iteration 191/1000 | Loss: 0.00039959
Iteration 192/1000 | Loss: 0.00039052
Iteration 193/1000 | Loss: 0.00024984
Iteration 194/1000 | Loss: 0.00040378
Iteration 195/1000 | Loss: 0.00036051
Iteration 196/1000 | Loss: 0.00035469
Iteration 197/1000 | Loss: 0.00032864
Iteration 198/1000 | Loss: 0.00041044
Iteration 199/1000 | Loss: 0.00059989
Iteration 200/1000 | Loss: 0.00037657
Iteration 201/1000 | Loss: 0.00039701
Iteration 202/1000 | Loss: 0.00100217
Iteration 203/1000 | Loss: 0.00039680
Iteration 204/1000 | Loss: 0.00042798
Iteration 205/1000 | Loss: 0.00039293
Iteration 206/1000 | Loss: 0.00041517
Iteration 207/1000 | Loss: 0.00036275
Iteration 208/1000 | Loss: 0.00044437
Iteration 209/1000 | Loss: 0.00044195
Iteration 210/1000 | Loss: 0.00037423
Iteration 211/1000 | Loss: 0.00024029
Iteration 212/1000 | Loss: 0.00030996
Iteration 213/1000 | Loss: 0.00015390
Iteration 214/1000 | Loss: 0.00040520
Iteration 215/1000 | Loss: 0.00030927
Iteration 216/1000 | Loss: 0.00026869
Iteration 217/1000 | Loss: 0.00116275
Iteration 218/1000 | Loss: 0.00191021
Iteration 219/1000 | Loss: 0.00169196
Iteration 220/1000 | Loss: 0.00110442
Iteration 221/1000 | Loss: 0.00103991
Iteration 222/1000 | Loss: 0.00029758
Iteration 223/1000 | Loss: 0.00031260
Iteration 224/1000 | Loss: 0.00026152
Iteration 225/1000 | Loss: 0.00026944
Iteration 226/1000 | Loss: 0.00021434
Iteration 227/1000 | Loss: 0.00038979
Iteration 228/1000 | Loss: 0.00048162
Iteration 229/1000 | Loss: 0.00040457
Iteration 230/1000 | Loss: 0.00038514
Iteration 231/1000 | Loss: 0.00024121
Iteration 232/1000 | Loss: 0.00029398
Iteration 233/1000 | Loss: 0.00025530
Iteration 234/1000 | Loss: 0.00022057
Iteration 235/1000 | Loss: 0.00017571
Iteration 236/1000 | Loss: 0.00019938
Iteration 237/1000 | Loss: 0.00030142
Iteration 238/1000 | Loss: 0.00026561
Iteration 239/1000 | Loss: 0.00029268
Iteration 240/1000 | Loss: 0.00176448
Iteration 241/1000 | Loss: 0.00077249
Iteration 242/1000 | Loss: 0.00027391
Iteration 243/1000 | Loss: 0.00028343
Iteration 244/1000 | Loss: 0.00187421
Iteration 245/1000 | Loss: 0.00077142
Iteration 246/1000 | Loss: 0.00029065
Iteration 247/1000 | Loss: 0.00037009
Iteration 248/1000 | Loss: 0.00028316
Iteration 249/1000 | Loss: 0.00030906
Iteration 250/1000 | Loss: 0.00023842
Iteration 251/1000 | Loss: 0.00009920
Iteration 252/1000 | Loss: 0.00009483
Iteration 253/1000 | Loss: 0.00083587
Iteration 254/1000 | Loss: 0.00009233
Iteration 255/1000 | Loss: 0.00028117
Iteration 256/1000 | Loss: 0.00021457
Iteration 257/1000 | Loss: 0.00012030
Iteration 258/1000 | Loss: 0.00012689
Iteration 259/1000 | Loss: 0.00011103
Iteration 260/1000 | Loss: 0.00008831
Iteration 261/1000 | Loss: 0.00016847
Iteration 262/1000 | Loss: 0.00163522
Iteration 263/1000 | Loss: 0.00106403
Iteration 264/1000 | Loss: 0.00064567
Iteration 265/1000 | Loss: 0.00012727
Iteration 266/1000 | Loss: 0.00009510
Iteration 267/1000 | Loss: 0.00116429
Iteration 268/1000 | Loss: 0.00050357
Iteration 269/1000 | Loss: 0.00009644
Iteration 270/1000 | Loss: 0.00008218
Iteration 271/1000 | Loss: 0.00007635
Iteration 272/1000 | Loss: 0.00007421
Iteration 273/1000 | Loss: 0.00117647
Iteration 274/1000 | Loss: 0.00085689
Iteration 275/1000 | Loss: 0.00018475
Iteration 276/1000 | Loss: 0.00089773
Iteration 277/1000 | Loss: 0.00049580
Iteration 278/1000 | Loss: 0.00088884
Iteration 279/1000 | Loss: 0.00010319
Iteration 280/1000 | Loss: 0.00008538
Iteration 281/1000 | Loss: 0.00022962
Iteration 282/1000 | Loss: 0.00017883
Iteration 283/1000 | Loss: 0.00007840
Iteration 284/1000 | Loss: 0.00158151
Iteration 285/1000 | Loss: 0.00031824
Iteration 286/1000 | Loss: 0.00037484
Iteration 287/1000 | Loss: 0.00060054
Iteration 288/1000 | Loss: 0.00044095
Iteration 289/1000 | Loss: 0.00039708
Iteration 290/1000 | Loss: 0.00090102
Iteration 291/1000 | Loss: 0.00009349
Iteration 292/1000 | Loss: 0.00007890
Iteration 293/1000 | Loss: 0.00007394
Iteration 294/1000 | Loss: 0.00007097
Iteration 295/1000 | Loss: 0.00070522
Iteration 296/1000 | Loss: 0.00019061
Iteration 297/1000 | Loss: 0.00007213
Iteration 298/1000 | Loss: 0.00006786
Iteration 299/1000 | Loss: 0.00013888
Iteration 300/1000 | Loss: 0.00006603
Iteration 301/1000 | Loss: 0.00006472
Iteration 302/1000 | Loss: 0.00082492
Iteration 303/1000 | Loss: 0.00006475
Iteration 304/1000 | Loss: 0.00006209
Iteration 305/1000 | Loss: 0.00005982
Iteration 306/1000 | Loss: 0.00005795
Iteration 307/1000 | Loss: 0.00005711
Iteration 308/1000 | Loss: 0.00005671
Iteration 309/1000 | Loss: 0.00081052
Iteration 310/1000 | Loss: 0.00008610
Iteration 311/1000 | Loss: 0.00007455
Iteration 312/1000 | Loss: 0.00007841
Iteration 313/1000 | Loss: 0.00007500
Iteration 314/1000 | Loss: 0.00007724
Iteration 315/1000 | Loss: 0.00079100
Iteration 316/1000 | Loss: 0.00058068
Iteration 317/1000 | Loss: 0.00076761
Iteration 318/1000 | Loss: 0.00032347
Iteration 319/1000 | Loss: 0.00085172
Iteration 320/1000 | Loss: 0.00063440
Iteration 321/1000 | Loss: 0.00025231
Iteration 322/1000 | Loss: 0.00031978
Iteration 323/1000 | Loss: 0.00007799
Iteration 324/1000 | Loss: 0.00006170
Iteration 325/1000 | Loss: 0.00005767
Iteration 326/1000 | Loss: 0.00005971
Iteration 327/1000 | Loss: 0.00005510
Iteration 328/1000 | Loss: 0.00005346
Iteration 329/1000 | Loss: 0.00005221
Iteration 330/1000 | Loss: 0.00005127
Iteration 331/1000 | Loss: 0.00065588
Iteration 332/1000 | Loss: 0.00005483
Iteration 333/1000 | Loss: 0.00005143
Iteration 334/1000 | Loss: 0.00004986
Iteration 335/1000 | Loss: 0.00004864
Iteration 336/1000 | Loss: 0.00004799
Iteration 337/1000 | Loss: 0.00004750
Iteration 338/1000 | Loss: 0.00004746
Iteration 339/1000 | Loss: 0.00076195
Iteration 340/1000 | Loss: 0.00136471
Iteration 341/1000 | Loss: 0.00127815
Iteration 342/1000 | Loss: 0.00007021
Iteration 343/1000 | Loss: 0.00005460
Iteration 344/1000 | Loss: 0.00004894
Iteration 345/1000 | Loss: 0.00078471
Iteration 346/1000 | Loss: 0.00006402
Iteration 347/1000 | Loss: 0.00005060
Iteration 348/1000 | Loss: 0.00004462
Iteration 349/1000 | Loss: 0.00004130
Iteration 350/1000 | Loss: 0.00003964
Iteration 351/1000 | Loss: 0.00068646
Iteration 352/1000 | Loss: 0.00004149
Iteration 353/1000 | Loss: 0.00003774
Iteration 354/1000 | Loss: 0.00003608
Iteration 355/1000 | Loss: 0.00003457
Iteration 356/1000 | Loss: 0.00003377
Iteration 357/1000 | Loss: 0.00003326
Iteration 358/1000 | Loss: 0.00003297
Iteration 359/1000 | Loss: 0.00003280
Iteration 360/1000 | Loss: 0.00003278
Iteration 361/1000 | Loss: 0.00003272
Iteration 362/1000 | Loss: 0.00003255
Iteration 363/1000 | Loss: 0.00003253
Iteration 364/1000 | Loss: 0.00003251
Iteration 365/1000 | Loss: 0.00003249
Iteration 366/1000 | Loss: 0.00003248
Iteration 367/1000 | Loss: 0.00003248
Iteration 368/1000 | Loss: 0.00003247
Iteration 369/1000 | Loss: 0.00003247
Iteration 370/1000 | Loss: 0.00003246
Iteration 371/1000 | Loss: 0.00003246
Iteration 372/1000 | Loss: 0.00003246
Iteration 373/1000 | Loss: 0.00003246
Iteration 374/1000 | Loss: 0.00003246
Iteration 375/1000 | Loss: 0.00003246
Iteration 376/1000 | Loss: 0.00003246
Iteration 377/1000 | Loss: 0.00003246
Iteration 378/1000 | Loss: 0.00003245
Iteration 379/1000 | Loss: 0.00003245
Iteration 380/1000 | Loss: 0.00003244
Iteration 381/1000 | Loss: 0.00003243
Iteration 382/1000 | Loss: 0.00003243
Iteration 383/1000 | Loss: 0.00003242
Iteration 384/1000 | Loss: 0.00003242
Iteration 385/1000 | Loss: 0.00003242
Iteration 386/1000 | Loss: 0.00003242
Iteration 387/1000 | Loss: 0.00003241
Iteration 388/1000 | Loss: 0.00003241
Iteration 389/1000 | Loss: 0.00003241
Iteration 390/1000 | Loss: 0.00003241
Iteration 391/1000 | Loss: 0.00003241
Iteration 392/1000 | Loss: 0.00003240
Iteration 393/1000 | Loss: 0.00003240
Iteration 394/1000 | Loss: 0.00003240
Iteration 395/1000 | Loss: 0.00003240
Iteration 396/1000 | Loss: 0.00003240
Iteration 397/1000 | Loss: 0.00003239
Iteration 398/1000 | Loss: 0.00003239
Iteration 399/1000 | Loss: 0.00003239
Iteration 400/1000 | Loss: 0.00003239
Iteration 401/1000 | Loss: 0.00003239
Iteration 402/1000 | Loss: 0.00003239
Iteration 403/1000 | Loss: 0.00003239
Iteration 404/1000 | Loss: 0.00003238
Iteration 405/1000 | Loss: 0.00003238
Iteration 406/1000 | Loss: 0.00003238
Iteration 407/1000 | Loss: 0.00003238
Iteration 408/1000 | Loss: 0.00003238
Iteration 409/1000 | Loss: 0.00003238
Iteration 410/1000 | Loss: 0.00003238
Iteration 411/1000 | Loss: 0.00003237
Iteration 412/1000 | Loss: 0.00003237
Iteration 413/1000 | Loss: 0.00003237
Iteration 414/1000 | Loss: 0.00003237
Iteration 415/1000 | Loss: 0.00003237
Iteration 416/1000 | Loss: 0.00003237
Iteration 417/1000 | Loss: 0.00003236
Iteration 418/1000 | Loss: 0.00003236
Iteration 419/1000 | Loss: 0.00003236
Iteration 420/1000 | Loss: 0.00003236
Iteration 421/1000 | Loss: 0.00003236
Iteration 422/1000 | Loss: 0.00003236
Iteration 423/1000 | Loss: 0.00003236
Iteration 424/1000 | Loss: 0.00003236
Iteration 425/1000 | Loss: 0.00003235
Iteration 426/1000 | Loss: 0.00003235
Iteration 427/1000 | Loss: 0.00003234
Iteration 428/1000 | Loss: 0.00003234
Iteration 429/1000 | Loss: 0.00003234
Iteration 430/1000 | Loss: 0.00003234
Iteration 431/1000 | Loss: 0.00003234
Iteration 432/1000 | Loss: 0.00003234
Iteration 433/1000 | Loss: 0.00003234
Iteration 434/1000 | Loss: 0.00003234
Iteration 435/1000 | Loss: 0.00003234
Iteration 436/1000 | Loss: 0.00003233
Iteration 437/1000 | Loss: 0.00003233
Iteration 438/1000 | Loss: 0.00003233
Iteration 439/1000 | Loss: 0.00003233
Iteration 440/1000 | Loss: 0.00003233
Iteration 441/1000 | Loss: 0.00003233
Iteration 442/1000 | Loss: 0.00003233
Iteration 443/1000 | Loss: 0.00003232
Iteration 444/1000 | Loss: 0.00003232
Iteration 445/1000 | Loss: 0.00003232
Iteration 446/1000 | Loss: 0.00003232
Iteration 447/1000 | Loss: 0.00003232
Iteration 448/1000 | Loss: 0.00003232
Iteration 449/1000 | Loss: 0.00003232
Iteration 450/1000 | Loss: 0.00003231
Iteration 451/1000 | Loss: 0.00003230
Iteration 452/1000 | Loss: 0.00003230
Iteration 453/1000 | Loss: 0.00003229
Iteration 454/1000 | Loss: 0.00003229
Iteration 455/1000 | Loss: 0.00003229
Iteration 456/1000 | Loss: 0.00003229
Iteration 457/1000 | Loss: 0.00003229
Iteration 458/1000 | Loss: 0.00003228
Iteration 459/1000 | Loss: 0.00003228
Iteration 460/1000 | Loss: 0.00003228
Iteration 461/1000 | Loss: 0.00003228
Iteration 462/1000 | Loss: 0.00003228
Iteration 463/1000 | Loss: 0.00003228
Iteration 464/1000 | Loss: 0.00003228
Iteration 465/1000 | Loss: 0.00003228
Iteration 466/1000 | Loss: 0.00003228
Iteration 467/1000 | Loss: 0.00003228
Iteration 468/1000 | Loss: 0.00003228
Iteration 469/1000 | Loss: 0.00003228
Iteration 470/1000 | Loss: 0.00003227
Iteration 471/1000 | Loss: 0.00003227
Iteration 472/1000 | Loss: 0.00003227
Iteration 473/1000 | Loss: 0.00003226
Iteration 474/1000 | Loss: 0.00003226
Iteration 475/1000 | Loss: 0.00003226
Iteration 476/1000 | Loss: 0.00003226
Iteration 477/1000 | Loss: 0.00003226
Iteration 478/1000 | Loss: 0.00003226
Iteration 479/1000 | Loss: 0.00003226
Iteration 480/1000 | Loss: 0.00003226
Iteration 481/1000 | Loss: 0.00003226
Iteration 482/1000 | Loss: 0.00003226
Iteration 483/1000 | Loss: 0.00003226
Iteration 484/1000 | Loss: 0.00003226
Iteration 485/1000 | Loss: 0.00003225
Iteration 486/1000 | Loss: 0.00003225
Iteration 487/1000 | Loss: 0.00003225
Iteration 488/1000 | Loss: 0.00003225
Iteration 489/1000 | Loss: 0.00003225
Iteration 490/1000 | Loss: 0.00003225
Iteration 491/1000 | Loss: 0.00003225
Iteration 492/1000 | Loss: 0.00003225
Iteration 493/1000 | Loss: 0.00003225
Iteration 494/1000 | Loss: 0.00003225
Iteration 495/1000 | Loss: 0.00003225
Iteration 496/1000 | Loss: 0.00003225
Iteration 497/1000 | Loss: 0.00003224
Iteration 498/1000 | Loss: 0.00003224
Iteration 499/1000 | Loss: 0.00003224
Iteration 500/1000 | Loss: 0.00003224
Iteration 501/1000 | Loss: 0.00003224
Iteration 502/1000 | Loss: 0.00003223
Iteration 503/1000 | Loss: 0.00003223
Iteration 504/1000 | Loss: 0.00003223
Iteration 505/1000 | Loss: 0.00003222
Iteration 506/1000 | Loss: 0.00003221
Iteration 507/1000 | Loss: 0.00003221
Iteration 508/1000 | Loss: 0.00003220
Iteration 509/1000 | Loss: 0.00003220
Iteration 510/1000 | Loss: 0.00003220
Iteration 511/1000 | Loss: 0.00003219
Iteration 512/1000 | Loss: 0.00003219
Iteration 513/1000 | Loss: 0.00003218
Iteration 514/1000 | Loss: 0.00003218
Iteration 515/1000 | Loss: 0.00003218
Iteration 516/1000 | Loss: 0.00003218
Iteration 517/1000 | Loss: 0.00003218
Iteration 518/1000 | Loss: 0.00003218
Iteration 519/1000 | Loss: 0.00003217
Iteration 520/1000 | Loss: 0.00003217
Iteration 521/1000 | Loss: 0.00003217
Iteration 522/1000 | Loss: 0.00003217
Iteration 523/1000 | Loss: 0.00003217
Iteration 524/1000 | Loss: 0.00003216
Iteration 525/1000 | Loss: 0.00003216
Iteration 526/1000 | Loss: 0.00003216
Iteration 527/1000 | Loss: 0.00003216
Iteration 528/1000 | Loss: 0.00003216
Iteration 529/1000 | Loss: 0.00003216
Iteration 530/1000 | Loss: 0.00003216
Iteration 531/1000 | Loss: 0.00003216
Iteration 532/1000 | Loss: 0.00003216
Iteration 533/1000 | Loss: 0.00003215
Iteration 534/1000 | Loss: 0.00003215
Iteration 535/1000 | Loss: 0.00003215
Iteration 536/1000 | Loss: 0.00003215
Iteration 537/1000 | Loss: 0.00003215
Iteration 538/1000 | Loss: 0.00003215
Iteration 539/1000 | Loss: 0.00003215
Iteration 540/1000 | Loss: 0.00003214
Iteration 541/1000 | Loss: 0.00003214
Iteration 542/1000 | Loss: 0.00003214
Iteration 543/1000 | Loss: 0.00003214
Iteration 544/1000 | Loss: 0.00003214
Iteration 545/1000 | Loss: 0.00003214
Iteration 546/1000 | Loss: 0.00003214
Iteration 547/1000 | Loss: 0.00003214
Iteration 548/1000 | Loss: 0.00003213
Iteration 549/1000 | Loss: 0.00003213
Iteration 550/1000 | Loss: 0.00003213
Iteration 551/1000 | Loss: 0.00003213
Iteration 552/1000 | Loss: 0.00003213
Iteration 553/1000 | Loss: 0.00003213
Iteration 554/1000 | Loss: 0.00003213
Iteration 555/1000 | Loss: 0.00003213
Iteration 556/1000 | Loss: 0.00003213
Iteration 557/1000 | Loss: 0.00003213
Iteration 558/1000 | Loss: 0.00003213
Iteration 559/1000 | Loss: 0.00003213
Iteration 560/1000 | Loss: 0.00003212
Iteration 561/1000 | Loss: 0.00003212
Iteration 562/1000 | Loss: 0.00003212
Iteration 563/1000 | Loss: 0.00003212
Iteration 564/1000 | Loss: 0.00003212
Iteration 565/1000 | Loss: 0.00003212
Iteration 566/1000 | Loss: 0.00003212
Iteration 567/1000 | Loss: 0.00003212
Iteration 568/1000 | Loss: 0.00003212
Iteration 569/1000 | Loss: 0.00003212
Iteration 570/1000 | Loss: 0.00003212
Iteration 571/1000 | Loss: 0.00003212
Iteration 572/1000 | Loss: 0.00003211
Iteration 573/1000 | Loss: 0.00003211
Iteration 574/1000 | Loss: 0.00003211
Iteration 575/1000 | Loss: 0.00003211
Iteration 576/1000 | Loss: 0.00003211
Iteration 577/1000 | Loss: 0.00003211
Iteration 578/1000 | Loss: 0.00003211
Iteration 579/1000 | Loss: 0.00003211
Iteration 580/1000 | Loss: 0.00003211
Iteration 581/1000 | Loss: 0.00003211
Iteration 582/1000 | Loss: 0.00003211
Iteration 583/1000 | Loss: 0.00003211
Iteration 584/1000 | Loss: 0.00003211
Iteration 585/1000 | Loss: 0.00003211
Iteration 586/1000 | Loss: 0.00003211
Iteration 587/1000 | Loss: 0.00003211
Iteration 588/1000 | Loss: 0.00003211
Iteration 589/1000 | Loss: 0.00003211
Iteration 590/1000 | Loss: 0.00003211
Iteration 591/1000 | Loss: 0.00003210
Iteration 592/1000 | Loss: 0.00003210
Iteration 593/1000 | Loss: 0.00003210
Iteration 594/1000 | Loss: 0.00003210
Iteration 595/1000 | Loss: 0.00003210
Iteration 596/1000 | Loss: 0.00003210
Iteration 597/1000 | Loss: 0.00003210
Iteration 598/1000 | Loss: 0.00003210
Iteration 599/1000 | Loss: 0.00003210
Iteration 600/1000 | Loss: 0.00003210
Iteration 601/1000 | Loss: 0.00003210
Iteration 602/1000 | Loss: 0.00003210
Iteration 603/1000 | Loss: 0.00003210
Iteration 604/1000 | Loss: 0.00003210
Iteration 605/1000 | Loss: 0.00003210
Iteration 606/1000 | Loss: 0.00003210
Iteration 607/1000 | Loss: 0.00003210
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 607. Stopping optimization.
Last 5 losses: [3.21042061841581e-05, 3.21042061841581e-05, 3.21042061841581e-05, 3.21042061841581e-05, 3.21042061841581e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.21042061841581e-05

Optimization complete. Final v2v error: 4.448351860046387 mm

Highest mean error: 5.2038726806640625 mm for frame 150

Lowest mean error: 3.7939255237579346 mm for frame 64

Saving results

Total time: 578.4077033996582
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ben_posed_008/1087/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_008/1087.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_008/1087
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00839815
Iteration 2/25 | Loss: 0.00076195
Iteration 3/25 | Loss: 0.00058178
Iteration 4/25 | Loss: 0.00055580
Iteration 5/25 | Loss: 0.00054859
Iteration 6/25 | Loss: 0.00054647
Iteration 7/25 | Loss: 0.00054598
Iteration 8/25 | Loss: 0.00054598
Iteration 9/25 | Loss: 0.00054598
Iteration 10/25 | Loss: 0.00054597
Iteration 11/25 | Loss: 0.00054597
Iteration 12/25 | Loss: 0.00054597
Iteration 13/25 | Loss: 0.00054597
Iteration 14/25 | Loss: 0.00054597
Iteration 15/25 | Loss: 0.00054597
Iteration 16/25 | Loss: 0.00054597
Iteration 17/25 | Loss: 0.00054597
Iteration 18/25 | Loss: 0.00054597
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0005459743551909924, 0.0005459743551909924, 0.0005459743551909924, 0.0005459743551909924, 0.0005459743551909924]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005459743551909924

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44733620
Iteration 2/25 | Loss: 0.00014382
Iteration 3/25 | Loss: 0.00014382
Iteration 4/25 | Loss: 0.00014382
Iteration 5/25 | Loss: 0.00014382
Iteration 6/25 | Loss: 0.00014382
Iteration 7/25 | Loss: 0.00014382
Iteration 8/25 | Loss: 0.00014382
Iteration 9/25 | Loss: 0.00014382
Iteration 10/25 | Loss: 0.00014382
Iteration 11/25 | Loss: 0.00014382
Iteration 12/25 | Loss: 0.00014382
Iteration 13/25 | Loss: 0.00014382
Iteration 14/25 | Loss: 0.00014382
Iteration 15/25 | Loss: 0.00014382
Iteration 16/25 | Loss: 0.00014382
Iteration 17/25 | Loss: 0.00014382
Iteration 18/25 | Loss: 0.00014382
Iteration 19/25 | Loss: 0.00014382
Iteration 20/25 | Loss: 0.00014382
Iteration 21/25 | Loss: 0.00014382
Iteration 22/25 | Loss: 0.00014382
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0001438205799786374, 0.0001438205799786374, 0.0001438205799786374, 0.0001438205799786374, 0.0001438205799786374]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0001438205799786374

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00014382
Iteration 2/1000 | Loss: 0.00001702
Iteration 3/1000 | Loss: 0.00001255
Iteration 4/1000 | Loss: 0.00001168
Iteration 5/1000 | Loss: 0.00001115
Iteration 6/1000 | Loss: 0.00001083
Iteration 7/1000 | Loss: 0.00001054
Iteration 8/1000 | Loss: 0.00001048
Iteration 9/1000 | Loss: 0.00001042
Iteration 10/1000 | Loss: 0.00001041
Iteration 11/1000 | Loss: 0.00001038
Iteration 12/1000 | Loss: 0.00001037
Iteration 13/1000 | Loss: 0.00001036
Iteration 14/1000 | Loss: 0.00001036
Iteration 15/1000 | Loss: 0.00001032
Iteration 16/1000 | Loss: 0.00001028
Iteration 17/1000 | Loss: 0.00001027
Iteration 18/1000 | Loss: 0.00001025
Iteration 19/1000 | Loss: 0.00001024
Iteration 20/1000 | Loss: 0.00001024
Iteration 21/1000 | Loss: 0.00001023
Iteration 22/1000 | Loss: 0.00001023
Iteration 23/1000 | Loss: 0.00001022
Iteration 24/1000 | Loss: 0.00001022
Iteration 25/1000 | Loss: 0.00001021
Iteration 26/1000 | Loss: 0.00001021
Iteration 27/1000 | Loss: 0.00001019
Iteration 28/1000 | Loss: 0.00001018
Iteration 29/1000 | Loss: 0.00001018
Iteration 30/1000 | Loss: 0.00001015
Iteration 31/1000 | Loss: 0.00001013
Iteration 32/1000 | Loss: 0.00001013
Iteration 33/1000 | Loss: 0.00001013
Iteration 34/1000 | Loss: 0.00001012
Iteration 35/1000 | Loss: 0.00001012
Iteration 36/1000 | Loss: 0.00001012
Iteration 37/1000 | Loss: 0.00001011
Iteration 38/1000 | Loss: 0.00001010
Iteration 39/1000 | Loss: 0.00001009
Iteration 40/1000 | Loss: 0.00001009
Iteration 41/1000 | Loss: 0.00001005
Iteration 42/1000 | Loss: 0.00001005
Iteration 43/1000 | Loss: 0.00001003
Iteration 44/1000 | Loss: 0.00001003
Iteration 45/1000 | Loss: 0.00001003
Iteration 46/1000 | Loss: 0.00001002
Iteration 47/1000 | Loss: 0.00001002
Iteration 48/1000 | Loss: 0.00001001
Iteration 49/1000 | Loss: 0.00001001
Iteration 50/1000 | Loss: 0.00001001
Iteration 51/1000 | Loss: 0.00001000
Iteration 52/1000 | Loss: 0.00001000
Iteration 53/1000 | Loss: 0.00001000
Iteration 54/1000 | Loss: 0.00000999
Iteration 55/1000 | Loss: 0.00000999
Iteration 56/1000 | Loss: 0.00000998
Iteration 57/1000 | Loss: 0.00000998
Iteration 58/1000 | Loss: 0.00000998
Iteration 59/1000 | Loss: 0.00000998
Iteration 60/1000 | Loss: 0.00000997
Iteration 61/1000 | Loss: 0.00000997
Iteration 62/1000 | Loss: 0.00000997
Iteration 63/1000 | Loss: 0.00000997
Iteration 64/1000 | Loss: 0.00000996
Iteration 65/1000 | Loss: 0.00000996
Iteration 66/1000 | Loss: 0.00000996
Iteration 67/1000 | Loss: 0.00000995
Iteration 68/1000 | Loss: 0.00000995
Iteration 69/1000 | Loss: 0.00000995
Iteration 70/1000 | Loss: 0.00000994
Iteration 71/1000 | Loss: 0.00000994
Iteration 72/1000 | Loss: 0.00000994
Iteration 73/1000 | Loss: 0.00000993
Iteration 74/1000 | Loss: 0.00000993
Iteration 75/1000 | Loss: 0.00000993
Iteration 76/1000 | Loss: 0.00000993
Iteration 77/1000 | Loss: 0.00000993
Iteration 78/1000 | Loss: 0.00000992
Iteration 79/1000 | Loss: 0.00000992
Iteration 80/1000 | Loss: 0.00000992
Iteration 81/1000 | Loss: 0.00000992
Iteration 82/1000 | Loss: 0.00000992
Iteration 83/1000 | Loss: 0.00000992
Iteration 84/1000 | Loss: 0.00000991
Iteration 85/1000 | Loss: 0.00000991
Iteration 86/1000 | Loss: 0.00000991
Iteration 87/1000 | Loss: 0.00000991
Iteration 88/1000 | Loss: 0.00000991
Iteration 89/1000 | Loss: 0.00000991
Iteration 90/1000 | Loss: 0.00000991
Iteration 91/1000 | Loss: 0.00000990
Iteration 92/1000 | Loss: 0.00000990
Iteration 93/1000 | Loss: 0.00000990
Iteration 94/1000 | Loss: 0.00000990
Iteration 95/1000 | Loss: 0.00000990
Iteration 96/1000 | Loss: 0.00000990
Iteration 97/1000 | Loss: 0.00000990
Iteration 98/1000 | Loss: 0.00000989
Iteration 99/1000 | Loss: 0.00000989
Iteration 100/1000 | Loss: 0.00000989
Iteration 101/1000 | Loss: 0.00000989
Iteration 102/1000 | Loss: 0.00000989
Iteration 103/1000 | Loss: 0.00000989
Iteration 104/1000 | Loss: 0.00000989
Iteration 105/1000 | Loss: 0.00000989
Iteration 106/1000 | Loss: 0.00000989
Iteration 107/1000 | Loss: 0.00000989
Iteration 108/1000 | Loss: 0.00000989
Iteration 109/1000 | Loss: 0.00000989
Iteration 110/1000 | Loss: 0.00000989
Iteration 111/1000 | Loss: 0.00000988
Iteration 112/1000 | Loss: 0.00000988
Iteration 113/1000 | Loss: 0.00000988
Iteration 114/1000 | Loss: 0.00000988
Iteration 115/1000 | Loss: 0.00000988
Iteration 116/1000 | Loss: 0.00000988
Iteration 117/1000 | Loss: 0.00000988
Iteration 118/1000 | Loss: 0.00000988
Iteration 119/1000 | Loss: 0.00000988
Iteration 120/1000 | Loss: 0.00000988
Iteration 121/1000 | Loss: 0.00000988
Iteration 122/1000 | Loss: 0.00000987
Iteration 123/1000 | Loss: 0.00000987
Iteration 124/1000 | Loss: 0.00000987
Iteration 125/1000 | Loss: 0.00000987
Iteration 126/1000 | Loss: 0.00000987
Iteration 127/1000 | Loss: 0.00000987
Iteration 128/1000 | Loss: 0.00000987
Iteration 129/1000 | Loss: 0.00000987
Iteration 130/1000 | Loss: 0.00000987
Iteration 131/1000 | Loss: 0.00000987
Iteration 132/1000 | Loss: 0.00000987
Iteration 133/1000 | Loss: 0.00000987
Iteration 134/1000 | Loss: 0.00000986
Iteration 135/1000 | Loss: 0.00000986
Iteration 136/1000 | Loss: 0.00000986
Iteration 137/1000 | Loss: 0.00000986
Iteration 138/1000 | Loss: 0.00000986
Iteration 139/1000 | Loss: 0.00000986
Iteration 140/1000 | Loss: 0.00000986
Iteration 141/1000 | Loss: 0.00000986
Iteration 142/1000 | Loss: 0.00000985
Iteration 143/1000 | Loss: 0.00000985
Iteration 144/1000 | Loss: 0.00000985
Iteration 145/1000 | Loss: 0.00000985
Iteration 146/1000 | Loss: 0.00000985
Iteration 147/1000 | Loss: 0.00000985
Iteration 148/1000 | Loss: 0.00000985
Iteration 149/1000 | Loss: 0.00000984
Iteration 150/1000 | Loss: 0.00000984
Iteration 151/1000 | Loss: 0.00000984
Iteration 152/1000 | Loss: 0.00000984
Iteration 153/1000 | Loss: 0.00000984
Iteration 154/1000 | Loss: 0.00000984
Iteration 155/1000 | Loss: 0.00000984
Iteration 156/1000 | Loss: 0.00000984
Iteration 157/1000 | Loss: 0.00000984
Iteration 158/1000 | Loss: 0.00000984
Iteration 159/1000 | Loss: 0.00000984
Iteration 160/1000 | Loss: 0.00000984
Iteration 161/1000 | Loss: 0.00000984
Iteration 162/1000 | Loss: 0.00000984
Iteration 163/1000 | Loss: 0.00000984
Iteration 164/1000 | Loss: 0.00000984
Iteration 165/1000 | Loss: 0.00000983
Iteration 166/1000 | Loss: 0.00000983
Iteration 167/1000 | Loss: 0.00000983
Iteration 168/1000 | Loss: 0.00000983
Iteration 169/1000 | Loss: 0.00000983
Iteration 170/1000 | Loss: 0.00000983
Iteration 171/1000 | Loss: 0.00000983
Iteration 172/1000 | Loss: 0.00000983
Iteration 173/1000 | Loss: 0.00000983
Iteration 174/1000 | Loss: 0.00000983
Iteration 175/1000 | Loss: 0.00000983
Iteration 176/1000 | Loss: 0.00000983
Iteration 177/1000 | Loss: 0.00000983
Iteration 178/1000 | Loss: 0.00000983
Iteration 179/1000 | Loss: 0.00000983
Iteration 180/1000 | Loss: 0.00000983
Iteration 181/1000 | Loss: 0.00000982
Iteration 182/1000 | Loss: 0.00000982
Iteration 183/1000 | Loss: 0.00000982
Iteration 184/1000 | Loss: 0.00000982
Iteration 185/1000 | Loss: 0.00000982
Iteration 186/1000 | Loss: 0.00000982
Iteration 187/1000 | Loss: 0.00000982
Iteration 188/1000 | Loss: 0.00000982
Iteration 189/1000 | Loss: 0.00000982
Iteration 190/1000 | Loss: 0.00000982
Iteration 191/1000 | Loss: 0.00000982
Iteration 192/1000 | Loss: 0.00000982
Iteration 193/1000 | Loss: 0.00000982
Iteration 194/1000 | Loss: 0.00000982
Iteration 195/1000 | Loss: 0.00000982
Iteration 196/1000 | Loss: 0.00000982
Iteration 197/1000 | Loss: 0.00000982
Iteration 198/1000 | Loss: 0.00000982
Iteration 199/1000 | Loss: 0.00000982
Iteration 200/1000 | Loss: 0.00000982
Iteration 201/1000 | Loss: 0.00000982
Iteration 202/1000 | Loss: 0.00000982
Iteration 203/1000 | Loss: 0.00000982
Iteration 204/1000 | Loss: 0.00000982
Iteration 205/1000 | Loss: 0.00000982
Iteration 206/1000 | Loss: 0.00000982
Iteration 207/1000 | Loss: 0.00000982
Iteration 208/1000 | Loss: 0.00000982
Iteration 209/1000 | Loss: 0.00000982
Iteration 210/1000 | Loss: 0.00000982
Iteration 211/1000 | Loss: 0.00000982
Iteration 212/1000 | Loss: 0.00000982
Iteration 213/1000 | Loss: 0.00000982
Iteration 214/1000 | Loss: 0.00000982
Iteration 215/1000 | Loss: 0.00000982
Iteration 216/1000 | Loss: 0.00000982
Iteration 217/1000 | Loss: 0.00000982
Iteration 218/1000 | Loss: 0.00000982
Iteration 219/1000 | Loss: 0.00000982
Iteration 220/1000 | Loss: 0.00000982
Iteration 221/1000 | Loss: 0.00000982
Iteration 222/1000 | Loss: 0.00000982
Iteration 223/1000 | Loss: 0.00000982
Iteration 224/1000 | Loss: 0.00000982
Iteration 225/1000 | Loss: 0.00000982
Iteration 226/1000 | Loss: 0.00000982
Iteration 227/1000 | Loss: 0.00000982
Iteration 228/1000 | Loss: 0.00000982
Iteration 229/1000 | Loss: 0.00000982
Iteration 230/1000 | Loss: 0.00000982
Iteration 231/1000 | Loss: 0.00000982
Iteration 232/1000 | Loss: 0.00000982
Iteration 233/1000 | Loss: 0.00000982
Iteration 234/1000 | Loss: 0.00000982
Iteration 235/1000 | Loss: 0.00000982
Iteration 236/1000 | Loss: 0.00000982
Iteration 237/1000 | Loss: 0.00000982
Iteration 238/1000 | Loss: 0.00000982
Iteration 239/1000 | Loss: 0.00000982
Iteration 240/1000 | Loss: 0.00000982
Iteration 241/1000 | Loss: 0.00000982
Iteration 242/1000 | Loss: 0.00000982
Iteration 243/1000 | Loss: 0.00000982
Iteration 244/1000 | Loss: 0.00000982
Iteration 245/1000 | Loss: 0.00000982
Iteration 246/1000 | Loss: 0.00000982
Iteration 247/1000 | Loss: 0.00000982
Iteration 248/1000 | Loss: 0.00000982
Iteration 249/1000 | Loss: 0.00000982
Iteration 250/1000 | Loss: 0.00000982
Iteration 251/1000 | Loss: 0.00000982
Iteration 252/1000 | Loss: 0.00000982
Iteration 253/1000 | Loss: 0.00000982
Iteration 254/1000 | Loss: 0.00000982
Iteration 255/1000 | Loss: 0.00000982
Iteration 256/1000 | Loss: 0.00000982
Iteration 257/1000 | Loss: 0.00000982
Iteration 258/1000 | Loss: 0.00000982
Iteration 259/1000 | Loss: 0.00000982
Iteration 260/1000 | Loss: 0.00000982
Iteration 261/1000 | Loss: 0.00000982
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 261. Stopping optimization.
Last 5 losses: [9.817114005272742e-06, 9.817114005272742e-06, 9.817114005272742e-06, 9.817114005272742e-06, 9.817114005272742e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.817114005272742e-06

Optimization complete. Final v2v error: 2.6242384910583496 mm

Highest mean error: 2.924482583999634 mm for frame 57

Lowest mean error: 2.4178109169006348 mm for frame 143

Saving results

Total time: 39.78496289253235
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ben_posed_008/1035/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_008/1035.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_008/1035
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01007751
Iteration 2/25 | Loss: 0.00159846
Iteration 3/25 | Loss: 0.00119938
Iteration 4/25 | Loss: 0.00110649
Iteration 5/25 | Loss: 0.00110821
Iteration 6/25 | Loss: 0.00110608
Iteration 7/25 | Loss: 0.00104782
Iteration 8/25 | Loss: 0.00099172
Iteration 9/25 | Loss: 0.00096539
Iteration 10/25 | Loss: 0.00094527
Iteration 11/25 | Loss: 0.00092612
Iteration 12/25 | Loss: 0.00092867
Iteration 13/25 | Loss: 0.00092369
Iteration 14/25 | Loss: 0.00091539
Iteration 15/25 | Loss: 0.00091237
Iteration 16/25 | Loss: 0.00091176
Iteration 17/25 | Loss: 0.00091160
Iteration 18/25 | Loss: 0.00091160
Iteration 19/25 | Loss: 0.00091160
Iteration 20/25 | Loss: 0.00091159
Iteration 21/25 | Loss: 0.00091159
Iteration 22/25 | Loss: 0.00091159
Iteration 23/25 | Loss: 0.00091159
Iteration 24/25 | Loss: 0.00091159
Iteration 25/25 | Loss: 0.00091159

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.20403790
Iteration 2/25 | Loss: 0.00100113
Iteration 3/25 | Loss: 0.00100113
Iteration 4/25 | Loss: 0.00100113
Iteration 5/25 | Loss: 0.00100113
Iteration 6/25 | Loss: 0.00100113
Iteration 7/25 | Loss: 0.00100112
Iteration 8/25 | Loss: 0.00100112
Iteration 9/25 | Loss: 0.00100112
Iteration 10/25 | Loss: 0.00100112
Iteration 11/25 | Loss: 0.00100112
Iteration 12/25 | Loss: 0.00100112
Iteration 13/25 | Loss: 0.00100112
Iteration 14/25 | Loss: 0.00100112
Iteration 15/25 | Loss: 0.00100112
Iteration 16/25 | Loss: 0.00100112
Iteration 17/25 | Loss: 0.00100112
Iteration 18/25 | Loss: 0.00100112
Iteration 19/25 | Loss: 0.00100112
Iteration 20/25 | Loss: 0.00100112
Iteration 21/25 | Loss: 0.00100112
Iteration 22/25 | Loss: 0.00100112
Iteration 23/25 | Loss: 0.00100112
Iteration 24/25 | Loss: 0.00100112
Iteration 25/25 | Loss: 0.00100112
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.001001124270260334, 0.001001124270260334, 0.001001124270260334, 0.001001124270260334, 0.001001124270260334]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001001124270260334

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00100112
Iteration 2/1000 | Loss: 0.00012110
Iteration 3/1000 | Loss: 0.00009458
Iteration 4/1000 | Loss: 0.00018262
Iteration 5/1000 | Loss: 0.00030694
Iteration 6/1000 | Loss: 0.00008115
Iteration 7/1000 | Loss: 0.00007414
Iteration 8/1000 | Loss: 0.00035944
Iteration 9/1000 | Loss: 0.00007290
Iteration 10/1000 | Loss: 0.00006418
Iteration 11/1000 | Loss: 0.00042465
Iteration 12/1000 | Loss: 0.00006668
Iteration 13/1000 | Loss: 0.00005856
Iteration 14/1000 | Loss: 0.00039554
Iteration 15/1000 | Loss: 0.00006086
Iteration 16/1000 | Loss: 0.00035179
Iteration 17/1000 | Loss: 0.00005916
Iteration 18/1000 | Loss: 0.00033006
Iteration 19/1000 | Loss: 0.00005511
Iteration 20/1000 | Loss: 0.00004806
Iteration 21/1000 | Loss: 0.00004468
Iteration 22/1000 | Loss: 0.00004315
Iteration 23/1000 | Loss: 0.00004229
Iteration 24/1000 | Loss: 0.00004153
Iteration 25/1000 | Loss: 0.00004115
Iteration 26/1000 | Loss: 0.00004066
Iteration 27/1000 | Loss: 0.00004013
Iteration 28/1000 | Loss: 0.00003983
Iteration 29/1000 | Loss: 0.00003953
Iteration 30/1000 | Loss: 0.00003929
Iteration 31/1000 | Loss: 0.00003918
Iteration 32/1000 | Loss: 0.00003912
Iteration 33/1000 | Loss: 0.00003906
Iteration 34/1000 | Loss: 0.00003904
Iteration 35/1000 | Loss: 0.00003903
Iteration 36/1000 | Loss: 0.00003888
Iteration 37/1000 | Loss: 0.00003875
Iteration 38/1000 | Loss: 0.00003874
Iteration 39/1000 | Loss: 0.00003872
Iteration 40/1000 | Loss: 0.00003872
Iteration 41/1000 | Loss: 0.00003871
Iteration 42/1000 | Loss: 0.00003871
Iteration 43/1000 | Loss: 0.00003865
Iteration 44/1000 | Loss: 0.00003863
Iteration 45/1000 | Loss: 0.00003862
Iteration 46/1000 | Loss: 0.00003862
Iteration 47/1000 | Loss: 0.00003862
Iteration 48/1000 | Loss: 0.00003862
Iteration 49/1000 | Loss: 0.00003862
Iteration 50/1000 | Loss: 0.00003862
Iteration 51/1000 | Loss: 0.00003862
Iteration 52/1000 | Loss: 0.00003862
Iteration 53/1000 | Loss: 0.00003862
Iteration 54/1000 | Loss: 0.00003862
Iteration 55/1000 | Loss: 0.00003862
Iteration 56/1000 | Loss: 0.00003862
Iteration 57/1000 | Loss: 0.00003861
Iteration 58/1000 | Loss: 0.00003861
Iteration 59/1000 | Loss: 0.00003861
Iteration 60/1000 | Loss: 0.00003861
Iteration 61/1000 | Loss: 0.00003861
Iteration 62/1000 | Loss: 0.00003861
Iteration 63/1000 | Loss: 0.00003861
Iteration 64/1000 | Loss: 0.00003860
Iteration 65/1000 | Loss: 0.00003859
Iteration 66/1000 | Loss: 0.00003858
Iteration 67/1000 | Loss: 0.00003856
Iteration 68/1000 | Loss: 0.00003851
Iteration 69/1000 | Loss: 0.00003849
Iteration 70/1000 | Loss: 0.00075585
Iteration 71/1000 | Loss: 0.00003883
Iteration 72/1000 | Loss: 0.00003708
Iteration 73/1000 | Loss: 0.00003635
Iteration 74/1000 | Loss: 0.00003553
Iteration 75/1000 | Loss: 0.00003508
Iteration 76/1000 | Loss: 0.00003493
Iteration 77/1000 | Loss: 0.00003490
Iteration 78/1000 | Loss: 0.00003488
Iteration 79/1000 | Loss: 0.00003481
Iteration 80/1000 | Loss: 0.00003480
Iteration 81/1000 | Loss: 0.00003479
Iteration 82/1000 | Loss: 0.00003479
Iteration 83/1000 | Loss: 0.00003478
Iteration 84/1000 | Loss: 0.00003478
Iteration 85/1000 | Loss: 0.00003477
Iteration 86/1000 | Loss: 0.00003476
Iteration 87/1000 | Loss: 0.00003475
Iteration 88/1000 | Loss: 0.00003475
Iteration 89/1000 | Loss: 0.00003473
Iteration 90/1000 | Loss: 0.00003473
Iteration 91/1000 | Loss: 0.00003473
Iteration 92/1000 | Loss: 0.00003473
Iteration 93/1000 | Loss: 0.00003472
Iteration 94/1000 | Loss: 0.00003470
Iteration 95/1000 | Loss: 0.00003470
Iteration 96/1000 | Loss: 0.00003470
Iteration 97/1000 | Loss: 0.00003470
Iteration 98/1000 | Loss: 0.00003470
Iteration 99/1000 | Loss: 0.00003470
Iteration 100/1000 | Loss: 0.00003470
Iteration 101/1000 | Loss: 0.00003469
Iteration 102/1000 | Loss: 0.00003469
Iteration 103/1000 | Loss: 0.00003469
Iteration 104/1000 | Loss: 0.00003469
Iteration 105/1000 | Loss: 0.00003469
Iteration 106/1000 | Loss: 0.00003469
Iteration 107/1000 | Loss: 0.00003469
Iteration 108/1000 | Loss: 0.00003469
Iteration 109/1000 | Loss: 0.00003469
Iteration 110/1000 | Loss: 0.00003469
Iteration 111/1000 | Loss: 0.00003467
Iteration 112/1000 | Loss: 0.00003467
Iteration 113/1000 | Loss: 0.00003467
Iteration 114/1000 | Loss: 0.00003466
Iteration 115/1000 | Loss: 0.00003466
Iteration 116/1000 | Loss: 0.00003466
Iteration 117/1000 | Loss: 0.00003466
Iteration 118/1000 | Loss: 0.00003465
Iteration 119/1000 | Loss: 0.00003465
Iteration 120/1000 | Loss: 0.00003465
Iteration 121/1000 | Loss: 0.00003465
Iteration 122/1000 | Loss: 0.00003465
Iteration 123/1000 | Loss: 0.00003464
Iteration 124/1000 | Loss: 0.00003464
Iteration 125/1000 | Loss: 0.00003464
Iteration 126/1000 | Loss: 0.00003464
Iteration 127/1000 | Loss: 0.00003464
Iteration 128/1000 | Loss: 0.00003464
Iteration 129/1000 | Loss: 0.00003464
Iteration 130/1000 | Loss: 0.00003464
Iteration 131/1000 | Loss: 0.00003464
Iteration 132/1000 | Loss: 0.00003464
Iteration 133/1000 | Loss: 0.00003464
Iteration 134/1000 | Loss: 0.00003464
Iteration 135/1000 | Loss: 0.00003464
Iteration 136/1000 | Loss: 0.00003464
Iteration 137/1000 | Loss: 0.00003464
Iteration 138/1000 | Loss: 0.00003464
Iteration 139/1000 | Loss: 0.00003464
Iteration 140/1000 | Loss: 0.00003464
Iteration 141/1000 | Loss: 0.00003464
Iteration 142/1000 | Loss: 0.00003464
Iteration 143/1000 | Loss: 0.00003464
Iteration 144/1000 | Loss: 0.00003464
Iteration 145/1000 | Loss: 0.00003464
Iteration 146/1000 | Loss: 0.00003464
Iteration 147/1000 | Loss: 0.00003464
Iteration 148/1000 | Loss: 0.00003464
Iteration 149/1000 | Loss: 0.00003464
Iteration 150/1000 | Loss: 0.00003464
Iteration 151/1000 | Loss: 0.00003464
Iteration 152/1000 | Loss: 0.00003464
Iteration 153/1000 | Loss: 0.00003464
Iteration 154/1000 | Loss: 0.00003464
Iteration 155/1000 | Loss: 0.00003464
Iteration 156/1000 | Loss: 0.00003464
Iteration 157/1000 | Loss: 0.00003464
Iteration 158/1000 | Loss: 0.00003464
Iteration 159/1000 | Loss: 0.00003464
Iteration 160/1000 | Loss: 0.00003464
Iteration 161/1000 | Loss: 0.00003464
Iteration 162/1000 | Loss: 0.00003464
Iteration 163/1000 | Loss: 0.00003464
Iteration 164/1000 | Loss: 0.00003464
Iteration 165/1000 | Loss: 0.00003464
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 165. Stopping optimization.
Last 5 losses: [3.4638513170648366e-05, 3.4638513170648366e-05, 3.4638513170648366e-05, 3.4638513170648366e-05, 3.4638513170648366e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.4638513170648366e-05

Optimization complete. Final v2v error: 4.600912570953369 mm

Highest mean error: 5.877316474914551 mm for frame 30

Lowest mean error: 3.6309332847595215 mm for frame 138

Saving results

Total time: 97.40475535392761
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ben_posed_008/1093/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_008/1093.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_008/1093
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00421587
Iteration 2/25 | Loss: 0.00077850
Iteration 3/25 | Loss: 0.00063458
Iteration 4/25 | Loss: 0.00061141
Iteration 5/25 | Loss: 0.00060463
Iteration 6/25 | Loss: 0.00060338
Iteration 7/25 | Loss: 0.00060306
Iteration 8/25 | Loss: 0.00060306
Iteration 9/25 | Loss: 0.00060306
Iteration 10/25 | Loss: 0.00060306
Iteration 11/25 | Loss: 0.00060306
Iteration 12/25 | Loss: 0.00060306
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0006030569784343243, 0.0006030569784343243, 0.0006030569784343243, 0.0006030569784343243, 0.0006030569784343243]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006030569784343243

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 6.61449862
Iteration 2/25 | Loss: 0.00016152
Iteration 3/25 | Loss: 0.00016152
Iteration 4/25 | Loss: 0.00016152
Iteration 5/25 | Loss: 0.00016152
Iteration 6/25 | Loss: 0.00016152
Iteration 7/25 | Loss: 0.00016152
Iteration 8/25 | Loss: 0.00016152
Iteration 9/25 | Loss: 0.00016152
Iteration 10/25 | Loss: 0.00016152
Iteration 11/25 | Loss: 0.00016152
Iteration 12/25 | Loss: 0.00016152
Iteration 13/25 | Loss: 0.00016152
Iteration 14/25 | Loss: 0.00016152
Iteration 15/25 | Loss: 0.00016152
Iteration 16/25 | Loss: 0.00016152
Iteration 17/25 | Loss: 0.00016152
Iteration 18/25 | Loss: 0.00016152
Iteration 19/25 | Loss: 0.00016152
Iteration 20/25 | Loss: 0.00016152
Iteration 21/25 | Loss: 0.00016152
Iteration 22/25 | Loss: 0.00016152
Iteration 23/25 | Loss: 0.00016152
Iteration 24/25 | Loss: 0.00016152
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.00016151682939380407, 0.00016151682939380407, 0.00016151682939380407, 0.00016151682939380407, 0.00016151682939380407]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00016151682939380407

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00016152
Iteration 2/1000 | Loss: 0.00002326
Iteration 3/1000 | Loss: 0.00001729
Iteration 4/1000 | Loss: 0.00001563
Iteration 5/1000 | Loss: 0.00001497
Iteration 6/1000 | Loss: 0.00001435
Iteration 7/1000 | Loss: 0.00001399
Iteration 8/1000 | Loss: 0.00001370
Iteration 9/1000 | Loss: 0.00001354
Iteration 10/1000 | Loss: 0.00001351
Iteration 11/1000 | Loss: 0.00001342
Iteration 12/1000 | Loss: 0.00001340
Iteration 13/1000 | Loss: 0.00001336
Iteration 14/1000 | Loss: 0.00001336
Iteration 15/1000 | Loss: 0.00001336
Iteration 16/1000 | Loss: 0.00001332
Iteration 17/1000 | Loss: 0.00001332
Iteration 18/1000 | Loss: 0.00001332
Iteration 19/1000 | Loss: 0.00001332
Iteration 20/1000 | Loss: 0.00001331
Iteration 21/1000 | Loss: 0.00001331
Iteration 22/1000 | Loss: 0.00001326
Iteration 23/1000 | Loss: 0.00001326
Iteration 24/1000 | Loss: 0.00001326
Iteration 25/1000 | Loss: 0.00001325
Iteration 26/1000 | Loss: 0.00001324
Iteration 27/1000 | Loss: 0.00001324
Iteration 28/1000 | Loss: 0.00001324
Iteration 29/1000 | Loss: 0.00001323
Iteration 30/1000 | Loss: 0.00001323
Iteration 31/1000 | Loss: 0.00001323
Iteration 32/1000 | Loss: 0.00001322
Iteration 33/1000 | Loss: 0.00001322
Iteration 34/1000 | Loss: 0.00001321
Iteration 35/1000 | Loss: 0.00001321
Iteration 36/1000 | Loss: 0.00001321
Iteration 37/1000 | Loss: 0.00001321
Iteration 38/1000 | Loss: 0.00001321
Iteration 39/1000 | Loss: 0.00001321
Iteration 40/1000 | Loss: 0.00001321
Iteration 41/1000 | Loss: 0.00001320
Iteration 42/1000 | Loss: 0.00001320
Iteration 43/1000 | Loss: 0.00001320
Iteration 44/1000 | Loss: 0.00001320
Iteration 45/1000 | Loss: 0.00001320
Iteration 46/1000 | Loss: 0.00001320
Iteration 47/1000 | Loss: 0.00001320
Iteration 48/1000 | Loss: 0.00001320
Iteration 49/1000 | Loss: 0.00001320
Iteration 50/1000 | Loss: 0.00001320
Iteration 51/1000 | Loss: 0.00001319
Iteration 52/1000 | Loss: 0.00001319
Iteration 53/1000 | Loss: 0.00001319
Iteration 54/1000 | Loss: 0.00001319
Iteration 55/1000 | Loss: 0.00001318
Iteration 56/1000 | Loss: 0.00001318
Iteration 57/1000 | Loss: 0.00001318
Iteration 58/1000 | Loss: 0.00001317
Iteration 59/1000 | Loss: 0.00001317
Iteration 60/1000 | Loss: 0.00001317
Iteration 61/1000 | Loss: 0.00001316
Iteration 62/1000 | Loss: 0.00001316
Iteration 63/1000 | Loss: 0.00001316
Iteration 64/1000 | Loss: 0.00001315
Iteration 65/1000 | Loss: 0.00001315
Iteration 66/1000 | Loss: 0.00001315
Iteration 67/1000 | Loss: 0.00001314
Iteration 68/1000 | Loss: 0.00001314
Iteration 69/1000 | Loss: 0.00001314
Iteration 70/1000 | Loss: 0.00001313
Iteration 71/1000 | Loss: 0.00001313
Iteration 72/1000 | Loss: 0.00001312
Iteration 73/1000 | Loss: 0.00001312
Iteration 74/1000 | Loss: 0.00001311
Iteration 75/1000 | Loss: 0.00001311
Iteration 76/1000 | Loss: 0.00001311
Iteration 77/1000 | Loss: 0.00001311
Iteration 78/1000 | Loss: 0.00001311
Iteration 79/1000 | Loss: 0.00001311
Iteration 80/1000 | Loss: 0.00001311
Iteration 81/1000 | Loss: 0.00001310
Iteration 82/1000 | Loss: 0.00001310
Iteration 83/1000 | Loss: 0.00001310
Iteration 84/1000 | Loss: 0.00001310
Iteration 85/1000 | Loss: 0.00001309
Iteration 86/1000 | Loss: 0.00001309
Iteration 87/1000 | Loss: 0.00001309
Iteration 88/1000 | Loss: 0.00001308
Iteration 89/1000 | Loss: 0.00001308
Iteration 90/1000 | Loss: 0.00001308
Iteration 91/1000 | Loss: 0.00001308
Iteration 92/1000 | Loss: 0.00001308
Iteration 93/1000 | Loss: 0.00001308
Iteration 94/1000 | Loss: 0.00001308
Iteration 95/1000 | Loss: 0.00001308
Iteration 96/1000 | Loss: 0.00001308
Iteration 97/1000 | Loss: 0.00001308
Iteration 98/1000 | Loss: 0.00001308
Iteration 99/1000 | Loss: 0.00001308
Iteration 100/1000 | Loss: 0.00001308
Iteration 101/1000 | Loss: 0.00001308
Iteration 102/1000 | Loss: 0.00001308
Iteration 103/1000 | Loss: 0.00001308
Iteration 104/1000 | Loss: 0.00001308
Iteration 105/1000 | Loss: 0.00001308
Iteration 106/1000 | Loss: 0.00001308
Iteration 107/1000 | Loss: 0.00001308
Iteration 108/1000 | Loss: 0.00001308
Iteration 109/1000 | Loss: 0.00001308
Iteration 110/1000 | Loss: 0.00001308
Iteration 111/1000 | Loss: 0.00001308
Iteration 112/1000 | Loss: 0.00001308
Iteration 113/1000 | Loss: 0.00001308
Iteration 114/1000 | Loss: 0.00001308
Iteration 115/1000 | Loss: 0.00001308
Iteration 116/1000 | Loss: 0.00001308
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 116. Stopping optimization.
Last 5 losses: [1.3078243682684842e-05, 1.3078243682684842e-05, 1.3078243682684842e-05, 1.3078243682684842e-05, 1.3078243682684842e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3078243682684842e-05

Optimization complete. Final v2v error: 3.058300495147705 mm

Highest mean error: 3.3752782344818115 mm for frame 56

Lowest mean error: 2.889354705810547 mm for frame 28

Saving results

Total time: 33.21905183792114
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ben_posed_008/1098/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_008/1098.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_008/1098
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00885165
Iteration 2/25 | Loss: 0.00142380
Iteration 3/25 | Loss: 0.00085428
Iteration 4/25 | Loss: 0.00074036
Iteration 5/25 | Loss: 0.00068809
Iteration 6/25 | Loss: 0.00067644
Iteration 7/25 | Loss: 0.00066328
Iteration 8/25 | Loss: 0.00062155
Iteration 9/25 | Loss: 0.00062427
Iteration 10/25 | Loss: 0.00062460
Iteration 11/25 | Loss: 0.00061814
Iteration 12/25 | Loss: 0.00062108
Iteration 13/25 | Loss: 0.00061425
Iteration 14/25 | Loss: 0.00061170
Iteration 15/25 | Loss: 0.00061405
Iteration 16/25 | Loss: 0.00061208
Iteration 17/25 | Loss: 0.00060933
Iteration 18/25 | Loss: 0.00060920
Iteration 19/25 | Loss: 0.00060911
Iteration 20/25 | Loss: 0.00060903
Iteration 21/25 | Loss: 0.00060870
Iteration 22/25 | Loss: 0.00060978
Iteration 23/25 | Loss: 0.00060769
Iteration 24/25 | Loss: 0.00060706
Iteration 25/25 | Loss: 0.00060678

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 12.50206947
Iteration 2/25 | Loss: 0.00029616
Iteration 3/25 | Loss: 0.00021294
Iteration 4/25 | Loss: 0.00021294
Iteration 5/25 | Loss: 0.00021294
Iteration 6/25 | Loss: 0.00021294
Iteration 7/25 | Loss: 0.00021294
Iteration 8/25 | Loss: 0.00021294
Iteration 9/25 | Loss: 0.00021294
Iteration 10/25 | Loss: 0.00021294
Iteration 11/25 | Loss: 0.00021294
Iteration 12/25 | Loss: 0.00021294
Iteration 13/25 | Loss: 0.00021294
Iteration 14/25 | Loss: 0.00021294
Iteration 15/25 | Loss: 0.00021294
Iteration 16/25 | Loss: 0.00021294
Iteration 17/25 | Loss: 0.00021294
Iteration 18/25 | Loss: 0.00021294
Iteration 19/25 | Loss: 0.00021294
Iteration 20/25 | Loss: 0.00021294
Iteration 21/25 | Loss: 0.00021294
Iteration 22/25 | Loss: 0.00021294
Iteration 23/25 | Loss: 0.00021294
Iteration 24/25 | Loss: 0.00021294
Iteration 25/25 | Loss: 0.00021294

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00021294
Iteration 2/1000 | Loss: 0.00003716
Iteration 3/1000 | Loss: 0.00002486
Iteration 4/1000 | Loss: 0.00002164
Iteration 5/1000 | Loss: 0.00009572
Iteration 6/1000 | Loss: 0.00055781
Iteration 7/1000 | Loss: 0.00002483
Iteration 8/1000 | Loss: 0.00001976
Iteration 9/1000 | Loss: 0.00001924
Iteration 10/1000 | Loss: 0.00001887
Iteration 11/1000 | Loss: 0.00007322
Iteration 12/1000 | Loss: 0.00003324
Iteration 13/1000 | Loss: 0.00003960
Iteration 14/1000 | Loss: 0.00056259
Iteration 15/1000 | Loss: 0.00006355
Iteration 16/1000 | Loss: 0.00005696
Iteration 17/1000 | Loss: 0.00001926
Iteration 18/1000 | Loss: 0.00001801
Iteration 19/1000 | Loss: 0.00004801
Iteration 20/1000 | Loss: 0.00001661
Iteration 21/1000 | Loss: 0.00003398
Iteration 22/1000 | Loss: 0.00001619
Iteration 23/1000 | Loss: 0.00001603
Iteration 24/1000 | Loss: 0.00001603
Iteration 25/1000 | Loss: 0.00001598
Iteration 26/1000 | Loss: 0.00002624
Iteration 27/1000 | Loss: 0.00001592
Iteration 28/1000 | Loss: 0.00001580
Iteration 29/1000 | Loss: 0.00001576
Iteration 30/1000 | Loss: 0.00001573
Iteration 31/1000 | Loss: 0.00001571
Iteration 32/1000 | Loss: 0.00001571
Iteration 33/1000 | Loss: 0.00001570
Iteration 34/1000 | Loss: 0.00001570
Iteration 35/1000 | Loss: 0.00001569
Iteration 36/1000 | Loss: 0.00001569
Iteration 37/1000 | Loss: 0.00001569
Iteration 38/1000 | Loss: 0.00001568
Iteration 39/1000 | Loss: 0.00001567
Iteration 40/1000 | Loss: 0.00001566
Iteration 41/1000 | Loss: 0.00001566
Iteration 42/1000 | Loss: 0.00001565
Iteration 43/1000 | Loss: 0.00001564
Iteration 44/1000 | Loss: 0.00001564
Iteration 45/1000 | Loss: 0.00001564
Iteration 46/1000 | Loss: 0.00001564
Iteration 47/1000 | Loss: 0.00001563
Iteration 48/1000 | Loss: 0.00004157
Iteration 49/1000 | Loss: 0.00001567
Iteration 50/1000 | Loss: 0.00001562
Iteration 51/1000 | Loss: 0.00001561
Iteration 52/1000 | Loss: 0.00001561
Iteration 53/1000 | Loss: 0.00001561
Iteration 54/1000 | Loss: 0.00001560
Iteration 55/1000 | Loss: 0.00001559
Iteration 56/1000 | Loss: 0.00001559
Iteration 57/1000 | Loss: 0.00001559
Iteration 58/1000 | Loss: 0.00001558
Iteration 59/1000 | Loss: 0.00001558
Iteration 60/1000 | Loss: 0.00001557
Iteration 61/1000 | Loss: 0.00001557
Iteration 62/1000 | Loss: 0.00001557
Iteration 63/1000 | Loss: 0.00001557
Iteration 64/1000 | Loss: 0.00001557
Iteration 65/1000 | Loss: 0.00001556
Iteration 66/1000 | Loss: 0.00001556
Iteration 67/1000 | Loss: 0.00001556
Iteration 68/1000 | Loss: 0.00001556
Iteration 69/1000 | Loss: 0.00001556
Iteration 70/1000 | Loss: 0.00001555
Iteration 71/1000 | Loss: 0.00001555
Iteration 72/1000 | Loss: 0.00001555
Iteration 73/1000 | Loss: 0.00001555
Iteration 74/1000 | Loss: 0.00001554
Iteration 75/1000 | Loss: 0.00001552
Iteration 76/1000 | Loss: 0.00001549
Iteration 77/1000 | Loss: 0.00001549
Iteration 78/1000 | Loss: 0.00001549
Iteration 79/1000 | Loss: 0.00001548
Iteration 80/1000 | Loss: 0.00001548
Iteration 81/1000 | Loss: 0.00001548
Iteration 82/1000 | Loss: 0.00001547
Iteration 83/1000 | Loss: 0.00001547
Iteration 84/1000 | Loss: 0.00001547
Iteration 85/1000 | Loss: 0.00001546
Iteration 86/1000 | Loss: 0.00001546
Iteration 87/1000 | Loss: 0.00001546
Iteration 88/1000 | Loss: 0.00001545
Iteration 89/1000 | Loss: 0.00001545
Iteration 90/1000 | Loss: 0.00001545
Iteration 91/1000 | Loss: 0.00001544
Iteration 92/1000 | Loss: 0.00001544
Iteration 93/1000 | Loss: 0.00001544
Iteration 94/1000 | Loss: 0.00001543
Iteration 95/1000 | Loss: 0.00001543
Iteration 96/1000 | Loss: 0.00001543
Iteration 97/1000 | Loss: 0.00001543
Iteration 98/1000 | Loss: 0.00001543
Iteration 99/1000 | Loss: 0.00001543
Iteration 100/1000 | Loss: 0.00001542
Iteration 101/1000 | Loss: 0.00001542
Iteration 102/1000 | Loss: 0.00001542
Iteration 103/1000 | Loss: 0.00001541
Iteration 104/1000 | Loss: 0.00001541
Iteration 105/1000 | Loss: 0.00001541
Iteration 106/1000 | Loss: 0.00001540
Iteration 107/1000 | Loss: 0.00001540
Iteration 108/1000 | Loss: 0.00001540
Iteration 109/1000 | Loss: 0.00001539
Iteration 110/1000 | Loss: 0.00001539
Iteration 111/1000 | Loss: 0.00001539
Iteration 112/1000 | Loss: 0.00001539
Iteration 113/1000 | Loss: 0.00001539
Iteration 114/1000 | Loss: 0.00001539
Iteration 115/1000 | Loss: 0.00001539
Iteration 116/1000 | Loss: 0.00001539
Iteration 117/1000 | Loss: 0.00001539
Iteration 118/1000 | Loss: 0.00001539
Iteration 119/1000 | Loss: 0.00001539
Iteration 120/1000 | Loss: 0.00001538
Iteration 121/1000 | Loss: 0.00001538
Iteration 122/1000 | Loss: 0.00001538
Iteration 123/1000 | Loss: 0.00001538
Iteration 124/1000 | Loss: 0.00001538
Iteration 125/1000 | Loss: 0.00001537
Iteration 126/1000 | Loss: 0.00001537
Iteration 127/1000 | Loss: 0.00001537
Iteration 128/1000 | Loss: 0.00001537
Iteration 129/1000 | Loss: 0.00001537
Iteration 130/1000 | Loss: 0.00001537
Iteration 131/1000 | Loss: 0.00001537
Iteration 132/1000 | Loss: 0.00001537
Iteration 133/1000 | Loss: 0.00001537
Iteration 134/1000 | Loss: 0.00001537
Iteration 135/1000 | Loss: 0.00001537
Iteration 136/1000 | Loss: 0.00001537
Iteration 137/1000 | Loss: 0.00001537
Iteration 138/1000 | Loss: 0.00001537
Iteration 139/1000 | Loss: 0.00001537
Iteration 140/1000 | Loss: 0.00001537
Iteration 141/1000 | Loss: 0.00001537
Iteration 142/1000 | Loss: 0.00001537
Iteration 143/1000 | Loss: 0.00001537
Iteration 144/1000 | Loss: 0.00001537
Iteration 145/1000 | Loss: 0.00001537
Iteration 146/1000 | Loss: 0.00001537
Iteration 147/1000 | Loss: 0.00001537
Iteration 148/1000 | Loss: 0.00001537
Iteration 149/1000 | Loss: 0.00001537
Iteration 150/1000 | Loss: 0.00001537
Iteration 151/1000 | Loss: 0.00001537
Iteration 152/1000 | Loss: 0.00001537
Iteration 153/1000 | Loss: 0.00001537
Iteration 154/1000 | Loss: 0.00001537
Iteration 155/1000 | Loss: 0.00001537
Iteration 156/1000 | Loss: 0.00001537
Iteration 157/1000 | Loss: 0.00001537
Iteration 158/1000 | Loss: 0.00001537
Iteration 159/1000 | Loss: 0.00001537
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 159. Stopping optimization.
Last 5 losses: [1.536589661554899e-05, 1.536589661554899e-05, 1.536589661554899e-05, 1.536589661554899e-05, 1.536589661554899e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.536589661554899e-05

Optimization complete. Final v2v error: 3.272944927215576 mm

Highest mean error: 4.203511714935303 mm for frame 96

Lowest mean error: 2.680497884750366 mm for frame 11

Saving results

Total time: 98.07222628593445
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ben_posed_008/1011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_008/1011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_008/1011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00639726
Iteration 2/25 | Loss: 0.00108402
Iteration 3/25 | Loss: 0.00081838
Iteration 4/25 | Loss: 0.00077043
Iteration 5/25 | Loss: 0.00075707
Iteration 6/25 | Loss: 0.00075456
Iteration 7/25 | Loss: 0.00075413
Iteration 8/25 | Loss: 0.00075413
Iteration 9/25 | Loss: 0.00075413
Iteration 10/25 | Loss: 0.00075393
Iteration 11/25 | Loss: 0.00075393
Iteration 12/25 | Loss: 0.00075393
Iteration 13/25 | Loss: 0.00075393
Iteration 14/25 | Loss: 0.00075393
Iteration 15/25 | Loss: 0.00075393
Iteration 16/25 | Loss: 0.00075393
Iteration 17/25 | Loss: 0.00075393
Iteration 18/25 | Loss: 0.00075393
Iteration 19/25 | Loss: 0.00075393
Iteration 20/25 | Loss: 0.00075393
Iteration 21/25 | Loss: 0.00075393
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0007539318175986409, 0.0007539318175986409, 0.0007539318175986409, 0.0007539318175986409, 0.0007539318175986409]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007539318175986409

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 6.83473873
Iteration 2/25 | Loss: 0.00026139
Iteration 3/25 | Loss: 0.00026133
Iteration 4/25 | Loss: 0.00026133
Iteration 5/25 | Loss: 0.00026133
Iteration 6/25 | Loss: 0.00026133
Iteration 7/25 | Loss: 0.00026133
Iteration 8/25 | Loss: 0.00026133
Iteration 9/25 | Loss: 0.00026133
Iteration 10/25 | Loss: 0.00026133
Iteration 11/25 | Loss: 0.00026133
Iteration 12/25 | Loss: 0.00026133
Iteration 13/25 | Loss: 0.00026133
Iteration 14/25 | Loss: 0.00026133
Iteration 15/25 | Loss: 0.00026133
Iteration 16/25 | Loss: 0.00026133
Iteration 17/25 | Loss: 0.00026133
Iteration 18/25 | Loss: 0.00026133
Iteration 19/25 | Loss: 0.00026133
Iteration 20/25 | Loss: 0.00026133
Iteration 21/25 | Loss: 0.00026133
Iteration 22/25 | Loss: 0.00026133
Iteration 23/25 | Loss: 0.00026133
Iteration 24/25 | Loss: 0.00026133
Iteration 25/25 | Loss: 0.00026133

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00026133
Iteration 2/1000 | Loss: 0.00003120
Iteration 3/1000 | Loss: 0.00002448
Iteration 4/1000 | Loss: 0.00002221
Iteration 5/1000 | Loss: 0.00002154
Iteration 6/1000 | Loss: 0.00002112
Iteration 7/1000 | Loss: 0.00002091
Iteration 8/1000 | Loss: 0.00002072
Iteration 9/1000 | Loss: 0.00002047
Iteration 10/1000 | Loss: 0.00002044
Iteration 11/1000 | Loss: 0.00002037
Iteration 12/1000 | Loss: 0.00002030
Iteration 13/1000 | Loss: 0.00002026
Iteration 14/1000 | Loss: 0.00002026
Iteration 15/1000 | Loss: 0.00002025
Iteration 16/1000 | Loss: 0.00002024
Iteration 17/1000 | Loss: 0.00002021
Iteration 18/1000 | Loss: 0.00002020
Iteration 19/1000 | Loss: 0.00002020
Iteration 20/1000 | Loss: 0.00002020
Iteration 21/1000 | Loss: 0.00002020
Iteration 22/1000 | Loss: 0.00002020
Iteration 23/1000 | Loss: 0.00002020
Iteration 24/1000 | Loss: 0.00002020
Iteration 25/1000 | Loss: 0.00002019
Iteration 26/1000 | Loss: 0.00002017
Iteration 27/1000 | Loss: 0.00002017
Iteration 28/1000 | Loss: 0.00002017
Iteration 29/1000 | Loss: 0.00002016
Iteration 30/1000 | Loss: 0.00002016
Iteration 31/1000 | Loss: 0.00002016
Iteration 32/1000 | Loss: 0.00002016
Iteration 33/1000 | Loss: 0.00002016
Iteration 34/1000 | Loss: 0.00002016
Iteration 35/1000 | Loss: 0.00002015
Iteration 36/1000 | Loss: 0.00002015
Iteration 37/1000 | Loss: 0.00002015
Iteration 38/1000 | Loss: 0.00002014
Iteration 39/1000 | Loss: 0.00002014
Iteration 40/1000 | Loss: 0.00002014
Iteration 41/1000 | Loss: 0.00002014
Iteration 42/1000 | Loss: 0.00002014
Iteration 43/1000 | Loss: 0.00002013
Iteration 44/1000 | Loss: 0.00002013
Iteration 45/1000 | Loss: 0.00002013
Iteration 46/1000 | Loss: 0.00002013
Iteration 47/1000 | Loss: 0.00002013
Iteration 48/1000 | Loss: 0.00002012
Iteration 49/1000 | Loss: 0.00002012
Iteration 50/1000 | Loss: 0.00002012
Iteration 51/1000 | Loss: 0.00002011
Iteration 52/1000 | Loss: 0.00002011
Iteration 53/1000 | Loss: 0.00002010
Iteration 54/1000 | Loss: 0.00002009
Iteration 55/1000 | Loss: 0.00002009
Iteration 56/1000 | Loss: 0.00002009
Iteration 57/1000 | Loss: 0.00002009
Iteration 58/1000 | Loss: 0.00002008
Iteration 59/1000 | Loss: 0.00002008
Iteration 60/1000 | Loss: 0.00002008
Iteration 61/1000 | Loss: 0.00002008
Iteration 62/1000 | Loss: 0.00002008
Iteration 63/1000 | Loss: 0.00002007
Iteration 64/1000 | Loss: 0.00002007
Iteration 65/1000 | Loss: 0.00002007
Iteration 66/1000 | Loss: 0.00002007
Iteration 67/1000 | Loss: 0.00002006
Iteration 68/1000 | Loss: 0.00002006
Iteration 69/1000 | Loss: 0.00002006
Iteration 70/1000 | Loss: 0.00002005
Iteration 71/1000 | Loss: 0.00002005
Iteration 72/1000 | Loss: 0.00002005
Iteration 73/1000 | Loss: 0.00002005
Iteration 74/1000 | Loss: 0.00002005
Iteration 75/1000 | Loss: 0.00002005
Iteration 76/1000 | Loss: 0.00002005
Iteration 77/1000 | Loss: 0.00002005
Iteration 78/1000 | Loss: 0.00002005
Iteration 79/1000 | Loss: 0.00002004
Iteration 80/1000 | Loss: 0.00002004
Iteration 81/1000 | Loss: 0.00002004
Iteration 82/1000 | Loss: 0.00002004
Iteration 83/1000 | Loss: 0.00002004
Iteration 84/1000 | Loss: 0.00002004
Iteration 85/1000 | Loss: 0.00002004
Iteration 86/1000 | Loss: 0.00002004
Iteration 87/1000 | Loss: 0.00002004
Iteration 88/1000 | Loss: 0.00002004
Iteration 89/1000 | Loss: 0.00002004
Iteration 90/1000 | Loss: 0.00002004
Iteration 91/1000 | Loss: 0.00002004
Iteration 92/1000 | Loss: 0.00002004
Iteration 93/1000 | Loss: 0.00002004
Iteration 94/1000 | Loss: 0.00002003
Iteration 95/1000 | Loss: 0.00002003
Iteration 96/1000 | Loss: 0.00002003
Iteration 97/1000 | Loss: 0.00002003
Iteration 98/1000 | Loss: 0.00002003
Iteration 99/1000 | Loss: 0.00002003
Iteration 100/1000 | Loss: 0.00002003
Iteration 101/1000 | Loss: 0.00002003
Iteration 102/1000 | Loss: 0.00002003
Iteration 103/1000 | Loss: 0.00002002
Iteration 104/1000 | Loss: 0.00002002
Iteration 105/1000 | Loss: 0.00002002
Iteration 106/1000 | Loss: 0.00002002
Iteration 107/1000 | Loss: 0.00002002
Iteration 108/1000 | Loss: 0.00002002
Iteration 109/1000 | Loss: 0.00002002
Iteration 110/1000 | Loss: 0.00002002
Iteration 111/1000 | Loss: 0.00002002
Iteration 112/1000 | Loss: 0.00002002
Iteration 113/1000 | Loss: 0.00002002
Iteration 114/1000 | Loss: 0.00002002
Iteration 115/1000 | Loss: 0.00002002
Iteration 116/1000 | Loss: 0.00002002
Iteration 117/1000 | Loss: 0.00002002
Iteration 118/1000 | Loss: 0.00002002
Iteration 119/1000 | Loss: 0.00002002
Iteration 120/1000 | Loss: 0.00002002
Iteration 121/1000 | Loss: 0.00002002
Iteration 122/1000 | Loss: 0.00002002
Iteration 123/1000 | Loss: 0.00002002
Iteration 124/1000 | Loss: 0.00002002
Iteration 125/1000 | Loss: 0.00002002
Iteration 126/1000 | Loss: 0.00002002
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 126. Stopping optimization.
Last 5 losses: [2.002020482905209e-05, 2.002020482905209e-05, 2.002020482905209e-05, 2.002020482905209e-05, 2.002020482905209e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.002020482905209e-05

Optimization complete. Final v2v error: 3.724475622177124 mm

Highest mean error: 4.013197422027588 mm for frame 56

Lowest mean error: 3.2688395977020264 mm for frame 214

Saving results

Total time: 38.239930391311646
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ben_posed_008/1017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_008/1017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_008/1017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00866553
Iteration 2/25 | Loss: 0.00111471
Iteration 3/25 | Loss: 0.00080657
Iteration 4/25 | Loss: 0.00073590
Iteration 5/25 | Loss: 0.00071980
Iteration 6/25 | Loss: 0.00071740
Iteration 7/25 | Loss: 0.00071681
Iteration 8/25 | Loss: 0.00071681
Iteration 9/25 | Loss: 0.00071681
Iteration 10/25 | Loss: 0.00071681
Iteration 11/25 | Loss: 0.00071681
Iteration 12/25 | Loss: 0.00071681
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0007168101728893816, 0.0007168101728893816, 0.0007168101728893816, 0.0007168101728893816, 0.0007168101728893816]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007168101728893816

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.02418220
Iteration 2/25 | Loss: 0.00029122
Iteration 3/25 | Loss: 0.00029121
Iteration 4/25 | Loss: 0.00029121
Iteration 5/25 | Loss: 0.00029121
Iteration 6/25 | Loss: 0.00029121
Iteration 7/25 | Loss: 0.00029121
Iteration 8/25 | Loss: 0.00029121
Iteration 9/25 | Loss: 0.00029120
Iteration 10/25 | Loss: 0.00029120
Iteration 11/25 | Loss: 0.00029120
Iteration 12/25 | Loss: 0.00029120
Iteration 13/25 | Loss: 0.00029120
Iteration 14/25 | Loss: 0.00029120
Iteration 15/25 | Loss: 0.00029120
Iteration 16/25 | Loss: 0.00029120
Iteration 17/25 | Loss: 0.00029120
Iteration 18/25 | Loss: 0.00029120
Iteration 19/25 | Loss: 0.00029120
Iteration 20/25 | Loss: 0.00029120
Iteration 21/25 | Loss: 0.00029120
Iteration 22/25 | Loss: 0.00029120
Iteration 23/25 | Loss: 0.00029120
Iteration 24/25 | Loss: 0.00029120
Iteration 25/25 | Loss: 0.00029120

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00029120
Iteration 2/1000 | Loss: 0.00005238
Iteration 3/1000 | Loss: 0.00004137
Iteration 4/1000 | Loss: 0.00003748
Iteration 5/1000 | Loss: 0.00003484
Iteration 6/1000 | Loss: 0.00003257
Iteration 7/1000 | Loss: 0.00003120
Iteration 8/1000 | Loss: 0.00003040
Iteration 9/1000 | Loss: 0.00002997
Iteration 10/1000 | Loss: 0.00002965
Iteration 11/1000 | Loss: 0.00002935
Iteration 12/1000 | Loss: 0.00002921
Iteration 13/1000 | Loss: 0.00002907
Iteration 14/1000 | Loss: 0.00002906
Iteration 15/1000 | Loss: 0.00002905
Iteration 16/1000 | Loss: 0.00002905
Iteration 17/1000 | Loss: 0.00002904
Iteration 18/1000 | Loss: 0.00002904
Iteration 19/1000 | Loss: 0.00002902
Iteration 20/1000 | Loss: 0.00002899
Iteration 21/1000 | Loss: 0.00002896
Iteration 22/1000 | Loss: 0.00002895
Iteration 23/1000 | Loss: 0.00002895
Iteration 24/1000 | Loss: 0.00002895
Iteration 25/1000 | Loss: 0.00002895
Iteration 26/1000 | Loss: 0.00002895
Iteration 27/1000 | Loss: 0.00002895
Iteration 28/1000 | Loss: 0.00002894
Iteration 29/1000 | Loss: 0.00002894
Iteration 30/1000 | Loss: 0.00002894
Iteration 31/1000 | Loss: 0.00002894
Iteration 32/1000 | Loss: 0.00002894
Iteration 33/1000 | Loss: 0.00002894
Iteration 34/1000 | Loss: 0.00002894
Iteration 35/1000 | Loss: 0.00002894
Iteration 36/1000 | Loss: 0.00002894
Iteration 37/1000 | Loss: 0.00002894
Iteration 38/1000 | Loss: 0.00002894
Iteration 39/1000 | Loss: 0.00002894
Iteration 40/1000 | Loss: 0.00002893
Iteration 41/1000 | Loss: 0.00002893
Iteration 42/1000 | Loss: 0.00002893
Iteration 43/1000 | Loss: 0.00002893
Iteration 44/1000 | Loss: 0.00002893
Iteration 45/1000 | Loss: 0.00002893
Iteration 46/1000 | Loss: 0.00002893
Iteration 47/1000 | Loss: 0.00002893
Iteration 48/1000 | Loss: 0.00002893
Iteration 49/1000 | Loss: 0.00002893
Iteration 50/1000 | Loss: 0.00002893
Iteration 51/1000 | Loss: 0.00002892
Iteration 52/1000 | Loss: 0.00002892
Iteration 53/1000 | Loss: 0.00002892
Iteration 54/1000 | Loss: 0.00002892
Iteration 55/1000 | Loss: 0.00002892
Iteration 56/1000 | Loss: 0.00002892
Iteration 57/1000 | Loss: 0.00002892
Iteration 58/1000 | Loss: 0.00002892
Iteration 59/1000 | Loss: 0.00002891
Iteration 60/1000 | Loss: 0.00002891
Iteration 61/1000 | Loss: 0.00002891
Iteration 62/1000 | Loss: 0.00002891
Iteration 63/1000 | Loss: 0.00002891
Iteration 64/1000 | Loss: 0.00002890
Iteration 65/1000 | Loss: 0.00002890
Iteration 66/1000 | Loss: 0.00002890
Iteration 67/1000 | Loss: 0.00002890
Iteration 68/1000 | Loss: 0.00002890
Iteration 69/1000 | Loss: 0.00002890
Iteration 70/1000 | Loss: 0.00002890
Iteration 71/1000 | Loss: 0.00002890
Iteration 72/1000 | Loss: 0.00002889
Iteration 73/1000 | Loss: 0.00002889
Iteration 74/1000 | Loss: 0.00002889
Iteration 75/1000 | Loss: 0.00002889
Iteration 76/1000 | Loss: 0.00002889
Iteration 77/1000 | Loss: 0.00002889
Iteration 78/1000 | Loss: 0.00002889
Iteration 79/1000 | Loss: 0.00002889
Iteration 80/1000 | Loss: 0.00002889
Iteration 81/1000 | Loss: 0.00002888
Iteration 82/1000 | Loss: 0.00002888
Iteration 83/1000 | Loss: 0.00002888
Iteration 84/1000 | Loss: 0.00002888
Iteration 85/1000 | Loss: 0.00002888
Iteration 86/1000 | Loss: 0.00002888
Iteration 87/1000 | Loss: 0.00002888
Iteration 88/1000 | Loss: 0.00002888
Iteration 89/1000 | Loss: 0.00002888
Iteration 90/1000 | Loss: 0.00002888
Iteration 91/1000 | Loss: 0.00002888
Iteration 92/1000 | Loss: 0.00002888
Iteration 93/1000 | Loss: 0.00002888
Iteration 94/1000 | Loss: 0.00002888
Iteration 95/1000 | Loss: 0.00002888
Iteration 96/1000 | Loss: 0.00002887
Iteration 97/1000 | Loss: 0.00002887
Iteration 98/1000 | Loss: 0.00002887
Iteration 99/1000 | Loss: 0.00002887
Iteration 100/1000 | Loss: 0.00002887
Iteration 101/1000 | Loss: 0.00002887
Iteration 102/1000 | Loss: 0.00002887
Iteration 103/1000 | Loss: 0.00002887
Iteration 104/1000 | Loss: 0.00002886
Iteration 105/1000 | Loss: 0.00002886
Iteration 106/1000 | Loss: 0.00002886
Iteration 107/1000 | Loss: 0.00002886
Iteration 108/1000 | Loss: 0.00002886
Iteration 109/1000 | Loss: 0.00002886
Iteration 110/1000 | Loss: 0.00002885
Iteration 111/1000 | Loss: 0.00002885
Iteration 112/1000 | Loss: 0.00002885
Iteration 113/1000 | Loss: 0.00002885
Iteration 114/1000 | Loss: 0.00002885
Iteration 115/1000 | Loss: 0.00002885
Iteration 116/1000 | Loss: 0.00002885
Iteration 117/1000 | Loss: 0.00002884
Iteration 118/1000 | Loss: 0.00002884
Iteration 119/1000 | Loss: 0.00002884
Iteration 120/1000 | Loss: 0.00002884
Iteration 121/1000 | Loss: 0.00002884
Iteration 122/1000 | Loss: 0.00002884
Iteration 123/1000 | Loss: 0.00002884
Iteration 124/1000 | Loss: 0.00002884
Iteration 125/1000 | Loss: 0.00002883
Iteration 126/1000 | Loss: 0.00002883
Iteration 127/1000 | Loss: 0.00002883
Iteration 128/1000 | Loss: 0.00002883
Iteration 129/1000 | Loss: 0.00002883
Iteration 130/1000 | Loss: 0.00002883
Iteration 131/1000 | Loss: 0.00002883
Iteration 132/1000 | Loss: 0.00002882
Iteration 133/1000 | Loss: 0.00002882
Iteration 134/1000 | Loss: 0.00002882
Iteration 135/1000 | Loss: 0.00002882
Iteration 136/1000 | Loss: 0.00002882
Iteration 137/1000 | Loss: 0.00002882
Iteration 138/1000 | Loss: 0.00002881
Iteration 139/1000 | Loss: 0.00002881
Iteration 140/1000 | Loss: 0.00002881
Iteration 141/1000 | Loss: 0.00002881
Iteration 142/1000 | Loss: 0.00002881
Iteration 143/1000 | Loss: 0.00002881
Iteration 144/1000 | Loss: 0.00002881
Iteration 145/1000 | Loss: 0.00002881
Iteration 146/1000 | Loss: 0.00002881
Iteration 147/1000 | Loss: 0.00002881
Iteration 148/1000 | Loss: 0.00002881
Iteration 149/1000 | Loss: 0.00002881
Iteration 150/1000 | Loss: 0.00002881
Iteration 151/1000 | Loss: 0.00002881
Iteration 152/1000 | Loss: 0.00002881
Iteration 153/1000 | Loss: 0.00002881
Iteration 154/1000 | Loss: 0.00002881
Iteration 155/1000 | Loss: 0.00002881
Iteration 156/1000 | Loss: 0.00002881
Iteration 157/1000 | Loss: 0.00002881
Iteration 158/1000 | Loss: 0.00002881
Iteration 159/1000 | Loss: 0.00002881
Iteration 160/1000 | Loss: 0.00002881
Iteration 161/1000 | Loss: 0.00002881
Iteration 162/1000 | Loss: 0.00002881
Iteration 163/1000 | Loss: 0.00002881
Iteration 164/1000 | Loss: 0.00002881
Iteration 165/1000 | Loss: 0.00002881
Iteration 166/1000 | Loss: 0.00002881
Iteration 167/1000 | Loss: 0.00002881
Iteration 168/1000 | Loss: 0.00002881
Iteration 169/1000 | Loss: 0.00002881
Iteration 170/1000 | Loss: 0.00002881
Iteration 171/1000 | Loss: 0.00002881
Iteration 172/1000 | Loss: 0.00002881
Iteration 173/1000 | Loss: 0.00002881
Iteration 174/1000 | Loss: 0.00002881
Iteration 175/1000 | Loss: 0.00002881
Iteration 176/1000 | Loss: 0.00002881
Iteration 177/1000 | Loss: 0.00002881
Iteration 178/1000 | Loss: 0.00002881
Iteration 179/1000 | Loss: 0.00002881
Iteration 180/1000 | Loss: 0.00002881
Iteration 181/1000 | Loss: 0.00002881
Iteration 182/1000 | Loss: 0.00002881
Iteration 183/1000 | Loss: 0.00002881
Iteration 184/1000 | Loss: 0.00002881
Iteration 185/1000 | Loss: 0.00002881
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 185. Stopping optimization.
Last 5 losses: [2.8806327463826165e-05, 2.8806327463826165e-05, 2.8806327463826165e-05, 2.8806327463826165e-05, 2.8806327463826165e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.8806327463826165e-05

Optimization complete. Final v2v error: 4.50842809677124 mm

Highest mean error: 4.885686874389648 mm for frame 149

Lowest mean error: 4.103180885314941 mm for frame 93

Saving results

Total time: 38.35730981826782
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ben_posed_008/1097/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_008/1097.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_008/1097
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01027911
Iteration 2/25 | Loss: 0.00371468
Iteration 3/25 | Loss: 0.00231059
Iteration 4/25 | Loss: 0.00178793
Iteration 5/25 | Loss: 0.00170927
Iteration 6/25 | Loss: 0.00165345
Iteration 7/25 | Loss: 0.00152447
Iteration 8/25 | Loss: 0.00142060
Iteration 9/25 | Loss: 0.00135238
Iteration 10/25 | Loss: 0.00132702
Iteration 11/25 | Loss: 0.00125404
Iteration 12/25 | Loss: 0.00119663
Iteration 13/25 | Loss: 0.00116681
Iteration 14/25 | Loss: 0.00114323
Iteration 15/25 | Loss: 0.00113440
Iteration 16/25 | Loss: 0.00113046
Iteration 17/25 | Loss: 0.00112781
Iteration 18/25 | Loss: 0.00112686
Iteration 19/25 | Loss: 0.00112655
Iteration 20/25 | Loss: 0.00112643
Iteration 21/25 | Loss: 0.00112634
Iteration 22/25 | Loss: 0.00112767
Iteration 23/25 | Loss: 0.00112599
Iteration 24/25 | Loss: 0.00112496
Iteration 25/25 | Loss: 0.00112445

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39994431
Iteration 2/25 | Loss: 0.00432761
Iteration 3/25 | Loss: 0.00432761
Iteration 4/25 | Loss: 0.00432761
Iteration 5/25 | Loss: 0.00432761
Iteration 6/25 | Loss: 0.00432761
Iteration 7/25 | Loss: 0.00432761
Iteration 8/25 | Loss: 0.00432761
Iteration 9/25 | Loss: 0.00432761
Iteration 10/25 | Loss: 0.00432761
Iteration 11/25 | Loss: 0.00432761
Iteration 12/25 | Loss: 0.00432761
Iteration 13/25 | Loss: 0.00432761
Iteration 14/25 | Loss: 0.00432761
Iteration 15/25 | Loss: 0.00432761
Iteration 16/25 | Loss: 0.00432761
Iteration 17/25 | Loss: 0.00432761
Iteration 18/25 | Loss: 0.00432761
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.004327609669417143, 0.004327609669417143, 0.004327609669417143, 0.004327609669417143, 0.004327609669417143]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.004327609669417143

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00432761
Iteration 2/1000 | Loss: 0.00093536
Iteration 3/1000 | Loss: 0.00065885
Iteration 4/1000 | Loss: 0.00053153
Iteration 5/1000 | Loss: 0.00130263
Iteration 6/1000 | Loss: 0.01367934
Iteration 7/1000 | Loss: 0.00212587
Iteration 8/1000 | Loss: 0.00093635
Iteration 9/1000 | Loss: 0.00842441
Iteration 10/1000 | Loss: 0.00253804
Iteration 11/1000 | Loss: 0.00950050
Iteration 12/1000 | Loss: 0.00114825
Iteration 13/1000 | Loss: 0.00104160
Iteration 14/1000 | Loss: 0.00129471
Iteration 15/1000 | Loss: 0.00053874
Iteration 16/1000 | Loss: 0.00047051
Iteration 17/1000 | Loss: 0.00020309
Iteration 18/1000 | Loss: 0.00084899
Iteration 19/1000 | Loss: 0.00180988
Iteration 20/1000 | Loss: 0.00211193
Iteration 21/1000 | Loss: 0.00026088
Iteration 22/1000 | Loss: 0.00032621
Iteration 23/1000 | Loss: 0.00008020
Iteration 24/1000 | Loss: 0.00023393
Iteration 25/1000 | Loss: 0.00032265
Iteration 26/1000 | Loss: 0.00034063
Iteration 27/1000 | Loss: 0.00060142
Iteration 28/1000 | Loss: 0.00021178
Iteration 29/1000 | Loss: 0.00008682
Iteration 30/1000 | Loss: 0.00017131
Iteration 31/1000 | Loss: 0.00012669
Iteration 32/1000 | Loss: 0.00015993
Iteration 33/1000 | Loss: 0.00004868
Iteration 34/1000 | Loss: 0.00004082
Iteration 35/1000 | Loss: 0.00021619
Iteration 36/1000 | Loss: 0.00009303
Iteration 37/1000 | Loss: 0.00039768
Iteration 38/1000 | Loss: 0.00025591
Iteration 39/1000 | Loss: 0.00018979
Iteration 40/1000 | Loss: 0.00004605
Iteration 41/1000 | Loss: 0.00003482
Iteration 42/1000 | Loss: 0.00003278
Iteration 43/1000 | Loss: 0.00040478
Iteration 44/1000 | Loss: 0.00004764
Iteration 45/1000 | Loss: 0.00045759
Iteration 46/1000 | Loss: 0.00025461
Iteration 47/1000 | Loss: 0.00034702
Iteration 48/1000 | Loss: 0.00060821
Iteration 49/1000 | Loss: 0.00056826
Iteration 50/1000 | Loss: 0.00009254
Iteration 51/1000 | Loss: 0.00003338
Iteration 52/1000 | Loss: 0.00010266
Iteration 53/1000 | Loss: 0.00010343
Iteration 54/1000 | Loss: 0.00002778
Iteration 55/1000 | Loss: 0.00002660
Iteration 56/1000 | Loss: 0.00002567
Iteration 57/1000 | Loss: 0.00002522
Iteration 58/1000 | Loss: 0.00042341
Iteration 59/1000 | Loss: 0.00026622
Iteration 60/1000 | Loss: 0.00025883
Iteration 61/1000 | Loss: 0.00026527
Iteration 62/1000 | Loss: 0.00021353
Iteration 63/1000 | Loss: 0.00010219
Iteration 64/1000 | Loss: 0.00015775
Iteration 65/1000 | Loss: 0.00002532
Iteration 66/1000 | Loss: 0.00002426
Iteration 67/1000 | Loss: 0.00002349
Iteration 68/1000 | Loss: 0.00002290
Iteration 69/1000 | Loss: 0.00002268
Iteration 70/1000 | Loss: 0.00002260
Iteration 71/1000 | Loss: 0.00002242
Iteration 72/1000 | Loss: 0.00002241
Iteration 73/1000 | Loss: 0.00002240
Iteration 74/1000 | Loss: 0.00002222
Iteration 75/1000 | Loss: 0.00002212
Iteration 76/1000 | Loss: 0.00002211
Iteration 77/1000 | Loss: 0.00002211
Iteration 78/1000 | Loss: 0.00002211
Iteration 79/1000 | Loss: 0.00002210
Iteration 80/1000 | Loss: 0.00002210
Iteration 81/1000 | Loss: 0.00002210
Iteration 82/1000 | Loss: 0.00002208
Iteration 83/1000 | Loss: 0.00002207
Iteration 84/1000 | Loss: 0.00002206
Iteration 85/1000 | Loss: 0.00002206
Iteration 86/1000 | Loss: 0.00002206
Iteration 87/1000 | Loss: 0.00002206
Iteration 88/1000 | Loss: 0.00002206
Iteration 89/1000 | Loss: 0.00002205
Iteration 90/1000 | Loss: 0.00002205
Iteration 91/1000 | Loss: 0.00002205
Iteration 92/1000 | Loss: 0.00002205
Iteration 93/1000 | Loss: 0.00002205
Iteration 94/1000 | Loss: 0.00002205
Iteration 95/1000 | Loss: 0.00002205
Iteration 96/1000 | Loss: 0.00002205
Iteration 97/1000 | Loss: 0.00002204
Iteration 98/1000 | Loss: 0.00002204
Iteration 99/1000 | Loss: 0.00002203
Iteration 100/1000 | Loss: 0.00002203
Iteration 101/1000 | Loss: 0.00002203
Iteration 102/1000 | Loss: 0.00002202
Iteration 103/1000 | Loss: 0.00002202
Iteration 104/1000 | Loss: 0.00002202
Iteration 105/1000 | Loss: 0.00002202
Iteration 106/1000 | Loss: 0.00002202
Iteration 107/1000 | Loss: 0.00002202
Iteration 108/1000 | Loss: 0.00002202
Iteration 109/1000 | Loss: 0.00002202
Iteration 110/1000 | Loss: 0.00002202
Iteration 111/1000 | Loss: 0.00002202
Iteration 112/1000 | Loss: 0.00002202
Iteration 113/1000 | Loss: 0.00002201
Iteration 114/1000 | Loss: 0.00002201
Iteration 115/1000 | Loss: 0.00002200
Iteration 116/1000 | Loss: 0.00002200
Iteration 117/1000 | Loss: 0.00002200
Iteration 118/1000 | Loss: 0.00002200
Iteration 119/1000 | Loss: 0.00002199
Iteration 120/1000 | Loss: 0.00002199
Iteration 121/1000 | Loss: 0.00002199
Iteration 122/1000 | Loss: 0.00002199
Iteration 123/1000 | Loss: 0.00002198
Iteration 124/1000 | Loss: 0.00002198
Iteration 125/1000 | Loss: 0.00002198
Iteration 126/1000 | Loss: 0.00002198
Iteration 127/1000 | Loss: 0.00002198
Iteration 128/1000 | Loss: 0.00002198
Iteration 129/1000 | Loss: 0.00002198
Iteration 130/1000 | Loss: 0.00002198
Iteration 131/1000 | Loss: 0.00002198
Iteration 132/1000 | Loss: 0.00002198
Iteration 133/1000 | Loss: 0.00002198
Iteration 134/1000 | Loss: 0.00002197
Iteration 135/1000 | Loss: 0.00002197
Iteration 136/1000 | Loss: 0.00002197
Iteration 137/1000 | Loss: 0.00002197
Iteration 138/1000 | Loss: 0.00002196
Iteration 139/1000 | Loss: 0.00002196
Iteration 140/1000 | Loss: 0.00002195
Iteration 141/1000 | Loss: 0.00002195
Iteration 142/1000 | Loss: 0.00002195
Iteration 143/1000 | Loss: 0.00002195
Iteration 144/1000 | Loss: 0.00002194
Iteration 145/1000 | Loss: 0.00002194
Iteration 146/1000 | Loss: 0.00002194
Iteration 147/1000 | Loss: 0.00002193
Iteration 148/1000 | Loss: 0.00002193
Iteration 149/1000 | Loss: 0.00002193
Iteration 150/1000 | Loss: 0.00002193
Iteration 151/1000 | Loss: 0.00002193
Iteration 152/1000 | Loss: 0.00002193
Iteration 153/1000 | Loss: 0.00002193
Iteration 154/1000 | Loss: 0.00002192
Iteration 155/1000 | Loss: 0.00002192
Iteration 156/1000 | Loss: 0.00002192
Iteration 157/1000 | Loss: 0.00002192
Iteration 158/1000 | Loss: 0.00002192
Iteration 159/1000 | Loss: 0.00002192
Iteration 160/1000 | Loss: 0.00002192
Iteration 161/1000 | Loss: 0.00002192
Iteration 162/1000 | Loss: 0.00002192
Iteration 163/1000 | Loss: 0.00002192
Iteration 164/1000 | Loss: 0.00002192
Iteration 165/1000 | Loss: 0.00002192
Iteration 166/1000 | Loss: 0.00002192
Iteration 167/1000 | Loss: 0.00002191
Iteration 168/1000 | Loss: 0.00002191
Iteration 169/1000 | Loss: 0.00002191
Iteration 170/1000 | Loss: 0.00002190
Iteration 171/1000 | Loss: 0.00002190
Iteration 172/1000 | Loss: 0.00002190
Iteration 173/1000 | Loss: 0.00002189
Iteration 174/1000 | Loss: 0.00002189
Iteration 175/1000 | Loss: 0.00002189
Iteration 176/1000 | Loss: 0.00002189
Iteration 177/1000 | Loss: 0.00002189
Iteration 178/1000 | Loss: 0.00002189
Iteration 179/1000 | Loss: 0.00002189
Iteration 180/1000 | Loss: 0.00002189
Iteration 181/1000 | Loss: 0.00002188
Iteration 182/1000 | Loss: 0.00002188
Iteration 183/1000 | Loss: 0.00002188
Iteration 184/1000 | Loss: 0.00002188
Iteration 185/1000 | Loss: 0.00002188
Iteration 186/1000 | Loss: 0.00002188
Iteration 187/1000 | Loss: 0.00002188
Iteration 188/1000 | Loss: 0.00002188
Iteration 189/1000 | Loss: 0.00002188
Iteration 190/1000 | Loss: 0.00002188
Iteration 191/1000 | Loss: 0.00002187
Iteration 192/1000 | Loss: 0.00002187
Iteration 193/1000 | Loss: 0.00002187
Iteration 194/1000 | Loss: 0.00002187
Iteration 195/1000 | Loss: 0.00002187
Iteration 196/1000 | Loss: 0.00002187
Iteration 197/1000 | Loss: 0.00002187
Iteration 198/1000 | Loss: 0.00002187
Iteration 199/1000 | Loss: 0.00002187
Iteration 200/1000 | Loss: 0.00002187
Iteration 201/1000 | Loss: 0.00002187
Iteration 202/1000 | Loss: 0.00002187
Iteration 203/1000 | Loss: 0.00002187
Iteration 204/1000 | Loss: 0.00002186
Iteration 205/1000 | Loss: 0.00002186
Iteration 206/1000 | Loss: 0.00002186
Iteration 207/1000 | Loss: 0.00002186
Iteration 208/1000 | Loss: 0.00002186
Iteration 209/1000 | Loss: 0.00002186
Iteration 210/1000 | Loss: 0.00002186
Iteration 211/1000 | Loss: 0.00002186
Iteration 212/1000 | Loss: 0.00002186
Iteration 213/1000 | Loss: 0.00002186
Iteration 214/1000 | Loss: 0.00002186
Iteration 215/1000 | Loss: 0.00002186
Iteration 216/1000 | Loss: 0.00002185
Iteration 217/1000 | Loss: 0.00002185
Iteration 218/1000 | Loss: 0.00002185
Iteration 219/1000 | Loss: 0.00002185
Iteration 220/1000 | Loss: 0.00002185
Iteration 221/1000 | Loss: 0.00002185
Iteration 222/1000 | Loss: 0.00002185
Iteration 223/1000 | Loss: 0.00002185
Iteration 224/1000 | Loss: 0.00002185
Iteration 225/1000 | Loss: 0.00002185
Iteration 226/1000 | Loss: 0.00002185
Iteration 227/1000 | Loss: 0.00002185
Iteration 228/1000 | Loss: 0.00002185
Iteration 229/1000 | Loss: 0.00002185
Iteration 230/1000 | Loss: 0.00002185
Iteration 231/1000 | Loss: 0.00002185
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 231. Stopping optimization.
Last 5 losses: [2.1850390112376772e-05, 2.1850390112376772e-05, 2.1850390112376772e-05, 2.1850390112376772e-05, 2.1850390112376772e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1850390112376772e-05

Optimization complete. Final v2v error: 3.864962339401245 mm

Highest mean error: 11.550590515136719 mm for frame 16

Lowest mean error: 3.408460855484009 mm for frame 9

Saving results

Total time: 175.3425953388214
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ben_posed_008/1062/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_008/1062.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_008/1062
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00570837
Iteration 2/25 | Loss: 0.00085082
Iteration 3/25 | Loss: 0.00066309
Iteration 4/25 | Loss: 0.00063643
Iteration 5/25 | Loss: 0.00063171
Iteration 6/25 | Loss: 0.00063086
Iteration 7/25 | Loss: 0.00063086
Iteration 8/25 | Loss: 0.00063086
Iteration 9/25 | Loss: 0.00063086
Iteration 10/25 | Loss: 0.00063086
Iteration 11/25 | Loss: 0.00063086
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0006308640004135668, 0.0006308640004135668, 0.0006308640004135668, 0.0006308640004135668, 0.0006308640004135668]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006308640004135668

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42168748
Iteration 2/25 | Loss: 0.00014754
Iteration 3/25 | Loss: 0.00014747
Iteration 4/25 | Loss: 0.00014747
Iteration 5/25 | Loss: 0.00014747
Iteration 6/25 | Loss: 0.00014747
Iteration 7/25 | Loss: 0.00014747
Iteration 8/25 | Loss: 0.00014747
Iteration 9/25 | Loss: 0.00014747
Iteration 10/25 | Loss: 0.00014747
Iteration 11/25 | Loss: 0.00014747
Iteration 12/25 | Loss: 0.00014747
Iteration 13/25 | Loss: 0.00014747
Iteration 14/25 | Loss: 0.00014747
Iteration 15/25 | Loss: 0.00014747
Iteration 16/25 | Loss: 0.00014747
Iteration 17/25 | Loss: 0.00014747
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.00014746685337740928, 0.00014746685337740928, 0.00014746685337740928, 0.00014746685337740928, 0.00014746685337740928]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00014746685337740928

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00014747
Iteration 2/1000 | Loss: 0.00002219
Iteration 3/1000 | Loss: 0.00001781
Iteration 4/1000 | Loss: 0.00001680
Iteration 5/1000 | Loss: 0.00001585
Iteration 6/1000 | Loss: 0.00001530
Iteration 7/1000 | Loss: 0.00001500
Iteration 8/1000 | Loss: 0.00001493
Iteration 9/1000 | Loss: 0.00001493
Iteration 10/1000 | Loss: 0.00001488
Iteration 11/1000 | Loss: 0.00001487
Iteration 12/1000 | Loss: 0.00001486
Iteration 13/1000 | Loss: 0.00001484
Iteration 14/1000 | Loss: 0.00001484
Iteration 15/1000 | Loss: 0.00001483
Iteration 16/1000 | Loss: 0.00001479
Iteration 17/1000 | Loss: 0.00001471
Iteration 18/1000 | Loss: 0.00001471
Iteration 19/1000 | Loss: 0.00001471
Iteration 20/1000 | Loss: 0.00001471
Iteration 21/1000 | Loss: 0.00001471
Iteration 22/1000 | Loss: 0.00001471
Iteration 23/1000 | Loss: 0.00001470
Iteration 24/1000 | Loss: 0.00001470
Iteration 25/1000 | Loss: 0.00001470
Iteration 26/1000 | Loss: 0.00001470
Iteration 27/1000 | Loss: 0.00001470
Iteration 28/1000 | Loss: 0.00001470
Iteration 29/1000 | Loss: 0.00001467
Iteration 30/1000 | Loss: 0.00001463
Iteration 31/1000 | Loss: 0.00001460
Iteration 32/1000 | Loss: 0.00001460
Iteration 33/1000 | Loss: 0.00001460
Iteration 34/1000 | Loss: 0.00001460
Iteration 35/1000 | Loss: 0.00001460
Iteration 36/1000 | Loss: 0.00001459
Iteration 37/1000 | Loss: 0.00001459
Iteration 38/1000 | Loss: 0.00001459
Iteration 39/1000 | Loss: 0.00001459
Iteration 40/1000 | Loss: 0.00001459
Iteration 41/1000 | Loss: 0.00001459
Iteration 42/1000 | Loss: 0.00001459
Iteration 43/1000 | Loss: 0.00001459
Iteration 44/1000 | Loss: 0.00001459
Iteration 45/1000 | Loss: 0.00001452
Iteration 46/1000 | Loss: 0.00001450
Iteration 47/1000 | Loss: 0.00001450
Iteration 48/1000 | Loss: 0.00001450
Iteration 49/1000 | Loss: 0.00001449
Iteration 50/1000 | Loss: 0.00001449
Iteration 51/1000 | Loss: 0.00001449
Iteration 52/1000 | Loss: 0.00001449
Iteration 53/1000 | Loss: 0.00001449
Iteration 54/1000 | Loss: 0.00001449
Iteration 55/1000 | Loss: 0.00001449
Iteration 56/1000 | Loss: 0.00001449
Iteration 57/1000 | Loss: 0.00001448
Iteration 58/1000 | Loss: 0.00001448
Iteration 59/1000 | Loss: 0.00001448
Iteration 60/1000 | Loss: 0.00001447
Iteration 61/1000 | Loss: 0.00001447
Iteration 62/1000 | Loss: 0.00001446
Iteration 63/1000 | Loss: 0.00001445
Iteration 64/1000 | Loss: 0.00001445
Iteration 65/1000 | Loss: 0.00001444
Iteration 66/1000 | Loss: 0.00001444
Iteration 67/1000 | Loss: 0.00001444
Iteration 68/1000 | Loss: 0.00001444
Iteration 69/1000 | Loss: 0.00001443
Iteration 70/1000 | Loss: 0.00001443
Iteration 71/1000 | Loss: 0.00001442
Iteration 72/1000 | Loss: 0.00001440
Iteration 73/1000 | Loss: 0.00001440
Iteration 74/1000 | Loss: 0.00001439
Iteration 75/1000 | Loss: 0.00001439
Iteration 76/1000 | Loss: 0.00001439
Iteration 77/1000 | Loss: 0.00001439
Iteration 78/1000 | Loss: 0.00001439
Iteration 79/1000 | Loss: 0.00001439
Iteration 80/1000 | Loss: 0.00001439
Iteration 81/1000 | Loss: 0.00001439
Iteration 82/1000 | Loss: 0.00001439
Iteration 83/1000 | Loss: 0.00001439
Iteration 84/1000 | Loss: 0.00001439
Iteration 85/1000 | Loss: 0.00001438
Iteration 86/1000 | Loss: 0.00001438
Iteration 87/1000 | Loss: 0.00001438
Iteration 88/1000 | Loss: 0.00001438
Iteration 89/1000 | Loss: 0.00001438
Iteration 90/1000 | Loss: 0.00001437
Iteration 91/1000 | Loss: 0.00001437
Iteration 92/1000 | Loss: 0.00001436
Iteration 93/1000 | Loss: 0.00001436
Iteration 94/1000 | Loss: 0.00001436
Iteration 95/1000 | Loss: 0.00001435
Iteration 96/1000 | Loss: 0.00001435
Iteration 97/1000 | Loss: 0.00001435
Iteration 98/1000 | Loss: 0.00001435
Iteration 99/1000 | Loss: 0.00001435
Iteration 100/1000 | Loss: 0.00001435
Iteration 101/1000 | Loss: 0.00001435
Iteration 102/1000 | Loss: 0.00001435
Iteration 103/1000 | Loss: 0.00001434
Iteration 104/1000 | Loss: 0.00001434
Iteration 105/1000 | Loss: 0.00001433
Iteration 106/1000 | Loss: 0.00001433
Iteration 107/1000 | Loss: 0.00001433
Iteration 108/1000 | Loss: 0.00001431
Iteration 109/1000 | Loss: 0.00001431
Iteration 110/1000 | Loss: 0.00001431
Iteration 111/1000 | Loss: 0.00001431
Iteration 112/1000 | Loss: 0.00001431
Iteration 113/1000 | Loss: 0.00001431
Iteration 114/1000 | Loss: 0.00001431
Iteration 115/1000 | Loss: 0.00001431
Iteration 116/1000 | Loss: 0.00001431
Iteration 117/1000 | Loss: 0.00001430
Iteration 118/1000 | Loss: 0.00001430
Iteration 119/1000 | Loss: 0.00001430
Iteration 120/1000 | Loss: 0.00001430
Iteration 121/1000 | Loss: 0.00001430
Iteration 122/1000 | Loss: 0.00001430
Iteration 123/1000 | Loss: 0.00001430
Iteration 124/1000 | Loss: 0.00001430
Iteration 125/1000 | Loss: 0.00001430
Iteration 126/1000 | Loss: 0.00001430
Iteration 127/1000 | Loss: 0.00001430
Iteration 128/1000 | Loss: 0.00001429
Iteration 129/1000 | Loss: 0.00001429
Iteration 130/1000 | Loss: 0.00001428
Iteration 131/1000 | Loss: 0.00001428
Iteration 132/1000 | Loss: 0.00001427
Iteration 133/1000 | Loss: 0.00001427
Iteration 134/1000 | Loss: 0.00001427
Iteration 135/1000 | Loss: 0.00001426
Iteration 136/1000 | Loss: 0.00001426
Iteration 137/1000 | Loss: 0.00001426
Iteration 138/1000 | Loss: 0.00001426
Iteration 139/1000 | Loss: 0.00001425
Iteration 140/1000 | Loss: 0.00001425
Iteration 141/1000 | Loss: 0.00001425
Iteration 142/1000 | Loss: 0.00001424
Iteration 143/1000 | Loss: 0.00001424
Iteration 144/1000 | Loss: 0.00001424
Iteration 145/1000 | Loss: 0.00001424
Iteration 146/1000 | Loss: 0.00001424
Iteration 147/1000 | Loss: 0.00001423
Iteration 148/1000 | Loss: 0.00001423
Iteration 149/1000 | Loss: 0.00001422
Iteration 150/1000 | Loss: 0.00001422
Iteration 151/1000 | Loss: 0.00001422
Iteration 152/1000 | Loss: 0.00001422
Iteration 153/1000 | Loss: 0.00001422
Iteration 154/1000 | Loss: 0.00001422
Iteration 155/1000 | Loss: 0.00001422
Iteration 156/1000 | Loss: 0.00001422
Iteration 157/1000 | Loss: 0.00001422
Iteration 158/1000 | Loss: 0.00001422
Iteration 159/1000 | Loss: 0.00001422
Iteration 160/1000 | Loss: 0.00001421
Iteration 161/1000 | Loss: 0.00001421
Iteration 162/1000 | Loss: 0.00001421
Iteration 163/1000 | Loss: 0.00001421
Iteration 164/1000 | Loss: 0.00001421
Iteration 165/1000 | Loss: 0.00001421
Iteration 166/1000 | Loss: 0.00001420
Iteration 167/1000 | Loss: 0.00001420
Iteration 168/1000 | Loss: 0.00001420
Iteration 169/1000 | Loss: 0.00001420
Iteration 170/1000 | Loss: 0.00001420
Iteration 171/1000 | Loss: 0.00001420
Iteration 172/1000 | Loss: 0.00001420
Iteration 173/1000 | Loss: 0.00001420
Iteration 174/1000 | Loss: 0.00001420
Iteration 175/1000 | Loss: 0.00001420
Iteration 176/1000 | Loss: 0.00001419
Iteration 177/1000 | Loss: 0.00001419
Iteration 178/1000 | Loss: 0.00001419
Iteration 179/1000 | Loss: 0.00001419
Iteration 180/1000 | Loss: 0.00001419
Iteration 181/1000 | Loss: 0.00001419
Iteration 182/1000 | Loss: 0.00001419
Iteration 183/1000 | Loss: 0.00001419
Iteration 184/1000 | Loss: 0.00001419
Iteration 185/1000 | Loss: 0.00001419
Iteration 186/1000 | Loss: 0.00001419
Iteration 187/1000 | Loss: 0.00001419
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 187. Stopping optimization.
Last 5 losses: [1.419385716872057e-05, 1.419385716872057e-05, 1.419385716872057e-05, 1.419385716872057e-05, 1.419385716872057e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.419385716872057e-05

Optimization complete. Final v2v error: 3.19762921333313 mm

Highest mean error: 3.410830020904541 mm for frame 57

Lowest mean error: 3.0606367588043213 mm for frame 197

Saving results

Total time: 41.47113108634949
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ben_posed_008/1050/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_008/1050.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_008/1050
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00829238
Iteration 2/25 | Loss: 0.00073904
Iteration 3/25 | Loss: 0.00056732
Iteration 4/25 | Loss: 0.00054726
Iteration 5/25 | Loss: 0.00054093
Iteration 6/25 | Loss: 0.00053949
Iteration 7/25 | Loss: 0.00053916
Iteration 8/25 | Loss: 0.00053916
Iteration 9/25 | Loss: 0.00053916
Iteration 10/25 | Loss: 0.00053916
Iteration 11/25 | Loss: 0.00053916
Iteration 12/25 | Loss: 0.00053916
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0005391622544266284, 0.0005391622544266284, 0.0005391622544266284, 0.0005391622544266284, 0.0005391622544266284]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005391622544266284

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45550978
Iteration 2/25 | Loss: 0.00013736
Iteration 3/25 | Loss: 0.00013736
Iteration 4/25 | Loss: 0.00013736
Iteration 5/25 | Loss: 0.00013736
Iteration 6/25 | Loss: 0.00013736
Iteration 7/25 | Loss: 0.00013736
Iteration 8/25 | Loss: 0.00013736
Iteration 9/25 | Loss: 0.00013736
Iteration 10/25 | Loss: 0.00013736
Iteration 11/25 | Loss: 0.00013736
Iteration 12/25 | Loss: 0.00013736
Iteration 13/25 | Loss: 0.00013736
Iteration 14/25 | Loss: 0.00013736
Iteration 15/25 | Loss: 0.00013736
Iteration 16/25 | Loss: 0.00013736
Iteration 17/25 | Loss: 0.00013736
Iteration 18/25 | Loss: 0.00013736
Iteration 19/25 | Loss: 0.00013736
Iteration 20/25 | Loss: 0.00013736
Iteration 21/25 | Loss: 0.00013736
Iteration 22/25 | Loss: 0.00013736
Iteration 23/25 | Loss: 0.00013736
Iteration 24/25 | Loss: 0.00013736
Iteration 25/25 | Loss: 0.00013736

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00013736
Iteration 2/1000 | Loss: 0.00001616
Iteration 3/1000 | Loss: 0.00001108
Iteration 4/1000 | Loss: 0.00001043
Iteration 5/1000 | Loss: 0.00000991
Iteration 6/1000 | Loss: 0.00000953
Iteration 7/1000 | Loss: 0.00000948
Iteration 8/1000 | Loss: 0.00000923
Iteration 9/1000 | Loss: 0.00000919
Iteration 10/1000 | Loss: 0.00000915
Iteration 11/1000 | Loss: 0.00000914
Iteration 12/1000 | Loss: 0.00000914
Iteration 13/1000 | Loss: 0.00000911
Iteration 14/1000 | Loss: 0.00000908
Iteration 15/1000 | Loss: 0.00000907
Iteration 16/1000 | Loss: 0.00000907
Iteration 17/1000 | Loss: 0.00000906
Iteration 18/1000 | Loss: 0.00000906
Iteration 19/1000 | Loss: 0.00000906
Iteration 20/1000 | Loss: 0.00000906
Iteration 21/1000 | Loss: 0.00000905
Iteration 22/1000 | Loss: 0.00000904
Iteration 23/1000 | Loss: 0.00000903
Iteration 24/1000 | Loss: 0.00000903
Iteration 25/1000 | Loss: 0.00000902
Iteration 26/1000 | Loss: 0.00000902
Iteration 27/1000 | Loss: 0.00000902
Iteration 28/1000 | Loss: 0.00000901
Iteration 29/1000 | Loss: 0.00000901
Iteration 30/1000 | Loss: 0.00000900
Iteration 31/1000 | Loss: 0.00000900
Iteration 32/1000 | Loss: 0.00000899
Iteration 33/1000 | Loss: 0.00000899
Iteration 34/1000 | Loss: 0.00000898
Iteration 35/1000 | Loss: 0.00000898
Iteration 36/1000 | Loss: 0.00000897
Iteration 37/1000 | Loss: 0.00000897
Iteration 38/1000 | Loss: 0.00000896
Iteration 39/1000 | Loss: 0.00000896
Iteration 40/1000 | Loss: 0.00000895
Iteration 41/1000 | Loss: 0.00000895
Iteration 42/1000 | Loss: 0.00000895
Iteration 43/1000 | Loss: 0.00000894
Iteration 44/1000 | Loss: 0.00000894
Iteration 45/1000 | Loss: 0.00000894
Iteration 46/1000 | Loss: 0.00000894
Iteration 47/1000 | Loss: 0.00000893
Iteration 48/1000 | Loss: 0.00000893
Iteration 49/1000 | Loss: 0.00000893
Iteration 50/1000 | Loss: 0.00000893
Iteration 51/1000 | Loss: 0.00000892
Iteration 52/1000 | Loss: 0.00000892
Iteration 53/1000 | Loss: 0.00000892
Iteration 54/1000 | Loss: 0.00000892
Iteration 55/1000 | Loss: 0.00000892
Iteration 56/1000 | Loss: 0.00000891
Iteration 57/1000 | Loss: 0.00000891
Iteration 58/1000 | Loss: 0.00000890
Iteration 59/1000 | Loss: 0.00000889
Iteration 60/1000 | Loss: 0.00000889
Iteration 61/1000 | Loss: 0.00000889
Iteration 62/1000 | Loss: 0.00000889
Iteration 63/1000 | Loss: 0.00000889
Iteration 64/1000 | Loss: 0.00000889
Iteration 65/1000 | Loss: 0.00000888
Iteration 66/1000 | Loss: 0.00000888
Iteration 67/1000 | Loss: 0.00000888
Iteration 68/1000 | Loss: 0.00000888
Iteration 69/1000 | Loss: 0.00000888
Iteration 70/1000 | Loss: 0.00000888
Iteration 71/1000 | Loss: 0.00000888
Iteration 72/1000 | Loss: 0.00000888
Iteration 73/1000 | Loss: 0.00000888
Iteration 74/1000 | Loss: 0.00000887
Iteration 75/1000 | Loss: 0.00000887
Iteration 76/1000 | Loss: 0.00000887
Iteration 77/1000 | Loss: 0.00000886
Iteration 78/1000 | Loss: 0.00000886
Iteration 79/1000 | Loss: 0.00000886
Iteration 80/1000 | Loss: 0.00000886
Iteration 81/1000 | Loss: 0.00000886
Iteration 82/1000 | Loss: 0.00000886
Iteration 83/1000 | Loss: 0.00000886
Iteration 84/1000 | Loss: 0.00000886
Iteration 85/1000 | Loss: 0.00000885
Iteration 86/1000 | Loss: 0.00000885
Iteration 87/1000 | Loss: 0.00000885
Iteration 88/1000 | Loss: 0.00000885
Iteration 89/1000 | Loss: 0.00000885
Iteration 90/1000 | Loss: 0.00000885
Iteration 91/1000 | Loss: 0.00000885
Iteration 92/1000 | Loss: 0.00000884
Iteration 93/1000 | Loss: 0.00000884
Iteration 94/1000 | Loss: 0.00000883
Iteration 95/1000 | Loss: 0.00000882
Iteration 96/1000 | Loss: 0.00000882
Iteration 97/1000 | Loss: 0.00000882
Iteration 98/1000 | Loss: 0.00000882
Iteration 99/1000 | Loss: 0.00000881
Iteration 100/1000 | Loss: 0.00000881
Iteration 101/1000 | Loss: 0.00000880
Iteration 102/1000 | Loss: 0.00000880
Iteration 103/1000 | Loss: 0.00000880
Iteration 104/1000 | Loss: 0.00000879
Iteration 105/1000 | Loss: 0.00000879
Iteration 106/1000 | Loss: 0.00000879
Iteration 107/1000 | Loss: 0.00000879
Iteration 108/1000 | Loss: 0.00000879
Iteration 109/1000 | Loss: 0.00000879
Iteration 110/1000 | Loss: 0.00000879
Iteration 111/1000 | Loss: 0.00000879
Iteration 112/1000 | Loss: 0.00000879
Iteration 113/1000 | Loss: 0.00000878
Iteration 114/1000 | Loss: 0.00000878
Iteration 115/1000 | Loss: 0.00000878
Iteration 116/1000 | Loss: 0.00000878
Iteration 117/1000 | Loss: 0.00000878
Iteration 118/1000 | Loss: 0.00000878
Iteration 119/1000 | Loss: 0.00000878
Iteration 120/1000 | Loss: 0.00000878
Iteration 121/1000 | Loss: 0.00000878
Iteration 122/1000 | Loss: 0.00000878
Iteration 123/1000 | Loss: 0.00000878
Iteration 124/1000 | Loss: 0.00000878
Iteration 125/1000 | Loss: 0.00000878
Iteration 126/1000 | Loss: 0.00000877
Iteration 127/1000 | Loss: 0.00000877
Iteration 128/1000 | Loss: 0.00000877
Iteration 129/1000 | Loss: 0.00000877
Iteration 130/1000 | Loss: 0.00000877
Iteration 131/1000 | Loss: 0.00000877
Iteration 132/1000 | Loss: 0.00000877
Iteration 133/1000 | Loss: 0.00000877
Iteration 134/1000 | Loss: 0.00000877
Iteration 135/1000 | Loss: 0.00000877
Iteration 136/1000 | Loss: 0.00000877
Iteration 137/1000 | Loss: 0.00000877
Iteration 138/1000 | Loss: 0.00000877
Iteration 139/1000 | Loss: 0.00000877
Iteration 140/1000 | Loss: 0.00000877
Iteration 141/1000 | Loss: 0.00000877
Iteration 142/1000 | Loss: 0.00000877
Iteration 143/1000 | Loss: 0.00000877
Iteration 144/1000 | Loss: 0.00000877
Iteration 145/1000 | Loss: 0.00000876
Iteration 146/1000 | Loss: 0.00000876
Iteration 147/1000 | Loss: 0.00000876
Iteration 148/1000 | Loss: 0.00000876
Iteration 149/1000 | Loss: 0.00000876
Iteration 150/1000 | Loss: 0.00000876
Iteration 151/1000 | Loss: 0.00000876
Iteration 152/1000 | Loss: 0.00000876
Iteration 153/1000 | Loss: 0.00000876
Iteration 154/1000 | Loss: 0.00000876
Iteration 155/1000 | Loss: 0.00000876
Iteration 156/1000 | Loss: 0.00000876
Iteration 157/1000 | Loss: 0.00000876
Iteration 158/1000 | Loss: 0.00000876
Iteration 159/1000 | Loss: 0.00000876
Iteration 160/1000 | Loss: 0.00000876
Iteration 161/1000 | Loss: 0.00000876
Iteration 162/1000 | Loss: 0.00000876
Iteration 163/1000 | Loss: 0.00000876
Iteration 164/1000 | Loss: 0.00000876
Iteration 165/1000 | Loss: 0.00000876
Iteration 166/1000 | Loss: 0.00000875
Iteration 167/1000 | Loss: 0.00000875
Iteration 168/1000 | Loss: 0.00000875
Iteration 169/1000 | Loss: 0.00000875
Iteration 170/1000 | Loss: 0.00000875
Iteration 171/1000 | Loss: 0.00000875
Iteration 172/1000 | Loss: 0.00000875
Iteration 173/1000 | Loss: 0.00000875
Iteration 174/1000 | Loss: 0.00000875
Iteration 175/1000 | Loss: 0.00000875
Iteration 176/1000 | Loss: 0.00000875
Iteration 177/1000 | Loss: 0.00000875
Iteration 178/1000 | Loss: 0.00000875
Iteration 179/1000 | Loss: 0.00000875
Iteration 180/1000 | Loss: 0.00000875
Iteration 181/1000 | Loss: 0.00000875
Iteration 182/1000 | Loss: 0.00000875
Iteration 183/1000 | Loss: 0.00000875
Iteration 184/1000 | Loss: 0.00000875
Iteration 185/1000 | Loss: 0.00000875
Iteration 186/1000 | Loss: 0.00000874
Iteration 187/1000 | Loss: 0.00000874
Iteration 188/1000 | Loss: 0.00000874
Iteration 189/1000 | Loss: 0.00000874
Iteration 190/1000 | Loss: 0.00000874
Iteration 191/1000 | Loss: 0.00000874
Iteration 192/1000 | Loss: 0.00000874
Iteration 193/1000 | Loss: 0.00000874
Iteration 194/1000 | Loss: 0.00000874
Iteration 195/1000 | Loss: 0.00000873
Iteration 196/1000 | Loss: 0.00000873
Iteration 197/1000 | Loss: 0.00000873
Iteration 198/1000 | Loss: 0.00000873
Iteration 199/1000 | Loss: 0.00000873
Iteration 200/1000 | Loss: 0.00000873
Iteration 201/1000 | Loss: 0.00000873
Iteration 202/1000 | Loss: 0.00000872
Iteration 203/1000 | Loss: 0.00000872
Iteration 204/1000 | Loss: 0.00000872
Iteration 205/1000 | Loss: 0.00000872
Iteration 206/1000 | Loss: 0.00000872
Iteration 207/1000 | Loss: 0.00000872
Iteration 208/1000 | Loss: 0.00000872
Iteration 209/1000 | Loss: 0.00000872
Iteration 210/1000 | Loss: 0.00000872
Iteration 211/1000 | Loss: 0.00000872
Iteration 212/1000 | Loss: 0.00000872
Iteration 213/1000 | Loss: 0.00000872
Iteration 214/1000 | Loss: 0.00000872
Iteration 215/1000 | Loss: 0.00000872
Iteration 216/1000 | Loss: 0.00000872
Iteration 217/1000 | Loss: 0.00000872
Iteration 218/1000 | Loss: 0.00000871
Iteration 219/1000 | Loss: 0.00000871
Iteration 220/1000 | Loss: 0.00000871
Iteration 221/1000 | Loss: 0.00000871
Iteration 222/1000 | Loss: 0.00000871
Iteration 223/1000 | Loss: 0.00000871
Iteration 224/1000 | Loss: 0.00000871
Iteration 225/1000 | Loss: 0.00000871
Iteration 226/1000 | Loss: 0.00000871
Iteration 227/1000 | Loss: 0.00000871
Iteration 228/1000 | Loss: 0.00000871
Iteration 229/1000 | Loss: 0.00000871
Iteration 230/1000 | Loss: 0.00000871
Iteration 231/1000 | Loss: 0.00000871
Iteration 232/1000 | Loss: 0.00000871
Iteration 233/1000 | Loss: 0.00000871
Iteration 234/1000 | Loss: 0.00000870
Iteration 235/1000 | Loss: 0.00000870
Iteration 236/1000 | Loss: 0.00000870
Iteration 237/1000 | Loss: 0.00000870
Iteration 238/1000 | Loss: 0.00000870
Iteration 239/1000 | Loss: 0.00000870
Iteration 240/1000 | Loss: 0.00000870
Iteration 241/1000 | Loss: 0.00000870
Iteration 242/1000 | Loss: 0.00000870
Iteration 243/1000 | Loss: 0.00000870
Iteration 244/1000 | Loss: 0.00000870
Iteration 245/1000 | Loss: 0.00000870
Iteration 246/1000 | Loss: 0.00000870
Iteration 247/1000 | Loss: 0.00000869
Iteration 248/1000 | Loss: 0.00000869
Iteration 249/1000 | Loss: 0.00000869
Iteration 250/1000 | Loss: 0.00000869
Iteration 251/1000 | Loss: 0.00000869
Iteration 252/1000 | Loss: 0.00000869
Iteration 253/1000 | Loss: 0.00000869
Iteration 254/1000 | Loss: 0.00000869
Iteration 255/1000 | Loss: 0.00000869
Iteration 256/1000 | Loss: 0.00000869
Iteration 257/1000 | Loss: 0.00000869
Iteration 258/1000 | Loss: 0.00000869
Iteration 259/1000 | Loss: 0.00000869
Iteration 260/1000 | Loss: 0.00000869
Iteration 261/1000 | Loss: 0.00000869
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 261. Stopping optimization.
Last 5 losses: [8.68938059284119e-06, 8.68938059284119e-06, 8.68938059284119e-06, 8.68938059284119e-06, 8.68938059284119e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.68938059284119e-06

Optimization complete. Final v2v error: 2.4897735118865967 mm

Highest mean error: 2.6316351890563965 mm for frame 66

Lowest mean error: 2.3839404582977295 mm for frame 3

Saving results

Total time: 37.8130989074707
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ben_posed_008/1080/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_008/1080.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_008/1080
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00848517
Iteration 2/25 | Loss: 0.00109626
Iteration 3/25 | Loss: 0.00079776
Iteration 4/25 | Loss: 0.00068097
Iteration 5/25 | Loss: 0.00067213
Iteration 6/25 | Loss: 0.00065351
Iteration 7/25 | Loss: 0.00065073
Iteration 8/25 | Loss: 0.00065019
Iteration 9/25 | Loss: 0.00065008
Iteration 10/25 | Loss: 0.00065007
Iteration 11/25 | Loss: 0.00065007
Iteration 12/25 | Loss: 0.00065007
Iteration 13/25 | Loss: 0.00065007
Iteration 14/25 | Loss: 0.00065007
Iteration 15/25 | Loss: 0.00065007
Iteration 16/25 | Loss: 0.00065007
Iteration 17/25 | Loss: 0.00065007
Iteration 18/25 | Loss: 0.00065006
Iteration 19/25 | Loss: 0.00065006
Iteration 20/25 | Loss: 0.00065006
Iteration 21/25 | Loss: 0.00065006
Iteration 22/25 | Loss: 0.00065006
Iteration 23/25 | Loss: 0.00065006
Iteration 24/25 | Loss: 0.00065006
Iteration 25/25 | Loss: 0.00065006

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.02096081
Iteration 2/25 | Loss: 0.00023474
Iteration 3/25 | Loss: 0.00023469
Iteration 4/25 | Loss: 0.00023469
Iteration 5/25 | Loss: 0.00023469
Iteration 6/25 | Loss: 0.00023469
Iteration 7/25 | Loss: 0.00023469
Iteration 8/25 | Loss: 0.00023469
Iteration 9/25 | Loss: 0.00023469
Iteration 10/25 | Loss: 0.00023469
Iteration 11/25 | Loss: 0.00023469
Iteration 12/25 | Loss: 0.00023469
Iteration 13/25 | Loss: 0.00023469
Iteration 14/25 | Loss: 0.00023469
Iteration 15/25 | Loss: 0.00023469
Iteration 16/25 | Loss: 0.00023469
Iteration 17/25 | Loss: 0.00023469
Iteration 18/25 | Loss: 0.00023469
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.00023468761355616152, 0.00023468761355616152, 0.00023468761355616152, 0.00023468761355616152, 0.00023468761355616152]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00023468761355616152

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00023469
Iteration 2/1000 | Loss: 0.00002986
Iteration 3/1000 | Loss: 0.00002183
Iteration 4/1000 | Loss: 0.00001936
Iteration 5/1000 | Loss: 0.00001850
Iteration 6/1000 | Loss: 0.00001801
Iteration 7/1000 | Loss: 0.00001767
Iteration 8/1000 | Loss: 0.00001761
Iteration 9/1000 | Loss: 0.00001736
Iteration 10/1000 | Loss: 0.00001717
Iteration 11/1000 | Loss: 0.00001714
Iteration 12/1000 | Loss: 0.00001704
Iteration 13/1000 | Loss: 0.00001702
Iteration 14/1000 | Loss: 0.00001700
Iteration 15/1000 | Loss: 0.00001699
Iteration 16/1000 | Loss: 0.00001698
Iteration 17/1000 | Loss: 0.00001697
Iteration 18/1000 | Loss: 0.00001696
Iteration 19/1000 | Loss: 0.00001692
Iteration 20/1000 | Loss: 0.00001684
Iteration 21/1000 | Loss: 0.00001681
Iteration 22/1000 | Loss: 0.00001680
Iteration 23/1000 | Loss: 0.00001679
Iteration 24/1000 | Loss: 0.00001678
Iteration 25/1000 | Loss: 0.00001674
Iteration 26/1000 | Loss: 0.00001674
Iteration 27/1000 | Loss: 0.00001672
Iteration 28/1000 | Loss: 0.00001671
Iteration 29/1000 | Loss: 0.00001671
Iteration 30/1000 | Loss: 0.00001670
Iteration 31/1000 | Loss: 0.00001670
Iteration 32/1000 | Loss: 0.00001670
Iteration 33/1000 | Loss: 0.00001669
Iteration 34/1000 | Loss: 0.00001668
Iteration 35/1000 | Loss: 0.00001668
Iteration 36/1000 | Loss: 0.00001667
Iteration 37/1000 | Loss: 0.00001667
Iteration 38/1000 | Loss: 0.00001666
Iteration 39/1000 | Loss: 0.00001666
Iteration 40/1000 | Loss: 0.00001665
Iteration 41/1000 | Loss: 0.00001665
Iteration 42/1000 | Loss: 0.00001665
Iteration 43/1000 | Loss: 0.00001664
Iteration 44/1000 | Loss: 0.00001664
Iteration 45/1000 | Loss: 0.00001663
Iteration 46/1000 | Loss: 0.00001663
Iteration 47/1000 | Loss: 0.00001662
Iteration 48/1000 | Loss: 0.00001662
Iteration 49/1000 | Loss: 0.00001662
Iteration 50/1000 | Loss: 0.00001661
Iteration 51/1000 | Loss: 0.00001661
Iteration 52/1000 | Loss: 0.00001660
Iteration 53/1000 | Loss: 0.00001660
Iteration 54/1000 | Loss: 0.00001660
Iteration 55/1000 | Loss: 0.00001659
Iteration 56/1000 | Loss: 0.00001659
Iteration 57/1000 | Loss: 0.00001658
Iteration 58/1000 | Loss: 0.00001658
Iteration 59/1000 | Loss: 0.00001656
Iteration 60/1000 | Loss: 0.00001656
Iteration 61/1000 | Loss: 0.00001655
Iteration 62/1000 | Loss: 0.00001655
Iteration 63/1000 | Loss: 0.00001655
Iteration 64/1000 | Loss: 0.00001655
Iteration 65/1000 | Loss: 0.00001653
Iteration 66/1000 | Loss: 0.00001652
Iteration 67/1000 | Loss: 0.00001652
Iteration 68/1000 | Loss: 0.00001652
Iteration 69/1000 | Loss: 0.00001652
Iteration 70/1000 | Loss: 0.00001652
Iteration 71/1000 | Loss: 0.00001652
Iteration 72/1000 | Loss: 0.00001652
Iteration 73/1000 | Loss: 0.00001652
Iteration 74/1000 | Loss: 0.00001652
Iteration 75/1000 | Loss: 0.00001652
Iteration 76/1000 | Loss: 0.00001652
Iteration 77/1000 | Loss: 0.00001651
Iteration 78/1000 | Loss: 0.00001651
Iteration 79/1000 | Loss: 0.00001651
Iteration 80/1000 | Loss: 0.00001650
Iteration 81/1000 | Loss: 0.00001650
Iteration 82/1000 | Loss: 0.00001650
Iteration 83/1000 | Loss: 0.00001649
Iteration 84/1000 | Loss: 0.00001649
Iteration 85/1000 | Loss: 0.00001649
Iteration 86/1000 | Loss: 0.00001649
Iteration 87/1000 | Loss: 0.00001649
Iteration 88/1000 | Loss: 0.00001649
Iteration 89/1000 | Loss: 0.00001649
Iteration 90/1000 | Loss: 0.00001649
Iteration 91/1000 | Loss: 0.00001649
Iteration 92/1000 | Loss: 0.00001648
Iteration 93/1000 | Loss: 0.00001648
Iteration 94/1000 | Loss: 0.00001648
Iteration 95/1000 | Loss: 0.00001648
Iteration 96/1000 | Loss: 0.00001648
Iteration 97/1000 | Loss: 0.00001648
Iteration 98/1000 | Loss: 0.00001647
Iteration 99/1000 | Loss: 0.00001647
Iteration 100/1000 | Loss: 0.00001647
Iteration 101/1000 | Loss: 0.00001647
Iteration 102/1000 | Loss: 0.00001647
Iteration 103/1000 | Loss: 0.00001647
Iteration 104/1000 | Loss: 0.00001647
Iteration 105/1000 | Loss: 0.00001647
Iteration 106/1000 | Loss: 0.00001646
Iteration 107/1000 | Loss: 0.00001646
Iteration 108/1000 | Loss: 0.00001646
Iteration 109/1000 | Loss: 0.00001646
Iteration 110/1000 | Loss: 0.00001646
Iteration 111/1000 | Loss: 0.00001646
Iteration 112/1000 | Loss: 0.00001646
Iteration 113/1000 | Loss: 0.00001646
Iteration 114/1000 | Loss: 0.00001646
Iteration 115/1000 | Loss: 0.00001645
Iteration 116/1000 | Loss: 0.00001645
Iteration 117/1000 | Loss: 0.00001645
Iteration 118/1000 | Loss: 0.00001645
Iteration 119/1000 | Loss: 0.00001645
Iteration 120/1000 | Loss: 0.00001645
Iteration 121/1000 | Loss: 0.00001645
Iteration 122/1000 | Loss: 0.00001645
Iteration 123/1000 | Loss: 0.00001645
Iteration 124/1000 | Loss: 0.00001645
Iteration 125/1000 | Loss: 0.00001645
Iteration 126/1000 | Loss: 0.00001645
Iteration 127/1000 | Loss: 0.00001644
Iteration 128/1000 | Loss: 0.00001644
Iteration 129/1000 | Loss: 0.00001644
Iteration 130/1000 | Loss: 0.00001644
Iteration 131/1000 | Loss: 0.00001644
Iteration 132/1000 | Loss: 0.00001644
Iteration 133/1000 | Loss: 0.00001643
Iteration 134/1000 | Loss: 0.00001643
Iteration 135/1000 | Loss: 0.00001643
Iteration 136/1000 | Loss: 0.00001643
Iteration 137/1000 | Loss: 0.00001643
Iteration 138/1000 | Loss: 0.00001642
Iteration 139/1000 | Loss: 0.00001642
Iteration 140/1000 | Loss: 0.00001642
Iteration 141/1000 | Loss: 0.00001642
Iteration 142/1000 | Loss: 0.00001642
Iteration 143/1000 | Loss: 0.00001642
Iteration 144/1000 | Loss: 0.00001642
Iteration 145/1000 | Loss: 0.00001642
Iteration 146/1000 | Loss: 0.00001642
Iteration 147/1000 | Loss: 0.00001642
Iteration 148/1000 | Loss: 0.00001641
Iteration 149/1000 | Loss: 0.00001641
Iteration 150/1000 | Loss: 0.00001641
Iteration 151/1000 | Loss: 0.00001641
Iteration 152/1000 | Loss: 0.00001641
Iteration 153/1000 | Loss: 0.00001641
Iteration 154/1000 | Loss: 0.00001641
Iteration 155/1000 | Loss: 0.00001641
Iteration 156/1000 | Loss: 0.00001641
Iteration 157/1000 | Loss: 0.00001641
Iteration 158/1000 | Loss: 0.00001641
Iteration 159/1000 | Loss: 0.00001641
Iteration 160/1000 | Loss: 0.00001641
Iteration 161/1000 | Loss: 0.00001641
Iteration 162/1000 | Loss: 0.00001640
Iteration 163/1000 | Loss: 0.00001640
Iteration 164/1000 | Loss: 0.00001640
Iteration 165/1000 | Loss: 0.00001640
Iteration 166/1000 | Loss: 0.00001640
Iteration 167/1000 | Loss: 0.00001640
Iteration 168/1000 | Loss: 0.00001640
Iteration 169/1000 | Loss: 0.00001640
Iteration 170/1000 | Loss: 0.00001640
Iteration 171/1000 | Loss: 0.00001640
Iteration 172/1000 | Loss: 0.00001640
Iteration 173/1000 | Loss: 0.00001640
Iteration 174/1000 | Loss: 0.00001639
Iteration 175/1000 | Loss: 0.00001639
Iteration 176/1000 | Loss: 0.00001639
Iteration 177/1000 | Loss: 0.00001639
Iteration 178/1000 | Loss: 0.00001639
Iteration 179/1000 | Loss: 0.00001639
Iteration 180/1000 | Loss: 0.00001639
Iteration 181/1000 | Loss: 0.00001639
Iteration 182/1000 | Loss: 0.00001639
Iteration 183/1000 | Loss: 0.00001639
Iteration 184/1000 | Loss: 0.00001639
Iteration 185/1000 | Loss: 0.00001639
Iteration 186/1000 | Loss: 0.00001639
Iteration 187/1000 | Loss: 0.00001639
Iteration 188/1000 | Loss: 0.00001639
Iteration 189/1000 | Loss: 0.00001639
Iteration 190/1000 | Loss: 0.00001639
Iteration 191/1000 | Loss: 0.00001639
Iteration 192/1000 | Loss: 0.00001639
Iteration 193/1000 | Loss: 0.00001639
Iteration 194/1000 | Loss: 0.00001639
Iteration 195/1000 | Loss: 0.00001639
Iteration 196/1000 | Loss: 0.00001639
Iteration 197/1000 | Loss: 0.00001639
Iteration 198/1000 | Loss: 0.00001639
Iteration 199/1000 | Loss: 0.00001639
Iteration 200/1000 | Loss: 0.00001639
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 200. Stopping optimization.
Last 5 losses: [1.639141555642709e-05, 1.639141555642709e-05, 1.639141555642709e-05, 1.639141555642709e-05, 1.639141555642709e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.639141555642709e-05

Optimization complete. Final v2v error: 3.3783938884735107 mm

Highest mean error: 4.056491851806641 mm for frame 36

Lowest mean error: 2.8226091861724854 mm for frame 112

Saving results

Total time: 44.49471616744995
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ben_posed_008/1019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_008/1019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_008/1019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00796165
Iteration 2/25 | Loss: 0.00127778
Iteration 3/25 | Loss: 0.00095639
Iteration 4/25 | Loss: 0.00081214
Iteration 5/25 | Loss: 0.00077661
Iteration 6/25 | Loss: 0.00076462
Iteration 7/25 | Loss: 0.00075612
Iteration 8/25 | Loss: 0.00075515
Iteration 9/25 | Loss: 0.00075368
Iteration 10/25 | Loss: 0.00075328
Iteration 11/25 | Loss: 0.00075306
Iteration 12/25 | Loss: 0.00075373
Iteration 13/25 | Loss: 0.00075176
Iteration 14/25 | Loss: 0.00075022
Iteration 15/25 | Loss: 0.00075129
Iteration 16/25 | Loss: 0.00075106
Iteration 17/25 | Loss: 0.00074984
Iteration 18/25 | Loss: 0.00075117
Iteration 19/25 | Loss: 0.00075118
Iteration 20/25 | Loss: 0.00074991
Iteration 21/25 | Loss: 0.00074979
Iteration 22/25 | Loss: 0.00074979
Iteration 23/25 | Loss: 0.00074978
Iteration 24/25 | Loss: 0.00074978
Iteration 25/25 | Loss: 0.00074978

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.16357517
Iteration 2/25 | Loss: 0.00029099
Iteration 3/25 | Loss: 0.00029098
Iteration 4/25 | Loss: 0.00029098
Iteration 5/25 | Loss: 0.00029098
Iteration 6/25 | Loss: 0.00029098
Iteration 7/25 | Loss: 0.00029098
Iteration 8/25 | Loss: 0.00029098
Iteration 9/25 | Loss: 0.00029098
Iteration 10/25 | Loss: 0.00029098
Iteration 11/25 | Loss: 0.00029098
Iteration 12/25 | Loss: 0.00029098
Iteration 13/25 | Loss: 0.00029098
Iteration 14/25 | Loss: 0.00029098
Iteration 15/25 | Loss: 0.00029098
Iteration 16/25 | Loss: 0.00029098
Iteration 17/25 | Loss: 0.00029098
Iteration 18/25 | Loss: 0.00029098
Iteration 19/25 | Loss: 0.00029098
Iteration 20/25 | Loss: 0.00029098
Iteration 21/25 | Loss: 0.00029098
Iteration 22/25 | Loss: 0.00029098
Iteration 23/25 | Loss: 0.00029098
Iteration 24/25 | Loss: 0.00029098
Iteration 25/25 | Loss: 0.00029098

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00029098
Iteration 2/1000 | Loss: 0.00003730
Iteration 3/1000 | Loss: 0.00003016
Iteration 4/1000 | Loss: 0.00002864
Iteration 5/1000 | Loss: 0.00002774
Iteration 6/1000 | Loss: 0.00002709
Iteration 7/1000 | Loss: 0.00002663
Iteration 8/1000 | Loss: 0.00002619
Iteration 9/1000 | Loss: 0.00002589
Iteration 10/1000 | Loss: 0.00002568
Iteration 11/1000 | Loss: 0.00002555
Iteration 12/1000 | Loss: 0.00002547
Iteration 13/1000 | Loss: 0.00002535
Iteration 14/1000 | Loss: 0.00002530
Iteration 15/1000 | Loss: 0.00002529
Iteration 16/1000 | Loss: 0.00002528
Iteration 17/1000 | Loss: 0.00002527
Iteration 18/1000 | Loss: 0.00002527
Iteration 19/1000 | Loss: 0.00002521
Iteration 20/1000 | Loss: 0.00002518
Iteration 21/1000 | Loss: 0.00002518
Iteration 22/1000 | Loss: 0.00002517
Iteration 23/1000 | Loss: 0.00002517
Iteration 24/1000 | Loss: 0.00002516
Iteration 25/1000 | Loss: 0.00002516
Iteration 26/1000 | Loss: 0.00002515
Iteration 27/1000 | Loss: 0.00002515
Iteration 28/1000 | Loss: 0.00002514
Iteration 29/1000 | Loss: 0.00002514
Iteration 30/1000 | Loss: 0.00002513
Iteration 31/1000 | Loss: 0.00002513
Iteration 32/1000 | Loss: 0.00002512
Iteration 33/1000 | Loss: 0.00002512
Iteration 34/1000 | Loss: 0.00002512
Iteration 35/1000 | Loss: 0.00002512
Iteration 36/1000 | Loss: 0.00002512
Iteration 37/1000 | Loss: 0.00002512
Iteration 38/1000 | Loss: 0.00002512
Iteration 39/1000 | Loss: 0.00002512
Iteration 40/1000 | Loss: 0.00002512
Iteration 41/1000 | Loss: 0.00002512
Iteration 42/1000 | Loss: 0.00002512
Iteration 43/1000 | Loss: 0.00002512
Iteration 44/1000 | Loss: 0.00002512
Iteration 45/1000 | Loss: 0.00002512
Iteration 46/1000 | Loss: 0.00002512
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 46. Stopping optimization.
Last 5 losses: [2.5116842152783647e-05, 2.5116842152783647e-05, 2.5116842152783647e-05, 2.5116842152783647e-05, 2.5116842152783647e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.5116842152783647e-05

Optimization complete. Final v2v error: 4.198094844818115 mm

Highest mean error: 10.395406723022461 mm for frame 47

Lowest mean error: 3.2136409282684326 mm for frame 193

Saving results

Total time: 64.16214966773987
