Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=208, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 11648-11703
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_025/1067/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_025/1067.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_025/1067
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00925088
Iteration 2/25 | Loss: 0.00216746
Iteration 3/25 | Loss: 0.00150368
Iteration 4/25 | Loss: 0.00141581
Iteration 5/25 | Loss: 0.00136892
Iteration 6/25 | Loss: 0.00134056
Iteration 7/25 | Loss: 0.00134315
Iteration 8/25 | Loss: 0.00133362
Iteration 9/25 | Loss: 0.00132318
Iteration 10/25 | Loss: 0.00132132
Iteration 11/25 | Loss: 0.00132045
Iteration 12/25 | Loss: 0.00132013
Iteration 13/25 | Loss: 0.00132000
Iteration 14/25 | Loss: 0.00131990
Iteration 15/25 | Loss: 0.00131951
Iteration 16/25 | Loss: 0.00131823
Iteration 17/25 | Loss: 0.00132171
Iteration 18/25 | Loss: 0.00131440
Iteration 19/25 | Loss: 0.00131337
Iteration 20/25 | Loss: 0.00131318
Iteration 21/25 | Loss: 0.00131308
Iteration 22/25 | Loss: 0.00131307
Iteration 23/25 | Loss: 0.00131307
Iteration 24/25 | Loss: 0.00131307
Iteration 25/25 | Loss: 0.00131307

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.87778068
Iteration 2/25 | Loss: 0.00072755
Iteration 3/25 | Loss: 0.00072751
Iteration 4/25 | Loss: 0.00072751
Iteration 5/25 | Loss: 0.00072751
Iteration 6/25 | Loss: 0.00072751
Iteration 7/25 | Loss: 0.00072751
Iteration 8/25 | Loss: 0.00072751
Iteration 9/25 | Loss: 0.00072751
Iteration 10/25 | Loss: 0.00072751
Iteration 11/25 | Loss: 0.00072751
Iteration 12/25 | Loss: 0.00072751
Iteration 13/25 | Loss: 0.00072751
Iteration 14/25 | Loss: 0.00072751
Iteration 15/25 | Loss: 0.00072751
Iteration 16/25 | Loss: 0.00072751
Iteration 17/25 | Loss: 0.00072751
Iteration 18/25 | Loss: 0.00072751
Iteration 19/25 | Loss: 0.00072751
Iteration 20/25 | Loss: 0.00072751
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0007275087991729379, 0.0007275087991729379, 0.0007275087991729379, 0.0007275087991729379, 0.0007275087991729379]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007275087991729379

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00072751
Iteration 2/1000 | Loss: 0.00003545
Iteration 3/1000 | Loss: 0.00002633
Iteration 4/1000 | Loss: 0.00002422
Iteration 5/1000 | Loss: 0.00002336
Iteration 6/1000 | Loss: 0.00002282
Iteration 7/1000 | Loss: 0.00014665
Iteration 8/1000 | Loss: 0.00002230
Iteration 9/1000 | Loss: 0.00002200
Iteration 10/1000 | Loss: 0.00002167
Iteration 11/1000 | Loss: 0.00002166
Iteration 12/1000 | Loss: 0.00002154
Iteration 13/1000 | Loss: 0.00002150
Iteration 14/1000 | Loss: 0.00002149
Iteration 15/1000 | Loss: 0.00002149
Iteration 16/1000 | Loss: 0.00002147
Iteration 17/1000 | Loss: 0.00002146
Iteration 18/1000 | Loss: 0.00002140
Iteration 19/1000 | Loss: 0.00002131
Iteration 20/1000 | Loss: 0.00002131
Iteration 21/1000 | Loss: 0.00002128
Iteration 22/1000 | Loss: 0.00002122
Iteration 23/1000 | Loss: 0.00002119
Iteration 24/1000 | Loss: 0.00002118
Iteration 25/1000 | Loss: 0.00002118
Iteration 26/1000 | Loss: 0.00002118
Iteration 27/1000 | Loss: 0.00002117
Iteration 28/1000 | Loss: 0.00002116
Iteration 29/1000 | Loss: 0.00002114
Iteration 30/1000 | Loss: 0.00002114
Iteration 31/1000 | Loss: 0.00002114
Iteration 32/1000 | Loss: 0.00002114
Iteration 33/1000 | Loss: 0.00002114
Iteration 34/1000 | Loss: 0.00002114
Iteration 35/1000 | Loss: 0.00002113
Iteration 36/1000 | Loss: 0.00002113
Iteration 37/1000 | Loss: 0.00002113
Iteration 38/1000 | Loss: 0.00002112
Iteration 39/1000 | Loss: 0.00002112
Iteration 40/1000 | Loss: 0.00002110
Iteration 41/1000 | Loss: 0.00002110
Iteration 42/1000 | Loss: 0.00002110
Iteration 43/1000 | Loss: 0.00002110
Iteration 44/1000 | Loss: 0.00002110
Iteration 45/1000 | Loss: 0.00002110
Iteration 46/1000 | Loss: 0.00002110
Iteration 47/1000 | Loss: 0.00002110
Iteration 48/1000 | Loss: 0.00002110
Iteration 49/1000 | Loss: 0.00002109
Iteration 50/1000 | Loss: 0.00002108
Iteration 51/1000 | Loss: 0.00002108
Iteration 52/1000 | Loss: 0.00002108
Iteration 53/1000 | Loss: 0.00002107
Iteration 54/1000 | Loss: 0.00002106
Iteration 55/1000 | Loss: 0.00002106
Iteration 56/1000 | Loss: 0.00002106
Iteration 57/1000 | Loss: 0.00002106
Iteration 58/1000 | Loss: 0.00002106
Iteration 59/1000 | Loss: 0.00002106
Iteration 60/1000 | Loss: 0.00002105
Iteration 61/1000 | Loss: 0.00002105
Iteration 62/1000 | Loss: 0.00002105
Iteration 63/1000 | Loss: 0.00002105
Iteration 64/1000 | Loss: 0.00002105
Iteration 65/1000 | Loss: 0.00002105
Iteration 66/1000 | Loss: 0.00002105
Iteration 67/1000 | Loss: 0.00002105
Iteration 68/1000 | Loss: 0.00002105
Iteration 69/1000 | Loss: 0.00002104
Iteration 70/1000 | Loss: 0.00002104
Iteration 71/1000 | Loss: 0.00002104
Iteration 72/1000 | Loss: 0.00002104
Iteration 73/1000 | Loss: 0.00002104
Iteration 74/1000 | Loss: 0.00002103
Iteration 75/1000 | Loss: 0.00002103
Iteration 76/1000 | Loss: 0.00002103
Iteration 77/1000 | Loss: 0.00002103
Iteration 78/1000 | Loss: 0.00002102
Iteration 79/1000 | Loss: 0.00002102
Iteration 80/1000 | Loss: 0.00002102
Iteration 81/1000 | Loss: 0.00002102
Iteration 82/1000 | Loss: 0.00002101
Iteration 83/1000 | Loss: 0.00002101
Iteration 84/1000 | Loss: 0.00002101
Iteration 85/1000 | Loss: 0.00002100
Iteration 86/1000 | Loss: 0.00002100
Iteration 87/1000 | Loss: 0.00002100
Iteration 88/1000 | Loss: 0.00002099
Iteration 89/1000 | Loss: 0.00002099
Iteration 90/1000 | Loss: 0.00002099
Iteration 91/1000 | Loss: 0.00002099
Iteration 92/1000 | Loss: 0.00002099
Iteration 93/1000 | Loss: 0.00002099
Iteration 94/1000 | Loss: 0.00002099
Iteration 95/1000 | Loss: 0.00002099
Iteration 96/1000 | Loss: 0.00002099
Iteration 97/1000 | Loss: 0.00002099
Iteration 98/1000 | Loss: 0.00002099
Iteration 99/1000 | Loss: 0.00002099
Iteration 100/1000 | Loss: 0.00002098
Iteration 101/1000 | Loss: 0.00002098
Iteration 102/1000 | Loss: 0.00002098
Iteration 103/1000 | Loss: 0.00002098
Iteration 104/1000 | Loss: 0.00002098
Iteration 105/1000 | Loss: 0.00002098
Iteration 106/1000 | Loss: 0.00002098
Iteration 107/1000 | Loss: 0.00002098
Iteration 108/1000 | Loss: 0.00002098
Iteration 109/1000 | Loss: 0.00002097
Iteration 110/1000 | Loss: 0.00002097
Iteration 111/1000 | Loss: 0.00002097
Iteration 112/1000 | Loss: 0.00002097
Iteration 113/1000 | Loss: 0.00002097
Iteration 114/1000 | Loss: 0.00002097
Iteration 115/1000 | Loss: 0.00002097
Iteration 116/1000 | Loss: 0.00002097
Iteration 117/1000 | Loss: 0.00002097
Iteration 118/1000 | Loss: 0.00002097
Iteration 119/1000 | Loss: 0.00002096
Iteration 120/1000 | Loss: 0.00002096
Iteration 121/1000 | Loss: 0.00002096
Iteration 122/1000 | Loss: 0.00002096
Iteration 123/1000 | Loss: 0.00002096
Iteration 124/1000 | Loss: 0.00002095
Iteration 125/1000 | Loss: 0.00002095
Iteration 126/1000 | Loss: 0.00002095
Iteration 127/1000 | Loss: 0.00002095
Iteration 128/1000 | Loss: 0.00002095
Iteration 129/1000 | Loss: 0.00002095
Iteration 130/1000 | Loss: 0.00002094
Iteration 131/1000 | Loss: 0.00002094
Iteration 132/1000 | Loss: 0.00002094
Iteration 133/1000 | Loss: 0.00002094
Iteration 134/1000 | Loss: 0.00002094
Iteration 135/1000 | Loss: 0.00002094
Iteration 136/1000 | Loss: 0.00002094
Iteration 137/1000 | Loss: 0.00002094
Iteration 138/1000 | Loss: 0.00002094
Iteration 139/1000 | Loss: 0.00002094
Iteration 140/1000 | Loss: 0.00002093
Iteration 141/1000 | Loss: 0.00002093
Iteration 142/1000 | Loss: 0.00002093
Iteration 143/1000 | Loss: 0.00002093
Iteration 144/1000 | Loss: 0.00002093
Iteration 145/1000 | Loss: 0.00002093
Iteration 146/1000 | Loss: 0.00002093
Iteration 147/1000 | Loss: 0.00002093
Iteration 148/1000 | Loss: 0.00002093
Iteration 149/1000 | Loss: 0.00002093
Iteration 150/1000 | Loss: 0.00002093
Iteration 151/1000 | Loss: 0.00002093
Iteration 152/1000 | Loss: 0.00002093
Iteration 153/1000 | Loss: 0.00002093
Iteration 154/1000 | Loss: 0.00002093
Iteration 155/1000 | Loss: 0.00002093
Iteration 156/1000 | Loss: 0.00002093
Iteration 157/1000 | Loss: 0.00002093
Iteration 158/1000 | Loss: 0.00002093
Iteration 159/1000 | Loss: 0.00002093
Iteration 160/1000 | Loss: 0.00002093
Iteration 161/1000 | Loss: 0.00002093
Iteration 162/1000 | Loss: 0.00002093
Iteration 163/1000 | Loss: 0.00002093
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 163. Stopping optimization.
Last 5 losses: [2.0928999219904654e-05, 2.0928999219904654e-05, 2.0928999219904654e-05, 2.0928999219904654e-05, 2.0928999219904654e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0928999219904654e-05

Optimization complete. Final v2v error: 3.6961798667907715 mm

Highest mean error: 4.890702247619629 mm for frame 16

Lowest mean error: 3.415613889694214 mm for frame 151

Saving results

Total time: 75.879154920578
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_025/1057/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_025/1057.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_025/1057
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00768122
Iteration 2/25 | Loss: 0.00145254
Iteration 3/25 | Loss: 0.00127136
Iteration 4/25 | Loss: 0.00123029
Iteration 5/25 | Loss: 0.00120915
Iteration 6/25 | Loss: 0.00119638
Iteration 7/25 | Loss: 0.00118310
Iteration 8/25 | Loss: 0.00118109
Iteration 9/25 | Loss: 0.00118032
Iteration 10/25 | Loss: 0.00117905
Iteration 11/25 | Loss: 0.00118157
Iteration 12/25 | Loss: 0.00118081
Iteration 13/25 | Loss: 0.00117979
Iteration 14/25 | Loss: 0.00118359
Iteration 15/25 | Loss: 0.00118061
Iteration 16/25 | Loss: 0.00117803
Iteration 17/25 | Loss: 0.00118112
Iteration 18/25 | Loss: 0.00117819
Iteration 19/25 | Loss: 0.00117247
Iteration 20/25 | Loss: 0.00117278
Iteration 21/25 | Loss: 0.00116683
Iteration 22/25 | Loss: 0.00116455
Iteration 23/25 | Loss: 0.00116362
Iteration 24/25 | Loss: 0.00116330
Iteration 25/25 | Loss: 0.00116314

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.27658534
Iteration 2/25 | Loss: 0.00116440
Iteration 3/25 | Loss: 0.00115491
Iteration 4/25 | Loss: 0.00115491
Iteration 5/25 | Loss: 0.00115491
Iteration 6/25 | Loss: 0.00115491
Iteration 7/25 | Loss: 0.00115491
Iteration 8/25 | Loss: 0.00115491
Iteration 9/25 | Loss: 0.00115491
Iteration 10/25 | Loss: 0.00115491
Iteration 11/25 | Loss: 0.00115491
Iteration 12/25 | Loss: 0.00115491
Iteration 13/25 | Loss: 0.00115491
Iteration 14/25 | Loss: 0.00115491
Iteration 15/25 | Loss: 0.00115491
Iteration 16/25 | Loss: 0.00115491
Iteration 17/25 | Loss: 0.00115491
Iteration 18/25 | Loss: 0.00115491
Iteration 19/25 | Loss: 0.00115491
Iteration 20/25 | Loss: 0.00115491
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0011549060000106692, 0.0011549060000106692, 0.0011549060000106692, 0.0011549060000106692, 0.0011549060000106692]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011549060000106692

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00115491
Iteration 2/1000 | Loss: 0.00010418
Iteration 3/1000 | Loss: 0.00012997
Iteration 4/1000 | Loss: 0.00031360
Iteration 5/1000 | Loss: 0.00020760
Iteration 6/1000 | Loss: 0.00019322
Iteration 7/1000 | Loss: 0.00004364
Iteration 8/1000 | Loss: 0.00031319
Iteration 9/1000 | Loss: 0.00041575
Iteration 10/1000 | Loss: 0.00018526
Iteration 11/1000 | Loss: 0.00053963
Iteration 12/1000 | Loss: 0.00008971
Iteration 13/1000 | Loss: 0.00003480
Iteration 14/1000 | Loss: 0.00002999
Iteration 15/1000 | Loss: 0.00002764
Iteration 16/1000 | Loss: 0.00002533
Iteration 17/1000 | Loss: 0.00002361
Iteration 18/1000 | Loss: 0.00002241
Iteration 19/1000 | Loss: 0.00002153
Iteration 20/1000 | Loss: 0.00002111
Iteration 21/1000 | Loss: 0.00002062
Iteration 22/1000 | Loss: 0.00002034
Iteration 23/1000 | Loss: 0.00002009
Iteration 24/1000 | Loss: 0.00001995
Iteration 25/1000 | Loss: 0.00001989
Iteration 26/1000 | Loss: 0.00001988
Iteration 27/1000 | Loss: 0.00001977
Iteration 28/1000 | Loss: 0.00001965
Iteration 29/1000 | Loss: 0.00001962
Iteration 30/1000 | Loss: 0.00001960
Iteration 31/1000 | Loss: 0.00001960
Iteration 32/1000 | Loss: 0.00001960
Iteration 33/1000 | Loss: 0.00001960
Iteration 34/1000 | Loss: 0.00001960
Iteration 35/1000 | Loss: 0.00001960
Iteration 36/1000 | Loss: 0.00001960
Iteration 37/1000 | Loss: 0.00001959
Iteration 38/1000 | Loss: 0.00001959
Iteration 39/1000 | Loss: 0.00001959
Iteration 40/1000 | Loss: 0.00001955
Iteration 41/1000 | Loss: 0.00001955
Iteration 42/1000 | Loss: 0.00001954
Iteration 43/1000 | Loss: 0.00001954
Iteration 44/1000 | Loss: 0.00001953
Iteration 45/1000 | Loss: 0.00001953
Iteration 46/1000 | Loss: 0.00001953
Iteration 47/1000 | Loss: 0.00001952
Iteration 48/1000 | Loss: 0.00001951
Iteration 49/1000 | Loss: 0.00001950
Iteration 50/1000 | Loss: 0.00001949
Iteration 51/1000 | Loss: 0.00001949
Iteration 52/1000 | Loss: 0.00001948
Iteration 53/1000 | Loss: 0.00001948
Iteration 54/1000 | Loss: 0.00001948
Iteration 55/1000 | Loss: 0.00001948
Iteration 56/1000 | Loss: 0.00001947
Iteration 57/1000 | Loss: 0.00001946
Iteration 58/1000 | Loss: 0.00001945
Iteration 59/1000 | Loss: 0.00001945
Iteration 60/1000 | Loss: 0.00001944
Iteration 61/1000 | Loss: 0.00001944
Iteration 62/1000 | Loss: 0.00001943
Iteration 63/1000 | Loss: 0.00001943
Iteration 64/1000 | Loss: 0.00001943
Iteration 65/1000 | Loss: 0.00001943
Iteration 66/1000 | Loss: 0.00001942
Iteration 67/1000 | Loss: 0.00001942
Iteration 68/1000 | Loss: 0.00001942
Iteration 69/1000 | Loss: 0.00001942
Iteration 70/1000 | Loss: 0.00001942
Iteration 71/1000 | Loss: 0.00001941
Iteration 72/1000 | Loss: 0.00001941
Iteration 73/1000 | Loss: 0.00001941
Iteration 74/1000 | Loss: 0.00001941
Iteration 75/1000 | Loss: 0.00001941
Iteration 76/1000 | Loss: 0.00001940
Iteration 77/1000 | Loss: 0.00001940
Iteration 78/1000 | Loss: 0.00001940
Iteration 79/1000 | Loss: 0.00001939
Iteration 80/1000 | Loss: 0.00001939
Iteration 81/1000 | Loss: 0.00001939
Iteration 82/1000 | Loss: 0.00001939
Iteration 83/1000 | Loss: 0.00001939
Iteration 84/1000 | Loss: 0.00001938
Iteration 85/1000 | Loss: 0.00001938
Iteration 86/1000 | Loss: 0.00001937
Iteration 87/1000 | Loss: 0.00001937
Iteration 88/1000 | Loss: 0.00001937
Iteration 89/1000 | Loss: 0.00001937
Iteration 90/1000 | Loss: 0.00001937
Iteration 91/1000 | Loss: 0.00001936
Iteration 92/1000 | Loss: 0.00001936
Iteration 93/1000 | Loss: 0.00001936
Iteration 94/1000 | Loss: 0.00001936
Iteration 95/1000 | Loss: 0.00001936
Iteration 96/1000 | Loss: 0.00001936
Iteration 97/1000 | Loss: 0.00001936
Iteration 98/1000 | Loss: 0.00001936
Iteration 99/1000 | Loss: 0.00001936
Iteration 100/1000 | Loss: 0.00001935
Iteration 101/1000 | Loss: 0.00001935
Iteration 102/1000 | Loss: 0.00001935
Iteration 103/1000 | Loss: 0.00001935
Iteration 104/1000 | Loss: 0.00001935
Iteration 105/1000 | Loss: 0.00001935
Iteration 106/1000 | Loss: 0.00001935
Iteration 107/1000 | Loss: 0.00001935
Iteration 108/1000 | Loss: 0.00001935
Iteration 109/1000 | Loss: 0.00001935
Iteration 110/1000 | Loss: 0.00001935
Iteration 111/1000 | Loss: 0.00001935
Iteration 112/1000 | Loss: 0.00001935
Iteration 113/1000 | Loss: 0.00001935
Iteration 114/1000 | Loss: 0.00001935
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 114. Stopping optimization.
Last 5 losses: [1.9347036868566647e-05, 1.9347036868566647e-05, 1.9347036868566647e-05, 1.9347036868566647e-05, 1.9347036868566647e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9347036868566647e-05

Optimization complete. Final v2v error: 3.7023181915283203 mm

Highest mean error: 4.3814496994018555 mm for frame 182

Lowest mean error: 3.2980167865753174 mm for frame 19

Saving results

Total time: 102.62933874130249
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_025/1025/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_025/1025.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_025/1025
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01008393
Iteration 2/25 | Loss: 0.01008393
Iteration 3/25 | Loss: 0.01008392
Iteration 4/25 | Loss: 0.00248232
Iteration 5/25 | Loss: 0.00162237
Iteration 6/25 | Loss: 0.00137717
Iteration 7/25 | Loss: 0.00132004
Iteration 8/25 | Loss: 0.00129545
Iteration 9/25 | Loss: 0.00127817
Iteration 10/25 | Loss: 0.00130128
Iteration 11/25 | Loss: 0.00126618
Iteration 12/25 | Loss: 0.00124239
Iteration 13/25 | Loss: 0.00121629
Iteration 14/25 | Loss: 0.00120870
Iteration 15/25 | Loss: 0.00120125
Iteration 16/25 | Loss: 0.00119700
Iteration 17/25 | Loss: 0.00119476
Iteration 18/25 | Loss: 0.00119376
Iteration 19/25 | Loss: 0.00119490
Iteration 20/25 | Loss: 0.00119421
Iteration 21/25 | Loss: 0.00119297
Iteration 22/25 | Loss: 0.00119412
Iteration 23/25 | Loss: 0.00119320
Iteration 24/25 | Loss: 0.00119195
Iteration 25/25 | Loss: 0.00119140

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36892056
Iteration 2/25 | Loss: 0.00147436
Iteration 3/25 | Loss: 0.00098432
Iteration 4/25 | Loss: 0.00098432
Iteration 5/25 | Loss: 0.00098432
Iteration 6/25 | Loss: 0.00098432
Iteration 7/25 | Loss: 0.00098431
Iteration 8/25 | Loss: 0.00098431
Iteration 9/25 | Loss: 0.00098431
Iteration 10/25 | Loss: 0.00098431
Iteration 11/25 | Loss: 0.00098431
Iteration 12/25 | Loss: 0.00098431
Iteration 13/25 | Loss: 0.00098431
Iteration 14/25 | Loss: 0.00098431
Iteration 15/25 | Loss: 0.00098431
Iteration 16/25 | Loss: 0.00098431
Iteration 17/25 | Loss: 0.00098431
Iteration 18/25 | Loss: 0.00098431
Iteration 19/25 | Loss: 0.00098431
Iteration 20/25 | Loss: 0.00098431
Iteration 21/25 | Loss: 0.00098431
Iteration 22/25 | Loss: 0.00098431
Iteration 23/25 | Loss: 0.00098431
Iteration 24/25 | Loss: 0.00098431
Iteration 25/25 | Loss: 0.00098431

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00098431
Iteration 2/1000 | Loss: 0.00007628
Iteration 3/1000 | Loss: 0.00045693
Iteration 4/1000 | Loss: 0.00002666
Iteration 5/1000 | Loss: 0.00021239
Iteration 6/1000 | Loss: 0.00002809
Iteration 7/1000 | Loss: 0.00004474
Iteration 8/1000 | Loss: 0.00008521
Iteration 9/1000 | Loss: 0.00002302
Iteration 10/1000 | Loss: 0.00006256
Iteration 11/1000 | Loss: 0.00026999
Iteration 12/1000 | Loss: 0.00018018
Iteration 13/1000 | Loss: 0.00002064
Iteration 14/1000 | Loss: 0.00001992
Iteration 15/1000 | Loss: 0.00003255
Iteration 16/1000 | Loss: 0.00005679
Iteration 17/1000 | Loss: 0.00002208
Iteration 18/1000 | Loss: 0.00002460
Iteration 19/1000 | Loss: 0.00003629
Iteration 20/1000 | Loss: 0.00002070
Iteration 21/1000 | Loss: 0.00001903
Iteration 22/1000 | Loss: 0.00001889
Iteration 23/1000 | Loss: 0.00001870
Iteration 24/1000 | Loss: 0.00001909
Iteration 25/1000 | Loss: 0.00001851
Iteration 26/1000 | Loss: 0.00006753
Iteration 27/1000 | Loss: 0.00008218
Iteration 28/1000 | Loss: 0.00008176
Iteration 29/1000 | Loss: 0.00001868
Iteration 30/1000 | Loss: 0.00003090
Iteration 31/1000 | Loss: 0.00001977
Iteration 32/1000 | Loss: 0.00001845
Iteration 33/1000 | Loss: 0.00001842
Iteration 34/1000 | Loss: 0.00001828
Iteration 35/1000 | Loss: 0.00001827
Iteration 36/1000 | Loss: 0.00001827
Iteration 37/1000 | Loss: 0.00001827
Iteration 38/1000 | Loss: 0.00001826
Iteration 39/1000 | Loss: 0.00001826
Iteration 40/1000 | Loss: 0.00001825
Iteration 41/1000 | Loss: 0.00001825
Iteration 42/1000 | Loss: 0.00001825
Iteration 43/1000 | Loss: 0.00001825
Iteration 44/1000 | Loss: 0.00001825
Iteration 45/1000 | Loss: 0.00001825
Iteration 46/1000 | Loss: 0.00001825
Iteration 47/1000 | Loss: 0.00001825
Iteration 48/1000 | Loss: 0.00001824
Iteration 49/1000 | Loss: 0.00001824
Iteration 50/1000 | Loss: 0.00001824
Iteration 51/1000 | Loss: 0.00001823
Iteration 52/1000 | Loss: 0.00001823
Iteration 53/1000 | Loss: 0.00001823
Iteration 54/1000 | Loss: 0.00001823
Iteration 55/1000 | Loss: 0.00001823
Iteration 56/1000 | Loss: 0.00001822
Iteration 57/1000 | Loss: 0.00001822
Iteration 58/1000 | Loss: 0.00001822
Iteration 59/1000 | Loss: 0.00001822
Iteration 60/1000 | Loss: 0.00001821
Iteration 61/1000 | Loss: 0.00001821
Iteration 62/1000 | Loss: 0.00006783
Iteration 63/1000 | Loss: 0.00004717
Iteration 64/1000 | Loss: 0.00004319
Iteration 65/1000 | Loss: 0.00004847
Iteration 66/1000 | Loss: 0.00002037
Iteration 67/1000 | Loss: 0.00002125
Iteration 68/1000 | Loss: 0.00001819
Iteration 69/1000 | Loss: 0.00001824
Iteration 70/1000 | Loss: 0.00001818
Iteration 71/1000 | Loss: 0.00001818
Iteration 72/1000 | Loss: 0.00001818
Iteration 73/1000 | Loss: 0.00001818
Iteration 74/1000 | Loss: 0.00001818
Iteration 75/1000 | Loss: 0.00001818
Iteration 76/1000 | Loss: 0.00001818
Iteration 77/1000 | Loss: 0.00001818
Iteration 78/1000 | Loss: 0.00001818
Iteration 79/1000 | Loss: 0.00001818
Iteration 80/1000 | Loss: 0.00001818
Iteration 81/1000 | Loss: 0.00001817
Iteration 82/1000 | Loss: 0.00001817
Iteration 83/1000 | Loss: 0.00001817
Iteration 84/1000 | Loss: 0.00001817
Iteration 85/1000 | Loss: 0.00001817
Iteration 86/1000 | Loss: 0.00001817
Iteration 87/1000 | Loss: 0.00001817
Iteration 88/1000 | Loss: 0.00001817
Iteration 89/1000 | Loss: 0.00001817
Iteration 90/1000 | Loss: 0.00001817
Iteration 91/1000 | Loss: 0.00001817
Iteration 92/1000 | Loss: 0.00001817
Iteration 93/1000 | Loss: 0.00001817
Iteration 94/1000 | Loss: 0.00001817
Iteration 95/1000 | Loss: 0.00001817
Iteration 96/1000 | Loss: 0.00001817
Iteration 97/1000 | Loss: 0.00001817
Iteration 98/1000 | Loss: 0.00001817
Iteration 99/1000 | Loss: 0.00001817
Iteration 100/1000 | Loss: 0.00001817
Iteration 101/1000 | Loss: 0.00001817
Iteration 102/1000 | Loss: 0.00001817
Iteration 103/1000 | Loss: 0.00001817
Iteration 104/1000 | Loss: 0.00001817
Iteration 105/1000 | Loss: 0.00001817
Iteration 106/1000 | Loss: 0.00001817
Iteration 107/1000 | Loss: 0.00001817
Iteration 108/1000 | Loss: 0.00001817
Iteration 109/1000 | Loss: 0.00001817
Iteration 110/1000 | Loss: 0.00001816
Iteration 111/1000 | Loss: 0.00001816
Iteration 112/1000 | Loss: 0.00001816
Iteration 113/1000 | Loss: 0.00001816
Iteration 114/1000 | Loss: 0.00001816
Iteration 115/1000 | Loss: 0.00001816
Iteration 116/1000 | Loss: 0.00001816
Iteration 117/1000 | Loss: 0.00001816
Iteration 118/1000 | Loss: 0.00001816
Iteration 119/1000 | Loss: 0.00001816
Iteration 120/1000 | Loss: 0.00001816
Iteration 121/1000 | Loss: 0.00001816
Iteration 122/1000 | Loss: 0.00001816
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 122. Stopping optimization.
Last 5 losses: [1.8155124053009786e-05, 1.8155124053009786e-05, 1.8155124053009786e-05, 1.8155124053009786e-05, 1.8155124053009786e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8155124053009786e-05

Optimization complete. Final v2v error: 3.6562933921813965 mm

Highest mean error: 5.3430328369140625 mm for frame 41

Lowest mean error: 3.331981658935547 mm for frame 111

Saving results

Total time: 113.94166254997253
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_025/1055/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_025/1055.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_025/1055
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00762373
Iteration 2/25 | Loss: 0.00130989
Iteration 3/25 | Loss: 0.00119423
Iteration 4/25 | Loss: 0.00118190
Iteration 5/25 | Loss: 0.00118071
Iteration 6/25 | Loss: 0.00118071
Iteration 7/25 | Loss: 0.00118071
Iteration 8/25 | Loss: 0.00118071
Iteration 9/25 | Loss: 0.00118071
Iteration 10/25 | Loss: 0.00118071
Iteration 11/25 | Loss: 0.00118071
Iteration 12/25 | Loss: 0.00118071
Iteration 13/25 | Loss: 0.00118071
Iteration 14/25 | Loss: 0.00118071
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.001180708990432322, 0.001180708990432322, 0.001180708990432322, 0.001180708990432322, 0.001180708990432322]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001180708990432322

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34546709
Iteration 2/25 | Loss: 0.00079084
Iteration 3/25 | Loss: 0.00079075
Iteration 4/25 | Loss: 0.00079075
Iteration 5/25 | Loss: 0.00079075
Iteration 6/25 | Loss: 0.00079075
Iteration 7/25 | Loss: 0.00079074
Iteration 8/25 | Loss: 0.00079074
Iteration 9/25 | Loss: 0.00079074
Iteration 10/25 | Loss: 0.00079074
Iteration 11/25 | Loss: 0.00079074
Iteration 12/25 | Loss: 0.00079074
Iteration 13/25 | Loss: 0.00079074
Iteration 14/25 | Loss: 0.00079074
Iteration 15/25 | Loss: 0.00079074
Iteration 16/25 | Loss: 0.00079074
Iteration 17/25 | Loss: 0.00079074
Iteration 18/25 | Loss: 0.00079074
Iteration 19/25 | Loss: 0.00079074
Iteration 20/25 | Loss: 0.00079074
Iteration 21/25 | Loss: 0.00079074
Iteration 22/25 | Loss: 0.00079074
Iteration 23/25 | Loss: 0.00079074
Iteration 24/25 | Loss: 0.00079074
Iteration 25/25 | Loss: 0.00079074

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00079074
Iteration 2/1000 | Loss: 0.00003330
Iteration 3/1000 | Loss: 0.00001874
Iteration 4/1000 | Loss: 0.00001637
Iteration 5/1000 | Loss: 0.00001526
Iteration 6/1000 | Loss: 0.00001451
Iteration 7/1000 | Loss: 0.00001406
Iteration 8/1000 | Loss: 0.00001373
Iteration 9/1000 | Loss: 0.00001339
Iteration 10/1000 | Loss: 0.00001310
Iteration 11/1000 | Loss: 0.00001305
Iteration 12/1000 | Loss: 0.00001290
Iteration 13/1000 | Loss: 0.00001272
Iteration 14/1000 | Loss: 0.00001265
Iteration 15/1000 | Loss: 0.00001264
Iteration 16/1000 | Loss: 0.00001262
Iteration 17/1000 | Loss: 0.00001259
Iteration 18/1000 | Loss: 0.00001259
Iteration 19/1000 | Loss: 0.00001254
Iteration 20/1000 | Loss: 0.00001253
Iteration 21/1000 | Loss: 0.00001245
Iteration 22/1000 | Loss: 0.00001240
Iteration 23/1000 | Loss: 0.00001239
Iteration 24/1000 | Loss: 0.00001237
Iteration 25/1000 | Loss: 0.00001232
Iteration 26/1000 | Loss: 0.00001229
Iteration 27/1000 | Loss: 0.00001223
Iteration 28/1000 | Loss: 0.00001221
Iteration 29/1000 | Loss: 0.00001220
Iteration 30/1000 | Loss: 0.00001220
Iteration 31/1000 | Loss: 0.00001219
Iteration 32/1000 | Loss: 0.00001218
Iteration 33/1000 | Loss: 0.00001217
Iteration 34/1000 | Loss: 0.00001217
Iteration 35/1000 | Loss: 0.00001217
Iteration 36/1000 | Loss: 0.00001217
Iteration 37/1000 | Loss: 0.00001217
Iteration 38/1000 | Loss: 0.00001217
Iteration 39/1000 | Loss: 0.00001216
Iteration 40/1000 | Loss: 0.00001216
Iteration 41/1000 | Loss: 0.00001216
Iteration 42/1000 | Loss: 0.00001216
Iteration 43/1000 | Loss: 0.00001216
Iteration 44/1000 | Loss: 0.00001216
Iteration 45/1000 | Loss: 0.00001215
Iteration 46/1000 | Loss: 0.00001215
Iteration 47/1000 | Loss: 0.00001215
Iteration 48/1000 | Loss: 0.00001215
Iteration 49/1000 | Loss: 0.00001215
Iteration 50/1000 | Loss: 0.00001215
Iteration 51/1000 | Loss: 0.00001214
Iteration 52/1000 | Loss: 0.00001214
Iteration 53/1000 | Loss: 0.00001214
Iteration 54/1000 | Loss: 0.00001214
Iteration 55/1000 | Loss: 0.00001214
Iteration 56/1000 | Loss: 0.00001214
Iteration 57/1000 | Loss: 0.00001214
Iteration 58/1000 | Loss: 0.00001214
Iteration 59/1000 | Loss: 0.00001214
Iteration 60/1000 | Loss: 0.00001214
Iteration 61/1000 | Loss: 0.00001214
Iteration 62/1000 | Loss: 0.00001214
Iteration 63/1000 | Loss: 0.00001213
Iteration 64/1000 | Loss: 0.00001213
Iteration 65/1000 | Loss: 0.00001213
Iteration 66/1000 | Loss: 0.00001213
Iteration 67/1000 | Loss: 0.00001212
Iteration 68/1000 | Loss: 0.00001212
Iteration 69/1000 | Loss: 0.00001212
Iteration 70/1000 | Loss: 0.00001211
Iteration 71/1000 | Loss: 0.00001211
Iteration 72/1000 | Loss: 0.00001211
Iteration 73/1000 | Loss: 0.00001210
Iteration 74/1000 | Loss: 0.00001210
Iteration 75/1000 | Loss: 0.00001210
Iteration 76/1000 | Loss: 0.00001210
Iteration 77/1000 | Loss: 0.00001209
Iteration 78/1000 | Loss: 0.00001209
Iteration 79/1000 | Loss: 0.00001209
Iteration 80/1000 | Loss: 0.00001208
Iteration 81/1000 | Loss: 0.00001208
Iteration 82/1000 | Loss: 0.00001208
Iteration 83/1000 | Loss: 0.00001208
Iteration 84/1000 | Loss: 0.00001207
Iteration 85/1000 | Loss: 0.00001207
Iteration 86/1000 | Loss: 0.00001207
Iteration 87/1000 | Loss: 0.00001207
Iteration 88/1000 | Loss: 0.00001206
Iteration 89/1000 | Loss: 0.00001206
Iteration 90/1000 | Loss: 0.00001205
Iteration 91/1000 | Loss: 0.00001205
Iteration 92/1000 | Loss: 0.00001205
Iteration 93/1000 | Loss: 0.00001205
Iteration 94/1000 | Loss: 0.00001205
Iteration 95/1000 | Loss: 0.00001205
Iteration 96/1000 | Loss: 0.00001205
Iteration 97/1000 | Loss: 0.00001205
Iteration 98/1000 | Loss: 0.00001205
Iteration 99/1000 | Loss: 0.00001205
Iteration 100/1000 | Loss: 0.00001205
Iteration 101/1000 | Loss: 0.00001204
Iteration 102/1000 | Loss: 0.00001204
Iteration 103/1000 | Loss: 0.00001204
Iteration 104/1000 | Loss: 0.00001204
Iteration 105/1000 | Loss: 0.00001204
Iteration 106/1000 | Loss: 0.00001204
Iteration 107/1000 | Loss: 0.00001204
Iteration 108/1000 | Loss: 0.00001204
Iteration 109/1000 | Loss: 0.00001203
Iteration 110/1000 | Loss: 0.00001203
Iteration 111/1000 | Loss: 0.00001203
Iteration 112/1000 | Loss: 0.00001203
Iteration 113/1000 | Loss: 0.00001203
Iteration 114/1000 | Loss: 0.00001203
Iteration 115/1000 | Loss: 0.00001203
Iteration 116/1000 | Loss: 0.00001203
Iteration 117/1000 | Loss: 0.00001202
Iteration 118/1000 | Loss: 0.00001202
Iteration 119/1000 | Loss: 0.00001202
Iteration 120/1000 | Loss: 0.00001202
Iteration 121/1000 | Loss: 0.00001202
Iteration 122/1000 | Loss: 0.00001201
Iteration 123/1000 | Loss: 0.00001201
Iteration 124/1000 | Loss: 0.00001201
Iteration 125/1000 | Loss: 0.00001201
Iteration 126/1000 | Loss: 0.00001201
Iteration 127/1000 | Loss: 0.00001201
Iteration 128/1000 | Loss: 0.00001201
Iteration 129/1000 | Loss: 0.00001201
Iteration 130/1000 | Loss: 0.00001201
Iteration 131/1000 | Loss: 0.00001201
Iteration 132/1000 | Loss: 0.00001201
Iteration 133/1000 | Loss: 0.00001201
Iteration 134/1000 | Loss: 0.00001201
Iteration 135/1000 | Loss: 0.00001201
Iteration 136/1000 | Loss: 0.00001201
Iteration 137/1000 | Loss: 0.00001201
Iteration 138/1000 | Loss: 0.00001201
Iteration 139/1000 | Loss: 0.00001201
Iteration 140/1000 | Loss: 0.00001201
Iteration 141/1000 | Loss: 0.00001201
Iteration 142/1000 | Loss: 0.00001200
Iteration 143/1000 | Loss: 0.00001200
Iteration 144/1000 | Loss: 0.00001200
Iteration 145/1000 | Loss: 0.00001200
Iteration 146/1000 | Loss: 0.00001200
Iteration 147/1000 | Loss: 0.00001200
Iteration 148/1000 | Loss: 0.00001200
Iteration 149/1000 | Loss: 0.00001200
Iteration 150/1000 | Loss: 0.00001200
Iteration 151/1000 | Loss: 0.00001199
Iteration 152/1000 | Loss: 0.00001199
Iteration 153/1000 | Loss: 0.00001199
Iteration 154/1000 | Loss: 0.00001198
Iteration 155/1000 | Loss: 0.00001198
Iteration 156/1000 | Loss: 0.00001198
Iteration 157/1000 | Loss: 0.00001198
Iteration 158/1000 | Loss: 0.00001198
Iteration 159/1000 | Loss: 0.00001198
Iteration 160/1000 | Loss: 0.00001197
Iteration 161/1000 | Loss: 0.00001197
Iteration 162/1000 | Loss: 0.00001197
Iteration 163/1000 | Loss: 0.00001197
Iteration 164/1000 | Loss: 0.00001197
Iteration 165/1000 | Loss: 0.00001197
Iteration 166/1000 | Loss: 0.00001196
Iteration 167/1000 | Loss: 0.00001196
Iteration 168/1000 | Loss: 0.00001196
Iteration 169/1000 | Loss: 0.00001196
Iteration 170/1000 | Loss: 0.00001195
Iteration 171/1000 | Loss: 0.00001195
Iteration 172/1000 | Loss: 0.00001195
Iteration 173/1000 | Loss: 0.00001195
Iteration 174/1000 | Loss: 0.00001195
Iteration 175/1000 | Loss: 0.00001195
Iteration 176/1000 | Loss: 0.00001194
Iteration 177/1000 | Loss: 0.00001194
Iteration 178/1000 | Loss: 0.00001194
Iteration 179/1000 | Loss: 0.00001194
Iteration 180/1000 | Loss: 0.00001194
Iteration 181/1000 | Loss: 0.00001193
Iteration 182/1000 | Loss: 0.00001192
Iteration 183/1000 | Loss: 0.00001192
Iteration 184/1000 | Loss: 0.00001192
Iteration 185/1000 | Loss: 0.00001192
Iteration 186/1000 | Loss: 0.00001192
Iteration 187/1000 | Loss: 0.00001191
Iteration 188/1000 | Loss: 0.00001191
Iteration 189/1000 | Loss: 0.00001191
Iteration 190/1000 | Loss: 0.00001191
Iteration 191/1000 | Loss: 0.00001191
Iteration 192/1000 | Loss: 0.00001191
Iteration 193/1000 | Loss: 0.00001191
Iteration 194/1000 | Loss: 0.00001191
Iteration 195/1000 | Loss: 0.00001190
Iteration 196/1000 | Loss: 0.00001190
Iteration 197/1000 | Loss: 0.00001190
Iteration 198/1000 | Loss: 0.00001190
Iteration 199/1000 | Loss: 0.00001190
Iteration 200/1000 | Loss: 0.00001190
Iteration 201/1000 | Loss: 0.00001190
Iteration 202/1000 | Loss: 0.00001190
Iteration 203/1000 | Loss: 0.00001190
Iteration 204/1000 | Loss: 0.00001190
Iteration 205/1000 | Loss: 0.00001190
Iteration 206/1000 | Loss: 0.00001190
Iteration 207/1000 | Loss: 0.00001189
Iteration 208/1000 | Loss: 0.00001189
Iteration 209/1000 | Loss: 0.00001189
Iteration 210/1000 | Loss: 0.00001189
Iteration 211/1000 | Loss: 0.00001189
Iteration 212/1000 | Loss: 0.00001189
Iteration 213/1000 | Loss: 0.00001189
Iteration 214/1000 | Loss: 0.00001189
Iteration 215/1000 | Loss: 0.00001189
Iteration 216/1000 | Loss: 0.00001189
Iteration 217/1000 | Loss: 0.00001189
Iteration 218/1000 | Loss: 0.00001189
Iteration 219/1000 | Loss: 0.00001189
Iteration 220/1000 | Loss: 0.00001189
Iteration 221/1000 | Loss: 0.00001189
Iteration 222/1000 | Loss: 0.00001189
Iteration 223/1000 | Loss: 0.00001189
Iteration 224/1000 | Loss: 0.00001189
Iteration 225/1000 | Loss: 0.00001189
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 225. Stopping optimization.
Last 5 losses: [1.1894187082361896e-05, 1.1894187082361896e-05, 1.1894187082361896e-05, 1.1894187082361896e-05, 1.1894187082361896e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1894187082361896e-05

Optimization complete. Final v2v error: 2.8914732933044434 mm

Highest mean error: 3.28075909614563 mm for frame 17

Lowest mean error: 2.5975866317749023 mm for frame 185

Saving results

Total time: 50.48217487335205
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_025/1013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_025/1013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_025/1013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00822114
Iteration 2/25 | Loss: 0.00151786
Iteration 3/25 | Loss: 0.00128258
Iteration 4/25 | Loss: 0.00125854
Iteration 5/25 | Loss: 0.00125567
Iteration 6/25 | Loss: 0.00125567
Iteration 7/25 | Loss: 0.00125567
Iteration 8/25 | Loss: 0.00125567
Iteration 9/25 | Loss: 0.00125567
Iteration 10/25 | Loss: 0.00125567
Iteration 11/25 | Loss: 0.00125567
Iteration 12/25 | Loss: 0.00125567
Iteration 13/25 | Loss: 0.00125567
Iteration 14/25 | Loss: 0.00125567
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0012556657893583179, 0.0012556657893583179, 0.0012556657893583179, 0.0012556657893583179, 0.0012556657893583179]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012556657893583179

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.98986328
Iteration 2/25 | Loss: 0.00055719
Iteration 3/25 | Loss: 0.00055718
Iteration 4/25 | Loss: 0.00055718
Iteration 5/25 | Loss: 0.00055718
Iteration 6/25 | Loss: 0.00055718
Iteration 7/25 | Loss: 0.00055718
Iteration 8/25 | Loss: 0.00055718
Iteration 9/25 | Loss: 0.00055718
Iteration 10/25 | Loss: 0.00055718
Iteration 11/25 | Loss: 0.00055718
Iteration 12/25 | Loss: 0.00055718
Iteration 13/25 | Loss: 0.00055718
Iteration 14/25 | Loss: 0.00055718
Iteration 15/25 | Loss: 0.00055718
Iteration 16/25 | Loss: 0.00055718
Iteration 17/25 | Loss: 0.00055718
Iteration 18/25 | Loss: 0.00055718
Iteration 19/25 | Loss: 0.00055718
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0005571816000156105, 0.0005571816000156105, 0.0005571816000156105, 0.0005571816000156105, 0.0005571816000156105]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005571816000156105

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00055718
Iteration 2/1000 | Loss: 0.00004147
Iteration 3/1000 | Loss: 0.00003074
Iteration 4/1000 | Loss: 0.00002758
Iteration 5/1000 | Loss: 0.00002590
Iteration 6/1000 | Loss: 0.00002500
Iteration 7/1000 | Loss: 0.00002434
Iteration 8/1000 | Loss: 0.00002392
Iteration 9/1000 | Loss: 0.00002356
Iteration 10/1000 | Loss: 0.00002336
Iteration 11/1000 | Loss: 0.00002304
Iteration 12/1000 | Loss: 0.00002270
Iteration 13/1000 | Loss: 0.00002241
Iteration 14/1000 | Loss: 0.00002221
Iteration 15/1000 | Loss: 0.00002215
Iteration 16/1000 | Loss: 0.00002203
Iteration 17/1000 | Loss: 0.00002201
Iteration 18/1000 | Loss: 0.00002190
Iteration 19/1000 | Loss: 0.00002185
Iteration 20/1000 | Loss: 0.00002173
Iteration 21/1000 | Loss: 0.00002173
Iteration 22/1000 | Loss: 0.00002172
Iteration 23/1000 | Loss: 0.00002170
Iteration 24/1000 | Loss: 0.00002168
Iteration 25/1000 | Loss: 0.00002168
Iteration 26/1000 | Loss: 0.00002168
Iteration 27/1000 | Loss: 0.00002168
Iteration 28/1000 | Loss: 0.00002168
Iteration 29/1000 | Loss: 0.00002168
Iteration 30/1000 | Loss: 0.00002167
Iteration 31/1000 | Loss: 0.00002167
Iteration 32/1000 | Loss: 0.00002166
Iteration 33/1000 | Loss: 0.00002160
Iteration 34/1000 | Loss: 0.00002159
Iteration 35/1000 | Loss: 0.00002158
Iteration 36/1000 | Loss: 0.00002158
Iteration 37/1000 | Loss: 0.00002157
Iteration 38/1000 | Loss: 0.00002157
Iteration 39/1000 | Loss: 0.00002157
Iteration 40/1000 | Loss: 0.00002157
Iteration 41/1000 | Loss: 0.00002157
Iteration 42/1000 | Loss: 0.00002157
Iteration 43/1000 | Loss: 0.00002157
Iteration 44/1000 | Loss: 0.00002157
Iteration 45/1000 | Loss: 0.00002156
Iteration 46/1000 | Loss: 0.00002156
Iteration 47/1000 | Loss: 0.00002156
Iteration 48/1000 | Loss: 0.00002156
Iteration 49/1000 | Loss: 0.00002156
Iteration 50/1000 | Loss: 0.00002156
Iteration 51/1000 | Loss: 0.00002156
Iteration 52/1000 | Loss: 0.00002156
Iteration 53/1000 | Loss: 0.00002156
Iteration 54/1000 | Loss: 0.00002155
Iteration 55/1000 | Loss: 0.00002155
Iteration 56/1000 | Loss: 0.00002155
Iteration 57/1000 | Loss: 0.00002155
Iteration 58/1000 | Loss: 0.00002155
Iteration 59/1000 | Loss: 0.00002155
Iteration 60/1000 | Loss: 0.00002155
Iteration 61/1000 | Loss: 0.00002155
Iteration 62/1000 | Loss: 0.00002155
Iteration 63/1000 | Loss: 0.00002155
Iteration 64/1000 | Loss: 0.00002155
Iteration 65/1000 | Loss: 0.00002155
Iteration 66/1000 | Loss: 0.00002154
Iteration 67/1000 | Loss: 0.00002154
Iteration 68/1000 | Loss: 0.00002154
Iteration 69/1000 | Loss: 0.00002154
Iteration 70/1000 | Loss: 0.00002154
Iteration 71/1000 | Loss: 0.00002154
Iteration 72/1000 | Loss: 0.00002154
Iteration 73/1000 | Loss: 0.00002154
Iteration 74/1000 | Loss: 0.00002154
Iteration 75/1000 | Loss: 0.00002154
Iteration 76/1000 | Loss: 0.00002153
Iteration 77/1000 | Loss: 0.00002153
Iteration 78/1000 | Loss: 0.00002153
Iteration 79/1000 | Loss: 0.00002153
Iteration 80/1000 | Loss: 0.00002153
Iteration 81/1000 | Loss: 0.00002153
Iteration 82/1000 | Loss: 0.00002153
Iteration 83/1000 | Loss: 0.00002153
Iteration 84/1000 | Loss: 0.00002153
Iteration 85/1000 | Loss: 0.00002153
Iteration 86/1000 | Loss: 0.00002153
Iteration 87/1000 | Loss: 0.00002152
Iteration 88/1000 | Loss: 0.00002152
Iteration 89/1000 | Loss: 0.00002152
Iteration 90/1000 | Loss: 0.00002152
Iteration 91/1000 | Loss: 0.00002152
Iteration 92/1000 | Loss: 0.00002152
Iteration 93/1000 | Loss: 0.00002152
Iteration 94/1000 | Loss: 0.00002152
Iteration 95/1000 | Loss: 0.00002152
Iteration 96/1000 | Loss: 0.00002152
Iteration 97/1000 | Loss: 0.00002151
Iteration 98/1000 | Loss: 0.00002151
Iteration 99/1000 | Loss: 0.00002151
Iteration 100/1000 | Loss: 0.00002151
Iteration 101/1000 | Loss: 0.00002151
Iteration 102/1000 | Loss: 0.00002151
Iteration 103/1000 | Loss: 0.00002151
Iteration 104/1000 | Loss: 0.00002151
Iteration 105/1000 | Loss: 0.00002151
Iteration 106/1000 | Loss: 0.00002151
Iteration 107/1000 | Loss: 0.00002151
Iteration 108/1000 | Loss: 0.00002151
Iteration 109/1000 | Loss: 0.00002151
Iteration 110/1000 | Loss: 0.00002151
Iteration 111/1000 | Loss: 0.00002151
Iteration 112/1000 | Loss: 0.00002151
Iteration 113/1000 | Loss: 0.00002151
Iteration 114/1000 | Loss: 0.00002151
Iteration 115/1000 | Loss: 0.00002151
Iteration 116/1000 | Loss: 0.00002151
Iteration 117/1000 | Loss: 0.00002151
Iteration 118/1000 | Loss: 0.00002151
Iteration 119/1000 | Loss: 0.00002151
Iteration 120/1000 | Loss: 0.00002151
Iteration 121/1000 | Loss: 0.00002151
Iteration 122/1000 | Loss: 0.00002151
Iteration 123/1000 | Loss: 0.00002151
Iteration 124/1000 | Loss: 0.00002151
Iteration 125/1000 | Loss: 0.00002151
Iteration 126/1000 | Loss: 0.00002151
Iteration 127/1000 | Loss: 0.00002151
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 127. Stopping optimization.
Last 5 losses: [2.1508751160581596e-05, 2.1508751160581596e-05, 2.1508751160581596e-05, 2.1508751160581596e-05, 2.1508751160581596e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1508751160581596e-05

Optimization complete. Final v2v error: 3.8952267169952393 mm

Highest mean error: 3.927011489868164 mm for frame 141

Lowest mean error: 3.869023323059082 mm for frame 77

Saving results

Total time: 38.87783622741699
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_025/1056/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_025/1056.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_025/1056
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00725295
Iteration 2/25 | Loss: 0.00121633
Iteration 3/25 | Loss: 0.00115393
Iteration 4/25 | Loss: 0.00114392
Iteration 5/25 | Loss: 0.00114174
Iteration 6/25 | Loss: 0.00114118
Iteration 7/25 | Loss: 0.00114094
Iteration 8/25 | Loss: 0.00114087
Iteration 9/25 | Loss: 0.00114087
Iteration 10/25 | Loss: 0.00114087
Iteration 11/25 | Loss: 0.00114087
Iteration 12/25 | Loss: 0.00114087
Iteration 13/25 | Loss: 0.00114087
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0011408706195652485, 0.0011408706195652485, 0.0011408706195652485, 0.0011408706195652485, 0.0011408706195652485]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011408706195652485

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38845289
Iteration 2/25 | Loss: 0.00084048
Iteration 3/25 | Loss: 0.00084048
Iteration 4/25 | Loss: 0.00084048
Iteration 5/25 | Loss: 0.00084048
Iteration 6/25 | Loss: 0.00084048
Iteration 7/25 | Loss: 0.00084048
Iteration 8/25 | Loss: 0.00084048
Iteration 9/25 | Loss: 0.00084048
Iteration 10/25 | Loss: 0.00084048
Iteration 11/25 | Loss: 0.00084048
Iteration 12/25 | Loss: 0.00084048
Iteration 13/25 | Loss: 0.00084048
Iteration 14/25 | Loss: 0.00084048
Iteration 15/25 | Loss: 0.00084048
Iteration 16/25 | Loss: 0.00084048
Iteration 17/25 | Loss: 0.00084048
Iteration 18/25 | Loss: 0.00084048
Iteration 19/25 | Loss: 0.00084048
Iteration 20/25 | Loss: 0.00084048
Iteration 21/25 | Loss: 0.00084048
Iteration 22/25 | Loss: 0.00084048
Iteration 23/25 | Loss: 0.00084048
Iteration 24/25 | Loss: 0.00084048
Iteration 25/25 | Loss: 0.00084048

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00084048
Iteration 2/1000 | Loss: 0.00002513
Iteration 3/1000 | Loss: 0.00001833
Iteration 4/1000 | Loss: 0.00001564
Iteration 5/1000 | Loss: 0.00001491
Iteration 6/1000 | Loss: 0.00001432
Iteration 7/1000 | Loss: 0.00001403
Iteration 8/1000 | Loss: 0.00001366
Iteration 9/1000 | Loss: 0.00001344
Iteration 10/1000 | Loss: 0.00001333
Iteration 11/1000 | Loss: 0.00001309
Iteration 12/1000 | Loss: 0.00001299
Iteration 13/1000 | Loss: 0.00001295
Iteration 14/1000 | Loss: 0.00001295
Iteration 15/1000 | Loss: 0.00001294
Iteration 16/1000 | Loss: 0.00001290
Iteration 17/1000 | Loss: 0.00001284
Iteration 18/1000 | Loss: 0.00001283
Iteration 19/1000 | Loss: 0.00001280
Iteration 20/1000 | Loss: 0.00001279
Iteration 21/1000 | Loss: 0.00001279
Iteration 22/1000 | Loss: 0.00001279
Iteration 23/1000 | Loss: 0.00001279
Iteration 24/1000 | Loss: 0.00001279
Iteration 25/1000 | Loss: 0.00001279
Iteration 26/1000 | Loss: 0.00001279
Iteration 27/1000 | Loss: 0.00001277
Iteration 28/1000 | Loss: 0.00001277
Iteration 29/1000 | Loss: 0.00001276
Iteration 30/1000 | Loss: 0.00001274
Iteration 31/1000 | Loss: 0.00001274
Iteration 32/1000 | Loss: 0.00001273
Iteration 33/1000 | Loss: 0.00001272
Iteration 34/1000 | Loss: 0.00001272
Iteration 35/1000 | Loss: 0.00001271
Iteration 36/1000 | Loss: 0.00001271
Iteration 37/1000 | Loss: 0.00001269
Iteration 38/1000 | Loss: 0.00001269
Iteration 39/1000 | Loss: 0.00001269
Iteration 40/1000 | Loss: 0.00001269
Iteration 41/1000 | Loss: 0.00001269
Iteration 42/1000 | Loss: 0.00001269
Iteration 43/1000 | Loss: 0.00001269
Iteration 44/1000 | Loss: 0.00001269
Iteration 45/1000 | Loss: 0.00001269
Iteration 46/1000 | Loss: 0.00001268
Iteration 47/1000 | Loss: 0.00001268
Iteration 48/1000 | Loss: 0.00001268
Iteration 49/1000 | Loss: 0.00001268
Iteration 50/1000 | Loss: 0.00001267
Iteration 51/1000 | Loss: 0.00001267
Iteration 52/1000 | Loss: 0.00001266
Iteration 53/1000 | Loss: 0.00001266
Iteration 54/1000 | Loss: 0.00001266
Iteration 55/1000 | Loss: 0.00001265
Iteration 56/1000 | Loss: 0.00001264
Iteration 57/1000 | Loss: 0.00001264
Iteration 58/1000 | Loss: 0.00001263
Iteration 59/1000 | Loss: 0.00001262
Iteration 60/1000 | Loss: 0.00001262
Iteration 61/1000 | Loss: 0.00001261
Iteration 62/1000 | Loss: 0.00001261
Iteration 63/1000 | Loss: 0.00001261
Iteration 64/1000 | Loss: 0.00001260
Iteration 65/1000 | Loss: 0.00001260
Iteration 66/1000 | Loss: 0.00001260
Iteration 67/1000 | Loss: 0.00001259
Iteration 68/1000 | Loss: 0.00001259
Iteration 69/1000 | Loss: 0.00001258
Iteration 70/1000 | Loss: 0.00001258
Iteration 71/1000 | Loss: 0.00001258
Iteration 72/1000 | Loss: 0.00001258
Iteration 73/1000 | Loss: 0.00001258
Iteration 74/1000 | Loss: 0.00001257
Iteration 75/1000 | Loss: 0.00001255
Iteration 76/1000 | Loss: 0.00001254
Iteration 77/1000 | Loss: 0.00001253
Iteration 78/1000 | Loss: 0.00001253
Iteration 79/1000 | Loss: 0.00001253
Iteration 80/1000 | Loss: 0.00001253
Iteration 81/1000 | Loss: 0.00001253
Iteration 82/1000 | Loss: 0.00001252
Iteration 83/1000 | Loss: 0.00001251
Iteration 84/1000 | Loss: 0.00001249
Iteration 85/1000 | Loss: 0.00001248
Iteration 86/1000 | Loss: 0.00001248
Iteration 87/1000 | Loss: 0.00001248
Iteration 88/1000 | Loss: 0.00001248
Iteration 89/1000 | Loss: 0.00001247
Iteration 90/1000 | Loss: 0.00001247
Iteration 91/1000 | Loss: 0.00001247
Iteration 92/1000 | Loss: 0.00001246
Iteration 93/1000 | Loss: 0.00001246
Iteration 94/1000 | Loss: 0.00001245
Iteration 95/1000 | Loss: 0.00001245
Iteration 96/1000 | Loss: 0.00001245
Iteration 97/1000 | Loss: 0.00001245
Iteration 98/1000 | Loss: 0.00001245
Iteration 99/1000 | Loss: 0.00001244
Iteration 100/1000 | Loss: 0.00001244
Iteration 101/1000 | Loss: 0.00001244
Iteration 102/1000 | Loss: 0.00001244
Iteration 103/1000 | Loss: 0.00001244
Iteration 104/1000 | Loss: 0.00001243
Iteration 105/1000 | Loss: 0.00001243
Iteration 106/1000 | Loss: 0.00001243
Iteration 107/1000 | Loss: 0.00001243
Iteration 108/1000 | Loss: 0.00001242
Iteration 109/1000 | Loss: 0.00001242
Iteration 110/1000 | Loss: 0.00001242
Iteration 111/1000 | Loss: 0.00001242
Iteration 112/1000 | Loss: 0.00001242
Iteration 113/1000 | Loss: 0.00001242
Iteration 114/1000 | Loss: 0.00001242
Iteration 115/1000 | Loss: 0.00001241
Iteration 116/1000 | Loss: 0.00001241
Iteration 117/1000 | Loss: 0.00001241
Iteration 118/1000 | Loss: 0.00001241
Iteration 119/1000 | Loss: 0.00001241
Iteration 120/1000 | Loss: 0.00001241
Iteration 121/1000 | Loss: 0.00001241
Iteration 122/1000 | Loss: 0.00001241
Iteration 123/1000 | Loss: 0.00001241
Iteration 124/1000 | Loss: 0.00001241
Iteration 125/1000 | Loss: 0.00001241
Iteration 126/1000 | Loss: 0.00001241
Iteration 127/1000 | Loss: 0.00001241
Iteration 128/1000 | Loss: 0.00001240
Iteration 129/1000 | Loss: 0.00001240
Iteration 130/1000 | Loss: 0.00001240
Iteration 131/1000 | Loss: 0.00001240
Iteration 132/1000 | Loss: 0.00001240
Iteration 133/1000 | Loss: 0.00001240
Iteration 134/1000 | Loss: 0.00001240
Iteration 135/1000 | Loss: 0.00001240
Iteration 136/1000 | Loss: 0.00001239
Iteration 137/1000 | Loss: 0.00001239
Iteration 138/1000 | Loss: 0.00001239
Iteration 139/1000 | Loss: 0.00001239
Iteration 140/1000 | Loss: 0.00001239
Iteration 141/1000 | Loss: 0.00001239
Iteration 142/1000 | Loss: 0.00001239
Iteration 143/1000 | Loss: 0.00001238
Iteration 144/1000 | Loss: 0.00001238
Iteration 145/1000 | Loss: 0.00001238
Iteration 146/1000 | Loss: 0.00001238
Iteration 147/1000 | Loss: 0.00001238
Iteration 148/1000 | Loss: 0.00001238
Iteration 149/1000 | Loss: 0.00001238
Iteration 150/1000 | Loss: 0.00001238
Iteration 151/1000 | Loss: 0.00001238
Iteration 152/1000 | Loss: 0.00001238
Iteration 153/1000 | Loss: 0.00001238
Iteration 154/1000 | Loss: 0.00001238
Iteration 155/1000 | Loss: 0.00001238
Iteration 156/1000 | Loss: 0.00001238
Iteration 157/1000 | Loss: 0.00001238
Iteration 158/1000 | Loss: 0.00001237
Iteration 159/1000 | Loss: 0.00001237
Iteration 160/1000 | Loss: 0.00001237
Iteration 161/1000 | Loss: 0.00001237
Iteration 162/1000 | Loss: 0.00001237
Iteration 163/1000 | Loss: 0.00001237
Iteration 164/1000 | Loss: 0.00001237
Iteration 165/1000 | Loss: 0.00001237
Iteration 166/1000 | Loss: 0.00001237
Iteration 167/1000 | Loss: 0.00001237
Iteration 168/1000 | Loss: 0.00001237
Iteration 169/1000 | Loss: 0.00001237
Iteration 170/1000 | Loss: 0.00001237
Iteration 171/1000 | Loss: 0.00001237
Iteration 172/1000 | Loss: 0.00001237
Iteration 173/1000 | Loss: 0.00001237
Iteration 174/1000 | Loss: 0.00001236
Iteration 175/1000 | Loss: 0.00001236
Iteration 176/1000 | Loss: 0.00001236
Iteration 177/1000 | Loss: 0.00001236
Iteration 178/1000 | Loss: 0.00001236
Iteration 179/1000 | Loss: 0.00001236
Iteration 180/1000 | Loss: 0.00001236
Iteration 181/1000 | Loss: 0.00001236
Iteration 182/1000 | Loss: 0.00001236
Iteration 183/1000 | Loss: 0.00001236
Iteration 184/1000 | Loss: 0.00001236
Iteration 185/1000 | Loss: 0.00001236
Iteration 186/1000 | Loss: 0.00001236
Iteration 187/1000 | Loss: 0.00001236
Iteration 188/1000 | Loss: 0.00001236
Iteration 189/1000 | Loss: 0.00001236
Iteration 190/1000 | Loss: 0.00001236
Iteration 191/1000 | Loss: 0.00001236
Iteration 192/1000 | Loss: 0.00001236
Iteration 193/1000 | Loss: 0.00001236
Iteration 194/1000 | Loss: 0.00001236
Iteration 195/1000 | Loss: 0.00001236
Iteration 196/1000 | Loss: 0.00001236
Iteration 197/1000 | Loss: 0.00001236
Iteration 198/1000 | Loss: 0.00001236
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 198. Stopping optimization.
Last 5 losses: [1.2357094419712666e-05, 1.2357094419712666e-05, 1.2357094419712666e-05, 1.2357094419712666e-05, 1.2357094419712666e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2357094419712666e-05

Optimization complete. Final v2v error: 2.9950037002563477 mm

Highest mean error: 3.31718111038208 mm for frame 115

Lowest mean error: 2.9252519607543945 mm for frame 122

Saving results

Total time: 42.20215606689453
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_025/1099/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_025/1099.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_025/1099
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00400369
Iteration 2/25 | Loss: 0.00121663
Iteration 3/25 | Loss: 0.00112990
Iteration 4/25 | Loss: 0.00111361
Iteration 5/25 | Loss: 0.00110834
Iteration 6/25 | Loss: 0.00110698
Iteration 7/25 | Loss: 0.00110679
Iteration 8/25 | Loss: 0.00110679
Iteration 9/25 | Loss: 0.00110679
Iteration 10/25 | Loss: 0.00110679
Iteration 11/25 | Loss: 0.00110679
Iteration 12/25 | Loss: 0.00110679
Iteration 13/25 | Loss: 0.00110679
Iteration 14/25 | Loss: 0.00110679
Iteration 15/25 | Loss: 0.00110679
Iteration 16/25 | Loss: 0.00110679
Iteration 17/25 | Loss: 0.00110679
Iteration 18/25 | Loss: 0.00110679
Iteration 19/25 | Loss: 0.00110679
Iteration 20/25 | Loss: 0.00110679
Iteration 21/25 | Loss: 0.00110679
Iteration 22/25 | Loss: 0.00110679
Iteration 23/25 | Loss: 0.00110679
Iteration 24/25 | Loss: 0.00110679
Iteration 25/25 | Loss: 0.00110679

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.95829606
Iteration 2/25 | Loss: 0.00079484
Iteration 3/25 | Loss: 0.00079483
Iteration 4/25 | Loss: 0.00079483
Iteration 5/25 | Loss: 0.00079482
Iteration 6/25 | Loss: 0.00079482
Iteration 7/25 | Loss: 0.00079482
Iteration 8/25 | Loss: 0.00079482
Iteration 9/25 | Loss: 0.00079482
Iteration 10/25 | Loss: 0.00079482
Iteration 11/25 | Loss: 0.00079482
Iteration 12/25 | Loss: 0.00079482
Iteration 13/25 | Loss: 0.00079482
Iteration 14/25 | Loss: 0.00079482
Iteration 15/25 | Loss: 0.00079482
Iteration 16/25 | Loss: 0.00079482
Iteration 17/25 | Loss: 0.00079482
Iteration 18/25 | Loss: 0.00079482
Iteration 19/25 | Loss: 0.00079482
Iteration 20/25 | Loss: 0.00079482
Iteration 21/25 | Loss: 0.00079482
Iteration 22/25 | Loss: 0.00079482
Iteration 23/25 | Loss: 0.00079482
Iteration 24/25 | Loss: 0.00079482
Iteration 25/25 | Loss: 0.00079482

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00079482
Iteration 2/1000 | Loss: 0.00002658
Iteration 3/1000 | Loss: 0.00001905
Iteration 4/1000 | Loss: 0.00001665
Iteration 5/1000 | Loss: 0.00001556
Iteration 6/1000 | Loss: 0.00001480
Iteration 7/1000 | Loss: 0.00001423
Iteration 8/1000 | Loss: 0.00001392
Iteration 9/1000 | Loss: 0.00001357
Iteration 10/1000 | Loss: 0.00001351
Iteration 11/1000 | Loss: 0.00001338
Iteration 12/1000 | Loss: 0.00001323
Iteration 13/1000 | Loss: 0.00001302
Iteration 14/1000 | Loss: 0.00001301
Iteration 15/1000 | Loss: 0.00001297
Iteration 16/1000 | Loss: 0.00001293
Iteration 17/1000 | Loss: 0.00001292
Iteration 18/1000 | Loss: 0.00001290
Iteration 19/1000 | Loss: 0.00001289
Iteration 20/1000 | Loss: 0.00001289
Iteration 21/1000 | Loss: 0.00001288
Iteration 22/1000 | Loss: 0.00001288
Iteration 23/1000 | Loss: 0.00001280
Iteration 24/1000 | Loss: 0.00001273
Iteration 25/1000 | Loss: 0.00001272
Iteration 26/1000 | Loss: 0.00001268
Iteration 27/1000 | Loss: 0.00001264
Iteration 28/1000 | Loss: 0.00001263
Iteration 29/1000 | Loss: 0.00001258
Iteration 30/1000 | Loss: 0.00001255
Iteration 31/1000 | Loss: 0.00001254
Iteration 32/1000 | Loss: 0.00001248
Iteration 33/1000 | Loss: 0.00001248
Iteration 34/1000 | Loss: 0.00001247
Iteration 35/1000 | Loss: 0.00001247
Iteration 36/1000 | Loss: 0.00001246
Iteration 37/1000 | Loss: 0.00001246
Iteration 38/1000 | Loss: 0.00001244
Iteration 39/1000 | Loss: 0.00001244
Iteration 40/1000 | Loss: 0.00001243
Iteration 41/1000 | Loss: 0.00001242
Iteration 42/1000 | Loss: 0.00001242
Iteration 43/1000 | Loss: 0.00001241
Iteration 44/1000 | Loss: 0.00001241
Iteration 45/1000 | Loss: 0.00001240
Iteration 46/1000 | Loss: 0.00001240
Iteration 47/1000 | Loss: 0.00001239
Iteration 48/1000 | Loss: 0.00001239
Iteration 49/1000 | Loss: 0.00001238
Iteration 50/1000 | Loss: 0.00001238
Iteration 51/1000 | Loss: 0.00001237
Iteration 52/1000 | Loss: 0.00001237
Iteration 53/1000 | Loss: 0.00001236
Iteration 54/1000 | Loss: 0.00001235
Iteration 55/1000 | Loss: 0.00001234
Iteration 56/1000 | Loss: 0.00001234
Iteration 57/1000 | Loss: 0.00001233
Iteration 58/1000 | Loss: 0.00001233
Iteration 59/1000 | Loss: 0.00001232
Iteration 60/1000 | Loss: 0.00001232
Iteration 61/1000 | Loss: 0.00001231
Iteration 62/1000 | Loss: 0.00001231
Iteration 63/1000 | Loss: 0.00001229
Iteration 64/1000 | Loss: 0.00001229
Iteration 65/1000 | Loss: 0.00001228
Iteration 66/1000 | Loss: 0.00001228
Iteration 67/1000 | Loss: 0.00001228
Iteration 68/1000 | Loss: 0.00001227
Iteration 69/1000 | Loss: 0.00001227
Iteration 70/1000 | Loss: 0.00001227
Iteration 71/1000 | Loss: 0.00001227
Iteration 72/1000 | Loss: 0.00001227
Iteration 73/1000 | Loss: 0.00001227
Iteration 74/1000 | Loss: 0.00001227
Iteration 75/1000 | Loss: 0.00001226
Iteration 76/1000 | Loss: 0.00001226
Iteration 77/1000 | Loss: 0.00001226
Iteration 78/1000 | Loss: 0.00001225
Iteration 79/1000 | Loss: 0.00001225
Iteration 80/1000 | Loss: 0.00001224
Iteration 81/1000 | Loss: 0.00001224
Iteration 82/1000 | Loss: 0.00001224
Iteration 83/1000 | Loss: 0.00001224
Iteration 84/1000 | Loss: 0.00001224
Iteration 85/1000 | Loss: 0.00001223
Iteration 86/1000 | Loss: 0.00001223
Iteration 87/1000 | Loss: 0.00001223
Iteration 88/1000 | Loss: 0.00001222
Iteration 89/1000 | Loss: 0.00001222
Iteration 90/1000 | Loss: 0.00001222
Iteration 91/1000 | Loss: 0.00001221
Iteration 92/1000 | Loss: 0.00001221
Iteration 93/1000 | Loss: 0.00001221
Iteration 94/1000 | Loss: 0.00001221
Iteration 95/1000 | Loss: 0.00001220
Iteration 96/1000 | Loss: 0.00001220
Iteration 97/1000 | Loss: 0.00001220
Iteration 98/1000 | Loss: 0.00001220
Iteration 99/1000 | Loss: 0.00001220
Iteration 100/1000 | Loss: 0.00001220
Iteration 101/1000 | Loss: 0.00001219
Iteration 102/1000 | Loss: 0.00001219
Iteration 103/1000 | Loss: 0.00001219
Iteration 104/1000 | Loss: 0.00001218
Iteration 105/1000 | Loss: 0.00001218
Iteration 106/1000 | Loss: 0.00001218
Iteration 107/1000 | Loss: 0.00001217
Iteration 108/1000 | Loss: 0.00001217
Iteration 109/1000 | Loss: 0.00001217
Iteration 110/1000 | Loss: 0.00001216
Iteration 111/1000 | Loss: 0.00001216
Iteration 112/1000 | Loss: 0.00001216
Iteration 113/1000 | Loss: 0.00001216
Iteration 114/1000 | Loss: 0.00001216
Iteration 115/1000 | Loss: 0.00001215
Iteration 116/1000 | Loss: 0.00001215
Iteration 117/1000 | Loss: 0.00001215
Iteration 118/1000 | Loss: 0.00001215
Iteration 119/1000 | Loss: 0.00001215
Iteration 120/1000 | Loss: 0.00001215
Iteration 121/1000 | Loss: 0.00001214
Iteration 122/1000 | Loss: 0.00001214
Iteration 123/1000 | Loss: 0.00001214
Iteration 124/1000 | Loss: 0.00001214
Iteration 125/1000 | Loss: 0.00001214
Iteration 126/1000 | Loss: 0.00001213
Iteration 127/1000 | Loss: 0.00001213
Iteration 128/1000 | Loss: 0.00001213
Iteration 129/1000 | Loss: 0.00001213
Iteration 130/1000 | Loss: 0.00001213
Iteration 131/1000 | Loss: 0.00001213
Iteration 132/1000 | Loss: 0.00001213
Iteration 133/1000 | Loss: 0.00001212
Iteration 134/1000 | Loss: 0.00001212
Iteration 135/1000 | Loss: 0.00001212
Iteration 136/1000 | Loss: 0.00001212
Iteration 137/1000 | Loss: 0.00001212
Iteration 138/1000 | Loss: 0.00001212
Iteration 139/1000 | Loss: 0.00001212
Iteration 140/1000 | Loss: 0.00001212
Iteration 141/1000 | Loss: 0.00001212
Iteration 142/1000 | Loss: 0.00001212
Iteration 143/1000 | Loss: 0.00001212
Iteration 144/1000 | Loss: 0.00001212
Iteration 145/1000 | Loss: 0.00001212
Iteration 146/1000 | Loss: 0.00001212
Iteration 147/1000 | Loss: 0.00001212
Iteration 148/1000 | Loss: 0.00001212
Iteration 149/1000 | Loss: 0.00001212
Iteration 150/1000 | Loss: 0.00001212
Iteration 151/1000 | Loss: 0.00001212
Iteration 152/1000 | Loss: 0.00001212
Iteration 153/1000 | Loss: 0.00001211
Iteration 154/1000 | Loss: 0.00001211
Iteration 155/1000 | Loss: 0.00001211
Iteration 156/1000 | Loss: 0.00001211
Iteration 157/1000 | Loss: 0.00001211
Iteration 158/1000 | Loss: 0.00001211
Iteration 159/1000 | Loss: 0.00001210
Iteration 160/1000 | Loss: 0.00001210
Iteration 161/1000 | Loss: 0.00001210
Iteration 162/1000 | Loss: 0.00001210
Iteration 163/1000 | Loss: 0.00001210
Iteration 164/1000 | Loss: 0.00001210
Iteration 165/1000 | Loss: 0.00001210
Iteration 166/1000 | Loss: 0.00001210
Iteration 167/1000 | Loss: 0.00001210
Iteration 168/1000 | Loss: 0.00001210
Iteration 169/1000 | Loss: 0.00001210
Iteration 170/1000 | Loss: 0.00001210
Iteration 171/1000 | Loss: 0.00001210
Iteration 172/1000 | Loss: 0.00001210
Iteration 173/1000 | Loss: 0.00001210
Iteration 174/1000 | Loss: 0.00001210
Iteration 175/1000 | Loss: 0.00001210
Iteration 176/1000 | Loss: 0.00001210
Iteration 177/1000 | Loss: 0.00001209
Iteration 178/1000 | Loss: 0.00001209
Iteration 179/1000 | Loss: 0.00001209
Iteration 180/1000 | Loss: 0.00001209
Iteration 181/1000 | Loss: 0.00001209
Iteration 182/1000 | Loss: 0.00001209
Iteration 183/1000 | Loss: 0.00001209
Iteration 184/1000 | Loss: 0.00001209
Iteration 185/1000 | Loss: 0.00001209
Iteration 186/1000 | Loss: 0.00001209
Iteration 187/1000 | Loss: 0.00001209
Iteration 188/1000 | Loss: 0.00001209
Iteration 189/1000 | Loss: 0.00001209
Iteration 190/1000 | Loss: 0.00001209
Iteration 191/1000 | Loss: 0.00001208
Iteration 192/1000 | Loss: 0.00001208
Iteration 193/1000 | Loss: 0.00001208
Iteration 194/1000 | Loss: 0.00001208
Iteration 195/1000 | Loss: 0.00001208
Iteration 196/1000 | Loss: 0.00001208
Iteration 197/1000 | Loss: 0.00001208
Iteration 198/1000 | Loss: 0.00001208
Iteration 199/1000 | Loss: 0.00001208
Iteration 200/1000 | Loss: 0.00001208
Iteration 201/1000 | Loss: 0.00001208
Iteration 202/1000 | Loss: 0.00001208
Iteration 203/1000 | Loss: 0.00001208
Iteration 204/1000 | Loss: 0.00001208
Iteration 205/1000 | Loss: 0.00001208
Iteration 206/1000 | Loss: 0.00001208
Iteration 207/1000 | Loss: 0.00001208
Iteration 208/1000 | Loss: 0.00001208
Iteration 209/1000 | Loss: 0.00001208
Iteration 210/1000 | Loss: 0.00001208
Iteration 211/1000 | Loss: 0.00001208
Iteration 212/1000 | Loss: 0.00001208
Iteration 213/1000 | Loss: 0.00001208
Iteration 214/1000 | Loss: 0.00001208
Iteration 215/1000 | Loss: 0.00001208
Iteration 216/1000 | Loss: 0.00001208
Iteration 217/1000 | Loss: 0.00001208
Iteration 218/1000 | Loss: 0.00001208
Iteration 219/1000 | Loss: 0.00001208
Iteration 220/1000 | Loss: 0.00001208
Iteration 221/1000 | Loss: 0.00001208
Iteration 222/1000 | Loss: 0.00001208
Iteration 223/1000 | Loss: 0.00001208
Iteration 224/1000 | Loss: 0.00001208
Iteration 225/1000 | Loss: 0.00001208
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 225. Stopping optimization.
Last 5 losses: [1.2080956366844475e-05, 1.2080956366844475e-05, 1.2080956366844475e-05, 1.2080956366844475e-05, 1.2080956366844475e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2080956366844475e-05

Optimization complete. Final v2v error: 2.975694179534912 mm

Highest mean error: 3.972565174102783 mm for frame 57

Lowest mean error: 2.7018613815307617 mm for frame 91

Saving results

Total time: 46.13966393470764
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_025/1083/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_025/1083.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_025/1083
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00454905
Iteration 2/25 | Loss: 0.00127215
Iteration 3/25 | Loss: 0.00117090
Iteration 4/25 | Loss: 0.00115484
Iteration 5/25 | Loss: 0.00114936
Iteration 6/25 | Loss: 0.00114792
Iteration 7/25 | Loss: 0.00114786
Iteration 8/25 | Loss: 0.00114786
Iteration 9/25 | Loss: 0.00114786
Iteration 10/25 | Loss: 0.00114786
Iteration 11/25 | Loss: 0.00114786
Iteration 12/25 | Loss: 0.00114786
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0011478561209514737, 0.0011478561209514737, 0.0011478561209514737, 0.0011478561209514737, 0.0011478561209514737]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011478561209514737

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.31660235
Iteration 2/25 | Loss: 0.00077762
Iteration 3/25 | Loss: 0.00077761
Iteration 4/25 | Loss: 0.00077761
Iteration 5/25 | Loss: 0.00077761
Iteration 6/25 | Loss: 0.00077761
Iteration 7/25 | Loss: 0.00077761
Iteration 8/25 | Loss: 0.00077761
Iteration 9/25 | Loss: 0.00077761
Iteration 10/25 | Loss: 0.00077761
Iteration 11/25 | Loss: 0.00077761
Iteration 12/25 | Loss: 0.00077761
Iteration 13/25 | Loss: 0.00077761
Iteration 14/25 | Loss: 0.00077761
Iteration 15/25 | Loss: 0.00077761
Iteration 16/25 | Loss: 0.00077761
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0007776117417961359, 0.0007776117417961359, 0.0007776117417961359, 0.0007776117417961359, 0.0007776117417961359]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007776117417961359

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00077761
Iteration 2/1000 | Loss: 0.00003106
Iteration 3/1000 | Loss: 0.00002101
Iteration 4/1000 | Loss: 0.00001701
Iteration 5/1000 | Loss: 0.00001593
Iteration 6/1000 | Loss: 0.00001523
Iteration 7/1000 | Loss: 0.00001485
Iteration 8/1000 | Loss: 0.00001442
Iteration 9/1000 | Loss: 0.00001422
Iteration 10/1000 | Loss: 0.00001401
Iteration 11/1000 | Loss: 0.00001384
Iteration 12/1000 | Loss: 0.00001373
Iteration 13/1000 | Loss: 0.00001371
Iteration 14/1000 | Loss: 0.00001368
Iteration 15/1000 | Loss: 0.00001367
Iteration 16/1000 | Loss: 0.00001366
Iteration 17/1000 | Loss: 0.00001365
Iteration 18/1000 | Loss: 0.00001365
Iteration 19/1000 | Loss: 0.00001364
Iteration 20/1000 | Loss: 0.00001363
Iteration 21/1000 | Loss: 0.00001363
Iteration 22/1000 | Loss: 0.00001360
Iteration 23/1000 | Loss: 0.00001359
Iteration 24/1000 | Loss: 0.00001359
Iteration 25/1000 | Loss: 0.00001358
Iteration 26/1000 | Loss: 0.00001357
Iteration 27/1000 | Loss: 0.00001357
Iteration 28/1000 | Loss: 0.00001351
Iteration 29/1000 | Loss: 0.00001347
Iteration 30/1000 | Loss: 0.00001347
Iteration 31/1000 | Loss: 0.00001347
Iteration 32/1000 | Loss: 0.00001346
Iteration 33/1000 | Loss: 0.00001346
Iteration 34/1000 | Loss: 0.00001345
Iteration 35/1000 | Loss: 0.00001345
Iteration 36/1000 | Loss: 0.00001344
Iteration 37/1000 | Loss: 0.00001342
Iteration 38/1000 | Loss: 0.00001342
Iteration 39/1000 | Loss: 0.00001341
Iteration 40/1000 | Loss: 0.00001341
Iteration 41/1000 | Loss: 0.00001341
Iteration 42/1000 | Loss: 0.00001340
Iteration 43/1000 | Loss: 0.00001340
Iteration 44/1000 | Loss: 0.00001339
Iteration 45/1000 | Loss: 0.00001339
Iteration 46/1000 | Loss: 0.00001338
Iteration 47/1000 | Loss: 0.00001338
Iteration 48/1000 | Loss: 0.00001337
Iteration 49/1000 | Loss: 0.00001337
Iteration 50/1000 | Loss: 0.00001337
Iteration 51/1000 | Loss: 0.00001336
Iteration 52/1000 | Loss: 0.00001336
Iteration 53/1000 | Loss: 0.00001336
Iteration 54/1000 | Loss: 0.00001336
Iteration 55/1000 | Loss: 0.00001336
Iteration 56/1000 | Loss: 0.00001336
Iteration 57/1000 | Loss: 0.00001336
Iteration 58/1000 | Loss: 0.00001336
Iteration 59/1000 | Loss: 0.00001336
Iteration 60/1000 | Loss: 0.00001336
Iteration 61/1000 | Loss: 0.00001335
Iteration 62/1000 | Loss: 0.00001335
Iteration 63/1000 | Loss: 0.00001335
Iteration 64/1000 | Loss: 0.00001335
Iteration 65/1000 | Loss: 0.00001334
Iteration 66/1000 | Loss: 0.00001334
Iteration 67/1000 | Loss: 0.00001334
Iteration 68/1000 | Loss: 0.00001333
Iteration 69/1000 | Loss: 0.00001333
Iteration 70/1000 | Loss: 0.00001333
Iteration 71/1000 | Loss: 0.00001332
Iteration 72/1000 | Loss: 0.00001332
Iteration 73/1000 | Loss: 0.00001332
Iteration 74/1000 | Loss: 0.00001332
Iteration 75/1000 | Loss: 0.00001332
Iteration 76/1000 | Loss: 0.00001332
Iteration 77/1000 | Loss: 0.00001332
Iteration 78/1000 | Loss: 0.00001332
Iteration 79/1000 | Loss: 0.00001332
Iteration 80/1000 | Loss: 0.00001331
Iteration 81/1000 | Loss: 0.00001331
Iteration 82/1000 | Loss: 0.00001330
Iteration 83/1000 | Loss: 0.00001330
Iteration 84/1000 | Loss: 0.00001330
Iteration 85/1000 | Loss: 0.00001329
Iteration 86/1000 | Loss: 0.00001329
Iteration 87/1000 | Loss: 0.00001329
Iteration 88/1000 | Loss: 0.00001329
Iteration 89/1000 | Loss: 0.00001328
Iteration 90/1000 | Loss: 0.00001328
Iteration 91/1000 | Loss: 0.00001327
Iteration 92/1000 | Loss: 0.00001327
Iteration 93/1000 | Loss: 0.00001327
Iteration 94/1000 | Loss: 0.00001327
Iteration 95/1000 | Loss: 0.00001326
Iteration 96/1000 | Loss: 0.00001326
Iteration 97/1000 | Loss: 0.00001326
Iteration 98/1000 | Loss: 0.00001326
Iteration 99/1000 | Loss: 0.00001326
Iteration 100/1000 | Loss: 0.00001325
Iteration 101/1000 | Loss: 0.00001325
Iteration 102/1000 | Loss: 0.00001325
Iteration 103/1000 | Loss: 0.00001325
Iteration 104/1000 | Loss: 0.00001325
Iteration 105/1000 | Loss: 0.00001324
Iteration 106/1000 | Loss: 0.00001324
Iteration 107/1000 | Loss: 0.00001323
Iteration 108/1000 | Loss: 0.00001323
Iteration 109/1000 | Loss: 0.00001322
Iteration 110/1000 | Loss: 0.00001322
Iteration 111/1000 | Loss: 0.00001322
Iteration 112/1000 | Loss: 0.00001322
Iteration 113/1000 | Loss: 0.00001322
Iteration 114/1000 | Loss: 0.00001322
Iteration 115/1000 | Loss: 0.00001322
Iteration 116/1000 | Loss: 0.00001322
Iteration 117/1000 | Loss: 0.00001321
Iteration 118/1000 | Loss: 0.00001321
Iteration 119/1000 | Loss: 0.00001321
Iteration 120/1000 | Loss: 0.00001321
Iteration 121/1000 | Loss: 0.00001321
Iteration 122/1000 | Loss: 0.00001321
Iteration 123/1000 | Loss: 0.00001320
Iteration 124/1000 | Loss: 0.00001319
Iteration 125/1000 | Loss: 0.00001319
Iteration 126/1000 | Loss: 0.00001319
Iteration 127/1000 | Loss: 0.00001319
Iteration 128/1000 | Loss: 0.00001319
Iteration 129/1000 | Loss: 0.00001318
Iteration 130/1000 | Loss: 0.00001318
Iteration 131/1000 | Loss: 0.00001317
Iteration 132/1000 | Loss: 0.00001317
Iteration 133/1000 | Loss: 0.00001317
Iteration 134/1000 | Loss: 0.00001317
Iteration 135/1000 | Loss: 0.00001317
Iteration 136/1000 | Loss: 0.00001316
Iteration 137/1000 | Loss: 0.00001316
Iteration 138/1000 | Loss: 0.00001316
Iteration 139/1000 | Loss: 0.00001316
Iteration 140/1000 | Loss: 0.00001315
Iteration 141/1000 | Loss: 0.00001315
Iteration 142/1000 | Loss: 0.00001315
Iteration 143/1000 | Loss: 0.00001315
Iteration 144/1000 | Loss: 0.00001314
Iteration 145/1000 | Loss: 0.00001314
Iteration 146/1000 | Loss: 0.00001314
Iteration 147/1000 | Loss: 0.00001314
Iteration 148/1000 | Loss: 0.00001313
Iteration 149/1000 | Loss: 0.00001313
Iteration 150/1000 | Loss: 0.00001313
Iteration 151/1000 | Loss: 0.00001312
Iteration 152/1000 | Loss: 0.00001312
Iteration 153/1000 | Loss: 0.00001312
Iteration 154/1000 | Loss: 0.00001312
Iteration 155/1000 | Loss: 0.00001312
Iteration 156/1000 | Loss: 0.00001312
Iteration 157/1000 | Loss: 0.00001312
Iteration 158/1000 | Loss: 0.00001311
Iteration 159/1000 | Loss: 0.00001311
Iteration 160/1000 | Loss: 0.00001311
Iteration 161/1000 | Loss: 0.00001311
Iteration 162/1000 | Loss: 0.00001311
Iteration 163/1000 | Loss: 0.00001311
Iteration 164/1000 | Loss: 0.00001311
Iteration 165/1000 | Loss: 0.00001311
Iteration 166/1000 | Loss: 0.00001311
Iteration 167/1000 | Loss: 0.00001311
Iteration 168/1000 | Loss: 0.00001311
Iteration 169/1000 | Loss: 0.00001311
Iteration 170/1000 | Loss: 0.00001310
Iteration 171/1000 | Loss: 0.00001310
Iteration 172/1000 | Loss: 0.00001310
Iteration 173/1000 | Loss: 0.00001310
Iteration 174/1000 | Loss: 0.00001310
Iteration 175/1000 | Loss: 0.00001310
Iteration 176/1000 | Loss: 0.00001309
Iteration 177/1000 | Loss: 0.00001309
Iteration 178/1000 | Loss: 0.00001309
Iteration 179/1000 | Loss: 0.00001309
Iteration 180/1000 | Loss: 0.00001308
Iteration 181/1000 | Loss: 0.00001308
Iteration 182/1000 | Loss: 0.00001308
Iteration 183/1000 | Loss: 0.00001308
Iteration 184/1000 | Loss: 0.00001308
Iteration 185/1000 | Loss: 0.00001308
Iteration 186/1000 | Loss: 0.00001308
Iteration 187/1000 | Loss: 0.00001308
Iteration 188/1000 | Loss: 0.00001307
Iteration 189/1000 | Loss: 0.00001307
Iteration 190/1000 | Loss: 0.00001307
Iteration 191/1000 | Loss: 0.00001307
Iteration 192/1000 | Loss: 0.00001307
Iteration 193/1000 | Loss: 0.00001307
Iteration 194/1000 | Loss: 0.00001306
Iteration 195/1000 | Loss: 0.00001306
Iteration 196/1000 | Loss: 0.00001306
Iteration 197/1000 | Loss: 0.00001306
Iteration 198/1000 | Loss: 0.00001306
Iteration 199/1000 | Loss: 0.00001306
Iteration 200/1000 | Loss: 0.00001306
Iteration 201/1000 | Loss: 0.00001306
Iteration 202/1000 | Loss: 0.00001306
Iteration 203/1000 | Loss: 0.00001306
Iteration 204/1000 | Loss: 0.00001306
Iteration 205/1000 | Loss: 0.00001306
Iteration 206/1000 | Loss: 0.00001306
Iteration 207/1000 | Loss: 0.00001306
Iteration 208/1000 | Loss: 0.00001306
Iteration 209/1000 | Loss: 0.00001305
Iteration 210/1000 | Loss: 0.00001305
Iteration 211/1000 | Loss: 0.00001305
Iteration 212/1000 | Loss: 0.00001305
Iteration 213/1000 | Loss: 0.00001305
Iteration 214/1000 | Loss: 0.00001305
Iteration 215/1000 | Loss: 0.00001305
Iteration 216/1000 | Loss: 0.00001305
Iteration 217/1000 | Loss: 0.00001305
Iteration 218/1000 | Loss: 0.00001305
Iteration 219/1000 | Loss: 0.00001305
Iteration 220/1000 | Loss: 0.00001305
Iteration 221/1000 | Loss: 0.00001305
Iteration 222/1000 | Loss: 0.00001305
Iteration 223/1000 | Loss: 0.00001305
Iteration 224/1000 | Loss: 0.00001305
Iteration 225/1000 | Loss: 0.00001305
Iteration 226/1000 | Loss: 0.00001305
Iteration 227/1000 | Loss: 0.00001305
Iteration 228/1000 | Loss: 0.00001305
Iteration 229/1000 | Loss: 0.00001305
Iteration 230/1000 | Loss: 0.00001305
Iteration 231/1000 | Loss: 0.00001305
Iteration 232/1000 | Loss: 0.00001305
Iteration 233/1000 | Loss: 0.00001305
Iteration 234/1000 | Loss: 0.00001305
Iteration 235/1000 | Loss: 0.00001305
Iteration 236/1000 | Loss: 0.00001305
Iteration 237/1000 | Loss: 0.00001305
Iteration 238/1000 | Loss: 0.00001305
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 238. Stopping optimization.
Last 5 losses: [1.304880152019905e-05, 1.304880152019905e-05, 1.304880152019905e-05, 1.304880152019905e-05, 1.304880152019905e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.304880152019905e-05

Optimization complete. Final v2v error: 3.0731403827667236 mm

Highest mean error: 3.5253067016601562 mm for frame 43

Lowest mean error: 2.686769962310791 mm for frame 83

Saving results

Total time: 41.592467069625854
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_025/1078/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_025/1078.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_025/1078
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00784616
Iteration 2/25 | Loss: 0.00141744
Iteration 3/25 | Loss: 0.00119464
Iteration 4/25 | Loss: 0.00116603
Iteration 5/25 | Loss: 0.00115910
Iteration 6/25 | Loss: 0.00115814
Iteration 7/25 | Loss: 0.00115789
Iteration 8/25 | Loss: 0.00115789
Iteration 9/25 | Loss: 0.00115789
Iteration 10/25 | Loss: 0.00115789
Iteration 11/25 | Loss: 0.00115789
Iteration 12/25 | Loss: 0.00115789
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0011578872799873352, 0.0011578872799873352, 0.0011578872799873352, 0.0011578872799873352, 0.0011578872799873352]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011578872799873352

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.31751001
Iteration 2/25 | Loss: 0.00073591
Iteration 3/25 | Loss: 0.00073590
Iteration 4/25 | Loss: 0.00073590
Iteration 5/25 | Loss: 0.00073590
Iteration 6/25 | Loss: 0.00073590
Iteration 7/25 | Loss: 0.00073590
Iteration 8/25 | Loss: 0.00073590
Iteration 9/25 | Loss: 0.00073590
Iteration 10/25 | Loss: 0.00073590
Iteration 11/25 | Loss: 0.00073590
Iteration 12/25 | Loss: 0.00073590
Iteration 13/25 | Loss: 0.00073590
Iteration 14/25 | Loss: 0.00073590
Iteration 15/25 | Loss: 0.00073590
Iteration 16/25 | Loss: 0.00073590
Iteration 17/25 | Loss: 0.00073590
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0007358980365097523, 0.0007358980365097523, 0.0007358980365097523, 0.0007358980365097523, 0.0007358980365097523]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007358980365097523

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00073590
Iteration 2/1000 | Loss: 0.00004014
Iteration 3/1000 | Loss: 0.00002739
Iteration 4/1000 | Loss: 0.00002213
Iteration 5/1000 | Loss: 0.00002083
Iteration 6/1000 | Loss: 0.00001963
Iteration 7/1000 | Loss: 0.00001881
Iteration 8/1000 | Loss: 0.00001834
Iteration 9/1000 | Loss: 0.00001799
Iteration 10/1000 | Loss: 0.00001774
Iteration 11/1000 | Loss: 0.00001741
Iteration 12/1000 | Loss: 0.00001721
Iteration 13/1000 | Loss: 0.00001711
Iteration 14/1000 | Loss: 0.00001703
Iteration 15/1000 | Loss: 0.00001700
Iteration 16/1000 | Loss: 0.00001699
Iteration 17/1000 | Loss: 0.00001697
Iteration 18/1000 | Loss: 0.00001696
Iteration 19/1000 | Loss: 0.00001693
Iteration 20/1000 | Loss: 0.00001691
Iteration 21/1000 | Loss: 0.00001690
Iteration 22/1000 | Loss: 0.00001689
Iteration 23/1000 | Loss: 0.00001689
Iteration 24/1000 | Loss: 0.00001688
Iteration 25/1000 | Loss: 0.00001687
Iteration 26/1000 | Loss: 0.00001686
Iteration 27/1000 | Loss: 0.00001686
Iteration 28/1000 | Loss: 0.00001685
Iteration 29/1000 | Loss: 0.00001684
Iteration 30/1000 | Loss: 0.00001683
Iteration 31/1000 | Loss: 0.00001683
Iteration 32/1000 | Loss: 0.00001682
Iteration 33/1000 | Loss: 0.00001681
Iteration 34/1000 | Loss: 0.00001681
Iteration 35/1000 | Loss: 0.00001677
Iteration 36/1000 | Loss: 0.00001676
Iteration 37/1000 | Loss: 0.00001675
Iteration 38/1000 | Loss: 0.00001675
Iteration 39/1000 | Loss: 0.00001674
Iteration 40/1000 | Loss: 0.00001674
Iteration 41/1000 | Loss: 0.00001674
Iteration 42/1000 | Loss: 0.00001674
Iteration 43/1000 | Loss: 0.00001672
Iteration 44/1000 | Loss: 0.00001672
Iteration 45/1000 | Loss: 0.00001672
Iteration 46/1000 | Loss: 0.00001672
Iteration 47/1000 | Loss: 0.00001672
Iteration 48/1000 | Loss: 0.00001672
Iteration 49/1000 | Loss: 0.00001672
Iteration 50/1000 | Loss: 0.00001672
Iteration 51/1000 | Loss: 0.00001671
Iteration 52/1000 | Loss: 0.00001671
Iteration 53/1000 | Loss: 0.00001670
Iteration 54/1000 | Loss: 0.00001670
Iteration 55/1000 | Loss: 0.00001669
Iteration 56/1000 | Loss: 0.00001669
Iteration 57/1000 | Loss: 0.00001669
Iteration 58/1000 | Loss: 0.00001668
Iteration 59/1000 | Loss: 0.00001667
Iteration 60/1000 | Loss: 0.00001667
Iteration 61/1000 | Loss: 0.00001667
Iteration 62/1000 | Loss: 0.00001667
Iteration 63/1000 | Loss: 0.00001666
Iteration 64/1000 | Loss: 0.00001666
Iteration 65/1000 | Loss: 0.00001666
Iteration 66/1000 | Loss: 0.00001665
Iteration 67/1000 | Loss: 0.00001665
Iteration 68/1000 | Loss: 0.00001664
Iteration 69/1000 | Loss: 0.00001664
Iteration 70/1000 | Loss: 0.00001664
Iteration 71/1000 | Loss: 0.00001663
Iteration 72/1000 | Loss: 0.00001662
Iteration 73/1000 | Loss: 0.00001662
Iteration 74/1000 | Loss: 0.00001661
Iteration 75/1000 | Loss: 0.00001661
Iteration 76/1000 | Loss: 0.00001661
Iteration 77/1000 | Loss: 0.00001660
Iteration 78/1000 | Loss: 0.00001660
Iteration 79/1000 | Loss: 0.00001660
Iteration 80/1000 | Loss: 0.00001659
Iteration 81/1000 | Loss: 0.00001659
Iteration 82/1000 | Loss: 0.00001659
Iteration 83/1000 | Loss: 0.00001658
Iteration 84/1000 | Loss: 0.00001658
Iteration 85/1000 | Loss: 0.00001658
Iteration 86/1000 | Loss: 0.00001658
Iteration 87/1000 | Loss: 0.00001658
Iteration 88/1000 | Loss: 0.00001657
Iteration 89/1000 | Loss: 0.00001657
Iteration 90/1000 | Loss: 0.00001657
Iteration 91/1000 | Loss: 0.00001657
Iteration 92/1000 | Loss: 0.00001657
Iteration 93/1000 | Loss: 0.00001656
Iteration 94/1000 | Loss: 0.00001656
Iteration 95/1000 | Loss: 0.00001656
Iteration 96/1000 | Loss: 0.00001656
Iteration 97/1000 | Loss: 0.00001656
Iteration 98/1000 | Loss: 0.00001656
Iteration 99/1000 | Loss: 0.00001656
Iteration 100/1000 | Loss: 0.00001656
Iteration 101/1000 | Loss: 0.00001656
Iteration 102/1000 | Loss: 0.00001656
Iteration 103/1000 | Loss: 0.00001656
Iteration 104/1000 | Loss: 0.00001656
Iteration 105/1000 | Loss: 0.00001656
Iteration 106/1000 | Loss: 0.00001656
Iteration 107/1000 | Loss: 0.00001656
Iteration 108/1000 | Loss: 0.00001656
Iteration 109/1000 | Loss: 0.00001656
Iteration 110/1000 | Loss: 0.00001656
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 110. Stopping optimization.
Last 5 losses: [1.6557114577153698e-05, 1.6557114577153698e-05, 1.6557114577153698e-05, 1.6557114577153698e-05, 1.6557114577153698e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6557114577153698e-05

Optimization complete. Final v2v error: 3.3860042095184326 mm

Highest mean error: 4.385910987854004 mm for frame 164

Lowest mean error: 2.849379062652588 mm for frame 217

Saving results

Total time: 41.812575340270996
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_025/1028/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_025/1028.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_025/1028
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01058601
Iteration 2/25 | Loss: 0.01058601
Iteration 3/25 | Loss: 0.01058601
Iteration 4/25 | Loss: 0.01058601
Iteration 5/25 | Loss: 0.01058601
Iteration 6/25 | Loss: 0.01058601
Iteration 7/25 | Loss: 0.01058601
Iteration 8/25 | Loss: 0.01058601
Iteration 9/25 | Loss: 0.01058601
Iteration 10/25 | Loss: 0.01058601
Iteration 11/25 | Loss: 0.01058601
Iteration 12/25 | Loss: 0.01058601
Iteration 13/25 | Loss: 0.01058601
Iteration 14/25 | Loss: 0.01058601
Iteration 15/25 | Loss: 0.01058601
Iteration 16/25 | Loss: 0.01058601
Iteration 17/25 | Loss: 0.01058601
Iteration 18/25 | Loss: 0.01058601
Iteration 19/25 | Loss: 0.01058601
Iteration 20/25 | Loss: 0.01058601
Iteration 21/25 | Loss: 0.01058601
Iteration 22/25 | Loss: 0.01058601
Iteration 23/25 | Loss: 0.01058600
Iteration 24/25 | Loss: 0.01058600
Iteration 25/25 | Loss: 0.01058600

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.75052774
Iteration 2/25 | Loss: 0.06115836
Iteration 3/25 | Loss: 0.06098142
Iteration 4/25 | Loss: 0.05975322
Iteration 5/25 | Loss: 0.05975320
Iteration 6/25 | Loss: 0.05975319
Iteration 7/25 | Loss: 0.05975319
Iteration 8/25 | Loss: 0.05975319
Iteration 9/25 | Loss: 0.05975319
Iteration 10/25 | Loss: 0.05975319
Iteration 11/25 | Loss: 0.05975319
Iteration 12/25 | Loss: 0.05975319
Iteration 13/25 | Loss: 0.05975319
Iteration 14/25 | Loss: 0.05975319
Iteration 15/25 | Loss: 0.05975318
Iteration 16/25 | Loss: 0.05975319
Iteration 17/25 | Loss: 0.05975319
Iteration 18/25 | Loss: 0.05975319
Iteration 19/25 | Loss: 0.05975319
Iteration 20/25 | Loss: 0.05975318
Iteration 21/25 | Loss: 0.05975319
Iteration 22/25 | Loss: 0.05975319
Iteration 23/25 | Loss: 0.05975319
Iteration 24/25 | Loss: 0.05975318
Iteration 25/25 | Loss: 0.05975318

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.05975318
Iteration 2/1000 | Loss: 0.00188394
Iteration 3/1000 | Loss: 0.00130919
Iteration 4/1000 | Loss: 0.00079482
Iteration 5/1000 | Loss: 0.00031695
Iteration 6/1000 | Loss: 0.00011680
Iteration 7/1000 | Loss: 0.00040939
Iteration 8/1000 | Loss: 0.00006546
Iteration 9/1000 | Loss: 0.00013760
Iteration 10/1000 | Loss: 0.00005343
Iteration 11/1000 | Loss: 0.00023712
Iteration 12/1000 | Loss: 0.00003830
Iteration 13/1000 | Loss: 0.00004466
Iteration 14/1000 | Loss: 0.00003333
Iteration 15/1000 | Loss: 0.00003042
Iteration 16/1000 | Loss: 0.00014016
Iteration 17/1000 | Loss: 0.00020675
Iteration 18/1000 | Loss: 0.00005648
Iteration 19/1000 | Loss: 0.00019807
Iteration 20/1000 | Loss: 0.00002563
Iteration 21/1000 | Loss: 0.00011866
Iteration 22/1000 | Loss: 0.00002348
Iteration 23/1000 | Loss: 0.00002247
Iteration 24/1000 | Loss: 0.00003948
Iteration 25/1000 | Loss: 0.00002156
Iteration 26/1000 | Loss: 0.00004565
Iteration 27/1000 | Loss: 0.00006658
Iteration 28/1000 | Loss: 0.00002070
Iteration 29/1000 | Loss: 0.00002035
Iteration 30/1000 | Loss: 0.00005111
Iteration 31/1000 | Loss: 0.00004092
Iteration 32/1000 | Loss: 0.00001963
Iteration 33/1000 | Loss: 0.00003049
Iteration 34/1000 | Loss: 0.00001942
Iteration 35/1000 | Loss: 0.00001937
Iteration 36/1000 | Loss: 0.00001918
Iteration 37/1000 | Loss: 0.00005227
Iteration 38/1000 | Loss: 0.00004252
Iteration 39/1000 | Loss: 0.00006864
Iteration 40/1000 | Loss: 0.00009671
Iteration 41/1000 | Loss: 0.00005986
Iteration 42/1000 | Loss: 0.00001909
Iteration 43/1000 | Loss: 0.00003088
Iteration 44/1000 | Loss: 0.00004138
Iteration 45/1000 | Loss: 0.00002221
Iteration 46/1000 | Loss: 0.00001895
Iteration 47/1000 | Loss: 0.00001895
Iteration 48/1000 | Loss: 0.00001895
Iteration 49/1000 | Loss: 0.00001895
Iteration 50/1000 | Loss: 0.00001895
Iteration 51/1000 | Loss: 0.00001895
Iteration 52/1000 | Loss: 0.00001895
Iteration 53/1000 | Loss: 0.00001895
Iteration 54/1000 | Loss: 0.00001895
Iteration 55/1000 | Loss: 0.00001894
Iteration 56/1000 | Loss: 0.00003775
Iteration 57/1000 | Loss: 0.00002900
Iteration 58/1000 | Loss: 0.00002372
Iteration 59/1000 | Loss: 0.00008884
Iteration 60/1000 | Loss: 0.00002796
Iteration 61/1000 | Loss: 0.00002881
Iteration 62/1000 | Loss: 0.00001953
Iteration 63/1000 | Loss: 0.00001884
Iteration 64/1000 | Loss: 0.00001883
Iteration 65/1000 | Loss: 0.00001883
Iteration 66/1000 | Loss: 0.00001883
Iteration 67/1000 | Loss: 0.00001883
Iteration 68/1000 | Loss: 0.00001883
Iteration 69/1000 | Loss: 0.00001883
Iteration 70/1000 | Loss: 0.00001883
Iteration 71/1000 | Loss: 0.00001883
Iteration 72/1000 | Loss: 0.00001883
Iteration 73/1000 | Loss: 0.00001883
Iteration 74/1000 | Loss: 0.00001883
Iteration 75/1000 | Loss: 0.00001883
Iteration 76/1000 | Loss: 0.00001883
Iteration 77/1000 | Loss: 0.00001883
Iteration 78/1000 | Loss: 0.00001883
Iteration 79/1000 | Loss: 0.00001883
Iteration 80/1000 | Loss: 0.00001883
Iteration 81/1000 | Loss: 0.00001883
Iteration 82/1000 | Loss: 0.00001883
Iteration 83/1000 | Loss: 0.00001883
Iteration 84/1000 | Loss: 0.00001883
Iteration 85/1000 | Loss: 0.00001883
Iteration 86/1000 | Loss: 0.00001883
Iteration 87/1000 | Loss: 0.00001883
Iteration 88/1000 | Loss: 0.00001883
Iteration 89/1000 | Loss: 0.00001883
Iteration 90/1000 | Loss: 0.00001883
Iteration 91/1000 | Loss: 0.00001882
Iteration 92/1000 | Loss: 0.00001882
Iteration 93/1000 | Loss: 0.00001882
Iteration 94/1000 | Loss: 0.00001882
Iteration 95/1000 | Loss: 0.00001882
Iteration 96/1000 | Loss: 0.00001882
Iteration 97/1000 | Loss: 0.00001882
Iteration 98/1000 | Loss: 0.00001882
Iteration 99/1000 | Loss: 0.00001882
Iteration 100/1000 | Loss: 0.00001882
Iteration 101/1000 | Loss: 0.00001882
Iteration 102/1000 | Loss: 0.00001882
Iteration 103/1000 | Loss: 0.00001882
Iteration 104/1000 | Loss: 0.00001882
Iteration 105/1000 | Loss: 0.00001882
Iteration 106/1000 | Loss: 0.00001882
Iteration 107/1000 | Loss: 0.00001882
Iteration 108/1000 | Loss: 0.00001882
Iteration 109/1000 | Loss: 0.00001882
Iteration 110/1000 | Loss: 0.00001882
Iteration 111/1000 | Loss: 0.00001882
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 111. Stopping optimization.
Last 5 losses: [1.8823580830940045e-05, 1.8823580830940045e-05, 1.8823580830940045e-05, 1.8823580830940045e-05, 1.8823580830940045e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8823580830940045e-05

Optimization complete. Final v2v error: 3.7297892570495605 mm

Highest mean error: 4.148074626922607 mm for frame 153

Lowest mean error: 3.381314992904663 mm for frame 167

Saving results

Total time: 92.16039729118347
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_025/1071/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_025/1071.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_025/1071
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00443803
Iteration 2/25 | Loss: 0.00129089
Iteration 3/25 | Loss: 0.00120316
Iteration 4/25 | Loss: 0.00119318
Iteration 5/25 | Loss: 0.00119093
Iteration 6/25 | Loss: 0.00119093
Iteration 7/25 | Loss: 0.00119093
Iteration 8/25 | Loss: 0.00119093
Iteration 9/25 | Loss: 0.00119093
Iteration 10/25 | Loss: 0.00119093
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0011909296736121178, 0.0011909296736121178, 0.0011909296736121178, 0.0011909296736121178, 0.0011909296736121178]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011909296736121178

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37240028
Iteration 2/25 | Loss: 0.00086033
Iteration 3/25 | Loss: 0.00086033
Iteration 4/25 | Loss: 0.00086033
Iteration 5/25 | Loss: 0.00086033
Iteration 6/25 | Loss: 0.00086033
Iteration 7/25 | Loss: 0.00086033
Iteration 8/25 | Loss: 0.00086033
Iteration 9/25 | Loss: 0.00086033
Iteration 10/25 | Loss: 0.00086032
Iteration 11/25 | Loss: 0.00086032
Iteration 12/25 | Loss: 0.00086032
Iteration 13/25 | Loss: 0.00086032
Iteration 14/25 | Loss: 0.00086032
Iteration 15/25 | Loss: 0.00086032
Iteration 16/25 | Loss: 0.00086032
Iteration 17/25 | Loss: 0.00086032
Iteration 18/25 | Loss: 0.00086032
Iteration 19/25 | Loss: 0.00086032
Iteration 20/25 | Loss: 0.00086032
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0008603243040852249, 0.0008603243040852249, 0.0008603243040852249, 0.0008603243040852249, 0.0008603243040852249]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008603243040852249

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00086032
Iteration 2/1000 | Loss: 0.00002654
Iteration 3/1000 | Loss: 0.00001946
Iteration 4/1000 | Loss: 0.00001779
Iteration 5/1000 | Loss: 0.00001707
Iteration 6/1000 | Loss: 0.00001645
Iteration 7/1000 | Loss: 0.00001611
Iteration 8/1000 | Loss: 0.00001588
Iteration 9/1000 | Loss: 0.00001574
Iteration 10/1000 | Loss: 0.00001558
Iteration 11/1000 | Loss: 0.00001552
Iteration 12/1000 | Loss: 0.00001547
Iteration 13/1000 | Loss: 0.00001547
Iteration 14/1000 | Loss: 0.00001546
Iteration 15/1000 | Loss: 0.00001540
Iteration 16/1000 | Loss: 0.00001539
Iteration 17/1000 | Loss: 0.00001535
Iteration 18/1000 | Loss: 0.00001535
Iteration 19/1000 | Loss: 0.00001535
Iteration 20/1000 | Loss: 0.00001535
Iteration 21/1000 | Loss: 0.00001535
Iteration 22/1000 | Loss: 0.00001530
Iteration 23/1000 | Loss: 0.00001529
Iteration 24/1000 | Loss: 0.00001526
Iteration 25/1000 | Loss: 0.00001526
Iteration 26/1000 | Loss: 0.00001526
Iteration 27/1000 | Loss: 0.00001525
Iteration 28/1000 | Loss: 0.00001524
Iteration 29/1000 | Loss: 0.00001523
Iteration 30/1000 | Loss: 0.00001523
Iteration 31/1000 | Loss: 0.00001521
Iteration 32/1000 | Loss: 0.00001521
Iteration 33/1000 | Loss: 0.00001520
Iteration 34/1000 | Loss: 0.00001520
Iteration 35/1000 | Loss: 0.00001520
Iteration 36/1000 | Loss: 0.00001519
Iteration 37/1000 | Loss: 0.00001518
Iteration 38/1000 | Loss: 0.00001517
Iteration 39/1000 | Loss: 0.00001517
Iteration 40/1000 | Loss: 0.00001515
Iteration 41/1000 | Loss: 0.00001514
Iteration 42/1000 | Loss: 0.00001514
Iteration 43/1000 | Loss: 0.00001513
Iteration 44/1000 | Loss: 0.00001513
Iteration 45/1000 | Loss: 0.00001512
Iteration 46/1000 | Loss: 0.00001511
Iteration 47/1000 | Loss: 0.00001511
Iteration 48/1000 | Loss: 0.00001510
Iteration 49/1000 | Loss: 0.00001510
Iteration 50/1000 | Loss: 0.00001509
Iteration 51/1000 | Loss: 0.00001508
Iteration 52/1000 | Loss: 0.00001508
Iteration 53/1000 | Loss: 0.00001508
Iteration 54/1000 | Loss: 0.00001507
Iteration 55/1000 | Loss: 0.00001507
Iteration 56/1000 | Loss: 0.00001507
Iteration 57/1000 | Loss: 0.00001505
Iteration 58/1000 | Loss: 0.00001505
Iteration 59/1000 | Loss: 0.00001504
Iteration 60/1000 | Loss: 0.00001504
Iteration 61/1000 | Loss: 0.00001504
Iteration 62/1000 | Loss: 0.00001503
Iteration 63/1000 | Loss: 0.00001503
Iteration 64/1000 | Loss: 0.00001502
Iteration 65/1000 | Loss: 0.00001502
Iteration 66/1000 | Loss: 0.00001501
Iteration 67/1000 | Loss: 0.00001497
Iteration 68/1000 | Loss: 0.00001497
Iteration 69/1000 | Loss: 0.00001497
Iteration 70/1000 | Loss: 0.00001497
Iteration 71/1000 | Loss: 0.00001497
Iteration 72/1000 | Loss: 0.00001496
Iteration 73/1000 | Loss: 0.00001496
Iteration 74/1000 | Loss: 0.00001496
Iteration 75/1000 | Loss: 0.00001496
Iteration 76/1000 | Loss: 0.00001496
Iteration 77/1000 | Loss: 0.00001496
Iteration 78/1000 | Loss: 0.00001496
Iteration 79/1000 | Loss: 0.00001494
Iteration 80/1000 | Loss: 0.00001494
Iteration 81/1000 | Loss: 0.00001493
Iteration 82/1000 | Loss: 0.00001493
Iteration 83/1000 | Loss: 0.00001493
Iteration 84/1000 | Loss: 0.00001493
Iteration 85/1000 | Loss: 0.00001493
Iteration 86/1000 | Loss: 0.00001493
Iteration 87/1000 | Loss: 0.00001493
Iteration 88/1000 | Loss: 0.00001493
Iteration 89/1000 | Loss: 0.00001493
Iteration 90/1000 | Loss: 0.00001493
Iteration 91/1000 | Loss: 0.00001493
Iteration 92/1000 | Loss: 0.00001493
Iteration 93/1000 | Loss: 0.00001493
Iteration 94/1000 | Loss: 0.00001492
Iteration 95/1000 | Loss: 0.00001492
Iteration 96/1000 | Loss: 0.00001491
Iteration 97/1000 | Loss: 0.00001491
Iteration 98/1000 | Loss: 0.00001491
Iteration 99/1000 | Loss: 0.00001490
Iteration 100/1000 | Loss: 0.00001490
Iteration 101/1000 | Loss: 0.00001490
Iteration 102/1000 | Loss: 0.00001490
Iteration 103/1000 | Loss: 0.00001490
Iteration 104/1000 | Loss: 0.00001490
Iteration 105/1000 | Loss: 0.00001489
Iteration 106/1000 | Loss: 0.00001489
Iteration 107/1000 | Loss: 0.00001489
Iteration 108/1000 | Loss: 0.00001489
Iteration 109/1000 | Loss: 0.00001488
Iteration 110/1000 | Loss: 0.00001488
Iteration 111/1000 | Loss: 0.00001488
Iteration 112/1000 | Loss: 0.00001488
Iteration 113/1000 | Loss: 0.00001487
Iteration 114/1000 | Loss: 0.00001487
Iteration 115/1000 | Loss: 0.00001486
Iteration 116/1000 | Loss: 0.00001486
Iteration 117/1000 | Loss: 0.00001486
Iteration 118/1000 | Loss: 0.00001486
Iteration 119/1000 | Loss: 0.00001486
Iteration 120/1000 | Loss: 0.00001485
Iteration 121/1000 | Loss: 0.00001485
Iteration 122/1000 | Loss: 0.00001485
Iteration 123/1000 | Loss: 0.00001485
Iteration 124/1000 | Loss: 0.00001485
Iteration 125/1000 | Loss: 0.00001485
Iteration 126/1000 | Loss: 0.00001484
Iteration 127/1000 | Loss: 0.00001484
Iteration 128/1000 | Loss: 0.00001484
Iteration 129/1000 | Loss: 0.00001484
Iteration 130/1000 | Loss: 0.00001483
Iteration 131/1000 | Loss: 0.00001483
Iteration 132/1000 | Loss: 0.00001483
Iteration 133/1000 | Loss: 0.00001483
Iteration 134/1000 | Loss: 0.00001483
Iteration 135/1000 | Loss: 0.00001483
Iteration 136/1000 | Loss: 0.00001483
Iteration 137/1000 | Loss: 0.00001483
Iteration 138/1000 | Loss: 0.00001483
Iteration 139/1000 | Loss: 0.00001483
Iteration 140/1000 | Loss: 0.00001483
Iteration 141/1000 | Loss: 0.00001482
Iteration 142/1000 | Loss: 0.00001482
Iteration 143/1000 | Loss: 0.00001482
Iteration 144/1000 | Loss: 0.00001482
Iteration 145/1000 | Loss: 0.00001482
Iteration 146/1000 | Loss: 0.00001482
Iteration 147/1000 | Loss: 0.00001482
Iteration 148/1000 | Loss: 0.00001482
Iteration 149/1000 | Loss: 0.00001481
Iteration 150/1000 | Loss: 0.00001481
Iteration 151/1000 | Loss: 0.00001481
Iteration 152/1000 | Loss: 0.00001481
Iteration 153/1000 | Loss: 0.00001481
Iteration 154/1000 | Loss: 0.00001481
Iteration 155/1000 | Loss: 0.00001480
Iteration 156/1000 | Loss: 0.00001480
Iteration 157/1000 | Loss: 0.00001480
Iteration 158/1000 | Loss: 0.00001480
Iteration 159/1000 | Loss: 0.00001480
Iteration 160/1000 | Loss: 0.00001480
Iteration 161/1000 | Loss: 0.00001480
Iteration 162/1000 | Loss: 0.00001480
Iteration 163/1000 | Loss: 0.00001480
Iteration 164/1000 | Loss: 0.00001479
Iteration 165/1000 | Loss: 0.00001479
Iteration 166/1000 | Loss: 0.00001479
Iteration 167/1000 | Loss: 0.00001479
Iteration 168/1000 | Loss: 0.00001479
Iteration 169/1000 | Loss: 0.00001478
Iteration 170/1000 | Loss: 0.00001478
Iteration 171/1000 | Loss: 0.00001477
Iteration 172/1000 | Loss: 0.00001477
Iteration 173/1000 | Loss: 0.00001477
Iteration 174/1000 | Loss: 0.00001477
Iteration 175/1000 | Loss: 0.00001477
Iteration 176/1000 | Loss: 0.00001477
Iteration 177/1000 | Loss: 0.00001476
Iteration 178/1000 | Loss: 0.00001476
Iteration 179/1000 | Loss: 0.00001476
Iteration 180/1000 | Loss: 0.00001476
Iteration 181/1000 | Loss: 0.00001476
Iteration 182/1000 | Loss: 0.00001476
Iteration 183/1000 | Loss: 0.00001476
Iteration 184/1000 | Loss: 0.00001476
Iteration 185/1000 | Loss: 0.00001476
Iteration 186/1000 | Loss: 0.00001475
Iteration 187/1000 | Loss: 0.00001475
Iteration 188/1000 | Loss: 0.00001475
Iteration 189/1000 | Loss: 0.00001475
Iteration 190/1000 | Loss: 0.00001475
Iteration 191/1000 | Loss: 0.00001475
Iteration 192/1000 | Loss: 0.00001475
Iteration 193/1000 | Loss: 0.00001475
Iteration 194/1000 | Loss: 0.00001474
Iteration 195/1000 | Loss: 0.00001474
Iteration 196/1000 | Loss: 0.00001474
Iteration 197/1000 | Loss: 0.00001474
Iteration 198/1000 | Loss: 0.00001474
Iteration 199/1000 | Loss: 0.00001474
Iteration 200/1000 | Loss: 0.00001474
Iteration 201/1000 | Loss: 0.00001474
Iteration 202/1000 | Loss: 0.00001474
Iteration 203/1000 | Loss: 0.00001474
Iteration 204/1000 | Loss: 0.00001474
Iteration 205/1000 | Loss: 0.00001474
Iteration 206/1000 | Loss: 0.00001474
Iteration 207/1000 | Loss: 0.00001474
Iteration 208/1000 | Loss: 0.00001474
Iteration 209/1000 | Loss: 0.00001474
Iteration 210/1000 | Loss: 0.00001474
Iteration 211/1000 | Loss: 0.00001474
Iteration 212/1000 | Loss: 0.00001474
Iteration 213/1000 | Loss: 0.00001474
Iteration 214/1000 | Loss: 0.00001474
Iteration 215/1000 | Loss: 0.00001474
Iteration 216/1000 | Loss: 0.00001474
Iteration 217/1000 | Loss: 0.00001474
Iteration 218/1000 | Loss: 0.00001474
Iteration 219/1000 | Loss: 0.00001474
Iteration 220/1000 | Loss: 0.00001474
Iteration 221/1000 | Loss: 0.00001474
Iteration 222/1000 | Loss: 0.00001474
Iteration 223/1000 | Loss: 0.00001474
Iteration 224/1000 | Loss: 0.00001474
Iteration 225/1000 | Loss: 0.00001474
Iteration 226/1000 | Loss: 0.00001474
Iteration 227/1000 | Loss: 0.00001474
Iteration 228/1000 | Loss: 0.00001474
Iteration 229/1000 | Loss: 0.00001474
Iteration 230/1000 | Loss: 0.00001474
Iteration 231/1000 | Loss: 0.00001474
Iteration 232/1000 | Loss: 0.00001474
Iteration 233/1000 | Loss: 0.00001474
Iteration 234/1000 | Loss: 0.00001474
Iteration 235/1000 | Loss: 0.00001474
Iteration 236/1000 | Loss: 0.00001474
Iteration 237/1000 | Loss: 0.00001474
Iteration 238/1000 | Loss: 0.00001474
Iteration 239/1000 | Loss: 0.00001474
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 239. Stopping optimization.
Last 5 losses: [1.4736070625076536e-05, 1.4736070625076536e-05, 1.4736070625076536e-05, 1.4736070625076536e-05, 1.4736070625076536e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4736070625076536e-05

Optimization complete. Final v2v error: 3.148380756378174 mm

Highest mean error: 3.5666637420654297 mm for frame 145

Lowest mean error: 2.7777397632598877 mm for frame 131

Saving results

Total time: 47.13283014297485
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_025/1085/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_025/1085.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_025/1085
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00604862
Iteration 2/25 | Loss: 0.00178602
Iteration 3/25 | Loss: 0.00133516
Iteration 4/25 | Loss: 0.00130918
Iteration 5/25 | Loss: 0.00130519
Iteration 6/25 | Loss: 0.00130334
Iteration 7/25 | Loss: 0.00130332
Iteration 8/25 | Loss: 0.00130332
Iteration 9/25 | Loss: 0.00130332
Iteration 10/25 | Loss: 0.00130332
Iteration 11/25 | Loss: 0.00130332
Iteration 12/25 | Loss: 0.00130332
Iteration 13/25 | Loss: 0.00130332
Iteration 14/25 | Loss: 0.00130332
Iteration 15/25 | Loss: 0.00130332
Iteration 16/25 | Loss: 0.00130332
Iteration 17/25 | Loss: 0.00130332
Iteration 18/25 | Loss: 0.00130332
Iteration 19/25 | Loss: 0.00130332
Iteration 20/25 | Loss: 0.00130332
Iteration 21/25 | Loss: 0.00130332
Iteration 22/25 | Loss: 0.00130332
Iteration 23/25 | Loss: 0.00130332
Iteration 24/25 | Loss: 0.00130332
Iteration 25/25 | Loss: 0.00130332

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.02607489
Iteration 2/25 | Loss: 0.00094614
Iteration 3/25 | Loss: 0.00094612
Iteration 4/25 | Loss: 0.00094612
Iteration 5/25 | Loss: 0.00094612
Iteration 6/25 | Loss: 0.00094612
Iteration 7/25 | Loss: 0.00094612
Iteration 8/25 | Loss: 0.00094612
Iteration 9/25 | Loss: 0.00094612
Iteration 10/25 | Loss: 0.00094612
Iteration 11/25 | Loss: 0.00094612
Iteration 12/25 | Loss: 0.00094612
Iteration 13/25 | Loss: 0.00094612
Iteration 14/25 | Loss: 0.00094612
Iteration 15/25 | Loss: 0.00094612
Iteration 16/25 | Loss: 0.00094612
Iteration 17/25 | Loss: 0.00094612
Iteration 18/25 | Loss: 0.00094612
Iteration 19/25 | Loss: 0.00094612
Iteration 20/25 | Loss: 0.00094612
Iteration 21/25 | Loss: 0.00094612
Iteration 22/25 | Loss: 0.00094612
Iteration 23/25 | Loss: 0.00094612
Iteration 24/25 | Loss: 0.00094612
Iteration 25/25 | Loss: 0.00094612

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00094612
Iteration 2/1000 | Loss: 0.00007370
Iteration 3/1000 | Loss: 0.00004997
Iteration 4/1000 | Loss: 0.00004199
Iteration 5/1000 | Loss: 0.00003916
Iteration 6/1000 | Loss: 0.00003776
Iteration 7/1000 | Loss: 0.00003700
Iteration 8/1000 | Loss: 0.00003625
Iteration 9/1000 | Loss: 0.00003548
Iteration 10/1000 | Loss: 0.00003507
Iteration 11/1000 | Loss: 0.00003465
Iteration 12/1000 | Loss: 0.00003422
Iteration 13/1000 | Loss: 0.00003387
Iteration 14/1000 | Loss: 0.00003349
Iteration 15/1000 | Loss: 0.00003318
Iteration 16/1000 | Loss: 0.00003293
Iteration 17/1000 | Loss: 0.00003276
Iteration 18/1000 | Loss: 0.00003256
Iteration 19/1000 | Loss: 0.00003256
Iteration 20/1000 | Loss: 0.00003238
Iteration 21/1000 | Loss: 0.00003238
Iteration 22/1000 | Loss: 0.00003238
Iteration 23/1000 | Loss: 0.00003238
Iteration 24/1000 | Loss: 0.00003238
Iteration 25/1000 | Loss: 0.00003226
Iteration 26/1000 | Loss: 0.00003226
Iteration 27/1000 | Loss: 0.00003220
Iteration 28/1000 | Loss: 0.00003214
Iteration 29/1000 | Loss: 0.00003211
Iteration 30/1000 | Loss: 0.00003211
Iteration 31/1000 | Loss: 0.00003211
Iteration 32/1000 | Loss: 0.00003210
Iteration 33/1000 | Loss: 0.00003210
Iteration 34/1000 | Loss: 0.00003210
Iteration 35/1000 | Loss: 0.00003208
Iteration 36/1000 | Loss: 0.00003208
Iteration 37/1000 | Loss: 0.00003207
Iteration 38/1000 | Loss: 0.00003207
Iteration 39/1000 | Loss: 0.00003207
Iteration 40/1000 | Loss: 0.00003207
Iteration 41/1000 | Loss: 0.00003207
Iteration 42/1000 | Loss: 0.00003206
Iteration 43/1000 | Loss: 0.00003206
Iteration 44/1000 | Loss: 0.00003206
Iteration 45/1000 | Loss: 0.00003205
Iteration 46/1000 | Loss: 0.00003205
Iteration 47/1000 | Loss: 0.00003205
Iteration 48/1000 | Loss: 0.00003205
Iteration 49/1000 | Loss: 0.00003204
Iteration 50/1000 | Loss: 0.00003204
Iteration 51/1000 | Loss: 0.00003204
Iteration 52/1000 | Loss: 0.00003203
Iteration 53/1000 | Loss: 0.00003203
Iteration 54/1000 | Loss: 0.00003202
Iteration 55/1000 | Loss: 0.00003202
Iteration 56/1000 | Loss: 0.00003202
Iteration 57/1000 | Loss: 0.00003202
Iteration 58/1000 | Loss: 0.00003202
Iteration 59/1000 | Loss: 0.00003201
Iteration 60/1000 | Loss: 0.00003201
Iteration 61/1000 | Loss: 0.00003201
Iteration 62/1000 | Loss: 0.00003201
Iteration 63/1000 | Loss: 0.00003201
Iteration 64/1000 | Loss: 0.00003201
Iteration 65/1000 | Loss: 0.00003200
Iteration 66/1000 | Loss: 0.00003200
Iteration 67/1000 | Loss: 0.00003200
Iteration 68/1000 | Loss: 0.00003200
Iteration 69/1000 | Loss: 0.00003200
Iteration 70/1000 | Loss: 0.00003200
Iteration 71/1000 | Loss: 0.00003200
Iteration 72/1000 | Loss: 0.00003199
Iteration 73/1000 | Loss: 0.00003199
Iteration 74/1000 | Loss: 0.00003199
Iteration 75/1000 | Loss: 0.00003199
Iteration 76/1000 | Loss: 0.00003199
Iteration 77/1000 | Loss: 0.00003199
Iteration 78/1000 | Loss: 0.00003199
Iteration 79/1000 | Loss: 0.00003199
Iteration 80/1000 | Loss: 0.00003199
Iteration 81/1000 | Loss: 0.00003199
Iteration 82/1000 | Loss: 0.00003199
Iteration 83/1000 | Loss: 0.00003199
Iteration 84/1000 | Loss: 0.00003199
Iteration 85/1000 | Loss: 0.00003199
Iteration 86/1000 | Loss: 0.00003199
Iteration 87/1000 | Loss: 0.00003199
Iteration 88/1000 | Loss: 0.00003199
Iteration 89/1000 | Loss: 0.00003199
Iteration 90/1000 | Loss: 0.00003199
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 90. Stopping optimization.
Last 5 losses: [3.199110142304562e-05, 3.199110142304562e-05, 3.199110142304562e-05, 3.199110142304562e-05, 3.199110142304562e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.199110142304562e-05

Optimization complete. Final v2v error: 4.2653889656066895 mm

Highest mean error: 5.361852645874023 mm for frame 149

Lowest mean error: 3.2026031017303467 mm for frame 49

Saving results

Total time: 45.69660806655884
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_025/1045/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_025/1045.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_025/1045
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00797756
Iteration 2/25 | Loss: 0.00135700
Iteration 3/25 | Loss: 0.00127040
Iteration 4/25 | Loss: 0.00124779
Iteration 5/25 | Loss: 0.00124090
Iteration 6/25 | Loss: 0.00123993
Iteration 7/25 | Loss: 0.00123993
Iteration 8/25 | Loss: 0.00123993
Iteration 9/25 | Loss: 0.00123993
Iteration 10/25 | Loss: 0.00123993
Iteration 11/25 | Loss: 0.00123993
Iteration 12/25 | Loss: 0.00123993
Iteration 13/25 | Loss: 0.00123993
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0012399289989843965, 0.0012399289989843965, 0.0012399289989843965, 0.0012399289989843965, 0.0012399289989843965]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012399289989843965

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.01441836
Iteration 2/25 | Loss: 0.00102781
Iteration 3/25 | Loss: 0.00102781
Iteration 4/25 | Loss: 0.00102781
Iteration 5/25 | Loss: 0.00102781
Iteration 6/25 | Loss: 0.00102781
Iteration 7/25 | Loss: 0.00102781
Iteration 8/25 | Loss: 0.00102781
Iteration 9/25 | Loss: 0.00102781
Iteration 10/25 | Loss: 0.00102781
Iteration 11/25 | Loss: 0.00102781
Iteration 12/25 | Loss: 0.00102781
Iteration 13/25 | Loss: 0.00102781
Iteration 14/25 | Loss: 0.00102781
Iteration 15/25 | Loss: 0.00102781
Iteration 16/25 | Loss: 0.00102781
Iteration 17/25 | Loss: 0.00102781
Iteration 18/25 | Loss: 0.00102781
Iteration 19/25 | Loss: 0.00102781
Iteration 20/25 | Loss: 0.00102781
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.001027807709760964, 0.001027807709760964, 0.001027807709760964, 0.001027807709760964, 0.001027807709760964]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001027807709760964

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00102781
Iteration 2/1000 | Loss: 0.00003909
Iteration 3/1000 | Loss: 0.00002908
Iteration 4/1000 | Loss: 0.00002693
Iteration 5/1000 | Loss: 0.00002617
Iteration 6/1000 | Loss: 0.00002563
Iteration 7/1000 | Loss: 0.00002532
Iteration 8/1000 | Loss: 0.00002519
Iteration 9/1000 | Loss: 0.00002488
Iteration 10/1000 | Loss: 0.00002467
Iteration 11/1000 | Loss: 0.00002454
Iteration 12/1000 | Loss: 0.00002444
Iteration 13/1000 | Loss: 0.00002436
Iteration 14/1000 | Loss: 0.00002428
Iteration 15/1000 | Loss: 0.00002426
Iteration 16/1000 | Loss: 0.00002421
Iteration 17/1000 | Loss: 0.00002417
Iteration 18/1000 | Loss: 0.00002416
Iteration 19/1000 | Loss: 0.00002416
Iteration 20/1000 | Loss: 0.00002415
Iteration 21/1000 | Loss: 0.00002414
Iteration 22/1000 | Loss: 0.00002413
Iteration 23/1000 | Loss: 0.00002413
Iteration 24/1000 | Loss: 0.00002412
Iteration 25/1000 | Loss: 0.00002412
Iteration 26/1000 | Loss: 0.00002411
Iteration 27/1000 | Loss: 0.00002411
Iteration 28/1000 | Loss: 0.00002411
Iteration 29/1000 | Loss: 0.00002410
Iteration 30/1000 | Loss: 0.00002409
Iteration 31/1000 | Loss: 0.00002409
Iteration 32/1000 | Loss: 0.00002408
Iteration 33/1000 | Loss: 0.00002407
Iteration 34/1000 | Loss: 0.00002407
Iteration 35/1000 | Loss: 0.00002407
Iteration 36/1000 | Loss: 0.00002406
Iteration 37/1000 | Loss: 0.00002406
Iteration 38/1000 | Loss: 0.00002405
Iteration 39/1000 | Loss: 0.00002405
Iteration 40/1000 | Loss: 0.00002402
Iteration 41/1000 | Loss: 0.00002402
Iteration 42/1000 | Loss: 0.00002402
Iteration 43/1000 | Loss: 0.00002402
Iteration 44/1000 | Loss: 0.00002401
Iteration 45/1000 | Loss: 0.00002401
Iteration 46/1000 | Loss: 0.00002401
Iteration 47/1000 | Loss: 0.00002400
Iteration 48/1000 | Loss: 0.00002400
Iteration 49/1000 | Loss: 0.00002399
Iteration 50/1000 | Loss: 0.00002399
Iteration 51/1000 | Loss: 0.00002399
Iteration 52/1000 | Loss: 0.00002398
Iteration 53/1000 | Loss: 0.00002398
Iteration 54/1000 | Loss: 0.00002398
Iteration 55/1000 | Loss: 0.00002397
Iteration 56/1000 | Loss: 0.00002397
Iteration 57/1000 | Loss: 0.00002397
Iteration 58/1000 | Loss: 0.00002396
Iteration 59/1000 | Loss: 0.00002396
Iteration 60/1000 | Loss: 0.00002396
Iteration 61/1000 | Loss: 0.00002396
Iteration 62/1000 | Loss: 0.00002396
Iteration 63/1000 | Loss: 0.00002395
Iteration 64/1000 | Loss: 0.00002395
Iteration 65/1000 | Loss: 0.00002395
Iteration 66/1000 | Loss: 0.00002395
Iteration 67/1000 | Loss: 0.00002395
Iteration 68/1000 | Loss: 0.00002394
Iteration 69/1000 | Loss: 0.00002394
Iteration 70/1000 | Loss: 0.00002394
Iteration 71/1000 | Loss: 0.00002393
Iteration 72/1000 | Loss: 0.00002393
Iteration 73/1000 | Loss: 0.00002393
Iteration 74/1000 | Loss: 0.00002393
Iteration 75/1000 | Loss: 0.00002392
Iteration 76/1000 | Loss: 0.00002392
Iteration 77/1000 | Loss: 0.00002392
Iteration 78/1000 | Loss: 0.00002392
Iteration 79/1000 | Loss: 0.00002392
Iteration 80/1000 | Loss: 0.00002392
Iteration 81/1000 | Loss: 0.00002392
Iteration 82/1000 | Loss: 0.00002392
Iteration 83/1000 | Loss: 0.00002392
Iteration 84/1000 | Loss: 0.00002392
Iteration 85/1000 | Loss: 0.00002392
Iteration 86/1000 | Loss: 0.00002392
Iteration 87/1000 | Loss: 0.00002392
Iteration 88/1000 | Loss: 0.00002392
Iteration 89/1000 | Loss: 0.00002392
Iteration 90/1000 | Loss: 0.00002392
Iteration 91/1000 | Loss: 0.00002392
Iteration 92/1000 | Loss: 0.00002392
Iteration 93/1000 | Loss: 0.00002392
Iteration 94/1000 | Loss: 0.00002392
Iteration 95/1000 | Loss: 0.00002392
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 95. Stopping optimization.
Last 5 losses: [2.3916414647828788e-05, 2.3916414647828788e-05, 2.3916414647828788e-05, 2.3916414647828788e-05, 2.3916414647828788e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3916414647828788e-05

Optimization complete. Final v2v error: 3.9427988529205322 mm

Highest mean error: 4.613356113433838 mm for frame 143

Lowest mean error: 3.70717191696167 mm for frame 121

Saving results

Total time: 38.09504532814026
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_025/1070/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_025/1070.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_025/1070
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00474365
Iteration 2/25 | Loss: 0.00133283
Iteration 3/25 | Loss: 0.00119010
Iteration 4/25 | Loss: 0.00116708
Iteration 5/25 | Loss: 0.00116134
Iteration 6/25 | Loss: 0.00116107
Iteration 7/25 | Loss: 0.00116107
Iteration 8/25 | Loss: 0.00116107
Iteration 9/25 | Loss: 0.00116107
Iteration 10/25 | Loss: 0.00116107
Iteration 11/25 | Loss: 0.00116107
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011610662331804633, 0.0011610662331804633, 0.0011610662331804633, 0.0011610662331804633, 0.0011610662331804633]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011610662331804633

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33047938
Iteration 2/25 | Loss: 0.00082747
Iteration 3/25 | Loss: 0.00082744
Iteration 4/25 | Loss: 0.00082744
Iteration 5/25 | Loss: 0.00082744
Iteration 6/25 | Loss: 0.00082744
Iteration 7/25 | Loss: 0.00082744
Iteration 8/25 | Loss: 0.00082744
Iteration 9/25 | Loss: 0.00082744
Iteration 10/25 | Loss: 0.00082744
Iteration 11/25 | Loss: 0.00082744
Iteration 12/25 | Loss: 0.00082744
Iteration 13/25 | Loss: 0.00082744
Iteration 14/25 | Loss: 0.00082744
Iteration 15/25 | Loss: 0.00082744
Iteration 16/25 | Loss: 0.00082744
Iteration 17/25 | Loss: 0.00082744
Iteration 18/25 | Loss: 0.00082744
Iteration 19/25 | Loss: 0.00082744
Iteration 20/25 | Loss: 0.00082744
Iteration 21/25 | Loss: 0.00082744
Iteration 22/25 | Loss: 0.00082744
Iteration 23/25 | Loss: 0.00082744
Iteration 24/25 | Loss: 0.00082744
Iteration 25/25 | Loss: 0.00082744

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00082744
Iteration 2/1000 | Loss: 0.00002222
Iteration 3/1000 | Loss: 0.00001574
Iteration 4/1000 | Loss: 0.00001416
Iteration 5/1000 | Loss: 0.00001313
Iteration 6/1000 | Loss: 0.00001253
Iteration 7/1000 | Loss: 0.00001226
Iteration 8/1000 | Loss: 0.00001207
Iteration 9/1000 | Loss: 0.00001186
Iteration 10/1000 | Loss: 0.00001185
Iteration 11/1000 | Loss: 0.00001172
Iteration 12/1000 | Loss: 0.00001171
Iteration 13/1000 | Loss: 0.00001165
Iteration 14/1000 | Loss: 0.00001164
Iteration 15/1000 | Loss: 0.00001162
Iteration 16/1000 | Loss: 0.00001162
Iteration 17/1000 | Loss: 0.00001162
Iteration 18/1000 | Loss: 0.00001162
Iteration 19/1000 | Loss: 0.00001162
Iteration 20/1000 | Loss: 0.00001161
Iteration 21/1000 | Loss: 0.00001161
Iteration 22/1000 | Loss: 0.00001159
Iteration 23/1000 | Loss: 0.00001156
Iteration 24/1000 | Loss: 0.00001155
Iteration 25/1000 | Loss: 0.00001155
Iteration 26/1000 | Loss: 0.00001154
Iteration 27/1000 | Loss: 0.00001153
Iteration 28/1000 | Loss: 0.00001148
Iteration 29/1000 | Loss: 0.00001148
Iteration 30/1000 | Loss: 0.00001145
Iteration 31/1000 | Loss: 0.00001145
Iteration 32/1000 | Loss: 0.00001145
Iteration 33/1000 | Loss: 0.00001145
Iteration 34/1000 | Loss: 0.00001144
Iteration 35/1000 | Loss: 0.00001144
Iteration 36/1000 | Loss: 0.00001144
Iteration 37/1000 | Loss: 0.00001144
Iteration 38/1000 | Loss: 0.00001144
Iteration 39/1000 | Loss: 0.00001144
Iteration 40/1000 | Loss: 0.00001144
Iteration 41/1000 | Loss: 0.00001144
Iteration 42/1000 | Loss: 0.00001144
Iteration 43/1000 | Loss: 0.00001144
Iteration 44/1000 | Loss: 0.00001144
Iteration 45/1000 | Loss: 0.00001143
Iteration 46/1000 | Loss: 0.00001143
Iteration 47/1000 | Loss: 0.00001142
Iteration 48/1000 | Loss: 0.00001141
Iteration 49/1000 | Loss: 0.00001140
Iteration 50/1000 | Loss: 0.00001140
Iteration 51/1000 | Loss: 0.00001140
Iteration 52/1000 | Loss: 0.00001140
Iteration 53/1000 | Loss: 0.00001140
Iteration 54/1000 | Loss: 0.00001140
Iteration 55/1000 | Loss: 0.00001139
Iteration 56/1000 | Loss: 0.00001139
Iteration 57/1000 | Loss: 0.00001139
Iteration 58/1000 | Loss: 0.00001139
Iteration 59/1000 | Loss: 0.00001139
Iteration 60/1000 | Loss: 0.00001139
Iteration 61/1000 | Loss: 0.00001139
Iteration 62/1000 | Loss: 0.00001138
Iteration 63/1000 | Loss: 0.00001138
Iteration 64/1000 | Loss: 0.00001138
Iteration 65/1000 | Loss: 0.00001138
Iteration 66/1000 | Loss: 0.00001138
Iteration 67/1000 | Loss: 0.00001138
Iteration 68/1000 | Loss: 0.00001137
Iteration 69/1000 | Loss: 0.00001137
Iteration 70/1000 | Loss: 0.00001137
Iteration 71/1000 | Loss: 0.00001137
Iteration 72/1000 | Loss: 0.00001137
Iteration 73/1000 | Loss: 0.00001137
Iteration 74/1000 | Loss: 0.00001137
Iteration 75/1000 | Loss: 0.00001137
Iteration 76/1000 | Loss: 0.00001137
Iteration 77/1000 | Loss: 0.00001137
Iteration 78/1000 | Loss: 0.00001136
Iteration 79/1000 | Loss: 0.00001136
Iteration 80/1000 | Loss: 0.00001136
Iteration 81/1000 | Loss: 0.00001136
Iteration 82/1000 | Loss: 0.00001136
Iteration 83/1000 | Loss: 0.00001136
Iteration 84/1000 | Loss: 0.00001136
Iteration 85/1000 | Loss: 0.00001136
Iteration 86/1000 | Loss: 0.00001136
Iteration 87/1000 | Loss: 0.00001136
Iteration 88/1000 | Loss: 0.00001136
Iteration 89/1000 | Loss: 0.00001136
Iteration 90/1000 | Loss: 0.00001136
Iteration 91/1000 | Loss: 0.00001136
Iteration 92/1000 | Loss: 0.00001136
Iteration 93/1000 | Loss: 0.00001136
Iteration 94/1000 | Loss: 0.00001136
Iteration 95/1000 | Loss: 0.00001136
Iteration 96/1000 | Loss: 0.00001136
Iteration 97/1000 | Loss: 0.00001136
Iteration 98/1000 | Loss: 0.00001136
Iteration 99/1000 | Loss: 0.00001136
Iteration 100/1000 | Loss: 0.00001136
Iteration 101/1000 | Loss: 0.00001136
Iteration 102/1000 | Loss: 0.00001136
Iteration 103/1000 | Loss: 0.00001136
Iteration 104/1000 | Loss: 0.00001136
Iteration 105/1000 | Loss: 0.00001136
Iteration 106/1000 | Loss: 0.00001136
Iteration 107/1000 | Loss: 0.00001136
Iteration 108/1000 | Loss: 0.00001136
Iteration 109/1000 | Loss: 0.00001136
Iteration 110/1000 | Loss: 0.00001136
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 110. Stopping optimization.
Last 5 losses: [1.1355561582604423e-05, 1.1355561582604423e-05, 1.1355561582604423e-05, 1.1355561582604423e-05, 1.1355561582604423e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1355561582604423e-05

Optimization complete. Final v2v error: 2.8993942737579346 mm

Highest mean error: 3.030911445617676 mm for frame 44

Lowest mean error: 2.757941246032715 mm for frame 204

Saving results

Total time: 34.657933950424194
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_025/1049/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_025/1049.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_025/1049
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00523426
Iteration 2/25 | Loss: 0.00141820
Iteration 3/25 | Loss: 0.00121268
Iteration 4/25 | Loss: 0.00119468
Iteration 5/25 | Loss: 0.00119096
Iteration 6/25 | Loss: 0.00119038
Iteration 7/25 | Loss: 0.00119038
Iteration 8/25 | Loss: 0.00119038
Iteration 9/25 | Loss: 0.00119038
Iteration 10/25 | Loss: 0.00119038
Iteration 11/25 | Loss: 0.00119038
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011903754202648997, 0.0011903754202648997, 0.0011903754202648997, 0.0011903754202648997, 0.0011903754202648997]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011903754202648997

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34829152
Iteration 2/25 | Loss: 0.00065103
Iteration 3/25 | Loss: 0.00065101
Iteration 4/25 | Loss: 0.00065101
Iteration 5/25 | Loss: 0.00065101
Iteration 6/25 | Loss: 0.00065101
Iteration 7/25 | Loss: 0.00065101
Iteration 8/25 | Loss: 0.00065101
Iteration 9/25 | Loss: 0.00065101
Iteration 10/25 | Loss: 0.00065101
Iteration 11/25 | Loss: 0.00065101
Iteration 12/25 | Loss: 0.00065101
Iteration 13/25 | Loss: 0.00065101
Iteration 14/25 | Loss: 0.00065101
Iteration 15/25 | Loss: 0.00065101
Iteration 16/25 | Loss: 0.00065101
Iteration 17/25 | Loss: 0.00065101
Iteration 18/25 | Loss: 0.00065101
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0006510108942165971, 0.0006510108942165971, 0.0006510108942165971, 0.0006510108942165971, 0.0006510108942165971]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006510108942165971

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00065101
Iteration 2/1000 | Loss: 0.00002870
Iteration 3/1000 | Loss: 0.00002115
Iteration 4/1000 | Loss: 0.00001853
Iteration 5/1000 | Loss: 0.00001765
Iteration 6/1000 | Loss: 0.00001707
Iteration 7/1000 | Loss: 0.00001661
Iteration 8/1000 | Loss: 0.00001628
Iteration 9/1000 | Loss: 0.00001620
Iteration 10/1000 | Loss: 0.00001616
Iteration 11/1000 | Loss: 0.00001599
Iteration 12/1000 | Loss: 0.00001582
Iteration 13/1000 | Loss: 0.00001578
Iteration 14/1000 | Loss: 0.00001561
Iteration 15/1000 | Loss: 0.00001553
Iteration 16/1000 | Loss: 0.00001544
Iteration 17/1000 | Loss: 0.00001541
Iteration 18/1000 | Loss: 0.00001540
Iteration 19/1000 | Loss: 0.00001539
Iteration 20/1000 | Loss: 0.00001538
Iteration 21/1000 | Loss: 0.00001538
Iteration 22/1000 | Loss: 0.00001537
Iteration 23/1000 | Loss: 0.00001536
Iteration 24/1000 | Loss: 0.00001536
Iteration 25/1000 | Loss: 0.00001534
Iteration 26/1000 | Loss: 0.00001533
Iteration 27/1000 | Loss: 0.00001533
Iteration 28/1000 | Loss: 0.00001533
Iteration 29/1000 | Loss: 0.00001533
Iteration 30/1000 | Loss: 0.00001532
Iteration 31/1000 | Loss: 0.00001532
Iteration 32/1000 | Loss: 0.00001532
Iteration 33/1000 | Loss: 0.00001531
Iteration 34/1000 | Loss: 0.00001531
Iteration 35/1000 | Loss: 0.00001530
Iteration 36/1000 | Loss: 0.00001530
Iteration 37/1000 | Loss: 0.00001530
Iteration 38/1000 | Loss: 0.00001529
Iteration 39/1000 | Loss: 0.00001529
Iteration 40/1000 | Loss: 0.00001529
Iteration 41/1000 | Loss: 0.00001528
Iteration 42/1000 | Loss: 0.00001528
Iteration 43/1000 | Loss: 0.00001528
Iteration 44/1000 | Loss: 0.00001527
Iteration 45/1000 | Loss: 0.00001527
Iteration 46/1000 | Loss: 0.00001527
Iteration 47/1000 | Loss: 0.00001526
Iteration 48/1000 | Loss: 0.00001526
Iteration 49/1000 | Loss: 0.00001526
Iteration 50/1000 | Loss: 0.00001525
Iteration 51/1000 | Loss: 0.00001525
Iteration 52/1000 | Loss: 0.00001525
Iteration 53/1000 | Loss: 0.00001524
Iteration 54/1000 | Loss: 0.00001524
Iteration 55/1000 | Loss: 0.00001524
Iteration 56/1000 | Loss: 0.00001523
Iteration 57/1000 | Loss: 0.00001523
Iteration 58/1000 | Loss: 0.00001522
Iteration 59/1000 | Loss: 0.00001522
Iteration 60/1000 | Loss: 0.00001522
Iteration 61/1000 | Loss: 0.00001521
Iteration 62/1000 | Loss: 0.00001521
Iteration 63/1000 | Loss: 0.00001520
Iteration 64/1000 | Loss: 0.00001520
Iteration 65/1000 | Loss: 0.00001519
Iteration 66/1000 | Loss: 0.00001519
Iteration 67/1000 | Loss: 0.00001518
Iteration 68/1000 | Loss: 0.00001518
Iteration 69/1000 | Loss: 0.00001517
Iteration 70/1000 | Loss: 0.00001517
Iteration 71/1000 | Loss: 0.00001516
Iteration 72/1000 | Loss: 0.00001516
Iteration 73/1000 | Loss: 0.00001516
Iteration 74/1000 | Loss: 0.00001515
Iteration 75/1000 | Loss: 0.00001515
Iteration 76/1000 | Loss: 0.00001514
Iteration 77/1000 | Loss: 0.00001514
Iteration 78/1000 | Loss: 0.00001514
Iteration 79/1000 | Loss: 0.00001514
Iteration 80/1000 | Loss: 0.00001513
Iteration 81/1000 | Loss: 0.00001513
Iteration 82/1000 | Loss: 0.00001512
Iteration 83/1000 | Loss: 0.00001512
Iteration 84/1000 | Loss: 0.00001512
Iteration 85/1000 | Loss: 0.00001512
Iteration 86/1000 | Loss: 0.00001512
Iteration 87/1000 | Loss: 0.00001512
Iteration 88/1000 | Loss: 0.00001511
Iteration 89/1000 | Loss: 0.00001511
Iteration 90/1000 | Loss: 0.00001510
Iteration 91/1000 | Loss: 0.00001510
Iteration 92/1000 | Loss: 0.00001510
Iteration 93/1000 | Loss: 0.00001510
Iteration 94/1000 | Loss: 0.00001510
Iteration 95/1000 | Loss: 0.00001510
Iteration 96/1000 | Loss: 0.00001510
Iteration 97/1000 | Loss: 0.00001510
Iteration 98/1000 | Loss: 0.00001509
Iteration 99/1000 | Loss: 0.00001509
Iteration 100/1000 | Loss: 0.00001509
Iteration 101/1000 | Loss: 0.00001509
Iteration 102/1000 | Loss: 0.00001509
Iteration 103/1000 | Loss: 0.00001509
Iteration 104/1000 | Loss: 0.00001509
Iteration 105/1000 | Loss: 0.00001509
Iteration 106/1000 | Loss: 0.00001509
Iteration 107/1000 | Loss: 0.00001509
Iteration 108/1000 | Loss: 0.00001508
Iteration 109/1000 | Loss: 0.00001508
Iteration 110/1000 | Loss: 0.00001508
Iteration 111/1000 | Loss: 0.00001508
Iteration 112/1000 | Loss: 0.00001507
Iteration 113/1000 | Loss: 0.00001507
Iteration 114/1000 | Loss: 0.00001507
Iteration 115/1000 | Loss: 0.00001507
Iteration 116/1000 | Loss: 0.00001507
Iteration 117/1000 | Loss: 0.00001507
Iteration 118/1000 | Loss: 0.00001507
Iteration 119/1000 | Loss: 0.00001507
Iteration 120/1000 | Loss: 0.00001506
Iteration 121/1000 | Loss: 0.00001506
Iteration 122/1000 | Loss: 0.00001506
Iteration 123/1000 | Loss: 0.00001506
Iteration 124/1000 | Loss: 0.00001506
Iteration 125/1000 | Loss: 0.00001505
Iteration 126/1000 | Loss: 0.00001505
Iteration 127/1000 | Loss: 0.00001505
Iteration 128/1000 | Loss: 0.00001505
Iteration 129/1000 | Loss: 0.00001505
Iteration 130/1000 | Loss: 0.00001505
Iteration 131/1000 | Loss: 0.00001505
Iteration 132/1000 | Loss: 0.00001505
Iteration 133/1000 | Loss: 0.00001505
Iteration 134/1000 | Loss: 0.00001505
Iteration 135/1000 | Loss: 0.00001505
Iteration 136/1000 | Loss: 0.00001505
Iteration 137/1000 | Loss: 0.00001505
Iteration 138/1000 | Loss: 0.00001505
Iteration 139/1000 | Loss: 0.00001505
Iteration 140/1000 | Loss: 0.00001504
Iteration 141/1000 | Loss: 0.00001504
Iteration 142/1000 | Loss: 0.00001504
Iteration 143/1000 | Loss: 0.00001504
Iteration 144/1000 | Loss: 0.00001504
Iteration 145/1000 | Loss: 0.00001504
Iteration 146/1000 | Loss: 0.00001504
Iteration 147/1000 | Loss: 0.00001504
Iteration 148/1000 | Loss: 0.00001504
Iteration 149/1000 | Loss: 0.00001504
Iteration 150/1000 | Loss: 0.00001504
Iteration 151/1000 | Loss: 0.00001504
Iteration 152/1000 | Loss: 0.00001503
Iteration 153/1000 | Loss: 0.00001503
Iteration 154/1000 | Loss: 0.00001503
Iteration 155/1000 | Loss: 0.00001503
Iteration 156/1000 | Loss: 0.00001503
Iteration 157/1000 | Loss: 0.00001503
Iteration 158/1000 | Loss: 0.00001503
Iteration 159/1000 | Loss: 0.00001503
Iteration 160/1000 | Loss: 0.00001503
Iteration 161/1000 | Loss: 0.00001503
Iteration 162/1000 | Loss: 0.00001503
Iteration 163/1000 | Loss: 0.00001503
Iteration 164/1000 | Loss: 0.00001503
Iteration 165/1000 | Loss: 0.00001503
Iteration 166/1000 | Loss: 0.00001503
Iteration 167/1000 | Loss: 0.00001502
Iteration 168/1000 | Loss: 0.00001502
Iteration 169/1000 | Loss: 0.00001502
Iteration 170/1000 | Loss: 0.00001502
Iteration 171/1000 | Loss: 0.00001502
Iteration 172/1000 | Loss: 0.00001502
Iteration 173/1000 | Loss: 0.00001501
Iteration 174/1000 | Loss: 0.00001501
Iteration 175/1000 | Loss: 0.00001501
Iteration 176/1000 | Loss: 0.00001501
Iteration 177/1000 | Loss: 0.00001501
Iteration 178/1000 | Loss: 0.00001501
Iteration 179/1000 | Loss: 0.00001501
Iteration 180/1000 | Loss: 0.00001501
Iteration 181/1000 | Loss: 0.00001500
Iteration 182/1000 | Loss: 0.00001500
Iteration 183/1000 | Loss: 0.00001500
Iteration 184/1000 | Loss: 0.00001500
Iteration 185/1000 | Loss: 0.00001500
Iteration 186/1000 | Loss: 0.00001500
Iteration 187/1000 | Loss: 0.00001500
Iteration 188/1000 | Loss: 0.00001500
Iteration 189/1000 | Loss: 0.00001500
Iteration 190/1000 | Loss: 0.00001499
Iteration 191/1000 | Loss: 0.00001499
Iteration 192/1000 | Loss: 0.00001499
Iteration 193/1000 | Loss: 0.00001499
Iteration 194/1000 | Loss: 0.00001499
Iteration 195/1000 | Loss: 0.00001499
Iteration 196/1000 | Loss: 0.00001499
Iteration 197/1000 | Loss: 0.00001499
Iteration 198/1000 | Loss: 0.00001499
Iteration 199/1000 | Loss: 0.00001499
Iteration 200/1000 | Loss: 0.00001499
Iteration 201/1000 | Loss: 0.00001499
Iteration 202/1000 | Loss: 0.00001499
Iteration 203/1000 | Loss: 0.00001499
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 203. Stopping optimization.
Last 5 losses: [1.4990627278166357e-05, 1.4990627278166357e-05, 1.4990627278166357e-05, 1.4990627278166357e-05, 1.4990627278166357e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4990627278166357e-05

Optimization complete. Final v2v error: 3.216939926147461 mm

Highest mean error: 3.6693434715270996 mm for frame 48

Lowest mean error: 2.7923479080200195 mm for frame 104

Saving results

Total time: 40.88650035858154
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_025/1068/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_025/1068.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_025/1068
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00841110
Iteration 2/25 | Loss: 0.00159321
Iteration 3/25 | Loss: 0.00133415
Iteration 4/25 | Loss: 0.00129679
Iteration 5/25 | Loss: 0.00127894
Iteration 6/25 | Loss: 0.00130482
Iteration 7/25 | Loss: 0.00123651
Iteration 8/25 | Loss: 0.00122054
Iteration 9/25 | Loss: 0.00120901
Iteration 10/25 | Loss: 0.00120838
Iteration 11/25 | Loss: 0.00120231
Iteration 12/25 | Loss: 0.00120616
Iteration 13/25 | Loss: 0.00120013
Iteration 14/25 | Loss: 0.00118662
Iteration 15/25 | Loss: 0.00118834
Iteration 16/25 | Loss: 0.00117921
Iteration 17/25 | Loss: 0.00117763
Iteration 18/25 | Loss: 0.00117749
Iteration 19/25 | Loss: 0.00117748
Iteration 20/25 | Loss: 0.00117747
Iteration 21/25 | Loss: 0.00117747
Iteration 22/25 | Loss: 0.00117747
Iteration 23/25 | Loss: 0.00117747
Iteration 24/25 | Loss: 0.00117747
Iteration 25/25 | Loss: 0.00117747

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.92962891
Iteration 2/25 | Loss: 0.00049928
Iteration 3/25 | Loss: 0.00049927
Iteration 4/25 | Loss: 0.00049927
Iteration 5/25 | Loss: 0.00049927
Iteration 6/25 | Loss: 0.00049927
Iteration 7/25 | Loss: 0.00049927
Iteration 8/25 | Loss: 0.00049927
Iteration 9/25 | Loss: 0.00049927
Iteration 10/25 | Loss: 0.00049927
Iteration 11/25 | Loss: 0.00049927
Iteration 12/25 | Loss: 0.00049927
Iteration 13/25 | Loss: 0.00049927
Iteration 14/25 | Loss: 0.00049927
Iteration 15/25 | Loss: 0.00049927
Iteration 16/25 | Loss: 0.00049927
Iteration 17/25 | Loss: 0.00049927
Iteration 18/25 | Loss: 0.00049927
Iteration 19/25 | Loss: 0.00049927
Iteration 20/25 | Loss: 0.00049927
Iteration 21/25 | Loss: 0.00049927
Iteration 22/25 | Loss: 0.00049927
Iteration 23/25 | Loss: 0.00049927
Iteration 24/25 | Loss: 0.00049927
Iteration 25/25 | Loss: 0.00049927

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00049927
Iteration 2/1000 | Loss: 0.00003819
Iteration 3/1000 | Loss: 0.00002857
Iteration 4/1000 | Loss: 0.00002549
Iteration 5/1000 | Loss: 0.00002439
Iteration 6/1000 | Loss: 0.00002332
Iteration 7/1000 | Loss: 0.00002264
Iteration 8/1000 | Loss: 0.00002214
Iteration 9/1000 | Loss: 0.00002177
Iteration 10/1000 | Loss: 0.00002139
Iteration 11/1000 | Loss: 0.00042173
Iteration 12/1000 | Loss: 0.00002629
Iteration 13/1000 | Loss: 0.00002219
Iteration 14/1000 | Loss: 0.00002057
Iteration 15/1000 | Loss: 0.00001959
Iteration 16/1000 | Loss: 0.00001893
Iteration 17/1000 | Loss: 0.00001867
Iteration 18/1000 | Loss: 0.00001862
Iteration 19/1000 | Loss: 0.00001855
Iteration 20/1000 | Loss: 0.00001851
Iteration 21/1000 | Loss: 0.00001845
Iteration 22/1000 | Loss: 0.00001844
Iteration 23/1000 | Loss: 0.00001843
Iteration 24/1000 | Loss: 0.00001841
Iteration 25/1000 | Loss: 0.00001841
Iteration 26/1000 | Loss: 0.00001841
Iteration 27/1000 | Loss: 0.00001841
Iteration 28/1000 | Loss: 0.00001841
Iteration 29/1000 | Loss: 0.00001841
Iteration 30/1000 | Loss: 0.00001841
Iteration 31/1000 | Loss: 0.00001841
Iteration 32/1000 | Loss: 0.00001841
Iteration 33/1000 | Loss: 0.00001841
Iteration 34/1000 | Loss: 0.00001841
Iteration 35/1000 | Loss: 0.00001840
Iteration 36/1000 | Loss: 0.00001840
Iteration 37/1000 | Loss: 0.00001840
Iteration 38/1000 | Loss: 0.00001839
Iteration 39/1000 | Loss: 0.00001839
Iteration 40/1000 | Loss: 0.00001839
Iteration 41/1000 | Loss: 0.00001838
Iteration 42/1000 | Loss: 0.00001838
Iteration 43/1000 | Loss: 0.00001838
Iteration 44/1000 | Loss: 0.00001838
Iteration 45/1000 | Loss: 0.00001838
Iteration 46/1000 | Loss: 0.00001838
Iteration 47/1000 | Loss: 0.00001837
Iteration 48/1000 | Loss: 0.00001837
Iteration 49/1000 | Loss: 0.00001837
Iteration 50/1000 | Loss: 0.00001837
Iteration 51/1000 | Loss: 0.00001837
Iteration 52/1000 | Loss: 0.00001837
Iteration 53/1000 | Loss: 0.00001837
Iteration 54/1000 | Loss: 0.00001837
Iteration 55/1000 | Loss: 0.00001836
Iteration 56/1000 | Loss: 0.00001836
Iteration 57/1000 | Loss: 0.00001835
Iteration 58/1000 | Loss: 0.00001835
Iteration 59/1000 | Loss: 0.00001835
Iteration 60/1000 | Loss: 0.00001834
Iteration 61/1000 | Loss: 0.00001834
Iteration 62/1000 | Loss: 0.00001834
Iteration 63/1000 | Loss: 0.00001833
Iteration 64/1000 | Loss: 0.00001833
Iteration 65/1000 | Loss: 0.00001833
Iteration 66/1000 | Loss: 0.00001833
Iteration 67/1000 | Loss: 0.00001832
Iteration 68/1000 | Loss: 0.00001832
Iteration 69/1000 | Loss: 0.00001832
Iteration 70/1000 | Loss: 0.00001831
Iteration 71/1000 | Loss: 0.00001831
Iteration 72/1000 | Loss: 0.00001831
Iteration 73/1000 | Loss: 0.00001831
Iteration 74/1000 | Loss: 0.00001831
Iteration 75/1000 | Loss: 0.00001831
Iteration 76/1000 | Loss: 0.00001831
Iteration 77/1000 | Loss: 0.00001831
Iteration 78/1000 | Loss: 0.00001830
Iteration 79/1000 | Loss: 0.00001830
Iteration 80/1000 | Loss: 0.00001830
Iteration 81/1000 | Loss: 0.00001830
Iteration 82/1000 | Loss: 0.00001830
Iteration 83/1000 | Loss: 0.00001830
Iteration 84/1000 | Loss: 0.00001830
Iteration 85/1000 | Loss: 0.00001830
Iteration 86/1000 | Loss: 0.00001830
Iteration 87/1000 | Loss: 0.00001829
Iteration 88/1000 | Loss: 0.00001829
Iteration 89/1000 | Loss: 0.00001829
Iteration 90/1000 | Loss: 0.00001829
Iteration 91/1000 | Loss: 0.00001829
Iteration 92/1000 | Loss: 0.00001829
Iteration 93/1000 | Loss: 0.00001829
Iteration 94/1000 | Loss: 0.00001829
Iteration 95/1000 | Loss: 0.00001829
Iteration 96/1000 | Loss: 0.00001829
Iteration 97/1000 | Loss: 0.00001829
Iteration 98/1000 | Loss: 0.00001829
Iteration 99/1000 | Loss: 0.00001829
Iteration 100/1000 | Loss: 0.00001829
Iteration 101/1000 | Loss: 0.00001829
Iteration 102/1000 | Loss: 0.00001828
Iteration 103/1000 | Loss: 0.00001828
Iteration 104/1000 | Loss: 0.00001828
Iteration 105/1000 | Loss: 0.00001828
Iteration 106/1000 | Loss: 0.00001828
Iteration 107/1000 | Loss: 0.00001828
Iteration 108/1000 | Loss: 0.00001828
Iteration 109/1000 | Loss: 0.00001828
Iteration 110/1000 | Loss: 0.00001828
Iteration 111/1000 | Loss: 0.00001828
Iteration 112/1000 | Loss: 0.00001828
Iteration 113/1000 | Loss: 0.00001828
Iteration 114/1000 | Loss: 0.00001828
Iteration 115/1000 | Loss: 0.00001828
Iteration 116/1000 | Loss: 0.00001828
Iteration 117/1000 | Loss: 0.00001828
Iteration 118/1000 | Loss: 0.00001828
Iteration 119/1000 | Loss: 0.00001828
Iteration 120/1000 | Loss: 0.00001828
Iteration 121/1000 | Loss: 0.00001828
Iteration 122/1000 | Loss: 0.00001828
Iteration 123/1000 | Loss: 0.00001828
Iteration 124/1000 | Loss: 0.00001828
Iteration 125/1000 | Loss: 0.00001828
Iteration 126/1000 | Loss: 0.00001828
Iteration 127/1000 | Loss: 0.00001828
Iteration 128/1000 | Loss: 0.00001828
Iteration 129/1000 | Loss: 0.00001828
Iteration 130/1000 | Loss: 0.00001828
Iteration 131/1000 | Loss: 0.00001828
Iteration 132/1000 | Loss: 0.00001828
Iteration 133/1000 | Loss: 0.00001828
Iteration 134/1000 | Loss: 0.00001828
Iteration 135/1000 | Loss: 0.00001828
Iteration 136/1000 | Loss: 0.00001828
Iteration 137/1000 | Loss: 0.00001828
Iteration 138/1000 | Loss: 0.00001828
Iteration 139/1000 | Loss: 0.00001828
Iteration 140/1000 | Loss: 0.00001828
Iteration 141/1000 | Loss: 0.00001828
Iteration 142/1000 | Loss: 0.00001828
Iteration 143/1000 | Loss: 0.00001828
Iteration 144/1000 | Loss: 0.00001828
Iteration 145/1000 | Loss: 0.00001828
Iteration 146/1000 | Loss: 0.00001828
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 146. Stopping optimization.
Last 5 losses: [1.8282307792105712e-05, 1.8282307792105712e-05, 1.8282307792105712e-05, 1.8282307792105712e-05, 1.8282307792105712e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8282307792105712e-05

Optimization complete. Final v2v error: 3.604708194732666 mm

Highest mean error: 4.168747901916504 mm for frame 130

Lowest mean error: 3.479924440383911 mm for frame 0

Saving results

Total time: 67.8844051361084
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_025/1079/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_025/1079.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_025/1079
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00765580
Iteration 2/25 | Loss: 0.00141569
Iteration 3/25 | Loss: 0.00119988
Iteration 4/25 | Loss: 0.00115784
Iteration 5/25 | Loss: 0.00114818
Iteration 6/25 | Loss: 0.00114185
Iteration 7/25 | Loss: 0.00114046
Iteration 8/25 | Loss: 0.00113972
Iteration 9/25 | Loss: 0.00113958
Iteration 10/25 | Loss: 0.00113948
Iteration 11/25 | Loss: 0.00113948
Iteration 12/25 | Loss: 0.00113948
Iteration 13/25 | Loss: 0.00113947
Iteration 14/25 | Loss: 0.00113947
Iteration 15/25 | Loss: 0.00113947
Iteration 16/25 | Loss: 0.00113947
Iteration 17/25 | Loss: 0.00113947
Iteration 18/25 | Loss: 0.00113947
Iteration 19/25 | Loss: 0.00113947
Iteration 20/25 | Loss: 0.00113947
Iteration 21/25 | Loss: 0.00113947
Iteration 22/25 | Loss: 0.00113947
Iteration 23/25 | Loss: 0.00113946
Iteration 24/25 | Loss: 0.00113946
Iteration 25/25 | Loss: 0.00113946

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.68755555
Iteration 2/25 | Loss: 0.00080013
Iteration 3/25 | Loss: 0.00079986
Iteration 4/25 | Loss: 0.00079986
Iteration 5/25 | Loss: 0.00079986
Iteration 6/25 | Loss: 0.00079985
Iteration 7/25 | Loss: 0.00079985
Iteration 8/25 | Loss: 0.00079985
Iteration 9/25 | Loss: 0.00079985
Iteration 10/25 | Loss: 0.00079985
Iteration 11/25 | Loss: 0.00079985
Iteration 12/25 | Loss: 0.00079985
Iteration 13/25 | Loss: 0.00079985
Iteration 14/25 | Loss: 0.00079985
Iteration 15/25 | Loss: 0.00079985
Iteration 16/25 | Loss: 0.00079985
Iteration 17/25 | Loss: 0.00079985
Iteration 18/25 | Loss: 0.00079985
Iteration 19/25 | Loss: 0.00079985
Iteration 20/25 | Loss: 0.00079985
Iteration 21/25 | Loss: 0.00079985
Iteration 22/25 | Loss: 0.00079985
Iteration 23/25 | Loss: 0.00079985
Iteration 24/25 | Loss: 0.00079985
Iteration 25/25 | Loss: 0.00079985

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00079985
Iteration 2/1000 | Loss: 0.00002104
Iteration 3/1000 | Loss: 0.00001730
Iteration 4/1000 | Loss: 0.00001525
Iteration 5/1000 | Loss: 0.00001418
Iteration 6/1000 | Loss: 0.00001397
Iteration 7/1000 | Loss: 0.00001329
Iteration 8/1000 | Loss: 0.00001323
Iteration 9/1000 | Loss: 0.00001290
Iteration 10/1000 | Loss: 0.00001342
Iteration 11/1000 | Loss: 0.00001273
Iteration 12/1000 | Loss: 0.00001257
Iteration 13/1000 | Loss: 0.00001253
Iteration 14/1000 | Loss: 0.00001252
Iteration 15/1000 | Loss: 0.00001251
Iteration 16/1000 | Loss: 0.00001251
Iteration 17/1000 | Loss: 0.00001250
Iteration 18/1000 | Loss: 0.00001995
Iteration 19/1000 | Loss: 0.00001245
Iteration 20/1000 | Loss: 0.00001244
Iteration 21/1000 | Loss: 0.00001240
Iteration 22/1000 | Loss: 0.00001237
Iteration 23/1000 | Loss: 0.00001236
Iteration 24/1000 | Loss: 0.00001236
Iteration 25/1000 | Loss: 0.00001234
Iteration 26/1000 | Loss: 0.00001232
Iteration 27/1000 | Loss: 0.00001232
Iteration 28/1000 | Loss: 0.00001232
Iteration 29/1000 | Loss: 0.00001232
Iteration 30/1000 | Loss: 0.00001232
Iteration 31/1000 | Loss: 0.00001232
Iteration 32/1000 | Loss: 0.00001231
Iteration 33/1000 | Loss: 0.00001231
Iteration 34/1000 | Loss: 0.00001231
Iteration 35/1000 | Loss: 0.00001229
Iteration 36/1000 | Loss: 0.00001229
Iteration 37/1000 | Loss: 0.00001228
Iteration 38/1000 | Loss: 0.00001228
Iteration 39/1000 | Loss: 0.00001228
Iteration 40/1000 | Loss: 0.00001227
Iteration 41/1000 | Loss: 0.00001226
Iteration 42/1000 | Loss: 0.00001225
Iteration 43/1000 | Loss: 0.00001224
Iteration 44/1000 | Loss: 0.00001224
Iteration 45/1000 | Loss: 0.00001235
Iteration 46/1000 | Loss: 0.00001220
Iteration 47/1000 | Loss: 0.00001219
Iteration 48/1000 | Loss: 0.00001219
Iteration 49/1000 | Loss: 0.00001218
Iteration 50/1000 | Loss: 0.00001218
Iteration 51/1000 | Loss: 0.00001218
Iteration 52/1000 | Loss: 0.00001218
Iteration 53/1000 | Loss: 0.00001218
Iteration 54/1000 | Loss: 0.00001218
Iteration 55/1000 | Loss: 0.00001217
Iteration 56/1000 | Loss: 0.00001217
Iteration 57/1000 | Loss: 0.00001217
Iteration 58/1000 | Loss: 0.00001217
Iteration 59/1000 | Loss: 0.00001217
Iteration 60/1000 | Loss: 0.00001217
Iteration 61/1000 | Loss: 0.00001217
Iteration 62/1000 | Loss: 0.00001217
Iteration 63/1000 | Loss: 0.00001217
Iteration 64/1000 | Loss: 0.00001217
Iteration 65/1000 | Loss: 0.00001217
Iteration 66/1000 | Loss: 0.00001217
Iteration 67/1000 | Loss: 0.00001217
Iteration 68/1000 | Loss: 0.00001217
Iteration 69/1000 | Loss: 0.00001217
Iteration 70/1000 | Loss: 0.00001216
Iteration 71/1000 | Loss: 0.00001216
Iteration 72/1000 | Loss: 0.00001216
Iteration 73/1000 | Loss: 0.00001216
Iteration 74/1000 | Loss: 0.00001216
Iteration 75/1000 | Loss: 0.00001216
Iteration 76/1000 | Loss: 0.00001216
Iteration 77/1000 | Loss: 0.00001216
Iteration 78/1000 | Loss: 0.00001215
Iteration 79/1000 | Loss: 0.00001215
Iteration 80/1000 | Loss: 0.00001215
Iteration 81/1000 | Loss: 0.00001214
Iteration 82/1000 | Loss: 0.00001214
Iteration 83/1000 | Loss: 0.00001212
Iteration 84/1000 | Loss: 0.00001212
Iteration 85/1000 | Loss: 0.00001211
Iteration 86/1000 | Loss: 0.00001211
Iteration 87/1000 | Loss: 0.00001211
Iteration 88/1000 | Loss: 0.00001210
Iteration 89/1000 | Loss: 0.00001210
Iteration 90/1000 | Loss: 0.00001210
Iteration 91/1000 | Loss: 0.00001210
Iteration 92/1000 | Loss: 0.00001210
Iteration 93/1000 | Loss: 0.00001210
Iteration 94/1000 | Loss: 0.00001210
Iteration 95/1000 | Loss: 0.00001210
Iteration 96/1000 | Loss: 0.00001210
Iteration 97/1000 | Loss: 0.00001210
Iteration 98/1000 | Loss: 0.00001209
Iteration 99/1000 | Loss: 0.00001209
Iteration 100/1000 | Loss: 0.00001209
Iteration 101/1000 | Loss: 0.00001209
Iteration 102/1000 | Loss: 0.00001209
Iteration 103/1000 | Loss: 0.00001209
Iteration 104/1000 | Loss: 0.00001209
Iteration 105/1000 | Loss: 0.00001357
Iteration 106/1000 | Loss: 0.00001213
Iteration 107/1000 | Loss: 0.00001213
Iteration 108/1000 | Loss: 0.00001213
Iteration 109/1000 | Loss: 0.00001212
Iteration 110/1000 | Loss: 0.00001207
Iteration 111/1000 | Loss: 0.00001207
Iteration 112/1000 | Loss: 0.00001207
Iteration 113/1000 | Loss: 0.00001207
Iteration 114/1000 | Loss: 0.00001207
Iteration 115/1000 | Loss: 0.00001207
Iteration 116/1000 | Loss: 0.00001207
Iteration 117/1000 | Loss: 0.00001207
Iteration 118/1000 | Loss: 0.00001207
Iteration 119/1000 | Loss: 0.00001207
Iteration 120/1000 | Loss: 0.00001207
Iteration 121/1000 | Loss: 0.00001207
Iteration 122/1000 | Loss: 0.00001207
Iteration 123/1000 | Loss: 0.00001207
Iteration 124/1000 | Loss: 0.00001207
Iteration 125/1000 | Loss: 0.00001207
Iteration 126/1000 | Loss: 0.00001207
Iteration 127/1000 | Loss: 0.00001207
Iteration 128/1000 | Loss: 0.00001207
Iteration 129/1000 | Loss: 0.00001207
Iteration 130/1000 | Loss: 0.00001207
Iteration 131/1000 | Loss: 0.00001207
Iteration 132/1000 | Loss: 0.00001207
Iteration 133/1000 | Loss: 0.00001207
Iteration 134/1000 | Loss: 0.00001207
Iteration 135/1000 | Loss: 0.00001207
Iteration 136/1000 | Loss: 0.00001207
Iteration 137/1000 | Loss: 0.00001207
Iteration 138/1000 | Loss: 0.00001207
Iteration 139/1000 | Loss: 0.00001207
Iteration 140/1000 | Loss: 0.00001207
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 140. Stopping optimization.
Last 5 losses: [1.2067762327205855e-05, 1.2067762327205855e-05, 1.2067762327205855e-05, 1.2067762327205855e-05, 1.2067762327205855e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2067762327205855e-05

Optimization complete. Final v2v error: 2.9388139247894287 mm

Highest mean error: 3.2642247676849365 mm for frame 138

Lowest mean error: 2.703794002532959 mm for frame 57

Saving results

Total time: 54.99815225601196
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_025/1020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_025/1020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_025/1020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00818816
Iteration 2/25 | Loss: 0.00143372
Iteration 3/25 | Loss: 0.00123980
Iteration 4/25 | Loss: 0.00120241
Iteration 5/25 | Loss: 0.00118692
Iteration 6/25 | Loss: 0.00118380
Iteration 7/25 | Loss: 0.00118227
Iteration 8/25 | Loss: 0.00118562
Iteration 9/25 | Loss: 0.00117835
Iteration 10/25 | Loss: 0.00117642
Iteration 11/25 | Loss: 0.00117571
Iteration 12/25 | Loss: 0.00117567
Iteration 13/25 | Loss: 0.00117567
Iteration 14/25 | Loss: 0.00117566
Iteration 15/25 | Loss: 0.00117566
Iteration 16/25 | Loss: 0.00117566
Iteration 17/25 | Loss: 0.00117566
Iteration 18/25 | Loss: 0.00117566
Iteration 19/25 | Loss: 0.00117566
Iteration 20/25 | Loss: 0.00117566
Iteration 21/25 | Loss: 0.00117566
Iteration 22/25 | Loss: 0.00117566
Iteration 23/25 | Loss: 0.00117566
Iteration 24/25 | Loss: 0.00117566
Iteration 25/25 | Loss: 0.00117566

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.93541682
Iteration 2/25 | Loss: 0.00096641
Iteration 3/25 | Loss: 0.00096641
Iteration 4/25 | Loss: 0.00096640
Iteration 5/25 | Loss: 0.00096640
Iteration 6/25 | Loss: 0.00096640
Iteration 7/25 | Loss: 0.00096640
Iteration 8/25 | Loss: 0.00096640
Iteration 9/25 | Loss: 0.00096640
Iteration 10/25 | Loss: 0.00096640
Iteration 11/25 | Loss: 0.00096640
Iteration 12/25 | Loss: 0.00096640
Iteration 13/25 | Loss: 0.00096640
Iteration 14/25 | Loss: 0.00096640
Iteration 15/25 | Loss: 0.00096640
Iteration 16/25 | Loss: 0.00096640
Iteration 17/25 | Loss: 0.00096640
Iteration 18/25 | Loss: 0.00096640
Iteration 19/25 | Loss: 0.00096640
Iteration 20/25 | Loss: 0.00096640
Iteration 21/25 | Loss: 0.00096640
Iteration 22/25 | Loss: 0.00096640
Iteration 23/25 | Loss: 0.00096640
Iteration 24/25 | Loss: 0.00096640
Iteration 25/25 | Loss: 0.00096640

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00096640
Iteration 2/1000 | Loss: 0.00003987
Iteration 3/1000 | Loss: 0.00002626
Iteration 4/1000 | Loss: 0.00001975
Iteration 5/1000 | Loss: 0.00001766
Iteration 6/1000 | Loss: 0.00001666
Iteration 7/1000 | Loss: 0.00001600
Iteration 8/1000 | Loss: 0.00001571
Iteration 9/1000 | Loss: 0.00001535
Iteration 10/1000 | Loss: 0.00001531
Iteration 11/1000 | Loss: 0.00001514
Iteration 12/1000 | Loss: 0.00001511
Iteration 13/1000 | Loss: 0.00001509
Iteration 14/1000 | Loss: 0.00001500
Iteration 15/1000 | Loss: 0.00001495
Iteration 16/1000 | Loss: 0.00001490
Iteration 17/1000 | Loss: 0.00001488
Iteration 18/1000 | Loss: 0.00001483
Iteration 19/1000 | Loss: 0.00001479
Iteration 20/1000 | Loss: 0.00001477
Iteration 21/1000 | Loss: 0.00001477
Iteration 22/1000 | Loss: 0.00001476
Iteration 23/1000 | Loss: 0.00001468
Iteration 24/1000 | Loss: 0.00001465
Iteration 25/1000 | Loss: 0.00001464
Iteration 26/1000 | Loss: 0.00001464
Iteration 27/1000 | Loss: 0.00001463
Iteration 28/1000 | Loss: 0.00001460
Iteration 29/1000 | Loss: 0.00001455
Iteration 30/1000 | Loss: 0.00001453
Iteration 31/1000 | Loss: 0.00001452
Iteration 32/1000 | Loss: 0.00001451
Iteration 33/1000 | Loss: 0.00001450
Iteration 34/1000 | Loss: 0.00001450
Iteration 35/1000 | Loss: 0.00001446
Iteration 36/1000 | Loss: 0.00001445
Iteration 37/1000 | Loss: 0.00001444
Iteration 38/1000 | Loss: 0.00001443
Iteration 39/1000 | Loss: 0.00001443
Iteration 40/1000 | Loss: 0.00001442
Iteration 41/1000 | Loss: 0.00001442
Iteration 42/1000 | Loss: 0.00001441
Iteration 43/1000 | Loss: 0.00001441
Iteration 44/1000 | Loss: 0.00001441
Iteration 45/1000 | Loss: 0.00001440
Iteration 46/1000 | Loss: 0.00001440
Iteration 47/1000 | Loss: 0.00001439
Iteration 48/1000 | Loss: 0.00001439
Iteration 49/1000 | Loss: 0.00001438
Iteration 50/1000 | Loss: 0.00001438
Iteration 51/1000 | Loss: 0.00001438
Iteration 52/1000 | Loss: 0.00001437
Iteration 53/1000 | Loss: 0.00001437
Iteration 54/1000 | Loss: 0.00001437
Iteration 55/1000 | Loss: 0.00001437
Iteration 56/1000 | Loss: 0.00001437
Iteration 57/1000 | Loss: 0.00001437
Iteration 58/1000 | Loss: 0.00001436
Iteration 59/1000 | Loss: 0.00001436
Iteration 60/1000 | Loss: 0.00001436
Iteration 61/1000 | Loss: 0.00001436
Iteration 62/1000 | Loss: 0.00001435
Iteration 63/1000 | Loss: 0.00001435
Iteration 64/1000 | Loss: 0.00001435
Iteration 65/1000 | Loss: 0.00001435
Iteration 66/1000 | Loss: 0.00001435
Iteration 67/1000 | Loss: 0.00001434
Iteration 68/1000 | Loss: 0.00001434
Iteration 69/1000 | Loss: 0.00001434
Iteration 70/1000 | Loss: 0.00001434
Iteration 71/1000 | Loss: 0.00001433
Iteration 72/1000 | Loss: 0.00001432
Iteration 73/1000 | Loss: 0.00001432
Iteration 74/1000 | Loss: 0.00001432
Iteration 75/1000 | Loss: 0.00001432
Iteration 76/1000 | Loss: 0.00001432
Iteration 77/1000 | Loss: 0.00001431
Iteration 78/1000 | Loss: 0.00001431
Iteration 79/1000 | Loss: 0.00001430
Iteration 80/1000 | Loss: 0.00001430
Iteration 81/1000 | Loss: 0.00001430
Iteration 82/1000 | Loss: 0.00001430
Iteration 83/1000 | Loss: 0.00001430
Iteration 84/1000 | Loss: 0.00001430
Iteration 85/1000 | Loss: 0.00001430
Iteration 86/1000 | Loss: 0.00001429
Iteration 87/1000 | Loss: 0.00001429
Iteration 88/1000 | Loss: 0.00001429
Iteration 89/1000 | Loss: 0.00001429
Iteration 90/1000 | Loss: 0.00001429
Iteration 91/1000 | Loss: 0.00001428
Iteration 92/1000 | Loss: 0.00001428
Iteration 93/1000 | Loss: 0.00001428
Iteration 94/1000 | Loss: 0.00001427
Iteration 95/1000 | Loss: 0.00001427
Iteration 96/1000 | Loss: 0.00001427
Iteration 97/1000 | Loss: 0.00001427
Iteration 98/1000 | Loss: 0.00001427
Iteration 99/1000 | Loss: 0.00001426
Iteration 100/1000 | Loss: 0.00001426
Iteration 101/1000 | Loss: 0.00001426
Iteration 102/1000 | Loss: 0.00001426
Iteration 103/1000 | Loss: 0.00001426
Iteration 104/1000 | Loss: 0.00001426
Iteration 105/1000 | Loss: 0.00001426
Iteration 106/1000 | Loss: 0.00001426
Iteration 107/1000 | Loss: 0.00001426
Iteration 108/1000 | Loss: 0.00001426
Iteration 109/1000 | Loss: 0.00001425
Iteration 110/1000 | Loss: 0.00001425
Iteration 111/1000 | Loss: 0.00001425
Iteration 112/1000 | Loss: 0.00001425
Iteration 113/1000 | Loss: 0.00001425
Iteration 114/1000 | Loss: 0.00001425
Iteration 115/1000 | Loss: 0.00001424
Iteration 116/1000 | Loss: 0.00001424
Iteration 117/1000 | Loss: 0.00001424
Iteration 118/1000 | Loss: 0.00001424
Iteration 119/1000 | Loss: 0.00001424
Iteration 120/1000 | Loss: 0.00001424
Iteration 121/1000 | Loss: 0.00001423
Iteration 122/1000 | Loss: 0.00001423
Iteration 123/1000 | Loss: 0.00001423
Iteration 124/1000 | Loss: 0.00001423
Iteration 125/1000 | Loss: 0.00001423
Iteration 126/1000 | Loss: 0.00001423
Iteration 127/1000 | Loss: 0.00001423
Iteration 128/1000 | Loss: 0.00001423
Iteration 129/1000 | Loss: 0.00001423
Iteration 130/1000 | Loss: 0.00001423
Iteration 131/1000 | Loss: 0.00001422
Iteration 132/1000 | Loss: 0.00001422
Iteration 133/1000 | Loss: 0.00001422
Iteration 134/1000 | Loss: 0.00001422
Iteration 135/1000 | Loss: 0.00001422
Iteration 136/1000 | Loss: 0.00001422
Iteration 137/1000 | Loss: 0.00001422
Iteration 138/1000 | Loss: 0.00001422
Iteration 139/1000 | Loss: 0.00001422
Iteration 140/1000 | Loss: 0.00001422
Iteration 141/1000 | Loss: 0.00001422
Iteration 142/1000 | Loss: 0.00001422
Iteration 143/1000 | Loss: 0.00001422
Iteration 144/1000 | Loss: 0.00001422
Iteration 145/1000 | Loss: 0.00001422
Iteration 146/1000 | Loss: 0.00001422
Iteration 147/1000 | Loss: 0.00001422
Iteration 148/1000 | Loss: 0.00001422
Iteration 149/1000 | Loss: 0.00001421
Iteration 150/1000 | Loss: 0.00001421
Iteration 151/1000 | Loss: 0.00001421
Iteration 152/1000 | Loss: 0.00001421
Iteration 153/1000 | Loss: 0.00001421
Iteration 154/1000 | Loss: 0.00001421
Iteration 155/1000 | Loss: 0.00001421
Iteration 156/1000 | Loss: 0.00001421
Iteration 157/1000 | Loss: 0.00001421
Iteration 158/1000 | Loss: 0.00001420
Iteration 159/1000 | Loss: 0.00001420
Iteration 160/1000 | Loss: 0.00001420
Iteration 161/1000 | Loss: 0.00001420
Iteration 162/1000 | Loss: 0.00001420
Iteration 163/1000 | Loss: 0.00001420
Iteration 164/1000 | Loss: 0.00001420
Iteration 165/1000 | Loss: 0.00001420
Iteration 166/1000 | Loss: 0.00001420
Iteration 167/1000 | Loss: 0.00001420
Iteration 168/1000 | Loss: 0.00001419
Iteration 169/1000 | Loss: 0.00001419
Iteration 170/1000 | Loss: 0.00001419
Iteration 171/1000 | Loss: 0.00001419
Iteration 172/1000 | Loss: 0.00001419
Iteration 173/1000 | Loss: 0.00001419
Iteration 174/1000 | Loss: 0.00001419
Iteration 175/1000 | Loss: 0.00001419
Iteration 176/1000 | Loss: 0.00001419
Iteration 177/1000 | Loss: 0.00001419
Iteration 178/1000 | Loss: 0.00001419
Iteration 179/1000 | Loss: 0.00001419
Iteration 180/1000 | Loss: 0.00001419
Iteration 181/1000 | Loss: 0.00001419
Iteration 182/1000 | Loss: 0.00001419
Iteration 183/1000 | Loss: 0.00001419
Iteration 184/1000 | Loss: 0.00001419
Iteration 185/1000 | Loss: 0.00001419
Iteration 186/1000 | Loss: 0.00001419
Iteration 187/1000 | Loss: 0.00001418
Iteration 188/1000 | Loss: 0.00001418
Iteration 189/1000 | Loss: 0.00001418
Iteration 190/1000 | Loss: 0.00001418
Iteration 191/1000 | Loss: 0.00001418
Iteration 192/1000 | Loss: 0.00001418
Iteration 193/1000 | Loss: 0.00001418
Iteration 194/1000 | Loss: 0.00001418
Iteration 195/1000 | Loss: 0.00001418
Iteration 196/1000 | Loss: 0.00001418
Iteration 197/1000 | Loss: 0.00001418
Iteration 198/1000 | Loss: 0.00001418
Iteration 199/1000 | Loss: 0.00001418
Iteration 200/1000 | Loss: 0.00001418
Iteration 201/1000 | Loss: 0.00001418
Iteration 202/1000 | Loss: 0.00001418
Iteration 203/1000 | Loss: 0.00001418
Iteration 204/1000 | Loss: 0.00001418
Iteration 205/1000 | Loss: 0.00001418
Iteration 206/1000 | Loss: 0.00001418
Iteration 207/1000 | Loss: 0.00001418
Iteration 208/1000 | Loss: 0.00001418
Iteration 209/1000 | Loss: 0.00001418
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 209. Stopping optimization.
Last 5 losses: [1.4181107871991117e-05, 1.4181107871991117e-05, 1.4181107871991117e-05, 1.4181107871991117e-05, 1.4181107871991117e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4181107871991117e-05

Optimization complete. Final v2v error: 3.1931440830230713 mm

Highest mean error: 3.804130792617798 mm for frame 80

Lowest mean error: 2.689239263534546 mm for frame 125

Saving results

Total time: 53.56848907470703
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_025/1090/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_025/1090.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_025/1090
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00377231
Iteration 2/25 | Loss: 0.00118661
Iteration 3/25 | Loss: 0.00111721
Iteration 4/25 | Loss: 0.00110702
Iteration 5/25 | Loss: 0.00110354
Iteration 6/25 | Loss: 0.00110295
Iteration 7/25 | Loss: 0.00110295
Iteration 8/25 | Loss: 0.00110295
Iteration 9/25 | Loss: 0.00110295
Iteration 10/25 | Loss: 0.00110295
Iteration 11/25 | Loss: 0.00110295
Iteration 12/25 | Loss: 0.00110295
Iteration 13/25 | Loss: 0.00110295
Iteration 14/25 | Loss: 0.00110295
Iteration 15/25 | Loss: 0.00110295
Iteration 16/25 | Loss: 0.00110295
Iteration 17/25 | Loss: 0.00110295
Iteration 18/25 | Loss: 0.00110295
Iteration 19/25 | Loss: 0.00110295
Iteration 20/25 | Loss: 0.00110295
Iteration 21/25 | Loss: 0.00110295
Iteration 22/25 | Loss: 0.00110295
Iteration 23/25 | Loss: 0.00110295
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.001102947280742228, 0.001102947280742228, 0.001102947280742228, 0.001102947280742228, 0.001102947280742228]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001102947280742228

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.86958885
Iteration 2/25 | Loss: 0.00082936
Iteration 3/25 | Loss: 0.00082936
Iteration 4/25 | Loss: 0.00082936
Iteration 5/25 | Loss: 0.00082936
Iteration 6/25 | Loss: 0.00082936
Iteration 7/25 | Loss: 0.00082936
Iteration 8/25 | Loss: 0.00082935
Iteration 9/25 | Loss: 0.00082935
Iteration 10/25 | Loss: 0.00082935
Iteration 11/25 | Loss: 0.00082935
Iteration 12/25 | Loss: 0.00082935
Iteration 13/25 | Loss: 0.00082935
Iteration 14/25 | Loss: 0.00082935
Iteration 15/25 | Loss: 0.00082935
Iteration 16/25 | Loss: 0.00082935
Iteration 17/25 | Loss: 0.00082935
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0008293537539429963, 0.0008293537539429963, 0.0008293537539429963, 0.0008293537539429963, 0.0008293537539429963]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008293537539429963

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00082935
Iteration 2/1000 | Loss: 0.00001891
Iteration 3/1000 | Loss: 0.00001341
Iteration 4/1000 | Loss: 0.00001164
Iteration 5/1000 | Loss: 0.00001105
Iteration 6/1000 | Loss: 0.00001056
Iteration 7/1000 | Loss: 0.00001026
Iteration 8/1000 | Loss: 0.00001000
Iteration 9/1000 | Loss: 0.00000982
Iteration 10/1000 | Loss: 0.00000959
Iteration 11/1000 | Loss: 0.00000952
Iteration 12/1000 | Loss: 0.00000949
Iteration 13/1000 | Loss: 0.00000948
Iteration 14/1000 | Loss: 0.00000943
Iteration 15/1000 | Loss: 0.00000942
Iteration 16/1000 | Loss: 0.00000941
Iteration 17/1000 | Loss: 0.00000940
Iteration 18/1000 | Loss: 0.00000938
Iteration 19/1000 | Loss: 0.00000933
Iteration 20/1000 | Loss: 0.00000933
Iteration 21/1000 | Loss: 0.00000930
Iteration 22/1000 | Loss: 0.00000929
Iteration 23/1000 | Loss: 0.00000929
Iteration 24/1000 | Loss: 0.00000927
Iteration 25/1000 | Loss: 0.00000927
Iteration 26/1000 | Loss: 0.00000925
Iteration 27/1000 | Loss: 0.00000925
Iteration 28/1000 | Loss: 0.00000923
Iteration 29/1000 | Loss: 0.00000923
Iteration 30/1000 | Loss: 0.00000922
Iteration 31/1000 | Loss: 0.00000921
Iteration 32/1000 | Loss: 0.00000921
Iteration 33/1000 | Loss: 0.00000920
Iteration 34/1000 | Loss: 0.00000920
Iteration 35/1000 | Loss: 0.00000919
Iteration 36/1000 | Loss: 0.00000918
Iteration 37/1000 | Loss: 0.00000918
Iteration 38/1000 | Loss: 0.00000918
Iteration 39/1000 | Loss: 0.00000918
Iteration 40/1000 | Loss: 0.00000917
Iteration 41/1000 | Loss: 0.00000917
Iteration 42/1000 | Loss: 0.00000916
Iteration 43/1000 | Loss: 0.00000915
Iteration 44/1000 | Loss: 0.00000915
Iteration 45/1000 | Loss: 0.00000915
Iteration 46/1000 | Loss: 0.00000914
Iteration 47/1000 | Loss: 0.00000914
Iteration 48/1000 | Loss: 0.00000914
Iteration 49/1000 | Loss: 0.00000913
Iteration 50/1000 | Loss: 0.00000913
Iteration 51/1000 | Loss: 0.00000912
Iteration 52/1000 | Loss: 0.00000912
Iteration 53/1000 | Loss: 0.00000912
Iteration 54/1000 | Loss: 0.00000911
Iteration 55/1000 | Loss: 0.00000911
Iteration 56/1000 | Loss: 0.00000910
Iteration 57/1000 | Loss: 0.00000910
Iteration 58/1000 | Loss: 0.00000910
Iteration 59/1000 | Loss: 0.00000909
Iteration 60/1000 | Loss: 0.00000908
Iteration 61/1000 | Loss: 0.00000908
Iteration 62/1000 | Loss: 0.00000907
Iteration 63/1000 | Loss: 0.00000907
Iteration 64/1000 | Loss: 0.00000907
Iteration 65/1000 | Loss: 0.00000906
Iteration 66/1000 | Loss: 0.00000906
Iteration 67/1000 | Loss: 0.00000905
Iteration 68/1000 | Loss: 0.00000905
Iteration 69/1000 | Loss: 0.00000904
Iteration 70/1000 | Loss: 0.00000904
Iteration 71/1000 | Loss: 0.00000904
Iteration 72/1000 | Loss: 0.00000903
Iteration 73/1000 | Loss: 0.00000903
Iteration 74/1000 | Loss: 0.00000903
Iteration 75/1000 | Loss: 0.00000902
Iteration 76/1000 | Loss: 0.00000902
Iteration 77/1000 | Loss: 0.00000901
Iteration 78/1000 | Loss: 0.00000901
Iteration 79/1000 | Loss: 0.00000901
Iteration 80/1000 | Loss: 0.00000901
Iteration 81/1000 | Loss: 0.00000901
Iteration 82/1000 | Loss: 0.00000900
Iteration 83/1000 | Loss: 0.00000900
Iteration 84/1000 | Loss: 0.00000900
Iteration 85/1000 | Loss: 0.00000899
Iteration 86/1000 | Loss: 0.00000899
Iteration 87/1000 | Loss: 0.00000899
Iteration 88/1000 | Loss: 0.00000899
Iteration 89/1000 | Loss: 0.00000898
Iteration 90/1000 | Loss: 0.00000898
Iteration 91/1000 | Loss: 0.00000898
Iteration 92/1000 | Loss: 0.00000898
Iteration 93/1000 | Loss: 0.00000897
Iteration 94/1000 | Loss: 0.00000897
Iteration 95/1000 | Loss: 0.00000897
Iteration 96/1000 | Loss: 0.00000897
Iteration 97/1000 | Loss: 0.00000897
Iteration 98/1000 | Loss: 0.00000897
Iteration 99/1000 | Loss: 0.00000897
Iteration 100/1000 | Loss: 0.00000896
Iteration 101/1000 | Loss: 0.00000896
Iteration 102/1000 | Loss: 0.00000896
Iteration 103/1000 | Loss: 0.00000896
Iteration 104/1000 | Loss: 0.00000896
Iteration 105/1000 | Loss: 0.00000896
Iteration 106/1000 | Loss: 0.00000896
Iteration 107/1000 | Loss: 0.00000896
Iteration 108/1000 | Loss: 0.00000896
Iteration 109/1000 | Loss: 0.00000896
Iteration 110/1000 | Loss: 0.00000896
Iteration 111/1000 | Loss: 0.00000896
Iteration 112/1000 | Loss: 0.00000895
Iteration 113/1000 | Loss: 0.00000895
Iteration 114/1000 | Loss: 0.00000895
Iteration 115/1000 | Loss: 0.00000895
Iteration 116/1000 | Loss: 0.00000895
Iteration 117/1000 | Loss: 0.00000895
Iteration 118/1000 | Loss: 0.00000895
Iteration 119/1000 | Loss: 0.00000895
Iteration 120/1000 | Loss: 0.00000895
Iteration 121/1000 | Loss: 0.00000895
Iteration 122/1000 | Loss: 0.00000895
Iteration 123/1000 | Loss: 0.00000895
Iteration 124/1000 | Loss: 0.00000894
Iteration 125/1000 | Loss: 0.00000894
Iteration 126/1000 | Loss: 0.00000894
Iteration 127/1000 | Loss: 0.00000894
Iteration 128/1000 | Loss: 0.00000894
Iteration 129/1000 | Loss: 0.00000894
Iteration 130/1000 | Loss: 0.00000893
Iteration 131/1000 | Loss: 0.00000893
Iteration 132/1000 | Loss: 0.00000893
Iteration 133/1000 | Loss: 0.00000893
Iteration 134/1000 | Loss: 0.00000893
Iteration 135/1000 | Loss: 0.00000892
Iteration 136/1000 | Loss: 0.00000892
Iteration 137/1000 | Loss: 0.00000892
Iteration 138/1000 | Loss: 0.00000892
Iteration 139/1000 | Loss: 0.00000892
Iteration 140/1000 | Loss: 0.00000892
Iteration 141/1000 | Loss: 0.00000892
Iteration 142/1000 | Loss: 0.00000892
Iteration 143/1000 | Loss: 0.00000892
Iteration 144/1000 | Loss: 0.00000891
Iteration 145/1000 | Loss: 0.00000891
Iteration 146/1000 | Loss: 0.00000891
Iteration 147/1000 | Loss: 0.00000891
Iteration 148/1000 | Loss: 0.00000891
Iteration 149/1000 | Loss: 0.00000891
Iteration 150/1000 | Loss: 0.00000891
Iteration 151/1000 | Loss: 0.00000891
Iteration 152/1000 | Loss: 0.00000891
Iteration 153/1000 | Loss: 0.00000891
Iteration 154/1000 | Loss: 0.00000891
Iteration 155/1000 | Loss: 0.00000891
Iteration 156/1000 | Loss: 0.00000891
Iteration 157/1000 | Loss: 0.00000891
Iteration 158/1000 | Loss: 0.00000891
Iteration 159/1000 | Loss: 0.00000891
Iteration 160/1000 | Loss: 0.00000891
Iteration 161/1000 | Loss: 0.00000890
Iteration 162/1000 | Loss: 0.00000890
Iteration 163/1000 | Loss: 0.00000890
Iteration 164/1000 | Loss: 0.00000890
Iteration 165/1000 | Loss: 0.00000890
Iteration 166/1000 | Loss: 0.00000890
Iteration 167/1000 | Loss: 0.00000890
Iteration 168/1000 | Loss: 0.00000890
Iteration 169/1000 | Loss: 0.00000890
Iteration 170/1000 | Loss: 0.00000890
Iteration 171/1000 | Loss: 0.00000890
Iteration 172/1000 | Loss: 0.00000890
Iteration 173/1000 | Loss: 0.00000890
Iteration 174/1000 | Loss: 0.00000890
Iteration 175/1000 | Loss: 0.00000890
Iteration 176/1000 | Loss: 0.00000890
Iteration 177/1000 | Loss: 0.00000890
Iteration 178/1000 | Loss: 0.00000890
Iteration 179/1000 | Loss: 0.00000890
Iteration 180/1000 | Loss: 0.00000890
Iteration 181/1000 | Loss: 0.00000890
Iteration 182/1000 | Loss: 0.00000890
Iteration 183/1000 | Loss: 0.00000890
Iteration 184/1000 | Loss: 0.00000890
Iteration 185/1000 | Loss: 0.00000890
Iteration 186/1000 | Loss: 0.00000890
Iteration 187/1000 | Loss: 0.00000890
Iteration 188/1000 | Loss: 0.00000890
Iteration 189/1000 | Loss: 0.00000890
Iteration 190/1000 | Loss: 0.00000890
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 190. Stopping optimization.
Last 5 losses: [8.896104191080667e-06, 8.896104191080667e-06, 8.896104191080667e-06, 8.896104191080667e-06, 8.896104191080667e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.896104191080667e-06

Optimization complete. Final v2v error: 2.5676145553588867 mm

Highest mean error: 3.038417100906372 mm for frame 82

Lowest mean error: 2.4489426612854004 mm for frame 151

Saving results

Total time: 39.48493051528931
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_025/1018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_025/1018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_025/1018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00533150
Iteration 2/25 | Loss: 0.00119920
Iteration 3/25 | Loss: 0.00113061
Iteration 4/25 | Loss: 0.00111983
Iteration 5/25 | Loss: 0.00111583
Iteration 6/25 | Loss: 0.00111554
Iteration 7/25 | Loss: 0.00111554
Iteration 8/25 | Loss: 0.00111554
Iteration 9/25 | Loss: 0.00111554
Iteration 10/25 | Loss: 0.00111554
Iteration 11/25 | Loss: 0.00111554
Iteration 12/25 | Loss: 0.00111554
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0011155445827171206, 0.0011155445827171206, 0.0011155445827171206, 0.0011155445827171206, 0.0011155445827171206]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011155445827171206

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.63501596
Iteration 2/25 | Loss: 0.00079166
Iteration 3/25 | Loss: 0.00079166
Iteration 4/25 | Loss: 0.00079166
Iteration 5/25 | Loss: 0.00079166
Iteration 6/25 | Loss: 0.00079165
Iteration 7/25 | Loss: 0.00079165
Iteration 8/25 | Loss: 0.00079165
Iteration 9/25 | Loss: 0.00079165
Iteration 10/25 | Loss: 0.00079165
Iteration 11/25 | Loss: 0.00079165
Iteration 12/25 | Loss: 0.00079165
Iteration 13/25 | Loss: 0.00079165
Iteration 14/25 | Loss: 0.00079165
Iteration 15/25 | Loss: 0.00079165
Iteration 16/25 | Loss: 0.00079165
Iteration 17/25 | Loss: 0.00079165
Iteration 18/25 | Loss: 0.00079165
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0007916530012153089, 0.0007916530012153089, 0.0007916530012153089, 0.0007916530012153089, 0.0007916530012153089]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007916530012153089

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00079165
Iteration 2/1000 | Loss: 0.00002323
Iteration 3/1000 | Loss: 0.00001632
Iteration 4/1000 | Loss: 0.00001445
Iteration 5/1000 | Loss: 0.00001346
Iteration 6/1000 | Loss: 0.00001285
Iteration 7/1000 | Loss: 0.00001238
Iteration 8/1000 | Loss: 0.00001203
Iteration 9/1000 | Loss: 0.00001181
Iteration 10/1000 | Loss: 0.00001148
Iteration 11/1000 | Loss: 0.00001130
Iteration 12/1000 | Loss: 0.00001115
Iteration 13/1000 | Loss: 0.00001115
Iteration 14/1000 | Loss: 0.00001107
Iteration 15/1000 | Loss: 0.00001103
Iteration 16/1000 | Loss: 0.00001098
Iteration 17/1000 | Loss: 0.00001097
Iteration 18/1000 | Loss: 0.00001097
Iteration 19/1000 | Loss: 0.00001095
Iteration 20/1000 | Loss: 0.00001094
Iteration 21/1000 | Loss: 0.00001093
Iteration 22/1000 | Loss: 0.00001092
Iteration 23/1000 | Loss: 0.00001092
Iteration 24/1000 | Loss: 0.00001091
Iteration 25/1000 | Loss: 0.00001091
Iteration 26/1000 | Loss: 0.00001090
Iteration 27/1000 | Loss: 0.00001089
Iteration 28/1000 | Loss: 0.00001088
Iteration 29/1000 | Loss: 0.00001088
Iteration 30/1000 | Loss: 0.00001087
Iteration 31/1000 | Loss: 0.00001087
Iteration 32/1000 | Loss: 0.00001086
Iteration 33/1000 | Loss: 0.00001086
Iteration 34/1000 | Loss: 0.00001086
Iteration 35/1000 | Loss: 0.00001085
Iteration 36/1000 | Loss: 0.00001085
Iteration 37/1000 | Loss: 0.00001085
Iteration 38/1000 | Loss: 0.00001084
Iteration 39/1000 | Loss: 0.00001083
Iteration 40/1000 | Loss: 0.00001079
Iteration 41/1000 | Loss: 0.00001076
Iteration 42/1000 | Loss: 0.00001075
Iteration 43/1000 | Loss: 0.00001074
Iteration 44/1000 | Loss: 0.00001073
Iteration 45/1000 | Loss: 0.00001072
Iteration 46/1000 | Loss: 0.00001071
Iteration 47/1000 | Loss: 0.00001070
Iteration 48/1000 | Loss: 0.00001069
Iteration 49/1000 | Loss: 0.00001069
Iteration 50/1000 | Loss: 0.00001067
Iteration 51/1000 | Loss: 0.00001065
Iteration 52/1000 | Loss: 0.00001065
Iteration 53/1000 | Loss: 0.00001064
Iteration 54/1000 | Loss: 0.00001064
Iteration 55/1000 | Loss: 0.00001064
Iteration 56/1000 | Loss: 0.00001064
Iteration 57/1000 | Loss: 0.00001064
Iteration 58/1000 | Loss: 0.00001063
Iteration 59/1000 | Loss: 0.00001062
Iteration 60/1000 | Loss: 0.00001062
Iteration 61/1000 | Loss: 0.00001061
Iteration 62/1000 | Loss: 0.00001061
Iteration 63/1000 | Loss: 0.00001060
Iteration 64/1000 | Loss: 0.00001060
Iteration 65/1000 | Loss: 0.00001059
Iteration 66/1000 | Loss: 0.00001059
Iteration 67/1000 | Loss: 0.00001058
Iteration 68/1000 | Loss: 0.00001057
Iteration 69/1000 | Loss: 0.00001057
Iteration 70/1000 | Loss: 0.00001057
Iteration 71/1000 | Loss: 0.00001057
Iteration 72/1000 | Loss: 0.00001057
Iteration 73/1000 | Loss: 0.00001057
Iteration 74/1000 | Loss: 0.00001057
Iteration 75/1000 | Loss: 0.00001057
Iteration 76/1000 | Loss: 0.00001057
Iteration 77/1000 | Loss: 0.00001056
Iteration 78/1000 | Loss: 0.00001056
Iteration 79/1000 | Loss: 0.00001056
Iteration 80/1000 | Loss: 0.00001056
Iteration 81/1000 | Loss: 0.00001056
Iteration 82/1000 | Loss: 0.00001056
Iteration 83/1000 | Loss: 0.00001056
Iteration 84/1000 | Loss: 0.00001055
Iteration 85/1000 | Loss: 0.00001054
Iteration 86/1000 | Loss: 0.00001054
Iteration 87/1000 | Loss: 0.00001054
Iteration 88/1000 | Loss: 0.00001053
Iteration 89/1000 | Loss: 0.00001053
Iteration 90/1000 | Loss: 0.00001053
Iteration 91/1000 | Loss: 0.00001053
Iteration 92/1000 | Loss: 0.00001053
Iteration 93/1000 | Loss: 0.00001053
Iteration 94/1000 | Loss: 0.00001052
Iteration 95/1000 | Loss: 0.00001052
Iteration 96/1000 | Loss: 0.00001052
Iteration 97/1000 | Loss: 0.00001052
Iteration 98/1000 | Loss: 0.00001051
Iteration 99/1000 | Loss: 0.00001050
Iteration 100/1000 | Loss: 0.00001050
Iteration 101/1000 | Loss: 0.00001050
Iteration 102/1000 | Loss: 0.00001050
Iteration 103/1000 | Loss: 0.00001049
Iteration 104/1000 | Loss: 0.00001049
Iteration 105/1000 | Loss: 0.00001049
Iteration 106/1000 | Loss: 0.00001049
Iteration 107/1000 | Loss: 0.00001049
Iteration 108/1000 | Loss: 0.00001049
Iteration 109/1000 | Loss: 0.00001049
Iteration 110/1000 | Loss: 0.00001048
Iteration 111/1000 | Loss: 0.00001048
Iteration 112/1000 | Loss: 0.00001048
Iteration 113/1000 | Loss: 0.00001048
Iteration 114/1000 | Loss: 0.00001048
Iteration 115/1000 | Loss: 0.00001048
Iteration 116/1000 | Loss: 0.00001048
Iteration 117/1000 | Loss: 0.00001048
Iteration 118/1000 | Loss: 0.00001048
Iteration 119/1000 | Loss: 0.00001048
Iteration 120/1000 | Loss: 0.00001048
Iteration 121/1000 | Loss: 0.00001047
Iteration 122/1000 | Loss: 0.00001047
Iteration 123/1000 | Loss: 0.00001047
Iteration 124/1000 | Loss: 0.00001046
Iteration 125/1000 | Loss: 0.00001046
Iteration 126/1000 | Loss: 0.00001046
Iteration 127/1000 | Loss: 0.00001046
Iteration 128/1000 | Loss: 0.00001046
Iteration 129/1000 | Loss: 0.00001046
Iteration 130/1000 | Loss: 0.00001045
Iteration 131/1000 | Loss: 0.00001045
Iteration 132/1000 | Loss: 0.00001045
Iteration 133/1000 | Loss: 0.00001045
Iteration 134/1000 | Loss: 0.00001045
Iteration 135/1000 | Loss: 0.00001045
Iteration 136/1000 | Loss: 0.00001045
Iteration 137/1000 | Loss: 0.00001045
Iteration 138/1000 | Loss: 0.00001045
Iteration 139/1000 | Loss: 0.00001045
Iteration 140/1000 | Loss: 0.00001044
Iteration 141/1000 | Loss: 0.00001044
Iteration 142/1000 | Loss: 0.00001044
Iteration 143/1000 | Loss: 0.00001044
Iteration 144/1000 | Loss: 0.00001044
Iteration 145/1000 | Loss: 0.00001043
Iteration 146/1000 | Loss: 0.00001043
Iteration 147/1000 | Loss: 0.00001043
Iteration 148/1000 | Loss: 0.00001043
Iteration 149/1000 | Loss: 0.00001043
Iteration 150/1000 | Loss: 0.00001043
Iteration 151/1000 | Loss: 0.00001043
Iteration 152/1000 | Loss: 0.00001043
Iteration 153/1000 | Loss: 0.00001043
Iteration 154/1000 | Loss: 0.00001043
Iteration 155/1000 | Loss: 0.00001043
Iteration 156/1000 | Loss: 0.00001042
Iteration 157/1000 | Loss: 0.00001042
Iteration 158/1000 | Loss: 0.00001042
Iteration 159/1000 | Loss: 0.00001042
Iteration 160/1000 | Loss: 0.00001042
Iteration 161/1000 | Loss: 0.00001042
Iteration 162/1000 | Loss: 0.00001042
Iteration 163/1000 | Loss: 0.00001041
Iteration 164/1000 | Loss: 0.00001041
Iteration 165/1000 | Loss: 0.00001041
Iteration 166/1000 | Loss: 0.00001041
Iteration 167/1000 | Loss: 0.00001040
Iteration 168/1000 | Loss: 0.00001040
Iteration 169/1000 | Loss: 0.00001040
Iteration 170/1000 | Loss: 0.00001040
Iteration 171/1000 | Loss: 0.00001040
Iteration 172/1000 | Loss: 0.00001040
Iteration 173/1000 | Loss: 0.00001040
Iteration 174/1000 | Loss: 0.00001040
Iteration 175/1000 | Loss: 0.00001040
Iteration 176/1000 | Loss: 0.00001040
Iteration 177/1000 | Loss: 0.00001039
Iteration 178/1000 | Loss: 0.00001039
Iteration 179/1000 | Loss: 0.00001039
Iteration 180/1000 | Loss: 0.00001039
Iteration 181/1000 | Loss: 0.00001039
Iteration 182/1000 | Loss: 0.00001039
Iteration 183/1000 | Loss: 0.00001039
Iteration 184/1000 | Loss: 0.00001039
Iteration 185/1000 | Loss: 0.00001039
Iteration 186/1000 | Loss: 0.00001039
Iteration 187/1000 | Loss: 0.00001039
Iteration 188/1000 | Loss: 0.00001039
Iteration 189/1000 | Loss: 0.00001039
Iteration 190/1000 | Loss: 0.00001039
Iteration 191/1000 | Loss: 0.00001039
Iteration 192/1000 | Loss: 0.00001039
Iteration 193/1000 | Loss: 0.00001038
Iteration 194/1000 | Loss: 0.00001038
Iteration 195/1000 | Loss: 0.00001038
Iteration 196/1000 | Loss: 0.00001038
Iteration 197/1000 | Loss: 0.00001038
Iteration 198/1000 | Loss: 0.00001038
Iteration 199/1000 | Loss: 0.00001038
Iteration 200/1000 | Loss: 0.00001038
Iteration 201/1000 | Loss: 0.00001038
Iteration 202/1000 | Loss: 0.00001038
Iteration 203/1000 | Loss: 0.00001038
Iteration 204/1000 | Loss: 0.00001038
Iteration 205/1000 | Loss: 0.00001038
Iteration 206/1000 | Loss: 0.00001038
Iteration 207/1000 | Loss: 0.00001038
Iteration 208/1000 | Loss: 0.00001038
Iteration 209/1000 | Loss: 0.00001038
Iteration 210/1000 | Loss: 0.00001038
Iteration 211/1000 | Loss: 0.00001038
Iteration 212/1000 | Loss: 0.00001038
Iteration 213/1000 | Loss: 0.00001038
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 213. Stopping optimization.
Last 5 losses: [1.0379446393926628e-05, 1.0379446393926628e-05, 1.0379446393926628e-05, 1.0379446393926628e-05, 1.0379446393926628e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0379446393926628e-05

Optimization complete. Final v2v error: 2.781358003616333 mm

Highest mean error: 3.1739630699157715 mm for frame 151

Lowest mean error: 2.561699867248535 mm for frame 42

Saving results

Total time: 43.38981795310974
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_025/1034/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_025/1034.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_025/1034
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00871882
Iteration 2/25 | Loss: 0.00179448
Iteration 3/25 | Loss: 0.00136755
Iteration 4/25 | Loss: 0.00134625
Iteration 5/25 | Loss: 0.00133903
Iteration 6/25 | Loss: 0.00133708
Iteration 7/25 | Loss: 0.00133690
Iteration 8/25 | Loss: 0.00133690
Iteration 9/25 | Loss: 0.00133690
Iteration 10/25 | Loss: 0.00133690
Iteration 11/25 | Loss: 0.00133690
Iteration 12/25 | Loss: 0.00133690
Iteration 13/25 | Loss: 0.00133690
Iteration 14/25 | Loss: 0.00133690
Iteration 15/25 | Loss: 0.00133690
Iteration 16/25 | Loss: 0.00133690
Iteration 17/25 | Loss: 0.00133690
Iteration 18/25 | Loss: 0.00133690
Iteration 19/25 | Loss: 0.00133690
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.001336895045824349, 0.001336895045824349, 0.001336895045824349, 0.001336895045824349, 0.001336895045824349]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001336895045824349

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.86367291
Iteration 2/25 | Loss: 0.00111858
Iteration 3/25 | Loss: 0.00111857
Iteration 4/25 | Loss: 0.00111857
Iteration 5/25 | Loss: 0.00111857
Iteration 6/25 | Loss: 0.00111857
Iteration 7/25 | Loss: 0.00111857
Iteration 8/25 | Loss: 0.00111857
Iteration 9/25 | Loss: 0.00111857
Iteration 10/25 | Loss: 0.00111857
Iteration 11/25 | Loss: 0.00111857
Iteration 12/25 | Loss: 0.00111857
Iteration 13/25 | Loss: 0.00111857
Iteration 14/25 | Loss: 0.00111857
Iteration 15/25 | Loss: 0.00111857
Iteration 16/25 | Loss: 0.00111857
Iteration 17/25 | Loss: 0.00111857
Iteration 18/25 | Loss: 0.00111857
Iteration 19/25 | Loss: 0.00111857
Iteration 20/25 | Loss: 0.00111857
Iteration 21/25 | Loss: 0.00111857
Iteration 22/25 | Loss: 0.00111857
Iteration 23/25 | Loss: 0.00111857
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0011185662588104606, 0.0011185662588104606, 0.0011185662588104606, 0.0011185662588104606, 0.0011185662588104606]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011185662588104606

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00111857
Iteration 2/1000 | Loss: 0.00006336
Iteration 3/1000 | Loss: 0.00004330
Iteration 4/1000 | Loss: 0.00003338
Iteration 5/1000 | Loss: 0.00003077
Iteration 6/1000 | Loss: 0.00002940
Iteration 7/1000 | Loss: 0.00002846
Iteration 8/1000 | Loss: 0.00002768
Iteration 9/1000 | Loss: 0.00002695
Iteration 10/1000 | Loss: 0.00002660
Iteration 11/1000 | Loss: 0.00002624
Iteration 12/1000 | Loss: 0.00002592
Iteration 13/1000 | Loss: 0.00002563
Iteration 14/1000 | Loss: 0.00002540
Iteration 15/1000 | Loss: 0.00002522
Iteration 16/1000 | Loss: 0.00002509
Iteration 17/1000 | Loss: 0.00002506
Iteration 18/1000 | Loss: 0.00002497
Iteration 19/1000 | Loss: 0.00002491
Iteration 20/1000 | Loss: 0.00002488
Iteration 21/1000 | Loss: 0.00002482
Iteration 22/1000 | Loss: 0.00002481
Iteration 23/1000 | Loss: 0.00002480
Iteration 24/1000 | Loss: 0.00002479
Iteration 25/1000 | Loss: 0.00002479
Iteration 26/1000 | Loss: 0.00002478
Iteration 27/1000 | Loss: 0.00002478
Iteration 28/1000 | Loss: 0.00002477
Iteration 29/1000 | Loss: 0.00002476
Iteration 30/1000 | Loss: 0.00002476
Iteration 31/1000 | Loss: 0.00002476
Iteration 32/1000 | Loss: 0.00002473
Iteration 33/1000 | Loss: 0.00002471
Iteration 34/1000 | Loss: 0.00002471
Iteration 35/1000 | Loss: 0.00002469
Iteration 36/1000 | Loss: 0.00002468
Iteration 37/1000 | Loss: 0.00002468
Iteration 38/1000 | Loss: 0.00002468
Iteration 39/1000 | Loss: 0.00002466
Iteration 40/1000 | Loss: 0.00002465
Iteration 41/1000 | Loss: 0.00002464
Iteration 42/1000 | Loss: 0.00002463
Iteration 43/1000 | Loss: 0.00002463
Iteration 44/1000 | Loss: 0.00002463
Iteration 45/1000 | Loss: 0.00002463
Iteration 46/1000 | Loss: 0.00002463
Iteration 47/1000 | Loss: 0.00002462
Iteration 48/1000 | Loss: 0.00002462
Iteration 49/1000 | Loss: 0.00002461
Iteration 50/1000 | Loss: 0.00002460
Iteration 51/1000 | Loss: 0.00002460
Iteration 52/1000 | Loss: 0.00002460
Iteration 53/1000 | Loss: 0.00002460
Iteration 54/1000 | Loss: 0.00002460
Iteration 55/1000 | Loss: 0.00002459
Iteration 56/1000 | Loss: 0.00002459
Iteration 57/1000 | Loss: 0.00002459
Iteration 58/1000 | Loss: 0.00002459
Iteration 59/1000 | Loss: 0.00002459
Iteration 60/1000 | Loss: 0.00002459
Iteration 61/1000 | Loss: 0.00002459
Iteration 62/1000 | Loss: 0.00002459
Iteration 63/1000 | Loss: 0.00002459
Iteration 64/1000 | Loss: 0.00002459
Iteration 65/1000 | Loss: 0.00002459
Iteration 66/1000 | Loss: 0.00002459
Iteration 67/1000 | Loss: 0.00002458
Iteration 68/1000 | Loss: 0.00002457
Iteration 69/1000 | Loss: 0.00002457
Iteration 70/1000 | Loss: 0.00002457
Iteration 71/1000 | Loss: 0.00002457
Iteration 72/1000 | Loss: 0.00002457
Iteration 73/1000 | Loss: 0.00002457
Iteration 74/1000 | Loss: 0.00002457
Iteration 75/1000 | Loss: 0.00002456
Iteration 76/1000 | Loss: 0.00002456
Iteration 77/1000 | Loss: 0.00002456
Iteration 78/1000 | Loss: 0.00002456
Iteration 79/1000 | Loss: 0.00002455
Iteration 80/1000 | Loss: 0.00002455
Iteration 81/1000 | Loss: 0.00002455
Iteration 82/1000 | Loss: 0.00002455
Iteration 83/1000 | Loss: 0.00002455
Iteration 84/1000 | Loss: 0.00002455
Iteration 85/1000 | Loss: 0.00002454
Iteration 86/1000 | Loss: 0.00002454
Iteration 87/1000 | Loss: 0.00002454
Iteration 88/1000 | Loss: 0.00002454
Iteration 89/1000 | Loss: 0.00002454
Iteration 90/1000 | Loss: 0.00002453
Iteration 91/1000 | Loss: 0.00002453
Iteration 92/1000 | Loss: 0.00002453
Iteration 93/1000 | Loss: 0.00002453
Iteration 94/1000 | Loss: 0.00002453
Iteration 95/1000 | Loss: 0.00002453
Iteration 96/1000 | Loss: 0.00002453
Iteration 97/1000 | Loss: 0.00002453
Iteration 98/1000 | Loss: 0.00002453
Iteration 99/1000 | Loss: 0.00002453
Iteration 100/1000 | Loss: 0.00002453
Iteration 101/1000 | Loss: 0.00002453
Iteration 102/1000 | Loss: 0.00002452
Iteration 103/1000 | Loss: 0.00002452
Iteration 104/1000 | Loss: 0.00002452
Iteration 105/1000 | Loss: 0.00002452
Iteration 106/1000 | Loss: 0.00002452
Iteration 107/1000 | Loss: 0.00002452
Iteration 108/1000 | Loss: 0.00002452
Iteration 109/1000 | Loss: 0.00002452
Iteration 110/1000 | Loss: 0.00002452
Iteration 111/1000 | Loss: 0.00002451
Iteration 112/1000 | Loss: 0.00002451
Iteration 113/1000 | Loss: 0.00002451
Iteration 114/1000 | Loss: 0.00002451
Iteration 115/1000 | Loss: 0.00002451
Iteration 116/1000 | Loss: 0.00002451
Iteration 117/1000 | Loss: 0.00002451
Iteration 118/1000 | Loss: 0.00002451
Iteration 119/1000 | Loss: 0.00002451
Iteration 120/1000 | Loss: 0.00002451
Iteration 121/1000 | Loss: 0.00002451
Iteration 122/1000 | Loss: 0.00002451
Iteration 123/1000 | Loss: 0.00002451
Iteration 124/1000 | Loss: 0.00002451
Iteration 125/1000 | Loss: 0.00002451
Iteration 126/1000 | Loss: 0.00002451
Iteration 127/1000 | Loss: 0.00002451
Iteration 128/1000 | Loss: 0.00002451
Iteration 129/1000 | Loss: 0.00002451
Iteration 130/1000 | Loss: 0.00002451
Iteration 131/1000 | Loss: 0.00002451
Iteration 132/1000 | Loss: 0.00002451
Iteration 133/1000 | Loss: 0.00002451
Iteration 134/1000 | Loss: 0.00002451
Iteration 135/1000 | Loss: 0.00002451
Iteration 136/1000 | Loss: 0.00002451
Iteration 137/1000 | Loss: 0.00002451
Iteration 138/1000 | Loss: 0.00002451
Iteration 139/1000 | Loss: 0.00002451
Iteration 140/1000 | Loss: 0.00002451
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 140. Stopping optimization.
Last 5 losses: [2.451003820169717e-05, 2.451003820169717e-05, 2.451003820169717e-05, 2.451003820169717e-05, 2.451003820169717e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.451003820169717e-05

Optimization complete. Final v2v error: 4.0808305740356445 mm

Highest mean error: 4.922419548034668 mm for frame 26

Lowest mean error: 3.1045286655426025 mm for frame 1

Saving results

Total time: 45.28161668777466
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_025/1008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_025/1008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_025/1008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00609918
Iteration 2/25 | Loss: 0.00117627
Iteration 3/25 | Loss: 0.00111399
Iteration 4/25 | Loss: 0.00110372
Iteration 5/25 | Loss: 0.00110023
Iteration 6/25 | Loss: 0.00109976
Iteration 7/25 | Loss: 0.00109976
Iteration 8/25 | Loss: 0.00109976
Iteration 9/25 | Loss: 0.00109976
Iteration 10/25 | Loss: 0.00109976
Iteration 11/25 | Loss: 0.00109976
Iteration 12/25 | Loss: 0.00109976
Iteration 13/25 | Loss: 0.00109976
Iteration 14/25 | Loss: 0.00109976
Iteration 15/25 | Loss: 0.00109976
Iteration 16/25 | Loss: 0.00109976
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0010997647186741233, 0.0010997647186741233, 0.0010997647186741233, 0.0010997647186741233, 0.0010997647186741233]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010997647186741233

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.64797544
Iteration 2/25 | Loss: 0.00076661
Iteration 3/25 | Loss: 0.00076661
Iteration 4/25 | Loss: 0.00076661
Iteration 5/25 | Loss: 0.00076661
Iteration 6/25 | Loss: 0.00076661
Iteration 7/25 | Loss: 0.00076661
Iteration 8/25 | Loss: 0.00076661
Iteration 9/25 | Loss: 0.00076661
Iteration 10/25 | Loss: 0.00076661
Iteration 11/25 | Loss: 0.00076661
Iteration 12/25 | Loss: 0.00076661
Iteration 13/25 | Loss: 0.00076661
Iteration 14/25 | Loss: 0.00076661
Iteration 15/25 | Loss: 0.00076661
Iteration 16/25 | Loss: 0.00076661
Iteration 17/25 | Loss: 0.00076661
Iteration 18/25 | Loss: 0.00076661
Iteration 19/25 | Loss: 0.00076661
Iteration 20/25 | Loss: 0.00076661
Iteration 21/25 | Loss: 0.00076661
Iteration 22/25 | Loss: 0.00076661
Iteration 23/25 | Loss: 0.00076661
Iteration 24/25 | Loss: 0.00076661
Iteration 25/25 | Loss: 0.00076661

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00076661
Iteration 2/1000 | Loss: 0.00002101
Iteration 3/1000 | Loss: 0.00001575
Iteration 4/1000 | Loss: 0.00001436
Iteration 5/1000 | Loss: 0.00001346
Iteration 6/1000 | Loss: 0.00001286
Iteration 7/1000 | Loss: 0.00001233
Iteration 8/1000 | Loss: 0.00001214
Iteration 9/1000 | Loss: 0.00001214
Iteration 10/1000 | Loss: 0.00001176
Iteration 11/1000 | Loss: 0.00001153
Iteration 12/1000 | Loss: 0.00001151
Iteration 13/1000 | Loss: 0.00001142
Iteration 14/1000 | Loss: 0.00001141
Iteration 15/1000 | Loss: 0.00001139
Iteration 16/1000 | Loss: 0.00001129
Iteration 17/1000 | Loss: 0.00001120
Iteration 18/1000 | Loss: 0.00001113
Iteration 19/1000 | Loss: 0.00001112
Iteration 20/1000 | Loss: 0.00001111
Iteration 21/1000 | Loss: 0.00001111
Iteration 22/1000 | Loss: 0.00001110
Iteration 23/1000 | Loss: 0.00001107
Iteration 24/1000 | Loss: 0.00001107
Iteration 25/1000 | Loss: 0.00001106
Iteration 26/1000 | Loss: 0.00001106
Iteration 27/1000 | Loss: 0.00001105
Iteration 28/1000 | Loss: 0.00001104
Iteration 29/1000 | Loss: 0.00001104
Iteration 30/1000 | Loss: 0.00001103
Iteration 31/1000 | Loss: 0.00001103
Iteration 32/1000 | Loss: 0.00001102
Iteration 33/1000 | Loss: 0.00001102
Iteration 34/1000 | Loss: 0.00001102
Iteration 35/1000 | Loss: 0.00001100
Iteration 36/1000 | Loss: 0.00001099
Iteration 37/1000 | Loss: 0.00001097
Iteration 38/1000 | Loss: 0.00001096
Iteration 39/1000 | Loss: 0.00001095
Iteration 40/1000 | Loss: 0.00001095
Iteration 41/1000 | Loss: 0.00001094
Iteration 42/1000 | Loss: 0.00001094
Iteration 43/1000 | Loss: 0.00001090
Iteration 44/1000 | Loss: 0.00001090
Iteration 45/1000 | Loss: 0.00001088
Iteration 46/1000 | Loss: 0.00001083
Iteration 47/1000 | Loss: 0.00001083
Iteration 48/1000 | Loss: 0.00001083
Iteration 49/1000 | Loss: 0.00001083
Iteration 50/1000 | Loss: 0.00001083
Iteration 51/1000 | Loss: 0.00001083
Iteration 52/1000 | Loss: 0.00001082
Iteration 53/1000 | Loss: 0.00001082
Iteration 54/1000 | Loss: 0.00001080
Iteration 55/1000 | Loss: 0.00001079
Iteration 56/1000 | Loss: 0.00001079
Iteration 57/1000 | Loss: 0.00001079
Iteration 58/1000 | Loss: 0.00001079
Iteration 59/1000 | Loss: 0.00001078
Iteration 60/1000 | Loss: 0.00001078
Iteration 61/1000 | Loss: 0.00001078
Iteration 62/1000 | Loss: 0.00001078
Iteration 63/1000 | Loss: 0.00001077
Iteration 64/1000 | Loss: 0.00001076
Iteration 65/1000 | Loss: 0.00001076
Iteration 66/1000 | Loss: 0.00001076
Iteration 67/1000 | Loss: 0.00001076
Iteration 68/1000 | Loss: 0.00001075
Iteration 69/1000 | Loss: 0.00001075
Iteration 70/1000 | Loss: 0.00001075
Iteration 71/1000 | Loss: 0.00001075
Iteration 72/1000 | Loss: 0.00001075
Iteration 73/1000 | Loss: 0.00001075
Iteration 74/1000 | Loss: 0.00001075
Iteration 75/1000 | Loss: 0.00001075
Iteration 76/1000 | Loss: 0.00001074
Iteration 77/1000 | Loss: 0.00001074
Iteration 78/1000 | Loss: 0.00001074
Iteration 79/1000 | Loss: 0.00001074
Iteration 80/1000 | Loss: 0.00001074
Iteration 81/1000 | Loss: 0.00001074
Iteration 82/1000 | Loss: 0.00001073
Iteration 83/1000 | Loss: 0.00001073
Iteration 84/1000 | Loss: 0.00001073
Iteration 85/1000 | Loss: 0.00001072
Iteration 86/1000 | Loss: 0.00001072
Iteration 87/1000 | Loss: 0.00001071
Iteration 88/1000 | Loss: 0.00001071
Iteration 89/1000 | Loss: 0.00001071
Iteration 90/1000 | Loss: 0.00001071
Iteration 91/1000 | Loss: 0.00001071
Iteration 92/1000 | Loss: 0.00001071
Iteration 93/1000 | Loss: 0.00001071
Iteration 94/1000 | Loss: 0.00001071
Iteration 95/1000 | Loss: 0.00001071
Iteration 96/1000 | Loss: 0.00001071
Iteration 97/1000 | Loss: 0.00001071
Iteration 98/1000 | Loss: 0.00001069
Iteration 99/1000 | Loss: 0.00001068
Iteration 100/1000 | Loss: 0.00001068
Iteration 101/1000 | Loss: 0.00001068
Iteration 102/1000 | Loss: 0.00001067
Iteration 103/1000 | Loss: 0.00001066
Iteration 104/1000 | Loss: 0.00001066
Iteration 105/1000 | Loss: 0.00001066
Iteration 106/1000 | Loss: 0.00001065
Iteration 107/1000 | Loss: 0.00001065
Iteration 108/1000 | Loss: 0.00001065
Iteration 109/1000 | Loss: 0.00001065
Iteration 110/1000 | Loss: 0.00001064
Iteration 111/1000 | Loss: 0.00001064
Iteration 112/1000 | Loss: 0.00001064
Iteration 113/1000 | Loss: 0.00001064
Iteration 114/1000 | Loss: 0.00001064
Iteration 115/1000 | Loss: 0.00001064
Iteration 116/1000 | Loss: 0.00001064
Iteration 117/1000 | Loss: 0.00001064
Iteration 118/1000 | Loss: 0.00001064
Iteration 119/1000 | Loss: 0.00001064
Iteration 120/1000 | Loss: 0.00001063
Iteration 121/1000 | Loss: 0.00001063
Iteration 122/1000 | Loss: 0.00001063
Iteration 123/1000 | Loss: 0.00001063
Iteration 124/1000 | Loss: 0.00001063
Iteration 125/1000 | Loss: 0.00001063
Iteration 126/1000 | Loss: 0.00001063
Iteration 127/1000 | Loss: 0.00001063
Iteration 128/1000 | Loss: 0.00001063
Iteration 129/1000 | Loss: 0.00001063
Iteration 130/1000 | Loss: 0.00001062
Iteration 131/1000 | Loss: 0.00001062
Iteration 132/1000 | Loss: 0.00001062
Iteration 133/1000 | Loss: 0.00001062
Iteration 134/1000 | Loss: 0.00001062
Iteration 135/1000 | Loss: 0.00001062
Iteration 136/1000 | Loss: 0.00001062
Iteration 137/1000 | Loss: 0.00001061
Iteration 138/1000 | Loss: 0.00001061
Iteration 139/1000 | Loss: 0.00001061
Iteration 140/1000 | Loss: 0.00001061
Iteration 141/1000 | Loss: 0.00001061
Iteration 142/1000 | Loss: 0.00001061
Iteration 143/1000 | Loss: 0.00001061
Iteration 144/1000 | Loss: 0.00001061
Iteration 145/1000 | Loss: 0.00001061
Iteration 146/1000 | Loss: 0.00001061
Iteration 147/1000 | Loss: 0.00001061
Iteration 148/1000 | Loss: 0.00001061
Iteration 149/1000 | Loss: 0.00001061
Iteration 150/1000 | Loss: 0.00001061
Iteration 151/1000 | Loss: 0.00001061
Iteration 152/1000 | Loss: 0.00001061
Iteration 153/1000 | Loss: 0.00001060
Iteration 154/1000 | Loss: 0.00001060
Iteration 155/1000 | Loss: 0.00001060
Iteration 156/1000 | Loss: 0.00001060
Iteration 157/1000 | Loss: 0.00001060
Iteration 158/1000 | Loss: 0.00001060
Iteration 159/1000 | Loss: 0.00001060
Iteration 160/1000 | Loss: 0.00001059
Iteration 161/1000 | Loss: 0.00001059
Iteration 162/1000 | Loss: 0.00001059
Iteration 163/1000 | Loss: 0.00001059
Iteration 164/1000 | Loss: 0.00001059
Iteration 165/1000 | Loss: 0.00001059
Iteration 166/1000 | Loss: 0.00001059
Iteration 167/1000 | Loss: 0.00001059
Iteration 168/1000 | Loss: 0.00001059
Iteration 169/1000 | Loss: 0.00001059
Iteration 170/1000 | Loss: 0.00001059
Iteration 171/1000 | Loss: 0.00001059
Iteration 172/1000 | Loss: 0.00001059
Iteration 173/1000 | Loss: 0.00001059
Iteration 174/1000 | Loss: 0.00001059
Iteration 175/1000 | Loss: 0.00001059
Iteration 176/1000 | Loss: 0.00001059
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 176. Stopping optimization.
Last 5 losses: [1.0590208148641977e-05, 1.0590208148641977e-05, 1.0590208148641977e-05, 1.0590208148641977e-05, 1.0590208148641977e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0590208148641977e-05

Optimization complete. Final v2v error: 2.8140909671783447 mm

Highest mean error: 3.1271941661834717 mm for frame 55

Lowest mean error: 2.6414318084716797 mm for frame 183

Saving results

Total time: 41.03892111778259
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_025/1052/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_025/1052.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_025/1052
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00974444
Iteration 2/25 | Loss: 0.00483191
Iteration 3/25 | Loss: 0.00289705
Iteration 4/25 | Loss: 0.00249824
Iteration 5/25 | Loss: 0.00223119
Iteration 6/25 | Loss: 0.00219308
Iteration 7/25 | Loss: 0.00201101
Iteration 8/25 | Loss: 0.00201432
Iteration 9/25 | Loss: 0.00189655
Iteration 10/25 | Loss: 0.00183161
Iteration 11/25 | Loss: 0.00174950
Iteration 12/25 | Loss: 0.00176013
Iteration 13/25 | Loss: 0.00174494
Iteration 14/25 | Loss: 0.00170566
Iteration 15/25 | Loss: 0.00170684
Iteration 16/25 | Loss: 0.00168534
Iteration 17/25 | Loss: 0.00170543
Iteration 18/25 | Loss: 0.00169453
Iteration 19/25 | Loss: 0.00168721
Iteration 20/25 | Loss: 0.00168652
Iteration 21/25 | Loss: 0.00168579
Iteration 22/25 | Loss: 0.00166512
Iteration 23/25 | Loss: 0.00166048
Iteration 24/25 | Loss: 0.00166648
Iteration 25/25 | Loss: 0.00166491

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33845627
Iteration 2/25 | Loss: 0.00776248
Iteration 3/25 | Loss: 0.00301221
Iteration 4/25 | Loss: 0.00301221
Iteration 5/25 | Loss: 0.00301221
Iteration 6/25 | Loss: 0.00301220
Iteration 7/25 | Loss: 0.00301220
Iteration 8/25 | Loss: 0.00301220
Iteration 9/25 | Loss: 0.00301220
Iteration 10/25 | Loss: 0.00301220
Iteration 11/25 | Loss: 0.00301220
Iteration 12/25 | Loss: 0.00301220
Iteration 13/25 | Loss: 0.00301220
Iteration 14/25 | Loss: 0.00301220
Iteration 15/25 | Loss: 0.00301220
Iteration 16/25 | Loss: 0.00301220
Iteration 17/25 | Loss: 0.00301220
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.003012202912941575, 0.003012202912941575, 0.003012202912941575, 0.003012202912941575, 0.003012202912941575]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.003012202912941575

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00301220
Iteration 2/1000 | Loss: 0.00626266
Iteration 3/1000 | Loss: 0.00036102
Iteration 4/1000 | Loss: 0.00033131
Iteration 5/1000 | Loss: 0.00167222
Iteration 6/1000 | Loss: 0.00037798
Iteration 7/1000 | Loss: 0.00041647
Iteration 8/1000 | Loss: 0.00023952
Iteration 9/1000 | Loss: 0.00093961
Iteration 10/1000 | Loss: 0.00029608
Iteration 11/1000 | Loss: 0.00021994
Iteration 12/1000 | Loss: 0.00050349
Iteration 13/1000 | Loss: 0.00826961
Iteration 14/1000 | Loss: 0.00028027
Iteration 15/1000 | Loss: 0.00058746
Iteration 16/1000 | Loss: 0.00063574
Iteration 17/1000 | Loss: 0.00042603
Iteration 18/1000 | Loss: 0.00102946
Iteration 19/1000 | Loss: 0.00068976
Iteration 20/1000 | Loss: 0.00341199
Iteration 21/1000 | Loss: 0.00053174
Iteration 22/1000 | Loss: 0.00039250
Iteration 23/1000 | Loss: 0.00031843
Iteration 24/1000 | Loss: 0.00014536
Iteration 25/1000 | Loss: 0.00030324
Iteration 26/1000 | Loss: 0.00013021
Iteration 27/1000 | Loss: 0.00003251
Iteration 28/1000 | Loss: 0.00026334
Iteration 29/1000 | Loss: 0.00028360
Iteration 30/1000 | Loss: 0.00148292
Iteration 31/1000 | Loss: 0.00008912
Iteration 32/1000 | Loss: 0.00006907
Iteration 33/1000 | Loss: 0.00002186
Iteration 34/1000 | Loss: 0.00033120
Iteration 35/1000 | Loss: 0.00001898
Iteration 36/1000 | Loss: 0.00026329
Iteration 37/1000 | Loss: 0.00001721
Iteration 38/1000 | Loss: 0.00015012
Iteration 39/1000 | Loss: 0.00009960
Iteration 40/1000 | Loss: 0.00001561
Iteration 41/1000 | Loss: 0.00007387
Iteration 42/1000 | Loss: 0.00023123
Iteration 43/1000 | Loss: 0.00003880
Iteration 44/1000 | Loss: 0.00001456
Iteration 45/1000 | Loss: 0.00001452
Iteration 46/1000 | Loss: 0.00001424
Iteration 47/1000 | Loss: 0.00001393
Iteration 48/1000 | Loss: 0.00001375
Iteration 49/1000 | Loss: 0.00001373
Iteration 50/1000 | Loss: 0.00001358
Iteration 51/1000 | Loss: 0.00001347
Iteration 52/1000 | Loss: 0.00001346
Iteration 53/1000 | Loss: 0.00001346
Iteration 54/1000 | Loss: 0.00001346
Iteration 55/1000 | Loss: 0.00001346
Iteration 56/1000 | Loss: 0.00001346
Iteration 57/1000 | Loss: 0.00001346
Iteration 58/1000 | Loss: 0.00001346
Iteration 59/1000 | Loss: 0.00001346
Iteration 60/1000 | Loss: 0.00001345
Iteration 61/1000 | Loss: 0.00001345
Iteration 62/1000 | Loss: 0.00001345
Iteration 63/1000 | Loss: 0.00001345
Iteration 64/1000 | Loss: 0.00001345
Iteration 65/1000 | Loss: 0.00001345
Iteration 66/1000 | Loss: 0.00001345
Iteration 67/1000 | Loss: 0.00001345
Iteration 68/1000 | Loss: 0.00001345
Iteration 69/1000 | Loss: 0.00001344
Iteration 70/1000 | Loss: 0.00001344
Iteration 71/1000 | Loss: 0.00001344
Iteration 72/1000 | Loss: 0.00001344
Iteration 73/1000 | Loss: 0.00001344
Iteration 74/1000 | Loss: 0.00001343
Iteration 75/1000 | Loss: 0.00001342
Iteration 76/1000 | Loss: 0.00001342
Iteration 77/1000 | Loss: 0.00001341
Iteration 78/1000 | Loss: 0.00001341
Iteration 79/1000 | Loss: 0.00001341
Iteration 80/1000 | Loss: 0.00001341
Iteration 81/1000 | Loss: 0.00001341
Iteration 82/1000 | Loss: 0.00001340
Iteration 83/1000 | Loss: 0.00001340
Iteration 84/1000 | Loss: 0.00001340
Iteration 85/1000 | Loss: 0.00001339
Iteration 86/1000 | Loss: 0.00001339
Iteration 87/1000 | Loss: 0.00001338
Iteration 88/1000 | Loss: 0.00001338
Iteration 89/1000 | Loss: 0.00001338
Iteration 90/1000 | Loss: 0.00001337
Iteration 91/1000 | Loss: 0.00001337
Iteration 92/1000 | Loss: 0.00001337
Iteration 93/1000 | Loss: 0.00001336
Iteration 94/1000 | Loss: 0.00001336
Iteration 95/1000 | Loss: 0.00001336
Iteration 96/1000 | Loss: 0.00001336
Iteration 97/1000 | Loss: 0.00001336
Iteration 98/1000 | Loss: 0.00001336
Iteration 99/1000 | Loss: 0.00011205
Iteration 100/1000 | Loss: 0.00080527
Iteration 101/1000 | Loss: 0.00004379
Iteration 102/1000 | Loss: 0.00009261
Iteration 103/1000 | Loss: 0.00004828
Iteration 104/1000 | Loss: 0.00002173
Iteration 105/1000 | Loss: 0.00001349
Iteration 106/1000 | Loss: 0.00001338
Iteration 107/1000 | Loss: 0.00001335
Iteration 108/1000 | Loss: 0.00007108
Iteration 109/1000 | Loss: 0.00001627
Iteration 110/1000 | Loss: 0.00001356
Iteration 111/1000 | Loss: 0.00001334
Iteration 112/1000 | Loss: 0.00001326
Iteration 113/1000 | Loss: 0.00001326
Iteration 114/1000 | Loss: 0.00001326
Iteration 115/1000 | Loss: 0.00001326
Iteration 116/1000 | Loss: 0.00001326
Iteration 117/1000 | Loss: 0.00001326
Iteration 118/1000 | Loss: 0.00001326
Iteration 119/1000 | Loss: 0.00001325
Iteration 120/1000 | Loss: 0.00001325
Iteration 121/1000 | Loss: 0.00001325
Iteration 122/1000 | Loss: 0.00001325
Iteration 123/1000 | Loss: 0.00001325
Iteration 124/1000 | Loss: 0.00001325
Iteration 125/1000 | Loss: 0.00001325
Iteration 126/1000 | Loss: 0.00001325
Iteration 127/1000 | Loss: 0.00001325
Iteration 128/1000 | Loss: 0.00001325
Iteration 129/1000 | Loss: 0.00001325
Iteration 130/1000 | Loss: 0.00001325
Iteration 131/1000 | Loss: 0.00001324
Iteration 132/1000 | Loss: 0.00001324
Iteration 133/1000 | Loss: 0.00001324
Iteration 134/1000 | Loss: 0.00001324
Iteration 135/1000 | Loss: 0.00001324
Iteration 136/1000 | Loss: 0.00001324
Iteration 137/1000 | Loss: 0.00001324
Iteration 138/1000 | Loss: 0.00001324
Iteration 139/1000 | Loss: 0.00001324
Iteration 140/1000 | Loss: 0.00001324
Iteration 141/1000 | Loss: 0.00001324
Iteration 142/1000 | Loss: 0.00001324
Iteration 143/1000 | Loss: 0.00001324
Iteration 144/1000 | Loss: 0.00001324
Iteration 145/1000 | Loss: 0.00001324
Iteration 146/1000 | Loss: 0.00001324
Iteration 147/1000 | Loss: 0.00001324
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 147. Stopping optimization.
Last 5 losses: [1.3239929103292525e-05, 1.3239929103292525e-05, 1.3239929103292525e-05, 1.3239929103292525e-05, 1.3239929103292525e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3239929103292525e-05

Optimization complete. Final v2v error: 3.0861363410949707 mm

Highest mean error: 3.3240787982940674 mm for frame 89

Lowest mean error: 2.8361029624938965 mm for frame 116

Saving results

Total time: 134.09449124336243
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_025/1029/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_025/1029.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_025/1029
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00834474
Iteration 2/25 | Loss: 0.00142349
Iteration 3/25 | Loss: 0.00119096
Iteration 4/25 | Loss: 0.00115618
Iteration 5/25 | Loss: 0.00115146
Iteration 6/25 | Loss: 0.00115046
Iteration 7/25 | Loss: 0.00115046
Iteration 8/25 | Loss: 0.00115046
Iteration 9/25 | Loss: 0.00115046
Iteration 10/25 | Loss: 0.00115046
Iteration 11/25 | Loss: 0.00115046
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011504596332088113, 0.0011504596332088113, 0.0011504596332088113, 0.0011504596332088113, 0.0011504596332088113]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011504596332088113

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36373794
Iteration 2/25 | Loss: 0.00083697
Iteration 3/25 | Loss: 0.00083697
Iteration 4/25 | Loss: 0.00083697
Iteration 5/25 | Loss: 0.00083697
Iteration 6/25 | Loss: 0.00083697
Iteration 7/25 | Loss: 0.00083697
Iteration 8/25 | Loss: 0.00083697
Iteration 9/25 | Loss: 0.00083697
Iteration 10/25 | Loss: 0.00083697
Iteration 11/25 | Loss: 0.00083697
Iteration 12/25 | Loss: 0.00083697
Iteration 13/25 | Loss: 0.00083697
Iteration 14/25 | Loss: 0.00083697
Iteration 15/25 | Loss: 0.00083697
Iteration 16/25 | Loss: 0.00083697
Iteration 17/25 | Loss: 0.00083697
Iteration 18/25 | Loss: 0.00083697
Iteration 19/25 | Loss: 0.00083697
Iteration 20/25 | Loss: 0.00083697
Iteration 21/25 | Loss: 0.00083697
Iteration 22/25 | Loss: 0.00083697
Iteration 23/25 | Loss: 0.00083697
Iteration 24/25 | Loss: 0.00083697
Iteration 25/25 | Loss: 0.00083697

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00083697
Iteration 2/1000 | Loss: 0.00003322
Iteration 3/1000 | Loss: 0.00001909
Iteration 4/1000 | Loss: 0.00001638
Iteration 5/1000 | Loss: 0.00001512
Iteration 6/1000 | Loss: 0.00001412
Iteration 7/1000 | Loss: 0.00001357
Iteration 8/1000 | Loss: 0.00001318
Iteration 9/1000 | Loss: 0.00001287
Iteration 10/1000 | Loss: 0.00001255
Iteration 11/1000 | Loss: 0.00001243
Iteration 12/1000 | Loss: 0.00001233
Iteration 13/1000 | Loss: 0.00001230
Iteration 14/1000 | Loss: 0.00001230
Iteration 15/1000 | Loss: 0.00001228
Iteration 16/1000 | Loss: 0.00001228
Iteration 17/1000 | Loss: 0.00001226
Iteration 18/1000 | Loss: 0.00001221
Iteration 19/1000 | Loss: 0.00001220
Iteration 20/1000 | Loss: 0.00001220
Iteration 21/1000 | Loss: 0.00001219
Iteration 22/1000 | Loss: 0.00001216
Iteration 23/1000 | Loss: 0.00001213
Iteration 24/1000 | Loss: 0.00001211
Iteration 25/1000 | Loss: 0.00001208
Iteration 26/1000 | Loss: 0.00001208
Iteration 27/1000 | Loss: 0.00001207
Iteration 28/1000 | Loss: 0.00001207
Iteration 29/1000 | Loss: 0.00001207
Iteration 30/1000 | Loss: 0.00001206
Iteration 31/1000 | Loss: 0.00001205
Iteration 32/1000 | Loss: 0.00001205
Iteration 33/1000 | Loss: 0.00001204
Iteration 34/1000 | Loss: 0.00001204
Iteration 35/1000 | Loss: 0.00001204
Iteration 36/1000 | Loss: 0.00001204
Iteration 37/1000 | Loss: 0.00001203
Iteration 38/1000 | Loss: 0.00001203
Iteration 39/1000 | Loss: 0.00001203
Iteration 40/1000 | Loss: 0.00001203
Iteration 41/1000 | Loss: 0.00001202
Iteration 42/1000 | Loss: 0.00001202
Iteration 43/1000 | Loss: 0.00001202
Iteration 44/1000 | Loss: 0.00001201
Iteration 45/1000 | Loss: 0.00001201
Iteration 46/1000 | Loss: 0.00001200
Iteration 47/1000 | Loss: 0.00001200
Iteration 48/1000 | Loss: 0.00001200
Iteration 49/1000 | Loss: 0.00001199
Iteration 50/1000 | Loss: 0.00001199
Iteration 51/1000 | Loss: 0.00001199
Iteration 52/1000 | Loss: 0.00001198
Iteration 53/1000 | Loss: 0.00001198
Iteration 54/1000 | Loss: 0.00001198
Iteration 55/1000 | Loss: 0.00001197
Iteration 56/1000 | Loss: 0.00001197
Iteration 57/1000 | Loss: 0.00001196
Iteration 58/1000 | Loss: 0.00001196
Iteration 59/1000 | Loss: 0.00001196
Iteration 60/1000 | Loss: 0.00001196
Iteration 61/1000 | Loss: 0.00001196
Iteration 62/1000 | Loss: 0.00001196
Iteration 63/1000 | Loss: 0.00001196
Iteration 64/1000 | Loss: 0.00001196
Iteration 65/1000 | Loss: 0.00001196
Iteration 66/1000 | Loss: 0.00001196
Iteration 67/1000 | Loss: 0.00001196
Iteration 68/1000 | Loss: 0.00001196
Iteration 69/1000 | Loss: 0.00001196
Iteration 70/1000 | Loss: 0.00001195
Iteration 71/1000 | Loss: 0.00001195
Iteration 72/1000 | Loss: 0.00001195
Iteration 73/1000 | Loss: 0.00001195
Iteration 74/1000 | Loss: 0.00001195
Iteration 75/1000 | Loss: 0.00001194
Iteration 76/1000 | Loss: 0.00001194
Iteration 77/1000 | Loss: 0.00001194
Iteration 78/1000 | Loss: 0.00001194
Iteration 79/1000 | Loss: 0.00001194
Iteration 80/1000 | Loss: 0.00001193
Iteration 81/1000 | Loss: 0.00001193
Iteration 82/1000 | Loss: 0.00001193
Iteration 83/1000 | Loss: 0.00001193
Iteration 84/1000 | Loss: 0.00001193
Iteration 85/1000 | Loss: 0.00001192
Iteration 86/1000 | Loss: 0.00001192
Iteration 87/1000 | Loss: 0.00001192
Iteration 88/1000 | Loss: 0.00001191
Iteration 89/1000 | Loss: 0.00001191
Iteration 90/1000 | Loss: 0.00001191
Iteration 91/1000 | Loss: 0.00001191
Iteration 92/1000 | Loss: 0.00001190
Iteration 93/1000 | Loss: 0.00001190
Iteration 94/1000 | Loss: 0.00001190
Iteration 95/1000 | Loss: 0.00001189
Iteration 96/1000 | Loss: 0.00001189
Iteration 97/1000 | Loss: 0.00001189
Iteration 98/1000 | Loss: 0.00001189
Iteration 99/1000 | Loss: 0.00001189
Iteration 100/1000 | Loss: 0.00001189
Iteration 101/1000 | Loss: 0.00001189
Iteration 102/1000 | Loss: 0.00001188
Iteration 103/1000 | Loss: 0.00001188
Iteration 104/1000 | Loss: 0.00001188
Iteration 105/1000 | Loss: 0.00001188
Iteration 106/1000 | Loss: 0.00001187
Iteration 107/1000 | Loss: 0.00001187
Iteration 108/1000 | Loss: 0.00001187
Iteration 109/1000 | Loss: 0.00001187
Iteration 110/1000 | Loss: 0.00001186
Iteration 111/1000 | Loss: 0.00001186
Iteration 112/1000 | Loss: 0.00001186
Iteration 113/1000 | Loss: 0.00001186
Iteration 114/1000 | Loss: 0.00001186
Iteration 115/1000 | Loss: 0.00001185
Iteration 116/1000 | Loss: 0.00001185
Iteration 117/1000 | Loss: 0.00001185
Iteration 118/1000 | Loss: 0.00001184
Iteration 119/1000 | Loss: 0.00001184
Iteration 120/1000 | Loss: 0.00001184
Iteration 121/1000 | Loss: 0.00001184
Iteration 122/1000 | Loss: 0.00001184
Iteration 123/1000 | Loss: 0.00001184
Iteration 124/1000 | Loss: 0.00001184
Iteration 125/1000 | Loss: 0.00001184
Iteration 126/1000 | Loss: 0.00001184
Iteration 127/1000 | Loss: 0.00001184
Iteration 128/1000 | Loss: 0.00001183
Iteration 129/1000 | Loss: 0.00001183
Iteration 130/1000 | Loss: 0.00001183
Iteration 131/1000 | Loss: 0.00001183
Iteration 132/1000 | Loss: 0.00001183
Iteration 133/1000 | Loss: 0.00001183
Iteration 134/1000 | Loss: 0.00001182
Iteration 135/1000 | Loss: 0.00001182
Iteration 136/1000 | Loss: 0.00001182
Iteration 137/1000 | Loss: 0.00001182
Iteration 138/1000 | Loss: 0.00001181
Iteration 139/1000 | Loss: 0.00001181
Iteration 140/1000 | Loss: 0.00001181
Iteration 141/1000 | Loss: 0.00001181
Iteration 142/1000 | Loss: 0.00001181
Iteration 143/1000 | Loss: 0.00001181
Iteration 144/1000 | Loss: 0.00001181
Iteration 145/1000 | Loss: 0.00001181
Iteration 146/1000 | Loss: 0.00001181
Iteration 147/1000 | Loss: 0.00001181
Iteration 148/1000 | Loss: 0.00001181
Iteration 149/1000 | Loss: 0.00001181
Iteration 150/1000 | Loss: 0.00001181
Iteration 151/1000 | Loss: 0.00001181
Iteration 152/1000 | Loss: 0.00001181
Iteration 153/1000 | Loss: 0.00001181
Iteration 154/1000 | Loss: 0.00001181
Iteration 155/1000 | Loss: 0.00001181
Iteration 156/1000 | Loss: 0.00001181
Iteration 157/1000 | Loss: 0.00001181
Iteration 158/1000 | Loss: 0.00001181
Iteration 159/1000 | Loss: 0.00001181
Iteration 160/1000 | Loss: 0.00001181
Iteration 161/1000 | Loss: 0.00001181
Iteration 162/1000 | Loss: 0.00001181
Iteration 163/1000 | Loss: 0.00001181
Iteration 164/1000 | Loss: 0.00001181
Iteration 165/1000 | Loss: 0.00001181
Iteration 166/1000 | Loss: 0.00001181
Iteration 167/1000 | Loss: 0.00001181
Iteration 168/1000 | Loss: 0.00001181
Iteration 169/1000 | Loss: 0.00001181
Iteration 170/1000 | Loss: 0.00001181
Iteration 171/1000 | Loss: 0.00001181
Iteration 172/1000 | Loss: 0.00001181
Iteration 173/1000 | Loss: 0.00001181
Iteration 174/1000 | Loss: 0.00001181
Iteration 175/1000 | Loss: 0.00001181
Iteration 176/1000 | Loss: 0.00001181
Iteration 177/1000 | Loss: 0.00001181
Iteration 178/1000 | Loss: 0.00001181
Iteration 179/1000 | Loss: 0.00001181
Iteration 180/1000 | Loss: 0.00001181
Iteration 181/1000 | Loss: 0.00001181
Iteration 182/1000 | Loss: 0.00001181
Iteration 183/1000 | Loss: 0.00001181
Iteration 184/1000 | Loss: 0.00001181
Iteration 185/1000 | Loss: 0.00001181
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 185. Stopping optimization.
Last 5 losses: [1.1807994269474875e-05, 1.1807994269474875e-05, 1.1807994269474875e-05, 1.1807994269474875e-05, 1.1807994269474875e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1807994269474875e-05

Optimization complete. Final v2v error: 2.921560525894165 mm

Highest mean error: 3.8486897945404053 mm for frame 164

Lowest mean error: 2.497239828109741 mm for frame 54

Saving results

Total time: 42.900795221328735
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_025/1006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_025/1006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_025/1006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00805252
Iteration 2/25 | Loss: 0.00123128
Iteration 3/25 | Loss: 0.00112515
Iteration 4/25 | Loss: 0.00111283
Iteration 5/25 | Loss: 0.00111060
Iteration 6/25 | Loss: 0.00111060
Iteration 7/25 | Loss: 0.00111060
Iteration 8/25 | Loss: 0.00111060
Iteration 9/25 | Loss: 0.00111060
Iteration 10/25 | Loss: 0.00111060
Iteration 11/25 | Loss: 0.00111060
Iteration 12/25 | Loss: 0.00111060
Iteration 13/25 | Loss: 0.00111060
Iteration 14/25 | Loss: 0.00111060
Iteration 15/25 | Loss: 0.00111060
Iteration 16/25 | Loss: 0.00111060
Iteration 17/25 | Loss: 0.00111060
Iteration 18/25 | Loss: 0.00111060
Iteration 19/25 | Loss: 0.00111060
Iteration 20/25 | Loss: 0.00111060
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0011105978628620505, 0.0011105978628620505, 0.0011105978628620505, 0.0011105978628620505, 0.0011105978628620505]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011105978628620505

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36339724
Iteration 2/25 | Loss: 0.00077391
Iteration 3/25 | Loss: 0.00077391
Iteration 4/25 | Loss: 0.00077391
Iteration 5/25 | Loss: 0.00077391
Iteration 6/25 | Loss: 0.00077391
Iteration 7/25 | Loss: 0.00077391
Iteration 8/25 | Loss: 0.00077391
Iteration 9/25 | Loss: 0.00077391
Iteration 10/25 | Loss: 0.00077391
Iteration 11/25 | Loss: 0.00077391
Iteration 12/25 | Loss: 0.00077391
Iteration 13/25 | Loss: 0.00077391
Iteration 14/25 | Loss: 0.00077391
Iteration 15/25 | Loss: 0.00077391
Iteration 16/25 | Loss: 0.00077391
Iteration 17/25 | Loss: 0.00077391
Iteration 18/25 | Loss: 0.00077391
Iteration 19/25 | Loss: 0.00077391
Iteration 20/25 | Loss: 0.00077391
Iteration 21/25 | Loss: 0.00077391
Iteration 22/25 | Loss: 0.00077391
Iteration 23/25 | Loss: 0.00077391
Iteration 24/25 | Loss: 0.00077391
Iteration 25/25 | Loss: 0.00077391

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00077391
Iteration 2/1000 | Loss: 0.00002226
Iteration 3/1000 | Loss: 0.00001570
Iteration 4/1000 | Loss: 0.00001345
Iteration 5/1000 | Loss: 0.00001228
Iteration 6/1000 | Loss: 0.00001161
Iteration 7/1000 | Loss: 0.00001113
Iteration 8/1000 | Loss: 0.00001077
Iteration 9/1000 | Loss: 0.00001071
Iteration 10/1000 | Loss: 0.00001066
Iteration 11/1000 | Loss: 0.00001065
Iteration 12/1000 | Loss: 0.00001053
Iteration 13/1000 | Loss: 0.00001035
Iteration 14/1000 | Loss: 0.00001033
Iteration 15/1000 | Loss: 0.00001032
Iteration 16/1000 | Loss: 0.00001030
Iteration 17/1000 | Loss: 0.00001029
Iteration 18/1000 | Loss: 0.00001028
Iteration 19/1000 | Loss: 0.00001027
Iteration 20/1000 | Loss: 0.00001027
Iteration 21/1000 | Loss: 0.00001026
Iteration 22/1000 | Loss: 0.00001026
Iteration 23/1000 | Loss: 0.00001025
Iteration 24/1000 | Loss: 0.00001025
Iteration 25/1000 | Loss: 0.00001024
Iteration 26/1000 | Loss: 0.00001024
Iteration 27/1000 | Loss: 0.00001020
Iteration 28/1000 | Loss: 0.00001019
Iteration 29/1000 | Loss: 0.00001017
Iteration 30/1000 | Loss: 0.00001016
Iteration 31/1000 | Loss: 0.00001016
Iteration 32/1000 | Loss: 0.00001009
Iteration 33/1000 | Loss: 0.00001007
Iteration 34/1000 | Loss: 0.00001006
Iteration 35/1000 | Loss: 0.00001006
Iteration 36/1000 | Loss: 0.00001005
Iteration 37/1000 | Loss: 0.00001005
Iteration 38/1000 | Loss: 0.00001004
Iteration 39/1000 | Loss: 0.00001002
Iteration 40/1000 | Loss: 0.00001001
Iteration 41/1000 | Loss: 0.00001001
Iteration 42/1000 | Loss: 0.00001001
Iteration 43/1000 | Loss: 0.00001000
Iteration 44/1000 | Loss: 0.00001000
Iteration 45/1000 | Loss: 0.00000999
Iteration 46/1000 | Loss: 0.00000998
Iteration 47/1000 | Loss: 0.00000998
Iteration 48/1000 | Loss: 0.00000998
Iteration 49/1000 | Loss: 0.00000997
Iteration 50/1000 | Loss: 0.00000997
Iteration 51/1000 | Loss: 0.00000997
Iteration 52/1000 | Loss: 0.00000996
Iteration 53/1000 | Loss: 0.00000993
Iteration 54/1000 | Loss: 0.00000993
Iteration 55/1000 | Loss: 0.00000992
Iteration 56/1000 | Loss: 0.00000992
Iteration 57/1000 | Loss: 0.00000992
Iteration 58/1000 | Loss: 0.00000991
Iteration 59/1000 | Loss: 0.00000991
Iteration 60/1000 | Loss: 0.00000990
Iteration 61/1000 | Loss: 0.00000990
Iteration 62/1000 | Loss: 0.00000990
Iteration 63/1000 | Loss: 0.00000989
Iteration 64/1000 | Loss: 0.00000989
Iteration 65/1000 | Loss: 0.00000988
Iteration 66/1000 | Loss: 0.00000988
Iteration 67/1000 | Loss: 0.00000987
Iteration 68/1000 | Loss: 0.00000987
Iteration 69/1000 | Loss: 0.00000986
Iteration 70/1000 | Loss: 0.00000985
Iteration 71/1000 | Loss: 0.00000985
Iteration 72/1000 | Loss: 0.00000984
Iteration 73/1000 | Loss: 0.00000984
Iteration 74/1000 | Loss: 0.00000984
Iteration 75/1000 | Loss: 0.00000984
Iteration 76/1000 | Loss: 0.00000984
Iteration 77/1000 | Loss: 0.00000984
Iteration 78/1000 | Loss: 0.00000984
Iteration 79/1000 | Loss: 0.00000984
Iteration 80/1000 | Loss: 0.00000984
Iteration 81/1000 | Loss: 0.00000984
Iteration 82/1000 | Loss: 0.00000984
Iteration 83/1000 | Loss: 0.00000982
Iteration 84/1000 | Loss: 0.00000981
Iteration 85/1000 | Loss: 0.00000980
Iteration 86/1000 | Loss: 0.00000980
Iteration 87/1000 | Loss: 0.00000980
Iteration 88/1000 | Loss: 0.00000979
Iteration 89/1000 | Loss: 0.00000979
Iteration 90/1000 | Loss: 0.00000979
Iteration 91/1000 | Loss: 0.00000979
Iteration 92/1000 | Loss: 0.00000979
Iteration 93/1000 | Loss: 0.00000978
Iteration 94/1000 | Loss: 0.00000978
Iteration 95/1000 | Loss: 0.00000978
Iteration 96/1000 | Loss: 0.00000978
Iteration 97/1000 | Loss: 0.00000977
Iteration 98/1000 | Loss: 0.00000976
Iteration 99/1000 | Loss: 0.00000976
Iteration 100/1000 | Loss: 0.00000975
Iteration 101/1000 | Loss: 0.00000975
Iteration 102/1000 | Loss: 0.00000975
Iteration 103/1000 | Loss: 0.00000973
Iteration 104/1000 | Loss: 0.00000973
Iteration 105/1000 | Loss: 0.00000973
Iteration 106/1000 | Loss: 0.00000973
Iteration 107/1000 | Loss: 0.00000973
Iteration 108/1000 | Loss: 0.00000973
Iteration 109/1000 | Loss: 0.00000973
Iteration 110/1000 | Loss: 0.00000973
Iteration 111/1000 | Loss: 0.00000973
Iteration 112/1000 | Loss: 0.00000973
Iteration 113/1000 | Loss: 0.00000973
Iteration 114/1000 | Loss: 0.00000972
Iteration 115/1000 | Loss: 0.00000972
Iteration 116/1000 | Loss: 0.00000972
Iteration 117/1000 | Loss: 0.00000971
Iteration 118/1000 | Loss: 0.00000971
Iteration 119/1000 | Loss: 0.00000971
Iteration 120/1000 | Loss: 0.00000971
Iteration 121/1000 | Loss: 0.00000971
Iteration 122/1000 | Loss: 0.00000971
Iteration 123/1000 | Loss: 0.00000971
Iteration 124/1000 | Loss: 0.00000971
Iteration 125/1000 | Loss: 0.00000970
Iteration 126/1000 | Loss: 0.00000970
Iteration 127/1000 | Loss: 0.00000970
Iteration 128/1000 | Loss: 0.00000970
Iteration 129/1000 | Loss: 0.00000970
Iteration 130/1000 | Loss: 0.00000969
Iteration 131/1000 | Loss: 0.00000969
Iteration 132/1000 | Loss: 0.00000969
Iteration 133/1000 | Loss: 0.00000968
Iteration 134/1000 | Loss: 0.00000968
Iteration 135/1000 | Loss: 0.00000968
Iteration 136/1000 | Loss: 0.00000968
Iteration 137/1000 | Loss: 0.00000968
Iteration 138/1000 | Loss: 0.00000968
Iteration 139/1000 | Loss: 0.00000968
Iteration 140/1000 | Loss: 0.00000968
Iteration 141/1000 | Loss: 0.00000968
Iteration 142/1000 | Loss: 0.00000967
Iteration 143/1000 | Loss: 0.00000967
Iteration 144/1000 | Loss: 0.00000967
Iteration 145/1000 | Loss: 0.00000967
Iteration 146/1000 | Loss: 0.00000967
Iteration 147/1000 | Loss: 0.00000967
Iteration 148/1000 | Loss: 0.00000967
Iteration 149/1000 | Loss: 0.00000967
Iteration 150/1000 | Loss: 0.00000967
Iteration 151/1000 | Loss: 0.00000966
Iteration 152/1000 | Loss: 0.00000966
Iteration 153/1000 | Loss: 0.00000966
Iteration 154/1000 | Loss: 0.00000965
Iteration 155/1000 | Loss: 0.00000965
Iteration 156/1000 | Loss: 0.00000965
Iteration 157/1000 | Loss: 0.00000965
Iteration 158/1000 | Loss: 0.00000965
Iteration 159/1000 | Loss: 0.00000965
Iteration 160/1000 | Loss: 0.00000964
Iteration 161/1000 | Loss: 0.00000964
Iteration 162/1000 | Loss: 0.00000964
Iteration 163/1000 | Loss: 0.00000964
Iteration 164/1000 | Loss: 0.00000964
Iteration 165/1000 | Loss: 0.00000964
Iteration 166/1000 | Loss: 0.00000963
Iteration 167/1000 | Loss: 0.00000963
Iteration 168/1000 | Loss: 0.00000963
Iteration 169/1000 | Loss: 0.00000963
Iteration 170/1000 | Loss: 0.00000963
Iteration 171/1000 | Loss: 0.00000963
Iteration 172/1000 | Loss: 0.00000962
Iteration 173/1000 | Loss: 0.00000962
Iteration 174/1000 | Loss: 0.00000962
Iteration 175/1000 | Loss: 0.00000962
Iteration 176/1000 | Loss: 0.00000962
Iteration 177/1000 | Loss: 0.00000962
Iteration 178/1000 | Loss: 0.00000962
Iteration 179/1000 | Loss: 0.00000962
Iteration 180/1000 | Loss: 0.00000961
Iteration 181/1000 | Loss: 0.00000961
Iteration 182/1000 | Loss: 0.00000961
Iteration 183/1000 | Loss: 0.00000961
Iteration 184/1000 | Loss: 0.00000961
Iteration 185/1000 | Loss: 0.00000961
Iteration 186/1000 | Loss: 0.00000960
Iteration 187/1000 | Loss: 0.00000960
Iteration 188/1000 | Loss: 0.00000960
Iteration 189/1000 | Loss: 0.00000960
Iteration 190/1000 | Loss: 0.00000960
Iteration 191/1000 | Loss: 0.00000960
Iteration 192/1000 | Loss: 0.00000960
Iteration 193/1000 | Loss: 0.00000960
Iteration 194/1000 | Loss: 0.00000960
Iteration 195/1000 | Loss: 0.00000960
Iteration 196/1000 | Loss: 0.00000959
Iteration 197/1000 | Loss: 0.00000959
Iteration 198/1000 | Loss: 0.00000959
Iteration 199/1000 | Loss: 0.00000959
Iteration 200/1000 | Loss: 0.00000959
Iteration 201/1000 | Loss: 0.00000959
Iteration 202/1000 | Loss: 0.00000959
Iteration 203/1000 | Loss: 0.00000959
Iteration 204/1000 | Loss: 0.00000959
Iteration 205/1000 | Loss: 0.00000959
Iteration 206/1000 | Loss: 0.00000959
Iteration 207/1000 | Loss: 0.00000959
Iteration 208/1000 | Loss: 0.00000959
Iteration 209/1000 | Loss: 0.00000959
Iteration 210/1000 | Loss: 0.00000959
Iteration 211/1000 | Loss: 0.00000958
Iteration 212/1000 | Loss: 0.00000958
Iteration 213/1000 | Loss: 0.00000958
Iteration 214/1000 | Loss: 0.00000958
Iteration 215/1000 | Loss: 0.00000958
Iteration 216/1000 | Loss: 0.00000958
Iteration 217/1000 | Loss: 0.00000958
Iteration 218/1000 | Loss: 0.00000958
Iteration 219/1000 | Loss: 0.00000958
Iteration 220/1000 | Loss: 0.00000958
Iteration 221/1000 | Loss: 0.00000957
Iteration 222/1000 | Loss: 0.00000957
Iteration 223/1000 | Loss: 0.00000957
Iteration 224/1000 | Loss: 0.00000957
Iteration 225/1000 | Loss: 0.00000957
Iteration 226/1000 | Loss: 0.00000957
Iteration 227/1000 | Loss: 0.00000957
Iteration 228/1000 | Loss: 0.00000957
Iteration 229/1000 | Loss: 0.00000957
Iteration 230/1000 | Loss: 0.00000957
Iteration 231/1000 | Loss: 0.00000957
Iteration 232/1000 | Loss: 0.00000957
Iteration 233/1000 | Loss: 0.00000957
Iteration 234/1000 | Loss: 0.00000957
Iteration 235/1000 | Loss: 0.00000956
Iteration 236/1000 | Loss: 0.00000956
Iteration 237/1000 | Loss: 0.00000956
Iteration 238/1000 | Loss: 0.00000956
Iteration 239/1000 | Loss: 0.00000956
Iteration 240/1000 | Loss: 0.00000956
Iteration 241/1000 | Loss: 0.00000956
Iteration 242/1000 | Loss: 0.00000956
Iteration 243/1000 | Loss: 0.00000955
Iteration 244/1000 | Loss: 0.00000955
Iteration 245/1000 | Loss: 0.00000955
Iteration 246/1000 | Loss: 0.00000955
Iteration 247/1000 | Loss: 0.00000955
Iteration 248/1000 | Loss: 0.00000955
Iteration 249/1000 | Loss: 0.00000955
Iteration 250/1000 | Loss: 0.00000955
Iteration 251/1000 | Loss: 0.00000955
Iteration 252/1000 | Loss: 0.00000955
Iteration 253/1000 | Loss: 0.00000955
Iteration 254/1000 | Loss: 0.00000955
Iteration 255/1000 | Loss: 0.00000955
Iteration 256/1000 | Loss: 0.00000955
Iteration 257/1000 | Loss: 0.00000955
Iteration 258/1000 | Loss: 0.00000954
Iteration 259/1000 | Loss: 0.00000954
Iteration 260/1000 | Loss: 0.00000954
Iteration 261/1000 | Loss: 0.00000954
Iteration 262/1000 | Loss: 0.00000954
Iteration 263/1000 | Loss: 0.00000954
Iteration 264/1000 | Loss: 0.00000954
Iteration 265/1000 | Loss: 0.00000954
Iteration 266/1000 | Loss: 0.00000954
Iteration 267/1000 | Loss: 0.00000954
Iteration 268/1000 | Loss: 0.00000954
Iteration 269/1000 | Loss: 0.00000954
Iteration 270/1000 | Loss: 0.00000954
Iteration 271/1000 | Loss: 0.00000954
Iteration 272/1000 | Loss: 0.00000954
Iteration 273/1000 | Loss: 0.00000954
Iteration 274/1000 | Loss: 0.00000954
Iteration 275/1000 | Loss: 0.00000954
Iteration 276/1000 | Loss: 0.00000954
Iteration 277/1000 | Loss: 0.00000954
Iteration 278/1000 | Loss: 0.00000954
Iteration 279/1000 | Loss: 0.00000954
Iteration 280/1000 | Loss: 0.00000954
Iteration 281/1000 | Loss: 0.00000954
Iteration 282/1000 | Loss: 0.00000954
Iteration 283/1000 | Loss: 0.00000954
Iteration 284/1000 | Loss: 0.00000954
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 284. Stopping optimization.
Last 5 losses: [9.537366167933214e-06, 9.537366167933214e-06, 9.537366167933214e-06, 9.537366167933214e-06, 9.537366167933214e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.537366167933214e-06

Optimization complete. Final v2v error: 2.6266400814056396 mm

Highest mean error: 2.9304580688476562 mm for frame 46

Lowest mean error: 2.4720234870910645 mm for frame 8

Saving results

Total time: 44.83940124511719
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_025/1033/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_025/1033.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_025/1033
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00824819
Iteration 2/25 | Loss: 0.00129360
Iteration 3/25 | Loss: 0.00119483
Iteration 4/25 | Loss: 0.00118157
Iteration 5/25 | Loss: 0.00117830
Iteration 6/25 | Loss: 0.00117763
Iteration 7/25 | Loss: 0.00117763
Iteration 8/25 | Loss: 0.00117763
Iteration 9/25 | Loss: 0.00117763
Iteration 10/25 | Loss: 0.00117763
Iteration 11/25 | Loss: 0.00117763
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001177631551399827, 0.001177631551399827, 0.001177631551399827, 0.001177631551399827, 0.001177631551399827]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001177631551399827

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.31890249
Iteration 2/25 | Loss: 0.00066555
Iteration 3/25 | Loss: 0.00066548
Iteration 4/25 | Loss: 0.00066548
Iteration 5/25 | Loss: 0.00066548
Iteration 6/25 | Loss: 0.00066548
Iteration 7/25 | Loss: 0.00066548
Iteration 8/25 | Loss: 0.00066548
Iteration 9/25 | Loss: 0.00066548
Iteration 10/25 | Loss: 0.00066548
Iteration 11/25 | Loss: 0.00066548
Iteration 12/25 | Loss: 0.00066548
Iteration 13/25 | Loss: 0.00066548
Iteration 14/25 | Loss: 0.00066548
Iteration 15/25 | Loss: 0.00066548
Iteration 16/25 | Loss: 0.00066548
Iteration 17/25 | Loss: 0.00066548
Iteration 18/25 | Loss: 0.00066548
Iteration 19/25 | Loss: 0.00066548
Iteration 20/25 | Loss: 0.00066548
Iteration 21/25 | Loss: 0.00066548
Iteration 22/25 | Loss: 0.00066548
Iteration 23/25 | Loss: 0.00066548
Iteration 24/25 | Loss: 0.00066548
Iteration 25/25 | Loss: 0.00066548

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00066548
Iteration 2/1000 | Loss: 0.00004281
Iteration 3/1000 | Loss: 0.00002444
Iteration 4/1000 | Loss: 0.00001688
Iteration 5/1000 | Loss: 0.00001463
Iteration 6/1000 | Loss: 0.00001381
Iteration 7/1000 | Loss: 0.00001329
Iteration 8/1000 | Loss: 0.00001291
Iteration 9/1000 | Loss: 0.00001264
Iteration 10/1000 | Loss: 0.00001239
Iteration 11/1000 | Loss: 0.00001229
Iteration 12/1000 | Loss: 0.00001229
Iteration 13/1000 | Loss: 0.00001211
Iteration 14/1000 | Loss: 0.00001208
Iteration 15/1000 | Loss: 0.00001200
Iteration 16/1000 | Loss: 0.00001199
Iteration 17/1000 | Loss: 0.00001191
Iteration 18/1000 | Loss: 0.00001190
Iteration 19/1000 | Loss: 0.00001189
Iteration 20/1000 | Loss: 0.00001184
Iteration 21/1000 | Loss: 0.00001183
Iteration 22/1000 | Loss: 0.00001183
Iteration 23/1000 | Loss: 0.00001182
Iteration 24/1000 | Loss: 0.00001182
Iteration 25/1000 | Loss: 0.00001182
Iteration 26/1000 | Loss: 0.00001182
Iteration 27/1000 | Loss: 0.00001181
Iteration 28/1000 | Loss: 0.00001181
Iteration 29/1000 | Loss: 0.00001180
Iteration 30/1000 | Loss: 0.00001180
Iteration 31/1000 | Loss: 0.00001180
Iteration 32/1000 | Loss: 0.00001179
Iteration 33/1000 | Loss: 0.00001179
Iteration 34/1000 | Loss: 0.00001177
Iteration 35/1000 | Loss: 0.00001177
Iteration 36/1000 | Loss: 0.00001177
Iteration 37/1000 | Loss: 0.00001177
Iteration 38/1000 | Loss: 0.00001177
Iteration 39/1000 | Loss: 0.00001177
Iteration 40/1000 | Loss: 0.00001177
Iteration 41/1000 | Loss: 0.00001176
Iteration 42/1000 | Loss: 0.00001176
Iteration 43/1000 | Loss: 0.00001171
Iteration 44/1000 | Loss: 0.00001171
Iteration 45/1000 | Loss: 0.00001169
Iteration 46/1000 | Loss: 0.00001168
Iteration 47/1000 | Loss: 0.00001167
Iteration 48/1000 | Loss: 0.00001167
Iteration 49/1000 | Loss: 0.00001166
Iteration 50/1000 | Loss: 0.00001164
Iteration 51/1000 | Loss: 0.00001163
Iteration 52/1000 | Loss: 0.00001163
Iteration 53/1000 | Loss: 0.00001163
Iteration 54/1000 | Loss: 0.00001162
Iteration 55/1000 | Loss: 0.00001162
Iteration 56/1000 | Loss: 0.00001160
Iteration 57/1000 | Loss: 0.00001155
Iteration 58/1000 | Loss: 0.00001154
Iteration 59/1000 | Loss: 0.00001148
Iteration 60/1000 | Loss: 0.00001147
Iteration 61/1000 | Loss: 0.00001147
Iteration 62/1000 | Loss: 0.00001146
Iteration 63/1000 | Loss: 0.00001145
Iteration 64/1000 | Loss: 0.00001145
Iteration 65/1000 | Loss: 0.00001145
Iteration 66/1000 | Loss: 0.00001144
Iteration 67/1000 | Loss: 0.00001144
Iteration 68/1000 | Loss: 0.00001144
Iteration 69/1000 | Loss: 0.00001143
Iteration 70/1000 | Loss: 0.00001143
Iteration 71/1000 | Loss: 0.00001143
Iteration 72/1000 | Loss: 0.00001143
Iteration 73/1000 | Loss: 0.00001142
Iteration 74/1000 | Loss: 0.00001142
Iteration 75/1000 | Loss: 0.00001142
Iteration 76/1000 | Loss: 0.00001141
Iteration 77/1000 | Loss: 0.00001141
Iteration 78/1000 | Loss: 0.00001141
Iteration 79/1000 | Loss: 0.00001141
Iteration 80/1000 | Loss: 0.00001141
Iteration 81/1000 | Loss: 0.00001140
Iteration 82/1000 | Loss: 0.00001140
Iteration 83/1000 | Loss: 0.00001140
Iteration 84/1000 | Loss: 0.00001140
Iteration 85/1000 | Loss: 0.00001140
Iteration 86/1000 | Loss: 0.00001139
Iteration 87/1000 | Loss: 0.00001139
Iteration 88/1000 | Loss: 0.00001139
Iteration 89/1000 | Loss: 0.00001139
Iteration 90/1000 | Loss: 0.00001139
Iteration 91/1000 | Loss: 0.00001138
Iteration 92/1000 | Loss: 0.00001138
Iteration 93/1000 | Loss: 0.00001138
Iteration 94/1000 | Loss: 0.00001137
Iteration 95/1000 | Loss: 0.00001137
Iteration 96/1000 | Loss: 0.00001137
Iteration 97/1000 | Loss: 0.00001136
Iteration 98/1000 | Loss: 0.00001136
Iteration 99/1000 | Loss: 0.00001136
Iteration 100/1000 | Loss: 0.00001135
Iteration 101/1000 | Loss: 0.00001135
Iteration 102/1000 | Loss: 0.00001135
Iteration 103/1000 | Loss: 0.00001135
Iteration 104/1000 | Loss: 0.00001134
Iteration 105/1000 | Loss: 0.00001134
Iteration 106/1000 | Loss: 0.00001134
Iteration 107/1000 | Loss: 0.00001134
Iteration 108/1000 | Loss: 0.00001134
Iteration 109/1000 | Loss: 0.00001134
Iteration 110/1000 | Loss: 0.00001133
Iteration 111/1000 | Loss: 0.00001133
Iteration 112/1000 | Loss: 0.00001133
Iteration 113/1000 | Loss: 0.00001132
Iteration 114/1000 | Loss: 0.00001132
Iteration 115/1000 | Loss: 0.00001132
Iteration 116/1000 | Loss: 0.00001132
Iteration 117/1000 | Loss: 0.00001132
Iteration 118/1000 | Loss: 0.00001132
Iteration 119/1000 | Loss: 0.00001132
Iteration 120/1000 | Loss: 0.00001132
Iteration 121/1000 | Loss: 0.00001132
Iteration 122/1000 | Loss: 0.00001132
Iteration 123/1000 | Loss: 0.00001131
Iteration 124/1000 | Loss: 0.00001131
Iteration 125/1000 | Loss: 0.00001131
Iteration 126/1000 | Loss: 0.00001131
Iteration 127/1000 | Loss: 0.00001131
Iteration 128/1000 | Loss: 0.00001131
Iteration 129/1000 | Loss: 0.00001131
Iteration 130/1000 | Loss: 0.00001131
Iteration 131/1000 | Loss: 0.00001131
Iteration 132/1000 | Loss: 0.00001131
Iteration 133/1000 | Loss: 0.00001131
Iteration 134/1000 | Loss: 0.00001131
Iteration 135/1000 | Loss: 0.00001131
Iteration 136/1000 | Loss: 0.00001130
Iteration 137/1000 | Loss: 0.00001130
Iteration 138/1000 | Loss: 0.00001130
Iteration 139/1000 | Loss: 0.00001130
Iteration 140/1000 | Loss: 0.00001130
Iteration 141/1000 | Loss: 0.00001130
Iteration 142/1000 | Loss: 0.00001130
Iteration 143/1000 | Loss: 0.00001130
Iteration 144/1000 | Loss: 0.00001130
Iteration 145/1000 | Loss: 0.00001130
Iteration 146/1000 | Loss: 0.00001130
Iteration 147/1000 | Loss: 0.00001129
Iteration 148/1000 | Loss: 0.00001129
Iteration 149/1000 | Loss: 0.00001129
Iteration 150/1000 | Loss: 0.00001129
Iteration 151/1000 | Loss: 0.00001129
Iteration 152/1000 | Loss: 0.00001129
Iteration 153/1000 | Loss: 0.00001129
Iteration 154/1000 | Loss: 0.00001129
Iteration 155/1000 | Loss: 0.00001129
Iteration 156/1000 | Loss: 0.00001129
Iteration 157/1000 | Loss: 0.00001129
Iteration 158/1000 | Loss: 0.00001129
Iteration 159/1000 | Loss: 0.00001129
Iteration 160/1000 | Loss: 0.00001129
Iteration 161/1000 | Loss: 0.00001129
Iteration 162/1000 | Loss: 0.00001129
Iteration 163/1000 | Loss: 0.00001129
Iteration 164/1000 | Loss: 0.00001128
Iteration 165/1000 | Loss: 0.00001128
Iteration 166/1000 | Loss: 0.00001128
Iteration 167/1000 | Loss: 0.00001128
Iteration 168/1000 | Loss: 0.00001128
Iteration 169/1000 | Loss: 0.00001128
Iteration 170/1000 | Loss: 0.00001128
Iteration 171/1000 | Loss: 0.00001128
Iteration 172/1000 | Loss: 0.00001128
Iteration 173/1000 | Loss: 0.00001128
Iteration 174/1000 | Loss: 0.00001127
Iteration 175/1000 | Loss: 0.00001127
Iteration 176/1000 | Loss: 0.00001127
Iteration 177/1000 | Loss: 0.00001127
Iteration 178/1000 | Loss: 0.00001127
Iteration 179/1000 | Loss: 0.00001127
Iteration 180/1000 | Loss: 0.00001126
Iteration 181/1000 | Loss: 0.00001126
Iteration 182/1000 | Loss: 0.00001126
Iteration 183/1000 | Loss: 0.00001126
Iteration 184/1000 | Loss: 0.00001126
Iteration 185/1000 | Loss: 0.00001126
Iteration 186/1000 | Loss: 0.00001126
Iteration 187/1000 | Loss: 0.00001126
Iteration 188/1000 | Loss: 0.00001126
Iteration 189/1000 | Loss: 0.00001126
Iteration 190/1000 | Loss: 0.00001126
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 190. Stopping optimization.
Last 5 losses: [1.1262752195762005e-05, 1.1262752195762005e-05, 1.1262752195762005e-05, 1.1262752195762005e-05, 1.1262752195762005e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1262752195762005e-05

Optimization complete. Final v2v error: 2.878859519958496 mm

Highest mean error: 3.1690144538879395 mm for frame 2

Lowest mean error: 2.723881483078003 mm for frame 103

Saving results

Total time: 41.4674129486084
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_025/1005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_025/1005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_025/1005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01017861
Iteration 2/25 | Loss: 0.00258874
Iteration 3/25 | Loss: 0.00200280
Iteration 4/25 | Loss: 0.00174967
Iteration 5/25 | Loss: 0.00165861
Iteration 6/25 | Loss: 0.00147892
Iteration 7/25 | Loss: 0.00147851
Iteration 8/25 | Loss: 0.00149425
Iteration 9/25 | Loss: 0.00131045
Iteration 10/25 | Loss: 0.00125609
Iteration 11/25 | Loss: 0.00124840
Iteration 12/25 | Loss: 0.00124554
Iteration 13/25 | Loss: 0.00124435
Iteration 14/25 | Loss: 0.00124475
Iteration 15/25 | Loss: 0.00124422
Iteration 16/25 | Loss: 0.00124370
Iteration 17/25 | Loss: 0.00124254
Iteration 18/25 | Loss: 0.00124608
Iteration 19/25 | Loss: 0.00124313
Iteration 20/25 | Loss: 0.00124623
Iteration 21/25 | Loss: 0.00124278
Iteration 22/25 | Loss: 0.00124516
Iteration 23/25 | Loss: 0.00124263
Iteration 24/25 | Loss: 0.00124509
Iteration 25/25 | Loss: 0.00124304

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38870084
Iteration 2/25 | Loss: 0.00071763
Iteration 3/25 | Loss: 0.00071762
Iteration 4/25 | Loss: 0.00071762
Iteration 5/25 | Loss: 0.00071762
Iteration 6/25 | Loss: 0.00071762
Iteration 7/25 | Loss: 0.00071762
Iteration 8/25 | Loss: 0.00071762
Iteration 9/25 | Loss: 0.00071762
Iteration 10/25 | Loss: 0.00071762
Iteration 11/25 | Loss: 0.00071762
Iteration 12/25 | Loss: 0.00071762
Iteration 13/25 | Loss: 0.00071762
Iteration 14/25 | Loss: 0.00071762
Iteration 15/25 | Loss: 0.00071762
Iteration 16/25 | Loss: 0.00071762
Iteration 17/25 | Loss: 0.00071762
Iteration 18/25 | Loss: 0.00071762
Iteration 19/25 | Loss: 0.00071762
Iteration 20/25 | Loss: 0.00071762
Iteration 21/25 | Loss: 0.00071762
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0007176202489063144, 0.0007176202489063144, 0.0007176202489063144, 0.0007176202489063144, 0.0007176202489063144]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007176202489063144

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00071762
Iteration 2/1000 | Loss: 0.00005576
Iteration 3/1000 | Loss: 0.00003240
Iteration 4/1000 | Loss: 0.00003282
Iteration 5/1000 | Loss: 0.00002958
Iteration 6/1000 | Loss: 0.00002767
Iteration 7/1000 | Loss: 0.00003825
Iteration 8/1000 | Loss: 0.00003556
Iteration 9/1000 | Loss: 0.00003129
Iteration 10/1000 | Loss: 0.00003308
Iteration 11/1000 | Loss: 0.00003111
Iteration 12/1000 | Loss: 0.00003081
Iteration 13/1000 | Loss: 0.00002827
Iteration 14/1000 | Loss: 0.00002595
Iteration 15/1000 | Loss: 0.00003412
Iteration 16/1000 | Loss: 0.00003336
Iteration 17/1000 | Loss: 0.00002725
Iteration 18/1000 | Loss: 0.00002775
Iteration 19/1000 | Loss: 0.00003596
Iteration 20/1000 | Loss: 0.00015190
Iteration 21/1000 | Loss: 0.00015022
Iteration 22/1000 | Loss: 0.00012284
Iteration 23/1000 | Loss: 0.00010372
Iteration 24/1000 | Loss: 0.00003143
Iteration 25/1000 | Loss: 0.00010708
Iteration 26/1000 | Loss: 0.00002733
Iteration 27/1000 | Loss: 0.00010733
Iteration 28/1000 | Loss: 0.00002568
Iteration 29/1000 | Loss: 0.00002459
Iteration 30/1000 | Loss: 0.00002417
Iteration 31/1000 | Loss: 0.00002387
Iteration 32/1000 | Loss: 0.00002369
Iteration 33/1000 | Loss: 0.00002363
Iteration 34/1000 | Loss: 0.00002345
Iteration 35/1000 | Loss: 0.00002331
Iteration 36/1000 | Loss: 0.00002330
Iteration 37/1000 | Loss: 0.00002330
Iteration 38/1000 | Loss: 0.00002329
Iteration 39/1000 | Loss: 0.00002329
Iteration 40/1000 | Loss: 0.00002328
Iteration 41/1000 | Loss: 0.00002328
Iteration 42/1000 | Loss: 0.00002328
Iteration 43/1000 | Loss: 0.00002326
Iteration 44/1000 | Loss: 0.00002326
Iteration 45/1000 | Loss: 0.00002326
Iteration 46/1000 | Loss: 0.00002325
Iteration 47/1000 | Loss: 0.00002325
Iteration 48/1000 | Loss: 0.00002325
Iteration 49/1000 | Loss: 0.00002325
Iteration 50/1000 | Loss: 0.00002325
Iteration 51/1000 | Loss: 0.00002324
Iteration 52/1000 | Loss: 0.00002324
Iteration 53/1000 | Loss: 0.00002324
Iteration 54/1000 | Loss: 0.00002324
Iteration 55/1000 | Loss: 0.00002324
Iteration 56/1000 | Loss: 0.00002324
Iteration 57/1000 | Loss: 0.00002324
Iteration 58/1000 | Loss: 0.00002323
Iteration 59/1000 | Loss: 0.00002322
Iteration 60/1000 | Loss: 0.00002322
Iteration 61/1000 | Loss: 0.00002322
Iteration 62/1000 | Loss: 0.00002321
Iteration 63/1000 | Loss: 0.00002320
Iteration 64/1000 | Loss: 0.00002320
Iteration 65/1000 | Loss: 0.00002320
Iteration 66/1000 | Loss: 0.00002320
Iteration 67/1000 | Loss: 0.00002319
Iteration 68/1000 | Loss: 0.00002319
Iteration 69/1000 | Loss: 0.00002319
Iteration 70/1000 | Loss: 0.00002319
Iteration 71/1000 | Loss: 0.00002318
Iteration 72/1000 | Loss: 0.00002318
Iteration 73/1000 | Loss: 0.00002318
Iteration 74/1000 | Loss: 0.00002318
Iteration 75/1000 | Loss: 0.00002318
Iteration 76/1000 | Loss: 0.00002317
Iteration 77/1000 | Loss: 0.00002317
Iteration 78/1000 | Loss: 0.00002317
Iteration 79/1000 | Loss: 0.00002317
Iteration 80/1000 | Loss: 0.00002317
Iteration 81/1000 | Loss: 0.00002317
Iteration 82/1000 | Loss: 0.00002317
Iteration 83/1000 | Loss: 0.00002317
Iteration 84/1000 | Loss: 0.00002317
Iteration 85/1000 | Loss: 0.00002316
Iteration 86/1000 | Loss: 0.00002316
Iteration 87/1000 | Loss: 0.00002316
Iteration 88/1000 | Loss: 0.00002316
Iteration 89/1000 | Loss: 0.00002316
Iteration 90/1000 | Loss: 0.00002316
Iteration 91/1000 | Loss: 0.00002316
Iteration 92/1000 | Loss: 0.00002316
Iteration 93/1000 | Loss: 0.00002316
Iteration 94/1000 | Loss: 0.00002316
Iteration 95/1000 | Loss: 0.00002315
Iteration 96/1000 | Loss: 0.00002315
Iteration 97/1000 | Loss: 0.00002315
Iteration 98/1000 | Loss: 0.00002315
Iteration 99/1000 | Loss: 0.00002314
Iteration 100/1000 | Loss: 0.00002313
Iteration 101/1000 | Loss: 0.00002313
Iteration 102/1000 | Loss: 0.00002313
Iteration 103/1000 | Loss: 0.00002313
Iteration 104/1000 | Loss: 0.00002312
Iteration 105/1000 | Loss: 0.00002312
Iteration 106/1000 | Loss: 0.00002312
Iteration 107/1000 | Loss: 0.00002312
Iteration 108/1000 | Loss: 0.00002312
Iteration 109/1000 | Loss: 0.00002312
Iteration 110/1000 | Loss: 0.00002312
Iteration 111/1000 | Loss: 0.00002311
Iteration 112/1000 | Loss: 0.00002311
Iteration 113/1000 | Loss: 0.00002311
Iteration 114/1000 | Loss: 0.00002311
Iteration 115/1000 | Loss: 0.00002311
Iteration 116/1000 | Loss: 0.00002310
Iteration 117/1000 | Loss: 0.00002310
Iteration 118/1000 | Loss: 0.00002310
Iteration 119/1000 | Loss: 0.00002310
Iteration 120/1000 | Loss: 0.00002310
Iteration 121/1000 | Loss: 0.00002310
Iteration 122/1000 | Loss: 0.00002310
Iteration 123/1000 | Loss: 0.00002309
Iteration 124/1000 | Loss: 0.00002309
Iteration 125/1000 | Loss: 0.00002309
Iteration 126/1000 | Loss: 0.00002309
Iteration 127/1000 | Loss: 0.00002309
Iteration 128/1000 | Loss: 0.00002308
Iteration 129/1000 | Loss: 0.00002308
Iteration 130/1000 | Loss: 0.00002308
Iteration 131/1000 | Loss: 0.00002308
Iteration 132/1000 | Loss: 0.00002308
Iteration 133/1000 | Loss: 0.00002308
Iteration 134/1000 | Loss: 0.00002308
Iteration 135/1000 | Loss: 0.00002308
Iteration 136/1000 | Loss: 0.00002308
Iteration 137/1000 | Loss: 0.00002308
Iteration 138/1000 | Loss: 0.00002308
Iteration 139/1000 | Loss: 0.00002308
Iteration 140/1000 | Loss: 0.00002307
Iteration 141/1000 | Loss: 0.00002307
Iteration 142/1000 | Loss: 0.00002307
Iteration 143/1000 | Loss: 0.00002307
Iteration 144/1000 | Loss: 0.00002307
Iteration 145/1000 | Loss: 0.00002307
Iteration 146/1000 | Loss: 0.00002307
Iteration 147/1000 | Loss: 0.00002307
Iteration 148/1000 | Loss: 0.00002307
Iteration 149/1000 | Loss: 0.00002307
Iteration 150/1000 | Loss: 0.00002307
Iteration 151/1000 | Loss: 0.00002307
Iteration 152/1000 | Loss: 0.00002307
Iteration 153/1000 | Loss: 0.00002306
Iteration 154/1000 | Loss: 0.00002306
Iteration 155/1000 | Loss: 0.00002306
Iteration 156/1000 | Loss: 0.00002306
Iteration 157/1000 | Loss: 0.00002306
Iteration 158/1000 | Loss: 0.00002306
Iteration 159/1000 | Loss: 0.00002306
Iteration 160/1000 | Loss: 0.00002306
Iteration 161/1000 | Loss: 0.00002306
Iteration 162/1000 | Loss: 0.00002306
Iteration 163/1000 | Loss: 0.00002306
Iteration 164/1000 | Loss: 0.00002306
Iteration 165/1000 | Loss: 0.00002306
Iteration 166/1000 | Loss: 0.00002306
Iteration 167/1000 | Loss: 0.00002306
Iteration 168/1000 | Loss: 0.00002306
Iteration 169/1000 | Loss: 0.00002306
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 169. Stopping optimization.
Last 5 losses: [2.306202077306807e-05, 2.306202077306807e-05, 2.306202077306807e-05, 2.306202077306807e-05, 2.306202077306807e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.306202077306807e-05

Optimization complete. Final v2v error: 4.0355634689331055 mm

Highest mean error: 4.747235298156738 mm for frame 231

Lowest mean error: 3.6082751750946045 mm for frame 58

Saving results

Total time: 116.20427131652832
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_025/1043/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_025/1043.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_025/1043
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00382617
Iteration 2/25 | Loss: 0.00129499
Iteration 3/25 | Loss: 0.00116353
Iteration 4/25 | Loss: 0.00113915
Iteration 5/25 | Loss: 0.00113178
Iteration 6/25 | Loss: 0.00112992
Iteration 7/25 | Loss: 0.00112988
Iteration 8/25 | Loss: 0.00112988
Iteration 9/25 | Loss: 0.00112988
Iteration 10/25 | Loss: 0.00112988
Iteration 11/25 | Loss: 0.00112988
Iteration 12/25 | Loss: 0.00112988
Iteration 13/25 | Loss: 0.00112988
Iteration 14/25 | Loss: 0.00112988
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0011298799654468894, 0.0011298799654468894, 0.0011298799654468894, 0.0011298799654468894, 0.0011298799654468894]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011298799654468894

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32337737
Iteration 2/25 | Loss: 0.00086181
Iteration 3/25 | Loss: 0.00086181
Iteration 4/25 | Loss: 0.00086181
Iteration 5/25 | Loss: 0.00086181
Iteration 6/25 | Loss: 0.00086181
Iteration 7/25 | Loss: 0.00086180
Iteration 8/25 | Loss: 0.00086180
Iteration 9/25 | Loss: 0.00086180
Iteration 10/25 | Loss: 0.00086180
Iteration 11/25 | Loss: 0.00086180
Iteration 12/25 | Loss: 0.00086180
Iteration 13/25 | Loss: 0.00086180
Iteration 14/25 | Loss: 0.00086180
Iteration 15/25 | Loss: 0.00086180
Iteration 16/25 | Loss: 0.00086180
Iteration 17/25 | Loss: 0.00086180
Iteration 18/25 | Loss: 0.00086180
Iteration 19/25 | Loss: 0.00086180
Iteration 20/25 | Loss: 0.00086180
Iteration 21/25 | Loss: 0.00086180
Iteration 22/25 | Loss: 0.00086180
Iteration 23/25 | Loss: 0.00086180
Iteration 24/25 | Loss: 0.00086180
Iteration 25/25 | Loss: 0.00086180

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00086180
Iteration 2/1000 | Loss: 0.00004641
Iteration 3/1000 | Loss: 0.00003276
Iteration 4/1000 | Loss: 0.00002527
Iteration 5/1000 | Loss: 0.00002296
Iteration 6/1000 | Loss: 0.00002100
Iteration 7/1000 | Loss: 0.00001949
Iteration 8/1000 | Loss: 0.00001869
Iteration 9/1000 | Loss: 0.00001814
Iteration 10/1000 | Loss: 0.00001779
Iteration 11/1000 | Loss: 0.00001752
Iteration 12/1000 | Loss: 0.00001726
Iteration 13/1000 | Loss: 0.00001705
Iteration 14/1000 | Loss: 0.00001701
Iteration 15/1000 | Loss: 0.00001684
Iteration 16/1000 | Loss: 0.00001676
Iteration 17/1000 | Loss: 0.00001665
Iteration 18/1000 | Loss: 0.00001664
Iteration 19/1000 | Loss: 0.00001663
Iteration 20/1000 | Loss: 0.00001657
Iteration 21/1000 | Loss: 0.00001656
Iteration 22/1000 | Loss: 0.00001655
Iteration 23/1000 | Loss: 0.00001655
Iteration 24/1000 | Loss: 0.00001652
Iteration 25/1000 | Loss: 0.00001652
Iteration 26/1000 | Loss: 0.00001651
Iteration 27/1000 | Loss: 0.00001651
Iteration 28/1000 | Loss: 0.00001650
Iteration 29/1000 | Loss: 0.00001650
Iteration 30/1000 | Loss: 0.00001650
Iteration 31/1000 | Loss: 0.00001650
Iteration 32/1000 | Loss: 0.00001650
Iteration 33/1000 | Loss: 0.00001650
Iteration 34/1000 | Loss: 0.00001650
Iteration 35/1000 | Loss: 0.00001650
Iteration 36/1000 | Loss: 0.00001650
Iteration 37/1000 | Loss: 0.00001650
Iteration 38/1000 | Loss: 0.00001650
Iteration 39/1000 | Loss: 0.00001650
Iteration 40/1000 | Loss: 0.00001650
Iteration 41/1000 | Loss: 0.00001649
Iteration 42/1000 | Loss: 0.00001649
Iteration 43/1000 | Loss: 0.00001649
Iteration 44/1000 | Loss: 0.00001649
Iteration 45/1000 | Loss: 0.00001649
Iteration 46/1000 | Loss: 0.00001649
Iteration 47/1000 | Loss: 0.00001649
Iteration 48/1000 | Loss: 0.00001649
Iteration 49/1000 | Loss: 0.00001649
Iteration 50/1000 | Loss: 0.00001648
Iteration 51/1000 | Loss: 0.00001647
Iteration 52/1000 | Loss: 0.00001647
Iteration 53/1000 | Loss: 0.00001646
Iteration 54/1000 | Loss: 0.00001646
Iteration 55/1000 | Loss: 0.00001646
Iteration 56/1000 | Loss: 0.00001645
Iteration 57/1000 | Loss: 0.00001645
Iteration 58/1000 | Loss: 0.00001645
Iteration 59/1000 | Loss: 0.00001644
Iteration 60/1000 | Loss: 0.00001644
Iteration 61/1000 | Loss: 0.00001644
Iteration 62/1000 | Loss: 0.00001643
Iteration 63/1000 | Loss: 0.00001643
Iteration 64/1000 | Loss: 0.00001642
Iteration 65/1000 | Loss: 0.00001642
Iteration 66/1000 | Loss: 0.00001641
Iteration 67/1000 | Loss: 0.00001640
Iteration 68/1000 | Loss: 0.00001640
Iteration 69/1000 | Loss: 0.00001640
Iteration 70/1000 | Loss: 0.00001639
Iteration 71/1000 | Loss: 0.00001639
Iteration 72/1000 | Loss: 0.00001639
Iteration 73/1000 | Loss: 0.00001639
Iteration 74/1000 | Loss: 0.00001638
Iteration 75/1000 | Loss: 0.00001638
Iteration 76/1000 | Loss: 0.00001638
Iteration 77/1000 | Loss: 0.00001637
Iteration 78/1000 | Loss: 0.00001637
Iteration 79/1000 | Loss: 0.00001637
Iteration 80/1000 | Loss: 0.00001637
Iteration 81/1000 | Loss: 0.00001636
Iteration 82/1000 | Loss: 0.00001636
Iteration 83/1000 | Loss: 0.00001636
Iteration 84/1000 | Loss: 0.00001636
Iteration 85/1000 | Loss: 0.00001636
Iteration 86/1000 | Loss: 0.00001635
Iteration 87/1000 | Loss: 0.00001635
Iteration 88/1000 | Loss: 0.00001635
Iteration 89/1000 | Loss: 0.00001635
Iteration 90/1000 | Loss: 0.00001635
Iteration 91/1000 | Loss: 0.00001635
Iteration 92/1000 | Loss: 0.00001634
Iteration 93/1000 | Loss: 0.00001634
Iteration 94/1000 | Loss: 0.00001633
Iteration 95/1000 | Loss: 0.00001633
Iteration 96/1000 | Loss: 0.00001633
Iteration 97/1000 | Loss: 0.00001633
Iteration 98/1000 | Loss: 0.00001632
Iteration 99/1000 | Loss: 0.00001632
Iteration 100/1000 | Loss: 0.00001632
Iteration 101/1000 | Loss: 0.00001632
Iteration 102/1000 | Loss: 0.00001632
Iteration 103/1000 | Loss: 0.00001632
Iteration 104/1000 | Loss: 0.00001632
Iteration 105/1000 | Loss: 0.00001632
Iteration 106/1000 | Loss: 0.00001631
Iteration 107/1000 | Loss: 0.00001631
Iteration 108/1000 | Loss: 0.00001631
Iteration 109/1000 | Loss: 0.00001631
Iteration 110/1000 | Loss: 0.00001631
Iteration 111/1000 | Loss: 0.00001631
Iteration 112/1000 | Loss: 0.00001631
Iteration 113/1000 | Loss: 0.00001630
Iteration 114/1000 | Loss: 0.00001630
Iteration 115/1000 | Loss: 0.00001630
Iteration 116/1000 | Loss: 0.00001630
Iteration 117/1000 | Loss: 0.00001630
Iteration 118/1000 | Loss: 0.00001630
Iteration 119/1000 | Loss: 0.00001630
Iteration 120/1000 | Loss: 0.00001630
Iteration 121/1000 | Loss: 0.00001630
Iteration 122/1000 | Loss: 0.00001630
Iteration 123/1000 | Loss: 0.00001629
Iteration 124/1000 | Loss: 0.00001629
Iteration 125/1000 | Loss: 0.00001629
Iteration 126/1000 | Loss: 0.00001629
Iteration 127/1000 | Loss: 0.00001629
Iteration 128/1000 | Loss: 0.00001629
Iteration 129/1000 | Loss: 0.00001629
Iteration 130/1000 | Loss: 0.00001629
Iteration 131/1000 | Loss: 0.00001629
Iteration 132/1000 | Loss: 0.00001629
Iteration 133/1000 | Loss: 0.00001629
Iteration 134/1000 | Loss: 0.00001629
Iteration 135/1000 | Loss: 0.00001629
Iteration 136/1000 | Loss: 0.00001629
Iteration 137/1000 | Loss: 0.00001629
Iteration 138/1000 | Loss: 0.00001629
Iteration 139/1000 | Loss: 0.00001629
Iteration 140/1000 | Loss: 0.00001629
Iteration 141/1000 | Loss: 0.00001629
Iteration 142/1000 | Loss: 0.00001629
Iteration 143/1000 | Loss: 0.00001629
Iteration 144/1000 | Loss: 0.00001629
Iteration 145/1000 | Loss: 0.00001629
Iteration 146/1000 | Loss: 0.00001629
Iteration 147/1000 | Loss: 0.00001629
Iteration 148/1000 | Loss: 0.00001629
Iteration 149/1000 | Loss: 0.00001629
Iteration 150/1000 | Loss: 0.00001629
Iteration 151/1000 | Loss: 0.00001629
Iteration 152/1000 | Loss: 0.00001629
Iteration 153/1000 | Loss: 0.00001629
Iteration 154/1000 | Loss: 0.00001629
Iteration 155/1000 | Loss: 0.00001629
Iteration 156/1000 | Loss: 0.00001629
Iteration 157/1000 | Loss: 0.00001629
Iteration 158/1000 | Loss: 0.00001629
Iteration 159/1000 | Loss: 0.00001629
Iteration 160/1000 | Loss: 0.00001629
Iteration 161/1000 | Loss: 0.00001629
Iteration 162/1000 | Loss: 0.00001629
Iteration 163/1000 | Loss: 0.00001629
Iteration 164/1000 | Loss: 0.00001629
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 164. Stopping optimization.
Last 5 losses: [1.628621066629421e-05, 1.628621066629421e-05, 1.628621066629421e-05, 1.628621066629421e-05, 1.628621066629421e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.628621066629421e-05

Optimization complete. Final v2v error: 3.396091938018799 mm

Highest mean error: 4.0832977294921875 mm for frame 50

Lowest mean error: 2.7542905807495117 mm for frame 9

Saving results

Total time: 42.99305987358093
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_025/1026/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_025/1026.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_025/1026
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00426580
Iteration 2/25 | Loss: 0.00133947
Iteration 3/25 | Loss: 0.00118107
Iteration 4/25 | Loss: 0.00116021
Iteration 5/25 | Loss: 0.00115436
Iteration 6/25 | Loss: 0.00115348
Iteration 7/25 | Loss: 0.00115348
Iteration 8/25 | Loss: 0.00115348
Iteration 9/25 | Loss: 0.00115348
Iteration 10/25 | Loss: 0.00115348
Iteration 11/25 | Loss: 0.00115348
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011534796794876456, 0.0011534796794876456, 0.0011534796794876456, 0.0011534796794876456, 0.0011534796794876456]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011534796794876456

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.40022469
Iteration 2/25 | Loss: 0.00102352
Iteration 3/25 | Loss: 0.00102351
Iteration 4/25 | Loss: 0.00102351
Iteration 5/25 | Loss: 0.00102351
Iteration 6/25 | Loss: 0.00102351
Iteration 7/25 | Loss: 0.00102351
Iteration 8/25 | Loss: 0.00102351
Iteration 9/25 | Loss: 0.00102351
Iteration 10/25 | Loss: 0.00102351
Iteration 11/25 | Loss: 0.00102351
Iteration 12/25 | Loss: 0.00102351
Iteration 13/25 | Loss: 0.00102351
Iteration 14/25 | Loss: 0.00102351
Iteration 15/25 | Loss: 0.00102351
Iteration 16/25 | Loss: 0.00102351
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0010235097724944353, 0.0010235097724944353, 0.0010235097724944353, 0.0010235097724944353, 0.0010235097724944353]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010235097724944353

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00102351
Iteration 2/1000 | Loss: 0.00003492
Iteration 3/1000 | Loss: 0.00002179
Iteration 4/1000 | Loss: 0.00001935
Iteration 5/1000 | Loss: 0.00001818
Iteration 6/1000 | Loss: 0.00001731
Iteration 7/1000 | Loss: 0.00001661
Iteration 8/1000 | Loss: 0.00001619
Iteration 9/1000 | Loss: 0.00001587
Iteration 10/1000 | Loss: 0.00001552
Iteration 11/1000 | Loss: 0.00001530
Iteration 12/1000 | Loss: 0.00001512
Iteration 13/1000 | Loss: 0.00001509
Iteration 14/1000 | Loss: 0.00001509
Iteration 15/1000 | Loss: 0.00001504
Iteration 16/1000 | Loss: 0.00001503
Iteration 17/1000 | Loss: 0.00001502
Iteration 18/1000 | Loss: 0.00001498
Iteration 19/1000 | Loss: 0.00001496
Iteration 20/1000 | Loss: 0.00001493
Iteration 21/1000 | Loss: 0.00001493
Iteration 22/1000 | Loss: 0.00001491
Iteration 23/1000 | Loss: 0.00001490
Iteration 24/1000 | Loss: 0.00001487
Iteration 25/1000 | Loss: 0.00001485
Iteration 26/1000 | Loss: 0.00001482
Iteration 27/1000 | Loss: 0.00001478
Iteration 28/1000 | Loss: 0.00001478
Iteration 29/1000 | Loss: 0.00001478
Iteration 30/1000 | Loss: 0.00001477
Iteration 31/1000 | Loss: 0.00001477
Iteration 32/1000 | Loss: 0.00001475
Iteration 33/1000 | Loss: 0.00001475
Iteration 34/1000 | Loss: 0.00001475
Iteration 35/1000 | Loss: 0.00001475
Iteration 36/1000 | Loss: 0.00001475
Iteration 37/1000 | Loss: 0.00001474
Iteration 38/1000 | Loss: 0.00001474
Iteration 39/1000 | Loss: 0.00001474
Iteration 40/1000 | Loss: 0.00001474
Iteration 41/1000 | Loss: 0.00001474
Iteration 42/1000 | Loss: 0.00001474
Iteration 43/1000 | Loss: 0.00001474
Iteration 44/1000 | Loss: 0.00001474
Iteration 45/1000 | Loss: 0.00001474
Iteration 46/1000 | Loss: 0.00001473
Iteration 47/1000 | Loss: 0.00001473
Iteration 48/1000 | Loss: 0.00001473
Iteration 49/1000 | Loss: 0.00001472
Iteration 50/1000 | Loss: 0.00001472
Iteration 51/1000 | Loss: 0.00001470
Iteration 52/1000 | Loss: 0.00001470
Iteration 53/1000 | Loss: 0.00001469
Iteration 54/1000 | Loss: 0.00001468
Iteration 55/1000 | Loss: 0.00001467
Iteration 56/1000 | Loss: 0.00001467
Iteration 57/1000 | Loss: 0.00001465
Iteration 58/1000 | Loss: 0.00001465
Iteration 59/1000 | Loss: 0.00001465
Iteration 60/1000 | Loss: 0.00001465
Iteration 61/1000 | Loss: 0.00001465
Iteration 62/1000 | Loss: 0.00001465
Iteration 63/1000 | Loss: 0.00001465
Iteration 64/1000 | Loss: 0.00001465
Iteration 65/1000 | Loss: 0.00001464
Iteration 66/1000 | Loss: 0.00001464
Iteration 67/1000 | Loss: 0.00001464
Iteration 68/1000 | Loss: 0.00001464
Iteration 69/1000 | Loss: 0.00001464
Iteration 70/1000 | Loss: 0.00001464
Iteration 71/1000 | Loss: 0.00001464
Iteration 72/1000 | Loss: 0.00001464
Iteration 73/1000 | Loss: 0.00001464
Iteration 74/1000 | Loss: 0.00001462
Iteration 75/1000 | Loss: 0.00001462
Iteration 76/1000 | Loss: 0.00001462
Iteration 77/1000 | Loss: 0.00001461
Iteration 78/1000 | Loss: 0.00001461
Iteration 79/1000 | Loss: 0.00001461
Iteration 80/1000 | Loss: 0.00001461
Iteration 81/1000 | Loss: 0.00001461
Iteration 82/1000 | Loss: 0.00001461
Iteration 83/1000 | Loss: 0.00001460
Iteration 84/1000 | Loss: 0.00001460
Iteration 85/1000 | Loss: 0.00001460
Iteration 86/1000 | Loss: 0.00001460
Iteration 87/1000 | Loss: 0.00001459
Iteration 88/1000 | Loss: 0.00001459
Iteration 89/1000 | Loss: 0.00001459
Iteration 90/1000 | Loss: 0.00001459
Iteration 91/1000 | Loss: 0.00001458
Iteration 92/1000 | Loss: 0.00001458
Iteration 93/1000 | Loss: 0.00001458
Iteration 94/1000 | Loss: 0.00001458
Iteration 95/1000 | Loss: 0.00001457
Iteration 96/1000 | Loss: 0.00001457
Iteration 97/1000 | Loss: 0.00001457
Iteration 98/1000 | Loss: 0.00001457
Iteration 99/1000 | Loss: 0.00001457
Iteration 100/1000 | Loss: 0.00001457
Iteration 101/1000 | Loss: 0.00001456
Iteration 102/1000 | Loss: 0.00001456
Iteration 103/1000 | Loss: 0.00001456
Iteration 104/1000 | Loss: 0.00001455
Iteration 105/1000 | Loss: 0.00001455
Iteration 106/1000 | Loss: 0.00001455
Iteration 107/1000 | Loss: 0.00001454
Iteration 108/1000 | Loss: 0.00001454
Iteration 109/1000 | Loss: 0.00001454
Iteration 110/1000 | Loss: 0.00001453
Iteration 111/1000 | Loss: 0.00001453
Iteration 112/1000 | Loss: 0.00001452
Iteration 113/1000 | Loss: 0.00001451
Iteration 114/1000 | Loss: 0.00001451
Iteration 115/1000 | Loss: 0.00001451
Iteration 116/1000 | Loss: 0.00001451
Iteration 117/1000 | Loss: 0.00001451
Iteration 118/1000 | Loss: 0.00001450
Iteration 119/1000 | Loss: 0.00001450
Iteration 120/1000 | Loss: 0.00001450
Iteration 121/1000 | Loss: 0.00001449
Iteration 122/1000 | Loss: 0.00001449
Iteration 123/1000 | Loss: 0.00001449
Iteration 124/1000 | Loss: 0.00001448
Iteration 125/1000 | Loss: 0.00001448
Iteration 126/1000 | Loss: 0.00001448
Iteration 127/1000 | Loss: 0.00001448
Iteration 128/1000 | Loss: 0.00001447
Iteration 129/1000 | Loss: 0.00001447
Iteration 130/1000 | Loss: 0.00001447
Iteration 131/1000 | Loss: 0.00001447
Iteration 132/1000 | Loss: 0.00001446
Iteration 133/1000 | Loss: 0.00001446
Iteration 134/1000 | Loss: 0.00001446
Iteration 135/1000 | Loss: 0.00001446
Iteration 136/1000 | Loss: 0.00001445
Iteration 137/1000 | Loss: 0.00001445
Iteration 138/1000 | Loss: 0.00001445
Iteration 139/1000 | Loss: 0.00001445
Iteration 140/1000 | Loss: 0.00001445
Iteration 141/1000 | Loss: 0.00001445
Iteration 142/1000 | Loss: 0.00001444
Iteration 143/1000 | Loss: 0.00001444
Iteration 144/1000 | Loss: 0.00001444
Iteration 145/1000 | Loss: 0.00001444
Iteration 146/1000 | Loss: 0.00001444
Iteration 147/1000 | Loss: 0.00001444
Iteration 148/1000 | Loss: 0.00001444
Iteration 149/1000 | Loss: 0.00001444
Iteration 150/1000 | Loss: 0.00001444
Iteration 151/1000 | Loss: 0.00001443
Iteration 152/1000 | Loss: 0.00001443
Iteration 153/1000 | Loss: 0.00001443
Iteration 154/1000 | Loss: 0.00001443
Iteration 155/1000 | Loss: 0.00001443
Iteration 156/1000 | Loss: 0.00001443
Iteration 157/1000 | Loss: 0.00001443
Iteration 158/1000 | Loss: 0.00001443
Iteration 159/1000 | Loss: 0.00001443
Iteration 160/1000 | Loss: 0.00001443
Iteration 161/1000 | Loss: 0.00001443
Iteration 162/1000 | Loss: 0.00001443
Iteration 163/1000 | Loss: 0.00001443
Iteration 164/1000 | Loss: 0.00001443
Iteration 165/1000 | Loss: 0.00001443
Iteration 166/1000 | Loss: 0.00001443
Iteration 167/1000 | Loss: 0.00001443
Iteration 168/1000 | Loss: 0.00001443
Iteration 169/1000 | Loss: 0.00001443
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 169. Stopping optimization.
Last 5 losses: [1.4427429050556384e-05, 1.4427429050556384e-05, 1.4427429050556384e-05, 1.4427429050556384e-05, 1.4427429050556384e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4427429050556384e-05

Optimization complete. Final v2v error: 3.204740285873413 mm

Highest mean error: 3.8667895793914795 mm for frame 14

Lowest mean error: 2.71185564994812 mm for frame 166

Saving results

Total time: 47.684669494628906
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_025/1063/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_025/1063.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_025/1063
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01011565
Iteration 2/25 | Loss: 0.00252589
Iteration 3/25 | Loss: 0.00186294
Iteration 4/25 | Loss: 0.00167029
Iteration 5/25 | Loss: 0.00145678
Iteration 6/25 | Loss: 0.00143868
Iteration 7/25 | Loss: 0.00137404
Iteration 8/25 | Loss: 0.00139937
Iteration 9/25 | Loss: 0.00138762
Iteration 10/25 | Loss: 0.00139264
Iteration 11/25 | Loss: 0.00137926
Iteration 12/25 | Loss: 0.00134200
Iteration 13/25 | Loss: 0.00135923
Iteration 14/25 | Loss: 0.00135366
Iteration 15/25 | Loss: 0.00132705
Iteration 16/25 | Loss: 0.00129858
Iteration 17/25 | Loss: 0.00129299
Iteration 18/25 | Loss: 0.00130600
Iteration 19/25 | Loss: 0.00128737
Iteration 20/25 | Loss: 0.00127595
Iteration 21/25 | Loss: 0.00127308
Iteration 22/25 | Loss: 0.00126852
Iteration 23/25 | Loss: 0.00126744
Iteration 24/25 | Loss: 0.00126712
Iteration 25/25 | Loss: 0.00126692

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.41776550
Iteration 2/25 | Loss: 0.00118083
Iteration 3/25 | Loss: 0.00118083
Iteration 4/25 | Loss: 0.00118083
Iteration 5/25 | Loss: 0.00118083
Iteration 6/25 | Loss: 0.00118083
Iteration 7/25 | Loss: 0.00118083
Iteration 8/25 | Loss: 0.00118083
Iteration 9/25 | Loss: 0.00118083
Iteration 10/25 | Loss: 0.00118083
Iteration 11/25 | Loss: 0.00118083
Iteration 12/25 | Loss: 0.00118083
Iteration 13/25 | Loss: 0.00118083
Iteration 14/25 | Loss: 0.00118083
Iteration 15/25 | Loss: 0.00118082
Iteration 16/25 | Loss: 0.00118082
Iteration 17/25 | Loss: 0.00118082
Iteration 18/25 | Loss: 0.00118082
Iteration 19/25 | Loss: 0.00118082
Iteration 20/25 | Loss: 0.00118082
Iteration 21/25 | Loss: 0.00118082
Iteration 22/25 | Loss: 0.00118082
Iteration 23/25 | Loss: 0.00118082
Iteration 24/25 | Loss: 0.00118082
Iteration 25/25 | Loss: 0.00118082

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00118082
Iteration 2/1000 | Loss: 0.00009159
Iteration 3/1000 | Loss: 0.00031607
Iteration 4/1000 | Loss: 0.00015711
Iteration 5/1000 | Loss: 0.00016204
Iteration 6/1000 | Loss: 0.00023801
Iteration 7/1000 | Loss: 0.00014400
Iteration 8/1000 | Loss: 0.00034646
Iteration 9/1000 | Loss: 0.00013140
Iteration 10/1000 | Loss: 0.00013983
Iteration 11/1000 | Loss: 0.00004833
Iteration 12/1000 | Loss: 0.00005376
Iteration 13/1000 | Loss: 0.00004390
Iteration 14/1000 | Loss: 0.00011916
Iteration 15/1000 | Loss: 0.00029679
Iteration 16/1000 | Loss: 0.00005392
Iteration 17/1000 | Loss: 0.00013506
Iteration 18/1000 | Loss: 0.00005359
Iteration 19/1000 | Loss: 0.00004167
Iteration 20/1000 | Loss: 0.00004111
Iteration 21/1000 | Loss: 0.00132271
Iteration 22/1000 | Loss: 0.00059788
Iteration 23/1000 | Loss: 0.00009384
Iteration 24/1000 | Loss: 0.00025444
Iteration 25/1000 | Loss: 0.00013879
Iteration 26/1000 | Loss: 0.00040115
Iteration 27/1000 | Loss: 0.00003290
Iteration 28/1000 | Loss: 0.00041342
Iteration 29/1000 | Loss: 0.00009340
Iteration 30/1000 | Loss: 0.00031836
Iteration 31/1000 | Loss: 0.00008211
Iteration 32/1000 | Loss: 0.00004045
Iteration 33/1000 | Loss: 0.00012933
Iteration 34/1000 | Loss: 0.00087825
Iteration 35/1000 | Loss: 0.00003228
Iteration 36/1000 | Loss: 0.00010671
Iteration 37/1000 | Loss: 0.00004973
Iteration 38/1000 | Loss: 0.00002357
Iteration 39/1000 | Loss: 0.00003485
Iteration 40/1000 | Loss: 0.00002149
Iteration 41/1000 | Loss: 0.00007715
Iteration 42/1000 | Loss: 0.00085961
Iteration 43/1000 | Loss: 0.00422954
Iteration 44/1000 | Loss: 0.00106837
Iteration 45/1000 | Loss: 0.00107703
Iteration 46/1000 | Loss: 0.00174786
Iteration 47/1000 | Loss: 0.00003705
Iteration 48/1000 | Loss: 0.00009982
Iteration 49/1000 | Loss: 0.00003336
Iteration 50/1000 | Loss: 0.00002234
Iteration 51/1000 | Loss: 0.00004758
Iteration 52/1000 | Loss: 0.00002021
Iteration 53/1000 | Loss: 0.00001991
Iteration 54/1000 | Loss: 0.00001982
Iteration 55/1000 | Loss: 0.00001961
Iteration 56/1000 | Loss: 0.00001944
Iteration 57/1000 | Loss: 0.00001941
Iteration 58/1000 | Loss: 0.00001938
Iteration 59/1000 | Loss: 0.00001932
Iteration 60/1000 | Loss: 0.00001915
Iteration 61/1000 | Loss: 0.00001912
Iteration 62/1000 | Loss: 0.00001905
Iteration 63/1000 | Loss: 0.00001904
Iteration 64/1000 | Loss: 0.00001904
Iteration 65/1000 | Loss: 0.00001902
Iteration 66/1000 | Loss: 0.00006899
Iteration 67/1000 | Loss: 0.00023471
Iteration 68/1000 | Loss: 0.00047717
Iteration 69/1000 | Loss: 0.00012438
Iteration 70/1000 | Loss: 0.00029208
Iteration 71/1000 | Loss: 0.00001984
Iteration 72/1000 | Loss: 0.00001905
Iteration 73/1000 | Loss: 0.00001889
Iteration 74/1000 | Loss: 0.00001883
Iteration 75/1000 | Loss: 0.00006114
Iteration 76/1000 | Loss: 0.00001898
Iteration 77/1000 | Loss: 0.00001874
Iteration 78/1000 | Loss: 0.00001873
Iteration 79/1000 | Loss: 0.00001871
Iteration 80/1000 | Loss: 0.00001870
Iteration 81/1000 | Loss: 0.00001870
Iteration 82/1000 | Loss: 0.00001870
Iteration 83/1000 | Loss: 0.00001870
Iteration 84/1000 | Loss: 0.00008449
Iteration 85/1000 | Loss: 0.00003085
Iteration 86/1000 | Loss: 0.00001872
Iteration 87/1000 | Loss: 0.00001869
Iteration 88/1000 | Loss: 0.00001869
Iteration 89/1000 | Loss: 0.00001869
Iteration 90/1000 | Loss: 0.00001869
Iteration 91/1000 | Loss: 0.00001869
Iteration 92/1000 | Loss: 0.00001869
Iteration 93/1000 | Loss: 0.00001869
Iteration 94/1000 | Loss: 0.00001869
Iteration 95/1000 | Loss: 0.00001869
Iteration 96/1000 | Loss: 0.00006344
Iteration 97/1000 | Loss: 0.00001878
Iteration 98/1000 | Loss: 0.00001872
Iteration 99/1000 | Loss: 0.00001871
Iteration 100/1000 | Loss: 0.00001871
Iteration 101/1000 | Loss: 0.00007109
Iteration 102/1000 | Loss: 0.00023223
Iteration 103/1000 | Loss: 0.00001913
Iteration 104/1000 | Loss: 0.00005892
Iteration 105/1000 | Loss: 0.00002042
Iteration 106/1000 | Loss: 0.00007702
Iteration 107/1000 | Loss: 0.00028201
Iteration 108/1000 | Loss: 0.00002210
Iteration 109/1000 | Loss: 0.00001892
Iteration 110/1000 | Loss: 0.00001874
Iteration 111/1000 | Loss: 0.00004504
Iteration 112/1000 | Loss: 0.00001885
Iteration 113/1000 | Loss: 0.00001872
Iteration 114/1000 | Loss: 0.00001871
Iteration 115/1000 | Loss: 0.00001871
Iteration 116/1000 | Loss: 0.00001870
Iteration 117/1000 | Loss: 0.00001870
Iteration 118/1000 | Loss: 0.00001869
Iteration 119/1000 | Loss: 0.00001869
Iteration 120/1000 | Loss: 0.00001869
Iteration 121/1000 | Loss: 0.00001869
Iteration 122/1000 | Loss: 0.00001869
Iteration 123/1000 | Loss: 0.00001869
Iteration 124/1000 | Loss: 0.00001869
Iteration 125/1000 | Loss: 0.00001869
Iteration 126/1000 | Loss: 0.00001869
Iteration 127/1000 | Loss: 0.00001868
Iteration 128/1000 | Loss: 0.00001868
Iteration 129/1000 | Loss: 0.00001868
Iteration 130/1000 | Loss: 0.00001868
Iteration 131/1000 | Loss: 0.00001868
Iteration 132/1000 | Loss: 0.00001868
Iteration 133/1000 | Loss: 0.00001867
Iteration 134/1000 | Loss: 0.00001867
Iteration 135/1000 | Loss: 0.00001867
Iteration 136/1000 | Loss: 0.00001867
Iteration 137/1000 | Loss: 0.00001866
Iteration 138/1000 | Loss: 0.00001866
Iteration 139/1000 | Loss: 0.00001866
Iteration 140/1000 | Loss: 0.00001866
Iteration 141/1000 | Loss: 0.00001866
Iteration 142/1000 | Loss: 0.00001866
Iteration 143/1000 | Loss: 0.00001866
Iteration 144/1000 | Loss: 0.00001866
Iteration 145/1000 | Loss: 0.00001866
Iteration 146/1000 | Loss: 0.00001866
Iteration 147/1000 | Loss: 0.00001866
Iteration 148/1000 | Loss: 0.00001866
Iteration 149/1000 | Loss: 0.00001865
Iteration 150/1000 | Loss: 0.00001865
Iteration 151/1000 | Loss: 0.00001865
Iteration 152/1000 | Loss: 0.00001865
Iteration 153/1000 | Loss: 0.00001865
Iteration 154/1000 | Loss: 0.00001865
Iteration 155/1000 | Loss: 0.00001865
Iteration 156/1000 | Loss: 0.00001865
Iteration 157/1000 | Loss: 0.00001865
Iteration 158/1000 | Loss: 0.00001865
Iteration 159/1000 | Loss: 0.00001865
Iteration 160/1000 | Loss: 0.00001865
Iteration 161/1000 | Loss: 0.00001864
Iteration 162/1000 | Loss: 0.00001864
Iteration 163/1000 | Loss: 0.00001864
Iteration 164/1000 | Loss: 0.00001864
Iteration 165/1000 | Loss: 0.00001864
Iteration 166/1000 | Loss: 0.00001864
Iteration 167/1000 | Loss: 0.00001864
Iteration 168/1000 | Loss: 0.00001864
Iteration 169/1000 | Loss: 0.00001864
Iteration 170/1000 | Loss: 0.00001864
Iteration 171/1000 | Loss: 0.00001864
Iteration 172/1000 | Loss: 0.00001864
Iteration 173/1000 | Loss: 0.00001863
Iteration 174/1000 | Loss: 0.00001863
Iteration 175/1000 | Loss: 0.00001863
Iteration 176/1000 | Loss: 0.00001863
Iteration 177/1000 | Loss: 0.00001863
Iteration 178/1000 | Loss: 0.00001863
Iteration 179/1000 | Loss: 0.00001863
Iteration 180/1000 | Loss: 0.00001863
Iteration 181/1000 | Loss: 0.00001863
Iteration 182/1000 | Loss: 0.00001863
Iteration 183/1000 | Loss: 0.00001863
Iteration 184/1000 | Loss: 0.00001863
Iteration 185/1000 | Loss: 0.00001863
Iteration 186/1000 | Loss: 0.00001863
Iteration 187/1000 | Loss: 0.00001863
Iteration 188/1000 | Loss: 0.00001863
Iteration 189/1000 | Loss: 0.00001863
Iteration 190/1000 | Loss: 0.00001863
Iteration 191/1000 | Loss: 0.00001863
Iteration 192/1000 | Loss: 0.00001863
Iteration 193/1000 | Loss: 0.00001863
Iteration 194/1000 | Loss: 0.00001863
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 194. Stopping optimization.
Last 5 losses: [1.8630764316185378e-05, 1.8630764316185378e-05, 1.8630764316185378e-05, 1.8630764316185378e-05, 1.8630764316185378e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8630764316185378e-05

Optimization complete. Final v2v error: 3.6159536838531494 mm

Highest mean error: 4.285253524780273 mm for frame 81

Lowest mean error: 3.239551305770874 mm for frame 151

Saving results

Total time: 169.1246635913849
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_025/1095/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_025/1095.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_025/1095
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00739770
Iteration 2/25 | Loss: 0.00171006
Iteration 3/25 | Loss: 0.00131117
Iteration 4/25 | Loss: 0.00126926
Iteration 5/25 | Loss: 0.00127168
Iteration 6/25 | Loss: 0.00126290
Iteration 7/25 | Loss: 0.00124008
Iteration 8/25 | Loss: 0.00124963
Iteration 9/25 | Loss: 0.00123308
Iteration 10/25 | Loss: 0.00123384
Iteration 11/25 | Loss: 0.00123066
Iteration 12/25 | Loss: 0.00123021
Iteration 13/25 | Loss: 0.00123005
Iteration 14/25 | Loss: 0.00123004
Iteration 15/25 | Loss: 0.00123003
Iteration 16/25 | Loss: 0.00123003
Iteration 17/25 | Loss: 0.00123003
Iteration 18/25 | Loss: 0.00123003
Iteration 19/25 | Loss: 0.00123003
Iteration 20/25 | Loss: 0.00123003
Iteration 21/25 | Loss: 0.00123003
Iteration 22/25 | Loss: 0.00123003
Iteration 23/25 | Loss: 0.00123003
Iteration 24/25 | Loss: 0.00123002
Iteration 25/25 | Loss: 0.00123002

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.70758033
Iteration 2/25 | Loss: 0.00139042
Iteration 3/25 | Loss: 0.00089037
Iteration 4/25 | Loss: 0.00089037
Iteration 5/25 | Loss: 0.00089037
Iteration 6/25 | Loss: 0.00089037
Iteration 7/25 | Loss: 0.00089037
Iteration 8/25 | Loss: 0.00089037
Iteration 9/25 | Loss: 0.00089037
Iteration 10/25 | Loss: 0.00089037
Iteration 11/25 | Loss: 0.00089037
Iteration 12/25 | Loss: 0.00089037
Iteration 13/25 | Loss: 0.00089037
Iteration 14/25 | Loss: 0.00089037
Iteration 15/25 | Loss: 0.00089037
Iteration 16/25 | Loss: 0.00089037
Iteration 17/25 | Loss: 0.00089037
Iteration 18/25 | Loss: 0.00089037
Iteration 19/25 | Loss: 0.00089037
Iteration 20/25 | Loss: 0.00089037
Iteration 21/25 | Loss: 0.00089037
Iteration 22/25 | Loss: 0.00089037
Iteration 23/25 | Loss: 0.00089037
Iteration 24/25 | Loss: 0.00089037
Iteration 25/25 | Loss: 0.00089037

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00089037
Iteration 2/1000 | Loss: 0.00007077
Iteration 3/1000 | Loss: 0.00023136
Iteration 4/1000 | Loss: 0.00010206
Iteration 5/1000 | Loss: 0.00003763
Iteration 6/1000 | Loss: 0.00005778
Iteration 7/1000 | Loss: 0.00003184
Iteration 8/1000 | Loss: 0.00067840
Iteration 9/1000 | Loss: 0.00002905
Iteration 10/1000 | Loss: 0.00002610
Iteration 11/1000 | Loss: 0.00002502
Iteration 12/1000 | Loss: 0.00015518
Iteration 13/1000 | Loss: 0.00002628
Iteration 14/1000 | Loss: 0.00002398
Iteration 15/1000 | Loss: 0.00002306
Iteration 16/1000 | Loss: 0.00002229
Iteration 17/1000 | Loss: 0.00002170
Iteration 18/1000 | Loss: 0.00002138
Iteration 19/1000 | Loss: 0.00002129
Iteration 20/1000 | Loss: 0.00002121
Iteration 21/1000 | Loss: 0.00002111
Iteration 22/1000 | Loss: 0.00002110
Iteration 23/1000 | Loss: 0.00002101
Iteration 24/1000 | Loss: 0.00002089
Iteration 25/1000 | Loss: 0.00002088
Iteration 26/1000 | Loss: 0.00002088
Iteration 27/1000 | Loss: 0.00002083
Iteration 28/1000 | Loss: 0.00002076
Iteration 29/1000 | Loss: 0.00002075
Iteration 30/1000 | Loss: 0.00002074
Iteration 31/1000 | Loss: 0.00002073
Iteration 32/1000 | Loss: 0.00002073
Iteration 33/1000 | Loss: 0.00002072
Iteration 34/1000 | Loss: 0.00002072
Iteration 35/1000 | Loss: 0.00002072
Iteration 36/1000 | Loss: 0.00002072
Iteration 37/1000 | Loss: 0.00002072
Iteration 38/1000 | Loss: 0.00002071
Iteration 39/1000 | Loss: 0.00002071
Iteration 40/1000 | Loss: 0.00002071
Iteration 41/1000 | Loss: 0.00002071
Iteration 42/1000 | Loss: 0.00002071
Iteration 43/1000 | Loss: 0.00002068
Iteration 44/1000 | Loss: 0.00002068
Iteration 45/1000 | Loss: 0.00002068
Iteration 46/1000 | Loss: 0.00002067
Iteration 47/1000 | Loss: 0.00002067
Iteration 48/1000 | Loss: 0.00002067
Iteration 49/1000 | Loss: 0.00002066
Iteration 50/1000 | Loss: 0.00002066
Iteration 51/1000 | Loss: 0.00002066
Iteration 52/1000 | Loss: 0.00002065
Iteration 53/1000 | Loss: 0.00002065
Iteration 54/1000 | Loss: 0.00002064
Iteration 55/1000 | Loss: 0.00002064
Iteration 56/1000 | Loss: 0.00002063
Iteration 57/1000 | Loss: 0.00002063
Iteration 58/1000 | Loss: 0.00002062
Iteration 59/1000 | Loss: 0.00002062
Iteration 60/1000 | Loss: 0.00002061
Iteration 61/1000 | Loss: 0.00002061
Iteration 62/1000 | Loss: 0.00002061
Iteration 63/1000 | Loss: 0.00002060
Iteration 64/1000 | Loss: 0.00002060
Iteration 65/1000 | Loss: 0.00002059
Iteration 66/1000 | Loss: 0.00002059
Iteration 67/1000 | Loss: 0.00002059
Iteration 68/1000 | Loss: 0.00002059
Iteration 69/1000 | Loss: 0.00002059
Iteration 70/1000 | Loss: 0.00002059
Iteration 71/1000 | Loss: 0.00002058
Iteration 72/1000 | Loss: 0.00002058
Iteration 73/1000 | Loss: 0.00002058
Iteration 74/1000 | Loss: 0.00002058
Iteration 75/1000 | Loss: 0.00002058
Iteration 76/1000 | Loss: 0.00002058
Iteration 77/1000 | Loss: 0.00002058
Iteration 78/1000 | Loss: 0.00002057
Iteration 79/1000 | Loss: 0.00002057
Iteration 80/1000 | Loss: 0.00002057
Iteration 81/1000 | Loss: 0.00002057
Iteration 82/1000 | Loss: 0.00002057
Iteration 83/1000 | Loss: 0.00002057
Iteration 84/1000 | Loss: 0.00002057
Iteration 85/1000 | Loss: 0.00002057
Iteration 86/1000 | Loss: 0.00002057
Iteration 87/1000 | Loss: 0.00002057
Iteration 88/1000 | Loss: 0.00002057
Iteration 89/1000 | Loss: 0.00002057
Iteration 90/1000 | Loss: 0.00002057
Iteration 91/1000 | Loss: 0.00002057
Iteration 92/1000 | Loss: 0.00002057
Iteration 93/1000 | Loss: 0.00002057
Iteration 94/1000 | Loss: 0.00002056
Iteration 95/1000 | Loss: 0.00002056
Iteration 96/1000 | Loss: 0.00002056
Iteration 97/1000 | Loss: 0.00002056
Iteration 98/1000 | Loss: 0.00002056
Iteration 99/1000 | Loss: 0.00002056
Iteration 100/1000 | Loss: 0.00002055
Iteration 101/1000 | Loss: 0.00002055
Iteration 102/1000 | Loss: 0.00002055
Iteration 103/1000 | Loss: 0.00002055
Iteration 104/1000 | Loss: 0.00002055
Iteration 105/1000 | Loss: 0.00002055
Iteration 106/1000 | Loss: 0.00002055
Iteration 107/1000 | Loss: 0.00002055
Iteration 108/1000 | Loss: 0.00002055
Iteration 109/1000 | Loss: 0.00002055
Iteration 110/1000 | Loss: 0.00002055
Iteration 111/1000 | Loss: 0.00002055
Iteration 112/1000 | Loss: 0.00002055
Iteration 113/1000 | Loss: 0.00002055
Iteration 114/1000 | Loss: 0.00002055
Iteration 115/1000 | Loss: 0.00002055
Iteration 116/1000 | Loss: 0.00002055
Iteration 117/1000 | Loss: 0.00002055
Iteration 118/1000 | Loss: 0.00002055
Iteration 119/1000 | Loss: 0.00002055
Iteration 120/1000 | Loss: 0.00002055
Iteration 121/1000 | Loss: 0.00002055
Iteration 122/1000 | Loss: 0.00002055
Iteration 123/1000 | Loss: 0.00002055
Iteration 124/1000 | Loss: 0.00002055
Iteration 125/1000 | Loss: 0.00002055
Iteration 126/1000 | Loss: 0.00002055
Iteration 127/1000 | Loss: 0.00002055
Iteration 128/1000 | Loss: 0.00002055
Iteration 129/1000 | Loss: 0.00002055
Iteration 130/1000 | Loss: 0.00002055
Iteration 131/1000 | Loss: 0.00002055
Iteration 132/1000 | Loss: 0.00002055
Iteration 133/1000 | Loss: 0.00002055
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 133. Stopping optimization.
Last 5 losses: [2.0547679014271125e-05, 2.0547679014271125e-05, 2.0547679014271125e-05, 2.0547679014271125e-05, 2.0547679014271125e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0547679014271125e-05

Optimization complete. Final v2v error: 3.787372350692749 mm

Highest mean error: 4.819641590118408 mm for frame 192

Lowest mean error: 3.066368579864502 mm for frame 230

Saving results

Total time: 71.72838234901428
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_025/1088/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_025/1088.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_025/1088
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00975415
Iteration 2/25 | Loss: 0.00260155
Iteration 3/25 | Loss: 0.00201720
Iteration 4/25 | Loss: 0.00190538
Iteration 5/25 | Loss: 0.00186166
Iteration 6/25 | Loss: 0.00194238
Iteration 7/25 | Loss: 0.00189496
Iteration 8/25 | Loss: 0.00175245
Iteration 9/25 | Loss: 0.00165871
Iteration 10/25 | Loss: 0.00162719
Iteration 11/25 | Loss: 0.00159553
Iteration 12/25 | Loss: 0.00157612
Iteration 13/25 | Loss: 0.00155138
Iteration 14/25 | Loss: 0.00152527
Iteration 15/25 | Loss: 0.00152524
Iteration 16/25 | Loss: 0.00152434
Iteration 17/25 | Loss: 0.00152120
Iteration 18/25 | Loss: 0.00150760
Iteration 19/25 | Loss: 0.00149671
Iteration 20/25 | Loss: 0.00149063
Iteration 21/25 | Loss: 0.00148583
Iteration 22/25 | Loss: 0.00148360
Iteration 23/25 | Loss: 0.00147582
Iteration 24/25 | Loss: 0.00147548
Iteration 25/25 | Loss: 0.00147047

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43776107
Iteration 2/25 | Loss: 0.00414220
Iteration 3/25 | Loss: 0.00414219
Iteration 4/25 | Loss: 0.00414219
Iteration 5/25 | Loss: 0.00414219
Iteration 6/25 | Loss: 0.00414219
Iteration 7/25 | Loss: 0.00414219
Iteration 8/25 | Loss: 0.00414219
Iteration 9/25 | Loss: 0.00414219
Iteration 10/25 | Loss: 0.00414219
Iteration 11/25 | Loss: 0.00414219
Iteration 12/25 | Loss: 0.00414219
Iteration 13/25 | Loss: 0.00414219
Iteration 14/25 | Loss: 0.00414219
Iteration 15/25 | Loss: 0.00414219
Iteration 16/25 | Loss: 0.00414219
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.004142190329730511, 0.004142190329730511, 0.004142190329730511, 0.004142190329730511, 0.004142190329730511]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.004142190329730511

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00414219
Iteration 2/1000 | Loss: 0.00277530
Iteration 3/1000 | Loss: 0.00081119
Iteration 4/1000 | Loss: 0.00065592
Iteration 5/1000 | Loss: 0.00094768
Iteration 6/1000 | Loss: 0.00049789
Iteration 7/1000 | Loss: 0.00079665
Iteration 8/1000 | Loss: 0.00061085
Iteration 9/1000 | Loss: 0.00079116
Iteration 10/1000 | Loss: 0.00147020
Iteration 11/1000 | Loss: 0.00050768
Iteration 12/1000 | Loss: 0.00039334
Iteration 13/1000 | Loss: 0.00063810
Iteration 14/1000 | Loss: 0.00042028
Iteration 15/1000 | Loss: 0.00037738
Iteration 16/1000 | Loss: 0.00128233
Iteration 17/1000 | Loss: 0.00063757
Iteration 18/1000 | Loss: 0.00045509
Iteration 19/1000 | Loss: 0.00035959
Iteration 20/1000 | Loss: 0.00035298
Iteration 21/1000 | Loss: 0.00075502
Iteration 22/1000 | Loss: 0.00064385
Iteration 23/1000 | Loss: 0.00039576
Iteration 24/1000 | Loss: 0.00033686
Iteration 25/1000 | Loss: 0.00027824
Iteration 26/1000 | Loss: 0.00018056
Iteration 27/1000 | Loss: 0.00019495
Iteration 28/1000 | Loss: 0.00036117
Iteration 29/1000 | Loss: 0.00034961
Iteration 30/1000 | Loss: 0.00036307
Iteration 31/1000 | Loss: 0.00037977
Iteration 32/1000 | Loss: 0.00033305
Iteration 33/1000 | Loss: 0.00068276
Iteration 34/1000 | Loss: 0.00028640
Iteration 35/1000 | Loss: 0.00051130
Iteration 36/1000 | Loss: 0.00044039
Iteration 37/1000 | Loss: 0.00045763
Iteration 38/1000 | Loss: 0.00036813
Iteration 39/1000 | Loss: 0.00076877
Iteration 40/1000 | Loss: 0.00053397
Iteration 41/1000 | Loss: 0.00024212
Iteration 42/1000 | Loss: 0.00037978
Iteration 43/1000 | Loss: 0.00065641
Iteration 44/1000 | Loss: 0.00070557
Iteration 45/1000 | Loss: 0.00045053
Iteration 46/1000 | Loss: 0.00045300
Iteration 47/1000 | Loss: 0.00075184
Iteration 48/1000 | Loss: 0.00083078
Iteration 49/1000 | Loss: 0.00033419
Iteration 50/1000 | Loss: 0.00030986
Iteration 51/1000 | Loss: 0.00040446
Iteration 52/1000 | Loss: 0.00037626
Iteration 53/1000 | Loss: 0.00055236
Iteration 54/1000 | Loss: 0.00073516
Iteration 55/1000 | Loss: 0.00030401
Iteration 56/1000 | Loss: 0.00018144
Iteration 57/1000 | Loss: 0.00017269
Iteration 58/1000 | Loss: 0.00021163
Iteration 59/1000 | Loss: 0.00018982
Iteration 60/1000 | Loss: 0.00124571
Iteration 61/1000 | Loss: 0.00039576
Iteration 62/1000 | Loss: 0.00026468
Iteration 63/1000 | Loss: 0.00017632
Iteration 64/1000 | Loss: 0.00029667
Iteration 65/1000 | Loss: 0.00036638
Iteration 66/1000 | Loss: 0.00020109
Iteration 67/1000 | Loss: 0.00017422
Iteration 68/1000 | Loss: 0.00016307
Iteration 69/1000 | Loss: 0.00015807
Iteration 70/1000 | Loss: 0.00036117
Iteration 71/1000 | Loss: 0.00022597
Iteration 72/1000 | Loss: 0.00025471
Iteration 73/1000 | Loss: 0.00017513
Iteration 74/1000 | Loss: 0.00028275
Iteration 75/1000 | Loss: 0.00031789
Iteration 76/1000 | Loss: 0.00033558
Iteration 77/1000 | Loss: 0.00048788
Iteration 78/1000 | Loss: 0.00083276
Iteration 79/1000 | Loss: 0.00074711
Iteration 80/1000 | Loss: 0.00032663
Iteration 81/1000 | Loss: 0.00029019
Iteration 82/1000 | Loss: 0.00016272
Iteration 83/1000 | Loss: 0.00031253
Iteration 84/1000 | Loss: 0.00015401
Iteration 85/1000 | Loss: 0.00013444
Iteration 86/1000 | Loss: 0.00014479
Iteration 87/1000 | Loss: 0.00028755
Iteration 88/1000 | Loss: 0.00014570
Iteration 89/1000 | Loss: 0.00014345
Iteration 90/1000 | Loss: 0.00012129
Iteration 91/1000 | Loss: 0.00015674
Iteration 92/1000 | Loss: 0.00015564
Iteration 93/1000 | Loss: 0.00047707
Iteration 94/1000 | Loss: 0.00044682
Iteration 95/1000 | Loss: 0.00014953
Iteration 96/1000 | Loss: 0.00013822
Iteration 97/1000 | Loss: 0.00015252
Iteration 98/1000 | Loss: 0.00015817
Iteration 99/1000 | Loss: 0.00052693
Iteration 100/1000 | Loss: 0.00052061
Iteration 101/1000 | Loss: 0.00026062
Iteration 102/1000 | Loss: 0.00024448
Iteration 103/1000 | Loss: 0.00016741
Iteration 104/1000 | Loss: 0.00016041
Iteration 105/1000 | Loss: 0.00046865
Iteration 106/1000 | Loss: 0.00027567
Iteration 107/1000 | Loss: 0.00026608
Iteration 108/1000 | Loss: 0.00019845
Iteration 109/1000 | Loss: 0.00022714
Iteration 110/1000 | Loss: 0.00028701
Iteration 111/1000 | Loss: 0.00021888
Iteration 112/1000 | Loss: 0.00026908
Iteration 113/1000 | Loss: 0.00020374
Iteration 114/1000 | Loss: 0.00013584
Iteration 115/1000 | Loss: 0.00029333
Iteration 116/1000 | Loss: 0.00014040
Iteration 117/1000 | Loss: 0.00013440
Iteration 118/1000 | Loss: 0.00014158
Iteration 119/1000 | Loss: 0.00014414
Iteration 120/1000 | Loss: 0.00015471
Iteration 121/1000 | Loss: 0.00013682
Iteration 122/1000 | Loss: 0.00014457
Iteration 123/1000 | Loss: 0.00014625
Iteration 124/1000 | Loss: 0.00014509
Iteration 125/1000 | Loss: 0.00014659
Iteration 126/1000 | Loss: 0.00013722
Iteration 127/1000 | Loss: 0.00023914
Iteration 128/1000 | Loss: 0.00023937
Iteration 129/1000 | Loss: 0.00036093
Iteration 130/1000 | Loss: 0.00046837
Iteration 131/1000 | Loss: 0.00041537
Iteration 132/1000 | Loss: 0.00032969
Iteration 133/1000 | Loss: 0.00058902
Iteration 134/1000 | Loss: 0.00027523
Iteration 135/1000 | Loss: 0.00013379
Iteration 136/1000 | Loss: 0.00012284
Iteration 137/1000 | Loss: 0.00011375
Iteration 138/1000 | Loss: 0.00010988
Iteration 139/1000 | Loss: 0.00011833
Iteration 140/1000 | Loss: 0.00012030
Iteration 141/1000 | Loss: 0.00011238
Iteration 142/1000 | Loss: 0.00011447
Iteration 143/1000 | Loss: 0.00051047
Iteration 144/1000 | Loss: 0.00011308
Iteration 145/1000 | Loss: 0.00019456
Iteration 146/1000 | Loss: 0.00021703
Iteration 147/1000 | Loss: 0.00032340
Iteration 148/1000 | Loss: 0.00012344
Iteration 149/1000 | Loss: 0.00011943
Iteration 150/1000 | Loss: 0.00011748
Iteration 151/1000 | Loss: 0.00012615
Iteration 152/1000 | Loss: 0.00011690
Iteration 153/1000 | Loss: 0.00012713
Iteration 154/1000 | Loss: 0.00011696
Iteration 155/1000 | Loss: 0.00011228
Iteration 156/1000 | Loss: 0.00027059
Iteration 157/1000 | Loss: 0.00054301
Iteration 158/1000 | Loss: 0.00032377
Iteration 159/1000 | Loss: 0.00013029
Iteration 160/1000 | Loss: 0.00013493
Iteration 161/1000 | Loss: 0.00024976
Iteration 162/1000 | Loss: 0.00013965
Iteration 163/1000 | Loss: 0.00011772
Iteration 164/1000 | Loss: 0.00010923
Iteration 165/1000 | Loss: 0.00011967
Iteration 166/1000 | Loss: 0.00012485
Iteration 167/1000 | Loss: 0.00012129
Iteration 168/1000 | Loss: 0.00012191
Iteration 169/1000 | Loss: 0.00012147
Iteration 170/1000 | Loss: 0.00012077
Iteration 171/1000 | Loss: 0.00055832
Iteration 172/1000 | Loss: 0.00019281
Iteration 173/1000 | Loss: 0.00013735
Iteration 174/1000 | Loss: 0.00011569
Iteration 175/1000 | Loss: 0.00011078
Iteration 176/1000 | Loss: 0.00025698
Iteration 177/1000 | Loss: 0.00022309
Iteration 178/1000 | Loss: 0.00031401
Iteration 179/1000 | Loss: 0.00020424
Iteration 180/1000 | Loss: 0.00032041
Iteration 181/1000 | Loss: 0.00012086
Iteration 182/1000 | Loss: 0.00012085
Iteration 183/1000 | Loss: 0.00011751
Iteration 184/1000 | Loss: 0.00026098
Iteration 185/1000 | Loss: 0.00010877
Iteration 186/1000 | Loss: 0.00012541
Iteration 187/1000 | Loss: 0.00012395
Iteration 188/1000 | Loss: 0.00012575
Iteration 189/1000 | Loss: 0.00011394
Iteration 190/1000 | Loss: 0.00028073
Iteration 191/1000 | Loss: 0.00024731
Iteration 192/1000 | Loss: 0.00030254
Iteration 193/1000 | Loss: 0.00027976
Iteration 194/1000 | Loss: 0.00022541
Iteration 195/1000 | Loss: 0.00022413
Iteration 196/1000 | Loss: 0.00012119
Iteration 197/1000 | Loss: 0.00012306
Iteration 198/1000 | Loss: 0.00011130
Iteration 199/1000 | Loss: 0.00011707
Iteration 200/1000 | Loss: 0.00011915
Iteration 201/1000 | Loss: 0.00011555
Iteration 202/1000 | Loss: 0.00011270
Iteration 203/1000 | Loss: 0.00011609
Iteration 204/1000 | Loss: 0.00012004
Iteration 205/1000 | Loss: 0.00012173
Iteration 206/1000 | Loss: 0.00012143
Iteration 207/1000 | Loss: 0.00011396
Iteration 208/1000 | Loss: 0.00011468
Iteration 209/1000 | Loss: 0.00032095
Iteration 210/1000 | Loss: 0.00039683
Iteration 211/1000 | Loss: 0.00032271
Iteration 212/1000 | Loss: 0.00015285
Iteration 213/1000 | Loss: 0.00012541
Iteration 214/1000 | Loss: 0.00043798
Iteration 215/1000 | Loss: 0.00022980
Iteration 216/1000 | Loss: 0.00045416
Iteration 217/1000 | Loss: 0.00048643
Iteration 218/1000 | Loss: 0.00102631
Iteration 219/1000 | Loss: 0.00019145
Iteration 220/1000 | Loss: 0.00025611
Iteration 221/1000 | Loss: 0.00014299
Iteration 222/1000 | Loss: 0.00049736
Iteration 223/1000 | Loss: 0.00041295
Iteration 224/1000 | Loss: 0.00052994
Iteration 225/1000 | Loss: 0.00036458
Iteration 226/1000 | Loss: 0.00014140
Iteration 227/1000 | Loss: 0.00019903
Iteration 228/1000 | Loss: 0.00037766
Iteration 229/1000 | Loss: 0.00034070
Iteration 230/1000 | Loss: 0.00014967
Iteration 231/1000 | Loss: 0.00011010
Iteration 232/1000 | Loss: 0.00024683
Iteration 233/1000 | Loss: 0.00025436
Iteration 234/1000 | Loss: 0.00022074
Iteration 235/1000 | Loss: 0.00041845
Iteration 236/1000 | Loss: 0.00028556
Iteration 237/1000 | Loss: 0.00032550
Iteration 238/1000 | Loss: 0.00027514
Iteration 239/1000 | Loss: 0.00020815
Iteration 240/1000 | Loss: 0.00010798
Iteration 241/1000 | Loss: 0.00010514
Iteration 242/1000 | Loss: 0.00010300
Iteration 243/1000 | Loss: 0.00018675
Iteration 244/1000 | Loss: 0.00018667
Iteration 245/1000 | Loss: 0.00123750
Iteration 246/1000 | Loss: 0.00109360
Iteration 247/1000 | Loss: 0.00097299
Iteration 248/1000 | Loss: 0.00027958
Iteration 249/1000 | Loss: 0.00026693
Iteration 250/1000 | Loss: 0.00014533
Iteration 251/1000 | Loss: 0.00015934
Iteration 252/1000 | Loss: 0.00013593
Iteration 253/1000 | Loss: 0.00010553
Iteration 254/1000 | Loss: 0.00011394
Iteration 255/1000 | Loss: 0.00018455
Iteration 256/1000 | Loss: 0.00017156
Iteration 257/1000 | Loss: 0.00083152
Iteration 258/1000 | Loss: 0.00018184
Iteration 259/1000 | Loss: 0.00011825
Iteration 260/1000 | Loss: 0.00010620
Iteration 261/1000 | Loss: 0.00058120
Iteration 262/1000 | Loss: 0.00063850
Iteration 263/1000 | Loss: 0.00045451
Iteration 264/1000 | Loss: 0.00014305
Iteration 265/1000 | Loss: 0.00020355
Iteration 266/1000 | Loss: 0.00038341
Iteration 267/1000 | Loss: 0.00039608
Iteration 268/1000 | Loss: 0.00042235
Iteration 269/1000 | Loss: 0.00047806
Iteration 270/1000 | Loss: 0.00050620
Iteration 271/1000 | Loss: 0.00024894
Iteration 272/1000 | Loss: 0.00009878
Iteration 273/1000 | Loss: 0.00022088
Iteration 274/1000 | Loss: 0.00009905
Iteration 275/1000 | Loss: 0.00009433
Iteration 276/1000 | Loss: 0.00009306
Iteration 277/1000 | Loss: 0.00009236
Iteration 278/1000 | Loss: 0.00050294
Iteration 279/1000 | Loss: 0.00009552
Iteration 280/1000 | Loss: 0.00024445
Iteration 281/1000 | Loss: 0.00010396
Iteration 282/1000 | Loss: 0.00009726
Iteration 283/1000 | Loss: 0.00009467
Iteration 284/1000 | Loss: 0.00009298
Iteration 285/1000 | Loss: 0.00009214
Iteration 286/1000 | Loss: 0.00027674
Iteration 287/1000 | Loss: 0.00012654
Iteration 288/1000 | Loss: 0.00018917
Iteration 289/1000 | Loss: 0.00047868
Iteration 290/1000 | Loss: 0.00021716
Iteration 291/1000 | Loss: 0.00009070
Iteration 292/1000 | Loss: 0.00039622
Iteration 293/1000 | Loss: 0.00049961
Iteration 294/1000 | Loss: 0.00023331
Iteration 295/1000 | Loss: 0.00009576
Iteration 296/1000 | Loss: 0.00009151
Iteration 297/1000 | Loss: 0.00008928
Iteration 298/1000 | Loss: 0.00008797
Iteration 299/1000 | Loss: 0.00008652
Iteration 300/1000 | Loss: 0.00008560
Iteration 301/1000 | Loss: 0.00008496
Iteration 302/1000 | Loss: 0.00045883
Iteration 303/1000 | Loss: 0.00016974
Iteration 304/1000 | Loss: 0.00051377
Iteration 305/1000 | Loss: 0.00009857
Iteration 306/1000 | Loss: 0.00061390
Iteration 307/1000 | Loss: 0.00030076
Iteration 308/1000 | Loss: 0.00018790
Iteration 309/1000 | Loss: 0.00029664
Iteration 310/1000 | Loss: 0.00036477
Iteration 311/1000 | Loss: 0.00027318
Iteration 312/1000 | Loss: 0.00023600
Iteration 313/1000 | Loss: 0.00033850
Iteration 314/1000 | Loss: 0.00036140
Iteration 315/1000 | Loss: 0.00011513
Iteration 316/1000 | Loss: 0.00008978
Iteration 317/1000 | Loss: 0.00020429
Iteration 318/1000 | Loss: 0.00008985
Iteration 319/1000 | Loss: 0.00008742
Iteration 320/1000 | Loss: 0.00016987
Iteration 321/1000 | Loss: 0.00015042
Iteration 322/1000 | Loss: 0.00016065
Iteration 323/1000 | Loss: 0.00012103
Iteration 324/1000 | Loss: 0.00009391
Iteration 325/1000 | Loss: 0.00008568
Iteration 326/1000 | Loss: 0.00008520
Iteration 327/1000 | Loss: 0.00026351
Iteration 328/1000 | Loss: 0.00056200
Iteration 329/1000 | Loss: 0.00029468
Iteration 330/1000 | Loss: 0.00060408
Iteration 331/1000 | Loss: 0.00025382
Iteration 332/1000 | Loss: 0.00013866
Iteration 333/1000 | Loss: 0.00021019
Iteration 334/1000 | Loss: 0.00031358
Iteration 335/1000 | Loss: 0.00041321
Iteration 336/1000 | Loss: 0.00032367
Iteration 337/1000 | Loss: 0.00025094
Iteration 338/1000 | Loss: 0.00025664
Iteration 339/1000 | Loss: 0.00028620
Iteration 340/1000 | Loss: 0.00018267
Iteration 341/1000 | Loss: 0.00014022
Iteration 342/1000 | Loss: 0.00042659
Iteration 343/1000 | Loss: 0.00022964
Iteration 344/1000 | Loss: 0.00018625
Iteration 345/1000 | Loss: 0.00023749
Iteration 346/1000 | Loss: 0.00035574
Iteration 347/1000 | Loss: 0.00026873
Iteration 348/1000 | Loss: 0.00010017
Iteration 349/1000 | Loss: 0.00016078
Iteration 350/1000 | Loss: 0.00014810
Iteration 351/1000 | Loss: 0.00013519
Iteration 352/1000 | Loss: 0.00018128
Iteration 353/1000 | Loss: 0.00021186
Iteration 354/1000 | Loss: 0.00018269
Iteration 355/1000 | Loss: 0.00017470
Iteration 356/1000 | Loss: 0.00019496
Iteration 357/1000 | Loss: 0.00012595
Iteration 358/1000 | Loss: 0.00022316
Iteration 359/1000 | Loss: 0.00018768
Iteration 360/1000 | Loss: 0.00015508
Iteration 361/1000 | Loss: 0.00018138
Iteration 362/1000 | Loss: 0.00018897
Iteration 363/1000 | Loss: 0.00017517
Iteration 364/1000 | Loss: 0.00021949
Iteration 365/1000 | Loss: 0.00015921
Iteration 366/1000 | Loss: 0.00010071
Iteration 367/1000 | Loss: 0.00012907
Iteration 368/1000 | Loss: 0.00011902
Iteration 369/1000 | Loss: 0.00014372
Iteration 370/1000 | Loss: 0.00017486
Iteration 371/1000 | Loss: 0.00017752
Iteration 372/1000 | Loss: 0.00023306
Iteration 373/1000 | Loss: 0.00010349
Iteration 374/1000 | Loss: 0.00009190
Iteration 375/1000 | Loss: 0.00008964
Iteration 376/1000 | Loss: 0.00014016
Iteration 377/1000 | Loss: 0.00011513
Iteration 378/1000 | Loss: 0.00009063
Iteration 379/1000 | Loss: 0.00008919
Iteration 380/1000 | Loss: 0.00008812
Iteration 381/1000 | Loss: 0.00008706
Iteration 382/1000 | Loss: 0.00009171
Iteration 383/1000 | Loss: 0.00009900
Iteration 384/1000 | Loss: 0.00008917
Iteration 385/1000 | Loss: 0.00009565
Iteration 386/1000 | Loss: 0.00008980
Iteration 387/1000 | Loss: 0.00010093
Iteration 388/1000 | Loss: 0.00008763
Iteration 389/1000 | Loss: 0.00010594
Iteration 390/1000 | Loss: 0.00010305
Iteration 391/1000 | Loss: 0.00010491
Iteration 392/1000 | Loss: 0.00010661
Iteration 393/1000 | Loss: 0.00010487
Iteration 394/1000 | Loss: 0.00010582
Iteration 395/1000 | Loss: 0.00009129
Iteration 396/1000 | Loss: 0.00008821
Iteration 397/1000 | Loss: 0.00009813
Iteration 398/1000 | Loss: 0.00009813
Iteration 399/1000 | Loss: 0.00010159
Iteration 400/1000 | Loss: 0.00009934
Iteration 401/1000 | Loss: 0.00010032
Iteration 402/1000 | Loss: 0.00010132
Iteration 403/1000 | Loss: 0.00010116
Iteration 404/1000 | Loss: 0.00010116
Iteration 405/1000 | Loss: 0.00010228
Iteration 406/1000 | Loss: 0.00009217
Iteration 407/1000 | Loss: 0.00010250
Iteration 408/1000 | Loss: 0.00009196
Iteration 409/1000 | Loss: 0.00010052
Iteration 410/1000 | Loss: 0.00009991
Iteration 411/1000 | Loss: 0.00010023
Iteration 412/1000 | Loss: 0.00009856
Iteration 413/1000 | Loss: 0.00008860
Iteration 414/1000 | Loss: 0.00009564
Iteration 415/1000 | Loss: 0.00009113
Iteration 416/1000 | Loss: 0.00010459
Iteration 417/1000 | Loss: 0.00009824
Iteration 418/1000 | Loss: 0.00010125
Iteration 419/1000 | Loss: 0.00010824
Iteration 420/1000 | Loss: 0.00009014
Iteration 421/1000 | Loss: 0.00009800
Iteration 422/1000 | Loss: 0.00047248
Iteration 423/1000 | Loss: 0.00018107
Iteration 424/1000 | Loss: 0.00011498
Iteration 425/1000 | Loss: 0.00009912
Iteration 426/1000 | Loss: 0.00009160
Iteration 427/1000 | Loss: 0.00008841
Iteration 428/1000 | Loss: 0.00008651
Iteration 429/1000 | Loss: 0.00008502
Iteration 430/1000 | Loss: 0.00009428
Iteration 431/1000 | Loss: 0.00008490
Iteration 432/1000 | Loss: 0.00008809
Iteration 433/1000 | Loss: 0.00008320
Iteration 434/1000 | Loss: 0.00008735
Iteration 435/1000 | Loss: 0.00008331
Iteration 436/1000 | Loss: 0.00008357
Iteration 437/1000 | Loss: 0.00008431
Iteration 438/1000 | Loss: 0.00008266
Iteration 439/1000 | Loss: 0.00008251
Iteration 440/1000 | Loss: 0.00008225
Iteration 441/1000 | Loss: 0.00008201
Iteration 442/1000 | Loss: 0.00008196
Iteration 443/1000 | Loss: 0.00008176
Iteration 444/1000 | Loss: 0.00009325
Iteration 445/1000 | Loss: 0.00008511
Iteration 446/1000 | Loss: 0.00008304
Iteration 447/1000 | Loss: 0.00008260
Iteration 448/1000 | Loss: 0.00008223
Iteration 449/1000 | Loss: 0.00008198
Iteration 450/1000 | Loss: 0.00008175
Iteration 451/1000 | Loss: 0.00008159
Iteration 452/1000 | Loss: 0.00008142
Iteration 453/1000 | Loss: 0.00008124
Iteration 454/1000 | Loss: 0.00008122
Iteration 455/1000 | Loss: 0.00008103
Iteration 456/1000 | Loss: 0.00008098
Iteration 457/1000 | Loss: 0.00008097
Iteration 458/1000 | Loss: 0.00008089
Iteration 459/1000 | Loss: 0.00008088
Iteration 460/1000 | Loss: 0.00008085
Iteration 461/1000 | Loss: 0.00008084
Iteration 462/1000 | Loss: 0.00008084
Iteration 463/1000 | Loss: 0.00008084
Iteration 464/1000 | Loss: 0.00008084
Iteration 465/1000 | Loss: 0.00008084
Iteration 466/1000 | Loss: 0.00008084
Iteration 467/1000 | Loss: 0.00008084
Iteration 468/1000 | Loss: 0.00008084
Iteration 469/1000 | Loss: 0.00008084
Iteration 470/1000 | Loss: 0.00008084
Iteration 471/1000 | Loss: 0.00008084
Iteration 472/1000 | Loss: 0.00008083
Iteration 473/1000 | Loss: 0.00008083
Iteration 474/1000 | Loss: 0.00008083
Iteration 475/1000 | Loss: 0.00008083
Iteration 476/1000 | Loss: 0.00008083
Iteration 477/1000 | Loss: 0.00008083
Iteration 478/1000 | Loss: 0.00008083
Iteration 479/1000 | Loss: 0.00008083
Iteration 480/1000 | Loss: 0.00008083
Iteration 481/1000 | Loss: 0.00008083
Iteration 482/1000 | Loss: 0.00008083
Iteration 483/1000 | Loss: 0.00008083
Iteration 484/1000 | Loss: 0.00008083
Iteration 485/1000 | Loss: 0.00008083
Iteration 486/1000 | Loss: 0.00008083
Iteration 487/1000 | Loss: 0.00008083
Iteration 488/1000 | Loss: 0.00008083
Iteration 489/1000 | Loss: 0.00008083
Iteration 490/1000 | Loss: 0.00008083
Iteration 491/1000 | Loss: 0.00008083
Iteration 492/1000 | Loss: 0.00008083
Iteration 493/1000 | Loss: 0.00008083
Iteration 494/1000 | Loss: 0.00008083
Iteration 495/1000 | Loss: 0.00008083
Iteration 496/1000 | Loss: 0.00008083
Iteration 497/1000 | Loss: 0.00008083
Iteration 498/1000 | Loss: 0.00008083
Iteration 499/1000 | Loss: 0.00008083
Iteration 500/1000 | Loss: 0.00008083
Iteration 501/1000 | Loss: 0.00008083
Iteration 502/1000 | Loss: 0.00008083
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 502. Stopping optimization.
Last 5 losses: [8.0825288023334e-05, 8.0825288023334e-05, 8.0825288023334e-05, 8.0825288023334e-05, 8.0825288023334e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.0825288023334e-05

Optimization complete. Final v2v error: 5.27500581741333 mm

Highest mean error: 12.05711841583252 mm for frame 185

Lowest mean error: 3.151848793029785 mm for frame 103

Saving results

Total time: 796.9845335483551
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_025/1096/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_025/1096.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_025/1096
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01000245
Iteration 2/25 | Loss: 0.01000245
Iteration 3/25 | Loss: 0.01000245
Iteration 4/25 | Loss: 0.00356461
Iteration 5/25 | Loss: 0.00201604
Iteration 6/25 | Loss: 0.00187480
Iteration 7/25 | Loss: 0.00185688
Iteration 8/25 | Loss: 0.00174182
Iteration 9/25 | Loss: 0.00167875
Iteration 10/25 | Loss: 0.00180414
Iteration 11/25 | Loss: 0.00165157
Iteration 12/25 | Loss: 0.00160213
Iteration 13/25 | Loss: 0.00157514
Iteration 14/25 | Loss: 0.00154178
Iteration 15/25 | Loss: 0.00151943
Iteration 16/25 | Loss: 0.00150485
Iteration 17/25 | Loss: 0.00149288
Iteration 18/25 | Loss: 0.00148924
Iteration 19/25 | Loss: 0.00147537
Iteration 20/25 | Loss: 0.00147989
Iteration 21/25 | Loss: 0.00147406
Iteration 22/25 | Loss: 0.00146874
Iteration 23/25 | Loss: 0.00148045
Iteration 24/25 | Loss: 0.00146791
Iteration 25/25 | Loss: 0.00147085

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35267389
Iteration 2/25 | Loss: 0.00492023
Iteration 3/25 | Loss: 0.00324239
Iteration 4/25 | Loss: 0.00324239
Iteration 5/25 | Loss: 0.00324239
Iteration 6/25 | Loss: 0.00324239
Iteration 7/25 | Loss: 0.00324239
Iteration 8/25 | Loss: 0.00324238
Iteration 9/25 | Loss: 0.00324238
Iteration 10/25 | Loss: 0.00324238
Iteration 11/25 | Loss: 0.00324238
Iteration 12/25 | Loss: 0.00324238
Iteration 13/25 | Loss: 0.00324238
Iteration 14/25 | Loss: 0.00324238
Iteration 15/25 | Loss: 0.00324238
Iteration 16/25 | Loss: 0.00324238
Iteration 17/25 | Loss: 0.00324238
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.00324238371104002, 0.00324238371104002, 0.00324238371104002, 0.00324238371104002, 0.00324238371104002]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00324238371104002

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00324238
Iteration 2/1000 | Loss: 0.00250374
Iteration 3/1000 | Loss: 0.00183360
Iteration 4/1000 | Loss: 0.00056919
Iteration 5/1000 | Loss: 0.00038340
Iteration 6/1000 | Loss: 0.00078125
Iteration 7/1000 | Loss: 0.00386843
Iteration 8/1000 | Loss: 0.00073554
Iteration 9/1000 | Loss: 0.00034275
Iteration 10/1000 | Loss: 0.00173292
Iteration 11/1000 | Loss: 0.00124812
Iteration 12/1000 | Loss: 0.00371463
Iteration 13/1000 | Loss: 0.00036736
Iteration 14/1000 | Loss: 0.00066084
Iteration 15/1000 | Loss: 0.00028673
Iteration 16/1000 | Loss: 0.00079676
Iteration 17/1000 | Loss: 0.00031143
Iteration 18/1000 | Loss: 0.00065824
Iteration 19/1000 | Loss: 0.00102848
Iteration 20/1000 | Loss: 0.00085846
Iteration 21/1000 | Loss: 0.00112686
Iteration 22/1000 | Loss: 0.00049951
Iteration 23/1000 | Loss: 0.00070834
Iteration 24/1000 | Loss: 0.00024349
Iteration 25/1000 | Loss: 0.00021122
Iteration 26/1000 | Loss: 0.00083123
Iteration 27/1000 | Loss: 0.00234478
Iteration 28/1000 | Loss: 0.00075060
Iteration 29/1000 | Loss: 0.00014195
Iteration 30/1000 | Loss: 0.00047192
Iteration 31/1000 | Loss: 0.00017615
Iteration 32/1000 | Loss: 0.00014774
Iteration 33/1000 | Loss: 0.00021977
Iteration 34/1000 | Loss: 0.00069124
Iteration 35/1000 | Loss: 0.00100389
Iteration 36/1000 | Loss: 0.00028449
Iteration 37/1000 | Loss: 0.00041939
Iteration 38/1000 | Loss: 0.00013794
Iteration 39/1000 | Loss: 0.00012300
Iteration 40/1000 | Loss: 0.00012073
Iteration 41/1000 | Loss: 0.00012410
Iteration 42/1000 | Loss: 0.00014559
Iteration 43/1000 | Loss: 0.00012467
Iteration 44/1000 | Loss: 0.00051230
Iteration 45/1000 | Loss: 0.00039888
Iteration 46/1000 | Loss: 0.00014543
Iteration 47/1000 | Loss: 0.00081410
Iteration 48/1000 | Loss: 0.00057482
Iteration 49/1000 | Loss: 0.00225274
Iteration 50/1000 | Loss: 0.00244199
Iteration 51/1000 | Loss: 0.00308230
Iteration 52/1000 | Loss: 0.00087201
Iteration 53/1000 | Loss: 0.00301785
Iteration 54/1000 | Loss: 0.00298613
Iteration 55/1000 | Loss: 0.00101193
Iteration 56/1000 | Loss: 0.00054266
Iteration 57/1000 | Loss: 0.00245242
Iteration 58/1000 | Loss: 0.00017906
Iteration 59/1000 | Loss: 0.00011672
Iteration 60/1000 | Loss: 0.00021560
Iteration 61/1000 | Loss: 0.00129017
Iteration 62/1000 | Loss: 0.00032529
Iteration 63/1000 | Loss: 0.00009136
Iteration 64/1000 | Loss: 0.00012383
Iteration 65/1000 | Loss: 0.00019381
Iteration 66/1000 | Loss: 0.00156485
Iteration 67/1000 | Loss: 0.00037259
Iteration 68/1000 | Loss: 0.00025204
Iteration 69/1000 | Loss: 0.00014548
Iteration 70/1000 | Loss: 0.00006307
Iteration 71/1000 | Loss: 0.00007446
Iteration 72/1000 | Loss: 0.00013352
Iteration 73/1000 | Loss: 0.00006254
Iteration 74/1000 | Loss: 0.00012088
Iteration 75/1000 | Loss: 0.00013634
Iteration 76/1000 | Loss: 0.00007824
Iteration 77/1000 | Loss: 0.00028585
Iteration 78/1000 | Loss: 0.00020211
Iteration 79/1000 | Loss: 0.00006952
Iteration 80/1000 | Loss: 0.00007717
Iteration 81/1000 | Loss: 0.00005811
Iteration 82/1000 | Loss: 0.00008344
Iteration 83/1000 | Loss: 0.00005758
Iteration 84/1000 | Loss: 0.00005917
Iteration 85/1000 | Loss: 0.00004941
Iteration 86/1000 | Loss: 0.00004953
Iteration 87/1000 | Loss: 0.00008647
Iteration 88/1000 | Loss: 0.00009045
Iteration 89/1000 | Loss: 0.00004745
Iteration 90/1000 | Loss: 0.00007250
Iteration 91/1000 | Loss: 0.00031988
Iteration 92/1000 | Loss: 0.00005858
Iteration 93/1000 | Loss: 0.00005135
Iteration 94/1000 | Loss: 0.00004760
Iteration 95/1000 | Loss: 0.00009482
Iteration 96/1000 | Loss: 0.00007676
Iteration 97/1000 | Loss: 0.00065141
Iteration 98/1000 | Loss: 0.00181816
Iteration 99/1000 | Loss: 0.00068363
Iteration 100/1000 | Loss: 0.00007489
Iteration 101/1000 | Loss: 0.00009474
Iteration 102/1000 | Loss: 0.00008276
Iteration 103/1000 | Loss: 0.00008125
Iteration 104/1000 | Loss: 0.00005061
Iteration 105/1000 | Loss: 0.00004147
Iteration 106/1000 | Loss: 0.00013561
Iteration 107/1000 | Loss: 0.00004086
Iteration 108/1000 | Loss: 0.00021896
Iteration 109/1000 | Loss: 0.00005477
Iteration 110/1000 | Loss: 0.00003754
Iteration 111/1000 | Loss: 0.00003640
Iteration 112/1000 | Loss: 0.00003704
Iteration 113/1000 | Loss: 0.00005153
Iteration 114/1000 | Loss: 0.00003537
Iteration 115/1000 | Loss: 0.00004771
Iteration 116/1000 | Loss: 0.00003484
Iteration 117/1000 | Loss: 0.00011233
Iteration 118/1000 | Loss: 0.00040350
Iteration 119/1000 | Loss: 0.00025087
Iteration 120/1000 | Loss: 0.00019739
Iteration 121/1000 | Loss: 0.00050754
Iteration 122/1000 | Loss: 0.00021651
Iteration 123/1000 | Loss: 0.00026953
Iteration 124/1000 | Loss: 0.00007192
Iteration 125/1000 | Loss: 0.00021237
Iteration 126/1000 | Loss: 0.00007038
Iteration 127/1000 | Loss: 0.00003539
Iteration 128/1000 | Loss: 0.00003464
Iteration 129/1000 | Loss: 0.00006077
Iteration 130/1000 | Loss: 0.00003274
Iteration 131/1000 | Loss: 0.00006696
Iteration 132/1000 | Loss: 0.00003867
Iteration 133/1000 | Loss: 0.00009761
Iteration 134/1000 | Loss: 0.00013691
Iteration 135/1000 | Loss: 0.00003584
Iteration 136/1000 | Loss: 0.00007305
Iteration 137/1000 | Loss: 0.00004128
Iteration 138/1000 | Loss: 0.00003948
Iteration 139/1000 | Loss: 0.00003219
Iteration 140/1000 | Loss: 0.00006332
Iteration 141/1000 | Loss: 0.00003165
Iteration 142/1000 | Loss: 0.00009343
Iteration 143/1000 | Loss: 0.00035000
Iteration 144/1000 | Loss: 0.00005995
Iteration 145/1000 | Loss: 0.00010748
Iteration 146/1000 | Loss: 0.00004127
Iteration 147/1000 | Loss: 0.00005706
Iteration 148/1000 | Loss: 0.00003159
Iteration 149/1000 | Loss: 0.00006427
Iteration 150/1000 | Loss: 0.00003556
Iteration 151/1000 | Loss: 0.00003251
Iteration 152/1000 | Loss: 0.00005781
Iteration 153/1000 | Loss: 0.00002993
Iteration 154/1000 | Loss: 0.00005243
Iteration 155/1000 | Loss: 0.00011452
Iteration 156/1000 | Loss: 0.00003199
Iteration 157/1000 | Loss: 0.00003071
Iteration 158/1000 | Loss: 0.00002954
Iteration 159/1000 | Loss: 0.00002927
Iteration 160/1000 | Loss: 0.00002927
Iteration 161/1000 | Loss: 0.00002915
Iteration 162/1000 | Loss: 0.00009860
Iteration 163/1000 | Loss: 0.00009860
Iteration 164/1000 | Loss: 0.00005190
Iteration 165/1000 | Loss: 0.00008971
Iteration 166/1000 | Loss: 0.00002985
Iteration 167/1000 | Loss: 0.00003510
Iteration 168/1000 | Loss: 0.00003037
Iteration 169/1000 | Loss: 0.00003896
Iteration 170/1000 | Loss: 0.00002891
Iteration 171/1000 | Loss: 0.00003258
Iteration 172/1000 | Loss: 0.00002886
Iteration 173/1000 | Loss: 0.00002885
Iteration 174/1000 | Loss: 0.00002885
Iteration 175/1000 | Loss: 0.00002885
Iteration 176/1000 | Loss: 0.00002885
Iteration 177/1000 | Loss: 0.00002885
Iteration 178/1000 | Loss: 0.00002980
Iteration 179/1000 | Loss: 0.00005753
Iteration 180/1000 | Loss: 0.00003964
Iteration 181/1000 | Loss: 0.00015400
Iteration 182/1000 | Loss: 0.00015088
Iteration 183/1000 | Loss: 0.00005618
Iteration 184/1000 | Loss: 0.00004811
Iteration 185/1000 | Loss: 0.00003090
Iteration 186/1000 | Loss: 0.00007510
Iteration 187/1000 | Loss: 0.00006414
Iteration 188/1000 | Loss: 0.00011654
Iteration 189/1000 | Loss: 0.00005380
Iteration 190/1000 | Loss: 0.00002868
Iteration 191/1000 | Loss: 0.00006450
Iteration 192/1000 | Loss: 0.00003172
Iteration 193/1000 | Loss: 0.00002787
Iteration 194/1000 | Loss: 0.00002777
Iteration 195/1000 | Loss: 0.00002772
Iteration 196/1000 | Loss: 0.00002765
Iteration 197/1000 | Loss: 0.00006492
Iteration 198/1000 | Loss: 0.00006383
Iteration 199/1000 | Loss: 0.00007769
Iteration 200/1000 | Loss: 0.00002759
Iteration 201/1000 | Loss: 0.00002749
Iteration 202/1000 | Loss: 0.00002746
Iteration 203/1000 | Loss: 0.00002745
Iteration 204/1000 | Loss: 0.00002745
Iteration 205/1000 | Loss: 0.00002745
Iteration 206/1000 | Loss: 0.00002744
Iteration 207/1000 | Loss: 0.00004540
Iteration 208/1000 | Loss: 0.00003376
Iteration 209/1000 | Loss: 0.00002741
Iteration 210/1000 | Loss: 0.00002741
Iteration 211/1000 | Loss: 0.00002741
Iteration 212/1000 | Loss: 0.00002741
Iteration 213/1000 | Loss: 0.00002741
Iteration 214/1000 | Loss: 0.00002741
Iteration 215/1000 | Loss: 0.00002741
Iteration 216/1000 | Loss: 0.00002741
Iteration 217/1000 | Loss: 0.00002740
Iteration 218/1000 | Loss: 0.00002740
Iteration 219/1000 | Loss: 0.00002740
Iteration 220/1000 | Loss: 0.00002739
Iteration 221/1000 | Loss: 0.00002739
Iteration 222/1000 | Loss: 0.00002738
Iteration 223/1000 | Loss: 0.00002738
Iteration 224/1000 | Loss: 0.00002738
Iteration 225/1000 | Loss: 0.00002738
Iteration 226/1000 | Loss: 0.00002737
Iteration 227/1000 | Loss: 0.00002737
Iteration 228/1000 | Loss: 0.00002736
Iteration 229/1000 | Loss: 0.00002736
Iteration 230/1000 | Loss: 0.00002736
Iteration 231/1000 | Loss: 0.00002735
Iteration 232/1000 | Loss: 0.00002735
Iteration 233/1000 | Loss: 0.00004183
Iteration 234/1000 | Loss: 0.00006460
Iteration 235/1000 | Loss: 0.00006947
Iteration 236/1000 | Loss: 0.00002997
Iteration 237/1000 | Loss: 0.00003380
Iteration 238/1000 | Loss: 0.00004042
Iteration 239/1000 | Loss: 0.00003472
Iteration 240/1000 | Loss: 0.00002980
Iteration 241/1000 | Loss: 0.00002731
Iteration 242/1000 | Loss: 0.00002731
Iteration 243/1000 | Loss: 0.00002731
Iteration 244/1000 | Loss: 0.00002731
Iteration 245/1000 | Loss: 0.00002731
Iteration 246/1000 | Loss: 0.00002731
Iteration 247/1000 | Loss: 0.00002731
Iteration 248/1000 | Loss: 0.00002731
Iteration 249/1000 | Loss: 0.00002731
Iteration 250/1000 | Loss: 0.00002731
Iteration 251/1000 | Loss: 0.00002731
Iteration 252/1000 | Loss: 0.00002731
Iteration 253/1000 | Loss: 0.00002730
Iteration 254/1000 | Loss: 0.00002730
Iteration 255/1000 | Loss: 0.00002730
Iteration 256/1000 | Loss: 0.00002730
Iteration 257/1000 | Loss: 0.00002730
Iteration 258/1000 | Loss: 0.00002730
Iteration 259/1000 | Loss: 0.00002730
Iteration 260/1000 | Loss: 0.00002931
Iteration 261/1000 | Loss: 0.00002744
Iteration 262/1000 | Loss: 0.00002730
Iteration 263/1000 | Loss: 0.00002730
Iteration 264/1000 | Loss: 0.00002729
Iteration 265/1000 | Loss: 0.00002729
Iteration 266/1000 | Loss: 0.00002729
Iteration 267/1000 | Loss: 0.00002729
Iteration 268/1000 | Loss: 0.00002729
Iteration 269/1000 | Loss: 0.00002729
Iteration 270/1000 | Loss: 0.00002753
Iteration 271/1000 | Loss: 0.00003360
Iteration 272/1000 | Loss: 0.00002833
Iteration 273/1000 | Loss: 0.00004186
Iteration 274/1000 | Loss: 0.00005327
Iteration 275/1000 | Loss: 0.00002732
Iteration 276/1000 | Loss: 0.00002731
Iteration 277/1000 | Loss: 0.00002731
Iteration 278/1000 | Loss: 0.00002731
Iteration 279/1000 | Loss: 0.00002731
Iteration 280/1000 | Loss: 0.00002731
Iteration 281/1000 | Loss: 0.00002731
Iteration 282/1000 | Loss: 0.00002731
Iteration 283/1000 | Loss: 0.00002730
Iteration 284/1000 | Loss: 0.00002730
Iteration 285/1000 | Loss: 0.00002730
Iteration 286/1000 | Loss: 0.00002730
Iteration 287/1000 | Loss: 0.00002730
Iteration 288/1000 | Loss: 0.00002976
Iteration 289/1000 | Loss: 0.00002729
Iteration 290/1000 | Loss: 0.00002729
Iteration 291/1000 | Loss: 0.00002729
Iteration 292/1000 | Loss: 0.00002729
Iteration 293/1000 | Loss: 0.00002729
Iteration 294/1000 | Loss: 0.00002729
Iteration 295/1000 | Loss: 0.00002729
Iteration 296/1000 | Loss: 0.00002729
Iteration 297/1000 | Loss: 0.00002729
Iteration 298/1000 | Loss: 0.00002729
Iteration 299/1000 | Loss: 0.00002729
Iteration 300/1000 | Loss: 0.00002728
Iteration 301/1000 | Loss: 0.00002728
Iteration 302/1000 | Loss: 0.00002728
Iteration 303/1000 | Loss: 0.00002728
Iteration 304/1000 | Loss: 0.00002728
Iteration 305/1000 | Loss: 0.00002728
Iteration 306/1000 | Loss: 0.00002728
Iteration 307/1000 | Loss: 0.00002728
Iteration 308/1000 | Loss: 0.00002727
Iteration 309/1000 | Loss: 0.00002727
Iteration 310/1000 | Loss: 0.00002727
Iteration 311/1000 | Loss: 0.00002727
Iteration 312/1000 | Loss: 0.00002727
Iteration 313/1000 | Loss: 0.00002727
Iteration 314/1000 | Loss: 0.00002727
Iteration 315/1000 | Loss: 0.00002727
Iteration 316/1000 | Loss: 0.00002727
Iteration 317/1000 | Loss: 0.00002727
Iteration 318/1000 | Loss: 0.00002727
Iteration 319/1000 | Loss: 0.00002727
Iteration 320/1000 | Loss: 0.00002727
Iteration 321/1000 | Loss: 0.00002727
Iteration 322/1000 | Loss: 0.00002727
Iteration 323/1000 | Loss: 0.00002727
Iteration 324/1000 | Loss: 0.00002727
Iteration 325/1000 | Loss: 0.00002727
Iteration 326/1000 | Loss: 0.00002727
Iteration 327/1000 | Loss: 0.00002727
Iteration 328/1000 | Loss: 0.00002727
Iteration 329/1000 | Loss: 0.00002727
Iteration 330/1000 | Loss: 0.00002727
Iteration 331/1000 | Loss: 0.00002727
Iteration 332/1000 | Loss: 0.00002727
Iteration 333/1000 | Loss: 0.00002727
Iteration 334/1000 | Loss: 0.00002727
Iteration 335/1000 | Loss: 0.00002727
Iteration 336/1000 | Loss: 0.00002727
Iteration 337/1000 | Loss: 0.00002727
Iteration 338/1000 | Loss: 0.00002727
Iteration 339/1000 | Loss: 0.00002727
Iteration 340/1000 | Loss: 0.00002727
Iteration 341/1000 | Loss: 0.00002727
Iteration 342/1000 | Loss: 0.00002727
Iteration 343/1000 | Loss: 0.00002727
Iteration 344/1000 | Loss: 0.00002727
Iteration 345/1000 | Loss: 0.00002727
Iteration 346/1000 | Loss: 0.00002727
Iteration 347/1000 | Loss: 0.00002727
Iteration 348/1000 | Loss: 0.00002727
Iteration 349/1000 | Loss: 0.00002727
Iteration 350/1000 | Loss: 0.00002727
Iteration 351/1000 | Loss: 0.00002727
Iteration 352/1000 | Loss: 0.00002727
Iteration 353/1000 | Loss: 0.00002727
Iteration 354/1000 | Loss: 0.00002727
Iteration 355/1000 | Loss: 0.00002727
Iteration 356/1000 | Loss: 0.00002727
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 356. Stopping optimization.
Last 5 losses: [2.7269754355074838e-05, 2.7269754355074838e-05, 2.7269754355074838e-05, 2.7269754355074838e-05, 2.7269754355074838e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.7269754355074838e-05

Optimization complete. Final v2v error: 3.5004329681396484 mm

Highest mean error: 11.071459770202637 mm for frame 144

Lowest mean error: 2.7062907218933105 mm for frame 236

Saving results

Total time: 390.2766697406769
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_025/1076/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_025/1076.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_025/1076
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00608714
Iteration 2/25 | Loss: 0.00150760
Iteration 3/25 | Loss: 0.00138567
Iteration 4/25 | Loss: 0.00136457
Iteration 5/25 | Loss: 0.00136190
Iteration 6/25 | Loss: 0.00136190
Iteration 7/25 | Loss: 0.00136190
Iteration 8/25 | Loss: 0.00136190
Iteration 9/25 | Loss: 0.00136190
Iteration 10/25 | Loss: 0.00136190
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0013618954690173268, 0.0013618954690173268, 0.0013618954690173268, 0.0013618954690173268, 0.0013618954690173268]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013618954690173268

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.63535678
Iteration 2/25 | Loss: 0.00069930
Iteration 3/25 | Loss: 0.00069930
Iteration 4/25 | Loss: 0.00069930
Iteration 5/25 | Loss: 0.00069930
Iteration 6/25 | Loss: 0.00069930
Iteration 7/25 | Loss: 0.00069930
Iteration 8/25 | Loss: 0.00069930
Iteration 9/25 | Loss: 0.00069930
Iteration 10/25 | Loss: 0.00069930
Iteration 11/25 | Loss: 0.00069930
Iteration 12/25 | Loss: 0.00069930
Iteration 13/25 | Loss: 0.00069930
Iteration 14/25 | Loss: 0.00069930
Iteration 15/25 | Loss: 0.00069930
Iteration 16/25 | Loss: 0.00069930
Iteration 17/25 | Loss: 0.00069930
Iteration 18/25 | Loss: 0.00069930
Iteration 19/25 | Loss: 0.00069930
Iteration 20/25 | Loss: 0.00069930
Iteration 21/25 | Loss: 0.00069930
Iteration 22/25 | Loss: 0.00069930
Iteration 23/25 | Loss: 0.00069930
Iteration 24/25 | Loss: 0.00069930
Iteration 25/25 | Loss: 0.00069930

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00069930
Iteration 2/1000 | Loss: 0.00004796
Iteration 3/1000 | Loss: 0.00003567
Iteration 4/1000 | Loss: 0.00003313
Iteration 5/1000 | Loss: 0.00003215
Iteration 6/1000 | Loss: 0.00003147
Iteration 7/1000 | Loss: 0.00003104
Iteration 8/1000 | Loss: 0.00003078
Iteration 9/1000 | Loss: 0.00003052
Iteration 10/1000 | Loss: 0.00003035
Iteration 11/1000 | Loss: 0.00003014
Iteration 12/1000 | Loss: 0.00003009
Iteration 13/1000 | Loss: 0.00002999
Iteration 14/1000 | Loss: 0.00002993
Iteration 15/1000 | Loss: 0.00002993
Iteration 16/1000 | Loss: 0.00002992
Iteration 17/1000 | Loss: 0.00002992
Iteration 18/1000 | Loss: 0.00002992
Iteration 19/1000 | Loss: 0.00002992
Iteration 20/1000 | Loss: 0.00002992
Iteration 21/1000 | Loss: 0.00002992
Iteration 22/1000 | Loss: 0.00002992
Iteration 23/1000 | Loss: 0.00002992
Iteration 24/1000 | Loss: 0.00002992
Iteration 25/1000 | Loss: 0.00002990
Iteration 26/1000 | Loss: 0.00002985
Iteration 27/1000 | Loss: 0.00002985
Iteration 28/1000 | Loss: 0.00002984
Iteration 29/1000 | Loss: 0.00002982
Iteration 30/1000 | Loss: 0.00002981
Iteration 31/1000 | Loss: 0.00002981
Iteration 32/1000 | Loss: 0.00002980
Iteration 33/1000 | Loss: 0.00002980
Iteration 34/1000 | Loss: 0.00002980
Iteration 35/1000 | Loss: 0.00002980
Iteration 36/1000 | Loss: 0.00002979
Iteration 37/1000 | Loss: 0.00002979
Iteration 38/1000 | Loss: 0.00002978
Iteration 39/1000 | Loss: 0.00002978
Iteration 40/1000 | Loss: 0.00002978
Iteration 41/1000 | Loss: 0.00002978
Iteration 42/1000 | Loss: 0.00002978
Iteration 43/1000 | Loss: 0.00002978
Iteration 44/1000 | Loss: 0.00002978
Iteration 45/1000 | Loss: 0.00002977
Iteration 46/1000 | Loss: 0.00002977
Iteration 47/1000 | Loss: 0.00002977
Iteration 48/1000 | Loss: 0.00002977
Iteration 49/1000 | Loss: 0.00002976
Iteration 50/1000 | Loss: 0.00002976
Iteration 51/1000 | Loss: 0.00002976
Iteration 52/1000 | Loss: 0.00002976
Iteration 53/1000 | Loss: 0.00002975
Iteration 54/1000 | Loss: 0.00002975
Iteration 55/1000 | Loss: 0.00002975
Iteration 56/1000 | Loss: 0.00002975
Iteration 57/1000 | Loss: 0.00002975
Iteration 58/1000 | Loss: 0.00002975
Iteration 59/1000 | Loss: 0.00002975
Iteration 60/1000 | Loss: 0.00002975
Iteration 61/1000 | Loss: 0.00002974
Iteration 62/1000 | Loss: 0.00002974
Iteration 63/1000 | Loss: 0.00002974
Iteration 64/1000 | Loss: 0.00002974
Iteration 65/1000 | Loss: 0.00002974
Iteration 66/1000 | Loss: 0.00002974
Iteration 67/1000 | Loss: 0.00002973
Iteration 68/1000 | Loss: 0.00002973
Iteration 69/1000 | Loss: 0.00002973
Iteration 70/1000 | Loss: 0.00002973
Iteration 71/1000 | Loss: 0.00002972
Iteration 72/1000 | Loss: 0.00002972
Iteration 73/1000 | Loss: 0.00002972
Iteration 74/1000 | Loss: 0.00002972
Iteration 75/1000 | Loss: 0.00002972
Iteration 76/1000 | Loss: 0.00002972
Iteration 77/1000 | Loss: 0.00002971
Iteration 78/1000 | Loss: 0.00002971
Iteration 79/1000 | Loss: 0.00002971
Iteration 80/1000 | Loss: 0.00002971
Iteration 81/1000 | Loss: 0.00002971
Iteration 82/1000 | Loss: 0.00002971
Iteration 83/1000 | Loss: 0.00002970
Iteration 84/1000 | Loss: 0.00002970
Iteration 85/1000 | Loss: 0.00002968
Iteration 86/1000 | Loss: 0.00002968
Iteration 87/1000 | Loss: 0.00002968
Iteration 88/1000 | Loss: 0.00002968
Iteration 89/1000 | Loss: 0.00002968
Iteration 90/1000 | Loss: 0.00002968
Iteration 91/1000 | Loss: 0.00002967
Iteration 92/1000 | Loss: 0.00002967
Iteration 93/1000 | Loss: 0.00002966
Iteration 94/1000 | Loss: 0.00002966
Iteration 95/1000 | Loss: 0.00002966
Iteration 96/1000 | Loss: 0.00002965
Iteration 97/1000 | Loss: 0.00002965
Iteration 98/1000 | Loss: 0.00002965
Iteration 99/1000 | Loss: 0.00002965
Iteration 100/1000 | Loss: 0.00002965
Iteration 101/1000 | Loss: 0.00002965
Iteration 102/1000 | Loss: 0.00002965
Iteration 103/1000 | Loss: 0.00002964
Iteration 104/1000 | Loss: 0.00002964
Iteration 105/1000 | Loss: 0.00002964
Iteration 106/1000 | Loss: 0.00002964
Iteration 107/1000 | Loss: 0.00002964
Iteration 108/1000 | Loss: 0.00002964
Iteration 109/1000 | Loss: 0.00002964
Iteration 110/1000 | Loss: 0.00002964
Iteration 111/1000 | Loss: 0.00002964
Iteration 112/1000 | Loss: 0.00002964
Iteration 113/1000 | Loss: 0.00002964
Iteration 114/1000 | Loss: 0.00002964
Iteration 115/1000 | Loss: 0.00002964
Iteration 116/1000 | Loss: 0.00002964
Iteration 117/1000 | Loss: 0.00002964
Iteration 118/1000 | Loss: 0.00002964
Iteration 119/1000 | Loss: 0.00002964
Iteration 120/1000 | Loss: 0.00002964
Iteration 121/1000 | Loss: 0.00002964
Iteration 122/1000 | Loss: 0.00002964
Iteration 123/1000 | Loss: 0.00002964
Iteration 124/1000 | Loss: 0.00002964
Iteration 125/1000 | Loss: 0.00002964
Iteration 126/1000 | Loss: 0.00002964
Iteration 127/1000 | Loss: 0.00002964
Iteration 128/1000 | Loss: 0.00002964
Iteration 129/1000 | Loss: 0.00002964
Iteration 130/1000 | Loss: 0.00002964
Iteration 131/1000 | Loss: 0.00002964
Iteration 132/1000 | Loss: 0.00002964
Iteration 133/1000 | Loss: 0.00002964
Iteration 134/1000 | Loss: 0.00002964
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 134. Stopping optimization.
Last 5 losses: [2.963751285278704e-05, 2.963751285278704e-05, 2.963751285278704e-05, 2.963751285278704e-05, 2.963751285278704e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.963751285278704e-05

Optimization complete. Final v2v error: 4.541676044464111 mm

Highest mean error: 4.639667510986328 mm for frame 0

Lowest mean error: 4.345945835113525 mm for frame 129

Saving results

Total time: 39.287776708602905
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_025/1075/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_025/1075.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_025/1075
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00495945
Iteration 2/25 | Loss: 0.00143349
Iteration 3/25 | Loss: 0.00126667
Iteration 4/25 | Loss: 0.00124789
Iteration 5/25 | Loss: 0.00124286
Iteration 6/25 | Loss: 0.00124286
Iteration 7/25 | Loss: 0.00124286
Iteration 8/25 | Loss: 0.00124286
Iteration 9/25 | Loss: 0.00124286
Iteration 10/25 | Loss: 0.00124286
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0012428557965904474, 0.0012428557965904474, 0.0012428557965904474, 0.0012428557965904474, 0.0012428557965904474]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012428557965904474

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36279655
Iteration 2/25 | Loss: 0.00084644
Iteration 3/25 | Loss: 0.00084643
Iteration 4/25 | Loss: 0.00084643
Iteration 5/25 | Loss: 0.00084643
Iteration 6/25 | Loss: 0.00084643
Iteration 7/25 | Loss: 0.00084643
Iteration 8/25 | Loss: 0.00084643
Iteration 9/25 | Loss: 0.00084643
Iteration 10/25 | Loss: 0.00084643
Iteration 11/25 | Loss: 0.00084643
Iteration 12/25 | Loss: 0.00084643
Iteration 13/25 | Loss: 0.00084643
Iteration 14/25 | Loss: 0.00084643
Iteration 15/25 | Loss: 0.00084643
Iteration 16/25 | Loss: 0.00084643
Iteration 17/25 | Loss: 0.00084643
Iteration 18/25 | Loss: 0.00084643
Iteration 19/25 | Loss: 0.00084643
Iteration 20/25 | Loss: 0.00084643
Iteration 21/25 | Loss: 0.00084643
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0008464275160804391, 0.0008464275160804391, 0.0008464275160804391, 0.0008464275160804391, 0.0008464275160804391]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008464275160804391

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00084643
Iteration 2/1000 | Loss: 0.00005157
Iteration 3/1000 | Loss: 0.00003332
Iteration 4/1000 | Loss: 0.00003062
Iteration 5/1000 | Loss: 0.00002961
Iteration 6/1000 | Loss: 0.00002851
Iteration 7/1000 | Loss: 0.00002747
Iteration 8/1000 | Loss: 0.00002683
Iteration 9/1000 | Loss: 0.00002644
Iteration 10/1000 | Loss: 0.00002613
Iteration 11/1000 | Loss: 0.00002592
Iteration 12/1000 | Loss: 0.00002575
Iteration 13/1000 | Loss: 0.00002557
Iteration 14/1000 | Loss: 0.00002544
Iteration 15/1000 | Loss: 0.00002527
Iteration 16/1000 | Loss: 0.00002523
Iteration 17/1000 | Loss: 0.00002520
Iteration 18/1000 | Loss: 0.00002519
Iteration 19/1000 | Loss: 0.00002517
Iteration 20/1000 | Loss: 0.00002516
Iteration 21/1000 | Loss: 0.00002510
Iteration 22/1000 | Loss: 0.00002509
Iteration 23/1000 | Loss: 0.00002509
Iteration 24/1000 | Loss: 0.00002508
Iteration 25/1000 | Loss: 0.00002508
Iteration 26/1000 | Loss: 0.00002507
Iteration 27/1000 | Loss: 0.00002506
Iteration 28/1000 | Loss: 0.00002505
Iteration 29/1000 | Loss: 0.00002502
Iteration 30/1000 | Loss: 0.00002499
Iteration 31/1000 | Loss: 0.00002498
Iteration 32/1000 | Loss: 0.00002498
Iteration 33/1000 | Loss: 0.00002497
Iteration 34/1000 | Loss: 0.00002496
Iteration 35/1000 | Loss: 0.00002493
Iteration 36/1000 | Loss: 0.00002493
Iteration 37/1000 | Loss: 0.00002493
Iteration 38/1000 | Loss: 0.00002491
Iteration 39/1000 | Loss: 0.00002490
Iteration 40/1000 | Loss: 0.00002490
Iteration 41/1000 | Loss: 0.00002490
Iteration 42/1000 | Loss: 0.00002490
Iteration 43/1000 | Loss: 0.00002490
Iteration 44/1000 | Loss: 0.00002490
Iteration 45/1000 | Loss: 0.00002490
Iteration 46/1000 | Loss: 0.00002490
Iteration 47/1000 | Loss: 0.00002490
Iteration 48/1000 | Loss: 0.00002490
Iteration 49/1000 | Loss: 0.00002490
Iteration 50/1000 | Loss: 0.00002490
Iteration 51/1000 | Loss: 0.00002490
Iteration 52/1000 | Loss: 0.00002490
Iteration 53/1000 | Loss: 0.00002490
Iteration 54/1000 | Loss: 0.00002490
Iteration 55/1000 | Loss: 0.00002490
Iteration 56/1000 | Loss: 0.00002490
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 56. Stopping optimization.
Last 5 losses: [2.4901817596401088e-05, 2.4901817596401088e-05, 2.4901817596401088e-05, 2.4901817596401088e-05, 2.4901817596401088e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4901817596401088e-05

Optimization complete. Final v2v error: 4.206022262573242 mm

Highest mean error: 4.884304523468018 mm for frame 179

Lowest mean error: 3.77457594871521 mm for frame 152

Saving results

Total time: 39.575560331344604
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_025/1041/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_025/1041.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_025/1041
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01000864
Iteration 2/25 | Loss: 0.00185308
Iteration 3/25 | Loss: 0.00166247
Iteration 4/25 | Loss: 0.00146456
Iteration 5/25 | Loss: 0.00142192
Iteration 6/25 | Loss: 0.00139931
Iteration 7/25 | Loss: 0.00135710
Iteration 8/25 | Loss: 0.00135219
Iteration 9/25 | Loss: 0.00139570
Iteration 10/25 | Loss: 0.00127632
Iteration 11/25 | Loss: 0.00124269
Iteration 12/25 | Loss: 0.00123302
Iteration 13/25 | Loss: 0.00121800
Iteration 14/25 | Loss: 0.00121850
Iteration 15/25 | Loss: 0.00121349
Iteration 16/25 | Loss: 0.00119910
Iteration 17/25 | Loss: 0.00120030
Iteration 18/25 | Loss: 0.00120832
Iteration 19/25 | Loss: 0.00120865
Iteration 20/25 | Loss: 0.00121483
Iteration 21/25 | Loss: 0.00120713
Iteration 22/25 | Loss: 0.00119668
Iteration 23/25 | Loss: 0.00120078
Iteration 24/25 | Loss: 0.00120059
Iteration 25/25 | Loss: 0.00120016

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37492621
Iteration 2/25 | Loss: 0.00125910
Iteration 3/25 | Loss: 0.00125909
Iteration 4/25 | Loss: 0.00125909
Iteration 5/25 | Loss: 0.00125909
Iteration 6/25 | Loss: 0.00125909
Iteration 7/25 | Loss: 0.00125909
Iteration 8/25 | Loss: 0.00125909
Iteration 9/25 | Loss: 0.00125909
Iteration 10/25 | Loss: 0.00125909
Iteration 11/25 | Loss: 0.00125909
Iteration 12/25 | Loss: 0.00125909
Iteration 13/25 | Loss: 0.00125909
Iteration 14/25 | Loss: 0.00125909
Iteration 15/25 | Loss: 0.00125909
Iteration 16/25 | Loss: 0.00125909
Iteration 17/25 | Loss: 0.00125909
Iteration 18/25 | Loss: 0.00125909
Iteration 19/25 | Loss: 0.00125909
Iteration 20/25 | Loss: 0.00125909
Iteration 21/25 | Loss: 0.00125909
Iteration 22/25 | Loss: 0.00125909
Iteration 23/25 | Loss: 0.00125909
Iteration 24/25 | Loss: 0.00125909
Iteration 25/25 | Loss: 0.00125909

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00125909
Iteration 2/1000 | Loss: 0.00085420
Iteration 3/1000 | Loss: 0.00053592
Iteration 4/1000 | Loss: 0.00060740
Iteration 5/1000 | Loss: 0.00075053
Iteration 6/1000 | Loss: 0.00062260
Iteration 7/1000 | Loss: 0.00032075
Iteration 8/1000 | Loss: 0.00037062
Iteration 9/1000 | Loss: 0.00039963
Iteration 10/1000 | Loss: 0.00030862
Iteration 11/1000 | Loss: 0.00060427
Iteration 12/1000 | Loss: 0.00037533
Iteration 13/1000 | Loss: 0.00043411
Iteration 14/1000 | Loss: 0.00044329
Iteration 15/1000 | Loss: 0.00053438
Iteration 16/1000 | Loss: 0.00045777
Iteration 17/1000 | Loss: 0.00034962
Iteration 18/1000 | Loss: 0.00035487
Iteration 19/1000 | Loss: 0.00037344
Iteration 20/1000 | Loss: 0.00056224
Iteration 21/1000 | Loss: 0.00051472
Iteration 22/1000 | Loss: 0.00061496
Iteration 23/1000 | Loss: 0.00101360
Iteration 24/1000 | Loss: 0.00076581
Iteration 25/1000 | Loss: 0.00048454
Iteration 26/1000 | Loss: 0.00057683
Iteration 27/1000 | Loss: 0.00059440
Iteration 28/1000 | Loss: 0.00056381
Iteration 29/1000 | Loss: 0.00057182
Iteration 30/1000 | Loss: 0.00033256
Iteration 31/1000 | Loss: 0.00017615
Iteration 32/1000 | Loss: 0.00033421
Iteration 33/1000 | Loss: 0.00032784
Iteration 34/1000 | Loss: 0.00043379
Iteration 35/1000 | Loss: 0.00045021
Iteration 36/1000 | Loss: 0.00067944
Iteration 37/1000 | Loss: 0.00128296
Iteration 38/1000 | Loss: 0.00097418
Iteration 39/1000 | Loss: 0.00076133
Iteration 40/1000 | Loss: 0.00192882
Iteration 41/1000 | Loss: 0.00197142
Iteration 42/1000 | Loss: 0.00116978
Iteration 43/1000 | Loss: 0.00133887
Iteration 44/1000 | Loss: 0.00182237
Iteration 45/1000 | Loss: 0.00191484
Iteration 46/1000 | Loss: 0.00171801
Iteration 47/1000 | Loss: 0.00046046
Iteration 48/1000 | Loss: 0.00053391
Iteration 49/1000 | Loss: 0.00057016
Iteration 50/1000 | Loss: 0.00048580
Iteration 51/1000 | Loss: 0.00048845
Iteration 52/1000 | Loss: 0.00052757
Iteration 53/1000 | Loss: 0.00054328
Iteration 54/1000 | Loss: 0.00054690
Iteration 55/1000 | Loss: 0.00055131
Iteration 56/1000 | Loss: 0.00058114
Iteration 57/1000 | Loss: 0.00058639
Iteration 58/1000 | Loss: 0.00118849
Iteration 59/1000 | Loss: 0.00042019
Iteration 60/1000 | Loss: 0.00039039
Iteration 61/1000 | Loss: 0.00055076
Iteration 62/1000 | Loss: 0.00034504
Iteration 63/1000 | Loss: 0.00052286
Iteration 64/1000 | Loss: 0.00045166
Iteration 65/1000 | Loss: 0.00027966
Iteration 66/1000 | Loss: 0.00043296
Iteration 67/1000 | Loss: 0.00065520
Iteration 68/1000 | Loss: 0.00057604
Iteration 69/1000 | Loss: 0.00171033
Iteration 70/1000 | Loss: 0.00056027
Iteration 71/1000 | Loss: 0.00007494
Iteration 72/1000 | Loss: 0.00021055
Iteration 73/1000 | Loss: 0.00028638
Iteration 74/1000 | Loss: 0.00025992
Iteration 75/1000 | Loss: 0.00034439
Iteration 76/1000 | Loss: 0.00045719
Iteration 77/1000 | Loss: 0.00039735
Iteration 78/1000 | Loss: 0.00016706
Iteration 79/1000 | Loss: 0.00013541
Iteration 80/1000 | Loss: 0.00021450
Iteration 81/1000 | Loss: 0.00028440
Iteration 82/1000 | Loss: 0.00023507
Iteration 83/1000 | Loss: 0.00030975
Iteration 84/1000 | Loss: 0.00039158
Iteration 85/1000 | Loss: 0.00036496
Iteration 86/1000 | Loss: 0.00044656
Iteration 87/1000 | Loss: 0.00033320
Iteration 88/1000 | Loss: 0.00056094
Iteration 89/1000 | Loss: 0.00032431
Iteration 90/1000 | Loss: 0.00038737
Iteration 91/1000 | Loss: 0.00053080
Iteration 92/1000 | Loss: 0.00040963
Iteration 93/1000 | Loss: 0.00033966
Iteration 94/1000 | Loss: 0.00034459
Iteration 95/1000 | Loss: 0.00029254
Iteration 96/1000 | Loss: 0.00029106
Iteration 97/1000 | Loss: 0.00035444
Iteration 98/1000 | Loss: 0.00038057
Iteration 99/1000 | Loss: 0.00195942
Iteration 100/1000 | Loss: 0.00176914
Iteration 101/1000 | Loss: 0.00118289
Iteration 102/1000 | Loss: 0.00032667
Iteration 103/1000 | Loss: 0.00046995
Iteration 104/1000 | Loss: 0.00093199
Iteration 105/1000 | Loss: 0.00124014
Iteration 106/1000 | Loss: 0.00093859
Iteration 107/1000 | Loss: 0.00115911
Iteration 108/1000 | Loss: 0.00068171
Iteration 109/1000 | Loss: 0.00018621
Iteration 110/1000 | Loss: 0.00012561
Iteration 111/1000 | Loss: 0.00007034
Iteration 112/1000 | Loss: 0.00008534
Iteration 113/1000 | Loss: 0.00004491
Iteration 114/1000 | Loss: 0.00007596
Iteration 115/1000 | Loss: 0.00004650
Iteration 116/1000 | Loss: 0.00006724
Iteration 117/1000 | Loss: 0.00027429
Iteration 118/1000 | Loss: 0.00007081
Iteration 119/1000 | Loss: 0.00005069
Iteration 120/1000 | Loss: 0.00004696
Iteration 121/1000 | Loss: 0.00005396
Iteration 122/1000 | Loss: 0.00003919
Iteration 123/1000 | Loss: 0.00019217
Iteration 124/1000 | Loss: 0.00014511
Iteration 125/1000 | Loss: 0.00005190
Iteration 126/1000 | Loss: 0.00003696
Iteration 127/1000 | Loss: 0.00038934
Iteration 128/1000 | Loss: 0.00023287
Iteration 129/1000 | Loss: 0.00021879
Iteration 130/1000 | Loss: 0.00019988
Iteration 131/1000 | Loss: 0.00003630
Iteration 132/1000 | Loss: 0.00005132
Iteration 133/1000 | Loss: 0.00038472
Iteration 134/1000 | Loss: 0.00033883
Iteration 135/1000 | Loss: 0.00037657
Iteration 136/1000 | Loss: 0.00030774
Iteration 137/1000 | Loss: 0.00035929
Iteration 138/1000 | Loss: 0.00052925
Iteration 139/1000 | Loss: 0.00056875
Iteration 140/1000 | Loss: 0.00040995
Iteration 141/1000 | Loss: 0.00007008
Iteration 142/1000 | Loss: 0.00010162
Iteration 143/1000 | Loss: 0.00007149
Iteration 144/1000 | Loss: 0.00037824
Iteration 145/1000 | Loss: 0.00028687
Iteration 146/1000 | Loss: 0.00032002
Iteration 147/1000 | Loss: 0.00025862
Iteration 148/1000 | Loss: 0.00007760
Iteration 149/1000 | Loss: 0.00003296
Iteration 150/1000 | Loss: 0.00002886
Iteration 151/1000 | Loss: 0.00002736
Iteration 152/1000 | Loss: 0.00016528
Iteration 153/1000 | Loss: 0.00002472
Iteration 154/1000 | Loss: 0.00002184
Iteration 155/1000 | Loss: 0.00002015
Iteration 156/1000 | Loss: 0.00001913
Iteration 157/1000 | Loss: 0.00001833
Iteration 158/1000 | Loss: 0.00001785
Iteration 159/1000 | Loss: 0.00001751
Iteration 160/1000 | Loss: 0.00001687
Iteration 161/1000 | Loss: 0.00001646
Iteration 162/1000 | Loss: 0.00002962
Iteration 163/1000 | Loss: 0.00001718
Iteration 164/1000 | Loss: 0.00001582
Iteration 165/1000 | Loss: 0.00001576
Iteration 166/1000 | Loss: 0.00001575
Iteration 167/1000 | Loss: 0.00001575
Iteration 168/1000 | Loss: 0.00001574
Iteration 169/1000 | Loss: 0.00001573
Iteration 170/1000 | Loss: 0.00002541
Iteration 171/1000 | Loss: 0.00009083
Iteration 172/1000 | Loss: 0.00002437
Iteration 173/1000 | Loss: 0.00001623
Iteration 174/1000 | Loss: 0.00001754
Iteration 175/1000 | Loss: 0.00001655
Iteration 176/1000 | Loss: 0.00001540
Iteration 177/1000 | Loss: 0.00001535
Iteration 178/1000 | Loss: 0.00001535
Iteration 179/1000 | Loss: 0.00001535
Iteration 180/1000 | Loss: 0.00001535
Iteration 181/1000 | Loss: 0.00001534
Iteration 182/1000 | Loss: 0.00001534
Iteration 183/1000 | Loss: 0.00001530
Iteration 184/1000 | Loss: 0.00001529
Iteration 185/1000 | Loss: 0.00001529
Iteration 186/1000 | Loss: 0.00001528
Iteration 187/1000 | Loss: 0.00001528
Iteration 188/1000 | Loss: 0.00001528
Iteration 189/1000 | Loss: 0.00001527
Iteration 190/1000 | Loss: 0.00001527
Iteration 191/1000 | Loss: 0.00001527
Iteration 192/1000 | Loss: 0.00001527
Iteration 193/1000 | Loss: 0.00001527
Iteration 194/1000 | Loss: 0.00001527
Iteration 195/1000 | Loss: 0.00001527
Iteration 196/1000 | Loss: 0.00001526
Iteration 197/1000 | Loss: 0.00001526
Iteration 198/1000 | Loss: 0.00001526
Iteration 199/1000 | Loss: 0.00001526
Iteration 200/1000 | Loss: 0.00001526
Iteration 201/1000 | Loss: 0.00001526
Iteration 202/1000 | Loss: 0.00001526
Iteration 203/1000 | Loss: 0.00001526
Iteration 204/1000 | Loss: 0.00001526
Iteration 205/1000 | Loss: 0.00001525
Iteration 206/1000 | Loss: 0.00001525
Iteration 207/1000 | Loss: 0.00001525
Iteration 208/1000 | Loss: 0.00001525
Iteration 209/1000 | Loss: 0.00001525
Iteration 210/1000 | Loss: 0.00001525
Iteration 211/1000 | Loss: 0.00001525
Iteration 212/1000 | Loss: 0.00001525
Iteration 213/1000 | Loss: 0.00001525
Iteration 214/1000 | Loss: 0.00001525
Iteration 215/1000 | Loss: 0.00001525
Iteration 216/1000 | Loss: 0.00001524
Iteration 217/1000 | Loss: 0.00001524
Iteration 218/1000 | Loss: 0.00001524
Iteration 219/1000 | Loss: 0.00001524
Iteration 220/1000 | Loss: 0.00001524
Iteration 221/1000 | Loss: 0.00001524
Iteration 222/1000 | Loss: 0.00001524
Iteration 223/1000 | Loss: 0.00001524
Iteration 224/1000 | Loss: 0.00001524
Iteration 225/1000 | Loss: 0.00001523
Iteration 226/1000 | Loss: 0.00001523
Iteration 227/1000 | Loss: 0.00001523
Iteration 228/1000 | Loss: 0.00001523
Iteration 229/1000 | Loss: 0.00001523
Iteration 230/1000 | Loss: 0.00001523
Iteration 231/1000 | Loss: 0.00001522
Iteration 232/1000 | Loss: 0.00001522
Iteration 233/1000 | Loss: 0.00001522
Iteration 234/1000 | Loss: 0.00001522
Iteration 235/1000 | Loss: 0.00001522
Iteration 236/1000 | Loss: 0.00001522
Iteration 237/1000 | Loss: 0.00001522
Iteration 238/1000 | Loss: 0.00001521
Iteration 239/1000 | Loss: 0.00001521
Iteration 240/1000 | Loss: 0.00001521
Iteration 241/1000 | Loss: 0.00001521
Iteration 242/1000 | Loss: 0.00001521
Iteration 243/1000 | Loss: 0.00001521
Iteration 244/1000 | Loss: 0.00001521
Iteration 245/1000 | Loss: 0.00001521
Iteration 246/1000 | Loss: 0.00001521
Iteration 247/1000 | Loss: 0.00001521
Iteration 248/1000 | Loss: 0.00001520
Iteration 249/1000 | Loss: 0.00001520
Iteration 250/1000 | Loss: 0.00001520
Iteration 251/1000 | Loss: 0.00001520
Iteration 252/1000 | Loss: 0.00001520
Iteration 253/1000 | Loss: 0.00001520
Iteration 254/1000 | Loss: 0.00001520
Iteration 255/1000 | Loss: 0.00001520
Iteration 256/1000 | Loss: 0.00001519
Iteration 257/1000 | Loss: 0.00001519
Iteration 258/1000 | Loss: 0.00001519
Iteration 259/1000 | Loss: 0.00001519
Iteration 260/1000 | Loss: 0.00001519
Iteration 261/1000 | Loss: 0.00001519
Iteration 262/1000 | Loss: 0.00001518
Iteration 263/1000 | Loss: 0.00001518
Iteration 264/1000 | Loss: 0.00001518
Iteration 265/1000 | Loss: 0.00001518
Iteration 266/1000 | Loss: 0.00001518
Iteration 267/1000 | Loss: 0.00001518
Iteration 268/1000 | Loss: 0.00001517
Iteration 269/1000 | Loss: 0.00001517
Iteration 270/1000 | Loss: 0.00001517
Iteration 271/1000 | Loss: 0.00001517
Iteration 272/1000 | Loss: 0.00001517
Iteration 273/1000 | Loss: 0.00001516
Iteration 274/1000 | Loss: 0.00001516
Iteration 275/1000 | Loss: 0.00001516
Iteration 276/1000 | Loss: 0.00001516
Iteration 277/1000 | Loss: 0.00001516
Iteration 278/1000 | Loss: 0.00001515
Iteration 279/1000 | Loss: 0.00001514
Iteration 280/1000 | Loss: 0.00001514
Iteration 281/1000 | Loss: 0.00001513
Iteration 282/1000 | Loss: 0.00001513
Iteration 283/1000 | Loss: 0.00001513
Iteration 284/1000 | Loss: 0.00001513
Iteration 285/1000 | Loss: 0.00001513
Iteration 286/1000 | Loss: 0.00001513
Iteration 287/1000 | Loss: 0.00001513
Iteration 288/1000 | Loss: 0.00001513
Iteration 289/1000 | Loss: 0.00001513
Iteration 290/1000 | Loss: 0.00001512
Iteration 291/1000 | Loss: 0.00001512
Iteration 292/1000 | Loss: 0.00001512
Iteration 293/1000 | Loss: 0.00001512
Iteration 294/1000 | Loss: 0.00001512
Iteration 295/1000 | Loss: 0.00001512
Iteration 296/1000 | Loss: 0.00001512
Iteration 297/1000 | Loss: 0.00001512
Iteration 298/1000 | Loss: 0.00001512
Iteration 299/1000 | Loss: 0.00001512
Iteration 300/1000 | Loss: 0.00001512
Iteration 301/1000 | Loss: 0.00001511
Iteration 302/1000 | Loss: 0.00001511
Iteration 303/1000 | Loss: 0.00001511
Iteration 304/1000 | Loss: 0.00001511
Iteration 305/1000 | Loss: 0.00001511
Iteration 306/1000 | Loss: 0.00001511
Iteration 307/1000 | Loss: 0.00001511
Iteration 308/1000 | Loss: 0.00003506
Iteration 309/1000 | Loss: 0.00001516
Iteration 310/1000 | Loss: 0.00001758
Iteration 311/1000 | Loss: 0.00001512
Iteration 312/1000 | Loss: 0.00001511
Iteration 313/1000 | Loss: 0.00001511
Iteration 314/1000 | Loss: 0.00001510
Iteration 315/1000 | Loss: 0.00001804
Iteration 316/1000 | Loss: 0.00001510
Iteration 317/1000 | Loss: 0.00001510
Iteration 318/1000 | Loss: 0.00001510
Iteration 319/1000 | Loss: 0.00001510
Iteration 320/1000 | Loss: 0.00001510
Iteration 321/1000 | Loss: 0.00001510
Iteration 322/1000 | Loss: 0.00001510
Iteration 323/1000 | Loss: 0.00001510
Iteration 324/1000 | Loss: 0.00001510
Iteration 325/1000 | Loss: 0.00001510
Iteration 326/1000 | Loss: 0.00001510
Iteration 327/1000 | Loss: 0.00001510
Iteration 328/1000 | Loss: 0.00001510
Iteration 329/1000 | Loss: 0.00001510
Iteration 330/1000 | Loss: 0.00001510
Iteration 331/1000 | Loss: 0.00001510
Iteration 332/1000 | Loss: 0.00001510
Iteration 333/1000 | Loss: 0.00001510
Iteration 334/1000 | Loss: 0.00001510
Iteration 335/1000 | Loss: 0.00001510
Iteration 336/1000 | Loss: 0.00001510
Iteration 337/1000 | Loss: 0.00001510
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 337. Stopping optimization.
Last 5 losses: [1.5095951312105171e-05, 1.5095951312105171e-05, 1.5095951312105171e-05, 1.5095951312105171e-05, 1.5095951312105171e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5095951312105171e-05

Optimization complete. Final v2v error: 3.2652368545532227 mm

Highest mean error: 4.579878807067871 mm for frame 43

Lowest mean error: 2.731935501098633 mm for frame 21

Saving results

Total time: 301.8561677932739
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_025/1074/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_025/1074.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_025/1074
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00785860
Iteration 2/25 | Loss: 0.00146113
Iteration 3/25 | Loss: 0.00126693
Iteration 4/25 | Loss: 0.00125147
Iteration 5/25 | Loss: 0.00124626
Iteration 6/25 | Loss: 0.00124587
Iteration 7/25 | Loss: 0.00124587
Iteration 8/25 | Loss: 0.00124587
Iteration 9/25 | Loss: 0.00124587
Iteration 10/25 | Loss: 0.00124587
Iteration 11/25 | Loss: 0.00124587
Iteration 12/25 | Loss: 0.00124587
Iteration 13/25 | Loss: 0.00124587
Iteration 14/25 | Loss: 0.00124587
Iteration 15/25 | Loss: 0.00124587
Iteration 16/25 | Loss: 0.00124587
Iteration 17/25 | Loss: 0.00124587
Iteration 18/25 | Loss: 0.00124587
Iteration 19/25 | Loss: 0.00124587
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0012458749115467072, 0.0012458749115467072, 0.0012458749115467072, 0.0012458749115467072, 0.0012458749115467072]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012458749115467072

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37659907
Iteration 2/25 | Loss: 0.00097829
Iteration 3/25 | Loss: 0.00097828
Iteration 4/25 | Loss: 0.00097828
Iteration 5/25 | Loss: 0.00097828
Iteration 6/25 | Loss: 0.00097828
Iteration 7/25 | Loss: 0.00097828
Iteration 8/25 | Loss: 0.00097828
Iteration 9/25 | Loss: 0.00097828
Iteration 10/25 | Loss: 0.00097828
Iteration 11/25 | Loss: 0.00097828
Iteration 12/25 | Loss: 0.00097828
Iteration 13/25 | Loss: 0.00097828
Iteration 14/25 | Loss: 0.00097828
Iteration 15/25 | Loss: 0.00097828
Iteration 16/25 | Loss: 0.00097828
Iteration 17/25 | Loss: 0.00097828
Iteration 18/25 | Loss: 0.00097828
Iteration 19/25 | Loss: 0.00097828
Iteration 20/25 | Loss: 0.00097828
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0009782795095816255, 0.0009782795095816255, 0.0009782795095816255, 0.0009782795095816255, 0.0009782795095816255]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009782795095816255

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00097828
Iteration 2/1000 | Loss: 0.00004727
Iteration 3/1000 | Loss: 0.00003294
Iteration 4/1000 | Loss: 0.00002941
Iteration 5/1000 | Loss: 0.00002716
Iteration 6/1000 | Loss: 0.00002573
Iteration 7/1000 | Loss: 0.00002482
Iteration 8/1000 | Loss: 0.00002413
Iteration 9/1000 | Loss: 0.00002376
Iteration 10/1000 | Loss: 0.00002335
Iteration 11/1000 | Loss: 0.00002298
Iteration 12/1000 | Loss: 0.00002274
Iteration 13/1000 | Loss: 0.00002255
Iteration 14/1000 | Loss: 0.00002249
Iteration 15/1000 | Loss: 0.00002248
Iteration 16/1000 | Loss: 0.00002247
Iteration 17/1000 | Loss: 0.00002245
Iteration 18/1000 | Loss: 0.00002243
Iteration 19/1000 | Loss: 0.00002243
Iteration 20/1000 | Loss: 0.00002241
Iteration 21/1000 | Loss: 0.00002240
Iteration 22/1000 | Loss: 0.00002228
Iteration 23/1000 | Loss: 0.00002225
Iteration 24/1000 | Loss: 0.00002224
Iteration 25/1000 | Loss: 0.00002223
Iteration 26/1000 | Loss: 0.00002222
Iteration 27/1000 | Loss: 0.00002221
Iteration 28/1000 | Loss: 0.00002221
Iteration 29/1000 | Loss: 0.00002217
Iteration 30/1000 | Loss: 0.00002217
Iteration 31/1000 | Loss: 0.00002214
Iteration 32/1000 | Loss: 0.00002213
Iteration 33/1000 | Loss: 0.00002213
Iteration 34/1000 | Loss: 0.00002212
Iteration 35/1000 | Loss: 0.00002212
Iteration 36/1000 | Loss: 0.00002212
Iteration 37/1000 | Loss: 0.00002209
Iteration 38/1000 | Loss: 0.00002209
Iteration 39/1000 | Loss: 0.00002207
Iteration 40/1000 | Loss: 0.00002207
Iteration 41/1000 | Loss: 0.00002207
Iteration 42/1000 | Loss: 0.00002207
Iteration 43/1000 | Loss: 0.00002207
Iteration 44/1000 | Loss: 0.00002207
Iteration 45/1000 | Loss: 0.00002207
Iteration 46/1000 | Loss: 0.00002207
Iteration 47/1000 | Loss: 0.00002207
Iteration 48/1000 | Loss: 0.00002207
Iteration 49/1000 | Loss: 0.00002206
Iteration 50/1000 | Loss: 0.00002206
Iteration 51/1000 | Loss: 0.00002206
Iteration 52/1000 | Loss: 0.00002206
Iteration 53/1000 | Loss: 0.00002206
Iteration 54/1000 | Loss: 0.00002206
Iteration 55/1000 | Loss: 0.00002206
Iteration 56/1000 | Loss: 0.00002206
Iteration 57/1000 | Loss: 0.00002206
Iteration 58/1000 | Loss: 0.00002206
Iteration 59/1000 | Loss: 0.00002206
Iteration 60/1000 | Loss: 0.00002205
Iteration 61/1000 | Loss: 0.00002205
Iteration 62/1000 | Loss: 0.00002204
Iteration 63/1000 | Loss: 0.00002204
Iteration 64/1000 | Loss: 0.00002204
Iteration 65/1000 | Loss: 0.00002203
Iteration 66/1000 | Loss: 0.00002203
Iteration 67/1000 | Loss: 0.00002203
Iteration 68/1000 | Loss: 0.00002202
Iteration 69/1000 | Loss: 0.00002202
Iteration 70/1000 | Loss: 0.00002202
Iteration 71/1000 | Loss: 0.00002202
Iteration 72/1000 | Loss: 0.00002201
Iteration 73/1000 | Loss: 0.00002201
Iteration 74/1000 | Loss: 0.00002201
Iteration 75/1000 | Loss: 0.00002200
Iteration 76/1000 | Loss: 0.00002200
Iteration 77/1000 | Loss: 0.00002200
Iteration 78/1000 | Loss: 0.00002200
Iteration 79/1000 | Loss: 0.00002200
Iteration 80/1000 | Loss: 0.00002200
Iteration 81/1000 | Loss: 0.00002200
Iteration 82/1000 | Loss: 0.00002200
Iteration 83/1000 | Loss: 0.00002199
Iteration 84/1000 | Loss: 0.00002199
Iteration 85/1000 | Loss: 0.00002199
Iteration 86/1000 | Loss: 0.00002198
Iteration 87/1000 | Loss: 0.00002198
Iteration 88/1000 | Loss: 0.00002198
Iteration 89/1000 | Loss: 0.00002197
Iteration 90/1000 | Loss: 0.00002197
Iteration 91/1000 | Loss: 0.00002197
Iteration 92/1000 | Loss: 0.00002196
Iteration 93/1000 | Loss: 0.00002196
Iteration 94/1000 | Loss: 0.00002196
Iteration 95/1000 | Loss: 0.00002196
Iteration 96/1000 | Loss: 0.00002196
Iteration 97/1000 | Loss: 0.00002196
Iteration 98/1000 | Loss: 0.00002195
Iteration 99/1000 | Loss: 0.00002195
Iteration 100/1000 | Loss: 0.00002195
Iteration 101/1000 | Loss: 0.00002195
Iteration 102/1000 | Loss: 0.00002195
Iteration 103/1000 | Loss: 0.00002194
Iteration 104/1000 | Loss: 0.00002194
Iteration 105/1000 | Loss: 0.00002194
Iteration 106/1000 | Loss: 0.00002194
Iteration 107/1000 | Loss: 0.00002194
Iteration 108/1000 | Loss: 0.00002194
Iteration 109/1000 | Loss: 0.00002194
Iteration 110/1000 | Loss: 0.00002193
Iteration 111/1000 | Loss: 0.00002193
Iteration 112/1000 | Loss: 0.00002193
Iteration 113/1000 | Loss: 0.00002193
Iteration 114/1000 | Loss: 0.00002193
Iteration 115/1000 | Loss: 0.00002193
Iteration 116/1000 | Loss: 0.00002192
Iteration 117/1000 | Loss: 0.00002192
Iteration 118/1000 | Loss: 0.00002192
Iteration 119/1000 | Loss: 0.00002192
Iteration 120/1000 | Loss: 0.00002192
Iteration 121/1000 | Loss: 0.00002192
Iteration 122/1000 | Loss: 0.00002192
Iteration 123/1000 | Loss: 0.00002191
Iteration 124/1000 | Loss: 0.00002191
Iteration 125/1000 | Loss: 0.00002191
Iteration 126/1000 | Loss: 0.00002190
Iteration 127/1000 | Loss: 0.00002190
Iteration 128/1000 | Loss: 0.00002190
Iteration 129/1000 | Loss: 0.00002189
Iteration 130/1000 | Loss: 0.00002189
Iteration 131/1000 | Loss: 0.00002189
Iteration 132/1000 | Loss: 0.00002189
Iteration 133/1000 | Loss: 0.00002188
Iteration 134/1000 | Loss: 0.00002188
Iteration 135/1000 | Loss: 0.00002188
Iteration 136/1000 | Loss: 0.00002187
Iteration 137/1000 | Loss: 0.00002187
Iteration 138/1000 | Loss: 0.00002187
Iteration 139/1000 | Loss: 0.00002186
Iteration 140/1000 | Loss: 0.00002186
Iteration 141/1000 | Loss: 0.00002186
Iteration 142/1000 | Loss: 0.00002186
Iteration 143/1000 | Loss: 0.00002186
Iteration 144/1000 | Loss: 0.00002186
Iteration 145/1000 | Loss: 0.00002186
Iteration 146/1000 | Loss: 0.00002186
Iteration 147/1000 | Loss: 0.00002186
Iteration 148/1000 | Loss: 0.00002185
Iteration 149/1000 | Loss: 0.00002185
Iteration 150/1000 | Loss: 0.00002185
Iteration 151/1000 | Loss: 0.00002185
Iteration 152/1000 | Loss: 0.00002185
Iteration 153/1000 | Loss: 0.00002185
Iteration 154/1000 | Loss: 0.00002185
Iteration 155/1000 | Loss: 0.00002185
Iteration 156/1000 | Loss: 0.00002185
Iteration 157/1000 | Loss: 0.00002184
Iteration 158/1000 | Loss: 0.00002184
Iteration 159/1000 | Loss: 0.00002184
Iteration 160/1000 | Loss: 0.00002184
Iteration 161/1000 | Loss: 0.00002184
Iteration 162/1000 | Loss: 0.00002184
Iteration 163/1000 | Loss: 0.00002184
Iteration 164/1000 | Loss: 0.00002184
Iteration 165/1000 | Loss: 0.00002184
Iteration 166/1000 | Loss: 0.00002184
Iteration 167/1000 | Loss: 0.00002184
Iteration 168/1000 | Loss: 0.00002184
Iteration 169/1000 | Loss: 0.00002184
Iteration 170/1000 | Loss: 0.00002184
Iteration 171/1000 | Loss: 0.00002184
Iteration 172/1000 | Loss: 0.00002184
Iteration 173/1000 | Loss: 0.00002184
Iteration 174/1000 | Loss: 0.00002184
Iteration 175/1000 | Loss: 0.00002184
Iteration 176/1000 | Loss: 0.00002184
Iteration 177/1000 | Loss: 0.00002184
Iteration 178/1000 | Loss: 0.00002184
Iteration 179/1000 | Loss: 0.00002184
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 179. Stopping optimization.
Last 5 losses: [2.1840351109858602e-05, 2.1840351109858602e-05, 2.1840351109858602e-05, 2.1840351109858602e-05, 2.1840351109858602e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1840351109858602e-05

Optimization complete. Final v2v error: 3.894531488418579 mm

Highest mean error: 4.563790321350098 mm for frame 168

Lowest mean error: 3.2521860599517822 mm for frame 197

Saving results

Total time: 47.81918382644653
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_025/1094/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_025/1094.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_025/1094
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00989461
Iteration 2/25 | Loss: 0.00319375
Iteration 3/25 | Loss: 0.00243167
Iteration 4/25 | Loss: 0.00211946
Iteration 5/25 | Loss: 0.00226612
Iteration 6/25 | Loss: 0.00250847
Iteration 7/25 | Loss: 0.00223353
Iteration 8/25 | Loss: 0.00182120
Iteration 9/25 | Loss: 0.00172155
Iteration 10/25 | Loss: 0.00163248
Iteration 11/25 | Loss: 0.00160963
Iteration 12/25 | Loss: 0.00160391
Iteration 13/25 | Loss: 0.00159944
Iteration 14/25 | Loss: 0.00159652
Iteration 15/25 | Loss: 0.00159554
Iteration 16/25 | Loss: 0.00159500
Iteration 17/25 | Loss: 0.00159408
Iteration 18/25 | Loss: 0.00159280
Iteration 19/25 | Loss: 0.00159177
Iteration 20/25 | Loss: 0.00159081
Iteration 21/25 | Loss: 0.00158952
Iteration 22/25 | Loss: 0.00158855
Iteration 23/25 | Loss: 0.00158760
Iteration 24/25 | Loss: 0.00158663
Iteration 25/25 | Loss: 0.00158567

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.37517071
Iteration 2/25 | Loss: 0.00626449
Iteration 3/25 | Loss: 0.00458505
Iteration 4/25 | Loss: 0.00458505
Iteration 5/25 | Loss: 0.00458505
Iteration 6/25 | Loss: 0.00458505
Iteration 7/25 | Loss: 0.00458505
Iteration 8/25 | Loss: 0.00458505
Iteration 9/25 | Loss: 0.00458505
Iteration 10/25 | Loss: 0.00458505
Iteration 11/25 | Loss: 0.00458505
Iteration 12/25 | Loss: 0.00458505
Iteration 13/25 | Loss: 0.00458505
Iteration 14/25 | Loss: 0.00458505
Iteration 15/25 | Loss: 0.00458505
Iteration 16/25 | Loss: 0.00458505
Iteration 17/25 | Loss: 0.00458505
Iteration 18/25 | Loss: 0.00458505
Iteration 19/25 | Loss: 0.00458505
Iteration 20/25 | Loss: 0.00458505
Iteration 21/25 | Loss: 0.00458505
Iteration 22/25 | Loss: 0.00458505
Iteration 23/25 | Loss: 0.00458505
Iteration 24/25 | Loss: 0.00458505
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.004585046321153641, 0.004585046321153641, 0.004585046321153641, 0.004585046321153641, 0.004585046321153641]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.004585046321153641

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00458505
Iteration 2/1000 | Loss: 0.00245887
Iteration 3/1000 | Loss: 0.00132212
Iteration 4/1000 | Loss: 0.00180241
Iteration 5/1000 | Loss: 0.00230322
Iteration 6/1000 | Loss: 0.00256988
Iteration 7/1000 | Loss: 0.00132664
Iteration 8/1000 | Loss: 0.00124825
Iteration 9/1000 | Loss: 0.00064776
Iteration 10/1000 | Loss: 0.00477005
Iteration 11/1000 | Loss: 0.00620570
Iteration 12/1000 | Loss: 0.00211202
Iteration 13/1000 | Loss: 0.00117675
Iteration 14/1000 | Loss: 0.00065244
Iteration 15/1000 | Loss: 0.00114850
Iteration 16/1000 | Loss: 0.00088939
Iteration 17/1000 | Loss: 0.00095296
Iteration 18/1000 | Loss: 0.00024235
Iteration 19/1000 | Loss: 0.00050623
Iteration 20/1000 | Loss: 0.00177854
Iteration 21/1000 | Loss: 0.00082621
Iteration 22/1000 | Loss: 0.00119153
Iteration 23/1000 | Loss: 0.00204653
Iteration 24/1000 | Loss: 0.00235042
Iteration 25/1000 | Loss: 0.00036287
Iteration 26/1000 | Loss: 0.00257604
Iteration 27/1000 | Loss: 0.00222228
Iteration 28/1000 | Loss: 0.00077613
Iteration 29/1000 | Loss: 0.00104793
Iteration 30/1000 | Loss: 0.00075867
Iteration 31/1000 | Loss: 0.00088411
Iteration 32/1000 | Loss: 0.00032867
Iteration 33/1000 | Loss: 0.00021332
Iteration 34/1000 | Loss: 0.00041146
Iteration 35/1000 | Loss: 0.00056103
Iteration 36/1000 | Loss: 0.00065454
Iteration 37/1000 | Loss: 0.00067906
Iteration 38/1000 | Loss: 0.00071322
Iteration 39/1000 | Loss: 0.00067489
Iteration 40/1000 | Loss: 0.00125841
Iteration 41/1000 | Loss: 0.00146896
Iteration 42/1000 | Loss: 0.00061483
Iteration 43/1000 | Loss: 0.00048778
Iteration 44/1000 | Loss: 0.00024271
Iteration 45/1000 | Loss: 0.00024366
Iteration 46/1000 | Loss: 0.00063929
Iteration 47/1000 | Loss: 0.00050377
Iteration 48/1000 | Loss: 0.00056645
Iteration 49/1000 | Loss: 0.00054069
Iteration 50/1000 | Loss: 0.00064796
Iteration 51/1000 | Loss: 0.00117958
Iteration 52/1000 | Loss: 0.00160888
Iteration 53/1000 | Loss: 0.00168316
Iteration 54/1000 | Loss: 0.00038973
Iteration 55/1000 | Loss: 0.00135714
Iteration 56/1000 | Loss: 0.00189432
Iteration 57/1000 | Loss: 0.00069216
Iteration 58/1000 | Loss: 0.00009577
Iteration 59/1000 | Loss: 0.00104782
Iteration 60/1000 | Loss: 0.00079302
Iteration 61/1000 | Loss: 0.00061858
Iteration 62/1000 | Loss: 0.00116169
Iteration 63/1000 | Loss: 0.00090175
Iteration 64/1000 | Loss: 0.00174762
Iteration 65/1000 | Loss: 0.00116066
Iteration 66/1000 | Loss: 0.00077703
Iteration 67/1000 | Loss: 0.00102417
Iteration 68/1000 | Loss: 0.00147229
Iteration 69/1000 | Loss: 0.00088813
Iteration 70/1000 | Loss: 0.00013382
Iteration 71/1000 | Loss: 0.00007513
Iteration 72/1000 | Loss: 0.00006699
Iteration 73/1000 | Loss: 0.00117073
Iteration 74/1000 | Loss: 0.00056446
Iteration 75/1000 | Loss: 0.00041849
Iteration 76/1000 | Loss: 0.00090812
Iteration 77/1000 | Loss: 0.00049409
Iteration 78/1000 | Loss: 0.00104599
Iteration 79/1000 | Loss: 0.00082387
Iteration 80/1000 | Loss: 0.00017249
Iteration 81/1000 | Loss: 0.00103951
Iteration 82/1000 | Loss: 0.00006946
Iteration 83/1000 | Loss: 0.00067493
Iteration 84/1000 | Loss: 0.00016710
Iteration 85/1000 | Loss: 0.00028272
Iteration 86/1000 | Loss: 0.00004976
Iteration 87/1000 | Loss: 0.00028520
Iteration 88/1000 | Loss: 0.00103745
Iteration 89/1000 | Loss: 0.00005961
Iteration 90/1000 | Loss: 0.00004368
Iteration 91/1000 | Loss: 0.00023055
Iteration 92/1000 | Loss: 0.00005953
Iteration 93/1000 | Loss: 0.00004749
Iteration 94/1000 | Loss: 0.00004300
Iteration 95/1000 | Loss: 0.00048570
Iteration 96/1000 | Loss: 0.00029838
Iteration 97/1000 | Loss: 0.00027358
Iteration 98/1000 | Loss: 0.00003888
Iteration 99/1000 | Loss: 0.00003700
Iteration 100/1000 | Loss: 0.00020968
Iteration 101/1000 | Loss: 0.00004183
Iteration 102/1000 | Loss: 0.00003619
Iteration 103/1000 | Loss: 0.00003480
Iteration 104/1000 | Loss: 0.00003372
Iteration 105/1000 | Loss: 0.00003259
Iteration 106/1000 | Loss: 0.00003181
Iteration 107/1000 | Loss: 0.00003134
Iteration 108/1000 | Loss: 0.00003094
Iteration 109/1000 | Loss: 0.00003059
Iteration 110/1000 | Loss: 0.00003018
Iteration 111/1000 | Loss: 0.00002993
Iteration 112/1000 | Loss: 0.00002969
Iteration 113/1000 | Loss: 0.00002948
Iteration 114/1000 | Loss: 0.00002948
Iteration 115/1000 | Loss: 0.00053835
Iteration 116/1000 | Loss: 0.00003106
Iteration 117/1000 | Loss: 0.00002955
Iteration 118/1000 | Loss: 0.00002857
Iteration 119/1000 | Loss: 0.00002769
Iteration 120/1000 | Loss: 0.00002715
Iteration 121/1000 | Loss: 0.00002694
Iteration 122/1000 | Loss: 0.00002693
Iteration 123/1000 | Loss: 0.00002682
Iteration 124/1000 | Loss: 0.00002679
Iteration 125/1000 | Loss: 0.00002673
Iteration 126/1000 | Loss: 0.00002670
Iteration 127/1000 | Loss: 0.00002670
Iteration 128/1000 | Loss: 0.00002669
Iteration 129/1000 | Loss: 0.00002669
Iteration 130/1000 | Loss: 0.00002669
Iteration 131/1000 | Loss: 0.00002668
Iteration 132/1000 | Loss: 0.00002667
Iteration 133/1000 | Loss: 0.00002667
Iteration 134/1000 | Loss: 0.00002667
Iteration 135/1000 | Loss: 0.00002667
Iteration 136/1000 | Loss: 0.00002667
Iteration 137/1000 | Loss: 0.00002666
Iteration 138/1000 | Loss: 0.00002666
Iteration 139/1000 | Loss: 0.00002666
Iteration 140/1000 | Loss: 0.00002666
Iteration 141/1000 | Loss: 0.00002666
Iteration 142/1000 | Loss: 0.00002665
Iteration 143/1000 | Loss: 0.00002665
Iteration 144/1000 | Loss: 0.00002665
Iteration 145/1000 | Loss: 0.00002664
Iteration 146/1000 | Loss: 0.00002664
Iteration 147/1000 | Loss: 0.00002664
Iteration 148/1000 | Loss: 0.00002663
Iteration 149/1000 | Loss: 0.00002663
Iteration 150/1000 | Loss: 0.00002663
Iteration 151/1000 | Loss: 0.00002663
Iteration 152/1000 | Loss: 0.00002662
Iteration 153/1000 | Loss: 0.00002662
Iteration 154/1000 | Loss: 0.00002662
Iteration 155/1000 | Loss: 0.00002662
Iteration 156/1000 | Loss: 0.00002661
Iteration 157/1000 | Loss: 0.00002661
Iteration 158/1000 | Loss: 0.00002661
Iteration 159/1000 | Loss: 0.00002661
Iteration 160/1000 | Loss: 0.00002661
Iteration 161/1000 | Loss: 0.00002661
Iteration 162/1000 | Loss: 0.00002661
Iteration 163/1000 | Loss: 0.00002661
Iteration 164/1000 | Loss: 0.00002661
Iteration 165/1000 | Loss: 0.00002661
Iteration 166/1000 | Loss: 0.00002661
Iteration 167/1000 | Loss: 0.00002661
Iteration 168/1000 | Loss: 0.00002660
Iteration 169/1000 | Loss: 0.00002660
Iteration 170/1000 | Loss: 0.00002660
Iteration 171/1000 | Loss: 0.00002660
Iteration 172/1000 | Loss: 0.00002660
Iteration 173/1000 | Loss: 0.00002660
Iteration 174/1000 | Loss: 0.00002659
Iteration 175/1000 | Loss: 0.00002659
Iteration 176/1000 | Loss: 0.00002659
Iteration 177/1000 | Loss: 0.00002659
Iteration 178/1000 | Loss: 0.00002659
Iteration 179/1000 | Loss: 0.00002658
Iteration 180/1000 | Loss: 0.00002658
Iteration 181/1000 | Loss: 0.00002658
Iteration 182/1000 | Loss: 0.00002658
Iteration 183/1000 | Loss: 0.00002658
Iteration 184/1000 | Loss: 0.00002657
Iteration 185/1000 | Loss: 0.00002657
Iteration 186/1000 | Loss: 0.00002657
Iteration 187/1000 | Loss: 0.00002656
Iteration 188/1000 | Loss: 0.00002656
Iteration 189/1000 | Loss: 0.00002655
Iteration 190/1000 | Loss: 0.00002655
Iteration 191/1000 | Loss: 0.00002654
Iteration 192/1000 | Loss: 0.00002654
Iteration 193/1000 | Loss: 0.00002654
Iteration 194/1000 | Loss: 0.00002653
Iteration 195/1000 | Loss: 0.00002652
Iteration 196/1000 | Loss: 0.00002652
Iteration 197/1000 | Loss: 0.00002652
Iteration 198/1000 | Loss: 0.00002652
Iteration 199/1000 | Loss: 0.00002652
Iteration 200/1000 | Loss: 0.00002652
Iteration 201/1000 | Loss: 0.00002652
Iteration 202/1000 | Loss: 0.00002652
Iteration 203/1000 | Loss: 0.00002652
Iteration 204/1000 | Loss: 0.00002651
Iteration 205/1000 | Loss: 0.00002651
Iteration 206/1000 | Loss: 0.00002651
Iteration 207/1000 | Loss: 0.00002651
Iteration 208/1000 | Loss: 0.00002651
Iteration 209/1000 | Loss: 0.00002651
Iteration 210/1000 | Loss: 0.00002651
Iteration 211/1000 | Loss: 0.00002651
Iteration 212/1000 | Loss: 0.00002651
Iteration 213/1000 | Loss: 0.00002650
Iteration 214/1000 | Loss: 0.00002650
Iteration 215/1000 | Loss: 0.00002650
Iteration 216/1000 | Loss: 0.00002650
Iteration 217/1000 | Loss: 0.00002650
Iteration 218/1000 | Loss: 0.00002650
Iteration 219/1000 | Loss: 0.00002650
Iteration 220/1000 | Loss: 0.00002650
Iteration 221/1000 | Loss: 0.00002650
Iteration 222/1000 | Loss: 0.00002649
Iteration 223/1000 | Loss: 0.00002649
Iteration 224/1000 | Loss: 0.00002649
Iteration 225/1000 | Loss: 0.00002649
Iteration 226/1000 | Loss: 0.00002649
Iteration 227/1000 | Loss: 0.00002649
Iteration 228/1000 | Loss: 0.00002649
Iteration 229/1000 | Loss: 0.00002649
Iteration 230/1000 | Loss: 0.00002649
Iteration 231/1000 | Loss: 0.00002649
Iteration 232/1000 | Loss: 0.00002648
Iteration 233/1000 | Loss: 0.00002648
Iteration 234/1000 | Loss: 0.00002648
Iteration 235/1000 | Loss: 0.00002648
Iteration 236/1000 | Loss: 0.00002648
Iteration 237/1000 | Loss: 0.00002648
Iteration 238/1000 | Loss: 0.00002647
Iteration 239/1000 | Loss: 0.00002647
Iteration 240/1000 | Loss: 0.00002647
Iteration 241/1000 | Loss: 0.00002647
Iteration 242/1000 | Loss: 0.00002647
Iteration 243/1000 | Loss: 0.00002647
Iteration 244/1000 | Loss: 0.00002647
Iteration 245/1000 | Loss: 0.00002647
Iteration 246/1000 | Loss: 0.00002647
Iteration 247/1000 | Loss: 0.00002646
Iteration 248/1000 | Loss: 0.00002646
Iteration 249/1000 | Loss: 0.00002646
Iteration 250/1000 | Loss: 0.00002646
Iteration 251/1000 | Loss: 0.00002646
Iteration 252/1000 | Loss: 0.00002646
Iteration 253/1000 | Loss: 0.00002646
Iteration 254/1000 | Loss: 0.00002646
Iteration 255/1000 | Loss: 0.00002646
Iteration 256/1000 | Loss: 0.00002646
Iteration 257/1000 | Loss: 0.00002646
Iteration 258/1000 | Loss: 0.00002646
Iteration 259/1000 | Loss: 0.00002646
Iteration 260/1000 | Loss: 0.00002646
Iteration 261/1000 | Loss: 0.00002646
Iteration 262/1000 | Loss: 0.00002646
Iteration 263/1000 | Loss: 0.00002646
Iteration 264/1000 | Loss: 0.00002646
Iteration 265/1000 | Loss: 0.00002646
Iteration 266/1000 | Loss: 0.00002646
Iteration 267/1000 | Loss: 0.00002646
Iteration 268/1000 | Loss: 0.00002646
Iteration 269/1000 | Loss: 0.00002646
Iteration 270/1000 | Loss: 0.00002646
Iteration 271/1000 | Loss: 0.00002646
Iteration 272/1000 | Loss: 0.00002646
Iteration 273/1000 | Loss: 0.00002646
Iteration 274/1000 | Loss: 0.00002646
Iteration 275/1000 | Loss: 0.00002646
Iteration 276/1000 | Loss: 0.00002646
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 276. Stopping optimization.
Last 5 losses: [2.645777931320481e-05, 2.645777931320481e-05, 2.645777931320481e-05, 2.645777931320481e-05, 2.645777931320481e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.645777931320481e-05

Optimization complete. Final v2v error: 3.698035478591919 mm

Highest mean error: 12.777837753295898 mm for frame 95

Lowest mean error: 2.8185553550720215 mm for frame 201

Saving results

Total time: 243.86434078216553
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_025/1066/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_025/1066.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_025/1066
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00372454
Iteration 2/25 | Loss: 0.00119664
Iteration 3/25 | Loss: 0.00114779
Iteration 4/25 | Loss: 0.00113623
Iteration 5/25 | Loss: 0.00113271
Iteration 6/25 | Loss: 0.00113195
Iteration 7/25 | Loss: 0.00113195
Iteration 8/25 | Loss: 0.00113195
Iteration 9/25 | Loss: 0.00113195
Iteration 10/25 | Loss: 0.00113195
Iteration 11/25 | Loss: 0.00113195
Iteration 12/25 | Loss: 0.00113195
Iteration 13/25 | Loss: 0.00113195
Iteration 14/25 | Loss: 0.00113195
Iteration 15/25 | Loss: 0.00113195
Iteration 16/25 | Loss: 0.00113195
Iteration 17/25 | Loss: 0.00113195
Iteration 18/25 | Loss: 0.00113195
Iteration 19/25 | Loss: 0.00113195
Iteration 20/25 | Loss: 0.00113195
Iteration 21/25 | Loss: 0.00113195
Iteration 22/25 | Loss: 0.00113195
Iteration 23/25 | Loss: 0.00113195
Iteration 24/25 | Loss: 0.00113195
Iteration 25/25 | Loss: 0.00113195

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38278782
Iteration 2/25 | Loss: 0.00099405
Iteration 3/25 | Loss: 0.00099404
Iteration 4/25 | Loss: 0.00099404
Iteration 5/25 | Loss: 0.00099404
Iteration 6/25 | Loss: 0.00099404
Iteration 7/25 | Loss: 0.00099404
Iteration 8/25 | Loss: 0.00099404
Iteration 9/25 | Loss: 0.00099404
Iteration 10/25 | Loss: 0.00099404
Iteration 11/25 | Loss: 0.00099404
Iteration 12/25 | Loss: 0.00099404
Iteration 13/25 | Loss: 0.00099404
Iteration 14/25 | Loss: 0.00099404
Iteration 15/25 | Loss: 0.00099404
Iteration 16/25 | Loss: 0.00099404
Iteration 17/25 | Loss: 0.00099404
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0009940421441569924, 0.0009940421441569924, 0.0009940421441569924, 0.0009940421441569924, 0.0009940421441569924]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009940421441569924

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00099404
Iteration 2/1000 | Loss: 0.00002654
Iteration 3/1000 | Loss: 0.00001801
Iteration 4/1000 | Loss: 0.00001579
Iteration 5/1000 | Loss: 0.00001518
Iteration 6/1000 | Loss: 0.00001483
Iteration 7/1000 | Loss: 0.00001457
Iteration 8/1000 | Loss: 0.00001423
Iteration 9/1000 | Loss: 0.00001404
Iteration 10/1000 | Loss: 0.00001381
Iteration 11/1000 | Loss: 0.00001378
Iteration 12/1000 | Loss: 0.00001374
Iteration 13/1000 | Loss: 0.00001368
Iteration 14/1000 | Loss: 0.00001368
Iteration 15/1000 | Loss: 0.00001367
Iteration 16/1000 | Loss: 0.00001366
Iteration 17/1000 | Loss: 0.00001366
Iteration 18/1000 | Loss: 0.00001365
Iteration 19/1000 | Loss: 0.00001362
Iteration 20/1000 | Loss: 0.00001360
Iteration 21/1000 | Loss: 0.00001355
Iteration 22/1000 | Loss: 0.00001354
Iteration 23/1000 | Loss: 0.00001354
Iteration 24/1000 | Loss: 0.00001353
Iteration 25/1000 | Loss: 0.00001352
Iteration 26/1000 | Loss: 0.00001352
Iteration 27/1000 | Loss: 0.00001352
Iteration 28/1000 | Loss: 0.00001351
Iteration 29/1000 | Loss: 0.00001351
Iteration 30/1000 | Loss: 0.00001350
Iteration 31/1000 | Loss: 0.00001350
Iteration 32/1000 | Loss: 0.00001350
Iteration 33/1000 | Loss: 0.00001350
Iteration 34/1000 | Loss: 0.00001350
Iteration 35/1000 | Loss: 0.00001350
Iteration 36/1000 | Loss: 0.00001350
Iteration 37/1000 | Loss: 0.00001350
Iteration 38/1000 | Loss: 0.00001350
Iteration 39/1000 | Loss: 0.00001350
Iteration 40/1000 | Loss: 0.00001350
Iteration 41/1000 | Loss: 0.00001350
Iteration 42/1000 | Loss: 0.00001350
Iteration 43/1000 | Loss: 0.00001349
Iteration 44/1000 | Loss: 0.00001349
Iteration 45/1000 | Loss: 0.00001349
Iteration 46/1000 | Loss: 0.00001349
Iteration 47/1000 | Loss: 0.00001348
Iteration 48/1000 | Loss: 0.00001348
Iteration 49/1000 | Loss: 0.00001348
Iteration 50/1000 | Loss: 0.00001348
Iteration 51/1000 | Loss: 0.00001348
Iteration 52/1000 | Loss: 0.00001348
Iteration 53/1000 | Loss: 0.00001348
Iteration 54/1000 | Loss: 0.00001348
Iteration 55/1000 | Loss: 0.00001348
Iteration 56/1000 | Loss: 0.00001348
Iteration 57/1000 | Loss: 0.00001348
Iteration 58/1000 | Loss: 0.00001348
Iteration 59/1000 | Loss: 0.00001348
Iteration 60/1000 | Loss: 0.00001347
Iteration 61/1000 | Loss: 0.00001347
Iteration 62/1000 | Loss: 0.00001347
Iteration 63/1000 | Loss: 0.00001347
Iteration 64/1000 | Loss: 0.00001347
Iteration 65/1000 | Loss: 0.00001347
Iteration 66/1000 | Loss: 0.00001347
Iteration 67/1000 | Loss: 0.00001347
Iteration 68/1000 | Loss: 0.00001346
Iteration 69/1000 | Loss: 0.00001346
Iteration 70/1000 | Loss: 0.00001346
Iteration 71/1000 | Loss: 0.00001346
Iteration 72/1000 | Loss: 0.00001346
Iteration 73/1000 | Loss: 0.00001346
Iteration 74/1000 | Loss: 0.00001346
Iteration 75/1000 | Loss: 0.00001346
Iteration 76/1000 | Loss: 0.00001346
Iteration 77/1000 | Loss: 0.00001346
Iteration 78/1000 | Loss: 0.00001345
Iteration 79/1000 | Loss: 0.00001345
Iteration 80/1000 | Loss: 0.00001344
Iteration 81/1000 | Loss: 0.00001344
Iteration 82/1000 | Loss: 0.00001344
Iteration 83/1000 | Loss: 0.00001343
Iteration 84/1000 | Loss: 0.00001343
Iteration 85/1000 | Loss: 0.00001343
Iteration 86/1000 | Loss: 0.00001343
Iteration 87/1000 | Loss: 0.00001343
Iteration 88/1000 | Loss: 0.00001343
Iteration 89/1000 | Loss: 0.00001343
Iteration 90/1000 | Loss: 0.00001343
Iteration 91/1000 | Loss: 0.00001343
Iteration 92/1000 | Loss: 0.00001343
Iteration 93/1000 | Loss: 0.00001343
Iteration 94/1000 | Loss: 0.00001343
Iteration 95/1000 | Loss: 0.00001343
Iteration 96/1000 | Loss: 0.00001342
Iteration 97/1000 | Loss: 0.00001342
Iteration 98/1000 | Loss: 0.00001342
Iteration 99/1000 | Loss: 0.00001342
Iteration 100/1000 | Loss: 0.00001342
Iteration 101/1000 | Loss: 0.00001342
Iteration 102/1000 | Loss: 0.00001342
Iteration 103/1000 | Loss: 0.00001342
Iteration 104/1000 | Loss: 0.00001342
Iteration 105/1000 | Loss: 0.00001342
Iteration 106/1000 | Loss: 0.00001342
Iteration 107/1000 | Loss: 0.00001342
Iteration 108/1000 | Loss: 0.00001342
Iteration 109/1000 | Loss: 0.00001342
Iteration 110/1000 | Loss: 0.00001342
Iteration 111/1000 | Loss: 0.00001341
Iteration 112/1000 | Loss: 0.00001341
Iteration 113/1000 | Loss: 0.00001341
Iteration 114/1000 | Loss: 0.00001341
Iteration 115/1000 | Loss: 0.00001341
Iteration 116/1000 | Loss: 0.00001341
Iteration 117/1000 | Loss: 0.00001341
Iteration 118/1000 | Loss: 0.00001341
Iteration 119/1000 | Loss: 0.00001341
Iteration 120/1000 | Loss: 0.00001341
Iteration 121/1000 | Loss: 0.00001341
Iteration 122/1000 | Loss: 0.00001341
Iteration 123/1000 | Loss: 0.00001341
Iteration 124/1000 | Loss: 0.00001341
Iteration 125/1000 | Loss: 0.00001341
Iteration 126/1000 | Loss: 0.00001341
Iteration 127/1000 | Loss: 0.00001341
Iteration 128/1000 | Loss: 0.00001341
Iteration 129/1000 | Loss: 0.00001341
Iteration 130/1000 | Loss: 0.00001340
Iteration 131/1000 | Loss: 0.00001340
Iteration 132/1000 | Loss: 0.00001340
Iteration 133/1000 | Loss: 0.00001340
Iteration 134/1000 | Loss: 0.00001340
Iteration 135/1000 | Loss: 0.00001340
Iteration 136/1000 | Loss: 0.00001340
Iteration 137/1000 | Loss: 0.00001340
Iteration 138/1000 | Loss: 0.00001340
Iteration 139/1000 | Loss: 0.00001340
Iteration 140/1000 | Loss: 0.00001340
Iteration 141/1000 | Loss: 0.00001340
Iteration 142/1000 | Loss: 0.00001340
Iteration 143/1000 | Loss: 0.00001340
Iteration 144/1000 | Loss: 0.00001340
Iteration 145/1000 | Loss: 0.00001340
Iteration 146/1000 | Loss: 0.00001340
Iteration 147/1000 | Loss: 0.00001340
Iteration 148/1000 | Loss: 0.00001340
Iteration 149/1000 | Loss: 0.00001340
Iteration 150/1000 | Loss: 0.00001340
Iteration 151/1000 | Loss: 0.00001340
Iteration 152/1000 | Loss: 0.00001340
Iteration 153/1000 | Loss: 0.00001340
Iteration 154/1000 | Loss: 0.00001340
Iteration 155/1000 | Loss: 0.00001340
Iteration 156/1000 | Loss: 0.00001340
Iteration 157/1000 | Loss: 0.00001340
Iteration 158/1000 | Loss: 0.00001340
Iteration 159/1000 | Loss: 0.00001340
Iteration 160/1000 | Loss: 0.00001340
Iteration 161/1000 | Loss: 0.00001340
Iteration 162/1000 | Loss: 0.00001340
Iteration 163/1000 | Loss: 0.00001340
Iteration 164/1000 | Loss: 0.00001340
Iteration 165/1000 | Loss: 0.00001340
Iteration 166/1000 | Loss: 0.00001340
Iteration 167/1000 | Loss: 0.00001340
Iteration 168/1000 | Loss: 0.00001340
Iteration 169/1000 | Loss: 0.00001340
Iteration 170/1000 | Loss: 0.00001340
Iteration 171/1000 | Loss: 0.00001340
Iteration 172/1000 | Loss: 0.00001340
Iteration 173/1000 | Loss: 0.00001340
Iteration 174/1000 | Loss: 0.00001340
Iteration 175/1000 | Loss: 0.00001340
Iteration 176/1000 | Loss: 0.00001340
Iteration 177/1000 | Loss: 0.00001340
Iteration 178/1000 | Loss: 0.00001340
Iteration 179/1000 | Loss: 0.00001340
Iteration 180/1000 | Loss: 0.00001340
Iteration 181/1000 | Loss: 0.00001340
Iteration 182/1000 | Loss: 0.00001340
Iteration 183/1000 | Loss: 0.00001340
Iteration 184/1000 | Loss: 0.00001340
Iteration 185/1000 | Loss: 0.00001340
Iteration 186/1000 | Loss: 0.00001340
Iteration 187/1000 | Loss: 0.00001340
Iteration 188/1000 | Loss: 0.00001340
Iteration 189/1000 | Loss: 0.00001340
Iteration 190/1000 | Loss: 0.00001340
Iteration 191/1000 | Loss: 0.00001340
Iteration 192/1000 | Loss: 0.00001340
Iteration 193/1000 | Loss: 0.00001340
Iteration 194/1000 | Loss: 0.00001340
Iteration 195/1000 | Loss: 0.00001340
Iteration 196/1000 | Loss: 0.00001340
Iteration 197/1000 | Loss: 0.00001340
Iteration 198/1000 | Loss: 0.00001340
Iteration 199/1000 | Loss: 0.00001340
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 199. Stopping optimization.
Last 5 losses: [1.3402598597167525e-05, 1.3402598597167525e-05, 1.3402598597167525e-05, 1.3402598597167525e-05, 1.3402598597167525e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3402598597167525e-05

Optimization complete. Final v2v error: 3.074131727218628 mm

Highest mean error: 3.2565243244171143 mm for frame 89

Lowest mean error: 2.9001853466033936 mm for frame 23

Saving results

Total time: 35.13571882247925
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_025/1046/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_025/1046.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_025/1046
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01000122
Iteration 2/25 | Loss: 0.00210599
Iteration 3/25 | Loss: 0.00151959
Iteration 4/25 | Loss: 0.00143689
Iteration 5/25 | Loss: 0.00140949
Iteration 6/25 | Loss: 0.00140303
Iteration 7/25 | Loss: 0.00134442
Iteration 8/25 | Loss: 0.00128500
Iteration 9/25 | Loss: 0.00126475
Iteration 10/25 | Loss: 0.00124830
Iteration 11/25 | Loss: 0.00123473
Iteration 12/25 | Loss: 0.00123478
Iteration 13/25 | Loss: 0.00122355
Iteration 14/25 | Loss: 0.00121347
Iteration 15/25 | Loss: 0.00120784
Iteration 16/25 | Loss: 0.00120819
Iteration 17/25 | Loss: 0.00120780
Iteration 18/25 | Loss: 0.00119454
Iteration 19/25 | Loss: 0.00118602
Iteration 20/25 | Loss: 0.00118318
Iteration 21/25 | Loss: 0.00118233
Iteration 22/25 | Loss: 0.00118216
Iteration 23/25 | Loss: 0.00118201
Iteration 24/25 | Loss: 0.00118181
Iteration 25/25 | Loss: 0.00118086

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37731957
Iteration 2/25 | Loss: 0.00093620
Iteration 3/25 | Loss: 0.00093619
Iteration 4/25 | Loss: 0.00091250
Iteration 5/25 | Loss: 0.00091250
Iteration 6/25 | Loss: 0.00091250
Iteration 7/25 | Loss: 0.00091250
Iteration 8/25 | Loss: 0.00091250
Iteration 9/25 | Loss: 0.00091250
Iteration 10/25 | Loss: 0.00091250
Iteration 11/25 | Loss: 0.00091250
Iteration 12/25 | Loss: 0.00091250
Iteration 13/25 | Loss: 0.00091250
Iteration 14/25 | Loss: 0.00091250
Iteration 15/25 | Loss: 0.00091250
Iteration 16/25 | Loss: 0.00091250
Iteration 17/25 | Loss: 0.00091250
Iteration 18/25 | Loss: 0.00091250
Iteration 19/25 | Loss: 0.00091250
Iteration 20/25 | Loss: 0.00091250
Iteration 21/25 | Loss: 0.00091250
Iteration 22/25 | Loss: 0.00091250
Iteration 23/25 | Loss: 0.00091250
Iteration 24/25 | Loss: 0.00091250
Iteration 25/25 | Loss: 0.00091250

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00091250
Iteration 2/1000 | Loss: 0.00007406
Iteration 3/1000 | Loss: 0.00003711
Iteration 4/1000 | Loss: 0.00003968
Iteration 5/1000 | Loss: 0.00002672
Iteration 6/1000 | Loss: 0.00002547
Iteration 7/1000 | Loss: 0.00011806
Iteration 8/1000 | Loss: 0.00061421
Iteration 9/1000 | Loss: 0.00011862
Iteration 10/1000 | Loss: 0.00009451
Iteration 11/1000 | Loss: 0.00004405
Iteration 12/1000 | Loss: 0.00002351
Iteration 13/1000 | Loss: 0.00002157
Iteration 14/1000 | Loss: 0.00004128
Iteration 15/1000 | Loss: 0.00002166
Iteration 16/1000 | Loss: 0.00003333
Iteration 17/1000 | Loss: 0.00003114
Iteration 18/1000 | Loss: 0.00002203
Iteration 19/1000 | Loss: 0.00001917
Iteration 20/1000 | Loss: 0.00013444
Iteration 21/1000 | Loss: 0.00006851
Iteration 22/1000 | Loss: 0.00001860
Iteration 23/1000 | Loss: 0.00001740
Iteration 24/1000 | Loss: 0.00002994
Iteration 25/1000 | Loss: 0.00001629
Iteration 26/1000 | Loss: 0.00002510
Iteration 27/1000 | Loss: 0.00001551
Iteration 28/1000 | Loss: 0.00001520
Iteration 29/1000 | Loss: 0.00001509
Iteration 30/1000 | Loss: 0.00001504
Iteration 31/1000 | Loss: 0.00001497
Iteration 32/1000 | Loss: 0.00001494
Iteration 33/1000 | Loss: 0.00001494
Iteration 34/1000 | Loss: 0.00001494
Iteration 35/1000 | Loss: 0.00001494
Iteration 36/1000 | Loss: 0.00001494
Iteration 37/1000 | Loss: 0.00001494
Iteration 38/1000 | Loss: 0.00001494
Iteration 39/1000 | Loss: 0.00001494
Iteration 40/1000 | Loss: 0.00001494
Iteration 41/1000 | Loss: 0.00001494
Iteration 42/1000 | Loss: 0.00001494
Iteration 43/1000 | Loss: 0.00001493
Iteration 44/1000 | Loss: 0.00001493
Iteration 45/1000 | Loss: 0.00001492
Iteration 46/1000 | Loss: 0.00001492
Iteration 47/1000 | Loss: 0.00001491
Iteration 48/1000 | Loss: 0.00001491
Iteration 49/1000 | Loss: 0.00001491
Iteration 50/1000 | Loss: 0.00001490
Iteration 51/1000 | Loss: 0.00001490
Iteration 52/1000 | Loss: 0.00001489
Iteration 53/1000 | Loss: 0.00001489
Iteration 54/1000 | Loss: 0.00001489
Iteration 55/1000 | Loss: 0.00001488
Iteration 56/1000 | Loss: 0.00001488
Iteration 57/1000 | Loss: 0.00001488
Iteration 58/1000 | Loss: 0.00001487
Iteration 59/1000 | Loss: 0.00001487
Iteration 60/1000 | Loss: 0.00001487
Iteration 61/1000 | Loss: 0.00001487
Iteration 62/1000 | Loss: 0.00001487
Iteration 63/1000 | Loss: 0.00001487
Iteration 64/1000 | Loss: 0.00001487
Iteration 65/1000 | Loss: 0.00001486
Iteration 66/1000 | Loss: 0.00001486
Iteration 67/1000 | Loss: 0.00001486
Iteration 68/1000 | Loss: 0.00001486
Iteration 69/1000 | Loss: 0.00001486
Iteration 70/1000 | Loss: 0.00001486
Iteration 71/1000 | Loss: 0.00001486
Iteration 72/1000 | Loss: 0.00001486
Iteration 73/1000 | Loss: 0.00001486
Iteration 74/1000 | Loss: 0.00001485
Iteration 75/1000 | Loss: 0.00001485
Iteration 76/1000 | Loss: 0.00001485
Iteration 77/1000 | Loss: 0.00001484
Iteration 78/1000 | Loss: 0.00001484
Iteration 79/1000 | Loss: 0.00001484
Iteration 80/1000 | Loss: 0.00001484
Iteration 81/1000 | Loss: 0.00001484
Iteration 82/1000 | Loss: 0.00001483
Iteration 83/1000 | Loss: 0.00001483
Iteration 84/1000 | Loss: 0.00001483
Iteration 85/1000 | Loss: 0.00001482
Iteration 86/1000 | Loss: 0.00001482
Iteration 87/1000 | Loss: 0.00001482
Iteration 88/1000 | Loss: 0.00001481
Iteration 89/1000 | Loss: 0.00001481
Iteration 90/1000 | Loss: 0.00001481
Iteration 91/1000 | Loss: 0.00001481
Iteration 92/1000 | Loss: 0.00001481
Iteration 93/1000 | Loss: 0.00001481
Iteration 94/1000 | Loss: 0.00001481
Iteration 95/1000 | Loss: 0.00001481
Iteration 96/1000 | Loss: 0.00001480
Iteration 97/1000 | Loss: 0.00001480
Iteration 98/1000 | Loss: 0.00001480
Iteration 99/1000 | Loss: 0.00001480
Iteration 100/1000 | Loss: 0.00001480
Iteration 101/1000 | Loss: 0.00001480
Iteration 102/1000 | Loss: 0.00001480
Iteration 103/1000 | Loss: 0.00001480
Iteration 104/1000 | Loss: 0.00001480
Iteration 105/1000 | Loss: 0.00001480
Iteration 106/1000 | Loss: 0.00001479
Iteration 107/1000 | Loss: 0.00001479
Iteration 108/1000 | Loss: 0.00001479
Iteration 109/1000 | Loss: 0.00001479
Iteration 110/1000 | Loss: 0.00001479
Iteration 111/1000 | Loss: 0.00001479
Iteration 112/1000 | Loss: 0.00001478
Iteration 113/1000 | Loss: 0.00001478
Iteration 114/1000 | Loss: 0.00001478
Iteration 115/1000 | Loss: 0.00001478
Iteration 116/1000 | Loss: 0.00001478
Iteration 117/1000 | Loss: 0.00001478
Iteration 118/1000 | Loss: 0.00001477
Iteration 119/1000 | Loss: 0.00001477
Iteration 120/1000 | Loss: 0.00001477
Iteration 121/1000 | Loss: 0.00001477
Iteration 122/1000 | Loss: 0.00001477
Iteration 123/1000 | Loss: 0.00001477
Iteration 124/1000 | Loss: 0.00001477
Iteration 125/1000 | Loss: 0.00001477
Iteration 126/1000 | Loss: 0.00001476
Iteration 127/1000 | Loss: 0.00001476
Iteration 128/1000 | Loss: 0.00001476
Iteration 129/1000 | Loss: 0.00001476
Iteration 130/1000 | Loss: 0.00001476
Iteration 131/1000 | Loss: 0.00001476
Iteration 132/1000 | Loss: 0.00001476
Iteration 133/1000 | Loss: 0.00001476
Iteration 134/1000 | Loss: 0.00001476
Iteration 135/1000 | Loss: 0.00001476
Iteration 136/1000 | Loss: 0.00001475
Iteration 137/1000 | Loss: 0.00001475
Iteration 138/1000 | Loss: 0.00001474
Iteration 139/1000 | Loss: 0.00001474
Iteration 140/1000 | Loss: 0.00001474
Iteration 141/1000 | Loss: 0.00001474
Iteration 142/1000 | Loss: 0.00001474
Iteration 143/1000 | Loss: 0.00001474
Iteration 144/1000 | Loss: 0.00001474
Iteration 145/1000 | Loss: 0.00001474
Iteration 146/1000 | Loss: 0.00001474
Iteration 147/1000 | Loss: 0.00001474
Iteration 148/1000 | Loss: 0.00001473
Iteration 149/1000 | Loss: 0.00001473
Iteration 150/1000 | Loss: 0.00001473
Iteration 151/1000 | Loss: 0.00001473
Iteration 152/1000 | Loss: 0.00001473
Iteration 153/1000 | Loss: 0.00001473
Iteration 154/1000 | Loss: 0.00001473
Iteration 155/1000 | Loss: 0.00001473
Iteration 156/1000 | Loss: 0.00001472
Iteration 157/1000 | Loss: 0.00001472
Iteration 158/1000 | Loss: 0.00001472
Iteration 159/1000 | Loss: 0.00001472
Iteration 160/1000 | Loss: 0.00001471
Iteration 161/1000 | Loss: 0.00001471
Iteration 162/1000 | Loss: 0.00001471
Iteration 163/1000 | Loss: 0.00001471
Iteration 164/1000 | Loss: 0.00001471
Iteration 165/1000 | Loss: 0.00001471
Iteration 166/1000 | Loss: 0.00001471
Iteration 167/1000 | Loss: 0.00001471
Iteration 168/1000 | Loss: 0.00001471
Iteration 169/1000 | Loss: 0.00001470
Iteration 170/1000 | Loss: 0.00001470
Iteration 171/1000 | Loss: 0.00001470
Iteration 172/1000 | Loss: 0.00001470
Iteration 173/1000 | Loss: 0.00001470
Iteration 174/1000 | Loss: 0.00001470
Iteration 175/1000 | Loss: 0.00001470
Iteration 176/1000 | Loss: 0.00001470
Iteration 177/1000 | Loss: 0.00001470
Iteration 178/1000 | Loss: 0.00001470
Iteration 179/1000 | Loss: 0.00001470
Iteration 180/1000 | Loss: 0.00001469
Iteration 181/1000 | Loss: 0.00001469
Iteration 182/1000 | Loss: 0.00001469
Iteration 183/1000 | Loss: 0.00001469
Iteration 184/1000 | Loss: 0.00001469
Iteration 185/1000 | Loss: 0.00001469
Iteration 186/1000 | Loss: 0.00001469
Iteration 187/1000 | Loss: 0.00001469
Iteration 188/1000 | Loss: 0.00001469
Iteration 189/1000 | Loss: 0.00001468
Iteration 190/1000 | Loss: 0.00001468
Iteration 191/1000 | Loss: 0.00001468
Iteration 192/1000 | Loss: 0.00001468
Iteration 193/1000 | Loss: 0.00001468
Iteration 194/1000 | Loss: 0.00001468
Iteration 195/1000 | Loss: 0.00001468
Iteration 196/1000 | Loss: 0.00001468
Iteration 197/1000 | Loss: 0.00001468
Iteration 198/1000 | Loss: 0.00001468
Iteration 199/1000 | Loss: 0.00001468
Iteration 200/1000 | Loss: 0.00001468
Iteration 201/1000 | Loss: 0.00001468
Iteration 202/1000 | Loss: 0.00001468
Iteration 203/1000 | Loss: 0.00001468
Iteration 204/1000 | Loss: 0.00001468
Iteration 205/1000 | Loss: 0.00001467
Iteration 206/1000 | Loss: 0.00001467
Iteration 207/1000 | Loss: 0.00001467
Iteration 208/1000 | Loss: 0.00001467
Iteration 209/1000 | Loss: 0.00001467
Iteration 210/1000 | Loss: 0.00001467
Iteration 211/1000 | Loss: 0.00001467
Iteration 212/1000 | Loss: 0.00001467
Iteration 213/1000 | Loss: 0.00001467
Iteration 214/1000 | Loss: 0.00001467
Iteration 215/1000 | Loss: 0.00001467
Iteration 216/1000 | Loss: 0.00001467
Iteration 217/1000 | Loss: 0.00001467
Iteration 218/1000 | Loss: 0.00001467
Iteration 219/1000 | Loss: 0.00001467
Iteration 220/1000 | Loss: 0.00001467
Iteration 221/1000 | Loss: 0.00001467
Iteration 222/1000 | Loss: 0.00001467
Iteration 223/1000 | Loss: 0.00001467
Iteration 224/1000 | Loss: 0.00001467
Iteration 225/1000 | Loss: 0.00001467
Iteration 226/1000 | Loss: 0.00001467
Iteration 227/1000 | Loss: 0.00001467
Iteration 228/1000 | Loss: 0.00001467
Iteration 229/1000 | Loss: 0.00001467
Iteration 230/1000 | Loss: 0.00001467
Iteration 231/1000 | Loss: 0.00001467
Iteration 232/1000 | Loss: 0.00001467
Iteration 233/1000 | Loss: 0.00001467
Iteration 234/1000 | Loss: 0.00001467
Iteration 235/1000 | Loss: 0.00001467
Iteration 236/1000 | Loss: 0.00001467
Iteration 237/1000 | Loss: 0.00001467
Iteration 238/1000 | Loss: 0.00001467
Iteration 239/1000 | Loss: 0.00001467
Iteration 240/1000 | Loss: 0.00001467
Iteration 241/1000 | Loss: 0.00001467
Iteration 242/1000 | Loss: 0.00001467
Iteration 243/1000 | Loss: 0.00001467
Iteration 244/1000 | Loss: 0.00001467
Iteration 245/1000 | Loss: 0.00001467
Iteration 246/1000 | Loss: 0.00001467
Iteration 247/1000 | Loss: 0.00001467
Iteration 248/1000 | Loss: 0.00001467
Iteration 249/1000 | Loss: 0.00001467
Iteration 250/1000 | Loss: 0.00001467
Iteration 251/1000 | Loss: 0.00001467
Iteration 252/1000 | Loss: 0.00001467
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 252. Stopping optimization.
Last 5 losses: [1.4665704838989768e-05, 1.4665704838989768e-05, 1.4665704838989768e-05, 1.4665704838989768e-05, 1.4665704838989768e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4665704838989768e-05

Optimization complete. Final v2v error: 3.0552759170532227 mm

Highest mean error: 5.479924201965332 mm for frame 42

Lowest mean error: 2.6485931873321533 mm for frame 12

Saving results

Total time: 100.79341292381287
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_025/1087/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_025/1087.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_025/1087
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01072419
Iteration 2/25 | Loss: 0.01072419
Iteration 3/25 | Loss: 0.00210717
Iteration 4/25 | Loss: 0.00143236
Iteration 5/25 | Loss: 0.00138278
Iteration 6/25 | Loss: 0.00137255
Iteration 7/25 | Loss: 0.00136805
Iteration 8/25 | Loss: 0.00134345
Iteration 9/25 | Loss: 0.00133057
Iteration 10/25 | Loss: 0.00132085
Iteration 11/25 | Loss: 0.00131495
Iteration 12/25 | Loss: 0.00131285
Iteration 13/25 | Loss: 0.00131205
Iteration 14/25 | Loss: 0.00130642
Iteration 15/25 | Loss: 0.00130008
Iteration 16/25 | Loss: 0.00129656
Iteration 17/25 | Loss: 0.00129471
Iteration 18/25 | Loss: 0.00129481
Iteration 19/25 | Loss: 0.00129434
Iteration 20/25 | Loss: 0.00129371
Iteration 21/25 | Loss: 0.00129127
Iteration 22/25 | Loss: 0.00129282
Iteration 23/25 | Loss: 0.00129471
Iteration 24/25 | Loss: 0.00129205
Iteration 25/25 | Loss: 0.00129355

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.70449018
Iteration 2/25 | Loss: 0.00126325
Iteration 3/25 | Loss: 0.00126325
Iteration 4/25 | Loss: 0.00126325
Iteration 5/25 | Loss: 0.00126325
Iteration 6/25 | Loss: 0.00126324
Iteration 7/25 | Loss: 0.00126324
Iteration 8/25 | Loss: 0.00126324
Iteration 9/25 | Loss: 0.00126324
Iteration 10/25 | Loss: 0.00126324
Iteration 11/25 | Loss: 0.00126324
Iteration 12/25 | Loss: 0.00126324
Iteration 13/25 | Loss: 0.00126324
Iteration 14/25 | Loss: 0.00126324
Iteration 15/25 | Loss: 0.00126324
Iteration 16/25 | Loss: 0.00126324
Iteration 17/25 | Loss: 0.00126324
Iteration 18/25 | Loss: 0.00126324
Iteration 19/25 | Loss: 0.00126324
Iteration 20/25 | Loss: 0.00126324
Iteration 21/25 | Loss: 0.00126324
Iteration 22/25 | Loss: 0.00126324
Iteration 23/25 | Loss: 0.00126324
Iteration 24/25 | Loss: 0.00126324
Iteration 25/25 | Loss: 0.00126324
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0012632423313334584, 0.0012632423313334584, 0.0012632423313334584, 0.0012632423313334584, 0.0012632423313334584]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012632423313334584

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00126324
Iteration 2/1000 | Loss: 0.00005321
Iteration 3/1000 | Loss: 0.00010747
Iteration 4/1000 | Loss: 0.00005484
Iteration 5/1000 | Loss: 0.00004244
Iteration 6/1000 | Loss: 0.00003832
Iteration 7/1000 | Loss: 0.00010845
Iteration 8/1000 | Loss: 0.00005437
Iteration 9/1000 | Loss: 0.00003944
Iteration 10/1000 | Loss: 0.00005220
Iteration 11/1000 | Loss: 0.00003989
Iteration 12/1000 | Loss: 0.00004614
Iteration 13/1000 | Loss: 0.00003165
Iteration 14/1000 | Loss: 0.00003090
Iteration 15/1000 | Loss: 0.00004627
Iteration 16/1000 | Loss: 0.00003702
Iteration 17/1000 | Loss: 0.00004683
Iteration 18/1000 | Loss: 0.00004406
Iteration 19/1000 | Loss: 0.00005185
Iteration 20/1000 | Loss: 0.00004668
Iteration 21/1000 | Loss: 0.00004759
Iteration 22/1000 | Loss: 0.00004489
Iteration 23/1000 | Loss: 0.00004793
Iteration 24/1000 | Loss: 0.00003776
Iteration 25/1000 | Loss: 0.00004250
Iteration 26/1000 | Loss: 0.00004818
Iteration 27/1000 | Loss: 0.00005600
Iteration 28/1000 | Loss: 0.00004710
Iteration 29/1000 | Loss: 0.00006689
Iteration 30/1000 | Loss: 0.00004208
Iteration 31/1000 | Loss: 0.00003974
Iteration 32/1000 | Loss: 0.00005177
Iteration 33/1000 | Loss: 0.00004239
Iteration 34/1000 | Loss: 0.00004856
Iteration 35/1000 | Loss: 0.00004448
Iteration 36/1000 | Loss: 0.00004633
Iteration 37/1000 | Loss: 0.00004380
Iteration 38/1000 | Loss: 0.00004654
Iteration 39/1000 | Loss: 0.00004071
Iteration 40/1000 | Loss: 0.00004517
Iteration 41/1000 | Loss: 0.00004061
Iteration 42/1000 | Loss: 0.00004475
Iteration 43/1000 | Loss: 0.00004039
Iteration 44/1000 | Loss: 0.00004511
Iteration 45/1000 | Loss: 0.00004540
Iteration 46/1000 | Loss: 0.00004456
Iteration 47/1000 | Loss: 0.00004129
Iteration 48/1000 | Loss: 0.00005026
Iteration 49/1000 | Loss: 0.00004714
Iteration 50/1000 | Loss: 0.00004621
Iteration 51/1000 | Loss: 0.00004384
Iteration 52/1000 | Loss: 0.00004907
Iteration 53/1000 | Loss: 0.00004264
Iteration 54/1000 | Loss: 0.00005486
Iteration 55/1000 | Loss: 0.00004329
Iteration 56/1000 | Loss: 0.00004631
Iteration 57/1000 | Loss: 0.00003995
Iteration 58/1000 | Loss: 0.00004745
Iteration 59/1000 | Loss: 0.00004238
Iteration 60/1000 | Loss: 0.00007405
Iteration 61/1000 | Loss: 0.00005742
Iteration 62/1000 | Loss: 0.00002852
Iteration 63/1000 | Loss: 0.00003545
Iteration 64/1000 | Loss: 0.00002489
Iteration 65/1000 | Loss: 0.00002347
Iteration 66/1000 | Loss: 0.00002234
Iteration 67/1000 | Loss: 0.00002201
Iteration 68/1000 | Loss: 0.00003457
Iteration 69/1000 | Loss: 0.00002172
Iteration 70/1000 | Loss: 0.00002163
Iteration 71/1000 | Loss: 0.00002143
Iteration 72/1000 | Loss: 0.00002127
Iteration 73/1000 | Loss: 0.00002116
Iteration 74/1000 | Loss: 0.00002115
Iteration 75/1000 | Loss: 0.00002113
Iteration 76/1000 | Loss: 0.00002113
Iteration 77/1000 | Loss: 0.00002113
Iteration 78/1000 | Loss: 0.00002111
Iteration 79/1000 | Loss: 0.00002111
Iteration 80/1000 | Loss: 0.00002110
Iteration 81/1000 | Loss: 0.00002110
Iteration 82/1000 | Loss: 0.00002104
Iteration 83/1000 | Loss: 0.00002101
Iteration 84/1000 | Loss: 0.00002100
Iteration 85/1000 | Loss: 0.00005354
Iteration 86/1000 | Loss: 0.00002098
Iteration 87/1000 | Loss: 0.00002092
Iteration 88/1000 | Loss: 0.00002091
Iteration 89/1000 | Loss: 0.00002091
Iteration 90/1000 | Loss: 0.00002091
Iteration 91/1000 | Loss: 0.00002091
Iteration 92/1000 | Loss: 0.00002091
Iteration 93/1000 | Loss: 0.00002090
Iteration 94/1000 | Loss: 0.00002090
Iteration 95/1000 | Loss: 0.00002089
Iteration 96/1000 | Loss: 0.00002089
Iteration 97/1000 | Loss: 0.00002089
Iteration 98/1000 | Loss: 0.00002088
Iteration 99/1000 | Loss: 0.00002088
Iteration 100/1000 | Loss: 0.00002088
Iteration 101/1000 | Loss: 0.00002088
Iteration 102/1000 | Loss: 0.00002088
Iteration 103/1000 | Loss: 0.00002087
Iteration 104/1000 | Loss: 0.00002087
Iteration 105/1000 | Loss: 0.00002087
Iteration 106/1000 | Loss: 0.00002086
Iteration 107/1000 | Loss: 0.00002086
Iteration 108/1000 | Loss: 0.00002086
Iteration 109/1000 | Loss: 0.00002086
Iteration 110/1000 | Loss: 0.00002086
Iteration 111/1000 | Loss: 0.00002086
Iteration 112/1000 | Loss: 0.00002086
Iteration 113/1000 | Loss: 0.00002086
Iteration 114/1000 | Loss: 0.00002086
Iteration 115/1000 | Loss: 0.00002086
Iteration 116/1000 | Loss: 0.00002086
Iteration 117/1000 | Loss: 0.00002086
Iteration 118/1000 | Loss: 0.00002086
Iteration 119/1000 | Loss: 0.00002086
Iteration 120/1000 | Loss: 0.00002086
Iteration 121/1000 | Loss: 0.00002086
Iteration 122/1000 | Loss: 0.00002086
Iteration 123/1000 | Loss: 0.00002086
Iteration 124/1000 | Loss: 0.00002086
Iteration 125/1000 | Loss: 0.00002086
Iteration 126/1000 | Loss: 0.00002086
Iteration 127/1000 | Loss: 0.00002086
Iteration 128/1000 | Loss: 0.00002086
Iteration 129/1000 | Loss: 0.00002086
Iteration 130/1000 | Loss: 0.00002086
Iteration 131/1000 | Loss: 0.00002086
Iteration 132/1000 | Loss: 0.00002086
Iteration 133/1000 | Loss: 0.00002086
Iteration 134/1000 | Loss: 0.00002086
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 134. Stopping optimization.
Last 5 losses: [2.0855010006926022e-05, 2.0855010006926022e-05, 2.0855010006926022e-05, 2.0855010006926022e-05, 2.0855010006926022e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0855010006926022e-05

Optimization complete. Final v2v error: 3.685293197631836 mm

Highest mean error: 4.858282089233398 mm for frame 183

Lowest mean error: 3.0619821548461914 mm for frame 1

Saving results

Total time: 178.30973982810974
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_025/1035/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_025/1035.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_025/1035
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00458482
Iteration 2/25 | Loss: 0.00130001
Iteration 3/25 | Loss: 0.00119772
Iteration 4/25 | Loss: 0.00117881
Iteration 5/25 | Loss: 0.00117330
Iteration 6/25 | Loss: 0.00117307
Iteration 7/25 | Loss: 0.00117307
Iteration 8/25 | Loss: 0.00117307
Iteration 9/25 | Loss: 0.00117307
Iteration 10/25 | Loss: 0.00117307
Iteration 11/25 | Loss: 0.00117307
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011730671394616365, 0.0011730671394616365, 0.0011730671394616365, 0.0011730671394616365, 0.0011730671394616365]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011730671394616365

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42366683
Iteration 2/25 | Loss: 0.00082255
Iteration 3/25 | Loss: 0.00082255
Iteration 4/25 | Loss: 0.00082255
Iteration 5/25 | Loss: 0.00082255
Iteration 6/25 | Loss: 0.00082255
Iteration 7/25 | Loss: 0.00082255
Iteration 8/25 | Loss: 0.00082255
Iteration 9/25 | Loss: 0.00082255
Iteration 10/25 | Loss: 0.00082254
Iteration 11/25 | Loss: 0.00082254
Iteration 12/25 | Loss: 0.00082254
Iteration 13/25 | Loss: 0.00082254
Iteration 14/25 | Loss: 0.00082254
Iteration 15/25 | Loss: 0.00082254
Iteration 16/25 | Loss: 0.00082254
Iteration 17/25 | Loss: 0.00082254
Iteration 18/25 | Loss: 0.00082254
Iteration 19/25 | Loss: 0.00082254
Iteration 20/25 | Loss: 0.00082254
Iteration 21/25 | Loss: 0.00082254
Iteration 22/25 | Loss: 0.00082254
Iteration 23/25 | Loss: 0.00082254
Iteration 24/25 | Loss: 0.00082254
Iteration 25/25 | Loss: 0.00082254

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00082254
Iteration 2/1000 | Loss: 0.00002933
Iteration 3/1000 | Loss: 0.00002293
Iteration 4/1000 | Loss: 0.00002145
Iteration 5/1000 | Loss: 0.00002073
Iteration 6/1000 | Loss: 0.00002009
Iteration 7/1000 | Loss: 0.00001964
Iteration 8/1000 | Loss: 0.00001920
Iteration 9/1000 | Loss: 0.00001877
Iteration 10/1000 | Loss: 0.00001846
Iteration 11/1000 | Loss: 0.00001834
Iteration 12/1000 | Loss: 0.00001831
Iteration 13/1000 | Loss: 0.00001804
Iteration 14/1000 | Loss: 0.00001802
Iteration 15/1000 | Loss: 0.00001795
Iteration 16/1000 | Loss: 0.00001792
Iteration 17/1000 | Loss: 0.00001791
Iteration 18/1000 | Loss: 0.00001787
Iteration 19/1000 | Loss: 0.00001786
Iteration 20/1000 | Loss: 0.00001786
Iteration 21/1000 | Loss: 0.00001777
Iteration 22/1000 | Loss: 0.00001777
Iteration 23/1000 | Loss: 0.00001776
Iteration 24/1000 | Loss: 0.00001774
Iteration 25/1000 | Loss: 0.00001772
Iteration 26/1000 | Loss: 0.00001767
Iteration 27/1000 | Loss: 0.00001764
Iteration 28/1000 | Loss: 0.00001764
Iteration 29/1000 | Loss: 0.00001763
Iteration 30/1000 | Loss: 0.00001763
Iteration 31/1000 | Loss: 0.00001762
Iteration 32/1000 | Loss: 0.00001760
Iteration 33/1000 | Loss: 0.00001757
Iteration 34/1000 | Loss: 0.00001756
Iteration 35/1000 | Loss: 0.00001756
Iteration 36/1000 | Loss: 0.00001756
Iteration 37/1000 | Loss: 0.00001756
Iteration 38/1000 | Loss: 0.00001756
Iteration 39/1000 | Loss: 0.00001756
Iteration 40/1000 | Loss: 0.00001756
Iteration 41/1000 | Loss: 0.00001756
Iteration 42/1000 | Loss: 0.00001756
Iteration 43/1000 | Loss: 0.00001756
Iteration 44/1000 | Loss: 0.00001755
Iteration 45/1000 | Loss: 0.00001755
Iteration 46/1000 | Loss: 0.00001752
Iteration 47/1000 | Loss: 0.00001752
Iteration 48/1000 | Loss: 0.00001752
Iteration 49/1000 | Loss: 0.00001751
Iteration 50/1000 | Loss: 0.00001751
Iteration 51/1000 | Loss: 0.00001750
Iteration 52/1000 | Loss: 0.00001750
Iteration 53/1000 | Loss: 0.00001749
Iteration 54/1000 | Loss: 0.00001749
Iteration 55/1000 | Loss: 0.00001747
Iteration 56/1000 | Loss: 0.00001747
Iteration 57/1000 | Loss: 0.00001747
Iteration 58/1000 | Loss: 0.00001747
Iteration 59/1000 | Loss: 0.00001747
Iteration 60/1000 | Loss: 0.00001747
Iteration 61/1000 | Loss: 0.00001747
Iteration 62/1000 | Loss: 0.00001746
Iteration 63/1000 | Loss: 0.00001746
Iteration 64/1000 | Loss: 0.00001746
Iteration 65/1000 | Loss: 0.00001745
Iteration 66/1000 | Loss: 0.00001745
Iteration 67/1000 | Loss: 0.00001744
Iteration 68/1000 | Loss: 0.00001744
Iteration 69/1000 | Loss: 0.00001743
Iteration 70/1000 | Loss: 0.00001743
Iteration 71/1000 | Loss: 0.00001743
Iteration 72/1000 | Loss: 0.00001742
Iteration 73/1000 | Loss: 0.00001742
Iteration 74/1000 | Loss: 0.00001742
Iteration 75/1000 | Loss: 0.00001742
Iteration 76/1000 | Loss: 0.00001742
Iteration 77/1000 | Loss: 0.00001742
Iteration 78/1000 | Loss: 0.00001741
Iteration 79/1000 | Loss: 0.00001741
Iteration 80/1000 | Loss: 0.00001741
Iteration 81/1000 | Loss: 0.00001741
Iteration 82/1000 | Loss: 0.00001741
Iteration 83/1000 | Loss: 0.00001741
Iteration 84/1000 | Loss: 0.00001741
Iteration 85/1000 | Loss: 0.00001740
Iteration 86/1000 | Loss: 0.00001740
Iteration 87/1000 | Loss: 0.00001739
Iteration 88/1000 | Loss: 0.00001739
Iteration 89/1000 | Loss: 0.00001739
Iteration 90/1000 | Loss: 0.00001739
Iteration 91/1000 | Loss: 0.00001739
Iteration 92/1000 | Loss: 0.00001739
Iteration 93/1000 | Loss: 0.00001739
Iteration 94/1000 | Loss: 0.00001738
Iteration 95/1000 | Loss: 0.00001738
Iteration 96/1000 | Loss: 0.00001738
Iteration 97/1000 | Loss: 0.00001738
Iteration 98/1000 | Loss: 0.00001738
Iteration 99/1000 | Loss: 0.00001738
Iteration 100/1000 | Loss: 0.00001738
Iteration 101/1000 | Loss: 0.00001737
Iteration 102/1000 | Loss: 0.00001737
Iteration 103/1000 | Loss: 0.00001737
Iteration 104/1000 | Loss: 0.00001737
Iteration 105/1000 | Loss: 0.00001736
Iteration 106/1000 | Loss: 0.00001736
Iteration 107/1000 | Loss: 0.00001736
Iteration 108/1000 | Loss: 0.00001736
Iteration 109/1000 | Loss: 0.00001736
Iteration 110/1000 | Loss: 0.00001735
Iteration 111/1000 | Loss: 0.00001735
Iteration 112/1000 | Loss: 0.00001735
Iteration 113/1000 | Loss: 0.00001735
Iteration 114/1000 | Loss: 0.00001735
Iteration 115/1000 | Loss: 0.00001735
Iteration 116/1000 | Loss: 0.00001735
Iteration 117/1000 | Loss: 0.00001735
Iteration 118/1000 | Loss: 0.00001735
Iteration 119/1000 | Loss: 0.00001735
Iteration 120/1000 | Loss: 0.00001735
Iteration 121/1000 | Loss: 0.00001734
Iteration 122/1000 | Loss: 0.00001734
Iteration 123/1000 | Loss: 0.00001734
Iteration 124/1000 | Loss: 0.00001734
Iteration 125/1000 | Loss: 0.00001734
Iteration 126/1000 | Loss: 0.00001734
Iteration 127/1000 | Loss: 0.00001734
Iteration 128/1000 | Loss: 0.00001734
Iteration 129/1000 | Loss: 0.00001734
Iteration 130/1000 | Loss: 0.00001734
Iteration 131/1000 | Loss: 0.00001734
Iteration 132/1000 | Loss: 0.00001734
Iteration 133/1000 | Loss: 0.00001734
Iteration 134/1000 | Loss: 0.00001734
Iteration 135/1000 | Loss: 0.00001734
Iteration 136/1000 | Loss: 0.00001734
Iteration 137/1000 | Loss: 0.00001734
Iteration 138/1000 | Loss: 0.00001734
Iteration 139/1000 | Loss: 0.00001734
Iteration 140/1000 | Loss: 0.00001734
Iteration 141/1000 | Loss: 0.00001734
Iteration 142/1000 | Loss: 0.00001734
Iteration 143/1000 | Loss: 0.00001734
Iteration 144/1000 | Loss: 0.00001734
Iteration 145/1000 | Loss: 0.00001734
Iteration 146/1000 | Loss: 0.00001734
Iteration 147/1000 | Loss: 0.00001734
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 147. Stopping optimization.
Last 5 losses: [1.7338052202831022e-05, 1.7338052202831022e-05, 1.7338052202831022e-05, 1.7338052202831022e-05, 1.7338052202831022e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7338052202831022e-05

Optimization complete. Final v2v error: 3.5464701652526855 mm

Highest mean error: 3.915985584259033 mm for frame 31

Lowest mean error: 3.2884364128112793 mm for frame 89

Saving results

Total time: 39.5999870300293
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_025/1093/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_025/1093.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_025/1093
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00784555
Iteration 2/25 | Loss: 0.00180260
Iteration 3/25 | Loss: 0.00128551
Iteration 4/25 | Loss: 0.00123904
Iteration 5/25 | Loss: 0.00123421
Iteration 6/25 | Loss: 0.00123309
Iteration 7/25 | Loss: 0.00123309
Iteration 8/25 | Loss: 0.00123309
Iteration 9/25 | Loss: 0.00123309
Iteration 10/25 | Loss: 0.00123309
Iteration 11/25 | Loss: 0.00123309
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001233091577887535, 0.001233091577887535, 0.001233091577887535, 0.001233091577887535, 0.001233091577887535]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001233091577887535

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37003398
Iteration 2/25 | Loss: 0.00079543
Iteration 3/25 | Loss: 0.00079542
Iteration 4/25 | Loss: 0.00079542
Iteration 5/25 | Loss: 0.00079542
Iteration 6/25 | Loss: 0.00079541
Iteration 7/25 | Loss: 0.00079541
Iteration 8/25 | Loss: 0.00079541
Iteration 9/25 | Loss: 0.00079541
Iteration 10/25 | Loss: 0.00079541
Iteration 11/25 | Loss: 0.00079541
Iteration 12/25 | Loss: 0.00079541
Iteration 13/25 | Loss: 0.00079541
Iteration 14/25 | Loss: 0.00079541
Iteration 15/25 | Loss: 0.00079541
Iteration 16/25 | Loss: 0.00079541
Iteration 17/25 | Loss: 0.00079541
Iteration 18/25 | Loss: 0.00079541
Iteration 19/25 | Loss: 0.00079541
Iteration 20/25 | Loss: 0.00079541
Iteration 21/25 | Loss: 0.00079541
Iteration 22/25 | Loss: 0.00079541
Iteration 23/25 | Loss: 0.00079541
Iteration 24/25 | Loss: 0.00079541
Iteration 25/25 | Loss: 0.00079541

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00079541
Iteration 2/1000 | Loss: 0.00004091
Iteration 3/1000 | Loss: 0.00002825
Iteration 4/1000 | Loss: 0.00002232
Iteration 5/1000 | Loss: 0.00002106
Iteration 6/1000 | Loss: 0.00002044
Iteration 7/1000 | Loss: 0.00002006
Iteration 8/1000 | Loss: 0.00001989
Iteration 9/1000 | Loss: 0.00001967
Iteration 10/1000 | Loss: 0.00001951
Iteration 11/1000 | Loss: 0.00001950
Iteration 12/1000 | Loss: 0.00001949
Iteration 13/1000 | Loss: 0.00001942
Iteration 14/1000 | Loss: 0.00001936
Iteration 15/1000 | Loss: 0.00001932
Iteration 16/1000 | Loss: 0.00001931
Iteration 17/1000 | Loss: 0.00001930
Iteration 18/1000 | Loss: 0.00001929
Iteration 19/1000 | Loss: 0.00001927
Iteration 20/1000 | Loss: 0.00001927
Iteration 21/1000 | Loss: 0.00001926
Iteration 22/1000 | Loss: 0.00001925
Iteration 23/1000 | Loss: 0.00001925
Iteration 24/1000 | Loss: 0.00001925
Iteration 25/1000 | Loss: 0.00001925
Iteration 26/1000 | Loss: 0.00001925
Iteration 27/1000 | Loss: 0.00001924
Iteration 28/1000 | Loss: 0.00001924
Iteration 29/1000 | Loss: 0.00001924
Iteration 30/1000 | Loss: 0.00001924
Iteration 31/1000 | Loss: 0.00001924
Iteration 32/1000 | Loss: 0.00001924
Iteration 33/1000 | Loss: 0.00001924
Iteration 34/1000 | Loss: 0.00001923
Iteration 35/1000 | Loss: 0.00001923
Iteration 36/1000 | Loss: 0.00001923
Iteration 37/1000 | Loss: 0.00001923
Iteration 38/1000 | Loss: 0.00001923
Iteration 39/1000 | Loss: 0.00001922
Iteration 40/1000 | Loss: 0.00001922
Iteration 41/1000 | Loss: 0.00001921
Iteration 42/1000 | Loss: 0.00001921
Iteration 43/1000 | Loss: 0.00001920
Iteration 44/1000 | Loss: 0.00001920
Iteration 45/1000 | Loss: 0.00001920
Iteration 46/1000 | Loss: 0.00001920
Iteration 47/1000 | Loss: 0.00001919
Iteration 48/1000 | Loss: 0.00001919
Iteration 49/1000 | Loss: 0.00001918
Iteration 50/1000 | Loss: 0.00001917
Iteration 51/1000 | Loss: 0.00001917
Iteration 52/1000 | Loss: 0.00001917
Iteration 53/1000 | Loss: 0.00001916
Iteration 54/1000 | Loss: 0.00001916
Iteration 55/1000 | Loss: 0.00001916
Iteration 56/1000 | Loss: 0.00001915
Iteration 57/1000 | Loss: 0.00001915
Iteration 58/1000 | Loss: 0.00001915
Iteration 59/1000 | Loss: 0.00001915
Iteration 60/1000 | Loss: 0.00001915
Iteration 61/1000 | Loss: 0.00001914
Iteration 62/1000 | Loss: 0.00001914
Iteration 63/1000 | Loss: 0.00001914
Iteration 64/1000 | Loss: 0.00001914
Iteration 65/1000 | Loss: 0.00001914
Iteration 66/1000 | Loss: 0.00001914
Iteration 67/1000 | Loss: 0.00001913
Iteration 68/1000 | Loss: 0.00001913
Iteration 69/1000 | Loss: 0.00001913
Iteration 70/1000 | Loss: 0.00001912
Iteration 71/1000 | Loss: 0.00001911
Iteration 72/1000 | Loss: 0.00001911
Iteration 73/1000 | Loss: 0.00001910
Iteration 74/1000 | Loss: 0.00001910
Iteration 75/1000 | Loss: 0.00001910
Iteration 76/1000 | Loss: 0.00001910
Iteration 77/1000 | Loss: 0.00001910
Iteration 78/1000 | Loss: 0.00001909
Iteration 79/1000 | Loss: 0.00001909
Iteration 80/1000 | Loss: 0.00001909
Iteration 81/1000 | Loss: 0.00001908
Iteration 82/1000 | Loss: 0.00001908
Iteration 83/1000 | Loss: 0.00001908
Iteration 84/1000 | Loss: 0.00001907
Iteration 85/1000 | Loss: 0.00001907
Iteration 86/1000 | Loss: 0.00001907
Iteration 87/1000 | Loss: 0.00001907
Iteration 88/1000 | Loss: 0.00001906
Iteration 89/1000 | Loss: 0.00001906
Iteration 90/1000 | Loss: 0.00001906
Iteration 91/1000 | Loss: 0.00001906
Iteration 92/1000 | Loss: 0.00001906
Iteration 93/1000 | Loss: 0.00001906
Iteration 94/1000 | Loss: 0.00001905
Iteration 95/1000 | Loss: 0.00001905
Iteration 96/1000 | Loss: 0.00001905
Iteration 97/1000 | Loss: 0.00001905
Iteration 98/1000 | Loss: 0.00001905
Iteration 99/1000 | Loss: 0.00001905
Iteration 100/1000 | Loss: 0.00001905
Iteration 101/1000 | Loss: 0.00001905
Iteration 102/1000 | Loss: 0.00001904
Iteration 103/1000 | Loss: 0.00001904
Iteration 104/1000 | Loss: 0.00001904
Iteration 105/1000 | Loss: 0.00001904
Iteration 106/1000 | Loss: 0.00001904
Iteration 107/1000 | Loss: 0.00001904
Iteration 108/1000 | Loss: 0.00001904
Iteration 109/1000 | Loss: 0.00001904
Iteration 110/1000 | Loss: 0.00001904
Iteration 111/1000 | Loss: 0.00001903
Iteration 112/1000 | Loss: 0.00001903
Iteration 113/1000 | Loss: 0.00001903
Iteration 114/1000 | Loss: 0.00001903
Iteration 115/1000 | Loss: 0.00001903
Iteration 116/1000 | Loss: 0.00001903
Iteration 117/1000 | Loss: 0.00001903
Iteration 118/1000 | Loss: 0.00001902
Iteration 119/1000 | Loss: 0.00001902
Iteration 120/1000 | Loss: 0.00001902
Iteration 121/1000 | Loss: 0.00001902
Iteration 122/1000 | Loss: 0.00001902
Iteration 123/1000 | Loss: 0.00001902
Iteration 124/1000 | Loss: 0.00001902
Iteration 125/1000 | Loss: 0.00001901
Iteration 126/1000 | Loss: 0.00001901
Iteration 127/1000 | Loss: 0.00001901
Iteration 128/1000 | Loss: 0.00001901
Iteration 129/1000 | Loss: 0.00001901
Iteration 130/1000 | Loss: 0.00001901
Iteration 131/1000 | Loss: 0.00001901
Iteration 132/1000 | Loss: 0.00001901
Iteration 133/1000 | Loss: 0.00001901
Iteration 134/1000 | Loss: 0.00001901
Iteration 135/1000 | Loss: 0.00001901
Iteration 136/1000 | Loss: 0.00001901
Iteration 137/1000 | Loss: 0.00001901
Iteration 138/1000 | Loss: 0.00001901
Iteration 139/1000 | Loss: 0.00001901
Iteration 140/1000 | Loss: 0.00001901
Iteration 141/1000 | Loss: 0.00001901
Iteration 142/1000 | Loss: 0.00001901
Iteration 143/1000 | Loss: 0.00001901
Iteration 144/1000 | Loss: 0.00001901
Iteration 145/1000 | Loss: 0.00001901
Iteration 146/1000 | Loss: 0.00001901
Iteration 147/1000 | Loss: 0.00001901
Iteration 148/1000 | Loss: 0.00001901
Iteration 149/1000 | Loss: 0.00001901
Iteration 150/1000 | Loss: 0.00001901
Iteration 151/1000 | Loss: 0.00001901
Iteration 152/1000 | Loss: 0.00001901
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 152. Stopping optimization.
Last 5 losses: [1.900901952467393e-05, 1.900901952467393e-05, 1.900901952467393e-05, 1.900901952467393e-05, 1.900901952467393e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.900901952467393e-05

Optimization complete. Final v2v error: 3.670372247695923 mm

Highest mean error: 4.200160503387451 mm for frame 135

Lowest mean error: 3.404147148132324 mm for frame 111

Saving results

Total time: 34.54675102233887
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_025/1098/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_025/1098.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_025/1098
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00821470
Iteration 2/25 | Loss: 0.00147963
Iteration 3/25 | Loss: 0.00128689
Iteration 4/25 | Loss: 0.00126707
Iteration 5/25 | Loss: 0.00126487
Iteration 6/25 | Loss: 0.00126487
Iteration 7/25 | Loss: 0.00126487
Iteration 8/25 | Loss: 0.00126487
Iteration 9/25 | Loss: 0.00126487
Iteration 10/25 | Loss: 0.00126487
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.001264871098101139, 0.001264871098101139, 0.001264871098101139, 0.001264871098101139, 0.001264871098101139]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001264871098101139

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.98883754
Iteration 2/25 | Loss: 0.00055432
Iteration 3/25 | Loss: 0.00055432
Iteration 4/25 | Loss: 0.00055432
Iteration 5/25 | Loss: 0.00055432
Iteration 6/25 | Loss: 0.00055431
Iteration 7/25 | Loss: 0.00055431
Iteration 8/25 | Loss: 0.00055431
Iteration 9/25 | Loss: 0.00055431
Iteration 10/25 | Loss: 0.00055431
Iteration 11/25 | Loss: 0.00055431
Iteration 12/25 | Loss: 0.00055431
Iteration 13/25 | Loss: 0.00055431
Iteration 14/25 | Loss: 0.00055431
Iteration 15/25 | Loss: 0.00055431
Iteration 16/25 | Loss: 0.00055431
Iteration 17/25 | Loss: 0.00055431
Iteration 18/25 | Loss: 0.00055431
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0005543134175240993, 0.0005543134175240993, 0.0005543134175240993, 0.0005543134175240993, 0.0005543134175240993]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005543134175240993

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00055431
Iteration 2/1000 | Loss: 0.00004100
Iteration 3/1000 | Loss: 0.00003107
Iteration 4/1000 | Loss: 0.00002778
Iteration 5/1000 | Loss: 0.00002612
Iteration 6/1000 | Loss: 0.00002514
Iteration 7/1000 | Loss: 0.00002468
Iteration 8/1000 | Loss: 0.00002428
Iteration 9/1000 | Loss: 0.00002389
Iteration 10/1000 | Loss: 0.00002360
Iteration 11/1000 | Loss: 0.00002325
Iteration 12/1000 | Loss: 0.00002286
Iteration 13/1000 | Loss: 0.00002262
Iteration 14/1000 | Loss: 0.00002252
Iteration 15/1000 | Loss: 0.00002237
Iteration 16/1000 | Loss: 0.00002236
Iteration 17/1000 | Loss: 0.00002220
Iteration 18/1000 | Loss: 0.00002220
Iteration 19/1000 | Loss: 0.00002219
Iteration 20/1000 | Loss: 0.00002219
Iteration 21/1000 | Loss: 0.00002219
Iteration 22/1000 | Loss: 0.00002219
Iteration 23/1000 | Loss: 0.00002219
Iteration 24/1000 | Loss: 0.00002219
Iteration 25/1000 | Loss: 0.00002218
Iteration 26/1000 | Loss: 0.00002212
Iteration 27/1000 | Loss: 0.00002208
Iteration 28/1000 | Loss: 0.00002208
Iteration 29/1000 | Loss: 0.00002207
Iteration 30/1000 | Loss: 0.00002199
Iteration 31/1000 | Loss: 0.00002194
Iteration 32/1000 | Loss: 0.00002194
Iteration 33/1000 | Loss: 0.00002194
Iteration 34/1000 | Loss: 0.00002194
Iteration 35/1000 | Loss: 0.00002194
Iteration 36/1000 | Loss: 0.00002194
Iteration 37/1000 | Loss: 0.00002194
Iteration 38/1000 | Loss: 0.00002194
Iteration 39/1000 | Loss: 0.00002194
Iteration 40/1000 | Loss: 0.00002194
Iteration 41/1000 | Loss: 0.00002194
Iteration 42/1000 | Loss: 0.00002194
Iteration 43/1000 | Loss: 0.00002194
Iteration 44/1000 | Loss: 0.00002194
Iteration 45/1000 | Loss: 0.00002194
Iteration 46/1000 | Loss: 0.00002194
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 46. Stopping optimization.
Last 5 losses: [2.193576074205339e-05, 2.193576074205339e-05, 2.193576074205339e-05, 2.193576074205339e-05, 2.193576074205339e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.193576074205339e-05

Optimization complete. Final v2v error: 3.9308290481567383 mm

Highest mean error: 3.9634244441986084 mm for frame 0

Lowest mean error: 3.899641990661621 mm for frame 70

Saving results

Total time: 33.93988275527954
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_025/1011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_025/1011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_025/1011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00406356
Iteration 2/25 | Loss: 0.00125913
Iteration 3/25 | Loss: 0.00117402
Iteration 4/25 | Loss: 0.00115995
Iteration 5/25 | Loss: 0.00115421
Iteration 6/25 | Loss: 0.00115326
Iteration 7/25 | Loss: 0.00115326
Iteration 8/25 | Loss: 0.00115326
Iteration 9/25 | Loss: 0.00115326
Iteration 10/25 | Loss: 0.00115326
Iteration 11/25 | Loss: 0.00115326
Iteration 12/25 | Loss: 0.00115326
Iteration 13/25 | Loss: 0.00115326
Iteration 14/25 | Loss: 0.00115326
Iteration 15/25 | Loss: 0.00115326
Iteration 16/25 | Loss: 0.00115326
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0011532611679285765, 0.0011532611679285765, 0.0011532611679285765, 0.0011532611679285765, 0.0011532611679285765]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011532611679285765

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34126210
Iteration 2/25 | Loss: 0.00117852
Iteration 3/25 | Loss: 0.00117852
Iteration 4/25 | Loss: 0.00117852
Iteration 5/25 | Loss: 0.00117851
Iteration 6/25 | Loss: 0.00117851
Iteration 7/25 | Loss: 0.00117851
Iteration 8/25 | Loss: 0.00117851
Iteration 9/25 | Loss: 0.00117851
Iteration 10/25 | Loss: 0.00117851
Iteration 11/25 | Loss: 0.00117851
Iteration 12/25 | Loss: 0.00117851
Iteration 13/25 | Loss: 0.00117851
Iteration 14/25 | Loss: 0.00117851
Iteration 15/25 | Loss: 0.00117851
Iteration 16/25 | Loss: 0.00117851
Iteration 17/25 | Loss: 0.00117851
Iteration 18/25 | Loss: 0.00117851
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.001178513397462666, 0.001178513397462666, 0.001178513397462666, 0.001178513397462666, 0.001178513397462666]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001178513397462666

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00117851
Iteration 2/1000 | Loss: 0.00004217
Iteration 3/1000 | Loss: 0.00002722
Iteration 4/1000 | Loss: 0.00002284
Iteration 5/1000 | Loss: 0.00002128
Iteration 6/1000 | Loss: 0.00001982
Iteration 7/1000 | Loss: 0.00001893
Iteration 8/1000 | Loss: 0.00001838
Iteration 9/1000 | Loss: 0.00001805
Iteration 10/1000 | Loss: 0.00001782
Iteration 11/1000 | Loss: 0.00001774
Iteration 12/1000 | Loss: 0.00001767
Iteration 13/1000 | Loss: 0.00001757
Iteration 14/1000 | Loss: 0.00001756
Iteration 15/1000 | Loss: 0.00001754
Iteration 16/1000 | Loss: 0.00001749
Iteration 17/1000 | Loss: 0.00001741
Iteration 18/1000 | Loss: 0.00001738
Iteration 19/1000 | Loss: 0.00001732
Iteration 20/1000 | Loss: 0.00001730
Iteration 21/1000 | Loss: 0.00001729
Iteration 22/1000 | Loss: 0.00001728
Iteration 23/1000 | Loss: 0.00001727
Iteration 24/1000 | Loss: 0.00001726
Iteration 25/1000 | Loss: 0.00001725
Iteration 26/1000 | Loss: 0.00001725
Iteration 27/1000 | Loss: 0.00001725
Iteration 28/1000 | Loss: 0.00001724
Iteration 29/1000 | Loss: 0.00001724
Iteration 30/1000 | Loss: 0.00001724
Iteration 31/1000 | Loss: 0.00001724
Iteration 32/1000 | Loss: 0.00001723
Iteration 33/1000 | Loss: 0.00001722
Iteration 34/1000 | Loss: 0.00001722
Iteration 35/1000 | Loss: 0.00001721
Iteration 36/1000 | Loss: 0.00001721
Iteration 37/1000 | Loss: 0.00001720
Iteration 38/1000 | Loss: 0.00001720
Iteration 39/1000 | Loss: 0.00001719
Iteration 40/1000 | Loss: 0.00001719
Iteration 41/1000 | Loss: 0.00001719
Iteration 42/1000 | Loss: 0.00001718
Iteration 43/1000 | Loss: 0.00001718
Iteration 44/1000 | Loss: 0.00001717
Iteration 45/1000 | Loss: 0.00001717
Iteration 46/1000 | Loss: 0.00001717
Iteration 47/1000 | Loss: 0.00001716
Iteration 48/1000 | Loss: 0.00001716
Iteration 49/1000 | Loss: 0.00001716
Iteration 50/1000 | Loss: 0.00001716
Iteration 51/1000 | Loss: 0.00001715
Iteration 52/1000 | Loss: 0.00001715
Iteration 53/1000 | Loss: 0.00001714
Iteration 54/1000 | Loss: 0.00001714
Iteration 55/1000 | Loss: 0.00001714
Iteration 56/1000 | Loss: 0.00001714
Iteration 57/1000 | Loss: 0.00001714
Iteration 58/1000 | Loss: 0.00001713
Iteration 59/1000 | Loss: 0.00001713
Iteration 60/1000 | Loss: 0.00001712
Iteration 61/1000 | Loss: 0.00001711
Iteration 62/1000 | Loss: 0.00001711
Iteration 63/1000 | Loss: 0.00001710
Iteration 64/1000 | Loss: 0.00001710
Iteration 65/1000 | Loss: 0.00001709
Iteration 66/1000 | Loss: 0.00001709
Iteration 67/1000 | Loss: 0.00001709
Iteration 68/1000 | Loss: 0.00001708
Iteration 69/1000 | Loss: 0.00001708
Iteration 70/1000 | Loss: 0.00001708
Iteration 71/1000 | Loss: 0.00001707
Iteration 72/1000 | Loss: 0.00001707
Iteration 73/1000 | Loss: 0.00001707
Iteration 74/1000 | Loss: 0.00001707
Iteration 75/1000 | Loss: 0.00001707
Iteration 76/1000 | Loss: 0.00001707
Iteration 77/1000 | Loss: 0.00001706
Iteration 78/1000 | Loss: 0.00001705
Iteration 79/1000 | Loss: 0.00001705
Iteration 80/1000 | Loss: 0.00001705
Iteration 81/1000 | Loss: 0.00001705
Iteration 82/1000 | Loss: 0.00001705
Iteration 83/1000 | Loss: 0.00001705
Iteration 84/1000 | Loss: 0.00001705
Iteration 85/1000 | Loss: 0.00001705
Iteration 86/1000 | Loss: 0.00001705
Iteration 87/1000 | Loss: 0.00001704
Iteration 88/1000 | Loss: 0.00001704
Iteration 89/1000 | Loss: 0.00001704
Iteration 90/1000 | Loss: 0.00001704
Iteration 91/1000 | Loss: 0.00001704
Iteration 92/1000 | Loss: 0.00001703
Iteration 93/1000 | Loss: 0.00001703
Iteration 94/1000 | Loss: 0.00001703
Iteration 95/1000 | Loss: 0.00001703
Iteration 96/1000 | Loss: 0.00001703
Iteration 97/1000 | Loss: 0.00001703
Iteration 98/1000 | Loss: 0.00001703
Iteration 99/1000 | Loss: 0.00001703
Iteration 100/1000 | Loss: 0.00001702
Iteration 101/1000 | Loss: 0.00001702
Iteration 102/1000 | Loss: 0.00001702
Iteration 103/1000 | Loss: 0.00001702
Iteration 104/1000 | Loss: 0.00001702
Iteration 105/1000 | Loss: 0.00001702
Iteration 106/1000 | Loss: 0.00001702
Iteration 107/1000 | Loss: 0.00001702
Iteration 108/1000 | Loss: 0.00001702
Iteration 109/1000 | Loss: 0.00001701
Iteration 110/1000 | Loss: 0.00001701
Iteration 111/1000 | Loss: 0.00001701
Iteration 112/1000 | Loss: 0.00001701
Iteration 113/1000 | Loss: 0.00001701
Iteration 114/1000 | Loss: 0.00001701
Iteration 115/1000 | Loss: 0.00001701
Iteration 116/1000 | Loss: 0.00001700
Iteration 117/1000 | Loss: 0.00001700
Iteration 118/1000 | Loss: 0.00001700
Iteration 119/1000 | Loss: 0.00001700
Iteration 120/1000 | Loss: 0.00001700
Iteration 121/1000 | Loss: 0.00001700
Iteration 122/1000 | Loss: 0.00001700
Iteration 123/1000 | Loss: 0.00001700
Iteration 124/1000 | Loss: 0.00001700
Iteration 125/1000 | Loss: 0.00001700
Iteration 126/1000 | Loss: 0.00001700
Iteration 127/1000 | Loss: 0.00001700
Iteration 128/1000 | Loss: 0.00001699
Iteration 129/1000 | Loss: 0.00001699
Iteration 130/1000 | Loss: 0.00001699
Iteration 131/1000 | Loss: 0.00001699
Iteration 132/1000 | Loss: 0.00001699
Iteration 133/1000 | Loss: 0.00001699
Iteration 134/1000 | Loss: 0.00001699
Iteration 135/1000 | Loss: 0.00001699
Iteration 136/1000 | Loss: 0.00001699
Iteration 137/1000 | Loss: 0.00001699
Iteration 138/1000 | Loss: 0.00001698
Iteration 139/1000 | Loss: 0.00001698
Iteration 140/1000 | Loss: 0.00001698
Iteration 141/1000 | Loss: 0.00001698
Iteration 142/1000 | Loss: 0.00001698
Iteration 143/1000 | Loss: 0.00001698
Iteration 144/1000 | Loss: 0.00001698
Iteration 145/1000 | Loss: 0.00001698
Iteration 146/1000 | Loss: 0.00001698
Iteration 147/1000 | Loss: 0.00001698
Iteration 148/1000 | Loss: 0.00001698
Iteration 149/1000 | Loss: 0.00001698
Iteration 150/1000 | Loss: 0.00001698
Iteration 151/1000 | Loss: 0.00001697
Iteration 152/1000 | Loss: 0.00001697
Iteration 153/1000 | Loss: 0.00001697
Iteration 154/1000 | Loss: 0.00001697
Iteration 155/1000 | Loss: 0.00001697
Iteration 156/1000 | Loss: 0.00001697
Iteration 157/1000 | Loss: 0.00001697
Iteration 158/1000 | Loss: 0.00001697
Iteration 159/1000 | Loss: 0.00001697
Iteration 160/1000 | Loss: 0.00001697
Iteration 161/1000 | Loss: 0.00001696
Iteration 162/1000 | Loss: 0.00001696
Iteration 163/1000 | Loss: 0.00001696
Iteration 164/1000 | Loss: 0.00001696
Iteration 165/1000 | Loss: 0.00001696
Iteration 166/1000 | Loss: 0.00001696
Iteration 167/1000 | Loss: 0.00001696
Iteration 168/1000 | Loss: 0.00001696
Iteration 169/1000 | Loss: 0.00001696
Iteration 170/1000 | Loss: 0.00001695
Iteration 171/1000 | Loss: 0.00001695
Iteration 172/1000 | Loss: 0.00001695
Iteration 173/1000 | Loss: 0.00001695
Iteration 174/1000 | Loss: 0.00001694
Iteration 175/1000 | Loss: 0.00001694
Iteration 176/1000 | Loss: 0.00001694
Iteration 177/1000 | Loss: 0.00001694
Iteration 178/1000 | Loss: 0.00001694
Iteration 179/1000 | Loss: 0.00001694
Iteration 180/1000 | Loss: 0.00001694
Iteration 181/1000 | Loss: 0.00001694
Iteration 182/1000 | Loss: 0.00001694
Iteration 183/1000 | Loss: 0.00001694
Iteration 184/1000 | Loss: 0.00001694
Iteration 185/1000 | Loss: 0.00001694
Iteration 186/1000 | Loss: 0.00001694
Iteration 187/1000 | Loss: 0.00001694
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 187. Stopping optimization.
Last 5 losses: [1.693988269835245e-05, 1.693988269835245e-05, 1.693988269835245e-05, 1.693988269835245e-05, 1.693988269835245e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.693988269835245e-05

Optimization complete. Final v2v error: 3.4009575843811035 mm

Highest mean error: 3.8766088485717773 mm for frame 238

Lowest mean error: 2.9914133548736572 mm for frame 53

Saving results

Total time: 45.386962890625
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_025/1017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_025/1017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_025/1017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00837754
Iteration 2/25 | Loss: 0.00151946
Iteration 3/25 | Loss: 0.00131553
Iteration 4/25 | Loss: 0.00129732
Iteration 5/25 | Loss: 0.00129612
Iteration 6/25 | Loss: 0.00129612
Iteration 7/25 | Loss: 0.00129612
Iteration 8/25 | Loss: 0.00129612
Iteration 9/25 | Loss: 0.00129612
Iteration 10/25 | Loss: 0.00129612
Iteration 11/25 | Loss: 0.00129612
Iteration 12/25 | Loss: 0.00129612
Iteration 13/25 | Loss: 0.00129612
Iteration 14/25 | Loss: 0.00129612
Iteration 15/25 | Loss: 0.00129612
Iteration 16/25 | Loss: 0.00129612
Iteration 17/25 | Loss: 0.00129612
Iteration 18/25 | Loss: 0.00129612
Iteration 19/25 | Loss: 0.00129612
Iteration 20/25 | Loss: 0.00129612
Iteration 21/25 | Loss: 0.00129612
Iteration 22/25 | Loss: 0.00129612
Iteration 23/25 | Loss: 0.00129612
Iteration 24/25 | Loss: 0.00129612
Iteration 25/25 | Loss: 0.00129612
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.001296116504818201, 0.001296116504818201, 0.001296116504818201, 0.001296116504818201, 0.001296116504818201]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001296116504818201

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.98134744
Iteration 2/25 | Loss: 0.00059325
Iteration 3/25 | Loss: 0.00059324
Iteration 4/25 | Loss: 0.00059324
Iteration 5/25 | Loss: 0.00059324
Iteration 6/25 | Loss: 0.00059324
Iteration 7/25 | Loss: 0.00059324
Iteration 8/25 | Loss: 0.00059324
Iteration 9/25 | Loss: 0.00059324
Iteration 10/25 | Loss: 0.00059324
Iteration 11/25 | Loss: 0.00059324
Iteration 12/25 | Loss: 0.00059324
Iteration 13/25 | Loss: 0.00059324
Iteration 14/25 | Loss: 0.00059324
Iteration 15/25 | Loss: 0.00059324
Iteration 16/25 | Loss: 0.00059324
Iteration 17/25 | Loss: 0.00059324
Iteration 18/25 | Loss: 0.00059324
Iteration 19/25 | Loss: 0.00059324
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0005932369385845959, 0.0005932369385845959, 0.0005932369385845959, 0.0005932369385845959, 0.0005932369385845959]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005932369385845959

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00059324
Iteration 2/1000 | Loss: 0.00004246
Iteration 3/1000 | Loss: 0.00003115
Iteration 4/1000 | Loss: 0.00002752
Iteration 5/1000 | Loss: 0.00002620
Iteration 6/1000 | Loss: 0.00002568
Iteration 7/1000 | Loss: 0.00002507
Iteration 8/1000 | Loss: 0.00002457
Iteration 9/1000 | Loss: 0.00002423
Iteration 10/1000 | Loss: 0.00002394
Iteration 11/1000 | Loss: 0.00002363
Iteration 12/1000 | Loss: 0.00002338
Iteration 13/1000 | Loss: 0.00002313
Iteration 14/1000 | Loss: 0.00002289
Iteration 15/1000 | Loss: 0.00002285
Iteration 16/1000 | Loss: 0.00002284
Iteration 17/1000 | Loss: 0.00002277
Iteration 18/1000 | Loss: 0.00002276
Iteration 19/1000 | Loss: 0.00002276
Iteration 20/1000 | Loss: 0.00002275
Iteration 21/1000 | Loss: 0.00002272
Iteration 22/1000 | Loss: 0.00002268
Iteration 23/1000 | Loss: 0.00002266
Iteration 24/1000 | Loss: 0.00002265
Iteration 25/1000 | Loss: 0.00002264
Iteration 26/1000 | Loss: 0.00002259
Iteration 27/1000 | Loss: 0.00002259
Iteration 28/1000 | Loss: 0.00002259
Iteration 29/1000 | Loss: 0.00002259
Iteration 30/1000 | Loss: 0.00002259
Iteration 31/1000 | Loss: 0.00002259
Iteration 32/1000 | Loss: 0.00002259
Iteration 33/1000 | Loss: 0.00002259
Iteration 34/1000 | Loss: 0.00002259
Iteration 35/1000 | Loss: 0.00002259
Iteration 36/1000 | Loss: 0.00002258
Iteration 37/1000 | Loss: 0.00002258
Iteration 38/1000 | Loss: 0.00002252
Iteration 39/1000 | Loss: 0.00002252
Iteration 40/1000 | Loss: 0.00002252
Iteration 41/1000 | Loss: 0.00002252
Iteration 42/1000 | Loss: 0.00002252
Iteration 43/1000 | Loss: 0.00002252
Iteration 44/1000 | Loss: 0.00002251
Iteration 45/1000 | Loss: 0.00002251
Iteration 46/1000 | Loss: 0.00002251
Iteration 47/1000 | Loss: 0.00002251
Iteration 48/1000 | Loss: 0.00002251
Iteration 49/1000 | Loss: 0.00002251
Iteration 50/1000 | Loss: 0.00002251
Iteration 51/1000 | Loss: 0.00002250
Iteration 52/1000 | Loss: 0.00002250
Iteration 53/1000 | Loss: 0.00002250
Iteration 54/1000 | Loss: 0.00002249
Iteration 55/1000 | Loss: 0.00002249
Iteration 56/1000 | Loss: 0.00002248
Iteration 57/1000 | Loss: 0.00002248
Iteration 58/1000 | Loss: 0.00002245
Iteration 59/1000 | Loss: 0.00002245
Iteration 60/1000 | Loss: 0.00002245
Iteration 61/1000 | Loss: 0.00002245
Iteration 62/1000 | Loss: 0.00002245
Iteration 63/1000 | Loss: 0.00002245
Iteration 64/1000 | Loss: 0.00002244
Iteration 65/1000 | Loss: 0.00002244
Iteration 66/1000 | Loss: 0.00002244
Iteration 67/1000 | Loss: 0.00002244
Iteration 68/1000 | Loss: 0.00002244
Iteration 69/1000 | Loss: 0.00002244
Iteration 70/1000 | Loss: 0.00002243
Iteration 71/1000 | Loss: 0.00002243
Iteration 72/1000 | Loss: 0.00002243
Iteration 73/1000 | Loss: 0.00002241
Iteration 74/1000 | Loss: 0.00002241
Iteration 75/1000 | Loss: 0.00002240
Iteration 76/1000 | Loss: 0.00002239
Iteration 77/1000 | Loss: 0.00002239
Iteration 78/1000 | Loss: 0.00002239
Iteration 79/1000 | Loss: 0.00002239
Iteration 80/1000 | Loss: 0.00002239
Iteration 81/1000 | Loss: 0.00002239
Iteration 82/1000 | Loss: 0.00002239
Iteration 83/1000 | Loss: 0.00002239
Iteration 84/1000 | Loss: 0.00002238
Iteration 85/1000 | Loss: 0.00002238
Iteration 86/1000 | Loss: 0.00002238
Iteration 87/1000 | Loss: 0.00002238
Iteration 88/1000 | Loss: 0.00002237
Iteration 89/1000 | Loss: 0.00002237
Iteration 90/1000 | Loss: 0.00002237
Iteration 91/1000 | Loss: 0.00002237
Iteration 92/1000 | Loss: 0.00002236
Iteration 93/1000 | Loss: 0.00002236
Iteration 94/1000 | Loss: 0.00002236
Iteration 95/1000 | Loss: 0.00002236
Iteration 96/1000 | Loss: 0.00002235
Iteration 97/1000 | Loss: 0.00002235
Iteration 98/1000 | Loss: 0.00002235
Iteration 99/1000 | Loss: 0.00002235
Iteration 100/1000 | Loss: 0.00002235
Iteration 101/1000 | Loss: 0.00002235
Iteration 102/1000 | Loss: 0.00002235
Iteration 103/1000 | Loss: 0.00002235
Iteration 104/1000 | Loss: 0.00002235
Iteration 105/1000 | Loss: 0.00002234
Iteration 106/1000 | Loss: 0.00002234
Iteration 107/1000 | Loss: 0.00002234
Iteration 108/1000 | Loss: 0.00002234
Iteration 109/1000 | Loss: 0.00002234
Iteration 110/1000 | Loss: 0.00002234
Iteration 111/1000 | Loss: 0.00002234
Iteration 112/1000 | Loss: 0.00002234
Iteration 113/1000 | Loss: 0.00002234
Iteration 114/1000 | Loss: 0.00002234
Iteration 115/1000 | Loss: 0.00002234
Iteration 116/1000 | Loss: 0.00002234
Iteration 117/1000 | Loss: 0.00002234
Iteration 118/1000 | Loss: 0.00002233
Iteration 119/1000 | Loss: 0.00002233
Iteration 120/1000 | Loss: 0.00002233
Iteration 121/1000 | Loss: 0.00002233
Iteration 122/1000 | Loss: 0.00002233
Iteration 123/1000 | Loss: 0.00002233
Iteration 124/1000 | Loss: 0.00002233
Iteration 125/1000 | Loss: 0.00002233
Iteration 126/1000 | Loss: 0.00002233
Iteration 127/1000 | Loss: 0.00002233
Iteration 128/1000 | Loss: 0.00002233
Iteration 129/1000 | Loss: 0.00002233
Iteration 130/1000 | Loss: 0.00002233
Iteration 131/1000 | Loss: 0.00002233
Iteration 132/1000 | Loss: 0.00002233
Iteration 133/1000 | Loss: 0.00002233
Iteration 134/1000 | Loss: 0.00002233
Iteration 135/1000 | Loss: 0.00002233
Iteration 136/1000 | Loss: 0.00002233
Iteration 137/1000 | Loss: 0.00002233
Iteration 138/1000 | Loss: 0.00002233
Iteration 139/1000 | Loss: 0.00002233
Iteration 140/1000 | Loss: 0.00002233
Iteration 141/1000 | Loss: 0.00002233
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 141. Stopping optimization.
Last 5 losses: [2.2334161258186214e-05, 2.2334161258186214e-05, 2.2334161258186214e-05, 2.2334161258186214e-05, 2.2334161258186214e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2334161258186214e-05

Optimization complete. Final v2v error: 3.9861578941345215 mm

Highest mean error: 4.038917541503906 mm for frame 125

Lowest mean error: 3.9349966049194336 mm for frame 145

Saving results

Total time: 37.70127558708191
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_025/1097/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_025/1097.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_025/1097
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01073489
Iteration 2/25 | Loss: 0.00182919
Iteration 3/25 | Loss: 0.00140003
Iteration 4/25 | Loss: 0.00136336
Iteration 5/25 | Loss: 0.00135475
Iteration 6/25 | Loss: 0.00135277
Iteration 7/25 | Loss: 0.00135273
Iteration 8/25 | Loss: 0.00135273
Iteration 9/25 | Loss: 0.00135273
Iteration 10/25 | Loss: 0.00135273
Iteration 11/25 | Loss: 0.00135273
Iteration 12/25 | Loss: 0.00135273
Iteration 13/25 | Loss: 0.00135273
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0013527310220524669, 0.0013527310220524669, 0.0013527310220524669, 0.0013527310220524669, 0.0013527310220524669]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013527310220524669

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.91257524
Iteration 2/25 | Loss: 0.00103666
Iteration 3/25 | Loss: 0.00103652
Iteration 4/25 | Loss: 0.00103652
Iteration 5/25 | Loss: 0.00103652
Iteration 6/25 | Loss: 0.00103652
Iteration 7/25 | Loss: 0.00103652
Iteration 8/25 | Loss: 0.00103651
Iteration 9/25 | Loss: 0.00103651
Iteration 10/25 | Loss: 0.00103651
Iteration 11/25 | Loss: 0.00103651
Iteration 12/25 | Loss: 0.00103651
Iteration 13/25 | Loss: 0.00103651
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0010365140624344349, 0.0010365140624344349, 0.0010365140624344349, 0.0010365140624344349, 0.0010365140624344349]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010365140624344349

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00103651
Iteration 2/1000 | Loss: 0.00007957
Iteration 3/1000 | Loss: 0.00004828
Iteration 4/1000 | Loss: 0.00004157
Iteration 5/1000 | Loss: 0.00003964
Iteration 6/1000 | Loss: 0.00003811
Iteration 7/1000 | Loss: 0.00003706
Iteration 8/1000 | Loss: 0.00003635
Iteration 9/1000 | Loss: 0.00003581
Iteration 10/1000 | Loss: 0.00003538
Iteration 11/1000 | Loss: 0.00003504
Iteration 12/1000 | Loss: 0.00003485
Iteration 13/1000 | Loss: 0.00003467
Iteration 14/1000 | Loss: 0.00003454
Iteration 15/1000 | Loss: 0.00003447
Iteration 16/1000 | Loss: 0.00003440
Iteration 17/1000 | Loss: 0.00003430
Iteration 18/1000 | Loss: 0.00003430
Iteration 19/1000 | Loss: 0.00003430
Iteration 20/1000 | Loss: 0.00003429
Iteration 21/1000 | Loss: 0.00003429
Iteration 22/1000 | Loss: 0.00003428
Iteration 23/1000 | Loss: 0.00003428
Iteration 24/1000 | Loss: 0.00003427
Iteration 25/1000 | Loss: 0.00003427
Iteration 26/1000 | Loss: 0.00003427
Iteration 27/1000 | Loss: 0.00003426
Iteration 28/1000 | Loss: 0.00003426
Iteration 29/1000 | Loss: 0.00003426
Iteration 30/1000 | Loss: 0.00003426
Iteration 31/1000 | Loss: 0.00003426
Iteration 32/1000 | Loss: 0.00003425
Iteration 33/1000 | Loss: 0.00003424
Iteration 34/1000 | Loss: 0.00003423
Iteration 35/1000 | Loss: 0.00003423
Iteration 36/1000 | Loss: 0.00003423
Iteration 37/1000 | Loss: 0.00003423
Iteration 38/1000 | Loss: 0.00003423
Iteration 39/1000 | Loss: 0.00003423
Iteration 40/1000 | Loss: 0.00003423
Iteration 41/1000 | Loss: 0.00003422
Iteration 42/1000 | Loss: 0.00003422
Iteration 43/1000 | Loss: 0.00003422
Iteration 44/1000 | Loss: 0.00003422
Iteration 45/1000 | Loss: 0.00003421
Iteration 46/1000 | Loss: 0.00003421
Iteration 47/1000 | Loss: 0.00003421
Iteration 48/1000 | Loss: 0.00003420
Iteration 49/1000 | Loss: 0.00003419
Iteration 50/1000 | Loss: 0.00003419
Iteration 51/1000 | Loss: 0.00003418
Iteration 52/1000 | Loss: 0.00003418
Iteration 53/1000 | Loss: 0.00003415
Iteration 54/1000 | Loss: 0.00003415
Iteration 55/1000 | Loss: 0.00003413
Iteration 56/1000 | Loss: 0.00003413
Iteration 57/1000 | Loss: 0.00003413
Iteration 58/1000 | Loss: 0.00003412
Iteration 59/1000 | Loss: 0.00003412
Iteration 60/1000 | Loss: 0.00003412
Iteration 61/1000 | Loss: 0.00003412
Iteration 62/1000 | Loss: 0.00003411
Iteration 63/1000 | Loss: 0.00003411
Iteration 64/1000 | Loss: 0.00003410
Iteration 65/1000 | Loss: 0.00003410
Iteration 66/1000 | Loss: 0.00003410
Iteration 67/1000 | Loss: 0.00003409
Iteration 68/1000 | Loss: 0.00003409
Iteration 69/1000 | Loss: 0.00003408
Iteration 70/1000 | Loss: 0.00003408
Iteration 71/1000 | Loss: 0.00003408
Iteration 72/1000 | Loss: 0.00003408
Iteration 73/1000 | Loss: 0.00003408
Iteration 74/1000 | Loss: 0.00003408
Iteration 75/1000 | Loss: 0.00003407
Iteration 76/1000 | Loss: 0.00003406
Iteration 77/1000 | Loss: 0.00003406
Iteration 78/1000 | Loss: 0.00003406
Iteration 79/1000 | Loss: 0.00003406
Iteration 80/1000 | Loss: 0.00003406
Iteration 81/1000 | Loss: 0.00003405
Iteration 82/1000 | Loss: 0.00003405
Iteration 83/1000 | Loss: 0.00003405
Iteration 84/1000 | Loss: 0.00003405
Iteration 85/1000 | Loss: 0.00003404
Iteration 86/1000 | Loss: 0.00003404
Iteration 87/1000 | Loss: 0.00003404
Iteration 88/1000 | Loss: 0.00003404
Iteration 89/1000 | Loss: 0.00003403
Iteration 90/1000 | Loss: 0.00003403
Iteration 91/1000 | Loss: 0.00003403
Iteration 92/1000 | Loss: 0.00003403
Iteration 93/1000 | Loss: 0.00003402
Iteration 94/1000 | Loss: 0.00003402
Iteration 95/1000 | Loss: 0.00003401
Iteration 96/1000 | Loss: 0.00003401
Iteration 97/1000 | Loss: 0.00003401
Iteration 98/1000 | Loss: 0.00003401
Iteration 99/1000 | Loss: 0.00003401
Iteration 100/1000 | Loss: 0.00003401
Iteration 101/1000 | Loss: 0.00003401
Iteration 102/1000 | Loss: 0.00003400
Iteration 103/1000 | Loss: 0.00003400
Iteration 104/1000 | Loss: 0.00003400
Iteration 105/1000 | Loss: 0.00003400
Iteration 106/1000 | Loss: 0.00003399
Iteration 107/1000 | Loss: 0.00003399
Iteration 108/1000 | Loss: 0.00003399
Iteration 109/1000 | Loss: 0.00003399
Iteration 110/1000 | Loss: 0.00003399
Iteration 111/1000 | Loss: 0.00003399
Iteration 112/1000 | Loss: 0.00003399
Iteration 113/1000 | Loss: 0.00003399
Iteration 114/1000 | Loss: 0.00003399
Iteration 115/1000 | Loss: 0.00003399
Iteration 116/1000 | Loss: 0.00003399
Iteration 117/1000 | Loss: 0.00003399
Iteration 118/1000 | Loss: 0.00003399
Iteration 119/1000 | Loss: 0.00003399
Iteration 120/1000 | Loss: 0.00003398
Iteration 121/1000 | Loss: 0.00003398
Iteration 122/1000 | Loss: 0.00003398
Iteration 123/1000 | Loss: 0.00003398
Iteration 124/1000 | Loss: 0.00003398
Iteration 125/1000 | Loss: 0.00003398
Iteration 126/1000 | Loss: 0.00003398
Iteration 127/1000 | Loss: 0.00003398
Iteration 128/1000 | Loss: 0.00003398
Iteration 129/1000 | Loss: 0.00003398
Iteration 130/1000 | Loss: 0.00003398
Iteration 131/1000 | Loss: 0.00003398
Iteration 132/1000 | Loss: 0.00003398
Iteration 133/1000 | Loss: 0.00003397
Iteration 134/1000 | Loss: 0.00003397
Iteration 135/1000 | Loss: 0.00003397
Iteration 136/1000 | Loss: 0.00003397
Iteration 137/1000 | Loss: 0.00003397
Iteration 138/1000 | Loss: 0.00003397
Iteration 139/1000 | Loss: 0.00003397
Iteration 140/1000 | Loss: 0.00003397
Iteration 141/1000 | Loss: 0.00003397
Iteration 142/1000 | Loss: 0.00003397
Iteration 143/1000 | Loss: 0.00003397
Iteration 144/1000 | Loss: 0.00003397
Iteration 145/1000 | Loss: 0.00003397
Iteration 146/1000 | Loss: 0.00003397
Iteration 147/1000 | Loss: 0.00003397
Iteration 148/1000 | Loss: 0.00003397
Iteration 149/1000 | Loss: 0.00003397
Iteration 150/1000 | Loss: 0.00003397
Iteration 151/1000 | Loss: 0.00003397
Iteration 152/1000 | Loss: 0.00003397
Iteration 153/1000 | Loss: 0.00003397
Iteration 154/1000 | Loss: 0.00003397
Iteration 155/1000 | Loss: 0.00003397
Iteration 156/1000 | Loss: 0.00003397
Iteration 157/1000 | Loss: 0.00003397
Iteration 158/1000 | Loss: 0.00003397
Iteration 159/1000 | Loss: 0.00003397
Iteration 160/1000 | Loss: 0.00003397
Iteration 161/1000 | Loss: 0.00003397
Iteration 162/1000 | Loss: 0.00003397
Iteration 163/1000 | Loss: 0.00003397
Iteration 164/1000 | Loss: 0.00003397
Iteration 165/1000 | Loss: 0.00003397
Iteration 166/1000 | Loss: 0.00003397
Iteration 167/1000 | Loss: 0.00003397
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 167. Stopping optimization.
Last 5 losses: [3.396657848497853e-05, 3.396657848497853e-05, 3.396657848497853e-05, 3.396657848497853e-05, 3.396657848497853e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.396657848497853e-05

Optimization complete. Final v2v error: 4.813999176025391 mm

Highest mean error: 5.624007225036621 mm for frame 232

Lowest mean error: 3.9046437740325928 mm for frame 162

Saving results

Total time: 48.75691604614258
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_025/1062/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_025/1062.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_025/1062
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00390457
Iteration 2/25 | Loss: 0.00121148
Iteration 3/25 | Loss: 0.00113890
Iteration 4/25 | Loss: 0.00113049
Iteration 5/25 | Loss: 0.00112745
Iteration 6/25 | Loss: 0.00112712
Iteration 7/25 | Loss: 0.00112712
Iteration 8/25 | Loss: 0.00112712
Iteration 9/25 | Loss: 0.00112712
Iteration 10/25 | Loss: 0.00112712
Iteration 11/25 | Loss: 0.00112712
Iteration 12/25 | Loss: 0.00112712
Iteration 13/25 | Loss: 0.00112712
Iteration 14/25 | Loss: 0.00112712
Iteration 15/25 | Loss: 0.00112712
Iteration 16/25 | Loss: 0.00112712
Iteration 17/25 | Loss: 0.00112712
Iteration 18/25 | Loss: 0.00112712
Iteration 19/25 | Loss: 0.00112712
Iteration 20/25 | Loss: 0.00112712
Iteration 21/25 | Loss: 0.00112712
Iteration 22/25 | Loss: 0.00112712
Iteration 23/25 | Loss: 0.00112712
Iteration 24/25 | Loss: 0.00112712
Iteration 25/25 | Loss: 0.00112712

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38989782
Iteration 2/25 | Loss: 0.00084525
Iteration 3/25 | Loss: 0.00084524
Iteration 4/25 | Loss: 0.00084524
Iteration 5/25 | Loss: 0.00084524
Iteration 6/25 | Loss: 0.00084524
Iteration 7/25 | Loss: 0.00084524
Iteration 8/25 | Loss: 0.00084524
Iteration 9/25 | Loss: 0.00084524
Iteration 10/25 | Loss: 0.00084524
Iteration 11/25 | Loss: 0.00084524
Iteration 12/25 | Loss: 0.00084524
Iteration 13/25 | Loss: 0.00084524
Iteration 14/25 | Loss: 0.00084524
Iteration 15/25 | Loss: 0.00084524
Iteration 16/25 | Loss: 0.00084524
Iteration 17/25 | Loss: 0.00084524
Iteration 18/25 | Loss: 0.00084524
Iteration 19/25 | Loss: 0.00084524
Iteration 20/25 | Loss: 0.00084524
Iteration 21/25 | Loss: 0.00084524
Iteration 22/25 | Loss: 0.00084524
Iteration 23/25 | Loss: 0.00084524
Iteration 24/25 | Loss: 0.00084524
Iteration 25/25 | Loss: 0.00084524

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00084524
Iteration 2/1000 | Loss: 0.00002045
Iteration 3/1000 | Loss: 0.00001357
Iteration 4/1000 | Loss: 0.00001233
Iteration 5/1000 | Loss: 0.00001173
Iteration 6/1000 | Loss: 0.00001141
Iteration 7/1000 | Loss: 0.00001129
Iteration 8/1000 | Loss: 0.00001122
Iteration 9/1000 | Loss: 0.00001115
Iteration 10/1000 | Loss: 0.00001104
Iteration 11/1000 | Loss: 0.00001089
Iteration 12/1000 | Loss: 0.00001084
Iteration 13/1000 | Loss: 0.00001077
Iteration 14/1000 | Loss: 0.00001076
Iteration 15/1000 | Loss: 0.00001071
Iteration 16/1000 | Loss: 0.00001070
Iteration 17/1000 | Loss: 0.00001065
Iteration 18/1000 | Loss: 0.00001063
Iteration 19/1000 | Loss: 0.00001063
Iteration 20/1000 | Loss: 0.00001063
Iteration 21/1000 | Loss: 0.00001062
Iteration 22/1000 | Loss: 0.00001062
Iteration 23/1000 | Loss: 0.00001061
Iteration 24/1000 | Loss: 0.00001060
Iteration 25/1000 | Loss: 0.00001059
Iteration 26/1000 | Loss: 0.00001059
Iteration 27/1000 | Loss: 0.00001058
Iteration 28/1000 | Loss: 0.00001058
Iteration 29/1000 | Loss: 0.00001058
Iteration 30/1000 | Loss: 0.00001058
Iteration 31/1000 | Loss: 0.00001058
Iteration 32/1000 | Loss: 0.00001058
Iteration 33/1000 | Loss: 0.00001057
Iteration 34/1000 | Loss: 0.00001053
Iteration 35/1000 | Loss: 0.00001053
Iteration 36/1000 | Loss: 0.00001053
Iteration 37/1000 | Loss: 0.00001053
Iteration 38/1000 | Loss: 0.00001052
Iteration 39/1000 | Loss: 0.00001052
Iteration 40/1000 | Loss: 0.00001052
Iteration 41/1000 | Loss: 0.00001052
Iteration 42/1000 | Loss: 0.00001050
Iteration 43/1000 | Loss: 0.00001050
Iteration 44/1000 | Loss: 0.00001049
Iteration 45/1000 | Loss: 0.00001047
Iteration 46/1000 | Loss: 0.00001047
Iteration 47/1000 | Loss: 0.00001047
Iteration 48/1000 | Loss: 0.00001047
Iteration 49/1000 | Loss: 0.00001047
Iteration 50/1000 | Loss: 0.00001047
Iteration 51/1000 | Loss: 0.00001046
Iteration 52/1000 | Loss: 0.00001046
Iteration 53/1000 | Loss: 0.00001046
Iteration 54/1000 | Loss: 0.00001045
Iteration 55/1000 | Loss: 0.00001045
Iteration 56/1000 | Loss: 0.00001044
Iteration 57/1000 | Loss: 0.00001044
Iteration 58/1000 | Loss: 0.00001044
Iteration 59/1000 | Loss: 0.00001043
Iteration 60/1000 | Loss: 0.00001043
Iteration 61/1000 | Loss: 0.00001043
Iteration 62/1000 | Loss: 0.00001043
Iteration 63/1000 | Loss: 0.00001043
Iteration 64/1000 | Loss: 0.00001043
Iteration 65/1000 | Loss: 0.00001042
Iteration 66/1000 | Loss: 0.00001042
Iteration 67/1000 | Loss: 0.00001042
Iteration 68/1000 | Loss: 0.00001042
Iteration 69/1000 | Loss: 0.00001042
Iteration 70/1000 | Loss: 0.00001041
Iteration 71/1000 | Loss: 0.00001041
Iteration 72/1000 | Loss: 0.00001041
Iteration 73/1000 | Loss: 0.00001041
Iteration 74/1000 | Loss: 0.00001041
Iteration 75/1000 | Loss: 0.00001041
Iteration 76/1000 | Loss: 0.00001041
Iteration 77/1000 | Loss: 0.00001040
Iteration 78/1000 | Loss: 0.00001040
Iteration 79/1000 | Loss: 0.00001040
Iteration 80/1000 | Loss: 0.00001040
Iteration 81/1000 | Loss: 0.00001040
Iteration 82/1000 | Loss: 0.00001040
Iteration 83/1000 | Loss: 0.00001040
Iteration 84/1000 | Loss: 0.00001039
Iteration 85/1000 | Loss: 0.00001039
Iteration 86/1000 | Loss: 0.00001039
Iteration 87/1000 | Loss: 0.00001039
Iteration 88/1000 | Loss: 0.00001039
Iteration 89/1000 | Loss: 0.00001039
Iteration 90/1000 | Loss: 0.00001039
Iteration 91/1000 | Loss: 0.00001039
Iteration 92/1000 | Loss: 0.00001038
Iteration 93/1000 | Loss: 0.00001038
Iteration 94/1000 | Loss: 0.00001038
Iteration 95/1000 | Loss: 0.00001038
Iteration 96/1000 | Loss: 0.00001038
Iteration 97/1000 | Loss: 0.00001038
Iteration 98/1000 | Loss: 0.00001037
Iteration 99/1000 | Loss: 0.00001037
Iteration 100/1000 | Loss: 0.00001036
Iteration 101/1000 | Loss: 0.00001036
Iteration 102/1000 | Loss: 0.00001036
Iteration 103/1000 | Loss: 0.00001036
Iteration 104/1000 | Loss: 0.00001035
Iteration 105/1000 | Loss: 0.00001035
Iteration 106/1000 | Loss: 0.00001035
Iteration 107/1000 | Loss: 0.00001035
Iteration 108/1000 | Loss: 0.00001034
Iteration 109/1000 | Loss: 0.00001034
Iteration 110/1000 | Loss: 0.00001034
Iteration 111/1000 | Loss: 0.00001033
Iteration 112/1000 | Loss: 0.00001033
Iteration 113/1000 | Loss: 0.00001032
Iteration 114/1000 | Loss: 0.00001032
Iteration 115/1000 | Loss: 0.00001032
Iteration 116/1000 | Loss: 0.00001032
Iteration 117/1000 | Loss: 0.00001032
Iteration 118/1000 | Loss: 0.00001032
Iteration 119/1000 | Loss: 0.00001031
Iteration 120/1000 | Loss: 0.00001031
Iteration 121/1000 | Loss: 0.00001031
Iteration 122/1000 | Loss: 0.00001031
Iteration 123/1000 | Loss: 0.00001031
Iteration 124/1000 | Loss: 0.00001031
Iteration 125/1000 | Loss: 0.00001030
Iteration 126/1000 | Loss: 0.00001030
Iteration 127/1000 | Loss: 0.00001030
Iteration 128/1000 | Loss: 0.00001030
Iteration 129/1000 | Loss: 0.00001030
Iteration 130/1000 | Loss: 0.00001030
Iteration 131/1000 | Loss: 0.00001030
Iteration 132/1000 | Loss: 0.00001030
Iteration 133/1000 | Loss: 0.00001030
Iteration 134/1000 | Loss: 0.00001030
Iteration 135/1000 | Loss: 0.00001030
Iteration 136/1000 | Loss: 0.00001030
Iteration 137/1000 | Loss: 0.00001030
Iteration 138/1000 | Loss: 0.00001030
Iteration 139/1000 | Loss: 0.00001029
Iteration 140/1000 | Loss: 0.00001029
Iteration 141/1000 | Loss: 0.00001029
Iteration 142/1000 | Loss: 0.00001029
Iteration 143/1000 | Loss: 0.00001029
Iteration 144/1000 | Loss: 0.00001029
Iteration 145/1000 | Loss: 0.00001029
Iteration 146/1000 | Loss: 0.00001029
Iteration 147/1000 | Loss: 0.00001029
Iteration 148/1000 | Loss: 0.00001029
Iteration 149/1000 | Loss: 0.00001028
Iteration 150/1000 | Loss: 0.00001028
Iteration 151/1000 | Loss: 0.00001028
Iteration 152/1000 | Loss: 0.00001028
Iteration 153/1000 | Loss: 0.00001028
Iteration 154/1000 | Loss: 0.00001028
Iteration 155/1000 | Loss: 0.00001028
Iteration 156/1000 | Loss: 0.00001028
Iteration 157/1000 | Loss: 0.00001028
Iteration 158/1000 | Loss: 0.00001028
Iteration 159/1000 | Loss: 0.00001028
Iteration 160/1000 | Loss: 0.00001028
Iteration 161/1000 | Loss: 0.00001028
Iteration 162/1000 | Loss: 0.00001028
Iteration 163/1000 | Loss: 0.00001028
Iteration 164/1000 | Loss: 0.00001028
Iteration 165/1000 | Loss: 0.00001028
Iteration 166/1000 | Loss: 0.00001028
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 166. Stopping optimization.
Last 5 losses: [1.0278172339894809e-05, 1.0278172339894809e-05, 1.0278172339894809e-05, 1.0278172339894809e-05, 1.0278172339894809e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0278172339894809e-05

Optimization complete. Final v2v error: 2.7554383277893066 mm

Highest mean error: 2.963974952697754 mm for frame 103

Lowest mean error: 2.6679420471191406 mm for frame 90

Saving results

Total time: 35.066770792007446
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_025/1050/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_025/1050.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_025/1050
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00466037
Iteration 2/25 | Loss: 0.00126727
Iteration 3/25 | Loss: 0.00118621
Iteration 4/25 | Loss: 0.00117552
Iteration 5/25 | Loss: 0.00117260
Iteration 6/25 | Loss: 0.00117211
Iteration 7/25 | Loss: 0.00117211
Iteration 8/25 | Loss: 0.00117211
Iteration 9/25 | Loss: 0.00117211
Iteration 10/25 | Loss: 0.00117211
Iteration 11/25 | Loss: 0.00117211
Iteration 12/25 | Loss: 0.00117211
Iteration 13/25 | Loss: 0.00117211
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0011721118353307247, 0.0011721118353307247, 0.0011721118353307247, 0.0011721118353307247, 0.0011721118353307247]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011721118353307247

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.08472061
Iteration 2/25 | Loss: 0.00084057
Iteration 3/25 | Loss: 0.00084057
Iteration 4/25 | Loss: 0.00084057
Iteration 5/25 | Loss: 0.00084057
Iteration 6/25 | Loss: 0.00084057
Iteration 7/25 | Loss: 0.00084057
Iteration 8/25 | Loss: 0.00084057
Iteration 9/25 | Loss: 0.00084057
Iteration 10/25 | Loss: 0.00084057
Iteration 11/25 | Loss: 0.00084057
Iteration 12/25 | Loss: 0.00084057
Iteration 13/25 | Loss: 0.00084057
Iteration 14/25 | Loss: 0.00084057
Iteration 15/25 | Loss: 0.00084057
Iteration 16/25 | Loss: 0.00084057
Iteration 17/25 | Loss: 0.00084057
Iteration 18/25 | Loss: 0.00084056
Iteration 19/25 | Loss: 0.00084056
Iteration 20/25 | Loss: 0.00084056
Iteration 21/25 | Loss: 0.00084056
Iteration 22/25 | Loss: 0.00084056
Iteration 23/25 | Loss: 0.00084056
Iteration 24/25 | Loss: 0.00084056
Iteration 25/25 | Loss: 0.00084056

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00084056
Iteration 2/1000 | Loss: 0.00002403
Iteration 3/1000 | Loss: 0.00001821
Iteration 4/1000 | Loss: 0.00001690
Iteration 5/1000 | Loss: 0.00001610
Iteration 6/1000 | Loss: 0.00001557
Iteration 7/1000 | Loss: 0.00001508
Iteration 8/1000 | Loss: 0.00001476
Iteration 9/1000 | Loss: 0.00001456
Iteration 10/1000 | Loss: 0.00001432
Iteration 11/1000 | Loss: 0.00001417
Iteration 12/1000 | Loss: 0.00001416
Iteration 13/1000 | Loss: 0.00001409
Iteration 14/1000 | Loss: 0.00001402
Iteration 15/1000 | Loss: 0.00001399
Iteration 16/1000 | Loss: 0.00001393
Iteration 17/1000 | Loss: 0.00001390
Iteration 18/1000 | Loss: 0.00001390
Iteration 19/1000 | Loss: 0.00001390
Iteration 20/1000 | Loss: 0.00001389
Iteration 21/1000 | Loss: 0.00001389
Iteration 22/1000 | Loss: 0.00001387
Iteration 23/1000 | Loss: 0.00001385
Iteration 24/1000 | Loss: 0.00001384
Iteration 25/1000 | Loss: 0.00001384
Iteration 26/1000 | Loss: 0.00001384
Iteration 27/1000 | Loss: 0.00001382
Iteration 28/1000 | Loss: 0.00001380
Iteration 29/1000 | Loss: 0.00001379
Iteration 30/1000 | Loss: 0.00001378
Iteration 31/1000 | Loss: 0.00001377
Iteration 32/1000 | Loss: 0.00001377
Iteration 33/1000 | Loss: 0.00001376
Iteration 34/1000 | Loss: 0.00001376
Iteration 35/1000 | Loss: 0.00001375
Iteration 36/1000 | Loss: 0.00001375
Iteration 37/1000 | Loss: 0.00001375
Iteration 38/1000 | Loss: 0.00001374
Iteration 39/1000 | Loss: 0.00001373
Iteration 40/1000 | Loss: 0.00001373
Iteration 41/1000 | Loss: 0.00001372
Iteration 42/1000 | Loss: 0.00001372
Iteration 43/1000 | Loss: 0.00001371
Iteration 44/1000 | Loss: 0.00001371
Iteration 45/1000 | Loss: 0.00001370
Iteration 46/1000 | Loss: 0.00001370
Iteration 47/1000 | Loss: 0.00001369
Iteration 48/1000 | Loss: 0.00001369
Iteration 49/1000 | Loss: 0.00001369
Iteration 50/1000 | Loss: 0.00001369
Iteration 51/1000 | Loss: 0.00001369
Iteration 52/1000 | Loss: 0.00001368
Iteration 53/1000 | Loss: 0.00001368
Iteration 54/1000 | Loss: 0.00001368
Iteration 55/1000 | Loss: 0.00001368
Iteration 56/1000 | Loss: 0.00001367
Iteration 57/1000 | Loss: 0.00001367
Iteration 58/1000 | Loss: 0.00001367
Iteration 59/1000 | Loss: 0.00001365
Iteration 60/1000 | Loss: 0.00001365
Iteration 61/1000 | Loss: 0.00001364
Iteration 62/1000 | Loss: 0.00001364
Iteration 63/1000 | Loss: 0.00001364
Iteration 64/1000 | Loss: 0.00001364
Iteration 65/1000 | Loss: 0.00001363
Iteration 66/1000 | Loss: 0.00001362
Iteration 67/1000 | Loss: 0.00001362
Iteration 68/1000 | Loss: 0.00001362
Iteration 69/1000 | Loss: 0.00001361
Iteration 70/1000 | Loss: 0.00001361
Iteration 71/1000 | Loss: 0.00001361
Iteration 72/1000 | Loss: 0.00001360
Iteration 73/1000 | Loss: 0.00001360
Iteration 74/1000 | Loss: 0.00001360
Iteration 75/1000 | Loss: 0.00001359
Iteration 76/1000 | Loss: 0.00001359
Iteration 77/1000 | Loss: 0.00001359
Iteration 78/1000 | Loss: 0.00001359
Iteration 79/1000 | Loss: 0.00001359
Iteration 80/1000 | Loss: 0.00001358
Iteration 81/1000 | Loss: 0.00001358
Iteration 82/1000 | Loss: 0.00001358
Iteration 83/1000 | Loss: 0.00001358
Iteration 84/1000 | Loss: 0.00001358
Iteration 85/1000 | Loss: 0.00001358
Iteration 86/1000 | Loss: 0.00001358
Iteration 87/1000 | Loss: 0.00001358
Iteration 88/1000 | Loss: 0.00001358
Iteration 89/1000 | Loss: 0.00001358
Iteration 90/1000 | Loss: 0.00001357
Iteration 91/1000 | Loss: 0.00001357
Iteration 92/1000 | Loss: 0.00001357
Iteration 93/1000 | Loss: 0.00001357
Iteration 94/1000 | Loss: 0.00001357
Iteration 95/1000 | Loss: 0.00001357
Iteration 96/1000 | Loss: 0.00001357
Iteration 97/1000 | Loss: 0.00001357
Iteration 98/1000 | Loss: 0.00001357
Iteration 99/1000 | Loss: 0.00001357
Iteration 100/1000 | Loss: 0.00001357
Iteration 101/1000 | Loss: 0.00001357
Iteration 102/1000 | Loss: 0.00001357
Iteration 103/1000 | Loss: 0.00001357
Iteration 104/1000 | Loss: 0.00001357
Iteration 105/1000 | Loss: 0.00001357
Iteration 106/1000 | Loss: 0.00001357
Iteration 107/1000 | Loss: 0.00001357
Iteration 108/1000 | Loss: 0.00001357
Iteration 109/1000 | Loss: 0.00001357
Iteration 110/1000 | Loss: 0.00001357
Iteration 111/1000 | Loss: 0.00001357
Iteration 112/1000 | Loss: 0.00001357
Iteration 113/1000 | Loss: 0.00001357
Iteration 114/1000 | Loss: 0.00001357
Iteration 115/1000 | Loss: 0.00001357
Iteration 116/1000 | Loss: 0.00001357
Iteration 117/1000 | Loss: 0.00001357
Iteration 118/1000 | Loss: 0.00001357
Iteration 119/1000 | Loss: 0.00001357
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 119. Stopping optimization.
Last 5 losses: [1.3570031114795711e-05, 1.3570031114795711e-05, 1.3570031114795711e-05, 1.3570031114795711e-05, 1.3570031114795711e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3570031114795711e-05

Optimization complete. Final v2v error: 3.1200852394104004 mm

Highest mean error: 3.449822425842285 mm for frame 82

Lowest mean error: 2.844477415084839 mm for frame 14

Saving results

Total time: 34.081620931625366
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_025/1080/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_025/1080.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_025/1080
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00641772
Iteration 2/25 | Loss: 0.00139539
Iteration 3/25 | Loss: 0.00123168
Iteration 4/25 | Loss: 0.00119794
Iteration 5/25 | Loss: 0.00119027
Iteration 6/25 | Loss: 0.00119677
Iteration 7/25 | Loss: 0.00118929
Iteration 8/25 | Loss: 0.00118410
Iteration 9/25 | Loss: 0.00118090
Iteration 10/25 | Loss: 0.00117929
Iteration 11/25 | Loss: 0.00117856
Iteration 12/25 | Loss: 0.00117828
Iteration 13/25 | Loss: 0.00117810
Iteration 14/25 | Loss: 0.00117729
Iteration 15/25 | Loss: 0.00117688
Iteration 16/25 | Loss: 0.00117666
Iteration 17/25 | Loss: 0.00117652
Iteration 18/25 | Loss: 0.00117646
Iteration 19/25 | Loss: 0.00117586
Iteration 20/25 | Loss: 0.00117539
Iteration 21/25 | Loss: 0.00117520
Iteration 22/25 | Loss: 0.00117513
Iteration 23/25 | Loss: 0.00117513
Iteration 24/25 | Loss: 0.00117513
Iteration 25/25 | Loss: 0.00117513

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.71903920
Iteration 2/25 | Loss: 0.00078838
Iteration 3/25 | Loss: 0.00078836
Iteration 4/25 | Loss: 0.00078836
Iteration 5/25 | Loss: 0.00078836
Iteration 6/25 | Loss: 0.00078836
Iteration 7/25 | Loss: 0.00078835
Iteration 8/25 | Loss: 0.00078835
Iteration 9/25 | Loss: 0.00078835
Iteration 10/25 | Loss: 0.00078835
Iteration 11/25 | Loss: 0.00078835
Iteration 12/25 | Loss: 0.00078835
Iteration 13/25 | Loss: 0.00078835
Iteration 14/25 | Loss: 0.00078835
Iteration 15/25 | Loss: 0.00078835
Iteration 16/25 | Loss: 0.00078835
Iteration 17/25 | Loss: 0.00078835
Iteration 18/25 | Loss: 0.00078835
Iteration 19/25 | Loss: 0.00078835
Iteration 20/25 | Loss: 0.00078835
Iteration 21/25 | Loss: 0.00078835
Iteration 22/25 | Loss: 0.00078835
Iteration 23/25 | Loss: 0.00078835
Iteration 24/25 | Loss: 0.00078835
Iteration 25/25 | Loss: 0.00078835

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00078835
Iteration 2/1000 | Loss: 0.00002993
Iteration 3/1000 | Loss: 0.00001796
Iteration 4/1000 | Loss: 0.00001633
Iteration 5/1000 | Loss: 0.00001562
Iteration 6/1000 | Loss: 0.00001502
Iteration 7/1000 | Loss: 0.00005360
Iteration 8/1000 | Loss: 0.00001464
Iteration 9/1000 | Loss: 0.00001448
Iteration 10/1000 | Loss: 0.00002773
Iteration 11/1000 | Loss: 0.00001839
Iteration 12/1000 | Loss: 0.00001421
Iteration 13/1000 | Loss: 0.00002182
Iteration 14/1000 | Loss: 0.00001402
Iteration 15/1000 | Loss: 0.00001393
Iteration 16/1000 | Loss: 0.00001390
Iteration 17/1000 | Loss: 0.00001389
Iteration 18/1000 | Loss: 0.00001389
Iteration 19/1000 | Loss: 0.00001387
Iteration 20/1000 | Loss: 0.00001386
Iteration 21/1000 | Loss: 0.00001386
Iteration 22/1000 | Loss: 0.00001385
Iteration 23/1000 | Loss: 0.00001385
Iteration 24/1000 | Loss: 0.00001383
Iteration 25/1000 | Loss: 0.00001381
Iteration 26/1000 | Loss: 0.00001380
Iteration 27/1000 | Loss: 0.00001379
Iteration 28/1000 | Loss: 0.00001378
Iteration 29/1000 | Loss: 0.00002689
Iteration 30/1000 | Loss: 0.00002887
Iteration 31/1000 | Loss: 0.00004338
Iteration 32/1000 | Loss: 0.00001383
Iteration 33/1000 | Loss: 0.00001370
Iteration 34/1000 | Loss: 0.00001653
Iteration 35/1000 | Loss: 0.00001370
Iteration 36/1000 | Loss: 0.00001370
Iteration 37/1000 | Loss: 0.00001370
Iteration 38/1000 | Loss: 0.00001370
Iteration 39/1000 | Loss: 0.00001370
Iteration 40/1000 | Loss: 0.00001370
Iteration 41/1000 | Loss: 0.00001370
Iteration 42/1000 | Loss: 0.00001369
Iteration 43/1000 | Loss: 0.00001369
Iteration 44/1000 | Loss: 0.00001369
Iteration 45/1000 | Loss: 0.00001369
Iteration 46/1000 | Loss: 0.00001369
Iteration 47/1000 | Loss: 0.00001368
Iteration 48/1000 | Loss: 0.00001368
Iteration 49/1000 | Loss: 0.00001368
Iteration 50/1000 | Loss: 0.00001367
Iteration 51/1000 | Loss: 0.00001367
Iteration 52/1000 | Loss: 0.00001367
Iteration 53/1000 | Loss: 0.00001367
Iteration 54/1000 | Loss: 0.00001367
Iteration 55/1000 | Loss: 0.00001367
Iteration 56/1000 | Loss: 0.00001367
Iteration 57/1000 | Loss: 0.00001367
Iteration 58/1000 | Loss: 0.00001367
Iteration 59/1000 | Loss: 0.00001367
Iteration 60/1000 | Loss: 0.00001367
Iteration 61/1000 | Loss: 0.00001366
Iteration 62/1000 | Loss: 0.00001366
Iteration 63/1000 | Loss: 0.00001590
Iteration 64/1000 | Loss: 0.00001590
Iteration 65/1000 | Loss: 0.00001363
Iteration 66/1000 | Loss: 0.00001363
Iteration 67/1000 | Loss: 0.00001363
Iteration 68/1000 | Loss: 0.00001363
Iteration 69/1000 | Loss: 0.00001363
Iteration 70/1000 | Loss: 0.00001363
Iteration 71/1000 | Loss: 0.00001363
Iteration 72/1000 | Loss: 0.00001362
Iteration 73/1000 | Loss: 0.00001362
Iteration 74/1000 | Loss: 0.00001362
Iteration 75/1000 | Loss: 0.00001362
Iteration 76/1000 | Loss: 0.00001362
Iteration 77/1000 | Loss: 0.00001362
Iteration 78/1000 | Loss: 0.00001362
Iteration 79/1000 | Loss: 0.00001362
Iteration 80/1000 | Loss: 0.00001361
Iteration 81/1000 | Loss: 0.00001361
Iteration 82/1000 | Loss: 0.00001361
Iteration 83/1000 | Loss: 0.00001361
Iteration 84/1000 | Loss: 0.00001360
Iteration 85/1000 | Loss: 0.00001360
Iteration 86/1000 | Loss: 0.00001360
Iteration 87/1000 | Loss: 0.00001360
Iteration 88/1000 | Loss: 0.00001359
Iteration 89/1000 | Loss: 0.00001359
Iteration 90/1000 | Loss: 0.00001925
Iteration 91/1000 | Loss: 0.00001355
Iteration 92/1000 | Loss: 0.00001355
Iteration 93/1000 | Loss: 0.00001355
Iteration 94/1000 | Loss: 0.00001355
Iteration 95/1000 | Loss: 0.00001355
Iteration 96/1000 | Loss: 0.00001355
Iteration 97/1000 | Loss: 0.00001355
Iteration 98/1000 | Loss: 0.00001355
Iteration 99/1000 | Loss: 0.00001355
Iteration 100/1000 | Loss: 0.00001355
Iteration 101/1000 | Loss: 0.00001355
Iteration 102/1000 | Loss: 0.00001355
Iteration 103/1000 | Loss: 0.00001545
Iteration 104/1000 | Loss: 0.00001355
Iteration 105/1000 | Loss: 0.00001355
Iteration 106/1000 | Loss: 0.00001355
Iteration 107/1000 | Loss: 0.00001355
Iteration 108/1000 | Loss: 0.00001355
Iteration 109/1000 | Loss: 0.00001355
Iteration 110/1000 | Loss: 0.00001355
Iteration 111/1000 | Loss: 0.00001355
Iteration 112/1000 | Loss: 0.00001355
Iteration 113/1000 | Loss: 0.00001355
Iteration 114/1000 | Loss: 0.00001355
Iteration 115/1000 | Loss: 0.00001354
Iteration 116/1000 | Loss: 0.00001354
Iteration 117/1000 | Loss: 0.00001354
Iteration 118/1000 | Loss: 0.00001354
Iteration 119/1000 | Loss: 0.00001354
Iteration 120/1000 | Loss: 0.00001354
Iteration 121/1000 | Loss: 0.00001354
Iteration 122/1000 | Loss: 0.00001354
Iteration 123/1000 | Loss: 0.00001354
Iteration 124/1000 | Loss: 0.00001354
Iteration 125/1000 | Loss: 0.00001354
Iteration 126/1000 | Loss: 0.00001354
Iteration 127/1000 | Loss: 0.00001353
Iteration 128/1000 | Loss: 0.00001353
Iteration 129/1000 | Loss: 0.00001353
Iteration 130/1000 | Loss: 0.00001353
Iteration 131/1000 | Loss: 0.00001353
Iteration 132/1000 | Loss: 0.00001353
Iteration 133/1000 | Loss: 0.00001353
Iteration 134/1000 | Loss: 0.00001353
Iteration 135/1000 | Loss: 0.00001353
Iteration 136/1000 | Loss: 0.00001352
Iteration 137/1000 | Loss: 0.00001352
Iteration 138/1000 | Loss: 0.00001352
Iteration 139/1000 | Loss: 0.00001352
Iteration 140/1000 | Loss: 0.00001351
Iteration 141/1000 | Loss: 0.00001621
Iteration 142/1000 | Loss: 0.00001351
Iteration 143/1000 | Loss: 0.00001350
Iteration 144/1000 | Loss: 0.00001350
Iteration 145/1000 | Loss: 0.00001350
Iteration 146/1000 | Loss: 0.00001350
Iteration 147/1000 | Loss: 0.00001350
Iteration 148/1000 | Loss: 0.00001350
Iteration 149/1000 | Loss: 0.00001350
Iteration 150/1000 | Loss: 0.00001350
Iteration 151/1000 | Loss: 0.00001350
Iteration 152/1000 | Loss: 0.00001350
Iteration 153/1000 | Loss: 0.00001350
Iteration 154/1000 | Loss: 0.00001349
Iteration 155/1000 | Loss: 0.00001349
Iteration 156/1000 | Loss: 0.00001349
Iteration 157/1000 | Loss: 0.00001349
Iteration 158/1000 | Loss: 0.00001349
Iteration 159/1000 | Loss: 0.00001349
Iteration 160/1000 | Loss: 0.00001349
Iteration 161/1000 | Loss: 0.00001349
Iteration 162/1000 | Loss: 0.00001349
Iteration 163/1000 | Loss: 0.00001349
Iteration 164/1000 | Loss: 0.00001349
Iteration 165/1000 | Loss: 0.00001349
Iteration 166/1000 | Loss: 0.00001349
Iteration 167/1000 | Loss: 0.00001349
Iteration 168/1000 | Loss: 0.00001348
Iteration 169/1000 | Loss: 0.00001348
Iteration 170/1000 | Loss: 0.00001614
Iteration 171/1000 | Loss: 0.00001347
Iteration 172/1000 | Loss: 0.00001346
Iteration 173/1000 | Loss: 0.00001346
Iteration 174/1000 | Loss: 0.00001346
Iteration 175/1000 | Loss: 0.00001346
Iteration 176/1000 | Loss: 0.00001345
Iteration 177/1000 | Loss: 0.00001345
Iteration 178/1000 | Loss: 0.00001345
Iteration 179/1000 | Loss: 0.00001345
Iteration 180/1000 | Loss: 0.00001345
Iteration 181/1000 | Loss: 0.00001345
Iteration 182/1000 | Loss: 0.00001345
Iteration 183/1000 | Loss: 0.00001345
Iteration 184/1000 | Loss: 0.00001345
Iteration 185/1000 | Loss: 0.00001345
Iteration 186/1000 | Loss: 0.00001345
Iteration 187/1000 | Loss: 0.00001345
Iteration 188/1000 | Loss: 0.00001345
Iteration 189/1000 | Loss: 0.00001345
Iteration 190/1000 | Loss: 0.00001345
Iteration 191/1000 | Loss: 0.00001345
Iteration 192/1000 | Loss: 0.00001345
Iteration 193/1000 | Loss: 0.00001345
Iteration 194/1000 | Loss: 0.00001344
Iteration 195/1000 | Loss: 0.00001344
Iteration 196/1000 | Loss: 0.00001344
Iteration 197/1000 | Loss: 0.00001344
Iteration 198/1000 | Loss: 0.00001344
Iteration 199/1000 | Loss: 0.00001344
Iteration 200/1000 | Loss: 0.00001344
Iteration 201/1000 | Loss: 0.00001344
Iteration 202/1000 | Loss: 0.00001344
Iteration 203/1000 | Loss: 0.00001344
Iteration 204/1000 | Loss: 0.00001344
Iteration 205/1000 | Loss: 0.00001344
Iteration 206/1000 | Loss: 0.00001344
Iteration 207/1000 | Loss: 0.00001344
Iteration 208/1000 | Loss: 0.00001344
Iteration 209/1000 | Loss: 0.00001344
Iteration 210/1000 | Loss: 0.00001344
Iteration 211/1000 | Loss: 0.00001344
Iteration 212/1000 | Loss: 0.00001344
Iteration 213/1000 | Loss: 0.00001344
Iteration 214/1000 | Loss: 0.00001344
Iteration 215/1000 | Loss: 0.00001344
Iteration 216/1000 | Loss: 0.00001344
Iteration 217/1000 | Loss: 0.00001344
Iteration 218/1000 | Loss: 0.00001344
Iteration 219/1000 | Loss: 0.00001344
Iteration 220/1000 | Loss: 0.00001344
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 220. Stopping optimization.
Last 5 losses: [1.3442375347949564e-05, 1.3442375347949564e-05, 1.3442375347949564e-05, 1.3442375347949564e-05, 1.3442375347949564e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3442375347949564e-05

Optimization complete. Final v2v error: 3.085258960723877 mm

Highest mean error: 3.9763026237487793 mm for frame 108

Lowest mean error: 2.718384027481079 mm for frame 155

Saving results

Total time: 91.79625391960144
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_025/1019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_025/1019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_025/1019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01051047
Iteration 2/25 | Loss: 0.00281045
Iteration 3/25 | Loss: 0.00188950
Iteration 4/25 | Loss: 0.00173958
Iteration 5/25 | Loss: 0.00172782
Iteration 6/25 | Loss: 0.00165624
Iteration 7/25 | Loss: 0.00162664
Iteration 8/25 | Loss: 0.00160705
Iteration 9/25 | Loss: 0.00161102
Iteration 10/25 | Loss: 0.00159312
Iteration 11/25 | Loss: 0.00158142
Iteration 12/25 | Loss: 0.00158107
Iteration 13/25 | Loss: 0.00156871
Iteration 14/25 | Loss: 0.00156609
Iteration 15/25 | Loss: 0.00156891
Iteration 16/25 | Loss: 0.00157010
Iteration 17/25 | Loss: 0.00156369
Iteration 18/25 | Loss: 0.00156134
Iteration 19/25 | Loss: 0.00156055
Iteration 20/25 | Loss: 0.00156030
Iteration 21/25 | Loss: 0.00156011
Iteration 22/25 | Loss: 0.00155997
Iteration 23/25 | Loss: 0.00155978
Iteration 24/25 | Loss: 0.00155954
Iteration 25/25 | Loss: 0.00155933

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.16808867
Iteration 2/25 | Loss: 0.00411966
Iteration 3/25 | Loss: 0.00411966
Iteration 4/25 | Loss: 0.00411966
Iteration 5/25 | Loss: 0.00411966
Iteration 6/25 | Loss: 0.00411966
Iteration 7/25 | Loss: 0.00411966
Iteration 8/25 | Loss: 0.00411966
Iteration 9/25 | Loss: 0.00411966
Iteration 10/25 | Loss: 0.00411966
Iteration 11/25 | Loss: 0.00411966
Iteration 12/25 | Loss: 0.00411966
Iteration 13/25 | Loss: 0.00411966
Iteration 14/25 | Loss: 0.00411966
Iteration 15/25 | Loss: 0.00411966
Iteration 16/25 | Loss: 0.00411966
Iteration 17/25 | Loss: 0.00411966
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0041196588426828384, 0.0041196588426828384, 0.0041196588426828384, 0.0041196588426828384, 0.0041196588426828384]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0041196588426828384

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00411966
Iteration 2/1000 | Loss: 0.00279134
Iteration 3/1000 | Loss: 0.00307817
Iteration 4/1000 | Loss: 0.00386510
Iteration 5/1000 | Loss: 0.00202431
Iteration 6/1000 | Loss: 0.00230492
Iteration 7/1000 | Loss: 0.00116863
Iteration 8/1000 | Loss: 0.00089376
Iteration 9/1000 | Loss: 0.00261646
Iteration 10/1000 | Loss: 0.00120746
Iteration 11/1000 | Loss: 0.00029506
Iteration 12/1000 | Loss: 0.00086938
Iteration 13/1000 | Loss: 0.00089538
Iteration 14/1000 | Loss: 0.00071032
Iteration 15/1000 | Loss: 0.00035218
Iteration 16/1000 | Loss: 0.00023719
Iteration 17/1000 | Loss: 0.00033894
Iteration 18/1000 | Loss: 0.00031436
Iteration 19/1000 | Loss: 0.00014207
Iteration 20/1000 | Loss: 0.00011992
Iteration 21/1000 | Loss: 0.00011150
Iteration 22/1000 | Loss: 0.00032794
Iteration 23/1000 | Loss: 0.00029864
Iteration 24/1000 | Loss: 0.00030622
Iteration 25/1000 | Loss: 0.00010479
Iteration 26/1000 | Loss: 0.00032730
Iteration 27/1000 | Loss: 0.00036530
Iteration 28/1000 | Loss: 0.00033047
Iteration 29/1000 | Loss: 0.00033865
Iteration 30/1000 | Loss: 0.00033635
Iteration 31/1000 | Loss: 0.00030073
Iteration 32/1000 | Loss: 0.00039318
Iteration 33/1000 | Loss: 0.00029614
Iteration 34/1000 | Loss: 0.00040444
Iteration 35/1000 | Loss: 0.00050403
Iteration 36/1000 | Loss: 0.00045077
Iteration 37/1000 | Loss: 0.00037639
Iteration 38/1000 | Loss: 0.00048251
Iteration 39/1000 | Loss: 0.00070006
Iteration 40/1000 | Loss: 0.00084568
Iteration 41/1000 | Loss: 0.00228953
Iteration 42/1000 | Loss: 0.00381606
Iteration 43/1000 | Loss: 0.00203105
Iteration 44/1000 | Loss: 0.00115145
Iteration 45/1000 | Loss: 0.00014748
Iteration 46/1000 | Loss: 0.00030347
Iteration 47/1000 | Loss: 0.00013781
Iteration 48/1000 | Loss: 0.00046639
Iteration 49/1000 | Loss: 0.00067169
Iteration 50/1000 | Loss: 0.00024883
Iteration 51/1000 | Loss: 0.00008320
Iteration 52/1000 | Loss: 0.00006299
Iteration 53/1000 | Loss: 0.00018780
Iteration 54/1000 | Loss: 0.00012614
Iteration 55/1000 | Loss: 0.00032663
Iteration 56/1000 | Loss: 0.00005430
Iteration 57/1000 | Loss: 0.00006698
Iteration 58/1000 | Loss: 0.00017237
Iteration 59/1000 | Loss: 0.00010284
Iteration 60/1000 | Loss: 0.00012124
Iteration 61/1000 | Loss: 0.00009394
Iteration 62/1000 | Loss: 0.00009182
Iteration 63/1000 | Loss: 0.00004745
Iteration 64/1000 | Loss: 0.00004341
Iteration 65/1000 | Loss: 0.00018686
Iteration 66/1000 | Loss: 0.00013081
Iteration 67/1000 | Loss: 0.00019594
Iteration 68/1000 | Loss: 0.00008863
Iteration 69/1000 | Loss: 0.00014977
Iteration 70/1000 | Loss: 0.00013511
Iteration 71/1000 | Loss: 0.00020527
Iteration 72/1000 | Loss: 0.00019387
Iteration 73/1000 | Loss: 0.00010069
Iteration 74/1000 | Loss: 0.00019338
Iteration 75/1000 | Loss: 0.00006488
Iteration 76/1000 | Loss: 0.00012378
Iteration 77/1000 | Loss: 0.00016167
Iteration 78/1000 | Loss: 0.00018229
Iteration 79/1000 | Loss: 0.00023428
Iteration 80/1000 | Loss: 0.00015863
Iteration 81/1000 | Loss: 0.00019571
Iteration 82/1000 | Loss: 0.00021186
Iteration 83/1000 | Loss: 0.00013832
Iteration 84/1000 | Loss: 0.00018537
Iteration 85/1000 | Loss: 0.00020466
Iteration 86/1000 | Loss: 0.00017697
Iteration 87/1000 | Loss: 0.00023715
Iteration 88/1000 | Loss: 0.00015013
Iteration 89/1000 | Loss: 0.00011346
Iteration 90/1000 | Loss: 0.00017404
Iteration 91/1000 | Loss: 0.00011407
Iteration 92/1000 | Loss: 0.00005925
Iteration 93/1000 | Loss: 0.00018208
Iteration 94/1000 | Loss: 0.00015374
Iteration 95/1000 | Loss: 0.00015432
Iteration 96/1000 | Loss: 0.00006848
Iteration 97/1000 | Loss: 0.00018667
Iteration 98/1000 | Loss: 0.00019556
Iteration 99/1000 | Loss: 0.00014392
Iteration 100/1000 | Loss: 0.00005045
Iteration 101/1000 | Loss: 0.00003354
Iteration 102/1000 | Loss: 0.00003284
Iteration 103/1000 | Loss: 0.00013962
Iteration 104/1000 | Loss: 0.00022938
Iteration 105/1000 | Loss: 0.00007399
Iteration 106/1000 | Loss: 0.00013726
Iteration 107/1000 | Loss: 0.00018903
Iteration 108/1000 | Loss: 0.00005358
Iteration 109/1000 | Loss: 0.00014267
Iteration 110/1000 | Loss: 0.00025733
Iteration 111/1000 | Loss: 0.00005398
Iteration 112/1000 | Loss: 0.00008723
Iteration 113/1000 | Loss: 0.00014624
Iteration 114/1000 | Loss: 0.00013713
Iteration 115/1000 | Loss: 0.00004601
Iteration 116/1000 | Loss: 0.00003182
Iteration 117/1000 | Loss: 0.00003146
Iteration 118/1000 | Loss: 0.00003071
Iteration 119/1000 | Loss: 0.00003041
Iteration 120/1000 | Loss: 0.00011621
Iteration 121/1000 | Loss: 0.00006387
Iteration 122/1000 | Loss: 0.00012103
Iteration 123/1000 | Loss: 0.00012417
Iteration 124/1000 | Loss: 0.00011564
Iteration 125/1000 | Loss: 0.00007811
Iteration 126/1000 | Loss: 0.00008399
Iteration 127/1000 | Loss: 0.00003229
Iteration 128/1000 | Loss: 0.00007311
Iteration 129/1000 | Loss: 0.00003041
Iteration 130/1000 | Loss: 0.00009524
Iteration 131/1000 | Loss: 0.00007786
Iteration 132/1000 | Loss: 0.00010482
Iteration 133/1000 | Loss: 0.00007004
Iteration 134/1000 | Loss: 0.00014529
Iteration 135/1000 | Loss: 0.00004083
Iteration 136/1000 | Loss: 0.00019229
Iteration 137/1000 | Loss: 0.00003569
Iteration 138/1000 | Loss: 0.00033122
Iteration 139/1000 | Loss: 0.00013063
Iteration 140/1000 | Loss: 0.00026767
Iteration 141/1000 | Loss: 0.00011893
Iteration 142/1000 | Loss: 0.00027637
Iteration 143/1000 | Loss: 0.00020034
Iteration 144/1000 | Loss: 0.00006290
Iteration 145/1000 | Loss: 0.00004134
Iteration 146/1000 | Loss: 0.00003554
Iteration 147/1000 | Loss: 0.00003291
Iteration 148/1000 | Loss: 0.00003161
Iteration 149/1000 | Loss: 0.00003048
Iteration 150/1000 | Loss: 0.00002979
Iteration 151/1000 | Loss: 0.00002942
Iteration 152/1000 | Loss: 0.00002912
Iteration 153/1000 | Loss: 0.00002896
Iteration 154/1000 | Loss: 0.00002889
Iteration 155/1000 | Loss: 0.00002883
Iteration 156/1000 | Loss: 0.00002881
Iteration 157/1000 | Loss: 0.00002878
Iteration 158/1000 | Loss: 0.00002866
Iteration 159/1000 | Loss: 0.00002863
Iteration 160/1000 | Loss: 0.00002858
Iteration 161/1000 | Loss: 0.00002857
Iteration 162/1000 | Loss: 0.00002856
Iteration 163/1000 | Loss: 0.00002856
Iteration 164/1000 | Loss: 0.00002851
Iteration 165/1000 | Loss: 0.00002850
Iteration 166/1000 | Loss: 0.00002847
Iteration 167/1000 | Loss: 0.00002847
Iteration 168/1000 | Loss: 0.00002846
Iteration 169/1000 | Loss: 0.00002846
Iteration 170/1000 | Loss: 0.00002844
Iteration 171/1000 | Loss: 0.00002844
Iteration 172/1000 | Loss: 0.00002844
Iteration 173/1000 | Loss: 0.00002844
Iteration 174/1000 | Loss: 0.00002843
Iteration 175/1000 | Loss: 0.00002843
Iteration 176/1000 | Loss: 0.00002843
Iteration 177/1000 | Loss: 0.00002843
Iteration 178/1000 | Loss: 0.00002842
Iteration 179/1000 | Loss: 0.00002842
Iteration 180/1000 | Loss: 0.00002842
Iteration 181/1000 | Loss: 0.00002842
Iteration 182/1000 | Loss: 0.00002842
Iteration 183/1000 | Loss: 0.00002841
Iteration 184/1000 | Loss: 0.00002841
Iteration 185/1000 | Loss: 0.00002841
Iteration 186/1000 | Loss: 0.00002841
Iteration 187/1000 | Loss: 0.00002840
Iteration 188/1000 | Loss: 0.00002840
Iteration 189/1000 | Loss: 0.00002840
Iteration 190/1000 | Loss: 0.00002840
Iteration 191/1000 | Loss: 0.00002839
Iteration 192/1000 | Loss: 0.00002839
Iteration 193/1000 | Loss: 0.00002839
Iteration 194/1000 | Loss: 0.00002838
Iteration 195/1000 | Loss: 0.00002838
Iteration 196/1000 | Loss: 0.00002838
Iteration 197/1000 | Loss: 0.00002838
Iteration 198/1000 | Loss: 0.00002837
Iteration 199/1000 | Loss: 0.00002837
Iteration 200/1000 | Loss: 0.00002837
Iteration 201/1000 | Loss: 0.00002837
Iteration 202/1000 | Loss: 0.00002837
Iteration 203/1000 | Loss: 0.00002837
Iteration 204/1000 | Loss: 0.00002837
Iteration 205/1000 | Loss: 0.00002837
Iteration 206/1000 | Loss: 0.00002837
Iteration 207/1000 | Loss: 0.00002836
Iteration 208/1000 | Loss: 0.00002836
Iteration 209/1000 | Loss: 0.00002836
Iteration 210/1000 | Loss: 0.00002836
Iteration 211/1000 | Loss: 0.00002836
Iteration 212/1000 | Loss: 0.00002836
Iteration 213/1000 | Loss: 0.00002835
Iteration 214/1000 | Loss: 0.00002835
Iteration 215/1000 | Loss: 0.00002835
Iteration 216/1000 | Loss: 0.00002835
Iteration 217/1000 | Loss: 0.00002835
Iteration 218/1000 | Loss: 0.00002835
Iteration 219/1000 | Loss: 0.00002835
Iteration 220/1000 | Loss: 0.00002835
Iteration 221/1000 | Loss: 0.00002835
Iteration 222/1000 | Loss: 0.00002834
Iteration 223/1000 | Loss: 0.00002834
Iteration 224/1000 | Loss: 0.00002834
Iteration 225/1000 | Loss: 0.00002834
Iteration 226/1000 | Loss: 0.00002834
Iteration 227/1000 | Loss: 0.00002834
Iteration 228/1000 | Loss: 0.00002834
Iteration 229/1000 | Loss: 0.00002834
Iteration 230/1000 | Loss: 0.00002834
Iteration 231/1000 | Loss: 0.00002833
Iteration 232/1000 | Loss: 0.00002833
Iteration 233/1000 | Loss: 0.00002833
Iteration 234/1000 | Loss: 0.00002833
Iteration 235/1000 | Loss: 0.00002832
Iteration 236/1000 | Loss: 0.00002832
Iteration 237/1000 | Loss: 0.00002832
Iteration 238/1000 | Loss: 0.00002832
Iteration 239/1000 | Loss: 0.00002832
Iteration 240/1000 | Loss: 0.00002832
Iteration 241/1000 | Loss: 0.00002831
Iteration 242/1000 | Loss: 0.00002831
Iteration 243/1000 | Loss: 0.00002831
Iteration 244/1000 | Loss: 0.00002831
Iteration 245/1000 | Loss: 0.00002831
Iteration 246/1000 | Loss: 0.00002831
Iteration 247/1000 | Loss: 0.00002831
Iteration 248/1000 | Loss: 0.00002831
Iteration 249/1000 | Loss: 0.00002831
Iteration 250/1000 | Loss: 0.00002831
Iteration 251/1000 | Loss: 0.00002831
Iteration 252/1000 | Loss: 0.00002831
Iteration 253/1000 | Loss: 0.00002830
Iteration 254/1000 | Loss: 0.00002830
Iteration 255/1000 | Loss: 0.00002830
Iteration 256/1000 | Loss: 0.00002830
Iteration 257/1000 | Loss: 0.00002830
Iteration 258/1000 | Loss: 0.00002830
Iteration 259/1000 | Loss: 0.00002830
Iteration 260/1000 | Loss: 0.00002830
Iteration 261/1000 | Loss: 0.00002830
Iteration 262/1000 | Loss: 0.00002830
Iteration 263/1000 | Loss: 0.00002830
Iteration 264/1000 | Loss: 0.00002829
Iteration 265/1000 | Loss: 0.00002829
Iteration 266/1000 | Loss: 0.00002829
Iteration 267/1000 | Loss: 0.00002829
Iteration 268/1000 | Loss: 0.00002828
Iteration 269/1000 | Loss: 0.00002828
Iteration 270/1000 | Loss: 0.00002828
Iteration 271/1000 | Loss: 0.00002828
Iteration 272/1000 | Loss: 0.00002827
Iteration 273/1000 | Loss: 0.00002827
Iteration 274/1000 | Loss: 0.00002827
Iteration 275/1000 | Loss: 0.00002827
Iteration 276/1000 | Loss: 0.00002827
Iteration 277/1000 | Loss: 0.00002827
Iteration 278/1000 | Loss: 0.00002827
Iteration 279/1000 | Loss: 0.00002826
Iteration 280/1000 | Loss: 0.00002826
Iteration 281/1000 | Loss: 0.00002826
Iteration 282/1000 | Loss: 0.00002826
Iteration 283/1000 | Loss: 0.00002826
Iteration 284/1000 | Loss: 0.00002826
Iteration 285/1000 | Loss: 0.00002825
Iteration 286/1000 | Loss: 0.00002825
Iteration 287/1000 | Loss: 0.00002825
Iteration 288/1000 | Loss: 0.00002825
Iteration 289/1000 | Loss: 0.00002824
Iteration 290/1000 | Loss: 0.00002824
Iteration 291/1000 | Loss: 0.00002824
Iteration 292/1000 | Loss: 0.00002824
Iteration 293/1000 | Loss: 0.00002824
Iteration 294/1000 | Loss: 0.00002823
Iteration 295/1000 | Loss: 0.00002823
Iteration 296/1000 | Loss: 0.00002823
Iteration 297/1000 | Loss: 0.00002823
Iteration 298/1000 | Loss: 0.00002823
Iteration 299/1000 | Loss: 0.00002823
Iteration 300/1000 | Loss: 0.00002823
Iteration 301/1000 | Loss: 0.00002823
Iteration 302/1000 | Loss: 0.00002823
Iteration 303/1000 | Loss: 0.00002823
Iteration 304/1000 | Loss: 0.00002823
Iteration 305/1000 | Loss: 0.00002823
Iteration 306/1000 | Loss: 0.00002822
Iteration 307/1000 | Loss: 0.00002822
Iteration 308/1000 | Loss: 0.00002822
Iteration 309/1000 | Loss: 0.00002822
Iteration 310/1000 | Loss: 0.00002821
Iteration 311/1000 | Loss: 0.00002821
Iteration 312/1000 | Loss: 0.00002821
Iteration 313/1000 | Loss: 0.00002821
Iteration 314/1000 | Loss: 0.00002821
Iteration 315/1000 | Loss: 0.00002821
Iteration 316/1000 | Loss: 0.00002821
Iteration 317/1000 | Loss: 0.00002821
Iteration 318/1000 | Loss: 0.00002821
Iteration 319/1000 | Loss: 0.00002821
Iteration 320/1000 | Loss: 0.00002821
Iteration 321/1000 | Loss: 0.00002820
Iteration 322/1000 | Loss: 0.00002820
Iteration 323/1000 | Loss: 0.00002820
Iteration 324/1000 | Loss: 0.00002820
Iteration 325/1000 | Loss: 0.00002820
Iteration 326/1000 | Loss: 0.00002820
Iteration 327/1000 | Loss: 0.00002820
Iteration 328/1000 | Loss: 0.00002820
Iteration 329/1000 | Loss: 0.00002819
Iteration 330/1000 | Loss: 0.00002819
Iteration 331/1000 | Loss: 0.00002819
Iteration 332/1000 | Loss: 0.00002819
Iteration 333/1000 | Loss: 0.00002819
Iteration 334/1000 | Loss: 0.00002819
Iteration 335/1000 | Loss: 0.00002819
Iteration 336/1000 | Loss: 0.00002819
Iteration 337/1000 | Loss: 0.00002819
Iteration 338/1000 | Loss: 0.00002819
Iteration 339/1000 | Loss: 0.00002819
Iteration 340/1000 | Loss: 0.00002819
Iteration 341/1000 | Loss: 0.00002819
Iteration 342/1000 | Loss: 0.00002819
Iteration 343/1000 | Loss: 0.00002819
Iteration 344/1000 | Loss: 0.00002819
Iteration 345/1000 | Loss: 0.00002818
Iteration 346/1000 | Loss: 0.00002818
Iteration 347/1000 | Loss: 0.00002818
Iteration 348/1000 | Loss: 0.00002818
Iteration 349/1000 | Loss: 0.00002818
Iteration 350/1000 | Loss: 0.00002818
Iteration 351/1000 | Loss: 0.00002818
Iteration 352/1000 | Loss: 0.00002818
Iteration 353/1000 | Loss: 0.00002818
Iteration 354/1000 | Loss: 0.00002818
Iteration 355/1000 | Loss: 0.00002818
Iteration 356/1000 | Loss: 0.00002818
Iteration 357/1000 | Loss: 0.00002818
Iteration 358/1000 | Loss: 0.00002818
Iteration 359/1000 | Loss: 0.00002818
Iteration 360/1000 | Loss: 0.00002818
Iteration 361/1000 | Loss: 0.00002818
Iteration 362/1000 | Loss: 0.00002818
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 362. Stopping optimization.
Last 5 losses: [2.8182355890749022e-05, 2.8182355890749022e-05, 2.8182355890749022e-05, 2.8182355890749022e-05, 2.8182355890749022e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.8182355890749022e-05

Optimization complete. Final v2v error: 3.803675651550293 mm

Highest mean error: 12.007826805114746 mm for frame 65

Lowest mean error: 2.8222122192382812 mm for frame 139

Saving results

Total time: 284.53059124946594
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_025/1009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_025/1009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_025/1009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00829044
Iteration 2/25 | Loss: 0.00158273
Iteration 3/25 | Loss: 0.00126538
Iteration 4/25 | Loss: 0.00120635
Iteration 5/25 | Loss: 0.00119938
Iteration 6/25 | Loss: 0.00119401
Iteration 7/25 | Loss: 0.00119018
Iteration 8/25 | Loss: 0.00119182
Iteration 9/25 | Loss: 0.00119053
Iteration 10/25 | Loss: 0.00118887
Iteration 11/25 | Loss: 0.00118724
Iteration 12/25 | Loss: 0.00118647
Iteration 13/25 | Loss: 0.00118542
Iteration 14/25 | Loss: 0.00118451
Iteration 15/25 | Loss: 0.00118425
Iteration 16/25 | Loss: 0.00118418
Iteration 17/25 | Loss: 0.00118418
Iteration 18/25 | Loss: 0.00118418
Iteration 19/25 | Loss: 0.00118418
Iteration 20/25 | Loss: 0.00118418
Iteration 21/25 | Loss: 0.00118418
Iteration 22/25 | Loss: 0.00118418
Iteration 23/25 | Loss: 0.00118418
Iteration 24/25 | Loss: 0.00118418
Iteration 25/25 | Loss: 0.00118418

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.85003805
Iteration 2/25 | Loss: 0.00082072
Iteration 3/25 | Loss: 0.00082072
Iteration 4/25 | Loss: 0.00082072
Iteration 5/25 | Loss: 0.00082072
Iteration 6/25 | Loss: 0.00082072
Iteration 7/25 | Loss: 0.00082072
Iteration 8/25 | Loss: 0.00082072
Iteration 9/25 | Loss: 0.00082072
Iteration 10/25 | Loss: 0.00082072
Iteration 11/25 | Loss: 0.00082072
Iteration 12/25 | Loss: 0.00082072
Iteration 13/25 | Loss: 0.00082072
Iteration 14/25 | Loss: 0.00082072
Iteration 15/25 | Loss: 0.00082072
Iteration 16/25 | Loss: 0.00082072
Iteration 17/25 | Loss: 0.00082072
Iteration 18/25 | Loss: 0.00082072
Iteration 19/25 | Loss: 0.00082072
Iteration 20/25 | Loss: 0.00082072
Iteration 21/25 | Loss: 0.00082072
Iteration 22/25 | Loss: 0.00082072
Iteration 23/25 | Loss: 0.00082072
Iteration 24/25 | Loss: 0.00082072
Iteration 25/25 | Loss: 0.00082072

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00082072
Iteration 2/1000 | Loss: 0.00002137
Iteration 3/1000 | Loss: 0.00001762
Iteration 4/1000 | Loss: 0.00001657
Iteration 5/1000 | Loss: 0.00001593
Iteration 6/1000 | Loss: 0.00009199
Iteration 7/1000 | Loss: 0.00001558
Iteration 8/1000 | Loss: 0.00006009
Iteration 9/1000 | Loss: 0.00001519
Iteration 10/1000 | Loss: 0.00008454
Iteration 11/1000 | Loss: 0.00001501
Iteration 12/1000 | Loss: 0.00001484
Iteration 13/1000 | Loss: 0.00001464
Iteration 14/1000 | Loss: 0.00001458
Iteration 15/1000 | Loss: 0.00001448
Iteration 16/1000 | Loss: 0.00001446
Iteration 17/1000 | Loss: 0.00001431
Iteration 18/1000 | Loss: 0.00001430
Iteration 19/1000 | Loss: 0.00001430
Iteration 20/1000 | Loss: 0.00001430
Iteration 21/1000 | Loss: 0.00001429
Iteration 22/1000 | Loss: 0.00001429
Iteration 23/1000 | Loss: 0.00001428
Iteration 24/1000 | Loss: 0.00001428
Iteration 25/1000 | Loss: 0.00001425
Iteration 26/1000 | Loss: 0.00001415
Iteration 27/1000 | Loss: 0.00001415
Iteration 28/1000 | Loss: 0.00001415
Iteration 29/1000 | Loss: 0.00001415
Iteration 30/1000 | Loss: 0.00001413
Iteration 31/1000 | Loss: 0.00001413
Iteration 32/1000 | Loss: 0.00001412
Iteration 33/1000 | Loss: 0.00001412
Iteration 34/1000 | Loss: 0.00001412
Iteration 35/1000 | Loss: 0.00001412
Iteration 36/1000 | Loss: 0.00001412
Iteration 37/1000 | Loss: 0.00001410
Iteration 38/1000 | Loss: 0.00001410
Iteration 39/1000 | Loss: 0.00001409
Iteration 40/1000 | Loss: 0.00001409
Iteration 41/1000 | Loss: 0.00001408
Iteration 42/1000 | Loss: 0.00008478
Iteration 43/1000 | Loss: 0.00002773
Iteration 44/1000 | Loss: 0.00001407
Iteration 45/1000 | Loss: 0.00001407
Iteration 46/1000 | Loss: 0.00001407
Iteration 47/1000 | Loss: 0.00001407
Iteration 48/1000 | Loss: 0.00001407
Iteration 49/1000 | Loss: 0.00001407
Iteration 50/1000 | Loss: 0.00001407
Iteration 51/1000 | Loss: 0.00001407
Iteration 52/1000 | Loss: 0.00001407
Iteration 53/1000 | Loss: 0.00001407
Iteration 54/1000 | Loss: 0.00001407
Iteration 55/1000 | Loss: 0.00001406
Iteration 56/1000 | Loss: 0.00001406
Iteration 57/1000 | Loss: 0.00001406
Iteration 58/1000 | Loss: 0.00001406
Iteration 59/1000 | Loss: 0.00005122
Iteration 60/1000 | Loss: 0.00001408
Iteration 61/1000 | Loss: 0.00001405
Iteration 62/1000 | Loss: 0.00001405
Iteration 63/1000 | Loss: 0.00001405
Iteration 64/1000 | Loss: 0.00001405
Iteration 65/1000 | Loss: 0.00001405
Iteration 66/1000 | Loss: 0.00001404
Iteration 67/1000 | Loss: 0.00001404
Iteration 68/1000 | Loss: 0.00001403
Iteration 69/1000 | Loss: 0.00001403
Iteration 70/1000 | Loss: 0.00001403
Iteration 71/1000 | Loss: 0.00001402
Iteration 72/1000 | Loss: 0.00001402
Iteration 73/1000 | Loss: 0.00001401
Iteration 74/1000 | Loss: 0.00001401
Iteration 75/1000 | Loss: 0.00001401
Iteration 76/1000 | Loss: 0.00001401
Iteration 77/1000 | Loss: 0.00001401
Iteration 78/1000 | Loss: 0.00001401
Iteration 79/1000 | Loss: 0.00001401
Iteration 80/1000 | Loss: 0.00001401
Iteration 81/1000 | Loss: 0.00001401
Iteration 82/1000 | Loss: 0.00001401
Iteration 83/1000 | Loss: 0.00001401
Iteration 84/1000 | Loss: 0.00001401
Iteration 85/1000 | Loss: 0.00001401
Iteration 86/1000 | Loss: 0.00001401
Iteration 87/1000 | Loss: 0.00001401
Iteration 88/1000 | Loss: 0.00001401
Iteration 89/1000 | Loss: 0.00001401
Iteration 90/1000 | Loss: 0.00001401
Iteration 91/1000 | Loss: 0.00001401
Iteration 92/1000 | Loss: 0.00001401
Iteration 93/1000 | Loss: 0.00001401
Iteration 94/1000 | Loss: 0.00001401
Iteration 95/1000 | Loss: 0.00001401
Iteration 96/1000 | Loss: 0.00001401
Iteration 97/1000 | Loss: 0.00001401
Iteration 98/1000 | Loss: 0.00001401
Iteration 99/1000 | Loss: 0.00001401
Iteration 100/1000 | Loss: 0.00001401
Iteration 101/1000 | Loss: 0.00001401
Iteration 102/1000 | Loss: 0.00001401
Iteration 103/1000 | Loss: 0.00001401
Iteration 104/1000 | Loss: 0.00001401
Iteration 105/1000 | Loss: 0.00001401
Iteration 106/1000 | Loss: 0.00001401
Iteration 107/1000 | Loss: 0.00001401
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 107. Stopping optimization.
Last 5 losses: [1.4007307072461117e-05, 1.4007307072461117e-05, 1.4007307072461117e-05, 1.4007307072461117e-05, 1.4007307072461117e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4007307072461117e-05

Optimization complete. Final v2v error: 3.1654770374298096 mm

Highest mean error: 3.727538585662842 mm for frame 55

Lowest mean error: 2.83406925201416 mm for frame 235

Saving results

Total time: 66.32117748260498
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_025/1032/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_025/1032.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_025/1032
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00559695
Iteration 2/25 | Loss: 0.00139083
Iteration 3/25 | Loss: 0.00126981
Iteration 4/25 | Loss: 0.00125682
Iteration 5/25 | Loss: 0.00125633
Iteration 6/25 | Loss: 0.00125633
Iteration 7/25 | Loss: 0.00125633
Iteration 8/25 | Loss: 0.00125633
Iteration 9/25 | Loss: 0.00125633
Iteration 10/25 | Loss: 0.00125633
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0012563304044306278, 0.0012563304044306278, 0.0012563304044306278, 0.0012563304044306278, 0.0012563304044306278]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012563304044306278

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.79718304
Iteration 2/25 | Loss: 0.00068049
Iteration 3/25 | Loss: 0.00067977
Iteration 4/25 | Loss: 0.00067977
Iteration 5/25 | Loss: 0.00067977
Iteration 6/25 | Loss: 0.00067977
Iteration 7/25 | Loss: 0.00067977
Iteration 8/25 | Loss: 0.00067977
Iteration 9/25 | Loss: 0.00067977
Iteration 10/25 | Loss: 0.00067977
Iteration 11/25 | Loss: 0.00067977
Iteration 12/25 | Loss: 0.00067976
Iteration 13/25 | Loss: 0.00067976
Iteration 14/25 | Loss: 0.00067976
Iteration 15/25 | Loss: 0.00067976
Iteration 16/25 | Loss: 0.00067976
Iteration 17/25 | Loss: 0.00067976
Iteration 18/25 | Loss: 0.00067976
Iteration 19/25 | Loss: 0.00067976
Iteration 20/25 | Loss: 0.00067976
Iteration 21/25 | Loss: 0.00067976
Iteration 22/25 | Loss: 0.00067976
Iteration 23/25 | Loss: 0.00067976
Iteration 24/25 | Loss: 0.00067976
Iteration 25/25 | Loss: 0.00067976
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0006797647220082581, 0.0006797647220082581, 0.0006797647220082581, 0.0006797647220082581, 0.0006797647220082581]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006797647220082581

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00067976
Iteration 2/1000 | Loss: 0.00003213
Iteration 3/1000 | Loss: 0.00002477
Iteration 4/1000 | Loss: 0.00002230
Iteration 5/1000 | Loss: 0.00002124
Iteration 6/1000 | Loss: 0.00002075
Iteration 7/1000 | Loss: 0.00002025
Iteration 8/1000 | Loss: 0.00001982
Iteration 9/1000 | Loss: 0.00001942
Iteration 10/1000 | Loss: 0.00001903
Iteration 11/1000 | Loss: 0.00001871
Iteration 12/1000 | Loss: 0.00001842
Iteration 13/1000 | Loss: 0.00001807
Iteration 14/1000 | Loss: 0.00001783
Iteration 15/1000 | Loss: 0.00001763
Iteration 16/1000 | Loss: 0.00001747
Iteration 17/1000 | Loss: 0.00001728
Iteration 18/1000 | Loss: 0.00001727
Iteration 19/1000 | Loss: 0.00001720
Iteration 20/1000 | Loss: 0.00001718
Iteration 21/1000 | Loss: 0.00001708
Iteration 22/1000 | Loss: 0.00001703
Iteration 23/1000 | Loss: 0.00001703
Iteration 24/1000 | Loss: 0.00001703
Iteration 25/1000 | Loss: 0.00001703
Iteration 26/1000 | Loss: 0.00001703
Iteration 27/1000 | Loss: 0.00001703
Iteration 28/1000 | Loss: 0.00001703
Iteration 29/1000 | Loss: 0.00001702
Iteration 30/1000 | Loss: 0.00001701
Iteration 31/1000 | Loss: 0.00001701
Iteration 32/1000 | Loss: 0.00001700
Iteration 33/1000 | Loss: 0.00001700
Iteration 34/1000 | Loss: 0.00001699
Iteration 35/1000 | Loss: 0.00001699
Iteration 36/1000 | Loss: 0.00001699
Iteration 37/1000 | Loss: 0.00001698
Iteration 38/1000 | Loss: 0.00001698
Iteration 39/1000 | Loss: 0.00001698
Iteration 40/1000 | Loss: 0.00001698
Iteration 41/1000 | Loss: 0.00001698
Iteration 42/1000 | Loss: 0.00001698
Iteration 43/1000 | Loss: 0.00001698
Iteration 44/1000 | Loss: 0.00001698
Iteration 45/1000 | Loss: 0.00001698
Iteration 46/1000 | Loss: 0.00001698
Iteration 47/1000 | Loss: 0.00001697
Iteration 48/1000 | Loss: 0.00001697
Iteration 49/1000 | Loss: 0.00001697
Iteration 50/1000 | Loss: 0.00001697
Iteration 51/1000 | Loss: 0.00001697
Iteration 52/1000 | Loss: 0.00001697
Iteration 53/1000 | Loss: 0.00001696
Iteration 54/1000 | Loss: 0.00001696
Iteration 55/1000 | Loss: 0.00001696
Iteration 56/1000 | Loss: 0.00001696
Iteration 57/1000 | Loss: 0.00001696
Iteration 58/1000 | Loss: 0.00001696
Iteration 59/1000 | Loss: 0.00001696
Iteration 60/1000 | Loss: 0.00001696
Iteration 61/1000 | Loss: 0.00001696
Iteration 62/1000 | Loss: 0.00001696
Iteration 63/1000 | Loss: 0.00001696
Iteration 64/1000 | Loss: 0.00001696
Iteration 65/1000 | Loss: 0.00001696
Iteration 66/1000 | Loss: 0.00001696
Iteration 67/1000 | Loss: 0.00001696
Iteration 68/1000 | Loss: 0.00001696
Iteration 69/1000 | Loss: 0.00001696
Iteration 70/1000 | Loss: 0.00001696
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 70. Stopping optimization.
Last 5 losses: [1.6955928003881127e-05, 1.6955928003881127e-05, 1.6955928003881127e-05, 1.6955928003881127e-05, 1.6955928003881127e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6955928003881127e-05

Optimization complete. Final v2v error: 3.422191858291626 mm

Highest mean error: 3.664822816848755 mm for frame 226

Lowest mean error: 3.236895799636841 mm for frame 24

Saving results

Total time: 41.51353621482849
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_025/1058/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_025/1058.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_025/1058
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00976644
Iteration 2/25 | Loss: 0.00251948
Iteration 3/25 | Loss: 0.00184932
Iteration 4/25 | Loss: 0.00173506
Iteration 5/25 | Loss: 0.00174037
Iteration 6/25 | Loss: 0.00168164
Iteration 7/25 | Loss: 0.00164962
Iteration 8/25 | Loss: 0.00165768
Iteration 9/25 | Loss: 0.00159397
Iteration 10/25 | Loss: 0.00151103
Iteration 11/25 | Loss: 0.00145237
Iteration 12/25 | Loss: 0.00142314
Iteration 13/25 | Loss: 0.00141262
Iteration 14/25 | Loss: 0.00137434
Iteration 15/25 | Loss: 0.00134128
Iteration 16/25 | Loss: 0.00131666
Iteration 17/25 | Loss: 0.00131282
Iteration 18/25 | Loss: 0.00131340
Iteration 19/25 | Loss: 0.00130927
Iteration 20/25 | Loss: 0.00130691
Iteration 21/25 | Loss: 0.00130204
Iteration 22/25 | Loss: 0.00129775
Iteration 23/25 | Loss: 0.00129664
Iteration 24/25 | Loss: 0.00129231
Iteration 25/25 | Loss: 0.00128784

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.40585542
Iteration 2/25 | Loss: 0.00548846
Iteration 3/25 | Loss: 0.00249035
Iteration 4/25 | Loss: 0.00249033
Iteration 5/25 | Loss: 0.00249033
Iteration 6/25 | Loss: 0.00249033
Iteration 7/25 | Loss: 0.00249032
Iteration 8/25 | Loss: 0.00249032
Iteration 9/25 | Loss: 0.00249032
Iteration 10/25 | Loss: 0.00249032
Iteration 11/25 | Loss: 0.00249032
Iteration 12/25 | Loss: 0.00249032
Iteration 13/25 | Loss: 0.00249032
Iteration 14/25 | Loss: 0.00249032
Iteration 15/25 | Loss: 0.00249032
Iteration 16/25 | Loss: 0.00249032
Iteration 17/25 | Loss: 0.00249032
Iteration 18/25 | Loss: 0.00249032
Iteration 19/25 | Loss: 0.00249032
Iteration 20/25 | Loss: 0.00249032
Iteration 21/25 | Loss: 0.00249032
Iteration 22/25 | Loss: 0.00249032
Iteration 23/25 | Loss: 0.00249032
Iteration 24/25 | Loss: 0.00249032
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0024903216399252415, 0.0024903216399252415, 0.0024903216399252415, 0.0024903216399252415, 0.0024903216399252415]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0024903216399252415

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00249032
Iteration 2/1000 | Loss: 0.00239855
Iteration 3/1000 | Loss: 0.00146468
Iteration 4/1000 | Loss: 0.00042836
Iteration 5/1000 | Loss: 0.00017948
Iteration 6/1000 | Loss: 0.00087502
Iteration 7/1000 | Loss: 0.00083705
Iteration 8/1000 | Loss: 0.00115104
Iteration 9/1000 | Loss: 0.00081431
Iteration 10/1000 | Loss: 0.00046874
Iteration 11/1000 | Loss: 0.00057922
Iteration 12/1000 | Loss: 0.00033724
Iteration 13/1000 | Loss: 0.00058874
Iteration 14/1000 | Loss: 0.00037274
Iteration 15/1000 | Loss: 0.00072499
Iteration 16/1000 | Loss: 0.00136661
Iteration 17/1000 | Loss: 0.00112859
Iteration 18/1000 | Loss: 0.00061786
Iteration 19/1000 | Loss: 0.00073380
Iteration 20/1000 | Loss: 0.00071006
Iteration 21/1000 | Loss: 0.00064985
Iteration 22/1000 | Loss: 0.00102376
Iteration 23/1000 | Loss: 0.00103870
Iteration 24/1000 | Loss: 0.00044707
Iteration 25/1000 | Loss: 0.00041518
Iteration 26/1000 | Loss: 0.00035811
Iteration 27/1000 | Loss: 0.00118300
Iteration 28/1000 | Loss: 0.00044232
Iteration 29/1000 | Loss: 0.00036646
Iteration 30/1000 | Loss: 0.00040341
Iteration 31/1000 | Loss: 0.00066798
Iteration 32/1000 | Loss: 0.00065585
Iteration 33/1000 | Loss: 0.00093731
Iteration 34/1000 | Loss: 0.00090723
Iteration 35/1000 | Loss: 0.00068524
Iteration 36/1000 | Loss: 0.00050846
Iteration 37/1000 | Loss: 0.00033089
Iteration 38/1000 | Loss: 0.00030303
Iteration 39/1000 | Loss: 0.00021113
Iteration 40/1000 | Loss: 0.00038784
Iteration 41/1000 | Loss: 0.00038132
Iteration 42/1000 | Loss: 0.00048700
Iteration 43/1000 | Loss: 0.00029996
Iteration 44/1000 | Loss: 0.00036254
Iteration 45/1000 | Loss: 0.00038520
Iteration 46/1000 | Loss: 0.00068193
Iteration 47/1000 | Loss: 0.00030729
Iteration 48/1000 | Loss: 0.00048914
Iteration 49/1000 | Loss: 0.00029227
Iteration 50/1000 | Loss: 0.00037326
Iteration 51/1000 | Loss: 0.00042804
Iteration 52/1000 | Loss: 0.00046503
Iteration 53/1000 | Loss: 0.00039408
Iteration 54/1000 | Loss: 0.00036032
Iteration 55/1000 | Loss: 0.00032201
Iteration 56/1000 | Loss: 0.00073392
Iteration 57/1000 | Loss: 0.00082497
Iteration 58/1000 | Loss: 0.00039089
Iteration 59/1000 | Loss: 0.00037974
Iteration 60/1000 | Loss: 0.00037650
Iteration 61/1000 | Loss: 0.00026605
Iteration 62/1000 | Loss: 0.00028817
Iteration 63/1000 | Loss: 0.00055124
Iteration 64/1000 | Loss: 0.00055240
Iteration 65/1000 | Loss: 0.00037547
Iteration 66/1000 | Loss: 0.00041738
Iteration 67/1000 | Loss: 0.00024766
Iteration 68/1000 | Loss: 0.00050773
Iteration 69/1000 | Loss: 0.00053491
Iteration 70/1000 | Loss: 0.00028492
Iteration 71/1000 | Loss: 0.00063321
Iteration 72/1000 | Loss: 0.00042206
Iteration 73/1000 | Loss: 0.00041633
Iteration 74/1000 | Loss: 0.00052232
Iteration 75/1000 | Loss: 0.00046848
Iteration 76/1000 | Loss: 0.00035538
Iteration 77/1000 | Loss: 0.00043776
Iteration 78/1000 | Loss: 0.00041157
Iteration 79/1000 | Loss: 0.00043747
Iteration 80/1000 | Loss: 0.00047329
Iteration 81/1000 | Loss: 0.00132421
Iteration 82/1000 | Loss: 0.00064081
Iteration 83/1000 | Loss: 0.00140020
Iteration 84/1000 | Loss: 0.00072990
Iteration 85/1000 | Loss: 0.00045061
Iteration 86/1000 | Loss: 0.00043627
Iteration 87/1000 | Loss: 0.00046273
Iteration 88/1000 | Loss: 0.00043209
Iteration 89/1000 | Loss: 0.00034524
Iteration 90/1000 | Loss: 0.00045106
Iteration 91/1000 | Loss: 0.00114886
Iteration 92/1000 | Loss: 0.00110564
Iteration 93/1000 | Loss: 0.00095178
Iteration 94/1000 | Loss: 0.00094713
Iteration 95/1000 | Loss: 0.00101763
Iteration 96/1000 | Loss: 0.00102844
Iteration 97/1000 | Loss: 0.00146747
Iteration 98/1000 | Loss: 0.00057560
Iteration 99/1000 | Loss: 0.00074182
Iteration 100/1000 | Loss: 0.00048476
Iteration 101/1000 | Loss: 0.00051827
Iteration 102/1000 | Loss: 0.00026353
Iteration 103/1000 | Loss: 0.00071755
Iteration 104/1000 | Loss: 0.00101517
Iteration 105/1000 | Loss: 0.00116019
Iteration 106/1000 | Loss: 0.00039332
Iteration 107/1000 | Loss: 0.00062781
Iteration 108/1000 | Loss: 0.00033941
Iteration 109/1000 | Loss: 0.00040739
Iteration 110/1000 | Loss: 0.00040237
Iteration 111/1000 | Loss: 0.00041912
Iteration 112/1000 | Loss: 0.00052082
Iteration 113/1000 | Loss: 0.00049234
Iteration 114/1000 | Loss: 0.00047955
Iteration 115/1000 | Loss: 0.00046223
Iteration 116/1000 | Loss: 0.00063628
Iteration 117/1000 | Loss: 0.00047215
Iteration 118/1000 | Loss: 0.00051359
Iteration 119/1000 | Loss: 0.00036392
Iteration 120/1000 | Loss: 0.00027156
Iteration 121/1000 | Loss: 0.00030949
Iteration 122/1000 | Loss: 0.00021644
Iteration 123/1000 | Loss: 0.00035959
Iteration 124/1000 | Loss: 0.00040623
Iteration 125/1000 | Loss: 0.00029443
Iteration 126/1000 | Loss: 0.00032186
Iteration 127/1000 | Loss: 0.00031802
Iteration 128/1000 | Loss: 0.00017658
Iteration 129/1000 | Loss: 0.00030841
Iteration 130/1000 | Loss: 0.00100796
Iteration 131/1000 | Loss: 0.00045717
Iteration 132/1000 | Loss: 0.00020625
Iteration 133/1000 | Loss: 0.00029924
Iteration 134/1000 | Loss: 0.00030065
Iteration 135/1000 | Loss: 0.00024561
Iteration 136/1000 | Loss: 0.00034115
Iteration 137/1000 | Loss: 0.00027549
Iteration 138/1000 | Loss: 0.00052050
Iteration 139/1000 | Loss: 0.00094639
Iteration 140/1000 | Loss: 0.00060916
Iteration 141/1000 | Loss: 0.00065455
Iteration 142/1000 | Loss: 0.00033364
Iteration 143/1000 | Loss: 0.00042911
Iteration 144/1000 | Loss: 0.00033252
Iteration 145/1000 | Loss: 0.00014021
Iteration 146/1000 | Loss: 0.00025133
Iteration 147/1000 | Loss: 0.00027740
Iteration 148/1000 | Loss: 0.00039538
Iteration 149/1000 | Loss: 0.00018511
Iteration 150/1000 | Loss: 0.00016901
Iteration 151/1000 | Loss: 0.00043060
Iteration 152/1000 | Loss: 0.00028000
Iteration 153/1000 | Loss: 0.00031183
Iteration 154/1000 | Loss: 0.00025189
Iteration 155/1000 | Loss: 0.00022995
Iteration 156/1000 | Loss: 0.00022613
Iteration 157/1000 | Loss: 0.00028946
Iteration 158/1000 | Loss: 0.00020583
Iteration 159/1000 | Loss: 0.00019826
Iteration 160/1000 | Loss: 0.00091374
Iteration 161/1000 | Loss: 0.00049026
Iteration 162/1000 | Loss: 0.00074617
Iteration 163/1000 | Loss: 0.00023260
Iteration 164/1000 | Loss: 0.00028545
Iteration 165/1000 | Loss: 0.00020614
Iteration 166/1000 | Loss: 0.00016483
Iteration 167/1000 | Loss: 0.00020667
Iteration 168/1000 | Loss: 0.00020283
Iteration 169/1000 | Loss: 0.00022433
Iteration 170/1000 | Loss: 0.00015093
Iteration 171/1000 | Loss: 0.00015221
Iteration 172/1000 | Loss: 0.00083821
Iteration 173/1000 | Loss: 0.00047824
Iteration 174/1000 | Loss: 0.00065405
Iteration 175/1000 | Loss: 0.00036078
Iteration 176/1000 | Loss: 0.00040201
Iteration 177/1000 | Loss: 0.00015011
Iteration 178/1000 | Loss: 0.00012770
Iteration 179/1000 | Loss: 0.00029507
Iteration 180/1000 | Loss: 0.00016723
Iteration 181/1000 | Loss: 0.00028416
Iteration 182/1000 | Loss: 0.00020753
Iteration 183/1000 | Loss: 0.00020294
Iteration 184/1000 | Loss: 0.00025572
Iteration 185/1000 | Loss: 0.00096659
Iteration 186/1000 | Loss: 0.00063612
Iteration 187/1000 | Loss: 0.00066607
Iteration 188/1000 | Loss: 0.00060848
Iteration 189/1000 | Loss: 0.00046593
Iteration 190/1000 | Loss: 0.00053416
Iteration 191/1000 | Loss: 0.00054427
Iteration 192/1000 | Loss: 0.00051666
Iteration 193/1000 | Loss: 0.00050244
Iteration 194/1000 | Loss: 0.00044608
Iteration 195/1000 | Loss: 0.00128810
Iteration 196/1000 | Loss: 0.00079459
Iteration 197/1000 | Loss: 0.00107424
Iteration 198/1000 | Loss: 0.00101899
Iteration 199/1000 | Loss: 0.00093010
Iteration 200/1000 | Loss: 0.00040130
Iteration 201/1000 | Loss: 0.00032531
Iteration 202/1000 | Loss: 0.00113695
Iteration 203/1000 | Loss: 0.00069921
Iteration 204/1000 | Loss: 0.00070069
Iteration 205/1000 | Loss: 0.00052236
Iteration 206/1000 | Loss: 0.00022614
Iteration 207/1000 | Loss: 0.00011664
Iteration 208/1000 | Loss: 0.00010811
Iteration 209/1000 | Loss: 0.00083864
Iteration 210/1000 | Loss: 0.00090893
Iteration 211/1000 | Loss: 0.00119447
Iteration 212/1000 | Loss: 0.00065148
Iteration 213/1000 | Loss: 0.00014550
Iteration 214/1000 | Loss: 0.00021195
Iteration 215/1000 | Loss: 0.00087711
Iteration 216/1000 | Loss: 0.00207309
Iteration 217/1000 | Loss: 0.00086399
Iteration 218/1000 | Loss: 0.00027470
Iteration 219/1000 | Loss: 0.00018028
Iteration 220/1000 | Loss: 0.00087351
Iteration 221/1000 | Loss: 0.00093367
Iteration 222/1000 | Loss: 0.00022424
Iteration 223/1000 | Loss: 0.00056742
Iteration 224/1000 | Loss: 0.00015498
Iteration 225/1000 | Loss: 0.00030774
Iteration 226/1000 | Loss: 0.00022536
Iteration 227/1000 | Loss: 0.00030508
Iteration 228/1000 | Loss: 0.00009150
Iteration 229/1000 | Loss: 0.00010156
Iteration 230/1000 | Loss: 0.00019786
Iteration 231/1000 | Loss: 0.00008133
Iteration 232/1000 | Loss: 0.00155865
Iteration 233/1000 | Loss: 0.00245979
Iteration 234/1000 | Loss: 0.00083355
Iteration 235/1000 | Loss: 0.00073137
Iteration 236/1000 | Loss: 0.00039280
Iteration 237/1000 | Loss: 0.00013308
Iteration 238/1000 | Loss: 0.00018172
Iteration 239/1000 | Loss: 0.00081436
Iteration 240/1000 | Loss: 0.00024665
Iteration 241/1000 | Loss: 0.00027530
Iteration 242/1000 | Loss: 0.00030512
Iteration 243/1000 | Loss: 0.00012778
Iteration 244/1000 | Loss: 0.00023756
Iteration 245/1000 | Loss: 0.00021053
Iteration 246/1000 | Loss: 0.00018486
Iteration 247/1000 | Loss: 0.00020970
Iteration 248/1000 | Loss: 0.00020689
Iteration 249/1000 | Loss: 0.00022719
Iteration 250/1000 | Loss: 0.00009061
Iteration 251/1000 | Loss: 0.00042434
Iteration 252/1000 | Loss: 0.00018437
Iteration 253/1000 | Loss: 0.00015922
Iteration 254/1000 | Loss: 0.00164084
Iteration 255/1000 | Loss: 0.00080259
Iteration 256/1000 | Loss: 0.00075177
Iteration 257/1000 | Loss: 0.00046736
Iteration 258/1000 | Loss: 0.00050491
Iteration 259/1000 | Loss: 0.00033786
Iteration 260/1000 | Loss: 0.00025468
Iteration 261/1000 | Loss: 0.00007638
Iteration 262/1000 | Loss: 0.00010370
Iteration 263/1000 | Loss: 0.00007563
Iteration 264/1000 | Loss: 0.00012790
Iteration 265/1000 | Loss: 0.00012047
Iteration 266/1000 | Loss: 0.00008696
Iteration 267/1000 | Loss: 0.00008031
Iteration 268/1000 | Loss: 0.00037173
Iteration 269/1000 | Loss: 0.00021374
Iteration 270/1000 | Loss: 0.00020941
Iteration 271/1000 | Loss: 0.00016520
Iteration 272/1000 | Loss: 0.00010252
Iteration 273/1000 | Loss: 0.00035194
Iteration 274/1000 | Loss: 0.00090654
Iteration 275/1000 | Loss: 0.00046278
Iteration 276/1000 | Loss: 0.00065089
Iteration 277/1000 | Loss: 0.00020866
Iteration 278/1000 | Loss: 0.00089017
Iteration 279/1000 | Loss: 0.00058545
Iteration 280/1000 | Loss: 0.00077308
Iteration 281/1000 | Loss: 0.00047228
Iteration 282/1000 | Loss: 0.00063598
Iteration 283/1000 | Loss: 0.00020390
Iteration 284/1000 | Loss: 0.00033448
Iteration 285/1000 | Loss: 0.00022191
Iteration 286/1000 | Loss: 0.00025326
Iteration 287/1000 | Loss: 0.00008529
Iteration 288/1000 | Loss: 0.00004972
Iteration 289/1000 | Loss: 0.00004446
Iteration 290/1000 | Loss: 0.00028028
Iteration 291/1000 | Loss: 0.00027112
Iteration 292/1000 | Loss: 0.00022345
Iteration 293/1000 | Loss: 0.00021525
Iteration 294/1000 | Loss: 0.00042100
Iteration 295/1000 | Loss: 0.00195012
Iteration 296/1000 | Loss: 0.00245691
Iteration 297/1000 | Loss: 0.00236390
Iteration 298/1000 | Loss: 0.00137663
Iteration 299/1000 | Loss: 0.00242004
Iteration 300/1000 | Loss: 0.00195225
Iteration 301/1000 | Loss: 0.00594564
Iteration 302/1000 | Loss: 0.00158791
Iteration 303/1000 | Loss: 0.00087062
Iteration 304/1000 | Loss: 0.00119610
Iteration 305/1000 | Loss: 0.00039171
Iteration 306/1000 | Loss: 0.00028108
Iteration 307/1000 | Loss: 0.00069514
Iteration 308/1000 | Loss: 0.00045544
Iteration 309/1000 | Loss: 0.00110274
Iteration 310/1000 | Loss: 0.00070707
Iteration 311/1000 | Loss: 0.00055802
Iteration 312/1000 | Loss: 0.00074366
Iteration 313/1000 | Loss: 0.00028571
Iteration 314/1000 | Loss: 0.00122624
Iteration 315/1000 | Loss: 0.00126017
Iteration 316/1000 | Loss: 0.00037560
Iteration 317/1000 | Loss: 0.00028278
Iteration 318/1000 | Loss: 0.00121079
Iteration 319/1000 | Loss: 0.00071679
Iteration 320/1000 | Loss: 0.00078893
Iteration 321/1000 | Loss: 0.00035451
Iteration 322/1000 | Loss: 0.00030838
Iteration 323/1000 | Loss: 0.00009153
Iteration 324/1000 | Loss: 0.00030083
Iteration 325/1000 | Loss: 0.00027035
Iteration 326/1000 | Loss: 0.00004147
Iteration 327/1000 | Loss: 0.00020990
Iteration 328/1000 | Loss: 0.00009553
Iteration 329/1000 | Loss: 0.00043400
Iteration 330/1000 | Loss: 0.00006220
Iteration 331/1000 | Loss: 0.00015445
Iteration 332/1000 | Loss: 0.00114952
Iteration 333/1000 | Loss: 0.00126660
Iteration 334/1000 | Loss: 0.00032919
Iteration 335/1000 | Loss: 0.00042206
Iteration 336/1000 | Loss: 0.00008790
Iteration 337/1000 | Loss: 0.00086789
Iteration 338/1000 | Loss: 0.00033092
Iteration 339/1000 | Loss: 0.00028678
Iteration 340/1000 | Loss: 0.00020866
Iteration 341/1000 | Loss: 0.00035206
Iteration 342/1000 | Loss: 0.00021981
Iteration 343/1000 | Loss: 0.00022251
Iteration 344/1000 | Loss: 0.00043317
Iteration 345/1000 | Loss: 0.00010002
Iteration 346/1000 | Loss: 0.00030628
Iteration 347/1000 | Loss: 0.00016866
Iteration 348/1000 | Loss: 0.00024160
Iteration 349/1000 | Loss: 0.00006410
Iteration 350/1000 | Loss: 0.00005612
Iteration 351/1000 | Loss: 0.00021406
Iteration 352/1000 | Loss: 0.00027308
Iteration 353/1000 | Loss: 0.00028891
Iteration 354/1000 | Loss: 0.00047875
Iteration 355/1000 | Loss: 0.00021877
Iteration 356/1000 | Loss: 0.00030166
Iteration 357/1000 | Loss: 0.00019672
Iteration 358/1000 | Loss: 0.00018905
Iteration 359/1000 | Loss: 0.00017175
Iteration 360/1000 | Loss: 0.00026503
Iteration 361/1000 | Loss: 0.00013885
Iteration 362/1000 | Loss: 0.00012106
Iteration 363/1000 | Loss: 0.00020853
Iteration 364/1000 | Loss: 0.00003178
Iteration 365/1000 | Loss: 0.00006410
Iteration 366/1000 | Loss: 0.00015340
Iteration 367/1000 | Loss: 0.00027209
Iteration 368/1000 | Loss: 0.00017273
Iteration 369/1000 | Loss: 0.00010604
Iteration 370/1000 | Loss: 0.00009576
Iteration 371/1000 | Loss: 0.00009716
Iteration 372/1000 | Loss: 0.00011369
Iteration 373/1000 | Loss: 0.00009437
Iteration 374/1000 | Loss: 0.00018413
Iteration 375/1000 | Loss: 0.00020493
Iteration 376/1000 | Loss: 0.00020308
Iteration 377/1000 | Loss: 0.00018904
Iteration 378/1000 | Loss: 0.00023862
Iteration 379/1000 | Loss: 0.00023766
Iteration 380/1000 | Loss: 0.00019749
Iteration 381/1000 | Loss: 0.00023153
Iteration 382/1000 | Loss: 0.00019259
Iteration 383/1000 | Loss: 0.00019854
Iteration 384/1000 | Loss: 0.00019032
Iteration 385/1000 | Loss: 0.00004485
Iteration 386/1000 | Loss: 0.00029438
Iteration 387/1000 | Loss: 0.00031065
Iteration 388/1000 | Loss: 0.00003580
Iteration 389/1000 | Loss: 0.00002951
Iteration 390/1000 | Loss: 0.00013912
Iteration 391/1000 | Loss: 0.00041698
Iteration 392/1000 | Loss: 0.00008024
Iteration 393/1000 | Loss: 0.00015118
Iteration 394/1000 | Loss: 0.00015343
Iteration 395/1000 | Loss: 0.00025636
Iteration 396/1000 | Loss: 0.00042417
Iteration 397/1000 | Loss: 0.00026462
Iteration 398/1000 | Loss: 0.00014505
Iteration 399/1000 | Loss: 0.00036455
Iteration 400/1000 | Loss: 0.00003103
Iteration 401/1000 | Loss: 0.00002461
Iteration 402/1000 | Loss: 0.00019992
Iteration 403/1000 | Loss: 0.00002077
Iteration 404/1000 | Loss: 0.00001956
Iteration 405/1000 | Loss: 0.00001881
Iteration 406/1000 | Loss: 0.00022488
Iteration 407/1000 | Loss: 0.00002935
Iteration 408/1000 | Loss: 0.00039849
Iteration 409/1000 | Loss: 0.00001972
Iteration 410/1000 | Loss: 0.00001845
Iteration 411/1000 | Loss: 0.00001766
Iteration 412/1000 | Loss: 0.00001761
Iteration 413/1000 | Loss: 0.00001730
Iteration 414/1000 | Loss: 0.00001711
Iteration 415/1000 | Loss: 0.00002078
Iteration 416/1000 | Loss: 0.00001709
Iteration 417/1000 | Loss: 0.00001687
Iteration 418/1000 | Loss: 0.00015321
Iteration 419/1000 | Loss: 0.00002629
Iteration 420/1000 | Loss: 0.00011266
Iteration 421/1000 | Loss: 0.00004463
Iteration 422/1000 | Loss: 0.00001848
Iteration 423/1000 | Loss: 0.00001734
Iteration 424/1000 | Loss: 0.00001655
Iteration 425/1000 | Loss: 0.00001646
Iteration 426/1000 | Loss: 0.00001645
Iteration 427/1000 | Loss: 0.00001643
Iteration 428/1000 | Loss: 0.00001637
Iteration 429/1000 | Loss: 0.00001623
Iteration 430/1000 | Loss: 0.00001606
Iteration 431/1000 | Loss: 0.00001600
Iteration 432/1000 | Loss: 0.00001598
Iteration 433/1000 | Loss: 0.00001593
Iteration 434/1000 | Loss: 0.00001586
Iteration 435/1000 | Loss: 0.00001570
Iteration 436/1000 | Loss: 0.00001557
Iteration 437/1000 | Loss: 0.00001554
Iteration 438/1000 | Loss: 0.00001554
Iteration 439/1000 | Loss: 0.00001553
Iteration 440/1000 | Loss: 0.00001553
Iteration 441/1000 | Loss: 0.00001553
Iteration 442/1000 | Loss: 0.00001552
Iteration 443/1000 | Loss: 0.00001552
Iteration 444/1000 | Loss: 0.00001551
Iteration 445/1000 | Loss: 0.00001551
Iteration 446/1000 | Loss: 0.00001550
Iteration 447/1000 | Loss: 0.00001549
Iteration 448/1000 | Loss: 0.00015529
Iteration 449/1000 | Loss: 0.00001553
Iteration 450/1000 | Loss: 0.00001546
Iteration 451/1000 | Loss: 0.00001543
Iteration 452/1000 | Loss: 0.00001543
Iteration 453/1000 | Loss: 0.00001542
Iteration 454/1000 | Loss: 0.00001542
Iteration 455/1000 | Loss: 0.00001541
Iteration 456/1000 | Loss: 0.00001541
Iteration 457/1000 | Loss: 0.00001541
Iteration 458/1000 | Loss: 0.00001540
Iteration 459/1000 | Loss: 0.00001540
Iteration 460/1000 | Loss: 0.00001539
Iteration 461/1000 | Loss: 0.00001539
Iteration 462/1000 | Loss: 0.00001538
Iteration 463/1000 | Loss: 0.00001538
Iteration 464/1000 | Loss: 0.00001538
Iteration 465/1000 | Loss: 0.00001538
Iteration 466/1000 | Loss: 0.00001537
Iteration 467/1000 | Loss: 0.00001537
Iteration 468/1000 | Loss: 0.00001536
Iteration 469/1000 | Loss: 0.00001535
Iteration 470/1000 | Loss: 0.00001535
Iteration 471/1000 | Loss: 0.00001534
Iteration 472/1000 | Loss: 0.00001534
Iteration 473/1000 | Loss: 0.00001533
Iteration 474/1000 | Loss: 0.00001533
Iteration 475/1000 | Loss: 0.00001533
Iteration 476/1000 | Loss: 0.00001532
Iteration 477/1000 | Loss: 0.00001532
Iteration 478/1000 | Loss: 0.00001532
Iteration 479/1000 | Loss: 0.00001531
Iteration 480/1000 | Loss: 0.00001531
Iteration 481/1000 | Loss: 0.00001531
Iteration 482/1000 | Loss: 0.00001530
Iteration 483/1000 | Loss: 0.00001530
Iteration 484/1000 | Loss: 0.00001529
Iteration 485/1000 | Loss: 0.00001529
Iteration 486/1000 | Loss: 0.00001529
Iteration 487/1000 | Loss: 0.00001529
Iteration 488/1000 | Loss: 0.00001528
Iteration 489/1000 | Loss: 0.00001528
Iteration 490/1000 | Loss: 0.00001528
Iteration 491/1000 | Loss: 0.00001528
Iteration 492/1000 | Loss: 0.00001527
Iteration 493/1000 | Loss: 0.00001527
Iteration 494/1000 | Loss: 0.00001527
Iteration 495/1000 | Loss: 0.00001527
Iteration 496/1000 | Loss: 0.00001527
Iteration 497/1000 | Loss: 0.00001526
Iteration 498/1000 | Loss: 0.00001526
Iteration 499/1000 | Loss: 0.00001526
Iteration 500/1000 | Loss: 0.00001526
Iteration 501/1000 | Loss: 0.00001526
Iteration 502/1000 | Loss: 0.00001526
Iteration 503/1000 | Loss: 0.00001526
Iteration 504/1000 | Loss: 0.00001525
Iteration 505/1000 | Loss: 0.00001525
Iteration 506/1000 | Loss: 0.00001525
Iteration 507/1000 | Loss: 0.00001524
Iteration 508/1000 | Loss: 0.00001524
Iteration 509/1000 | Loss: 0.00001524
Iteration 510/1000 | Loss: 0.00001524
Iteration 511/1000 | Loss: 0.00001523
Iteration 512/1000 | Loss: 0.00001523
Iteration 513/1000 | Loss: 0.00001523
Iteration 514/1000 | Loss: 0.00001523
Iteration 515/1000 | Loss: 0.00001523
Iteration 516/1000 | Loss: 0.00001523
Iteration 517/1000 | Loss: 0.00001523
Iteration 518/1000 | Loss: 0.00001523
Iteration 519/1000 | Loss: 0.00001523
Iteration 520/1000 | Loss: 0.00001523
Iteration 521/1000 | Loss: 0.00001523
Iteration 522/1000 | Loss: 0.00001523
Iteration 523/1000 | Loss: 0.00001522
Iteration 524/1000 | Loss: 0.00001522
Iteration 525/1000 | Loss: 0.00001522
Iteration 526/1000 | Loss: 0.00001522
Iteration 527/1000 | Loss: 0.00001522
Iteration 528/1000 | Loss: 0.00001522
Iteration 529/1000 | Loss: 0.00001522
Iteration 530/1000 | Loss: 0.00001522
Iteration 531/1000 | Loss: 0.00001522
Iteration 532/1000 | Loss: 0.00001522
Iteration 533/1000 | Loss: 0.00001522
Iteration 534/1000 | Loss: 0.00001522
Iteration 535/1000 | Loss: 0.00001522
Iteration 536/1000 | Loss: 0.00001522
Iteration 537/1000 | Loss: 0.00001521
Iteration 538/1000 | Loss: 0.00001521
Iteration 539/1000 | Loss: 0.00001521
Iteration 540/1000 | Loss: 0.00001521
Iteration 541/1000 | Loss: 0.00001521
Iteration 542/1000 | Loss: 0.00001521
Iteration 543/1000 | Loss: 0.00001521
Iteration 544/1000 | Loss: 0.00001521
Iteration 545/1000 | Loss: 0.00001521
Iteration 546/1000 | Loss: 0.00001521
Iteration 547/1000 | Loss: 0.00001521
Iteration 548/1000 | Loss: 0.00001521
Iteration 549/1000 | Loss: 0.00001521
Iteration 550/1000 | Loss: 0.00001521
Iteration 551/1000 | Loss: 0.00001521
Iteration 552/1000 | Loss: 0.00001521
Iteration 553/1000 | Loss: 0.00001521
Iteration 554/1000 | Loss: 0.00001521
Iteration 555/1000 | Loss: 0.00001521
Iteration 556/1000 | Loss: 0.00001521
Iteration 557/1000 | Loss: 0.00001520
Iteration 558/1000 | Loss: 0.00001520
Iteration 559/1000 | Loss: 0.00001520
Iteration 560/1000 | Loss: 0.00001520
Iteration 561/1000 | Loss: 0.00001520
Iteration 562/1000 | Loss: 0.00001520
Iteration 563/1000 | Loss: 0.00001520
Iteration 564/1000 | Loss: 0.00001520
Iteration 565/1000 | Loss: 0.00001519
Iteration 566/1000 | Loss: 0.00001519
Iteration 567/1000 | Loss: 0.00001519
Iteration 568/1000 | Loss: 0.00001519
Iteration 569/1000 | Loss: 0.00001518
Iteration 570/1000 | Loss: 0.00001518
Iteration 571/1000 | Loss: 0.00001518
Iteration 572/1000 | Loss: 0.00001517
Iteration 573/1000 | Loss: 0.00001517
Iteration 574/1000 | Loss: 0.00001517
Iteration 575/1000 | Loss: 0.00001517
Iteration 576/1000 | Loss: 0.00001517
Iteration 577/1000 | Loss: 0.00001516
Iteration 578/1000 | Loss: 0.00001516
Iteration 579/1000 | Loss: 0.00001516
Iteration 580/1000 | Loss: 0.00001516
Iteration 581/1000 | Loss: 0.00001516
Iteration 582/1000 | Loss: 0.00001516
Iteration 583/1000 | Loss: 0.00001516
Iteration 584/1000 | Loss: 0.00001516
Iteration 585/1000 | Loss: 0.00001516
Iteration 586/1000 | Loss: 0.00001516
Iteration 587/1000 | Loss: 0.00001515
Iteration 588/1000 | Loss: 0.00001515
Iteration 589/1000 | Loss: 0.00001515
Iteration 590/1000 | Loss: 0.00001515
Iteration 591/1000 | Loss: 0.00001515
Iteration 592/1000 | Loss: 0.00001514
Iteration 593/1000 | Loss: 0.00001514
Iteration 594/1000 | Loss: 0.00001514
Iteration 595/1000 | Loss: 0.00001514
Iteration 596/1000 | Loss: 0.00001514
Iteration 597/1000 | Loss: 0.00001514
Iteration 598/1000 | Loss: 0.00001514
Iteration 599/1000 | Loss: 0.00001514
Iteration 600/1000 | Loss: 0.00001514
Iteration 601/1000 | Loss: 0.00001514
Iteration 602/1000 | Loss: 0.00001514
Iteration 603/1000 | Loss: 0.00001514
Iteration 604/1000 | Loss: 0.00001514
Iteration 605/1000 | Loss: 0.00001514
Iteration 606/1000 | Loss: 0.00001514
Iteration 607/1000 | Loss: 0.00001514
Iteration 608/1000 | Loss: 0.00001513
Iteration 609/1000 | Loss: 0.00001513
Iteration 610/1000 | Loss: 0.00001513
Iteration 611/1000 | Loss: 0.00001513
Iteration 612/1000 | Loss: 0.00001513
Iteration 613/1000 | Loss: 0.00001513
Iteration 614/1000 | Loss: 0.00001513
Iteration 615/1000 | Loss: 0.00001513
Iteration 616/1000 | Loss: 0.00001513
Iteration 617/1000 | Loss: 0.00026201
Iteration 618/1000 | Loss: 0.00001406
Iteration 619/1000 | Loss: 0.00001351
Iteration 620/1000 | Loss: 0.00001295
Iteration 621/1000 | Loss: 0.00009479
Iteration 622/1000 | Loss: 0.00001941
Iteration 623/1000 | Loss: 0.00001251
Iteration 624/1000 | Loss: 0.00004831
Iteration 625/1000 | Loss: 0.00001436
Iteration 626/1000 | Loss: 0.00001530
Iteration 627/1000 | Loss: 0.00001217
Iteration 628/1000 | Loss: 0.00001216
Iteration 629/1000 | Loss: 0.00001216
Iteration 630/1000 | Loss: 0.00001216
Iteration 631/1000 | Loss: 0.00003935
Iteration 632/1000 | Loss: 0.00001206
Iteration 633/1000 | Loss: 0.00001199
Iteration 634/1000 | Loss: 0.00001199
Iteration 635/1000 | Loss: 0.00001197
Iteration 636/1000 | Loss: 0.00001197
Iteration 637/1000 | Loss: 0.00001196
Iteration 638/1000 | Loss: 0.00001196
Iteration 639/1000 | Loss: 0.00001195
Iteration 640/1000 | Loss: 0.00001195
Iteration 641/1000 | Loss: 0.00001195
Iteration 642/1000 | Loss: 0.00001195
Iteration 643/1000 | Loss: 0.00001195
Iteration 644/1000 | Loss: 0.00001194
Iteration 645/1000 | Loss: 0.00001193
Iteration 646/1000 | Loss: 0.00001192
Iteration 647/1000 | Loss: 0.00001192
Iteration 648/1000 | Loss: 0.00001191
Iteration 649/1000 | Loss: 0.00001189
Iteration 650/1000 | Loss: 0.00001188
Iteration 651/1000 | Loss: 0.00001188
Iteration 652/1000 | Loss: 0.00001188
Iteration 653/1000 | Loss: 0.00001188
Iteration 654/1000 | Loss: 0.00001187
Iteration 655/1000 | Loss: 0.00001187
Iteration 656/1000 | Loss: 0.00001187
Iteration 657/1000 | Loss: 0.00001186
Iteration 658/1000 | Loss: 0.00001186
Iteration 659/1000 | Loss: 0.00001186
Iteration 660/1000 | Loss: 0.00001186
Iteration 661/1000 | Loss: 0.00001186
Iteration 662/1000 | Loss: 0.00001186
Iteration 663/1000 | Loss: 0.00001185
Iteration 664/1000 | Loss: 0.00001185
Iteration 665/1000 | Loss: 0.00001185
Iteration 666/1000 | Loss: 0.00001184
Iteration 667/1000 | Loss: 0.00001184
Iteration 668/1000 | Loss: 0.00001184
Iteration 669/1000 | Loss: 0.00001183
Iteration 670/1000 | Loss: 0.00001183
Iteration 671/1000 | Loss: 0.00001183
Iteration 672/1000 | Loss: 0.00001183
Iteration 673/1000 | Loss: 0.00001182
Iteration 674/1000 | Loss: 0.00001182
Iteration 675/1000 | Loss: 0.00001182
Iteration 676/1000 | Loss: 0.00001182
Iteration 677/1000 | Loss: 0.00001181
Iteration 678/1000 | Loss: 0.00001181
Iteration 679/1000 | Loss: 0.00001181
Iteration 680/1000 | Loss: 0.00001181
Iteration 681/1000 | Loss: 0.00001181
Iteration 682/1000 | Loss: 0.00001181
Iteration 683/1000 | Loss: 0.00001181
Iteration 684/1000 | Loss: 0.00001181
Iteration 685/1000 | Loss: 0.00001181
Iteration 686/1000 | Loss: 0.00001181
Iteration 687/1000 | Loss: 0.00001180
Iteration 688/1000 | Loss: 0.00001180
Iteration 689/1000 | Loss: 0.00001180
Iteration 690/1000 | Loss: 0.00001180
Iteration 691/1000 | Loss: 0.00001180
Iteration 692/1000 | Loss: 0.00001180
Iteration 693/1000 | Loss: 0.00001180
Iteration 694/1000 | Loss: 0.00001180
Iteration 695/1000 | Loss: 0.00001180
Iteration 696/1000 | Loss: 0.00001180
Iteration 697/1000 | Loss: 0.00001180
Iteration 698/1000 | Loss: 0.00001180
Iteration 699/1000 | Loss: 0.00001180
Iteration 700/1000 | Loss: 0.00001180
Iteration 701/1000 | Loss: 0.00001180
Iteration 702/1000 | Loss: 0.00001179
Iteration 703/1000 | Loss: 0.00001179
Iteration 704/1000 | Loss: 0.00001179
Iteration 705/1000 | Loss: 0.00001179
Iteration 706/1000 | Loss: 0.00001179
Iteration 707/1000 | Loss: 0.00001179
Iteration 708/1000 | Loss: 0.00001179
Iteration 709/1000 | Loss: 0.00001179
Iteration 710/1000 | Loss: 0.00001179
Iteration 711/1000 | Loss: 0.00001179
Iteration 712/1000 | Loss: 0.00001179
Iteration 713/1000 | Loss: 0.00001179
Iteration 714/1000 | Loss: 0.00001179
Iteration 715/1000 | Loss: 0.00001179
Iteration 716/1000 | Loss: 0.00001178
Iteration 717/1000 | Loss: 0.00001178
Iteration 718/1000 | Loss: 0.00001178
Iteration 719/1000 | Loss: 0.00001178
Iteration 720/1000 | Loss: 0.00001178
Iteration 721/1000 | Loss: 0.00001178
Iteration 722/1000 | Loss: 0.00001178
Iteration 723/1000 | Loss: 0.00001178
Iteration 724/1000 | Loss: 0.00001178
Iteration 725/1000 | Loss: 0.00001178
Iteration 726/1000 | Loss: 0.00001178
Iteration 727/1000 | Loss: 0.00001178
Iteration 728/1000 | Loss: 0.00001177
Iteration 729/1000 | Loss: 0.00001177
Iteration 730/1000 | Loss: 0.00001177
Iteration 731/1000 | Loss: 0.00001177
Iteration 732/1000 | Loss: 0.00001177
Iteration 733/1000 | Loss: 0.00001177
Iteration 734/1000 | Loss: 0.00001177
Iteration 735/1000 | Loss: 0.00001177
Iteration 736/1000 | Loss: 0.00001177
Iteration 737/1000 | Loss: 0.00001177
Iteration 738/1000 | Loss: 0.00001177
Iteration 739/1000 | Loss: 0.00001177
Iteration 740/1000 | Loss: 0.00001177
Iteration 741/1000 | Loss: 0.00001177
Iteration 742/1000 | Loss: 0.00001177
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 742. Stopping optimization.
Last 5 losses: [1.1768934200517833e-05, 1.1768934200517833e-05, 1.1768934200517833e-05, 1.1768934200517833e-05, 1.1768934200517833e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1768934200517833e-05

Optimization complete. Final v2v error: 2.895498275756836 mm

Highest mean error: 4.200098991394043 mm for frame 45

Lowest mean error: 2.4736275672912598 mm for frame 0

Saving results

Total time: 692.5402500629425
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_025/1044/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_025/1044.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_025/1044
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00368417
Iteration 2/25 | Loss: 0.00127563
Iteration 3/25 | Loss: 0.00117588
Iteration 4/25 | Loss: 0.00116400
Iteration 5/25 | Loss: 0.00115975
Iteration 6/25 | Loss: 0.00115840
Iteration 7/25 | Loss: 0.00115840
Iteration 8/25 | Loss: 0.00115840
Iteration 9/25 | Loss: 0.00115840
Iteration 10/25 | Loss: 0.00115840
Iteration 11/25 | Loss: 0.00115840
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011584014864638448, 0.0011584014864638448, 0.0011584014864638448, 0.0011584014864638448, 0.0011584014864638448]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011584014864638448

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.85488838
Iteration 2/25 | Loss: 0.00105325
Iteration 3/25 | Loss: 0.00105324
Iteration 4/25 | Loss: 0.00105324
Iteration 5/25 | Loss: 0.00105324
Iteration 6/25 | Loss: 0.00105324
Iteration 7/25 | Loss: 0.00105324
Iteration 8/25 | Loss: 0.00105324
Iteration 9/25 | Loss: 0.00105324
Iteration 10/25 | Loss: 0.00105324
Iteration 11/25 | Loss: 0.00105324
Iteration 12/25 | Loss: 0.00105324
Iteration 13/25 | Loss: 0.00105324
Iteration 14/25 | Loss: 0.00105324
Iteration 15/25 | Loss: 0.00105324
Iteration 16/25 | Loss: 0.00105324
Iteration 17/25 | Loss: 0.00105324
Iteration 18/25 | Loss: 0.00105324
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0010532394517213106, 0.0010532394517213106, 0.0010532394517213106, 0.0010532394517213106, 0.0010532394517213106]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010532394517213106

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00105324
Iteration 2/1000 | Loss: 0.00003760
Iteration 3/1000 | Loss: 0.00002206
Iteration 4/1000 | Loss: 0.00001977
Iteration 5/1000 | Loss: 0.00001846
Iteration 6/1000 | Loss: 0.00001750
Iteration 7/1000 | Loss: 0.00001686
Iteration 8/1000 | Loss: 0.00001648
Iteration 9/1000 | Loss: 0.00001619
Iteration 10/1000 | Loss: 0.00001592
Iteration 11/1000 | Loss: 0.00001570
Iteration 12/1000 | Loss: 0.00001550
Iteration 13/1000 | Loss: 0.00001540
Iteration 14/1000 | Loss: 0.00001533
Iteration 15/1000 | Loss: 0.00001533
Iteration 16/1000 | Loss: 0.00001532
Iteration 17/1000 | Loss: 0.00001528
Iteration 18/1000 | Loss: 0.00001528
Iteration 19/1000 | Loss: 0.00001528
Iteration 20/1000 | Loss: 0.00001528
Iteration 21/1000 | Loss: 0.00001528
Iteration 22/1000 | Loss: 0.00001527
Iteration 23/1000 | Loss: 0.00001526
Iteration 24/1000 | Loss: 0.00001521
Iteration 25/1000 | Loss: 0.00001519
Iteration 26/1000 | Loss: 0.00001519
Iteration 27/1000 | Loss: 0.00001518
Iteration 28/1000 | Loss: 0.00001515
Iteration 29/1000 | Loss: 0.00001512
Iteration 30/1000 | Loss: 0.00001511
Iteration 31/1000 | Loss: 0.00001511
Iteration 32/1000 | Loss: 0.00001510
Iteration 33/1000 | Loss: 0.00001509
Iteration 34/1000 | Loss: 0.00001508
Iteration 35/1000 | Loss: 0.00001508
Iteration 36/1000 | Loss: 0.00001507
Iteration 37/1000 | Loss: 0.00001507
Iteration 38/1000 | Loss: 0.00001504
Iteration 39/1000 | Loss: 0.00001503
Iteration 40/1000 | Loss: 0.00001501
Iteration 41/1000 | Loss: 0.00001500
Iteration 42/1000 | Loss: 0.00001500
Iteration 43/1000 | Loss: 0.00001499
Iteration 44/1000 | Loss: 0.00001496
Iteration 45/1000 | Loss: 0.00001491
Iteration 46/1000 | Loss: 0.00001491
Iteration 47/1000 | Loss: 0.00001491
Iteration 48/1000 | Loss: 0.00001491
Iteration 49/1000 | Loss: 0.00001488
Iteration 50/1000 | Loss: 0.00001487
Iteration 51/1000 | Loss: 0.00001485
Iteration 52/1000 | Loss: 0.00001484
Iteration 53/1000 | Loss: 0.00001483
Iteration 54/1000 | Loss: 0.00001483
Iteration 55/1000 | Loss: 0.00001482
Iteration 56/1000 | Loss: 0.00001481
Iteration 57/1000 | Loss: 0.00001481
Iteration 58/1000 | Loss: 0.00001478
Iteration 59/1000 | Loss: 0.00001478
Iteration 60/1000 | Loss: 0.00001478
Iteration 61/1000 | Loss: 0.00001477
Iteration 62/1000 | Loss: 0.00001476
Iteration 63/1000 | Loss: 0.00001476
Iteration 64/1000 | Loss: 0.00001476
Iteration 65/1000 | Loss: 0.00001471
Iteration 66/1000 | Loss: 0.00001471
Iteration 67/1000 | Loss: 0.00001469
Iteration 68/1000 | Loss: 0.00001469
Iteration 69/1000 | Loss: 0.00001468
Iteration 70/1000 | Loss: 0.00001468
Iteration 71/1000 | Loss: 0.00001468
Iteration 72/1000 | Loss: 0.00001468
Iteration 73/1000 | Loss: 0.00001467
Iteration 74/1000 | Loss: 0.00001467
Iteration 75/1000 | Loss: 0.00001466
Iteration 76/1000 | Loss: 0.00001466
Iteration 77/1000 | Loss: 0.00001466
Iteration 78/1000 | Loss: 0.00001465
Iteration 79/1000 | Loss: 0.00001465
Iteration 80/1000 | Loss: 0.00001465
Iteration 81/1000 | Loss: 0.00001463
Iteration 82/1000 | Loss: 0.00001463
Iteration 83/1000 | Loss: 0.00001463
Iteration 84/1000 | Loss: 0.00001463
Iteration 85/1000 | Loss: 0.00001463
Iteration 86/1000 | Loss: 0.00001463
Iteration 87/1000 | Loss: 0.00001462
Iteration 88/1000 | Loss: 0.00001462
Iteration 89/1000 | Loss: 0.00001461
Iteration 90/1000 | Loss: 0.00001461
Iteration 91/1000 | Loss: 0.00001461
Iteration 92/1000 | Loss: 0.00001460
Iteration 93/1000 | Loss: 0.00001460
Iteration 94/1000 | Loss: 0.00001460
Iteration 95/1000 | Loss: 0.00001460
Iteration 96/1000 | Loss: 0.00001459
Iteration 97/1000 | Loss: 0.00001459
Iteration 98/1000 | Loss: 0.00001459
Iteration 99/1000 | Loss: 0.00001459
Iteration 100/1000 | Loss: 0.00001459
Iteration 101/1000 | Loss: 0.00001459
Iteration 102/1000 | Loss: 0.00001458
Iteration 103/1000 | Loss: 0.00001458
Iteration 104/1000 | Loss: 0.00001458
Iteration 105/1000 | Loss: 0.00001458
Iteration 106/1000 | Loss: 0.00001458
Iteration 107/1000 | Loss: 0.00001458
Iteration 108/1000 | Loss: 0.00001458
Iteration 109/1000 | Loss: 0.00001458
Iteration 110/1000 | Loss: 0.00001457
Iteration 111/1000 | Loss: 0.00001457
Iteration 112/1000 | Loss: 0.00001457
Iteration 113/1000 | Loss: 0.00001457
Iteration 114/1000 | Loss: 0.00001457
Iteration 115/1000 | Loss: 0.00001457
Iteration 116/1000 | Loss: 0.00001457
Iteration 117/1000 | Loss: 0.00001456
Iteration 118/1000 | Loss: 0.00001456
Iteration 119/1000 | Loss: 0.00001456
Iteration 120/1000 | Loss: 0.00001456
Iteration 121/1000 | Loss: 0.00001456
Iteration 122/1000 | Loss: 0.00001456
Iteration 123/1000 | Loss: 0.00001456
Iteration 124/1000 | Loss: 0.00001456
Iteration 125/1000 | Loss: 0.00001455
Iteration 126/1000 | Loss: 0.00001455
Iteration 127/1000 | Loss: 0.00001455
Iteration 128/1000 | Loss: 0.00001455
Iteration 129/1000 | Loss: 0.00001455
Iteration 130/1000 | Loss: 0.00001454
Iteration 131/1000 | Loss: 0.00001454
Iteration 132/1000 | Loss: 0.00001454
Iteration 133/1000 | Loss: 0.00001454
Iteration 134/1000 | Loss: 0.00001454
Iteration 135/1000 | Loss: 0.00001453
Iteration 136/1000 | Loss: 0.00001453
Iteration 137/1000 | Loss: 0.00001453
Iteration 138/1000 | Loss: 0.00001453
Iteration 139/1000 | Loss: 0.00001453
Iteration 140/1000 | Loss: 0.00001453
Iteration 141/1000 | Loss: 0.00001453
Iteration 142/1000 | Loss: 0.00001453
Iteration 143/1000 | Loss: 0.00001452
Iteration 144/1000 | Loss: 0.00001452
Iteration 145/1000 | Loss: 0.00001452
Iteration 146/1000 | Loss: 0.00001452
Iteration 147/1000 | Loss: 0.00001452
Iteration 148/1000 | Loss: 0.00001452
Iteration 149/1000 | Loss: 0.00001452
Iteration 150/1000 | Loss: 0.00001452
Iteration 151/1000 | Loss: 0.00001452
Iteration 152/1000 | Loss: 0.00001452
Iteration 153/1000 | Loss: 0.00001452
Iteration 154/1000 | Loss: 0.00001451
Iteration 155/1000 | Loss: 0.00001451
Iteration 156/1000 | Loss: 0.00001451
Iteration 157/1000 | Loss: 0.00001451
Iteration 158/1000 | Loss: 0.00001451
Iteration 159/1000 | Loss: 0.00001451
Iteration 160/1000 | Loss: 0.00001450
Iteration 161/1000 | Loss: 0.00001450
Iteration 162/1000 | Loss: 0.00001450
Iteration 163/1000 | Loss: 0.00001450
Iteration 164/1000 | Loss: 0.00001449
Iteration 165/1000 | Loss: 0.00001449
Iteration 166/1000 | Loss: 0.00001449
Iteration 167/1000 | Loss: 0.00001449
Iteration 168/1000 | Loss: 0.00001449
Iteration 169/1000 | Loss: 0.00001449
Iteration 170/1000 | Loss: 0.00001449
Iteration 171/1000 | Loss: 0.00001449
Iteration 172/1000 | Loss: 0.00001449
Iteration 173/1000 | Loss: 0.00001449
Iteration 174/1000 | Loss: 0.00001448
Iteration 175/1000 | Loss: 0.00001448
Iteration 176/1000 | Loss: 0.00001448
Iteration 177/1000 | Loss: 0.00001448
Iteration 178/1000 | Loss: 0.00001448
Iteration 179/1000 | Loss: 0.00001448
Iteration 180/1000 | Loss: 0.00001448
Iteration 181/1000 | Loss: 0.00001448
Iteration 182/1000 | Loss: 0.00001448
Iteration 183/1000 | Loss: 0.00001448
Iteration 184/1000 | Loss: 0.00001448
Iteration 185/1000 | Loss: 0.00001448
Iteration 186/1000 | Loss: 0.00001447
Iteration 187/1000 | Loss: 0.00001447
Iteration 188/1000 | Loss: 0.00001447
Iteration 189/1000 | Loss: 0.00001447
Iteration 190/1000 | Loss: 0.00001446
Iteration 191/1000 | Loss: 0.00001446
Iteration 192/1000 | Loss: 0.00001446
Iteration 193/1000 | Loss: 0.00001446
Iteration 194/1000 | Loss: 0.00001446
Iteration 195/1000 | Loss: 0.00001446
Iteration 196/1000 | Loss: 0.00001446
Iteration 197/1000 | Loss: 0.00001446
Iteration 198/1000 | Loss: 0.00001446
Iteration 199/1000 | Loss: 0.00001446
Iteration 200/1000 | Loss: 0.00001445
Iteration 201/1000 | Loss: 0.00001445
Iteration 202/1000 | Loss: 0.00001445
Iteration 203/1000 | Loss: 0.00001445
Iteration 204/1000 | Loss: 0.00001445
Iteration 205/1000 | Loss: 0.00001445
Iteration 206/1000 | Loss: 0.00001445
Iteration 207/1000 | Loss: 0.00001445
Iteration 208/1000 | Loss: 0.00001445
Iteration 209/1000 | Loss: 0.00001444
Iteration 210/1000 | Loss: 0.00001444
Iteration 211/1000 | Loss: 0.00001444
Iteration 212/1000 | Loss: 0.00001444
Iteration 213/1000 | Loss: 0.00001444
Iteration 214/1000 | Loss: 0.00001444
Iteration 215/1000 | Loss: 0.00001444
Iteration 216/1000 | Loss: 0.00001444
Iteration 217/1000 | Loss: 0.00001444
Iteration 218/1000 | Loss: 0.00001444
Iteration 219/1000 | Loss: 0.00001443
Iteration 220/1000 | Loss: 0.00001443
Iteration 221/1000 | Loss: 0.00001443
Iteration 222/1000 | Loss: 0.00001443
Iteration 223/1000 | Loss: 0.00001443
Iteration 224/1000 | Loss: 0.00001443
Iteration 225/1000 | Loss: 0.00001443
Iteration 226/1000 | Loss: 0.00001443
Iteration 227/1000 | Loss: 0.00001443
Iteration 228/1000 | Loss: 0.00001443
Iteration 229/1000 | Loss: 0.00001443
Iteration 230/1000 | Loss: 0.00001442
Iteration 231/1000 | Loss: 0.00001442
Iteration 232/1000 | Loss: 0.00001442
Iteration 233/1000 | Loss: 0.00001442
Iteration 234/1000 | Loss: 0.00001442
Iteration 235/1000 | Loss: 0.00001442
Iteration 236/1000 | Loss: 0.00001442
Iteration 237/1000 | Loss: 0.00001441
Iteration 238/1000 | Loss: 0.00001441
Iteration 239/1000 | Loss: 0.00001441
Iteration 240/1000 | Loss: 0.00001441
Iteration 241/1000 | Loss: 0.00001441
Iteration 242/1000 | Loss: 0.00001441
Iteration 243/1000 | Loss: 0.00001441
Iteration 244/1000 | Loss: 0.00001441
Iteration 245/1000 | Loss: 0.00001441
Iteration 246/1000 | Loss: 0.00001441
Iteration 247/1000 | Loss: 0.00001441
Iteration 248/1000 | Loss: 0.00001440
Iteration 249/1000 | Loss: 0.00001440
Iteration 250/1000 | Loss: 0.00001440
Iteration 251/1000 | Loss: 0.00001440
Iteration 252/1000 | Loss: 0.00001440
Iteration 253/1000 | Loss: 0.00001440
Iteration 254/1000 | Loss: 0.00001440
Iteration 255/1000 | Loss: 0.00001440
Iteration 256/1000 | Loss: 0.00001440
Iteration 257/1000 | Loss: 0.00001440
Iteration 258/1000 | Loss: 0.00001440
Iteration 259/1000 | Loss: 0.00001440
Iteration 260/1000 | Loss: 0.00001440
Iteration 261/1000 | Loss: 0.00001439
Iteration 262/1000 | Loss: 0.00001439
Iteration 263/1000 | Loss: 0.00001439
Iteration 264/1000 | Loss: 0.00001439
Iteration 265/1000 | Loss: 0.00001439
Iteration 266/1000 | Loss: 0.00001439
Iteration 267/1000 | Loss: 0.00001439
Iteration 268/1000 | Loss: 0.00001439
Iteration 269/1000 | Loss: 0.00001439
Iteration 270/1000 | Loss: 0.00001439
Iteration 271/1000 | Loss: 0.00001439
Iteration 272/1000 | Loss: 0.00001439
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 272. Stopping optimization.
Last 5 losses: [1.4391571312444285e-05, 1.4391571312444285e-05, 1.4391571312444285e-05, 1.4391571312444285e-05, 1.4391571312444285e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4391571312444285e-05

Optimization complete. Final v2v error: 3.1910200119018555 mm

Highest mean error: 3.304226875305176 mm for frame 215

Lowest mean error: 3.122471332550049 mm for frame 189

Saving results

Total time: 60.07592749595642
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_025/1077/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_025/1077.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_025/1077
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00960045
Iteration 2/25 | Loss: 0.00960045
Iteration 3/25 | Loss: 0.00220490
Iteration 4/25 | Loss: 0.00146157
Iteration 5/25 | Loss: 0.00138365
Iteration 6/25 | Loss: 0.00136206
Iteration 7/25 | Loss: 0.00131635
Iteration 8/25 | Loss: 0.00126256
Iteration 9/25 | Loss: 0.00124343
Iteration 10/25 | Loss: 0.00123085
Iteration 11/25 | Loss: 0.00123587
Iteration 12/25 | Loss: 0.00122499
Iteration 13/25 | Loss: 0.00121454
Iteration 14/25 | Loss: 0.00121164
Iteration 15/25 | Loss: 0.00121486
Iteration 16/25 | Loss: 0.00120994
Iteration 17/25 | Loss: 0.00121410
Iteration 18/25 | Loss: 0.00120892
Iteration 19/25 | Loss: 0.00120884
Iteration 20/25 | Loss: 0.00120883
Iteration 21/25 | Loss: 0.00120883
Iteration 22/25 | Loss: 0.00120882
Iteration 23/25 | Loss: 0.00120882
Iteration 24/25 | Loss: 0.00120882
Iteration 25/25 | Loss: 0.00120882

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35217810
Iteration 2/25 | Loss: 0.00070341
Iteration 3/25 | Loss: 0.00066109
Iteration 4/25 | Loss: 0.00066109
Iteration 5/25 | Loss: 0.00066109
Iteration 6/25 | Loss: 0.00066108
Iteration 7/25 | Loss: 0.00066108
Iteration 8/25 | Loss: 0.00066108
Iteration 9/25 | Loss: 0.00066108
Iteration 10/25 | Loss: 0.00066108
Iteration 11/25 | Loss: 0.00066108
Iteration 12/25 | Loss: 0.00066108
Iteration 13/25 | Loss: 0.00066108
Iteration 14/25 | Loss: 0.00066108
Iteration 15/25 | Loss: 0.00066108
Iteration 16/25 | Loss: 0.00066108
Iteration 17/25 | Loss: 0.00066108
Iteration 18/25 | Loss: 0.00066108
Iteration 19/25 | Loss: 0.00066108
Iteration 20/25 | Loss: 0.00066108
Iteration 21/25 | Loss: 0.00066108
Iteration 22/25 | Loss: 0.00066108
Iteration 23/25 | Loss: 0.00066108
Iteration 24/25 | Loss: 0.00066108
Iteration 25/25 | Loss: 0.00066108

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00066108
Iteration 2/1000 | Loss: 0.00003333
Iteration 3/1000 | Loss: 0.00002298
Iteration 4/1000 | Loss: 0.00002087
Iteration 5/1000 | Loss: 0.00001988
Iteration 6/1000 | Loss: 0.00008490
Iteration 7/1000 | Loss: 0.00001874
Iteration 8/1000 | Loss: 0.00001820
Iteration 9/1000 | Loss: 0.00006137
Iteration 10/1000 | Loss: 0.00001789
Iteration 11/1000 | Loss: 0.00001751
Iteration 12/1000 | Loss: 0.00002442
Iteration 13/1000 | Loss: 0.00007521
Iteration 14/1000 | Loss: 0.00001973
Iteration 15/1000 | Loss: 0.00001769
Iteration 16/1000 | Loss: 0.00005897
Iteration 17/1000 | Loss: 0.00001644
Iteration 18/1000 | Loss: 0.00001606
Iteration 19/1000 | Loss: 0.00001577
Iteration 20/1000 | Loss: 0.00001559
Iteration 21/1000 | Loss: 0.00001549
Iteration 22/1000 | Loss: 0.00001545
Iteration 23/1000 | Loss: 0.00001537
Iteration 24/1000 | Loss: 0.00001534
Iteration 25/1000 | Loss: 0.00001534
Iteration 26/1000 | Loss: 0.00001533
Iteration 27/1000 | Loss: 0.00001528
Iteration 28/1000 | Loss: 0.00001525
Iteration 29/1000 | Loss: 0.00001525
Iteration 30/1000 | Loss: 0.00001524
Iteration 31/1000 | Loss: 0.00001524
Iteration 32/1000 | Loss: 0.00001524
Iteration 33/1000 | Loss: 0.00001523
Iteration 34/1000 | Loss: 0.00001522
Iteration 35/1000 | Loss: 0.00001521
Iteration 36/1000 | Loss: 0.00001519
Iteration 37/1000 | Loss: 0.00001515
Iteration 38/1000 | Loss: 0.00001514
Iteration 39/1000 | Loss: 0.00001514
Iteration 40/1000 | Loss: 0.00001513
Iteration 41/1000 | Loss: 0.00001512
Iteration 42/1000 | Loss: 0.00001512
Iteration 43/1000 | Loss: 0.00001512
Iteration 44/1000 | Loss: 0.00001512
Iteration 45/1000 | Loss: 0.00001511
Iteration 46/1000 | Loss: 0.00001511
Iteration 47/1000 | Loss: 0.00001511
Iteration 48/1000 | Loss: 0.00001508
Iteration 49/1000 | Loss: 0.00001508
Iteration 50/1000 | Loss: 0.00001507
Iteration 51/1000 | Loss: 0.00001505
Iteration 52/1000 | Loss: 0.00001499
Iteration 53/1000 | Loss: 0.00001498
Iteration 54/1000 | Loss: 0.00001498
Iteration 55/1000 | Loss: 0.00001498
Iteration 56/1000 | Loss: 0.00001498
Iteration 57/1000 | Loss: 0.00001498
Iteration 58/1000 | Loss: 0.00001497
Iteration 59/1000 | Loss: 0.00001497
Iteration 60/1000 | Loss: 0.00001497
Iteration 61/1000 | Loss: 0.00001497
Iteration 62/1000 | Loss: 0.00001496
Iteration 63/1000 | Loss: 0.00001496
Iteration 64/1000 | Loss: 0.00001496
Iteration 65/1000 | Loss: 0.00001496
Iteration 66/1000 | Loss: 0.00001496
Iteration 67/1000 | Loss: 0.00001496
Iteration 68/1000 | Loss: 0.00001496
Iteration 69/1000 | Loss: 0.00001496
Iteration 70/1000 | Loss: 0.00001496
Iteration 71/1000 | Loss: 0.00001496
Iteration 72/1000 | Loss: 0.00001495
Iteration 73/1000 | Loss: 0.00001495
Iteration 74/1000 | Loss: 0.00001495
Iteration 75/1000 | Loss: 0.00001495
Iteration 76/1000 | Loss: 0.00001494
Iteration 77/1000 | Loss: 0.00001494
Iteration 78/1000 | Loss: 0.00001494
Iteration 79/1000 | Loss: 0.00001494
Iteration 80/1000 | Loss: 0.00001494
Iteration 81/1000 | Loss: 0.00001494
Iteration 82/1000 | Loss: 0.00001493
Iteration 83/1000 | Loss: 0.00001493
Iteration 84/1000 | Loss: 0.00001493
Iteration 85/1000 | Loss: 0.00001493
Iteration 86/1000 | Loss: 0.00001492
Iteration 87/1000 | Loss: 0.00001492
Iteration 88/1000 | Loss: 0.00001492
Iteration 89/1000 | Loss: 0.00001492
Iteration 90/1000 | Loss: 0.00001492
Iteration 91/1000 | Loss: 0.00001492
Iteration 92/1000 | Loss: 0.00001491
Iteration 93/1000 | Loss: 0.00001491
Iteration 94/1000 | Loss: 0.00001491
Iteration 95/1000 | Loss: 0.00001491
Iteration 96/1000 | Loss: 0.00001491
Iteration 97/1000 | Loss: 0.00001491
Iteration 98/1000 | Loss: 0.00001491
Iteration 99/1000 | Loss: 0.00001491
Iteration 100/1000 | Loss: 0.00001491
Iteration 101/1000 | Loss: 0.00001491
Iteration 102/1000 | Loss: 0.00001490
Iteration 103/1000 | Loss: 0.00001490
Iteration 104/1000 | Loss: 0.00001490
Iteration 105/1000 | Loss: 0.00001490
Iteration 106/1000 | Loss: 0.00001490
Iteration 107/1000 | Loss: 0.00001490
Iteration 108/1000 | Loss: 0.00001490
Iteration 109/1000 | Loss: 0.00001490
Iteration 110/1000 | Loss: 0.00001490
Iteration 111/1000 | Loss: 0.00001490
Iteration 112/1000 | Loss: 0.00001489
Iteration 113/1000 | Loss: 0.00001489
Iteration 114/1000 | Loss: 0.00001489
Iteration 115/1000 | Loss: 0.00001489
Iteration 116/1000 | Loss: 0.00001489
Iteration 117/1000 | Loss: 0.00001489
Iteration 118/1000 | Loss: 0.00001489
Iteration 119/1000 | Loss: 0.00001489
Iteration 120/1000 | Loss: 0.00001489
Iteration 121/1000 | Loss: 0.00001489
Iteration 122/1000 | Loss: 0.00001489
Iteration 123/1000 | Loss: 0.00001489
Iteration 124/1000 | Loss: 0.00001489
Iteration 125/1000 | Loss: 0.00001489
Iteration 126/1000 | Loss: 0.00001489
Iteration 127/1000 | Loss: 0.00001488
Iteration 128/1000 | Loss: 0.00001488
Iteration 129/1000 | Loss: 0.00001488
Iteration 130/1000 | Loss: 0.00001488
Iteration 131/1000 | Loss: 0.00001488
Iteration 132/1000 | Loss: 0.00001488
Iteration 133/1000 | Loss: 0.00001488
Iteration 134/1000 | Loss: 0.00001488
Iteration 135/1000 | Loss: 0.00001488
Iteration 136/1000 | Loss: 0.00001488
Iteration 137/1000 | Loss: 0.00001488
Iteration 138/1000 | Loss: 0.00001487
Iteration 139/1000 | Loss: 0.00001487
Iteration 140/1000 | Loss: 0.00001487
Iteration 141/1000 | Loss: 0.00001487
Iteration 142/1000 | Loss: 0.00001487
Iteration 143/1000 | Loss: 0.00001487
Iteration 144/1000 | Loss: 0.00001487
Iteration 145/1000 | Loss: 0.00001487
Iteration 146/1000 | Loss: 0.00001487
Iteration 147/1000 | Loss: 0.00001487
Iteration 148/1000 | Loss: 0.00001487
Iteration 149/1000 | Loss: 0.00001487
Iteration 150/1000 | Loss: 0.00001487
Iteration 151/1000 | Loss: 0.00001487
Iteration 152/1000 | Loss: 0.00001487
Iteration 153/1000 | Loss: 0.00001487
Iteration 154/1000 | Loss: 0.00001487
Iteration 155/1000 | Loss: 0.00001487
Iteration 156/1000 | Loss: 0.00001487
Iteration 157/1000 | Loss: 0.00001487
Iteration 158/1000 | Loss: 0.00001487
Iteration 159/1000 | Loss: 0.00001487
Iteration 160/1000 | Loss: 0.00001487
Iteration 161/1000 | Loss: 0.00001486
Iteration 162/1000 | Loss: 0.00001486
Iteration 163/1000 | Loss: 0.00001486
Iteration 164/1000 | Loss: 0.00001486
Iteration 165/1000 | Loss: 0.00001486
Iteration 166/1000 | Loss: 0.00001486
Iteration 167/1000 | Loss: 0.00001486
Iteration 168/1000 | Loss: 0.00001486
Iteration 169/1000 | Loss: 0.00001486
Iteration 170/1000 | Loss: 0.00001486
Iteration 171/1000 | Loss: 0.00001486
Iteration 172/1000 | Loss: 0.00001486
Iteration 173/1000 | Loss: 0.00001486
Iteration 174/1000 | Loss: 0.00001486
Iteration 175/1000 | Loss: 0.00001486
Iteration 176/1000 | Loss: 0.00001486
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 176. Stopping optimization.
Last 5 losses: [1.486464316258207e-05, 1.486464316258207e-05, 1.486464316258207e-05, 1.486464316258207e-05, 1.486464316258207e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.486464316258207e-05

Optimization complete. Final v2v error: 3.2203445434570312 mm

Highest mean error: 4.412323951721191 mm for frame 64

Lowest mean error: 2.737959384918213 mm for frame 187

Saving results

Total time: 90.19447445869446
