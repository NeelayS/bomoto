Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=86, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 4816-4871
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_32_us_0430/0006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_us_0430/0006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_us_0430/0006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00872914
Iteration 2/25 | Loss: 0.00096041
Iteration 3/25 | Loss: 0.00080838
Iteration 4/25 | Loss: 0.00078633
Iteration 5/25 | Loss: 0.00078073
Iteration 6/25 | Loss: 0.00077921
Iteration 7/25 | Loss: 0.00077884
Iteration 8/25 | Loss: 0.00077884
Iteration 9/25 | Loss: 0.00077884
Iteration 10/25 | Loss: 0.00077884
Iteration 11/25 | Loss: 0.00077884
Iteration 12/25 | Loss: 0.00077884
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0007788438815623522, 0.0007788438815623522, 0.0007788438815623522, 0.0007788438815623522, 0.0007788438815623522]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007788438815623522

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44688368
Iteration 2/25 | Loss: 0.00053728
Iteration 3/25 | Loss: 0.00053728
Iteration 4/25 | Loss: 0.00053728
Iteration 5/25 | Loss: 0.00053728
Iteration 6/25 | Loss: 0.00053728
Iteration 7/25 | Loss: 0.00053728
Iteration 8/25 | Loss: 0.00053728
Iteration 9/25 | Loss: 0.00053728
Iteration 10/25 | Loss: 0.00053728
Iteration 11/25 | Loss: 0.00053728
Iteration 12/25 | Loss: 0.00053728
Iteration 13/25 | Loss: 0.00053728
Iteration 14/25 | Loss: 0.00053728
Iteration 15/25 | Loss: 0.00053728
Iteration 16/25 | Loss: 0.00053728
Iteration 17/25 | Loss: 0.00053728
Iteration 18/25 | Loss: 0.00053728
Iteration 19/25 | Loss: 0.00053728
Iteration 20/25 | Loss: 0.00053728
Iteration 21/25 | Loss: 0.00053728
Iteration 22/25 | Loss: 0.00053728
Iteration 23/25 | Loss: 0.00053728
Iteration 24/25 | Loss: 0.00053728
Iteration 25/25 | Loss: 0.00053728

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00053728
Iteration 2/1000 | Loss: 0.00003478
Iteration 3/1000 | Loss: 0.00002044
Iteration 4/1000 | Loss: 0.00001801
Iteration 5/1000 | Loss: 0.00001638
Iteration 6/1000 | Loss: 0.00001559
Iteration 7/1000 | Loss: 0.00001510
Iteration 8/1000 | Loss: 0.00001487
Iteration 9/1000 | Loss: 0.00001471
Iteration 10/1000 | Loss: 0.00001469
Iteration 11/1000 | Loss: 0.00001465
Iteration 12/1000 | Loss: 0.00001464
Iteration 13/1000 | Loss: 0.00001464
Iteration 14/1000 | Loss: 0.00001464
Iteration 15/1000 | Loss: 0.00001464
Iteration 16/1000 | Loss: 0.00001464
Iteration 17/1000 | Loss: 0.00001463
Iteration 18/1000 | Loss: 0.00001461
Iteration 19/1000 | Loss: 0.00001461
Iteration 20/1000 | Loss: 0.00001461
Iteration 21/1000 | Loss: 0.00001461
Iteration 22/1000 | Loss: 0.00001460
Iteration 23/1000 | Loss: 0.00001459
Iteration 24/1000 | Loss: 0.00001458
Iteration 25/1000 | Loss: 0.00001458
Iteration 26/1000 | Loss: 0.00001457
Iteration 27/1000 | Loss: 0.00001457
Iteration 28/1000 | Loss: 0.00001457
Iteration 29/1000 | Loss: 0.00001457
Iteration 30/1000 | Loss: 0.00001457
Iteration 31/1000 | Loss: 0.00001457
Iteration 32/1000 | Loss: 0.00001456
Iteration 33/1000 | Loss: 0.00001456
Iteration 34/1000 | Loss: 0.00001456
Iteration 35/1000 | Loss: 0.00001455
Iteration 36/1000 | Loss: 0.00001455
Iteration 37/1000 | Loss: 0.00001455
Iteration 38/1000 | Loss: 0.00001454
Iteration 39/1000 | Loss: 0.00001454
Iteration 40/1000 | Loss: 0.00001454
Iteration 41/1000 | Loss: 0.00001454
Iteration 42/1000 | Loss: 0.00001454
Iteration 43/1000 | Loss: 0.00001454
Iteration 44/1000 | Loss: 0.00001454
Iteration 45/1000 | Loss: 0.00001453
Iteration 46/1000 | Loss: 0.00001453
Iteration 47/1000 | Loss: 0.00001453
Iteration 48/1000 | Loss: 0.00001453
Iteration 49/1000 | Loss: 0.00001453
Iteration 50/1000 | Loss: 0.00001453
Iteration 51/1000 | Loss: 0.00001453
Iteration 52/1000 | Loss: 0.00001453
Iteration 53/1000 | Loss: 0.00001453
Iteration 54/1000 | Loss: 0.00001453
Iteration 55/1000 | Loss: 0.00001453
Iteration 56/1000 | Loss: 0.00001452
Iteration 57/1000 | Loss: 0.00001452
Iteration 58/1000 | Loss: 0.00001452
Iteration 59/1000 | Loss: 0.00001452
Iteration 60/1000 | Loss: 0.00001452
Iteration 61/1000 | Loss: 0.00001451
Iteration 62/1000 | Loss: 0.00001451
Iteration 63/1000 | Loss: 0.00001451
Iteration 64/1000 | Loss: 0.00001451
Iteration 65/1000 | Loss: 0.00001450
Iteration 66/1000 | Loss: 0.00001450
Iteration 67/1000 | Loss: 0.00001450
Iteration 68/1000 | Loss: 0.00001449
Iteration 69/1000 | Loss: 0.00001449
Iteration 70/1000 | Loss: 0.00001449
Iteration 71/1000 | Loss: 0.00001449
Iteration 72/1000 | Loss: 0.00001448
Iteration 73/1000 | Loss: 0.00001448
Iteration 74/1000 | Loss: 0.00001448
Iteration 75/1000 | Loss: 0.00001448
Iteration 76/1000 | Loss: 0.00001448
Iteration 77/1000 | Loss: 0.00001448
Iteration 78/1000 | Loss: 0.00001448
Iteration 79/1000 | Loss: 0.00001448
Iteration 80/1000 | Loss: 0.00001448
Iteration 81/1000 | Loss: 0.00001448
Iteration 82/1000 | Loss: 0.00001448
Iteration 83/1000 | Loss: 0.00001448
Iteration 84/1000 | Loss: 0.00001448
Iteration 85/1000 | Loss: 0.00001448
Iteration 86/1000 | Loss: 0.00001448
Iteration 87/1000 | Loss: 0.00001447
Iteration 88/1000 | Loss: 0.00001447
Iteration 89/1000 | Loss: 0.00001447
Iteration 90/1000 | Loss: 0.00001447
Iteration 91/1000 | Loss: 0.00001447
Iteration 92/1000 | Loss: 0.00001447
Iteration 93/1000 | Loss: 0.00001447
Iteration 94/1000 | Loss: 0.00001447
Iteration 95/1000 | Loss: 0.00001447
Iteration 96/1000 | Loss: 0.00001447
Iteration 97/1000 | Loss: 0.00001447
Iteration 98/1000 | Loss: 0.00001447
Iteration 99/1000 | Loss: 0.00001447
Iteration 100/1000 | Loss: 0.00001447
Iteration 101/1000 | Loss: 0.00001447
Iteration 102/1000 | Loss: 0.00001447
Iteration 103/1000 | Loss: 0.00001447
Iteration 104/1000 | Loss: 0.00001447
Iteration 105/1000 | Loss: 0.00001447
Iteration 106/1000 | Loss: 0.00001447
Iteration 107/1000 | Loss: 0.00001447
Iteration 108/1000 | Loss: 0.00001447
Iteration 109/1000 | Loss: 0.00001447
Iteration 110/1000 | Loss: 0.00001447
Iteration 111/1000 | Loss: 0.00001447
Iteration 112/1000 | Loss: 0.00001447
Iteration 113/1000 | Loss: 0.00001447
Iteration 114/1000 | Loss: 0.00001447
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 114. Stopping optimization.
Last 5 losses: [1.4472164366452489e-05, 1.4472164366452489e-05, 1.4472164366452489e-05, 1.4472164366452489e-05, 1.4472164366452489e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4472164366452489e-05

Optimization complete. Final v2v error: 3.2209243774414062 mm

Highest mean error: 3.448202610015869 mm for frame 26

Lowest mean error: 3.010603904724121 mm for frame 115

Saving results

Total time: 32.20534920692444
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_32_us_0430/0023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_us_0430/0023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_us_0430/0023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00668212
Iteration 2/25 | Loss: 0.00097799
Iteration 3/25 | Loss: 0.00088012
Iteration 4/25 | Loss: 0.00086295
Iteration 5/25 | Loss: 0.00085947
Iteration 6/25 | Loss: 0.00085825
Iteration 7/25 | Loss: 0.00085825
Iteration 8/25 | Loss: 0.00085825
Iteration 9/25 | Loss: 0.00085825
Iteration 10/25 | Loss: 0.00085825
Iteration 11/25 | Loss: 0.00085825
Iteration 12/25 | Loss: 0.00085825
Iteration 13/25 | Loss: 0.00085825
Iteration 14/25 | Loss: 0.00085825
Iteration 15/25 | Loss: 0.00085825
Iteration 16/25 | Loss: 0.00085825
Iteration 17/25 | Loss: 0.00085825
Iteration 18/25 | Loss: 0.00085825
Iteration 19/25 | Loss: 0.00085825
Iteration 20/25 | Loss: 0.00085825
Iteration 21/25 | Loss: 0.00085825
Iteration 22/25 | Loss: 0.00085825
Iteration 23/25 | Loss: 0.00085825
Iteration 24/25 | Loss: 0.00085825
Iteration 25/25 | Loss: 0.00085825

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.00608110
Iteration 2/25 | Loss: 0.00059455
Iteration 3/25 | Loss: 0.00059455
Iteration 4/25 | Loss: 0.00059455
Iteration 5/25 | Loss: 0.00059455
Iteration 6/25 | Loss: 0.00059455
Iteration 7/25 | Loss: 0.00059455
Iteration 8/25 | Loss: 0.00059454
Iteration 9/25 | Loss: 0.00059454
Iteration 10/25 | Loss: 0.00059454
Iteration 11/25 | Loss: 0.00059454
Iteration 12/25 | Loss: 0.00059454
Iteration 13/25 | Loss: 0.00059454
Iteration 14/25 | Loss: 0.00059454
Iteration 15/25 | Loss: 0.00059454
Iteration 16/25 | Loss: 0.00059454
Iteration 17/25 | Loss: 0.00059454
Iteration 18/25 | Loss: 0.00059454
Iteration 19/25 | Loss: 0.00059454
Iteration 20/25 | Loss: 0.00059454
Iteration 21/25 | Loss: 0.00059454
Iteration 22/25 | Loss: 0.00059454
Iteration 23/25 | Loss: 0.00059454
Iteration 24/25 | Loss: 0.00059454
Iteration 25/25 | Loss: 0.00059454

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00059454
Iteration 2/1000 | Loss: 0.00004424
Iteration 3/1000 | Loss: 0.00003220
Iteration 4/1000 | Loss: 0.00002957
Iteration 5/1000 | Loss: 0.00002812
Iteration 6/1000 | Loss: 0.00002698
Iteration 7/1000 | Loss: 0.00002633
Iteration 8/1000 | Loss: 0.00002593
Iteration 9/1000 | Loss: 0.00002570
Iteration 10/1000 | Loss: 0.00002564
Iteration 11/1000 | Loss: 0.00002562
Iteration 12/1000 | Loss: 0.00002561
Iteration 13/1000 | Loss: 0.00002561
Iteration 14/1000 | Loss: 0.00002558
Iteration 15/1000 | Loss: 0.00002554
Iteration 16/1000 | Loss: 0.00002553
Iteration 17/1000 | Loss: 0.00002553
Iteration 18/1000 | Loss: 0.00002552
Iteration 19/1000 | Loss: 0.00002552
Iteration 20/1000 | Loss: 0.00002551
Iteration 21/1000 | Loss: 0.00002551
Iteration 22/1000 | Loss: 0.00002549
Iteration 23/1000 | Loss: 0.00002548
Iteration 24/1000 | Loss: 0.00002548
Iteration 25/1000 | Loss: 0.00002548
Iteration 26/1000 | Loss: 0.00002547
Iteration 27/1000 | Loss: 0.00002547
Iteration 28/1000 | Loss: 0.00002546
Iteration 29/1000 | Loss: 0.00002546
Iteration 30/1000 | Loss: 0.00002546
Iteration 31/1000 | Loss: 0.00002545
Iteration 32/1000 | Loss: 0.00002545
Iteration 33/1000 | Loss: 0.00002545
Iteration 34/1000 | Loss: 0.00002545
Iteration 35/1000 | Loss: 0.00002545
Iteration 36/1000 | Loss: 0.00002545
Iteration 37/1000 | Loss: 0.00002544
Iteration 38/1000 | Loss: 0.00002544
Iteration 39/1000 | Loss: 0.00002544
Iteration 40/1000 | Loss: 0.00002544
Iteration 41/1000 | Loss: 0.00002543
Iteration 42/1000 | Loss: 0.00002543
Iteration 43/1000 | Loss: 0.00002543
Iteration 44/1000 | Loss: 0.00002543
Iteration 45/1000 | Loss: 0.00002543
Iteration 46/1000 | Loss: 0.00002543
Iteration 47/1000 | Loss: 0.00002542
Iteration 48/1000 | Loss: 0.00002542
Iteration 49/1000 | Loss: 0.00002541
Iteration 50/1000 | Loss: 0.00002541
Iteration 51/1000 | Loss: 0.00002541
Iteration 52/1000 | Loss: 0.00002541
Iteration 53/1000 | Loss: 0.00002541
Iteration 54/1000 | Loss: 0.00002541
Iteration 55/1000 | Loss: 0.00002541
Iteration 56/1000 | Loss: 0.00002541
Iteration 57/1000 | Loss: 0.00002541
Iteration 58/1000 | Loss: 0.00002541
Iteration 59/1000 | Loss: 0.00002540
Iteration 60/1000 | Loss: 0.00002540
Iteration 61/1000 | Loss: 0.00002540
Iteration 62/1000 | Loss: 0.00002540
Iteration 63/1000 | Loss: 0.00002540
Iteration 64/1000 | Loss: 0.00002540
Iteration 65/1000 | Loss: 0.00002540
Iteration 66/1000 | Loss: 0.00002540
Iteration 67/1000 | Loss: 0.00002540
Iteration 68/1000 | Loss: 0.00002540
Iteration 69/1000 | Loss: 0.00002540
Iteration 70/1000 | Loss: 0.00002540
Iteration 71/1000 | Loss: 0.00002540
Iteration 72/1000 | Loss: 0.00002539
Iteration 73/1000 | Loss: 0.00002539
Iteration 74/1000 | Loss: 0.00002539
Iteration 75/1000 | Loss: 0.00002539
Iteration 76/1000 | Loss: 0.00002539
Iteration 77/1000 | Loss: 0.00002538
Iteration 78/1000 | Loss: 0.00002538
Iteration 79/1000 | Loss: 0.00002538
Iteration 80/1000 | Loss: 0.00002538
Iteration 81/1000 | Loss: 0.00002538
Iteration 82/1000 | Loss: 0.00002538
Iteration 83/1000 | Loss: 0.00002538
Iteration 84/1000 | Loss: 0.00002538
Iteration 85/1000 | Loss: 0.00002538
Iteration 86/1000 | Loss: 0.00002538
Iteration 87/1000 | Loss: 0.00002537
Iteration 88/1000 | Loss: 0.00002537
Iteration 89/1000 | Loss: 0.00002537
Iteration 90/1000 | Loss: 0.00002537
Iteration 91/1000 | Loss: 0.00002537
Iteration 92/1000 | Loss: 0.00002537
Iteration 93/1000 | Loss: 0.00002537
Iteration 94/1000 | Loss: 0.00002537
Iteration 95/1000 | Loss: 0.00002537
Iteration 96/1000 | Loss: 0.00002537
Iteration 97/1000 | Loss: 0.00002537
Iteration 98/1000 | Loss: 0.00002537
Iteration 99/1000 | Loss: 0.00002536
Iteration 100/1000 | Loss: 0.00002536
Iteration 101/1000 | Loss: 0.00002536
Iteration 102/1000 | Loss: 0.00002536
Iteration 103/1000 | Loss: 0.00002536
Iteration 104/1000 | Loss: 0.00002536
Iteration 105/1000 | Loss: 0.00002536
Iteration 106/1000 | Loss: 0.00002536
Iteration 107/1000 | Loss: 0.00002536
Iteration 108/1000 | Loss: 0.00002536
Iteration 109/1000 | Loss: 0.00002535
Iteration 110/1000 | Loss: 0.00002535
Iteration 111/1000 | Loss: 0.00002535
Iteration 112/1000 | Loss: 0.00002535
Iteration 113/1000 | Loss: 0.00002534
Iteration 114/1000 | Loss: 0.00002534
Iteration 115/1000 | Loss: 0.00002534
Iteration 116/1000 | Loss: 0.00002534
Iteration 117/1000 | Loss: 0.00002534
Iteration 118/1000 | Loss: 0.00002534
Iteration 119/1000 | Loss: 0.00002534
Iteration 120/1000 | Loss: 0.00002534
Iteration 121/1000 | Loss: 0.00002534
Iteration 122/1000 | Loss: 0.00002534
Iteration 123/1000 | Loss: 0.00002534
Iteration 124/1000 | Loss: 0.00002533
Iteration 125/1000 | Loss: 0.00002533
Iteration 126/1000 | Loss: 0.00002533
Iteration 127/1000 | Loss: 0.00002533
Iteration 128/1000 | Loss: 0.00002533
Iteration 129/1000 | Loss: 0.00002533
Iteration 130/1000 | Loss: 0.00002533
Iteration 131/1000 | Loss: 0.00002533
Iteration 132/1000 | Loss: 0.00002533
Iteration 133/1000 | Loss: 0.00002533
Iteration 134/1000 | Loss: 0.00002533
Iteration 135/1000 | Loss: 0.00002533
Iteration 136/1000 | Loss: 0.00002533
Iteration 137/1000 | Loss: 0.00002532
Iteration 138/1000 | Loss: 0.00002532
Iteration 139/1000 | Loss: 0.00002532
Iteration 140/1000 | Loss: 0.00002532
Iteration 141/1000 | Loss: 0.00002532
Iteration 142/1000 | Loss: 0.00002532
Iteration 143/1000 | Loss: 0.00002532
Iteration 144/1000 | Loss: 0.00002532
Iteration 145/1000 | Loss: 0.00002532
Iteration 146/1000 | Loss: 0.00002531
Iteration 147/1000 | Loss: 0.00002531
Iteration 148/1000 | Loss: 0.00002531
Iteration 149/1000 | Loss: 0.00002531
Iteration 150/1000 | Loss: 0.00002531
Iteration 151/1000 | Loss: 0.00002531
Iteration 152/1000 | Loss: 0.00002531
Iteration 153/1000 | Loss: 0.00002531
Iteration 154/1000 | Loss: 0.00002531
Iteration 155/1000 | Loss: 0.00002531
Iteration 156/1000 | Loss: 0.00002531
Iteration 157/1000 | Loss: 0.00002531
Iteration 158/1000 | Loss: 0.00002531
Iteration 159/1000 | Loss: 0.00002531
Iteration 160/1000 | Loss: 0.00002531
Iteration 161/1000 | Loss: 0.00002531
Iteration 162/1000 | Loss: 0.00002531
Iteration 163/1000 | Loss: 0.00002531
Iteration 164/1000 | Loss: 0.00002531
Iteration 165/1000 | Loss: 0.00002530
Iteration 166/1000 | Loss: 0.00002530
Iteration 167/1000 | Loss: 0.00002530
Iteration 168/1000 | Loss: 0.00002530
Iteration 169/1000 | Loss: 0.00002530
Iteration 170/1000 | Loss: 0.00002530
Iteration 171/1000 | Loss: 0.00002530
Iteration 172/1000 | Loss: 0.00002530
Iteration 173/1000 | Loss: 0.00002530
Iteration 174/1000 | Loss: 0.00002530
Iteration 175/1000 | Loss: 0.00002530
Iteration 176/1000 | Loss: 0.00002530
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 176. Stopping optimization.
Last 5 losses: [2.5304119844804518e-05, 2.5304119844804518e-05, 2.5304119844804518e-05, 2.5304119844804518e-05, 2.5304119844804518e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.5304119844804518e-05

Optimization complete. Final v2v error: 4.290446758270264 mm

Highest mean error: 4.626441955566406 mm for frame 107

Lowest mean error: 3.8758134841918945 mm for frame 139

Saving results

Total time: 35.63268423080444
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_32_us_0430/0024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_us_0430/0024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_us_0430/0024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01143952
Iteration 2/25 | Loss: 0.01143952
Iteration 3/25 | Loss: 0.01143951
Iteration 4/25 | Loss: 0.01143951
Iteration 5/25 | Loss: 0.00279510
Iteration 6/25 | Loss: 0.00219783
Iteration 7/25 | Loss: 0.00178171
Iteration 8/25 | Loss: 0.00166768
Iteration 9/25 | Loss: 0.00163977
Iteration 10/25 | Loss: 0.00150910
Iteration 11/25 | Loss: 0.00142248
Iteration 12/25 | Loss: 0.00141648
Iteration 13/25 | Loss: 0.00139584
Iteration 14/25 | Loss: 0.00132622
Iteration 15/25 | Loss: 0.00130232
Iteration 16/25 | Loss: 0.00130104
Iteration 17/25 | Loss: 0.00126158
Iteration 18/25 | Loss: 0.00124144
Iteration 19/25 | Loss: 0.00124404
Iteration 20/25 | Loss: 0.00124276
Iteration 21/25 | Loss: 0.00121281
Iteration 22/25 | Loss: 0.00122565
Iteration 23/25 | Loss: 0.00121862
Iteration 24/25 | Loss: 0.00120071
Iteration 25/25 | Loss: 0.00120155

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.50309169
Iteration 2/25 | Loss: 0.00587918
Iteration 3/25 | Loss: 0.00508230
Iteration 4/25 | Loss: 0.00508230
Iteration 5/25 | Loss: 0.00508230
Iteration 6/25 | Loss: 0.00508230
Iteration 7/25 | Loss: 0.00508230
Iteration 8/25 | Loss: 0.00508230
Iteration 9/25 | Loss: 0.00508230
Iteration 10/25 | Loss: 0.00508230
Iteration 11/25 | Loss: 0.00508230
Iteration 12/25 | Loss: 0.00508230
Iteration 13/25 | Loss: 0.00508230
Iteration 14/25 | Loss: 0.00508230
Iteration 15/25 | Loss: 0.00508230
Iteration 16/25 | Loss: 0.00508230
Iteration 17/25 | Loss: 0.00508230
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0050822957418859005, 0.0050822957418859005, 0.0050822957418859005, 0.0050822957418859005, 0.0050822957418859005]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0050822957418859005

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00508230
Iteration 2/1000 | Loss: 0.00718356
Iteration 3/1000 | Loss: 0.00641545
Iteration 4/1000 | Loss: 0.00669928
Iteration 5/1000 | Loss: 0.00775073
Iteration 6/1000 | Loss: 0.00833610
Iteration 7/1000 | Loss: 0.00878620
Iteration 8/1000 | Loss: 0.00837089
Iteration 9/1000 | Loss: 0.00935119
Iteration 10/1000 | Loss: 0.00921967
Iteration 11/1000 | Loss: 0.01218650
Iteration 12/1000 | Loss: 0.01543422
Iteration 13/1000 | Loss: 0.01191799
Iteration 14/1000 | Loss: 0.01524672
Iteration 15/1000 | Loss: 0.00968902
Iteration 16/1000 | Loss: 0.00882860
Iteration 17/1000 | Loss: 0.00975858
Iteration 18/1000 | Loss: 0.00888259
Iteration 19/1000 | Loss: 0.00891873
Iteration 20/1000 | Loss: 0.00794626
Iteration 21/1000 | Loss: 0.00593246
Iteration 22/1000 | Loss: 0.00665214
Iteration 23/1000 | Loss: 0.00630822
Iteration 24/1000 | Loss: 0.00563204
Iteration 25/1000 | Loss: 0.00746702
Iteration 26/1000 | Loss: 0.00774451
Iteration 27/1000 | Loss: 0.00634965
Iteration 28/1000 | Loss: 0.00638038
Iteration 29/1000 | Loss: 0.00664822
Iteration 30/1000 | Loss: 0.00947461
Iteration 31/1000 | Loss: 0.00727901
Iteration 32/1000 | Loss: 0.00612504
Iteration 33/1000 | Loss: 0.00583842
Iteration 34/1000 | Loss: 0.00658560
Iteration 35/1000 | Loss: 0.00702928
Iteration 36/1000 | Loss: 0.00658845
Iteration 37/1000 | Loss: 0.00884669
Iteration 38/1000 | Loss: 0.00732522
Iteration 39/1000 | Loss: 0.00785137
Iteration 40/1000 | Loss: 0.00744741
Iteration 41/1000 | Loss: 0.00777987
Iteration 42/1000 | Loss: 0.00802983
Iteration 43/1000 | Loss: 0.01146432
Iteration 44/1000 | Loss: 0.00726012
Iteration 45/1000 | Loss: 0.00801095
Iteration 46/1000 | Loss: 0.00956509
Iteration 47/1000 | Loss: 0.00985946
Iteration 48/1000 | Loss: 0.00834772
Iteration 49/1000 | Loss: 0.00855033
Iteration 50/1000 | Loss: 0.00842844
Iteration 51/1000 | Loss: 0.00714217
Iteration 52/1000 | Loss: 0.00784566
Iteration 53/1000 | Loss: 0.00714404
Iteration 54/1000 | Loss: 0.00775404
Iteration 55/1000 | Loss: 0.00882688
Iteration 56/1000 | Loss: 0.00745372
Iteration 57/1000 | Loss: 0.00690727
Iteration 58/1000 | Loss: 0.00767421
Iteration 59/1000 | Loss: 0.00768568
Iteration 60/1000 | Loss: 0.00748018
Iteration 61/1000 | Loss: 0.00997032
Iteration 62/1000 | Loss: 0.00783274
Iteration 63/1000 | Loss: 0.00697281
Iteration 64/1000 | Loss: 0.00947387
Iteration 65/1000 | Loss: 0.01307065
Iteration 66/1000 | Loss: 0.01414802
Iteration 67/1000 | Loss: 0.01162078
Iteration 68/1000 | Loss: 0.01176788
Iteration 69/1000 | Loss: 0.00777790
Iteration 70/1000 | Loss: 0.00821209
Iteration 71/1000 | Loss: 0.00741400
Iteration 72/1000 | Loss: 0.00751223
Iteration 73/1000 | Loss: 0.00747084
Iteration 74/1000 | Loss: 0.00711863
Iteration 75/1000 | Loss: 0.00746050
Iteration 76/1000 | Loss: 0.00942746
Iteration 77/1000 | Loss: 0.00808864
Iteration 78/1000 | Loss: 0.00809773
Iteration 79/1000 | Loss: 0.00777467
Iteration 80/1000 | Loss: 0.00880812
Iteration 81/1000 | Loss: 0.01045900
Iteration 82/1000 | Loss: 0.00919788
Iteration 83/1000 | Loss: 0.00930938
Iteration 84/1000 | Loss: 0.00927296
Iteration 85/1000 | Loss: 0.00801651
Iteration 86/1000 | Loss: 0.00721816
Iteration 87/1000 | Loss: 0.00876187
Iteration 88/1000 | Loss: 0.00794396
Iteration 89/1000 | Loss: 0.00812381
Iteration 90/1000 | Loss: 0.00680418
Iteration 91/1000 | Loss: 0.00999984
Iteration 92/1000 | Loss: 0.01032951
Iteration 93/1000 | Loss: 0.00776867
Iteration 94/1000 | Loss: 0.00756544
Iteration 95/1000 | Loss: 0.00694312
Iteration 96/1000 | Loss: 0.00713485
Iteration 97/1000 | Loss: 0.00696317
Iteration 98/1000 | Loss: 0.00346263
Iteration 99/1000 | Loss: 0.00666742
Iteration 100/1000 | Loss: 0.00918433
Iteration 101/1000 | Loss: 0.00461194
Iteration 102/1000 | Loss: 0.00419835
Iteration 103/1000 | Loss: 0.00514624
Iteration 104/1000 | Loss: 0.00617075
Iteration 105/1000 | Loss: 0.00463977
Iteration 106/1000 | Loss: 0.00487741
Iteration 107/1000 | Loss: 0.00432097
Iteration 108/1000 | Loss: 0.00597481
Iteration 109/1000 | Loss: 0.00346915
Iteration 110/1000 | Loss: 0.00349672
Iteration 111/1000 | Loss: 0.00434258
Iteration 112/1000 | Loss: 0.00275319
Iteration 113/1000 | Loss: 0.00464607
Iteration 114/1000 | Loss: 0.00386779
Iteration 115/1000 | Loss: 0.00327846
Iteration 116/1000 | Loss: 0.00326408
Iteration 117/1000 | Loss: 0.00359318
Iteration 118/1000 | Loss: 0.00361071
Iteration 119/1000 | Loss: 0.00355582
Iteration 120/1000 | Loss: 0.00365261
Iteration 121/1000 | Loss: 0.00473094
Iteration 122/1000 | Loss: 0.00360206
Iteration 123/1000 | Loss: 0.00338262
Iteration 124/1000 | Loss: 0.00359928
Iteration 125/1000 | Loss: 0.00347376
Iteration 126/1000 | Loss: 0.00343720
Iteration 127/1000 | Loss: 0.00269959
Iteration 128/1000 | Loss: 0.00321690
Iteration 129/1000 | Loss: 0.00347063
Iteration 130/1000 | Loss: 0.00321675
Iteration 131/1000 | Loss: 0.00325550
Iteration 132/1000 | Loss: 0.00323168
Iteration 133/1000 | Loss: 0.00326011
Iteration 134/1000 | Loss: 0.00328625
Iteration 135/1000 | Loss: 0.00455918
Iteration 136/1000 | Loss: 0.00328761
Iteration 137/1000 | Loss: 0.00327404
Iteration 138/1000 | Loss: 0.00316940
Iteration 139/1000 | Loss: 0.00307269
Iteration 140/1000 | Loss: 0.00291738
Iteration 141/1000 | Loss: 0.00363852
Iteration 142/1000 | Loss: 0.00373985
Iteration 143/1000 | Loss: 0.00304678
Iteration 144/1000 | Loss: 0.00292077
Iteration 145/1000 | Loss: 0.00343227
Iteration 146/1000 | Loss: 0.00371381
Iteration 147/1000 | Loss: 0.00377104
Iteration 148/1000 | Loss: 0.00293265
Iteration 149/1000 | Loss: 0.00386409
Iteration 150/1000 | Loss: 0.00288544
Iteration 151/1000 | Loss: 0.00307620
Iteration 152/1000 | Loss: 0.00285074
Iteration 153/1000 | Loss: 0.00284743
Iteration 154/1000 | Loss: 0.00253134
Iteration 155/1000 | Loss: 0.00232838
Iteration 156/1000 | Loss: 0.00273500
Iteration 157/1000 | Loss: 0.00311772
Iteration 158/1000 | Loss: 0.00282480
Iteration 159/1000 | Loss: 0.00283284
Iteration 160/1000 | Loss: 0.00276573
Iteration 161/1000 | Loss: 0.00295592
Iteration 162/1000 | Loss: 0.00274335
Iteration 163/1000 | Loss: 0.00387172
Iteration 164/1000 | Loss: 0.00327023
Iteration 165/1000 | Loss: 0.00323040
Iteration 166/1000 | Loss: 0.00240836
Iteration 167/1000 | Loss: 0.00305208
Iteration 168/1000 | Loss: 0.00205342
Iteration 169/1000 | Loss: 0.00206884
Iteration 170/1000 | Loss: 0.00242325
Iteration 171/1000 | Loss: 0.00261259
Iteration 172/1000 | Loss: 0.00249406
Iteration 173/1000 | Loss: 0.00261184
Iteration 174/1000 | Loss: 0.00266460
Iteration 175/1000 | Loss: 0.00307072
Iteration 176/1000 | Loss: 0.00278279
Iteration 177/1000 | Loss: 0.00277877
Iteration 178/1000 | Loss: 0.00274388
Iteration 179/1000 | Loss: 0.00364573
Iteration 180/1000 | Loss: 0.00310305
Iteration 181/1000 | Loss: 0.00264703
Iteration 182/1000 | Loss: 0.00329982
Iteration 183/1000 | Loss: 0.00361442
Iteration 184/1000 | Loss: 0.00273162
Iteration 185/1000 | Loss: 0.00235240
Iteration 186/1000 | Loss: 0.00228266
Iteration 187/1000 | Loss: 0.00269566
Iteration 188/1000 | Loss: 0.00266925
Iteration 189/1000 | Loss: 0.00284488
Iteration 190/1000 | Loss: 0.00251287
Iteration 191/1000 | Loss: 0.00321300
Iteration 192/1000 | Loss: 0.00371308
Iteration 193/1000 | Loss: 0.00272052
Iteration 194/1000 | Loss: 0.00259384
Iteration 195/1000 | Loss: 0.00284863
Iteration 196/1000 | Loss: 0.00257569
Iteration 197/1000 | Loss: 0.00451005
Iteration 198/1000 | Loss: 0.00288330
Iteration 199/1000 | Loss: 0.00287514
Iteration 200/1000 | Loss: 0.00278797
Iteration 201/1000 | Loss: 0.00276173
Iteration 202/1000 | Loss: 0.00326259
Iteration 203/1000 | Loss: 0.00279990
Iteration 204/1000 | Loss: 0.00285187
Iteration 205/1000 | Loss: 0.00284945
Iteration 206/1000 | Loss: 0.00278603
Iteration 207/1000 | Loss: 0.00256300
Iteration 208/1000 | Loss: 0.00379926
Iteration 209/1000 | Loss: 0.00270704
Iteration 210/1000 | Loss: 0.00304017
Iteration 211/1000 | Loss: 0.00393535
Iteration 212/1000 | Loss: 0.00255611
Iteration 213/1000 | Loss: 0.00248731
Iteration 214/1000 | Loss: 0.00261676
Iteration 215/1000 | Loss: 0.00288711
Iteration 216/1000 | Loss: 0.00285901
Iteration 217/1000 | Loss: 0.00363796
Iteration 218/1000 | Loss: 0.00265515
Iteration 219/1000 | Loss: 0.00294153
Iteration 220/1000 | Loss: 0.00243619
Iteration 221/1000 | Loss: 0.00293886
Iteration 222/1000 | Loss: 0.00306781
Iteration 223/1000 | Loss: 0.00330896
Iteration 224/1000 | Loss: 0.00283176
Iteration 225/1000 | Loss: 0.00420670
Iteration 226/1000 | Loss: 0.00350429
Iteration 227/1000 | Loss: 0.00364224
Iteration 228/1000 | Loss: 0.00354197
Iteration 229/1000 | Loss: 0.00386944
Iteration 230/1000 | Loss: 0.00372768
Iteration 231/1000 | Loss: 0.00254632
Iteration 232/1000 | Loss: 0.00406682
Iteration 233/1000 | Loss: 0.00332559
Iteration 234/1000 | Loss: 0.00255034
Iteration 235/1000 | Loss: 0.00264439
Iteration 236/1000 | Loss: 0.00220019
Iteration 237/1000 | Loss: 0.00296978
Iteration 238/1000 | Loss: 0.00361186
Iteration 239/1000 | Loss: 0.00218186
Iteration 240/1000 | Loss: 0.00216279
Iteration 241/1000 | Loss: 0.00259261
Iteration 242/1000 | Loss: 0.00237887
Iteration 243/1000 | Loss: 0.00297950
Iteration 244/1000 | Loss: 0.00229038
Iteration 245/1000 | Loss: 0.00345594
Iteration 246/1000 | Loss: 0.00290860
Iteration 247/1000 | Loss: 0.00168577
Iteration 248/1000 | Loss: 0.00217564
Iteration 249/1000 | Loss: 0.00194638
Iteration 250/1000 | Loss: 0.00250589
Iteration 251/1000 | Loss: 0.00222450
Iteration 252/1000 | Loss: 0.00229037
Iteration 253/1000 | Loss: 0.00315056
Iteration 254/1000 | Loss: 0.00271940
Iteration 255/1000 | Loss: 0.00240641
Iteration 256/1000 | Loss: 0.00149924
Iteration 257/1000 | Loss: 0.00188198
Iteration 258/1000 | Loss: 0.00245085
Iteration 259/1000 | Loss: 0.00257263
Iteration 260/1000 | Loss: 0.00260243
Iteration 261/1000 | Loss: 0.00238025
Iteration 262/1000 | Loss: 0.00233804
Iteration 263/1000 | Loss: 0.00259376
Iteration 264/1000 | Loss: 0.00191527
Iteration 265/1000 | Loss: 0.00230966
Iteration 266/1000 | Loss: 0.00235095
Iteration 267/1000 | Loss: 0.00304895
Iteration 268/1000 | Loss: 0.00200497
Iteration 269/1000 | Loss: 0.00221921
Iteration 270/1000 | Loss: 0.00303977
Iteration 271/1000 | Loss: 0.00214658
Iteration 272/1000 | Loss: 0.00207428
Iteration 273/1000 | Loss: 0.00226226
Iteration 274/1000 | Loss: 0.00191098
Iteration 275/1000 | Loss: 0.00330483
Iteration 276/1000 | Loss: 0.00298509
Iteration 277/1000 | Loss: 0.00238658
Iteration 278/1000 | Loss: 0.00195511
Iteration 279/1000 | Loss: 0.00215088
Iteration 280/1000 | Loss: 0.00217474
Iteration 281/1000 | Loss: 0.00228668
Iteration 282/1000 | Loss: 0.00230927
Iteration 283/1000 | Loss: 0.00222590
Iteration 284/1000 | Loss: 0.00206867
Iteration 285/1000 | Loss: 0.00206602
Iteration 286/1000 | Loss: 0.00204365
Iteration 287/1000 | Loss: 0.00222430
Iteration 288/1000 | Loss: 0.00271139
Iteration 289/1000 | Loss: 0.00222766
Iteration 290/1000 | Loss: 0.00289943
Iteration 291/1000 | Loss: 0.00216135
Iteration 292/1000 | Loss: 0.00201397
Iteration 293/1000 | Loss: 0.00240515
Iteration 294/1000 | Loss: 0.00228479
Iteration 295/1000 | Loss: 0.00243348
Iteration 296/1000 | Loss: 0.00169614
Iteration 297/1000 | Loss: 0.00227013
Iteration 298/1000 | Loss: 0.00384216
Iteration 299/1000 | Loss: 0.00238694
Iteration 300/1000 | Loss: 0.00181882
Iteration 301/1000 | Loss: 0.00289352
Iteration 302/1000 | Loss: 0.00220292
Iteration 303/1000 | Loss: 0.00193154
Iteration 304/1000 | Loss: 0.00201410
Iteration 305/1000 | Loss: 0.00152451
Iteration 306/1000 | Loss: 0.00192494
Iteration 307/1000 | Loss: 0.00193817
Iteration 308/1000 | Loss: 0.00228358
Iteration 309/1000 | Loss: 0.00223798
Iteration 310/1000 | Loss: 0.00202757
Iteration 311/1000 | Loss: 0.00200507
Iteration 312/1000 | Loss: 0.00224698
Iteration 313/1000 | Loss: 0.00215050
Iteration 314/1000 | Loss: 0.00258864
Iteration 315/1000 | Loss: 0.00242014
Iteration 316/1000 | Loss: 0.00231633
Iteration 317/1000 | Loss: 0.00203363
Iteration 318/1000 | Loss: 0.00355706
Iteration 319/1000 | Loss: 0.00293023
Iteration 320/1000 | Loss: 0.00270975
Iteration 321/1000 | Loss: 0.00244904
Iteration 322/1000 | Loss: 0.00271727
Iteration 323/1000 | Loss: 0.00179895
Iteration 324/1000 | Loss: 0.00221974
Iteration 325/1000 | Loss: 0.00223883
Iteration 326/1000 | Loss: 0.00186332
Iteration 327/1000 | Loss: 0.00188046
Iteration 328/1000 | Loss: 0.00201970
Iteration 329/1000 | Loss: 0.00214741
Iteration 330/1000 | Loss: 0.00189448
Iteration 331/1000 | Loss: 0.00221926
Iteration 332/1000 | Loss: 0.00185235
Iteration 333/1000 | Loss: 0.00210708
Iteration 334/1000 | Loss: 0.00188067
Iteration 335/1000 | Loss: 0.00238571
Iteration 336/1000 | Loss: 0.00169205
Iteration 337/1000 | Loss: 0.00400507
Iteration 338/1000 | Loss: 0.00202268
Iteration 339/1000 | Loss: 0.00236392
Iteration 340/1000 | Loss: 0.00248992
Iteration 341/1000 | Loss: 0.00252065
Iteration 342/1000 | Loss: 0.00244931
Iteration 343/1000 | Loss: 0.00212909
Iteration 344/1000 | Loss: 0.00185019
Iteration 345/1000 | Loss: 0.00160436
Iteration 346/1000 | Loss: 0.00177605
Iteration 347/1000 | Loss: 0.00226885
Iteration 348/1000 | Loss: 0.00220915
Iteration 349/1000 | Loss: 0.00222342
Iteration 350/1000 | Loss: 0.00225726
Iteration 351/1000 | Loss: 0.00192871
Iteration 352/1000 | Loss: 0.00185908
Iteration 353/1000 | Loss: 0.00209984
Iteration 354/1000 | Loss: 0.00304933
Iteration 355/1000 | Loss: 0.00396766
Iteration 356/1000 | Loss: 0.00240430
Iteration 357/1000 | Loss: 0.00132481
Iteration 358/1000 | Loss: 0.00113533
Iteration 359/1000 | Loss: 0.00156512
Iteration 360/1000 | Loss: 0.00266194
Iteration 361/1000 | Loss: 0.00198608
Iteration 362/1000 | Loss: 0.00162056
Iteration 363/1000 | Loss: 0.00163663
Iteration 364/1000 | Loss: 0.00175504
Iteration 365/1000 | Loss: 0.00186206
Iteration 366/1000 | Loss: 0.00202199
Iteration 367/1000 | Loss: 0.00180316
Iteration 368/1000 | Loss: 0.00183972
Iteration 369/1000 | Loss: 0.00170329
Iteration 370/1000 | Loss: 0.00217265
Iteration 371/1000 | Loss: 0.00229274
Iteration 372/1000 | Loss: 0.00297604
Iteration 373/1000 | Loss: 0.00149712
Iteration 374/1000 | Loss: 0.00098465
Iteration 375/1000 | Loss: 0.00113101
Iteration 376/1000 | Loss: 0.00088498
Iteration 377/1000 | Loss: 0.00107914
Iteration 378/1000 | Loss: 0.00115826
Iteration 379/1000 | Loss: 0.00118001
Iteration 380/1000 | Loss: 0.00149013
Iteration 381/1000 | Loss: 0.00134848
Iteration 382/1000 | Loss: 0.00112233
Iteration 383/1000 | Loss: 0.00148438
Iteration 384/1000 | Loss: 0.00149714
Iteration 385/1000 | Loss: 0.00133744
Iteration 386/1000 | Loss: 0.00148056
Iteration 387/1000 | Loss: 0.00146798
Iteration 388/1000 | Loss: 0.00140440
Iteration 389/1000 | Loss: 0.00115692
Iteration 390/1000 | Loss: 0.00113118
Iteration 391/1000 | Loss: 0.00116650
Iteration 392/1000 | Loss: 0.00107693
Iteration 393/1000 | Loss: 0.00106782
Iteration 394/1000 | Loss: 0.00120377
Iteration 395/1000 | Loss: 0.00104006
Iteration 396/1000 | Loss: 0.00096763
Iteration 397/1000 | Loss: 0.00118999
Iteration 398/1000 | Loss: 0.00122970
Iteration 399/1000 | Loss: 0.00127546
Iteration 400/1000 | Loss: 0.00120971
Iteration 401/1000 | Loss: 0.00127530
Iteration 402/1000 | Loss: 0.00123830
Iteration 403/1000 | Loss: 0.00159769
Iteration 404/1000 | Loss: 0.00119009
Iteration 405/1000 | Loss: 0.00150504
Iteration 406/1000 | Loss: 0.00115553
Iteration 407/1000 | Loss: 0.00143947
Iteration 408/1000 | Loss: 0.00122766
Iteration 409/1000 | Loss: 0.00134473
Iteration 410/1000 | Loss: 0.00114287
Iteration 411/1000 | Loss: 0.00119815
Iteration 412/1000 | Loss: 0.00121504
Iteration 413/1000 | Loss: 0.00117112
Iteration 414/1000 | Loss: 0.00119722
Iteration 415/1000 | Loss: 0.00170886
Iteration 416/1000 | Loss: 0.00168449
Iteration 417/1000 | Loss: 0.00115736
Iteration 418/1000 | Loss: 0.00095022
Iteration 419/1000 | Loss: 0.00111157
Iteration 420/1000 | Loss: 0.00176178
Iteration 421/1000 | Loss: 0.00197119
Iteration 422/1000 | Loss: 0.00173892
Iteration 423/1000 | Loss: 0.00218883
Iteration 424/1000 | Loss: 0.00243550
Iteration 425/1000 | Loss: 0.00144357
Iteration 426/1000 | Loss: 0.00119595
Iteration 427/1000 | Loss: 0.00104063
Iteration 428/1000 | Loss: 0.00114758
Iteration 429/1000 | Loss: 0.00126454
Iteration 430/1000 | Loss: 0.00155313
Iteration 431/1000 | Loss: 0.00094514
Iteration 432/1000 | Loss: 0.00093640
Iteration 433/1000 | Loss: 0.00106843
Iteration 434/1000 | Loss: 0.00103761
Iteration 435/1000 | Loss: 0.00128052
Iteration 436/1000 | Loss: 0.00092543
Iteration 437/1000 | Loss: 0.00091718
Iteration 438/1000 | Loss: 0.00161185
Iteration 439/1000 | Loss: 0.00076677
Iteration 440/1000 | Loss: 0.00100941
Iteration 441/1000 | Loss: 0.00098667
Iteration 442/1000 | Loss: 0.00110088
Iteration 443/1000 | Loss: 0.00104641
Iteration 444/1000 | Loss: 0.00121188
Iteration 445/1000 | Loss: 0.00089875
Iteration 446/1000 | Loss: 0.00104017
Iteration 447/1000 | Loss: 0.00094414
Iteration 448/1000 | Loss: 0.00123302
Iteration 449/1000 | Loss: 0.00106855
Iteration 450/1000 | Loss: 0.00121142
Iteration 451/1000 | Loss: 0.00107614
Iteration 452/1000 | Loss: 0.00108842
Iteration 453/1000 | Loss: 0.00126851
Iteration 454/1000 | Loss: 0.00127089
Iteration 455/1000 | Loss: 0.00098957
Iteration 456/1000 | Loss: 0.00095162
Iteration 457/1000 | Loss: 0.00111439
Iteration 458/1000 | Loss: 0.00120259
Iteration 459/1000 | Loss: 0.00143689
Iteration 460/1000 | Loss: 0.00070582
Iteration 461/1000 | Loss: 0.00118575
Iteration 462/1000 | Loss: 0.00084599
Iteration 463/1000 | Loss: 0.00037014
Iteration 464/1000 | Loss: 0.00046887
Iteration 465/1000 | Loss: 0.00075104
Iteration 466/1000 | Loss: 0.00068949
Iteration 467/1000 | Loss: 0.00076227
Iteration 468/1000 | Loss: 0.00071902
Iteration 469/1000 | Loss: 0.00086920
Iteration 470/1000 | Loss: 0.00077690
Iteration 471/1000 | Loss: 0.00062682
Iteration 472/1000 | Loss: 0.00086314
Iteration 473/1000 | Loss: 0.00102640
Iteration 474/1000 | Loss: 0.00059993
Iteration 475/1000 | Loss: 0.00056851
Iteration 476/1000 | Loss: 0.00069442
Iteration 477/1000 | Loss: 0.00075458
Iteration 478/1000 | Loss: 0.00079300
Iteration 479/1000 | Loss: 0.00081207
Iteration 480/1000 | Loss: 0.00053310
Iteration 481/1000 | Loss: 0.00074674
Iteration 482/1000 | Loss: 0.00070355
Iteration 483/1000 | Loss: 0.00076130
Iteration 484/1000 | Loss: 0.00062340
Iteration 485/1000 | Loss: 0.00079150
Iteration 486/1000 | Loss: 0.00054442
Iteration 487/1000 | Loss: 0.00078358
Iteration 488/1000 | Loss: 0.00082380
Iteration 489/1000 | Loss: 0.00079202
Iteration 490/1000 | Loss: 0.00096285
Iteration 491/1000 | Loss: 0.00088768
Iteration 492/1000 | Loss: 0.00095224
Iteration 493/1000 | Loss: 0.00083952
Iteration 494/1000 | Loss: 0.00099691
Iteration 495/1000 | Loss: 0.00079912
Iteration 496/1000 | Loss: 0.00063725
Iteration 497/1000 | Loss: 0.00063426
Iteration 498/1000 | Loss: 0.00073140
Iteration 499/1000 | Loss: 0.00070453
Iteration 500/1000 | Loss: 0.00051281
Iteration 501/1000 | Loss: 0.00101978
Iteration 502/1000 | Loss: 0.00087724
Iteration 503/1000 | Loss: 0.00081739
Iteration 504/1000 | Loss: 0.00074970
Iteration 505/1000 | Loss: 0.00083790
Iteration 506/1000 | Loss: 0.00204532
Iteration 507/1000 | Loss: 0.00114159
Iteration 508/1000 | Loss: 0.00082291
Iteration 509/1000 | Loss: 0.00107958
Iteration 510/1000 | Loss: 0.00150767
Iteration 511/1000 | Loss: 0.00110509
Iteration 512/1000 | Loss: 0.00106859
Iteration 513/1000 | Loss: 0.00099706
Iteration 514/1000 | Loss: 0.00117362
Iteration 515/1000 | Loss: 0.00124949
Iteration 516/1000 | Loss: 0.00179927
Iteration 517/1000 | Loss: 0.00081836
Iteration 518/1000 | Loss: 0.00082174
Iteration 519/1000 | Loss: 0.00094710
Iteration 520/1000 | Loss: 0.00085488
Iteration 521/1000 | Loss: 0.00068424
Iteration 522/1000 | Loss: 0.00078541
Iteration 523/1000 | Loss: 0.00098723
Iteration 524/1000 | Loss: 0.00065746
Iteration 525/1000 | Loss: 0.00052613
Iteration 526/1000 | Loss: 0.00053114
Iteration 527/1000 | Loss: 0.00054432
Iteration 528/1000 | Loss: 0.00084790
Iteration 529/1000 | Loss: 0.00069879
Iteration 530/1000 | Loss: 0.00072900
Iteration 531/1000 | Loss: 0.00068760
Iteration 532/1000 | Loss: 0.00068780
Iteration 533/1000 | Loss: 0.00070763
Iteration 534/1000 | Loss: 0.00069507
Iteration 535/1000 | Loss: 0.00091230
Iteration 536/1000 | Loss: 0.00068968
Iteration 537/1000 | Loss: 0.00072457
Iteration 538/1000 | Loss: 0.00064454
Iteration 539/1000 | Loss: 0.00060521
Iteration 540/1000 | Loss: 0.00061000
Iteration 541/1000 | Loss: 0.00064564
Iteration 542/1000 | Loss: 0.00060503
Iteration 543/1000 | Loss: 0.00066072
Iteration 544/1000 | Loss: 0.00058887
Iteration 545/1000 | Loss: 0.00052534
Iteration 546/1000 | Loss: 0.00045777
Iteration 547/1000 | Loss: 0.00069825
Iteration 548/1000 | Loss: 0.00080890
Iteration 549/1000 | Loss: 0.00092861
Iteration 550/1000 | Loss: 0.00074977
Iteration 551/1000 | Loss: 0.00064059
Iteration 552/1000 | Loss: 0.00068600
Iteration 553/1000 | Loss: 0.00078054
Iteration 554/1000 | Loss: 0.00071898
Iteration 555/1000 | Loss: 0.00064919
Iteration 556/1000 | Loss: 0.00061765
Iteration 557/1000 | Loss: 0.00066121
Iteration 558/1000 | Loss: 0.00063416
Iteration 559/1000 | Loss: 0.00068191
Iteration 560/1000 | Loss: 0.00055264
Iteration 561/1000 | Loss: 0.00064206
Iteration 562/1000 | Loss: 0.00067477
Iteration 563/1000 | Loss: 0.00076316
Iteration 564/1000 | Loss: 0.00075603
Iteration 565/1000 | Loss: 0.00047655
Iteration 566/1000 | Loss: 0.00048000
Iteration 567/1000 | Loss: 0.00066681
Iteration 568/1000 | Loss: 0.00064145
Iteration 569/1000 | Loss: 0.00064885
Iteration 570/1000 | Loss: 0.00063397
Iteration 571/1000 | Loss: 0.00061084
Iteration 572/1000 | Loss: 0.00038973
Iteration 573/1000 | Loss: 0.00057879
Iteration 574/1000 | Loss: 0.00065359
Iteration 575/1000 | Loss: 0.00059080
Iteration 576/1000 | Loss: 0.00059931
Iteration 577/1000 | Loss: 0.00067348
Iteration 578/1000 | Loss: 0.00061120
Iteration 579/1000 | Loss: 0.00078182
Iteration 580/1000 | Loss: 0.00089180
Iteration 581/1000 | Loss: 0.00069350
Iteration 582/1000 | Loss: 0.00061230
Iteration 583/1000 | Loss: 0.00115154
Iteration 584/1000 | Loss: 0.00057436
Iteration 585/1000 | Loss: 0.00056555
Iteration 586/1000 | Loss: 0.00056497
Iteration 587/1000 | Loss: 0.00062540
Iteration 588/1000 | Loss: 0.00067398
Iteration 589/1000 | Loss: 0.00070304
Iteration 590/1000 | Loss: 0.00065443
Iteration 591/1000 | Loss: 0.00069955
Iteration 592/1000 | Loss: 0.00073672
Iteration 593/1000 | Loss: 0.00085447
Iteration 594/1000 | Loss: 0.00067923
Iteration 595/1000 | Loss: 0.00066688
Iteration 596/1000 | Loss: 0.00067772
Iteration 597/1000 | Loss: 0.00110456
Iteration 598/1000 | Loss: 0.00073239
Iteration 599/1000 | Loss: 0.00059664
Iteration 600/1000 | Loss: 0.00053817
Iteration 601/1000 | Loss: 0.00065421
Iteration 602/1000 | Loss: 0.00060778
Iteration 603/1000 | Loss: 0.00056902
Iteration 604/1000 | Loss: 0.00060214
Iteration 605/1000 | Loss: 0.00055514
Iteration 606/1000 | Loss: 0.00055459
Iteration 607/1000 | Loss: 0.00069533
Iteration 608/1000 | Loss: 0.00064801
Iteration 609/1000 | Loss: 0.00061517
Iteration 610/1000 | Loss: 0.00058681
Iteration 611/1000 | Loss: 0.00059273
Iteration 612/1000 | Loss: 0.00056510
Iteration 613/1000 | Loss: 0.00044525
Iteration 614/1000 | Loss: 0.00050362
Iteration 615/1000 | Loss: 0.00065503
Iteration 616/1000 | Loss: 0.00049604
Iteration 617/1000 | Loss: 0.00056146
Iteration 618/1000 | Loss: 0.00046181
Iteration 619/1000 | Loss: 0.00042757
Iteration 620/1000 | Loss: 0.00062177
Iteration 621/1000 | Loss: 0.00055123
Iteration 622/1000 | Loss: 0.00100518
Iteration 623/1000 | Loss: 0.00054806
Iteration 624/1000 | Loss: 0.00059445
Iteration 625/1000 | Loss: 0.00061258
Iteration 626/1000 | Loss: 0.00061957
Iteration 627/1000 | Loss: 0.00060611
Iteration 628/1000 | Loss: 0.00076465
Iteration 629/1000 | Loss: 0.00064831
Iteration 630/1000 | Loss: 0.00063777
Iteration 631/1000 | Loss: 0.00064098
Iteration 632/1000 | Loss: 0.00069779
Iteration 633/1000 | Loss: 0.00051007
Iteration 634/1000 | Loss: 0.00066033
Iteration 635/1000 | Loss: 0.00059246
Iteration 636/1000 | Loss: 0.00051904
Iteration 637/1000 | Loss: 0.00056257
Iteration 638/1000 | Loss: 0.00068211
Iteration 639/1000 | Loss: 0.00056157
Iteration 640/1000 | Loss: 0.00067424
Iteration 641/1000 | Loss: 0.00061380
Iteration 642/1000 | Loss: 0.00056383
Iteration 643/1000 | Loss: 0.00060179
Iteration 644/1000 | Loss: 0.00059386
Iteration 645/1000 | Loss: 0.00089685
Iteration 646/1000 | Loss: 0.00163016
Iteration 647/1000 | Loss: 0.00156925
Iteration 648/1000 | Loss: 0.00460050
Iteration 649/1000 | Loss: 0.00238889
Iteration 650/1000 | Loss: 0.00131513
Iteration 651/1000 | Loss: 0.00080767
Iteration 652/1000 | Loss: 0.00067763
Iteration 653/1000 | Loss: 0.00061146
Iteration 654/1000 | Loss: 0.00054619
Iteration 655/1000 | Loss: 0.00037189
Iteration 656/1000 | Loss: 0.00059510
Iteration 657/1000 | Loss: 0.00063079
Iteration 658/1000 | Loss: 0.00058712
Iteration 659/1000 | Loss: 0.00060058
Iteration 660/1000 | Loss: 0.00052078
Iteration 661/1000 | Loss: 0.00063374
Iteration 662/1000 | Loss: 0.00075271
Iteration 663/1000 | Loss: 0.00062425
Iteration 664/1000 | Loss: 0.00060940
Iteration 665/1000 | Loss: 0.00068258
Iteration 666/1000 | Loss: 0.00061574
Iteration 667/1000 | Loss: 0.00065306
Iteration 668/1000 | Loss: 0.00067196
Iteration 669/1000 | Loss: 0.00059979
Iteration 670/1000 | Loss: 0.00080868
Iteration 671/1000 | Loss: 0.00062156
Iteration 672/1000 | Loss: 0.00052739
Iteration 673/1000 | Loss: 0.00053583
Iteration 674/1000 | Loss: 0.00058993
Iteration 675/1000 | Loss: 0.00060454
Iteration 676/1000 | Loss: 0.00060506
Iteration 677/1000 | Loss: 0.00077298
Iteration 678/1000 | Loss: 0.00046854
Iteration 679/1000 | Loss: 0.00100461
Iteration 680/1000 | Loss: 0.00064014
Iteration 681/1000 | Loss: 0.00083293
Iteration 682/1000 | Loss: 0.00063610
Iteration 683/1000 | Loss: 0.00072919
Iteration 684/1000 | Loss: 0.00064627
Iteration 685/1000 | Loss: 0.00070558
Iteration 686/1000 | Loss: 0.00065993
Iteration 687/1000 | Loss: 0.00069044
Iteration 688/1000 | Loss: 0.00060126
Iteration 689/1000 | Loss: 0.00044227
Iteration 690/1000 | Loss: 0.00054016
Iteration 691/1000 | Loss: 0.00067105
Iteration 692/1000 | Loss: 0.00055062
Iteration 693/1000 | Loss: 0.00067052
Iteration 694/1000 | Loss: 0.00061632
Iteration 695/1000 | Loss: 0.00095717
Iteration 696/1000 | Loss: 0.00091731
Iteration 697/1000 | Loss: 0.00053375
Iteration 698/1000 | Loss: 0.00106202
Iteration 699/1000 | Loss: 0.00079201
Iteration 700/1000 | Loss: 0.00051110
Iteration 701/1000 | Loss: 0.00081081
Iteration 702/1000 | Loss: 0.00083189
Iteration 703/1000 | Loss: 0.00062920
Iteration 704/1000 | Loss: 0.00071085
Iteration 705/1000 | Loss: 0.00027709
Iteration 706/1000 | Loss: 0.00010408
Iteration 707/1000 | Loss: 0.00025831
Iteration 708/1000 | Loss: 0.00012442
Iteration 709/1000 | Loss: 0.00016991
Iteration 710/1000 | Loss: 0.00069055
Iteration 711/1000 | Loss: 0.00019927
Iteration 712/1000 | Loss: 0.00015948
Iteration 713/1000 | Loss: 0.00006110
Iteration 714/1000 | Loss: 0.00018267
Iteration 715/1000 | Loss: 0.00012438
Iteration 716/1000 | Loss: 0.00013554
Iteration 717/1000 | Loss: 0.00016622
Iteration 718/1000 | Loss: 0.00017178
Iteration 719/1000 | Loss: 0.00013426
Iteration 720/1000 | Loss: 0.00003016
Iteration 721/1000 | Loss: 0.00006483
Iteration 722/1000 | Loss: 0.00005217
Iteration 723/1000 | Loss: 0.00001758
Iteration 724/1000 | Loss: 0.00001677
Iteration 725/1000 | Loss: 0.00005340
Iteration 726/1000 | Loss: 0.00001598
Iteration 727/1000 | Loss: 0.00001545
Iteration 728/1000 | Loss: 0.00001503
Iteration 729/1000 | Loss: 0.00001470
Iteration 730/1000 | Loss: 0.00001450
Iteration 731/1000 | Loss: 0.00001449
Iteration 732/1000 | Loss: 0.00001448
Iteration 733/1000 | Loss: 0.00001448
Iteration 734/1000 | Loss: 0.00001447
Iteration 735/1000 | Loss: 0.00001446
Iteration 736/1000 | Loss: 0.00001444
Iteration 737/1000 | Loss: 0.00001444
Iteration 738/1000 | Loss: 0.00001441
Iteration 739/1000 | Loss: 0.00001441
Iteration 740/1000 | Loss: 0.00001440
Iteration 741/1000 | Loss: 0.00001440
Iteration 742/1000 | Loss: 0.00001436
Iteration 743/1000 | Loss: 0.00001435
Iteration 744/1000 | Loss: 0.00001435
Iteration 745/1000 | Loss: 0.00001435
Iteration 746/1000 | Loss: 0.00001435
Iteration 747/1000 | Loss: 0.00001434
Iteration 748/1000 | Loss: 0.00001434
Iteration 749/1000 | Loss: 0.00001434
Iteration 750/1000 | Loss: 0.00001434
Iteration 751/1000 | Loss: 0.00001434
Iteration 752/1000 | Loss: 0.00001433
Iteration 753/1000 | Loss: 0.00001432
Iteration 754/1000 | Loss: 0.00001432
Iteration 755/1000 | Loss: 0.00001432
Iteration 756/1000 | Loss: 0.00001432
Iteration 757/1000 | Loss: 0.00001431
Iteration 758/1000 | Loss: 0.00001431
Iteration 759/1000 | Loss: 0.00001430
Iteration 760/1000 | Loss: 0.00001429
Iteration 761/1000 | Loss: 0.00001429
Iteration 762/1000 | Loss: 0.00001429
Iteration 763/1000 | Loss: 0.00001428
Iteration 764/1000 | Loss: 0.00001428
Iteration 765/1000 | Loss: 0.00001427
Iteration 766/1000 | Loss: 0.00001427
Iteration 767/1000 | Loss: 0.00001426
Iteration 768/1000 | Loss: 0.00001426
Iteration 769/1000 | Loss: 0.00001425
Iteration 770/1000 | Loss: 0.00001425
Iteration 771/1000 | Loss: 0.00001424
Iteration 772/1000 | Loss: 0.00001424
Iteration 773/1000 | Loss: 0.00001423
Iteration 774/1000 | Loss: 0.00001422
Iteration 775/1000 | Loss: 0.00001422
Iteration 776/1000 | Loss: 0.00001422
Iteration 777/1000 | Loss: 0.00001422
Iteration 778/1000 | Loss: 0.00001421
Iteration 779/1000 | Loss: 0.00001421
Iteration 780/1000 | Loss: 0.00001421
Iteration 781/1000 | Loss: 0.00001421
Iteration 782/1000 | Loss: 0.00001421
Iteration 783/1000 | Loss: 0.00001420
Iteration 784/1000 | Loss: 0.00001420
Iteration 785/1000 | Loss: 0.00001420
Iteration 786/1000 | Loss: 0.00001419
Iteration 787/1000 | Loss: 0.00001419
Iteration 788/1000 | Loss: 0.00001419
Iteration 789/1000 | Loss: 0.00001418
Iteration 790/1000 | Loss: 0.00001418
Iteration 791/1000 | Loss: 0.00001418
Iteration 792/1000 | Loss: 0.00001418
Iteration 793/1000 | Loss: 0.00001418
Iteration 794/1000 | Loss: 0.00001417
Iteration 795/1000 | Loss: 0.00001417
Iteration 796/1000 | Loss: 0.00001417
Iteration 797/1000 | Loss: 0.00001417
Iteration 798/1000 | Loss: 0.00001417
Iteration 799/1000 | Loss: 0.00001416
Iteration 800/1000 | Loss: 0.00001416
Iteration 801/1000 | Loss: 0.00001416
Iteration 802/1000 | Loss: 0.00001415
Iteration 803/1000 | Loss: 0.00001415
Iteration 804/1000 | Loss: 0.00001415
Iteration 805/1000 | Loss: 0.00001415
Iteration 806/1000 | Loss: 0.00001415
Iteration 807/1000 | Loss: 0.00001415
Iteration 808/1000 | Loss: 0.00001415
Iteration 809/1000 | Loss: 0.00001415
Iteration 810/1000 | Loss: 0.00001415
Iteration 811/1000 | Loss: 0.00001415
Iteration 812/1000 | Loss: 0.00001415
Iteration 813/1000 | Loss: 0.00001415
Iteration 814/1000 | Loss: 0.00001414
Iteration 815/1000 | Loss: 0.00001414
Iteration 816/1000 | Loss: 0.00001414
Iteration 817/1000 | Loss: 0.00001414
Iteration 818/1000 | Loss: 0.00001414
Iteration 819/1000 | Loss: 0.00001413
Iteration 820/1000 | Loss: 0.00001413
Iteration 821/1000 | Loss: 0.00001413
Iteration 822/1000 | Loss: 0.00001413
Iteration 823/1000 | Loss: 0.00001413
Iteration 824/1000 | Loss: 0.00001413
Iteration 825/1000 | Loss: 0.00001413
Iteration 826/1000 | Loss: 0.00001413
Iteration 827/1000 | Loss: 0.00001413
Iteration 828/1000 | Loss: 0.00001413
Iteration 829/1000 | Loss: 0.00001413
Iteration 830/1000 | Loss: 0.00001413
Iteration 831/1000 | Loss: 0.00001413
Iteration 832/1000 | Loss: 0.00001413
Iteration 833/1000 | Loss: 0.00001413
Iteration 834/1000 | Loss: 0.00001413
Iteration 835/1000 | Loss: 0.00001413
Iteration 836/1000 | Loss: 0.00001413
Iteration 837/1000 | Loss: 0.00001413
Iteration 838/1000 | Loss: 0.00001413
Iteration 839/1000 | Loss: 0.00001413
Iteration 840/1000 | Loss: 0.00001413
Iteration 841/1000 | Loss: 0.00001413
Iteration 842/1000 | Loss: 0.00001413
Iteration 843/1000 | Loss: 0.00001413
Iteration 844/1000 | Loss: 0.00001413
Iteration 845/1000 | Loss: 0.00001413
Iteration 846/1000 | Loss: 0.00001413
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 846. Stopping optimization.
Last 5 losses: [1.4127397662377916e-05, 1.4127397662377916e-05, 1.4127397662377916e-05, 1.4127397662377916e-05, 1.4127397662377916e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4127397662377916e-05

Optimization complete. Final v2v error: 3.2711679935455322 mm

Highest mean error: 4.085263729095459 mm for frame 170

Lowest mean error: 2.787208080291748 mm for frame 203

Saving results

Total time: 1139.1457722187042
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_32_us_0430/0012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_us_0430/0012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_us_0430/0012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00424006
Iteration 2/25 | Loss: 0.00093453
Iteration 3/25 | Loss: 0.00081573
Iteration 4/25 | Loss: 0.00079832
Iteration 5/25 | Loss: 0.00079191
Iteration 6/25 | Loss: 0.00079026
Iteration 7/25 | Loss: 0.00079023
Iteration 8/25 | Loss: 0.00079023
Iteration 9/25 | Loss: 0.00079023
Iteration 10/25 | Loss: 0.00079023
Iteration 11/25 | Loss: 0.00079023
Iteration 12/25 | Loss: 0.00079023
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0007902274955995381, 0.0007902274955995381, 0.0007902274955995381, 0.0007902274955995381, 0.0007902274955995381]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007902274955995381

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43868840
Iteration 2/25 | Loss: 0.00048601
Iteration 3/25 | Loss: 0.00048600
Iteration 4/25 | Loss: 0.00048600
Iteration 5/25 | Loss: 0.00048600
Iteration 6/25 | Loss: 0.00048600
Iteration 7/25 | Loss: 0.00048600
Iteration 8/25 | Loss: 0.00048600
Iteration 9/25 | Loss: 0.00048600
Iteration 10/25 | Loss: 0.00048600
Iteration 11/25 | Loss: 0.00048600
Iteration 12/25 | Loss: 0.00048600
Iteration 13/25 | Loss: 0.00048600
Iteration 14/25 | Loss: 0.00048600
Iteration 15/25 | Loss: 0.00048600
Iteration 16/25 | Loss: 0.00048600
Iteration 17/25 | Loss: 0.00048600
Iteration 18/25 | Loss: 0.00048600
Iteration 19/25 | Loss: 0.00048600
Iteration 20/25 | Loss: 0.00048600
Iteration 21/25 | Loss: 0.00048600
Iteration 22/25 | Loss: 0.00048600
Iteration 23/25 | Loss: 0.00048600
Iteration 24/25 | Loss: 0.00048600
Iteration 25/25 | Loss: 0.00048600

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00048600
Iteration 2/1000 | Loss: 0.00002544
Iteration 3/1000 | Loss: 0.00001883
Iteration 4/1000 | Loss: 0.00001729
Iteration 5/1000 | Loss: 0.00001641
Iteration 6/1000 | Loss: 0.00001595
Iteration 7/1000 | Loss: 0.00001582
Iteration 8/1000 | Loss: 0.00001563
Iteration 9/1000 | Loss: 0.00001547
Iteration 10/1000 | Loss: 0.00001544
Iteration 11/1000 | Loss: 0.00001538
Iteration 12/1000 | Loss: 0.00001538
Iteration 13/1000 | Loss: 0.00001538
Iteration 14/1000 | Loss: 0.00001538
Iteration 15/1000 | Loss: 0.00001538
Iteration 16/1000 | Loss: 0.00001538
Iteration 17/1000 | Loss: 0.00001538
Iteration 18/1000 | Loss: 0.00001537
Iteration 19/1000 | Loss: 0.00001537
Iteration 20/1000 | Loss: 0.00001536
Iteration 21/1000 | Loss: 0.00001535
Iteration 22/1000 | Loss: 0.00001534
Iteration 23/1000 | Loss: 0.00001534
Iteration 24/1000 | Loss: 0.00001534
Iteration 25/1000 | Loss: 0.00001534
Iteration 26/1000 | Loss: 0.00001534
Iteration 27/1000 | Loss: 0.00001534
Iteration 28/1000 | Loss: 0.00001533
Iteration 29/1000 | Loss: 0.00001533
Iteration 30/1000 | Loss: 0.00001532
Iteration 31/1000 | Loss: 0.00001532
Iteration 32/1000 | Loss: 0.00001532
Iteration 33/1000 | Loss: 0.00001531
Iteration 34/1000 | Loss: 0.00001531
Iteration 35/1000 | Loss: 0.00001530
Iteration 36/1000 | Loss: 0.00001530
Iteration 37/1000 | Loss: 0.00001530
Iteration 38/1000 | Loss: 0.00001530
Iteration 39/1000 | Loss: 0.00001530
Iteration 40/1000 | Loss: 0.00001529
Iteration 41/1000 | Loss: 0.00001529
Iteration 42/1000 | Loss: 0.00001528
Iteration 43/1000 | Loss: 0.00001528
Iteration 44/1000 | Loss: 0.00001528
Iteration 45/1000 | Loss: 0.00001528
Iteration 46/1000 | Loss: 0.00001527
Iteration 47/1000 | Loss: 0.00001527
Iteration 48/1000 | Loss: 0.00001527
Iteration 49/1000 | Loss: 0.00001527
Iteration 50/1000 | Loss: 0.00001526
Iteration 51/1000 | Loss: 0.00001526
Iteration 52/1000 | Loss: 0.00001526
Iteration 53/1000 | Loss: 0.00001525
Iteration 54/1000 | Loss: 0.00001525
Iteration 55/1000 | Loss: 0.00001525
Iteration 56/1000 | Loss: 0.00001525
Iteration 57/1000 | Loss: 0.00001524
Iteration 58/1000 | Loss: 0.00001524
Iteration 59/1000 | Loss: 0.00001524
Iteration 60/1000 | Loss: 0.00001524
Iteration 61/1000 | Loss: 0.00001524
Iteration 62/1000 | Loss: 0.00001524
Iteration 63/1000 | Loss: 0.00001524
Iteration 64/1000 | Loss: 0.00001523
Iteration 65/1000 | Loss: 0.00001523
Iteration 66/1000 | Loss: 0.00001523
Iteration 67/1000 | Loss: 0.00001523
Iteration 68/1000 | Loss: 0.00001523
Iteration 69/1000 | Loss: 0.00001523
Iteration 70/1000 | Loss: 0.00001523
Iteration 71/1000 | Loss: 0.00001523
Iteration 72/1000 | Loss: 0.00001523
Iteration 73/1000 | Loss: 0.00001523
Iteration 74/1000 | Loss: 0.00001522
Iteration 75/1000 | Loss: 0.00001522
Iteration 76/1000 | Loss: 0.00001522
Iteration 77/1000 | Loss: 0.00001522
Iteration 78/1000 | Loss: 0.00001522
Iteration 79/1000 | Loss: 0.00001522
Iteration 80/1000 | Loss: 0.00001522
Iteration 81/1000 | Loss: 0.00001522
Iteration 82/1000 | Loss: 0.00001522
Iteration 83/1000 | Loss: 0.00001522
Iteration 84/1000 | Loss: 0.00001522
Iteration 85/1000 | Loss: 0.00001522
Iteration 86/1000 | Loss: 0.00001522
Iteration 87/1000 | Loss: 0.00001522
Iteration 88/1000 | Loss: 0.00001521
Iteration 89/1000 | Loss: 0.00001521
Iteration 90/1000 | Loss: 0.00001521
Iteration 91/1000 | Loss: 0.00001521
Iteration 92/1000 | Loss: 0.00001521
Iteration 93/1000 | Loss: 0.00001521
Iteration 94/1000 | Loss: 0.00001521
Iteration 95/1000 | Loss: 0.00001520
Iteration 96/1000 | Loss: 0.00001520
Iteration 97/1000 | Loss: 0.00001520
Iteration 98/1000 | Loss: 0.00001520
Iteration 99/1000 | Loss: 0.00001520
Iteration 100/1000 | Loss: 0.00001520
Iteration 101/1000 | Loss: 0.00001520
Iteration 102/1000 | Loss: 0.00001520
Iteration 103/1000 | Loss: 0.00001520
Iteration 104/1000 | Loss: 0.00001520
Iteration 105/1000 | Loss: 0.00001519
Iteration 106/1000 | Loss: 0.00001519
Iteration 107/1000 | Loss: 0.00001519
Iteration 108/1000 | Loss: 0.00001519
Iteration 109/1000 | Loss: 0.00001519
Iteration 110/1000 | Loss: 0.00001519
Iteration 111/1000 | Loss: 0.00001519
Iteration 112/1000 | Loss: 0.00001519
Iteration 113/1000 | Loss: 0.00001519
Iteration 114/1000 | Loss: 0.00001518
Iteration 115/1000 | Loss: 0.00001518
Iteration 116/1000 | Loss: 0.00001518
Iteration 117/1000 | Loss: 0.00001518
Iteration 118/1000 | Loss: 0.00001518
Iteration 119/1000 | Loss: 0.00001518
Iteration 120/1000 | Loss: 0.00001518
Iteration 121/1000 | Loss: 0.00001518
Iteration 122/1000 | Loss: 0.00001518
Iteration 123/1000 | Loss: 0.00001518
Iteration 124/1000 | Loss: 0.00001518
Iteration 125/1000 | Loss: 0.00001518
Iteration 126/1000 | Loss: 0.00001518
Iteration 127/1000 | Loss: 0.00001518
Iteration 128/1000 | Loss: 0.00001518
Iteration 129/1000 | Loss: 0.00001518
Iteration 130/1000 | Loss: 0.00001518
Iteration 131/1000 | Loss: 0.00001518
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 131. Stopping optimization.
Last 5 losses: [1.5179405636445154e-05, 1.5179405636445154e-05, 1.5179405636445154e-05, 1.5179405636445154e-05, 1.5179405636445154e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5179405636445154e-05

Optimization complete. Final v2v error: 3.3316500186920166 mm

Highest mean error: 3.8055789470672607 mm for frame 223

Lowest mean error: 2.938619613647461 mm for frame 10

Saving results

Total time: 35.230308294296265
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_32_us_0430/0007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_us_0430/0007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_us_0430/0007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00899600
Iteration 2/25 | Loss: 0.00134656
Iteration 3/25 | Loss: 0.00101418
Iteration 4/25 | Loss: 0.00097001
Iteration 5/25 | Loss: 0.00095894
Iteration 6/25 | Loss: 0.00095632
Iteration 7/25 | Loss: 0.00095578
Iteration 8/25 | Loss: 0.00095578
Iteration 9/25 | Loss: 0.00095578
Iteration 10/25 | Loss: 0.00095578
Iteration 11/25 | Loss: 0.00095578
Iteration 12/25 | Loss: 0.00095578
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0009557773591950536, 0.0009557773591950536, 0.0009557773591950536, 0.0009557773591950536, 0.0009557773591950536]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009557773591950536

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.03149724
Iteration 2/25 | Loss: 0.00070486
Iteration 3/25 | Loss: 0.00070485
Iteration 4/25 | Loss: 0.00070485
Iteration 5/25 | Loss: 0.00070485
Iteration 6/25 | Loss: 0.00070485
Iteration 7/25 | Loss: 0.00070485
Iteration 8/25 | Loss: 0.00070485
Iteration 9/25 | Loss: 0.00070485
Iteration 10/25 | Loss: 0.00070484
Iteration 11/25 | Loss: 0.00070484
Iteration 12/25 | Loss: 0.00070484
Iteration 13/25 | Loss: 0.00070484
Iteration 14/25 | Loss: 0.00070484
Iteration 15/25 | Loss: 0.00070484
Iteration 16/25 | Loss: 0.00070484
Iteration 17/25 | Loss: 0.00070484
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0007048447150737047, 0.0007048447150737047, 0.0007048447150737047, 0.0007048447150737047, 0.0007048447150737047]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007048447150737047

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00070484
Iteration 2/1000 | Loss: 0.00005244
Iteration 3/1000 | Loss: 0.00003747
Iteration 4/1000 | Loss: 0.00003419
Iteration 5/1000 | Loss: 0.00003133
Iteration 6/1000 | Loss: 0.00003025
Iteration 7/1000 | Loss: 0.00002889
Iteration 8/1000 | Loss: 0.00002835
Iteration 9/1000 | Loss: 0.00002797
Iteration 10/1000 | Loss: 0.00002769
Iteration 11/1000 | Loss: 0.00002769
Iteration 12/1000 | Loss: 0.00002767
Iteration 13/1000 | Loss: 0.00002765
Iteration 14/1000 | Loss: 0.00002762
Iteration 15/1000 | Loss: 0.00002760
Iteration 16/1000 | Loss: 0.00002759
Iteration 17/1000 | Loss: 0.00002758
Iteration 18/1000 | Loss: 0.00002758
Iteration 19/1000 | Loss: 0.00002758
Iteration 20/1000 | Loss: 0.00002757
Iteration 21/1000 | Loss: 0.00002757
Iteration 22/1000 | Loss: 0.00002757
Iteration 23/1000 | Loss: 0.00002757
Iteration 24/1000 | Loss: 0.00002757
Iteration 25/1000 | Loss: 0.00002757
Iteration 26/1000 | Loss: 0.00002757
Iteration 27/1000 | Loss: 0.00002756
Iteration 28/1000 | Loss: 0.00002756
Iteration 29/1000 | Loss: 0.00002756
Iteration 30/1000 | Loss: 0.00002756
Iteration 31/1000 | Loss: 0.00002756
Iteration 32/1000 | Loss: 0.00002756
Iteration 33/1000 | Loss: 0.00002756
Iteration 34/1000 | Loss: 0.00002756
Iteration 35/1000 | Loss: 0.00002756
Iteration 36/1000 | Loss: 0.00002756
Iteration 37/1000 | Loss: 0.00002756
Iteration 38/1000 | Loss: 0.00002756
Iteration 39/1000 | Loss: 0.00002756
Iteration 40/1000 | Loss: 0.00002756
Iteration 41/1000 | Loss: 0.00002756
Iteration 42/1000 | Loss: 0.00002756
Iteration 43/1000 | Loss: 0.00002756
Iteration 44/1000 | Loss: 0.00002756
Iteration 45/1000 | Loss: 0.00002756
Iteration 46/1000 | Loss: 0.00002756
Iteration 47/1000 | Loss: 0.00002756
Iteration 48/1000 | Loss: 0.00002756
Iteration 49/1000 | Loss: 0.00002756
Iteration 50/1000 | Loss: 0.00002756
Iteration 51/1000 | Loss: 0.00002756
Iteration 52/1000 | Loss: 0.00002756
Iteration 53/1000 | Loss: 0.00002756
Iteration 54/1000 | Loss: 0.00002756
Iteration 55/1000 | Loss: 0.00002756
Iteration 56/1000 | Loss: 0.00002756
Iteration 57/1000 | Loss: 0.00002756
Iteration 58/1000 | Loss: 0.00002756
Iteration 59/1000 | Loss: 0.00002756
Iteration 60/1000 | Loss: 0.00002756
Iteration 61/1000 | Loss: 0.00002756
Iteration 62/1000 | Loss: 0.00002756
Iteration 63/1000 | Loss: 0.00002756
Iteration 64/1000 | Loss: 0.00002756
Iteration 65/1000 | Loss: 0.00002756
Iteration 66/1000 | Loss: 0.00002756
Iteration 67/1000 | Loss: 0.00002756
Iteration 68/1000 | Loss: 0.00002756
Iteration 69/1000 | Loss: 0.00002756
Iteration 70/1000 | Loss: 0.00002756
Iteration 71/1000 | Loss: 0.00002756
Iteration 72/1000 | Loss: 0.00002756
Iteration 73/1000 | Loss: 0.00002755
Iteration 74/1000 | Loss: 0.00002755
Iteration 75/1000 | Loss: 0.00002755
Iteration 76/1000 | Loss: 0.00002755
Iteration 77/1000 | Loss: 0.00002755
Iteration 78/1000 | Loss: 0.00002755
Iteration 79/1000 | Loss: 0.00002755
Iteration 80/1000 | Loss: 0.00002755
Iteration 81/1000 | Loss: 0.00002755
Iteration 82/1000 | Loss: 0.00002755
Iteration 83/1000 | Loss: 0.00002755
Iteration 84/1000 | Loss: 0.00002755
Iteration 85/1000 | Loss: 0.00002755
Iteration 86/1000 | Loss: 0.00002755
Iteration 87/1000 | Loss: 0.00002755
Iteration 88/1000 | Loss: 0.00002755
Iteration 89/1000 | Loss: 0.00002755
Iteration 90/1000 | Loss: 0.00002755
Iteration 91/1000 | Loss: 0.00002755
Iteration 92/1000 | Loss: 0.00002755
Iteration 93/1000 | Loss: 0.00002755
Iteration 94/1000 | Loss: 0.00002755
Iteration 95/1000 | Loss: 0.00002755
Iteration 96/1000 | Loss: 0.00002755
Iteration 97/1000 | Loss: 0.00002755
Iteration 98/1000 | Loss: 0.00002755
Iteration 99/1000 | Loss: 0.00002755
Iteration 100/1000 | Loss: 0.00002755
Iteration 101/1000 | Loss: 0.00002755
Iteration 102/1000 | Loss: 0.00002755
Iteration 103/1000 | Loss: 0.00002755
Iteration 104/1000 | Loss: 0.00002755
Iteration 105/1000 | Loss: 0.00002755
Iteration 106/1000 | Loss: 0.00002755
Iteration 107/1000 | Loss: 0.00002755
Iteration 108/1000 | Loss: 0.00002755
Iteration 109/1000 | Loss: 0.00002755
Iteration 110/1000 | Loss: 0.00002755
Iteration 111/1000 | Loss: 0.00002755
Iteration 112/1000 | Loss: 0.00002755
Iteration 113/1000 | Loss: 0.00002755
Iteration 114/1000 | Loss: 0.00002755
Iteration 115/1000 | Loss: 0.00002755
Iteration 116/1000 | Loss: 0.00002755
Iteration 117/1000 | Loss: 0.00002755
Iteration 118/1000 | Loss: 0.00002755
Iteration 119/1000 | Loss: 0.00002755
Iteration 120/1000 | Loss: 0.00002755
Iteration 121/1000 | Loss: 0.00002755
Iteration 122/1000 | Loss: 0.00002755
Iteration 123/1000 | Loss: 0.00002755
Iteration 124/1000 | Loss: 0.00002755
Iteration 125/1000 | Loss: 0.00002755
Iteration 126/1000 | Loss: 0.00002755
Iteration 127/1000 | Loss: 0.00002755
Iteration 128/1000 | Loss: 0.00002755
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 128. Stopping optimization.
Last 5 losses: [2.7551364837563597e-05, 2.7551364837563597e-05, 2.7551364837563597e-05, 2.7551364837563597e-05, 2.7551364837563597e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.7551364837563597e-05

Optimization complete. Final v2v error: 4.441795825958252 mm

Highest mean error: 4.952883720397949 mm for frame 1

Lowest mean error: 4.104920864105225 mm for frame 46

Saving results

Total time: 30.5340895652771
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_32_us_0430/0003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_us_0430/0003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_us_0430/0003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00844090
Iteration 2/25 | Loss: 0.00117767
Iteration 3/25 | Loss: 0.00091263
Iteration 4/25 | Loss: 0.00087550
Iteration 5/25 | Loss: 0.00086947
Iteration 6/25 | Loss: 0.00086916
Iteration 7/25 | Loss: 0.00086915
Iteration 8/25 | Loss: 0.00086915
Iteration 9/25 | Loss: 0.00086915
Iteration 10/25 | Loss: 0.00086915
Iteration 11/25 | Loss: 0.00086915
Iteration 12/25 | Loss: 0.00086915
Iteration 13/25 | Loss: 0.00086915
Iteration 14/25 | Loss: 0.00086915
Iteration 15/25 | Loss: 0.00086915
Iteration 16/25 | Loss: 0.00086915
Iteration 17/25 | Loss: 0.00086915
Iteration 18/25 | Loss: 0.00086915
Iteration 19/25 | Loss: 0.00086915
Iteration 20/25 | Loss: 0.00086915
Iteration 21/25 | Loss: 0.00086915
Iteration 22/25 | Loss: 0.00086915
Iteration 23/25 | Loss: 0.00086915
Iteration 24/25 | Loss: 0.00086915
Iteration 25/25 | Loss: 0.00086915

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42928874
Iteration 2/25 | Loss: 0.00068041
Iteration 3/25 | Loss: 0.00068038
Iteration 4/25 | Loss: 0.00068038
Iteration 5/25 | Loss: 0.00068038
Iteration 6/25 | Loss: 0.00068038
Iteration 7/25 | Loss: 0.00068038
Iteration 8/25 | Loss: 0.00068038
Iteration 9/25 | Loss: 0.00068038
Iteration 10/25 | Loss: 0.00068038
Iteration 11/25 | Loss: 0.00068038
Iteration 12/25 | Loss: 0.00068038
Iteration 13/25 | Loss: 0.00068038
Iteration 14/25 | Loss: 0.00068038
Iteration 15/25 | Loss: 0.00068038
Iteration 16/25 | Loss: 0.00068038
Iteration 17/25 | Loss: 0.00068038
Iteration 18/25 | Loss: 0.00068038
Iteration 19/25 | Loss: 0.00068038
Iteration 20/25 | Loss: 0.00068038
Iteration 21/25 | Loss: 0.00068038
Iteration 22/25 | Loss: 0.00068038
Iteration 23/25 | Loss: 0.00068038
Iteration 24/25 | Loss: 0.00068038
Iteration 25/25 | Loss: 0.00068038

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00068038
Iteration 2/1000 | Loss: 0.00004501
Iteration 3/1000 | Loss: 0.00003200
Iteration 4/1000 | Loss: 0.00002842
Iteration 5/1000 | Loss: 0.00002629
Iteration 6/1000 | Loss: 0.00002511
Iteration 7/1000 | Loss: 0.00002443
Iteration 8/1000 | Loss: 0.00002415
Iteration 9/1000 | Loss: 0.00002390
Iteration 10/1000 | Loss: 0.00002370
Iteration 11/1000 | Loss: 0.00002358
Iteration 12/1000 | Loss: 0.00002358
Iteration 13/1000 | Loss: 0.00002358
Iteration 14/1000 | Loss: 0.00002355
Iteration 15/1000 | Loss: 0.00002353
Iteration 16/1000 | Loss: 0.00002353
Iteration 17/1000 | Loss: 0.00002352
Iteration 18/1000 | Loss: 0.00002352
Iteration 19/1000 | Loss: 0.00002352
Iteration 20/1000 | Loss: 0.00002351
Iteration 21/1000 | Loss: 0.00002351
Iteration 22/1000 | Loss: 0.00002351
Iteration 23/1000 | Loss: 0.00002351
Iteration 24/1000 | Loss: 0.00002350
Iteration 25/1000 | Loss: 0.00002350
Iteration 26/1000 | Loss: 0.00002350
Iteration 27/1000 | Loss: 0.00002350
Iteration 28/1000 | Loss: 0.00002350
Iteration 29/1000 | Loss: 0.00002350
Iteration 30/1000 | Loss: 0.00002350
Iteration 31/1000 | Loss: 0.00002350
Iteration 32/1000 | Loss: 0.00002350
Iteration 33/1000 | Loss: 0.00002350
Iteration 34/1000 | Loss: 0.00002350
Iteration 35/1000 | Loss: 0.00002350
Iteration 36/1000 | Loss: 0.00002350
Iteration 37/1000 | Loss: 0.00002350
Iteration 38/1000 | Loss: 0.00002350
Iteration 39/1000 | Loss: 0.00002350
Iteration 40/1000 | Loss: 0.00002350
Iteration 41/1000 | Loss: 0.00002350
Iteration 42/1000 | Loss: 0.00002350
Iteration 43/1000 | Loss: 0.00002350
Iteration 44/1000 | Loss: 0.00002350
Iteration 45/1000 | Loss: 0.00002350
Iteration 46/1000 | Loss: 0.00002350
Iteration 47/1000 | Loss: 0.00002350
Iteration 48/1000 | Loss: 0.00002350
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 48. Stopping optimization.
Last 5 losses: [2.3498296286561526e-05, 2.3498296286561526e-05, 2.3498296286561526e-05, 2.3498296286561526e-05, 2.3498296286561526e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3498296286561526e-05

Optimization complete. Final v2v error: 4.164569854736328 mm

Highest mean error: 4.517143726348877 mm for frame 117

Lowest mean error: 3.7495200634002686 mm for frame 5

Saving results

Total time: 26.44574809074402
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_32_us_0430/0018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_us_0430/0018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_us_0430/0018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01050482
Iteration 2/25 | Loss: 0.00234122
Iteration 3/25 | Loss: 0.00189101
Iteration 4/25 | Loss: 0.00178656
Iteration 5/25 | Loss: 0.00167778
Iteration 6/25 | Loss: 0.00158424
Iteration 7/25 | Loss: 0.00149410
Iteration 8/25 | Loss: 0.00150500
Iteration 9/25 | Loss: 0.00149237
Iteration 10/25 | Loss: 0.00143290
Iteration 11/25 | Loss: 0.00138252
Iteration 12/25 | Loss: 0.00137021
Iteration 13/25 | Loss: 0.00134404
Iteration 14/25 | Loss: 0.00132252
Iteration 15/25 | Loss: 0.00132203
Iteration 16/25 | Loss: 0.00131995
Iteration 17/25 | Loss: 0.00128684
Iteration 18/25 | Loss: 0.00127911
Iteration 19/25 | Loss: 0.00128656
Iteration 20/25 | Loss: 0.00127643
Iteration 21/25 | Loss: 0.00127348
Iteration 22/25 | Loss: 0.00127324
Iteration 23/25 | Loss: 0.00127302
Iteration 24/25 | Loss: 0.00127287
Iteration 25/25 | Loss: 0.00127281

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44108784
Iteration 2/25 | Loss: 0.00385474
Iteration 3/25 | Loss: 0.00385474
Iteration 4/25 | Loss: 0.00385474
Iteration 5/25 | Loss: 0.00385474
Iteration 6/25 | Loss: 0.00385474
Iteration 7/25 | Loss: 0.00385474
Iteration 8/25 | Loss: 0.00385474
Iteration 9/25 | Loss: 0.00385474
Iteration 10/25 | Loss: 0.00385474
Iteration 11/25 | Loss: 0.00385474
Iteration 12/25 | Loss: 0.00385474
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0038547373842447996, 0.0038547373842447996, 0.0038547373842447996, 0.0038547373842447996, 0.0038547373842447996]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0038547373842447996

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00385474
Iteration 2/1000 | Loss: 0.00068348
Iteration 3/1000 | Loss: 0.00045808
Iteration 4/1000 | Loss: 0.00037218
Iteration 5/1000 | Loss: 0.00032100
Iteration 6/1000 | Loss: 0.00029646
Iteration 7/1000 | Loss: 0.00235058
Iteration 8/1000 | Loss: 0.01406380
Iteration 9/1000 | Loss: 0.00373360
Iteration 10/1000 | Loss: 0.00057250
Iteration 11/1000 | Loss: 0.00081770
Iteration 12/1000 | Loss: 0.00020448
Iteration 13/1000 | Loss: 0.00013806
Iteration 14/1000 | Loss: 0.00008944
Iteration 15/1000 | Loss: 0.00006688
Iteration 16/1000 | Loss: 0.00004638
Iteration 17/1000 | Loss: 0.00003850
Iteration 18/1000 | Loss: 0.00003110
Iteration 19/1000 | Loss: 0.00002721
Iteration 20/1000 | Loss: 0.00002457
Iteration 21/1000 | Loss: 0.00002299
Iteration 22/1000 | Loss: 0.00002181
Iteration 23/1000 | Loss: 0.00002060
Iteration 24/1000 | Loss: 0.00002011
Iteration 25/1000 | Loss: 0.00001961
Iteration 26/1000 | Loss: 0.00001927
Iteration 27/1000 | Loss: 0.00001899
Iteration 28/1000 | Loss: 0.00001884
Iteration 29/1000 | Loss: 0.00001879
Iteration 30/1000 | Loss: 0.00001879
Iteration 31/1000 | Loss: 0.00001877
Iteration 32/1000 | Loss: 0.00001877
Iteration 33/1000 | Loss: 0.00001877
Iteration 34/1000 | Loss: 0.00001877
Iteration 35/1000 | Loss: 0.00001877
Iteration 36/1000 | Loss: 0.00001877
Iteration 37/1000 | Loss: 0.00001876
Iteration 38/1000 | Loss: 0.00001876
Iteration 39/1000 | Loss: 0.00001875
Iteration 40/1000 | Loss: 0.00001875
Iteration 41/1000 | Loss: 0.00001874
Iteration 42/1000 | Loss: 0.00001872
Iteration 43/1000 | Loss: 0.00001872
Iteration 44/1000 | Loss: 0.00001872
Iteration 45/1000 | Loss: 0.00001870
Iteration 46/1000 | Loss: 0.00001870
Iteration 47/1000 | Loss: 0.00001870
Iteration 48/1000 | Loss: 0.00001869
Iteration 49/1000 | Loss: 0.00001869
Iteration 50/1000 | Loss: 0.00001869
Iteration 51/1000 | Loss: 0.00001869
Iteration 52/1000 | Loss: 0.00001869
Iteration 53/1000 | Loss: 0.00001869
Iteration 54/1000 | Loss: 0.00001869
Iteration 55/1000 | Loss: 0.00001868
Iteration 56/1000 | Loss: 0.00001868
Iteration 57/1000 | Loss: 0.00001868
Iteration 58/1000 | Loss: 0.00001868
Iteration 59/1000 | Loss: 0.00001868
Iteration 60/1000 | Loss: 0.00001868
Iteration 61/1000 | Loss: 0.00001868
Iteration 62/1000 | Loss: 0.00001868
Iteration 63/1000 | Loss: 0.00001868
Iteration 64/1000 | Loss: 0.00001868
Iteration 65/1000 | Loss: 0.00001868
Iteration 66/1000 | Loss: 0.00001868
Iteration 67/1000 | Loss: 0.00001867
Iteration 68/1000 | Loss: 0.00001867
Iteration 69/1000 | Loss: 0.00001867
Iteration 70/1000 | Loss: 0.00001867
Iteration 71/1000 | Loss: 0.00001867
Iteration 72/1000 | Loss: 0.00001867
Iteration 73/1000 | Loss: 0.00001867
Iteration 74/1000 | Loss: 0.00001867
Iteration 75/1000 | Loss: 0.00001867
Iteration 76/1000 | Loss: 0.00001866
Iteration 77/1000 | Loss: 0.00001866
Iteration 78/1000 | Loss: 0.00001866
Iteration 79/1000 | Loss: 0.00001866
Iteration 80/1000 | Loss: 0.00001866
Iteration 81/1000 | Loss: 0.00001866
Iteration 82/1000 | Loss: 0.00001866
Iteration 83/1000 | Loss: 0.00001866
Iteration 84/1000 | Loss: 0.00001865
Iteration 85/1000 | Loss: 0.00001865
Iteration 86/1000 | Loss: 0.00001865
Iteration 87/1000 | Loss: 0.00001865
Iteration 88/1000 | Loss: 0.00001865
Iteration 89/1000 | Loss: 0.00001865
Iteration 90/1000 | Loss: 0.00001865
Iteration 91/1000 | Loss: 0.00001865
Iteration 92/1000 | Loss: 0.00001865
Iteration 93/1000 | Loss: 0.00001865
Iteration 94/1000 | Loss: 0.00001865
Iteration 95/1000 | Loss: 0.00001865
Iteration 96/1000 | Loss: 0.00001865
Iteration 97/1000 | Loss: 0.00001865
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 97. Stopping optimization.
Last 5 losses: [1.865218291641213e-05, 1.865218291641213e-05, 1.865218291641213e-05, 1.865218291641213e-05, 1.865218291641213e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.865218291641213e-05

Optimization complete. Final v2v error: 3.538783550262451 mm

Highest mean error: 9.853042602539062 mm for frame 73

Lowest mean error: 3.1187803745269775 mm for frame 42

Saving results

Total time: 86.42107915878296
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_32_us_0430/0019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_us_0430/0019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_us_0430/0019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00505144
Iteration 2/25 | Loss: 0.00113674
Iteration 3/25 | Loss: 0.00099499
Iteration 4/25 | Loss: 0.00094866
Iteration 5/25 | Loss: 0.00094015
Iteration 6/25 | Loss: 0.00093723
Iteration 7/25 | Loss: 0.00093661
Iteration 8/25 | Loss: 0.00093661
Iteration 9/25 | Loss: 0.00093661
Iteration 10/25 | Loss: 0.00093661
Iteration 11/25 | Loss: 0.00093661
Iteration 12/25 | Loss: 0.00093661
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0009366137092001736, 0.0009366137092001736, 0.0009366137092001736, 0.0009366137092001736, 0.0009366137092001736]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009366137092001736

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.66088355
Iteration 2/25 | Loss: 0.00098547
Iteration 3/25 | Loss: 0.00098546
Iteration 4/25 | Loss: 0.00098546
Iteration 5/25 | Loss: 0.00098546
Iteration 6/25 | Loss: 0.00098546
Iteration 7/25 | Loss: 0.00098546
Iteration 8/25 | Loss: 0.00098546
Iteration 9/25 | Loss: 0.00098546
Iteration 10/25 | Loss: 0.00098546
Iteration 11/25 | Loss: 0.00098546
Iteration 12/25 | Loss: 0.00098546
Iteration 13/25 | Loss: 0.00098546
Iteration 14/25 | Loss: 0.00098546
Iteration 15/25 | Loss: 0.00098546
Iteration 16/25 | Loss: 0.00098546
Iteration 17/25 | Loss: 0.00098546
Iteration 18/25 | Loss: 0.00098546
Iteration 19/25 | Loss: 0.00098546
Iteration 20/25 | Loss: 0.00098546
Iteration 21/25 | Loss: 0.00098546
Iteration 22/25 | Loss: 0.00098546
Iteration 23/25 | Loss: 0.00098546
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0009854621021077037, 0.0009854621021077037, 0.0009854621021077037, 0.0009854621021077037, 0.0009854621021077037]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009854621021077037

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00098546
Iteration 2/1000 | Loss: 0.00004938
Iteration 3/1000 | Loss: 0.00003278
Iteration 4/1000 | Loss: 0.00002826
Iteration 5/1000 | Loss: 0.00002661
Iteration 6/1000 | Loss: 0.00002535
Iteration 7/1000 | Loss: 0.00002445
Iteration 8/1000 | Loss: 0.00002383
Iteration 9/1000 | Loss: 0.00002335
Iteration 10/1000 | Loss: 0.00002297
Iteration 11/1000 | Loss: 0.00002267
Iteration 12/1000 | Loss: 0.00002246
Iteration 13/1000 | Loss: 0.00002237
Iteration 14/1000 | Loss: 0.00002225
Iteration 15/1000 | Loss: 0.00002223
Iteration 16/1000 | Loss: 0.00002219
Iteration 17/1000 | Loss: 0.00002218
Iteration 18/1000 | Loss: 0.00002217
Iteration 19/1000 | Loss: 0.00002215
Iteration 20/1000 | Loss: 0.00002214
Iteration 21/1000 | Loss: 0.00002213
Iteration 22/1000 | Loss: 0.00002212
Iteration 23/1000 | Loss: 0.00002212
Iteration 24/1000 | Loss: 0.00002212
Iteration 25/1000 | Loss: 0.00002211
Iteration 26/1000 | Loss: 0.00002211
Iteration 27/1000 | Loss: 0.00002208
Iteration 28/1000 | Loss: 0.00002208
Iteration 29/1000 | Loss: 0.00002207
Iteration 30/1000 | Loss: 0.00002207
Iteration 31/1000 | Loss: 0.00002206
Iteration 32/1000 | Loss: 0.00002206
Iteration 33/1000 | Loss: 0.00002205
Iteration 34/1000 | Loss: 0.00002205
Iteration 35/1000 | Loss: 0.00002204
Iteration 36/1000 | Loss: 0.00002204
Iteration 37/1000 | Loss: 0.00002203
Iteration 38/1000 | Loss: 0.00002203
Iteration 39/1000 | Loss: 0.00002203
Iteration 40/1000 | Loss: 0.00002202
Iteration 41/1000 | Loss: 0.00002202
Iteration 42/1000 | Loss: 0.00002202
Iteration 43/1000 | Loss: 0.00002202
Iteration 44/1000 | Loss: 0.00002201
Iteration 45/1000 | Loss: 0.00002200
Iteration 46/1000 | Loss: 0.00002200
Iteration 47/1000 | Loss: 0.00002200
Iteration 48/1000 | Loss: 0.00002200
Iteration 49/1000 | Loss: 0.00002200
Iteration 50/1000 | Loss: 0.00002200
Iteration 51/1000 | Loss: 0.00002199
Iteration 52/1000 | Loss: 0.00002199
Iteration 53/1000 | Loss: 0.00002199
Iteration 54/1000 | Loss: 0.00002199
Iteration 55/1000 | Loss: 0.00002198
Iteration 56/1000 | Loss: 0.00002198
Iteration 57/1000 | Loss: 0.00002197
Iteration 58/1000 | Loss: 0.00002197
Iteration 59/1000 | Loss: 0.00002197
Iteration 60/1000 | Loss: 0.00002196
Iteration 61/1000 | Loss: 0.00002196
Iteration 62/1000 | Loss: 0.00002196
Iteration 63/1000 | Loss: 0.00002196
Iteration 64/1000 | Loss: 0.00002196
Iteration 65/1000 | Loss: 0.00002196
Iteration 66/1000 | Loss: 0.00002195
Iteration 67/1000 | Loss: 0.00002195
Iteration 68/1000 | Loss: 0.00002195
Iteration 69/1000 | Loss: 0.00002195
Iteration 70/1000 | Loss: 0.00002195
Iteration 71/1000 | Loss: 0.00002194
Iteration 72/1000 | Loss: 0.00002194
Iteration 73/1000 | Loss: 0.00002194
Iteration 74/1000 | Loss: 0.00002194
Iteration 75/1000 | Loss: 0.00002194
Iteration 76/1000 | Loss: 0.00002193
Iteration 77/1000 | Loss: 0.00002193
Iteration 78/1000 | Loss: 0.00002193
Iteration 79/1000 | Loss: 0.00002193
Iteration 80/1000 | Loss: 0.00002193
Iteration 81/1000 | Loss: 0.00002192
Iteration 82/1000 | Loss: 0.00002192
Iteration 83/1000 | Loss: 0.00002192
Iteration 84/1000 | Loss: 0.00002192
Iteration 85/1000 | Loss: 0.00002192
Iteration 86/1000 | Loss: 0.00002192
Iteration 87/1000 | Loss: 0.00002192
Iteration 88/1000 | Loss: 0.00002192
Iteration 89/1000 | Loss: 0.00002192
Iteration 90/1000 | Loss: 0.00002192
Iteration 91/1000 | Loss: 0.00002191
Iteration 92/1000 | Loss: 0.00002191
Iteration 93/1000 | Loss: 0.00002191
Iteration 94/1000 | Loss: 0.00002191
Iteration 95/1000 | Loss: 0.00002191
Iteration 96/1000 | Loss: 0.00002191
Iteration 97/1000 | Loss: 0.00002190
Iteration 98/1000 | Loss: 0.00002190
Iteration 99/1000 | Loss: 0.00002190
Iteration 100/1000 | Loss: 0.00002190
Iteration 101/1000 | Loss: 0.00002190
Iteration 102/1000 | Loss: 0.00002189
Iteration 103/1000 | Loss: 0.00002189
Iteration 104/1000 | Loss: 0.00002189
Iteration 105/1000 | Loss: 0.00002189
Iteration 106/1000 | Loss: 0.00002189
Iteration 107/1000 | Loss: 0.00002188
Iteration 108/1000 | Loss: 0.00002188
Iteration 109/1000 | Loss: 0.00002188
Iteration 110/1000 | Loss: 0.00002188
Iteration 111/1000 | Loss: 0.00002188
Iteration 112/1000 | Loss: 0.00002188
Iteration 113/1000 | Loss: 0.00002187
Iteration 114/1000 | Loss: 0.00002187
Iteration 115/1000 | Loss: 0.00002187
Iteration 116/1000 | Loss: 0.00002187
Iteration 117/1000 | Loss: 0.00002187
Iteration 118/1000 | Loss: 0.00002187
Iteration 119/1000 | Loss: 0.00002187
Iteration 120/1000 | Loss: 0.00002187
Iteration 121/1000 | Loss: 0.00002186
Iteration 122/1000 | Loss: 0.00002186
Iteration 123/1000 | Loss: 0.00002186
Iteration 124/1000 | Loss: 0.00002186
Iteration 125/1000 | Loss: 0.00002186
Iteration 126/1000 | Loss: 0.00002186
Iteration 127/1000 | Loss: 0.00002186
Iteration 128/1000 | Loss: 0.00002186
Iteration 129/1000 | Loss: 0.00002185
Iteration 130/1000 | Loss: 0.00002185
Iteration 131/1000 | Loss: 0.00002185
Iteration 132/1000 | Loss: 0.00002185
Iteration 133/1000 | Loss: 0.00002185
Iteration 134/1000 | Loss: 0.00002185
Iteration 135/1000 | Loss: 0.00002185
Iteration 136/1000 | Loss: 0.00002185
Iteration 137/1000 | Loss: 0.00002185
Iteration 138/1000 | Loss: 0.00002185
Iteration 139/1000 | Loss: 0.00002185
Iteration 140/1000 | Loss: 0.00002185
Iteration 141/1000 | Loss: 0.00002185
Iteration 142/1000 | Loss: 0.00002184
Iteration 143/1000 | Loss: 0.00002184
Iteration 144/1000 | Loss: 0.00002184
Iteration 145/1000 | Loss: 0.00002184
Iteration 146/1000 | Loss: 0.00002184
Iteration 147/1000 | Loss: 0.00002184
Iteration 148/1000 | Loss: 0.00002184
Iteration 149/1000 | Loss: 0.00002184
Iteration 150/1000 | Loss: 0.00002184
Iteration 151/1000 | Loss: 0.00002184
Iteration 152/1000 | Loss: 0.00002184
Iteration 153/1000 | Loss: 0.00002184
Iteration 154/1000 | Loss: 0.00002184
Iteration 155/1000 | Loss: 0.00002183
Iteration 156/1000 | Loss: 0.00002183
Iteration 157/1000 | Loss: 0.00002183
Iteration 158/1000 | Loss: 0.00002183
Iteration 159/1000 | Loss: 0.00002183
Iteration 160/1000 | Loss: 0.00002183
Iteration 161/1000 | Loss: 0.00002183
Iteration 162/1000 | Loss: 0.00002183
Iteration 163/1000 | Loss: 0.00002183
Iteration 164/1000 | Loss: 0.00002183
Iteration 165/1000 | Loss: 0.00002183
Iteration 166/1000 | Loss: 0.00002183
Iteration 167/1000 | Loss: 0.00002183
Iteration 168/1000 | Loss: 0.00002183
Iteration 169/1000 | Loss: 0.00002183
Iteration 170/1000 | Loss: 0.00002183
Iteration 171/1000 | Loss: 0.00002183
Iteration 172/1000 | Loss: 0.00002183
Iteration 173/1000 | Loss: 0.00002183
Iteration 174/1000 | Loss: 0.00002182
Iteration 175/1000 | Loss: 0.00002182
Iteration 176/1000 | Loss: 0.00002182
Iteration 177/1000 | Loss: 0.00002182
Iteration 178/1000 | Loss: 0.00002182
Iteration 179/1000 | Loss: 0.00002182
Iteration 180/1000 | Loss: 0.00002182
Iteration 181/1000 | Loss: 0.00002182
Iteration 182/1000 | Loss: 0.00002182
Iteration 183/1000 | Loss: 0.00002182
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 183. Stopping optimization.
Last 5 losses: [2.1824771465617232e-05, 2.1824771465617232e-05, 2.1824771465617232e-05, 2.1824771465617232e-05, 2.1824771465617232e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1824771465617232e-05

Optimization complete. Final v2v error: 3.9368784427642822 mm

Highest mean error: 5.0969624519348145 mm for frame 53

Lowest mean error: 3.275479555130005 mm for frame 232

Saving results

Total time: 47.92689776420593
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_32_us_0430/0009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_us_0430/0009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_us_0430/0009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00483086
Iteration 2/25 | Loss: 0.00118431
Iteration 3/25 | Loss: 0.00090400
Iteration 4/25 | Loss: 0.00085131
Iteration 5/25 | Loss: 0.00084409
Iteration 6/25 | Loss: 0.00084206
Iteration 7/25 | Loss: 0.00084196
Iteration 8/25 | Loss: 0.00084196
Iteration 9/25 | Loss: 0.00084196
Iteration 10/25 | Loss: 0.00084196
Iteration 11/25 | Loss: 0.00084196
Iteration 12/25 | Loss: 0.00084196
Iteration 13/25 | Loss: 0.00084196
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0008419579826295376, 0.0008419579826295376, 0.0008419579826295376, 0.0008419579826295376, 0.0008419579826295376]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008419579826295376

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46661556
Iteration 2/25 | Loss: 0.00068019
Iteration 3/25 | Loss: 0.00068017
Iteration 4/25 | Loss: 0.00068017
Iteration 5/25 | Loss: 0.00068017
Iteration 6/25 | Loss: 0.00068017
Iteration 7/25 | Loss: 0.00068017
Iteration 8/25 | Loss: 0.00068017
Iteration 9/25 | Loss: 0.00068017
Iteration 10/25 | Loss: 0.00068017
Iteration 11/25 | Loss: 0.00068017
Iteration 12/25 | Loss: 0.00068017
Iteration 13/25 | Loss: 0.00068017
Iteration 14/25 | Loss: 0.00068017
Iteration 15/25 | Loss: 0.00068017
Iteration 16/25 | Loss: 0.00068017
Iteration 17/25 | Loss: 0.00068017
Iteration 18/25 | Loss: 0.00068017
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0006801667623221874, 0.0006801667623221874, 0.0006801667623221874, 0.0006801667623221874, 0.0006801667623221874]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006801667623221874

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00068017
Iteration 2/1000 | Loss: 0.00003322
Iteration 3/1000 | Loss: 0.00002284
Iteration 4/1000 | Loss: 0.00002094
Iteration 5/1000 | Loss: 0.00001969
Iteration 6/1000 | Loss: 0.00001896
Iteration 7/1000 | Loss: 0.00001856
Iteration 8/1000 | Loss: 0.00001827
Iteration 9/1000 | Loss: 0.00001805
Iteration 10/1000 | Loss: 0.00001791
Iteration 11/1000 | Loss: 0.00001779
Iteration 12/1000 | Loss: 0.00001777
Iteration 13/1000 | Loss: 0.00001774
Iteration 14/1000 | Loss: 0.00001773
Iteration 15/1000 | Loss: 0.00001773
Iteration 16/1000 | Loss: 0.00001773
Iteration 17/1000 | Loss: 0.00001771
Iteration 18/1000 | Loss: 0.00001771
Iteration 19/1000 | Loss: 0.00001770
Iteration 20/1000 | Loss: 0.00001770
Iteration 21/1000 | Loss: 0.00001770
Iteration 22/1000 | Loss: 0.00001769
Iteration 23/1000 | Loss: 0.00001769
Iteration 24/1000 | Loss: 0.00001768
Iteration 25/1000 | Loss: 0.00001768
Iteration 26/1000 | Loss: 0.00001767
Iteration 27/1000 | Loss: 0.00001767
Iteration 28/1000 | Loss: 0.00001766
Iteration 29/1000 | Loss: 0.00001766
Iteration 30/1000 | Loss: 0.00001766
Iteration 31/1000 | Loss: 0.00001765
Iteration 32/1000 | Loss: 0.00001765
Iteration 33/1000 | Loss: 0.00001765
Iteration 34/1000 | Loss: 0.00001765
Iteration 35/1000 | Loss: 0.00001765
Iteration 36/1000 | Loss: 0.00001764
Iteration 37/1000 | Loss: 0.00001764
Iteration 38/1000 | Loss: 0.00001764
Iteration 39/1000 | Loss: 0.00001763
Iteration 40/1000 | Loss: 0.00001763
Iteration 41/1000 | Loss: 0.00001763
Iteration 42/1000 | Loss: 0.00001763
Iteration 43/1000 | Loss: 0.00001763
Iteration 44/1000 | Loss: 0.00001763
Iteration 45/1000 | Loss: 0.00001762
Iteration 46/1000 | Loss: 0.00001762
Iteration 47/1000 | Loss: 0.00001762
Iteration 48/1000 | Loss: 0.00001762
Iteration 49/1000 | Loss: 0.00001762
Iteration 50/1000 | Loss: 0.00001762
Iteration 51/1000 | Loss: 0.00001762
Iteration 52/1000 | Loss: 0.00001761
Iteration 53/1000 | Loss: 0.00001761
Iteration 54/1000 | Loss: 0.00001761
Iteration 55/1000 | Loss: 0.00001760
Iteration 56/1000 | Loss: 0.00001760
Iteration 57/1000 | Loss: 0.00001760
Iteration 58/1000 | Loss: 0.00001760
Iteration 59/1000 | Loss: 0.00001760
Iteration 60/1000 | Loss: 0.00001760
Iteration 61/1000 | Loss: 0.00001760
Iteration 62/1000 | Loss: 0.00001759
Iteration 63/1000 | Loss: 0.00001759
Iteration 64/1000 | Loss: 0.00001759
Iteration 65/1000 | Loss: 0.00001759
Iteration 66/1000 | Loss: 0.00001759
Iteration 67/1000 | Loss: 0.00001759
Iteration 68/1000 | Loss: 0.00001758
Iteration 69/1000 | Loss: 0.00001758
Iteration 70/1000 | Loss: 0.00001758
Iteration 71/1000 | Loss: 0.00001758
Iteration 72/1000 | Loss: 0.00001757
Iteration 73/1000 | Loss: 0.00001757
Iteration 74/1000 | Loss: 0.00001757
Iteration 75/1000 | Loss: 0.00001757
Iteration 76/1000 | Loss: 0.00001756
Iteration 77/1000 | Loss: 0.00001756
Iteration 78/1000 | Loss: 0.00001755
Iteration 79/1000 | Loss: 0.00001755
Iteration 80/1000 | Loss: 0.00001755
Iteration 81/1000 | Loss: 0.00001755
Iteration 82/1000 | Loss: 0.00001754
Iteration 83/1000 | Loss: 0.00001754
Iteration 84/1000 | Loss: 0.00001753
Iteration 85/1000 | Loss: 0.00001752
Iteration 86/1000 | Loss: 0.00001752
Iteration 87/1000 | Loss: 0.00001752
Iteration 88/1000 | Loss: 0.00001751
Iteration 89/1000 | Loss: 0.00001751
Iteration 90/1000 | Loss: 0.00001751
Iteration 91/1000 | Loss: 0.00001750
Iteration 92/1000 | Loss: 0.00001750
Iteration 93/1000 | Loss: 0.00001750
Iteration 94/1000 | Loss: 0.00001749
Iteration 95/1000 | Loss: 0.00001749
Iteration 96/1000 | Loss: 0.00001748
Iteration 97/1000 | Loss: 0.00001748
Iteration 98/1000 | Loss: 0.00001748
Iteration 99/1000 | Loss: 0.00001748
Iteration 100/1000 | Loss: 0.00001748
Iteration 101/1000 | Loss: 0.00001748
Iteration 102/1000 | Loss: 0.00001747
Iteration 103/1000 | Loss: 0.00001747
Iteration 104/1000 | Loss: 0.00001747
Iteration 105/1000 | Loss: 0.00001747
Iteration 106/1000 | Loss: 0.00001747
Iteration 107/1000 | Loss: 0.00001746
Iteration 108/1000 | Loss: 0.00001746
Iteration 109/1000 | Loss: 0.00001746
Iteration 110/1000 | Loss: 0.00001746
Iteration 111/1000 | Loss: 0.00001746
Iteration 112/1000 | Loss: 0.00001746
Iteration 113/1000 | Loss: 0.00001745
Iteration 114/1000 | Loss: 0.00001745
Iteration 115/1000 | Loss: 0.00001745
Iteration 116/1000 | Loss: 0.00001745
Iteration 117/1000 | Loss: 0.00001745
Iteration 118/1000 | Loss: 0.00001745
Iteration 119/1000 | Loss: 0.00001745
Iteration 120/1000 | Loss: 0.00001745
Iteration 121/1000 | Loss: 0.00001745
Iteration 122/1000 | Loss: 0.00001745
Iteration 123/1000 | Loss: 0.00001745
Iteration 124/1000 | Loss: 0.00001745
Iteration 125/1000 | Loss: 0.00001745
Iteration 126/1000 | Loss: 0.00001745
Iteration 127/1000 | Loss: 0.00001745
Iteration 128/1000 | Loss: 0.00001745
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 128. Stopping optimization.
Last 5 losses: [1.7452272004447877e-05, 1.7452272004447877e-05, 1.7452272004447877e-05, 1.7452272004447877e-05, 1.7452272004447877e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7452272004447877e-05

Optimization complete. Final v2v error: 3.474116802215576 mm

Highest mean error: 3.864661931991577 mm for frame 218

Lowest mean error: 3.195303201675415 mm for frame 134

Saving results

Total time: 39.032875061035156
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_grace_posed_004/1030/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_grace_posed_004/1030.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_grace_posed_004/1030
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00525798
Iteration 2/25 | Loss: 0.00117460
Iteration 3/25 | Loss: 0.00108477
Iteration 4/25 | Loss: 0.00107528
Iteration 5/25 | Loss: 0.00107259
Iteration 6/25 | Loss: 0.00107259
Iteration 7/25 | Loss: 0.00107259
Iteration 8/25 | Loss: 0.00107259
Iteration 9/25 | Loss: 0.00107259
Iteration 10/25 | Loss: 0.00107259
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.001072589191608131, 0.001072589191608131, 0.001072589191608131, 0.001072589191608131, 0.001072589191608131]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001072589191608131

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.31922460
Iteration 2/25 | Loss: 0.00107687
Iteration 3/25 | Loss: 0.00107684
Iteration 4/25 | Loss: 0.00107684
Iteration 5/25 | Loss: 0.00107684
Iteration 6/25 | Loss: 0.00107684
Iteration 7/25 | Loss: 0.00107684
Iteration 8/25 | Loss: 0.00107683
Iteration 9/25 | Loss: 0.00107683
Iteration 10/25 | Loss: 0.00107683
Iteration 11/25 | Loss: 0.00107683
Iteration 12/25 | Loss: 0.00107683
Iteration 13/25 | Loss: 0.00107683
Iteration 14/25 | Loss: 0.00107683
Iteration 15/25 | Loss: 0.00107683
Iteration 16/25 | Loss: 0.00107683
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0010768345091491938, 0.0010768345091491938, 0.0010768345091491938, 0.0010768345091491938, 0.0010768345091491938]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010768345091491938

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00107683
Iteration 2/1000 | Loss: 0.00002490
Iteration 3/1000 | Loss: 0.00001765
Iteration 4/1000 | Loss: 0.00001564
Iteration 5/1000 | Loss: 0.00001484
Iteration 6/1000 | Loss: 0.00001444
Iteration 7/1000 | Loss: 0.00001412
Iteration 8/1000 | Loss: 0.00001400
Iteration 9/1000 | Loss: 0.00001379
Iteration 10/1000 | Loss: 0.00001362
Iteration 11/1000 | Loss: 0.00001358
Iteration 12/1000 | Loss: 0.00001353
Iteration 13/1000 | Loss: 0.00001352
Iteration 14/1000 | Loss: 0.00001351
Iteration 15/1000 | Loss: 0.00001343
Iteration 16/1000 | Loss: 0.00001342
Iteration 17/1000 | Loss: 0.00001341
Iteration 18/1000 | Loss: 0.00001337
Iteration 19/1000 | Loss: 0.00001331
Iteration 20/1000 | Loss: 0.00001328
Iteration 21/1000 | Loss: 0.00001328
Iteration 22/1000 | Loss: 0.00001328
Iteration 23/1000 | Loss: 0.00001326
Iteration 24/1000 | Loss: 0.00001322
Iteration 25/1000 | Loss: 0.00001317
Iteration 26/1000 | Loss: 0.00001317
Iteration 27/1000 | Loss: 0.00001314
Iteration 28/1000 | Loss: 0.00001313
Iteration 29/1000 | Loss: 0.00001313
Iteration 30/1000 | Loss: 0.00001312
Iteration 31/1000 | Loss: 0.00001311
Iteration 32/1000 | Loss: 0.00001311
Iteration 33/1000 | Loss: 0.00001306
Iteration 34/1000 | Loss: 0.00001306
Iteration 35/1000 | Loss: 0.00001303
Iteration 36/1000 | Loss: 0.00001302
Iteration 37/1000 | Loss: 0.00001300
Iteration 38/1000 | Loss: 0.00001300
Iteration 39/1000 | Loss: 0.00001300
Iteration 40/1000 | Loss: 0.00001299
Iteration 41/1000 | Loss: 0.00001298
Iteration 42/1000 | Loss: 0.00001297
Iteration 43/1000 | Loss: 0.00001297
Iteration 44/1000 | Loss: 0.00001296
Iteration 45/1000 | Loss: 0.00001293
Iteration 46/1000 | Loss: 0.00001293
Iteration 47/1000 | Loss: 0.00001293
Iteration 48/1000 | Loss: 0.00001293
Iteration 49/1000 | Loss: 0.00001293
Iteration 50/1000 | Loss: 0.00001292
Iteration 51/1000 | Loss: 0.00001292
Iteration 52/1000 | Loss: 0.00001292
Iteration 53/1000 | Loss: 0.00001292
Iteration 54/1000 | Loss: 0.00001291
Iteration 55/1000 | Loss: 0.00001289
Iteration 56/1000 | Loss: 0.00001289
Iteration 57/1000 | Loss: 0.00001289
Iteration 58/1000 | Loss: 0.00001289
Iteration 59/1000 | Loss: 0.00001289
Iteration 60/1000 | Loss: 0.00001288
Iteration 61/1000 | Loss: 0.00001288
Iteration 62/1000 | Loss: 0.00001288
Iteration 63/1000 | Loss: 0.00001288
Iteration 64/1000 | Loss: 0.00001287
Iteration 65/1000 | Loss: 0.00001286
Iteration 66/1000 | Loss: 0.00001285
Iteration 67/1000 | Loss: 0.00001285
Iteration 68/1000 | Loss: 0.00001284
Iteration 69/1000 | Loss: 0.00001284
Iteration 70/1000 | Loss: 0.00001284
Iteration 71/1000 | Loss: 0.00001283
Iteration 72/1000 | Loss: 0.00001283
Iteration 73/1000 | Loss: 0.00001283
Iteration 74/1000 | Loss: 0.00001282
Iteration 75/1000 | Loss: 0.00001282
Iteration 76/1000 | Loss: 0.00001282
Iteration 77/1000 | Loss: 0.00001281
Iteration 78/1000 | Loss: 0.00001281
Iteration 79/1000 | Loss: 0.00001280
Iteration 80/1000 | Loss: 0.00001280
Iteration 81/1000 | Loss: 0.00001280
Iteration 82/1000 | Loss: 0.00001280
Iteration 83/1000 | Loss: 0.00001280
Iteration 84/1000 | Loss: 0.00001280
Iteration 85/1000 | Loss: 0.00001280
Iteration 86/1000 | Loss: 0.00001280
Iteration 87/1000 | Loss: 0.00001279
Iteration 88/1000 | Loss: 0.00001279
Iteration 89/1000 | Loss: 0.00001279
Iteration 90/1000 | Loss: 0.00001279
Iteration 91/1000 | Loss: 0.00001279
Iteration 92/1000 | Loss: 0.00001278
Iteration 93/1000 | Loss: 0.00001278
Iteration 94/1000 | Loss: 0.00001278
Iteration 95/1000 | Loss: 0.00001278
Iteration 96/1000 | Loss: 0.00001277
Iteration 97/1000 | Loss: 0.00001277
Iteration 98/1000 | Loss: 0.00001276
Iteration 99/1000 | Loss: 0.00001276
Iteration 100/1000 | Loss: 0.00001276
Iteration 101/1000 | Loss: 0.00001275
Iteration 102/1000 | Loss: 0.00001275
Iteration 103/1000 | Loss: 0.00001275
Iteration 104/1000 | Loss: 0.00001275
Iteration 105/1000 | Loss: 0.00001275
Iteration 106/1000 | Loss: 0.00001274
Iteration 107/1000 | Loss: 0.00001274
Iteration 108/1000 | Loss: 0.00001274
Iteration 109/1000 | Loss: 0.00001274
Iteration 110/1000 | Loss: 0.00001273
Iteration 111/1000 | Loss: 0.00001273
Iteration 112/1000 | Loss: 0.00001273
Iteration 113/1000 | Loss: 0.00001273
Iteration 114/1000 | Loss: 0.00001273
Iteration 115/1000 | Loss: 0.00001273
Iteration 116/1000 | Loss: 0.00001273
Iteration 117/1000 | Loss: 0.00001272
Iteration 118/1000 | Loss: 0.00001272
Iteration 119/1000 | Loss: 0.00001272
Iteration 120/1000 | Loss: 0.00001272
Iteration 121/1000 | Loss: 0.00001271
Iteration 122/1000 | Loss: 0.00001271
Iteration 123/1000 | Loss: 0.00001271
Iteration 124/1000 | Loss: 0.00001270
Iteration 125/1000 | Loss: 0.00001270
Iteration 126/1000 | Loss: 0.00001270
Iteration 127/1000 | Loss: 0.00001270
Iteration 128/1000 | Loss: 0.00001270
Iteration 129/1000 | Loss: 0.00001269
Iteration 130/1000 | Loss: 0.00001269
Iteration 131/1000 | Loss: 0.00001269
Iteration 132/1000 | Loss: 0.00001268
Iteration 133/1000 | Loss: 0.00001268
Iteration 134/1000 | Loss: 0.00001268
Iteration 135/1000 | Loss: 0.00001268
Iteration 136/1000 | Loss: 0.00001267
Iteration 137/1000 | Loss: 0.00001266
Iteration 138/1000 | Loss: 0.00001266
Iteration 139/1000 | Loss: 0.00001266
Iteration 140/1000 | Loss: 0.00001266
Iteration 141/1000 | Loss: 0.00001265
Iteration 142/1000 | Loss: 0.00001265
Iteration 143/1000 | Loss: 0.00001265
Iteration 144/1000 | Loss: 0.00001264
Iteration 145/1000 | Loss: 0.00001264
Iteration 146/1000 | Loss: 0.00001264
Iteration 147/1000 | Loss: 0.00001263
Iteration 148/1000 | Loss: 0.00001263
Iteration 149/1000 | Loss: 0.00001263
Iteration 150/1000 | Loss: 0.00001263
Iteration 151/1000 | Loss: 0.00001263
Iteration 152/1000 | Loss: 0.00001263
Iteration 153/1000 | Loss: 0.00001262
Iteration 154/1000 | Loss: 0.00001262
Iteration 155/1000 | Loss: 0.00001262
Iteration 156/1000 | Loss: 0.00001262
Iteration 157/1000 | Loss: 0.00001262
Iteration 158/1000 | Loss: 0.00001262
Iteration 159/1000 | Loss: 0.00001262
Iteration 160/1000 | Loss: 0.00001262
Iteration 161/1000 | Loss: 0.00001262
Iteration 162/1000 | Loss: 0.00001261
Iteration 163/1000 | Loss: 0.00001261
Iteration 164/1000 | Loss: 0.00001261
Iteration 165/1000 | Loss: 0.00001261
Iteration 166/1000 | Loss: 0.00001261
Iteration 167/1000 | Loss: 0.00001261
Iteration 168/1000 | Loss: 0.00001261
Iteration 169/1000 | Loss: 0.00001260
Iteration 170/1000 | Loss: 0.00001260
Iteration 171/1000 | Loss: 0.00001260
Iteration 172/1000 | Loss: 0.00001260
Iteration 173/1000 | Loss: 0.00001260
Iteration 174/1000 | Loss: 0.00001260
Iteration 175/1000 | Loss: 0.00001260
Iteration 176/1000 | Loss: 0.00001260
Iteration 177/1000 | Loss: 0.00001259
Iteration 178/1000 | Loss: 0.00001259
Iteration 179/1000 | Loss: 0.00001259
Iteration 180/1000 | Loss: 0.00001259
Iteration 181/1000 | Loss: 0.00001259
Iteration 182/1000 | Loss: 0.00001259
Iteration 183/1000 | Loss: 0.00001259
Iteration 184/1000 | Loss: 0.00001259
Iteration 185/1000 | Loss: 0.00001259
Iteration 186/1000 | Loss: 0.00001259
Iteration 187/1000 | Loss: 0.00001259
Iteration 188/1000 | Loss: 0.00001259
Iteration 189/1000 | Loss: 0.00001259
Iteration 190/1000 | Loss: 0.00001258
Iteration 191/1000 | Loss: 0.00001258
Iteration 192/1000 | Loss: 0.00001258
Iteration 193/1000 | Loss: 0.00001258
Iteration 194/1000 | Loss: 0.00001258
Iteration 195/1000 | Loss: 0.00001258
Iteration 196/1000 | Loss: 0.00001258
Iteration 197/1000 | Loss: 0.00001258
Iteration 198/1000 | Loss: 0.00001258
Iteration 199/1000 | Loss: 0.00001258
Iteration 200/1000 | Loss: 0.00001258
Iteration 201/1000 | Loss: 0.00001258
Iteration 202/1000 | Loss: 0.00001258
Iteration 203/1000 | Loss: 0.00001258
Iteration 204/1000 | Loss: 0.00001258
Iteration 205/1000 | Loss: 0.00001258
Iteration 206/1000 | Loss: 0.00001258
Iteration 207/1000 | Loss: 0.00001257
Iteration 208/1000 | Loss: 0.00001257
Iteration 209/1000 | Loss: 0.00001257
Iteration 210/1000 | Loss: 0.00001257
Iteration 211/1000 | Loss: 0.00001257
Iteration 212/1000 | Loss: 0.00001257
Iteration 213/1000 | Loss: 0.00001256
Iteration 214/1000 | Loss: 0.00001256
Iteration 215/1000 | Loss: 0.00001256
Iteration 216/1000 | Loss: 0.00001256
Iteration 217/1000 | Loss: 0.00001256
Iteration 218/1000 | Loss: 0.00001256
Iteration 219/1000 | Loss: 0.00001256
Iteration 220/1000 | Loss: 0.00001256
Iteration 221/1000 | Loss: 0.00001256
Iteration 222/1000 | Loss: 0.00001256
Iteration 223/1000 | Loss: 0.00001256
Iteration 224/1000 | Loss: 0.00001256
Iteration 225/1000 | Loss: 0.00001256
Iteration 226/1000 | Loss: 0.00001256
Iteration 227/1000 | Loss: 0.00001256
Iteration 228/1000 | Loss: 0.00001256
Iteration 229/1000 | Loss: 0.00001256
Iteration 230/1000 | Loss: 0.00001256
Iteration 231/1000 | Loss: 0.00001255
Iteration 232/1000 | Loss: 0.00001255
Iteration 233/1000 | Loss: 0.00001255
Iteration 234/1000 | Loss: 0.00001255
Iteration 235/1000 | Loss: 0.00001255
Iteration 236/1000 | Loss: 0.00001255
Iteration 237/1000 | Loss: 0.00001255
Iteration 238/1000 | Loss: 0.00001255
Iteration 239/1000 | Loss: 0.00001255
Iteration 240/1000 | Loss: 0.00001255
Iteration 241/1000 | Loss: 0.00001255
Iteration 242/1000 | Loss: 0.00001255
Iteration 243/1000 | Loss: 0.00001255
Iteration 244/1000 | Loss: 0.00001255
Iteration 245/1000 | Loss: 0.00001255
Iteration 246/1000 | Loss: 0.00001255
Iteration 247/1000 | Loss: 0.00001255
Iteration 248/1000 | Loss: 0.00001255
Iteration 249/1000 | Loss: 0.00001255
Iteration 250/1000 | Loss: 0.00001255
Iteration 251/1000 | Loss: 0.00001255
Iteration 252/1000 | Loss: 0.00001255
Iteration 253/1000 | Loss: 0.00001255
Iteration 254/1000 | Loss: 0.00001255
Iteration 255/1000 | Loss: 0.00001255
Iteration 256/1000 | Loss: 0.00001255
Iteration 257/1000 | Loss: 0.00001255
Iteration 258/1000 | Loss: 0.00001255
Iteration 259/1000 | Loss: 0.00001255
Iteration 260/1000 | Loss: 0.00001255
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 260. Stopping optimization.
Last 5 losses: [1.2553567103168461e-05, 1.2553567103168461e-05, 1.2553567103168461e-05, 1.2553567103168461e-05, 1.2553567103168461e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2553567103168461e-05

Optimization complete. Final v2v error: 2.9490175247192383 mm

Highest mean error: 3.561971426010132 mm for frame 143

Lowest mean error: 2.3530054092407227 mm for frame 36

Saving results

Total time: 49.37213349342346
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_grace_posed_004/1078/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_grace_posed_004/1078.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_grace_posed_004/1078
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00857556
Iteration 2/25 | Loss: 0.00111557
Iteration 3/25 | Loss: 0.00102945
Iteration 4/25 | Loss: 0.00101880
Iteration 5/25 | Loss: 0.00101598
Iteration 6/25 | Loss: 0.00101520
Iteration 7/25 | Loss: 0.00101517
Iteration 8/25 | Loss: 0.00101517
Iteration 9/25 | Loss: 0.00101517
Iteration 10/25 | Loss: 0.00101517
Iteration 11/25 | Loss: 0.00101517
Iteration 12/25 | Loss: 0.00101517
Iteration 13/25 | Loss: 0.00101517
Iteration 14/25 | Loss: 0.00101517
Iteration 15/25 | Loss: 0.00101517
Iteration 16/25 | Loss: 0.00101517
Iteration 17/25 | Loss: 0.00101517
Iteration 18/25 | Loss: 0.00101517
Iteration 19/25 | Loss: 0.00101517
Iteration 20/25 | Loss: 0.00101517
Iteration 21/25 | Loss: 0.00101517
Iteration 22/25 | Loss: 0.00101517
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0010151681490242481, 0.0010151681490242481, 0.0010151681490242481, 0.0010151681490242481, 0.0010151681490242481]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010151681490242481

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42562246
Iteration 2/25 | Loss: 0.00112657
Iteration 3/25 | Loss: 0.00112656
Iteration 4/25 | Loss: 0.00112656
Iteration 5/25 | Loss: 0.00112656
Iteration 6/25 | Loss: 0.00112656
Iteration 7/25 | Loss: 0.00112656
Iteration 8/25 | Loss: 0.00112656
Iteration 9/25 | Loss: 0.00112656
Iteration 10/25 | Loss: 0.00112656
Iteration 11/25 | Loss: 0.00112656
Iteration 12/25 | Loss: 0.00112656
Iteration 13/25 | Loss: 0.00112656
Iteration 14/25 | Loss: 0.00112656
Iteration 15/25 | Loss: 0.00112656
Iteration 16/25 | Loss: 0.00112656
Iteration 17/25 | Loss: 0.00112656
Iteration 18/25 | Loss: 0.00112656
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0011265595676377416, 0.0011265595676377416, 0.0011265595676377416, 0.0011265595676377416, 0.0011265595676377416]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011265595676377416

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00112656
Iteration 2/1000 | Loss: 0.00002474
Iteration 3/1000 | Loss: 0.00001589
Iteration 4/1000 | Loss: 0.00001209
Iteration 5/1000 | Loss: 0.00001105
Iteration 6/1000 | Loss: 0.00001042
Iteration 7/1000 | Loss: 0.00000993
Iteration 8/1000 | Loss: 0.00000963
Iteration 9/1000 | Loss: 0.00000934
Iteration 10/1000 | Loss: 0.00000918
Iteration 11/1000 | Loss: 0.00000915
Iteration 12/1000 | Loss: 0.00000915
Iteration 13/1000 | Loss: 0.00000914
Iteration 14/1000 | Loss: 0.00000913
Iteration 15/1000 | Loss: 0.00000912
Iteration 16/1000 | Loss: 0.00000911
Iteration 17/1000 | Loss: 0.00000911
Iteration 18/1000 | Loss: 0.00000911
Iteration 19/1000 | Loss: 0.00000910
Iteration 20/1000 | Loss: 0.00000908
Iteration 21/1000 | Loss: 0.00000901
Iteration 22/1000 | Loss: 0.00000898
Iteration 23/1000 | Loss: 0.00000897
Iteration 24/1000 | Loss: 0.00000897
Iteration 25/1000 | Loss: 0.00000896
Iteration 26/1000 | Loss: 0.00000895
Iteration 27/1000 | Loss: 0.00000892
Iteration 28/1000 | Loss: 0.00000889
Iteration 29/1000 | Loss: 0.00000888
Iteration 30/1000 | Loss: 0.00000888
Iteration 31/1000 | Loss: 0.00000888
Iteration 32/1000 | Loss: 0.00000886
Iteration 33/1000 | Loss: 0.00000886
Iteration 34/1000 | Loss: 0.00000885
Iteration 35/1000 | Loss: 0.00000884
Iteration 36/1000 | Loss: 0.00000884
Iteration 37/1000 | Loss: 0.00000883
Iteration 38/1000 | Loss: 0.00000882
Iteration 39/1000 | Loss: 0.00000882
Iteration 40/1000 | Loss: 0.00000882
Iteration 41/1000 | Loss: 0.00000882
Iteration 42/1000 | Loss: 0.00000882
Iteration 43/1000 | Loss: 0.00000882
Iteration 44/1000 | Loss: 0.00000880
Iteration 45/1000 | Loss: 0.00000878
Iteration 46/1000 | Loss: 0.00000878
Iteration 47/1000 | Loss: 0.00000878
Iteration 48/1000 | Loss: 0.00000878
Iteration 49/1000 | Loss: 0.00000877
Iteration 50/1000 | Loss: 0.00000877
Iteration 51/1000 | Loss: 0.00000877
Iteration 52/1000 | Loss: 0.00000877
Iteration 53/1000 | Loss: 0.00000876
Iteration 54/1000 | Loss: 0.00000876
Iteration 55/1000 | Loss: 0.00000875
Iteration 56/1000 | Loss: 0.00000875
Iteration 57/1000 | Loss: 0.00000875
Iteration 58/1000 | Loss: 0.00000874
Iteration 59/1000 | Loss: 0.00000874
Iteration 60/1000 | Loss: 0.00000874
Iteration 61/1000 | Loss: 0.00000873
Iteration 62/1000 | Loss: 0.00000873
Iteration 63/1000 | Loss: 0.00000872
Iteration 64/1000 | Loss: 0.00000872
Iteration 65/1000 | Loss: 0.00000871
Iteration 66/1000 | Loss: 0.00000871
Iteration 67/1000 | Loss: 0.00000870
Iteration 68/1000 | Loss: 0.00000870
Iteration 69/1000 | Loss: 0.00000869
Iteration 70/1000 | Loss: 0.00000869
Iteration 71/1000 | Loss: 0.00000869
Iteration 72/1000 | Loss: 0.00000869
Iteration 73/1000 | Loss: 0.00000869
Iteration 74/1000 | Loss: 0.00000868
Iteration 75/1000 | Loss: 0.00000867
Iteration 76/1000 | Loss: 0.00000866
Iteration 77/1000 | Loss: 0.00000866
Iteration 78/1000 | Loss: 0.00000865
Iteration 79/1000 | Loss: 0.00000865
Iteration 80/1000 | Loss: 0.00000865
Iteration 81/1000 | Loss: 0.00000865
Iteration 82/1000 | Loss: 0.00000865
Iteration 83/1000 | Loss: 0.00000865
Iteration 84/1000 | Loss: 0.00000864
Iteration 85/1000 | Loss: 0.00000864
Iteration 86/1000 | Loss: 0.00000864
Iteration 87/1000 | Loss: 0.00000864
Iteration 88/1000 | Loss: 0.00000863
Iteration 89/1000 | Loss: 0.00000863
Iteration 90/1000 | Loss: 0.00000863
Iteration 91/1000 | Loss: 0.00000863
Iteration 92/1000 | Loss: 0.00000862
Iteration 93/1000 | Loss: 0.00000862
Iteration 94/1000 | Loss: 0.00000862
Iteration 95/1000 | Loss: 0.00000862
Iteration 96/1000 | Loss: 0.00000862
Iteration 97/1000 | Loss: 0.00000862
Iteration 98/1000 | Loss: 0.00000862
Iteration 99/1000 | Loss: 0.00000862
Iteration 100/1000 | Loss: 0.00000862
Iteration 101/1000 | Loss: 0.00000862
Iteration 102/1000 | Loss: 0.00000861
Iteration 103/1000 | Loss: 0.00000861
Iteration 104/1000 | Loss: 0.00000861
Iteration 105/1000 | Loss: 0.00000861
Iteration 106/1000 | Loss: 0.00000861
Iteration 107/1000 | Loss: 0.00000861
Iteration 108/1000 | Loss: 0.00000861
Iteration 109/1000 | Loss: 0.00000861
Iteration 110/1000 | Loss: 0.00000861
Iteration 111/1000 | Loss: 0.00000861
Iteration 112/1000 | Loss: 0.00000861
Iteration 113/1000 | Loss: 0.00000861
Iteration 114/1000 | Loss: 0.00000861
Iteration 115/1000 | Loss: 0.00000861
Iteration 116/1000 | Loss: 0.00000861
Iteration 117/1000 | Loss: 0.00000861
Iteration 118/1000 | Loss: 0.00000860
Iteration 119/1000 | Loss: 0.00000860
Iteration 120/1000 | Loss: 0.00000860
Iteration 121/1000 | Loss: 0.00000860
Iteration 122/1000 | Loss: 0.00000860
Iteration 123/1000 | Loss: 0.00000860
Iteration 124/1000 | Loss: 0.00000860
Iteration 125/1000 | Loss: 0.00000860
Iteration 126/1000 | Loss: 0.00000859
Iteration 127/1000 | Loss: 0.00000859
Iteration 128/1000 | Loss: 0.00000859
Iteration 129/1000 | Loss: 0.00000859
Iteration 130/1000 | Loss: 0.00000859
Iteration 131/1000 | Loss: 0.00000859
Iteration 132/1000 | Loss: 0.00000859
Iteration 133/1000 | Loss: 0.00000859
Iteration 134/1000 | Loss: 0.00000859
Iteration 135/1000 | Loss: 0.00000859
Iteration 136/1000 | Loss: 0.00000859
Iteration 137/1000 | Loss: 0.00000859
Iteration 138/1000 | Loss: 0.00000859
Iteration 139/1000 | Loss: 0.00000858
Iteration 140/1000 | Loss: 0.00000858
Iteration 141/1000 | Loss: 0.00000858
Iteration 142/1000 | Loss: 0.00000858
Iteration 143/1000 | Loss: 0.00000858
Iteration 144/1000 | Loss: 0.00000858
Iteration 145/1000 | Loss: 0.00000858
Iteration 146/1000 | Loss: 0.00000858
Iteration 147/1000 | Loss: 0.00000858
Iteration 148/1000 | Loss: 0.00000858
Iteration 149/1000 | Loss: 0.00000858
Iteration 150/1000 | Loss: 0.00000858
Iteration 151/1000 | Loss: 0.00000858
Iteration 152/1000 | Loss: 0.00000858
Iteration 153/1000 | Loss: 0.00000858
Iteration 154/1000 | Loss: 0.00000858
Iteration 155/1000 | Loss: 0.00000858
Iteration 156/1000 | Loss: 0.00000858
Iteration 157/1000 | Loss: 0.00000857
Iteration 158/1000 | Loss: 0.00000857
Iteration 159/1000 | Loss: 0.00000857
Iteration 160/1000 | Loss: 0.00000857
Iteration 161/1000 | Loss: 0.00000857
Iteration 162/1000 | Loss: 0.00000857
Iteration 163/1000 | Loss: 0.00000857
Iteration 164/1000 | Loss: 0.00000857
Iteration 165/1000 | Loss: 0.00000857
Iteration 166/1000 | Loss: 0.00000857
Iteration 167/1000 | Loss: 0.00000857
Iteration 168/1000 | Loss: 0.00000856
Iteration 169/1000 | Loss: 0.00000856
Iteration 170/1000 | Loss: 0.00000856
Iteration 171/1000 | Loss: 0.00000856
Iteration 172/1000 | Loss: 0.00000856
Iteration 173/1000 | Loss: 0.00000856
Iteration 174/1000 | Loss: 0.00000856
Iteration 175/1000 | Loss: 0.00000856
Iteration 176/1000 | Loss: 0.00000856
Iteration 177/1000 | Loss: 0.00000856
Iteration 178/1000 | Loss: 0.00000856
Iteration 179/1000 | Loss: 0.00000856
Iteration 180/1000 | Loss: 0.00000856
Iteration 181/1000 | Loss: 0.00000856
Iteration 182/1000 | Loss: 0.00000856
Iteration 183/1000 | Loss: 0.00000856
Iteration 184/1000 | Loss: 0.00000856
Iteration 185/1000 | Loss: 0.00000856
Iteration 186/1000 | Loss: 0.00000856
Iteration 187/1000 | Loss: 0.00000856
Iteration 188/1000 | Loss: 0.00000856
Iteration 189/1000 | Loss: 0.00000856
Iteration 190/1000 | Loss: 0.00000856
Iteration 191/1000 | Loss: 0.00000856
Iteration 192/1000 | Loss: 0.00000856
Iteration 193/1000 | Loss: 0.00000856
Iteration 194/1000 | Loss: 0.00000855
Iteration 195/1000 | Loss: 0.00000855
Iteration 196/1000 | Loss: 0.00000855
Iteration 197/1000 | Loss: 0.00000855
Iteration 198/1000 | Loss: 0.00000855
Iteration 199/1000 | Loss: 0.00000855
Iteration 200/1000 | Loss: 0.00000855
Iteration 201/1000 | Loss: 0.00000855
Iteration 202/1000 | Loss: 0.00000855
Iteration 203/1000 | Loss: 0.00000855
Iteration 204/1000 | Loss: 0.00000855
Iteration 205/1000 | Loss: 0.00000855
Iteration 206/1000 | Loss: 0.00000855
Iteration 207/1000 | Loss: 0.00000855
Iteration 208/1000 | Loss: 0.00000854
Iteration 209/1000 | Loss: 0.00000854
Iteration 210/1000 | Loss: 0.00000854
Iteration 211/1000 | Loss: 0.00000854
Iteration 212/1000 | Loss: 0.00000854
Iteration 213/1000 | Loss: 0.00000854
Iteration 214/1000 | Loss: 0.00000854
Iteration 215/1000 | Loss: 0.00000854
Iteration 216/1000 | Loss: 0.00000854
Iteration 217/1000 | Loss: 0.00000854
Iteration 218/1000 | Loss: 0.00000854
Iteration 219/1000 | Loss: 0.00000854
Iteration 220/1000 | Loss: 0.00000854
Iteration 221/1000 | Loss: 0.00000854
Iteration 222/1000 | Loss: 0.00000853
Iteration 223/1000 | Loss: 0.00000853
Iteration 224/1000 | Loss: 0.00000853
Iteration 225/1000 | Loss: 0.00000853
Iteration 226/1000 | Loss: 0.00000853
Iteration 227/1000 | Loss: 0.00000853
Iteration 228/1000 | Loss: 0.00000853
Iteration 229/1000 | Loss: 0.00000853
Iteration 230/1000 | Loss: 0.00000853
Iteration 231/1000 | Loss: 0.00000853
Iteration 232/1000 | Loss: 0.00000853
Iteration 233/1000 | Loss: 0.00000853
Iteration 234/1000 | Loss: 0.00000853
Iteration 235/1000 | Loss: 0.00000853
Iteration 236/1000 | Loss: 0.00000853
Iteration 237/1000 | Loss: 0.00000853
Iteration 238/1000 | Loss: 0.00000853
Iteration 239/1000 | Loss: 0.00000853
Iteration 240/1000 | Loss: 0.00000853
Iteration 241/1000 | Loss: 0.00000853
Iteration 242/1000 | Loss: 0.00000852
Iteration 243/1000 | Loss: 0.00000852
Iteration 244/1000 | Loss: 0.00000852
Iteration 245/1000 | Loss: 0.00000852
Iteration 246/1000 | Loss: 0.00000852
Iteration 247/1000 | Loss: 0.00000852
Iteration 248/1000 | Loss: 0.00000852
Iteration 249/1000 | Loss: 0.00000852
Iteration 250/1000 | Loss: 0.00000852
Iteration 251/1000 | Loss: 0.00000852
Iteration 252/1000 | Loss: 0.00000852
Iteration 253/1000 | Loss: 0.00000852
Iteration 254/1000 | Loss: 0.00000852
Iteration 255/1000 | Loss: 0.00000852
Iteration 256/1000 | Loss: 0.00000852
Iteration 257/1000 | Loss: 0.00000852
Iteration 258/1000 | Loss: 0.00000851
Iteration 259/1000 | Loss: 0.00000851
Iteration 260/1000 | Loss: 0.00000851
Iteration 261/1000 | Loss: 0.00000851
Iteration 262/1000 | Loss: 0.00000851
Iteration 263/1000 | Loss: 0.00000851
Iteration 264/1000 | Loss: 0.00000851
Iteration 265/1000 | Loss: 0.00000851
Iteration 266/1000 | Loss: 0.00000851
Iteration 267/1000 | Loss: 0.00000851
Iteration 268/1000 | Loss: 0.00000851
Iteration 269/1000 | Loss: 0.00000851
Iteration 270/1000 | Loss: 0.00000851
Iteration 271/1000 | Loss: 0.00000851
Iteration 272/1000 | Loss: 0.00000851
Iteration 273/1000 | Loss: 0.00000851
Iteration 274/1000 | Loss: 0.00000851
Iteration 275/1000 | Loss: 0.00000850
Iteration 276/1000 | Loss: 0.00000850
Iteration 277/1000 | Loss: 0.00000850
Iteration 278/1000 | Loss: 0.00000850
Iteration 279/1000 | Loss: 0.00000850
Iteration 280/1000 | Loss: 0.00000850
Iteration 281/1000 | Loss: 0.00000850
Iteration 282/1000 | Loss: 0.00000850
Iteration 283/1000 | Loss: 0.00000850
Iteration 284/1000 | Loss: 0.00000850
Iteration 285/1000 | Loss: 0.00000850
Iteration 286/1000 | Loss: 0.00000850
Iteration 287/1000 | Loss: 0.00000850
Iteration 288/1000 | Loss: 0.00000850
Iteration 289/1000 | Loss: 0.00000849
Iteration 290/1000 | Loss: 0.00000849
Iteration 291/1000 | Loss: 0.00000849
Iteration 292/1000 | Loss: 0.00000849
Iteration 293/1000 | Loss: 0.00000849
Iteration 294/1000 | Loss: 0.00000849
Iteration 295/1000 | Loss: 0.00000849
Iteration 296/1000 | Loss: 0.00000849
Iteration 297/1000 | Loss: 0.00000849
Iteration 298/1000 | Loss: 0.00000849
Iteration 299/1000 | Loss: 0.00000849
Iteration 300/1000 | Loss: 0.00000849
Iteration 301/1000 | Loss: 0.00000849
Iteration 302/1000 | Loss: 0.00000848
Iteration 303/1000 | Loss: 0.00000848
Iteration 304/1000 | Loss: 0.00000848
Iteration 305/1000 | Loss: 0.00000848
Iteration 306/1000 | Loss: 0.00000848
Iteration 307/1000 | Loss: 0.00000848
Iteration 308/1000 | Loss: 0.00000848
Iteration 309/1000 | Loss: 0.00000848
Iteration 310/1000 | Loss: 0.00000848
Iteration 311/1000 | Loss: 0.00000848
Iteration 312/1000 | Loss: 0.00000848
Iteration 313/1000 | Loss: 0.00000848
Iteration 314/1000 | Loss: 0.00000848
Iteration 315/1000 | Loss: 0.00000848
Iteration 316/1000 | Loss: 0.00000848
Iteration 317/1000 | Loss: 0.00000848
Iteration 318/1000 | Loss: 0.00000848
Iteration 319/1000 | Loss: 0.00000848
Iteration 320/1000 | Loss: 0.00000848
Iteration 321/1000 | Loss: 0.00000848
Iteration 322/1000 | Loss: 0.00000848
Iteration 323/1000 | Loss: 0.00000848
Iteration 324/1000 | Loss: 0.00000848
Iteration 325/1000 | Loss: 0.00000848
Iteration 326/1000 | Loss: 0.00000848
Iteration 327/1000 | Loss: 0.00000848
Iteration 328/1000 | Loss: 0.00000848
Iteration 329/1000 | Loss: 0.00000848
Iteration 330/1000 | Loss: 0.00000848
Iteration 331/1000 | Loss: 0.00000848
Iteration 332/1000 | Loss: 0.00000848
Iteration 333/1000 | Loss: 0.00000848
Iteration 334/1000 | Loss: 0.00000848
Iteration 335/1000 | Loss: 0.00000848
Iteration 336/1000 | Loss: 0.00000848
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 336. Stopping optimization.
Last 5 losses: [8.47528463054914e-06, 8.47528463054914e-06, 8.47528463054914e-06, 8.47528463054914e-06, 8.47528463054914e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.47528463054914e-06

Optimization complete. Final v2v error: 2.4331400394439697 mm

Highest mean error: 3.1118717193603516 mm for frame 47

Lowest mean error: 2.2361249923706055 mm for frame 24

Saving results

Total time: 45.24998950958252
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_grace_posed_004/1013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_grace_posed_004/1013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_grace_posed_004/1013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00388867
Iteration 2/25 | Loss: 0.00107211
Iteration 3/25 | Loss: 0.00099391
Iteration 4/25 | Loss: 0.00098271
Iteration 5/25 | Loss: 0.00097943
Iteration 6/25 | Loss: 0.00097837
Iteration 7/25 | Loss: 0.00097837
Iteration 8/25 | Loss: 0.00097837
Iteration 9/25 | Loss: 0.00097837
Iteration 10/25 | Loss: 0.00097837
Iteration 11/25 | Loss: 0.00097837
Iteration 12/25 | Loss: 0.00097837
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.000978370662778616, 0.000978370662778616, 0.000978370662778616, 0.000978370662778616, 0.000978370662778616]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000978370662778616

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.23972631
Iteration 2/25 | Loss: 0.00104701
Iteration 3/25 | Loss: 0.00104701
Iteration 4/25 | Loss: 0.00104701
Iteration 5/25 | Loss: 0.00104701
Iteration 6/25 | Loss: 0.00104701
Iteration 7/25 | Loss: 0.00104701
Iteration 8/25 | Loss: 0.00104701
Iteration 9/25 | Loss: 0.00104701
Iteration 10/25 | Loss: 0.00104701
Iteration 11/25 | Loss: 0.00104701
Iteration 12/25 | Loss: 0.00104701
Iteration 13/25 | Loss: 0.00104701
Iteration 14/25 | Loss: 0.00104701
Iteration 15/25 | Loss: 0.00104701
Iteration 16/25 | Loss: 0.00104701
Iteration 17/25 | Loss: 0.00104701
Iteration 18/25 | Loss: 0.00104701
Iteration 19/25 | Loss: 0.00104701
Iteration 20/25 | Loss: 0.00104701
Iteration 21/25 | Loss: 0.00104701
Iteration 22/25 | Loss: 0.00104701
Iteration 23/25 | Loss: 0.00104701
Iteration 24/25 | Loss: 0.00104701
Iteration 25/25 | Loss: 0.00104701
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0010470066918060184, 0.0010470066918060184, 0.0010470066918060184, 0.0010470066918060184, 0.0010470066918060184]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010470066918060184

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00104701
Iteration 2/1000 | Loss: 0.00001567
Iteration 3/1000 | Loss: 0.00001086
Iteration 4/1000 | Loss: 0.00000975
Iteration 5/1000 | Loss: 0.00000913
Iteration 6/1000 | Loss: 0.00000868
Iteration 7/1000 | Loss: 0.00000840
Iteration 8/1000 | Loss: 0.00000831
Iteration 9/1000 | Loss: 0.00000812
Iteration 10/1000 | Loss: 0.00000806
Iteration 11/1000 | Loss: 0.00000802
Iteration 12/1000 | Loss: 0.00000799
Iteration 13/1000 | Loss: 0.00000798
Iteration 14/1000 | Loss: 0.00000796
Iteration 15/1000 | Loss: 0.00000795
Iteration 16/1000 | Loss: 0.00000794
Iteration 17/1000 | Loss: 0.00000794
Iteration 18/1000 | Loss: 0.00000788
Iteration 19/1000 | Loss: 0.00000787
Iteration 20/1000 | Loss: 0.00000785
Iteration 21/1000 | Loss: 0.00000783
Iteration 22/1000 | Loss: 0.00000782
Iteration 23/1000 | Loss: 0.00000781
Iteration 24/1000 | Loss: 0.00000781
Iteration 25/1000 | Loss: 0.00000780
Iteration 26/1000 | Loss: 0.00000779
Iteration 27/1000 | Loss: 0.00000779
Iteration 28/1000 | Loss: 0.00000778
Iteration 29/1000 | Loss: 0.00000777
Iteration 30/1000 | Loss: 0.00000776
Iteration 31/1000 | Loss: 0.00000775
Iteration 32/1000 | Loss: 0.00000774
Iteration 33/1000 | Loss: 0.00000774
Iteration 34/1000 | Loss: 0.00000774
Iteration 35/1000 | Loss: 0.00000774
Iteration 36/1000 | Loss: 0.00000774
Iteration 37/1000 | Loss: 0.00000774
Iteration 38/1000 | Loss: 0.00000773
Iteration 39/1000 | Loss: 0.00000773
Iteration 40/1000 | Loss: 0.00000773
Iteration 41/1000 | Loss: 0.00000772
Iteration 42/1000 | Loss: 0.00000771
Iteration 43/1000 | Loss: 0.00000770
Iteration 44/1000 | Loss: 0.00000770
Iteration 45/1000 | Loss: 0.00000770
Iteration 46/1000 | Loss: 0.00000769
Iteration 47/1000 | Loss: 0.00000769
Iteration 48/1000 | Loss: 0.00000769
Iteration 49/1000 | Loss: 0.00000769
Iteration 50/1000 | Loss: 0.00000769
Iteration 51/1000 | Loss: 0.00000768
Iteration 52/1000 | Loss: 0.00000768
Iteration 53/1000 | Loss: 0.00000768
Iteration 54/1000 | Loss: 0.00000768
Iteration 55/1000 | Loss: 0.00000767
Iteration 56/1000 | Loss: 0.00000767
Iteration 57/1000 | Loss: 0.00000766
Iteration 58/1000 | Loss: 0.00000766
Iteration 59/1000 | Loss: 0.00000765
Iteration 60/1000 | Loss: 0.00000765
Iteration 61/1000 | Loss: 0.00000765
Iteration 62/1000 | Loss: 0.00000764
Iteration 63/1000 | Loss: 0.00000764
Iteration 64/1000 | Loss: 0.00000764
Iteration 65/1000 | Loss: 0.00000764
Iteration 66/1000 | Loss: 0.00000763
Iteration 67/1000 | Loss: 0.00000763
Iteration 68/1000 | Loss: 0.00000761
Iteration 69/1000 | Loss: 0.00000761
Iteration 70/1000 | Loss: 0.00000761
Iteration 71/1000 | Loss: 0.00000761
Iteration 72/1000 | Loss: 0.00000760
Iteration 73/1000 | Loss: 0.00000760
Iteration 74/1000 | Loss: 0.00000760
Iteration 75/1000 | Loss: 0.00000760
Iteration 76/1000 | Loss: 0.00000760
Iteration 77/1000 | Loss: 0.00000760
Iteration 78/1000 | Loss: 0.00000760
Iteration 79/1000 | Loss: 0.00000760
Iteration 80/1000 | Loss: 0.00000760
Iteration 81/1000 | Loss: 0.00000760
Iteration 82/1000 | Loss: 0.00000759
Iteration 83/1000 | Loss: 0.00000759
Iteration 84/1000 | Loss: 0.00000759
Iteration 85/1000 | Loss: 0.00000758
Iteration 86/1000 | Loss: 0.00000758
Iteration 87/1000 | Loss: 0.00000757
Iteration 88/1000 | Loss: 0.00000757
Iteration 89/1000 | Loss: 0.00000757
Iteration 90/1000 | Loss: 0.00000757
Iteration 91/1000 | Loss: 0.00000756
Iteration 92/1000 | Loss: 0.00000756
Iteration 93/1000 | Loss: 0.00000756
Iteration 94/1000 | Loss: 0.00000756
Iteration 95/1000 | Loss: 0.00000756
Iteration 96/1000 | Loss: 0.00000756
Iteration 97/1000 | Loss: 0.00000756
Iteration 98/1000 | Loss: 0.00000755
Iteration 99/1000 | Loss: 0.00000755
Iteration 100/1000 | Loss: 0.00000755
Iteration 101/1000 | Loss: 0.00000754
Iteration 102/1000 | Loss: 0.00000754
Iteration 103/1000 | Loss: 0.00000754
Iteration 104/1000 | Loss: 0.00000753
Iteration 105/1000 | Loss: 0.00000753
Iteration 106/1000 | Loss: 0.00000752
Iteration 107/1000 | Loss: 0.00000752
Iteration 108/1000 | Loss: 0.00000752
Iteration 109/1000 | Loss: 0.00000752
Iteration 110/1000 | Loss: 0.00000752
Iteration 111/1000 | Loss: 0.00000752
Iteration 112/1000 | Loss: 0.00000751
Iteration 113/1000 | Loss: 0.00000751
Iteration 114/1000 | Loss: 0.00000751
Iteration 115/1000 | Loss: 0.00000751
Iteration 116/1000 | Loss: 0.00000751
Iteration 117/1000 | Loss: 0.00000751
Iteration 118/1000 | Loss: 0.00000750
Iteration 119/1000 | Loss: 0.00000750
Iteration 120/1000 | Loss: 0.00000750
Iteration 121/1000 | Loss: 0.00000750
Iteration 122/1000 | Loss: 0.00000750
Iteration 123/1000 | Loss: 0.00000749
Iteration 124/1000 | Loss: 0.00000749
Iteration 125/1000 | Loss: 0.00000749
Iteration 126/1000 | Loss: 0.00000749
Iteration 127/1000 | Loss: 0.00000748
Iteration 128/1000 | Loss: 0.00000748
Iteration 129/1000 | Loss: 0.00000748
Iteration 130/1000 | Loss: 0.00000748
Iteration 131/1000 | Loss: 0.00000748
Iteration 132/1000 | Loss: 0.00000747
Iteration 133/1000 | Loss: 0.00000747
Iteration 134/1000 | Loss: 0.00000747
Iteration 135/1000 | Loss: 0.00000747
Iteration 136/1000 | Loss: 0.00000747
Iteration 137/1000 | Loss: 0.00000746
Iteration 138/1000 | Loss: 0.00000746
Iteration 139/1000 | Loss: 0.00000746
Iteration 140/1000 | Loss: 0.00000746
Iteration 141/1000 | Loss: 0.00000746
Iteration 142/1000 | Loss: 0.00000746
Iteration 143/1000 | Loss: 0.00000746
Iteration 144/1000 | Loss: 0.00000745
Iteration 145/1000 | Loss: 0.00000745
Iteration 146/1000 | Loss: 0.00000745
Iteration 147/1000 | Loss: 0.00000745
Iteration 148/1000 | Loss: 0.00000745
Iteration 149/1000 | Loss: 0.00000745
Iteration 150/1000 | Loss: 0.00000745
Iteration 151/1000 | Loss: 0.00000744
Iteration 152/1000 | Loss: 0.00000744
Iteration 153/1000 | Loss: 0.00000744
Iteration 154/1000 | Loss: 0.00000744
Iteration 155/1000 | Loss: 0.00000744
Iteration 156/1000 | Loss: 0.00000744
Iteration 157/1000 | Loss: 0.00000744
Iteration 158/1000 | Loss: 0.00000744
Iteration 159/1000 | Loss: 0.00000744
Iteration 160/1000 | Loss: 0.00000743
Iteration 161/1000 | Loss: 0.00000743
Iteration 162/1000 | Loss: 0.00000743
Iteration 163/1000 | Loss: 0.00000743
Iteration 164/1000 | Loss: 0.00000743
Iteration 165/1000 | Loss: 0.00000743
Iteration 166/1000 | Loss: 0.00000743
Iteration 167/1000 | Loss: 0.00000743
Iteration 168/1000 | Loss: 0.00000743
Iteration 169/1000 | Loss: 0.00000743
Iteration 170/1000 | Loss: 0.00000743
Iteration 171/1000 | Loss: 0.00000743
Iteration 172/1000 | Loss: 0.00000743
Iteration 173/1000 | Loss: 0.00000743
Iteration 174/1000 | Loss: 0.00000743
Iteration 175/1000 | Loss: 0.00000743
Iteration 176/1000 | Loss: 0.00000743
Iteration 177/1000 | Loss: 0.00000743
Iteration 178/1000 | Loss: 0.00000742
Iteration 179/1000 | Loss: 0.00000742
Iteration 180/1000 | Loss: 0.00000742
Iteration 181/1000 | Loss: 0.00000742
Iteration 182/1000 | Loss: 0.00000742
Iteration 183/1000 | Loss: 0.00000742
Iteration 184/1000 | Loss: 0.00000742
Iteration 185/1000 | Loss: 0.00000742
Iteration 186/1000 | Loss: 0.00000742
Iteration 187/1000 | Loss: 0.00000742
Iteration 188/1000 | Loss: 0.00000742
Iteration 189/1000 | Loss: 0.00000742
Iteration 190/1000 | Loss: 0.00000742
Iteration 191/1000 | Loss: 0.00000742
Iteration 192/1000 | Loss: 0.00000742
Iteration 193/1000 | Loss: 0.00000742
Iteration 194/1000 | Loss: 0.00000741
Iteration 195/1000 | Loss: 0.00000741
Iteration 196/1000 | Loss: 0.00000741
Iteration 197/1000 | Loss: 0.00000741
Iteration 198/1000 | Loss: 0.00000741
Iteration 199/1000 | Loss: 0.00000741
Iteration 200/1000 | Loss: 0.00000741
Iteration 201/1000 | Loss: 0.00000741
Iteration 202/1000 | Loss: 0.00000741
Iteration 203/1000 | Loss: 0.00000741
Iteration 204/1000 | Loss: 0.00000741
Iteration 205/1000 | Loss: 0.00000741
Iteration 206/1000 | Loss: 0.00000741
Iteration 207/1000 | Loss: 0.00000741
Iteration 208/1000 | Loss: 0.00000741
Iteration 209/1000 | Loss: 0.00000741
Iteration 210/1000 | Loss: 0.00000741
Iteration 211/1000 | Loss: 0.00000741
Iteration 212/1000 | Loss: 0.00000741
Iteration 213/1000 | Loss: 0.00000741
Iteration 214/1000 | Loss: 0.00000741
Iteration 215/1000 | Loss: 0.00000741
Iteration 216/1000 | Loss: 0.00000741
Iteration 217/1000 | Loss: 0.00000741
Iteration 218/1000 | Loss: 0.00000741
Iteration 219/1000 | Loss: 0.00000741
Iteration 220/1000 | Loss: 0.00000741
Iteration 221/1000 | Loss: 0.00000741
Iteration 222/1000 | Loss: 0.00000741
Iteration 223/1000 | Loss: 0.00000741
Iteration 224/1000 | Loss: 0.00000741
Iteration 225/1000 | Loss: 0.00000741
Iteration 226/1000 | Loss: 0.00000741
Iteration 227/1000 | Loss: 0.00000741
Iteration 228/1000 | Loss: 0.00000741
Iteration 229/1000 | Loss: 0.00000741
Iteration 230/1000 | Loss: 0.00000741
Iteration 231/1000 | Loss: 0.00000741
Iteration 232/1000 | Loss: 0.00000741
Iteration 233/1000 | Loss: 0.00000741
Iteration 234/1000 | Loss: 0.00000741
Iteration 235/1000 | Loss: 0.00000741
Iteration 236/1000 | Loss: 0.00000741
Iteration 237/1000 | Loss: 0.00000741
Iteration 238/1000 | Loss: 0.00000741
Iteration 239/1000 | Loss: 0.00000741
Iteration 240/1000 | Loss: 0.00000741
Iteration 241/1000 | Loss: 0.00000741
Iteration 242/1000 | Loss: 0.00000741
Iteration 243/1000 | Loss: 0.00000741
Iteration 244/1000 | Loss: 0.00000741
Iteration 245/1000 | Loss: 0.00000741
Iteration 246/1000 | Loss: 0.00000741
Iteration 247/1000 | Loss: 0.00000741
Iteration 248/1000 | Loss: 0.00000741
Iteration 249/1000 | Loss: 0.00000741
Iteration 250/1000 | Loss: 0.00000741
Iteration 251/1000 | Loss: 0.00000741
Iteration 252/1000 | Loss: 0.00000741
Iteration 253/1000 | Loss: 0.00000741
Iteration 254/1000 | Loss: 0.00000741
Iteration 255/1000 | Loss: 0.00000741
Iteration 256/1000 | Loss: 0.00000741
Iteration 257/1000 | Loss: 0.00000741
Iteration 258/1000 | Loss: 0.00000741
Iteration 259/1000 | Loss: 0.00000741
Iteration 260/1000 | Loss: 0.00000741
Iteration 261/1000 | Loss: 0.00000741
Iteration 262/1000 | Loss: 0.00000741
Iteration 263/1000 | Loss: 0.00000741
Iteration 264/1000 | Loss: 0.00000741
Iteration 265/1000 | Loss: 0.00000741
Iteration 266/1000 | Loss: 0.00000741
Iteration 267/1000 | Loss: 0.00000741
Iteration 268/1000 | Loss: 0.00000741
Iteration 269/1000 | Loss: 0.00000741
Iteration 270/1000 | Loss: 0.00000741
Iteration 271/1000 | Loss: 0.00000741
Iteration 272/1000 | Loss: 0.00000741
Iteration 273/1000 | Loss: 0.00000741
Iteration 274/1000 | Loss: 0.00000741
Iteration 275/1000 | Loss: 0.00000741
Iteration 276/1000 | Loss: 0.00000741
Iteration 277/1000 | Loss: 0.00000741
Iteration 278/1000 | Loss: 0.00000741
Iteration 279/1000 | Loss: 0.00000741
Iteration 280/1000 | Loss: 0.00000741
Iteration 281/1000 | Loss: 0.00000741
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 281. Stopping optimization.
Last 5 losses: [7.405585620290367e-06, 7.405585620290367e-06, 7.405585620290367e-06, 7.405585620290367e-06, 7.405585620290367e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 7.405585620290367e-06

Optimization complete. Final v2v error: 2.366680383682251 mm

Highest mean error: 2.5333120822906494 mm for frame 82

Lowest mean error: 2.221230983734131 mm for frame 0

Saving results

Total time: 40.55046534538269
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_grace_posed_004/1020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_grace_posed_004/1020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_grace_posed_004/1020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00801599
Iteration 2/25 | Loss: 0.00112479
Iteration 3/25 | Loss: 0.00100582
Iteration 4/25 | Loss: 0.00099259
Iteration 5/25 | Loss: 0.00099006
Iteration 6/25 | Loss: 0.00099003
Iteration 7/25 | Loss: 0.00099003
Iteration 8/25 | Loss: 0.00099003
Iteration 9/25 | Loss: 0.00099003
Iteration 10/25 | Loss: 0.00099003
Iteration 11/25 | Loss: 0.00099003
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.000990033382549882, 0.000990033382549882, 0.000990033382549882, 0.000990033382549882, 0.000990033382549882]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000990033382549882

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.31504428
Iteration 2/25 | Loss: 0.00102494
Iteration 3/25 | Loss: 0.00102494
Iteration 4/25 | Loss: 0.00102494
Iteration 5/25 | Loss: 0.00102494
Iteration 6/25 | Loss: 0.00102493
Iteration 7/25 | Loss: 0.00102493
Iteration 8/25 | Loss: 0.00102493
Iteration 9/25 | Loss: 0.00102493
Iteration 10/25 | Loss: 0.00102493
Iteration 11/25 | Loss: 0.00102493
Iteration 12/25 | Loss: 0.00102493
Iteration 13/25 | Loss: 0.00102493
Iteration 14/25 | Loss: 0.00102493
Iteration 15/25 | Loss: 0.00102493
Iteration 16/25 | Loss: 0.00102493
Iteration 17/25 | Loss: 0.00102493
Iteration 18/25 | Loss: 0.00102493
Iteration 19/25 | Loss: 0.00102493
Iteration 20/25 | Loss: 0.00102493
Iteration 21/25 | Loss: 0.00102493
Iteration 22/25 | Loss: 0.00102493
Iteration 23/25 | Loss: 0.00102493
Iteration 24/25 | Loss: 0.00102493
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.001024932716973126, 0.001024932716973126, 0.001024932716973126, 0.001024932716973126, 0.001024932716973126]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001024932716973126

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00102493
Iteration 2/1000 | Loss: 0.00001969
Iteration 3/1000 | Loss: 0.00001140
Iteration 4/1000 | Loss: 0.00001004
Iteration 5/1000 | Loss: 0.00000932
Iteration 6/1000 | Loss: 0.00000884
Iteration 7/1000 | Loss: 0.00000847
Iteration 8/1000 | Loss: 0.00000822
Iteration 9/1000 | Loss: 0.00000806
Iteration 10/1000 | Loss: 0.00000781
Iteration 11/1000 | Loss: 0.00000779
Iteration 12/1000 | Loss: 0.00000770
Iteration 13/1000 | Loss: 0.00000762
Iteration 14/1000 | Loss: 0.00000756
Iteration 15/1000 | Loss: 0.00000755
Iteration 16/1000 | Loss: 0.00000755
Iteration 17/1000 | Loss: 0.00000754
Iteration 18/1000 | Loss: 0.00000754
Iteration 19/1000 | Loss: 0.00000754
Iteration 20/1000 | Loss: 0.00000754
Iteration 21/1000 | Loss: 0.00000753
Iteration 22/1000 | Loss: 0.00000751
Iteration 23/1000 | Loss: 0.00000751
Iteration 24/1000 | Loss: 0.00000751
Iteration 25/1000 | Loss: 0.00000750
Iteration 26/1000 | Loss: 0.00000749
Iteration 27/1000 | Loss: 0.00000748
Iteration 28/1000 | Loss: 0.00000748
Iteration 29/1000 | Loss: 0.00000747
Iteration 30/1000 | Loss: 0.00000747
Iteration 31/1000 | Loss: 0.00000747
Iteration 32/1000 | Loss: 0.00000747
Iteration 33/1000 | Loss: 0.00000746
Iteration 34/1000 | Loss: 0.00000746
Iteration 35/1000 | Loss: 0.00000746
Iteration 36/1000 | Loss: 0.00000746
Iteration 37/1000 | Loss: 0.00000745
Iteration 38/1000 | Loss: 0.00000745
Iteration 39/1000 | Loss: 0.00000745
Iteration 40/1000 | Loss: 0.00000744
Iteration 41/1000 | Loss: 0.00000744
Iteration 42/1000 | Loss: 0.00000743
Iteration 43/1000 | Loss: 0.00000743
Iteration 44/1000 | Loss: 0.00000742
Iteration 45/1000 | Loss: 0.00000742
Iteration 46/1000 | Loss: 0.00000742
Iteration 47/1000 | Loss: 0.00000741
Iteration 48/1000 | Loss: 0.00000741
Iteration 49/1000 | Loss: 0.00000740
Iteration 50/1000 | Loss: 0.00000740
Iteration 51/1000 | Loss: 0.00000740
Iteration 52/1000 | Loss: 0.00000739
Iteration 53/1000 | Loss: 0.00000737
Iteration 54/1000 | Loss: 0.00000736
Iteration 55/1000 | Loss: 0.00000736
Iteration 56/1000 | Loss: 0.00000736
Iteration 57/1000 | Loss: 0.00000735
Iteration 58/1000 | Loss: 0.00000735
Iteration 59/1000 | Loss: 0.00000733
Iteration 60/1000 | Loss: 0.00000732
Iteration 61/1000 | Loss: 0.00000731
Iteration 62/1000 | Loss: 0.00000730
Iteration 63/1000 | Loss: 0.00000730
Iteration 64/1000 | Loss: 0.00000730
Iteration 65/1000 | Loss: 0.00000729
Iteration 66/1000 | Loss: 0.00000729
Iteration 67/1000 | Loss: 0.00000728
Iteration 68/1000 | Loss: 0.00000728
Iteration 69/1000 | Loss: 0.00000727
Iteration 70/1000 | Loss: 0.00000727
Iteration 71/1000 | Loss: 0.00000727
Iteration 72/1000 | Loss: 0.00000727
Iteration 73/1000 | Loss: 0.00000726
Iteration 74/1000 | Loss: 0.00000726
Iteration 75/1000 | Loss: 0.00000726
Iteration 76/1000 | Loss: 0.00000726
Iteration 77/1000 | Loss: 0.00000726
Iteration 78/1000 | Loss: 0.00000726
Iteration 79/1000 | Loss: 0.00000726
Iteration 80/1000 | Loss: 0.00000726
Iteration 81/1000 | Loss: 0.00000726
Iteration 82/1000 | Loss: 0.00000726
Iteration 83/1000 | Loss: 0.00000725
Iteration 84/1000 | Loss: 0.00000725
Iteration 85/1000 | Loss: 0.00000725
Iteration 86/1000 | Loss: 0.00000725
Iteration 87/1000 | Loss: 0.00000724
Iteration 88/1000 | Loss: 0.00000724
Iteration 89/1000 | Loss: 0.00000724
Iteration 90/1000 | Loss: 0.00000724
Iteration 91/1000 | Loss: 0.00000723
Iteration 92/1000 | Loss: 0.00000723
Iteration 93/1000 | Loss: 0.00000723
Iteration 94/1000 | Loss: 0.00000723
Iteration 95/1000 | Loss: 0.00000723
Iteration 96/1000 | Loss: 0.00000723
Iteration 97/1000 | Loss: 0.00000723
Iteration 98/1000 | Loss: 0.00000723
Iteration 99/1000 | Loss: 0.00000723
Iteration 100/1000 | Loss: 0.00000722
Iteration 101/1000 | Loss: 0.00000722
Iteration 102/1000 | Loss: 0.00000722
Iteration 103/1000 | Loss: 0.00000721
Iteration 104/1000 | Loss: 0.00000721
Iteration 105/1000 | Loss: 0.00000721
Iteration 106/1000 | Loss: 0.00000720
Iteration 107/1000 | Loss: 0.00000720
Iteration 108/1000 | Loss: 0.00000720
Iteration 109/1000 | Loss: 0.00000720
Iteration 110/1000 | Loss: 0.00000720
Iteration 111/1000 | Loss: 0.00000720
Iteration 112/1000 | Loss: 0.00000719
Iteration 113/1000 | Loss: 0.00000719
Iteration 114/1000 | Loss: 0.00000719
Iteration 115/1000 | Loss: 0.00000719
Iteration 116/1000 | Loss: 0.00000719
Iteration 117/1000 | Loss: 0.00000719
Iteration 118/1000 | Loss: 0.00000718
Iteration 119/1000 | Loss: 0.00000718
Iteration 120/1000 | Loss: 0.00000718
Iteration 121/1000 | Loss: 0.00000718
Iteration 122/1000 | Loss: 0.00000718
Iteration 123/1000 | Loss: 0.00000718
Iteration 124/1000 | Loss: 0.00000718
Iteration 125/1000 | Loss: 0.00000718
Iteration 126/1000 | Loss: 0.00000718
Iteration 127/1000 | Loss: 0.00000718
Iteration 128/1000 | Loss: 0.00000718
Iteration 129/1000 | Loss: 0.00000718
Iteration 130/1000 | Loss: 0.00000718
Iteration 131/1000 | Loss: 0.00000718
Iteration 132/1000 | Loss: 0.00000718
Iteration 133/1000 | Loss: 0.00000718
Iteration 134/1000 | Loss: 0.00000718
Iteration 135/1000 | Loss: 0.00000718
Iteration 136/1000 | Loss: 0.00000718
Iteration 137/1000 | Loss: 0.00000718
Iteration 138/1000 | Loss: 0.00000718
Iteration 139/1000 | Loss: 0.00000718
Iteration 140/1000 | Loss: 0.00000718
Iteration 141/1000 | Loss: 0.00000718
Iteration 142/1000 | Loss: 0.00000718
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 142. Stopping optimization.
Last 5 losses: [7.177570296335034e-06, 7.177570296335034e-06, 7.177570296335034e-06, 7.177570296335034e-06, 7.177570296335034e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 7.177570296335034e-06

Optimization complete. Final v2v error: 2.2868402004241943 mm

Highest mean error: 2.6189188957214355 mm for frame 55

Lowest mean error: 2.152935266494751 mm for frame 181

Saving results

Total time: 36.39580035209656
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_grace_posed_004/1009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_grace_posed_004/1009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_grace_posed_004/1009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00860083
Iteration 2/25 | Loss: 0.00110290
Iteration 3/25 | Loss: 0.00101571
Iteration 4/25 | Loss: 0.00100552
Iteration 5/25 | Loss: 0.00100254
Iteration 6/25 | Loss: 0.00100172
Iteration 7/25 | Loss: 0.00100172
Iteration 8/25 | Loss: 0.00100172
Iteration 9/25 | Loss: 0.00100172
Iteration 10/25 | Loss: 0.00100172
Iteration 11/25 | Loss: 0.00100172
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010017183376476169, 0.0010017183376476169, 0.0010017183376476169, 0.0010017183376476169, 0.0010017183376476169]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010017183376476169

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.96124613
Iteration 2/25 | Loss: 0.00103854
Iteration 3/25 | Loss: 0.00103854
Iteration 4/25 | Loss: 0.00103854
Iteration 5/25 | Loss: 0.00103853
Iteration 6/25 | Loss: 0.00103853
Iteration 7/25 | Loss: 0.00103853
Iteration 8/25 | Loss: 0.00103853
Iteration 9/25 | Loss: 0.00103853
Iteration 10/25 | Loss: 0.00103853
Iteration 11/25 | Loss: 0.00103853
Iteration 12/25 | Loss: 0.00103853
Iteration 13/25 | Loss: 0.00103853
Iteration 14/25 | Loss: 0.00103853
Iteration 15/25 | Loss: 0.00103853
Iteration 16/25 | Loss: 0.00103853
Iteration 17/25 | Loss: 0.00103853
Iteration 18/25 | Loss: 0.00103853
Iteration 19/25 | Loss: 0.00103853
Iteration 20/25 | Loss: 0.00103853
Iteration 21/25 | Loss: 0.00103853
Iteration 22/25 | Loss: 0.00103853
Iteration 23/25 | Loss: 0.00103853
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0010385322384536266, 0.0010385322384536266, 0.0010385322384536266, 0.0010385322384536266, 0.0010385322384536266]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010385322384536266

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00103853
Iteration 2/1000 | Loss: 0.00001899
Iteration 3/1000 | Loss: 0.00001222
Iteration 4/1000 | Loss: 0.00001094
Iteration 5/1000 | Loss: 0.00001046
Iteration 6/1000 | Loss: 0.00001002
Iteration 7/1000 | Loss: 0.00000968
Iteration 8/1000 | Loss: 0.00000939
Iteration 9/1000 | Loss: 0.00000933
Iteration 10/1000 | Loss: 0.00000922
Iteration 11/1000 | Loss: 0.00000917
Iteration 12/1000 | Loss: 0.00000912
Iteration 13/1000 | Loss: 0.00000904
Iteration 14/1000 | Loss: 0.00000900
Iteration 15/1000 | Loss: 0.00000897
Iteration 16/1000 | Loss: 0.00000896
Iteration 17/1000 | Loss: 0.00000896
Iteration 18/1000 | Loss: 0.00000894
Iteration 19/1000 | Loss: 0.00000893
Iteration 20/1000 | Loss: 0.00000893
Iteration 21/1000 | Loss: 0.00000892
Iteration 22/1000 | Loss: 0.00000891
Iteration 23/1000 | Loss: 0.00000891
Iteration 24/1000 | Loss: 0.00000890
Iteration 25/1000 | Loss: 0.00000888
Iteration 26/1000 | Loss: 0.00000887
Iteration 27/1000 | Loss: 0.00000887
Iteration 28/1000 | Loss: 0.00000886
Iteration 29/1000 | Loss: 0.00000886
Iteration 30/1000 | Loss: 0.00000882
Iteration 31/1000 | Loss: 0.00000881
Iteration 32/1000 | Loss: 0.00000878
Iteration 33/1000 | Loss: 0.00000877
Iteration 34/1000 | Loss: 0.00000877
Iteration 35/1000 | Loss: 0.00000877
Iteration 36/1000 | Loss: 0.00000876
Iteration 37/1000 | Loss: 0.00000876
Iteration 38/1000 | Loss: 0.00000876
Iteration 39/1000 | Loss: 0.00000875
Iteration 40/1000 | Loss: 0.00000875
Iteration 41/1000 | Loss: 0.00000873
Iteration 42/1000 | Loss: 0.00000872
Iteration 43/1000 | Loss: 0.00000872
Iteration 44/1000 | Loss: 0.00000872
Iteration 45/1000 | Loss: 0.00000872
Iteration 46/1000 | Loss: 0.00000872
Iteration 47/1000 | Loss: 0.00000872
Iteration 48/1000 | Loss: 0.00000872
Iteration 49/1000 | Loss: 0.00000872
Iteration 50/1000 | Loss: 0.00000872
Iteration 51/1000 | Loss: 0.00000871
Iteration 52/1000 | Loss: 0.00000871
Iteration 53/1000 | Loss: 0.00000871
Iteration 54/1000 | Loss: 0.00000870
Iteration 55/1000 | Loss: 0.00000868
Iteration 56/1000 | Loss: 0.00000868
Iteration 57/1000 | Loss: 0.00000868
Iteration 58/1000 | Loss: 0.00000868
Iteration 59/1000 | Loss: 0.00000867
Iteration 60/1000 | Loss: 0.00000867
Iteration 61/1000 | Loss: 0.00000867
Iteration 62/1000 | Loss: 0.00000866
Iteration 63/1000 | Loss: 0.00000865
Iteration 64/1000 | Loss: 0.00000865
Iteration 65/1000 | Loss: 0.00000865
Iteration 66/1000 | Loss: 0.00000865
Iteration 67/1000 | Loss: 0.00000864
Iteration 68/1000 | Loss: 0.00000864
Iteration 69/1000 | Loss: 0.00000864
Iteration 70/1000 | Loss: 0.00000863
Iteration 71/1000 | Loss: 0.00000863
Iteration 72/1000 | Loss: 0.00000863
Iteration 73/1000 | Loss: 0.00000862
Iteration 74/1000 | Loss: 0.00000862
Iteration 75/1000 | Loss: 0.00000862
Iteration 76/1000 | Loss: 0.00000861
Iteration 77/1000 | Loss: 0.00000861
Iteration 78/1000 | Loss: 0.00000861
Iteration 79/1000 | Loss: 0.00000861
Iteration 80/1000 | Loss: 0.00000861
Iteration 81/1000 | Loss: 0.00000861
Iteration 82/1000 | Loss: 0.00000860
Iteration 83/1000 | Loss: 0.00000860
Iteration 84/1000 | Loss: 0.00000860
Iteration 85/1000 | Loss: 0.00000860
Iteration 86/1000 | Loss: 0.00000859
Iteration 87/1000 | Loss: 0.00000859
Iteration 88/1000 | Loss: 0.00000859
Iteration 89/1000 | Loss: 0.00000859
Iteration 90/1000 | Loss: 0.00000859
Iteration 91/1000 | Loss: 0.00000859
Iteration 92/1000 | Loss: 0.00000859
Iteration 93/1000 | Loss: 0.00000859
Iteration 94/1000 | Loss: 0.00000859
Iteration 95/1000 | Loss: 0.00000858
Iteration 96/1000 | Loss: 0.00000858
Iteration 97/1000 | Loss: 0.00000858
Iteration 98/1000 | Loss: 0.00000858
Iteration 99/1000 | Loss: 0.00000858
Iteration 100/1000 | Loss: 0.00000858
Iteration 101/1000 | Loss: 0.00000858
Iteration 102/1000 | Loss: 0.00000858
Iteration 103/1000 | Loss: 0.00000857
Iteration 104/1000 | Loss: 0.00000857
Iteration 105/1000 | Loss: 0.00000857
Iteration 106/1000 | Loss: 0.00000857
Iteration 107/1000 | Loss: 0.00000857
Iteration 108/1000 | Loss: 0.00000857
Iteration 109/1000 | Loss: 0.00000856
Iteration 110/1000 | Loss: 0.00000856
Iteration 111/1000 | Loss: 0.00000856
Iteration 112/1000 | Loss: 0.00000856
Iteration 113/1000 | Loss: 0.00000856
Iteration 114/1000 | Loss: 0.00000856
Iteration 115/1000 | Loss: 0.00000856
Iteration 116/1000 | Loss: 0.00000856
Iteration 117/1000 | Loss: 0.00000856
Iteration 118/1000 | Loss: 0.00000856
Iteration 119/1000 | Loss: 0.00000856
Iteration 120/1000 | Loss: 0.00000856
Iteration 121/1000 | Loss: 0.00000856
Iteration 122/1000 | Loss: 0.00000856
Iteration 123/1000 | Loss: 0.00000856
Iteration 124/1000 | Loss: 0.00000855
Iteration 125/1000 | Loss: 0.00000855
Iteration 126/1000 | Loss: 0.00000855
Iteration 127/1000 | Loss: 0.00000854
Iteration 128/1000 | Loss: 0.00000854
Iteration 129/1000 | Loss: 0.00000854
Iteration 130/1000 | Loss: 0.00000853
Iteration 131/1000 | Loss: 0.00000853
Iteration 132/1000 | Loss: 0.00000853
Iteration 133/1000 | Loss: 0.00000853
Iteration 134/1000 | Loss: 0.00000853
Iteration 135/1000 | Loss: 0.00000853
Iteration 136/1000 | Loss: 0.00000853
Iteration 137/1000 | Loss: 0.00000853
Iteration 138/1000 | Loss: 0.00000853
Iteration 139/1000 | Loss: 0.00000853
Iteration 140/1000 | Loss: 0.00000853
Iteration 141/1000 | Loss: 0.00000853
Iteration 142/1000 | Loss: 0.00000852
Iteration 143/1000 | Loss: 0.00000852
Iteration 144/1000 | Loss: 0.00000852
Iteration 145/1000 | Loss: 0.00000852
Iteration 146/1000 | Loss: 0.00000852
Iteration 147/1000 | Loss: 0.00000852
Iteration 148/1000 | Loss: 0.00000852
Iteration 149/1000 | Loss: 0.00000851
Iteration 150/1000 | Loss: 0.00000851
Iteration 151/1000 | Loss: 0.00000851
Iteration 152/1000 | Loss: 0.00000851
Iteration 153/1000 | Loss: 0.00000851
Iteration 154/1000 | Loss: 0.00000851
Iteration 155/1000 | Loss: 0.00000850
Iteration 156/1000 | Loss: 0.00000850
Iteration 157/1000 | Loss: 0.00000850
Iteration 158/1000 | Loss: 0.00000850
Iteration 159/1000 | Loss: 0.00000850
Iteration 160/1000 | Loss: 0.00000850
Iteration 161/1000 | Loss: 0.00000850
Iteration 162/1000 | Loss: 0.00000850
Iteration 163/1000 | Loss: 0.00000850
Iteration 164/1000 | Loss: 0.00000849
Iteration 165/1000 | Loss: 0.00000849
Iteration 166/1000 | Loss: 0.00000849
Iteration 167/1000 | Loss: 0.00000849
Iteration 168/1000 | Loss: 0.00000849
Iteration 169/1000 | Loss: 0.00000849
Iteration 170/1000 | Loss: 0.00000849
Iteration 171/1000 | Loss: 0.00000849
Iteration 172/1000 | Loss: 0.00000849
Iteration 173/1000 | Loss: 0.00000849
Iteration 174/1000 | Loss: 0.00000849
Iteration 175/1000 | Loss: 0.00000849
Iteration 176/1000 | Loss: 0.00000849
Iteration 177/1000 | Loss: 0.00000849
Iteration 178/1000 | Loss: 0.00000849
Iteration 179/1000 | Loss: 0.00000849
Iteration 180/1000 | Loss: 0.00000849
Iteration 181/1000 | Loss: 0.00000849
Iteration 182/1000 | Loss: 0.00000849
Iteration 183/1000 | Loss: 0.00000849
Iteration 184/1000 | Loss: 0.00000849
Iteration 185/1000 | Loss: 0.00000849
Iteration 186/1000 | Loss: 0.00000849
Iteration 187/1000 | Loss: 0.00000849
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 187. Stopping optimization.
Last 5 losses: [8.485483704134822e-06, 8.485483704134822e-06, 8.485483704134822e-06, 8.485483704134822e-06, 8.485483704134822e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.485483704134822e-06

Optimization complete. Final v2v error: 2.4309606552124023 mm

Highest mean error: 3.1893997192382812 mm for frame 53

Lowest mean error: 2.176482677459717 mm for frame 11

Saving results

Total time: 36.85602116584778
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_grace_posed_004/1037/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_grace_posed_004/1037.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_grace_posed_004/1037
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00869282
Iteration 2/25 | Loss: 0.00133018
Iteration 3/25 | Loss: 0.00110828
Iteration 4/25 | Loss: 0.00108693
Iteration 5/25 | Loss: 0.00108239
Iteration 6/25 | Loss: 0.00108118
Iteration 7/25 | Loss: 0.00108086
Iteration 8/25 | Loss: 0.00108086
Iteration 9/25 | Loss: 0.00108086
Iteration 10/25 | Loss: 0.00108086
Iteration 11/25 | Loss: 0.00108086
Iteration 12/25 | Loss: 0.00108086
Iteration 13/25 | Loss: 0.00108086
Iteration 14/25 | Loss: 0.00108086
Iteration 15/25 | Loss: 0.00108086
Iteration 16/25 | Loss: 0.00108086
Iteration 17/25 | Loss: 0.00108086
Iteration 18/25 | Loss: 0.00108086
Iteration 19/25 | Loss: 0.00108086
Iteration 20/25 | Loss: 0.00108086
Iteration 21/25 | Loss: 0.00108086
Iteration 22/25 | Loss: 0.00108086
Iteration 23/25 | Loss: 0.00108086
Iteration 24/25 | Loss: 0.00108086
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0010808559600263834, 0.0010808559600263834, 0.0010808559600263834, 0.0010808559600263834, 0.0010808559600263834]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010808559600263834

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.82468379
Iteration 2/25 | Loss: 0.00085138
Iteration 3/25 | Loss: 0.00085137
Iteration 4/25 | Loss: 0.00085137
Iteration 5/25 | Loss: 0.00085137
Iteration 6/25 | Loss: 0.00085137
Iteration 7/25 | Loss: 0.00085137
Iteration 8/25 | Loss: 0.00085137
Iteration 9/25 | Loss: 0.00085137
Iteration 10/25 | Loss: 0.00085137
Iteration 11/25 | Loss: 0.00085137
Iteration 12/25 | Loss: 0.00085137
Iteration 13/25 | Loss: 0.00085137
Iteration 14/25 | Loss: 0.00085137
Iteration 15/25 | Loss: 0.00085137
Iteration 16/25 | Loss: 0.00085137
Iteration 17/25 | Loss: 0.00085137
Iteration 18/25 | Loss: 0.00085137
Iteration 19/25 | Loss: 0.00085137
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0008513670181855559, 0.0008513670181855559, 0.0008513670181855559, 0.0008513670181855559, 0.0008513670181855559]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008513670181855559

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00085137
Iteration 2/1000 | Loss: 0.00002214
Iteration 3/1000 | Loss: 0.00001576
Iteration 4/1000 | Loss: 0.00001415
Iteration 5/1000 | Loss: 0.00001350
Iteration 6/1000 | Loss: 0.00001305
Iteration 7/1000 | Loss: 0.00001277
Iteration 8/1000 | Loss: 0.00001251
Iteration 9/1000 | Loss: 0.00001229
Iteration 10/1000 | Loss: 0.00001227
Iteration 11/1000 | Loss: 0.00001216
Iteration 12/1000 | Loss: 0.00001211
Iteration 13/1000 | Loss: 0.00001210
Iteration 14/1000 | Loss: 0.00001208
Iteration 15/1000 | Loss: 0.00001207
Iteration 16/1000 | Loss: 0.00001206
Iteration 17/1000 | Loss: 0.00001206
Iteration 18/1000 | Loss: 0.00001199
Iteration 19/1000 | Loss: 0.00001199
Iteration 20/1000 | Loss: 0.00001198
Iteration 21/1000 | Loss: 0.00001198
Iteration 22/1000 | Loss: 0.00001198
Iteration 23/1000 | Loss: 0.00001196
Iteration 24/1000 | Loss: 0.00001196
Iteration 25/1000 | Loss: 0.00001196
Iteration 26/1000 | Loss: 0.00001195
Iteration 27/1000 | Loss: 0.00001195
Iteration 28/1000 | Loss: 0.00001195
Iteration 29/1000 | Loss: 0.00001195
Iteration 30/1000 | Loss: 0.00001195
Iteration 31/1000 | Loss: 0.00001195
Iteration 32/1000 | Loss: 0.00001195
Iteration 33/1000 | Loss: 0.00001194
Iteration 34/1000 | Loss: 0.00001192
Iteration 35/1000 | Loss: 0.00001192
Iteration 36/1000 | Loss: 0.00001192
Iteration 37/1000 | Loss: 0.00001191
Iteration 38/1000 | Loss: 0.00001191
Iteration 39/1000 | Loss: 0.00001191
Iteration 40/1000 | Loss: 0.00001190
Iteration 41/1000 | Loss: 0.00001190
Iteration 42/1000 | Loss: 0.00001190
Iteration 43/1000 | Loss: 0.00001189
Iteration 44/1000 | Loss: 0.00001189
Iteration 45/1000 | Loss: 0.00001189
Iteration 46/1000 | Loss: 0.00001188
Iteration 47/1000 | Loss: 0.00001188
Iteration 48/1000 | Loss: 0.00001188
Iteration 49/1000 | Loss: 0.00001188
Iteration 50/1000 | Loss: 0.00001187
Iteration 51/1000 | Loss: 0.00001187
Iteration 52/1000 | Loss: 0.00001187
Iteration 53/1000 | Loss: 0.00001186
Iteration 54/1000 | Loss: 0.00001186
Iteration 55/1000 | Loss: 0.00001186
Iteration 56/1000 | Loss: 0.00001185
Iteration 57/1000 | Loss: 0.00001185
Iteration 58/1000 | Loss: 0.00001185
Iteration 59/1000 | Loss: 0.00001184
Iteration 60/1000 | Loss: 0.00001184
Iteration 61/1000 | Loss: 0.00001184
Iteration 62/1000 | Loss: 0.00001184
Iteration 63/1000 | Loss: 0.00001184
Iteration 64/1000 | Loss: 0.00001183
Iteration 65/1000 | Loss: 0.00001183
Iteration 66/1000 | Loss: 0.00001183
Iteration 67/1000 | Loss: 0.00001183
Iteration 68/1000 | Loss: 0.00001183
Iteration 69/1000 | Loss: 0.00001183
Iteration 70/1000 | Loss: 0.00001183
Iteration 71/1000 | Loss: 0.00001183
Iteration 72/1000 | Loss: 0.00001183
Iteration 73/1000 | Loss: 0.00001182
Iteration 74/1000 | Loss: 0.00001182
Iteration 75/1000 | Loss: 0.00001181
Iteration 76/1000 | Loss: 0.00001181
Iteration 77/1000 | Loss: 0.00001181
Iteration 78/1000 | Loss: 0.00001180
Iteration 79/1000 | Loss: 0.00001180
Iteration 80/1000 | Loss: 0.00001180
Iteration 81/1000 | Loss: 0.00001179
Iteration 82/1000 | Loss: 0.00001179
Iteration 83/1000 | Loss: 0.00001179
Iteration 84/1000 | Loss: 0.00001179
Iteration 85/1000 | Loss: 0.00001179
Iteration 86/1000 | Loss: 0.00001178
Iteration 87/1000 | Loss: 0.00001178
Iteration 88/1000 | Loss: 0.00001178
Iteration 89/1000 | Loss: 0.00001178
Iteration 90/1000 | Loss: 0.00001178
Iteration 91/1000 | Loss: 0.00001178
Iteration 92/1000 | Loss: 0.00001178
Iteration 93/1000 | Loss: 0.00001177
Iteration 94/1000 | Loss: 0.00001177
Iteration 95/1000 | Loss: 0.00001177
Iteration 96/1000 | Loss: 0.00001177
Iteration 97/1000 | Loss: 0.00001177
Iteration 98/1000 | Loss: 0.00001177
Iteration 99/1000 | Loss: 0.00001176
Iteration 100/1000 | Loss: 0.00001176
Iteration 101/1000 | Loss: 0.00001175
Iteration 102/1000 | Loss: 0.00001175
Iteration 103/1000 | Loss: 0.00001175
Iteration 104/1000 | Loss: 0.00001175
Iteration 105/1000 | Loss: 0.00001175
Iteration 106/1000 | Loss: 0.00001175
Iteration 107/1000 | Loss: 0.00001175
Iteration 108/1000 | Loss: 0.00001174
Iteration 109/1000 | Loss: 0.00001174
Iteration 110/1000 | Loss: 0.00001174
Iteration 111/1000 | Loss: 0.00001174
Iteration 112/1000 | Loss: 0.00001174
Iteration 113/1000 | Loss: 0.00001174
Iteration 114/1000 | Loss: 0.00001174
Iteration 115/1000 | Loss: 0.00001174
Iteration 116/1000 | Loss: 0.00001174
Iteration 117/1000 | Loss: 0.00001174
Iteration 118/1000 | Loss: 0.00001174
Iteration 119/1000 | Loss: 0.00001173
Iteration 120/1000 | Loss: 0.00001173
Iteration 121/1000 | Loss: 0.00001173
Iteration 122/1000 | Loss: 0.00001173
Iteration 123/1000 | Loss: 0.00001173
Iteration 124/1000 | Loss: 0.00001173
Iteration 125/1000 | Loss: 0.00001173
Iteration 126/1000 | Loss: 0.00001173
Iteration 127/1000 | Loss: 0.00001173
Iteration 128/1000 | Loss: 0.00001173
Iteration 129/1000 | Loss: 0.00001172
Iteration 130/1000 | Loss: 0.00001172
Iteration 131/1000 | Loss: 0.00001172
Iteration 132/1000 | Loss: 0.00001172
Iteration 133/1000 | Loss: 0.00001172
Iteration 134/1000 | Loss: 0.00001172
Iteration 135/1000 | Loss: 0.00001172
Iteration 136/1000 | Loss: 0.00001172
Iteration 137/1000 | Loss: 0.00001172
Iteration 138/1000 | Loss: 0.00001172
Iteration 139/1000 | Loss: 0.00001172
Iteration 140/1000 | Loss: 0.00001172
Iteration 141/1000 | Loss: 0.00001172
Iteration 142/1000 | Loss: 0.00001171
Iteration 143/1000 | Loss: 0.00001171
Iteration 144/1000 | Loss: 0.00001171
Iteration 145/1000 | Loss: 0.00001171
Iteration 146/1000 | Loss: 0.00001171
Iteration 147/1000 | Loss: 0.00001171
Iteration 148/1000 | Loss: 0.00001171
Iteration 149/1000 | Loss: 0.00001171
Iteration 150/1000 | Loss: 0.00001171
Iteration 151/1000 | Loss: 0.00001171
Iteration 152/1000 | Loss: 0.00001171
Iteration 153/1000 | Loss: 0.00001171
Iteration 154/1000 | Loss: 0.00001171
Iteration 155/1000 | Loss: 0.00001171
Iteration 156/1000 | Loss: 0.00001171
Iteration 157/1000 | Loss: 0.00001171
Iteration 158/1000 | Loss: 0.00001171
Iteration 159/1000 | Loss: 0.00001171
Iteration 160/1000 | Loss: 0.00001171
Iteration 161/1000 | Loss: 0.00001171
Iteration 162/1000 | Loss: 0.00001171
Iteration 163/1000 | Loss: 0.00001171
Iteration 164/1000 | Loss: 0.00001171
Iteration 165/1000 | Loss: 0.00001171
Iteration 166/1000 | Loss: 0.00001171
Iteration 167/1000 | Loss: 0.00001171
Iteration 168/1000 | Loss: 0.00001171
Iteration 169/1000 | Loss: 0.00001171
Iteration 170/1000 | Loss: 0.00001171
Iteration 171/1000 | Loss: 0.00001171
Iteration 172/1000 | Loss: 0.00001171
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 172. Stopping optimization.
Last 5 losses: [1.1711028491845354e-05, 1.1711028491845354e-05, 1.1711028491845354e-05, 1.1711028491845354e-05, 1.1711028491845354e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1711028491845354e-05

Optimization complete. Final v2v error: 2.8490443229675293 mm

Highest mean error: 7.928966522216797 mm for frame 18

Lowest mean error: 2.4947586059570312 mm for frame 61

Saving results

Total time: 43.257941484451294
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_grace_posed_004/1075/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_grace_posed_004/1075.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_grace_posed_004/1075
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00835442
Iteration 2/25 | Loss: 0.00135526
Iteration 3/25 | Loss: 0.00110099
Iteration 4/25 | Loss: 0.00108426
Iteration 5/25 | Loss: 0.00108084
Iteration 6/25 | Loss: 0.00108953
Iteration 7/25 | Loss: 0.00108002
Iteration 8/25 | Loss: 0.00106655
Iteration 9/25 | Loss: 0.00106392
Iteration 10/25 | Loss: 0.00106327
Iteration 11/25 | Loss: 0.00106317
Iteration 12/25 | Loss: 0.00106316
Iteration 13/25 | Loss: 0.00106316
Iteration 14/25 | Loss: 0.00106316
Iteration 15/25 | Loss: 0.00106316
Iteration 16/25 | Loss: 0.00106316
Iteration 17/25 | Loss: 0.00106316
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0010631625773385167, 0.0010631625773385167, 0.0010631625773385167, 0.0010631625773385167, 0.0010631625773385167]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010631625773385167

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.28881347
Iteration 2/25 | Loss: 0.00082974
Iteration 3/25 | Loss: 0.00082971
Iteration 4/25 | Loss: 0.00082971
Iteration 5/25 | Loss: 0.00082971
Iteration 6/25 | Loss: 0.00082971
Iteration 7/25 | Loss: 0.00082970
Iteration 8/25 | Loss: 0.00082970
Iteration 9/25 | Loss: 0.00082970
Iteration 10/25 | Loss: 0.00082970
Iteration 11/25 | Loss: 0.00082970
Iteration 12/25 | Loss: 0.00082970
Iteration 13/25 | Loss: 0.00082970
Iteration 14/25 | Loss: 0.00082970
Iteration 15/25 | Loss: 0.00082970
Iteration 16/25 | Loss: 0.00082970
Iteration 17/25 | Loss: 0.00082970
Iteration 18/25 | Loss: 0.00082970
Iteration 19/25 | Loss: 0.00082970
Iteration 20/25 | Loss: 0.00082970
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.000829703698400408, 0.000829703698400408, 0.000829703698400408, 0.000829703698400408, 0.000829703698400408]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000829703698400408

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00082970
Iteration 2/1000 | Loss: 0.00003055
Iteration 3/1000 | Loss: 0.00002359
Iteration 4/1000 | Loss: 0.00002072
Iteration 5/1000 | Loss: 0.00001949
Iteration 6/1000 | Loss: 0.00001884
Iteration 7/1000 | Loss: 0.00001811
Iteration 8/1000 | Loss: 0.00001769
Iteration 9/1000 | Loss: 0.00001731
Iteration 10/1000 | Loss: 0.00001700
Iteration 11/1000 | Loss: 0.00001679
Iteration 12/1000 | Loss: 0.00001674
Iteration 13/1000 | Loss: 0.00001668
Iteration 14/1000 | Loss: 0.00001663
Iteration 15/1000 | Loss: 0.00001659
Iteration 16/1000 | Loss: 0.00001646
Iteration 17/1000 | Loss: 0.00001638
Iteration 18/1000 | Loss: 0.00001635
Iteration 19/1000 | Loss: 0.00001634
Iteration 20/1000 | Loss: 0.00001633
Iteration 21/1000 | Loss: 0.00001631
Iteration 22/1000 | Loss: 0.00001626
Iteration 23/1000 | Loss: 0.00001625
Iteration 24/1000 | Loss: 0.00001622
Iteration 25/1000 | Loss: 0.00001621
Iteration 26/1000 | Loss: 0.00001620
Iteration 27/1000 | Loss: 0.00001620
Iteration 28/1000 | Loss: 0.00001619
Iteration 29/1000 | Loss: 0.00001618
Iteration 30/1000 | Loss: 0.00001617
Iteration 31/1000 | Loss: 0.00001616
Iteration 32/1000 | Loss: 0.00001616
Iteration 33/1000 | Loss: 0.00001616
Iteration 34/1000 | Loss: 0.00001615
Iteration 35/1000 | Loss: 0.00001614
Iteration 36/1000 | Loss: 0.00001614
Iteration 37/1000 | Loss: 0.00001613
Iteration 38/1000 | Loss: 0.00001612
Iteration 39/1000 | Loss: 0.00001611
Iteration 40/1000 | Loss: 0.00001611
Iteration 41/1000 | Loss: 0.00001610
Iteration 42/1000 | Loss: 0.00001610
Iteration 43/1000 | Loss: 0.00001609
Iteration 44/1000 | Loss: 0.00001609
Iteration 45/1000 | Loss: 0.00001608
Iteration 46/1000 | Loss: 0.00001608
Iteration 47/1000 | Loss: 0.00001607
Iteration 48/1000 | Loss: 0.00001607
Iteration 49/1000 | Loss: 0.00001606
Iteration 50/1000 | Loss: 0.00001606
Iteration 51/1000 | Loss: 0.00001606
Iteration 52/1000 | Loss: 0.00001606
Iteration 53/1000 | Loss: 0.00001605
Iteration 54/1000 | Loss: 0.00001605
Iteration 55/1000 | Loss: 0.00001605
Iteration 56/1000 | Loss: 0.00001604
Iteration 57/1000 | Loss: 0.00001604
Iteration 58/1000 | Loss: 0.00001604
Iteration 59/1000 | Loss: 0.00001604
Iteration 60/1000 | Loss: 0.00001604
Iteration 61/1000 | Loss: 0.00001603
Iteration 62/1000 | Loss: 0.00001603
Iteration 63/1000 | Loss: 0.00001603
Iteration 64/1000 | Loss: 0.00001603
Iteration 65/1000 | Loss: 0.00001602
Iteration 66/1000 | Loss: 0.00001602
Iteration 67/1000 | Loss: 0.00001602
Iteration 68/1000 | Loss: 0.00001602
Iteration 69/1000 | Loss: 0.00001602
Iteration 70/1000 | Loss: 0.00001601
Iteration 71/1000 | Loss: 0.00001601
Iteration 72/1000 | Loss: 0.00001601
Iteration 73/1000 | Loss: 0.00001600
Iteration 74/1000 | Loss: 0.00001600
Iteration 75/1000 | Loss: 0.00001600
Iteration 76/1000 | Loss: 0.00001600
Iteration 77/1000 | Loss: 0.00001599
Iteration 78/1000 | Loss: 0.00001599
Iteration 79/1000 | Loss: 0.00001599
Iteration 80/1000 | Loss: 0.00001599
Iteration 81/1000 | Loss: 0.00001598
Iteration 82/1000 | Loss: 0.00001598
Iteration 83/1000 | Loss: 0.00001598
Iteration 84/1000 | Loss: 0.00001597
Iteration 85/1000 | Loss: 0.00001596
Iteration 86/1000 | Loss: 0.00001596
Iteration 87/1000 | Loss: 0.00001596
Iteration 88/1000 | Loss: 0.00001595
Iteration 89/1000 | Loss: 0.00001595
Iteration 90/1000 | Loss: 0.00001595
Iteration 91/1000 | Loss: 0.00001595
Iteration 92/1000 | Loss: 0.00001595
Iteration 93/1000 | Loss: 0.00001595
Iteration 94/1000 | Loss: 0.00001595
Iteration 95/1000 | Loss: 0.00001595
Iteration 96/1000 | Loss: 0.00001595
Iteration 97/1000 | Loss: 0.00001594
Iteration 98/1000 | Loss: 0.00001594
Iteration 99/1000 | Loss: 0.00001593
Iteration 100/1000 | Loss: 0.00001593
Iteration 101/1000 | Loss: 0.00001593
Iteration 102/1000 | Loss: 0.00001593
Iteration 103/1000 | Loss: 0.00001593
Iteration 104/1000 | Loss: 0.00001592
Iteration 105/1000 | Loss: 0.00001592
Iteration 106/1000 | Loss: 0.00001592
Iteration 107/1000 | Loss: 0.00001592
Iteration 108/1000 | Loss: 0.00001592
Iteration 109/1000 | Loss: 0.00001592
Iteration 110/1000 | Loss: 0.00001591
Iteration 111/1000 | Loss: 0.00001591
Iteration 112/1000 | Loss: 0.00001591
Iteration 113/1000 | Loss: 0.00001590
Iteration 114/1000 | Loss: 0.00001590
Iteration 115/1000 | Loss: 0.00001590
Iteration 116/1000 | Loss: 0.00001590
Iteration 117/1000 | Loss: 0.00001589
Iteration 118/1000 | Loss: 0.00001589
Iteration 119/1000 | Loss: 0.00001589
Iteration 120/1000 | Loss: 0.00001589
Iteration 121/1000 | Loss: 0.00001589
Iteration 122/1000 | Loss: 0.00001589
Iteration 123/1000 | Loss: 0.00001589
Iteration 124/1000 | Loss: 0.00001589
Iteration 125/1000 | Loss: 0.00001589
Iteration 126/1000 | Loss: 0.00001589
Iteration 127/1000 | Loss: 0.00001589
Iteration 128/1000 | Loss: 0.00001588
Iteration 129/1000 | Loss: 0.00001588
Iteration 130/1000 | Loss: 0.00001588
Iteration 131/1000 | Loss: 0.00001588
Iteration 132/1000 | Loss: 0.00001588
Iteration 133/1000 | Loss: 0.00001587
Iteration 134/1000 | Loss: 0.00001587
Iteration 135/1000 | Loss: 0.00001587
Iteration 136/1000 | Loss: 0.00001587
Iteration 137/1000 | Loss: 0.00001587
Iteration 138/1000 | Loss: 0.00001587
Iteration 139/1000 | Loss: 0.00001587
Iteration 140/1000 | Loss: 0.00001587
Iteration 141/1000 | Loss: 0.00001587
Iteration 142/1000 | Loss: 0.00001586
Iteration 143/1000 | Loss: 0.00001586
Iteration 144/1000 | Loss: 0.00001586
Iteration 145/1000 | Loss: 0.00001586
Iteration 146/1000 | Loss: 0.00001586
Iteration 147/1000 | Loss: 0.00001586
Iteration 148/1000 | Loss: 0.00001586
Iteration 149/1000 | Loss: 0.00001586
Iteration 150/1000 | Loss: 0.00001586
Iteration 151/1000 | Loss: 0.00001586
Iteration 152/1000 | Loss: 0.00001586
Iteration 153/1000 | Loss: 0.00001586
Iteration 154/1000 | Loss: 0.00001585
Iteration 155/1000 | Loss: 0.00001585
Iteration 156/1000 | Loss: 0.00001585
Iteration 157/1000 | Loss: 0.00001585
Iteration 158/1000 | Loss: 0.00001585
Iteration 159/1000 | Loss: 0.00001585
Iteration 160/1000 | Loss: 0.00001585
Iteration 161/1000 | Loss: 0.00001585
Iteration 162/1000 | Loss: 0.00001585
Iteration 163/1000 | Loss: 0.00001585
Iteration 164/1000 | Loss: 0.00001585
Iteration 165/1000 | Loss: 0.00001584
Iteration 166/1000 | Loss: 0.00001584
Iteration 167/1000 | Loss: 0.00001584
Iteration 168/1000 | Loss: 0.00001584
Iteration 169/1000 | Loss: 0.00001584
Iteration 170/1000 | Loss: 0.00001584
Iteration 171/1000 | Loss: 0.00001584
Iteration 172/1000 | Loss: 0.00001584
Iteration 173/1000 | Loss: 0.00001584
Iteration 174/1000 | Loss: 0.00001584
Iteration 175/1000 | Loss: 0.00001583
Iteration 176/1000 | Loss: 0.00001583
Iteration 177/1000 | Loss: 0.00001583
Iteration 178/1000 | Loss: 0.00001583
Iteration 179/1000 | Loss: 0.00001583
Iteration 180/1000 | Loss: 0.00001583
Iteration 181/1000 | Loss: 0.00001583
Iteration 182/1000 | Loss: 0.00001583
Iteration 183/1000 | Loss: 0.00001583
Iteration 184/1000 | Loss: 0.00001583
Iteration 185/1000 | Loss: 0.00001582
Iteration 186/1000 | Loss: 0.00001582
Iteration 187/1000 | Loss: 0.00001582
Iteration 188/1000 | Loss: 0.00001582
Iteration 189/1000 | Loss: 0.00001582
Iteration 190/1000 | Loss: 0.00001582
Iteration 191/1000 | Loss: 0.00001582
Iteration 192/1000 | Loss: 0.00001582
Iteration 193/1000 | Loss: 0.00001582
Iteration 194/1000 | Loss: 0.00001582
Iteration 195/1000 | Loss: 0.00001582
Iteration 196/1000 | Loss: 0.00001582
Iteration 197/1000 | Loss: 0.00001582
Iteration 198/1000 | Loss: 0.00001582
Iteration 199/1000 | Loss: 0.00001582
Iteration 200/1000 | Loss: 0.00001582
Iteration 201/1000 | Loss: 0.00001582
Iteration 202/1000 | Loss: 0.00001582
Iteration 203/1000 | Loss: 0.00001582
Iteration 204/1000 | Loss: 0.00001582
Iteration 205/1000 | Loss: 0.00001582
Iteration 206/1000 | Loss: 0.00001582
Iteration 207/1000 | Loss: 0.00001582
Iteration 208/1000 | Loss: 0.00001582
Iteration 209/1000 | Loss: 0.00001582
Iteration 210/1000 | Loss: 0.00001582
Iteration 211/1000 | Loss: 0.00001582
Iteration 212/1000 | Loss: 0.00001582
Iteration 213/1000 | Loss: 0.00001582
Iteration 214/1000 | Loss: 0.00001582
Iteration 215/1000 | Loss: 0.00001582
Iteration 216/1000 | Loss: 0.00001582
Iteration 217/1000 | Loss: 0.00001582
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 217. Stopping optimization.
Last 5 losses: [1.5818275642232038e-05, 1.5818275642232038e-05, 1.5818275642232038e-05, 1.5818275642232038e-05, 1.5818275642232038e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5818275642232038e-05

Optimization complete. Final v2v error: 3.274789810180664 mm

Highest mean error: 4.1567511558532715 mm for frame 32

Lowest mean error: 2.695988893508911 mm for frame 150

Saving results

Total time: 60.037753105163574
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_grace_posed_004/1051/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_grace_posed_004/1051.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_grace_posed_004/1051
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00278095
Iteration 2/25 | Loss: 0.00129965
Iteration 3/25 | Loss: 0.00107913
Iteration 4/25 | Loss: 0.00103445
Iteration 5/25 | Loss: 0.00102299
Iteration 6/25 | Loss: 0.00101786
Iteration 7/25 | Loss: 0.00101575
Iteration 8/25 | Loss: 0.00101500
Iteration 9/25 | Loss: 0.00101469
Iteration 10/25 | Loss: 0.00101452
Iteration 11/25 | Loss: 0.00101448
Iteration 12/25 | Loss: 0.00101448
Iteration 13/25 | Loss: 0.00101448
Iteration 14/25 | Loss: 0.00101448
Iteration 15/25 | Loss: 0.00101448
Iteration 16/25 | Loss: 0.00101448
Iteration 17/25 | Loss: 0.00101448
Iteration 18/25 | Loss: 0.00101448
Iteration 19/25 | Loss: 0.00101448
Iteration 20/25 | Loss: 0.00101448
Iteration 21/25 | Loss: 0.00101448
Iteration 22/25 | Loss: 0.00101448
Iteration 23/25 | Loss: 0.00101448
Iteration 24/25 | Loss: 0.00101448
Iteration 25/25 | Loss: 0.00101448

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.29354894
Iteration 2/25 | Loss: 0.00213545
Iteration 3/25 | Loss: 0.00213545
Iteration 4/25 | Loss: 0.00213545
Iteration 5/25 | Loss: 0.00213545
Iteration 6/25 | Loss: 0.00213545
Iteration 7/25 | Loss: 0.00213545
Iteration 8/25 | Loss: 0.00213545
Iteration 9/25 | Loss: 0.00213545
Iteration 10/25 | Loss: 0.00213545
Iteration 11/25 | Loss: 0.00213545
Iteration 12/25 | Loss: 0.00213545
Iteration 13/25 | Loss: 0.00213545
Iteration 14/25 | Loss: 0.00213545
Iteration 15/25 | Loss: 0.00213545
Iteration 16/25 | Loss: 0.00213545
Iteration 17/25 | Loss: 0.00213545
Iteration 18/25 | Loss: 0.00213545
Iteration 19/25 | Loss: 0.00213545
Iteration 20/25 | Loss: 0.00213545
Iteration 21/25 | Loss: 0.00213545
Iteration 22/25 | Loss: 0.00213545
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0021354462951421738, 0.0021354462951421738, 0.0021354462951421738, 0.0021354462951421738, 0.0021354462951421738]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0021354462951421738

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00213545
Iteration 2/1000 | Loss: 0.00020118
Iteration 3/1000 | Loss: 0.00013210
Iteration 4/1000 | Loss: 0.00009524
Iteration 5/1000 | Loss: 0.00010093
Iteration 6/1000 | Loss: 0.00007353
Iteration 7/1000 | Loss: 0.00004322
Iteration 8/1000 | Loss: 0.00007568
Iteration 9/1000 | Loss: 0.00002752
Iteration 10/1000 | Loss: 0.00004731
Iteration 11/1000 | Loss: 0.00007119
Iteration 12/1000 | Loss: 0.00010094
Iteration 13/1000 | Loss: 0.00011625
Iteration 14/1000 | Loss: 0.00004429
Iteration 15/1000 | Loss: 0.00006725
Iteration 16/1000 | Loss: 0.00005892
Iteration 17/1000 | Loss: 0.00009207
Iteration 18/1000 | Loss: 0.00007417
Iteration 19/1000 | Loss: 0.00004144
Iteration 20/1000 | Loss: 0.00005882
Iteration 21/1000 | Loss: 0.00005017
Iteration 22/1000 | Loss: 0.00003755
Iteration 23/1000 | Loss: 0.00010811
Iteration 24/1000 | Loss: 0.00005874
Iteration 25/1000 | Loss: 0.00005486
Iteration 26/1000 | Loss: 0.00008013
Iteration 27/1000 | Loss: 0.00007222
Iteration 28/1000 | Loss: 0.00011291
Iteration 29/1000 | Loss: 0.00007146
Iteration 30/1000 | Loss: 0.00005111
Iteration 31/1000 | Loss: 0.00004936
Iteration 32/1000 | Loss: 0.00008751
Iteration 33/1000 | Loss: 0.00007054
Iteration 34/1000 | Loss: 0.00010517
Iteration 35/1000 | Loss: 0.00008752
Iteration 36/1000 | Loss: 0.00009588
Iteration 37/1000 | Loss: 0.00009797
Iteration 38/1000 | Loss: 0.00010345
Iteration 39/1000 | Loss: 0.00008585
Iteration 40/1000 | Loss: 0.00009763
Iteration 41/1000 | Loss: 0.00006791
Iteration 42/1000 | Loss: 0.00004614
Iteration 43/1000 | Loss: 0.00006101
Iteration 44/1000 | Loss: 0.00002519
Iteration 45/1000 | Loss: 0.00003138
Iteration 46/1000 | Loss: 0.00001900
Iteration 47/1000 | Loss: 0.00001615
Iteration 48/1000 | Loss: 0.00001453
Iteration 49/1000 | Loss: 0.00001293
Iteration 50/1000 | Loss: 0.00001175
Iteration 51/1000 | Loss: 0.00001116
Iteration 52/1000 | Loss: 0.00001080
Iteration 53/1000 | Loss: 0.00001053
Iteration 54/1000 | Loss: 0.00001034
Iteration 55/1000 | Loss: 0.00001030
Iteration 56/1000 | Loss: 0.00001025
Iteration 57/1000 | Loss: 0.00001018
Iteration 58/1000 | Loss: 0.00001016
Iteration 59/1000 | Loss: 0.00001011
Iteration 60/1000 | Loss: 0.00001000
Iteration 61/1000 | Loss: 0.00000998
Iteration 62/1000 | Loss: 0.00000998
Iteration 63/1000 | Loss: 0.00000997
Iteration 64/1000 | Loss: 0.00000997
Iteration 65/1000 | Loss: 0.00000997
Iteration 66/1000 | Loss: 0.00000996
Iteration 67/1000 | Loss: 0.00000996
Iteration 68/1000 | Loss: 0.00000996
Iteration 69/1000 | Loss: 0.00000995
Iteration 70/1000 | Loss: 0.00000992
Iteration 71/1000 | Loss: 0.00000992
Iteration 72/1000 | Loss: 0.00000991
Iteration 73/1000 | Loss: 0.00000990
Iteration 74/1000 | Loss: 0.00000990
Iteration 75/1000 | Loss: 0.00000990
Iteration 76/1000 | Loss: 0.00000989
Iteration 77/1000 | Loss: 0.00000989
Iteration 78/1000 | Loss: 0.00000988
Iteration 79/1000 | Loss: 0.00000987
Iteration 80/1000 | Loss: 0.00000987
Iteration 81/1000 | Loss: 0.00000987
Iteration 82/1000 | Loss: 0.00000987
Iteration 83/1000 | Loss: 0.00000987
Iteration 84/1000 | Loss: 0.00000987
Iteration 85/1000 | Loss: 0.00000986
Iteration 86/1000 | Loss: 0.00000986
Iteration 87/1000 | Loss: 0.00000986
Iteration 88/1000 | Loss: 0.00000986
Iteration 89/1000 | Loss: 0.00000986
Iteration 90/1000 | Loss: 0.00000986
Iteration 91/1000 | Loss: 0.00000986
Iteration 92/1000 | Loss: 0.00000984
Iteration 93/1000 | Loss: 0.00000984
Iteration 94/1000 | Loss: 0.00000983
Iteration 95/1000 | Loss: 0.00000983
Iteration 96/1000 | Loss: 0.00000982
Iteration 97/1000 | Loss: 0.00000982
Iteration 98/1000 | Loss: 0.00000982
Iteration 99/1000 | Loss: 0.00000981
Iteration 100/1000 | Loss: 0.00000981
Iteration 101/1000 | Loss: 0.00000981
Iteration 102/1000 | Loss: 0.00000980
Iteration 103/1000 | Loss: 0.00000980
Iteration 104/1000 | Loss: 0.00000980
Iteration 105/1000 | Loss: 0.00000979
Iteration 106/1000 | Loss: 0.00000979
Iteration 107/1000 | Loss: 0.00000979
Iteration 108/1000 | Loss: 0.00000978
Iteration 109/1000 | Loss: 0.00000978
Iteration 110/1000 | Loss: 0.00000978
Iteration 111/1000 | Loss: 0.00000977
Iteration 112/1000 | Loss: 0.00000977
Iteration 113/1000 | Loss: 0.00000977
Iteration 114/1000 | Loss: 0.00000977
Iteration 115/1000 | Loss: 0.00000977
Iteration 116/1000 | Loss: 0.00000977
Iteration 117/1000 | Loss: 0.00000977
Iteration 118/1000 | Loss: 0.00000977
Iteration 119/1000 | Loss: 0.00000977
Iteration 120/1000 | Loss: 0.00000976
Iteration 121/1000 | Loss: 0.00000976
Iteration 122/1000 | Loss: 0.00000976
Iteration 123/1000 | Loss: 0.00000975
Iteration 124/1000 | Loss: 0.00000975
Iteration 125/1000 | Loss: 0.00000975
Iteration 126/1000 | Loss: 0.00000975
Iteration 127/1000 | Loss: 0.00000974
Iteration 128/1000 | Loss: 0.00000974
Iteration 129/1000 | Loss: 0.00000974
Iteration 130/1000 | Loss: 0.00000974
Iteration 131/1000 | Loss: 0.00000974
Iteration 132/1000 | Loss: 0.00000974
Iteration 133/1000 | Loss: 0.00000974
Iteration 134/1000 | Loss: 0.00000974
Iteration 135/1000 | Loss: 0.00000974
Iteration 136/1000 | Loss: 0.00000974
Iteration 137/1000 | Loss: 0.00000974
Iteration 138/1000 | Loss: 0.00000973
Iteration 139/1000 | Loss: 0.00000973
Iteration 140/1000 | Loss: 0.00000973
Iteration 141/1000 | Loss: 0.00000973
Iteration 142/1000 | Loss: 0.00000973
Iteration 143/1000 | Loss: 0.00000973
Iteration 144/1000 | Loss: 0.00000973
Iteration 145/1000 | Loss: 0.00000973
Iteration 146/1000 | Loss: 0.00000973
Iteration 147/1000 | Loss: 0.00000973
Iteration 148/1000 | Loss: 0.00000973
Iteration 149/1000 | Loss: 0.00000973
Iteration 150/1000 | Loss: 0.00000973
Iteration 151/1000 | Loss: 0.00000973
Iteration 152/1000 | Loss: 0.00000973
Iteration 153/1000 | Loss: 0.00000972
Iteration 154/1000 | Loss: 0.00000972
Iteration 155/1000 | Loss: 0.00000972
Iteration 156/1000 | Loss: 0.00000972
Iteration 157/1000 | Loss: 0.00000972
Iteration 158/1000 | Loss: 0.00000972
Iteration 159/1000 | Loss: 0.00000972
Iteration 160/1000 | Loss: 0.00000971
Iteration 161/1000 | Loss: 0.00000971
Iteration 162/1000 | Loss: 0.00000971
Iteration 163/1000 | Loss: 0.00000971
Iteration 164/1000 | Loss: 0.00000971
Iteration 165/1000 | Loss: 0.00000971
Iteration 166/1000 | Loss: 0.00000970
Iteration 167/1000 | Loss: 0.00000970
Iteration 168/1000 | Loss: 0.00000970
Iteration 169/1000 | Loss: 0.00000970
Iteration 170/1000 | Loss: 0.00000970
Iteration 171/1000 | Loss: 0.00000970
Iteration 172/1000 | Loss: 0.00000970
Iteration 173/1000 | Loss: 0.00000969
Iteration 174/1000 | Loss: 0.00000969
Iteration 175/1000 | Loss: 0.00000969
Iteration 176/1000 | Loss: 0.00000969
Iteration 177/1000 | Loss: 0.00000969
Iteration 178/1000 | Loss: 0.00000969
Iteration 179/1000 | Loss: 0.00000969
Iteration 180/1000 | Loss: 0.00000969
Iteration 181/1000 | Loss: 0.00000969
Iteration 182/1000 | Loss: 0.00000969
Iteration 183/1000 | Loss: 0.00000969
Iteration 184/1000 | Loss: 0.00000969
Iteration 185/1000 | Loss: 0.00000968
Iteration 186/1000 | Loss: 0.00000968
Iteration 187/1000 | Loss: 0.00000968
Iteration 188/1000 | Loss: 0.00000968
Iteration 189/1000 | Loss: 0.00000968
Iteration 190/1000 | Loss: 0.00000968
Iteration 191/1000 | Loss: 0.00000967
Iteration 192/1000 | Loss: 0.00000967
Iteration 193/1000 | Loss: 0.00000967
Iteration 194/1000 | Loss: 0.00000967
Iteration 195/1000 | Loss: 0.00000967
Iteration 196/1000 | Loss: 0.00000967
Iteration 197/1000 | Loss: 0.00000967
Iteration 198/1000 | Loss: 0.00000967
Iteration 199/1000 | Loss: 0.00000967
Iteration 200/1000 | Loss: 0.00000967
Iteration 201/1000 | Loss: 0.00000966
Iteration 202/1000 | Loss: 0.00000966
Iteration 203/1000 | Loss: 0.00000966
Iteration 204/1000 | Loss: 0.00000966
Iteration 205/1000 | Loss: 0.00000966
Iteration 206/1000 | Loss: 0.00000966
Iteration 207/1000 | Loss: 0.00000966
Iteration 208/1000 | Loss: 0.00000966
Iteration 209/1000 | Loss: 0.00000966
Iteration 210/1000 | Loss: 0.00000966
Iteration 211/1000 | Loss: 0.00000966
Iteration 212/1000 | Loss: 0.00000965
Iteration 213/1000 | Loss: 0.00000965
Iteration 214/1000 | Loss: 0.00000965
Iteration 215/1000 | Loss: 0.00000965
Iteration 216/1000 | Loss: 0.00000965
Iteration 217/1000 | Loss: 0.00000964
Iteration 218/1000 | Loss: 0.00000964
Iteration 219/1000 | Loss: 0.00000964
Iteration 220/1000 | Loss: 0.00000964
Iteration 221/1000 | Loss: 0.00000964
Iteration 222/1000 | Loss: 0.00000964
Iteration 223/1000 | Loss: 0.00000964
Iteration 224/1000 | Loss: 0.00000964
Iteration 225/1000 | Loss: 0.00000963
Iteration 226/1000 | Loss: 0.00000963
Iteration 227/1000 | Loss: 0.00000963
Iteration 228/1000 | Loss: 0.00000963
Iteration 229/1000 | Loss: 0.00000963
Iteration 230/1000 | Loss: 0.00000963
Iteration 231/1000 | Loss: 0.00000963
Iteration 232/1000 | Loss: 0.00000963
Iteration 233/1000 | Loss: 0.00000963
Iteration 234/1000 | Loss: 0.00000963
Iteration 235/1000 | Loss: 0.00000963
Iteration 236/1000 | Loss: 0.00000963
Iteration 237/1000 | Loss: 0.00000963
Iteration 238/1000 | Loss: 0.00000963
Iteration 239/1000 | Loss: 0.00000963
Iteration 240/1000 | Loss: 0.00000962
Iteration 241/1000 | Loss: 0.00000962
Iteration 242/1000 | Loss: 0.00000962
Iteration 243/1000 | Loss: 0.00000962
Iteration 244/1000 | Loss: 0.00000962
Iteration 245/1000 | Loss: 0.00000962
Iteration 246/1000 | Loss: 0.00000962
Iteration 247/1000 | Loss: 0.00000962
Iteration 248/1000 | Loss: 0.00000962
Iteration 249/1000 | Loss: 0.00000962
Iteration 250/1000 | Loss: 0.00000962
Iteration 251/1000 | Loss: 0.00000962
Iteration 252/1000 | Loss: 0.00000962
Iteration 253/1000 | Loss: 0.00000962
Iteration 254/1000 | Loss: 0.00000962
Iteration 255/1000 | Loss: 0.00000961
Iteration 256/1000 | Loss: 0.00000961
Iteration 257/1000 | Loss: 0.00000961
Iteration 258/1000 | Loss: 0.00000961
Iteration 259/1000 | Loss: 0.00000961
Iteration 260/1000 | Loss: 0.00000961
Iteration 261/1000 | Loss: 0.00000961
Iteration 262/1000 | Loss: 0.00000961
Iteration 263/1000 | Loss: 0.00000961
Iteration 264/1000 | Loss: 0.00000961
Iteration 265/1000 | Loss: 0.00000961
Iteration 266/1000 | Loss: 0.00000961
Iteration 267/1000 | Loss: 0.00000961
Iteration 268/1000 | Loss: 0.00000961
Iteration 269/1000 | Loss: 0.00000960
Iteration 270/1000 | Loss: 0.00000960
Iteration 271/1000 | Loss: 0.00000960
Iteration 272/1000 | Loss: 0.00000960
Iteration 273/1000 | Loss: 0.00000960
Iteration 274/1000 | Loss: 0.00000960
Iteration 275/1000 | Loss: 0.00000960
Iteration 276/1000 | Loss: 0.00000960
Iteration 277/1000 | Loss: 0.00000960
Iteration 278/1000 | Loss: 0.00000960
Iteration 279/1000 | Loss: 0.00000960
Iteration 280/1000 | Loss: 0.00000960
Iteration 281/1000 | Loss: 0.00000960
Iteration 282/1000 | Loss: 0.00000960
Iteration 283/1000 | Loss: 0.00000960
Iteration 284/1000 | Loss: 0.00000960
Iteration 285/1000 | Loss: 0.00000960
Iteration 286/1000 | Loss: 0.00000960
Iteration 287/1000 | Loss: 0.00000959
Iteration 288/1000 | Loss: 0.00000959
Iteration 289/1000 | Loss: 0.00000959
Iteration 290/1000 | Loss: 0.00000959
Iteration 291/1000 | Loss: 0.00000959
Iteration 292/1000 | Loss: 0.00000959
Iteration 293/1000 | Loss: 0.00000959
Iteration 294/1000 | Loss: 0.00000959
Iteration 295/1000 | Loss: 0.00000959
Iteration 296/1000 | Loss: 0.00000959
Iteration 297/1000 | Loss: 0.00000959
Iteration 298/1000 | Loss: 0.00000959
Iteration 299/1000 | Loss: 0.00000959
Iteration 300/1000 | Loss: 0.00000959
Iteration 301/1000 | Loss: 0.00000959
Iteration 302/1000 | Loss: 0.00000959
Iteration 303/1000 | Loss: 0.00000959
Iteration 304/1000 | Loss: 0.00000959
Iteration 305/1000 | Loss: 0.00000959
Iteration 306/1000 | Loss: 0.00000959
Iteration 307/1000 | Loss: 0.00000958
Iteration 308/1000 | Loss: 0.00000958
Iteration 309/1000 | Loss: 0.00000958
Iteration 310/1000 | Loss: 0.00000958
Iteration 311/1000 | Loss: 0.00000958
Iteration 312/1000 | Loss: 0.00000958
Iteration 313/1000 | Loss: 0.00000958
Iteration 314/1000 | Loss: 0.00000958
Iteration 315/1000 | Loss: 0.00000958
Iteration 316/1000 | Loss: 0.00000958
Iteration 317/1000 | Loss: 0.00000958
Iteration 318/1000 | Loss: 0.00000958
Iteration 319/1000 | Loss: 0.00000958
Iteration 320/1000 | Loss: 0.00000958
Iteration 321/1000 | Loss: 0.00000958
Iteration 322/1000 | Loss: 0.00000958
Iteration 323/1000 | Loss: 0.00000958
Iteration 324/1000 | Loss: 0.00000958
Iteration 325/1000 | Loss: 0.00000958
Iteration 326/1000 | Loss: 0.00000957
Iteration 327/1000 | Loss: 0.00000957
Iteration 328/1000 | Loss: 0.00000957
Iteration 329/1000 | Loss: 0.00000957
Iteration 330/1000 | Loss: 0.00000957
Iteration 331/1000 | Loss: 0.00000957
Iteration 332/1000 | Loss: 0.00000957
Iteration 333/1000 | Loss: 0.00000957
Iteration 334/1000 | Loss: 0.00000957
Iteration 335/1000 | Loss: 0.00000957
Iteration 336/1000 | Loss: 0.00000957
Iteration 337/1000 | Loss: 0.00000957
Iteration 338/1000 | Loss: 0.00000957
Iteration 339/1000 | Loss: 0.00000957
Iteration 340/1000 | Loss: 0.00000957
Iteration 341/1000 | Loss: 0.00000957
Iteration 342/1000 | Loss: 0.00000956
Iteration 343/1000 | Loss: 0.00000956
Iteration 344/1000 | Loss: 0.00000956
Iteration 345/1000 | Loss: 0.00000956
Iteration 346/1000 | Loss: 0.00000956
Iteration 347/1000 | Loss: 0.00000956
Iteration 348/1000 | Loss: 0.00000956
Iteration 349/1000 | Loss: 0.00000956
Iteration 350/1000 | Loss: 0.00000956
Iteration 351/1000 | Loss: 0.00000956
Iteration 352/1000 | Loss: 0.00000956
Iteration 353/1000 | Loss: 0.00000956
Iteration 354/1000 | Loss: 0.00000956
Iteration 355/1000 | Loss: 0.00000956
Iteration 356/1000 | Loss: 0.00000956
Iteration 357/1000 | Loss: 0.00000956
Iteration 358/1000 | Loss: 0.00000956
Iteration 359/1000 | Loss: 0.00000956
Iteration 360/1000 | Loss: 0.00000956
Iteration 361/1000 | Loss: 0.00000956
Iteration 362/1000 | Loss: 0.00000956
Iteration 363/1000 | Loss: 0.00000956
Iteration 364/1000 | Loss: 0.00000956
Iteration 365/1000 | Loss: 0.00000956
Iteration 366/1000 | Loss: 0.00000956
Iteration 367/1000 | Loss: 0.00000956
Iteration 368/1000 | Loss: 0.00000956
Iteration 369/1000 | Loss: 0.00000956
Iteration 370/1000 | Loss: 0.00000956
Iteration 371/1000 | Loss: 0.00000955
Iteration 372/1000 | Loss: 0.00000955
Iteration 373/1000 | Loss: 0.00000955
Iteration 374/1000 | Loss: 0.00000955
Iteration 375/1000 | Loss: 0.00000955
Iteration 376/1000 | Loss: 0.00000955
Iteration 377/1000 | Loss: 0.00000955
Iteration 378/1000 | Loss: 0.00000955
Iteration 379/1000 | Loss: 0.00000955
Iteration 380/1000 | Loss: 0.00000955
Iteration 381/1000 | Loss: 0.00000955
Iteration 382/1000 | Loss: 0.00000955
Iteration 383/1000 | Loss: 0.00000955
Iteration 384/1000 | Loss: 0.00000955
Iteration 385/1000 | Loss: 0.00000955
Iteration 386/1000 | Loss: 0.00000955
Iteration 387/1000 | Loss: 0.00000955
Iteration 388/1000 | Loss: 0.00000955
Iteration 389/1000 | Loss: 0.00000955
Iteration 390/1000 | Loss: 0.00000955
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 390. Stopping optimization.
Last 5 losses: [9.552786650601774e-06, 9.552786650601774e-06, 9.552786650601774e-06, 9.552786650601774e-06, 9.552786650601774e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.552786650601774e-06

Optimization complete. Final v2v error: 2.649038076400757 mm

Highest mean error: 3.073049783706665 mm for frame 98

Lowest mean error: 2.3602447509765625 mm for frame 120

Saving results

Total time: 124.3805513381958
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_grace_posed_004/1055/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_grace_posed_004/1055.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_grace_posed_004/1055
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01050666
Iteration 2/25 | Loss: 0.00227757
Iteration 3/25 | Loss: 0.00151349
Iteration 4/25 | Loss: 0.00164216
Iteration 5/25 | Loss: 0.00152426
Iteration 6/25 | Loss: 0.00140434
Iteration 7/25 | Loss: 0.00134703
Iteration 8/25 | Loss: 0.00132743
Iteration 9/25 | Loss: 0.00118152
Iteration 10/25 | Loss: 0.00115831
Iteration 11/25 | Loss: 0.00112364
Iteration 12/25 | Loss: 0.00110646
Iteration 13/25 | Loss: 0.00111515
Iteration 14/25 | Loss: 0.00111340
Iteration 15/25 | Loss: 0.00111065
Iteration 16/25 | Loss: 0.00110468
Iteration 17/25 | Loss: 0.00110600
Iteration 18/25 | Loss: 0.00109327
Iteration 19/25 | Loss: 0.00109524
Iteration 20/25 | Loss: 0.00109325
Iteration 21/25 | Loss: 0.00109716
Iteration 22/25 | Loss: 0.00109229
Iteration 23/25 | Loss: 0.00109930
Iteration 24/25 | Loss: 0.00109224
Iteration 25/25 | Loss: 0.00109734

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44636405
Iteration 2/25 | Loss: 0.00190133
Iteration 3/25 | Loss: 0.00184989
Iteration 4/25 | Loss: 0.00184989
Iteration 5/25 | Loss: 0.00184989
Iteration 6/25 | Loss: 0.00184989
Iteration 7/25 | Loss: 0.00184989
Iteration 8/25 | Loss: 0.00184989
Iteration 9/25 | Loss: 0.00184988
Iteration 10/25 | Loss: 0.00184988
Iteration 11/25 | Loss: 0.00184988
Iteration 12/25 | Loss: 0.00184988
Iteration 13/25 | Loss: 0.00184988
Iteration 14/25 | Loss: 0.00184988
Iteration 15/25 | Loss: 0.00184988
Iteration 16/25 | Loss: 0.00184988
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.001849884050898254, 0.001849884050898254, 0.001849884050898254, 0.001849884050898254, 0.001849884050898254]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001849884050898254

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00184988
Iteration 2/1000 | Loss: 0.00137594
Iteration 3/1000 | Loss: 0.00029537
Iteration 4/1000 | Loss: 0.00074073
Iteration 5/1000 | Loss: 0.00107495
Iteration 6/1000 | Loss: 0.00282333
Iteration 7/1000 | Loss: 0.00014656
Iteration 8/1000 | Loss: 0.00010315
Iteration 9/1000 | Loss: 0.00004886
Iteration 10/1000 | Loss: 0.00019566
Iteration 11/1000 | Loss: 0.00021105
Iteration 12/1000 | Loss: 0.00027137
Iteration 13/1000 | Loss: 0.00013684
Iteration 14/1000 | Loss: 0.00014471
Iteration 15/1000 | Loss: 0.00021922
Iteration 16/1000 | Loss: 0.00002827
Iteration 17/1000 | Loss: 0.00002054
Iteration 18/1000 | Loss: 0.00001786
Iteration 19/1000 | Loss: 0.00031219
Iteration 20/1000 | Loss: 0.00005653
Iteration 21/1000 | Loss: 0.00018347
Iteration 22/1000 | Loss: 0.00001984
Iteration 23/1000 | Loss: 0.00001465
Iteration 24/1000 | Loss: 0.00001363
Iteration 25/1000 | Loss: 0.00001300
Iteration 26/1000 | Loss: 0.00001212
Iteration 27/1000 | Loss: 0.00001159
Iteration 28/1000 | Loss: 0.00001115
Iteration 29/1000 | Loss: 0.00001085
Iteration 30/1000 | Loss: 0.00001080
Iteration 31/1000 | Loss: 0.00001069
Iteration 32/1000 | Loss: 0.00001050
Iteration 33/1000 | Loss: 0.00001032
Iteration 34/1000 | Loss: 0.00001018
Iteration 35/1000 | Loss: 0.00001018
Iteration 36/1000 | Loss: 0.00001015
Iteration 37/1000 | Loss: 0.00001015
Iteration 38/1000 | Loss: 0.00001011
Iteration 39/1000 | Loss: 0.00001008
Iteration 40/1000 | Loss: 0.00001007
Iteration 41/1000 | Loss: 0.00001006
Iteration 42/1000 | Loss: 0.00001006
Iteration 43/1000 | Loss: 0.00001006
Iteration 44/1000 | Loss: 0.00001005
Iteration 45/1000 | Loss: 0.00001005
Iteration 46/1000 | Loss: 0.00001004
Iteration 47/1000 | Loss: 0.00001004
Iteration 48/1000 | Loss: 0.00001004
Iteration 49/1000 | Loss: 0.00001004
Iteration 50/1000 | Loss: 0.00001004
Iteration 51/1000 | Loss: 0.00001004
Iteration 52/1000 | Loss: 0.00001003
Iteration 53/1000 | Loss: 0.00001003
Iteration 54/1000 | Loss: 0.00001003
Iteration 55/1000 | Loss: 0.00001003
Iteration 56/1000 | Loss: 0.00001003
Iteration 57/1000 | Loss: 0.00001002
Iteration 58/1000 | Loss: 0.00001002
Iteration 59/1000 | Loss: 0.00001002
Iteration 60/1000 | Loss: 0.00001001
Iteration 61/1000 | Loss: 0.00001001
Iteration 62/1000 | Loss: 0.00001000
Iteration 63/1000 | Loss: 0.00001000
Iteration 64/1000 | Loss: 0.00001000
Iteration 65/1000 | Loss: 0.00001000
Iteration 66/1000 | Loss: 0.00001000
Iteration 67/1000 | Loss: 0.00001000
Iteration 68/1000 | Loss: 0.00001000
Iteration 69/1000 | Loss: 0.00001000
Iteration 70/1000 | Loss: 0.00001000
Iteration 71/1000 | Loss: 0.00001000
Iteration 72/1000 | Loss: 0.00000999
Iteration 73/1000 | Loss: 0.00000999
Iteration 74/1000 | Loss: 0.00000999
Iteration 75/1000 | Loss: 0.00000998
Iteration 76/1000 | Loss: 0.00000998
Iteration 77/1000 | Loss: 0.00000998
Iteration 78/1000 | Loss: 0.00000997
Iteration 79/1000 | Loss: 0.00000997
Iteration 80/1000 | Loss: 0.00000997
Iteration 81/1000 | Loss: 0.00000997
Iteration 82/1000 | Loss: 0.00000997
Iteration 83/1000 | Loss: 0.00000997
Iteration 84/1000 | Loss: 0.00000997
Iteration 85/1000 | Loss: 0.00000997
Iteration 86/1000 | Loss: 0.00000997
Iteration 87/1000 | Loss: 0.00000997
Iteration 88/1000 | Loss: 0.00000997
Iteration 89/1000 | Loss: 0.00000996
Iteration 90/1000 | Loss: 0.00000996
Iteration 91/1000 | Loss: 0.00000995
Iteration 92/1000 | Loss: 0.00000995
Iteration 93/1000 | Loss: 0.00000995
Iteration 94/1000 | Loss: 0.00000995
Iteration 95/1000 | Loss: 0.00000995
Iteration 96/1000 | Loss: 0.00000995
Iteration 97/1000 | Loss: 0.00000994
Iteration 98/1000 | Loss: 0.00000994
Iteration 99/1000 | Loss: 0.00000994
Iteration 100/1000 | Loss: 0.00000994
Iteration 101/1000 | Loss: 0.00000994
Iteration 102/1000 | Loss: 0.00000993
Iteration 103/1000 | Loss: 0.00000993
Iteration 104/1000 | Loss: 0.00000992
Iteration 105/1000 | Loss: 0.00000992
Iteration 106/1000 | Loss: 0.00000992
Iteration 107/1000 | Loss: 0.00000992
Iteration 108/1000 | Loss: 0.00000992
Iteration 109/1000 | Loss: 0.00000992
Iteration 110/1000 | Loss: 0.00000992
Iteration 111/1000 | Loss: 0.00000992
Iteration 112/1000 | Loss: 0.00000992
Iteration 113/1000 | Loss: 0.00000992
Iteration 114/1000 | Loss: 0.00000992
Iteration 115/1000 | Loss: 0.00000992
Iteration 116/1000 | Loss: 0.00000991
Iteration 117/1000 | Loss: 0.00000991
Iteration 118/1000 | Loss: 0.00000991
Iteration 119/1000 | Loss: 0.00000991
Iteration 120/1000 | Loss: 0.00000991
Iteration 121/1000 | Loss: 0.00000990
Iteration 122/1000 | Loss: 0.00000990
Iteration 123/1000 | Loss: 0.00000990
Iteration 124/1000 | Loss: 0.00000990
Iteration 125/1000 | Loss: 0.00000990
Iteration 126/1000 | Loss: 0.00000990
Iteration 127/1000 | Loss: 0.00000990
Iteration 128/1000 | Loss: 0.00000990
Iteration 129/1000 | Loss: 0.00000989
Iteration 130/1000 | Loss: 0.00000989
Iteration 131/1000 | Loss: 0.00000989
Iteration 132/1000 | Loss: 0.00000989
Iteration 133/1000 | Loss: 0.00000989
Iteration 134/1000 | Loss: 0.00000988
Iteration 135/1000 | Loss: 0.00000988
Iteration 136/1000 | Loss: 0.00000988
Iteration 137/1000 | Loss: 0.00000988
Iteration 138/1000 | Loss: 0.00000988
Iteration 139/1000 | Loss: 0.00000987
Iteration 140/1000 | Loss: 0.00000987
Iteration 141/1000 | Loss: 0.00000987
Iteration 142/1000 | Loss: 0.00000986
Iteration 143/1000 | Loss: 0.00000986
Iteration 144/1000 | Loss: 0.00000986
Iteration 145/1000 | Loss: 0.00000986
Iteration 146/1000 | Loss: 0.00000986
Iteration 147/1000 | Loss: 0.00000985
Iteration 148/1000 | Loss: 0.00000985
Iteration 149/1000 | Loss: 0.00000985
Iteration 150/1000 | Loss: 0.00000985
Iteration 151/1000 | Loss: 0.00000985
Iteration 152/1000 | Loss: 0.00000985
Iteration 153/1000 | Loss: 0.00000985
Iteration 154/1000 | Loss: 0.00000985
Iteration 155/1000 | Loss: 0.00000985
Iteration 156/1000 | Loss: 0.00000985
Iteration 157/1000 | Loss: 0.00000984
Iteration 158/1000 | Loss: 0.00000984
Iteration 159/1000 | Loss: 0.00000984
Iteration 160/1000 | Loss: 0.00000984
Iteration 161/1000 | Loss: 0.00000984
Iteration 162/1000 | Loss: 0.00000983
Iteration 163/1000 | Loss: 0.00000983
Iteration 164/1000 | Loss: 0.00000983
Iteration 165/1000 | Loss: 0.00000983
Iteration 166/1000 | Loss: 0.00000983
Iteration 167/1000 | Loss: 0.00000983
Iteration 168/1000 | Loss: 0.00000982
Iteration 169/1000 | Loss: 0.00000982
Iteration 170/1000 | Loss: 0.00000982
Iteration 171/1000 | Loss: 0.00000982
Iteration 172/1000 | Loss: 0.00000982
Iteration 173/1000 | Loss: 0.00000982
Iteration 174/1000 | Loss: 0.00000982
Iteration 175/1000 | Loss: 0.00000982
Iteration 176/1000 | Loss: 0.00000982
Iteration 177/1000 | Loss: 0.00000982
Iteration 178/1000 | Loss: 0.00000981
Iteration 179/1000 | Loss: 0.00000981
Iteration 180/1000 | Loss: 0.00000981
Iteration 181/1000 | Loss: 0.00000981
Iteration 182/1000 | Loss: 0.00000981
Iteration 183/1000 | Loss: 0.00000981
Iteration 184/1000 | Loss: 0.00000981
Iteration 185/1000 | Loss: 0.00000981
Iteration 186/1000 | Loss: 0.00000981
Iteration 187/1000 | Loss: 0.00000981
Iteration 188/1000 | Loss: 0.00000981
Iteration 189/1000 | Loss: 0.00000980
Iteration 190/1000 | Loss: 0.00000980
Iteration 191/1000 | Loss: 0.00000980
Iteration 192/1000 | Loss: 0.00000980
Iteration 193/1000 | Loss: 0.00000980
Iteration 194/1000 | Loss: 0.00000980
Iteration 195/1000 | Loss: 0.00000980
Iteration 196/1000 | Loss: 0.00000980
Iteration 197/1000 | Loss: 0.00000980
Iteration 198/1000 | Loss: 0.00000980
Iteration 199/1000 | Loss: 0.00000980
Iteration 200/1000 | Loss: 0.00000980
Iteration 201/1000 | Loss: 0.00000980
Iteration 202/1000 | Loss: 0.00000980
Iteration 203/1000 | Loss: 0.00000980
Iteration 204/1000 | Loss: 0.00000980
Iteration 205/1000 | Loss: 0.00000980
Iteration 206/1000 | Loss: 0.00000980
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 206. Stopping optimization.
Last 5 losses: [9.80397999228444e-06, 9.80397999228444e-06, 9.80397999228444e-06, 9.80397999228444e-06, 9.80397999228444e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.80397999228444e-06

Optimization complete. Final v2v error: 2.565105438232422 mm

Highest mean error: 8.582572937011719 mm for frame 40

Lowest mean error: 2.1764638423919678 mm for frame 44

Saving results

Total time: 106.86053943634033
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_grace_posed_004/1043/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_grace_posed_004/1043.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_grace_posed_004/1043
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01058120
Iteration 2/25 | Loss: 0.00200916
Iteration 3/25 | Loss: 0.00149643
Iteration 4/25 | Loss: 0.00133738
Iteration 5/25 | Loss: 0.00125111
Iteration 6/25 | Loss: 0.00123653
Iteration 7/25 | Loss: 0.00115025
Iteration 8/25 | Loss: 0.00111560
Iteration 9/25 | Loss: 0.00108890
Iteration 10/25 | Loss: 0.00107733
Iteration 11/25 | Loss: 0.00105714
Iteration 12/25 | Loss: 0.00104199
Iteration 13/25 | Loss: 0.00103996
Iteration 14/25 | Loss: 0.00103812
Iteration 15/25 | Loss: 0.00103856
Iteration 16/25 | Loss: 0.00103746
Iteration 17/25 | Loss: 0.00103660
Iteration 18/25 | Loss: 0.00103686
Iteration 19/25 | Loss: 0.00103596
Iteration 20/25 | Loss: 0.00103624
Iteration 21/25 | Loss: 0.00103571
Iteration 22/25 | Loss: 0.00103517
Iteration 23/25 | Loss: 0.00103360
Iteration 24/25 | Loss: 0.00103354
Iteration 25/25 | Loss: 0.00103437

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43081892
Iteration 2/25 | Loss: 0.00123355
Iteration 3/25 | Loss: 0.00123345
Iteration 4/25 | Loss: 0.00116545
Iteration 5/25 | Loss: 0.00116545
Iteration 6/25 | Loss: 0.00116545
Iteration 7/25 | Loss: 0.00116545
Iteration 8/25 | Loss: 0.00116545
Iteration 9/25 | Loss: 0.00116545
Iteration 10/25 | Loss: 0.00116545
Iteration 11/25 | Loss: 0.00116545
Iteration 12/25 | Loss: 0.00116545
Iteration 13/25 | Loss: 0.00116545
Iteration 14/25 | Loss: 0.00116545
Iteration 15/25 | Loss: 0.00116545
Iteration 16/25 | Loss: 0.00116545
Iteration 17/25 | Loss: 0.00116545
Iteration 18/25 | Loss: 0.00116545
Iteration 19/25 | Loss: 0.00116545
Iteration 20/25 | Loss: 0.00116545
Iteration 21/25 | Loss: 0.00116545
Iteration 22/25 | Loss: 0.00116545
Iteration 23/25 | Loss: 0.00116545
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0011654451955109835, 0.0011654451955109835, 0.0011654451955109835, 0.0011654451955109835, 0.0011654451955109835]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011654451955109835

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00116545
Iteration 2/1000 | Loss: 0.00032589
Iteration 3/1000 | Loss: 0.00048817
Iteration 4/1000 | Loss: 0.00012103
Iteration 5/1000 | Loss: 0.00007475
Iteration 6/1000 | Loss: 0.00001982
Iteration 7/1000 | Loss: 0.00001796
Iteration 8/1000 | Loss: 0.00005646
Iteration 9/1000 | Loss: 0.00001519
Iteration 10/1000 | Loss: 0.00004749
Iteration 11/1000 | Loss: 0.00001458
Iteration 12/1000 | Loss: 0.00001425
Iteration 13/1000 | Loss: 0.00001399
Iteration 14/1000 | Loss: 0.00001378
Iteration 15/1000 | Loss: 0.00001377
Iteration 16/1000 | Loss: 0.00001356
Iteration 17/1000 | Loss: 0.00001345
Iteration 18/1000 | Loss: 0.00001339
Iteration 19/1000 | Loss: 0.00001336
Iteration 20/1000 | Loss: 0.00001335
Iteration 21/1000 | Loss: 0.00001334
Iteration 22/1000 | Loss: 0.00001330
Iteration 23/1000 | Loss: 0.00001327
Iteration 24/1000 | Loss: 0.00001325
Iteration 25/1000 | Loss: 0.00001325
Iteration 26/1000 | Loss: 0.00001325
Iteration 27/1000 | Loss: 0.00001325
Iteration 28/1000 | Loss: 0.00001324
Iteration 29/1000 | Loss: 0.00001324
Iteration 30/1000 | Loss: 0.00001324
Iteration 31/1000 | Loss: 0.00001324
Iteration 32/1000 | Loss: 0.00001323
Iteration 33/1000 | Loss: 0.00001323
Iteration 34/1000 | Loss: 0.00001323
Iteration 35/1000 | Loss: 0.00001323
Iteration 36/1000 | Loss: 0.00001323
Iteration 37/1000 | Loss: 0.00001322
Iteration 38/1000 | Loss: 0.00001322
Iteration 39/1000 | Loss: 0.00001322
Iteration 40/1000 | Loss: 0.00001322
Iteration 41/1000 | Loss: 0.00001322
Iteration 42/1000 | Loss: 0.00001322
Iteration 43/1000 | Loss: 0.00001322
Iteration 44/1000 | Loss: 0.00001322
Iteration 45/1000 | Loss: 0.00001321
Iteration 46/1000 | Loss: 0.00001321
Iteration 47/1000 | Loss: 0.00001321
Iteration 48/1000 | Loss: 0.00001321
Iteration 49/1000 | Loss: 0.00001321
Iteration 50/1000 | Loss: 0.00001321
Iteration 51/1000 | Loss: 0.00001320
Iteration 52/1000 | Loss: 0.00001319
Iteration 53/1000 | Loss: 0.00001319
Iteration 54/1000 | Loss: 0.00001319
Iteration 55/1000 | Loss: 0.00001319
Iteration 56/1000 | Loss: 0.00001319
Iteration 57/1000 | Loss: 0.00001318
Iteration 58/1000 | Loss: 0.00001318
Iteration 59/1000 | Loss: 0.00001318
Iteration 60/1000 | Loss: 0.00001318
Iteration 61/1000 | Loss: 0.00001318
Iteration 62/1000 | Loss: 0.00001317
Iteration 63/1000 | Loss: 0.00001317
Iteration 64/1000 | Loss: 0.00001317
Iteration 65/1000 | Loss: 0.00001317
Iteration 66/1000 | Loss: 0.00001317
Iteration 67/1000 | Loss: 0.00001317
Iteration 68/1000 | Loss: 0.00001317
Iteration 69/1000 | Loss: 0.00001316
Iteration 70/1000 | Loss: 0.00001316
Iteration 71/1000 | Loss: 0.00001315
Iteration 72/1000 | Loss: 0.00001315
Iteration 73/1000 | Loss: 0.00001315
Iteration 74/1000 | Loss: 0.00001315
Iteration 75/1000 | Loss: 0.00001315
Iteration 76/1000 | Loss: 0.00001315
Iteration 77/1000 | Loss: 0.00001315
Iteration 78/1000 | Loss: 0.00001315
Iteration 79/1000 | Loss: 0.00001315
Iteration 80/1000 | Loss: 0.00001314
Iteration 81/1000 | Loss: 0.00001314
Iteration 82/1000 | Loss: 0.00001313
Iteration 83/1000 | Loss: 0.00001313
Iteration 84/1000 | Loss: 0.00001313
Iteration 85/1000 | Loss: 0.00001313
Iteration 86/1000 | Loss: 0.00001312
Iteration 87/1000 | Loss: 0.00001312
Iteration 88/1000 | Loss: 0.00001312
Iteration 89/1000 | Loss: 0.00001312
Iteration 90/1000 | Loss: 0.00001312
Iteration 91/1000 | Loss: 0.00001312
Iteration 92/1000 | Loss: 0.00001311
Iteration 93/1000 | Loss: 0.00001311
Iteration 94/1000 | Loss: 0.00001311
Iteration 95/1000 | Loss: 0.00001311
Iteration 96/1000 | Loss: 0.00001311
Iteration 97/1000 | Loss: 0.00001311
Iteration 98/1000 | Loss: 0.00001311
Iteration 99/1000 | Loss: 0.00001310
Iteration 100/1000 | Loss: 0.00001310
Iteration 101/1000 | Loss: 0.00001309
Iteration 102/1000 | Loss: 0.00001309
Iteration 103/1000 | Loss: 0.00001309
Iteration 104/1000 | Loss: 0.00001308
Iteration 105/1000 | Loss: 0.00001308
Iteration 106/1000 | Loss: 0.00001308
Iteration 107/1000 | Loss: 0.00001308
Iteration 108/1000 | Loss: 0.00001308
Iteration 109/1000 | Loss: 0.00001308
Iteration 110/1000 | Loss: 0.00001308
Iteration 111/1000 | Loss: 0.00001308
Iteration 112/1000 | Loss: 0.00001308
Iteration 113/1000 | Loss: 0.00001308
Iteration 114/1000 | Loss: 0.00001307
Iteration 115/1000 | Loss: 0.00001307
Iteration 116/1000 | Loss: 0.00001307
Iteration 117/1000 | Loss: 0.00001307
Iteration 118/1000 | Loss: 0.00001307
Iteration 119/1000 | Loss: 0.00001307
Iteration 120/1000 | Loss: 0.00001306
Iteration 121/1000 | Loss: 0.00001306
Iteration 122/1000 | Loss: 0.00001306
Iteration 123/1000 | Loss: 0.00001306
Iteration 124/1000 | Loss: 0.00001306
Iteration 125/1000 | Loss: 0.00001306
Iteration 126/1000 | Loss: 0.00001306
Iteration 127/1000 | Loss: 0.00001305
Iteration 128/1000 | Loss: 0.00001305
Iteration 129/1000 | Loss: 0.00001305
Iteration 130/1000 | Loss: 0.00001305
Iteration 131/1000 | Loss: 0.00001305
Iteration 132/1000 | Loss: 0.00001305
Iteration 133/1000 | Loss: 0.00001305
Iteration 134/1000 | Loss: 0.00001305
Iteration 135/1000 | Loss: 0.00001305
Iteration 136/1000 | Loss: 0.00001305
Iteration 137/1000 | Loss: 0.00001304
Iteration 138/1000 | Loss: 0.00001304
Iteration 139/1000 | Loss: 0.00001304
Iteration 140/1000 | Loss: 0.00001304
Iteration 141/1000 | Loss: 0.00001304
Iteration 142/1000 | Loss: 0.00001304
Iteration 143/1000 | Loss: 0.00001304
Iteration 144/1000 | Loss: 0.00001303
Iteration 145/1000 | Loss: 0.00001303
Iteration 146/1000 | Loss: 0.00001303
Iteration 147/1000 | Loss: 0.00001303
Iteration 148/1000 | Loss: 0.00001303
Iteration 149/1000 | Loss: 0.00001303
Iteration 150/1000 | Loss: 0.00001303
Iteration 151/1000 | Loss: 0.00001302
Iteration 152/1000 | Loss: 0.00001302
Iteration 153/1000 | Loss: 0.00001302
Iteration 154/1000 | Loss: 0.00001302
Iteration 155/1000 | Loss: 0.00001302
Iteration 156/1000 | Loss: 0.00001302
Iteration 157/1000 | Loss: 0.00001302
Iteration 158/1000 | Loss: 0.00001302
Iteration 159/1000 | Loss: 0.00001302
Iteration 160/1000 | Loss: 0.00001302
Iteration 161/1000 | Loss: 0.00001302
Iteration 162/1000 | Loss: 0.00001301
Iteration 163/1000 | Loss: 0.00001301
Iteration 164/1000 | Loss: 0.00001301
Iteration 165/1000 | Loss: 0.00001301
Iteration 166/1000 | Loss: 0.00001301
Iteration 167/1000 | Loss: 0.00001301
Iteration 168/1000 | Loss: 0.00001301
Iteration 169/1000 | Loss: 0.00001300
Iteration 170/1000 | Loss: 0.00001300
Iteration 171/1000 | Loss: 0.00001300
Iteration 172/1000 | Loss: 0.00001300
Iteration 173/1000 | Loss: 0.00001300
Iteration 174/1000 | Loss: 0.00001300
Iteration 175/1000 | Loss: 0.00001300
Iteration 176/1000 | Loss: 0.00001299
Iteration 177/1000 | Loss: 0.00001299
Iteration 178/1000 | Loss: 0.00001299
Iteration 179/1000 | Loss: 0.00001299
Iteration 180/1000 | Loss: 0.00001299
Iteration 181/1000 | Loss: 0.00001299
Iteration 182/1000 | Loss: 0.00001298
Iteration 183/1000 | Loss: 0.00001298
Iteration 184/1000 | Loss: 0.00001298
Iteration 185/1000 | Loss: 0.00001298
Iteration 186/1000 | Loss: 0.00001298
Iteration 187/1000 | Loss: 0.00001298
Iteration 188/1000 | Loss: 0.00001298
Iteration 189/1000 | Loss: 0.00001298
Iteration 190/1000 | Loss: 0.00001298
Iteration 191/1000 | Loss: 0.00001298
Iteration 192/1000 | Loss: 0.00001298
Iteration 193/1000 | Loss: 0.00001298
Iteration 194/1000 | Loss: 0.00001298
Iteration 195/1000 | Loss: 0.00001298
Iteration 196/1000 | Loss: 0.00001298
Iteration 197/1000 | Loss: 0.00001298
Iteration 198/1000 | Loss: 0.00001297
Iteration 199/1000 | Loss: 0.00001297
Iteration 200/1000 | Loss: 0.00001297
Iteration 201/1000 | Loss: 0.00001297
Iteration 202/1000 | Loss: 0.00001297
Iteration 203/1000 | Loss: 0.00001297
Iteration 204/1000 | Loss: 0.00001297
Iteration 205/1000 | Loss: 0.00001297
Iteration 206/1000 | Loss: 0.00001297
Iteration 207/1000 | Loss: 0.00001296
Iteration 208/1000 | Loss: 0.00001296
Iteration 209/1000 | Loss: 0.00001296
Iteration 210/1000 | Loss: 0.00001296
Iteration 211/1000 | Loss: 0.00001296
Iteration 212/1000 | Loss: 0.00001296
Iteration 213/1000 | Loss: 0.00001296
Iteration 214/1000 | Loss: 0.00001296
Iteration 215/1000 | Loss: 0.00001296
Iteration 216/1000 | Loss: 0.00001296
Iteration 217/1000 | Loss: 0.00001295
Iteration 218/1000 | Loss: 0.00001295
Iteration 219/1000 | Loss: 0.00001295
Iteration 220/1000 | Loss: 0.00001295
Iteration 221/1000 | Loss: 0.00001295
Iteration 222/1000 | Loss: 0.00001295
Iteration 223/1000 | Loss: 0.00001295
Iteration 224/1000 | Loss: 0.00001294
Iteration 225/1000 | Loss: 0.00001294
Iteration 226/1000 | Loss: 0.00001294
Iteration 227/1000 | Loss: 0.00001294
Iteration 228/1000 | Loss: 0.00001294
Iteration 229/1000 | Loss: 0.00010624
Iteration 230/1000 | Loss: 0.00007024
Iteration 231/1000 | Loss: 0.00024081
Iteration 232/1000 | Loss: 0.00013461
Iteration 233/1000 | Loss: 0.00003016
Iteration 234/1000 | Loss: 0.00003387
Iteration 235/1000 | Loss: 0.00002811
Iteration 236/1000 | Loss: 0.00001552
Iteration 237/1000 | Loss: 0.00003636
Iteration 238/1000 | Loss: 0.00001512
Iteration 239/1000 | Loss: 0.00001510
Iteration 240/1000 | Loss: 0.00001510
Iteration 241/1000 | Loss: 0.00001509
Iteration 242/1000 | Loss: 0.00001509
Iteration 243/1000 | Loss: 0.00001508
Iteration 244/1000 | Loss: 0.00001507
Iteration 245/1000 | Loss: 0.00001506
Iteration 246/1000 | Loss: 0.00001506
Iteration 247/1000 | Loss: 0.00001505
Iteration 248/1000 | Loss: 0.00001505
Iteration 249/1000 | Loss: 0.00001504
Iteration 250/1000 | Loss: 0.00001499
Iteration 251/1000 | Loss: 0.00001492
Iteration 252/1000 | Loss: 0.00001483
Iteration 253/1000 | Loss: 0.00001481
Iteration 254/1000 | Loss: 0.00005164
Iteration 255/1000 | Loss: 0.00026872
Iteration 256/1000 | Loss: 0.00018480
Iteration 257/1000 | Loss: 0.00008486
Iteration 258/1000 | Loss: 0.00056967
Iteration 259/1000 | Loss: 0.00005961
Iteration 260/1000 | Loss: 0.00003479
Iteration 261/1000 | Loss: 0.00004310
Iteration 262/1000 | Loss: 0.00001840
Iteration 263/1000 | Loss: 0.00002127
Iteration 264/1000 | Loss: 0.00001470
Iteration 265/1000 | Loss: 0.00009152
Iteration 266/1000 | Loss: 0.00065704
Iteration 267/1000 | Loss: 0.00001411
Iteration 268/1000 | Loss: 0.00007533
Iteration 269/1000 | Loss: 0.00001379
Iteration 270/1000 | Loss: 0.00001365
Iteration 271/1000 | Loss: 0.00012929
Iteration 272/1000 | Loss: 0.00020720
Iteration 273/1000 | Loss: 0.00001738
Iteration 274/1000 | Loss: 0.00001420
Iteration 275/1000 | Loss: 0.00001322
Iteration 276/1000 | Loss: 0.00003702
Iteration 277/1000 | Loss: 0.00001289
Iteration 278/1000 | Loss: 0.00001283
Iteration 279/1000 | Loss: 0.00001283
Iteration 280/1000 | Loss: 0.00001281
Iteration 281/1000 | Loss: 0.00001277
Iteration 282/1000 | Loss: 0.00001276
Iteration 283/1000 | Loss: 0.00001276
Iteration 284/1000 | Loss: 0.00001275
Iteration 285/1000 | Loss: 0.00001275
Iteration 286/1000 | Loss: 0.00001275
Iteration 287/1000 | Loss: 0.00001274
Iteration 288/1000 | Loss: 0.00001274
Iteration 289/1000 | Loss: 0.00001273
Iteration 290/1000 | Loss: 0.00001271
Iteration 291/1000 | Loss: 0.00001270
Iteration 292/1000 | Loss: 0.00001270
Iteration 293/1000 | Loss: 0.00001269
Iteration 294/1000 | Loss: 0.00001267
Iteration 295/1000 | Loss: 0.00001267
Iteration 296/1000 | Loss: 0.00001267
Iteration 297/1000 | Loss: 0.00001267
Iteration 298/1000 | Loss: 0.00001267
Iteration 299/1000 | Loss: 0.00001266
Iteration 300/1000 | Loss: 0.00001265
Iteration 301/1000 | Loss: 0.00001265
Iteration 302/1000 | Loss: 0.00001264
Iteration 303/1000 | Loss: 0.00001264
Iteration 304/1000 | Loss: 0.00001263
Iteration 305/1000 | Loss: 0.00001263
Iteration 306/1000 | Loss: 0.00001263
Iteration 307/1000 | Loss: 0.00001263
Iteration 308/1000 | Loss: 0.00001263
Iteration 309/1000 | Loss: 0.00001262
Iteration 310/1000 | Loss: 0.00001261
Iteration 311/1000 | Loss: 0.00001261
Iteration 312/1000 | Loss: 0.00001261
Iteration 313/1000 | Loss: 0.00001261
Iteration 314/1000 | Loss: 0.00001261
Iteration 315/1000 | Loss: 0.00001261
Iteration 316/1000 | Loss: 0.00001261
Iteration 317/1000 | Loss: 0.00001260
Iteration 318/1000 | Loss: 0.00001260
Iteration 319/1000 | Loss: 0.00001260
Iteration 320/1000 | Loss: 0.00001259
Iteration 321/1000 | Loss: 0.00001259
Iteration 322/1000 | Loss: 0.00001259
Iteration 323/1000 | Loss: 0.00001259
Iteration 324/1000 | Loss: 0.00001259
Iteration 325/1000 | Loss: 0.00001259
Iteration 326/1000 | Loss: 0.00001259
Iteration 327/1000 | Loss: 0.00001259
Iteration 328/1000 | Loss: 0.00001258
Iteration 329/1000 | Loss: 0.00001258
Iteration 330/1000 | Loss: 0.00001258
Iteration 331/1000 | Loss: 0.00001258
Iteration 332/1000 | Loss: 0.00001258
Iteration 333/1000 | Loss: 0.00001258
Iteration 334/1000 | Loss: 0.00001258
Iteration 335/1000 | Loss: 0.00001258
Iteration 336/1000 | Loss: 0.00001258
Iteration 337/1000 | Loss: 0.00001258
Iteration 338/1000 | Loss: 0.00001258
Iteration 339/1000 | Loss: 0.00001258
Iteration 340/1000 | Loss: 0.00001258
Iteration 341/1000 | Loss: 0.00001258
Iteration 342/1000 | Loss: 0.00001258
Iteration 343/1000 | Loss: 0.00001258
Iteration 344/1000 | Loss: 0.00001258
Iteration 345/1000 | Loss: 0.00001258
Iteration 346/1000 | Loss: 0.00001258
Iteration 347/1000 | Loss: 0.00001258
Iteration 348/1000 | Loss: 0.00001258
Iteration 349/1000 | Loss: 0.00001258
Iteration 350/1000 | Loss: 0.00001258
Iteration 351/1000 | Loss: 0.00001258
Iteration 352/1000 | Loss: 0.00001258
Iteration 353/1000 | Loss: 0.00001258
Iteration 354/1000 | Loss: 0.00001258
Iteration 355/1000 | Loss: 0.00001258
Iteration 356/1000 | Loss: 0.00001258
Iteration 357/1000 | Loss: 0.00001258
Iteration 358/1000 | Loss: 0.00001258
Iteration 359/1000 | Loss: 0.00001258
Iteration 360/1000 | Loss: 0.00001258
Iteration 361/1000 | Loss: 0.00001258
Iteration 362/1000 | Loss: 0.00001258
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 362. Stopping optimization.
Last 5 losses: [1.2581002920342144e-05, 1.2581002920342144e-05, 1.2581002920342144e-05, 1.2581002920342144e-05, 1.2581002920342144e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2581002920342144e-05

Optimization complete. Final v2v error: 2.933743476867676 mm

Highest mean error: 4.7194952964782715 mm for frame 198

Lowest mean error: 2.3276801109313965 mm for frame 15

Saving results

Total time: 160.6204583644867
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_grace_posed_004/1085/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_grace_posed_004/1085.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_grace_posed_004/1085
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00740055
Iteration 2/25 | Loss: 0.00150827
Iteration 3/25 | Loss: 0.00116211
Iteration 4/25 | Loss: 0.00112073
Iteration 5/25 | Loss: 0.00111935
Iteration 6/25 | Loss: 0.00111142
Iteration 7/25 | Loss: 0.00111016
Iteration 8/25 | Loss: 0.00110989
Iteration 9/25 | Loss: 0.00110979
Iteration 10/25 | Loss: 0.00110975
Iteration 11/25 | Loss: 0.00110975
Iteration 12/25 | Loss: 0.00110975
Iteration 13/25 | Loss: 0.00110975
Iteration 14/25 | Loss: 0.00110975
Iteration 15/25 | Loss: 0.00110974
Iteration 16/25 | Loss: 0.00110974
Iteration 17/25 | Loss: 0.00110974
Iteration 18/25 | Loss: 0.00110974
Iteration 19/25 | Loss: 0.00110974
Iteration 20/25 | Loss: 0.00110974
Iteration 21/25 | Loss: 0.00110974
Iteration 22/25 | Loss: 0.00110974
Iteration 23/25 | Loss: 0.00110974
Iteration 24/25 | Loss: 0.00110974
Iteration 25/25 | Loss: 0.00110974

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.89192414
Iteration 2/25 | Loss: 0.00070099
Iteration 3/25 | Loss: 0.00070096
Iteration 4/25 | Loss: 0.00070096
Iteration 5/25 | Loss: 0.00070096
Iteration 6/25 | Loss: 0.00070096
Iteration 7/25 | Loss: 0.00070096
Iteration 8/25 | Loss: 0.00070096
Iteration 9/25 | Loss: 0.00070096
Iteration 10/25 | Loss: 0.00070096
Iteration 11/25 | Loss: 0.00070096
Iteration 12/25 | Loss: 0.00070096
Iteration 13/25 | Loss: 0.00070096
Iteration 14/25 | Loss: 0.00070096
Iteration 15/25 | Loss: 0.00070096
Iteration 16/25 | Loss: 0.00070096
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0007009620894677937, 0.0007009620894677937, 0.0007009620894677937, 0.0007009620894677937, 0.0007009620894677937]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007009620894677937

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00070096
Iteration 2/1000 | Loss: 0.00002769
Iteration 3/1000 | Loss: 0.00002110
Iteration 4/1000 | Loss: 0.00001957
Iteration 5/1000 | Loss: 0.00001878
Iteration 6/1000 | Loss: 0.00001821
Iteration 7/1000 | Loss: 0.00001766
Iteration 8/1000 | Loss: 0.00001735
Iteration 9/1000 | Loss: 0.00001715
Iteration 10/1000 | Loss: 0.00001697
Iteration 11/1000 | Loss: 0.00001682
Iteration 12/1000 | Loss: 0.00001680
Iteration 13/1000 | Loss: 0.00001677
Iteration 14/1000 | Loss: 0.00001677
Iteration 15/1000 | Loss: 0.00001676
Iteration 16/1000 | Loss: 0.00001672
Iteration 17/1000 | Loss: 0.00001672
Iteration 18/1000 | Loss: 0.00001671
Iteration 19/1000 | Loss: 0.00001668
Iteration 20/1000 | Loss: 0.00001668
Iteration 21/1000 | Loss: 0.00001668
Iteration 22/1000 | Loss: 0.00001667
Iteration 23/1000 | Loss: 0.00001667
Iteration 24/1000 | Loss: 0.00001667
Iteration 25/1000 | Loss: 0.00001666
Iteration 26/1000 | Loss: 0.00001666
Iteration 27/1000 | Loss: 0.00001666
Iteration 28/1000 | Loss: 0.00001666
Iteration 29/1000 | Loss: 0.00001666
Iteration 30/1000 | Loss: 0.00001665
Iteration 31/1000 | Loss: 0.00017358
Iteration 32/1000 | Loss: 0.00002100
Iteration 33/1000 | Loss: 0.00001904
Iteration 34/1000 | Loss: 0.00001787
Iteration 35/1000 | Loss: 0.00001718
Iteration 36/1000 | Loss: 0.00001648
Iteration 37/1000 | Loss: 0.00001618
Iteration 38/1000 | Loss: 0.00001618
Iteration 39/1000 | Loss: 0.00001618
Iteration 40/1000 | Loss: 0.00001617
Iteration 41/1000 | Loss: 0.00001616
Iteration 42/1000 | Loss: 0.00001616
Iteration 43/1000 | Loss: 0.00001616
Iteration 44/1000 | Loss: 0.00001616
Iteration 45/1000 | Loss: 0.00001616
Iteration 46/1000 | Loss: 0.00001616
Iteration 47/1000 | Loss: 0.00001616
Iteration 48/1000 | Loss: 0.00001616
Iteration 49/1000 | Loss: 0.00001615
Iteration 50/1000 | Loss: 0.00001615
Iteration 51/1000 | Loss: 0.00001614
Iteration 52/1000 | Loss: 0.00001614
Iteration 53/1000 | Loss: 0.00001614
Iteration 54/1000 | Loss: 0.00001613
Iteration 55/1000 | Loss: 0.00001612
Iteration 56/1000 | Loss: 0.00001612
Iteration 57/1000 | Loss: 0.00001612
Iteration 58/1000 | Loss: 0.00001612
Iteration 59/1000 | Loss: 0.00001611
Iteration 60/1000 | Loss: 0.00001611
Iteration 61/1000 | Loss: 0.00001611
Iteration 62/1000 | Loss: 0.00001610
Iteration 63/1000 | Loss: 0.00001610
Iteration 64/1000 | Loss: 0.00001609
Iteration 65/1000 | Loss: 0.00001609
Iteration 66/1000 | Loss: 0.00001609
Iteration 67/1000 | Loss: 0.00001608
Iteration 68/1000 | Loss: 0.00001608
Iteration 69/1000 | Loss: 0.00001608
Iteration 70/1000 | Loss: 0.00001607
Iteration 71/1000 | Loss: 0.00001607
Iteration 72/1000 | Loss: 0.00001607
Iteration 73/1000 | Loss: 0.00001607
Iteration 74/1000 | Loss: 0.00001606
Iteration 75/1000 | Loss: 0.00001606
Iteration 76/1000 | Loss: 0.00001606
Iteration 77/1000 | Loss: 0.00001605
Iteration 78/1000 | Loss: 0.00001605
Iteration 79/1000 | Loss: 0.00001605
Iteration 80/1000 | Loss: 0.00001605
Iteration 81/1000 | Loss: 0.00001605
Iteration 82/1000 | Loss: 0.00001605
Iteration 83/1000 | Loss: 0.00001605
Iteration 84/1000 | Loss: 0.00001604
Iteration 85/1000 | Loss: 0.00001604
Iteration 86/1000 | Loss: 0.00001604
Iteration 87/1000 | Loss: 0.00001604
Iteration 88/1000 | Loss: 0.00001604
Iteration 89/1000 | Loss: 0.00001604
Iteration 90/1000 | Loss: 0.00001604
Iteration 91/1000 | Loss: 0.00001603
Iteration 92/1000 | Loss: 0.00001603
Iteration 93/1000 | Loss: 0.00001603
Iteration 94/1000 | Loss: 0.00001602
Iteration 95/1000 | Loss: 0.00001602
Iteration 96/1000 | Loss: 0.00001602
Iteration 97/1000 | Loss: 0.00001602
Iteration 98/1000 | Loss: 0.00001602
Iteration 99/1000 | Loss: 0.00001602
Iteration 100/1000 | Loss: 0.00001602
Iteration 101/1000 | Loss: 0.00001602
Iteration 102/1000 | Loss: 0.00001602
Iteration 103/1000 | Loss: 0.00001602
Iteration 104/1000 | Loss: 0.00001602
Iteration 105/1000 | Loss: 0.00001602
Iteration 106/1000 | Loss: 0.00001602
Iteration 107/1000 | Loss: 0.00001602
Iteration 108/1000 | Loss: 0.00001602
Iteration 109/1000 | Loss: 0.00001602
Iteration 110/1000 | Loss: 0.00001602
Iteration 111/1000 | Loss: 0.00001602
Iteration 112/1000 | Loss: 0.00001602
Iteration 113/1000 | Loss: 0.00001602
Iteration 114/1000 | Loss: 0.00001602
Iteration 115/1000 | Loss: 0.00001602
Iteration 116/1000 | Loss: 0.00001602
Iteration 117/1000 | Loss: 0.00001602
Iteration 118/1000 | Loss: 0.00001602
Iteration 119/1000 | Loss: 0.00001602
Iteration 120/1000 | Loss: 0.00001602
Iteration 121/1000 | Loss: 0.00001602
Iteration 122/1000 | Loss: 0.00001602
Iteration 123/1000 | Loss: 0.00001602
Iteration 124/1000 | Loss: 0.00001602
Iteration 125/1000 | Loss: 0.00001602
Iteration 126/1000 | Loss: 0.00001602
Iteration 127/1000 | Loss: 0.00001602
Iteration 128/1000 | Loss: 0.00001602
Iteration 129/1000 | Loss: 0.00001602
Iteration 130/1000 | Loss: 0.00001602
Iteration 131/1000 | Loss: 0.00001602
Iteration 132/1000 | Loss: 0.00001602
Iteration 133/1000 | Loss: 0.00001602
Iteration 134/1000 | Loss: 0.00001602
Iteration 135/1000 | Loss: 0.00001602
Iteration 136/1000 | Loss: 0.00001602
Iteration 137/1000 | Loss: 0.00001602
Iteration 138/1000 | Loss: 0.00001602
Iteration 139/1000 | Loss: 0.00001602
Iteration 140/1000 | Loss: 0.00001602
Iteration 141/1000 | Loss: 0.00001602
Iteration 142/1000 | Loss: 0.00001602
Iteration 143/1000 | Loss: 0.00001602
Iteration 144/1000 | Loss: 0.00001602
Iteration 145/1000 | Loss: 0.00001602
Iteration 146/1000 | Loss: 0.00001602
Iteration 147/1000 | Loss: 0.00001602
Iteration 148/1000 | Loss: 0.00001602
Iteration 149/1000 | Loss: 0.00001602
Iteration 150/1000 | Loss: 0.00001602
Iteration 151/1000 | Loss: 0.00001602
Iteration 152/1000 | Loss: 0.00001602
Iteration 153/1000 | Loss: 0.00001602
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 153. Stopping optimization.
Last 5 losses: [1.6016661902540363e-05, 1.6016661902540363e-05, 1.6016661902540363e-05, 1.6016661902540363e-05, 1.6016661902540363e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6016661902540363e-05

Optimization complete. Final v2v error: 3.355111837387085 mm

Highest mean error: 5.02446985244751 mm for frame 202

Lowest mean error: 2.962918519973755 mm for frame 168

Saving results

Total time: 57.02558970451355
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_grace_posed_004/1000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_grace_posed_004/1000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_grace_posed_004/1000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01062984
Iteration 2/25 | Loss: 0.00181198
Iteration 3/25 | Loss: 0.00126167
Iteration 4/25 | Loss: 0.00123359
Iteration 5/25 | Loss: 0.00118622
Iteration 6/25 | Loss: 0.00113536
Iteration 7/25 | Loss: 0.00112664
Iteration 8/25 | Loss: 0.00112397
Iteration 9/25 | Loss: 0.00111249
Iteration 10/25 | Loss: 0.00110928
Iteration 11/25 | Loss: 0.00111702
Iteration 12/25 | Loss: 0.00111264
Iteration 13/25 | Loss: 0.00112576
Iteration 14/25 | Loss: 0.00109592
Iteration 15/25 | Loss: 0.00109245
Iteration 16/25 | Loss: 0.00109732
Iteration 17/25 | Loss: 0.00108989
Iteration 18/25 | Loss: 0.00108889
Iteration 19/25 | Loss: 0.00109141
Iteration 20/25 | Loss: 0.00108666
Iteration 21/25 | Loss: 0.00108461
Iteration 22/25 | Loss: 0.00108665
Iteration 23/25 | Loss: 0.00109113
Iteration 24/25 | Loss: 0.00108573
Iteration 25/25 | Loss: 0.00108412

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 7.01767159
Iteration 2/25 | Loss: 0.00142903
Iteration 3/25 | Loss: 0.00140786
Iteration 4/25 | Loss: 0.00140786
Iteration 5/25 | Loss: 0.00140786
Iteration 6/25 | Loss: 0.00140786
Iteration 7/25 | Loss: 0.00140786
Iteration 8/25 | Loss: 0.00140786
Iteration 9/25 | Loss: 0.00140786
Iteration 10/25 | Loss: 0.00140786
Iteration 11/25 | Loss: 0.00140786
Iteration 12/25 | Loss: 0.00140786
Iteration 13/25 | Loss: 0.00140786
Iteration 14/25 | Loss: 0.00140786
Iteration 15/25 | Loss: 0.00140786
Iteration 16/25 | Loss: 0.00140786
Iteration 17/25 | Loss: 0.00140786
Iteration 18/25 | Loss: 0.00140786
Iteration 19/25 | Loss: 0.00140786
Iteration 20/25 | Loss: 0.00140786
Iteration 21/25 | Loss: 0.00140786
Iteration 22/25 | Loss: 0.00140786
Iteration 23/25 | Loss: 0.00140786
Iteration 24/25 | Loss: 0.00140786
Iteration 25/25 | Loss: 0.00140786

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00140786
Iteration 2/1000 | Loss: 0.00012858
Iteration 3/1000 | Loss: 0.00004912
Iteration 4/1000 | Loss: 0.00017517
Iteration 5/1000 | Loss: 0.00004399
Iteration 6/1000 | Loss: 0.00003658
Iteration 7/1000 | Loss: 0.00022750
Iteration 8/1000 | Loss: 0.00008630
Iteration 9/1000 | Loss: 0.00005620
Iteration 10/1000 | Loss: 0.00004129
Iteration 11/1000 | Loss: 0.00004195
Iteration 12/1000 | Loss: 0.00004166
Iteration 13/1000 | Loss: 0.00013125
Iteration 14/1000 | Loss: 0.00007283
Iteration 15/1000 | Loss: 0.00011097
Iteration 16/1000 | Loss: 0.00004964
Iteration 17/1000 | Loss: 0.00015390
Iteration 18/1000 | Loss: 0.00005163
Iteration 19/1000 | Loss: 0.00011885
Iteration 20/1000 | Loss: 0.00013475
Iteration 21/1000 | Loss: 0.00019560
Iteration 22/1000 | Loss: 0.00066795
Iteration 23/1000 | Loss: 0.00009896
Iteration 24/1000 | Loss: 0.00005404
Iteration 25/1000 | Loss: 0.00005635
Iteration 26/1000 | Loss: 0.00003563
Iteration 27/1000 | Loss: 0.00003295
Iteration 28/1000 | Loss: 0.00004632
Iteration 29/1000 | Loss: 0.00003785
Iteration 30/1000 | Loss: 0.00003938
Iteration 31/1000 | Loss: 0.00003285
Iteration 32/1000 | Loss: 0.00003657
Iteration 33/1000 | Loss: 0.00003363
Iteration 34/1000 | Loss: 0.00003322
Iteration 35/1000 | Loss: 0.00004362
Iteration 36/1000 | Loss: 0.00004231
Iteration 37/1000 | Loss: 0.00004353
Iteration 38/1000 | Loss: 0.00003614
Iteration 39/1000 | Loss: 0.00003210
Iteration 40/1000 | Loss: 0.00004238
Iteration 41/1000 | Loss: 0.00004543
Iteration 42/1000 | Loss: 0.00004785
Iteration 43/1000 | Loss: 0.00004093
Iteration 44/1000 | Loss: 0.00003118
Iteration 45/1000 | Loss: 0.00004913
Iteration 46/1000 | Loss: 0.00005063
Iteration 47/1000 | Loss: 0.00004765
Iteration 48/1000 | Loss: 0.00003427
Iteration 49/1000 | Loss: 0.00003793
Iteration 50/1000 | Loss: 0.00004919
Iteration 51/1000 | Loss: 0.00002759
Iteration 52/1000 | Loss: 0.00002556
Iteration 53/1000 | Loss: 0.00002465
Iteration 54/1000 | Loss: 0.00002426
Iteration 55/1000 | Loss: 0.00002396
Iteration 56/1000 | Loss: 0.00002360
Iteration 57/1000 | Loss: 0.00002324
Iteration 58/1000 | Loss: 0.00002301
Iteration 59/1000 | Loss: 0.00002289
Iteration 60/1000 | Loss: 0.00002286
Iteration 61/1000 | Loss: 0.00002282
Iteration 62/1000 | Loss: 0.00002281
Iteration 63/1000 | Loss: 0.00002280
Iteration 64/1000 | Loss: 0.00002280
Iteration 65/1000 | Loss: 0.00002279
Iteration 66/1000 | Loss: 0.00002279
Iteration 67/1000 | Loss: 0.00002278
Iteration 68/1000 | Loss: 0.00002266
Iteration 69/1000 | Loss: 0.00002266
Iteration 70/1000 | Loss: 0.00002259
Iteration 71/1000 | Loss: 0.00002259
Iteration 72/1000 | Loss: 0.00002258
Iteration 73/1000 | Loss: 0.00002257
Iteration 74/1000 | Loss: 0.00002256
Iteration 75/1000 | Loss: 0.00002256
Iteration 76/1000 | Loss: 0.00002256
Iteration 77/1000 | Loss: 0.00002255
Iteration 78/1000 | Loss: 0.00002255
Iteration 79/1000 | Loss: 0.00002255
Iteration 80/1000 | Loss: 0.00002255
Iteration 81/1000 | Loss: 0.00002254
Iteration 82/1000 | Loss: 0.00002254
Iteration 83/1000 | Loss: 0.00002254
Iteration 84/1000 | Loss: 0.00002254
Iteration 85/1000 | Loss: 0.00002254
Iteration 86/1000 | Loss: 0.00002254
Iteration 87/1000 | Loss: 0.00002254
Iteration 88/1000 | Loss: 0.00002254
Iteration 89/1000 | Loss: 0.00002254
Iteration 90/1000 | Loss: 0.00002254
Iteration 91/1000 | Loss: 0.00002253
Iteration 92/1000 | Loss: 0.00002252
Iteration 93/1000 | Loss: 0.00002252
Iteration 94/1000 | Loss: 0.00002251
Iteration 95/1000 | Loss: 0.00002251
Iteration 96/1000 | Loss: 0.00002251
Iteration 97/1000 | Loss: 0.00002251
Iteration 98/1000 | Loss: 0.00002250
Iteration 99/1000 | Loss: 0.00002250
Iteration 100/1000 | Loss: 0.00002249
Iteration 101/1000 | Loss: 0.00002249
Iteration 102/1000 | Loss: 0.00002248
Iteration 103/1000 | Loss: 0.00002248
Iteration 104/1000 | Loss: 0.00002248
Iteration 105/1000 | Loss: 0.00002247
Iteration 106/1000 | Loss: 0.00002247
Iteration 107/1000 | Loss: 0.00002245
Iteration 108/1000 | Loss: 0.00002245
Iteration 109/1000 | Loss: 0.00002244
Iteration 110/1000 | Loss: 0.00002244
Iteration 111/1000 | Loss: 0.00002244
Iteration 112/1000 | Loss: 0.00002243
Iteration 113/1000 | Loss: 0.00002243
Iteration 114/1000 | Loss: 0.00002243
Iteration 115/1000 | Loss: 0.00002242
Iteration 116/1000 | Loss: 0.00002242
Iteration 117/1000 | Loss: 0.00002239
Iteration 118/1000 | Loss: 0.00002239
Iteration 119/1000 | Loss: 0.00002239
Iteration 120/1000 | Loss: 0.00002238
Iteration 121/1000 | Loss: 0.00002237
Iteration 122/1000 | Loss: 0.00002237
Iteration 123/1000 | Loss: 0.00002236
Iteration 124/1000 | Loss: 0.00002236
Iteration 125/1000 | Loss: 0.00002236
Iteration 126/1000 | Loss: 0.00002235
Iteration 127/1000 | Loss: 0.00002235
Iteration 128/1000 | Loss: 0.00002234
Iteration 129/1000 | Loss: 0.00002232
Iteration 130/1000 | Loss: 0.00002231
Iteration 131/1000 | Loss: 0.00002231
Iteration 132/1000 | Loss: 0.00002230
Iteration 133/1000 | Loss: 0.00002230
Iteration 134/1000 | Loss: 0.00002229
Iteration 135/1000 | Loss: 0.00002229
Iteration 136/1000 | Loss: 0.00002229
Iteration 137/1000 | Loss: 0.00002228
Iteration 138/1000 | Loss: 0.00002228
Iteration 139/1000 | Loss: 0.00002228
Iteration 140/1000 | Loss: 0.00002228
Iteration 141/1000 | Loss: 0.00002227
Iteration 142/1000 | Loss: 0.00002227
Iteration 143/1000 | Loss: 0.00002227
Iteration 144/1000 | Loss: 0.00002227
Iteration 145/1000 | Loss: 0.00002227
Iteration 146/1000 | Loss: 0.00002227
Iteration 147/1000 | Loss: 0.00002227
Iteration 148/1000 | Loss: 0.00002227
Iteration 149/1000 | Loss: 0.00002227
Iteration 150/1000 | Loss: 0.00002227
Iteration 151/1000 | Loss: 0.00002226
Iteration 152/1000 | Loss: 0.00002226
Iteration 153/1000 | Loss: 0.00002226
Iteration 154/1000 | Loss: 0.00002226
Iteration 155/1000 | Loss: 0.00002226
Iteration 156/1000 | Loss: 0.00002226
Iteration 157/1000 | Loss: 0.00002226
Iteration 158/1000 | Loss: 0.00002226
Iteration 159/1000 | Loss: 0.00002225
Iteration 160/1000 | Loss: 0.00002225
Iteration 161/1000 | Loss: 0.00002225
Iteration 162/1000 | Loss: 0.00002225
Iteration 163/1000 | Loss: 0.00002225
Iteration 164/1000 | Loss: 0.00002225
Iteration 165/1000 | Loss: 0.00002225
Iteration 166/1000 | Loss: 0.00002225
Iteration 167/1000 | Loss: 0.00002224
Iteration 168/1000 | Loss: 0.00002224
Iteration 169/1000 | Loss: 0.00002224
Iteration 170/1000 | Loss: 0.00002224
Iteration 171/1000 | Loss: 0.00002223
Iteration 172/1000 | Loss: 0.00002223
Iteration 173/1000 | Loss: 0.00002223
Iteration 174/1000 | Loss: 0.00002222
Iteration 175/1000 | Loss: 0.00002222
Iteration 176/1000 | Loss: 0.00002222
Iteration 177/1000 | Loss: 0.00002222
Iteration 178/1000 | Loss: 0.00002222
Iteration 179/1000 | Loss: 0.00002222
Iteration 180/1000 | Loss: 0.00002222
Iteration 181/1000 | Loss: 0.00002222
Iteration 182/1000 | Loss: 0.00002222
Iteration 183/1000 | Loss: 0.00002222
Iteration 184/1000 | Loss: 0.00002222
Iteration 185/1000 | Loss: 0.00002222
Iteration 186/1000 | Loss: 0.00002222
Iteration 187/1000 | Loss: 0.00002222
Iteration 188/1000 | Loss: 0.00002221
Iteration 189/1000 | Loss: 0.00002221
Iteration 190/1000 | Loss: 0.00002221
Iteration 191/1000 | Loss: 0.00002221
Iteration 192/1000 | Loss: 0.00002221
Iteration 193/1000 | Loss: 0.00002221
Iteration 194/1000 | Loss: 0.00002221
Iteration 195/1000 | Loss: 0.00002221
Iteration 196/1000 | Loss: 0.00002221
Iteration 197/1000 | Loss: 0.00002221
Iteration 198/1000 | Loss: 0.00002221
Iteration 199/1000 | Loss: 0.00002221
Iteration 200/1000 | Loss: 0.00002221
Iteration 201/1000 | Loss: 0.00002220
Iteration 202/1000 | Loss: 0.00002220
Iteration 203/1000 | Loss: 0.00002220
Iteration 204/1000 | Loss: 0.00002220
Iteration 205/1000 | Loss: 0.00002220
Iteration 206/1000 | Loss: 0.00002220
Iteration 207/1000 | Loss: 0.00002220
Iteration 208/1000 | Loss: 0.00002220
Iteration 209/1000 | Loss: 0.00002220
Iteration 210/1000 | Loss: 0.00002220
Iteration 211/1000 | Loss: 0.00002220
Iteration 212/1000 | Loss: 0.00002220
Iteration 213/1000 | Loss: 0.00002220
Iteration 214/1000 | Loss: 0.00002220
Iteration 215/1000 | Loss: 0.00002220
Iteration 216/1000 | Loss: 0.00002220
Iteration 217/1000 | Loss: 0.00002219
Iteration 218/1000 | Loss: 0.00002219
Iteration 219/1000 | Loss: 0.00002219
Iteration 220/1000 | Loss: 0.00002219
Iteration 221/1000 | Loss: 0.00002219
Iteration 222/1000 | Loss: 0.00002219
Iteration 223/1000 | Loss: 0.00002219
Iteration 224/1000 | Loss: 0.00002219
Iteration 225/1000 | Loss: 0.00002219
Iteration 226/1000 | Loss: 0.00002218
Iteration 227/1000 | Loss: 0.00002218
Iteration 228/1000 | Loss: 0.00002218
Iteration 229/1000 | Loss: 0.00002218
Iteration 230/1000 | Loss: 0.00002218
Iteration 231/1000 | Loss: 0.00002218
Iteration 232/1000 | Loss: 0.00002218
Iteration 233/1000 | Loss: 0.00002218
Iteration 234/1000 | Loss: 0.00002217
Iteration 235/1000 | Loss: 0.00002217
Iteration 236/1000 | Loss: 0.00002217
Iteration 237/1000 | Loss: 0.00002217
Iteration 238/1000 | Loss: 0.00002217
Iteration 239/1000 | Loss: 0.00002217
Iteration 240/1000 | Loss: 0.00002217
Iteration 241/1000 | Loss: 0.00002217
Iteration 242/1000 | Loss: 0.00002217
Iteration 243/1000 | Loss: 0.00002217
Iteration 244/1000 | Loss: 0.00002217
Iteration 245/1000 | Loss: 0.00002217
Iteration 246/1000 | Loss: 0.00002217
Iteration 247/1000 | Loss: 0.00002217
Iteration 248/1000 | Loss: 0.00002217
Iteration 249/1000 | Loss: 0.00002217
Iteration 250/1000 | Loss: 0.00002217
Iteration 251/1000 | Loss: 0.00002217
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 251. Stopping optimization.
Last 5 losses: [2.2167527276906185e-05, 2.2167527276906185e-05, 2.2167527276906185e-05, 2.2167527276906185e-05, 2.2167527276906185e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2167527276906185e-05

Optimization complete. Final v2v error: 3.827479600906372 mm

Highest mean error: 5.313945293426514 mm for frame 35

Lowest mean error: 2.545868396759033 mm for frame 71

Saving results

Total time: 149.76253461837769
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_grace_posed_004/1038/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_grace_posed_004/1038.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_grace_posed_004/1038
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01058245
Iteration 2/25 | Loss: 0.01058245
Iteration 3/25 | Loss: 0.01058245
Iteration 4/25 | Loss: 0.01058244
Iteration 5/25 | Loss: 0.01058244
Iteration 6/25 | Loss: 0.01058244
Iteration 7/25 | Loss: 0.01058244
Iteration 8/25 | Loss: 0.01058244
Iteration 9/25 | Loss: 0.01058243
Iteration 10/25 | Loss: 0.01058243
Iteration 11/25 | Loss: 0.01058243
Iteration 12/25 | Loss: 0.01058243
Iteration 13/25 | Loss: 0.01058243
Iteration 14/25 | Loss: 0.01058242
Iteration 15/25 | Loss: 0.01058242
Iteration 16/25 | Loss: 0.01058242
Iteration 17/25 | Loss: 0.01058242
Iteration 18/25 | Loss: 0.01058241
Iteration 19/25 | Loss: 0.01058241
Iteration 20/25 | Loss: 0.01058241
Iteration 21/25 | Loss: 0.01058241
Iteration 22/25 | Loss: 0.01058240
Iteration 23/25 | Loss: 0.01058240
Iteration 24/25 | Loss: 0.01058240
Iteration 25/25 | Loss: 0.01058240

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.90784192
Iteration 2/25 | Loss: 0.09430742
Iteration 3/25 | Loss: 0.09349740
Iteration 4/25 | Loss: 0.09103482
Iteration 5/25 | Loss: 0.09090745
Iteration 6/25 | Loss: 0.09090744
Iteration 7/25 | Loss: 0.09090743
Iteration 8/25 | Loss: 0.09090743
Iteration 9/25 | Loss: 0.09090742
Iteration 10/25 | Loss: 0.09090743
Iteration 11/25 | Loss: 0.09090743
Iteration 12/25 | Loss: 0.09090742
Iteration 13/25 | Loss: 0.09090742
Iteration 14/25 | Loss: 0.09090742
Iteration 15/25 | Loss: 0.09090742
Iteration 16/25 | Loss: 0.09090742
Iteration 17/25 | Loss: 0.09090742
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.09090742468833923, 0.09090742468833923, 0.09090742468833923, 0.09090742468833923, 0.09090742468833923]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.09090742468833923

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.09090742
Iteration 2/1000 | Loss: 0.00175943
Iteration 3/1000 | Loss: 0.00227948
Iteration 4/1000 | Loss: 0.00243987
Iteration 5/1000 | Loss: 0.00434920
Iteration 6/1000 | Loss: 0.00125847
Iteration 7/1000 | Loss: 0.00278483
Iteration 8/1000 | Loss: 0.00128336
Iteration 9/1000 | Loss: 0.00284978
Iteration 10/1000 | Loss: 0.00381456
Iteration 11/1000 | Loss: 0.00033286
Iteration 12/1000 | Loss: 0.00086939
Iteration 13/1000 | Loss: 0.00010043
Iteration 14/1000 | Loss: 0.00047240
Iteration 15/1000 | Loss: 0.00119625
Iteration 16/1000 | Loss: 0.00033769
Iteration 17/1000 | Loss: 0.00014033
Iteration 18/1000 | Loss: 0.00014845
Iteration 19/1000 | Loss: 0.00012247
Iteration 20/1000 | Loss: 0.00060317
Iteration 21/1000 | Loss: 0.00010034
Iteration 22/1000 | Loss: 0.00004692
Iteration 23/1000 | Loss: 0.00015914
Iteration 24/1000 | Loss: 0.00005210
Iteration 25/1000 | Loss: 0.00014958
Iteration 26/1000 | Loss: 0.00006976
Iteration 27/1000 | Loss: 0.00035283
Iteration 28/1000 | Loss: 0.00011201
Iteration 29/1000 | Loss: 0.00006973
Iteration 30/1000 | Loss: 0.00005830
Iteration 31/1000 | Loss: 0.00009324
Iteration 32/1000 | Loss: 0.00006383
Iteration 33/1000 | Loss: 0.00003435
Iteration 34/1000 | Loss: 0.00030697
Iteration 35/1000 | Loss: 0.00005482
Iteration 36/1000 | Loss: 0.00027653
Iteration 37/1000 | Loss: 0.00048980
Iteration 38/1000 | Loss: 0.00007810
Iteration 39/1000 | Loss: 0.00004850
Iteration 40/1000 | Loss: 0.00003266
Iteration 41/1000 | Loss: 0.00004958
Iteration 42/1000 | Loss: 0.00004962
Iteration 43/1000 | Loss: 0.00019139
Iteration 44/1000 | Loss: 0.00002987
Iteration 45/1000 | Loss: 0.00002853
Iteration 46/1000 | Loss: 0.00010534
Iteration 47/1000 | Loss: 0.00008828
Iteration 48/1000 | Loss: 0.00003362
Iteration 49/1000 | Loss: 0.00002658
Iteration 50/1000 | Loss: 0.00005284
Iteration 51/1000 | Loss: 0.00002523
Iteration 52/1000 | Loss: 0.00004636
Iteration 53/1000 | Loss: 0.00040822
Iteration 54/1000 | Loss: 0.00003410
Iteration 55/1000 | Loss: 0.00003560
Iteration 56/1000 | Loss: 0.00006622
Iteration 57/1000 | Loss: 0.00002684
Iteration 58/1000 | Loss: 0.00002371
Iteration 59/1000 | Loss: 0.00002232
Iteration 60/1000 | Loss: 0.00005944
Iteration 61/1000 | Loss: 0.00006522
Iteration 62/1000 | Loss: 0.00002620
Iteration 63/1000 | Loss: 0.00008256
Iteration 64/1000 | Loss: 0.00002308
Iteration 65/1000 | Loss: 0.00003899
Iteration 66/1000 | Loss: 0.00009650
Iteration 67/1000 | Loss: 0.00065005
Iteration 68/1000 | Loss: 0.00003507
Iteration 69/1000 | Loss: 0.00002332
Iteration 70/1000 | Loss: 0.00004135
Iteration 71/1000 | Loss: 0.00002336
Iteration 72/1000 | Loss: 0.00002375
Iteration 73/1000 | Loss: 0.00003240
Iteration 74/1000 | Loss: 0.00002389
Iteration 75/1000 | Loss: 0.00003398
Iteration 76/1000 | Loss: 0.00002796
Iteration 77/1000 | Loss: 0.00002432
Iteration 78/1000 | Loss: 0.00002144
Iteration 79/1000 | Loss: 0.00002144
Iteration 80/1000 | Loss: 0.00002144
Iteration 81/1000 | Loss: 0.00002144
Iteration 82/1000 | Loss: 0.00002144
Iteration 83/1000 | Loss: 0.00002144
Iteration 84/1000 | Loss: 0.00002144
Iteration 85/1000 | Loss: 0.00002144
Iteration 86/1000 | Loss: 0.00002144
Iteration 87/1000 | Loss: 0.00002144
Iteration 88/1000 | Loss: 0.00002143
Iteration 89/1000 | Loss: 0.00002142
Iteration 90/1000 | Loss: 0.00002142
Iteration 91/1000 | Loss: 0.00002271
Iteration 92/1000 | Loss: 0.00002171
Iteration 93/1000 | Loss: 0.00002399
Iteration 94/1000 | Loss: 0.00011634
Iteration 95/1000 | Loss: 0.00005345
Iteration 96/1000 | Loss: 0.00002156
Iteration 97/1000 | Loss: 0.00002148
Iteration 98/1000 | Loss: 0.00002118
Iteration 99/1000 | Loss: 0.00004267
Iteration 100/1000 | Loss: 0.00002363
Iteration 101/1000 | Loss: 0.00002624
Iteration 102/1000 | Loss: 0.00002285
Iteration 103/1000 | Loss: 0.00002087
Iteration 104/1000 | Loss: 0.00002086
Iteration 105/1000 | Loss: 0.00002086
Iteration 106/1000 | Loss: 0.00002086
Iteration 107/1000 | Loss: 0.00002086
Iteration 108/1000 | Loss: 0.00002086
Iteration 109/1000 | Loss: 0.00002086
Iteration 110/1000 | Loss: 0.00002086
Iteration 111/1000 | Loss: 0.00002086
Iteration 112/1000 | Loss: 0.00002086
Iteration 113/1000 | Loss: 0.00002086
Iteration 114/1000 | Loss: 0.00002086
Iteration 115/1000 | Loss: 0.00002086
Iteration 116/1000 | Loss: 0.00002085
Iteration 117/1000 | Loss: 0.00002085
Iteration 118/1000 | Loss: 0.00002084
Iteration 119/1000 | Loss: 0.00002084
Iteration 120/1000 | Loss: 0.00002084
Iteration 121/1000 | Loss: 0.00002083
Iteration 122/1000 | Loss: 0.00002083
Iteration 123/1000 | Loss: 0.00002133
Iteration 124/1000 | Loss: 0.00002885
Iteration 125/1000 | Loss: 0.00006109
Iteration 126/1000 | Loss: 0.00002263
Iteration 127/1000 | Loss: 0.00002407
Iteration 128/1000 | Loss: 0.00002207
Iteration 129/1000 | Loss: 0.00002071
Iteration 130/1000 | Loss: 0.00002071
Iteration 131/1000 | Loss: 0.00002070
Iteration 132/1000 | Loss: 0.00002070
Iteration 133/1000 | Loss: 0.00002070
Iteration 134/1000 | Loss: 0.00002070
Iteration 135/1000 | Loss: 0.00002070
Iteration 136/1000 | Loss: 0.00002070
Iteration 137/1000 | Loss: 0.00002070
Iteration 138/1000 | Loss: 0.00002070
Iteration 139/1000 | Loss: 0.00002087
Iteration 140/1000 | Loss: 0.00002819
Iteration 141/1000 | Loss: 0.00002534
Iteration 142/1000 | Loss: 0.00002065
Iteration 143/1000 | Loss: 0.00002058
Iteration 144/1000 | Loss: 0.00002058
Iteration 145/1000 | Loss: 0.00002058
Iteration 146/1000 | Loss: 0.00002058
Iteration 147/1000 | Loss: 0.00002058
Iteration 148/1000 | Loss: 0.00002058
Iteration 149/1000 | Loss: 0.00002058
Iteration 150/1000 | Loss: 0.00002057
Iteration 151/1000 | Loss: 0.00002057
Iteration 152/1000 | Loss: 0.00002057
Iteration 153/1000 | Loss: 0.00004798
Iteration 154/1000 | Loss: 0.00002056
Iteration 155/1000 | Loss: 0.00002214
Iteration 156/1000 | Loss: 0.00002674
Iteration 157/1000 | Loss: 0.00004413
Iteration 158/1000 | Loss: 0.00002049
Iteration 159/1000 | Loss: 0.00002251
Iteration 160/1000 | Loss: 0.00002044
Iteration 161/1000 | Loss: 0.00002043
Iteration 162/1000 | Loss: 0.00002043
Iteration 163/1000 | Loss: 0.00002043
Iteration 164/1000 | Loss: 0.00002043
Iteration 165/1000 | Loss: 0.00002042
Iteration 166/1000 | Loss: 0.00002042
Iteration 167/1000 | Loss: 0.00002042
Iteration 168/1000 | Loss: 0.00002042
Iteration 169/1000 | Loss: 0.00002042
Iteration 170/1000 | Loss: 0.00002042
Iteration 171/1000 | Loss: 0.00002042
Iteration 172/1000 | Loss: 0.00002042
Iteration 173/1000 | Loss: 0.00002042
Iteration 174/1000 | Loss: 0.00002042
Iteration 175/1000 | Loss: 0.00002042
Iteration 176/1000 | Loss: 0.00002041
Iteration 177/1000 | Loss: 0.00002041
Iteration 178/1000 | Loss: 0.00002041
Iteration 179/1000 | Loss: 0.00002041
Iteration 180/1000 | Loss: 0.00002041
Iteration 181/1000 | Loss: 0.00002041
Iteration 182/1000 | Loss: 0.00002040
Iteration 183/1000 | Loss: 0.00002040
Iteration 184/1000 | Loss: 0.00002040
Iteration 185/1000 | Loss: 0.00002040
Iteration 186/1000 | Loss: 0.00002040
Iteration 187/1000 | Loss: 0.00002040
Iteration 188/1000 | Loss: 0.00002039
Iteration 189/1000 | Loss: 0.00002039
Iteration 190/1000 | Loss: 0.00002188
Iteration 191/1000 | Loss: 0.00002046
Iteration 192/1000 | Loss: 0.00002044
Iteration 193/1000 | Loss: 0.00002754
Iteration 194/1000 | Loss: 0.00002073
Iteration 195/1000 | Loss: 0.00002101
Iteration 196/1000 | Loss: 0.00002058
Iteration 197/1000 | Loss: 0.00002047
Iteration 198/1000 | Loss: 0.00002037
Iteration 199/1000 | Loss: 0.00002037
Iteration 200/1000 | Loss: 0.00002037
Iteration 201/1000 | Loss: 0.00002037
Iteration 202/1000 | Loss: 0.00002037
Iteration 203/1000 | Loss: 0.00002037
Iteration 204/1000 | Loss: 0.00002037
Iteration 205/1000 | Loss: 0.00002037
Iteration 206/1000 | Loss: 0.00002037
Iteration 207/1000 | Loss: 0.00002037
Iteration 208/1000 | Loss: 0.00002037
Iteration 209/1000 | Loss: 0.00002037
Iteration 210/1000 | Loss: 0.00002037
Iteration 211/1000 | Loss: 0.00002037
Iteration 212/1000 | Loss: 0.00002037
Iteration 213/1000 | Loss: 0.00002037
Iteration 214/1000 | Loss: 0.00002037
Iteration 215/1000 | Loss: 0.00002037
Iteration 216/1000 | Loss: 0.00002037
Iteration 217/1000 | Loss: 0.00002037
Iteration 218/1000 | Loss: 0.00002037
Iteration 219/1000 | Loss: 0.00002037
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 219. Stopping optimization.
Last 5 losses: [2.0368173863971606e-05, 2.0368173863971606e-05, 2.0368173863971606e-05, 2.0368173863971606e-05, 2.0368173863971606e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0368173863971606e-05

Optimization complete. Final v2v error: 3.3353207111358643 mm

Highest mean error: 20.601139068603516 mm for frame 89

Lowest mean error: 2.4617578983306885 mm for frame 108

Saving results

Total time: 181.49444556236267
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_grace_posed_004/1025/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_grace_posed_004/1025.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_grace_posed_004/1025
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00627034
Iteration 2/25 | Loss: 0.00110774
Iteration 3/25 | Loss: 0.00101605
Iteration 4/25 | Loss: 0.00100449
Iteration 5/25 | Loss: 0.00100074
Iteration 6/25 | Loss: 0.00099948
Iteration 7/25 | Loss: 0.00099948
Iteration 8/25 | Loss: 0.00099948
Iteration 9/25 | Loss: 0.00099948
Iteration 10/25 | Loss: 0.00099948
Iteration 11/25 | Loss: 0.00099948
Iteration 12/25 | Loss: 0.00099948
Iteration 13/25 | Loss: 0.00099948
Iteration 14/25 | Loss: 0.00099948
Iteration 15/25 | Loss: 0.00099948
Iteration 16/25 | Loss: 0.00099948
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0009994831634685397, 0.0009994831634685397, 0.0009994831634685397, 0.0009994831634685397, 0.0009994831634685397]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009994831634685397

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34645820
Iteration 2/25 | Loss: 0.00100102
Iteration 3/25 | Loss: 0.00100102
Iteration 4/25 | Loss: 0.00100102
Iteration 5/25 | Loss: 0.00100102
Iteration 6/25 | Loss: 0.00100102
Iteration 7/25 | Loss: 0.00100102
Iteration 8/25 | Loss: 0.00100102
Iteration 9/25 | Loss: 0.00100102
Iteration 10/25 | Loss: 0.00100102
Iteration 11/25 | Loss: 0.00100102
Iteration 12/25 | Loss: 0.00100102
Iteration 13/25 | Loss: 0.00100102
Iteration 14/25 | Loss: 0.00100102
Iteration 15/25 | Loss: 0.00100102
Iteration 16/25 | Loss: 0.00100102
Iteration 17/25 | Loss: 0.00100102
Iteration 18/25 | Loss: 0.00100102
Iteration 19/25 | Loss: 0.00100102
Iteration 20/25 | Loss: 0.00100102
Iteration 21/25 | Loss: 0.00100102
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0010010207770392299, 0.0010010207770392299, 0.0010010207770392299, 0.0010010207770392299, 0.0010010207770392299]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010010207770392299

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00100102
Iteration 2/1000 | Loss: 0.00001664
Iteration 3/1000 | Loss: 0.00001033
Iteration 4/1000 | Loss: 0.00000911
Iteration 5/1000 | Loss: 0.00000864
Iteration 6/1000 | Loss: 0.00000826
Iteration 7/1000 | Loss: 0.00000792
Iteration 8/1000 | Loss: 0.00000778
Iteration 9/1000 | Loss: 0.00000773
Iteration 10/1000 | Loss: 0.00000768
Iteration 11/1000 | Loss: 0.00000762
Iteration 12/1000 | Loss: 0.00000761
Iteration 13/1000 | Loss: 0.00000749
Iteration 14/1000 | Loss: 0.00000749
Iteration 15/1000 | Loss: 0.00000748
Iteration 16/1000 | Loss: 0.00000747
Iteration 17/1000 | Loss: 0.00000745
Iteration 18/1000 | Loss: 0.00000744
Iteration 19/1000 | Loss: 0.00000741
Iteration 20/1000 | Loss: 0.00000733
Iteration 21/1000 | Loss: 0.00000730
Iteration 22/1000 | Loss: 0.00000730
Iteration 23/1000 | Loss: 0.00000730
Iteration 24/1000 | Loss: 0.00000730
Iteration 25/1000 | Loss: 0.00000730
Iteration 26/1000 | Loss: 0.00000730
Iteration 27/1000 | Loss: 0.00000730
Iteration 28/1000 | Loss: 0.00000729
Iteration 29/1000 | Loss: 0.00000727
Iteration 30/1000 | Loss: 0.00000726
Iteration 31/1000 | Loss: 0.00000725
Iteration 32/1000 | Loss: 0.00000725
Iteration 33/1000 | Loss: 0.00000725
Iteration 34/1000 | Loss: 0.00000725
Iteration 35/1000 | Loss: 0.00000724
Iteration 36/1000 | Loss: 0.00000724
Iteration 37/1000 | Loss: 0.00000724
Iteration 38/1000 | Loss: 0.00000724
Iteration 39/1000 | Loss: 0.00000724
Iteration 40/1000 | Loss: 0.00000724
Iteration 41/1000 | Loss: 0.00000722
Iteration 42/1000 | Loss: 0.00000722
Iteration 43/1000 | Loss: 0.00000722
Iteration 44/1000 | Loss: 0.00000721
Iteration 45/1000 | Loss: 0.00000721
Iteration 46/1000 | Loss: 0.00000720
Iteration 47/1000 | Loss: 0.00000720
Iteration 48/1000 | Loss: 0.00000719
Iteration 49/1000 | Loss: 0.00000719
Iteration 50/1000 | Loss: 0.00000718
Iteration 51/1000 | Loss: 0.00000717
Iteration 52/1000 | Loss: 0.00000717
Iteration 53/1000 | Loss: 0.00000716
Iteration 54/1000 | Loss: 0.00000716
Iteration 55/1000 | Loss: 0.00000716
Iteration 56/1000 | Loss: 0.00000715
Iteration 57/1000 | Loss: 0.00000715
Iteration 58/1000 | Loss: 0.00000713
Iteration 59/1000 | Loss: 0.00000712
Iteration 60/1000 | Loss: 0.00000712
Iteration 61/1000 | Loss: 0.00000711
Iteration 62/1000 | Loss: 0.00000711
Iteration 63/1000 | Loss: 0.00000711
Iteration 64/1000 | Loss: 0.00000710
Iteration 65/1000 | Loss: 0.00000709
Iteration 66/1000 | Loss: 0.00000708
Iteration 67/1000 | Loss: 0.00000708
Iteration 68/1000 | Loss: 0.00000708
Iteration 69/1000 | Loss: 0.00000707
Iteration 70/1000 | Loss: 0.00000707
Iteration 71/1000 | Loss: 0.00000707
Iteration 72/1000 | Loss: 0.00000707
Iteration 73/1000 | Loss: 0.00000707
Iteration 74/1000 | Loss: 0.00000707
Iteration 75/1000 | Loss: 0.00000707
Iteration 76/1000 | Loss: 0.00000707
Iteration 77/1000 | Loss: 0.00000706
Iteration 78/1000 | Loss: 0.00000706
Iteration 79/1000 | Loss: 0.00000706
Iteration 80/1000 | Loss: 0.00000706
Iteration 81/1000 | Loss: 0.00000706
Iteration 82/1000 | Loss: 0.00000706
Iteration 83/1000 | Loss: 0.00000706
Iteration 84/1000 | Loss: 0.00000706
Iteration 85/1000 | Loss: 0.00000706
Iteration 86/1000 | Loss: 0.00000705
Iteration 87/1000 | Loss: 0.00000705
Iteration 88/1000 | Loss: 0.00000705
Iteration 89/1000 | Loss: 0.00000705
Iteration 90/1000 | Loss: 0.00000705
Iteration 91/1000 | Loss: 0.00000705
Iteration 92/1000 | Loss: 0.00000705
Iteration 93/1000 | Loss: 0.00000705
Iteration 94/1000 | Loss: 0.00000705
Iteration 95/1000 | Loss: 0.00000705
Iteration 96/1000 | Loss: 0.00000705
Iteration 97/1000 | Loss: 0.00000704
Iteration 98/1000 | Loss: 0.00000704
Iteration 99/1000 | Loss: 0.00000704
Iteration 100/1000 | Loss: 0.00000704
Iteration 101/1000 | Loss: 0.00000704
Iteration 102/1000 | Loss: 0.00000704
Iteration 103/1000 | Loss: 0.00000704
Iteration 104/1000 | Loss: 0.00000704
Iteration 105/1000 | Loss: 0.00000704
Iteration 106/1000 | Loss: 0.00000704
Iteration 107/1000 | Loss: 0.00000703
Iteration 108/1000 | Loss: 0.00000703
Iteration 109/1000 | Loss: 0.00000703
Iteration 110/1000 | Loss: 0.00000703
Iteration 111/1000 | Loss: 0.00000703
Iteration 112/1000 | Loss: 0.00000703
Iteration 113/1000 | Loss: 0.00000703
Iteration 114/1000 | Loss: 0.00000703
Iteration 115/1000 | Loss: 0.00000703
Iteration 116/1000 | Loss: 0.00000702
Iteration 117/1000 | Loss: 0.00000702
Iteration 118/1000 | Loss: 0.00000702
Iteration 119/1000 | Loss: 0.00000701
Iteration 120/1000 | Loss: 0.00000701
Iteration 121/1000 | Loss: 0.00000701
Iteration 122/1000 | Loss: 0.00000701
Iteration 123/1000 | Loss: 0.00000701
Iteration 124/1000 | Loss: 0.00000701
Iteration 125/1000 | Loss: 0.00000701
Iteration 126/1000 | Loss: 0.00000701
Iteration 127/1000 | Loss: 0.00000701
Iteration 128/1000 | Loss: 0.00000701
Iteration 129/1000 | Loss: 0.00000701
Iteration 130/1000 | Loss: 0.00000701
Iteration 131/1000 | Loss: 0.00000701
Iteration 132/1000 | Loss: 0.00000701
Iteration 133/1000 | Loss: 0.00000701
Iteration 134/1000 | Loss: 0.00000701
Iteration 135/1000 | Loss: 0.00000701
Iteration 136/1000 | Loss: 0.00000701
Iteration 137/1000 | Loss: 0.00000701
Iteration 138/1000 | Loss: 0.00000701
Iteration 139/1000 | Loss: 0.00000701
Iteration 140/1000 | Loss: 0.00000701
Iteration 141/1000 | Loss: 0.00000701
Iteration 142/1000 | Loss: 0.00000701
Iteration 143/1000 | Loss: 0.00000701
Iteration 144/1000 | Loss: 0.00000701
Iteration 145/1000 | Loss: 0.00000701
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 145. Stopping optimization.
Last 5 losses: [7.007837211858714e-06, 7.007837211858714e-06, 7.007837211858714e-06, 7.007837211858714e-06, 7.007837211858714e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 7.007837211858714e-06

Optimization complete. Final v2v error: 2.2751996517181396 mm

Highest mean error: 2.869521141052246 mm for frame 80

Lowest mean error: 2.105695962905884 mm for frame 105

Saving results

Total time: 33.31280493736267
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_grace_posed_004/1008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_grace_posed_004/1008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_grace_posed_004/1008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00995381
Iteration 2/25 | Loss: 0.00298692
Iteration 3/25 | Loss: 0.00176871
Iteration 4/25 | Loss: 0.00147676
Iteration 5/25 | Loss: 0.00141006
Iteration 6/25 | Loss: 0.00137625
Iteration 7/25 | Loss: 0.00135451
Iteration 8/25 | Loss: 0.00135239
Iteration 9/25 | Loss: 0.00132630
Iteration 10/25 | Loss: 0.00129318
Iteration 11/25 | Loss: 0.00128575
Iteration 12/25 | Loss: 0.00127390
Iteration 13/25 | Loss: 0.00125720
Iteration 14/25 | Loss: 0.00124635
Iteration 15/25 | Loss: 0.00125396
Iteration 16/25 | Loss: 0.00124731
Iteration 17/25 | Loss: 0.00124350
Iteration 18/25 | Loss: 0.00123912
Iteration 19/25 | Loss: 0.00123373
Iteration 20/25 | Loss: 0.00123066
Iteration 21/25 | Loss: 0.00122933
Iteration 22/25 | Loss: 0.00122909
Iteration 23/25 | Loss: 0.00122906
Iteration 24/25 | Loss: 0.00122905
Iteration 25/25 | Loss: 0.00122905

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.27994764
Iteration 2/25 | Loss: 0.00240128
Iteration 3/25 | Loss: 0.00208151
Iteration 4/25 | Loss: 0.00206268
Iteration 5/25 | Loss: 0.00206268
Iteration 6/25 | Loss: 0.00206268
Iteration 7/25 | Loss: 0.00206268
Iteration 8/25 | Loss: 0.00206268
Iteration 9/25 | Loss: 0.00206268
Iteration 10/25 | Loss: 0.00206268
Iteration 11/25 | Loss: 0.00206268
Iteration 12/25 | Loss: 0.00206268
Iteration 13/25 | Loss: 0.00206268
Iteration 14/25 | Loss: 0.00206268
Iteration 15/25 | Loss: 0.00206268
Iteration 16/25 | Loss: 0.00206268
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0020626767072826624, 0.0020626767072826624, 0.0020626767072826624, 0.0020626767072826624, 0.0020626767072826624]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0020626767072826624

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00206268
Iteration 2/1000 | Loss: 0.00026722
Iteration 3/1000 | Loss: 0.00026827
Iteration 4/1000 | Loss: 0.00017092
Iteration 5/1000 | Loss: 0.00036253
Iteration 6/1000 | Loss: 0.00013488
Iteration 7/1000 | Loss: 0.00013600
Iteration 8/1000 | Loss: 0.00010393
Iteration 9/1000 | Loss: 0.00011881
Iteration 10/1000 | Loss: 0.00009644
Iteration 11/1000 | Loss: 0.00009062
Iteration 12/1000 | Loss: 0.00008357
Iteration 13/1000 | Loss: 0.00018352
Iteration 14/1000 | Loss: 0.00015663
Iteration 15/1000 | Loss: 0.00009711
Iteration 16/1000 | Loss: 0.00008148
Iteration 17/1000 | Loss: 0.00010652
Iteration 18/1000 | Loss: 0.00008557
Iteration 19/1000 | Loss: 0.00040619
Iteration 20/1000 | Loss: 0.00322305
Iteration 21/1000 | Loss: 0.00170087
Iteration 22/1000 | Loss: 0.00061600
Iteration 23/1000 | Loss: 0.00058686
Iteration 24/1000 | Loss: 0.00063017
Iteration 25/1000 | Loss: 0.00015317
Iteration 26/1000 | Loss: 0.00022380
Iteration 27/1000 | Loss: 0.00014369
Iteration 28/1000 | Loss: 0.00014848
Iteration 29/1000 | Loss: 0.00009632
Iteration 30/1000 | Loss: 0.00008996
Iteration 31/1000 | Loss: 0.00008087
Iteration 32/1000 | Loss: 0.00004829
Iteration 33/1000 | Loss: 0.00004445
Iteration 34/1000 | Loss: 0.00004384
Iteration 35/1000 | Loss: 0.00055313
Iteration 36/1000 | Loss: 0.00023133
Iteration 37/1000 | Loss: 0.00038185
Iteration 38/1000 | Loss: 0.00013152
Iteration 39/1000 | Loss: 0.00046719
Iteration 40/1000 | Loss: 0.00021032
Iteration 41/1000 | Loss: 0.00005521
Iteration 42/1000 | Loss: 0.00067126
Iteration 43/1000 | Loss: 0.00037653
Iteration 44/1000 | Loss: 0.00020452
Iteration 45/1000 | Loss: 0.00039346
Iteration 46/1000 | Loss: 0.00067675
Iteration 47/1000 | Loss: 0.00027363
Iteration 48/1000 | Loss: 0.00003709
Iteration 49/1000 | Loss: 0.00002370
Iteration 50/1000 | Loss: 0.00002408
Iteration 51/1000 | Loss: 0.00002000
Iteration 52/1000 | Loss: 0.00005436
Iteration 53/1000 | Loss: 0.00003162
Iteration 54/1000 | Loss: 0.00005037
Iteration 55/1000 | Loss: 0.00002977
Iteration 56/1000 | Loss: 0.00003901
Iteration 57/1000 | Loss: 0.00002105
Iteration 58/1000 | Loss: 0.00001881
Iteration 59/1000 | Loss: 0.00002055
Iteration 60/1000 | Loss: 0.00001874
Iteration 61/1000 | Loss: 0.00004120
Iteration 62/1000 | Loss: 0.00002246
Iteration 63/1000 | Loss: 0.00001909
Iteration 64/1000 | Loss: 0.00001592
Iteration 65/1000 | Loss: 0.00002224
Iteration 66/1000 | Loss: 0.00001735
Iteration 67/1000 | Loss: 0.00001761
Iteration 68/1000 | Loss: 0.00004777
Iteration 69/1000 | Loss: 0.00001907
Iteration 70/1000 | Loss: 0.00001483
Iteration 71/1000 | Loss: 0.00001482
Iteration 72/1000 | Loss: 0.00001482
Iteration 73/1000 | Loss: 0.00001839
Iteration 74/1000 | Loss: 0.00001485
Iteration 75/1000 | Loss: 0.00001484
Iteration 76/1000 | Loss: 0.00003141
Iteration 77/1000 | Loss: 0.00001656
Iteration 78/1000 | Loss: 0.00001495
Iteration 79/1000 | Loss: 0.00001960
Iteration 80/1000 | Loss: 0.00001469
Iteration 81/1000 | Loss: 0.00001469
Iteration 82/1000 | Loss: 0.00001468
Iteration 83/1000 | Loss: 0.00001483
Iteration 84/1000 | Loss: 0.00001468
Iteration 85/1000 | Loss: 0.00001468
Iteration 86/1000 | Loss: 0.00001468
Iteration 87/1000 | Loss: 0.00001468
Iteration 88/1000 | Loss: 0.00001467
Iteration 89/1000 | Loss: 0.00001467
Iteration 90/1000 | Loss: 0.00001467
Iteration 91/1000 | Loss: 0.00001466
Iteration 92/1000 | Loss: 0.00001626
Iteration 93/1000 | Loss: 0.00001626
Iteration 94/1000 | Loss: 0.00001626
Iteration 95/1000 | Loss: 0.00001625
Iteration 96/1000 | Loss: 0.00001625
Iteration 97/1000 | Loss: 0.00003166
Iteration 98/1000 | Loss: 0.00001466
Iteration 99/1000 | Loss: 0.00001450
Iteration 100/1000 | Loss: 0.00001617
Iteration 101/1000 | Loss: 0.00002456
Iteration 102/1000 | Loss: 0.00001446
Iteration 103/1000 | Loss: 0.00001446
Iteration 104/1000 | Loss: 0.00001446
Iteration 105/1000 | Loss: 0.00001446
Iteration 106/1000 | Loss: 0.00001445
Iteration 107/1000 | Loss: 0.00001502
Iteration 108/1000 | Loss: 0.00001502
Iteration 109/1000 | Loss: 0.00001444
Iteration 110/1000 | Loss: 0.00001444
Iteration 111/1000 | Loss: 0.00001444
Iteration 112/1000 | Loss: 0.00001444
Iteration 113/1000 | Loss: 0.00001444
Iteration 114/1000 | Loss: 0.00001444
Iteration 115/1000 | Loss: 0.00001444
Iteration 116/1000 | Loss: 0.00001444
Iteration 117/1000 | Loss: 0.00001444
Iteration 118/1000 | Loss: 0.00001444
Iteration 119/1000 | Loss: 0.00001444
Iteration 120/1000 | Loss: 0.00001444
Iteration 121/1000 | Loss: 0.00001444
Iteration 122/1000 | Loss: 0.00001444
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 122. Stopping optimization.
Last 5 losses: [1.4438797734328546e-05, 1.4438797734328546e-05, 1.4438797734328546e-05, 1.4438797734328546e-05, 1.4438797734328546e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4438797734328546e-05

Optimization complete. Final v2v error: 2.9547152519226074 mm

Highest mean error: 5.285459518432617 mm for frame 175

Lowest mean error: 2.420011043548584 mm for frame 135

Saving results

Total time: 173.37421584129333
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_grace_posed_004/1062/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_grace_posed_004/1062.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_grace_posed_004/1062
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01053772
Iteration 2/25 | Loss: 0.00422443
Iteration 3/25 | Loss: 0.00327707
Iteration 4/25 | Loss: 0.00316063
Iteration 5/25 | Loss: 0.00303357
Iteration 6/25 | Loss: 0.00264432
Iteration 7/25 | Loss: 0.00256399
Iteration 8/25 | Loss: 0.00244059
Iteration 9/25 | Loss: 0.00247143
Iteration 10/25 | Loss: 0.00233868
Iteration 11/25 | Loss: 0.00222700
Iteration 12/25 | Loss: 0.00213419
Iteration 13/25 | Loss: 0.00209996
Iteration 14/25 | Loss: 0.00198154
Iteration 15/25 | Loss: 0.00187912
Iteration 16/25 | Loss: 0.00197852
Iteration 17/25 | Loss: 0.00177497
Iteration 18/25 | Loss: 0.00176188
Iteration 19/25 | Loss: 0.00172863
Iteration 20/25 | Loss: 0.00168126
Iteration 21/25 | Loss: 0.00167397
Iteration 22/25 | Loss: 0.00167489
Iteration 23/25 | Loss: 0.00167838
Iteration 24/25 | Loss: 0.00166173
Iteration 25/25 | Loss: 0.00165400

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.03469181
Iteration 2/25 | Loss: 0.00378276
Iteration 3/25 | Loss: 0.00378276
Iteration 4/25 | Loss: 0.00378276
Iteration 5/25 | Loss: 0.00378276
Iteration 6/25 | Loss: 0.00378276
Iteration 7/25 | Loss: 0.00378276
Iteration 8/25 | Loss: 0.00378276
Iteration 9/25 | Loss: 0.00378276
Iteration 10/25 | Loss: 0.00378275
Iteration 11/25 | Loss: 0.00378275
Iteration 12/25 | Loss: 0.00378275
Iteration 13/25 | Loss: 0.00378275
Iteration 14/25 | Loss: 0.00378275
Iteration 15/25 | Loss: 0.00378275
Iteration 16/25 | Loss: 0.00378275
Iteration 17/25 | Loss: 0.00378275
Iteration 18/25 | Loss: 0.00378275
Iteration 19/25 | Loss: 0.00378275
Iteration 20/25 | Loss: 0.00378275
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.003782754298299551, 0.003782754298299551, 0.003782754298299551, 0.003782754298299551, 0.003782754298299551]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.003782754298299551

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00378275
Iteration 2/1000 | Loss: 0.00085387
Iteration 3/1000 | Loss: 0.00120815
Iteration 4/1000 | Loss: 0.00029817
Iteration 5/1000 | Loss: 0.00031315
Iteration 6/1000 | Loss: 0.00027177
Iteration 7/1000 | Loss: 0.00060466
Iteration 8/1000 | Loss: 0.00028740
Iteration 9/1000 | Loss: 0.00029002
Iteration 10/1000 | Loss: 0.00025368
Iteration 11/1000 | Loss: 0.00023327
Iteration 12/1000 | Loss: 0.00028984
Iteration 13/1000 | Loss: 0.00025183
Iteration 14/1000 | Loss: 0.00023170
Iteration 15/1000 | Loss: 0.00021357
Iteration 16/1000 | Loss: 0.00073555
Iteration 17/1000 | Loss: 0.00054858
Iteration 18/1000 | Loss: 0.00068405
Iteration 19/1000 | Loss: 0.00046887
Iteration 20/1000 | Loss: 0.00087961
Iteration 21/1000 | Loss: 0.00046688
Iteration 22/1000 | Loss: 0.00166139
Iteration 23/1000 | Loss: 0.00569150
Iteration 24/1000 | Loss: 0.00657695
Iteration 25/1000 | Loss: 0.00199382
Iteration 26/1000 | Loss: 0.00158487
Iteration 27/1000 | Loss: 0.00029836
Iteration 28/1000 | Loss: 0.00036066
Iteration 29/1000 | Loss: 0.00039800
Iteration 30/1000 | Loss: 0.00046687
Iteration 31/1000 | Loss: 0.00018249
Iteration 32/1000 | Loss: 0.00024586
Iteration 33/1000 | Loss: 0.00050150
Iteration 34/1000 | Loss: 0.00034284
Iteration 35/1000 | Loss: 0.00022814
Iteration 36/1000 | Loss: 0.00009256
Iteration 37/1000 | Loss: 0.00033350
Iteration 38/1000 | Loss: 0.00023934
Iteration 39/1000 | Loss: 0.00033055
Iteration 40/1000 | Loss: 0.00020860
Iteration 41/1000 | Loss: 0.00025001
Iteration 42/1000 | Loss: 0.00007547
Iteration 43/1000 | Loss: 0.00006377
Iteration 44/1000 | Loss: 0.00005840
Iteration 45/1000 | Loss: 0.00019142
Iteration 46/1000 | Loss: 0.00006025
Iteration 47/1000 | Loss: 0.00005539
Iteration 48/1000 | Loss: 0.00027251
Iteration 49/1000 | Loss: 0.00013989
Iteration 50/1000 | Loss: 0.00024579
Iteration 51/1000 | Loss: 0.00005386
Iteration 52/1000 | Loss: 0.00016984
Iteration 53/1000 | Loss: 0.00105182
Iteration 54/1000 | Loss: 0.00062702
Iteration 55/1000 | Loss: 0.00074569
Iteration 56/1000 | Loss: 0.00026185
Iteration 57/1000 | Loss: 0.00008369
Iteration 58/1000 | Loss: 0.00019065
Iteration 59/1000 | Loss: 0.00006463
Iteration 60/1000 | Loss: 0.00005339
Iteration 61/1000 | Loss: 0.00004881
Iteration 62/1000 | Loss: 0.00005110
Iteration 63/1000 | Loss: 0.00004812
Iteration 64/1000 | Loss: 0.00018833
Iteration 65/1000 | Loss: 0.00007567
Iteration 66/1000 | Loss: 0.00007710
Iteration 67/1000 | Loss: 0.00010135
Iteration 68/1000 | Loss: 0.00006444
Iteration 69/1000 | Loss: 0.00005403
Iteration 70/1000 | Loss: 0.00006984
Iteration 71/1000 | Loss: 0.00006547
Iteration 72/1000 | Loss: 0.00005503
Iteration 73/1000 | Loss: 0.00004286
Iteration 74/1000 | Loss: 0.00004118
Iteration 75/1000 | Loss: 0.00003943
Iteration 76/1000 | Loss: 0.00003826
Iteration 77/1000 | Loss: 0.00003724
Iteration 78/1000 | Loss: 0.00003681
Iteration 79/1000 | Loss: 0.00010021
Iteration 80/1000 | Loss: 0.00005088
Iteration 81/1000 | Loss: 0.00008389
Iteration 82/1000 | Loss: 0.00007374
Iteration 83/1000 | Loss: 0.00005265
Iteration 84/1000 | Loss: 0.00004031
Iteration 85/1000 | Loss: 0.00005171
Iteration 86/1000 | Loss: 0.00003631
Iteration 87/1000 | Loss: 0.00003608
Iteration 88/1000 | Loss: 0.00003592
Iteration 89/1000 | Loss: 0.00005689
Iteration 90/1000 | Loss: 0.00018385
Iteration 91/1000 | Loss: 0.00009789
Iteration 92/1000 | Loss: 0.00005464
Iteration 93/1000 | Loss: 0.00003872
Iteration 94/1000 | Loss: 0.00003649
Iteration 95/1000 | Loss: 0.00003586
Iteration 96/1000 | Loss: 0.00005630
Iteration 97/1000 | Loss: 0.00003845
Iteration 98/1000 | Loss: 0.00005674
Iteration 99/1000 | Loss: 0.00005836
Iteration 100/1000 | Loss: 0.00004375
Iteration 101/1000 | Loss: 0.00015853
Iteration 102/1000 | Loss: 0.00004930
Iteration 103/1000 | Loss: 0.00004385
Iteration 104/1000 | Loss: 0.00003925
Iteration 105/1000 | Loss: 0.00004771
Iteration 106/1000 | Loss: 0.00007083
Iteration 107/1000 | Loss: 0.00006062
Iteration 108/1000 | Loss: 0.00005424
Iteration 109/1000 | Loss: 0.00006190
Iteration 110/1000 | Loss: 0.00004286
Iteration 111/1000 | Loss: 0.00009695
Iteration 112/1000 | Loss: 0.00004758
Iteration 113/1000 | Loss: 0.00005217
Iteration 114/1000 | Loss: 0.00004431
Iteration 115/1000 | Loss: 0.00004475
Iteration 116/1000 | Loss: 0.00004821
Iteration 117/1000 | Loss: 0.00004867
Iteration 118/1000 | Loss: 0.00005620
Iteration 119/1000 | Loss: 0.00006585
Iteration 120/1000 | Loss: 0.00004394
Iteration 121/1000 | Loss: 0.00005637
Iteration 122/1000 | Loss: 0.00004238
Iteration 123/1000 | Loss: 0.00004610
Iteration 124/1000 | Loss: 0.00004318
Iteration 125/1000 | Loss: 0.00005534
Iteration 126/1000 | Loss: 0.00005534
Iteration 127/1000 | Loss: 0.00004665
Iteration 128/1000 | Loss: 0.00004595
Iteration 129/1000 | Loss: 0.00004386
Iteration 130/1000 | Loss: 0.00004354
Iteration 131/1000 | Loss: 0.00004335
Iteration 132/1000 | Loss: 0.00004210
Iteration 133/1000 | Loss: 0.00004381
Iteration 134/1000 | Loss: 0.00004605
Iteration 135/1000 | Loss: 0.00004436
Iteration 136/1000 | Loss: 0.00004679
Iteration 137/1000 | Loss: 0.00004708
Iteration 138/1000 | Loss: 0.00004853
Iteration 139/1000 | Loss: 0.00004288
Iteration 140/1000 | Loss: 0.00004080
Iteration 141/1000 | Loss: 0.00003984
Iteration 142/1000 | Loss: 0.00004429
Iteration 143/1000 | Loss: 0.00010693
Iteration 144/1000 | Loss: 0.00006067
Iteration 145/1000 | Loss: 0.00010595
Iteration 146/1000 | Loss: 0.00006815
Iteration 147/1000 | Loss: 0.00004243
Iteration 148/1000 | Loss: 0.00004237
Iteration 149/1000 | Loss: 0.00005279
Iteration 150/1000 | Loss: 0.00004656
Iteration 151/1000 | Loss: 0.00004827
Iteration 152/1000 | Loss: 0.00004682
Iteration 153/1000 | Loss: 0.00004523
Iteration 154/1000 | Loss: 0.00004962
Iteration 155/1000 | Loss: 0.00004402
Iteration 156/1000 | Loss: 0.00004709
Iteration 157/1000 | Loss: 0.00004303
Iteration 158/1000 | Loss: 0.00003887
Iteration 159/1000 | Loss: 0.00004548
Iteration 160/1000 | Loss: 0.00004261
Iteration 161/1000 | Loss: 0.00004313
Iteration 162/1000 | Loss: 0.00005225
Iteration 163/1000 | Loss: 0.00005419
Iteration 164/1000 | Loss: 0.00004145
Iteration 165/1000 | Loss: 0.00004651
Iteration 166/1000 | Loss: 0.00006016
Iteration 167/1000 | Loss: 0.00004039
Iteration 168/1000 | Loss: 0.00004241
Iteration 169/1000 | Loss: 0.00005278
Iteration 170/1000 | Loss: 0.00005599
Iteration 171/1000 | Loss: 0.00004217
Iteration 172/1000 | Loss: 0.00004369
Iteration 173/1000 | Loss: 0.00005431
Iteration 174/1000 | Loss: 0.00005256
Iteration 175/1000 | Loss: 0.00003938
Iteration 176/1000 | Loss: 0.00005384
Iteration 177/1000 | Loss: 0.00004197
Iteration 178/1000 | Loss: 0.00004403
Iteration 179/1000 | Loss: 0.00005410
Iteration 180/1000 | Loss: 0.00005539
Iteration 181/1000 | Loss: 0.00004255
Iteration 182/1000 | Loss: 0.00005386
Iteration 183/1000 | Loss: 0.00006136
Iteration 184/1000 | Loss: 0.00005374
Iteration 185/1000 | Loss: 0.00006063
Iteration 186/1000 | Loss: 0.00005370
Iteration 187/1000 | Loss: 0.00006325
Iteration 188/1000 | Loss: 0.00005322
Iteration 189/1000 | Loss: 0.00005151
Iteration 190/1000 | Loss: 0.00004107
Iteration 191/1000 | Loss: 0.00004967
Iteration 192/1000 | Loss: 0.00005734
Iteration 193/1000 | Loss: 0.00005370
Iteration 194/1000 | Loss: 0.00004891
Iteration 195/1000 | Loss: 0.00005585
Iteration 196/1000 | Loss: 0.00005033
Iteration 197/1000 | Loss: 0.00004955
Iteration 198/1000 | Loss: 0.00004291
Iteration 199/1000 | Loss: 0.00004950
Iteration 200/1000 | Loss: 0.00005477
Iteration 201/1000 | Loss: 0.00005220
Iteration 202/1000 | Loss: 0.00004828
Iteration 203/1000 | Loss: 0.00005920
Iteration 204/1000 | Loss: 0.00005271
Iteration 205/1000 | Loss: 0.00004177
Iteration 206/1000 | Loss: 0.00003711
Iteration 207/1000 | Loss: 0.00003622
Iteration 208/1000 | Loss: 0.00003583
Iteration 209/1000 | Loss: 0.00004945
Iteration 210/1000 | Loss: 0.00003800
Iteration 211/1000 | Loss: 0.00003651
Iteration 212/1000 | Loss: 0.00004194
Iteration 213/1000 | Loss: 0.00003838
Iteration 214/1000 | Loss: 0.00004606
Iteration 215/1000 | Loss: 0.00004155
Iteration 216/1000 | Loss: 0.00004157
Iteration 217/1000 | Loss: 0.00004390
Iteration 218/1000 | Loss: 0.00004238
Iteration 219/1000 | Loss: 0.00011146
Iteration 220/1000 | Loss: 0.00006627
Iteration 221/1000 | Loss: 0.00004455
Iteration 222/1000 | Loss: 0.00003711
Iteration 223/1000 | Loss: 0.00003527
Iteration 224/1000 | Loss: 0.00003475
Iteration 225/1000 | Loss: 0.00003434
Iteration 226/1000 | Loss: 0.00003404
Iteration 227/1000 | Loss: 0.00003386
Iteration 228/1000 | Loss: 0.00003378
Iteration 229/1000 | Loss: 0.00003370
Iteration 230/1000 | Loss: 0.00003370
Iteration 231/1000 | Loss: 0.00003369
Iteration 232/1000 | Loss: 0.00003369
Iteration 233/1000 | Loss: 0.00003366
Iteration 234/1000 | Loss: 0.00003365
Iteration 235/1000 | Loss: 0.00003365
Iteration 236/1000 | Loss: 0.00003365
Iteration 237/1000 | Loss: 0.00003365
Iteration 238/1000 | Loss: 0.00003365
Iteration 239/1000 | Loss: 0.00003365
Iteration 240/1000 | Loss: 0.00003365
Iteration 241/1000 | Loss: 0.00003365
Iteration 242/1000 | Loss: 0.00003365
Iteration 243/1000 | Loss: 0.00003365
Iteration 244/1000 | Loss: 0.00003365
Iteration 245/1000 | Loss: 0.00003364
Iteration 246/1000 | Loss: 0.00003364
Iteration 247/1000 | Loss: 0.00003364
Iteration 248/1000 | Loss: 0.00003364
Iteration 249/1000 | Loss: 0.00003364
Iteration 250/1000 | Loss: 0.00003363
Iteration 251/1000 | Loss: 0.00003361
Iteration 252/1000 | Loss: 0.00003361
Iteration 253/1000 | Loss: 0.00003361
Iteration 254/1000 | Loss: 0.00003361
Iteration 255/1000 | Loss: 0.00003359
Iteration 256/1000 | Loss: 0.00003359
Iteration 257/1000 | Loss: 0.00003357
Iteration 258/1000 | Loss: 0.00003357
Iteration 259/1000 | Loss: 0.00003357
Iteration 260/1000 | Loss: 0.00003357
Iteration 261/1000 | Loss: 0.00003356
Iteration 262/1000 | Loss: 0.00003356
Iteration 263/1000 | Loss: 0.00003356
Iteration 264/1000 | Loss: 0.00003356
Iteration 265/1000 | Loss: 0.00003356
Iteration 266/1000 | Loss: 0.00003356
Iteration 267/1000 | Loss: 0.00003355
Iteration 268/1000 | Loss: 0.00003353
Iteration 269/1000 | Loss: 0.00003352
Iteration 270/1000 | Loss: 0.00003352
Iteration 271/1000 | Loss: 0.00003352
Iteration 272/1000 | Loss: 0.00003352
Iteration 273/1000 | Loss: 0.00003351
Iteration 274/1000 | Loss: 0.00003351
Iteration 275/1000 | Loss: 0.00003351
Iteration 276/1000 | Loss: 0.00003351
Iteration 277/1000 | Loss: 0.00003351
Iteration 278/1000 | Loss: 0.00003351
Iteration 279/1000 | Loss: 0.00003351
Iteration 280/1000 | Loss: 0.00003350
Iteration 281/1000 | Loss: 0.00003350
Iteration 282/1000 | Loss: 0.00003350
Iteration 283/1000 | Loss: 0.00003350
Iteration 284/1000 | Loss: 0.00003350
Iteration 285/1000 | Loss: 0.00003350
Iteration 286/1000 | Loss: 0.00003350
Iteration 287/1000 | Loss: 0.00003349
Iteration 288/1000 | Loss: 0.00003348
Iteration 289/1000 | Loss: 0.00003348
Iteration 290/1000 | Loss: 0.00003348
Iteration 291/1000 | Loss: 0.00003348
Iteration 292/1000 | Loss: 0.00003347
Iteration 293/1000 | Loss: 0.00003347
Iteration 294/1000 | Loss: 0.00003347
Iteration 295/1000 | Loss: 0.00003346
Iteration 296/1000 | Loss: 0.00003346
Iteration 297/1000 | Loss: 0.00003345
Iteration 298/1000 | Loss: 0.00003345
Iteration 299/1000 | Loss: 0.00003345
Iteration 300/1000 | Loss: 0.00003345
Iteration 301/1000 | Loss: 0.00003345
Iteration 302/1000 | Loss: 0.00003345
Iteration 303/1000 | Loss: 0.00003345
Iteration 304/1000 | Loss: 0.00003344
Iteration 305/1000 | Loss: 0.00003344
Iteration 306/1000 | Loss: 0.00003344
Iteration 307/1000 | Loss: 0.00005306
Iteration 308/1000 | Loss: 0.00005924
Iteration 309/1000 | Loss: 0.00004466
Iteration 310/1000 | Loss: 0.00005165
Iteration 311/1000 | Loss: 0.00004464
Iteration 312/1000 | Loss: 0.00005525
Iteration 313/1000 | Loss: 0.00004891
Iteration 314/1000 | Loss: 0.00004662
Iteration 315/1000 | Loss: 0.00005176
Iteration 316/1000 | Loss: 0.00005063
Iteration 317/1000 | Loss: 0.00004906
Iteration 318/1000 | Loss: 0.00005129
Iteration 319/1000 | Loss: 0.00008575
Iteration 320/1000 | Loss: 0.00003712
Iteration 321/1000 | Loss: 0.00003492
Iteration 322/1000 | Loss: 0.00003413
Iteration 323/1000 | Loss: 0.00003384
Iteration 324/1000 | Loss: 0.00003367
Iteration 325/1000 | Loss: 0.00003354
Iteration 326/1000 | Loss: 0.00003345
Iteration 327/1000 | Loss: 0.00005673
Iteration 328/1000 | Loss: 0.00003443
Iteration 329/1000 | Loss: 0.00003343
Iteration 330/1000 | Loss: 0.00004637
Iteration 331/1000 | Loss: 0.00012273
Iteration 332/1000 | Loss: 0.00004590
Iteration 333/1000 | Loss: 0.00003356
Iteration 334/1000 | Loss: 0.00003340
Iteration 335/1000 | Loss: 0.00004317
Iteration 336/1000 | Loss: 0.00011492
Iteration 337/1000 | Loss: 0.00004745
Iteration 338/1000 | Loss: 0.00008591
Iteration 339/1000 | Loss: 0.00005242
Iteration 340/1000 | Loss: 0.00004551
Iteration 341/1000 | Loss: 0.00010913
Iteration 342/1000 | Loss: 0.00005179
Iteration 343/1000 | Loss: 0.00004403
Iteration 344/1000 | Loss: 0.00010025
Iteration 345/1000 | Loss: 0.00005035
Iteration 346/1000 | Loss: 0.00005310
Iteration 347/1000 | Loss: 0.00004028
Iteration 348/1000 | Loss: 0.00006026
Iteration 349/1000 | Loss: 0.00009676
Iteration 350/1000 | Loss: 0.00005016
Iteration 351/1000 | Loss: 0.00004592
Iteration 352/1000 | Loss: 0.00004217
Iteration 353/1000 | Loss: 0.00008630
Iteration 354/1000 | Loss: 0.00011785
Iteration 355/1000 | Loss: 0.00005749
Iteration 356/1000 | Loss: 0.00007380
Iteration 357/1000 | Loss: 0.00005444
Iteration 358/1000 | Loss: 0.00005835
Iteration 359/1000 | Loss: 0.00003810
Iteration 360/1000 | Loss: 0.00004625
Iteration 361/1000 | Loss: 0.00005082
Iteration 362/1000 | Loss: 0.00004549
Iteration 363/1000 | Loss: 0.00004953
Iteration 364/1000 | Loss: 0.00004528
Iteration 365/1000 | Loss: 0.00003636
Iteration 366/1000 | Loss: 0.00003505
Iteration 367/1000 | Loss: 0.00005311
Iteration 368/1000 | Loss: 0.00004657
Iteration 369/1000 | Loss: 0.00010628
Iteration 370/1000 | Loss: 0.00034931
Iteration 371/1000 | Loss: 0.00008974
Iteration 372/1000 | Loss: 0.00006552
Iteration 373/1000 | Loss: 0.00008881
Iteration 374/1000 | Loss: 0.00004076
Iteration 375/1000 | Loss: 0.00003588
Iteration 376/1000 | Loss: 0.00003453
Iteration 377/1000 | Loss: 0.00003390
Iteration 378/1000 | Loss: 0.00003361
Iteration 379/1000 | Loss: 0.00003355
Iteration 380/1000 | Loss: 0.00003335
Iteration 381/1000 | Loss: 0.00003321
Iteration 382/1000 | Loss: 0.00003315
Iteration 383/1000 | Loss: 0.00003305
Iteration 384/1000 | Loss: 0.00003305
Iteration 385/1000 | Loss: 0.00003304
Iteration 386/1000 | Loss: 0.00003303
Iteration 387/1000 | Loss: 0.00003302
Iteration 388/1000 | Loss: 0.00003301
Iteration 389/1000 | Loss: 0.00003300
Iteration 390/1000 | Loss: 0.00003300
Iteration 391/1000 | Loss: 0.00003300
Iteration 392/1000 | Loss: 0.00003295
Iteration 393/1000 | Loss: 0.00003292
Iteration 394/1000 | Loss: 0.00003284
Iteration 395/1000 | Loss: 0.00003284
Iteration 396/1000 | Loss: 0.00003282
Iteration 397/1000 | Loss: 0.00003280
Iteration 398/1000 | Loss: 0.00003280
Iteration 399/1000 | Loss: 0.00003280
Iteration 400/1000 | Loss: 0.00003280
Iteration 401/1000 | Loss: 0.00003280
Iteration 402/1000 | Loss: 0.00003280
Iteration 403/1000 | Loss: 0.00003279
Iteration 404/1000 | Loss: 0.00003279
Iteration 405/1000 | Loss: 0.00003279
Iteration 406/1000 | Loss: 0.00003279
Iteration 407/1000 | Loss: 0.00003279
Iteration 408/1000 | Loss: 0.00003279
Iteration 409/1000 | Loss: 0.00003279
Iteration 410/1000 | Loss: 0.00003279
Iteration 411/1000 | Loss: 0.00003278
Iteration 412/1000 | Loss: 0.00003277
Iteration 413/1000 | Loss: 0.00003277
Iteration 414/1000 | Loss: 0.00003276
Iteration 415/1000 | Loss: 0.00003275
Iteration 416/1000 | Loss: 0.00003275
Iteration 417/1000 | Loss: 0.00003274
Iteration 418/1000 | Loss: 0.00003274
Iteration 419/1000 | Loss: 0.00003274
Iteration 420/1000 | Loss: 0.00003274
Iteration 421/1000 | Loss: 0.00003273
Iteration 422/1000 | Loss: 0.00003272
Iteration 423/1000 | Loss: 0.00003272
Iteration 424/1000 | Loss: 0.00003271
Iteration 425/1000 | Loss: 0.00003270
Iteration 426/1000 | Loss: 0.00003270
Iteration 427/1000 | Loss: 0.00003269
Iteration 428/1000 | Loss: 0.00003269
Iteration 429/1000 | Loss: 0.00003268
Iteration 430/1000 | Loss: 0.00003268
Iteration 431/1000 | Loss: 0.00003268
Iteration 432/1000 | Loss: 0.00003268
Iteration 433/1000 | Loss: 0.00003268
Iteration 434/1000 | Loss: 0.00003267
Iteration 435/1000 | Loss: 0.00003267
Iteration 436/1000 | Loss: 0.00003267
Iteration 437/1000 | Loss: 0.00003267
Iteration 438/1000 | Loss: 0.00003267
Iteration 439/1000 | Loss: 0.00003267
Iteration 440/1000 | Loss: 0.00003267
Iteration 441/1000 | Loss: 0.00003267
Iteration 442/1000 | Loss: 0.00003267
Iteration 443/1000 | Loss: 0.00003266
Iteration 444/1000 | Loss: 0.00003266
Iteration 445/1000 | Loss: 0.00003266
Iteration 446/1000 | Loss: 0.00003265
Iteration 447/1000 | Loss: 0.00003265
Iteration 448/1000 | Loss: 0.00003265
Iteration 449/1000 | Loss: 0.00003265
Iteration 450/1000 | Loss: 0.00003265
Iteration 451/1000 | Loss: 0.00003265
Iteration 452/1000 | Loss: 0.00003265
Iteration 453/1000 | Loss: 0.00003264
Iteration 454/1000 | Loss: 0.00003264
Iteration 455/1000 | Loss: 0.00003264
Iteration 456/1000 | Loss: 0.00003264
Iteration 457/1000 | Loss: 0.00003264
Iteration 458/1000 | Loss: 0.00003264
Iteration 459/1000 | Loss: 0.00003263
Iteration 460/1000 | Loss: 0.00003263
Iteration 461/1000 | Loss: 0.00003263
Iteration 462/1000 | Loss: 0.00003263
Iteration 463/1000 | Loss: 0.00003263
Iteration 464/1000 | Loss: 0.00003263
Iteration 465/1000 | Loss: 0.00003263
Iteration 466/1000 | Loss: 0.00003262
Iteration 467/1000 | Loss: 0.00003262
Iteration 468/1000 | Loss: 0.00003262
Iteration 469/1000 | Loss: 0.00003262
Iteration 470/1000 | Loss: 0.00003262
Iteration 471/1000 | Loss: 0.00003262
Iteration 472/1000 | Loss: 0.00003262
Iteration 473/1000 | Loss: 0.00003262
Iteration 474/1000 | Loss: 0.00003262
Iteration 475/1000 | Loss: 0.00003262
Iteration 476/1000 | Loss: 0.00003262
Iteration 477/1000 | Loss: 0.00003262
Iteration 478/1000 | Loss: 0.00003262
Iteration 479/1000 | Loss: 0.00003262
Iteration 480/1000 | Loss: 0.00003262
Iteration 481/1000 | Loss: 0.00003262
Iteration 482/1000 | Loss: 0.00003262
Iteration 483/1000 | Loss: 0.00003262
Iteration 484/1000 | Loss: 0.00003262
Iteration 485/1000 | Loss: 0.00003262
Iteration 486/1000 | Loss: 0.00003262
Iteration 487/1000 | Loss: 0.00003262
Iteration 488/1000 | Loss: 0.00003262
Iteration 489/1000 | Loss: 0.00003262
Iteration 490/1000 | Loss: 0.00003262
Iteration 491/1000 | Loss: 0.00003262
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 491. Stopping optimization.
Last 5 losses: [3.261580059188418e-05, 3.261580059188418e-05, 3.261580059188418e-05, 3.261580059188418e-05, 3.261580059188418e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.261580059188418e-05

Optimization complete. Final v2v error: 4.305386066436768 mm

Highest mean error: 10.080265998840332 mm for frame 98

Lowest mean error: 3.831366777420044 mm for frame 162

Saving results

Total time: 500.3532192707062
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_grace_posed_004/1032/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_grace_posed_004/1032.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_grace_posed_004/1032
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00392480
Iteration 2/25 | Loss: 0.00118045
Iteration 3/25 | Loss: 0.00102664
Iteration 4/25 | Loss: 0.00101735
Iteration 5/25 | Loss: 0.00101575
Iteration 6/25 | Loss: 0.00101569
Iteration 7/25 | Loss: 0.00101569
Iteration 8/25 | Loss: 0.00101569
Iteration 9/25 | Loss: 0.00101569
Iteration 10/25 | Loss: 0.00101569
Iteration 11/25 | Loss: 0.00101569
Iteration 12/25 | Loss: 0.00101569
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0010156914358958602, 0.0010156914358958602, 0.0010156914358958602, 0.0010156914358958602, 0.0010156914358958602]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010156914358958602

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.30115592
Iteration 2/25 | Loss: 0.00080090
Iteration 3/25 | Loss: 0.00080090
Iteration 4/25 | Loss: 0.00080090
Iteration 5/25 | Loss: 0.00080090
Iteration 6/25 | Loss: 0.00080090
Iteration 7/25 | Loss: 0.00080089
Iteration 8/25 | Loss: 0.00080089
Iteration 9/25 | Loss: 0.00080089
Iteration 10/25 | Loss: 0.00080089
Iteration 11/25 | Loss: 0.00080089
Iteration 12/25 | Loss: 0.00080089
Iteration 13/25 | Loss: 0.00080089
Iteration 14/25 | Loss: 0.00080089
Iteration 15/25 | Loss: 0.00080089
Iteration 16/25 | Loss: 0.00080089
Iteration 17/25 | Loss: 0.00080089
Iteration 18/25 | Loss: 0.00080089
Iteration 19/25 | Loss: 0.00080089
Iteration 20/25 | Loss: 0.00080089
Iteration 21/25 | Loss: 0.00080089
Iteration 22/25 | Loss: 0.00080089
Iteration 23/25 | Loss: 0.00080089
Iteration 24/25 | Loss: 0.00080089
Iteration 25/25 | Loss: 0.00080089

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00080089
Iteration 2/1000 | Loss: 0.00002035
Iteration 3/1000 | Loss: 0.00001383
Iteration 4/1000 | Loss: 0.00001237
Iteration 5/1000 | Loss: 0.00001160
Iteration 6/1000 | Loss: 0.00001106
Iteration 7/1000 | Loss: 0.00001061
Iteration 8/1000 | Loss: 0.00001035
Iteration 9/1000 | Loss: 0.00001010
Iteration 10/1000 | Loss: 0.00000997
Iteration 11/1000 | Loss: 0.00000985
Iteration 12/1000 | Loss: 0.00000976
Iteration 13/1000 | Loss: 0.00000976
Iteration 14/1000 | Loss: 0.00000972
Iteration 15/1000 | Loss: 0.00000972
Iteration 16/1000 | Loss: 0.00000971
Iteration 17/1000 | Loss: 0.00000970
Iteration 18/1000 | Loss: 0.00000969
Iteration 19/1000 | Loss: 0.00000969
Iteration 20/1000 | Loss: 0.00000965
Iteration 21/1000 | Loss: 0.00000964
Iteration 22/1000 | Loss: 0.00000963
Iteration 23/1000 | Loss: 0.00000961
Iteration 24/1000 | Loss: 0.00000961
Iteration 25/1000 | Loss: 0.00000960
Iteration 26/1000 | Loss: 0.00000959
Iteration 27/1000 | Loss: 0.00000959
Iteration 28/1000 | Loss: 0.00000958
Iteration 29/1000 | Loss: 0.00000958
Iteration 30/1000 | Loss: 0.00000957
Iteration 31/1000 | Loss: 0.00000957
Iteration 32/1000 | Loss: 0.00000956
Iteration 33/1000 | Loss: 0.00000956
Iteration 34/1000 | Loss: 0.00000955
Iteration 35/1000 | Loss: 0.00000955
Iteration 36/1000 | Loss: 0.00000954
Iteration 37/1000 | Loss: 0.00000953
Iteration 38/1000 | Loss: 0.00000953
Iteration 39/1000 | Loss: 0.00000953
Iteration 40/1000 | Loss: 0.00000952
Iteration 41/1000 | Loss: 0.00000952
Iteration 42/1000 | Loss: 0.00000952
Iteration 43/1000 | Loss: 0.00000952
Iteration 44/1000 | Loss: 0.00000951
Iteration 45/1000 | Loss: 0.00000948
Iteration 46/1000 | Loss: 0.00000947
Iteration 47/1000 | Loss: 0.00000947
Iteration 48/1000 | Loss: 0.00000947
Iteration 49/1000 | Loss: 0.00000946
Iteration 50/1000 | Loss: 0.00000945
Iteration 51/1000 | Loss: 0.00000945
Iteration 52/1000 | Loss: 0.00000945
Iteration 53/1000 | Loss: 0.00000945
Iteration 54/1000 | Loss: 0.00000944
Iteration 55/1000 | Loss: 0.00000944
Iteration 56/1000 | Loss: 0.00000944
Iteration 57/1000 | Loss: 0.00000944
Iteration 58/1000 | Loss: 0.00000944
Iteration 59/1000 | Loss: 0.00000944
Iteration 60/1000 | Loss: 0.00000944
Iteration 61/1000 | Loss: 0.00000944
Iteration 62/1000 | Loss: 0.00000944
Iteration 63/1000 | Loss: 0.00000943
Iteration 64/1000 | Loss: 0.00000943
Iteration 65/1000 | Loss: 0.00000939
Iteration 66/1000 | Loss: 0.00000939
Iteration 67/1000 | Loss: 0.00000939
Iteration 68/1000 | Loss: 0.00000939
Iteration 69/1000 | Loss: 0.00000939
Iteration 70/1000 | Loss: 0.00000937
Iteration 71/1000 | Loss: 0.00000936
Iteration 72/1000 | Loss: 0.00000936
Iteration 73/1000 | Loss: 0.00000935
Iteration 74/1000 | Loss: 0.00000935
Iteration 75/1000 | Loss: 0.00000934
Iteration 76/1000 | Loss: 0.00000934
Iteration 77/1000 | Loss: 0.00000933
Iteration 78/1000 | Loss: 0.00000933
Iteration 79/1000 | Loss: 0.00000933
Iteration 80/1000 | Loss: 0.00000932
Iteration 81/1000 | Loss: 0.00000932
Iteration 82/1000 | Loss: 0.00000932
Iteration 83/1000 | Loss: 0.00000932
Iteration 84/1000 | Loss: 0.00000932
Iteration 85/1000 | Loss: 0.00000931
Iteration 86/1000 | Loss: 0.00000931
Iteration 87/1000 | Loss: 0.00000931
Iteration 88/1000 | Loss: 0.00000931
Iteration 89/1000 | Loss: 0.00000930
Iteration 90/1000 | Loss: 0.00000930
Iteration 91/1000 | Loss: 0.00000930
Iteration 92/1000 | Loss: 0.00000929
Iteration 93/1000 | Loss: 0.00000929
Iteration 94/1000 | Loss: 0.00000929
Iteration 95/1000 | Loss: 0.00000928
Iteration 96/1000 | Loss: 0.00000928
Iteration 97/1000 | Loss: 0.00000928
Iteration 98/1000 | Loss: 0.00000928
Iteration 99/1000 | Loss: 0.00000927
Iteration 100/1000 | Loss: 0.00000927
Iteration 101/1000 | Loss: 0.00000926
Iteration 102/1000 | Loss: 0.00000926
Iteration 103/1000 | Loss: 0.00000926
Iteration 104/1000 | Loss: 0.00000926
Iteration 105/1000 | Loss: 0.00000926
Iteration 106/1000 | Loss: 0.00000926
Iteration 107/1000 | Loss: 0.00000926
Iteration 108/1000 | Loss: 0.00000926
Iteration 109/1000 | Loss: 0.00000925
Iteration 110/1000 | Loss: 0.00000925
Iteration 111/1000 | Loss: 0.00000925
Iteration 112/1000 | Loss: 0.00000925
Iteration 113/1000 | Loss: 0.00000925
Iteration 114/1000 | Loss: 0.00000925
Iteration 115/1000 | Loss: 0.00000925
Iteration 116/1000 | Loss: 0.00000924
Iteration 117/1000 | Loss: 0.00000924
Iteration 118/1000 | Loss: 0.00000924
Iteration 119/1000 | Loss: 0.00000924
Iteration 120/1000 | Loss: 0.00000924
Iteration 121/1000 | Loss: 0.00000924
Iteration 122/1000 | Loss: 0.00000924
Iteration 123/1000 | Loss: 0.00000923
Iteration 124/1000 | Loss: 0.00000923
Iteration 125/1000 | Loss: 0.00000923
Iteration 126/1000 | Loss: 0.00000923
Iteration 127/1000 | Loss: 0.00000922
Iteration 128/1000 | Loss: 0.00000922
Iteration 129/1000 | Loss: 0.00000922
Iteration 130/1000 | Loss: 0.00000922
Iteration 131/1000 | Loss: 0.00000922
Iteration 132/1000 | Loss: 0.00000922
Iteration 133/1000 | Loss: 0.00000922
Iteration 134/1000 | Loss: 0.00000922
Iteration 135/1000 | Loss: 0.00000922
Iteration 136/1000 | Loss: 0.00000922
Iteration 137/1000 | Loss: 0.00000922
Iteration 138/1000 | Loss: 0.00000921
Iteration 139/1000 | Loss: 0.00000921
Iteration 140/1000 | Loss: 0.00000921
Iteration 141/1000 | Loss: 0.00000921
Iteration 142/1000 | Loss: 0.00000921
Iteration 143/1000 | Loss: 0.00000921
Iteration 144/1000 | Loss: 0.00000921
Iteration 145/1000 | Loss: 0.00000920
Iteration 146/1000 | Loss: 0.00000920
Iteration 147/1000 | Loss: 0.00000920
Iteration 148/1000 | Loss: 0.00000920
Iteration 149/1000 | Loss: 0.00000920
Iteration 150/1000 | Loss: 0.00000920
Iteration 151/1000 | Loss: 0.00000920
Iteration 152/1000 | Loss: 0.00000920
Iteration 153/1000 | Loss: 0.00000920
Iteration 154/1000 | Loss: 0.00000919
Iteration 155/1000 | Loss: 0.00000919
Iteration 156/1000 | Loss: 0.00000919
Iteration 157/1000 | Loss: 0.00000919
Iteration 158/1000 | Loss: 0.00000919
Iteration 159/1000 | Loss: 0.00000919
Iteration 160/1000 | Loss: 0.00000919
Iteration 161/1000 | Loss: 0.00000919
Iteration 162/1000 | Loss: 0.00000919
Iteration 163/1000 | Loss: 0.00000919
Iteration 164/1000 | Loss: 0.00000919
Iteration 165/1000 | Loss: 0.00000919
Iteration 166/1000 | Loss: 0.00000919
Iteration 167/1000 | Loss: 0.00000919
Iteration 168/1000 | Loss: 0.00000919
Iteration 169/1000 | Loss: 0.00000919
Iteration 170/1000 | Loss: 0.00000918
Iteration 171/1000 | Loss: 0.00000918
Iteration 172/1000 | Loss: 0.00000918
Iteration 173/1000 | Loss: 0.00000918
Iteration 174/1000 | Loss: 0.00000918
Iteration 175/1000 | Loss: 0.00000918
Iteration 176/1000 | Loss: 0.00000918
Iteration 177/1000 | Loss: 0.00000918
Iteration 178/1000 | Loss: 0.00000918
Iteration 179/1000 | Loss: 0.00000918
Iteration 180/1000 | Loss: 0.00000918
Iteration 181/1000 | Loss: 0.00000918
Iteration 182/1000 | Loss: 0.00000918
Iteration 183/1000 | Loss: 0.00000918
Iteration 184/1000 | Loss: 0.00000918
Iteration 185/1000 | Loss: 0.00000918
Iteration 186/1000 | Loss: 0.00000918
Iteration 187/1000 | Loss: 0.00000918
Iteration 188/1000 | Loss: 0.00000918
Iteration 189/1000 | Loss: 0.00000917
Iteration 190/1000 | Loss: 0.00000917
Iteration 191/1000 | Loss: 0.00000917
Iteration 192/1000 | Loss: 0.00000917
Iteration 193/1000 | Loss: 0.00000917
Iteration 194/1000 | Loss: 0.00000917
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 194. Stopping optimization.
Last 5 losses: [9.174743354378734e-06, 9.174743354378734e-06, 9.174743354378734e-06, 9.174743354378734e-06, 9.174743354378734e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.174743354378734e-06

Optimization complete. Final v2v error: 2.6151351928710938 mm

Highest mean error: 2.7267613410949707 mm for frame 67

Lowest mean error: 2.487337827682495 mm for frame 2

Saving results

Total time: 39.798404693603516
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_grace_posed_004/1040/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_grace_posed_004/1040.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_grace_posed_004/1040
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00995661
Iteration 2/25 | Loss: 0.00182259
Iteration 3/25 | Loss: 0.00118605
Iteration 4/25 | Loss: 0.00113979
Iteration 5/25 | Loss: 0.00113595
Iteration 6/25 | Loss: 0.00113595
Iteration 7/25 | Loss: 0.00113595
Iteration 8/25 | Loss: 0.00113595
Iteration 9/25 | Loss: 0.00113595
Iteration 10/25 | Loss: 0.00113595
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0011359491618350148, 0.0011359491618350148, 0.0011359491618350148, 0.0011359491618350148, 0.0011359491618350148]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011359491618350148

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.26846743
Iteration 2/25 | Loss: 0.00128452
Iteration 3/25 | Loss: 0.00128452
Iteration 4/25 | Loss: 0.00128452
Iteration 5/25 | Loss: 0.00128452
Iteration 6/25 | Loss: 0.00128451
Iteration 7/25 | Loss: 0.00128451
Iteration 8/25 | Loss: 0.00128451
Iteration 9/25 | Loss: 0.00128451
Iteration 10/25 | Loss: 0.00128451
Iteration 11/25 | Loss: 0.00128451
Iteration 12/25 | Loss: 0.00128451
Iteration 13/25 | Loss: 0.00128451
Iteration 14/25 | Loss: 0.00128451
Iteration 15/25 | Loss: 0.00128451
Iteration 16/25 | Loss: 0.00128451
Iteration 17/25 | Loss: 0.00128451
Iteration 18/25 | Loss: 0.00128451
Iteration 19/25 | Loss: 0.00128451
Iteration 20/25 | Loss: 0.00128451
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0012845136225223541, 0.0012845136225223541, 0.0012845136225223541, 0.0012845136225223541, 0.0012845136225223541]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012845136225223541

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00128451
Iteration 2/1000 | Loss: 0.00002740
Iteration 3/1000 | Loss: 0.00001824
Iteration 4/1000 | Loss: 0.00001566
Iteration 5/1000 | Loss: 0.00001445
Iteration 6/1000 | Loss: 0.00001375
Iteration 7/1000 | Loss: 0.00001326
Iteration 8/1000 | Loss: 0.00001283
Iteration 9/1000 | Loss: 0.00001274
Iteration 10/1000 | Loss: 0.00001248
Iteration 11/1000 | Loss: 0.00001233
Iteration 12/1000 | Loss: 0.00001228
Iteration 13/1000 | Loss: 0.00001224
Iteration 14/1000 | Loss: 0.00001222
Iteration 15/1000 | Loss: 0.00001221
Iteration 16/1000 | Loss: 0.00001221
Iteration 17/1000 | Loss: 0.00001220
Iteration 18/1000 | Loss: 0.00001219
Iteration 19/1000 | Loss: 0.00001217
Iteration 20/1000 | Loss: 0.00001217
Iteration 21/1000 | Loss: 0.00001216
Iteration 22/1000 | Loss: 0.00001215
Iteration 23/1000 | Loss: 0.00001212
Iteration 24/1000 | Loss: 0.00001212
Iteration 25/1000 | Loss: 0.00001209
Iteration 26/1000 | Loss: 0.00001209
Iteration 27/1000 | Loss: 0.00001209
Iteration 28/1000 | Loss: 0.00001208
Iteration 29/1000 | Loss: 0.00001208
Iteration 30/1000 | Loss: 0.00001207
Iteration 31/1000 | Loss: 0.00001207
Iteration 32/1000 | Loss: 0.00001207
Iteration 33/1000 | Loss: 0.00001207
Iteration 34/1000 | Loss: 0.00001206
Iteration 35/1000 | Loss: 0.00001206
Iteration 36/1000 | Loss: 0.00001206
Iteration 37/1000 | Loss: 0.00001205
Iteration 38/1000 | Loss: 0.00001202
Iteration 39/1000 | Loss: 0.00001202
Iteration 40/1000 | Loss: 0.00001200
Iteration 41/1000 | Loss: 0.00001199
Iteration 42/1000 | Loss: 0.00001199
Iteration 43/1000 | Loss: 0.00001199
Iteration 44/1000 | Loss: 0.00001199
Iteration 45/1000 | Loss: 0.00001198
Iteration 46/1000 | Loss: 0.00001198
Iteration 47/1000 | Loss: 0.00001198
Iteration 48/1000 | Loss: 0.00001197
Iteration 49/1000 | Loss: 0.00001197
Iteration 50/1000 | Loss: 0.00001197
Iteration 51/1000 | Loss: 0.00001197
Iteration 52/1000 | Loss: 0.00001197
Iteration 53/1000 | Loss: 0.00001197
Iteration 54/1000 | Loss: 0.00001197
Iteration 55/1000 | Loss: 0.00001197
Iteration 56/1000 | Loss: 0.00001197
Iteration 57/1000 | Loss: 0.00001196
Iteration 58/1000 | Loss: 0.00001196
Iteration 59/1000 | Loss: 0.00001196
Iteration 60/1000 | Loss: 0.00001195
Iteration 61/1000 | Loss: 0.00001195
Iteration 62/1000 | Loss: 0.00001194
Iteration 63/1000 | Loss: 0.00001194
Iteration 64/1000 | Loss: 0.00001194
Iteration 65/1000 | Loss: 0.00001193
Iteration 66/1000 | Loss: 0.00001193
Iteration 67/1000 | Loss: 0.00001193
Iteration 68/1000 | Loss: 0.00001193
Iteration 69/1000 | Loss: 0.00001193
Iteration 70/1000 | Loss: 0.00001193
Iteration 71/1000 | Loss: 0.00001193
Iteration 72/1000 | Loss: 0.00001193
Iteration 73/1000 | Loss: 0.00001193
Iteration 74/1000 | Loss: 0.00001193
Iteration 75/1000 | Loss: 0.00001192
Iteration 76/1000 | Loss: 0.00001192
Iteration 77/1000 | Loss: 0.00001192
Iteration 78/1000 | Loss: 0.00001192
Iteration 79/1000 | Loss: 0.00001192
Iteration 80/1000 | Loss: 0.00001192
Iteration 81/1000 | Loss: 0.00001192
Iteration 82/1000 | Loss: 0.00001192
Iteration 83/1000 | Loss: 0.00001192
Iteration 84/1000 | Loss: 0.00001192
Iteration 85/1000 | Loss: 0.00001192
Iteration 86/1000 | Loss: 0.00001192
Iteration 87/1000 | Loss: 0.00001192
Iteration 88/1000 | Loss: 0.00001192
Iteration 89/1000 | Loss: 0.00001192
Iteration 90/1000 | Loss: 0.00001192
Iteration 91/1000 | Loss: 0.00001191
Iteration 92/1000 | Loss: 0.00001191
Iteration 93/1000 | Loss: 0.00001191
Iteration 94/1000 | Loss: 0.00001191
Iteration 95/1000 | Loss: 0.00001191
Iteration 96/1000 | Loss: 0.00001191
Iteration 97/1000 | Loss: 0.00001191
Iteration 98/1000 | Loss: 0.00001191
Iteration 99/1000 | Loss: 0.00001191
Iteration 100/1000 | Loss: 0.00001191
Iteration 101/1000 | Loss: 0.00001191
Iteration 102/1000 | Loss: 0.00001191
Iteration 103/1000 | Loss: 0.00001191
Iteration 104/1000 | Loss: 0.00001191
Iteration 105/1000 | Loss: 0.00001191
Iteration 106/1000 | Loss: 0.00001191
Iteration 107/1000 | Loss: 0.00001191
Iteration 108/1000 | Loss: 0.00001191
Iteration 109/1000 | Loss: 0.00001191
Iteration 110/1000 | Loss: 0.00001191
Iteration 111/1000 | Loss: 0.00001191
Iteration 112/1000 | Loss: 0.00001191
Iteration 113/1000 | Loss: 0.00001191
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 113. Stopping optimization.
Last 5 losses: [1.1908231499546673e-05, 1.1908231499546673e-05, 1.1908231499546673e-05, 1.1908231499546673e-05, 1.1908231499546673e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1908231499546673e-05

Optimization complete. Final v2v error: 2.8882179260253906 mm

Highest mean error: 3.683711290359497 mm for frame 212

Lowest mean error: 2.415609121322632 mm for frame 176

Saving results

Total time: 34.85667967796326
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_grace_posed_004/1006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_grace_posed_004/1006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_grace_posed_004/1006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00977201
Iteration 2/25 | Loss: 0.00225395
Iteration 3/25 | Loss: 0.00149699
Iteration 4/25 | Loss: 0.00126031
Iteration 5/25 | Loss: 0.00121971
Iteration 6/25 | Loss: 0.00122686
Iteration 7/25 | Loss: 0.00117409
Iteration 8/25 | Loss: 0.00115755
Iteration 9/25 | Loss: 0.00116668
Iteration 10/25 | Loss: 0.00117222
Iteration 11/25 | Loss: 0.00117566
Iteration 12/25 | Loss: 0.00112325
Iteration 13/25 | Loss: 0.00111185
Iteration 14/25 | Loss: 0.00111019
Iteration 15/25 | Loss: 0.00110187
Iteration 16/25 | Loss: 0.00109928
Iteration 17/25 | Loss: 0.00109657
Iteration 18/25 | Loss: 0.00109128
Iteration 19/25 | Loss: 0.00109123
Iteration 20/25 | Loss: 0.00109231
Iteration 21/25 | Loss: 0.00109123
Iteration 22/25 | Loss: 0.00109040
Iteration 23/25 | Loss: 0.00108873
Iteration 24/25 | Loss: 0.00108694
Iteration 25/25 | Loss: 0.00108802

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.20461130
Iteration 2/25 | Loss: 0.00104721
Iteration 3/25 | Loss: 0.00096788
Iteration 4/25 | Loss: 0.00096788
Iteration 5/25 | Loss: 0.00096787
Iteration 6/25 | Loss: 0.00096787
Iteration 7/25 | Loss: 0.00096787
Iteration 8/25 | Loss: 0.00096787
Iteration 9/25 | Loss: 0.00096787
Iteration 10/25 | Loss: 0.00096787
Iteration 11/25 | Loss: 0.00096787
Iteration 12/25 | Loss: 0.00096787
Iteration 13/25 | Loss: 0.00096787
Iteration 14/25 | Loss: 0.00096787
Iteration 15/25 | Loss: 0.00096787
Iteration 16/25 | Loss: 0.00096787
Iteration 17/25 | Loss: 0.00096787
Iteration 18/25 | Loss: 0.00096787
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.000967871630564332, 0.000967871630564332, 0.000967871630564332, 0.000967871630564332, 0.000967871630564332]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000967871630564332

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00096787
Iteration 2/1000 | Loss: 0.00025458
Iteration 3/1000 | Loss: 0.00038432
Iteration 4/1000 | Loss: 0.00007031
Iteration 5/1000 | Loss: 0.00021794
Iteration 6/1000 | Loss: 0.00008615
Iteration 7/1000 | Loss: 0.00007179
Iteration 8/1000 | Loss: 0.00006727
Iteration 9/1000 | Loss: 0.00006630
Iteration 10/1000 | Loss: 0.00003296
Iteration 11/1000 | Loss: 0.00003032
Iteration 12/1000 | Loss: 0.00012289
Iteration 13/1000 | Loss: 0.00003281
Iteration 14/1000 | Loss: 0.00003066
Iteration 15/1000 | Loss: 0.00002887
Iteration 16/1000 | Loss: 0.00023965
Iteration 17/1000 | Loss: 0.00022008
Iteration 18/1000 | Loss: 0.00003291
Iteration 19/1000 | Loss: 0.00003684
Iteration 20/1000 | Loss: 0.00009740
Iteration 21/1000 | Loss: 0.00017350
Iteration 22/1000 | Loss: 0.00015885
Iteration 23/1000 | Loss: 0.00022972
Iteration 24/1000 | Loss: 0.00005417
Iteration 25/1000 | Loss: 0.00013604
Iteration 26/1000 | Loss: 0.00011414
Iteration 27/1000 | Loss: 0.00003796
Iteration 28/1000 | Loss: 0.00017102
Iteration 29/1000 | Loss: 0.00013685
Iteration 30/1000 | Loss: 0.00028710
Iteration 31/1000 | Loss: 0.00019309
Iteration 32/1000 | Loss: 0.00008652
Iteration 33/1000 | Loss: 0.00003050
Iteration 34/1000 | Loss: 0.00008695
Iteration 35/1000 | Loss: 0.00007143
Iteration 36/1000 | Loss: 0.00008708
Iteration 37/1000 | Loss: 0.00003193
Iteration 38/1000 | Loss: 0.00014420
Iteration 39/1000 | Loss: 0.00014572
Iteration 40/1000 | Loss: 0.00009620
Iteration 41/1000 | Loss: 0.00010294
Iteration 42/1000 | Loss: 0.00002931
Iteration 43/1000 | Loss: 0.00002844
Iteration 44/1000 | Loss: 0.00022518
Iteration 45/1000 | Loss: 0.00024187
Iteration 46/1000 | Loss: 0.00004742
Iteration 47/1000 | Loss: 0.00003598
Iteration 48/1000 | Loss: 0.00021486
Iteration 49/1000 | Loss: 0.00003886
Iteration 50/1000 | Loss: 0.00002765
Iteration 51/1000 | Loss: 0.00002637
Iteration 52/1000 | Loss: 0.00013111
Iteration 53/1000 | Loss: 0.00005268
Iteration 54/1000 | Loss: 0.00003580
Iteration 55/1000 | Loss: 0.00018100
Iteration 56/1000 | Loss: 0.00007296
Iteration 57/1000 | Loss: 0.00039788
Iteration 58/1000 | Loss: 0.00030755
Iteration 59/1000 | Loss: 0.00059265
Iteration 60/1000 | Loss: 0.00022450
Iteration 61/1000 | Loss: 0.00023941
Iteration 62/1000 | Loss: 0.00022377
Iteration 63/1000 | Loss: 0.00015943
Iteration 64/1000 | Loss: 0.00019042
Iteration 65/1000 | Loss: 0.00004426
Iteration 66/1000 | Loss: 0.00003770
Iteration 67/1000 | Loss: 0.00002804
Iteration 68/1000 | Loss: 0.00002838
Iteration 69/1000 | Loss: 0.00002614
Iteration 70/1000 | Loss: 0.00004474
Iteration 71/1000 | Loss: 0.00002535
Iteration 72/1000 | Loss: 0.00002491
Iteration 73/1000 | Loss: 0.00003591
Iteration 74/1000 | Loss: 0.00020523
Iteration 75/1000 | Loss: 0.00005247
Iteration 76/1000 | Loss: 0.00004493
Iteration 77/1000 | Loss: 0.00010572
Iteration 78/1000 | Loss: 0.00005305
Iteration 79/1000 | Loss: 0.00002560
Iteration 80/1000 | Loss: 0.00002446
Iteration 81/1000 | Loss: 0.00002387
Iteration 82/1000 | Loss: 0.00002363
Iteration 83/1000 | Loss: 0.00002330
Iteration 84/1000 | Loss: 0.00013800
Iteration 85/1000 | Loss: 0.00004303
Iteration 86/1000 | Loss: 0.00015896
Iteration 87/1000 | Loss: 0.00005851
Iteration 88/1000 | Loss: 0.00006018
Iteration 89/1000 | Loss: 0.00006074
Iteration 90/1000 | Loss: 0.00017897
Iteration 91/1000 | Loss: 0.00002905
Iteration 92/1000 | Loss: 0.00004631
Iteration 93/1000 | Loss: 0.00002579
Iteration 94/1000 | Loss: 0.00005876
Iteration 95/1000 | Loss: 0.00004402
Iteration 96/1000 | Loss: 0.00015473
Iteration 97/1000 | Loss: 0.00007179
Iteration 98/1000 | Loss: 0.00004248
Iteration 99/1000 | Loss: 0.00005737
Iteration 100/1000 | Loss: 0.00005622
Iteration 101/1000 | Loss: 0.00016179
Iteration 102/1000 | Loss: 0.00014942
Iteration 103/1000 | Loss: 0.00012991
Iteration 104/1000 | Loss: 0.00012598
Iteration 105/1000 | Loss: 0.00003162
Iteration 106/1000 | Loss: 0.00009730
Iteration 107/1000 | Loss: 0.00014285
Iteration 108/1000 | Loss: 0.00040198
Iteration 109/1000 | Loss: 0.00017097
Iteration 110/1000 | Loss: 0.00036599
Iteration 111/1000 | Loss: 0.00036223
Iteration 112/1000 | Loss: 0.00012222
Iteration 113/1000 | Loss: 0.00029366
Iteration 114/1000 | Loss: 0.00013531
Iteration 115/1000 | Loss: 0.00021496
Iteration 116/1000 | Loss: 0.00008607
Iteration 117/1000 | Loss: 0.00002607
Iteration 118/1000 | Loss: 0.00002480
Iteration 119/1000 | Loss: 0.00002431
Iteration 120/1000 | Loss: 0.00002387
Iteration 121/1000 | Loss: 0.00002363
Iteration 122/1000 | Loss: 0.00008063
Iteration 123/1000 | Loss: 0.00007118
Iteration 124/1000 | Loss: 0.00005619
Iteration 125/1000 | Loss: 0.00002828
Iteration 126/1000 | Loss: 0.00002546
Iteration 127/1000 | Loss: 0.00002404
Iteration 128/1000 | Loss: 0.00002319
Iteration 129/1000 | Loss: 0.00002256
Iteration 130/1000 | Loss: 0.00002204
Iteration 131/1000 | Loss: 0.00002163
Iteration 132/1000 | Loss: 0.00002129
Iteration 133/1000 | Loss: 0.00002108
Iteration 134/1000 | Loss: 0.00002101
Iteration 135/1000 | Loss: 0.00002097
Iteration 136/1000 | Loss: 0.00002095
Iteration 137/1000 | Loss: 0.00002093
Iteration 138/1000 | Loss: 0.00002093
Iteration 139/1000 | Loss: 0.00002092
Iteration 140/1000 | Loss: 0.00002081
Iteration 141/1000 | Loss: 0.00002080
Iteration 142/1000 | Loss: 0.00002079
Iteration 143/1000 | Loss: 0.00002078
Iteration 144/1000 | Loss: 0.00002077
Iteration 145/1000 | Loss: 0.00002077
Iteration 146/1000 | Loss: 0.00002077
Iteration 147/1000 | Loss: 0.00002076
Iteration 148/1000 | Loss: 0.00002076
Iteration 149/1000 | Loss: 0.00003071
Iteration 150/1000 | Loss: 0.00002537
Iteration 151/1000 | Loss: 0.00002260
Iteration 152/1000 | Loss: 0.00002161
Iteration 153/1000 | Loss: 0.00002110
Iteration 154/1000 | Loss: 0.00002067
Iteration 155/1000 | Loss: 0.00002051
Iteration 156/1000 | Loss: 0.00002051
Iteration 157/1000 | Loss: 0.00002049
Iteration 158/1000 | Loss: 0.00002041
Iteration 159/1000 | Loss: 0.00002041
Iteration 160/1000 | Loss: 0.00002040
Iteration 161/1000 | Loss: 0.00002037
Iteration 162/1000 | Loss: 0.00002030
Iteration 163/1000 | Loss: 0.00002030
Iteration 164/1000 | Loss: 0.00002029
Iteration 165/1000 | Loss: 0.00002029
Iteration 166/1000 | Loss: 0.00002028
Iteration 167/1000 | Loss: 0.00002027
Iteration 168/1000 | Loss: 0.00002027
Iteration 169/1000 | Loss: 0.00002025
Iteration 170/1000 | Loss: 0.00002024
Iteration 171/1000 | Loss: 0.00002023
Iteration 172/1000 | Loss: 0.00002023
Iteration 173/1000 | Loss: 0.00002022
Iteration 174/1000 | Loss: 0.00002022
Iteration 175/1000 | Loss: 0.00002021
Iteration 176/1000 | Loss: 0.00002021
Iteration 177/1000 | Loss: 0.00002021
Iteration 178/1000 | Loss: 0.00002021
Iteration 179/1000 | Loss: 0.00002021
Iteration 180/1000 | Loss: 0.00002021
Iteration 181/1000 | Loss: 0.00002021
Iteration 182/1000 | Loss: 0.00002021
Iteration 183/1000 | Loss: 0.00002021
Iteration 184/1000 | Loss: 0.00002021
Iteration 185/1000 | Loss: 0.00002020
Iteration 186/1000 | Loss: 0.00002020
Iteration 187/1000 | Loss: 0.00002020
Iteration 188/1000 | Loss: 0.00002020
Iteration 189/1000 | Loss: 0.00002019
Iteration 190/1000 | Loss: 0.00002019
Iteration 191/1000 | Loss: 0.00002018
Iteration 192/1000 | Loss: 0.00002018
Iteration 193/1000 | Loss: 0.00002018
Iteration 194/1000 | Loss: 0.00002018
Iteration 195/1000 | Loss: 0.00002017
Iteration 196/1000 | Loss: 0.00002017
Iteration 197/1000 | Loss: 0.00002016
Iteration 198/1000 | Loss: 0.00002016
Iteration 199/1000 | Loss: 0.00002016
Iteration 200/1000 | Loss: 0.00002016
Iteration 201/1000 | Loss: 0.00002016
Iteration 202/1000 | Loss: 0.00002015
Iteration 203/1000 | Loss: 0.00002015
Iteration 204/1000 | Loss: 0.00002015
Iteration 205/1000 | Loss: 0.00002015
Iteration 206/1000 | Loss: 0.00002014
Iteration 207/1000 | Loss: 0.00002014
Iteration 208/1000 | Loss: 0.00002014
Iteration 209/1000 | Loss: 0.00002014
Iteration 210/1000 | Loss: 0.00002014
Iteration 211/1000 | Loss: 0.00002014
Iteration 212/1000 | Loss: 0.00002013
Iteration 213/1000 | Loss: 0.00002012
Iteration 214/1000 | Loss: 0.00002012
Iteration 215/1000 | Loss: 0.00002012
Iteration 216/1000 | Loss: 0.00002011
Iteration 217/1000 | Loss: 0.00002011
Iteration 218/1000 | Loss: 0.00002010
Iteration 219/1000 | Loss: 0.00002010
Iteration 220/1000 | Loss: 0.00002010
Iteration 221/1000 | Loss: 0.00002010
Iteration 222/1000 | Loss: 0.00002009
Iteration 223/1000 | Loss: 0.00002009
Iteration 224/1000 | Loss: 0.00002008
Iteration 225/1000 | Loss: 0.00002008
Iteration 226/1000 | Loss: 0.00002008
Iteration 227/1000 | Loss: 0.00002008
Iteration 228/1000 | Loss: 0.00002008
Iteration 229/1000 | Loss: 0.00002008
Iteration 230/1000 | Loss: 0.00002008
Iteration 231/1000 | Loss: 0.00002007
Iteration 232/1000 | Loss: 0.00002007
Iteration 233/1000 | Loss: 0.00002007
Iteration 234/1000 | Loss: 0.00002007
Iteration 235/1000 | Loss: 0.00002006
Iteration 236/1000 | Loss: 0.00002006
Iteration 237/1000 | Loss: 0.00002006
Iteration 238/1000 | Loss: 0.00002005
Iteration 239/1000 | Loss: 0.00002005
Iteration 240/1000 | Loss: 0.00002005
Iteration 241/1000 | Loss: 0.00002004
Iteration 242/1000 | Loss: 0.00002004
Iteration 243/1000 | Loss: 0.00002004
Iteration 244/1000 | Loss: 0.00002004
Iteration 245/1000 | Loss: 0.00002004
Iteration 246/1000 | Loss: 0.00002004
Iteration 247/1000 | Loss: 0.00002004
Iteration 248/1000 | Loss: 0.00002004
Iteration 249/1000 | Loss: 0.00002004
Iteration 250/1000 | Loss: 0.00002003
Iteration 251/1000 | Loss: 0.00002003
Iteration 252/1000 | Loss: 0.00002003
Iteration 253/1000 | Loss: 0.00002003
Iteration 254/1000 | Loss: 0.00002003
Iteration 255/1000 | Loss: 0.00002003
Iteration 256/1000 | Loss: 0.00002003
Iteration 257/1000 | Loss: 0.00002003
Iteration 258/1000 | Loss: 0.00002003
Iteration 259/1000 | Loss: 0.00002002
Iteration 260/1000 | Loss: 0.00002002
Iteration 261/1000 | Loss: 0.00002002
Iteration 262/1000 | Loss: 0.00002002
Iteration 263/1000 | Loss: 0.00002002
Iteration 264/1000 | Loss: 0.00002002
Iteration 265/1000 | Loss: 0.00002002
Iteration 266/1000 | Loss: 0.00002002
Iteration 267/1000 | Loss: 0.00002002
Iteration 268/1000 | Loss: 0.00002002
Iteration 269/1000 | Loss: 0.00002002
Iteration 270/1000 | Loss: 0.00002002
Iteration 271/1000 | Loss: 0.00002001
Iteration 272/1000 | Loss: 0.00002001
Iteration 273/1000 | Loss: 0.00002001
Iteration 274/1000 | Loss: 0.00002001
Iteration 275/1000 | Loss: 0.00002001
Iteration 276/1000 | Loss: 0.00002001
Iteration 277/1000 | Loss: 0.00002001
Iteration 278/1000 | Loss: 0.00002000
Iteration 279/1000 | Loss: 0.00002000
Iteration 280/1000 | Loss: 0.00002000
Iteration 281/1000 | Loss: 0.00002000
Iteration 282/1000 | Loss: 0.00002000
Iteration 283/1000 | Loss: 0.00002000
Iteration 284/1000 | Loss: 0.00002000
Iteration 285/1000 | Loss: 0.00002000
Iteration 286/1000 | Loss: 0.00002000
Iteration 287/1000 | Loss: 0.00001999
Iteration 288/1000 | Loss: 0.00001999
Iteration 289/1000 | Loss: 0.00001999
Iteration 290/1000 | Loss: 0.00001999
Iteration 291/1000 | Loss: 0.00001999
Iteration 292/1000 | Loss: 0.00001999
Iteration 293/1000 | Loss: 0.00001999
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 293. Stopping optimization.
Last 5 losses: [1.9994415197288617e-05, 1.9994415197288617e-05, 1.9994415197288617e-05, 1.9994415197288617e-05, 1.9994415197288617e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9994415197288617e-05

Optimization complete. Final v2v error: 3.6723616123199463 mm

Highest mean error: 6.1839070320129395 mm for frame 8

Lowest mean error: 2.688913583755493 mm for frame 60

Saving results

Total time: 296.89214754104614
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_grace_posed_004/1077/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_grace_posed_004/1077.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_grace_posed_004/1077
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00378300
Iteration 2/25 | Loss: 0.00111938
Iteration 3/25 | Loss: 0.00103186
Iteration 4/25 | Loss: 0.00102289
Iteration 5/25 | Loss: 0.00102024
Iteration 6/25 | Loss: 0.00102008
Iteration 7/25 | Loss: 0.00102008
Iteration 8/25 | Loss: 0.00102008
Iteration 9/25 | Loss: 0.00102008
Iteration 10/25 | Loss: 0.00102008
Iteration 11/25 | Loss: 0.00102008
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010200765682384372, 0.0010200765682384372, 0.0010200765682384372, 0.0010200765682384372, 0.0010200765682384372]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010200765682384372

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.29828739
Iteration 2/25 | Loss: 0.00129521
Iteration 3/25 | Loss: 0.00129521
Iteration 4/25 | Loss: 0.00129520
Iteration 5/25 | Loss: 0.00129520
Iteration 6/25 | Loss: 0.00129520
Iteration 7/25 | Loss: 0.00129520
Iteration 8/25 | Loss: 0.00129520
Iteration 9/25 | Loss: 0.00129520
Iteration 10/25 | Loss: 0.00129520
Iteration 11/25 | Loss: 0.00129520
Iteration 12/25 | Loss: 0.00129520
Iteration 13/25 | Loss: 0.00129520
Iteration 14/25 | Loss: 0.00129520
Iteration 15/25 | Loss: 0.00129520
Iteration 16/25 | Loss: 0.00129520
Iteration 17/25 | Loss: 0.00129520
Iteration 18/25 | Loss: 0.00129520
Iteration 19/25 | Loss: 0.00129520
Iteration 20/25 | Loss: 0.00129520
Iteration 21/25 | Loss: 0.00129520
Iteration 22/25 | Loss: 0.00129520
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0012952026445418596, 0.0012952026445418596, 0.0012952026445418596, 0.0012952026445418596, 0.0012952026445418596]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012952026445418596

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00129520
Iteration 2/1000 | Loss: 0.00003788
Iteration 3/1000 | Loss: 0.00002535
Iteration 4/1000 | Loss: 0.00002030
Iteration 5/1000 | Loss: 0.00001814
Iteration 6/1000 | Loss: 0.00001694
Iteration 7/1000 | Loss: 0.00001604
Iteration 8/1000 | Loss: 0.00001551
Iteration 9/1000 | Loss: 0.00001511
Iteration 10/1000 | Loss: 0.00001486
Iteration 11/1000 | Loss: 0.00001462
Iteration 12/1000 | Loss: 0.00001434
Iteration 13/1000 | Loss: 0.00001414
Iteration 14/1000 | Loss: 0.00001398
Iteration 15/1000 | Loss: 0.00001395
Iteration 16/1000 | Loss: 0.00001392
Iteration 17/1000 | Loss: 0.00001389
Iteration 18/1000 | Loss: 0.00001389
Iteration 19/1000 | Loss: 0.00001389
Iteration 20/1000 | Loss: 0.00001388
Iteration 21/1000 | Loss: 0.00001385
Iteration 22/1000 | Loss: 0.00001382
Iteration 23/1000 | Loss: 0.00001381
Iteration 24/1000 | Loss: 0.00001380
Iteration 25/1000 | Loss: 0.00001380
Iteration 26/1000 | Loss: 0.00001378
Iteration 27/1000 | Loss: 0.00001378
Iteration 28/1000 | Loss: 0.00001377
Iteration 29/1000 | Loss: 0.00001376
Iteration 30/1000 | Loss: 0.00001375
Iteration 31/1000 | Loss: 0.00001374
Iteration 32/1000 | Loss: 0.00001373
Iteration 33/1000 | Loss: 0.00001371
Iteration 34/1000 | Loss: 0.00001371
Iteration 35/1000 | Loss: 0.00001371
Iteration 36/1000 | Loss: 0.00001371
Iteration 37/1000 | Loss: 0.00001370
Iteration 38/1000 | Loss: 0.00001370
Iteration 39/1000 | Loss: 0.00001368
Iteration 40/1000 | Loss: 0.00001368
Iteration 41/1000 | Loss: 0.00001368
Iteration 42/1000 | Loss: 0.00001368
Iteration 43/1000 | Loss: 0.00001368
Iteration 44/1000 | Loss: 0.00001368
Iteration 45/1000 | Loss: 0.00001368
Iteration 46/1000 | Loss: 0.00001368
Iteration 47/1000 | Loss: 0.00001368
Iteration 48/1000 | Loss: 0.00001367
Iteration 49/1000 | Loss: 0.00001367
Iteration 50/1000 | Loss: 0.00001367
Iteration 51/1000 | Loss: 0.00001367
Iteration 52/1000 | Loss: 0.00001367
Iteration 53/1000 | Loss: 0.00001365
Iteration 54/1000 | Loss: 0.00001365
Iteration 55/1000 | Loss: 0.00001364
Iteration 56/1000 | Loss: 0.00001364
Iteration 57/1000 | Loss: 0.00001364
Iteration 58/1000 | Loss: 0.00001363
Iteration 59/1000 | Loss: 0.00001363
Iteration 60/1000 | Loss: 0.00001363
Iteration 61/1000 | Loss: 0.00001362
Iteration 62/1000 | Loss: 0.00001362
Iteration 63/1000 | Loss: 0.00001362
Iteration 64/1000 | Loss: 0.00001361
Iteration 65/1000 | Loss: 0.00001361
Iteration 66/1000 | Loss: 0.00001360
Iteration 67/1000 | Loss: 0.00001360
Iteration 68/1000 | Loss: 0.00001359
Iteration 69/1000 | Loss: 0.00001359
Iteration 70/1000 | Loss: 0.00001359
Iteration 71/1000 | Loss: 0.00001359
Iteration 72/1000 | Loss: 0.00001358
Iteration 73/1000 | Loss: 0.00001358
Iteration 74/1000 | Loss: 0.00001358
Iteration 75/1000 | Loss: 0.00001358
Iteration 76/1000 | Loss: 0.00001358
Iteration 77/1000 | Loss: 0.00001357
Iteration 78/1000 | Loss: 0.00001357
Iteration 79/1000 | Loss: 0.00001357
Iteration 80/1000 | Loss: 0.00001356
Iteration 81/1000 | Loss: 0.00001356
Iteration 82/1000 | Loss: 0.00001355
Iteration 83/1000 | Loss: 0.00001354
Iteration 84/1000 | Loss: 0.00001354
Iteration 85/1000 | Loss: 0.00001354
Iteration 86/1000 | Loss: 0.00001354
Iteration 87/1000 | Loss: 0.00001354
Iteration 88/1000 | Loss: 0.00001354
Iteration 89/1000 | Loss: 0.00001353
Iteration 90/1000 | Loss: 0.00001353
Iteration 91/1000 | Loss: 0.00001353
Iteration 92/1000 | Loss: 0.00001353
Iteration 93/1000 | Loss: 0.00001353
Iteration 94/1000 | Loss: 0.00001353
Iteration 95/1000 | Loss: 0.00001353
Iteration 96/1000 | Loss: 0.00001353
Iteration 97/1000 | Loss: 0.00001352
Iteration 98/1000 | Loss: 0.00001351
Iteration 99/1000 | Loss: 0.00001351
Iteration 100/1000 | Loss: 0.00001351
Iteration 101/1000 | Loss: 0.00001351
Iteration 102/1000 | Loss: 0.00001351
Iteration 103/1000 | Loss: 0.00001350
Iteration 104/1000 | Loss: 0.00001350
Iteration 105/1000 | Loss: 0.00001350
Iteration 106/1000 | Loss: 0.00001350
Iteration 107/1000 | Loss: 0.00001349
Iteration 108/1000 | Loss: 0.00001349
Iteration 109/1000 | Loss: 0.00001349
Iteration 110/1000 | Loss: 0.00001349
Iteration 111/1000 | Loss: 0.00001349
Iteration 112/1000 | Loss: 0.00001349
Iteration 113/1000 | Loss: 0.00001348
Iteration 114/1000 | Loss: 0.00001348
Iteration 115/1000 | Loss: 0.00001348
Iteration 116/1000 | Loss: 0.00001348
Iteration 117/1000 | Loss: 0.00001347
Iteration 118/1000 | Loss: 0.00001347
Iteration 119/1000 | Loss: 0.00001347
Iteration 120/1000 | Loss: 0.00001347
Iteration 121/1000 | Loss: 0.00001346
Iteration 122/1000 | Loss: 0.00001346
Iteration 123/1000 | Loss: 0.00001346
Iteration 124/1000 | Loss: 0.00001346
Iteration 125/1000 | Loss: 0.00001346
Iteration 126/1000 | Loss: 0.00001346
Iteration 127/1000 | Loss: 0.00001346
Iteration 128/1000 | Loss: 0.00001346
Iteration 129/1000 | Loss: 0.00001346
Iteration 130/1000 | Loss: 0.00001346
Iteration 131/1000 | Loss: 0.00001345
Iteration 132/1000 | Loss: 0.00001345
Iteration 133/1000 | Loss: 0.00001345
Iteration 134/1000 | Loss: 0.00001345
Iteration 135/1000 | Loss: 0.00001345
Iteration 136/1000 | Loss: 0.00001345
Iteration 137/1000 | Loss: 0.00001345
Iteration 138/1000 | Loss: 0.00001345
Iteration 139/1000 | Loss: 0.00001345
Iteration 140/1000 | Loss: 0.00001345
Iteration 141/1000 | Loss: 0.00001345
Iteration 142/1000 | Loss: 0.00001345
Iteration 143/1000 | Loss: 0.00001345
Iteration 144/1000 | Loss: 0.00001345
Iteration 145/1000 | Loss: 0.00001345
Iteration 146/1000 | Loss: 0.00001345
Iteration 147/1000 | Loss: 0.00001345
Iteration 148/1000 | Loss: 0.00001345
Iteration 149/1000 | Loss: 0.00001345
Iteration 150/1000 | Loss: 0.00001345
Iteration 151/1000 | Loss: 0.00001345
Iteration 152/1000 | Loss: 0.00001345
Iteration 153/1000 | Loss: 0.00001345
Iteration 154/1000 | Loss: 0.00001345
Iteration 155/1000 | Loss: 0.00001345
Iteration 156/1000 | Loss: 0.00001345
Iteration 157/1000 | Loss: 0.00001345
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 157. Stopping optimization.
Last 5 losses: [1.3454718100547325e-05, 1.3454718100547325e-05, 1.3454718100547325e-05, 1.3454718100547325e-05, 1.3454718100547325e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3454718100547325e-05

Optimization complete. Final v2v error: 3.026855707168579 mm

Highest mean error: 3.865542411804199 mm for frame 143

Lowest mean error: 2.3468871116638184 mm for frame 3

Saving results

Total time: 43.387911796569824
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_grace_posed_004/1073/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_grace_posed_004/1073.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_grace_posed_004/1073
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00826055
Iteration 2/25 | Loss: 0.00134013
Iteration 3/25 | Loss: 0.00110972
Iteration 4/25 | Loss: 0.00108831
Iteration 5/25 | Loss: 0.00108676
Iteration 6/25 | Loss: 0.00108676
Iteration 7/25 | Loss: 0.00108676
Iteration 8/25 | Loss: 0.00108676
Iteration 9/25 | Loss: 0.00108676
Iteration 10/25 | Loss: 0.00108676
Iteration 11/25 | Loss: 0.00108676
Iteration 12/25 | Loss: 0.00108676
Iteration 13/25 | Loss: 0.00108676
Iteration 14/25 | Loss: 0.00108676
Iteration 15/25 | Loss: 0.00108676
Iteration 16/25 | Loss: 0.00108676
Iteration 17/25 | Loss: 0.00108676
Iteration 18/25 | Loss: 0.00108676
Iteration 19/25 | Loss: 0.00108676
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0010867587989196181, 0.0010867587989196181, 0.0010867587989196181, 0.0010867587989196181, 0.0010867587989196181]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010867587989196181

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.93866044
Iteration 2/25 | Loss: 0.00047007
Iteration 3/25 | Loss: 0.00047006
Iteration 4/25 | Loss: 0.00047006
Iteration 5/25 | Loss: 0.00047006
Iteration 6/25 | Loss: 0.00047006
Iteration 7/25 | Loss: 0.00047006
Iteration 8/25 | Loss: 0.00047006
Iteration 9/25 | Loss: 0.00047006
Iteration 10/25 | Loss: 0.00047006
Iteration 11/25 | Loss: 0.00047006
Iteration 12/25 | Loss: 0.00047006
Iteration 13/25 | Loss: 0.00047006
Iteration 14/25 | Loss: 0.00047006
Iteration 15/25 | Loss: 0.00047006
Iteration 16/25 | Loss: 0.00047006
Iteration 17/25 | Loss: 0.00047006
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.00047006021486595273, 0.00047006021486595273, 0.00047006021486595273, 0.00047006021486595273, 0.00047006021486595273]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00047006021486595273

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00047006
Iteration 2/1000 | Loss: 0.00003001
Iteration 3/1000 | Loss: 0.00002230
Iteration 4/1000 | Loss: 0.00002022
Iteration 5/1000 | Loss: 0.00001934
Iteration 6/1000 | Loss: 0.00001884
Iteration 7/1000 | Loss: 0.00001834
Iteration 8/1000 | Loss: 0.00001802
Iteration 9/1000 | Loss: 0.00001784
Iteration 10/1000 | Loss: 0.00001772
Iteration 11/1000 | Loss: 0.00001769
Iteration 12/1000 | Loss: 0.00001760
Iteration 13/1000 | Loss: 0.00001760
Iteration 14/1000 | Loss: 0.00001760
Iteration 15/1000 | Loss: 0.00001755
Iteration 16/1000 | Loss: 0.00001755
Iteration 17/1000 | Loss: 0.00001755
Iteration 18/1000 | Loss: 0.00001755
Iteration 19/1000 | Loss: 0.00001755
Iteration 20/1000 | Loss: 0.00001755
Iteration 21/1000 | Loss: 0.00001751
Iteration 22/1000 | Loss: 0.00001751
Iteration 23/1000 | Loss: 0.00001751
Iteration 24/1000 | Loss: 0.00001751
Iteration 25/1000 | Loss: 0.00001751
Iteration 26/1000 | Loss: 0.00001750
Iteration 27/1000 | Loss: 0.00001750
Iteration 28/1000 | Loss: 0.00001749
Iteration 29/1000 | Loss: 0.00001749
Iteration 30/1000 | Loss: 0.00001749
Iteration 31/1000 | Loss: 0.00001747
Iteration 32/1000 | Loss: 0.00001746
Iteration 33/1000 | Loss: 0.00001742
Iteration 34/1000 | Loss: 0.00001739
Iteration 35/1000 | Loss: 0.00001738
Iteration 36/1000 | Loss: 0.00001737
Iteration 37/1000 | Loss: 0.00001737
Iteration 38/1000 | Loss: 0.00001737
Iteration 39/1000 | Loss: 0.00001737
Iteration 40/1000 | Loss: 0.00001736
Iteration 41/1000 | Loss: 0.00001736
Iteration 42/1000 | Loss: 0.00001736
Iteration 43/1000 | Loss: 0.00001736
Iteration 44/1000 | Loss: 0.00001736
Iteration 45/1000 | Loss: 0.00001736
Iteration 46/1000 | Loss: 0.00001736
Iteration 47/1000 | Loss: 0.00001736
Iteration 48/1000 | Loss: 0.00001734
Iteration 49/1000 | Loss: 0.00001734
Iteration 50/1000 | Loss: 0.00001734
Iteration 51/1000 | Loss: 0.00001734
Iteration 52/1000 | Loss: 0.00001731
Iteration 53/1000 | Loss: 0.00001731
Iteration 54/1000 | Loss: 0.00001730
Iteration 55/1000 | Loss: 0.00001730
Iteration 56/1000 | Loss: 0.00001730
Iteration 57/1000 | Loss: 0.00001730
Iteration 58/1000 | Loss: 0.00001730
Iteration 59/1000 | Loss: 0.00001729
Iteration 60/1000 | Loss: 0.00001729
Iteration 61/1000 | Loss: 0.00001729
Iteration 62/1000 | Loss: 0.00001728
Iteration 63/1000 | Loss: 0.00001728
Iteration 64/1000 | Loss: 0.00001728
Iteration 65/1000 | Loss: 0.00001728
Iteration 66/1000 | Loss: 0.00001728
Iteration 67/1000 | Loss: 0.00001728
Iteration 68/1000 | Loss: 0.00001728
Iteration 69/1000 | Loss: 0.00001727
Iteration 70/1000 | Loss: 0.00001727
Iteration 71/1000 | Loss: 0.00001727
Iteration 72/1000 | Loss: 0.00001726
Iteration 73/1000 | Loss: 0.00001725
Iteration 74/1000 | Loss: 0.00001725
Iteration 75/1000 | Loss: 0.00001725
Iteration 76/1000 | Loss: 0.00001725
Iteration 77/1000 | Loss: 0.00001725
Iteration 78/1000 | Loss: 0.00001725
Iteration 79/1000 | Loss: 0.00001725
Iteration 80/1000 | Loss: 0.00001724
Iteration 81/1000 | Loss: 0.00001724
Iteration 82/1000 | Loss: 0.00001724
Iteration 83/1000 | Loss: 0.00001723
Iteration 84/1000 | Loss: 0.00001723
Iteration 85/1000 | Loss: 0.00001723
Iteration 86/1000 | Loss: 0.00001723
Iteration 87/1000 | Loss: 0.00001723
Iteration 88/1000 | Loss: 0.00001723
Iteration 89/1000 | Loss: 0.00001723
Iteration 90/1000 | Loss: 0.00001723
Iteration 91/1000 | Loss: 0.00001723
Iteration 92/1000 | Loss: 0.00001723
Iteration 93/1000 | Loss: 0.00001723
Iteration 94/1000 | Loss: 0.00001723
Iteration 95/1000 | Loss: 0.00001723
Iteration 96/1000 | Loss: 0.00001722
Iteration 97/1000 | Loss: 0.00001722
Iteration 98/1000 | Loss: 0.00001722
Iteration 99/1000 | Loss: 0.00001722
Iteration 100/1000 | Loss: 0.00001722
Iteration 101/1000 | Loss: 0.00001722
Iteration 102/1000 | Loss: 0.00001722
Iteration 103/1000 | Loss: 0.00001722
Iteration 104/1000 | Loss: 0.00001722
Iteration 105/1000 | Loss: 0.00001721
Iteration 106/1000 | Loss: 0.00001721
Iteration 107/1000 | Loss: 0.00001721
Iteration 108/1000 | Loss: 0.00001721
Iteration 109/1000 | Loss: 0.00001721
Iteration 110/1000 | Loss: 0.00001721
Iteration 111/1000 | Loss: 0.00001721
Iteration 112/1000 | Loss: 0.00001721
Iteration 113/1000 | Loss: 0.00001721
Iteration 114/1000 | Loss: 0.00001721
Iteration 115/1000 | Loss: 0.00001721
Iteration 116/1000 | Loss: 0.00001721
Iteration 117/1000 | Loss: 0.00001721
Iteration 118/1000 | Loss: 0.00001720
Iteration 119/1000 | Loss: 0.00001720
Iteration 120/1000 | Loss: 0.00001720
Iteration 121/1000 | Loss: 0.00001720
Iteration 122/1000 | Loss: 0.00001720
Iteration 123/1000 | Loss: 0.00001720
Iteration 124/1000 | Loss: 0.00001720
Iteration 125/1000 | Loss: 0.00001720
Iteration 126/1000 | Loss: 0.00001720
Iteration 127/1000 | Loss: 0.00001720
Iteration 128/1000 | Loss: 0.00001720
Iteration 129/1000 | Loss: 0.00001720
Iteration 130/1000 | Loss: 0.00001720
Iteration 131/1000 | Loss: 0.00001720
Iteration 132/1000 | Loss: 0.00001720
Iteration 133/1000 | Loss: 0.00001720
Iteration 134/1000 | Loss: 0.00001720
Iteration 135/1000 | Loss: 0.00001720
Iteration 136/1000 | Loss: 0.00001720
Iteration 137/1000 | Loss: 0.00001720
Iteration 138/1000 | Loss: 0.00001720
Iteration 139/1000 | Loss: 0.00001720
Iteration 140/1000 | Loss: 0.00001720
Iteration 141/1000 | Loss: 0.00001720
Iteration 142/1000 | Loss: 0.00001720
Iteration 143/1000 | Loss: 0.00001720
Iteration 144/1000 | Loss: 0.00001720
Iteration 145/1000 | Loss: 0.00001720
Iteration 146/1000 | Loss: 0.00001720
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 146. Stopping optimization.
Last 5 losses: [1.7202568415086716e-05, 1.7202568415086716e-05, 1.7202568415086716e-05, 1.7202568415086716e-05, 1.7202568415086716e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7202568415086716e-05

Optimization complete. Final v2v error: 3.492586135864258 mm

Highest mean error: 3.5894253253936768 mm for frame 118

Lowest mean error: 3.376563310623169 mm for frame 96

Saving results

Total time: 31.838455200195312
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_grace_posed_004/1094/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_grace_posed_004/1094.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_grace_posed_004/1094
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00392255
Iteration 2/25 | Loss: 0.00112264
Iteration 3/25 | Loss: 0.00100100
Iteration 4/25 | Loss: 0.00098854
Iteration 5/25 | Loss: 0.00098569
Iteration 6/25 | Loss: 0.00098492
Iteration 7/25 | Loss: 0.00098492
Iteration 8/25 | Loss: 0.00098492
Iteration 9/25 | Loss: 0.00098492
Iteration 10/25 | Loss: 0.00098492
Iteration 11/25 | Loss: 0.00098492
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.000984923099167645, 0.000984923099167645, 0.000984923099167645, 0.000984923099167645, 0.000984923099167645]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000984923099167645

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32750690
Iteration 2/25 | Loss: 0.00113803
Iteration 3/25 | Loss: 0.00113803
Iteration 4/25 | Loss: 0.00113802
Iteration 5/25 | Loss: 0.00113802
Iteration 6/25 | Loss: 0.00113802
Iteration 7/25 | Loss: 0.00113802
Iteration 8/25 | Loss: 0.00113802
Iteration 9/25 | Loss: 0.00113802
Iteration 10/25 | Loss: 0.00113802
Iteration 11/25 | Loss: 0.00113802
Iteration 12/25 | Loss: 0.00113802
Iteration 13/25 | Loss: 0.00113802
Iteration 14/25 | Loss: 0.00113802
Iteration 15/25 | Loss: 0.00113802
Iteration 16/25 | Loss: 0.00113802
Iteration 17/25 | Loss: 0.00113802
Iteration 18/25 | Loss: 0.00113802
Iteration 19/25 | Loss: 0.00113802
Iteration 20/25 | Loss: 0.00113802
Iteration 21/25 | Loss: 0.00113802
Iteration 22/25 | Loss: 0.00113802
Iteration 23/25 | Loss: 0.00113802
Iteration 24/25 | Loss: 0.00113802
Iteration 25/25 | Loss: 0.00113802

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00113802
Iteration 2/1000 | Loss: 0.00002219
Iteration 3/1000 | Loss: 0.00001352
Iteration 4/1000 | Loss: 0.00001056
Iteration 5/1000 | Loss: 0.00000963
Iteration 6/1000 | Loss: 0.00000895
Iteration 7/1000 | Loss: 0.00000854
Iteration 8/1000 | Loss: 0.00000820
Iteration 9/1000 | Loss: 0.00000798
Iteration 10/1000 | Loss: 0.00000781
Iteration 11/1000 | Loss: 0.00000779
Iteration 12/1000 | Loss: 0.00000763
Iteration 13/1000 | Loss: 0.00000762
Iteration 14/1000 | Loss: 0.00000760
Iteration 15/1000 | Loss: 0.00000757
Iteration 16/1000 | Loss: 0.00000755
Iteration 17/1000 | Loss: 0.00000754
Iteration 18/1000 | Loss: 0.00000752
Iteration 19/1000 | Loss: 0.00000751
Iteration 20/1000 | Loss: 0.00000751
Iteration 21/1000 | Loss: 0.00000750
Iteration 22/1000 | Loss: 0.00000750
Iteration 23/1000 | Loss: 0.00000749
Iteration 24/1000 | Loss: 0.00000749
Iteration 25/1000 | Loss: 0.00000748
Iteration 26/1000 | Loss: 0.00000747
Iteration 27/1000 | Loss: 0.00000747
Iteration 28/1000 | Loss: 0.00000746
Iteration 29/1000 | Loss: 0.00000745
Iteration 30/1000 | Loss: 0.00000744
Iteration 31/1000 | Loss: 0.00000744
Iteration 32/1000 | Loss: 0.00000743
Iteration 33/1000 | Loss: 0.00000743
Iteration 34/1000 | Loss: 0.00000743
Iteration 35/1000 | Loss: 0.00000742
Iteration 36/1000 | Loss: 0.00000741
Iteration 37/1000 | Loss: 0.00000741
Iteration 38/1000 | Loss: 0.00000741
Iteration 39/1000 | Loss: 0.00000737
Iteration 40/1000 | Loss: 0.00000737
Iteration 41/1000 | Loss: 0.00000735
Iteration 42/1000 | Loss: 0.00000733
Iteration 43/1000 | Loss: 0.00000733
Iteration 44/1000 | Loss: 0.00000733
Iteration 45/1000 | Loss: 0.00000732
Iteration 46/1000 | Loss: 0.00000732
Iteration 47/1000 | Loss: 0.00000732
Iteration 48/1000 | Loss: 0.00000731
Iteration 49/1000 | Loss: 0.00000731
Iteration 50/1000 | Loss: 0.00000731
Iteration 51/1000 | Loss: 0.00000730
Iteration 52/1000 | Loss: 0.00000730
Iteration 53/1000 | Loss: 0.00000730
Iteration 54/1000 | Loss: 0.00000730
Iteration 55/1000 | Loss: 0.00000730
Iteration 56/1000 | Loss: 0.00000730
Iteration 57/1000 | Loss: 0.00000729
Iteration 58/1000 | Loss: 0.00000728
Iteration 59/1000 | Loss: 0.00000728
Iteration 60/1000 | Loss: 0.00000728
Iteration 61/1000 | Loss: 0.00000727
Iteration 62/1000 | Loss: 0.00000727
Iteration 63/1000 | Loss: 0.00000727
Iteration 64/1000 | Loss: 0.00000726
Iteration 65/1000 | Loss: 0.00000725
Iteration 66/1000 | Loss: 0.00000725
Iteration 67/1000 | Loss: 0.00000725
Iteration 68/1000 | Loss: 0.00000724
Iteration 69/1000 | Loss: 0.00000724
Iteration 70/1000 | Loss: 0.00000724
Iteration 71/1000 | Loss: 0.00000724
Iteration 72/1000 | Loss: 0.00000724
Iteration 73/1000 | Loss: 0.00000723
Iteration 74/1000 | Loss: 0.00000723
Iteration 75/1000 | Loss: 0.00000723
Iteration 76/1000 | Loss: 0.00000723
Iteration 77/1000 | Loss: 0.00000722
Iteration 78/1000 | Loss: 0.00000722
Iteration 79/1000 | Loss: 0.00000721
Iteration 80/1000 | Loss: 0.00000721
Iteration 81/1000 | Loss: 0.00000721
Iteration 82/1000 | Loss: 0.00000720
Iteration 83/1000 | Loss: 0.00000720
Iteration 84/1000 | Loss: 0.00000720
Iteration 85/1000 | Loss: 0.00000719
Iteration 86/1000 | Loss: 0.00000719
Iteration 87/1000 | Loss: 0.00000719
Iteration 88/1000 | Loss: 0.00000719
Iteration 89/1000 | Loss: 0.00000719
Iteration 90/1000 | Loss: 0.00000719
Iteration 91/1000 | Loss: 0.00000718
Iteration 92/1000 | Loss: 0.00000718
Iteration 93/1000 | Loss: 0.00000718
Iteration 94/1000 | Loss: 0.00000718
Iteration 95/1000 | Loss: 0.00000718
Iteration 96/1000 | Loss: 0.00000717
Iteration 97/1000 | Loss: 0.00000717
Iteration 98/1000 | Loss: 0.00000717
Iteration 99/1000 | Loss: 0.00000717
Iteration 100/1000 | Loss: 0.00000717
Iteration 101/1000 | Loss: 0.00000717
Iteration 102/1000 | Loss: 0.00000717
Iteration 103/1000 | Loss: 0.00000717
Iteration 104/1000 | Loss: 0.00000716
Iteration 105/1000 | Loss: 0.00000716
Iteration 106/1000 | Loss: 0.00000716
Iteration 107/1000 | Loss: 0.00000715
Iteration 108/1000 | Loss: 0.00000715
Iteration 109/1000 | Loss: 0.00000715
Iteration 110/1000 | Loss: 0.00000715
Iteration 111/1000 | Loss: 0.00000715
Iteration 112/1000 | Loss: 0.00000714
Iteration 113/1000 | Loss: 0.00000714
Iteration 114/1000 | Loss: 0.00000714
Iteration 115/1000 | Loss: 0.00000714
Iteration 116/1000 | Loss: 0.00000714
Iteration 117/1000 | Loss: 0.00000714
Iteration 118/1000 | Loss: 0.00000714
Iteration 119/1000 | Loss: 0.00000714
Iteration 120/1000 | Loss: 0.00000713
Iteration 121/1000 | Loss: 0.00000713
Iteration 122/1000 | Loss: 0.00000713
Iteration 123/1000 | Loss: 0.00000713
Iteration 124/1000 | Loss: 0.00000713
Iteration 125/1000 | Loss: 0.00000713
Iteration 126/1000 | Loss: 0.00000713
Iteration 127/1000 | Loss: 0.00000713
Iteration 128/1000 | Loss: 0.00000713
Iteration 129/1000 | Loss: 0.00000713
Iteration 130/1000 | Loss: 0.00000713
Iteration 131/1000 | Loss: 0.00000712
Iteration 132/1000 | Loss: 0.00000712
Iteration 133/1000 | Loss: 0.00000712
Iteration 134/1000 | Loss: 0.00000712
Iteration 135/1000 | Loss: 0.00000712
Iteration 136/1000 | Loss: 0.00000712
Iteration 137/1000 | Loss: 0.00000712
Iteration 138/1000 | Loss: 0.00000712
Iteration 139/1000 | Loss: 0.00000712
Iteration 140/1000 | Loss: 0.00000712
Iteration 141/1000 | Loss: 0.00000712
Iteration 142/1000 | Loss: 0.00000712
Iteration 143/1000 | Loss: 0.00000712
Iteration 144/1000 | Loss: 0.00000712
Iteration 145/1000 | Loss: 0.00000712
Iteration 146/1000 | Loss: 0.00000712
Iteration 147/1000 | Loss: 0.00000711
Iteration 148/1000 | Loss: 0.00000711
Iteration 149/1000 | Loss: 0.00000711
Iteration 150/1000 | Loss: 0.00000710
Iteration 151/1000 | Loss: 0.00000710
Iteration 152/1000 | Loss: 0.00000710
Iteration 153/1000 | Loss: 0.00000710
Iteration 154/1000 | Loss: 0.00000709
Iteration 155/1000 | Loss: 0.00000709
Iteration 156/1000 | Loss: 0.00000709
Iteration 157/1000 | Loss: 0.00000708
Iteration 158/1000 | Loss: 0.00000708
Iteration 159/1000 | Loss: 0.00000707
Iteration 160/1000 | Loss: 0.00000707
Iteration 161/1000 | Loss: 0.00000707
Iteration 162/1000 | Loss: 0.00000707
Iteration 163/1000 | Loss: 0.00000707
Iteration 164/1000 | Loss: 0.00000707
Iteration 165/1000 | Loss: 0.00000706
Iteration 166/1000 | Loss: 0.00000706
Iteration 167/1000 | Loss: 0.00000706
Iteration 168/1000 | Loss: 0.00000706
Iteration 169/1000 | Loss: 0.00000706
Iteration 170/1000 | Loss: 0.00000706
Iteration 171/1000 | Loss: 0.00000705
Iteration 172/1000 | Loss: 0.00000705
Iteration 173/1000 | Loss: 0.00000705
Iteration 174/1000 | Loss: 0.00000705
Iteration 175/1000 | Loss: 0.00000705
Iteration 176/1000 | Loss: 0.00000705
Iteration 177/1000 | Loss: 0.00000705
Iteration 178/1000 | Loss: 0.00000705
Iteration 179/1000 | Loss: 0.00000705
Iteration 180/1000 | Loss: 0.00000705
Iteration 181/1000 | Loss: 0.00000705
Iteration 182/1000 | Loss: 0.00000705
Iteration 183/1000 | Loss: 0.00000705
Iteration 184/1000 | Loss: 0.00000704
Iteration 185/1000 | Loss: 0.00000704
Iteration 186/1000 | Loss: 0.00000704
Iteration 187/1000 | Loss: 0.00000704
Iteration 188/1000 | Loss: 0.00000704
Iteration 189/1000 | Loss: 0.00000704
Iteration 190/1000 | Loss: 0.00000704
Iteration 191/1000 | Loss: 0.00000704
Iteration 192/1000 | Loss: 0.00000703
Iteration 193/1000 | Loss: 0.00000703
Iteration 194/1000 | Loss: 0.00000703
Iteration 195/1000 | Loss: 0.00000703
Iteration 196/1000 | Loss: 0.00000703
Iteration 197/1000 | Loss: 0.00000703
Iteration 198/1000 | Loss: 0.00000703
Iteration 199/1000 | Loss: 0.00000703
Iteration 200/1000 | Loss: 0.00000703
Iteration 201/1000 | Loss: 0.00000703
Iteration 202/1000 | Loss: 0.00000703
Iteration 203/1000 | Loss: 0.00000703
Iteration 204/1000 | Loss: 0.00000703
Iteration 205/1000 | Loss: 0.00000703
Iteration 206/1000 | Loss: 0.00000703
Iteration 207/1000 | Loss: 0.00000702
Iteration 208/1000 | Loss: 0.00000702
Iteration 209/1000 | Loss: 0.00000702
Iteration 210/1000 | Loss: 0.00000702
Iteration 211/1000 | Loss: 0.00000702
Iteration 212/1000 | Loss: 0.00000702
Iteration 213/1000 | Loss: 0.00000702
Iteration 214/1000 | Loss: 0.00000702
Iteration 215/1000 | Loss: 0.00000702
Iteration 216/1000 | Loss: 0.00000702
Iteration 217/1000 | Loss: 0.00000702
Iteration 218/1000 | Loss: 0.00000702
Iteration 219/1000 | Loss: 0.00000702
Iteration 220/1000 | Loss: 0.00000702
Iteration 221/1000 | Loss: 0.00000702
Iteration 222/1000 | Loss: 0.00000702
Iteration 223/1000 | Loss: 0.00000702
Iteration 224/1000 | Loss: 0.00000702
Iteration 225/1000 | Loss: 0.00000702
Iteration 226/1000 | Loss: 0.00000702
Iteration 227/1000 | Loss: 0.00000702
Iteration 228/1000 | Loss: 0.00000701
Iteration 229/1000 | Loss: 0.00000701
Iteration 230/1000 | Loss: 0.00000701
Iteration 231/1000 | Loss: 0.00000701
Iteration 232/1000 | Loss: 0.00000701
Iteration 233/1000 | Loss: 0.00000701
Iteration 234/1000 | Loss: 0.00000701
Iteration 235/1000 | Loss: 0.00000701
Iteration 236/1000 | Loss: 0.00000701
Iteration 237/1000 | Loss: 0.00000701
Iteration 238/1000 | Loss: 0.00000701
Iteration 239/1000 | Loss: 0.00000701
Iteration 240/1000 | Loss: 0.00000701
Iteration 241/1000 | Loss: 0.00000701
Iteration 242/1000 | Loss: 0.00000701
Iteration 243/1000 | Loss: 0.00000701
Iteration 244/1000 | Loss: 0.00000701
Iteration 245/1000 | Loss: 0.00000701
Iteration 246/1000 | Loss: 0.00000701
Iteration 247/1000 | Loss: 0.00000701
Iteration 248/1000 | Loss: 0.00000701
Iteration 249/1000 | Loss: 0.00000701
Iteration 250/1000 | Loss: 0.00000701
Iteration 251/1000 | Loss: 0.00000701
Iteration 252/1000 | Loss: 0.00000701
Iteration 253/1000 | Loss: 0.00000701
Iteration 254/1000 | Loss: 0.00000701
Iteration 255/1000 | Loss: 0.00000701
Iteration 256/1000 | Loss: 0.00000701
Iteration 257/1000 | Loss: 0.00000701
Iteration 258/1000 | Loss: 0.00000701
Iteration 259/1000 | Loss: 0.00000701
Iteration 260/1000 | Loss: 0.00000701
Iteration 261/1000 | Loss: 0.00000701
Iteration 262/1000 | Loss: 0.00000701
Iteration 263/1000 | Loss: 0.00000701
Iteration 264/1000 | Loss: 0.00000701
Iteration 265/1000 | Loss: 0.00000701
Iteration 266/1000 | Loss: 0.00000701
Iteration 267/1000 | Loss: 0.00000701
Iteration 268/1000 | Loss: 0.00000701
Iteration 269/1000 | Loss: 0.00000701
Iteration 270/1000 | Loss: 0.00000701
Iteration 271/1000 | Loss: 0.00000701
Iteration 272/1000 | Loss: 0.00000701
Iteration 273/1000 | Loss: 0.00000701
Iteration 274/1000 | Loss: 0.00000701
Iteration 275/1000 | Loss: 0.00000701
Iteration 276/1000 | Loss: 0.00000701
Iteration 277/1000 | Loss: 0.00000701
Iteration 278/1000 | Loss: 0.00000701
Iteration 279/1000 | Loss: 0.00000701
Iteration 280/1000 | Loss: 0.00000701
Iteration 281/1000 | Loss: 0.00000701
Iteration 282/1000 | Loss: 0.00000701
Iteration 283/1000 | Loss: 0.00000701
Iteration 284/1000 | Loss: 0.00000701
Iteration 285/1000 | Loss: 0.00000701
Iteration 286/1000 | Loss: 0.00000701
Iteration 287/1000 | Loss: 0.00000701
Iteration 288/1000 | Loss: 0.00000701
Iteration 289/1000 | Loss: 0.00000701
Iteration 290/1000 | Loss: 0.00000701
Iteration 291/1000 | Loss: 0.00000701
Iteration 292/1000 | Loss: 0.00000701
Iteration 293/1000 | Loss: 0.00000701
Iteration 294/1000 | Loss: 0.00000701
Iteration 295/1000 | Loss: 0.00000701
Iteration 296/1000 | Loss: 0.00000701
Iteration 297/1000 | Loss: 0.00000701
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 297. Stopping optimization.
Last 5 losses: [7.0070382207632065e-06, 7.0070382207632065e-06, 7.0070382207632065e-06, 7.0070382207632065e-06, 7.0070382207632065e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 7.0070382207632065e-06

Optimization complete. Final v2v error: 2.2455055713653564 mm

Highest mean error: 3.227020025253296 mm for frame 65

Lowest mean error: 2.050553560256958 mm for frame 107

Saving results

Total time: 43.15823817253113
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_grace_posed_004/1084/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_grace_posed_004/1084.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_grace_posed_004/1084
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01004707
Iteration 2/25 | Loss: 0.00249857
Iteration 3/25 | Loss: 0.00184680
Iteration 4/25 | Loss: 0.00173048
Iteration 5/25 | Loss: 0.00169233
Iteration 6/25 | Loss: 0.00164103
Iteration 7/25 | Loss: 0.00159538
Iteration 8/25 | Loss: 0.00158380
Iteration 9/25 | Loss: 0.00155346
Iteration 10/25 | Loss: 0.00154879
Iteration 11/25 | Loss: 0.00152972
Iteration 12/25 | Loss: 0.00151082
Iteration 13/25 | Loss: 0.00150624
Iteration 14/25 | Loss: 0.00149910
Iteration 15/25 | Loss: 0.00149976
Iteration 16/25 | Loss: 0.00149567
Iteration 17/25 | Loss: 0.00149636
Iteration 18/25 | Loss: 0.00149257
Iteration 19/25 | Loss: 0.00148670
Iteration 20/25 | Loss: 0.00148545
Iteration 21/25 | Loss: 0.00148484
Iteration 22/25 | Loss: 0.00148434
Iteration 23/25 | Loss: 0.00148405
Iteration 24/25 | Loss: 0.00148616
Iteration 25/25 | Loss: 0.00148281

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.26428080
Iteration 2/25 | Loss: 0.00409758
Iteration 3/25 | Loss: 0.00387410
Iteration 4/25 | Loss: 0.00387410
Iteration 5/25 | Loss: 0.00387410
Iteration 6/25 | Loss: 0.00387410
Iteration 7/25 | Loss: 0.00387409
Iteration 8/25 | Loss: 0.00387409
Iteration 9/25 | Loss: 0.00387410
Iteration 10/25 | Loss: 0.00387409
Iteration 11/25 | Loss: 0.00387409
Iteration 12/25 | Loss: 0.00387409
Iteration 13/25 | Loss: 0.00387409
Iteration 14/25 | Loss: 0.00387409
Iteration 15/25 | Loss: 0.00387409
Iteration 16/25 | Loss: 0.00387409
Iteration 17/25 | Loss: 0.00387409
Iteration 18/25 | Loss: 0.00387409
Iteration 19/25 | Loss: 0.00387409
Iteration 20/25 | Loss: 0.00387409
Iteration 21/25 | Loss: 0.00387409
Iteration 22/25 | Loss: 0.00387409
Iteration 23/25 | Loss: 0.00387409
Iteration 24/25 | Loss: 0.00387409
Iteration 25/25 | Loss: 0.00387409

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00387409
Iteration 2/1000 | Loss: 0.00068995
Iteration 3/1000 | Loss: 0.00039021
Iteration 4/1000 | Loss: 0.00036749
Iteration 5/1000 | Loss: 0.00052072
Iteration 6/1000 | Loss: 0.00027938
Iteration 7/1000 | Loss: 0.00026078
Iteration 8/1000 | Loss: 0.00024273
Iteration 9/1000 | Loss: 0.00023267
Iteration 10/1000 | Loss: 0.00022273
Iteration 11/1000 | Loss: 0.00021463
Iteration 12/1000 | Loss: 0.00020896
Iteration 13/1000 | Loss: 0.00051403
Iteration 14/1000 | Loss: 0.00078458
Iteration 15/1000 | Loss: 0.00389463
Iteration 16/1000 | Loss: 0.00674366
Iteration 17/1000 | Loss: 0.00238645
Iteration 18/1000 | Loss: 0.00094994
Iteration 19/1000 | Loss: 0.00077922
Iteration 20/1000 | Loss: 0.00169377
Iteration 21/1000 | Loss: 0.00024374
Iteration 22/1000 | Loss: 0.00016185
Iteration 23/1000 | Loss: 0.00022672
Iteration 24/1000 | Loss: 0.00009392
Iteration 25/1000 | Loss: 0.00010638
Iteration 26/1000 | Loss: 0.00006497
Iteration 27/1000 | Loss: 0.00017418
Iteration 28/1000 | Loss: 0.00005117
Iteration 29/1000 | Loss: 0.00004563
Iteration 30/1000 | Loss: 0.00004225
Iteration 31/1000 | Loss: 0.00003542
Iteration 32/1000 | Loss: 0.00003116
Iteration 33/1000 | Loss: 0.00002795
Iteration 34/1000 | Loss: 0.00002537
Iteration 35/1000 | Loss: 0.00002383
Iteration 36/1000 | Loss: 0.00002257
Iteration 37/1000 | Loss: 0.00002168
Iteration 38/1000 | Loss: 0.00002111
Iteration 39/1000 | Loss: 0.00002070
Iteration 40/1000 | Loss: 0.00002042
Iteration 41/1000 | Loss: 0.00002024
Iteration 42/1000 | Loss: 0.00002004
Iteration 43/1000 | Loss: 0.00001995
Iteration 44/1000 | Loss: 0.00001986
Iteration 45/1000 | Loss: 0.00001982
Iteration 46/1000 | Loss: 0.00001967
Iteration 47/1000 | Loss: 0.00001964
Iteration 48/1000 | Loss: 0.00001964
Iteration 49/1000 | Loss: 0.00001961
Iteration 50/1000 | Loss: 0.00001960
Iteration 51/1000 | Loss: 0.00001960
Iteration 52/1000 | Loss: 0.00001959
Iteration 53/1000 | Loss: 0.00001958
Iteration 54/1000 | Loss: 0.00001957
Iteration 55/1000 | Loss: 0.00001957
Iteration 56/1000 | Loss: 0.00001954
Iteration 57/1000 | Loss: 0.00001954
Iteration 58/1000 | Loss: 0.00001953
Iteration 59/1000 | Loss: 0.00001953
Iteration 60/1000 | Loss: 0.00001950
Iteration 61/1000 | Loss: 0.00001950
Iteration 62/1000 | Loss: 0.00001949
Iteration 63/1000 | Loss: 0.00001949
Iteration 64/1000 | Loss: 0.00001949
Iteration 65/1000 | Loss: 0.00001948
Iteration 66/1000 | Loss: 0.00001947
Iteration 67/1000 | Loss: 0.00001947
Iteration 68/1000 | Loss: 0.00001944
Iteration 69/1000 | Loss: 0.00001944
Iteration 70/1000 | Loss: 0.00001944
Iteration 71/1000 | Loss: 0.00001944
Iteration 72/1000 | Loss: 0.00001944
Iteration 73/1000 | Loss: 0.00001943
Iteration 74/1000 | Loss: 0.00001943
Iteration 75/1000 | Loss: 0.00001943
Iteration 76/1000 | Loss: 0.00001943
Iteration 77/1000 | Loss: 0.00001943
Iteration 78/1000 | Loss: 0.00001943
Iteration 79/1000 | Loss: 0.00001943
Iteration 80/1000 | Loss: 0.00001943
Iteration 81/1000 | Loss: 0.00001942
Iteration 82/1000 | Loss: 0.00001941
Iteration 83/1000 | Loss: 0.00001941
Iteration 84/1000 | Loss: 0.00001941
Iteration 85/1000 | Loss: 0.00001941
Iteration 86/1000 | Loss: 0.00001940
Iteration 87/1000 | Loss: 0.00001940
Iteration 88/1000 | Loss: 0.00001940
Iteration 89/1000 | Loss: 0.00001940
Iteration 90/1000 | Loss: 0.00001940
Iteration 91/1000 | Loss: 0.00001939
Iteration 92/1000 | Loss: 0.00001938
Iteration 93/1000 | Loss: 0.00001938
Iteration 94/1000 | Loss: 0.00001938
Iteration 95/1000 | Loss: 0.00001938
Iteration 96/1000 | Loss: 0.00001938
Iteration 97/1000 | Loss: 0.00001937
Iteration 98/1000 | Loss: 0.00001937
Iteration 99/1000 | Loss: 0.00001937
Iteration 100/1000 | Loss: 0.00001937
Iteration 101/1000 | Loss: 0.00001937
Iteration 102/1000 | Loss: 0.00001937
Iteration 103/1000 | Loss: 0.00001937
Iteration 104/1000 | Loss: 0.00001937
Iteration 105/1000 | Loss: 0.00001936
Iteration 106/1000 | Loss: 0.00001936
Iteration 107/1000 | Loss: 0.00001936
Iteration 108/1000 | Loss: 0.00001936
Iteration 109/1000 | Loss: 0.00001936
Iteration 110/1000 | Loss: 0.00001936
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 110. Stopping optimization.
Last 5 losses: [1.9363775209058076e-05, 1.9363775209058076e-05, 1.9363775209058076e-05, 1.9363775209058076e-05, 1.9363775209058076e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9363775209058076e-05

Optimization complete. Final v2v error: 2.8603949546813965 mm

Highest mean error: 11.107259750366211 mm for frame 90

Lowest mean error: 2.4711403846740723 mm for frame 72

Saving results

Total time: 129.59680461883545
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_grace_posed_004/1004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_grace_posed_004/1004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_grace_posed_004/1004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00836193
Iteration 2/25 | Loss: 0.00129760
Iteration 3/25 | Loss: 0.00116036
Iteration 4/25 | Loss: 0.00113909
Iteration 5/25 | Loss: 0.00113253
Iteration 6/25 | Loss: 0.00113167
Iteration 7/25 | Loss: 0.00113167
Iteration 8/25 | Loss: 0.00113167
Iteration 9/25 | Loss: 0.00113167
Iteration 10/25 | Loss: 0.00113167
Iteration 11/25 | Loss: 0.00113167
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011316712480038404, 0.0011316712480038404, 0.0011316712480038404, 0.0011316712480038404, 0.0011316712480038404]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011316712480038404

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.13461792
Iteration 2/25 | Loss: 0.00080432
Iteration 3/25 | Loss: 0.00080431
Iteration 4/25 | Loss: 0.00080431
Iteration 5/25 | Loss: 0.00080431
Iteration 6/25 | Loss: 0.00080431
Iteration 7/25 | Loss: 0.00080431
Iteration 8/25 | Loss: 0.00080431
Iteration 9/25 | Loss: 0.00080431
Iteration 10/25 | Loss: 0.00080431
Iteration 11/25 | Loss: 0.00080431
Iteration 12/25 | Loss: 0.00080431
Iteration 13/25 | Loss: 0.00080431
Iteration 14/25 | Loss: 0.00080431
Iteration 15/25 | Loss: 0.00080431
Iteration 16/25 | Loss: 0.00080431
Iteration 17/25 | Loss: 0.00080431
Iteration 18/25 | Loss: 0.00080431
Iteration 19/25 | Loss: 0.00080431
Iteration 20/25 | Loss: 0.00080431
Iteration 21/25 | Loss: 0.00080431
Iteration 22/25 | Loss: 0.00080431
Iteration 23/25 | Loss: 0.00080431
Iteration 24/25 | Loss: 0.00080431
Iteration 25/25 | Loss: 0.00080431

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00080431
Iteration 2/1000 | Loss: 0.00004055
Iteration 3/1000 | Loss: 0.00002834
Iteration 4/1000 | Loss: 0.00002617
Iteration 5/1000 | Loss: 0.00002451
Iteration 6/1000 | Loss: 0.00002363
Iteration 7/1000 | Loss: 0.00002302
Iteration 8/1000 | Loss: 0.00002261
Iteration 9/1000 | Loss: 0.00002222
Iteration 10/1000 | Loss: 0.00002195
Iteration 11/1000 | Loss: 0.00002175
Iteration 12/1000 | Loss: 0.00002158
Iteration 13/1000 | Loss: 0.00002145
Iteration 14/1000 | Loss: 0.00002143
Iteration 15/1000 | Loss: 0.00002136
Iteration 16/1000 | Loss: 0.00002133
Iteration 17/1000 | Loss: 0.00002133
Iteration 18/1000 | Loss: 0.00002133
Iteration 19/1000 | Loss: 0.00002132
Iteration 20/1000 | Loss: 0.00002131
Iteration 21/1000 | Loss: 0.00002131
Iteration 22/1000 | Loss: 0.00002130
Iteration 23/1000 | Loss: 0.00002130
Iteration 24/1000 | Loss: 0.00002130
Iteration 25/1000 | Loss: 0.00002128
Iteration 26/1000 | Loss: 0.00002128
Iteration 27/1000 | Loss: 0.00002128
Iteration 28/1000 | Loss: 0.00002128
Iteration 29/1000 | Loss: 0.00002128
Iteration 30/1000 | Loss: 0.00002128
Iteration 31/1000 | Loss: 0.00002128
Iteration 32/1000 | Loss: 0.00002127
Iteration 33/1000 | Loss: 0.00002127
Iteration 34/1000 | Loss: 0.00002127
Iteration 35/1000 | Loss: 0.00002127
Iteration 36/1000 | Loss: 0.00002125
Iteration 37/1000 | Loss: 0.00002125
Iteration 38/1000 | Loss: 0.00002125
Iteration 39/1000 | Loss: 0.00002124
Iteration 40/1000 | Loss: 0.00002124
Iteration 41/1000 | Loss: 0.00002124
Iteration 42/1000 | Loss: 0.00002124
Iteration 43/1000 | Loss: 0.00002123
Iteration 44/1000 | Loss: 0.00002123
Iteration 45/1000 | Loss: 0.00002122
Iteration 46/1000 | Loss: 0.00002121
Iteration 47/1000 | Loss: 0.00002121
Iteration 48/1000 | Loss: 0.00002121
Iteration 49/1000 | Loss: 0.00002121
Iteration 50/1000 | Loss: 0.00002121
Iteration 51/1000 | Loss: 0.00002121
Iteration 52/1000 | Loss: 0.00002121
Iteration 53/1000 | Loss: 0.00002120
Iteration 54/1000 | Loss: 0.00002120
Iteration 55/1000 | Loss: 0.00002120
Iteration 56/1000 | Loss: 0.00002120
Iteration 57/1000 | Loss: 0.00002120
Iteration 58/1000 | Loss: 0.00002120
Iteration 59/1000 | Loss: 0.00002120
Iteration 60/1000 | Loss: 0.00002120
Iteration 61/1000 | Loss: 0.00002120
Iteration 62/1000 | Loss: 0.00002120
Iteration 63/1000 | Loss: 0.00002118
Iteration 64/1000 | Loss: 0.00002117
Iteration 65/1000 | Loss: 0.00002117
Iteration 66/1000 | Loss: 0.00002117
Iteration 67/1000 | Loss: 0.00002116
Iteration 68/1000 | Loss: 0.00002116
Iteration 69/1000 | Loss: 0.00002116
Iteration 70/1000 | Loss: 0.00002115
Iteration 71/1000 | Loss: 0.00002115
Iteration 72/1000 | Loss: 0.00002114
Iteration 73/1000 | Loss: 0.00002114
Iteration 74/1000 | Loss: 0.00002114
Iteration 75/1000 | Loss: 0.00002113
Iteration 76/1000 | Loss: 0.00002113
Iteration 77/1000 | Loss: 0.00002113
Iteration 78/1000 | Loss: 0.00002113
Iteration 79/1000 | Loss: 0.00002113
Iteration 80/1000 | Loss: 0.00002113
Iteration 81/1000 | Loss: 0.00002113
Iteration 82/1000 | Loss: 0.00002113
Iteration 83/1000 | Loss: 0.00002113
Iteration 84/1000 | Loss: 0.00002112
Iteration 85/1000 | Loss: 0.00002112
Iteration 86/1000 | Loss: 0.00002112
Iteration 87/1000 | Loss: 0.00002112
Iteration 88/1000 | Loss: 0.00002112
Iteration 89/1000 | Loss: 0.00002112
Iteration 90/1000 | Loss: 0.00002112
Iteration 91/1000 | Loss: 0.00002112
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 91. Stopping optimization.
Last 5 losses: [2.1124331397004426e-05, 2.1124331397004426e-05, 2.1124331397004426e-05, 2.1124331397004426e-05, 2.1124331397004426e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1124331397004426e-05

Optimization complete. Final v2v error: 3.7714080810546875 mm

Highest mean error: 5.046875953674316 mm for frame 126

Lowest mean error: 2.811736822128296 mm for frame 50

Saving results

Total time: 37.40867257118225
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_grace_posed_004/1086/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_grace_posed_004/1086.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_grace_posed_004/1086
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00427573
Iteration 2/25 | Loss: 0.00110538
Iteration 3/25 | Loss: 0.00102191
Iteration 4/25 | Loss: 0.00101257
Iteration 5/25 | Loss: 0.00100994
Iteration 6/25 | Loss: 0.00100892
Iteration 7/25 | Loss: 0.00100881
Iteration 8/25 | Loss: 0.00100881
Iteration 9/25 | Loss: 0.00100881
Iteration 10/25 | Loss: 0.00100881
Iteration 11/25 | Loss: 0.00100881
Iteration 12/25 | Loss: 0.00100881
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0010088124545291066, 0.0010088124545291066, 0.0010088124545291066, 0.0010088124545291066, 0.0010088124545291066]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010088124545291066

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.29487848
Iteration 2/25 | Loss: 0.00108543
Iteration 3/25 | Loss: 0.00108541
Iteration 4/25 | Loss: 0.00108541
Iteration 5/25 | Loss: 0.00108541
Iteration 6/25 | Loss: 0.00108541
Iteration 7/25 | Loss: 0.00108541
Iteration 8/25 | Loss: 0.00108541
Iteration 9/25 | Loss: 0.00108541
Iteration 10/25 | Loss: 0.00108541
Iteration 11/25 | Loss: 0.00108541
Iteration 12/25 | Loss: 0.00108540
Iteration 13/25 | Loss: 0.00108540
Iteration 14/25 | Loss: 0.00108540
Iteration 15/25 | Loss: 0.00108540
Iteration 16/25 | Loss: 0.00108540
Iteration 17/25 | Loss: 0.00108540
Iteration 18/25 | Loss: 0.00108540
Iteration 19/25 | Loss: 0.00108540
Iteration 20/25 | Loss: 0.00108540
Iteration 21/25 | Loss: 0.00108540
Iteration 22/25 | Loss: 0.00108540
Iteration 23/25 | Loss: 0.00108540
Iteration 24/25 | Loss: 0.00108540
Iteration 25/25 | Loss: 0.00108540

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00108540
Iteration 2/1000 | Loss: 0.00002422
Iteration 3/1000 | Loss: 0.00001484
Iteration 4/1000 | Loss: 0.00001154
Iteration 5/1000 | Loss: 0.00001079
Iteration 6/1000 | Loss: 0.00001013
Iteration 7/1000 | Loss: 0.00000978
Iteration 8/1000 | Loss: 0.00000956
Iteration 9/1000 | Loss: 0.00000948
Iteration 10/1000 | Loss: 0.00000947
Iteration 11/1000 | Loss: 0.00000944
Iteration 12/1000 | Loss: 0.00000933
Iteration 13/1000 | Loss: 0.00000925
Iteration 14/1000 | Loss: 0.00000924
Iteration 15/1000 | Loss: 0.00000924
Iteration 16/1000 | Loss: 0.00000924
Iteration 17/1000 | Loss: 0.00000924
Iteration 18/1000 | Loss: 0.00000924
Iteration 19/1000 | Loss: 0.00000924
Iteration 20/1000 | Loss: 0.00000924
Iteration 21/1000 | Loss: 0.00000922
Iteration 22/1000 | Loss: 0.00000921
Iteration 23/1000 | Loss: 0.00000920
Iteration 24/1000 | Loss: 0.00000919
Iteration 25/1000 | Loss: 0.00000919
Iteration 26/1000 | Loss: 0.00000919
Iteration 27/1000 | Loss: 0.00000918
Iteration 28/1000 | Loss: 0.00000918
Iteration 29/1000 | Loss: 0.00000918
Iteration 30/1000 | Loss: 0.00000917
Iteration 31/1000 | Loss: 0.00000917
Iteration 32/1000 | Loss: 0.00000916
Iteration 33/1000 | Loss: 0.00000916
Iteration 34/1000 | Loss: 0.00000916
Iteration 35/1000 | Loss: 0.00000915
Iteration 36/1000 | Loss: 0.00000915
Iteration 37/1000 | Loss: 0.00000914
Iteration 38/1000 | Loss: 0.00000914
Iteration 39/1000 | Loss: 0.00000914
Iteration 40/1000 | Loss: 0.00000913
Iteration 41/1000 | Loss: 0.00000913
Iteration 42/1000 | Loss: 0.00000913
Iteration 43/1000 | Loss: 0.00000912
Iteration 44/1000 | Loss: 0.00000912
Iteration 45/1000 | Loss: 0.00000912
Iteration 46/1000 | Loss: 0.00000912
Iteration 47/1000 | Loss: 0.00000912
Iteration 48/1000 | Loss: 0.00000912
Iteration 49/1000 | Loss: 0.00000912
Iteration 50/1000 | Loss: 0.00000911
Iteration 51/1000 | Loss: 0.00000911
Iteration 52/1000 | Loss: 0.00000910
Iteration 53/1000 | Loss: 0.00000910
Iteration 54/1000 | Loss: 0.00000910
Iteration 55/1000 | Loss: 0.00000908
Iteration 56/1000 | Loss: 0.00000908
Iteration 57/1000 | Loss: 0.00000908
Iteration 58/1000 | Loss: 0.00000908
Iteration 59/1000 | Loss: 0.00000908
Iteration 60/1000 | Loss: 0.00000908
Iteration 61/1000 | Loss: 0.00000907
Iteration 62/1000 | Loss: 0.00000907
Iteration 63/1000 | Loss: 0.00000907
Iteration 64/1000 | Loss: 0.00000907
Iteration 65/1000 | Loss: 0.00000906
Iteration 66/1000 | Loss: 0.00000906
Iteration 67/1000 | Loss: 0.00000906
Iteration 68/1000 | Loss: 0.00000905
Iteration 69/1000 | Loss: 0.00000905
Iteration 70/1000 | Loss: 0.00000904
Iteration 71/1000 | Loss: 0.00000904
Iteration 72/1000 | Loss: 0.00000903
Iteration 73/1000 | Loss: 0.00000903
Iteration 74/1000 | Loss: 0.00000903
Iteration 75/1000 | Loss: 0.00000903
Iteration 76/1000 | Loss: 0.00000902
Iteration 77/1000 | Loss: 0.00000902
Iteration 78/1000 | Loss: 0.00000902
Iteration 79/1000 | Loss: 0.00000902
Iteration 80/1000 | Loss: 0.00000902
Iteration 81/1000 | Loss: 0.00000901
Iteration 82/1000 | Loss: 0.00000901
Iteration 83/1000 | Loss: 0.00000901
Iteration 84/1000 | Loss: 0.00000901
Iteration 85/1000 | Loss: 0.00000901
Iteration 86/1000 | Loss: 0.00000901
Iteration 87/1000 | Loss: 0.00000900
Iteration 88/1000 | Loss: 0.00000900
Iteration 89/1000 | Loss: 0.00000900
Iteration 90/1000 | Loss: 0.00000900
Iteration 91/1000 | Loss: 0.00000899
Iteration 92/1000 | Loss: 0.00000899
Iteration 93/1000 | Loss: 0.00000899
Iteration 94/1000 | Loss: 0.00000899
Iteration 95/1000 | Loss: 0.00000898
Iteration 96/1000 | Loss: 0.00000898
Iteration 97/1000 | Loss: 0.00000898
Iteration 98/1000 | Loss: 0.00000898
Iteration 99/1000 | Loss: 0.00000898
Iteration 100/1000 | Loss: 0.00000898
Iteration 101/1000 | Loss: 0.00000898
Iteration 102/1000 | Loss: 0.00000898
Iteration 103/1000 | Loss: 0.00000898
Iteration 104/1000 | Loss: 0.00000897
Iteration 105/1000 | Loss: 0.00000897
Iteration 106/1000 | Loss: 0.00000897
Iteration 107/1000 | Loss: 0.00000897
Iteration 108/1000 | Loss: 0.00000897
Iteration 109/1000 | Loss: 0.00000897
Iteration 110/1000 | Loss: 0.00000897
Iteration 111/1000 | Loss: 0.00000897
Iteration 112/1000 | Loss: 0.00000897
Iteration 113/1000 | Loss: 0.00000897
Iteration 114/1000 | Loss: 0.00000897
Iteration 115/1000 | Loss: 0.00000896
Iteration 116/1000 | Loss: 0.00000896
Iteration 117/1000 | Loss: 0.00000896
Iteration 118/1000 | Loss: 0.00000896
Iteration 119/1000 | Loss: 0.00000896
Iteration 120/1000 | Loss: 0.00000896
Iteration 121/1000 | Loss: 0.00000896
Iteration 122/1000 | Loss: 0.00000896
Iteration 123/1000 | Loss: 0.00000896
Iteration 124/1000 | Loss: 0.00000896
Iteration 125/1000 | Loss: 0.00000896
Iteration 126/1000 | Loss: 0.00000896
Iteration 127/1000 | Loss: 0.00000896
Iteration 128/1000 | Loss: 0.00000896
Iteration 129/1000 | Loss: 0.00000895
Iteration 130/1000 | Loss: 0.00000895
Iteration 131/1000 | Loss: 0.00000895
Iteration 132/1000 | Loss: 0.00000895
Iteration 133/1000 | Loss: 0.00000895
Iteration 134/1000 | Loss: 0.00000895
Iteration 135/1000 | Loss: 0.00000895
Iteration 136/1000 | Loss: 0.00000895
Iteration 137/1000 | Loss: 0.00000894
Iteration 138/1000 | Loss: 0.00000894
Iteration 139/1000 | Loss: 0.00000894
Iteration 140/1000 | Loss: 0.00000894
Iteration 141/1000 | Loss: 0.00000894
Iteration 142/1000 | Loss: 0.00000894
Iteration 143/1000 | Loss: 0.00000894
Iteration 144/1000 | Loss: 0.00000894
Iteration 145/1000 | Loss: 0.00000894
Iteration 146/1000 | Loss: 0.00000894
Iteration 147/1000 | Loss: 0.00000894
Iteration 148/1000 | Loss: 0.00000893
Iteration 149/1000 | Loss: 0.00000893
Iteration 150/1000 | Loss: 0.00000893
Iteration 151/1000 | Loss: 0.00000893
Iteration 152/1000 | Loss: 0.00000893
Iteration 153/1000 | Loss: 0.00000893
Iteration 154/1000 | Loss: 0.00000893
Iteration 155/1000 | Loss: 0.00000893
Iteration 156/1000 | Loss: 0.00000893
Iteration 157/1000 | Loss: 0.00000893
Iteration 158/1000 | Loss: 0.00000893
Iteration 159/1000 | Loss: 0.00000893
Iteration 160/1000 | Loss: 0.00000893
Iteration 161/1000 | Loss: 0.00000893
Iteration 162/1000 | Loss: 0.00000893
Iteration 163/1000 | Loss: 0.00000893
Iteration 164/1000 | Loss: 0.00000893
Iteration 165/1000 | Loss: 0.00000893
Iteration 166/1000 | Loss: 0.00000893
Iteration 167/1000 | Loss: 0.00000893
Iteration 168/1000 | Loss: 0.00000892
Iteration 169/1000 | Loss: 0.00000892
Iteration 170/1000 | Loss: 0.00000892
Iteration 171/1000 | Loss: 0.00000892
Iteration 172/1000 | Loss: 0.00000892
Iteration 173/1000 | Loss: 0.00000892
Iteration 174/1000 | Loss: 0.00000892
Iteration 175/1000 | Loss: 0.00000892
Iteration 176/1000 | Loss: 0.00000892
Iteration 177/1000 | Loss: 0.00000892
Iteration 178/1000 | Loss: 0.00000892
Iteration 179/1000 | Loss: 0.00000892
Iteration 180/1000 | Loss: 0.00000892
Iteration 181/1000 | Loss: 0.00000892
Iteration 182/1000 | Loss: 0.00000892
Iteration 183/1000 | Loss: 0.00000892
Iteration 184/1000 | Loss: 0.00000892
Iteration 185/1000 | Loss: 0.00000892
Iteration 186/1000 | Loss: 0.00000892
Iteration 187/1000 | Loss: 0.00000892
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 187. Stopping optimization.
Last 5 losses: [8.916530532587785e-06, 8.916530532587785e-06, 8.916530532587785e-06, 8.916530532587785e-06, 8.916530532587785e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.916530532587785e-06

Optimization complete. Final v2v error: 2.5193445682525635 mm

Highest mean error: 3.3098855018615723 mm for frame 62

Lowest mean error: 2.089553117752075 mm for frame 125

Saving results

Total time: 36.69181227684021
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_grace_posed_004/1066/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_grace_posed_004/1066.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_grace_posed_004/1066
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01077968
Iteration 2/25 | Loss: 0.01077967
Iteration 3/25 | Loss: 0.00235287
Iteration 4/25 | Loss: 0.00197956
Iteration 5/25 | Loss: 0.00163080
Iteration 6/25 | Loss: 0.00140861
Iteration 7/25 | Loss: 0.00121565
Iteration 8/25 | Loss: 0.00115401
Iteration 9/25 | Loss: 0.00114066
Iteration 10/25 | Loss: 0.00113000
Iteration 11/25 | Loss: 0.00111821
Iteration 12/25 | Loss: 0.00110678
Iteration 13/25 | Loss: 0.00110518
Iteration 14/25 | Loss: 0.00110376
Iteration 15/25 | Loss: 0.00110141
Iteration 16/25 | Loss: 0.00109887
Iteration 17/25 | Loss: 0.00109766
Iteration 18/25 | Loss: 0.00109738
Iteration 19/25 | Loss: 0.00110058
Iteration 20/25 | Loss: 0.00110026
Iteration 21/25 | Loss: 0.00109996
Iteration 22/25 | Loss: 0.00109804
Iteration 23/25 | Loss: 0.00109719
Iteration 24/25 | Loss: 0.00109679
Iteration 25/25 | Loss: 0.00109597

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.26809716
Iteration 2/25 | Loss: 0.00351235
Iteration 3/25 | Loss: 0.00124096
Iteration 4/25 | Loss: 0.00124096
Iteration 5/25 | Loss: 0.00124096
Iteration 6/25 | Loss: 0.00124096
Iteration 7/25 | Loss: 0.00124096
Iteration 8/25 | Loss: 0.00124096
Iteration 9/25 | Loss: 0.00124096
Iteration 10/25 | Loss: 0.00124096
Iteration 11/25 | Loss: 0.00124096
Iteration 12/25 | Loss: 0.00124096
Iteration 13/25 | Loss: 0.00124096
Iteration 14/25 | Loss: 0.00124096
Iteration 15/25 | Loss: 0.00124096
Iteration 16/25 | Loss: 0.00124096
Iteration 17/25 | Loss: 0.00124096
Iteration 18/25 | Loss: 0.00124096
Iteration 19/25 | Loss: 0.00124096
Iteration 20/25 | Loss: 0.00124096
Iteration 21/25 | Loss: 0.00124096
Iteration 22/25 | Loss: 0.00124096
Iteration 23/25 | Loss: 0.00124096
Iteration 24/25 | Loss: 0.00124096
Iteration 25/25 | Loss: 0.00124096

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00124096
Iteration 2/1000 | Loss: 0.00008924
Iteration 3/1000 | Loss: 0.00183789
Iteration 4/1000 | Loss: 0.00005115
Iteration 5/1000 | Loss: 0.00004337
Iteration 6/1000 | Loss: 0.00003651
Iteration 7/1000 | Loss: 0.00003356
Iteration 8/1000 | Loss: 0.00003142
Iteration 9/1000 | Loss: 0.00003006
Iteration 10/1000 | Loss: 0.00002916
Iteration 11/1000 | Loss: 0.00002877
Iteration 12/1000 | Loss: 0.00002835
Iteration 13/1000 | Loss: 0.00002801
Iteration 14/1000 | Loss: 0.00002795
Iteration 15/1000 | Loss: 0.00002780
Iteration 16/1000 | Loss: 0.00002778
Iteration 17/1000 | Loss: 0.00002757
Iteration 18/1000 | Loss: 0.00002737
Iteration 19/1000 | Loss: 0.00030628
Iteration 20/1000 | Loss: 0.00134965
Iteration 21/1000 | Loss: 0.00004226
Iteration 22/1000 | Loss: 0.00002306
Iteration 23/1000 | Loss: 0.00001753
Iteration 24/1000 | Loss: 0.00001514
Iteration 25/1000 | Loss: 0.00001406
Iteration 26/1000 | Loss: 0.00001342
Iteration 27/1000 | Loss: 0.00001301
Iteration 28/1000 | Loss: 0.00001274
Iteration 29/1000 | Loss: 0.00001246
Iteration 30/1000 | Loss: 0.00001243
Iteration 31/1000 | Loss: 0.00001242
Iteration 32/1000 | Loss: 0.00001236
Iteration 33/1000 | Loss: 0.00001223
Iteration 34/1000 | Loss: 0.00001222
Iteration 35/1000 | Loss: 0.00001218
Iteration 36/1000 | Loss: 0.00001218
Iteration 37/1000 | Loss: 0.00001216
Iteration 38/1000 | Loss: 0.00001215
Iteration 39/1000 | Loss: 0.00001214
Iteration 40/1000 | Loss: 0.00001210
Iteration 41/1000 | Loss: 0.00001204
Iteration 42/1000 | Loss: 0.00001201
Iteration 43/1000 | Loss: 0.00001200
Iteration 44/1000 | Loss: 0.00001200
Iteration 45/1000 | Loss: 0.00001199
Iteration 46/1000 | Loss: 0.00001196
Iteration 47/1000 | Loss: 0.00001196
Iteration 48/1000 | Loss: 0.00001195
Iteration 49/1000 | Loss: 0.00001194
Iteration 50/1000 | Loss: 0.00001194
Iteration 51/1000 | Loss: 0.00001193
Iteration 52/1000 | Loss: 0.00001191
Iteration 53/1000 | Loss: 0.00001191
Iteration 54/1000 | Loss: 0.00001190
Iteration 55/1000 | Loss: 0.00001190
Iteration 56/1000 | Loss: 0.00001189
Iteration 57/1000 | Loss: 0.00001189
Iteration 58/1000 | Loss: 0.00001188
Iteration 59/1000 | Loss: 0.00001188
Iteration 60/1000 | Loss: 0.00001187
Iteration 61/1000 | Loss: 0.00001187
Iteration 62/1000 | Loss: 0.00001187
Iteration 63/1000 | Loss: 0.00001187
Iteration 64/1000 | Loss: 0.00001187
Iteration 65/1000 | Loss: 0.00001187
Iteration 66/1000 | Loss: 0.00001187
Iteration 67/1000 | Loss: 0.00001187
Iteration 68/1000 | Loss: 0.00001186
Iteration 69/1000 | Loss: 0.00001186
Iteration 70/1000 | Loss: 0.00001186
Iteration 71/1000 | Loss: 0.00001185
Iteration 72/1000 | Loss: 0.00001185
Iteration 73/1000 | Loss: 0.00001185
Iteration 74/1000 | Loss: 0.00001185
Iteration 75/1000 | Loss: 0.00001185
Iteration 76/1000 | Loss: 0.00001185
Iteration 77/1000 | Loss: 0.00001184
Iteration 78/1000 | Loss: 0.00001184
Iteration 79/1000 | Loss: 0.00001184
Iteration 80/1000 | Loss: 0.00001183
Iteration 81/1000 | Loss: 0.00001183
Iteration 82/1000 | Loss: 0.00001182
Iteration 83/1000 | Loss: 0.00001182
Iteration 84/1000 | Loss: 0.00001182
Iteration 85/1000 | Loss: 0.00001182
Iteration 86/1000 | Loss: 0.00001182
Iteration 87/1000 | Loss: 0.00001182
Iteration 88/1000 | Loss: 0.00001182
Iteration 89/1000 | Loss: 0.00001181
Iteration 90/1000 | Loss: 0.00001181
Iteration 91/1000 | Loss: 0.00001181
Iteration 92/1000 | Loss: 0.00001181
Iteration 93/1000 | Loss: 0.00001181
Iteration 94/1000 | Loss: 0.00001181
Iteration 95/1000 | Loss: 0.00001181
Iteration 96/1000 | Loss: 0.00001180
Iteration 97/1000 | Loss: 0.00001180
Iteration 98/1000 | Loss: 0.00001180
Iteration 99/1000 | Loss: 0.00001180
Iteration 100/1000 | Loss: 0.00001180
Iteration 101/1000 | Loss: 0.00001180
Iteration 102/1000 | Loss: 0.00001180
Iteration 103/1000 | Loss: 0.00001180
Iteration 104/1000 | Loss: 0.00001179
Iteration 105/1000 | Loss: 0.00001179
Iteration 106/1000 | Loss: 0.00001179
Iteration 107/1000 | Loss: 0.00001179
Iteration 108/1000 | Loss: 0.00001179
Iteration 109/1000 | Loss: 0.00001179
Iteration 110/1000 | Loss: 0.00001179
Iteration 111/1000 | Loss: 0.00001179
Iteration 112/1000 | Loss: 0.00001179
Iteration 113/1000 | Loss: 0.00001179
Iteration 114/1000 | Loss: 0.00001179
Iteration 115/1000 | Loss: 0.00001178
Iteration 116/1000 | Loss: 0.00001178
Iteration 117/1000 | Loss: 0.00001178
Iteration 118/1000 | Loss: 0.00001178
Iteration 119/1000 | Loss: 0.00001178
Iteration 120/1000 | Loss: 0.00001178
Iteration 121/1000 | Loss: 0.00001178
Iteration 122/1000 | Loss: 0.00001178
Iteration 123/1000 | Loss: 0.00001178
Iteration 124/1000 | Loss: 0.00001178
Iteration 125/1000 | Loss: 0.00001178
Iteration 126/1000 | Loss: 0.00001178
Iteration 127/1000 | Loss: 0.00001178
Iteration 128/1000 | Loss: 0.00001178
Iteration 129/1000 | Loss: 0.00001178
Iteration 130/1000 | Loss: 0.00001178
Iteration 131/1000 | Loss: 0.00001177
Iteration 132/1000 | Loss: 0.00001177
Iteration 133/1000 | Loss: 0.00001177
Iteration 134/1000 | Loss: 0.00001177
Iteration 135/1000 | Loss: 0.00001177
Iteration 136/1000 | Loss: 0.00001177
Iteration 137/1000 | Loss: 0.00001177
Iteration 138/1000 | Loss: 0.00001177
Iteration 139/1000 | Loss: 0.00001177
Iteration 140/1000 | Loss: 0.00001177
Iteration 141/1000 | Loss: 0.00001177
Iteration 142/1000 | Loss: 0.00001177
Iteration 143/1000 | Loss: 0.00001177
Iteration 144/1000 | Loss: 0.00001177
Iteration 145/1000 | Loss: 0.00001177
Iteration 146/1000 | Loss: 0.00001176
Iteration 147/1000 | Loss: 0.00001176
Iteration 148/1000 | Loss: 0.00001176
Iteration 149/1000 | Loss: 0.00001176
Iteration 150/1000 | Loss: 0.00001176
Iteration 151/1000 | Loss: 0.00001176
Iteration 152/1000 | Loss: 0.00001176
Iteration 153/1000 | Loss: 0.00001176
Iteration 154/1000 | Loss: 0.00001176
Iteration 155/1000 | Loss: 0.00001176
Iteration 156/1000 | Loss: 0.00001176
Iteration 157/1000 | Loss: 0.00001176
Iteration 158/1000 | Loss: 0.00001176
Iteration 159/1000 | Loss: 0.00001176
Iteration 160/1000 | Loss: 0.00001176
Iteration 161/1000 | Loss: 0.00001176
Iteration 162/1000 | Loss: 0.00001176
Iteration 163/1000 | Loss: 0.00001176
Iteration 164/1000 | Loss: 0.00001176
Iteration 165/1000 | Loss: 0.00001176
Iteration 166/1000 | Loss: 0.00001176
Iteration 167/1000 | Loss: 0.00001176
Iteration 168/1000 | Loss: 0.00001176
Iteration 169/1000 | Loss: 0.00001176
Iteration 170/1000 | Loss: 0.00001176
Iteration 171/1000 | Loss: 0.00001176
Iteration 172/1000 | Loss: 0.00001176
Iteration 173/1000 | Loss: 0.00001176
Iteration 174/1000 | Loss: 0.00001176
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 174. Stopping optimization.
Last 5 losses: [1.1763986549340189e-05, 1.1763986549340189e-05, 1.1763986549340189e-05, 1.1763986549340189e-05, 1.1763986549340189e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1763986549340189e-05

Optimization complete. Final v2v error: 2.905587911605835 mm

Highest mean error: 3.1986663341522217 mm for frame 83

Lowest mean error: 2.645592212677002 mm for frame 198

Saving results

Total time: 110.91280341148376
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_grace_posed_004/1005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_grace_posed_004/1005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_grace_posed_004/1005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00389899
Iteration 2/25 | Loss: 0.00111532
Iteration 3/25 | Loss: 0.00103513
Iteration 4/25 | Loss: 0.00102077
Iteration 5/25 | Loss: 0.00101610
Iteration 6/25 | Loss: 0.00101591
Iteration 7/25 | Loss: 0.00101591
Iteration 8/25 | Loss: 0.00101591
Iteration 9/25 | Loss: 0.00101591
Iteration 10/25 | Loss: 0.00101591
Iteration 11/25 | Loss: 0.00101591
Iteration 12/25 | Loss: 0.00101591
Iteration 13/25 | Loss: 0.00101591
Iteration 14/25 | Loss: 0.00101591
Iteration 15/25 | Loss: 0.00101591
Iteration 16/25 | Loss: 0.00101591
Iteration 17/25 | Loss: 0.00101591
Iteration 18/25 | Loss: 0.00101591
Iteration 19/25 | Loss: 0.00101591
Iteration 20/25 | Loss: 0.00101591
Iteration 21/25 | Loss: 0.00101591
Iteration 22/25 | Loss: 0.00101591
Iteration 23/25 | Loss: 0.00101591
Iteration 24/25 | Loss: 0.00101591
Iteration 25/25 | Loss: 0.00101591

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32166195
Iteration 2/25 | Loss: 0.00108681
Iteration 3/25 | Loss: 0.00108681
Iteration 4/25 | Loss: 0.00108681
Iteration 5/25 | Loss: 0.00108681
Iteration 6/25 | Loss: 0.00108681
Iteration 7/25 | Loss: 0.00108681
Iteration 8/25 | Loss: 0.00108681
Iteration 9/25 | Loss: 0.00108681
Iteration 10/25 | Loss: 0.00108681
Iteration 11/25 | Loss: 0.00108681
Iteration 12/25 | Loss: 0.00108681
Iteration 13/25 | Loss: 0.00108681
Iteration 14/25 | Loss: 0.00108681
Iteration 15/25 | Loss: 0.00108681
Iteration 16/25 | Loss: 0.00108681
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.001086810720153153, 0.001086810720153153, 0.001086810720153153, 0.001086810720153153, 0.001086810720153153]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001086810720153153

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00108681
Iteration 2/1000 | Loss: 0.00001769
Iteration 3/1000 | Loss: 0.00001130
Iteration 4/1000 | Loss: 0.00001012
Iteration 5/1000 | Loss: 0.00000972
Iteration 6/1000 | Loss: 0.00000929
Iteration 7/1000 | Loss: 0.00000896
Iteration 8/1000 | Loss: 0.00000894
Iteration 9/1000 | Loss: 0.00000894
Iteration 10/1000 | Loss: 0.00000866
Iteration 11/1000 | Loss: 0.00000844
Iteration 12/1000 | Loss: 0.00000830
Iteration 13/1000 | Loss: 0.00000824
Iteration 14/1000 | Loss: 0.00000823
Iteration 15/1000 | Loss: 0.00000820
Iteration 16/1000 | Loss: 0.00000814
Iteration 17/1000 | Loss: 0.00000812
Iteration 18/1000 | Loss: 0.00000807
Iteration 19/1000 | Loss: 0.00000805
Iteration 20/1000 | Loss: 0.00000804
Iteration 21/1000 | Loss: 0.00000803
Iteration 22/1000 | Loss: 0.00000803
Iteration 23/1000 | Loss: 0.00000802
Iteration 24/1000 | Loss: 0.00000802
Iteration 25/1000 | Loss: 0.00000799
Iteration 26/1000 | Loss: 0.00000794
Iteration 27/1000 | Loss: 0.00000793
Iteration 28/1000 | Loss: 0.00000791
Iteration 29/1000 | Loss: 0.00000789
Iteration 30/1000 | Loss: 0.00000789
Iteration 31/1000 | Loss: 0.00000789
Iteration 32/1000 | Loss: 0.00000788
Iteration 33/1000 | Loss: 0.00000788
Iteration 34/1000 | Loss: 0.00000782
Iteration 35/1000 | Loss: 0.00000781
Iteration 36/1000 | Loss: 0.00000778
Iteration 37/1000 | Loss: 0.00000778
Iteration 38/1000 | Loss: 0.00000777
Iteration 39/1000 | Loss: 0.00000777
Iteration 40/1000 | Loss: 0.00000777
Iteration 41/1000 | Loss: 0.00000776
Iteration 42/1000 | Loss: 0.00000776
Iteration 43/1000 | Loss: 0.00000775
Iteration 44/1000 | Loss: 0.00000775
Iteration 45/1000 | Loss: 0.00000775
Iteration 46/1000 | Loss: 0.00000774
Iteration 47/1000 | Loss: 0.00000774
Iteration 48/1000 | Loss: 0.00000774
Iteration 49/1000 | Loss: 0.00000774
Iteration 50/1000 | Loss: 0.00000774
Iteration 51/1000 | Loss: 0.00000774
Iteration 52/1000 | Loss: 0.00000773
Iteration 53/1000 | Loss: 0.00000773
Iteration 54/1000 | Loss: 0.00000773
Iteration 55/1000 | Loss: 0.00000773
Iteration 56/1000 | Loss: 0.00000772
Iteration 57/1000 | Loss: 0.00000771
Iteration 58/1000 | Loss: 0.00000771
Iteration 59/1000 | Loss: 0.00000771
Iteration 60/1000 | Loss: 0.00000771
Iteration 61/1000 | Loss: 0.00000771
Iteration 62/1000 | Loss: 0.00000771
Iteration 63/1000 | Loss: 0.00000770
Iteration 64/1000 | Loss: 0.00000770
Iteration 65/1000 | Loss: 0.00000770
Iteration 66/1000 | Loss: 0.00000769
Iteration 67/1000 | Loss: 0.00000769
Iteration 68/1000 | Loss: 0.00000769
Iteration 69/1000 | Loss: 0.00000769
Iteration 70/1000 | Loss: 0.00000769
Iteration 71/1000 | Loss: 0.00000768
Iteration 72/1000 | Loss: 0.00000768
Iteration 73/1000 | Loss: 0.00000768
Iteration 74/1000 | Loss: 0.00000768
Iteration 75/1000 | Loss: 0.00000768
Iteration 76/1000 | Loss: 0.00000768
Iteration 77/1000 | Loss: 0.00000768
Iteration 78/1000 | Loss: 0.00000767
Iteration 79/1000 | Loss: 0.00000767
Iteration 80/1000 | Loss: 0.00000767
Iteration 81/1000 | Loss: 0.00000767
Iteration 82/1000 | Loss: 0.00000766
Iteration 83/1000 | Loss: 0.00000766
Iteration 84/1000 | Loss: 0.00000766
Iteration 85/1000 | Loss: 0.00000766
Iteration 86/1000 | Loss: 0.00000766
Iteration 87/1000 | Loss: 0.00000766
Iteration 88/1000 | Loss: 0.00000765
Iteration 89/1000 | Loss: 0.00000765
Iteration 90/1000 | Loss: 0.00000765
Iteration 91/1000 | Loss: 0.00000765
Iteration 92/1000 | Loss: 0.00000765
Iteration 93/1000 | Loss: 0.00000765
Iteration 94/1000 | Loss: 0.00000765
Iteration 95/1000 | Loss: 0.00000765
Iteration 96/1000 | Loss: 0.00000765
Iteration 97/1000 | Loss: 0.00000765
Iteration 98/1000 | Loss: 0.00000764
Iteration 99/1000 | Loss: 0.00000764
Iteration 100/1000 | Loss: 0.00000764
Iteration 101/1000 | Loss: 0.00000764
Iteration 102/1000 | Loss: 0.00000764
Iteration 103/1000 | Loss: 0.00000764
Iteration 104/1000 | Loss: 0.00000764
Iteration 105/1000 | Loss: 0.00000764
Iteration 106/1000 | Loss: 0.00000763
Iteration 107/1000 | Loss: 0.00000763
Iteration 108/1000 | Loss: 0.00000763
Iteration 109/1000 | Loss: 0.00000763
Iteration 110/1000 | Loss: 0.00000763
Iteration 111/1000 | Loss: 0.00000763
Iteration 112/1000 | Loss: 0.00000763
Iteration 113/1000 | Loss: 0.00000763
Iteration 114/1000 | Loss: 0.00000763
Iteration 115/1000 | Loss: 0.00000763
Iteration 116/1000 | Loss: 0.00000763
Iteration 117/1000 | Loss: 0.00000763
Iteration 118/1000 | Loss: 0.00000763
Iteration 119/1000 | Loss: 0.00000763
Iteration 120/1000 | Loss: 0.00000762
Iteration 121/1000 | Loss: 0.00000762
Iteration 122/1000 | Loss: 0.00000762
Iteration 123/1000 | Loss: 0.00000762
Iteration 124/1000 | Loss: 0.00000762
Iteration 125/1000 | Loss: 0.00000762
Iteration 126/1000 | Loss: 0.00000762
Iteration 127/1000 | Loss: 0.00000762
Iteration 128/1000 | Loss: 0.00000762
Iteration 129/1000 | Loss: 0.00000762
Iteration 130/1000 | Loss: 0.00000761
Iteration 131/1000 | Loss: 0.00000761
Iteration 132/1000 | Loss: 0.00000761
Iteration 133/1000 | Loss: 0.00000761
Iteration 134/1000 | Loss: 0.00000761
Iteration 135/1000 | Loss: 0.00000761
Iteration 136/1000 | Loss: 0.00000761
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 136. Stopping optimization.
Last 5 losses: [7.614239166287007e-06, 7.614239166287007e-06, 7.614239166287007e-06, 7.614239166287007e-06, 7.614239166287007e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 7.614239166287007e-06

Optimization complete. Final v2v error: 2.4031341075897217 mm

Highest mean error: 2.5160977840423584 mm for frame 53

Lowest mean error: 2.3209292888641357 mm for frame 159

Saving results

Total time: 36.02123713493347
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_grace_posed_004/1096/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_grace_posed_004/1096.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_grace_posed_004/1096
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00356282
Iteration 2/25 | Loss: 0.00128044
Iteration 3/25 | Loss: 0.00107478
Iteration 4/25 | Loss: 0.00104815
Iteration 5/25 | Loss: 0.00104249
Iteration 6/25 | Loss: 0.00104013
Iteration 7/25 | Loss: 0.00103953
Iteration 8/25 | Loss: 0.00103953
Iteration 9/25 | Loss: 0.00103953
Iteration 10/25 | Loss: 0.00103953
Iteration 11/25 | Loss: 0.00103953
Iteration 12/25 | Loss: 0.00103953
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0010395282879471779, 0.0010395282879471779, 0.0010395282879471779, 0.0010395282879471779, 0.0010395282879471779]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010395282879471779

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.30153239
Iteration 2/25 | Loss: 0.00141603
Iteration 3/25 | Loss: 0.00141603
Iteration 4/25 | Loss: 0.00141603
Iteration 5/25 | Loss: 0.00141603
Iteration 6/25 | Loss: 0.00141603
Iteration 7/25 | Loss: 0.00141603
Iteration 8/25 | Loss: 0.00141603
Iteration 9/25 | Loss: 0.00141603
Iteration 10/25 | Loss: 0.00141603
Iteration 11/25 | Loss: 0.00141603
Iteration 12/25 | Loss: 0.00141603
Iteration 13/25 | Loss: 0.00141603
Iteration 14/25 | Loss: 0.00141603
Iteration 15/25 | Loss: 0.00141603
Iteration 16/25 | Loss: 0.00141603
Iteration 17/25 | Loss: 0.00141603
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.001416026265360415, 0.001416026265360415, 0.001416026265360415, 0.001416026265360415, 0.001416026265360415]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001416026265360415

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00141603
Iteration 2/1000 | Loss: 0.00003305
Iteration 3/1000 | Loss: 0.00001792
Iteration 4/1000 | Loss: 0.00001215
Iteration 5/1000 | Loss: 0.00001120
Iteration 6/1000 | Loss: 0.00001059
Iteration 7/1000 | Loss: 0.00001032
Iteration 8/1000 | Loss: 0.00001016
Iteration 9/1000 | Loss: 0.00001004
Iteration 10/1000 | Loss: 0.00000994
Iteration 11/1000 | Loss: 0.00000990
Iteration 12/1000 | Loss: 0.00000988
Iteration 13/1000 | Loss: 0.00000987
Iteration 14/1000 | Loss: 0.00000986
Iteration 15/1000 | Loss: 0.00000985
Iteration 16/1000 | Loss: 0.00000983
Iteration 17/1000 | Loss: 0.00000974
Iteration 18/1000 | Loss: 0.00000970
Iteration 19/1000 | Loss: 0.00000969
Iteration 20/1000 | Loss: 0.00000969
Iteration 21/1000 | Loss: 0.00000968
Iteration 22/1000 | Loss: 0.00000968
Iteration 23/1000 | Loss: 0.00000968
Iteration 24/1000 | Loss: 0.00000968
Iteration 25/1000 | Loss: 0.00000968
Iteration 26/1000 | Loss: 0.00000967
Iteration 27/1000 | Loss: 0.00000966
Iteration 28/1000 | Loss: 0.00000965
Iteration 29/1000 | Loss: 0.00000964
Iteration 30/1000 | Loss: 0.00000963
Iteration 31/1000 | Loss: 0.00000962
Iteration 32/1000 | Loss: 0.00000962
Iteration 33/1000 | Loss: 0.00000958
Iteration 34/1000 | Loss: 0.00000957
Iteration 35/1000 | Loss: 0.00000957
Iteration 36/1000 | Loss: 0.00000956
Iteration 37/1000 | Loss: 0.00000955
Iteration 38/1000 | Loss: 0.00000955
Iteration 39/1000 | Loss: 0.00000951
Iteration 40/1000 | Loss: 0.00000951
Iteration 41/1000 | Loss: 0.00000949
Iteration 42/1000 | Loss: 0.00000949
Iteration 43/1000 | Loss: 0.00000948
Iteration 44/1000 | Loss: 0.00000948
Iteration 45/1000 | Loss: 0.00000947
Iteration 46/1000 | Loss: 0.00000947
Iteration 47/1000 | Loss: 0.00000946
Iteration 48/1000 | Loss: 0.00000946
Iteration 49/1000 | Loss: 0.00000946
Iteration 50/1000 | Loss: 0.00000946
Iteration 51/1000 | Loss: 0.00000946
Iteration 52/1000 | Loss: 0.00000945
Iteration 53/1000 | Loss: 0.00000945
Iteration 54/1000 | Loss: 0.00000945
Iteration 55/1000 | Loss: 0.00000945
Iteration 56/1000 | Loss: 0.00000945
Iteration 57/1000 | Loss: 0.00000944
Iteration 58/1000 | Loss: 0.00000944
Iteration 59/1000 | Loss: 0.00000944
Iteration 60/1000 | Loss: 0.00000943
Iteration 61/1000 | Loss: 0.00000943
Iteration 62/1000 | Loss: 0.00000943
Iteration 63/1000 | Loss: 0.00000943
Iteration 64/1000 | Loss: 0.00000942
Iteration 65/1000 | Loss: 0.00000942
Iteration 66/1000 | Loss: 0.00000942
Iteration 67/1000 | Loss: 0.00000942
Iteration 68/1000 | Loss: 0.00000942
Iteration 69/1000 | Loss: 0.00000941
Iteration 70/1000 | Loss: 0.00000941
Iteration 71/1000 | Loss: 0.00000941
Iteration 72/1000 | Loss: 0.00000940
Iteration 73/1000 | Loss: 0.00000940
Iteration 74/1000 | Loss: 0.00000940
Iteration 75/1000 | Loss: 0.00000939
Iteration 76/1000 | Loss: 0.00000939
Iteration 77/1000 | Loss: 0.00000939
Iteration 78/1000 | Loss: 0.00000939
Iteration 79/1000 | Loss: 0.00000939
Iteration 80/1000 | Loss: 0.00000938
Iteration 81/1000 | Loss: 0.00000938
Iteration 82/1000 | Loss: 0.00000938
Iteration 83/1000 | Loss: 0.00000937
Iteration 84/1000 | Loss: 0.00000937
Iteration 85/1000 | Loss: 0.00000937
Iteration 86/1000 | Loss: 0.00000937
Iteration 87/1000 | Loss: 0.00000937
Iteration 88/1000 | Loss: 0.00000937
Iteration 89/1000 | Loss: 0.00000937
Iteration 90/1000 | Loss: 0.00000936
Iteration 91/1000 | Loss: 0.00000936
Iteration 92/1000 | Loss: 0.00000936
Iteration 93/1000 | Loss: 0.00000936
Iteration 94/1000 | Loss: 0.00000936
Iteration 95/1000 | Loss: 0.00000936
Iteration 96/1000 | Loss: 0.00000936
Iteration 97/1000 | Loss: 0.00000935
Iteration 98/1000 | Loss: 0.00000935
Iteration 99/1000 | Loss: 0.00000934
Iteration 100/1000 | Loss: 0.00000934
Iteration 101/1000 | Loss: 0.00000933
Iteration 102/1000 | Loss: 0.00000933
Iteration 103/1000 | Loss: 0.00000933
Iteration 104/1000 | Loss: 0.00000932
Iteration 105/1000 | Loss: 0.00000932
Iteration 106/1000 | Loss: 0.00000932
Iteration 107/1000 | Loss: 0.00000931
Iteration 108/1000 | Loss: 0.00000931
Iteration 109/1000 | Loss: 0.00000931
Iteration 110/1000 | Loss: 0.00000930
Iteration 111/1000 | Loss: 0.00000930
Iteration 112/1000 | Loss: 0.00000930
Iteration 113/1000 | Loss: 0.00000930
Iteration 114/1000 | Loss: 0.00000929
Iteration 115/1000 | Loss: 0.00000929
Iteration 116/1000 | Loss: 0.00000929
Iteration 117/1000 | Loss: 0.00000929
Iteration 118/1000 | Loss: 0.00000929
Iteration 119/1000 | Loss: 0.00000929
Iteration 120/1000 | Loss: 0.00000928
Iteration 121/1000 | Loss: 0.00000928
Iteration 122/1000 | Loss: 0.00000928
Iteration 123/1000 | Loss: 0.00000928
Iteration 124/1000 | Loss: 0.00000927
Iteration 125/1000 | Loss: 0.00000927
Iteration 126/1000 | Loss: 0.00000927
Iteration 127/1000 | Loss: 0.00000927
Iteration 128/1000 | Loss: 0.00000927
Iteration 129/1000 | Loss: 0.00000927
Iteration 130/1000 | Loss: 0.00000927
Iteration 131/1000 | Loss: 0.00000927
Iteration 132/1000 | Loss: 0.00000927
Iteration 133/1000 | Loss: 0.00000926
Iteration 134/1000 | Loss: 0.00000926
Iteration 135/1000 | Loss: 0.00000926
Iteration 136/1000 | Loss: 0.00000926
Iteration 137/1000 | Loss: 0.00000926
Iteration 138/1000 | Loss: 0.00000926
Iteration 139/1000 | Loss: 0.00000926
Iteration 140/1000 | Loss: 0.00000926
Iteration 141/1000 | Loss: 0.00000926
Iteration 142/1000 | Loss: 0.00000925
Iteration 143/1000 | Loss: 0.00000925
Iteration 144/1000 | Loss: 0.00000925
Iteration 145/1000 | Loss: 0.00000925
Iteration 146/1000 | Loss: 0.00000925
Iteration 147/1000 | Loss: 0.00000924
Iteration 148/1000 | Loss: 0.00000924
Iteration 149/1000 | Loss: 0.00000924
Iteration 150/1000 | Loss: 0.00000924
Iteration 151/1000 | Loss: 0.00000924
Iteration 152/1000 | Loss: 0.00000924
Iteration 153/1000 | Loss: 0.00000924
Iteration 154/1000 | Loss: 0.00000924
Iteration 155/1000 | Loss: 0.00000924
Iteration 156/1000 | Loss: 0.00000924
Iteration 157/1000 | Loss: 0.00000924
Iteration 158/1000 | Loss: 0.00000924
Iteration 159/1000 | Loss: 0.00000924
Iteration 160/1000 | Loss: 0.00000924
Iteration 161/1000 | Loss: 0.00000924
Iteration 162/1000 | Loss: 0.00000924
Iteration 163/1000 | Loss: 0.00000924
Iteration 164/1000 | Loss: 0.00000924
Iteration 165/1000 | Loss: 0.00000924
Iteration 166/1000 | Loss: 0.00000924
Iteration 167/1000 | Loss: 0.00000924
Iteration 168/1000 | Loss: 0.00000924
Iteration 169/1000 | Loss: 0.00000924
Iteration 170/1000 | Loss: 0.00000924
Iteration 171/1000 | Loss: 0.00000924
Iteration 172/1000 | Loss: 0.00000924
Iteration 173/1000 | Loss: 0.00000924
Iteration 174/1000 | Loss: 0.00000924
Iteration 175/1000 | Loss: 0.00000924
Iteration 176/1000 | Loss: 0.00000924
Iteration 177/1000 | Loss: 0.00000924
Iteration 178/1000 | Loss: 0.00000924
Iteration 179/1000 | Loss: 0.00000924
Iteration 180/1000 | Loss: 0.00000924
Iteration 181/1000 | Loss: 0.00000924
Iteration 182/1000 | Loss: 0.00000924
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 182. Stopping optimization.
Last 5 losses: [9.239043720299378e-06, 9.239043720299378e-06, 9.239043720299378e-06, 9.239043720299378e-06, 9.239043720299378e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.239043720299378e-06

Optimization complete. Final v2v error: 2.608030080795288 mm

Highest mean error: 2.840266227722168 mm for frame 103

Lowest mean error: 2.2898359298706055 mm for frame 8

Saving results

Total time: 36.91930913925171
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_grace_posed_004/1010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_grace_posed_004/1010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_grace_posed_004/1010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00397035
Iteration 2/25 | Loss: 0.00117013
Iteration 3/25 | Loss: 0.00109567
Iteration 4/25 | Loss: 0.00107694
Iteration 5/25 | Loss: 0.00106998
Iteration 6/25 | Loss: 0.00106816
Iteration 7/25 | Loss: 0.00106816
Iteration 8/25 | Loss: 0.00106816
Iteration 9/25 | Loss: 0.00106816
Iteration 10/25 | Loss: 0.00106816
Iteration 11/25 | Loss: 0.00106816
Iteration 12/25 | Loss: 0.00106816
Iteration 13/25 | Loss: 0.00106816
Iteration 14/25 | Loss: 0.00106816
Iteration 15/25 | Loss: 0.00106816
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0010681560961529613, 0.0010681560961529613, 0.0010681560961529613, 0.0010681560961529613, 0.0010681560961529613]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010681560961529613

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.61578393
Iteration 2/25 | Loss: 0.00107909
Iteration 3/25 | Loss: 0.00107909
Iteration 4/25 | Loss: 0.00107909
Iteration 5/25 | Loss: 0.00107909
Iteration 6/25 | Loss: 0.00107909
Iteration 7/25 | Loss: 0.00107909
Iteration 8/25 | Loss: 0.00107909
Iteration 9/25 | Loss: 0.00107909
Iteration 10/25 | Loss: 0.00107909
Iteration 11/25 | Loss: 0.00107909
Iteration 12/25 | Loss: 0.00107909
Iteration 13/25 | Loss: 0.00107909
Iteration 14/25 | Loss: 0.00107909
Iteration 15/25 | Loss: 0.00107909
Iteration 16/25 | Loss: 0.00107909
Iteration 17/25 | Loss: 0.00107909
Iteration 18/25 | Loss: 0.00107909
Iteration 19/25 | Loss: 0.00107909
Iteration 20/25 | Loss: 0.00107909
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.001079088426195085, 0.001079088426195085, 0.001079088426195085, 0.001079088426195085, 0.001079088426195085]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001079088426195085

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00107909
Iteration 2/1000 | Loss: 0.00002904
Iteration 3/1000 | Loss: 0.00002230
Iteration 4/1000 | Loss: 0.00002107
Iteration 5/1000 | Loss: 0.00002046
Iteration 6/1000 | Loss: 0.00002002
Iteration 7/1000 | Loss: 0.00001956
Iteration 8/1000 | Loss: 0.00001925
Iteration 9/1000 | Loss: 0.00001904
Iteration 10/1000 | Loss: 0.00001902
Iteration 11/1000 | Loss: 0.00001896
Iteration 12/1000 | Loss: 0.00001887
Iteration 13/1000 | Loss: 0.00001885
Iteration 14/1000 | Loss: 0.00001883
Iteration 15/1000 | Loss: 0.00001872
Iteration 16/1000 | Loss: 0.00001868
Iteration 17/1000 | Loss: 0.00001868
Iteration 18/1000 | Loss: 0.00001867
Iteration 19/1000 | Loss: 0.00001857
Iteration 20/1000 | Loss: 0.00001855
Iteration 21/1000 | Loss: 0.00001853
Iteration 22/1000 | Loss: 0.00001851
Iteration 23/1000 | Loss: 0.00001851
Iteration 24/1000 | Loss: 0.00001851
Iteration 25/1000 | Loss: 0.00001850
Iteration 26/1000 | Loss: 0.00001850
Iteration 27/1000 | Loss: 0.00001849
Iteration 28/1000 | Loss: 0.00001849
Iteration 29/1000 | Loss: 0.00001849
Iteration 30/1000 | Loss: 0.00001848
Iteration 31/1000 | Loss: 0.00001847
Iteration 32/1000 | Loss: 0.00001847
Iteration 33/1000 | Loss: 0.00001846
Iteration 34/1000 | Loss: 0.00001846
Iteration 35/1000 | Loss: 0.00001846
Iteration 36/1000 | Loss: 0.00001846
Iteration 37/1000 | Loss: 0.00001845
Iteration 38/1000 | Loss: 0.00001845
Iteration 39/1000 | Loss: 0.00001845
Iteration 40/1000 | Loss: 0.00001844
Iteration 41/1000 | Loss: 0.00001843
Iteration 42/1000 | Loss: 0.00001842
Iteration 43/1000 | Loss: 0.00001841
Iteration 44/1000 | Loss: 0.00001841
Iteration 45/1000 | Loss: 0.00001841
Iteration 46/1000 | Loss: 0.00001841
Iteration 47/1000 | Loss: 0.00001841
Iteration 48/1000 | Loss: 0.00001841
Iteration 49/1000 | Loss: 0.00001840
Iteration 50/1000 | Loss: 0.00001840
Iteration 51/1000 | Loss: 0.00001840
Iteration 52/1000 | Loss: 0.00001839
Iteration 53/1000 | Loss: 0.00001839
Iteration 54/1000 | Loss: 0.00001839
Iteration 55/1000 | Loss: 0.00001838
Iteration 56/1000 | Loss: 0.00001838
Iteration 57/1000 | Loss: 0.00001838
Iteration 58/1000 | Loss: 0.00001838
Iteration 59/1000 | Loss: 0.00001837
Iteration 60/1000 | Loss: 0.00001837
Iteration 61/1000 | Loss: 0.00001837
Iteration 62/1000 | Loss: 0.00001837
Iteration 63/1000 | Loss: 0.00001837
Iteration 64/1000 | Loss: 0.00001837
Iteration 65/1000 | Loss: 0.00001837
Iteration 66/1000 | Loss: 0.00001836
Iteration 67/1000 | Loss: 0.00001836
Iteration 68/1000 | Loss: 0.00001836
Iteration 69/1000 | Loss: 0.00001836
Iteration 70/1000 | Loss: 0.00001836
Iteration 71/1000 | Loss: 0.00001835
Iteration 72/1000 | Loss: 0.00001835
Iteration 73/1000 | Loss: 0.00001835
Iteration 74/1000 | Loss: 0.00001834
Iteration 75/1000 | Loss: 0.00001834
Iteration 76/1000 | Loss: 0.00001834
Iteration 77/1000 | Loss: 0.00001834
Iteration 78/1000 | Loss: 0.00001834
Iteration 79/1000 | Loss: 0.00001833
Iteration 80/1000 | Loss: 0.00001833
Iteration 81/1000 | Loss: 0.00001833
Iteration 82/1000 | Loss: 0.00001833
Iteration 83/1000 | Loss: 0.00001833
Iteration 84/1000 | Loss: 0.00001833
Iteration 85/1000 | Loss: 0.00001833
Iteration 86/1000 | Loss: 0.00001832
Iteration 87/1000 | Loss: 0.00001832
Iteration 88/1000 | Loss: 0.00001832
Iteration 89/1000 | Loss: 0.00001832
Iteration 90/1000 | Loss: 0.00001832
Iteration 91/1000 | Loss: 0.00001832
Iteration 92/1000 | Loss: 0.00001832
Iteration 93/1000 | Loss: 0.00001832
Iteration 94/1000 | Loss: 0.00001832
Iteration 95/1000 | Loss: 0.00001832
Iteration 96/1000 | Loss: 0.00001831
Iteration 97/1000 | Loss: 0.00001831
Iteration 98/1000 | Loss: 0.00001831
Iteration 99/1000 | Loss: 0.00001830
Iteration 100/1000 | Loss: 0.00001829
Iteration 101/1000 | Loss: 0.00001829
Iteration 102/1000 | Loss: 0.00001829
Iteration 103/1000 | Loss: 0.00001829
Iteration 104/1000 | Loss: 0.00001829
Iteration 105/1000 | Loss: 0.00001828
Iteration 106/1000 | Loss: 0.00001828
Iteration 107/1000 | Loss: 0.00001828
Iteration 108/1000 | Loss: 0.00001828
Iteration 109/1000 | Loss: 0.00001828
Iteration 110/1000 | Loss: 0.00001828
Iteration 111/1000 | Loss: 0.00001828
Iteration 112/1000 | Loss: 0.00001827
Iteration 113/1000 | Loss: 0.00001827
Iteration 114/1000 | Loss: 0.00001827
Iteration 115/1000 | Loss: 0.00001827
Iteration 116/1000 | Loss: 0.00001826
Iteration 117/1000 | Loss: 0.00001826
Iteration 118/1000 | Loss: 0.00001826
Iteration 119/1000 | Loss: 0.00001826
Iteration 120/1000 | Loss: 0.00001826
Iteration 121/1000 | Loss: 0.00001826
Iteration 122/1000 | Loss: 0.00001826
Iteration 123/1000 | Loss: 0.00001826
Iteration 124/1000 | Loss: 0.00001825
Iteration 125/1000 | Loss: 0.00001825
Iteration 126/1000 | Loss: 0.00001825
Iteration 127/1000 | Loss: 0.00001825
Iteration 128/1000 | Loss: 0.00001825
Iteration 129/1000 | Loss: 0.00001824
Iteration 130/1000 | Loss: 0.00001824
Iteration 131/1000 | Loss: 0.00001824
Iteration 132/1000 | Loss: 0.00001824
Iteration 133/1000 | Loss: 0.00001824
Iteration 134/1000 | Loss: 0.00001824
Iteration 135/1000 | Loss: 0.00001823
Iteration 136/1000 | Loss: 0.00001823
Iteration 137/1000 | Loss: 0.00001823
Iteration 138/1000 | Loss: 0.00001823
Iteration 139/1000 | Loss: 0.00001823
Iteration 140/1000 | Loss: 0.00001823
Iteration 141/1000 | Loss: 0.00001823
Iteration 142/1000 | Loss: 0.00001823
Iteration 143/1000 | Loss: 0.00001823
Iteration 144/1000 | Loss: 0.00001823
Iteration 145/1000 | Loss: 0.00001822
Iteration 146/1000 | Loss: 0.00001822
Iteration 147/1000 | Loss: 0.00001822
Iteration 148/1000 | Loss: 0.00001822
Iteration 149/1000 | Loss: 0.00001822
Iteration 150/1000 | Loss: 0.00001822
Iteration 151/1000 | Loss: 0.00001822
Iteration 152/1000 | Loss: 0.00001822
Iteration 153/1000 | Loss: 0.00001822
Iteration 154/1000 | Loss: 0.00001822
Iteration 155/1000 | Loss: 0.00001822
Iteration 156/1000 | Loss: 0.00001822
Iteration 157/1000 | Loss: 0.00001821
Iteration 158/1000 | Loss: 0.00001821
Iteration 159/1000 | Loss: 0.00001821
Iteration 160/1000 | Loss: 0.00001821
Iteration 161/1000 | Loss: 0.00001821
Iteration 162/1000 | Loss: 0.00001821
Iteration 163/1000 | Loss: 0.00001821
Iteration 164/1000 | Loss: 0.00001821
Iteration 165/1000 | Loss: 0.00001821
Iteration 166/1000 | Loss: 0.00001821
Iteration 167/1000 | Loss: 0.00001821
Iteration 168/1000 | Loss: 0.00001821
Iteration 169/1000 | Loss: 0.00001821
Iteration 170/1000 | Loss: 0.00001821
Iteration 171/1000 | Loss: 0.00001821
Iteration 172/1000 | Loss: 0.00001821
Iteration 173/1000 | Loss: 0.00001821
Iteration 174/1000 | Loss: 0.00001821
Iteration 175/1000 | Loss: 0.00001821
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 175. Stopping optimization.
Last 5 losses: [1.820637407945469e-05, 1.820637407945469e-05, 1.820637407945469e-05, 1.820637407945469e-05, 1.820637407945469e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.820637407945469e-05

Optimization complete. Final v2v error: 3.6035008430480957 mm

Highest mean error: 3.8946917057037354 mm for frame 75

Lowest mean error: 3.3099939823150635 mm for frame 10

Saving results

Total time: 38.162511587142944
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_grace_posed_004/1097/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_grace_posed_004/1097.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_grace_posed_004/1097
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00928977
Iteration 2/25 | Loss: 0.00119549
Iteration 3/25 | Loss: 0.00106254
Iteration 4/25 | Loss: 0.00105023
Iteration 5/25 | Loss: 0.00104765
Iteration 6/25 | Loss: 0.00104765
Iteration 7/25 | Loss: 0.00104765
Iteration 8/25 | Loss: 0.00104765
Iteration 9/25 | Loss: 0.00104765
Iteration 10/25 | Loss: 0.00104765
Iteration 11/25 | Loss: 0.00104765
Iteration 12/25 | Loss: 0.00104765
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0010476504685357213, 0.0010476504685357213, 0.0010476504685357213, 0.0010476504685357213, 0.0010476504685357213]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010476504685357213

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32720494
Iteration 2/25 | Loss: 0.00098551
Iteration 3/25 | Loss: 0.00098551
Iteration 4/25 | Loss: 0.00098551
Iteration 5/25 | Loss: 0.00098550
Iteration 6/25 | Loss: 0.00098550
Iteration 7/25 | Loss: 0.00098550
Iteration 8/25 | Loss: 0.00098550
Iteration 9/25 | Loss: 0.00098550
Iteration 10/25 | Loss: 0.00098550
Iteration 11/25 | Loss: 0.00098550
Iteration 12/25 | Loss: 0.00098550
Iteration 13/25 | Loss: 0.00098550
Iteration 14/25 | Loss: 0.00098550
Iteration 15/25 | Loss: 0.00098550
Iteration 16/25 | Loss: 0.00098550
Iteration 17/25 | Loss: 0.00098550
Iteration 18/25 | Loss: 0.00098550
Iteration 19/25 | Loss: 0.00098550
Iteration 20/25 | Loss: 0.00098550
Iteration 21/25 | Loss: 0.00098550
Iteration 22/25 | Loss: 0.00098550
Iteration 23/25 | Loss: 0.00098550
Iteration 24/25 | Loss: 0.00098550
Iteration 25/25 | Loss: 0.00098550

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00098550
Iteration 2/1000 | Loss: 0.00002018
Iteration 3/1000 | Loss: 0.00001328
Iteration 4/1000 | Loss: 0.00001206
Iteration 5/1000 | Loss: 0.00001146
Iteration 6/1000 | Loss: 0.00001114
Iteration 7/1000 | Loss: 0.00001072
Iteration 8/1000 | Loss: 0.00001038
Iteration 9/1000 | Loss: 0.00001011
Iteration 10/1000 | Loss: 0.00000987
Iteration 11/1000 | Loss: 0.00000971
Iteration 12/1000 | Loss: 0.00000970
Iteration 13/1000 | Loss: 0.00000963
Iteration 14/1000 | Loss: 0.00000959
Iteration 15/1000 | Loss: 0.00000955
Iteration 16/1000 | Loss: 0.00000955
Iteration 17/1000 | Loss: 0.00000954
Iteration 18/1000 | Loss: 0.00000953
Iteration 19/1000 | Loss: 0.00000952
Iteration 20/1000 | Loss: 0.00000951
Iteration 21/1000 | Loss: 0.00000947
Iteration 22/1000 | Loss: 0.00000946
Iteration 23/1000 | Loss: 0.00000945
Iteration 24/1000 | Loss: 0.00000942
Iteration 25/1000 | Loss: 0.00000941
Iteration 26/1000 | Loss: 0.00000941
Iteration 27/1000 | Loss: 0.00000940
Iteration 28/1000 | Loss: 0.00000940
Iteration 29/1000 | Loss: 0.00000939
Iteration 30/1000 | Loss: 0.00000936
Iteration 31/1000 | Loss: 0.00000935
Iteration 32/1000 | Loss: 0.00000935
Iteration 33/1000 | Loss: 0.00000935
Iteration 34/1000 | Loss: 0.00000935
Iteration 35/1000 | Loss: 0.00000935
Iteration 36/1000 | Loss: 0.00000935
Iteration 37/1000 | Loss: 0.00000934
Iteration 38/1000 | Loss: 0.00000934
Iteration 39/1000 | Loss: 0.00000931
Iteration 40/1000 | Loss: 0.00000930
Iteration 41/1000 | Loss: 0.00000930
Iteration 42/1000 | Loss: 0.00000929
Iteration 43/1000 | Loss: 0.00000929
Iteration 44/1000 | Loss: 0.00000929
Iteration 45/1000 | Loss: 0.00000928
Iteration 46/1000 | Loss: 0.00000927
Iteration 47/1000 | Loss: 0.00000927
Iteration 48/1000 | Loss: 0.00000926
Iteration 49/1000 | Loss: 0.00000926
Iteration 50/1000 | Loss: 0.00000925
Iteration 51/1000 | Loss: 0.00000925
Iteration 52/1000 | Loss: 0.00000925
Iteration 53/1000 | Loss: 0.00000925
Iteration 54/1000 | Loss: 0.00000925
Iteration 55/1000 | Loss: 0.00000924
Iteration 56/1000 | Loss: 0.00000924
Iteration 57/1000 | Loss: 0.00000923
Iteration 58/1000 | Loss: 0.00000922
Iteration 59/1000 | Loss: 0.00000922
Iteration 60/1000 | Loss: 0.00000922
Iteration 61/1000 | Loss: 0.00000921
Iteration 62/1000 | Loss: 0.00000921
Iteration 63/1000 | Loss: 0.00000921
Iteration 64/1000 | Loss: 0.00000921
Iteration 65/1000 | Loss: 0.00000921
Iteration 66/1000 | Loss: 0.00000921
Iteration 67/1000 | Loss: 0.00000921
Iteration 68/1000 | Loss: 0.00000920
Iteration 69/1000 | Loss: 0.00000920
Iteration 70/1000 | Loss: 0.00000920
Iteration 71/1000 | Loss: 0.00000920
Iteration 72/1000 | Loss: 0.00000919
Iteration 73/1000 | Loss: 0.00000919
Iteration 74/1000 | Loss: 0.00000919
Iteration 75/1000 | Loss: 0.00000918
Iteration 76/1000 | Loss: 0.00000918
Iteration 77/1000 | Loss: 0.00000918
Iteration 78/1000 | Loss: 0.00000918
Iteration 79/1000 | Loss: 0.00000917
Iteration 80/1000 | Loss: 0.00000917
Iteration 81/1000 | Loss: 0.00000917
Iteration 82/1000 | Loss: 0.00000916
Iteration 83/1000 | Loss: 0.00000915
Iteration 84/1000 | Loss: 0.00000915
Iteration 85/1000 | Loss: 0.00000915
Iteration 86/1000 | Loss: 0.00000915
Iteration 87/1000 | Loss: 0.00000915
Iteration 88/1000 | Loss: 0.00000914
Iteration 89/1000 | Loss: 0.00000914
Iteration 90/1000 | Loss: 0.00000914
Iteration 91/1000 | Loss: 0.00000914
Iteration 92/1000 | Loss: 0.00000914
Iteration 93/1000 | Loss: 0.00000914
Iteration 94/1000 | Loss: 0.00000914
Iteration 95/1000 | Loss: 0.00000913
Iteration 96/1000 | Loss: 0.00000913
Iteration 97/1000 | Loss: 0.00000913
Iteration 98/1000 | Loss: 0.00000912
Iteration 99/1000 | Loss: 0.00000912
Iteration 100/1000 | Loss: 0.00000912
Iteration 101/1000 | Loss: 0.00000912
Iteration 102/1000 | Loss: 0.00000912
Iteration 103/1000 | Loss: 0.00000912
Iteration 104/1000 | Loss: 0.00000912
Iteration 105/1000 | Loss: 0.00000911
Iteration 106/1000 | Loss: 0.00000911
Iteration 107/1000 | Loss: 0.00000911
Iteration 108/1000 | Loss: 0.00000910
Iteration 109/1000 | Loss: 0.00000910
Iteration 110/1000 | Loss: 0.00000910
Iteration 111/1000 | Loss: 0.00000910
Iteration 112/1000 | Loss: 0.00000910
Iteration 113/1000 | Loss: 0.00000910
Iteration 114/1000 | Loss: 0.00000910
Iteration 115/1000 | Loss: 0.00000910
Iteration 116/1000 | Loss: 0.00000909
Iteration 117/1000 | Loss: 0.00000909
Iteration 118/1000 | Loss: 0.00000909
Iteration 119/1000 | Loss: 0.00000909
Iteration 120/1000 | Loss: 0.00000909
Iteration 121/1000 | Loss: 0.00000908
Iteration 122/1000 | Loss: 0.00000908
Iteration 123/1000 | Loss: 0.00000908
Iteration 124/1000 | Loss: 0.00000908
Iteration 125/1000 | Loss: 0.00000908
Iteration 126/1000 | Loss: 0.00000908
Iteration 127/1000 | Loss: 0.00000908
Iteration 128/1000 | Loss: 0.00000908
Iteration 129/1000 | Loss: 0.00000908
Iteration 130/1000 | Loss: 0.00000908
Iteration 131/1000 | Loss: 0.00000908
Iteration 132/1000 | Loss: 0.00000908
Iteration 133/1000 | Loss: 0.00000908
Iteration 134/1000 | Loss: 0.00000908
Iteration 135/1000 | Loss: 0.00000908
Iteration 136/1000 | Loss: 0.00000908
Iteration 137/1000 | Loss: 0.00000908
Iteration 138/1000 | Loss: 0.00000908
Iteration 139/1000 | Loss: 0.00000908
Iteration 140/1000 | Loss: 0.00000908
Iteration 141/1000 | Loss: 0.00000908
Iteration 142/1000 | Loss: 0.00000908
Iteration 143/1000 | Loss: 0.00000908
Iteration 144/1000 | Loss: 0.00000908
Iteration 145/1000 | Loss: 0.00000908
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 145. Stopping optimization.
Last 5 losses: [9.081133612198755e-06, 9.081133612198755e-06, 9.081133612198755e-06, 9.081133612198755e-06, 9.081133612198755e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.081133612198755e-06

Optimization complete. Final v2v error: 2.5713024139404297 mm

Highest mean error: 3.1646196842193604 mm for frame 58

Lowest mean error: 2.3171517848968506 mm for frame 112

Saving results

Total time: 35.614781618118286
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_grace_posed_004/1049/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_grace_posed_004/1049.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_grace_posed_004/1049
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00986413
Iteration 2/25 | Loss: 0.00986413
Iteration 3/25 | Loss: 0.00986413
Iteration 4/25 | Loss: 0.00289969
Iteration 5/25 | Loss: 0.00208933
Iteration 6/25 | Loss: 0.00181607
Iteration 7/25 | Loss: 0.00169037
Iteration 8/25 | Loss: 0.00169906
Iteration 9/25 | Loss: 0.00169992
Iteration 10/25 | Loss: 0.00163948
Iteration 11/25 | Loss: 0.00159241
Iteration 12/25 | Loss: 0.00153954
Iteration 13/25 | Loss: 0.00149811
Iteration 14/25 | Loss: 0.00155877
Iteration 15/25 | Loss: 0.00148702
Iteration 16/25 | Loss: 0.00145745
Iteration 17/25 | Loss: 0.00142907
Iteration 18/25 | Loss: 0.00141520
Iteration 19/25 | Loss: 0.00139037
Iteration 20/25 | Loss: 0.00139119
Iteration 21/25 | Loss: 0.00138546
Iteration 22/25 | Loss: 0.00137774
Iteration 23/25 | Loss: 0.00137467
Iteration 24/25 | Loss: 0.00136294
Iteration 25/25 | Loss: 0.00136280

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.31421864
Iteration 2/25 | Loss: 0.00471478
Iteration 3/25 | Loss: 0.00320112
Iteration 4/25 | Loss: 0.00320111
Iteration 5/25 | Loss: 0.00320111
Iteration 6/25 | Loss: 0.00320111
Iteration 7/25 | Loss: 0.00320111
Iteration 8/25 | Loss: 0.00320111
Iteration 9/25 | Loss: 0.00320111
Iteration 10/25 | Loss: 0.00320111
Iteration 11/25 | Loss: 0.00320111
Iteration 12/25 | Loss: 0.00320111
Iteration 13/25 | Loss: 0.00320111
Iteration 14/25 | Loss: 0.00320111
Iteration 15/25 | Loss: 0.00320111
Iteration 16/25 | Loss: 0.00320111
Iteration 17/25 | Loss: 0.00320111
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0032011112198233604, 0.0032011112198233604, 0.0032011112198233604, 0.0032011112198233604, 0.0032011112198233604]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0032011112198233604

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00320111
Iteration 2/1000 | Loss: 0.00321252
Iteration 3/1000 | Loss: 0.00254701
Iteration 4/1000 | Loss: 0.00133739
Iteration 5/1000 | Loss: 0.00058248
Iteration 6/1000 | Loss: 0.00076220
Iteration 7/1000 | Loss: 0.00060146
Iteration 8/1000 | Loss: 0.00079982
Iteration 9/1000 | Loss: 0.00060508
Iteration 10/1000 | Loss: 0.00086075
Iteration 11/1000 | Loss: 0.00029820
Iteration 12/1000 | Loss: 0.00027688
Iteration 13/1000 | Loss: 0.00042794
Iteration 14/1000 | Loss: 0.00034364
Iteration 15/1000 | Loss: 0.00129412
Iteration 16/1000 | Loss: 0.00081159
Iteration 17/1000 | Loss: 0.00035292
Iteration 18/1000 | Loss: 0.00039957
Iteration 19/1000 | Loss: 0.00027908
Iteration 20/1000 | Loss: 0.00037439
Iteration 21/1000 | Loss: 0.00052912
Iteration 22/1000 | Loss: 0.00066098
Iteration 23/1000 | Loss: 0.00030603
Iteration 24/1000 | Loss: 0.00083280
Iteration 25/1000 | Loss: 0.00054702
Iteration 26/1000 | Loss: 0.00085739
Iteration 27/1000 | Loss: 0.00041115
Iteration 28/1000 | Loss: 0.00122719
Iteration 29/1000 | Loss: 0.00021650
Iteration 30/1000 | Loss: 0.00036295
Iteration 31/1000 | Loss: 0.00037424
Iteration 32/1000 | Loss: 0.00022375
Iteration 33/1000 | Loss: 0.00044971
Iteration 34/1000 | Loss: 0.00024314
Iteration 35/1000 | Loss: 0.00033577
Iteration 36/1000 | Loss: 0.00073927
Iteration 37/1000 | Loss: 0.00033974
Iteration 38/1000 | Loss: 0.00019915
Iteration 39/1000 | Loss: 0.00039977
Iteration 40/1000 | Loss: 0.00022239
Iteration 41/1000 | Loss: 0.00030693
Iteration 42/1000 | Loss: 0.00021398
Iteration 43/1000 | Loss: 0.00017204
Iteration 44/1000 | Loss: 0.00051679
Iteration 45/1000 | Loss: 0.00096576
Iteration 46/1000 | Loss: 0.00205124
Iteration 47/1000 | Loss: 0.00086048
Iteration 48/1000 | Loss: 0.00087642
Iteration 49/1000 | Loss: 0.00016089
Iteration 50/1000 | Loss: 0.00031719
Iteration 51/1000 | Loss: 0.00051934
Iteration 52/1000 | Loss: 0.00020646
Iteration 53/1000 | Loss: 0.00056544
Iteration 54/1000 | Loss: 0.00053377
Iteration 55/1000 | Loss: 0.00376653
Iteration 56/1000 | Loss: 0.00040715
Iteration 57/1000 | Loss: 0.00079615
Iteration 58/1000 | Loss: 0.00014850
Iteration 59/1000 | Loss: 0.00016656
Iteration 60/1000 | Loss: 0.00032470
Iteration 61/1000 | Loss: 0.00035775
Iteration 62/1000 | Loss: 0.00015257
Iteration 63/1000 | Loss: 0.00038489
Iteration 64/1000 | Loss: 0.00256823
Iteration 65/1000 | Loss: 0.00025447
Iteration 66/1000 | Loss: 0.00056117
Iteration 67/1000 | Loss: 0.00013674
Iteration 68/1000 | Loss: 0.00013054
Iteration 69/1000 | Loss: 0.00015262
Iteration 70/1000 | Loss: 0.00012953
Iteration 71/1000 | Loss: 0.00097658
Iteration 72/1000 | Loss: 0.00054140
Iteration 73/1000 | Loss: 0.00017139
Iteration 74/1000 | Loss: 0.00102741
Iteration 75/1000 | Loss: 0.00031162
Iteration 76/1000 | Loss: 0.00026361
Iteration 77/1000 | Loss: 0.00039285
Iteration 78/1000 | Loss: 0.00021796
Iteration 79/1000 | Loss: 0.00019722
Iteration 80/1000 | Loss: 0.00016113
Iteration 81/1000 | Loss: 0.00041968
Iteration 82/1000 | Loss: 0.00024448
Iteration 83/1000 | Loss: 0.00036718
Iteration 84/1000 | Loss: 0.00012603
Iteration 85/1000 | Loss: 0.00035680
Iteration 86/1000 | Loss: 0.00231542
Iteration 87/1000 | Loss: 0.00029875
Iteration 88/1000 | Loss: 0.00023306
Iteration 89/1000 | Loss: 0.00017490
Iteration 90/1000 | Loss: 0.00022152
Iteration 91/1000 | Loss: 0.00023028
Iteration 92/1000 | Loss: 0.00021417
Iteration 93/1000 | Loss: 0.00009043
Iteration 94/1000 | Loss: 0.00017608
Iteration 95/1000 | Loss: 0.00010039
Iteration 96/1000 | Loss: 0.00048831
Iteration 97/1000 | Loss: 0.00015591
Iteration 98/1000 | Loss: 0.00012056
Iteration 99/1000 | Loss: 0.00009487
Iteration 100/1000 | Loss: 0.00008286
Iteration 101/1000 | Loss: 0.00039642
Iteration 102/1000 | Loss: 0.00016599
Iteration 103/1000 | Loss: 0.00011551
Iteration 104/1000 | Loss: 0.00034933
Iteration 105/1000 | Loss: 0.00146353
Iteration 106/1000 | Loss: 0.00176473
Iteration 107/1000 | Loss: 0.00122867
Iteration 108/1000 | Loss: 0.00113795
Iteration 109/1000 | Loss: 0.00019369
Iteration 110/1000 | Loss: 0.00054734
Iteration 111/1000 | Loss: 0.00021011
Iteration 112/1000 | Loss: 0.00014443
Iteration 113/1000 | Loss: 0.00016006
Iteration 114/1000 | Loss: 0.00178819
Iteration 115/1000 | Loss: 0.00009899
Iteration 116/1000 | Loss: 0.00013256
Iteration 117/1000 | Loss: 0.00011116
Iteration 118/1000 | Loss: 0.00031703
Iteration 119/1000 | Loss: 0.00043213
Iteration 120/1000 | Loss: 0.00028392
Iteration 121/1000 | Loss: 0.00031785
Iteration 122/1000 | Loss: 0.00013669
Iteration 123/1000 | Loss: 0.00024869
Iteration 124/1000 | Loss: 0.00104282
Iteration 125/1000 | Loss: 0.00033776
Iteration 126/1000 | Loss: 0.00029164
Iteration 127/1000 | Loss: 0.00016606
Iteration 128/1000 | Loss: 0.00018972
Iteration 129/1000 | Loss: 0.00049660
Iteration 130/1000 | Loss: 0.00009994
Iteration 131/1000 | Loss: 0.00009116
Iteration 132/1000 | Loss: 0.00010751
Iteration 133/1000 | Loss: 0.00011089
Iteration 134/1000 | Loss: 0.00019821
Iteration 135/1000 | Loss: 0.00007940
Iteration 136/1000 | Loss: 0.00009336
Iteration 137/1000 | Loss: 0.00043447
Iteration 138/1000 | Loss: 0.00011583
Iteration 139/1000 | Loss: 0.00008018
Iteration 140/1000 | Loss: 0.00033396
Iteration 141/1000 | Loss: 0.00015857
Iteration 142/1000 | Loss: 0.00007163
Iteration 143/1000 | Loss: 0.00017029
Iteration 144/1000 | Loss: 0.00057612
Iteration 145/1000 | Loss: 0.00007945
Iteration 146/1000 | Loss: 0.00026653
Iteration 147/1000 | Loss: 0.00025182
Iteration 148/1000 | Loss: 0.00020083
Iteration 149/1000 | Loss: 0.00020125
Iteration 150/1000 | Loss: 0.00061187
Iteration 151/1000 | Loss: 0.00026140
Iteration 152/1000 | Loss: 0.00042342
Iteration 153/1000 | Loss: 0.00019568
Iteration 154/1000 | Loss: 0.00014944
Iteration 155/1000 | Loss: 0.00015534
Iteration 156/1000 | Loss: 0.00020324
Iteration 157/1000 | Loss: 0.00007705
Iteration 158/1000 | Loss: 0.00004184
Iteration 159/1000 | Loss: 0.00084436
Iteration 160/1000 | Loss: 0.00009860
Iteration 161/1000 | Loss: 0.00006745
Iteration 162/1000 | Loss: 0.00005633
Iteration 163/1000 | Loss: 0.00003629
Iteration 164/1000 | Loss: 0.00008880
Iteration 165/1000 | Loss: 0.00051094
Iteration 166/1000 | Loss: 0.00020604
Iteration 167/1000 | Loss: 0.00006536
Iteration 168/1000 | Loss: 0.00005311
Iteration 169/1000 | Loss: 0.00041726
Iteration 170/1000 | Loss: 0.00129841
Iteration 171/1000 | Loss: 0.00019395
Iteration 172/1000 | Loss: 0.00046345
Iteration 173/1000 | Loss: 0.00004163
Iteration 174/1000 | Loss: 0.00004935
Iteration 175/1000 | Loss: 0.00003072
Iteration 176/1000 | Loss: 0.00012788
Iteration 177/1000 | Loss: 0.00006274
Iteration 178/1000 | Loss: 0.00005092
Iteration 179/1000 | Loss: 0.00003579
Iteration 180/1000 | Loss: 0.00019277
Iteration 181/1000 | Loss: 0.00041305
Iteration 182/1000 | Loss: 0.00153258
Iteration 183/1000 | Loss: 0.00065976
Iteration 184/1000 | Loss: 0.00023957
Iteration 185/1000 | Loss: 0.00017700
Iteration 186/1000 | Loss: 0.00007284
Iteration 187/1000 | Loss: 0.00002854
Iteration 188/1000 | Loss: 0.00003659
Iteration 189/1000 | Loss: 0.00003252
Iteration 190/1000 | Loss: 0.00002913
Iteration 191/1000 | Loss: 0.00002885
Iteration 192/1000 | Loss: 0.00025328
Iteration 193/1000 | Loss: 0.00005408
Iteration 194/1000 | Loss: 0.00003748
Iteration 195/1000 | Loss: 0.00003856
Iteration 196/1000 | Loss: 0.00003514
Iteration 197/1000 | Loss: 0.00019838
Iteration 198/1000 | Loss: 0.00003046
Iteration 199/1000 | Loss: 0.00014429
Iteration 200/1000 | Loss: 0.00019604
Iteration 201/1000 | Loss: 0.00029996
Iteration 202/1000 | Loss: 0.00021972
Iteration 203/1000 | Loss: 0.00124005
Iteration 204/1000 | Loss: 0.00007435
Iteration 205/1000 | Loss: 0.00020394
Iteration 206/1000 | Loss: 0.00006072
Iteration 207/1000 | Loss: 0.00004468
Iteration 208/1000 | Loss: 0.00005804
Iteration 209/1000 | Loss: 0.00004507
Iteration 210/1000 | Loss: 0.00002704
Iteration 211/1000 | Loss: 0.00004756
Iteration 212/1000 | Loss: 0.00006564
Iteration 213/1000 | Loss: 0.00007287
Iteration 214/1000 | Loss: 0.00002337
Iteration 215/1000 | Loss: 0.00002405
Iteration 216/1000 | Loss: 0.00002711
Iteration 217/1000 | Loss: 0.00002649
Iteration 218/1000 | Loss: 0.00010258
Iteration 219/1000 | Loss: 0.00004800
Iteration 220/1000 | Loss: 0.00003175
Iteration 221/1000 | Loss: 0.00003466
Iteration 222/1000 | Loss: 0.00002129
Iteration 223/1000 | Loss: 0.00006583
Iteration 224/1000 | Loss: 0.00002526
Iteration 225/1000 | Loss: 0.00004685
Iteration 226/1000 | Loss: 0.00002060
Iteration 227/1000 | Loss: 0.00002124
Iteration 228/1000 | Loss: 0.00003143
Iteration 229/1000 | Loss: 0.00009527
Iteration 230/1000 | Loss: 0.00001481
Iteration 231/1000 | Loss: 0.00001387
Iteration 232/1000 | Loss: 0.00004926
Iteration 233/1000 | Loss: 0.00004209
Iteration 234/1000 | Loss: 0.00001251
Iteration 235/1000 | Loss: 0.00002855
Iteration 236/1000 | Loss: 0.00001239
Iteration 237/1000 | Loss: 0.00005743
Iteration 238/1000 | Loss: 0.00001206
Iteration 239/1000 | Loss: 0.00002318
Iteration 240/1000 | Loss: 0.00001185
Iteration 241/1000 | Loss: 0.00007141
Iteration 242/1000 | Loss: 0.00001301
Iteration 243/1000 | Loss: 0.00002652
Iteration 244/1000 | Loss: 0.00001163
Iteration 245/1000 | Loss: 0.00001157
Iteration 246/1000 | Loss: 0.00001151
Iteration 247/1000 | Loss: 0.00001151
Iteration 248/1000 | Loss: 0.00001998
Iteration 249/1000 | Loss: 0.00001546
Iteration 250/1000 | Loss: 0.00001148
Iteration 251/1000 | Loss: 0.00001147
Iteration 252/1000 | Loss: 0.00001147
Iteration 253/1000 | Loss: 0.00001764
Iteration 254/1000 | Loss: 0.00002428
Iteration 255/1000 | Loss: 0.00001145
Iteration 256/1000 | Loss: 0.00001145
Iteration 257/1000 | Loss: 0.00001145
Iteration 258/1000 | Loss: 0.00001145
Iteration 259/1000 | Loss: 0.00001145
Iteration 260/1000 | Loss: 0.00001145
Iteration 261/1000 | Loss: 0.00001145
Iteration 262/1000 | Loss: 0.00001145
Iteration 263/1000 | Loss: 0.00001145
Iteration 264/1000 | Loss: 0.00001144
Iteration 265/1000 | Loss: 0.00001143
Iteration 266/1000 | Loss: 0.00003031
Iteration 267/1000 | Loss: 0.00034477
Iteration 268/1000 | Loss: 0.00002376
Iteration 269/1000 | Loss: 0.00002601
Iteration 270/1000 | Loss: 0.00002527
Iteration 271/1000 | Loss: 0.00003084
Iteration 272/1000 | Loss: 0.00005318
Iteration 273/1000 | Loss: 0.00004449
Iteration 274/1000 | Loss: 0.00009452
Iteration 275/1000 | Loss: 0.00001513
Iteration 276/1000 | Loss: 0.00002618
Iteration 277/1000 | Loss: 0.00002121
Iteration 278/1000 | Loss: 0.00001124
Iteration 279/1000 | Loss: 0.00001122
Iteration 280/1000 | Loss: 0.00001116
Iteration 281/1000 | Loss: 0.00001115
Iteration 282/1000 | Loss: 0.00001115
Iteration 283/1000 | Loss: 0.00003673
Iteration 284/1000 | Loss: 0.00002623
Iteration 285/1000 | Loss: 0.00001711
Iteration 286/1000 | Loss: 0.00002126
Iteration 287/1000 | Loss: 0.00001118
Iteration 288/1000 | Loss: 0.00001369
Iteration 289/1000 | Loss: 0.00001111
Iteration 290/1000 | Loss: 0.00001111
Iteration 291/1000 | Loss: 0.00001110
Iteration 292/1000 | Loss: 0.00001110
Iteration 293/1000 | Loss: 0.00001110
Iteration 294/1000 | Loss: 0.00001110
Iteration 295/1000 | Loss: 0.00001110
Iteration 296/1000 | Loss: 0.00001110
Iteration 297/1000 | Loss: 0.00001110
Iteration 298/1000 | Loss: 0.00001110
Iteration 299/1000 | Loss: 0.00001109
Iteration 300/1000 | Loss: 0.00001109
Iteration 301/1000 | Loss: 0.00001109
Iteration 302/1000 | Loss: 0.00001109
Iteration 303/1000 | Loss: 0.00001109
Iteration 304/1000 | Loss: 0.00001109
Iteration 305/1000 | Loss: 0.00001109
Iteration 306/1000 | Loss: 0.00001109
Iteration 307/1000 | Loss: 0.00001109
Iteration 308/1000 | Loss: 0.00001109
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 308. Stopping optimization.
Last 5 losses: [1.109362710849382e-05, 1.109362710849382e-05, 1.109362710849382e-05, 1.109362710849382e-05, 1.109362710849382e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.109362710849382e-05

Optimization complete. Final v2v error: 2.604975461959839 mm

Highest mean error: 11.086039543151855 mm for frame 123

Lowest mean error: 2.277395486831665 mm for frame 66

Saving results

Total time: 472.2588541507721
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_grace_posed_004/1039/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_grace_posed_004/1039.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_grace_posed_004/1039
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00880251
Iteration 2/25 | Loss: 0.00119553
Iteration 3/25 | Loss: 0.00108764
Iteration 4/25 | Loss: 0.00106895
Iteration 5/25 | Loss: 0.00106217
Iteration 6/25 | Loss: 0.00106031
Iteration 7/25 | Loss: 0.00106022
Iteration 8/25 | Loss: 0.00106022
Iteration 9/25 | Loss: 0.00106022
Iteration 10/25 | Loss: 0.00106022
Iteration 11/25 | Loss: 0.00106022
Iteration 12/25 | Loss: 0.00106022
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0010602184338495135, 0.0010602184338495135, 0.0010602184338495135, 0.0010602184338495135, 0.0010602184338495135]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010602184338495135

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33124483
Iteration 2/25 | Loss: 0.00100369
Iteration 3/25 | Loss: 0.00100366
Iteration 4/25 | Loss: 0.00100366
Iteration 5/25 | Loss: 0.00100366
Iteration 6/25 | Loss: 0.00100365
Iteration 7/25 | Loss: 0.00100365
Iteration 8/25 | Loss: 0.00100365
Iteration 9/25 | Loss: 0.00100365
Iteration 10/25 | Loss: 0.00100365
Iteration 11/25 | Loss: 0.00100365
Iteration 12/25 | Loss: 0.00100365
Iteration 13/25 | Loss: 0.00100365
Iteration 14/25 | Loss: 0.00100365
Iteration 15/25 | Loss: 0.00100365
Iteration 16/25 | Loss: 0.00100365
Iteration 17/25 | Loss: 0.00100365
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0010036536259576678, 0.0010036536259576678, 0.0010036536259576678, 0.0010036536259576678, 0.0010036536259576678]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010036536259576678

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00100365
Iteration 2/1000 | Loss: 0.00004252
Iteration 3/1000 | Loss: 0.00002704
Iteration 4/1000 | Loss: 0.00002151
Iteration 5/1000 | Loss: 0.00001993
Iteration 6/1000 | Loss: 0.00001883
Iteration 7/1000 | Loss: 0.00001808
Iteration 8/1000 | Loss: 0.00001759
Iteration 9/1000 | Loss: 0.00001709
Iteration 10/1000 | Loss: 0.00001684
Iteration 11/1000 | Loss: 0.00001674
Iteration 12/1000 | Loss: 0.00001666
Iteration 13/1000 | Loss: 0.00001665
Iteration 14/1000 | Loss: 0.00001663
Iteration 15/1000 | Loss: 0.00001658
Iteration 16/1000 | Loss: 0.00001653
Iteration 17/1000 | Loss: 0.00001652
Iteration 18/1000 | Loss: 0.00001651
Iteration 19/1000 | Loss: 0.00001650
Iteration 20/1000 | Loss: 0.00001650
Iteration 21/1000 | Loss: 0.00001649
Iteration 22/1000 | Loss: 0.00001645
Iteration 23/1000 | Loss: 0.00001642
Iteration 24/1000 | Loss: 0.00001641
Iteration 25/1000 | Loss: 0.00001635
Iteration 26/1000 | Loss: 0.00001632
Iteration 27/1000 | Loss: 0.00001632
Iteration 28/1000 | Loss: 0.00001632
Iteration 29/1000 | Loss: 0.00001631
Iteration 30/1000 | Loss: 0.00001630
Iteration 31/1000 | Loss: 0.00001630
Iteration 32/1000 | Loss: 0.00001630
Iteration 33/1000 | Loss: 0.00001628
Iteration 34/1000 | Loss: 0.00001628
Iteration 35/1000 | Loss: 0.00001627
Iteration 36/1000 | Loss: 0.00001627
Iteration 37/1000 | Loss: 0.00001627
Iteration 38/1000 | Loss: 0.00001626
Iteration 39/1000 | Loss: 0.00001626
Iteration 40/1000 | Loss: 0.00001625
Iteration 41/1000 | Loss: 0.00001621
Iteration 42/1000 | Loss: 0.00001621
Iteration 43/1000 | Loss: 0.00001621
Iteration 44/1000 | Loss: 0.00001621
Iteration 45/1000 | Loss: 0.00001621
Iteration 46/1000 | Loss: 0.00001619
Iteration 47/1000 | Loss: 0.00001619
Iteration 48/1000 | Loss: 0.00001617
Iteration 49/1000 | Loss: 0.00001617
Iteration 50/1000 | Loss: 0.00001617
Iteration 51/1000 | Loss: 0.00001617
Iteration 52/1000 | Loss: 0.00001617
Iteration 53/1000 | Loss: 0.00001617
Iteration 54/1000 | Loss: 0.00001617
Iteration 55/1000 | Loss: 0.00001617
Iteration 56/1000 | Loss: 0.00001617
Iteration 57/1000 | Loss: 0.00001616
Iteration 58/1000 | Loss: 0.00001616
Iteration 59/1000 | Loss: 0.00001616
Iteration 60/1000 | Loss: 0.00001616
Iteration 61/1000 | Loss: 0.00001616
Iteration 62/1000 | Loss: 0.00001616
Iteration 63/1000 | Loss: 0.00001616
Iteration 64/1000 | Loss: 0.00001615
Iteration 65/1000 | Loss: 0.00001615
Iteration 66/1000 | Loss: 0.00001615
Iteration 67/1000 | Loss: 0.00001615
Iteration 68/1000 | Loss: 0.00001615
Iteration 69/1000 | Loss: 0.00001615
Iteration 70/1000 | Loss: 0.00001614
Iteration 71/1000 | Loss: 0.00001614
Iteration 72/1000 | Loss: 0.00001614
Iteration 73/1000 | Loss: 0.00001614
Iteration 74/1000 | Loss: 0.00001614
Iteration 75/1000 | Loss: 0.00001614
Iteration 76/1000 | Loss: 0.00001614
Iteration 77/1000 | Loss: 0.00001614
Iteration 78/1000 | Loss: 0.00001613
Iteration 79/1000 | Loss: 0.00001613
Iteration 80/1000 | Loss: 0.00001613
Iteration 81/1000 | Loss: 0.00001613
Iteration 82/1000 | Loss: 0.00001613
Iteration 83/1000 | Loss: 0.00001613
Iteration 84/1000 | Loss: 0.00001613
Iteration 85/1000 | Loss: 0.00001613
Iteration 86/1000 | Loss: 0.00001613
Iteration 87/1000 | Loss: 0.00001613
Iteration 88/1000 | Loss: 0.00001613
Iteration 89/1000 | Loss: 0.00001613
Iteration 90/1000 | Loss: 0.00001613
Iteration 91/1000 | Loss: 0.00001612
Iteration 92/1000 | Loss: 0.00001612
Iteration 93/1000 | Loss: 0.00001612
Iteration 94/1000 | Loss: 0.00001612
Iteration 95/1000 | Loss: 0.00001612
Iteration 96/1000 | Loss: 0.00001612
Iteration 97/1000 | Loss: 0.00001612
Iteration 98/1000 | Loss: 0.00001612
Iteration 99/1000 | Loss: 0.00001612
Iteration 100/1000 | Loss: 0.00001612
Iteration 101/1000 | Loss: 0.00001612
Iteration 102/1000 | Loss: 0.00001612
Iteration 103/1000 | Loss: 0.00001612
Iteration 104/1000 | Loss: 0.00001612
Iteration 105/1000 | Loss: 0.00001612
Iteration 106/1000 | Loss: 0.00001612
Iteration 107/1000 | Loss: 0.00001612
Iteration 108/1000 | Loss: 0.00001612
Iteration 109/1000 | Loss: 0.00001612
Iteration 110/1000 | Loss: 0.00001612
Iteration 111/1000 | Loss: 0.00001612
Iteration 112/1000 | Loss: 0.00001612
Iteration 113/1000 | Loss: 0.00001612
Iteration 114/1000 | Loss: 0.00001612
Iteration 115/1000 | Loss: 0.00001612
Iteration 116/1000 | Loss: 0.00001612
Iteration 117/1000 | Loss: 0.00001612
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 117. Stopping optimization.
Last 5 losses: [1.6120959116960876e-05, 1.6120959116960876e-05, 1.6120959116960876e-05, 1.6120959116960876e-05, 1.6120959116960876e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6120959116960876e-05

Optimization complete. Final v2v error: 3.328503370285034 mm

Highest mean error: 5.0537285804748535 mm for frame 67

Lowest mean error: 2.725893497467041 mm for frame 98

Saving results

Total time: 34.30227971076965
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_grace_posed_004/1056/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_grace_posed_004/1056.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_grace_posed_004/1056
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00403262
Iteration 2/25 | Loss: 0.00114670
Iteration 3/25 | Loss: 0.00105438
Iteration 4/25 | Loss: 0.00104488
Iteration 5/25 | Loss: 0.00104150
Iteration 6/25 | Loss: 0.00104099
Iteration 7/25 | Loss: 0.00104099
Iteration 8/25 | Loss: 0.00104099
Iteration 9/25 | Loss: 0.00104099
Iteration 10/25 | Loss: 0.00104099
Iteration 11/25 | Loss: 0.00104099
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001040991279296577, 0.001040991279296577, 0.001040991279296577, 0.001040991279296577, 0.001040991279296577]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001040991279296577

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.31158769
Iteration 2/25 | Loss: 0.00101256
Iteration 3/25 | Loss: 0.00101256
Iteration 4/25 | Loss: 0.00101256
Iteration 5/25 | Loss: 0.00101255
Iteration 6/25 | Loss: 0.00101255
Iteration 7/25 | Loss: 0.00101255
Iteration 8/25 | Loss: 0.00101255
Iteration 9/25 | Loss: 0.00101255
Iteration 10/25 | Loss: 0.00101255
Iteration 11/25 | Loss: 0.00101255
Iteration 12/25 | Loss: 0.00101255
Iteration 13/25 | Loss: 0.00101255
Iteration 14/25 | Loss: 0.00101255
Iteration 15/25 | Loss: 0.00101255
Iteration 16/25 | Loss: 0.00101255
Iteration 17/25 | Loss: 0.00101255
Iteration 18/25 | Loss: 0.00101255
Iteration 19/25 | Loss: 0.00101255
Iteration 20/25 | Loss: 0.00101255
Iteration 21/25 | Loss: 0.00101255
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.001012552878819406, 0.001012552878819406, 0.001012552878819406, 0.001012552878819406, 0.001012552878819406]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001012552878819406

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00101255
Iteration 2/1000 | Loss: 0.00001989
Iteration 3/1000 | Loss: 0.00001227
Iteration 4/1000 | Loss: 0.00001108
Iteration 5/1000 | Loss: 0.00001044
Iteration 6/1000 | Loss: 0.00001005
Iteration 7/1000 | Loss: 0.00000975
Iteration 8/1000 | Loss: 0.00000957
Iteration 9/1000 | Loss: 0.00000957
Iteration 10/1000 | Loss: 0.00000949
Iteration 11/1000 | Loss: 0.00000938
Iteration 12/1000 | Loss: 0.00000933
Iteration 13/1000 | Loss: 0.00000931
Iteration 14/1000 | Loss: 0.00000929
Iteration 15/1000 | Loss: 0.00000928
Iteration 16/1000 | Loss: 0.00000927
Iteration 17/1000 | Loss: 0.00000923
Iteration 18/1000 | Loss: 0.00000921
Iteration 19/1000 | Loss: 0.00000921
Iteration 20/1000 | Loss: 0.00000920
Iteration 21/1000 | Loss: 0.00000920
Iteration 22/1000 | Loss: 0.00000918
Iteration 23/1000 | Loss: 0.00000918
Iteration 24/1000 | Loss: 0.00000918
Iteration 25/1000 | Loss: 0.00000918
Iteration 26/1000 | Loss: 0.00000917
Iteration 27/1000 | Loss: 0.00000917
Iteration 28/1000 | Loss: 0.00000917
Iteration 29/1000 | Loss: 0.00000917
Iteration 30/1000 | Loss: 0.00000917
Iteration 31/1000 | Loss: 0.00000917
Iteration 32/1000 | Loss: 0.00000917
Iteration 33/1000 | Loss: 0.00000916
Iteration 34/1000 | Loss: 0.00000916
Iteration 35/1000 | Loss: 0.00000915
Iteration 36/1000 | Loss: 0.00000912
Iteration 37/1000 | Loss: 0.00000912
Iteration 38/1000 | Loss: 0.00000911
Iteration 39/1000 | Loss: 0.00000910
Iteration 40/1000 | Loss: 0.00000909
Iteration 41/1000 | Loss: 0.00000909
Iteration 42/1000 | Loss: 0.00000908
Iteration 43/1000 | Loss: 0.00000907
Iteration 44/1000 | Loss: 0.00000907
Iteration 45/1000 | Loss: 0.00000906
Iteration 46/1000 | Loss: 0.00000905
Iteration 47/1000 | Loss: 0.00000905
Iteration 48/1000 | Loss: 0.00000904
Iteration 49/1000 | Loss: 0.00000904
Iteration 50/1000 | Loss: 0.00000904
Iteration 51/1000 | Loss: 0.00000903
Iteration 52/1000 | Loss: 0.00000903
Iteration 53/1000 | Loss: 0.00000903
Iteration 54/1000 | Loss: 0.00000902
Iteration 55/1000 | Loss: 0.00000901
Iteration 56/1000 | Loss: 0.00000900
Iteration 57/1000 | Loss: 0.00000900
Iteration 58/1000 | Loss: 0.00000900
Iteration 59/1000 | Loss: 0.00000899
Iteration 60/1000 | Loss: 0.00000899
Iteration 61/1000 | Loss: 0.00000899
Iteration 62/1000 | Loss: 0.00000899
Iteration 63/1000 | Loss: 0.00000898
Iteration 64/1000 | Loss: 0.00000898
Iteration 65/1000 | Loss: 0.00000898
Iteration 66/1000 | Loss: 0.00000898
Iteration 67/1000 | Loss: 0.00000897
Iteration 68/1000 | Loss: 0.00000897
Iteration 69/1000 | Loss: 0.00000897
Iteration 70/1000 | Loss: 0.00000897
Iteration 71/1000 | Loss: 0.00000897
Iteration 72/1000 | Loss: 0.00000897
Iteration 73/1000 | Loss: 0.00000897
Iteration 74/1000 | Loss: 0.00000897
Iteration 75/1000 | Loss: 0.00000896
Iteration 76/1000 | Loss: 0.00000896
Iteration 77/1000 | Loss: 0.00000896
Iteration 78/1000 | Loss: 0.00000896
Iteration 79/1000 | Loss: 0.00000896
Iteration 80/1000 | Loss: 0.00000895
Iteration 81/1000 | Loss: 0.00000895
Iteration 82/1000 | Loss: 0.00000895
Iteration 83/1000 | Loss: 0.00000894
Iteration 84/1000 | Loss: 0.00000894
Iteration 85/1000 | Loss: 0.00000894
Iteration 86/1000 | Loss: 0.00000894
Iteration 87/1000 | Loss: 0.00000894
Iteration 88/1000 | Loss: 0.00000894
Iteration 89/1000 | Loss: 0.00000894
Iteration 90/1000 | Loss: 0.00000894
Iteration 91/1000 | Loss: 0.00000894
Iteration 92/1000 | Loss: 0.00000894
Iteration 93/1000 | Loss: 0.00000894
Iteration 94/1000 | Loss: 0.00000894
Iteration 95/1000 | Loss: 0.00000894
Iteration 96/1000 | Loss: 0.00000894
Iteration 97/1000 | Loss: 0.00000894
Iteration 98/1000 | Loss: 0.00000894
Iteration 99/1000 | Loss: 0.00000894
Iteration 100/1000 | Loss: 0.00000893
Iteration 101/1000 | Loss: 0.00000893
Iteration 102/1000 | Loss: 0.00000893
Iteration 103/1000 | Loss: 0.00000893
Iteration 104/1000 | Loss: 0.00000893
Iteration 105/1000 | Loss: 0.00000892
Iteration 106/1000 | Loss: 0.00000892
Iteration 107/1000 | Loss: 0.00000892
Iteration 108/1000 | Loss: 0.00000892
Iteration 109/1000 | Loss: 0.00000892
Iteration 110/1000 | Loss: 0.00000891
Iteration 111/1000 | Loss: 0.00000891
Iteration 112/1000 | Loss: 0.00000891
Iteration 113/1000 | Loss: 0.00000891
Iteration 114/1000 | Loss: 0.00000891
Iteration 115/1000 | Loss: 0.00000891
Iteration 116/1000 | Loss: 0.00000891
Iteration 117/1000 | Loss: 0.00000891
Iteration 118/1000 | Loss: 0.00000890
Iteration 119/1000 | Loss: 0.00000889
Iteration 120/1000 | Loss: 0.00000889
Iteration 121/1000 | Loss: 0.00000889
Iteration 122/1000 | Loss: 0.00000889
Iteration 123/1000 | Loss: 0.00000889
Iteration 124/1000 | Loss: 0.00000888
Iteration 125/1000 | Loss: 0.00000888
Iteration 126/1000 | Loss: 0.00000888
Iteration 127/1000 | Loss: 0.00000888
Iteration 128/1000 | Loss: 0.00000888
Iteration 129/1000 | Loss: 0.00000888
Iteration 130/1000 | Loss: 0.00000888
Iteration 131/1000 | Loss: 0.00000888
Iteration 132/1000 | Loss: 0.00000888
Iteration 133/1000 | Loss: 0.00000888
Iteration 134/1000 | Loss: 0.00000887
Iteration 135/1000 | Loss: 0.00000887
Iteration 136/1000 | Loss: 0.00000887
Iteration 137/1000 | Loss: 0.00000887
Iteration 138/1000 | Loss: 0.00000886
Iteration 139/1000 | Loss: 0.00000886
Iteration 140/1000 | Loss: 0.00000886
Iteration 141/1000 | Loss: 0.00000886
Iteration 142/1000 | Loss: 0.00000886
Iteration 143/1000 | Loss: 0.00000886
Iteration 144/1000 | Loss: 0.00000886
Iteration 145/1000 | Loss: 0.00000886
Iteration 146/1000 | Loss: 0.00000886
Iteration 147/1000 | Loss: 0.00000886
Iteration 148/1000 | Loss: 0.00000886
Iteration 149/1000 | Loss: 0.00000886
Iteration 150/1000 | Loss: 0.00000886
Iteration 151/1000 | Loss: 0.00000886
Iteration 152/1000 | Loss: 0.00000886
Iteration 153/1000 | Loss: 0.00000886
Iteration 154/1000 | Loss: 0.00000886
Iteration 155/1000 | Loss: 0.00000886
Iteration 156/1000 | Loss: 0.00000886
Iteration 157/1000 | Loss: 0.00000886
Iteration 158/1000 | Loss: 0.00000886
Iteration 159/1000 | Loss: 0.00000886
Iteration 160/1000 | Loss: 0.00000886
Iteration 161/1000 | Loss: 0.00000886
Iteration 162/1000 | Loss: 0.00000886
Iteration 163/1000 | Loss: 0.00000886
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 163. Stopping optimization.
Last 5 losses: [8.855462510837242e-06, 8.855462510837242e-06, 8.855462510837242e-06, 8.855462510837242e-06, 8.855462510837242e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.855462510837242e-06

Optimization complete. Final v2v error: 2.541940927505493 mm

Highest mean error: 2.913684606552124 mm for frame 5

Lowest mean error: 2.37740159034729 mm for frame 115

Saving results

Total time: 33.29456329345703
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_grace_posed_004/1046/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_grace_posed_004/1046.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_grace_posed_004/1046
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00676100
Iteration 2/25 | Loss: 0.00152954
Iteration 3/25 | Loss: 0.00131328
Iteration 4/25 | Loss: 0.00129906
Iteration 5/25 | Loss: 0.00129731
Iteration 6/25 | Loss: 0.00129730
Iteration 7/25 | Loss: 0.00129730
Iteration 8/25 | Loss: 0.00129730
Iteration 9/25 | Loss: 0.00129730
Iteration 10/25 | Loss: 0.00129730
Iteration 11/25 | Loss: 0.00129730
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001297298353165388, 0.001297298353165388, 0.001297298353165388, 0.001297298353165388, 0.001297298353165388]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001297298353165388

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.60758090
Iteration 2/25 | Loss: 0.00068421
Iteration 3/25 | Loss: 0.00068421
Iteration 4/25 | Loss: 0.00068420
Iteration 5/25 | Loss: 0.00068420
Iteration 6/25 | Loss: 0.00068420
Iteration 7/25 | Loss: 0.00068420
Iteration 8/25 | Loss: 0.00068420
Iteration 9/25 | Loss: 0.00068420
Iteration 10/25 | Loss: 0.00068420
Iteration 11/25 | Loss: 0.00068420
Iteration 12/25 | Loss: 0.00068420
Iteration 13/25 | Loss: 0.00068420
Iteration 14/25 | Loss: 0.00068420
Iteration 15/25 | Loss: 0.00068420
Iteration 16/25 | Loss: 0.00068420
Iteration 17/25 | Loss: 0.00068420
Iteration 18/25 | Loss: 0.00068420
Iteration 19/25 | Loss: 0.00068420
Iteration 20/25 | Loss: 0.00068420
Iteration 21/25 | Loss: 0.00068420
Iteration 22/25 | Loss: 0.00068420
Iteration 23/25 | Loss: 0.00068420
Iteration 24/25 | Loss: 0.00068420
Iteration 25/25 | Loss: 0.00068420
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0006842026486992836, 0.0006842026486992836, 0.0006842026486992836, 0.0006842026486992836, 0.0006842026486992836]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006842026486992836

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00068420
Iteration 2/1000 | Loss: 0.00007355
Iteration 3/1000 | Loss: 0.00004603
Iteration 4/1000 | Loss: 0.00003884
Iteration 5/1000 | Loss: 0.00003685
Iteration 6/1000 | Loss: 0.00003568
Iteration 7/1000 | Loss: 0.00003492
Iteration 8/1000 | Loss: 0.00003405
Iteration 9/1000 | Loss: 0.00003324
Iteration 10/1000 | Loss: 0.00003273
Iteration 11/1000 | Loss: 0.00003227
Iteration 12/1000 | Loss: 0.00003195
Iteration 13/1000 | Loss: 0.00003170
Iteration 14/1000 | Loss: 0.00003144
Iteration 15/1000 | Loss: 0.00003124
Iteration 16/1000 | Loss: 0.00003105
Iteration 17/1000 | Loss: 0.00003102
Iteration 18/1000 | Loss: 0.00003091
Iteration 19/1000 | Loss: 0.00003091
Iteration 20/1000 | Loss: 0.00003084
Iteration 21/1000 | Loss: 0.00003066
Iteration 22/1000 | Loss: 0.00003050
Iteration 23/1000 | Loss: 0.00003034
Iteration 24/1000 | Loss: 0.00003032
Iteration 25/1000 | Loss: 0.00003028
Iteration 26/1000 | Loss: 0.00003023
Iteration 27/1000 | Loss: 0.00003022
Iteration 28/1000 | Loss: 0.00003021
Iteration 29/1000 | Loss: 0.00003020
Iteration 30/1000 | Loss: 0.00003018
Iteration 31/1000 | Loss: 0.00003014
Iteration 32/1000 | Loss: 0.00003011
Iteration 33/1000 | Loss: 0.00003009
Iteration 34/1000 | Loss: 0.00003009
Iteration 35/1000 | Loss: 0.00003009
Iteration 36/1000 | Loss: 0.00003008
Iteration 37/1000 | Loss: 0.00003008
Iteration 38/1000 | Loss: 0.00003008
Iteration 39/1000 | Loss: 0.00003008
Iteration 40/1000 | Loss: 0.00003008
Iteration 41/1000 | Loss: 0.00003007
Iteration 42/1000 | Loss: 0.00003007
Iteration 43/1000 | Loss: 0.00003003
Iteration 44/1000 | Loss: 0.00003002
Iteration 45/1000 | Loss: 0.00003000
Iteration 46/1000 | Loss: 0.00002999
Iteration 47/1000 | Loss: 0.00002999
Iteration 48/1000 | Loss: 0.00002999
Iteration 49/1000 | Loss: 0.00002999
Iteration 50/1000 | Loss: 0.00002999
Iteration 51/1000 | Loss: 0.00002999
Iteration 52/1000 | Loss: 0.00002999
Iteration 53/1000 | Loss: 0.00002999
Iteration 54/1000 | Loss: 0.00002998
Iteration 55/1000 | Loss: 0.00002998
Iteration 56/1000 | Loss: 0.00002998
Iteration 57/1000 | Loss: 0.00002998
Iteration 58/1000 | Loss: 0.00002998
Iteration 59/1000 | Loss: 0.00002998
Iteration 60/1000 | Loss: 0.00002997
Iteration 61/1000 | Loss: 0.00002997
Iteration 62/1000 | Loss: 0.00002997
Iteration 63/1000 | Loss: 0.00002997
Iteration 64/1000 | Loss: 0.00002996
Iteration 65/1000 | Loss: 0.00002995
Iteration 66/1000 | Loss: 0.00002995
Iteration 67/1000 | Loss: 0.00002995
Iteration 68/1000 | Loss: 0.00002994
Iteration 69/1000 | Loss: 0.00002994
Iteration 70/1000 | Loss: 0.00002994
Iteration 71/1000 | Loss: 0.00002993
Iteration 72/1000 | Loss: 0.00002993
Iteration 73/1000 | Loss: 0.00002993
Iteration 74/1000 | Loss: 0.00002993
Iteration 75/1000 | Loss: 0.00002993
Iteration 76/1000 | Loss: 0.00002993
Iteration 77/1000 | Loss: 0.00002993
Iteration 78/1000 | Loss: 0.00002992
Iteration 79/1000 | Loss: 0.00002992
Iteration 80/1000 | Loss: 0.00002992
Iteration 81/1000 | Loss: 0.00002992
Iteration 82/1000 | Loss: 0.00002992
Iteration 83/1000 | Loss: 0.00002991
Iteration 84/1000 | Loss: 0.00002991
Iteration 85/1000 | Loss: 0.00002991
Iteration 86/1000 | Loss: 0.00002991
Iteration 87/1000 | Loss: 0.00002991
Iteration 88/1000 | Loss: 0.00002991
Iteration 89/1000 | Loss: 0.00002991
Iteration 90/1000 | Loss: 0.00002991
Iteration 91/1000 | Loss: 0.00002991
Iteration 92/1000 | Loss: 0.00002991
Iteration 93/1000 | Loss: 0.00002991
Iteration 94/1000 | Loss: 0.00002991
Iteration 95/1000 | Loss: 0.00002991
Iteration 96/1000 | Loss: 0.00002991
Iteration 97/1000 | Loss: 0.00002991
Iteration 98/1000 | Loss: 0.00002991
Iteration 99/1000 | Loss: 0.00002991
Iteration 100/1000 | Loss: 0.00002991
Iteration 101/1000 | Loss: 0.00002991
Iteration 102/1000 | Loss: 0.00002991
Iteration 103/1000 | Loss: 0.00002991
Iteration 104/1000 | Loss: 0.00002991
Iteration 105/1000 | Loss: 0.00002991
Iteration 106/1000 | Loss: 0.00002991
Iteration 107/1000 | Loss: 0.00002991
Iteration 108/1000 | Loss: 0.00002991
Iteration 109/1000 | Loss: 0.00002991
Iteration 110/1000 | Loss: 0.00002991
Iteration 111/1000 | Loss: 0.00002991
Iteration 112/1000 | Loss: 0.00002991
Iteration 113/1000 | Loss: 0.00002991
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 113. Stopping optimization.
Last 5 losses: [2.9909999284427613e-05, 2.9909999284427613e-05, 2.9909999284427613e-05, 2.9909999284427613e-05, 2.9909999284427613e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.9909999284427613e-05

Optimization complete. Final v2v error: 4.456057071685791 mm

Highest mean error: 4.993866920471191 mm for frame 93

Lowest mean error: 3.974853038787842 mm for frame 107

Saving results

Total time: 46.68823170661926
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_grace_posed_004/1001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_grace_posed_004/1001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_grace_posed_004/1001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00842130
Iteration 2/25 | Loss: 0.00112057
Iteration 3/25 | Loss: 0.00100911
Iteration 4/25 | Loss: 0.00099825
Iteration 5/25 | Loss: 0.00099575
Iteration 6/25 | Loss: 0.00099545
Iteration 7/25 | Loss: 0.00099545
Iteration 8/25 | Loss: 0.00099545
Iteration 9/25 | Loss: 0.00099545
Iteration 10/25 | Loss: 0.00099545
Iteration 11/25 | Loss: 0.00099545
Iteration 12/25 | Loss: 0.00099545
Iteration 13/25 | Loss: 0.00099545
Iteration 14/25 | Loss: 0.00099545
Iteration 15/25 | Loss: 0.00099545
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.000995454378426075, 0.000995454378426075, 0.000995454378426075, 0.000995454378426075, 0.000995454378426075]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000995454378426075

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.30516052
Iteration 2/25 | Loss: 0.00088467
Iteration 3/25 | Loss: 0.00088465
Iteration 4/25 | Loss: 0.00088465
Iteration 5/25 | Loss: 0.00088465
Iteration 6/25 | Loss: 0.00088465
Iteration 7/25 | Loss: 0.00088465
Iteration 8/25 | Loss: 0.00088465
Iteration 9/25 | Loss: 0.00088465
Iteration 10/25 | Loss: 0.00088465
Iteration 11/25 | Loss: 0.00088465
Iteration 12/25 | Loss: 0.00088465
Iteration 13/25 | Loss: 0.00088465
Iteration 14/25 | Loss: 0.00088465
Iteration 15/25 | Loss: 0.00088465
Iteration 16/25 | Loss: 0.00088465
Iteration 17/25 | Loss: 0.00088465
Iteration 18/25 | Loss: 0.00088465
Iteration 19/25 | Loss: 0.00088465
Iteration 20/25 | Loss: 0.00088465
Iteration 21/25 | Loss: 0.00088465
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0008846457349136472, 0.0008846457349136472, 0.0008846457349136472, 0.0008846457349136472, 0.0008846457349136472]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008846457349136472

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00088465
Iteration 2/1000 | Loss: 0.00001670
Iteration 3/1000 | Loss: 0.00001052
Iteration 4/1000 | Loss: 0.00000909
Iteration 5/1000 | Loss: 0.00000829
Iteration 6/1000 | Loss: 0.00000778
Iteration 7/1000 | Loss: 0.00000742
Iteration 8/1000 | Loss: 0.00000728
Iteration 9/1000 | Loss: 0.00000722
Iteration 10/1000 | Loss: 0.00000711
Iteration 11/1000 | Loss: 0.00000696
Iteration 12/1000 | Loss: 0.00000694
Iteration 13/1000 | Loss: 0.00000693
Iteration 14/1000 | Loss: 0.00000692
Iteration 15/1000 | Loss: 0.00000691
Iteration 16/1000 | Loss: 0.00000691
Iteration 17/1000 | Loss: 0.00000690
Iteration 18/1000 | Loss: 0.00000690
Iteration 19/1000 | Loss: 0.00000688
Iteration 20/1000 | Loss: 0.00000685
Iteration 21/1000 | Loss: 0.00000685
Iteration 22/1000 | Loss: 0.00000685
Iteration 23/1000 | Loss: 0.00000685
Iteration 24/1000 | Loss: 0.00000684
Iteration 25/1000 | Loss: 0.00000684
Iteration 26/1000 | Loss: 0.00000684
Iteration 27/1000 | Loss: 0.00000680
Iteration 28/1000 | Loss: 0.00000680
Iteration 29/1000 | Loss: 0.00000680
Iteration 30/1000 | Loss: 0.00000680
Iteration 31/1000 | Loss: 0.00000680
Iteration 32/1000 | Loss: 0.00000679
Iteration 33/1000 | Loss: 0.00000679
Iteration 34/1000 | Loss: 0.00000679
Iteration 35/1000 | Loss: 0.00000679
Iteration 36/1000 | Loss: 0.00000679
Iteration 37/1000 | Loss: 0.00000679
Iteration 38/1000 | Loss: 0.00000679
Iteration 39/1000 | Loss: 0.00000678
Iteration 40/1000 | Loss: 0.00000678
Iteration 41/1000 | Loss: 0.00000676
Iteration 42/1000 | Loss: 0.00000676
Iteration 43/1000 | Loss: 0.00000675
Iteration 44/1000 | Loss: 0.00000675
Iteration 45/1000 | Loss: 0.00000674
Iteration 46/1000 | Loss: 0.00000674
Iteration 47/1000 | Loss: 0.00000674
Iteration 48/1000 | Loss: 0.00000673
Iteration 49/1000 | Loss: 0.00000673
Iteration 50/1000 | Loss: 0.00000672
Iteration 51/1000 | Loss: 0.00000672
Iteration 52/1000 | Loss: 0.00000672
Iteration 53/1000 | Loss: 0.00000671
Iteration 54/1000 | Loss: 0.00000671
Iteration 55/1000 | Loss: 0.00000670
Iteration 56/1000 | Loss: 0.00000670
Iteration 57/1000 | Loss: 0.00000670
Iteration 58/1000 | Loss: 0.00000669
Iteration 59/1000 | Loss: 0.00000669
Iteration 60/1000 | Loss: 0.00000669
Iteration 61/1000 | Loss: 0.00000669
Iteration 62/1000 | Loss: 0.00000668
Iteration 63/1000 | Loss: 0.00000667
Iteration 64/1000 | Loss: 0.00000667
Iteration 65/1000 | Loss: 0.00000667
Iteration 66/1000 | Loss: 0.00000666
Iteration 67/1000 | Loss: 0.00000666
Iteration 68/1000 | Loss: 0.00000666
Iteration 69/1000 | Loss: 0.00000666
Iteration 70/1000 | Loss: 0.00000666
Iteration 71/1000 | Loss: 0.00000665
Iteration 72/1000 | Loss: 0.00000665
Iteration 73/1000 | Loss: 0.00000665
Iteration 74/1000 | Loss: 0.00000665
Iteration 75/1000 | Loss: 0.00000665
Iteration 76/1000 | Loss: 0.00000665
Iteration 77/1000 | Loss: 0.00000664
Iteration 78/1000 | Loss: 0.00000664
Iteration 79/1000 | Loss: 0.00000664
Iteration 80/1000 | Loss: 0.00000664
Iteration 81/1000 | Loss: 0.00000664
Iteration 82/1000 | Loss: 0.00000664
Iteration 83/1000 | Loss: 0.00000664
Iteration 84/1000 | Loss: 0.00000664
Iteration 85/1000 | Loss: 0.00000664
Iteration 86/1000 | Loss: 0.00000663
Iteration 87/1000 | Loss: 0.00000663
Iteration 88/1000 | Loss: 0.00000663
Iteration 89/1000 | Loss: 0.00000663
Iteration 90/1000 | Loss: 0.00000663
Iteration 91/1000 | Loss: 0.00000663
Iteration 92/1000 | Loss: 0.00000663
Iteration 93/1000 | Loss: 0.00000663
Iteration 94/1000 | Loss: 0.00000663
Iteration 95/1000 | Loss: 0.00000663
Iteration 96/1000 | Loss: 0.00000663
Iteration 97/1000 | Loss: 0.00000662
Iteration 98/1000 | Loss: 0.00000662
Iteration 99/1000 | Loss: 0.00000662
Iteration 100/1000 | Loss: 0.00000662
Iteration 101/1000 | Loss: 0.00000662
Iteration 102/1000 | Loss: 0.00000662
Iteration 103/1000 | Loss: 0.00000662
Iteration 104/1000 | Loss: 0.00000661
Iteration 105/1000 | Loss: 0.00000661
Iteration 106/1000 | Loss: 0.00000661
Iteration 107/1000 | Loss: 0.00000661
Iteration 108/1000 | Loss: 0.00000661
Iteration 109/1000 | Loss: 0.00000661
Iteration 110/1000 | Loss: 0.00000661
Iteration 111/1000 | Loss: 0.00000661
Iteration 112/1000 | Loss: 0.00000661
Iteration 113/1000 | Loss: 0.00000660
Iteration 114/1000 | Loss: 0.00000660
Iteration 115/1000 | Loss: 0.00000660
Iteration 116/1000 | Loss: 0.00000660
Iteration 117/1000 | Loss: 0.00000659
Iteration 118/1000 | Loss: 0.00000659
Iteration 119/1000 | Loss: 0.00000659
Iteration 120/1000 | Loss: 0.00000659
Iteration 121/1000 | Loss: 0.00000659
Iteration 122/1000 | Loss: 0.00000659
Iteration 123/1000 | Loss: 0.00000659
Iteration 124/1000 | Loss: 0.00000659
Iteration 125/1000 | Loss: 0.00000659
Iteration 126/1000 | Loss: 0.00000658
Iteration 127/1000 | Loss: 0.00000658
Iteration 128/1000 | Loss: 0.00000658
Iteration 129/1000 | Loss: 0.00000658
Iteration 130/1000 | Loss: 0.00000658
Iteration 131/1000 | Loss: 0.00000658
Iteration 132/1000 | Loss: 0.00000658
Iteration 133/1000 | Loss: 0.00000658
Iteration 134/1000 | Loss: 0.00000658
Iteration 135/1000 | Loss: 0.00000658
Iteration 136/1000 | Loss: 0.00000657
Iteration 137/1000 | Loss: 0.00000657
Iteration 138/1000 | Loss: 0.00000657
Iteration 139/1000 | Loss: 0.00000657
Iteration 140/1000 | Loss: 0.00000657
Iteration 141/1000 | Loss: 0.00000657
Iteration 142/1000 | Loss: 0.00000657
Iteration 143/1000 | Loss: 0.00000657
Iteration 144/1000 | Loss: 0.00000657
Iteration 145/1000 | Loss: 0.00000657
Iteration 146/1000 | Loss: 0.00000657
Iteration 147/1000 | Loss: 0.00000656
Iteration 148/1000 | Loss: 0.00000656
Iteration 149/1000 | Loss: 0.00000656
Iteration 150/1000 | Loss: 0.00000656
Iteration 151/1000 | Loss: 0.00000656
Iteration 152/1000 | Loss: 0.00000656
Iteration 153/1000 | Loss: 0.00000656
Iteration 154/1000 | Loss: 0.00000656
Iteration 155/1000 | Loss: 0.00000655
Iteration 156/1000 | Loss: 0.00000655
Iteration 157/1000 | Loss: 0.00000655
Iteration 158/1000 | Loss: 0.00000655
Iteration 159/1000 | Loss: 0.00000655
Iteration 160/1000 | Loss: 0.00000654
Iteration 161/1000 | Loss: 0.00000654
Iteration 162/1000 | Loss: 0.00000654
Iteration 163/1000 | Loss: 0.00000654
Iteration 164/1000 | Loss: 0.00000654
Iteration 165/1000 | Loss: 0.00000654
Iteration 166/1000 | Loss: 0.00000654
Iteration 167/1000 | Loss: 0.00000654
Iteration 168/1000 | Loss: 0.00000654
Iteration 169/1000 | Loss: 0.00000654
Iteration 170/1000 | Loss: 0.00000653
Iteration 171/1000 | Loss: 0.00000653
Iteration 172/1000 | Loss: 0.00000653
Iteration 173/1000 | Loss: 0.00000653
Iteration 174/1000 | Loss: 0.00000653
Iteration 175/1000 | Loss: 0.00000653
Iteration 176/1000 | Loss: 0.00000653
Iteration 177/1000 | Loss: 0.00000653
Iteration 178/1000 | Loss: 0.00000653
Iteration 179/1000 | Loss: 0.00000653
Iteration 180/1000 | Loss: 0.00000653
Iteration 181/1000 | Loss: 0.00000653
Iteration 182/1000 | Loss: 0.00000653
Iteration 183/1000 | Loss: 0.00000652
Iteration 184/1000 | Loss: 0.00000652
Iteration 185/1000 | Loss: 0.00000652
Iteration 186/1000 | Loss: 0.00000652
Iteration 187/1000 | Loss: 0.00000652
Iteration 188/1000 | Loss: 0.00000651
Iteration 189/1000 | Loss: 0.00000651
Iteration 190/1000 | Loss: 0.00000651
Iteration 191/1000 | Loss: 0.00000651
Iteration 192/1000 | Loss: 0.00000650
Iteration 193/1000 | Loss: 0.00000650
Iteration 194/1000 | Loss: 0.00000650
Iteration 195/1000 | Loss: 0.00000650
Iteration 196/1000 | Loss: 0.00000650
Iteration 197/1000 | Loss: 0.00000650
Iteration 198/1000 | Loss: 0.00000650
Iteration 199/1000 | Loss: 0.00000650
Iteration 200/1000 | Loss: 0.00000650
Iteration 201/1000 | Loss: 0.00000650
Iteration 202/1000 | Loss: 0.00000650
Iteration 203/1000 | Loss: 0.00000650
Iteration 204/1000 | Loss: 0.00000650
Iteration 205/1000 | Loss: 0.00000650
Iteration 206/1000 | Loss: 0.00000650
Iteration 207/1000 | Loss: 0.00000649
Iteration 208/1000 | Loss: 0.00000649
Iteration 209/1000 | Loss: 0.00000649
Iteration 210/1000 | Loss: 0.00000649
Iteration 211/1000 | Loss: 0.00000649
Iteration 212/1000 | Loss: 0.00000649
Iteration 213/1000 | Loss: 0.00000649
Iteration 214/1000 | Loss: 0.00000649
Iteration 215/1000 | Loss: 0.00000649
Iteration 216/1000 | Loss: 0.00000649
Iteration 217/1000 | Loss: 0.00000649
Iteration 218/1000 | Loss: 0.00000649
Iteration 219/1000 | Loss: 0.00000649
Iteration 220/1000 | Loss: 0.00000648
Iteration 221/1000 | Loss: 0.00000648
Iteration 222/1000 | Loss: 0.00000648
Iteration 223/1000 | Loss: 0.00000648
Iteration 224/1000 | Loss: 0.00000648
Iteration 225/1000 | Loss: 0.00000648
Iteration 226/1000 | Loss: 0.00000648
Iteration 227/1000 | Loss: 0.00000648
Iteration 228/1000 | Loss: 0.00000648
Iteration 229/1000 | Loss: 0.00000648
Iteration 230/1000 | Loss: 0.00000648
Iteration 231/1000 | Loss: 0.00000648
Iteration 232/1000 | Loss: 0.00000648
Iteration 233/1000 | Loss: 0.00000648
Iteration 234/1000 | Loss: 0.00000648
Iteration 235/1000 | Loss: 0.00000648
Iteration 236/1000 | Loss: 0.00000648
Iteration 237/1000 | Loss: 0.00000648
Iteration 238/1000 | Loss: 0.00000648
Iteration 239/1000 | Loss: 0.00000648
Iteration 240/1000 | Loss: 0.00000648
Iteration 241/1000 | Loss: 0.00000648
Iteration 242/1000 | Loss: 0.00000648
Iteration 243/1000 | Loss: 0.00000648
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 243. Stopping optimization.
Last 5 losses: [6.482292519649491e-06, 6.482292519649491e-06, 6.482292519649491e-06, 6.482292519649491e-06, 6.482292519649491e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 6.482292519649491e-06

Optimization complete. Final v2v error: 2.2109322547912598 mm

Highest mean error: 2.445704221725464 mm for frame 24

Lowest mean error: 2.1064813137054443 mm for frame 107

Saving results

Total time: 37.0136821269989
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_grace_posed_004/1057/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_grace_posed_004/1057.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_grace_posed_004/1057
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00515197
Iteration 2/25 | Loss: 0.00117185
Iteration 3/25 | Loss: 0.00106476
Iteration 4/25 | Loss: 0.00104763
Iteration 5/25 | Loss: 0.00104466
Iteration 6/25 | Loss: 0.00104466
Iteration 7/25 | Loss: 0.00104466
Iteration 8/25 | Loss: 0.00104466
Iteration 9/25 | Loss: 0.00104466
Iteration 10/25 | Loss: 0.00104466
Iteration 11/25 | Loss: 0.00104466
Iteration 12/25 | Loss: 0.00104466
Iteration 13/25 | Loss: 0.00104466
Iteration 14/25 | Loss: 0.00104466
Iteration 15/25 | Loss: 0.00104466
Iteration 16/25 | Loss: 0.00104466
Iteration 17/25 | Loss: 0.00104466
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0010446627857163548, 0.0010446627857163548, 0.0010446627857163548, 0.0010446627857163548, 0.0010446627857163548]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010446627857163548

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.89985561
Iteration 2/25 | Loss: 0.00104629
Iteration 3/25 | Loss: 0.00104629
Iteration 4/25 | Loss: 0.00104629
Iteration 5/25 | Loss: 0.00104629
Iteration 6/25 | Loss: 0.00104629
Iteration 7/25 | Loss: 0.00104629
Iteration 8/25 | Loss: 0.00104629
Iteration 9/25 | Loss: 0.00104629
Iteration 10/25 | Loss: 0.00104629
Iteration 11/25 | Loss: 0.00104628
Iteration 12/25 | Loss: 0.00104628
Iteration 13/25 | Loss: 0.00104628
Iteration 14/25 | Loss: 0.00104628
Iteration 15/25 | Loss: 0.00104628
Iteration 16/25 | Loss: 0.00104628
Iteration 17/25 | Loss: 0.00104628
Iteration 18/25 | Loss: 0.00104628
Iteration 19/25 | Loss: 0.00104628
Iteration 20/25 | Loss: 0.00104628
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0010462845675647259, 0.0010462845675647259, 0.0010462845675647259, 0.0010462845675647259, 0.0010462845675647259]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010462845675647259

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00104628
Iteration 2/1000 | Loss: 0.00002334
Iteration 3/1000 | Loss: 0.00001660
Iteration 4/1000 | Loss: 0.00001543
Iteration 5/1000 | Loss: 0.00001454
Iteration 6/1000 | Loss: 0.00001405
Iteration 7/1000 | Loss: 0.00001366
Iteration 8/1000 | Loss: 0.00001326
Iteration 9/1000 | Loss: 0.00001300
Iteration 10/1000 | Loss: 0.00001276
Iteration 11/1000 | Loss: 0.00001274
Iteration 12/1000 | Loss: 0.00001272
Iteration 13/1000 | Loss: 0.00001257
Iteration 14/1000 | Loss: 0.00001256
Iteration 15/1000 | Loss: 0.00001251
Iteration 16/1000 | Loss: 0.00001249
Iteration 17/1000 | Loss: 0.00001248
Iteration 18/1000 | Loss: 0.00001247
Iteration 19/1000 | Loss: 0.00001246
Iteration 20/1000 | Loss: 0.00001238
Iteration 21/1000 | Loss: 0.00001236
Iteration 22/1000 | Loss: 0.00001235
Iteration 23/1000 | Loss: 0.00001234
Iteration 24/1000 | Loss: 0.00001234
Iteration 25/1000 | Loss: 0.00001233
Iteration 26/1000 | Loss: 0.00001232
Iteration 27/1000 | Loss: 0.00001232
Iteration 28/1000 | Loss: 0.00001230
Iteration 29/1000 | Loss: 0.00001227
Iteration 30/1000 | Loss: 0.00001227
Iteration 31/1000 | Loss: 0.00001225
Iteration 32/1000 | Loss: 0.00001225
Iteration 33/1000 | Loss: 0.00001224
Iteration 34/1000 | Loss: 0.00001223
Iteration 35/1000 | Loss: 0.00001223
Iteration 36/1000 | Loss: 0.00001223
Iteration 37/1000 | Loss: 0.00001222
Iteration 38/1000 | Loss: 0.00001222
Iteration 39/1000 | Loss: 0.00001222
Iteration 40/1000 | Loss: 0.00001221
Iteration 41/1000 | Loss: 0.00001219
Iteration 42/1000 | Loss: 0.00001219
Iteration 43/1000 | Loss: 0.00001219
Iteration 44/1000 | Loss: 0.00001219
Iteration 45/1000 | Loss: 0.00001219
Iteration 46/1000 | Loss: 0.00001219
Iteration 47/1000 | Loss: 0.00001219
Iteration 48/1000 | Loss: 0.00001219
Iteration 49/1000 | Loss: 0.00001219
Iteration 50/1000 | Loss: 0.00001218
Iteration 51/1000 | Loss: 0.00001218
Iteration 52/1000 | Loss: 0.00001218
Iteration 53/1000 | Loss: 0.00001217
Iteration 54/1000 | Loss: 0.00001217
Iteration 55/1000 | Loss: 0.00001216
Iteration 56/1000 | Loss: 0.00001216
Iteration 57/1000 | Loss: 0.00001216
Iteration 58/1000 | Loss: 0.00001216
Iteration 59/1000 | Loss: 0.00001216
Iteration 60/1000 | Loss: 0.00001215
Iteration 61/1000 | Loss: 0.00001215
Iteration 62/1000 | Loss: 0.00001215
Iteration 63/1000 | Loss: 0.00001215
Iteration 64/1000 | Loss: 0.00001214
Iteration 65/1000 | Loss: 0.00001214
Iteration 66/1000 | Loss: 0.00001214
Iteration 67/1000 | Loss: 0.00001214
Iteration 68/1000 | Loss: 0.00001214
Iteration 69/1000 | Loss: 0.00001213
Iteration 70/1000 | Loss: 0.00001213
Iteration 71/1000 | Loss: 0.00001213
Iteration 72/1000 | Loss: 0.00001213
Iteration 73/1000 | Loss: 0.00001213
Iteration 74/1000 | Loss: 0.00001213
Iteration 75/1000 | Loss: 0.00001212
Iteration 76/1000 | Loss: 0.00001212
Iteration 77/1000 | Loss: 0.00001212
Iteration 78/1000 | Loss: 0.00001212
Iteration 79/1000 | Loss: 0.00001212
Iteration 80/1000 | Loss: 0.00001212
Iteration 81/1000 | Loss: 0.00001212
Iteration 82/1000 | Loss: 0.00001212
Iteration 83/1000 | Loss: 0.00001212
Iteration 84/1000 | Loss: 0.00001212
Iteration 85/1000 | Loss: 0.00001212
Iteration 86/1000 | Loss: 0.00001212
Iteration 87/1000 | Loss: 0.00001211
Iteration 88/1000 | Loss: 0.00001211
Iteration 89/1000 | Loss: 0.00001211
Iteration 90/1000 | Loss: 0.00001211
Iteration 91/1000 | Loss: 0.00001211
Iteration 92/1000 | Loss: 0.00001210
Iteration 93/1000 | Loss: 0.00001210
Iteration 94/1000 | Loss: 0.00001210
Iteration 95/1000 | Loss: 0.00001210
Iteration 96/1000 | Loss: 0.00001210
Iteration 97/1000 | Loss: 0.00001209
Iteration 98/1000 | Loss: 0.00001209
Iteration 99/1000 | Loss: 0.00001209
Iteration 100/1000 | Loss: 0.00001209
Iteration 101/1000 | Loss: 0.00001208
Iteration 102/1000 | Loss: 0.00001208
Iteration 103/1000 | Loss: 0.00001208
Iteration 104/1000 | Loss: 0.00001208
Iteration 105/1000 | Loss: 0.00001208
Iteration 106/1000 | Loss: 0.00001208
Iteration 107/1000 | Loss: 0.00001207
Iteration 108/1000 | Loss: 0.00001207
Iteration 109/1000 | Loss: 0.00001207
Iteration 110/1000 | Loss: 0.00001207
Iteration 111/1000 | Loss: 0.00001206
Iteration 112/1000 | Loss: 0.00001206
Iteration 113/1000 | Loss: 0.00001206
Iteration 114/1000 | Loss: 0.00001206
Iteration 115/1000 | Loss: 0.00001205
Iteration 116/1000 | Loss: 0.00001205
Iteration 117/1000 | Loss: 0.00001205
Iteration 118/1000 | Loss: 0.00001205
Iteration 119/1000 | Loss: 0.00001205
Iteration 120/1000 | Loss: 0.00001205
Iteration 121/1000 | Loss: 0.00001205
Iteration 122/1000 | Loss: 0.00001205
Iteration 123/1000 | Loss: 0.00001205
Iteration 124/1000 | Loss: 0.00001205
Iteration 125/1000 | Loss: 0.00001205
Iteration 126/1000 | Loss: 0.00001205
Iteration 127/1000 | Loss: 0.00001205
Iteration 128/1000 | Loss: 0.00001205
Iteration 129/1000 | Loss: 0.00001205
Iteration 130/1000 | Loss: 0.00001205
Iteration 131/1000 | Loss: 0.00001205
Iteration 132/1000 | Loss: 0.00001205
Iteration 133/1000 | Loss: 0.00001205
Iteration 134/1000 | Loss: 0.00001205
Iteration 135/1000 | Loss: 0.00001205
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 135. Stopping optimization.
Last 5 losses: [1.2045760740875266e-05, 1.2045760740875266e-05, 1.2045760740875266e-05, 1.2045760740875266e-05, 1.2045760740875266e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2045760740875266e-05

Optimization complete. Final v2v error: 2.9400148391723633 mm

Highest mean error: 3.450270652770996 mm for frame 74

Lowest mean error: 2.6119418144226074 mm for frame 217

Saving results

Total time: 40.45153474807739
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_grace_posed_004/1058/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_grace_posed_004/1058.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_grace_posed_004/1058
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00831368
Iteration 2/25 | Loss: 0.00113676
Iteration 3/25 | Loss: 0.00101298
Iteration 4/25 | Loss: 0.00100161
Iteration 5/25 | Loss: 0.00099991
Iteration 6/25 | Loss: 0.00099991
Iteration 7/25 | Loss: 0.00099991
Iteration 8/25 | Loss: 0.00099991
Iteration 9/25 | Loss: 0.00099991
Iteration 10/25 | Loss: 0.00099991
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0009999116882681847, 0.0009999116882681847, 0.0009999116882681847, 0.0009999116882681847, 0.0009999116882681847]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009999116882681847

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.30365157
Iteration 2/25 | Loss: 0.00094782
Iteration 3/25 | Loss: 0.00094782
Iteration 4/25 | Loss: 0.00094781
Iteration 5/25 | Loss: 0.00094781
Iteration 6/25 | Loss: 0.00094781
Iteration 7/25 | Loss: 0.00094781
Iteration 8/25 | Loss: 0.00094781
Iteration 9/25 | Loss: 0.00094781
Iteration 10/25 | Loss: 0.00094781
Iteration 11/25 | Loss: 0.00094781
Iteration 12/25 | Loss: 0.00094781
Iteration 13/25 | Loss: 0.00094781
Iteration 14/25 | Loss: 0.00094781
Iteration 15/25 | Loss: 0.00094781
Iteration 16/25 | Loss: 0.00094781
Iteration 17/25 | Loss: 0.00094781
Iteration 18/25 | Loss: 0.00094781
Iteration 19/25 | Loss: 0.00094781
Iteration 20/25 | Loss: 0.00094781
Iteration 21/25 | Loss: 0.00094781
Iteration 22/25 | Loss: 0.00094781
Iteration 23/25 | Loss: 0.00094781
Iteration 24/25 | Loss: 0.00094781
Iteration 25/25 | Loss: 0.00094781
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0009478116407990456, 0.0009478116407990456, 0.0009478116407990456, 0.0009478116407990456, 0.0009478116407990456]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009478116407990456

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00094781
Iteration 2/1000 | Loss: 0.00001752
Iteration 3/1000 | Loss: 0.00001201
Iteration 4/1000 | Loss: 0.00001047
Iteration 5/1000 | Loss: 0.00000974
Iteration 6/1000 | Loss: 0.00000930
Iteration 7/1000 | Loss: 0.00000904
Iteration 8/1000 | Loss: 0.00000883
Iteration 9/1000 | Loss: 0.00000860
Iteration 10/1000 | Loss: 0.00000848
Iteration 11/1000 | Loss: 0.00000845
Iteration 12/1000 | Loss: 0.00000841
Iteration 13/1000 | Loss: 0.00000840
Iteration 14/1000 | Loss: 0.00000836
Iteration 15/1000 | Loss: 0.00000831
Iteration 16/1000 | Loss: 0.00000830
Iteration 17/1000 | Loss: 0.00000830
Iteration 18/1000 | Loss: 0.00000829
Iteration 19/1000 | Loss: 0.00000828
Iteration 20/1000 | Loss: 0.00000827
Iteration 21/1000 | Loss: 0.00000823
Iteration 22/1000 | Loss: 0.00000820
Iteration 23/1000 | Loss: 0.00000820
Iteration 24/1000 | Loss: 0.00000816
Iteration 25/1000 | Loss: 0.00000816
Iteration 26/1000 | Loss: 0.00000816
Iteration 27/1000 | Loss: 0.00000816
Iteration 28/1000 | Loss: 0.00000813
Iteration 29/1000 | Loss: 0.00000813
Iteration 30/1000 | Loss: 0.00000811
Iteration 31/1000 | Loss: 0.00000811
Iteration 32/1000 | Loss: 0.00000811
Iteration 33/1000 | Loss: 0.00000811
Iteration 34/1000 | Loss: 0.00000811
Iteration 35/1000 | Loss: 0.00000811
Iteration 36/1000 | Loss: 0.00000810
Iteration 37/1000 | Loss: 0.00000810
Iteration 38/1000 | Loss: 0.00000810
Iteration 39/1000 | Loss: 0.00000810
Iteration 40/1000 | Loss: 0.00000810
Iteration 41/1000 | Loss: 0.00000810
Iteration 42/1000 | Loss: 0.00000810
Iteration 43/1000 | Loss: 0.00000810
Iteration 44/1000 | Loss: 0.00000808
Iteration 45/1000 | Loss: 0.00000808
Iteration 46/1000 | Loss: 0.00000807
Iteration 47/1000 | Loss: 0.00000807
Iteration 48/1000 | Loss: 0.00000807
Iteration 49/1000 | Loss: 0.00000807
Iteration 50/1000 | Loss: 0.00000807
Iteration 51/1000 | Loss: 0.00000807
Iteration 52/1000 | Loss: 0.00000807
Iteration 53/1000 | Loss: 0.00000807
Iteration 54/1000 | Loss: 0.00000807
Iteration 55/1000 | Loss: 0.00000807
Iteration 56/1000 | Loss: 0.00000806
Iteration 57/1000 | Loss: 0.00000806
Iteration 58/1000 | Loss: 0.00000806
Iteration 59/1000 | Loss: 0.00000806
Iteration 60/1000 | Loss: 0.00000804
Iteration 61/1000 | Loss: 0.00000802
Iteration 62/1000 | Loss: 0.00000802
Iteration 63/1000 | Loss: 0.00000802
Iteration 64/1000 | Loss: 0.00000802
Iteration 65/1000 | Loss: 0.00000801
Iteration 66/1000 | Loss: 0.00000801
Iteration 67/1000 | Loss: 0.00000801
Iteration 68/1000 | Loss: 0.00000801
Iteration 69/1000 | Loss: 0.00000800
Iteration 70/1000 | Loss: 0.00000800
Iteration 71/1000 | Loss: 0.00000798
Iteration 72/1000 | Loss: 0.00000797
Iteration 73/1000 | Loss: 0.00000796
Iteration 74/1000 | Loss: 0.00000796
Iteration 75/1000 | Loss: 0.00000795
Iteration 76/1000 | Loss: 0.00000795
Iteration 77/1000 | Loss: 0.00000795
Iteration 78/1000 | Loss: 0.00000795
Iteration 79/1000 | Loss: 0.00000794
Iteration 80/1000 | Loss: 0.00000793
Iteration 81/1000 | Loss: 0.00000793
Iteration 82/1000 | Loss: 0.00000792
Iteration 83/1000 | Loss: 0.00000792
Iteration 84/1000 | Loss: 0.00000792
Iteration 85/1000 | Loss: 0.00000792
Iteration 86/1000 | Loss: 0.00000792
Iteration 87/1000 | Loss: 0.00000792
Iteration 88/1000 | Loss: 0.00000792
Iteration 89/1000 | Loss: 0.00000791
Iteration 90/1000 | Loss: 0.00000791
Iteration 91/1000 | Loss: 0.00000791
Iteration 92/1000 | Loss: 0.00000791
Iteration 93/1000 | Loss: 0.00000791
Iteration 94/1000 | Loss: 0.00000791
Iteration 95/1000 | Loss: 0.00000791
Iteration 96/1000 | Loss: 0.00000790
Iteration 97/1000 | Loss: 0.00000790
Iteration 98/1000 | Loss: 0.00000790
Iteration 99/1000 | Loss: 0.00000789
Iteration 100/1000 | Loss: 0.00000789
Iteration 101/1000 | Loss: 0.00000789
Iteration 102/1000 | Loss: 0.00000789
Iteration 103/1000 | Loss: 0.00000788
Iteration 104/1000 | Loss: 0.00000788
Iteration 105/1000 | Loss: 0.00000788
Iteration 106/1000 | Loss: 0.00000788
Iteration 107/1000 | Loss: 0.00000788
Iteration 108/1000 | Loss: 0.00000788
Iteration 109/1000 | Loss: 0.00000787
Iteration 110/1000 | Loss: 0.00000786
Iteration 111/1000 | Loss: 0.00000785
Iteration 112/1000 | Loss: 0.00000785
Iteration 113/1000 | Loss: 0.00000784
Iteration 114/1000 | Loss: 0.00000784
Iteration 115/1000 | Loss: 0.00000784
Iteration 116/1000 | Loss: 0.00000783
Iteration 117/1000 | Loss: 0.00000783
Iteration 118/1000 | Loss: 0.00000782
Iteration 119/1000 | Loss: 0.00000782
Iteration 120/1000 | Loss: 0.00000781
Iteration 121/1000 | Loss: 0.00000781
Iteration 122/1000 | Loss: 0.00000781
Iteration 123/1000 | Loss: 0.00000781
Iteration 124/1000 | Loss: 0.00000781
Iteration 125/1000 | Loss: 0.00000781
Iteration 126/1000 | Loss: 0.00000781
Iteration 127/1000 | Loss: 0.00000781
Iteration 128/1000 | Loss: 0.00000780
Iteration 129/1000 | Loss: 0.00000780
Iteration 130/1000 | Loss: 0.00000779
Iteration 131/1000 | Loss: 0.00000779
Iteration 132/1000 | Loss: 0.00000779
Iteration 133/1000 | Loss: 0.00000779
Iteration 134/1000 | Loss: 0.00000779
Iteration 135/1000 | Loss: 0.00000779
Iteration 136/1000 | Loss: 0.00000779
Iteration 137/1000 | Loss: 0.00000779
Iteration 138/1000 | Loss: 0.00000779
Iteration 139/1000 | Loss: 0.00000779
Iteration 140/1000 | Loss: 0.00000779
Iteration 141/1000 | Loss: 0.00000779
Iteration 142/1000 | Loss: 0.00000778
Iteration 143/1000 | Loss: 0.00000778
Iteration 144/1000 | Loss: 0.00000778
Iteration 145/1000 | Loss: 0.00000778
Iteration 146/1000 | Loss: 0.00000778
Iteration 147/1000 | Loss: 0.00000778
Iteration 148/1000 | Loss: 0.00000778
Iteration 149/1000 | Loss: 0.00000778
Iteration 150/1000 | Loss: 0.00000778
Iteration 151/1000 | Loss: 0.00000778
Iteration 152/1000 | Loss: 0.00000778
Iteration 153/1000 | Loss: 0.00000778
Iteration 154/1000 | Loss: 0.00000778
Iteration 155/1000 | Loss: 0.00000778
Iteration 156/1000 | Loss: 0.00000778
Iteration 157/1000 | Loss: 0.00000778
Iteration 158/1000 | Loss: 0.00000778
Iteration 159/1000 | Loss: 0.00000778
Iteration 160/1000 | Loss: 0.00000778
Iteration 161/1000 | Loss: 0.00000778
Iteration 162/1000 | Loss: 0.00000778
Iteration 163/1000 | Loss: 0.00000778
Iteration 164/1000 | Loss: 0.00000778
Iteration 165/1000 | Loss: 0.00000778
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 165. Stopping optimization.
Last 5 losses: [7.775257472530939e-06, 7.775257472530939e-06, 7.775257472530939e-06, 7.775257472530939e-06, 7.775257472530939e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 7.775257472530939e-06

Optimization complete. Final v2v error: 2.3733296394348145 mm

Highest mean error: 2.478816270828247 mm for frame 63

Lowest mean error: 2.2961745262145996 mm for frame 120

Saving results

Total time: 37.03355360031128
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_grace_posed_004/1091/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_grace_posed_004/1091.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_grace_posed_004/1091
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00852970
Iteration 2/25 | Loss: 0.00105498
Iteration 3/25 | Loss: 0.00099623
Iteration 4/25 | Loss: 0.00098981
Iteration 5/25 | Loss: 0.00098789
Iteration 6/25 | Loss: 0.00098736
Iteration 7/25 | Loss: 0.00098736
Iteration 8/25 | Loss: 0.00098736
Iteration 9/25 | Loss: 0.00098736
Iteration 10/25 | Loss: 0.00098736
Iteration 11/25 | Loss: 0.00098736
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0009873609524220228, 0.0009873609524220228, 0.0009873609524220228, 0.0009873609524220228, 0.0009873609524220228]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009873609524220228

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.62472391
Iteration 2/25 | Loss: 0.00103870
Iteration 3/25 | Loss: 0.00103870
Iteration 4/25 | Loss: 0.00103870
Iteration 5/25 | Loss: 0.00103870
Iteration 6/25 | Loss: 0.00103870
Iteration 7/25 | Loss: 0.00103870
Iteration 8/25 | Loss: 0.00103870
Iteration 9/25 | Loss: 0.00103869
Iteration 10/25 | Loss: 0.00103869
Iteration 11/25 | Loss: 0.00103869
Iteration 12/25 | Loss: 0.00103869
Iteration 13/25 | Loss: 0.00103869
Iteration 14/25 | Loss: 0.00103869
Iteration 15/25 | Loss: 0.00103869
Iteration 16/25 | Loss: 0.00103869
Iteration 17/25 | Loss: 0.00103869
Iteration 18/25 | Loss: 0.00103869
Iteration 19/25 | Loss: 0.00103869
Iteration 20/25 | Loss: 0.00103869
Iteration 21/25 | Loss: 0.00103869
Iteration 22/25 | Loss: 0.00103869
Iteration 23/25 | Loss: 0.00103869
Iteration 24/25 | Loss: 0.00103869
Iteration 25/25 | Loss: 0.00103869

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00103869
Iteration 2/1000 | Loss: 0.00001753
Iteration 3/1000 | Loss: 0.00001112
Iteration 4/1000 | Loss: 0.00000962
Iteration 5/1000 | Loss: 0.00000892
Iteration 6/1000 | Loss: 0.00000844
Iteration 7/1000 | Loss: 0.00000813
Iteration 8/1000 | Loss: 0.00000793
Iteration 9/1000 | Loss: 0.00000785
Iteration 10/1000 | Loss: 0.00000783
Iteration 11/1000 | Loss: 0.00000781
Iteration 12/1000 | Loss: 0.00000780
Iteration 13/1000 | Loss: 0.00000779
Iteration 14/1000 | Loss: 0.00000771
Iteration 15/1000 | Loss: 0.00000766
Iteration 16/1000 | Loss: 0.00000764
Iteration 17/1000 | Loss: 0.00000764
Iteration 18/1000 | Loss: 0.00000763
Iteration 19/1000 | Loss: 0.00000763
Iteration 20/1000 | Loss: 0.00000762
Iteration 21/1000 | Loss: 0.00000762
Iteration 22/1000 | Loss: 0.00000762
Iteration 23/1000 | Loss: 0.00000761
Iteration 24/1000 | Loss: 0.00000760
Iteration 25/1000 | Loss: 0.00000760
Iteration 26/1000 | Loss: 0.00000760
Iteration 27/1000 | Loss: 0.00000759
Iteration 28/1000 | Loss: 0.00000759
Iteration 29/1000 | Loss: 0.00000758
Iteration 30/1000 | Loss: 0.00000758
Iteration 31/1000 | Loss: 0.00000758
Iteration 32/1000 | Loss: 0.00000757
Iteration 33/1000 | Loss: 0.00000757
Iteration 34/1000 | Loss: 0.00000757
Iteration 35/1000 | Loss: 0.00000756
Iteration 36/1000 | Loss: 0.00000756
Iteration 37/1000 | Loss: 0.00000756
Iteration 38/1000 | Loss: 0.00000755
Iteration 39/1000 | Loss: 0.00000754
Iteration 40/1000 | Loss: 0.00000754
Iteration 41/1000 | Loss: 0.00000753
Iteration 42/1000 | Loss: 0.00000753
Iteration 43/1000 | Loss: 0.00000753
Iteration 44/1000 | Loss: 0.00000753
Iteration 45/1000 | Loss: 0.00000753
Iteration 46/1000 | Loss: 0.00000752
Iteration 47/1000 | Loss: 0.00000752
Iteration 48/1000 | Loss: 0.00000752
Iteration 49/1000 | Loss: 0.00000749
Iteration 50/1000 | Loss: 0.00000749
Iteration 51/1000 | Loss: 0.00000748
Iteration 52/1000 | Loss: 0.00000745
Iteration 53/1000 | Loss: 0.00000743
Iteration 54/1000 | Loss: 0.00000743
Iteration 55/1000 | Loss: 0.00000743
Iteration 56/1000 | Loss: 0.00000743
Iteration 57/1000 | Loss: 0.00000743
Iteration 58/1000 | Loss: 0.00000743
Iteration 59/1000 | Loss: 0.00000742
Iteration 60/1000 | Loss: 0.00000742
Iteration 61/1000 | Loss: 0.00000741
Iteration 62/1000 | Loss: 0.00000741
Iteration 63/1000 | Loss: 0.00000741
Iteration 64/1000 | Loss: 0.00000741
Iteration 65/1000 | Loss: 0.00000741
Iteration 66/1000 | Loss: 0.00000741
Iteration 67/1000 | Loss: 0.00000740
Iteration 68/1000 | Loss: 0.00000740
Iteration 69/1000 | Loss: 0.00000740
Iteration 70/1000 | Loss: 0.00000740
Iteration 71/1000 | Loss: 0.00000739
Iteration 72/1000 | Loss: 0.00000739
Iteration 73/1000 | Loss: 0.00000739
Iteration 74/1000 | Loss: 0.00000739
Iteration 75/1000 | Loss: 0.00000738
Iteration 76/1000 | Loss: 0.00000738
Iteration 77/1000 | Loss: 0.00000738
Iteration 78/1000 | Loss: 0.00000738
Iteration 79/1000 | Loss: 0.00000738
Iteration 80/1000 | Loss: 0.00000738
Iteration 81/1000 | Loss: 0.00000738
Iteration 82/1000 | Loss: 0.00000737
Iteration 83/1000 | Loss: 0.00000737
Iteration 84/1000 | Loss: 0.00000737
Iteration 85/1000 | Loss: 0.00000737
Iteration 86/1000 | Loss: 0.00000737
Iteration 87/1000 | Loss: 0.00000737
Iteration 88/1000 | Loss: 0.00000737
Iteration 89/1000 | Loss: 0.00000737
Iteration 90/1000 | Loss: 0.00000737
Iteration 91/1000 | Loss: 0.00000737
Iteration 92/1000 | Loss: 0.00000737
Iteration 93/1000 | Loss: 0.00000736
Iteration 94/1000 | Loss: 0.00000736
Iteration 95/1000 | Loss: 0.00000736
Iteration 96/1000 | Loss: 0.00000736
Iteration 97/1000 | Loss: 0.00000735
Iteration 98/1000 | Loss: 0.00000735
Iteration 99/1000 | Loss: 0.00000735
Iteration 100/1000 | Loss: 0.00000735
Iteration 101/1000 | Loss: 0.00000735
Iteration 102/1000 | Loss: 0.00000735
Iteration 103/1000 | Loss: 0.00000735
Iteration 104/1000 | Loss: 0.00000735
Iteration 105/1000 | Loss: 0.00000734
Iteration 106/1000 | Loss: 0.00000734
Iteration 107/1000 | Loss: 0.00000734
Iteration 108/1000 | Loss: 0.00000734
Iteration 109/1000 | Loss: 0.00000734
Iteration 110/1000 | Loss: 0.00000733
Iteration 111/1000 | Loss: 0.00000733
Iteration 112/1000 | Loss: 0.00000732
Iteration 113/1000 | Loss: 0.00000732
Iteration 114/1000 | Loss: 0.00000732
Iteration 115/1000 | Loss: 0.00000732
Iteration 116/1000 | Loss: 0.00000732
Iteration 117/1000 | Loss: 0.00000732
Iteration 118/1000 | Loss: 0.00000732
Iteration 119/1000 | Loss: 0.00000732
Iteration 120/1000 | Loss: 0.00000732
Iteration 121/1000 | Loss: 0.00000732
Iteration 122/1000 | Loss: 0.00000732
Iteration 123/1000 | Loss: 0.00000732
Iteration 124/1000 | Loss: 0.00000732
Iteration 125/1000 | Loss: 0.00000732
Iteration 126/1000 | Loss: 0.00000732
Iteration 127/1000 | Loss: 0.00000731
Iteration 128/1000 | Loss: 0.00000731
Iteration 129/1000 | Loss: 0.00000731
Iteration 130/1000 | Loss: 0.00000731
Iteration 131/1000 | Loss: 0.00000731
Iteration 132/1000 | Loss: 0.00000731
Iteration 133/1000 | Loss: 0.00000731
Iteration 134/1000 | Loss: 0.00000731
Iteration 135/1000 | Loss: 0.00000730
Iteration 136/1000 | Loss: 0.00000730
Iteration 137/1000 | Loss: 0.00000730
Iteration 138/1000 | Loss: 0.00000730
Iteration 139/1000 | Loss: 0.00000730
Iteration 140/1000 | Loss: 0.00000730
Iteration 141/1000 | Loss: 0.00000730
Iteration 142/1000 | Loss: 0.00000730
Iteration 143/1000 | Loss: 0.00000730
Iteration 144/1000 | Loss: 0.00000730
Iteration 145/1000 | Loss: 0.00000730
Iteration 146/1000 | Loss: 0.00000730
Iteration 147/1000 | Loss: 0.00000729
Iteration 148/1000 | Loss: 0.00000729
Iteration 149/1000 | Loss: 0.00000729
Iteration 150/1000 | Loss: 0.00000729
Iteration 151/1000 | Loss: 0.00000729
Iteration 152/1000 | Loss: 0.00000729
Iteration 153/1000 | Loss: 0.00000729
Iteration 154/1000 | Loss: 0.00000729
Iteration 155/1000 | Loss: 0.00000729
Iteration 156/1000 | Loss: 0.00000728
Iteration 157/1000 | Loss: 0.00000728
Iteration 158/1000 | Loss: 0.00000728
Iteration 159/1000 | Loss: 0.00000728
Iteration 160/1000 | Loss: 0.00000728
Iteration 161/1000 | Loss: 0.00000728
Iteration 162/1000 | Loss: 0.00000728
Iteration 163/1000 | Loss: 0.00000727
Iteration 164/1000 | Loss: 0.00000727
Iteration 165/1000 | Loss: 0.00000727
Iteration 166/1000 | Loss: 0.00000727
Iteration 167/1000 | Loss: 0.00000727
Iteration 168/1000 | Loss: 0.00000727
Iteration 169/1000 | Loss: 0.00000727
Iteration 170/1000 | Loss: 0.00000726
Iteration 171/1000 | Loss: 0.00000726
Iteration 172/1000 | Loss: 0.00000726
Iteration 173/1000 | Loss: 0.00000726
Iteration 174/1000 | Loss: 0.00000726
Iteration 175/1000 | Loss: 0.00000725
Iteration 176/1000 | Loss: 0.00000725
Iteration 177/1000 | Loss: 0.00000725
Iteration 178/1000 | Loss: 0.00000724
Iteration 179/1000 | Loss: 0.00000724
Iteration 180/1000 | Loss: 0.00000724
Iteration 181/1000 | Loss: 0.00000724
Iteration 182/1000 | Loss: 0.00000724
Iteration 183/1000 | Loss: 0.00000724
Iteration 184/1000 | Loss: 0.00000724
Iteration 185/1000 | Loss: 0.00000724
Iteration 186/1000 | Loss: 0.00000723
Iteration 187/1000 | Loss: 0.00000723
Iteration 188/1000 | Loss: 0.00000723
Iteration 189/1000 | Loss: 0.00000723
Iteration 190/1000 | Loss: 0.00000723
Iteration 191/1000 | Loss: 0.00000723
Iteration 192/1000 | Loss: 0.00000723
Iteration 193/1000 | Loss: 0.00000723
Iteration 194/1000 | Loss: 0.00000723
Iteration 195/1000 | Loss: 0.00000723
Iteration 196/1000 | Loss: 0.00000723
Iteration 197/1000 | Loss: 0.00000723
Iteration 198/1000 | Loss: 0.00000723
Iteration 199/1000 | Loss: 0.00000723
Iteration 200/1000 | Loss: 0.00000723
Iteration 201/1000 | Loss: 0.00000723
Iteration 202/1000 | Loss: 0.00000723
Iteration 203/1000 | Loss: 0.00000722
Iteration 204/1000 | Loss: 0.00000722
Iteration 205/1000 | Loss: 0.00000722
Iteration 206/1000 | Loss: 0.00000722
Iteration 207/1000 | Loss: 0.00000722
Iteration 208/1000 | Loss: 0.00000722
Iteration 209/1000 | Loss: 0.00000722
Iteration 210/1000 | Loss: 0.00000722
Iteration 211/1000 | Loss: 0.00000721
Iteration 212/1000 | Loss: 0.00000721
Iteration 213/1000 | Loss: 0.00000721
Iteration 214/1000 | Loss: 0.00000721
Iteration 215/1000 | Loss: 0.00000721
Iteration 216/1000 | Loss: 0.00000721
Iteration 217/1000 | Loss: 0.00000721
Iteration 218/1000 | Loss: 0.00000721
Iteration 219/1000 | Loss: 0.00000721
Iteration 220/1000 | Loss: 0.00000721
Iteration 221/1000 | Loss: 0.00000721
Iteration 222/1000 | Loss: 0.00000720
Iteration 223/1000 | Loss: 0.00000720
Iteration 224/1000 | Loss: 0.00000720
Iteration 225/1000 | Loss: 0.00000720
Iteration 226/1000 | Loss: 0.00000720
Iteration 227/1000 | Loss: 0.00000720
Iteration 228/1000 | Loss: 0.00000720
Iteration 229/1000 | Loss: 0.00000720
Iteration 230/1000 | Loss: 0.00000720
Iteration 231/1000 | Loss: 0.00000720
Iteration 232/1000 | Loss: 0.00000720
Iteration 233/1000 | Loss: 0.00000720
Iteration 234/1000 | Loss: 0.00000720
Iteration 235/1000 | Loss: 0.00000720
Iteration 236/1000 | Loss: 0.00000720
Iteration 237/1000 | Loss: 0.00000720
Iteration 238/1000 | Loss: 0.00000720
Iteration 239/1000 | Loss: 0.00000720
Iteration 240/1000 | Loss: 0.00000720
Iteration 241/1000 | Loss: 0.00000719
Iteration 242/1000 | Loss: 0.00000719
Iteration 243/1000 | Loss: 0.00000719
Iteration 244/1000 | Loss: 0.00000719
Iteration 245/1000 | Loss: 0.00000719
Iteration 246/1000 | Loss: 0.00000719
Iteration 247/1000 | Loss: 0.00000719
Iteration 248/1000 | Loss: 0.00000719
Iteration 249/1000 | Loss: 0.00000719
Iteration 250/1000 | Loss: 0.00000719
Iteration 251/1000 | Loss: 0.00000719
Iteration 252/1000 | Loss: 0.00000718
Iteration 253/1000 | Loss: 0.00000718
Iteration 254/1000 | Loss: 0.00000718
Iteration 255/1000 | Loss: 0.00000718
Iteration 256/1000 | Loss: 0.00000718
Iteration 257/1000 | Loss: 0.00000718
Iteration 258/1000 | Loss: 0.00000718
Iteration 259/1000 | Loss: 0.00000718
Iteration 260/1000 | Loss: 0.00000718
Iteration 261/1000 | Loss: 0.00000718
Iteration 262/1000 | Loss: 0.00000718
Iteration 263/1000 | Loss: 0.00000718
Iteration 264/1000 | Loss: 0.00000718
Iteration 265/1000 | Loss: 0.00000718
Iteration 266/1000 | Loss: 0.00000718
Iteration 267/1000 | Loss: 0.00000718
Iteration 268/1000 | Loss: 0.00000718
Iteration 269/1000 | Loss: 0.00000718
Iteration 270/1000 | Loss: 0.00000718
Iteration 271/1000 | Loss: 0.00000718
Iteration 272/1000 | Loss: 0.00000718
Iteration 273/1000 | Loss: 0.00000718
Iteration 274/1000 | Loss: 0.00000718
Iteration 275/1000 | Loss: 0.00000718
Iteration 276/1000 | Loss: 0.00000718
Iteration 277/1000 | Loss: 0.00000718
Iteration 278/1000 | Loss: 0.00000718
Iteration 279/1000 | Loss: 0.00000718
Iteration 280/1000 | Loss: 0.00000718
Iteration 281/1000 | Loss: 0.00000718
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 281. Stopping optimization.
Last 5 losses: [7.183433808677364e-06, 7.183433808677364e-06, 7.183433808677364e-06, 7.183433808677364e-06, 7.183433808677364e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 7.183433808677364e-06

Optimization complete. Final v2v error: 2.295408010482788 mm

Highest mean error: 2.9907734394073486 mm for frame 44

Lowest mean error: 2.1383397579193115 mm for frame 96

Saving results

Total time: 39.92847967147827
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_grace_posed_004/1007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_grace_posed_004/1007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_grace_posed_004/1007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01024483
Iteration 2/25 | Loss: 0.00322962
Iteration 3/25 | Loss: 0.00299494
Iteration 4/25 | Loss: 0.00218903
Iteration 5/25 | Loss: 0.00163912
Iteration 6/25 | Loss: 0.00131974
Iteration 7/25 | Loss: 0.00119424
Iteration 8/25 | Loss: 0.00115512
Iteration 9/25 | Loss: 0.00113711
Iteration 10/25 | Loss: 0.00113315
Iteration 11/25 | Loss: 0.00113240
Iteration 12/25 | Loss: 0.00113206
Iteration 13/25 | Loss: 0.00113191
Iteration 14/25 | Loss: 0.00113186
Iteration 15/25 | Loss: 0.00113186
Iteration 16/25 | Loss: 0.00113186
Iteration 17/25 | Loss: 0.00113185
Iteration 18/25 | Loss: 0.00113185
Iteration 19/25 | Loss: 0.00113185
Iteration 20/25 | Loss: 0.00113185
Iteration 21/25 | Loss: 0.00113185
Iteration 22/25 | Loss: 0.00113185
Iteration 23/25 | Loss: 0.00113185
Iteration 24/25 | Loss: 0.00113185
Iteration 25/25 | Loss: 0.00113185

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.29250526
Iteration 2/25 | Loss: 0.00125123
Iteration 3/25 | Loss: 0.00125123
Iteration 4/25 | Loss: 0.00125123
Iteration 5/25 | Loss: 0.00125123
Iteration 6/25 | Loss: 0.00125123
Iteration 7/25 | Loss: 0.00125123
Iteration 8/25 | Loss: 0.00125123
Iteration 9/25 | Loss: 0.00125123
Iteration 10/25 | Loss: 0.00125123
Iteration 11/25 | Loss: 0.00125122
Iteration 12/25 | Loss: 0.00125122
Iteration 13/25 | Loss: 0.00125122
Iteration 14/25 | Loss: 0.00125122
Iteration 15/25 | Loss: 0.00125122
Iteration 16/25 | Loss: 0.00125122
Iteration 17/25 | Loss: 0.00125122
Iteration 18/25 | Loss: 0.00125122
Iteration 19/25 | Loss: 0.00125122
Iteration 20/25 | Loss: 0.00125122
Iteration 21/25 | Loss: 0.00125122
Iteration 22/25 | Loss: 0.00125122
Iteration 23/25 | Loss: 0.00125122
Iteration 24/25 | Loss: 0.00125122
Iteration 25/25 | Loss: 0.00125122

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00125122
Iteration 2/1000 | Loss: 0.00005864
Iteration 3/1000 | Loss: 0.00004368
Iteration 4/1000 | Loss: 0.00003818
Iteration 5/1000 | Loss: 0.00003499
Iteration 6/1000 | Loss: 0.00003326
Iteration 7/1000 | Loss: 0.00003194
Iteration 8/1000 | Loss: 0.00003110
Iteration 9/1000 | Loss: 0.00003016
Iteration 10/1000 | Loss: 0.00002948
Iteration 11/1000 | Loss: 0.00051299
Iteration 12/1000 | Loss: 0.00118864
Iteration 13/1000 | Loss: 0.00006384
Iteration 14/1000 | Loss: 0.00004308
Iteration 15/1000 | Loss: 0.00003187
Iteration 16/1000 | Loss: 0.00002547
Iteration 17/1000 | Loss: 0.00002167
Iteration 18/1000 | Loss: 0.00001856
Iteration 19/1000 | Loss: 0.00001629
Iteration 20/1000 | Loss: 0.00001488
Iteration 21/1000 | Loss: 0.00001427
Iteration 22/1000 | Loss: 0.00001355
Iteration 23/1000 | Loss: 0.00001312
Iteration 24/1000 | Loss: 0.00001266
Iteration 25/1000 | Loss: 0.00001228
Iteration 26/1000 | Loss: 0.00001202
Iteration 27/1000 | Loss: 0.00001199
Iteration 28/1000 | Loss: 0.00001191
Iteration 29/1000 | Loss: 0.00001190
Iteration 30/1000 | Loss: 0.00001178
Iteration 31/1000 | Loss: 0.00001177
Iteration 32/1000 | Loss: 0.00001174
Iteration 33/1000 | Loss: 0.00001172
Iteration 34/1000 | Loss: 0.00001168
Iteration 35/1000 | Loss: 0.00001168
Iteration 36/1000 | Loss: 0.00001166
Iteration 37/1000 | Loss: 0.00001166
Iteration 38/1000 | Loss: 0.00001165
Iteration 39/1000 | Loss: 0.00001165
Iteration 40/1000 | Loss: 0.00001165
Iteration 41/1000 | Loss: 0.00001164
Iteration 42/1000 | Loss: 0.00001164
Iteration 43/1000 | Loss: 0.00001164
Iteration 44/1000 | Loss: 0.00001164
Iteration 45/1000 | Loss: 0.00001164
Iteration 46/1000 | Loss: 0.00001164
Iteration 47/1000 | Loss: 0.00001164
Iteration 48/1000 | Loss: 0.00001163
Iteration 49/1000 | Loss: 0.00001163
Iteration 50/1000 | Loss: 0.00001163
Iteration 51/1000 | Loss: 0.00001163
Iteration 52/1000 | Loss: 0.00001163
Iteration 53/1000 | Loss: 0.00001163
Iteration 54/1000 | Loss: 0.00001162
Iteration 55/1000 | Loss: 0.00001162
Iteration 56/1000 | Loss: 0.00001162
Iteration 57/1000 | Loss: 0.00001162
Iteration 58/1000 | Loss: 0.00001162
Iteration 59/1000 | Loss: 0.00001162
Iteration 60/1000 | Loss: 0.00001162
Iteration 61/1000 | Loss: 0.00001162
Iteration 62/1000 | Loss: 0.00001162
Iteration 63/1000 | Loss: 0.00001161
Iteration 64/1000 | Loss: 0.00001161
Iteration 65/1000 | Loss: 0.00001161
Iteration 66/1000 | Loss: 0.00001161
Iteration 67/1000 | Loss: 0.00001161
Iteration 68/1000 | Loss: 0.00001161
Iteration 69/1000 | Loss: 0.00001161
Iteration 70/1000 | Loss: 0.00001160
Iteration 71/1000 | Loss: 0.00001160
Iteration 72/1000 | Loss: 0.00001160
Iteration 73/1000 | Loss: 0.00001160
Iteration 74/1000 | Loss: 0.00001159
Iteration 75/1000 | Loss: 0.00001159
Iteration 76/1000 | Loss: 0.00001159
Iteration 77/1000 | Loss: 0.00001159
Iteration 78/1000 | Loss: 0.00001158
Iteration 79/1000 | Loss: 0.00001158
Iteration 80/1000 | Loss: 0.00001158
Iteration 81/1000 | Loss: 0.00001158
Iteration 82/1000 | Loss: 0.00001158
Iteration 83/1000 | Loss: 0.00001158
Iteration 84/1000 | Loss: 0.00001158
Iteration 85/1000 | Loss: 0.00001158
Iteration 86/1000 | Loss: 0.00001158
Iteration 87/1000 | Loss: 0.00001158
Iteration 88/1000 | Loss: 0.00001157
Iteration 89/1000 | Loss: 0.00001157
Iteration 90/1000 | Loss: 0.00001157
Iteration 91/1000 | Loss: 0.00001157
Iteration 92/1000 | Loss: 0.00001157
Iteration 93/1000 | Loss: 0.00001157
Iteration 94/1000 | Loss: 0.00001157
Iteration 95/1000 | Loss: 0.00001157
Iteration 96/1000 | Loss: 0.00001157
Iteration 97/1000 | Loss: 0.00001156
Iteration 98/1000 | Loss: 0.00001156
Iteration 99/1000 | Loss: 0.00001156
Iteration 100/1000 | Loss: 0.00001155
Iteration 101/1000 | Loss: 0.00001155
Iteration 102/1000 | Loss: 0.00001155
Iteration 103/1000 | Loss: 0.00001155
Iteration 104/1000 | Loss: 0.00001154
Iteration 105/1000 | Loss: 0.00001154
Iteration 106/1000 | Loss: 0.00001154
Iteration 107/1000 | Loss: 0.00001154
Iteration 108/1000 | Loss: 0.00001153
Iteration 109/1000 | Loss: 0.00001153
Iteration 110/1000 | Loss: 0.00001153
Iteration 111/1000 | Loss: 0.00001153
Iteration 112/1000 | Loss: 0.00001153
Iteration 113/1000 | Loss: 0.00001153
Iteration 114/1000 | Loss: 0.00001153
Iteration 115/1000 | Loss: 0.00001153
Iteration 116/1000 | Loss: 0.00001153
Iteration 117/1000 | Loss: 0.00001153
Iteration 118/1000 | Loss: 0.00001153
Iteration 119/1000 | Loss: 0.00001152
Iteration 120/1000 | Loss: 0.00001152
Iteration 121/1000 | Loss: 0.00001152
Iteration 122/1000 | Loss: 0.00001152
Iteration 123/1000 | Loss: 0.00001152
Iteration 124/1000 | Loss: 0.00001152
Iteration 125/1000 | Loss: 0.00001152
Iteration 126/1000 | Loss: 0.00001152
Iteration 127/1000 | Loss: 0.00001152
Iteration 128/1000 | Loss: 0.00001152
Iteration 129/1000 | Loss: 0.00001152
Iteration 130/1000 | Loss: 0.00001152
Iteration 131/1000 | Loss: 0.00001152
Iteration 132/1000 | Loss: 0.00001152
Iteration 133/1000 | Loss: 0.00001152
Iteration 134/1000 | Loss: 0.00001152
Iteration 135/1000 | Loss: 0.00001152
Iteration 136/1000 | Loss: 0.00001152
Iteration 137/1000 | Loss: 0.00001151
Iteration 138/1000 | Loss: 0.00001151
Iteration 139/1000 | Loss: 0.00001151
Iteration 140/1000 | Loss: 0.00001151
Iteration 141/1000 | Loss: 0.00001151
Iteration 142/1000 | Loss: 0.00001151
Iteration 143/1000 | Loss: 0.00001151
Iteration 144/1000 | Loss: 0.00001151
Iteration 145/1000 | Loss: 0.00001151
Iteration 146/1000 | Loss: 0.00001151
Iteration 147/1000 | Loss: 0.00001151
Iteration 148/1000 | Loss: 0.00001151
Iteration 149/1000 | Loss: 0.00001151
Iteration 150/1000 | Loss: 0.00001151
Iteration 151/1000 | Loss: 0.00001151
Iteration 152/1000 | Loss: 0.00001151
Iteration 153/1000 | Loss: 0.00001151
Iteration 154/1000 | Loss: 0.00001151
Iteration 155/1000 | Loss: 0.00001151
Iteration 156/1000 | Loss: 0.00001151
Iteration 157/1000 | Loss: 0.00001151
Iteration 158/1000 | Loss: 0.00001151
Iteration 159/1000 | Loss: 0.00001151
Iteration 160/1000 | Loss: 0.00001151
Iteration 161/1000 | Loss: 0.00001151
Iteration 162/1000 | Loss: 0.00001151
Iteration 163/1000 | Loss: 0.00001151
Iteration 164/1000 | Loss: 0.00001150
Iteration 165/1000 | Loss: 0.00001150
Iteration 166/1000 | Loss: 0.00001150
Iteration 167/1000 | Loss: 0.00001150
Iteration 168/1000 | Loss: 0.00001150
Iteration 169/1000 | Loss: 0.00001150
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 169. Stopping optimization.
Last 5 losses: [1.1504907888593152e-05, 1.1504907888593152e-05, 1.1504907888593152e-05, 1.1504907888593152e-05, 1.1504907888593152e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1504907888593152e-05

Optimization complete. Final v2v error: 2.889035701751709 mm

Highest mean error: 3.112556219100952 mm for frame 11

Lowest mean error: 2.744100570678711 mm for frame 80

Saving results

Total time: 70.75616025924683
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_grace_posed_004/1029/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_grace_posed_004/1029.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_grace_posed_004/1029
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00558841
Iteration 2/25 | Loss: 0.00116780
Iteration 3/25 | Loss: 0.00105828
Iteration 4/25 | Loss: 0.00103463
Iteration 5/25 | Loss: 0.00103017
Iteration 6/25 | Loss: 0.00102877
Iteration 7/25 | Loss: 0.00102877
Iteration 8/25 | Loss: 0.00102877
Iteration 9/25 | Loss: 0.00102877
Iteration 10/25 | Loss: 0.00102877
Iteration 11/25 | Loss: 0.00102877
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001028768252581358, 0.001028768252581358, 0.001028768252581358, 0.001028768252581358, 0.001028768252581358]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001028768252581358

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.96185887
Iteration 2/25 | Loss: 0.00057143
Iteration 3/25 | Loss: 0.00057143
Iteration 4/25 | Loss: 0.00057143
Iteration 5/25 | Loss: 0.00057143
Iteration 6/25 | Loss: 0.00057143
Iteration 7/25 | Loss: 0.00057143
Iteration 8/25 | Loss: 0.00057143
Iteration 9/25 | Loss: 0.00057143
Iteration 10/25 | Loss: 0.00057143
Iteration 11/25 | Loss: 0.00057143
Iteration 12/25 | Loss: 0.00057143
Iteration 13/25 | Loss: 0.00057143
Iteration 14/25 | Loss: 0.00057143
Iteration 15/25 | Loss: 0.00057143
Iteration 16/25 | Loss: 0.00057143
Iteration 17/25 | Loss: 0.00057143
Iteration 18/25 | Loss: 0.00057143
Iteration 19/25 | Loss: 0.00057143
Iteration 20/25 | Loss: 0.00057143
Iteration 21/25 | Loss: 0.00057143
Iteration 22/25 | Loss: 0.00057143
Iteration 23/25 | Loss: 0.00057143
Iteration 24/25 | Loss: 0.00057143
Iteration 25/25 | Loss: 0.00057143

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00057143
Iteration 2/1000 | Loss: 0.00003419
Iteration 3/1000 | Loss: 0.00003006
Iteration 4/1000 | Loss: 0.00002770
Iteration 5/1000 | Loss: 0.00002625
Iteration 6/1000 | Loss: 0.00002539
Iteration 7/1000 | Loss: 0.00002479
Iteration 8/1000 | Loss: 0.00002435
Iteration 9/1000 | Loss: 0.00002413
Iteration 10/1000 | Loss: 0.00002393
Iteration 11/1000 | Loss: 0.00002367
Iteration 12/1000 | Loss: 0.00002366
Iteration 13/1000 | Loss: 0.00002365
Iteration 14/1000 | Loss: 0.00002365
Iteration 15/1000 | Loss: 0.00002347
Iteration 16/1000 | Loss: 0.00002340
Iteration 17/1000 | Loss: 0.00002330
Iteration 18/1000 | Loss: 0.00002330
Iteration 19/1000 | Loss: 0.00002330
Iteration 20/1000 | Loss: 0.00002330
Iteration 21/1000 | Loss: 0.00002329
Iteration 22/1000 | Loss: 0.00002329
Iteration 23/1000 | Loss: 0.00002329
Iteration 24/1000 | Loss: 0.00002329
Iteration 25/1000 | Loss: 0.00002329
Iteration 26/1000 | Loss: 0.00002329
Iteration 27/1000 | Loss: 0.00002329
Iteration 28/1000 | Loss: 0.00002329
Iteration 29/1000 | Loss: 0.00002329
Iteration 30/1000 | Loss: 0.00002329
Iteration 31/1000 | Loss: 0.00002328
Iteration 32/1000 | Loss: 0.00002328
Iteration 33/1000 | Loss: 0.00002328
Iteration 34/1000 | Loss: 0.00002328
Iteration 35/1000 | Loss: 0.00002328
Iteration 36/1000 | Loss: 0.00002328
Iteration 37/1000 | Loss: 0.00002328
Iteration 38/1000 | Loss: 0.00002328
Iteration 39/1000 | Loss: 0.00002328
Iteration 40/1000 | Loss: 0.00002328
Iteration 41/1000 | Loss: 0.00002328
Iteration 42/1000 | Loss: 0.00002328
Iteration 43/1000 | Loss: 0.00002328
Iteration 44/1000 | Loss: 0.00002328
Iteration 45/1000 | Loss: 0.00002328
Iteration 46/1000 | Loss: 0.00002328
Iteration 47/1000 | Loss: 0.00002327
Iteration 48/1000 | Loss: 0.00002327
Iteration 49/1000 | Loss: 0.00002327
Iteration 50/1000 | Loss: 0.00002327
Iteration 51/1000 | Loss: 0.00002327
Iteration 52/1000 | Loss: 0.00002327
Iteration 53/1000 | Loss: 0.00002327
Iteration 54/1000 | Loss: 0.00002327
Iteration 55/1000 | Loss: 0.00002327
Iteration 56/1000 | Loss: 0.00002327
Iteration 57/1000 | Loss: 0.00002326
Iteration 58/1000 | Loss: 0.00002326
Iteration 59/1000 | Loss: 0.00002324
Iteration 60/1000 | Loss: 0.00002323
Iteration 61/1000 | Loss: 0.00002323
Iteration 62/1000 | Loss: 0.00002323
Iteration 63/1000 | Loss: 0.00002323
Iteration 64/1000 | Loss: 0.00002323
Iteration 65/1000 | Loss: 0.00002323
Iteration 66/1000 | Loss: 0.00002323
Iteration 67/1000 | Loss: 0.00002323
Iteration 68/1000 | Loss: 0.00002323
Iteration 69/1000 | Loss: 0.00002323
Iteration 70/1000 | Loss: 0.00002322
Iteration 71/1000 | Loss: 0.00002322
Iteration 72/1000 | Loss: 0.00002322
Iteration 73/1000 | Loss: 0.00002322
Iteration 74/1000 | Loss: 0.00002322
Iteration 75/1000 | Loss: 0.00002321
Iteration 76/1000 | Loss: 0.00002321
Iteration 77/1000 | Loss: 0.00002321
Iteration 78/1000 | Loss: 0.00002321
Iteration 79/1000 | Loss: 0.00002321
Iteration 80/1000 | Loss: 0.00002321
Iteration 81/1000 | Loss: 0.00002321
Iteration 82/1000 | Loss: 0.00002321
Iteration 83/1000 | Loss: 0.00002321
Iteration 84/1000 | Loss: 0.00002321
Iteration 85/1000 | Loss: 0.00002321
Iteration 86/1000 | Loss: 0.00002321
Iteration 87/1000 | Loss: 0.00002321
Iteration 88/1000 | Loss: 0.00002321
Iteration 89/1000 | Loss: 0.00002321
Iteration 90/1000 | Loss: 0.00002321
Iteration 91/1000 | Loss: 0.00002321
Iteration 92/1000 | Loss: 0.00002321
Iteration 93/1000 | Loss: 0.00002321
Iteration 94/1000 | Loss: 0.00002321
Iteration 95/1000 | Loss: 0.00002321
Iteration 96/1000 | Loss: 0.00002321
Iteration 97/1000 | Loss: 0.00002321
Iteration 98/1000 | Loss: 0.00002321
Iteration 99/1000 | Loss: 0.00002321
Iteration 100/1000 | Loss: 0.00002321
Iteration 101/1000 | Loss: 0.00002321
Iteration 102/1000 | Loss: 0.00002321
Iteration 103/1000 | Loss: 0.00002321
Iteration 104/1000 | Loss: 0.00002321
Iteration 105/1000 | Loss: 0.00002321
Iteration 106/1000 | Loss: 0.00002321
Iteration 107/1000 | Loss: 0.00002321
Iteration 108/1000 | Loss: 0.00002321
Iteration 109/1000 | Loss: 0.00002321
Iteration 110/1000 | Loss: 0.00002321
Iteration 111/1000 | Loss: 0.00002321
Iteration 112/1000 | Loss: 0.00002321
Iteration 113/1000 | Loss: 0.00002321
Iteration 114/1000 | Loss: 0.00002321
Iteration 115/1000 | Loss: 0.00002321
Iteration 116/1000 | Loss: 0.00002321
Iteration 117/1000 | Loss: 0.00002321
Iteration 118/1000 | Loss: 0.00002321
Iteration 119/1000 | Loss: 0.00002321
Iteration 120/1000 | Loss: 0.00002321
Iteration 121/1000 | Loss: 0.00002321
Iteration 122/1000 | Loss: 0.00002321
Iteration 123/1000 | Loss: 0.00002321
Iteration 124/1000 | Loss: 0.00002321
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 124. Stopping optimization.
Last 5 losses: [2.3210724975797348e-05, 2.3210724975797348e-05, 2.3210724975797348e-05, 2.3210724975797348e-05, 2.3210724975797348e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3210724975797348e-05

Optimization complete. Final v2v error: 4.14722204208374 mm

Highest mean error: 4.273149013519287 mm for frame 16

Lowest mean error: 3.9735589027404785 mm for frame 142

Saving results

Total time: 31.995139837265015
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_grace_posed_004/1033/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_grace_posed_004/1033.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_grace_posed_004/1033
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00777697
Iteration 2/25 | Loss: 0.00111800
Iteration 3/25 | Loss: 0.00100547
Iteration 4/25 | Loss: 0.00099590
Iteration 5/25 | Loss: 0.00099516
Iteration 6/25 | Loss: 0.00099516
Iteration 7/25 | Loss: 0.00099516
Iteration 8/25 | Loss: 0.00099516
Iteration 9/25 | Loss: 0.00099516
Iteration 10/25 | Loss: 0.00099516
Iteration 11/25 | Loss: 0.00099516
Iteration 12/25 | Loss: 0.00099516
Iteration 13/25 | Loss: 0.00099516
Iteration 14/25 | Loss: 0.00099516
Iteration 15/25 | Loss: 0.00099516
Iteration 16/25 | Loss: 0.00099516
Iteration 17/25 | Loss: 0.00099516
Iteration 18/25 | Loss: 0.00099516
Iteration 19/25 | Loss: 0.00099516
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.000995161710307002, 0.000995161710307002, 0.000995161710307002, 0.000995161710307002, 0.000995161710307002]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000995161710307002

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.31306458
Iteration 2/25 | Loss: 0.00093880
Iteration 3/25 | Loss: 0.00093879
Iteration 4/25 | Loss: 0.00093879
Iteration 5/25 | Loss: 0.00093879
Iteration 6/25 | Loss: 0.00093879
Iteration 7/25 | Loss: 0.00093879
Iteration 8/25 | Loss: 0.00093879
Iteration 9/25 | Loss: 0.00093879
Iteration 10/25 | Loss: 0.00093879
Iteration 11/25 | Loss: 0.00093879
Iteration 12/25 | Loss: 0.00093879
Iteration 13/25 | Loss: 0.00093879
Iteration 14/25 | Loss: 0.00093879
Iteration 15/25 | Loss: 0.00093879
Iteration 16/25 | Loss: 0.00093879
Iteration 17/25 | Loss: 0.00093879
Iteration 18/25 | Loss: 0.00093879
Iteration 19/25 | Loss: 0.00093879
Iteration 20/25 | Loss: 0.00093879
Iteration 21/25 | Loss: 0.00093879
Iteration 22/25 | Loss: 0.00093879
Iteration 23/25 | Loss: 0.00093879
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0009387914324179292, 0.0009387914324179292, 0.0009387914324179292, 0.0009387914324179292, 0.0009387914324179292]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009387914324179292

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00093879
Iteration 2/1000 | Loss: 0.00001553
Iteration 3/1000 | Loss: 0.00001196
Iteration 4/1000 | Loss: 0.00001059
Iteration 5/1000 | Loss: 0.00000988
Iteration 6/1000 | Loss: 0.00000939
Iteration 7/1000 | Loss: 0.00000910
Iteration 8/1000 | Loss: 0.00000884
Iteration 9/1000 | Loss: 0.00000855
Iteration 10/1000 | Loss: 0.00000851
Iteration 11/1000 | Loss: 0.00000848
Iteration 12/1000 | Loss: 0.00000847
Iteration 13/1000 | Loss: 0.00000844
Iteration 14/1000 | Loss: 0.00000839
Iteration 15/1000 | Loss: 0.00000838
Iteration 16/1000 | Loss: 0.00000833
Iteration 17/1000 | Loss: 0.00000827
Iteration 18/1000 | Loss: 0.00000822
Iteration 19/1000 | Loss: 0.00000822
Iteration 20/1000 | Loss: 0.00000822
Iteration 21/1000 | Loss: 0.00000822
Iteration 22/1000 | Loss: 0.00000818
Iteration 23/1000 | Loss: 0.00000817
Iteration 24/1000 | Loss: 0.00000816
Iteration 25/1000 | Loss: 0.00000815
Iteration 26/1000 | Loss: 0.00000815
Iteration 27/1000 | Loss: 0.00000814
Iteration 28/1000 | Loss: 0.00000812
Iteration 29/1000 | Loss: 0.00000812
Iteration 30/1000 | Loss: 0.00000812
Iteration 31/1000 | Loss: 0.00000812
Iteration 32/1000 | Loss: 0.00000812
Iteration 33/1000 | Loss: 0.00000811
Iteration 34/1000 | Loss: 0.00000811
Iteration 35/1000 | Loss: 0.00000811
Iteration 36/1000 | Loss: 0.00000811
Iteration 37/1000 | Loss: 0.00000810
Iteration 38/1000 | Loss: 0.00000810
Iteration 39/1000 | Loss: 0.00000809
Iteration 40/1000 | Loss: 0.00000808
Iteration 41/1000 | Loss: 0.00000807
Iteration 42/1000 | Loss: 0.00000806
Iteration 43/1000 | Loss: 0.00000806
Iteration 44/1000 | Loss: 0.00000804
Iteration 45/1000 | Loss: 0.00000802
Iteration 46/1000 | Loss: 0.00000801
Iteration 47/1000 | Loss: 0.00000797
Iteration 48/1000 | Loss: 0.00000795
Iteration 49/1000 | Loss: 0.00000795
Iteration 50/1000 | Loss: 0.00000795
Iteration 51/1000 | Loss: 0.00000794
Iteration 52/1000 | Loss: 0.00000793
Iteration 53/1000 | Loss: 0.00000791
Iteration 54/1000 | Loss: 0.00000791
Iteration 55/1000 | Loss: 0.00000791
Iteration 56/1000 | Loss: 0.00000791
Iteration 57/1000 | Loss: 0.00000788
Iteration 58/1000 | Loss: 0.00000787
Iteration 59/1000 | Loss: 0.00000786
Iteration 60/1000 | Loss: 0.00000786
Iteration 61/1000 | Loss: 0.00000785
Iteration 62/1000 | Loss: 0.00000785
Iteration 63/1000 | Loss: 0.00000785
Iteration 64/1000 | Loss: 0.00000785
Iteration 65/1000 | Loss: 0.00000784
Iteration 66/1000 | Loss: 0.00000783
Iteration 67/1000 | Loss: 0.00000782
Iteration 68/1000 | Loss: 0.00000782
Iteration 69/1000 | Loss: 0.00000782
Iteration 70/1000 | Loss: 0.00000782
Iteration 71/1000 | Loss: 0.00000782
Iteration 72/1000 | Loss: 0.00000782
Iteration 73/1000 | Loss: 0.00000781
Iteration 74/1000 | Loss: 0.00000781
Iteration 75/1000 | Loss: 0.00000780
Iteration 76/1000 | Loss: 0.00000780
Iteration 77/1000 | Loss: 0.00000780
Iteration 78/1000 | Loss: 0.00000780
Iteration 79/1000 | Loss: 0.00000780
Iteration 80/1000 | Loss: 0.00000779
Iteration 81/1000 | Loss: 0.00000779
Iteration 82/1000 | Loss: 0.00000779
Iteration 83/1000 | Loss: 0.00000779
Iteration 84/1000 | Loss: 0.00000779
Iteration 85/1000 | Loss: 0.00000779
Iteration 86/1000 | Loss: 0.00000779
Iteration 87/1000 | Loss: 0.00000779
Iteration 88/1000 | Loss: 0.00000779
Iteration 89/1000 | Loss: 0.00000779
Iteration 90/1000 | Loss: 0.00000779
Iteration 91/1000 | Loss: 0.00000779
Iteration 92/1000 | Loss: 0.00000779
Iteration 93/1000 | Loss: 0.00000778
Iteration 94/1000 | Loss: 0.00000778
Iteration 95/1000 | Loss: 0.00000778
Iteration 96/1000 | Loss: 0.00000778
Iteration 97/1000 | Loss: 0.00000778
Iteration 98/1000 | Loss: 0.00000778
Iteration 99/1000 | Loss: 0.00000777
Iteration 100/1000 | Loss: 0.00000777
Iteration 101/1000 | Loss: 0.00000777
Iteration 102/1000 | Loss: 0.00000777
Iteration 103/1000 | Loss: 0.00000776
Iteration 104/1000 | Loss: 0.00000776
Iteration 105/1000 | Loss: 0.00000776
Iteration 106/1000 | Loss: 0.00000776
Iteration 107/1000 | Loss: 0.00000776
Iteration 108/1000 | Loss: 0.00000776
Iteration 109/1000 | Loss: 0.00000776
Iteration 110/1000 | Loss: 0.00000776
Iteration 111/1000 | Loss: 0.00000776
Iteration 112/1000 | Loss: 0.00000776
Iteration 113/1000 | Loss: 0.00000776
Iteration 114/1000 | Loss: 0.00000776
Iteration 115/1000 | Loss: 0.00000776
Iteration 116/1000 | Loss: 0.00000776
Iteration 117/1000 | Loss: 0.00000776
Iteration 118/1000 | Loss: 0.00000776
Iteration 119/1000 | Loss: 0.00000776
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 119. Stopping optimization.
Last 5 losses: [7.758839274174534e-06, 7.758839274174534e-06, 7.758839274174534e-06, 7.758839274174534e-06, 7.758839274174534e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 7.758839274174534e-06

Optimization complete. Final v2v error: 2.3969318866729736 mm

Highest mean error: 2.5868282318115234 mm for frame 62

Lowest mean error: 2.2434933185577393 mm for frame 4

Saving results

Total time: 38.619622468948364
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_grace_posed_004/1036/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_grace_posed_004/1036.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_grace_posed_004/1036
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00911779
Iteration 2/25 | Loss: 0.00160172
Iteration 3/25 | Loss: 0.00147915
Iteration 4/25 | Loss: 0.00145405
Iteration 5/25 | Loss: 0.00144717
Iteration 6/25 | Loss: 0.00144201
Iteration 7/25 | Loss: 0.00143918
Iteration 8/25 | Loss: 0.00144076
Iteration 9/25 | Loss: 0.00144055
Iteration 10/25 | Loss: 0.00143657
Iteration 11/25 | Loss: 0.00143519
Iteration 12/25 | Loss: 0.00143471
Iteration 13/25 | Loss: 0.00143457
Iteration 14/25 | Loss: 0.00143454
Iteration 15/25 | Loss: 0.00143454
Iteration 16/25 | Loss: 0.00143453
Iteration 17/25 | Loss: 0.00143453
Iteration 18/25 | Loss: 0.00143453
Iteration 19/25 | Loss: 0.00143453
Iteration 20/25 | Loss: 0.00143453
Iteration 21/25 | Loss: 0.00143453
Iteration 22/25 | Loss: 0.00143453
Iteration 23/25 | Loss: 0.00143453
Iteration 24/25 | Loss: 0.00143453
Iteration 25/25 | Loss: 0.00143453

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.24555099
Iteration 2/25 | Loss: 0.00401811
Iteration 3/25 | Loss: 0.00401802
Iteration 4/25 | Loss: 0.00401802
Iteration 5/25 | Loss: 0.00401802
Iteration 6/25 | Loss: 0.00401802
Iteration 7/25 | Loss: 0.00401802
Iteration 8/25 | Loss: 0.00401802
Iteration 9/25 | Loss: 0.00401802
Iteration 10/25 | Loss: 0.00401802
Iteration 11/25 | Loss: 0.00401802
Iteration 12/25 | Loss: 0.00401802
Iteration 13/25 | Loss: 0.00401802
Iteration 14/25 | Loss: 0.00401802
Iteration 15/25 | Loss: 0.00401802
Iteration 16/25 | Loss: 0.00401802
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.004018016625195742, 0.004018016625195742, 0.004018016625195742, 0.004018016625195742, 0.004018016625195742]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.004018016625195742

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00401802
Iteration 2/1000 | Loss: 0.00038256
Iteration 3/1000 | Loss: 0.00024469
Iteration 4/1000 | Loss: 0.00074140
Iteration 5/1000 | Loss: 0.00103344
Iteration 6/1000 | Loss: 0.00164538
Iteration 7/1000 | Loss: 0.00221678
Iteration 8/1000 | Loss: 0.00030907
Iteration 9/1000 | Loss: 0.00149860
Iteration 10/1000 | Loss: 0.00054543
Iteration 11/1000 | Loss: 0.00015535
Iteration 12/1000 | Loss: 0.00182039
Iteration 13/1000 | Loss: 0.00774264
Iteration 14/1000 | Loss: 0.00466462
Iteration 15/1000 | Loss: 0.00142447
Iteration 16/1000 | Loss: 0.00084949
Iteration 17/1000 | Loss: 0.00017696
Iteration 18/1000 | Loss: 0.00097498
Iteration 19/1000 | Loss: 0.00013146
Iteration 20/1000 | Loss: 0.00121074
Iteration 21/1000 | Loss: 0.00401948
Iteration 22/1000 | Loss: 0.00681842
Iteration 23/1000 | Loss: 0.00366013
Iteration 24/1000 | Loss: 0.00061766
Iteration 25/1000 | Loss: 0.00012364
Iteration 26/1000 | Loss: 0.00047110
Iteration 27/1000 | Loss: 0.00017786
Iteration 28/1000 | Loss: 0.00075196
Iteration 29/1000 | Loss: 0.00009471
Iteration 30/1000 | Loss: 0.00080385
Iteration 31/1000 | Loss: 0.00076016
Iteration 32/1000 | Loss: 0.00042064
Iteration 33/1000 | Loss: 0.00351133
Iteration 34/1000 | Loss: 0.00189418
Iteration 35/1000 | Loss: 0.00034729
Iteration 36/1000 | Loss: 0.00062881
Iteration 37/1000 | Loss: 0.00048400
Iteration 38/1000 | Loss: 0.00040219
Iteration 39/1000 | Loss: 0.00057308
Iteration 40/1000 | Loss: 0.00008518
Iteration 41/1000 | Loss: 0.00006752
Iteration 42/1000 | Loss: 0.00027099
Iteration 43/1000 | Loss: 0.00096000
Iteration 44/1000 | Loss: 0.00062142
Iteration 45/1000 | Loss: 0.00039455
Iteration 46/1000 | Loss: 0.00007039
Iteration 47/1000 | Loss: 0.00037455
Iteration 48/1000 | Loss: 0.00181377
Iteration 49/1000 | Loss: 0.00100272
Iteration 50/1000 | Loss: 0.00104461
Iteration 51/1000 | Loss: 0.00010303
Iteration 52/1000 | Loss: 0.00008328
Iteration 53/1000 | Loss: 0.00032356
Iteration 54/1000 | Loss: 0.00006786
Iteration 55/1000 | Loss: 0.00021277
Iteration 56/1000 | Loss: 0.00025465
Iteration 57/1000 | Loss: 0.00032539
Iteration 58/1000 | Loss: 0.00026743
Iteration 59/1000 | Loss: 0.00124882
Iteration 60/1000 | Loss: 0.00048398
Iteration 61/1000 | Loss: 0.00006494
Iteration 62/1000 | Loss: 0.00056300
Iteration 63/1000 | Loss: 0.00016184
Iteration 64/1000 | Loss: 0.00057410
Iteration 65/1000 | Loss: 0.00046854
Iteration 66/1000 | Loss: 0.00005830
Iteration 67/1000 | Loss: 0.00048547
Iteration 68/1000 | Loss: 0.00022366
Iteration 69/1000 | Loss: 0.00029547
Iteration 70/1000 | Loss: 0.00033583
Iteration 71/1000 | Loss: 0.00036329
Iteration 72/1000 | Loss: 0.00039205
Iteration 73/1000 | Loss: 0.00006515
Iteration 74/1000 | Loss: 0.00018622
Iteration 75/1000 | Loss: 0.00031793
Iteration 76/1000 | Loss: 0.00027585
Iteration 77/1000 | Loss: 0.00010004
Iteration 78/1000 | Loss: 0.00004096
Iteration 79/1000 | Loss: 0.00014138
Iteration 80/1000 | Loss: 0.00005363
Iteration 81/1000 | Loss: 0.00003771
Iteration 82/1000 | Loss: 0.00017448
Iteration 83/1000 | Loss: 0.00012602
Iteration 84/1000 | Loss: 0.00034308
Iteration 85/1000 | Loss: 0.00005870
Iteration 86/1000 | Loss: 0.00003984
Iteration 87/1000 | Loss: 0.00031104
Iteration 88/1000 | Loss: 0.00004491
Iteration 89/1000 | Loss: 0.00003564
Iteration 90/1000 | Loss: 0.00003390
Iteration 91/1000 | Loss: 0.00003245
Iteration 92/1000 | Loss: 0.00003163
Iteration 93/1000 | Loss: 0.00021812
Iteration 94/1000 | Loss: 0.00011038
Iteration 95/1000 | Loss: 0.00003089
Iteration 96/1000 | Loss: 0.00028040
Iteration 97/1000 | Loss: 0.00011615
Iteration 98/1000 | Loss: 0.00003038
Iteration 99/1000 | Loss: 0.00002988
Iteration 100/1000 | Loss: 0.00002962
Iteration 101/1000 | Loss: 0.00002927
Iteration 102/1000 | Loss: 0.00002891
Iteration 103/1000 | Loss: 0.00002865
Iteration 104/1000 | Loss: 0.00002844
Iteration 105/1000 | Loss: 0.00002829
Iteration 106/1000 | Loss: 0.00002828
Iteration 107/1000 | Loss: 0.00002824
Iteration 108/1000 | Loss: 0.00002819
Iteration 109/1000 | Loss: 0.00047265
Iteration 110/1000 | Loss: 0.00003084
Iteration 111/1000 | Loss: 0.00002901
Iteration 112/1000 | Loss: 0.00002816
Iteration 113/1000 | Loss: 0.00002712
Iteration 114/1000 | Loss: 0.00002648
Iteration 115/1000 | Loss: 0.00002619
Iteration 116/1000 | Loss: 0.00002602
Iteration 117/1000 | Loss: 0.00002600
Iteration 118/1000 | Loss: 0.00002599
Iteration 119/1000 | Loss: 0.00002598
Iteration 120/1000 | Loss: 0.00002597
Iteration 121/1000 | Loss: 0.00002597
Iteration 122/1000 | Loss: 0.00002596
Iteration 123/1000 | Loss: 0.00002596
Iteration 124/1000 | Loss: 0.00002595
Iteration 125/1000 | Loss: 0.00002595
Iteration 126/1000 | Loss: 0.00002594
Iteration 127/1000 | Loss: 0.00002594
Iteration 128/1000 | Loss: 0.00002592
Iteration 129/1000 | Loss: 0.00002591
Iteration 130/1000 | Loss: 0.00002590
Iteration 131/1000 | Loss: 0.00002586
Iteration 132/1000 | Loss: 0.00002585
Iteration 133/1000 | Loss: 0.00002584
Iteration 134/1000 | Loss: 0.00002583
Iteration 135/1000 | Loss: 0.00002582
Iteration 136/1000 | Loss: 0.00002582
Iteration 137/1000 | Loss: 0.00002580
Iteration 138/1000 | Loss: 0.00002580
Iteration 139/1000 | Loss: 0.00002579
Iteration 140/1000 | Loss: 0.00002578
Iteration 141/1000 | Loss: 0.00002577
Iteration 142/1000 | Loss: 0.00002576
Iteration 143/1000 | Loss: 0.00002576
Iteration 144/1000 | Loss: 0.00002574
Iteration 145/1000 | Loss: 0.00002573
Iteration 146/1000 | Loss: 0.00002569
Iteration 147/1000 | Loss: 0.00002568
Iteration 148/1000 | Loss: 0.00002566
Iteration 149/1000 | Loss: 0.00002566
Iteration 150/1000 | Loss: 0.00002565
Iteration 151/1000 | Loss: 0.00002563
Iteration 152/1000 | Loss: 0.00002563
Iteration 153/1000 | Loss: 0.00002562
Iteration 154/1000 | Loss: 0.00002562
Iteration 155/1000 | Loss: 0.00002561
Iteration 156/1000 | Loss: 0.00002561
Iteration 157/1000 | Loss: 0.00002559
Iteration 158/1000 | Loss: 0.00002559
Iteration 159/1000 | Loss: 0.00002558
Iteration 160/1000 | Loss: 0.00002558
Iteration 161/1000 | Loss: 0.00002557
Iteration 162/1000 | Loss: 0.00002556
Iteration 163/1000 | Loss: 0.00002556
Iteration 164/1000 | Loss: 0.00002556
Iteration 165/1000 | Loss: 0.00002555
Iteration 166/1000 | Loss: 0.00002554
Iteration 167/1000 | Loss: 0.00002554
Iteration 168/1000 | Loss: 0.00002554
Iteration 169/1000 | Loss: 0.00002554
Iteration 170/1000 | Loss: 0.00002554
Iteration 171/1000 | Loss: 0.00002554
Iteration 172/1000 | Loss: 0.00002554
Iteration 173/1000 | Loss: 0.00002554
Iteration 174/1000 | Loss: 0.00002554
Iteration 175/1000 | Loss: 0.00002553
Iteration 176/1000 | Loss: 0.00002553
Iteration 177/1000 | Loss: 0.00002552
Iteration 178/1000 | Loss: 0.00002552
Iteration 179/1000 | Loss: 0.00002552
Iteration 180/1000 | Loss: 0.00002552
Iteration 181/1000 | Loss: 0.00002552
Iteration 182/1000 | Loss: 0.00002551
Iteration 183/1000 | Loss: 0.00002551
Iteration 184/1000 | Loss: 0.00002551
Iteration 185/1000 | Loss: 0.00002550
Iteration 186/1000 | Loss: 0.00002550
Iteration 187/1000 | Loss: 0.00008149
Iteration 188/1000 | Loss: 0.00002743
Iteration 189/1000 | Loss: 0.00002540
Iteration 190/1000 | Loss: 0.00002485
Iteration 191/1000 | Loss: 0.00002453
Iteration 192/1000 | Loss: 0.00002429
Iteration 193/1000 | Loss: 0.00002425
Iteration 194/1000 | Loss: 0.00002424
Iteration 195/1000 | Loss: 0.00002421
Iteration 196/1000 | Loss: 0.00002421
Iteration 197/1000 | Loss: 0.00002420
Iteration 198/1000 | Loss: 0.00002420
Iteration 199/1000 | Loss: 0.00002419
Iteration 200/1000 | Loss: 0.00002419
Iteration 201/1000 | Loss: 0.00002419
Iteration 202/1000 | Loss: 0.00002418
Iteration 203/1000 | Loss: 0.00002412
Iteration 204/1000 | Loss: 0.00002410
Iteration 205/1000 | Loss: 0.00002410
Iteration 206/1000 | Loss: 0.00002410
Iteration 207/1000 | Loss: 0.00002409
Iteration 208/1000 | Loss: 0.00002409
Iteration 209/1000 | Loss: 0.00002408
Iteration 210/1000 | Loss: 0.00002407
Iteration 211/1000 | Loss: 0.00002407
Iteration 212/1000 | Loss: 0.00002406
Iteration 213/1000 | Loss: 0.00002406
Iteration 214/1000 | Loss: 0.00002405
Iteration 215/1000 | Loss: 0.00002405
Iteration 216/1000 | Loss: 0.00002404
Iteration 217/1000 | Loss: 0.00002404
Iteration 218/1000 | Loss: 0.00002403
Iteration 219/1000 | Loss: 0.00002403
Iteration 220/1000 | Loss: 0.00002402
Iteration 221/1000 | Loss: 0.00002401
Iteration 222/1000 | Loss: 0.00002401
Iteration 223/1000 | Loss: 0.00002400
Iteration 224/1000 | Loss: 0.00002400
Iteration 225/1000 | Loss: 0.00002399
Iteration 226/1000 | Loss: 0.00002398
Iteration 227/1000 | Loss: 0.00002397
Iteration 228/1000 | Loss: 0.00002397
Iteration 229/1000 | Loss: 0.00002392
Iteration 230/1000 | Loss: 0.00002390
Iteration 231/1000 | Loss: 0.00002389
Iteration 232/1000 | Loss: 0.00002389
Iteration 233/1000 | Loss: 0.00002388
Iteration 234/1000 | Loss: 0.00002387
Iteration 235/1000 | Loss: 0.00002387
Iteration 236/1000 | Loss: 0.00002387
Iteration 237/1000 | Loss: 0.00002387
Iteration 238/1000 | Loss: 0.00002386
Iteration 239/1000 | Loss: 0.00002386
Iteration 240/1000 | Loss: 0.00002386
Iteration 241/1000 | Loss: 0.00002386
Iteration 242/1000 | Loss: 0.00002386
Iteration 243/1000 | Loss: 0.00002386
Iteration 244/1000 | Loss: 0.00002386
Iteration 245/1000 | Loss: 0.00002386
Iteration 246/1000 | Loss: 0.00002386
Iteration 247/1000 | Loss: 0.00002385
Iteration 248/1000 | Loss: 0.00002385
Iteration 249/1000 | Loss: 0.00002385
Iteration 250/1000 | Loss: 0.00002385
Iteration 251/1000 | Loss: 0.00002385
Iteration 252/1000 | Loss: 0.00002385
Iteration 253/1000 | Loss: 0.00002384
Iteration 254/1000 | Loss: 0.00002384
Iteration 255/1000 | Loss: 0.00002384
Iteration 256/1000 | Loss: 0.00002383
Iteration 257/1000 | Loss: 0.00002383
Iteration 258/1000 | Loss: 0.00002383
Iteration 259/1000 | Loss: 0.00002383
Iteration 260/1000 | Loss: 0.00002382
Iteration 261/1000 | Loss: 0.00002382
Iteration 262/1000 | Loss: 0.00002382
Iteration 263/1000 | Loss: 0.00002382
Iteration 264/1000 | Loss: 0.00002382
Iteration 265/1000 | Loss: 0.00002381
Iteration 266/1000 | Loss: 0.00002381
Iteration 267/1000 | Loss: 0.00002381
Iteration 268/1000 | Loss: 0.00002381
Iteration 269/1000 | Loss: 0.00002380
Iteration 270/1000 | Loss: 0.00002380
Iteration 271/1000 | Loss: 0.00002380
Iteration 272/1000 | Loss: 0.00002380
Iteration 273/1000 | Loss: 0.00002380
Iteration 274/1000 | Loss: 0.00002379
Iteration 275/1000 | Loss: 0.00002379
Iteration 276/1000 | Loss: 0.00002379
Iteration 277/1000 | Loss: 0.00002379
Iteration 278/1000 | Loss: 0.00002379
Iteration 279/1000 | Loss: 0.00002379
Iteration 280/1000 | Loss: 0.00002379
Iteration 281/1000 | Loss: 0.00002379
Iteration 282/1000 | Loss: 0.00002378
Iteration 283/1000 | Loss: 0.00002378
Iteration 284/1000 | Loss: 0.00002378
Iteration 285/1000 | Loss: 0.00002378
Iteration 286/1000 | Loss: 0.00002378
Iteration 287/1000 | Loss: 0.00002378
Iteration 288/1000 | Loss: 0.00002377
Iteration 289/1000 | Loss: 0.00002377
Iteration 290/1000 | Loss: 0.00002377
Iteration 291/1000 | Loss: 0.00002376
Iteration 292/1000 | Loss: 0.00002376
Iteration 293/1000 | Loss: 0.00002376
Iteration 294/1000 | Loss: 0.00002376
Iteration 295/1000 | Loss: 0.00002376
Iteration 296/1000 | Loss: 0.00002375
Iteration 297/1000 | Loss: 0.00002375
Iteration 298/1000 | Loss: 0.00002375
Iteration 299/1000 | Loss: 0.00002374
Iteration 300/1000 | Loss: 0.00002374
Iteration 301/1000 | Loss: 0.00002374
Iteration 302/1000 | Loss: 0.00002374
Iteration 303/1000 | Loss: 0.00002374
Iteration 304/1000 | Loss: 0.00002374
Iteration 305/1000 | Loss: 0.00002374
Iteration 306/1000 | Loss: 0.00002374
Iteration 307/1000 | Loss: 0.00002374
Iteration 308/1000 | Loss: 0.00002374
Iteration 309/1000 | Loss: 0.00002374
Iteration 310/1000 | Loss: 0.00002373
Iteration 311/1000 | Loss: 0.00002373
Iteration 312/1000 | Loss: 0.00002373
Iteration 313/1000 | Loss: 0.00002373
Iteration 314/1000 | Loss: 0.00002373
Iteration 315/1000 | Loss: 0.00002373
Iteration 316/1000 | Loss: 0.00002373
Iteration 317/1000 | Loss: 0.00002373
Iteration 318/1000 | Loss: 0.00002372
Iteration 319/1000 | Loss: 0.00002372
Iteration 320/1000 | Loss: 0.00002372
Iteration 321/1000 | Loss: 0.00002371
Iteration 322/1000 | Loss: 0.00002371
Iteration 323/1000 | Loss: 0.00002371
Iteration 324/1000 | Loss: 0.00002371
Iteration 325/1000 | Loss: 0.00002371
Iteration 326/1000 | Loss: 0.00002371
Iteration 327/1000 | Loss: 0.00002370
Iteration 328/1000 | Loss: 0.00002370
Iteration 329/1000 | Loss: 0.00002370
Iteration 330/1000 | Loss: 0.00002370
Iteration 331/1000 | Loss: 0.00002370
Iteration 332/1000 | Loss: 0.00002370
Iteration 333/1000 | Loss: 0.00002369
Iteration 334/1000 | Loss: 0.00002369
Iteration 335/1000 | Loss: 0.00002369
Iteration 336/1000 | Loss: 0.00002369
Iteration 337/1000 | Loss: 0.00002369
Iteration 338/1000 | Loss: 0.00002369
Iteration 339/1000 | Loss: 0.00002369
Iteration 340/1000 | Loss: 0.00002369
Iteration 341/1000 | Loss: 0.00002369
Iteration 342/1000 | Loss: 0.00002369
Iteration 343/1000 | Loss: 0.00002369
Iteration 344/1000 | Loss: 0.00002369
Iteration 345/1000 | Loss: 0.00002368
Iteration 346/1000 | Loss: 0.00002368
Iteration 347/1000 | Loss: 0.00002368
Iteration 348/1000 | Loss: 0.00002368
Iteration 349/1000 | Loss: 0.00002368
Iteration 350/1000 | Loss: 0.00002368
Iteration 351/1000 | Loss: 0.00002368
Iteration 352/1000 | Loss: 0.00002368
Iteration 353/1000 | Loss: 0.00002368
Iteration 354/1000 | Loss: 0.00002367
Iteration 355/1000 | Loss: 0.00002367
Iteration 356/1000 | Loss: 0.00002367
Iteration 357/1000 | Loss: 0.00002367
Iteration 358/1000 | Loss: 0.00002367
Iteration 359/1000 | Loss: 0.00002367
Iteration 360/1000 | Loss: 0.00002367
Iteration 361/1000 | Loss: 0.00002367
Iteration 362/1000 | Loss: 0.00002367
Iteration 363/1000 | Loss: 0.00002367
Iteration 364/1000 | Loss: 0.00002367
Iteration 365/1000 | Loss: 0.00002367
Iteration 366/1000 | Loss: 0.00002366
Iteration 367/1000 | Loss: 0.00002366
Iteration 368/1000 | Loss: 0.00002366
Iteration 369/1000 | Loss: 0.00002366
Iteration 370/1000 | Loss: 0.00002366
Iteration 371/1000 | Loss: 0.00002366
Iteration 372/1000 | Loss: 0.00002366
Iteration 373/1000 | Loss: 0.00002366
Iteration 374/1000 | Loss: 0.00002366
Iteration 375/1000 | Loss: 0.00002366
Iteration 376/1000 | Loss: 0.00002366
Iteration 377/1000 | Loss: 0.00002366
Iteration 378/1000 | Loss: 0.00002366
Iteration 379/1000 | Loss: 0.00002366
Iteration 380/1000 | Loss: 0.00002366
Iteration 381/1000 | Loss: 0.00002366
Iteration 382/1000 | Loss: 0.00002366
Iteration 383/1000 | Loss: 0.00002366
Iteration 384/1000 | Loss: 0.00002366
Iteration 385/1000 | Loss: 0.00002366
Iteration 386/1000 | Loss: 0.00002366
Iteration 387/1000 | Loss: 0.00002366
Iteration 388/1000 | Loss: 0.00002366
Iteration 389/1000 | Loss: 0.00002365
Iteration 390/1000 | Loss: 0.00002365
Iteration 391/1000 | Loss: 0.00002365
Iteration 392/1000 | Loss: 0.00002365
Iteration 393/1000 | Loss: 0.00002365
Iteration 394/1000 | Loss: 0.00002365
Iteration 395/1000 | Loss: 0.00002365
Iteration 396/1000 | Loss: 0.00002365
Iteration 397/1000 | Loss: 0.00002365
Iteration 398/1000 | Loss: 0.00002365
Iteration 399/1000 | Loss: 0.00002365
Iteration 400/1000 | Loss: 0.00002365
Iteration 401/1000 | Loss: 0.00002365
Iteration 402/1000 | Loss: 0.00002365
Iteration 403/1000 | Loss: 0.00002365
Iteration 404/1000 | Loss: 0.00002365
Iteration 405/1000 | Loss: 0.00002365
Iteration 406/1000 | Loss: 0.00002365
Iteration 407/1000 | Loss: 0.00002365
Iteration 408/1000 | Loss: 0.00002365
Iteration 409/1000 | Loss: 0.00002365
Iteration 410/1000 | Loss: 0.00002365
Iteration 411/1000 | Loss: 0.00002365
Iteration 412/1000 | Loss: 0.00002365
Iteration 413/1000 | Loss: 0.00002365
Iteration 414/1000 | Loss: 0.00002365
Iteration 415/1000 | Loss: 0.00002365
Iteration 416/1000 | Loss: 0.00002365
Iteration 417/1000 | Loss: 0.00002365
Iteration 418/1000 | Loss: 0.00002365
Iteration 419/1000 | Loss: 0.00002365
Iteration 420/1000 | Loss: 0.00002365
Iteration 421/1000 | Loss: 0.00002365
Iteration 422/1000 | Loss: 0.00002365
Iteration 423/1000 | Loss: 0.00002365
Iteration 424/1000 | Loss: 0.00002365
Iteration 425/1000 | Loss: 0.00002365
Iteration 426/1000 | Loss: 0.00002365
Iteration 427/1000 | Loss: 0.00002365
Iteration 428/1000 | Loss: 0.00002365
Iteration 429/1000 | Loss: 0.00002365
Iteration 430/1000 | Loss: 0.00002365
Iteration 431/1000 | Loss: 0.00002365
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 431. Stopping optimization.
Last 5 losses: [2.3647407942917198e-05, 2.3647407942917198e-05, 2.3647407942917198e-05, 2.3647407942917198e-05, 2.3647407942917198e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3647407942917198e-05

Optimization complete. Final v2v error: 3.7924931049346924 mm

Highest mean error: 5.30350399017334 mm for frame 175

Lowest mean error: 2.6747636795043945 mm for frame 2

Saving results

Total time: 243.9104197025299
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_grace_posed_004/1090/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_grace_posed_004/1090.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_grace_posed_004/1090
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00833080
Iteration 2/25 | Loss: 0.00143557
Iteration 3/25 | Loss: 0.00116888
Iteration 4/25 | Loss: 0.00112578
Iteration 5/25 | Loss: 0.00111523
Iteration 6/25 | Loss: 0.00109660
Iteration 7/25 | Loss: 0.00108910
Iteration 8/25 | Loss: 0.00108527
Iteration 9/25 | Loss: 0.00108575
Iteration 10/25 | Loss: 0.00108044
Iteration 11/25 | Loss: 0.00107818
Iteration 12/25 | Loss: 0.00107923
Iteration 13/25 | Loss: 0.00107337
Iteration 14/25 | Loss: 0.00107174
Iteration 15/25 | Loss: 0.00107119
Iteration 16/25 | Loss: 0.00107098
Iteration 17/25 | Loss: 0.00107095
Iteration 18/25 | Loss: 0.00107095
Iteration 19/25 | Loss: 0.00107094
Iteration 20/25 | Loss: 0.00107094
Iteration 21/25 | Loss: 0.00107094
Iteration 22/25 | Loss: 0.00107094
Iteration 23/25 | Loss: 0.00107094
Iteration 24/25 | Loss: 0.00107093
Iteration 25/25 | Loss: 0.00107093

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.88136113
Iteration 2/25 | Loss: 0.00100797
Iteration 3/25 | Loss: 0.00100796
Iteration 4/25 | Loss: 0.00100795
Iteration 5/25 | Loss: 0.00100795
Iteration 6/25 | Loss: 0.00100795
Iteration 7/25 | Loss: 0.00100795
Iteration 8/25 | Loss: 0.00100795
Iteration 9/25 | Loss: 0.00100795
Iteration 10/25 | Loss: 0.00100795
Iteration 11/25 | Loss: 0.00100795
Iteration 12/25 | Loss: 0.00100795
Iteration 13/25 | Loss: 0.00100795
Iteration 14/25 | Loss: 0.00100795
Iteration 15/25 | Loss: 0.00100795
Iteration 16/25 | Loss: 0.00100795
Iteration 17/25 | Loss: 0.00100795
Iteration 18/25 | Loss: 0.00100795
Iteration 19/25 | Loss: 0.00100795
Iteration 20/25 | Loss: 0.00100795
Iteration 21/25 | Loss: 0.00100795
Iteration 22/25 | Loss: 0.00100795
Iteration 23/25 | Loss: 0.00100795
Iteration 24/25 | Loss: 0.00100795
Iteration 25/25 | Loss: 0.00100795

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00100795
Iteration 2/1000 | Loss: 0.00002314
Iteration 3/1000 | Loss: 0.00001524
Iteration 4/1000 | Loss: 0.00001375
Iteration 5/1000 | Loss: 0.00001697
Iteration 6/1000 | Loss: 0.00001582
Iteration 7/1000 | Loss: 0.00003338
Iteration 8/1000 | Loss: 0.00001528
Iteration 9/1000 | Loss: 0.00001266
Iteration 10/1000 | Loss: 0.00001237
Iteration 11/1000 | Loss: 0.00001231
Iteration 12/1000 | Loss: 0.00001221
Iteration 13/1000 | Loss: 0.00001214
Iteration 14/1000 | Loss: 0.00001209
Iteration 15/1000 | Loss: 0.00001208
Iteration 16/1000 | Loss: 0.00001207
Iteration 17/1000 | Loss: 0.00001206
Iteration 18/1000 | Loss: 0.00001206
Iteration 19/1000 | Loss: 0.00001204
Iteration 20/1000 | Loss: 0.00001204
Iteration 21/1000 | Loss: 0.00001201
Iteration 22/1000 | Loss: 0.00001201
Iteration 23/1000 | Loss: 0.00001201
Iteration 24/1000 | Loss: 0.00001201
Iteration 25/1000 | Loss: 0.00001200
Iteration 26/1000 | Loss: 0.00001200
Iteration 27/1000 | Loss: 0.00001200
Iteration 28/1000 | Loss: 0.00001200
Iteration 29/1000 | Loss: 0.00001200
Iteration 30/1000 | Loss: 0.00001200
Iteration 31/1000 | Loss: 0.00001200
Iteration 32/1000 | Loss: 0.00001200
Iteration 33/1000 | Loss: 0.00001200
Iteration 34/1000 | Loss: 0.00001200
Iteration 35/1000 | Loss: 0.00001199
Iteration 36/1000 | Loss: 0.00001199
Iteration 37/1000 | Loss: 0.00001198
Iteration 38/1000 | Loss: 0.00001197
Iteration 39/1000 | Loss: 0.00001197
Iteration 40/1000 | Loss: 0.00001197
Iteration 41/1000 | Loss: 0.00001196
Iteration 42/1000 | Loss: 0.00001196
Iteration 43/1000 | Loss: 0.00001195
Iteration 44/1000 | Loss: 0.00001195
Iteration 45/1000 | Loss: 0.00001195
Iteration 46/1000 | Loss: 0.00001194
Iteration 47/1000 | Loss: 0.00001194
Iteration 48/1000 | Loss: 0.00001194
Iteration 49/1000 | Loss: 0.00001194
Iteration 50/1000 | Loss: 0.00001194
Iteration 51/1000 | Loss: 0.00001194
Iteration 52/1000 | Loss: 0.00001194
Iteration 53/1000 | Loss: 0.00001193
Iteration 54/1000 | Loss: 0.00001193
Iteration 55/1000 | Loss: 0.00001188
Iteration 56/1000 | Loss: 0.00001188
Iteration 57/1000 | Loss: 0.00001188
Iteration 58/1000 | Loss: 0.00001188
Iteration 59/1000 | Loss: 0.00001187
Iteration 60/1000 | Loss: 0.00001186
Iteration 61/1000 | Loss: 0.00001185
Iteration 62/1000 | Loss: 0.00001185
Iteration 63/1000 | Loss: 0.00001184
Iteration 64/1000 | Loss: 0.00001184
Iteration 65/1000 | Loss: 0.00001183
Iteration 66/1000 | Loss: 0.00001183
Iteration 67/1000 | Loss: 0.00001182
Iteration 68/1000 | Loss: 0.00001182
Iteration 69/1000 | Loss: 0.00001182
Iteration 70/1000 | Loss: 0.00001182
Iteration 71/1000 | Loss: 0.00001182
Iteration 72/1000 | Loss: 0.00001182
Iteration 73/1000 | Loss: 0.00001181
Iteration 74/1000 | Loss: 0.00001181
Iteration 75/1000 | Loss: 0.00001181
Iteration 76/1000 | Loss: 0.00001181
Iteration 77/1000 | Loss: 0.00001181
Iteration 78/1000 | Loss: 0.00001180
Iteration 79/1000 | Loss: 0.00001180
Iteration 80/1000 | Loss: 0.00001180
Iteration 81/1000 | Loss: 0.00001180
Iteration 82/1000 | Loss: 0.00001180
Iteration 83/1000 | Loss: 0.00001180
Iteration 84/1000 | Loss: 0.00001180
Iteration 85/1000 | Loss: 0.00001180
Iteration 86/1000 | Loss: 0.00001180
Iteration 87/1000 | Loss: 0.00001180
Iteration 88/1000 | Loss: 0.00001179
Iteration 89/1000 | Loss: 0.00001179
Iteration 90/1000 | Loss: 0.00001179
Iteration 91/1000 | Loss: 0.00001179
Iteration 92/1000 | Loss: 0.00001179
Iteration 93/1000 | Loss: 0.00001179
Iteration 94/1000 | Loss: 0.00001179
Iteration 95/1000 | Loss: 0.00001179
Iteration 96/1000 | Loss: 0.00001179
Iteration 97/1000 | Loss: 0.00001178
Iteration 98/1000 | Loss: 0.00001178
Iteration 99/1000 | Loss: 0.00001178
Iteration 100/1000 | Loss: 0.00001178
Iteration 101/1000 | Loss: 0.00001178
Iteration 102/1000 | Loss: 0.00001178
Iteration 103/1000 | Loss: 0.00001178
Iteration 104/1000 | Loss: 0.00001178
Iteration 105/1000 | Loss: 0.00001178
Iteration 106/1000 | Loss: 0.00001178
Iteration 107/1000 | Loss: 0.00001178
Iteration 108/1000 | Loss: 0.00001178
Iteration 109/1000 | Loss: 0.00001178
Iteration 110/1000 | Loss: 0.00001178
Iteration 111/1000 | Loss: 0.00001178
Iteration 112/1000 | Loss: 0.00001178
Iteration 113/1000 | Loss: 0.00001178
Iteration 114/1000 | Loss: 0.00001178
Iteration 115/1000 | Loss: 0.00001178
Iteration 116/1000 | Loss: 0.00001178
Iteration 117/1000 | Loss: 0.00001178
Iteration 118/1000 | Loss: 0.00001178
Iteration 119/1000 | Loss: 0.00001178
Iteration 120/1000 | Loss: 0.00001178
Iteration 121/1000 | Loss: 0.00001178
Iteration 122/1000 | Loss: 0.00001178
Iteration 123/1000 | Loss: 0.00001178
Iteration 124/1000 | Loss: 0.00001178
Iteration 125/1000 | Loss: 0.00001178
Iteration 126/1000 | Loss: 0.00001178
Iteration 127/1000 | Loss: 0.00001178
Iteration 128/1000 | Loss: 0.00001178
Iteration 129/1000 | Loss: 0.00001178
Iteration 130/1000 | Loss: 0.00001178
Iteration 131/1000 | Loss: 0.00001178
Iteration 132/1000 | Loss: 0.00001178
Iteration 133/1000 | Loss: 0.00001178
Iteration 134/1000 | Loss: 0.00001178
Iteration 135/1000 | Loss: 0.00001178
Iteration 136/1000 | Loss: 0.00001178
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 136. Stopping optimization.
Last 5 losses: [1.177614012703998e-05, 1.177614012703998e-05, 1.177614012703998e-05, 1.177614012703998e-05, 1.177614012703998e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.177614012703998e-05

Optimization complete. Final v2v error: 2.9091978073120117 mm

Highest mean error: 3.330834150314331 mm for frame 171

Lowest mean error: 2.4613211154937744 mm for frame 92

Saving results

Total time: 62.20375919342041
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_grace_posed_004/1072/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_grace_posed_004/1072.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_grace_posed_004/1072
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00819321
Iteration 2/25 | Loss: 0.00134835
Iteration 3/25 | Loss: 0.00110711
Iteration 4/25 | Loss: 0.00106682
Iteration 5/25 | Loss: 0.00105611
Iteration 6/25 | Loss: 0.00105260
Iteration 7/25 | Loss: 0.00104999
Iteration 8/25 | Loss: 0.00105262
Iteration 9/25 | Loss: 0.00105732
Iteration 10/25 | Loss: 0.00105577
Iteration 11/25 | Loss: 0.00105784
Iteration 12/25 | Loss: 0.00105663
Iteration 13/25 | Loss: 0.00105743
Iteration 14/25 | Loss: 0.00105634
Iteration 15/25 | Loss: 0.00105676
Iteration 16/25 | Loss: 0.00105940
Iteration 17/25 | Loss: 0.00105782
Iteration 18/25 | Loss: 0.00105820
Iteration 19/25 | Loss: 0.00105813
Iteration 20/25 | Loss: 0.00105634
Iteration 21/25 | Loss: 0.00105677
Iteration 22/25 | Loss: 0.00105703
Iteration 23/25 | Loss: 0.00105579
Iteration 24/25 | Loss: 0.00105767
Iteration 25/25 | Loss: 0.00105921

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.31191337
Iteration 2/25 | Loss: 0.00116231
Iteration 3/25 | Loss: 0.00116231
Iteration 4/25 | Loss: 0.00116231
Iteration 5/25 | Loss: 0.00116231
Iteration 6/25 | Loss: 0.00116231
Iteration 7/25 | Loss: 0.00116231
Iteration 8/25 | Loss: 0.00116231
Iteration 9/25 | Loss: 0.00116231
Iteration 10/25 | Loss: 0.00116231
Iteration 11/25 | Loss: 0.00116231
Iteration 12/25 | Loss: 0.00116230
Iteration 13/25 | Loss: 0.00116230
Iteration 14/25 | Loss: 0.00116230
Iteration 15/25 | Loss: 0.00116230
Iteration 16/25 | Loss: 0.00116230
Iteration 17/25 | Loss: 0.00116230
Iteration 18/25 | Loss: 0.00116230
Iteration 19/25 | Loss: 0.00116230
Iteration 20/25 | Loss: 0.00116230
Iteration 21/25 | Loss: 0.00116230
Iteration 22/25 | Loss: 0.00116230
Iteration 23/25 | Loss: 0.00116230
Iteration 24/25 | Loss: 0.00116230
Iteration 25/25 | Loss: 0.00116230

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00116230
Iteration 2/1000 | Loss: 0.00008912
Iteration 3/1000 | Loss: 0.00008274
Iteration 4/1000 | Loss: 0.00007573
Iteration 5/1000 | Loss: 0.00010514
Iteration 6/1000 | Loss: 0.00008108
Iteration 7/1000 | Loss: 0.00013542
Iteration 8/1000 | Loss: 0.00010015
Iteration 9/1000 | Loss: 0.00010785
Iteration 10/1000 | Loss: 0.00013124
Iteration 11/1000 | Loss: 0.00008077
Iteration 12/1000 | Loss: 0.00008551
Iteration 13/1000 | Loss: 0.00008375
Iteration 14/1000 | Loss: 0.00008461
Iteration 15/1000 | Loss: 0.00010197
Iteration 16/1000 | Loss: 0.00010819
Iteration 17/1000 | Loss: 0.00010437
Iteration 18/1000 | Loss: 0.00007600
Iteration 19/1000 | Loss: 0.00006431
Iteration 20/1000 | Loss: 0.00010723
Iteration 21/1000 | Loss: 0.00012615
Iteration 22/1000 | Loss: 0.00007585
Iteration 23/1000 | Loss: 0.00004973
Iteration 24/1000 | Loss: 0.00004941
Iteration 25/1000 | Loss: 0.00006208
Iteration 26/1000 | Loss: 0.00007644
Iteration 27/1000 | Loss: 0.00008374
Iteration 28/1000 | Loss: 0.00008604
Iteration 29/1000 | Loss: 0.00012541
Iteration 30/1000 | Loss: 0.00011178
Iteration 31/1000 | Loss: 0.00008185
Iteration 32/1000 | Loss: 0.00009300
Iteration 33/1000 | Loss: 0.00007470
Iteration 34/1000 | Loss: 0.00009420
Iteration 35/1000 | Loss: 0.00010368
Iteration 36/1000 | Loss: 0.00007998
Iteration 37/1000 | Loss: 0.00007899
Iteration 38/1000 | Loss: 0.00008562
Iteration 39/1000 | Loss: 0.00011665
Iteration 40/1000 | Loss: 0.00007365
Iteration 41/1000 | Loss: 0.00008203
Iteration 42/1000 | Loss: 0.00008931
Iteration 43/1000 | Loss: 0.00009980
Iteration 44/1000 | Loss: 0.00014092
Iteration 45/1000 | Loss: 0.00007229
Iteration 46/1000 | Loss: 0.00007730
Iteration 47/1000 | Loss: 0.00007217
Iteration 48/1000 | Loss: 0.00008753
Iteration 49/1000 | Loss: 0.00007649
Iteration 50/1000 | Loss: 0.00008807
Iteration 51/1000 | Loss: 0.00009119
Iteration 52/1000 | Loss: 0.00004651
Iteration 53/1000 | Loss: 0.00004588
Iteration 54/1000 | Loss: 0.00008436
Iteration 55/1000 | Loss: 0.00010280
Iteration 56/1000 | Loss: 0.00005868
Iteration 57/1000 | Loss: 0.00006577
Iteration 58/1000 | Loss: 0.00007539
Iteration 59/1000 | Loss: 0.00006128
Iteration 60/1000 | Loss: 0.00002839
Iteration 61/1000 | Loss: 0.00003524
Iteration 62/1000 | Loss: 0.00004017
Iteration 63/1000 | Loss: 0.00004969
Iteration 64/1000 | Loss: 0.00004678
Iteration 65/1000 | Loss: 0.00019727
Iteration 66/1000 | Loss: 0.00014861
Iteration 67/1000 | Loss: 0.00018075
Iteration 68/1000 | Loss: 0.00007561
Iteration 69/1000 | Loss: 0.00004892
Iteration 70/1000 | Loss: 0.00003930
Iteration 71/1000 | Loss: 0.00004863
Iteration 72/1000 | Loss: 0.00004697
Iteration 73/1000 | Loss: 0.00007120
Iteration 74/1000 | Loss: 0.00003844
Iteration 75/1000 | Loss: 0.00006509
Iteration 76/1000 | Loss: 0.00004704
Iteration 77/1000 | Loss: 0.00005457
Iteration 78/1000 | Loss: 0.00008235
Iteration 79/1000 | Loss: 0.00014177
Iteration 80/1000 | Loss: 0.00016156
Iteration 81/1000 | Loss: 0.00005127
Iteration 82/1000 | Loss: 0.00002396
Iteration 83/1000 | Loss: 0.00005201
Iteration 84/1000 | Loss: 0.00004126
Iteration 85/1000 | Loss: 0.00004358
Iteration 86/1000 | Loss: 0.00003658
Iteration 87/1000 | Loss: 0.00004423
Iteration 88/1000 | Loss: 0.00005447
Iteration 89/1000 | Loss: 0.00003612
Iteration 90/1000 | Loss: 0.00019825
Iteration 91/1000 | Loss: 0.00010337
Iteration 92/1000 | Loss: 0.00004127
Iteration 93/1000 | Loss: 0.00003499
Iteration 94/1000 | Loss: 0.00018714
Iteration 95/1000 | Loss: 0.00006018
Iteration 96/1000 | Loss: 0.00005364
Iteration 97/1000 | Loss: 0.00006605
Iteration 98/1000 | Loss: 0.00005064
Iteration 99/1000 | Loss: 0.00004238
Iteration 100/1000 | Loss: 0.00006423
Iteration 101/1000 | Loss: 0.00006756
Iteration 102/1000 | Loss: 0.00002437
Iteration 103/1000 | Loss: 0.00003404
Iteration 104/1000 | Loss: 0.00002332
Iteration 105/1000 | Loss: 0.00004886
Iteration 106/1000 | Loss: 0.00004598
Iteration 107/1000 | Loss: 0.00002413
Iteration 108/1000 | Loss: 0.00003300
Iteration 109/1000 | Loss: 0.00003782
Iteration 110/1000 | Loss: 0.00004554
Iteration 111/1000 | Loss: 0.00004817
Iteration 112/1000 | Loss: 0.00004373
Iteration 113/1000 | Loss: 0.00004442
Iteration 114/1000 | Loss: 0.00004839
Iteration 115/1000 | Loss: 0.00004503
Iteration 116/1000 | Loss: 0.00006271
Iteration 117/1000 | Loss: 0.00004567
Iteration 118/1000 | Loss: 0.00005456
Iteration 119/1000 | Loss: 0.00005712
Iteration 120/1000 | Loss: 0.00005286
Iteration 121/1000 | Loss: 0.00007360
Iteration 122/1000 | Loss: 0.00014297
Iteration 123/1000 | Loss: 0.00005219
Iteration 124/1000 | Loss: 0.00006130
Iteration 125/1000 | Loss: 0.00005293
Iteration 126/1000 | Loss: 0.00004806
Iteration 127/1000 | Loss: 0.00004212
Iteration 128/1000 | Loss: 0.00004616
Iteration 129/1000 | Loss: 0.00004038
Iteration 130/1000 | Loss: 0.00005588
Iteration 131/1000 | Loss: 0.00004316
Iteration 132/1000 | Loss: 0.00003990
Iteration 133/1000 | Loss: 0.00005419
Iteration 134/1000 | Loss: 0.00004973
Iteration 135/1000 | Loss: 0.00005227
Iteration 136/1000 | Loss: 0.00014970
Iteration 137/1000 | Loss: 0.00006959
Iteration 138/1000 | Loss: 0.00004791
Iteration 139/1000 | Loss: 0.00004675
Iteration 140/1000 | Loss: 0.00003956
Iteration 141/1000 | Loss: 0.00005671
Iteration 142/1000 | Loss: 0.00005260
Iteration 143/1000 | Loss: 0.00004670
Iteration 144/1000 | Loss: 0.00004877
Iteration 145/1000 | Loss: 0.00004389
Iteration 146/1000 | Loss: 0.00004594
Iteration 147/1000 | Loss: 0.00005229
Iteration 148/1000 | Loss: 0.00009012
Iteration 149/1000 | Loss: 0.00008698
Iteration 150/1000 | Loss: 0.00008931
Iteration 151/1000 | Loss: 0.00006092
Iteration 152/1000 | Loss: 0.00005781
Iteration 153/1000 | Loss: 0.00004523
Iteration 154/1000 | Loss: 0.00005583
Iteration 155/1000 | Loss: 0.00004703
Iteration 156/1000 | Loss: 0.00004762
Iteration 157/1000 | Loss: 0.00004555
Iteration 158/1000 | Loss: 0.00003758
Iteration 159/1000 | Loss: 0.00003168
Iteration 160/1000 | Loss: 0.00002915
Iteration 161/1000 | Loss: 0.00003758
Iteration 162/1000 | Loss: 0.00003904
Iteration 163/1000 | Loss: 0.00003330
Iteration 164/1000 | Loss: 0.00002723
Iteration 165/1000 | Loss: 0.00003253
Iteration 166/1000 | Loss: 0.00004427
Iteration 167/1000 | Loss: 0.00003750
Iteration 168/1000 | Loss: 0.00003459
Iteration 169/1000 | Loss: 0.00001605
Iteration 170/1000 | Loss: 0.00004134
Iteration 171/1000 | Loss: 0.00002419
Iteration 172/1000 | Loss: 0.00003032
Iteration 173/1000 | Loss: 0.00003064
Iteration 174/1000 | Loss: 0.00002688
Iteration 175/1000 | Loss: 0.00002996
Iteration 176/1000 | Loss: 0.00003822
Iteration 177/1000 | Loss: 0.00002790
Iteration 178/1000 | Loss: 0.00004174
Iteration 179/1000 | Loss: 0.00005638
Iteration 180/1000 | Loss: 0.00004987
Iteration 181/1000 | Loss: 0.00003081
Iteration 182/1000 | Loss: 0.00003082
Iteration 183/1000 | Loss: 0.00003179
Iteration 184/1000 | Loss: 0.00002922
Iteration 185/1000 | Loss: 0.00002075
Iteration 186/1000 | Loss: 0.00002284
Iteration 187/1000 | Loss: 0.00004289
Iteration 188/1000 | Loss: 0.00002921
Iteration 189/1000 | Loss: 0.00002622
Iteration 190/1000 | Loss: 0.00003201
Iteration 191/1000 | Loss: 0.00003562
Iteration 192/1000 | Loss: 0.00003278
Iteration 193/1000 | Loss: 0.00002860
Iteration 194/1000 | Loss: 0.00002520
Iteration 195/1000 | Loss: 0.00002748
Iteration 196/1000 | Loss: 0.00003272
Iteration 197/1000 | Loss: 0.00002549
Iteration 198/1000 | Loss: 0.00002742
Iteration 199/1000 | Loss: 0.00003433
Iteration 200/1000 | Loss: 0.00003994
Iteration 201/1000 | Loss: 0.00003759
Iteration 202/1000 | Loss: 0.00003223
Iteration 203/1000 | Loss: 0.00003766
Iteration 204/1000 | Loss: 0.00003127
Iteration 205/1000 | Loss: 0.00004540
Iteration 206/1000 | Loss: 0.00004349
Iteration 207/1000 | Loss: 0.00004758
Iteration 208/1000 | Loss: 0.00003628
Iteration 209/1000 | Loss: 0.00003511
Iteration 210/1000 | Loss: 0.00004576
Iteration 211/1000 | Loss: 0.00003764
Iteration 212/1000 | Loss: 0.00002875
Iteration 213/1000 | Loss: 0.00001421
Iteration 214/1000 | Loss: 0.00001210
Iteration 215/1000 | Loss: 0.00001129
Iteration 216/1000 | Loss: 0.00001100
Iteration 217/1000 | Loss: 0.00002490
Iteration 218/1000 | Loss: 0.00001291
Iteration 219/1000 | Loss: 0.00001144
Iteration 220/1000 | Loss: 0.00001065
Iteration 221/1000 | Loss: 0.00001030
Iteration 222/1000 | Loss: 0.00000992
Iteration 223/1000 | Loss: 0.00000947
Iteration 224/1000 | Loss: 0.00000915
Iteration 225/1000 | Loss: 0.00000907
Iteration 226/1000 | Loss: 0.00000905
Iteration 227/1000 | Loss: 0.00000903
Iteration 228/1000 | Loss: 0.00000902
Iteration 229/1000 | Loss: 0.00000899
Iteration 230/1000 | Loss: 0.00000898
Iteration 231/1000 | Loss: 0.00000893
Iteration 232/1000 | Loss: 0.00000889
Iteration 233/1000 | Loss: 0.00000888
Iteration 234/1000 | Loss: 0.00000888
Iteration 235/1000 | Loss: 0.00000887
Iteration 236/1000 | Loss: 0.00000886
Iteration 237/1000 | Loss: 0.00000886
Iteration 238/1000 | Loss: 0.00000885
Iteration 239/1000 | Loss: 0.00000884
Iteration 240/1000 | Loss: 0.00000879
Iteration 241/1000 | Loss: 0.00000876
Iteration 242/1000 | Loss: 0.00000870
Iteration 243/1000 | Loss: 0.00000868
Iteration 244/1000 | Loss: 0.00000868
Iteration 245/1000 | Loss: 0.00000868
Iteration 246/1000 | Loss: 0.00000868
Iteration 247/1000 | Loss: 0.00000868
Iteration 248/1000 | Loss: 0.00000868
Iteration 249/1000 | Loss: 0.00000868
Iteration 250/1000 | Loss: 0.00000868
Iteration 251/1000 | Loss: 0.00000867
Iteration 252/1000 | Loss: 0.00000867
Iteration 253/1000 | Loss: 0.00000867
Iteration 254/1000 | Loss: 0.00000867
Iteration 255/1000 | Loss: 0.00000867
Iteration 256/1000 | Loss: 0.00000866
Iteration 257/1000 | Loss: 0.00000866
Iteration 258/1000 | Loss: 0.00000866
Iteration 259/1000 | Loss: 0.00000865
Iteration 260/1000 | Loss: 0.00000865
Iteration 261/1000 | Loss: 0.00000865
Iteration 262/1000 | Loss: 0.00000865
Iteration 263/1000 | Loss: 0.00000865
Iteration 264/1000 | Loss: 0.00000865
Iteration 265/1000 | Loss: 0.00000865
Iteration 266/1000 | Loss: 0.00000864
Iteration 267/1000 | Loss: 0.00000864
Iteration 268/1000 | Loss: 0.00000864
Iteration 269/1000 | Loss: 0.00000864
Iteration 270/1000 | Loss: 0.00000864
Iteration 271/1000 | Loss: 0.00000863
Iteration 272/1000 | Loss: 0.00000863
Iteration 273/1000 | Loss: 0.00000863
Iteration 274/1000 | Loss: 0.00000863
Iteration 275/1000 | Loss: 0.00000863
Iteration 276/1000 | Loss: 0.00000863
Iteration 277/1000 | Loss: 0.00000863
Iteration 278/1000 | Loss: 0.00000863
Iteration 279/1000 | Loss: 0.00000863
Iteration 280/1000 | Loss: 0.00000863
Iteration 281/1000 | Loss: 0.00000863
Iteration 282/1000 | Loss: 0.00000863
Iteration 283/1000 | Loss: 0.00000862
Iteration 284/1000 | Loss: 0.00000862
Iteration 285/1000 | Loss: 0.00000862
Iteration 286/1000 | Loss: 0.00000862
Iteration 287/1000 | Loss: 0.00000862
Iteration 288/1000 | Loss: 0.00000862
Iteration 289/1000 | Loss: 0.00000862
Iteration 290/1000 | Loss: 0.00000862
Iteration 291/1000 | Loss: 0.00000862
Iteration 292/1000 | Loss: 0.00000862
Iteration 293/1000 | Loss: 0.00000862
Iteration 294/1000 | Loss: 0.00000862
Iteration 295/1000 | Loss: 0.00000862
Iteration 296/1000 | Loss: 0.00000862
Iteration 297/1000 | Loss: 0.00000862
Iteration 298/1000 | Loss: 0.00000861
Iteration 299/1000 | Loss: 0.00000861
Iteration 300/1000 | Loss: 0.00000861
Iteration 301/1000 | Loss: 0.00000861
Iteration 302/1000 | Loss: 0.00000861
Iteration 303/1000 | Loss: 0.00000861
Iteration 304/1000 | Loss: 0.00000861
Iteration 305/1000 | Loss: 0.00000861
Iteration 306/1000 | Loss: 0.00000861
Iteration 307/1000 | Loss: 0.00000861
Iteration 308/1000 | Loss: 0.00000861
Iteration 309/1000 | Loss: 0.00000861
Iteration 310/1000 | Loss: 0.00000861
Iteration 311/1000 | Loss: 0.00000860
Iteration 312/1000 | Loss: 0.00000860
Iteration 313/1000 | Loss: 0.00000860
Iteration 314/1000 | Loss: 0.00000860
Iteration 315/1000 | Loss: 0.00000860
Iteration 316/1000 | Loss: 0.00000860
Iteration 317/1000 | Loss: 0.00000860
Iteration 318/1000 | Loss: 0.00000860
Iteration 319/1000 | Loss: 0.00000860
Iteration 320/1000 | Loss: 0.00000860
Iteration 321/1000 | Loss: 0.00000859
Iteration 322/1000 | Loss: 0.00000859
Iteration 323/1000 | Loss: 0.00000859
Iteration 324/1000 | Loss: 0.00000859
Iteration 325/1000 | Loss: 0.00000859
Iteration 326/1000 | Loss: 0.00000859
Iteration 327/1000 | Loss: 0.00000858
Iteration 328/1000 | Loss: 0.00000858
Iteration 329/1000 | Loss: 0.00000858
Iteration 330/1000 | Loss: 0.00000858
Iteration 331/1000 | Loss: 0.00000858
Iteration 332/1000 | Loss: 0.00000858
Iteration 333/1000 | Loss: 0.00000858
Iteration 334/1000 | Loss: 0.00000857
Iteration 335/1000 | Loss: 0.00000857
Iteration 336/1000 | Loss: 0.00000857
Iteration 337/1000 | Loss: 0.00000857
Iteration 338/1000 | Loss: 0.00000857
Iteration 339/1000 | Loss: 0.00000857
Iteration 340/1000 | Loss: 0.00000857
Iteration 341/1000 | Loss: 0.00000857
Iteration 342/1000 | Loss: 0.00000857
Iteration 343/1000 | Loss: 0.00000856
Iteration 344/1000 | Loss: 0.00000856
Iteration 345/1000 | Loss: 0.00000856
Iteration 346/1000 | Loss: 0.00000856
Iteration 347/1000 | Loss: 0.00000856
Iteration 348/1000 | Loss: 0.00000856
Iteration 349/1000 | Loss: 0.00000856
Iteration 350/1000 | Loss: 0.00000856
Iteration 351/1000 | Loss: 0.00000856
Iteration 352/1000 | Loss: 0.00000856
Iteration 353/1000 | Loss: 0.00000856
Iteration 354/1000 | Loss: 0.00000856
Iteration 355/1000 | Loss: 0.00000856
Iteration 356/1000 | Loss: 0.00000856
Iteration 357/1000 | Loss: 0.00000856
Iteration 358/1000 | Loss: 0.00000856
Iteration 359/1000 | Loss: 0.00000856
Iteration 360/1000 | Loss: 0.00000855
Iteration 361/1000 | Loss: 0.00000855
Iteration 362/1000 | Loss: 0.00000855
Iteration 363/1000 | Loss: 0.00000855
Iteration 364/1000 | Loss: 0.00000855
Iteration 365/1000 | Loss: 0.00000855
Iteration 366/1000 | Loss: 0.00000855
Iteration 367/1000 | Loss: 0.00000855
Iteration 368/1000 | Loss: 0.00000855
Iteration 369/1000 | Loss: 0.00000855
Iteration 370/1000 | Loss: 0.00000855
Iteration 371/1000 | Loss: 0.00000854
Iteration 372/1000 | Loss: 0.00000854
Iteration 373/1000 | Loss: 0.00000854
Iteration 374/1000 | Loss: 0.00000854
Iteration 375/1000 | Loss: 0.00000853
Iteration 376/1000 | Loss: 0.00000853
Iteration 377/1000 | Loss: 0.00000853
Iteration 378/1000 | Loss: 0.00000852
Iteration 379/1000 | Loss: 0.00000852
Iteration 380/1000 | Loss: 0.00000852
Iteration 381/1000 | Loss: 0.00000852
Iteration 382/1000 | Loss: 0.00000852
Iteration 383/1000 | Loss: 0.00000852
Iteration 384/1000 | Loss: 0.00000852
Iteration 385/1000 | Loss: 0.00000852
Iteration 386/1000 | Loss: 0.00000852
Iteration 387/1000 | Loss: 0.00000852
Iteration 388/1000 | Loss: 0.00000852
Iteration 389/1000 | Loss: 0.00000852
Iteration 390/1000 | Loss: 0.00000852
Iteration 391/1000 | Loss: 0.00000852
Iteration 392/1000 | Loss: 0.00000852
Iteration 393/1000 | Loss: 0.00000851
Iteration 394/1000 | Loss: 0.00000851
Iteration 395/1000 | Loss: 0.00000851
Iteration 396/1000 | Loss: 0.00000851
Iteration 397/1000 | Loss: 0.00000851
Iteration 398/1000 | Loss: 0.00000851
Iteration 399/1000 | Loss: 0.00000850
Iteration 400/1000 | Loss: 0.00000850
Iteration 401/1000 | Loss: 0.00000850
Iteration 402/1000 | Loss: 0.00000850
Iteration 403/1000 | Loss: 0.00000850
Iteration 404/1000 | Loss: 0.00000850
Iteration 405/1000 | Loss: 0.00000849
Iteration 406/1000 | Loss: 0.00000849
Iteration 407/1000 | Loss: 0.00000849
Iteration 408/1000 | Loss: 0.00000849
Iteration 409/1000 | Loss: 0.00000849
Iteration 410/1000 | Loss: 0.00000849
Iteration 411/1000 | Loss: 0.00000849
Iteration 412/1000 | Loss: 0.00000849
Iteration 413/1000 | Loss: 0.00000848
Iteration 414/1000 | Loss: 0.00000848
Iteration 415/1000 | Loss: 0.00000848
Iteration 416/1000 | Loss: 0.00000848
Iteration 417/1000 | Loss: 0.00000848
Iteration 418/1000 | Loss: 0.00000848
Iteration 419/1000 | Loss: 0.00000848
Iteration 420/1000 | Loss: 0.00000848
Iteration 421/1000 | Loss: 0.00000848
Iteration 422/1000 | Loss: 0.00000848
Iteration 423/1000 | Loss: 0.00000848
Iteration 424/1000 | Loss: 0.00000848
Iteration 425/1000 | Loss: 0.00000847
Iteration 426/1000 | Loss: 0.00000847
Iteration 427/1000 | Loss: 0.00000847
Iteration 428/1000 | Loss: 0.00000847
Iteration 429/1000 | Loss: 0.00000847
Iteration 430/1000 | Loss: 0.00000847
Iteration 431/1000 | Loss: 0.00000847
Iteration 432/1000 | Loss: 0.00000847
Iteration 433/1000 | Loss: 0.00000847
Iteration 434/1000 | Loss: 0.00000847
Iteration 435/1000 | Loss: 0.00000847
Iteration 436/1000 | Loss: 0.00000847
Iteration 437/1000 | Loss: 0.00000847
Iteration 438/1000 | Loss: 0.00000847
Iteration 439/1000 | Loss: 0.00000847
Iteration 440/1000 | Loss: 0.00000847
Iteration 441/1000 | Loss: 0.00000847
Iteration 442/1000 | Loss: 0.00000847
Iteration 443/1000 | Loss: 0.00000846
Iteration 444/1000 | Loss: 0.00000846
Iteration 445/1000 | Loss: 0.00000846
Iteration 446/1000 | Loss: 0.00000846
Iteration 447/1000 | Loss: 0.00000846
Iteration 448/1000 | Loss: 0.00000846
Iteration 449/1000 | Loss: 0.00000845
Iteration 450/1000 | Loss: 0.00000845
Iteration 451/1000 | Loss: 0.00000845
Iteration 452/1000 | Loss: 0.00000845
Iteration 453/1000 | Loss: 0.00000844
Iteration 454/1000 | Loss: 0.00000844
Iteration 455/1000 | Loss: 0.00000844
Iteration 456/1000 | Loss: 0.00000844
Iteration 457/1000 | Loss: 0.00000844
Iteration 458/1000 | Loss: 0.00000844
Iteration 459/1000 | Loss: 0.00000844
Iteration 460/1000 | Loss: 0.00000844
Iteration 461/1000 | Loss: 0.00000844
Iteration 462/1000 | Loss: 0.00000844
Iteration 463/1000 | Loss: 0.00000844
Iteration 464/1000 | Loss: 0.00000844
Iteration 465/1000 | Loss: 0.00000844
Iteration 466/1000 | Loss: 0.00000844
Iteration 467/1000 | Loss: 0.00000844
Iteration 468/1000 | Loss: 0.00000843
Iteration 469/1000 | Loss: 0.00000843
Iteration 470/1000 | Loss: 0.00000843
Iteration 471/1000 | Loss: 0.00000843
Iteration 472/1000 | Loss: 0.00000843
Iteration 473/1000 | Loss: 0.00000843
Iteration 474/1000 | Loss: 0.00000843
Iteration 475/1000 | Loss: 0.00000843
Iteration 476/1000 | Loss: 0.00000843
Iteration 477/1000 | Loss: 0.00000843
Iteration 478/1000 | Loss: 0.00000843
Iteration 479/1000 | Loss: 0.00000843
Iteration 480/1000 | Loss: 0.00000843
Iteration 481/1000 | Loss: 0.00000843
Iteration 482/1000 | Loss: 0.00000843
Iteration 483/1000 | Loss: 0.00000843
Iteration 484/1000 | Loss: 0.00000843
Iteration 485/1000 | Loss: 0.00000843
Iteration 486/1000 | Loss: 0.00000843
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 486. Stopping optimization.
Last 5 losses: [8.42988447402604e-06, 8.42988447402604e-06, 8.42988447402604e-06, 8.42988447402604e-06, 8.42988447402604e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.42988447402604e-06

Optimization complete. Final v2v error: 2.50834321975708 mm

Highest mean error: 3.8100574016571045 mm for frame 59

Lowest mean error: 2.318514585494995 mm for frame 42

Saving results

Total time: 428.53252696990967
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_grace_posed_004/1060/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_grace_posed_004/1060.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_grace_posed_004/1060
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00380411
Iteration 2/25 | Loss: 0.00106969
Iteration 3/25 | Loss: 0.00099133
Iteration 4/25 | Loss: 0.00098297
Iteration 5/25 | Loss: 0.00098129
Iteration 6/25 | Loss: 0.00098129
Iteration 7/25 | Loss: 0.00098129
Iteration 8/25 | Loss: 0.00098129
Iteration 9/25 | Loss: 0.00098129
Iteration 10/25 | Loss: 0.00098129
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0009812870994210243, 0.0009812870994210243, 0.0009812870994210243, 0.0009812870994210243, 0.0009812870994210243]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009812870994210243

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.30225134
Iteration 2/25 | Loss: 0.00091073
Iteration 3/25 | Loss: 0.00091073
Iteration 4/25 | Loss: 0.00091073
Iteration 5/25 | Loss: 0.00091073
Iteration 6/25 | Loss: 0.00091073
Iteration 7/25 | Loss: 0.00091073
Iteration 8/25 | Loss: 0.00091073
Iteration 9/25 | Loss: 0.00091073
Iteration 10/25 | Loss: 0.00091073
Iteration 11/25 | Loss: 0.00091073
Iteration 12/25 | Loss: 0.00091073
Iteration 13/25 | Loss: 0.00091073
Iteration 14/25 | Loss: 0.00091073
Iteration 15/25 | Loss: 0.00091073
Iteration 16/25 | Loss: 0.00091073
Iteration 17/25 | Loss: 0.00091073
Iteration 18/25 | Loss: 0.00091073
Iteration 19/25 | Loss: 0.00091073
Iteration 20/25 | Loss: 0.00091073
Iteration 21/25 | Loss: 0.00091073
Iteration 22/25 | Loss: 0.00091073
Iteration 23/25 | Loss: 0.00091073
Iteration 24/25 | Loss: 0.00091073
Iteration 25/25 | Loss: 0.00091073

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00091073
Iteration 2/1000 | Loss: 0.00001897
Iteration 3/1000 | Loss: 0.00001282
Iteration 4/1000 | Loss: 0.00001158
Iteration 5/1000 | Loss: 0.00001083
Iteration 6/1000 | Loss: 0.00001003
Iteration 7/1000 | Loss: 0.00000959
Iteration 8/1000 | Loss: 0.00000922
Iteration 9/1000 | Loss: 0.00000890
Iteration 10/1000 | Loss: 0.00000870
Iteration 11/1000 | Loss: 0.00000867
Iteration 12/1000 | Loss: 0.00000858
Iteration 13/1000 | Loss: 0.00000854
Iteration 14/1000 | Loss: 0.00000846
Iteration 15/1000 | Loss: 0.00000842
Iteration 16/1000 | Loss: 0.00000839
Iteration 17/1000 | Loss: 0.00000839
Iteration 18/1000 | Loss: 0.00000838
Iteration 19/1000 | Loss: 0.00000838
Iteration 20/1000 | Loss: 0.00000837
Iteration 21/1000 | Loss: 0.00000835
Iteration 22/1000 | Loss: 0.00000834
Iteration 23/1000 | Loss: 0.00000834
Iteration 24/1000 | Loss: 0.00000834
Iteration 25/1000 | Loss: 0.00000834
Iteration 26/1000 | Loss: 0.00000833
Iteration 27/1000 | Loss: 0.00000833
Iteration 28/1000 | Loss: 0.00000832
Iteration 29/1000 | Loss: 0.00000830
Iteration 30/1000 | Loss: 0.00000830
Iteration 31/1000 | Loss: 0.00000830
Iteration 32/1000 | Loss: 0.00000829
Iteration 33/1000 | Loss: 0.00000829
Iteration 34/1000 | Loss: 0.00000829
Iteration 35/1000 | Loss: 0.00000829
Iteration 36/1000 | Loss: 0.00000829
Iteration 37/1000 | Loss: 0.00000829
Iteration 38/1000 | Loss: 0.00000829
Iteration 39/1000 | Loss: 0.00000828
Iteration 40/1000 | Loss: 0.00000823
Iteration 41/1000 | Loss: 0.00000823
Iteration 42/1000 | Loss: 0.00000820
Iteration 43/1000 | Loss: 0.00000819
Iteration 44/1000 | Loss: 0.00000819
Iteration 45/1000 | Loss: 0.00000818
Iteration 46/1000 | Loss: 0.00000818
Iteration 47/1000 | Loss: 0.00000818
Iteration 48/1000 | Loss: 0.00000817
Iteration 49/1000 | Loss: 0.00000816
Iteration 50/1000 | Loss: 0.00000816
Iteration 51/1000 | Loss: 0.00000815
Iteration 52/1000 | Loss: 0.00000815
Iteration 53/1000 | Loss: 0.00000815
Iteration 54/1000 | Loss: 0.00000815
Iteration 55/1000 | Loss: 0.00000814
Iteration 56/1000 | Loss: 0.00000814
Iteration 57/1000 | Loss: 0.00000813
Iteration 58/1000 | Loss: 0.00000812
Iteration 59/1000 | Loss: 0.00000812
Iteration 60/1000 | Loss: 0.00000811
Iteration 61/1000 | Loss: 0.00000811
Iteration 62/1000 | Loss: 0.00000810
Iteration 63/1000 | Loss: 0.00000810
Iteration 64/1000 | Loss: 0.00000808
Iteration 65/1000 | Loss: 0.00000807
Iteration 66/1000 | Loss: 0.00000806
Iteration 67/1000 | Loss: 0.00000806
Iteration 68/1000 | Loss: 0.00000806
Iteration 69/1000 | Loss: 0.00000806
Iteration 70/1000 | Loss: 0.00000805
Iteration 71/1000 | Loss: 0.00000804
Iteration 72/1000 | Loss: 0.00000804
Iteration 73/1000 | Loss: 0.00000804
Iteration 74/1000 | Loss: 0.00000804
Iteration 75/1000 | Loss: 0.00000804
Iteration 76/1000 | Loss: 0.00000804
Iteration 77/1000 | Loss: 0.00000803
Iteration 78/1000 | Loss: 0.00000803
Iteration 79/1000 | Loss: 0.00000803
Iteration 80/1000 | Loss: 0.00000803
Iteration 81/1000 | Loss: 0.00000803
Iteration 82/1000 | Loss: 0.00000803
Iteration 83/1000 | Loss: 0.00000803
Iteration 84/1000 | Loss: 0.00000803
Iteration 85/1000 | Loss: 0.00000803
Iteration 86/1000 | Loss: 0.00000803
Iteration 87/1000 | Loss: 0.00000803
Iteration 88/1000 | Loss: 0.00000803
Iteration 89/1000 | Loss: 0.00000803
Iteration 90/1000 | Loss: 0.00000803
Iteration 91/1000 | Loss: 0.00000803
Iteration 92/1000 | Loss: 0.00000802
Iteration 93/1000 | Loss: 0.00000802
Iteration 94/1000 | Loss: 0.00000802
Iteration 95/1000 | Loss: 0.00000802
Iteration 96/1000 | Loss: 0.00000802
Iteration 97/1000 | Loss: 0.00000802
Iteration 98/1000 | Loss: 0.00000802
Iteration 99/1000 | Loss: 0.00000802
Iteration 100/1000 | Loss: 0.00000802
Iteration 101/1000 | Loss: 0.00000801
Iteration 102/1000 | Loss: 0.00000801
Iteration 103/1000 | Loss: 0.00000801
Iteration 104/1000 | Loss: 0.00000801
Iteration 105/1000 | Loss: 0.00000801
Iteration 106/1000 | Loss: 0.00000801
Iteration 107/1000 | Loss: 0.00000801
Iteration 108/1000 | Loss: 0.00000801
Iteration 109/1000 | Loss: 0.00000801
Iteration 110/1000 | Loss: 0.00000801
Iteration 111/1000 | Loss: 0.00000801
Iteration 112/1000 | Loss: 0.00000800
Iteration 113/1000 | Loss: 0.00000800
Iteration 114/1000 | Loss: 0.00000800
Iteration 115/1000 | Loss: 0.00000800
Iteration 116/1000 | Loss: 0.00000800
Iteration 117/1000 | Loss: 0.00000799
Iteration 118/1000 | Loss: 0.00000799
Iteration 119/1000 | Loss: 0.00000798
Iteration 120/1000 | Loss: 0.00000798
Iteration 121/1000 | Loss: 0.00000798
Iteration 122/1000 | Loss: 0.00000797
Iteration 123/1000 | Loss: 0.00000797
Iteration 124/1000 | Loss: 0.00000797
Iteration 125/1000 | Loss: 0.00000797
Iteration 126/1000 | Loss: 0.00000797
Iteration 127/1000 | Loss: 0.00000797
Iteration 128/1000 | Loss: 0.00000797
Iteration 129/1000 | Loss: 0.00000797
Iteration 130/1000 | Loss: 0.00000796
Iteration 131/1000 | Loss: 0.00000796
Iteration 132/1000 | Loss: 0.00000796
Iteration 133/1000 | Loss: 0.00000796
Iteration 134/1000 | Loss: 0.00000795
Iteration 135/1000 | Loss: 0.00000795
Iteration 136/1000 | Loss: 0.00000795
Iteration 137/1000 | Loss: 0.00000795
Iteration 138/1000 | Loss: 0.00000795
Iteration 139/1000 | Loss: 0.00000794
Iteration 140/1000 | Loss: 0.00000794
Iteration 141/1000 | Loss: 0.00000794
Iteration 142/1000 | Loss: 0.00000794
Iteration 143/1000 | Loss: 0.00000793
Iteration 144/1000 | Loss: 0.00000793
Iteration 145/1000 | Loss: 0.00000793
Iteration 146/1000 | Loss: 0.00000793
Iteration 147/1000 | Loss: 0.00000792
Iteration 148/1000 | Loss: 0.00000792
Iteration 149/1000 | Loss: 0.00000792
Iteration 150/1000 | Loss: 0.00000791
Iteration 151/1000 | Loss: 0.00000791
Iteration 152/1000 | Loss: 0.00000791
Iteration 153/1000 | Loss: 0.00000791
Iteration 154/1000 | Loss: 0.00000791
Iteration 155/1000 | Loss: 0.00000791
Iteration 156/1000 | Loss: 0.00000791
Iteration 157/1000 | Loss: 0.00000791
Iteration 158/1000 | Loss: 0.00000791
Iteration 159/1000 | Loss: 0.00000791
Iteration 160/1000 | Loss: 0.00000790
Iteration 161/1000 | Loss: 0.00000790
Iteration 162/1000 | Loss: 0.00000790
Iteration 163/1000 | Loss: 0.00000789
Iteration 164/1000 | Loss: 0.00000789
Iteration 165/1000 | Loss: 0.00000789
Iteration 166/1000 | Loss: 0.00000789
Iteration 167/1000 | Loss: 0.00000789
Iteration 168/1000 | Loss: 0.00000789
Iteration 169/1000 | Loss: 0.00000789
Iteration 170/1000 | Loss: 0.00000789
Iteration 171/1000 | Loss: 0.00000789
Iteration 172/1000 | Loss: 0.00000789
Iteration 173/1000 | Loss: 0.00000789
Iteration 174/1000 | Loss: 0.00000789
Iteration 175/1000 | Loss: 0.00000789
Iteration 176/1000 | Loss: 0.00000789
Iteration 177/1000 | Loss: 0.00000789
Iteration 178/1000 | Loss: 0.00000789
Iteration 179/1000 | Loss: 0.00000789
Iteration 180/1000 | Loss: 0.00000789
Iteration 181/1000 | Loss: 0.00000789
Iteration 182/1000 | Loss: 0.00000789
Iteration 183/1000 | Loss: 0.00000789
Iteration 184/1000 | Loss: 0.00000789
Iteration 185/1000 | Loss: 0.00000789
Iteration 186/1000 | Loss: 0.00000789
Iteration 187/1000 | Loss: 0.00000789
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 187. Stopping optimization.
Last 5 losses: [7.889591870480217e-06, 7.889591870480217e-06, 7.889591870480217e-06, 7.889591870480217e-06, 7.889591870480217e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 7.889591870480217e-06

Optimization complete. Final v2v error: 2.4135396480560303 mm

Highest mean error: 2.696425437927246 mm for frame 159

Lowest mean error: 2.249424934387207 mm for frame 241

Saving results

Total time: 45.608083963394165
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_grace_posed_004/1015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_grace_posed_004/1015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_grace_posed_004/1015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00791714
Iteration 2/25 | Loss: 0.00174479
Iteration 3/25 | Loss: 0.00118849
Iteration 4/25 | Loss: 0.00111993
Iteration 5/25 | Loss: 0.00111655
Iteration 6/25 | Loss: 0.00111557
Iteration 7/25 | Loss: 0.00111557
Iteration 8/25 | Loss: 0.00111557
Iteration 9/25 | Loss: 0.00111557
Iteration 10/25 | Loss: 0.00111557
Iteration 11/25 | Loss: 0.00111557
Iteration 12/25 | Loss: 0.00111557
Iteration 13/25 | Loss: 0.00111557
Iteration 14/25 | Loss: 0.00111557
Iteration 15/25 | Loss: 0.00111557
Iteration 16/25 | Loss: 0.00111557
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0011155676329508424, 0.0011155676329508424, 0.0011155676329508424, 0.0011155676329508424, 0.0011155676329508424]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011155676329508424

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.31349683
Iteration 2/25 | Loss: 0.00088777
Iteration 3/25 | Loss: 0.00088775
Iteration 4/25 | Loss: 0.00088775
Iteration 5/25 | Loss: 0.00088775
Iteration 6/25 | Loss: 0.00088775
Iteration 7/25 | Loss: 0.00088775
Iteration 8/25 | Loss: 0.00088775
Iteration 9/25 | Loss: 0.00088775
Iteration 10/25 | Loss: 0.00088775
Iteration 11/25 | Loss: 0.00088775
Iteration 12/25 | Loss: 0.00088775
Iteration 13/25 | Loss: 0.00088775
Iteration 14/25 | Loss: 0.00088775
Iteration 15/25 | Loss: 0.00088775
Iteration 16/25 | Loss: 0.00088775
Iteration 17/25 | Loss: 0.00088775
Iteration 18/25 | Loss: 0.00088775
Iteration 19/25 | Loss: 0.00088775
Iteration 20/25 | Loss: 0.00088775
Iteration 21/25 | Loss: 0.00088775
Iteration 22/25 | Loss: 0.00088775
Iteration 23/25 | Loss: 0.00088775
Iteration 24/25 | Loss: 0.00088775
Iteration 25/25 | Loss: 0.00088775

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00088775
Iteration 2/1000 | Loss: 0.00003266
Iteration 3/1000 | Loss: 0.00002124
Iteration 4/1000 | Loss: 0.00001857
Iteration 5/1000 | Loss: 0.00001762
Iteration 6/1000 | Loss: 0.00001718
Iteration 7/1000 | Loss: 0.00001692
Iteration 8/1000 | Loss: 0.00001673
Iteration 9/1000 | Loss: 0.00001671
Iteration 10/1000 | Loss: 0.00001660
Iteration 11/1000 | Loss: 0.00001659
Iteration 12/1000 | Loss: 0.00001658
Iteration 13/1000 | Loss: 0.00001656
Iteration 14/1000 | Loss: 0.00001655
Iteration 15/1000 | Loss: 0.00001654
Iteration 16/1000 | Loss: 0.00001650
Iteration 17/1000 | Loss: 0.00001645
Iteration 18/1000 | Loss: 0.00001645
Iteration 19/1000 | Loss: 0.00001644
Iteration 20/1000 | Loss: 0.00001644
Iteration 21/1000 | Loss: 0.00001644
Iteration 22/1000 | Loss: 0.00001643
Iteration 23/1000 | Loss: 0.00001643
Iteration 24/1000 | Loss: 0.00001636
Iteration 25/1000 | Loss: 0.00001636
Iteration 26/1000 | Loss: 0.00001634
Iteration 27/1000 | Loss: 0.00001634
Iteration 28/1000 | Loss: 0.00001633
Iteration 29/1000 | Loss: 0.00001632
Iteration 30/1000 | Loss: 0.00001632
Iteration 31/1000 | Loss: 0.00001631
Iteration 32/1000 | Loss: 0.00001631
Iteration 33/1000 | Loss: 0.00001631
Iteration 34/1000 | Loss: 0.00001631
Iteration 35/1000 | Loss: 0.00001630
Iteration 36/1000 | Loss: 0.00001630
Iteration 37/1000 | Loss: 0.00001630
Iteration 38/1000 | Loss: 0.00001630
Iteration 39/1000 | Loss: 0.00001629
Iteration 40/1000 | Loss: 0.00001629
Iteration 41/1000 | Loss: 0.00001628
Iteration 42/1000 | Loss: 0.00001627
Iteration 43/1000 | Loss: 0.00001627
Iteration 44/1000 | Loss: 0.00001626
Iteration 45/1000 | Loss: 0.00001626
Iteration 46/1000 | Loss: 0.00001626
Iteration 47/1000 | Loss: 0.00001625
Iteration 48/1000 | Loss: 0.00001625
Iteration 49/1000 | Loss: 0.00001625
Iteration 50/1000 | Loss: 0.00001624
Iteration 51/1000 | Loss: 0.00001624
Iteration 52/1000 | Loss: 0.00001624
Iteration 53/1000 | Loss: 0.00001624
Iteration 54/1000 | Loss: 0.00001623
Iteration 55/1000 | Loss: 0.00001623
Iteration 56/1000 | Loss: 0.00001623
Iteration 57/1000 | Loss: 0.00001623
Iteration 58/1000 | Loss: 0.00001623
Iteration 59/1000 | Loss: 0.00001622
Iteration 60/1000 | Loss: 0.00001622
Iteration 61/1000 | Loss: 0.00001622
Iteration 62/1000 | Loss: 0.00001621
Iteration 63/1000 | Loss: 0.00001621
Iteration 64/1000 | Loss: 0.00001621
Iteration 65/1000 | Loss: 0.00001621
Iteration 66/1000 | Loss: 0.00001620
Iteration 67/1000 | Loss: 0.00001620
Iteration 68/1000 | Loss: 0.00001620
Iteration 69/1000 | Loss: 0.00001619
Iteration 70/1000 | Loss: 0.00001619
Iteration 71/1000 | Loss: 0.00001619
Iteration 72/1000 | Loss: 0.00001618
Iteration 73/1000 | Loss: 0.00001618
Iteration 74/1000 | Loss: 0.00001618
Iteration 75/1000 | Loss: 0.00001618
Iteration 76/1000 | Loss: 0.00001617
Iteration 77/1000 | Loss: 0.00001617
Iteration 78/1000 | Loss: 0.00001617
Iteration 79/1000 | Loss: 0.00001617
Iteration 80/1000 | Loss: 0.00001617
Iteration 81/1000 | Loss: 0.00001617
Iteration 82/1000 | Loss: 0.00001616
Iteration 83/1000 | Loss: 0.00001616
Iteration 84/1000 | Loss: 0.00001616
Iteration 85/1000 | Loss: 0.00001616
Iteration 86/1000 | Loss: 0.00001615
Iteration 87/1000 | Loss: 0.00001615
Iteration 88/1000 | Loss: 0.00001615
Iteration 89/1000 | Loss: 0.00001615
Iteration 90/1000 | Loss: 0.00001614
Iteration 91/1000 | Loss: 0.00001614
Iteration 92/1000 | Loss: 0.00001614
Iteration 93/1000 | Loss: 0.00001614
Iteration 94/1000 | Loss: 0.00001614
Iteration 95/1000 | Loss: 0.00001614
Iteration 96/1000 | Loss: 0.00001613
Iteration 97/1000 | Loss: 0.00001613
Iteration 98/1000 | Loss: 0.00001613
Iteration 99/1000 | Loss: 0.00001613
Iteration 100/1000 | Loss: 0.00001613
Iteration 101/1000 | Loss: 0.00001613
Iteration 102/1000 | Loss: 0.00001613
Iteration 103/1000 | Loss: 0.00001613
Iteration 104/1000 | Loss: 0.00001613
Iteration 105/1000 | Loss: 0.00001613
Iteration 106/1000 | Loss: 0.00001613
Iteration 107/1000 | Loss: 0.00001613
Iteration 108/1000 | Loss: 0.00001612
Iteration 109/1000 | Loss: 0.00001612
Iteration 110/1000 | Loss: 0.00001612
Iteration 111/1000 | Loss: 0.00001612
Iteration 112/1000 | Loss: 0.00001611
Iteration 113/1000 | Loss: 0.00001611
Iteration 114/1000 | Loss: 0.00001611
Iteration 115/1000 | Loss: 0.00001611
Iteration 116/1000 | Loss: 0.00001610
Iteration 117/1000 | Loss: 0.00001610
Iteration 118/1000 | Loss: 0.00001610
Iteration 119/1000 | Loss: 0.00001610
Iteration 120/1000 | Loss: 0.00001610
Iteration 121/1000 | Loss: 0.00001610
Iteration 122/1000 | Loss: 0.00001610
Iteration 123/1000 | Loss: 0.00001610
Iteration 124/1000 | Loss: 0.00001610
Iteration 125/1000 | Loss: 0.00001610
Iteration 126/1000 | Loss: 0.00001610
Iteration 127/1000 | Loss: 0.00001610
Iteration 128/1000 | Loss: 0.00001610
Iteration 129/1000 | Loss: 0.00001610
Iteration 130/1000 | Loss: 0.00001610
Iteration 131/1000 | Loss: 0.00001610
Iteration 132/1000 | Loss: 0.00001610
Iteration 133/1000 | Loss: 0.00001610
Iteration 134/1000 | Loss: 0.00001610
Iteration 135/1000 | Loss: 0.00001610
Iteration 136/1000 | Loss: 0.00001610
Iteration 137/1000 | Loss: 0.00001610
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 137. Stopping optimization.
Last 5 losses: [1.6097854313557036e-05, 1.6097854313557036e-05, 1.6097854313557036e-05, 1.6097854313557036e-05, 1.6097854313557036e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6097854313557036e-05

Optimization complete. Final v2v error: 3.3776423931121826 mm

Highest mean error: 3.879039764404297 mm for frame 136

Lowest mean error: 3.0941267013549805 mm for frame 108

Saving results

Total time: 32.174073934555054
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_grace_posed_004/1098/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_grace_posed_004/1098.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_grace_posed_004/1098
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00862169
Iteration 2/25 | Loss: 0.00113284
Iteration 3/25 | Loss: 0.00102315
Iteration 4/25 | Loss: 0.00101067
Iteration 5/25 | Loss: 0.00100728
Iteration 6/25 | Loss: 0.00100692
Iteration 7/25 | Loss: 0.00100692
Iteration 8/25 | Loss: 0.00100692
Iteration 9/25 | Loss: 0.00100692
Iteration 10/25 | Loss: 0.00100692
Iteration 11/25 | Loss: 0.00100692
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010069244308397174, 0.0010069244308397174, 0.0010069244308397174, 0.0010069244308397174, 0.0010069244308397174]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010069244308397174

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.70380020
Iteration 2/25 | Loss: 0.00102619
Iteration 3/25 | Loss: 0.00102619
Iteration 4/25 | Loss: 0.00102619
Iteration 5/25 | Loss: 0.00102619
Iteration 6/25 | Loss: 0.00102619
Iteration 7/25 | Loss: 0.00102618
Iteration 8/25 | Loss: 0.00102618
Iteration 9/25 | Loss: 0.00102618
Iteration 10/25 | Loss: 0.00102618
Iteration 11/25 | Loss: 0.00102618
Iteration 12/25 | Loss: 0.00102618
Iteration 13/25 | Loss: 0.00102618
Iteration 14/25 | Loss: 0.00102618
Iteration 15/25 | Loss: 0.00102618
Iteration 16/25 | Loss: 0.00102618
Iteration 17/25 | Loss: 0.00102618
Iteration 18/25 | Loss: 0.00102618
Iteration 19/25 | Loss: 0.00102618
Iteration 20/25 | Loss: 0.00102618
Iteration 21/25 | Loss: 0.00102618
Iteration 22/25 | Loss: 0.00102618
Iteration 23/25 | Loss: 0.00102618
Iteration 24/25 | Loss: 0.00102618
Iteration 25/25 | Loss: 0.00102618

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00102618
Iteration 2/1000 | Loss: 0.00001512
Iteration 3/1000 | Loss: 0.00001153
Iteration 4/1000 | Loss: 0.00001044
Iteration 5/1000 | Loss: 0.00000985
Iteration 6/1000 | Loss: 0.00000944
Iteration 7/1000 | Loss: 0.00000924
Iteration 8/1000 | Loss: 0.00000895
Iteration 9/1000 | Loss: 0.00000875
Iteration 10/1000 | Loss: 0.00000864
Iteration 11/1000 | Loss: 0.00000860
Iteration 12/1000 | Loss: 0.00000856
Iteration 13/1000 | Loss: 0.00000852
Iteration 14/1000 | Loss: 0.00000852
Iteration 15/1000 | Loss: 0.00000852
Iteration 16/1000 | Loss: 0.00000852
Iteration 17/1000 | Loss: 0.00000852
Iteration 18/1000 | Loss: 0.00000852
Iteration 19/1000 | Loss: 0.00000851
Iteration 20/1000 | Loss: 0.00000851
Iteration 21/1000 | Loss: 0.00000851
Iteration 22/1000 | Loss: 0.00000851
Iteration 23/1000 | Loss: 0.00000851
Iteration 24/1000 | Loss: 0.00000851
Iteration 25/1000 | Loss: 0.00000851
Iteration 26/1000 | Loss: 0.00000851
Iteration 27/1000 | Loss: 0.00000850
Iteration 28/1000 | Loss: 0.00000849
Iteration 29/1000 | Loss: 0.00000848
Iteration 30/1000 | Loss: 0.00000848
Iteration 31/1000 | Loss: 0.00000847
Iteration 32/1000 | Loss: 0.00000847
Iteration 33/1000 | Loss: 0.00000847
Iteration 34/1000 | Loss: 0.00000844
Iteration 35/1000 | Loss: 0.00000844
Iteration 36/1000 | Loss: 0.00000844
Iteration 37/1000 | Loss: 0.00000843
Iteration 38/1000 | Loss: 0.00000840
Iteration 39/1000 | Loss: 0.00000839
Iteration 40/1000 | Loss: 0.00000838
Iteration 41/1000 | Loss: 0.00000838
Iteration 42/1000 | Loss: 0.00000837
Iteration 43/1000 | Loss: 0.00000837
Iteration 44/1000 | Loss: 0.00000836
Iteration 45/1000 | Loss: 0.00000836
Iteration 46/1000 | Loss: 0.00000835
Iteration 47/1000 | Loss: 0.00000835
Iteration 48/1000 | Loss: 0.00000833
Iteration 49/1000 | Loss: 0.00000833
Iteration 50/1000 | Loss: 0.00000833
Iteration 51/1000 | Loss: 0.00000832
Iteration 52/1000 | Loss: 0.00000832
Iteration 53/1000 | Loss: 0.00000831
Iteration 54/1000 | Loss: 0.00000831
Iteration 55/1000 | Loss: 0.00000830
Iteration 56/1000 | Loss: 0.00000830
Iteration 57/1000 | Loss: 0.00000830
Iteration 58/1000 | Loss: 0.00000829
Iteration 59/1000 | Loss: 0.00000829
Iteration 60/1000 | Loss: 0.00000828
Iteration 61/1000 | Loss: 0.00000828
Iteration 62/1000 | Loss: 0.00000822
Iteration 63/1000 | Loss: 0.00000822
Iteration 64/1000 | Loss: 0.00000822
Iteration 65/1000 | Loss: 0.00000822
Iteration 66/1000 | Loss: 0.00000822
Iteration 67/1000 | Loss: 0.00000821
Iteration 68/1000 | Loss: 0.00000820
Iteration 69/1000 | Loss: 0.00000820
Iteration 70/1000 | Loss: 0.00000820
Iteration 71/1000 | Loss: 0.00000819
Iteration 72/1000 | Loss: 0.00000819
Iteration 73/1000 | Loss: 0.00000819
Iteration 74/1000 | Loss: 0.00000819
Iteration 75/1000 | Loss: 0.00000819
Iteration 76/1000 | Loss: 0.00000819
Iteration 77/1000 | Loss: 0.00000818
Iteration 78/1000 | Loss: 0.00000818
Iteration 79/1000 | Loss: 0.00000818
Iteration 80/1000 | Loss: 0.00000817
Iteration 81/1000 | Loss: 0.00000817
Iteration 82/1000 | Loss: 0.00000816
Iteration 83/1000 | Loss: 0.00000816
Iteration 84/1000 | Loss: 0.00000816
Iteration 85/1000 | Loss: 0.00000816
Iteration 86/1000 | Loss: 0.00000815
Iteration 87/1000 | Loss: 0.00000815
Iteration 88/1000 | Loss: 0.00000815
Iteration 89/1000 | Loss: 0.00000815
Iteration 90/1000 | Loss: 0.00000814
Iteration 91/1000 | Loss: 0.00000814
Iteration 92/1000 | Loss: 0.00000814
Iteration 93/1000 | Loss: 0.00000813
Iteration 94/1000 | Loss: 0.00000813
Iteration 95/1000 | Loss: 0.00000813
Iteration 96/1000 | Loss: 0.00000813
Iteration 97/1000 | Loss: 0.00000813
Iteration 98/1000 | Loss: 0.00000813
Iteration 99/1000 | Loss: 0.00000813
Iteration 100/1000 | Loss: 0.00000813
Iteration 101/1000 | Loss: 0.00000813
Iteration 102/1000 | Loss: 0.00000812
Iteration 103/1000 | Loss: 0.00000812
Iteration 104/1000 | Loss: 0.00000812
Iteration 105/1000 | Loss: 0.00000812
Iteration 106/1000 | Loss: 0.00000811
Iteration 107/1000 | Loss: 0.00000811
Iteration 108/1000 | Loss: 0.00000811
Iteration 109/1000 | Loss: 0.00000811
Iteration 110/1000 | Loss: 0.00000811
Iteration 111/1000 | Loss: 0.00000811
Iteration 112/1000 | Loss: 0.00000811
Iteration 113/1000 | Loss: 0.00000811
Iteration 114/1000 | Loss: 0.00000811
Iteration 115/1000 | Loss: 0.00000811
Iteration 116/1000 | Loss: 0.00000811
Iteration 117/1000 | Loss: 0.00000810
Iteration 118/1000 | Loss: 0.00000810
Iteration 119/1000 | Loss: 0.00000810
Iteration 120/1000 | Loss: 0.00000810
Iteration 121/1000 | Loss: 0.00000810
Iteration 122/1000 | Loss: 0.00000810
Iteration 123/1000 | Loss: 0.00000810
Iteration 124/1000 | Loss: 0.00000810
Iteration 125/1000 | Loss: 0.00000810
Iteration 126/1000 | Loss: 0.00000810
Iteration 127/1000 | Loss: 0.00000810
Iteration 128/1000 | Loss: 0.00000810
Iteration 129/1000 | Loss: 0.00000810
Iteration 130/1000 | Loss: 0.00000810
Iteration 131/1000 | Loss: 0.00000810
Iteration 132/1000 | Loss: 0.00000810
Iteration 133/1000 | Loss: 0.00000810
Iteration 134/1000 | Loss: 0.00000810
Iteration 135/1000 | Loss: 0.00000810
Iteration 136/1000 | Loss: 0.00000809
Iteration 137/1000 | Loss: 0.00000809
Iteration 138/1000 | Loss: 0.00000809
Iteration 139/1000 | Loss: 0.00000809
Iteration 140/1000 | Loss: 0.00000809
Iteration 141/1000 | Loss: 0.00000809
Iteration 142/1000 | Loss: 0.00000809
Iteration 143/1000 | Loss: 0.00000809
Iteration 144/1000 | Loss: 0.00000809
Iteration 145/1000 | Loss: 0.00000809
Iteration 146/1000 | Loss: 0.00000809
Iteration 147/1000 | Loss: 0.00000809
Iteration 148/1000 | Loss: 0.00000808
Iteration 149/1000 | Loss: 0.00000808
Iteration 150/1000 | Loss: 0.00000808
Iteration 151/1000 | Loss: 0.00000808
Iteration 152/1000 | Loss: 0.00000808
Iteration 153/1000 | Loss: 0.00000808
Iteration 154/1000 | Loss: 0.00000808
Iteration 155/1000 | Loss: 0.00000808
Iteration 156/1000 | Loss: 0.00000808
Iteration 157/1000 | Loss: 0.00000808
Iteration 158/1000 | Loss: 0.00000808
Iteration 159/1000 | Loss: 0.00000807
Iteration 160/1000 | Loss: 0.00000807
Iteration 161/1000 | Loss: 0.00000807
Iteration 162/1000 | Loss: 0.00000807
Iteration 163/1000 | Loss: 0.00000807
Iteration 164/1000 | Loss: 0.00000807
Iteration 165/1000 | Loss: 0.00000807
Iteration 166/1000 | Loss: 0.00000807
Iteration 167/1000 | Loss: 0.00000807
Iteration 168/1000 | Loss: 0.00000807
Iteration 169/1000 | Loss: 0.00000807
Iteration 170/1000 | Loss: 0.00000807
Iteration 171/1000 | Loss: 0.00000807
Iteration 172/1000 | Loss: 0.00000807
Iteration 173/1000 | Loss: 0.00000807
Iteration 174/1000 | Loss: 0.00000807
Iteration 175/1000 | Loss: 0.00000807
Iteration 176/1000 | Loss: 0.00000807
Iteration 177/1000 | Loss: 0.00000807
Iteration 178/1000 | Loss: 0.00000807
Iteration 179/1000 | Loss: 0.00000807
Iteration 180/1000 | Loss: 0.00000807
Iteration 181/1000 | Loss: 0.00000807
Iteration 182/1000 | Loss: 0.00000807
Iteration 183/1000 | Loss: 0.00000807
Iteration 184/1000 | Loss: 0.00000807
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 184. Stopping optimization.
Last 5 losses: [8.06777006800985e-06, 8.06777006800985e-06, 8.06777006800985e-06, 8.06777006800985e-06, 8.06777006800985e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.06777006800985e-06

Optimization complete. Final v2v error: 2.431431293487549 mm

Highest mean error: 2.9966483116149902 mm for frame 155

Lowest mean error: 2.1664257049560547 mm for frame 229

Saving results

Total time: 41.130120277404785
