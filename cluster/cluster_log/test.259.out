Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=259, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 14504-14559
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_018/1084/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_018/1084.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_018/1084
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00985819
Iteration 2/25 | Loss: 0.00985818
Iteration 3/25 | Loss: 0.00985818
Iteration 4/25 | Loss: 0.00985818
Iteration 5/25 | Loss: 0.00985818
Iteration 6/25 | Loss: 0.00985818
Iteration 7/25 | Loss: 0.00985818
Iteration 8/25 | Loss: 0.00985818
Iteration 9/25 | Loss: 0.00985818
Iteration 10/25 | Loss: 0.00985818
Iteration 11/25 | Loss: 0.00985817
Iteration 12/25 | Loss: 0.00985817
Iteration 13/25 | Loss: 0.00985817
Iteration 14/25 | Loss: 0.00985817
Iteration 15/25 | Loss: 0.00985817
Iteration 16/25 | Loss: 0.00985817
Iteration 17/25 | Loss: 0.00985817
Iteration 18/25 | Loss: 0.00985817
Iteration 19/25 | Loss: 0.00985816
Iteration 20/25 | Loss: 0.00985816
Iteration 21/25 | Loss: 0.00985816
Iteration 22/25 | Loss: 0.00985816
Iteration 23/25 | Loss: 0.00985816
Iteration 24/25 | Loss: 0.00985816
Iteration 25/25 | Loss: 0.00985816

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44377267
Iteration 2/25 | Loss: 0.18467121
Iteration 3/25 | Loss: 0.18465234
Iteration 4/25 | Loss: 0.18465231
Iteration 5/25 | Loss: 0.18465228
Iteration 6/25 | Loss: 0.18465228
Iteration 7/25 | Loss: 0.18465228
Iteration 8/25 | Loss: 0.18465228
Iteration 9/25 | Loss: 0.18465228
Iteration 10/25 | Loss: 0.18465228
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.18465228378772736, 0.18465228378772736, 0.18465228378772736, 0.18465228378772736, 0.18465228378772736]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.18465228378772736

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.18465231
Iteration 2/1000 | Loss: 0.00598894
Iteration 3/1000 | Loss: 0.00225261
Iteration 4/1000 | Loss: 0.00178305
Iteration 5/1000 | Loss: 0.00227039
Iteration 6/1000 | Loss: 0.00065088
Iteration 7/1000 | Loss: 0.00047904
Iteration 8/1000 | Loss: 0.00059593
Iteration 9/1000 | Loss: 0.00029606
Iteration 10/1000 | Loss: 0.00223275
Iteration 11/1000 | Loss: 0.00033753
Iteration 12/1000 | Loss: 0.00030250
Iteration 13/1000 | Loss: 0.00005990
Iteration 14/1000 | Loss: 0.00022913
Iteration 15/1000 | Loss: 0.00039689
Iteration 16/1000 | Loss: 0.00004323
Iteration 17/1000 | Loss: 0.00009728
Iteration 18/1000 | Loss: 0.00010582
Iteration 19/1000 | Loss: 0.00018681
Iteration 20/1000 | Loss: 0.00052313
Iteration 21/1000 | Loss: 0.00007207
Iteration 22/1000 | Loss: 0.00018250
Iteration 23/1000 | Loss: 0.00004589
Iteration 24/1000 | Loss: 0.00010964
Iteration 25/1000 | Loss: 0.00002557
Iteration 26/1000 | Loss: 0.00013612
Iteration 27/1000 | Loss: 0.00050090
Iteration 28/1000 | Loss: 0.00023948
Iteration 29/1000 | Loss: 0.00006704
Iteration 30/1000 | Loss: 0.00004289
Iteration 31/1000 | Loss: 0.00002345
Iteration 32/1000 | Loss: 0.00009087
Iteration 33/1000 | Loss: 0.00002807
Iteration 34/1000 | Loss: 0.00010139
Iteration 35/1000 | Loss: 0.00009972
Iteration 36/1000 | Loss: 0.00003821
Iteration 37/1000 | Loss: 0.00002122
Iteration 38/1000 | Loss: 0.00002066
Iteration 39/1000 | Loss: 0.00002027
Iteration 40/1000 | Loss: 0.00009855
Iteration 41/1000 | Loss: 0.00017238
Iteration 42/1000 | Loss: 0.00002527
Iteration 43/1000 | Loss: 0.00001985
Iteration 44/1000 | Loss: 0.00003851
Iteration 45/1000 | Loss: 0.00001976
Iteration 46/1000 | Loss: 0.00004832
Iteration 47/1000 | Loss: 0.00001962
Iteration 48/1000 | Loss: 0.00001954
Iteration 49/1000 | Loss: 0.00001952
Iteration 50/1000 | Loss: 0.00001952
Iteration 51/1000 | Loss: 0.00001951
Iteration 52/1000 | Loss: 0.00001948
Iteration 53/1000 | Loss: 0.00001948
Iteration 54/1000 | Loss: 0.00001947
Iteration 55/1000 | Loss: 0.00001945
Iteration 56/1000 | Loss: 0.00001945
Iteration 57/1000 | Loss: 0.00001945
Iteration 58/1000 | Loss: 0.00001945
Iteration 59/1000 | Loss: 0.00001944
Iteration 60/1000 | Loss: 0.00001944
Iteration 61/1000 | Loss: 0.00001944
Iteration 62/1000 | Loss: 0.00001944
Iteration 63/1000 | Loss: 0.00001944
Iteration 64/1000 | Loss: 0.00001944
Iteration 65/1000 | Loss: 0.00001944
Iteration 66/1000 | Loss: 0.00001944
Iteration 67/1000 | Loss: 0.00001944
Iteration 68/1000 | Loss: 0.00001943
Iteration 69/1000 | Loss: 0.00001942
Iteration 70/1000 | Loss: 0.00001941
Iteration 71/1000 | Loss: 0.00001937
Iteration 72/1000 | Loss: 0.00001935
Iteration 73/1000 | Loss: 0.00001935
Iteration 74/1000 | Loss: 0.00001935
Iteration 75/1000 | Loss: 0.00001935
Iteration 76/1000 | Loss: 0.00001934
Iteration 77/1000 | Loss: 0.00001934
Iteration 78/1000 | Loss: 0.00001934
Iteration 79/1000 | Loss: 0.00001933
Iteration 80/1000 | Loss: 0.00010312
Iteration 81/1000 | Loss: 0.00001969
Iteration 82/1000 | Loss: 0.00001930
Iteration 83/1000 | Loss: 0.00001928
Iteration 84/1000 | Loss: 0.00001928
Iteration 85/1000 | Loss: 0.00001928
Iteration 86/1000 | Loss: 0.00001928
Iteration 87/1000 | Loss: 0.00001928
Iteration 88/1000 | Loss: 0.00001928
Iteration 89/1000 | Loss: 0.00001928
Iteration 90/1000 | Loss: 0.00001928
Iteration 91/1000 | Loss: 0.00001928
Iteration 92/1000 | Loss: 0.00001928
Iteration 93/1000 | Loss: 0.00001927
Iteration 94/1000 | Loss: 0.00001927
Iteration 95/1000 | Loss: 0.00001927
Iteration 96/1000 | Loss: 0.00001927
Iteration 97/1000 | Loss: 0.00001926
Iteration 98/1000 | Loss: 0.00001926
Iteration 99/1000 | Loss: 0.00001926
Iteration 100/1000 | Loss: 0.00001926
Iteration 101/1000 | Loss: 0.00001926
Iteration 102/1000 | Loss: 0.00001925
Iteration 103/1000 | Loss: 0.00001925
Iteration 104/1000 | Loss: 0.00001925
Iteration 105/1000 | Loss: 0.00001925
Iteration 106/1000 | Loss: 0.00001925
Iteration 107/1000 | Loss: 0.00001924
Iteration 108/1000 | Loss: 0.00001924
Iteration 109/1000 | Loss: 0.00001924
Iteration 110/1000 | Loss: 0.00001923
Iteration 111/1000 | Loss: 0.00001923
Iteration 112/1000 | Loss: 0.00001923
Iteration 113/1000 | Loss: 0.00001923
Iteration 114/1000 | Loss: 0.00001923
Iteration 115/1000 | Loss: 0.00001923
Iteration 116/1000 | Loss: 0.00001923
Iteration 117/1000 | Loss: 0.00001922
Iteration 118/1000 | Loss: 0.00001922
Iteration 119/1000 | Loss: 0.00001922
Iteration 120/1000 | Loss: 0.00001922
Iteration 121/1000 | Loss: 0.00001921
Iteration 122/1000 | Loss: 0.00001921
Iteration 123/1000 | Loss: 0.00001921
Iteration 124/1000 | Loss: 0.00001921
Iteration 125/1000 | Loss: 0.00001921
Iteration 126/1000 | Loss: 0.00001921
Iteration 127/1000 | Loss: 0.00001921
Iteration 128/1000 | Loss: 0.00001921
Iteration 129/1000 | Loss: 0.00001921
Iteration 130/1000 | Loss: 0.00001921
Iteration 131/1000 | Loss: 0.00001920
Iteration 132/1000 | Loss: 0.00001920
Iteration 133/1000 | Loss: 0.00001920
Iteration 134/1000 | Loss: 0.00001920
Iteration 135/1000 | Loss: 0.00001920
Iteration 136/1000 | Loss: 0.00001920
Iteration 137/1000 | Loss: 0.00001920
Iteration 138/1000 | Loss: 0.00001920
Iteration 139/1000 | Loss: 0.00001920
Iteration 140/1000 | Loss: 0.00001920
Iteration 141/1000 | Loss: 0.00001920
Iteration 142/1000 | Loss: 0.00001919
Iteration 143/1000 | Loss: 0.00001919
Iteration 144/1000 | Loss: 0.00001919
Iteration 145/1000 | Loss: 0.00001919
Iteration 146/1000 | Loss: 0.00001919
Iteration 147/1000 | Loss: 0.00001919
Iteration 148/1000 | Loss: 0.00001919
Iteration 149/1000 | Loss: 0.00001919
Iteration 150/1000 | Loss: 0.00001919
Iteration 151/1000 | Loss: 0.00001919
Iteration 152/1000 | Loss: 0.00001919
Iteration 153/1000 | Loss: 0.00001919
Iteration 154/1000 | Loss: 0.00001919
Iteration 155/1000 | Loss: 0.00001919
Iteration 156/1000 | Loss: 0.00001919
Iteration 157/1000 | Loss: 0.00001919
Iteration 158/1000 | Loss: 0.00001919
Iteration 159/1000 | Loss: 0.00001919
Iteration 160/1000 | Loss: 0.00001919
Iteration 161/1000 | Loss: 0.00001919
Iteration 162/1000 | Loss: 0.00001919
Iteration 163/1000 | Loss: 0.00001919
Iteration 164/1000 | Loss: 0.00001919
Iteration 165/1000 | Loss: 0.00001919
Iteration 166/1000 | Loss: 0.00001919
Iteration 167/1000 | Loss: 0.00001919
Iteration 168/1000 | Loss: 0.00001919
Iteration 169/1000 | Loss: 0.00001919
Iteration 170/1000 | Loss: 0.00001919
Iteration 171/1000 | Loss: 0.00001919
Iteration 172/1000 | Loss: 0.00001919
Iteration 173/1000 | Loss: 0.00001919
Iteration 174/1000 | Loss: 0.00001919
Iteration 175/1000 | Loss: 0.00001919
Iteration 176/1000 | Loss: 0.00001919
Iteration 177/1000 | Loss: 0.00001919
Iteration 178/1000 | Loss: 0.00001919
Iteration 179/1000 | Loss: 0.00001919
Iteration 180/1000 | Loss: 0.00001919
Iteration 181/1000 | Loss: 0.00001919
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 181. Stopping optimization.
Last 5 losses: [1.9185197743354365e-05, 1.9185197743354365e-05, 1.9185197743354365e-05, 1.9185197743354365e-05, 1.9185197743354365e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9185197743354365e-05

Optimization complete. Final v2v error: 3.7410359382629395 mm

Highest mean error: 4.301437854766846 mm for frame 18

Lowest mean error: 3.452375650405884 mm for frame 158

Saving results

Total time: 96.93294620513916
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_018/1048/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_018/1048.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_018/1048
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00787042
Iteration 2/25 | Loss: 0.00118522
Iteration 3/25 | Loss: 0.00112160
Iteration 4/25 | Loss: 0.00111732
Iteration 5/25 | Loss: 0.00111598
Iteration 6/25 | Loss: 0.00111598
Iteration 7/25 | Loss: 0.00111598
Iteration 8/25 | Loss: 0.00111598
Iteration 9/25 | Loss: 0.00111598
Iteration 10/25 | Loss: 0.00111598
Iteration 11/25 | Loss: 0.00111598
Iteration 12/25 | Loss: 0.00111598
Iteration 13/25 | Loss: 0.00111598
Iteration 14/25 | Loss: 0.00111598
Iteration 15/25 | Loss: 0.00111598
Iteration 16/25 | Loss: 0.00111598
Iteration 17/25 | Loss: 0.00111598
Iteration 18/25 | Loss: 0.00111598
Iteration 19/25 | Loss: 0.00111598
Iteration 20/25 | Loss: 0.00111598
Iteration 21/25 | Loss: 0.00111598
Iteration 22/25 | Loss: 0.00111598
Iteration 23/25 | Loss: 0.00111598
Iteration 24/25 | Loss: 0.00111598
Iteration 25/25 | Loss: 0.00111598

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37163520
Iteration 2/25 | Loss: 0.00080478
Iteration 3/25 | Loss: 0.00080478
Iteration 4/25 | Loss: 0.00080477
Iteration 5/25 | Loss: 0.00080477
Iteration 6/25 | Loss: 0.00080477
Iteration 7/25 | Loss: 0.00080477
Iteration 8/25 | Loss: 0.00080477
Iteration 9/25 | Loss: 0.00080477
Iteration 10/25 | Loss: 0.00080477
Iteration 11/25 | Loss: 0.00080477
Iteration 12/25 | Loss: 0.00080477
Iteration 13/25 | Loss: 0.00080477
Iteration 14/25 | Loss: 0.00080477
Iteration 15/25 | Loss: 0.00080477
Iteration 16/25 | Loss: 0.00080477
Iteration 17/25 | Loss: 0.00080477
Iteration 18/25 | Loss: 0.00080477
Iteration 19/25 | Loss: 0.00080477
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0008047726005315781, 0.0008047726005315781, 0.0008047726005315781, 0.0008047726005315781, 0.0008047726005315781]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008047726005315781

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00080477
Iteration 2/1000 | Loss: 0.00002681
Iteration 3/1000 | Loss: 0.00001635
Iteration 4/1000 | Loss: 0.00001355
Iteration 5/1000 | Loss: 0.00001217
Iteration 6/1000 | Loss: 0.00001153
Iteration 7/1000 | Loss: 0.00001103
Iteration 8/1000 | Loss: 0.00001062
Iteration 9/1000 | Loss: 0.00001058
Iteration 10/1000 | Loss: 0.00001042
Iteration 11/1000 | Loss: 0.00001024
Iteration 12/1000 | Loss: 0.00001005
Iteration 13/1000 | Loss: 0.00001002
Iteration 14/1000 | Loss: 0.00000996
Iteration 15/1000 | Loss: 0.00000996
Iteration 16/1000 | Loss: 0.00000990
Iteration 17/1000 | Loss: 0.00000989
Iteration 18/1000 | Loss: 0.00000989
Iteration 19/1000 | Loss: 0.00000985
Iteration 20/1000 | Loss: 0.00000984
Iteration 21/1000 | Loss: 0.00000975
Iteration 22/1000 | Loss: 0.00000974
Iteration 23/1000 | Loss: 0.00000974
Iteration 24/1000 | Loss: 0.00000974
Iteration 25/1000 | Loss: 0.00000973
Iteration 26/1000 | Loss: 0.00000972
Iteration 27/1000 | Loss: 0.00000971
Iteration 28/1000 | Loss: 0.00000971
Iteration 29/1000 | Loss: 0.00000971
Iteration 30/1000 | Loss: 0.00000970
Iteration 31/1000 | Loss: 0.00000970
Iteration 32/1000 | Loss: 0.00000969
Iteration 33/1000 | Loss: 0.00000966
Iteration 34/1000 | Loss: 0.00000965
Iteration 35/1000 | Loss: 0.00000965
Iteration 36/1000 | Loss: 0.00000965
Iteration 37/1000 | Loss: 0.00000965
Iteration 38/1000 | Loss: 0.00000964
Iteration 39/1000 | Loss: 0.00000964
Iteration 40/1000 | Loss: 0.00000964
Iteration 41/1000 | Loss: 0.00000963
Iteration 42/1000 | Loss: 0.00000963
Iteration 43/1000 | Loss: 0.00000963
Iteration 44/1000 | Loss: 0.00000962
Iteration 45/1000 | Loss: 0.00000962
Iteration 46/1000 | Loss: 0.00000962
Iteration 47/1000 | Loss: 0.00000962
Iteration 48/1000 | Loss: 0.00000961
Iteration 49/1000 | Loss: 0.00000961
Iteration 50/1000 | Loss: 0.00000961
Iteration 51/1000 | Loss: 0.00000961
Iteration 52/1000 | Loss: 0.00000959
Iteration 53/1000 | Loss: 0.00000959
Iteration 54/1000 | Loss: 0.00000958
Iteration 55/1000 | Loss: 0.00000958
Iteration 56/1000 | Loss: 0.00000958
Iteration 57/1000 | Loss: 0.00000957
Iteration 58/1000 | Loss: 0.00000957
Iteration 59/1000 | Loss: 0.00000956
Iteration 60/1000 | Loss: 0.00000956
Iteration 61/1000 | Loss: 0.00000956
Iteration 62/1000 | Loss: 0.00000955
Iteration 63/1000 | Loss: 0.00000955
Iteration 64/1000 | Loss: 0.00000955
Iteration 65/1000 | Loss: 0.00000954
Iteration 66/1000 | Loss: 0.00000954
Iteration 67/1000 | Loss: 0.00000954
Iteration 68/1000 | Loss: 0.00000953
Iteration 69/1000 | Loss: 0.00000953
Iteration 70/1000 | Loss: 0.00000953
Iteration 71/1000 | Loss: 0.00000952
Iteration 72/1000 | Loss: 0.00000952
Iteration 73/1000 | Loss: 0.00000952
Iteration 74/1000 | Loss: 0.00000951
Iteration 75/1000 | Loss: 0.00000951
Iteration 76/1000 | Loss: 0.00000951
Iteration 77/1000 | Loss: 0.00000951
Iteration 78/1000 | Loss: 0.00000950
Iteration 79/1000 | Loss: 0.00000950
Iteration 80/1000 | Loss: 0.00000949
Iteration 81/1000 | Loss: 0.00000949
Iteration 82/1000 | Loss: 0.00000949
Iteration 83/1000 | Loss: 0.00000949
Iteration 84/1000 | Loss: 0.00000949
Iteration 85/1000 | Loss: 0.00000948
Iteration 86/1000 | Loss: 0.00000948
Iteration 87/1000 | Loss: 0.00000948
Iteration 88/1000 | Loss: 0.00000947
Iteration 89/1000 | Loss: 0.00000947
Iteration 90/1000 | Loss: 0.00000947
Iteration 91/1000 | Loss: 0.00000946
Iteration 92/1000 | Loss: 0.00000946
Iteration 93/1000 | Loss: 0.00000946
Iteration 94/1000 | Loss: 0.00000945
Iteration 95/1000 | Loss: 0.00000945
Iteration 96/1000 | Loss: 0.00000945
Iteration 97/1000 | Loss: 0.00000944
Iteration 98/1000 | Loss: 0.00000944
Iteration 99/1000 | Loss: 0.00000944
Iteration 100/1000 | Loss: 0.00000944
Iteration 101/1000 | Loss: 0.00000943
Iteration 102/1000 | Loss: 0.00000943
Iteration 103/1000 | Loss: 0.00000943
Iteration 104/1000 | Loss: 0.00000943
Iteration 105/1000 | Loss: 0.00000943
Iteration 106/1000 | Loss: 0.00000942
Iteration 107/1000 | Loss: 0.00000942
Iteration 108/1000 | Loss: 0.00000942
Iteration 109/1000 | Loss: 0.00000942
Iteration 110/1000 | Loss: 0.00000941
Iteration 111/1000 | Loss: 0.00000941
Iteration 112/1000 | Loss: 0.00000941
Iteration 113/1000 | Loss: 0.00000941
Iteration 114/1000 | Loss: 0.00000941
Iteration 115/1000 | Loss: 0.00000940
Iteration 116/1000 | Loss: 0.00000940
Iteration 117/1000 | Loss: 0.00000940
Iteration 118/1000 | Loss: 0.00000940
Iteration 119/1000 | Loss: 0.00000940
Iteration 120/1000 | Loss: 0.00000940
Iteration 121/1000 | Loss: 0.00000940
Iteration 122/1000 | Loss: 0.00000940
Iteration 123/1000 | Loss: 0.00000940
Iteration 124/1000 | Loss: 0.00000940
Iteration 125/1000 | Loss: 0.00000939
Iteration 126/1000 | Loss: 0.00000938
Iteration 127/1000 | Loss: 0.00000938
Iteration 128/1000 | Loss: 0.00000938
Iteration 129/1000 | Loss: 0.00000938
Iteration 130/1000 | Loss: 0.00000937
Iteration 131/1000 | Loss: 0.00000937
Iteration 132/1000 | Loss: 0.00000937
Iteration 133/1000 | Loss: 0.00000937
Iteration 134/1000 | Loss: 0.00000937
Iteration 135/1000 | Loss: 0.00000937
Iteration 136/1000 | Loss: 0.00000936
Iteration 137/1000 | Loss: 0.00000936
Iteration 138/1000 | Loss: 0.00000936
Iteration 139/1000 | Loss: 0.00000935
Iteration 140/1000 | Loss: 0.00000935
Iteration 141/1000 | Loss: 0.00000935
Iteration 142/1000 | Loss: 0.00000935
Iteration 143/1000 | Loss: 0.00000935
Iteration 144/1000 | Loss: 0.00000935
Iteration 145/1000 | Loss: 0.00000935
Iteration 146/1000 | Loss: 0.00000935
Iteration 147/1000 | Loss: 0.00000935
Iteration 148/1000 | Loss: 0.00000935
Iteration 149/1000 | Loss: 0.00000935
Iteration 150/1000 | Loss: 0.00000935
Iteration 151/1000 | Loss: 0.00000935
Iteration 152/1000 | Loss: 0.00000935
Iteration 153/1000 | Loss: 0.00000935
Iteration 154/1000 | Loss: 0.00000935
Iteration 155/1000 | Loss: 0.00000935
Iteration 156/1000 | Loss: 0.00000935
Iteration 157/1000 | Loss: 0.00000935
Iteration 158/1000 | Loss: 0.00000935
Iteration 159/1000 | Loss: 0.00000935
Iteration 160/1000 | Loss: 0.00000935
Iteration 161/1000 | Loss: 0.00000935
Iteration 162/1000 | Loss: 0.00000935
Iteration 163/1000 | Loss: 0.00000935
Iteration 164/1000 | Loss: 0.00000935
Iteration 165/1000 | Loss: 0.00000935
Iteration 166/1000 | Loss: 0.00000935
Iteration 167/1000 | Loss: 0.00000935
Iteration 168/1000 | Loss: 0.00000935
Iteration 169/1000 | Loss: 0.00000935
Iteration 170/1000 | Loss: 0.00000935
Iteration 171/1000 | Loss: 0.00000935
Iteration 172/1000 | Loss: 0.00000935
Iteration 173/1000 | Loss: 0.00000935
Iteration 174/1000 | Loss: 0.00000935
Iteration 175/1000 | Loss: 0.00000935
Iteration 176/1000 | Loss: 0.00000935
Iteration 177/1000 | Loss: 0.00000935
Iteration 178/1000 | Loss: 0.00000935
Iteration 179/1000 | Loss: 0.00000935
Iteration 180/1000 | Loss: 0.00000935
Iteration 181/1000 | Loss: 0.00000935
Iteration 182/1000 | Loss: 0.00000935
Iteration 183/1000 | Loss: 0.00000935
Iteration 184/1000 | Loss: 0.00000935
Iteration 185/1000 | Loss: 0.00000935
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 185. Stopping optimization.
Last 5 losses: [9.345686521555763e-06, 9.345686521555763e-06, 9.345686521555763e-06, 9.345686521555763e-06, 9.345686521555763e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.345686521555763e-06

Optimization complete. Final v2v error: 2.6157751083374023 mm

Highest mean error: 2.8392210006713867 mm for frame 45

Lowest mean error: 2.4661693572998047 mm for frame 150

Saving results

Total time: 38.011234998703
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_018/1016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_018/1016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_018/1016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01066324
Iteration 2/25 | Loss: 0.00199659
Iteration 3/25 | Loss: 0.00147259
Iteration 4/25 | Loss: 0.00140070
Iteration 5/25 | Loss: 0.00138694
Iteration 6/25 | Loss: 0.00138338
Iteration 7/25 | Loss: 0.00138278
Iteration 8/25 | Loss: 0.00138278
Iteration 9/25 | Loss: 0.00138278
Iteration 10/25 | Loss: 0.00138278
Iteration 11/25 | Loss: 0.00138278
Iteration 12/25 | Loss: 0.00138278
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0013827845687046647, 0.0013827845687046647, 0.0013827845687046647, 0.0013827845687046647, 0.0013827845687046647]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013827845687046647

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.06691360
Iteration 2/25 | Loss: 0.00107870
Iteration 3/25 | Loss: 0.00107841
Iteration 4/25 | Loss: 0.00107841
Iteration 5/25 | Loss: 0.00107840
Iteration 6/25 | Loss: 0.00107840
Iteration 7/25 | Loss: 0.00107840
Iteration 8/25 | Loss: 0.00107840
Iteration 9/25 | Loss: 0.00107840
Iteration 10/25 | Loss: 0.00107840
Iteration 11/25 | Loss: 0.00107840
Iteration 12/25 | Loss: 0.00107840
Iteration 13/25 | Loss: 0.00107840
Iteration 14/25 | Loss: 0.00107840
Iteration 15/25 | Loss: 0.00107840
Iteration 16/25 | Loss: 0.00107840
Iteration 17/25 | Loss: 0.00107840
Iteration 18/25 | Loss: 0.00107840
Iteration 19/25 | Loss: 0.00107840
Iteration 20/25 | Loss: 0.00107840
Iteration 21/25 | Loss: 0.00107840
Iteration 22/25 | Loss: 0.00107840
Iteration 23/25 | Loss: 0.00107840
Iteration 24/25 | Loss: 0.00107840
Iteration 25/25 | Loss: 0.00107840

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00107840
Iteration 2/1000 | Loss: 0.00008164
Iteration 3/1000 | Loss: 0.00004843
Iteration 4/1000 | Loss: 0.00004357
Iteration 5/1000 | Loss: 0.00004189
Iteration 6/1000 | Loss: 0.00004035
Iteration 7/1000 | Loss: 0.00003935
Iteration 8/1000 | Loss: 0.00003870
Iteration 9/1000 | Loss: 0.00003820
Iteration 10/1000 | Loss: 0.00003786
Iteration 11/1000 | Loss: 0.00003747
Iteration 12/1000 | Loss: 0.00003726
Iteration 13/1000 | Loss: 0.00003710
Iteration 14/1000 | Loss: 0.00003694
Iteration 15/1000 | Loss: 0.00003693
Iteration 16/1000 | Loss: 0.00003679
Iteration 17/1000 | Loss: 0.00003675
Iteration 18/1000 | Loss: 0.00003674
Iteration 19/1000 | Loss: 0.00003671
Iteration 20/1000 | Loss: 0.00003670
Iteration 21/1000 | Loss: 0.00003670
Iteration 22/1000 | Loss: 0.00003669
Iteration 23/1000 | Loss: 0.00003668
Iteration 24/1000 | Loss: 0.00003662
Iteration 25/1000 | Loss: 0.00003662
Iteration 26/1000 | Loss: 0.00003660
Iteration 27/1000 | Loss: 0.00003660
Iteration 28/1000 | Loss: 0.00003659
Iteration 29/1000 | Loss: 0.00003659
Iteration 30/1000 | Loss: 0.00003659
Iteration 31/1000 | Loss: 0.00003659
Iteration 32/1000 | Loss: 0.00003657
Iteration 33/1000 | Loss: 0.00003657
Iteration 34/1000 | Loss: 0.00003657
Iteration 35/1000 | Loss: 0.00003656
Iteration 36/1000 | Loss: 0.00003655
Iteration 37/1000 | Loss: 0.00003654
Iteration 38/1000 | Loss: 0.00003654
Iteration 39/1000 | Loss: 0.00003654
Iteration 40/1000 | Loss: 0.00003653
Iteration 41/1000 | Loss: 0.00003652
Iteration 42/1000 | Loss: 0.00003650
Iteration 43/1000 | Loss: 0.00003650
Iteration 44/1000 | Loss: 0.00003650
Iteration 45/1000 | Loss: 0.00003649
Iteration 46/1000 | Loss: 0.00003648
Iteration 47/1000 | Loss: 0.00003647
Iteration 48/1000 | Loss: 0.00003646
Iteration 49/1000 | Loss: 0.00003644
Iteration 50/1000 | Loss: 0.00003641
Iteration 51/1000 | Loss: 0.00003640
Iteration 52/1000 | Loss: 0.00003640
Iteration 53/1000 | Loss: 0.00003639
Iteration 54/1000 | Loss: 0.00003639
Iteration 55/1000 | Loss: 0.00003639
Iteration 56/1000 | Loss: 0.00003639
Iteration 57/1000 | Loss: 0.00003639
Iteration 58/1000 | Loss: 0.00003638
Iteration 59/1000 | Loss: 0.00003638
Iteration 60/1000 | Loss: 0.00003638
Iteration 61/1000 | Loss: 0.00003638
Iteration 62/1000 | Loss: 0.00003637
Iteration 63/1000 | Loss: 0.00003637
Iteration 64/1000 | Loss: 0.00003636
Iteration 65/1000 | Loss: 0.00003636
Iteration 66/1000 | Loss: 0.00003635
Iteration 67/1000 | Loss: 0.00003635
Iteration 68/1000 | Loss: 0.00003635
Iteration 69/1000 | Loss: 0.00003635
Iteration 70/1000 | Loss: 0.00003635
Iteration 71/1000 | Loss: 0.00003635
Iteration 72/1000 | Loss: 0.00003635
Iteration 73/1000 | Loss: 0.00003635
Iteration 74/1000 | Loss: 0.00003635
Iteration 75/1000 | Loss: 0.00003635
Iteration 76/1000 | Loss: 0.00003635
Iteration 77/1000 | Loss: 0.00003635
Iteration 78/1000 | Loss: 0.00003634
Iteration 79/1000 | Loss: 0.00003634
Iteration 80/1000 | Loss: 0.00003634
Iteration 81/1000 | Loss: 0.00003633
Iteration 82/1000 | Loss: 0.00003633
Iteration 83/1000 | Loss: 0.00003633
Iteration 84/1000 | Loss: 0.00003633
Iteration 85/1000 | Loss: 0.00003633
Iteration 86/1000 | Loss: 0.00003632
Iteration 87/1000 | Loss: 0.00003632
Iteration 88/1000 | Loss: 0.00003632
Iteration 89/1000 | Loss: 0.00003632
Iteration 90/1000 | Loss: 0.00003632
Iteration 91/1000 | Loss: 0.00003632
Iteration 92/1000 | Loss: 0.00003632
Iteration 93/1000 | Loss: 0.00003631
Iteration 94/1000 | Loss: 0.00003631
Iteration 95/1000 | Loss: 0.00003631
Iteration 96/1000 | Loss: 0.00003631
Iteration 97/1000 | Loss: 0.00003630
Iteration 98/1000 | Loss: 0.00003630
Iteration 99/1000 | Loss: 0.00003630
Iteration 100/1000 | Loss: 0.00003630
Iteration 101/1000 | Loss: 0.00003630
Iteration 102/1000 | Loss: 0.00003630
Iteration 103/1000 | Loss: 0.00003630
Iteration 104/1000 | Loss: 0.00003630
Iteration 105/1000 | Loss: 0.00003630
Iteration 106/1000 | Loss: 0.00003629
Iteration 107/1000 | Loss: 0.00003629
Iteration 108/1000 | Loss: 0.00003629
Iteration 109/1000 | Loss: 0.00003629
Iteration 110/1000 | Loss: 0.00003629
Iteration 111/1000 | Loss: 0.00003629
Iteration 112/1000 | Loss: 0.00003629
Iteration 113/1000 | Loss: 0.00003628
Iteration 114/1000 | Loss: 0.00003628
Iteration 115/1000 | Loss: 0.00003628
Iteration 116/1000 | Loss: 0.00003628
Iteration 117/1000 | Loss: 0.00003628
Iteration 118/1000 | Loss: 0.00003628
Iteration 119/1000 | Loss: 0.00003628
Iteration 120/1000 | Loss: 0.00003628
Iteration 121/1000 | Loss: 0.00003628
Iteration 122/1000 | Loss: 0.00003627
Iteration 123/1000 | Loss: 0.00003627
Iteration 124/1000 | Loss: 0.00003627
Iteration 125/1000 | Loss: 0.00003627
Iteration 126/1000 | Loss: 0.00003627
Iteration 127/1000 | Loss: 0.00003627
Iteration 128/1000 | Loss: 0.00003626
Iteration 129/1000 | Loss: 0.00003626
Iteration 130/1000 | Loss: 0.00003626
Iteration 131/1000 | Loss: 0.00003626
Iteration 132/1000 | Loss: 0.00003626
Iteration 133/1000 | Loss: 0.00003626
Iteration 134/1000 | Loss: 0.00003626
Iteration 135/1000 | Loss: 0.00003625
Iteration 136/1000 | Loss: 0.00003625
Iteration 137/1000 | Loss: 0.00003625
Iteration 138/1000 | Loss: 0.00003625
Iteration 139/1000 | Loss: 0.00003625
Iteration 140/1000 | Loss: 0.00003625
Iteration 141/1000 | Loss: 0.00003625
Iteration 142/1000 | Loss: 0.00003625
Iteration 143/1000 | Loss: 0.00003625
Iteration 144/1000 | Loss: 0.00003625
Iteration 145/1000 | Loss: 0.00003625
Iteration 146/1000 | Loss: 0.00003625
Iteration 147/1000 | Loss: 0.00003625
Iteration 148/1000 | Loss: 0.00003625
Iteration 149/1000 | Loss: 0.00003624
Iteration 150/1000 | Loss: 0.00003624
Iteration 151/1000 | Loss: 0.00003624
Iteration 152/1000 | Loss: 0.00003624
Iteration 153/1000 | Loss: 0.00003624
Iteration 154/1000 | Loss: 0.00003624
Iteration 155/1000 | Loss: 0.00003624
Iteration 156/1000 | Loss: 0.00003624
Iteration 157/1000 | Loss: 0.00003624
Iteration 158/1000 | Loss: 0.00003624
Iteration 159/1000 | Loss: 0.00003624
Iteration 160/1000 | Loss: 0.00003624
Iteration 161/1000 | Loss: 0.00003624
Iteration 162/1000 | Loss: 0.00003624
Iteration 163/1000 | Loss: 0.00003624
Iteration 164/1000 | Loss: 0.00003624
Iteration 165/1000 | Loss: 0.00003624
Iteration 166/1000 | Loss: 0.00003624
Iteration 167/1000 | Loss: 0.00003624
Iteration 168/1000 | Loss: 0.00003623
Iteration 169/1000 | Loss: 0.00003623
Iteration 170/1000 | Loss: 0.00003623
Iteration 171/1000 | Loss: 0.00003623
Iteration 172/1000 | Loss: 0.00003623
Iteration 173/1000 | Loss: 0.00003623
Iteration 174/1000 | Loss: 0.00003623
Iteration 175/1000 | Loss: 0.00003623
Iteration 176/1000 | Loss: 0.00003623
Iteration 177/1000 | Loss: 0.00003623
Iteration 178/1000 | Loss: 0.00003623
Iteration 179/1000 | Loss: 0.00003623
Iteration 180/1000 | Loss: 0.00003623
Iteration 181/1000 | Loss: 0.00003623
Iteration 182/1000 | Loss: 0.00003623
Iteration 183/1000 | Loss: 0.00003623
Iteration 184/1000 | Loss: 0.00003623
Iteration 185/1000 | Loss: 0.00003623
Iteration 186/1000 | Loss: 0.00003623
Iteration 187/1000 | Loss: 0.00003623
Iteration 188/1000 | Loss: 0.00003623
Iteration 189/1000 | Loss: 0.00003623
Iteration 190/1000 | Loss: 0.00003623
Iteration 191/1000 | Loss: 0.00003623
Iteration 192/1000 | Loss: 0.00003623
Iteration 193/1000 | Loss: 0.00003623
Iteration 194/1000 | Loss: 0.00003623
Iteration 195/1000 | Loss: 0.00003623
Iteration 196/1000 | Loss: 0.00003623
Iteration 197/1000 | Loss: 0.00003623
Iteration 198/1000 | Loss: 0.00003623
Iteration 199/1000 | Loss: 0.00003623
Iteration 200/1000 | Loss: 0.00003623
Iteration 201/1000 | Loss: 0.00003623
Iteration 202/1000 | Loss: 0.00003623
Iteration 203/1000 | Loss: 0.00003623
Iteration 204/1000 | Loss: 0.00003623
Iteration 205/1000 | Loss: 0.00003623
Iteration 206/1000 | Loss: 0.00003623
Iteration 207/1000 | Loss: 0.00003623
Iteration 208/1000 | Loss: 0.00003623
Iteration 209/1000 | Loss: 0.00003623
Iteration 210/1000 | Loss: 0.00003623
Iteration 211/1000 | Loss: 0.00003623
Iteration 212/1000 | Loss: 0.00003623
Iteration 213/1000 | Loss: 0.00003623
Iteration 214/1000 | Loss: 0.00003623
Iteration 215/1000 | Loss: 0.00003623
Iteration 216/1000 | Loss: 0.00003623
Iteration 217/1000 | Loss: 0.00003623
Iteration 218/1000 | Loss: 0.00003623
Iteration 219/1000 | Loss: 0.00003623
Iteration 220/1000 | Loss: 0.00003623
Iteration 221/1000 | Loss: 0.00003623
Iteration 222/1000 | Loss: 0.00003623
Iteration 223/1000 | Loss: 0.00003623
Iteration 224/1000 | Loss: 0.00003623
Iteration 225/1000 | Loss: 0.00003623
Iteration 226/1000 | Loss: 0.00003623
Iteration 227/1000 | Loss: 0.00003623
Iteration 228/1000 | Loss: 0.00003623
Iteration 229/1000 | Loss: 0.00003623
Iteration 230/1000 | Loss: 0.00003623
Iteration 231/1000 | Loss: 0.00003623
Iteration 232/1000 | Loss: 0.00003623
Iteration 233/1000 | Loss: 0.00003623
Iteration 234/1000 | Loss: 0.00003623
Iteration 235/1000 | Loss: 0.00003623
Iteration 236/1000 | Loss: 0.00003623
Iteration 237/1000 | Loss: 0.00003623
Iteration 238/1000 | Loss: 0.00003623
Iteration 239/1000 | Loss: 0.00003623
Iteration 240/1000 | Loss: 0.00003623
Iteration 241/1000 | Loss: 0.00003623
Iteration 242/1000 | Loss: 0.00003623
Iteration 243/1000 | Loss: 0.00003623
Iteration 244/1000 | Loss: 0.00003623
Iteration 245/1000 | Loss: 0.00003623
Iteration 246/1000 | Loss: 0.00003623
Iteration 247/1000 | Loss: 0.00003623
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 247. Stopping optimization.
Last 5 losses: [3.6228935641702265e-05, 3.6228935641702265e-05, 3.6228935641702265e-05, 3.6228935641702265e-05, 3.6228935641702265e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.6228935641702265e-05

Optimization complete. Final v2v error: 4.970629692077637 mm

Highest mean error: 6.1270222663879395 mm for frame 211

Lowest mean error: 3.93149995803833 mm for frame 70

Saving results

Total time: 54.8678674697876
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_018/1073/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_018/1073.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_018/1073
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00527344
Iteration 2/25 | Loss: 0.00136581
Iteration 3/25 | Loss: 0.00121268
Iteration 4/25 | Loss: 0.00120647
Iteration 5/25 | Loss: 0.00120635
Iteration 6/25 | Loss: 0.00120635
Iteration 7/25 | Loss: 0.00120635
Iteration 8/25 | Loss: 0.00120635
Iteration 9/25 | Loss: 0.00120635
Iteration 10/25 | Loss: 0.00120635
Iteration 11/25 | Loss: 0.00120635
Iteration 12/25 | Loss: 0.00120635
Iteration 13/25 | Loss: 0.00120635
Iteration 14/25 | Loss: 0.00120635
Iteration 15/25 | Loss: 0.00120635
Iteration 16/25 | Loss: 0.00120635
Iteration 17/25 | Loss: 0.00120635
Iteration 18/25 | Loss: 0.00120635
Iteration 19/25 | Loss: 0.00120635
Iteration 20/25 | Loss: 0.00120635
Iteration 21/25 | Loss: 0.00120635
Iteration 22/25 | Loss: 0.00120635
Iteration 23/25 | Loss: 0.00120635
Iteration 24/25 | Loss: 0.00120635
Iteration 25/25 | Loss: 0.00120635

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35578418
Iteration 2/25 | Loss: 0.00062806
Iteration 3/25 | Loss: 0.00062806
Iteration 4/25 | Loss: 0.00062806
Iteration 5/25 | Loss: 0.00062806
Iteration 6/25 | Loss: 0.00062806
Iteration 7/25 | Loss: 0.00062806
Iteration 8/25 | Loss: 0.00062806
Iteration 9/25 | Loss: 0.00062806
Iteration 10/25 | Loss: 0.00062806
Iteration 11/25 | Loss: 0.00062806
Iteration 12/25 | Loss: 0.00062806
Iteration 13/25 | Loss: 0.00062806
Iteration 14/25 | Loss: 0.00062806
Iteration 15/25 | Loss: 0.00062806
Iteration 16/25 | Loss: 0.00062806
Iteration 17/25 | Loss: 0.00062806
Iteration 18/25 | Loss: 0.00062806
Iteration 19/25 | Loss: 0.00062806
Iteration 20/25 | Loss: 0.00062806
Iteration 21/25 | Loss: 0.00062806
Iteration 22/25 | Loss: 0.00062806
Iteration 23/25 | Loss: 0.00062806
Iteration 24/25 | Loss: 0.00062806
Iteration 25/25 | Loss: 0.00062806

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00062806
Iteration 2/1000 | Loss: 0.00003583
Iteration 3/1000 | Loss: 0.00002377
Iteration 4/1000 | Loss: 0.00002177
Iteration 5/1000 | Loss: 0.00002074
Iteration 6/1000 | Loss: 0.00002005
Iteration 7/1000 | Loss: 0.00001954
Iteration 8/1000 | Loss: 0.00001923
Iteration 9/1000 | Loss: 0.00001883
Iteration 10/1000 | Loss: 0.00001856
Iteration 11/1000 | Loss: 0.00001845
Iteration 12/1000 | Loss: 0.00001843
Iteration 13/1000 | Loss: 0.00001837
Iteration 14/1000 | Loss: 0.00001833
Iteration 15/1000 | Loss: 0.00001833
Iteration 16/1000 | Loss: 0.00001832
Iteration 17/1000 | Loss: 0.00001831
Iteration 18/1000 | Loss: 0.00001831
Iteration 19/1000 | Loss: 0.00001830
Iteration 20/1000 | Loss: 0.00001830
Iteration 21/1000 | Loss: 0.00001829
Iteration 22/1000 | Loss: 0.00001827
Iteration 23/1000 | Loss: 0.00001826
Iteration 24/1000 | Loss: 0.00001824
Iteration 25/1000 | Loss: 0.00001822
Iteration 26/1000 | Loss: 0.00001820
Iteration 27/1000 | Loss: 0.00001820
Iteration 28/1000 | Loss: 0.00001819
Iteration 29/1000 | Loss: 0.00001819
Iteration 30/1000 | Loss: 0.00001819
Iteration 31/1000 | Loss: 0.00001819
Iteration 32/1000 | Loss: 0.00001819
Iteration 33/1000 | Loss: 0.00001819
Iteration 34/1000 | Loss: 0.00001819
Iteration 35/1000 | Loss: 0.00001818
Iteration 36/1000 | Loss: 0.00001818
Iteration 37/1000 | Loss: 0.00001818
Iteration 38/1000 | Loss: 0.00001818
Iteration 39/1000 | Loss: 0.00001818
Iteration 40/1000 | Loss: 0.00001818
Iteration 41/1000 | Loss: 0.00001818
Iteration 42/1000 | Loss: 0.00001818
Iteration 43/1000 | Loss: 0.00001817
Iteration 44/1000 | Loss: 0.00001817
Iteration 45/1000 | Loss: 0.00001817
Iteration 46/1000 | Loss: 0.00001817
Iteration 47/1000 | Loss: 0.00001817
Iteration 48/1000 | Loss: 0.00001817
Iteration 49/1000 | Loss: 0.00001817
Iteration 50/1000 | Loss: 0.00001817
Iteration 51/1000 | Loss: 0.00001817
Iteration 52/1000 | Loss: 0.00001817
Iteration 53/1000 | Loss: 0.00001817
Iteration 54/1000 | Loss: 0.00001817
Iteration 55/1000 | Loss: 0.00001816
Iteration 56/1000 | Loss: 0.00001816
Iteration 57/1000 | Loss: 0.00001816
Iteration 58/1000 | Loss: 0.00001816
Iteration 59/1000 | Loss: 0.00001816
Iteration 60/1000 | Loss: 0.00001816
Iteration 61/1000 | Loss: 0.00001816
Iteration 62/1000 | Loss: 0.00001815
Iteration 63/1000 | Loss: 0.00001815
Iteration 64/1000 | Loss: 0.00001815
Iteration 65/1000 | Loss: 0.00001814
Iteration 66/1000 | Loss: 0.00001814
Iteration 67/1000 | Loss: 0.00001813
Iteration 68/1000 | Loss: 0.00001813
Iteration 69/1000 | Loss: 0.00001812
Iteration 70/1000 | Loss: 0.00001812
Iteration 71/1000 | Loss: 0.00001812
Iteration 72/1000 | Loss: 0.00001812
Iteration 73/1000 | Loss: 0.00001812
Iteration 74/1000 | Loss: 0.00001811
Iteration 75/1000 | Loss: 0.00001811
Iteration 76/1000 | Loss: 0.00001810
Iteration 77/1000 | Loss: 0.00001809
Iteration 78/1000 | Loss: 0.00001809
Iteration 79/1000 | Loss: 0.00001809
Iteration 80/1000 | Loss: 0.00001809
Iteration 81/1000 | Loss: 0.00001808
Iteration 82/1000 | Loss: 0.00001808
Iteration 83/1000 | Loss: 0.00001807
Iteration 84/1000 | Loss: 0.00001807
Iteration 85/1000 | Loss: 0.00001807
Iteration 86/1000 | Loss: 0.00001807
Iteration 87/1000 | Loss: 0.00001807
Iteration 88/1000 | Loss: 0.00001806
Iteration 89/1000 | Loss: 0.00001806
Iteration 90/1000 | Loss: 0.00001805
Iteration 91/1000 | Loss: 0.00001805
Iteration 92/1000 | Loss: 0.00001804
Iteration 93/1000 | Loss: 0.00001804
Iteration 94/1000 | Loss: 0.00001804
Iteration 95/1000 | Loss: 0.00001804
Iteration 96/1000 | Loss: 0.00001803
Iteration 97/1000 | Loss: 0.00001803
Iteration 98/1000 | Loss: 0.00001803
Iteration 99/1000 | Loss: 0.00001803
Iteration 100/1000 | Loss: 0.00001803
Iteration 101/1000 | Loss: 0.00001802
Iteration 102/1000 | Loss: 0.00001802
Iteration 103/1000 | Loss: 0.00001802
Iteration 104/1000 | Loss: 0.00001802
Iteration 105/1000 | Loss: 0.00001802
Iteration 106/1000 | Loss: 0.00001802
Iteration 107/1000 | Loss: 0.00001802
Iteration 108/1000 | Loss: 0.00001802
Iteration 109/1000 | Loss: 0.00001801
Iteration 110/1000 | Loss: 0.00001801
Iteration 111/1000 | Loss: 0.00001801
Iteration 112/1000 | Loss: 0.00001801
Iteration 113/1000 | Loss: 0.00001801
Iteration 114/1000 | Loss: 0.00001801
Iteration 115/1000 | Loss: 0.00001801
Iteration 116/1000 | Loss: 0.00001801
Iteration 117/1000 | Loss: 0.00001801
Iteration 118/1000 | Loss: 0.00001801
Iteration 119/1000 | Loss: 0.00001801
Iteration 120/1000 | Loss: 0.00001801
Iteration 121/1000 | Loss: 0.00001801
Iteration 122/1000 | Loss: 0.00001801
Iteration 123/1000 | Loss: 0.00001801
Iteration 124/1000 | Loss: 0.00001801
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 124. Stopping optimization.
Last 5 losses: [1.8014845409197733e-05, 1.8014845409197733e-05, 1.8014845409197733e-05, 1.8014845409197733e-05, 1.8014845409197733e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8014845409197733e-05

Optimization complete. Final v2v error: 3.5806725025177 mm

Highest mean error: 3.9257466793060303 mm for frame 161

Lowest mean error: 3.3378868103027344 mm for frame 22

Saving results

Total time: 32.84101939201355
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_018/1081/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_018/1081.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_018/1081
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00589700
Iteration 2/25 | Loss: 0.00119425
Iteration 3/25 | Loss: 0.00111396
Iteration 4/25 | Loss: 0.00110530
Iteration 5/25 | Loss: 0.00110228
Iteration 6/25 | Loss: 0.00110225
Iteration 7/25 | Loss: 0.00110225
Iteration 8/25 | Loss: 0.00110225
Iteration 9/25 | Loss: 0.00110225
Iteration 10/25 | Loss: 0.00110225
Iteration 11/25 | Loss: 0.00110225
Iteration 12/25 | Loss: 0.00110225
Iteration 13/25 | Loss: 0.00110225
Iteration 14/25 | Loss: 0.00110225
Iteration 15/25 | Loss: 0.00110225
Iteration 16/25 | Loss: 0.00110225
Iteration 17/25 | Loss: 0.00110225
Iteration 18/25 | Loss: 0.00110225
Iteration 19/25 | Loss: 0.00110225
Iteration 20/25 | Loss: 0.00110225
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0011022459948435426, 0.0011022459948435426, 0.0011022459948435426, 0.0011022459948435426, 0.0011022459948435426]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011022459948435426

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.06578922
Iteration 2/25 | Loss: 0.00082500
Iteration 3/25 | Loss: 0.00082500
Iteration 4/25 | Loss: 0.00082500
Iteration 5/25 | Loss: 0.00082500
Iteration 6/25 | Loss: 0.00082500
Iteration 7/25 | Loss: 0.00082500
Iteration 8/25 | Loss: 0.00082500
Iteration 9/25 | Loss: 0.00082500
Iteration 10/25 | Loss: 0.00082500
Iteration 11/25 | Loss: 0.00082500
Iteration 12/25 | Loss: 0.00082500
Iteration 13/25 | Loss: 0.00082500
Iteration 14/25 | Loss: 0.00082500
Iteration 15/25 | Loss: 0.00082500
Iteration 16/25 | Loss: 0.00082500
Iteration 17/25 | Loss: 0.00082500
Iteration 18/25 | Loss: 0.00082500
Iteration 19/25 | Loss: 0.00082500
Iteration 20/25 | Loss: 0.00082500
Iteration 21/25 | Loss: 0.00082500
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0008249989477917552, 0.0008249989477917552, 0.0008249989477917552, 0.0008249989477917552, 0.0008249989477917552]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008249989477917552

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00082500
Iteration 2/1000 | Loss: 0.00002029
Iteration 3/1000 | Loss: 0.00001418
Iteration 4/1000 | Loss: 0.00001280
Iteration 5/1000 | Loss: 0.00001193
Iteration 6/1000 | Loss: 0.00001138
Iteration 7/1000 | Loss: 0.00001103
Iteration 8/1000 | Loss: 0.00001099
Iteration 9/1000 | Loss: 0.00001076
Iteration 10/1000 | Loss: 0.00001050
Iteration 11/1000 | Loss: 0.00001042
Iteration 12/1000 | Loss: 0.00001037
Iteration 13/1000 | Loss: 0.00001035
Iteration 14/1000 | Loss: 0.00001027
Iteration 15/1000 | Loss: 0.00001019
Iteration 16/1000 | Loss: 0.00001019
Iteration 17/1000 | Loss: 0.00001018
Iteration 18/1000 | Loss: 0.00001017
Iteration 19/1000 | Loss: 0.00001011
Iteration 20/1000 | Loss: 0.00001011
Iteration 21/1000 | Loss: 0.00001010
Iteration 22/1000 | Loss: 0.00001009
Iteration 23/1000 | Loss: 0.00001008
Iteration 24/1000 | Loss: 0.00001005
Iteration 25/1000 | Loss: 0.00001003
Iteration 26/1000 | Loss: 0.00001001
Iteration 27/1000 | Loss: 0.00001000
Iteration 28/1000 | Loss: 0.00001000
Iteration 29/1000 | Loss: 0.00001000
Iteration 30/1000 | Loss: 0.00001000
Iteration 31/1000 | Loss: 0.00001000
Iteration 32/1000 | Loss: 0.00001000
Iteration 33/1000 | Loss: 0.00001000
Iteration 34/1000 | Loss: 0.00001000
Iteration 35/1000 | Loss: 0.00000999
Iteration 36/1000 | Loss: 0.00000995
Iteration 37/1000 | Loss: 0.00000995
Iteration 38/1000 | Loss: 0.00000994
Iteration 39/1000 | Loss: 0.00000994
Iteration 40/1000 | Loss: 0.00000993
Iteration 41/1000 | Loss: 0.00000992
Iteration 42/1000 | Loss: 0.00000991
Iteration 43/1000 | Loss: 0.00000991
Iteration 44/1000 | Loss: 0.00000990
Iteration 45/1000 | Loss: 0.00000989
Iteration 46/1000 | Loss: 0.00000989
Iteration 47/1000 | Loss: 0.00000988
Iteration 48/1000 | Loss: 0.00000988
Iteration 49/1000 | Loss: 0.00000988
Iteration 50/1000 | Loss: 0.00000988
Iteration 51/1000 | Loss: 0.00000987
Iteration 52/1000 | Loss: 0.00000986
Iteration 53/1000 | Loss: 0.00000985
Iteration 54/1000 | Loss: 0.00000984
Iteration 55/1000 | Loss: 0.00000983
Iteration 56/1000 | Loss: 0.00000982
Iteration 57/1000 | Loss: 0.00000981
Iteration 58/1000 | Loss: 0.00000979
Iteration 59/1000 | Loss: 0.00000978
Iteration 60/1000 | Loss: 0.00000977
Iteration 61/1000 | Loss: 0.00000976
Iteration 62/1000 | Loss: 0.00000975
Iteration 63/1000 | Loss: 0.00000975
Iteration 64/1000 | Loss: 0.00000975
Iteration 65/1000 | Loss: 0.00000974
Iteration 66/1000 | Loss: 0.00000974
Iteration 67/1000 | Loss: 0.00000974
Iteration 68/1000 | Loss: 0.00000973
Iteration 69/1000 | Loss: 0.00000973
Iteration 70/1000 | Loss: 0.00000973
Iteration 71/1000 | Loss: 0.00000972
Iteration 72/1000 | Loss: 0.00000972
Iteration 73/1000 | Loss: 0.00000971
Iteration 74/1000 | Loss: 0.00000971
Iteration 75/1000 | Loss: 0.00000971
Iteration 76/1000 | Loss: 0.00000971
Iteration 77/1000 | Loss: 0.00000971
Iteration 78/1000 | Loss: 0.00000971
Iteration 79/1000 | Loss: 0.00000971
Iteration 80/1000 | Loss: 0.00000971
Iteration 81/1000 | Loss: 0.00000971
Iteration 82/1000 | Loss: 0.00000970
Iteration 83/1000 | Loss: 0.00000970
Iteration 84/1000 | Loss: 0.00000970
Iteration 85/1000 | Loss: 0.00000970
Iteration 86/1000 | Loss: 0.00000970
Iteration 87/1000 | Loss: 0.00000970
Iteration 88/1000 | Loss: 0.00000969
Iteration 89/1000 | Loss: 0.00000969
Iteration 90/1000 | Loss: 0.00000969
Iteration 91/1000 | Loss: 0.00000968
Iteration 92/1000 | Loss: 0.00000966
Iteration 93/1000 | Loss: 0.00000966
Iteration 94/1000 | Loss: 0.00000965
Iteration 95/1000 | Loss: 0.00000965
Iteration 96/1000 | Loss: 0.00000965
Iteration 97/1000 | Loss: 0.00000965
Iteration 98/1000 | Loss: 0.00000965
Iteration 99/1000 | Loss: 0.00000964
Iteration 100/1000 | Loss: 0.00000964
Iteration 101/1000 | Loss: 0.00000964
Iteration 102/1000 | Loss: 0.00000964
Iteration 103/1000 | Loss: 0.00000964
Iteration 104/1000 | Loss: 0.00000962
Iteration 105/1000 | Loss: 0.00000962
Iteration 106/1000 | Loss: 0.00000961
Iteration 107/1000 | Loss: 0.00000961
Iteration 108/1000 | Loss: 0.00000961
Iteration 109/1000 | Loss: 0.00000961
Iteration 110/1000 | Loss: 0.00000961
Iteration 111/1000 | Loss: 0.00000961
Iteration 112/1000 | Loss: 0.00000960
Iteration 113/1000 | Loss: 0.00000960
Iteration 114/1000 | Loss: 0.00000960
Iteration 115/1000 | Loss: 0.00000960
Iteration 116/1000 | Loss: 0.00000958
Iteration 117/1000 | Loss: 0.00000958
Iteration 118/1000 | Loss: 0.00000958
Iteration 119/1000 | Loss: 0.00000958
Iteration 120/1000 | Loss: 0.00000958
Iteration 121/1000 | Loss: 0.00000958
Iteration 122/1000 | Loss: 0.00000958
Iteration 123/1000 | Loss: 0.00000958
Iteration 124/1000 | Loss: 0.00000958
Iteration 125/1000 | Loss: 0.00000958
Iteration 126/1000 | Loss: 0.00000957
Iteration 127/1000 | Loss: 0.00000957
Iteration 128/1000 | Loss: 0.00000957
Iteration 129/1000 | Loss: 0.00000957
Iteration 130/1000 | Loss: 0.00000957
Iteration 131/1000 | Loss: 0.00000957
Iteration 132/1000 | Loss: 0.00000957
Iteration 133/1000 | Loss: 0.00000957
Iteration 134/1000 | Loss: 0.00000957
Iteration 135/1000 | Loss: 0.00000957
Iteration 136/1000 | Loss: 0.00000957
Iteration 137/1000 | Loss: 0.00000957
Iteration 138/1000 | Loss: 0.00000957
Iteration 139/1000 | Loss: 0.00000957
Iteration 140/1000 | Loss: 0.00000957
Iteration 141/1000 | Loss: 0.00000957
Iteration 142/1000 | Loss: 0.00000957
Iteration 143/1000 | Loss: 0.00000957
Iteration 144/1000 | Loss: 0.00000957
Iteration 145/1000 | Loss: 0.00000957
Iteration 146/1000 | Loss: 0.00000957
Iteration 147/1000 | Loss: 0.00000957
Iteration 148/1000 | Loss: 0.00000957
Iteration 149/1000 | Loss: 0.00000957
Iteration 150/1000 | Loss: 0.00000957
Iteration 151/1000 | Loss: 0.00000957
Iteration 152/1000 | Loss: 0.00000957
Iteration 153/1000 | Loss: 0.00000957
Iteration 154/1000 | Loss: 0.00000957
Iteration 155/1000 | Loss: 0.00000957
Iteration 156/1000 | Loss: 0.00000957
Iteration 157/1000 | Loss: 0.00000957
Iteration 158/1000 | Loss: 0.00000957
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 158. Stopping optimization.
Last 5 losses: [9.573248462402262e-06, 9.573248462402262e-06, 9.573248462402262e-06, 9.573248462402262e-06, 9.573248462402262e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.573248462402262e-06

Optimization complete. Final v2v error: 2.6831235885620117 mm

Highest mean error: 2.94081711769104 mm for frame 112

Lowest mean error: 2.551912307739258 mm for frame 159

Saving results

Total time: 38.17692542076111
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_018/1038/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_018/1038.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_018/1038
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00389411
Iteration 2/25 | Loss: 0.00123521
Iteration 3/25 | Loss: 0.00112715
Iteration 4/25 | Loss: 0.00111398
Iteration 5/25 | Loss: 0.00111099
Iteration 6/25 | Loss: 0.00111035
Iteration 7/25 | Loss: 0.00111035
Iteration 8/25 | Loss: 0.00111035
Iteration 9/25 | Loss: 0.00111035
Iteration 10/25 | Loss: 0.00111035
Iteration 11/25 | Loss: 0.00111035
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011103488504886627, 0.0011103488504886627, 0.0011103488504886627, 0.0011103488504886627, 0.0011103488504886627]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011103488504886627

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37245619
Iteration 2/25 | Loss: 0.00092423
Iteration 3/25 | Loss: 0.00092423
Iteration 4/25 | Loss: 0.00092423
Iteration 5/25 | Loss: 0.00092423
Iteration 6/25 | Loss: 0.00092423
Iteration 7/25 | Loss: 0.00092423
Iteration 8/25 | Loss: 0.00092423
Iteration 9/25 | Loss: 0.00092423
Iteration 10/25 | Loss: 0.00092423
Iteration 11/25 | Loss: 0.00092423
Iteration 12/25 | Loss: 0.00092423
Iteration 13/25 | Loss: 0.00092423
Iteration 14/25 | Loss: 0.00092422
Iteration 15/25 | Loss: 0.00092422
Iteration 16/25 | Loss: 0.00092422
Iteration 17/25 | Loss: 0.00092422
Iteration 18/25 | Loss: 0.00092422
Iteration 19/25 | Loss: 0.00092422
Iteration 20/25 | Loss: 0.00092422
Iteration 21/25 | Loss: 0.00092422
Iteration 22/25 | Loss: 0.00092422
Iteration 23/25 | Loss: 0.00092422
Iteration 24/25 | Loss: 0.00092422
Iteration 25/25 | Loss: 0.00092422

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00092422
Iteration 2/1000 | Loss: 0.00002661
Iteration 3/1000 | Loss: 0.00001597
Iteration 4/1000 | Loss: 0.00001296
Iteration 5/1000 | Loss: 0.00001151
Iteration 6/1000 | Loss: 0.00001072
Iteration 7/1000 | Loss: 0.00001021
Iteration 8/1000 | Loss: 0.00000985
Iteration 9/1000 | Loss: 0.00000975
Iteration 10/1000 | Loss: 0.00000965
Iteration 11/1000 | Loss: 0.00000960
Iteration 12/1000 | Loss: 0.00000958
Iteration 13/1000 | Loss: 0.00000956
Iteration 14/1000 | Loss: 0.00000955
Iteration 15/1000 | Loss: 0.00000948
Iteration 16/1000 | Loss: 0.00000941
Iteration 17/1000 | Loss: 0.00000939
Iteration 18/1000 | Loss: 0.00000938
Iteration 19/1000 | Loss: 0.00000937
Iteration 20/1000 | Loss: 0.00000933
Iteration 21/1000 | Loss: 0.00000931
Iteration 22/1000 | Loss: 0.00000930
Iteration 23/1000 | Loss: 0.00000925
Iteration 24/1000 | Loss: 0.00000925
Iteration 25/1000 | Loss: 0.00000925
Iteration 26/1000 | Loss: 0.00000921
Iteration 27/1000 | Loss: 0.00000920
Iteration 28/1000 | Loss: 0.00000918
Iteration 29/1000 | Loss: 0.00000917
Iteration 30/1000 | Loss: 0.00000916
Iteration 31/1000 | Loss: 0.00000915
Iteration 32/1000 | Loss: 0.00000914
Iteration 33/1000 | Loss: 0.00000914
Iteration 34/1000 | Loss: 0.00000913
Iteration 35/1000 | Loss: 0.00000913
Iteration 36/1000 | Loss: 0.00000912
Iteration 37/1000 | Loss: 0.00000911
Iteration 38/1000 | Loss: 0.00000910
Iteration 39/1000 | Loss: 0.00000910
Iteration 40/1000 | Loss: 0.00000909
Iteration 41/1000 | Loss: 0.00000908
Iteration 42/1000 | Loss: 0.00000907
Iteration 43/1000 | Loss: 0.00000907
Iteration 44/1000 | Loss: 0.00000907
Iteration 45/1000 | Loss: 0.00000906
Iteration 46/1000 | Loss: 0.00000906
Iteration 47/1000 | Loss: 0.00000905
Iteration 48/1000 | Loss: 0.00000905
Iteration 49/1000 | Loss: 0.00000905
Iteration 50/1000 | Loss: 0.00000904
Iteration 51/1000 | Loss: 0.00000904
Iteration 52/1000 | Loss: 0.00000903
Iteration 53/1000 | Loss: 0.00000903
Iteration 54/1000 | Loss: 0.00000903
Iteration 55/1000 | Loss: 0.00000903
Iteration 56/1000 | Loss: 0.00000902
Iteration 57/1000 | Loss: 0.00000901
Iteration 58/1000 | Loss: 0.00000901
Iteration 59/1000 | Loss: 0.00000901
Iteration 60/1000 | Loss: 0.00000900
Iteration 61/1000 | Loss: 0.00000900
Iteration 62/1000 | Loss: 0.00000900
Iteration 63/1000 | Loss: 0.00000899
Iteration 64/1000 | Loss: 0.00000899
Iteration 65/1000 | Loss: 0.00000899
Iteration 66/1000 | Loss: 0.00000899
Iteration 67/1000 | Loss: 0.00000899
Iteration 68/1000 | Loss: 0.00000898
Iteration 69/1000 | Loss: 0.00000898
Iteration 70/1000 | Loss: 0.00000897
Iteration 71/1000 | Loss: 0.00000897
Iteration 72/1000 | Loss: 0.00000897
Iteration 73/1000 | Loss: 0.00000896
Iteration 74/1000 | Loss: 0.00000896
Iteration 75/1000 | Loss: 0.00000896
Iteration 76/1000 | Loss: 0.00000896
Iteration 77/1000 | Loss: 0.00000895
Iteration 78/1000 | Loss: 0.00000895
Iteration 79/1000 | Loss: 0.00000895
Iteration 80/1000 | Loss: 0.00000893
Iteration 81/1000 | Loss: 0.00000892
Iteration 82/1000 | Loss: 0.00000892
Iteration 83/1000 | Loss: 0.00000892
Iteration 84/1000 | Loss: 0.00000891
Iteration 85/1000 | Loss: 0.00000891
Iteration 86/1000 | Loss: 0.00000890
Iteration 87/1000 | Loss: 0.00000890
Iteration 88/1000 | Loss: 0.00000890
Iteration 89/1000 | Loss: 0.00000890
Iteration 90/1000 | Loss: 0.00000889
Iteration 91/1000 | Loss: 0.00000889
Iteration 92/1000 | Loss: 0.00000889
Iteration 93/1000 | Loss: 0.00000889
Iteration 94/1000 | Loss: 0.00000889
Iteration 95/1000 | Loss: 0.00000889
Iteration 96/1000 | Loss: 0.00000889
Iteration 97/1000 | Loss: 0.00000888
Iteration 98/1000 | Loss: 0.00000888
Iteration 99/1000 | Loss: 0.00000888
Iteration 100/1000 | Loss: 0.00000888
Iteration 101/1000 | Loss: 0.00000888
Iteration 102/1000 | Loss: 0.00000888
Iteration 103/1000 | Loss: 0.00000888
Iteration 104/1000 | Loss: 0.00000888
Iteration 105/1000 | Loss: 0.00000888
Iteration 106/1000 | Loss: 0.00000887
Iteration 107/1000 | Loss: 0.00000887
Iteration 108/1000 | Loss: 0.00000886
Iteration 109/1000 | Loss: 0.00000886
Iteration 110/1000 | Loss: 0.00000886
Iteration 111/1000 | Loss: 0.00000886
Iteration 112/1000 | Loss: 0.00000886
Iteration 113/1000 | Loss: 0.00000886
Iteration 114/1000 | Loss: 0.00000886
Iteration 115/1000 | Loss: 0.00000886
Iteration 116/1000 | Loss: 0.00000885
Iteration 117/1000 | Loss: 0.00000885
Iteration 118/1000 | Loss: 0.00000885
Iteration 119/1000 | Loss: 0.00000885
Iteration 120/1000 | Loss: 0.00000885
Iteration 121/1000 | Loss: 0.00000885
Iteration 122/1000 | Loss: 0.00000885
Iteration 123/1000 | Loss: 0.00000885
Iteration 124/1000 | Loss: 0.00000885
Iteration 125/1000 | Loss: 0.00000884
Iteration 126/1000 | Loss: 0.00000884
Iteration 127/1000 | Loss: 0.00000884
Iteration 128/1000 | Loss: 0.00000883
Iteration 129/1000 | Loss: 0.00000883
Iteration 130/1000 | Loss: 0.00000883
Iteration 131/1000 | Loss: 0.00000882
Iteration 132/1000 | Loss: 0.00000882
Iteration 133/1000 | Loss: 0.00000882
Iteration 134/1000 | Loss: 0.00000882
Iteration 135/1000 | Loss: 0.00000881
Iteration 136/1000 | Loss: 0.00000881
Iteration 137/1000 | Loss: 0.00000880
Iteration 138/1000 | Loss: 0.00000880
Iteration 139/1000 | Loss: 0.00000880
Iteration 140/1000 | Loss: 0.00000879
Iteration 141/1000 | Loss: 0.00000879
Iteration 142/1000 | Loss: 0.00000879
Iteration 143/1000 | Loss: 0.00000879
Iteration 144/1000 | Loss: 0.00000878
Iteration 145/1000 | Loss: 0.00000878
Iteration 146/1000 | Loss: 0.00000878
Iteration 147/1000 | Loss: 0.00000878
Iteration 148/1000 | Loss: 0.00000878
Iteration 149/1000 | Loss: 0.00000878
Iteration 150/1000 | Loss: 0.00000878
Iteration 151/1000 | Loss: 0.00000878
Iteration 152/1000 | Loss: 0.00000877
Iteration 153/1000 | Loss: 0.00000877
Iteration 154/1000 | Loss: 0.00000877
Iteration 155/1000 | Loss: 0.00000877
Iteration 156/1000 | Loss: 0.00000877
Iteration 157/1000 | Loss: 0.00000877
Iteration 158/1000 | Loss: 0.00000877
Iteration 159/1000 | Loss: 0.00000877
Iteration 160/1000 | Loss: 0.00000877
Iteration 161/1000 | Loss: 0.00000877
Iteration 162/1000 | Loss: 0.00000877
Iteration 163/1000 | Loss: 0.00000877
Iteration 164/1000 | Loss: 0.00000877
Iteration 165/1000 | Loss: 0.00000877
Iteration 166/1000 | Loss: 0.00000877
Iteration 167/1000 | Loss: 0.00000876
Iteration 168/1000 | Loss: 0.00000876
Iteration 169/1000 | Loss: 0.00000876
Iteration 170/1000 | Loss: 0.00000876
Iteration 171/1000 | Loss: 0.00000876
Iteration 172/1000 | Loss: 0.00000876
Iteration 173/1000 | Loss: 0.00000876
Iteration 174/1000 | Loss: 0.00000876
Iteration 175/1000 | Loss: 0.00000876
Iteration 176/1000 | Loss: 0.00000876
Iteration 177/1000 | Loss: 0.00000876
Iteration 178/1000 | Loss: 0.00000876
Iteration 179/1000 | Loss: 0.00000876
Iteration 180/1000 | Loss: 0.00000876
Iteration 181/1000 | Loss: 0.00000876
Iteration 182/1000 | Loss: 0.00000875
Iteration 183/1000 | Loss: 0.00000875
Iteration 184/1000 | Loss: 0.00000875
Iteration 185/1000 | Loss: 0.00000875
Iteration 186/1000 | Loss: 0.00000875
Iteration 187/1000 | Loss: 0.00000875
Iteration 188/1000 | Loss: 0.00000874
Iteration 189/1000 | Loss: 0.00000874
Iteration 190/1000 | Loss: 0.00000874
Iteration 191/1000 | Loss: 0.00000874
Iteration 192/1000 | Loss: 0.00000874
Iteration 193/1000 | Loss: 0.00000874
Iteration 194/1000 | Loss: 0.00000874
Iteration 195/1000 | Loss: 0.00000874
Iteration 196/1000 | Loss: 0.00000874
Iteration 197/1000 | Loss: 0.00000874
Iteration 198/1000 | Loss: 0.00000874
Iteration 199/1000 | Loss: 0.00000874
Iteration 200/1000 | Loss: 0.00000873
Iteration 201/1000 | Loss: 0.00000873
Iteration 202/1000 | Loss: 0.00000873
Iteration 203/1000 | Loss: 0.00000873
Iteration 204/1000 | Loss: 0.00000873
Iteration 205/1000 | Loss: 0.00000873
Iteration 206/1000 | Loss: 0.00000873
Iteration 207/1000 | Loss: 0.00000873
Iteration 208/1000 | Loss: 0.00000873
Iteration 209/1000 | Loss: 0.00000873
Iteration 210/1000 | Loss: 0.00000873
Iteration 211/1000 | Loss: 0.00000873
Iteration 212/1000 | Loss: 0.00000873
Iteration 213/1000 | Loss: 0.00000873
Iteration 214/1000 | Loss: 0.00000873
Iteration 215/1000 | Loss: 0.00000873
Iteration 216/1000 | Loss: 0.00000872
Iteration 217/1000 | Loss: 0.00000872
Iteration 218/1000 | Loss: 0.00000872
Iteration 219/1000 | Loss: 0.00000872
Iteration 220/1000 | Loss: 0.00000872
Iteration 221/1000 | Loss: 0.00000872
Iteration 222/1000 | Loss: 0.00000872
Iteration 223/1000 | Loss: 0.00000872
Iteration 224/1000 | Loss: 0.00000872
Iteration 225/1000 | Loss: 0.00000872
Iteration 226/1000 | Loss: 0.00000872
Iteration 227/1000 | Loss: 0.00000872
Iteration 228/1000 | Loss: 0.00000872
Iteration 229/1000 | Loss: 0.00000872
Iteration 230/1000 | Loss: 0.00000872
Iteration 231/1000 | Loss: 0.00000872
Iteration 232/1000 | Loss: 0.00000872
Iteration 233/1000 | Loss: 0.00000872
Iteration 234/1000 | Loss: 0.00000872
Iteration 235/1000 | Loss: 0.00000872
Iteration 236/1000 | Loss: 0.00000872
Iteration 237/1000 | Loss: 0.00000872
Iteration 238/1000 | Loss: 0.00000872
Iteration 239/1000 | Loss: 0.00000872
Iteration 240/1000 | Loss: 0.00000872
Iteration 241/1000 | Loss: 0.00000872
Iteration 242/1000 | Loss: 0.00000872
Iteration 243/1000 | Loss: 0.00000872
Iteration 244/1000 | Loss: 0.00000872
Iteration 245/1000 | Loss: 0.00000872
Iteration 246/1000 | Loss: 0.00000872
Iteration 247/1000 | Loss: 0.00000872
Iteration 248/1000 | Loss: 0.00000872
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 248. Stopping optimization.
Last 5 losses: [8.722674465388991e-06, 8.722674465388991e-06, 8.722674465388991e-06, 8.722674465388991e-06, 8.722674465388991e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.722674465388991e-06

Optimization complete. Final v2v error: 2.527454137802124 mm

Highest mean error: 3.4569735527038574 mm for frame 64

Lowest mean error: 2.3591997623443604 mm for frame 142

Saving results

Total time: 42.08921265602112
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_018/1001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_018/1001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_018/1001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00403010
Iteration 2/25 | Loss: 0.00119743
Iteration 3/25 | Loss: 0.00112104
Iteration 4/25 | Loss: 0.00110796
Iteration 5/25 | Loss: 0.00110372
Iteration 6/25 | Loss: 0.00110292
Iteration 7/25 | Loss: 0.00110292
Iteration 8/25 | Loss: 0.00110292
Iteration 9/25 | Loss: 0.00110292
Iteration 10/25 | Loss: 0.00110292
Iteration 11/25 | Loss: 0.00110292
Iteration 12/25 | Loss: 0.00110292
Iteration 13/25 | Loss: 0.00110292
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0011029236484318972, 0.0011029236484318972, 0.0011029236484318972, 0.0011029236484318972, 0.0011029236484318972]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011029236484318972

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.74387884
Iteration 2/25 | Loss: 0.00078003
Iteration 3/25 | Loss: 0.00078003
Iteration 4/25 | Loss: 0.00078003
Iteration 5/25 | Loss: 0.00078003
Iteration 6/25 | Loss: 0.00078003
Iteration 7/25 | Loss: 0.00078003
Iteration 8/25 | Loss: 0.00078003
Iteration 9/25 | Loss: 0.00078003
Iteration 10/25 | Loss: 0.00078003
Iteration 11/25 | Loss: 0.00078003
Iteration 12/25 | Loss: 0.00078003
Iteration 13/25 | Loss: 0.00078003
Iteration 14/25 | Loss: 0.00078003
Iteration 15/25 | Loss: 0.00078003
Iteration 16/25 | Loss: 0.00078003
Iteration 17/25 | Loss: 0.00078003
Iteration 18/25 | Loss: 0.00078003
Iteration 19/25 | Loss: 0.00078003
Iteration 20/25 | Loss: 0.00078003
Iteration 21/25 | Loss: 0.00078003
Iteration 22/25 | Loss: 0.00078003
Iteration 23/25 | Loss: 0.00078003
Iteration 24/25 | Loss: 0.00078003
Iteration 25/25 | Loss: 0.00078003

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00078003
Iteration 2/1000 | Loss: 0.00002181
Iteration 3/1000 | Loss: 0.00001409
Iteration 4/1000 | Loss: 0.00001297
Iteration 5/1000 | Loss: 0.00001231
Iteration 6/1000 | Loss: 0.00001188
Iteration 7/1000 | Loss: 0.00001147
Iteration 8/1000 | Loss: 0.00001133
Iteration 9/1000 | Loss: 0.00001120
Iteration 10/1000 | Loss: 0.00001119
Iteration 11/1000 | Loss: 0.00001118
Iteration 12/1000 | Loss: 0.00001117
Iteration 13/1000 | Loss: 0.00001104
Iteration 14/1000 | Loss: 0.00001082
Iteration 15/1000 | Loss: 0.00001076
Iteration 16/1000 | Loss: 0.00001069
Iteration 17/1000 | Loss: 0.00001068
Iteration 18/1000 | Loss: 0.00001066
Iteration 19/1000 | Loss: 0.00001053
Iteration 20/1000 | Loss: 0.00001049
Iteration 21/1000 | Loss: 0.00001042
Iteration 22/1000 | Loss: 0.00001038
Iteration 23/1000 | Loss: 0.00001035
Iteration 24/1000 | Loss: 0.00001034
Iteration 25/1000 | Loss: 0.00001034
Iteration 26/1000 | Loss: 0.00001033
Iteration 27/1000 | Loss: 0.00001031
Iteration 28/1000 | Loss: 0.00001030
Iteration 29/1000 | Loss: 0.00001028
Iteration 30/1000 | Loss: 0.00001027
Iteration 31/1000 | Loss: 0.00001027
Iteration 32/1000 | Loss: 0.00001027
Iteration 33/1000 | Loss: 0.00001026
Iteration 34/1000 | Loss: 0.00001026
Iteration 35/1000 | Loss: 0.00001026
Iteration 36/1000 | Loss: 0.00001026
Iteration 37/1000 | Loss: 0.00001025
Iteration 38/1000 | Loss: 0.00001025
Iteration 39/1000 | Loss: 0.00001024
Iteration 40/1000 | Loss: 0.00001024
Iteration 41/1000 | Loss: 0.00001023
Iteration 42/1000 | Loss: 0.00001023
Iteration 43/1000 | Loss: 0.00001023
Iteration 44/1000 | Loss: 0.00001022
Iteration 45/1000 | Loss: 0.00001022
Iteration 46/1000 | Loss: 0.00001021
Iteration 47/1000 | Loss: 0.00001017
Iteration 48/1000 | Loss: 0.00001015
Iteration 49/1000 | Loss: 0.00001014
Iteration 50/1000 | Loss: 0.00001010
Iteration 51/1000 | Loss: 0.00001008
Iteration 52/1000 | Loss: 0.00001007
Iteration 53/1000 | Loss: 0.00001007
Iteration 54/1000 | Loss: 0.00001007
Iteration 55/1000 | Loss: 0.00001007
Iteration 56/1000 | Loss: 0.00001007
Iteration 57/1000 | Loss: 0.00001007
Iteration 58/1000 | Loss: 0.00001007
Iteration 59/1000 | Loss: 0.00001006
Iteration 60/1000 | Loss: 0.00001006
Iteration 61/1000 | Loss: 0.00001005
Iteration 62/1000 | Loss: 0.00001005
Iteration 63/1000 | Loss: 0.00001005
Iteration 64/1000 | Loss: 0.00001005
Iteration 65/1000 | Loss: 0.00001004
Iteration 66/1000 | Loss: 0.00001004
Iteration 67/1000 | Loss: 0.00001004
Iteration 68/1000 | Loss: 0.00001004
Iteration 69/1000 | Loss: 0.00001004
Iteration 70/1000 | Loss: 0.00001003
Iteration 71/1000 | Loss: 0.00001003
Iteration 72/1000 | Loss: 0.00001003
Iteration 73/1000 | Loss: 0.00001003
Iteration 74/1000 | Loss: 0.00001002
Iteration 75/1000 | Loss: 0.00001002
Iteration 76/1000 | Loss: 0.00001002
Iteration 77/1000 | Loss: 0.00001002
Iteration 78/1000 | Loss: 0.00001002
Iteration 79/1000 | Loss: 0.00001002
Iteration 80/1000 | Loss: 0.00001001
Iteration 81/1000 | Loss: 0.00001001
Iteration 82/1000 | Loss: 0.00001001
Iteration 83/1000 | Loss: 0.00001001
Iteration 84/1000 | Loss: 0.00001001
Iteration 85/1000 | Loss: 0.00001001
Iteration 86/1000 | Loss: 0.00001001
Iteration 87/1000 | Loss: 0.00001000
Iteration 88/1000 | Loss: 0.00001000
Iteration 89/1000 | Loss: 0.00001000
Iteration 90/1000 | Loss: 0.00001000
Iteration 91/1000 | Loss: 0.00001000
Iteration 92/1000 | Loss: 0.00001000
Iteration 93/1000 | Loss: 0.00001000
Iteration 94/1000 | Loss: 0.00001000
Iteration 95/1000 | Loss: 0.00001000
Iteration 96/1000 | Loss: 0.00000999
Iteration 97/1000 | Loss: 0.00000999
Iteration 98/1000 | Loss: 0.00000999
Iteration 99/1000 | Loss: 0.00000999
Iteration 100/1000 | Loss: 0.00000999
Iteration 101/1000 | Loss: 0.00000999
Iteration 102/1000 | Loss: 0.00000999
Iteration 103/1000 | Loss: 0.00000999
Iteration 104/1000 | Loss: 0.00000999
Iteration 105/1000 | Loss: 0.00000999
Iteration 106/1000 | Loss: 0.00000999
Iteration 107/1000 | Loss: 0.00000998
Iteration 108/1000 | Loss: 0.00000998
Iteration 109/1000 | Loss: 0.00000998
Iteration 110/1000 | Loss: 0.00000998
Iteration 111/1000 | Loss: 0.00000998
Iteration 112/1000 | Loss: 0.00000997
Iteration 113/1000 | Loss: 0.00000997
Iteration 114/1000 | Loss: 0.00000997
Iteration 115/1000 | Loss: 0.00000996
Iteration 116/1000 | Loss: 0.00000996
Iteration 117/1000 | Loss: 0.00000996
Iteration 118/1000 | Loss: 0.00000996
Iteration 119/1000 | Loss: 0.00000995
Iteration 120/1000 | Loss: 0.00000995
Iteration 121/1000 | Loss: 0.00000995
Iteration 122/1000 | Loss: 0.00000995
Iteration 123/1000 | Loss: 0.00000995
Iteration 124/1000 | Loss: 0.00000995
Iteration 125/1000 | Loss: 0.00000995
Iteration 126/1000 | Loss: 0.00000994
Iteration 127/1000 | Loss: 0.00000994
Iteration 128/1000 | Loss: 0.00000994
Iteration 129/1000 | Loss: 0.00000994
Iteration 130/1000 | Loss: 0.00000994
Iteration 131/1000 | Loss: 0.00000993
Iteration 132/1000 | Loss: 0.00000993
Iteration 133/1000 | Loss: 0.00000992
Iteration 134/1000 | Loss: 0.00000992
Iteration 135/1000 | Loss: 0.00000992
Iteration 136/1000 | Loss: 0.00000992
Iteration 137/1000 | Loss: 0.00000992
Iteration 138/1000 | Loss: 0.00000992
Iteration 139/1000 | Loss: 0.00000992
Iteration 140/1000 | Loss: 0.00000992
Iteration 141/1000 | Loss: 0.00000992
Iteration 142/1000 | Loss: 0.00000992
Iteration 143/1000 | Loss: 0.00000991
Iteration 144/1000 | Loss: 0.00000991
Iteration 145/1000 | Loss: 0.00000991
Iteration 146/1000 | Loss: 0.00000991
Iteration 147/1000 | Loss: 0.00000991
Iteration 148/1000 | Loss: 0.00000991
Iteration 149/1000 | Loss: 0.00000991
Iteration 150/1000 | Loss: 0.00000991
Iteration 151/1000 | Loss: 0.00000991
Iteration 152/1000 | Loss: 0.00000991
Iteration 153/1000 | Loss: 0.00000991
Iteration 154/1000 | Loss: 0.00000990
Iteration 155/1000 | Loss: 0.00000990
Iteration 156/1000 | Loss: 0.00000990
Iteration 157/1000 | Loss: 0.00000990
Iteration 158/1000 | Loss: 0.00000990
Iteration 159/1000 | Loss: 0.00000990
Iteration 160/1000 | Loss: 0.00000990
Iteration 161/1000 | Loss: 0.00000990
Iteration 162/1000 | Loss: 0.00000990
Iteration 163/1000 | Loss: 0.00000990
Iteration 164/1000 | Loss: 0.00000990
Iteration 165/1000 | Loss: 0.00000990
Iteration 166/1000 | Loss: 0.00000990
Iteration 167/1000 | Loss: 0.00000990
Iteration 168/1000 | Loss: 0.00000990
Iteration 169/1000 | Loss: 0.00000990
Iteration 170/1000 | Loss: 0.00000990
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 170. Stopping optimization.
Last 5 losses: [9.898209100356326e-06, 9.898209100356326e-06, 9.898209100356326e-06, 9.898209100356326e-06, 9.898209100356326e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.898209100356326e-06

Optimization complete. Final v2v error: 2.732271194458008 mm

Highest mean error: 3.0114693641662598 mm for frame 124

Lowest mean error: 2.5845236778259277 mm for frame 180

Saving results

Total time: 41.22224450111389
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_018/1002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_018/1002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_018/1002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00449687
Iteration 2/25 | Loss: 0.00131010
Iteration 3/25 | Loss: 0.00121652
Iteration 4/25 | Loss: 0.00120643
Iteration 5/25 | Loss: 0.00120383
Iteration 6/25 | Loss: 0.00120342
Iteration 7/25 | Loss: 0.00120342
Iteration 8/25 | Loss: 0.00120342
Iteration 9/25 | Loss: 0.00120342
Iteration 10/25 | Loss: 0.00120342
Iteration 11/25 | Loss: 0.00120342
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012034177780151367, 0.0012034177780151367, 0.0012034177780151367, 0.0012034177780151367, 0.0012034177780151367]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012034177780151367

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36374664
Iteration 2/25 | Loss: 0.00091738
Iteration 3/25 | Loss: 0.00091738
Iteration 4/25 | Loss: 0.00091738
Iteration 5/25 | Loss: 0.00091738
Iteration 6/25 | Loss: 0.00091738
Iteration 7/25 | Loss: 0.00091738
Iteration 8/25 | Loss: 0.00091738
Iteration 9/25 | Loss: 0.00091738
Iteration 10/25 | Loss: 0.00091738
Iteration 11/25 | Loss: 0.00091738
Iteration 12/25 | Loss: 0.00091738
Iteration 13/25 | Loss: 0.00091738
Iteration 14/25 | Loss: 0.00091738
Iteration 15/25 | Loss: 0.00091738
Iteration 16/25 | Loss: 0.00091738
Iteration 17/25 | Loss: 0.00091738
Iteration 18/25 | Loss: 0.00091738
Iteration 19/25 | Loss: 0.00091738
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0009173819562420249, 0.0009173819562420249, 0.0009173819562420249, 0.0009173819562420249, 0.0009173819562420249]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009173819562420249

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00091738
Iteration 2/1000 | Loss: 0.00002790
Iteration 3/1000 | Loss: 0.00001954
Iteration 4/1000 | Loss: 0.00001816
Iteration 5/1000 | Loss: 0.00001732
Iteration 6/1000 | Loss: 0.00001686
Iteration 7/1000 | Loss: 0.00001653
Iteration 8/1000 | Loss: 0.00001641
Iteration 9/1000 | Loss: 0.00001628
Iteration 10/1000 | Loss: 0.00001616
Iteration 11/1000 | Loss: 0.00001605
Iteration 12/1000 | Loss: 0.00001599
Iteration 13/1000 | Loss: 0.00001587
Iteration 14/1000 | Loss: 0.00001582
Iteration 15/1000 | Loss: 0.00001582
Iteration 16/1000 | Loss: 0.00001581
Iteration 17/1000 | Loss: 0.00001581
Iteration 18/1000 | Loss: 0.00001580
Iteration 19/1000 | Loss: 0.00001578
Iteration 20/1000 | Loss: 0.00001577
Iteration 21/1000 | Loss: 0.00001576
Iteration 22/1000 | Loss: 0.00001575
Iteration 23/1000 | Loss: 0.00001575
Iteration 24/1000 | Loss: 0.00001575
Iteration 25/1000 | Loss: 0.00001574
Iteration 26/1000 | Loss: 0.00001574
Iteration 27/1000 | Loss: 0.00001570
Iteration 28/1000 | Loss: 0.00001565
Iteration 29/1000 | Loss: 0.00001563
Iteration 30/1000 | Loss: 0.00001563
Iteration 31/1000 | Loss: 0.00001562
Iteration 32/1000 | Loss: 0.00001562
Iteration 33/1000 | Loss: 0.00001562
Iteration 34/1000 | Loss: 0.00001562
Iteration 35/1000 | Loss: 0.00001561
Iteration 36/1000 | Loss: 0.00001561
Iteration 37/1000 | Loss: 0.00001560
Iteration 38/1000 | Loss: 0.00001560
Iteration 39/1000 | Loss: 0.00001558
Iteration 40/1000 | Loss: 0.00001557
Iteration 41/1000 | Loss: 0.00001557
Iteration 42/1000 | Loss: 0.00001555
Iteration 43/1000 | Loss: 0.00001555
Iteration 44/1000 | Loss: 0.00001554
Iteration 45/1000 | Loss: 0.00001554
Iteration 46/1000 | Loss: 0.00001553
Iteration 47/1000 | Loss: 0.00001551
Iteration 48/1000 | Loss: 0.00001550
Iteration 49/1000 | Loss: 0.00001550
Iteration 50/1000 | Loss: 0.00001549
Iteration 51/1000 | Loss: 0.00001549
Iteration 52/1000 | Loss: 0.00001549
Iteration 53/1000 | Loss: 0.00001549
Iteration 54/1000 | Loss: 0.00001548
Iteration 55/1000 | Loss: 0.00001548
Iteration 56/1000 | Loss: 0.00001548
Iteration 57/1000 | Loss: 0.00001547
Iteration 58/1000 | Loss: 0.00001547
Iteration 59/1000 | Loss: 0.00001546
Iteration 60/1000 | Loss: 0.00001546
Iteration 61/1000 | Loss: 0.00001546
Iteration 62/1000 | Loss: 0.00001545
Iteration 63/1000 | Loss: 0.00001545
Iteration 64/1000 | Loss: 0.00001545
Iteration 65/1000 | Loss: 0.00001545
Iteration 66/1000 | Loss: 0.00001544
Iteration 67/1000 | Loss: 0.00001543
Iteration 68/1000 | Loss: 0.00001543
Iteration 69/1000 | Loss: 0.00001543
Iteration 70/1000 | Loss: 0.00001542
Iteration 71/1000 | Loss: 0.00001542
Iteration 72/1000 | Loss: 0.00001541
Iteration 73/1000 | Loss: 0.00001541
Iteration 74/1000 | Loss: 0.00001541
Iteration 75/1000 | Loss: 0.00001541
Iteration 76/1000 | Loss: 0.00001541
Iteration 77/1000 | Loss: 0.00001541
Iteration 78/1000 | Loss: 0.00001540
Iteration 79/1000 | Loss: 0.00001540
Iteration 80/1000 | Loss: 0.00001540
Iteration 81/1000 | Loss: 0.00001540
Iteration 82/1000 | Loss: 0.00001540
Iteration 83/1000 | Loss: 0.00001540
Iteration 84/1000 | Loss: 0.00001539
Iteration 85/1000 | Loss: 0.00001539
Iteration 86/1000 | Loss: 0.00001539
Iteration 87/1000 | Loss: 0.00001538
Iteration 88/1000 | Loss: 0.00001538
Iteration 89/1000 | Loss: 0.00001538
Iteration 90/1000 | Loss: 0.00001538
Iteration 91/1000 | Loss: 0.00001538
Iteration 92/1000 | Loss: 0.00001538
Iteration 93/1000 | Loss: 0.00001537
Iteration 94/1000 | Loss: 0.00001537
Iteration 95/1000 | Loss: 0.00001537
Iteration 96/1000 | Loss: 0.00001537
Iteration 97/1000 | Loss: 0.00001536
Iteration 98/1000 | Loss: 0.00001536
Iteration 99/1000 | Loss: 0.00001536
Iteration 100/1000 | Loss: 0.00001536
Iteration 101/1000 | Loss: 0.00001535
Iteration 102/1000 | Loss: 0.00001535
Iteration 103/1000 | Loss: 0.00001535
Iteration 104/1000 | Loss: 0.00001534
Iteration 105/1000 | Loss: 0.00001534
Iteration 106/1000 | Loss: 0.00001534
Iteration 107/1000 | Loss: 0.00001534
Iteration 108/1000 | Loss: 0.00001534
Iteration 109/1000 | Loss: 0.00001533
Iteration 110/1000 | Loss: 0.00001533
Iteration 111/1000 | Loss: 0.00001533
Iteration 112/1000 | Loss: 0.00001532
Iteration 113/1000 | Loss: 0.00001532
Iteration 114/1000 | Loss: 0.00001532
Iteration 115/1000 | Loss: 0.00001532
Iteration 116/1000 | Loss: 0.00001532
Iteration 117/1000 | Loss: 0.00001532
Iteration 118/1000 | Loss: 0.00001532
Iteration 119/1000 | Loss: 0.00001532
Iteration 120/1000 | Loss: 0.00001532
Iteration 121/1000 | Loss: 0.00001531
Iteration 122/1000 | Loss: 0.00001531
Iteration 123/1000 | Loss: 0.00001531
Iteration 124/1000 | Loss: 0.00001531
Iteration 125/1000 | Loss: 0.00001531
Iteration 126/1000 | Loss: 0.00001531
Iteration 127/1000 | Loss: 0.00001531
Iteration 128/1000 | Loss: 0.00001531
Iteration 129/1000 | Loss: 0.00001531
Iteration 130/1000 | Loss: 0.00001530
Iteration 131/1000 | Loss: 0.00001530
Iteration 132/1000 | Loss: 0.00001530
Iteration 133/1000 | Loss: 0.00001530
Iteration 134/1000 | Loss: 0.00001530
Iteration 135/1000 | Loss: 0.00001530
Iteration 136/1000 | Loss: 0.00001529
Iteration 137/1000 | Loss: 0.00001529
Iteration 138/1000 | Loss: 0.00001529
Iteration 139/1000 | Loss: 0.00001529
Iteration 140/1000 | Loss: 0.00001529
Iteration 141/1000 | Loss: 0.00001529
Iteration 142/1000 | Loss: 0.00001529
Iteration 143/1000 | Loss: 0.00001529
Iteration 144/1000 | Loss: 0.00001529
Iteration 145/1000 | Loss: 0.00001529
Iteration 146/1000 | Loss: 0.00001529
Iteration 147/1000 | Loss: 0.00001529
Iteration 148/1000 | Loss: 0.00001529
Iteration 149/1000 | Loss: 0.00001528
Iteration 150/1000 | Loss: 0.00001528
Iteration 151/1000 | Loss: 0.00001528
Iteration 152/1000 | Loss: 0.00001528
Iteration 153/1000 | Loss: 0.00001528
Iteration 154/1000 | Loss: 0.00001528
Iteration 155/1000 | Loss: 0.00001528
Iteration 156/1000 | Loss: 0.00001528
Iteration 157/1000 | Loss: 0.00001528
Iteration 158/1000 | Loss: 0.00001528
Iteration 159/1000 | Loss: 0.00001528
Iteration 160/1000 | Loss: 0.00001528
Iteration 161/1000 | Loss: 0.00001528
Iteration 162/1000 | Loss: 0.00001528
Iteration 163/1000 | Loss: 0.00001527
Iteration 164/1000 | Loss: 0.00001527
Iteration 165/1000 | Loss: 0.00001527
Iteration 166/1000 | Loss: 0.00001527
Iteration 167/1000 | Loss: 0.00001527
Iteration 168/1000 | Loss: 0.00001527
Iteration 169/1000 | Loss: 0.00001526
Iteration 170/1000 | Loss: 0.00001526
Iteration 171/1000 | Loss: 0.00001526
Iteration 172/1000 | Loss: 0.00001526
Iteration 173/1000 | Loss: 0.00001526
Iteration 174/1000 | Loss: 0.00001526
Iteration 175/1000 | Loss: 0.00001526
Iteration 176/1000 | Loss: 0.00001526
Iteration 177/1000 | Loss: 0.00001526
Iteration 178/1000 | Loss: 0.00001526
Iteration 179/1000 | Loss: 0.00001526
Iteration 180/1000 | Loss: 0.00001526
Iteration 181/1000 | Loss: 0.00001526
Iteration 182/1000 | Loss: 0.00001526
Iteration 183/1000 | Loss: 0.00001525
Iteration 184/1000 | Loss: 0.00001525
Iteration 185/1000 | Loss: 0.00001525
Iteration 186/1000 | Loss: 0.00001525
Iteration 187/1000 | Loss: 0.00001525
Iteration 188/1000 | Loss: 0.00001525
Iteration 189/1000 | Loss: 0.00001525
Iteration 190/1000 | Loss: 0.00001525
Iteration 191/1000 | Loss: 0.00001525
Iteration 192/1000 | Loss: 0.00001525
Iteration 193/1000 | Loss: 0.00001525
Iteration 194/1000 | Loss: 0.00001524
Iteration 195/1000 | Loss: 0.00001524
Iteration 196/1000 | Loss: 0.00001524
Iteration 197/1000 | Loss: 0.00001524
Iteration 198/1000 | Loss: 0.00001524
Iteration 199/1000 | Loss: 0.00001524
Iteration 200/1000 | Loss: 0.00001524
Iteration 201/1000 | Loss: 0.00001524
Iteration 202/1000 | Loss: 0.00001524
Iteration 203/1000 | Loss: 0.00001524
Iteration 204/1000 | Loss: 0.00001524
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 204. Stopping optimization.
Last 5 losses: [1.5242874724208377e-05, 1.5242874724208377e-05, 1.5242874724208377e-05, 1.5242874724208377e-05, 1.5242874724208377e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5242874724208377e-05

Optimization complete. Final v2v error: 3.21248459815979 mm

Highest mean error: 3.5342605113983154 mm for frame 160

Lowest mean error: 2.947061538696289 mm for frame 2

Saving results

Total time: 39.53078031539917
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_018/1031/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_018/1031.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_018/1031
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00979400
Iteration 2/25 | Loss: 0.00348077
Iteration 3/25 | Loss: 0.00225544
Iteration 4/25 | Loss: 0.00203525
Iteration 5/25 | Loss: 0.00182732
Iteration 6/25 | Loss: 0.00169090
Iteration 7/25 | Loss: 0.00163424
Iteration 8/25 | Loss: 0.00158963
Iteration 9/25 | Loss: 0.00155707
Iteration 10/25 | Loss: 0.00154390
Iteration 11/25 | Loss: 0.00152397
Iteration 12/25 | Loss: 0.00153496
Iteration 13/25 | Loss: 0.00151009
Iteration 14/25 | Loss: 0.00147891
Iteration 15/25 | Loss: 0.00146604
Iteration 16/25 | Loss: 0.00144651
Iteration 17/25 | Loss: 0.00143215
Iteration 18/25 | Loss: 0.00143067
Iteration 19/25 | Loss: 0.00141536
Iteration 20/25 | Loss: 0.00140586
Iteration 21/25 | Loss: 0.00140013
Iteration 22/25 | Loss: 0.00139237
Iteration 23/25 | Loss: 0.00139101
Iteration 24/25 | Loss: 0.00138806
Iteration 25/25 | Loss: 0.00138614

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.40182853
Iteration 2/25 | Loss: 0.01197329
Iteration 3/25 | Loss: 0.03230702
Iteration 4/25 | Loss: 0.01500133
Iteration 5/25 | Loss: 0.00397763
Iteration 6/25 | Loss: 0.00285081
Iteration 7/25 | Loss: 0.00251169
Iteration 8/25 | Loss: 0.00251169
Iteration 9/25 | Loss: 0.00251169
Iteration 10/25 | Loss: 0.00251169
Iteration 11/25 | Loss: 0.00251169
Iteration 12/25 | Loss: 0.00251169
Iteration 13/25 | Loss: 0.00251169
Iteration 14/25 | Loss: 0.00251169
Iteration 15/25 | Loss: 0.00251169
Iteration 16/25 | Loss: 0.00251169
Iteration 17/25 | Loss: 0.00251169
Iteration 18/25 | Loss: 0.00251169
Iteration 19/25 | Loss: 0.00251169
Iteration 20/25 | Loss: 0.00251169
Iteration 21/25 | Loss: 0.00251169
Iteration 22/25 | Loss: 0.00251169
Iteration 23/25 | Loss: 0.00251169
Iteration 24/25 | Loss: 0.00251169
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0025116901379078627, 0.0025116901379078627, 0.0025116901379078627, 0.0025116901379078627, 0.0025116901379078627]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0025116901379078627

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00251169
Iteration 2/1000 | Loss: 0.00047906
Iteration 3/1000 | Loss: 0.00286374
Iteration 4/1000 | Loss: 0.00048590
Iteration 5/1000 | Loss: 0.00086998
Iteration 6/1000 | Loss: 0.00182783
Iteration 7/1000 | Loss: 0.00032072
Iteration 8/1000 | Loss: 0.00019247
Iteration 9/1000 | Loss: 0.00047118
Iteration 10/1000 | Loss: 0.00040540
Iteration 11/1000 | Loss: 0.00043264
Iteration 12/1000 | Loss: 0.00067069
Iteration 13/1000 | Loss: 0.00074793
Iteration 14/1000 | Loss: 0.00207414
Iteration 15/1000 | Loss: 0.00035558
Iteration 16/1000 | Loss: 0.00030295
Iteration 17/1000 | Loss: 0.00018450
Iteration 18/1000 | Loss: 0.00046462
Iteration 19/1000 | Loss: 0.00018975
Iteration 20/1000 | Loss: 0.00014512
Iteration 21/1000 | Loss: 0.00013770
Iteration 22/1000 | Loss: 0.00045334
Iteration 23/1000 | Loss: 0.00040199
Iteration 24/1000 | Loss: 0.00018543
Iteration 25/1000 | Loss: 0.00105042
Iteration 26/1000 | Loss: 0.00042760
Iteration 27/1000 | Loss: 0.00078065
Iteration 28/1000 | Loss: 0.00076424
Iteration 29/1000 | Loss: 0.00066545
Iteration 30/1000 | Loss: 0.00071962
Iteration 31/1000 | Loss: 0.00022242
Iteration 32/1000 | Loss: 0.00020040
Iteration 33/1000 | Loss: 0.00014035
Iteration 34/1000 | Loss: 0.00049698
Iteration 35/1000 | Loss: 0.00013664
Iteration 36/1000 | Loss: 0.00013321
Iteration 37/1000 | Loss: 0.00014290
Iteration 38/1000 | Loss: 0.00015139
Iteration 39/1000 | Loss: 0.00012947
Iteration 40/1000 | Loss: 0.00047185
Iteration 41/1000 | Loss: 0.00020579
Iteration 42/1000 | Loss: 0.00013060
Iteration 43/1000 | Loss: 0.00019733
Iteration 44/1000 | Loss: 0.00024002
Iteration 45/1000 | Loss: 0.00016726
Iteration 46/1000 | Loss: 0.00041999
Iteration 47/1000 | Loss: 0.00025108
Iteration 48/1000 | Loss: 0.00090599
Iteration 49/1000 | Loss: 0.00406240
Iteration 50/1000 | Loss: 0.00171759
Iteration 51/1000 | Loss: 0.00247933
Iteration 52/1000 | Loss: 0.00234719
Iteration 53/1000 | Loss: 0.00042796
Iteration 54/1000 | Loss: 0.00216786
Iteration 55/1000 | Loss: 0.00022296
Iteration 56/1000 | Loss: 0.00040728
Iteration 57/1000 | Loss: 0.00268749
Iteration 58/1000 | Loss: 0.00050245
Iteration 59/1000 | Loss: 0.00010437
Iteration 60/1000 | Loss: 0.00021862
Iteration 61/1000 | Loss: 0.00009546
Iteration 62/1000 | Loss: 0.00086639
Iteration 63/1000 | Loss: 0.00042429
Iteration 64/1000 | Loss: 0.00008687
Iteration 65/1000 | Loss: 0.00022471
Iteration 66/1000 | Loss: 0.00039156
Iteration 67/1000 | Loss: 0.00008461
Iteration 68/1000 | Loss: 0.00091790
Iteration 69/1000 | Loss: 0.00101222
Iteration 70/1000 | Loss: 0.00038879
Iteration 71/1000 | Loss: 0.00010207
Iteration 72/1000 | Loss: 0.00024817
Iteration 73/1000 | Loss: 0.00007317
Iteration 74/1000 | Loss: 0.00031312
Iteration 75/1000 | Loss: 0.00007840
Iteration 76/1000 | Loss: 0.00081858
Iteration 77/1000 | Loss: 0.00050184
Iteration 78/1000 | Loss: 0.00007548
Iteration 79/1000 | Loss: 0.00006502
Iteration 80/1000 | Loss: 0.00079943
Iteration 81/1000 | Loss: 0.00065929
Iteration 82/1000 | Loss: 0.00008422
Iteration 83/1000 | Loss: 0.00022829
Iteration 84/1000 | Loss: 0.00078522
Iteration 85/1000 | Loss: 0.00006813
Iteration 86/1000 | Loss: 0.00012005
Iteration 87/1000 | Loss: 0.00117437
Iteration 88/1000 | Loss: 0.00033933
Iteration 89/1000 | Loss: 0.00007413
Iteration 90/1000 | Loss: 0.00008650
Iteration 91/1000 | Loss: 0.00011009
Iteration 92/1000 | Loss: 0.00018616
Iteration 93/1000 | Loss: 0.00005617
Iteration 94/1000 | Loss: 0.00005688
Iteration 95/1000 | Loss: 0.00025962
Iteration 96/1000 | Loss: 0.00004853
Iteration 97/1000 | Loss: 0.00018183
Iteration 98/1000 | Loss: 0.00024911
Iteration 99/1000 | Loss: 0.00044219
Iteration 100/1000 | Loss: 0.00011806
Iteration 101/1000 | Loss: 0.00005850
Iteration 102/1000 | Loss: 0.00012007
Iteration 103/1000 | Loss: 0.00004781
Iteration 104/1000 | Loss: 0.00004664
Iteration 105/1000 | Loss: 0.00004616
Iteration 106/1000 | Loss: 0.00057248
Iteration 107/1000 | Loss: 0.00005129
Iteration 108/1000 | Loss: 0.00012238
Iteration 109/1000 | Loss: 0.00006131
Iteration 110/1000 | Loss: 0.00004544
Iteration 111/1000 | Loss: 0.00004509
Iteration 112/1000 | Loss: 0.00006006
Iteration 113/1000 | Loss: 0.00088225
Iteration 114/1000 | Loss: 0.00116356
Iteration 115/1000 | Loss: 0.00073286
Iteration 116/1000 | Loss: 0.00005358
Iteration 117/1000 | Loss: 0.00005765
Iteration 118/1000 | Loss: 0.00022052
Iteration 119/1000 | Loss: 0.00006744
Iteration 120/1000 | Loss: 0.00031092
Iteration 121/1000 | Loss: 0.00016256
Iteration 122/1000 | Loss: 0.00013835
Iteration 123/1000 | Loss: 0.00017800
Iteration 124/1000 | Loss: 0.00004488
Iteration 125/1000 | Loss: 0.00013436
Iteration 126/1000 | Loss: 0.00006720
Iteration 127/1000 | Loss: 0.00006978
Iteration 128/1000 | Loss: 0.00010562
Iteration 129/1000 | Loss: 0.00005473
Iteration 130/1000 | Loss: 0.00004377
Iteration 131/1000 | Loss: 0.00004342
Iteration 132/1000 | Loss: 0.00004322
Iteration 133/1000 | Loss: 0.00004314
Iteration 134/1000 | Loss: 0.00004312
Iteration 135/1000 | Loss: 0.00004311
Iteration 136/1000 | Loss: 0.00004307
Iteration 137/1000 | Loss: 0.00004307
Iteration 138/1000 | Loss: 0.00004306
Iteration 139/1000 | Loss: 0.00004301
Iteration 140/1000 | Loss: 0.00004299
Iteration 141/1000 | Loss: 0.00004299
Iteration 142/1000 | Loss: 0.00004296
Iteration 143/1000 | Loss: 0.00004296
Iteration 144/1000 | Loss: 0.00004296
Iteration 145/1000 | Loss: 0.00004287
Iteration 146/1000 | Loss: 0.00004281
Iteration 147/1000 | Loss: 0.00004275
Iteration 148/1000 | Loss: 0.00004274
Iteration 149/1000 | Loss: 0.00004273
Iteration 150/1000 | Loss: 0.00004272
Iteration 151/1000 | Loss: 0.00004271
Iteration 152/1000 | Loss: 0.00004271
Iteration 153/1000 | Loss: 0.00004270
Iteration 154/1000 | Loss: 0.00004270
Iteration 155/1000 | Loss: 0.00004270
Iteration 156/1000 | Loss: 0.00004270
Iteration 157/1000 | Loss: 0.00004270
Iteration 158/1000 | Loss: 0.00004270
Iteration 159/1000 | Loss: 0.00004269
Iteration 160/1000 | Loss: 0.00004269
Iteration 161/1000 | Loss: 0.00004269
Iteration 162/1000 | Loss: 0.00004269
Iteration 163/1000 | Loss: 0.00004269
Iteration 164/1000 | Loss: 0.00004268
Iteration 165/1000 | Loss: 0.00004268
Iteration 166/1000 | Loss: 0.00004268
Iteration 167/1000 | Loss: 0.00004268
Iteration 168/1000 | Loss: 0.00004268
Iteration 169/1000 | Loss: 0.00004268
Iteration 170/1000 | Loss: 0.00004268
Iteration 171/1000 | Loss: 0.00004267
Iteration 172/1000 | Loss: 0.00004267
Iteration 173/1000 | Loss: 0.00004267
Iteration 174/1000 | Loss: 0.00004267
Iteration 175/1000 | Loss: 0.00004267
Iteration 176/1000 | Loss: 0.00004266
Iteration 177/1000 | Loss: 0.00004266
Iteration 178/1000 | Loss: 0.00004266
Iteration 179/1000 | Loss: 0.00004265
Iteration 180/1000 | Loss: 0.00004265
Iteration 181/1000 | Loss: 0.00004265
Iteration 182/1000 | Loss: 0.00004264
Iteration 183/1000 | Loss: 0.00004264
Iteration 184/1000 | Loss: 0.00004262
Iteration 185/1000 | Loss: 0.00004261
Iteration 186/1000 | Loss: 0.00004261
Iteration 187/1000 | Loss: 0.00004260
Iteration 188/1000 | Loss: 0.00004260
Iteration 189/1000 | Loss: 0.00004259
Iteration 190/1000 | Loss: 0.00004258
Iteration 191/1000 | Loss: 0.00004257
Iteration 192/1000 | Loss: 0.00004256
Iteration 193/1000 | Loss: 0.00004256
Iteration 194/1000 | Loss: 0.00004255
Iteration 195/1000 | Loss: 0.00004255
Iteration 196/1000 | Loss: 0.00004254
Iteration 197/1000 | Loss: 0.00004254
Iteration 198/1000 | Loss: 0.00004253
Iteration 199/1000 | Loss: 0.00004253
Iteration 200/1000 | Loss: 0.00004252
Iteration 201/1000 | Loss: 0.00004250
Iteration 202/1000 | Loss: 0.00004250
Iteration 203/1000 | Loss: 0.00004250
Iteration 204/1000 | Loss: 0.00004249
Iteration 205/1000 | Loss: 0.00004249
Iteration 206/1000 | Loss: 0.00004248
Iteration 207/1000 | Loss: 0.00004248
Iteration 208/1000 | Loss: 0.00004248
Iteration 209/1000 | Loss: 0.00058915
Iteration 210/1000 | Loss: 0.00023448
Iteration 211/1000 | Loss: 0.00014947
Iteration 212/1000 | Loss: 0.00020060
Iteration 213/1000 | Loss: 0.00063438
Iteration 214/1000 | Loss: 0.00022247
Iteration 215/1000 | Loss: 0.00005325
Iteration 216/1000 | Loss: 0.00004365
Iteration 217/1000 | Loss: 0.00008588
Iteration 218/1000 | Loss: 0.00004439
Iteration 219/1000 | Loss: 0.00026607
Iteration 220/1000 | Loss: 0.00006290
Iteration 221/1000 | Loss: 0.00005193
Iteration 222/1000 | Loss: 0.00004153
Iteration 223/1000 | Loss: 0.00004088
Iteration 224/1000 | Loss: 0.00004070
Iteration 225/1000 | Loss: 0.00004067
Iteration 226/1000 | Loss: 0.00004066
Iteration 227/1000 | Loss: 0.00004050
Iteration 228/1000 | Loss: 0.00004046
Iteration 229/1000 | Loss: 0.00004045
Iteration 230/1000 | Loss: 0.00004045
Iteration 231/1000 | Loss: 0.00004045
Iteration 232/1000 | Loss: 0.00004045
Iteration 233/1000 | Loss: 0.00004045
Iteration 234/1000 | Loss: 0.00004045
Iteration 235/1000 | Loss: 0.00004044
Iteration 236/1000 | Loss: 0.00004044
Iteration 237/1000 | Loss: 0.00004044
Iteration 238/1000 | Loss: 0.00004044
Iteration 239/1000 | Loss: 0.00004044
Iteration 240/1000 | Loss: 0.00004044
Iteration 241/1000 | Loss: 0.00004044
Iteration 242/1000 | Loss: 0.00004044
Iteration 243/1000 | Loss: 0.00004044
Iteration 244/1000 | Loss: 0.00004044
Iteration 245/1000 | Loss: 0.00004044
Iteration 246/1000 | Loss: 0.00004044
Iteration 247/1000 | Loss: 0.00004044
Iteration 248/1000 | Loss: 0.00004043
Iteration 249/1000 | Loss: 0.00004043
Iteration 250/1000 | Loss: 0.00004043
Iteration 251/1000 | Loss: 0.00004043
Iteration 252/1000 | Loss: 0.00004043
Iteration 253/1000 | Loss: 0.00004042
Iteration 254/1000 | Loss: 0.00004041
Iteration 255/1000 | Loss: 0.00004041
Iteration 256/1000 | Loss: 0.00004041
Iteration 257/1000 | Loss: 0.00004041
Iteration 258/1000 | Loss: 0.00004041
Iteration 259/1000 | Loss: 0.00004040
Iteration 260/1000 | Loss: 0.00004040
Iteration 261/1000 | Loss: 0.00004040
Iteration 262/1000 | Loss: 0.00004040
Iteration 263/1000 | Loss: 0.00004040
Iteration 264/1000 | Loss: 0.00004039
Iteration 265/1000 | Loss: 0.00004039
Iteration 266/1000 | Loss: 0.00004039
Iteration 267/1000 | Loss: 0.00004039
Iteration 268/1000 | Loss: 0.00004039
Iteration 269/1000 | Loss: 0.00004039
Iteration 270/1000 | Loss: 0.00004039
Iteration 271/1000 | Loss: 0.00004039
Iteration 272/1000 | Loss: 0.00004039
Iteration 273/1000 | Loss: 0.00004039
Iteration 274/1000 | Loss: 0.00004039
Iteration 275/1000 | Loss: 0.00004039
Iteration 276/1000 | Loss: 0.00004039
Iteration 277/1000 | Loss: 0.00004039
Iteration 278/1000 | Loss: 0.00004039
Iteration 279/1000 | Loss: 0.00004039
Iteration 280/1000 | Loss: 0.00004039
Iteration 281/1000 | Loss: 0.00004039
Iteration 282/1000 | Loss: 0.00004039
Iteration 283/1000 | Loss: 0.00004039
Iteration 284/1000 | Loss: 0.00004039
Iteration 285/1000 | Loss: 0.00004039
Iteration 286/1000 | Loss: 0.00004039
Iteration 287/1000 | Loss: 0.00004039
Iteration 288/1000 | Loss: 0.00004039
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 288. Stopping optimization.
Last 5 losses: [4.038619954371825e-05, 4.038619954371825e-05, 4.038619954371825e-05, 4.038619954371825e-05, 4.038619954371825e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.038619954371825e-05

Optimization complete. Final v2v error: 4.778417110443115 mm

Highest mean error: 20.150659561157227 mm for frame 48

Lowest mean error: 3.2237634658813477 mm for frame 110

Saving results

Total time: 322.56935119628906
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_018/1039/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_018/1039.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_018/1039
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00799875
Iteration 2/25 | Loss: 0.00176148
Iteration 3/25 | Loss: 0.00137403
Iteration 4/25 | Loss: 0.00135030
Iteration 5/25 | Loss: 0.00134393
Iteration 6/25 | Loss: 0.00134185
Iteration 7/25 | Loss: 0.00134167
Iteration 8/25 | Loss: 0.00134167
Iteration 9/25 | Loss: 0.00134167
Iteration 10/25 | Loss: 0.00134167
Iteration 11/25 | Loss: 0.00134167
Iteration 12/25 | Loss: 0.00134167
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0013416678411886096, 0.0013416678411886096, 0.0013416678411886096, 0.0013416678411886096, 0.0013416678411886096]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013416678411886096

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.00288653
Iteration 2/25 | Loss: 0.00108168
Iteration 3/25 | Loss: 0.00108167
Iteration 4/25 | Loss: 0.00108167
Iteration 5/25 | Loss: 0.00108167
Iteration 6/25 | Loss: 0.00108167
Iteration 7/25 | Loss: 0.00108167
Iteration 8/25 | Loss: 0.00108167
Iteration 9/25 | Loss: 0.00108167
Iteration 10/25 | Loss: 0.00108167
Iteration 11/25 | Loss: 0.00108167
Iteration 12/25 | Loss: 0.00108167
Iteration 13/25 | Loss: 0.00108167
Iteration 14/25 | Loss: 0.00108167
Iteration 15/25 | Loss: 0.00108167
Iteration 16/25 | Loss: 0.00108167
Iteration 17/25 | Loss: 0.00108167
Iteration 18/25 | Loss: 0.00108167
Iteration 19/25 | Loss: 0.00108167
Iteration 20/25 | Loss: 0.00108167
Iteration 21/25 | Loss: 0.00108167
Iteration 22/25 | Loss: 0.00108167
Iteration 23/25 | Loss: 0.00108167
Iteration 24/25 | Loss: 0.00108167
Iteration 25/25 | Loss: 0.00108167

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00108167
Iteration 2/1000 | Loss: 0.00006352
Iteration 3/1000 | Loss: 0.00004422
Iteration 4/1000 | Loss: 0.00003899
Iteration 5/1000 | Loss: 0.00003688
Iteration 6/1000 | Loss: 0.00003545
Iteration 7/1000 | Loss: 0.00003431
Iteration 8/1000 | Loss: 0.00003332
Iteration 9/1000 | Loss: 0.00003268
Iteration 10/1000 | Loss: 0.00003227
Iteration 11/1000 | Loss: 0.00003183
Iteration 12/1000 | Loss: 0.00003151
Iteration 13/1000 | Loss: 0.00003119
Iteration 14/1000 | Loss: 0.00003092
Iteration 15/1000 | Loss: 0.00003069
Iteration 16/1000 | Loss: 0.00003054
Iteration 17/1000 | Loss: 0.00003047
Iteration 18/1000 | Loss: 0.00003046
Iteration 19/1000 | Loss: 0.00003045
Iteration 20/1000 | Loss: 0.00003031
Iteration 21/1000 | Loss: 0.00003025
Iteration 22/1000 | Loss: 0.00003020
Iteration 23/1000 | Loss: 0.00003015
Iteration 24/1000 | Loss: 0.00003010
Iteration 25/1000 | Loss: 0.00003006
Iteration 26/1000 | Loss: 0.00003003
Iteration 27/1000 | Loss: 0.00003001
Iteration 28/1000 | Loss: 0.00003001
Iteration 29/1000 | Loss: 0.00003000
Iteration 30/1000 | Loss: 0.00002996
Iteration 31/1000 | Loss: 0.00002995
Iteration 32/1000 | Loss: 0.00002994
Iteration 33/1000 | Loss: 0.00002994
Iteration 34/1000 | Loss: 0.00002992
Iteration 35/1000 | Loss: 0.00002992
Iteration 36/1000 | Loss: 0.00002991
Iteration 37/1000 | Loss: 0.00002991
Iteration 38/1000 | Loss: 0.00002991
Iteration 39/1000 | Loss: 0.00002991
Iteration 40/1000 | Loss: 0.00002990
Iteration 41/1000 | Loss: 0.00002990
Iteration 42/1000 | Loss: 0.00002990
Iteration 43/1000 | Loss: 0.00002989
Iteration 44/1000 | Loss: 0.00002989
Iteration 45/1000 | Loss: 0.00002989
Iteration 46/1000 | Loss: 0.00002989
Iteration 47/1000 | Loss: 0.00002989
Iteration 48/1000 | Loss: 0.00002989
Iteration 49/1000 | Loss: 0.00002989
Iteration 50/1000 | Loss: 0.00002989
Iteration 51/1000 | Loss: 0.00002989
Iteration 52/1000 | Loss: 0.00002989
Iteration 53/1000 | Loss: 0.00002988
Iteration 54/1000 | Loss: 0.00002987
Iteration 55/1000 | Loss: 0.00002987
Iteration 56/1000 | Loss: 0.00002986
Iteration 57/1000 | Loss: 0.00002986
Iteration 58/1000 | Loss: 0.00002986
Iteration 59/1000 | Loss: 0.00002986
Iteration 60/1000 | Loss: 0.00002986
Iteration 61/1000 | Loss: 0.00002986
Iteration 62/1000 | Loss: 0.00002985
Iteration 63/1000 | Loss: 0.00002985
Iteration 64/1000 | Loss: 0.00002985
Iteration 65/1000 | Loss: 0.00002985
Iteration 66/1000 | Loss: 0.00002985
Iteration 67/1000 | Loss: 0.00002985
Iteration 68/1000 | Loss: 0.00002985
Iteration 69/1000 | Loss: 0.00002985
Iteration 70/1000 | Loss: 0.00002985
Iteration 71/1000 | Loss: 0.00002985
Iteration 72/1000 | Loss: 0.00002984
Iteration 73/1000 | Loss: 0.00002984
Iteration 74/1000 | Loss: 0.00002982
Iteration 75/1000 | Loss: 0.00002982
Iteration 76/1000 | Loss: 0.00002982
Iteration 77/1000 | Loss: 0.00002982
Iteration 78/1000 | Loss: 0.00002982
Iteration 79/1000 | Loss: 0.00002982
Iteration 80/1000 | Loss: 0.00002982
Iteration 81/1000 | Loss: 0.00002982
Iteration 82/1000 | Loss: 0.00002982
Iteration 83/1000 | Loss: 0.00002981
Iteration 84/1000 | Loss: 0.00002981
Iteration 85/1000 | Loss: 0.00002981
Iteration 86/1000 | Loss: 0.00002981
Iteration 87/1000 | Loss: 0.00002981
Iteration 88/1000 | Loss: 0.00002980
Iteration 89/1000 | Loss: 0.00002980
Iteration 90/1000 | Loss: 0.00002980
Iteration 91/1000 | Loss: 0.00002980
Iteration 92/1000 | Loss: 0.00002980
Iteration 93/1000 | Loss: 0.00002980
Iteration 94/1000 | Loss: 0.00002979
Iteration 95/1000 | Loss: 0.00002979
Iteration 96/1000 | Loss: 0.00002979
Iteration 97/1000 | Loss: 0.00002979
Iteration 98/1000 | Loss: 0.00002979
Iteration 99/1000 | Loss: 0.00002979
Iteration 100/1000 | Loss: 0.00002978
Iteration 101/1000 | Loss: 0.00002978
Iteration 102/1000 | Loss: 0.00002978
Iteration 103/1000 | Loss: 0.00002978
Iteration 104/1000 | Loss: 0.00002978
Iteration 105/1000 | Loss: 0.00002978
Iteration 106/1000 | Loss: 0.00002978
Iteration 107/1000 | Loss: 0.00002978
Iteration 108/1000 | Loss: 0.00002978
Iteration 109/1000 | Loss: 0.00002977
Iteration 110/1000 | Loss: 0.00002977
Iteration 111/1000 | Loss: 0.00002977
Iteration 112/1000 | Loss: 0.00002977
Iteration 113/1000 | Loss: 0.00002977
Iteration 114/1000 | Loss: 0.00002977
Iteration 115/1000 | Loss: 0.00002977
Iteration 116/1000 | Loss: 0.00002977
Iteration 117/1000 | Loss: 0.00002977
Iteration 118/1000 | Loss: 0.00002976
Iteration 119/1000 | Loss: 0.00002976
Iteration 120/1000 | Loss: 0.00002976
Iteration 121/1000 | Loss: 0.00002976
Iteration 122/1000 | Loss: 0.00002976
Iteration 123/1000 | Loss: 0.00002976
Iteration 124/1000 | Loss: 0.00002976
Iteration 125/1000 | Loss: 0.00002976
Iteration 126/1000 | Loss: 0.00002975
Iteration 127/1000 | Loss: 0.00002975
Iteration 128/1000 | Loss: 0.00002975
Iteration 129/1000 | Loss: 0.00002975
Iteration 130/1000 | Loss: 0.00002975
Iteration 131/1000 | Loss: 0.00002975
Iteration 132/1000 | Loss: 0.00002975
Iteration 133/1000 | Loss: 0.00002975
Iteration 134/1000 | Loss: 0.00002975
Iteration 135/1000 | Loss: 0.00002975
Iteration 136/1000 | Loss: 0.00002975
Iteration 137/1000 | Loss: 0.00002974
Iteration 138/1000 | Loss: 0.00002974
Iteration 139/1000 | Loss: 0.00002974
Iteration 140/1000 | Loss: 0.00002974
Iteration 141/1000 | Loss: 0.00002974
Iteration 142/1000 | Loss: 0.00002974
Iteration 143/1000 | Loss: 0.00002974
Iteration 144/1000 | Loss: 0.00002973
Iteration 145/1000 | Loss: 0.00002973
Iteration 146/1000 | Loss: 0.00002973
Iteration 147/1000 | Loss: 0.00002973
Iteration 148/1000 | Loss: 0.00002973
Iteration 149/1000 | Loss: 0.00002973
Iteration 150/1000 | Loss: 0.00002973
Iteration 151/1000 | Loss: 0.00002973
Iteration 152/1000 | Loss: 0.00002973
Iteration 153/1000 | Loss: 0.00002973
Iteration 154/1000 | Loss: 0.00002973
Iteration 155/1000 | Loss: 0.00002972
Iteration 156/1000 | Loss: 0.00002972
Iteration 157/1000 | Loss: 0.00002972
Iteration 158/1000 | Loss: 0.00002972
Iteration 159/1000 | Loss: 0.00002972
Iteration 160/1000 | Loss: 0.00002972
Iteration 161/1000 | Loss: 0.00002972
Iteration 162/1000 | Loss: 0.00002972
Iteration 163/1000 | Loss: 0.00002972
Iteration 164/1000 | Loss: 0.00002972
Iteration 165/1000 | Loss: 0.00002972
Iteration 166/1000 | Loss: 0.00002972
Iteration 167/1000 | Loss: 0.00002972
Iteration 168/1000 | Loss: 0.00002972
Iteration 169/1000 | Loss: 0.00002972
Iteration 170/1000 | Loss: 0.00002972
Iteration 171/1000 | Loss: 0.00002972
Iteration 172/1000 | Loss: 0.00002972
Iteration 173/1000 | Loss: 0.00002972
Iteration 174/1000 | Loss: 0.00002972
Iteration 175/1000 | Loss: 0.00002972
Iteration 176/1000 | Loss: 0.00002972
Iteration 177/1000 | Loss: 0.00002972
Iteration 178/1000 | Loss: 0.00002972
Iteration 179/1000 | Loss: 0.00002972
Iteration 180/1000 | Loss: 0.00002972
Iteration 181/1000 | Loss: 0.00002972
Iteration 182/1000 | Loss: 0.00002972
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 182. Stopping optimization.
Last 5 losses: [2.9722184990532696e-05, 2.9722184990532696e-05, 2.9722184990532696e-05, 2.9722184990532696e-05, 2.9722184990532696e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.9722184990532696e-05

Optimization complete. Final v2v error: 4.381572723388672 mm

Highest mean error: 5.604515075683594 mm for frame 151

Lowest mean error: 3.3570141792297363 mm for frame 15

Saving results

Total time: 49.519277572631836
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_018/1092/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_018/1092.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_018/1092
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00544323
Iteration 2/25 | Loss: 0.00118286
Iteration 3/25 | Loss: 0.00111797
Iteration 4/25 | Loss: 0.00110974
Iteration 5/25 | Loss: 0.00110719
Iteration 6/25 | Loss: 0.00110668
Iteration 7/25 | Loss: 0.00110668
Iteration 8/25 | Loss: 0.00110668
Iteration 9/25 | Loss: 0.00110668
Iteration 10/25 | Loss: 0.00110668
Iteration 11/25 | Loss: 0.00110668
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001106677227653563, 0.001106677227653563, 0.001106677227653563, 0.001106677227653563, 0.001106677227653563]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001106677227653563

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.19523072
Iteration 2/25 | Loss: 0.00082327
Iteration 3/25 | Loss: 0.00082327
Iteration 4/25 | Loss: 0.00082327
Iteration 5/25 | Loss: 0.00082327
Iteration 6/25 | Loss: 0.00082327
Iteration 7/25 | Loss: 0.00082327
Iteration 8/25 | Loss: 0.00082327
Iteration 9/25 | Loss: 0.00082327
Iteration 10/25 | Loss: 0.00082326
Iteration 11/25 | Loss: 0.00082326
Iteration 12/25 | Loss: 0.00082326
Iteration 13/25 | Loss: 0.00082326
Iteration 14/25 | Loss: 0.00082326
Iteration 15/25 | Loss: 0.00082326
Iteration 16/25 | Loss: 0.00082326
Iteration 17/25 | Loss: 0.00082326
Iteration 18/25 | Loss: 0.00082326
Iteration 19/25 | Loss: 0.00082326
Iteration 20/25 | Loss: 0.00082326
Iteration 21/25 | Loss: 0.00082326
Iteration 22/25 | Loss: 0.00082326
Iteration 23/25 | Loss: 0.00082326
Iteration 24/25 | Loss: 0.00082326
Iteration 25/25 | Loss: 0.00082326

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00082326
Iteration 2/1000 | Loss: 0.00001953
Iteration 3/1000 | Loss: 0.00001330
Iteration 4/1000 | Loss: 0.00001185
Iteration 5/1000 | Loss: 0.00001132
Iteration 6/1000 | Loss: 0.00001081
Iteration 7/1000 | Loss: 0.00001045
Iteration 8/1000 | Loss: 0.00001026
Iteration 9/1000 | Loss: 0.00001003
Iteration 10/1000 | Loss: 0.00000987
Iteration 11/1000 | Loss: 0.00000982
Iteration 12/1000 | Loss: 0.00000981
Iteration 13/1000 | Loss: 0.00000980
Iteration 14/1000 | Loss: 0.00000977
Iteration 15/1000 | Loss: 0.00000966
Iteration 16/1000 | Loss: 0.00000964
Iteration 17/1000 | Loss: 0.00000962
Iteration 18/1000 | Loss: 0.00000962
Iteration 19/1000 | Loss: 0.00000959
Iteration 20/1000 | Loss: 0.00000958
Iteration 21/1000 | Loss: 0.00000956
Iteration 22/1000 | Loss: 0.00000955
Iteration 23/1000 | Loss: 0.00000955
Iteration 24/1000 | Loss: 0.00000954
Iteration 25/1000 | Loss: 0.00000951
Iteration 26/1000 | Loss: 0.00000950
Iteration 27/1000 | Loss: 0.00000949
Iteration 28/1000 | Loss: 0.00000949
Iteration 29/1000 | Loss: 0.00000949
Iteration 30/1000 | Loss: 0.00000949
Iteration 31/1000 | Loss: 0.00000949
Iteration 32/1000 | Loss: 0.00000947
Iteration 33/1000 | Loss: 0.00000946
Iteration 34/1000 | Loss: 0.00000946
Iteration 35/1000 | Loss: 0.00000945
Iteration 36/1000 | Loss: 0.00000945
Iteration 37/1000 | Loss: 0.00000945
Iteration 38/1000 | Loss: 0.00000944
Iteration 39/1000 | Loss: 0.00000944
Iteration 40/1000 | Loss: 0.00000943
Iteration 41/1000 | Loss: 0.00000943
Iteration 42/1000 | Loss: 0.00000943
Iteration 43/1000 | Loss: 0.00000943
Iteration 44/1000 | Loss: 0.00000942
Iteration 45/1000 | Loss: 0.00000942
Iteration 46/1000 | Loss: 0.00000942
Iteration 47/1000 | Loss: 0.00000942
Iteration 48/1000 | Loss: 0.00000941
Iteration 49/1000 | Loss: 0.00000940
Iteration 50/1000 | Loss: 0.00000940
Iteration 51/1000 | Loss: 0.00000939
Iteration 52/1000 | Loss: 0.00000938
Iteration 53/1000 | Loss: 0.00000937
Iteration 54/1000 | Loss: 0.00000937
Iteration 55/1000 | Loss: 0.00000937
Iteration 56/1000 | Loss: 0.00000937
Iteration 57/1000 | Loss: 0.00000936
Iteration 58/1000 | Loss: 0.00000936
Iteration 59/1000 | Loss: 0.00000936
Iteration 60/1000 | Loss: 0.00000935
Iteration 61/1000 | Loss: 0.00000935
Iteration 62/1000 | Loss: 0.00000935
Iteration 63/1000 | Loss: 0.00000934
Iteration 64/1000 | Loss: 0.00000934
Iteration 65/1000 | Loss: 0.00000934
Iteration 66/1000 | Loss: 0.00000934
Iteration 67/1000 | Loss: 0.00000934
Iteration 68/1000 | Loss: 0.00000934
Iteration 69/1000 | Loss: 0.00000933
Iteration 70/1000 | Loss: 0.00000932
Iteration 71/1000 | Loss: 0.00000932
Iteration 72/1000 | Loss: 0.00000932
Iteration 73/1000 | Loss: 0.00000931
Iteration 74/1000 | Loss: 0.00000931
Iteration 75/1000 | Loss: 0.00000931
Iteration 76/1000 | Loss: 0.00000931
Iteration 77/1000 | Loss: 0.00000931
Iteration 78/1000 | Loss: 0.00000931
Iteration 79/1000 | Loss: 0.00000931
Iteration 80/1000 | Loss: 0.00000931
Iteration 81/1000 | Loss: 0.00000931
Iteration 82/1000 | Loss: 0.00000931
Iteration 83/1000 | Loss: 0.00000931
Iteration 84/1000 | Loss: 0.00000931
Iteration 85/1000 | Loss: 0.00000931
Iteration 86/1000 | Loss: 0.00000931
Iteration 87/1000 | Loss: 0.00000931
Iteration 88/1000 | Loss: 0.00000931
Iteration 89/1000 | Loss: 0.00000931
Iteration 90/1000 | Loss: 0.00000931
Iteration 91/1000 | Loss: 0.00000931
Iteration 92/1000 | Loss: 0.00000931
Iteration 93/1000 | Loss: 0.00000931
Iteration 94/1000 | Loss: 0.00000931
Iteration 95/1000 | Loss: 0.00000931
Iteration 96/1000 | Loss: 0.00000931
Iteration 97/1000 | Loss: 0.00000931
Iteration 98/1000 | Loss: 0.00000931
Iteration 99/1000 | Loss: 0.00000931
Iteration 100/1000 | Loss: 0.00000931
Iteration 101/1000 | Loss: 0.00000931
Iteration 102/1000 | Loss: 0.00000931
Iteration 103/1000 | Loss: 0.00000931
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 103. Stopping optimization.
Last 5 losses: [9.30551323108375e-06, 9.30551323108375e-06, 9.30551323108375e-06, 9.30551323108375e-06, 9.30551323108375e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.30551323108375e-06

Optimization complete. Final v2v error: 2.6303720474243164 mm

Highest mean error: 2.8214685916900635 mm for frame 75

Lowest mean error: 2.522149085998535 mm for frame 137

Saving results

Total time: 33.267632722854614
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_018/1091/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_018/1091.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_018/1091
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00482085
Iteration 2/25 | Loss: 0.00129297
Iteration 3/25 | Loss: 0.00121581
Iteration 4/25 | Loss: 0.00121152
Iteration 5/25 | Loss: 0.00121042
Iteration 6/25 | Loss: 0.00121042
Iteration 7/25 | Loss: 0.00121042
Iteration 8/25 | Loss: 0.00121042
Iteration 9/25 | Loss: 0.00121042
Iteration 10/25 | Loss: 0.00121042
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.001210418762639165, 0.001210418762639165, 0.001210418762639165, 0.001210418762639165, 0.001210418762639165]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001210418762639165

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36622572
Iteration 2/25 | Loss: 0.00092842
Iteration 3/25 | Loss: 0.00092841
Iteration 4/25 | Loss: 0.00092841
Iteration 5/25 | Loss: 0.00092841
Iteration 6/25 | Loss: 0.00092841
Iteration 7/25 | Loss: 0.00092841
Iteration 8/25 | Loss: 0.00092841
Iteration 9/25 | Loss: 0.00092841
Iteration 10/25 | Loss: 0.00092841
Iteration 11/25 | Loss: 0.00092841
Iteration 12/25 | Loss: 0.00092841
Iteration 13/25 | Loss: 0.00092841
Iteration 14/25 | Loss: 0.00092841
Iteration 15/25 | Loss: 0.00092841
Iteration 16/25 | Loss: 0.00092841
Iteration 17/25 | Loss: 0.00092841
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0009284118423238397, 0.0009284118423238397, 0.0009284118423238397, 0.0009284118423238397, 0.0009284118423238397]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009284118423238397

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00092841
Iteration 2/1000 | Loss: 0.00003074
Iteration 3/1000 | Loss: 0.00002190
Iteration 4/1000 | Loss: 0.00001967
Iteration 5/1000 | Loss: 0.00001840
Iteration 6/1000 | Loss: 0.00001743
Iteration 7/1000 | Loss: 0.00001688
Iteration 8/1000 | Loss: 0.00001660
Iteration 9/1000 | Loss: 0.00001641
Iteration 10/1000 | Loss: 0.00001620
Iteration 11/1000 | Loss: 0.00001607
Iteration 12/1000 | Loss: 0.00001606
Iteration 13/1000 | Loss: 0.00001605
Iteration 14/1000 | Loss: 0.00001600
Iteration 15/1000 | Loss: 0.00001598
Iteration 16/1000 | Loss: 0.00001593
Iteration 17/1000 | Loss: 0.00001592
Iteration 18/1000 | Loss: 0.00001586
Iteration 19/1000 | Loss: 0.00001584
Iteration 20/1000 | Loss: 0.00001580
Iteration 21/1000 | Loss: 0.00001579
Iteration 22/1000 | Loss: 0.00001579
Iteration 23/1000 | Loss: 0.00001578
Iteration 24/1000 | Loss: 0.00001578
Iteration 25/1000 | Loss: 0.00001577
Iteration 26/1000 | Loss: 0.00001577
Iteration 27/1000 | Loss: 0.00001576
Iteration 28/1000 | Loss: 0.00001575
Iteration 29/1000 | Loss: 0.00001575
Iteration 30/1000 | Loss: 0.00001575
Iteration 31/1000 | Loss: 0.00001574
Iteration 32/1000 | Loss: 0.00001573
Iteration 33/1000 | Loss: 0.00001573
Iteration 34/1000 | Loss: 0.00001572
Iteration 35/1000 | Loss: 0.00001572
Iteration 36/1000 | Loss: 0.00001571
Iteration 37/1000 | Loss: 0.00001571
Iteration 38/1000 | Loss: 0.00001569
Iteration 39/1000 | Loss: 0.00001568
Iteration 40/1000 | Loss: 0.00001568
Iteration 41/1000 | Loss: 0.00001568
Iteration 42/1000 | Loss: 0.00001568
Iteration 43/1000 | Loss: 0.00001567
Iteration 44/1000 | Loss: 0.00001567
Iteration 45/1000 | Loss: 0.00001563
Iteration 46/1000 | Loss: 0.00001560
Iteration 47/1000 | Loss: 0.00001560
Iteration 48/1000 | Loss: 0.00001558
Iteration 49/1000 | Loss: 0.00001553
Iteration 50/1000 | Loss: 0.00001552
Iteration 51/1000 | Loss: 0.00001548
Iteration 52/1000 | Loss: 0.00001548
Iteration 53/1000 | Loss: 0.00001547
Iteration 54/1000 | Loss: 0.00001546
Iteration 55/1000 | Loss: 0.00001546
Iteration 56/1000 | Loss: 0.00001546
Iteration 57/1000 | Loss: 0.00001546
Iteration 58/1000 | Loss: 0.00001546
Iteration 59/1000 | Loss: 0.00001545
Iteration 60/1000 | Loss: 0.00001545
Iteration 61/1000 | Loss: 0.00001545
Iteration 62/1000 | Loss: 0.00001544
Iteration 63/1000 | Loss: 0.00001544
Iteration 64/1000 | Loss: 0.00001541
Iteration 65/1000 | Loss: 0.00001541
Iteration 66/1000 | Loss: 0.00001540
Iteration 67/1000 | Loss: 0.00001539
Iteration 68/1000 | Loss: 0.00001539
Iteration 69/1000 | Loss: 0.00001538
Iteration 70/1000 | Loss: 0.00001538
Iteration 71/1000 | Loss: 0.00001538
Iteration 72/1000 | Loss: 0.00001537
Iteration 73/1000 | Loss: 0.00001537
Iteration 74/1000 | Loss: 0.00001536
Iteration 75/1000 | Loss: 0.00001535
Iteration 76/1000 | Loss: 0.00001534
Iteration 77/1000 | Loss: 0.00001534
Iteration 78/1000 | Loss: 0.00001532
Iteration 79/1000 | Loss: 0.00001532
Iteration 80/1000 | Loss: 0.00001531
Iteration 81/1000 | Loss: 0.00001531
Iteration 82/1000 | Loss: 0.00001531
Iteration 83/1000 | Loss: 0.00001530
Iteration 84/1000 | Loss: 0.00001530
Iteration 85/1000 | Loss: 0.00001529
Iteration 86/1000 | Loss: 0.00001529
Iteration 87/1000 | Loss: 0.00001528
Iteration 88/1000 | Loss: 0.00001528
Iteration 89/1000 | Loss: 0.00001528
Iteration 90/1000 | Loss: 0.00001528
Iteration 91/1000 | Loss: 0.00001527
Iteration 92/1000 | Loss: 0.00001526
Iteration 93/1000 | Loss: 0.00001526
Iteration 94/1000 | Loss: 0.00001526
Iteration 95/1000 | Loss: 0.00001526
Iteration 96/1000 | Loss: 0.00001526
Iteration 97/1000 | Loss: 0.00001526
Iteration 98/1000 | Loss: 0.00001526
Iteration 99/1000 | Loss: 0.00001525
Iteration 100/1000 | Loss: 0.00001525
Iteration 101/1000 | Loss: 0.00001524
Iteration 102/1000 | Loss: 0.00001524
Iteration 103/1000 | Loss: 0.00001524
Iteration 104/1000 | Loss: 0.00001524
Iteration 105/1000 | Loss: 0.00001524
Iteration 106/1000 | Loss: 0.00001523
Iteration 107/1000 | Loss: 0.00001523
Iteration 108/1000 | Loss: 0.00001523
Iteration 109/1000 | Loss: 0.00001522
Iteration 110/1000 | Loss: 0.00001520
Iteration 111/1000 | Loss: 0.00001520
Iteration 112/1000 | Loss: 0.00001520
Iteration 113/1000 | Loss: 0.00001519
Iteration 114/1000 | Loss: 0.00001519
Iteration 115/1000 | Loss: 0.00001519
Iteration 116/1000 | Loss: 0.00001518
Iteration 117/1000 | Loss: 0.00001517
Iteration 118/1000 | Loss: 0.00001517
Iteration 119/1000 | Loss: 0.00001516
Iteration 120/1000 | Loss: 0.00001516
Iteration 121/1000 | Loss: 0.00001516
Iteration 122/1000 | Loss: 0.00001516
Iteration 123/1000 | Loss: 0.00001516
Iteration 124/1000 | Loss: 0.00001516
Iteration 125/1000 | Loss: 0.00001515
Iteration 126/1000 | Loss: 0.00001515
Iteration 127/1000 | Loss: 0.00001515
Iteration 128/1000 | Loss: 0.00001514
Iteration 129/1000 | Loss: 0.00001513
Iteration 130/1000 | Loss: 0.00001513
Iteration 131/1000 | Loss: 0.00001512
Iteration 132/1000 | Loss: 0.00001512
Iteration 133/1000 | Loss: 0.00001512
Iteration 134/1000 | Loss: 0.00001512
Iteration 135/1000 | Loss: 0.00001512
Iteration 136/1000 | Loss: 0.00001512
Iteration 137/1000 | Loss: 0.00001512
Iteration 138/1000 | Loss: 0.00001511
Iteration 139/1000 | Loss: 0.00001511
Iteration 140/1000 | Loss: 0.00001511
Iteration 141/1000 | Loss: 0.00001510
Iteration 142/1000 | Loss: 0.00001510
Iteration 143/1000 | Loss: 0.00001510
Iteration 144/1000 | Loss: 0.00001510
Iteration 145/1000 | Loss: 0.00001510
Iteration 146/1000 | Loss: 0.00001509
Iteration 147/1000 | Loss: 0.00001509
Iteration 148/1000 | Loss: 0.00001509
Iteration 149/1000 | Loss: 0.00001509
Iteration 150/1000 | Loss: 0.00001509
Iteration 151/1000 | Loss: 0.00001509
Iteration 152/1000 | Loss: 0.00001508
Iteration 153/1000 | Loss: 0.00001508
Iteration 154/1000 | Loss: 0.00001508
Iteration 155/1000 | Loss: 0.00001508
Iteration 156/1000 | Loss: 0.00001508
Iteration 157/1000 | Loss: 0.00001508
Iteration 158/1000 | Loss: 0.00001508
Iteration 159/1000 | Loss: 0.00001508
Iteration 160/1000 | Loss: 0.00001508
Iteration 161/1000 | Loss: 0.00001508
Iteration 162/1000 | Loss: 0.00001508
Iteration 163/1000 | Loss: 0.00001508
Iteration 164/1000 | Loss: 0.00001507
Iteration 165/1000 | Loss: 0.00001507
Iteration 166/1000 | Loss: 0.00001507
Iteration 167/1000 | Loss: 0.00001507
Iteration 168/1000 | Loss: 0.00001507
Iteration 169/1000 | Loss: 0.00001507
Iteration 170/1000 | Loss: 0.00001507
Iteration 171/1000 | Loss: 0.00001507
Iteration 172/1000 | Loss: 0.00001507
Iteration 173/1000 | Loss: 0.00001507
Iteration 174/1000 | Loss: 0.00001506
Iteration 175/1000 | Loss: 0.00001506
Iteration 176/1000 | Loss: 0.00001506
Iteration 177/1000 | Loss: 0.00001506
Iteration 178/1000 | Loss: 0.00001506
Iteration 179/1000 | Loss: 0.00001505
Iteration 180/1000 | Loss: 0.00001505
Iteration 181/1000 | Loss: 0.00001505
Iteration 182/1000 | Loss: 0.00001504
Iteration 183/1000 | Loss: 0.00001504
Iteration 184/1000 | Loss: 0.00001504
Iteration 185/1000 | Loss: 0.00001504
Iteration 186/1000 | Loss: 0.00001504
Iteration 187/1000 | Loss: 0.00001504
Iteration 188/1000 | Loss: 0.00001503
Iteration 189/1000 | Loss: 0.00001503
Iteration 190/1000 | Loss: 0.00001503
Iteration 191/1000 | Loss: 0.00001503
Iteration 192/1000 | Loss: 0.00001503
Iteration 193/1000 | Loss: 0.00001503
Iteration 194/1000 | Loss: 0.00001503
Iteration 195/1000 | Loss: 0.00001502
Iteration 196/1000 | Loss: 0.00001502
Iteration 197/1000 | Loss: 0.00001502
Iteration 198/1000 | Loss: 0.00001502
Iteration 199/1000 | Loss: 0.00001501
Iteration 200/1000 | Loss: 0.00001501
Iteration 201/1000 | Loss: 0.00001501
Iteration 202/1000 | Loss: 0.00001500
Iteration 203/1000 | Loss: 0.00001500
Iteration 204/1000 | Loss: 0.00001500
Iteration 205/1000 | Loss: 0.00001500
Iteration 206/1000 | Loss: 0.00001500
Iteration 207/1000 | Loss: 0.00001500
Iteration 208/1000 | Loss: 0.00001499
Iteration 209/1000 | Loss: 0.00001499
Iteration 210/1000 | Loss: 0.00001499
Iteration 211/1000 | Loss: 0.00001499
Iteration 212/1000 | Loss: 0.00001499
Iteration 213/1000 | Loss: 0.00001498
Iteration 214/1000 | Loss: 0.00001498
Iteration 215/1000 | Loss: 0.00001498
Iteration 216/1000 | Loss: 0.00001498
Iteration 217/1000 | Loss: 0.00001498
Iteration 218/1000 | Loss: 0.00001498
Iteration 219/1000 | Loss: 0.00001498
Iteration 220/1000 | Loss: 0.00001498
Iteration 221/1000 | Loss: 0.00001498
Iteration 222/1000 | Loss: 0.00001497
Iteration 223/1000 | Loss: 0.00001497
Iteration 224/1000 | Loss: 0.00001497
Iteration 225/1000 | Loss: 0.00001497
Iteration 226/1000 | Loss: 0.00001497
Iteration 227/1000 | Loss: 0.00001497
Iteration 228/1000 | Loss: 0.00001497
Iteration 229/1000 | Loss: 0.00001497
Iteration 230/1000 | Loss: 0.00001497
Iteration 231/1000 | Loss: 0.00001497
Iteration 232/1000 | Loss: 0.00001497
Iteration 233/1000 | Loss: 0.00001497
Iteration 234/1000 | Loss: 0.00001497
Iteration 235/1000 | Loss: 0.00001497
Iteration 236/1000 | Loss: 0.00001497
Iteration 237/1000 | Loss: 0.00001497
Iteration 238/1000 | Loss: 0.00001497
Iteration 239/1000 | Loss: 0.00001497
Iteration 240/1000 | Loss: 0.00001497
Iteration 241/1000 | Loss: 0.00001497
Iteration 242/1000 | Loss: 0.00001497
Iteration 243/1000 | Loss: 0.00001497
Iteration 244/1000 | Loss: 0.00001497
Iteration 245/1000 | Loss: 0.00001497
Iteration 246/1000 | Loss: 0.00001497
Iteration 247/1000 | Loss: 0.00001497
Iteration 248/1000 | Loss: 0.00001497
Iteration 249/1000 | Loss: 0.00001497
Iteration 250/1000 | Loss: 0.00001497
Iteration 251/1000 | Loss: 0.00001497
Iteration 252/1000 | Loss: 0.00001497
Iteration 253/1000 | Loss: 0.00001497
Iteration 254/1000 | Loss: 0.00001497
Iteration 255/1000 | Loss: 0.00001497
Iteration 256/1000 | Loss: 0.00001497
Iteration 257/1000 | Loss: 0.00001497
Iteration 258/1000 | Loss: 0.00001497
Iteration 259/1000 | Loss: 0.00001497
Iteration 260/1000 | Loss: 0.00001497
Iteration 261/1000 | Loss: 0.00001497
Iteration 262/1000 | Loss: 0.00001497
Iteration 263/1000 | Loss: 0.00001497
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 263. Stopping optimization.
Last 5 losses: [1.4971765267546289e-05, 1.4971765267546289e-05, 1.4971765267546289e-05, 1.4971765267546289e-05, 1.4971765267546289e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4971765267546289e-05

Optimization complete. Final v2v error: 3.22216796875 mm

Highest mean error: 3.6012685298919678 mm for frame 62

Lowest mean error: 3.074551582336426 mm for frame 31

Saving results

Total time: 46.8513388633728
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_018/1040/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_018/1040.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_018/1040
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00827002
Iteration 2/25 | Loss: 0.00118786
Iteration 3/25 | Loss: 0.00112889
Iteration 4/25 | Loss: 0.00111888
Iteration 5/25 | Loss: 0.00111606
Iteration 6/25 | Loss: 0.00111557
Iteration 7/25 | Loss: 0.00111557
Iteration 8/25 | Loss: 0.00111557
Iteration 9/25 | Loss: 0.00111557
Iteration 10/25 | Loss: 0.00111557
Iteration 11/25 | Loss: 0.00111557
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011155694955959916, 0.0011155694955959916, 0.0011155694955959916, 0.0011155694955959916, 0.0011155694955959916]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011155694955959916

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 6.47515869
Iteration 2/25 | Loss: 0.00077970
Iteration 3/25 | Loss: 0.00077969
Iteration 4/25 | Loss: 0.00077969
Iteration 5/25 | Loss: 0.00077969
Iteration 6/25 | Loss: 0.00077969
Iteration 7/25 | Loss: 0.00077969
Iteration 8/25 | Loss: 0.00077969
Iteration 9/25 | Loss: 0.00077969
Iteration 10/25 | Loss: 0.00077969
Iteration 11/25 | Loss: 0.00077969
Iteration 12/25 | Loss: 0.00077969
Iteration 13/25 | Loss: 0.00077969
Iteration 14/25 | Loss: 0.00077969
Iteration 15/25 | Loss: 0.00077969
Iteration 16/25 | Loss: 0.00077969
Iteration 17/25 | Loss: 0.00077969
Iteration 18/25 | Loss: 0.00077969
Iteration 19/25 | Loss: 0.00077969
Iteration 20/25 | Loss: 0.00077969
Iteration 21/25 | Loss: 0.00077969
Iteration 22/25 | Loss: 0.00077969
Iteration 23/25 | Loss: 0.00077969
Iteration 24/25 | Loss: 0.00077969
Iteration 25/25 | Loss: 0.00077969

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00077969
Iteration 2/1000 | Loss: 0.00002681
Iteration 3/1000 | Loss: 0.00001894
Iteration 4/1000 | Loss: 0.00001569
Iteration 5/1000 | Loss: 0.00001484
Iteration 6/1000 | Loss: 0.00001425
Iteration 7/1000 | Loss: 0.00001388
Iteration 8/1000 | Loss: 0.00001338
Iteration 9/1000 | Loss: 0.00001308
Iteration 10/1000 | Loss: 0.00001288
Iteration 11/1000 | Loss: 0.00001269
Iteration 12/1000 | Loss: 0.00001269
Iteration 13/1000 | Loss: 0.00001266
Iteration 14/1000 | Loss: 0.00001255
Iteration 15/1000 | Loss: 0.00001249
Iteration 16/1000 | Loss: 0.00001246
Iteration 17/1000 | Loss: 0.00001245
Iteration 18/1000 | Loss: 0.00001244
Iteration 19/1000 | Loss: 0.00001244
Iteration 20/1000 | Loss: 0.00001243
Iteration 21/1000 | Loss: 0.00001243
Iteration 22/1000 | Loss: 0.00001242
Iteration 23/1000 | Loss: 0.00001239
Iteration 24/1000 | Loss: 0.00001239
Iteration 25/1000 | Loss: 0.00001238
Iteration 26/1000 | Loss: 0.00001237
Iteration 27/1000 | Loss: 0.00001237
Iteration 28/1000 | Loss: 0.00001236
Iteration 29/1000 | Loss: 0.00001236
Iteration 30/1000 | Loss: 0.00001232
Iteration 31/1000 | Loss: 0.00001232
Iteration 32/1000 | Loss: 0.00001231
Iteration 33/1000 | Loss: 0.00001231
Iteration 34/1000 | Loss: 0.00001231
Iteration 35/1000 | Loss: 0.00001231
Iteration 36/1000 | Loss: 0.00001230
Iteration 37/1000 | Loss: 0.00001230
Iteration 38/1000 | Loss: 0.00001230
Iteration 39/1000 | Loss: 0.00001230
Iteration 40/1000 | Loss: 0.00001229
Iteration 41/1000 | Loss: 0.00001229
Iteration 42/1000 | Loss: 0.00001228
Iteration 43/1000 | Loss: 0.00001227
Iteration 44/1000 | Loss: 0.00001227
Iteration 45/1000 | Loss: 0.00001227
Iteration 46/1000 | Loss: 0.00001227
Iteration 47/1000 | Loss: 0.00001227
Iteration 48/1000 | Loss: 0.00001227
Iteration 49/1000 | Loss: 0.00001227
Iteration 50/1000 | Loss: 0.00001227
Iteration 51/1000 | Loss: 0.00001227
Iteration 52/1000 | Loss: 0.00001227
Iteration 53/1000 | Loss: 0.00001226
Iteration 54/1000 | Loss: 0.00001226
Iteration 55/1000 | Loss: 0.00001226
Iteration 56/1000 | Loss: 0.00001226
Iteration 57/1000 | Loss: 0.00001226
Iteration 58/1000 | Loss: 0.00001225
Iteration 59/1000 | Loss: 0.00001225
Iteration 60/1000 | Loss: 0.00001224
Iteration 61/1000 | Loss: 0.00001224
Iteration 62/1000 | Loss: 0.00001224
Iteration 63/1000 | Loss: 0.00001223
Iteration 64/1000 | Loss: 0.00001223
Iteration 65/1000 | Loss: 0.00001223
Iteration 66/1000 | Loss: 0.00001222
Iteration 67/1000 | Loss: 0.00001222
Iteration 68/1000 | Loss: 0.00001222
Iteration 69/1000 | Loss: 0.00001222
Iteration 70/1000 | Loss: 0.00001222
Iteration 71/1000 | Loss: 0.00001221
Iteration 72/1000 | Loss: 0.00001221
Iteration 73/1000 | Loss: 0.00001220
Iteration 74/1000 | Loss: 0.00001220
Iteration 75/1000 | Loss: 0.00001220
Iteration 76/1000 | Loss: 0.00001220
Iteration 77/1000 | Loss: 0.00001219
Iteration 78/1000 | Loss: 0.00001219
Iteration 79/1000 | Loss: 0.00001219
Iteration 80/1000 | Loss: 0.00001219
Iteration 81/1000 | Loss: 0.00001219
Iteration 82/1000 | Loss: 0.00001219
Iteration 83/1000 | Loss: 0.00001218
Iteration 84/1000 | Loss: 0.00001218
Iteration 85/1000 | Loss: 0.00001218
Iteration 86/1000 | Loss: 0.00001218
Iteration 87/1000 | Loss: 0.00001218
Iteration 88/1000 | Loss: 0.00001217
Iteration 89/1000 | Loss: 0.00001217
Iteration 90/1000 | Loss: 0.00001217
Iteration 91/1000 | Loss: 0.00001216
Iteration 92/1000 | Loss: 0.00001216
Iteration 93/1000 | Loss: 0.00001215
Iteration 94/1000 | Loss: 0.00001215
Iteration 95/1000 | Loss: 0.00001215
Iteration 96/1000 | Loss: 0.00001214
Iteration 97/1000 | Loss: 0.00001212
Iteration 98/1000 | Loss: 0.00001211
Iteration 99/1000 | Loss: 0.00001211
Iteration 100/1000 | Loss: 0.00001211
Iteration 101/1000 | Loss: 0.00001210
Iteration 102/1000 | Loss: 0.00001210
Iteration 103/1000 | Loss: 0.00001209
Iteration 104/1000 | Loss: 0.00001209
Iteration 105/1000 | Loss: 0.00001208
Iteration 106/1000 | Loss: 0.00001207
Iteration 107/1000 | Loss: 0.00001207
Iteration 108/1000 | Loss: 0.00001207
Iteration 109/1000 | Loss: 0.00001207
Iteration 110/1000 | Loss: 0.00001207
Iteration 111/1000 | Loss: 0.00001206
Iteration 112/1000 | Loss: 0.00001206
Iteration 113/1000 | Loss: 0.00001206
Iteration 114/1000 | Loss: 0.00001206
Iteration 115/1000 | Loss: 0.00001205
Iteration 116/1000 | Loss: 0.00001205
Iteration 117/1000 | Loss: 0.00001205
Iteration 118/1000 | Loss: 0.00001205
Iteration 119/1000 | Loss: 0.00001205
Iteration 120/1000 | Loss: 0.00001204
Iteration 121/1000 | Loss: 0.00001204
Iteration 122/1000 | Loss: 0.00001204
Iteration 123/1000 | Loss: 0.00001204
Iteration 124/1000 | Loss: 0.00001204
Iteration 125/1000 | Loss: 0.00001204
Iteration 126/1000 | Loss: 0.00001204
Iteration 127/1000 | Loss: 0.00001204
Iteration 128/1000 | Loss: 0.00001203
Iteration 129/1000 | Loss: 0.00001203
Iteration 130/1000 | Loss: 0.00001203
Iteration 131/1000 | Loss: 0.00001203
Iteration 132/1000 | Loss: 0.00001202
Iteration 133/1000 | Loss: 0.00001202
Iteration 134/1000 | Loss: 0.00001202
Iteration 135/1000 | Loss: 0.00001202
Iteration 136/1000 | Loss: 0.00001201
Iteration 137/1000 | Loss: 0.00001201
Iteration 138/1000 | Loss: 0.00001201
Iteration 139/1000 | Loss: 0.00001201
Iteration 140/1000 | Loss: 0.00001201
Iteration 141/1000 | Loss: 0.00001200
Iteration 142/1000 | Loss: 0.00001200
Iteration 143/1000 | Loss: 0.00001200
Iteration 144/1000 | Loss: 0.00001200
Iteration 145/1000 | Loss: 0.00001200
Iteration 146/1000 | Loss: 0.00001200
Iteration 147/1000 | Loss: 0.00001199
Iteration 148/1000 | Loss: 0.00001199
Iteration 149/1000 | Loss: 0.00001199
Iteration 150/1000 | Loss: 0.00001199
Iteration 151/1000 | Loss: 0.00001199
Iteration 152/1000 | Loss: 0.00001199
Iteration 153/1000 | Loss: 0.00001198
Iteration 154/1000 | Loss: 0.00001198
Iteration 155/1000 | Loss: 0.00001198
Iteration 156/1000 | Loss: 0.00001198
Iteration 157/1000 | Loss: 0.00001198
Iteration 158/1000 | Loss: 0.00001197
Iteration 159/1000 | Loss: 0.00001197
Iteration 160/1000 | Loss: 0.00001197
Iteration 161/1000 | Loss: 0.00001197
Iteration 162/1000 | Loss: 0.00001197
Iteration 163/1000 | Loss: 0.00001197
Iteration 164/1000 | Loss: 0.00001197
Iteration 165/1000 | Loss: 0.00001196
Iteration 166/1000 | Loss: 0.00001196
Iteration 167/1000 | Loss: 0.00001196
Iteration 168/1000 | Loss: 0.00001196
Iteration 169/1000 | Loss: 0.00001196
Iteration 170/1000 | Loss: 0.00001196
Iteration 171/1000 | Loss: 0.00001196
Iteration 172/1000 | Loss: 0.00001196
Iteration 173/1000 | Loss: 0.00001196
Iteration 174/1000 | Loss: 0.00001196
Iteration 175/1000 | Loss: 0.00001195
Iteration 176/1000 | Loss: 0.00001195
Iteration 177/1000 | Loss: 0.00001195
Iteration 178/1000 | Loss: 0.00001195
Iteration 179/1000 | Loss: 0.00001195
Iteration 180/1000 | Loss: 0.00001195
Iteration 181/1000 | Loss: 0.00001194
Iteration 182/1000 | Loss: 0.00001194
Iteration 183/1000 | Loss: 0.00001194
Iteration 184/1000 | Loss: 0.00001194
Iteration 185/1000 | Loss: 0.00001194
Iteration 186/1000 | Loss: 0.00001194
Iteration 187/1000 | Loss: 0.00001194
Iteration 188/1000 | Loss: 0.00001194
Iteration 189/1000 | Loss: 0.00001194
Iteration 190/1000 | Loss: 0.00001194
Iteration 191/1000 | Loss: 0.00001193
Iteration 192/1000 | Loss: 0.00001193
Iteration 193/1000 | Loss: 0.00001193
Iteration 194/1000 | Loss: 0.00001193
Iteration 195/1000 | Loss: 0.00001193
Iteration 196/1000 | Loss: 0.00001193
Iteration 197/1000 | Loss: 0.00001193
Iteration 198/1000 | Loss: 0.00001193
Iteration 199/1000 | Loss: 0.00001193
Iteration 200/1000 | Loss: 0.00001193
Iteration 201/1000 | Loss: 0.00001193
Iteration 202/1000 | Loss: 0.00001193
Iteration 203/1000 | Loss: 0.00001193
Iteration 204/1000 | Loss: 0.00001193
Iteration 205/1000 | Loss: 0.00001193
Iteration 206/1000 | Loss: 0.00001193
Iteration 207/1000 | Loss: 0.00001193
Iteration 208/1000 | Loss: 0.00001193
Iteration 209/1000 | Loss: 0.00001193
Iteration 210/1000 | Loss: 0.00001193
Iteration 211/1000 | Loss: 0.00001193
Iteration 212/1000 | Loss: 0.00001193
Iteration 213/1000 | Loss: 0.00001193
Iteration 214/1000 | Loss: 0.00001193
Iteration 215/1000 | Loss: 0.00001193
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 215. Stopping optimization.
Last 5 losses: [1.1932179404539056e-05, 1.1932179404539056e-05, 1.1932179404539056e-05, 1.1932179404539056e-05, 1.1932179404539056e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1932179404539056e-05

Optimization complete. Final v2v error: 2.938369035720825 mm

Highest mean error: 3.5483458042144775 mm for frame 102

Lowest mean error: 2.639538288116455 mm for frame 13

Saving results

Total time: 40.563396692276
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_018/1060/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_018/1060.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_018/1060
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00786380
Iteration 2/25 | Loss: 0.00136031
Iteration 3/25 | Loss: 0.00119180
Iteration 4/25 | Loss: 0.00116935
Iteration 5/25 | Loss: 0.00116470
Iteration 6/25 | Loss: 0.00116353
Iteration 7/25 | Loss: 0.00116342
Iteration 8/25 | Loss: 0.00116342
Iteration 9/25 | Loss: 0.00116342
Iteration 10/25 | Loss: 0.00116342
Iteration 11/25 | Loss: 0.00116342
Iteration 12/25 | Loss: 0.00116342
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.001163416774943471, 0.001163416774943471, 0.001163416774943471, 0.001163416774943471, 0.001163416774943471]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001163416774943471

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35687363
Iteration 2/25 | Loss: 0.00080925
Iteration 3/25 | Loss: 0.00080925
Iteration 4/25 | Loss: 0.00080925
Iteration 5/25 | Loss: 0.00080925
Iteration 6/25 | Loss: 0.00080925
Iteration 7/25 | Loss: 0.00080925
Iteration 8/25 | Loss: 0.00080925
Iteration 9/25 | Loss: 0.00080925
Iteration 10/25 | Loss: 0.00080925
Iteration 11/25 | Loss: 0.00080925
Iteration 12/25 | Loss: 0.00080925
Iteration 13/25 | Loss: 0.00080925
Iteration 14/25 | Loss: 0.00080925
Iteration 15/25 | Loss: 0.00080925
Iteration 16/25 | Loss: 0.00080925
Iteration 17/25 | Loss: 0.00080925
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.000809251272585243, 0.000809251272585243, 0.000809251272585243, 0.000809251272585243, 0.000809251272585243]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000809251272585243

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00080925
Iteration 2/1000 | Loss: 0.00003605
Iteration 3/1000 | Loss: 0.00002402
Iteration 4/1000 | Loss: 0.00001992
Iteration 5/1000 | Loss: 0.00001896
Iteration 6/1000 | Loss: 0.00001822
Iteration 7/1000 | Loss: 0.00001735
Iteration 8/1000 | Loss: 0.00001681
Iteration 9/1000 | Loss: 0.00001617
Iteration 10/1000 | Loss: 0.00001587
Iteration 11/1000 | Loss: 0.00001556
Iteration 12/1000 | Loss: 0.00001536
Iteration 13/1000 | Loss: 0.00001521
Iteration 14/1000 | Loss: 0.00001515
Iteration 15/1000 | Loss: 0.00001514
Iteration 16/1000 | Loss: 0.00001513
Iteration 17/1000 | Loss: 0.00001512
Iteration 18/1000 | Loss: 0.00001511
Iteration 19/1000 | Loss: 0.00001509
Iteration 20/1000 | Loss: 0.00001508
Iteration 21/1000 | Loss: 0.00001507
Iteration 22/1000 | Loss: 0.00001506
Iteration 23/1000 | Loss: 0.00001503
Iteration 24/1000 | Loss: 0.00001502
Iteration 25/1000 | Loss: 0.00001501
Iteration 26/1000 | Loss: 0.00001500
Iteration 27/1000 | Loss: 0.00001500
Iteration 28/1000 | Loss: 0.00001499
Iteration 29/1000 | Loss: 0.00001499
Iteration 30/1000 | Loss: 0.00001498
Iteration 31/1000 | Loss: 0.00001498
Iteration 32/1000 | Loss: 0.00001498
Iteration 33/1000 | Loss: 0.00001498
Iteration 34/1000 | Loss: 0.00001497
Iteration 35/1000 | Loss: 0.00001497
Iteration 36/1000 | Loss: 0.00001497
Iteration 37/1000 | Loss: 0.00001497
Iteration 38/1000 | Loss: 0.00001496
Iteration 39/1000 | Loss: 0.00001496
Iteration 40/1000 | Loss: 0.00001496
Iteration 41/1000 | Loss: 0.00001495
Iteration 42/1000 | Loss: 0.00001495
Iteration 43/1000 | Loss: 0.00001495
Iteration 44/1000 | Loss: 0.00001495
Iteration 45/1000 | Loss: 0.00001494
Iteration 46/1000 | Loss: 0.00001494
Iteration 47/1000 | Loss: 0.00001494
Iteration 48/1000 | Loss: 0.00001493
Iteration 49/1000 | Loss: 0.00001493
Iteration 50/1000 | Loss: 0.00001493
Iteration 51/1000 | Loss: 0.00001493
Iteration 52/1000 | Loss: 0.00001493
Iteration 53/1000 | Loss: 0.00001492
Iteration 54/1000 | Loss: 0.00001492
Iteration 55/1000 | Loss: 0.00001492
Iteration 56/1000 | Loss: 0.00001491
Iteration 57/1000 | Loss: 0.00001491
Iteration 58/1000 | Loss: 0.00001491
Iteration 59/1000 | Loss: 0.00001491
Iteration 60/1000 | Loss: 0.00001491
Iteration 61/1000 | Loss: 0.00001491
Iteration 62/1000 | Loss: 0.00001491
Iteration 63/1000 | Loss: 0.00001490
Iteration 64/1000 | Loss: 0.00001490
Iteration 65/1000 | Loss: 0.00001490
Iteration 66/1000 | Loss: 0.00001490
Iteration 67/1000 | Loss: 0.00001490
Iteration 68/1000 | Loss: 0.00001489
Iteration 69/1000 | Loss: 0.00001489
Iteration 70/1000 | Loss: 0.00001489
Iteration 71/1000 | Loss: 0.00001489
Iteration 72/1000 | Loss: 0.00001488
Iteration 73/1000 | Loss: 0.00001488
Iteration 74/1000 | Loss: 0.00001488
Iteration 75/1000 | Loss: 0.00001488
Iteration 76/1000 | Loss: 0.00001488
Iteration 77/1000 | Loss: 0.00001488
Iteration 78/1000 | Loss: 0.00001487
Iteration 79/1000 | Loss: 0.00001487
Iteration 80/1000 | Loss: 0.00001487
Iteration 81/1000 | Loss: 0.00001486
Iteration 82/1000 | Loss: 0.00001486
Iteration 83/1000 | Loss: 0.00001486
Iteration 84/1000 | Loss: 0.00001486
Iteration 85/1000 | Loss: 0.00001485
Iteration 86/1000 | Loss: 0.00001485
Iteration 87/1000 | Loss: 0.00001485
Iteration 88/1000 | Loss: 0.00001485
Iteration 89/1000 | Loss: 0.00001485
Iteration 90/1000 | Loss: 0.00001484
Iteration 91/1000 | Loss: 0.00001484
Iteration 92/1000 | Loss: 0.00001484
Iteration 93/1000 | Loss: 0.00001484
Iteration 94/1000 | Loss: 0.00001484
Iteration 95/1000 | Loss: 0.00001484
Iteration 96/1000 | Loss: 0.00001484
Iteration 97/1000 | Loss: 0.00001484
Iteration 98/1000 | Loss: 0.00001484
Iteration 99/1000 | Loss: 0.00001484
Iteration 100/1000 | Loss: 0.00001483
Iteration 101/1000 | Loss: 0.00001483
Iteration 102/1000 | Loss: 0.00001483
Iteration 103/1000 | Loss: 0.00001483
Iteration 104/1000 | Loss: 0.00001482
Iteration 105/1000 | Loss: 0.00001482
Iteration 106/1000 | Loss: 0.00001482
Iteration 107/1000 | Loss: 0.00001482
Iteration 108/1000 | Loss: 0.00001482
Iteration 109/1000 | Loss: 0.00001481
Iteration 110/1000 | Loss: 0.00001481
Iteration 111/1000 | Loss: 0.00001481
Iteration 112/1000 | Loss: 0.00001481
Iteration 113/1000 | Loss: 0.00001481
Iteration 114/1000 | Loss: 0.00001481
Iteration 115/1000 | Loss: 0.00001480
Iteration 116/1000 | Loss: 0.00001480
Iteration 117/1000 | Loss: 0.00001480
Iteration 118/1000 | Loss: 0.00001480
Iteration 119/1000 | Loss: 0.00001480
Iteration 120/1000 | Loss: 0.00001480
Iteration 121/1000 | Loss: 0.00001479
Iteration 122/1000 | Loss: 0.00001479
Iteration 123/1000 | Loss: 0.00001478
Iteration 124/1000 | Loss: 0.00001478
Iteration 125/1000 | Loss: 0.00001478
Iteration 126/1000 | Loss: 0.00001478
Iteration 127/1000 | Loss: 0.00001478
Iteration 128/1000 | Loss: 0.00001478
Iteration 129/1000 | Loss: 0.00001478
Iteration 130/1000 | Loss: 0.00001478
Iteration 131/1000 | Loss: 0.00001478
Iteration 132/1000 | Loss: 0.00001478
Iteration 133/1000 | Loss: 0.00001478
Iteration 134/1000 | Loss: 0.00001478
Iteration 135/1000 | Loss: 0.00001478
Iteration 136/1000 | Loss: 0.00001478
Iteration 137/1000 | Loss: 0.00001477
Iteration 138/1000 | Loss: 0.00001477
Iteration 139/1000 | Loss: 0.00001477
Iteration 140/1000 | Loss: 0.00001477
Iteration 141/1000 | Loss: 0.00001477
Iteration 142/1000 | Loss: 0.00001477
Iteration 143/1000 | Loss: 0.00001477
Iteration 144/1000 | Loss: 0.00001477
Iteration 145/1000 | Loss: 0.00001477
Iteration 146/1000 | Loss: 0.00001476
Iteration 147/1000 | Loss: 0.00001476
Iteration 148/1000 | Loss: 0.00001476
Iteration 149/1000 | Loss: 0.00001476
Iteration 150/1000 | Loss: 0.00001476
Iteration 151/1000 | Loss: 0.00001476
Iteration 152/1000 | Loss: 0.00001476
Iteration 153/1000 | Loss: 0.00001476
Iteration 154/1000 | Loss: 0.00001476
Iteration 155/1000 | Loss: 0.00001476
Iteration 156/1000 | Loss: 0.00001476
Iteration 157/1000 | Loss: 0.00001476
Iteration 158/1000 | Loss: 0.00001476
Iteration 159/1000 | Loss: 0.00001476
Iteration 160/1000 | Loss: 0.00001476
Iteration 161/1000 | Loss: 0.00001476
Iteration 162/1000 | Loss: 0.00001476
Iteration 163/1000 | Loss: 0.00001476
Iteration 164/1000 | Loss: 0.00001476
Iteration 165/1000 | Loss: 0.00001476
Iteration 166/1000 | Loss: 0.00001476
Iteration 167/1000 | Loss: 0.00001476
Iteration 168/1000 | Loss: 0.00001476
Iteration 169/1000 | Loss: 0.00001476
Iteration 170/1000 | Loss: 0.00001476
Iteration 171/1000 | Loss: 0.00001476
Iteration 172/1000 | Loss: 0.00001476
Iteration 173/1000 | Loss: 0.00001476
Iteration 174/1000 | Loss: 0.00001476
Iteration 175/1000 | Loss: 0.00001476
Iteration 176/1000 | Loss: 0.00001476
Iteration 177/1000 | Loss: 0.00001476
Iteration 178/1000 | Loss: 0.00001476
Iteration 179/1000 | Loss: 0.00001476
Iteration 180/1000 | Loss: 0.00001476
Iteration 181/1000 | Loss: 0.00001476
Iteration 182/1000 | Loss: 0.00001476
Iteration 183/1000 | Loss: 0.00001476
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 183. Stopping optimization.
Last 5 losses: [1.4759583791601472e-05, 1.4759583791601472e-05, 1.4759583791601472e-05, 1.4759583791601472e-05, 1.4759583791601472e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4759583791601472e-05

Optimization complete. Final v2v error: 3.2298314571380615 mm

Highest mean error: 3.9525907039642334 mm for frame 90

Lowest mean error: 2.8133394718170166 mm for frame 56

Saving results

Total time: 39.70340156555176
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_018/1003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_018/1003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_018/1003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00647765
Iteration 2/25 | Loss: 0.00132897
Iteration 3/25 | Loss: 0.00119898
Iteration 4/25 | Loss: 0.00118343
Iteration 5/25 | Loss: 0.00117989
Iteration 6/25 | Loss: 0.00117989
Iteration 7/25 | Loss: 0.00117989
Iteration 8/25 | Loss: 0.00117989
Iteration 9/25 | Loss: 0.00117989
Iteration 10/25 | Loss: 0.00117989
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0011798877967521548, 0.0011798877967521548, 0.0011798877967521548, 0.0011798877967521548, 0.0011798877967521548]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011798877967521548

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.08069181
Iteration 2/25 | Loss: 0.00082801
Iteration 3/25 | Loss: 0.00082801
Iteration 4/25 | Loss: 0.00082801
Iteration 5/25 | Loss: 0.00082801
Iteration 6/25 | Loss: 0.00082800
Iteration 7/25 | Loss: 0.00082800
Iteration 8/25 | Loss: 0.00082800
Iteration 9/25 | Loss: 0.00082800
Iteration 10/25 | Loss: 0.00082800
Iteration 11/25 | Loss: 0.00082800
Iteration 12/25 | Loss: 0.00082800
Iteration 13/25 | Loss: 0.00082800
Iteration 14/25 | Loss: 0.00082800
Iteration 15/25 | Loss: 0.00082800
Iteration 16/25 | Loss: 0.00082800
Iteration 17/25 | Loss: 0.00082800
Iteration 18/25 | Loss: 0.00082800
Iteration 19/25 | Loss: 0.00082800
Iteration 20/25 | Loss: 0.00082800
Iteration 21/25 | Loss: 0.00082800
Iteration 22/25 | Loss: 0.00082800
Iteration 23/25 | Loss: 0.00082800
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0008280035690404475, 0.0008280035690404475, 0.0008280035690404475, 0.0008280035690404475, 0.0008280035690404475]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008280035690404475

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00082800
Iteration 2/1000 | Loss: 0.00003780
Iteration 3/1000 | Loss: 0.00002591
Iteration 4/1000 | Loss: 0.00002062
Iteration 5/1000 | Loss: 0.00001898
Iteration 6/1000 | Loss: 0.00001792
Iteration 7/1000 | Loss: 0.00001718
Iteration 8/1000 | Loss: 0.00001661
Iteration 9/1000 | Loss: 0.00001628
Iteration 10/1000 | Loss: 0.00001593
Iteration 11/1000 | Loss: 0.00001558
Iteration 12/1000 | Loss: 0.00001540
Iteration 13/1000 | Loss: 0.00001534
Iteration 14/1000 | Loss: 0.00001519
Iteration 15/1000 | Loss: 0.00001516
Iteration 16/1000 | Loss: 0.00001514
Iteration 17/1000 | Loss: 0.00001512
Iteration 18/1000 | Loss: 0.00001510
Iteration 19/1000 | Loss: 0.00001507
Iteration 20/1000 | Loss: 0.00001497
Iteration 21/1000 | Loss: 0.00001490
Iteration 22/1000 | Loss: 0.00001488
Iteration 23/1000 | Loss: 0.00001487
Iteration 24/1000 | Loss: 0.00001486
Iteration 25/1000 | Loss: 0.00001486
Iteration 26/1000 | Loss: 0.00001485
Iteration 27/1000 | Loss: 0.00001484
Iteration 28/1000 | Loss: 0.00001483
Iteration 29/1000 | Loss: 0.00001481
Iteration 30/1000 | Loss: 0.00001480
Iteration 31/1000 | Loss: 0.00001480
Iteration 32/1000 | Loss: 0.00001477
Iteration 33/1000 | Loss: 0.00001477
Iteration 34/1000 | Loss: 0.00001477
Iteration 35/1000 | Loss: 0.00001476
Iteration 36/1000 | Loss: 0.00001475
Iteration 37/1000 | Loss: 0.00001474
Iteration 38/1000 | Loss: 0.00001474
Iteration 39/1000 | Loss: 0.00001473
Iteration 40/1000 | Loss: 0.00001473
Iteration 41/1000 | Loss: 0.00001473
Iteration 42/1000 | Loss: 0.00001472
Iteration 43/1000 | Loss: 0.00001472
Iteration 44/1000 | Loss: 0.00001471
Iteration 45/1000 | Loss: 0.00001471
Iteration 46/1000 | Loss: 0.00001471
Iteration 47/1000 | Loss: 0.00001471
Iteration 48/1000 | Loss: 0.00001470
Iteration 49/1000 | Loss: 0.00001470
Iteration 50/1000 | Loss: 0.00001470
Iteration 51/1000 | Loss: 0.00001469
Iteration 52/1000 | Loss: 0.00001469
Iteration 53/1000 | Loss: 0.00001469
Iteration 54/1000 | Loss: 0.00001468
Iteration 55/1000 | Loss: 0.00001468
Iteration 56/1000 | Loss: 0.00001468
Iteration 57/1000 | Loss: 0.00001467
Iteration 58/1000 | Loss: 0.00001467
Iteration 59/1000 | Loss: 0.00001467
Iteration 60/1000 | Loss: 0.00001466
Iteration 61/1000 | Loss: 0.00001466
Iteration 62/1000 | Loss: 0.00001466
Iteration 63/1000 | Loss: 0.00001465
Iteration 64/1000 | Loss: 0.00001465
Iteration 65/1000 | Loss: 0.00001465
Iteration 66/1000 | Loss: 0.00001465
Iteration 67/1000 | Loss: 0.00001465
Iteration 68/1000 | Loss: 0.00001464
Iteration 69/1000 | Loss: 0.00001464
Iteration 70/1000 | Loss: 0.00001464
Iteration 71/1000 | Loss: 0.00001464
Iteration 72/1000 | Loss: 0.00001464
Iteration 73/1000 | Loss: 0.00001464
Iteration 74/1000 | Loss: 0.00001464
Iteration 75/1000 | Loss: 0.00001464
Iteration 76/1000 | Loss: 0.00001464
Iteration 77/1000 | Loss: 0.00001464
Iteration 78/1000 | Loss: 0.00001464
Iteration 79/1000 | Loss: 0.00001463
Iteration 80/1000 | Loss: 0.00001463
Iteration 81/1000 | Loss: 0.00001463
Iteration 82/1000 | Loss: 0.00001463
Iteration 83/1000 | Loss: 0.00001463
Iteration 84/1000 | Loss: 0.00001463
Iteration 85/1000 | Loss: 0.00001463
Iteration 86/1000 | Loss: 0.00001463
Iteration 87/1000 | Loss: 0.00001463
Iteration 88/1000 | Loss: 0.00001463
Iteration 89/1000 | Loss: 0.00001463
Iteration 90/1000 | Loss: 0.00001463
Iteration 91/1000 | Loss: 0.00001463
Iteration 92/1000 | Loss: 0.00001463
Iteration 93/1000 | Loss: 0.00001463
Iteration 94/1000 | Loss: 0.00001463
Iteration 95/1000 | Loss: 0.00001463
Iteration 96/1000 | Loss: 0.00001463
Iteration 97/1000 | Loss: 0.00001463
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 97. Stopping optimization.
Last 5 losses: [1.4631601516157389e-05, 1.4631601516157389e-05, 1.4631601516157389e-05, 1.4631601516157389e-05, 1.4631601516157389e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4631601516157389e-05

Optimization complete. Final v2v error: 3.1824798583984375 mm

Highest mean error: 3.6923654079437256 mm for frame 7

Lowest mean error: 2.745649814605713 mm for frame 90

Saving results

Total time: 40.55453443527222
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_018/1072/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_018/1072.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_018/1072
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00737883
Iteration 2/25 | Loss: 0.00125624
Iteration 3/25 | Loss: 0.00116362
Iteration 4/25 | Loss: 0.00115064
Iteration 5/25 | Loss: 0.00114833
Iteration 6/25 | Loss: 0.00114833
Iteration 7/25 | Loss: 0.00114833
Iteration 8/25 | Loss: 0.00114833
Iteration 9/25 | Loss: 0.00114833
Iteration 10/25 | Loss: 0.00114833
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0011483257403597236, 0.0011483257403597236, 0.0011483257403597236, 0.0011483257403597236, 0.0011483257403597236]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011483257403597236

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33031571
Iteration 2/25 | Loss: 0.00054099
Iteration 3/25 | Loss: 0.00054096
Iteration 4/25 | Loss: 0.00054095
Iteration 5/25 | Loss: 0.00054095
Iteration 6/25 | Loss: 0.00054095
Iteration 7/25 | Loss: 0.00054095
Iteration 8/25 | Loss: 0.00054095
Iteration 9/25 | Loss: 0.00054095
Iteration 10/25 | Loss: 0.00054095
Iteration 11/25 | Loss: 0.00054095
Iteration 12/25 | Loss: 0.00054095
Iteration 13/25 | Loss: 0.00054095
Iteration 14/25 | Loss: 0.00054095
Iteration 15/25 | Loss: 0.00054095
Iteration 16/25 | Loss: 0.00054095
Iteration 17/25 | Loss: 0.00054095
Iteration 18/25 | Loss: 0.00054095
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0005409511504694819, 0.0005409511504694819, 0.0005409511504694819, 0.0005409511504694819, 0.0005409511504694819]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005409511504694819

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00054095
Iteration 2/1000 | Loss: 0.00002720
Iteration 3/1000 | Loss: 0.00001993
Iteration 4/1000 | Loss: 0.00001840
Iteration 5/1000 | Loss: 0.00001693
Iteration 6/1000 | Loss: 0.00001602
Iteration 7/1000 | Loss: 0.00001539
Iteration 8/1000 | Loss: 0.00001490
Iteration 9/1000 | Loss: 0.00001448
Iteration 10/1000 | Loss: 0.00001422
Iteration 11/1000 | Loss: 0.00001418
Iteration 12/1000 | Loss: 0.00001395
Iteration 13/1000 | Loss: 0.00001390
Iteration 14/1000 | Loss: 0.00001374
Iteration 15/1000 | Loss: 0.00001372
Iteration 16/1000 | Loss: 0.00001356
Iteration 17/1000 | Loss: 0.00001351
Iteration 18/1000 | Loss: 0.00001346
Iteration 19/1000 | Loss: 0.00001345
Iteration 20/1000 | Loss: 0.00001344
Iteration 21/1000 | Loss: 0.00001342
Iteration 22/1000 | Loss: 0.00001341
Iteration 23/1000 | Loss: 0.00001338
Iteration 24/1000 | Loss: 0.00001338
Iteration 25/1000 | Loss: 0.00001338
Iteration 26/1000 | Loss: 0.00001338
Iteration 27/1000 | Loss: 0.00001338
Iteration 28/1000 | Loss: 0.00001338
Iteration 29/1000 | Loss: 0.00001338
Iteration 30/1000 | Loss: 0.00001338
Iteration 31/1000 | Loss: 0.00001337
Iteration 32/1000 | Loss: 0.00001337
Iteration 33/1000 | Loss: 0.00001337
Iteration 34/1000 | Loss: 0.00001337
Iteration 35/1000 | Loss: 0.00001337
Iteration 36/1000 | Loss: 0.00001337
Iteration 37/1000 | Loss: 0.00001334
Iteration 38/1000 | Loss: 0.00001334
Iteration 39/1000 | Loss: 0.00001334
Iteration 40/1000 | Loss: 0.00001333
Iteration 41/1000 | Loss: 0.00001333
Iteration 42/1000 | Loss: 0.00001332
Iteration 43/1000 | Loss: 0.00001332
Iteration 44/1000 | Loss: 0.00001331
Iteration 45/1000 | Loss: 0.00001331
Iteration 46/1000 | Loss: 0.00001329
Iteration 47/1000 | Loss: 0.00001329
Iteration 48/1000 | Loss: 0.00001328
Iteration 49/1000 | Loss: 0.00001328
Iteration 50/1000 | Loss: 0.00001328
Iteration 51/1000 | Loss: 0.00001328
Iteration 52/1000 | Loss: 0.00001328
Iteration 53/1000 | Loss: 0.00001328
Iteration 54/1000 | Loss: 0.00001328
Iteration 55/1000 | Loss: 0.00001328
Iteration 56/1000 | Loss: 0.00001328
Iteration 57/1000 | Loss: 0.00001328
Iteration 58/1000 | Loss: 0.00001328
Iteration 59/1000 | Loss: 0.00001328
Iteration 60/1000 | Loss: 0.00001328
Iteration 61/1000 | Loss: 0.00001328
Iteration 62/1000 | Loss: 0.00001325
Iteration 63/1000 | Loss: 0.00001325
Iteration 64/1000 | Loss: 0.00001324
Iteration 65/1000 | Loss: 0.00001324
Iteration 66/1000 | Loss: 0.00001323
Iteration 67/1000 | Loss: 0.00001323
Iteration 68/1000 | Loss: 0.00001323
Iteration 69/1000 | Loss: 0.00001322
Iteration 70/1000 | Loss: 0.00001322
Iteration 71/1000 | Loss: 0.00001322
Iteration 72/1000 | Loss: 0.00001321
Iteration 73/1000 | Loss: 0.00001321
Iteration 74/1000 | Loss: 0.00001321
Iteration 75/1000 | Loss: 0.00001320
Iteration 76/1000 | Loss: 0.00001320
Iteration 77/1000 | Loss: 0.00001320
Iteration 78/1000 | Loss: 0.00001319
Iteration 79/1000 | Loss: 0.00001319
Iteration 80/1000 | Loss: 0.00001319
Iteration 81/1000 | Loss: 0.00001318
Iteration 82/1000 | Loss: 0.00001318
Iteration 83/1000 | Loss: 0.00001317
Iteration 84/1000 | Loss: 0.00001317
Iteration 85/1000 | Loss: 0.00001316
Iteration 86/1000 | Loss: 0.00001316
Iteration 87/1000 | Loss: 0.00001316
Iteration 88/1000 | Loss: 0.00001316
Iteration 89/1000 | Loss: 0.00001316
Iteration 90/1000 | Loss: 0.00001316
Iteration 91/1000 | Loss: 0.00001316
Iteration 92/1000 | Loss: 0.00001315
Iteration 93/1000 | Loss: 0.00001314
Iteration 94/1000 | Loss: 0.00001313
Iteration 95/1000 | Loss: 0.00001313
Iteration 96/1000 | Loss: 0.00001313
Iteration 97/1000 | Loss: 0.00001313
Iteration 98/1000 | Loss: 0.00001312
Iteration 99/1000 | Loss: 0.00001312
Iteration 100/1000 | Loss: 0.00001312
Iteration 101/1000 | Loss: 0.00001312
Iteration 102/1000 | Loss: 0.00001312
Iteration 103/1000 | Loss: 0.00001312
Iteration 104/1000 | Loss: 0.00001312
Iteration 105/1000 | Loss: 0.00001312
Iteration 106/1000 | Loss: 0.00001312
Iteration 107/1000 | Loss: 0.00001311
Iteration 108/1000 | Loss: 0.00001311
Iteration 109/1000 | Loss: 0.00001310
Iteration 110/1000 | Loss: 0.00001309
Iteration 111/1000 | Loss: 0.00001309
Iteration 112/1000 | Loss: 0.00001309
Iteration 113/1000 | Loss: 0.00001308
Iteration 114/1000 | Loss: 0.00001308
Iteration 115/1000 | Loss: 0.00001308
Iteration 116/1000 | Loss: 0.00001308
Iteration 117/1000 | Loss: 0.00001308
Iteration 118/1000 | Loss: 0.00001308
Iteration 119/1000 | Loss: 0.00001307
Iteration 120/1000 | Loss: 0.00001306
Iteration 121/1000 | Loss: 0.00001306
Iteration 122/1000 | Loss: 0.00001305
Iteration 123/1000 | Loss: 0.00001305
Iteration 124/1000 | Loss: 0.00001305
Iteration 125/1000 | Loss: 0.00001304
Iteration 126/1000 | Loss: 0.00001304
Iteration 127/1000 | Loss: 0.00001304
Iteration 128/1000 | Loss: 0.00001304
Iteration 129/1000 | Loss: 0.00001304
Iteration 130/1000 | Loss: 0.00001304
Iteration 131/1000 | Loss: 0.00001304
Iteration 132/1000 | Loss: 0.00001303
Iteration 133/1000 | Loss: 0.00001303
Iteration 134/1000 | Loss: 0.00001303
Iteration 135/1000 | Loss: 0.00001303
Iteration 136/1000 | Loss: 0.00001302
Iteration 137/1000 | Loss: 0.00001302
Iteration 138/1000 | Loss: 0.00001302
Iteration 139/1000 | Loss: 0.00001302
Iteration 140/1000 | Loss: 0.00001302
Iteration 141/1000 | Loss: 0.00001302
Iteration 142/1000 | Loss: 0.00001302
Iteration 143/1000 | Loss: 0.00001301
Iteration 144/1000 | Loss: 0.00001301
Iteration 145/1000 | Loss: 0.00001301
Iteration 146/1000 | Loss: 0.00001301
Iteration 147/1000 | Loss: 0.00001300
Iteration 148/1000 | Loss: 0.00001300
Iteration 149/1000 | Loss: 0.00001300
Iteration 150/1000 | Loss: 0.00001300
Iteration 151/1000 | Loss: 0.00001300
Iteration 152/1000 | Loss: 0.00001300
Iteration 153/1000 | Loss: 0.00001300
Iteration 154/1000 | Loss: 0.00001300
Iteration 155/1000 | Loss: 0.00001300
Iteration 156/1000 | Loss: 0.00001300
Iteration 157/1000 | Loss: 0.00001300
Iteration 158/1000 | Loss: 0.00001300
Iteration 159/1000 | Loss: 0.00001300
Iteration 160/1000 | Loss: 0.00001300
Iteration 161/1000 | Loss: 0.00001300
Iteration 162/1000 | Loss: 0.00001300
Iteration 163/1000 | Loss: 0.00001300
Iteration 164/1000 | Loss: 0.00001300
Iteration 165/1000 | Loss: 0.00001300
Iteration 166/1000 | Loss: 0.00001300
Iteration 167/1000 | Loss: 0.00001300
Iteration 168/1000 | Loss: 0.00001300
Iteration 169/1000 | Loss: 0.00001300
Iteration 170/1000 | Loss: 0.00001300
Iteration 171/1000 | Loss: 0.00001300
Iteration 172/1000 | Loss: 0.00001300
Iteration 173/1000 | Loss: 0.00001300
Iteration 174/1000 | Loss: 0.00001300
Iteration 175/1000 | Loss: 0.00001300
Iteration 176/1000 | Loss: 0.00001300
Iteration 177/1000 | Loss: 0.00001300
Iteration 178/1000 | Loss: 0.00001300
Iteration 179/1000 | Loss: 0.00001300
Iteration 180/1000 | Loss: 0.00001300
Iteration 181/1000 | Loss: 0.00001300
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 181. Stopping optimization.
Last 5 losses: [1.2998264537600335e-05, 1.2998264537600335e-05, 1.2998264537600335e-05, 1.2998264537600335e-05, 1.2998264537600335e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2998264537600335e-05

Optimization complete. Final v2v error: 3.0940911769866943 mm

Highest mean error: 3.4760611057281494 mm for frame 229

Lowest mean error: 2.9053945541381836 mm for frame 30

Saving results

Total time: 43.79775428771973
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_018/1030/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_018/1030.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_018/1030
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01044642
Iteration 2/25 | Loss: 0.00175957
Iteration 3/25 | Loss: 0.00150846
Iteration 4/25 | Loss: 0.00117162
Iteration 5/25 | Loss: 0.00116009
Iteration 6/25 | Loss: 0.00114869
Iteration 7/25 | Loss: 0.00114330
Iteration 8/25 | Loss: 0.00114084
Iteration 9/25 | Loss: 0.00114088
Iteration 10/25 | Loss: 0.00113772
Iteration 11/25 | Loss: 0.00113719
Iteration 12/25 | Loss: 0.00113650
Iteration 13/25 | Loss: 0.00113592
Iteration 14/25 | Loss: 0.00113573
Iteration 15/25 | Loss: 0.00113570
Iteration 16/25 | Loss: 0.00113569
Iteration 17/25 | Loss: 0.00113569
Iteration 18/25 | Loss: 0.00113569
Iteration 19/25 | Loss: 0.00113569
Iteration 20/25 | Loss: 0.00113569
Iteration 21/25 | Loss: 0.00113569
Iteration 22/25 | Loss: 0.00113568
Iteration 23/25 | Loss: 0.00113568
Iteration 24/25 | Loss: 0.00113568
Iteration 25/25 | Loss: 0.00113568

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 6.57991505
Iteration 2/25 | Loss: 0.00107840
Iteration 3/25 | Loss: 0.00086583
Iteration 4/25 | Loss: 0.00086583
Iteration 5/25 | Loss: 0.00086582
Iteration 6/25 | Loss: 0.00086582
Iteration 7/25 | Loss: 0.00086582
Iteration 8/25 | Loss: 0.00086582
Iteration 9/25 | Loss: 0.00086582
Iteration 10/25 | Loss: 0.00086582
Iteration 11/25 | Loss: 0.00086582
Iteration 12/25 | Loss: 0.00086582
Iteration 13/25 | Loss: 0.00086582
Iteration 14/25 | Loss: 0.00086582
Iteration 15/25 | Loss: 0.00086582
Iteration 16/25 | Loss: 0.00086582
Iteration 17/25 | Loss: 0.00086582
Iteration 18/25 | Loss: 0.00086582
Iteration 19/25 | Loss: 0.00086582
Iteration 20/25 | Loss: 0.00086582
Iteration 21/25 | Loss: 0.00086582
Iteration 22/25 | Loss: 0.00086582
Iteration 23/25 | Loss: 0.00086582
Iteration 24/25 | Loss: 0.00086582
Iteration 25/25 | Loss: 0.00086582

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00086582
Iteration 2/1000 | Loss: 0.00021575
Iteration 3/1000 | Loss: 0.00011012
Iteration 4/1000 | Loss: 0.00001905
Iteration 5/1000 | Loss: 0.00001450
Iteration 6/1000 | Loss: 0.00005532
Iteration 7/1000 | Loss: 0.00029977
Iteration 8/1000 | Loss: 0.00002649
Iteration 9/1000 | Loss: 0.00002350
Iteration 10/1000 | Loss: 0.00001358
Iteration 11/1000 | Loss: 0.00001304
Iteration 12/1000 | Loss: 0.00001277
Iteration 13/1000 | Loss: 0.00010113
Iteration 14/1000 | Loss: 0.00001271
Iteration 15/1000 | Loss: 0.00001248
Iteration 16/1000 | Loss: 0.00006392
Iteration 17/1000 | Loss: 0.00007695
Iteration 18/1000 | Loss: 0.00007220
Iteration 19/1000 | Loss: 0.00001241
Iteration 20/1000 | Loss: 0.00008298
Iteration 21/1000 | Loss: 0.00004040
Iteration 22/1000 | Loss: 0.00001302
Iteration 23/1000 | Loss: 0.00008490
Iteration 24/1000 | Loss: 0.00004356
Iteration 25/1000 | Loss: 0.00002894
Iteration 26/1000 | Loss: 0.00004802
Iteration 27/1000 | Loss: 0.00001222
Iteration 28/1000 | Loss: 0.00001219
Iteration 29/1000 | Loss: 0.00001219
Iteration 30/1000 | Loss: 0.00001218
Iteration 31/1000 | Loss: 0.00001218
Iteration 32/1000 | Loss: 0.00001217
Iteration 33/1000 | Loss: 0.00001216
Iteration 34/1000 | Loss: 0.00001216
Iteration 35/1000 | Loss: 0.00001215
Iteration 36/1000 | Loss: 0.00001215
Iteration 37/1000 | Loss: 0.00001214
Iteration 38/1000 | Loss: 0.00001214
Iteration 39/1000 | Loss: 0.00001213
Iteration 40/1000 | Loss: 0.00007078
Iteration 41/1000 | Loss: 0.00003693
Iteration 42/1000 | Loss: 0.00002315
Iteration 43/1000 | Loss: 0.00009379
Iteration 44/1000 | Loss: 0.00002196
Iteration 45/1000 | Loss: 0.00001219
Iteration 46/1000 | Loss: 0.00008432
Iteration 47/1000 | Loss: 0.00001302
Iteration 48/1000 | Loss: 0.00001324
Iteration 49/1000 | Loss: 0.00001206
Iteration 50/1000 | Loss: 0.00001204
Iteration 51/1000 | Loss: 0.00001204
Iteration 52/1000 | Loss: 0.00001204
Iteration 53/1000 | Loss: 0.00001203
Iteration 54/1000 | Loss: 0.00001203
Iteration 55/1000 | Loss: 0.00001203
Iteration 56/1000 | Loss: 0.00001202
Iteration 57/1000 | Loss: 0.00001200
Iteration 58/1000 | Loss: 0.00002526
Iteration 59/1000 | Loss: 0.00001199
Iteration 60/1000 | Loss: 0.00001194
Iteration 61/1000 | Loss: 0.00001194
Iteration 62/1000 | Loss: 0.00001193
Iteration 63/1000 | Loss: 0.00001193
Iteration 64/1000 | Loss: 0.00002398
Iteration 65/1000 | Loss: 0.00001195
Iteration 66/1000 | Loss: 0.00001195
Iteration 67/1000 | Loss: 0.00001195
Iteration 68/1000 | Loss: 0.00001194
Iteration 69/1000 | Loss: 0.00001194
Iteration 70/1000 | Loss: 0.00001194
Iteration 71/1000 | Loss: 0.00001194
Iteration 72/1000 | Loss: 0.00001194
Iteration 73/1000 | Loss: 0.00001194
Iteration 74/1000 | Loss: 0.00001194
Iteration 75/1000 | Loss: 0.00001194
Iteration 76/1000 | Loss: 0.00001193
Iteration 77/1000 | Loss: 0.00011283
Iteration 78/1000 | Loss: 0.00002906
Iteration 79/1000 | Loss: 0.00002895
Iteration 80/1000 | Loss: 0.00002208
Iteration 81/1000 | Loss: 0.00002637
Iteration 82/1000 | Loss: 0.00002586
Iteration 83/1000 | Loss: 0.00002925
Iteration 84/1000 | Loss: 0.00001256
Iteration 85/1000 | Loss: 0.00001256
Iteration 86/1000 | Loss: 0.00001255
Iteration 87/1000 | Loss: 0.00001208
Iteration 88/1000 | Loss: 0.00001568
Iteration 89/1000 | Loss: 0.00001181
Iteration 90/1000 | Loss: 0.00001181
Iteration 91/1000 | Loss: 0.00001181
Iteration 92/1000 | Loss: 0.00001181
Iteration 93/1000 | Loss: 0.00001181
Iteration 94/1000 | Loss: 0.00001181
Iteration 95/1000 | Loss: 0.00001181
Iteration 96/1000 | Loss: 0.00001181
Iteration 97/1000 | Loss: 0.00001180
Iteration 98/1000 | Loss: 0.00001180
Iteration 99/1000 | Loss: 0.00001180
Iteration 100/1000 | Loss: 0.00001180
Iteration 101/1000 | Loss: 0.00001180
Iteration 102/1000 | Loss: 0.00001179
Iteration 103/1000 | Loss: 0.00001179
Iteration 104/1000 | Loss: 0.00001179
Iteration 105/1000 | Loss: 0.00001179
Iteration 106/1000 | Loss: 0.00001179
Iteration 107/1000 | Loss: 0.00001179
Iteration 108/1000 | Loss: 0.00001179
Iteration 109/1000 | Loss: 0.00001179
Iteration 110/1000 | Loss: 0.00001179
Iteration 111/1000 | Loss: 0.00001179
Iteration 112/1000 | Loss: 0.00001179
Iteration 113/1000 | Loss: 0.00001178
Iteration 114/1000 | Loss: 0.00001178
Iteration 115/1000 | Loss: 0.00001178
Iteration 116/1000 | Loss: 0.00001178
Iteration 117/1000 | Loss: 0.00001196
Iteration 118/1000 | Loss: 0.00001507
Iteration 119/1000 | Loss: 0.00001304
Iteration 120/1000 | Loss: 0.00001176
Iteration 121/1000 | Loss: 0.00001176
Iteration 122/1000 | Loss: 0.00001176
Iteration 123/1000 | Loss: 0.00001176
Iteration 124/1000 | Loss: 0.00001176
Iteration 125/1000 | Loss: 0.00001176
Iteration 126/1000 | Loss: 0.00001176
Iteration 127/1000 | Loss: 0.00001176
Iteration 128/1000 | Loss: 0.00001175
Iteration 129/1000 | Loss: 0.00001175
Iteration 130/1000 | Loss: 0.00001175
Iteration 131/1000 | Loss: 0.00001175
Iteration 132/1000 | Loss: 0.00001175
Iteration 133/1000 | Loss: 0.00001237
Iteration 134/1000 | Loss: 0.00001175
Iteration 135/1000 | Loss: 0.00001175
Iteration 136/1000 | Loss: 0.00001174
Iteration 137/1000 | Loss: 0.00001174
Iteration 138/1000 | Loss: 0.00001174
Iteration 139/1000 | Loss: 0.00001174
Iteration 140/1000 | Loss: 0.00001174
Iteration 141/1000 | Loss: 0.00001189
Iteration 142/1000 | Loss: 0.00001223
Iteration 143/1000 | Loss: 0.00001173
Iteration 144/1000 | Loss: 0.00001172
Iteration 145/1000 | Loss: 0.00001172
Iteration 146/1000 | Loss: 0.00001172
Iteration 147/1000 | Loss: 0.00001172
Iteration 148/1000 | Loss: 0.00001171
Iteration 149/1000 | Loss: 0.00001171
Iteration 150/1000 | Loss: 0.00001171
Iteration 151/1000 | Loss: 0.00001171
Iteration 152/1000 | Loss: 0.00001171
Iteration 153/1000 | Loss: 0.00001171
Iteration 154/1000 | Loss: 0.00001171
Iteration 155/1000 | Loss: 0.00001171
Iteration 156/1000 | Loss: 0.00001171
Iteration 157/1000 | Loss: 0.00001171
Iteration 158/1000 | Loss: 0.00001171
Iteration 159/1000 | Loss: 0.00001171
Iteration 160/1000 | Loss: 0.00001171
Iteration 161/1000 | Loss: 0.00001171
Iteration 162/1000 | Loss: 0.00001171
Iteration 163/1000 | Loss: 0.00001171
Iteration 164/1000 | Loss: 0.00001171
Iteration 165/1000 | Loss: 0.00001170
Iteration 166/1000 | Loss: 0.00001170
Iteration 167/1000 | Loss: 0.00001170
Iteration 168/1000 | Loss: 0.00001170
Iteration 169/1000 | Loss: 0.00001170
Iteration 170/1000 | Loss: 0.00001170
Iteration 171/1000 | Loss: 0.00001170
Iteration 172/1000 | Loss: 0.00001170
Iteration 173/1000 | Loss: 0.00001170
Iteration 174/1000 | Loss: 0.00001170
Iteration 175/1000 | Loss: 0.00001170
Iteration 176/1000 | Loss: 0.00001170
Iteration 177/1000 | Loss: 0.00001170
Iteration 178/1000 | Loss: 0.00001170
Iteration 179/1000 | Loss: 0.00001170
Iteration 180/1000 | Loss: 0.00001170
Iteration 181/1000 | Loss: 0.00001170
Iteration 182/1000 | Loss: 0.00001170
Iteration 183/1000 | Loss: 0.00001170
Iteration 184/1000 | Loss: 0.00001170
Iteration 185/1000 | Loss: 0.00001170
Iteration 186/1000 | Loss: 0.00001170
Iteration 187/1000 | Loss: 0.00001170
Iteration 188/1000 | Loss: 0.00001170
Iteration 189/1000 | Loss: 0.00001170
Iteration 190/1000 | Loss: 0.00001170
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 190. Stopping optimization.
Last 5 losses: [1.1704159987857565e-05, 1.1704159987857565e-05, 1.1704159987857565e-05, 1.1704159987857565e-05, 1.1704159987857565e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1704159987857565e-05

Optimization complete. Final v2v error: 2.927002191543579 mm

Highest mean error: 3.5278074741363525 mm for frame 76

Lowest mean error: 2.5871338844299316 mm for frame 23

Saving results

Total time: 106.43613386154175
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_018/1061/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_018/1061.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_018/1061
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00844918
Iteration 2/25 | Loss: 0.00165168
Iteration 3/25 | Loss: 0.00131567
Iteration 4/25 | Loss: 0.00128472
Iteration 5/25 | Loss: 0.00127883
Iteration 6/25 | Loss: 0.00127855
Iteration 7/25 | Loss: 0.00127855
Iteration 8/25 | Loss: 0.00127855
Iteration 9/25 | Loss: 0.00127855
Iteration 10/25 | Loss: 0.00127855
Iteration 11/25 | Loss: 0.00127855
Iteration 12/25 | Loss: 0.00127855
Iteration 13/25 | Loss: 0.00127855
Iteration 14/25 | Loss: 0.00127855
Iteration 15/25 | Loss: 0.00127855
Iteration 16/25 | Loss: 0.00127855
Iteration 17/25 | Loss: 0.00127855
Iteration 18/25 | Loss: 0.00127855
Iteration 19/25 | Loss: 0.00127855
Iteration 20/25 | Loss: 0.00127855
Iteration 21/25 | Loss: 0.00127855
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0012785535072907805, 0.0012785535072907805, 0.0012785535072907805, 0.0012785535072907805, 0.0012785535072907805]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012785535072907805

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.93230772
Iteration 2/25 | Loss: 0.00092358
Iteration 3/25 | Loss: 0.00092358
Iteration 4/25 | Loss: 0.00092358
Iteration 5/25 | Loss: 0.00092358
Iteration 6/25 | Loss: 0.00092358
Iteration 7/25 | Loss: 0.00092358
Iteration 8/25 | Loss: 0.00092358
Iteration 9/25 | Loss: 0.00092358
Iteration 10/25 | Loss: 0.00092358
Iteration 11/25 | Loss: 0.00092358
Iteration 12/25 | Loss: 0.00092358
Iteration 13/25 | Loss: 0.00092358
Iteration 14/25 | Loss: 0.00092358
Iteration 15/25 | Loss: 0.00092358
Iteration 16/25 | Loss: 0.00092358
Iteration 17/25 | Loss: 0.00092358
Iteration 18/25 | Loss: 0.00092358
Iteration 19/25 | Loss: 0.00092358
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0009235803154297173, 0.0009235803154297173, 0.0009235803154297173, 0.0009235803154297173, 0.0009235803154297173]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009235803154297173

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00092358
Iteration 2/1000 | Loss: 0.00003376
Iteration 3/1000 | Loss: 0.00002128
Iteration 4/1000 | Loss: 0.00001909
Iteration 5/1000 | Loss: 0.00001841
Iteration 6/1000 | Loss: 0.00001769
Iteration 7/1000 | Loss: 0.00001729
Iteration 8/1000 | Loss: 0.00001703
Iteration 9/1000 | Loss: 0.00001672
Iteration 10/1000 | Loss: 0.00001652
Iteration 11/1000 | Loss: 0.00001633
Iteration 12/1000 | Loss: 0.00001629
Iteration 13/1000 | Loss: 0.00001625
Iteration 14/1000 | Loss: 0.00001620
Iteration 15/1000 | Loss: 0.00001619
Iteration 16/1000 | Loss: 0.00001618
Iteration 17/1000 | Loss: 0.00001618
Iteration 18/1000 | Loss: 0.00001615
Iteration 19/1000 | Loss: 0.00001612
Iteration 20/1000 | Loss: 0.00001607
Iteration 21/1000 | Loss: 0.00001606
Iteration 22/1000 | Loss: 0.00001605
Iteration 23/1000 | Loss: 0.00001600
Iteration 24/1000 | Loss: 0.00001600
Iteration 25/1000 | Loss: 0.00001598
Iteration 26/1000 | Loss: 0.00001598
Iteration 27/1000 | Loss: 0.00001598
Iteration 28/1000 | Loss: 0.00001598
Iteration 29/1000 | Loss: 0.00001597
Iteration 30/1000 | Loss: 0.00001597
Iteration 31/1000 | Loss: 0.00001597
Iteration 32/1000 | Loss: 0.00001597
Iteration 33/1000 | Loss: 0.00001597
Iteration 34/1000 | Loss: 0.00001597
Iteration 35/1000 | Loss: 0.00001596
Iteration 36/1000 | Loss: 0.00001595
Iteration 37/1000 | Loss: 0.00001595
Iteration 38/1000 | Loss: 0.00001594
Iteration 39/1000 | Loss: 0.00001594
Iteration 40/1000 | Loss: 0.00001593
Iteration 41/1000 | Loss: 0.00001593
Iteration 42/1000 | Loss: 0.00001593
Iteration 43/1000 | Loss: 0.00001592
Iteration 44/1000 | Loss: 0.00001592
Iteration 45/1000 | Loss: 0.00001592
Iteration 46/1000 | Loss: 0.00001592
Iteration 47/1000 | Loss: 0.00001592
Iteration 48/1000 | Loss: 0.00001592
Iteration 49/1000 | Loss: 0.00001591
Iteration 50/1000 | Loss: 0.00001591
Iteration 51/1000 | Loss: 0.00001591
Iteration 52/1000 | Loss: 0.00001591
Iteration 53/1000 | Loss: 0.00001591
Iteration 54/1000 | Loss: 0.00001591
Iteration 55/1000 | Loss: 0.00001591
Iteration 56/1000 | Loss: 0.00001591
Iteration 57/1000 | Loss: 0.00001591
Iteration 58/1000 | Loss: 0.00001591
Iteration 59/1000 | Loss: 0.00001591
Iteration 60/1000 | Loss: 0.00001591
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 60. Stopping optimization.
Last 5 losses: [1.5908366549410857e-05, 1.5908366549410857e-05, 1.5908366549410857e-05, 1.5908366549410857e-05, 1.5908366549410857e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5908366549410857e-05

Optimization complete. Final v2v error: 3.327047824859619 mm

Highest mean error: 3.7133724689483643 mm for frame 0

Lowest mean error: 3.0643224716186523 mm for frame 76

Saving results

Total time: 33.79270601272583
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_018/1082/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_018/1082.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_018/1082
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00799819
Iteration 2/25 | Loss: 0.00157869
Iteration 3/25 | Loss: 0.00136951
Iteration 4/25 | Loss: 0.00133070
Iteration 5/25 | Loss: 0.00137697
Iteration 6/25 | Loss: 0.00142443
Iteration 7/25 | Loss: 0.00138367
Iteration 8/25 | Loss: 0.00126486
Iteration 9/25 | Loss: 0.00130103
Iteration 10/25 | Loss: 0.00123553
Iteration 11/25 | Loss: 0.00121883
Iteration 12/25 | Loss: 0.00121684
Iteration 13/25 | Loss: 0.00121605
Iteration 14/25 | Loss: 0.00125418
Iteration 15/25 | Loss: 0.00120542
Iteration 16/25 | Loss: 0.00120356
Iteration 17/25 | Loss: 0.00120318
Iteration 18/25 | Loss: 0.00121146
Iteration 19/25 | Loss: 0.00120658
Iteration 20/25 | Loss: 0.00120291
Iteration 21/25 | Loss: 0.00124592
Iteration 22/25 | Loss: 0.00122885
Iteration 23/25 | Loss: 0.00119979
Iteration 24/25 | Loss: 0.00118250
Iteration 25/25 | Loss: 0.00117779

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.28170657
Iteration 2/25 | Loss: 0.00112759
Iteration 3/25 | Loss: 0.00112758
Iteration 4/25 | Loss: 0.00112758
Iteration 5/25 | Loss: 0.00112758
Iteration 6/25 | Loss: 0.00112758
Iteration 7/25 | Loss: 0.00112758
Iteration 8/25 | Loss: 0.00112758
Iteration 9/25 | Loss: 0.00112758
Iteration 10/25 | Loss: 0.00112758
Iteration 11/25 | Loss: 0.00112758
Iteration 12/25 | Loss: 0.00112758
Iteration 13/25 | Loss: 0.00112758
Iteration 14/25 | Loss: 0.00112758
Iteration 15/25 | Loss: 0.00112758
Iteration 16/25 | Loss: 0.00112758
Iteration 17/25 | Loss: 0.00112758
Iteration 18/25 | Loss: 0.00112758
Iteration 19/25 | Loss: 0.00112758
Iteration 20/25 | Loss: 0.00112758
Iteration 21/25 | Loss: 0.00112758
Iteration 22/25 | Loss: 0.00112758
Iteration 23/25 | Loss: 0.00112758
Iteration 24/25 | Loss: 0.00112758
Iteration 25/25 | Loss: 0.00112758

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00112758
Iteration 2/1000 | Loss: 0.00005607
Iteration 3/1000 | Loss: 0.00140897
Iteration 4/1000 | Loss: 0.00129881
Iteration 5/1000 | Loss: 0.00006668
Iteration 6/1000 | Loss: 0.00003641
Iteration 7/1000 | Loss: 0.00002988
Iteration 8/1000 | Loss: 0.00002669
Iteration 9/1000 | Loss: 0.00002509
Iteration 10/1000 | Loss: 0.00002375
Iteration 11/1000 | Loss: 0.00002267
Iteration 12/1000 | Loss: 0.00002192
Iteration 13/1000 | Loss: 0.00008103
Iteration 14/1000 | Loss: 0.00003894
Iteration 15/1000 | Loss: 0.00005437
Iteration 16/1000 | Loss: 0.00002347
Iteration 17/1000 | Loss: 0.00002139
Iteration 18/1000 | Loss: 0.00002087
Iteration 19/1000 | Loss: 0.00002047
Iteration 20/1000 | Loss: 0.00002011
Iteration 21/1000 | Loss: 0.00001982
Iteration 22/1000 | Loss: 0.00001948
Iteration 23/1000 | Loss: 0.00001904
Iteration 24/1000 | Loss: 0.00001841
Iteration 25/1000 | Loss: 0.00001799
Iteration 26/1000 | Loss: 0.00001775
Iteration 27/1000 | Loss: 0.00001754
Iteration 28/1000 | Loss: 0.00001740
Iteration 29/1000 | Loss: 0.00001737
Iteration 30/1000 | Loss: 0.00001736
Iteration 31/1000 | Loss: 0.00001736
Iteration 32/1000 | Loss: 0.00001735
Iteration 33/1000 | Loss: 0.00001734
Iteration 34/1000 | Loss: 0.00001733
Iteration 35/1000 | Loss: 0.00001733
Iteration 36/1000 | Loss: 0.00001732
Iteration 37/1000 | Loss: 0.00001731
Iteration 38/1000 | Loss: 0.00001728
Iteration 39/1000 | Loss: 0.00001727
Iteration 40/1000 | Loss: 0.00001724
Iteration 41/1000 | Loss: 0.00001712
Iteration 42/1000 | Loss: 0.00001706
Iteration 43/1000 | Loss: 0.00001702
Iteration 44/1000 | Loss: 0.00001701
Iteration 45/1000 | Loss: 0.00001699
Iteration 46/1000 | Loss: 0.00001699
Iteration 47/1000 | Loss: 0.00001699
Iteration 48/1000 | Loss: 0.00001698
Iteration 49/1000 | Loss: 0.00001698
Iteration 50/1000 | Loss: 0.00001697
Iteration 51/1000 | Loss: 0.00001697
Iteration 52/1000 | Loss: 0.00001696
Iteration 53/1000 | Loss: 0.00001694
Iteration 54/1000 | Loss: 0.00001694
Iteration 55/1000 | Loss: 0.00001692
Iteration 56/1000 | Loss: 0.00001692
Iteration 57/1000 | Loss: 0.00001691
Iteration 58/1000 | Loss: 0.00001691
Iteration 59/1000 | Loss: 0.00001691
Iteration 60/1000 | Loss: 0.00001691
Iteration 61/1000 | Loss: 0.00001690
Iteration 62/1000 | Loss: 0.00001690
Iteration 63/1000 | Loss: 0.00001690
Iteration 64/1000 | Loss: 0.00001690
Iteration 65/1000 | Loss: 0.00001690
Iteration 66/1000 | Loss: 0.00001689
Iteration 67/1000 | Loss: 0.00001689
Iteration 68/1000 | Loss: 0.00001689
Iteration 69/1000 | Loss: 0.00001689
Iteration 70/1000 | Loss: 0.00001688
Iteration 71/1000 | Loss: 0.00001688
Iteration 72/1000 | Loss: 0.00001688
Iteration 73/1000 | Loss: 0.00001688
Iteration 74/1000 | Loss: 0.00001688
Iteration 75/1000 | Loss: 0.00001688
Iteration 76/1000 | Loss: 0.00001688
Iteration 77/1000 | Loss: 0.00001688
Iteration 78/1000 | Loss: 0.00001688
Iteration 79/1000 | Loss: 0.00001688
Iteration 80/1000 | Loss: 0.00001687
Iteration 81/1000 | Loss: 0.00001687
Iteration 82/1000 | Loss: 0.00001687
Iteration 83/1000 | Loss: 0.00001687
Iteration 84/1000 | Loss: 0.00001687
Iteration 85/1000 | Loss: 0.00001687
Iteration 86/1000 | Loss: 0.00001686
Iteration 87/1000 | Loss: 0.00001686
Iteration 88/1000 | Loss: 0.00001686
Iteration 89/1000 | Loss: 0.00001686
Iteration 90/1000 | Loss: 0.00001686
Iteration 91/1000 | Loss: 0.00001686
Iteration 92/1000 | Loss: 0.00001685
Iteration 93/1000 | Loss: 0.00001685
Iteration 94/1000 | Loss: 0.00001685
Iteration 95/1000 | Loss: 0.00001685
Iteration 96/1000 | Loss: 0.00001685
Iteration 97/1000 | Loss: 0.00001685
Iteration 98/1000 | Loss: 0.00001685
Iteration 99/1000 | Loss: 0.00001685
Iteration 100/1000 | Loss: 0.00001685
Iteration 101/1000 | Loss: 0.00001684
Iteration 102/1000 | Loss: 0.00001684
Iteration 103/1000 | Loss: 0.00001684
Iteration 104/1000 | Loss: 0.00001684
Iteration 105/1000 | Loss: 0.00001684
Iteration 106/1000 | Loss: 0.00001684
Iteration 107/1000 | Loss: 0.00001684
Iteration 108/1000 | Loss: 0.00001684
Iteration 109/1000 | Loss: 0.00001684
Iteration 110/1000 | Loss: 0.00001684
Iteration 111/1000 | Loss: 0.00001684
Iteration 112/1000 | Loss: 0.00001684
Iteration 113/1000 | Loss: 0.00001684
Iteration 114/1000 | Loss: 0.00001684
Iteration 115/1000 | Loss: 0.00001684
Iteration 116/1000 | Loss: 0.00001683
Iteration 117/1000 | Loss: 0.00001683
Iteration 118/1000 | Loss: 0.00001683
Iteration 119/1000 | Loss: 0.00001683
Iteration 120/1000 | Loss: 0.00001683
Iteration 121/1000 | Loss: 0.00001683
Iteration 122/1000 | Loss: 0.00001683
Iteration 123/1000 | Loss: 0.00001683
Iteration 124/1000 | Loss: 0.00001683
Iteration 125/1000 | Loss: 0.00001683
Iteration 126/1000 | Loss: 0.00001683
Iteration 127/1000 | Loss: 0.00001683
Iteration 128/1000 | Loss: 0.00001682
Iteration 129/1000 | Loss: 0.00001682
Iteration 130/1000 | Loss: 0.00001682
Iteration 131/1000 | Loss: 0.00001682
Iteration 132/1000 | Loss: 0.00001682
Iteration 133/1000 | Loss: 0.00001682
Iteration 134/1000 | Loss: 0.00001682
Iteration 135/1000 | Loss: 0.00001682
Iteration 136/1000 | Loss: 0.00001682
Iteration 137/1000 | Loss: 0.00001682
Iteration 138/1000 | Loss: 0.00001682
Iteration 139/1000 | Loss: 0.00001682
Iteration 140/1000 | Loss: 0.00001682
Iteration 141/1000 | Loss: 0.00001681
Iteration 142/1000 | Loss: 0.00001681
Iteration 143/1000 | Loss: 0.00001681
Iteration 144/1000 | Loss: 0.00001681
Iteration 145/1000 | Loss: 0.00001681
Iteration 146/1000 | Loss: 0.00001681
Iteration 147/1000 | Loss: 0.00001681
Iteration 148/1000 | Loss: 0.00001681
Iteration 149/1000 | Loss: 0.00001681
Iteration 150/1000 | Loss: 0.00001681
Iteration 151/1000 | Loss: 0.00001681
Iteration 152/1000 | Loss: 0.00001681
Iteration 153/1000 | Loss: 0.00001681
Iteration 154/1000 | Loss: 0.00001681
Iteration 155/1000 | Loss: 0.00001681
Iteration 156/1000 | Loss: 0.00001681
Iteration 157/1000 | Loss: 0.00001681
Iteration 158/1000 | Loss: 0.00001681
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 158. Stopping optimization.
Last 5 losses: [1.6810499801067635e-05, 1.6810499801067635e-05, 1.6810499801067635e-05, 1.6810499801067635e-05, 1.6810499801067635e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6810499801067635e-05

Optimization complete. Final v2v error: 3.430842161178589 mm

Highest mean error: 4.916594505310059 mm for frame 36

Lowest mean error: 3.003540277481079 mm for frame 154

Saving results

Total time: 97.89320850372314
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_018/1067/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_018/1067.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_018/1067
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00423714
Iteration 2/25 | Loss: 0.00125609
Iteration 3/25 | Loss: 0.00119153
Iteration 4/25 | Loss: 0.00118394
Iteration 5/25 | Loss: 0.00118155
Iteration 6/25 | Loss: 0.00118124
Iteration 7/25 | Loss: 0.00118124
Iteration 8/25 | Loss: 0.00118124
Iteration 9/25 | Loss: 0.00118124
Iteration 10/25 | Loss: 0.00118124
Iteration 11/25 | Loss: 0.00118124
Iteration 12/25 | Loss: 0.00118124
Iteration 13/25 | Loss: 0.00118124
Iteration 14/25 | Loss: 0.00118124
Iteration 15/25 | Loss: 0.00118124
Iteration 16/25 | Loss: 0.00118124
Iteration 17/25 | Loss: 0.00118124
Iteration 18/25 | Loss: 0.00118124
Iteration 19/25 | Loss: 0.00118124
Iteration 20/25 | Loss: 0.00118124
Iteration 21/25 | Loss: 0.00118124
Iteration 22/25 | Loss: 0.00118124
Iteration 23/25 | Loss: 0.00118124
Iteration 24/25 | Loss: 0.00118124
Iteration 25/25 | Loss: 0.00118124

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36481559
Iteration 2/25 | Loss: 0.00087336
Iteration 3/25 | Loss: 0.00087336
Iteration 4/25 | Loss: 0.00087336
Iteration 5/25 | Loss: 0.00087336
Iteration 6/25 | Loss: 0.00087336
Iteration 7/25 | Loss: 0.00087336
Iteration 8/25 | Loss: 0.00087336
Iteration 9/25 | Loss: 0.00087336
Iteration 10/25 | Loss: 0.00087336
Iteration 11/25 | Loss: 0.00087336
Iteration 12/25 | Loss: 0.00087336
Iteration 13/25 | Loss: 0.00087336
Iteration 14/25 | Loss: 0.00087336
Iteration 15/25 | Loss: 0.00087336
Iteration 16/25 | Loss: 0.00087336
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0008733572321943939, 0.0008733572321943939, 0.0008733572321943939, 0.0008733572321943939, 0.0008733572321943939]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008733572321943939

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00087336
Iteration 2/1000 | Loss: 0.00003458
Iteration 3/1000 | Loss: 0.00001994
Iteration 4/1000 | Loss: 0.00001704
Iteration 5/1000 | Loss: 0.00001613
Iteration 6/1000 | Loss: 0.00001558
Iteration 7/1000 | Loss: 0.00001506
Iteration 8/1000 | Loss: 0.00001480
Iteration 9/1000 | Loss: 0.00001460
Iteration 10/1000 | Loss: 0.00001450
Iteration 11/1000 | Loss: 0.00001449
Iteration 12/1000 | Loss: 0.00001430
Iteration 13/1000 | Loss: 0.00001408
Iteration 14/1000 | Loss: 0.00001401
Iteration 15/1000 | Loss: 0.00001401
Iteration 16/1000 | Loss: 0.00001400
Iteration 17/1000 | Loss: 0.00001400
Iteration 18/1000 | Loss: 0.00001398
Iteration 19/1000 | Loss: 0.00001397
Iteration 20/1000 | Loss: 0.00001395
Iteration 21/1000 | Loss: 0.00001394
Iteration 22/1000 | Loss: 0.00001394
Iteration 23/1000 | Loss: 0.00001393
Iteration 24/1000 | Loss: 0.00001392
Iteration 25/1000 | Loss: 0.00001392
Iteration 26/1000 | Loss: 0.00001387
Iteration 27/1000 | Loss: 0.00001387
Iteration 28/1000 | Loss: 0.00001387
Iteration 29/1000 | Loss: 0.00001386
Iteration 30/1000 | Loss: 0.00001385
Iteration 31/1000 | Loss: 0.00001384
Iteration 32/1000 | Loss: 0.00001383
Iteration 33/1000 | Loss: 0.00001382
Iteration 34/1000 | Loss: 0.00001382
Iteration 35/1000 | Loss: 0.00001381
Iteration 36/1000 | Loss: 0.00001375
Iteration 37/1000 | Loss: 0.00001373
Iteration 38/1000 | Loss: 0.00001373
Iteration 39/1000 | Loss: 0.00001372
Iteration 40/1000 | Loss: 0.00001372
Iteration 41/1000 | Loss: 0.00001372
Iteration 42/1000 | Loss: 0.00001372
Iteration 43/1000 | Loss: 0.00001372
Iteration 44/1000 | Loss: 0.00001372
Iteration 45/1000 | Loss: 0.00001372
Iteration 46/1000 | Loss: 0.00001371
Iteration 47/1000 | Loss: 0.00001371
Iteration 48/1000 | Loss: 0.00001371
Iteration 49/1000 | Loss: 0.00001370
Iteration 50/1000 | Loss: 0.00001370
Iteration 51/1000 | Loss: 0.00001370
Iteration 52/1000 | Loss: 0.00001370
Iteration 53/1000 | Loss: 0.00001370
Iteration 54/1000 | Loss: 0.00001370
Iteration 55/1000 | Loss: 0.00001370
Iteration 56/1000 | Loss: 0.00001370
Iteration 57/1000 | Loss: 0.00001370
Iteration 58/1000 | Loss: 0.00001370
Iteration 59/1000 | Loss: 0.00001370
Iteration 60/1000 | Loss: 0.00001369
Iteration 61/1000 | Loss: 0.00001369
Iteration 62/1000 | Loss: 0.00001369
Iteration 63/1000 | Loss: 0.00001369
Iteration 64/1000 | Loss: 0.00001369
Iteration 65/1000 | Loss: 0.00001369
Iteration 66/1000 | Loss: 0.00001369
Iteration 67/1000 | Loss: 0.00001369
Iteration 68/1000 | Loss: 0.00001369
Iteration 69/1000 | Loss: 0.00001369
Iteration 70/1000 | Loss: 0.00001369
Iteration 71/1000 | Loss: 0.00001369
Iteration 72/1000 | Loss: 0.00001369
Iteration 73/1000 | Loss: 0.00001369
Iteration 74/1000 | Loss: 0.00001369
Iteration 75/1000 | Loss: 0.00001369
Iteration 76/1000 | Loss: 0.00001369
Iteration 77/1000 | Loss: 0.00001369
Iteration 78/1000 | Loss: 0.00001369
Iteration 79/1000 | Loss: 0.00001369
Iteration 80/1000 | Loss: 0.00001369
Iteration 81/1000 | Loss: 0.00001369
Iteration 82/1000 | Loss: 0.00001369
Iteration 83/1000 | Loss: 0.00001369
Iteration 84/1000 | Loss: 0.00001369
Iteration 85/1000 | Loss: 0.00001369
Iteration 86/1000 | Loss: 0.00001369
Iteration 87/1000 | Loss: 0.00001369
Iteration 88/1000 | Loss: 0.00001369
Iteration 89/1000 | Loss: 0.00001369
Iteration 90/1000 | Loss: 0.00001369
Iteration 91/1000 | Loss: 0.00001369
Iteration 92/1000 | Loss: 0.00001369
Iteration 93/1000 | Loss: 0.00001369
Iteration 94/1000 | Loss: 0.00001369
Iteration 95/1000 | Loss: 0.00001369
Iteration 96/1000 | Loss: 0.00001369
Iteration 97/1000 | Loss: 0.00001369
Iteration 98/1000 | Loss: 0.00001369
Iteration 99/1000 | Loss: 0.00001369
Iteration 100/1000 | Loss: 0.00001369
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 100. Stopping optimization.
Last 5 losses: [1.3692345419258345e-05, 1.3692345419258345e-05, 1.3692345419258345e-05, 1.3692345419258345e-05, 1.3692345419258345e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3692345419258345e-05

Optimization complete. Final v2v error: 3.211878776550293 mm

Highest mean error: 3.468925714492798 mm for frame 29

Lowest mean error: 3.0451231002807617 mm for frame 8

Saving results

Total time: 29.869382858276367
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_018/1057/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_018/1057.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_018/1057
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00710324
Iteration 2/25 | Loss: 0.00162125
Iteration 3/25 | Loss: 0.00136328
Iteration 4/25 | Loss: 0.00118998
Iteration 5/25 | Loss: 0.00117847
Iteration 6/25 | Loss: 0.00117752
Iteration 7/25 | Loss: 0.00117440
Iteration 8/25 | Loss: 0.00116801
Iteration 9/25 | Loss: 0.00115183
Iteration 10/25 | Loss: 0.00114709
Iteration 11/25 | Loss: 0.00114583
Iteration 12/25 | Loss: 0.00114484
Iteration 13/25 | Loss: 0.00114444
Iteration 14/25 | Loss: 0.00114429
Iteration 15/25 | Loss: 0.00114425
Iteration 16/25 | Loss: 0.00114425
Iteration 17/25 | Loss: 0.00114425
Iteration 18/25 | Loss: 0.00114425
Iteration 19/25 | Loss: 0.00114425
Iteration 20/25 | Loss: 0.00114424
Iteration 21/25 | Loss: 0.00114424
Iteration 22/25 | Loss: 0.00114424
Iteration 23/25 | Loss: 0.00114424
Iteration 24/25 | Loss: 0.00114424
Iteration 25/25 | Loss: 0.00114424

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.75801015
Iteration 2/25 | Loss: 0.00097022
Iteration 3/25 | Loss: 0.00092520
Iteration 4/25 | Loss: 0.00092520
Iteration 5/25 | Loss: 0.00092520
Iteration 6/25 | Loss: 0.00092520
Iteration 7/25 | Loss: 0.00092520
Iteration 8/25 | Loss: 0.00092519
Iteration 9/25 | Loss: 0.00092519
Iteration 10/25 | Loss: 0.00092519
Iteration 11/25 | Loss: 0.00092519
Iteration 12/25 | Loss: 0.00092519
Iteration 13/25 | Loss: 0.00092519
Iteration 14/25 | Loss: 0.00092519
Iteration 15/25 | Loss: 0.00092519
Iteration 16/25 | Loss: 0.00092519
Iteration 17/25 | Loss: 0.00092519
Iteration 18/25 | Loss: 0.00092519
Iteration 19/25 | Loss: 0.00092519
Iteration 20/25 | Loss: 0.00092519
Iteration 21/25 | Loss: 0.00092519
Iteration 22/25 | Loss: 0.00092519
Iteration 23/25 | Loss: 0.00092519
Iteration 24/25 | Loss: 0.00092519
Iteration 25/25 | Loss: 0.00092519

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00092519
Iteration 2/1000 | Loss: 0.00009317
Iteration 3/1000 | Loss: 0.00002736
Iteration 4/1000 | Loss: 0.00001700
Iteration 5/1000 | Loss: 0.00004151
Iteration 6/1000 | Loss: 0.00001746
Iteration 7/1000 | Loss: 0.00001539
Iteration 8/1000 | Loss: 0.00001503
Iteration 9/1000 | Loss: 0.00001473
Iteration 10/1000 | Loss: 0.00001436
Iteration 11/1000 | Loss: 0.00031808
Iteration 12/1000 | Loss: 0.00002319
Iteration 13/1000 | Loss: 0.00002571
Iteration 14/1000 | Loss: 0.00001582
Iteration 15/1000 | Loss: 0.00002636
Iteration 16/1000 | Loss: 0.00005403
Iteration 17/1000 | Loss: 0.00001355
Iteration 18/1000 | Loss: 0.00001592
Iteration 19/1000 | Loss: 0.00003197
Iteration 20/1000 | Loss: 0.00001312
Iteration 21/1000 | Loss: 0.00001300
Iteration 22/1000 | Loss: 0.00001285
Iteration 23/1000 | Loss: 0.00001279
Iteration 24/1000 | Loss: 0.00001279
Iteration 25/1000 | Loss: 0.00001245
Iteration 26/1000 | Loss: 0.00001244
Iteration 27/1000 | Loss: 0.00001244
Iteration 28/1000 | Loss: 0.00001243
Iteration 29/1000 | Loss: 0.00001240
Iteration 30/1000 | Loss: 0.00001239
Iteration 31/1000 | Loss: 0.00001239
Iteration 32/1000 | Loss: 0.00001239
Iteration 33/1000 | Loss: 0.00001238
Iteration 34/1000 | Loss: 0.00001238
Iteration 35/1000 | Loss: 0.00001238
Iteration 36/1000 | Loss: 0.00001237
Iteration 37/1000 | Loss: 0.00001235
Iteration 38/1000 | Loss: 0.00001235
Iteration 39/1000 | Loss: 0.00001232
Iteration 40/1000 | Loss: 0.00001229
Iteration 41/1000 | Loss: 0.00002325
Iteration 42/1000 | Loss: 0.00002273
Iteration 43/1000 | Loss: 0.00001218
Iteration 44/1000 | Loss: 0.00001212
Iteration 45/1000 | Loss: 0.00001210
Iteration 46/1000 | Loss: 0.00001210
Iteration 47/1000 | Loss: 0.00001210
Iteration 48/1000 | Loss: 0.00001210
Iteration 49/1000 | Loss: 0.00001210
Iteration 50/1000 | Loss: 0.00001210
Iteration 51/1000 | Loss: 0.00001210
Iteration 52/1000 | Loss: 0.00001209
Iteration 53/1000 | Loss: 0.00001209
Iteration 54/1000 | Loss: 0.00001209
Iteration 55/1000 | Loss: 0.00001209
Iteration 56/1000 | Loss: 0.00001209
Iteration 57/1000 | Loss: 0.00001208
Iteration 58/1000 | Loss: 0.00001208
Iteration 59/1000 | Loss: 0.00001208
Iteration 60/1000 | Loss: 0.00001208
Iteration 61/1000 | Loss: 0.00001207
Iteration 62/1000 | Loss: 0.00001207
Iteration 63/1000 | Loss: 0.00001207
Iteration 64/1000 | Loss: 0.00001207
Iteration 65/1000 | Loss: 0.00001207
Iteration 66/1000 | Loss: 0.00001207
Iteration 67/1000 | Loss: 0.00001207
Iteration 68/1000 | Loss: 0.00001207
Iteration 69/1000 | Loss: 0.00001207
Iteration 70/1000 | Loss: 0.00001207
Iteration 71/1000 | Loss: 0.00001207
Iteration 72/1000 | Loss: 0.00001207
Iteration 73/1000 | Loss: 0.00001207
Iteration 74/1000 | Loss: 0.00001206
Iteration 75/1000 | Loss: 0.00001205
Iteration 76/1000 | Loss: 0.00001203
Iteration 77/1000 | Loss: 0.00001202
Iteration 78/1000 | Loss: 0.00001202
Iteration 79/1000 | Loss: 0.00001202
Iteration 80/1000 | Loss: 0.00001202
Iteration 81/1000 | Loss: 0.00001201
Iteration 82/1000 | Loss: 0.00001201
Iteration 83/1000 | Loss: 0.00001200
Iteration 84/1000 | Loss: 0.00001200
Iteration 85/1000 | Loss: 0.00001198
Iteration 86/1000 | Loss: 0.00001198
Iteration 87/1000 | Loss: 0.00001198
Iteration 88/1000 | Loss: 0.00001198
Iteration 89/1000 | Loss: 0.00001198
Iteration 90/1000 | Loss: 0.00001198
Iteration 91/1000 | Loss: 0.00001198
Iteration 92/1000 | Loss: 0.00001198
Iteration 93/1000 | Loss: 0.00001198
Iteration 94/1000 | Loss: 0.00001198
Iteration 95/1000 | Loss: 0.00002202
Iteration 96/1000 | Loss: 0.00002855
Iteration 97/1000 | Loss: 0.00001358
Iteration 98/1000 | Loss: 0.00001416
Iteration 99/1000 | Loss: 0.00001198
Iteration 100/1000 | Loss: 0.00001197
Iteration 101/1000 | Loss: 0.00001197
Iteration 102/1000 | Loss: 0.00001197
Iteration 103/1000 | Loss: 0.00001197
Iteration 104/1000 | Loss: 0.00001196
Iteration 105/1000 | Loss: 0.00001196
Iteration 106/1000 | Loss: 0.00001196
Iteration 107/1000 | Loss: 0.00001196
Iteration 108/1000 | Loss: 0.00001196
Iteration 109/1000 | Loss: 0.00001196
Iteration 110/1000 | Loss: 0.00001196
Iteration 111/1000 | Loss: 0.00001196
Iteration 112/1000 | Loss: 0.00001196
Iteration 113/1000 | Loss: 0.00001195
Iteration 114/1000 | Loss: 0.00001195
Iteration 115/1000 | Loss: 0.00001195
Iteration 116/1000 | Loss: 0.00001195
Iteration 117/1000 | Loss: 0.00001195
Iteration 118/1000 | Loss: 0.00001195
Iteration 119/1000 | Loss: 0.00001195
Iteration 120/1000 | Loss: 0.00001195
Iteration 121/1000 | Loss: 0.00001195
Iteration 122/1000 | Loss: 0.00001195
Iteration 123/1000 | Loss: 0.00001195
Iteration 124/1000 | Loss: 0.00001195
Iteration 125/1000 | Loss: 0.00001195
Iteration 126/1000 | Loss: 0.00001194
Iteration 127/1000 | Loss: 0.00001194
Iteration 128/1000 | Loss: 0.00001194
Iteration 129/1000 | Loss: 0.00001194
Iteration 130/1000 | Loss: 0.00001194
Iteration 131/1000 | Loss: 0.00001194
Iteration 132/1000 | Loss: 0.00001194
Iteration 133/1000 | Loss: 0.00001194
Iteration 134/1000 | Loss: 0.00001194
Iteration 135/1000 | Loss: 0.00001194
Iteration 136/1000 | Loss: 0.00001194
Iteration 137/1000 | Loss: 0.00001538
Iteration 138/1000 | Loss: 0.00002456
Iteration 139/1000 | Loss: 0.00003573
Iteration 140/1000 | Loss: 0.00001195
Iteration 141/1000 | Loss: 0.00001192
Iteration 142/1000 | Loss: 0.00001191
Iteration 143/1000 | Loss: 0.00001191
Iteration 144/1000 | Loss: 0.00001191
Iteration 145/1000 | Loss: 0.00001191
Iteration 146/1000 | Loss: 0.00001191
Iteration 147/1000 | Loss: 0.00001191
Iteration 148/1000 | Loss: 0.00001191
Iteration 149/1000 | Loss: 0.00001190
Iteration 150/1000 | Loss: 0.00001190
Iteration 151/1000 | Loss: 0.00001190
Iteration 152/1000 | Loss: 0.00001190
Iteration 153/1000 | Loss: 0.00001190
Iteration 154/1000 | Loss: 0.00001190
Iteration 155/1000 | Loss: 0.00001190
Iteration 156/1000 | Loss: 0.00001190
Iteration 157/1000 | Loss: 0.00001190
Iteration 158/1000 | Loss: 0.00001190
Iteration 159/1000 | Loss: 0.00001190
Iteration 160/1000 | Loss: 0.00001190
Iteration 161/1000 | Loss: 0.00001190
Iteration 162/1000 | Loss: 0.00001190
Iteration 163/1000 | Loss: 0.00001190
Iteration 164/1000 | Loss: 0.00001190
Iteration 165/1000 | Loss: 0.00001190
Iteration 166/1000 | Loss: 0.00001190
Iteration 167/1000 | Loss: 0.00001190
Iteration 168/1000 | Loss: 0.00001190
Iteration 169/1000 | Loss: 0.00001190
Iteration 170/1000 | Loss: 0.00001190
Iteration 171/1000 | Loss: 0.00001190
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 171. Stopping optimization.
Last 5 losses: [1.1902865480806213e-05, 1.1902865480806213e-05, 1.1902865480806213e-05, 1.1902865480806213e-05, 1.1902865480806213e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1902865480806213e-05

Optimization complete. Final v2v error: 2.927079916000366 mm

Highest mean error: 3.6053123474121094 mm for frame 43

Lowest mean error: 2.595670461654663 mm for frame 131

Saving results

Total time: 90.2081732749939
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_018/1025/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_018/1025.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_018/1025
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00463926
Iteration 2/25 | Loss: 0.00129737
Iteration 3/25 | Loss: 0.00116606
Iteration 4/25 | Loss: 0.00113846
Iteration 5/25 | Loss: 0.00112925
Iteration 6/25 | Loss: 0.00112754
Iteration 7/25 | Loss: 0.00112754
Iteration 8/25 | Loss: 0.00112754
Iteration 9/25 | Loss: 0.00112754
Iteration 10/25 | Loss: 0.00112754
Iteration 11/25 | Loss: 0.00112754
Iteration 12/25 | Loss: 0.00112754
Iteration 13/25 | Loss: 0.00112754
Iteration 14/25 | Loss: 0.00112754
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.001127537339925766, 0.001127537339925766, 0.001127537339925766, 0.001127537339925766, 0.001127537339925766]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001127537339925766

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.31073427
Iteration 2/25 | Loss: 0.00091751
Iteration 3/25 | Loss: 0.00091751
Iteration 4/25 | Loss: 0.00091751
Iteration 5/25 | Loss: 0.00091751
Iteration 6/25 | Loss: 0.00091751
Iteration 7/25 | Loss: 0.00091751
Iteration 8/25 | Loss: 0.00091751
Iteration 9/25 | Loss: 0.00091751
Iteration 10/25 | Loss: 0.00091751
Iteration 11/25 | Loss: 0.00091751
Iteration 12/25 | Loss: 0.00091751
Iteration 13/25 | Loss: 0.00091751
Iteration 14/25 | Loss: 0.00091751
Iteration 15/25 | Loss: 0.00091751
Iteration 16/25 | Loss: 0.00091751
Iteration 17/25 | Loss: 0.00091751
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0009175108280032873, 0.0009175108280032873, 0.0009175108280032873, 0.0009175108280032873, 0.0009175108280032873]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009175108280032873

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00091751
Iteration 2/1000 | Loss: 0.00006109
Iteration 3/1000 | Loss: 0.00003852
Iteration 4/1000 | Loss: 0.00002795
Iteration 5/1000 | Loss: 0.00002631
Iteration 6/1000 | Loss: 0.00002516
Iteration 7/1000 | Loss: 0.00002424
Iteration 8/1000 | Loss: 0.00002351
Iteration 9/1000 | Loss: 0.00002312
Iteration 10/1000 | Loss: 0.00002292
Iteration 11/1000 | Loss: 0.00002263
Iteration 12/1000 | Loss: 0.00002256
Iteration 13/1000 | Loss: 0.00002256
Iteration 14/1000 | Loss: 0.00002238
Iteration 15/1000 | Loss: 0.00002232
Iteration 16/1000 | Loss: 0.00002228
Iteration 17/1000 | Loss: 0.00002228
Iteration 18/1000 | Loss: 0.00002220
Iteration 19/1000 | Loss: 0.00002214
Iteration 20/1000 | Loss: 0.00002203
Iteration 21/1000 | Loss: 0.00002200
Iteration 22/1000 | Loss: 0.00002197
Iteration 23/1000 | Loss: 0.00002196
Iteration 24/1000 | Loss: 0.00002196
Iteration 25/1000 | Loss: 0.00002194
Iteration 26/1000 | Loss: 0.00002193
Iteration 27/1000 | Loss: 0.00002192
Iteration 28/1000 | Loss: 0.00002191
Iteration 29/1000 | Loss: 0.00002191
Iteration 30/1000 | Loss: 0.00002191
Iteration 31/1000 | Loss: 0.00002190
Iteration 32/1000 | Loss: 0.00002190
Iteration 33/1000 | Loss: 0.00002190
Iteration 34/1000 | Loss: 0.00002189
Iteration 35/1000 | Loss: 0.00002189
Iteration 36/1000 | Loss: 0.00002187
Iteration 37/1000 | Loss: 0.00002187
Iteration 38/1000 | Loss: 0.00002187
Iteration 39/1000 | Loss: 0.00002187
Iteration 40/1000 | Loss: 0.00002187
Iteration 41/1000 | Loss: 0.00002187
Iteration 42/1000 | Loss: 0.00002186
Iteration 43/1000 | Loss: 0.00002185
Iteration 44/1000 | Loss: 0.00002185
Iteration 45/1000 | Loss: 0.00002185
Iteration 46/1000 | Loss: 0.00002184
Iteration 47/1000 | Loss: 0.00002184
Iteration 48/1000 | Loss: 0.00002184
Iteration 49/1000 | Loss: 0.00002184
Iteration 50/1000 | Loss: 0.00002184
Iteration 51/1000 | Loss: 0.00002184
Iteration 52/1000 | Loss: 0.00002184
Iteration 53/1000 | Loss: 0.00002184
Iteration 54/1000 | Loss: 0.00002184
Iteration 55/1000 | Loss: 0.00002184
Iteration 56/1000 | Loss: 0.00002184
Iteration 57/1000 | Loss: 0.00002183
Iteration 58/1000 | Loss: 0.00002181
Iteration 59/1000 | Loss: 0.00002181
Iteration 60/1000 | Loss: 0.00002181
Iteration 61/1000 | Loss: 0.00002181
Iteration 62/1000 | Loss: 0.00002181
Iteration 63/1000 | Loss: 0.00002181
Iteration 64/1000 | Loss: 0.00002181
Iteration 65/1000 | Loss: 0.00002181
Iteration 66/1000 | Loss: 0.00002181
Iteration 67/1000 | Loss: 0.00002180
Iteration 68/1000 | Loss: 0.00002180
Iteration 69/1000 | Loss: 0.00002180
Iteration 70/1000 | Loss: 0.00002180
Iteration 71/1000 | Loss: 0.00002179
Iteration 72/1000 | Loss: 0.00002179
Iteration 73/1000 | Loss: 0.00002178
Iteration 74/1000 | Loss: 0.00002178
Iteration 75/1000 | Loss: 0.00002178
Iteration 76/1000 | Loss: 0.00002178
Iteration 77/1000 | Loss: 0.00002178
Iteration 78/1000 | Loss: 0.00002178
Iteration 79/1000 | Loss: 0.00002178
Iteration 80/1000 | Loss: 0.00002178
Iteration 81/1000 | Loss: 0.00002178
Iteration 82/1000 | Loss: 0.00002178
Iteration 83/1000 | Loss: 0.00002178
Iteration 84/1000 | Loss: 0.00002178
Iteration 85/1000 | Loss: 0.00002177
Iteration 86/1000 | Loss: 0.00002176
Iteration 87/1000 | Loss: 0.00002176
Iteration 88/1000 | Loss: 0.00002176
Iteration 89/1000 | Loss: 0.00002175
Iteration 90/1000 | Loss: 0.00002175
Iteration 91/1000 | Loss: 0.00002175
Iteration 92/1000 | Loss: 0.00002175
Iteration 93/1000 | Loss: 0.00002175
Iteration 94/1000 | Loss: 0.00002175
Iteration 95/1000 | Loss: 0.00002175
Iteration 96/1000 | Loss: 0.00002175
Iteration 97/1000 | Loss: 0.00002175
Iteration 98/1000 | Loss: 0.00002174
Iteration 99/1000 | Loss: 0.00002174
Iteration 100/1000 | Loss: 0.00002174
Iteration 101/1000 | Loss: 0.00002174
Iteration 102/1000 | Loss: 0.00002174
Iteration 103/1000 | Loss: 0.00002173
Iteration 104/1000 | Loss: 0.00002173
Iteration 105/1000 | Loss: 0.00002173
Iteration 106/1000 | Loss: 0.00002173
Iteration 107/1000 | Loss: 0.00002173
Iteration 108/1000 | Loss: 0.00002173
Iteration 109/1000 | Loss: 0.00002173
Iteration 110/1000 | Loss: 0.00002173
Iteration 111/1000 | Loss: 0.00002173
Iteration 112/1000 | Loss: 0.00002173
Iteration 113/1000 | Loss: 0.00002172
Iteration 114/1000 | Loss: 0.00002172
Iteration 115/1000 | Loss: 0.00002172
Iteration 116/1000 | Loss: 0.00002172
Iteration 117/1000 | Loss: 0.00002172
Iteration 118/1000 | Loss: 0.00002171
Iteration 119/1000 | Loss: 0.00002171
Iteration 120/1000 | Loss: 0.00002171
Iteration 121/1000 | Loss: 0.00002171
Iteration 122/1000 | Loss: 0.00002170
Iteration 123/1000 | Loss: 0.00002170
Iteration 124/1000 | Loss: 0.00002170
Iteration 125/1000 | Loss: 0.00002170
Iteration 126/1000 | Loss: 0.00002170
Iteration 127/1000 | Loss: 0.00002170
Iteration 128/1000 | Loss: 0.00002170
Iteration 129/1000 | Loss: 0.00002170
Iteration 130/1000 | Loss: 0.00002170
Iteration 131/1000 | Loss: 0.00002170
Iteration 132/1000 | Loss: 0.00002170
Iteration 133/1000 | Loss: 0.00002170
Iteration 134/1000 | Loss: 0.00002170
Iteration 135/1000 | Loss: 0.00002170
Iteration 136/1000 | Loss: 0.00002170
Iteration 137/1000 | Loss: 0.00002170
Iteration 138/1000 | Loss: 0.00002170
Iteration 139/1000 | Loss: 0.00002170
Iteration 140/1000 | Loss: 0.00002170
Iteration 141/1000 | Loss: 0.00002170
Iteration 142/1000 | Loss: 0.00002170
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 142. Stopping optimization.
Last 5 losses: [2.169705658161547e-05, 2.169705658161547e-05, 2.169705658161547e-05, 2.169705658161547e-05, 2.169705658161547e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.169705658161547e-05

Optimization complete. Final v2v error: 3.78131365776062 mm

Highest mean error: 4.36513090133667 mm for frame 175

Lowest mean error: 3.4412217140197754 mm for frame 166

Saving results

Total time: 40.37163496017456
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_018/1055/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_018/1055.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_018/1055
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00349856
Iteration 2/25 | Loss: 0.00129811
Iteration 3/25 | Loss: 0.00114517
Iteration 4/25 | Loss: 0.00113024
Iteration 5/25 | Loss: 0.00112596
Iteration 6/25 | Loss: 0.00112539
Iteration 7/25 | Loss: 0.00112539
Iteration 8/25 | Loss: 0.00112539
Iteration 9/25 | Loss: 0.00112539
Iteration 10/25 | Loss: 0.00112539
Iteration 11/25 | Loss: 0.00112539
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011253943666815758, 0.0011253943666815758, 0.0011253943666815758, 0.0011253943666815758, 0.0011253943666815758]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011253943666815758

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33424413
Iteration 2/25 | Loss: 0.00082332
Iteration 3/25 | Loss: 0.00082332
Iteration 4/25 | Loss: 0.00082332
Iteration 5/25 | Loss: 0.00082332
Iteration 6/25 | Loss: 0.00082331
Iteration 7/25 | Loss: 0.00082331
Iteration 8/25 | Loss: 0.00082331
Iteration 9/25 | Loss: 0.00082331
Iteration 10/25 | Loss: 0.00082331
Iteration 11/25 | Loss: 0.00082331
Iteration 12/25 | Loss: 0.00082331
Iteration 13/25 | Loss: 0.00082331
Iteration 14/25 | Loss: 0.00082331
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0008233137195929885, 0.0008233137195929885, 0.0008233137195929885, 0.0008233137195929885, 0.0008233137195929885]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008233137195929885

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00082331
Iteration 2/1000 | Loss: 0.00003227
Iteration 3/1000 | Loss: 0.00002052
Iteration 4/1000 | Loss: 0.00001675
Iteration 5/1000 | Loss: 0.00001577
Iteration 6/1000 | Loss: 0.00001491
Iteration 7/1000 | Loss: 0.00001415
Iteration 8/1000 | Loss: 0.00001379
Iteration 9/1000 | Loss: 0.00001343
Iteration 10/1000 | Loss: 0.00001316
Iteration 11/1000 | Loss: 0.00001286
Iteration 12/1000 | Loss: 0.00001283
Iteration 13/1000 | Loss: 0.00001280
Iteration 14/1000 | Loss: 0.00001274
Iteration 15/1000 | Loss: 0.00001272
Iteration 16/1000 | Loss: 0.00001267
Iteration 17/1000 | Loss: 0.00001266
Iteration 18/1000 | Loss: 0.00001264
Iteration 19/1000 | Loss: 0.00001253
Iteration 20/1000 | Loss: 0.00001246
Iteration 21/1000 | Loss: 0.00001245
Iteration 22/1000 | Loss: 0.00001244
Iteration 23/1000 | Loss: 0.00001244
Iteration 24/1000 | Loss: 0.00001243
Iteration 25/1000 | Loss: 0.00001243
Iteration 26/1000 | Loss: 0.00001242
Iteration 27/1000 | Loss: 0.00001242
Iteration 28/1000 | Loss: 0.00001239
Iteration 29/1000 | Loss: 0.00001238
Iteration 30/1000 | Loss: 0.00001237
Iteration 31/1000 | Loss: 0.00001235
Iteration 32/1000 | Loss: 0.00001235
Iteration 33/1000 | Loss: 0.00001234
Iteration 34/1000 | Loss: 0.00001233
Iteration 35/1000 | Loss: 0.00001233
Iteration 36/1000 | Loss: 0.00001232
Iteration 37/1000 | Loss: 0.00001231
Iteration 38/1000 | Loss: 0.00001231
Iteration 39/1000 | Loss: 0.00001231
Iteration 40/1000 | Loss: 0.00001230
Iteration 41/1000 | Loss: 0.00001230
Iteration 42/1000 | Loss: 0.00001229
Iteration 43/1000 | Loss: 0.00001229
Iteration 44/1000 | Loss: 0.00001229
Iteration 45/1000 | Loss: 0.00001228
Iteration 46/1000 | Loss: 0.00001228
Iteration 47/1000 | Loss: 0.00001227
Iteration 48/1000 | Loss: 0.00001227
Iteration 49/1000 | Loss: 0.00001226
Iteration 50/1000 | Loss: 0.00001226
Iteration 51/1000 | Loss: 0.00001225
Iteration 52/1000 | Loss: 0.00001225
Iteration 53/1000 | Loss: 0.00001224
Iteration 54/1000 | Loss: 0.00001223
Iteration 55/1000 | Loss: 0.00001222
Iteration 56/1000 | Loss: 0.00001221
Iteration 57/1000 | Loss: 0.00001221
Iteration 58/1000 | Loss: 0.00001220
Iteration 59/1000 | Loss: 0.00001220
Iteration 60/1000 | Loss: 0.00001219
Iteration 61/1000 | Loss: 0.00001219
Iteration 62/1000 | Loss: 0.00001218
Iteration 63/1000 | Loss: 0.00001218
Iteration 64/1000 | Loss: 0.00001217
Iteration 65/1000 | Loss: 0.00001217
Iteration 66/1000 | Loss: 0.00001216
Iteration 67/1000 | Loss: 0.00001216
Iteration 68/1000 | Loss: 0.00001216
Iteration 69/1000 | Loss: 0.00001216
Iteration 70/1000 | Loss: 0.00001215
Iteration 71/1000 | Loss: 0.00001215
Iteration 72/1000 | Loss: 0.00001215
Iteration 73/1000 | Loss: 0.00001215
Iteration 74/1000 | Loss: 0.00001214
Iteration 75/1000 | Loss: 0.00001214
Iteration 76/1000 | Loss: 0.00001214
Iteration 77/1000 | Loss: 0.00001214
Iteration 78/1000 | Loss: 0.00001213
Iteration 79/1000 | Loss: 0.00001213
Iteration 80/1000 | Loss: 0.00001213
Iteration 81/1000 | Loss: 0.00001213
Iteration 82/1000 | Loss: 0.00001213
Iteration 83/1000 | Loss: 0.00001213
Iteration 84/1000 | Loss: 0.00001212
Iteration 85/1000 | Loss: 0.00001212
Iteration 86/1000 | Loss: 0.00001212
Iteration 87/1000 | Loss: 0.00001212
Iteration 88/1000 | Loss: 0.00001212
Iteration 89/1000 | Loss: 0.00001212
Iteration 90/1000 | Loss: 0.00001212
Iteration 91/1000 | Loss: 0.00001212
Iteration 92/1000 | Loss: 0.00001212
Iteration 93/1000 | Loss: 0.00001211
Iteration 94/1000 | Loss: 0.00001211
Iteration 95/1000 | Loss: 0.00001211
Iteration 96/1000 | Loss: 0.00001211
Iteration 97/1000 | Loss: 0.00001211
Iteration 98/1000 | Loss: 0.00001211
Iteration 99/1000 | Loss: 0.00001211
Iteration 100/1000 | Loss: 0.00001210
Iteration 101/1000 | Loss: 0.00001210
Iteration 102/1000 | Loss: 0.00001210
Iteration 103/1000 | Loss: 0.00001210
Iteration 104/1000 | Loss: 0.00001210
Iteration 105/1000 | Loss: 0.00001210
Iteration 106/1000 | Loss: 0.00001210
Iteration 107/1000 | Loss: 0.00001210
Iteration 108/1000 | Loss: 0.00001210
Iteration 109/1000 | Loss: 0.00001209
Iteration 110/1000 | Loss: 0.00001209
Iteration 111/1000 | Loss: 0.00001209
Iteration 112/1000 | Loss: 0.00001209
Iteration 113/1000 | Loss: 0.00001209
Iteration 114/1000 | Loss: 0.00001209
Iteration 115/1000 | Loss: 0.00001208
Iteration 116/1000 | Loss: 0.00001208
Iteration 117/1000 | Loss: 0.00001208
Iteration 118/1000 | Loss: 0.00001208
Iteration 119/1000 | Loss: 0.00001208
Iteration 120/1000 | Loss: 0.00001208
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 120. Stopping optimization.
Last 5 losses: [1.208415960718412e-05, 1.208415960718412e-05, 1.208415960718412e-05, 1.208415960718412e-05, 1.208415960718412e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.208415960718412e-05

Optimization complete. Final v2v error: 2.9446704387664795 mm

Highest mean error: 3.396285057067871 mm for frame 202

Lowest mean error: 2.590953826904297 mm for frame 220

Saving results

Total time: 40.60731220245361
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_018/1013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_018/1013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_018/1013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00589781
Iteration 2/25 | Loss: 0.00161791
Iteration 3/25 | Loss: 0.00135264
Iteration 4/25 | Loss: 0.00133161
Iteration 5/25 | Loss: 0.00132770
Iteration 6/25 | Loss: 0.00132730
Iteration 7/25 | Loss: 0.00132730
Iteration 8/25 | Loss: 0.00132730
Iteration 9/25 | Loss: 0.00132730
Iteration 10/25 | Loss: 0.00132730
Iteration 11/25 | Loss: 0.00132730
Iteration 12/25 | Loss: 0.00132730
Iteration 13/25 | Loss: 0.00132730
Iteration 14/25 | Loss: 0.00132730
Iteration 15/25 | Loss: 0.00132730
Iteration 16/25 | Loss: 0.00132730
Iteration 17/25 | Loss: 0.00132730
Iteration 18/25 | Loss: 0.00132730
Iteration 19/25 | Loss: 0.00132730
Iteration 20/25 | Loss: 0.00132730
Iteration 21/25 | Loss: 0.00132730
Iteration 22/25 | Loss: 0.00132730
Iteration 23/25 | Loss: 0.00132730
Iteration 24/25 | Loss: 0.00132730
Iteration 25/25 | Loss: 0.00132730

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33538783
Iteration 2/25 | Loss: 0.00092345
Iteration 3/25 | Loss: 0.00092343
Iteration 4/25 | Loss: 0.00092342
Iteration 5/25 | Loss: 0.00092342
Iteration 6/25 | Loss: 0.00092342
Iteration 7/25 | Loss: 0.00092342
Iteration 8/25 | Loss: 0.00092342
Iteration 9/25 | Loss: 0.00092342
Iteration 10/25 | Loss: 0.00092342
Iteration 11/25 | Loss: 0.00092342
Iteration 12/25 | Loss: 0.00092342
Iteration 13/25 | Loss: 0.00092342
Iteration 14/25 | Loss: 0.00092342
Iteration 15/25 | Loss: 0.00092342
Iteration 16/25 | Loss: 0.00092342
Iteration 17/25 | Loss: 0.00092342
Iteration 18/25 | Loss: 0.00092342
Iteration 19/25 | Loss: 0.00092342
Iteration 20/25 | Loss: 0.00092342
Iteration 21/25 | Loss: 0.00092342
Iteration 22/25 | Loss: 0.00092342
Iteration 23/25 | Loss: 0.00092342
Iteration 24/25 | Loss: 0.00092342
Iteration 25/25 | Loss: 0.00092342

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00092342
Iteration 2/1000 | Loss: 0.00005138
Iteration 3/1000 | Loss: 0.00003050
Iteration 4/1000 | Loss: 0.00002638
Iteration 5/1000 | Loss: 0.00002439
Iteration 6/1000 | Loss: 0.00002346
Iteration 7/1000 | Loss: 0.00002268
Iteration 8/1000 | Loss: 0.00002225
Iteration 9/1000 | Loss: 0.00002196
Iteration 10/1000 | Loss: 0.00002186
Iteration 11/1000 | Loss: 0.00002184
Iteration 12/1000 | Loss: 0.00002183
Iteration 13/1000 | Loss: 0.00002173
Iteration 14/1000 | Loss: 0.00002173
Iteration 15/1000 | Loss: 0.00002154
Iteration 16/1000 | Loss: 0.00002153
Iteration 17/1000 | Loss: 0.00002153
Iteration 18/1000 | Loss: 0.00002147
Iteration 19/1000 | Loss: 0.00002145
Iteration 20/1000 | Loss: 0.00002141
Iteration 21/1000 | Loss: 0.00002140
Iteration 22/1000 | Loss: 0.00002139
Iteration 23/1000 | Loss: 0.00002139
Iteration 24/1000 | Loss: 0.00002138
Iteration 25/1000 | Loss: 0.00002137
Iteration 26/1000 | Loss: 0.00002137
Iteration 27/1000 | Loss: 0.00002135
Iteration 28/1000 | Loss: 0.00002133
Iteration 29/1000 | Loss: 0.00002132
Iteration 30/1000 | Loss: 0.00002131
Iteration 31/1000 | Loss: 0.00002130
Iteration 32/1000 | Loss: 0.00002128
Iteration 33/1000 | Loss: 0.00002128
Iteration 34/1000 | Loss: 0.00002128
Iteration 35/1000 | Loss: 0.00002128
Iteration 36/1000 | Loss: 0.00002128
Iteration 37/1000 | Loss: 0.00002128
Iteration 38/1000 | Loss: 0.00002128
Iteration 39/1000 | Loss: 0.00002128
Iteration 40/1000 | Loss: 0.00002127
Iteration 41/1000 | Loss: 0.00002127
Iteration 42/1000 | Loss: 0.00002127
Iteration 43/1000 | Loss: 0.00002127
Iteration 44/1000 | Loss: 0.00002126
Iteration 45/1000 | Loss: 0.00002126
Iteration 46/1000 | Loss: 0.00002126
Iteration 47/1000 | Loss: 0.00002125
Iteration 48/1000 | Loss: 0.00002124
Iteration 49/1000 | Loss: 0.00002124
Iteration 50/1000 | Loss: 0.00002123
Iteration 51/1000 | Loss: 0.00002123
Iteration 52/1000 | Loss: 0.00002122
Iteration 53/1000 | Loss: 0.00002121
Iteration 54/1000 | Loss: 0.00002121
Iteration 55/1000 | Loss: 0.00002120
Iteration 56/1000 | Loss: 0.00002120
Iteration 57/1000 | Loss: 0.00002119
Iteration 58/1000 | Loss: 0.00002119
Iteration 59/1000 | Loss: 0.00002118
Iteration 60/1000 | Loss: 0.00002118
Iteration 61/1000 | Loss: 0.00002118
Iteration 62/1000 | Loss: 0.00002118
Iteration 63/1000 | Loss: 0.00002118
Iteration 64/1000 | Loss: 0.00002118
Iteration 65/1000 | Loss: 0.00002117
Iteration 66/1000 | Loss: 0.00002117
Iteration 67/1000 | Loss: 0.00002117
Iteration 68/1000 | Loss: 0.00002117
Iteration 69/1000 | Loss: 0.00002117
Iteration 70/1000 | Loss: 0.00002117
Iteration 71/1000 | Loss: 0.00002117
Iteration 72/1000 | Loss: 0.00002117
Iteration 73/1000 | Loss: 0.00002116
Iteration 74/1000 | Loss: 0.00002115
Iteration 75/1000 | Loss: 0.00002115
Iteration 76/1000 | Loss: 0.00002114
Iteration 77/1000 | Loss: 0.00002114
Iteration 78/1000 | Loss: 0.00002114
Iteration 79/1000 | Loss: 0.00002114
Iteration 80/1000 | Loss: 0.00002113
Iteration 81/1000 | Loss: 0.00002113
Iteration 82/1000 | Loss: 0.00002113
Iteration 83/1000 | Loss: 0.00002113
Iteration 84/1000 | Loss: 0.00002113
Iteration 85/1000 | Loss: 0.00002113
Iteration 86/1000 | Loss: 0.00002113
Iteration 87/1000 | Loss: 0.00002112
Iteration 88/1000 | Loss: 0.00002112
Iteration 89/1000 | Loss: 0.00002112
Iteration 90/1000 | Loss: 0.00002112
Iteration 91/1000 | Loss: 0.00002112
Iteration 92/1000 | Loss: 0.00002112
Iteration 93/1000 | Loss: 0.00002112
Iteration 94/1000 | Loss: 0.00002112
Iteration 95/1000 | Loss: 0.00002112
Iteration 96/1000 | Loss: 0.00002112
Iteration 97/1000 | Loss: 0.00002112
Iteration 98/1000 | Loss: 0.00002112
Iteration 99/1000 | Loss: 0.00002112
Iteration 100/1000 | Loss: 0.00002112
Iteration 101/1000 | Loss: 0.00002112
Iteration 102/1000 | Loss: 0.00002112
Iteration 103/1000 | Loss: 0.00002112
Iteration 104/1000 | Loss: 0.00002112
Iteration 105/1000 | Loss: 0.00002112
Iteration 106/1000 | Loss: 0.00002112
Iteration 107/1000 | Loss: 0.00002112
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 107. Stopping optimization.
Last 5 losses: [2.1115056370035745e-05, 2.1115056370035745e-05, 2.1115056370035745e-05, 2.1115056370035745e-05, 2.1115056370035745e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1115056370035745e-05

Optimization complete. Final v2v error: 3.908958673477173 mm

Highest mean error: 4.0400238037109375 mm for frame 84

Lowest mean error: 3.4633281230926514 mm for frame 4

Saving results

Total time: 34.01467728614807
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_018/1056/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_018/1056.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_018/1056
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00396030
Iteration 2/25 | Loss: 0.00126643
Iteration 3/25 | Loss: 0.00116903
Iteration 4/25 | Loss: 0.00115159
Iteration 5/25 | Loss: 0.00114576
Iteration 6/25 | Loss: 0.00114546
Iteration 7/25 | Loss: 0.00114546
Iteration 8/25 | Loss: 0.00114546
Iteration 9/25 | Loss: 0.00114546
Iteration 10/25 | Loss: 0.00114546
Iteration 11/25 | Loss: 0.00114546
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011454591294750571, 0.0011454591294750571, 0.0011454591294750571, 0.0011454591294750571, 0.0011454591294750571]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011454591294750571

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32060266
Iteration 2/25 | Loss: 0.00082335
Iteration 3/25 | Loss: 0.00082335
Iteration 4/25 | Loss: 0.00082334
Iteration 5/25 | Loss: 0.00082334
Iteration 6/25 | Loss: 0.00082334
Iteration 7/25 | Loss: 0.00082334
Iteration 8/25 | Loss: 0.00082334
Iteration 9/25 | Loss: 0.00082334
Iteration 10/25 | Loss: 0.00082334
Iteration 11/25 | Loss: 0.00082334
Iteration 12/25 | Loss: 0.00082334
Iteration 13/25 | Loss: 0.00082334
Iteration 14/25 | Loss: 0.00082334
Iteration 15/25 | Loss: 0.00082334
Iteration 16/25 | Loss: 0.00082334
Iteration 17/25 | Loss: 0.00082334
Iteration 18/25 | Loss: 0.00082334
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.000823342939838767, 0.000823342939838767, 0.000823342939838767, 0.000823342939838767, 0.000823342939838767]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000823342939838767

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00082334
Iteration 2/1000 | Loss: 0.00004509
Iteration 3/1000 | Loss: 0.00003347
Iteration 4/1000 | Loss: 0.00002906
Iteration 5/1000 | Loss: 0.00002725
Iteration 6/1000 | Loss: 0.00002558
Iteration 7/1000 | Loss: 0.00002460
Iteration 8/1000 | Loss: 0.00002398
Iteration 9/1000 | Loss: 0.00002349
Iteration 10/1000 | Loss: 0.00002304
Iteration 11/1000 | Loss: 0.00002281
Iteration 12/1000 | Loss: 0.00002262
Iteration 13/1000 | Loss: 0.00002235
Iteration 14/1000 | Loss: 0.00002213
Iteration 15/1000 | Loss: 0.00002198
Iteration 16/1000 | Loss: 0.00002190
Iteration 17/1000 | Loss: 0.00002187
Iteration 18/1000 | Loss: 0.00002182
Iteration 19/1000 | Loss: 0.00002181
Iteration 20/1000 | Loss: 0.00002179
Iteration 21/1000 | Loss: 0.00002179
Iteration 22/1000 | Loss: 0.00002178
Iteration 23/1000 | Loss: 0.00002177
Iteration 24/1000 | Loss: 0.00002177
Iteration 25/1000 | Loss: 0.00002177
Iteration 26/1000 | Loss: 0.00002176
Iteration 27/1000 | Loss: 0.00002176
Iteration 28/1000 | Loss: 0.00002175
Iteration 29/1000 | Loss: 0.00002175
Iteration 30/1000 | Loss: 0.00002174
Iteration 31/1000 | Loss: 0.00002173
Iteration 32/1000 | Loss: 0.00002172
Iteration 33/1000 | Loss: 0.00002172
Iteration 34/1000 | Loss: 0.00002171
Iteration 35/1000 | Loss: 0.00002171
Iteration 36/1000 | Loss: 0.00002170
Iteration 37/1000 | Loss: 0.00002170
Iteration 38/1000 | Loss: 0.00002170
Iteration 39/1000 | Loss: 0.00002169
Iteration 40/1000 | Loss: 0.00002169
Iteration 41/1000 | Loss: 0.00002169
Iteration 42/1000 | Loss: 0.00002169
Iteration 43/1000 | Loss: 0.00002168
Iteration 44/1000 | Loss: 0.00002168
Iteration 45/1000 | Loss: 0.00002167
Iteration 46/1000 | Loss: 0.00002167
Iteration 47/1000 | Loss: 0.00002167
Iteration 48/1000 | Loss: 0.00002167
Iteration 49/1000 | Loss: 0.00002166
Iteration 50/1000 | Loss: 0.00002166
Iteration 51/1000 | Loss: 0.00002166
Iteration 52/1000 | Loss: 0.00002165
Iteration 53/1000 | Loss: 0.00002165
Iteration 54/1000 | Loss: 0.00002165
Iteration 55/1000 | Loss: 0.00002165
Iteration 56/1000 | Loss: 0.00002164
Iteration 57/1000 | Loss: 0.00002164
Iteration 58/1000 | Loss: 0.00002164
Iteration 59/1000 | Loss: 0.00002164
Iteration 60/1000 | Loss: 0.00002163
Iteration 61/1000 | Loss: 0.00002163
Iteration 62/1000 | Loss: 0.00002163
Iteration 63/1000 | Loss: 0.00002163
Iteration 64/1000 | Loss: 0.00002163
Iteration 65/1000 | Loss: 0.00002162
Iteration 66/1000 | Loss: 0.00002162
Iteration 67/1000 | Loss: 0.00002162
Iteration 68/1000 | Loss: 0.00002162
Iteration 69/1000 | Loss: 0.00002162
Iteration 70/1000 | Loss: 0.00002162
Iteration 71/1000 | Loss: 0.00002162
Iteration 72/1000 | Loss: 0.00002162
Iteration 73/1000 | Loss: 0.00002161
Iteration 74/1000 | Loss: 0.00002161
Iteration 75/1000 | Loss: 0.00002161
Iteration 76/1000 | Loss: 0.00002161
Iteration 77/1000 | Loss: 0.00002161
Iteration 78/1000 | Loss: 0.00002161
Iteration 79/1000 | Loss: 0.00002161
Iteration 80/1000 | Loss: 0.00002161
Iteration 81/1000 | Loss: 0.00002161
Iteration 82/1000 | Loss: 0.00002161
Iteration 83/1000 | Loss: 0.00002161
Iteration 84/1000 | Loss: 0.00002160
Iteration 85/1000 | Loss: 0.00002160
Iteration 86/1000 | Loss: 0.00002160
Iteration 87/1000 | Loss: 0.00002160
Iteration 88/1000 | Loss: 0.00002159
Iteration 89/1000 | Loss: 0.00002159
Iteration 90/1000 | Loss: 0.00002159
Iteration 91/1000 | Loss: 0.00002159
Iteration 92/1000 | Loss: 0.00002159
Iteration 93/1000 | Loss: 0.00002158
Iteration 94/1000 | Loss: 0.00002158
Iteration 95/1000 | Loss: 0.00002158
Iteration 96/1000 | Loss: 0.00002158
Iteration 97/1000 | Loss: 0.00002158
Iteration 98/1000 | Loss: 0.00002157
Iteration 99/1000 | Loss: 0.00002157
Iteration 100/1000 | Loss: 0.00002157
Iteration 101/1000 | Loss: 0.00002156
Iteration 102/1000 | Loss: 0.00002156
Iteration 103/1000 | Loss: 0.00002156
Iteration 104/1000 | Loss: 0.00002155
Iteration 105/1000 | Loss: 0.00002155
Iteration 106/1000 | Loss: 0.00002155
Iteration 107/1000 | Loss: 0.00002155
Iteration 108/1000 | Loss: 0.00002154
Iteration 109/1000 | Loss: 0.00002154
Iteration 110/1000 | Loss: 0.00002154
Iteration 111/1000 | Loss: 0.00002154
Iteration 112/1000 | Loss: 0.00002154
Iteration 113/1000 | Loss: 0.00002154
Iteration 114/1000 | Loss: 0.00002154
Iteration 115/1000 | Loss: 0.00002153
Iteration 116/1000 | Loss: 0.00002153
Iteration 117/1000 | Loss: 0.00002153
Iteration 118/1000 | Loss: 0.00002153
Iteration 119/1000 | Loss: 0.00002153
Iteration 120/1000 | Loss: 0.00002153
Iteration 121/1000 | Loss: 0.00002153
Iteration 122/1000 | Loss: 0.00002152
Iteration 123/1000 | Loss: 0.00002152
Iteration 124/1000 | Loss: 0.00002152
Iteration 125/1000 | Loss: 0.00002152
Iteration 126/1000 | Loss: 0.00002152
Iteration 127/1000 | Loss: 0.00002151
Iteration 128/1000 | Loss: 0.00002151
Iteration 129/1000 | Loss: 0.00002151
Iteration 130/1000 | Loss: 0.00002151
Iteration 131/1000 | Loss: 0.00002150
Iteration 132/1000 | Loss: 0.00002150
Iteration 133/1000 | Loss: 0.00002150
Iteration 134/1000 | Loss: 0.00002149
Iteration 135/1000 | Loss: 0.00002149
Iteration 136/1000 | Loss: 0.00002149
Iteration 137/1000 | Loss: 0.00002149
Iteration 138/1000 | Loss: 0.00002149
Iteration 139/1000 | Loss: 0.00002149
Iteration 140/1000 | Loss: 0.00002149
Iteration 141/1000 | Loss: 0.00002149
Iteration 142/1000 | Loss: 0.00002149
Iteration 143/1000 | Loss: 0.00002149
Iteration 144/1000 | Loss: 0.00002149
Iteration 145/1000 | Loss: 0.00002149
Iteration 146/1000 | Loss: 0.00002149
Iteration 147/1000 | Loss: 0.00002149
Iteration 148/1000 | Loss: 0.00002149
Iteration 149/1000 | Loss: 0.00002148
Iteration 150/1000 | Loss: 0.00002148
Iteration 151/1000 | Loss: 0.00002148
Iteration 152/1000 | Loss: 0.00002148
Iteration 153/1000 | Loss: 0.00002148
Iteration 154/1000 | Loss: 0.00002148
Iteration 155/1000 | Loss: 0.00002148
Iteration 156/1000 | Loss: 0.00002148
Iteration 157/1000 | Loss: 0.00002148
Iteration 158/1000 | Loss: 0.00002148
Iteration 159/1000 | Loss: 0.00002148
Iteration 160/1000 | Loss: 0.00002148
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 160. Stopping optimization.
Last 5 losses: [2.147557279386092e-05, 2.147557279386092e-05, 2.147557279386092e-05, 2.147557279386092e-05, 2.147557279386092e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.147557279386092e-05

Optimization complete. Final v2v error: 3.748417854309082 mm

Highest mean error: 4.19615364074707 mm for frame 81

Lowest mean error: 3.2714271545410156 mm for frame 210

Saving results

Total time: 46.29618835449219
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_018/1099/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_018/1099.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_018/1099
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00987024
Iteration 2/25 | Loss: 0.00244940
Iteration 3/25 | Loss: 0.00175331
Iteration 4/25 | Loss: 0.00163872
Iteration 5/25 | Loss: 0.00173412
Iteration 6/25 | Loss: 0.00166200
Iteration 7/25 | Loss: 0.00147602
Iteration 8/25 | Loss: 0.00144751
Iteration 9/25 | Loss: 0.00132278
Iteration 10/25 | Loss: 0.00139114
Iteration 11/25 | Loss: 0.00132512
Iteration 12/25 | Loss: 0.00127520
Iteration 13/25 | Loss: 0.00124150
Iteration 14/25 | Loss: 0.00122484
Iteration 15/25 | Loss: 0.00121880
Iteration 16/25 | Loss: 0.00121514
Iteration 17/25 | Loss: 0.00122319
Iteration 18/25 | Loss: 0.00121338
Iteration 19/25 | Loss: 0.00121733
Iteration 20/25 | Loss: 0.00122076
Iteration 21/25 | Loss: 0.00121169
Iteration 22/25 | Loss: 0.00121331
Iteration 23/25 | Loss: 0.00121136
Iteration 24/25 | Loss: 0.00121321
Iteration 25/25 | Loss: 0.00121323

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.74272990
Iteration 2/25 | Loss: 0.00244794
Iteration 3/25 | Loss: 0.00153407
Iteration 4/25 | Loss: 0.00153405
Iteration 5/25 | Loss: 0.00153405
Iteration 6/25 | Loss: 0.00153405
Iteration 7/25 | Loss: 0.00153405
Iteration 8/25 | Loss: 0.00153405
Iteration 9/25 | Loss: 0.00153405
Iteration 10/25 | Loss: 0.00153405
Iteration 11/25 | Loss: 0.00153405
Iteration 12/25 | Loss: 0.00153405
Iteration 13/25 | Loss: 0.00153405
Iteration 14/25 | Loss: 0.00153405
Iteration 15/25 | Loss: 0.00153405
Iteration 16/25 | Loss: 0.00153405
Iteration 17/25 | Loss: 0.00153405
Iteration 18/25 | Loss: 0.00153405
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0015340478857979178, 0.0015340478857979178, 0.0015340478857979178, 0.0015340478857979178, 0.0015340478857979178]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015340478857979178

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00153405
Iteration 2/1000 | Loss: 0.00034459
Iteration 3/1000 | Loss: 0.00079985
Iteration 4/1000 | Loss: 0.00044821
Iteration 5/1000 | Loss: 0.00180557
Iteration 6/1000 | Loss: 0.00044030
Iteration 7/1000 | Loss: 0.00033233
Iteration 8/1000 | Loss: 0.00144052
Iteration 9/1000 | Loss: 0.00105659
Iteration 10/1000 | Loss: 0.00198889
Iteration 11/1000 | Loss: 0.00221883
Iteration 12/1000 | Loss: 0.00176671
Iteration 13/1000 | Loss: 0.00082918
Iteration 14/1000 | Loss: 0.00117100
Iteration 15/1000 | Loss: 0.00075150
Iteration 16/1000 | Loss: 0.00126309
Iteration 17/1000 | Loss: 0.00052975
Iteration 18/1000 | Loss: 0.00041828
Iteration 19/1000 | Loss: 0.00041202
Iteration 20/1000 | Loss: 0.00023240
Iteration 21/1000 | Loss: 0.00033733
Iteration 22/1000 | Loss: 0.00035850
Iteration 23/1000 | Loss: 0.00036512
Iteration 24/1000 | Loss: 0.00033684
Iteration 25/1000 | Loss: 0.00036747
Iteration 26/1000 | Loss: 0.00040264
Iteration 27/1000 | Loss: 0.00032032
Iteration 28/1000 | Loss: 0.00025924
Iteration 29/1000 | Loss: 0.00031623
Iteration 30/1000 | Loss: 0.00033510
Iteration 31/1000 | Loss: 0.00036825
Iteration 32/1000 | Loss: 0.00062318
Iteration 33/1000 | Loss: 0.00027754
Iteration 34/1000 | Loss: 0.00029953
Iteration 35/1000 | Loss: 0.00031520
Iteration 36/1000 | Loss: 0.00039270
Iteration 37/1000 | Loss: 0.00033213
Iteration 38/1000 | Loss: 0.00044270
Iteration 39/1000 | Loss: 0.00022411
Iteration 40/1000 | Loss: 0.00046557
Iteration 41/1000 | Loss: 0.00048843
Iteration 42/1000 | Loss: 0.00040884
Iteration 43/1000 | Loss: 0.00031021
Iteration 44/1000 | Loss: 0.00039391
Iteration 45/1000 | Loss: 0.00022670
Iteration 46/1000 | Loss: 0.00054533
Iteration 47/1000 | Loss: 0.00042145
Iteration 48/1000 | Loss: 0.00039767
Iteration 49/1000 | Loss: 0.00066062
Iteration 50/1000 | Loss: 0.00047237
Iteration 51/1000 | Loss: 0.00057615
Iteration 52/1000 | Loss: 0.00041389
Iteration 53/1000 | Loss: 0.00042805
Iteration 54/1000 | Loss: 0.00040665
Iteration 55/1000 | Loss: 0.00049660
Iteration 56/1000 | Loss: 0.00044315
Iteration 57/1000 | Loss: 0.00045073
Iteration 58/1000 | Loss: 0.00041120
Iteration 59/1000 | Loss: 0.00045395
Iteration 60/1000 | Loss: 0.00006168
Iteration 61/1000 | Loss: 0.00025850
Iteration 62/1000 | Loss: 0.00004697
Iteration 63/1000 | Loss: 0.00003469
Iteration 64/1000 | Loss: 0.00003012
Iteration 65/1000 | Loss: 0.00002736
Iteration 66/1000 | Loss: 0.00054397
Iteration 67/1000 | Loss: 0.00025241
Iteration 68/1000 | Loss: 0.00003000
Iteration 69/1000 | Loss: 0.00003102
Iteration 70/1000 | Loss: 0.00039682
Iteration 71/1000 | Loss: 0.00017671
Iteration 72/1000 | Loss: 0.00002768
Iteration 73/1000 | Loss: 0.00065273
Iteration 74/1000 | Loss: 0.00011937
Iteration 75/1000 | Loss: 0.00018888
Iteration 76/1000 | Loss: 0.00003897
Iteration 77/1000 | Loss: 0.00029833
Iteration 78/1000 | Loss: 0.00002634
Iteration 79/1000 | Loss: 0.00008980
Iteration 80/1000 | Loss: 0.00064539
Iteration 81/1000 | Loss: 0.00043979
Iteration 82/1000 | Loss: 0.00003851
Iteration 83/1000 | Loss: 0.00002770
Iteration 84/1000 | Loss: 0.00058366
Iteration 85/1000 | Loss: 0.00048202
Iteration 86/1000 | Loss: 0.00060616
Iteration 87/1000 | Loss: 0.00020733
Iteration 88/1000 | Loss: 0.00006200
Iteration 89/1000 | Loss: 0.00030208
Iteration 90/1000 | Loss: 0.00013186
Iteration 91/1000 | Loss: 0.00015086
Iteration 92/1000 | Loss: 0.00002314
Iteration 93/1000 | Loss: 0.00041505
Iteration 94/1000 | Loss: 0.00047156
Iteration 95/1000 | Loss: 0.00027147
Iteration 96/1000 | Loss: 0.00054188
Iteration 97/1000 | Loss: 0.00003563
Iteration 98/1000 | Loss: 0.00002750
Iteration 99/1000 | Loss: 0.00002295
Iteration 100/1000 | Loss: 0.00001881
Iteration 101/1000 | Loss: 0.00001624
Iteration 102/1000 | Loss: 0.00001466
Iteration 103/1000 | Loss: 0.00001371
Iteration 104/1000 | Loss: 0.00001314
Iteration 105/1000 | Loss: 0.00001289
Iteration 106/1000 | Loss: 0.00001270
Iteration 107/1000 | Loss: 0.00001269
Iteration 108/1000 | Loss: 0.00001268
Iteration 109/1000 | Loss: 0.00001266
Iteration 110/1000 | Loss: 0.00001262
Iteration 111/1000 | Loss: 0.00001252
Iteration 112/1000 | Loss: 0.00001243
Iteration 113/1000 | Loss: 0.00001228
Iteration 114/1000 | Loss: 0.00001226
Iteration 115/1000 | Loss: 0.00001226
Iteration 116/1000 | Loss: 0.00001224
Iteration 117/1000 | Loss: 0.00001223
Iteration 118/1000 | Loss: 0.00001223
Iteration 119/1000 | Loss: 0.00001222
Iteration 120/1000 | Loss: 0.00001222
Iteration 121/1000 | Loss: 0.00001221
Iteration 122/1000 | Loss: 0.00001220
Iteration 123/1000 | Loss: 0.00001220
Iteration 124/1000 | Loss: 0.00001220
Iteration 125/1000 | Loss: 0.00001217
Iteration 126/1000 | Loss: 0.00001210
Iteration 127/1000 | Loss: 0.00001210
Iteration 128/1000 | Loss: 0.00001209
Iteration 129/1000 | Loss: 0.00001209
Iteration 130/1000 | Loss: 0.00001208
Iteration 131/1000 | Loss: 0.00001207
Iteration 132/1000 | Loss: 0.00001206
Iteration 133/1000 | Loss: 0.00001206
Iteration 134/1000 | Loss: 0.00001205
Iteration 135/1000 | Loss: 0.00001205
Iteration 136/1000 | Loss: 0.00001204
Iteration 137/1000 | Loss: 0.00001202
Iteration 138/1000 | Loss: 0.00001202
Iteration 139/1000 | Loss: 0.00001202
Iteration 140/1000 | Loss: 0.00001201
Iteration 141/1000 | Loss: 0.00001201
Iteration 142/1000 | Loss: 0.00001201
Iteration 143/1000 | Loss: 0.00001201
Iteration 144/1000 | Loss: 0.00001201
Iteration 145/1000 | Loss: 0.00001201
Iteration 146/1000 | Loss: 0.00001201
Iteration 147/1000 | Loss: 0.00001201
Iteration 148/1000 | Loss: 0.00001201
Iteration 149/1000 | Loss: 0.00001201
Iteration 150/1000 | Loss: 0.00001201
Iteration 151/1000 | Loss: 0.00001200
Iteration 152/1000 | Loss: 0.00001200
Iteration 153/1000 | Loss: 0.00001200
Iteration 154/1000 | Loss: 0.00001200
Iteration 155/1000 | Loss: 0.00001200
Iteration 156/1000 | Loss: 0.00001200
Iteration 157/1000 | Loss: 0.00001200
Iteration 158/1000 | Loss: 0.00001200
Iteration 159/1000 | Loss: 0.00001200
Iteration 160/1000 | Loss: 0.00001200
Iteration 161/1000 | Loss: 0.00001200
Iteration 162/1000 | Loss: 0.00001200
Iteration 163/1000 | Loss: 0.00001200
Iteration 164/1000 | Loss: 0.00001200
Iteration 165/1000 | Loss: 0.00001199
Iteration 166/1000 | Loss: 0.00001199
Iteration 167/1000 | Loss: 0.00001199
Iteration 168/1000 | Loss: 0.00001199
Iteration 169/1000 | Loss: 0.00001199
Iteration 170/1000 | Loss: 0.00001199
Iteration 171/1000 | Loss: 0.00001199
Iteration 172/1000 | Loss: 0.00001199
Iteration 173/1000 | Loss: 0.00001199
Iteration 174/1000 | Loss: 0.00001199
Iteration 175/1000 | Loss: 0.00001199
Iteration 176/1000 | Loss: 0.00001199
Iteration 177/1000 | Loss: 0.00001199
Iteration 178/1000 | Loss: 0.00001199
Iteration 179/1000 | Loss: 0.00001199
Iteration 180/1000 | Loss: 0.00001199
Iteration 181/1000 | Loss: 0.00001199
Iteration 182/1000 | Loss: 0.00001198
Iteration 183/1000 | Loss: 0.00001198
Iteration 184/1000 | Loss: 0.00001198
Iteration 185/1000 | Loss: 0.00001198
Iteration 186/1000 | Loss: 0.00001198
Iteration 187/1000 | Loss: 0.00001198
Iteration 188/1000 | Loss: 0.00001198
Iteration 189/1000 | Loss: 0.00001198
Iteration 190/1000 | Loss: 0.00001198
Iteration 191/1000 | Loss: 0.00001198
Iteration 192/1000 | Loss: 0.00001198
Iteration 193/1000 | Loss: 0.00001198
Iteration 194/1000 | Loss: 0.00001198
Iteration 195/1000 | Loss: 0.00001198
Iteration 196/1000 | Loss: 0.00001198
Iteration 197/1000 | Loss: 0.00001198
Iteration 198/1000 | Loss: 0.00001198
Iteration 199/1000 | Loss: 0.00001198
Iteration 200/1000 | Loss: 0.00001198
Iteration 201/1000 | Loss: 0.00001198
Iteration 202/1000 | Loss: 0.00001198
Iteration 203/1000 | Loss: 0.00001198
Iteration 204/1000 | Loss: 0.00001198
Iteration 205/1000 | Loss: 0.00001198
Iteration 206/1000 | Loss: 0.00001198
Iteration 207/1000 | Loss: 0.00001198
Iteration 208/1000 | Loss: 0.00001198
Iteration 209/1000 | Loss: 0.00001198
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 209. Stopping optimization.
Last 5 losses: [1.1975908819295e-05, 1.1975908819295e-05, 1.1975908819295e-05, 1.1975908819295e-05, 1.1975908819295e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1975908819295e-05

Optimization complete. Final v2v error: 2.954068183898926 mm

Highest mean error: 4.2381486892700195 mm for frame 57

Lowest mean error: 2.5738184452056885 mm for frame 90

Saving results

Total time: 203.99096393585205
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_018/1083/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_018/1083.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_018/1083
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00908759
Iteration 2/25 | Loss: 0.00155820
Iteration 3/25 | Loss: 0.00132708
Iteration 4/25 | Loss: 0.00131581
Iteration 5/25 | Loss: 0.00131540
Iteration 6/25 | Loss: 0.00129650
Iteration 7/25 | Loss: 0.00127949
Iteration 8/25 | Loss: 0.00127070
Iteration 9/25 | Loss: 0.00126523
Iteration 10/25 | Loss: 0.00126218
Iteration 11/25 | Loss: 0.00126023
Iteration 12/25 | Loss: 0.00125840
Iteration 13/25 | Loss: 0.00125443
Iteration 14/25 | Loss: 0.00125296
Iteration 15/25 | Loss: 0.00125145
Iteration 16/25 | Loss: 0.00125085
Iteration 17/25 | Loss: 0.00125066
Iteration 18/25 | Loss: 0.00125058
Iteration 19/25 | Loss: 0.00125058
Iteration 20/25 | Loss: 0.00125058
Iteration 21/25 | Loss: 0.00125058
Iteration 22/25 | Loss: 0.00125058
Iteration 23/25 | Loss: 0.00125058
Iteration 24/25 | Loss: 0.00125058
Iteration 25/25 | Loss: 0.00125057

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.80817854
Iteration 2/25 | Loss: 0.00086887
Iteration 3/25 | Loss: 0.00086887
Iteration 4/25 | Loss: 0.00086887
Iteration 5/25 | Loss: 0.00086887
Iteration 6/25 | Loss: 0.00086887
Iteration 7/25 | Loss: 0.00086887
Iteration 8/25 | Loss: 0.00086887
Iteration 9/25 | Loss: 0.00086887
Iteration 10/25 | Loss: 0.00086887
Iteration 11/25 | Loss: 0.00086887
Iteration 12/25 | Loss: 0.00086887
Iteration 13/25 | Loss: 0.00086887
Iteration 14/25 | Loss: 0.00086887
Iteration 15/25 | Loss: 0.00086887
Iteration 16/25 | Loss: 0.00086887
Iteration 17/25 | Loss: 0.00086887
Iteration 18/25 | Loss: 0.00086887
Iteration 19/25 | Loss: 0.00086887
Iteration 20/25 | Loss: 0.00086887
Iteration 21/25 | Loss: 0.00086887
Iteration 22/25 | Loss: 0.00086887
Iteration 23/25 | Loss: 0.00086887
Iteration 24/25 | Loss: 0.00086887
Iteration 25/25 | Loss: 0.00086887

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00086887
Iteration 2/1000 | Loss: 0.00007124
Iteration 3/1000 | Loss: 0.00005700
Iteration 4/1000 | Loss: 0.00005041
Iteration 5/1000 | Loss: 0.00004696
Iteration 6/1000 | Loss: 0.00004478
Iteration 7/1000 | Loss: 0.00036066
Iteration 8/1000 | Loss: 0.00016841
Iteration 9/1000 | Loss: 0.00015296
Iteration 10/1000 | Loss: 0.00006644
Iteration 11/1000 | Loss: 0.00004139
Iteration 12/1000 | Loss: 0.00014831
Iteration 13/1000 | Loss: 0.00023951
Iteration 14/1000 | Loss: 0.00010359
Iteration 15/1000 | Loss: 0.00069276
Iteration 16/1000 | Loss: 0.00019952
Iteration 17/1000 | Loss: 0.00005232
Iteration 18/1000 | Loss: 0.00003824
Iteration 19/1000 | Loss: 0.00135043
Iteration 20/1000 | Loss: 0.00013705
Iteration 21/1000 | Loss: 0.00013304
Iteration 22/1000 | Loss: 0.00008489
Iteration 23/1000 | Loss: 0.00015606
Iteration 24/1000 | Loss: 0.00204942
Iteration 25/1000 | Loss: 0.00018036
Iteration 26/1000 | Loss: 0.00007233
Iteration 27/1000 | Loss: 0.00005345
Iteration 28/1000 | Loss: 0.00003622
Iteration 29/1000 | Loss: 0.00157376
Iteration 30/1000 | Loss: 0.00245448
Iteration 31/1000 | Loss: 0.00040578
Iteration 32/1000 | Loss: 0.00035776
Iteration 33/1000 | Loss: 0.00094842
Iteration 34/1000 | Loss: 0.00034710
Iteration 35/1000 | Loss: 0.00058532
Iteration 36/1000 | Loss: 0.00033159
Iteration 37/1000 | Loss: 0.00034125
Iteration 38/1000 | Loss: 0.00023354
Iteration 39/1000 | Loss: 0.00047545
Iteration 40/1000 | Loss: 0.00004610
Iteration 41/1000 | Loss: 0.00024652
Iteration 42/1000 | Loss: 0.00026921
Iteration 43/1000 | Loss: 0.00061407
Iteration 44/1000 | Loss: 0.00060448
Iteration 45/1000 | Loss: 0.00041724
Iteration 46/1000 | Loss: 0.00045201
Iteration 47/1000 | Loss: 0.00004300
Iteration 48/1000 | Loss: 0.00003522
Iteration 49/1000 | Loss: 0.00003263
Iteration 50/1000 | Loss: 0.00003125
Iteration 51/1000 | Loss: 0.00003029
Iteration 52/1000 | Loss: 0.00002906
Iteration 53/1000 | Loss: 0.00002787
Iteration 54/1000 | Loss: 0.00002680
Iteration 55/1000 | Loss: 0.00002631
Iteration 56/1000 | Loss: 0.00002581
Iteration 57/1000 | Loss: 0.00002556
Iteration 58/1000 | Loss: 0.00002524
Iteration 59/1000 | Loss: 0.00038543
Iteration 60/1000 | Loss: 0.00072497
Iteration 61/1000 | Loss: 0.00031725
Iteration 62/1000 | Loss: 0.00002771
Iteration 63/1000 | Loss: 0.00002524
Iteration 64/1000 | Loss: 0.00002416
Iteration 65/1000 | Loss: 0.00002369
Iteration 66/1000 | Loss: 0.00002328
Iteration 67/1000 | Loss: 0.00002302
Iteration 68/1000 | Loss: 0.00039591
Iteration 69/1000 | Loss: 0.00003982
Iteration 70/1000 | Loss: 0.00003055
Iteration 71/1000 | Loss: 0.00002799
Iteration 72/1000 | Loss: 0.00002717
Iteration 73/1000 | Loss: 0.00002631
Iteration 74/1000 | Loss: 0.00002568
Iteration 75/1000 | Loss: 0.00002495
Iteration 76/1000 | Loss: 0.00002446
Iteration 77/1000 | Loss: 0.00002433
Iteration 78/1000 | Loss: 0.00002421
Iteration 79/1000 | Loss: 0.00002402
Iteration 80/1000 | Loss: 0.00002389
Iteration 81/1000 | Loss: 0.00002389
Iteration 82/1000 | Loss: 0.00002388
Iteration 83/1000 | Loss: 0.00002387
Iteration 84/1000 | Loss: 0.00002387
Iteration 85/1000 | Loss: 0.00002386
Iteration 86/1000 | Loss: 0.00002386
Iteration 87/1000 | Loss: 0.00002386
Iteration 88/1000 | Loss: 0.00002386
Iteration 89/1000 | Loss: 0.00002385
Iteration 90/1000 | Loss: 0.00002385
Iteration 91/1000 | Loss: 0.00002385
Iteration 92/1000 | Loss: 0.00002384
Iteration 93/1000 | Loss: 0.00002384
Iteration 94/1000 | Loss: 0.00002383
Iteration 95/1000 | Loss: 0.00002383
Iteration 96/1000 | Loss: 0.00002383
Iteration 97/1000 | Loss: 0.00002383
Iteration 98/1000 | Loss: 0.00002383
Iteration 99/1000 | Loss: 0.00002382
Iteration 100/1000 | Loss: 0.00002382
Iteration 101/1000 | Loss: 0.00002381
Iteration 102/1000 | Loss: 0.00002381
Iteration 103/1000 | Loss: 0.00002381
Iteration 104/1000 | Loss: 0.00002380
Iteration 105/1000 | Loss: 0.00002380
Iteration 106/1000 | Loss: 0.00002380
Iteration 107/1000 | Loss: 0.00002380
Iteration 108/1000 | Loss: 0.00002380
Iteration 109/1000 | Loss: 0.00002380
Iteration 110/1000 | Loss: 0.00002379
Iteration 111/1000 | Loss: 0.00002379
Iteration 112/1000 | Loss: 0.00002379
Iteration 113/1000 | Loss: 0.00002379
Iteration 114/1000 | Loss: 0.00002379
Iteration 115/1000 | Loss: 0.00002379
Iteration 116/1000 | Loss: 0.00002379
Iteration 117/1000 | Loss: 0.00002378
Iteration 118/1000 | Loss: 0.00002378
Iteration 119/1000 | Loss: 0.00002378
Iteration 120/1000 | Loss: 0.00002378
Iteration 121/1000 | Loss: 0.00002378
Iteration 122/1000 | Loss: 0.00002378
Iteration 123/1000 | Loss: 0.00002378
Iteration 124/1000 | Loss: 0.00002378
Iteration 125/1000 | Loss: 0.00002378
Iteration 126/1000 | Loss: 0.00002378
Iteration 127/1000 | Loss: 0.00002378
Iteration 128/1000 | Loss: 0.00002378
Iteration 129/1000 | Loss: 0.00002378
Iteration 130/1000 | Loss: 0.00002377
Iteration 131/1000 | Loss: 0.00002377
Iteration 132/1000 | Loss: 0.00002377
Iteration 133/1000 | Loss: 0.00002377
Iteration 134/1000 | Loss: 0.00002377
Iteration 135/1000 | Loss: 0.00002377
Iteration 136/1000 | Loss: 0.00002377
Iteration 137/1000 | Loss: 0.00002377
Iteration 138/1000 | Loss: 0.00002377
Iteration 139/1000 | Loss: 0.00002377
Iteration 140/1000 | Loss: 0.00002377
Iteration 141/1000 | Loss: 0.00002377
Iteration 142/1000 | Loss: 0.00002377
Iteration 143/1000 | Loss: 0.00002377
Iteration 144/1000 | Loss: 0.00002377
Iteration 145/1000 | Loss: 0.00002376
Iteration 146/1000 | Loss: 0.00002376
Iteration 147/1000 | Loss: 0.00002376
Iteration 148/1000 | Loss: 0.00002376
Iteration 149/1000 | Loss: 0.00002376
Iteration 150/1000 | Loss: 0.00002376
Iteration 151/1000 | Loss: 0.00002376
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 151. Stopping optimization.
Last 5 losses: [2.3764561774441972e-05, 2.3764561774441972e-05, 2.3764561774441972e-05, 2.3764561774441972e-05, 2.3764561774441972e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3764561774441972e-05

Optimization complete. Final v2v error: 3.6791837215423584 mm

Highest mean error: 13.922151565551758 mm for frame 141

Lowest mean error: 2.977395534515381 mm for frame 239

Saving results

Total time: 170.09062337875366
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_018/1078/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_018/1078.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_018/1078
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00894985
Iteration 2/25 | Loss: 0.00121175
Iteration 3/25 | Loss: 0.00115678
Iteration 4/25 | Loss: 0.00114536
Iteration 5/25 | Loss: 0.00114240
Iteration 6/25 | Loss: 0.00114240
Iteration 7/25 | Loss: 0.00114240
Iteration 8/25 | Loss: 0.00114240
Iteration 9/25 | Loss: 0.00114240
Iteration 10/25 | Loss: 0.00114240
Iteration 11/25 | Loss: 0.00114240
Iteration 12/25 | Loss: 0.00114240
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0011423976393416524, 0.0011423976393416524, 0.0011423976393416524, 0.0011423976393416524, 0.0011423976393416524]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011423976393416524

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44171226
Iteration 2/25 | Loss: 0.00077713
Iteration 3/25 | Loss: 0.00077713
Iteration 4/25 | Loss: 0.00077713
Iteration 5/25 | Loss: 0.00077713
Iteration 6/25 | Loss: 0.00077713
Iteration 7/25 | Loss: 0.00077713
Iteration 8/25 | Loss: 0.00077712
Iteration 9/25 | Loss: 0.00077712
Iteration 10/25 | Loss: 0.00077712
Iteration 11/25 | Loss: 0.00077712
Iteration 12/25 | Loss: 0.00077712
Iteration 13/25 | Loss: 0.00077712
Iteration 14/25 | Loss: 0.00077712
Iteration 15/25 | Loss: 0.00077712
Iteration 16/25 | Loss: 0.00077712
Iteration 17/25 | Loss: 0.00077712
Iteration 18/25 | Loss: 0.00077712
Iteration 19/25 | Loss: 0.00077712
Iteration 20/25 | Loss: 0.00077712
Iteration 21/25 | Loss: 0.00077712
Iteration 22/25 | Loss: 0.00077712
Iteration 23/25 | Loss: 0.00077712
Iteration 24/25 | Loss: 0.00077712
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.000777124019805342, 0.000777124019805342, 0.000777124019805342, 0.000777124019805342, 0.000777124019805342]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000777124019805342

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00077712
Iteration 2/1000 | Loss: 0.00002300
Iteration 3/1000 | Loss: 0.00001685
Iteration 4/1000 | Loss: 0.00001544
Iteration 5/1000 | Loss: 0.00001460
Iteration 6/1000 | Loss: 0.00001404
Iteration 7/1000 | Loss: 0.00001371
Iteration 8/1000 | Loss: 0.00001371
Iteration 9/1000 | Loss: 0.00001335
Iteration 10/1000 | Loss: 0.00001305
Iteration 11/1000 | Loss: 0.00001292
Iteration 12/1000 | Loss: 0.00001281
Iteration 13/1000 | Loss: 0.00001281
Iteration 14/1000 | Loss: 0.00001280
Iteration 15/1000 | Loss: 0.00001280
Iteration 16/1000 | Loss: 0.00001279
Iteration 17/1000 | Loss: 0.00001277
Iteration 18/1000 | Loss: 0.00001275
Iteration 19/1000 | Loss: 0.00001266
Iteration 20/1000 | Loss: 0.00001265
Iteration 21/1000 | Loss: 0.00001262
Iteration 22/1000 | Loss: 0.00001261
Iteration 23/1000 | Loss: 0.00001260
Iteration 24/1000 | Loss: 0.00001259
Iteration 25/1000 | Loss: 0.00001254
Iteration 26/1000 | Loss: 0.00001248
Iteration 27/1000 | Loss: 0.00001248
Iteration 28/1000 | Loss: 0.00001248
Iteration 29/1000 | Loss: 0.00001247
Iteration 30/1000 | Loss: 0.00001245
Iteration 31/1000 | Loss: 0.00001245
Iteration 32/1000 | Loss: 0.00001245
Iteration 33/1000 | Loss: 0.00001243
Iteration 34/1000 | Loss: 0.00001242
Iteration 35/1000 | Loss: 0.00001241
Iteration 36/1000 | Loss: 0.00001239
Iteration 37/1000 | Loss: 0.00001236
Iteration 38/1000 | Loss: 0.00001236
Iteration 39/1000 | Loss: 0.00001236
Iteration 40/1000 | Loss: 0.00001235
Iteration 41/1000 | Loss: 0.00001235
Iteration 42/1000 | Loss: 0.00001235
Iteration 43/1000 | Loss: 0.00001235
Iteration 44/1000 | Loss: 0.00001235
Iteration 45/1000 | Loss: 0.00001235
Iteration 46/1000 | Loss: 0.00001235
Iteration 47/1000 | Loss: 0.00001234
Iteration 48/1000 | Loss: 0.00001234
Iteration 49/1000 | Loss: 0.00001234
Iteration 50/1000 | Loss: 0.00001233
Iteration 51/1000 | Loss: 0.00001232
Iteration 52/1000 | Loss: 0.00001232
Iteration 53/1000 | Loss: 0.00001231
Iteration 54/1000 | Loss: 0.00001231
Iteration 55/1000 | Loss: 0.00001231
Iteration 56/1000 | Loss: 0.00001231
Iteration 57/1000 | Loss: 0.00001231
Iteration 58/1000 | Loss: 0.00001230
Iteration 59/1000 | Loss: 0.00001230
Iteration 60/1000 | Loss: 0.00001230
Iteration 61/1000 | Loss: 0.00001230
Iteration 62/1000 | Loss: 0.00001229
Iteration 63/1000 | Loss: 0.00001229
Iteration 64/1000 | Loss: 0.00001228
Iteration 65/1000 | Loss: 0.00001228
Iteration 66/1000 | Loss: 0.00001228
Iteration 67/1000 | Loss: 0.00001227
Iteration 68/1000 | Loss: 0.00001227
Iteration 69/1000 | Loss: 0.00001227
Iteration 70/1000 | Loss: 0.00001227
Iteration 71/1000 | Loss: 0.00001226
Iteration 72/1000 | Loss: 0.00001226
Iteration 73/1000 | Loss: 0.00001225
Iteration 74/1000 | Loss: 0.00001224
Iteration 75/1000 | Loss: 0.00001224
Iteration 76/1000 | Loss: 0.00001224
Iteration 77/1000 | Loss: 0.00001224
Iteration 78/1000 | Loss: 0.00001223
Iteration 79/1000 | Loss: 0.00001223
Iteration 80/1000 | Loss: 0.00001223
Iteration 81/1000 | Loss: 0.00001223
Iteration 82/1000 | Loss: 0.00001223
Iteration 83/1000 | Loss: 0.00001223
Iteration 84/1000 | Loss: 0.00001223
Iteration 85/1000 | Loss: 0.00001223
Iteration 86/1000 | Loss: 0.00001223
Iteration 87/1000 | Loss: 0.00001223
Iteration 88/1000 | Loss: 0.00001222
Iteration 89/1000 | Loss: 0.00001221
Iteration 90/1000 | Loss: 0.00001220
Iteration 91/1000 | Loss: 0.00001219
Iteration 92/1000 | Loss: 0.00001219
Iteration 93/1000 | Loss: 0.00001218
Iteration 94/1000 | Loss: 0.00001218
Iteration 95/1000 | Loss: 0.00001217
Iteration 96/1000 | Loss: 0.00001217
Iteration 97/1000 | Loss: 0.00001217
Iteration 98/1000 | Loss: 0.00001217
Iteration 99/1000 | Loss: 0.00001217
Iteration 100/1000 | Loss: 0.00001217
Iteration 101/1000 | Loss: 0.00001217
Iteration 102/1000 | Loss: 0.00001216
Iteration 103/1000 | Loss: 0.00001216
Iteration 104/1000 | Loss: 0.00001216
Iteration 105/1000 | Loss: 0.00001216
Iteration 106/1000 | Loss: 0.00001216
Iteration 107/1000 | Loss: 0.00001216
Iteration 108/1000 | Loss: 0.00001216
Iteration 109/1000 | Loss: 0.00001216
Iteration 110/1000 | Loss: 0.00001216
Iteration 111/1000 | Loss: 0.00001215
Iteration 112/1000 | Loss: 0.00001214
Iteration 113/1000 | Loss: 0.00001214
Iteration 114/1000 | Loss: 0.00001213
Iteration 115/1000 | Loss: 0.00001213
Iteration 116/1000 | Loss: 0.00001213
Iteration 117/1000 | Loss: 0.00001213
Iteration 118/1000 | Loss: 0.00001213
Iteration 119/1000 | Loss: 0.00001213
Iteration 120/1000 | Loss: 0.00001213
Iteration 121/1000 | Loss: 0.00001213
Iteration 122/1000 | Loss: 0.00001212
Iteration 123/1000 | Loss: 0.00001212
Iteration 124/1000 | Loss: 0.00001211
Iteration 125/1000 | Loss: 0.00001211
Iteration 126/1000 | Loss: 0.00001211
Iteration 127/1000 | Loss: 0.00001211
Iteration 128/1000 | Loss: 0.00001210
Iteration 129/1000 | Loss: 0.00001210
Iteration 130/1000 | Loss: 0.00001210
Iteration 131/1000 | Loss: 0.00001210
Iteration 132/1000 | Loss: 0.00001209
Iteration 133/1000 | Loss: 0.00001209
Iteration 134/1000 | Loss: 0.00001209
Iteration 135/1000 | Loss: 0.00001208
Iteration 136/1000 | Loss: 0.00001208
Iteration 137/1000 | Loss: 0.00001208
Iteration 138/1000 | Loss: 0.00001207
Iteration 139/1000 | Loss: 0.00001207
Iteration 140/1000 | Loss: 0.00001207
Iteration 141/1000 | Loss: 0.00001207
Iteration 142/1000 | Loss: 0.00001206
Iteration 143/1000 | Loss: 0.00001206
Iteration 144/1000 | Loss: 0.00001206
Iteration 145/1000 | Loss: 0.00001206
Iteration 146/1000 | Loss: 0.00001205
Iteration 147/1000 | Loss: 0.00001205
Iteration 148/1000 | Loss: 0.00001205
Iteration 149/1000 | Loss: 0.00001205
Iteration 150/1000 | Loss: 0.00001205
Iteration 151/1000 | Loss: 0.00001205
Iteration 152/1000 | Loss: 0.00001205
Iteration 153/1000 | Loss: 0.00001205
Iteration 154/1000 | Loss: 0.00001205
Iteration 155/1000 | Loss: 0.00001205
Iteration 156/1000 | Loss: 0.00001205
Iteration 157/1000 | Loss: 0.00001205
Iteration 158/1000 | Loss: 0.00001205
Iteration 159/1000 | Loss: 0.00001205
Iteration 160/1000 | Loss: 0.00001204
Iteration 161/1000 | Loss: 0.00001204
Iteration 162/1000 | Loss: 0.00001204
Iteration 163/1000 | Loss: 0.00001204
Iteration 164/1000 | Loss: 0.00001204
Iteration 165/1000 | Loss: 0.00001204
Iteration 166/1000 | Loss: 0.00001204
Iteration 167/1000 | Loss: 0.00001204
Iteration 168/1000 | Loss: 0.00001204
Iteration 169/1000 | Loss: 0.00001204
Iteration 170/1000 | Loss: 0.00001204
Iteration 171/1000 | Loss: 0.00001204
Iteration 172/1000 | Loss: 0.00001204
Iteration 173/1000 | Loss: 0.00001204
Iteration 174/1000 | Loss: 0.00001204
Iteration 175/1000 | Loss: 0.00001204
Iteration 176/1000 | Loss: 0.00001204
Iteration 177/1000 | Loss: 0.00001204
Iteration 178/1000 | Loss: 0.00001204
Iteration 179/1000 | Loss: 0.00001204
Iteration 180/1000 | Loss: 0.00001204
Iteration 181/1000 | Loss: 0.00001204
Iteration 182/1000 | Loss: 0.00001203
Iteration 183/1000 | Loss: 0.00001203
Iteration 184/1000 | Loss: 0.00001203
Iteration 185/1000 | Loss: 0.00001203
Iteration 186/1000 | Loss: 0.00001203
Iteration 187/1000 | Loss: 0.00001203
Iteration 188/1000 | Loss: 0.00001203
Iteration 189/1000 | Loss: 0.00001203
Iteration 190/1000 | Loss: 0.00001203
Iteration 191/1000 | Loss: 0.00001203
Iteration 192/1000 | Loss: 0.00001203
Iteration 193/1000 | Loss: 0.00001203
Iteration 194/1000 | Loss: 0.00001203
Iteration 195/1000 | Loss: 0.00001203
Iteration 196/1000 | Loss: 0.00001203
Iteration 197/1000 | Loss: 0.00001203
Iteration 198/1000 | Loss: 0.00001203
Iteration 199/1000 | Loss: 0.00001202
Iteration 200/1000 | Loss: 0.00001202
Iteration 201/1000 | Loss: 0.00001202
Iteration 202/1000 | Loss: 0.00001202
Iteration 203/1000 | Loss: 0.00001202
Iteration 204/1000 | Loss: 0.00001202
Iteration 205/1000 | Loss: 0.00001202
Iteration 206/1000 | Loss: 0.00001202
Iteration 207/1000 | Loss: 0.00001202
Iteration 208/1000 | Loss: 0.00001202
Iteration 209/1000 | Loss: 0.00001202
Iteration 210/1000 | Loss: 0.00001202
Iteration 211/1000 | Loss: 0.00001202
Iteration 212/1000 | Loss: 0.00001202
Iteration 213/1000 | Loss: 0.00001202
Iteration 214/1000 | Loss: 0.00001202
Iteration 215/1000 | Loss: 0.00001202
Iteration 216/1000 | Loss: 0.00001202
Iteration 217/1000 | Loss: 0.00001201
Iteration 218/1000 | Loss: 0.00001201
Iteration 219/1000 | Loss: 0.00001201
Iteration 220/1000 | Loss: 0.00001201
Iteration 221/1000 | Loss: 0.00001201
Iteration 222/1000 | Loss: 0.00001201
Iteration 223/1000 | Loss: 0.00001201
Iteration 224/1000 | Loss: 0.00001201
Iteration 225/1000 | Loss: 0.00001201
Iteration 226/1000 | Loss: 0.00001201
Iteration 227/1000 | Loss: 0.00001201
Iteration 228/1000 | Loss: 0.00001201
Iteration 229/1000 | Loss: 0.00001201
Iteration 230/1000 | Loss: 0.00001201
Iteration 231/1000 | Loss: 0.00001201
Iteration 232/1000 | Loss: 0.00001201
Iteration 233/1000 | Loss: 0.00001201
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 233. Stopping optimization.
Last 5 losses: [1.201070517709013e-05, 1.201070517709013e-05, 1.201070517709013e-05, 1.201070517709013e-05, 1.201070517709013e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.201070517709013e-05

Optimization complete. Final v2v error: 2.956359386444092 mm

Highest mean error: 3.418577194213867 mm for frame 146

Lowest mean error: 2.870135545730591 mm for frame 204

Saving results

Total time: 47.362170457839966
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_018/1028/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_018/1028.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_018/1028
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00452016
Iteration 2/25 | Loss: 0.00126298
Iteration 3/25 | Loss: 0.00117983
Iteration 4/25 | Loss: 0.00116817
Iteration 5/25 | Loss: 0.00116499
Iteration 6/25 | Loss: 0.00116482
Iteration 7/25 | Loss: 0.00116482
Iteration 8/25 | Loss: 0.00116482
Iteration 9/25 | Loss: 0.00116482
Iteration 10/25 | Loss: 0.00116482
Iteration 11/25 | Loss: 0.00116482
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011648216750472784, 0.0011648216750472784, 0.0011648216750472784, 0.0011648216750472784, 0.0011648216750472784]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011648216750472784

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.67652178
Iteration 2/25 | Loss: 0.00082674
Iteration 3/25 | Loss: 0.00082674
Iteration 4/25 | Loss: 0.00082674
Iteration 5/25 | Loss: 0.00082674
Iteration 6/25 | Loss: 0.00082674
Iteration 7/25 | Loss: 0.00082674
Iteration 8/25 | Loss: 0.00082674
Iteration 9/25 | Loss: 0.00082674
Iteration 10/25 | Loss: 0.00082674
Iteration 11/25 | Loss: 0.00082674
Iteration 12/25 | Loss: 0.00082674
Iteration 13/25 | Loss: 0.00082674
Iteration 14/25 | Loss: 0.00082674
Iteration 15/25 | Loss: 0.00082674
Iteration 16/25 | Loss: 0.00082674
Iteration 17/25 | Loss: 0.00082674
Iteration 18/25 | Loss: 0.00082674
Iteration 19/25 | Loss: 0.00082674
Iteration 20/25 | Loss: 0.00082674
Iteration 21/25 | Loss: 0.00082674
Iteration 22/25 | Loss: 0.00082674
Iteration 23/25 | Loss: 0.00082674
Iteration 24/25 | Loss: 0.00082674
Iteration 25/25 | Loss: 0.00082674

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00082674
Iteration 2/1000 | Loss: 0.00002542
Iteration 3/1000 | Loss: 0.00001822
Iteration 4/1000 | Loss: 0.00001612
Iteration 5/1000 | Loss: 0.00001535
Iteration 6/1000 | Loss: 0.00001475
Iteration 7/1000 | Loss: 0.00001439
Iteration 8/1000 | Loss: 0.00001405
Iteration 9/1000 | Loss: 0.00001378
Iteration 10/1000 | Loss: 0.00001352
Iteration 11/1000 | Loss: 0.00001333
Iteration 12/1000 | Loss: 0.00001320
Iteration 13/1000 | Loss: 0.00001319
Iteration 14/1000 | Loss: 0.00001318
Iteration 15/1000 | Loss: 0.00001316
Iteration 16/1000 | Loss: 0.00001313
Iteration 17/1000 | Loss: 0.00001306
Iteration 18/1000 | Loss: 0.00001305
Iteration 19/1000 | Loss: 0.00001304
Iteration 20/1000 | Loss: 0.00001304
Iteration 21/1000 | Loss: 0.00001302
Iteration 22/1000 | Loss: 0.00001302
Iteration 23/1000 | Loss: 0.00001301
Iteration 24/1000 | Loss: 0.00001299
Iteration 25/1000 | Loss: 0.00001298
Iteration 26/1000 | Loss: 0.00001292
Iteration 27/1000 | Loss: 0.00001291
Iteration 28/1000 | Loss: 0.00001289
Iteration 29/1000 | Loss: 0.00001287
Iteration 30/1000 | Loss: 0.00001287
Iteration 31/1000 | Loss: 0.00001286
Iteration 32/1000 | Loss: 0.00001285
Iteration 33/1000 | Loss: 0.00001285
Iteration 34/1000 | Loss: 0.00001284
Iteration 35/1000 | Loss: 0.00001284
Iteration 36/1000 | Loss: 0.00001284
Iteration 37/1000 | Loss: 0.00001283
Iteration 38/1000 | Loss: 0.00001283
Iteration 39/1000 | Loss: 0.00001281
Iteration 40/1000 | Loss: 0.00001281
Iteration 41/1000 | Loss: 0.00001280
Iteration 42/1000 | Loss: 0.00001280
Iteration 43/1000 | Loss: 0.00001279
Iteration 44/1000 | Loss: 0.00001279
Iteration 45/1000 | Loss: 0.00001279
Iteration 46/1000 | Loss: 0.00001278
Iteration 47/1000 | Loss: 0.00001278
Iteration 48/1000 | Loss: 0.00001278
Iteration 49/1000 | Loss: 0.00001277
Iteration 50/1000 | Loss: 0.00001277
Iteration 51/1000 | Loss: 0.00001277
Iteration 52/1000 | Loss: 0.00001277
Iteration 53/1000 | Loss: 0.00001276
Iteration 54/1000 | Loss: 0.00001276
Iteration 55/1000 | Loss: 0.00001276
Iteration 56/1000 | Loss: 0.00001275
Iteration 57/1000 | Loss: 0.00001275
Iteration 58/1000 | Loss: 0.00001275
Iteration 59/1000 | Loss: 0.00001275
Iteration 60/1000 | Loss: 0.00001275
Iteration 61/1000 | Loss: 0.00001274
Iteration 62/1000 | Loss: 0.00001274
Iteration 63/1000 | Loss: 0.00001273
Iteration 64/1000 | Loss: 0.00001273
Iteration 65/1000 | Loss: 0.00001273
Iteration 66/1000 | Loss: 0.00001272
Iteration 67/1000 | Loss: 0.00001272
Iteration 68/1000 | Loss: 0.00001272
Iteration 69/1000 | Loss: 0.00001271
Iteration 70/1000 | Loss: 0.00001271
Iteration 71/1000 | Loss: 0.00001270
Iteration 72/1000 | Loss: 0.00001270
Iteration 73/1000 | Loss: 0.00001269
Iteration 74/1000 | Loss: 0.00001269
Iteration 75/1000 | Loss: 0.00001269
Iteration 76/1000 | Loss: 0.00001269
Iteration 77/1000 | Loss: 0.00001268
Iteration 78/1000 | Loss: 0.00001268
Iteration 79/1000 | Loss: 0.00001268
Iteration 80/1000 | Loss: 0.00001268
Iteration 81/1000 | Loss: 0.00001268
Iteration 82/1000 | Loss: 0.00001268
Iteration 83/1000 | Loss: 0.00001268
Iteration 84/1000 | Loss: 0.00001268
Iteration 85/1000 | Loss: 0.00001268
Iteration 86/1000 | Loss: 0.00001267
Iteration 87/1000 | Loss: 0.00001267
Iteration 88/1000 | Loss: 0.00001267
Iteration 89/1000 | Loss: 0.00001267
Iteration 90/1000 | Loss: 0.00001267
Iteration 91/1000 | Loss: 0.00001267
Iteration 92/1000 | Loss: 0.00001266
Iteration 93/1000 | Loss: 0.00001266
Iteration 94/1000 | Loss: 0.00001266
Iteration 95/1000 | Loss: 0.00001265
Iteration 96/1000 | Loss: 0.00001265
Iteration 97/1000 | Loss: 0.00001265
Iteration 98/1000 | Loss: 0.00001265
Iteration 99/1000 | Loss: 0.00001265
Iteration 100/1000 | Loss: 0.00001265
Iteration 101/1000 | Loss: 0.00001265
Iteration 102/1000 | Loss: 0.00001264
Iteration 103/1000 | Loss: 0.00001264
Iteration 104/1000 | Loss: 0.00001264
Iteration 105/1000 | Loss: 0.00001264
Iteration 106/1000 | Loss: 0.00001264
Iteration 107/1000 | Loss: 0.00001264
Iteration 108/1000 | Loss: 0.00001264
Iteration 109/1000 | Loss: 0.00001264
Iteration 110/1000 | Loss: 0.00001264
Iteration 111/1000 | Loss: 0.00001263
Iteration 112/1000 | Loss: 0.00001263
Iteration 113/1000 | Loss: 0.00001263
Iteration 114/1000 | Loss: 0.00001263
Iteration 115/1000 | Loss: 0.00001263
Iteration 116/1000 | Loss: 0.00001263
Iteration 117/1000 | Loss: 0.00001263
Iteration 118/1000 | Loss: 0.00001263
Iteration 119/1000 | Loss: 0.00001263
Iteration 120/1000 | Loss: 0.00001263
Iteration 121/1000 | Loss: 0.00001263
Iteration 122/1000 | Loss: 0.00001263
Iteration 123/1000 | Loss: 0.00001262
Iteration 124/1000 | Loss: 0.00001262
Iteration 125/1000 | Loss: 0.00001262
Iteration 126/1000 | Loss: 0.00001261
Iteration 127/1000 | Loss: 0.00001261
Iteration 128/1000 | Loss: 0.00001261
Iteration 129/1000 | Loss: 0.00001261
Iteration 130/1000 | Loss: 0.00001261
Iteration 131/1000 | Loss: 0.00001261
Iteration 132/1000 | Loss: 0.00001261
Iteration 133/1000 | Loss: 0.00001261
Iteration 134/1000 | Loss: 0.00001261
Iteration 135/1000 | Loss: 0.00001261
Iteration 136/1000 | Loss: 0.00001260
Iteration 137/1000 | Loss: 0.00001260
Iteration 138/1000 | Loss: 0.00001260
Iteration 139/1000 | Loss: 0.00001260
Iteration 140/1000 | Loss: 0.00001260
Iteration 141/1000 | Loss: 0.00001260
Iteration 142/1000 | Loss: 0.00001260
Iteration 143/1000 | Loss: 0.00001260
Iteration 144/1000 | Loss: 0.00001260
Iteration 145/1000 | Loss: 0.00001260
Iteration 146/1000 | Loss: 0.00001260
Iteration 147/1000 | Loss: 0.00001260
Iteration 148/1000 | Loss: 0.00001260
Iteration 149/1000 | Loss: 0.00001260
Iteration 150/1000 | Loss: 0.00001260
Iteration 151/1000 | Loss: 0.00001259
Iteration 152/1000 | Loss: 0.00001259
Iteration 153/1000 | Loss: 0.00001259
Iteration 154/1000 | Loss: 0.00001259
Iteration 155/1000 | Loss: 0.00001259
Iteration 156/1000 | Loss: 0.00001259
Iteration 157/1000 | Loss: 0.00001259
Iteration 158/1000 | Loss: 0.00001259
Iteration 159/1000 | Loss: 0.00001259
Iteration 160/1000 | Loss: 0.00001259
Iteration 161/1000 | Loss: 0.00001259
Iteration 162/1000 | Loss: 0.00001259
Iteration 163/1000 | Loss: 0.00001259
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 163. Stopping optimization.
Last 5 losses: [1.2593177416420076e-05, 1.2593177416420076e-05, 1.2593177416420076e-05, 1.2593177416420076e-05, 1.2593177416420076e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2593177416420076e-05

Optimization complete. Final v2v error: 3.0063371658325195 mm

Highest mean error: 3.3882648944854736 mm for frame 87

Lowest mean error: 2.716233253479004 mm for frame 2

Saving results

Total time: 41.62042689323425
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_018/1071/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_018/1071.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_018/1071
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00886596
Iteration 2/25 | Loss: 0.00160998
Iteration 3/25 | Loss: 0.00129011
Iteration 4/25 | Loss: 0.00123732
Iteration 5/25 | Loss: 0.00122204
Iteration 6/25 | Loss: 0.00121866
Iteration 7/25 | Loss: 0.00121792
Iteration 8/25 | Loss: 0.00121792
Iteration 9/25 | Loss: 0.00121792
Iteration 10/25 | Loss: 0.00121792
Iteration 11/25 | Loss: 0.00121792
Iteration 12/25 | Loss: 0.00121792
Iteration 13/25 | Loss: 0.00121792
Iteration 14/25 | Loss: 0.00121792
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0012179160257801414, 0.0012179160257801414, 0.0012179160257801414, 0.0012179160257801414, 0.0012179160257801414]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012179160257801414

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.08069229
Iteration 2/25 | Loss: 0.00109714
Iteration 3/25 | Loss: 0.00109708
Iteration 4/25 | Loss: 0.00109708
Iteration 5/25 | Loss: 0.00109708
Iteration 6/25 | Loss: 0.00109708
Iteration 7/25 | Loss: 0.00109708
Iteration 8/25 | Loss: 0.00109708
Iteration 9/25 | Loss: 0.00109708
Iteration 10/25 | Loss: 0.00109708
Iteration 11/25 | Loss: 0.00109708
Iteration 12/25 | Loss: 0.00109708
Iteration 13/25 | Loss: 0.00109708
Iteration 14/25 | Loss: 0.00109708
Iteration 15/25 | Loss: 0.00109708
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0010970778530463576, 0.0010970778530463576, 0.0010970778530463576, 0.0010970778530463576, 0.0010970778530463576]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010970778530463576

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00109708
Iteration 2/1000 | Loss: 0.00007727
Iteration 3/1000 | Loss: 0.00004940
Iteration 4/1000 | Loss: 0.00003647
Iteration 5/1000 | Loss: 0.00003338
Iteration 6/1000 | Loss: 0.00003151
Iteration 7/1000 | Loss: 0.00003027
Iteration 8/1000 | Loss: 0.00002919
Iteration 9/1000 | Loss: 0.00002854
Iteration 10/1000 | Loss: 0.00002807
Iteration 11/1000 | Loss: 0.00002767
Iteration 12/1000 | Loss: 0.00002737
Iteration 13/1000 | Loss: 0.00002716
Iteration 14/1000 | Loss: 0.00002695
Iteration 15/1000 | Loss: 0.00002687
Iteration 16/1000 | Loss: 0.00002686
Iteration 17/1000 | Loss: 0.00002684
Iteration 18/1000 | Loss: 0.00002669
Iteration 19/1000 | Loss: 0.00002664
Iteration 20/1000 | Loss: 0.00002661
Iteration 21/1000 | Loss: 0.00002660
Iteration 22/1000 | Loss: 0.00002659
Iteration 23/1000 | Loss: 0.00002657
Iteration 24/1000 | Loss: 0.00002656
Iteration 25/1000 | Loss: 0.00002655
Iteration 26/1000 | Loss: 0.00002655
Iteration 27/1000 | Loss: 0.00002651
Iteration 28/1000 | Loss: 0.00002648
Iteration 29/1000 | Loss: 0.00002648
Iteration 30/1000 | Loss: 0.00002647
Iteration 31/1000 | Loss: 0.00002646
Iteration 32/1000 | Loss: 0.00002645
Iteration 33/1000 | Loss: 0.00002645
Iteration 34/1000 | Loss: 0.00002644
Iteration 35/1000 | Loss: 0.00002642
Iteration 36/1000 | Loss: 0.00002642
Iteration 37/1000 | Loss: 0.00002641
Iteration 38/1000 | Loss: 0.00002640
Iteration 39/1000 | Loss: 0.00002639
Iteration 40/1000 | Loss: 0.00002639
Iteration 41/1000 | Loss: 0.00002638
Iteration 42/1000 | Loss: 0.00002637
Iteration 43/1000 | Loss: 0.00002636
Iteration 44/1000 | Loss: 0.00002635
Iteration 45/1000 | Loss: 0.00002635
Iteration 46/1000 | Loss: 0.00002634
Iteration 47/1000 | Loss: 0.00002634
Iteration 48/1000 | Loss: 0.00002634
Iteration 49/1000 | Loss: 0.00002633
Iteration 50/1000 | Loss: 0.00002633
Iteration 51/1000 | Loss: 0.00002633
Iteration 52/1000 | Loss: 0.00002631
Iteration 53/1000 | Loss: 0.00002631
Iteration 54/1000 | Loss: 0.00002630
Iteration 55/1000 | Loss: 0.00002630
Iteration 56/1000 | Loss: 0.00002630
Iteration 57/1000 | Loss: 0.00002628
Iteration 58/1000 | Loss: 0.00002628
Iteration 59/1000 | Loss: 0.00002628
Iteration 60/1000 | Loss: 0.00002628
Iteration 61/1000 | Loss: 0.00002627
Iteration 62/1000 | Loss: 0.00002627
Iteration 63/1000 | Loss: 0.00002626
Iteration 64/1000 | Loss: 0.00002626
Iteration 65/1000 | Loss: 0.00002625
Iteration 66/1000 | Loss: 0.00002625
Iteration 67/1000 | Loss: 0.00002625
Iteration 68/1000 | Loss: 0.00002625
Iteration 69/1000 | Loss: 0.00002624
Iteration 70/1000 | Loss: 0.00002624
Iteration 71/1000 | Loss: 0.00002624
Iteration 72/1000 | Loss: 0.00002624
Iteration 73/1000 | Loss: 0.00002624
Iteration 74/1000 | Loss: 0.00002624
Iteration 75/1000 | Loss: 0.00002624
Iteration 76/1000 | Loss: 0.00002624
Iteration 77/1000 | Loss: 0.00002623
Iteration 78/1000 | Loss: 0.00002623
Iteration 79/1000 | Loss: 0.00002623
Iteration 80/1000 | Loss: 0.00002622
Iteration 81/1000 | Loss: 0.00002622
Iteration 82/1000 | Loss: 0.00002622
Iteration 83/1000 | Loss: 0.00002622
Iteration 84/1000 | Loss: 0.00002621
Iteration 85/1000 | Loss: 0.00002621
Iteration 86/1000 | Loss: 0.00002621
Iteration 87/1000 | Loss: 0.00002621
Iteration 88/1000 | Loss: 0.00002621
Iteration 89/1000 | Loss: 0.00002620
Iteration 90/1000 | Loss: 0.00002620
Iteration 91/1000 | Loss: 0.00002620
Iteration 92/1000 | Loss: 0.00002620
Iteration 93/1000 | Loss: 0.00002620
Iteration 94/1000 | Loss: 0.00002619
Iteration 95/1000 | Loss: 0.00002619
Iteration 96/1000 | Loss: 0.00002619
Iteration 97/1000 | Loss: 0.00002619
Iteration 98/1000 | Loss: 0.00002619
Iteration 99/1000 | Loss: 0.00002619
Iteration 100/1000 | Loss: 0.00002619
Iteration 101/1000 | Loss: 0.00002619
Iteration 102/1000 | Loss: 0.00002619
Iteration 103/1000 | Loss: 0.00002619
Iteration 104/1000 | Loss: 0.00002618
Iteration 105/1000 | Loss: 0.00002618
Iteration 106/1000 | Loss: 0.00002618
Iteration 107/1000 | Loss: 0.00002618
Iteration 108/1000 | Loss: 0.00002618
Iteration 109/1000 | Loss: 0.00002618
Iteration 110/1000 | Loss: 0.00002618
Iteration 111/1000 | Loss: 0.00002618
Iteration 112/1000 | Loss: 0.00002618
Iteration 113/1000 | Loss: 0.00002618
Iteration 114/1000 | Loss: 0.00002618
Iteration 115/1000 | Loss: 0.00002618
Iteration 116/1000 | Loss: 0.00002618
Iteration 117/1000 | Loss: 0.00002618
Iteration 118/1000 | Loss: 0.00002618
Iteration 119/1000 | Loss: 0.00002618
Iteration 120/1000 | Loss: 0.00002618
Iteration 121/1000 | Loss: 0.00002618
Iteration 122/1000 | Loss: 0.00002618
Iteration 123/1000 | Loss: 0.00002618
Iteration 124/1000 | Loss: 0.00002618
Iteration 125/1000 | Loss: 0.00002618
Iteration 126/1000 | Loss: 0.00002618
Iteration 127/1000 | Loss: 0.00002618
Iteration 128/1000 | Loss: 0.00002618
Iteration 129/1000 | Loss: 0.00002618
Iteration 130/1000 | Loss: 0.00002618
Iteration 131/1000 | Loss: 0.00002618
Iteration 132/1000 | Loss: 0.00002618
Iteration 133/1000 | Loss: 0.00002618
Iteration 134/1000 | Loss: 0.00002618
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 134. Stopping optimization.
Last 5 losses: [2.618169310153462e-05, 2.618169310153462e-05, 2.618169310153462e-05, 2.618169310153462e-05, 2.618169310153462e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.618169310153462e-05

Optimization complete. Final v2v error: 4.152766227722168 mm

Highest mean error: 7.103417873382568 mm for frame 116

Lowest mean error: 2.8425140380859375 mm for frame 76

Saving results

Total time: 43.30480766296387
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_018/1085/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_018/1085.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_018/1085
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00569596
Iteration 2/25 | Loss: 0.00127180
Iteration 3/25 | Loss: 0.00118951
Iteration 4/25 | Loss: 0.00117855
Iteration 5/25 | Loss: 0.00117592
Iteration 6/25 | Loss: 0.00117556
Iteration 7/25 | Loss: 0.00117556
Iteration 8/25 | Loss: 0.00117556
Iteration 9/25 | Loss: 0.00117556
Iteration 10/25 | Loss: 0.00117556
Iteration 11/25 | Loss: 0.00117556
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011755554005503654, 0.0011755554005503654, 0.0011755554005503654, 0.0011755554005503654, 0.0011755554005503654]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011755554005503654

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.94130707
Iteration 2/25 | Loss: 0.00080529
Iteration 3/25 | Loss: 0.00080528
Iteration 4/25 | Loss: 0.00080528
Iteration 5/25 | Loss: 0.00080528
Iteration 6/25 | Loss: 0.00080528
Iteration 7/25 | Loss: 0.00080528
Iteration 8/25 | Loss: 0.00080528
Iteration 9/25 | Loss: 0.00080528
Iteration 10/25 | Loss: 0.00080528
Iteration 11/25 | Loss: 0.00080528
Iteration 12/25 | Loss: 0.00080528
Iteration 13/25 | Loss: 0.00080528
Iteration 14/25 | Loss: 0.00080528
Iteration 15/25 | Loss: 0.00080528
Iteration 16/25 | Loss: 0.00080528
Iteration 17/25 | Loss: 0.00080528
Iteration 18/25 | Loss: 0.00080528
Iteration 19/25 | Loss: 0.00080528
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0008052781922742724, 0.0008052781922742724, 0.0008052781922742724, 0.0008052781922742724, 0.0008052781922742724]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008052781922742724

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00080528
Iteration 2/1000 | Loss: 0.00003721
Iteration 3/1000 | Loss: 0.00001987
Iteration 4/1000 | Loss: 0.00001573
Iteration 5/1000 | Loss: 0.00001491
Iteration 6/1000 | Loss: 0.00001446
Iteration 7/1000 | Loss: 0.00001406
Iteration 8/1000 | Loss: 0.00001380
Iteration 9/1000 | Loss: 0.00001368
Iteration 10/1000 | Loss: 0.00001358
Iteration 11/1000 | Loss: 0.00001338
Iteration 12/1000 | Loss: 0.00001323
Iteration 13/1000 | Loss: 0.00001317
Iteration 14/1000 | Loss: 0.00001311
Iteration 15/1000 | Loss: 0.00001307
Iteration 16/1000 | Loss: 0.00001303
Iteration 17/1000 | Loss: 0.00001303
Iteration 18/1000 | Loss: 0.00001302
Iteration 19/1000 | Loss: 0.00001302
Iteration 20/1000 | Loss: 0.00001297
Iteration 21/1000 | Loss: 0.00001297
Iteration 22/1000 | Loss: 0.00001297
Iteration 23/1000 | Loss: 0.00001296
Iteration 24/1000 | Loss: 0.00001294
Iteration 25/1000 | Loss: 0.00001294
Iteration 26/1000 | Loss: 0.00001292
Iteration 27/1000 | Loss: 0.00001292
Iteration 28/1000 | Loss: 0.00001292
Iteration 29/1000 | Loss: 0.00001291
Iteration 30/1000 | Loss: 0.00001291
Iteration 31/1000 | Loss: 0.00001291
Iteration 32/1000 | Loss: 0.00001291
Iteration 33/1000 | Loss: 0.00001290
Iteration 34/1000 | Loss: 0.00001290
Iteration 35/1000 | Loss: 0.00001290
Iteration 36/1000 | Loss: 0.00001290
Iteration 37/1000 | Loss: 0.00001289
Iteration 38/1000 | Loss: 0.00001289
Iteration 39/1000 | Loss: 0.00001289
Iteration 40/1000 | Loss: 0.00001289
Iteration 41/1000 | Loss: 0.00001288
Iteration 42/1000 | Loss: 0.00001288
Iteration 43/1000 | Loss: 0.00001288
Iteration 44/1000 | Loss: 0.00001287
Iteration 45/1000 | Loss: 0.00001287
Iteration 46/1000 | Loss: 0.00001287
Iteration 47/1000 | Loss: 0.00001287
Iteration 48/1000 | Loss: 0.00001287
Iteration 49/1000 | Loss: 0.00001287
Iteration 50/1000 | Loss: 0.00001286
Iteration 51/1000 | Loss: 0.00001286
Iteration 52/1000 | Loss: 0.00001286
Iteration 53/1000 | Loss: 0.00001286
Iteration 54/1000 | Loss: 0.00001286
Iteration 55/1000 | Loss: 0.00001286
Iteration 56/1000 | Loss: 0.00001285
Iteration 57/1000 | Loss: 0.00001285
Iteration 58/1000 | Loss: 0.00001285
Iteration 59/1000 | Loss: 0.00001285
Iteration 60/1000 | Loss: 0.00001285
Iteration 61/1000 | Loss: 0.00001285
Iteration 62/1000 | Loss: 0.00001285
Iteration 63/1000 | Loss: 0.00001285
Iteration 64/1000 | Loss: 0.00001284
Iteration 65/1000 | Loss: 0.00001284
Iteration 66/1000 | Loss: 0.00001284
Iteration 67/1000 | Loss: 0.00001284
Iteration 68/1000 | Loss: 0.00001283
Iteration 69/1000 | Loss: 0.00001283
Iteration 70/1000 | Loss: 0.00001283
Iteration 71/1000 | Loss: 0.00001283
Iteration 72/1000 | Loss: 0.00001283
Iteration 73/1000 | Loss: 0.00001283
Iteration 74/1000 | Loss: 0.00001283
Iteration 75/1000 | Loss: 0.00001283
Iteration 76/1000 | Loss: 0.00001283
Iteration 77/1000 | Loss: 0.00001283
Iteration 78/1000 | Loss: 0.00001283
Iteration 79/1000 | Loss: 0.00001283
Iteration 80/1000 | Loss: 0.00001283
Iteration 81/1000 | Loss: 0.00001283
Iteration 82/1000 | Loss: 0.00001283
Iteration 83/1000 | Loss: 0.00001283
Iteration 84/1000 | Loss: 0.00001283
Iteration 85/1000 | Loss: 0.00001282
Iteration 86/1000 | Loss: 0.00001282
Iteration 87/1000 | Loss: 0.00001282
Iteration 88/1000 | Loss: 0.00001282
Iteration 89/1000 | Loss: 0.00001282
Iteration 90/1000 | Loss: 0.00001282
Iteration 91/1000 | Loss: 0.00001281
Iteration 92/1000 | Loss: 0.00001281
Iteration 93/1000 | Loss: 0.00001281
Iteration 94/1000 | Loss: 0.00001281
Iteration 95/1000 | Loss: 0.00001281
Iteration 96/1000 | Loss: 0.00001281
Iteration 97/1000 | Loss: 0.00001281
Iteration 98/1000 | Loss: 0.00001281
Iteration 99/1000 | Loss: 0.00001281
Iteration 100/1000 | Loss: 0.00001281
Iteration 101/1000 | Loss: 0.00001281
Iteration 102/1000 | Loss: 0.00001281
Iteration 103/1000 | Loss: 0.00001281
Iteration 104/1000 | Loss: 0.00001280
Iteration 105/1000 | Loss: 0.00001280
Iteration 106/1000 | Loss: 0.00001280
Iteration 107/1000 | Loss: 0.00001279
Iteration 108/1000 | Loss: 0.00001279
Iteration 109/1000 | Loss: 0.00001279
Iteration 110/1000 | Loss: 0.00001279
Iteration 111/1000 | Loss: 0.00001279
Iteration 112/1000 | Loss: 0.00001279
Iteration 113/1000 | Loss: 0.00001279
Iteration 114/1000 | Loss: 0.00001278
Iteration 115/1000 | Loss: 0.00001278
Iteration 116/1000 | Loss: 0.00001278
Iteration 117/1000 | Loss: 0.00001278
Iteration 118/1000 | Loss: 0.00001278
Iteration 119/1000 | Loss: 0.00001278
Iteration 120/1000 | Loss: 0.00001278
Iteration 121/1000 | Loss: 0.00001278
Iteration 122/1000 | Loss: 0.00001278
Iteration 123/1000 | Loss: 0.00001278
Iteration 124/1000 | Loss: 0.00001278
Iteration 125/1000 | Loss: 0.00001278
Iteration 126/1000 | Loss: 0.00001277
Iteration 127/1000 | Loss: 0.00001277
Iteration 128/1000 | Loss: 0.00001277
Iteration 129/1000 | Loss: 0.00001277
Iteration 130/1000 | Loss: 0.00001277
Iteration 131/1000 | Loss: 0.00001277
Iteration 132/1000 | Loss: 0.00001277
Iteration 133/1000 | Loss: 0.00001276
Iteration 134/1000 | Loss: 0.00001276
Iteration 135/1000 | Loss: 0.00001276
Iteration 136/1000 | Loss: 0.00001276
Iteration 137/1000 | Loss: 0.00001276
Iteration 138/1000 | Loss: 0.00001276
Iteration 139/1000 | Loss: 0.00001276
Iteration 140/1000 | Loss: 0.00001276
Iteration 141/1000 | Loss: 0.00001276
Iteration 142/1000 | Loss: 0.00001276
Iteration 143/1000 | Loss: 0.00001276
Iteration 144/1000 | Loss: 0.00001276
Iteration 145/1000 | Loss: 0.00001276
Iteration 146/1000 | Loss: 0.00001276
Iteration 147/1000 | Loss: 0.00001276
Iteration 148/1000 | Loss: 0.00001276
Iteration 149/1000 | Loss: 0.00001276
Iteration 150/1000 | Loss: 0.00001276
Iteration 151/1000 | Loss: 0.00001276
Iteration 152/1000 | Loss: 0.00001276
Iteration 153/1000 | Loss: 0.00001276
Iteration 154/1000 | Loss: 0.00001276
Iteration 155/1000 | Loss: 0.00001276
Iteration 156/1000 | Loss: 0.00001276
Iteration 157/1000 | Loss: 0.00001276
Iteration 158/1000 | Loss: 0.00001276
Iteration 159/1000 | Loss: 0.00001276
Iteration 160/1000 | Loss: 0.00001276
Iteration 161/1000 | Loss: 0.00001276
Iteration 162/1000 | Loss: 0.00001276
Iteration 163/1000 | Loss: 0.00001276
Iteration 164/1000 | Loss: 0.00001276
Iteration 165/1000 | Loss: 0.00001276
Iteration 166/1000 | Loss: 0.00001276
Iteration 167/1000 | Loss: 0.00001276
Iteration 168/1000 | Loss: 0.00001276
Iteration 169/1000 | Loss: 0.00001276
Iteration 170/1000 | Loss: 0.00001276
Iteration 171/1000 | Loss: 0.00001276
Iteration 172/1000 | Loss: 0.00001276
Iteration 173/1000 | Loss: 0.00001276
Iteration 174/1000 | Loss: 0.00001276
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 174. Stopping optimization.
Last 5 losses: [1.2757567674270831e-05, 1.2757567674270831e-05, 1.2757567674270831e-05, 1.2757567674270831e-05, 1.2757567674270831e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2757567674270831e-05

Optimization complete. Final v2v error: 3.0349652767181396 mm

Highest mean error: 3.2909462451934814 mm for frame 98

Lowest mean error: 2.7916922569274902 mm for frame 109

Saving results

Total time: 35.227585792541504
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_018/1045/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_018/1045.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_018/1045
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00427095
Iteration 2/25 | Loss: 0.00123878
Iteration 3/25 | Loss: 0.00117220
Iteration 4/25 | Loss: 0.00116255
Iteration 5/25 | Loss: 0.00116008
Iteration 6/25 | Loss: 0.00115968
Iteration 7/25 | Loss: 0.00115968
Iteration 8/25 | Loss: 0.00115968
Iteration 9/25 | Loss: 0.00115968
Iteration 10/25 | Loss: 0.00115968
Iteration 11/25 | Loss: 0.00115968
Iteration 12/25 | Loss: 0.00115968
Iteration 13/25 | Loss: 0.00115968
Iteration 14/25 | Loss: 0.00115968
Iteration 15/25 | Loss: 0.00115968
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0011596818221732974, 0.0011596818221732974, 0.0011596818221732974, 0.0011596818221732974, 0.0011596818221732974]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011596818221732974

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.90112770
Iteration 2/25 | Loss: 0.00083455
Iteration 3/25 | Loss: 0.00083455
Iteration 4/25 | Loss: 0.00083455
Iteration 5/25 | Loss: 0.00083455
Iteration 6/25 | Loss: 0.00083455
Iteration 7/25 | Loss: 0.00083455
Iteration 8/25 | Loss: 0.00083455
Iteration 9/25 | Loss: 0.00083455
Iteration 10/25 | Loss: 0.00083455
Iteration 11/25 | Loss: 0.00083455
Iteration 12/25 | Loss: 0.00083455
Iteration 13/25 | Loss: 0.00083455
Iteration 14/25 | Loss: 0.00083455
Iteration 15/25 | Loss: 0.00083455
Iteration 16/25 | Loss: 0.00083455
Iteration 17/25 | Loss: 0.00083455
Iteration 18/25 | Loss: 0.00083455
Iteration 19/25 | Loss: 0.00083455
Iteration 20/25 | Loss: 0.00083455
Iteration 21/25 | Loss: 0.00083455
Iteration 22/25 | Loss: 0.00083455
Iteration 23/25 | Loss: 0.00083455
Iteration 24/25 | Loss: 0.00083455
Iteration 25/25 | Loss: 0.00083455

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00083455
Iteration 2/1000 | Loss: 0.00003139
Iteration 3/1000 | Loss: 0.00002278
Iteration 4/1000 | Loss: 0.00001942
Iteration 5/1000 | Loss: 0.00001857
Iteration 6/1000 | Loss: 0.00001790
Iteration 7/1000 | Loss: 0.00001745
Iteration 8/1000 | Loss: 0.00001698
Iteration 9/1000 | Loss: 0.00001679
Iteration 10/1000 | Loss: 0.00001663
Iteration 11/1000 | Loss: 0.00001638
Iteration 12/1000 | Loss: 0.00001626
Iteration 13/1000 | Loss: 0.00001625
Iteration 14/1000 | Loss: 0.00001624
Iteration 15/1000 | Loss: 0.00001607
Iteration 16/1000 | Loss: 0.00001591
Iteration 17/1000 | Loss: 0.00001589
Iteration 18/1000 | Loss: 0.00001589
Iteration 19/1000 | Loss: 0.00001588
Iteration 20/1000 | Loss: 0.00001586
Iteration 21/1000 | Loss: 0.00001586
Iteration 22/1000 | Loss: 0.00001586
Iteration 23/1000 | Loss: 0.00001585
Iteration 24/1000 | Loss: 0.00001584
Iteration 25/1000 | Loss: 0.00001583
Iteration 26/1000 | Loss: 0.00001582
Iteration 27/1000 | Loss: 0.00001582
Iteration 28/1000 | Loss: 0.00001580
Iteration 29/1000 | Loss: 0.00001579
Iteration 30/1000 | Loss: 0.00001578
Iteration 31/1000 | Loss: 0.00001578
Iteration 32/1000 | Loss: 0.00001577
Iteration 33/1000 | Loss: 0.00001577
Iteration 34/1000 | Loss: 0.00001574
Iteration 35/1000 | Loss: 0.00001573
Iteration 36/1000 | Loss: 0.00001572
Iteration 37/1000 | Loss: 0.00001571
Iteration 38/1000 | Loss: 0.00001571
Iteration 39/1000 | Loss: 0.00001570
Iteration 40/1000 | Loss: 0.00001569
Iteration 41/1000 | Loss: 0.00001568
Iteration 42/1000 | Loss: 0.00001566
Iteration 43/1000 | Loss: 0.00001565
Iteration 44/1000 | Loss: 0.00001565
Iteration 45/1000 | Loss: 0.00001565
Iteration 46/1000 | Loss: 0.00001564
Iteration 47/1000 | Loss: 0.00001563
Iteration 48/1000 | Loss: 0.00001563
Iteration 49/1000 | Loss: 0.00001561
Iteration 50/1000 | Loss: 0.00001561
Iteration 51/1000 | Loss: 0.00001560
Iteration 52/1000 | Loss: 0.00001559
Iteration 53/1000 | Loss: 0.00001555
Iteration 54/1000 | Loss: 0.00001555
Iteration 55/1000 | Loss: 0.00001555
Iteration 56/1000 | Loss: 0.00001554
Iteration 57/1000 | Loss: 0.00001554
Iteration 58/1000 | Loss: 0.00001551
Iteration 59/1000 | Loss: 0.00001550
Iteration 60/1000 | Loss: 0.00001550
Iteration 61/1000 | Loss: 0.00001550
Iteration 62/1000 | Loss: 0.00001549
Iteration 63/1000 | Loss: 0.00001549
Iteration 64/1000 | Loss: 0.00001549
Iteration 65/1000 | Loss: 0.00001548
Iteration 66/1000 | Loss: 0.00001548
Iteration 67/1000 | Loss: 0.00001547
Iteration 68/1000 | Loss: 0.00001547
Iteration 69/1000 | Loss: 0.00001547
Iteration 70/1000 | Loss: 0.00001547
Iteration 71/1000 | Loss: 0.00001546
Iteration 72/1000 | Loss: 0.00001546
Iteration 73/1000 | Loss: 0.00001546
Iteration 74/1000 | Loss: 0.00001546
Iteration 75/1000 | Loss: 0.00001546
Iteration 76/1000 | Loss: 0.00001546
Iteration 77/1000 | Loss: 0.00001546
Iteration 78/1000 | Loss: 0.00001546
Iteration 79/1000 | Loss: 0.00001545
Iteration 80/1000 | Loss: 0.00001545
Iteration 81/1000 | Loss: 0.00001545
Iteration 82/1000 | Loss: 0.00001544
Iteration 83/1000 | Loss: 0.00001544
Iteration 84/1000 | Loss: 0.00001544
Iteration 85/1000 | Loss: 0.00001543
Iteration 86/1000 | Loss: 0.00001543
Iteration 87/1000 | Loss: 0.00001543
Iteration 88/1000 | Loss: 0.00001543
Iteration 89/1000 | Loss: 0.00001543
Iteration 90/1000 | Loss: 0.00001543
Iteration 91/1000 | Loss: 0.00001542
Iteration 92/1000 | Loss: 0.00001542
Iteration 93/1000 | Loss: 0.00001542
Iteration 94/1000 | Loss: 0.00001542
Iteration 95/1000 | Loss: 0.00001542
Iteration 96/1000 | Loss: 0.00001541
Iteration 97/1000 | Loss: 0.00001541
Iteration 98/1000 | Loss: 0.00001541
Iteration 99/1000 | Loss: 0.00001541
Iteration 100/1000 | Loss: 0.00001540
Iteration 101/1000 | Loss: 0.00001540
Iteration 102/1000 | Loss: 0.00001540
Iteration 103/1000 | Loss: 0.00001540
Iteration 104/1000 | Loss: 0.00001540
Iteration 105/1000 | Loss: 0.00001539
Iteration 106/1000 | Loss: 0.00001539
Iteration 107/1000 | Loss: 0.00001539
Iteration 108/1000 | Loss: 0.00001539
Iteration 109/1000 | Loss: 0.00001539
Iteration 110/1000 | Loss: 0.00001539
Iteration 111/1000 | Loss: 0.00001539
Iteration 112/1000 | Loss: 0.00001539
Iteration 113/1000 | Loss: 0.00001539
Iteration 114/1000 | Loss: 0.00001539
Iteration 115/1000 | Loss: 0.00001539
Iteration 116/1000 | Loss: 0.00001539
Iteration 117/1000 | Loss: 0.00001539
Iteration 118/1000 | Loss: 0.00001539
Iteration 119/1000 | Loss: 0.00001538
Iteration 120/1000 | Loss: 0.00001538
Iteration 121/1000 | Loss: 0.00001538
Iteration 122/1000 | Loss: 0.00001538
Iteration 123/1000 | Loss: 0.00001538
Iteration 124/1000 | Loss: 0.00001538
Iteration 125/1000 | Loss: 0.00001538
Iteration 126/1000 | Loss: 0.00001538
Iteration 127/1000 | Loss: 0.00001538
Iteration 128/1000 | Loss: 0.00001538
Iteration 129/1000 | Loss: 0.00001538
Iteration 130/1000 | Loss: 0.00001538
Iteration 131/1000 | Loss: 0.00001537
Iteration 132/1000 | Loss: 0.00001537
Iteration 133/1000 | Loss: 0.00001537
Iteration 134/1000 | Loss: 0.00001537
Iteration 135/1000 | Loss: 0.00001537
Iteration 136/1000 | Loss: 0.00001537
Iteration 137/1000 | Loss: 0.00001537
Iteration 138/1000 | Loss: 0.00001537
Iteration 139/1000 | Loss: 0.00001537
Iteration 140/1000 | Loss: 0.00001537
Iteration 141/1000 | Loss: 0.00001537
Iteration 142/1000 | Loss: 0.00001537
Iteration 143/1000 | Loss: 0.00001537
Iteration 144/1000 | Loss: 0.00001537
Iteration 145/1000 | Loss: 0.00001537
Iteration 146/1000 | Loss: 0.00001537
Iteration 147/1000 | Loss: 0.00001537
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 147. Stopping optimization.
Last 5 losses: [1.53667224367382e-05, 1.53667224367382e-05, 1.53667224367382e-05, 1.53667224367382e-05, 1.53667224367382e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.53667224367382e-05

Optimization complete. Final v2v error: 3.342031240463257 mm

Highest mean error: 3.9336838722229004 mm for frame 20

Lowest mean error: 3.0639851093292236 mm for frame 43

Saving results

Total time: 39.2340030670166
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_018/1070/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_018/1070.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_018/1070
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00969473
Iteration 2/25 | Loss: 0.00403941
Iteration 3/25 | Loss: 0.00246353
Iteration 4/25 | Loss: 0.00227265
Iteration 5/25 | Loss: 0.00218862
Iteration 6/25 | Loss: 0.00213912
Iteration 7/25 | Loss: 0.00211469
Iteration 8/25 | Loss: 0.00203289
Iteration 9/25 | Loss: 0.00198606
Iteration 10/25 | Loss: 0.00194227
Iteration 11/25 | Loss: 0.00192240
Iteration 12/25 | Loss: 0.00192048
Iteration 13/25 | Loss: 0.00190025
Iteration 14/25 | Loss: 0.00189695
Iteration 15/25 | Loss: 0.00188651
Iteration 16/25 | Loss: 0.00188569
Iteration 17/25 | Loss: 0.00188031
Iteration 18/25 | Loss: 0.00187677
Iteration 19/25 | Loss: 0.00187581
Iteration 20/25 | Loss: 0.00187497
Iteration 21/25 | Loss: 0.00187447
Iteration 22/25 | Loss: 0.00187422
Iteration 23/25 | Loss: 0.00187363
Iteration 24/25 | Loss: 0.00187282
Iteration 25/25 | Loss: 0.00187236

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33626604
Iteration 2/25 | Loss: 0.00387893
Iteration 3/25 | Loss: 0.00387893
Iteration 4/25 | Loss: 0.00387893
Iteration 5/25 | Loss: 0.00387893
Iteration 6/25 | Loss: 0.00387893
Iteration 7/25 | Loss: 0.00387893
Iteration 8/25 | Loss: 0.00387893
Iteration 9/25 | Loss: 0.00387893
Iteration 10/25 | Loss: 0.00387893
Iteration 11/25 | Loss: 0.00387893
Iteration 12/25 | Loss: 0.00387893
Iteration 13/25 | Loss: 0.00387893
Iteration 14/25 | Loss: 0.00387893
Iteration 15/25 | Loss: 0.00387893
Iteration 16/25 | Loss: 0.00387893
Iteration 17/25 | Loss: 0.00387893
Iteration 18/25 | Loss: 0.00387893
Iteration 19/25 | Loss: 0.00387893
Iteration 20/25 | Loss: 0.00387893
Iteration 21/25 | Loss: 0.00387893
Iteration 22/25 | Loss: 0.00387893
Iteration 23/25 | Loss: 0.00387893
Iteration 24/25 | Loss: 0.00387893
Iteration 25/25 | Loss: 0.00387893

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00387893
Iteration 2/1000 | Loss: 0.00128363
Iteration 3/1000 | Loss: 0.00090298
Iteration 4/1000 | Loss: 0.00047067
Iteration 5/1000 | Loss: 0.00051499
Iteration 6/1000 | Loss: 0.00077231
Iteration 7/1000 | Loss: 0.00051089
Iteration 8/1000 | Loss: 0.00036886
Iteration 9/1000 | Loss: 0.00041057
Iteration 10/1000 | Loss: 0.00032453
Iteration 11/1000 | Loss: 0.00075537
Iteration 12/1000 | Loss: 0.00113360
Iteration 13/1000 | Loss: 0.00679842
Iteration 14/1000 | Loss: 0.00422260
Iteration 15/1000 | Loss: 0.00049801
Iteration 16/1000 | Loss: 0.00030976
Iteration 17/1000 | Loss: 0.00030793
Iteration 18/1000 | Loss: 0.00014105
Iteration 19/1000 | Loss: 0.00009415
Iteration 20/1000 | Loss: 0.00007643
Iteration 21/1000 | Loss: 0.00009728
Iteration 22/1000 | Loss: 0.00012571
Iteration 23/1000 | Loss: 0.00003958
Iteration 24/1000 | Loss: 0.00003177
Iteration 25/1000 | Loss: 0.00020367
Iteration 26/1000 | Loss: 0.00002550
Iteration 27/1000 | Loss: 0.00009344
Iteration 28/1000 | Loss: 0.00002320
Iteration 29/1000 | Loss: 0.00002053
Iteration 30/1000 | Loss: 0.00001900
Iteration 31/1000 | Loss: 0.00001809
Iteration 32/1000 | Loss: 0.00001742
Iteration 33/1000 | Loss: 0.00001707
Iteration 34/1000 | Loss: 0.00001669
Iteration 35/1000 | Loss: 0.00001642
Iteration 36/1000 | Loss: 0.00001617
Iteration 37/1000 | Loss: 0.00001613
Iteration 38/1000 | Loss: 0.00001611
Iteration 39/1000 | Loss: 0.00001610
Iteration 40/1000 | Loss: 0.00001610
Iteration 41/1000 | Loss: 0.00001609
Iteration 42/1000 | Loss: 0.00001609
Iteration 43/1000 | Loss: 0.00001608
Iteration 44/1000 | Loss: 0.00001607
Iteration 45/1000 | Loss: 0.00001602
Iteration 46/1000 | Loss: 0.00001600
Iteration 47/1000 | Loss: 0.00001599
Iteration 48/1000 | Loss: 0.00001598
Iteration 49/1000 | Loss: 0.00001595
Iteration 50/1000 | Loss: 0.00001595
Iteration 51/1000 | Loss: 0.00001595
Iteration 52/1000 | Loss: 0.00001595
Iteration 53/1000 | Loss: 0.00001595
Iteration 54/1000 | Loss: 0.00001595
Iteration 55/1000 | Loss: 0.00001595
Iteration 56/1000 | Loss: 0.00001595
Iteration 57/1000 | Loss: 0.00001595
Iteration 58/1000 | Loss: 0.00001595
Iteration 59/1000 | Loss: 0.00001594
Iteration 60/1000 | Loss: 0.00001594
Iteration 61/1000 | Loss: 0.00001594
Iteration 62/1000 | Loss: 0.00001594
Iteration 63/1000 | Loss: 0.00001594
Iteration 64/1000 | Loss: 0.00001591
Iteration 65/1000 | Loss: 0.00001590
Iteration 66/1000 | Loss: 0.00001589
Iteration 67/1000 | Loss: 0.00001589
Iteration 68/1000 | Loss: 0.00001589
Iteration 69/1000 | Loss: 0.00001588
Iteration 70/1000 | Loss: 0.00001588
Iteration 71/1000 | Loss: 0.00001587
Iteration 72/1000 | Loss: 0.00001587
Iteration 73/1000 | Loss: 0.00001587
Iteration 74/1000 | Loss: 0.00001586
Iteration 75/1000 | Loss: 0.00001586
Iteration 76/1000 | Loss: 0.00001585
Iteration 77/1000 | Loss: 0.00001584
Iteration 78/1000 | Loss: 0.00001584
Iteration 79/1000 | Loss: 0.00001584
Iteration 80/1000 | Loss: 0.00001584
Iteration 81/1000 | Loss: 0.00001583
Iteration 82/1000 | Loss: 0.00001583
Iteration 83/1000 | Loss: 0.00001583
Iteration 84/1000 | Loss: 0.00001582
Iteration 85/1000 | Loss: 0.00001582
Iteration 86/1000 | Loss: 0.00001582
Iteration 87/1000 | Loss: 0.00001581
Iteration 88/1000 | Loss: 0.00001581
Iteration 89/1000 | Loss: 0.00001581
Iteration 90/1000 | Loss: 0.00001581
Iteration 91/1000 | Loss: 0.00001580
Iteration 92/1000 | Loss: 0.00001580
Iteration 93/1000 | Loss: 0.00001580
Iteration 94/1000 | Loss: 0.00001580
Iteration 95/1000 | Loss: 0.00001579
Iteration 96/1000 | Loss: 0.00001579
Iteration 97/1000 | Loss: 0.00001579
Iteration 98/1000 | Loss: 0.00001579
Iteration 99/1000 | Loss: 0.00001579
Iteration 100/1000 | Loss: 0.00001579
Iteration 101/1000 | Loss: 0.00001579
Iteration 102/1000 | Loss: 0.00001579
Iteration 103/1000 | Loss: 0.00001578
Iteration 104/1000 | Loss: 0.00001578
Iteration 105/1000 | Loss: 0.00001578
Iteration 106/1000 | Loss: 0.00001578
Iteration 107/1000 | Loss: 0.00001577
Iteration 108/1000 | Loss: 0.00001577
Iteration 109/1000 | Loss: 0.00001576
Iteration 110/1000 | Loss: 0.00001576
Iteration 111/1000 | Loss: 0.00001576
Iteration 112/1000 | Loss: 0.00001576
Iteration 113/1000 | Loss: 0.00001576
Iteration 114/1000 | Loss: 0.00001576
Iteration 115/1000 | Loss: 0.00001575
Iteration 116/1000 | Loss: 0.00001575
Iteration 117/1000 | Loss: 0.00001575
Iteration 118/1000 | Loss: 0.00001575
Iteration 119/1000 | Loss: 0.00001575
Iteration 120/1000 | Loss: 0.00001574
Iteration 121/1000 | Loss: 0.00001574
Iteration 122/1000 | Loss: 0.00001574
Iteration 123/1000 | Loss: 0.00001574
Iteration 124/1000 | Loss: 0.00001574
Iteration 125/1000 | Loss: 0.00001574
Iteration 126/1000 | Loss: 0.00001573
Iteration 127/1000 | Loss: 0.00001573
Iteration 128/1000 | Loss: 0.00001573
Iteration 129/1000 | Loss: 0.00001573
Iteration 130/1000 | Loss: 0.00001573
Iteration 131/1000 | Loss: 0.00001573
Iteration 132/1000 | Loss: 0.00001572
Iteration 133/1000 | Loss: 0.00001572
Iteration 134/1000 | Loss: 0.00001572
Iteration 135/1000 | Loss: 0.00001572
Iteration 136/1000 | Loss: 0.00001572
Iteration 137/1000 | Loss: 0.00001572
Iteration 138/1000 | Loss: 0.00001572
Iteration 139/1000 | Loss: 0.00001572
Iteration 140/1000 | Loss: 0.00001572
Iteration 141/1000 | Loss: 0.00001572
Iteration 142/1000 | Loss: 0.00001572
Iteration 143/1000 | Loss: 0.00001572
Iteration 144/1000 | Loss: 0.00001572
Iteration 145/1000 | Loss: 0.00001572
Iteration 146/1000 | Loss: 0.00001572
Iteration 147/1000 | Loss: 0.00001572
Iteration 148/1000 | Loss: 0.00001572
Iteration 149/1000 | Loss: 0.00001572
Iteration 150/1000 | Loss: 0.00001572
Iteration 151/1000 | Loss: 0.00001572
Iteration 152/1000 | Loss: 0.00001572
Iteration 153/1000 | Loss: 0.00001572
Iteration 154/1000 | Loss: 0.00001572
Iteration 155/1000 | Loss: 0.00001572
Iteration 156/1000 | Loss: 0.00001572
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 156. Stopping optimization.
Last 5 losses: [1.571532447997015e-05, 1.571532447997015e-05, 1.571532447997015e-05, 1.571532447997015e-05, 1.571532447997015e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.571532447997015e-05

Optimization complete. Final v2v error: 3.403130054473877 mm

Highest mean error: 3.6755764484405518 mm for frame 191

Lowest mean error: 3.1954925060272217 mm for frame 4

Saving results

Total time: 120.48125457763672
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_018/1049/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_018/1049.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_018/1049
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00874439
Iteration 2/25 | Loss: 0.00134915
Iteration 3/25 | Loss: 0.00118584
Iteration 4/25 | Loss: 0.00117053
Iteration 5/25 | Loss: 0.00116657
Iteration 6/25 | Loss: 0.00116584
Iteration 7/25 | Loss: 0.00116584
Iteration 8/25 | Loss: 0.00116584
Iteration 9/25 | Loss: 0.00116584
Iteration 10/25 | Loss: 0.00116584
Iteration 11/25 | Loss: 0.00116584
Iteration 12/25 | Loss: 0.00116584
Iteration 13/25 | Loss: 0.00116584
Iteration 14/25 | Loss: 0.00116584
Iteration 15/25 | Loss: 0.00116584
Iteration 16/25 | Loss: 0.00116584
Iteration 17/25 | Loss: 0.00116584
Iteration 18/25 | Loss: 0.00116584
Iteration 19/25 | Loss: 0.00116584
Iteration 20/25 | Loss: 0.00116584
Iteration 21/25 | Loss: 0.00116584
Iteration 22/25 | Loss: 0.00116584
Iteration 23/25 | Loss: 0.00116584
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0011658368166536093, 0.0011658368166536093, 0.0011658368166536093, 0.0011658368166536093, 0.0011658368166536093]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011658368166536093

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44728708
Iteration 2/25 | Loss: 0.00082562
Iteration 3/25 | Loss: 0.00082562
Iteration 4/25 | Loss: 0.00082562
Iteration 5/25 | Loss: 0.00082562
Iteration 6/25 | Loss: 0.00082562
Iteration 7/25 | Loss: 0.00082562
Iteration 8/25 | Loss: 0.00082562
Iteration 9/25 | Loss: 0.00082562
Iteration 10/25 | Loss: 0.00082562
Iteration 11/25 | Loss: 0.00082562
Iteration 12/25 | Loss: 0.00082562
Iteration 13/25 | Loss: 0.00082562
Iteration 14/25 | Loss: 0.00082562
Iteration 15/25 | Loss: 0.00082562
Iteration 16/25 | Loss: 0.00082562
Iteration 17/25 | Loss: 0.00082562
Iteration 18/25 | Loss: 0.00082562
Iteration 19/25 | Loss: 0.00082562
Iteration 20/25 | Loss: 0.00082562
Iteration 21/25 | Loss: 0.00082562
Iteration 22/25 | Loss: 0.00082562
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0008256171713583171, 0.0008256171713583171, 0.0008256171713583171, 0.0008256171713583171, 0.0008256171713583171]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008256171713583171

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00082562
Iteration 2/1000 | Loss: 0.00002917
Iteration 3/1000 | Loss: 0.00002044
Iteration 4/1000 | Loss: 0.00001913
Iteration 5/1000 | Loss: 0.00001825
Iteration 6/1000 | Loss: 0.00001769
Iteration 7/1000 | Loss: 0.00001731
Iteration 8/1000 | Loss: 0.00001687
Iteration 9/1000 | Loss: 0.00001662
Iteration 10/1000 | Loss: 0.00001630
Iteration 11/1000 | Loss: 0.00001607
Iteration 12/1000 | Loss: 0.00001596
Iteration 13/1000 | Loss: 0.00001586
Iteration 14/1000 | Loss: 0.00001580
Iteration 15/1000 | Loss: 0.00001572
Iteration 16/1000 | Loss: 0.00001569
Iteration 17/1000 | Loss: 0.00001565
Iteration 18/1000 | Loss: 0.00001564
Iteration 19/1000 | Loss: 0.00001560
Iteration 20/1000 | Loss: 0.00001560
Iteration 21/1000 | Loss: 0.00001557
Iteration 22/1000 | Loss: 0.00001556
Iteration 23/1000 | Loss: 0.00001556
Iteration 24/1000 | Loss: 0.00001555
Iteration 25/1000 | Loss: 0.00001555
Iteration 26/1000 | Loss: 0.00001554
Iteration 27/1000 | Loss: 0.00001554
Iteration 28/1000 | Loss: 0.00001552
Iteration 29/1000 | Loss: 0.00001552
Iteration 30/1000 | Loss: 0.00001552
Iteration 31/1000 | Loss: 0.00001552
Iteration 32/1000 | Loss: 0.00001552
Iteration 33/1000 | Loss: 0.00001551
Iteration 34/1000 | Loss: 0.00001551
Iteration 35/1000 | Loss: 0.00001550
Iteration 36/1000 | Loss: 0.00001550
Iteration 37/1000 | Loss: 0.00001549
Iteration 38/1000 | Loss: 0.00001549
Iteration 39/1000 | Loss: 0.00001548
Iteration 40/1000 | Loss: 0.00001548
Iteration 41/1000 | Loss: 0.00001547
Iteration 42/1000 | Loss: 0.00001547
Iteration 43/1000 | Loss: 0.00001547
Iteration 44/1000 | Loss: 0.00001546
Iteration 45/1000 | Loss: 0.00001546
Iteration 46/1000 | Loss: 0.00001545
Iteration 47/1000 | Loss: 0.00001545
Iteration 48/1000 | Loss: 0.00001544
Iteration 49/1000 | Loss: 0.00001544
Iteration 50/1000 | Loss: 0.00001544
Iteration 51/1000 | Loss: 0.00001544
Iteration 52/1000 | Loss: 0.00001544
Iteration 53/1000 | Loss: 0.00001543
Iteration 54/1000 | Loss: 0.00001543
Iteration 55/1000 | Loss: 0.00001543
Iteration 56/1000 | Loss: 0.00001542
Iteration 57/1000 | Loss: 0.00001542
Iteration 58/1000 | Loss: 0.00001542
Iteration 59/1000 | Loss: 0.00001541
Iteration 60/1000 | Loss: 0.00001541
Iteration 61/1000 | Loss: 0.00001540
Iteration 62/1000 | Loss: 0.00001540
Iteration 63/1000 | Loss: 0.00001540
Iteration 64/1000 | Loss: 0.00001540
Iteration 65/1000 | Loss: 0.00001539
Iteration 66/1000 | Loss: 0.00001539
Iteration 67/1000 | Loss: 0.00001539
Iteration 68/1000 | Loss: 0.00001539
Iteration 69/1000 | Loss: 0.00001539
Iteration 70/1000 | Loss: 0.00001538
Iteration 71/1000 | Loss: 0.00001538
Iteration 72/1000 | Loss: 0.00001538
Iteration 73/1000 | Loss: 0.00001537
Iteration 74/1000 | Loss: 0.00001537
Iteration 75/1000 | Loss: 0.00001536
Iteration 76/1000 | Loss: 0.00001536
Iteration 77/1000 | Loss: 0.00001535
Iteration 78/1000 | Loss: 0.00001535
Iteration 79/1000 | Loss: 0.00001535
Iteration 80/1000 | Loss: 0.00001535
Iteration 81/1000 | Loss: 0.00001535
Iteration 82/1000 | Loss: 0.00001534
Iteration 83/1000 | Loss: 0.00001534
Iteration 84/1000 | Loss: 0.00001534
Iteration 85/1000 | Loss: 0.00001534
Iteration 86/1000 | Loss: 0.00001534
Iteration 87/1000 | Loss: 0.00001533
Iteration 88/1000 | Loss: 0.00001533
Iteration 89/1000 | Loss: 0.00001533
Iteration 90/1000 | Loss: 0.00001532
Iteration 91/1000 | Loss: 0.00001532
Iteration 92/1000 | Loss: 0.00001532
Iteration 93/1000 | Loss: 0.00001532
Iteration 94/1000 | Loss: 0.00001532
Iteration 95/1000 | Loss: 0.00001532
Iteration 96/1000 | Loss: 0.00001532
Iteration 97/1000 | Loss: 0.00001531
Iteration 98/1000 | Loss: 0.00001531
Iteration 99/1000 | Loss: 0.00001531
Iteration 100/1000 | Loss: 0.00001531
Iteration 101/1000 | Loss: 0.00001531
Iteration 102/1000 | Loss: 0.00001531
Iteration 103/1000 | Loss: 0.00001531
Iteration 104/1000 | Loss: 0.00001531
Iteration 105/1000 | Loss: 0.00001530
Iteration 106/1000 | Loss: 0.00001530
Iteration 107/1000 | Loss: 0.00001530
Iteration 108/1000 | Loss: 0.00001530
Iteration 109/1000 | Loss: 0.00001530
Iteration 110/1000 | Loss: 0.00001529
Iteration 111/1000 | Loss: 0.00001529
Iteration 112/1000 | Loss: 0.00001529
Iteration 113/1000 | Loss: 0.00001529
Iteration 114/1000 | Loss: 0.00001529
Iteration 115/1000 | Loss: 0.00001529
Iteration 116/1000 | Loss: 0.00001529
Iteration 117/1000 | Loss: 0.00001529
Iteration 118/1000 | Loss: 0.00001529
Iteration 119/1000 | Loss: 0.00001528
Iteration 120/1000 | Loss: 0.00001528
Iteration 121/1000 | Loss: 0.00001528
Iteration 122/1000 | Loss: 0.00001528
Iteration 123/1000 | Loss: 0.00001527
Iteration 124/1000 | Loss: 0.00001527
Iteration 125/1000 | Loss: 0.00001527
Iteration 126/1000 | Loss: 0.00001527
Iteration 127/1000 | Loss: 0.00001527
Iteration 128/1000 | Loss: 0.00001527
Iteration 129/1000 | Loss: 0.00001527
Iteration 130/1000 | Loss: 0.00001527
Iteration 131/1000 | Loss: 0.00001527
Iteration 132/1000 | Loss: 0.00001527
Iteration 133/1000 | Loss: 0.00001527
Iteration 134/1000 | Loss: 0.00001527
Iteration 135/1000 | Loss: 0.00001527
Iteration 136/1000 | Loss: 0.00001527
Iteration 137/1000 | Loss: 0.00001527
Iteration 138/1000 | Loss: 0.00001527
Iteration 139/1000 | Loss: 0.00001527
Iteration 140/1000 | Loss: 0.00001527
Iteration 141/1000 | Loss: 0.00001527
Iteration 142/1000 | Loss: 0.00001527
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 142. Stopping optimization.
Last 5 losses: [1.526556479802821e-05, 1.526556479802821e-05, 1.526556479802821e-05, 1.526556479802821e-05, 1.526556479802821e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.526556479802821e-05

Optimization complete. Final v2v error: 3.1986215114593506 mm

Highest mean error: 4.8242573738098145 mm for frame 55

Lowest mean error: 2.721738815307617 mm for frame 9

Saving results

Total time: 38.91754937171936
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_018/1068/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_018/1068.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_018/1068
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00880032
Iteration 2/25 | Loss: 0.00171169
Iteration 3/25 | Loss: 0.00137217
Iteration 4/25 | Loss: 0.00136847
Iteration 5/25 | Loss: 0.00130547
Iteration 6/25 | Loss: 0.00130287
Iteration 7/25 | Loss: 0.00129889
Iteration 8/25 | Loss: 0.00127343
Iteration 9/25 | Loss: 0.00126848
Iteration 10/25 | Loss: 0.00125834
Iteration 11/25 | Loss: 0.00126223
Iteration 12/25 | Loss: 0.00125124
Iteration 13/25 | Loss: 0.00124562
Iteration 14/25 | Loss: 0.00125073
Iteration 15/25 | Loss: 0.00124532
Iteration 16/25 | Loss: 0.00123865
Iteration 17/25 | Loss: 0.00123369
Iteration 18/25 | Loss: 0.00123094
Iteration 19/25 | Loss: 0.00123010
Iteration 20/25 | Loss: 0.00122992
Iteration 21/25 | Loss: 0.00122978
Iteration 22/25 | Loss: 0.00122968
Iteration 23/25 | Loss: 0.00122966
Iteration 24/25 | Loss: 0.00122966
Iteration 25/25 | Loss: 0.00122965

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.66982126
Iteration 2/25 | Loss: 0.00125466
Iteration 3/25 | Loss: 0.00125457
Iteration 4/25 | Loss: 0.00125457
Iteration 5/25 | Loss: 0.00125457
Iteration 6/25 | Loss: 0.00125457
Iteration 7/25 | Loss: 0.00125457
Iteration 8/25 | Loss: 0.00125457
Iteration 9/25 | Loss: 0.00125457
Iteration 10/25 | Loss: 0.00125457
Iteration 11/25 | Loss: 0.00125457
Iteration 12/25 | Loss: 0.00125457
Iteration 13/25 | Loss: 0.00125457
Iteration 14/25 | Loss: 0.00125457
Iteration 15/25 | Loss: 0.00125457
Iteration 16/25 | Loss: 0.00125457
Iteration 17/25 | Loss: 0.00125457
Iteration 18/25 | Loss: 0.00125457
Iteration 19/25 | Loss: 0.00125457
Iteration 20/25 | Loss: 0.00125457
Iteration 21/25 | Loss: 0.00125457
Iteration 22/25 | Loss: 0.00125457
Iteration 23/25 | Loss: 0.00125457
Iteration 24/25 | Loss: 0.00125457
Iteration 25/25 | Loss: 0.00125457

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00125457
Iteration 2/1000 | Loss: 0.00010997
Iteration 3/1000 | Loss: 0.00008367
Iteration 4/1000 | Loss: 0.00005492
Iteration 5/1000 | Loss: 0.00008066
Iteration 6/1000 | Loss: 0.00004688
Iteration 7/1000 | Loss: 0.00006806
Iteration 8/1000 | Loss: 0.00004219
Iteration 9/1000 | Loss: 0.00003676
Iteration 10/1000 | Loss: 0.00005678
Iteration 11/1000 | Loss: 0.00005182
Iteration 12/1000 | Loss: 0.00004588
Iteration 13/1000 | Loss: 0.00004302
Iteration 14/1000 | Loss: 0.00003747
Iteration 15/1000 | Loss: 0.00003424
Iteration 16/1000 | Loss: 0.00003252
Iteration 17/1000 | Loss: 0.00003173
Iteration 18/1000 | Loss: 0.00003106
Iteration 19/1000 | Loss: 0.00003065
Iteration 20/1000 | Loss: 0.00018045
Iteration 21/1000 | Loss: 0.00007016
Iteration 22/1000 | Loss: 0.00017770
Iteration 23/1000 | Loss: 0.00003048
Iteration 24/1000 | Loss: 0.00002989
Iteration 25/1000 | Loss: 0.00002958
Iteration 26/1000 | Loss: 0.00002937
Iteration 27/1000 | Loss: 0.00002922
Iteration 28/1000 | Loss: 0.00002922
Iteration 29/1000 | Loss: 0.00002912
Iteration 30/1000 | Loss: 0.00002904
Iteration 31/1000 | Loss: 0.00002890
Iteration 32/1000 | Loss: 0.00002890
Iteration 33/1000 | Loss: 0.00002889
Iteration 34/1000 | Loss: 0.00002888
Iteration 35/1000 | Loss: 0.00002882
Iteration 36/1000 | Loss: 0.00002880
Iteration 37/1000 | Loss: 0.00002876
Iteration 38/1000 | Loss: 0.00002872
Iteration 39/1000 | Loss: 0.00002865
Iteration 40/1000 | Loss: 0.00002852
Iteration 41/1000 | Loss: 0.00002849
Iteration 42/1000 | Loss: 0.00002849
Iteration 43/1000 | Loss: 0.00002846
Iteration 44/1000 | Loss: 0.00002845
Iteration 45/1000 | Loss: 0.00002845
Iteration 46/1000 | Loss: 0.00002843
Iteration 47/1000 | Loss: 0.00002842
Iteration 48/1000 | Loss: 0.00002842
Iteration 49/1000 | Loss: 0.00002841
Iteration 50/1000 | Loss: 0.00002841
Iteration 51/1000 | Loss: 0.00002841
Iteration 52/1000 | Loss: 0.00002840
Iteration 53/1000 | Loss: 0.00002840
Iteration 54/1000 | Loss: 0.00002840
Iteration 55/1000 | Loss: 0.00002840
Iteration 56/1000 | Loss: 0.00002840
Iteration 57/1000 | Loss: 0.00002839
Iteration 58/1000 | Loss: 0.00002839
Iteration 59/1000 | Loss: 0.00002839
Iteration 60/1000 | Loss: 0.00002839
Iteration 61/1000 | Loss: 0.00002838
Iteration 62/1000 | Loss: 0.00002838
Iteration 63/1000 | Loss: 0.00002838
Iteration 64/1000 | Loss: 0.00002837
Iteration 65/1000 | Loss: 0.00002837
Iteration 66/1000 | Loss: 0.00002837
Iteration 67/1000 | Loss: 0.00002836
Iteration 68/1000 | Loss: 0.00002836
Iteration 69/1000 | Loss: 0.00002836
Iteration 70/1000 | Loss: 0.00002835
Iteration 71/1000 | Loss: 0.00002835
Iteration 72/1000 | Loss: 0.00002835
Iteration 73/1000 | Loss: 0.00002835
Iteration 74/1000 | Loss: 0.00002835
Iteration 75/1000 | Loss: 0.00002835
Iteration 76/1000 | Loss: 0.00002835
Iteration 77/1000 | Loss: 0.00002834
Iteration 78/1000 | Loss: 0.00002834
Iteration 79/1000 | Loss: 0.00002834
Iteration 80/1000 | Loss: 0.00002834
Iteration 81/1000 | Loss: 0.00002834
Iteration 82/1000 | Loss: 0.00002834
Iteration 83/1000 | Loss: 0.00002834
Iteration 84/1000 | Loss: 0.00002833
Iteration 85/1000 | Loss: 0.00002833
Iteration 86/1000 | Loss: 0.00002833
Iteration 87/1000 | Loss: 0.00002833
Iteration 88/1000 | Loss: 0.00002833
Iteration 89/1000 | Loss: 0.00002833
Iteration 90/1000 | Loss: 0.00002833
Iteration 91/1000 | Loss: 0.00002833
Iteration 92/1000 | Loss: 0.00002832
Iteration 93/1000 | Loss: 0.00002832
Iteration 94/1000 | Loss: 0.00002832
Iteration 95/1000 | Loss: 0.00002832
Iteration 96/1000 | Loss: 0.00002832
Iteration 97/1000 | Loss: 0.00002832
Iteration 98/1000 | Loss: 0.00002832
Iteration 99/1000 | Loss: 0.00002832
Iteration 100/1000 | Loss: 0.00002831
Iteration 101/1000 | Loss: 0.00002831
Iteration 102/1000 | Loss: 0.00002830
Iteration 103/1000 | Loss: 0.00002830
Iteration 104/1000 | Loss: 0.00002830
Iteration 105/1000 | Loss: 0.00002830
Iteration 106/1000 | Loss: 0.00002830
Iteration 107/1000 | Loss: 0.00002830
Iteration 108/1000 | Loss: 0.00002829
Iteration 109/1000 | Loss: 0.00002829
Iteration 110/1000 | Loss: 0.00002829
Iteration 111/1000 | Loss: 0.00002829
Iteration 112/1000 | Loss: 0.00002828
Iteration 113/1000 | Loss: 0.00002828
Iteration 114/1000 | Loss: 0.00002828
Iteration 115/1000 | Loss: 0.00002827
Iteration 116/1000 | Loss: 0.00002827
Iteration 117/1000 | Loss: 0.00002827
Iteration 118/1000 | Loss: 0.00002826
Iteration 119/1000 | Loss: 0.00002826
Iteration 120/1000 | Loss: 0.00002826
Iteration 121/1000 | Loss: 0.00002825
Iteration 122/1000 | Loss: 0.00002825
Iteration 123/1000 | Loss: 0.00002825
Iteration 124/1000 | Loss: 0.00002824
Iteration 125/1000 | Loss: 0.00002824
Iteration 126/1000 | Loss: 0.00002824
Iteration 127/1000 | Loss: 0.00002824
Iteration 128/1000 | Loss: 0.00002824
Iteration 129/1000 | Loss: 0.00002823
Iteration 130/1000 | Loss: 0.00002823
Iteration 131/1000 | Loss: 0.00002823
Iteration 132/1000 | Loss: 0.00002822
Iteration 133/1000 | Loss: 0.00002822
Iteration 134/1000 | Loss: 0.00018536
Iteration 135/1000 | Loss: 0.00003319
Iteration 136/1000 | Loss: 0.00003072
Iteration 137/1000 | Loss: 0.00002950
Iteration 138/1000 | Loss: 0.00002855
Iteration 139/1000 | Loss: 0.00002798
Iteration 140/1000 | Loss: 0.00002797
Iteration 141/1000 | Loss: 0.00002781
Iteration 142/1000 | Loss: 0.00002779
Iteration 143/1000 | Loss: 0.00002779
Iteration 144/1000 | Loss: 0.00002778
Iteration 145/1000 | Loss: 0.00002776
Iteration 146/1000 | Loss: 0.00002773
Iteration 147/1000 | Loss: 0.00002769
Iteration 148/1000 | Loss: 0.00002768
Iteration 149/1000 | Loss: 0.00002766
Iteration 150/1000 | Loss: 0.00002765
Iteration 151/1000 | Loss: 0.00002761
Iteration 152/1000 | Loss: 0.00002761
Iteration 153/1000 | Loss: 0.00002761
Iteration 154/1000 | Loss: 0.00002761
Iteration 155/1000 | Loss: 0.00002761
Iteration 156/1000 | Loss: 0.00002760
Iteration 157/1000 | Loss: 0.00002759
Iteration 158/1000 | Loss: 0.00002759
Iteration 159/1000 | Loss: 0.00002759
Iteration 160/1000 | Loss: 0.00002758
Iteration 161/1000 | Loss: 0.00002758
Iteration 162/1000 | Loss: 0.00002758
Iteration 163/1000 | Loss: 0.00002758
Iteration 164/1000 | Loss: 0.00002757
Iteration 165/1000 | Loss: 0.00002757
Iteration 166/1000 | Loss: 0.00002757
Iteration 167/1000 | Loss: 0.00002756
Iteration 168/1000 | Loss: 0.00002756
Iteration 169/1000 | Loss: 0.00002756
Iteration 170/1000 | Loss: 0.00002756
Iteration 171/1000 | Loss: 0.00002755
Iteration 172/1000 | Loss: 0.00002755
Iteration 173/1000 | Loss: 0.00002755
Iteration 174/1000 | Loss: 0.00002755
Iteration 175/1000 | Loss: 0.00002755
Iteration 176/1000 | Loss: 0.00002754
Iteration 177/1000 | Loss: 0.00002754
Iteration 178/1000 | Loss: 0.00002754
Iteration 179/1000 | Loss: 0.00002754
Iteration 180/1000 | Loss: 0.00002754
Iteration 181/1000 | Loss: 0.00002754
Iteration 182/1000 | Loss: 0.00002754
Iteration 183/1000 | Loss: 0.00002754
Iteration 184/1000 | Loss: 0.00002754
Iteration 185/1000 | Loss: 0.00002754
Iteration 186/1000 | Loss: 0.00002754
Iteration 187/1000 | Loss: 0.00002753
Iteration 188/1000 | Loss: 0.00002753
Iteration 189/1000 | Loss: 0.00002753
Iteration 190/1000 | Loss: 0.00002753
Iteration 191/1000 | Loss: 0.00002753
Iteration 192/1000 | Loss: 0.00002753
Iteration 193/1000 | Loss: 0.00002753
Iteration 194/1000 | Loss: 0.00002753
Iteration 195/1000 | Loss: 0.00002753
Iteration 196/1000 | Loss: 0.00002753
Iteration 197/1000 | Loss: 0.00002753
Iteration 198/1000 | Loss: 0.00002753
Iteration 199/1000 | Loss: 0.00002753
Iteration 200/1000 | Loss: 0.00002753
Iteration 201/1000 | Loss: 0.00002753
Iteration 202/1000 | Loss: 0.00002752
Iteration 203/1000 | Loss: 0.00002752
Iteration 204/1000 | Loss: 0.00002752
Iteration 205/1000 | Loss: 0.00002752
Iteration 206/1000 | Loss: 0.00002752
Iteration 207/1000 | Loss: 0.00002751
Iteration 208/1000 | Loss: 0.00002751
Iteration 209/1000 | Loss: 0.00002751
Iteration 210/1000 | Loss: 0.00002751
Iteration 211/1000 | Loss: 0.00002750
Iteration 212/1000 | Loss: 0.00002750
Iteration 213/1000 | Loss: 0.00002750
Iteration 214/1000 | Loss: 0.00002749
Iteration 215/1000 | Loss: 0.00002749
Iteration 216/1000 | Loss: 0.00002749
Iteration 217/1000 | Loss: 0.00002749
Iteration 218/1000 | Loss: 0.00002749
Iteration 219/1000 | Loss: 0.00002748
Iteration 220/1000 | Loss: 0.00002748
Iteration 221/1000 | Loss: 0.00002748
Iteration 222/1000 | Loss: 0.00002748
Iteration 223/1000 | Loss: 0.00002748
Iteration 224/1000 | Loss: 0.00002748
Iteration 225/1000 | Loss: 0.00002748
Iteration 226/1000 | Loss: 0.00002748
Iteration 227/1000 | Loss: 0.00002748
Iteration 228/1000 | Loss: 0.00002748
Iteration 229/1000 | Loss: 0.00002748
Iteration 230/1000 | Loss: 0.00002748
Iteration 231/1000 | Loss: 0.00002748
Iteration 232/1000 | Loss: 0.00002748
Iteration 233/1000 | Loss: 0.00002748
Iteration 234/1000 | Loss: 0.00002748
Iteration 235/1000 | Loss: 0.00002748
Iteration 236/1000 | Loss: 0.00002748
Iteration 237/1000 | Loss: 0.00002748
Iteration 238/1000 | Loss: 0.00002748
Iteration 239/1000 | Loss: 0.00002748
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 239. Stopping optimization.
Last 5 losses: [2.7478668926050887e-05, 2.7478668926050887e-05, 2.7478668926050887e-05, 2.7478668926050887e-05, 2.7478668926050887e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.7478668926050887e-05

Optimization complete. Final v2v error: 4.286182880401611 mm

Highest mean error: 6.879478454589844 mm for frame 41

Lowest mean error: 2.994236469268799 mm for frame 81

Saving results

Total time: 124.66622471809387
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_018/1079/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_018/1079.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_018/1079
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00744141
Iteration 2/25 | Loss: 0.00126674
Iteration 3/25 | Loss: 0.00111277
Iteration 4/25 | Loss: 0.00109733
Iteration 5/25 | Loss: 0.00109373
Iteration 6/25 | Loss: 0.00109330
Iteration 7/25 | Loss: 0.00109330
Iteration 8/25 | Loss: 0.00109330
Iteration 9/25 | Loss: 0.00109330
Iteration 10/25 | Loss: 0.00109330
Iteration 11/25 | Loss: 0.00109330
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001093300641514361, 0.001093300641514361, 0.001093300641514361, 0.001093300641514361, 0.001093300641514361]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001093300641514361

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32302344
Iteration 2/25 | Loss: 0.00060526
Iteration 3/25 | Loss: 0.00060522
Iteration 4/25 | Loss: 0.00060522
Iteration 5/25 | Loss: 0.00060522
Iteration 6/25 | Loss: 0.00060522
Iteration 7/25 | Loss: 0.00060522
Iteration 8/25 | Loss: 0.00060522
Iteration 9/25 | Loss: 0.00060522
Iteration 10/25 | Loss: 0.00060522
Iteration 11/25 | Loss: 0.00060522
Iteration 12/25 | Loss: 0.00060522
Iteration 13/25 | Loss: 0.00060522
Iteration 14/25 | Loss: 0.00060522
Iteration 15/25 | Loss: 0.00060522
Iteration 16/25 | Loss: 0.00060522
Iteration 17/25 | Loss: 0.00060522
Iteration 18/25 | Loss: 0.00060522
Iteration 19/25 | Loss: 0.00060522
Iteration 20/25 | Loss: 0.00060522
Iteration 21/25 | Loss: 0.00060522
Iteration 22/25 | Loss: 0.00060522
Iteration 23/25 | Loss: 0.00060522
Iteration 24/25 | Loss: 0.00060522
Iteration 25/25 | Loss: 0.00060522

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00060522
Iteration 2/1000 | Loss: 0.00003026
Iteration 3/1000 | Loss: 0.00002256
Iteration 4/1000 | Loss: 0.00001905
Iteration 5/1000 | Loss: 0.00001736
Iteration 6/1000 | Loss: 0.00001617
Iteration 7/1000 | Loss: 0.00001540
Iteration 8/1000 | Loss: 0.00001474
Iteration 9/1000 | Loss: 0.00001429
Iteration 10/1000 | Loss: 0.00001388
Iteration 11/1000 | Loss: 0.00001359
Iteration 12/1000 | Loss: 0.00001329
Iteration 13/1000 | Loss: 0.00001311
Iteration 14/1000 | Loss: 0.00001289
Iteration 15/1000 | Loss: 0.00001267
Iteration 16/1000 | Loss: 0.00001252
Iteration 17/1000 | Loss: 0.00001252
Iteration 18/1000 | Loss: 0.00001251
Iteration 19/1000 | Loss: 0.00001246
Iteration 20/1000 | Loss: 0.00001245
Iteration 21/1000 | Loss: 0.00001245
Iteration 22/1000 | Loss: 0.00001244
Iteration 23/1000 | Loss: 0.00001244
Iteration 24/1000 | Loss: 0.00001243
Iteration 25/1000 | Loss: 0.00001242
Iteration 26/1000 | Loss: 0.00001241
Iteration 27/1000 | Loss: 0.00001241
Iteration 28/1000 | Loss: 0.00001240
Iteration 29/1000 | Loss: 0.00001240
Iteration 30/1000 | Loss: 0.00001238
Iteration 31/1000 | Loss: 0.00001237
Iteration 32/1000 | Loss: 0.00001237
Iteration 33/1000 | Loss: 0.00001237
Iteration 34/1000 | Loss: 0.00001237
Iteration 35/1000 | Loss: 0.00001237
Iteration 36/1000 | Loss: 0.00001236
Iteration 37/1000 | Loss: 0.00001236
Iteration 38/1000 | Loss: 0.00001236
Iteration 39/1000 | Loss: 0.00001236
Iteration 40/1000 | Loss: 0.00001235
Iteration 41/1000 | Loss: 0.00001235
Iteration 42/1000 | Loss: 0.00001235
Iteration 43/1000 | Loss: 0.00001235
Iteration 44/1000 | Loss: 0.00001235
Iteration 45/1000 | Loss: 0.00001234
Iteration 46/1000 | Loss: 0.00001234
Iteration 47/1000 | Loss: 0.00001234
Iteration 48/1000 | Loss: 0.00001234
Iteration 49/1000 | Loss: 0.00001233
Iteration 50/1000 | Loss: 0.00001233
Iteration 51/1000 | Loss: 0.00001232
Iteration 52/1000 | Loss: 0.00001232
Iteration 53/1000 | Loss: 0.00001232
Iteration 54/1000 | Loss: 0.00001232
Iteration 55/1000 | Loss: 0.00001232
Iteration 56/1000 | Loss: 0.00001232
Iteration 57/1000 | Loss: 0.00001232
Iteration 58/1000 | Loss: 0.00001232
Iteration 59/1000 | Loss: 0.00001231
Iteration 60/1000 | Loss: 0.00001230
Iteration 61/1000 | Loss: 0.00001229
Iteration 62/1000 | Loss: 0.00001229
Iteration 63/1000 | Loss: 0.00001229
Iteration 64/1000 | Loss: 0.00001229
Iteration 65/1000 | Loss: 0.00001228
Iteration 66/1000 | Loss: 0.00001228
Iteration 67/1000 | Loss: 0.00001228
Iteration 68/1000 | Loss: 0.00001228
Iteration 69/1000 | Loss: 0.00001228
Iteration 70/1000 | Loss: 0.00001228
Iteration 71/1000 | Loss: 0.00001228
Iteration 72/1000 | Loss: 0.00001228
Iteration 73/1000 | Loss: 0.00001228
Iteration 74/1000 | Loss: 0.00001228
Iteration 75/1000 | Loss: 0.00001227
Iteration 76/1000 | Loss: 0.00001227
Iteration 77/1000 | Loss: 0.00001227
Iteration 78/1000 | Loss: 0.00001226
Iteration 79/1000 | Loss: 0.00001226
Iteration 80/1000 | Loss: 0.00001225
Iteration 81/1000 | Loss: 0.00001225
Iteration 82/1000 | Loss: 0.00001225
Iteration 83/1000 | Loss: 0.00001224
Iteration 84/1000 | Loss: 0.00001224
Iteration 85/1000 | Loss: 0.00001224
Iteration 86/1000 | Loss: 0.00001224
Iteration 87/1000 | Loss: 0.00001223
Iteration 88/1000 | Loss: 0.00001223
Iteration 89/1000 | Loss: 0.00001223
Iteration 90/1000 | Loss: 0.00001222
Iteration 91/1000 | Loss: 0.00001222
Iteration 92/1000 | Loss: 0.00001221
Iteration 93/1000 | Loss: 0.00001221
Iteration 94/1000 | Loss: 0.00001221
Iteration 95/1000 | Loss: 0.00001221
Iteration 96/1000 | Loss: 0.00001221
Iteration 97/1000 | Loss: 0.00001220
Iteration 98/1000 | Loss: 0.00001220
Iteration 99/1000 | Loss: 0.00001219
Iteration 100/1000 | Loss: 0.00001218
Iteration 101/1000 | Loss: 0.00001217
Iteration 102/1000 | Loss: 0.00001216
Iteration 103/1000 | Loss: 0.00001215
Iteration 104/1000 | Loss: 0.00001215
Iteration 105/1000 | Loss: 0.00001215
Iteration 106/1000 | Loss: 0.00001215
Iteration 107/1000 | Loss: 0.00001214
Iteration 108/1000 | Loss: 0.00001214
Iteration 109/1000 | Loss: 0.00001214
Iteration 110/1000 | Loss: 0.00001214
Iteration 111/1000 | Loss: 0.00001214
Iteration 112/1000 | Loss: 0.00001214
Iteration 113/1000 | Loss: 0.00001214
Iteration 114/1000 | Loss: 0.00001214
Iteration 115/1000 | Loss: 0.00001214
Iteration 116/1000 | Loss: 0.00001213
Iteration 117/1000 | Loss: 0.00001213
Iteration 118/1000 | Loss: 0.00001213
Iteration 119/1000 | Loss: 0.00001213
Iteration 120/1000 | Loss: 0.00001212
Iteration 121/1000 | Loss: 0.00001212
Iteration 122/1000 | Loss: 0.00001212
Iteration 123/1000 | Loss: 0.00001211
Iteration 124/1000 | Loss: 0.00001211
Iteration 125/1000 | Loss: 0.00001211
Iteration 126/1000 | Loss: 0.00001211
Iteration 127/1000 | Loss: 0.00001211
Iteration 128/1000 | Loss: 0.00001211
Iteration 129/1000 | Loss: 0.00001210
Iteration 130/1000 | Loss: 0.00001210
Iteration 131/1000 | Loss: 0.00001210
Iteration 132/1000 | Loss: 0.00001210
Iteration 133/1000 | Loss: 0.00001210
Iteration 134/1000 | Loss: 0.00001210
Iteration 135/1000 | Loss: 0.00001210
Iteration 136/1000 | Loss: 0.00001210
Iteration 137/1000 | Loss: 0.00001210
Iteration 138/1000 | Loss: 0.00001209
Iteration 139/1000 | Loss: 0.00001209
Iteration 140/1000 | Loss: 0.00001209
Iteration 141/1000 | Loss: 0.00001209
Iteration 142/1000 | Loss: 0.00001209
Iteration 143/1000 | Loss: 0.00001209
Iteration 144/1000 | Loss: 0.00001209
Iteration 145/1000 | Loss: 0.00001209
Iteration 146/1000 | Loss: 0.00001209
Iteration 147/1000 | Loss: 0.00001208
Iteration 148/1000 | Loss: 0.00001208
Iteration 149/1000 | Loss: 0.00001208
Iteration 150/1000 | Loss: 0.00001208
Iteration 151/1000 | Loss: 0.00001208
Iteration 152/1000 | Loss: 0.00001208
Iteration 153/1000 | Loss: 0.00001208
Iteration 154/1000 | Loss: 0.00001208
Iteration 155/1000 | Loss: 0.00001208
Iteration 156/1000 | Loss: 0.00001208
Iteration 157/1000 | Loss: 0.00001208
Iteration 158/1000 | Loss: 0.00001208
Iteration 159/1000 | Loss: 0.00001208
Iteration 160/1000 | Loss: 0.00001208
Iteration 161/1000 | Loss: 0.00001208
Iteration 162/1000 | Loss: 0.00001208
Iteration 163/1000 | Loss: 0.00001208
Iteration 164/1000 | Loss: 0.00001208
Iteration 165/1000 | Loss: 0.00001207
Iteration 166/1000 | Loss: 0.00001207
Iteration 167/1000 | Loss: 0.00001207
Iteration 168/1000 | Loss: 0.00001207
Iteration 169/1000 | Loss: 0.00001207
Iteration 170/1000 | Loss: 0.00001207
Iteration 171/1000 | Loss: 0.00001207
Iteration 172/1000 | Loss: 0.00001207
Iteration 173/1000 | Loss: 0.00001207
Iteration 174/1000 | Loss: 0.00001207
Iteration 175/1000 | Loss: 0.00001207
Iteration 176/1000 | Loss: 0.00001207
Iteration 177/1000 | Loss: 0.00001207
Iteration 178/1000 | Loss: 0.00001207
Iteration 179/1000 | Loss: 0.00001207
Iteration 180/1000 | Loss: 0.00001207
Iteration 181/1000 | Loss: 0.00001207
Iteration 182/1000 | Loss: 0.00001207
Iteration 183/1000 | Loss: 0.00001207
Iteration 184/1000 | Loss: 0.00001207
Iteration 185/1000 | Loss: 0.00001206
Iteration 186/1000 | Loss: 0.00001206
Iteration 187/1000 | Loss: 0.00001206
Iteration 188/1000 | Loss: 0.00001206
Iteration 189/1000 | Loss: 0.00001206
Iteration 190/1000 | Loss: 0.00001206
Iteration 191/1000 | Loss: 0.00001206
Iteration 192/1000 | Loss: 0.00001206
Iteration 193/1000 | Loss: 0.00001206
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 193. Stopping optimization.
Last 5 losses: [1.2064062502759043e-05, 1.2064062502759043e-05, 1.2064062502759043e-05, 1.2064062502759043e-05, 1.2064062502759043e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2064062502759043e-05

Optimization complete. Final v2v error: 2.973412275314331 mm

Highest mean error: 3.412519931793213 mm for frame 193

Lowest mean error: 2.652838706970215 mm for frame 67

Saving results

Total time: 49.178462266922
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_018/1020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_018/1020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_018/1020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00797137
Iteration 2/25 | Loss: 0.00155407
Iteration 3/25 | Loss: 0.00136107
Iteration 4/25 | Loss: 0.00122014
Iteration 5/25 | Loss: 0.00123510
Iteration 6/25 | Loss: 0.00120378
Iteration 7/25 | Loss: 0.00120906
Iteration 8/25 | Loss: 0.00117955
Iteration 9/25 | Loss: 0.00117656
Iteration 10/25 | Loss: 0.00117551
Iteration 11/25 | Loss: 0.00117502
Iteration 12/25 | Loss: 0.00117573
Iteration 13/25 | Loss: 0.00117454
Iteration 14/25 | Loss: 0.00117450
Iteration 15/25 | Loss: 0.00117449
Iteration 16/25 | Loss: 0.00117449
Iteration 17/25 | Loss: 0.00117449
Iteration 18/25 | Loss: 0.00117448
Iteration 19/25 | Loss: 0.00117448
Iteration 20/25 | Loss: 0.00117448
Iteration 21/25 | Loss: 0.00117448
Iteration 22/25 | Loss: 0.00117448
Iteration 23/25 | Loss: 0.00117448
Iteration 24/25 | Loss: 0.00117448
Iteration 25/25 | Loss: 0.00117448

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.83975470
Iteration 2/25 | Loss: 0.00111995
Iteration 3/25 | Loss: 0.00111995
Iteration 4/25 | Loss: 0.00111995
Iteration 5/25 | Loss: 0.00091648
Iteration 6/25 | Loss: 0.00091648
Iteration 7/25 | Loss: 0.00091648
Iteration 8/25 | Loss: 0.00091648
Iteration 9/25 | Loss: 0.00091648
Iteration 10/25 | Loss: 0.00091648
Iteration 11/25 | Loss: 0.00091648
Iteration 12/25 | Loss: 0.00091648
Iteration 13/25 | Loss: 0.00091648
Iteration 14/25 | Loss: 0.00091648
Iteration 15/25 | Loss: 0.00091648
Iteration 16/25 | Loss: 0.00091648
Iteration 17/25 | Loss: 0.00091648
Iteration 18/25 | Loss: 0.00091648
Iteration 19/25 | Loss: 0.00091648
Iteration 20/25 | Loss: 0.00091648
Iteration 21/25 | Loss: 0.00091648
Iteration 22/25 | Loss: 0.00091648
Iteration 23/25 | Loss: 0.00091648
Iteration 24/25 | Loss: 0.00091648
Iteration 25/25 | Loss: 0.00091648

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00091648
Iteration 2/1000 | Loss: 0.00003050
Iteration 3/1000 | Loss: 0.00027911
Iteration 4/1000 | Loss: 0.00002101
Iteration 5/1000 | Loss: 0.00002014
Iteration 6/1000 | Loss: 0.00007995
Iteration 7/1000 | Loss: 0.00001922
Iteration 8/1000 | Loss: 0.00007327
Iteration 9/1000 | Loss: 0.00001869
Iteration 10/1000 | Loss: 0.00001841
Iteration 11/1000 | Loss: 0.00001836
Iteration 12/1000 | Loss: 0.00001835
Iteration 13/1000 | Loss: 0.00001808
Iteration 14/1000 | Loss: 0.00001790
Iteration 15/1000 | Loss: 0.00001775
Iteration 16/1000 | Loss: 0.00001774
Iteration 17/1000 | Loss: 0.00008514
Iteration 18/1000 | Loss: 0.00008292
Iteration 19/1000 | Loss: 0.00001839
Iteration 20/1000 | Loss: 0.00001741
Iteration 21/1000 | Loss: 0.00001739
Iteration 22/1000 | Loss: 0.00001737
Iteration 23/1000 | Loss: 0.00001736
Iteration 24/1000 | Loss: 0.00001734
Iteration 25/1000 | Loss: 0.00001733
Iteration 26/1000 | Loss: 0.00001733
Iteration 27/1000 | Loss: 0.00001732
Iteration 28/1000 | Loss: 0.00001732
Iteration 29/1000 | Loss: 0.00004715
Iteration 30/1000 | Loss: 0.00001831
Iteration 31/1000 | Loss: 0.00001740
Iteration 32/1000 | Loss: 0.00001740
Iteration 33/1000 | Loss: 0.00001739
Iteration 34/1000 | Loss: 0.00001739
Iteration 35/1000 | Loss: 0.00001739
Iteration 36/1000 | Loss: 0.00001739
Iteration 37/1000 | Loss: 0.00001739
Iteration 38/1000 | Loss: 0.00001739
Iteration 39/1000 | Loss: 0.00001738
Iteration 40/1000 | Loss: 0.00001735
Iteration 41/1000 | Loss: 0.00002082
Iteration 42/1000 | Loss: 0.00001731
Iteration 43/1000 | Loss: 0.00001725
Iteration 44/1000 | Loss: 0.00001725
Iteration 45/1000 | Loss: 0.00001723
Iteration 46/1000 | Loss: 0.00001723
Iteration 47/1000 | Loss: 0.00001721
Iteration 48/1000 | Loss: 0.00001720
Iteration 49/1000 | Loss: 0.00001720
Iteration 50/1000 | Loss: 0.00001720
Iteration 51/1000 | Loss: 0.00001719
Iteration 52/1000 | Loss: 0.00001719
Iteration 53/1000 | Loss: 0.00001718
Iteration 54/1000 | Loss: 0.00001717
Iteration 55/1000 | Loss: 0.00001717
Iteration 56/1000 | Loss: 0.00001717
Iteration 57/1000 | Loss: 0.00001717
Iteration 58/1000 | Loss: 0.00001717
Iteration 59/1000 | Loss: 0.00001717
Iteration 60/1000 | Loss: 0.00001716
Iteration 61/1000 | Loss: 0.00001716
Iteration 62/1000 | Loss: 0.00001716
Iteration 63/1000 | Loss: 0.00001714
Iteration 64/1000 | Loss: 0.00001714
Iteration 65/1000 | Loss: 0.00001714
Iteration 66/1000 | Loss: 0.00001714
Iteration 67/1000 | Loss: 0.00001712
Iteration 68/1000 | Loss: 0.00001711
Iteration 69/1000 | Loss: 0.00001711
Iteration 70/1000 | Loss: 0.00005312
Iteration 71/1000 | Loss: 0.00001903
Iteration 72/1000 | Loss: 0.00001724
Iteration 73/1000 | Loss: 0.00001724
Iteration 74/1000 | Loss: 0.00001714
Iteration 75/1000 | Loss: 0.00001714
Iteration 76/1000 | Loss: 0.00001714
Iteration 77/1000 | Loss: 0.00001713
Iteration 78/1000 | Loss: 0.00001713
Iteration 79/1000 | Loss: 0.00001877
Iteration 80/1000 | Loss: 0.00001831
Iteration 81/1000 | Loss: 0.00001712
Iteration 82/1000 | Loss: 0.00001712
Iteration 83/1000 | Loss: 0.00001712
Iteration 84/1000 | Loss: 0.00001711
Iteration 85/1000 | Loss: 0.00001711
Iteration 86/1000 | Loss: 0.00001711
Iteration 87/1000 | Loss: 0.00001711
Iteration 88/1000 | Loss: 0.00001711
Iteration 89/1000 | Loss: 0.00001711
Iteration 90/1000 | Loss: 0.00001710
Iteration 91/1000 | Loss: 0.00001710
Iteration 92/1000 | Loss: 0.00002178
Iteration 93/1000 | Loss: 0.00001711
Iteration 94/1000 | Loss: 0.00001711
Iteration 95/1000 | Loss: 0.00001711
Iteration 96/1000 | Loss: 0.00001711
Iteration 97/1000 | Loss: 0.00001711
Iteration 98/1000 | Loss: 0.00001711
Iteration 99/1000 | Loss: 0.00001711
Iteration 100/1000 | Loss: 0.00001711
Iteration 101/1000 | Loss: 0.00001711
Iteration 102/1000 | Loss: 0.00001711
Iteration 103/1000 | Loss: 0.00001711
Iteration 104/1000 | Loss: 0.00001711
Iteration 105/1000 | Loss: 0.00001708
Iteration 106/1000 | Loss: 0.00001707
Iteration 107/1000 | Loss: 0.00001707
Iteration 108/1000 | Loss: 0.00001707
Iteration 109/1000 | Loss: 0.00001707
Iteration 110/1000 | Loss: 0.00001706
Iteration 111/1000 | Loss: 0.00001705
Iteration 112/1000 | Loss: 0.00002419
Iteration 113/1000 | Loss: 0.00001705
Iteration 114/1000 | Loss: 0.00001705
Iteration 115/1000 | Loss: 0.00001705
Iteration 116/1000 | Loss: 0.00001704
Iteration 117/1000 | Loss: 0.00001703
Iteration 118/1000 | Loss: 0.00001703
Iteration 119/1000 | Loss: 0.00001703
Iteration 120/1000 | Loss: 0.00001703
Iteration 121/1000 | Loss: 0.00001703
Iteration 122/1000 | Loss: 0.00001703
Iteration 123/1000 | Loss: 0.00001703
Iteration 124/1000 | Loss: 0.00001703
Iteration 125/1000 | Loss: 0.00001703
Iteration 126/1000 | Loss: 0.00001703
Iteration 127/1000 | Loss: 0.00001703
Iteration 128/1000 | Loss: 0.00001702
Iteration 129/1000 | Loss: 0.00001702
Iteration 130/1000 | Loss: 0.00001702
Iteration 131/1000 | Loss: 0.00001701
Iteration 132/1000 | Loss: 0.00001701
Iteration 133/1000 | Loss: 0.00001701
Iteration 134/1000 | Loss: 0.00001701
Iteration 135/1000 | Loss: 0.00002096
Iteration 136/1000 | Loss: 0.00001700
Iteration 137/1000 | Loss: 0.00001700
Iteration 138/1000 | Loss: 0.00001700
Iteration 139/1000 | Loss: 0.00001700
Iteration 140/1000 | Loss: 0.00001700
Iteration 141/1000 | Loss: 0.00001700
Iteration 142/1000 | Loss: 0.00001700
Iteration 143/1000 | Loss: 0.00001700
Iteration 144/1000 | Loss: 0.00001700
Iteration 145/1000 | Loss: 0.00001700
Iteration 146/1000 | Loss: 0.00001700
Iteration 147/1000 | Loss: 0.00001700
Iteration 148/1000 | Loss: 0.00001700
Iteration 149/1000 | Loss: 0.00001700
Iteration 150/1000 | Loss: 0.00001700
Iteration 151/1000 | Loss: 0.00001700
Iteration 152/1000 | Loss: 0.00001700
Iteration 153/1000 | Loss: 0.00001700
Iteration 154/1000 | Loss: 0.00001700
Iteration 155/1000 | Loss: 0.00001700
Iteration 156/1000 | Loss: 0.00001700
Iteration 157/1000 | Loss: 0.00001700
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 157. Stopping optimization.
Last 5 losses: [1.6995012629195116e-05, 1.6995012629195116e-05, 1.6995012629195116e-05, 1.6995012629195116e-05, 1.6995012629195116e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6995012629195116e-05

Optimization complete. Final v2v error: 3.50274658203125 mm

Highest mean error: 4.083311557769775 mm for frame 149

Lowest mean error: 3.154097318649292 mm for frame 138

Saving results

Total time: 74.90790462493896
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_018/1090/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_018/1090.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_018/1090
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00452947
Iteration 2/25 | Loss: 0.00126636
Iteration 3/25 | Loss: 0.00115493
Iteration 4/25 | Loss: 0.00114077
Iteration 5/25 | Loss: 0.00113618
Iteration 6/25 | Loss: 0.00113543
Iteration 7/25 | Loss: 0.00113543
Iteration 8/25 | Loss: 0.00113543
Iteration 9/25 | Loss: 0.00113543
Iteration 10/25 | Loss: 0.00113543
Iteration 11/25 | Loss: 0.00113543
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001135427737608552, 0.001135427737608552, 0.001135427737608552, 0.001135427737608552, 0.001135427737608552]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001135427737608552

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43205786
Iteration 2/25 | Loss: 0.00090916
Iteration 3/25 | Loss: 0.00090916
Iteration 4/25 | Loss: 0.00090916
Iteration 5/25 | Loss: 0.00090916
Iteration 6/25 | Loss: 0.00090916
Iteration 7/25 | Loss: 0.00090916
Iteration 8/25 | Loss: 0.00090916
Iteration 9/25 | Loss: 0.00090916
Iteration 10/25 | Loss: 0.00090916
Iteration 11/25 | Loss: 0.00090916
Iteration 12/25 | Loss: 0.00090916
Iteration 13/25 | Loss: 0.00090916
Iteration 14/25 | Loss: 0.00090915
Iteration 15/25 | Loss: 0.00090915
Iteration 16/25 | Loss: 0.00090916
Iteration 17/25 | Loss: 0.00090916
Iteration 18/25 | Loss: 0.00090915
Iteration 19/25 | Loss: 0.00090915
Iteration 20/25 | Loss: 0.00090915
Iteration 21/25 | Loss: 0.00090915
Iteration 22/25 | Loss: 0.00090915
Iteration 23/25 | Loss: 0.00090915
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0009091549436561763, 0.0009091549436561763, 0.0009091549436561763, 0.0009091549436561763, 0.0009091549436561763]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009091549436561763

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00090915
Iteration 2/1000 | Loss: 0.00002383
Iteration 3/1000 | Loss: 0.00001713
Iteration 4/1000 | Loss: 0.00001560
Iteration 5/1000 | Loss: 0.00001484
Iteration 6/1000 | Loss: 0.00001432
Iteration 7/1000 | Loss: 0.00001402
Iteration 8/1000 | Loss: 0.00001372
Iteration 9/1000 | Loss: 0.00001341
Iteration 10/1000 | Loss: 0.00001324
Iteration 11/1000 | Loss: 0.00001323
Iteration 12/1000 | Loss: 0.00001318
Iteration 13/1000 | Loss: 0.00001317
Iteration 14/1000 | Loss: 0.00001316
Iteration 15/1000 | Loss: 0.00001303
Iteration 16/1000 | Loss: 0.00001300
Iteration 17/1000 | Loss: 0.00001292
Iteration 18/1000 | Loss: 0.00001289
Iteration 19/1000 | Loss: 0.00001289
Iteration 20/1000 | Loss: 0.00001284
Iteration 21/1000 | Loss: 0.00001284
Iteration 22/1000 | Loss: 0.00001282
Iteration 23/1000 | Loss: 0.00001281
Iteration 24/1000 | Loss: 0.00001281
Iteration 25/1000 | Loss: 0.00001280
Iteration 26/1000 | Loss: 0.00001276
Iteration 27/1000 | Loss: 0.00001276
Iteration 28/1000 | Loss: 0.00001274
Iteration 29/1000 | Loss: 0.00001274
Iteration 30/1000 | Loss: 0.00001273
Iteration 31/1000 | Loss: 0.00001273
Iteration 32/1000 | Loss: 0.00001272
Iteration 33/1000 | Loss: 0.00001272
Iteration 34/1000 | Loss: 0.00001272
Iteration 35/1000 | Loss: 0.00001271
Iteration 36/1000 | Loss: 0.00001271
Iteration 37/1000 | Loss: 0.00001270
Iteration 38/1000 | Loss: 0.00001269
Iteration 39/1000 | Loss: 0.00001268
Iteration 40/1000 | Loss: 0.00001268
Iteration 41/1000 | Loss: 0.00001267
Iteration 42/1000 | Loss: 0.00001266
Iteration 43/1000 | Loss: 0.00001266
Iteration 44/1000 | Loss: 0.00001265
Iteration 45/1000 | Loss: 0.00001265
Iteration 46/1000 | Loss: 0.00001265
Iteration 47/1000 | Loss: 0.00001265
Iteration 48/1000 | Loss: 0.00001265
Iteration 49/1000 | Loss: 0.00001265
Iteration 50/1000 | Loss: 0.00001265
Iteration 51/1000 | Loss: 0.00001264
Iteration 52/1000 | Loss: 0.00001264
Iteration 53/1000 | Loss: 0.00001263
Iteration 54/1000 | Loss: 0.00001263
Iteration 55/1000 | Loss: 0.00001262
Iteration 56/1000 | Loss: 0.00001262
Iteration 57/1000 | Loss: 0.00001262
Iteration 58/1000 | Loss: 0.00001261
Iteration 59/1000 | Loss: 0.00001260
Iteration 60/1000 | Loss: 0.00001255
Iteration 61/1000 | Loss: 0.00001251
Iteration 62/1000 | Loss: 0.00001250
Iteration 63/1000 | Loss: 0.00001250
Iteration 64/1000 | Loss: 0.00001249
Iteration 65/1000 | Loss: 0.00001248
Iteration 66/1000 | Loss: 0.00001248
Iteration 67/1000 | Loss: 0.00001248
Iteration 68/1000 | Loss: 0.00001247
Iteration 69/1000 | Loss: 0.00001247
Iteration 70/1000 | Loss: 0.00001247
Iteration 71/1000 | Loss: 0.00001246
Iteration 72/1000 | Loss: 0.00001246
Iteration 73/1000 | Loss: 0.00001246
Iteration 74/1000 | Loss: 0.00001245
Iteration 75/1000 | Loss: 0.00001245
Iteration 76/1000 | Loss: 0.00001245
Iteration 77/1000 | Loss: 0.00001244
Iteration 78/1000 | Loss: 0.00001244
Iteration 79/1000 | Loss: 0.00001244
Iteration 80/1000 | Loss: 0.00001243
Iteration 81/1000 | Loss: 0.00001243
Iteration 82/1000 | Loss: 0.00001243
Iteration 83/1000 | Loss: 0.00001243
Iteration 84/1000 | Loss: 0.00001243
Iteration 85/1000 | Loss: 0.00001243
Iteration 86/1000 | Loss: 0.00001243
Iteration 87/1000 | Loss: 0.00001242
Iteration 88/1000 | Loss: 0.00001242
Iteration 89/1000 | Loss: 0.00001242
Iteration 90/1000 | Loss: 0.00001242
Iteration 91/1000 | Loss: 0.00001241
Iteration 92/1000 | Loss: 0.00001241
Iteration 93/1000 | Loss: 0.00001241
Iteration 94/1000 | Loss: 0.00001241
Iteration 95/1000 | Loss: 0.00001241
Iteration 96/1000 | Loss: 0.00001241
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 96. Stopping optimization.
Last 5 losses: [1.2414439879648853e-05, 1.2414439879648853e-05, 1.2414439879648853e-05, 1.2414439879648853e-05, 1.2414439879648853e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2414439879648853e-05

Optimization complete. Final v2v error: 2.9826769828796387 mm

Highest mean error: 3.913870096206665 mm for frame 158

Lowest mean error: 2.5493335723876953 mm for frame 13

Saving results

Total time: 40.35388469696045
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_018/1018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_018/1018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_018/1018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00956746
Iteration 2/25 | Loss: 0.00223185
Iteration 3/25 | Loss: 0.00140672
Iteration 4/25 | Loss: 0.00132049
Iteration 5/25 | Loss: 0.00133229
Iteration 6/25 | Loss: 0.00127687
Iteration 7/25 | Loss: 0.00125040
Iteration 8/25 | Loss: 0.00123555
Iteration 9/25 | Loss: 0.00122317
Iteration 10/25 | Loss: 0.00121823
Iteration 11/25 | Loss: 0.00121677
Iteration 12/25 | Loss: 0.00121801
Iteration 13/25 | Loss: 0.00121726
Iteration 14/25 | Loss: 0.00121987
Iteration 15/25 | Loss: 0.00121676
Iteration 16/25 | Loss: 0.00121343
Iteration 17/25 | Loss: 0.00121259
Iteration 18/25 | Loss: 0.00121525
Iteration 19/25 | Loss: 0.00121586
Iteration 20/25 | Loss: 0.00121506
Iteration 21/25 | Loss: 0.00121328
Iteration 22/25 | Loss: 0.00121461
Iteration 23/25 | Loss: 0.00121428
Iteration 24/25 | Loss: 0.00121425
Iteration 25/25 | Loss: 0.00121282

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35698926
Iteration 2/25 | Loss: 0.00067289
Iteration 3/25 | Loss: 0.00067289
Iteration 4/25 | Loss: 0.00067288
Iteration 5/25 | Loss: 0.00067288
Iteration 6/25 | Loss: 0.00067288
Iteration 7/25 | Loss: 0.00067288
Iteration 8/25 | Loss: 0.00067288
Iteration 9/25 | Loss: 0.00067288
Iteration 10/25 | Loss: 0.00067288
Iteration 11/25 | Loss: 0.00067288
Iteration 12/25 | Loss: 0.00067288
Iteration 13/25 | Loss: 0.00067288
Iteration 14/25 | Loss: 0.00067288
Iteration 15/25 | Loss: 0.00067288
Iteration 16/25 | Loss: 0.00067288
Iteration 17/25 | Loss: 0.00067288
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.000672882073558867, 0.000672882073558867, 0.000672882073558867, 0.000672882073558867, 0.000672882073558867]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000672882073558867

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00067288
Iteration 2/1000 | Loss: 0.00003813
Iteration 3/1000 | Loss: 0.00002375
Iteration 4/1000 | Loss: 0.00006725
Iteration 5/1000 | Loss: 0.00003774
Iteration 6/1000 | Loss: 0.00001971
Iteration 7/1000 | Loss: 0.00007074
Iteration 8/1000 | Loss: 0.00001839
Iteration 9/1000 | Loss: 0.00001776
Iteration 10/1000 | Loss: 0.00001740
Iteration 11/1000 | Loss: 0.00001703
Iteration 12/1000 | Loss: 0.00016220
Iteration 13/1000 | Loss: 0.00005924
Iteration 14/1000 | Loss: 0.00004198
Iteration 15/1000 | Loss: 0.00004306
Iteration 16/1000 | Loss: 0.00005571
Iteration 17/1000 | Loss: 0.00008681
Iteration 18/1000 | Loss: 0.00006601
Iteration 19/1000 | Loss: 0.00011116
Iteration 20/1000 | Loss: 0.00007587
Iteration 21/1000 | Loss: 0.00007562
Iteration 22/1000 | Loss: 0.00010768
Iteration 23/1000 | Loss: 0.00008660
Iteration 24/1000 | Loss: 0.00004027
Iteration 25/1000 | Loss: 0.00008254
Iteration 26/1000 | Loss: 0.00009040
Iteration 27/1000 | Loss: 0.00005390
Iteration 28/1000 | Loss: 0.00007533
Iteration 29/1000 | Loss: 0.00010154
Iteration 30/1000 | Loss: 0.00006375
Iteration 31/1000 | Loss: 0.00004162
Iteration 32/1000 | Loss: 0.00008149
Iteration 33/1000 | Loss: 0.00007288
Iteration 34/1000 | Loss: 0.00006583
Iteration 35/1000 | Loss: 0.00001800
Iteration 36/1000 | Loss: 0.00001719
Iteration 37/1000 | Loss: 0.00012060
Iteration 38/1000 | Loss: 0.00002874
Iteration 39/1000 | Loss: 0.00002591
Iteration 40/1000 | Loss: 0.00001806
Iteration 41/1000 | Loss: 0.00001743
Iteration 42/1000 | Loss: 0.00001725
Iteration 43/1000 | Loss: 0.00017883
Iteration 44/1000 | Loss: 0.00007451
Iteration 45/1000 | Loss: 0.00010310
Iteration 46/1000 | Loss: 0.00008777
Iteration 47/1000 | Loss: 0.00009323
Iteration 48/1000 | Loss: 0.00002338
Iteration 49/1000 | Loss: 0.00001899
Iteration 50/1000 | Loss: 0.00001730
Iteration 51/1000 | Loss: 0.00001645
Iteration 52/1000 | Loss: 0.00001598
Iteration 53/1000 | Loss: 0.00001578
Iteration 54/1000 | Loss: 0.00001554
Iteration 55/1000 | Loss: 0.00001550
Iteration 56/1000 | Loss: 0.00001544
Iteration 57/1000 | Loss: 0.00001536
Iteration 58/1000 | Loss: 0.00001534
Iteration 59/1000 | Loss: 0.00001534
Iteration 60/1000 | Loss: 0.00001533
Iteration 61/1000 | Loss: 0.00001533
Iteration 62/1000 | Loss: 0.00001532
Iteration 63/1000 | Loss: 0.00001531
Iteration 64/1000 | Loss: 0.00001530
Iteration 65/1000 | Loss: 0.00001529
Iteration 66/1000 | Loss: 0.00001529
Iteration 67/1000 | Loss: 0.00001528
Iteration 68/1000 | Loss: 0.00001527
Iteration 69/1000 | Loss: 0.00001527
Iteration 70/1000 | Loss: 0.00001527
Iteration 71/1000 | Loss: 0.00001526
Iteration 72/1000 | Loss: 0.00001526
Iteration 73/1000 | Loss: 0.00001526
Iteration 74/1000 | Loss: 0.00001525
Iteration 75/1000 | Loss: 0.00001525
Iteration 76/1000 | Loss: 0.00001525
Iteration 77/1000 | Loss: 0.00001525
Iteration 78/1000 | Loss: 0.00001525
Iteration 79/1000 | Loss: 0.00001525
Iteration 80/1000 | Loss: 0.00001525
Iteration 81/1000 | Loss: 0.00001524
Iteration 82/1000 | Loss: 0.00001524
Iteration 83/1000 | Loss: 0.00001524
Iteration 84/1000 | Loss: 0.00001523
Iteration 85/1000 | Loss: 0.00001523
Iteration 86/1000 | Loss: 0.00001523
Iteration 87/1000 | Loss: 0.00001522
Iteration 88/1000 | Loss: 0.00001522
Iteration 89/1000 | Loss: 0.00001522
Iteration 90/1000 | Loss: 0.00001522
Iteration 91/1000 | Loss: 0.00001521
Iteration 92/1000 | Loss: 0.00001521
Iteration 93/1000 | Loss: 0.00001521
Iteration 94/1000 | Loss: 0.00001520
Iteration 95/1000 | Loss: 0.00001520
Iteration 96/1000 | Loss: 0.00001520
Iteration 97/1000 | Loss: 0.00001520
Iteration 98/1000 | Loss: 0.00001520
Iteration 99/1000 | Loss: 0.00001519
Iteration 100/1000 | Loss: 0.00001519
Iteration 101/1000 | Loss: 0.00001519
Iteration 102/1000 | Loss: 0.00001519
Iteration 103/1000 | Loss: 0.00001519
Iteration 104/1000 | Loss: 0.00001519
Iteration 105/1000 | Loss: 0.00001519
Iteration 106/1000 | Loss: 0.00001519
Iteration 107/1000 | Loss: 0.00001518
Iteration 108/1000 | Loss: 0.00001518
Iteration 109/1000 | Loss: 0.00001518
Iteration 110/1000 | Loss: 0.00001517
Iteration 111/1000 | Loss: 0.00001517
Iteration 112/1000 | Loss: 0.00001517
Iteration 113/1000 | Loss: 0.00001517
Iteration 114/1000 | Loss: 0.00001517
Iteration 115/1000 | Loss: 0.00001517
Iteration 116/1000 | Loss: 0.00001517
Iteration 117/1000 | Loss: 0.00001517
Iteration 118/1000 | Loss: 0.00001517
Iteration 119/1000 | Loss: 0.00001517
Iteration 120/1000 | Loss: 0.00001517
Iteration 121/1000 | Loss: 0.00001517
Iteration 122/1000 | Loss: 0.00001516
Iteration 123/1000 | Loss: 0.00001516
Iteration 124/1000 | Loss: 0.00001516
Iteration 125/1000 | Loss: 0.00001516
Iteration 126/1000 | Loss: 0.00001516
Iteration 127/1000 | Loss: 0.00001516
Iteration 128/1000 | Loss: 0.00001516
Iteration 129/1000 | Loss: 0.00001516
Iteration 130/1000 | Loss: 0.00001516
Iteration 131/1000 | Loss: 0.00001516
Iteration 132/1000 | Loss: 0.00001515
Iteration 133/1000 | Loss: 0.00001515
Iteration 134/1000 | Loss: 0.00001515
Iteration 135/1000 | Loss: 0.00001514
Iteration 136/1000 | Loss: 0.00001514
Iteration 137/1000 | Loss: 0.00001514
Iteration 138/1000 | Loss: 0.00001514
Iteration 139/1000 | Loss: 0.00001514
Iteration 140/1000 | Loss: 0.00001514
Iteration 141/1000 | Loss: 0.00001514
Iteration 142/1000 | Loss: 0.00001514
Iteration 143/1000 | Loss: 0.00001514
Iteration 144/1000 | Loss: 0.00001514
Iteration 145/1000 | Loss: 0.00001514
Iteration 146/1000 | Loss: 0.00001514
Iteration 147/1000 | Loss: 0.00001514
Iteration 148/1000 | Loss: 0.00001514
Iteration 149/1000 | Loss: 0.00001514
Iteration 150/1000 | Loss: 0.00001514
Iteration 151/1000 | Loss: 0.00001514
Iteration 152/1000 | Loss: 0.00001514
Iteration 153/1000 | Loss: 0.00001514
Iteration 154/1000 | Loss: 0.00001514
Iteration 155/1000 | Loss: 0.00001514
Iteration 156/1000 | Loss: 0.00001514
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 156. Stopping optimization.
Last 5 losses: [1.5138548405957408e-05, 1.5138548405957408e-05, 1.5138548405957408e-05, 1.5138548405957408e-05, 1.5138548405957408e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5138548405957408e-05

Optimization complete. Final v2v error: 3.2559096813201904 mm

Highest mean error: 4.683253288269043 mm for frame 160

Lowest mean error: 2.7513723373413086 mm for frame 208

Saving results

Total time: 151.39478731155396
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_018/1034/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_018/1034.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_018/1034
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01040870
Iteration 2/25 | Loss: 0.00293230
Iteration 3/25 | Loss: 0.00235601
Iteration 4/25 | Loss: 0.00186787
Iteration 5/25 | Loss: 0.00170332
Iteration 6/25 | Loss: 0.00163255
Iteration 7/25 | Loss: 0.00146745
Iteration 8/25 | Loss: 0.00139325
Iteration 9/25 | Loss: 0.00131026
Iteration 10/25 | Loss: 0.00128161
Iteration 11/25 | Loss: 0.00127493
Iteration 12/25 | Loss: 0.00126478
Iteration 13/25 | Loss: 0.00127624
Iteration 14/25 | Loss: 0.00122076
Iteration 15/25 | Loss: 0.00121541
Iteration 16/25 | Loss: 0.00121445
Iteration 17/25 | Loss: 0.00123003
Iteration 18/25 | Loss: 0.00120019
Iteration 19/25 | Loss: 0.00119843
Iteration 20/25 | Loss: 0.00119822
Iteration 21/25 | Loss: 0.00119810
Iteration 22/25 | Loss: 0.00119784
Iteration 23/25 | Loss: 0.00119680
Iteration 24/25 | Loss: 0.00119923
Iteration 25/25 | Loss: 0.00119692

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.31963742
Iteration 2/25 | Loss: 0.00886063
Iteration 3/25 | Loss: 0.00130425
Iteration 4/25 | Loss: 0.00130325
Iteration 5/25 | Loss: 0.00130325
Iteration 6/25 | Loss: 0.00130325
Iteration 7/25 | Loss: 0.00130325
Iteration 8/25 | Loss: 0.00130325
Iteration 9/25 | Loss: 0.00130325
Iteration 10/25 | Loss: 0.00130325
Iteration 11/25 | Loss: 0.00130325
Iteration 12/25 | Loss: 0.00130325
Iteration 13/25 | Loss: 0.00130325
Iteration 14/25 | Loss: 0.00130325
Iteration 15/25 | Loss: 0.00130325
Iteration 16/25 | Loss: 0.00130325
Iteration 17/25 | Loss: 0.00130325
Iteration 18/25 | Loss: 0.00130325
Iteration 19/25 | Loss: 0.00130325
Iteration 20/25 | Loss: 0.00130325
Iteration 21/25 | Loss: 0.00130325
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0013032486895099282, 0.0013032486895099282, 0.0013032486895099282, 0.0013032486895099282, 0.0013032486895099282]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013032486895099282

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00130325
Iteration 2/1000 | Loss: 0.00019689
Iteration 3/1000 | Loss: 0.00006179
Iteration 4/1000 | Loss: 0.00005548
Iteration 5/1000 | Loss: 0.00004776
Iteration 6/1000 | Loss: 0.00004425
Iteration 7/1000 | Loss: 0.00183368
Iteration 8/1000 | Loss: 0.00004755
Iteration 9/1000 | Loss: 0.00004161
Iteration 10/1000 | Loss: 0.00004033
Iteration 11/1000 | Loss: 0.00237118
Iteration 12/1000 | Loss: 0.00976186
Iteration 13/1000 | Loss: 0.01013946
Iteration 14/1000 | Loss: 0.00743950
Iteration 15/1000 | Loss: 0.00145941
Iteration 16/1000 | Loss: 0.00106206
Iteration 17/1000 | Loss: 0.00310161
Iteration 18/1000 | Loss: 0.00290478
Iteration 19/1000 | Loss: 0.00062040
Iteration 20/1000 | Loss: 0.00056225
Iteration 21/1000 | Loss: 0.00088707
Iteration 22/1000 | Loss: 0.00050487
Iteration 23/1000 | Loss: 0.00064234
Iteration 24/1000 | Loss: 0.00027017
Iteration 25/1000 | Loss: 0.00042038
Iteration 26/1000 | Loss: 0.00048249
Iteration 27/1000 | Loss: 0.00003604
Iteration 28/1000 | Loss: 0.00003256
Iteration 29/1000 | Loss: 0.00067641
Iteration 30/1000 | Loss: 0.00044799
Iteration 31/1000 | Loss: 0.00004246
Iteration 32/1000 | Loss: 0.00011782
Iteration 33/1000 | Loss: 0.00013707
Iteration 34/1000 | Loss: 0.00015717
Iteration 35/1000 | Loss: 0.00003114
Iteration 36/1000 | Loss: 0.00027832
Iteration 37/1000 | Loss: 0.00039143
Iteration 38/1000 | Loss: 0.00030566
Iteration 39/1000 | Loss: 0.00007513
Iteration 40/1000 | Loss: 0.00009956
Iteration 41/1000 | Loss: 0.00003352
Iteration 42/1000 | Loss: 0.00022167
Iteration 43/1000 | Loss: 0.00003656
Iteration 44/1000 | Loss: 0.00002894
Iteration 45/1000 | Loss: 0.00002327
Iteration 46/1000 | Loss: 0.00002157
Iteration 47/1000 | Loss: 0.00002047
Iteration 48/1000 | Loss: 0.00001981
Iteration 49/1000 | Loss: 0.00029727
Iteration 50/1000 | Loss: 0.00002036
Iteration 51/1000 | Loss: 0.00001865
Iteration 52/1000 | Loss: 0.00001822
Iteration 53/1000 | Loss: 0.00001791
Iteration 54/1000 | Loss: 0.00001785
Iteration 55/1000 | Loss: 0.00001761
Iteration 56/1000 | Loss: 0.00001741
Iteration 57/1000 | Loss: 0.00001741
Iteration 58/1000 | Loss: 0.00001730
Iteration 59/1000 | Loss: 0.00001727
Iteration 60/1000 | Loss: 0.00001727
Iteration 61/1000 | Loss: 0.00001726
Iteration 62/1000 | Loss: 0.00001725
Iteration 63/1000 | Loss: 0.00001725
Iteration 64/1000 | Loss: 0.00001724
Iteration 65/1000 | Loss: 0.00001723
Iteration 66/1000 | Loss: 0.00001723
Iteration 67/1000 | Loss: 0.00001722
Iteration 68/1000 | Loss: 0.00001722
Iteration 69/1000 | Loss: 0.00001719
Iteration 70/1000 | Loss: 0.00001719
Iteration 71/1000 | Loss: 0.00001717
Iteration 72/1000 | Loss: 0.00001717
Iteration 73/1000 | Loss: 0.00001716
Iteration 74/1000 | Loss: 0.00001716
Iteration 75/1000 | Loss: 0.00001716
Iteration 76/1000 | Loss: 0.00001716
Iteration 77/1000 | Loss: 0.00001715
Iteration 78/1000 | Loss: 0.00001715
Iteration 79/1000 | Loss: 0.00001715
Iteration 80/1000 | Loss: 0.00001715
Iteration 81/1000 | Loss: 0.00001715
Iteration 82/1000 | Loss: 0.00001714
Iteration 83/1000 | Loss: 0.00001714
Iteration 84/1000 | Loss: 0.00001714
Iteration 85/1000 | Loss: 0.00001714
Iteration 86/1000 | Loss: 0.00001714
Iteration 87/1000 | Loss: 0.00001714
Iteration 88/1000 | Loss: 0.00001714
Iteration 89/1000 | Loss: 0.00001714
Iteration 90/1000 | Loss: 0.00001713
Iteration 91/1000 | Loss: 0.00001713
Iteration 92/1000 | Loss: 0.00001712
Iteration 93/1000 | Loss: 0.00001711
Iteration 94/1000 | Loss: 0.00001711
Iteration 95/1000 | Loss: 0.00001711
Iteration 96/1000 | Loss: 0.00001711
Iteration 97/1000 | Loss: 0.00001710
Iteration 98/1000 | Loss: 0.00001710
Iteration 99/1000 | Loss: 0.00001709
Iteration 100/1000 | Loss: 0.00001709
Iteration 101/1000 | Loss: 0.00001709
Iteration 102/1000 | Loss: 0.00001709
Iteration 103/1000 | Loss: 0.00001709
Iteration 104/1000 | Loss: 0.00001708
Iteration 105/1000 | Loss: 0.00001708
Iteration 106/1000 | Loss: 0.00001708
Iteration 107/1000 | Loss: 0.00001708
Iteration 108/1000 | Loss: 0.00001707
Iteration 109/1000 | Loss: 0.00001707
Iteration 110/1000 | Loss: 0.00001707
Iteration 111/1000 | Loss: 0.00001707
Iteration 112/1000 | Loss: 0.00001706
Iteration 113/1000 | Loss: 0.00001706
Iteration 114/1000 | Loss: 0.00001706
Iteration 115/1000 | Loss: 0.00001705
Iteration 116/1000 | Loss: 0.00001705
Iteration 117/1000 | Loss: 0.00001705
Iteration 118/1000 | Loss: 0.00001705
Iteration 119/1000 | Loss: 0.00001705
Iteration 120/1000 | Loss: 0.00001705
Iteration 121/1000 | Loss: 0.00001705
Iteration 122/1000 | Loss: 0.00001705
Iteration 123/1000 | Loss: 0.00001705
Iteration 124/1000 | Loss: 0.00001704
Iteration 125/1000 | Loss: 0.00001704
Iteration 126/1000 | Loss: 0.00001704
Iteration 127/1000 | Loss: 0.00001704
Iteration 128/1000 | Loss: 0.00001704
Iteration 129/1000 | Loss: 0.00001704
Iteration 130/1000 | Loss: 0.00001704
Iteration 131/1000 | Loss: 0.00001703
Iteration 132/1000 | Loss: 0.00001703
Iteration 133/1000 | Loss: 0.00001703
Iteration 134/1000 | Loss: 0.00001703
Iteration 135/1000 | Loss: 0.00001702
Iteration 136/1000 | Loss: 0.00001702
Iteration 137/1000 | Loss: 0.00001702
Iteration 138/1000 | Loss: 0.00001701
Iteration 139/1000 | Loss: 0.00001701
Iteration 140/1000 | Loss: 0.00001701
Iteration 141/1000 | Loss: 0.00001701
Iteration 142/1000 | Loss: 0.00001701
Iteration 143/1000 | Loss: 0.00001701
Iteration 144/1000 | Loss: 0.00001701
Iteration 145/1000 | Loss: 0.00001701
Iteration 146/1000 | Loss: 0.00001701
Iteration 147/1000 | Loss: 0.00001701
Iteration 148/1000 | Loss: 0.00001701
Iteration 149/1000 | Loss: 0.00001701
Iteration 150/1000 | Loss: 0.00001700
Iteration 151/1000 | Loss: 0.00001700
Iteration 152/1000 | Loss: 0.00001700
Iteration 153/1000 | Loss: 0.00001700
Iteration 154/1000 | Loss: 0.00001700
Iteration 155/1000 | Loss: 0.00001700
Iteration 156/1000 | Loss: 0.00001700
Iteration 157/1000 | Loss: 0.00001700
Iteration 158/1000 | Loss: 0.00001700
Iteration 159/1000 | Loss: 0.00001699
Iteration 160/1000 | Loss: 0.00001699
Iteration 161/1000 | Loss: 0.00001699
Iteration 162/1000 | Loss: 0.00001699
Iteration 163/1000 | Loss: 0.00001699
Iteration 164/1000 | Loss: 0.00001699
Iteration 165/1000 | Loss: 0.00001699
Iteration 166/1000 | Loss: 0.00001699
Iteration 167/1000 | Loss: 0.00001699
Iteration 168/1000 | Loss: 0.00001698
Iteration 169/1000 | Loss: 0.00001698
Iteration 170/1000 | Loss: 0.00001698
Iteration 171/1000 | Loss: 0.00001698
Iteration 172/1000 | Loss: 0.00001698
Iteration 173/1000 | Loss: 0.00001698
Iteration 174/1000 | Loss: 0.00001698
Iteration 175/1000 | Loss: 0.00001698
Iteration 176/1000 | Loss: 0.00001698
Iteration 177/1000 | Loss: 0.00001698
Iteration 178/1000 | Loss: 0.00001697
Iteration 179/1000 | Loss: 0.00001697
Iteration 180/1000 | Loss: 0.00001697
Iteration 181/1000 | Loss: 0.00001697
Iteration 182/1000 | Loss: 0.00001697
Iteration 183/1000 | Loss: 0.00001697
Iteration 184/1000 | Loss: 0.00001697
Iteration 185/1000 | Loss: 0.00001697
Iteration 186/1000 | Loss: 0.00001697
Iteration 187/1000 | Loss: 0.00001697
Iteration 188/1000 | Loss: 0.00001697
Iteration 189/1000 | Loss: 0.00001697
Iteration 190/1000 | Loss: 0.00001697
Iteration 191/1000 | Loss: 0.00001697
Iteration 192/1000 | Loss: 0.00001696
Iteration 193/1000 | Loss: 0.00001696
Iteration 194/1000 | Loss: 0.00001696
Iteration 195/1000 | Loss: 0.00001696
Iteration 196/1000 | Loss: 0.00001696
Iteration 197/1000 | Loss: 0.00001696
Iteration 198/1000 | Loss: 0.00001696
Iteration 199/1000 | Loss: 0.00001696
Iteration 200/1000 | Loss: 0.00001696
Iteration 201/1000 | Loss: 0.00001696
Iteration 202/1000 | Loss: 0.00001696
Iteration 203/1000 | Loss: 0.00001696
Iteration 204/1000 | Loss: 0.00001696
Iteration 205/1000 | Loss: 0.00001695
Iteration 206/1000 | Loss: 0.00001695
Iteration 207/1000 | Loss: 0.00001695
Iteration 208/1000 | Loss: 0.00001695
Iteration 209/1000 | Loss: 0.00001695
Iteration 210/1000 | Loss: 0.00001695
Iteration 211/1000 | Loss: 0.00001695
Iteration 212/1000 | Loss: 0.00001695
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 212. Stopping optimization.
Last 5 losses: [1.6954814782366157e-05, 1.6954814782366157e-05, 1.6954814782366157e-05, 1.6954814782366157e-05, 1.6954814782366157e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6954814782366157e-05

Optimization complete. Final v2v error: 3.502732038497925 mm

Highest mean error: 4.287975311279297 mm for frame 51

Lowest mean error: 2.9585139751434326 mm for frame 1

Saving results

Total time: 144.39482736587524
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_018/1008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_018/1008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_018/1008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00994089
Iteration 2/25 | Loss: 0.00340968
Iteration 3/25 | Loss: 0.00215294
Iteration 4/25 | Loss: 0.00199377
Iteration 5/25 | Loss: 0.00182807
Iteration 6/25 | Loss: 0.00183648
Iteration 7/25 | Loss: 0.00175364
Iteration 8/25 | Loss: 0.00164789
Iteration 9/25 | Loss: 0.00156299
Iteration 10/25 | Loss: 0.00154529
Iteration 11/25 | Loss: 0.00155228
Iteration 12/25 | Loss: 0.00151026
Iteration 13/25 | Loss: 0.00146314
Iteration 14/25 | Loss: 0.00145261
Iteration 15/25 | Loss: 0.00145298
Iteration 16/25 | Loss: 0.00145853
Iteration 17/25 | Loss: 0.00143428
Iteration 18/25 | Loss: 0.00142749
Iteration 19/25 | Loss: 0.00141960
Iteration 20/25 | Loss: 0.00142254
Iteration 21/25 | Loss: 0.00141134
Iteration 22/25 | Loss: 0.00140857
Iteration 23/25 | Loss: 0.00141048
Iteration 24/25 | Loss: 0.00141076
Iteration 25/25 | Loss: 0.00141071

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35570061
Iteration 2/25 | Loss: 0.00375829
Iteration 3/25 | Loss: 0.00317726
Iteration 4/25 | Loss: 0.00317726
Iteration 5/25 | Loss: 0.00317726
Iteration 6/25 | Loss: 0.00317726
Iteration 7/25 | Loss: 0.00317726
Iteration 8/25 | Loss: 0.00317726
Iteration 9/25 | Loss: 0.00317726
Iteration 10/25 | Loss: 0.00317726
Iteration 11/25 | Loss: 0.00317726
Iteration 12/25 | Loss: 0.00317726
Iteration 13/25 | Loss: 0.00317726
Iteration 14/25 | Loss: 0.00317726
Iteration 15/25 | Loss: 0.00317726
Iteration 16/25 | Loss: 0.00317726
Iteration 17/25 | Loss: 0.00317726
Iteration 18/25 | Loss: 0.00317726
Iteration 19/25 | Loss: 0.00317726
Iteration 20/25 | Loss: 0.00317726
Iteration 21/25 | Loss: 0.00317726
Iteration 22/25 | Loss: 0.00317726
Iteration 23/25 | Loss: 0.00317726
Iteration 24/25 | Loss: 0.00317726
Iteration 25/25 | Loss: 0.00317726

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00317726
Iteration 2/1000 | Loss: 0.00082394
Iteration 3/1000 | Loss: 0.00061029
Iteration 4/1000 | Loss: 0.00101098
Iteration 5/1000 | Loss: 0.00116734
Iteration 6/1000 | Loss: 0.00058664
Iteration 7/1000 | Loss: 0.00077756
Iteration 8/1000 | Loss: 0.00088789
Iteration 9/1000 | Loss: 0.00208872
Iteration 10/1000 | Loss: 0.00242035
Iteration 11/1000 | Loss: 0.00201416
Iteration 12/1000 | Loss: 0.00296864
Iteration 13/1000 | Loss: 0.00170821
Iteration 14/1000 | Loss: 0.00023379
Iteration 15/1000 | Loss: 0.00041464
Iteration 16/1000 | Loss: 0.00078254
Iteration 17/1000 | Loss: 0.00130558
Iteration 18/1000 | Loss: 0.00021735
Iteration 19/1000 | Loss: 0.00023573
Iteration 20/1000 | Loss: 0.00018711
Iteration 21/1000 | Loss: 0.00029972
Iteration 22/1000 | Loss: 0.00024836
Iteration 23/1000 | Loss: 0.00014606
Iteration 24/1000 | Loss: 0.00105143
Iteration 25/1000 | Loss: 0.00026482
Iteration 26/1000 | Loss: 0.00019105
Iteration 27/1000 | Loss: 0.00018190
Iteration 28/1000 | Loss: 0.00016860
Iteration 29/1000 | Loss: 0.00015650
Iteration 30/1000 | Loss: 0.00040674
Iteration 31/1000 | Loss: 0.00058175
Iteration 32/1000 | Loss: 0.00035886
Iteration 33/1000 | Loss: 0.00018774
Iteration 34/1000 | Loss: 0.00018283
Iteration 35/1000 | Loss: 0.00016109
Iteration 36/1000 | Loss: 0.00015561
Iteration 37/1000 | Loss: 0.00017793
Iteration 38/1000 | Loss: 0.00032848
Iteration 39/1000 | Loss: 0.00031705
Iteration 40/1000 | Loss: 0.00028663
Iteration 41/1000 | Loss: 0.00014152
Iteration 42/1000 | Loss: 0.00012340
Iteration 43/1000 | Loss: 0.00012375
Iteration 44/1000 | Loss: 0.00014536
Iteration 45/1000 | Loss: 0.00012705
Iteration 46/1000 | Loss: 0.00011916
Iteration 47/1000 | Loss: 0.00012680
Iteration 48/1000 | Loss: 0.00011821
Iteration 49/1000 | Loss: 0.00014138
Iteration 50/1000 | Loss: 0.00034642
Iteration 51/1000 | Loss: 0.00027872
Iteration 52/1000 | Loss: 0.00028495
Iteration 53/1000 | Loss: 0.00058739
Iteration 54/1000 | Loss: 0.00044527
Iteration 55/1000 | Loss: 0.00023736
Iteration 56/1000 | Loss: 0.00037893
Iteration 57/1000 | Loss: 0.00036395
Iteration 58/1000 | Loss: 0.00026037
Iteration 59/1000 | Loss: 0.00044386
Iteration 60/1000 | Loss: 0.00028349
Iteration 61/1000 | Loss: 0.00021924
Iteration 62/1000 | Loss: 0.00037553
Iteration 63/1000 | Loss: 0.00026266
Iteration 64/1000 | Loss: 0.00030357
Iteration 65/1000 | Loss: 0.00022363
Iteration 66/1000 | Loss: 0.00012953
Iteration 67/1000 | Loss: 0.00016667
Iteration 68/1000 | Loss: 0.00021807
Iteration 69/1000 | Loss: 0.00015629
Iteration 70/1000 | Loss: 0.00015040
Iteration 71/1000 | Loss: 0.00016128
Iteration 72/1000 | Loss: 0.00012274
Iteration 73/1000 | Loss: 0.00011463
Iteration 74/1000 | Loss: 0.00024409
Iteration 75/1000 | Loss: 0.00050073
Iteration 76/1000 | Loss: 0.00011835
Iteration 77/1000 | Loss: 0.00023672
Iteration 78/1000 | Loss: 0.00026061
Iteration 79/1000 | Loss: 0.00035336
Iteration 80/1000 | Loss: 0.00018814
Iteration 81/1000 | Loss: 0.00036439
Iteration 82/1000 | Loss: 0.00017861
Iteration 83/1000 | Loss: 0.00032409
Iteration 84/1000 | Loss: 0.00022071
Iteration 85/1000 | Loss: 0.00029679
Iteration 86/1000 | Loss: 0.00037139
Iteration 87/1000 | Loss: 0.00022499
Iteration 88/1000 | Loss: 0.00026202
Iteration 89/1000 | Loss: 0.00027976
Iteration 90/1000 | Loss: 0.00034724
Iteration 91/1000 | Loss: 0.00012460
Iteration 92/1000 | Loss: 0.00011925
Iteration 93/1000 | Loss: 0.00028353
Iteration 94/1000 | Loss: 0.00029968
Iteration 95/1000 | Loss: 0.00012693
Iteration 96/1000 | Loss: 0.00012198
Iteration 97/1000 | Loss: 0.00011010
Iteration 98/1000 | Loss: 0.00011346
Iteration 99/1000 | Loss: 0.00010405
Iteration 100/1000 | Loss: 0.00012745
Iteration 101/1000 | Loss: 0.00010749
Iteration 102/1000 | Loss: 0.00012846
Iteration 103/1000 | Loss: 0.00010592
Iteration 104/1000 | Loss: 0.00009907
Iteration 105/1000 | Loss: 0.00010110
Iteration 106/1000 | Loss: 0.00011671
Iteration 107/1000 | Loss: 0.00009820
Iteration 108/1000 | Loss: 0.00010348
Iteration 109/1000 | Loss: 0.00009827
Iteration 110/1000 | Loss: 0.00009632
Iteration 111/1000 | Loss: 0.00010773
Iteration 112/1000 | Loss: 0.00009521
Iteration 113/1000 | Loss: 0.00009574
Iteration 114/1000 | Loss: 0.00023770
Iteration 115/1000 | Loss: 0.00106072
Iteration 116/1000 | Loss: 0.00427564
Iteration 117/1000 | Loss: 0.00346051
Iteration 118/1000 | Loss: 0.00452912
Iteration 119/1000 | Loss: 0.00130240
Iteration 120/1000 | Loss: 0.00123741
Iteration 121/1000 | Loss: 0.00076359
Iteration 122/1000 | Loss: 0.00064121
Iteration 123/1000 | Loss: 0.00033169
Iteration 124/1000 | Loss: 0.00032834
Iteration 125/1000 | Loss: 0.00015001
Iteration 126/1000 | Loss: 0.00014414
Iteration 127/1000 | Loss: 0.00017188
Iteration 128/1000 | Loss: 0.00016710
Iteration 129/1000 | Loss: 0.00009532
Iteration 130/1000 | Loss: 0.00010125
Iteration 131/1000 | Loss: 0.00062896
Iteration 132/1000 | Loss: 0.00011395
Iteration 133/1000 | Loss: 0.00026076
Iteration 134/1000 | Loss: 0.00007915
Iteration 135/1000 | Loss: 0.00010088
Iteration 136/1000 | Loss: 0.00006248
Iteration 137/1000 | Loss: 0.00009685
Iteration 138/1000 | Loss: 0.00005966
Iteration 139/1000 | Loss: 0.00007548
Iteration 140/1000 | Loss: 0.00005015
Iteration 141/1000 | Loss: 0.00005335
Iteration 142/1000 | Loss: 0.00004494
Iteration 143/1000 | Loss: 0.00005790
Iteration 144/1000 | Loss: 0.00006279
Iteration 145/1000 | Loss: 0.00004169
Iteration 146/1000 | Loss: 0.00004668
Iteration 147/1000 | Loss: 0.00004235
Iteration 148/1000 | Loss: 0.00005761
Iteration 149/1000 | Loss: 0.00004034
Iteration 150/1000 | Loss: 0.00003910
Iteration 151/1000 | Loss: 0.00003868
Iteration 152/1000 | Loss: 0.00004506
Iteration 153/1000 | Loss: 0.00004171
Iteration 154/1000 | Loss: 0.00004334
Iteration 155/1000 | Loss: 0.00004138
Iteration 156/1000 | Loss: 0.00003944
Iteration 157/1000 | Loss: 0.00003765
Iteration 158/1000 | Loss: 0.00004945
Iteration 159/1000 | Loss: 0.00003723
Iteration 160/1000 | Loss: 0.00003657
Iteration 161/1000 | Loss: 0.00022238
Iteration 162/1000 | Loss: 0.00005258
Iteration 163/1000 | Loss: 0.00004406
Iteration 164/1000 | Loss: 0.00004040
Iteration 165/1000 | Loss: 0.00003829
Iteration 166/1000 | Loss: 0.00003779
Iteration 167/1000 | Loss: 0.00003687
Iteration 168/1000 | Loss: 0.00003589
Iteration 169/1000 | Loss: 0.00003540
Iteration 170/1000 | Loss: 0.00003494
Iteration 171/1000 | Loss: 0.00003937
Iteration 172/1000 | Loss: 0.00003473
Iteration 173/1000 | Loss: 0.00003421
Iteration 174/1000 | Loss: 0.00003421
Iteration 175/1000 | Loss: 0.00003450
Iteration 176/1000 | Loss: 0.00003404
Iteration 177/1000 | Loss: 0.00003404
Iteration 178/1000 | Loss: 0.00003404
Iteration 179/1000 | Loss: 0.00003404
Iteration 180/1000 | Loss: 0.00003404
Iteration 181/1000 | Loss: 0.00003404
Iteration 182/1000 | Loss: 0.00003404
Iteration 183/1000 | Loss: 0.00003404
Iteration 184/1000 | Loss: 0.00003404
Iteration 185/1000 | Loss: 0.00003403
Iteration 186/1000 | Loss: 0.00003403
Iteration 187/1000 | Loss: 0.00003403
Iteration 188/1000 | Loss: 0.00003402
Iteration 189/1000 | Loss: 0.00003402
Iteration 190/1000 | Loss: 0.00003402
Iteration 191/1000 | Loss: 0.00003401
Iteration 192/1000 | Loss: 0.00003401
Iteration 193/1000 | Loss: 0.00003401
Iteration 194/1000 | Loss: 0.00003401
Iteration 195/1000 | Loss: 0.00003400
Iteration 196/1000 | Loss: 0.00003400
Iteration 197/1000 | Loss: 0.00003464
Iteration 198/1000 | Loss: 0.00003399
Iteration 199/1000 | Loss: 0.00003397
Iteration 200/1000 | Loss: 0.00003397
Iteration 201/1000 | Loss: 0.00003396
Iteration 202/1000 | Loss: 0.00003396
Iteration 203/1000 | Loss: 0.00003396
Iteration 204/1000 | Loss: 0.00003396
Iteration 205/1000 | Loss: 0.00003396
Iteration 206/1000 | Loss: 0.00003396
Iteration 207/1000 | Loss: 0.00003396
Iteration 208/1000 | Loss: 0.00003395
Iteration 209/1000 | Loss: 0.00003395
Iteration 210/1000 | Loss: 0.00003395
Iteration 211/1000 | Loss: 0.00003395
Iteration 212/1000 | Loss: 0.00003395
Iteration 213/1000 | Loss: 0.00003395
Iteration 214/1000 | Loss: 0.00003395
Iteration 215/1000 | Loss: 0.00003395
Iteration 216/1000 | Loss: 0.00003395
Iteration 217/1000 | Loss: 0.00003395
Iteration 218/1000 | Loss: 0.00003394
Iteration 219/1000 | Loss: 0.00003394
Iteration 220/1000 | Loss: 0.00003394
Iteration 221/1000 | Loss: 0.00003394
Iteration 222/1000 | Loss: 0.00003394
Iteration 223/1000 | Loss: 0.00003393
Iteration 224/1000 | Loss: 0.00003393
Iteration 225/1000 | Loss: 0.00003393
Iteration 226/1000 | Loss: 0.00003393
Iteration 227/1000 | Loss: 0.00003393
Iteration 228/1000 | Loss: 0.00003393
Iteration 229/1000 | Loss: 0.00003393
Iteration 230/1000 | Loss: 0.00003393
Iteration 231/1000 | Loss: 0.00003393
Iteration 232/1000 | Loss: 0.00003393
Iteration 233/1000 | Loss: 0.00003392
Iteration 234/1000 | Loss: 0.00003392
Iteration 235/1000 | Loss: 0.00003392
Iteration 236/1000 | Loss: 0.00003392
Iteration 237/1000 | Loss: 0.00003392
Iteration 238/1000 | Loss: 0.00003392
Iteration 239/1000 | Loss: 0.00003391
Iteration 240/1000 | Loss: 0.00003391
Iteration 241/1000 | Loss: 0.00003391
Iteration 242/1000 | Loss: 0.00003391
Iteration 243/1000 | Loss: 0.00003391
Iteration 244/1000 | Loss: 0.00003391
Iteration 245/1000 | Loss: 0.00003390
Iteration 246/1000 | Loss: 0.00003390
Iteration 247/1000 | Loss: 0.00003390
Iteration 248/1000 | Loss: 0.00003390
Iteration 249/1000 | Loss: 0.00003390
Iteration 250/1000 | Loss: 0.00003390
Iteration 251/1000 | Loss: 0.00003390
Iteration 252/1000 | Loss: 0.00003390
Iteration 253/1000 | Loss: 0.00003390
Iteration 254/1000 | Loss: 0.00003390
Iteration 255/1000 | Loss: 0.00003390
Iteration 256/1000 | Loss: 0.00003390
Iteration 257/1000 | Loss: 0.00003390
Iteration 258/1000 | Loss: 0.00003390
Iteration 259/1000 | Loss: 0.00003390
Iteration 260/1000 | Loss: 0.00003390
Iteration 261/1000 | Loss: 0.00003389
Iteration 262/1000 | Loss: 0.00003389
Iteration 263/1000 | Loss: 0.00003389
Iteration 264/1000 | Loss: 0.00003389
Iteration 265/1000 | Loss: 0.00003389
Iteration 266/1000 | Loss: 0.00003389
Iteration 267/1000 | Loss: 0.00003389
Iteration 268/1000 | Loss: 0.00003389
Iteration 269/1000 | Loss: 0.00003389
Iteration 270/1000 | Loss: 0.00003389
Iteration 271/1000 | Loss: 0.00003389
Iteration 272/1000 | Loss: 0.00003389
Iteration 273/1000 | Loss: 0.00003389
Iteration 274/1000 | Loss: 0.00003389
Iteration 275/1000 | Loss: 0.00003389
Iteration 276/1000 | Loss: 0.00003389
Iteration 277/1000 | Loss: 0.00003389
Iteration 278/1000 | Loss: 0.00003389
Iteration 279/1000 | Loss: 0.00003389
Iteration 280/1000 | Loss: 0.00003389
Iteration 281/1000 | Loss: 0.00003389
Iteration 282/1000 | Loss: 0.00003389
Iteration 283/1000 | Loss: 0.00003389
Iteration 284/1000 | Loss: 0.00003389
Iteration 285/1000 | Loss: 0.00003389
Iteration 286/1000 | Loss: 0.00003389
Iteration 287/1000 | Loss: 0.00003389
Iteration 288/1000 | Loss: 0.00003389
Iteration 289/1000 | Loss: 0.00003389
Iteration 290/1000 | Loss: 0.00003389
Iteration 291/1000 | Loss: 0.00003389
Iteration 292/1000 | Loss: 0.00003389
Iteration 293/1000 | Loss: 0.00003389
Iteration 294/1000 | Loss: 0.00003389
Iteration 295/1000 | Loss: 0.00003389
Iteration 296/1000 | Loss: 0.00003389
Iteration 297/1000 | Loss: 0.00003389
Iteration 298/1000 | Loss: 0.00003389
Iteration 299/1000 | Loss: 0.00003389
Iteration 300/1000 | Loss: 0.00003389
Iteration 301/1000 | Loss: 0.00003389
Iteration 302/1000 | Loss: 0.00003389
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 302. Stopping optimization.
Last 5 losses: [3.3891763450810686e-05, 3.3891763450810686e-05, 3.3891763450810686e-05, 3.3891763450810686e-05, 3.3891763450810686e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.3891763450810686e-05

Optimization complete. Final v2v error: 3.49297833442688 mm

Highest mean error: 11.433405876159668 mm for frame 184

Lowest mean error: 2.514803171157837 mm for frame 162

Saving results

Total time: 345.5360164642334
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_018/1052/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_018/1052.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_018/1052
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00411602
Iteration 2/25 | Loss: 0.00120373
Iteration 3/25 | Loss: 0.00113377
Iteration 4/25 | Loss: 0.00112257
Iteration 5/25 | Loss: 0.00111917
Iteration 6/25 | Loss: 0.00111849
Iteration 7/25 | Loss: 0.00111849
Iteration 8/25 | Loss: 0.00111849
Iteration 9/25 | Loss: 0.00111849
Iteration 10/25 | Loss: 0.00111849
Iteration 11/25 | Loss: 0.00111849
Iteration 12/25 | Loss: 0.00111849
Iteration 13/25 | Loss: 0.00111849
Iteration 14/25 | Loss: 0.00111849
Iteration 15/25 | Loss: 0.00111849
Iteration 16/25 | Loss: 0.00111849
Iteration 17/25 | Loss: 0.00111849
Iteration 18/25 | Loss: 0.00111849
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.001118487911298871, 0.001118487911298871, 0.001118487911298871, 0.001118487911298871, 0.001118487911298871]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001118487911298871

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.45582819
Iteration 2/25 | Loss: 0.00081212
Iteration 3/25 | Loss: 0.00081211
Iteration 4/25 | Loss: 0.00081211
Iteration 5/25 | Loss: 0.00081211
Iteration 6/25 | Loss: 0.00081211
Iteration 7/25 | Loss: 0.00081211
Iteration 8/25 | Loss: 0.00081211
Iteration 9/25 | Loss: 0.00081211
Iteration 10/25 | Loss: 0.00081211
Iteration 11/25 | Loss: 0.00081211
Iteration 12/25 | Loss: 0.00081211
Iteration 13/25 | Loss: 0.00081211
Iteration 14/25 | Loss: 0.00081211
Iteration 15/25 | Loss: 0.00081211
Iteration 16/25 | Loss: 0.00081211
Iteration 17/25 | Loss: 0.00081211
Iteration 18/25 | Loss: 0.00081211
Iteration 19/25 | Loss: 0.00081211
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0008121102000586689, 0.0008121102000586689, 0.0008121102000586689, 0.0008121102000586689, 0.0008121102000586689]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008121102000586689

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00081211
Iteration 2/1000 | Loss: 0.00002050
Iteration 3/1000 | Loss: 0.00001389
Iteration 4/1000 | Loss: 0.00001267
Iteration 5/1000 | Loss: 0.00001207
Iteration 6/1000 | Loss: 0.00001172
Iteration 7/1000 | Loss: 0.00001140
Iteration 8/1000 | Loss: 0.00001133
Iteration 9/1000 | Loss: 0.00001115
Iteration 10/1000 | Loss: 0.00001092
Iteration 11/1000 | Loss: 0.00001092
Iteration 12/1000 | Loss: 0.00001084
Iteration 13/1000 | Loss: 0.00001083
Iteration 14/1000 | Loss: 0.00001081
Iteration 15/1000 | Loss: 0.00001073
Iteration 16/1000 | Loss: 0.00001072
Iteration 17/1000 | Loss: 0.00001072
Iteration 18/1000 | Loss: 0.00001070
Iteration 19/1000 | Loss: 0.00001065
Iteration 20/1000 | Loss: 0.00001057
Iteration 21/1000 | Loss: 0.00001056
Iteration 22/1000 | Loss: 0.00001054
Iteration 23/1000 | Loss: 0.00001054
Iteration 24/1000 | Loss: 0.00001049
Iteration 25/1000 | Loss: 0.00001049
Iteration 26/1000 | Loss: 0.00001046
Iteration 27/1000 | Loss: 0.00001045
Iteration 28/1000 | Loss: 0.00001044
Iteration 29/1000 | Loss: 0.00001043
Iteration 30/1000 | Loss: 0.00001043
Iteration 31/1000 | Loss: 0.00001042
Iteration 32/1000 | Loss: 0.00001042
Iteration 33/1000 | Loss: 0.00001042
Iteration 34/1000 | Loss: 0.00001042
Iteration 35/1000 | Loss: 0.00001042
Iteration 36/1000 | Loss: 0.00001042
Iteration 37/1000 | Loss: 0.00001041
Iteration 38/1000 | Loss: 0.00001041
Iteration 39/1000 | Loss: 0.00001041
Iteration 40/1000 | Loss: 0.00001040
Iteration 41/1000 | Loss: 0.00001040
Iteration 42/1000 | Loss: 0.00001039
Iteration 43/1000 | Loss: 0.00001039
Iteration 44/1000 | Loss: 0.00001039
Iteration 45/1000 | Loss: 0.00001038
Iteration 46/1000 | Loss: 0.00001038
Iteration 47/1000 | Loss: 0.00001038
Iteration 48/1000 | Loss: 0.00001037
Iteration 49/1000 | Loss: 0.00001036
Iteration 50/1000 | Loss: 0.00001035
Iteration 51/1000 | Loss: 0.00001035
Iteration 52/1000 | Loss: 0.00001035
Iteration 53/1000 | Loss: 0.00001034
Iteration 54/1000 | Loss: 0.00001034
Iteration 55/1000 | Loss: 0.00001034
Iteration 56/1000 | Loss: 0.00001033
Iteration 57/1000 | Loss: 0.00001033
Iteration 58/1000 | Loss: 0.00001032
Iteration 59/1000 | Loss: 0.00001032
Iteration 60/1000 | Loss: 0.00001031
Iteration 61/1000 | Loss: 0.00001031
Iteration 62/1000 | Loss: 0.00001031
Iteration 63/1000 | Loss: 0.00001031
Iteration 64/1000 | Loss: 0.00001031
Iteration 65/1000 | Loss: 0.00001031
Iteration 66/1000 | Loss: 0.00001030
Iteration 67/1000 | Loss: 0.00001030
Iteration 68/1000 | Loss: 0.00001030
Iteration 69/1000 | Loss: 0.00001030
Iteration 70/1000 | Loss: 0.00001029
Iteration 71/1000 | Loss: 0.00001029
Iteration 72/1000 | Loss: 0.00001029
Iteration 73/1000 | Loss: 0.00001029
Iteration 74/1000 | Loss: 0.00001029
Iteration 75/1000 | Loss: 0.00001029
Iteration 76/1000 | Loss: 0.00001028
Iteration 77/1000 | Loss: 0.00001028
Iteration 78/1000 | Loss: 0.00001027
Iteration 79/1000 | Loss: 0.00001027
Iteration 80/1000 | Loss: 0.00001027
Iteration 81/1000 | Loss: 0.00001027
Iteration 82/1000 | Loss: 0.00001027
Iteration 83/1000 | Loss: 0.00001027
Iteration 84/1000 | Loss: 0.00001027
Iteration 85/1000 | Loss: 0.00001027
Iteration 86/1000 | Loss: 0.00001027
Iteration 87/1000 | Loss: 0.00001027
Iteration 88/1000 | Loss: 0.00001026
Iteration 89/1000 | Loss: 0.00001026
Iteration 90/1000 | Loss: 0.00001026
Iteration 91/1000 | Loss: 0.00001026
Iteration 92/1000 | Loss: 0.00001026
Iteration 93/1000 | Loss: 0.00001025
Iteration 94/1000 | Loss: 0.00001025
Iteration 95/1000 | Loss: 0.00001025
Iteration 96/1000 | Loss: 0.00001025
Iteration 97/1000 | Loss: 0.00001025
Iteration 98/1000 | Loss: 0.00001024
Iteration 99/1000 | Loss: 0.00001024
Iteration 100/1000 | Loss: 0.00001024
Iteration 101/1000 | Loss: 0.00001024
Iteration 102/1000 | Loss: 0.00001024
Iteration 103/1000 | Loss: 0.00001024
Iteration 104/1000 | Loss: 0.00001024
Iteration 105/1000 | Loss: 0.00001023
Iteration 106/1000 | Loss: 0.00001023
Iteration 107/1000 | Loss: 0.00001023
Iteration 108/1000 | Loss: 0.00001023
Iteration 109/1000 | Loss: 0.00001022
Iteration 110/1000 | Loss: 0.00001022
Iteration 111/1000 | Loss: 0.00001022
Iteration 112/1000 | Loss: 0.00001022
Iteration 113/1000 | Loss: 0.00001022
Iteration 114/1000 | Loss: 0.00001022
Iteration 115/1000 | Loss: 0.00001022
Iteration 116/1000 | Loss: 0.00001022
Iteration 117/1000 | Loss: 0.00001022
Iteration 118/1000 | Loss: 0.00001022
Iteration 119/1000 | Loss: 0.00001022
Iteration 120/1000 | Loss: 0.00001022
Iteration 121/1000 | Loss: 0.00001022
Iteration 122/1000 | Loss: 0.00001022
Iteration 123/1000 | Loss: 0.00001022
Iteration 124/1000 | Loss: 0.00001022
Iteration 125/1000 | Loss: 0.00001021
Iteration 126/1000 | Loss: 0.00001021
Iteration 127/1000 | Loss: 0.00001020
Iteration 128/1000 | Loss: 0.00001020
Iteration 129/1000 | Loss: 0.00001020
Iteration 130/1000 | Loss: 0.00001020
Iteration 131/1000 | Loss: 0.00001020
Iteration 132/1000 | Loss: 0.00001020
Iteration 133/1000 | Loss: 0.00001019
Iteration 134/1000 | Loss: 0.00001019
Iteration 135/1000 | Loss: 0.00001019
Iteration 136/1000 | Loss: 0.00001018
Iteration 137/1000 | Loss: 0.00001018
Iteration 138/1000 | Loss: 0.00001018
Iteration 139/1000 | Loss: 0.00001018
Iteration 140/1000 | Loss: 0.00001018
Iteration 141/1000 | Loss: 0.00001018
Iteration 142/1000 | Loss: 0.00001018
Iteration 143/1000 | Loss: 0.00001018
Iteration 144/1000 | Loss: 0.00001018
Iteration 145/1000 | Loss: 0.00001017
Iteration 146/1000 | Loss: 0.00001017
Iteration 147/1000 | Loss: 0.00001017
Iteration 148/1000 | Loss: 0.00001017
Iteration 149/1000 | Loss: 0.00001017
Iteration 150/1000 | Loss: 0.00001017
Iteration 151/1000 | Loss: 0.00001016
Iteration 152/1000 | Loss: 0.00001016
Iteration 153/1000 | Loss: 0.00001016
Iteration 154/1000 | Loss: 0.00001016
Iteration 155/1000 | Loss: 0.00001016
Iteration 156/1000 | Loss: 0.00001016
Iteration 157/1000 | Loss: 0.00001015
Iteration 158/1000 | Loss: 0.00001015
Iteration 159/1000 | Loss: 0.00001015
Iteration 160/1000 | Loss: 0.00001015
Iteration 161/1000 | Loss: 0.00001015
Iteration 162/1000 | Loss: 0.00001015
Iteration 163/1000 | Loss: 0.00001015
Iteration 164/1000 | Loss: 0.00001015
Iteration 165/1000 | Loss: 0.00001015
Iteration 166/1000 | Loss: 0.00001014
Iteration 167/1000 | Loss: 0.00001014
Iteration 168/1000 | Loss: 0.00001014
Iteration 169/1000 | Loss: 0.00001014
Iteration 170/1000 | Loss: 0.00001014
Iteration 171/1000 | Loss: 0.00001013
Iteration 172/1000 | Loss: 0.00001013
Iteration 173/1000 | Loss: 0.00001013
Iteration 174/1000 | Loss: 0.00001013
Iteration 175/1000 | Loss: 0.00001013
Iteration 176/1000 | Loss: 0.00001013
Iteration 177/1000 | Loss: 0.00001013
Iteration 178/1000 | Loss: 0.00001013
Iteration 179/1000 | Loss: 0.00001013
Iteration 180/1000 | Loss: 0.00001013
Iteration 181/1000 | Loss: 0.00001013
Iteration 182/1000 | Loss: 0.00001013
Iteration 183/1000 | Loss: 0.00001013
Iteration 184/1000 | Loss: 0.00001013
Iteration 185/1000 | Loss: 0.00001013
Iteration 186/1000 | Loss: 0.00001013
Iteration 187/1000 | Loss: 0.00001013
Iteration 188/1000 | Loss: 0.00001013
Iteration 189/1000 | Loss: 0.00001013
Iteration 190/1000 | Loss: 0.00001013
Iteration 191/1000 | Loss: 0.00001013
Iteration 192/1000 | Loss: 0.00001013
Iteration 193/1000 | Loss: 0.00001013
Iteration 194/1000 | Loss: 0.00001013
Iteration 195/1000 | Loss: 0.00001013
Iteration 196/1000 | Loss: 0.00001013
Iteration 197/1000 | Loss: 0.00001013
Iteration 198/1000 | Loss: 0.00001013
Iteration 199/1000 | Loss: 0.00001013
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 199. Stopping optimization.
Last 5 losses: [1.0125902917934582e-05, 1.0125902917934582e-05, 1.0125902917934582e-05, 1.0125902917934582e-05, 1.0125902917934582e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0125902917934582e-05

Optimization complete. Final v2v error: 2.738405704498291 mm

Highest mean error: 3.1354475021362305 mm for frame 58

Lowest mean error: 2.48895263671875 mm for frame 45

Saving results

Total time: 37.89978361129761
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_018/1029/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_018/1029.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_018/1029
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00365913
Iteration 2/25 | Loss: 0.00113941
Iteration 3/25 | Loss: 0.00109857
Iteration 4/25 | Loss: 0.00109117
Iteration 5/25 | Loss: 0.00108901
Iteration 6/25 | Loss: 0.00108872
Iteration 7/25 | Loss: 0.00108872
Iteration 8/25 | Loss: 0.00108872
Iteration 9/25 | Loss: 0.00108872
Iteration 10/25 | Loss: 0.00108872
Iteration 11/25 | Loss: 0.00108872
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010887160897254944, 0.0010887160897254944, 0.0010887160897254944, 0.0010887160897254944, 0.0010887160897254944]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010887160897254944

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37775922
Iteration 2/25 | Loss: 0.00081954
Iteration 3/25 | Loss: 0.00081954
Iteration 4/25 | Loss: 0.00081954
Iteration 5/25 | Loss: 0.00081954
Iteration 6/25 | Loss: 0.00081954
Iteration 7/25 | Loss: 0.00081953
Iteration 8/25 | Loss: 0.00081953
Iteration 9/25 | Loss: 0.00081953
Iteration 10/25 | Loss: 0.00081953
Iteration 11/25 | Loss: 0.00081953
Iteration 12/25 | Loss: 0.00081953
Iteration 13/25 | Loss: 0.00081953
Iteration 14/25 | Loss: 0.00081953
Iteration 15/25 | Loss: 0.00081953
Iteration 16/25 | Loss: 0.00081953
Iteration 17/25 | Loss: 0.00081953
Iteration 18/25 | Loss: 0.00081953
Iteration 19/25 | Loss: 0.00081953
Iteration 20/25 | Loss: 0.00081953
Iteration 21/25 | Loss: 0.00081953
Iteration 22/25 | Loss: 0.00081953
Iteration 23/25 | Loss: 0.00081953
Iteration 24/25 | Loss: 0.00081953
Iteration 25/25 | Loss: 0.00081953

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00081953
Iteration 2/1000 | Loss: 0.00001856
Iteration 3/1000 | Loss: 0.00001220
Iteration 4/1000 | Loss: 0.00001089
Iteration 5/1000 | Loss: 0.00001030
Iteration 6/1000 | Loss: 0.00000985
Iteration 7/1000 | Loss: 0.00000955
Iteration 8/1000 | Loss: 0.00000939
Iteration 9/1000 | Loss: 0.00000912
Iteration 10/1000 | Loss: 0.00000903
Iteration 11/1000 | Loss: 0.00000900
Iteration 12/1000 | Loss: 0.00000897
Iteration 13/1000 | Loss: 0.00000897
Iteration 14/1000 | Loss: 0.00000896
Iteration 15/1000 | Loss: 0.00000893
Iteration 16/1000 | Loss: 0.00000890
Iteration 17/1000 | Loss: 0.00000888
Iteration 18/1000 | Loss: 0.00000888
Iteration 19/1000 | Loss: 0.00000887
Iteration 20/1000 | Loss: 0.00000886
Iteration 21/1000 | Loss: 0.00000883
Iteration 22/1000 | Loss: 0.00000882
Iteration 23/1000 | Loss: 0.00000881
Iteration 24/1000 | Loss: 0.00000881
Iteration 25/1000 | Loss: 0.00000880
Iteration 26/1000 | Loss: 0.00000879
Iteration 27/1000 | Loss: 0.00000878
Iteration 28/1000 | Loss: 0.00000878
Iteration 29/1000 | Loss: 0.00000877
Iteration 30/1000 | Loss: 0.00000877
Iteration 31/1000 | Loss: 0.00000877
Iteration 32/1000 | Loss: 0.00000876
Iteration 33/1000 | Loss: 0.00000876
Iteration 34/1000 | Loss: 0.00000876
Iteration 35/1000 | Loss: 0.00000875
Iteration 36/1000 | Loss: 0.00000875
Iteration 37/1000 | Loss: 0.00000875
Iteration 38/1000 | Loss: 0.00000875
Iteration 39/1000 | Loss: 0.00000874
Iteration 40/1000 | Loss: 0.00000872
Iteration 41/1000 | Loss: 0.00000871
Iteration 42/1000 | Loss: 0.00000871
Iteration 43/1000 | Loss: 0.00000871
Iteration 44/1000 | Loss: 0.00000871
Iteration 45/1000 | Loss: 0.00000871
Iteration 46/1000 | Loss: 0.00000871
Iteration 47/1000 | Loss: 0.00000871
Iteration 48/1000 | Loss: 0.00000870
Iteration 49/1000 | Loss: 0.00000870
Iteration 50/1000 | Loss: 0.00000870
Iteration 51/1000 | Loss: 0.00000870
Iteration 52/1000 | Loss: 0.00000870
Iteration 53/1000 | Loss: 0.00000870
Iteration 54/1000 | Loss: 0.00000870
Iteration 55/1000 | Loss: 0.00000870
Iteration 56/1000 | Loss: 0.00000869
Iteration 57/1000 | Loss: 0.00000869
Iteration 58/1000 | Loss: 0.00000869
Iteration 59/1000 | Loss: 0.00000869
Iteration 60/1000 | Loss: 0.00000869
Iteration 61/1000 | Loss: 0.00000869
Iteration 62/1000 | Loss: 0.00000869
Iteration 63/1000 | Loss: 0.00000869
Iteration 64/1000 | Loss: 0.00000869
Iteration 65/1000 | Loss: 0.00000868
Iteration 66/1000 | Loss: 0.00000868
Iteration 67/1000 | Loss: 0.00000867
Iteration 68/1000 | Loss: 0.00000867
Iteration 69/1000 | Loss: 0.00000866
Iteration 70/1000 | Loss: 0.00000866
Iteration 71/1000 | Loss: 0.00000865
Iteration 72/1000 | Loss: 0.00000865
Iteration 73/1000 | Loss: 0.00000865
Iteration 74/1000 | Loss: 0.00000864
Iteration 75/1000 | Loss: 0.00000864
Iteration 76/1000 | Loss: 0.00000864
Iteration 77/1000 | Loss: 0.00000864
Iteration 78/1000 | Loss: 0.00000863
Iteration 79/1000 | Loss: 0.00000863
Iteration 80/1000 | Loss: 0.00000860
Iteration 81/1000 | Loss: 0.00000860
Iteration 82/1000 | Loss: 0.00000857
Iteration 83/1000 | Loss: 0.00000857
Iteration 84/1000 | Loss: 0.00000857
Iteration 85/1000 | Loss: 0.00000857
Iteration 86/1000 | Loss: 0.00000856
Iteration 87/1000 | Loss: 0.00000856
Iteration 88/1000 | Loss: 0.00000856
Iteration 89/1000 | Loss: 0.00000856
Iteration 90/1000 | Loss: 0.00000855
Iteration 91/1000 | Loss: 0.00000855
Iteration 92/1000 | Loss: 0.00000855
Iteration 93/1000 | Loss: 0.00000854
Iteration 94/1000 | Loss: 0.00000854
Iteration 95/1000 | Loss: 0.00000853
Iteration 96/1000 | Loss: 0.00000853
Iteration 97/1000 | Loss: 0.00000853
Iteration 98/1000 | Loss: 0.00000853
Iteration 99/1000 | Loss: 0.00000853
Iteration 100/1000 | Loss: 0.00000853
Iteration 101/1000 | Loss: 0.00000853
Iteration 102/1000 | Loss: 0.00000853
Iteration 103/1000 | Loss: 0.00000853
Iteration 104/1000 | Loss: 0.00000852
Iteration 105/1000 | Loss: 0.00000852
Iteration 106/1000 | Loss: 0.00000852
Iteration 107/1000 | Loss: 0.00000852
Iteration 108/1000 | Loss: 0.00000852
Iteration 109/1000 | Loss: 0.00000852
Iteration 110/1000 | Loss: 0.00000851
Iteration 111/1000 | Loss: 0.00000851
Iteration 112/1000 | Loss: 0.00000851
Iteration 113/1000 | Loss: 0.00000850
Iteration 114/1000 | Loss: 0.00000850
Iteration 115/1000 | Loss: 0.00000850
Iteration 116/1000 | Loss: 0.00000850
Iteration 117/1000 | Loss: 0.00000850
Iteration 118/1000 | Loss: 0.00000850
Iteration 119/1000 | Loss: 0.00000850
Iteration 120/1000 | Loss: 0.00000850
Iteration 121/1000 | Loss: 0.00000850
Iteration 122/1000 | Loss: 0.00000850
Iteration 123/1000 | Loss: 0.00000850
Iteration 124/1000 | Loss: 0.00000850
Iteration 125/1000 | Loss: 0.00000849
Iteration 126/1000 | Loss: 0.00000849
Iteration 127/1000 | Loss: 0.00000849
Iteration 128/1000 | Loss: 0.00000849
Iteration 129/1000 | Loss: 0.00000849
Iteration 130/1000 | Loss: 0.00000849
Iteration 131/1000 | Loss: 0.00000848
Iteration 132/1000 | Loss: 0.00000848
Iteration 133/1000 | Loss: 0.00000847
Iteration 134/1000 | Loss: 0.00000847
Iteration 135/1000 | Loss: 0.00000847
Iteration 136/1000 | Loss: 0.00000847
Iteration 137/1000 | Loss: 0.00000847
Iteration 138/1000 | Loss: 0.00000847
Iteration 139/1000 | Loss: 0.00000847
Iteration 140/1000 | Loss: 0.00000847
Iteration 141/1000 | Loss: 0.00000847
Iteration 142/1000 | Loss: 0.00000847
Iteration 143/1000 | Loss: 0.00000847
Iteration 144/1000 | Loss: 0.00000846
Iteration 145/1000 | Loss: 0.00000846
Iteration 146/1000 | Loss: 0.00000846
Iteration 147/1000 | Loss: 0.00000846
Iteration 148/1000 | Loss: 0.00000846
Iteration 149/1000 | Loss: 0.00000846
Iteration 150/1000 | Loss: 0.00000846
Iteration 151/1000 | Loss: 0.00000846
Iteration 152/1000 | Loss: 0.00000845
Iteration 153/1000 | Loss: 0.00000845
Iteration 154/1000 | Loss: 0.00000845
Iteration 155/1000 | Loss: 0.00000845
Iteration 156/1000 | Loss: 0.00000845
Iteration 157/1000 | Loss: 0.00000845
Iteration 158/1000 | Loss: 0.00000845
Iteration 159/1000 | Loss: 0.00000844
Iteration 160/1000 | Loss: 0.00000844
Iteration 161/1000 | Loss: 0.00000844
Iteration 162/1000 | Loss: 0.00000844
Iteration 163/1000 | Loss: 0.00000844
Iteration 164/1000 | Loss: 0.00000844
Iteration 165/1000 | Loss: 0.00000844
Iteration 166/1000 | Loss: 0.00000844
Iteration 167/1000 | Loss: 0.00000844
Iteration 168/1000 | Loss: 0.00000844
Iteration 169/1000 | Loss: 0.00000844
Iteration 170/1000 | Loss: 0.00000844
Iteration 171/1000 | Loss: 0.00000843
Iteration 172/1000 | Loss: 0.00000843
Iteration 173/1000 | Loss: 0.00000843
Iteration 174/1000 | Loss: 0.00000843
Iteration 175/1000 | Loss: 0.00000843
Iteration 176/1000 | Loss: 0.00000843
Iteration 177/1000 | Loss: 0.00000843
Iteration 178/1000 | Loss: 0.00000843
Iteration 179/1000 | Loss: 0.00000843
Iteration 180/1000 | Loss: 0.00000842
Iteration 181/1000 | Loss: 0.00000842
Iteration 182/1000 | Loss: 0.00000842
Iteration 183/1000 | Loss: 0.00000842
Iteration 184/1000 | Loss: 0.00000841
Iteration 185/1000 | Loss: 0.00000841
Iteration 186/1000 | Loss: 0.00000841
Iteration 187/1000 | Loss: 0.00000841
Iteration 188/1000 | Loss: 0.00000841
Iteration 189/1000 | Loss: 0.00000841
Iteration 190/1000 | Loss: 0.00000841
Iteration 191/1000 | Loss: 0.00000841
Iteration 192/1000 | Loss: 0.00000841
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 192. Stopping optimization.
Last 5 losses: [8.412795978074428e-06, 8.412795978074428e-06, 8.412795978074428e-06, 8.412795978074428e-06, 8.412795978074428e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.412795978074428e-06

Optimization complete. Final v2v error: 2.5111706256866455 mm

Highest mean error: 2.614577293395996 mm for frame 139

Lowest mean error: 2.4344353675842285 mm for frame 151

Saving results

Total time: 38.347888469696045
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_018/1006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_018/1006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_018/1006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00414306
Iteration 2/25 | Loss: 0.00147452
Iteration 3/25 | Loss: 0.00120831
Iteration 4/25 | Loss: 0.00116959
Iteration 5/25 | Loss: 0.00116348
Iteration 6/25 | Loss: 0.00116186
Iteration 7/25 | Loss: 0.00116157
Iteration 8/25 | Loss: 0.00116157
Iteration 9/25 | Loss: 0.00116157
Iteration 10/25 | Loss: 0.00116157
Iteration 11/25 | Loss: 0.00116157
Iteration 12/25 | Loss: 0.00116157
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0011615668190643191, 0.0011615668190643191, 0.0011615668190643191, 0.0011615668190643191, 0.0011615668190643191]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011615668190643191

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33068407
Iteration 2/25 | Loss: 0.00073211
Iteration 3/25 | Loss: 0.00073210
Iteration 4/25 | Loss: 0.00073210
Iteration 5/25 | Loss: 0.00073210
Iteration 6/25 | Loss: 0.00073210
Iteration 7/25 | Loss: 0.00073210
Iteration 8/25 | Loss: 0.00073210
Iteration 9/25 | Loss: 0.00073210
Iteration 10/25 | Loss: 0.00073210
Iteration 11/25 | Loss: 0.00073210
Iteration 12/25 | Loss: 0.00073210
Iteration 13/25 | Loss: 0.00073210
Iteration 14/25 | Loss: 0.00073210
Iteration 15/25 | Loss: 0.00073210
Iteration 16/25 | Loss: 0.00073210
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0007321026059798896, 0.0007321026059798896, 0.0007321026059798896, 0.0007321026059798896, 0.0007321026059798896]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007321026059798896

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00073210
Iteration 2/1000 | Loss: 0.00003954
Iteration 3/1000 | Loss: 0.00002446
Iteration 4/1000 | Loss: 0.00001889
Iteration 5/1000 | Loss: 0.00001732
Iteration 6/1000 | Loss: 0.00001638
Iteration 7/1000 | Loss: 0.00001582
Iteration 8/1000 | Loss: 0.00001527
Iteration 9/1000 | Loss: 0.00001497
Iteration 10/1000 | Loss: 0.00001466
Iteration 11/1000 | Loss: 0.00001463
Iteration 12/1000 | Loss: 0.00001442
Iteration 13/1000 | Loss: 0.00001433
Iteration 14/1000 | Loss: 0.00001431
Iteration 15/1000 | Loss: 0.00001427
Iteration 16/1000 | Loss: 0.00001422
Iteration 17/1000 | Loss: 0.00001422
Iteration 18/1000 | Loss: 0.00001420
Iteration 19/1000 | Loss: 0.00001419
Iteration 20/1000 | Loss: 0.00001419
Iteration 21/1000 | Loss: 0.00001419
Iteration 22/1000 | Loss: 0.00001419
Iteration 23/1000 | Loss: 0.00001417
Iteration 24/1000 | Loss: 0.00001416
Iteration 25/1000 | Loss: 0.00001414
Iteration 26/1000 | Loss: 0.00001413
Iteration 27/1000 | Loss: 0.00001412
Iteration 28/1000 | Loss: 0.00001411
Iteration 29/1000 | Loss: 0.00001410
Iteration 30/1000 | Loss: 0.00001410
Iteration 31/1000 | Loss: 0.00001409
Iteration 32/1000 | Loss: 0.00001409
Iteration 33/1000 | Loss: 0.00001409
Iteration 34/1000 | Loss: 0.00001404
Iteration 35/1000 | Loss: 0.00001403
Iteration 36/1000 | Loss: 0.00001403
Iteration 37/1000 | Loss: 0.00001403
Iteration 38/1000 | Loss: 0.00001403
Iteration 39/1000 | Loss: 0.00001403
Iteration 40/1000 | Loss: 0.00001402
Iteration 41/1000 | Loss: 0.00001402
Iteration 42/1000 | Loss: 0.00001402
Iteration 43/1000 | Loss: 0.00001401
Iteration 44/1000 | Loss: 0.00001401
Iteration 45/1000 | Loss: 0.00001400
Iteration 46/1000 | Loss: 0.00001399
Iteration 47/1000 | Loss: 0.00001399
Iteration 48/1000 | Loss: 0.00001399
Iteration 49/1000 | Loss: 0.00001399
Iteration 50/1000 | Loss: 0.00001398
Iteration 51/1000 | Loss: 0.00001398
Iteration 52/1000 | Loss: 0.00001397
Iteration 53/1000 | Loss: 0.00001397
Iteration 54/1000 | Loss: 0.00001397
Iteration 55/1000 | Loss: 0.00001397
Iteration 56/1000 | Loss: 0.00001397
Iteration 57/1000 | Loss: 0.00001396
Iteration 58/1000 | Loss: 0.00001396
Iteration 59/1000 | Loss: 0.00001396
Iteration 60/1000 | Loss: 0.00001395
Iteration 61/1000 | Loss: 0.00001395
Iteration 62/1000 | Loss: 0.00001394
Iteration 63/1000 | Loss: 0.00001394
Iteration 64/1000 | Loss: 0.00001394
Iteration 65/1000 | Loss: 0.00001393
Iteration 66/1000 | Loss: 0.00001393
Iteration 67/1000 | Loss: 0.00001393
Iteration 68/1000 | Loss: 0.00001393
Iteration 69/1000 | Loss: 0.00001393
Iteration 70/1000 | Loss: 0.00001393
Iteration 71/1000 | Loss: 0.00001392
Iteration 72/1000 | Loss: 0.00001392
Iteration 73/1000 | Loss: 0.00001392
Iteration 74/1000 | Loss: 0.00001392
Iteration 75/1000 | Loss: 0.00001392
Iteration 76/1000 | Loss: 0.00001392
Iteration 77/1000 | Loss: 0.00001392
Iteration 78/1000 | Loss: 0.00001392
Iteration 79/1000 | Loss: 0.00001391
Iteration 80/1000 | Loss: 0.00001391
Iteration 81/1000 | Loss: 0.00001391
Iteration 82/1000 | Loss: 0.00001391
Iteration 83/1000 | Loss: 0.00001391
Iteration 84/1000 | Loss: 0.00001391
Iteration 85/1000 | Loss: 0.00001391
Iteration 86/1000 | Loss: 0.00001391
Iteration 87/1000 | Loss: 0.00001390
Iteration 88/1000 | Loss: 0.00001390
Iteration 89/1000 | Loss: 0.00001390
Iteration 90/1000 | Loss: 0.00001390
Iteration 91/1000 | Loss: 0.00001390
Iteration 92/1000 | Loss: 0.00001390
Iteration 93/1000 | Loss: 0.00001389
Iteration 94/1000 | Loss: 0.00001389
Iteration 95/1000 | Loss: 0.00001388
Iteration 96/1000 | Loss: 0.00001388
Iteration 97/1000 | Loss: 0.00001388
Iteration 98/1000 | Loss: 0.00001388
Iteration 99/1000 | Loss: 0.00001388
Iteration 100/1000 | Loss: 0.00001388
Iteration 101/1000 | Loss: 0.00001388
Iteration 102/1000 | Loss: 0.00001387
Iteration 103/1000 | Loss: 0.00001387
Iteration 104/1000 | Loss: 0.00001386
Iteration 105/1000 | Loss: 0.00001386
Iteration 106/1000 | Loss: 0.00001385
Iteration 107/1000 | Loss: 0.00001385
Iteration 108/1000 | Loss: 0.00001385
Iteration 109/1000 | Loss: 0.00001385
Iteration 110/1000 | Loss: 0.00001384
Iteration 111/1000 | Loss: 0.00001384
Iteration 112/1000 | Loss: 0.00001384
Iteration 113/1000 | Loss: 0.00001384
Iteration 114/1000 | Loss: 0.00001384
Iteration 115/1000 | Loss: 0.00001383
Iteration 116/1000 | Loss: 0.00001383
Iteration 117/1000 | Loss: 0.00001383
Iteration 118/1000 | Loss: 0.00001382
Iteration 119/1000 | Loss: 0.00001382
Iteration 120/1000 | Loss: 0.00001382
Iteration 121/1000 | Loss: 0.00001382
Iteration 122/1000 | Loss: 0.00001382
Iteration 123/1000 | Loss: 0.00001382
Iteration 124/1000 | Loss: 0.00001382
Iteration 125/1000 | Loss: 0.00001382
Iteration 126/1000 | Loss: 0.00001381
Iteration 127/1000 | Loss: 0.00001381
Iteration 128/1000 | Loss: 0.00001381
Iteration 129/1000 | Loss: 0.00001381
Iteration 130/1000 | Loss: 0.00001380
Iteration 131/1000 | Loss: 0.00001380
Iteration 132/1000 | Loss: 0.00001379
Iteration 133/1000 | Loss: 0.00001379
Iteration 134/1000 | Loss: 0.00001379
Iteration 135/1000 | Loss: 0.00001378
Iteration 136/1000 | Loss: 0.00001378
Iteration 137/1000 | Loss: 0.00001378
Iteration 138/1000 | Loss: 0.00001378
Iteration 139/1000 | Loss: 0.00001378
Iteration 140/1000 | Loss: 0.00001378
Iteration 141/1000 | Loss: 0.00001377
Iteration 142/1000 | Loss: 0.00001377
Iteration 143/1000 | Loss: 0.00001377
Iteration 144/1000 | Loss: 0.00001377
Iteration 145/1000 | Loss: 0.00001377
Iteration 146/1000 | Loss: 0.00001377
Iteration 147/1000 | Loss: 0.00001377
Iteration 148/1000 | Loss: 0.00001377
Iteration 149/1000 | Loss: 0.00001376
Iteration 150/1000 | Loss: 0.00001376
Iteration 151/1000 | Loss: 0.00001376
Iteration 152/1000 | Loss: 0.00001376
Iteration 153/1000 | Loss: 0.00001375
Iteration 154/1000 | Loss: 0.00001375
Iteration 155/1000 | Loss: 0.00001375
Iteration 156/1000 | Loss: 0.00001375
Iteration 157/1000 | Loss: 0.00001375
Iteration 158/1000 | Loss: 0.00001375
Iteration 159/1000 | Loss: 0.00001375
Iteration 160/1000 | Loss: 0.00001375
Iteration 161/1000 | Loss: 0.00001375
Iteration 162/1000 | Loss: 0.00001374
Iteration 163/1000 | Loss: 0.00001374
Iteration 164/1000 | Loss: 0.00001374
Iteration 165/1000 | Loss: 0.00001374
Iteration 166/1000 | Loss: 0.00001374
Iteration 167/1000 | Loss: 0.00001374
Iteration 168/1000 | Loss: 0.00001374
Iteration 169/1000 | Loss: 0.00001373
Iteration 170/1000 | Loss: 0.00001373
Iteration 171/1000 | Loss: 0.00001373
Iteration 172/1000 | Loss: 0.00001373
Iteration 173/1000 | Loss: 0.00001373
Iteration 174/1000 | Loss: 0.00001373
Iteration 175/1000 | Loss: 0.00001372
Iteration 176/1000 | Loss: 0.00001372
Iteration 177/1000 | Loss: 0.00001372
Iteration 178/1000 | Loss: 0.00001372
Iteration 179/1000 | Loss: 0.00001372
Iteration 180/1000 | Loss: 0.00001372
Iteration 181/1000 | Loss: 0.00001372
Iteration 182/1000 | Loss: 0.00001371
Iteration 183/1000 | Loss: 0.00001371
Iteration 184/1000 | Loss: 0.00001371
Iteration 185/1000 | Loss: 0.00001371
Iteration 186/1000 | Loss: 0.00001371
Iteration 187/1000 | Loss: 0.00001371
Iteration 188/1000 | Loss: 0.00001371
Iteration 189/1000 | Loss: 0.00001371
Iteration 190/1000 | Loss: 0.00001371
Iteration 191/1000 | Loss: 0.00001371
Iteration 192/1000 | Loss: 0.00001371
Iteration 193/1000 | Loss: 0.00001371
Iteration 194/1000 | Loss: 0.00001370
Iteration 195/1000 | Loss: 0.00001370
Iteration 196/1000 | Loss: 0.00001370
Iteration 197/1000 | Loss: 0.00001370
Iteration 198/1000 | Loss: 0.00001370
Iteration 199/1000 | Loss: 0.00001370
Iteration 200/1000 | Loss: 0.00001369
Iteration 201/1000 | Loss: 0.00001369
Iteration 202/1000 | Loss: 0.00001369
Iteration 203/1000 | Loss: 0.00001369
Iteration 204/1000 | Loss: 0.00001369
Iteration 205/1000 | Loss: 0.00001369
Iteration 206/1000 | Loss: 0.00001369
Iteration 207/1000 | Loss: 0.00001369
Iteration 208/1000 | Loss: 0.00001369
Iteration 209/1000 | Loss: 0.00001369
Iteration 210/1000 | Loss: 0.00001369
Iteration 211/1000 | Loss: 0.00001369
Iteration 212/1000 | Loss: 0.00001369
Iteration 213/1000 | Loss: 0.00001369
Iteration 214/1000 | Loss: 0.00001368
Iteration 215/1000 | Loss: 0.00001368
Iteration 216/1000 | Loss: 0.00001368
Iteration 217/1000 | Loss: 0.00001368
Iteration 218/1000 | Loss: 0.00001368
Iteration 219/1000 | Loss: 0.00001368
Iteration 220/1000 | Loss: 0.00001368
Iteration 221/1000 | Loss: 0.00001368
Iteration 222/1000 | Loss: 0.00001368
Iteration 223/1000 | Loss: 0.00001367
Iteration 224/1000 | Loss: 0.00001367
Iteration 225/1000 | Loss: 0.00001367
Iteration 226/1000 | Loss: 0.00001367
Iteration 227/1000 | Loss: 0.00001367
Iteration 228/1000 | Loss: 0.00001367
Iteration 229/1000 | Loss: 0.00001367
Iteration 230/1000 | Loss: 0.00001367
Iteration 231/1000 | Loss: 0.00001367
Iteration 232/1000 | Loss: 0.00001367
Iteration 233/1000 | Loss: 0.00001367
Iteration 234/1000 | Loss: 0.00001366
Iteration 235/1000 | Loss: 0.00001366
Iteration 236/1000 | Loss: 0.00001366
Iteration 237/1000 | Loss: 0.00001366
Iteration 238/1000 | Loss: 0.00001366
Iteration 239/1000 | Loss: 0.00001366
Iteration 240/1000 | Loss: 0.00001366
Iteration 241/1000 | Loss: 0.00001366
Iteration 242/1000 | Loss: 0.00001366
Iteration 243/1000 | Loss: 0.00001366
Iteration 244/1000 | Loss: 0.00001366
Iteration 245/1000 | Loss: 0.00001366
Iteration 246/1000 | Loss: 0.00001366
Iteration 247/1000 | Loss: 0.00001366
Iteration 248/1000 | Loss: 0.00001366
Iteration 249/1000 | Loss: 0.00001365
Iteration 250/1000 | Loss: 0.00001365
Iteration 251/1000 | Loss: 0.00001365
Iteration 252/1000 | Loss: 0.00001365
Iteration 253/1000 | Loss: 0.00001365
Iteration 254/1000 | Loss: 0.00001365
Iteration 255/1000 | Loss: 0.00001365
Iteration 256/1000 | Loss: 0.00001365
Iteration 257/1000 | Loss: 0.00001365
Iteration 258/1000 | Loss: 0.00001365
Iteration 259/1000 | Loss: 0.00001365
Iteration 260/1000 | Loss: 0.00001365
Iteration 261/1000 | Loss: 0.00001365
Iteration 262/1000 | Loss: 0.00001365
Iteration 263/1000 | Loss: 0.00001365
Iteration 264/1000 | Loss: 0.00001365
Iteration 265/1000 | Loss: 0.00001365
Iteration 266/1000 | Loss: 0.00001365
Iteration 267/1000 | Loss: 0.00001365
Iteration 268/1000 | Loss: 0.00001365
Iteration 269/1000 | Loss: 0.00001365
Iteration 270/1000 | Loss: 0.00001365
Iteration 271/1000 | Loss: 0.00001365
Iteration 272/1000 | Loss: 0.00001365
Iteration 273/1000 | Loss: 0.00001365
Iteration 274/1000 | Loss: 0.00001365
Iteration 275/1000 | Loss: 0.00001365
Iteration 276/1000 | Loss: 0.00001365
Iteration 277/1000 | Loss: 0.00001365
Iteration 278/1000 | Loss: 0.00001365
Iteration 279/1000 | Loss: 0.00001365
Iteration 280/1000 | Loss: 0.00001365
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 280. Stopping optimization.
Last 5 losses: [1.3654969734488986e-05, 1.3654969734488986e-05, 1.3654969734488986e-05, 1.3654969734488986e-05, 1.3654969734488986e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3654969734488986e-05

Optimization complete. Final v2v error: 3.1418397426605225 mm

Highest mean error: 3.9153425693511963 mm for frame 72

Lowest mean error: 2.6846108436584473 mm for frame 13

Saving results

Total time: 46.10778522491455
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_018/1033/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_018/1033.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_018/1033
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01015119
Iteration 2/25 | Loss: 0.00307339
Iteration 3/25 | Loss: 0.00258882
Iteration 4/25 | Loss: 0.00235144
Iteration 5/25 | Loss: 0.00215226
Iteration 6/25 | Loss: 0.00207450
Iteration 7/25 | Loss: 0.00195015
Iteration 8/25 | Loss: 0.00182456
Iteration 9/25 | Loss: 0.00175856
Iteration 10/25 | Loss: 0.00168907
Iteration 11/25 | Loss: 0.00165189
Iteration 12/25 | Loss: 0.00162693
Iteration 13/25 | Loss: 0.00162413
Iteration 14/25 | Loss: 0.00159847
Iteration 15/25 | Loss: 0.00158963
Iteration 16/25 | Loss: 0.00157948
Iteration 17/25 | Loss: 0.00157590
Iteration 18/25 | Loss: 0.00157665
Iteration 19/25 | Loss: 0.00157567
Iteration 20/25 | Loss: 0.00157142
Iteration 21/25 | Loss: 0.00157053
Iteration 22/25 | Loss: 0.00157025
Iteration 23/25 | Loss: 0.00157010
Iteration 24/25 | Loss: 0.00157004
Iteration 25/25 | Loss: 0.00157004

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.31493139
Iteration 2/25 | Loss: 0.00506048
Iteration 3/25 | Loss: 0.00506046
Iteration 4/25 | Loss: 0.00506046
Iteration 5/25 | Loss: 0.00506046
Iteration 6/25 | Loss: 0.00506046
Iteration 7/25 | Loss: 0.00506046
Iteration 8/25 | Loss: 0.00506046
Iteration 9/25 | Loss: 0.00506046
Iteration 10/25 | Loss: 0.00506046
Iteration 11/25 | Loss: 0.00506046
Iteration 12/25 | Loss: 0.00506046
Iteration 13/25 | Loss: 0.00506046
Iteration 14/25 | Loss: 0.00506046
Iteration 15/25 | Loss: 0.00506046
Iteration 16/25 | Loss: 0.00506046
Iteration 17/25 | Loss: 0.00506046
Iteration 18/25 | Loss: 0.00506046
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0050604576244950294, 0.0050604576244950294, 0.0050604576244950294, 0.0050604576244950294, 0.0050604576244950294]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0050604576244950294

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00506046
Iteration 2/1000 | Loss: 0.00673941
Iteration 3/1000 | Loss: 0.00061691
Iteration 4/1000 | Loss: 0.00039777
Iteration 5/1000 | Loss: 0.00025727
Iteration 6/1000 | Loss: 0.00015199
Iteration 7/1000 | Loss: 0.00010940
Iteration 8/1000 | Loss: 0.00008662
Iteration 9/1000 | Loss: 0.00167804
Iteration 10/1000 | Loss: 0.00015118
Iteration 11/1000 | Loss: 0.00008294
Iteration 12/1000 | Loss: 0.00006572
Iteration 13/1000 | Loss: 0.00005969
Iteration 14/1000 | Loss: 0.00005598
Iteration 15/1000 | Loss: 0.00005273
Iteration 16/1000 | Loss: 0.00005055
Iteration 17/1000 | Loss: 0.00004825
Iteration 18/1000 | Loss: 0.00004664
Iteration 19/1000 | Loss: 0.00004532
Iteration 20/1000 | Loss: 0.00004408
Iteration 21/1000 | Loss: 0.00004272
Iteration 22/1000 | Loss: 0.00004188
Iteration 23/1000 | Loss: 0.00004116
Iteration 24/1000 | Loss: 0.00004069
Iteration 25/1000 | Loss: 0.00004041
Iteration 26/1000 | Loss: 0.00004009
Iteration 27/1000 | Loss: 0.00004006
Iteration 28/1000 | Loss: 0.00003980
Iteration 29/1000 | Loss: 0.00003973
Iteration 30/1000 | Loss: 0.00003972
Iteration 31/1000 | Loss: 0.00003972
Iteration 32/1000 | Loss: 0.00003961
Iteration 33/1000 | Loss: 0.00003960
Iteration 34/1000 | Loss: 0.00003960
Iteration 35/1000 | Loss: 0.00003958
Iteration 36/1000 | Loss: 0.00003957
Iteration 37/1000 | Loss: 0.00003957
Iteration 38/1000 | Loss: 0.00003945
Iteration 39/1000 | Loss: 0.00003938
Iteration 40/1000 | Loss: 0.00003935
Iteration 41/1000 | Loss: 0.00003934
Iteration 42/1000 | Loss: 0.00003934
Iteration 43/1000 | Loss: 0.00003933
Iteration 44/1000 | Loss: 0.00003933
Iteration 45/1000 | Loss: 0.00003933
Iteration 46/1000 | Loss: 0.00003932
Iteration 47/1000 | Loss: 0.00003931
Iteration 48/1000 | Loss: 0.00003929
Iteration 49/1000 | Loss: 0.00003929
Iteration 50/1000 | Loss: 0.00003928
Iteration 51/1000 | Loss: 0.00003927
Iteration 52/1000 | Loss: 0.00003926
Iteration 53/1000 | Loss: 0.00003926
Iteration 54/1000 | Loss: 0.00003925
Iteration 55/1000 | Loss: 0.00003924
Iteration 56/1000 | Loss: 0.00003924
Iteration 57/1000 | Loss: 0.00003923
Iteration 58/1000 | Loss: 0.00003923
Iteration 59/1000 | Loss: 0.00003923
Iteration 60/1000 | Loss: 0.00003923
Iteration 61/1000 | Loss: 0.00003923
Iteration 62/1000 | Loss: 0.00003923
Iteration 63/1000 | Loss: 0.00003923
Iteration 64/1000 | Loss: 0.00003923
Iteration 65/1000 | Loss: 0.00003922
Iteration 66/1000 | Loss: 0.00003922
Iteration 67/1000 | Loss: 0.00003922
Iteration 68/1000 | Loss: 0.00003919
Iteration 69/1000 | Loss: 0.00003919
Iteration 70/1000 | Loss: 0.00003918
Iteration 71/1000 | Loss: 0.00003918
Iteration 72/1000 | Loss: 0.00003918
Iteration 73/1000 | Loss: 0.00003917
Iteration 74/1000 | Loss: 0.00003917
Iteration 75/1000 | Loss: 0.00003916
Iteration 76/1000 | Loss: 0.00003916
Iteration 77/1000 | Loss: 0.00003916
Iteration 78/1000 | Loss: 0.00003915
Iteration 79/1000 | Loss: 0.00003915
Iteration 80/1000 | Loss: 0.00003915
Iteration 81/1000 | Loss: 0.00003915
Iteration 82/1000 | Loss: 0.00003915
Iteration 83/1000 | Loss: 0.00003915
Iteration 84/1000 | Loss: 0.00003914
Iteration 85/1000 | Loss: 0.00003914
Iteration 86/1000 | Loss: 0.00003914
Iteration 87/1000 | Loss: 0.00003914
Iteration 88/1000 | Loss: 0.00003914
Iteration 89/1000 | Loss: 0.00003914
Iteration 90/1000 | Loss: 0.00003913
Iteration 91/1000 | Loss: 0.00003913
Iteration 92/1000 | Loss: 0.00003913
Iteration 93/1000 | Loss: 0.00003913
Iteration 94/1000 | Loss: 0.00003913
Iteration 95/1000 | Loss: 0.00003913
Iteration 96/1000 | Loss: 0.00003913
Iteration 97/1000 | Loss: 0.00003913
Iteration 98/1000 | Loss: 0.00003913
Iteration 99/1000 | Loss: 0.00003912
Iteration 100/1000 | Loss: 0.00003912
Iteration 101/1000 | Loss: 0.00003912
Iteration 102/1000 | Loss: 0.00003912
Iteration 103/1000 | Loss: 0.00003912
Iteration 104/1000 | Loss: 0.00003912
Iteration 105/1000 | Loss: 0.00003912
Iteration 106/1000 | Loss: 0.00003912
Iteration 107/1000 | Loss: 0.00003911
Iteration 108/1000 | Loss: 0.00003911
Iteration 109/1000 | Loss: 0.00003911
Iteration 110/1000 | Loss: 0.00003911
Iteration 111/1000 | Loss: 0.00003911
Iteration 112/1000 | Loss: 0.00003910
Iteration 113/1000 | Loss: 0.00003910
Iteration 114/1000 | Loss: 0.00003910
Iteration 115/1000 | Loss: 0.00003909
Iteration 116/1000 | Loss: 0.00003909
Iteration 117/1000 | Loss: 0.00003909
Iteration 118/1000 | Loss: 0.00003909
Iteration 119/1000 | Loss: 0.00003908
Iteration 120/1000 | Loss: 0.00003908
Iteration 121/1000 | Loss: 0.00003908
Iteration 122/1000 | Loss: 0.00003908
Iteration 123/1000 | Loss: 0.00003908
Iteration 124/1000 | Loss: 0.00003907
Iteration 125/1000 | Loss: 0.00003907
Iteration 126/1000 | Loss: 0.00003907
Iteration 127/1000 | Loss: 0.00003907
Iteration 128/1000 | Loss: 0.00003907
Iteration 129/1000 | Loss: 0.00003906
Iteration 130/1000 | Loss: 0.00003906
Iteration 131/1000 | Loss: 0.00003906
Iteration 132/1000 | Loss: 0.00003905
Iteration 133/1000 | Loss: 0.00003905
Iteration 134/1000 | Loss: 0.00003905
Iteration 135/1000 | Loss: 0.00003905
Iteration 136/1000 | Loss: 0.00003905
Iteration 137/1000 | Loss: 0.00003905
Iteration 138/1000 | Loss: 0.00003904
Iteration 139/1000 | Loss: 0.00003904
Iteration 140/1000 | Loss: 0.00003904
Iteration 141/1000 | Loss: 0.00003904
Iteration 142/1000 | Loss: 0.00003904
Iteration 143/1000 | Loss: 0.00003904
Iteration 144/1000 | Loss: 0.00003904
Iteration 145/1000 | Loss: 0.00003904
Iteration 146/1000 | Loss: 0.00003904
Iteration 147/1000 | Loss: 0.00003904
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 147. Stopping optimization.
Last 5 losses: [3.904363984474912e-05, 3.904363984474912e-05, 3.904363984474912e-05, 3.904363984474912e-05, 3.904363984474912e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.904363984474912e-05

Optimization complete. Final v2v error: 3.8236916065216064 mm

Highest mean error: 11.585906982421875 mm for frame 166

Lowest mean error: 2.976020574569702 mm for frame 63

Saving results

Total time: 95.07930707931519
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_018/1005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_018/1005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_018/1005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00450957
Iteration 2/25 | Loss: 0.00133474
Iteration 3/25 | Loss: 0.00119465
Iteration 4/25 | Loss: 0.00118048
Iteration 5/25 | Loss: 0.00117517
Iteration 6/25 | Loss: 0.00117443
Iteration 7/25 | Loss: 0.00117443
Iteration 8/25 | Loss: 0.00117443
Iteration 9/25 | Loss: 0.00117443
Iteration 10/25 | Loss: 0.00117443
Iteration 11/25 | Loss: 0.00117443
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001174434321001172, 0.001174434321001172, 0.001174434321001172, 0.001174434321001172, 0.001174434321001172]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001174434321001172

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36613536
Iteration 2/25 | Loss: 0.00104560
Iteration 3/25 | Loss: 0.00104560
Iteration 4/25 | Loss: 0.00104560
Iteration 5/25 | Loss: 0.00104559
Iteration 6/25 | Loss: 0.00104559
Iteration 7/25 | Loss: 0.00104559
Iteration 8/25 | Loss: 0.00104559
Iteration 9/25 | Loss: 0.00104559
Iteration 10/25 | Loss: 0.00104559
Iteration 11/25 | Loss: 0.00104559
Iteration 12/25 | Loss: 0.00104559
Iteration 13/25 | Loss: 0.00104559
Iteration 14/25 | Loss: 0.00104559
Iteration 15/25 | Loss: 0.00104559
Iteration 16/25 | Loss: 0.00104559
Iteration 17/25 | Loss: 0.00104559
Iteration 18/25 | Loss: 0.00104559
Iteration 19/25 | Loss: 0.00104559
Iteration 20/25 | Loss: 0.00104559
Iteration 21/25 | Loss: 0.00104559
Iteration 22/25 | Loss: 0.00104559
Iteration 23/25 | Loss: 0.00104559
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0010455924784764647, 0.0010455924784764647, 0.0010455924784764647, 0.0010455924784764647, 0.0010455924784764647]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010455924784764647

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00104559
Iteration 2/1000 | Loss: 0.00002264
Iteration 3/1000 | Loss: 0.00001657
Iteration 4/1000 | Loss: 0.00001511
Iteration 5/1000 | Loss: 0.00001430
Iteration 6/1000 | Loss: 0.00001381
Iteration 7/1000 | Loss: 0.00001355
Iteration 8/1000 | Loss: 0.00001340
Iteration 9/1000 | Loss: 0.00001315
Iteration 10/1000 | Loss: 0.00001294
Iteration 11/1000 | Loss: 0.00001290
Iteration 12/1000 | Loss: 0.00001275
Iteration 13/1000 | Loss: 0.00001273
Iteration 14/1000 | Loss: 0.00001273
Iteration 15/1000 | Loss: 0.00001272
Iteration 16/1000 | Loss: 0.00001271
Iteration 17/1000 | Loss: 0.00001271
Iteration 18/1000 | Loss: 0.00001270
Iteration 19/1000 | Loss: 0.00001270
Iteration 20/1000 | Loss: 0.00001268
Iteration 21/1000 | Loss: 0.00001268
Iteration 22/1000 | Loss: 0.00001268
Iteration 23/1000 | Loss: 0.00001268
Iteration 24/1000 | Loss: 0.00001268
Iteration 25/1000 | Loss: 0.00001267
Iteration 26/1000 | Loss: 0.00001267
Iteration 27/1000 | Loss: 0.00001267
Iteration 28/1000 | Loss: 0.00001267
Iteration 29/1000 | Loss: 0.00001267
Iteration 30/1000 | Loss: 0.00001267
Iteration 31/1000 | Loss: 0.00001267
Iteration 32/1000 | Loss: 0.00001266
Iteration 33/1000 | Loss: 0.00001266
Iteration 34/1000 | Loss: 0.00001264
Iteration 35/1000 | Loss: 0.00001263
Iteration 36/1000 | Loss: 0.00001263
Iteration 37/1000 | Loss: 0.00001263
Iteration 38/1000 | Loss: 0.00001262
Iteration 39/1000 | Loss: 0.00001261
Iteration 40/1000 | Loss: 0.00001261
Iteration 41/1000 | Loss: 0.00001260
Iteration 42/1000 | Loss: 0.00001259
Iteration 43/1000 | Loss: 0.00001259
Iteration 44/1000 | Loss: 0.00001259
Iteration 45/1000 | Loss: 0.00001259
Iteration 46/1000 | Loss: 0.00001258
Iteration 47/1000 | Loss: 0.00001258
Iteration 48/1000 | Loss: 0.00001257
Iteration 49/1000 | Loss: 0.00001257
Iteration 50/1000 | Loss: 0.00001256
Iteration 51/1000 | Loss: 0.00001256
Iteration 52/1000 | Loss: 0.00001255
Iteration 53/1000 | Loss: 0.00001255
Iteration 54/1000 | Loss: 0.00001255
Iteration 55/1000 | Loss: 0.00001254
Iteration 56/1000 | Loss: 0.00001253
Iteration 57/1000 | Loss: 0.00001253
Iteration 58/1000 | Loss: 0.00001252
Iteration 59/1000 | Loss: 0.00001252
Iteration 60/1000 | Loss: 0.00001251
Iteration 61/1000 | Loss: 0.00001251
Iteration 62/1000 | Loss: 0.00001250
Iteration 63/1000 | Loss: 0.00001250
Iteration 64/1000 | Loss: 0.00001250
Iteration 65/1000 | Loss: 0.00001249
Iteration 66/1000 | Loss: 0.00001249
Iteration 67/1000 | Loss: 0.00001249
Iteration 68/1000 | Loss: 0.00001248
Iteration 69/1000 | Loss: 0.00001248
Iteration 70/1000 | Loss: 0.00001247
Iteration 71/1000 | Loss: 0.00001247
Iteration 72/1000 | Loss: 0.00001247
Iteration 73/1000 | Loss: 0.00001246
Iteration 74/1000 | Loss: 0.00001245
Iteration 75/1000 | Loss: 0.00001245
Iteration 76/1000 | Loss: 0.00001244
Iteration 77/1000 | Loss: 0.00001244
Iteration 78/1000 | Loss: 0.00001243
Iteration 79/1000 | Loss: 0.00001241
Iteration 80/1000 | Loss: 0.00001240
Iteration 81/1000 | Loss: 0.00001240
Iteration 82/1000 | Loss: 0.00001239
Iteration 83/1000 | Loss: 0.00001239
Iteration 84/1000 | Loss: 0.00001237
Iteration 85/1000 | Loss: 0.00001237
Iteration 86/1000 | Loss: 0.00001237
Iteration 87/1000 | Loss: 0.00001237
Iteration 88/1000 | Loss: 0.00001237
Iteration 89/1000 | Loss: 0.00001237
Iteration 90/1000 | Loss: 0.00001237
Iteration 91/1000 | Loss: 0.00001237
Iteration 92/1000 | Loss: 0.00001237
Iteration 93/1000 | Loss: 0.00001236
Iteration 94/1000 | Loss: 0.00001236
Iteration 95/1000 | Loss: 0.00001236
Iteration 96/1000 | Loss: 0.00001236
Iteration 97/1000 | Loss: 0.00001236
Iteration 98/1000 | Loss: 0.00001236
Iteration 99/1000 | Loss: 0.00001236
Iteration 100/1000 | Loss: 0.00001236
Iteration 101/1000 | Loss: 0.00001236
Iteration 102/1000 | Loss: 0.00001236
Iteration 103/1000 | Loss: 0.00001235
Iteration 104/1000 | Loss: 0.00001235
Iteration 105/1000 | Loss: 0.00001235
Iteration 106/1000 | Loss: 0.00001234
Iteration 107/1000 | Loss: 0.00001234
Iteration 108/1000 | Loss: 0.00001234
Iteration 109/1000 | Loss: 0.00001234
Iteration 110/1000 | Loss: 0.00001234
Iteration 111/1000 | Loss: 0.00001234
Iteration 112/1000 | Loss: 0.00001233
Iteration 113/1000 | Loss: 0.00001233
Iteration 114/1000 | Loss: 0.00001233
Iteration 115/1000 | Loss: 0.00001233
Iteration 116/1000 | Loss: 0.00001233
Iteration 117/1000 | Loss: 0.00001232
Iteration 118/1000 | Loss: 0.00001232
Iteration 119/1000 | Loss: 0.00001232
Iteration 120/1000 | Loss: 0.00001232
Iteration 121/1000 | Loss: 0.00001232
Iteration 122/1000 | Loss: 0.00001232
Iteration 123/1000 | Loss: 0.00001232
Iteration 124/1000 | Loss: 0.00001232
Iteration 125/1000 | Loss: 0.00001232
Iteration 126/1000 | Loss: 0.00001232
Iteration 127/1000 | Loss: 0.00001232
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 127. Stopping optimization.
Last 5 losses: [1.2319090274104383e-05, 1.2319090274104383e-05, 1.2319090274104383e-05, 1.2319090274104383e-05, 1.2319090274104383e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2319090274104383e-05

Optimization complete. Final v2v error: 2.938412666320801 mm

Highest mean error: 3.395017385482788 mm for frame 77

Lowest mean error: 2.657870054244995 mm for frame 160

Saving results

Total time: 38.13136291503906
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_018/1043/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_018/1043.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_018/1043
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00533287
Iteration 2/25 | Loss: 0.00122626
Iteration 3/25 | Loss: 0.00115553
Iteration 4/25 | Loss: 0.00114526
Iteration 5/25 | Loss: 0.00114201
Iteration 6/25 | Loss: 0.00114153
Iteration 7/25 | Loss: 0.00114153
Iteration 8/25 | Loss: 0.00114153
Iteration 9/25 | Loss: 0.00114153
Iteration 10/25 | Loss: 0.00114153
Iteration 11/25 | Loss: 0.00114153
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011415296467021108, 0.0011415296467021108, 0.0011415296467021108, 0.0011415296467021108, 0.0011415296467021108]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011415296467021108

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.12630939
Iteration 2/25 | Loss: 0.00080310
Iteration 3/25 | Loss: 0.00080309
Iteration 4/25 | Loss: 0.00080309
Iteration 5/25 | Loss: 0.00080309
Iteration 6/25 | Loss: 0.00080309
Iteration 7/25 | Loss: 0.00080309
Iteration 8/25 | Loss: 0.00080309
Iteration 9/25 | Loss: 0.00080309
Iteration 10/25 | Loss: 0.00080309
Iteration 11/25 | Loss: 0.00080309
Iteration 12/25 | Loss: 0.00080309
Iteration 13/25 | Loss: 0.00080309
Iteration 14/25 | Loss: 0.00080309
Iteration 15/25 | Loss: 0.00080309
Iteration 16/25 | Loss: 0.00080309
Iteration 17/25 | Loss: 0.00080309
Iteration 18/25 | Loss: 0.00080309
Iteration 19/25 | Loss: 0.00080309
Iteration 20/25 | Loss: 0.00080309
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.000803090282715857, 0.000803090282715857, 0.000803090282715857, 0.000803090282715857, 0.000803090282715857]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000803090282715857

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00080309
Iteration 2/1000 | Loss: 0.00002601
Iteration 3/1000 | Loss: 0.00001849
Iteration 4/1000 | Loss: 0.00001561
Iteration 5/1000 | Loss: 0.00001469
Iteration 6/1000 | Loss: 0.00001392
Iteration 7/1000 | Loss: 0.00001352
Iteration 8/1000 | Loss: 0.00001304
Iteration 9/1000 | Loss: 0.00001277
Iteration 10/1000 | Loss: 0.00001250
Iteration 11/1000 | Loss: 0.00001227
Iteration 12/1000 | Loss: 0.00001224
Iteration 13/1000 | Loss: 0.00001218
Iteration 14/1000 | Loss: 0.00001209
Iteration 15/1000 | Loss: 0.00001208
Iteration 16/1000 | Loss: 0.00001207
Iteration 17/1000 | Loss: 0.00001201
Iteration 18/1000 | Loss: 0.00001197
Iteration 19/1000 | Loss: 0.00001196
Iteration 20/1000 | Loss: 0.00001193
Iteration 21/1000 | Loss: 0.00001192
Iteration 22/1000 | Loss: 0.00001191
Iteration 23/1000 | Loss: 0.00001191
Iteration 24/1000 | Loss: 0.00001191
Iteration 25/1000 | Loss: 0.00001191
Iteration 26/1000 | Loss: 0.00001190
Iteration 27/1000 | Loss: 0.00001190
Iteration 28/1000 | Loss: 0.00001186
Iteration 29/1000 | Loss: 0.00001186
Iteration 30/1000 | Loss: 0.00001184
Iteration 31/1000 | Loss: 0.00001182
Iteration 32/1000 | Loss: 0.00001181
Iteration 33/1000 | Loss: 0.00001181
Iteration 34/1000 | Loss: 0.00001181
Iteration 35/1000 | Loss: 0.00001181
Iteration 36/1000 | Loss: 0.00001181
Iteration 37/1000 | Loss: 0.00001180
Iteration 38/1000 | Loss: 0.00001180
Iteration 39/1000 | Loss: 0.00001179
Iteration 40/1000 | Loss: 0.00001179
Iteration 41/1000 | Loss: 0.00001179
Iteration 42/1000 | Loss: 0.00001179
Iteration 43/1000 | Loss: 0.00001178
Iteration 44/1000 | Loss: 0.00001178
Iteration 45/1000 | Loss: 0.00001178
Iteration 46/1000 | Loss: 0.00001178
Iteration 47/1000 | Loss: 0.00001177
Iteration 48/1000 | Loss: 0.00001177
Iteration 49/1000 | Loss: 0.00001177
Iteration 50/1000 | Loss: 0.00001176
Iteration 51/1000 | Loss: 0.00001176
Iteration 52/1000 | Loss: 0.00001176
Iteration 53/1000 | Loss: 0.00001175
Iteration 54/1000 | Loss: 0.00001173
Iteration 55/1000 | Loss: 0.00001173
Iteration 56/1000 | Loss: 0.00001173
Iteration 57/1000 | Loss: 0.00001172
Iteration 58/1000 | Loss: 0.00001172
Iteration 59/1000 | Loss: 0.00001172
Iteration 60/1000 | Loss: 0.00001172
Iteration 61/1000 | Loss: 0.00001171
Iteration 62/1000 | Loss: 0.00001170
Iteration 63/1000 | Loss: 0.00001170
Iteration 64/1000 | Loss: 0.00001169
Iteration 65/1000 | Loss: 0.00001168
Iteration 66/1000 | Loss: 0.00001168
Iteration 67/1000 | Loss: 0.00001168
Iteration 68/1000 | Loss: 0.00001167
Iteration 69/1000 | Loss: 0.00001167
Iteration 70/1000 | Loss: 0.00001166
Iteration 71/1000 | Loss: 0.00001166
Iteration 72/1000 | Loss: 0.00001165
Iteration 73/1000 | Loss: 0.00001165
Iteration 74/1000 | Loss: 0.00001164
Iteration 75/1000 | Loss: 0.00001164
Iteration 76/1000 | Loss: 0.00001163
Iteration 77/1000 | Loss: 0.00001163
Iteration 78/1000 | Loss: 0.00001163
Iteration 79/1000 | Loss: 0.00001163
Iteration 80/1000 | Loss: 0.00001163
Iteration 81/1000 | Loss: 0.00001163
Iteration 82/1000 | Loss: 0.00001163
Iteration 83/1000 | Loss: 0.00001163
Iteration 84/1000 | Loss: 0.00001163
Iteration 85/1000 | Loss: 0.00001163
Iteration 86/1000 | Loss: 0.00001162
Iteration 87/1000 | Loss: 0.00001160
Iteration 88/1000 | Loss: 0.00001160
Iteration 89/1000 | Loss: 0.00001160
Iteration 90/1000 | Loss: 0.00001160
Iteration 91/1000 | Loss: 0.00001160
Iteration 92/1000 | Loss: 0.00001160
Iteration 93/1000 | Loss: 0.00001159
Iteration 94/1000 | Loss: 0.00001159
Iteration 95/1000 | Loss: 0.00001159
Iteration 96/1000 | Loss: 0.00001159
Iteration 97/1000 | Loss: 0.00001159
Iteration 98/1000 | Loss: 0.00001159
Iteration 99/1000 | Loss: 0.00001157
Iteration 100/1000 | Loss: 0.00001157
Iteration 101/1000 | Loss: 0.00001157
Iteration 102/1000 | Loss: 0.00001156
Iteration 103/1000 | Loss: 0.00001156
Iteration 104/1000 | Loss: 0.00001156
Iteration 105/1000 | Loss: 0.00001156
Iteration 106/1000 | Loss: 0.00001155
Iteration 107/1000 | Loss: 0.00001155
Iteration 108/1000 | Loss: 0.00001154
Iteration 109/1000 | Loss: 0.00001154
Iteration 110/1000 | Loss: 0.00001154
Iteration 111/1000 | Loss: 0.00001154
Iteration 112/1000 | Loss: 0.00001153
Iteration 113/1000 | Loss: 0.00001153
Iteration 114/1000 | Loss: 0.00001153
Iteration 115/1000 | Loss: 0.00001153
Iteration 116/1000 | Loss: 0.00001153
Iteration 117/1000 | Loss: 0.00001152
Iteration 118/1000 | Loss: 0.00001152
Iteration 119/1000 | Loss: 0.00001152
Iteration 120/1000 | Loss: 0.00001151
Iteration 121/1000 | Loss: 0.00001151
Iteration 122/1000 | Loss: 0.00001151
Iteration 123/1000 | Loss: 0.00001151
Iteration 124/1000 | Loss: 0.00001150
Iteration 125/1000 | Loss: 0.00001150
Iteration 126/1000 | Loss: 0.00001149
Iteration 127/1000 | Loss: 0.00001149
Iteration 128/1000 | Loss: 0.00001149
Iteration 129/1000 | Loss: 0.00001149
Iteration 130/1000 | Loss: 0.00001149
Iteration 131/1000 | Loss: 0.00001149
Iteration 132/1000 | Loss: 0.00001148
Iteration 133/1000 | Loss: 0.00001148
Iteration 134/1000 | Loss: 0.00001148
Iteration 135/1000 | Loss: 0.00001148
Iteration 136/1000 | Loss: 0.00001148
Iteration 137/1000 | Loss: 0.00001148
Iteration 138/1000 | Loss: 0.00001148
Iteration 139/1000 | Loss: 0.00001148
Iteration 140/1000 | Loss: 0.00001148
Iteration 141/1000 | Loss: 0.00001148
Iteration 142/1000 | Loss: 0.00001148
Iteration 143/1000 | Loss: 0.00001147
Iteration 144/1000 | Loss: 0.00001147
Iteration 145/1000 | Loss: 0.00001147
Iteration 146/1000 | Loss: 0.00001146
Iteration 147/1000 | Loss: 0.00001146
Iteration 148/1000 | Loss: 0.00001146
Iteration 149/1000 | Loss: 0.00001146
Iteration 150/1000 | Loss: 0.00001146
Iteration 151/1000 | Loss: 0.00001146
Iteration 152/1000 | Loss: 0.00001146
Iteration 153/1000 | Loss: 0.00001146
Iteration 154/1000 | Loss: 0.00001145
Iteration 155/1000 | Loss: 0.00001145
Iteration 156/1000 | Loss: 0.00001145
Iteration 157/1000 | Loss: 0.00001145
Iteration 158/1000 | Loss: 0.00001145
Iteration 159/1000 | Loss: 0.00001145
Iteration 160/1000 | Loss: 0.00001145
Iteration 161/1000 | Loss: 0.00001145
Iteration 162/1000 | Loss: 0.00001145
Iteration 163/1000 | Loss: 0.00001145
Iteration 164/1000 | Loss: 0.00001145
Iteration 165/1000 | Loss: 0.00001144
Iteration 166/1000 | Loss: 0.00001144
Iteration 167/1000 | Loss: 0.00001144
Iteration 168/1000 | Loss: 0.00001144
Iteration 169/1000 | Loss: 0.00001144
Iteration 170/1000 | Loss: 0.00001144
Iteration 171/1000 | Loss: 0.00001144
Iteration 172/1000 | Loss: 0.00001144
Iteration 173/1000 | Loss: 0.00001144
Iteration 174/1000 | Loss: 0.00001144
Iteration 175/1000 | Loss: 0.00001144
Iteration 176/1000 | Loss: 0.00001144
Iteration 177/1000 | Loss: 0.00001144
Iteration 178/1000 | Loss: 0.00001143
Iteration 179/1000 | Loss: 0.00001143
Iteration 180/1000 | Loss: 0.00001143
Iteration 181/1000 | Loss: 0.00001143
Iteration 182/1000 | Loss: 0.00001143
Iteration 183/1000 | Loss: 0.00001143
Iteration 184/1000 | Loss: 0.00001143
Iteration 185/1000 | Loss: 0.00001143
Iteration 186/1000 | Loss: 0.00001143
Iteration 187/1000 | Loss: 0.00001143
Iteration 188/1000 | Loss: 0.00001143
Iteration 189/1000 | Loss: 0.00001143
Iteration 190/1000 | Loss: 0.00001143
Iteration 191/1000 | Loss: 0.00001143
Iteration 192/1000 | Loss: 0.00001143
Iteration 193/1000 | Loss: 0.00001143
Iteration 194/1000 | Loss: 0.00001143
Iteration 195/1000 | Loss: 0.00001143
Iteration 196/1000 | Loss: 0.00001143
Iteration 197/1000 | Loss: 0.00001143
Iteration 198/1000 | Loss: 0.00001143
Iteration 199/1000 | Loss: 0.00001143
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 199. Stopping optimization.
Last 5 losses: [1.1428503967181314e-05, 1.1428503967181314e-05, 1.1428503967181314e-05, 1.1428503967181314e-05, 1.1428503967181314e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1428503967181314e-05

Optimization complete. Final v2v error: 2.8913488388061523 mm

Highest mean error: 3.4629061222076416 mm for frame 59

Lowest mean error: 2.5785484313964844 mm for frame 37

Saving results

Total time: 41.702526330947876
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_018/1026/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_018/1026.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_018/1026
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00681663
Iteration 2/25 | Loss: 0.00135211
Iteration 3/25 | Loss: 0.00125862
Iteration 4/25 | Loss: 0.00123746
Iteration 5/25 | Loss: 0.00123078
Iteration 6/25 | Loss: 0.00122977
Iteration 7/25 | Loss: 0.00122977
Iteration 8/25 | Loss: 0.00122977
Iteration 9/25 | Loss: 0.00122977
Iteration 10/25 | Loss: 0.00122977
Iteration 11/25 | Loss: 0.00122977
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012297664070501924, 0.0012297664070501924, 0.0012297664070501924, 0.0012297664070501924, 0.0012297664070501924]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012297664070501924

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.62071598
Iteration 2/25 | Loss: 0.00096033
Iteration 3/25 | Loss: 0.00096032
Iteration 4/25 | Loss: 0.00096031
Iteration 5/25 | Loss: 0.00096031
Iteration 6/25 | Loss: 0.00096031
Iteration 7/25 | Loss: 0.00096031
Iteration 8/25 | Loss: 0.00096031
Iteration 9/25 | Loss: 0.00096031
Iteration 10/25 | Loss: 0.00096031
Iteration 11/25 | Loss: 0.00096031
Iteration 12/25 | Loss: 0.00096031
Iteration 13/25 | Loss: 0.00096031
Iteration 14/25 | Loss: 0.00096031
Iteration 15/25 | Loss: 0.00096031
Iteration 16/25 | Loss: 0.00096031
Iteration 17/25 | Loss: 0.00096031
Iteration 18/25 | Loss: 0.00096031
Iteration 19/25 | Loss: 0.00096031
Iteration 20/25 | Loss: 0.00096031
Iteration 21/25 | Loss: 0.00096031
Iteration 22/25 | Loss: 0.00096031
Iteration 23/25 | Loss: 0.00096031
Iteration 24/25 | Loss: 0.00096031
Iteration 25/25 | Loss: 0.00096031
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0009603112703189254, 0.0009603112703189254, 0.0009603112703189254, 0.0009603112703189254, 0.0009603112703189254]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009603112703189254

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00096031
Iteration 2/1000 | Loss: 0.00006490
Iteration 3/1000 | Loss: 0.00003940
Iteration 4/1000 | Loss: 0.00003331
Iteration 5/1000 | Loss: 0.00003069
Iteration 6/1000 | Loss: 0.00002920
Iteration 7/1000 | Loss: 0.00002808
Iteration 8/1000 | Loss: 0.00002736
Iteration 9/1000 | Loss: 0.00002688
Iteration 10/1000 | Loss: 0.00002651
Iteration 11/1000 | Loss: 0.00002626
Iteration 12/1000 | Loss: 0.00002610
Iteration 13/1000 | Loss: 0.00002594
Iteration 14/1000 | Loss: 0.00002589
Iteration 15/1000 | Loss: 0.00002588
Iteration 16/1000 | Loss: 0.00002573
Iteration 17/1000 | Loss: 0.00002571
Iteration 18/1000 | Loss: 0.00002570
Iteration 19/1000 | Loss: 0.00002569
Iteration 20/1000 | Loss: 0.00002563
Iteration 21/1000 | Loss: 0.00002558
Iteration 22/1000 | Loss: 0.00002558
Iteration 23/1000 | Loss: 0.00002558
Iteration 24/1000 | Loss: 0.00002557
Iteration 25/1000 | Loss: 0.00002557
Iteration 26/1000 | Loss: 0.00002555
Iteration 27/1000 | Loss: 0.00002555
Iteration 28/1000 | Loss: 0.00002554
Iteration 29/1000 | Loss: 0.00002554
Iteration 30/1000 | Loss: 0.00002554
Iteration 31/1000 | Loss: 0.00002553
Iteration 32/1000 | Loss: 0.00002553
Iteration 33/1000 | Loss: 0.00002552
Iteration 34/1000 | Loss: 0.00002552
Iteration 35/1000 | Loss: 0.00002552
Iteration 36/1000 | Loss: 0.00002551
Iteration 37/1000 | Loss: 0.00002551
Iteration 38/1000 | Loss: 0.00002550
Iteration 39/1000 | Loss: 0.00002549
Iteration 40/1000 | Loss: 0.00002549
Iteration 41/1000 | Loss: 0.00002549
Iteration 42/1000 | Loss: 0.00002548
Iteration 43/1000 | Loss: 0.00002548
Iteration 44/1000 | Loss: 0.00002547
Iteration 45/1000 | Loss: 0.00002546
Iteration 46/1000 | Loss: 0.00002546
Iteration 47/1000 | Loss: 0.00002546
Iteration 48/1000 | Loss: 0.00002545
Iteration 49/1000 | Loss: 0.00002545
Iteration 50/1000 | Loss: 0.00002544
Iteration 51/1000 | Loss: 0.00002543
Iteration 52/1000 | Loss: 0.00002543
Iteration 53/1000 | Loss: 0.00002542
Iteration 54/1000 | Loss: 0.00002542
Iteration 55/1000 | Loss: 0.00002541
Iteration 56/1000 | Loss: 0.00002541
Iteration 57/1000 | Loss: 0.00002541
Iteration 58/1000 | Loss: 0.00002540
Iteration 59/1000 | Loss: 0.00002540
Iteration 60/1000 | Loss: 0.00002540
Iteration 61/1000 | Loss: 0.00002539
Iteration 62/1000 | Loss: 0.00002539
Iteration 63/1000 | Loss: 0.00002539
Iteration 64/1000 | Loss: 0.00002539
Iteration 65/1000 | Loss: 0.00002539
Iteration 66/1000 | Loss: 0.00002539
Iteration 67/1000 | Loss: 0.00002539
Iteration 68/1000 | Loss: 0.00002539
Iteration 69/1000 | Loss: 0.00002539
Iteration 70/1000 | Loss: 0.00002539
Iteration 71/1000 | Loss: 0.00002538
Iteration 72/1000 | Loss: 0.00002538
Iteration 73/1000 | Loss: 0.00002538
Iteration 74/1000 | Loss: 0.00002538
Iteration 75/1000 | Loss: 0.00002538
Iteration 76/1000 | Loss: 0.00002538
Iteration 77/1000 | Loss: 0.00002538
Iteration 78/1000 | Loss: 0.00002538
Iteration 79/1000 | Loss: 0.00002538
Iteration 80/1000 | Loss: 0.00002538
Iteration 81/1000 | Loss: 0.00002538
Iteration 82/1000 | Loss: 0.00002537
Iteration 83/1000 | Loss: 0.00002537
Iteration 84/1000 | Loss: 0.00002537
Iteration 85/1000 | Loss: 0.00002537
Iteration 86/1000 | Loss: 0.00002536
Iteration 87/1000 | Loss: 0.00002536
Iteration 88/1000 | Loss: 0.00002535
Iteration 89/1000 | Loss: 0.00002535
Iteration 90/1000 | Loss: 0.00002534
Iteration 91/1000 | Loss: 0.00002534
Iteration 92/1000 | Loss: 0.00002534
Iteration 93/1000 | Loss: 0.00002534
Iteration 94/1000 | Loss: 0.00002534
Iteration 95/1000 | Loss: 0.00002534
Iteration 96/1000 | Loss: 0.00002534
Iteration 97/1000 | Loss: 0.00002534
Iteration 98/1000 | Loss: 0.00002534
Iteration 99/1000 | Loss: 0.00002533
Iteration 100/1000 | Loss: 0.00002533
Iteration 101/1000 | Loss: 0.00002533
Iteration 102/1000 | Loss: 0.00002533
Iteration 103/1000 | Loss: 0.00002532
Iteration 104/1000 | Loss: 0.00002532
Iteration 105/1000 | Loss: 0.00002532
Iteration 106/1000 | Loss: 0.00002532
Iteration 107/1000 | Loss: 0.00002532
Iteration 108/1000 | Loss: 0.00002532
Iteration 109/1000 | Loss: 0.00002531
Iteration 110/1000 | Loss: 0.00002531
Iteration 111/1000 | Loss: 0.00002531
Iteration 112/1000 | Loss: 0.00002531
Iteration 113/1000 | Loss: 0.00002531
Iteration 114/1000 | Loss: 0.00002531
Iteration 115/1000 | Loss: 0.00002531
Iteration 116/1000 | Loss: 0.00002531
Iteration 117/1000 | Loss: 0.00002530
Iteration 118/1000 | Loss: 0.00002530
Iteration 119/1000 | Loss: 0.00002530
Iteration 120/1000 | Loss: 0.00002530
Iteration 121/1000 | Loss: 0.00002530
Iteration 122/1000 | Loss: 0.00002530
Iteration 123/1000 | Loss: 0.00002530
Iteration 124/1000 | Loss: 0.00002530
Iteration 125/1000 | Loss: 0.00002530
Iteration 126/1000 | Loss: 0.00002530
Iteration 127/1000 | Loss: 0.00002529
Iteration 128/1000 | Loss: 0.00002529
Iteration 129/1000 | Loss: 0.00002529
Iteration 130/1000 | Loss: 0.00002529
Iteration 131/1000 | Loss: 0.00002529
Iteration 132/1000 | Loss: 0.00002529
Iteration 133/1000 | Loss: 0.00002529
Iteration 134/1000 | Loss: 0.00002528
Iteration 135/1000 | Loss: 0.00002528
Iteration 136/1000 | Loss: 0.00002528
Iteration 137/1000 | Loss: 0.00002528
Iteration 138/1000 | Loss: 0.00002528
Iteration 139/1000 | Loss: 0.00002528
Iteration 140/1000 | Loss: 0.00002527
Iteration 141/1000 | Loss: 0.00002527
Iteration 142/1000 | Loss: 0.00002527
Iteration 143/1000 | Loss: 0.00002527
Iteration 144/1000 | Loss: 0.00002526
Iteration 145/1000 | Loss: 0.00002526
Iteration 146/1000 | Loss: 0.00002526
Iteration 147/1000 | Loss: 0.00002526
Iteration 148/1000 | Loss: 0.00002525
Iteration 149/1000 | Loss: 0.00002525
Iteration 150/1000 | Loss: 0.00002525
Iteration 151/1000 | Loss: 0.00002525
Iteration 152/1000 | Loss: 0.00002525
Iteration 153/1000 | Loss: 0.00002525
Iteration 154/1000 | Loss: 0.00002525
Iteration 155/1000 | Loss: 0.00002524
Iteration 156/1000 | Loss: 0.00002524
Iteration 157/1000 | Loss: 0.00002524
Iteration 158/1000 | Loss: 0.00002524
Iteration 159/1000 | Loss: 0.00002524
Iteration 160/1000 | Loss: 0.00002524
Iteration 161/1000 | Loss: 0.00002524
Iteration 162/1000 | Loss: 0.00002524
Iteration 163/1000 | Loss: 0.00002524
Iteration 164/1000 | Loss: 0.00002524
Iteration 165/1000 | Loss: 0.00002524
Iteration 166/1000 | Loss: 0.00002524
Iteration 167/1000 | Loss: 0.00002524
Iteration 168/1000 | Loss: 0.00002523
Iteration 169/1000 | Loss: 0.00002523
Iteration 170/1000 | Loss: 0.00002523
Iteration 171/1000 | Loss: 0.00002523
Iteration 172/1000 | Loss: 0.00002523
Iteration 173/1000 | Loss: 0.00002523
Iteration 174/1000 | Loss: 0.00002522
Iteration 175/1000 | Loss: 0.00002522
Iteration 176/1000 | Loss: 0.00002522
Iteration 177/1000 | Loss: 0.00002522
Iteration 178/1000 | Loss: 0.00002522
Iteration 179/1000 | Loss: 0.00002522
Iteration 180/1000 | Loss: 0.00002521
Iteration 181/1000 | Loss: 0.00002521
Iteration 182/1000 | Loss: 0.00002521
Iteration 183/1000 | Loss: 0.00002521
Iteration 184/1000 | Loss: 0.00002521
Iteration 185/1000 | Loss: 0.00002521
Iteration 186/1000 | Loss: 0.00002521
Iteration 187/1000 | Loss: 0.00002521
Iteration 188/1000 | Loss: 0.00002521
Iteration 189/1000 | Loss: 0.00002521
Iteration 190/1000 | Loss: 0.00002521
Iteration 191/1000 | Loss: 0.00002521
Iteration 192/1000 | Loss: 0.00002521
Iteration 193/1000 | Loss: 0.00002521
Iteration 194/1000 | Loss: 0.00002521
Iteration 195/1000 | Loss: 0.00002521
Iteration 196/1000 | Loss: 0.00002521
Iteration 197/1000 | Loss: 0.00002520
Iteration 198/1000 | Loss: 0.00002520
Iteration 199/1000 | Loss: 0.00002520
Iteration 200/1000 | Loss: 0.00002520
Iteration 201/1000 | Loss: 0.00002520
Iteration 202/1000 | Loss: 0.00002520
Iteration 203/1000 | Loss: 0.00002520
Iteration 204/1000 | Loss: 0.00002520
Iteration 205/1000 | Loss: 0.00002520
Iteration 206/1000 | Loss: 0.00002520
Iteration 207/1000 | Loss: 0.00002520
Iteration 208/1000 | Loss: 0.00002520
Iteration 209/1000 | Loss: 0.00002520
Iteration 210/1000 | Loss: 0.00002520
Iteration 211/1000 | Loss: 0.00002520
Iteration 212/1000 | Loss: 0.00002520
Iteration 213/1000 | Loss: 0.00002520
Iteration 214/1000 | Loss: 0.00002520
Iteration 215/1000 | Loss: 0.00002520
Iteration 216/1000 | Loss: 0.00002520
Iteration 217/1000 | Loss: 0.00002520
Iteration 218/1000 | Loss: 0.00002520
Iteration 219/1000 | Loss: 0.00002520
Iteration 220/1000 | Loss: 0.00002520
Iteration 221/1000 | Loss: 0.00002520
Iteration 222/1000 | Loss: 0.00002520
Iteration 223/1000 | Loss: 0.00002520
Iteration 224/1000 | Loss: 0.00002520
Iteration 225/1000 | Loss: 0.00002520
Iteration 226/1000 | Loss: 0.00002520
Iteration 227/1000 | Loss: 0.00002520
Iteration 228/1000 | Loss: 0.00002520
Iteration 229/1000 | Loss: 0.00002520
Iteration 230/1000 | Loss: 0.00002520
Iteration 231/1000 | Loss: 0.00002520
Iteration 232/1000 | Loss: 0.00002520
Iteration 233/1000 | Loss: 0.00002520
Iteration 234/1000 | Loss: 0.00002520
Iteration 235/1000 | Loss: 0.00002520
Iteration 236/1000 | Loss: 0.00002520
Iteration 237/1000 | Loss: 0.00002520
Iteration 238/1000 | Loss: 0.00002520
Iteration 239/1000 | Loss: 0.00002520
Iteration 240/1000 | Loss: 0.00002520
Iteration 241/1000 | Loss: 0.00002520
Iteration 242/1000 | Loss: 0.00002520
Iteration 243/1000 | Loss: 0.00002520
Iteration 244/1000 | Loss: 0.00002520
Iteration 245/1000 | Loss: 0.00002520
Iteration 246/1000 | Loss: 0.00002520
Iteration 247/1000 | Loss: 0.00002520
Iteration 248/1000 | Loss: 0.00002520
Iteration 249/1000 | Loss: 0.00002520
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 249. Stopping optimization.
Last 5 losses: [2.5199622541549616e-05, 2.5199622541549616e-05, 2.5199622541549616e-05, 2.5199622541549616e-05, 2.5199622541549616e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.5199622541549616e-05

Optimization complete. Final v2v error: 4.079904079437256 mm

Highest mean error: 5.182000160217285 mm for frame 164

Lowest mean error: 3.356905221939087 mm for frame 70

Saving results

Total time: 50.662354946136475
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_018/1063/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_018/1063.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_018/1063
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00388727
Iteration 2/25 | Loss: 0.00125174
Iteration 3/25 | Loss: 0.00116992
Iteration 4/25 | Loss: 0.00116309
Iteration 5/25 | Loss: 0.00116065
Iteration 6/25 | Loss: 0.00115992
Iteration 7/25 | Loss: 0.00115982
Iteration 8/25 | Loss: 0.00115982
Iteration 9/25 | Loss: 0.00115982
Iteration 10/25 | Loss: 0.00115982
Iteration 11/25 | Loss: 0.00115982
Iteration 12/25 | Loss: 0.00115982
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0011598190758377314, 0.0011598190758377314, 0.0011598190758377314, 0.0011598190758377314, 0.0011598190758377314]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011598190758377314

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46803319
Iteration 2/25 | Loss: 0.00099141
Iteration 3/25 | Loss: 0.00099141
Iteration 4/25 | Loss: 0.00099141
Iteration 5/25 | Loss: 0.00099141
Iteration 6/25 | Loss: 0.00099141
Iteration 7/25 | Loss: 0.00099141
Iteration 8/25 | Loss: 0.00099141
Iteration 9/25 | Loss: 0.00099141
Iteration 10/25 | Loss: 0.00099141
Iteration 11/25 | Loss: 0.00099141
Iteration 12/25 | Loss: 0.00099141
Iteration 13/25 | Loss: 0.00099141
Iteration 14/25 | Loss: 0.00099141
Iteration 15/25 | Loss: 0.00099141
Iteration 16/25 | Loss: 0.00099141
Iteration 17/25 | Loss: 0.00099141
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.000991411623544991, 0.000991411623544991, 0.000991411623544991, 0.000991411623544991, 0.000991411623544991]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000991411623544991

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00099141
Iteration 2/1000 | Loss: 0.00002936
Iteration 3/1000 | Loss: 0.00002128
Iteration 4/1000 | Loss: 0.00001596
Iteration 5/1000 | Loss: 0.00001459
Iteration 6/1000 | Loss: 0.00001361
Iteration 7/1000 | Loss: 0.00001321
Iteration 8/1000 | Loss: 0.00001281
Iteration 9/1000 | Loss: 0.00001258
Iteration 10/1000 | Loss: 0.00001247
Iteration 11/1000 | Loss: 0.00001242
Iteration 12/1000 | Loss: 0.00001227
Iteration 13/1000 | Loss: 0.00001224
Iteration 14/1000 | Loss: 0.00001208
Iteration 15/1000 | Loss: 0.00001206
Iteration 16/1000 | Loss: 0.00001198
Iteration 17/1000 | Loss: 0.00001194
Iteration 18/1000 | Loss: 0.00001193
Iteration 19/1000 | Loss: 0.00001192
Iteration 20/1000 | Loss: 0.00001192
Iteration 21/1000 | Loss: 0.00001191
Iteration 22/1000 | Loss: 0.00001191
Iteration 23/1000 | Loss: 0.00001187
Iteration 24/1000 | Loss: 0.00001185
Iteration 25/1000 | Loss: 0.00001185
Iteration 26/1000 | Loss: 0.00001185
Iteration 27/1000 | Loss: 0.00001184
Iteration 28/1000 | Loss: 0.00001184
Iteration 29/1000 | Loss: 0.00001183
Iteration 30/1000 | Loss: 0.00001182
Iteration 31/1000 | Loss: 0.00001182
Iteration 32/1000 | Loss: 0.00001181
Iteration 33/1000 | Loss: 0.00001181
Iteration 34/1000 | Loss: 0.00001181
Iteration 35/1000 | Loss: 0.00001180
Iteration 36/1000 | Loss: 0.00001180
Iteration 37/1000 | Loss: 0.00001177
Iteration 38/1000 | Loss: 0.00001177
Iteration 39/1000 | Loss: 0.00001176
Iteration 40/1000 | Loss: 0.00001176
Iteration 41/1000 | Loss: 0.00001175
Iteration 42/1000 | Loss: 0.00001175
Iteration 43/1000 | Loss: 0.00001174
Iteration 44/1000 | Loss: 0.00001174
Iteration 45/1000 | Loss: 0.00001174
Iteration 46/1000 | Loss: 0.00001173
Iteration 47/1000 | Loss: 0.00001173
Iteration 48/1000 | Loss: 0.00001172
Iteration 49/1000 | Loss: 0.00001172
Iteration 50/1000 | Loss: 0.00001171
Iteration 51/1000 | Loss: 0.00001171
Iteration 52/1000 | Loss: 0.00001170
Iteration 53/1000 | Loss: 0.00001170
Iteration 54/1000 | Loss: 0.00001170
Iteration 55/1000 | Loss: 0.00001169
Iteration 56/1000 | Loss: 0.00001169
Iteration 57/1000 | Loss: 0.00001169
Iteration 58/1000 | Loss: 0.00001169
Iteration 59/1000 | Loss: 0.00001168
Iteration 60/1000 | Loss: 0.00001168
Iteration 61/1000 | Loss: 0.00001168
Iteration 62/1000 | Loss: 0.00001168
Iteration 63/1000 | Loss: 0.00001168
Iteration 64/1000 | Loss: 0.00001167
Iteration 65/1000 | Loss: 0.00001167
Iteration 66/1000 | Loss: 0.00001167
Iteration 67/1000 | Loss: 0.00001167
Iteration 68/1000 | Loss: 0.00001167
Iteration 69/1000 | Loss: 0.00001167
Iteration 70/1000 | Loss: 0.00001167
Iteration 71/1000 | Loss: 0.00001167
Iteration 72/1000 | Loss: 0.00001166
Iteration 73/1000 | Loss: 0.00001166
Iteration 74/1000 | Loss: 0.00001166
Iteration 75/1000 | Loss: 0.00001166
Iteration 76/1000 | Loss: 0.00001166
Iteration 77/1000 | Loss: 0.00001166
Iteration 78/1000 | Loss: 0.00001166
Iteration 79/1000 | Loss: 0.00001165
Iteration 80/1000 | Loss: 0.00001165
Iteration 81/1000 | Loss: 0.00001165
Iteration 82/1000 | Loss: 0.00001165
Iteration 83/1000 | Loss: 0.00001165
Iteration 84/1000 | Loss: 0.00001165
Iteration 85/1000 | Loss: 0.00001165
Iteration 86/1000 | Loss: 0.00001164
Iteration 87/1000 | Loss: 0.00001164
Iteration 88/1000 | Loss: 0.00001164
Iteration 89/1000 | Loss: 0.00001164
Iteration 90/1000 | Loss: 0.00001164
Iteration 91/1000 | Loss: 0.00001164
Iteration 92/1000 | Loss: 0.00001164
Iteration 93/1000 | Loss: 0.00001164
Iteration 94/1000 | Loss: 0.00001163
Iteration 95/1000 | Loss: 0.00001163
Iteration 96/1000 | Loss: 0.00001163
Iteration 97/1000 | Loss: 0.00001163
Iteration 98/1000 | Loss: 0.00001163
Iteration 99/1000 | Loss: 0.00001163
Iteration 100/1000 | Loss: 0.00001163
Iteration 101/1000 | Loss: 0.00001163
Iteration 102/1000 | Loss: 0.00001163
Iteration 103/1000 | Loss: 0.00001163
Iteration 104/1000 | Loss: 0.00001163
Iteration 105/1000 | Loss: 0.00001163
Iteration 106/1000 | Loss: 0.00001163
Iteration 107/1000 | Loss: 0.00001162
Iteration 108/1000 | Loss: 0.00001162
Iteration 109/1000 | Loss: 0.00001162
Iteration 110/1000 | Loss: 0.00001162
Iteration 111/1000 | Loss: 0.00001162
Iteration 112/1000 | Loss: 0.00001162
Iteration 113/1000 | Loss: 0.00001162
Iteration 114/1000 | Loss: 0.00001162
Iteration 115/1000 | Loss: 0.00001161
Iteration 116/1000 | Loss: 0.00001161
Iteration 117/1000 | Loss: 0.00001161
Iteration 118/1000 | Loss: 0.00001161
Iteration 119/1000 | Loss: 0.00001161
Iteration 120/1000 | Loss: 0.00001161
Iteration 121/1000 | Loss: 0.00001161
Iteration 122/1000 | Loss: 0.00001161
Iteration 123/1000 | Loss: 0.00001161
Iteration 124/1000 | Loss: 0.00001161
Iteration 125/1000 | Loss: 0.00001161
Iteration 126/1000 | Loss: 0.00001161
Iteration 127/1000 | Loss: 0.00001161
Iteration 128/1000 | Loss: 0.00001160
Iteration 129/1000 | Loss: 0.00001160
Iteration 130/1000 | Loss: 0.00001160
Iteration 131/1000 | Loss: 0.00001160
Iteration 132/1000 | Loss: 0.00001160
Iteration 133/1000 | Loss: 0.00001160
Iteration 134/1000 | Loss: 0.00001160
Iteration 135/1000 | Loss: 0.00001160
Iteration 136/1000 | Loss: 0.00001160
Iteration 137/1000 | Loss: 0.00001160
Iteration 138/1000 | Loss: 0.00001160
Iteration 139/1000 | Loss: 0.00001160
Iteration 140/1000 | Loss: 0.00001159
Iteration 141/1000 | Loss: 0.00001159
Iteration 142/1000 | Loss: 0.00001159
Iteration 143/1000 | Loss: 0.00001159
Iteration 144/1000 | Loss: 0.00001159
Iteration 145/1000 | Loss: 0.00001159
Iteration 146/1000 | Loss: 0.00001159
Iteration 147/1000 | Loss: 0.00001159
Iteration 148/1000 | Loss: 0.00001159
Iteration 149/1000 | Loss: 0.00001159
Iteration 150/1000 | Loss: 0.00001159
Iteration 151/1000 | Loss: 0.00001159
Iteration 152/1000 | Loss: 0.00001159
Iteration 153/1000 | Loss: 0.00001159
Iteration 154/1000 | Loss: 0.00001159
Iteration 155/1000 | Loss: 0.00001159
Iteration 156/1000 | Loss: 0.00001158
Iteration 157/1000 | Loss: 0.00001158
Iteration 158/1000 | Loss: 0.00001158
Iteration 159/1000 | Loss: 0.00001158
Iteration 160/1000 | Loss: 0.00001158
Iteration 161/1000 | Loss: 0.00001158
Iteration 162/1000 | Loss: 0.00001158
Iteration 163/1000 | Loss: 0.00001158
Iteration 164/1000 | Loss: 0.00001158
Iteration 165/1000 | Loss: 0.00001158
Iteration 166/1000 | Loss: 0.00001158
Iteration 167/1000 | Loss: 0.00001158
Iteration 168/1000 | Loss: 0.00001158
Iteration 169/1000 | Loss: 0.00001158
Iteration 170/1000 | Loss: 0.00001158
Iteration 171/1000 | Loss: 0.00001158
Iteration 172/1000 | Loss: 0.00001158
Iteration 173/1000 | Loss: 0.00001158
Iteration 174/1000 | Loss: 0.00001158
Iteration 175/1000 | Loss: 0.00001158
Iteration 176/1000 | Loss: 0.00001158
Iteration 177/1000 | Loss: 0.00001157
Iteration 178/1000 | Loss: 0.00001157
Iteration 179/1000 | Loss: 0.00001157
Iteration 180/1000 | Loss: 0.00001157
Iteration 181/1000 | Loss: 0.00001157
Iteration 182/1000 | Loss: 0.00001157
Iteration 183/1000 | Loss: 0.00001157
Iteration 184/1000 | Loss: 0.00001157
Iteration 185/1000 | Loss: 0.00001157
Iteration 186/1000 | Loss: 0.00001157
Iteration 187/1000 | Loss: 0.00001157
Iteration 188/1000 | Loss: 0.00001157
Iteration 189/1000 | Loss: 0.00001157
Iteration 190/1000 | Loss: 0.00001157
Iteration 191/1000 | Loss: 0.00001157
Iteration 192/1000 | Loss: 0.00001157
Iteration 193/1000 | Loss: 0.00001156
Iteration 194/1000 | Loss: 0.00001156
Iteration 195/1000 | Loss: 0.00001156
Iteration 196/1000 | Loss: 0.00001156
Iteration 197/1000 | Loss: 0.00001156
Iteration 198/1000 | Loss: 0.00001156
Iteration 199/1000 | Loss: 0.00001156
Iteration 200/1000 | Loss: 0.00001156
Iteration 201/1000 | Loss: 0.00001156
Iteration 202/1000 | Loss: 0.00001156
Iteration 203/1000 | Loss: 0.00001156
Iteration 204/1000 | Loss: 0.00001156
Iteration 205/1000 | Loss: 0.00001156
Iteration 206/1000 | Loss: 0.00001156
Iteration 207/1000 | Loss: 0.00001156
Iteration 208/1000 | Loss: 0.00001156
Iteration 209/1000 | Loss: 0.00001156
Iteration 210/1000 | Loss: 0.00001155
Iteration 211/1000 | Loss: 0.00001155
Iteration 212/1000 | Loss: 0.00001155
Iteration 213/1000 | Loss: 0.00001155
Iteration 214/1000 | Loss: 0.00001155
Iteration 215/1000 | Loss: 0.00001154
Iteration 216/1000 | Loss: 0.00001154
Iteration 217/1000 | Loss: 0.00001154
Iteration 218/1000 | Loss: 0.00001154
Iteration 219/1000 | Loss: 0.00001154
Iteration 220/1000 | Loss: 0.00001154
Iteration 221/1000 | Loss: 0.00001154
Iteration 222/1000 | Loss: 0.00001154
Iteration 223/1000 | Loss: 0.00001153
Iteration 224/1000 | Loss: 0.00001153
Iteration 225/1000 | Loss: 0.00001153
Iteration 226/1000 | Loss: 0.00001153
Iteration 227/1000 | Loss: 0.00001153
Iteration 228/1000 | Loss: 0.00001153
Iteration 229/1000 | Loss: 0.00001153
Iteration 230/1000 | Loss: 0.00001153
Iteration 231/1000 | Loss: 0.00001153
Iteration 232/1000 | Loss: 0.00001153
Iteration 233/1000 | Loss: 0.00001153
Iteration 234/1000 | Loss: 0.00001153
Iteration 235/1000 | Loss: 0.00001152
Iteration 236/1000 | Loss: 0.00001152
Iteration 237/1000 | Loss: 0.00001152
Iteration 238/1000 | Loss: 0.00001152
Iteration 239/1000 | Loss: 0.00001152
Iteration 240/1000 | Loss: 0.00001152
Iteration 241/1000 | Loss: 0.00001152
Iteration 242/1000 | Loss: 0.00001152
Iteration 243/1000 | Loss: 0.00001152
Iteration 244/1000 | Loss: 0.00001152
Iteration 245/1000 | Loss: 0.00001152
Iteration 246/1000 | Loss: 0.00001151
Iteration 247/1000 | Loss: 0.00001151
Iteration 248/1000 | Loss: 0.00001151
Iteration 249/1000 | Loss: 0.00001151
Iteration 250/1000 | Loss: 0.00001151
Iteration 251/1000 | Loss: 0.00001151
Iteration 252/1000 | Loss: 0.00001151
Iteration 253/1000 | Loss: 0.00001151
Iteration 254/1000 | Loss: 0.00001151
Iteration 255/1000 | Loss: 0.00001151
Iteration 256/1000 | Loss: 0.00001151
Iteration 257/1000 | Loss: 0.00001151
Iteration 258/1000 | Loss: 0.00001151
Iteration 259/1000 | Loss: 0.00001151
Iteration 260/1000 | Loss: 0.00001151
Iteration 261/1000 | Loss: 0.00001151
Iteration 262/1000 | Loss: 0.00001151
Iteration 263/1000 | Loss: 0.00001151
Iteration 264/1000 | Loss: 0.00001151
Iteration 265/1000 | Loss: 0.00001150
Iteration 266/1000 | Loss: 0.00001150
Iteration 267/1000 | Loss: 0.00001150
Iteration 268/1000 | Loss: 0.00001150
Iteration 269/1000 | Loss: 0.00001150
Iteration 270/1000 | Loss: 0.00001150
Iteration 271/1000 | Loss: 0.00001150
Iteration 272/1000 | Loss: 0.00001150
Iteration 273/1000 | Loss: 0.00001150
Iteration 274/1000 | Loss: 0.00001150
Iteration 275/1000 | Loss: 0.00001150
Iteration 276/1000 | Loss: 0.00001150
Iteration 277/1000 | Loss: 0.00001150
Iteration 278/1000 | Loss: 0.00001150
Iteration 279/1000 | Loss: 0.00001150
Iteration 280/1000 | Loss: 0.00001150
Iteration 281/1000 | Loss: 0.00001150
Iteration 282/1000 | Loss: 0.00001150
Iteration 283/1000 | Loss: 0.00001150
Iteration 284/1000 | Loss: 0.00001150
Iteration 285/1000 | Loss: 0.00001150
Iteration 286/1000 | Loss: 0.00001150
Iteration 287/1000 | Loss: 0.00001150
Iteration 288/1000 | Loss: 0.00001150
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 288. Stopping optimization.
Last 5 losses: [1.1502197594381869e-05, 1.1502197594381869e-05, 1.1502197594381869e-05, 1.1502197594381869e-05, 1.1502197594381869e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1502197594381869e-05

Optimization complete. Final v2v error: 2.8313658237457275 mm

Highest mean error: 4.221226215362549 mm for frame 55

Lowest mean error: 2.454946994781494 mm for frame 23

Saving results

Total time: 44.96728491783142
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_018/1095/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_018/1095.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_018/1095
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00513977
Iteration 2/25 | Loss: 0.00129325
Iteration 3/25 | Loss: 0.00119835
Iteration 4/25 | Loss: 0.00118080
Iteration 5/25 | Loss: 0.00117376
Iteration 6/25 | Loss: 0.00117376
Iteration 7/25 | Loss: 0.00117376
Iteration 8/25 | Loss: 0.00117376
Iteration 9/25 | Loss: 0.00117376
Iteration 10/25 | Loss: 0.00117376
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0011737648164853454, 0.0011737648164853454, 0.0011737648164853454, 0.0011737648164853454, 0.0011737648164853454]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011737648164853454

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.79263771
Iteration 2/25 | Loss: 0.00072394
Iteration 3/25 | Loss: 0.00072394
Iteration 4/25 | Loss: 0.00072394
Iteration 5/25 | Loss: 0.00072394
Iteration 6/25 | Loss: 0.00072394
Iteration 7/25 | Loss: 0.00072394
Iteration 8/25 | Loss: 0.00072394
Iteration 9/25 | Loss: 0.00072394
Iteration 10/25 | Loss: 0.00072394
Iteration 11/25 | Loss: 0.00072394
Iteration 12/25 | Loss: 0.00072394
Iteration 13/25 | Loss: 0.00072394
Iteration 14/25 | Loss: 0.00072394
Iteration 15/25 | Loss: 0.00072394
Iteration 16/25 | Loss: 0.00072394
Iteration 17/25 | Loss: 0.00072394
Iteration 18/25 | Loss: 0.00072394
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0007239358965307474, 0.0007239358965307474, 0.0007239358965307474, 0.0007239358965307474, 0.0007239358965307474]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007239358965307474

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00072394
Iteration 2/1000 | Loss: 0.00003437
Iteration 3/1000 | Loss: 0.00002537
Iteration 4/1000 | Loss: 0.00002315
Iteration 5/1000 | Loss: 0.00002240
Iteration 6/1000 | Loss: 0.00002153
Iteration 7/1000 | Loss: 0.00002088
Iteration 8/1000 | Loss: 0.00002050
Iteration 9/1000 | Loss: 0.00002001
Iteration 10/1000 | Loss: 0.00001966
Iteration 11/1000 | Loss: 0.00001943
Iteration 12/1000 | Loss: 0.00001943
Iteration 13/1000 | Loss: 0.00001913
Iteration 14/1000 | Loss: 0.00001887
Iteration 15/1000 | Loss: 0.00001865
Iteration 16/1000 | Loss: 0.00001844
Iteration 17/1000 | Loss: 0.00001840
Iteration 18/1000 | Loss: 0.00001836
Iteration 19/1000 | Loss: 0.00001821
Iteration 20/1000 | Loss: 0.00001816
Iteration 21/1000 | Loss: 0.00001806
Iteration 22/1000 | Loss: 0.00001806
Iteration 23/1000 | Loss: 0.00001803
Iteration 24/1000 | Loss: 0.00001803
Iteration 25/1000 | Loss: 0.00001802
Iteration 26/1000 | Loss: 0.00001802
Iteration 27/1000 | Loss: 0.00001800
Iteration 28/1000 | Loss: 0.00001800
Iteration 29/1000 | Loss: 0.00001792
Iteration 30/1000 | Loss: 0.00001789
Iteration 31/1000 | Loss: 0.00001788
Iteration 32/1000 | Loss: 0.00001787
Iteration 33/1000 | Loss: 0.00001785
Iteration 34/1000 | Loss: 0.00001785
Iteration 35/1000 | Loss: 0.00001783
Iteration 36/1000 | Loss: 0.00001783
Iteration 37/1000 | Loss: 0.00001783
Iteration 38/1000 | Loss: 0.00001783
Iteration 39/1000 | Loss: 0.00001783
Iteration 40/1000 | Loss: 0.00001783
Iteration 41/1000 | Loss: 0.00001782
Iteration 42/1000 | Loss: 0.00001782
Iteration 43/1000 | Loss: 0.00001782
Iteration 44/1000 | Loss: 0.00001782
Iteration 45/1000 | Loss: 0.00001782
Iteration 46/1000 | Loss: 0.00001781
Iteration 47/1000 | Loss: 0.00001781
Iteration 48/1000 | Loss: 0.00001780
Iteration 49/1000 | Loss: 0.00001780
Iteration 50/1000 | Loss: 0.00001780
Iteration 51/1000 | Loss: 0.00001779
Iteration 52/1000 | Loss: 0.00001778
Iteration 53/1000 | Loss: 0.00001778
Iteration 54/1000 | Loss: 0.00001778
Iteration 55/1000 | Loss: 0.00001777
Iteration 56/1000 | Loss: 0.00001777
Iteration 57/1000 | Loss: 0.00001774
Iteration 58/1000 | Loss: 0.00001774
Iteration 59/1000 | Loss: 0.00001773
Iteration 60/1000 | Loss: 0.00001771
Iteration 61/1000 | Loss: 0.00001770
Iteration 62/1000 | Loss: 0.00001770
Iteration 63/1000 | Loss: 0.00001770
Iteration 64/1000 | Loss: 0.00001769
Iteration 65/1000 | Loss: 0.00001769
Iteration 66/1000 | Loss: 0.00001769
Iteration 67/1000 | Loss: 0.00001769
Iteration 68/1000 | Loss: 0.00001768
Iteration 69/1000 | Loss: 0.00001768
Iteration 70/1000 | Loss: 0.00001768
Iteration 71/1000 | Loss: 0.00001768
Iteration 72/1000 | Loss: 0.00001768
Iteration 73/1000 | Loss: 0.00001768
Iteration 74/1000 | Loss: 0.00001768
Iteration 75/1000 | Loss: 0.00001767
Iteration 76/1000 | Loss: 0.00001766
Iteration 77/1000 | Loss: 0.00001766
Iteration 78/1000 | Loss: 0.00001766
Iteration 79/1000 | Loss: 0.00001765
Iteration 80/1000 | Loss: 0.00001765
Iteration 81/1000 | Loss: 0.00001765
Iteration 82/1000 | Loss: 0.00001765
Iteration 83/1000 | Loss: 0.00001765
Iteration 84/1000 | Loss: 0.00001765
Iteration 85/1000 | Loss: 0.00001765
Iteration 86/1000 | Loss: 0.00001765
Iteration 87/1000 | Loss: 0.00001764
Iteration 88/1000 | Loss: 0.00001764
Iteration 89/1000 | Loss: 0.00001764
Iteration 90/1000 | Loss: 0.00001764
Iteration 91/1000 | Loss: 0.00001764
Iteration 92/1000 | Loss: 0.00001764
Iteration 93/1000 | Loss: 0.00001764
Iteration 94/1000 | Loss: 0.00001764
Iteration 95/1000 | Loss: 0.00001763
Iteration 96/1000 | Loss: 0.00001763
Iteration 97/1000 | Loss: 0.00001762
Iteration 98/1000 | Loss: 0.00001762
Iteration 99/1000 | Loss: 0.00001762
Iteration 100/1000 | Loss: 0.00001762
Iteration 101/1000 | Loss: 0.00001761
Iteration 102/1000 | Loss: 0.00001761
Iteration 103/1000 | Loss: 0.00001761
Iteration 104/1000 | Loss: 0.00001761
Iteration 105/1000 | Loss: 0.00001761
Iteration 106/1000 | Loss: 0.00001761
Iteration 107/1000 | Loss: 0.00001761
Iteration 108/1000 | Loss: 0.00001760
Iteration 109/1000 | Loss: 0.00001760
Iteration 110/1000 | Loss: 0.00001760
Iteration 111/1000 | Loss: 0.00001760
Iteration 112/1000 | Loss: 0.00001760
Iteration 113/1000 | Loss: 0.00001759
Iteration 114/1000 | Loss: 0.00001759
Iteration 115/1000 | Loss: 0.00001759
Iteration 116/1000 | Loss: 0.00001759
Iteration 117/1000 | Loss: 0.00001759
Iteration 118/1000 | Loss: 0.00001758
Iteration 119/1000 | Loss: 0.00001757
Iteration 120/1000 | Loss: 0.00001757
Iteration 121/1000 | Loss: 0.00001757
Iteration 122/1000 | Loss: 0.00001757
Iteration 123/1000 | Loss: 0.00001757
Iteration 124/1000 | Loss: 0.00001757
Iteration 125/1000 | Loss: 0.00001757
Iteration 126/1000 | Loss: 0.00001757
Iteration 127/1000 | Loss: 0.00001756
Iteration 128/1000 | Loss: 0.00001756
Iteration 129/1000 | Loss: 0.00001756
Iteration 130/1000 | Loss: 0.00001756
Iteration 131/1000 | Loss: 0.00001756
Iteration 132/1000 | Loss: 0.00001756
Iteration 133/1000 | Loss: 0.00001756
Iteration 134/1000 | Loss: 0.00001756
Iteration 135/1000 | Loss: 0.00001756
Iteration 136/1000 | Loss: 0.00001756
Iteration 137/1000 | Loss: 0.00001755
Iteration 138/1000 | Loss: 0.00001755
Iteration 139/1000 | Loss: 0.00001755
Iteration 140/1000 | Loss: 0.00001755
Iteration 141/1000 | Loss: 0.00001755
Iteration 142/1000 | Loss: 0.00001754
Iteration 143/1000 | Loss: 0.00001754
Iteration 144/1000 | Loss: 0.00001754
Iteration 145/1000 | Loss: 0.00001754
Iteration 146/1000 | Loss: 0.00001754
Iteration 147/1000 | Loss: 0.00001753
Iteration 148/1000 | Loss: 0.00001753
Iteration 149/1000 | Loss: 0.00001753
Iteration 150/1000 | Loss: 0.00001753
Iteration 151/1000 | Loss: 0.00001753
Iteration 152/1000 | Loss: 0.00001753
Iteration 153/1000 | Loss: 0.00001753
Iteration 154/1000 | Loss: 0.00001753
Iteration 155/1000 | Loss: 0.00001753
Iteration 156/1000 | Loss: 0.00001752
Iteration 157/1000 | Loss: 0.00001752
Iteration 158/1000 | Loss: 0.00001752
Iteration 159/1000 | Loss: 0.00001752
Iteration 160/1000 | Loss: 0.00001752
Iteration 161/1000 | Loss: 0.00001752
Iteration 162/1000 | Loss: 0.00001752
Iteration 163/1000 | Loss: 0.00001752
Iteration 164/1000 | Loss: 0.00001752
Iteration 165/1000 | Loss: 0.00001752
Iteration 166/1000 | Loss: 0.00001752
Iteration 167/1000 | Loss: 0.00001752
Iteration 168/1000 | Loss: 0.00001752
Iteration 169/1000 | Loss: 0.00001752
Iteration 170/1000 | Loss: 0.00001752
Iteration 171/1000 | Loss: 0.00001752
Iteration 172/1000 | Loss: 0.00001752
Iteration 173/1000 | Loss: 0.00001752
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 173. Stopping optimization.
Last 5 losses: [1.751765375956893e-05, 1.751765375956893e-05, 1.751765375956893e-05, 1.751765375956893e-05, 1.751765375956893e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.751765375956893e-05

Optimization complete. Final v2v error: 3.5529086589813232 mm

Highest mean error: 4.142948627471924 mm for frame 265

Lowest mean error: 3.465473175048828 mm for frame 28

Saving results

Total time: 53.63814997673035
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_018/1088/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_018/1088.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_018/1088
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00810894
Iteration 2/25 | Loss: 0.00176472
Iteration 3/25 | Loss: 0.00140413
Iteration 4/25 | Loss: 0.00133067
Iteration 5/25 | Loss: 0.00131039
Iteration 6/25 | Loss: 0.00128841
Iteration 7/25 | Loss: 0.00132302
Iteration 8/25 | Loss: 0.00130111
Iteration 9/25 | Loss: 0.00131917
Iteration 10/25 | Loss: 0.00127848
Iteration 11/25 | Loss: 0.00127264
Iteration 12/25 | Loss: 0.00127170
Iteration 13/25 | Loss: 0.00127138
Iteration 14/25 | Loss: 0.00127091
Iteration 15/25 | Loss: 0.00127737
Iteration 16/25 | Loss: 0.00127671
Iteration 17/25 | Loss: 0.00127332
Iteration 18/25 | Loss: 0.00127681
Iteration 19/25 | Loss: 0.00127498
Iteration 20/25 | Loss: 0.00127224
Iteration 21/25 | Loss: 0.00127520
Iteration 22/25 | Loss: 0.00127525
Iteration 23/25 | Loss: 0.00127341
Iteration 24/25 | Loss: 0.00127401
Iteration 25/25 | Loss: 0.00127189

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 6.62605715
Iteration 2/25 | Loss: 0.00151834
Iteration 3/25 | Loss: 0.00151816
Iteration 4/25 | Loss: 0.00151816
Iteration 5/25 | Loss: 0.00151816
Iteration 6/25 | Loss: 0.00151816
Iteration 7/25 | Loss: 0.00151816
Iteration 8/25 | Loss: 0.00151816
Iteration 9/25 | Loss: 0.00151816
Iteration 10/25 | Loss: 0.00151816
Iteration 11/25 | Loss: 0.00151816
Iteration 12/25 | Loss: 0.00151816
Iteration 13/25 | Loss: 0.00151816
Iteration 14/25 | Loss: 0.00151816
Iteration 15/25 | Loss: 0.00151816
Iteration 16/25 | Loss: 0.00151816
Iteration 17/25 | Loss: 0.00151816
Iteration 18/25 | Loss: 0.00151816
Iteration 19/25 | Loss: 0.00151816
Iteration 20/25 | Loss: 0.00151816
Iteration 21/25 | Loss: 0.00151816
Iteration 22/25 | Loss: 0.00151816
Iteration 23/25 | Loss: 0.00151816
Iteration 24/25 | Loss: 0.00151816
Iteration 25/25 | Loss: 0.00151816

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00151816
Iteration 2/1000 | Loss: 0.00031944
Iteration 3/1000 | Loss: 0.00011617
Iteration 4/1000 | Loss: 0.00017544
Iteration 5/1000 | Loss: 0.00009391
Iteration 6/1000 | Loss: 0.00012752
Iteration 7/1000 | Loss: 0.00008251
Iteration 8/1000 | Loss: 0.00017508
Iteration 9/1000 | Loss: 0.00019640
Iteration 10/1000 | Loss: 0.00030370
Iteration 11/1000 | Loss: 0.00027843
Iteration 12/1000 | Loss: 0.00022720
Iteration 13/1000 | Loss: 0.00025609
Iteration 14/1000 | Loss: 0.00008254
Iteration 15/1000 | Loss: 0.00026489
Iteration 16/1000 | Loss: 0.00021640
Iteration 17/1000 | Loss: 0.00014338
Iteration 18/1000 | Loss: 0.00020079
Iteration 19/1000 | Loss: 0.00013759
Iteration 20/1000 | Loss: 0.00043583
Iteration 21/1000 | Loss: 0.00025670
Iteration 22/1000 | Loss: 0.00022925
Iteration 23/1000 | Loss: 0.00025225
Iteration 24/1000 | Loss: 0.00025810
Iteration 25/1000 | Loss: 0.00016384
Iteration 26/1000 | Loss: 0.00011819
Iteration 27/1000 | Loss: 0.00017860
Iteration 28/1000 | Loss: 0.00016926
Iteration 29/1000 | Loss: 0.00026155
Iteration 30/1000 | Loss: 0.00026038
Iteration 31/1000 | Loss: 0.00027178
Iteration 32/1000 | Loss: 0.00027891
Iteration 33/1000 | Loss: 0.00007524
Iteration 34/1000 | Loss: 0.00010159
Iteration 35/1000 | Loss: 0.00109468
Iteration 36/1000 | Loss: 0.00062947
Iteration 37/1000 | Loss: 0.00019614
Iteration 38/1000 | Loss: 0.00023821
Iteration 39/1000 | Loss: 0.00022875
Iteration 40/1000 | Loss: 0.00012949
Iteration 41/1000 | Loss: 0.00013667
Iteration 42/1000 | Loss: 0.00019854
Iteration 43/1000 | Loss: 0.00017030
Iteration 44/1000 | Loss: 0.00020104
Iteration 45/1000 | Loss: 0.00040792
Iteration 46/1000 | Loss: 0.00019274
Iteration 47/1000 | Loss: 0.00027105
Iteration 48/1000 | Loss: 0.00023408
Iteration 49/1000 | Loss: 0.00037438
Iteration 50/1000 | Loss: 0.00008014
Iteration 51/1000 | Loss: 0.00007097
Iteration 52/1000 | Loss: 0.00006785
Iteration 53/1000 | Loss: 0.00006549
Iteration 54/1000 | Loss: 0.00006415
Iteration 55/1000 | Loss: 0.00006263
Iteration 56/1000 | Loss: 0.00006152
Iteration 57/1000 | Loss: 0.00086801
Iteration 58/1000 | Loss: 0.00070498
Iteration 59/1000 | Loss: 0.00139501
Iteration 60/1000 | Loss: 0.00012329
Iteration 61/1000 | Loss: 0.00006902
Iteration 62/1000 | Loss: 0.00006040
Iteration 63/1000 | Loss: 0.00005481
Iteration 64/1000 | Loss: 0.00005188
Iteration 65/1000 | Loss: 0.00004886
Iteration 66/1000 | Loss: 0.00004708
Iteration 67/1000 | Loss: 0.00004578
Iteration 68/1000 | Loss: 0.00004494
Iteration 69/1000 | Loss: 0.00004440
Iteration 70/1000 | Loss: 0.00004388
Iteration 71/1000 | Loss: 0.00004348
Iteration 72/1000 | Loss: 0.00004313
Iteration 73/1000 | Loss: 0.00004278
Iteration 74/1000 | Loss: 0.00004258
Iteration 75/1000 | Loss: 0.00004244
Iteration 76/1000 | Loss: 0.00004244
Iteration 77/1000 | Loss: 0.00004235
Iteration 78/1000 | Loss: 0.00004229
Iteration 79/1000 | Loss: 0.00004229
Iteration 80/1000 | Loss: 0.00004227
Iteration 81/1000 | Loss: 0.00004226
Iteration 82/1000 | Loss: 0.00004226
Iteration 83/1000 | Loss: 0.00004223
Iteration 84/1000 | Loss: 0.00004213
Iteration 85/1000 | Loss: 0.00004213
Iteration 86/1000 | Loss: 0.00004213
Iteration 87/1000 | Loss: 0.00004211
Iteration 88/1000 | Loss: 0.00004210
Iteration 89/1000 | Loss: 0.00004209
Iteration 90/1000 | Loss: 0.00004209
Iteration 91/1000 | Loss: 0.00004205
Iteration 92/1000 | Loss: 0.00004204
Iteration 93/1000 | Loss: 0.00004201
Iteration 94/1000 | Loss: 0.00004192
Iteration 95/1000 | Loss: 0.00004192
Iteration 96/1000 | Loss: 0.00004186
Iteration 97/1000 | Loss: 0.00004183
Iteration 98/1000 | Loss: 0.00004182
Iteration 99/1000 | Loss: 0.00004181
Iteration 100/1000 | Loss: 0.00004177
Iteration 101/1000 | Loss: 0.00004176
Iteration 102/1000 | Loss: 0.00004176
Iteration 103/1000 | Loss: 0.00004175
Iteration 104/1000 | Loss: 0.00004171
Iteration 105/1000 | Loss: 0.00004171
Iteration 106/1000 | Loss: 0.00004168
Iteration 107/1000 | Loss: 0.00004167
Iteration 108/1000 | Loss: 0.00004165
Iteration 109/1000 | Loss: 0.00004165
Iteration 110/1000 | Loss: 0.00004164
Iteration 111/1000 | Loss: 0.00004164
Iteration 112/1000 | Loss: 0.00004163
Iteration 113/1000 | Loss: 0.00004163
Iteration 114/1000 | Loss: 0.00004162
Iteration 115/1000 | Loss: 0.00004161
Iteration 116/1000 | Loss: 0.00004158
Iteration 117/1000 | Loss: 0.00004155
Iteration 118/1000 | Loss: 0.00004155
Iteration 119/1000 | Loss: 0.00004154
Iteration 120/1000 | Loss: 0.00004153
Iteration 121/1000 | Loss: 0.00004153
Iteration 122/1000 | Loss: 0.00004153
Iteration 123/1000 | Loss: 0.00004153
Iteration 124/1000 | Loss: 0.00004152
Iteration 125/1000 | Loss: 0.00004152
Iteration 126/1000 | Loss: 0.00004152
Iteration 127/1000 | Loss: 0.00004151
Iteration 128/1000 | Loss: 0.00004151
Iteration 129/1000 | Loss: 0.00016692
Iteration 130/1000 | Loss: 0.00020492
Iteration 131/1000 | Loss: 0.00004139
Iteration 132/1000 | Loss: 0.00003838
Iteration 133/1000 | Loss: 0.00003641
Iteration 134/1000 | Loss: 0.00003483
Iteration 135/1000 | Loss: 0.00003375
Iteration 136/1000 | Loss: 0.00003319
Iteration 137/1000 | Loss: 0.00003269
Iteration 138/1000 | Loss: 0.00003228
Iteration 139/1000 | Loss: 0.00003199
Iteration 140/1000 | Loss: 0.00003179
Iteration 141/1000 | Loss: 0.00003161
Iteration 142/1000 | Loss: 0.00003157
Iteration 143/1000 | Loss: 0.00003148
Iteration 144/1000 | Loss: 0.00003145
Iteration 145/1000 | Loss: 0.00003141
Iteration 146/1000 | Loss: 0.00003139
Iteration 147/1000 | Loss: 0.00003138
Iteration 148/1000 | Loss: 0.00003138
Iteration 149/1000 | Loss: 0.00003138
Iteration 150/1000 | Loss: 0.00003137
Iteration 151/1000 | Loss: 0.00003137
Iteration 152/1000 | Loss: 0.00003133
Iteration 153/1000 | Loss: 0.00003132
Iteration 154/1000 | Loss: 0.00003131
Iteration 155/1000 | Loss: 0.00003131
Iteration 156/1000 | Loss: 0.00003130
Iteration 157/1000 | Loss: 0.00003129
Iteration 158/1000 | Loss: 0.00003129
Iteration 159/1000 | Loss: 0.00003128
Iteration 160/1000 | Loss: 0.00003128
Iteration 161/1000 | Loss: 0.00003128
Iteration 162/1000 | Loss: 0.00003128
Iteration 163/1000 | Loss: 0.00003128
Iteration 164/1000 | Loss: 0.00003127
Iteration 165/1000 | Loss: 0.00003127
Iteration 166/1000 | Loss: 0.00003127
Iteration 167/1000 | Loss: 0.00003127
Iteration 168/1000 | Loss: 0.00003126
Iteration 169/1000 | Loss: 0.00003126
Iteration 170/1000 | Loss: 0.00003125
Iteration 171/1000 | Loss: 0.00003125
Iteration 172/1000 | Loss: 0.00003125
Iteration 173/1000 | Loss: 0.00003124
Iteration 174/1000 | Loss: 0.00003124
Iteration 175/1000 | Loss: 0.00003124
Iteration 176/1000 | Loss: 0.00003123
Iteration 177/1000 | Loss: 0.00003123
Iteration 178/1000 | Loss: 0.00003123
Iteration 179/1000 | Loss: 0.00003122
Iteration 180/1000 | Loss: 0.00003122
Iteration 181/1000 | Loss: 0.00003122
Iteration 182/1000 | Loss: 0.00003121
Iteration 183/1000 | Loss: 0.00003121
Iteration 184/1000 | Loss: 0.00003120
Iteration 185/1000 | Loss: 0.00003120
Iteration 186/1000 | Loss: 0.00003120
Iteration 187/1000 | Loss: 0.00003120
Iteration 188/1000 | Loss: 0.00003119
Iteration 189/1000 | Loss: 0.00003119
Iteration 190/1000 | Loss: 0.00003119
Iteration 191/1000 | Loss: 0.00003118
Iteration 192/1000 | Loss: 0.00003118
Iteration 193/1000 | Loss: 0.00003118
Iteration 194/1000 | Loss: 0.00003118
Iteration 195/1000 | Loss: 0.00003117
Iteration 196/1000 | Loss: 0.00003117
Iteration 197/1000 | Loss: 0.00003117
Iteration 198/1000 | Loss: 0.00003117
Iteration 199/1000 | Loss: 0.00003117
Iteration 200/1000 | Loss: 0.00003116
Iteration 201/1000 | Loss: 0.00003116
Iteration 202/1000 | Loss: 0.00003116
Iteration 203/1000 | Loss: 0.00003116
Iteration 204/1000 | Loss: 0.00003115
Iteration 205/1000 | Loss: 0.00003115
Iteration 206/1000 | Loss: 0.00003115
Iteration 207/1000 | Loss: 0.00003115
Iteration 208/1000 | Loss: 0.00003114
Iteration 209/1000 | Loss: 0.00003114
Iteration 210/1000 | Loss: 0.00003114
Iteration 211/1000 | Loss: 0.00003113
Iteration 212/1000 | Loss: 0.00003113
Iteration 213/1000 | Loss: 0.00003113
Iteration 214/1000 | Loss: 0.00003113
Iteration 215/1000 | Loss: 0.00003112
Iteration 216/1000 | Loss: 0.00003112
Iteration 217/1000 | Loss: 0.00003112
Iteration 218/1000 | Loss: 0.00003112
Iteration 219/1000 | Loss: 0.00003111
Iteration 220/1000 | Loss: 0.00003111
Iteration 221/1000 | Loss: 0.00003111
Iteration 222/1000 | Loss: 0.00003111
Iteration 223/1000 | Loss: 0.00003111
Iteration 224/1000 | Loss: 0.00003111
Iteration 225/1000 | Loss: 0.00003111
Iteration 226/1000 | Loss: 0.00003111
Iteration 227/1000 | Loss: 0.00003110
Iteration 228/1000 | Loss: 0.00003110
Iteration 229/1000 | Loss: 0.00003110
Iteration 230/1000 | Loss: 0.00003110
Iteration 231/1000 | Loss: 0.00003109
Iteration 232/1000 | Loss: 0.00003109
Iteration 233/1000 | Loss: 0.00003109
Iteration 234/1000 | Loss: 0.00003109
Iteration 235/1000 | Loss: 0.00003109
Iteration 236/1000 | Loss: 0.00003109
Iteration 237/1000 | Loss: 0.00003109
Iteration 238/1000 | Loss: 0.00003109
Iteration 239/1000 | Loss: 0.00003109
Iteration 240/1000 | Loss: 0.00003109
Iteration 241/1000 | Loss: 0.00003109
Iteration 242/1000 | Loss: 0.00003109
Iteration 243/1000 | Loss: 0.00003108
Iteration 244/1000 | Loss: 0.00003108
Iteration 245/1000 | Loss: 0.00003108
Iteration 246/1000 | Loss: 0.00003108
Iteration 247/1000 | Loss: 0.00003108
Iteration 248/1000 | Loss: 0.00003108
Iteration 249/1000 | Loss: 0.00003108
Iteration 250/1000 | Loss: 0.00003108
Iteration 251/1000 | Loss: 0.00003108
Iteration 252/1000 | Loss: 0.00003108
Iteration 253/1000 | Loss: 0.00003107
Iteration 254/1000 | Loss: 0.00003107
Iteration 255/1000 | Loss: 0.00003107
Iteration 256/1000 | Loss: 0.00003107
Iteration 257/1000 | Loss: 0.00003107
Iteration 258/1000 | Loss: 0.00003107
Iteration 259/1000 | Loss: 0.00003106
Iteration 260/1000 | Loss: 0.00003106
Iteration 261/1000 | Loss: 0.00003106
Iteration 262/1000 | Loss: 0.00003106
Iteration 263/1000 | Loss: 0.00003106
Iteration 264/1000 | Loss: 0.00003106
Iteration 265/1000 | Loss: 0.00003106
Iteration 266/1000 | Loss: 0.00003106
Iteration 267/1000 | Loss: 0.00003106
Iteration 268/1000 | Loss: 0.00003106
Iteration 269/1000 | Loss: 0.00003105
Iteration 270/1000 | Loss: 0.00003105
Iteration 271/1000 | Loss: 0.00003105
Iteration 272/1000 | Loss: 0.00003105
Iteration 273/1000 | Loss: 0.00003105
Iteration 274/1000 | Loss: 0.00003105
Iteration 275/1000 | Loss: 0.00003105
Iteration 276/1000 | Loss: 0.00003105
Iteration 277/1000 | Loss: 0.00003105
Iteration 278/1000 | Loss: 0.00003105
Iteration 279/1000 | Loss: 0.00003105
Iteration 280/1000 | Loss: 0.00003105
Iteration 281/1000 | Loss: 0.00003105
Iteration 282/1000 | Loss: 0.00003105
Iteration 283/1000 | Loss: 0.00003105
Iteration 284/1000 | Loss: 0.00003105
Iteration 285/1000 | Loss: 0.00003105
Iteration 286/1000 | Loss: 0.00003105
Iteration 287/1000 | Loss: 0.00003105
Iteration 288/1000 | Loss: 0.00003105
Iteration 289/1000 | Loss: 0.00003105
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 289. Stopping optimization.
Last 5 losses: [3.104853385593742e-05, 3.104853385593742e-05, 3.104853385593742e-05, 3.104853385593742e-05, 3.104853385593742e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.104853385593742e-05

Optimization complete. Final v2v error: 4.2292962074279785 mm

Highest mean error: 12.492685317993164 mm for frame 64

Lowest mean error: 2.7625489234924316 mm for frame 104

Saving results

Total time: 193.35091471672058
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_018/1096/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_018/1096.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_018/1096
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00809534
Iteration 2/25 | Loss: 0.00134448
Iteration 3/25 | Loss: 0.00117876
Iteration 4/25 | Loss: 0.00113901
Iteration 5/25 | Loss: 0.00115025
Iteration 6/25 | Loss: 0.00113243
Iteration 7/25 | Loss: 0.00113197
Iteration 8/25 | Loss: 0.00113173
Iteration 9/25 | Loss: 0.00113160
Iteration 10/25 | Loss: 0.00113159
Iteration 11/25 | Loss: 0.00113159
Iteration 12/25 | Loss: 0.00113159
Iteration 13/25 | Loss: 0.00113159
Iteration 14/25 | Loss: 0.00113159
Iteration 15/25 | Loss: 0.00113159
Iteration 16/25 | Loss: 0.00113159
Iteration 17/25 | Loss: 0.00113159
Iteration 18/25 | Loss: 0.00113158
Iteration 19/25 | Loss: 0.00113158
Iteration 20/25 | Loss: 0.00113158
Iteration 21/25 | Loss: 0.00113158
Iteration 22/25 | Loss: 0.00113158
Iteration 23/25 | Loss: 0.00113158
Iteration 24/25 | Loss: 0.00113158
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0011315835872665048, 0.0011315835872665048, 0.0011315835872665048, 0.0011315835872665048, 0.0011315835872665048]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011315835872665048

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.47821045
Iteration 2/25 | Loss: 0.00086548
Iteration 3/25 | Loss: 0.00086548
Iteration 4/25 | Loss: 0.00086548
Iteration 5/25 | Loss: 0.00086548
Iteration 6/25 | Loss: 0.00086548
Iteration 7/25 | Loss: 0.00086548
Iteration 8/25 | Loss: 0.00086548
Iteration 9/25 | Loss: 0.00086548
Iteration 10/25 | Loss: 0.00086548
Iteration 11/25 | Loss: 0.00086547
Iteration 12/25 | Loss: 0.00086547
Iteration 13/25 | Loss: 0.00086547
Iteration 14/25 | Loss: 0.00086547
Iteration 15/25 | Loss: 0.00086547
Iteration 16/25 | Loss: 0.00086547
Iteration 17/25 | Loss: 0.00086547
Iteration 18/25 | Loss: 0.00086547
Iteration 19/25 | Loss: 0.00086547
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0008654748671688139, 0.0008654748671688139, 0.0008654748671688139, 0.0008654748671688139, 0.0008654748671688139]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008654748671688139

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00086547
Iteration 2/1000 | Loss: 0.00002412
Iteration 3/1000 | Loss: 0.00001722
Iteration 4/1000 | Loss: 0.00001496
Iteration 5/1000 | Loss: 0.00001391
Iteration 6/1000 | Loss: 0.00001330
Iteration 7/1000 | Loss: 0.00001293
Iteration 8/1000 | Loss: 0.00001256
Iteration 9/1000 | Loss: 0.00001238
Iteration 10/1000 | Loss: 0.00001222
Iteration 11/1000 | Loss: 0.00001200
Iteration 12/1000 | Loss: 0.00001189
Iteration 13/1000 | Loss: 0.00001182
Iteration 14/1000 | Loss: 0.00001176
Iteration 15/1000 | Loss: 0.00001172
Iteration 16/1000 | Loss: 0.00001171
Iteration 17/1000 | Loss: 0.00001158
Iteration 18/1000 | Loss: 0.00001153
Iteration 19/1000 | Loss: 0.00001150
Iteration 20/1000 | Loss: 0.00001149
Iteration 21/1000 | Loss: 0.00001148
Iteration 22/1000 | Loss: 0.00001147
Iteration 23/1000 | Loss: 0.00001147
Iteration 24/1000 | Loss: 0.00001145
Iteration 25/1000 | Loss: 0.00001145
Iteration 26/1000 | Loss: 0.00001144
Iteration 27/1000 | Loss: 0.00001144
Iteration 28/1000 | Loss: 0.00001143
Iteration 29/1000 | Loss: 0.00001140
Iteration 30/1000 | Loss: 0.00001137
Iteration 31/1000 | Loss: 0.00001136
Iteration 32/1000 | Loss: 0.00001134
Iteration 33/1000 | Loss: 0.00001134
Iteration 34/1000 | Loss: 0.00001133
Iteration 35/1000 | Loss: 0.00001133
Iteration 36/1000 | Loss: 0.00001133
Iteration 37/1000 | Loss: 0.00001132
Iteration 38/1000 | Loss: 0.00001132
Iteration 39/1000 | Loss: 0.00001131
Iteration 40/1000 | Loss: 0.00001131
Iteration 41/1000 | Loss: 0.00001130
Iteration 42/1000 | Loss: 0.00001129
Iteration 43/1000 | Loss: 0.00001129
Iteration 44/1000 | Loss: 0.00001129
Iteration 45/1000 | Loss: 0.00001128
Iteration 46/1000 | Loss: 0.00001128
Iteration 47/1000 | Loss: 0.00001128
Iteration 48/1000 | Loss: 0.00001125
Iteration 49/1000 | Loss: 0.00001125
Iteration 50/1000 | Loss: 0.00001125
Iteration 51/1000 | Loss: 0.00001125
Iteration 52/1000 | Loss: 0.00001125
Iteration 53/1000 | Loss: 0.00001125
Iteration 54/1000 | Loss: 0.00001125
Iteration 55/1000 | Loss: 0.00001125
Iteration 56/1000 | Loss: 0.00001125
Iteration 57/1000 | Loss: 0.00001125
Iteration 58/1000 | Loss: 0.00001125
Iteration 59/1000 | Loss: 0.00001124
Iteration 60/1000 | Loss: 0.00001124
Iteration 61/1000 | Loss: 0.00001124
Iteration 62/1000 | Loss: 0.00001123
Iteration 63/1000 | Loss: 0.00001123
Iteration 64/1000 | Loss: 0.00001123
Iteration 65/1000 | Loss: 0.00001120
Iteration 66/1000 | Loss: 0.00001120
Iteration 67/1000 | Loss: 0.00001120
Iteration 68/1000 | Loss: 0.00001120
Iteration 69/1000 | Loss: 0.00001120
Iteration 70/1000 | Loss: 0.00001120
Iteration 71/1000 | Loss: 0.00001120
Iteration 72/1000 | Loss: 0.00001120
Iteration 73/1000 | Loss: 0.00001120
Iteration 74/1000 | Loss: 0.00001120
Iteration 75/1000 | Loss: 0.00001119
Iteration 76/1000 | Loss: 0.00001119
Iteration 77/1000 | Loss: 0.00001119
Iteration 78/1000 | Loss: 0.00001119
Iteration 79/1000 | Loss: 0.00001119
Iteration 80/1000 | Loss: 0.00001119
Iteration 81/1000 | Loss: 0.00001118
Iteration 82/1000 | Loss: 0.00001117
Iteration 83/1000 | Loss: 0.00001117
Iteration 84/1000 | Loss: 0.00001116
Iteration 85/1000 | Loss: 0.00001116
Iteration 86/1000 | Loss: 0.00001116
Iteration 87/1000 | Loss: 0.00001115
Iteration 88/1000 | Loss: 0.00001115
Iteration 89/1000 | Loss: 0.00001115
Iteration 90/1000 | Loss: 0.00001114
Iteration 91/1000 | Loss: 0.00001114
Iteration 92/1000 | Loss: 0.00001114
Iteration 93/1000 | Loss: 0.00001114
Iteration 94/1000 | Loss: 0.00001114
Iteration 95/1000 | Loss: 0.00001114
Iteration 96/1000 | Loss: 0.00001114
Iteration 97/1000 | Loss: 0.00001114
Iteration 98/1000 | Loss: 0.00001113
Iteration 99/1000 | Loss: 0.00001113
Iteration 100/1000 | Loss: 0.00001113
Iteration 101/1000 | Loss: 0.00001113
Iteration 102/1000 | Loss: 0.00001112
Iteration 103/1000 | Loss: 0.00001112
Iteration 104/1000 | Loss: 0.00001112
Iteration 105/1000 | Loss: 0.00001112
Iteration 106/1000 | Loss: 0.00001112
Iteration 107/1000 | Loss: 0.00001112
Iteration 108/1000 | Loss: 0.00001112
Iteration 109/1000 | Loss: 0.00001111
Iteration 110/1000 | Loss: 0.00001111
Iteration 111/1000 | Loss: 0.00001111
Iteration 112/1000 | Loss: 0.00001111
Iteration 113/1000 | Loss: 0.00001111
Iteration 114/1000 | Loss: 0.00001111
Iteration 115/1000 | Loss: 0.00001111
Iteration 116/1000 | Loss: 0.00001111
Iteration 117/1000 | Loss: 0.00001111
Iteration 118/1000 | Loss: 0.00001111
Iteration 119/1000 | Loss: 0.00001110
Iteration 120/1000 | Loss: 0.00001110
Iteration 121/1000 | Loss: 0.00001110
Iteration 122/1000 | Loss: 0.00001109
Iteration 123/1000 | Loss: 0.00001109
Iteration 124/1000 | Loss: 0.00001109
Iteration 125/1000 | Loss: 0.00001109
Iteration 126/1000 | Loss: 0.00001109
Iteration 127/1000 | Loss: 0.00001109
Iteration 128/1000 | Loss: 0.00001109
Iteration 129/1000 | Loss: 0.00001109
Iteration 130/1000 | Loss: 0.00001109
Iteration 131/1000 | Loss: 0.00001109
Iteration 132/1000 | Loss: 0.00001109
Iteration 133/1000 | Loss: 0.00001109
Iteration 134/1000 | Loss: 0.00001109
Iteration 135/1000 | Loss: 0.00001109
Iteration 136/1000 | Loss: 0.00001109
Iteration 137/1000 | Loss: 0.00001109
Iteration 138/1000 | Loss: 0.00001109
Iteration 139/1000 | Loss: 0.00001109
Iteration 140/1000 | Loss: 0.00001109
Iteration 141/1000 | Loss: 0.00001109
Iteration 142/1000 | Loss: 0.00001109
Iteration 143/1000 | Loss: 0.00001109
Iteration 144/1000 | Loss: 0.00001109
Iteration 145/1000 | Loss: 0.00001109
Iteration 146/1000 | Loss: 0.00001109
Iteration 147/1000 | Loss: 0.00001109
Iteration 148/1000 | Loss: 0.00001109
Iteration 149/1000 | Loss: 0.00001109
Iteration 150/1000 | Loss: 0.00001109
Iteration 151/1000 | Loss: 0.00001109
Iteration 152/1000 | Loss: 0.00001109
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 152. Stopping optimization.
Last 5 losses: [1.1086469385190867e-05, 1.1086469385190867e-05, 1.1086469385190867e-05, 1.1086469385190867e-05, 1.1086469385190867e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1086469385190867e-05

Optimization complete. Final v2v error: 2.8642466068267822 mm

Highest mean error: 3.2143096923828125 mm for frame 49

Lowest mean error: 2.625150680541992 mm for frame 157

Saving results

Total time: 44.50627851486206
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_018/1076/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_018/1076.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_018/1076
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00459807
Iteration 2/25 | Loss: 0.00131247
Iteration 3/25 | Loss: 0.00119843
Iteration 4/25 | Loss: 0.00118726
Iteration 5/25 | Loss: 0.00118408
Iteration 6/25 | Loss: 0.00118331
Iteration 7/25 | Loss: 0.00118331
Iteration 8/25 | Loss: 0.00118331
Iteration 9/25 | Loss: 0.00118331
Iteration 10/25 | Loss: 0.00118331
Iteration 11/25 | Loss: 0.00118331
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011833085445687175, 0.0011833085445687175, 0.0011833085445687175, 0.0011833085445687175, 0.0011833085445687175]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011833085445687175

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.47969079
Iteration 2/25 | Loss: 0.00075600
Iteration 3/25 | Loss: 0.00075599
Iteration 4/25 | Loss: 0.00075599
Iteration 5/25 | Loss: 0.00075599
Iteration 6/25 | Loss: 0.00075599
Iteration 7/25 | Loss: 0.00075599
Iteration 8/25 | Loss: 0.00075599
Iteration 9/25 | Loss: 0.00075599
Iteration 10/25 | Loss: 0.00075599
Iteration 11/25 | Loss: 0.00075599
Iteration 12/25 | Loss: 0.00075599
Iteration 13/25 | Loss: 0.00075599
Iteration 14/25 | Loss: 0.00075599
Iteration 15/25 | Loss: 0.00075599
Iteration 16/25 | Loss: 0.00075599
Iteration 17/25 | Loss: 0.00075599
Iteration 18/25 | Loss: 0.00075599
Iteration 19/25 | Loss: 0.00075599
Iteration 20/25 | Loss: 0.00075599
Iteration 21/25 | Loss: 0.00075599
Iteration 22/25 | Loss: 0.00075599
Iteration 23/25 | Loss: 0.00075599
Iteration 24/25 | Loss: 0.00075599
Iteration 25/25 | Loss: 0.00075599

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00075599
Iteration 2/1000 | Loss: 0.00003009
Iteration 3/1000 | Loss: 0.00002157
Iteration 4/1000 | Loss: 0.00002009
Iteration 5/1000 | Loss: 0.00001936
Iteration 6/1000 | Loss: 0.00001870
Iteration 7/1000 | Loss: 0.00001816
Iteration 8/1000 | Loss: 0.00001778
Iteration 9/1000 | Loss: 0.00001751
Iteration 10/1000 | Loss: 0.00001728
Iteration 11/1000 | Loss: 0.00001713
Iteration 12/1000 | Loss: 0.00001712
Iteration 13/1000 | Loss: 0.00001699
Iteration 14/1000 | Loss: 0.00001696
Iteration 15/1000 | Loss: 0.00001687
Iteration 16/1000 | Loss: 0.00001682
Iteration 17/1000 | Loss: 0.00001682
Iteration 18/1000 | Loss: 0.00001680
Iteration 19/1000 | Loss: 0.00001676
Iteration 20/1000 | Loss: 0.00001676
Iteration 21/1000 | Loss: 0.00001676
Iteration 22/1000 | Loss: 0.00001676
Iteration 23/1000 | Loss: 0.00001675
Iteration 24/1000 | Loss: 0.00001673
Iteration 25/1000 | Loss: 0.00001672
Iteration 26/1000 | Loss: 0.00001672
Iteration 27/1000 | Loss: 0.00001671
Iteration 28/1000 | Loss: 0.00001671
Iteration 29/1000 | Loss: 0.00001670
Iteration 30/1000 | Loss: 0.00001668
Iteration 31/1000 | Loss: 0.00001668
Iteration 32/1000 | Loss: 0.00001667
Iteration 33/1000 | Loss: 0.00001667
Iteration 34/1000 | Loss: 0.00001667
Iteration 35/1000 | Loss: 0.00001667
Iteration 36/1000 | Loss: 0.00001667
Iteration 37/1000 | Loss: 0.00001666
Iteration 38/1000 | Loss: 0.00001666
Iteration 39/1000 | Loss: 0.00001666
Iteration 40/1000 | Loss: 0.00001666
Iteration 41/1000 | Loss: 0.00001666
Iteration 42/1000 | Loss: 0.00001665
Iteration 43/1000 | Loss: 0.00001665
Iteration 44/1000 | Loss: 0.00001662
Iteration 45/1000 | Loss: 0.00001661
Iteration 46/1000 | Loss: 0.00001661
Iteration 47/1000 | Loss: 0.00001661
Iteration 48/1000 | Loss: 0.00001659
Iteration 49/1000 | Loss: 0.00001659
Iteration 50/1000 | Loss: 0.00001658
Iteration 51/1000 | Loss: 0.00001658
Iteration 52/1000 | Loss: 0.00001658
Iteration 53/1000 | Loss: 0.00001658
Iteration 54/1000 | Loss: 0.00001657
Iteration 55/1000 | Loss: 0.00001657
Iteration 56/1000 | Loss: 0.00001656
Iteration 57/1000 | Loss: 0.00001656
Iteration 58/1000 | Loss: 0.00001655
Iteration 59/1000 | Loss: 0.00001655
Iteration 60/1000 | Loss: 0.00001655
Iteration 61/1000 | Loss: 0.00001655
Iteration 62/1000 | Loss: 0.00001654
Iteration 63/1000 | Loss: 0.00001653
Iteration 64/1000 | Loss: 0.00001653
Iteration 65/1000 | Loss: 0.00001653
Iteration 66/1000 | Loss: 0.00001652
Iteration 67/1000 | Loss: 0.00001652
Iteration 68/1000 | Loss: 0.00001651
Iteration 69/1000 | Loss: 0.00001651
Iteration 70/1000 | Loss: 0.00001650
Iteration 71/1000 | Loss: 0.00001650
Iteration 72/1000 | Loss: 0.00001650
Iteration 73/1000 | Loss: 0.00001650
Iteration 74/1000 | Loss: 0.00001650
Iteration 75/1000 | Loss: 0.00001650
Iteration 76/1000 | Loss: 0.00001650
Iteration 77/1000 | Loss: 0.00001649
Iteration 78/1000 | Loss: 0.00001649
Iteration 79/1000 | Loss: 0.00001648
Iteration 80/1000 | Loss: 0.00001648
Iteration 81/1000 | Loss: 0.00001647
Iteration 82/1000 | Loss: 0.00001647
Iteration 83/1000 | Loss: 0.00001646
Iteration 84/1000 | Loss: 0.00001646
Iteration 85/1000 | Loss: 0.00001646
Iteration 86/1000 | Loss: 0.00001646
Iteration 87/1000 | Loss: 0.00001645
Iteration 88/1000 | Loss: 0.00001645
Iteration 89/1000 | Loss: 0.00001644
Iteration 90/1000 | Loss: 0.00001644
Iteration 91/1000 | Loss: 0.00001644
Iteration 92/1000 | Loss: 0.00001643
Iteration 93/1000 | Loss: 0.00001643
Iteration 94/1000 | Loss: 0.00001643
Iteration 95/1000 | Loss: 0.00001643
Iteration 96/1000 | Loss: 0.00001643
Iteration 97/1000 | Loss: 0.00001643
Iteration 98/1000 | Loss: 0.00001643
Iteration 99/1000 | Loss: 0.00001643
Iteration 100/1000 | Loss: 0.00001642
Iteration 101/1000 | Loss: 0.00001642
Iteration 102/1000 | Loss: 0.00001642
Iteration 103/1000 | Loss: 0.00001642
Iteration 104/1000 | Loss: 0.00001641
Iteration 105/1000 | Loss: 0.00001641
Iteration 106/1000 | Loss: 0.00001641
Iteration 107/1000 | Loss: 0.00001641
Iteration 108/1000 | Loss: 0.00001641
Iteration 109/1000 | Loss: 0.00001640
Iteration 110/1000 | Loss: 0.00001640
Iteration 111/1000 | Loss: 0.00001640
Iteration 112/1000 | Loss: 0.00001640
Iteration 113/1000 | Loss: 0.00001640
Iteration 114/1000 | Loss: 0.00001640
Iteration 115/1000 | Loss: 0.00001640
Iteration 116/1000 | Loss: 0.00001640
Iteration 117/1000 | Loss: 0.00001640
Iteration 118/1000 | Loss: 0.00001640
Iteration 119/1000 | Loss: 0.00001639
Iteration 120/1000 | Loss: 0.00001639
Iteration 121/1000 | Loss: 0.00001639
Iteration 122/1000 | Loss: 0.00001639
Iteration 123/1000 | Loss: 0.00001639
Iteration 124/1000 | Loss: 0.00001638
Iteration 125/1000 | Loss: 0.00001638
Iteration 126/1000 | Loss: 0.00001638
Iteration 127/1000 | Loss: 0.00001638
Iteration 128/1000 | Loss: 0.00001638
Iteration 129/1000 | Loss: 0.00001638
Iteration 130/1000 | Loss: 0.00001638
Iteration 131/1000 | Loss: 0.00001638
Iteration 132/1000 | Loss: 0.00001638
Iteration 133/1000 | Loss: 0.00001638
Iteration 134/1000 | Loss: 0.00001638
Iteration 135/1000 | Loss: 0.00001638
Iteration 136/1000 | Loss: 0.00001638
Iteration 137/1000 | Loss: 0.00001637
Iteration 138/1000 | Loss: 0.00001637
Iteration 139/1000 | Loss: 0.00001637
Iteration 140/1000 | Loss: 0.00001637
Iteration 141/1000 | Loss: 0.00001637
Iteration 142/1000 | Loss: 0.00001637
Iteration 143/1000 | Loss: 0.00001637
Iteration 144/1000 | Loss: 0.00001637
Iteration 145/1000 | Loss: 0.00001637
Iteration 146/1000 | Loss: 0.00001636
Iteration 147/1000 | Loss: 0.00001636
Iteration 148/1000 | Loss: 0.00001636
Iteration 149/1000 | Loss: 0.00001636
Iteration 150/1000 | Loss: 0.00001636
Iteration 151/1000 | Loss: 0.00001636
Iteration 152/1000 | Loss: 0.00001636
Iteration 153/1000 | Loss: 0.00001636
Iteration 154/1000 | Loss: 0.00001636
Iteration 155/1000 | Loss: 0.00001636
Iteration 156/1000 | Loss: 0.00001636
Iteration 157/1000 | Loss: 0.00001636
Iteration 158/1000 | Loss: 0.00001636
Iteration 159/1000 | Loss: 0.00001635
Iteration 160/1000 | Loss: 0.00001635
Iteration 161/1000 | Loss: 0.00001635
Iteration 162/1000 | Loss: 0.00001635
Iteration 163/1000 | Loss: 0.00001635
Iteration 164/1000 | Loss: 0.00001635
Iteration 165/1000 | Loss: 0.00001635
Iteration 166/1000 | Loss: 0.00001635
Iteration 167/1000 | Loss: 0.00001635
Iteration 168/1000 | Loss: 0.00001634
Iteration 169/1000 | Loss: 0.00001634
Iteration 170/1000 | Loss: 0.00001634
Iteration 171/1000 | Loss: 0.00001634
Iteration 172/1000 | Loss: 0.00001634
Iteration 173/1000 | Loss: 0.00001634
Iteration 174/1000 | Loss: 0.00001634
Iteration 175/1000 | Loss: 0.00001634
Iteration 176/1000 | Loss: 0.00001634
Iteration 177/1000 | Loss: 0.00001634
Iteration 178/1000 | Loss: 0.00001634
Iteration 179/1000 | Loss: 0.00001634
Iteration 180/1000 | Loss: 0.00001634
Iteration 181/1000 | Loss: 0.00001634
Iteration 182/1000 | Loss: 0.00001634
Iteration 183/1000 | Loss: 0.00001634
Iteration 184/1000 | Loss: 0.00001633
Iteration 185/1000 | Loss: 0.00001633
Iteration 186/1000 | Loss: 0.00001633
Iteration 187/1000 | Loss: 0.00001633
Iteration 188/1000 | Loss: 0.00001633
Iteration 189/1000 | Loss: 0.00001633
Iteration 190/1000 | Loss: 0.00001633
Iteration 191/1000 | Loss: 0.00001633
Iteration 192/1000 | Loss: 0.00001633
Iteration 193/1000 | Loss: 0.00001633
Iteration 194/1000 | Loss: 0.00001633
Iteration 195/1000 | Loss: 0.00001633
Iteration 196/1000 | Loss: 0.00001633
Iteration 197/1000 | Loss: 0.00001632
Iteration 198/1000 | Loss: 0.00001632
Iteration 199/1000 | Loss: 0.00001632
Iteration 200/1000 | Loss: 0.00001632
Iteration 201/1000 | Loss: 0.00001632
Iteration 202/1000 | Loss: 0.00001632
Iteration 203/1000 | Loss: 0.00001632
Iteration 204/1000 | Loss: 0.00001632
Iteration 205/1000 | Loss: 0.00001632
Iteration 206/1000 | Loss: 0.00001632
Iteration 207/1000 | Loss: 0.00001632
Iteration 208/1000 | Loss: 0.00001632
Iteration 209/1000 | Loss: 0.00001632
Iteration 210/1000 | Loss: 0.00001632
Iteration 211/1000 | Loss: 0.00001632
Iteration 212/1000 | Loss: 0.00001632
Iteration 213/1000 | Loss: 0.00001632
Iteration 214/1000 | Loss: 0.00001631
Iteration 215/1000 | Loss: 0.00001631
Iteration 216/1000 | Loss: 0.00001631
Iteration 217/1000 | Loss: 0.00001631
Iteration 218/1000 | Loss: 0.00001631
Iteration 219/1000 | Loss: 0.00001631
Iteration 220/1000 | Loss: 0.00001631
Iteration 221/1000 | Loss: 0.00001631
Iteration 222/1000 | Loss: 0.00001631
Iteration 223/1000 | Loss: 0.00001631
Iteration 224/1000 | Loss: 0.00001631
Iteration 225/1000 | Loss: 0.00001631
Iteration 226/1000 | Loss: 0.00001631
Iteration 227/1000 | Loss: 0.00001631
Iteration 228/1000 | Loss: 0.00001631
Iteration 229/1000 | Loss: 0.00001631
Iteration 230/1000 | Loss: 0.00001631
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 230. Stopping optimization.
Last 5 losses: [1.630596125323791e-05, 1.630596125323791e-05, 1.630596125323791e-05, 1.630596125323791e-05, 1.630596125323791e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.630596125323791e-05

Optimization complete. Final v2v error: 3.3274989128112793 mm

Highest mean error: 4.169432640075684 mm for frame 147

Lowest mean error: 2.4690778255462646 mm for frame 0

Saving results

Total time: 44.272298097610474
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_018/1075/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_018/1075.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_018/1075
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00853219
Iteration 2/25 | Loss: 0.00124475
Iteration 3/25 | Loss: 0.00114697
Iteration 4/25 | Loss: 0.00113621
Iteration 5/25 | Loss: 0.00113292
Iteration 6/25 | Loss: 0.00113292
Iteration 7/25 | Loss: 0.00113292
Iteration 8/25 | Loss: 0.00113292
Iteration 9/25 | Loss: 0.00113292
Iteration 10/25 | Loss: 0.00113292
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0011329151457175612, 0.0011329151457175612, 0.0011329151457175612, 0.0011329151457175612, 0.0011329151457175612]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011329151457175612

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.75293446
Iteration 2/25 | Loss: 0.00086583
Iteration 3/25 | Loss: 0.00086582
Iteration 4/25 | Loss: 0.00086582
Iteration 5/25 | Loss: 0.00086582
Iteration 6/25 | Loss: 0.00086582
Iteration 7/25 | Loss: 0.00086582
Iteration 8/25 | Loss: 0.00086582
Iteration 9/25 | Loss: 0.00086582
Iteration 10/25 | Loss: 0.00086582
Iteration 11/25 | Loss: 0.00086582
Iteration 12/25 | Loss: 0.00086582
Iteration 13/25 | Loss: 0.00086582
Iteration 14/25 | Loss: 0.00086582
Iteration 15/25 | Loss: 0.00086582
Iteration 16/25 | Loss: 0.00086582
Iteration 17/25 | Loss: 0.00086582
Iteration 18/25 | Loss: 0.00086582
Iteration 19/25 | Loss: 0.00086582
Iteration 20/25 | Loss: 0.00086582
Iteration 21/25 | Loss: 0.00086582
Iteration 22/25 | Loss: 0.00086582
Iteration 23/25 | Loss: 0.00086582
Iteration 24/25 | Loss: 0.00086582
Iteration 25/25 | Loss: 0.00086582

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00086582
Iteration 2/1000 | Loss: 0.00001841
Iteration 3/1000 | Loss: 0.00001386
Iteration 4/1000 | Loss: 0.00001281
Iteration 5/1000 | Loss: 0.00001209
Iteration 6/1000 | Loss: 0.00001161
Iteration 7/1000 | Loss: 0.00001131
Iteration 8/1000 | Loss: 0.00001093
Iteration 9/1000 | Loss: 0.00001070
Iteration 10/1000 | Loss: 0.00001064
Iteration 11/1000 | Loss: 0.00001059
Iteration 12/1000 | Loss: 0.00001058
Iteration 13/1000 | Loss: 0.00001050
Iteration 14/1000 | Loss: 0.00001050
Iteration 15/1000 | Loss: 0.00001049
Iteration 16/1000 | Loss: 0.00001044
Iteration 17/1000 | Loss: 0.00001034
Iteration 18/1000 | Loss: 0.00001034
Iteration 19/1000 | Loss: 0.00001031
Iteration 20/1000 | Loss: 0.00001029
Iteration 21/1000 | Loss: 0.00001026
Iteration 22/1000 | Loss: 0.00001024
Iteration 23/1000 | Loss: 0.00001023
Iteration 24/1000 | Loss: 0.00001023
Iteration 25/1000 | Loss: 0.00001021
Iteration 26/1000 | Loss: 0.00001020
Iteration 27/1000 | Loss: 0.00001019
Iteration 28/1000 | Loss: 0.00001017
Iteration 29/1000 | Loss: 0.00001017
Iteration 30/1000 | Loss: 0.00001015
Iteration 31/1000 | Loss: 0.00001014
Iteration 32/1000 | Loss: 0.00001014
Iteration 33/1000 | Loss: 0.00001013
Iteration 34/1000 | Loss: 0.00001012
Iteration 35/1000 | Loss: 0.00001011
Iteration 36/1000 | Loss: 0.00001011
Iteration 37/1000 | Loss: 0.00001010
Iteration 38/1000 | Loss: 0.00001010
Iteration 39/1000 | Loss: 0.00001009
Iteration 40/1000 | Loss: 0.00001009
Iteration 41/1000 | Loss: 0.00001009
Iteration 42/1000 | Loss: 0.00001009
Iteration 43/1000 | Loss: 0.00001008
Iteration 44/1000 | Loss: 0.00001008
Iteration 45/1000 | Loss: 0.00001008
Iteration 46/1000 | Loss: 0.00001008
Iteration 47/1000 | Loss: 0.00001007
Iteration 48/1000 | Loss: 0.00001007
Iteration 49/1000 | Loss: 0.00001006
Iteration 50/1000 | Loss: 0.00001006
Iteration 51/1000 | Loss: 0.00001005
Iteration 52/1000 | Loss: 0.00001005
Iteration 53/1000 | Loss: 0.00001005
Iteration 54/1000 | Loss: 0.00001005
Iteration 55/1000 | Loss: 0.00001005
Iteration 56/1000 | Loss: 0.00001005
Iteration 57/1000 | Loss: 0.00001003
Iteration 58/1000 | Loss: 0.00001003
Iteration 59/1000 | Loss: 0.00001003
Iteration 60/1000 | Loss: 0.00001003
Iteration 61/1000 | Loss: 0.00001003
Iteration 62/1000 | Loss: 0.00001002
Iteration 63/1000 | Loss: 0.00001002
Iteration 64/1000 | Loss: 0.00001002
Iteration 65/1000 | Loss: 0.00001002
Iteration 66/1000 | Loss: 0.00001002
Iteration 67/1000 | Loss: 0.00001002
Iteration 68/1000 | Loss: 0.00001002
Iteration 69/1000 | Loss: 0.00001002
Iteration 70/1000 | Loss: 0.00001002
Iteration 71/1000 | Loss: 0.00001002
Iteration 72/1000 | Loss: 0.00001001
Iteration 73/1000 | Loss: 0.00001001
Iteration 74/1000 | Loss: 0.00001000
Iteration 75/1000 | Loss: 0.00001000
Iteration 76/1000 | Loss: 0.00000999
Iteration 77/1000 | Loss: 0.00000999
Iteration 78/1000 | Loss: 0.00000999
Iteration 79/1000 | Loss: 0.00000998
Iteration 80/1000 | Loss: 0.00000998
Iteration 81/1000 | Loss: 0.00000998
Iteration 82/1000 | Loss: 0.00000998
Iteration 83/1000 | Loss: 0.00000998
Iteration 84/1000 | Loss: 0.00000998
Iteration 85/1000 | Loss: 0.00000998
Iteration 86/1000 | Loss: 0.00000997
Iteration 87/1000 | Loss: 0.00000997
Iteration 88/1000 | Loss: 0.00000997
Iteration 89/1000 | Loss: 0.00000997
Iteration 90/1000 | Loss: 0.00000997
Iteration 91/1000 | Loss: 0.00000997
Iteration 92/1000 | Loss: 0.00000997
Iteration 93/1000 | Loss: 0.00000996
Iteration 94/1000 | Loss: 0.00000996
Iteration 95/1000 | Loss: 0.00000996
Iteration 96/1000 | Loss: 0.00000995
Iteration 97/1000 | Loss: 0.00000995
Iteration 98/1000 | Loss: 0.00000995
Iteration 99/1000 | Loss: 0.00000995
Iteration 100/1000 | Loss: 0.00000995
Iteration 101/1000 | Loss: 0.00000995
Iteration 102/1000 | Loss: 0.00000995
Iteration 103/1000 | Loss: 0.00000995
Iteration 104/1000 | Loss: 0.00000994
Iteration 105/1000 | Loss: 0.00000993
Iteration 106/1000 | Loss: 0.00000993
Iteration 107/1000 | Loss: 0.00000993
Iteration 108/1000 | Loss: 0.00000992
Iteration 109/1000 | Loss: 0.00000992
Iteration 110/1000 | Loss: 0.00000992
Iteration 111/1000 | Loss: 0.00000991
Iteration 112/1000 | Loss: 0.00000991
Iteration 113/1000 | Loss: 0.00000991
Iteration 114/1000 | Loss: 0.00000991
Iteration 115/1000 | Loss: 0.00000991
Iteration 116/1000 | Loss: 0.00000991
Iteration 117/1000 | Loss: 0.00000990
Iteration 118/1000 | Loss: 0.00000990
Iteration 119/1000 | Loss: 0.00000990
Iteration 120/1000 | Loss: 0.00000990
Iteration 121/1000 | Loss: 0.00000989
Iteration 122/1000 | Loss: 0.00000989
Iteration 123/1000 | Loss: 0.00000989
Iteration 124/1000 | Loss: 0.00000989
Iteration 125/1000 | Loss: 0.00000989
Iteration 126/1000 | Loss: 0.00000989
Iteration 127/1000 | Loss: 0.00000989
Iteration 128/1000 | Loss: 0.00000989
Iteration 129/1000 | Loss: 0.00000988
Iteration 130/1000 | Loss: 0.00000988
Iteration 131/1000 | Loss: 0.00000988
Iteration 132/1000 | Loss: 0.00000988
Iteration 133/1000 | Loss: 0.00000988
Iteration 134/1000 | Loss: 0.00000988
Iteration 135/1000 | Loss: 0.00000988
Iteration 136/1000 | Loss: 0.00000988
Iteration 137/1000 | Loss: 0.00000988
Iteration 138/1000 | Loss: 0.00000988
Iteration 139/1000 | Loss: 0.00000988
Iteration 140/1000 | Loss: 0.00000988
Iteration 141/1000 | Loss: 0.00000988
Iteration 142/1000 | Loss: 0.00000988
Iteration 143/1000 | Loss: 0.00000988
Iteration 144/1000 | Loss: 0.00000988
Iteration 145/1000 | Loss: 0.00000988
Iteration 146/1000 | Loss: 0.00000988
Iteration 147/1000 | Loss: 0.00000988
Iteration 148/1000 | Loss: 0.00000988
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 148. Stopping optimization.
Last 5 losses: [9.877421689452603e-06, 9.877421689452603e-06, 9.877421689452603e-06, 9.877421689452603e-06, 9.877421689452603e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.877421689452603e-06

Optimization complete. Final v2v error: 2.6978702545166016 mm

Highest mean error: 3.242630958557129 mm for frame 156

Lowest mean error: 2.4206273555755615 mm for frame 236

Saving results

Total time: 39.80764055252075
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_018/1041/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_018/1041.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_018/1041
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00791693
Iteration 2/25 | Loss: 0.00121282
Iteration 3/25 | Loss: 0.00114154
Iteration 4/25 | Loss: 0.00112783
Iteration 5/25 | Loss: 0.00112414
Iteration 6/25 | Loss: 0.00112414
Iteration 7/25 | Loss: 0.00112414
Iteration 8/25 | Loss: 0.00112414
Iteration 9/25 | Loss: 0.00112414
Iteration 10/25 | Loss: 0.00112414
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0011241381289437413, 0.0011241381289437413, 0.0011241381289437413, 0.0011241381289437413, 0.0011241381289437413]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011241381289437413

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39805806
Iteration 2/25 | Loss: 0.00078951
Iteration 3/25 | Loss: 0.00078951
Iteration 4/25 | Loss: 0.00078951
Iteration 5/25 | Loss: 0.00078951
Iteration 6/25 | Loss: 0.00078951
Iteration 7/25 | Loss: 0.00078951
Iteration 8/25 | Loss: 0.00078951
Iteration 9/25 | Loss: 0.00078951
Iteration 10/25 | Loss: 0.00078951
Iteration 11/25 | Loss: 0.00078951
Iteration 12/25 | Loss: 0.00078951
Iteration 13/25 | Loss: 0.00078951
Iteration 14/25 | Loss: 0.00078951
Iteration 15/25 | Loss: 0.00078951
Iteration 16/25 | Loss: 0.00078951
Iteration 17/25 | Loss: 0.00078951
Iteration 18/25 | Loss: 0.00078951
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0007895081653259695, 0.0007895081653259695, 0.0007895081653259695, 0.0007895081653259695, 0.0007895081653259695]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007895081653259695

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00078951
Iteration 2/1000 | Loss: 0.00002382
Iteration 3/1000 | Loss: 0.00001637
Iteration 4/1000 | Loss: 0.00001478
Iteration 5/1000 | Loss: 0.00001390
Iteration 6/1000 | Loss: 0.00001335
Iteration 7/1000 | Loss: 0.00001314
Iteration 8/1000 | Loss: 0.00001275
Iteration 9/1000 | Loss: 0.00001255
Iteration 10/1000 | Loss: 0.00001228
Iteration 11/1000 | Loss: 0.00001205
Iteration 12/1000 | Loss: 0.00001201
Iteration 13/1000 | Loss: 0.00001199
Iteration 14/1000 | Loss: 0.00001191
Iteration 15/1000 | Loss: 0.00001175
Iteration 16/1000 | Loss: 0.00001174
Iteration 17/1000 | Loss: 0.00001173
Iteration 18/1000 | Loss: 0.00001163
Iteration 19/1000 | Loss: 0.00001158
Iteration 20/1000 | Loss: 0.00001155
Iteration 21/1000 | Loss: 0.00001154
Iteration 22/1000 | Loss: 0.00001148
Iteration 23/1000 | Loss: 0.00001148
Iteration 24/1000 | Loss: 0.00001148
Iteration 25/1000 | Loss: 0.00001148
Iteration 26/1000 | Loss: 0.00001148
Iteration 27/1000 | Loss: 0.00001148
Iteration 28/1000 | Loss: 0.00001148
Iteration 29/1000 | Loss: 0.00001148
Iteration 30/1000 | Loss: 0.00001148
Iteration 31/1000 | Loss: 0.00001148
Iteration 32/1000 | Loss: 0.00001148
Iteration 33/1000 | Loss: 0.00001148
Iteration 34/1000 | Loss: 0.00001148
Iteration 35/1000 | Loss: 0.00001148
Iteration 36/1000 | Loss: 0.00001148
Iteration 37/1000 | Loss: 0.00001148
Iteration 38/1000 | Loss: 0.00001148
Iteration 39/1000 | Loss: 0.00001148
Iteration 40/1000 | Loss: 0.00001148
Iteration 41/1000 | Loss: 0.00001148
Iteration 42/1000 | Loss: 0.00001148
Iteration 43/1000 | Loss: 0.00001148
Iteration 44/1000 | Loss: 0.00001148
Iteration 45/1000 | Loss: 0.00001148
Iteration 46/1000 | Loss: 0.00001148
Iteration 47/1000 | Loss: 0.00001147
Iteration 48/1000 | Loss: 0.00001147
Iteration 49/1000 | Loss: 0.00001146
Iteration 50/1000 | Loss: 0.00001145
Iteration 51/1000 | Loss: 0.00001145
Iteration 52/1000 | Loss: 0.00001142
Iteration 53/1000 | Loss: 0.00001141
Iteration 54/1000 | Loss: 0.00001141
Iteration 55/1000 | Loss: 0.00001141
Iteration 56/1000 | Loss: 0.00001140
Iteration 57/1000 | Loss: 0.00001139
Iteration 58/1000 | Loss: 0.00001139
Iteration 59/1000 | Loss: 0.00001137
Iteration 60/1000 | Loss: 0.00001136
Iteration 61/1000 | Loss: 0.00001136
Iteration 62/1000 | Loss: 0.00001133
Iteration 63/1000 | Loss: 0.00001131
Iteration 64/1000 | Loss: 0.00001131
Iteration 65/1000 | Loss: 0.00001129
Iteration 66/1000 | Loss: 0.00001129
Iteration 67/1000 | Loss: 0.00001129
Iteration 68/1000 | Loss: 0.00001129
Iteration 69/1000 | Loss: 0.00001129
Iteration 70/1000 | Loss: 0.00001129
Iteration 71/1000 | Loss: 0.00001129
Iteration 72/1000 | Loss: 0.00001128
Iteration 73/1000 | Loss: 0.00001128
Iteration 74/1000 | Loss: 0.00001128
Iteration 75/1000 | Loss: 0.00001127
Iteration 76/1000 | Loss: 0.00001127
Iteration 77/1000 | Loss: 0.00001127
Iteration 78/1000 | Loss: 0.00001126
Iteration 79/1000 | Loss: 0.00001126
Iteration 80/1000 | Loss: 0.00001126
Iteration 81/1000 | Loss: 0.00001126
Iteration 82/1000 | Loss: 0.00001126
Iteration 83/1000 | Loss: 0.00001125
Iteration 84/1000 | Loss: 0.00001125
Iteration 85/1000 | Loss: 0.00001125
Iteration 86/1000 | Loss: 0.00001125
Iteration 87/1000 | Loss: 0.00001125
Iteration 88/1000 | Loss: 0.00001125
Iteration 89/1000 | Loss: 0.00001125
Iteration 90/1000 | Loss: 0.00001125
Iteration 91/1000 | Loss: 0.00001125
Iteration 92/1000 | Loss: 0.00001125
Iteration 93/1000 | Loss: 0.00001124
Iteration 94/1000 | Loss: 0.00001124
Iteration 95/1000 | Loss: 0.00001124
Iteration 96/1000 | Loss: 0.00001123
Iteration 97/1000 | Loss: 0.00001123
Iteration 98/1000 | Loss: 0.00001123
Iteration 99/1000 | Loss: 0.00001123
Iteration 100/1000 | Loss: 0.00001123
Iteration 101/1000 | Loss: 0.00001123
Iteration 102/1000 | Loss: 0.00001123
Iteration 103/1000 | Loss: 0.00001123
Iteration 104/1000 | Loss: 0.00001123
Iteration 105/1000 | Loss: 0.00001123
Iteration 106/1000 | Loss: 0.00001123
Iteration 107/1000 | Loss: 0.00001122
Iteration 108/1000 | Loss: 0.00001122
Iteration 109/1000 | Loss: 0.00001122
Iteration 110/1000 | Loss: 0.00001122
Iteration 111/1000 | Loss: 0.00001122
Iteration 112/1000 | Loss: 0.00001122
Iteration 113/1000 | Loss: 0.00001122
Iteration 114/1000 | Loss: 0.00001122
Iteration 115/1000 | Loss: 0.00001121
Iteration 116/1000 | Loss: 0.00001121
Iteration 117/1000 | Loss: 0.00001121
Iteration 118/1000 | Loss: 0.00001121
Iteration 119/1000 | Loss: 0.00001120
Iteration 120/1000 | Loss: 0.00001120
Iteration 121/1000 | Loss: 0.00001120
Iteration 122/1000 | Loss: 0.00001120
Iteration 123/1000 | Loss: 0.00001120
Iteration 124/1000 | Loss: 0.00001120
Iteration 125/1000 | Loss: 0.00001120
Iteration 126/1000 | Loss: 0.00001119
Iteration 127/1000 | Loss: 0.00001119
Iteration 128/1000 | Loss: 0.00001119
Iteration 129/1000 | Loss: 0.00001119
Iteration 130/1000 | Loss: 0.00001119
Iteration 131/1000 | Loss: 0.00001118
Iteration 132/1000 | Loss: 0.00001116
Iteration 133/1000 | Loss: 0.00001116
Iteration 134/1000 | Loss: 0.00001116
Iteration 135/1000 | Loss: 0.00001116
Iteration 136/1000 | Loss: 0.00001115
Iteration 137/1000 | Loss: 0.00001115
Iteration 138/1000 | Loss: 0.00001115
Iteration 139/1000 | Loss: 0.00001115
Iteration 140/1000 | Loss: 0.00001115
Iteration 141/1000 | Loss: 0.00001115
Iteration 142/1000 | Loss: 0.00001115
Iteration 143/1000 | Loss: 0.00001115
Iteration 144/1000 | Loss: 0.00001114
Iteration 145/1000 | Loss: 0.00001114
Iteration 146/1000 | Loss: 0.00001114
Iteration 147/1000 | Loss: 0.00001114
Iteration 148/1000 | Loss: 0.00001114
Iteration 149/1000 | Loss: 0.00001114
Iteration 150/1000 | Loss: 0.00001114
Iteration 151/1000 | Loss: 0.00001114
Iteration 152/1000 | Loss: 0.00001114
Iteration 153/1000 | Loss: 0.00001113
Iteration 154/1000 | Loss: 0.00001113
Iteration 155/1000 | Loss: 0.00001113
Iteration 156/1000 | Loss: 0.00001113
Iteration 157/1000 | Loss: 0.00001113
Iteration 158/1000 | Loss: 0.00001113
Iteration 159/1000 | Loss: 0.00001113
Iteration 160/1000 | Loss: 0.00001113
Iteration 161/1000 | Loss: 0.00001112
Iteration 162/1000 | Loss: 0.00001112
Iteration 163/1000 | Loss: 0.00001112
Iteration 164/1000 | Loss: 0.00001112
Iteration 165/1000 | Loss: 0.00001112
Iteration 166/1000 | Loss: 0.00001112
Iteration 167/1000 | Loss: 0.00001112
Iteration 168/1000 | Loss: 0.00001112
Iteration 169/1000 | Loss: 0.00001112
Iteration 170/1000 | Loss: 0.00001112
Iteration 171/1000 | Loss: 0.00001112
Iteration 172/1000 | Loss: 0.00001111
Iteration 173/1000 | Loss: 0.00001111
Iteration 174/1000 | Loss: 0.00001111
Iteration 175/1000 | Loss: 0.00001111
Iteration 176/1000 | Loss: 0.00001111
Iteration 177/1000 | Loss: 0.00001111
Iteration 178/1000 | Loss: 0.00001111
Iteration 179/1000 | Loss: 0.00001111
Iteration 180/1000 | Loss: 0.00001111
Iteration 181/1000 | Loss: 0.00001111
Iteration 182/1000 | Loss: 0.00001111
Iteration 183/1000 | Loss: 0.00001111
Iteration 184/1000 | Loss: 0.00001111
Iteration 185/1000 | Loss: 0.00001111
Iteration 186/1000 | Loss: 0.00001111
Iteration 187/1000 | Loss: 0.00001111
Iteration 188/1000 | Loss: 0.00001111
Iteration 189/1000 | Loss: 0.00001111
Iteration 190/1000 | Loss: 0.00001111
Iteration 191/1000 | Loss: 0.00001111
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 191. Stopping optimization.
Last 5 losses: [1.1111003004771192e-05, 1.1111003004771192e-05, 1.1111003004771192e-05, 1.1111003004771192e-05, 1.1111003004771192e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1111003004771192e-05

Optimization complete. Final v2v error: 2.8601226806640625 mm

Highest mean error: 3.1114487648010254 mm for frame 68

Lowest mean error: 2.744662284851074 mm for frame 142

Saving results

Total time: 40.02061867713928
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_018/1074/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_018/1074.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_018/1074
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00950333
Iteration 2/25 | Loss: 0.00259297
Iteration 3/25 | Loss: 0.00201453
Iteration 4/25 | Loss: 0.00210187
Iteration 5/25 | Loss: 0.00190470
Iteration 6/25 | Loss: 0.00162163
Iteration 7/25 | Loss: 0.00150196
Iteration 8/25 | Loss: 0.00145776
Iteration 9/25 | Loss: 0.00143703
Iteration 10/25 | Loss: 0.00142436
Iteration 11/25 | Loss: 0.00141293
Iteration 12/25 | Loss: 0.00141406
Iteration 13/25 | Loss: 0.00140088
Iteration 14/25 | Loss: 0.00139123
Iteration 15/25 | Loss: 0.00138920
Iteration 16/25 | Loss: 0.00138844
Iteration 17/25 | Loss: 0.00138826
Iteration 18/25 | Loss: 0.00138814
Iteration 19/25 | Loss: 0.00138805
Iteration 20/25 | Loss: 0.00138805
Iteration 21/25 | Loss: 0.00138804
Iteration 22/25 | Loss: 0.00138804
Iteration 23/25 | Loss: 0.00138804
Iteration 24/25 | Loss: 0.00138804
Iteration 25/25 | Loss: 0.00138804

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.29403126
Iteration 2/25 | Loss: 0.00231365
Iteration 3/25 | Loss: 0.00231365
Iteration 4/25 | Loss: 0.00231365
Iteration 5/25 | Loss: 0.00231365
Iteration 6/25 | Loss: 0.00231365
Iteration 7/25 | Loss: 0.00231365
Iteration 8/25 | Loss: 0.00231365
Iteration 9/25 | Loss: 0.00231365
Iteration 10/25 | Loss: 0.00231365
Iteration 11/25 | Loss: 0.00231365
Iteration 12/25 | Loss: 0.00231365
Iteration 13/25 | Loss: 0.00231365
Iteration 14/25 | Loss: 0.00231365
Iteration 15/25 | Loss: 0.00231365
Iteration 16/25 | Loss: 0.00231365
Iteration 17/25 | Loss: 0.00231365
Iteration 18/25 | Loss: 0.00231365
Iteration 19/25 | Loss: 0.00231365
Iteration 20/25 | Loss: 0.00231365
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0023136469535529613, 0.0023136469535529613, 0.0023136469535529613, 0.0023136469535529613, 0.0023136469535529613]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0023136469535529613

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00231365
Iteration 2/1000 | Loss: 0.00024214
Iteration 3/1000 | Loss: 0.00025924
Iteration 4/1000 | Loss: 0.00034673
Iteration 5/1000 | Loss: 0.00014018
Iteration 6/1000 | Loss: 0.00012103
Iteration 7/1000 | Loss: 0.00011170
Iteration 8/1000 | Loss: 0.00010394
Iteration 9/1000 | Loss: 0.00009931
Iteration 10/1000 | Loss: 0.00009648
Iteration 11/1000 | Loss: 0.00009338
Iteration 12/1000 | Loss: 0.00009183
Iteration 13/1000 | Loss: 0.00009057
Iteration 14/1000 | Loss: 0.00008948
Iteration 15/1000 | Loss: 0.00008856
Iteration 16/1000 | Loss: 0.00009964
Iteration 17/1000 | Loss: 0.00009112
Iteration 18/1000 | Loss: 0.00008950
Iteration 19/1000 | Loss: 0.00008877
Iteration 20/1000 | Loss: 0.00008734
Iteration 21/1000 | Loss: 0.00008626
Iteration 22/1000 | Loss: 0.00008542
Iteration 23/1000 | Loss: 0.00008489
Iteration 24/1000 | Loss: 0.00008443
Iteration 25/1000 | Loss: 0.00008409
Iteration 26/1000 | Loss: 0.00008375
Iteration 27/1000 | Loss: 0.00008349
Iteration 28/1000 | Loss: 0.00008326
Iteration 29/1000 | Loss: 0.00008301
Iteration 30/1000 | Loss: 0.00008298
Iteration 31/1000 | Loss: 0.00008292
Iteration 32/1000 | Loss: 0.00008275
Iteration 33/1000 | Loss: 0.00008254
Iteration 34/1000 | Loss: 0.00008240
Iteration 35/1000 | Loss: 0.00008224
Iteration 36/1000 | Loss: 0.00008224
Iteration 37/1000 | Loss: 0.00008207
Iteration 38/1000 | Loss: 0.00008199
Iteration 39/1000 | Loss: 0.00008197
Iteration 40/1000 | Loss: 0.00008195
Iteration 41/1000 | Loss: 0.00008194
Iteration 42/1000 | Loss: 0.00008193
Iteration 43/1000 | Loss: 0.00008192
Iteration 44/1000 | Loss: 0.00008191
Iteration 45/1000 | Loss: 0.00008189
Iteration 46/1000 | Loss: 0.00008188
Iteration 47/1000 | Loss: 0.00008188
Iteration 48/1000 | Loss: 0.00008187
Iteration 49/1000 | Loss: 0.00008187
Iteration 50/1000 | Loss: 0.00008187
Iteration 51/1000 | Loss: 0.00008187
Iteration 52/1000 | Loss: 0.00008187
Iteration 53/1000 | Loss: 0.00008187
Iteration 54/1000 | Loss: 0.00008187
Iteration 55/1000 | Loss: 0.00008187
Iteration 56/1000 | Loss: 0.00008187
Iteration 57/1000 | Loss: 0.00008187
Iteration 58/1000 | Loss: 0.00008186
Iteration 59/1000 | Loss: 0.00008186
Iteration 60/1000 | Loss: 0.00008186
Iteration 61/1000 | Loss: 0.00008186
Iteration 62/1000 | Loss: 0.00008185
Iteration 63/1000 | Loss: 0.00008185
Iteration 64/1000 | Loss: 0.00008185
Iteration 65/1000 | Loss: 0.00008185
Iteration 66/1000 | Loss: 0.00008185
Iteration 67/1000 | Loss: 0.00008185
Iteration 68/1000 | Loss: 0.00008184
Iteration 69/1000 | Loss: 0.00008184
Iteration 70/1000 | Loss: 0.00008183
Iteration 71/1000 | Loss: 0.00008182
Iteration 72/1000 | Loss: 0.00008182
Iteration 73/1000 | Loss: 0.00008182
Iteration 74/1000 | Loss: 0.00008182
Iteration 75/1000 | Loss: 0.00008182
Iteration 76/1000 | Loss: 0.00008182
Iteration 77/1000 | Loss: 0.00008181
Iteration 78/1000 | Loss: 0.00008181
Iteration 79/1000 | Loss: 0.00008181
Iteration 80/1000 | Loss: 0.00008180
Iteration 81/1000 | Loss: 0.00008180
Iteration 82/1000 | Loss: 0.00008180
Iteration 83/1000 | Loss: 0.00008179
Iteration 84/1000 | Loss: 0.00008179
Iteration 85/1000 | Loss: 0.00008179
Iteration 86/1000 | Loss: 0.00008178
Iteration 87/1000 | Loss: 0.00008178
Iteration 88/1000 | Loss: 0.00008177
Iteration 89/1000 | Loss: 0.00008177
Iteration 90/1000 | Loss: 0.00008177
Iteration 91/1000 | Loss: 0.00008177
Iteration 92/1000 | Loss: 0.00008177
Iteration 93/1000 | Loss: 0.00008177
Iteration 94/1000 | Loss: 0.00008177
Iteration 95/1000 | Loss: 0.00008177
Iteration 96/1000 | Loss: 0.00008177
Iteration 97/1000 | Loss: 0.00008177
Iteration 98/1000 | Loss: 0.00008176
Iteration 99/1000 | Loss: 0.00008176
Iteration 100/1000 | Loss: 0.00008176
Iteration 101/1000 | Loss: 0.00008176
Iteration 102/1000 | Loss: 0.00008176
Iteration 103/1000 | Loss: 0.00008176
Iteration 104/1000 | Loss: 0.00008176
Iteration 105/1000 | Loss: 0.00008176
Iteration 106/1000 | Loss: 0.00008176
Iteration 107/1000 | Loss: 0.00008176
Iteration 108/1000 | Loss: 0.00008176
Iteration 109/1000 | Loss: 0.00008176
Iteration 110/1000 | Loss: 0.00008176
Iteration 111/1000 | Loss: 0.00008175
Iteration 112/1000 | Loss: 0.00008175
Iteration 113/1000 | Loss: 0.00008175
Iteration 114/1000 | Loss: 0.00008175
Iteration 115/1000 | Loss: 0.00008175
Iteration 116/1000 | Loss: 0.00008175
Iteration 117/1000 | Loss: 0.00008175
Iteration 118/1000 | Loss: 0.00008175
Iteration 119/1000 | Loss: 0.00008175
Iteration 120/1000 | Loss: 0.00008175
Iteration 121/1000 | Loss: 0.00008175
Iteration 122/1000 | Loss: 0.00008175
Iteration 123/1000 | Loss: 0.00008175
Iteration 124/1000 | Loss: 0.00008175
Iteration 125/1000 | Loss: 0.00008175
Iteration 126/1000 | Loss: 0.00008175
Iteration 127/1000 | Loss: 0.00008174
Iteration 128/1000 | Loss: 0.00008174
Iteration 129/1000 | Loss: 0.00008174
Iteration 130/1000 | Loss: 0.00008174
Iteration 131/1000 | Loss: 0.00008174
Iteration 132/1000 | Loss: 0.00008174
Iteration 133/1000 | Loss: 0.00008174
Iteration 134/1000 | Loss: 0.00008174
Iteration 135/1000 | Loss: 0.00008174
Iteration 136/1000 | Loss: 0.00008174
Iteration 137/1000 | Loss: 0.00008174
Iteration 138/1000 | Loss: 0.00008174
Iteration 139/1000 | Loss: 0.00008174
Iteration 140/1000 | Loss: 0.00008174
Iteration 141/1000 | Loss: 0.00008174
Iteration 142/1000 | Loss: 0.00008174
Iteration 143/1000 | Loss: 0.00008174
Iteration 144/1000 | Loss: 0.00008173
Iteration 145/1000 | Loss: 0.00008173
Iteration 146/1000 | Loss: 0.00008173
Iteration 147/1000 | Loss: 0.00008173
Iteration 148/1000 | Loss: 0.00008173
Iteration 149/1000 | Loss: 0.00008173
Iteration 150/1000 | Loss: 0.00008173
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 150. Stopping optimization.
Last 5 losses: [8.173473906936124e-05, 8.173473906936124e-05, 8.173473906936124e-05, 8.173473906936124e-05, 8.173473906936124e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.173473906936124e-05

Optimization complete. Final v2v error: 5.134345054626465 mm

Highest mean error: 11.492335319519043 mm for frame 143

Lowest mean error: 3.633720636367798 mm for frame 152

Saving results

Total time: 91.4516110420227
