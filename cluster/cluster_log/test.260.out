Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=260, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 14560-14615
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_018/1094/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_018/1094.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_018/1094
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00884936
Iteration 2/25 | Loss: 0.00135351
Iteration 3/25 | Loss: 0.00123292
Iteration 4/25 | Loss: 0.00122224
Iteration 5/25 | Loss: 0.00121980
Iteration 6/25 | Loss: 0.00121980
Iteration 7/25 | Loss: 0.00121980
Iteration 8/25 | Loss: 0.00121980
Iteration 9/25 | Loss: 0.00121980
Iteration 10/25 | Loss: 0.00121980
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0012197974137961864, 0.0012197974137961864, 0.0012197974137961864, 0.0012197974137961864, 0.0012197974137961864]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012197974137961864

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.29456496
Iteration 2/25 | Loss: 0.00080174
Iteration 3/25 | Loss: 0.00080166
Iteration 4/25 | Loss: 0.00080166
Iteration 5/25 | Loss: 0.00080165
Iteration 6/25 | Loss: 0.00080165
Iteration 7/25 | Loss: 0.00080165
Iteration 8/25 | Loss: 0.00080165
Iteration 9/25 | Loss: 0.00080165
Iteration 10/25 | Loss: 0.00080165
Iteration 11/25 | Loss: 0.00080165
Iteration 12/25 | Loss: 0.00080165
Iteration 13/25 | Loss: 0.00080165
Iteration 14/25 | Loss: 0.00080165
Iteration 15/25 | Loss: 0.00080165
Iteration 16/25 | Loss: 0.00080165
Iteration 17/25 | Loss: 0.00080165
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.000801653484813869, 0.000801653484813869, 0.000801653484813869, 0.000801653484813869, 0.000801653484813869]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000801653484813869

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00080165
Iteration 2/1000 | Loss: 0.00005099
Iteration 3/1000 | Loss: 0.00003855
Iteration 4/1000 | Loss: 0.00003597
Iteration 5/1000 | Loss: 0.00003430
Iteration 6/1000 | Loss: 0.00003351
Iteration 7/1000 | Loss: 0.00003299
Iteration 8/1000 | Loss: 0.00003246
Iteration 9/1000 | Loss: 0.00003201
Iteration 10/1000 | Loss: 0.00003148
Iteration 11/1000 | Loss: 0.00003118
Iteration 12/1000 | Loss: 0.00003080
Iteration 13/1000 | Loss: 0.00003051
Iteration 14/1000 | Loss: 0.00003036
Iteration 15/1000 | Loss: 0.00003034
Iteration 16/1000 | Loss: 0.00003033
Iteration 17/1000 | Loss: 0.00003021
Iteration 18/1000 | Loss: 0.00003020
Iteration 19/1000 | Loss: 0.00003013
Iteration 20/1000 | Loss: 0.00003012
Iteration 21/1000 | Loss: 0.00003010
Iteration 22/1000 | Loss: 0.00003009
Iteration 23/1000 | Loss: 0.00003009
Iteration 24/1000 | Loss: 0.00003009
Iteration 25/1000 | Loss: 0.00003009
Iteration 26/1000 | Loss: 0.00003009
Iteration 27/1000 | Loss: 0.00003009
Iteration 28/1000 | Loss: 0.00003009
Iteration 29/1000 | Loss: 0.00003009
Iteration 30/1000 | Loss: 0.00003009
Iteration 31/1000 | Loss: 0.00003008
Iteration 32/1000 | Loss: 0.00003008
Iteration 33/1000 | Loss: 0.00003007
Iteration 34/1000 | Loss: 0.00003006
Iteration 35/1000 | Loss: 0.00003005
Iteration 36/1000 | Loss: 0.00003005
Iteration 37/1000 | Loss: 0.00003004
Iteration 38/1000 | Loss: 0.00003003
Iteration 39/1000 | Loss: 0.00003003
Iteration 40/1000 | Loss: 0.00003003
Iteration 41/1000 | Loss: 0.00003003
Iteration 42/1000 | Loss: 0.00003003
Iteration 43/1000 | Loss: 0.00003001
Iteration 44/1000 | Loss: 0.00003000
Iteration 45/1000 | Loss: 0.00003000
Iteration 46/1000 | Loss: 0.00003000
Iteration 47/1000 | Loss: 0.00003000
Iteration 48/1000 | Loss: 0.00002999
Iteration 49/1000 | Loss: 0.00002999
Iteration 50/1000 | Loss: 0.00002999
Iteration 51/1000 | Loss: 0.00002999
Iteration 52/1000 | Loss: 0.00002999
Iteration 53/1000 | Loss: 0.00002998
Iteration 54/1000 | Loss: 0.00002997
Iteration 55/1000 | Loss: 0.00002997
Iteration 56/1000 | Loss: 0.00002996
Iteration 57/1000 | Loss: 0.00002995
Iteration 58/1000 | Loss: 0.00002995
Iteration 59/1000 | Loss: 0.00002993
Iteration 60/1000 | Loss: 0.00002993
Iteration 61/1000 | Loss: 0.00002993
Iteration 62/1000 | Loss: 0.00002992
Iteration 63/1000 | Loss: 0.00002992
Iteration 64/1000 | Loss: 0.00002991
Iteration 65/1000 | Loss: 0.00002991
Iteration 66/1000 | Loss: 0.00002990
Iteration 67/1000 | Loss: 0.00002990
Iteration 68/1000 | Loss: 0.00002989
Iteration 69/1000 | Loss: 0.00002989
Iteration 70/1000 | Loss: 0.00002989
Iteration 71/1000 | Loss: 0.00002989
Iteration 72/1000 | Loss: 0.00002988
Iteration 73/1000 | Loss: 0.00002988
Iteration 74/1000 | Loss: 0.00002987
Iteration 75/1000 | Loss: 0.00002986
Iteration 76/1000 | Loss: 0.00002986
Iteration 77/1000 | Loss: 0.00002985
Iteration 78/1000 | Loss: 0.00002985
Iteration 79/1000 | Loss: 0.00002985
Iteration 80/1000 | Loss: 0.00002985
Iteration 81/1000 | Loss: 0.00002985
Iteration 82/1000 | Loss: 0.00002984
Iteration 83/1000 | Loss: 0.00002984
Iteration 84/1000 | Loss: 0.00002984
Iteration 85/1000 | Loss: 0.00002983
Iteration 86/1000 | Loss: 0.00002983
Iteration 87/1000 | Loss: 0.00002983
Iteration 88/1000 | Loss: 0.00002982
Iteration 89/1000 | Loss: 0.00002982
Iteration 90/1000 | Loss: 0.00002982
Iteration 91/1000 | Loss: 0.00002982
Iteration 92/1000 | Loss: 0.00002982
Iteration 93/1000 | Loss: 0.00002982
Iteration 94/1000 | Loss: 0.00002982
Iteration 95/1000 | Loss: 0.00002981
Iteration 96/1000 | Loss: 0.00002981
Iteration 97/1000 | Loss: 0.00002981
Iteration 98/1000 | Loss: 0.00002981
Iteration 99/1000 | Loss: 0.00002981
Iteration 100/1000 | Loss: 0.00002981
Iteration 101/1000 | Loss: 0.00002981
Iteration 102/1000 | Loss: 0.00002981
Iteration 103/1000 | Loss: 0.00002980
Iteration 104/1000 | Loss: 0.00002980
Iteration 105/1000 | Loss: 0.00002980
Iteration 106/1000 | Loss: 0.00002980
Iteration 107/1000 | Loss: 0.00002980
Iteration 108/1000 | Loss: 0.00002980
Iteration 109/1000 | Loss: 0.00002980
Iteration 110/1000 | Loss: 0.00002979
Iteration 111/1000 | Loss: 0.00002979
Iteration 112/1000 | Loss: 0.00002979
Iteration 113/1000 | Loss: 0.00002979
Iteration 114/1000 | Loss: 0.00002979
Iteration 115/1000 | Loss: 0.00002978
Iteration 116/1000 | Loss: 0.00002978
Iteration 117/1000 | Loss: 0.00002978
Iteration 118/1000 | Loss: 0.00002978
Iteration 119/1000 | Loss: 0.00002978
Iteration 120/1000 | Loss: 0.00002978
Iteration 121/1000 | Loss: 0.00002978
Iteration 122/1000 | Loss: 0.00002977
Iteration 123/1000 | Loss: 0.00002977
Iteration 124/1000 | Loss: 0.00002977
Iteration 125/1000 | Loss: 0.00002977
Iteration 126/1000 | Loss: 0.00002976
Iteration 127/1000 | Loss: 0.00002976
Iteration 128/1000 | Loss: 0.00002976
Iteration 129/1000 | Loss: 0.00002976
Iteration 130/1000 | Loss: 0.00002976
Iteration 131/1000 | Loss: 0.00002976
Iteration 132/1000 | Loss: 0.00002976
Iteration 133/1000 | Loss: 0.00002976
Iteration 134/1000 | Loss: 0.00002976
Iteration 135/1000 | Loss: 0.00002976
Iteration 136/1000 | Loss: 0.00002976
Iteration 137/1000 | Loss: 0.00002975
Iteration 138/1000 | Loss: 0.00002975
Iteration 139/1000 | Loss: 0.00002975
Iteration 140/1000 | Loss: 0.00002975
Iteration 141/1000 | Loss: 0.00002975
Iteration 142/1000 | Loss: 0.00002975
Iteration 143/1000 | Loss: 0.00002975
Iteration 144/1000 | Loss: 0.00002975
Iteration 145/1000 | Loss: 0.00002975
Iteration 146/1000 | Loss: 0.00002975
Iteration 147/1000 | Loss: 0.00002975
Iteration 148/1000 | Loss: 0.00002975
Iteration 149/1000 | Loss: 0.00002975
Iteration 150/1000 | Loss: 0.00002975
Iteration 151/1000 | Loss: 0.00002975
Iteration 152/1000 | Loss: 0.00002975
Iteration 153/1000 | Loss: 0.00002975
Iteration 154/1000 | Loss: 0.00002975
Iteration 155/1000 | Loss: 0.00002975
Iteration 156/1000 | Loss: 0.00002975
Iteration 157/1000 | Loss: 0.00002975
Iteration 158/1000 | Loss: 0.00002975
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 158. Stopping optimization.
Last 5 losses: [2.9754781280644238e-05, 2.9754781280644238e-05, 2.9754781280644238e-05, 2.9754781280644238e-05, 2.9754781280644238e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.9754781280644238e-05

Optimization complete. Final v2v error: 4.434247016906738 mm

Highest mean error: 4.636663436889648 mm for frame 5

Lowest mean error: 4.283576488494873 mm for frame 75

Saving results

Total time: 47.69822144508362
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_018/1066/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_018/1066.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_018/1066
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00570301
Iteration 2/25 | Loss: 0.00146060
Iteration 3/25 | Loss: 0.00126146
Iteration 4/25 | Loss: 0.00124370
Iteration 5/25 | Loss: 0.00124010
Iteration 6/25 | Loss: 0.00123955
Iteration 7/25 | Loss: 0.00123955
Iteration 8/25 | Loss: 0.00123955
Iteration 9/25 | Loss: 0.00123955
Iteration 10/25 | Loss: 0.00123955
Iteration 11/25 | Loss: 0.00123955
Iteration 12/25 | Loss: 0.00123955
Iteration 13/25 | Loss: 0.00123955
Iteration 14/25 | Loss: 0.00123955
Iteration 15/25 | Loss: 0.00123955
Iteration 16/25 | Loss: 0.00123955
Iteration 17/25 | Loss: 0.00123955
Iteration 18/25 | Loss: 0.00123955
Iteration 19/25 | Loss: 0.00123955
Iteration 20/25 | Loss: 0.00123955
Iteration 21/25 | Loss: 0.00123955
Iteration 22/25 | Loss: 0.00123955
Iteration 23/25 | Loss: 0.00123955
Iteration 24/25 | Loss: 0.00123955
Iteration 25/25 | Loss: 0.00123955

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.59595370
Iteration 2/25 | Loss: 0.00074862
Iteration 3/25 | Loss: 0.00074859
Iteration 4/25 | Loss: 0.00074859
Iteration 5/25 | Loss: 0.00074859
Iteration 6/25 | Loss: 0.00074859
Iteration 7/25 | Loss: 0.00074859
Iteration 8/25 | Loss: 0.00074859
Iteration 9/25 | Loss: 0.00074859
Iteration 10/25 | Loss: 0.00074859
Iteration 11/25 | Loss: 0.00074859
Iteration 12/25 | Loss: 0.00074859
Iteration 13/25 | Loss: 0.00074859
Iteration 14/25 | Loss: 0.00074859
Iteration 15/25 | Loss: 0.00074859
Iteration 16/25 | Loss: 0.00074859
Iteration 17/25 | Loss: 0.00074859
Iteration 18/25 | Loss: 0.00074859
Iteration 19/25 | Loss: 0.00074859
Iteration 20/25 | Loss: 0.00074859
Iteration 21/25 | Loss: 0.00074859
Iteration 22/25 | Loss: 0.00074859
Iteration 23/25 | Loss: 0.00074859
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0007485893438570201, 0.0007485893438570201, 0.0007485893438570201, 0.0007485893438570201, 0.0007485893438570201]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007485893438570201

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00074859
Iteration 2/1000 | Loss: 0.00003506
Iteration 3/1000 | Loss: 0.00002818
Iteration 4/1000 | Loss: 0.00002527
Iteration 5/1000 | Loss: 0.00002360
Iteration 6/1000 | Loss: 0.00002248
Iteration 7/1000 | Loss: 0.00002180
Iteration 8/1000 | Loss: 0.00002137
Iteration 9/1000 | Loss: 0.00002102
Iteration 10/1000 | Loss: 0.00002069
Iteration 11/1000 | Loss: 0.00002045
Iteration 12/1000 | Loss: 0.00002024
Iteration 13/1000 | Loss: 0.00002002
Iteration 14/1000 | Loss: 0.00001985
Iteration 15/1000 | Loss: 0.00001970
Iteration 16/1000 | Loss: 0.00001962
Iteration 17/1000 | Loss: 0.00001962
Iteration 18/1000 | Loss: 0.00001954
Iteration 19/1000 | Loss: 0.00001951
Iteration 20/1000 | Loss: 0.00001947
Iteration 21/1000 | Loss: 0.00001939
Iteration 22/1000 | Loss: 0.00001933
Iteration 23/1000 | Loss: 0.00001929
Iteration 24/1000 | Loss: 0.00001929
Iteration 25/1000 | Loss: 0.00001929
Iteration 26/1000 | Loss: 0.00001926
Iteration 27/1000 | Loss: 0.00001925
Iteration 28/1000 | Loss: 0.00001925
Iteration 29/1000 | Loss: 0.00001924
Iteration 30/1000 | Loss: 0.00001924
Iteration 31/1000 | Loss: 0.00001923
Iteration 32/1000 | Loss: 0.00001923
Iteration 33/1000 | Loss: 0.00001922
Iteration 34/1000 | Loss: 0.00001922
Iteration 35/1000 | Loss: 0.00001922
Iteration 36/1000 | Loss: 0.00001921
Iteration 37/1000 | Loss: 0.00001921
Iteration 38/1000 | Loss: 0.00001921
Iteration 39/1000 | Loss: 0.00001920
Iteration 40/1000 | Loss: 0.00001920
Iteration 41/1000 | Loss: 0.00001920
Iteration 42/1000 | Loss: 0.00001920
Iteration 43/1000 | Loss: 0.00001920
Iteration 44/1000 | Loss: 0.00001920
Iteration 45/1000 | Loss: 0.00001920
Iteration 46/1000 | Loss: 0.00001920
Iteration 47/1000 | Loss: 0.00001920
Iteration 48/1000 | Loss: 0.00001919
Iteration 49/1000 | Loss: 0.00001919
Iteration 50/1000 | Loss: 0.00001918
Iteration 51/1000 | Loss: 0.00001918
Iteration 52/1000 | Loss: 0.00001918
Iteration 53/1000 | Loss: 0.00001918
Iteration 54/1000 | Loss: 0.00001918
Iteration 55/1000 | Loss: 0.00001917
Iteration 56/1000 | Loss: 0.00001917
Iteration 57/1000 | Loss: 0.00001917
Iteration 58/1000 | Loss: 0.00001917
Iteration 59/1000 | Loss: 0.00001916
Iteration 60/1000 | Loss: 0.00001916
Iteration 61/1000 | Loss: 0.00001916
Iteration 62/1000 | Loss: 0.00001916
Iteration 63/1000 | Loss: 0.00001916
Iteration 64/1000 | Loss: 0.00001916
Iteration 65/1000 | Loss: 0.00001915
Iteration 66/1000 | Loss: 0.00001915
Iteration 67/1000 | Loss: 0.00001915
Iteration 68/1000 | Loss: 0.00001915
Iteration 69/1000 | Loss: 0.00001915
Iteration 70/1000 | Loss: 0.00001915
Iteration 71/1000 | Loss: 0.00001914
Iteration 72/1000 | Loss: 0.00001914
Iteration 73/1000 | Loss: 0.00001914
Iteration 74/1000 | Loss: 0.00001914
Iteration 75/1000 | Loss: 0.00001913
Iteration 76/1000 | Loss: 0.00001913
Iteration 77/1000 | Loss: 0.00001913
Iteration 78/1000 | Loss: 0.00001913
Iteration 79/1000 | Loss: 0.00001913
Iteration 80/1000 | Loss: 0.00001913
Iteration 81/1000 | Loss: 0.00001913
Iteration 82/1000 | Loss: 0.00001913
Iteration 83/1000 | Loss: 0.00001913
Iteration 84/1000 | Loss: 0.00001913
Iteration 85/1000 | Loss: 0.00001913
Iteration 86/1000 | Loss: 0.00001912
Iteration 87/1000 | Loss: 0.00001912
Iteration 88/1000 | Loss: 0.00001912
Iteration 89/1000 | Loss: 0.00001912
Iteration 90/1000 | Loss: 0.00001912
Iteration 91/1000 | Loss: 0.00001911
Iteration 92/1000 | Loss: 0.00001911
Iteration 93/1000 | Loss: 0.00001911
Iteration 94/1000 | Loss: 0.00001911
Iteration 95/1000 | Loss: 0.00001910
Iteration 96/1000 | Loss: 0.00001910
Iteration 97/1000 | Loss: 0.00001910
Iteration 98/1000 | Loss: 0.00001910
Iteration 99/1000 | Loss: 0.00001910
Iteration 100/1000 | Loss: 0.00001910
Iteration 101/1000 | Loss: 0.00001910
Iteration 102/1000 | Loss: 0.00001910
Iteration 103/1000 | Loss: 0.00001910
Iteration 104/1000 | Loss: 0.00001910
Iteration 105/1000 | Loss: 0.00001910
Iteration 106/1000 | Loss: 0.00001910
Iteration 107/1000 | Loss: 0.00001910
Iteration 108/1000 | Loss: 0.00001910
Iteration 109/1000 | Loss: 0.00001910
Iteration 110/1000 | Loss: 0.00001910
Iteration 111/1000 | Loss: 0.00001910
Iteration 112/1000 | Loss: 0.00001910
Iteration 113/1000 | Loss: 0.00001910
Iteration 114/1000 | Loss: 0.00001910
Iteration 115/1000 | Loss: 0.00001910
Iteration 116/1000 | Loss: 0.00001910
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 116. Stopping optimization.
Last 5 losses: [1.909540696942713e-05, 1.909540696942713e-05, 1.909540696942713e-05, 1.909540696942713e-05, 1.909540696942713e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.909540696942713e-05

Optimization complete. Final v2v error: 3.6248183250427246 mm

Highest mean error: 4.128346920013428 mm for frame 117

Lowest mean error: 2.9377591609954834 mm for frame 11

Saving results

Total time: 45.6814661026001
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_018/1046/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_018/1046.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_018/1046
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00775000
Iteration 2/25 | Loss: 0.00154314
Iteration 3/25 | Loss: 0.00126340
Iteration 4/25 | Loss: 0.00122483
Iteration 5/25 | Loss: 0.00121518
Iteration 6/25 | Loss: 0.00121263
Iteration 7/25 | Loss: 0.00121063
Iteration 8/25 | Loss: 0.00121274
Iteration 9/25 | Loss: 0.00121155
Iteration 10/25 | Loss: 0.00120980
Iteration 11/25 | Loss: 0.00120877
Iteration 12/25 | Loss: 0.00120858
Iteration 13/25 | Loss: 0.00120846
Iteration 14/25 | Loss: 0.00120846
Iteration 15/25 | Loss: 0.00120846
Iteration 16/25 | Loss: 0.00120845
Iteration 17/25 | Loss: 0.00120845
Iteration 18/25 | Loss: 0.00120845
Iteration 19/25 | Loss: 0.00120845
Iteration 20/25 | Loss: 0.00120845
Iteration 21/25 | Loss: 0.00120845
Iteration 22/25 | Loss: 0.00120845
Iteration 23/25 | Loss: 0.00120845
Iteration 24/25 | Loss: 0.00120845
Iteration 25/25 | Loss: 0.00120845

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.29806602
Iteration 2/25 | Loss: 0.00070817
Iteration 3/25 | Loss: 0.00070817
Iteration 4/25 | Loss: 0.00070817
Iteration 5/25 | Loss: 0.00070817
Iteration 6/25 | Loss: 0.00070817
Iteration 7/25 | Loss: 0.00070817
Iteration 8/25 | Loss: 0.00070817
Iteration 9/25 | Loss: 0.00070817
Iteration 10/25 | Loss: 0.00070817
Iteration 11/25 | Loss: 0.00070817
Iteration 12/25 | Loss: 0.00070817
Iteration 13/25 | Loss: 0.00070817
Iteration 14/25 | Loss: 0.00070817
Iteration 15/25 | Loss: 0.00070817
Iteration 16/25 | Loss: 0.00070817
Iteration 17/25 | Loss: 0.00070817
Iteration 18/25 | Loss: 0.00070817
Iteration 19/25 | Loss: 0.00070817
Iteration 20/25 | Loss: 0.00070817
Iteration 21/25 | Loss: 0.00070817
Iteration 22/25 | Loss: 0.00070817
Iteration 23/25 | Loss: 0.00070817
Iteration 24/25 | Loss: 0.00070817
Iteration 25/25 | Loss: 0.00070817

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00070817
Iteration 2/1000 | Loss: 0.00004549
Iteration 3/1000 | Loss: 0.00002979
Iteration 4/1000 | Loss: 0.00002563
Iteration 5/1000 | Loss: 0.00002402
Iteration 6/1000 | Loss: 0.00002297
Iteration 7/1000 | Loss: 0.00002225
Iteration 8/1000 | Loss: 0.00002182
Iteration 9/1000 | Loss: 0.00002142
Iteration 10/1000 | Loss: 0.00002106
Iteration 11/1000 | Loss: 0.00002082
Iteration 12/1000 | Loss: 0.00002060
Iteration 13/1000 | Loss: 0.00002059
Iteration 14/1000 | Loss: 0.00002054
Iteration 15/1000 | Loss: 0.00002046
Iteration 16/1000 | Loss: 0.00002035
Iteration 17/1000 | Loss: 0.00002031
Iteration 18/1000 | Loss: 0.00002022
Iteration 19/1000 | Loss: 0.00002021
Iteration 20/1000 | Loss: 0.00002021
Iteration 21/1000 | Loss: 0.00002020
Iteration 22/1000 | Loss: 0.00002019
Iteration 23/1000 | Loss: 0.00002019
Iteration 24/1000 | Loss: 0.00002019
Iteration 25/1000 | Loss: 0.00002019
Iteration 26/1000 | Loss: 0.00002018
Iteration 27/1000 | Loss: 0.00002017
Iteration 28/1000 | Loss: 0.00002017
Iteration 29/1000 | Loss: 0.00002017
Iteration 30/1000 | Loss: 0.00002016
Iteration 31/1000 | Loss: 0.00002014
Iteration 32/1000 | Loss: 0.00002014
Iteration 33/1000 | Loss: 0.00002013
Iteration 34/1000 | Loss: 0.00002012
Iteration 35/1000 | Loss: 0.00002011
Iteration 36/1000 | Loss: 0.00002010
Iteration 37/1000 | Loss: 0.00002010
Iteration 38/1000 | Loss: 0.00002010
Iteration 39/1000 | Loss: 0.00002010
Iteration 40/1000 | Loss: 0.00002010
Iteration 41/1000 | Loss: 0.00002009
Iteration 42/1000 | Loss: 0.00002009
Iteration 43/1000 | Loss: 0.00002009
Iteration 44/1000 | Loss: 0.00002009
Iteration 45/1000 | Loss: 0.00002008
Iteration 46/1000 | Loss: 0.00002008
Iteration 47/1000 | Loss: 0.00002008
Iteration 48/1000 | Loss: 0.00002008
Iteration 49/1000 | Loss: 0.00002008
Iteration 50/1000 | Loss: 0.00002008
Iteration 51/1000 | Loss: 0.00002008
Iteration 52/1000 | Loss: 0.00002007
Iteration 53/1000 | Loss: 0.00002007
Iteration 54/1000 | Loss: 0.00002007
Iteration 55/1000 | Loss: 0.00002007
Iteration 56/1000 | Loss: 0.00002007
Iteration 57/1000 | Loss: 0.00002007
Iteration 58/1000 | Loss: 0.00002007
Iteration 59/1000 | Loss: 0.00002007
Iteration 60/1000 | Loss: 0.00002007
Iteration 61/1000 | Loss: 0.00002007
Iteration 62/1000 | Loss: 0.00002007
Iteration 63/1000 | Loss: 0.00002007
Iteration 64/1000 | Loss: 0.00002007
Iteration 65/1000 | Loss: 0.00002007
Iteration 66/1000 | Loss: 0.00002007
Iteration 67/1000 | Loss: 0.00002007
Iteration 68/1000 | Loss: 0.00002007
Iteration 69/1000 | Loss: 0.00002007
Iteration 70/1000 | Loss: 0.00002007
Iteration 71/1000 | Loss: 0.00002007
Iteration 72/1000 | Loss: 0.00002007
Iteration 73/1000 | Loss: 0.00002007
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 73. Stopping optimization.
Last 5 losses: [2.0074741769349203e-05, 2.0074741769349203e-05, 2.0074741769349203e-05, 2.0074741769349203e-05, 2.0074741769349203e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0074741769349203e-05

Optimization complete. Final v2v error: 3.7800133228302 mm

Highest mean error: 4.468770980834961 mm for frame 41

Lowest mean error: 2.9317736625671387 mm for frame 123

Saving results

Total time: 53.650195360183716
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_018/1087/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_018/1087.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_018/1087
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00793952
Iteration 2/25 | Loss: 0.00210868
Iteration 3/25 | Loss: 0.00148275
Iteration 4/25 | Loss: 0.00142336
Iteration 5/25 | Loss: 0.00139882
Iteration 6/25 | Loss: 0.00139270
Iteration 7/25 | Loss: 0.00139135
Iteration 8/25 | Loss: 0.00139111
Iteration 9/25 | Loss: 0.00139111
Iteration 10/25 | Loss: 0.00139111
Iteration 11/25 | Loss: 0.00139111
Iteration 12/25 | Loss: 0.00139111
Iteration 13/25 | Loss: 0.00139111
Iteration 14/25 | Loss: 0.00139111
Iteration 15/25 | Loss: 0.00139111
Iteration 16/25 | Loss: 0.00139111
Iteration 17/25 | Loss: 0.00139111
Iteration 18/25 | Loss: 0.00139111
Iteration 19/25 | Loss: 0.00139111
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0013911142013967037, 0.0013911142013967037, 0.0013911142013967037, 0.0013911142013967037, 0.0013911142013967037]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013911142013967037

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33807957
Iteration 2/25 | Loss: 0.00089596
Iteration 3/25 | Loss: 0.00089595
Iteration 4/25 | Loss: 0.00089595
Iteration 5/25 | Loss: 0.00089595
Iteration 6/25 | Loss: 0.00089595
Iteration 7/25 | Loss: 0.00089595
Iteration 8/25 | Loss: 0.00089595
Iteration 9/25 | Loss: 0.00089595
Iteration 10/25 | Loss: 0.00089595
Iteration 11/25 | Loss: 0.00089595
Iteration 12/25 | Loss: 0.00089595
Iteration 13/25 | Loss: 0.00089595
Iteration 14/25 | Loss: 0.00089595
Iteration 15/25 | Loss: 0.00089595
Iteration 16/25 | Loss: 0.00089595
Iteration 17/25 | Loss: 0.00089595
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0008959469269029796, 0.0008959469269029796, 0.0008959469269029796, 0.0008959469269029796, 0.0008959469269029796]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008959469269029796

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00089595
Iteration 2/1000 | Loss: 0.00006495
Iteration 3/1000 | Loss: 0.00003997
Iteration 4/1000 | Loss: 0.00003444
Iteration 5/1000 | Loss: 0.00003203
Iteration 6/1000 | Loss: 0.00003058
Iteration 7/1000 | Loss: 0.00003000
Iteration 8/1000 | Loss: 0.00002953
Iteration 9/1000 | Loss: 0.00002922
Iteration 10/1000 | Loss: 0.00002906
Iteration 11/1000 | Loss: 0.00002888
Iteration 12/1000 | Loss: 0.00002877
Iteration 13/1000 | Loss: 0.00002874
Iteration 14/1000 | Loss: 0.00002874
Iteration 15/1000 | Loss: 0.00002872
Iteration 16/1000 | Loss: 0.00002871
Iteration 17/1000 | Loss: 0.00002871
Iteration 18/1000 | Loss: 0.00002871
Iteration 19/1000 | Loss: 0.00002870
Iteration 20/1000 | Loss: 0.00002870
Iteration 21/1000 | Loss: 0.00002870
Iteration 22/1000 | Loss: 0.00002870
Iteration 23/1000 | Loss: 0.00002869
Iteration 24/1000 | Loss: 0.00002869
Iteration 25/1000 | Loss: 0.00002868
Iteration 26/1000 | Loss: 0.00002868
Iteration 27/1000 | Loss: 0.00002868
Iteration 28/1000 | Loss: 0.00002867
Iteration 29/1000 | Loss: 0.00002867
Iteration 30/1000 | Loss: 0.00002866
Iteration 31/1000 | Loss: 0.00002865
Iteration 32/1000 | Loss: 0.00002865
Iteration 33/1000 | Loss: 0.00002865
Iteration 34/1000 | Loss: 0.00002864
Iteration 35/1000 | Loss: 0.00002864
Iteration 36/1000 | Loss: 0.00002864
Iteration 37/1000 | Loss: 0.00002862
Iteration 38/1000 | Loss: 0.00002862
Iteration 39/1000 | Loss: 0.00002862
Iteration 40/1000 | Loss: 0.00002861
Iteration 41/1000 | Loss: 0.00002861
Iteration 42/1000 | Loss: 0.00002860
Iteration 43/1000 | Loss: 0.00002859
Iteration 44/1000 | Loss: 0.00002859
Iteration 45/1000 | Loss: 0.00002858
Iteration 46/1000 | Loss: 0.00002858
Iteration 47/1000 | Loss: 0.00002858
Iteration 48/1000 | Loss: 0.00002858
Iteration 49/1000 | Loss: 0.00002858
Iteration 50/1000 | Loss: 0.00002858
Iteration 51/1000 | Loss: 0.00002858
Iteration 52/1000 | Loss: 0.00002858
Iteration 53/1000 | Loss: 0.00002857
Iteration 54/1000 | Loss: 0.00002857
Iteration 55/1000 | Loss: 0.00002857
Iteration 56/1000 | Loss: 0.00002857
Iteration 57/1000 | Loss: 0.00002856
Iteration 58/1000 | Loss: 0.00002856
Iteration 59/1000 | Loss: 0.00002855
Iteration 60/1000 | Loss: 0.00002855
Iteration 61/1000 | Loss: 0.00002854
Iteration 62/1000 | Loss: 0.00002854
Iteration 63/1000 | Loss: 0.00002854
Iteration 64/1000 | Loss: 0.00002853
Iteration 65/1000 | Loss: 0.00002853
Iteration 66/1000 | Loss: 0.00002853
Iteration 67/1000 | Loss: 0.00002853
Iteration 68/1000 | Loss: 0.00002853
Iteration 69/1000 | Loss: 0.00002852
Iteration 70/1000 | Loss: 0.00002852
Iteration 71/1000 | Loss: 0.00002847
Iteration 72/1000 | Loss: 0.00002847
Iteration 73/1000 | Loss: 0.00002847
Iteration 74/1000 | Loss: 0.00002846
Iteration 75/1000 | Loss: 0.00002846
Iteration 76/1000 | Loss: 0.00002845
Iteration 77/1000 | Loss: 0.00002845
Iteration 78/1000 | Loss: 0.00002845
Iteration 79/1000 | Loss: 0.00002845
Iteration 80/1000 | Loss: 0.00002845
Iteration 81/1000 | Loss: 0.00002845
Iteration 82/1000 | Loss: 0.00002845
Iteration 83/1000 | Loss: 0.00002845
Iteration 84/1000 | Loss: 0.00002845
Iteration 85/1000 | Loss: 0.00002845
Iteration 86/1000 | Loss: 0.00002845
Iteration 87/1000 | Loss: 0.00002844
Iteration 88/1000 | Loss: 0.00002844
Iteration 89/1000 | Loss: 0.00002844
Iteration 90/1000 | Loss: 0.00002843
Iteration 91/1000 | Loss: 0.00002843
Iteration 92/1000 | Loss: 0.00002843
Iteration 93/1000 | Loss: 0.00002843
Iteration 94/1000 | Loss: 0.00002843
Iteration 95/1000 | Loss: 0.00002843
Iteration 96/1000 | Loss: 0.00002842
Iteration 97/1000 | Loss: 0.00002842
Iteration 98/1000 | Loss: 0.00002842
Iteration 99/1000 | Loss: 0.00002842
Iteration 100/1000 | Loss: 0.00002841
Iteration 101/1000 | Loss: 0.00002841
Iteration 102/1000 | Loss: 0.00002841
Iteration 103/1000 | Loss: 0.00002841
Iteration 104/1000 | Loss: 0.00002841
Iteration 105/1000 | Loss: 0.00002841
Iteration 106/1000 | Loss: 0.00002841
Iteration 107/1000 | Loss: 0.00002841
Iteration 108/1000 | Loss: 0.00002841
Iteration 109/1000 | Loss: 0.00002841
Iteration 110/1000 | Loss: 0.00002841
Iteration 111/1000 | Loss: 0.00002841
Iteration 112/1000 | Loss: 0.00002840
Iteration 113/1000 | Loss: 0.00002840
Iteration 114/1000 | Loss: 0.00002840
Iteration 115/1000 | Loss: 0.00002840
Iteration 116/1000 | Loss: 0.00002840
Iteration 117/1000 | Loss: 0.00002840
Iteration 118/1000 | Loss: 0.00002840
Iteration 119/1000 | Loss: 0.00002840
Iteration 120/1000 | Loss: 0.00002840
Iteration 121/1000 | Loss: 0.00002840
Iteration 122/1000 | Loss: 0.00002840
Iteration 123/1000 | Loss: 0.00002840
Iteration 124/1000 | Loss: 0.00002840
Iteration 125/1000 | Loss: 0.00002840
Iteration 126/1000 | Loss: 0.00002840
Iteration 127/1000 | Loss: 0.00002840
Iteration 128/1000 | Loss: 0.00002840
Iteration 129/1000 | Loss: 0.00002840
Iteration 130/1000 | Loss: 0.00002840
Iteration 131/1000 | Loss: 0.00002840
Iteration 132/1000 | Loss: 0.00002840
Iteration 133/1000 | Loss: 0.00002840
Iteration 134/1000 | Loss: 0.00002840
Iteration 135/1000 | Loss: 0.00002840
Iteration 136/1000 | Loss: 0.00002840
Iteration 137/1000 | Loss: 0.00002840
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 137. Stopping optimization.
Last 5 losses: [2.8398089852998964e-05, 2.8398089852998964e-05, 2.8398089852998964e-05, 2.8398089852998964e-05, 2.8398089852998964e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.8398089852998964e-05

Optimization complete. Final v2v error: 4.611697673797607 mm

Highest mean error: 4.989003658294678 mm for frame 223

Lowest mean error: 4.076056480407715 mm for frame 43

Saving results

Total time: 45.03786826133728
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_018/1035/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_018/1035.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_018/1035
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00965797
Iteration 2/25 | Loss: 0.00176038
Iteration 3/25 | Loss: 0.00143637
Iteration 4/25 | Loss: 0.00142328
Iteration 5/25 | Loss: 0.00141983
Iteration 6/25 | Loss: 0.00141916
Iteration 7/25 | Loss: 0.00141916
Iteration 8/25 | Loss: 0.00141916
Iteration 9/25 | Loss: 0.00141916
Iteration 10/25 | Loss: 0.00141916
Iteration 11/25 | Loss: 0.00141908
Iteration 12/25 | Loss: 0.00141908
Iteration 13/25 | Loss: 0.00141908
Iteration 14/25 | Loss: 0.00141908
Iteration 15/25 | Loss: 0.00141908
Iteration 16/25 | Loss: 0.00141908
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0014190812362357974, 0.0014190812362357974, 0.0014190812362357974, 0.0014190812362357974, 0.0014190812362357974]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014190812362357974

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.22904634
Iteration 2/25 | Loss: 0.00091566
Iteration 3/25 | Loss: 0.00091565
Iteration 4/25 | Loss: 0.00091565
Iteration 5/25 | Loss: 0.00091565
Iteration 6/25 | Loss: 0.00091565
Iteration 7/25 | Loss: 0.00091565
Iteration 8/25 | Loss: 0.00091565
Iteration 9/25 | Loss: 0.00091565
Iteration 10/25 | Loss: 0.00091565
Iteration 11/25 | Loss: 0.00091565
Iteration 12/25 | Loss: 0.00091565
Iteration 13/25 | Loss: 0.00091565
Iteration 14/25 | Loss: 0.00091565
Iteration 15/25 | Loss: 0.00091565
Iteration 16/25 | Loss: 0.00091565
Iteration 17/25 | Loss: 0.00091565
Iteration 18/25 | Loss: 0.00091565
Iteration 19/25 | Loss: 0.00091565
Iteration 20/25 | Loss: 0.00091565
Iteration 21/25 | Loss: 0.00091565
Iteration 22/25 | Loss: 0.00091565
Iteration 23/25 | Loss: 0.00091565
Iteration 24/25 | Loss: 0.00091565
Iteration 25/25 | Loss: 0.00091565

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00091565
Iteration 2/1000 | Loss: 0.00009620
Iteration 3/1000 | Loss: 0.00006007
Iteration 4/1000 | Loss: 0.00004596
Iteration 5/1000 | Loss: 0.00004343
Iteration 6/1000 | Loss: 0.00004176
Iteration 7/1000 | Loss: 0.00004104
Iteration 8/1000 | Loss: 0.00004048
Iteration 9/1000 | Loss: 0.00003981
Iteration 10/1000 | Loss: 0.00003900
Iteration 11/1000 | Loss: 0.00003854
Iteration 12/1000 | Loss: 0.00003814
Iteration 13/1000 | Loss: 0.00003763
Iteration 14/1000 | Loss: 0.00003731
Iteration 15/1000 | Loss: 0.00003699
Iteration 16/1000 | Loss: 0.00003678
Iteration 17/1000 | Loss: 0.00003653
Iteration 18/1000 | Loss: 0.00003635
Iteration 19/1000 | Loss: 0.00003632
Iteration 20/1000 | Loss: 0.00003614
Iteration 21/1000 | Loss: 0.00003602
Iteration 22/1000 | Loss: 0.00003594
Iteration 23/1000 | Loss: 0.00003586
Iteration 24/1000 | Loss: 0.00003579
Iteration 25/1000 | Loss: 0.00003576
Iteration 26/1000 | Loss: 0.00003564
Iteration 27/1000 | Loss: 0.00003558
Iteration 28/1000 | Loss: 0.00003546
Iteration 29/1000 | Loss: 0.00003545
Iteration 30/1000 | Loss: 0.00003536
Iteration 31/1000 | Loss: 0.00003536
Iteration 32/1000 | Loss: 0.00003535
Iteration 33/1000 | Loss: 0.00003535
Iteration 34/1000 | Loss: 0.00003535
Iteration 35/1000 | Loss: 0.00003535
Iteration 36/1000 | Loss: 0.00003534
Iteration 37/1000 | Loss: 0.00003534
Iteration 38/1000 | Loss: 0.00003533
Iteration 39/1000 | Loss: 0.00003533
Iteration 40/1000 | Loss: 0.00003533
Iteration 41/1000 | Loss: 0.00003533
Iteration 42/1000 | Loss: 0.00003532
Iteration 43/1000 | Loss: 0.00003532
Iteration 44/1000 | Loss: 0.00003532
Iteration 45/1000 | Loss: 0.00003532
Iteration 46/1000 | Loss: 0.00003532
Iteration 47/1000 | Loss: 0.00003532
Iteration 48/1000 | Loss: 0.00003532
Iteration 49/1000 | Loss: 0.00003532
Iteration 50/1000 | Loss: 0.00003531
Iteration 51/1000 | Loss: 0.00003530
Iteration 52/1000 | Loss: 0.00003530
Iteration 53/1000 | Loss: 0.00003530
Iteration 54/1000 | Loss: 0.00003530
Iteration 55/1000 | Loss: 0.00003530
Iteration 56/1000 | Loss: 0.00003530
Iteration 57/1000 | Loss: 0.00003530
Iteration 58/1000 | Loss: 0.00003529
Iteration 59/1000 | Loss: 0.00003525
Iteration 60/1000 | Loss: 0.00003525
Iteration 61/1000 | Loss: 0.00003525
Iteration 62/1000 | Loss: 0.00003525
Iteration 63/1000 | Loss: 0.00003525
Iteration 64/1000 | Loss: 0.00003524
Iteration 65/1000 | Loss: 0.00003524
Iteration 66/1000 | Loss: 0.00003524
Iteration 67/1000 | Loss: 0.00003524
Iteration 68/1000 | Loss: 0.00003524
Iteration 69/1000 | Loss: 0.00003524
Iteration 70/1000 | Loss: 0.00003524
Iteration 71/1000 | Loss: 0.00003524
Iteration 72/1000 | Loss: 0.00003524
Iteration 73/1000 | Loss: 0.00003524
Iteration 74/1000 | Loss: 0.00003523
Iteration 75/1000 | Loss: 0.00003523
Iteration 76/1000 | Loss: 0.00003523
Iteration 77/1000 | Loss: 0.00003523
Iteration 78/1000 | Loss: 0.00003523
Iteration 79/1000 | Loss: 0.00003523
Iteration 80/1000 | Loss: 0.00003523
Iteration 81/1000 | Loss: 0.00003523
Iteration 82/1000 | Loss: 0.00003522
Iteration 83/1000 | Loss: 0.00003522
Iteration 84/1000 | Loss: 0.00003522
Iteration 85/1000 | Loss: 0.00003521
Iteration 86/1000 | Loss: 0.00003521
Iteration 87/1000 | Loss: 0.00003521
Iteration 88/1000 | Loss: 0.00003520
Iteration 89/1000 | Loss: 0.00003520
Iteration 90/1000 | Loss: 0.00003520
Iteration 91/1000 | Loss: 0.00003519
Iteration 92/1000 | Loss: 0.00003519
Iteration 93/1000 | Loss: 0.00003519
Iteration 94/1000 | Loss: 0.00003519
Iteration 95/1000 | Loss: 0.00003519
Iteration 96/1000 | Loss: 0.00003519
Iteration 97/1000 | Loss: 0.00003519
Iteration 98/1000 | Loss: 0.00003519
Iteration 99/1000 | Loss: 0.00003519
Iteration 100/1000 | Loss: 0.00003519
Iteration 101/1000 | Loss: 0.00003519
Iteration 102/1000 | Loss: 0.00003519
Iteration 103/1000 | Loss: 0.00003519
Iteration 104/1000 | Loss: 0.00003518
Iteration 105/1000 | Loss: 0.00003518
Iteration 106/1000 | Loss: 0.00003518
Iteration 107/1000 | Loss: 0.00003518
Iteration 108/1000 | Loss: 0.00003518
Iteration 109/1000 | Loss: 0.00003518
Iteration 110/1000 | Loss: 0.00003518
Iteration 111/1000 | Loss: 0.00003518
Iteration 112/1000 | Loss: 0.00003518
Iteration 113/1000 | Loss: 0.00003518
Iteration 114/1000 | Loss: 0.00003517
Iteration 115/1000 | Loss: 0.00003517
Iteration 116/1000 | Loss: 0.00003517
Iteration 117/1000 | Loss: 0.00003517
Iteration 118/1000 | Loss: 0.00003517
Iteration 119/1000 | Loss: 0.00003517
Iteration 120/1000 | Loss: 0.00003517
Iteration 121/1000 | Loss: 0.00003517
Iteration 122/1000 | Loss: 0.00003517
Iteration 123/1000 | Loss: 0.00003517
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 123. Stopping optimization.
Last 5 losses: [3.5173561627743766e-05, 3.5173561627743766e-05, 3.5173561627743766e-05, 3.5173561627743766e-05, 3.5173561627743766e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.5173561627743766e-05

Optimization complete. Final v2v error: 4.737231731414795 mm

Highest mean error: 5.587531089782715 mm for frame 25

Lowest mean error: 4.122326850891113 mm for frame 37

Saving results

Total time: 52.3511106967926
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_018/1093/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_018/1093.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_018/1093
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00858014
Iteration 2/25 | Loss: 0.00123654
Iteration 3/25 | Loss: 0.00115275
Iteration 4/25 | Loss: 0.00113926
Iteration 5/25 | Loss: 0.00113579
Iteration 6/25 | Loss: 0.00113575
Iteration 7/25 | Loss: 0.00113575
Iteration 8/25 | Loss: 0.00113575
Iteration 9/25 | Loss: 0.00113575
Iteration 10/25 | Loss: 0.00113575
Iteration 11/25 | Loss: 0.00113575
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011357530020177364, 0.0011357530020177364, 0.0011357530020177364, 0.0011357530020177364, 0.0011357530020177364]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011357530020177364

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39576709
Iteration 2/25 | Loss: 0.00082141
Iteration 3/25 | Loss: 0.00082140
Iteration 4/25 | Loss: 0.00082140
Iteration 5/25 | Loss: 0.00082140
Iteration 6/25 | Loss: 0.00082140
Iteration 7/25 | Loss: 0.00082140
Iteration 8/25 | Loss: 0.00082140
Iteration 9/25 | Loss: 0.00082140
Iteration 10/25 | Loss: 0.00082140
Iteration 11/25 | Loss: 0.00082140
Iteration 12/25 | Loss: 0.00082140
Iteration 13/25 | Loss: 0.00082140
Iteration 14/25 | Loss: 0.00082140
Iteration 15/25 | Loss: 0.00082140
Iteration 16/25 | Loss: 0.00082140
Iteration 17/25 | Loss: 0.00082140
Iteration 18/25 | Loss: 0.00082140
Iteration 19/25 | Loss: 0.00082140
Iteration 20/25 | Loss: 0.00082140
Iteration 21/25 | Loss: 0.00082140
Iteration 22/25 | Loss: 0.00082140
Iteration 23/25 | Loss: 0.00082140
Iteration 24/25 | Loss: 0.00082140
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0008213990950025618, 0.0008213990950025618, 0.0008213990950025618, 0.0008213990950025618, 0.0008213990950025618]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008213990950025618

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00082140
Iteration 2/1000 | Loss: 0.00002310
Iteration 3/1000 | Loss: 0.00001684
Iteration 4/1000 | Loss: 0.00001548
Iteration 5/1000 | Loss: 0.00001481
Iteration 6/1000 | Loss: 0.00001424
Iteration 7/1000 | Loss: 0.00001384
Iteration 8/1000 | Loss: 0.00001363
Iteration 9/1000 | Loss: 0.00001331
Iteration 10/1000 | Loss: 0.00001315
Iteration 11/1000 | Loss: 0.00001312
Iteration 12/1000 | Loss: 0.00001307
Iteration 13/1000 | Loss: 0.00001306
Iteration 14/1000 | Loss: 0.00001306
Iteration 15/1000 | Loss: 0.00001305
Iteration 16/1000 | Loss: 0.00001305
Iteration 17/1000 | Loss: 0.00001295
Iteration 18/1000 | Loss: 0.00001290
Iteration 19/1000 | Loss: 0.00001288
Iteration 20/1000 | Loss: 0.00001287
Iteration 21/1000 | Loss: 0.00001285
Iteration 22/1000 | Loss: 0.00001284
Iteration 23/1000 | Loss: 0.00001283
Iteration 24/1000 | Loss: 0.00001282
Iteration 25/1000 | Loss: 0.00001278
Iteration 26/1000 | Loss: 0.00001277
Iteration 27/1000 | Loss: 0.00001276
Iteration 28/1000 | Loss: 0.00001276
Iteration 29/1000 | Loss: 0.00001276
Iteration 30/1000 | Loss: 0.00001275
Iteration 31/1000 | Loss: 0.00001275
Iteration 32/1000 | Loss: 0.00001274
Iteration 33/1000 | Loss: 0.00001274
Iteration 34/1000 | Loss: 0.00001271
Iteration 35/1000 | Loss: 0.00001271
Iteration 36/1000 | Loss: 0.00001270
Iteration 37/1000 | Loss: 0.00001270
Iteration 38/1000 | Loss: 0.00001269
Iteration 39/1000 | Loss: 0.00001268
Iteration 40/1000 | Loss: 0.00001268
Iteration 41/1000 | Loss: 0.00001267
Iteration 42/1000 | Loss: 0.00001267
Iteration 43/1000 | Loss: 0.00001266
Iteration 44/1000 | Loss: 0.00001266
Iteration 45/1000 | Loss: 0.00001266
Iteration 46/1000 | Loss: 0.00001266
Iteration 47/1000 | Loss: 0.00001262
Iteration 48/1000 | Loss: 0.00001262
Iteration 49/1000 | Loss: 0.00001260
Iteration 50/1000 | Loss: 0.00001260
Iteration 51/1000 | Loss: 0.00001259
Iteration 52/1000 | Loss: 0.00001259
Iteration 53/1000 | Loss: 0.00001258
Iteration 54/1000 | Loss: 0.00001255
Iteration 55/1000 | Loss: 0.00001255
Iteration 56/1000 | Loss: 0.00001255
Iteration 57/1000 | Loss: 0.00001255
Iteration 58/1000 | Loss: 0.00001255
Iteration 59/1000 | Loss: 0.00001255
Iteration 60/1000 | Loss: 0.00001254
Iteration 61/1000 | Loss: 0.00001253
Iteration 62/1000 | Loss: 0.00001253
Iteration 63/1000 | Loss: 0.00001253
Iteration 64/1000 | Loss: 0.00001252
Iteration 65/1000 | Loss: 0.00001252
Iteration 66/1000 | Loss: 0.00001252
Iteration 67/1000 | Loss: 0.00001252
Iteration 68/1000 | Loss: 0.00001252
Iteration 69/1000 | Loss: 0.00001251
Iteration 70/1000 | Loss: 0.00001251
Iteration 71/1000 | Loss: 0.00001251
Iteration 72/1000 | Loss: 0.00001251
Iteration 73/1000 | Loss: 0.00001250
Iteration 74/1000 | Loss: 0.00001250
Iteration 75/1000 | Loss: 0.00001250
Iteration 76/1000 | Loss: 0.00001250
Iteration 77/1000 | Loss: 0.00001249
Iteration 78/1000 | Loss: 0.00001249
Iteration 79/1000 | Loss: 0.00001249
Iteration 80/1000 | Loss: 0.00001248
Iteration 81/1000 | Loss: 0.00001248
Iteration 82/1000 | Loss: 0.00001248
Iteration 83/1000 | Loss: 0.00001248
Iteration 84/1000 | Loss: 0.00001247
Iteration 85/1000 | Loss: 0.00001247
Iteration 86/1000 | Loss: 0.00001247
Iteration 87/1000 | Loss: 0.00001247
Iteration 88/1000 | Loss: 0.00001247
Iteration 89/1000 | Loss: 0.00001247
Iteration 90/1000 | Loss: 0.00001247
Iteration 91/1000 | Loss: 0.00001247
Iteration 92/1000 | Loss: 0.00001247
Iteration 93/1000 | Loss: 0.00001247
Iteration 94/1000 | Loss: 0.00001247
Iteration 95/1000 | Loss: 0.00001247
Iteration 96/1000 | Loss: 0.00001246
Iteration 97/1000 | Loss: 0.00001246
Iteration 98/1000 | Loss: 0.00001246
Iteration 99/1000 | Loss: 0.00001246
Iteration 100/1000 | Loss: 0.00001246
Iteration 101/1000 | Loss: 0.00001245
Iteration 102/1000 | Loss: 0.00001245
Iteration 103/1000 | Loss: 0.00001245
Iteration 104/1000 | Loss: 0.00001245
Iteration 105/1000 | Loss: 0.00001245
Iteration 106/1000 | Loss: 0.00001245
Iteration 107/1000 | Loss: 0.00001244
Iteration 108/1000 | Loss: 0.00001244
Iteration 109/1000 | Loss: 0.00001244
Iteration 110/1000 | Loss: 0.00001243
Iteration 111/1000 | Loss: 0.00001243
Iteration 112/1000 | Loss: 0.00001242
Iteration 113/1000 | Loss: 0.00001242
Iteration 114/1000 | Loss: 0.00001242
Iteration 115/1000 | Loss: 0.00001242
Iteration 116/1000 | Loss: 0.00001241
Iteration 117/1000 | Loss: 0.00001241
Iteration 118/1000 | Loss: 0.00001241
Iteration 119/1000 | Loss: 0.00001241
Iteration 120/1000 | Loss: 0.00001241
Iteration 121/1000 | Loss: 0.00001241
Iteration 122/1000 | Loss: 0.00001241
Iteration 123/1000 | Loss: 0.00001240
Iteration 124/1000 | Loss: 0.00001240
Iteration 125/1000 | Loss: 0.00001240
Iteration 126/1000 | Loss: 0.00001240
Iteration 127/1000 | Loss: 0.00001240
Iteration 128/1000 | Loss: 0.00001240
Iteration 129/1000 | Loss: 0.00001239
Iteration 130/1000 | Loss: 0.00001239
Iteration 131/1000 | Loss: 0.00001239
Iteration 132/1000 | Loss: 0.00001239
Iteration 133/1000 | Loss: 0.00001238
Iteration 134/1000 | Loss: 0.00001238
Iteration 135/1000 | Loss: 0.00001238
Iteration 136/1000 | Loss: 0.00001238
Iteration 137/1000 | Loss: 0.00001238
Iteration 138/1000 | Loss: 0.00001238
Iteration 139/1000 | Loss: 0.00001238
Iteration 140/1000 | Loss: 0.00001238
Iteration 141/1000 | Loss: 0.00001238
Iteration 142/1000 | Loss: 0.00001238
Iteration 143/1000 | Loss: 0.00001238
Iteration 144/1000 | Loss: 0.00001237
Iteration 145/1000 | Loss: 0.00001237
Iteration 146/1000 | Loss: 0.00001237
Iteration 147/1000 | Loss: 0.00001237
Iteration 148/1000 | Loss: 0.00001237
Iteration 149/1000 | Loss: 0.00001236
Iteration 150/1000 | Loss: 0.00001236
Iteration 151/1000 | Loss: 0.00001236
Iteration 152/1000 | Loss: 0.00001236
Iteration 153/1000 | Loss: 0.00001236
Iteration 154/1000 | Loss: 0.00001236
Iteration 155/1000 | Loss: 0.00001236
Iteration 156/1000 | Loss: 0.00001235
Iteration 157/1000 | Loss: 0.00001235
Iteration 158/1000 | Loss: 0.00001235
Iteration 159/1000 | Loss: 0.00001235
Iteration 160/1000 | Loss: 0.00001235
Iteration 161/1000 | Loss: 0.00001235
Iteration 162/1000 | Loss: 0.00001235
Iteration 163/1000 | Loss: 0.00001235
Iteration 164/1000 | Loss: 0.00001235
Iteration 165/1000 | Loss: 0.00001234
Iteration 166/1000 | Loss: 0.00001234
Iteration 167/1000 | Loss: 0.00001234
Iteration 168/1000 | Loss: 0.00001234
Iteration 169/1000 | Loss: 0.00001234
Iteration 170/1000 | Loss: 0.00001234
Iteration 171/1000 | Loss: 0.00001234
Iteration 172/1000 | Loss: 0.00001233
Iteration 173/1000 | Loss: 0.00001233
Iteration 174/1000 | Loss: 0.00001232
Iteration 175/1000 | Loss: 0.00001232
Iteration 176/1000 | Loss: 0.00001232
Iteration 177/1000 | Loss: 0.00001232
Iteration 178/1000 | Loss: 0.00001232
Iteration 179/1000 | Loss: 0.00001232
Iteration 180/1000 | Loss: 0.00001232
Iteration 181/1000 | Loss: 0.00001232
Iteration 182/1000 | Loss: 0.00001232
Iteration 183/1000 | Loss: 0.00001231
Iteration 184/1000 | Loss: 0.00001231
Iteration 185/1000 | Loss: 0.00001231
Iteration 186/1000 | Loss: 0.00001231
Iteration 187/1000 | Loss: 0.00001231
Iteration 188/1000 | Loss: 0.00001230
Iteration 189/1000 | Loss: 0.00001230
Iteration 190/1000 | Loss: 0.00001230
Iteration 191/1000 | Loss: 0.00001230
Iteration 192/1000 | Loss: 0.00001230
Iteration 193/1000 | Loss: 0.00001230
Iteration 194/1000 | Loss: 0.00001230
Iteration 195/1000 | Loss: 0.00001230
Iteration 196/1000 | Loss: 0.00001230
Iteration 197/1000 | Loss: 0.00001229
Iteration 198/1000 | Loss: 0.00001229
Iteration 199/1000 | Loss: 0.00001229
Iteration 200/1000 | Loss: 0.00001229
Iteration 201/1000 | Loss: 0.00001229
Iteration 202/1000 | Loss: 0.00001229
Iteration 203/1000 | Loss: 0.00001229
Iteration 204/1000 | Loss: 0.00001229
Iteration 205/1000 | Loss: 0.00001229
Iteration 206/1000 | Loss: 0.00001229
Iteration 207/1000 | Loss: 0.00001229
Iteration 208/1000 | Loss: 0.00001229
Iteration 209/1000 | Loss: 0.00001229
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 209. Stopping optimization.
Last 5 losses: [1.2292501196498051e-05, 1.2292501196498051e-05, 1.2292501196498051e-05, 1.2292501196498051e-05, 1.2292501196498051e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2292501196498051e-05

Optimization complete. Final v2v error: 2.9749019145965576 mm

Highest mean error: 3.9092350006103516 mm for frame 92

Lowest mean error: 2.751157760620117 mm for frame 157

Saving results

Total time: 44.675599336624146
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_018/1098/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_018/1098.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_018/1098
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00863319
Iteration 2/25 | Loss: 0.00177886
Iteration 3/25 | Loss: 0.00144842
Iteration 4/25 | Loss: 0.00137569
Iteration 5/25 | Loss: 0.00142792
Iteration 6/25 | Loss: 0.00137581
Iteration 7/25 | Loss: 0.00135118
Iteration 8/25 | Loss: 0.00126662
Iteration 9/25 | Loss: 0.00125226
Iteration 10/25 | Loss: 0.00125621
Iteration 11/25 | Loss: 0.00125095
Iteration 12/25 | Loss: 0.00124535
Iteration 13/25 | Loss: 0.00122975
Iteration 14/25 | Loss: 0.00122483
Iteration 15/25 | Loss: 0.00122768
Iteration 16/25 | Loss: 0.00122652
Iteration 17/25 | Loss: 0.00122305
Iteration 18/25 | Loss: 0.00122157
Iteration 19/25 | Loss: 0.00122129
Iteration 20/25 | Loss: 0.00122121
Iteration 21/25 | Loss: 0.00122121
Iteration 22/25 | Loss: 0.00122121
Iteration 23/25 | Loss: 0.00122120
Iteration 24/25 | Loss: 0.00122120
Iteration 25/25 | Loss: 0.00122120

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 6.52986956
Iteration 2/25 | Loss: 0.00095161
Iteration 3/25 | Loss: 0.00095160
Iteration 4/25 | Loss: 0.00095160
Iteration 5/25 | Loss: 0.00095160
Iteration 6/25 | Loss: 0.00095160
Iteration 7/25 | Loss: 0.00095160
Iteration 8/25 | Loss: 0.00095160
Iteration 9/25 | Loss: 0.00095160
Iteration 10/25 | Loss: 0.00095160
Iteration 11/25 | Loss: 0.00095160
Iteration 12/25 | Loss: 0.00095160
Iteration 13/25 | Loss: 0.00095160
Iteration 14/25 | Loss: 0.00095160
Iteration 15/25 | Loss: 0.00095160
Iteration 16/25 | Loss: 0.00095160
Iteration 17/25 | Loss: 0.00095160
Iteration 18/25 | Loss: 0.00095160
Iteration 19/25 | Loss: 0.00095160
Iteration 20/25 | Loss: 0.00095160
Iteration 21/25 | Loss: 0.00095160
Iteration 22/25 | Loss: 0.00095160
Iteration 23/25 | Loss: 0.00095160
Iteration 24/25 | Loss: 0.00095160
Iteration 25/25 | Loss: 0.00095160

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00095160
Iteration 2/1000 | Loss: 0.00006300
Iteration 3/1000 | Loss: 0.00017350
Iteration 4/1000 | Loss: 0.00003175
Iteration 5/1000 | Loss: 0.00010883
Iteration 6/1000 | Loss: 0.00007821
Iteration 7/1000 | Loss: 0.00003239
Iteration 8/1000 | Loss: 0.00002662
Iteration 9/1000 | Loss: 0.00006797
Iteration 10/1000 | Loss: 0.00032987
Iteration 11/1000 | Loss: 0.00038740
Iteration 12/1000 | Loss: 0.00003772
Iteration 13/1000 | Loss: 0.00004413
Iteration 14/1000 | Loss: 0.00004316
Iteration 15/1000 | Loss: 0.00007828
Iteration 16/1000 | Loss: 0.00009523
Iteration 17/1000 | Loss: 0.00003599
Iteration 18/1000 | Loss: 0.00003164
Iteration 19/1000 | Loss: 0.00002987
Iteration 20/1000 | Loss: 0.00004546
Iteration 21/1000 | Loss: 0.00003135
Iteration 22/1000 | Loss: 0.00003165
Iteration 23/1000 | Loss: 0.00003078
Iteration 24/1000 | Loss: 0.00003172
Iteration 25/1000 | Loss: 0.00003212
Iteration 26/1000 | Loss: 0.00003439
Iteration 27/1000 | Loss: 0.00003186
Iteration 28/1000 | Loss: 0.00008678
Iteration 29/1000 | Loss: 0.00003121
Iteration 30/1000 | Loss: 0.00002873
Iteration 31/1000 | Loss: 0.00003087
Iteration 32/1000 | Loss: 0.00020648
Iteration 33/1000 | Loss: 0.00004419
Iteration 34/1000 | Loss: 0.00003208
Iteration 35/1000 | Loss: 0.00002628
Iteration 36/1000 | Loss: 0.00004711
Iteration 37/1000 | Loss: 0.00004604
Iteration 38/1000 | Loss: 0.00012506
Iteration 39/1000 | Loss: 0.00003088
Iteration 40/1000 | Loss: 0.00003545
Iteration 41/1000 | Loss: 0.00011124
Iteration 42/1000 | Loss: 0.00001934
Iteration 43/1000 | Loss: 0.00001795
Iteration 44/1000 | Loss: 0.00004122
Iteration 45/1000 | Loss: 0.00001739
Iteration 46/1000 | Loss: 0.00001732
Iteration 47/1000 | Loss: 0.00001715
Iteration 48/1000 | Loss: 0.00001714
Iteration 49/1000 | Loss: 0.00001713
Iteration 50/1000 | Loss: 0.00001713
Iteration 51/1000 | Loss: 0.00001712
Iteration 52/1000 | Loss: 0.00001712
Iteration 53/1000 | Loss: 0.00001711
Iteration 54/1000 | Loss: 0.00001711
Iteration 55/1000 | Loss: 0.00001710
Iteration 56/1000 | Loss: 0.00001710
Iteration 57/1000 | Loss: 0.00001710
Iteration 58/1000 | Loss: 0.00001709
Iteration 59/1000 | Loss: 0.00001709
Iteration 60/1000 | Loss: 0.00001707
Iteration 61/1000 | Loss: 0.00001706
Iteration 62/1000 | Loss: 0.00001705
Iteration 63/1000 | Loss: 0.00001705
Iteration 64/1000 | Loss: 0.00001705
Iteration 65/1000 | Loss: 0.00001704
Iteration 66/1000 | Loss: 0.00001704
Iteration 67/1000 | Loss: 0.00001704
Iteration 68/1000 | Loss: 0.00001704
Iteration 69/1000 | Loss: 0.00001704
Iteration 70/1000 | Loss: 0.00001704
Iteration 71/1000 | Loss: 0.00001704
Iteration 72/1000 | Loss: 0.00001704
Iteration 73/1000 | Loss: 0.00001704
Iteration 74/1000 | Loss: 0.00001704
Iteration 75/1000 | Loss: 0.00001704
Iteration 76/1000 | Loss: 0.00001704
Iteration 77/1000 | Loss: 0.00001704
Iteration 78/1000 | Loss: 0.00001702
Iteration 79/1000 | Loss: 0.00001702
Iteration 80/1000 | Loss: 0.00001701
Iteration 81/1000 | Loss: 0.00001701
Iteration 82/1000 | Loss: 0.00001701
Iteration 83/1000 | Loss: 0.00001700
Iteration 84/1000 | Loss: 0.00001700
Iteration 85/1000 | Loss: 0.00001699
Iteration 86/1000 | Loss: 0.00001698
Iteration 87/1000 | Loss: 0.00001697
Iteration 88/1000 | Loss: 0.00001696
Iteration 89/1000 | Loss: 0.00001695
Iteration 90/1000 | Loss: 0.00001691
Iteration 91/1000 | Loss: 0.00001691
Iteration 92/1000 | Loss: 0.00001691
Iteration 93/1000 | Loss: 0.00001691
Iteration 94/1000 | Loss: 0.00001690
Iteration 95/1000 | Loss: 0.00001688
Iteration 96/1000 | Loss: 0.00001688
Iteration 97/1000 | Loss: 0.00001688
Iteration 98/1000 | Loss: 0.00001687
Iteration 99/1000 | Loss: 0.00001687
Iteration 100/1000 | Loss: 0.00001687
Iteration 101/1000 | Loss: 0.00001687
Iteration 102/1000 | Loss: 0.00001687
Iteration 103/1000 | Loss: 0.00001686
Iteration 104/1000 | Loss: 0.00001686
Iteration 105/1000 | Loss: 0.00001685
Iteration 106/1000 | Loss: 0.00001684
Iteration 107/1000 | Loss: 0.00001683
Iteration 108/1000 | Loss: 0.00001683
Iteration 109/1000 | Loss: 0.00001683
Iteration 110/1000 | Loss: 0.00001683
Iteration 111/1000 | Loss: 0.00001682
Iteration 112/1000 | Loss: 0.00001681
Iteration 113/1000 | Loss: 0.00001680
Iteration 114/1000 | Loss: 0.00001679
Iteration 115/1000 | Loss: 0.00001679
Iteration 116/1000 | Loss: 0.00001678
Iteration 117/1000 | Loss: 0.00001678
Iteration 118/1000 | Loss: 0.00001678
Iteration 119/1000 | Loss: 0.00001678
Iteration 120/1000 | Loss: 0.00001677
Iteration 121/1000 | Loss: 0.00001677
Iteration 122/1000 | Loss: 0.00001677
Iteration 123/1000 | Loss: 0.00001677
Iteration 124/1000 | Loss: 0.00001676
Iteration 125/1000 | Loss: 0.00001676
Iteration 126/1000 | Loss: 0.00001676
Iteration 127/1000 | Loss: 0.00001675
Iteration 128/1000 | Loss: 0.00001675
Iteration 129/1000 | Loss: 0.00001675
Iteration 130/1000 | Loss: 0.00001675
Iteration 131/1000 | Loss: 0.00001674
Iteration 132/1000 | Loss: 0.00001674
Iteration 133/1000 | Loss: 0.00001674
Iteration 134/1000 | Loss: 0.00001674
Iteration 135/1000 | Loss: 0.00001674
Iteration 136/1000 | Loss: 0.00001673
Iteration 137/1000 | Loss: 0.00001673
Iteration 138/1000 | Loss: 0.00001673
Iteration 139/1000 | Loss: 0.00001672
Iteration 140/1000 | Loss: 0.00001672
Iteration 141/1000 | Loss: 0.00001672
Iteration 142/1000 | Loss: 0.00001672
Iteration 143/1000 | Loss: 0.00001672
Iteration 144/1000 | Loss: 0.00001671
Iteration 145/1000 | Loss: 0.00001671
Iteration 146/1000 | Loss: 0.00001670
Iteration 147/1000 | Loss: 0.00001669
Iteration 148/1000 | Loss: 0.00001669
Iteration 149/1000 | Loss: 0.00001668
Iteration 150/1000 | Loss: 0.00001668
Iteration 151/1000 | Loss: 0.00001668
Iteration 152/1000 | Loss: 0.00001667
Iteration 153/1000 | Loss: 0.00001666
Iteration 154/1000 | Loss: 0.00001666
Iteration 155/1000 | Loss: 0.00001666
Iteration 156/1000 | Loss: 0.00001666
Iteration 157/1000 | Loss: 0.00001666
Iteration 158/1000 | Loss: 0.00001666
Iteration 159/1000 | Loss: 0.00001666
Iteration 160/1000 | Loss: 0.00001666
Iteration 161/1000 | Loss: 0.00001666
Iteration 162/1000 | Loss: 0.00001665
Iteration 163/1000 | Loss: 0.00001665
Iteration 164/1000 | Loss: 0.00001665
Iteration 165/1000 | Loss: 0.00001665
Iteration 166/1000 | Loss: 0.00001665
Iteration 167/1000 | Loss: 0.00001665
Iteration 168/1000 | Loss: 0.00001665
Iteration 169/1000 | Loss: 0.00001664
Iteration 170/1000 | Loss: 0.00001664
Iteration 171/1000 | Loss: 0.00001664
Iteration 172/1000 | Loss: 0.00001664
Iteration 173/1000 | Loss: 0.00001664
Iteration 174/1000 | Loss: 0.00001664
Iteration 175/1000 | Loss: 0.00001663
Iteration 176/1000 | Loss: 0.00001663
Iteration 177/1000 | Loss: 0.00001663
Iteration 178/1000 | Loss: 0.00001663
Iteration 179/1000 | Loss: 0.00001663
Iteration 180/1000 | Loss: 0.00001663
Iteration 181/1000 | Loss: 0.00001662
Iteration 182/1000 | Loss: 0.00001662
Iteration 183/1000 | Loss: 0.00001662
Iteration 184/1000 | Loss: 0.00001662
Iteration 185/1000 | Loss: 0.00001662
Iteration 186/1000 | Loss: 0.00001662
Iteration 187/1000 | Loss: 0.00001661
Iteration 188/1000 | Loss: 0.00001661
Iteration 189/1000 | Loss: 0.00001661
Iteration 190/1000 | Loss: 0.00001660
Iteration 191/1000 | Loss: 0.00001660
Iteration 192/1000 | Loss: 0.00001660
Iteration 193/1000 | Loss: 0.00001659
Iteration 194/1000 | Loss: 0.00001659
Iteration 195/1000 | Loss: 0.00001659
Iteration 196/1000 | Loss: 0.00001659
Iteration 197/1000 | Loss: 0.00001659
Iteration 198/1000 | Loss: 0.00001659
Iteration 199/1000 | Loss: 0.00001659
Iteration 200/1000 | Loss: 0.00001659
Iteration 201/1000 | Loss: 0.00001659
Iteration 202/1000 | Loss: 0.00001659
Iteration 203/1000 | Loss: 0.00001659
Iteration 204/1000 | Loss: 0.00001659
Iteration 205/1000 | Loss: 0.00001659
Iteration 206/1000 | Loss: 0.00001659
Iteration 207/1000 | Loss: 0.00001659
Iteration 208/1000 | Loss: 0.00001659
Iteration 209/1000 | Loss: 0.00001659
Iteration 210/1000 | Loss: 0.00001659
Iteration 211/1000 | Loss: 0.00001659
Iteration 212/1000 | Loss: 0.00001659
Iteration 213/1000 | Loss: 0.00001659
Iteration 214/1000 | Loss: 0.00001659
Iteration 215/1000 | Loss: 0.00001659
Iteration 216/1000 | Loss: 0.00001659
Iteration 217/1000 | Loss: 0.00001659
Iteration 218/1000 | Loss: 0.00001659
Iteration 219/1000 | Loss: 0.00001659
Iteration 220/1000 | Loss: 0.00001659
Iteration 221/1000 | Loss: 0.00001659
Iteration 222/1000 | Loss: 0.00001659
Iteration 223/1000 | Loss: 0.00001659
Iteration 224/1000 | Loss: 0.00001659
Iteration 225/1000 | Loss: 0.00001659
Iteration 226/1000 | Loss: 0.00001659
Iteration 227/1000 | Loss: 0.00001659
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 227. Stopping optimization.
Last 5 losses: [1.658824839978479e-05, 1.658824839978479e-05, 1.658824839978479e-05, 1.658824839978479e-05, 1.658824839978479e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.658824839978479e-05

Optimization complete. Final v2v error: 3.3545572757720947 mm

Highest mean error: 5.335447788238525 mm for frame 95

Lowest mean error: 2.9079127311706543 mm for frame 52

Saving results

Total time: 121.09552025794983
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_018/1011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_018/1011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_018/1011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01012658
Iteration 2/25 | Loss: 0.00187287
Iteration 3/25 | Loss: 0.00148655
Iteration 4/25 | Loss: 0.00142346
Iteration 5/25 | Loss: 0.00146804
Iteration 6/25 | Loss: 0.00148583
Iteration 7/25 | Loss: 0.00143064
Iteration 8/25 | Loss: 0.00135739
Iteration 9/25 | Loss: 0.00134918
Iteration 10/25 | Loss: 0.00134104
Iteration 11/25 | Loss: 0.00134026
Iteration 12/25 | Loss: 0.00134044
Iteration 13/25 | Loss: 0.00132982
Iteration 14/25 | Loss: 0.00131951
Iteration 15/25 | Loss: 0.00131333
Iteration 16/25 | Loss: 0.00131740
Iteration 17/25 | Loss: 0.00130433
Iteration 18/25 | Loss: 0.00130506
Iteration 19/25 | Loss: 0.00130476
Iteration 20/25 | Loss: 0.00131128
Iteration 21/25 | Loss: 0.00130252
Iteration 22/25 | Loss: 0.00130617
Iteration 23/25 | Loss: 0.00130130
Iteration 24/25 | Loss: 0.00130494
Iteration 25/25 | Loss: 0.00131921

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36587834
Iteration 2/25 | Loss: 0.00285780
Iteration 3/25 | Loss: 0.00227845
Iteration 4/25 | Loss: 0.00227845
Iteration 5/25 | Loss: 0.00227844
Iteration 6/25 | Loss: 0.00227844
Iteration 7/25 | Loss: 0.00227844
Iteration 8/25 | Loss: 0.00227844
Iteration 9/25 | Loss: 0.00227844
Iteration 10/25 | Loss: 0.00227844
Iteration 11/25 | Loss: 0.00227844
Iteration 12/25 | Loss: 0.00227844
Iteration 13/25 | Loss: 0.00227844
Iteration 14/25 | Loss: 0.00227844
Iteration 15/25 | Loss: 0.00227844
Iteration 16/25 | Loss: 0.00227844
Iteration 17/25 | Loss: 0.00227844
Iteration 18/25 | Loss: 0.00227844
Iteration 19/25 | Loss: 0.00227844
Iteration 20/25 | Loss: 0.00227844
Iteration 21/25 | Loss: 0.00227844
Iteration 22/25 | Loss: 0.00227844
Iteration 23/25 | Loss: 0.00227844
Iteration 24/25 | Loss: 0.00227844
Iteration 25/25 | Loss: 0.00227844

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00227844
Iteration 2/1000 | Loss: 0.00048861
Iteration 3/1000 | Loss: 0.00079017
Iteration 4/1000 | Loss: 0.00044187
Iteration 5/1000 | Loss: 0.00050746
Iteration 6/1000 | Loss: 0.00037397
Iteration 7/1000 | Loss: 0.00050539
Iteration 8/1000 | Loss: 0.00025719
Iteration 9/1000 | Loss: 0.00023655
Iteration 10/1000 | Loss: 0.00093113
Iteration 11/1000 | Loss: 0.00136728
Iteration 12/1000 | Loss: 0.00083359
Iteration 13/1000 | Loss: 0.00034894
Iteration 14/1000 | Loss: 0.00067093
Iteration 15/1000 | Loss: 0.00024048
Iteration 16/1000 | Loss: 0.00040745
Iteration 17/1000 | Loss: 0.00020375
Iteration 18/1000 | Loss: 0.00057847
Iteration 19/1000 | Loss: 0.00021782
Iteration 20/1000 | Loss: 0.00085665
Iteration 21/1000 | Loss: 0.00051894
Iteration 22/1000 | Loss: 0.00053068
Iteration 23/1000 | Loss: 0.00054063
Iteration 24/1000 | Loss: 0.00022068
Iteration 25/1000 | Loss: 0.00041202
Iteration 26/1000 | Loss: 0.00017331
Iteration 27/1000 | Loss: 0.00021641
Iteration 28/1000 | Loss: 0.00041605
Iteration 29/1000 | Loss: 0.00012840
Iteration 30/1000 | Loss: 0.00013121
Iteration 31/1000 | Loss: 0.00014896
Iteration 32/1000 | Loss: 0.00012771
Iteration 33/1000 | Loss: 0.00067977
Iteration 34/1000 | Loss: 0.00018819
Iteration 35/1000 | Loss: 0.00034942
Iteration 36/1000 | Loss: 0.00017561
Iteration 37/1000 | Loss: 0.00078380
Iteration 38/1000 | Loss: 0.00092140
Iteration 39/1000 | Loss: 0.00036504
Iteration 40/1000 | Loss: 0.00013717
Iteration 41/1000 | Loss: 0.00010353
Iteration 42/1000 | Loss: 0.00068872
Iteration 43/1000 | Loss: 0.00014276
Iteration 44/1000 | Loss: 0.00028441
Iteration 45/1000 | Loss: 0.00013578
Iteration 46/1000 | Loss: 0.00012698
Iteration 47/1000 | Loss: 0.00011048
Iteration 48/1000 | Loss: 0.00012270
Iteration 49/1000 | Loss: 0.00012317
Iteration 50/1000 | Loss: 0.00058500
Iteration 51/1000 | Loss: 0.00038499
Iteration 52/1000 | Loss: 0.00054401
Iteration 53/1000 | Loss: 0.00183072
Iteration 54/1000 | Loss: 0.00010347
Iteration 55/1000 | Loss: 0.00030244
Iteration 56/1000 | Loss: 0.00053572
Iteration 57/1000 | Loss: 0.00015262
Iteration 58/1000 | Loss: 0.00014722
Iteration 59/1000 | Loss: 0.00010988
Iteration 60/1000 | Loss: 0.00012226
Iteration 61/1000 | Loss: 0.00023143
Iteration 62/1000 | Loss: 0.00025917
Iteration 63/1000 | Loss: 0.00016570
Iteration 64/1000 | Loss: 0.00011295
Iteration 65/1000 | Loss: 0.00010830
Iteration 66/1000 | Loss: 0.00015344
Iteration 67/1000 | Loss: 0.00009989
Iteration 68/1000 | Loss: 0.00010982
Iteration 69/1000 | Loss: 0.00009885
Iteration 70/1000 | Loss: 0.00010661
Iteration 71/1000 | Loss: 0.00009086
Iteration 72/1000 | Loss: 0.00010380
Iteration 73/1000 | Loss: 0.00011467
Iteration 74/1000 | Loss: 0.00010831
Iteration 75/1000 | Loss: 0.00009528
Iteration 76/1000 | Loss: 0.00010919
Iteration 77/1000 | Loss: 0.00011669
Iteration 78/1000 | Loss: 0.00012340
Iteration 79/1000 | Loss: 0.00011709
Iteration 80/1000 | Loss: 0.00018199
Iteration 81/1000 | Loss: 0.00010351
Iteration 82/1000 | Loss: 0.00029349
Iteration 83/1000 | Loss: 0.00012214
Iteration 84/1000 | Loss: 0.00014887
Iteration 85/1000 | Loss: 0.00011994
Iteration 86/1000 | Loss: 0.00011542
Iteration 87/1000 | Loss: 0.00013074
Iteration 88/1000 | Loss: 0.00010808
Iteration 89/1000 | Loss: 0.00013769
Iteration 90/1000 | Loss: 0.00018557
Iteration 91/1000 | Loss: 0.00009841
Iteration 92/1000 | Loss: 0.00011020
Iteration 93/1000 | Loss: 0.00012599
Iteration 94/1000 | Loss: 0.00016519
Iteration 95/1000 | Loss: 0.00013090
Iteration 96/1000 | Loss: 0.00013420
Iteration 97/1000 | Loss: 0.00011299
Iteration 98/1000 | Loss: 0.00011325
Iteration 99/1000 | Loss: 0.00012742
Iteration 100/1000 | Loss: 0.00010901
Iteration 101/1000 | Loss: 0.00012960
Iteration 102/1000 | Loss: 0.00011388
Iteration 103/1000 | Loss: 0.00011345
Iteration 104/1000 | Loss: 0.00035180
Iteration 105/1000 | Loss: 0.00011424
Iteration 106/1000 | Loss: 0.00012823
Iteration 107/1000 | Loss: 0.00012161
Iteration 108/1000 | Loss: 0.00010947
Iteration 109/1000 | Loss: 0.00010271
Iteration 110/1000 | Loss: 0.00009742
Iteration 111/1000 | Loss: 0.00009778
Iteration 112/1000 | Loss: 0.00010400
Iteration 113/1000 | Loss: 0.00011436
Iteration 114/1000 | Loss: 0.00011627
Iteration 115/1000 | Loss: 0.00010925
Iteration 116/1000 | Loss: 0.00011069
Iteration 117/1000 | Loss: 0.00010889
Iteration 118/1000 | Loss: 0.00011257
Iteration 119/1000 | Loss: 0.00014782
Iteration 120/1000 | Loss: 0.00011248
Iteration 121/1000 | Loss: 0.00012329
Iteration 122/1000 | Loss: 0.00011189
Iteration 123/1000 | Loss: 0.00010790
Iteration 124/1000 | Loss: 0.00014398
Iteration 125/1000 | Loss: 0.00011561
Iteration 126/1000 | Loss: 0.00012000
Iteration 127/1000 | Loss: 0.00083356
Iteration 128/1000 | Loss: 0.00113880
Iteration 129/1000 | Loss: 0.00071996
Iteration 130/1000 | Loss: 0.00016884
Iteration 131/1000 | Loss: 0.00022148
Iteration 132/1000 | Loss: 0.00011184
Iteration 133/1000 | Loss: 0.00007459
Iteration 134/1000 | Loss: 0.00006813
Iteration 135/1000 | Loss: 0.00031318
Iteration 136/1000 | Loss: 0.00008360
Iteration 137/1000 | Loss: 0.00005712
Iteration 138/1000 | Loss: 0.00007405
Iteration 139/1000 | Loss: 0.00009437
Iteration 140/1000 | Loss: 0.00006860
Iteration 141/1000 | Loss: 0.00007191
Iteration 142/1000 | Loss: 0.00006321
Iteration 143/1000 | Loss: 0.00006825
Iteration 144/1000 | Loss: 0.00007073
Iteration 145/1000 | Loss: 0.00006558
Iteration 146/1000 | Loss: 0.00005857
Iteration 147/1000 | Loss: 0.00016693
Iteration 148/1000 | Loss: 0.00006817
Iteration 149/1000 | Loss: 0.00042117
Iteration 150/1000 | Loss: 0.00031425
Iteration 151/1000 | Loss: 0.00035353
Iteration 152/1000 | Loss: 0.00020406
Iteration 153/1000 | Loss: 0.00005745
Iteration 154/1000 | Loss: 0.00012727
Iteration 155/1000 | Loss: 0.00005476
Iteration 156/1000 | Loss: 0.00005285
Iteration 157/1000 | Loss: 0.00020304
Iteration 158/1000 | Loss: 0.00127019
Iteration 159/1000 | Loss: 0.00025393
Iteration 160/1000 | Loss: 0.00015138
Iteration 161/1000 | Loss: 0.00011477
Iteration 162/1000 | Loss: 0.00004875
Iteration 163/1000 | Loss: 0.00004973
Iteration 164/1000 | Loss: 0.00004052
Iteration 165/1000 | Loss: 0.00004495
Iteration 166/1000 | Loss: 0.00018552
Iteration 167/1000 | Loss: 0.00031947
Iteration 168/1000 | Loss: 0.00009162
Iteration 169/1000 | Loss: 0.00005726
Iteration 170/1000 | Loss: 0.00005777
Iteration 171/1000 | Loss: 0.00005605
Iteration 172/1000 | Loss: 0.00003792
Iteration 173/1000 | Loss: 0.00007599
Iteration 174/1000 | Loss: 0.00003325
Iteration 175/1000 | Loss: 0.00005799
Iteration 176/1000 | Loss: 0.00004020
Iteration 177/1000 | Loss: 0.00004190
Iteration 178/1000 | Loss: 0.00004287
Iteration 179/1000 | Loss: 0.00004708
Iteration 180/1000 | Loss: 0.00004402
Iteration 181/1000 | Loss: 0.00004692
Iteration 182/1000 | Loss: 0.00004433
Iteration 183/1000 | Loss: 0.00072473
Iteration 184/1000 | Loss: 0.00051513
Iteration 185/1000 | Loss: 0.00019479
Iteration 186/1000 | Loss: 0.00003701
Iteration 187/1000 | Loss: 0.00003421
Iteration 188/1000 | Loss: 0.00021288
Iteration 189/1000 | Loss: 0.00020239
Iteration 190/1000 | Loss: 0.00019309
Iteration 191/1000 | Loss: 0.00003576
Iteration 192/1000 | Loss: 0.00002907
Iteration 193/1000 | Loss: 0.00021060
Iteration 194/1000 | Loss: 0.00002548
Iteration 195/1000 | Loss: 0.00002592
Iteration 196/1000 | Loss: 0.00002627
Iteration 197/1000 | Loss: 0.00002640
Iteration 198/1000 | Loss: 0.00002272
Iteration 199/1000 | Loss: 0.00028093
Iteration 200/1000 | Loss: 0.00002402
Iteration 201/1000 | Loss: 0.00002258
Iteration 202/1000 | Loss: 0.00002153
Iteration 203/1000 | Loss: 0.00002068
Iteration 204/1000 | Loss: 0.00030534
Iteration 205/1000 | Loss: 0.00002419
Iteration 206/1000 | Loss: 0.00002045
Iteration 207/1000 | Loss: 0.00001939
Iteration 208/1000 | Loss: 0.00002218
Iteration 209/1000 | Loss: 0.00001823
Iteration 210/1000 | Loss: 0.00046611
Iteration 211/1000 | Loss: 0.00004352
Iteration 212/1000 | Loss: 0.00003865
Iteration 213/1000 | Loss: 0.00001934
Iteration 214/1000 | Loss: 0.00001840
Iteration 215/1000 | Loss: 0.00002920
Iteration 216/1000 | Loss: 0.00002834
Iteration 217/1000 | Loss: 0.00030249
Iteration 218/1000 | Loss: 0.00002078
Iteration 219/1000 | Loss: 0.00001774
Iteration 220/1000 | Loss: 0.00001615
Iteration 221/1000 | Loss: 0.00028262
Iteration 222/1000 | Loss: 0.00002016
Iteration 223/1000 | Loss: 0.00001673
Iteration 224/1000 | Loss: 0.00001544
Iteration 225/1000 | Loss: 0.00001415
Iteration 226/1000 | Loss: 0.00001302
Iteration 227/1000 | Loss: 0.00001313
Iteration 228/1000 | Loss: 0.00001200
Iteration 229/1000 | Loss: 0.00001284
Iteration 230/1000 | Loss: 0.00001390
Iteration 231/1000 | Loss: 0.00001159
Iteration 232/1000 | Loss: 0.00001157
Iteration 233/1000 | Loss: 0.00001240
Iteration 234/1000 | Loss: 0.00001144
Iteration 235/1000 | Loss: 0.00001189
Iteration 236/1000 | Loss: 0.00001121
Iteration 237/1000 | Loss: 0.00001120
Iteration 238/1000 | Loss: 0.00001219
Iteration 239/1000 | Loss: 0.00001109
Iteration 240/1000 | Loss: 0.00001109
Iteration 241/1000 | Loss: 0.00001108
Iteration 242/1000 | Loss: 0.00001108
Iteration 243/1000 | Loss: 0.00001105
Iteration 244/1000 | Loss: 0.00001105
Iteration 245/1000 | Loss: 0.00001105
Iteration 246/1000 | Loss: 0.00001105
Iteration 247/1000 | Loss: 0.00001105
Iteration 248/1000 | Loss: 0.00001105
Iteration 249/1000 | Loss: 0.00001104
Iteration 250/1000 | Loss: 0.00001104
Iteration 251/1000 | Loss: 0.00001104
Iteration 252/1000 | Loss: 0.00001104
Iteration 253/1000 | Loss: 0.00001104
Iteration 254/1000 | Loss: 0.00001104
Iteration 255/1000 | Loss: 0.00001104
Iteration 256/1000 | Loss: 0.00001104
Iteration 257/1000 | Loss: 0.00001104
Iteration 258/1000 | Loss: 0.00001104
Iteration 259/1000 | Loss: 0.00001103
Iteration 260/1000 | Loss: 0.00001102
Iteration 261/1000 | Loss: 0.00001099
Iteration 262/1000 | Loss: 0.00001098
Iteration 263/1000 | Loss: 0.00001098
Iteration 264/1000 | Loss: 0.00001097
Iteration 265/1000 | Loss: 0.00001097
Iteration 266/1000 | Loss: 0.00001097
Iteration 267/1000 | Loss: 0.00001097
Iteration 268/1000 | Loss: 0.00001097
Iteration 269/1000 | Loss: 0.00001096
Iteration 270/1000 | Loss: 0.00001096
Iteration 271/1000 | Loss: 0.00001096
Iteration 272/1000 | Loss: 0.00001096
Iteration 273/1000 | Loss: 0.00001095
Iteration 274/1000 | Loss: 0.00001095
Iteration 275/1000 | Loss: 0.00001094
Iteration 276/1000 | Loss: 0.00001093
Iteration 277/1000 | Loss: 0.00001093
Iteration 278/1000 | Loss: 0.00001092
Iteration 279/1000 | Loss: 0.00001092
Iteration 280/1000 | Loss: 0.00001092
Iteration 281/1000 | Loss: 0.00001092
Iteration 282/1000 | Loss: 0.00001092
Iteration 283/1000 | Loss: 0.00001092
Iteration 284/1000 | Loss: 0.00001091
Iteration 285/1000 | Loss: 0.00001090
Iteration 286/1000 | Loss: 0.00001090
Iteration 287/1000 | Loss: 0.00001090
Iteration 288/1000 | Loss: 0.00001090
Iteration 289/1000 | Loss: 0.00001101
Iteration 290/1000 | Loss: 0.00001101
Iteration 291/1000 | Loss: 0.00001088
Iteration 292/1000 | Loss: 0.00001088
Iteration 293/1000 | Loss: 0.00001087
Iteration 294/1000 | Loss: 0.00001087
Iteration 295/1000 | Loss: 0.00001087
Iteration 296/1000 | Loss: 0.00001087
Iteration 297/1000 | Loss: 0.00001087
Iteration 298/1000 | Loss: 0.00001087
Iteration 299/1000 | Loss: 0.00001087
Iteration 300/1000 | Loss: 0.00001087
Iteration 301/1000 | Loss: 0.00001087
Iteration 302/1000 | Loss: 0.00001087
Iteration 303/1000 | Loss: 0.00001087
Iteration 304/1000 | Loss: 0.00001087
Iteration 305/1000 | Loss: 0.00001087
Iteration 306/1000 | Loss: 0.00001087
Iteration 307/1000 | Loss: 0.00001087
Iteration 308/1000 | Loss: 0.00001087
Iteration 309/1000 | Loss: 0.00001087
Iteration 310/1000 | Loss: 0.00001087
Iteration 311/1000 | Loss: 0.00001087
Iteration 312/1000 | Loss: 0.00001087
Iteration 313/1000 | Loss: 0.00001087
Iteration 314/1000 | Loss: 0.00001087
Iteration 315/1000 | Loss: 0.00001087
Iteration 316/1000 | Loss: 0.00001087
Iteration 317/1000 | Loss: 0.00001087
Iteration 318/1000 | Loss: 0.00001087
Iteration 319/1000 | Loss: 0.00001087
Iteration 320/1000 | Loss: 0.00001087
Iteration 321/1000 | Loss: 0.00001087
Iteration 322/1000 | Loss: 0.00001087
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 322. Stopping optimization.
Last 5 losses: [1.0866421689570416e-05, 1.0866421689570416e-05, 1.0866421689570416e-05, 1.0866421689570416e-05, 1.0866421689570416e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0866421689570416e-05

Optimization complete. Final v2v error: 2.816932439804077 mm

Highest mean error: 4.613933086395264 mm for frame 132

Lowest mean error: 2.452744245529175 mm for frame 180

Saving results

Total time: 440.6872351169586
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_018/1017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_018/1017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_018/1017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00393855
Iteration 2/25 | Loss: 0.00121950
Iteration 3/25 | Loss: 0.00114003
Iteration 4/25 | Loss: 0.00113307
Iteration 5/25 | Loss: 0.00112987
Iteration 6/25 | Loss: 0.00112967
Iteration 7/25 | Loss: 0.00112967
Iteration 8/25 | Loss: 0.00112967
Iteration 9/25 | Loss: 0.00112967
Iteration 10/25 | Loss: 0.00112967
Iteration 11/25 | Loss: 0.00112967
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011296734446659684, 0.0011296734446659684, 0.0011296734446659684, 0.0011296734446659684, 0.0011296734446659684]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011296734446659684

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.61357403
Iteration 2/25 | Loss: 0.00067648
Iteration 3/25 | Loss: 0.00067648
Iteration 4/25 | Loss: 0.00067648
Iteration 5/25 | Loss: 0.00067648
Iteration 6/25 | Loss: 0.00067648
Iteration 7/25 | Loss: 0.00067648
Iteration 8/25 | Loss: 0.00067648
Iteration 9/25 | Loss: 0.00067648
Iteration 10/25 | Loss: 0.00067648
Iteration 11/25 | Loss: 0.00067648
Iteration 12/25 | Loss: 0.00067648
Iteration 13/25 | Loss: 0.00067648
Iteration 14/25 | Loss: 0.00067648
Iteration 15/25 | Loss: 0.00067648
Iteration 16/25 | Loss: 0.00067648
Iteration 17/25 | Loss: 0.00067648
Iteration 18/25 | Loss: 0.00067648
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0006764794234186411, 0.0006764794234186411, 0.0006764794234186411, 0.0006764794234186411, 0.0006764794234186411]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006764794234186411

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00067648
Iteration 2/1000 | Loss: 0.00002242
Iteration 3/1000 | Loss: 0.00001490
Iteration 4/1000 | Loss: 0.00001314
Iteration 5/1000 | Loss: 0.00001249
Iteration 6/1000 | Loss: 0.00001189
Iteration 7/1000 | Loss: 0.00001153
Iteration 8/1000 | Loss: 0.00001125
Iteration 9/1000 | Loss: 0.00001088
Iteration 10/1000 | Loss: 0.00001063
Iteration 11/1000 | Loss: 0.00001058
Iteration 12/1000 | Loss: 0.00001057
Iteration 13/1000 | Loss: 0.00001055
Iteration 14/1000 | Loss: 0.00001040
Iteration 15/1000 | Loss: 0.00001039
Iteration 16/1000 | Loss: 0.00001039
Iteration 17/1000 | Loss: 0.00001026
Iteration 18/1000 | Loss: 0.00001018
Iteration 19/1000 | Loss: 0.00001017
Iteration 20/1000 | Loss: 0.00001017
Iteration 21/1000 | Loss: 0.00001016
Iteration 22/1000 | Loss: 0.00001015
Iteration 23/1000 | Loss: 0.00001015
Iteration 24/1000 | Loss: 0.00001014
Iteration 25/1000 | Loss: 0.00001013
Iteration 26/1000 | Loss: 0.00001013
Iteration 27/1000 | Loss: 0.00001013
Iteration 28/1000 | Loss: 0.00001012
Iteration 29/1000 | Loss: 0.00001012
Iteration 30/1000 | Loss: 0.00001012
Iteration 31/1000 | Loss: 0.00001012
Iteration 32/1000 | Loss: 0.00001012
Iteration 33/1000 | Loss: 0.00001012
Iteration 34/1000 | Loss: 0.00001012
Iteration 35/1000 | Loss: 0.00001012
Iteration 36/1000 | Loss: 0.00001012
Iteration 37/1000 | Loss: 0.00001011
Iteration 38/1000 | Loss: 0.00001011
Iteration 39/1000 | Loss: 0.00001011
Iteration 40/1000 | Loss: 0.00001011
Iteration 41/1000 | Loss: 0.00001011
Iteration 42/1000 | Loss: 0.00001011
Iteration 43/1000 | Loss: 0.00001011
Iteration 44/1000 | Loss: 0.00001011
Iteration 45/1000 | Loss: 0.00001011
Iteration 46/1000 | Loss: 0.00001011
Iteration 47/1000 | Loss: 0.00001011
Iteration 48/1000 | Loss: 0.00001010
Iteration 49/1000 | Loss: 0.00001010
Iteration 50/1000 | Loss: 0.00001010
Iteration 51/1000 | Loss: 0.00001009
Iteration 52/1000 | Loss: 0.00001009
Iteration 53/1000 | Loss: 0.00001008
Iteration 54/1000 | Loss: 0.00001007
Iteration 55/1000 | Loss: 0.00001007
Iteration 56/1000 | Loss: 0.00001007
Iteration 57/1000 | Loss: 0.00001006
Iteration 58/1000 | Loss: 0.00001006
Iteration 59/1000 | Loss: 0.00001006
Iteration 60/1000 | Loss: 0.00001005
Iteration 61/1000 | Loss: 0.00001004
Iteration 62/1000 | Loss: 0.00001004
Iteration 63/1000 | Loss: 0.00001004
Iteration 64/1000 | Loss: 0.00001003
Iteration 65/1000 | Loss: 0.00001003
Iteration 66/1000 | Loss: 0.00001002
Iteration 67/1000 | Loss: 0.00001002
Iteration 68/1000 | Loss: 0.00001001
Iteration 69/1000 | Loss: 0.00001001
Iteration 70/1000 | Loss: 0.00001001
Iteration 71/1000 | Loss: 0.00001001
Iteration 72/1000 | Loss: 0.00001000
Iteration 73/1000 | Loss: 0.00001000
Iteration 74/1000 | Loss: 0.00001000
Iteration 75/1000 | Loss: 0.00001000
Iteration 76/1000 | Loss: 0.00001000
Iteration 77/1000 | Loss: 0.00000999
Iteration 78/1000 | Loss: 0.00000999
Iteration 79/1000 | Loss: 0.00000999
Iteration 80/1000 | Loss: 0.00000999
Iteration 81/1000 | Loss: 0.00000999
Iteration 82/1000 | Loss: 0.00000999
Iteration 83/1000 | Loss: 0.00000999
Iteration 84/1000 | Loss: 0.00000999
Iteration 85/1000 | Loss: 0.00000999
Iteration 86/1000 | Loss: 0.00000998
Iteration 87/1000 | Loss: 0.00000998
Iteration 88/1000 | Loss: 0.00000998
Iteration 89/1000 | Loss: 0.00000998
Iteration 90/1000 | Loss: 0.00000998
Iteration 91/1000 | Loss: 0.00000998
Iteration 92/1000 | Loss: 0.00000998
Iteration 93/1000 | Loss: 0.00000998
Iteration 94/1000 | Loss: 0.00000998
Iteration 95/1000 | Loss: 0.00000998
Iteration 96/1000 | Loss: 0.00000998
Iteration 97/1000 | Loss: 0.00000998
Iteration 98/1000 | Loss: 0.00000997
Iteration 99/1000 | Loss: 0.00000997
Iteration 100/1000 | Loss: 0.00000997
Iteration 101/1000 | Loss: 0.00000997
Iteration 102/1000 | Loss: 0.00000997
Iteration 103/1000 | Loss: 0.00000997
Iteration 104/1000 | Loss: 0.00000997
Iteration 105/1000 | Loss: 0.00000996
Iteration 106/1000 | Loss: 0.00000996
Iteration 107/1000 | Loss: 0.00000995
Iteration 108/1000 | Loss: 0.00000995
Iteration 109/1000 | Loss: 0.00000995
Iteration 110/1000 | Loss: 0.00000995
Iteration 111/1000 | Loss: 0.00000995
Iteration 112/1000 | Loss: 0.00000995
Iteration 113/1000 | Loss: 0.00000995
Iteration 114/1000 | Loss: 0.00000995
Iteration 115/1000 | Loss: 0.00000995
Iteration 116/1000 | Loss: 0.00000995
Iteration 117/1000 | Loss: 0.00000995
Iteration 118/1000 | Loss: 0.00000995
Iteration 119/1000 | Loss: 0.00000995
Iteration 120/1000 | Loss: 0.00000995
Iteration 121/1000 | Loss: 0.00000994
Iteration 122/1000 | Loss: 0.00000994
Iteration 123/1000 | Loss: 0.00000994
Iteration 124/1000 | Loss: 0.00000994
Iteration 125/1000 | Loss: 0.00000994
Iteration 126/1000 | Loss: 0.00000994
Iteration 127/1000 | Loss: 0.00000994
Iteration 128/1000 | Loss: 0.00000993
Iteration 129/1000 | Loss: 0.00000993
Iteration 130/1000 | Loss: 0.00000993
Iteration 131/1000 | Loss: 0.00000993
Iteration 132/1000 | Loss: 0.00000993
Iteration 133/1000 | Loss: 0.00000993
Iteration 134/1000 | Loss: 0.00000993
Iteration 135/1000 | Loss: 0.00000993
Iteration 136/1000 | Loss: 0.00000993
Iteration 137/1000 | Loss: 0.00000993
Iteration 138/1000 | Loss: 0.00000993
Iteration 139/1000 | Loss: 0.00000993
Iteration 140/1000 | Loss: 0.00000993
Iteration 141/1000 | Loss: 0.00000993
Iteration 142/1000 | Loss: 0.00000993
Iteration 143/1000 | Loss: 0.00000993
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 143. Stopping optimization.
Last 5 losses: [9.928910913004074e-06, 9.928910913004074e-06, 9.928910913004074e-06, 9.928910913004074e-06, 9.928910913004074e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.928910913004074e-06

Optimization complete. Final v2v error: 2.70029616355896 mm

Highest mean error: 3.061737537384033 mm for frame 219

Lowest mean error: 2.426889181137085 mm for frame 244

Saving results

Total time: 40.05182886123657
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_018/1097/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_018/1097.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_018/1097
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01071281
Iteration 2/25 | Loss: 0.01071281
Iteration 3/25 | Loss: 0.01071281
Iteration 4/25 | Loss: 0.00643608
Iteration 5/25 | Loss: 0.00366439
Iteration 6/25 | Loss: 0.00330814
Iteration 7/25 | Loss: 0.00271014
Iteration 8/25 | Loss: 0.00218022
Iteration 9/25 | Loss: 0.00248983
Iteration 10/25 | Loss: 0.00163017
Iteration 11/25 | Loss: 0.00153630
Iteration 12/25 | Loss: 0.00145619
Iteration 13/25 | Loss: 0.00141292
Iteration 14/25 | Loss: 0.00143017
Iteration 15/25 | Loss: 0.00138572
Iteration 16/25 | Loss: 0.00134886
Iteration 17/25 | Loss: 0.00135144
Iteration 18/25 | Loss: 0.00133945
Iteration 19/25 | Loss: 0.00134179
Iteration 20/25 | Loss: 0.00133048
Iteration 21/25 | Loss: 0.00132888
Iteration 22/25 | Loss: 0.00132625
Iteration 23/25 | Loss: 0.00132778
Iteration 24/25 | Loss: 0.00132612
Iteration 25/25 | Loss: 0.00132545

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.76106536
Iteration 2/25 | Loss: 0.00097044
Iteration 3/25 | Loss: 0.00090901
Iteration 4/25 | Loss: 0.00089518
Iteration 5/25 | Loss: 0.00089518
Iteration 6/25 | Loss: 0.00089518
Iteration 7/25 | Loss: 0.00089518
Iteration 8/25 | Loss: 0.00089518
Iteration 9/25 | Loss: 0.00089518
Iteration 10/25 | Loss: 0.00089518
Iteration 11/25 | Loss: 0.00089518
Iteration 12/25 | Loss: 0.00089518
Iteration 13/25 | Loss: 0.00089518
Iteration 14/25 | Loss: 0.00089518
Iteration 15/25 | Loss: 0.00089518
Iteration 16/25 | Loss: 0.00089518
Iteration 17/25 | Loss: 0.00089518
Iteration 18/25 | Loss: 0.00089518
Iteration 19/25 | Loss: 0.00089518
Iteration 20/25 | Loss: 0.00089518
Iteration 21/25 | Loss: 0.00089518
Iteration 22/25 | Loss: 0.00089518
Iteration 23/25 | Loss: 0.00089518
Iteration 24/25 | Loss: 0.00089518
Iteration 25/25 | Loss: 0.00089518

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00089518
Iteration 2/1000 | Loss: 0.00023439
Iteration 3/1000 | Loss: 0.00005454
Iteration 4/1000 | Loss: 0.00003587
Iteration 5/1000 | Loss: 0.00003339
Iteration 6/1000 | Loss: 0.00003237
Iteration 7/1000 | Loss: 0.00003183
Iteration 8/1000 | Loss: 0.00003123
Iteration 9/1000 | Loss: 0.00003069
Iteration 10/1000 | Loss: 0.00003034
Iteration 11/1000 | Loss: 0.00071560
Iteration 12/1000 | Loss: 0.00164208
Iteration 13/1000 | Loss: 0.00003968
Iteration 14/1000 | Loss: 0.00002966
Iteration 15/1000 | Loss: 0.00003110
Iteration 16/1000 | Loss: 0.00005524
Iteration 17/1000 | Loss: 0.00002593
Iteration 18/1000 | Loss: 0.00002250
Iteration 19/1000 | Loss: 0.00002407
Iteration 20/1000 | Loss: 0.00002063
Iteration 21/1000 | Loss: 0.00001990
Iteration 22/1000 | Loss: 0.00004763
Iteration 23/1000 | Loss: 0.00002163
Iteration 24/1000 | Loss: 0.00001903
Iteration 25/1000 | Loss: 0.00001874
Iteration 26/1000 | Loss: 0.00001858
Iteration 27/1000 | Loss: 0.00001847
Iteration 28/1000 | Loss: 0.00001839
Iteration 29/1000 | Loss: 0.00001835
Iteration 30/1000 | Loss: 0.00001835
Iteration 31/1000 | Loss: 0.00001834
Iteration 32/1000 | Loss: 0.00001833
Iteration 33/1000 | Loss: 0.00001831
Iteration 34/1000 | Loss: 0.00001831
Iteration 35/1000 | Loss: 0.00001830
Iteration 36/1000 | Loss: 0.00001830
Iteration 37/1000 | Loss: 0.00001830
Iteration 38/1000 | Loss: 0.00001830
Iteration 39/1000 | Loss: 0.00001830
Iteration 40/1000 | Loss: 0.00001830
Iteration 41/1000 | Loss: 0.00001830
Iteration 42/1000 | Loss: 0.00001830
Iteration 43/1000 | Loss: 0.00001830
Iteration 44/1000 | Loss: 0.00001830
Iteration 45/1000 | Loss: 0.00001829
Iteration 46/1000 | Loss: 0.00001829
Iteration 47/1000 | Loss: 0.00001829
Iteration 48/1000 | Loss: 0.00001828
Iteration 49/1000 | Loss: 0.00001828
Iteration 50/1000 | Loss: 0.00001828
Iteration 51/1000 | Loss: 0.00001828
Iteration 52/1000 | Loss: 0.00001828
Iteration 53/1000 | Loss: 0.00001827
Iteration 54/1000 | Loss: 0.00001827
Iteration 55/1000 | Loss: 0.00001827
Iteration 56/1000 | Loss: 0.00001827
Iteration 57/1000 | Loss: 0.00001826
Iteration 58/1000 | Loss: 0.00001825
Iteration 59/1000 | Loss: 0.00001824
Iteration 60/1000 | Loss: 0.00001824
Iteration 61/1000 | Loss: 0.00001824
Iteration 62/1000 | Loss: 0.00001824
Iteration 63/1000 | Loss: 0.00001824
Iteration 64/1000 | Loss: 0.00001824
Iteration 65/1000 | Loss: 0.00001824
Iteration 66/1000 | Loss: 0.00001824
Iteration 67/1000 | Loss: 0.00001824
Iteration 68/1000 | Loss: 0.00001824
Iteration 69/1000 | Loss: 0.00001824
Iteration 70/1000 | Loss: 0.00001824
Iteration 71/1000 | Loss: 0.00001824
Iteration 72/1000 | Loss: 0.00001823
Iteration 73/1000 | Loss: 0.00001822
Iteration 74/1000 | Loss: 0.00001822
Iteration 75/1000 | Loss: 0.00001822
Iteration 76/1000 | Loss: 0.00001822
Iteration 77/1000 | Loss: 0.00001821
Iteration 78/1000 | Loss: 0.00001821
Iteration 79/1000 | Loss: 0.00001821
Iteration 80/1000 | Loss: 0.00001820
Iteration 81/1000 | Loss: 0.00001820
Iteration 82/1000 | Loss: 0.00001820
Iteration 83/1000 | Loss: 0.00001820
Iteration 84/1000 | Loss: 0.00001819
Iteration 85/1000 | Loss: 0.00001819
Iteration 86/1000 | Loss: 0.00001819
Iteration 87/1000 | Loss: 0.00001819
Iteration 88/1000 | Loss: 0.00001818
Iteration 89/1000 | Loss: 0.00001818
Iteration 90/1000 | Loss: 0.00001818
Iteration 91/1000 | Loss: 0.00001818
Iteration 92/1000 | Loss: 0.00001818
Iteration 93/1000 | Loss: 0.00001817
Iteration 94/1000 | Loss: 0.00001817
Iteration 95/1000 | Loss: 0.00001817
Iteration 96/1000 | Loss: 0.00001817
Iteration 97/1000 | Loss: 0.00001817
Iteration 98/1000 | Loss: 0.00001817
Iteration 99/1000 | Loss: 0.00001817
Iteration 100/1000 | Loss: 0.00001817
Iteration 101/1000 | Loss: 0.00001817
Iteration 102/1000 | Loss: 0.00001817
Iteration 103/1000 | Loss: 0.00001817
Iteration 104/1000 | Loss: 0.00001817
Iteration 105/1000 | Loss: 0.00001817
Iteration 106/1000 | Loss: 0.00001817
Iteration 107/1000 | Loss: 0.00001817
Iteration 108/1000 | Loss: 0.00001817
Iteration 109/1000 | Loss: 0.00001817
Iteration 110/1000 | Loss: 0.00001817
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 110. Stopping optimization.
Last 5 losses: [1.8173635908169672e-05, 1.8173635908169672e-05, 1.8173635908169672e-05, 1.8173635908169672e-05, 1.8173635908169672e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8173635908169672e-05

Optimization complete. Final v2v error: 3.675558567047119 mm

Highest mean error: 3.983928918838501 mm for frame 87

Lowest mean error: 3.5677590370178223 mm for frame 22

Saving results

Total time: 99.0378839969635
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_018/1062/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_018/1062.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_018/1062
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01019208
Iteration 2/25 | Loss: 0.00289699
Iteration 3/25 | Loss: 0.00178268
Iteration 4/25 | Loss: 0.00165420
Iteration 5/25 | Loss: 0.00151002
Iteration 6/25 | Loss: 0.00140884
Iteration 7/25 | Loss: 0.00134113
Iteration 8/25 | Loss: 0.00132860
Iteration 9/25 | Loss: 0.00129591
Iteration 10/25 | Loss: 0.00128182
Iteration 11/25 | Loss: 0.00125670
Iteration 12/25 | Loss: 0.00124543
Iteration 13/25 | Loss: 0.00124619
Iteration 14/25 | Loss: 0.00126035
Iteration 15/25 | Loss: 0.00130391
Iteration 16/25 | Loss: 0.00130905
Iteration 17/25 | Loss: 0.00126262
Iteration 18/25 | Loss: 0.00122426
Iteration 19/25 | Loss: 0.00120041
Iteration 20/25 | Loss: 0.00118380
Iteration 21/25 | Loss: 0.00117579
Iteration 22/25 | Loss: 0.00117493
Iteration 23/25 | Loss: 0.00117288
Iteration 24/25 | Loss: 0.00116610
Iteration 25/25 | Loss: 0.00116719

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.41494560
Iteration 2/25 | Loss: 0.00155189
Iteration 3/25 | Loss: 0.00149635
Iteration 4/25 | Loss: 0.00149634
Iteration 5/25 | Loss: 0.00149634
Iteration 6/25 | Loss: 0.00149634
Iteration 7/25 | Loss: 0.00149634
Iteration 8/25 | Loss: 0.00149634
Iteration 9/25 | Loss: 0.00149634
Iteration 10/25 | Loss: 0.00149634
Iteration 11/25 | Loss: 0.00149634
Iteration 12/25 | Loss: 0.00149634
Iteration 13/25 | Loss: 0.00149634
Iteration 14/25 | Loss: 0.00149634
Iteration 15/25 | Loss: 0.00149634
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0014963399153202772, 0.0014963399153202772, 0.0014963399153202772, 0.0014963399153202772, 0.0014963399153202772]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014963399153202772

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00149634
Iteration 2/1000 | Loss: 0.00045675
Iteration 3/1000 | Loss: 0.00031173
Iteration 4/1000 | Loss: 0.00040471
Iteration 5/1000 | Loss: 0.00048440
Iteration 6/1000 | Loss: 0.00027920
Iteration 7/1000 | Loss: 0.00017904
Iteration 8/1000 | Loss: 0.00059722
Iteration 9/1000 | Loss: 0.00053839
Iteration 10/1000 | Loss: 0.00019617
Iteration 11/1000 | Loss: 0.00052998
Iteration 12/1000 | Loss: 0.00074692
Iteration 13/1000 | Loss: 0.00061127
Iteration 14/1000 | Loss: 0.00045994
Iteration 15/1000 | Loss: 0.00038044
Iteration 16/1000 | Loss: 0.00067056
Iteration 17/1000 | Loss: 0.00067833
Iteration 18/1000 | Loss: 0.00056172
Iteration 19/1000 | Loss: 0.00046701
Iteration 20/1000 | Loss: 0.00033937
Iteration 21/1000 | Loss: 0.00086695
Iteration 22/1000 | Loss: 0.00061415
Iteration 23/1000 | Loss: 0.00092105
Iteration 24/1000 | Loss: 0.00028409
Iteration 25/1000 | Loss: 0.00043379
Iteration 26/1000 | Loss: 0.00035809
Iteration 27/1000 | Loss: 0.00022372
Iteration 28/1000 | Loss: 0.00020054
Iteration 29/1000 | Loss: 0.00047304
Iteration 30/1000 | Loss: 0.00025626
Iteration 31/1000 | Loss: 0.00012557
Iteration 32/1000 | Loss: 0.00019717
Iteration 33/1000 | Loss: 0.00025743
Iteration 34/1000 | Loss: 0.00034546
Iteration 35/1000 | Loss: 0.00029787
Iteration 36/1000 | Loss: 0.00030050
Iteration 37/1000 | Loss: 0.00039039
Iteration 38/1000 | Loss: 0.00025780
Iteration 39/1000 | Loss: 0.00027628
Iteration 40/1000 | Loss: 0.00042520
Iteration 41/1000 | Loss: 0.00050038
Iteration 42/1000 | Loss: 0.00031764
Iteration 43/1000 | Loss: 0.00007040
Iteration 44/1000 | Loss: 0.00018196
Iteration 45/1000 | Loss: 0.00009767
Iteration 46/1000 | Loss: 0.00012879
Iteration 47/1000 | Loss: 0.00016560
Iteration 48/1000 | Loss: 0.00010292
Iteration 49/1000 | Loss: 0.00010060
Iteration 50/1000 | Loss: 0.00011742
Iteration 51/1000 | Loss: 0.00018288
Iteration 52/1000 | Loss: 0.00027885
Iteration 53/1000 | Loss: 0.00023391
Iteration 54/1000 | Loss: 0.00027974
Iteration 55/1000 | Loss: 0.00061084
Iteration 56/1000 | Loss: 0.00046797
Iteration 57/1000 | Loss: 0.00041146
Iteration 58/1000 | Loss: 0.00030036
Iteration 59/1000 | Loss: 0.00069523
Iteration 60/1000 | Loss: 0.00027337
Iteration 61/1000 | Loss: 0.00032497
Iteration 62/1000 | Loss: 0.00030981
Iteration 63/1000 | Loss: 0.00022403
Iteration 64/1000 | Loss: 0.00048229
Iteration 65/1000 | Loss: 0.00023235
Iteration 66/1000 | Loss: 0.00015258
Iteration 67/1000 | Loss: 0.00024368
Iteration 68/1000 | Loss: 0.00019375
Iteration 69/1000 | Loss: 0.00053520
Iteration 70/1000 | Loss: 0.00034843
Iteration 71/1000 | Loss: 0.00034870
Iteration 72/1000 | Loss: 0.00049220
Iteration 73/1000 | Loss: 0.00020867
Iteration 74/1000 | Loss: 0.00018235
Iteration 75/1000 | Loss: 0.00013978
Iteration 76/1000 | Loss: 0.00008629
Iteration 77/1000 | Loss: 0.00021648
Iteration 78/1000 | Loss: 0.00028679
Iteration 79/1000 | Loss: 0.00035926
Iteration 80/1000 | Loss: 0.00031869
Iteration 81/1000 | Loss: 0.00037758
Iteration 82/1000 | Loss: 0.00030091
Iteration 83/1000 | Loss: 0.00012963
Iteration 84/1000 | Loss: 0.00020624
Iteration 85/1000 | Loss: 0.00019581
Iteration 86/1000 | Loss: 0.00008756
Iteration 87/1000 | Loss: 0.00006327
Iteration 88/1000 | Loss: 0.00016268
Iteration 89/1000 | Loss: 0.00022035
Iteration 90/1000 | Loss: 0.00006908
Iteration 91/1000 | Loss: 0.00014087
Iteration 92/1000 | Loss: 0.00009113
Iteration 93/1000 | Loss: 0.00008679
Iteration 94/1000 | Loss: 0.00013936
Iteration 95/1000 | Loss: 0.00013614
Iteration 96/1000 | Loss: 0.00012273
Iteration 97/1000 | Loss: 0.00029294
Iteration 98/1000 | Loss: 0.00022158
Iteration 99/1000 | Loss: 0.00017695
Iteration 100/1000 | Loss: 0.00023472
Iteration 101/1000 | Loss: 0.00018223
Iteration 102/1000 | Loss: 0.00012642
Iteration 103/1000 | Loss: 0.00013419
Iteration 104/1000 | Loss: 0.00023246
Iteration 105/1000 | Loss: 0.00021064
Iteration 106/1000 | Loss: 0.00014443
Iteration 107/1000 | Loss: 0.00016443
Iteration 108/1000 | Loss: 0.00014837
Iteration 109/1000 | Loss: 0.00012117
Iteration 110/1000 | Loss: 0.00006863
Iteration 111/1000 | Loss: 0.00011393
Iteration 112/1000 | Loss: 0.00022142
Iteration 113/1000 | Loss: 0.00025010
Iteration 114/1000 | Loss: 0.00023143
Iteration 115/1000 | Loss: 0.00014939
Iteration 116/1000 | Loss: 0.00051569
Iteration 117/1000 | Loss: 0.00033448
Iteration 118/1000 | Loss: 0.00009867
Iteration 119/1000 | Loss: 0.00004349
Iteration 120/1000 | Loss: 0.00020340
Iteration 121/1000 | Loss: 0.00052730
Iteration 122/1000 | Loss: 0.00039148
Iteration 123/1000 | Loss: 0.00041855
Iteration 124/1000 | Loss: 0.00012523
Iteration 125/1000 | Loss: 0.00015912
Iteration 126/1000 | Loss: 0.00018796
Iteration 127/1000 | Loss: 0.00014031
Iteration 128/1000 | Loss: 0.00021053
Iteration 129/1000 | Loss: 0.00020239
Iteration 130/1000 | Loss: 0.00018585
Iteration 131/1000 | Loss: 0.00014180
Iteration 132/1000 | Loss: 0.00014330
Iteration 133/1000 | Loss: 0.00019245
Iteration 134/1000 | Loss: 0.00025855
Iteration 135/1000 | Loss: 0.00023350
Iteration 136/1000 | Loss: 0.00017187
Iteration 137/1000 | Loss: 0.00025021
Iteration 138/1000 | Loss: 0.00027170
Iteration 139/1000 | Loss: 0.00022400
Iteration 140/1000 | Loss: 0.00015638
Iteration 141/1000 | Loss: 0.00019139
Iteration 142/1000 | Loss: 0.00014434
Iteration 143/1000 | Loss: 0.00021707
Iteration 144/1000 | Loss: 0.00014316
Iteration 145/1000 | Loss: 0.00024989
Iteration 146/1000 | Loss: 0.00027049
Iteration 147/1000 | Loss: 0.00021390
Iteration 148/1000 | Loss: 0.00021540
Iteration 149/1000 | Loss: 0.00023670
Iteration 150/1000 | Loss: 0.00022232
Iteration 151/1000 | Loss: 0.00013502
Iteration 152/1000 | Loss: 0.00039171
Iteration 153/1000 | Loss: 0.00017059
Iteration 154/1000 | Loss: 0.00031268
Iteration 155/1000 | Loss: 0.00031964
Iteration 156/1000 | Loss: 0.00039280
Iteration 157/1000 | Loss: 0.00028702
Iteration 158/1000 | Loss: 0.00024617
Iteration 159/1000 | Loss: 0.00032854
Iteration 160/1000 | Loss: 0.00034169
Iteration 161/1000 | Loss: 0.00020102
Iteration 162/1000 | Loss: 0.00020985
Iteration 163/1000 | Loss: 0.00015976
Iteration 164/1000 | Loss: 0.00026392
Iteration 165/1000 | Loss: 0.00023224
Iteration 166/1000 | Loss: 0.00028502
Iteration 167/1000 | Loss: 0.00035126
Iteration 168/1000 | Loss: 0.00026912
Iteration 169/1000 | Loss: 0.00032250
Iteration 170/1000 | Loss: 0.00026653
Iteration 171/1000 | Loss: 0.00060998
Iteration 172/1000 | Loss: 0.00039614
Iteration 173/1000 | Loss: 0.00009401
Iteration 174/1000 | Loss: 0.00006564
Iteration 175/1000 | Loss: 0.00008409
Iteration 176/1000 | Loss: 0.00004555
Iteration 177/1000 | Loss: 0.00013122
Iteration 178/1000 | Loss: 0.00017858
Iteration 179/1000 | Loss: 0.00009498
Iteration 180/1000 | Loss: 0.00010412
Iteration 181/1000 | Loss: 0.00009148
Iteration 182/1000 | Loss: 0.00010309
Iteration 183/1000 | Loss: 0.00007352
Iteration 184/1000 | Loss: 0.00008980
Iteration 185/1000 | Loss: 0.00009772
Iteration 186/1000 | Loss: 0.00011365
Iteration 187/1000 | Loss: 0.00007150
Iteration 188/1000 | Loss: 0.00009343
Iteration 189/1000 | Loss: 0.00008574
Iteration 190/1000 | Loss: 0.00005416
Iteration 191/1000 | Loss: 0.00005376
Iteration 192/1000 | Loss: 0.00004507
Iteration 193/1000 | Loss: 0.00005802
Iteration 194/1000 | Loss: 0.00017195
Iteration 195/1000 | Loss: 0.00011289
Iteration 196/1000 | Loss: 0.00005745
Iteration 197/1000 | Loss: 0.00012416
Iteration 198/1000 | Loss: 0.00009169
Iteration 199/1000 | Loss: 0.00009552
Iteration 200/1000 | Loss: 0.00013902
Iteration 201/1000 | Loss: 0.00011360
Iteration 202/1000 | Loss: 0.00011030
Iteration 203/1000 | Loss: 0.00023125
Iteration 204/1000 | Loss: 0.00013772
Iteration 205/1000 | Loss: 0.00011657
Iteration 206/1000 | Loss: 0.00013934
Iteration 207/1000 | Loss: 0.00013180
Iteration 208/1000 | Loss: 0.00022565
Iteration 209/1000 | Loss: 0.00013273
Iteration 210/1000 | Loss: 0.00010991
Iteration 211/1000 | Loss: 0.00022086
Iteration 212/1000 | Loss: 0.00012539
Iteration 213/1000 | Loss: 0.00022075
Iteration 214/1000 | Loss: 0.00014521
Iteration 215/1000 | Loss: 0.00016533
Iteration 216/1000 | Loss: 0.00013259
Iteration 217/1000 | Loss: 0.00013471
Iteration 218/1000 | Loss: 0.00002532
Iteration 219/1000 | Loss: 0.00011183
Iteration 220/1000 | Loss: 0.00014762
Iteration 221/1000 | Loss: 0.00005036
Iteration 222/1000 | Loss: 0.00003258
Iteration 223/1000 | Loss: 0.00002962
Iteration 224/1000 | Loss: 0.00003248
Iteration 225/1000 | Loss: 0.00004869
Iteration 226/1000 | Loss: 0.00007934
Iteration 227/1000 | Loss: 0.00003275
Iteration 228/1000 | Loss: 0.00003050
Iteration 229/1000 | Loss: 0.00002963
Iteration 230/1000 | Loss: 0.00005027
Iteration 231/1000 | Loss: 0.00002794
Iteration 232/1000 | Loss: 0.00007680
Iteration 233/1000 | Loss: 0.00003143
Iteration 234/1000 | Loss: 0.00004856
Iteration 235/1000 | Loss: 0.00003021
Iteration 236/1000 | Loss: 0.00026584
Iteration 237/1000 | Loss: 0.00049471
Iteration 238/1000 | Loss: 0.00008728
Iteration 239/1000 | Loss: 0.00012310
Iteration 240/1000 | Loss: 0.00010740
Iteration 241/1000 | Loss: 0.00001994
Iteration 242/1000 | Loss: 0.00003445
Iteration 243/1000 | Loss: 0.00001777
Iteration 244/1000 | Loss: 0.00004600
Iteration 245/1000 | Loss: 0.00008305
Iteration 246/1000 | Loss: 0.00004943
Iteration 247/1000 | Loss: 0.00011817
Iteration 248/1000 | Loss: 0.00021721
Iteration 249/1000 | Loss: 0.00002303
Iteration 250/1000 | Loss: 0.00003802
Iteration 251/1000 | Loss: 0.00004078
Iteration 252/1000 | Loss: 0.00004174
Iteration 253/1000 | Loss: 0.00010177
Iteration 254/1000 | Loss: 0.00005245
Iteration 255/1000 | Loss: 0.00001788
Iteration 256/1000 | Loss: 0.00001700
Iteration 257/1000 | Loss: 0.00001675
Iteration 258/1000 | Loss: 0.00001654
Iteration 259/1000 | Loss: 0.00001638
Iteration 260/1000 | Loss: 0.00001872
Iteration 261/1000 | Loss: 0.00001634
Iteration 262/1000 | Loss: 0.00001632
Iteration 263/1000 | Loss: 0.00001631
Iteration 264/1000 | Loss: 0.00001629
Iteration 265/1000 | Loss: 0.00001629
Iteration 266/1000 | Loss: 0.00001629
Iteration 267/1000 | Loss: 0.00001626
Iteration 268/1000 | Loss: 0.00001625
Iteration 269/1000 | Loss: 0.00001625
Iteration 270/1000 | Loss: 0.00008491
Iteration 271/1000 | Loss: 0.00012491
Iteration 272/1000 | Loss: 0.00004614
Iteration 273/1000 | Loss: 0.00001630
Iteration 274/1000 | Loss: 0.00008488
Iteration 275/1000 | Loss: 0.00021492
Iteration 276/1000 | Loss: 0.00009573
Iteration 277/1000 | Loss: 0.00019622
Iteration 278/1000 | Loss: 0.00010136
Iteration 279/1000 | Loss: 0.00015957
Iteration 280/1000 | Loss: 0.00009434
Iteration 281/1000 | Loss: 0.00020630
Iteration 282/1000 | Loss: 0.00022955
Iteration 283/1000 | Loss: 0.00015009
Iteration 284/1000 | Loss: 0.00003322
Iteration 285/1000 | Loss: 0.00007688
Iteration 286/1000 | Loss: 0.00003328
Iteration 287/1000 | Loss: 0.00016353
Iteration 288/1000 | Loss: 0.00015618
Iteration 289/1000 | Loss: 0.00004427
Iteration 290/1000 | Loss: 0.00001683
Iteration 291/1000 | Loss: 0.00005568
Iteration 292/1000 | Loss: 0.00004004
Iteration 293/1000 | Loss: 0.00005457
Iteration 294/1000 | Loss: 0.00003782
Iteration 295/1000 | Loss: 0.00005173
Iteration 296/1000 | Loss: 0.00001885
Iteration 297/1000 | Loss: 0.00003659
Iteration 298/1000 | Loss: 0.00004353
Iteration 299/1000 | Loss: 0.00003478
Iteration 300/1000 | Loss: 0.00001622
Iteration 301/1000 | Loss: 0.00001610
Iteration 302/1000 | Loss: 0.00001606
Iteration 303/1000 | Loss: 0.00001606
Iteration 304/1000 | Loss: 0.00001605
Iteration 305/1000 | Loss: 0.00001605
Iteration 306/1000 | Loss: 0.00001605
Iteration 307/1000 | Loss: 0.00001605
Iteration 308/1000 | Loss: 0.00001604
Iteration 309/1000 | Loss: 0.00001604
Iteration 310/1000 | Loss: 0.00001604
Iteration 311/1000 | Loss: 0.00001600
Iteration 312/1000 | Loss: 0.00001600
Iteration 313/1000 | Loss: 0.00001600
Iteration 314/1000 | Loss: 0.00001600
Iteration 315/1000 | Loss: 0.00001600
Iteration 316/1000 | Loss: 0.00001600
Iteration 317/1000 | Loss: 0.00001600
Iteration 318/1000 | Loss: 0.00001600
Iteration 319/1000 | Loss: 0.00001599
Iteration 320/1000 | Loss: 0.00001599
Iteration 321/1000 | Loss: 0.00001599
Iteration 322/1000 | Loss: 0.00001599
Iteration 323/1000 | Loss: 0.00001599
Iteration 324/1000 | Loss: 0.00001599
Iteration 325/1000 | Loss: 0.00001598
Iteration 326/1000 | Loss: 0.00001598
Iteration 327/1000 | Loss: 0.00001598
Iteration 328/1000 | Loss: 0.00001598
Iteration 329/1000 | Loss: 0.00001598
Iteration 330/1000 | Loss: 0.00001598
Iteration 331/1000 | Loss: 0.00001598
Iteration 332/1000 | Loss: 0.00001598
Iteration 333/1000 | Loss: 0.00001598
Iteration 334/1000 | Loss: 0.00001598
Iteration 335/1000 | Loss: 0.00001598
Iteration 336/1000 | Loss: 0.00001597
Iteration 337/1000 | Loss: 0.00001597
Iteration 338/1000 | Loss: 0.00001597
Iteration 339/1000 | Loss: 0.00001597
Iteration 340/1000 | Loss: 0.00001597
Iteration 341/1000 | Loss: 0.00001597
Iteration 342/1000 | Loss: 0.00001597
Iteration 343/1000 | Loss: 0.00001597
Iteration 344/1000 | Loss: 0.00001597
Iteration 345/1000 | Loss: 0.00001597
Iteration 346/1000 | Loss: 0.00001597
Iteration 347/1000 | Loss: 0.00001597
Iteration 348/1000 | Loss: 0.00001597
Iteration 349/1000 | Loss: 0.00001597
Iteration 350/1000 | Loss: 0.00001597
Iteration 351/1000 | Loss: 0.00001597
Iteration 352/1000 | Loss: 0.00001597
Iteration 353/1000 | Loss: 0.00001597
Iteration 354/1000 | Loss: 0.00001597
Iteration 355/1000 | Loss: 0.00001596
Iteration 356/1000 | Loss: 0.00001596
Iteration 357/1000 | Loss: 0.00001596
Iteration 358/1000 | Loss: 0.00001596
Iteration 359/1000 | Loss: 0.00008027
Iteration 360/1000 | Loss: 0.00008156
Iteration 361/1000 | Loss: 0.00003354
Iteration 362/1000 | Loss: 0.00001603
Iteration 363/1000 | Loss: 0.00001596
Iteration 364/1000 | Loss: 0.00001594
Iteration 365/1000 | Loss: 0.00001593
Iteration 366/1000 | Loss: 0.00001593
Iteration 367/1000 | Loss: 0.00001592
Iteration 368/1000 | Loss: 0.00001592
Iteration 369/1000 | Loss: 0.00007992
Iteration 370/1000 | Loss: 0.00009707
Iteration 371/1000 | Loss: 0.00005001
Iteration 372/1000 | Loss: 0.00002375
Iteration 373/1000 | Loss: 0.00011418
Iteration 374/1000 | Loss: 0.00010024
Iteration 375/1000 | Loss: 0.00006399
Iteration 376/1000 | Loss: 0.00001613
Iteration 377/1000 | Loss: 0.00001592
Iteration 378/1000 | Loss: 0.00001590
Iteration 379/1000 | Loss: 0.00007833
Iteration 380/1000 | Loss: 0.00004054
Iteration 381/1000 | Loss: 0.00007107
Iteration 382/1000 | Loss: 0.00017855
Iteration 383/1000 | Loss: 0.00006308
Iteration 384/1000 | Loss: 0.00016540
Iteration 385/1000 | Loss: 0.00008554
Iteration 386/1000 | Loss: 0.00037808
Iteration 387/1000 | Loss: 0.00024113
Iteration 388/1000 | Loss: 0.00001778
Iteration 389/1000 | Loss: 0.00005936
Iteration 390/1000 | Loss: 0.00004333
Iteration 391/1000 | Loss: 0.00005338
Iteration 392/1000 | Loss: 0.00004394
Iteration 393/1000 | Loss: 0.00005362
Iteration 394/1000 | Loss: 0.00011513
Iteration 395/1000 | Loss: 0.00004706
Iteration 396/1000 | Loss: 0.00009340
Iteration 397/1000 | Loss: 0.00009150
Iteration 398/1000 | Loss: 0.00001800
Iteration 399/1000 | Loss: 0.00001628
Iteration 400/1000 | Loss: 0.00001600
Iteration 401/1000 | Loss: 0.00008817
Iteration 402/1000 | Loss: 0.00004626
Iteration 403/1000 | Loss: 0.00001590
Iteration 404/1000 | Loss: 0.00008110
Iteration 405/1000 | Loss: 0.00002310
Iteration 406/1000 | Loss: 0.00005283
Iteration 407/1000 | Loss: 0.00025400
Iteration 408/1000 | Loss: 0.00008208
Iteration 409/1000 | Loss: 0.00007582
Iteration 410/1000 | Loss: 0.00004679
Iteration 411/1000 | Loss: 0.00001677
Iteration 412/1000 | Loss: 0.00001619
Iteration 413/1000 | Loss: 0.00001585
Iteration 414/1000 | Loss: 0.00001584
Iteration 415/1000 | Loss: 0.00001672
Iteration 416/1000 | Loss: 0.00001636
Iteration 417/1000 | Loss: 0.00001714
Iteration 418/1000 | Loss: 0.00001579
Iteration 419/1000 | Loss: 0.00001579
Iteration 420/1000 | Loss: 0.00001579
Iteration 421/1000 | Loss: 0.00001579
Iteration 422/1000 | Loss: 0.00001579
Iteration 423/1000 | Loss: 0.00001579
Iteration 424/1000 | Loss: 0.00001579
Iteration 425/1000 | Loss: 0.00001579
Iteration 426/1000 | Loss: 0.00001579
Iteration 427/1000 | Loss: 0.00001579
Iteration 428/1000 | Loss: 0.00001579
Iteration 429/1000 | Loss: 0.00001579
Iteration 430/1000 | Loss: 0.00001579
Iteration 431/1000 | Loss: 0.00001579
Iteration 432/1000 | Loss: 0.00001579
Iteration 433/1000 | Loss: 0.00001579
Iteration 434/1000 | Loss: 0.00001579
Iteration 435/1000 | Loss: 0.00001579
Iteration 436/1000 | Loss: 0.00001578
Iteration 437/1000 | Loss: 0.00001578
Iteration 438/1000 | Loss: 0.00001578
Iteration 439/1000 | Loss: 0.00001578
Iteration 440/1000 | Loss: 0.00001578
Iteration 441/1000 | Loss: 0.00001578
Iteration 442/1000 | Loss: 0.00001578
Iteration 443/1000 | Loss: 0.00001578
Iteration 444/1000 | Loss: 0.00001578
Iteration 445/1000 | Loss: 0.00001578
Iteration 446/1000 | Loss: 0.00001578
Iteration 447/1000 | Loss: 0.00001578
Iteration 448/1000 | Loss: 0.00001578
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 448. Stopping optimization.
Last 5 losses: [1.5782463378855027e-05, 1.5782463378855027e-05, 1.5782463378855027e-05, 1.5782463378855027e-05, 1.5782463378855027e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5782463378855027e-05

Optimization complete. Final v2v error: 3.1906204223632812 mm

Highest mean error: 7.625682830810547 mm for frame 71

Lowest mean error: 2.4514167308807373 mm for frame 164

Saving results

Total time: 600.6174156665802
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_018/1050/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_018/1050.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_018/1050
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00816735
Iteration 2/25 | Loss: 0.00141014
Iteration 3/25 | Loss: 0.00125396
Iteration 4/25 | Loss: 0.00120353
Iteration 5/25 | Loss: 0.00119478
Iteration 6/25 | Loss: 0.00119252
Iteration 7/25 | Loss: 0.00119189
Iteration 8/25 | Loss: 0.00119171
Iteration 9/25 | Loss: 0.00119162
Iteration 10/25 | Loss: 0.00119161
Iteration 11/25 | Loss: 0.00119161
Iteration 12/25 | Loss: 0.00119161
Iteration 13/25 | Loss: 0.00119161
Iteration 14/25 | Loss: 0.00119161
Iteration 15/25 | Loss: 0.00119161
Iteration 16/25 | Loss: 0.00119161
Iteration 17/25 | Loss: 0.00119161
Iteration 18/25 | Loss: 0.00119161
Iteration 19/25 | Loss: 0.00119161
Iteration 20/25 | Loss: 0.00119160
Iteration 21/25 | Loss: 0.00119160
Iteration 22/25 | Loss: 0.00119160
Iteration 23/25 | Loss: 0.00119160
Iteration 24/25 | Loss: 0.00119160
Iteration 25/25 | Loss: 0.00119160

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.92056179
Iteration 2/25 | Loss: 0.00105703
Iteration 3/25 | Loss: 0.00105698
Iteration 4/25 | Loss: 0.00105698
Iteration 5/25 | Loss: 0.00105698
Iteration 6/25 | Loss: 0.00105698
Iteration 7/25 | Loss: 0.00105698
Iteration 8/25 | Loss: 0.00105698
Iteration 9/25 | Loss: 0.00105698
Iteration 10/25 | Loss: 0.00105698
Iteration 11/25 | Loss: 0.00105698
Iteration 12/25 | Loss: 0.00105698
Iteration 13/25 | Loss: 0.00105698
Iteration 14/25 | Loss: 0.00105698
Iteration 15/25 | Loss: 0.00105698
Iteration 16/25 | Loss: 0.00105698
Iteration 17/25 | Loss: 0.00105698
Iteration 18/25 | Loss: 0.00105698
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0010569766163825989, 0.0010569766163825989, 0.0010569766163825989, 0.0010569766163825989, 0.0010569766163825989]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010569766163825989

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00105698
Iteration 2/1000 | Loss: 0.00004359
Iteration 3/1000 | Loss: 0.00002792
Iteration 4/1000 | Loss: 0.00002324
Iteration 5/1000 | Loss: 0.00016018
Iteration 6/1000 | Loss: 0.00002054
Iteration 7/1000 | Loss: 0.00001953
Iteration 8/1000 | Loss: 0.00001881
Iteration 9/1000 | Loss: 0.00001843
Iteration 10/1000 | Loss: 0.00001802
Iteration 11/1000 | Loss: 0.00001776
Iteration 12/1000 | Loss: 0.00001754
Iteration 13/1000 | Loss: 0.00001751
Iteration 14/1000 | Loss: 0.00001743
Iteration 15/1000 | Loss: 0.00001733
Iteration 16/1000 | Loss: 0.00001723
Iteration 17/1000 | Loss: 0.00001720
Iteration 18/1000 | Loss: 0.00001719
Iteration 19/1000 | Loss: 0.00001717
Iteration 20/1000 | Loss: 0.00001715
Iteration 21/1000 | Loss: 0.00001715
Iteration 22/1000 | Loss: 0.00001714
Iteration 23/1000 | Loss: 0.00001712
Iteration 24/1000 | Loss: 0.00001699
Iteration 25/1000 | Loss: 0.00001699
Iteration 26/1000 | Loss: 0.00001698
Iteration 27/1000 | Loss: 0.00001697
Iteration 28/1000 | Loss: 0.00001696
Iteration 29/1000 | Loss: 0.00001695
Iteration 30/1000 | Loss: 0.00001692
Iteration 31/1000 | Loss: 0.00001692
Iteration 32/1000 | Loss: 0.00001692
Iteration 33/1000 | Loss: 0.00001692
Iteration 34/1000 | Loss: 0.00001692
Iteration 35/1000 | Loss: 0.00001692
Iteration 36/1000 | Loss: 0.00001692
Iteration 37/1000 | Loss: 0.00001692
Iteration 38/1000 | Loss: 0.00001691
Iteration 39/1000 | Loss: 0.00001691
Iteration 40/1000 | Loss: 0.00001690
Iteration 41/1000 | Loss: 0.00001689
Iteration 42/1000 | Loss: 0.00001689
Iteration 43/1000 | Loss: 0.00001689
Iteration 44/1000 | Loss: 0.00001688
Iteration 45/1000 | Loss: 0.00001688
Iteration 46/1000 | Loss: 0.00001688
Iteration 47/1000 | Loss: 0.00001687
Iteration 48/1000 | Loss: 0.00001687
Iteration 49/1000 | Loss: 0.00001687
Iteration 50/1000 | Loss: 0.00001686
Iteration 51/1000 | Loss: 0.00001686
Iteration 52/1000 | Loss: 0.00001686
Iteration 53/1000 | Loss: 0.00001686
Iteration 54/1000 | Loss: 0.00001686
Iteration 55/1000 | Loss: 0.00001685
Iteration 56/1000 | Loss: 0.00001685
Iteration 57/1000 | Loss: 0.00001685
Iteration 58/1000 | Loss: 0.00001685
Iteration 59/1000 | Loss: 0.00001685
Iteration 60/1000 | Loss: 0.00001684
Iteration 61/1000 | Loss: 0.00001684
Iteration 62/1000 | Loss: 0.00001684
Iteration 63/1000 | Loss: 0.00001683
Iteration 64/1000 | Loss: 0.00001683
Iteration 65/1000 | Loss: 0.00001683
Iteration 66/1000 | Loss: 0.00001682
Iteration 67/1000 | Loss: 0.00001682
Iteration 68/1000 | Loss: 0.00001682
Iteration 69/1000 | Loss: 0.00001681
Iteration 70/1000 | Loss: 0.00001681
Iteration 71/1000 | Loss: 0.00001681
Iteration 72/1000 | Loss: 0.00001680
Iteration 73/1000 | Loss: 0.00001680
Iteration 74/1000 | Loss: 0.00001679
Iteration 75/1000 | Loss: 0.00001679
Iteration 76/1000 | Loss: 0.00001679
Iteration 77/1000 | Loss: 0.00001679
Iteration 78/1000 | Loss: 0.00001678
Iteration 79/1000 | Loss: 0.00001678
Iteration 80/1000 | Loss: 0.00001677
Iteration 81/1000 | Loss: 0.00001677
Iteration 82/1000 | Loss: 0.00001676
Iteration 83/1000 | Loss: 0.00001676
Iteration 84/1000 | Loss: 0.00001676
Iteration 85/1000 | Loss: 0.00001676
Iteration 86/1000 | Loss: 0.00001675
Iteration 87/1000 | Loss: 0.00001674
Iteration 88/1000 | Loss: 0.00001674
Iteration 89/1000 | Loss: 0.00001674
Iteration 90/1000 | Loss: 0.00001674
Iteration 91/1000 | Loss: 0.00001673
Iteration 92/1000 | Loss: 0.00001673
Iteration 93/1000 | Loss: 0.00001673
Iteration 94/1000 | Loss: 0.00001672
Iteration 95/1000 | Loss: 0.00001672
Iteration 96/1000 | Loss: 0.00001672
Iteration 97/1000 | Loss: 0.00001671
Iteration 98/1000 | Loss: 0.00001671
Iteration 99/1000 | Loss: 0.00001671
Iteration 100/1000 | Loss: 0.00001671
Iteration 101/1000 | Loss: 0.00001670
Iteration 102/1000 | Loss: 0.00001670
Iteration 103/1000 | Loss: 0.00001670
Iteration 104/1000 | Loss: 0.00001670
Iteration 105/1000 | Loss: 0.00001670
Iteration 106/1000 | Loss: 0.00001669
Iteration 107/1000 | Loss: 0.00001669
Iteration 108/1000 | Loss: 0.00001669
Iteration 109/1000 | Loss: 0.00001669
Iteration 110/1000 | Loss: 0.00001668
Iteration 111/1000 | Loss: 0.00001668
Iteration 112/1000 | Loss: 0.00001668
Iteration 113/1000 | Loss: 0.00001667
Iteration 114/1000 | Loss: 0.00001666
Iteration 115/1000 | Loss: 0.00001666
Iteration 116/1000 | Loss: 0.00001666
Iteration 117/1000 | Loss: 0.00001666
Iteration 118/1000 | Loss: 0.00001666
Iteration 119/1000 | Loss: 0.00001665
Iteration 120/1000 | Loss: 0.00001665
Iteration 121/1000 | Loss: 0.00001665
Iteration 122/1000 | Loss: 0.00001665
Iteration 123/1000 | Loss: 0.00001664
Iteration 124/1000 | Loss: 0.00001664
Iteration 125/1000 | Loss: 0.00001664
Iteration 126/1000 | Loss: 0.00001664
Iteration 127/1000 | Loss: 0.00001664
Iteration 128/1000 | Loss: 0.00001664
Iteration 129/1000 | Loss: 0.00001663
Iteration 130/1000 | Loss: 0.00001663
Iteration 131/1000 | Loss: 0.00001663
Iteration 132/1000 | Loss: 0.00001663
Iteration 133/1000 | Loss: 0.00001663
Iteration 134/1000 | Loss: 0.00001663
Iteration 135/1000 | Loss: 0.00001662
Iteration 136/1000 | Loss: 0.00001662
Iteration 137/1000 | Loss: 0.00001662
Iteration 138/1000 | Loss: 0.00001662
Iteration 139/1000 | Loss: 0.00001662
Iteration 140/1000 | Loss: 0.00001662
Iteration 141/1000 | Loss: 0.00001662
Iteration 142/1000 | Loss: 0.00001661
Iteration 143/1000 | Loss: 0.00001661
Iteration 144/1000 | Loss: 0.00001661
Iteration 145/1000 | Loss: 0.00001661
Iteration 146/1000 | Loss: 0.00001661
Iteration 147/1000 | Loss: 0.00001661
Iteration 148/1000 | Loss: 0.00001661
Iteration 149/1000 | Loss: 0.00001661
Iteration 150/1000 | Loss: 0.00001661
Iteration 151/1000 | Loss: 0.00001661
Iteration 152/1000 | Loss: 0.00001661
Iteration 153/1000 | Loss: 0.00001660
Iteration 154/1000 | Loss: 0.00001660
Iteration 155/1000 | Loss: 0.00001660
Iteration 156/1000 | Loss: 0.00001660
Iteration 157/1000 | Loss: 0.00001660
Iteration 158/1000 | Loss: 0.00001660
Iteration 159/1000 | Loss: 0.00001660
Iteration 160/1000 | Loss: 0.00001660
Iteration 161/1000 | Loss: 0.00001659
Iteration 162/1000 | Loss: 0.00001659
Iteration 163/1000 | Loss: 0.00001659
Iteration 164/1000 | Loss: 0.00001659
Iteration 165/1000 | Loss: 0.00001659
Iteration 166/1000 | Loss: 0.00001659
Iteration 167/1000 | Loss: 0.00001659
Iteration 168/1000 | Loss: 0.00001659
Iteration 169/1000 | Loss: 0.00001659
Iteration 170/1000 | Loss: 0.00001659
Iteration 171/1000 | Loss: 0.00001659
Iteration 172/1000 | Loss: 0.00001659
Iteration 173/1000 | Loss: 0.00001659
Iteration 174/1000 | Loss: 0.00001658
Iteration 175/1000 | Loss: 0.00001658
Iteration 176/1000 | Loss: 0.00001658
Iteration 177/1000 | Loss: 0.00001658
Iteration 178/1000 | Loss: 0.00001658
Iteration 179/1000 | Loss: 0.00001658
Iteration 180/1000 | Loss: 0.00001658
Iteration 181/1000 | Loss: 0.00001658
Iteration 182/1000 | Loss: 0.00001658
Iteration 183/1000 | Loss: 0.00001658
Iteration 184/1000 | Loss: 0.00001658
Iteration 185/1000 | Loss: 0.00001658
Iteration 186/1000 | Loss: 0.00001657
Iteration 187/1000 | Loss: 0.00001657
Iteration 188/1000 | Loss: 0.00001657
Iteration 189/1000 | Loss: 0.00001657
Iteration 190/1000 | Loss: 0.00001657
Iteration 191/1000 | Loss: 0.00001657
Iteration 192/1000 | Loss: 0.00001657
Iteration 193/1000 | Loss: 0.00001657
Iteration 194/1000 | Loss: 0.00001657
Iteration 195/1000 | Loss: 0.00001657
Iteration 196/1000 | Loss: 0.00001657
Iteration 197/1000 | Loss: 0.00001657
Iteration 198/1000 | Loss: 0.00001657
Iteration 199/1000 | Loss: 0.00001657
Iteration 200/1000 | Loss: 0.00001657
Iteration 201/1000 | Loss: 0.00001657
Iteration 202/1000 | Loss: 0.00001657
Iteration 203/1000 | Loss: 0.00001657
Iteration 204/1000 | Loss: 0.00001657
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 204. Stopping optimization.
Last 5 losses: [1.6571404557907954e-05, 1.6571404557907954e-05, 1.6571404557907954e-05, 1.6571404557907954e-05, 1.6571404557907954e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6571404557907954e-05

Optimization complete. Final v2v error: 3.3654556274414062 mm

Highest mean error: 4.176608562469482 mm for frame 56

Lowest mean error: 2.767205238342285 mm for frame 109

Saving results

Total time: 51.070290327072144
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_018/1080/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_018/1080.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_018/1080
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00799114
Iteration 2/25 | Loss: 0.00156079
Iteration 3/25 | Loss: 0.00137319
Iteration 4/25 | Loss: 0.00136413
Iteration 5/25 | Loss: 0.00136277
Iteration 6/25 | Loss: 0.00136277
Iteration 7/25 | Loss: 0.00136277
Iteration 8/25 | Loss: 0.00136277
Iteration 9/25 | Loss: 0.00136277
Iteration 10/25 | Loss: 0.00136277
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0013627749867737293, 0.0013627749867737293, 0.0013627749867737293, 0.0013627749867737293, 0.0013627749867737293]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013627749867737293

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 8.00314713
Iteration 2/25 | Loss: 0.00123164
Iteration 3/25 | Loss: 0.00123164
Iteration 4/25 | Loss: 0.00123164
Iteration 5/25 | Loss: 0.00123164
Iteration 6/25 | Loss: 0.00123164
Iteration 7/25 | Loss: 0.00123164
Iteration 8/25 | Loss: 0.00123164
Iteration 9/25 | Loss: 0.00123164
Iteration 10/25 | Loss: 0.00123164
Iteration 11/25 | Loss: 0.00123164
Iteration 12/25 | Loss: 0.00123164
Iteration 13/25 | Loss: 0.00123164
Iteration 14/25 | Loss: 0.00123164
Iteration 15/25 | Loss: 0.00123164
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0012316367356106639, 0.0012316367356106639, 0.0012316367356106639, 0.0012316367356106639, 0.0012316367356106639]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012316367356106639

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00123164
Iteration 2/1000 | Loss: 0.00005264
Iteration 3/1000 | Loss: 0.00003389
Iteration 4/1000 | Loss: 0.00002746
Iteration 5/1000 | Loss: 0.00002574
Iteration 6/1000 | Loss: 0.00002455
Iteration 7/1000 | Loss: 0.00002386
Iteration 8/1000 | Loss: 0.00002343
Iteration 9/1000 | Loss: 0.00002319
Iteration 10/1000 | Loss: 0.00002298
Iteration 11/1000 | Loss: 0.00002282
Iteration 12/1000 | Loss: 0.00002273
Iteration 13/1000 | Loss: 0.00002271
Iteration 14/1000 | Loss: 0.00002271
Iteration 15/1000 | Loss: 0.00002264
Iteration 16/1000 | Loss: 0.00002259
Iteration 17/1000 | Loss: 0.00002256
Iteration 18/1000 | Loss: 0.00002253
Iteration 19/1000 | Loss: 0.00002252
Iteration 20/1000 | Loss: 0.00002251
Iteration 21/1000 | Loss: 0.00002251
Iteration 22/1000 | Loss: 0.00002250
Iteration 23/1000 | Loss: 0.00002248
Iteration 24/1000 | Loss: 0.00002248
Iteration 25/1000 | Loss: 0.00002248
Iteration 26/1000 | Loss: 0.00002248
Iteration 27/1000 | Loss: 0.00002248
Iteration 28/1000 | Loss: 0.00002247
Iteration 29/1000 | Loss: 0.00002247
Iteration 30/1000 | Loss: 0.00002246
Iteration 31/1000 | Loss: 0.00002245
Iteration 32/1000 | Loss: 0.00002244
Iteration 33/1000 | Loss: 0.00002244
Iteration 34/1000 | Loss: 0.00002244
Iteration 35/1000 | Loss: 0.00002243
Iteration 36/1000 | Loss: 0.00002243
Iteration 37/1000 | Loss: 0.00002243
Iteration 38/1000 | Loss: 0.00002242
Iteration 39/1000 | Loss: 0.00002242
Iteration 40/1000 | Loss: 0.00002242
Iteration 41/1000 | Loss: 0.00002242
Iteration 42/1000 | Loss: 0.00002242
Iteration 43/1000 | Loss: 0.00002241
Iteration 44/1000 | Loss: 0.00002241
Iteration 45/1000 | Loss: 0.00002241
Iteration 46/1000 | Loss: 0.00002241
Iteration 47/1000 | Loss: 0.00002240
Iteration 48/1000 | Loss: 0.00002240
Iteration 49/1000 | Loss: 0.00002240
Iteration 50/1000 | Loss: 0.00002240
Iteration 51/1000 | Loss: 0.00002240
Iteration 52/1000 | Loss: 0.00002240
Iteration 53/1000 | Loss: 0.00002239
Iteration 54/1000 | Loss: 0.00002239
Iteration 55/1000 | Loss: 0.00002239
Iteration 56/1000 | Loss: 0.00002239
Iteration 57/1000 | Loss: 0.00002239
Iteration 58/1000 | Loss: 0.00002239
Iteration 59/1000 | Loss: 0.00002239
Iteration 60/1000 | Loss: 0.00002239
Iteration 61/1000 | Loss: 0.00002239
Iteration 62/1000 | Loss: 0.00002239
Iteration 63/1000 | Loss: 0.00002239
Iteration 64/1000 | Loss: 0.00002239
Iteration 65/1000 | Loss: 0.00002238
Iteration 66/1000 | Loss: 0.00002238
Iteration 67/1000 | Loss: 0.00002238
Iteration 68/1000 | Loss: 0.00002238
Iteration 69/1000 | Loss: 0.00002238
Iteration 70/1000 | Loss: 0.00002238
Iteration 71/1000 | Loss: 0.00002238
Iteration 72/1000 | Loss: 0.00002238
Iteration 73/1000 | Loss: 0.00002238
Iteration 74/1000 | Loss: 0.00002238
Iteration 75/1000 | Loss: 0.00002237
Iteration 76/1000 | Loss: 0.00002237
Iteration 77/1000 | Loss: 0.00002237
Iteration 78/1000 | Loss: 0.00002237
Iteration 79/1000 | Loss: 0.00002237
Iteration 80/1000 | Loss: 0.00002237
Iteration 81/1000 | Loss: 0.00002237
Iteration 82/1000 | Loss: 0.00002237
Iteration 83/1000 | Loss: 0.00002237
Iteration 84/1000 | Loss: 0.00002237
Iteration 85/1000 | Loss: 0.00002237
Iteration 86/1000 | Loss: 0.00002237
Iteration 87/1000 | Loss: 0.00002237
Iteration 88/1000 | Loss: 0.00002237
Iteration 89/1000 | Loss: 0.00002236
Iteration 90/1000 | Loss: 0.00002236
Iteration 91/1000 | Loss: 0.00002236
Iteration 92/1000 | Loss: 0.00002236
Iteration 93/1000 | Loss: 0.00002236
Iteration 94/1000 | Loss: 0.00002236
Iteration 95/1000 | Loss: 0.00002236
Iteration 96/1000 | Loss: 0.00002236
Iteration 97/1000 | Loss: 0.00002236
Iteration 98/1000 | Loss: 0.00002236
Iteration 99/1000 | Loss: 0.00002236
Iteration 100/1000 | Loss: 0.00002236
Iteration 101/1000 | Loss: 0.00002236
Iteration 102/1000 | Loss: 0.00002236
Iteration 103/1000 | Loss: 0.00002236
Iteration 104/1000 | Loss: 0.00002235
Iteration 105/1000 | Loss: 0.00002235
Iteration 106/1000 | Loss: 0.00002235
Iteration 107/1000 | Loss: 0.00002235
Iteration 108/1000 | Loss: 0.00002235
Iteration 109/1000 | Loss: 0.00002235
Iteration 110/1000 | Loss: 0.00002234
Iteration 111/1000 | Loss: 0.00002234
Iteration 112/1000 | Loss: 0.00002234
Iteration 113/1000 | Loss: 0.00002234
Iteration 114/1000 | Loss: 0.00002234
Iteration 115/1000 | Loss: 0.00002234
Iteration 116/1000 | Loss: 0.00002234
Iteration 117/1000 | Loss: 0.00002234
Iteration 118/1000 | Loss: 0.00002233
Iteration 119/1000 | Loss: 0.00002233
Iteration 120/1000 | Loss: 0.00002233
Iteration 121/1000 | Loss: 0.00002233
Iteration 122/1000 | Loss: 0.00002233
Iteration 123/1000 | Loss: 0.00002233
Iteration 124/1000 | Loss: 0.00002233
Iteration 125/1000 | Loss: 0.00002232
Iteration 126/1000 | Loss: 0.00002232
Iteration 127/1000 | Loss: 0.00002232
Iteration 128/1000 | Loss: 0.00002232
Iteration 129/1000 | Loss: 0.00002232
Iteration 130/1000 | Loss: 0.00002232
Iteration 131/1000 | Loss: 0.00002232
Iteration 132/1000 | Loss: 0.00002232
Iteration 133/1000 | Loss: 0.00002232
Iteration 134/1000 | Loss: 0.00002232
Iteration 135/1000 | Loss: 0.00002232
Iteration 136/1000 | Loss: 0.00002232
Iteration 137/1000 | Loss: 0.00002232
Iteration 138/1000 | Loss: 0.00002232
Iteration 139/1000 | Loss: 0.00002231
Iteration 140/1000 | Loss: 0.00002231
Iteration 141/1000 | Loss: 0.00002231
Iteration 142/1000 | Loss: 0.00002231
Iteration 143/1000 | Loss: 0.00002230
Iteration 144/1000 | Loss: 0.00002229
Iteration 145/1000 | Loss: 0.00002229
Iteration 146/1000 | Loss: 0.00002229
Iteration 147/1000 | Loss: 0.00002229
Iteration 148/1000 | Loss: 0.00002228
Iteration 149/1000 | Loss: 0.00002228
Iteration 150/1000 | Loss: 0.00002228
Iteration 151/1000 | Loss: 0.00002228
Iteration 152/1000 | Loss: 0.00002228
Iteration 153/1000 | Loss: 0.00002227
Iteration 154/1000 | Loss: 0.00002227
Iteration 155/1000 | Loss: 0.00002227
Iteration 156/1000 | Loss: 0.00002226
Iteration 157/1000 | Loss: 0.00002226
Iteration 158/1000 | Loss: 0.00002226
Iteration 159/1000 | Loss: 0.00002226
Iteration 160/1000 | Loss: 0.00002226
Iteration 161/1000 | Loss: 0.00002226
Iteration 162/1000 | Loss: 0.00002225
Iteration 163/1000 | Loss: 0.00002225
Iteration 164/1000 | Loss: 0.00002225
Iteration 165/1000 | Loss: 0.00002225
Iteration 166/1000 | Loss: 0.00002225
Iteration 167/1000 | Loss: 0.00002225
Iteration 168/1000 | Loss: 0.00002224
Iteration 169/1000 | Loss: 0.00002224
Iteration 170/1000 | Loss: 0.00002224
Iteration 171/1000 | Loss: 0.00002224
Iteration 172/1000 | Loss: 0.00002224
Iteration 173/1000 | Loss: 0.00002224
Iteration 174/1000 | Loss: 0.00002224
Iteration 175/1000 | Loss: 0.00002224
Iteration 176/1000 | Loss: 0.00002224
Iteration 177/1000 | Loss: 0.00002224
Iteration 178/1000 | Loss: 0.00002224
Iteration 179/1000 | Loss: 0.00002224
Iteration 180/1000 | Loss: 0.00002223
Iteration 181/1000 | Loss: 0.00002223
Iteration 182/1000 | Loss: 0.00002223
Iteration 183/1000 | Loss: 0.00002223
Iteration 184/1000 | Loss: 0.00002223
Iteration 185/1000 | Loss: 0.00002223
Iteration 186/1000 | Loss: 0.00002223
Iteration 187/1000 | Loss: 0.00002223
Iteration 188/1000 | Loss: 0.00002223
Iteration 189/1000 | Loss: 0.00002223
Iteration 190/1000 | Loss: 0.00002223
Iteration 191/1000 | Loss: 0.00002222
Iteration 192/1000 | Loss: 0.00002222
Iteration 193/1000 | Loss: 0.00002222
Iteration 194/1000 | Loss: 0.00002222
Iteration 195/1000 | Loss: 0.00002222
Iteration 196/1000 | Loss: 0.00002222
Iteration 197/1000 | Loss: 0.00002222
Iteration 198/1000 | Loss: 0.00002222
Iteration 199/1000 | Loss: 0.00002221
Iteration 200/1000 | Loss: 0.00002221
Iteration 201/1000 | Loss: 0.00002221
Iteration 202/1000 | Loss: 0.00002221
Iteration 203/1000 | Loss: 0.00002220
Iteration 204/1000 | Loss: 0.00002220
Iteration 205/1000 | Loss: 0.00002220
Iteration 206/1000 | Loss: 0.00002220
Iteration 207/1000 | Loss: 0.00002220
Iteration 208/1000 | Loss: 0.00002220
Iteration 209/1000 | Loss: 0.00002220
Iteration 210/1000 | Loss: 0.00002220
Iteration 211/1000 | Loss: 0.00002220
Iteration 212/1000 | Loss: 0.00002220
Iteration 213/1000 | Loss: 0.00002220
Iteration 214/1000 | Loss: 0.00002220
Iteration 215/1000 | Loss: 0.00002220
Iteration 216/1000 | Loss: 0.00002220
Iteration 217/1000 | Loss: 0.00002219
Iteration 218/1000 | Loss: 0.00002219
Iteration 219/1000 | Loss: 0.00002219
Iteration 220/1000 | Loss: 0.00002219
Iteration 221/1000 | Loss: 0.00002219
Iteration 222/1000 | Loss: 0.00002219
Iteration 223/1000 | Loss: 0.00002219
Iteration 224/1000 | Loss: 0.00002219
Iteration 225/1000 | Loss: 0.00002219
Iteration 226/1000 | Loss: 0.00002219
Iteration 227/1000 | Loss: 0.00002218
Iteration 228/1000 | Loss: 0.00002218
Iteration 229/1000 | Loss: 0.00002218
Iteration 230/1000 | Loss: 0.00002218
Iteration 231/1000 | Loss: 0.00002218
Iteration 232/1000 | Loss: 0.00002218
Iteration 233/1000 | Loss: 0.00002218
Iteration 234/1000 | Loss: 0.00002218
Iteration 235/1000 | Loss: 0.00002218
Iteration 236/1000 | Loss: 0.00002218
Iteration 237/1000 | Loss: 0.00002218
Iteration 238/1000 | Loss: 0.00002218
Iteration 239/1000 | Loss: 0.00002218
Iteration 240/1000 | Loss: 0.00002218
Iteration 241/1000 | Loss: 0.00002218
Iteration 242/1000 | Loss: 0.00002218
Iteration 243/1000 | Loss: 0.00002218
Iteration 244/1000 | Loss: 0.00002218
Iteration 245/1000 | Loss: 0.00002218
Iteration 246/1000 | Loss: 0.00002218
Iteration 247/1000 | Loss: 0.00002218
Iteration 248/1000 | Loss: 0.00002218
Iteration 249/1000 | Loss: 0.00002218
Iteration 250/1000 | Loss: 0.00002218
Iteration 251/1000 | Loss: 0.00002218
Iteration 252/1000 | Loss: 0.00002218
Iteration 253/1000 | Loss: 0.00002218
Iteration 254/1000 | Loss: 0.00002218
Iteration 255/1000 | Loss: 0.00002218
Iteration 256/1000 | Loss: 0.00002218
Iteration 257/1000 | Loss: 0.00002218
Iteration 258/1000 | Loss: 0.00002218
Iteration 259/1000 | Loss: 0.00002218
Iteration 260/1000 | Loss: 0.00002218
Iteration 261/1000 | Loss: 0.00002218
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 261. Stopping optimization.
Last 5 losses: [2.2175618141773157e-05, 2.2175618141773157e-05, 2.2175618141773157e-05, 2.2175618141773157e-05, 2.2175618141773157e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2175618141773157e-05

Optimization complete. Final v2v error: 3.9782700538635254 mm

Highest mean error: 4.505824089050293 mm for frame 114

Lowest mean error: 3.5734801292419434 mm for frame 174

Saving results

Total time: 42.96309041976929
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_018/1019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_018/1019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_018/1019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00974250
Iteration 2/25 | Loss: 0.00162484
Iteration 3/25 | Loss: 0.00138093
Iteration 4/25 | Loss: 0.00135135
Iteration 5/25 | Loss: 0.00134491
Iteration 6/25 | Loss: 0.00134423
Iteration 7/25 | Loss: 0.00134423
Iteration 8/25 | Loss: 0.00134423
Iteration 9/25 | Loss: 0.00134423
Iteration 10/25 | Loss: 0.00134423
Iteration 11/25 | Loss: 0.00134423
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001344227814115584, 0.001344227814115584, 0.001344227814115584, 0.001344227814115584, 0.001344227814115584]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001344227814115584

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.04395914
Iteration 2/25 | Loss: 0.00093952
Iteration 3/25 | Loss: 0.00093890
Iteration 4/25 | Loss: 0.00093890
Iteration 5/25 | Loss: 0.00093890
Iteration 6/25 | Loss: 0.00093890
Iteration 7/25 | Loss: 0.00093890
Iteration 8/25 | Loss: 0.00093890
Iteration 9/25 | Loss: 0.00093890
Iteration 10/25 | Loss: 0.00093890
Iteration 11/25 | Loss: 0.00093890
Iteration 12/25 | Loss: 0.00093890
Iteration 13/25 | Loss: 0.00093890
Iteration 14/25 | Loss: 0.00093890
Iteration 15/25 | Loss: 0.00093890
Iteration 16/25 | Loss: 0.00093890
Iteration 17/25 | Loss: 0.00093890
Iteration 18/25 | Loss: 0.00093890
Iteration 19/25 | Loss: 0.00093890
Iteration 20/25 | Loss: 0.00093890
Iteration 21/25 | Loss: 0.00093890
Iteration 22/25 | Loss: 0.00093890
Iteration 23/25 | Loss: 0.00093890
Iteration 24/25 | Loss: 0.00093890
Iteration 25/25 | Loss: 0.00093890

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00093890
Iteration 2/1000 | Loss: 0.00007372
Iteration 3/1000 | Loss: 0.00004399
Iteration 4/1000 | Loss: 0.00003907
Iteration 5/1000 | Loss: 0.00003715
Iteration 6/1000 | Loss: 0.00003574
Iteration 7/1000 | Loss: 0.00003491
Iteration 8/1000 | Loss: 0.00003438
Iteration 9/1000 | Loss: 0.00003397
Iteration 10/1000 | Loss: 0.00003359
Iteration 11/1000 | Loss: 0.00003327
Iteration 12/1000 | Loss: 0.00003315
Iteration 13/1000 | Loss: 0.00003311
Iteration 14/1000 | Loss: 0.00003293
Iteration 15/1000 | Loss: 0.00003288
Iteration 16/1000 | Loss: 0.00003283
Iteration 17/1000 | Loss: 0.00003276
Iteration 18/1000 | Loss: 0.00003276
Iteration 19/1000 | Loss: 0.00003273
Iteration 20/1000 | Loss: 0.00003270
Iteration 21/1000 | Loss: 0.00003266
Iteration 22/1000 | Loss: 0.00003266
Iteration 23/1000 | Loss: 0.00003265
Iteration 24/1000 | Loss: 0.00003265
Iteration 25/1000 | Loss: 0.00003265
Iteration 26/1000 | Loss: 0.00003264
Iteration 27/1000 | Loss: 0.00003263
Iteration 28/1000 | Loss: 0.00003263
Iteration 29/1000 | Loss: 0.00003263
Iteration 30/1000 | Loss: 0.00003262
Iteration 31/1000 | Loss: 0.00003262
Iteration 32/1000 | Loss: 0.00003261
Iteration 33/1000 | Loss: 0.00003261
Iteration 34/1000 | Loss: 0.00003261
Iteration 35/1000 | Loss: 0.00003261
Iteration 36/1000 | Loss: 0.00003261
Iteration 37/1000 | Loss: 0.00003261
Iteration 38/1000 | Loss: 0.00003261
Iteration 39/1000 | Loss: 0.00003261
Iteration 40/1000 | Loss: 0.00003261
Iteration 41/1000 | Loss: 0.00003261
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 41. Stopping optimization.
Last 5 losses: [3.261067467974499e-05, 3.261067467974499e-05, 3.261067467974499e-05, 3.261067467974499e-05, 3.261067467974499e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.261067467974499e-05

Optimization complete. Final v2v error: 4.5798234939575195 mm

Highest mean error: 6.209601879119873 mm for frame 48

Lowest mean error: 3.378875970840454 mm for frame 76

Saving results

Total time: 35.81505537033081
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_018/1009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_018/1009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_018/1009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00779589
Iteration 2/25 | Loss: 0.00132533
Iteration 3/25 | Loss: 0.00116890
Iteration 4/25 | Loss: 0.00114943
Iteration 5/25 | Loss: 0.00114596
Iteration 6/25 | Loss: 0.00114596
Iteration 7/25 | Loss: 0.00114596
Iteration 8/25 | Loss: 0.00114596
Iteration 9/25 | Loss: 0.00114596
Iteration 10/25 | Loss: 0.00114596
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0011459637898951769, 0.0011459637898951769, 0.0011459637898951769, 0.0011459637898951769, 0.0011459637898951769]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011459637898951769

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35514724
Iteration 2/25 | Loss: 0.00081247
Iteration 3/25 | Loss: 0.00081247
Iteration 4/25 | Loss: 0.00081247
Iteration 5/25 | Loss: 0.00081247
Iteration 6/25 | Loss: 0.00081247
Iteration 7/25 | Loss: 0.00081247
Iteration 8/25 | Loss: 0.00081247
Iteration 9/25 | Loss: 0.00081247
Iteration 10/25 | Loss: 0.00081247
Iteration 11/25 | Loss: 0.00081247
Iteration 12/25 | Loss: 0.00081247
Iteration 13/25 | Loss: 0.00081247
Iteration 14/25 | Loss: 0.00081247
Iteration 15/25 | Loss: 0.00081247
Iteration 16/25 | Loss: 0.00081247
Iteration 17/25 | Loss: 0.00081247
Iteration 18/25 | Loss: 0.00081247
Iteration 19/25 | Loss: 0.00081247
Iteration 20/25 | Loss: 0.00081247
Iteration 21/25 | Loss: 0.00081247
Iteration 22/25 | Loss: 0.00081247
Iteration 23/25 | Loss: 0.00081247
Iteration 24/25 | Loss: 0.00081247
Iteration 25/25 | Loss: 0.00081247

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00081247
Iteration 2/1000 | Loss: 0.00002748
Iteration 3/1000 | Loss: 0.00001602
Iteration 4/1000 | Loss: 0.00001432
Iteration 5/1000 | Loss: 0.00001357
Iteration 6/1000 | Loss: 0.00001293
Iteration 7/1000 | Loss: 0.00001258
Iteration 8/1000 | Loss: 0.00001238
Iteration 9/1000 | Loss: 0.00001209
Iteration 10/1000 | Loss: 0.00001193
Iteration 11/1000 | Loss: 0.00001185
Iteration 12/1000 | Loss: 0.00001185
Iteration 13/1000 | Loss: 0.00001176
Iteration 14/1000 | Loss: 0.00001173
Iteration 15/1000 | Loss: 0.00001171
Iteration 16/1000 | Loss: 0.00001170
Iteration 17/1000 | Loss: 0.00001170
Iteration 18/1000 | Loss: 0.00001168
Iteration 19/1000 | Loss: 0.00001168
Iteration 20/1000 | Loss: 0.00001167
Iteration 21/1000 | Loss: 0.00001167
Iteration 22/1000 | Loss: 0.00001166
Iteration 23/1000 | Loss: 0.00001166
Iteration 24/1000 | Loss: 0.00001166
Iteration 25/1000 | Loss: 0.00001165
Iteration 26/1000 | Loss: 0.00001165
Iteration 27/1000 | Loss: 0.00001164
Iteration 28/1000 | Loss: 0.00001164
Iteration 29/1000 | Loss: 0.00001164
Iteration 30/1000 | Loss: 0.00001164
Iteration 31/1000 | Loss: 0.00001163
Iteration 32/1000 | Loss: 0.00001163
Iteration 33/1000 | Loss: 0.00001162
Iteration 34/1000 | Loss: 0.00001162
Iteration 35/1000 | Loss: 0.00001162
Iteration 36/1000 | Loss: 0.00001160
Iteration 37/1000 | Loss: 0.00001160
Iteration 38/1000 | Loss: 0.00001160
Iteration 39/1000 | Loss: 0.00001160
Iteration 40/1000 | Loss: 0.00001159
Iteration 41/1000 | Loss: 0.00001159
Iteration 42/1000 | Loss: 0.00001158
Iteration 43/1000 | Loss: 0.00001158
Iteration 44/1000 | Loss: 0.00001158
Iteration 45/1000 | Loss: 0.00001157
Iteration 46/1000 | Loss: 0.00001157
Iteration 47/1000 | Loss: 0.00001157
Iteration 48/1000 | Loss: 0.00001157
Iteration 49/1000 | Loss: 0.00001156
Iteration 50/1000 | Loss: 0.00001156
Iteration 51/1000 | Loss: 0.00001155
Iteration 52/1000 | Loss: 0.00001155
Iteration 53/1000 | Loss: 0.00001155
Iteration 54/1000 | Loss: 0.00001154
Iteration 55/1000 | Loss: 0.00001154
Iteration 56/1000 | Loss: 0.00001154
Iteration 57/1000 | Loss: 0.00001153
Iteration 58/1000 | Loss: 0.00001153
Iteration 59/1000 | Loss: 0.00001152
Iteration 60/1000 | Loss: 0.00001152
Iteration 61/1000 | Loss: 0.00001151
Iteration 62/1000 | Loss: 0.00001150
Iteration 63/1000 | Loss: 0.00001150
Iteration 64/1000 | Loss: 0.00001150
Iteration 65/1000 | Loss: 0.00001150
Iteration 66/1000 | Loss: 0.00001150
Iteration 67/1000 | Loss: 0.00001150
Iteration 68/1000 | Loss: 0.00001149
Iteration 69/1000 | Loss: 0.00001149
Iteration 70/1000 | Loss: 0.00001149
Iteration 71/1000 | Loss: 0.00001148
Iteration 72/1000 | Loss: 0.00001148
Iteration 73/1000 | Loss: 0.00001145
Iteration 74/1000 | Loss: 0.00001145
Iteration 75/1000 | Loss: 0.00001145
Iteration 76/1000 | Loss: 0.00001144
Iteration 77/1000 | Loss: 0.00001144
Iteration 78/1000 | Loss: 0.00001144
Iteration 79/1000 | Loss: 0.00001143
Iteration 80/1000 | Loss: 0.00001143
Iteration 81/1000 | Loss: 0.00001143
Iteration 82/1000 | Loss: 0.00001143
Iteration 83/1000 | Loss: 0.00001143
Iteration 84/1000 | Loss: 0.00001143
Iteration 85/1000 | Loss: 0.00001142
Iteration 86/1000 | Loss: 0.00001142
Iteration 87/1000 | Loss: 0.00001142
Iteration 88/1000 | Loss: 0.00001142
Iteration 89/1000 | Loss: 0.00001142
Iteration 90/1000 | Loss: 0.00001142
Iteration 91/1000 | Loss: 0.00001142
Iteration 92/1000 | Loss: 0.00001142
Iteration 93/1000 | Loss: 0.00001142
Iteration 94/1000 | Loss: 0.00001142
Iteration 95/1000 | Loss: 0.00001142
Iteration 96/1000 | Loss: 0.00001142
Iteration 97/1000 | Loss: 0.00001142
Iteration 98/1000 | Loss: 0.00001141
Iteration 99/1000 | Loss: 0.00001141
Iteration 100/1000 | Loss: 0.00001141
Iteration 101/1000 | Loss: 0.00001141
Iteration 102/1000 | Loss: 0.00001141
Iteration 103/1000 | Loss: 0.00001141
Iteration 104/1000 | Loss: 0.00001141
Iteration 105/1000 | Loss: 0.00001140
Iteration 106/1000 | Loss: 0.00001140
Iteration 107/1000 | Loss: 0.00001140
Iteration 108/1000 | Loss: 0.00001140
Iteration 109/1000 | Loss: 0.00001140
Iteration 110/1000 | Loss: 0.00001140
Iteration 111/1000 | Loss: 0.00001140
Iteration 112/1000 | Loss: 0.00001139
Iteration 113/1000 | Loss: 0.00001139
Iteration 114/1000 | Loss: 0.00001139
Iteration 115/1000 | Loss: 0.00001139
Iteration 116/1000 | Loss: 0.00001139
Iteration 117/1000 | Loss: 0.00001139
Iteration 118/1000 | Loss: 0.00001139
Iteration 119/1000 | Loss: 0.00001139
Iteration 120/1000 | Loss: 0.00001138
Iteration 121/1000 | Loss: 0.00001138
Iteration 122/1000 | Loss: 0.00001138
Iteration 123/1000 | Loss: 0.00001138
Iteration 124/1000 | Loss: 0.00001138
Iteration 125/1000 | Loss: 0.00001138
Iteration 126/1000 | Loss: 0.00001138
Iteration 127/1000 | Loss: 0.00001138
Iteration 128/1000 | Loss: 0.00001137
Iteration 129/1000 | Loss: 0.00001137
Iteration 130/1000 | Loss: 0.00001137
Iteration 131/1000 | Loss: 0.00001137
Iteration 132/1000 | Loss: 0.00001137
Iteration 133/1000 | Loss: 0.00001137
Iteration 134/1000 | Loss: 0.00001137
Iteration 135/1000 | Loss: 0.00001136
Iteration 136/1000 | Loss: 0.00001136
Iteration 137/1000 | Loss: 0.00001136
Iteration 138/1000 | Loss: 0.00001136
Iteration 139/1000 | Loss: 0.00001136
Iteration 140/1000 | Loss: 0.00001136
Iteration 141/1000 | Loss: 0.00001136
Iteration 142/1000 | Loss: 0.00001135
Iteration 143/1000 | Loss: 0.00001135
Iteration 144/1000 | Loss: 0.00001135
Iteration 145/1000 | Loss: 0.00001135
Iteration 146/1000 | Loss: 0.00001135
Iteration 147/1000 | Loss: 0.00001135
Iteration 148/1000 | Loss: 0.00001135
Iteration 149/1000 | Loss: 0.00001135
Iteration 150/1000 | Loss: 0.00001135
Iteration 151/1000 | Loss: 0.00001135
Iteration 152/1000 | Loss: 0.00001135
Iteration 153/1000 | Loss: 0.00001135
Iteration 154/1000 | Loss: 0.00001135
Iteration 155/1000 | Loss: 0.00001135
Iteration 156/1000 | Loss: 0.00001135
Iteration 157/1000 | Loss: 0.00001135
Iteration 158/1000 | Loss: 0.00001135
Iteration 159/1000 | Loss: 0.00001135
Iteration 160/1000 | Loss: 0.00001135
Iteration 161/1000 | Loss: 0.00001135
Iteration 162/1000 | Loss: 0.00001135
Iteration 163/1000 | Loss: 0.00001135
Iteration 164/1000 | Loss: 0.00001135
Iteration 165/1000 | Loss: 0.00001135
Iteration 166/1000 | Loss: 0.00001135
Iteration 167/1000 | Loss: 0.00001135
Iteration 168/1000 | Loss: 0.00001135
Iteration 169/1000 | Loss: 0.00001135
Iteration 170/1000 | Loss: 0.00001135
Iteration 171/1000 | Loss: 0.00001135
Iteration 172/1000 | Loss: 0.00001135
Iteration 173/1000 | Loss: 0.00001135
Iteration 174/1000 | Loss: 0.00001135
Iteration 175/1000 | Loss: 0.00001135
Iteration 176/1000 | Loss: 0.00001135
Iteration 177/1000 | Loss: 0.00001135
Iteration 178/1000 | Loss: 0.00001135
Iteration 179/1000 | Loss: 0.00001135
Iteration 180/1000 | Loss: 0.00001135
Iteration 181/1000 | Loss: 0.00001135
Iteration 182/1000 | Loss: 0.00001135
Iteration 183/1000 | Loss: 0.00001135
Iteration 184/1000 | Loss: 0.00001135
Iteration 185/1000 | Loss: 0.00001135
Iteration 186/1000 | Loss: 0.00001135
Iteration 187/1000 | Loss: 0.00001135
Iteration 188/1000 | Loss: 0.00001135
Iteration 189/1000 | Loss: 0.00001135
Iteration 190/1000 | Loss: 0.00001135
Iteration 191/1000 | Loss: 0.00001135
Iteration 192/1000 | Loss: 0.00001135
Iteration 193/1000 | Loss: 0.00001135
Iteration 194/1000 | Loss: 0.00001135
Iteration 195/1000 | Loss: 0.00001135
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 195. Stopping optimization.
Last 5 losses: [1.135447746491991e-05, 1.135447746491991e-05, 1.135447746491991e-05, 1.135447746491991e-05, 1.135447746491991e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.135447746491991e-05

Optimization complete. Final v2v error: 2.857311248779297 mm

Highest mean error: 3.024735689163208 mm for frame 63

Lowest mean error: 2.641441822052002 mm for frame 234

Saving results

Total time: 39.65827131271362
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_018/1032/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_018/1032.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_018/1032
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00385186
Iteration 2/25 | Loss: 0.00120361
Iteration 3/25 | Loss: 0.00113493
Iteration 4/25 | Loss: 0.00112450
Iteration 5/25 | Loss: 0.00112147
Iteration 6/25 | Loss: 0.00112125
Iteration 7/25 | Loss: 0.00112125
Iteration 8/25 | Loss: 0.00112125
Iteration 9/25 | Loss: 0.00112125
Iteration 10/25 | Loss: 0.00112125
Iteration 11/25 | Loss: 0.00112125
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001121249981224537, 0.001121249981224537, 0.001121249981224537, 0.001121249981224537, 0.001121249981224537]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001121249981224537

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37772810
Iteration 2/25 | Loss: 0.00081854
Iteration 3/25 | Loss: 0.00081854
Iteration 4/25 | Loss: 0.00081854
Iteration 5/25 | Loss: 0.00081854
Iteration 6/25 | Loss: 0.00081854
Iteration 7/25 | Loss: 0.00081854
Iteration 8/25 | Loss: 0.00081854
Iteration 9/25 | Loss: 0.00081854
Iteration 10/25 | Loss: 0.00081854
Iteration 11/25 | Loss: 0.00081854
Iteration 12/25 | Loss: 0.00081854
Iteration 13/25 | Loss: 0.00081854
Iteration 14/25 | Loss: 0.00081854
Iteration 15/25 | Loss: 0.00081854
Iteration 16/25 | Loss: 0.00081854
Iteration 17/25 | Loss: 0.00081854
Iteration 18/25 | Loss: 0.00081854
Iteration 19/25 | Loss: 0.00081854
Iteration 20/25 | Loss: 0.00081854
Iteration 21/25 | Loss: 0.00081854
Iteration 22/25 | Loss: 0.00081854
Iteration 23/25 | Loss: 0.00081854
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0008185398182831705, 0.0008185398182831705, 0.0008185398182831705, 0.0008185398182831705, 0.0008185398182831705]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008185398182831705

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00081854
Iteration 2/1000 | Loss: 0.00002018
Iteration 3/1000 | Loss: 0.00001504
Iteration 4/1000 | Loss: 0.00001373
Iteration 5/1000 | Loss: 0.00001293
Iteration 6/1000 | Loss: 0.00001239
Iteration 7/1000 | Loss: 0.00001213
Iteration 8/1000 | Loss: 0.00001209
Iteration 9/1000 | Loss: 0.00001176
Iteration 10/1000 | Loss: 0.00001163
Iteration 11/1000 | Loss: 0.00001163
Iteration 12/1000 | Loss: 0.00001159
Iteration 13/1000 | Loss: 0.00001156
Iteration 14/1000 | Loss: 0.00001155
Iteration 15/1000 | Loss: 0.00001154
Iteration 16/1000 | Loss: 0.00001154
Iteration 17/1000 | Loss: 0.00001153
Iteration 18/1000 | Loss: 0.00001153
Iteration 19/1000 | Loss: 0.00001147
Iteration 20/1000 | Loss: 0.00001147
Iteration 21/1000 | Loss: 0.00001145
Iteration 22/1000 | Loss: 0.00001143
Iteration 23/1000 | Loss: 0.00001142
Iteration 24/1000 | Loss: 0.00001141
Iteration 25/1000 | Loss: 0.00001140
Iteration 26/1000 | Loss: 0.00001139
Iteration 27/1000 | Loss: 0.00001134
Iteration 28/1000 | Loss: 0.00001134
Iteration 29/1000 | Loss: 0.00001134
Iteration 30/1000 | Loss: 0.00001134
Iteration 31/1000 | Loss: 0.00001134
Iteration 32/1000 | Loss: 0.00001134
Iteration 33/1000 | Loss: 0.00001133
Iteration 34/1000 | Loss: 0.00001133
Iteration 35/1000 | Loss: 0.00001133
Iteration 36/1000 | Loss: 0.00001128
Iteration 37/1000 | Loss: 0.00001126
Iteration 38/1000 | Loss: 0.00001124
Iteration 39/1000 | Loss: 0.00001123
Iteration 40/1000 | Loss: 0.00001121
Iteration 41/1000 | Loss: 0.00001121
Iteration 42/1000 | Loss: 0.00001121
Iteration 43/1000 | Loss: 0.00001121
Iteration 44/1000 | Loss: 0.00001120
Iteration 45/1000 | Loss: 0.00001120
Iteration 46/1000 | Loss: 0.00001120
Iteration 47/1000 | Loss: 0.00001120
Iteration 48/1000 | Loss: 0.00001120
Iteration 49/1000 | Loss: 0.00001118
Iteration 50/1000 | Loss: 0.00001117
Iteration 51/1000 | Loss: 0.00001116
Iteration 52/1000 | Loss: 0.00001116
Iteration 53/1000 | Loss: 0.00001116
Iteration 54/1000 | Loss: 0.00001113
Iteration 55/1000 | Loss: 0.00001111
Iteration 56/1000 | Loss: 0.00001110
Iteration 57/1000 | Loss: 0.00001109
Iteration 58/1000 | Loss: 0.00001108
Iteration 59/1000 | Loss: 0.00001108
Iteration 60/1000 | Loss: 0.00001107
Iteration 61/1000 | Loss: 0.00001106
Iteration 62/1000 | Loss: 0.00001106
Iteration 63/1000 | Loss: 0.00001106
Iteration 64/1000 | Loss: 0.00001106
Iteration 65/1000 | Loss: 0.00001105
Iteration 66/1000 | Loss: 0.00001104
Iteration 67/1000 | Loss: 0.00001104
Iteration 68/1000 | Loss: 0.00001102
Iteration 69/1000 | Loss: 0.00001102
Iteration 70/1000 | Loss: 0.00001102
Iteration 71/1000 | Loss: 0.00001102
Iteration 72/1000 | Loss: 0.00001102
Iteration 73/1000 | Loss: 0.00001102
Iteration 74/1000 | Loss: 0.00001102
Iteration 75/1000 | Loss: 0.00001101
Iteration 76/1000 | Loss: 0.00001101
Iteration 77/1000 | Loss: 0.00001101
Iteration 78/1000 | Loss: 0.00001101
Iteration 79/1000 | Loss: 0.00001101
Iteration 80/1000 | Loss: 0.00001101
Iteration 81/1000 | Loss: 0.00001101
Iteration 82/1000 | Loss: 0.00001101
Iteration 83/1000 | Loss: 0.00001101
Iteration 84/1000 | Loss: 0.00001101
Iteration 85/1000 | Loss: 0.00001101
Iteration 86/1000 | Loss: 0.00001100
Iteration 87/1000 | Loss: 0.00001100
Iteration 88/1000 | Loss: 0.00001100
Iteration 89/1000 | Loss: 0.00001100
Iteration 90/1000 | Loss: 0.00001100
Iteration 91/1000 | Loss: 0.00001100
Iteration 92/1000 | Loss: 0.00001100
Iteration 93/1000 | Loss: 0.00001099
Iteration 94/1000 | Loss: 0.00001099
Iteration 95/1000 | Loss: 0.00001099
Iteration 96/1000 | Loss: 0.00001099
Iteration 97/1000 | Loss: 0.00001099
Iteration 98/1000 | Loss: 0.00001098
Iteration 99/1000 | Loss: 0.00001098
Iteration 100/1000 | Loss: 0.00001098
Iteration 101/1000 | Loss: 0.00001098
Iteration 102/1000 | Loss: 0.00001098
Iteration 103/1000 | Loss: 0.00001097
Iteration 104/1000 | Loss: 0.00001097
Iteration 105/1000 | Loss: 0.00001097
Iteration 106/1000 | Loss: 0.00001096
Iteration 107/1000 | Loss: 0.00001096
Iteration 108/1000 | Loss: 0.00001096
Iteration 109/1000 | Loss: 0.00001096
Iteration 110/1000 | Loss: 0.00001095
Iteration 111/1000 | Loss: 0.00001095
Iteration 112/1000 | Loss: 0.00001095
Iteration 113/1000 | Loss: 0.00001095
Iteration 114/1000 | Loss: 0.00001095
Iteration 115/1000 | Loss: 0.00001095
Iteration 116/1000 | Loss: 0.00001095
Iteration 117/1000 | Loss: 0.00001095
Iteration 118/1000 | Loss: 0.00001095
Iteration 119/1000 | Loss: 0.00001095
Iteration 120/1000 | Loss: 0.00001095
Iteration 121/1000 | Loss: 0.00001095
Iteration 122/1000 | Loss: 0.00001094
Iteration 123/1000 | Loss: 0.00001094
Iteration 124/1000 | Loss: 0.00001094
Iteration 125/1000 | Loss: 0.00001093
Iteration 126/1000 | Loss: 0.00001093
Iteration 127/1000 | Loss: 0.00001093
Iteration 128/1000 | Loss: 0.00001093
Iteration 129/1000 | Loss: 0.00001093
Iteration 130/1000 | Loss: 0.00001092
Iteration 131/1000 | Loss: 0.00001092
Iteration 132/1000 | Loss: 0.00001091
Iteration 133/1000 | Loss: 0.00001091
Iteration 134/1000 | Loss: 0.00001090
Iteration 135/1000 | Loss: 0.00001090
Iteration 136/1000 | Loss: 0.00001089
Iteration 137/1000 | Loss: 0.00001089
Iteration 138/1000 | Loss: 0.00001089
Iteration 139/1000 | Loss: 0.00001088
Iteration 140/1000 | Loss: 0.00001088
Iteration 141/1000 | Loss: 0.00001088
Iteration 142/1000 | Loss: 0.00001088
Iteration 143/1000 | Loss: 0.00001087
Iteration 144/1000 | Loss: 0.00001087
Iteration 145/1000 | Loss: 0.00001087
Iteration 146/1000 | Loss: 0.00001087
Iteration 147/1000 | Loss: 0.00001087
Iteration 148/1000 | Loss: 0.00001087
Iteration 149/1000 | Loss: 0.00001087
Iteration 150/1000 | Loss: 0.00001087
Iteration 151/1000 | Loss: 0.00001087
Iteration 152/1000 | Loss: 0.00001087
Iteration 153/1000 | Loss: 0.00001086
Iteration 154/1000 | Loss: 0.00001086
Iteration 155/1000 | Loss: 0.00001086
Iteration 156/1000 | Loss: 0.00001086
Iteration 157/1000 | Loss: 0.00001086
Iteration 158/1000 | Loss: 0.00001085
Iteration 159/1000 | Loss: 0.00001085
Iteration 160/1000 | Loss: 0.00001085
Iteration 161/1000 | Loss: 0.00001085
Iteration 162/1000 | Loss: 0.00001085
Iteration 163/1000 | Loss: 0.00001085
Iteration 164/1000 | Loss: 0.00001084
Iteration 165/1000 | Loss: 0.00001084
Iteration 166/1000 | Loss: 0.00001084
Iteration 167/1000 | Loss: 0.00001084
Iteration 168/1000 | Loss: 0.00001084
Iteration 169/1000 | Loss: 0.00001084
Iteration 170/1000 | Loss: 0.00001084
Iteration 171/1000 | Loss: 0.00001084
Iteration 172/1000 | Loss: 0.00001083
Iteration 173/1000 | Loss: 0.00001083
Iteration 174/1000 | Loss: 0.00001083
Iteration 175/1000 | Loss: 0.00001083
Iteration 176/1000 | Loss: 0.00001083
Iteration 177/1000 | Loss: 0.00001083
Iteration 178/1000 | Loss: 0.00001083
Iteration 179/1000 | Loss: 0.00001083
Iteration 180/1000 | Loss: 0.00001083
Iteration 181/1000 | Loss: 0.00001083
Iteration 182/1000 | Loss: 0.00001083
Iteration 183/1000 | Loss: 0.00001083
Iteration 184/1000 | Loss: 0.00001082
Iteration 185/1000 | Loss: 0.00001082
Iteration 186/1000 | Loss: 0.00001082
Iteration 187/1000 | Loss: 0.00001082
Iteration 188/1000 | Loss: 0.00001082
Iteration 189/1000 | Loss: 0.00001081
Iteration 190/1000 | Loss: 0.00001081
Iteration 191/1000 | Loss: 0.00001081
Iteration 192/1000 | Loss: 0.00001080
Iteration 193/1000 | Loss: 0.00001080
Iteration 194/1000 | Loss: 0.00001080
Iteration 195/1000 | Loss: 0.00001080
Iteration 196/1000 | Loss: 0.00001079
Iteration 197/1000 | Loss: 0.00001079
Iteration 198/1000 | Loss: 0.00001079
Iteration 199/1000 | Loss: 0.00001079
Iteration 200/1000 | Loss: 0.00001079
Iteration 201/1000 | Loss: 0.00001079
Iteration 202/1000 | Loss: 0.00001079
Iteration 203/1000 | Loss: 0.00001079
Iteration 204/1000 | Loss: 0.00001079
Iteration 205/1000 | Loss: 0.00001079
Iteration 206/1000 | Loss: 0.00001078
Iteration 207/1000 | Loss: 0.00001078
Iteration 208/1000 | Loss: 0.00001078
Iteration 209/1000 | Loss: 0.00001078
Iteration 210/1000 | Loss: 0.00001077
Iteration 211/1000 | Loss: 0.00001077
Iteration 212/1000 | Loss: 0.00001077
Iteration 213/1000 | Loss: 0.00001076
Iteration 214/1000 | Loss: 0.00001076
Iteration 215/1000 | Loss: 0.00001076
Iteration 216/1000 | Loss: 0.00001076
Iteration 217/1000 | Loss: 0.00001076
Iteration 218/1000 | Loss: 0.00001076
Iteration 219/1000 | Loss: 0.00001076
Iteration 220/1000 | Loss: 0.00001076
Iteration 221/1000 | Loss: 0.00001076
Iteration 222/1000 | Loss: 0.00001076
Iteration 223/1000 | Loss: 0.00001076
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 223. Stopping optimization.
Last 5 losses: [1.0762793863250408e-05, 1.0762793863250408e-05, 1.0762793863250408e-05, 1.0762793863250408e-05, 1.0762793863250408e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0762793863250408e-05

Optimization complete. Final v2v error: 2.8105757236480713 mm

Highest mean error: 2.9089159965515137 mm for frame 188

Lowest mean error: 2.7487645149230957 mm for frame 205

Saving results

Total time: 44.85843086242676
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_018/1058/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_018/1058.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_018/1058
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00794165
Iteration 2/25 | Loss: 0.00136370
Iteration 3/25 | Loss: 0.00117268
Iteration 4/25 | Loss: 0.00114746
Iteration 5/25 | Loss: 0.00114333
Iteration 6/25 | Loss: 0.00114256
Iteration 7/25 | Loss: 0.00114256
Iteration 8/25 | Loss: 0.00114256
Iteration 9/25 | Loss: 0.00114256
Iteration 10/25 | Loss: 0.00114256
Iteration 11/25 | Loss: 0.00114256
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011425630655139685, 0.0011425630655139685, 0.0011425630655139685, 0.0011425630655139685, 0.0011425630655139685]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011425630655139685

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.28330207
Iteration 2/25 | Loss: 0.00078247
Iteration 3/25 | Loss: 0.00078242
Iteration 4/25 | Loss: 0.00078242
Iteration 5/25 | Loss: 0.00078242
Iteration 6/25 | Loss: 0.00078242
Iteration 7/25 | Loss: 0.00078242
Iteration 8/25 | Loss: 0.00078242
Iteration 9/25 | Loss: 0.00078242
Iteration 10/25 | Loss: 0.00078242
Iteration 11/25 | Loss: 0.00078242
Iteration 12/25 | Loss: 0.00078242
Iteration 13/25 | Loss: 0.00078242
Iteration 14/25 | Loss: 0.00078242
Iteration 15/25 | Loss: 0.00078242
Iteration 16/25 | Loss: 0.00078242
Iteration 17/25 | Loss: 0.00078242
Iteration 18/25 | Loss: 0.00078242
Iteration 19/25 | Loss: 0.00078242
Iteration 20/25 | Loss: 0.00078242
Iteration 21/25 | Loss: 0.00078242
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0007824202184565365, 0.0007824202184565365, 0.0007824202184565365, 0.0007824202184565365, 0.0007824202184565365]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007824202184565365

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00078242
Iteration 2/1000 | Loss: 0.00003460
Iteration 3/1000 | Loss: 0.00002457
Iteration 4/1000 | Loss: 0.00002166
Iteration 5/1000 | Loss: 0.00002048
Iteration 6/1000 | Loss: 0.00001952
Iteration 7/1000 | Loss: 0.00001886
Iteration 8/1000 | Loss: 0.00001844
Iteration 9/1000 | Loss: 0.00001818
Iteration 10/1000 | Loss: 0.00001788
Iteration 11/1000 | Loss: 0.00001758
Iteration 12/1000 | Loss: 0.00001741
Iteration 13/1000 | Loss: 0.00001736
Iteration 14/1000 | Loss: 0.00001734
Iteration 15/1000 | Loss: 0.00001731
Iteration 16/1000 | Loss: 0.00001730
Iteration 17/1000 | Loss: 0.00001727
Iteration 18/1000 | Loss: 0.00001716
Iteration 19/1000 | Loss: 0.00001713
Iteration 20/1000 | Loss: 0.00001712
Iteration 21/1000 | Loss: 0.00001711
Iteration 22/1000 | Loss: 0.00001710
Iteration 23/1000 | Loss: 0.00001709
Iteration 24/1000 | Loss: 0.00001708
Iteration 25/1000 | Loss: 0.00001707
Iteration 26/1000 | Loss: 0.00001707
Iteration 27/1000 | Loss: 0.00001704
Iteration 28/1000 | Loss: 0.00001703
Iteration 29/1000 | Loss: 0.00001703
Iteration 30/1000 | Loss: 0.00001703
Iteration 31/1000 | Loss: 0.00001702
Iteration 32/1000 | Loss: 0.00001701
Iteration 33/1000 | Loss: 0.00001697
Iteration 34/1000 | Loss: 0.00001696
Iteration 35/1000 | Loss: 0.00001695
Iteration 36/1000 | Loss: 0.00001693
Iteration 37/1000 | Loss: 0.00001692
Iteration 38/1000 | Loss: 0.00001692
Iteration 39/1000 | Loss: 0.00001690
Iteration 40/1000 | Loss: 0.00001682
Iteration 41/1000 | Loss: 0.00001681
Iteration 42/1000 | Loss: 0.00001681
Iteration 43/1000 | Loss: 0.00001681
Iteration 44/1000 | Loss: 0.00001680
Iteration 45/1000 | Loss: 0.00001680
Iteration 46/1000 | Loss: 0.00001679
Iteration 47/1000 | Loss: 0.00001679
Iteration 48/1000 | Loss: 0.00001679
Iteration 49/1000 | Loss: 0.00001679
Iteration 50/1000 | Loss: 0.00001679
Iteration 51/1000 | Loss: 0.00001679
Iteration 52/1000 | Loss: 0.00001678
Iteration 53/1000 | Loss: 0.00001678
Iteration 54/1000 | Loss: 0.00001678
Iteration 55/1000 | Loss: 0.00001678
Iteration 56/1000 | Loss: 0.00001677
Iteration 57/1000 | Loss: 0.00001677
Iteration 58/1000 | Loss: 0.00001677
Iteration 59/1000 | Loss: 0.00001677
Iteration 60/1000 | Loss: 0.00001677
Iteration 61/1000 | Loss: 0.00001676
Iteration 62/1000 | Loss: 0.00001676
Iteration 63/1000 | Loss: 0.00001676
Iteration 64/1000 | Loss: 0.00001676
Iteration 65/1000 | Loss: 0.00001676
Iteration 66/1000 | Loss: 0.00001676
Iteration 67/1000 | Loss: 0.00001675
Iteration 68/1000 | Loss: 0.00001675
Iteration 69/1000 | Loss: 0.00001675
Iteration 70/1000 | Loss: 0.00001675
Iteration 71/1000 | Loss: 0.00001675
Iteration 72/1000 | Loss: 0.00001675
Iteration 73/1000 | Loss: 0.00001675
Iteration 74/1000 | Loss: 0.00001675
Iteration 75/1000 | Loss: 0.00001674
Iteration 76/1000 | Loss: 0.00001674
Iteration 77/1000 | Loss: 0.00001674
Iteration 78/1000 | Loss: 0.00001674
Iteration 79/1000 | Loss: 0.00001674
Iteration 80/1000 | Loss: 0.00001674
Iteration 81/1000 | Loss: 0.00001673
Iteration 82/1000 | Loss: 0.00001673
Iteration 83/1000 | Loss: 0.00001673
Iteration 84/1000 | Loss: 0.00001672
Iteration 85/1000 | Loss: 0.00001672
Iteration 86/1000 | Loss: 0.00001672
Iteration 87/1000 | Loss: 0.00001672
Iteration 88/1000 | Loss: 0.00001672
Iteration 89/1000 | Loss: 0.00001672
Iteration 90/1000 | Loss: 0.00001672
Iteration 91/1000 | Loss: 0.00001672
Iteration 92/1000 | Loss: 0.00001671
Iteration 93/1000 | Loss: 0.00001671
Iteration 94/1000 | Loss: 0.00001671
Iteration 95/1000 | Loss: 0.00001671
Iteration 96/1000 | Loss: 0.00001671
Iteration 97/1000 | Loss: 0.00001671
Iteration 98/1000 | Loss: 0.00001670
Iteration 99/1000 | Loss: 0.00001670
Iteration 100/1000 | Loss: 0.00001670
Iteration 101/1000 | Loss: 0.00001670
Iteration 102/1000 | Loss: 0.00001669
Iteration 103/1000 | Loss: 0.00001669
Iteration 104/1000 | Loss: 0.00001669
Iteration 105/1000 | Loss: 0.00001669
Iteration 106/1000 | Loss: 0.00001669
Iteration 107/1000 | Loss: 0.00001668
Iteration 108/1000 | Loss: 0.00001668
Iteration 109/1000 | Loss: 0.00001668
Iteration 110/1000 | Loss: 0.00001668
Iteration 111/1000 | Loss: 0.00001668
Iteration 112/1000 | Loss: 0.00001668
Iteration 113/1000 | Loss: 0.00001668
Iteration 114/1000 | Loss: 0.00001668
Iteration 115/1000 | Loss: 0.00001668
Iteration 116/1000 | Loss: 0.00001667
Iteration 117/1000 | Loss: 0.00001667
Iteration 118/1000 | Loss: 0.00001667
Iteration 119/1000 | Loss: 0.00001667
Iteration 120/1000 | Loss: 0.00001667
Iteration 121/1000 | Loss: 0.00001667
Iteration 122/1000 | Loss: 0.00001667
Iteration 123/1000 | Loss: 0.00001666
Iteration 124/1000 | Loss: 0.00001666
Iteration 125/1000 | Loss: 0.00001666
Iteration 126/1000 | Loss: 0.00001666
Iteration 127/1000 | Loss: 0.00001666
Iteration 128/1000 | Loss: 0.00001666
Iteration 129/1000 | Loss: 0.00001666
Iteration 130/1000 | Loss: 0.00001666
Iteration 131/1000 | Loss: 0.00001666
Iteration 132/1000 | Loss: 0.00001666
Iteration 133/1000 | Loss: 0.00001665
Iteration 134/1000 | Loss: 0.00001665
Iteration 135/1000 | Loss: 0.00001665
Iteration 136/1000 | Loss: 0.00001665
Iteration 137/1000 | Loss: 0.00001665
Iteration 138/1000 | Loss: 0.00001665
Iteration 139/1000 | Loss: 0.00001665
Iteration 140/1000 | Loss: 0.00001665
Iteration 141/1000 | Loss: 0.00001665
Iteration 142/1000 | Loss: 0.00001665
Iteration 143/1000 | Loss: 0.00001665
Iteration 144/1000 | Loss: 0.00001665
Iteration 145/1000 | Loss: 0.00001665
Iteration 146/1000 | Loss: 0.00001665
Iteration 147/1000 | Loss: 0.00001665
Iteration 148/1000 | Loss: 0.00001665
Iteration 149/1000 | Loss: 0.00001665
Iteration 150/1000 | Loss: 0.00001664
Iteration 151/1000 | Loss: 0.00001664
Iteration 152/1000 | Loss: 0.00001664
Iteration 153/1000 | Loss: 0.00001664
Iteration 154/1000 | Loss: 0.00001663
Iteration 155/1000 | Loss: 0.00001663
Iteration 156/1000 | Loss: 0.00001663
Iteration 157/1000 | Loss: 0.00001663
Iteration 158/1000 | Loss: 0.00001663
Iteration 159/1000 | Loss: 0.00001663
Iteration 160/1000 | Loss: 0.00001663
Iteration 161/1000 | Loss: 0.00001663
Iteration 162/1000 | Loss: 0.00001663
Iteration 163/1000 | Loss: 0.00001662
Iteration 164/1000 | Loss: 0.00001662
Iteration 165/1000 | Loss: 0.00001662
Iteration 166/1000 | Loss: 0.00001662
Iteration 167/1000 | Loss: 0.00001662
Iteration 168/1000 | Loss: 0.00001662
Iteration 169/1000 | Loss: 0.00001661
Iteration 170/1000 | Loss: 0.00001661
Iteration 171/1000 | Loss: 0.00001661
Iteration 172/1000 | Loss: 0.00001661
Iteration 173/1000 | Loss: 0.00001661
Iteration 174/1000 | Loss: 0.00001661
Iteration 175/1000 | Loss: 0.00001661
Iteration 176/1000 | Loss: 0.00001661
Iteration 177/1000 | Loss: 0.00001660
Iteration 178/1000 | Loss: 0.00001660
Iteration 179/1000 | Loss: 0.00001660
Iteration 180/1000 | Loss: 0.00001660
Iteration 181/1000 | Loss: 0.00001660
Iteration 182/1000 | Loss: 0.00001660
Iteration 183/1000 | Loss: 0.00001660
Iteration 184/1000 | Loss: 0.00001659
Iteration 185/1000 | Loss: 0.00001659
Iteration 186/1000 | Loss: 0.00001659
Iteration 187/1000 | Loss: 0.00001659
Iteration 188/1000 | Loss: 0.00001659
Iteration 189/1000 | Loss: 0.00001659
Iteration 190/1000 | Loss: 0.00001659
Iteration 191/1000 | Loss: 0.00001658
Iteration 192/1000 | Loss: 0.00001658
Iteration 193/1000 | Loss: 0.00001658
Iteration 194/1000 | Loss: 0.00001658
Iteration 195/1000 | Loss: 0.00001658
Iteration 196/1000 | Loss: 0.00001658
Iteration 197/1000 | Loss: 0.00001658
Iteration 198/1000 | Loss: 0.00001658
Iteration 199/1000 | Loss: 0.00001658
Iteration 200/1000 | Loss: 0.00001658
Iteration 201/1000 | Loss: 0.00001658
Iteration 202/1000 | Loss: 0.00001657
Iteration 203/1000 | Loss: 0.00001657
Iteration 204/1000 | Loss: 0.00001657
Iteration 205/1000 | Loss: 0.00001657
Iteration 206/1000 | Loss: 0.00001657
Iteration 207/1000 | Loss: 0.00001657
Iteration 208/1000 | Loss: 0.00001657
Iteration 209/1000 | Loss: 0.00001657
Iteration 210/1000 | Loss: 0.00001657
Iteration 211/1000 | Loss: 0.00001657
Iteration 212/1000 | Loss: 0.00001657
Iteration 213/1000 | Loss: 0.00001657
Iteration 214/1000 | Loss: 0.00001657
Iteration 215/1000 | Loss: 0.00001657
Iteration 216/1000 | Loss: 0.00001657
Iteration 217/1000 | Loss: 0.00001657
Iteration 218/1000 | Loss: 0.00001657
Iteration 219/1000 | Loss: 0.00001657
Iteration 220/1000 | Loss: 0.00001657
Iteration 221/1000 | Loss: 0.00001657
Iteration 222/1000 | Loss: 0.00001657
Iteration 223/1000 | Loss: 0.00001657
Iteration 224/1000 | Loss: 0.00001657
Iteration 225/1000 | Loss: 0.00001657
Iteration 226/1000 | Loss: 0.00001657
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 226. Stopping optimization.
Last 5 losses: [1.6572244931012392e-05, 1.6572244931012392e-05, 1.6572244931012392e-05, 1.6572244931012392e-05, 1.6572244931012392e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6572244931012392e-05

Optimization complete. Final v2v error: 3.3553578853607178 mm

Highest mean error: 4.690797805786133 mm for frame 66

Lowest mean error: 2.5071425437927246 mm for frame 181

Saving results

Total time: 52.06616401672363
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_018/1044/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_018/1044.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_018/1044
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00397986
Iteration 2/25 | Loss: 0.00119013
Iteration 3/25 | Loss: 0.00111693
Iteration 4/25 | Loss: 0.00110683
Iteration 5/25 | Loss: 0.00110360
Iteration 6/25 | Loss: 0.00110280
Iteration 7/25 | Loss: 0.00110280
Iteration 8/25 | Loss: 0.00110280
Iteration 9/25 | Loss: 0.00110280
Iteration 10/25 | Loss: 0.00110280
Iteration 11/25 | Loss: 0.00110280
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001102795940823853, 0.001102795940823853, 0.001102795940823853, 0.001102795940823853, 0.001102795940823853]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001102795940823853

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.28074217
Iteration 2/25 | Loss: 0.00084570
Iteration 3/25 | Loss: 0.00084570
Iteration 4/25 | Loss: 0.00084570
Iteration 5/25 | Loss: 0.00084570
Iteration 6/25 | Loss: 0.00084570
Iteration 7/25 | Loss: 0.00084570
Iteration 8/25 | Loss: 0.00084570
Iteration 9/25 | Loss: 0.00084570
Iteration 10/25 | Loss: 0.00084570
Iteration 11/25 | Loss: 0.00084570
Iteration 12/25 | Loss: 0.00084570
Iteration 13/25 | Loss: 0.00084570
Iteration 14/25 | Loss: 0.00084570
Iteration 15/25 | Loss: 0.00084570
Iteration 16/25 | Loss: 0.00084570
Iteration 17/25 | Loss: 0.00084570
Iteration 18/25 | Loss: 0.00084570
Iteration 19/25 | Loss: 0.00084570
Iteration 20/25 | Loss: 0.00084570
Iteration 21/25 | Loss: 0.00084570
Iteration 22/25 | Loss: 0.00084570
Iteration 23/25 | Loss: 0.00084570
Iteration 24/25 | Loss: 0.00084570
Iteration 25/25 | Loss: 0.00084570

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00084570
Iteration 2/1000 | Loss: 0.00002186
Iteration 3/1000 | Loss: 0.00001556
Iteration 4/1000 | Loss: 0.00001344
Iteration 5/1000 | Loss: 0.00001226
Iteration 6/1000 | Loss: 0.00001151
Iteration 7/1000 | Loss: 0.00001106
Iteration 8/1000 | Loss: 0.00001077
Iteration 9/1000 | Loss: 0.00001059
Iteration 10/1000 | Loss: 0.00001040
Iteration 11/1000 | Loss: 0.00001029
Iteration 12/1000 | Loss: 0.00001026
Iteration 13/1000 | Loss: 0.00001025
Iteration 14/1000 | Loss: 0.00001022
Iteration 15/1000 | Loss: 0.00001022
Iteration 16/1000 | Loss: 0.00001021
Iteration 17/1000 | Loss: 0.00001013
Iteration 18/1000 | Loss: 0.00001012
Iteration 19/1000 | Loss: 0.00001011
Iteration 20/1000 | Loss: 0.00001010
Iteration 21/1000 | Loss: 0.00001001
Iteration 22/1000 | Loss: 0.00001000
Iteration 23/1000 | Loss: 0.00000999
Iteration 24/1000 | Loss: 0.00000999
Iteration 25/1000 | Loss: 0.00000998
Iteration 26/1000 | Loss: 0.00000998
Iteration 27/1000 | Loss: 0.00000998
Iteration 28/1000 | Loss: 0.00000997
Iteration 29/1000 | Loss: 0.00000996
Iteration 30/1000 | Loss: 0.00000996
Iteration 31/1000 | Loss: 0.00000994
Iteration 32/1000 | Loss: 0.00000993
Iteration 33/1000 | Loss: 0.00000992
Iteration 34/1000 | Loss: 0.00000992
Iteration 35/1000 | Loss: 0.00000991
Iteration 36/1000 | Loss: 0.00000989
Iteration 37/1000 | Loss: 0.00000989
Iteration 38/1000 | Loss: 0.00000989
Iteration 39/1000 | Loss: 0.00000988
Iteration 40/1000 | Loss: 0.00000988
Iteration 41/1000 | Loss: 0.00000988
Iteration 42/1000 | Loss: 0.00000988
Iteration 43/1000 | Loss: 0.00000987
Iteration 44/1000 | Loss: 0.00000986
Iteration 45/1000 | Loss: 0.00000986
Iteration 46/1000 | Loss: 0.00000986
Iteration 47/1000 | Loss: 0.00000985
Iteration 48/1000 | Loss: 0.00000985
Iteration 49/1000 | Loss: 0.00000984
Iteration 50/1000 | Loss: 0.00000984
Iteration 51/1000 | Loss: 0.00000984
Iteration 52/1000 | Loss: 0.00000983
Iteration 53/1000 | Loss: 0.00000983
Iteration 54/1000 | Loss: 0.00000983
Iteration 55/1000 | Loss: 0.00000983
Iteration 56/1000 | Loss: 0.00000983
Iteration 57/1000 | Loss: 0.00000983
Iteration 58/1000 | Loss: 0.00000982
Iteration 59/1000 | Loss: 0.00000982
Iteration 60/1000 | Loss: 0.00000981
Iteration 61/1000 | Loss: 0.00000981
Iteration 62/1000 | Loss: 0.00000980
Iteration 63/1000 | Loss: 0.00000979
Iteration 64/1000 | Loss: 0.00000978
Iteration 65/1000 | Loss: 0.00000978
Iteration 66/1000 | Loss: 0.00000978
Iteration 67/1000 | Loss: 0.00000978
Iteration 68/1000 | Loss: 0.00000978
Iteration 69/1000 | Loss: 0.00000977
Iteration 70/1000 | Loss: 0.00000977
Iteration 71/1000 | Loss: 0.00000976
Iteration 72/1000 | Loss: 0.00000975
Iteration 73/1000 | Loss: 0.00000975
Iteration 74/1000 | Loss: 0.00000974
Iteration 75/1000 | Loss: 0.00000974
Iteration 76/1000 | Loss: 0.00000974
Iteration 77/1000 | Loss: 0.00000974
Iteration 78/1000 | Loss: 0.00000973
Iteration 79/1000 | Loss: 0.00000972
Iteration 80/1000 | Loss: 0.00000972
Iteration 81/1000 | Loss: 0.00000972
Iteration 82/1000 | Loss: 0.00000971
Iteration 83/1000 | Loss: 0.00000971
Iteration 84/1000 | Loss: 0.00000971
Iteration 85/1000 | Loss: 0.00000971
Iteration 86/1000 | Loss: 0.00000970
Iteration 87/1000 | Loss: 0.00000970
Iteration 88/1000 | Loss: 0.00000969
Iteration 89/1000 | Loss: 0.00000969
Iteration 90/1000 | Loss: 0.00000968
Iteration 91/1000 | Loss: 0.00000968
Iteration 92/1000 | Loss: 0.00000967
Iteration 93/1000 | Loss: 0.00000967
Iteration 94/1000 | Loss: 0.00000967
Iteration 95/1000 | Loss: 0.00000967
Iteration 96/1000 | Loss: 0.00000966
Iteration 97/1000 | Loss: 0.00000966
Iteration 98/1000 | Loss: 0.00000966
Iteration 99/1000 | Loss: 0.00000965
Iteration 100/1000 | Loss: 0.00000965
Iteration 101/1000 | Loss: 0.00000965
Iteration 102/1000 | Loss: 0.00000965
Iteration 103/1000 | Loss: 0.00000965
Iteration 104/1000 | Loss: 0.00000965
Iteration 105/1000 | Loss: 0.00000965
Iteration 106/1000 | Loss: 0.00000965
Iteration 107/1000 | Loss: 0.00000965
Iteration 108/1000 | Loss: 0.00000964
Iteration 109/1000 | Loss: 0.00000964
Iteration 110/1000 | Loss: 0.00000964
Iteration 111/1000 | Loss: 0.00000964
Iteration 112/1000 | Loss: 0.00000964
Iteration 113/1000 | Loss: 0.00000964
Iteration 114/1000 | Loss: 0.00000962
Iteration 115/1000 | Loss: 0.00000962
Iteration 116/1000 | Loss: 0.00000962
Iteration 117/1000 | Loss: 0.00000962
Iteration 118/1000 | Loss: 0.00000962
Iteration 119/1000 | Loss: 0.00000962
Iteration 120/1000 | Loss: 0.00000962
Iteration 121/1000 | Loss: 0.00000962
Iteration 122/1000 | Loss: 0.00000962
Iteration 123/1000 | Loss: 0.00000962
Iteration 124/1000 | Loss: 0.00000962
Iteration 125/1000 | Loss: 0.00000962
Iteration 126/1000 | Loss: 0.00000962
Iteration 127/1000 | Loss: 0.00000962
Iteration 128/1000 | Loss: 0.00000962
Iteration 129/1000 | Loss: 0.00000962
Iteration 130/1000 | Loss: 0.00000962
Iteration 131/1000 | Loss: 0.00000962
Iteration 132/1000 | Loss: 0.00000962
Iteration 133/1000 | Loss: 0.00000962
Iteration 134/1000 | Loss: 0.00000962
Iteration 135/1000 | Loss: 0.00000962
Iteration 136/1000 | Loss: 0.00000962
Iteration 137/1000 | Loss: 0.00000962
Iteration 138/1000 | Loss: 0.00000962
Iteration 139/1000 | Loss: 0.00000962
Iteration 140/1000 | Loss: 0.00000962
Iteration 141/1000 | Loss: 0.00000962
Iteration 142/1000 | Loss: 0.00000962
Iteration 143/1000 | Loss: 0.00000962
Iteration 144/1000 | Loss: 0.00000962
Iteration 145/1000 | Loss: 0.00000962
Iteration 146/1000 | Loss: 0.00000962
Iteration 147/1000 | Loss: 0.00000962
Iteration 148/1000 | Loss: 0.00000962
Iteration 149/1000 | Loss: 0.00000962
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 149. Stopping optimization.
Last 5 losses: [9.616295756131876e-06, 9.616295756131876e-06, 9.616295756131876e-06, 9.616295756131876e-06, 9.616295756131876e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.616295756131876e-06

Optimization complete. Final v2v error: 2.6871228218078613 mm

Highest mean error: 2.8978111743927 mm for frame 103

Lowest mean error: 2.562901258468628 mm for frame 13

Saving results

Total time: 36.958109617233276
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_018/1077/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_018/1077.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_018/1077
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00819746
Iteration 2/25 | Loss: 0.00143351
Iteration 3/25 | Loss: 0.00120693
Iteration 4/25 | Loss: 0.00118713
Iteration 5/25 | Loss: 0.00118492
Iteration 6/25 | Loss: 0.00118479
Iteration 7/25 | Loss: 0.00118479
Iteration 8/25 | Loss: 0.00118479
Iteration 9/25 | Loss: 0.00118479
Iteration 10/25 | Loss: 0.00118479
Iteration 11/25 | Loss: 0.00118479
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001184794818982482, 0.001184794818982482, 0.001184794818982482, 0.001184794818982482, 0.001184794818982482]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001184794818982482

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.97388536
Iteration 2/25 | Loss: 0.00046845
Iteration 3/25 | Loss: 0.00046844
Iteration 4/25 | Loss: 0.00046844
Iteration 5/25 | Loss: 0.00046844
Iteration 6/25 | Loss: 0.00046844
Iteration 7/25 | Loss: 0.00046844
Iteration 8/25 | Loss: 0.00046844
Iteration 9/25 | Loss: 0.00046844
Iteration 10/25 | Loss: 0.00046844
Iteration 11/25 | Loss: 0.00046844
Iteration 12/25 | Loss: 0.00046844
Iteration 13/25 | Loss: 0.00046844
Iteration 14/25 | Loss: 0.00046844
Iteration 15/25 | Loss: 0.00046844
Iteration 16/25 | Loss: 0.00046844
Iteration 17/25 | Loss: 0.00046844
Iteration 18/25 | Loss: 0.00046844
Iteration 19/25 | Loss: 0.00046844
Iteration 20/25 | Loss: 0.00046844
Iteration 21/25 | Loss: 0.00046844
Iteration 22/25 | Loss: 0.00046844
Iteration 23/25 | Loss: 0.00046844
Iteration 24/25 | Loss: 0.00046844
Iteration 25/25 | Loss: 0.00046844

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00046844
Iteration 2/1000 | Loss: 0.00002972
Iteration 3/1000 | Loss: 0.00002234
Iteration 4/1000 | Loss: 0.00001996
Iteration 5/1000 | Loss: 0.00001905
Iteration 6/1000 | Loss: 0.00001848
Iteration 7/1000 | Loss: 0.00001802
Iteration 8/1000 | Loss: 0.00001767
Iteration 9/1000 | Loss: 0.00001741
Iteration 10/1000 | Loss: 0.00001718
Iteration 11/1000 | Loss: 0.00001709
Iteration 12/1000 | Loss: 0.00001706
Iteration 13/1000 | Loss: 0.00001705
Iteration 14/1000 | Loss: 0.00001705
Iteration 15/1000 | Loss: 0.00001694
Iteration 16/1000 | Loss: 0.00001693
Iteration 17/1000 | Loss: 0.00001682
Iteration 18/1000 | Loss: 0.00001681
Iteration 19/1000 | Loss: 0.00001681
Iteration 20/1000 | Loss: 0.00001679
Iteration 21/1000 | Loss: 0.00001678
Iteration 22/1000 | Loss: 0.00001678
Iteration 23/1000 | Loss: 0.00001678
Iteration 24/1000 | Loss: 0.00001678
Iteration 25/1000 | Loss: 0.00001678
Iteration 26/1000 | Loss: 0.00001678
Iteration 27/1000 | Loss: 0.00001678
Iteration 28/1000 | Loss: 0.00001678
Iteration 29/1000 | Loss: 0.00001678
Iteration 30/1000 | Loss: 0.00001678
Iteration 31/1000 | Loss: 0.00001677
Iteration 32/1000 | Loss: 0.00001677
Iteration 33/1000 | Loss: 0.00001677
Iteration 34/1000 | Loss: 0.00001677
Iteration 35/1000 | Loss: 0.00001676
Iteration 36/1000 | Loss: 0.00001676
Iteration 37/1000 | Loss: 0.00001669
Iteration 38/1000 | Loss: 0.00001668
Iteration 39/1000 | Loss: 0.00001668
Iteration 40/1000 | Loss: 0.00001668
Iteration 41/1000 | Loss: 0.00001667
Iteration 42/1000 | Loss: 0.00001667
Iteration 43/1000 | Loss: 0.00001667
Iteration 44/1000 | Loss: 0.00001667
Iteration 45/1000 | Loss: 0.00001667
Iteration 46/1000 | Loss: 0.00001667
Iteration 47/1000 | Loss: 0.00001667
Iteration 48/1000 | Loss: 0.00001666
Iteration 49/1000 | Loss: 0.00001666
Iteration 50/1000 | Loss: 0.00001666
Iteration 51/1000 | Loss: 0.00001666
Iteration 52/1000 | Loss: 0.00001666
Iteration 53/1000 | Loss: 0.00001666
Iteration 54/1000 | Loss: 0.00001666
Iteration 55/1000 | Loss: 0.00001663
Iteration 56/1000 | Loss: 0.00001663
Iteration 57/1000 | Loss: 0.00001663
Iteration 58/1000 | Loss: 0.00001663
Iteration 59/1000 | Loss: 0.00001663
Iteration 60/1000 | Loss: 0.00001663
Iteration 61/1000 | Loss: 0.00001663
Iteration 62/1000 | Loss: 0.00001662
Iteration 63/1000 | Loss: 0.00001662
Iteration 64/1000 | Loss: 0.00001662
Iteration 65/1000 | Loss: 0.00001661
Iteration 66/1000 | Loss: 0.00001661
Iteration 67/1000 | Loss: 0.00001661
Iteration 68/1000 | Loss: 0.00001661
Iteration 69/1000 | Loss: 0.00001661
Iteration 70/1000 | Loss: 0.00001661
Iteration 71/1000 | Loss: 0.00001661
Iteration 72/1000 | Loss: 0.00001660
Iteration 73/1000 | Loss: 0.00001660
Iteration 74/1000 | Loss: 0.00001660
Iteration 75/1000 | Loss: 0.00001660
Iteration 76/1000 | Loss: 0.00001660
Iteration 77/1000 | Loss: 0.00001659
Iteration 78/1000 | Loss: 0.00001658
Iteration 79/1000 | Loss: 0.00001658
Iteration 80/1000 | Loss: 0.00001658
Iteration 81/1000 | Loss: 0.00001658
Iteration 82/1000 | Loss: 0.00001658
Iteration 83/1000 | Loss: 0.00001657
Iteration 84/1000 | Loss: 0.00001657
Iteration 85/1000 | Loss: 0.00001657
Iteration 86/1000 | Loss: 0.00001657
Iteration 87/1000 | Loss: 0.00001657
Iteration 88/1000 | Loss: 0.00001657
Iteration 89/1000 | Loss: 0.00001657
Iteration 90/1000 | Loss: 0.00001657
Iteration 91/1000 | Loss: 0.00001657
Iteration 92/1000 | Loss: 0.00001657
Iteration 93/1000 | Loss: 0.00001656
Iteration 94/1000 | Loss: 0.00001656
Iteration 95/1000 | Loss: 0.00001654
Iteration 96/1000 | Loss: 0.00001654
Iteration 97/1000 | Loss: 0.00001654
Iteration 98/1000 | Loss: 0.00001653
Iteration 99/1000 | Loss: 0.00001653
Iteration 100/1000 | Loss: 0.00001653
Iteration 101/1000 | Loss: 0.00001653
Iteration 102/1000 | Loss: 0.00001653
Iteration 103/1000 | Loss: 0.00001653
Iteration 104/1000 | Loss: 0.00001653
Iteration 105/1000 | Loss: 0.00001652
Iteration 106/1000 | Loss: 0.00001652
Iteration 107/1000 | Loss: 0.00001652
Iteration 108/1000 | Loss: 0.00001652
Iteration 109/1000 | Loss: 0.00001652
Iteration 110/1000 | Loss: 0.00001652
Iteration 111/1000 | Loss: 0.00001652
Iteration 112/1000 | Loss: 0.00001652
Iteration 113/1000 | Loss: 0.00001652
Iteration 114/1000 | Loss: 0.00001651
Iteration 115/1000 | Loss: 0.00001651
Iteration 116/1000 | Loss: 0.00001651
Iteration 117/1000 | Loss: 0.00001651
Iteration 118/1000 | Loss: 0.00001651
Iteration 119/1000 | Loss: 0.00001650
Iteration 120/1000 | Loss: 0.00001650
Iteration 121/1000 | Loss: 0.00001650
Iteration 122/1000 | Loss: 0.00001650
Iteration 123/1000 | Loss: 0.00001650
Iteration 124/1000 | Loss: 0.00001650
Iteration 125/1000 | Loss: 0.00001650
Iteration 126/1000 | Loss: 0.00001649
Iteration 127/1000 | Loss: 0.00001649
Iteration 128/1000 | Loss: 0.00001649
Iteration 129/1000 | Loss: 0.00001649
Iteration 130/1000 | Loss: 0.00001649
Iteration 131/1000 | Loss: 0.00001649
Iteration 132/1000 | Loss: 0.00001649
Iteration 133/1000 | Loss: 0.00001648
Iteration 134/1000 | Loss: 0.00001647
Iteration 135/1000 | Loss: 0.00001647
Iteration 136/1000 | Loss: 0.00001647
Iteration 137/1000 | Loss: 0.00001647
Iteration 138/1000 | Loss: 0.00001647
Iteration 139/1000 | Loss: 0.00001647
Iteration 140/1000 | Loss: 0.00001647
Iteration 141/1000 | Loss: 0.00001647
Iteration 142/1000 | Loss: 0.00001647
Iteration 143/1000 | Loss: 0.00001647
Iteration 144/1000 | Loss: 0.00001647
Iteration 145/1000 | Loss: 0.00001647
Iteration 146/1000 | Loss: 0.00001647
Iteration 147/1000 | Loss: 0.00001647
Iteration 148/1000 | Loss: 0.00001647
Iteration 149/1000 | Loss: 0.00001647
Iteration 150/1000 | Loss: 0.00001646
Iteration 151/1000 | Loss: 0.00001646
Iteration 152/1000 | Loss: 0.00001645
Iteration 153/1000 | Loss: 0.00001645
Iteration 154/1000 | Loss: 0.00001645
Iteration 155/1000 | Loss: 0.00001645
Iteration 156/1000 | Loss: 0.00001645
Iteration 157/1000 | Loss: 0.00001645
Iteration 158/1000 | Loss: 0.00001645
Iteration 159/1000 | Loss: 0.00001645
Iteration 160/1000 | Loss: 0.00001645
Iteration 161/1000 | Loss: 0.00001645
Iteration 162/1000 | Loss: 0.00001645
Iteration 163/1000 | Loss: 0.00001645
Iteration 164/1000 | Loss: 0.00001645
Iteration 165/1000 | Loss: 0.00001644
Iteration 166/1000 | Loss: 0.00001644
Iteration 167/1000 | Loss: 0.00001644
Iteration 168/1000 | Loss: 0.00001644
Iteration 169/1000 | Loss: 0.00001644
Iteration 170/1000 | Loss: 0.00001644
Iteration 171/1000 | Loss: 0.00001644
Iteration 172/1000 | Loss: 0.00001644
Iteration 173/1000 | Loss: 0.00001644
Iteration 174/1000 | Loss: 0.00001644
Iteration 175/1000 | Loss: 0.00001644
Iteration 176/1000 | Loss: 0.00001644
Iteration 177/1000 | Loss: 0.00001644
Iteration 178/1000 | Loss: 0.00001644
Iteration 179/1000 | Loss: 0.00001644
Iteration 180/1000 | Loss: 0.00001644
Iteration 181/1000 | Loss: 0.00001644
Iteration 182/1000 | Loss: 0.00001644
Iteration 183/1000 | Loss: 0.00001644
Iteration 184/1000 | Loss: 0.00001644
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 184. Stopping optimization.
Last 5 losses: [1.6438742022728547e-05, 1.6438742022728547e-05, 1.6438742022728547e-05, 1.6438742022728547e-05, 1.6438742022728547e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6438742022728547e-05

Optimization complete. Final v2v error: 3.3872227668762207 mm

Highest mean error: 3.7225327491760254 mm for frame 23

Lowest mean error: 3.200676441192627 mm for frame 141

Saving results

Total time: 38.19239783287048
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_018/1047/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_018/1047.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_018/1047
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00870770
Iteration 2/25 | Loss: 0.00249939
Iteration 3/25 | Loss: 0.00188737
Iteration 4/25 | Loss: 0.00151088
Iteration 5/25 | Loss: 0.00143552
Iteration 6/25 | Loss: 0.00138402
Iteration 7/25 | Loss: 0.00132561
Iteration 8/25 | Loss: 0.00128746
Iteration 9/25 | Loss: 0.00125011
Iteration 10/25 | Loss: 0.00123225
Iteration 11/25 | Loss: 0.00122421
Iteration 12/25 | Loss: 0.00122202
Iteration 13/25 | Loss: 0.00121431
Iteration 14/25 | Loss: 0.00120818
Iteration 15/25 | Loss: 0.00120617
Iteration 16/25 | Loss: 0.00120571
Iteration 17/25 | Loss: 0.00120557
Iteration 18/25 | Loss: 0.00120554
Iteration 19/25 | Loss: 0.00120549
Iteration 20/25 | Loss: 0.00120547
Iteration 21/25 | Loss: 0.00120546
Iteration 22/25 | Loss: 0.00120546
Iteration 23/25 | Loss: 0.00120545
Iteration 24/25 | Loss: 0.00120545
Iteration 25/25 | Loss: 0.00120545

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45744073
Iteration 2/25 | Loss: 0.00149160
Iteration 3/25 | Loss: 0.00149159
Iteration 4/25 | Loss: 0.00149159
Iteration 5/25 | Loss: 0.00149159
Iteration 6/25 | Loss: 0.00149159
Iteration 7/25 | Loss: 0.00149159
Iteration 8/25 | Loss: 0.00149159
Iteration 9/25 | Loss: 0.00149159
Iteration 10/25 | Loss: 0.00149159
Iteration 11/25 | Loss: 0.00149159
Iteration 12/25 | Loss: 0.00149159
Iteration 13/25 | Loss: 0.00149159
Iteration 14/25 | Loss: 0.00149159
Iteration 15/25 | Loss: 0.00149159
Iteration 16/25 | Loss: 0.00149159
Iteration 17/25 | Loss: 0.00149159
Iteration 18/25 | Loss: 0.00149159
Iteration 19/25 | Loss: 0.00149159
Iteration 20/25 | Loss: 0.00149159
Iteration 21/25 | Loss: 0.00149159
Iteration 22/25 | Loss: 0.00149159
Iteration 23/25 | Loss: 0.00149159
Iteration 24/25 | Loss: 0.00149159
Iteration 25/25 | Loss: 0.00149159

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00149159
Iteration 2/1000 | Loss: 0.00009946
Iteration 3/1000 | Loss: 0.00007345
Iteration 4/1000 | Loss: 0.00006144
Iteration 5/1000 | Loss: 0.00005577
Iteration 6/1000 | Loss: 0.00005158
Iteration 7/1000 | Loss: 0.00004880
Iteration 8/1000 | Loss: 0.00004691
Iteration 9/1000 | Loss: 0.00004556
Iteration 10/1000 | Loss: 0.00004463
Iteration 11/1000 | Loss: 0.00004390
Iteration 12/1000 | Loss: 0.00004344
Iteration 13/1000 | Loss: 0.00004310
Iteration 14/1000 | Loss: 0.00004291
Iteration 15/1000 | Loss: 0.00004267
Iteration 16/1000 | Loss: 0.00004247
Iteration 17/1000 | Loss: 0.00004239
Iteration 18/1000 | Loss: 0.00004224
Iteration 19/1000 | Loss: 0.00004202
Iteration 20/1000 | Loss: 0.00004177
Iteration 21/1000 | Loss: 0.00004132
Iteration 22/1000 | Loss: 0.00004077
Iteration 23/1000 | Loss: 0.00004016
Iteration 24/1000 | Loss: 0.00003948
Iteration 25/1000 | Loss: 0.00003902
Iteration 26/1000 | Loss: 0.00016852
Iteration 27/1000 | Loss: 0.00016405
Iteration 28/1000 | Loss: 0.00034971
Iteration 29/1000 | Loss: 0.00054352
Iteration 30/1000 | Loss: 0.00023961
Iteration 31/1000 | Loss: 0.00006481
Iteration 32/1000 | Loss: 0.00004802
Iteration 33/1000 | Loss: 0.00003672
Iteration 34/1000 | Loss: 0.00002749
Iteration 35/1000 | Loss: 0.00002288
Iteration 36/1000 | Loss: 0.00002114
Iteration 37/1000 | Loss: 0.00001988
Iteration 38/1000 | Loss: 0.00001860
Iteration 39/1000 | Loss: 0.00001767
Iteration 40/1000 | Loss: 0.00001715
Iteration 41/1000 | Loss: 0.00001672
Iteration 42/1000 | Loss: 0.00001634
Iteration 43/1000 | Loss: 0.00001619
Iteration 44/1000 | Loss: 0.00001612
Iteration 45/1000 | Loss: 0.00001610
Iteration 46/1000 | Loss: 0.00001610
Iteration 47/1000 | Loss: 0.00001606
Iteration 48/1000 | Loss: 0.00001597
Iteration 49/1000 | Loss: 0.00001596
Iteration 50/1000 | Loss: 0.00001591
Iteration 51/1000 | Loss: 0.00001591
Iteration 52/1000 | Loss: 0.00001589
Iteration 53/1000 | Loss: 0.00001589
Iteration 54/1000 | Loss: 0.00001589
Iteration 55/1000 | Loss: 0.00001589
Iteration 56/1000 | Loss: 0.00001589
Iteration 57/1000 | Loss: 0.00001588
Iteration 58/1000 | Loss: 0.00001588
Iteration 59/1000 | Loss: 0.00001587
Iteration 60/1000 | Loss: 0.00001586
Iteration 61/1000 | Loss: 0.00001586
Iteration 62/1000 | Loss: 0.00001585
Iteration 63/1000 | Loss: 0.00001585
Iteration 64/1000 | Loss: 0.00001584
Iteration 65/1000 | Loss: 0.00001584
Iteration 66/1000 | Loss: 0.00001584
Iteration 67/1000 | Loss: 0.00001584
Iteration 68/1000 | Loss: 0.00001584
Iteration 69/1000 | Loss: 0.00001583
Iteration 70/1000 | Loss: 0.00001583
Iteration 71/1000 | Loss: 0.00001583
Iteration 72/1000 | Loss: 0.00001583
Iteration 73/1000 | Loss: 0.00001582
Iteration 74/1000 | Loss: 0.00001582
Iteration 75/1000 | Loss: 0.00001582
Iteration 76/1000 | Loss: 0.00001582
Iteration 77/1000 | Loss: 0.00001581
Iteration 78/1000 | Loss: 0.00001581
Iteration 79/1000 | Loss: 0.00001580
Iteration 80/1000 | Loss: 0.00001580
Iteration 81/1000 | Loss: 0.00001579
Iteration 82/1000 | Loss: 0.00001579
Iteration 83/1000 | Loss: 0.00001579
Iteration 84/1000 | Loss: 0.00001579
Iteration 85/1000 | Loss: 0.00001578
Iteration 86/1000 | Loss: 0.00001578
Iteration 87/1000 | Loss: 0.00001578
Iteration 88/1000 | Loss: 0.00001577
Iteration 89/1000 | Loss: 0.00001577
Iteration 90/1000 | Loss: 0.00001577
Iteration 91/1000 | Loss: 0.00001577
Iteration 92/1000 | Loss: 0.00001577
Iteration 93/1000 | Loss: 0.00001577
Iteration 94/1000 | Loss: 0.00001577
Iteration 95/1000 | Loss: 0.00001577
Iteration 96/1000 | Loss: 0.00001576
Iteration 97/1000 | Loss: 0.00001576
Iteration 98/1000 | Loss: 0.00001576
Iteration 99/1000 | Loss: 0.00001576
Iteration 100/1000 | Loss: 0.00001575
Iteration 101/1000 | Loss: 0.00001575
Iteration 102/1000 | Loss: 0.00001575
Iteration 103/1000 | Loss: 0.00001574
Iteration 104/1000 | Loss: 0.00001574
Iteration 105/1000 | Loss: 0.00001574
Iteration 106/1000 | Loss: 0.00001574
Iteration 107/1000 | Loss: 0.00001574
Iteration 108/1000 | Loss: 0.00001574
Iteration 109/1000 | Loss: 0.00001574
Iteration 110/1000 | Loss: 0.00001573
Iteration 111/1000 | Loss: 0.00001573
Iteration 112/1000 | Loss: 0.00001573
Iteration 113/1000 | Loss: 0.00001573
Iteration 114/1000 | Loss: 0.00001573
Iteration 115/1000 | Loss: 0.00001573
Iteration 116/1000 | Loss: 0.00001573
Iteration 117/1000 | Loss: 0.00001573
Iteration 118/1000 | Loss: 0.00001573
Iteration 119/1000 | Loss: 0.00001573
Iteration 120/1000 | Loss: 0.00001572
Iteration 121/1000 | Loss: 0.00001572
Iteration 122/1000 | Loss: 0.00001572
Iteration 123/1000 | Loss: 0.00001572
Iteration 124/1000 | Loss: 0.00001572
Iteration 125/1000 | Loss: 0.00001572
Iteration 126/1000 | Loss: 0.00001572
Iteration 127/1000 | Loss: 0.00001572
Iteration 128/1000 | Loss: 0.00001572
Iteration 129/1000 | Loss: 0.00001572
Iteration 130/1000 | Loss: 0.00001572
Iteration 131/1000 | Loss: 0.00001571
Iteration 132/1000 | Loss: 0.00001571
Iteration 133/1000 | Loss: 0.00001571
Iteration 134/1000 | Loss: 0.00001571
Iteration 135/1000 | Loss: 0.00001571
Iteration 136/1000 | Loss: 0.00001571
Iteration 137/1000 | Loss: 0.00001571
Iteration 138/1000 | Loss: 0.00001571
Iteration 139/1000 | Loss: 0.00001571
Iteration 140/1000 | Loss: 0.00001571
Iteration 141/1000 | Loss: 0.00001571
Iteration 142/1000 | Loss: 0.00001571
Iteration 143/1000 | Loss: 0.00001571
Iteration 144/1000 | Loss: 0.00001571
Iteration 145/1000 | Loss: 0.00001571
Iteration 146/1000 | Loss: 0.00001571
Iteration 147/1000 | Loss: 0.00001571
Iteration 148/1000 | Loss: 0.00001571
Iteration 149/1000 | Loss: 0.00001570
Iteration 150/1000 | Loss: 0.00001570
Iteration 151/1000 | Loss: 0.00001570
Iteration 152/1000 | Loss: 0.00001570
Iteration 153/1000 | Loss: 0.00001570
Iteration 154/1000 | Loss: 0.00001570
Iteration 155/1000 | Loss: 0.00001570
Iteration 156/1000 | Loss: 0.00001570
Iteration 157/1000 | Loss: 0.00001570
Iteration 158/1000 | Loss: 0.00001570
Iteration 159/1000 | Loss: 0.00001570
Iteration 160/1000 | Loss: 0.00001570
Iteration 161/1000 | Loss: 0.00001570
Iteration 162/1000 | Loss: 0.00001570
Iteration 163/1000 | Loss: 0.00001570
Iteration 164/1000 | Loss: 0.00001570
Iteration 165/1000 | Loss: 0.00001570
Iteration 166/1000 | Loss: 0.00001570
Iteration 167/1000 | Loss: 0.00001570
Iteration 168/1000 | Loss: 0.00001570
Iteration 169/1000 | Loss: 0.00001570
Iteration 170/1000 | Loss: 0.00001570
Iteration 171/1000 | Loss: 0.00001570
Iteration 172/1000 | Loss: 0.00001570
Iteration 173/1000 | Loss: 0.00001570
Iteration 174/1000 | Loss: 0.00001570
Iteration 175/1000 | Loss: 0.00001570
Iteration 176/1000 | Loss: 0.00001570
Iteration 177/1000 | Loss: 0.00001570
Iteration 178/1000 | Loss: 0.00001570
Iteration 179/1000 | Loss: 0.00001570
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 179. Stopping optimization.
Last 5 losses: [1.5695130059611984e-05, 1.5695130059611984e-05, 1.5695130059611984e-05, 1.5695130059611984e-05, 1.5695130059611984e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5695130059611984e-05

Optimization complete. Final v2v error: 3.076728343963623 mm

Highest mean error: 10.810694694519043 mm for frame 74

Lowest mean error: 2.641826629638672 mm for frame 215

Saving results

Total time: 118.73362493515015
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_018/1059/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_018/1059.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_018/1059
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01016887
Iteration 2/25 | Loss: 0.00182073
Iteration 3/25 | Loss: 0.00135536
Iteration 4/25 | Loss: 0.00128293
Iteration 5/25 | Loss: 0.00127409
Iteration 6/25 | Loss: 0.00125907
Iteration 7/25 | Loss: 0.00124941
Iteration 8/25 | Loss: 0.00124690
Iteration 9/25 | Loss: 0.00124633
Iteration 10/25 | Loss: 0.00124618
Iteration 11/25 | Loss: 0.00124614
Iteration 12/25 | Loss: 0.00124614
Iteration 13/25 | Loss: 0.00124614
Iteration 14/25 | Loss: 0.00124614
Iteration 15/25 | Loss: 0.00124614
Iteration 16/25 | Loss: 0.00124614
Iteration 17/25 | Loss: 0.00124614
Iteration 18/25 | Loss: 0.00124614
Iteration 19/25 | Loss: 0.00124614
Iteration 20/25 | Loss: 0.00124614
Iteration 21/25 | Loss: 0.00124614
Iteration 22/25 | Loss: 0.00124614
Iteration 23/25 | Loss: 0.00124614
Iteration 24/25 | Loss: 0.00124614
Iteration 25/25 | Loss: 0.00124614

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.30063903
Iteration 2/25 | Loss: 0.00098465
Iteration 3/25 | Loss: 0.00098465
Iteration 4/25 | Loss: 0.00098465
Iteration 5/25 | Loss: 0.00098465
Iteration 6/25 | Loss: 0.00098465
Iteration 7/25 | Loss: 0.00098465
Iteration 8/25 | Loss: 0.00098465
Iteration 9/25 | Loss: 0.00098465
Iteration 10/25 | Loss: 0.00098465
Iteration 11/25 | Loss: 0.00098465
Iteration 12/25 | Loss: 0.00098465
Iteration 13/25 | Loss: 0.00098465
Iteration 14/25 | Loss: 0.00098465
Iteration 15/25 | Loss: 0.00098465
Iteration 16/25 | Loss: 0.00098465
Iteration 17/25 | Loss: 0.00098465
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0009846498724073172, 0.0009846498724073172, 0.0009846498724073172, 0.0009846498724073172, 0.0009846498724073172]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009846498724073172

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00098465
Iteration 2/1000 | Loss: 0.00003263
Iteration 3/1000 | Loss: 0.00002439
Iteration 4/1000 | Loss: 0.00002240
Iteration 5/1000 | Loss: 0.00002179
Iteration 6/1000 | Loss: 0.00002153
Iteration 7/1000 | Loss: 0.00002099
Iteration 8/1000 | Loss: 0.00002077
Iteration 9/1000 | Loss: 0.00002051
Iteration 10/1000 | Loss: 0.00002029
Iteration 11/1000 | Loss: 0.00002026
Iteration 12/1000 | Loss: 0.00002005
Iteration 13/1000 | Loss: 0.00001982
Iteration 14/1000 | Loss: 0.00001959
Iteration 15/1000 | Loss: 0.00001950
Iteration 16/1000 | Loss: 0.00001950
Iteration 17/1000 | Loss: 0.00001940
Iteration 18/1000 | Loss: 0.00001936
Iteration 19/1000 | Loss: 0.00001935
Iteration 20/1000 | Loss: 0.00001935
Iteration 21/1000 | Loss: 0.00001935
Iteration 22/1000 | Loss: 0.00001935
Iteration 23/1000 | Loss: 0.00001935
Iteration 24/1000 | Loss: 0.00001935
Iteration 25/1000 | Loss: 0.00001935
Iteration 26/1000 | Loss: 0.00001935
Iteration 27/1000 | Loss: 0.00001935
Iteration 28/1000 | Loss: 0.00001933
Iteration 29/1000 | Loss: 0.00001932
Iteration 30/1000 | Loss: 0.00001932
Iteration 31/1000 | Loss: 0.00001932
Iteration 32/1000 | Loss: 0.00001932
Iteration 33/1000 | Loss: 0.00001932
Iteration 34/1000 | Loss: 0.00001932
Iteration 35/1000 | Loss: 0.00001931
Iteration 36/1000 | Loss: 0.00001931
Iteration 37/1000 | Loss: 0.00001931
Iteration 38/1000 | Loss: 0.00001930
Iteration 39/1000 | Loss: 0.00001930
Iteration 40/1000 | Loss: 0.00001929
Iteration 41/1000 | Loss: 0.00001928
Iteration 42/1000 | Loss: 0.00001928
Iteration 43/1000 | Loss: 0.00001927
Iteration 44/1000 | Loss: 0.00001927
Iteration 45/1000 | Loss: 0.00001927
Iteration 46/1000 | Loss: 0.00001927
Iteration 47/1000 | Loss: 0.00001927
Iteration 48/1000 | Loss: 0.00001926
Iteration 49/1000 | Loss: 0.00001926
Iteration 50/1000 | Loss: 0.00001926
Iteration 51/1000 | Loss: 0.00001926
Iteration 52/1000 | Loss: 0.00001926
Iteration 53/1000 | Loss: 0.00001926
Iteration 54/1000 | Loss: 0.00001926
Iteration 55/1000 | Loss: 0.00001925
Iteration 56/1000 | Loss: 0.00001925
Iteration 57/1000 | Loss: 0.00001925
Iteration 58/1000 | Loss: 0.00001925
Iteration 59/1000 | Loss: 0.00001925
Iteration 60/1000 | Loss: 0.00001925
Iteration 61/1000 | Loss: 0.00001925
Iteration 62/1000 | Loss: 0.00001925
Iteration 63/1000 | Loss: 0.00001925
Iteration 64/1000 | Loss: 0.00001925
Iteration 65/1000 | Loss: 0.00001925
Iteration 66/1000 | Loss: 0.00001925
Iteration 67/1000 | Loss: 0.00001925
Iteration 68/1000 | Loss: 0.00001925
Iteration 69/1000 | Loss: 0.00001925
Iteration 70/1000 | Loss: 0.00001925
Iteration 71/1000 | Loss: 0.00001925
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 71. Stopping optimization.
Last 5 losses: [1.9245222574681975e-05, 1.9245222574681975e-05, 1.9245222574681975e-05, 1.9245222574681975e-05, 1.9245222574681975e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9245222574681975e-05

Optimization complete. Final v2v error: 3.701101779937744 mm

Highest mean error: 3.786658525466919 mm for frame 83

Lowest mean error: 3.4933059215545654 mm for frame 174

Saving results

Total time: 42.68195557594299
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_018/1012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_018/1012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_018/1012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00528376
Iteration 2/25 | Loss: 0.00133896
Iteration 3/25 | Loss: 0.00124815
Iteration 4/25 | Loss: 0.00122950
Iteration 5/25 | Loss: 0.00122294
Iteration 6/25 | Loss: 0.00122167
Iteration 7/25 | Loss: 0.00122153
Iteration 8/25 | Loss: 0.00122153
Iteration 9/25 | Loss: 0.00122153
Iteration 10/25 | Loss: 0.00122153
Iteration 11/25 | Loss: 0.00122153
Iteration 12/25 | Loss: 0.00122153
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0012215255992487073, 0.0012215255992487073, 0.0012215255992487073, 0.0012215255992487073, 0.0012215255992487073]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012215255992487073

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36446464
Iteration 2/25 | Loss: 0.00109534
Iteration 3/25 | Loss: 0.00109534
Iteration 4/25 | Loss: 0.00109534
Iteration 5/25 | Loss: 0.00109534
Iteration 6/25 | Loss: 0.00109534
Iteration 7/25 | Loss: 0.00109534
Iteration 8/25 | Loss: 0.00109533
Iteration 9/25 | Loss: 0.00109533
Iteration 10/25 | Loss: 0.00109533
Iteration 11/25 | Loss: 0.00109533
Iteration 12/25 | Loss: 0.00109533
Iteration 13/25 | Loss: 0.00109533
Iteration 14/25 | Loss: 0.00109533
Iteration 15/25 | Loss: 0.00109533
Iteration 16/25 | Loss: 0.00109533
Iteration 17/25 | Loss: 0.00109533
Iteration 18/25 | Loss: 0.00109533
Iteration 19/25 | Loss: 0.00109533
Iteration 20/25 | Loss: 0.00109533
Iteration 21/25 | Loss: 0.00109533
Iteration 22/25 | Loss: 0.00109533
Iteration 23/25 | Loss: 0.00109533
Iteration 24/25 | Loss: 0.00109533
Iteration 25/25 | Loss: 0.00109533

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00109533
Iteration 2/1000 | Loss: 0.00006514
Iteration 3/1000 | Loss: 0.00003952
Iteration 4/1000 | Loss: 0.00003110
Iteration 5/1000 | Loss: 0.00002819
Iteration 6/1000 | Loss: 0.00002695
Iteration 7/1000 | Loss: 0.00002589
Iteration 8/1000 | Loss: 0.00002525
Iteration 9/1000 | Loss: 0.00002484
Iteration 10/1000 | Loss: 0.00002438
Iteration 11/1000 | Loss: 0.00002408
Iteration 12/1000 | Loss: 0.00002383
Iteration 13/1000 | Loss: 0.00002364
Iteration 14/1000 | Loss: 0.00002358
Iteration 15/1000 | Loss: 0.00002354
Iteration 16/1000 | Loss: 0.00002350
Iteration 17/1000 | Loss: 0.00002336
Iteration 18/1000 | Loss: 0.00002333
Iteration 19/1000 | Loss: 0.00002331
Iteration 20/1000 | Loss: 0.00002324
Iteration 21/1000 | Loss: 0.00002322
Iteration 22/1000 | Loss: 0.00002318
Iteration 23/1000 | Loss: 0.00002314
Iteration 24/1000 | Loss: 0.00002314
Iteration 25/1000 | Loss: 0.00002309
Iteration 26/1000 | Loss: 0.00002306
Iteration 27/1000 | Loss: 0.00002305
Iteration 28/1000 | Loss: 0.00002305
Iteration 29/1000 | Loss: 0.00002304
Iteration 30/1000 | Loss: 0.00002304
Iteration 31/1000 | Loss: 0.00002304
Iteration 32/1000 | Loss: 0.00002303
Iteration 33/1000 | Loss: 0.00002302
Iteration 34/1000 | Loss: 0.00002302
Iteration 35/1000 | Loss: 0.00002302
Iteration 36/1000 | Loss: 0.00002301
Iteration 37/1000 | Loss: 0.00002301
Iteration 38/1000 | Loss: 0.00002300
Iteration 39/1000 | Loss: 0.00002299
Iteration 40/1000 | Loss: 0.00002299
Iteration 41/1000 | Loss: 0.00002299
Iteration 42/1000 | Loss: 0.00002298
Iteration 43/1000 | Loss: 0.00002298
Iteration 44/1000 | Loss: 0.00002297
Iteration 45/1000 | Loss: 0.00002296
Iteration 46/1000 | Loss: 0.00002295
Iteration 47/1000 | Loss: 0.00002295
Iteration 48/1000 | Loss: 0.00002295
Iteration 49/1000 | Loss: 0.00002294
Iteration 50/1000 | Loss: 0.00002294
Iteration 51/1000 | Loss: 0.00002293
Iteration 52/1000 | Loss: 0.00002293
Iteration 53/1000 | Loss: 0.00002293
Iteration 54/1000 | Loss: 0.00002292
Iteration 55/1000 | Loss: 0.00002292
Iteration 56/1000 | Loss: 0.00002292
Iteration 57/1000 | Loss: 0.00002292
Iteration 58/1000 | Loss: 0.00002292
Iteration 59/1000 | Loss: 0.00002291
Iteration 60/1000 | Loss: 0.00002291
Iteration 61/1000 | Loss: 0.00002290
Iteration 62/1000 | Loss: 0.00002290
Iteration 63/1000 | Loss: 0.00002290
Iteration 64/1000 | Loss: 0.00002289
Iteration 65/1000 | Loss: 0.00002289
Iteration 66/1000 | Loss: 0.00002289
Iteration 67/1000 | Loss: 0.00002289
Iteration 68/1000 | Loss: 0.00002289
Iteration 69/1000 | Loss: 0.00002289
Iteration 70/1000 | Loss: 0.00002289
Iteration 71/1000 | Loss: 0.00002289
Iteration 72/1000 | Loss: 0.00002289
Iteration 73/1000 | Loss: 0.00002289
Iteration 74/1000 | Loss: 0.00002288
Iteration 75/1000 | Loss: 0.00002288
Iteration 76/1000 | Loss: 0.00002288
Iteration 77/1000 | Loss: 0.00002287
Iteration 78/1000 | Loss: 0.00002287
Iteration 79/1000 | Loss: 0.00002287
Iteration 80/1000 | Loss: 0.00002287
Iteration 81/1000 | Loss: 0.00002286
Iteration 82/1000 | Loss: 0.00002286
Iteration 83/1000 | Loss: 0.00002286
Iteration 84/1000 | Loss: 0.00002286
Iteration 85/1000 | Loss: 0.00002286
Iteration 86/1000 | Loss: 0.00002285
Iteration 87/1000 | Loss: 0.00002285
Iteration 88/1000 | Loss: 0.00002285
Iteration 89/1000 | Loss: 0.00002285
Iteration 90/1000 | Loss: 0.00002284
Iteration 91/1000 | Loss: 0.00002284
Iteration 92/1000 | Loss: 0.00002284
Iteration 93/1000 | Loss: 0.00002284
Iteration 94/1000 | Loss: 0.00002284
Iteration 95/1000 | Loss: 0.00002283
Iteration 96/1000 | Loss: 0.00002283
Iteration 97/1000 | Loss: 0.00002283
Iteration 98/1000 | Loss: 0.00002283
Iteration 99/1000 | Loss: 0.00002283
Iteration 100/1000 | Loss: 0.00002283
Iteration 101/1000 | Loss: 0.00002283
Iteration 102/1000 | Loss: 0.00002283
Iteration 103/1000 | Loss: 0.00002283
Iteration 104/1000 | Loss: 0.00002282
Iteration 105/1000 | Loss: 0.00002282
Iteration 106/1000 | Loss: 0.00002282
Iteration 107/1000 | Loss: 0.00002282
Iteration 108/1000 | Loss: 0.00002282
Iteration 109/1000 | Loss: 0.00002281
Iteration 110/1000 | Loss: 0.00002281
Iteration 111/1000 | Loss: 0.00002281
Iteration 112/1000 | Loss: 0.00002281
Iteration 113/1000 | Loss: 0.00002281
Iteration 114/1000 | Loss: 0.00002281
Iteration 115/1000 | Loss: 0.00002280
Iteration 116/1000 | Loss: 0.00002280
Iteration 117/1000 | Loss: 0.00002280
Iteration 118/1000 | Loss: 0.00002280
Iteration 119/1000 | Loss: 0.00002280
Iteration 120/1000 | Loss: 0.00002280
Iteration 121/1000 | Loss: 0.00002280
Iteration 122/1000 | Loss: 0.00002280
Iteration 123/1000 | Loss: 0.00002280
Iteration 124/1000 | Loss: 0.00002280
Iteration 125/1000 | Loss: 0.00002280
Iteration 126/1000 | Loss: 0.00002279
Iteration 127/1000 | Loss: 0.00002279
Iteration 128/1000 | Loss: 0.00002279
Iteration 129/1000 | Loss: 0.00002279
Iteration 130/1000 | Loss: 0.00002279
Iteration 131/1000 | Loss: 0.00002279
Iteration 132/1000 | Loss: 0.00002279
Iteration 133/1000 | Loss: 0.00002279
Iteration 134/1000 | Loss: 0.00002279
Iteration 135/1000 | Loss: 0.00002279
Iteration 136/1000 | Loss: 0.00002279
Iteration 137/1000 | Loss: 0.00002279
Iteration 138/1000 | Loss: 0.00002278
Iteration 139/1000 | Loss: 0.00002278
Iteration 140/1000 | Loss: 0.00002278
Iteration 141/1000 | Loss: 0.00002278
Iteration 142/1000 | Loss: 0.00002278
Iteration 143/1000 | Loss: 0.00002278
Iteration 144/1000 | Loss: 0.00002278
Iteration 145/1000 | Loss: 0.00002278
Iteration 146/1000 | Loss: 0.00002278
Iteration 147/1000 | Loss: 0.00002278
Iteration 148/1000 | Loss: 0.00002278
Iteration 149/1000 | Loss: 0.00002278
Iteration 150/1000 | Loss: 0.00002278
Iteration 151/1000 | Loss: 0.00002278
Iteration 152/1000 | Loss: 0.00002278
Iteration 153/1000 | Loss: 0.00002277
Iteration 154/1000 | Loss: 0.00002277
Iteration 155/1000 | Loss: 0.00002277
Iteration 156/1000 | Loss: 0.00002277
Iteration 157/1000 | Loss: 0.00002277
Iteration 158/1000 | Loss: 0.00002277
Iteration 159/1000 | Loss: 0.00002276
Iteration 160/1000 | Loss: 0.00002276
Iteration 161/1000 | Loss: 0.00002276
Iteration 162/1000 | Loss: 0.00002276
Iteration 163/1000 | Loss: 0.00002276
Iteration 164/1000 | Loss: 0.00002276
Iteration 165/1000 | Loss: 0.00002276
Iteration 166/1000 | Loss: 0.00002276
Iteration 167/1000 | Loss: 0.00002275
Iteration 168/1000 | Loss: 0.00002275
Iteration 169/1000 | Loss: 0.00002275
Iteration 170/1000 | Loss: 0.00002275
Iteration 171/1000 | Loss: 0.00002275
Iteration 172/1000 | Loss: 0.00002275
Iteration 173/1000 | Loss: 0.00002275
Iteration 174/1000 | Loss: 0.00002275
Iteration 175/1000 | Loss: 0.00002275
Iteration 176/1000 | Loss: 0.00002274
Iteration 177/1000 | Loss: 0.00002274
Iteration 178/1000 | Loss: 0.00002274
Iteration 179/1000 | Loss: 0.00002274
Iteration 180/1000 | Loss: 0.00002274
Iteration 181/1000 | Loss: 0.00002273
Iteration 182/1000 | Loss: 0.00002273
Iteration 183/1000 | Loss: 0.00002273
Iteration 184/1000 | Loss: 0.00002273
Iteration 185/1000 | Loss: 0.00002273
Iteration 186/1000 | Loss: 0.00002273
Iteration 187/1000 | Loss: 0.00002273
Iteration 188/1000 | Loss: 0.00002272
Iteration 189/1000 | Loss: 0.00002272
Iteration 190/1000 | Loss: 0.00002272
Iteration 191/1000 | Loss: 0.00002272
Iteration 192/1000 | Loss: 0.00002272
Iteration 193/1000 | Loss: 0.00002272
Iteration 194/1000 | Loss: 0.00002272
Iteration 195/1000 | Loss: 0.00002272
Iteration 196/1000 | Loss: 0.00002272
Iteration 197/1000 | Loss: 0.00002272
Iteration 198/1000 | Loss: 0.00002272
Iteration 199/1000 | Loss: 0.00002272
Iteration 200/1000 | Loss: 0.00002272
Iteration 201/1000 | Loss: 0.00002272
Iteration 202/1000 | Loss: 0.00002272
Iteration 203/1000 | Loss: 0.00002272
Iteration 204/1000 | Loss: 0.00002272
Iteration 205/1000 | Loss: 0.00002272
Iteration 206/1000 | Loss: 0.00002272
Iteration 207/1000 | Loss: 0.00002272
Iteration 208/1000 | Loss: 0.00002272
Iteration 209/1000 | Loss: 0.00002272
Iteration 210/1000 | Loss: 0.00002272
Iteration 211/1000 | Loss: 0.00002272
Iteration 212/1000 | Loss: 0.00002272
Iteration 213/1000 | Loss: 0.00002272
Iteration 214/1000 | Loss: 0.00002272
Iteration 215/1000 | Loss: 0.00002272
Iteration 216/1000 | Loss: 0.00002272
Iteration 217/1000 | Loss: 0.00002272
Iteration 218/1000 | Loss: 0.00002272
Iteration 219/1000 | Loss: 0.00002272
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 219. Stopping optimization.
Last 5 losses: [2.271856828883756e-05, 2.271856828883756e-05, 2.271856828883756e-05, 2.271856828883756e-05, 2.271856828883756e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.271856828883756e-05

Optimization complete. Final v2v error: 3.79099440574646 mm

Highest mean error: 4.5259294509887695 mm for frame 110

Lowest mean error: 2.8351964950561523 mm for frame 8

Saving results

Total time: 46.40410256385803
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_018/1064/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_018/1064.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_018/1064
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00413976
Iteration 2/25 | Loss: 0.00123593
Iteration 3/25 | Loss: 0.00115218
Iteration 4/25 | Loss: 0.00113694
Iteration 5/25 | Loss: 0.00113205
Iteration 6/25 | Loss: 0.00113168
Iteration 7/25 | Loss: 0.00113168
Iteration 8/25 | Loss: 0.00113168
Iteration 9/25 | Loss: 0.00113168
Iteration 10/25 | Loss: 0.00113168
Iteration 11/25 | Loss: 0.00113168
Iteration 12/25 | Loss: 0.00113168
Iteration 13/25 | Loss: 0.00113168
Iteration 14/25 | Loss: 0.00113168
Iteration 15/25 | Loss: 0.00113168
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0011316798627376556, 0.0011316798627376556, 0.0011316798627376556, 0.0011316798627376556, 0.0011316798627376556]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011316798627376556

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.49810123
Iteration 2/25 | Loss: 0.00078954
Iteration 3/25 | Loss: 0.00078953
Iteration 4/25 | Loss: 0.00078953
Iteration 5/25 | Loss: 0.00078953
Iteration 6/25 | Loss: 0.00078953
Iteration 7/25 | Loss: 0.00078953
Iteration 8/25 | Loss: 0.00078953
Iteration 9/25 | Loss: 0.00078953
Iteration 10/25 | Loss: 0.00078953
Iteration 11/25 | Loss: 0.00078953
Iteration 12/25 | Loss: 0.00078953
Iteration 13/25 | Loss: 0.00078953
Iteration 14/25 | Loss: 0.00078953
Iteration 15/25 | Loss: 0.00078953
Iteration 16/25 | Loss: 0.00078953
Iteration 17/25 | Loss: 0.00078953
Iteration 18/25 | Loss: 0.00078953
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0007895267335698009, 0.0007895267335698009, 0.0007895267335698009, 0.0007895267335698009, 0.0007895267335698009]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007895267335698009

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00078953
Iteration 2/1000 | Loss: 0.00002229
Iteration 3/1000 | Loss: 0.00001578
Iteration 4/1000 | Loss: 0.00001471
Iteration 5/1000 | Loss: 0.00001407
Iteration 6/1000 | Loss: 0.00001360
Iteration 7/1000 | Loss: 0.00001329
Iteration 8/1000 | Loss: 0.00001299
Iteration 9/1000 | Loss: 0.00001271
Iteration 10/1000 | Loss: 0.00001250
Iteration 11/1000 | Loss: 0.00001248
Iteration 12/1000 | Loss: 0.00001237
Iteration 13/1000 | Loss: 0.00001236
Iteration 14/1000 | Loss: 0.00001228
Iteration 15/1000 | Loss: 0.00001228
Iteration 16/1000 | Loss: 0.00001227
Iteration 17/1000 | Loss: 0.00001227
Iteration 18/1000 | Loss: 0.00001227
Iteration 19/1000 | Loss: 0.00001218
Iteration 20/1000 | Loss: 0.00001217
Iteration 21/1000 | Loss: 0.00001210
Iteration 22/1000 | Loss: 0.00001210
Iteration 23/1000 | Loss: 0.00001209
Iteration 24/1000 | Loss: 0.00001208
Iteration 25/1000 | Loss: 0.00001207
Iteration 26/1000 | Loss: 0.00001206
Iteration 27/1000 | Loss: 0.00001200
Iteration 28/1000 | Loss: 0.00001199
Iteration 29/1000 | Loss: 0.00001193
Iteration 30/1000 | Loss: 0.00001193
Iteration 31/1000 | Loss: 0.00001191
Iteration 32/1000 | Loss: 0.00001191
Iteration 33/1000 | Loss: 0.00001191
Iteration 34/1000 | Loss: 0.00001190
Iteration 35/1000 | Loss: 0.00001189
Iteration 36/1000 | Loss: 0.00001188
Iteration 37/1000 | Loss: 0.00001187
Iteration 38/1000 | Loss: 0.00001186
Iteration 39/1000 | Loss: 0.00001186
Iteration 40/1000 | Loss: 0.00001186
Iteration 41/1000 | Loss: 0.00001186
Iteration 42/1000 | Loss: 0.00001185
Iteration 43/1000 | Loss: 0.00001184
Iteration 44/1000 | Loss: 0.00001184
Iteration 45/1000 | Loss: 0.00001183
Iteration 46/1000 | Loss: 0.00001183
Iteration 47/1000 | Loss: 0.00001182
Iteration 48/1000 | Loss: 0.00001182
Iteration 49/1000 | Loss: 0.00001182
Iteration 50/1000 | Loss: 0.00001182
Iteration 51/1000 | Loss: 0.00001181
Iteration 52/1000 | Loss: 0.00001181
Iteration 53/1000 | Loss: 0.00001181
Iteration 54/1000 | Loss: 0.00001181
Iteration 55/1000 | Loss: 0.00001181
Iteration 56/1000 | Loss: 0.00001181
Iteration 57/1000 | Loss: 0.00001180
Iteration 58/1000 | Loss: 0.00001180
Iteration 59/1000 | Loss: 0.00001179
Iteration 60/1000 | Loss: 0.00001179
Iteration 61/1000 | Loss: 0.00001179
Iteration 62/1000 | Loss: 0.00001179
Iteration 63/1000 | Loss: 0.00001179
Iteration 64/1000 | Loss: 0.00001178
Iteration 65/1000 | Loss: 0.00001178
Iteration 66/1000 | Loss: 0.00001178
Iteration 67/1000 | Loss: 0.00001177
Iteration 68/1000 | Loss: 0.00001177
Iteration 69/1000 | Loss: 0.00001177
Iteration 70/1000 | Loss: 0.00001176
Iteration 71/1000 | Loss: 0.00001176
Iteration 72/1000 | Loss: 0.00001176
Iteration 73/1000 | Loss: 0.00001176
Iteration 74/1000 | Loss: 0.00001176
Iteration 75/1000 | Loss: 0.00001176
Iteration 76/1000 | Loss: 0.00001175
Iteration 77/1000 | Loss: 0.00001175
Iteration 78/1000 | Loss: 0.00001175
Iteration 79/1000 | Loss: 0.00001174
Iteration 80/1000 | Loss: 0.00001174
Iteration 81/1000 | Loss: 0.00001173
Iteration 82/1000 | Loss: 0.00001173
Iteration 83/1000 | Loss: 0.00001173
Iteration 84/1000 | Loss: 0.00001172
Iteration 85/1000 | Loss: 0.00001172
Iteration 86/1000 | Loss: 0.00001172
Iteration 87/1000 | Loss: 0.00001172
Iteration 88/1000 | Loss: 0.00001171
Iteration 89/1000 | Loss: 0.00001171
Iteration 90/1000 | Loss: 0.00001171
Iteration 91/1000 | Loss: 0.00001170
Iteration 92/1000 | Loss: 0.00001169
Iteration 93/1000 | Loss: 0.00001169
Iteration 94/1000 | Loss: 0.00001168
Iteration 95/1000 | Loss: 0.00001168
Iteration 96/1000 | Loss: 0.00001168
Iteration 97/1000 | Loss: 0.00001168
Iteration 98/1000 | Loss: 0.00001168
Iteration 99/1000 | Loss: 0.00001168
Iteration 100/1000 | Loss: 0.00001168
Iteration 101/1000 | Loss: 0.00001168
Iteration 102/1000 | Loss: 0.00001167
Iteration 103/1000 | Loss: 0.00001167
Iteration 104/1000 | Loss: 0.00001167
Iteration 105/1000 | Loss: 0.00001167
Iteration 106/1000 | Loss: 0.00001167
Iteration 107/1000 | Loss: 0.00001167
Iteration 108/1000 | Loss: 0.00001167
Iteration 109/1000 | Loss: 0.00001167
Iteration 110/1000 | Loss: 0.00001167
Iteration 111/1000 | Loss: 0.00001167
Iteration 112/1000 | Loss: 0.00001167
Iteration 113/1000 | Loss: 0.00001166
Iteration 114/1000 | Loss: 0.00001166
Iteration 115/1000 | Loss: 0.00001166
Iteration 116/1000 | Loss: 0.00001165
Iteration 117/1000 | Loss: 0.00001165
Iteration 118/1000 | Loss: 0.00001164
Iteration 119/1000 | Loss: 0.00001164
Iteration 120/1000 | Loss: 0.00001163
Iteration 121/1000 | Loss: 0.00001163
Iteration 122/1000 | Loss: 0.00001162
Iteration 123/1000 | Loss: 0.00001162
Iteration 124/1000 | Loss: 0.00001162
Iteration 125/1000 | Loss: 0.00001162
Iteration 126/1000 | Loss: 0.00001162
Iteration 127/1000 | Loss: 0.00001162
Iteration 128/1000 | Loss: 0.00001162
Iteration 129/1000 | Loss: 0.00001162
Iteration 130/1000 | Loss: 0.00001162
Iteration 131/1000 | Loss: 0.00001162
Iteration 132/1000 | Loss: 0.00001162
Iteration 133/1000 | Loss: 0.00001162
Iteration 134/1000 | Loss: 0.00001162
Iteration 135/1000 | Loss: 0.00001162
Iteration 136/1000 | Loss: 0.00001162
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 136. Stopping optimization.
Last 5 losses: [1.1616175470408052e-05, 1.1616175470408052e-05, 1.1616175470408052e-05, 1.1616175470408052e-05, 1.1616175470408052e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1616175470408052e-05

Optimization complete. Final v2v error: 2.9356627464294434 mm

Highest mean error: 3.3193349838256836 mm for frame 89

Lowest mean error: 2.7757797241210938 mm for frame 74

Saving results

Total time: 37.95277547836304
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_018/1023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_018/1023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_018/1023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00400176
Iteration 2/25 | Loss: 0.00126193
Iteration 3/25 | Loss: 0.00115645
Iteration 4/25 | Loss: 0.00114470
Iteration 5/25 | Loss: 0.00114223
Iteration 6/25 | Loss: 0.00114148
Iteration 7/25 | Loss: 0.00114125
Iteration 8/25 | Loss: 0.00114125
Iteration 9/25 | Loss: 0.00114125
Iteration 10/25 | Loss: 0.00114125
Iteration 11/25 | Loss: 0.00114125
Iteration 12/25 | Loss: 0.00114125
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0011412517633289099, 0.0011412517633289099, 0.0011412517633289099, 0.0011412517633289099, 0.0011412517633289099]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011412517633289099

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45855916
Iteration 2/25 | Loss: 0.00081184
Iteration 3/25 | Loss: 0.00081184
Iteration 4/25 | Loss: 0.00081184
Iteration 5/25 | Loss: 0.00081184
Iteration 6/25 | Loss: 0.00081184
Iteration 7/25 | Loss: 0.00081184
Iteration 8/25 | Loss: 0.00081184
Iteration 9/25 | Loss: 0.00081184
Iteration 10/25 | Loss: 0.00081184
Iteration 11/25 | Loss: 0.00081184
Iteration 12/25 | Loss: 0.00081184
Iteration 13/25 | Loss: 0.00081184
Iteration 14/25 | Loss: 0.00081184
Iteration 15/25 | Loss: 0.00081184
Iteration 16/25 | Loss: 0.00081184
Iteration 17/25 | Loss: 0.00081184
Iteration 18/25 | Loss: 0.00081184
Iteration 19/25 | Loss: 0.00081184
Iteration 20/25 | Loss: 0.00081184
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0008118385449051857, 0.0008118385449051857, 0.0008118385449051857, 0.0008118385449051857, 0.0008118385449051857]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008118385449051857

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00081184
Iteration 2/1000 | Loss: 0.00003379
Iteration 3/1000 | Loss: 0.00002154
Iteration 4/1000 | Loss: 0.00001605
Iteration 5/1000 | Loss: 0.00001461
Iteration 6/1000 | Loss: 0.00001353
Iteration 7/1000 | Loss: 0.00001295
Iteration 8/1000 | Loss: 0.00001250
Iteration 9/1000 | Loss: 0.00001220
Iteration 10/1000 | Loss: 0.00001205
Iteration 11/1000 | Loss: 0.00001199
Iteration 12/1000 | Loss: 0.00001184
Iteration 13/1000 | Loss: 0.00001179
Iteration 14/1000 | Loss: 0.00001176
Iteration 15/1000 | Loss: 0.00001175
Iteration 16/1000 | Loss: 0.00001175
Iteration 17/1000 | Loss: 0.00001175
Iteration 18/1000 | Loss: 0.00001174
Iteration 19/1000 | Loss: 0.00001171
Iteration 20/1000 | Loss: 0.00001168
Iteration 21/1000 | Loss: 0.00001168
Iteration 22/1000 | Loss: 0.00001167
Iteration 23/1000 | Loss: 0.00001165
Iteration 24/1000 | Loss: 0.00001163
Iteration 25/1000 | Loss: 0.00001162
Iteration 26/1000 | Loss: 0.00001162
Iteration 27/1000 | Loss: 0.00001161
Iteration 28/1000 | Loss: 0.00001161
Iteration 29/1000 | Loss: 0.00001160
Iteration 30/1000 | Loss: 0.00001160
Iteration 31/1000 | Loss: 0.00001159
Iteration 32/1000 | Loss: 0.00001159
Iteration 33/1000 | Loss: 0.00001158
Iteration 34/1000 | Loss: 0.00001157
Iteration 35/1000 | Loss: 0.00001153
Iteration 36/1000 | Loss: 0.00001151
Iteration 37/1000 | Loss: 0.00001150
Iteration 38/1000 | Loss: 0.00001150
Iteration 39/1000 | Loss: 0.00001149
Iteration 40/1000 | Loss: 0.00001149
Iteration 41/1000 | Loss: 0.00001148
Iteration 42/1000 | Loss: 0.00001148
Iteration 43/1000 | Loss: 0.00001146
Iteration 44/1000 | Loss: 0.00001146
Iteration 45/1000 | Loss: 0.00001145
Iteration 46/1000 | Loss: 0.00001145
Iteration 47/1000 | Loss: 0.00001145
Iteration 48/1000 | Loss: 0.00001145
Iteration 49/1000 | Loss: 0.00001144
Iteration 50/1000 | Loss: 0.00001144
Iteration 51/1000 | Loss: 0.00001143
Iteration 52/1000 | Loss: 0.00001143
Iteration 53/1000 | Loss: 0.00001142
Iteration 54/1000 | Loss: 0.00001142
Iteration 55/1000 | Loss: 0.00001142
Iteration 56/1000 | Loss: 0.00001142
Iteration 57/1000 | Loss: 0.00001142
Iteration 58/1000 | Loss: 0.00001141
Iteration 59/1000 | Loss: 0.00001141
Iteration 60/1000 | Loss: 0.00001141
Iteration 61/1000 | Loss: 0.00001141
Iteration 62/1000 | Loss: 0.00001140
Iteration 63/1000 | Loss: 0.00001140
Iteration 64/1000 | Loss: 0.00001140
Iteration 65/1000 | Loss: 0.00001140
Iteration 66/1000 | Loss: 0.00001140
Iteration 67/1000 | Loss: 0.00001139
Iteration 68/1000 | Loss: 0.00001139
Iteration 69/1000 | Loss: 0.00001139
Iteration 70/1000 | Loss: 0.00001138
Iteration 71/1000 | Loss: 0.00001138
Iteration 72/1000 | Loss: 0.00001138
Iteration 73/1000 | Loss: 0.00001138
Iteration 74/1000 | Loss: 0.00001138
Iteration 75/1000 | Loss: 0.00001138
Iteration 76/1000 | Loss: 0.00001138
Iteration 77/1000 | Loss: 0.00001138
Iteration 78/1000 | Loss: 0.00001138
Iteration 79/1000 | Loss: 0.00001138
Iteration 80/1000 | Loss: 0.00001138
Iteration 81/1000 | Loss: 0.00001137
Iteration 82/1000 | Loss: 0.00001137
Iteration 83/1000 | Loss: 0.00001137
Iteration 84/1000 | Loss: 0.00001137
Iteration 85/1000 | Loss: 0.00001137
Iteration 86/1000 | Loss: 0.00001137
Iteration 87/1000 | Loss: 0.00001137
Iteration 88/1000 | Loss: 0.00001137
Iteration 89/1000 | Loss: 0.00001137
Iteration 90/1000 | Loss: 0.00001137
Iteration 91/1000 | Loss: 0.00001137
Iteration 92/1000 | Loss: 0.00001137
Iteration 93/1000 | Loss: 0.00001137
Iteration 94/1000 | Loss: 0.00001137
Iteration 95/1000 | Loss: 0.00001137
Iteration 96/1000 | Loss: 0.00001137
Iteration 97/1000 | Loss: 0.00001137
Iteration 98/1000 | Loss: 0.00001137
Iteration 99/1000 | Loss: 0.00001137
Iteration 100/1000 | Loss: 0.00001137
Iteration 101/1000 | Loss: 0.00001137
Iteration 102/1000 | Loss: 0.00001137
Iteration 103/1000 | Loss: 0.00001137
Iteration 104/1000 | Loss: 0.00001137
Iteration 105/1000 | Loss: 0.00001137
Iteration 106/1000 | Loss: 0.00001137
Iteration 107/1000 | Loss: 0.00001137
Iteration 108/1000 | Loss: 0.00001137
Iteration 109/1000 | Loss: 0.00001137
Iteration 110/1000 | Loss: 0.00001137
Iteration 111/1000 | Loss: 0.00001137
Iteration 112/1000 | Loss: 0.00001137
Iteration 113/1000 | Loss: 0.00001137
Iteration 114/1000 | Loss: 0.00001137
Iteration 115/1000 | Loss: 0.00001137
Iteration 116/1000 | Loss: 0.00001137
Iteration 117/1000 | Loss: 0.00001137
Iteration 118/1000 | Loss: 0.00001137
Iteration 119/1000 | Loss: 0.00001137
Iteration 120/1000 | Loss: 0.00001137
Iteration 121/1000 | Loss: 0.00001137
Iteration 122/1000 | Loss: 0.00001137
Iteration 123/1000 | Loss: 0.00001137
Iteration 124/1000 | Loss: 0.00001137
Iteration 125/1000 | Loss: 0.00001137
Iteration 126/1000 | Loss: 0.00001137
Iteration 127/1000 | Loss: 0.00001137
Iteration 128/1000 | Loss: 0.00001137
Iteration 129/1000 | Loss: 0.00001137
Iteration 130/1000 | Loss: 0.00001137
Iteration 131/1000 | Loss: 0.00001137
Iteration 132/1000 | Loss: 0.00001137
Iteration 133/1000 | Loss: 0.00001137
Iteration 134/1000 | Loss: 0.00001137
Iteration 135/1000 | Loss: 0.00001137
Iteration 136/1000 | Loss: 0.00001137
Iteration 137/1000 | Loss: 0.00001137
Iteration 138/1000 | Loss: 0.00001137
Iteration 139/1000 | Loss: 0.00001137
Iteration 140/1000 | Loss: 0.00001137
Iteration 141/1000 | Loss: 0.00001137
Iteration 142/1000 | Loss: 0.00001137
Iteration 143/1000 | Loss: 0.00001137
Iteration 144/1000 | Loss: 0.00001137
Iteration 145/1000 | Loss: 0.00001137
Iteration 146/1000 | Loss: 0.00001137
Iteration 147/1000 | Loss: 0.00001137
Iteration 148/1000 | Loss: 0.00001137
Iteration 149/1000 | Loss: 0.00001137
Iteration 150/1000 | Loss: 0.00001137
Iteration 151/1000 | Loss: 0.00001137
Iteration 152/1000 | Loss: 0.00001137
Iteration 153/1000 | Loss: 0.00001137
Iteration 154/1000 | Loss: 0.00001137
Iteration 155/1000 | Loss: 0.00001137
Iteration 156/1000 | Loss: 0.00001137
Iteration 157/1000 | Loss: 0.00001137
Iteration 158/1000 | Loss: 0.00001137
Iteration 159/1000 | Loss: 0.00001137
Iteration 160/1000 | Loss: 0.00001137
Iteration 161/1000 | Loss: 0.00001137
Iteration 162/1000 | Loss: 0.00001137
Iteration 163/1000 | Loss: 0.00001137
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 163. Stopping optimization.
Last 5 losses: [1.1366561921022367e-05, 1.1366561921022367e-05, 1.1366561921022367e-05, 1.1366561921022367e-05, 1.1366561921022367e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1366561921022367e-05

Optimization complete. Final v2v error: 2.84332275390625 mm

Highest mean error: 4.28131628036499 mm for frame 60

Lowest mean error: 2.568091630935669 mm for frame 100

Saving results

Total time: 37.35297465324402
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_018/1024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_018/1024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_018/1024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00829303
Iteration 2/25 | Loss: 0.00121640
Iteration 3/25 | Loss: 0.00112952
Iteration 4/25 | Loss: 0.00111885
Iteration 5/25 | Loss: 0.00111528
Iteration 6/25 | Loss: 0.00111452
Iteration 7/25 | Loss: 0.00111452
Iteration 8/25 | Loss: 0.00111452
Iteration 9/25 | Loss: 0.00111452
Iteration 10/25 | Loss: 0.00111452
Iteration 11/25 | Loss: 0.00111452
Iteration 12/25 | Loss: 0.00111452
Iteration 13/25 | Loss: 0.00111452
Iteration 14/25 | Loss: 0.00111452
Iteration 15/25 | Loss: 0.00111452
Iteration 16/25 | Loss: 0.00111452
Iteration 17/25 | Loss: 0.00111452
Iteration 18/25 | Loss: 0.00111452
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0011145176831632853, 0.0011145176831632853, 0.0011145176831632853, 0.0011145176831632853, 0.0011145176831632853]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011145176831632853

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44390726
Iteration 2/25 | Loss: 0.00085272
Iteration 3/25 | Loss: 0.00085272
Iteration 4/25 | Loss: 0.00085272
Iteration 5/25 | Loss: 0.00085272
Iteration 6/25 | Loss: 0.00085272
Iteration 7/25 | Loss: 0.00085272
Iteration 8/25 | Loss: 0.00085272
Iteration 9/25 | Loss: 0.00085272
Iteration 10/25 | Loss: 0.00085272
Iteration 11/25 | Loss: 0.00085272
Iteration 12/25 | Loss: 0.00085272
Iteration 13/25 | Loss: 0.00085272
Iteration 14/25 | Loss: 0.00085272
Iteration 15/25 | Loss: 0.00085272
Iteration 16/25 | Loss: 0.00085272
Iteration 17/25 | Loss: 0.00085272
Iteration 18/25 | Loss: 0.00085272
Iteration 19/25 | Loss: 0.00085272
Iteration 20/25 | Loss: 0.00085272
Iteration 21/25 | Loss: 0.00085272
Iteration 22/25 | Loss: 0.00085272
Iteration 23/25 | Loss: 0.00085272
Iteration 24/25 | Loss: 0.00085272
Iteration 25/25 | Loss: 0.00085272

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00085272
Iteration 2/1000 | Loss: 0.00001814
Iteration 3/1000 | Loss: 0.00001279
Iteration 4/1000 | Loss: 0.00001154
Iteration 5/1000 | Loss: 0.00001103
Iteration 6/1000 | Loss: 0.00001062
Iteration 7/1000 | Loss: 0.00001025
Iteration 8/1000 | Loss: 0.00001002
Iteration 9/1000 | Loss: 0.00000994
Iteration 10/1000 | Loss: 0.00000992
Iteration 11/1000 | Loss: 0.00000990
Iteration 12/1000 | Loss: 0.00000982
Iteration 13/1000 | Loss: 0.00000968
Iteration 14/1000 | Loss: 0.00000959
Iteration 15/1000 | Loss: 0.00000957
Iteration 16/1000 | Loss: 0.00000955
Iteration 17/1000 | Loss: 0.00000953
Iteration 18/1000 | Loss: 0.00000953
Iteration 19/1000 | Loss: 0.00000952
Iteration 20/1000 | Loss: 0.00000951
Iteration 21/1000 | Loss: 0.00000951
Iteration 22/1000 | Loss: 0.00000949
Iteration 23/1000 | Loss: 0.00000949
Iteration 24/1000 | Loss: 0.00000943
Iteration 25/1000 | Loss: 0.00000940
Iteration 26/1000 | Loss: 0.00000939
Iteration 27/1000 | Loss: 0.00000938
Iteration 28/1000 | Loss: 0.00000937
Iteration 29/1000 | Loss: 0.00000937
Iteration 30/1000 | Loss: 0.00000935
Iteration 31/1000 | Loss: 0.00000933
Iteration 32/1000 | Loss: 0.00000932
Iteration 33/1000 | Loss: 0.00000932
Iteration 34/1000 | Loss: 0.00000931
Iteration 35/1000 | Loss: 0.00000930
Iteration 36/1000 | Loss: 0.00000929
Iteration 37/1000 | Loss: 0.00000928
Iteration 38/1000 | Loss: 0.00000928
Iteration 39/1000 | Loss: 0.00000928
Iteration 40/1000 | Loss: 0.00000928
Iteration 41/1000 | Loss: 0.00000927
Iteration 42/1000 | Loss: 0.00000927
Iteration 43/1000 | Loss: 0.00000927
Iteration 44/1000 | Loss: 0.00000926
Iteration 45/1000 | Loss: 0.00000925
Iteration 46/1000 | Loss: 0.00000925
Iteration 47/1000 | Loss: 0.00000924
Iteration 48/1000 | Loss: 0.00000924
Iteration 49/1000 | Loss: 0.00000924
Iteration 50/1000 | Loss: 0.00000923
Iteration 51/1000 | Loss: 0.00000923
Iteration 52/1000 | Loss: 0.00000923
Iteration 53/1000 | Loss: 0.00000923
Iteration 54/1000 | Loss: 0.00000923
Iteration 55/1000 | Loss: 0.00000923
Iteration 56/1000 | Loss: 0.00000922
Iteration 57/1000 | Loss: 0.00000922
Iteration 58/1000 | Loss: 0.00000922
Iteration 59/1000 | Loss: 0.00000922
Iteration 60/1000 | Loss: 0.00000922
Iteration 61/1000 | Loss: 0.00000921
Iteration 62/1000 | Loss: 0.00000920
Iteration 63/1000 | Loss: 0.00000919
Iteration 64/1000 | Loss: 0.00000918
Iteration 65/1000 | Loss: 0.00000917
Iteration 66/1000 | Loss: 0.00000917
Iteration 67/1000 | Loss: 0.00000917
Iteration 68/1000 | Loss: 0.00000917
Iteration 69/1000 | Loss: 0.00000917
Iteration 70/1000 | Loss: 0.00000916
Iteration 71/1000 | Loss: 0.00000916
Iteration 72/1000 | Loss: 0.00000916
Iteration 73/1000 | Loss: 0.00000916
Iteration 74/1000 | Loss: 0.00000915
Iteration 75/1000 | Loss: 0.00000915
Iteration 76/1000 | Loss: 0.00000914
Iteration 77/1000 | Loss: 0.00000914
Iteration 78/1000 | Loss: 0.00000914
Iteration 79/1000 | Loss: 0.00000914
Iteration 80/1000 | Loss: 0.00000913
Iteration 81/1000 | Loss: 0.00000913
Iteration 82/1000 | Loss: 0.00000913
Iteration 83/1000 | Loss: 0.00000913
Iteration 84/1000 | Loss: 0.00000912
Iteration 85/1000 | Loss: 0.00000912
Iteration 86/1000 | Loss: 0.00000912
Iteration 87/1000 | Loss: 0.00000912
Iteration 88/1000 | Loss: 0.00000912
Iteration 89/1000 | Loss: 0.00000911
Iteration 90/1000 | Loss: 0.00000911
Iteration 91/1000 | Loss: 0.00000910
Iteration 92/1000 | Loss: 0.00000910
Iteration 93/1000 | Loss: 0.00000910
Iteration 94/1000 | Loss: 0.00000910
Iteration 95/1000 | Loss: 0.00000910
Iteration 96/1000 | Loss: 0.00000909
Iteration 97/1000 | Loss: 0.00000909
Iteration 98/1000 | Loss: 0.00000909
Iteration 99/1000 | Loss: 0.00000909
Iteration 100/1000 | Loss: 0.00000909
Iteration 101/1000 | Loss: 0.00000909
Iteration 102/1000 | Loss: 0.00000908
Iteration 103/1000 | Loss: 0.00000908
Iteration 104/1000 | Loss: 0.00000908
Iteration 105/1000 | Loss: 0.00000908
Iteration 106/1000 | Loss: 0.00000908
Iteration 107/1000 | Loss: 0.00000908
Iteration 108/1000 | Loss: 0.00000908
Iteration 109/1000 | Loss: 0.00000907
Iteration 110/1000 | Loss: 0.00000907
Iteration 111/1000 | Loss: 0.00000907
Iteration 112/1000 | Loss: 0.00000906
Iteration 113/1000 | Loss: 0.00000906
Iteration 114/1000 | Loss: 0.00000906
Iteration 115/1000 | Loss: 0.00000905
Iteration 116/1000 | Loss: 0.00000905
Iteration 117/1000 | Loss: 0.00000905
Iteration 118/1000 | Loss: 0.00000904
Iteration 119/1000 | Loss: 0.00000904
Iteration 120/1000 | Loss: 0.00000904
Iteration 121/1000 | Loss: 0.00000903
Iteration 122/1000 | Loss: 0.00000903
Iteration 123/1000 | Loss: 0.00000903
Iteration 124/1000 | Loss: 0.00000902
Iteration 125/1000 | Loss: 0.00000901
Iteration 126/1000 | Loss: 0.00000901
Iteration 127/1000 | Loss: 0.00000901
Iteration 128/1000 | Loss: 0.00000900
Iteration 129/1000 | Loss: 0.00000900
Iteration 130/1000 | Loss: 0.00000900
Iteration 131/1000 | Loss: 0.00000900
Iteration 132/1000 | Loss: 0.00000900
Iteration 133/1000 | Loss: 0.00000900
Iteration 134/1000 | Loss: 0.00000900
Iteration 135/1000 | Loss: 0.00000900
Iteration 136/1000 | Loss: 0.00000900
Iteration 137/1000 | Loss: 0.00000900
Iteration 138/1000 | Loss: 0.00000900
Iteration 139/1000 | Loss: 0.00000900
Iteration 140/1000 | Loss: 0.00000899
Iteration 141/1000 | Loss: 0.00000899
Iteration 142/1000 | Loss: 0.00000899
Iteration 143/1000 | Loss: 0.00000899
Iteration 144/1000 | Loss: 0.00000899
Iteration 145/1000 | Loss: 0.00000899
Iteration 146/1000 | Loss: 0.00000898
Iteration 147/1000 | Loss: 0.00000898
Iteration 148/1000 | Loss: 0.00000898
Iteration 149/1000 | Loss: 0.00000898
Iteration 150/1000 | Loss: 0.00000898
Iteration 151/1000 | Loss: 0.00000898
Iteration 152/1000 | Loss: 0.00000898
Iteration 153/1000 | Loss: 0.00000898
Iteration 154/1000 | Loss: 0.00000898
Iteration 155/1000 | Loss: 0.00000897
Iteration 156/1000 | Loss: 0.00000897
Iteration 157/1000 | Loss: 0.00000897
Iteration 158/1000 | Loss: 0.00000897
Iteration 159/1000 | Loss: 0.00000897
Iteration 160/1000 | Loss: 0.00000897
Iteration 161/1000 | Loss: 0.00000897
Iteration 162/1000 | Loss: 0.00000897
Iteration 163/1000 | Loss: 0.00000897
Iteration 164/1000 | Loss: 0.00000897
Iteration 165/1000 | Loss: 0.00000897
Iteration 166/1000 | Loss: 0.00000897
Iteration 167/1000 | Loss: 0.00000897
Iteration 168/1000 | Loss: 0.00000896
Iteration 169/1000 | Loss: 0.00000896
Iteration 170/1000 | Loss: 0.00000896
Iteration 171/1000 | Loss: 0.00000896
Iteration 172/1000 | Loss: 0.00000896
Iteration 173/1000 | Loss: 0.00000896
Iteration 174/1000 | Loss: 0.00000896
Iteration 175/1000 | Loss: 0.00000896
Iteration 176/1000 | Loss: 0.00000896
Iteration 177/1000 | Loss: 0.00000896
Iteration 178/1000 | Loss: 0.00000896
Iteration 179/1000 | Loss: 0.00000896
Iteration 180/1000 | Loss: 0.00000896
Iteration 181/1000 | Loss: 0.00000896
Iteration 182/1000 | Loss: 0.00000895
Iteration 183/1000 | Loss: 0.00000895
Iteration 184/1000 | Loss: 0.00000895
Iteration 185/1000 | Loss: 0.00000895
Iteration 186/1000 | Loss: 0.00000895
Iteration 187/1000 | Loss: 0.00000895
Iteration 188/1000 | Loss: 0.00000895
Iteration 189/1000 | Loss: 0.00000895
Iteration 190/1000 | Loss: 0.00000895
Iteration 191/1000 | Loss: 0.00000895
Iteration 192/1000 | Loss: 0.00000894
Iteration 193/1000 | Loss: 0.00000894
Iteration 194/1000 | Loss: 0.00000894
Iteration 195/1000 | Loss: 0.00000894
Iteration 196/1000 | Loss: 0.00000894
Iteration 197/1000 | Loss: 0.00000894
Iteration 198/1000 | Loss: 0.00000894
Iteration 199/1000 | Loss: 0.00000894
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 199. Stopping optimization.
Last 5 losses: [8.944821274781134e-06, 8.944821274781134e-06, 8.944821274781134e-06, 8.944821274781134e-06, 8.944821274781134e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.944821274781134e-06

Optimization complete. Final v2v error: 2.573284864425659 mm

Highest mean error: 3.038389205932617 mm for frame 89

Lowest mean error: 2.411764621734619 mm for frame 13

Saving results

Total time: 38.03649568557739
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_018/1065/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_018/1065.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_018/1065
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00743310
Iteration 2/25 | Loss: 0.00161976
Iteration 3/25 | Loss: 0.00140876
Iteration 4/25 | Loss: 0.00139557
Iteration 5/25 | Loss: 0.00139209
Iteration 6/25 | Loss: 0.00139209
Iteration 7/25 | Loss: 0.00139209
Iteration 8/25 | Loss: 0.00139209
Iteration 9/25 | Loss: 0.00139209
Iteration 10/25 | Loss: 0.00139209
Iteration 11/25 | Loss: 0.00139209
Iteration 12/25 | Loss: 0.00139209
Iteration 13/25 | Loss: 0.00139209
Iteration 14/25 | Loss: 0.00139209
Iteration 15/25 | Loss: 0.00139209
Iteration 16/25 | Loss: 0.00139209
Iteration 17/25 | Loss: 0.00139209
Iteration 18/25 | Loss: 0.00139209
Iteration 19/25 | Loss: 0.00139209
Iteration 20/25 | Loss: 0.00139209
Iteration 21/25 | Loss: 0.00139209
Iteration 22/25 | Loss: 0.00139209
Iteration 23/25 | Loss: 0.00139209
Iteration 24/25 | Loss: 0.00139209
Iteration 25/25 | Loss: 0.00139209

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.07444024
Iteration 2/25 | Loss: 0.00101442
Iteration 3/25 | Loss: 0.00101441
Iteration 4/25 | Loss: 0.00101441
Iteration 5/25 | Loss: 0.00101441
Iteration 6/25 | Loss: 0.00101441
Iteration 7/25 | Loss: 0.00101441
Iteration 8/25 | Loss: 0.00101441
Iteration 9/25 | Loss: 0.00101441
Iteration 10/25 | Loss: 0.00101441
Iteration 11/25 | Loss: 0.00101441
Iteration 12/25 | Loss: 0.00101441
Iteration 13/25 | Loss: 0.00101441
Iteration 14/25 | Loss: 0.00101441
Iteration 15/25 | Loss: 0.00101441
Iteration 16/25 | Loss: 0.00101441
Iteration 17/25 | Loss: 0.00101441
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0010144077241420746, 0.0010144077241420746, 0.0010144077241420746, 0.0010144077241420746, 0.0010144077241420746]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010144077241420746

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00101441
Iteration 2/1000 | Loss: 0.00006581
Iteration 3/1000 | Loss: 0.00004439
Iteration 4/1000 | Loss: 0.00003990
Iteration 5/1000 | Loss: 0.00003822
Iteration 6/1000 | Loss: 0.00003734
Iteration 7/1000 | Loss: 0.00003659
Iteration 8/1000 | Loss: 0.00003593
Iteration 9/1000 | Loss: 0.00003544
Iteration 10/1000 | Loss: 0.00003506
Iteration 11/1000 | Loss: 0.00003475
Iteration 12/1000 | Loss: 0.00003450
Iteration 13/1000 | Loss: 0.00003430
Iteration 14/1000 | Loss: 0.00003415
Iteration 15/1000 | Loss: 0.00003399
Iteration 16/1000 | Loss: 0.00003393
Iteration 17/1000 | Loss: 0.00003381
Iteration 18/1000 | Loss: 0.00003377
Iteration 19/1000 | Loss: 0.00003370
Iteration 20/1000 | Loss: 0.00003366
Iteration 21/1000 | Loss: 0.00003365
Iteration 22/1000 | Loss: 0.00003361
Iteration 23/1000 | Loss: 0.00003358
Iteration 24/1000 | Loss: 0.00003357
Iteration 25/1000 | Loss: 0.00003357
Iteration 26/1000 | Loss: 0.00003352
Iteration 27/1000 | Loss: 0.00003349
Iteration 28/1000 | Loss: 0.00003348
Iteration 29/1000 | Loss: 0.00003348
Iteration 30/1000 | Loss: 0.00003347
Iteration 31/1000 | Loss: 0.00003345
Iteration 32/1000 | Loss: 0.00003345
Iteration 33/1000 | Loss: 0.00003343
Iteration 34/1000 | Loss: 0.00003343
Iteration 35/1000 | Loss: 0.00003343
Iteration 36/1000 | Loss: 0.00003343
Iteration 37/1000 | Loss: 0.00003343
Iteration 38/1000 | Loss: 0.00003343
Iteration 39/1000 | Loss: 0.00003343
Iteration 40/1000 | Loss: 0.00003342
Iteration 41/1000 | Loss: 0.00003342
Iteration 42/1000 | Loss: 0.00003342
Iteration 43/1000 | Loss: 0.00003342
Iteration 44/1000 | Loss: 0.00003342
Iteration 45/1000 | Loss: 0.00003342
Iteration 46/1000 | Loss: 0.00003342
Iteration 47/1000 | Loss: 0.00003342
Iteration 48/1000 | Loss: 0.00003342
Iteration 49/1000 | Loss: 0.00003342
Iteration 50/1000 | Loss: 0.00003342
Iteration 51/1000 | Loss: 0.00003341
Iteration 52/1000 | Loss: 0.00003341
Iteration 53/1000 | Loss: 0.00003341
Iteration 54/1000 | Loss: 0.00003340
Iteration 55/1000 | Loss: 0.00003339
Iteration 56/1000 | Loss: 0.00003339
Iteration 57/1000 | Loss: 0.00003339
Iteration 58/1000 | Loss: 0.00003339
Iteration 59/1000 | Loss: 0.00003339
Iteration 60/1000 | Loss: 0.00003339
Iteration 61/1000 | Loss: 0.00003338
Iteration 62/1000 | Loss: 0.00003338
Iteration 63/1000 | Loss: 0.00003338
Iteration 64/1000 | Loss: 0.00003338
Iteration 65/1000 | Loss: 0.00003338
Iteration 66/1000 | Loss: 0.00003338
Iteration 67/1000 | Loss: 0.00003338
Iteration 68/1000 | Loss: 0.00003338
Iteration 69/1000 | Loss: 0.00003338
Iteration 70/1000 | Loss: 0.00003337
Iteration 71/1000 | Loss: 0.00003337
Iteration 72/1000 | Loss: 0.00003336
Iteration 73/1000 | Loss: 0.00003336
Iteration 74/1000 | Loss: 0.00003336
Iteration 75/1000 | Loss: 0.00003336
Iteration 76/1000 | Loss: 0.00003336
Iteration 77/1000 | Loss: 0.00003336
Iteration 78/1000 | Loss: 0.00003336
Iteration 79/1000 | Loss: 0.00003336
Iteration 80/1000 | Loss: 0.00003335
Iteration 81/1000 | Loss: 0.00003335
Iteration 82/1000 | Loss: 0.00003335
Iteration 83/1000 | Loss: 0.00003335
Iteration 84/1000 | Loss: 0.00003335
Iteration 85/1000 | Loss: 0.00003335
Iteration 86/1000 | Loss: 0.00003335
Iteration 87/1000 | Loss: 0.00003335
Iteration 88/1000 | Loss: 0.00003334
Iteration 89/1000 | Loss: 0.00003334
Iteration 90/1000 | Loss: 0.00003334
Iteration 91/1000 | Loss: 0.00003334
Iteration 92/1000 | Loss: 0.00003334
Iteration 93/1000 | Loss: 0.00003334
Iteration 94/1000 | Loss: 0.00003334
Iteration 95/1000 | Loss: 0.00003334
Iteration 96/1000 | Loss: 0.00003333
Iteration 97/1000 | Loss: 0.00003333
Iteration 98/1000 | Loss: 0.00003333
Iteration 99/1000 | Loss: 0.00003333
Iteration 100/1000 | Loss: 0.00003333
Iteration 101/1000 | Loss: 0.00003333
Iteration 102/1000 | Loss: 0.00003332
Iteration 103/1000 | Loss: 0.00003331
Iteration 104/1000 | Loss: 0.00003331
Iteration 105/1000 | Loss: 0.00003331
Iteration 106/1000 | Loss: 0.00003331
Iteration 107/1000 | Loss: 0.00003331
Iteration 108/1000 | Loss: 0.00003331
Iteration 109/1000 | Loss: 0.00003331
Iteration 110/1000 | Loss: 0.00003331
Iteration 111/1000 | Loss: 0.00003331
Iteration 112/1000 | Loss: 0.00003331
Iteration 113/1000 | Loss: 0.00003331
Iteration 114/1000 | Loss: 0.00003331
Iteration 115/1000 | Loss: 0.00003331
Iteration 116/1000 | Loss: 0.00003330
Iteration 117/1000 | Loss: 0.00003330
Iteration 118/1000 | Loss: 0.00003330
Iteration 119/1000 | Loss: 0.00003330
Iteration 120/1000 | Loss: 0.00003330
Iteration 121/1000 | Loss: 0.00003330
Iteration 122/1000 | Loss: 0.00003330
Iteration 123/1000 | Loss: 0.00003330
Iteration 124/1000 | Loss: 0.00003330
Iteration 125/1000 | Loss: 0.00003330
Iteration 126/1000 | Loss: 0.00003330
Iteration 127/1000 | Loss: 0.00003329
Iteration 128/1000 | Loss: 0.00003329
Iteration 129/1000 | Loss: 0.00003329
Iteration 130/1000 | Loss: 0.00003328
Iteration 131/1000 | Loss: 0.00003328
Iteration 132/1000 | Loss: 0.00003328
Iteration 133/1000 | Loss: 0.00003328
Iteration 134/1000 | Loss: 0.00003328
Iteration 135/1000 | Loss: 0.00003327
Iteration 136/1000 | Loss: 0.00003327
Iteration 137/1000 | Loss: 0.00003327
Iteration 138/1000 | Loss: 0.00003327
Iteration 139/1000 | Loss: 0.00003326
Iteration 140/1000 | Loss: 0.00003326
Iteration 141/1000 | Loss: 0.00003326
Iteration 142/1000 | Loss: 0.00003326
Iteration 143/1000 | Loss: 0.00003326
Iteration 144/1000 | Loss: 0.00003325
Iteration 145/1000 | Loss: 0.00003325
Iteration 146/1000 | Loss: 0.00003325
Iteration 147/1000 | Loss: 0.00003325
Iteration 148/1000 | Loss: 0.00003325
Iteration 149/1000 | Loss: 0.00003325
Iteration 150/1000 | Loss: 0.00003325
Iteration 151/1000 | Loss: 0.00003325
Iteration 152/1000 | Loss: 0.00003325
Iteration 153/1000 | Loss: 0.00003325
Iteration 154/1000 | Loss: 0.00003325
Iteration 155/1000 | Loss: 0.00003325
Iteration 156/1000 | Loss: 0.00003325
Iteration 157/1000 | Loss: 0.00003324
Iteration 158/1000 | Loss: 0.00003324
Iteration 159/1000 | Loss: 0.00003324
Iteration 160/1000 | Loss: 0.00003324
Iteration 161/1000 | Loss: 0.00003324
Iteration 162/1000 | Loss: 0.00003324
Iteration 163/1000 | Loss: 0.00003324
Iteration 164/1000 | Loss: 0.00003323
Iteration 165/1000 | Loss: 0.00003323
Iteration 166/1000 | Loss: 0.00003323
Iteration 167/1000 | Loss: 0.00003323
Iteration 168/1000 | Loss: 0.00003323
Iteration 169/1000 | Loss: 0.00003323
Iteration 170/1000 | Loss: 0.00003323
Iteration 171/1000 | Loss: 0.00003323
Iteration 172/1000 | Loss: 0.00003322
Iteration 173/1000 | Loss: 0.00003322
Iteration 174/1000 | Loss: 0.00003322
Iteration 175/1000 | Loss: 0.00003322
Iteration 176/1000 | Loss: 0.00003322
Iteration 177/1000 | Loss: 0.00003322
Iteration 178/1000 | Loss: 0.00003322
Iteration 179/1000 | Loss: 0.00003322
Iteration 180/1000 | Loss: 0.00003321
Iteration 181/1000 | Loss: 0.00003321
Iteration 182/1000 | Loss: 0.00003321
Iteration 183/1000 | Loss: 0.00003321
Iteration 184/1000 | Loss: 0.00003321
Iteration 185/1000 | Loss: 0.00003321
Iteration 186/1000 | Loss: 0.00003321
Iteration 187/1000 | Loss: 0.00003321
Iteration 188/1000 | Loss: 0.00003320
Iteration 189/1000 | Loss: 0.00003320
Iteration 190/1000 | Loss: 0.00003320
Iteration 191/1000 | Loss: 0.00003320
Iteration 192/1000 | Loss: 0.00003320
Iteration 193/1000 | Loss: 0.00003320
Iteration 194/1000 | Loss: 0.00003320
Iteration 195/1000 | Loss: 0.00003320
Iteration 196/1000 | Loss: 0.00003320
Iteration 197/1000 | Loss: 0.00003320
Iteration 198/1000 | Loss: 0.00003320
Iteration 199/1000 | Loss: 0.00003320
Iteration 200/1000 | Loss: 0.00003320
Iteration 201/1000 | Loss: 0.00003320
Iteration 202/1000 | Loss: 0.00003320
Iteration 203/1000 | Loss: 0.00003320
Iteration 204/1000 | Loss: 0.00003320
Iteration 205/1000 | Loss: 0.00003320
Iteration 206/1000 | Loss: 0.00003320
Iteration 207/1000 | Loss: 0.00003320
Iteration 208/1000 | Loss: 0.00003320
Iteration 209/1000 | Loss: 0.00003320
Iteration 210/1000 | Loss: 0.00003320
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 210. Stopping optimization.
Last 5 losses: [3.3197138691321015e-05, 3.3197138691321015e-05, 3.3197138691321015e-05, 3.3197138691321015e-05, 3.3197138691321015e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.3197138691321015e-05

Optimization complete. Final v2v error: 4.7054219245910645 mm

Highest mean error: 5.856147766113281 mm for frame 0

Lowest mean error: 4.188445568084717 mm for frame 72

Saving results

Total time: 54.327680826187134
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_018/1069/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_018/1069.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_018/1069
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01090424
Iteration 2/25 | Loss: 0.00156421
Iteration 3/25 | Loss: 0.00128451
Iteration 4/25 | Loss: 0.00126988
Iteration 5/25 | Loss: 0.00127373
Iteration 6/25 | Loss: 0.00126931
Iteration 7/25 | Loss: 0.00126688
Iteration 8/25 | Loss: 0.00126658
Iteration 9/25 | Loss: 0.00126658
Iteration 10/25 | Loss: 0.00126658
Iteration 11/25 | Loss: 0.00126658
Iteration 12/25 | Loss: 0.00126658
Iteration 13/25 | Loss: 0.00126658
Iteration 14/25 | Loss: 0.00126658
Iteration 15/25 | Loss: 0.00126658
Iteration 16/25 | Loss: 0.00126658
Iteration 17/25 | Loss: 0.00126657
Iteration 18/25 | Loss: 0.00126657
Iteration 19/25 | Loss: 0.00126657
Iteration 20/25 | Loss: 0.00126657
Iteration 21/25 | Loss: 0.00126657
Iteration 22/25 | Loss: 0.00126657
Iteration 23/25 | Loss: 0.00126657
Iteration 24/25 | Loss: 0.00126657
Iteration 25/25 | Loss: 0.00126657

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.84988058
Iteration 2/25 | Loss: 0.00060187
Iteration 3/25 | Loss: 0.00060185
Iteration 4/25 | Loss: 0.00060185
Iteration 5/25 | Loss: 0.00060185
Iteration 6/25 | Loss: 0.00060185
Iteration 7/25 | Loss: 0.00060185
Iteration 8/25 | Loss: 0.00060185
Iteration 9/25 | Loss: 0.00060185
Iteration 10/25 | Loss: 0.00060185
Iteration 11/25 | Loss: 0.00060185
Iteration 12/25 | Loss: 0.00060185
Iteration 13/25 | Loss: 0.00060185
Iteration 14/25 | Loss: 0.00060185
Iteration 15/25 | Loss: 0.00060185
Iteration 16/25 | Loss: 0.00060185
Iteration 17/25 | Loss: 0.00060185
Iteration 18/25 | Loss: 0.00060185
Iteration 19/25 | Loss: 0.00060185
Iteration 20/25 | Loss: 0.00060185
Iteration 21/25 | Loss: 0.00060185
Iteration 22/25 | Loss: 0.00060185
Iteration 23/25 | Loss: 0.00060185
Iteration 24/25 | Loss: 0.00060185
Iteration 25/25 | Loss: 0.00060185

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00060185
Iteration 2/1000 | Loss: 0.00003738
Iteration 3/1000 | Loss: 0.00002553
Iteration 4/1000 | Loss: 0.00002272
Iteration 5/1000 | Loss: 0.00002172
Iteration 6/1000 | Loss: 0.00002133
Iteration 7/1000 | Loss: 0.00002085
Iteration 8/1000 | Loss: 0.00002053
Iteration 9/1000 | Loss: 0.00002029
Iteration 10/1000 | Loss: 0.00002010
Iteration 11/1000 | Loss: 0.00002001
Iteration 12/1000 | Loss: 0.00001992
Iteration 13/1000 | Loss: 0.00001990
Iteration 14/1000 | Loss: 0.00001983
Iteration 15/1000 | Loss: 0.00001983
Iteration 16/1000 | Loss: 0.00001982
Iteration 17/1000 | Loss: 0.00001974
Iteration 18/1000 | Loss: 0.00001965
Iteration 19/1000 | Loss: 0.00001962
Iteration 20/1000 | Loss: 0.00001962
Iteration 21/1000 | Loss: 0.00001962
Iteration 22/1000 | Loss: 0.00001962
Iteration 23/1000 | Loss: 0.00001962
Iteration 24/1000 | Loss: 0.00001961
Iteration 25/1000 | Loss: 0.00001961
Iteration 26/1000 | Loss: 0.00001961
Iteration 27/1000 | Loss: 0.00001959
Iteration 28/1000 | Loss: 0.00001959
Iteration 29/1000 | Loss: 0.00001959
Iteration 30/1000 | Loss: 0.00001959
Iteration 31/1000 | Loss: 0.00001959
Iteration 32/1000 | Loss: 0.00001959
Iteration 33/1000 | Loss: 0.00001959
Iteration 34/1000 | Loss: 0.00001959
Iteration 35/1000 | Loss: 0.00001959
Iteration 36/1000 | Loss: 0.00001958
Iteration 37/1000 | Loss: 0.00001958
Iteration 38/1000 | Loss: 0.00001958
Iteration 39/1000 | Loss: 0.00001958
Iteration 40/1000 | Loss: 0.00001957
Iteration 41/1000 | Loss: 0.00001957
Iteration 42/1000 | Loss: 0.00001957
Iteration 43/1000 | Loss: 0.00001957
Iteration 44/1000 | Loss: 0.00001957
Iteration 45/1000 | Loss: 0.00001957
Iteration 46/1000 | Loss: 0.00001957
Iteration 47/1000 | Loss: 0.00001957
Iteration 48/1000 | Loss: 0.00001956
Iteration 49/1000 | Loss: 0.00001956
Iteration 50/1000 | Loss: 0.00001956
Iteration 51/1000 | Loss: 0.00001956
Iteration 52/1000 | Loss: 0.00001956
Iteration 53/1000 | Loss: 0.00001956
Iteration 54/1000 | Loss: 0.00001956
Iteration 55/1000 | Loss: 0.00001956
Iteration 56/1000 | Loss: 0.00001956
Iteration 57/1000 | Loss: 0.00001956
Iteration 58/1000 | Loss: 0.00001956
Iteration 59/1000 | Loss: 0.00001956
Iteration 60/1000 | Loss: 0.00001956
Iteration 61/1000 | Loss: 0.00001956
Iteration 62/1000 | Loss: 0.00001956
Iteration 63/1000 | Loss: 0.00001956
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 63. Stopping optimization.
Last 5 losses: [1.9557070118025877e-05, 1.9557070118025877e-05, 1.9557070118025877e-05, 1.9557070118025877e-05, 1.9557070118025877e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9557070118025877e-05

Optimization complete. Final v2v error: 3.5913450717926025 mm

Highest mean error: 4.084381580352783 mm for frame 104

Lowest mean error: 3.2540884017944336 mm for frame 52

Saving results

Total time: 35.70931935310364
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_018/1021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_018/1021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_018/1021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00462289
Iteration 2/25 | Loss: 0.00142471
Iteration 3/25 | Loss: 0.00123626
Iteration 4/25 | Loss: 0.00120851
Iteration 5/25 | Loss: 0.00120004
Iteration 6/25 | Loss: 0.00119851
Iteration 7/25 | Loss: 0.00119793
Iteration 8/25 | Loss: 0.00119793
Iteration 9/25 | Loss: 0.00119793
Iteration 10/25 | Loss: 0.00119793
Iteration 11/25 | Loss: 0.00119793
Iteration 12/25 | Loss: 0.00119793
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0011979280970990658, 0.0011979280970990658, 0.0011979280970990658, 0.0011979280970990658, 0.0011979280970990658]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011979280970990658

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.18508410
Iteration 2/25 | Loss: 0.00100572
Iteration 3/25 | Loss: 0.00100570
Iteration 4/25 | Loss: 0.00100570
Iteration 5/25 | Loss: 0.00100570
Iteration 6/25 | Loss: 0.00100570
Iteration 7/25 | Loss: 0.00100570
Iteration 8/25 | Loss: 0.00100570
Iteration 9/25 | Loss: 0.00100570
Iteration 10/25 | Loss: 0.00100570
Iteration 11/25 | Loss: 0.00100570
Iteration 12/25 | Loss: 0.00100570
Iteration 13/25 | Loss: 0.00100570
Iteration 14/25 | Loss: 0.00100570
Iteration 15/25 | Loss: 0.00100570
Iteration 16/25 | Loss: 0.00100570
Iteration 17/25 | Loss: 0.00100570
Iteration 18/25 | Loss: 0.00100570
Iteration 19/25 | Loss: 0.00100570
Iteration 20/25 | Loss: 0.00100570
Iteration 21/25 | Loss: 0.00100570
Iteration 22/25 | Loss: 0.00100570
Iteration 23/25 | Loss: 0.00100570
Iteration 24/25 | Loss: 0.00100570
Iteration 25/25 | Loss: 0.00100570
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0010057012550532818, 0.0010057012550532818, 0.0010057012550532818, 0.0010057012550532818, 0.0010057012550532818]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010057012550532818

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00100570
Iteration 2/1000 | Loss: 0.00006649
Iteration 3/1000 | Loss: 0.00003876
Iteration 4/1000 | Loss: 0.00003079
Iteration 5/1000 | Loss: 0.00002861
Iteration 6/1000 | Loss: 0.00002753
Iteration 7/1000 | Loss: 0.00002641
Iteration 8/1000 | Loss: 0.00002583
Iteration 9/1000 | Loss: 0.00002517
Iteration 10/1000 | Loss: 0.00002468
Iteration 11/1000 | Loss: 0.00002443
Iteration 12/1000 | Loss: 0.00002425
Iteration 13/1000 | Loss: 0.00002412
Iteration 14/1000 | Loss: 0.00002397
Iteration 15/1000 | Loss: 0.00002396
Iteration 16/1000 | Loss: 0.00002396
Iteration 17/1000 | Loss: 0.00002388
Iteration 18/1000 | Loss: 0.00002383
Iteration 19/1000 | Loss: 0.00002378
Iteration 20/1000 | Loss: 0.00002377
Iteration 21/1000 | Loss: 0.00002376
Iteration 22/1000 | Loss: 0.00002376
Iteration 23/1000 | Loss: 0.00002375
Iteration 24/1000 | Loss: 0.00002371
Iteration 25/1000 | Loss: 0.00002371
Iteration 26/1000 | Loss: 0.00002367
Iteration 27/1000 | Loss: 0.00002354
Iteration 28/1000 | Loss: 0.00002353
Iteration 29/1000 | Loss: 0.00002351
Iteration 30/1000 | Loss: 0.00002350
Iteration 31/1000 | Loss: 0.00002349
Iteration 32/1000 | Loss: 0.00002349
Iteration 33/1000 | Loss: 0.00002349
Iteration 34/1000 | Loss: 0.00002349
Iteration 35/1000 | Loss: 0.00002349
Iteration 36/1000 | Loss: 0.00002349
Iteration 37/1000 | Loss: 0.00002349
Iteration 38/1000 | Loss: 0.00002349
Iteration 39/1000 | Loss: 0.00002349
Iteration 40/1000 | Loss: 0.00002348
Iteration 41/1000 | Loss: 0.00002348
Iteration 42/1000 | Loss: 0.00002348
Iteration 43/1000 | Loss: 0.00002344
Iteration 44/1000 | Loss: 0.00002344
Iteration 45/1000 | Loss: 0.00002344
Iteration 46/1000 | Loss: 0.00002343
Iteration 47/1000 | Loss: 0.00002343
Iteration 48/1000 | Loss: 0.00002343
Iteration 49/1000 | Loss: 0.00002343
Iteration 50/1000 | Loss: 0.00002343
Iteration 51/1000 | Loss: 0.00002342
Iteration 52/1000 | Loss: 0.00002342
Iteration 53/1000 | Loss: 0.00002341
Iteration 54/1000 | Loss: 0.00002340
Iteration 55/1000 | Loss: 0.00002337
Iteration 56/1000 | Loss: 0.00002337
Iteration 57/1000 | Loss: 0.00002337
Iteration 58/1000 | Loss: 0.00002337
Iteration 59/1000 | Loss: 0.00002336
Iteration 60/1000 | Loss: 0.00002336
Iteration 61/1000 | Loss: 0.00002336
Iteration 62/1000 | Loss: 0.00002336
Iteration 63/1000 | Loss: 0.00002336
Iteration 64/1000 | Loss: 0.00002336
Iteration 65/1000 | Loss: 0.00002336
Iteration 66/1000 | Loss: 0.00002336
Iteration 67/1000 | Loss: 0.00002335
Iteration 68/1000 | Loss: 0.00002335
Iteration 69/1000 | Loss: 0.00002330
Iteration 70/1000 | Loss: 0.00002330
Iteration 71/1000 | Loss: 0.00002329
Iteration 72/1000 | Loss: 0.00002326
Iteration 73/1000 | Loss: 0.00002326
Iteration 74/1000 | Loss: 0.00002326
Iteration 75/1000 | Loss: 0.00002326
Iteration 76/1000 | Loss: 0.00002325
Iteration 77/1000 | Loss: 0.00002325
Iteration 78/1000 | Loss: 0.00002325
Iteration 79/1000 | Loss: 0.00002325
Iteration 80/1000 | Loss: 0.00002325
Iteration 81/1000 | Loss: 0.00002324
Iteration 82/1000 | Loss: 0.00002324
Iteration 83/1000 | Loss: 0.00002324
Iteration 84/1000 | Loss: 0.00002323
Iteration 85/1000 | Loss: 0.00002323
Iteration 86/1000 | Loss: 0.00002323
Iteration 87/1000 | Loss: 0.00002323
Iteration 88/1000 | Loss: 0.00002323
Iteration 89/1000 | Loss: 0.00002323
Iteration 90/1000 | Loss: 0.00002323
Iteration 91/1000 | Loss: 0.00002323
Iteration 92/1000 | Loss: 0.00002323
Iteration 93/1000 | Loss: 0.00002323
Iteration 94/1000 | Loss: 0.00002322
Iteration 95/1000 | Loss: 0.00002322
Iteration 96/1000 | Loss: 0.00002320
Iteration 97/1000 | Loss: 0.00002320
Iteration 98/1000 | Loss: 0.00002319
Iteration 99/1000 | Loss: 0.00002319
Iteration 100/1000 | Loss: 0.00002319
Iteration 101/1000 | Loss: 0.00002319
Iteration 102/1000 | Loss: 0.00002319
Iteration 103/1000 | Loss: 0.00002318
Iteration 104/1000 | Loss: 0.00002318
Iteration 105/1000 | Loss: 0.00002318
Iteration 106/1000 | Loss: 0.00002318
Iteration 107/1000 | Loss: 0.00002317
Iteration 108/1000 | Loss: 0.00002317
Iteration 109/1000 | Loss: 0.00002317
Iteration 110/1000 | Loss: 0.00002317
Iteration 111/1000 | Loss: 0.00002317
Iteration 112/1000 | Loss: 0.00002317
Iteration 113/1000 | Loss: 0.00002317
Iteration 114/1000 | Loss: 0.00002316
Iteration 115/1000 | Loss: 0.00002316
Iteration 116/1000 | Loss: 0.00002316
Iteration 117/1000 | Loss: 0.00002316
Iteration 118/1000 | Loss: 0.00002316
Iteration 119/1000 | Loss: 0.00002316
Iteration 120/1000 | Loss: 0.00002316
Iteration 121/1000 | Loss: 0.00002315
Iteration 122/1000 | Loss: 0.00002315
Iteration 123/1000 | Loss: 0.00002315
Iteration 124/1000 | Loss: 0.00002315
Iteration 125/1000 | Loss: 0.00002315
Iteration 126/1000 | Loss: 0.00002315
Iteration 127/1000 | Loss: 0.00002315
Iteration 128/1000 | Loss: 0.00002315
Iteration 129/1000 | Loss: 0.00002315
Iteration 130/1000 | Loss: 0.00002315
Iteration 131/1000 | Loss: 0.00002314
Iteration 132/1000 | Loss: 0.00002314
Iteration 133/1000 | Loss: 0.00002314
Iteration 134/1000 | Loss: 0.00002314
Iteration 135/1000 | Loss: 0.00002314
Iteration 136/1000 | Loss: 0.00002314
Iteration 137/1000 | Loss: 0.00002314
Iteration 138/1000 | Loss: 0.00002314
Iteration 139/1000 | Loss: 0.00002314
Iteration 140/1000 | Loss: 0.00002314
Iteration 141/1000 | Loss: 0.00002314
Iteration 142/1000 | Loss: 0.00002314
Iteration 143/1000 | Loss: 0.00002314
Iteration 144/1000 | Loss: 0.00002313
Iteration 145/1000 | Loss: 0.00002313
Iteration 146/1000 | Loss: 0.00002313
Iteration 147/1000 | Loss: 0.00002313
Iteration 148/1000 | Loss: 0.00002313
Iteration 149/1000 | Loss: 0.00002313
Iteration 150/1000 | Loss: 0.00002313
Iteration 151/1000 | Loss: 0.00002313
Iteration 152/1000 | Loss: 0.00002313
Iteration 153/1000 | Loss: 0.00002313
Iteration 154/1000 | Loss: 0.00002312
Iteration 155/1000 | Loss: 0.00002312
Iteration 156/1000 | Loss: 0.00002312
Iteration 157/1000 | Loss: 0.00002312
Iteration 158/1000 | Loss: 0.00002312
Iteration 159/1000 | Loss: 0.00002312
Iteration 160/1000 | Loss: 0.00002312
Iteration 161/1000 | Loss: 0.00002312
Iteration 162/1000 | Loss: 0.00002312
Iteration 163/1000 | Loss: 0.00002312
Iteration 164/1000 | Loss: 0.00002311
Iteration 165/1000 | Loss: 0.00002311
Iteration 166/1000 | Loss: 0.00002311
Iteration 167/1000 | Loss: 0.00002311
Iteration 168/1000 | Loss: 0.00002311
Iteration 169/1000 | Loss: 0.00002311
Iteration 170/1000 | Loss: 0.00002311
Iteration 171/1000 | Loss: 0.00002311
Iteration 172/1000 | Loss: 0.00002311
Iteration 173/1000 | Loss: 0.00002311
Iteration 174/1000 | Loss: 0.00002311
Iteration 175/1000 | Loss: 0.00002311
Iteration 176/1000 | Loss: 0.00002310
Iteration 177/1000 | Loss: 0.00002310
Iteration 178/1000 | Loss: 0.00002310
Iteration 179/1000 | Loss: 0.00002310
Iteration 180/1000 | Loss: 0.00002310
Iteration 181/1000 | Loss: 0.00002310
Iteration 182/1000 | Loss: 0.00002310
Iteration 183/1000 | Loss: 0.00002310
Iteration 184/1000 | Loss: 0.00002310
Iteration 185/1000 | Loss: 0.00002310
Iteration 186/1000 | Loss: 0.00002310
Iteration 187/1000 | Loss: 0.00002309
Iteration 188/1000 | Loss: 0.00002309
Iteration 189/1000 | Loss: 0.00002309
Iteration 190/1000 | Loss: 0.00002309
Iteration 191/1000 | Loss: 0.00002309
Iteration 192/1000 | Loss: 0.00002309
Iteration 193/1000 | Loss: 0.00002309
Iteration 194/1000 | Loss: 0.00002309
Iteration 195/1000 | Loss: 0.00002309
Iteration 196/1000 | Loss: 0.00002309
Iteration 197/1000 | Loss: 0.00002309
Iteration 198/1000 | Loss: 0.00002309
Iteration 199/1000 | Loss: 0.00002309
Iteration 200/1000 | Loss: 0.00002309
Iteration 201/1000 | Loss: 0.00002309
Iteration 202/1000 | Loss: 0.00002309
Iteration 203/1000 | Loss: 0.00002309
Iteration 204/1000 | Loss: 0.00002309
Iteration 205/1000 | Loss: 0.00002309
Iteration 206/1000 | Loss: 0.00002309
Iteration 207/1000 | Loss: 0.00002309
Iteration 208/1000 | Loss: 0.00002309
Iteration 209/1000 | Loss: 0.00002309
Iteration 210/1000 | Loss: 0.00002309
Iteration 211/1000 | Loss: 0.00002309
Iteration 212/1000 | Loss: 0.00002309
Iteration 213/1000 | Loss: 0.00002309
Iteration 214/1000 | Loss: 0.00002309
Iteration 215/1000 | Loss: 0.00002309
Iteration 216/1000 | Loss: 0.00002309
Iteration 217/1000 | Loss: 0.00002309
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 217. Stopping optimization.
Last 5 losses: [2.308732109668199e-05, 2.308732109668199e-05, 2.308732109668199e-05, 2.308732109668199e-05, 2.308732109668199e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.308732109668199e-05

Optimization complete. Final v2v error: 3.854407548904419 mm

Highest mean error: 5.515965461730957 mm for frame 73

Lowest mean error: 3.0179944038391113 mm for frame 35

Saving results

Total time: 49.38621258735657
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_018/1027/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_018/1027.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_018/1027
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01017767
Iteration 2/25 | Loss: 0.00216243
Iteration 3/25 | Loss: 0.00174902
Iteration 4/25 | Loss: 0.00166606
Iteration 5/25 | Loss: 0.00146158
Iteration 6/25 | Loss: 0.00136510
Iteration 7/25 | Loss: 0.00133246
Iteration 8/25 | Loss: 0.00132208
Iteration 9/25 | Loss: 0.00131939
Iteration 10/25 | Loss: 0.00130528
Iteration 11/25 | Loss: 0.00130437
Iteration 12/25 | Loss: 0.00130495
Iteration 13/25 | Loss: 0.00129583
Iteration 14/25 | Loss: 0.00129111
Iteration 15/25 | Loss: 0.00129593
Iteration 16/25 | Loss: 0.00129028
Iteration 17/25 | Loss: 0.00128915
Iteration 18/25 | Loss: 0.00129432
Iteration 19/25 | Loss: 0.00129166
Iteration 20/25 | Loss: 0.00128435
Iteration 21/25 | Loss: 0.00128389
Iteration 22/25 | Loss: 0.00128207
Iteration 23/25 | Loss: 0.00128695
Iteration 24/25 | Loss: 0.00128260
Iteration 25/25 | Loss: 0.00127495

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35935020
Iteration 2/25 | Loss: 0.00207335
Iteration 3/25 | Loss: 0.00164659
Iteration 4/25 | Loss: 0.00164658
Iteration 5/25 | Loss: 0.00164658
Iteration 6/25 | Loss: 0.00164658
Iteration 7/25 | Loss: 0.00164658
Iteration 8/25 | Loss: 0.00164658
Iteration 9/25 | Loss: 0.00164658
Iteration 10/25 | Loss: 0.00164658
Iteration 11/25 | Loss: 0.00164658
Iteration 12/25 | Loss: 0.00164658
Iteration 13/25 | Loss: 0.00164658
Iteration 14/25 | Loss: 0.00164658
Iteration 15/25 | Loss: 0.00164658
Iteration 16/25 | Loss: 0.00164658
Iteration 17/25 | Loss: 0.00164658
Iteration 18/25 | Loss: 0.00164658
Iteration 19/25 | Loss: 0.00164658
Iteration 20/25 | Loss: 0.00164658
Iteration 21/25 | Loss: 0.00164658
Iteration 22/25 | Loss: 0.00164658
Iteration 23/25 | Loss: 0.00164658
Iteration 24/25 | Loss: 0.00164658
Iteration 25/25 | Loss: 0.00164658

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00164658
Iteration 2/1000 | Loss: 0.00049191
Iteration 3/1000 | Loss: 0.00109909
Iteration 4/1000 | Loss: 0.00016681
Iteration 5/1000 | Loss: 0.00101846
Iteration 6/1000 | Loss: 0.00011524
Iteration 7/1000 | Loss: 0.00067696
Iteration 8/1000 | Loss: 0.00012785
Iteration 9/1000 | Loss: 0.00020681
Iteration 10/1000 | Loss: 0.00009112
Iteration 11/1000 | Loss: 0.00032954
Iteration 12/1000 | Loss: 0.00026538
Iteration 13/1000 | Loss: 0.00047712
Iteration 14/1000 | Loss: 0.00025517
Iteration 15/1000 | Loss: 0.00023057
Iteration 16/1000 | Loss: 0.00012469
Iteration 17/1000 | Loss: 0.00017220
Iteration 18/1000 | Loss: 0.00033696
Iteration 19/1000 | Loss: 0.00018534
Iteration 20/1000 | Loss: 0.00009743
Iteration 21/1000 | Loss: 0.00009114
Iteration 22/1000 | Loss: 0.00011193
Iteration 23/1000 | Loss: 0.00007925
Iteration 24/1000 | Loss: 0.00008557
Iteration 25/1000 | Loss: 0.00008573
Iteration 26/1000 | Loss: 0.00015091
Iteration 27/1000 | Loss: 0.00013125
Iteration 28/1000 | Loss: 0.00024709
Iteration 29/1000 | Loss: 0.00035324
Iteration 30/1000 | Loss: 0.00039473
Iteration 31/1000 | Loss: 0.00018424
Iteration 32/1000 | Loss: 0.00013071
Iteration 33/1000 | Loss: 0.00011218
Iteration 34/1000 | Loss: 0.00033161
Iteration 35/1000 | Loss: 0.00016787
Iteration 36/1000 | Loss: 0.00011717
Iteration 37/1000 | Loss: 0.00018686
Iteration 38/1000 | Loss: 0.00010498
Iteration 39/1000 | Loss: 0.00013010
Iteration 40/1000 | Loss: 0.00016320
Iteration 41/1000 | Loss: 0.00010520
Iteration 42/1000 | Loss: 0.00022904
Iteration 43/1000 | Loss: 0.00022015
Iteration 44/1000 | Loss: 0.00015815
Iteration 45/1000 | Loss: 0.00017360
Iteration 46/1000 | Loss: 0.00017019
Iteration 47/1000 | Loss: 0.00015211
Iteration 48/1000 | Loss: 0.00010876
Iteration 49/1000 | Loss: 0.00018166
Iteration 50/1000 | Loss: 0.00018424
Iteration 51/1000 | Loss: 0.00012416
Iteration 52/1000 | Loss: 0.00023116
Iteration 53/1000 | Loss: 0.00023066
Iteration 54/1000 | Loss: 0.00017413
Iteration 55/1000 | Loss: 0.00056846
Iteration 56/1000 | Loss: 0.00019650
Iteration 57/1000 | Loss: 0.00011858
Iteration 58/1000 | Loss: 0.00057030
Iteration 59/1000 | Loss: 0.00079230
Iteration 60/1000 | Loss: 0.00019015
Iteration 61/1000 | Loss: 0.00011562
Iteration 62/1000 | Loss: 0.00014436
Iteration 63/1000 | Loss: 0.00019444
Iteration 64/1000 | Loss: 0.00013935
Iteration 65/1000 | Loss: 0.00013457
Iteration 66/1000 | Loss: 0.00020850
Iteration 67/1000 | Loss: 0.00023831
Iteration 68/1000 | Loss: 0.00012912
Iteration 69/1000 | Loss: 0.00017188
Iteration 70/1000 | Loss: 0.00020501
Iteration 71/1000 | Loss: 0.00017101
Iteration 72/1000 | Loss: 0.00013670
Iteration 73/1000 | Loss: 0.00016824
Iteration 74/1000 | Loss: 0.00014716
Iteration 75/1000 | Loss: 0.00016769
Iteration 76/1000 | Loss: 0.00024676
Iteration 77/1000 | Loss: 0.00017583
Iteration 78/1000 | Loss: 0.00023284
Iteration 79/1000 | Loss: 0.00017372
Iteration 80/1000 | Loss: 0.00018863
Iteration 81/1000 | Loss: 0.00024951
Iteration 82/1000 | Loss: 0.00043054
Iteration 83/1000 | Loss: 0.00020666
Iteration 84/1000 | Loss: 0.00018928
Iteration 85/1000 | Loss: 0.00029292
Iteration 86/1000 | Loss: 0.00016759
Iteration 87/1000 | Loss: 0.00016708
Iteration 88/1000 | Loss: 0.00010162
Iteration 89/1000 | Loss: 0.00007250
Iteration 90/1000 | Loss: 0.00014836
Iteration 91/1000 | Loss: 0.00011316
Iteration 92/1000 | Loss: 0.00015984
Iteration 93/1000 | Loss: 0.00016061
Iteration 94/1000 | Loss: 0.00015872
Iteration 95/1000 | Loss: 0.00016317
Iteration 96/1000 | Loss: 0.00020913
Iteration 97/1000 | Loss: 0.00015533
Iteration 98/1000 | Loss: 0.00030475
Iteration 99/1000 | Loss: 0.00022326
Iteration 100/1000 | Loss: 0.00010097
Iteration 101/1000 | Loss: 0.00027926
Iteration 102/1000 | Loss: 0.00052159
Iteration 103/1000 | Loss: 0.00043641
Iteration 104/1000 | Loss: 0.00057186
Iteration 105/1000 | Loss: 0.00043446
Iteration 106/1000 | Loss: 0.00012103
Iteration 107/1000 | Loss: 0.00010486
Iteration 108/1000 | Loss: 0.00034695
Iteration 109/1000 | Loss: 0.00007420
Iteration 110/1000 | Loss: 0.00011992
Iteration 111/1000 | Loss: 0.00006711
Iteration 112/1000 | Loss: 0.00008066
Iteration 113/1000 | Loss: 0.00006446
Iteration 114/1000 | Loss: 0.00007709
Iteration 115/1000 | Loss: 0.00008491
Iteration 116/1000 | Loss: 0.00029872
Iteration 117/1000 | Loss: 0.00032289
Iteration 118/1000 | Loss: 0.00014629
Iteration 119/1000 | Loss: 0.00016818
Iteration 120/1000 | Loss: 0.00016682
Iteration 121/1000 | Loss: 0.00016200
Iteration 122/1000 | Loss: 0.00009287
Iteration 123/1000 | Loss: 0.00007907
Iteration 124/1000 | Loss: 0.00006264
Iteration 125/1000 | Loss: 0.00039705
Iteration 126/1000 | Loss: 0.00073668
Iteration 127/1000 | Loss: 0.00201590
Iteration 128/1000 | Loss: 0.00208206
Iteration 129/1000 | Loss: 0.00148381
Iteration 130/1000 | Loss: 0.00076740
Iteration 131/1000 | Loss: 0.00008093
Iteration 132/1000 | Loss: 0.00007227
Iteration 133/1000 | Loss: 0.00042043
Iteration 134/1000 | Loss: 0.00006721
Iteration 135/1000 | Loss: 0.00006267
Iteration 136/1000 | Loss: 0.00006440
Iteration 137/1000 | Loss: 0.00023626
Iteration 138/1000 | Loss: 0.00037427
Iteration 139/1000 | Loss: 0.00036325
Iteration 140/1000 | Loss: 0.00035678
Iteration 141/1000 | Loss: 0.00023510
Iteration 142/1000 | Loss: 0.00028598
Iteration 143/1000 | Loss: 0.00033602
Iteration 144/1000 | Loss: 0.00024596
Iteration 145/1000 | Loss: 0.00041900
Iteration 146/1000 | Loss: 0.00034938
Iteration 147/1000 | Loss: 0.00025338
Iteration 148/1000 | Loss: 0.00033814
Iteration 149/1000 | Loss: 0.00007886
Iteration 150/1000 | Loss: 0.00011532
Iteration 151/1000 | Loss: 0.00008676
Iteration 152/1000 | Loss: 0.00005895
Iteration 153/1000 | Loss: 0.00018337
Iteration 154/1000 | Loss: 0.00032799
Iteration 155/1000 | Loss: 0.00019393
Iteration 156/1000 | Loss: 0.00024057
Iteration 157/1000 | Loss: 0.00024540
Iteration 158/1000 | Loss: 0.00006612
Iteration 159/1000 | Loss: 0.00006515
Iteration 160/1000 | Loss: 0.00006187
Iteration 161/1000 | Loss: 0.00056715
Iteration 162/1000 | Loss: 0.00081170
Iteration 163/1000 | Loss: 0.00053306
Iteration 164/1000 | Loss: 0.00019615
Iteration 165/1000 | Loss: 0.00092507
Iteration 166/1000 | Loss: 0.00011606
Iteration 167/1000 | Loss: 0.00022142
Iteration 168/1000 | Loss: 0.00006380
Iteration 169/1000 | Loss: 0.00005864
Iteration 170/1000 | Loss: 0.00005937
Iteration 171/1000 | Loss: 0.00008600
Iteration 172/1000 | Loss: 0.00007850
Iteration 173/1000 | Loss: 0.00005241
Iteration 174/1000 | Loss: 0.00005771
Iteration 175/1000 | Loss: 0.00005144
Iteration 176/1000 | Loss: 0.00043707
Iteration 177/1000 | Loss: 0.00050496
Iteration 178/1000 | Loss: 0.00049489
Iteration 179/1000 | Loss: 0.00044408
Iteration 180/1000 | Loss: 0.00034788
Iteration 181/1000 | Loss: 0.00042033
Iteration 182/1000 | Loss: 0.00005992
Iteration 183/1000 | Loss: 0.00006629
Iteration 184/1000 | Loss: 0.00009580
Iteration 185/1000 | Loss: 0.00009666
Iteration 186/1000 | Loss: 0.00004969
Iteration 187/1000 | Loss: 0.00006211
Iteration 188/1000 | Loss: 0.00007664
Iteration 189/1000 | Loss: 0.00006080
Iteration 190/1000 | Loss: 0.00004690
Iteration 191/1000 | Loss: 0.00009060
Iteration 192/1000 | Loss: 0.00014241
Iteration 193/1000 | Loss: 0.00009814
Iteration 194/1000 | Loss: 0.00005798
Iteration 195/1000 | Loss: 0.00007148
Iteration 196/1000 | Loss: 0.00005003
Iteration 197/1000 | Loss: 0.00004864
Iteration 198/1000 | Loss: 0.00004779
Iteration 199/1000 | Loss: 0.00006211
Iteration 200/1000 | Loss: 0.00007680
Iteration 201/1000 | Loss: 0.00004701
Iteration 202/1000 | Loss: 0.00005574
Iteration 203/1000 | Loss: 0.00037723
Iteration 204/1000 | Loss: 0.00011061
Iteration 205/1000 | Loss: 0.00026505
Iteration 206/1000 | Loss: 0.00007719
Iteration 207/1000 | Loss: 0.00004971
Iteration 208/1000 | Loss: 0.00004679
Iteration 209/1000 | Loss: 0.00004899
Iteration 210/1000 | Loss: 0.00005616
Iteration 211/1000 | Loss: 0.00004253
Iteration 212/1000 | Loss: 0.00008227
Iteration 213/1000 | Loss: 0.00004662
Iteration 214/1000 | Loss: 0.00004184
Iteration 215/1000 | Loss: 0.00004582
Iteration 216/1000 | Loss: 0.00004154
Iteration 217/1000 | Loss: 0.00005132
Iteration 218/1000 | Loss: 0.00004129
Iteration 219/1000 | Loss: 0.00004122
Iteration 220/1000 | Loss: 0.00004116
Iteration 221/1000 | Loss: 0.00004116
Iteration 222/1000 | Loss: 0.00006954
Iteration 223/1000 | Loss: 0.00008379
Iteration 224/1000 | Loss: 0.00004390
Iteration 225/1000 | Loss: 0.00005374
Iteration 226/1000 | Loss: 0.00004444
Iteration 227/1000 | Loss: 0.00004104
Iteration 228/1000 | Loss: 0.00004102
Iteration 229/1000 | Loss: 0.00004101
Iteration 230/1000 | Loss: 0.00006092
Iteration 231/1000 | Loss: 0.00024472
Iteration 232/1000 | Loss: 0.00023309
Iteration 233/1000 | Loss: 0.00030662
Iteration 234/1000 | Loss: 0.00022539
Iteration 235/1000 | Loss: 0.00004216
Iteration 236/1000 | Loss: 0.00005950
Iteration 237/1000 | Loss: 0.00004109
Iteration 238/1000 | Loss: 0.00004095
Iteration 239/1000 | Loss: 0.00004094
Iteration 240/1000 | Loss: 0.00004094
Iteration 241/1000 | Loss: 0.00004092
Iteration 242/1000 | Loss: 0.00004091
Iteration 243/1000 | Loss: 0.00004088
Iteration 244/1000 | Loss: 0.00010831
Iteration 245/1000 | Loss: 0.00006744
Iteration 246/1000 | Loss: 0.00004086
Iteration 247/1000 | Loss: 0.00004083
Iteration 248/1000 | Loss: 0.00004675
Iteration 249/1000 | Loss: 0.00004082
Iteration 250/1000 | Loss: 0.00004082
Iteration 251/1000 | Loss: 0.00004082
Iteration 252/1000 | Loss: 0.00004082
Iteration 253/1000 | Loss: 0.00004082
Iteration 254/1000 | Loss: 0.00004082
Iteration 255/1000 | Loss: 0.00004082
Iteration 256/1000 | Loss: 0.00004082
Iteration 257/1000 | Loss: 0.00004082
Iteration 258/1000 | Loss: 0.00004082
Iteration 259/1000 | Loss: 0.00004082
Iteration 260/1000 | Loss: 0.00004081
Iteration 261/1000 | Loss: 0.00004081
Iteration 262/1000 | Loss: 0.00004081
Iteration 263/1000 | Loss: 0.00004081
Iteration 264/1000 | Loss: 0.00004081
Iteration 265/1000 | Loss: 0.00004081
Iteration 266/1000 | Loss: 0.00004081
Iteration 267/1000 | Loss: 0.00004081
Iteration 268/1000 | Loss: 0.00004081
Iteration 269/1000 | Loss: 0.00004081
Iteration 270/1000 | Loss: 0.00004080
Iteration 271/1000 | Loss: 0.00004080
Iteration 272/1000 | Loss: 0.00004080
Iteration 273/1000 | Loss: 0.00004080
Iteration 274/1000 | Loss: 0.00004080
Iteration 275/1000 | Loss: 0.00004080
Iteration 276/1000 | Loss: 0.00004080
Iteration 277/1000 | Loss: 0.00004080
Iteration 278/1000 | Loss: 0.00004080
Iteration 279/1000 | Loss: 0.00004080
Iteration 280/1000 | Loss: 0.00004080
Iteration 281/1000 | Loss: 0.00004080
Iteration 282/1000 | Loss: 0.00004080
Iteration 283/1000 | Loss: 0.00004080
Iteration 284/1000 | Loss: 0.00004079
Iteration 285/1000 | Loss: 0.00004079
Iteration 286/1000 | Loss: 0.00004079
Iteration 287/1000 | Loss: 0.00004079
Iteration 288/1000 | Loss: 0.00004079
Iteration 289/1000 | Loss: 0.00004079
Iteration 290/1000 | Loss: 0.00004079
Iteration 291/1000 | Loss: 0.00004079
Iteration 292/1000 | Loss: 0.00004079
Iteration 293/1000 | Loss: 0.00004079
Iteration 294/1000 | Loss: 0.00004079
Iteration 295/1000 | Loss: 0.00004079
Iteration 296/1000 | Loss: 0.00004079
Iteration 297/1000 | Loss: 0.00004079
Iteration 298/1000 | Loss: 0.00004079
Iteration 299/1000 | Loss: 0.00004079
Iteration 300/1000 | Loss: 0.00004079
Iteration 301/1000 | Loss: 0.00004079
Iteration 302/1000 | Loss: 0.00004079
Iteration 303/1000 | Loss: 0.00004079
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 303. Stopping optimization.
Last 5 losses: [4.079226346220821e-05, 4.079226346220821e-05, 4.079226346220821e-05, 4.079226346220821e-05, 4.079226346220821e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.079226346220821e-05

Optimization complete. Final v2v error: 3.624533176422119 mm

Highest mean error: 10.44998550415039 mm for frame 201

Lowest mean error: 2.7100555896759033 mm for frame 93

Saving results

Total time: 437.37268233299255
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_018/1004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_018/1004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_018/1004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00847409
Iteration 2/25 | Loss: 0.00145455
Iteration 3/25 | Loss: 0.00124286
Iteration 4/25 | Loss: 0.00121857
Iteration 5/25 | Loss: 0.00121200
Iteration 6/25 | Loss: 0.00122312
Iteration 7/25 | Loss: 0.00122145
Iteration 8/25 | Loss: 0.00120822
Iteration 9/25 | Loss: 0.00120222
Iteration 10/25 | Loss: 0.00119568
Iteration 11/25 | Loss: 0.00118714
Iteration 12/25 | Loss: 0.00117983
Iteration 13/25 | Loss: 0.00119748
Iteration 14/25 | Loss: 0.00118473
Iteration 15/25 | Loss: 0.00116797
Iteration 16/25 | Loss: 0.00116538
Iteration 17/25 | Loss: 0.00116492
Iteration 18/25 | Loss: 0.00116474
Iteration 19/25 | Loss: 0.00116465
Iteration 20/25 | Loss: 0.00116465
Iteration 21/25 | Loss: 0.00116465
Iteration 22/25 | Loss: 0.00116465
Iteration 23/25 | Loss: 0.00116465
Iteration 24/25 | Loss: 0.00116465
Iteration 25/25 | Loss: 0.00116465

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.16963148
Iteration 2/25 | Loss: 0.00081213
Iteration 3/25 | Loss: 0.00081208
Iteration 4/25 | Loss: 0.00081208
Iteration 5/25 | Loss: 0.00081208
Iteration 6/25 | Loss: 0.00081208
Iteration 7/25 | Loss: 0.00081208
Iteration 8/25 | Loss: 0.00081208
Iteration 9/25 | Loss: 0.00081208
Iteration 10/25 | Loss: 0.00081208
Iteration 11/25 | Loss: 0.00081208
Iteration 12/25 | Loss: 0.00081208
Iteration 13/25 | Loss: 0.00081208
Iteration 14/25 | Loss: 0.00081208
Iteration 15/25 | Loss: 0.00081208
Iteration 16/25 | Loss: 0.00081208
Iteration 17/25 | Loss: 0.00081208
Iteration 18/25 | Loss: 0.00081208
Iteration 19/25 | Loss: 0.00081208
Iteration 20/25 | Loss: 0.00081208
Iteration 21/25 | Loss: 0.00081208
Iteration 22/25 | Loss: 0.00081208
Iteration 23/25 | Loss: 0.00081208
Iteration 24/25 | Loss: 0.00081208
Iteration 25/25 | Loss: 0.00081208

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00081208
Iteration 2/1000 | Loss: 0.00002325
Iteration 3/1000 | Loss: 0.00001628
Iteration 4/1000 | Loss: 0.00001458
Iteration 5/1000 | Loss: 0.00001369
Iteration 6/1000 | Loss: 0.00001328
Iteration 7/1000 | Loss: 0.00001293
Iteration 8/1000 | Loss: 0.00001270
Iteration 9/1000 | Loss: 0.00001252
Iteration 10/1000 | Loss: 0.00001235
Iteration 11/1000 | Loss: 0.00001228
Iteration 12/1000 | Loss: 0.00001225
Iteration 13/1000 | Loss: 0.00001225
Iteration 14/1000 | Loss: 0.00001224
Iteration 15/1000 | Loss: 0.00001224
Iteration 16/1000 | Loss: 0.00001224
Iteration 17/1000 | Loss: 0.00001224
Iteration 18/1000 | Loss: 0.00001220
Iteration 19/1000 | Loss: 0.00001220
Iteration 20/1000 | Loss: 0.00001220
Iteration 21/1000 | Loss: 0.00001220
Iteration 22/1000 | Loss: 0.00001219
Iteration 23/1000 | Loss: 0.00001214
Iteration 24/1000 | Loss: 0.00001214
Iteration 25/1000 | Loss: 0.00001212
Iteration 26/1000 | Loss: 0.00001211
Iteration 27/1000 | Loss: 0.00001210
Iteration 28/1000 | Loss: 0.00001209
Iteration 29/1000 | Loss: 0.00001208
Iteration 30/1000 | Loss: 0.00001206
Iteration 31/1000 | Loss: 0.00001205
Iteration 32/1000 | Loss: 0.00001205
Iteration 33/1000 | Loss: 0.00001203
Iteration 34/1000 | Loss: 0.00001203
Iteration 35/1000 | Loss: 0.00001203
Iteration 36/1000 | Loss: 0.00001202
Iteration 37/1000 | Loss: 0.00001202
Iteration 38/1000 | Loss: 0.00001202
Iteration 39/1000 | Loss: 0.00001202
Iteration 40/1000 | Loss: 0.00001202
Iteration 41/1000 | Loss: 0.00001201
Iteration 42/1000 | Loss: 0.00001201
Iteration 43/1000 | Loss: 0.00001201
Iteration 44/1000 | Loss: 0.00001200
Iteration 45/1000 | Loss: 0.00001200
Iteration 46/1000 | Loss: 0.00001199
Iteration 47/1000 | Loss: 0.00001199
Iteration 48/1000 | Loss: 0.00001199
Iteration 49/1000 | Loss: 0.00001199
Iteration 50/1000 | Loss: 0.00001199
Iteration 51/1000 | Loss: 0.00001199
Iteration 52/1000 | Loss: 0.00001199
Iteration 53/1000 | Loss: 0.00001198
Iteration 54/1000 | Loss: 0.00001198
Iteration 55/1000 | Loss: 0.00001198
Iteration 56/1000 | Loss: 0.00001198
Iteration 57/1000 | Loss: 0.00001198
Iteration 58/1000 | Loss: 0.00001197
Iteration 59/1000 | Loss: 0.00001197
Iteration 60/1000 | Loss: 0.00001197
Iteration 61/1000 | Loss: 0.00001197
Iteration 62/1000 | Loss: 0.00001197
Iteration 63/1000 | Loss: 0.00001196
Iteration 64/1000 | Loss: 0.00001196
Iteration 65/1000 | Loss: 0.00001196
Iteration 66/1000 | Loss: 0.00001196
Iteration 67/1000 | Loss: 0.00001196
Iteration 68/1000 | Loss: 0.00001196
Iteration 69/1000 | Loss: 0.00001196
Iteration 70/1000 | Loss: 0.00001196
Iteration 71/1000 | Loss: 0.00001196
Iteration 72/1000 | Loss: 0.00001196
Iteration 73/1000 | Loss: 0.00001196
Iteration 74/1000 | Loss: 0.00001196
Iteration 75/1000 | Loss: 0.00001195
Iteration 76/1000 | Loss: 0.00001195
Iteration 77/1000 | Loss: 0.00001195
Iteration 78/1000 | Loss: 0.00001195
Iteration 79/1000 | Loss: 0.00001195
Iteration 80/1000 | Loss: 0.00001195
Iteration 81/1000 | Loss: 0.00001195
Iteration 82/1000 | Loss: 0.00001195
Iteration 83/1000 | Loss: 0.00001194
Iteration 84/1000 | Loss: 0.00001194
Iteration 85/1000 | Loss: 0.00001194
Iteration 86/1000 | Loss: 0.00001193
Iteration 87/1000 | Loss: 0.00001193
Iteration 88/1000 | Loss: 0.00001193
Iteration 89/1000 | Loss: 0.00001192
Iteration 90/1000 | Loss: 0.00001192
Iteration 91/1000 | Loss: 0.00001192
Iteration 92/1000 | Loss: 0.00001192
Iteration 93/1000 | Loss: 0.00001192
Iteration 94/1000 | Loss: 0.00001191
Iteration 95/1000 | Loss: 0.00001191
Iteration 96/1000 | Loss: 0.00001191
Iteration 97/1000 | Loss: 0.00001191
Iteration 98/1000 | Loss: 0.00001190
Iteration 99/1000 | Loss: 0.00001190
Iteration 100/1000 | Loss: 0.00001190
Iteration 101/1000 | Loss: 0.00001190
Iteration 102/1000 | Loss: 0.00001190
Iteration 103/1000 | Loss: 0.00001190
Iteration 104/1000 | Loss: 0.00001190
Iteration 105/1000 | Loss: 0.00001190
Iteration 106/1000 | Loss: 0.00001190
Iteration 107/1000 | Loss: 0.00001189
Iteration 108/1000 | Loss: 0.00001189
Iteration 109/1000 | Loss: 0.00001189
Iteration 110/1000 | Loss: 0.00001188
Iteration 111/1000 | Loss: 0.00001188
Iteration 112/1000 | Loss: 0.00001188
Iteration 113/1000 | Loss: 0.00001188
Iteration 114/1000 | Loss: 0.00001188
Iteration 115/1000 | Loss: 0.00001188
Iteration 116/1000 | Loss: 0.00001188
Iteration 117/1000 | Loss: 0.00001188
Iteration 118/1000 | Loss: 0.00001187
Iteration 119/1000 | Loss: 0.00001187
Iteration 120/1000 | Loss: 0.00001187
Iteration 121/1000 | Loss: 0.00001187
Iteration 122/1000 | Loss: 0.00001187
Iteration 123/1000 | Loss: 0.00001187
Iteration 124/1000 | Loss: 0.00001187
Iteration 125/1000 | Loss: 0.00001187
Iteration 126/1000 | Loss: 0.00001187
Iteration 127/1000 | Loss: 0.00001187
Iteration 128/1000 | Loss: 0.00001187
Iteration 129/1000 | Loss: 0.00001187
Iteration 130/1000 | Loss: 0.00001186
Iteration 131/1000 | Loss: 0.00001186
Iteration 132/1000 | Loss: 0.00001186
Iteration 133/1000 | Loss: 0.00001186
Iteration 134/1000 | Loss: 0.00001186
Iteration 135/1000 | Loss: 0.00001186
Iteration 136/1000 | Loss: 0.00001186
Iteration 137/1000 | Loss: 0.00001186
Iteration 138/1000 | Loss: 0.00001186
Iteration 139/1000 | Loss: 0.00001186
Iteration 140/1000 | Loss: 0.00001185
Iteration 141/1000 | Loss: 0.00001185
Iteration 142/1000 | Loss: 0.00001185
Iteration 143/1000 | Loss: 0.00001185
Iteration 144/1000 | Loss: 0.00001185
Iteration 145/1000 | Loss: 0.00001185
Iteration 146/1000 | Loss: 0.00001185
Iteration 147/1000 | Loss: 0.00001185
Iteration 148/1000 | Loss: 0.00001185
Iteration 149/1000 | Loss: 0.00001185
Iteration 150/1000 | Loss: 0.00001185
Iteration 151/1000 | Loss: 0.00001184
Iteration 152/1000 | Loss: 0.00001184
Iteration 153/1000 | Loss: 0.00001184
Iteration 154/1000 | Loss: 0.00001184
Iteration 155/1000 | Loss: 0.00001184
Iteration 156/1000 | Loss: 0.00001184
Iteration 157/1000 | Loss: 0.00001184
Iteration 158/1000 | Loss: 0.00001184
Iteration 159/1000 | Loss: 0.00001184
Iteration 160/1000 | Loss: 0.00001184
Iteration 161/1000 | Loss: 0.00001184
Iteration 162/1000 | Loss: 0.00001184
Iteration 163/1000 | Loss: 0.00001184
Iteration 164/1000 | Loss: 0.00001183
Iteration 165/1000 | Loss: 0.00001183
Iteration 166/1000 | Loss: 0.00001183
Iteration 167/1000 | Loss: 0.00001183
Iteration 168/1000 | Loss: 0.00001183
Iteration 169/1000 | Loss: 0.00001183
Iteration 170/1000 | Loss: 0.00001183
Iteration 171/1000 | Loss: 0.00001182
Iteration 172/1000 | Loss: 0.00001182
Iteration 173/1000 | Loss: 0.00001182
Iteration 174/1000 | Loss: 0.00001182
Iteration 175/1000 | Loss: 0.00001182
Iteration 176/1000 | Loss: 0.00001182
Iteration 177/1000 | Loss: 0.00001182
Iteration 178/1000 | Loss: 0.00001182
Iteration 179/1000 | Loss: 0.00001181
Iteration 180/1000 | Loss: 0.00001181
Iteration 181/1000 | Loss: 0.00001181
Iteration 182/1000 | Loss: 0.00001181
Iteration 183/1000 | Loss: 0.00001181
Iteration 184/1000 | Loss: 0.00001180
Iteration 185/1000 | Loss: 0.00001180
Iteration 186/1000 | Loss: 0.00001180
Iteration 187/1000 | Loss: 0.00001180
Iteration 188/1000 | Loss: 0.00001180
Iteration 189/1000 | Loss: 0.00001180
Iteration 190/1000 | Loss: 0.00001179
Iteration 191/1000 | Loss: 0.00001179
Iteration 192/1000 | Loss: 0.00001179
Iteration 193/1000 | Loss: 0.00001179
Iteration 194/1000 | Loss: 0.00001179
Iteration 195/1000 | Loss: 0.00001179
Iteration 196/1000 | Loss: 0.00001178
Iteration 197/1000 | Loss: 0.00001178
Iteration 198/1000 | Loss: 0.00001178
Iteration 199/1000 | Loss: 0.00001178
Iteration 200/1000 | Loss: 0.00001178
Iteration 201/1000 | Loss: 0.00001178
Iteration 202/1000 | Loss: 0.00001178
Iteration 203/1000 | Loss: 0.00001178
Iteration 204/1000 | Loss: 0.00001178
Iteration 205/1000 | Loss: 0.00001177
Iteration 206/1000 | Loss: 0.00001177
Iteration 207/1000 | Loss: 0.00001177
Iteration 208/1000 | Loss: 0.00001177
Iteration 209/1000 | Loss: 0.00001177
Iteration 210/1000 | Loss: 0.00001177
Iteration 211/1000 | Loss: 0.00001177
Iteration 212/1000 | Loss: 0.00001177
Iteration 213/1000 | Loss: 0.00001177
Iteration 214/1000 | Loss: 0.00001177
Iteration 215/1000 | Loss: 0.00001177
Iteration 216/1000 | Loss: 0.00001177
Iteration 217/1000 | Loss: 0.00001177
Iteration 218/1000 | Loss: 0.00001176
Iteration 219/1000 | Loss: 0.00001176
Iteration 220/1000 | Loss: 0.00001176
Iteration 221/1000 | Loss: 0.00001176
Iteration 222/1000 | Loss: 0.00001176
Iteration 223/1000 | Loss: 0.00001176
Iteration 224/1000 | Loss: 0.00001176
Iteration 225/1000 | Loss: 0.00001176
Iteration 226/1000 | Loss: 0.00001176
Iteration 227/1000 | Loss: 0.00001176
Iteration 228/1000 | Loss: 0.00001176
Iteration 229/1000 | Loss: 0.00001176
Iteration 230/1000 | Loss: 0.00001176
Iteration 231/1000 | Loss: 0.00001176
Iteration 232/1000 | Loss: 0.00001176
Iteration 233/1000 | Loss: 0.00001176
Iteration 234/1000 | Loss: 0.00001176
Iteration 235/1000 | Loss: 0.00001175
Iteration 236/1000 | Loss: 0.00001175
Iteration 237/1000 | Loss: 0.00001175
Iteration 238/1000 | Loss: 0.00001175
Iteration 239/1000 | Loss: 0.00001175
Iteration 240/1000 | Loss: 0.00001175
Iteration 241/1000 | Loss: 0.00001175
Iteration 242/1000 | Loss: 0.00001175
Iteration 243/1000 | Loss: 0.00001175
Iteration 244/1000 | Loss: 0.00001175
Iteration 245/1000 | Loss: 0.00001175
Iteration 246/1000 | Loss: 0.00001175
Iteration 247/1000 | Loss: 0.00001175
Iteration 248/1000 | Loss: 0.00001175
Iteration 249/1000 | Loss: 0.00001175
Iteration 250/1000 | Loss: 0.00001175
Iteration 251/1000 | Loss: 0.00001175
Iteration 252/1000 | Loss: 0.00001175
Iteration 253/1000 | Loss: 0.00001175
Iteration 254/1000 | Loss: 0.00001175
Iteration 255/1000 | Loss: 0.00001175
Iteration 256/1000 | Loss: 0.00001175
Iteration 257/1000 | Loss: 0.00001175
Iteration 258/1000 | Loss: 0.00001175
Iteration 259/1000 | Loss: 0.00001175
Iteration 260/1000 | Loss: 0.00001175
Iteration 261/1000 | Loss: 0.00001175
Iteration 262/1000 | Loss: 0.00001175
Iteration 263/1000 | Loss: 0.00001175
Iteration 264/1000 | Loss: 0.00001175
Iteration 265/1000 | Loss: 0.00001175
Iteration 266/1000 | Loss: 0.00001175
Iteration 267/1000 | Loss: 0.00001175
Iteration 268/1000 | Loss: 0.00001175
Iteration 269/1000 | Loss: 0.00001175
Iteration 270/1000 | Loss: 0.00001175
Iteration 271/1000 | Loss: 0.00001175
Iteration 272/1000 | Loss: 0.00001175
Iteration 273/1000 | Loss: 0.00001175
Iteration 274/1000 | Loss: 0.00001175
Iteration 275/1000 | Loss: 0.00001175
Iteration 276/1000 | Loss: 0.00001175
Iteration 277/1000 | Loss: 0.00001175
Iteration 278/1000 | Loss: 0.00001175
Iteration 279/1000 | Loss: 0.00001175
Iteration 280/1000 | Loss: 0.00001175
Iteration 281/1000 | Loss: 0.00001175
Iteration 282/1000 | Loss: 0.00001175
Iteration 283/1000 | Loss: 0.00001175
Iteration 284/1000 | Loss: 0.00001175
Iteration 285/1000 | Loss: 0.00001175
Iteration 286/1000 | Loss: 0.00001175
Iteration 287/1000 | Loss: 0.00001175
Iteration 288/1000 | Loss: 0.00001175
Iteration 289/1000 | Loss: 0.00001175
Iteration 290/1000 | Loss: 0.00001175
Iteration 291/1000 | Loss: 0.00001175
Iteration 292/1000 | Loss: 0.00001175
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 292. Stopping optimization.
Last 5 losses: [1.1747211829060689e-05, 1.1747211829060689e-05, 1.1747211829060689e-05, 1.1747211829060689e-05, 1.1747211829060689e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1747211829060689e-05

Optimization complete. Final v2v error: 2.9111459255218506 mm

Highest mean error: 3.438779830932617 mm for frame 116

Lowest mean error: 2.571406126022339 mm for frame 25

Saving results

Total time: 68.55838751792908
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_018/1086/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_018/1086.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_018/1086
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00716846
Iteration 2/25 | Loss: 0.00126294
Iteration 3/25 | Loss: 0.00117393
Iteration 4/25 | Loss: 0.00116400
Iteration 5/25 | Loss: 0.00116156
Iteration 6/25 | Loss: 0.00116154
Iteration 7/25 | Loss: 0.00116154
Iteration 8/25 | Loss: 0.00116154
Iteration 9/25 | Loss: 0.00116154
Iteration 10/25 | Loss: 0.00116154
Iteration 11/25 | Loss: 0.00116154
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011615356197580695, 0.0011615356197580695, 0.0011615356197580695, 0.0011615356197580695, 0.0011615356197580695]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011615356197580695

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.49008429
Iteration 2/25 | Loss: 0.00083493
Iteration 3/25 | Loss: 0.00083492
Iteration 4/25 | Loss: 0.00083492
Iteration 5/25 | Loss: 0.00083492
Iteration 6/25 | Loss: 0.00083492
Iteration 7/25 | Loss: 0.00083492
Iteration 8/25 | Loss: 0.00083492
Iteration 9/25 | Loss: 0.00083492
Iteration 10/25 | Loss: 0.00083492
Iteration 11/25 | Loss: 0.00083492
Iteration 12/25 | Loss: 0.00083492
Iteration 13/25 | Loss: 0.00083492
Iteration 14/25 | Loss: 0.00083492
Iteration 15/25 | Loss: 0.00083492
Iteration 16/25 | Loss: 0.00083492
Iteration 17/25 | Loss: 0.00083492
Iteration 18/25 | Loss: 0.00083492
Iteration 19/25 | Loss: 0.00083492
Iteration 20/25 | Loss: 0.00083492
Iteration 21/25 | Loss: 0.00083492
Iteration 22/25 | Loss: 0.00083492
Iteration 23/25 | Loss: 0.00083492
Iteration 24/25 | Loss: 0.00083492
Iteration 25/25 | Loss: 0.00083492

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00083492
Iteration 2/1000 | Loss: 0.00002491
Iteration 3/1000 | Loss: 0.00001481
Iteration 4/1000 | Loss: 0.00001346
Iteration 5/1000 | Loss: 0.00001280
Iteration 6/1000 | Loss: 0.00001234
Iteration 7/1000 | Loss: 0.00001202
Iteration 8/1000 | Loss: 0.00001180
Iteration 9/1000 | Loss: 0.00001153
Iteration 10/1000 | Loss: 0.00001133
Iteration 11/1000 | Loss: 0.00001121
Iteration 12/1000 | Loss: 0.00001115
Iteration 13/1000 | Loss: 0.00001104
Iteration 14/1000 | Loss: 0.00001100
Iteration 15/1000 | Loss: 0.00001097
Iteration 16/1000 | Loss: 0.00001094
Iteration 17/1000 | Loss: 0.00001093
Iteration 18/1000 | Loss: 0.00001092
Iteration 19/1000 | Loss: 0.00001092
Iteration 20/1000 | Loss: 0.00001091
Iteration 21/1000 | Loss: 0.00001089
Iteration 22/1000 | Loss: 0.00001081
Iteration 23/1000 | Loss: 0.00001080
Iteration 24/1000 | Loss: 0.00001079
Iteration 25/1000 | Loss: 0.00001079
Iteration 26/1000 | Loss: 0.00001078
Iteration 27/1000 | Loss: 0.00001072
Iteration 28/1000 | Loss: 0.00001070
Iteration 29/1000 | Loss: 0.00001069
Iteration 30/1000 | Loss: 0.00001069
Iteration 31/1000 | Loss: 0.00001068
Iteration 32/1000 | Loss: 0.00001066
Iteration 33/1000 | Loss: 0.00001066
Iteration 34/1000 | Loss: 0.00001066
Iteration 35/1000 | Loss: 0.00001066
Iteration 36/1000 | Loss: 0.00001066
Iteration 37/1000 | Loss: 0.00001066
Iteration 38/1000 | Loss: 0.00001066
Iteration 39/1000 | Loss: 0.00001065
Iteration 40/1000 | Loss: 0.00001065
Iteration 41/1000 | Loss: 0.00001065
Iteration 42/1000 | Loss: 0.00001065
Iteration 43/1000 | Loss: 0.00001065
Iteration 44/1000 | Loss: 0.00001065
Iteration 45/1000 | Loss: 0.00001064
Iteration 46/1000 | Loss: 0.00001063
Iteration 47/1000 | Loss: 0.00001062
Iteration 48/1000 | Loss: 0.00001062
Iteration 49/1000 | Loss: 0.00001061
Iteration 50/1000 | Loss: 0.00001061
Iteration 51/1000 | Loss: 0.00001061
Iteration 52/1000 | Loss: 0.00001060
Iteration 53/1000 | Loss: 0.00001060
Iteration 54/1000 | Loss: 0.00001060
Iteration 55/1000 | Loss: 0.00001059
Iteration 56/1000 | Loss: 0.00001059
Iteration 57/1000 | Loss: 0.00001058
Iteration 58/1000 | Loss: 0.00001058
Iteration 59/1000 | Loss: 0.00001058
Iteration 60/1000 | Loss: 0.00001058
Iteration 61/1000 | Loss: 0.00001058
Iteration 62/1000 | Loss: 0.00001057
Iteration 63/1000 | Loss: 0.00001057
Iteration 64/1000 | Loss: 0.00001057
Iteration 65/1000 | Loss: 0.00001057
Iteration 66/1000 | Loss: 0.00001056
Iteration 67/1000 | Loss: 0.00001056
Iteration 68/1000 | Loss: 0.00001056
Iteration 69/1000 | Loss: 0.00001055
Iteration 70/1000 | Loss: 0.00001055
Iteration 71/1000 | Loss: 0.00001054
Iteration 72/1000 | Loss: 0.00001054
Iteration 73/1000 | Loss: 0.00001053
Iteration 74/1000 | Loss: 0.00001053
Iteration 75/1000 | Loss: 0.00001053
Iteration 76/1000 | Loss: 0.00001052
Iteration 77/1000 | Loss: 0.00001052
Iteration 78/1000 | Loss: 0.00001052
Iteration 79/1000 | Loss: 0.00001051
Iteration 80/1000 | Loss: 0.00001051
Iteration 81/1000 | Loss: 0.00001051
Iteration 82/1000 | Loss: 0.00001050
Iteration 83/1000 | Loss: 0.00001050
Iteration 84/1000 | Loss: 0.00001050
Iteration 85/1000 | Loss: 0.00001049
Iteration 86/1000 | Loss: 0.00001049
Iteration 87/1000 | Loss: 0.00001049
Iteration 88/1000 | Loss: 0.00001049
Iteration 89/1000 | Loss: 0.00001049
Iteration 90/1000 | Loss: 0.00001048
Iteration 91/1000 | Loss: 0.00001048
Iteration 92/1000 | Loss: 0.00001048
Iteration 93/1000 | Loss: 0.00001047
Iteration 94/1000 | Loss: 0.00001047
Iteration 95/1000 | Loss: 0.00001047
Iteration 96/1000 | Loss: 0.00001046
Iteration 97/1000 | Loss: 0.00001046
Iteration 98/1000 | Loss: 0.00001046
Iteration 99/1000 | Loss: 0.00001046
Iteration 100/1000 | Loss: 0.00001046
Iteration 101/1000 | Loss: 0.00001046
Iteration 102/1000 | Loss: 0.00001046
Iteration 103/1000 | Loss: 0.00001046
Iteration 104/1000 | Loss: 0.00001046
Iteration 105/1000 | Loss: 0.00001046
Iteration 106/1000 | Loss: 0.00001046
Iteration 107/1000 | Loss: 0.00001046
Iteration 108/1000 | Loss: 0.00001046
Iteration 109/1000 | Loss: 0.00001046
Iteration 110/1000 | Loss: 0.00001046
Iteration 111/1000 | Loss: 0.00001045
Iteration 112/1000 | Loss: 0.00001045
Iteration 113/1000 | Loss: 0.00001045
Iteration 114/1000 | Loss: 0.00001045
Iteration 115/1000 | Loss: 0.00001045
Iteration 116/1000 | Loss: 0.00001045
Iteration 117/1000 | Loss: 0.00001045
Iteration 118/1000 | Loss: 0.00001045
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 118. Stopping optimization.
Last 5 losses: [1.0453252798470203e-05, 1.0453252798470203e-05, 1.0453252798470203e-05, 1.0453252798470203e-05, 1.0453252798470203e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0453252798470203e-05

Optimization complete. Final v2v error: 2.77097749710083 mm

Highest mean error: 3.0910303592681885 mm for frame 21

Lowest mean error: 2.5150046348571777 mm for frame 46

Saving results

Total time: 38.5491361618042
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_018/1010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_018/1010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_018/1010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01006596
Iteration 2/25 | Loss: 0.00250317
Iteration 3/25 | Loss: 0.00252774
Iteration 4/25 | Loss: 0.00219461
Iteration 5/25 | Loss: 0.00223859
Iteration 6/25 | Loss: 0.00183444
Iteration 7/25 | Loss: 0.00160565
Iteration 8/25 | Loss: 0.00151014
Iteration 9/25 | Loss: 0.00150177
Iteration 10/25 | Loss: 0.00147449
Iteration 11/25 | Loss: 0.00145918
Iteration 12/25 | Loss: 0.00142224
Iteration 13/25 | Loss: 0.00140405
Iteration 14/25 | Loss: 0.00141124
Iteration 15/25 | Loss: 0.00141084
Iteration 16/25 | Loss: 0.00139440
Iteration 17/25 | Loss: 0.00137923
Iteration 18/25 | Loss: 0.00137340
Iteration 19/25 | Loss: 0.00137394
Iteration 20/25 | Loss: 0.00137205
Iteration 21/25 | Loss: 0.00137360
Iteration 22/25 | Loss: 0.00137197
Iteration 23/25 | Loss: 0.00137195
Iteration 24/25 | Loss: 0.00137195
Iteration 25/25 | Loss: 0.00137195

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37871039
Iteration 2/25 | Loss: 0.00153750
Iteration 3/25 | Loss: 0.00120088
Iteration 4/25 | Loss: 0.00120057
Iteration 5/25 | Loss: 0.00120057
Iteration 6/25 | Loss: 0.00120057
Iteration 7/25 | Loss: 0.00120057
Iteration 8/25 | Loss: 0.00120057
Iteration 9/25 | Loss: 0.00120056
Iteration 10/25 | Loss: 0.00120056
Iteration 11/25 | Loss: 0.00120056
Iteration 12/25 | Loss: 0.00120056
Iteration 13/25 | Loss: 0.00120056
Iteration 14/25 | Loss: 0.00120056
Iteration 15/25 | Loss: 0.00120056
Iteration 16/25 | Loss: 0.00120056
Iteration 17/25 | Loss: 0.00120056
Iteration 18/25 | Loss: 0.00120056
Iteration 19/25 | Loss: 0.00120056
Iteration 20/25 | Loss: 0.00120056
Iteration 21/25 | Loss: 0.00120056
Iteration 22/25 | Loss: 0.00120056
Iteration 23/25 | Loss: 0.00120056
Iteration 24/25 | Loss: 0.00120056
Iteration 25/25 | Loss: 0.00120056

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00120056
Iteration 2/1000 | Loss: 0.00138005
Iteration 3/1000 | Loss: 0.00023498
Iteration 4/1000 | Loss: 0.00042614
Iteration 5/1000 | Loss: 0.00045561
Iteration 6/1000 | Loss: 0.00012716
Iteration 7/1000 | Loss: 0.00054313
Iteration 8/1000 | Loss: 0.00012235
Iteration 9/1000 | Loss: 0.00049259
Iteration 10/1000 | Loss: 0.00022191
Iteration 11/1000 | Loss: 0.00009753
Iteration 12/1000 | Loss: 0.00039811
Iteration 13/1000 | Loss: 0.00007465
Iteration 14/1000 | Loss: 0.00007152
Iteration 15/1000 | Loss: 0.00006971
Iteration 16/1000 | Loss: 0.00006758
Iteration 17/1000 | Loss: 0.00009211
Iteration 18/1000 | Loss: 0.00033912
Iteration 19/1000 | Loss: 0.00185993
Iteration 20/1000 | Loss: 0.00503420
Iteration 21/1000 | Loss: 0.00418641
Iteration 22/1000 | Loss: 0.00329452
Iteration 23/1000 | Loss: 0.00181498
Iteration 24/1000 | Loss: 0.00267200
Iteration 25/1000 | Loss: 0.00121456
Iteration 26/1000 | Loss: 0.00078933
Iteration 27/1000 | Loss: 0.00234931
Iteration 28/1000 | Loss: 0.00350377
Iteration 29/1000 | Loss: 0.00307505
Iteration 30/1000 | Loss: 0.00174930
Iteration 31/1000 | Loss: 0.00250292
Iteration 32/1000 | Loss: 0.00098313
Iteration 33/1000 | Loss: 0.00022949
Iteration 34/1000 | Loss: 0.00015501
Iteration 35/1000 | Loss: 0.00027377
Iteration 36/1000 | Loss: 0.00036349
Iteration 37/1000 | Loss: 0.00088679
Iteration 38/1000 | Loss: 0.00015091
Iteration 39/1000 | Loss: 0.00015115
Iteration 40/1000 | Loss: 0.00005724
Iteration 41/1000 | Loss: 0.00012130
Iteration 42/1000 | Loss: 0.00004896
Iteration 43/1000 | Loss: 0.00004760
Iteration 44/1000 | Loss: 0.00003537
Iteration 45/1000 | Loss: 0.00013376
Iteration 46/1000 | Loss: 0.00004409
Iteration 47/1000 | Loss: 0.00004266
Iteration 48/1000 | Loss: 0.00003061
Iteration 49/1000 | Loss: 0.00004272
Iteration 50/1000 | Loss: 0.00003668
Iteration 51/1000 | Loss: 0.00002842
Iteration 52/1000 | Loss: 0.00013453
Iteration 53/1000 | Loss: 0.00002873
Iteration 54/1000 | Loss: 0.00005070
Iteration 55/1000 | Loss: 0.00002759
Iteration 56/1000 | Loss: 0.00003312
Iteration 57/1000 | Loss: 0.00041316
Iteration 58/1000 | Loss: 0.00005068
Iteration 59/1000 | Loss: 0.00002200
Iteration 60/1000 | Loss: 0.00002164
Iteration 61/1000 | Loss: 0.00002134
Iteration 62/1000 | Loss: 0.00005129
Iteration 63/1000 | Loss: 0.00002112
Iteration 64/1000 | Loss: 0.00002098
Iteration 65/1000 | Loss: 0.00002086
Iteration 66/1000 | Loss: 0.00002085
Iteration 67/1000 | Loss: 0.00002081
Iteration 68/1000 | Loss: 0.00002081
Iteration 69/1000 | Loss: 0.00002080
Iteration 70/1000 | Loss: 0.00002079
Iteration 71/1000 | Loss: 0.00005055
Iteration 72/1000 | Loss: 0.00002576
Iteration 73/1000 | Loss: 0.00002190
Iteration 74/1000 | Loss: 0.00002091
Iteration 75/1000 | Loss: 0.00002073
Iteration 76/1000 | Loss: 0.00002070
Iteration 77/1000 | Loss: 0.00002070
Iteration 78/1000 | Loss: 0.00002070
Iteration 79/1000 | Loss: 0.00002070
Iteration 80/1000 | Loss: 0.00002070
Iteration 81/1000 | Loss: 0.00002070
Iteration 82/1000 | Loss: 0.00002070
Iteration 83/1000 | Loss: 0.00002070
Iteration 84/1000 | Loss: 0.00002070
Iteration 85/1000 | Loss: 0.00002069
Iteration 86/1000 | Loss: 0.00002069
Iteration 87/1000 | Loss: 0.00002069
Iteration 88/1000 | Loss: 0.00002069
Iteration 89/1000 | Loss: 0.00002069
Iteration 90/1000 | Loss: 0.00002069
Iteration 91/1000 | Loss: 0.00002069
Iteration 92/1000 | Loss: 0.00002069
Iteration 93/1000 | Loss: 0.00002069
Iteration 94/1000 | Loss: 0.00002069
Iteration 95/1000 | Loss: 0.00002069
Iteration 96/1000 | Loss: 0.00002069
Iteration 97/1000 | Loss: 0.00002069
Iteration 98/1000 | Loss: 0.00002069
Iteration 99/1000 | Loss: 0.00002069
Iteration 100/1000 | Loss: 0.00002069
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 100. Stopping optimization.
Last 5 losses: [2.0693296391982585e-05, 2.0693296391982585e-05, 2.0693296391982585e-05, 2.0693296391982585e-05, 2.0693296391982585e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0693296391982585e-05

Optimization complete. Final v2v error: 3.7842938899993896 mm

Highest mean error: 4.43375825881958 mm for frame 89

Lowest mean error: 3.4476075172424316 mm for frame 12

Saving results

Total time: 148.5343759059906
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_018/1007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_018/1007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_018/1007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00992598
Iteration 2/25 | Loss: 0.00329065
Iteration 3/25 | Loss: 0.00148664
Iteration 4/25 | Loss: 0.00130511
Iteration 5/25 | Loss: 0.00125369
Iteration 6/25 | Loss: 0.00125462
Iteration 7/25 | Loss: 0.00125070
Iteration 8/25 | Loss: 0.00122160
Iteration 9/25 | Loss: 0.00120847
Iteration 10/25 | Loss: 0.00119706
Iteration 11/25 | Loss: 0.00120240
Iteration 12/25 | Loss: 0.00120493
Iteration 13/25 | Loss: 0.00120217
Iteration 14/25 | Loss: 0.00119704
Iteration 15/25 | Loss: 0.00119336
Iteration 16/25 | Loss: 0.00119318
Iteration 17/25 | Loss: 0.00118937
Iteration 18/25 | Loss: 0.00118764
Iteration 19/25 | Loss: 0.00118695
Iteration 20/25 | Loss: 0.00118637
Iteration 21/25 | Loss: 0.00118620
Iteration 22/25 | Loss: 0.00118648
Iteration 23/25 | Loss: 0.00118542
Iteration 24/25 | Loss: 0.00118507
Iteration 25/25 | Loss: 0.00118501

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43728340
Iteration 2/25 | Loss: 0.00085447
Iteration 3/25 | Loss: 0.00085447
Iteration 4/25 | Loss: 0.00085447
Iteration 5/25 | Loss: 0.00085447
Iteration 6/25 | Loss: 0.00085447
Iteration 7/25 | Loss: 0.00085446
Iteration 8/25 | Loss: 0.00085446
Iteration 9/25 | Loss: 0.00085446
Iteration 10/25 | Loss: 0.00085446
Iteration 11/25 | Loss: 0.00085446
Iteration 12/25 | Loss: 0.00085446
Iteration 13/25 | Loss: 0.00085446
Iteration 14/25 | Loss: 0.00085446
Iteration 15/25 | Loss: 0.00085446
Iteration 16/25 | Loss: 0.00085446
Iteration 17/25 | Loss: 0.00085446
Iteration 18/25 | Loss: 0.00085446
Iteration 19/25 | Loss: 0.00085446
Iteration 20/25 | Loss: 0.00085446
Iteration 21/25 | Loss: 0.00085446
Iteration 22/25 | Loss: 0.00085446
Iteration 23/25 | Loss: 0.00085446
Iteration 24/25 | Loss: 0.00085446
Iteration 25/25 | Loss: 0.00085446

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00085446
Iteration 2/1000 | Loss: 0.00003145
Iteration 3/1000 | Loss: 0.00002040
Iteration 4/1000 | Loss: 0.00001875
Iteration 5/1000 | Loss: 0.00001774
Iteration 6/1000 | Loss: 0.00001703
Iteration 7/1000 | Loss: 0.00001665
Iteration 8/1000 | Loss: 0.00001639
Iteration 9/1000 | Loss: 0.00001613
Iteration 10/1000 | Loss: 0.00001595
Iteration 11/1000 | Loss: 0.00001590
Iteration 12/1000 | Loss: 0.00001573
Iteration 13/1000 | Loss: 0.00001571
Iteration 14/1000 | Loss: 0.00001565
Iteration 15/1000 | Loss: 0.00001556
Iteration 16/1000 | Loss: 0.00001550
Iteration 17/1000 | Loss: 0.00001548
Iteration 18/1000 | Loss: 0.00001547
Iteration 19/1000 | Loss: 0.00001546
Iteration 20/1000 | Loss: 0.00001546
Iteration 21/1000 | Loss: 0.00001544
Iteration 22/1000 | Loss: 0.00001541
Iteration 23/1000 | Loss: 0.00001540
Iteration 24/1000 | Loss: 0.00001538
Iteration 25/1000 | Loss: 0.00001535
Iteration 26/1000 | Loss: 0.00001534
Iteration 27/1000 | Loss: 0.00001532
Iteration 28/1000 | Loss: 0.00001532
Iteration 29/1000 | Loss: 0.00001532
Iteration 30/1000 | Loss: 0.00001531
Iteration 31/1000 | Loss: 0.00001529
Iteration 32/1000 | Loss: 0.00001526
Iteration 33/1000 | Loss: 0.00001526
Iteration 34/1000 | Loss: 0.00001526
Iteration 35/1000 | Loss: 0.00001525
Iteration 36/1000 | Loss: 0.00001525
Iteration 37/1000 | Loss: 0.00001525
Iteration 38/1000 | Loss: 0.00001524
Iteration 39/1000 | Loss: 0.00001524
Iteration 40/1000 | Loss: 0.00001523
Iteration 41/1000 | Loss: 0.00001523
Iteration 42/1000 | Loss: 0.00001523
Iteration 43/1000 | Loss: 0.00001522
Iteration 44/1000 | Loss: 0.00001522
Iteration 45/1000 | Loss: 0.00001522
Iteration 46/1000 | Loss: 0.00001522
Iteration 47/1000 | Loss: 0.00001522
Iteration 48/1000 | Loss: 0.00001522
Iteration 49/1000 | Loss: 0.00001522
Iteration 50/1000 | Loss: 0.00001522
Iteration 51/1000 | Loss: 0.00001521
Iteration 52/1000 | Loss: 0.00001521
Iteration 53/1000 | Loss: 0.00001521
Iteration 54/1000 | Loss: 0.00001521
Iteration 55/1000 | Loss: 0.00001521
Iteration 56/1000 | Loss: 0.00001520
Iteration 57/1000 | Loss: 0.00001520
Iteration 58/1000 | Loss: 0.00001520
Iteration 59/1000 | Loss: 0.00001520
Iteration 60/1000 | Loss: 0.00001520
Iteration 61/1000 | Loss: 0.00001520
Iteration 62/1000 | Loss: 0.00001520
Iteration 63/1000 | Loss: 0.00001519
Iteration 64/1000 | Loss: 0.00001519
Iteration 65/1000 | Loss: 0.00001519
Iteration 66/1000 | Loss: 0.00001518
Iteration 67/1000 | Loss: 0.00001518
Iteration 68/1000 | Loss: 0.00001518
Iteration 69/1000 | Loss: 0.00001518
Iteration 70/1000 | Loss: 0.00001517
Iteration 71/1000 | Loss: 0.00001517
Iteration 72/1000 | Loss: 0.00001517
Iteration 73/1000 | Loss: 0.00001517
Iteration 74/1000 | Loss: 0.00001517
Iteration 75/1000 | Loss: 0.00001517
Iteration 76/1000 | Loss: 0.00001517
Iteration 77/1000 | Loss: 0.00001517
Iteration 78/1000 | Loss: 0.00001517
Iteration 79/1000 | Loss: 0.00001517
Iteration 80/1000 | Loss: 0.00001516
Iteration 81/1000 | Loss: 0.00001516
Iteration 82/1000 | Loss: 0.00001516
Iteration 83/1000 | Loss: 0.00001516
Iteration 84/1000 | Loss: 0.00001515
Iteration 85/1000 | Loss: 0.00001515
Iteration 86/1000 | Loss: 0.00001515
Iteration 87/1000 | Loss: 0.00001515
Iteration 88/1000 | Loss: 0.00001514
Iteration 89/1000 | Loss: 0.00001514
Iteration 90/1000 | Loss: 0.00001514
Iteration 91/1000 | Loss: 0.00001513
Iteration 92/1000 | Loss: 0.00001513
Iteration 93/1000 | Loss: 0.00001513
Iteration 94/1000 | Loss: 0.00001513
Iteration 95/1000 | Loss: 0.00001512
Iteration 96/1000 | Loss: 0.00001512
Iteration 97/1000 | Loss: 0.00001512
Iteration 98/1000 | Loss: 0.00001511
Iteration 99/1000 | Loss: 0.00001511
Iteration 100/1000 | Loss: 0.00001510
Iteration 101/1000 | Loss: 0.00001510
Iteration 102/1000 | Loss: 0.00001509
Iteration 103/1000 | Loss: 0.00001509
Iteration 104/1000 | Loss: 0.00001509
Iteration 105/1000 | Loss: 0.00001509
Iteration 106/1000 | Loss: 0.00001509
Iteration 107/1000 | Loss: 0.00001508
Iteration 108/1000 | Loss: 0.00001508
Iteration 109/1000 | Loss: 0.00001508
Iteration 110/1000 | Loss: 0.00001508
Iteration 111/1000 | Loss: 0.00001508
Iteration 112/1000 | Loss: 0.00001508
Iteration 113/1000 | Loss: 0.00001508
Iteration 114/1000 | Loss: 0.00001507
Iteration 115/1000 | Loss: 0.00001507
Iteration 116/1000 | Loss: 0.00001507
Iteration 117/1000 | Loss: 0.00001507
Iteration 118/1000 | Loss: 0.00001507
Iteration 119/1000 | Loss: 0.00001507
Iteration 120/1000 | Loss: 0.00001507
Iteration 121/1000 | Loss: 0.00001507
Iteration 122/1000 | Loss: 0.00001507
Iteration 123/1000 | Loss: 0.00001506
Iteration 124/1000 | Loss: 0.00001506
Iteration 125/1000 | Loss: 0.00001506
Iteration 126/1000 | Loss: 0.00001506
Iteration 127/1000 | Loss: 0.00001506
Iteration 128/1000 | Loss: 0.00001506
Iteration 129/1000 | Loss: 0.00001506
Iteration 130/1000 | Loss: 0.00001506
Iteration 131/1000 | Loss: 0.00001505
Iteration 132/1000 | Loss: 0.00001505
Iteration 133/1000 | Loss: 0.00001505
Iteration 134/1000 | Loss: 0.00001505
Iteration 135/1000 | Loss: 0.00001505
Iteration 136/1000 | Loss: 0.00001505
Iteration 137/1000 | Loss: 0.00001505
Iteration 138/1000 | Loss: 0.00001505
Iteration 139/1000 | Loss: 0.00001504
Iteration 140/1000 | Loss: 0.00001504
Iteration 141/1000 | Loss: 0.00001504
Iteration 142/1000 | Loss: 0.00001504
Iteration 143/1000 | Loss: 0.00001504
Iteration 144/1000 | Loss: 0.00001504
Iteration 145/1000 | Loss: 0.00001504
Iteration 146/1000 | Loss: 0.00001504
Iteration 147/1000 | Loss: 0.00001504
Iteration 148/1000 | Loss: 0.00001503
Iteration 149/1000 | Loss: 0.00001503
Iteration 150/1000 | Loss: 0.00001503
Iteration 151/1000 | Loss: 0.00001503
Iteration 152/1000 | Loss: 0.00001502
Iteration 153/1000 | Loss: 0.00001502
Iteration 154/1000 | Loss: 0.00001502
Iteration 155/1000 | Loss: 0.00001502
Iteration 156/1000 | Loss: 0.00001502
Iteration 157/1000 | Loss: 0.00001501
Iteration 158/1000 | Loss: 0.00001501
Iteration 159/1000 | Loss: 0.00001501
Iteration 160/1000 | Loss: 0.00001501
Iteration 161/1000 | Loss: 0.00001501
Iteration 162/1000 | Loss: 0.00001501
Iteration 163/1000 | Loss: 0.00001500
Iteration 164/1000 | Loss: 0.00001500
Iteration 165/1000 | Loss: 0.00001500
Iteration 166/1000 | Loss: 0.00001500
Iteration 167/1000 | Loss: 0.00001499
Iteration 168/1000 | Loss: 0.00001499
Iteration 169/1000 | Loss: 0.00001499
Iteration 170/1000 | Loss: 0.00001499
Iteration 171/1000 | Loss: 0.00001499
Iteration 172/1000 | Loss: 0.00001499
Iteration 173/1000 | Loss: 0.00001499
Iteration 174/1000 | Loss: 0.00001499
Iteration 175/1000 | Loss: 0.00001499
Iteration 176/1000 | Loss: 0.00001499
Iteration 177/1000 | Loss: 0.00001499
Iteration 178/1000 | Loss: 0.00001499
Iteration 179/1000 | Loss: 0.00001499
Iteration 180/1000 | Loss: 0.00001499
Iteration 181/1000 | Loss: 0.00001499
Iteration 182/1000 | Loss: 0.00001499
Iteration 183/1000 | Loss: 0.00001499
Iteration 184/1000 | Loss: 0.00001499
Iteration 185/1000 | Loss: 0.00001499
Iteration 186/1000 | Loss: 0.00001499
Iteration 187/1000 | Loss: 0.00001499
Iteration 188/1000 | Loss: 0.00001499
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 188. Stopping optimization.
Last 5 losses: [1.4987746908445843e-05, 1.4987746908445843e-05, 1.4987746908445843e-05, 1.4987746908445843e-05, 1.4987746908445843e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4987746908445843e-05

Optimization complete. Final v2v error: 3.205657958984375 mm

Highest mean error: 5.788050651550293 mm for frame 186

Lowest mean error: 2.808380126953125 mm for frame 16

Saving results

Total time: 86.68658661842346
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_018/1089/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_018/1089.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_018/1089
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00863499
Iteration 2/25 | Loss: 0.00128208
Iteration 3/25 | Loss: 0.00119391
Iteration 4/25 | Loss: 0.00118187
Iteration 5/25 | Loss: 0.00117911
Iteration 6/25 | Loss: 0.00117867
Iteration 7/25 | Loss: 0.00117867
Iteration 8/25 | Loss: 0.00117867
Iteration 9/25 | Loss: 0.00117867
Iteration 10/25 | Loss: 0.00117867
Iteration 11/25 | Loss: 0.00117867
Iteration 12/25 | Loss: 0.00117867
Iteration 13/25 | Loss: 0.00117867
Iteration 14/25 | Loss: 0.00117867
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0011786664836108685, 0.0011786664836108685, 0.0011786664836108685, 0.0011786664836108685, 0.0011786664836108685]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011786664836108685

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.57166362
Iteration 2/25 | Loss: 0.00096542
Iteration 3/25 | Loss: 0.00096539
Iteration 4/25 | Loss: 0.00096539
Iteration 5/25 | Loss: 0.00096539
Iteration 6/25 | Loss: 0.00096539
Iteration 7/25 | Loss: 0.00096539
Iteration 8/25 | Loss: 0.00096539
Iteration 9/25 | Loss: 0.00096539
Iteration 10/25 | Loss: 0.00096539
Iteration 11/25 | Loss: 0.00096539
Iteration 12/25 | Loss: 0.00096539
Iteration 13/25 | Loss: 0.00096539
Iteration 14/25 | Loss: 0.00096539
Iteration 15/25 | Loss: 0.00096539
Iteration 16/25 | Loss: 0.00096539
Iteration 17/25 | Loss: 0.00096539
Iteration 18/25 | Loss: 0.00096539
Iteration 19/25 | Loss: 0.00096539
Iteration 20/25 | Loss: 0.00096539
Iteration 21/25 | Loss: 0.00096539
Iteration 22/25 | Loss: 0.00096539
Iteration 23/25 | Loss: 0.00096539
Iteration 24/25 | Loss: 0.00096539
Iteration 25/25 | Loss: 0.00096539
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0009653903543949127, 0.0009653903543949127, 0.0009653903543949127, 0.0009653903543949127, 0.0009653903543949127]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009653903543949127

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00096539
Iteration 2/1000 | Loss: 0.00003474
Iteration 3/1000 | Loss: 0.00002088
Iteration 4/1000 | Loss: 0.00001675
Iteration 5/1000 | Loss: 0.00001603
Iteration 6/1000 | Loss: 0.00001527
Iteration 7/1000 | Loss: 0.00001492
Iteration 8/1000 | Loss: 0.00001458
Iteration 9/1000 | Loss: 0.00001439
Iteration 10/1000 | Loss: 0.00001434
Iteration 11/1000 | Loss: 0.00001428
Iteration 12/1000 | Loss: 0.00001416
Iteration 13/1000 | Loss: 0.00001394
Iteration 14/1000 | Loss: 0.00001388
Iteration 15/1000 | Loss: 0.00001386
Iteration 16/1000 | Loss: 0.00001379
Iteration 17/1000 | Loss: 0.00001378
Iteration 18/1000 | Loss: 0.00001371
Iteration 19/1000 | Loss: 0.00001361
Iteration 20/1000 | Loss: 0.00001356
Iteration 21/1000 | Loss: 0.00001355
Iteration 22/1000 | Loss: 0.00001354
Iteration 23/1000 | Loss: 0.00001354
Iteration 24/1000 | Loss: 0.00001353
Iteration 25/1000 | Loss: 0.00001353
Iteration 26/1000 | Loss: 0.00001352
Iteration 27/1000 | Loss: 0.00001352
Iteration 28/1000 | Loss: 0.00001351
Iteration 29/1000 | Loss: 0.00001351
Iteration 30/1000 | Loss: 0.00001350
Iteration 31/1000 | Loss: 0.00001350
Iteration 32/1000 | Loss: 0.00001349
Iteration 33/1000 | Loss: 0.00001349
Iteration 34/1000 | Loss: 0.00001348
Iteration 35/1000 | Loss: 0.00001347
Iteration 36/1000 | Loss: 0.00001347
Iteration 37/1000 | Loss: 0.00001346
Iteration 38/1000 | Loss: 0.00001345
Iteration 39/1000 | Loss: 0.00001343
Iteration 40/1000 | Loss: 0.00001342
Iteration 41/1000 | Loss: 0.00001342
Iteration 42/1000 | Loss: 0.00001341
Iteration 43/1000 | Loss: 0.00001340
Iteration 44/1000 | Loss: 0.00001340
Iteration 45/1000 | Loss: 0.00001340
Iteration 46/1000 | Loss: 0.00001340
Iteration 47/1000 | Loss: 0.00001340
Iteration 48/1000 | Loss: 0.00001339
Iteration 49/1000 | Loss: 0.00001339
Iteration 50/1000 | Loss: 0.00001339
Iteration 51/1000 | Loss: 0.00001339
Iteration 52/1000 | Loss: 0.00001339
Iteration 53/1000 | Loss: 0.00001339
Iteration 54/1000 | Loss: 0.00001339
Iteration 55/1000 | Loss: 0.00001338
Iteration 56/1000 | Loss: 0.00001338
Iteration 57/1000 | Loss: 0.00001337
Iteration 58/1000 | Loss: 0.00001337
Iteration 59/1000 | Loss: 0.00001337
Iteration 60/1000 | Loss: 0.00001336
Iteration 61/1000 | Loss: 0.00001336
Iteration 62/1000 | Loss: 0.00001336
Iteration 63/1000 | Loss: 0.00001336
Iteration 64/1000 | Loss: 0.00001336
Iteration 65/1000 | Loss: 0.00001335
Iteration 66/1000 | Loss: 0.00001335
Iteration 67/1000 | Loss: 0.00001335
Iteration 68/1000 | Loss: 0.00001335
Iteration 69/1000 | Loss: 0.00001335
Iteration 70/1000 | Loss: 0.00001334
Iteration 71/1000 | Loss: 0.00001334
Iteration 72/1000 | Loss: 0.00001334
Iteration 73/1000 | Loss: 0.00001334
Iteration 74/1000 | Loss: 0.00001334
Iteration 75/1000 | Loss: 0.00001334
Iteration 76/1000 | Loss: 0.00001334
Iteration 77/1000 | Loss: 0.00001333
Iteration 78/1000 | Loss: 0.00001333
Iteration 79/1000 | Loss: 0.00001333
Iteration 80/1000 | Loss: 0.00001333
Iteration 81/1000 | Loss: 0.00001333
Iteration 82/1000 | Loss: 0.00001333
Iteration 83/1000 | Loss: 0.00001333
Iteration 84/1000 | Loss: 0.00001333
Iteration 85/1000 | Loss: 0.00001333
Iteration 86/1000 | Loss: 0.00001332
Iteration 87/1000 | Loss: 0.00001332
Iteration 88/1000 | Loss: 0.00001332
Iteration 89/1000 | Loss: 0.00001332
Iteration 90/1000 | Loss: 0.00001331
Iteration 91/1000 | Loss: 0.00001331
Iteration 92/1000 | Loss: 0.00001330
Iteration 93/1000 | Loss: 0.00001330
Iteration 94/1000 | Loss: 0.00001330
Iteration 95/1000 | Loss: 0.00001330
Iteration 96/1000 | Loss: 0.00001330
Iteration 97/1000 | Loss: 0.00001329
Iteration 98/1000 | Loss: 0.00001329
Iteration 99/1000 | Loss: 0.00001329
Iteration 100/1000 | Loss: 0.00001329
Iteration 101/1000 | Loss: 0.00001329
Iteration 102/1000 | Loss: 0.00001329
Iteration 103/1000 | Loss: 0.00001329
Iteration 104/1000 | Loss: 0.00001328
Iteration 105/1000 | Loss: 0.00001328
Iteration 106/1000 | Loss: 0.00001328
Iteration 107/1000 | Loss: 0.00001327
Iteration 108/1000 | Loss: 0.00001327
Iteration 109/1000 | Loss: 0.00001327
Iteration 110/1000 | Loss: 0.00001327
Iteration 111/1000 | Loss: 0.00001326
Iteration 112/1000 | Loss: 0.00001326
Iteration 113/1000 | Loss: 0.00001326
Iteration 114/1000 | Loss: 0.00001326
Iteration 115/1000 | Loss: 0.00001326
Iteration 116/1000 | Loss: 0.00001326
Iteration 117/1000 | Loss: 0.00001326
Iteration 118/1000 | Loss: 0.00001326
Iteration 119/1000 | Loss: 0.00001325
Iteration 120/1000 | Loss: 0.00001325
Iteration 121/1000 | Loss: 0.00001325
Iteration 122/1000 | Loss: 0.00001325
Iteration 123/1000 | Loss: 0.00001325
Iteration 124/1000 | Loss: 0.00001324
Iteration 125/1000 | Loss: 0.00001324
Iteration 126/1000 | Loss: 0.00001323
Iteration 127/1000 | Loss: 0.00001323
Iteration 128/1000 | Loss: 0.00001323
Iteration 129/1000 | Loss: 0.00001323
Iteration 130/1000 | Loss: 0.00001323
Iteration 131/1000 | Loss: 0.00001323
Iteration 132/1000 | Loss: 0.00001323
Iteration 133/1000 | Loss: 0.00001323
Iteration 134/1000 | Loss: 0.00001323
Iteration 135/1000 | Loss: 0.00001322
Iteration 136/1000 | Loss: 0.00001322
Iteration 137/1000 | Loss: 0.00001322
Iteration 138/1000 | Loss: 0.00001322
Iteration 139/1000 | Loss: 0.00001322
Iteration 140/1000 | Loss: 0.00001322
Iteration 141/1000 | Loss: 0.00001322
Iteration 142/1000 | Loss: 0.00001322
Iteration 143/1000 | Loss: 0.00001321
Iteration 144/1000 | Loss: 0.00001321
Iteration 145/1000 | Loss: 0.00001321
Iteration 146/1000 | Loss: 0.00001321
Iteration 147/1000 | Loss: 0.00001321
Iteration 148/1000 | Loss: 0.00001321
Iteration 149/1000 | Loss: 0.00001321
Iteration 150/1000 | Loss: 0.00001321
Iteration 151/1000 | Loss: 0.00001321
Iteration 152/1000 | Loss: 0.00001321
Iteration 153/1000 | Loss: 0.00001321
Iteration 154/1000 | Loss: 0.00001321
Iteration 155/1000 | Loss: 0.00001320
Iteration 156/1000 | Loss: 0.00001320
Iteration 157/1000 | Loss: 0.00001320
Iteration 158/1000 | Loss: 0.00001320
Iteration 159/1000 | Loss: 0.00001320
Iteration 160/1000 | Loss: 0.00001320
Iteration 161/1000 | Loss: 0.00001320
Iteration 162/1000 | Loss: 0.00001320
Iteration 163/1000 | Loss: 0.00001320
Iteration 164/1000 | Loss: 0.00001320
Iteration 165/1000 | Loss: 0.00001320
Iteration 166/1000 | Loss: 0.00001320
Iteration 167/1000 | Loss: 0.00001320
Iteration 168/1000 | Loss: 0.00001320
Iteration 169/1000 | Loss: 0.00001320
Iteration 170/1000 | Loss: 0.00001320
Iteration 171/1000 | Loss: 0.00001320
Iteration 172/1000 | Loss: 0.00001320
Iteration 173/1000 | Loss: 0.00001320
Iteration 174/1000 | Loss: 0.00001320
Iteration 175/1000 | Loss: 0.00001320
Iteration 176/1000 | Loss: 0.00001320
Iteration 177/1000 | Loss: 0.00001320
Iteration 178/1000 | Loss: 0.00001320
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 178. Stopping optimization.
Last 5 losses: [1.3201142792240717e-05, 1.3201142792240717e-05, 1.3201142792240717e-05, 1.3201142792240717e-05, 1.3201142792240717e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3201142792240717e-05

Optimization complete. Final v2v error: 3.071704864501953 mm

Highest mean error: 3.430551052093506 mm for frame 32

Lowest mean error: 2.8910934925079346 mm for frame 55

Saving results

Total time: 39.5089156627655
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_018/1036/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_018/1036.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_018/1036
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00776359
Iteration 2/25 | Loss: 0.00154242
Iteration 3/25 | Loss: 0.00126048
Iteration 4/25 | Loss: 0.00123305
Iteration 5/25 | Loss: 0.00122791
Iteration 6/25 | Loss: 0.00122712
Iteration 7/25 | Loss: 0.00122712
Iteration 8/25 | Loss: 0.00122712
Iteration 9/25 | Loss: 0.00122712
Iteration 10/25 | Loss: 0.00122712
Iteration 11/25 | Loss: 0.00122712
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012271172599866986, 0.0012271172599866986, 0.0012271172599866986, 0.0012271172599866986, 0.0012271172599866986]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012271172599866986

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34033263
Iteration 2/25 | Loss: 0.00069387
Iteration 3/25 | Loss: 0.00069385
Iteration 4/25 | Loss: 0.00069385
Iteration 5/25 | Loss: 0.00069385
Iteration 6/25 | Loss: 0.00069385
Iteration 7/25 | Loss: 0.00069385
Iteration 8/25 | Loss: 0.00069385
Iteration 9/25 | Loss: 0.00069385
Iteration 10/25 | Loss: 0.00069385
Iteration 11/25 | Loss: 0.00069385
Iteration 12/25 | Loss: 0.00069385
Iteration 13/25 | Loss: 0.00069385
Iteration 14/25 | Loss: 0.00069385
Iteration 15/25 | Loss: 0.00069385
Iteration 16/25 | Loss: 0.00069385
Iteration 17/25 | Loss: 0.00069385
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0006938519072718918, 0.0006938519072718918, 0.0006938519072718918, 0.0006938519072718918, 0.0006938519072718918]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006938519072718918

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00069385
Iteration 2/1000 | Loss: 0.00003220
Iteration 3/1000 | Loss: 0.00002401
Iteration 4/1000 | Loss: 0.00002222
Iteration 5/1000 | Loss: 0.00002138
Iteration 6/1000 | Loss: 0.00002057
Iteration 7/1000 | Loss: 0.00002008
Iteration 8/1000 | Loss: 0.00001969
Iteration 9/1000 | Loss: 0.00001932
Iteration 10/1000 | Loss: 0.00001909
Iteration 11/1000 | Loss: 0.00001898
Iteration 12/1000 | Loss: 0.00001880
Iteration 13/1000 | Loss: 0.00001879
Iteration 14/1000 | Loss: 0.00001872
Iteration 15/1000 | Loss: 0.00001872
Iteration 16/1000 | Loss: 0.00001871
Iteration 17/1000 | Loss: 0.00001871
Iteration 18/1000 | Loss: 0.00001871
Iteration 19/1000 | Loss: 0.00001869
Iteration 20/1000 | Loss: 0.00001862
Iteration 21/1000 | Loss: 0.00001862
Iteration 22/1000 | Loss: 0.00001860
Iteration 23/1000 | Loss: 0.00001859
Iteration 24/1000 | Loss: 0.00001859
Iteration 25/1000 | Loss: 0.00001857
Iteration 26/1000 | Loss: 0.00001857
Iteration 27/1000 | Loss: 0.00001856
Iteration 28/1000 | Loss: 0.00001855
Iteration 29/1000 | Loss: 0.00001855
Iteration 30/1000 | Loss: 0.00001855
Iteration 31/1000 | Loss: 0.00001854
Iteration 32/1000 | Loss: 0.00001854
Iteration 33/1000 | Loss: 0.00001854
Iteration 34/1000 | Loss: 0.00001853
Iteration 35/1000 | Loss: 0.00001853
Iteration 36/1000 | Loss: 0.00001853
Iteration 37/1000 | Loss: 0.00001853
Iteration 38/1000 | Loss: 0.00001853
Iteration 39/1000 | Loss: 0.00001852
Iteration 40/1000 | Loss: 0.00001852
Iteration 41/1000 | Loss: 0.00001852
Iteration 42/1000 | Loss: 0.00001852
Iteration 43/1000 | Loss: 0.00001851
Iteration 44/1000 | Loss: 0.00001851
Iteration 45/1000 | Loss: 0.00001851
Iteration 46/1000 | Loss: 0.00001851
Iteration 47/1000 | Loss: 0.00001850
Iteration 48/1000 | Loss: 0.00001850
Iteration 49/1000 | Loss: 0.00001850
Iteration 50/1000 | Loss: 0.00001850
Iteration 51/1000 | Loss: 0.00001850
Iteration 52/1000 | Loss: 0.00001850
Iteration 53/1000 | Loss: 0.00001849
Iteration 54/1000 | Loss: 0.00001849
Iteration 55/1000 | Loss: 0.00001849
Iteration 56/1000 | Loss: 0.00001849
Iteration 57/1000 | Loss: 0.00001849
Iteration 58/1000 | Loss: 0.00001848
Iteration 59/1000 | Loss: 0.00001848
Iteration 60/1000 | Loss: 0.00001848
Iteration 61/1000 | Loss: 0.00001848
Iteration 62/1000 | Loss: 0.00001848
Iteration 63/1000 | Loss: 0.00001848
Iteration 64/1000 | Loss: 0.00001848
Iteration 65/1000 | Loss: 0.00001848
Iteration 66/1000 | Loss: 0.00001848
Iteration 67/1000 | Loss: 0.00001848
Iteration 68/1000 | Loss: 0.00001848
Iteration 69/1000 | Loss: 0.00001848
Iteration 70/1000 | Loss: 0.00001848
Iteration 71/1000 | Loss: 0.00001848
Iteration 72/1000 | Loss: 0.00001847
Iteration 73/1000 | Loss: 0.00001847
Iteration 74/1000 | Loss: 0.00001847
Iteration 75/1000 | Loss: 0.00001846
Iteration 76/1000 | Loss: 0.00001846
Iteration 77/1000 | Loss: 0.00001846
Iteration 78/1000 | Loss: 0.00001846
Iteration 79/1000 | Loss: 0.00001845
Iteration 80/1000 | Loss: 0.00001845
Iteration 81/1000 | Loss: 0.00001845
Iteration 82/1000 | Loss: 0.00001845
Iteration 83/1000 | Loss: 0.00001845
Iteration 84/1000 | Loss: 0.00001845
Iteration 85/1000 | Loss: 0.00001845
Iteration 86/1000 | Loss: 0.00001845
Iteration 87/1000 | Loss: 0.00001845
Iteration 88/1000 | Loss: 0.00001845
Iteration 89/1000 | Loss: 0.00001845
Iteration 90/1000 | Loss: 0.00001845
Iteration 91/1000 | Loss: 0.00001845
Iteration 92/1000 | Loss: 0.00001844
Iteration 93/1000 | Loss: 0.00001844
Iteration 94/1000 | Loss: 0.00001844
Iteration 95/1000 | Loss: 0.00001844
Iteration 96/1000 | Loss: 0.00001844
Iteration 97/1000 | Loss: 0.00001844
Iteration 98/1000 | Loss: 0.00001844
Iteration 99/1000 | Loss: 0.00001843
Iteration 100/1000 | Loss: 0.00001843
Iteration 101/1000 | Loss: 0.00001843
Iteration 102/1000 | Loss: 0.00001843
Iteration 103/1000 | Loss: 0.00001843
Iteration 104/1000 | Loss: 0.00001842
Iteration 105/1000 | Loss: 0.00001842
Iteration 106/1000 | Loss: 0.00001842
Iteration 107/1000 | Loss: 0.00001841
Iteration 108/1000 | Loss: 0.00001841
Iteration 109/1000 | Loss: 0.00001841
Iteration 110/1000 | Loss: 0.00001840
Iteration 111/1000 | Loss: 0.00001840
Iteration 112/1000 | Loss: 0.00001840
Iteration 113/1000 | Loss: 0.00001839
Iteration 114/1000 | Loss: 0.00001839
Iteration 115/1000 | Loss: 0.00001839
Iteration 116/1000 | Loss: 0.00001839
Iteration 117/1000 | Loss: 0.00001839
Iteration 118/1000 | Loss: 0.00001838
Iteration 119/1000 | Loss: 0.00001838
Iteration 120/1000 | Loss: 0.00001838
Iteration 121/1000 | Loss: 0.00001838
Iteration 122/1000 | Loss: 0.00001838
Iteration 123/1000 | Loss: 0.00001838
Iteration 124/1000 | Loss: 0.00001838
Iteration 125/1000 | Loss: 0.00001838
Iteration 126/1000 | Loss: 0.00001838
Iteration 127/1000 | Loss: 0.00001838
Iteration 128/1000 | Loss: 0.00001838
Iteration 129/1000 | Loss: 0.00001838
Iteration 130/1000 | Loss: 0.00001838
Iteration 131/1000 | Loss: 0.00001837
Iteration 132/1000 | Loss: 0.00001837
Iteration 133/1000 | Loss: 0.00001837
Iteration 134/1000 | Loss: 0.00001837
Iteration 135/1000 | Loss: 0.00001837
Iteration 136/1000 | Loss: 0.00001837
Iteration 137/1000 | Loss: 0.00001837
Iteration 138/1000 | Loss: 0.00001837
Iteration 139/1000 | Loss: 0.00001837
Iteration 140/1000 | Loss: 0.00001837
Iteration 141/1000 | Loss: 0.00001837
Iteration 142/1000 | Loss: 0.00001837
Iteration 143/1000 | Loss: 0.00001837
Iteration 144/1000 | Loss: 0.00001837
Iteration 145/1000 | Loss: 0.00001837
Iteration 146/1000 | Loss: 0.00001837
Iteration 147/1000 | Loss: 0.00001836
Iteration 148/1000 | Loss: 0.00001836
Iteration 149/1000 | Loss: 0.00001836
Iteration 150/1000 | Loss: 0.00001836
Iteration 151/1000 | Loss: 0.00001836
Iteration 152/1000 | Loss: 0.00001836
Iteration 153/1000 | Loss: 0.00001835
Iteration 154/1000 | Loss: 0.00001835
Iteration 155/1000 | Loss: 0.00001835
Iteration 156/1000 | Loss: 0.00001835
Iteration 157/1000 | Loss: 0.00001835
Iteration 158/1000 | Loss: 0.00001834
Iteration 159/1000 | Loss: 0.00001834
Iteration 160/1000 | Loss: 0.00001834
Iteration 161/1000 | Loss: 0.00001834
Iteration 162/1000 | Loss: 0.00001834
Iteration 163/1000 | Loss: 0.00001834
Iteration 164/1000 | Loss: 0.00001834
Iteration 165/1000 | Loss: 0.00001834
Iteration 166/1000 | Loss: 0.00001834
Iteration 167/1000 | Loss: 0.00001834
Iteration 168/1000 | Loss: 0.00001834
Iteration 169/1000 | Loss: 0.00001834
Iteration 170/1000 | Loss: 0.00001834
Iteration 171/1000 | Loss: 0.00001834
Iteration 172/1000 | Loss: 0.00001834
Iteration 173/1000 | Loss: 0.00001834
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 173. Stopping optimization.
Last 5 losses: [1.8337563233217224e-05, 1.8337563233217224e-05, 1.8337563233217224e-05, 1.8337563233217224e-05, 1.8337563233217224e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8337563233217224e-05

Optimization complete. Final v2v error: 3.578657388687134 mm

Highest mean error: 3.779136896133423 mm for frame 213

Lowest mean error: 3.28359055519104 mm for frame 8

Saving results

Total time: 42.34041476249695
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_018/1054/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_018/1054.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_018/1054
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00967564
Iteration 2/25 | Loss: 0.00967563
Iteration 3/25 | Loss: 0.00967563
Iteration 4/25 | Loss: 0.00967563
Iteration 5/25 | Loss: 0.00967563
Iteration 6/25 | Loss: 0.00259307
Iteration 7/25 | Loss: 0.00198026
Iteration 8/25 | Loss: 0.00192813
Iteration 9/25 | Loss: 0.00178328
Iteration 10/25 | Loss: 0.00169730
Iteration 11/25 | Loss: 0.00156203
Iteration 12/25 | Loss: 0.00148006
Iteration 13/25 | Loss: 0.00143220
Iteration 14/25 | Loss: 0.00140479
Iteration 15/25 | Loss: 0.00136797
Iteration 16/25 | Loss: 0.00133725
Iteration 17/25 | Loss: 0.00133488
Iteration 18/25 | Loss: 0.00132733
Iteration 19/25 | Loss: 0.00131601
Iteration 20/25 | Loss: 0.00130483
Iteration 21/25 | Loss: 0.00129324
Iteration 22/25 | Loss: 0.00128225
Iteration 23/25 | Loss: 0.00127622
Iteration 24/25 | Loss: 0.00127371
Iteration 25/25 | Loss: 0.00127264

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33760583
Iteration 2/25 | Loss: 0.00166565
Iteration 3/25 | Loss: 0.00166562
Iteration 4/25 | Loss: 0.00166562
Iteration 5/25 | Loss: 0.00166562
Iteration 6/25 | Loss: 0.00166562
Iteration 7/25 | Loss: 0.00166562
Iteration 8/25 | Loss: 0.00166562
Iteration 9/25 | Loss: 0.00166562
Iteration 10/25 | Loss: 0.00166562
Iteration 11/25 | Loss: 0.00166562
Iteration 12/25 | Loss: 0.00166562
Iteration 13/25 | Loss: 0.00166562
Iteration 14/25 | Loss: 0.00166562
Iteration 15/25 | Loss: 0.00166562
Iteration 16/25 | Loss: 0.00166562
Iteration 17/25 | Loss: 0.00166562
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0016656172228977084, 0.0016656172228977084, 0.0016656172228977084, 0.0016656172228977084, 0.0016656172228977084]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0016656172228977084

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00166562
Iteration 2/1000 | Loss: 0.00125971
Iteration 3/1000 | Loss: 0.00022226
Iteration 4/1000 | Loss: 0.00014944
Iteration 5/1000 | Loss: 0.00020800
Iteration 6/1000 | Loss: 0.00008116
Iteration 7/1000 | Loss: 0.00040767
Iteration 8/1000 | Loss: 0.00012102
Iteration 9/1000 | Loss: 0.00007745
Iteration 10/1000 | Loss: 0.00025618
Iteration 11/1000 | Loss: 0.00018633
Iteration 12/1000 | Loss: 0.00034510
Iteration 13/1000 | Loss: 0.00008367
Iteration 14/1000 | Loss: 0.00006373
Iteration 15/1000 | Loss: 0.00014999
Iteration 16/1000 | Loss: 0.00007743
Iteration 17/1000 | Loss: 0.00005147
Iteration 18/1000 | Loss: 0.00004619
Iteration 19/1000 | Loss: 0.00010891
Iteration 20/1000 | Loss: 0.00007311
Iteration 21/1000 | Loss: 0.00019112
Iteration 22/1000 | Loss: 0.00004569
Iteration 23/1000 | Loss: 0.00004517
Iteration 24/1000 | Loss: 0.00004462
Iteration 25/1000 | Loss: 0.00003674
Iteration 26/1000 | Loss: 0.00003926
Iteration 27/1000 | Loss: 0.00004360
Iteration 28/1000 | Loss: 0.00002836
Iteration 29/1000 | Loss: 0.00003932
Iteration 30/1000 | Loss: 0.00003198
Iteration 31/1000 | Loss: 0.00004260
Iteration 32/1000 | Loss: 0.00003457
Iteration 33/1000 | Loss: 0.00003663
Iteration 34/1000 | Loss: 0.00003358
Iteration 35/1000 | Loss: 0.00003882
Iteration 36/1000 | Loss: 0.00003734
Iteration 37/1000 | Loss: 0.00004078
Iteration 38/1000 | Loss: 0.00004606
Iteration 39/1000 | Loss: 0.00004126
Iteration 40/1000 | Loss: 0.00002758
Iteration 41/1000 | Loss: 0.00002848
Iteration 42/1000 | Loss: 0.00003748
Iteration 43/1000 | Loss: 0.00003058
Iteration 44/1000 | Loss: 0.00003401
Iteration 45/1000 | Loss: 0.00002991
Iteration 46/1000 | Loss: 0.00003330
Iteration 47/1000 | Loss: 0.00003709
Iteration 48/1000 | Loss: 0.00003608
Iteration 49/1000 | Loss: 0.00003542
Iteration 50/1000 | Loss: 0.00003532
Iteration 51/1000 | Loss: 0.00004686
Iteration 52/1000 | Loss: 0.00003488
Iteration 53/1000 | Loss: 0.00003966
Iteration 54/1000 | Loss: 0.00003966
Iteration 55/1000 | Loss: 0.00004027
Iteration 56/1000 | Loss: 0.00003714
Iteration 57/1000 | Loss: 0.00003619
Iteration 58/1000 | Loss: 0.00004045
Iteration 59/1000 | Loss: 0.00005147
Iteration 60/1000 | Loss: 0.00002351
Iteration 61/1000 | Loss: 0.00002148
Iteration 62/1000 | Loss: 0.00002050
Iteration 63/1000 | Loss: 0.00002004
Iteration 64/1000 | Loss: 0.00001972
Iteration 65/1000 | Loss: 0.00001943
Iteration 66/1000 | Loss: 0.00001908
Iteration 67/1000 | Loss: 0.00001882
Iteration 68/1000 | Loss: 0.00001875
Iteration 69/1000 | Loss: 0.00001853
Iteration 70/1000 | Loss: 0.00001842
Iteration 71/1000 | Loss: 0.00001841
Iteration 72/1000 | Loss: 0.00001838
Iteration 73/1000 | Loss: 0.00001834
Iteration 74/1000 | Loss: 0.00001834
Iteration 75/1000 | Loss: 0.00001834
Iteration 76/1000 | Loss: 0.00001833
Iteration 77/1000 | Loss: 0.00001832
Iteration 78/1000 | Loss: 0.00001832
Iteration 79/1000 | Loss: 0.00001832
Iteration 80/1000 | Loss: 0.00001826
Iteration 81/1000 | Loss: 0.00001824
Iteration 82/1000 | Loss: 0.00001823
Iteration 83/1000 | Loss: 0.00001823
Iteration 84/1000 | Loss: 0.00001822
Iteration 85/1000 | Loss: 0.00001822
Iteration 86/1000 | Loss: 0.00001822
Iteration 87/1000 | Loss: 0.00001821
Iteration 88/1000 | Loss: 0.00001821
Iteration 89/1000 | Loss: 0.00001821
Iteration 90/1000 | Loss: 0.00001821
Iteration 91/1000 | Loss: 0.00001821
Iteration 92/1000 | Loss: 0.00001821
Iteration 93/1000 | Loss: 0.00001820
Iteration 94/1000 | Loss: 0.00001820
Iteration 95/1000 | Loss: 0.00001820
Iteration 96/1000 | Loss: 0.00001820
Iteration 97/1000 | Loss: 0.00001820
Iteration 98/1000 | Loss: 0.00001819
Iteration 99/1000 | Loss: 0.00001819
Iteration 100/1000 | Loss: 0.00001819
Iteration 101/1000 | Loss: 0.00001818
Iteration 102/1000 | Loss: 0.00001818
Iteration 103/1000 | Loss: 0.00001817
Iteration 104/1000 | Loss: 0.00001813
Iteration 105/1000 | Loss: 0.00001810
Iteration 106/1000 | Loss: 0.00001809
Iteration 107/1000 | Loss: 0.00001809
Iteration 108/1000 | Loss: 0.00001809
Iteration 109/1000 | Loss: 0.00001809
Iteration 110/1000 | Loss: 0.00001808
Iteration 111/1000 | Loss: 0.00001808
Iteration 112/1000 | Loss: 0.00001807
Iteration 113/1000 | Loss: 0.00001807
Iteration 114/1000 | Loss: 0.00001807
Iteration 115/1000 | Loss: 0.00001806
Iteration 116/1000 | Loss: 0.00001806
Iteration 117/1000 | Loss: 0.00001806
Iteration 118/1000 | Loss: 0.00001806
Iteration 119/1000 | Loss: 0.00001806
Iteration 120/1000 | Loss: 0.00001805
Iteration 121/1000 | Loss: 0.00001805
Iteration 122/1000 | Loss: 0.00001805
Iteration 123/1000 | Loss: 0.00001805
Iteration 124/1000 | Loss: 0.00001805
Iteration 125/1000 | Loss: 0.00001804
Iteration 126/1000 | Loss: 0.00001804
Iteration 127/1000 | Loss: 0.00001803
Iteration 128/1000 | Loss: 0.00001803
Iteration 129/1000 | Loss: 0.00001803
Iteration 130/1000 | Loss: 0.00001802
Iteration 131/1000 | Loss: 0.00001802
Iteration 132/1000 | Loss: 0.00001801
Iteration 133/1000 | Loss: 0.00001801
Iteration 134/1000 | Loss: 0.00001962
Iteration 135/1000 | Loss: 0.00001837
Iteration 136/1000 | Loss: 0.00001812
Iteration 137/1000 | Loss: 0.00001802
Iteration 138/1000 | Loss: 0.00001792
Iteration 139/1000 | Loss: 0.00001785
Iteration 140/1000 | Loss: 0.00001782
Iteration 141/1000 | Loss: 0.00001781
Iteration 142/1000 | Loss: 0.00001781
Iteration 143/1000 | Loss: 0.00001780
Iteration 144/1000 | Loss: 0.00001780
Iteration 145/1000 | Loss: 0.00001779
Iteration 146/1000 | Loss: 0.00001779
Iteration 147/1000 | Loss: 0.00001778
Iteration 148/1000 | Loss: 0.00001778
Iteration 149/1000 | Loss: 0.00001778
Iteration 150/1000 | Loss: 0.00001777
Iteration 151/1000 | Loss: 0.00001777
Iteration 152/1000 | Loss: 0.00001776
Iteration 153/1000 | Loss: 0.00001776
Iteration 154/1000 | Loss: 0.00001775
Iteration 155/1000 | Loss: 0.00001775
Iteration 156/1000 | Loss: 0.00001774
Iteration 157/1000 | Loss: 0.00001774
Iteration 158/1000 | Loss: 0.00001771
Iteration 159/1000 | Loss: 0.00001770
Iteration 160/1000 | Loss: 0.00001770
Iteration 161/1000 | Loss: 0.00001769
Iteration 162/1000 | Loss: 0.00001765
Iteration 163/1000 | Loss: 0.00001761
Iteration 164/1000 | Loss: 0.00001761
Iteration 165/1000 | Loss: 0.00001760
Iteration 166/1000 | Loss: 0.00001759
Iteration 167/1000 | Loss: 0.00001758
Iteration 168/1000 | Loss: 0.00001757
Iteration 169/1000 | Loss: 0.00001757
Iteration 170/1000 | Loss: 0.00001757
Iteration 171/1000 | Loss: 0.00001757
Iteration 172/1000 | Loss: 0.00001756
Iteration 173/1000 | Loss: 0.00001756
Iteration 174/1000 | Loss: 0.00001754
Iteration 175/1000 | Loss: 0.00001754
Iteration 176/1000 | Loss: 0.00001753
Iteration 177/1000 | Loss: 0.00001753
Iteration 178/1000 | Loss: 0.00001753
Iteration 179/1000 | Loss: 0.00001753
Iteration 180/1000 | Loss: 0.00001752
Iteration 181/1000 | Loss: 0.00001752
Iteration 182/1000 | Loss: 0.00001752
Iteration 183/1000 | Loss: 0.00001752
Iteration 184/1000 | Loss: 0.00001752
Iteration 185/1000 | Loss: 0.00001752
Iteration 186/1000 | Loss: 0.00001752
Iteration 187/1000 | Loss: 0.00001751
Iteration 188/1000 | Loss: 0.00001751
Iteration 189/1000 | Loss: 0.00001751
Iteration 190/1000 | Loss: 0.00001751
Iteration 191/1000 | Loss: 0.00001751
Iteration 192/1000 | Loss: 0.00001751
Iteration 193/1000 | Loss: 0.00001751
Iteration 194/1000 | Loss: 0.00001751
Iteration 195/1000 | Loss: 0.00001751
Iteration 196/1000 | Loss: 0.00001751
Iteration 197/1000 | Loss: 0.00001751
Iteration 198/1000 | Loss: 0.00001751
Iteration 199/1000 | Loss: 0.00001751
Iteration 200/1000 | Loss: 0.00001751
Iteration 201/1000 | Loss: 0.00001751
Iteration 202/1000 | Loss: 0.00001750
Iteration 203/1000 | Loss: 0.00001750
Iteration 204/1000 | Loss: 0.00001750
Iteration 205/1000 | Loss: 0.00001750
Iteration 206/1000 | Loss: 0.00001750
Iteration 207/1000 | Loss: 0.00001750
Iteration 208/1000 | Loss: 0.00001750
Iteration 209/1000 | Loss: 0.00001750
Iteration 210/1000 | Loss: 0.00001750
Iteration 211/1000 | Loss: 0.00001750
Iteration 212/1000 | Loss: 0.00001750
Iteration 213/1000 | Loss: 0.00001750
Iteration 214/1000 | Loss: 0.00001750
Iteration 215/1000 | Loss: 0.00001750
Iteration 216/1000 | Loss: 0.00001750
Iteration 217/1000 | Loss: 0.00001750
Iteration 218/1000 | Loss: 0.00001750
Iteration 219/1000 | Loss: 0.00001749
Iteration 220/1000 | Loss: 0.00001749
Iteration 221/1000 | Loss: 0.00001749
Iteration 222/1000 | Loss: 0.00001749
Iteration 223/1000 | Loss: 0.00001749
Iteration 224/1000 | Loss: 0.00001749
Iteration 225/1000 | Loss: 0.00001749
Iteration 226/1000 | Loss: 0.00001748
Iteration 227/1000 | Loss: 0.00001748
Iteration 228/1000 | Loss: 0.00001748
Iteration 229/1000 | Loss: 0.00001748
Iteration 230/1000 | Loss: 0.00001748
Iteration 231/1000 | Loss: 0.00001748
Iteration 232/1000 | Loss: 0.00001748
Iteration 233/1000 | Loss: 0.00001748
Iteration 234/1000 | Loss: 0.00001748
Iteration 235/1000 | Loss: 0.00001747
Iteration 236/1000 | Loss: 0.00001747
Iteration 237/1000 | Loss: 0.00001747
Iteration 238/1000 | Loss: 0.00001747
Iteration 239/1000 | Loss: 0.00001747
Iteration 240/1000 | Loss: 0.00001747
Iteration 241/1000 | Loss: 0.00001747
Iteration 242/1000 | Loss: 0.00001747
Iteration 243/1000 | Loss: 0.00001747
Iteration 244/1000 | Loss: 0.00001747
Iteration 245/1000 | Loss: 0.00001747
Iteration 246/1000 | Loss: 0.00001747
Iteration 247/1000 | Loss: 0.00001747
Iteration 248/1000 | Loss: 0.00001747
Iteration 249/1000 | Loss: 0.00001747
Iteration 250/1000 | Loss: 0.00001746
Iteration 251/1000 | Loss: 0.00001746
Iteration 252/1000 | Loss: 0.00001746
Iteration 253/1000 | Loss: 0.00001746
Iteration 254/1000 | Loss: 0.00001746
Iteration 255/1000 | Loss: 0.00001746
Iteration 256/1000 | Loss: 0.00001746
Iteration 257/1000 | Loss: 0.00001746
Iteration 258/1000 | Loss: 0.00001746
Iteration 259/1000 | Loss: 0.00001746
Iteration 260/1000 | Loss: 0.00001746
Iteration 261/1000 | Loss: 0.00001746
Iteration 262/1000 | Loss: 0.00001746
Iteration 263/1000 | Loss: 0.00001746
Iteration 264/1000 | Loss: 0.00001746
Iteration 265/1000 | Loss: 0.00001746
Iteration 266/1000 | Loss: 0.00001746
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 266. Stopping optimization.
Last 5 losses: [1.746370799082797e-05, 1.746370799082797e-05, 1.746370799082797e-05, 1.746370799082797e-05, 1.746370799082797e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.746370799082797e-05

Optimization complete. Final v2v error: 3.342419385910034 mm

Highest mean error: 10.959039688110352 mm for frame 84

Lowest mean error: 2.858872413635254 mm for frame 215

Saving results

Total time: 184.02922129631042
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_018/1015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_018/1015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_018/1015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00490963
Iteration 2/25 | Loss: 0.00127575
Iteration 3/25 | Loss: 0.00120919
Iteration 4/25 | Loss: 0.00119735
Iteration 5/25 | Loss: 0.00119322
Iteration 6/25 | Loss: 0.00119303
Iteration 7/25 | Loss: 0.00119303
Iteration 8/25 | Loss: 0.00119303
Iteration 9/25 | Loss: 0.00119303
Iteration 10/25 | Loss: 0.00119303
Iteration 11/25 | Loss: 0.00119303
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011930344626307487, 0.0011930344626307487, 0.0011930344626307487, 0.0011930344626307487, 0.0011930344626307487]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011930344626307487

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34758842
Iteration 2/25 | Loss: 0.00093825
Iteration 3/25 | Loss: 0.00093825
Iteration 4/25 | Loss: 0.00093825
Iteration 5/25 | Loss: 0.00093825
Iteration 6/25 | Loss: 0.00093825
Iteration 7/25 | Loss: 0.00093825
Iteration 8/25 | Loss: 0.00093825
Iteration 9/25 | Loss: 0.00093825
Iteration 10/25 | Loss: 0.00093825
Iteration 11/25 | Loss: 0.00093825
Iteration 12/25 | Loss: 0.00093825
Iteration 13/25 | Loss: 0.00093825
Iteration 14/25 | Loss: 0.00093825
Iteration 15/25 | Loss: 0.00093825
Iteration 16/25 | Loss: 0.00093825
Iteration 17/25 | Loss: 0.00093825
Iteration 18/25 | Loss: 0.00093825
Iteration 19/25 | Loss: 0.00093825
Iteration 20/25 | Loss: 0.00093825
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0009382485877722502, 0.0009382485877722502, 0.0009382485877722502, 0.0009382485877722502, 0.0009382485877722502]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009382485877722502

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00093825
Iteration 2/1000 | Loss: 0.00004630
Iteration 3/1000 | Loss: 0.00003139
Iteration 4/1000 | Loss: 0.00002640
Iteration 5/1000 | Loss: 0.00002500
Iteration 6/1000 | Loss: 0.00002423
Iteration 7/1000 | Loss: 0.00002363
Iteration 8/1000 | Loss: 0.00002316
Iteration 9/1000 | Loss: 0.00002283
Iteration 10/1000 | Loss: 0.00002257
Iteration 11/1000 | Loss: 0.00002236
Iteration 12/1000 | Loss: 0.00002235
Iteration 13/1000 | Loss: 0.00002226
Iteration 14/1000 | Loss: 0.00002226
Iteration 15/1000 | Loss: 0.00002219
Iteration 16/1000 | Loss: 0.00002219
Iteration 17/1000 | Loss: 0.00002217
Iteration 18/1000 | Loss: 0.00002217
Iteration 19/1000 | Loss: 0.00002211
Iteration 20/1000 | Loss: 0.00002207
Iteration 21/1000 | Loss: 0.00002206
Iteration 22/1000 | Loss: 0.00002206
Iteration 23/1000 | Loss: 0.00002204
Iteration 24/1000 | Loss: 0.00002203
Iteration 25/1000 | Loss: 0.00002202
Iteration 26/1000 | Loss: 0.00002201
Iteration 27/1000 | Loss: 0.00002201
Iteration 28/1000 | Loss: 0.00002200
Iteration 29/1000 | Loss: 0.00002198
Iteration 30/1000 | Loss: 0.00002196
Iteration 31/1000 | Loss: 0.00002195
Iteration 32/1000 | Loss: 0.00002195
Iteration 33/1000 | Loss: 0.00002195
Iteration 34/1000 | Loss: 0.00002194
Iteration 35/1000 | Loss: 0.00002194
Iteration 36/1000 | Loss: 0.00002193
Iteration 37/1000 | Loss: 0.00002187
Iteration 38/1000 | Loss: 0.00002185
Iteration 39/1000 | Loss: 0.00002183
Iteration 40/1000 | Loss: 0.00002181
Iteration 41/1000 | Loss: 0.00002180
Iteration 42/1000 | Loss: 0.00002179
Iteration 43/1000 | Loss: 0.00002178
Iteration 44/1000 | Loss: 0.00002178
Iteration 45/1000 | Loss: 0.00002178
Iteration 46/1000 | Loss: 0.00002177
Iteration 47/1000 | Loss: 0.00002177
Iteration 48/1000 | Loss: 0.00002176
Iteration 49/1000 | Loss: 0.00002175
Iteration 50/1000 | Loss: 0.00002175
Iteration 51/1000 | Loss: 0.00002174
Iteration 52/1000 | Loss: 0.00002174
Iteration 53/1000 | Loss: 0.00002173
Iteration 54/1000 | Loss: 0.00002173
Iteration 55/1000 | Loss: 0.00002173
Iteration 56/1000 | Loss: 0.00002172
Iteration 57/1000 | Loss: 0.00002171
Iteration 58/1000 | Loss: 0.00002169
Iteration 59/1000 | Loss: 0.00002169
Iteration 60/1000 | Loss: 0.00002169
Iteration 61/1000 | Loss: 0.00002168
Iteration 62/1000 | Loss: 0.00002168
Iteration 63/1000 | Loss: 0.00002167
Iteration 64/1000 | Loss: 0.00002167
Iteration 65/1000 | Loss: 0.00002166
Iteration 66/1000 | Loss: 0.00002165
Iteration 67/1000 | Loss: 0.00002165
Iteration 68/1000 | Loss: 0.00002165
Iteration 69/1000 | Loss: 0.00002164
Iteration 70/1000 | Loss: 0.00002164
Iteration 71/1000 | Loss: 0.00002164
Iteration 72/1000 | Loss: 0.00002164
Iteration 73/1000 | Loss: 0.00002164
Iteration 74/1000 | Loss: 0.00002164
Iteration 75/1000 | Loss: 0.00002163
Iteration 76/1000 | Loss: 0.00002162
Iteration 77/1000 | Loss: 0.00002162
Iteration 78/1000 | Loss: 0.00002162
Iteration 79/1000 | Loss: 0.00002162
Iteration 80/1000 | Loss: 0.00002161
Iteration 81/1000 | Loss: 0.00002161
Iteration 82/1000 | Loss: 0.00002161
Iteration 83/1000 | Loss: 0.00002161
Iteration 84/1000 | Loss: 0.00002161
Iteration 85/1000 | Loss: 0.00002160
Iteration 86/1000 | Loss: 0.00002160
Iteration 87/1000 | Loss: 0.00002160
Iteration 88/1000 | Loss: 0.00002160
Iteration 89/1000 | Loss: 0.00002159
Iteration 90/1000 | Loss: 0.00002159
Iteration 91/1000 | Loss: 0.00002158
Iteration 92/1000 | Loss: 0.00002158
Iteration 93/1000 | Loss: 0.00002158
Iteration 94/1000 | Loss: 0.00002158
Iteration 95/1000 | Loss: 0.00002158
Iteration 96/1000 | Loss: 0.00002157
Iteration 97/1000 | Loss: 0.00002157
Iteration 98/1000 | Loss: 0.00002157
Iteration 99/1000 | Loss: 0.00002156
Iteration 100/1000 | Loss: 0.00002156
Iteration 101/1000 | Loss: 0.00002156
Iteration 102/1000 | Loss: 0.00002156
Iteration 103/1000 | Loss: 0.00002156
Iteration 104/1000 | Loss: 0.00002155
Iteration 105/1000 | Loss: 0.00002155
Iteration 106/1000 | Loss: 0.00002155
Iteration 107/1000 | Loss: 0.00002155
Iteration 108/1000 | Loss: 0.00002154
Iteration 109/1000 | Loss: 0.00002154
Iteration 110/1000 | Loss: 0.00002154
Iteration 111/1000 | Loss: 0.00002154
Iteration 112/1000 | Loss: 0.00002153
Iteration 113/1000 | Loss: 0.00002153
Iteration 114/1000 | Loss: 0.00002153
Iteration 115/1000 | Loss: 0.00002153
Iteration 116/1000 | Loss: 0.00002152
Iteration 117/1000 | Loss: 0.00002152
Iteration 118/1000 | Loss: 0.00002152
Iteration 119/1000 | Loss: 0.00002152
Iteration 120/1000 | Loss: 0.00002152
Iteration 121/1000 | Loss: 0.00002152
Iteration 122/1000 | Loss: 0.00002151
Iteration 123/1000 | Loss: 0.00002151
Iteration 124/1000 | Loss: 0.00002151
Iteration 125/1000 | Loss: 0.00002151
Iteration 126/1000 | Loss: 0.00002151
Iteration 127/1000 | Loss: 0.00002151
Iteration 128/1000 | Loss: 0.00002151
Iteration 129/1000 | Loss: 0.00002151
Iteration 130/1000 | Loss: 0.00002151
Iteration 131/1000 | Loss: 0.00002150
Iteration 132/1000 | Loss: 0.00002150
Iteration 133/1000 | Loss: 0.00002150
Iteration 134/1000 | Loss: 0.00002149
Iteration 135/1000 | Loss: 0.00002149
Iteration 136/1000 | Loss: 0.00002149
Iteration 137/1000 | Loss: 0.00002149
Iteration 138/1000 | Loss: 0.00002149
Iteration 139/1000 | Loss: 0.00002149
Iteration 140/1000 | Loss: 0.00002149
Iteration 141/1000 | Loss: 0.00002149
Iteration 142/1000 | Loss: 0.00002149
Iteration 143/1000 | Loss: 0.00002149
Iteration 144/1000 | Loss: 0.00002149
Iteration 145/1000 | Loss: 0.00002149
Iteration 146/1000 | Loss: 0.00002149
Iteration 147/1000 | Loss: 0.00002149
Iteration 148/1000 | Loss: 0.00002149
Iteration 149/1000 | Loss: 0.00002148
Iteration 150/1000 | Loss: 0.00002148
Iteration 151/1000 | Loss: 0.00002148
Iteration 152/1000 | Loss: 0.00002148
Iteration 153/1000 | Loss: 0.00002148
Iteration 154/1000 | Loss: 0.00002148
Iteration 155/1000 | Loss: 0.00002148
Iteration 156/1000 | Loss: 0.00002148
Iteration 157/1000 | Loss: 0.00002148
Iteration 158/1000 | Loss: 0.00002148
Iteration 159/1000 | Loss: 0.00002148
Iteration 160/1000 | Loss: 0.00002148
Iteration 161/1000 | Loss: 0.00002148
Iteration 162/1000 | Loss: 0.00002148
Iteration 163/1000 | Loss: 0.00002148
Iteration 164/1000 | Loss: 0.00002147
Iteration 165/1000 | Loss: 0.00002147
Iteration 166/1000 | Loss: 0.00002147
Iteration 167/1000 | Loss: 0.00002147
Iteration 168/1000 | Loss: 0.00002147
Iteration 169/1000 | Loss: 0.00002147
Iteration 170/1000 | Loss: 0.00002147
Iteration 171/1000 | Loss: 0.00002147
Iteration 172/1000 | Loss: 0.00002147
Iteration 173/1000 | Loss: 0.00002147
Iteration 174/1000 | Loss: 0.00002147
Iteration 175/1000 | Loss: 0.00002147
Iteration 176/1000 | Loss: 0.00002147
Iteration 177/1000 | Loss: 0.00002147
Iteration 178/1000 | Loss: 0.00002147
Iteration 179/1000 | Loss: 0.00002147
Iteration 180/1000 | Loss: 0.00002147
Iteration 181/1000 | Loss: 0.00002147
Iteration 182/1000 | Loss: 0.00002147
Iteration 183/1000 | Loss: 0.00002147
Iteration 184/1000 | Loss: 0.00002147
Iteration 185/1000 | Loss: 0.00002147
Iteration 186/1000 | Loss: 0.00002147
Iteration 187/1000 | Loss: 0.00002147
Iteration 188/1000 | Loss: 0.00002147
Iteration 189/1000 | Loss: 0.00002147
Iteration 190/1000 | Loss: 0.00002147
Iteration 191/1000 | Loss: 0.00002147
Iteration 192/1000 | Loss: 0.00002147
Iteration 193/1000 | Loss: 0.00002147
Iteration 194/1000 | Loss: 0.00002147
Iteration 195/1000 | Loss: 0.00002147
Iteration 196/1000 | Loss: 0.00002147
Iteration 197/1000 | Loss: 0.00002147
Iteration 198/1000 | Loss: 0.00002147
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 198. Stopping optimization.
Last 5 losses: [2.1472391381394118e-05, 2.1472391381394118e-05, 2.1472391381394118e-05, 2.1472391381394118e-05, 2.1472391381394118e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1472391381394118e-05

Optimization complete. Final v2v error: 3.5468766689300537 mm

Highest mean error: 4.578654766082764 mm for frame 91

Lowest mean error: 3.0522375106811523 mm for frame 136

Saving results

Total time: 42.93811297416687
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_018/1053/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_018/1053.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_018/1053
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00810770
Iteration 2/25 | Loss: 0.00169369
Iteration 3/25 | Loss: 0.00125304
Iteration 4/25 | Loss: 0.00121615
Iteration 5/25 | Loss: 0.00121078
Iteration 6/25 | Loss: 0.00121078
Iteration 7/25 | Loss: 0.00121078
Iteration 8/25 | Loss: 0.00121078
Iteration 9/25 | Loss: 0.00121078
Iteration 10/25 | Loss: 0.00121078
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.001210778602398932, 0.001210778602398932, 0.001210778602398932, 0.001210778602398932, 0.001210778602398932]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001210778602398932

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35245323
Iteration 2/25 | Loss: 0.00065110
Iteration 3/25 | Loss: 0.00065110
Iteration 4/25 | Loss: 0.00065110
Iteration 5/25 | Loss: 0.00065110
Iteration 6/25 | Loss: 0.00065110
Iteration 7/25 | Loss: 0.00065110
Iteration 8/25 | Loss: 0.00065110
Iteration 9/25 | Loss: 0.00065110
Iteration 10/25 | Loss: 0.00065110
Iteration 11/25 | Loss: 0.00065110
Iteration 12/25 | Loss: 0.00065110
Iteration 13/25 | Loss: 0.00065110
Iteration 14/25 | Loss: 0.00065110
Iteration 15/25 | Loss: 0.00065110
Iteration 16/25 | Loss: 0.00065110
Iteration 17/25 | Loss: 0.00065110
Iteration 18/25 | Loss: 0.00065110
Iteration 19/25 | Loss: 0.00065110
Iteration 20/25 | Loss: 0.00065110
Iteration 21/25 | Loss: 0.00065110
Iteration 22/25 | Loss: 0.00065110
Iteration 23/25 | Loss: 0.00065110
Iteration 24/25 | Loss: 0.00065110
Iteration 25/25 | Loss: 0.00065110

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00065110
Iteration 2/1000 | Loss: 0.00003165
Iteration 3/1000 | Loss: 0.00002310
Iteration 4/1000 | Loss: 0.00002178
Iteration 5/1000 | Loss: 0.00002089
Iteration 6/1000 | Loss: 0.00002034
Iteration 7/1000 | Loss: 0.00002000
Iteration 8/1000 | Loss: 0.00001963
Iteration 9/1000 | Loss: 0.00001939
Iteration 10/1000 | Loss: 0.00001925
Iteration 11/1000 | Loss: 0.00001910
Iteration 12/1000 | Loss: 0.00001908
Iteration 13/1000 | Loss: 0.00001898
Iteration 14/1000 | Loss: 0.00001895
Iteration 15/1000 | Loss: 0.00001895
Iteration 16/1000 | Loss: 0.00001895
Iteration 17/1000 | Loss: 0.00001894
Iteration 18/1000 | Loss: 0.00001888
Iteration 19/1000 | Loss: 0.00001886
Iteration 20/1000 | Loss: 0.00001884
Iteration 21/1000 | Loss: 0.00001880
Iteration 22/1000 | Loss: 0.00001878
Iteration 23/1000 | Loss: 0.00001878
Iteration 24/1000 | Loss: 0.00001878
Iteration 25/1000 | Loss: 0.00001877
Iteration 26/1000 | Loss: 0.00001876
Iteration 27/1000 | Loss: 0.00001875
Iteration 28/1000 | Loss: 0.00001875
Iteration 29/1000 | Loss: 0.00001874
Iteration 30/1000 | Loss: 0.00001874
Iteration 31/1000 | Loss: 0.00001873
Iteration 32/1000 | Loss: 0.00001873
Iteration 33/1000 | Loss: 0.00001873
Iteration 34/1000 | Loss: 0.00001872
Iteration 35/1000 | Loss: 0.00001872
Iteration 36/1000 | Loss: 0.00001872
Iteration 37/1000 | Loss: 0.00001872
Iteration 38/1000 | Loss: 0.00001872
Iteration 39/1000 | Loss: 0.00001872
Iteration 40/1000 | Loss: 0.00001872
Iteration 41/1000 | Loss: 0.00001872
Iteration 42/1000 | Loss: 0.00001871
Iteration 43/1000 | Loss: 0.00001871
Iteration 44/1000 | Loss: 0.00001871
Iteration 45/1000 | Loss: 0.00001871
Iteration 46/1000 | Loss: 0.00001871
Iteration 47/1000 | Loss: 0.00001871
Iteration 48/1000 | Loss: 0.00001870
Iteration 49/1000 | Loss: 0.00001870
Iteration 50/1000 | Loss: 0.00001870
Iteration 51/1000 | Loss: 0.00001870
Iteration 52/1000 | Loss: 0.00001870
Iteration 53/1000 | Loss: 0.00001869
Iteration 54/1000 | Loss: 0.00001869
Iteration 55/1000 | Loss: 0.00001869
Iteration 56/1000 | Loss: 0.00001869
Iteration 57/1000 | Loss: 0.00001869
Iteration 58/1000 | Loss: 0.00001869
Iteration 59/1000 | Loss: 0.00001868
Iteration 60/1000 | Loss: 0.00001867
Iteration 61/1000 | Loss: 0.00001867
Iteration 62/1000 | Loss: 0.00001867
Iteration 63/1000 | Loss: 0.00001867
Iteration 64/1000 | Loss: 0.00001867
Iteration 65/1000 | Loss: 0.00001867
Iteration 66/1000 | Loss: 0.00001866
Iteration 67/1000 | Loss: 0.00001866
Iteration 68/1000 | Loss: 0.00001866
Iteration 69/1000 | Loss: 0.00001866
Iteration 70/1000 | Loss: 0.00001866
Iteration 71/1000 | Loss: 0.00001866
Iteration 72/1000 | Loss: 0.00001866
Iteration 73/1000 | Loss: 0.00001866
Iteration 74/1000 | Loss: 0.00001865
Iteration 75/1000 | Loss: 0.00001865
Iteration 76/1000 | Loss: 0.00001865
Iteration 77/1000 | Loss: 0.00001865
Iteration 78/1000 | Loss: 0.00001865
Iteration 79/1000 | Loss: 0.00001865
Iteration 80/1000 | Loss: 0.00001865
Iteration 81/1000 | Loss: 0.00001865
Iteration 82/1000 | Loss: 0.00001865
Iteration 83/1000 | Loss: 0.00001865
Iteration 84/1000 | Loss: 0.00001864
Iteration 85/1000 | Loss: 0.00001864
Iteration 86/1000 | Loss: 0.00001864
Iteration 87/1000 | Loss: 0.00001864
Iteration 88/1000 | Loss: 0.00001864
Iteration 89/1000 | Loss: 0.00001864
Iteration 90/1000 | Loss: 0.00001863
Iteration 91/1000 | Loss: 0.00001863
Iteration 92/1000 | Loss: 0.00001863
Iteration 93/1000 | Loss: 0.00001863
Iteration 94/1000 | Loss: 0.00001863
Iteration 95/1000 | Loss: 0.00001863
Iteration 96/1000 | Loss: 0.00001863
Iteration 97/1000 | Loss: 0.00001863
Iteration 98/1000 | Loss: 0.00001863
Iteration 99/1000 | Loss: 0.00001862
Iteration 100/1000 | Loss: 0.00001862
Iteration 101/1000 | Loss: 0.00001862
Iteration 102/1000 | Loss: 0.00001861
Iteration 103/1000 | Loss: 0.00001861
Iteration 104/1000 | Loss: 0.00001861
Iteration 105/1000 | Loss: 0.00001861
Iteration 106/1000 | Loss: 0.00001861
Iteration 107/1000 | Loss: 0.00001861
Iteration 108/1000 | Loss: 0.00001861
Iteration 109/1000 | Loss: 0.00001861
Iteration 110/1000 | Loss: 0.00001861
Iteration 111/1000 | Loss: 0.00001860
Iteration 112/1000 | Loss: 0.00001860
Iteration 113/1000 | Loss: 0.00001860
Iteration 114/1000 | Loss: 0.00001860
Iteration 115/1000 | Loss: 0.00001860
Iteration 116/1000 | Loss: 0.00001860
Iteration 117/1000 | Loss: 0.00001859
Iteration 118/1000 | Loss: 0.00001859
Iteration 119/1000 | Loss: 0.00001859
Iteration 120/1000 | Loss: 0.00001859
Iteration 121/1000 | Loss: 0.00001859
Iteration 122/1000 | Loss: 0.00001859
Iteration 123/1000 | Loss: 0.00001859
Iteration 124/1000 | Loss: 0.00001859
Iteration 125/1000 | Loss: 0.00001859
Iteration 126/1000 | Loss: 0.00001859
Iteration 127/1000 | Loss: 0.00001859
Iteration 128/1000 | Loss: 0.00001859
Iteration 129/1000 | Loss: 0.00001859
Iteration 130/1000 | Loss: 0.00001859
Iteration 131/1000 | Loss: 0.00001859
Iteration 132/1000 | Loss: 0.00001858
Iteration 133/1000 | Loss: 0.00001858
Iteration 134/1000 | Loss: 0.00001858
Iteration 135/1000 | Loss: 0.00001858
Iteration 136/1000 | Loss: 0.00001858
Iteration 137/1000 | Loss: 0.00001858
Iteration 138/1000 | Loss: 0.00001858
Iteration 139/1000 | Loss: 0.00001858
Iteration 140/1000 | Loss: 0.00001858
Iteration 141/1000 | Loss: 0.00001858
Iteration 142/1000 | Loss: 0.00001858
Iteration 143/1000 | Loss: 0.00001858
Iteration 144/1000 | Loss: 0.00001858
Iteration 145/1000 | Loss: 0.00001858
Iteration 146/1000 | Loss: 0.00001858
Iteration 147/1000 | Loss: 0.00001858
Iteration 148/1000 | Loss: 0.00001858
Iteration 149/1000 | Loss: 0.00001858
Iteration 150/1000 | Loss: 0.00001858
Iteration 151/1000 | Loss: 0.00001858
Iteration 152/1000 | Loss: 0.00001858
Iteration 153/1000 | Loss: 0.00001858
Iteration 154/1000 | Loss: 0.00001858
Iteration 155/1000 | Loss: 0.00001858
Iteration 156/1000 | Loss: 0.00001858
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 156. Stopping optimization.
Last 5 losses: [1.8577056835056283e-05, 1.8577056835056283e-05, 1.8577056835056283e-05, 1.8577056835056283e-05, 1.8577056835056283e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8577056835056283e-05

Optimization complete. Final v2v error: 3.5886337757110596 mm

Highest mean error: 3.6616086959838867 mm for frame 74

Lowest mean error: 3.21504282951355 mm for frame 4

Saving results

Total time: 40.78685450553894
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_018/1037/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_018/1037.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_018/1037
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01023937
Iteration 2/25 | Loss: 0.00245887
Iteration 3/25 | Loss: 0.00189613
Iteration 4/25 | Loss: 0.00159393
Iteration 5/25 | Loss: 0.00161158
Iteration 6/25 | Loss: 0.00151025
Iteration 7/25 | Loss: 0.00148143
Iteration 8/25 | Loss: 0.00139258
Iteration 9/25 | Loss: 0.00136486
Iteration 10/25 | Loss: 0.00134718
Iteration 11/25 | Loss: 0.00129458
Iteration 12/25 | Loss: 0.00128879
Iteration 13/25 | Loss: 0.00129336
Iteration 14/25 | Loss: 0.00128818
Iteration 15/25 | Loss: 0.00128632
Iteration 16/25 | Loss: 0.00128175
Iteration 17/25 | Loss: 0.00128685
Iteration 18/25 | Loss: 0.00127998
Iteration 19/25 | Loss: 0.00129061
Iteration 20/25 | Loss: 0.00128647
Iteration 21/25 | Loss: 0.00127723
Iteration 22/25 | Loss: 0.00127888
Iteration 23/25 | Loss: 0.00128778
Iteration 24/25 | Loss: 0.00127786
Iteration 25/25 | Loss: 0.00128984

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37729490
Iteration 2/25 | Loss: 0.00251425
Iteration 3/25 | Loss: 0.00148897
Iteration 4/25 | Loss: 0.00148897
Iteration 5/25 | Loss: 0.00148897
Iteration 6/25 | Loss: 0.00148897
Iteration 7/25 | Loss: 0.00148897
Iteration 8/25 | Loss: 0.00148897
Iteration 9/25 | Loss: 0.00148897
Iteration 10/25 | Loss: 0.00148897
Iteration 11/25 | Loss: 0.00148897
Iteration 12/25 | Loss: 0.00148897
Iteration 13/25 | Loss: 0.00148897
Iteration 14/25 | Loss: 0.00148897
Iteration 15/25 | Loss: 0.00148897
Iteration 16/25 | Loss: 0.00148897
Iteration 17/25 | Loss: 0.00148897
Iteration 18/25 | Loss: 0.00148897
Iteration 19/25 | Loss: 0.00148897
Iteration 20/25 | Loss: 0.00148897
Iteration 21/25 | Loss: 0.00148897
Iteration 22/25 | Loss: 0.00148897
Iteration 23/25 | Loss: 0.00148897
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0014889729209244251, 0.0014889729209244251, 0.0014889729209244251, 0.0014889729209244251, 0.0014889729209244251]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014889729209244251

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00148897
Iteration 2/1000 | Loss: 0.00073602
Iteration 3/1000 | Loss: 0.00038939
Iteration 4/1000 | Loss: 0.00046156
Iteration 5/1000 | Loss: 0.00009376
Iteration 6/1000 | Loss: 0.00038025
Iteration 7/1000 | Loss: 0.00071234
Iteration 8/1000 | Loss: 0.00007170
Iteration 9/1000 | Loss: 0.00030155
Iteration 10/1000 | Loss: 0.00533025
Iteration 11/1000 | Loss: 0.00159566
Iteration 12/1000 | Loss: 0.00438002
Iteration 13/1000 | Loss: 0.00220159
Iteration 14/1000 | Loss: 0.00021849
Iteration 15/1000 | Loss: 0.00036683
Iteration 16/1000 | Loss: 0.00055728
Iteration 17/1000 | Loss: 0.00269286
Iteration 18/1000 | Loss: 0.00006810
Iteration 19/1000 | Loss: 0.00067908
Iteration 20/1000 | Loss: 0.00021317
Iteration 21/1000 | Loss: 0.00005628
Iteration 22/1000 | Loss: 0.00040217
Iteration 23/1000 | Loss: 0.00007800
Iteration 24/1000 | Loss: 0.00049678
Iteration 25/1000 | Loss: 0.00010266
Iteration 26/1000 | Loss: 0.00008000
Iteration 27/1000 | Loss: 0.00028429
Iteration 28/1000 | Loss: 0.00005151
Iteration 29/1000 | Loss: 0.00005070
Iteration 30/1000 | Loss: 0.00004964
Iteration 31/1000 | Loss: 0.00066958
Iteration 32/1000 | Loss: 0.00004866
Iteration 33/1000 | Loss: 0.00004704
Iteration 34/1000 | Loss: 0.00004594
Iteration 35/1000 | Loss: 0.00065413
Iteration 36/1000 | Loss: 0.00084208
Iteration 37/1000 | Loss: 0.00255973
Iteration 38/1000 | Loss: 0.00134328
Iteration 39/1000 | Loss: 0.00154926
Iteration 40/1000 | Loss: 0.00270503
Iteration 41/1000 | Loss: 0.00200909
Iteration 42/1000 | Loss: 0.00065172
Iteration 43/1000 | Loss: 0.00004792
Iteration 44/1000 | Loss: 0.00003820
Iteration 45/1000 | Loss: 0.00042501
Iteration 46/1000 | Loss: 0.00045307
Iteration 47/1000 | Loss: 0.00010854
Iteration 48/1000 | Loss: 0.00015362
Iteration 49/1000 | Loss: 0.00002726
Iteration 50/1000 | Loss: 0.00043499
Iteration 51/1000 | Loss: 0.00002890
Iteration 52/1000 | Loss: 0.00002223
Iteration 53/1000 | Loss: 0.00022854
Iteration 54/1000 | Loss: 0.00023269
Iteration 55/1000 | Loss: 0.00039872
Iteration 56/1000 | Loss: 0.00008741
Iteration 57/1000 | Loss: 0.00017295
Iteration 58/1000 | Loss: 0.00019918
Iteration 59/1000 | Loss: 0.00011742
Iteration 60/1000 | Loss: 0.00001736
Iteration 61/1000 | Loss: 0.00001668
Iteration 62/1000 | Loss: 0.00001619
Iteration 63/1000 | Loss: 0.00015527
Iteration 64/1000 | Loss: 0.00023311
Iteration 65/1000 | Loss: 0.00004664
Iteration 66/1000 | Loss: 0.00015452
Iteration 67/1000 | Loss: 0.00003090
Iteration 68/1000 | Loss: 0.00001594
Iteration 69/1000 | Loss: 0.00005426
Iteration 70/1000 | Loss: 0.00001569
Iteration 71/1000 | Loss: 0.00007037
Iteration 72/1000 | Loss: 0.00003013
Iteration 73/1000 | Loss: 0.00001569
Iteration 74/1000 | Loss: 0.00006777
Iteration 75/1000 | Loss: 0.00001552
Iteration 76/1000 | Loss: 0.00001538
Iteration 77/1000 | Loss: 0.00001527
Iteration 78/1000 | Loss: 0.00001526
Iteration 79/1000 | Loss: 0.00001526
Iteration 80/1000 | Loss: 0.00001525
Iteration 81/1000 | Loss: 0.00001524
Iteration 82/1000 | Loss: 0.00001523
Iteration 83/1000 | Loss: 0.00001523
Iteration 84/1000 | Loss: 0.00001522
Iteration 85/1000 | Loss: 0.00001521
Iteration 86/1000 | Loss: 0.00001521
Iteration 87/1000 | Loss: 0.00001521
Iteration 88/1000 | Loss: 0.00001520
Iteration 89/1000 | Loss: 0.00001520
Iteration 90/1000 | Loss: 0.00001519
Iteration 91/1000 | Loss: 0.00001519
Iteration 92/1000 | Loss: 0.00001518
Iteration 93/1000 | Loss: 0.00001518
Iteration 94/1000 | Loss: 0.00001515
Iteration 95/1000 | Loss: 0.00001515
Iteration 96/1000 | Loss: 0.00001515
Iteration 97/1000 | Loss: 0.00001514
Iteration 98/1000 | Loss: 0.00001513
Iteration 99/1000 | Loss: 0.00001513
Iteration 100/1000 | Loss: 0.00001510
Iteration 101/1000 | Loss: 0.00001506
Iteration 102/1000 | Loss: 0.00001506
Iteration 103/1000 | Loss: 0.00001506
Iteration 104/1000 | Loss: 0.00001505
Iteration 105/1000 | Loss: 0.00001505
Iteration 106/1000 | Loss: 0.00001504
Iteration 107/1000 | Loss: 0.00001501
Iteration 108/1000 | Loss: 0.00001500
Iteration 109/1000 | Loss: 0.00001499
Iteration 110/1000 | Loss: 0.00001499
Iteration 111/1000 | Loss: 0.00001498
Iteration 112/1000 | Loss: 0.00001498
Iteration 113/1000 | Loss: 0.00001498
Iteration 114/1000 | Loss: 0.00001498
Iteration 115/1000 | Loss: 0.00001498
Iteration 116/1000 | Loss: 0.00001498
Iteration 117/1000 | Loss: 0.00001498
Iteration 118/1000 | Loss: 0.00001498
Iteration 119/1000 | Loss: 0.00001498
Iteration 120/1000 | Loss: 0.00001498
Iteration 121/1000 | Loss: 0.00001497
Iteration 122/1000 | Loss: 0.00001497
Iteration 123/1000 | Loss: 0.00001497
Iteration 124/1000 | Loss: 0.00001496
Iteration 125/1000 | Loss: 0.00001496
Iteration 126/1000 | Loss: 0.00001494
Iteration 127/1000 | Loss: 0.00001494
Iteration 128/1000 | Loss: 0.00001494
Iteration 129/1000 | Loss: 0.00001494
Iteration 130/1000 | Loss: 0.00001493
Iteration 131/1000 | Loss: 0.00001493
Iteration 132/1000 | Loss: 0.00001493
Iteration 133/1000 | Loss: 0.00001492
Iteration 134/1000 | Loss: 0.00001492
Iteration 135/1000 | Loss: 0.00001492
Iteration 136/1000 | Loss: 0.00001491
Iteration 137/1000 | Loss: 0.00001491
Iteration 138/1000 | Loss: 0.00001491
Iteration 139/1000 | Loss: 0.00001491
Iteration 140/1000 | Loss: 0.00001491
Iteration 141/1000 | Loss: 0.00001491
Iteration 142/1000 | Loss: 0.00001490
Iteration 143/1000 | Loss: 0.00001490
Iteration 144/1000 | Loss: 0.00001490
Iteration 145/1000 | Loss: 0.00001490
Iteration 146/1000 | Loss: 0.00001490
Iteration 147/1000 | Loss: 0.00001490
Iteration 148/1000 | Loss: 0.00001490
Iteration 149/1000 | Loss: 0.00001490
Iteration 150/1000 | Loss: 0.00001490
Iteration 151/1000 | Loss: 0.00001489
Iteration 152/1000 | Loss: 0.00001489
Iteration 153/1000 | Loss: 0.00001489
Iteration 154/1000 | Loss: 0.00001489
Iteration 155/1000 | Loss: 0.00001489
Iteration 156/1000 | Loss: 0.00001489
Iteration 157/1000 | Loss: 0.00001489
Iteration 158/1000 | Loss: 0.00001489
Iteration 159/1000 | Loss: 0.00001489
Iteration 160/1000 | Loss: 0.00001489
Iteration 161/1000 | Loss: 0.00001489
Iteration 162/1000 | Loss: 0.00001489
Iteration 163/1000 | Loss: 0.00001489
Iteration 164/1000 | Loss: 0.00001488
Iteration 165/1000 | Loss: 0.00001488
Iteration 166/1000 | Loss: 0.00001488
Iteration 167/1000 | Loss: 0.00001488
Iteration 168/1000 | Loss: 0.00001488
Iteration 169/1000 | Loss: 0.00001488
Iteration 170/1000 | Loss: 0.00001488
Iteration 171/1000 | Loss: 0.00001488
Iteration 172/1000 | Loss: 0.00001487
Iteration 173/1000 | Loss: 0.00001487
Iteration 174/1000 | Loss: 0.00001487
Iteration 175/1000 | Loss: 0.00001487
Iteration 176/1000 | Loss: 0.00001487
Iteration 177/1000 | Loss: 0.00001486
Iteration 178/1000 | Loss: 0.00001486
Iteration 179/1000 | Loss: 0.00001486
Iteration 180/1000 | Loss: 0.00001486
Iteration 181/1000 | Loss: 0.00001486
Iteration 182/1000 | Loss: 0.00001486
Iteration 183/1000 | Loss: 0.00001485
Iteration 184/1000 | Loss: 0.00001485
Iteration 185/1000 | Loss: 0.00001485
Iteration 186/1000 | Loss: 0.00001485
Iteration 187/1000 | Loss: 0.00001485
Iteration 188/1000 | Loss: 0.00001485
Iteration 189/1000 | Loss: 0.00001485
Iteration 190/1000 | Loss: 0.00001484
Iteration 191/1000 | Loss: 0.00001484
Iteration 192/1000 | Loss: 0.00001484
Iteration 193/1000 | Loss: 0.00001484
Iteration 194/1000 | Loss: 0.00001484
Iteration 195/1000 | Loss: 0.00001484
Iteration 196/1000 | Loss: 0.00001484
Iteration 197/1000 | Loss: 0.00001484
Iteration 198/1000 | Loss: 0.00001484
Iteration 199/1000 | Loss: 0.00001484
Iteration 200/1000 | Loss: 0.00001484
Iteration 201/1000 | Loss: 0.00001483
Iteration 202/1000 | Loss: 0.00001483
Iteration 203/1000 | Loss: 0.00001483
Iteration 204/1000 | Loss: 0.00001483
Iteration 205/1000 | Loss: 0.00001483
Iteration 206/1000 | Loss: 0.00001483
Iteration 207/1000 | Loss: 0.00001483
Iteration 208/1000 | Loss: 0.00001483
Iteration 209/1000 | Loss: 0.00001483
Iteration 210/1000 | Loss: 0.00001483
Iteration 211/1000 | Loss: 0.00001483
Iteration 212/1000 | Loss: 0.00001483
Iteration 213/1000 | Loss: 0.00001483
Iteration 214/1000 | Loss: 0.00001483
Iteration 215/1000 | Loss: 0.00001483
Iteration 216/1000 | Loss: 0.00001483
Iteration 217/1000 | Loss: 0.00001482
Iteration 218/1000 | Loss: 0.00001482
Iteration 219/1000 | Loss: 0.00001482
Iteration 220/1000 | Loss: 0.00001482
Iteration 221/1000 | Loss: 0.00001482
Iteration 222/1000 | Loss: 0.00001482
Iteration 223/1000 | Loss: 0.00001482
Iteration 224/1000 | Loss: 0.00001482
Iteration 225/1000 | Loss: 0.00001482
Iteration 226/1000 | Loss: 0.00001482
Iteration 227/1000 | Loss: 0.00001482
Iteration 228/1000 | Loss: 0.00001482
Iteration 229/1000 | Loss: 0.00001482
Iteration 230/1000 | Loss: 0.00001482
Iteration 231/1000 | Loss: 0.00001482
Iteration 232/1000 | Loss: 0.00001482
Iteration 233/1000 | Loss: 0.00001482
Iteration 234/1000 | Loss: 0.00001482
Iteration 235/1000 | Loss: 0.00001482
Iteration 236/1000 | Loss: 0.00001482
Iteration 237/1000 | Loss: 0.00001481
Iteration 238/1000 | Loss: 0.00001481
Iteration 239/1000 | Loss: 0.00001481
Iteration 240/1000 | Loss: 0.00001481
Iteration 241/1000 | Loss: 0.00001481
Iteration 242/1000 | Loss: 0.00001481
Iteration 243/1000 | Loss: 0.00001481
Iteration 244/1000 | Loss: 0.00001481
Iteration 245/1000 | Loss: 0.00001481
Iteration 246/1000 | Loss: 0.00001481
Iteration 247/1000 | Loss: 0.00001481
Iteration 248/1000 | Loss: 0.00001481
Iteration 249/1000 | Loss: 0.00001481
Iteration 250/1000 | Loss: 0.00001481
Iteration 251/1000 | Loss: 0.00001481
Iteration 252/1000 | Loss: 0.00001481
Iteration 253/1000 | Loss: 0.00001481
Iteration 254/1000 | Loss: 0.00001481
Iteration 255/1000 | Loss: 0.00001481
Iteration 256/1000 | Loss: 0.00001480
Iteration 257/1000 | Loss: 0.00001480
Iteration 258/1000 | Loss: 0.00001480
Iteration 259/1000 | Loss: 0.00001480
Iteration 260/1000 | Loss: 0.00001480
Iteration 261/1000 | Loss: 0.00001480
Iteration 262/1000 | Loss: 0.00001480
Iteration 263/1000 | Loss: 0.00001480
Iteration 264/1000 | Loss: 0.00001480
Iteration 265/1000 | Loss: 0.00001480
Iteration 266/1000 | Loss: 0.00001480
Iteration 267/1000 | Loss: 0.00001480
Iteration 268/1000 | Loss: 0.00001480
Iteration 269/1000 | Loss: 0.00001480
Iteration 270/1000 | Loss: 0.00001480
Iteration 271/1000 | Loss: 0.00001480
Iteration 272/1000 | Loss: 0.00001480
Iteration 273/1000 | Loss: 0.00001480
Iteration 274/1000 | Loss: 0.00001480
Iteration 275/1000 | Loss: 0.00001480
Iteration 276/1000 | Loss: 0.00001480
Iteration 277/1000 | Loss: 0.00001480
Iteration 278/1000 | Loss: 0.00001480
Iteration 279/1000 | Loss: 0.00001479
Iteration 280/1000 | Loss: 0.00001479
Iteration 281/1000 | Loss: 0.00001479
Iteration 282/1000 | Loss: 0.00001479
Iteration 283/1000 | Loss: 0.00001479
Iteration 284/1000 | Loss: 0.00001479
Iteration 285/1000 | Loss: 0.00001479
Iteration 286/1000 | Loss: 0.00001479
Iteration 287/1000 | Loss: 0.00001479
Iteration 288/1000 | Loss: 0.00001479
Iteration 289/1000 | Loss: 0.00001479
Iteration 290/1000 | Loss: 0.00001479
Iteration 291/1000 | Loss: 0.00001479
Iteration 292/1000 | Loss: 0.00001479
Iteration 293/1000 | Loss: 0.00001479
Iteration 294/1000 | Loss: 0.00001479
Iteration 295/1000 | Loss: 0.00001479
Iteration 296/1000 | Loss: 0.00001479
Iteration 297/1000 | Loss: 0.00001479
Iteration 298/1000 | Loss: 0.00001478
Iteration 299/1000 | Loss: 0.00001478
Iteration 300/1000 | Loss: 0.00001478
Iteration 301/1000 | Loss: 0.00001478
Iteration 302/1000 | Loss: 0.00001478
Iteration 303/1000 | Loss: 0.00001478
Iteration 304/1000 | Loss: 0.00001478
Iteration 305/1000 | Loss: 0.00001478
Iteration 306/1000 | Loss: 0.00001478
Iteration 307/1000 | Loss: 0.00001478
Iteration 308/1000 | Loss: 0.00001478
Iteration 309/1000 | Loss: 0.00001478
Iteration 310/1000 | Loss: 0.00001478
Iteration 311/1000 | Loss: 0.00001478
Iteration 312/1000 | Loss: 0.00001478
Iteration 313/1000 | Loss: 0.00001477
Iteration 314/1000 | Loss: 0.00001477
Iteration 315/1000 | Loss: 0.00001477
Iteration 316/1000 | Loss: 0.00001477
Iteration 317/1000 | Loss: 0.00001477
Iteration 318/1000 | Loss: 0.00001477
Iteration 319/1000 | Loss: 0.00001477
Iteration 320/1000 | Loss: 0.00001477
Iteration 321/1000 | Loss: 0.00001477
Iteration 322/1000 | Loss: 0.00001477
Iteration 323/1000 | Loss: 0.00001477
Iteration 324/1000 | Loss: 0.00001477
Iteration 325/1000 | Loss: 0.00001477
Iteration 326/1000 | Loss: 0.00001477
Iteration 327/1000 | Loss: 0.00001477
Iteration 328/1000 | Loss: 0.00001477
Iteration 329/1000 | Loss: 0.00001477
Iteration 330/1000 | Loss: 0.00001477
Iteration 331/1000 | Loss: 0.00001477
Iteration 332/1000 | Loss: 0.00001477
Iteration 333/1000 | Loss: 0.00001477
Iteration 334/1000 | Loss: 0.00001477
Iteration 335/1000 | Loss: 0.00001477
Iteration 336/1000 | Loss: 0.00001477
Iteration 337/1000 | Loss: 0.00001477
Iteration 338/1000 | Loss: 0.00001477
Iteration 339/1000 | Loss: 0.00001477
Iteration 340/1000 | Loss: 0.00001477
Iteration 341/1000 | Loss: 0.00001477
Iteration 342/1000 | Loss: 0.00001477
Iteration 343/1000 | Loss: 0.00001477
Iteration 344/1000 | Loss: 0.00001477
Iteration 345/1000 | Loss: 0.00001477
Iteration 346/1000 | Loss: 0.00001477
Iteration 347/1000 | Loss: 0.00001477
Iteration 348/1000 | Loss: 0.00001477
Iteration 349/1000 | Loss: 0.00001477
Iteration 350/1000 | Loss: 0.00001477
Iteration 351/1000 | Loss: 0.00001477
Iteration 352/1000 | Loss: 0.00001477
Iteration 353/1000 | Loss: 0.00001477
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 353. Stopping optimization.
Last 5 losses: [1.4769275367143564e-05, 1.4769275367143564e-05, 1.4769275367143564e-05, 1.4769275367143564e-05, 1.4769275367143564e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4769275367143564e-05

Optimization complete. Final v2v error: 3.2385172843933105 mm

Highest mean error: 4.194300174713135 mm for frame 66

Lowest mean error: 2.8784823417663574 mm for frame 109

Saving results

Total time: 169.82063388824463
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_018/1051/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_018/1051.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_018/1051
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00817666
Iteration 2/25 | Loss: 0.00143899
Iteration 3/25 | Loss: 0.00123566
Iteration 4/25 | Loss: 0.00121721
Iteration 5/25 | Loss: 0.00121578
Iteration 6/25 | Loss: 0.00121578
Iteration 7/25 | Loss: 0.00121578
Iteration 8/25 | Loss: 0.00121578
Iteration 9/25 | Loss: 0.00121578
Iteration 10/25 | Loss: 0.00121578
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0012157800374552608, 0.0012157800374552608, 0.0012157800374552608, 0.0012157800374552608, 0.0012157800374552608]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012157800374552608

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.97792304
Iteration 2/25 | Loss: 0.00047514
Iteration 3/25 | Loss: 0.00047513
Iteration 4/25 | Loss: 0.00047513
Iteration 5/25 | Loss: 0.00047513
Iteration 6/25 | Loss: 0.00047513
Iteration 7/25 | Loss: 0.00047513
Iteration 8/25 | Loss: 0.00047513
Iteration 9/25 | Loss: 0.00047513
Iteration 10/25 | Loss: 0.00047513
Iteration 11/25 | Loss: 0.00047513
Iteration 12/25 | Loss: 0.00047513
Iteration 13/25 | Loss: 0.00047513
Iteration 14/25 | Loss: 0.00047513
Iteration 15/25 | Loss: 0.00047513
Iteration 16/25 | Loss: 0.00047513
Iteration 17/25 | Loss: 0.00047513
Iteration 18/25 | Loss: 0.00047513
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.00047512692981399596, 0.00047512692981399596, 0.00047512692981399596, 0.00047512692981399596, 0.00047512692981399596]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00047512692981399596

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00047513
Iteration 2/1000 | Loss: 0.00003609
Iteration 3/1000 | Loss: 0.00002572
Iteration 4/1000 | Loss: 0.00002327
Iteration 5/1000 | Loss: 0.00002249
Iteration 6/1000 | Loss: 0.00002197
Iteration 7/1000 | Loss: 0.00002134
Iteration 8/1000 | Loss: 0.00002095
Iteration 9/1000 | Loss: 0.00002058
Iteration 10/1000 | Loss: 0.00002038
Iteration 11/1000 | Loss: 0.00002017
Iteration 12/1000 | Loss: 0.00002005
Iteration 13/1000 | Loss: 0.00002004
Iteration 14/1000 | Loss: 0.00002004
Iteration 15/1000 | Loss: 0.00002003
Iteration 16/1000 | Loss: 0.00002003
Iteration 17/1000 | Loss: 0.00002003
Iteration 18/1000 | Loss: 0.00002001
Iteration 19/1000 | Loss: 0.00001989
Iteration 20/1000 | Loss: 0.00001989
Iteration 21/1000 | Loss: 0.00001988
Iteration 22/1000 | Loss: 0.00001987
Iteration 23/1000 | Loss: 0.00001986
Iteration 24/1000 | Loss: 0.00001983
Iteration 25/1000 | Loss: 0.00001983
Iteration 26/1000 | Loss: 0.00001982
Iteration 27/1000 | Loss: 0.00001981
Iteration 28/1000 | Loss: 0.00001979
Iteration 29/1000 | Loss: 0.00001979
Iteration 30/1000 | Loss: 0.00001979
Iteration 31/1000 | Loss: 0.00001978
Iteration 32/1000 | Loss: 0.00001978
Iteration 33/1000 | Loss: 0.00001978
Iteration 34/1000 | Loss: 0.00001978
Iteration 35/1000 | Loss: 0.00001978
Iteration 36/1000 | Loss: 0.00001978
Iteration 37/1000 | Loss: 0.00001978
Iteration 38/1000 | Loss: 0.00001978
Iteration 39/1000 | Loss: 0.00001977
Iteration 40/1000 | Loss: 0.00001977
Iteration 41/1000 | Loss: 0.00001977
Iteration 42/1000 | Loss: 0.00001976
Iteration 43/1000 | Loss: 0.00001976
Iteration 44/1000 | Loss: 0.00001973
Iteration 45/1000 | Loss: 0.00001972
Iteration 46/1000 | Loss: 0.00001972
Iteration 47/1000 | Loss: 0.00001972
Iteration 48/1000 | Loss: 0.00001971
Iteration 49/1000 | Loss: 0.00001971
Iteration 50/1000 | Loss: 0.00001970
Iteration 51/1000 | Loss: 0.00001969
Iteration 52/1000 | Loss: 0.00001969
Iteration 53/1000 | Loss: 0.00001969
Iteration 54/1000 | Loss: 0.00001968
Iteration 55/1000 | Loss: 0.00001968
Iteration 56/1000 | Loss: 0.00001968
Iteration 57/1000 | Loss: 0.00001968
Iteration 58/1000 | Loss: 0.00001968
Iteration 59/1000 | Loss: 0.00001968
Iteration 60/1000 | Loss: 0.00001967
Iteration 61/1000 | Loss: 0.00001967
Iteration 62/1000 | Loss: 0.00001967
Iteration 63/1000 | Loss: 0.00001967
Iteration 64/1000 | Loss: 0.00001966
Iteration 65/1000 | Loss: 0.00001966
Iteration 66/1000 | Loss: 0.00001966
Iteration 67/1000 | Loss: 0.00001966
Iteration 68/1000 | Loss: 0.00001966
Iteration 69/1000 | Loss: 0.00001965
Iteration 70/1000 | Loss: 0.00001965
Iteration 71/1000 | Loss: 0.00001965
Iteration 72/1000 | Loss: 0.00001965
Iteration 73/1000 | Loss: 0.00001963
Iteration 74/1000 | Loss: 0.00001963
Iteration 75/1000 | Loss: 0.00001960
Iteration 76/1000 | Loss: 0.00001960
Iteration 77/1000 | Loss: 0.00001959
Iteration 78/1000 | Loss: 0.00001959
Iteration 79/1000 | Loss: 0.00001958
Iteration 80/1000 | Loss: 0.00001958
Iteration 81/1000 | Loss: 0.00001958
Iteration 82/1000 | Loss: 0.00001958
Iteration 83/1000 | Loss: 0.00001958
Iteration 84/1000 | Loss: 0.00001958
Iteration 85/1000 | Loss: 0.00001958
Iteration 86/1000 | Loss: 0.00001958
Iteration 87/1000 | Loss: 0.00001957
Iteration 88/1000 | Loss: 0.00001957
Iteration 89/1000 | Loss: 0.00001957
Iteration 90/1000 | Loss: 0.00001957
Iteration 91/1000 | Loss: 0.00001956
Iteration 92/1000 | Loss: 0.00001956
Iteration 93/1000 | Loss: 0.00001956
Iteration 94/1000 | Loss: 0.00001956
Iteration 95/1000 | Loss: 0.00001956
Iteration 96/1000 | Loss: 0.00001952
Iteration 97/1000 | Loss: 0.00001952
Iteration 98/1000 | Loss: 0.00001951
Iteration 99/1000 | Loss: 0.00001951
Iteration 100/1000 | Loss: 0.00001951
Iteration 101/1000 | Loss: 0.00001950
Iteration 102/1000 | Loss: 0.00001950
Iteration 103/1000 | Loss: 0.00001950
Iteration 104/1000 | Loss: 0.00001950
Iteration 105/1000 | Loss: 0.00001950
Iteration 106/1000 | Loss: 0.00001950
Iteration 107/1000 | Loss: 0.00001949
Iteration 108/1000 | Loss: 0.00001949
Iteration 109/1000 | Loss: 0.00001949
Iteration 110/1000 | Loss: 0.00001949
Iteration 111/1000 | Loss: 0.00001949
Iteration 112/1000 | Loss: 0.00001949
Iteration 113/1000 | Loss: 0.00001949
Iteration 114/1000 | Loss: 0.00001949
Iteration 115/1000 | Loss: 0.00001948
Iteration 116/1000 | Loss: 0.00001948
Iteration 117/1000 | Loss: 0.00001948
Iteration 118/1000 | Loss: 0.00001947
Iteration 119/1000 | Loss: 0.00001947
Iteration 120/1000 | Loss: 0.00001947
Iteration 121/1000 | Loss: 0.00001947
Iteration 122/1000 | Loss: 0.00001947
Iteration 123/1000 | Loss: 0.00001947
Iteration 124/1000 | Loss: 0.00001947
Iteration 125/1000 | Loss: 0.00001947
Iteration 126/1000 | Loss: 0.00001947
Iteration 127/1000 | Loss: 0.00001947
Iteration 128/1000 | Loss: 0.00001947
Iteration 129/1000 | Loss: 0.00001947
Iteration 130/1000 | Loss: 0.00001947
Iteration 131/1000 | Loss: 0.00001947
Iteration 132/1000 | Loss: 0.00001947
Iteration 133/1000 | Loss: 0.00001947
Iteration 134/1000 | Loss: 0.00001947
Iteration 135/1000 | Loss: 0.00001946
Iteration 136/1000 | Loss: 0.00001946
Iteration 137/1000 | Loss: 0.00001946
Iteration 138/1000 | Loss: 0.00001946
Iteration 139/1000 | Loss: 0.00001946
Iteration 140/1000 | Loss: 0.00001946
Iteration 141/1000 | Loss: 0.00001946
Iteration 142/1000 | Loss: 0.00001946
Iteration 143/1000 | Loss: 0.00001946
Iteration 144/1000 | Loss: 0.00001946
Iteration 145/1000 | Loss: 0.00001946
Iteration 146/1000 | Loss: 0.00001946
Iteration 147/1000 | Loss: 0.00001945
Iteration 148/1000 | Loss: 0.00001945
Iteration 149/1000 | Loss: 0.00001945
Iteration 150/1000 | Loss: 0.00001945
Iteration 151/1000 | Loss: 0.00001945
Iteration 152/1000 | Loss: 0.00001945
Iteration 153/1000 | Loss: 0.00001945
Iteration 154/1000 | Loss: 0.00001945
Iteration 155/1000 | Loss: 0.00001945
Iteration 156/1000 | Loss: 0.00001945
Iteration 157/1000 | Loss: 0.00001945
Iteration 158/1000 | Loss: 0.00001945
Iteration 159/1000 | Loss: 0.00001945
Iteration 160/1000 | Loss: 0.00001945
Iteration 161/1000 | Loss: 0.00001945
Iteration 162/1000 | Loss: 0.00001945
Iteration 163/1000 | Loss: 0.00001945
Iteration 164/1000 | Loss: 0.00001945
Iteration 165/1000 | Loss: 0.00001944
Iteration 166/1000 | Loss: 0.00001944
Iteration 167/1000 | Loss: 0.00001944
Iteration 168/1000 | Loss: 0.00001944
Iteration 169/1000 | Loss: 0.00001944
Iteration 170/1000 | Loss: 0.00001944
Iteration 171/1000 | Loss: 0.00001944
Iteration 172/1000 | Loss: 0.00001944
Iteration 173/1000 | Loss: 0.00001944
Iteration 174/1000 | Loss: 0.00001944
Iteration 175/1000 | Loss: 0.00001944
Iteration 176/1000 | Loss: 0.00001944
Iteration 177/1000 | Loss: 0.00001944
Iteration 178/1000 | Loss: 0.00001944
Iteration 179/1000 | Loss: 0.00001944
Iteration 180/1000 | Loss: 0.00001944
Iteration 181/1000 | Loss: 0.00001944
Iteration 182/1000 | Loss: 0.00001944
Iteration 183/1000 | Loss: 0.00001944
Iteration 184/1000 | Loss: 0.00001944
Iteration 185/1000 | Loss: 0.00001944
Iteration 186/1000 | Loss: 0.00001944
Iteration 187/1000 | Loss: 0.00001944
Iteration 188/1000 | Loss: 0.00001944
Iteration 189/1000 | Loss: 0.00001944
Iteration 190/1000 | Loss: 0.00001944
Iteration 191/1000 | Loss: 0.00001944
Iteration 192/1000 | Loss: 0.00001944
Iteration 193/1000 | Loss: 0.00001944
Iteration 194/1000 | Loss: 0.00001944
Iteration 195/1000 | Loss: 0.00001944
Iteration 196/1000 | Loss: 0.00001944
Iteration 197/1000 | Loss: 0.00001944
Iteration 198/1000 | Loss: 0.00001944
Iteration 199/1000 | Loss: 0.00001944
Iteration 200/1000 | Loss: 0.00001944
Iteration 201/1000 | Loss: 0.00001944
Iteration 202/1000 | Loss: 0.00001944
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 202. Stopping optimization.
Last 5 losses: [1.9440447431406938e-05, 1.9440447431406938e-05, 1.9440447431406938e-05, 1.9440447431406938e-05, 1.9440447431406938e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9440447431406938e-05

Optimization complete. Final v2v error: 3.6723861694335938 mm

Highest mean error: 3.7595667839050293 mm for frame 3

Lowest mean error: 3.547118902206421 mm for frame 96

Saving results

Total time: 38.41559839248657
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_022/1042/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_022/1042.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_022/1042
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00951357
Iteration 2/25 | Loss: 0.00152837
Iteration 3/25 | Loss: 0.00112678
Iteration 4/25 | Loss: 0.00096812
Iteration 5/25 | Loss: 0.00093868
Iteration 6/25 | Loss: 0.00092946
Iteration 7/25 | Loss: 0.00092649
Iteration 8/25 | Loss: 0.00092211
Iteration 9/25 | Loss: 0.00092099
Iteration 10/25 | Loss: 0.00092003
Iteration 11/25 | Loss: 0.00091949
Iteration 12/25 | Loss: 0.00092679
Iteration 13/25 | Loss: 0.00092864
Iteration 14/25 | Loss: 0.00091031
Iteration 15/25 | Loss: 0.00090786
Iteration 16/25 | Loss: 0.00090754
Iteration 17/25 | Loss: 0.00090743
Iteration 18/25 | Loss: 0.00090737
Iteration 19/25 | Loss: 0.00090737
Iteration 20/25 | Loss: 0.00090737
Iteration 21/25 | Loss: 0.00090737
Iteration 22/25 | Loss: 0.00090737
Iteration 23/25 | Loss: 0.00090737
Iteration 24/25 | Loss: 0.00090737
Iteration 25/25 | Loss: 0.00090737

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.55950928
Iteration 2/25 | Loss: 0.00146508
Iteration 3/25 | Loss: 0.00146508
Iteration 4/25 | Loss: 0.00146508
Iteration 5/25 | Loss: 0.00146508
Iteration 6/25 | Loss: 0.00146508
Iteration 7/25 | Loss: 0.00146508
Iteration 8/25 | Loss: 0.00146508
Iteration 9/25 | Loss: 0.00146508
Iteration 10/25 | Loss: 0.00146508
Iteration 11/25 | Loss: 0.00146508
Iteration 12/25 | Loss: 0.00146508
Iteration 13/25 | Loss: 0.00146508
Iteration 14/25 | Loss: 0.00146508
Iteration 15/25 | Loss: 0.00146508
Iteration 16/25 | Loss: 0.00146508
Iteration 17/25 | Loss: 0.00146508
Iteration 18/25 | Loss: 0.00146508
Iteration 19/25 | Loss: 0.00146508
Iteration 20/25 | Loss: 0.00146508
Iteration 21/25 | Loss: 0.00146508
Iteration 22/25 | Loss: 0.00146508
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0014650793746113777, 0.0014650793746113777, 0.0014650793746113777, 0.0014650793746113777, 0.0014650793746113777]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014650793746113777

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00146508
Iteration 2/1000 | Loss: 0.00003444
Iteration 3/1000 | Loss: 0.00002659
Iteration 4/1000 | Loss: 0.00002495
Iteration 5/1000 | Loss: 0.00002388
Iteration 6/1000 | Loss: 0.00002323
Iteration 7/1000 | Loss: 0.00002269
Iteration 8/1000 | Loss: 0.00002233
Iteration 9/1000 | Loss: 0.00002212
Iteration 10/1000 | Loss: 0.00002189
Iteration 11/1000 | Loss: 0.00002174
Iteration 12/1000 | Loss: 0.00002168
Iteration 13/1000 | Loss: 0.00002167
Iteration 14/1000 | Loss: 0.00002167
Iteration 15/1000 | Loss: 0.00002164
Iteration 16/1000 | Loss: 0.00002148
Iteration 17/1000 | Loss: 0.00002140
Iteration 18/1000 | Loss: 0.00002140
Iteration 19/1000 | Loss: 0.00002139
Iteration 20/1000 | Loss: 0.00002138
Iteration 21/1000 | Loss: 0.00002137
Iteration 22/1000 | Loss: 0.00002135
Iteration 23/1000 | Loss: 0.00002135
Iteration 24/1000 | Loss: 0.00002134
Iteration 25/1000 | Loss: 0.00002134
Iteration 26/1000 | Loss: 0.00002133
Iteration 27/1000 | Loss: 0.00002133
Iteration 28/1000 | Loss: 0.00002128
Iteration 29/1000 | Loss: 0.00002125
Iteration 30/1000 | Loss: 0.00002125
Iteration 31/1000 | Loss: 0.00002125
Iteration 32/1000 | Loss: 0.00002125
Iteration 33/1000 | Loss: 0.00002124
Iteration 34/1000 | Loss: 0.00002124
Iteration 35/1000 | Loss: 0.00002124
Iteration 36/1000 | Loss: 0.00002124
Iteration 37/1000 | Loss: 0.00002124
Iteration 38/1000 | Loss: 0.00002123
Iteration 39/1000 | Loss: 0.00002123
Iteration 40/1000 | Loss: 0.00002123
Iteration 41/1000 | Loss: 0.00002123
Iteration 42/1000 | Loss: 0.00002122
Iteration 43/1000 | Loss: 0.00002122
Iteration 44/1000 | Loss: 0.00002122
Iteration 45/1000 | Loss: 0.00002121
Iteration 46/1000 | Loss: 0.00002121
Iteration 47/1000 | Loss: 0.00002121
Iteration 48/1000 | Loss: 0.00002121
Iteration 49/1000 | Loss: 0.00002121
Iteration 50/1000 | Loss: 0.00002121
Iteration 51/1000 | Loss: 0.00002121
Iteration 52/1000 | Loss: 0.00002121
Iteration 53/1000 | Loss: 0.00002121
Iteration 54/1000 | Loss: 0.00002120
Iteration 55/1000 | Loss: 0.00002120
Iteration 56/1000 | Loss: 0.00002120
Iteration 57/1000 | Loss: 0.00002120
Iteration 58/1000 | Loss: 0.00002120
Iteration 59/1000 | Loss: 0.00002120
Iteration 60/1000 | Loss: 0.00002119
Iteration 61/1000 | Loss: 0.00002119
Iteration 62/1000 | Loss: 0.00002119
Iteration 63/1000 | Loss: 0.00002119
Iteration 64/1000 | Loss: 0.00002119
Iteration 65/1000 | Loss: 0.00002119
Iteration 66/1000 | Loss: 0.00002118
Iteration 67/1000 | Loss: 0.00002118
Iteration 68/1000 | Loss: 0.00002118
Iteration 69/1000 | Loss: 0.00002118
Iteration 70/1000 | Loss: 0.00002117
Iteration 71/1000 | Loss: 0.00002117
Iteration 72/1000 | Loss: 0.00002117
Iteration 73/1000 | Loss: 0.00002116
Iteration 74/1000 | Loss: 0.00002116
Iteration 75/1000 | Loss: 0.00002116
Iteration 76/1000 | Loss: 0.00002116
Iteration 77/1000 | Loss: 0.00002116
Iteration 78/1000 | Loss: 0.00004692
Iteration 79/1000 | Loss: 0.00002698
Iteration 80/1000 | Loss: 0.00002118
Iteration 81/1000 | Loss: 0.00002116
Iteration 82/1000 | Loss: 0.00002116
Iteration 83/1000 | Loss: 0.00002116
Iteration 84/1000 | Loss: 0.00002115
Iteration 85/1000 | Loss: 0.00002115
Iteration 86/1000 | Loss: 0.00002115
Iteration 87/1000 | Loss: 0.00002115
Iteration 88/1000 | Loss: 0.00002115
Iteration 89/1000 | Loss: 0.00002115
Iteration 90/1000 | Loss: 0.00002115
Iteration 91/1000 | Loss: 0.00002115
Iteration 92/1000 | Loss: 0.00002115
Iteration 93/1000 | Loss: 0.00002115
Iteration 94/1000 | Loss: 0.00002115
Iteration 95/1000 | Loss: 0.00002115
Iteration 96/1000 | Loss: 0.00002114
Iteration 97/1000 | Loss: 0.00002114
Iteration 98/1000 | Loss: 0.00002114
Iteration 99/1000 | Loss: 0.00002114
Iteration 100/1000 | Loss: 0.00002114
Iteration 101/1000 | Loss: 0.00002114
Iteration 102/1000 | Loss: 0.00002114
Iteration 103/1000 | Loss: 0.00002114
Iteration 104/1000 | Loss: 0.00002114
Iteration 105/1000 | Loss: 0.00002114
Iteration 106/1000 | Loss: 0.00002114
Iteration 107/1000 | Loss: 0.00002114
Iteration 108/1000 | Loss: 0.00002114
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 108. Stopping optimization.
Last 5 losses: [2.1136756913620047e-05, 2.1136756913620047e-05, 2.1136756913620047e-05, 2.1136756913620047e-05, 2.1136756913620047e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1136756913620047e-05

Optimization complete. Final v2v error: 3.774665355682373 mm

Highest mean error: 4.133440971374512 mm for frame 138

Lowest mean error: 3.4488089084625244 mm for frame 188

Saving results

Total time: 68.74413347244263
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_022/1022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_022/1022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_022/1022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00825587
Iteration 2/25 | Loss: 0.00106048
Iteration 3/25 | Loss: 0.00089048
Iteration 4/25 | Loss: 0.00084840
Iteration 5/25 | Loss: 0.00083287
Iteration 6/25 | Loss: 0.00082951
Iteration 7/25 | Loss: 0.00082808
Iteration 8/25 | Loss: 0.00082777
Iteration 9/25 | Loss: 0.00082777
Iteration 10/25 | Loss: 0.00082777
Iteration 11/25 | Loss: 0.00082777
Iteration 12/25 | Loss: 0.00082777
Iteration 13/25 | Loss: 0.00082777
Iteration 14/25 | Loss: 0.00082777
Iteration 15/25 | Loss: 0.00082777
Iteration 16/25 | Loss: 0.00082777
Iteration 17/25 | Loss: 0.00082777
Iteration 18/25 | Loss: 0.00082777
Iteration 19/25 | Loss: 0.00082777
Iteration 20/25 | Loss: 0.00082777
Iteration 21/25 | Loss: 0.00082777
Iteration 22/25 | Loss: 0.00082777
Iteration 23/25 | Loss: 0.00082777
Iteration 24/25 | Loss: 0.00082777
Iteration 25/25 | Loss: 0.00082777

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 8.43524075
Iteration 2/25 | Loss: 0.00156973
Iteration 3/25 | Loss: 0.00156945
Iteration 4/25 | Loss: 0.00156945
Iteration 5/25 | Loss: 0.00156945
Iteration 6/25 | Loss: 0.00156945
Iteration 7/25 | Loss: 0.00156945
Iteration 8/25 | Loss: 0.00156945
Iteration 9/25 | Loss: 0.00156945
Iteration 10/25 | Loss: 0.00156945
Iteration 11/25 | Loss: 0.00156945
Iteration 12/25 | Loss: 0.00156945
Iteration 13/25 | Loss: 0.00156945
Iteration 14/25 | Loss: 0.00156945
Iteration 15/25 | Loss: 0.00156945
Iteration 16/25 | Loss: 0.00156945
Iteration 17/25 | Loss: 0.00156945
Iteration 18/25 | Loss: 0.00156945
Iteration 19/25 | Loss: 0.00156945
Iteration 20/25 | Loss: 0.00156945
Iteration 21/25 | Loss: 0.00156945
Iteration 22/25 | Loss: 0.00156945
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0015694499015808105, 0.0015694499015808105, 0.0015694499015808105, 0.0015694499015808105, 0.0015694499015808105]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015694499015808105

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00156945
Iteration 2/1000 | Loss: 0.00005260
Iteration 3/1000 | Loss: 0.00003570
Iteration 4/1000 | Loss: 0.00002919
Iteration 5/1000 | Loss: 0.00002615
Iteration 6/1000 | Loss: 0.00002464
Iteration 7/1000 | Loss: 0.00002326
Iteration 8/1000 | Loss: 0.00002247
Iteration 9/1000 | Loss: 0.00002164
Iteration 10/1000 | Loss: 0.00002117
Iteration 11/1000 | Loss: 0.00002089
Iteration 12/1000 | Loss: 0.00002071
Iteration 13/1000 | Loss: 0.00002049
Iteration 14/1000 | Loss: 0.00002027
Iteration 15/1000 | Loss: 0.00002023
Iteration 16/1000 | Loss: 0.00002013
Iteration 17/1000 | Loss: 0.00002011
Iteration 18/1000 | Loss: 0.00002005
Iteration 19/1000 | Loss: 0.00001998
Iteration 20/1000 | Loss: 0.00001996
Iteration 21/1000 | Loss: 0.00001996
Iteration 22/1000 | Loss: 0.00001993
Iteration 23/1000 | Loss: 0.00001992
Iteration 24/1000 | Loss: 0.00001992
Iteration 25/1000 | Loss: 0.00001991
Iteration 26/1000 | Loss: 0.00001991
Iteration 27/1000 | Loss: 0.00001990
Iteration 28/1000 | Loss: 0.00001990
Iteration 29/1000 | Loss: 0.00001989
Iteration 30/1000 | Loss: 0.00001987
Iteration 31/1000 | Loss: 0.00001987
Iteration 32/1000 | Loss: 0.00001986
Iteration 33/1000 | Loss: 0.00001986
Iteration 34/1000 | Loss: 0.00001986
Iteration 35/1000 | Loss: 0.00001985
Iteration 36/1000 | Loss: 0.00001985
Iteration 37/1000 | Loss: 0.00001984
Iteration 38/1000 | Loss: 0.00001984
Iteration 39/1000 | Loss: 0.00001984
Iteration 40/1000 | Loss: 0.00001983
Iteration 41/1000 | Loss: 0.00001983
Iteration 42/1000 | Loss: 0.00001983
Iteration 43/1000 | Loss: 0.00001982
Iteration 44/1000 | Loss: 0.00001982
Iteration 45/1000 | Loss: 0.00001982
Iteration 46/1000 | Loss: 0.00001981
Iteration 47/1000 | Loss: 0.00001981
Iteration 48/1000 | Loss: 0.00001980
Iteration 49/1000 | Loss: 0.00001980
Iteration 50/1000 | Loss: 0.00001980
Iteration 51/1000 | Loss: 0.00001979
Iteration 52/1000 | Loss: 0.00001979
Iteration 53/1000 | Loss: 0.00001979
Iteration 54/1000 | Loss: 0.00001978
Iteration 55/1000 | Loss: 0.00001978
Iteration 56/1000 | Loss: 0.00001978
Iteration 57/1000 | Loss: 0.00001978
Iteration 58/1000 | Loss: 0.00001978
Iteration 59/1000 | Loss: 0.00001977
Iteration 60/1000 | Loss: 0.00001977
Iteration 61/1000 | Loss: 0.00001977
Iteration 62/1000 | Loss: 0.00001977
Iteration 63/1000 | Loss: 0.00001976
Iteration 64/1000 | Loss: 0.00001976
Iteration 65/1000 | Loss: 0.00001976
Iteration 66/1000 | Loss: 0.00001976
Iteration 67/1000 | Loss: 0.00001975
Iteration 68/1000 | Loss: 0.00001975
Iteration 69/1000 | Loss: 0.00001975
Iteration 70/1000 | Loss: 0.00001975
Iteration 71/1000 | Loss: 0.00001974
Iteration 72/1000 | Loss: 0.00001974
Iteration 73/1000 | Loss: 0.00001974
Iteration 74/1000 | Loss: 0.00001974
Iteration 75/1000 | Loss: 0.00001974
Iteration 76/1000 | Loss: 0.00001973
Iteration 77/1000 | Loss: 0.00001973
Iteration 78/1000 | Loss: 0.00001973
Iteration 79/1000 | Loss: 0.00001973
Iteration 80/1000 | Loss: 0.00001973
Iteration 81/1000 | Loss: 0.00001973
Iteration 82/1000 | Loss: 0.00001973
Iteration 83/1000 | Loss: 0.00001973
Iteration 84/1000 | Loss: 0.00001973
Iteration 85/1000 | Loss: 0.00001972
Iteration 86/1000 | Loss: 0.00001972
Iteration 87/1000 | Loss: 0.00001972
Iteration 88/1000 | Loss: 0.00001972
Iteration 89/1000 | Loss: 0.00001972
Iteration 90/1000 | Loss: 0.00001972
Iteration 91/1000 | Loss: 0.00001972
Iteration 92/1000 | Loss: 0.00001972
Iteration 93/1000 | Loss: 0.00001971
Iteration 94/1000 | Loss: 0.00001971
Iteration 95/1000 | Loss: 0.00001971
Iteration 96/1000 | Loss: 0.00001971
Iteration 97/1000 | Loss: 0.00001971
Iteration 98/1000 | Loss: 0.00001971
Iteration 99/1000 | Loss: 0.00001971
Iteration 100/1000 | Loss: 0.00001971
Iteration 101/1000 | Loss: 0.00001971
Iteration 102/1000 | Loss: 0.00001970
Iteration 103/1000 | Loss: 0.00001970
Iteration 104/1000 | Loss: 0.00001970
Iteration 105/1000 | Loss: 0.00001970
Iteration 106/1000 | Loss: 0.00001970
Iteration 107/1000 | Loss: 0.00001970
Iteration 108/1000 | Loss: 0.00001970
Iteration 109/1000 | Loss: 0.00001970
Iteration 110/1000 | Loss: 0.00001970
Iteration 111/1000 | Loss: 0.00001970
Iteration 112/1000 | Loss: 0.00001969
Iteration 113/1000 | Loss: 0.00001969
Iteration 114/1000 | Loss: 0.00001969
Iteration 115/1000 | Loss: 0.00001969
Iteration 116/1000 | Loss: 0.00001969
Iteration 117/1000 | Loss: 0.00001969
Iteration 118/1000 | Loss: 0.00001969
Iteration 119/1000 | Loss: 0.00001969
Iteration 120/1000 | Loss: 0.00001969
Iteration 121/1000 | Loss: 0.00001969
Iteration 122/1000 | Loss: 0.00001969
Iteration 123/1000 | Loss: 0.00001968
Iteration 124/1000 | Loss: 0.00001968
Iteration 125/1000 | Loss: 0.00001968
Iteration 126/1000 | Loss: 0.00001968
Iteration 127/1000 | Loss: 0.00001968
Iteration 128/1000 | Loss: 0.00001968
Iteration 129/1000 | Loss: 0.00001968
Iteration 130/1000 | Loss: 0.00001968
Iteration 131/1000 | Loss: 0.00001968
Iteration 132/1000 | Loss: 0.00001968
Iteration 133/1000 | Loss: 0.00001968
Iteration 134/1000 | Loss: 0.00001968
Iteration 135/1000 | Loss: 0.00001968
Iteration 136/1000 | Loss: 0.00001968
Iteration 137/1000 | Loss: 0.00001968
Iteration 138/1000 | Loss: 0.00001968
Iteration 139/1000 | Loss: 0.00001967
Iteration 140/1000 | Loss: 0.00001967
Iteration 141/1000 | Loss: 0.00001967
Iteration 142/1000 | Loss: 0.00001967
Iteration 143/1000 | Loss: 0.00001967
Iteration 144/1000 | Loss: 0.00001967
Iteration 145/1000 | Loss: 0.00001967
Iteration 146/1000 | Loss: 0.00001966
Iteration 147/1000 | Loss: 0.00001966
Iteration 148/1000 | Loss: 0.00001966
Iteration 149/1000 | Loss: 0.00001966
Iteration 150/1000 | Loss: 0.00001966
Iteration 151/1000 | Loss: 0.00001966
Iteration 152/1000 | Loss: 0.00001965
Iteration 153/1000 | Loss: 0.00001965
Iteration 154/1000 | Loss: 0.00001965
Iteration 155/1000 | Loss: 0.00001965
Iteration 156/1000 | Loss: 0.00001965
Iteration 157/1000 | Loss: 0.00001965
Iteration 158/1000 | Loss: 0.00001965
Iteration 159/1000 | Loss: 0.00001965
Iteration 160/1000 | Loss: 0.00001964
Iteration 161/1000 | Loss: 0.00001964
Iteration 162/1000 | Loss: 0.00001964
Iteration 163/1000 | Loss: 0.00001964
Iteration 164/1000 | Loss: 0.00001964
Iteration 165/1000 | Loss: 0.00001964
Iteration 166/1000 | Loss: 0.00001964
Iteration 167/1000 | Loss: 0.00001964
Iteration 168/1000 | Loss: 0.00001964
Iteration 169/1000 | Loss: 0.00001963
Iteration 170/1000 | Loss: 0.00001963
Iteration 171/1000 | Loss: 0.00001963
Iteration 172/1000 | Loss: 0.00001963
Iteration 173/1000 | Loss: 0.00001963
Iteration 174/1000 | Loss: 0.00001963
Iteration 175/1000 | Loss: 0.00001963
Iteration 176/1000 | Loss: 0.00001963
Iteration 177/1000 | Loss: 0.00001963
Iteration 178/1000 | Loss: 0.00001963
Iteration 179/1000 | Loss: 0.00001963
Iteration 180/1000 | Loss: 0.00001963
Iteration 181/1000 | Loss: 0.00001963
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 181. Stopping optimization.
Last 5 losses: [1.963316390174441e-05, 1.963316390174441e-05, 1.963316390174441e-05, 1.963316390174441e-05, 1.963316390174441e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.963316390174441e-05

Optimization complete. Final v2v error: 3.660858631134033 mm

Highest mean error: 5.65944766998291 mm for frame 55

Lowest mean error: 2.964524507522583 mm for frame 73

Saving results

Total time: 46.443852186203
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_022/1000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_022/1000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_022/1000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00501920
Iteration 2/25 | Loss: 0.00100909
Iteration 3/25 | Loss: 0.00087113
Iteration 4/25 | Loss: 0.00083313
Iteration 5/25 | Loss: 0.00082382
Iteration 6/25 | Loss: 0.00082258
Iteration 7/25 | Loss: 0.00082233
Iteration 8/25 | Loss: 0.00082233
Iteration 9/25 | Loss: 0.00082233
Iteration 10/25 | Loss: 0.00082233
Iteration 11/25 | Loss: 0.00082233
Iteration 12/25 | Loss: 0.00082233
Iteration 13/25 | Loss: 0.00082233
Iteration 14/25 | Loss: 0.00082233
Iteration 15/25 | Loss: 0.00082233
Iteration 16/25 | Loss: 0.00082233
Iteration 17/25 | Loss: 0.00082233
Iteration 18/25 | Loss: 0.00082233
Iteration 19/25 | Loss: 0.00082233
Iteration 20/25 | Loss: 0.00082231
Iteration 21/25 | Loss: 0.00082231
Iteration 22/25 | Loss: 0.00082231
Iteration 23/25 | Loss: 0.00082231
Iteration 24/25 | Loss: 0.00082231
Iteration 25/25 | Loss: 0.00082231

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.54479051
Iteration 2/25 | Loss: 0.00111776
Iteration 3/25 | Loss: 0.00111773
Iteration 4/25 | Loss: 0.00111773
Iteration 5/25 | Loss: 0.00111773
Iteration 6/25 | Loss: 0.00111773
Iteration 7/25 | Loss: 0.00111773
Iteration 8/25 | Loss: 0.00111773
Iteration 9/25 | Loss: 0.00111773
Iteration 10/25 | Loss: 0.00111773
Iteration 11/25 | Loss: 0.00111773
Iteration 12/25 | Loss: 0.00111773
Iteration 13/25 | Loss: 0.00111773
Iteration 14/25 | Loss: 0.00111773
Iteration 15/25 | Loss: 0.00111773
Iteration 16/25 | Loss: 0.00111773
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.001117729232646525, 0.001117729232646525, 0.001117729232646525, 0.001117729232646525, 0.001117729232646525]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001117729232646525

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00111773
Iteration 2/1000 | Loss: 0.00005310
Iteration 3/1000 | Loss: 0.00003373
Iteration 4/1000 | Loss: 0.00002817
Iteration 5/1000 | Loss: 0.00002657
Iteration 6/1000 | Loss: 0.00002510
Iteration 7/1000 | Loss: 0.00002434
Iteration 8/1000 | Loss: 0.00002373
Iteration 9/1000 | Loss: 0.00002335
Iteration 10/1000 | Loss: 0.00002310
Iteration 11/1000 | Loss: 0.00002290
Iteration 12/1000 | Loss: 0.00002286
Iteration 13/1000 | Loss: 0.00002265
Iteration 14/1000 | Loss: 0.00002260
Iteration 15/1000 | Loss: 0.00002252
Iteration 16/1000 | Loss: 0.00002250
Iteration 17/1000 | Loss: 0.00002249
Iteration 18/1000 | Loss: 0.00002248
Iteration 19/1000 | Loss: 0.00002248
Iteration 20/1000 | Loss: 0.00002248
Iteration 21/1000 | Loss: 0.00002248
Iteration 22/1000 | Loss: 0.00002247
Iteration 23/1000 | Loss: 0.00002247
Iteration 24/1000 | Loss: 0.00002246
Iteration 25/1000 | Loss: 0.00002246
Iteration 26/1000 | Loss: 0.00002245
Iteration 27/1000 | Loss: 0.00002244
Iteration 28/1000 | Loss: 0.00002243
Iteration 29/1000 | Loss: 0.00002243
Iteration 30/1000 | Loss: 0.00002242
Iteration 31/1000 | Loss: 0.00002241
Iteration 32/1000 | Loss: 0.00002241
Iteration 33/1000 | Loss: 0.00002241
Iteration 34/1000 | Loss: 0.00002241
Iteration 35/1000 | Loss: 0.00002240
Iteration 36/1000 | Loss: 0.00002240
Iteration 37/1000 | Loss: 0.00002239
Iteration 38/1000 | Loss: 0.00002239
Iteration 39/1000 | Loss: 0.00002239
Iteration 40/1000 | Loss: 0.00002238
Iteration 41/1000 | Loss: 0.00002238
Iteration 42/1000 | Loss: 0.00002237
Iteration 43/1000 | Loss: 0.00002237
Iteration 44/1000 | Loss: 0.00002237
Iteration 45/1000 | Loss: 0.00002236
Iteration 46/1000 | Loss: 0.00002236
Iteration 47/1000 | Loss: 0.00002235
Iteration 48/1000 | Loss: 0.00002235
Iteration 49/1000 | Loss: 0.00002234
Iteration 50/1000 | Loss: 0.00002234
Iteration 51/1000 | Loss: 0.00002234
Iteration 52/1000 | Loss: 0.00002233
Iteration 53/1000 | Loss: 0.00002233
Iteration 54/1000 | Loss: 0.00002233
Iteration 55/1000 | Loss: 0.00002232
Iteration 56/1000 | Loss: 0.00002232
Iteration 57/1000 | Loss: 0.00002232
Iteration 58/1000 | Loss: 0.00002231
Iteration 59/1000 | Loss: 0.00002231
Iteration 60/1000 | Loss: 0.00002231
Iteration 61/1000 | Loss: 0.00002231
Iteration 62/1000 | Loss: 0.00002230
Iteration 63/1000 | Loss: 0.00002230
Iteration 64/1000 | Loss: 0.00002230
Iteration 65/1000 | Loss: 0.00002229
Iteration 66/1000 | Loss: 0.00002229
Iteration 67/1000 | Loss: 0.00002229
Iteration 68/1000 | Loss: 0.00002228
Iteration 69/1000 | Loss: 0.00002228
Iteration 70/1000 | Loss: 0.00002228
Iteration 71/1000 | Loss: 0.00002228
Iteration 72/1000 | Loss: 0.00002228
Iteration 73/1000 | Loss: 0.00002228
Iteration 74/1000 | Loss: 0.00002227
Iteration 75/1000 | Loss: 0.00002227
Iteration 76/1000 | Loss: 0.00002227
Iteration 77/1000 | Loss: 0.00002226
Iteration 78/1000 | Loss: 0.00002226
Iteration 79/1000 | Loss: 0.00002226
Iteration 80/1000 | Loss: 0.00002225
Iteration 81/1000 | Loss: 0.00002225
Iteration 82/1000 | Loss: 0.00002225
Iteration 83/1000 | Loss: 0.00002225
Iteration 84/1000 | Loss: 0.00002225
Iteration 85/1000 | Loss: 0.00002225
Iteration 86/1000 | Loss: 0.00002225
Iteration 87/1000 | Loss: 0.00002225
Iteration 88/1000 | Loss: 0.00002225
Iteration 89/1000 | Loss: 0.00002225
Iteration 90/1000 | Loss: 0.00002225
Iteration 91/1000 | Loss: 0.00002224
Iteration 92/1000 | Loss: 0.00002224
Iteration 93/1000 | Loss: 0.00002224
Iteration 94/1000 | Loss: 0.00002224
Iteration 95/1000 | Loss: 0.00002223
Iteration 96/1000 | Loss: 0.00002223
Iteration 97/1000 | Loss: 0.00002223
Iteration 98/1000 | Loss: 0.00002222
Iteration 99/1000 | Loss: 0.00002222
Iteration 100/1000 | Loss: 0.00002222
Iteration 101/1000 | Loss: 0.00002222
Iteration 102/1000 | Loss: 0.00002222
Iteration 103/1000 | Loss: 0.00002221
Iteration 104/1000 | Loss: 0.00002221
Iteration 105/1000 | Loss: 0.00002221
Iteration 106/1000 | Loss: 0.00002221
Iteration 107/1000 | Loss: 0.00002221
Iteration 108/1000 | Loss: 0.00002221
Iteration 109/1000 | Loss: 0.00002221
Iteration 110/1000 | Loss: 0.00002221
Iteration 111/1000 | Loss: 0.00002220
Iteration 112/1000 | Loss: 0.00002220
Iteration 113/1000 | Loss: 0.00002220
Iteration 114/1000 | Loss: 0.00002220
Iteration 115/1000 | Loss: 0.00002220
Iteration 116/1000 | Loss: 0.00002220
Iteration 117/1000 | Loss: 0.00002220
Iteration 118/1000 | Loss: 0.00002220
Iteration 119/1000 | Loss: 0.00002220
Iteration 120/1000 | Loss: 0.00002220
Iteration 121/1000 | Loss: 0.00002220
Iteration 122/1000 | Loss: 0.00002220
Iteration 123/1000 | Loss: 0.00002220
Iteration 124/1000 | Loss: 0.00002220
Iteration 125/1000 | Loss: 0.00002219
Iteration 126/1000 | Loss: 0.00002219
Iteration 127/1000 | Loss: 0.00002219
Iteration 128/1000 | Loss: 0.00002218
Iteration 129/1000 | Loss: 0.00002218
Iteration 130/1000 | Loss: 0.00002218
Iteration 131/1000 | Loss: 0.00002218
Iteration 132/1000 | Loss: 0.00002218
Iteration 133/1000 | Loss: 0.00002218
Iteration 134/1000 | Loss: 0.00002218
Iteration 135/1000 | Loss: 0.00002218
Iteration 136/1000 | Loss: 0.00002218
Iteration 137/1000 | Loss: 0.00002218
Iteration 138/1000 | Loss: 0.00002218
Iteration 139/1000 | Loss: 0.00002218
Iteration 140/1000 | Loss: 0.00002218
Iteration 141/1000 | Loss: 0.00002218
Iteration 142/1000 | Loss: 0.00002218
Iteration 143/1000 | Loss: 0.00002218
Iteration 144/1000 | Loss: 0.00002218
Iteration 145/1000 | Loss: 0.00002218
Iteration 146/1000 | Loss: 0.00002218
Iteration 147/1000 | Loss: 0.00002218
Iteration 148/1000 | Loss: 0.00002218
Iteration 149/1000 | Loss: 0.00002218
Iteration 150/1000 | Loss: 0.00002218
Iteration 151/1000 | Loss: 0.00002218
Iteration 152/1000 | Loss: 0.00002218
Iteration 153/1000 | Loss: 0.00002218
Iteration 154/1000 | Loss: 0.00002218
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 154. Stopping optimization.
Last 5 losses: [2.2175167032401077e-05, 2.2175167032401077e-05, 2.2175167032401077e-05, 2.2175167032401077e-05, 2.2175167032401077e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2175167032401077e-05

Optimization complete. Final v2v error: 3.8872997760772705 mm

Highest mean error: 4.321932792663574 mm for frame 102

Lowest mean error: 3.286526679992676 mm for frame 21

Saving results

Total time: 39.76191473007202
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_022/1014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_022/1014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_022/1014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00952518
Iteration 2/25 | Loss: 0.00192144
Iteration 3/25 | Loss: 0.00283136
Iteration 4/25 | Loss: 0.00111435
Iteration 5/25 | Loss: 0.00095503
Iteration 6/25 | Loss: 0.00093652
Iteration 7/25 | Loss: 0.00093911
Iteration 8/25 | Loss: 0.00094365
Iteration 9/25 | Loss: 0.00092932
Iteration 10/25 | Loss: 0.00093226
Iteration 11/25 | Loss: 0.00092724
Iteration 12/25 | Loss: 0.00093235
Iteration 13/25 | Loss: 0.00091873
Iteration 14/25 | Loss: 0.00091190
Iteration 15/25 | Loss: 0.00090717
Iteration 16/25 | Loss: 0.00090265
Iteration 17/25 | Loss: 0.00090113
Iteration 18/25 | Loss: 0.00089980
Iteration 19/25 | Loss: 0.00089819
Iteration 20/25 | Loss: 0.00089755
Iteration 21/25 | Loss: 0.00089707
Iteration 22/25 | Loss: 0.00089688
Iteration 23/25 | Loss: 0.00089686
Iteration 24/25 | Loss: 0.00089686
Iteration 25/25 | Loss: 0.00089686

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.59140193
Iteration 2/25 | Loss: 0.00131043
Iteration 3/25 | Loss: 0.00131041
Iteration 4/25 | Loss: 0.00131041
Iteration 5/25 | Loss: 0.00131041
Iteration 6/25 | Loss: 0.00131041
Iteration 7/25 | Loss: 0.00131041
Iteration 8/25 | Loss: 0.00131040
Iteration 9/25 | Loss: 0.00131040
Iteration 10/25 | Loss: 0.00131040
Iteration 11/25 | Loss: 0.00131041
Iteration 12/25 | Loss: 0.00131040
Iteration 13/25 | Loss: 0.00131040
Iteration 14/25 | Loss: 0.00131040
Iteration 15/25 | Loss: 0.00131040
Iteration 16/25 | Loss: 0.00131040
Iteration 17/25 | Loss: 0.00131040
Iteration 18/25 | Loss: 0.00131040
Iteration 19/25 | Loss: 0.00131040
Iteration 20/25 | Loss: 0.00131040
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0013104043900966644, 0.0013104043900966644, 0.0013104043900966644, 0.0013104043900966644, 0.0013104043900966644]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013104043900966644

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00131040
Iteration 2/1000 | Loss: 0.00007542
Iteration 3/1000 | Loss: 0.00004957
Iteration 4/1000 | Loss: 0.00004177
Iteration 5/1000 | Loss: 0.00003944
Iteration 6/1000 | Loss: 0.00003765
Iteration 7/1000 | Loss: 0.00003648
Iteration 8/1000 | Loss: 0.00025916
Iteration 9/1000 | Loss: 0.00003663
Iteration 10/1000 | Loss: 0.00003407
Iteration 11/1000 | Loss: 0.00003254
Iteration 12/1000 | Loss: 0.00003142
Iteration 13/1000 | Loss: 0.00003083
Iteration 14/1000 | Loss: 0.00003050
Iteration 15/1000 | Loss: 0.00003017
Iteration 16/1000 | Loss: 0.00003001
Iteration 17/1000 | Loss: 0.00002982
Iteration 18/1000 | Loss: 0.00002977
Iteration 19/1000 | Loss: 0.00002974
Iteration 20/1000 | Loss: 0.00002973
Iteration 21/1000 | Loss: 0.00002973
Iteration 22/1000 | Loss: 0.00002972
Iteration 23/1000 | Loss: 0.00002960
Iteration 24/1000 | Loss: 0.00002959
Iteration 25/1000 | Loss: 0.00002959
Iteration 26/1000 | Loss: 0.00002959
Iteration 27/1000 | Loss: 0.00002954
Iteration 28/1000 | Loss: 0.00002953
Iteration 29/1000 | Loss: 0.00002953
Iteration 30/1000 | Loss: 0.00002953
Iteration 31/1000 | Loss: 0.00002950
Iteration 32/1000 | Loss: 0.00002950
Iteration 33/1000 | Loss: 0.00002949
Iteration 34/1000 | Loss: 0.00002949
Iteration 35/1000 | Loss: 0.00002949
Iteration 36/1000 | Loss: 0.00002948
Iteration 37/1000 | Loss: 0.00002948
Iteration 38/1000 | Loss: 0.00002947
Iteration 39/1000 | Loss: 0.00002946
Iteration 40/1000 | Loss: 0.00002944
Iteration 41/1000 | Loss: 0.00002944
Iteration 42/1000 | Loss: 0.00002944
Iteration 43/1000 | Loss: 0.00002943
Iteration 44/1000 | Loss: 0.00002943
Iteration 45/1000 | Loss: 0.00002943
Iteration 46/1000 | Loss: 0.00002942
Iteration 47/1000 | Loss: 0.00002942
Iteration 48/1000 | Loss: 0.00002941
Iteration 49/1000 | Loss: 0.00002941
Iteration 50/1000 | Loss: 0.00002940
Iteration 51/1000 | Loss: 0.00002940
Iteration 52/1000 | Loss: 0.00002940
Iteration 53/1000 | Loss: 0.00002940
Iteration 54/1000 | Loss: 0.00002940
Iteration 55/1000 | Loss: 0.00002940
Iteration 56/1000 | Loss: 0.00002940
Iteration 57/1000 | Loss: 0.00002939
Iteration 58/1000 | Loss: 0.00002939
Iteration 59/1000 | Loss: 0.00002939
Iteration 60/1000 | Loss: 0.00002939
Iteration 61/1000 | Loss: 0.00002939
Iteration 62/1000 | Loss: 0.00002938
Iteration 63/1000 | Loss: 0.00002938
Iteration 64/1000 | Loss: 0.00002938
Iteration 65/1000 | Loss: 0.00002937
Iteration 66/1000 | Loss: 0.00002937
Iteration 67/1000 | Loss: 0.00002936
Iteration 68/1000 | Loss: 0.00002936
Iteration 69/1000 | Loss: 0.00002936
Iteration 70/1000 | Loss: 0.00002934
Iteration 71/1000 | Loss: 0.00002934
Iteration 72/1000 | Loss: 0.00002933
Iteration 73/1000 | Loss: 0.00002933
Iteration 74/1000 | Loss: 0.00002932
Iteration 75/1000 | Loss: 0.00002931
Iteration 76/1000 | Loss: 0.00002931
Iteration 77/1000 | Loss: 0.00002931
Iteration 78/1000 | Loss: 0.00002931
Iteration 79/1000 | Loss: 0.00002931
Iteration 80/1000 | Loss: 0.00002931
Iteration 81/1000 | Loss: 0.00002930
Iteration 82/1000 | Loss: 0.00002930
Iteration 83/1000 | Loss: 0.00002930
Iteration 84/1000 | Loss: 0.00002930
Iteration 85/1000 | Loss: 0.00002930
Iteration 86/1000 | Loss: 0.00002930
Iteration 87/1000 | Loss: 0.00002930
Iteration 88/1000 | Loss: 0.00002930
Iteration 89/1000 | Loss: 0.00002930
Iteration 90/1000 | Loss: 0.00002930
Iteration 91/1000 | Loss: 0.00002930
Iteration 92/1000 | Loss: 0.00002929
Iteration 93/1000 | Loss: 0.00002929
Iteration 94/1000 | Loss: 0.00002929
Iteration 95/1000 | Loss: 0.00002929
Iteration 96/1000 | Loss: 0.00002929
Iteration 97/1000 | Loss: 0.00002929
Iteration 98/1000 | Loss: 0.00002929
Iteration 99/1000 | Loss: 0.00002929
Iteration 100/1000 | Loss: 0.00002928
Iteration 101/1000 | Loss: 0.00002928
Iteration 102/1000 | Loss: 0.00002928
Iteration 103/1000 | Loss: 0.00002928
Iteration 104/1000 | Loss: 0.00002928
Iteration 105/1000 | Loss: 0.00002928
Iteration 106/1000 | Loss: 0.00002928
Iteration 107/1000 | Loss: 0.00002927
Iteration 108/1000 | Loss: 0.00002927
Iteration 109/1000 | Loss: 0.00002927
Iteration 110/1000 | Loss: 0.00002927
Iteration 111/1000 | Loss: 0.00002927
Iteration 112/1000 | Loss: 0.00002927
Iteration 113/1000 | Loss: 0.00002927
Iteration 114/1000 | Loss: 0.00002927
Iteration 115/1000 | Loss: 0.00002927
Iteration 116/1000 | Loss: 0.00002927
Iteration 117/1000 | Loss: 0.00002927
Iteration 118/1000 | Loss: 0.00002926
Iteration 119/1000 | Loss: 0.00002926
Iteration 120/1000 | Loss: 0.00002926
Iteration 121/1000 | Loss: 0.00002926
Iteration 122/1000 | Loss: 0.00002926
Iteration 123/1000 | Loss: 0.00002926
Iteration 124/1000 | Loss: 0.00002926
Iteration 125/1000 | Loss: 0.00002926
Iteration 126/1000 | Loss: 0.00002926
Iteration 127/1000 | Loss: 0.00002926
Iteration 128/1000 | Loss: 0.00002926
Iteration 129/1000 | Loss: 0.00002926
Iteration 130/1000 | Loss: 0.00002926
Iteration 131/1000 | Loss: 0.00002926
Iteration 132/1000 | Loss: 0.00002926
Iteration 133/1000 | Loss: 0.00002926
Iteration 134/1000 | Loss: 0.00002926
Iteration 135/1000 | Loss: 0.00002926
Iteration 136/1000 | Loss: 0.00002926
Iteration 137/1000 | Loss: 0.00002926
Iteration 138/1000 | Loss: 0.00002926
Iteration 139/1000 | Loss: 0.00002926
Iteration 140/1000 | Loss: 0.00002926
Iteration 141/1000 | Loss: 0.00002926
Iteration 142/1000 | Loss: 0.00002926
Iteration 143/1000 | Loss: 0.00002926
Iteration 144/1000 | Loss: 0.00002926
Iteration 145/1000 | Loss: 0.00002926
Iteration 146/1000 | Loss: 0.00002926
Iteration 147/1000 | Loss: 0.00002926
Iteration 148/1000 | Loss: 0.00002926
Iteration 149/1000 | Loss: 0.00002926
Iteration 150/1000 | Loss: 0.00002926
Iteration 151/1000 | Loss: 0.00002926
Iteration 152/1000 | Loss: 0.00002926
Iteration 153/1000 | Loss: 0.00002926
Iteration 154/1000 | Loss: 0.00002926
Iteration 155/1000 | Loss: 0.00002926
Iteration 156/1000 | Loss: 0.00002926
Iteration 157/1000 | Loss: 0.00002926
Iteration 158/1000 | Loss: 0.00002926
Iteration 159/1000 | Loss: 0.00002926
Iteration 160/1000 | Loss: 0.00002926
Iteration 161/1000 | Loss: 0.00002926
Iteration 162/1000 | Loss: 0.00002926
Iteration 163/1000 | Loss: 0.00002926
Iteration 164/1000 | Loss: 0.00002926
Iteration 165/1000 | Loss: 0.00002926
Iteration 166/1000 | Loss: 0.00002926
Iteration 167/1000 | Loss: 0.00002926
Iteration 168/1000 | Loss: 0.00002926
Iteration 169/1000 | Loss: 0.00002926
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 169. Stopping optimization.
Last 5 losses: [2.925532498920802e-05, 2.925532498920802e-05, 2.925532498920802e-05, 2.925532498920802e-05, 2.925532498920802e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.925532498920802e-05

Optimization complete. Final v2v error: 4.509101390838623 mm

Highest mean error: 6.002330780029297 mm for frame 67

Lowest mean error: 3.874061346054077 mm for frame 44

Saving results

Total time: 74.80696940422058
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_022/1084/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_022/1084.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_022/1084
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00710188
Iteration 2/25 | Loss: 0.00146565
Iteration 3/25 | Loss: 0.00100897
Iteration 4/25 | Loss: 0.00091691
Iteration 5/25 | Loss: 0.00089930
Iteration 6/25 | Loss: 0.00088741
Iteration 7/25 | Loss: 0.00087285
Iteration 8/25 | Loss: 0.00087250
Iteration 9/25 | Loss: 0.00085983
Iteration 10/25 | Loss: 0.00085586
Iteration 11/25 | Loss: 0.00085574
Iteration 12/25 | Loss: 0.00085552
Iteration 13/25 | Loss: 0.00085557
Iteration 14/25 | Loss: 0.00085286
Iteration 15/25 | Loss: 0.00085068
Iteration 16/25 | Loss: 0.00085088
Iteration 17/25 | Loss: 0.00085165
Iteration 18/25 | Loss: 0.00085070
Iteration 19/25 | Loss: 0.00085073
Iteration 20/25 | Loss: 0.00085070
Iteration 21/25 | Loss: 0.00085070
Iteration 22/25 | Loss: 0.00085048
Iteration 23/25 | Loss: 0.00085057
Iteration 24/25 | Loss: 0.00085048
Iteration 25/25 | Loss: 0.00085054

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.28212833
Iteration 2/25 | Loss: 0.00155305
Iteration 3/25 | Loss: 0.00155277
Iteration 4/25 | Loss: 0.00155277
Iteration 5/25 | Loss: 0.00155277
Iteration 6/25 | Loss: 0.00155277
Iteration 7/25 | Loss: 0.00155277
Iteration 8/25 | Loss: 0.00155277
Iteration 9/25 | Loss: 0.00155277
Iteration 10/25 | Loss: 0.00155277
Iteration 11/25 | Loss: 0.00155277
Iteration 12/25 | Loss: 0.00155277
Iteration 13/25 | Loss: 0.00155277
Iteration 14/25 | Loss: 0.00155277
Iteration 15/25 | Loss: 0.00155277
Iteration 16/25 | Loss: 0.00155277
Iteration 17/25 | Loss: 0.00155277
Iteration 18/25 | Loss: 0.00155277
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.00155276816803962, 0.00155276816803962, 0.00155276816803962, 0.00155276816803962, 0.00155276816803962]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00155276816803962

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00155277
Iteration 2/1000 | Loss: 0.00006408
Iteration 3/1000 | Loss: 0.00004488
Iteration 4/1000 | Loss: 0.00004194
Iteration 5/1000 | Loss: 0.00003648
Iteration 6/1000 | Loss: 0.00003415
Iteration 7/1000 | Loss: 0.00003337
Iteration 8/1000 | Loss: 0.00003559
Iteration 9/1000 | Loss: 0.00003470
Iteration 10/1000 | Loss: 0.00003505
Iteration 11/1000 | Loss: 0.00003459
Iteration 12/1000 | Loss: 0.00003515
Iteration 13/1000 | Loss: 0.00003298
Iteration 14/1000 | Loss: 0.00003397
Iteration 15/1000 | Loss: 0.00003288
Iteration 16/1000 | Loss: 0.00003467
Iteration 17/1000 | Loss: 0.00003278
Iteration 18/1000 | Loss: 0.00003417
Iteration 19/1000 | Loss: 0.00003163
Iteration 20/1000 | Loss: 0.00003380
Iteration 21/1000 | Loss: 0.00003197
Iteration 22/1000 | Loss: 0.00003316
Iteration 23/1000 | Loss: 0.00002817
Iteration 24/1000 | Loss: 0.00002994
Iteration 25/1000 | Loss: 0.00003017
Iteration 26/1000 | Loss: 0.00003091
Iteration 27/1000 | Loss: 0.00003167
Iteration 28/1000 | Loss: 0.00003148
Iteration 29/1000 | Loss: 0.00003041
Iteration 30/1000 | Loss: 0.00003072
Iteration 31/1000 | Loss: 0.00003093
Iteration 32/1000 | Loss: 0.00002873
Iteration 33/1000 | Loss: 0.00003644
Iteration 34/1000 | Loss: 0.00003137
Iteration 35/1000 | Loss: 0.00003023
Iteration 36/1000 | Loss: 0.00002788
Iteration 37/1000 | Loss: 0.00003379
Iteration 38/1000 | Loss: 0.00002994
Iteration 39/1000 | Loss: 0.00003211
Iteration 40/1000 | Loss: 0.00003134
Iteration 41/1000 | Loss: 0.00002801
Iteration 42/1000 | Loss: 0.00002673
Iteration 43/1000 | Loss: 0.00002601
Iteration 44/1000 | Loss: 0.00002564
Iteration 45/1000 | Loss: 0.00002549
Iteration 46/1000 | Loss: 0.00002548
Iteration 47/1000 | Loss: 0.00002548
Iteration 48/1000 | Loss: 0.00002543
Iteration 49/1000 | Loss: 0.00002541
Iteration 50/1000 | Loss: 0.00002540
Iteration 51/1000 | Loss: 0.00002537
Iteration 52/1000 | Loss: 0.00002535
Iteration 53/1000 | Loss: 0.00002535
Iteration 54/1000 | Loss: 0.00002534
Iteration 55/1000 | Loss: 0.00002534
Iteration 56/1000 | Loss: 0.00002534
Iteration 57/1000 | Loss: 0.00002533
Iteration 58/1000 | Loss: 0.00002533
Iteration 59/1000 | Loss: 0.00002533
Iteration 60/1000 | Loss: 0.00002532
Iteration 61/1000 | Loss: 0.00002532
Iteration 62/1000 | Loss: 0.00002532
Iteration 63/1000 | Loss: 0.00002532
Iteration 64/1000 | Loss: 0.00002531
Iteration 65/1000 | Loss: 0.00002531
Iteration 66/1000 | Loss: 0.00002531
Iteration 67/1000 | Loss: 0.00002530
Iteration 68/1000 | Loss: 0.00002530
Iteration 69/1000 | Loss: 0.00002530
Iteration 70/1000 | Loss: 0.00002530
Iteration 71/1000 | Loss: 0.00002530
Iteration 72/1000 | Loss: 0.00002530
Iteration 73/1000 | Loss: 0.00002530
Iteration 74/1000 | Loss: 0.00002530
Iteration 75/1000 | Loss: 0.00002529
Iteration 76/1000 | Loss: 0.00002529
Iteration 77/1000 | Loss: 0.00002528
Iteration 78/1000 | Loss: 0.00002526
Iteration 79/1000 | Loss: 0.00002526
Iteration 80/1000 | Loss: 0.00002525
Iteration 81/1000 | Loss: 0.00002525
Iteration 82/1000 | Loss: 0.00002525
Iteration 83/1000 | Loss: 0.00002523
Iteration 84/1000 | Loss: 0.00002520
Iteration 85/1000 | Loss: 0.00002520
Iteration 86/1000 | Loss: 0.00002520
Iteration 87/1000 | Loss: 0.00002519
Iteration 88/1000 | Loss: 0.00002518
Iteration 89/1000 | Loss: 0.00002518
Iteration 90/1000 | Loss: 0.00002518
Iteration 91/1000 | Loss: 0.00002518
Iteration 92/1000 | Loss: 0.00002518
Iteration 93/1000 | Loss: 0.00002518
Iteration 94/1000 | Loss: 0.00002518
Iteration 95/1000 | Loss: 0.00002518
Iteration 96/1000 | Loss: 0.00002518
Iteration 97/1000 | Loss: 0.00002518
Iteration 98/1000 | Loss: 0.00002518
Iteration 99/1000 | Loss: 0.00002518
Iteration 100/1000 | Loss: 0.00002517
Iteration 101/1000 | Loss: 0.00002517
Iteration 102/1000 | Loss: 0.00002517
Iteration 103/1000 | Loss: 0.00002517
Iteration 104/1000 | Loss: 0.00002517
Iteration 105/1000 | Loss: 0.00002517
Iteration 106/1000 | Loss: 0.00002517
Iteration 107/1000 | Loss: 0.00002517
Iteration 108/1000 | Loss: 0.00002517
Iteration 109/1000 | Loss: 0.00002517
Iteration 110/1000 | Loss: 0.00002517
Iteration 111/1000 | Loss: 0.00002516
Iteration 112/1000 | Loss: 0.00002516
Iteration 113/1000 | Loss: 0.00002516
Iteration 114/1000 | Loss: 0.00002514
Iteration 115/1000 | Loss: 0.00002514
Iteration 116/1000 | Loss: 0.00002513
Iteration 117/1000 | Loss: 0.00002513
Iteration 118/1000 | Loss: 0.00002512
Iteration 119/1000 | Loss: 0.00002512
Iteration 120/1000 | Loss: 0.00002511
Iteration 121/1000 | Loss: 0.00002511
Iteration 122/1000 | Loss: 0.00002511
Iteration 123/1000 | Loss: 0.00002510
Iteration 124/1000 | Loss: 0.00002509
Iteration 125/1000 | Loss: 0.00002509
Iteration 126/1000 | Loss: 0.00002508
Iteration 127/1000 | Loss: 0.00002500
Iteration 128/1000 | Loss: 0.00002496
Iteration 129/1000 | Loss: 0.00002496
Iteration 130/1000 | Loss: 0.00002495
Iteration 131/1000 | Loss: 0.00002488
Iteration 132/1000 | Loss: 0.00002488
Iteration 133/1000 | Loss: 0.00002487
Iteration 134/1000 | Loss: 0.00002486
Iteration 135/1000 | Loss: 0.00002486
Iteration 136/1000 | Loss: 0.00002486
Iteration 137/1000 | Loss: 0.00002485
Iteration 138/1000 | Loss: 0.00002485
Iteration 139/1000 | Loss: 0.00002482
Iteration 140/1000 | Loss: 0.00002482
Iteration 141/1000 | Loss: 0.00002481
Iteration 142/1000 | Loss: 0.00002480
Iteration 143/1000 | Loss: 0.00002480
Iteration 144/1000 | Loss: 0.00002479
Iteration 145/1000 | Loss: 0.00002478
Iteration 146/1000 | Loss: 0.00002478
Iteration 147/1000 | Loss: 0.00002478
Iteration 148/1000 | Loss: 0.00002477
Iteration 149/1000 | Loss: 0.00002477
Iteration 150/1000 | Loss: 0.00002476
Iteration 151/1000 | Loss: 0.00002475
Iteration 152/1000 | Loss: 0.00002475
Iteration 153/1000 | Loss: 0.00002474
Iteration 154/1000 | Loss: 0.00002474
Iteration 155/1000 | Loss: 0.00002474
Iteration 156/1000 | Loss: 0.00002474
Iteration 157/1000 | Loss: 0.00002474
Iteration 158/1000 | Loss: 0.00002474
Iteration 159/1000 | Loss: 0.00002474
Iteration 160/1000 | Loss: 0.00002474
Iteration 161/1000 | Loss: 0.00002473
Iteration 162/1000 | Loss: 0.00002473
Iteration 163/1000 | Loss: 0.00002473
Iteration 164/1000 | Loss: 0.00002473
Iteration 165/1000 | Loss: 0.00002473
Iteration 166/1000 | Loss: 0.00002473
Iteration 167/1000 | Loss: 0.00002473
Iteration 168/1000 | Loss: 0.00002472
Iteration 169/1000 | Loss: 0.00002472
Iteration 170/1000 | Loss: 0.00002472
Iteration 171/1000 | Loss: 0.00002472
Iteration 172/1000 | Loss: 0.00002472
Iteration 173/1000 | Loss: 0.00002472
Iteration 174/1000 | Loss: 0.00002472
Iteration 175/1000 | Loss: 0.00002471
Iteration 176/1000 | Loss: 0.00002471
Iteration 177/1000 | Loss: 0.00002471
Iteration 178/1000 | Loss: 0.00002471
Iteration 179/1000 | Loss: 0.00002471
Iteration 180/1000 | Loss: 0.00002471
Iteration 181/1000 | Loss: 0.00002471
Iteration 182/1000 | Loss: 0.00002471
Iteration 183/1000 | Loss: 0.00002471
Iteration 184/1000 | Loss: 0.00002470
Iteration 185/1000 | Loss: 0.00002470
Iteration 186/1000 | Loss: 0.00002470
Iteration 187/1000 | Loss: 0.00002470
Iteration 188/1000 | Loss: 0.00002470
Iteration 189/1000 | Loss: 0.00002470
Iteration 190/1000 | Loss: 0.00002469
Iteration 191/1000 | Loss: 0.00002469
Iteration 192/1000 | Loss: 0.00002469
Iteration 193/1000 | Loss: 0.00002469
Iteration 194/1000 | Loss: 0.00002468
Iteration 195/1000 | Loss: 0.00002468
Iteration 196/1000 | Loss: 0.00002468
Iteration 197/1000 | Loss: 0.00002468
Iteration 198/1000 | Loss: 0.00002468
Iteration 199/1000 | Loss: 0.00002468
Iteration 200/1000 | Loss: 0.00002468
Iteration 201/1000 | Loss: 0.00002468
Iteration 202/1000 | Loss: 0.00002468
Iteration 203/1000 | Loss: 0.00002467
Iteration 204/1000 | Loss: 0.00002467
Iteration 205/1000 | Loss: 0.00002467
Iteration 206/1000 | Loss: 0.00002467
Iteration 207/1000 | Loss: 0.00002467
Iteration 208/1000 | Loss: 0.00002467
Iteration 209/1000 | Loss: 0.00002467
Iteration 210/1000 | Loss: 0.00002467
Iteration 211/1000 | Loss: 0.00002467
Iteration 212/1000 | Loss: 0.00002467
Iteration 213/1000 | Loss: 0.00002467
Iteration 214/1000 | Loss: 0.00002467
Iteration 215/1000 | Loss: 0.00002467
Iteration 216/1000 | Loss: 0.00002467
Iteration 217/1000 | Loss: 0.00002466
Iteration 218/1000 | Loss: 0.00002466
Iteration 219/1000 | Loss: 0.00002466
Iteration 220/1000 | Loss: 0.00002466
Iteration 221/1000 | Loss: 0.00002466
Iteration 222/1000 | Loss: 0.00002466
Iteration 223/1000 | Loss: 0.00002466
Iteration 224/1000 | Loss: 0.00002466
Iteration 225/1000 | Loss: 0.00002466
Iteration 226/1000 | Loss: 0.00002466
Iteration 227/1000 | Loss: 0.00002466
Iteration 228/1000 | Loss: 0.00002466
Iteration 229/1000 | Loss: 0.00002466
Iteration 230/1000 | Loss: 0.00002466
Iteration 231/1000 | Loss: 0.00002465
Iteration 232/1000 | Loss: 0.00002465
Iteration 233/1000 | Loss: 0.00002465
Iteration 234/1000 | Loss: 0.00002465
Iteration 235/1000 | Loss: 0.00002465
Iteration 236/1000 | Loss: 0.00002465
Iteration 237/1000 | Loss: 0.00002465
Iteration 238/1000 | Loss: 0.00002465
Iteration 239/1000 | Loss: 0.00002465
Iteration 240/1000 | Loss: 0.00002465
Iteration 241/1000 | Loss: 0.00002465
Iteration 242/1000 | Loss: 0.00002465
Iteration 243/1000 | Loss: 0.00002465
Iteration 244/1000 | Loss: 0.00002465
Iteration 245/1000 | Loss: 0.00002465
Iteration 246/1000 | Loss: 0.00002464
Iteration 247/1000 | Loss: 0.00002464
Iteration 248/1000 | Loss: 0.00002464
Iteration 249/1000 | Loss: 0.00002464
Iteration 250/1000 | Loss: 0.00002464
Iteration 251/1000 | Loss: 0.00002464
Iteration 252/1000 | Loss: 0.00002464
Iteration 253/1000 | Loss: 0.00002464
Iteration 254/1000 | Loss: 0.00002464
Iteration 255/1000 | Loss: 0.00002464
Iteration 256/1000 | Loss: 0.00002464
Iteration 257/1000 | Loss: 0.00002464
Iteration 258/1000 | Loss: 0.00002463
Iteration 259/1000 | Loss: 0.00002463
Iteration 260/1000 | Loss: 0.00002463
Iteration 261/1000 | Loss: 0.00002463
Iteration 262/1000 | Loss: 0.00002463
Iteration 263/1000 | Loss: 0.00002463
Iteration 264/1000 | Loss: 0.00002463
Iteration 265/1000 | Loss: 0.00002463
Iteration 266/1000 | Loss: 0.00002463
Iteration 267/1000 | Loss: 0.00002463
Iteration 268/1000 | Loss: 0.00002462
Iteration 269/1000 | Loss: 0.00002462
Iteration 270/1000 | Loss: 0.00002462
Iteration 271/1000 | Loss: 0.00002462
Iteration 272/1000 | Loss: 0.00002462
Iteration 273/1000 | Loss: 0.00002462
Iteration 274/1000 | Loss: 0.00002462
Iteration 275/1000 | Loss: 0.00002462
Iteration 276/1000 | Loss: 0.00002462
Iteration 277/1000 | Loss: 0.00002462
Iteration 278/1000 | Loss: 0.00002462
Iteration 279/1000 | Loss: 0.00002462
Iteration 280/1000 | Loss: 0.00002462
Iteration 281/1000 | Loss: 0.00002461
Iteration 282/1000 | Loss: 0.00002461
Iteration 283/1000 | Loss: 0.00002461
Iteration 284/1000 | Loss: 0.00002461
Iteration 285/1000 | Loss: 0.00002461
Iteration 286/1000 | Loss: 0.00002461
Iteration 287/1000 | Loss: 0.00002461
Iteration 288/1000 | Loss: 0.00002460
Iteration 289/1000 | Loss: 0.00002460
Iteration 290/1000 | Loss: 0.00002460
Iteration 291/1000 | Loss: 0.00002460
Iteration 292/1000 | Loss: 0.00002460
Iteration 293/1000 | Loss: 0.00002460
Iteration 294/1000 | Loss: 0.00002460
Iteration 295/1000 | Loss: 0.00002460
Iteration 296/1000 | Loss: 0.00002460
Iteration 297/1000 | Loss: 0.00002460
Iteration 298/1000 | Loss: 0.00002460
Iteration 299/1000 | Loss: 0.00002460
Iteration 300/1000 | Loss: 0.00002460
Iteration 301/1000 | Loss: 0.00002460
Iteration 302/1000 | Loss: 0.00002460
Iteration 303/1000 | Loss: 0.00002460
Iteration 304/1000 | Loss: 0.00002460
Iteration 305/1000 | Loss: 0.00002460
Iteration 306/1000 | Loss: 0.00002460
Iteration 307/1000 | Loss: 0.00002460
Iteration 308/1000 | Loss: 0.00002460
Iteration 309/1000 | Loss: 0.00002460
Iteration 310/1000 | Loss: 0.00002460
Iteration 311/1000 | Loss: 0.00002460
Iteration 312/1000 | Loss: 0.00002460
Iteration 313/1000 | Loss: 0.00002460
Iteration 314/1000 | Loss: 0.00002460
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 314. Stopping optimization.
Last 5 losses: [2.4600023607490584e-05, 2.4600023607490584e-05, 2.4600023607490584e-05, 2.4600023607490584e-05, 2.4600023607490584e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4600023607490584e-05

Optimization complete. Final v2v error: 4.025733947753906 mm

Highest mean error: 6.675910949707031 mm for frame 131

Lowest mean error: 3.149172782897949 mm for frame 172

Saving results

Total time: 151.2443265914917
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_022/1048/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_022/1048.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_022/1048
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01069630
Iteration 2/25 | Loss: 0.00215677
Iteration 3/25 | Loss: 0.00125422
Iteration 4/25 | Loss: 0.00117412
Iteration 5/25 | Loss: 0.00111588
Iteration 6/25 | Loss: 0.00114739
Iteration 7/25 | Loss: 0.00108020
Iteration 8/25 | Loss: 0.00103498
Iteration 9/25 | Loss: 0.00102168
Iteration 10/25 | Loss: 0.00096256
Iteration 11/25 | Loss: 0.00094091
Iteration 12/25 | Loss: 0.00093294
Iteration 13/25 | Loss: 0.00092125
Iteration 14/25 | Loss: 0.00086890
Iteration 15/25 | Loss: 0.00085850
Iteration 16/25 | Loss: 0.00085522
Iteration 17/25 | Loss: 0.00085006
Iteration 18/25 | Loss: 0.00085072
Iteration 19/25 | Loss: 0.00085392
Iteration 20/25 | Loss: 0.00085569
Iteration 21/25 | Loss: 0.00085362
Iteration 22/25 | Loss: 0.00084974
Iteration 23/25 | Loss: 0.00085215
Iteration 24/25 | Loss: 0.00086509
Iteration 25/25 | Loss: 0.00085907

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.67624235
Iteration 2/25 | Loss: 0.00267731
Iteration 3/25 | Loss: 0.00267731
Iteration 4/25 | Loss: 0.00267730
Iteration 5/25 | Loss: 0.00267730
Iteration 6/25 | Loss: 0.00267730
Iteration 7/25 | Loss: 0.00267730
Iteration 8/25 | Loss: 0.00267730
Iteration 9/25 | Loss: 0.00267730
Iteration 10/25 | Loss: 0.00267730
Iteration 11/25 | Loss: 0.00267730
Iteration 12/25 | Loss: 0.00267730
Iteration 13/25 | Loss: 0.00267730
Iteration 14/25 | Loss: 0.00267730
Iteration 15/25 | Loss: 0.00267730
Iteration 16/25 | Loss: 0.00267730
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.002677303273230791, 0.002677303273230791, 0.002677303273230791, 0.002677303273230791, 0.002677303273230791]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002677303273230791

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00267730
Iteration 2/1000 | Loss: 0.00082377
Iteration 3/1000 | Loss: 0.00053629
Iteration 4/1000 | Loss: 0.00051712
Iteration 5/1000 | Loss: 0.00044132
Iteration 6/1000 | Loss: 0.00122123
Iteration 7/1000 | Loss: 0.00088103
Iteration 8/1000 | Loss: 0.00068106
Iteration 9/1000 | Loss: 0.00050866
Iteration 10/1000 | Loss: 0.00056251
Iteration 11/1000 | Loss: 0.00046146
Iteration 12/1000 | Loss: 0.00021297
Iteration 13/1000 | Loss: 0.00012718
Iteration 14/1000 | Loss: 0.00020432
Iteration 15/1000 | Loss: 0.00023409
Iteration 16/1000 | Loss: 0.00027778
Iteration 17/1000 | Loss: 0.00020335
Iteration 18/1000 | Loss: 0.00018961
Iteration 19/1000 | Loss: 0.00027505
Iteration 20/1000 | Loss: 0.00053199
Iteration 21/1000 | Loss: 0.00032228
Iteration 22/1000 | Loss: 0.00059380
Iteration 23/1000 | Loss: 0.00057021
Iteration 24/1000 | Loss: 0.00059927
Iteration 25/1000 | Loss: 0.00061809
Iteration 26/1000 | Loss: 0.00041969
Iteration 27/1000 | Loss: 0.00058807
Iteration 28/1000 | Loss: 0.00060099
Iteration 29/1000 | Loss: 0.00057705
Iteration 30/1000 | Loss: 0.00056388
Iteration 31/1000 | Loss: 0.00061682
Iteration 32/1000 | Loss: 0.00058320
Iteration 33/1000 | Loss: 0.00076163
Iteration 34/1000 | Loss: 0.00062515
Iteration 35/1000 | Loss: 0.00059084
Iteration 36/1000 | Loss: 0.00060788
Iteration 37/1000 | Loss: 0.00066850
Iteration 38/1000 | Loss: 0.00060226
Iteration 39/1000 | Loss: 0.00054842
Iteration 40/1000 | Loss: 0.00063263
Iteration 41/1000 | Loss: 0.00042409
Iteration 42/1000 | Loss: 0.00058963
Iteration 43/1000 | Loss: 0.00037898
Iteration 44/1000 | Loss: 0.00051546
Iteration 45/1000 | Loss: 0.00022024
Iteration 46/1000 | Loss: 0.00026505
Iteration 47/1000 | Loss: 0.00013948
Iteration 48/1000 | Loss: 0.00018288
Iteration 49/1000 | Loss: 0.00012308
Iteration 50/1000 | Loss: 0.00017117
Iteration 51/1000 | Loss: 0.00015069
Iteration 52/1000 | Loss: 0.00021389
Iteration 53/1000 | Loss: 0.00010504
Iteration 54/1000 | Loss: 0.00027148
Iteration 55/1000 | Loss: 0.00031489
Iteration 56/1000 | Loss: 0.00011191
Iteration 57/1000 | Loss: 0.00022295
Iteration 58/1000 | Loss: 0.00019293
Iteration 59/1000 | Loss: 0.00037581
Iteration 60/1000 | Loss: 0.00027321
Iteration 61/1000 | Loss: 0.00015541
Iteration 62/1000 | Loss: 0.00010559
Iteration 63/1000 | Loss: 0.00008731
Iteration 64/1000 | Loss: 0.00022875
Iteration 65/1000 | Loss: 0.00009162
Iteration 66/1000 | Loss: 0.00029301
Iteration 67/1000 | Loss: 0.00032711
Iteration 68/1000 | Loss: 0.00040684
Iteration 69/1000 | Loss: 0.00048313
Iteration 70/1000 | Loss: 0.00050555
Iteration 71/1000 | Loss: 0.00031922
Iteration 72/1000 | Loss: 0.00039447
Iteration 73/1000 | Loss: 0.00041913
Iteration 74/1000 | Loss: 0.00022243
Iteration 75/1000 | Loss: 0.00051233
Iteration 76/1000 | Loss: 0.00032124
Iteration 77/1000 | Loss: 0.00028558
Iteration 78/1000 | Loss: 0.00035965
Iteration 79/1000 | Loss: 0.00039671
Iteration 80/1000 | Loss: 0.00050021
Iteration 81/1000 | Loss: 0.00005176
Iteration 82/1000 | Loss: 0.00024036
Iteration 83/1000 | Loss: 0.00031015
Iteration 84/1000 | Loss: 0.00024027
Iteration 85/1000 | Loss: 0.00003443
Iteration 86/1000 | Loss: 0.00012878
Iteration 87/1000 | Loss: 0.00014505
Iteration 88/1000 | Loss: 0.00056485
Iteration 89/1000 | Loss: 0.00029168
Iteration 90/1000 | Loss: 0.00033561
Iteration 91/1000 | Loss: 0.00051969
Iteration 92/1000 | Loss: 0.00027133
Iteration 93/1000 | Loss: 0.00019926
Iteration 94/1000 | Loss: 0.00005400
Iteration 95/1000 | Loss: 0.00010958
Iteration 96/1000 | Loss: 0.00010494
Iteration 97/1000 | Loss: 0.00008639
Iteration 98/1000 | Loss: 0.00005625
Iteration 99/1000 | Loss: 0.00008056
Iteration 100/1000 | Loss: 0.00006291
Iteration 101/1000 | Loss: 0.00021541
Iteration 102/1000 | Loss: 0.00013342
Iteration 103/1000 | Loss: 0.00016900
Iteration 104/1000 | Loss: 0.00005750
Iteration 105/1000 | Loss: 0.00005163
Iteration 106/1000 | Loss: 0.00019403
Iteration 107/1000 | Loss: 0.00017317
Iteration 108/1000 | Loss: 0.00036096
Iteration 109/1000 | Loss: 0.00017030
Iteration 110/1000 | Loss: 0.00011795
Iteration 111/1000 | Loss: 0.00036294
Iteration 112/1000 | Loss: 0.00008128
Iteration 113/1000 | Loss: 0.00016132
Iteration 114/1000 | Loss: 0.00017366
Iteration 115/1000 | Loss: 0.00018165
Iteration 116/1000 | Loss: 0.00032280
Iteration 117/1000 | Loss: 0.00020888
Iteration 118/1000 | Loss: 0.00020154
Iteration 119/1000 | Loss: 0.00005963
Iteration 120/1000 | Loss: 0.00014313
Iteration 121/1000 | Loss: 0.00004763
Iteration 122/1000 | Loss: 0.00025096
Iteration 123/1000 | Loss: 0.00006320
Iteration 124/1000 | Loss: 0.00004672
Iteration 125/1000 | Loss: 0.00101223
Iteration 126/1000 | Loss: 0.00074561
Iteration 127/1000 | Loss: 0.00059285
Iteration 128/1000 | Loss: 0.00015930
Iteration 129/1000 | Loss: 0.00004012
Iteration 130/1000 | Loss: 0.00038442
Iteration 131/1000 | Loss: 0.00027490
Iteration 132/1000 | Loss: 0.00003146
Iteration 133/1000 | Loss: 0.00003512
Iteration 134/1000 | Loss: 0.00005644
Iteration 135/1000 | Loss: 0.00005858
Iteration 136/1000 | Loss: 0.00005928
Iteration 137/1000 | Loss: 0.00018419
Iteration 138/1000 | Loss: 0.00004657
Iteration 139/1000 | Loss: 0.00007463
Iteration 140/1000 | Loss: 0.00016555
Iteration 141/1000 | Loss: 0.00008206
Iteration 142/1000 | Loss: 0.00003611
Iteration 143/1000 | Loss: 0.00003684
Iteration 144/1000 | Loss: 0.00004625
Iteration 145/1000 | Loss: 0.00004215
Iteration 146/1000 | Loss: 0.00002624
Iteration 147/1000 | Loss: 0.00003499
Iteration 148/1000 | Loss: 0.00005862
Iteration 149/1000 | Loss: 0.00003845
Iteration 150/1000 | Loss: 0.00002680
Iteration 151/1000 | Loss: 0.00002218
Iteration 152/1000 | Loss: 0.00004187
Iteration 153/1000 | Loss: 0.00002611
Iteration 154/1000 | Loss: 0.00004192
Iteration 155/1000 | Loss: 0.00002361
Iteration 156/1000 | Loss: 0.00003440
Iteration 157/1000 | Loss: 0.00004158
Iteration 158/1000 | Loss: 0.00002549
Iteration 159/1000 | Loss: 0.00002414
Iteration 160/1000 | Loss: 0.00003603
Iteration 161/1000 | Loss: 0.00002298
Iteration 162/1000 | Loss: 0.00004527
Iteration 163/1000 | Loss: 0.00004327
Iteration 164/1000 | Loss: 0.00003629
Iteration 165/1000 | Loss: 0.00002773
Iteration 166/1000 | Loss: 0.00003344
Iteration 167/1000 | Loss: 0.00004130
Iteration 168/1000 | Loss: 0.00003094
Iteration 169/1000 | Loss: 0.00003946
Iteration 170/1000 | Loss: 0.00004403
Iteration 171/1000 | Loss: 0.00003955
Iteration 172/1000 | Loss: 0.00005444
Iteration 173/1000 | Loss: 0.00003883
Iteration 174/1000 | Loss: 0.00004452
Iteration 175/1000 | Loss: 0.00003780
Iteration 176/1000 | Loss: 0.00002313
Iteration 177/1000 | Loss: 0.00002363
Iteration 178/1000 | Loss: 0.00002136
Iteration 179/1000 | Loss: 0.00003839
Iteration 180/1000 | Loss: 0.00004531
Iteration 181/1000 | Loss: 0.00002908
Iteration 182/1000 | Loss: 0.00003651
Iteration 183/1000 | Loss: 0.00004223
Iteration 184/1000 | Loss: 0.00004075
Iteration 185/1000 | Loss: 0.00004480
Iteration 186/1000 | Loss: 0.00004529
Iteration 187/1000 | Loss: 0.00006145
Iteration 188/1000 | Loss: 0.00003571
Iteration 189/1000 | Loss: 0.00003351
Iteration 190/1000 | Loss: 0.00002788
Iteration 191/1000 | Loss: 0.00003461
Iteration 192/1000 | Loss: 0.00002691
Iteration 193/1000 | Loss: 0.00003437
Iteration 194/1000 | Loss: 0.00003266
Iteration 195/1000 | Loss: 0.00004029
Iteration 196/1000 | Loss: 0.00003252
Iteration 197/1000 | Loss: 0.00004818
Iteration 198/1000 | Loss: 0.00004227
Iteration 199/1000 | Loss: 0.00003200
Iteration 200/1000 | Loss: 0.00005970
Iteration 201/1000 | Loss: 0.00003733
Iteration 202/1000 | Loss: 0.00002696
Iteration 203/1000 | Loss: 0.00002517
Iteration 204/1000 | Loss: 0.00004473
Iteration 205/1000 | Loss: 0.00004103
Iteration 206/1000 | Loss: 0.00003402
Iteration 207/1000 | Loss: 0.00003779
Iteration 208/1000 | Loss: 0.00004115
Iteration 209/1000 | Loss: 0.00005310
Iteration 210/1000 | Loss: 0.00004164
Iteration 211/1000 | Loss: 0.00004695
Iteration 212/1000 | Loss: 0.00004225
Iteration 213/1000 | Loss: 0.00004442
Iteration 214/1000 | Loss: 0.00004123
Iteration 215/1000 | Loss: 0.00006276
Iteration 216/1000 | Loss: 0.00007992
Iteration 217/1000 | Loss: 0.00003287
Iteration 218/1000 | Loss: 0.00003067
Iteration 219/1000 | Loss: 0.00002309
Iteration 220/1000 | Loss: 0.00001876
Iteration 221/1000 | Loss: 0.00018950
Iteration 222/1000 | Loss: 0.00002704
Iteration 223/1000 | Loss: 0.00002212
Iteration 224/1000 | Loss: 0.00002810
Iteration 225/1000 | Loss: 0.00001805
Iteration 226/1000 | Loss: 0.00001681
Iteration 227/1000 | Loss: 0.00002712
Iteration 228/1000 | Loss: 0.00001608
Iteration 229/1000 | Loss: 0.00001586
Iteration 230/1000 | Loss: 0.00001820
Iteration 231/1000 | Loss: 0.00003735
Iteration 232/1000 | Loss: 0.00001685
Iteration 233/1000 | Loss: 0.00001565
Iteration 234/1000 | Loss: 0.00001565
Iteration 235/1000 | Loss: 0.00001550
Iteration 236/1000 | Loss: 0.00001543
Iteration 237/1000 | Loss: 0.00001543
Iteration 238/1000 | Loss: 0.00001543
Iteration 239/1000 | Loss: 0.00001543
Iteration 240/1000 | Loss: 0.00001542
Iteration 241/1000 | Loss: 0.00001542
Iteration 242/1000 | Loss: 0.00001542
Iteration 243/1000 | Loss: 0.00001542
Iteration 244/1000 | Loss: 0.00001552
Iteration 245/1000 | Loss: 0.00001539
Iteration 246/1000 | Loss: 0.00001538
Iteration 247/1000 | Loss: 0.00001538
Iteration 248/1000 | Loss: 0.00001537
Iteration 249/1000 | Loss: 0.00001537
Iteration 250/1000 | Loss: 0.00001536
Iteration 251/1000 | Loss: 0.00001535
Iteration 252/1000 | Loss: 0.00001534
Iteration 253/1000 | Loss: 0.00001596
Iteration 254/1000 | Loss: 0.00001531
Iteration 255/1000 | Loss: 0.00001531
Iteration 256/1000 | Loss: 0.00001531
Iteration 257/1000 | Loss: 0.00001531
Iteration 258/1000 | Loss: 0.00001531
Iteration 259/1000 | Loss: 0.00001531
Iteration 260/1000 | Loss: 0.00001530
Iteration 261/1000 | Loss: 0.00001530
Iteration 262/1000 | Loss: 0.00001530
Iteration 263/1000 | Loss: 0.00001530
Iteration 264/1000 | Loss: 0.00001530
Iteration 265/1000 | Loss: 0.00001530
Iteration 266/1000 | Loss: 0.00001530
Iteration 267/1000 | Loss: 0.00001530
Iteration 268/1000 | Loss: 0.00001530
Iteration 269/1000 | Loss: 0.00001530
Iteration 270/1000 | Loss: 0.00001530
Iteration 271/1000 | Loss: 0.00001530
Iteration 272/1000 | Loss: 0.00001530
Iteration 273/1000 | Loss: 0.00001530
Iteration 274/1000 | Loss: 0.00001530
Iteration 275/1000 | Loss: 0.00001530
Iteration 276/1000 | Loss: 0.00001530
Iteration 277/1000 | Loss: 0.00001530
Iteration 278/1000 | Loss: 0.00001530
Iteration 279/1000 | Loss: 0.00001530
Iteration 280/1000 | Loss: 0.00001530
Iteration 281/1000 | Loss: 0.00001530
Iteration 282/1000 | Loss: 0.00001530
Iteration 283/1000 | Loss: 0.00001530
Iteration 284/1000 | Loss: 0.00001530
Iteration 285/1000 | Loss: 0.00001530
Iteration 286/1000 | Loss: 0.00001530
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 286. Stopping optimization.
Last 5 losses: [1.5303279724321328e-05, 1.5303279724321328e-05, 1.5303279724321328e-05, 1.5303279724321328e-05, 1.5303279724321328e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5303279724321328e-05

Optimization complete. Final v2v error: 3.272883415222168 mm

Highest mean error: 5.840064525604248 mm for frame 95

Lowest mean error: 2.8356070518493652 mm for frame 161

Saving results

Total time: 385.8023097515106
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_022/1016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_022/1016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_022/1016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00484321
Iteration 2/25 | Loss: 0.00107065
Iteration 3/25 | Loss: 0.00085554
Iteration 4/25 | Loss: 0.00081420
Iteration 5/25 | Loss: 0.00080647
Iteration 6/25 | Loss: 0.00080429
Iteration 7/25 | Loss: 0.00080376
Iteration 8/25 | Loss: 0.00080376
Iteration 9/25 | Loss: 0.00080376
Iteration 10/25 | Loss: 0.00080376
Iteration 11/25 | Loss: 0.00080376
Iteration 12/25 | Loss: 0.00080376
Iteration 13/25 | Loss: 0.00080376
Iteration 14/25 | Loss: 0.00080376
Iteration 15/25 | Loss: 0.00080376
Iteration 16/25 | Loss: 0.00080376
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0008037599036470056, 0.0008037599036470056, 0.0008037599036470056, 0.0008037599036470056, 0.0008037599036470056]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008037599036470056

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.50020099
Iteration 2/25 | Loss: 0.00135754
Iteration 3/25 | Loss: 0.00135750
Iteration 4/25 | Loss: 0.00135750
Iteration 5/25 | Loss: 0.00135750
Iteration 6/25 | Loss: 0.00135750
Iteration 7/25 | Loss: 0.00135750
Iteration 8/25 | Loss: 0.00135750
Iteration 9/25 | Loss: 0.00135750
Iteration 10/25 | Loss: 0.00135750
Iteration 11/25 | Loss: 0.00135750
Iteration 12/25 | Loss: 0.00135750
Iteration 13/25 | Loss: 0.00135750
Iteration 14/25 | Loss: 0.00135750
Iteration 15/25 | Loss: 0.00135750
Iteration 16/25 | Loss: 0.00135750
Iteration 17/25 | Loss: 0.00135750
Iteration 18/25 | Loss: 0.00135750
Iteration 19/25 | Loss: 0.00135750
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0013575013726949692, 0.0013575013726949692, 0.0013575013726949692, 0.0013575013726949692, 0.0013575013726949692]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013575013726949692

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00135750
Iteration 2/1000 | Loss: 0.00003890
Iteration 3/1000 | Loss: 0.00002616
Iteration 4/1000 | Loss: 0.00002284
Iteration 5/1000 | Loss: 0.00002143
Iteration 6/1000 | Loss: 0.00002032
Iteration 7/1000 | Loss: 0.00001967
Iteration 8/1000 | Loss: 0.00001927
Iteration 9/1000 | Loss: 0.00001887
Iteration 10/1000 | Loss: 0.00001861
Iteration 11/1000 | Loss: 0.00001835
Iteration 12/1000 | Loss: 0.00001820
Iteration 13/1000 | Loss: 0.00001801
Iteration 14/1000 | Loss: 0.00001794
Iteration 15/1000 | Loss: 0.00001794
Iteration 16/1000 | Loss: 0.00001791
Iteration 17/1000 | Loss: 0.00001788
Iteration 18/1000 | Loss: 0.00001787
Iteration 19/1000 | Loss: 0.00001786
Iteration 20/1000 | Loss: 0.00001786
Iteration 21/1000 | Loss: 0.00001784
Iteration 22/1000 | Loss: 0.00001783
Iteration 23/1000 | Loss: 0.00001781
Iteration 24/1000 | Loss: 0.00001781
Iteration 25/1000 | Loss: 0.00001780
Iteration 26/1000 | Loss: 0.00001779
Iteration 27/1000 | Loss: 0.00001779
Iteration 28/1000 | Loss: 0.00001778
Iteration 29/1000 | Loss: 0.00001777
Iteration 30/1000 | Loss: 0.00001777
Iteration 31/1000 | Loss: 0.00001776
Iteration 32/1000 | Loss: 0.00001776
Iteration 33/1000 | Loss: 0.00001775
Iteration 34/1000 | Loss: 0.00001774
Iteration 35/1000 | Loss: 0.00001774
Iteration 36/1000 | Loss: 0.00001774
Iteration 37/1000 | Loss: 0.00001773
Iteration 38/1000 | Loss: 0.00001773
Iteration 39/1000 | Loss: 0.00001772
Iteration 40/1000 | Loss: 0.00001772
Iteration 41/1000 | Loss: 0.00001771
Iteration 42/1000 | Loss: 0.00001771
Iteration 43/1000 | Loss: 0.00001771
Iteration 44/1000 | Loss: 0.00001770
Iteration 45/1000 | Loss: 0.00001770
Iteration 46/1000 | Loss: 0.00001769
Iteration 47/1000 | Loss: 0.00001769
Iteration 48/1000 | Loss: 0.00001769
Iteration 49/1000 | Loss: 0.00001768
Iteration 50/1000 | Loss: 0.00001768
Iteration 51/1000 | Loss: 0.00001768
Iteration 52/1000 | Loss: 0.00001768
Iteration 53/1000 | Loss: 0.00001768
Iteration 54/1000 | Loss: 0.00001767
Iteration 55/1000 | Loss: 0.00001767
Iteration 56/1000 | Loss: 0.00001767
Iteration 57/1000 | Loss: 0.00001766
Iteration 58/1000 | Loss: 0.00001765
Iteration 59/1000 | Loss: 0.00001765
Iteration 60/1000 | Loss: 0.00001765
Iteration 61/1000 | Loss: 0.00001764
Iteration 62/1000 | Loss: 0.00001764
Iteration 63/1000 | Loss: 0.00001764
Iteration 64/1000 | Loss: 0.00001764
Iteration 65/1000 | Loss: 0.00001764
Iteration 66/1000 | Loss: 0.00001764
Iteration 67/1000 | Loss: 0.00001763
Iteration 68/1000 | Loss: 0.00001763
Iteration 69/1000 | Loss: 0.00001762
Iteration 70/1000 | Loss: 0.00001762
Iteration 71/1000 | Loss: 0.00001762
Iteration 72/1000 | Loss: 0.00001762
Iteration 73/1000 | Loss: 0.00001762
Iteration 74/1000 | Loss: 0.00001761
Iteration 75/1000 | Loss: 0.00001761
Iteration 76/1000 | Loss: 0.00001761
Iteration 77/1000 | Loss: 0.00001760
Iteration 78/1000 | Loss: 0.00001760
Iteration 79/1000 | Loss: 0.00001760
Iteration 80/1000 | Loss: 0.00001760
Iteration 81/1000 | Loss: 0.00001760
Iteration 82/1000 | Loss: 0.00001759
Iteration 83/1000 | Loss: 0.00001759
Iteration 84/1000 | Loss: 0.00001759
Iteration 85/1000 | Loss: 0.00001758
Iteration 86/1000 | Loss: 0.00001758
Iteration 87/1000 | Loss: 0.00001758
Iteration 88/1000 | Loss: 0.00001758
Iteration 89/1000 | Loss: 0.00001757
Iteration 90/1000 | Loss: 0.00001757
Iteration 91/1000 | Loss: 0.00001756
Iteration 92/1000 | Loss: 0.00001756
Iteration 93/1000 | Loss: 0.00001756
Iteration 94/1000 | Loss: 0.00001755
Iteration 95/1000 | Loss: 0.00001755
Iteration 96/1000 | Loss: 0.00001755
Iteration 97/1000 | Loss: 0.00001755
Iteration 98/1000 | Loss: 0.00001754
Iteration 99/1000 | Loss: 0.00001754
Iteration 100/1000 | Loss: 0.00001754
Iteration 101/1000 | Loss: 0.00001753
Iteration 102/1000 | Loss: 0.00001753
Iteration 103/1000 | Loss: 0.00001753
Iteration 104/1000 | Loss: 0.00001753
Iteration 105/1000 | Loss: 0.00001753
Iteration 106/1000 | Loss: 0.00001753
Iteration 107/1000 | Loss: 0.00001753
Iteration 108/1000 | Loss: 0.00001752
Iteration 109/1000 | Loss: 0.00001752
Iteration 110/1000 | Loss: 0.00001752
Iteration 111/1000 | Loss: 0.00001752
Iteration 112/1000 | Loss: 0.00001752
Iteration 113/1000 | Loss: 0.00001752
Iteration 114/1000 | Loss: 0.00001752
Iteration 115/1000 | Loss: 0.00001752
Iteration 116/1000 | Loss: 0.00001752
Iteration 117/1000 | Loss: 0.00001751
Iteration 118/1000 | Loss: 0.00001751
Iteration 119/1000 | Loss: 0.00001751
Iteration 120/1000 | Loss: 0.00001751
Iteration 121/1000 | Loss: 0.00001751
Iteration 122/1000 | Loss: 0.00001751
Iteration 123/1000 | Loss: 0.00001751
Iteration 124/1000 | Loss: 0.00001750
Iteration 125/1000 | Loss: 0.00001750
Iteration 126/1000 | Loss: 0.00001750
Iteration 127/1000 | Loss: 0.00001750
Iteration 128/1000 | Loss: 0.00001750
Iteration 129/1000 | Loss: 0.00001750
Iteration 130/1000 | Loss: 0.00001749
Iteration 131/1000 | Loss: 0.00001749
Iteration 132/1000 | Loss: 0.00001749
Iteration 133/1000 | Loss: 0.00001749
Iteration 134/1000 | Loss: 0.00001749
Iteration 135/1000 | Loss: 0.00001749
Iteration 136/1000 | Loss: 0.00001749
Iteration 137/1000 | Loss: 0.00001749
Iteration 138/1000 | Loss: 0.00001748
Iteration 139/1000 | Loss: 0.00001748
Iteration 140/1000 | Loss: 0.00001748
Iteration 141/1000 | Loss: 0.00001748
Iteration 142/1000 | Loss: 0.00001748
Iteration 143/1000 | Loss: 0.00001748
Iteration 144/1000 | Loss: 0.00001748
Iteration 145/1000 | Loss: 0.00001748
Iteration 146/1000 | Loss: 0.00001748
Iteration 147/1000 | Loss: 0.00001748
Iteration 148/1000 | Loss: 0.00001748
Iteration 149/1000 | Loss: 0.00001747
Iteration 150/1000 | Loss: 0.00001747
Iteration 151/1000 | Loss: 0.00001747
Iteration 152/1000 | Loss: 0.00001747
Iteration 153/1000 | Loss: 0.00001747
Iteration 154/1000 | Loss: 0.00001747
Iteration 155/1000 | Loss: 0.00001747
Iteration 156/1000 | Loss: 0.00001747
Iteration 157/1000 | Loss: 0.00001747
Iteration 158/1000 | Loss: 0.00001747
Iteration 159/1000 | Loss: 0.00001747
Iteration 160/1000 | Loss: 0.00001747
Iteration 161/1000 | Loss: 0.00001747
Iteration 162/1000 | Loss: 0.00001747
Iteration 163/1000 | Loss: 0.00001747
Iteration 164/1000 | Loss: 0.00001747
Iteration 165/1000 | Loss: 0.00001747
Iteration 166/1000 | Loss: 0.00001747
Iteration 167/1000 | Loss: 0.00001746
Iteration 168/1000 | Loss: 0.00001746
Iteration 169/1000 | Loss: 0.00001746
Iteration 170/1000 | Loss: 0.00001746
Iteration 171/1000 | Loss: 0.00001746
Iteration 172/1000 | Loss: 0.00001746
Iteration 173/1000 | Loss: 0.00001746
Iteration 174/1000 | Loss: 0.00001746
Iteration 175/1000 | Loss: 0.00001746
Iteration 176/1000 | Loss: 0.00001746
Iteration 177/1000 | Loss: 0.00001746
Iteration 178/1000 | Loss: 0.00001746
Iteration 179/1000 | Loss: 0.00001746
Iteration 180/1000 | Loss: 0.00001746
Iteration 181/1000 | Loss: 0.00001746
Iteration 182/1000 | Loss: 0.00001746
Iteration 183/1000 | Loss: 0.00001746
Iteration 184/1000 | Loss: 0.00001746
Iteration 185/1000 | Loss: 0.00001746
Iteration 186/1000 | Loss: 0.00001746
Iteration 187/1000 | Loss: 0.00001746
Iteration 188/1000 | Loss: 0.00001746
Iteration 189/1000 | Loss: 0.00001746
Iteration 190/1000 | Loss: 0.00001746
Iteration 191/1000 | Loss: 0.00001746
Iteration 192/1000 | Loss: 0.00001746
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 192. Stopping optimization.
Last 5 losses: [1.7458751244703308e-05, 1.7458751244703308e-05, 1.7458751244703308e-05, 1.7458751244703308e-05, 1.7458751244703308e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7458751244703308e-05

Optimization complete. Final v2v error: 3.499143123626709 mm

Highest mean error: 4.427615165710449 mm for frame 33

Lowest mean error: 2.977402687072754 mm for frame 79

Saving results

Total time: 49.42292284965515
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_022/1073/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_022/1073.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_022/1073
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00933638
Iteration 2/25 | Loss: 0.00109406
Iteration 3/25 | Loss: 0.00090022
Iteration 4/25 | Loss: 0.00085823
Iteration 5/25 | Loss: 0.00083994
Iteration 6/25 | Loss: 0.00083571
Iteration 7/25 | Loss: 0.00083453
Iteration 8/25 | Loss: 0.00083440
Iteration 9/25 | Loss: 0.00083440
Iteration 10/25 | Loss: 0.00083440
Iteration 11/25 | Loss: 0.00083440
Iteration 12/25 | Loss: 0.00083440
Iteration 13/25 | Loss: 0.00083440
Iteration 14/25 | Loss: 0.00083440
Iteration 15/25 | Loss: 0.00083440
Iteration 16/25 | Loss: 0.00083440
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0008344023372046649, 0.0008344023372046649, 0.0008344023372046649, 0.0008344023372046649, 0.0008344023372046649]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008344023372046649

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.58272636
Iteration 2/25 | Loss: 0.00122589
Iteration 3/25 | Loss: 0.00122586
Iteration 4/25 | Loss: 0.00122586
Iteration 5/25 | Loss: 0.00122586
Iteration 6/25 | Loss: 0.00122586
Iteration 7/25 | Loss: 0.00122586
Iteration 8/25 | Loss: 0.00122586
Iteration 9/25 | Loss: 0.00122586
Iteration 10/25 | Loss: 0.00122586
Iteration 11/25 | Loss: 0.00122586
Iteration 12/25 | Loss: 0.00122586
Iteration 13/25 | Loss: 0.00122586
Iteration 14/25 | Loss: 0.00122586
Iteration 15/25 | Loss: 0.00122586
Iteration 16/25 | Loss: 0.00122586
Iteration 17/25 | Loss: 0.00122586
Iteration 18/25 | Loss: 0.00122586
Iteration 19/25 | Loss: 0.00122586
Iteration 20/25 | Loss: 0.00122586
Iteration 21/25 | Loss: 0.00122586
Iteration 22/25 | Loss: 0.00122586
Iteration 23/25 | Loss: 0.00122586
Iteration 24/25 | Loss: 0.00122586
Iteration 25/25 | Loss: 0.00122586

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00122586
Iteration 2/1000 | Loss: 0.00005648
Iteration 3/1000 | Loss: 0.00003638
Iteration 4/1000 | Loss: 0.00003074
Iteration 5/1000 | Loss: 0.00002913
Iteration 6/1000 | Loss: 0.00002779
Iteration 7/1000 | Loss: 0.00002684
Iteration 8/1000 | Loss: 0.00002609
Iteration 9/1000 | Loss: 0.00002553
Iteration 10/1000 | Loss: 0.00002514
Iteration 11/1000 | Loss: 0.00002490
Iteration 12/1000 | Loss: 0.00002471
Iteration 13/1000 | Loss: 0.00002456
Iteration 14/1000 | Loss: 0.00002444
Iteration 15/1000 | Loss: 0.00002443
Iteration 16/1000 | Loss: 0.00002443
Iteration 17/1000 | Loss: 0.00002441
Iteration 18/1000 | Loss: 0.00002440
Iteration 19/1000 | Loss: 0.00002440
Iteration 20/1000 | Loss: 0.00002435
Iteration 21/1000 | Loss: 0.00002435
Iteration 22/1000 | Loss: 0.00002434
Iteration 23/1000 | Loss: 0.00002433
Iteration 24/1000 | Loss: 0.00002433
Iteration 25/1000 | Loss: 0.00002432
Iteration 26/1000 | Loss: 0.00002432
Iteration 27/1000 | Loss: 0.00002431
Iteration 28/1000 | Loss: 0.00002431
Iteration 29/1000 | Loss: 0.00002430
Iteration 30/1000 | Loss: 0.00002430
Iteration 31/1000 | Loss: 0.00002430
Iteration 32/1000 | Loss: 0.00002430
Iteration 33/1000 | Loss: 0.00002430
Iteration 34/1000 | Loss: 0.00002429
Iteration 35/1000 | Loss: 0.00002429
Iteration 36/1000 | Loss: 0.00002428
Iteration 37/1000 | Loss: 0.00002428
Iteration 38/1000 | Loss: 0.00002427
Iteration 39/1000 | Loss: 0.00002427
Iteration 40/1000 | Loss: 0.00002427
Iteration 41/1000 | Loss: 0.00002426
Iteration 42/1000 | Loss: 0.00002426
Iteration 43/1000 | Loss: 0.00002425
Iteration 44/1000 | Loss: 0.00002424
Iteration 45/1000 | Loss: 0.00002424
Iteration 46/1000 | Loss: 0.00002421
Iteration 47/1000 | Loss: 0.00002421
Iteration 48/1000 | Loss: 0.00002419
Iteration 49/1000 | Loss: 0.00002418
Iteration 50/1000 | Loss: 0.00002418
Iteration 51/1000 | Loss: 0.00002417
Iteration 52/1000 | Loss: 0.00002416
Iteration 53/1000 | Loss: 0.00002416
Iteration 54/1000 | Loss: 0.00002416
Iteration 55/1000 | Loss: 0.00002415
Iteration 56/1000 | Loss: 0.00002415
Iteration 57/1000 | Loss: 0.00002414
Iteration 58/1000 | Loss: 0.00002414
Iteration 59/1000 | Loss: 0.00002413
Iteration 60/1000 | Loss: 0.00002413
Iteration 61/1000 | Loss: 0.00002413
Iteration 62/1000 | Loss: 0.00002413
Iteration 63/1000 | Loss: 0.00002413
Iteration 64/1000 | Loss: 0.00002413
Iteration 65/1000 | Loss: 0.00002413
Iteration 66/1000 | Loss: 0.00002413
Iteration 67/1000 | Loss: 0.00002412
Iteration 68/1000 | Loss: 0.00002412
Iteration 69/1000 | Loss: 0.00002412
Iteration 70/1000 | Loss: 0.00002411
Iteration 71/1000 | Loss: 0.00002411
Iteration 72/1000 | Loss: 0.00002411
Iteration 73/1000 | Loss: 0.00002410
Iteration 74/1000 | Loss: 0.00002410
Iteration 75/1000 | Loss: 0.00002410
Iteration 76/1000 | Loss: 0.00002409
Iteration 77/1000 | Loss: 0.00002409
Iteration 78/1000 | Loss: 0.00002409
Iteration 79/1000 | Loss: 0.00002409
Iteration 80/1000 | Loss: 0.00002409
Iteration 81/1000 | Loss: 0.00002409
Iteration 82/1000 | Loss: 0.00002409
Iteration 83/1000 | Loss: 0.00002409
Iteration 84/1000 | Loss: 0.00002409
Iteration 85/1000 | Loss: 0.00002409
Iteration 86/1000 | Loss: 0.00002408
Iteration 87/1000 | Loss: 0.00002408
Iteration 88/1000 | Loss: 0.00002408
Iteration 89/1000 | Loss: 0.00002408
Iteration 90/1000 | Loss: 0.00002408
Iteration 91/1000 | Loss: 0.00002408
Iteration 92/1000 | Loss: 0.00002407
Iteration 93/1000 | Loss: 0.00002407
Iteration 94/1000 | Loss: 0.00002407
Iteration 95/1000 | Loss: 0.00002407
Iteration 96/1000 | Loss: 0.00002406
Iteration 97/1000 | Loss: 0.00002406
Iteration 98/1000 | Loss: 0.00002406
Iteration 99/1000 | Loss: 0.00002405
Iteration 100/1000 | Loss: 0.00002405
Iteration 101/1000 | Loss: 0.00002405
Iteration 102/1000 | Loss: 0.00002405
Iteration 103/1000 | Loss: 0.00002405
Iteration 104/1000 | Loss: 0.00002405
Iteration 105/1000 | Loss: 0.00002404
Iteration 106/1000 | Loss: 0.00002404
Iteration 107/1000 | Loss: 0.00002404
Iteration 108/1000 | Loss: 0.00002404
Iteration 109/1000 | Loss: 0.00002404
Iteration 110/1000 | Loss: 0.00002404
Iteration 111/1000 | Loss: 0.00002403
Iteration 112/1000 | Loss: 0.00002403
Iteration 113/1000 | Loss: 0.00002403
Iteration 114/1000 | Loss: 0.00002403
Iteration 115/1000 | Loss: 0.00002403
Iteration 116/1000 | Loss: 0.00002403
Iteration 117/1000 | Loss: 0.00002403
Iteration 118/1000 | Loss: 0.00002403
Iteration 119/1000 | Loss: 0.00002403
Iteration 120/1000 | Loss: 0.00002402
Iteration 121/1000 | Loss: 0.00002402
Iteration 122/1000 | Loss: 0.00002402
Iteration 123/1000 | Loss: 0.00002402
Iteration 124/1000 | Loss: 0.00002402
Iteration 125/1000 | Loss: 0.00002402
Iteration 126/1000 | Loss: 0.00002402
Iteration 127/1000 | Loss: 0.00002402
Iteration 128/1000 | Loss: 0.00002402
Iteration 129/1000 | Loss: 0.00002402
Iteration 130/1000 | Loss: 0.00002402
Iteration 131/1000 | Loss: 0.00002401
Iteration 132/1000 | Loss: 0.00002401
Iteration 133/1000 | Loss: 0.00002401
Iteration 134/1000 | Loss: 0.00002401
Iteration 135/1000 | Loss: 0.00002401
Iteration 136/1000 | Loss: 0.00002401
Iteration 137/1000 | Loss: 0.00002401
Iteration 138/1000 | Loss: 0.00002401
Iteration 139/1000 | Loss: 0.00002401
Iteration 140/1000 | Loss: 0.00002401
Iteration 141/1000 | Loss: 0.00002401
Iteration 142/1000 | Loss: 0.00002401
Iteration 143/1000 | Loss: 0.00002401
Iteration 144/1000 | Loss: 0.00002401
Iteration 145/1000 | Loss: 0.00002400
Iteration 146/1000 | Loss: 0.00002400
Iteration 147/1000 | Loss: 0.00002400
Iteration 148/1000 | Loss: 0.00002400
Iteration 149/1000 | Loss: 0.00002400
Iteration 150/1000 | Loss: 0.00002400
Iteration 151/1000 | Loss: 0.00002400
Iteration 152/1000 | Loss: 0.00002400
Iteration 153/1000 | Loss: 0.00002400
Iteration 154/1000 | Loss: 0.00002400
Iteration 155/1000 | Loss: 0.00002400
Iteration 156/1000 | Loss: 0.00002400
Iteration 157/1000 | Loss: 0.00002400
Iteration 158/1000 | Loss: 0.00002399
Iteration 159/1000 | Loss: 0.00002399
Iteration 160/1000 | Loss: 0.00002399
Iteration 161/1000 | Loss: 0.00002399
Iteration 162/1000 | Loss: 0.00002399
Iteration 163/1000 | Loss: 0.00002399
Iteration 164/1000 | Loss: 0.00002399
Iteration 165/1000 | Loss: 0.00002399
Iteration 166/1000 | Loss: 0.00002399
Iteration 167/1000 | Loss: 0.00002399
Iteration 168/1000 | Loss: 0.00002399
Iteration 169/1000 | Loss: 0.00002398
Iteration 170/1000 | Loss: 0.00002398
Iteration 171/1000 | Loss: 0.00002398
Iteration 172/1000 | Loss: 0.00002398
Iteration 173/1000 | Loss: 0.00002398
Iteration 174/1000 | Loss: 0.00002398
Iteration 175/1000 | Loss: 0.00002398
Iteration 176/1000 | Loss: 0.00002398
Iteration 177/1000 | Loss: 0.00002398
Iteration 178/1000 | Loss: 0.00002398
Iteration 179/1000 | Loss: 0.00002398
Iteration 180/1000 | Loss: 0.00002398
Iteration 181/1000 | Loss: 0.00002397
Iteration 182/1000 | Loss: 0.00002397
Iteration 183/1000 | Loss: 0.00002397
Iteration 184/1000 | Loss: 0.00002397
Iteration 185/1000 | Loss: 0.00002397
Iteration 186/1000 | Loss: 0.00002397
Iteration 187/1000 | Loss: 0.00002397
Iteration 188/1000 | Loss: 0.00002397
Iteration 189/1000 | Loss: 0.00002397
Iteration 190/1000 | Loss: 0.00002397
Iteration 191/1000 | Loss: 0.00002397
Iteration 192/1000 | Loss: 0.00002397
Iteration 193/1000 | Loss: 0.00002397
Iteration 194/1000 | Loss: 0.00002397
Iteration 195/1000 | Loss: 0.00002397
Iteration 196/1000 | Loss: 0.00002397
Iteration 197/1000 | Loss: 0.00002397
Iteration 198/1000 | Loss: 0.00002397
Iteration 199/1000 | Loss: 0.00002397
Iteration 200/1000 | Loss: 0.00002397
Iteration 201/1000 | Loss: 0.00002397
Iteration 202/1000 | Loss: 0.00002397
Iteration 203/1000 | Loss: 0.00002397
Iteration 204/1000 | Loss: 0.00002397
Iteration 205/1000 | Loss: 0.00002397
Iteration 206/1000 | Loss: 0.00002397
Iteration 207/1000 | Loss: 0.00002397
Iteration 208/1000 | Loss: 0.00002397
Iteration 209/1000 | Loss: 0.00002397
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 209. Stopping optimization.
Last 5 losses: [2.3969760150066577e-05, 2.3969760150066577e-05, 2.3969760150066577e-05, 2.3969760150066577e-05, 2.3969760150066577e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3969760150066577e-05

Optimization complete. Final v2v error: 4.108912944793701 mm

Highest mean error: 5.783638954162598 mm for frame 69

Lowest mean error: 3.6491830348968506 mm for frame 94

Saving results

Total time: 46.571852922439575
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_022/1081/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_022/1081.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_022/1081
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01035845
Iteration 2/25 | Loss: 0.00324285
Iteration 3/25 | Loss: 0.00190279
Iteration 4/25 | Loss: 0.00159315
Iteration 5/25 | Loss: 0.00158543
Iteration 6/25 | Loss: 0.00159185
Iteration 7/25 | Loss: 0.00151004
Iteration 8/25 | Loss: 0.00135968
Iteration 9/25 | Loss: 0.00126046
Iteration 10/25 | Loss: 0.00116253
Iteration 11/25 | Loss: 0.00112025
Iteration 12/25 | Loss: 0.00108680
Iteration 13/25 | Loss: 0.00106182
Iteration 14/25 | Loss: 0.00104046
Iteration 15/25 | Loss: 0.00101641
Iteration 16/25 | Loss: 0.00100740
Iteration 17/25 | Loss: 0.00099085
Iteration 18/25 | Loss: 0.00097443
Iteration 19/25 | Loss: 0.00096439
Iteration 20/25 | Loss: 0.00095122
Iteration 21/25 | Loss: 0.00095101
Iteration 22/25 | Loss: 0.00094824
Iteration 23/25 | Loss: 0.00094265
Iteration 24/25 | Loss: 0.00094587
Iteration 25/25 | Loss: 0.00093386

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.59077477
Iteration 2/25 | Loss: 0.00271636
Iteration 3/25 | Loss: 0.00243071
Iteration 4/25 | Loss: 0.00243071
Iteration 5/25 | Loss: 0.00243071
Iteration 6/25 | Loss: 0.00243071
Iteration 7/25 | Loss: 0.00243071
Iteration 8/25 | Loss: 0.00243071
Iteration 9/25 | Loss: 0.00243071
Iteration 10/25 | Loss: 0.00243071
Iteration 11/25 | Loss: 0.00243071
Iteration 12/25 | Loss: 0.00243071
Iteration 13/25 | Loss: 0.00243071
Iteration 14/25 | Loss: 0.00243071
Iteration 15/25 | Loss: 0.00243071
Iteration 16/25 | Loss: 0.00243071
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.002430708846077323, 0.002430708846077323, 0.002430708846077323, 0.002430708846077323, 0.002430708846077323]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002430708846077323

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00243071
Iteration 2/1000 | Loss: 0.00112806
Iteration 3/1000 | Loss: 0.00064176
Iteration 4/1000 | Loss: 0.00038303
Iteration 5/1000 | Loss: 0.00027805
Iteration 6/1000 | Loss: 0.00108781
Iteration 7/1000 | Loss: 0.00123696
Iteration 8/1000 | Loss: 0.00038057
Iteration 9/1000 | Loss: 0.00065612
Iteration 10/1000 | Loss: 0.00036434
Iteration 11/1000 | Loss: 0.00039231
Iteration 12/1000 | Loss: 0.00039372
Iteration 13/1000 | Loss: 0.00038390
Iteration 14/1000 | Loss: 0.00032978
Iteration 15/1000 | Loss: 0.00070649
Iteration 16/1000 | Loss: 0.00058345
Iteration 17/1000 | Loss: 0.00083387
Iteration 18/1000 | Loss: 0.00121672
Iteration 19/1000 | Loss: 0.00091131
Iteration 20/1000 | Loss: 0.00057540
Iteration 21/1000 | Loss: 0.00069071
Iteration 22/1000 | Loss: 0.00037010
Iteration 23/1000 | Loss: 0.00071612
Iteration 24/1000 | Loss: 0.00026159
Iteration 25/1000 | Loss: 0.00041716
Iteration 26/1000 | Loss: 0.00028085
Iteration 27/1000 | Loss: 0.00034685
Iteration 28/1000 | Loss: 0.00045321
Iteration 29/1000 | Loss: 0.00054138
Iteration 30/1000 | Loss: 0.00028730
Iteration 31/1000 | Loss: 0.00015315
Iteration 32/1000 | Loss: 0.00042670
Iteration 33/1000 | Loss: 0.00033935
Iteration 34/1000 | Loss: 0.00017103
Iteration 35/1000 | Loss: 0.00054442
Iteration 36/1000 | Loss: 0.00054337
Iteration 37/1000 | Loss: 0.00088022
Iteration 38/1000 | Loss: 0.00016440
Iteration 39/1000 | Loss: 0.00018276
Iteration 40/1000 | Loss: 0.00035633
Iteration 41/1000 | Loss: 0.00014906
Iteration 42/1000 | Loss: 0.00044895
Iteration 43/1000 | Loss: 0.00048601
Iteration 44/1000 | Loss: 0.00019814
Iteration 45/1000 | Loss: 0.00011236
Iteration 46/1000 | Loss: 0.00011166
Iteration 47/1000 | Loss: 0.00010208
Iteration 48/1000 | Loss: 0.00013053
Iteration 49/1000 | Loss: 0.00084641
Iteration 50/1000 | Loss: 0.00035571
Iteration 51/1000 | Loss: 0.00047496
Iteration 52/1000 | Loss: 0.00016155
Iteration 53/1000 | Loss: 0.00031329
Iteration 54/1000 | Loss: 0.00015724
Iteration 55/1000 | Loss: 0.00039197
Iteration 56/1000 | Loss: 0.00030867
Iteration 57/1000 | Loss: 0.00029810
Iteration 58/1000 | Loss: 0.00014341
Iteration 59/1000 | Loss: 0.00016218
Iteration 60/1000 | Loss: 0.00014055
Iteration 61/1000 | Loss: 0.00021547
Iteration 62/1000 | Loss: 0.00041972
Iteration 63/1000 | Loss: 0.00052973
Iteration 64/1000 | Loss: 0.00063462
Iteration 65/1000 | Loss: 0.00031067
Iteration 66/1000 | Loss: 0.00039365
Iteration 67/1000 | Loss: 0.00016810
Iteration 68/1000 | Loss: 0.00015002
Iteration 69/1000 | Loss: 0.00027868
Iteration 70/1000 | Loss: 0.00027377
Iteration 71/1000 | Loss: 0.00011058
Iteration 72/1000 | Loss: 0.00025121
Iteration 73/1000 | Loss: 0.00020833
Iteration 74/1000 | Loss: 0.00014957
Iteration 75/1000 | Loss: 0.00014135
Iteration 76/1000 | Loss: 0.00011393
Iteration 77/1000 | Loss: 0.00026840
Iteration 78/1000 | Loss: 0.00039442
Iteration 79/1000 | Loss: 0.00022148
Iteration 80/1000 | Loss: 0.00032651
Iteration 81/1000 | Loss: 0.00013127
Iteration 82/1000 | Loss: 0.00027570
Iteration 83/1000 | Loss: 0.00025129
Iteration 84/1000 | Loss: 0.00025764
Iteration 85/1000 | Loss: 0.00015678
Iteration 86/1000 | Loss: 0.00014801
Iteration 87/1000 | Loss: 0.00025629
Iteration 88/1000 | Loss: 0.00058748
Iteration 89/1000 | Loss: 0.00016583
Iteration 90/1000 | Loss: 0.00012272
Iteration 91/1000 | Loss: 0.00026267
Iteration 92/1000 | Loss: 0.00011158
Iteration 93/1000 | Loss: 0.00012719
Iteration 94/1000 | Loss: 0.00013468
Iteration 95/1000 | Loss: 0.00013678
Iteration 96/1000 | Loss: 0.00025647
Iteration 97/1000 | Loss: 0.00026509
Iteration 98/1000 | Loss: 0.00012525
Iteration 99/1000 | Loss: 0.00010343
Iteration 100/1000 | Loss: 0.00012410
Iteration 101/1000 | Loss: 0.00022739
Iteration 102/1000 | Loss: 0.00045504
Iteration 103/1000 | Loss: 0.00024273
Iteration 104/1000 | Loss: 0.00022981
Iteration 105/1000 | Loss: 0.00012135
Iteration 106/1000 | Loss: 0.00003829
Iteration 107/1000 | Loss: 0.00004805
Iteration 108/1000 | Loss: 0.00004761
Iteration 109/1000 | Loss: 0.00005023
Iteration 110/1000 | Loss: 0.00005644
Iteration 111/1000 | Loss: 0.00004613
Iteration 112/1000 | Loss: 0.00017996
Iteration 113/1000 | Loss: 0.00033092
Iteration 114/1000 | Loss: 0.00027928
Iteration 115/1000 | Loss: 0.00039478
Iteration 116/1000 | Loss: 0.00003555
Iteration 117/1000 | Loss: 0.00002958
Iteration 118/1000 | Loss: 0.00023578
Iteration 119/1000 | Loss: 0.00027331
Iteration 120/1000 | Loss: 0.00021189
Iteration 121/1000 | Loss: 0.00004144
Iteration 122/1000 | Loss: 0.00002961
Iteration 123/1000 | Loss: 0.00032973
Iteration 124/1000 | Loss: 0.00018430
Iteration 125/1000 | Loss: 0.00003458
Iteration 126/1000 | Loss: 0.00045029
Iteration 127/1000 | Loss: 0.00015934
Iteration 128/1000 | Loss: 0.00011261
Iteration 129/1000 | Loss: 0.00003427
Iteration 130/1000 | Loss: 0.00043583
Iteration 131/1000 | Loss: 0.00064382
Iteration 132/1000 | Loss: 0.00049711
Iteration 133/1000 | Loss: 0.00019451
Iteration 134/1000 | Loss: 0.00025921
Iteration 135/1000 | Loss: 0.00030503
Iteration 136/1000 | Loss: 0.00022245
Iteration 137/1000 | Loss: 0.00016196
Iteration 138/1000 | Loss: 0.00021282
Iteration 139/1000 | Loss: 0.00059372
Iteration 140/1000 | Loss: 0.00068721
Iteration 141/1000 | Loss: 0.00055752
Iteration 142/1000 | Loss: 0.00084999
Iteration 143/1000 | Loss: 0.00102051
Iteration 144/1000 | Loss: 0.00016267
Iteration 145/1000 | Loss: 0.00036714
Iteration 146/1000 | Loss: 0.00010310
Iteration 147/1000 | Loss: 0.00013194
Iteration 148/1000 | Loss: 0.00007762
Iteration 149/1000 | Loss: 0.00003218
Iteration 150/1000 | Loss: 0.00058884
Iteration 151/1000 | Loss: 0.00055169
Iteration 152/1000 | Loss: 0.00025852
Iteration 153/1000 | Loss: 0.00030513
Iteration 154/1000 | Loss: 0.00022384
Iteration 155/1000 | Loss: 0.00020133
Iteration 156/1000 | Loss: 0.00018482
Iteration 157/1000 | Loss: 0.00045712
Iteration 158/1000 | Loss: 0.00022302
Iteration 159/1000 | Loss: 0.00023445
Iteration 160/1000 | Loss: 0.00012326
Iteration 161/1000 | Loss: 0.00016814
Iteration 162/1000 | Loss: 0.00017464
Iteration 163/1000 | Loss: 0.00009544
Iteration 164/1000 | Loss: 0.00012660
Iteration 165/1000 | Loss: 0.00009632
Iteration 166/1000 | Loss: 0.00011676
Iteration 167/1000 | Loss: 0.00008999
Iteration 168/1000 | Loss: 0.00072307
Iteration 169/1000 | Loss: 0.00027636
Iteration 170/1000 | Loss: 0.00022687
Iteration 171/1000 | Loss: 0.00016999
Iteration 172/1000 | Loss: 0.00017117
Iteration 173/1000 | Loss: 0.00002842
Iteration 174/1000 | Loss: 0.00059618
Iteration 175/1000 | Loss: 0.00059933
Iteration 176/1000 | Loss: 0.00023824
Iteration 177/1000 | Loss: 0.00046968
Iteration 178/1000 | Loss: 0.00075290
Iteration 179/1000 | Loss: 0.00038135
Iteration 180/1000 | Loss: 0.00027045
Iteration 181/1000 | Loss: 0.00005800
Iteration 182/1000 | Loss: 0.00003454
Iteration 183/1000 | Loss: 0.00002693
Iteration 184/1000 | Loss: 0.00002420
Iteration 185/1000 | Loss: 0.00002263
Iteration 186/1000 | Loss: 0.00002157
Iteration 187/1000 | Loss: 0.00002041
Iteration 188/1000 | Loss: 0.00001989
Iteration 189/1000 | Loss: 0.00001948
Iteration 190/1000 | Loss: 0.00001901
Iteration 191/1000 | Loss: 0.00025692
Iteration 192/1000 | Loss: 0.00028944
Iteration 193/1000 | Loss: 0.00030892
Iteration 194/1000 | Loss: 0.00058489
Iteration 195/1000 | Loss: 0.00003119
Iteration 196/1000 | Loss: 0.00070882
Iteration 197/1000 | Loss: 0.00003233
Iteration 198/1000 | Loss: 0.00003094
Iteration 199/1000 | Loss: 0.00002011
Iteration 200/1000 | Loss: 0.00001818
Iteration 201/1000 | Loss: 0.00001757
Iteration 202/1000 | Loss: 0.00001719
Iteration 203/1000 | Loss: 0.00017364
Iteration 204/1000 | Loss: 0.00016511
Iteration 205/1000 | Loss: 0.00015961
Iteration 206/1000 | Loss: 0.00017944
Iteration 207/1000 | Loss: 0.00001718
Iteration 208/1000 | Loss: 0.00014908
Iteration 209/1000 | Loss: 0.00002586
Iteration 210/1000 | Loss: 0.00002057
Iteration 211/1000 | Loss: 0.00001888
Iteration 212/1000 | Loss: 0.00015064
Iteration 213/1000 | Loss: 0.00003758
Iteration 214/1000 | Loss: 0.00003860
Iteration 215/1000 | Loss: 0.00001689
Iteration 216/1000 | Loss: 0.00016523
Iteration 217/1000 | Loss: 0.00006572
Iteration 218/1000 | Loss: 0.00002080
Iteration 219/1000 | Loss: 0.00016366
Iteration 220/1000 | Loss: 0.00002890
Iteration 221/1000 | Loss: 0.00011775
Iteration 222/1000 | Loss: 0.00002103
Iteration 223/1000 | Loss: 0.00010093
Iteration 224/1000 | Loss: 0.00001901
Iteration 225/1000 | Loss: 0.00008303
Iteration 226/1000 | Loss: 0.00002037
Iteration 227/1000 | Loss: 0.00007681
Iteration 228/1000 | Loss: 0.00001781
Iteration 229/1000 | Loss: 0.00001695
Iteration 230/1000 | Loss: 0.00001643
Iteration 231/1000 | Loss: 0.00001636
Iteration 232/1000 | Loss: 0.00001625
Iteration 233/1000 | Loss: 0.00001604
Iteration 234/1000 | Loss: 0.00001600
Iteration 235/1000 | Loss: 0.00001595
Iteration 236/1000 | Loss: 0.00001594
Iteration 237/1000 | Loss: 0.00001593
Iteration 238/1000 | Loss: 0.00001593
Iteration 239/1000 | Loss: 0.00001591
Iteration 240/1000 | Loss: 0.00001583
Iteration 241/1000 | Loss: 0.00001579
Iteration 242/1000 | Loss: 0.00001573
Iteration 243/1000 | Loss: 0.00001569
Iteration 244/1000 | Loss: 0.00001566
Iteration 245/1000 | Loss: 0.00001565
Iteration 246/1000 | Loss: 0.00001564
Iteration 247/1000 | Loss: 0.00001564
Iteration 248/1000 | Loss: 0.00001563
Iteration 249/1000 | Loss: 0.00001563
Iteration 250/1000 | Loss: 0.00001562
Iteration 251/1000 | Loss: 0.00001562
Iteration 252/1000 | Loss: 0.00001561
Iteration 253/1000 | Loss: 0.00001561
Iteration 254/1000 | Loss: 0.00001560
Iteration 255/1000 | Loss: 0.00001560
Iteration 256/1000 | Loss: 0.00001560
Iteration 257/1000 | Loss: 0.00001559
Iteration 258/1000 | Loss: 0.00001559
Iteration 259/1000 | Loss: 0.00001559
Iteration 260/1000 | Loss: 0.00001559
Iteration 261/1000 | Loss: 0.00001558
Iteration 262/1000 | Loss: 0.00001558
Iteration 263/1000 | Loss: 0.00001558
Iteration 264/1000 | Loss: 0.00001558
Iteration 265/1000 | Loss: 0.00001558
Iteration 266/1000 | Loss: 0.00001558
Iteration 267/1000 | Loss: 0.00001558
Iteration 268/1000 | Loss: 0.00001558
Iteration 269/1000 | Loss: 0.00001558
Iteration 270/1000 | Loss: 0.00001558
Iteration 271/1000 | Loss: 0.00001558
Iteration 272/1000 | Loss: 0.00001558
Iteration 273/1000 | Loss: 0.00001558
Iteration 274/1000 | Loss: 0.00001558
Iteration 275/1000 | Loss: 0.00001558
Iteration 276/1000 | Loss: 0.00001558
Iteration 277/1000 | Loss: 0.00001558
Iteration 278/1000 | Loss: 0.00001558
Iteration 279/1000 | Loss: 0.00001558
Iteration 280/1000 | Loss: 0.00001558
Iteration 281/1000 | Loss: 0.00001558
Iteration 282/1000 | Loss: 0.00001557
Iteration 283/1000 | Loss: 0.00001557
Iteration 284/1000 | Loss: 0.00001557
Iteration 285/1000 | Loss: 0.00001557
Iteration 286/1000 | Loss: 0.00001557
Iteration 287/1000 | Loss: 0.00001557
Iteration 288/1000 | Loss: 0.00001557
Iteration 289/1000 | Loss: 0.00001557
Iteration 290/1000 | Loss: 0.00001557
Iteration 291/1000 | Loss: 0.00001557
Iteration 292/1000 | Loss: 0.00001557
Iteration 293/1000 | Loss: 0.00001557
Iteration 294/1000 | Loss: 0.00001557
Iteration 295/1000 | Loss: 0.00001557
Iteration 296/1000 | Loss: 0.00001557
Iteration 297/1000 | Loss: 0.00001557
Iteration 298/1000 | Loss: 0.00001557
Iteration 299/1000 | Loss: 0.00001557
Iteration 300/1000 | Loss: 0.00001557
Iteration 301/1000 | Loss: 0.00001557
Iteration 302/1000 | Loss: 0.00001557
Iteration 303/1000 | Loss: 0.00001557
Iteration 304/1000 | Loss: 0.00001557
Iteration 305/1000 | Loss: 0.00001557
Iteration 306/1000 | Loss: 0.00001556
Iteration 307/1000 | Loss: 0.00001556
Iteration 308/1000 | Loss: 0.00001556
Iteration 309/1000 | Loss: 0.00001556
Iteration 310/1000 | Loss: 0.00001556
Iteration 311/1000 | Loss: 0.00001556
Iteration 312/1000 | Loss: 0.00001556
Iteration 313/1000 | Loss: 0.00001556
Iteration 314/1000 | Loss: 0.00001556
Iteration 315/1000 | Loss: 0.00001556
Iteration 316/1000 | Loss: 0.00001556
Iteration 317/1000 | Loss: 0.00001556
Iteration 318/1000 | Loss: 0.00001556
Iteration 319/1000 | Loss: 0.00001556
Iteration 320/1000 | Loss: 0.00001556
Iteration 321/1000 | Loss: 0.00001556
Iteration 322/1000 | Loss: 0.00001556
Iteration 323/1000 | Loss: 0.00001556
Iteration 324/1000 | Loss: 0.00001556
Iteration 325/1000 | Loss: 0.00001556
Iteration 326/1000 | Loss: 0.00001556
Iteration 327/1000 | Loss: 0.00001556
Iteration 328/1000 | Loss: 0.00001556
Iteration 329/1000 | Loss: 0.00001556
Iteration 330/1000 | Loss: 0.00001556
Iteration 331/1000 | Loss: 0.00001556
Iteration 332/1000 | Loss: 0.00001556
Iteration 333/1000 | Loss: 0.00001556
Iteration 334/1000 | Loss: 0.00001556
Iteration 335/1000 | Loss: 0.00001556
Iteration 336/1000 | Loss: 0.00001556
Iteration 337/1000 | Loss: 0.00001556
Iteration 338/1000 | Loss: 0.00001556
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 338. Stopping optimization.
Last 5 losses: [1.556273127789609e-05, 1.556273127789609e-05, 1.556273127789609e-05, 1.556273127789609e-05, 1.556273127789609e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.556273127789609e-05

Optimization complete. Final v2v error: 3.3357858657836914 mm

Highest mean error: 4.9689249992370605 mm for frame 233

Lowest mean error: 3.0514700412750244 mm for frame 128

Saving results

Total time: 437.5315103530884
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_022/1038/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_022/1038.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_022/1038
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00357533
Iteration 2/25 | Loss: 0.00084294
Iteration 3/25 | Loss: 0.00074544
Iteration 4/25 | Loss: 0.00073600
Iteration 5/25 | Loss: 0.00073308
Iteration 6/25 | Loss: 0.00073187
Iteration 7/25 | Loss: 0.00073162
Iteration 8/25 | Loss: 0.00073162
Iteration 9/25 | Loss: 0.00073162
Iteration 10/25 | Loss: 0.00073162
Iteration 11/25 | Loss: 0.00073162
Iteration 12/25 | Loss: 0.00073162
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0007316158153116703, 0.0007316158153116703, 0.0007316158153116703, 0.0007316158153116703, 0.0007316158153116703]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007316158153116703

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.87615395
Iteration 2/25 | Loss: 0.00128251
Iteration 3/25 | Loss: 0.00128251
Iteration 4/25 | Loss: 0.00128251
Iteration 5/25 | Loss: 0.00128251
Iteration 6/25 | Loss: 0.00128250
Iteration 7/25 | Loss: 0.00128250
Iteration 8/25 | Loss: 0.00128250
Iteration 9/25 | Loss: 0.00128250
Iteration 10/25 | Loss: 0.00128250
Iteration 11/25 | Loss: 0.00128250
Iteration 12/25 | Loss: 0.00128250
Iteration 13/25 | Loss: 0.00128250
Iteration 14/25 | Loss: 0.00128250
Iteration 15/25 | Loss: 0.00128250
Iteration 16/25 | Loss: 0.00128250
Iteration 17/25 | Loss: 0.00128250
Iteration 18/25 | Loss: 0.00128250
Iteration 19/25 | Loss: 0.00128250
Iteration 20/25 | Loss: 0.00128250
Iteration 21/25 | Loss: 0.00128250
Iteration 22/25 | Loss: 0.00128250
Iteration 23/25 | Loss: 0.00128250
Iteration 24/25 | Loss: 0.00128250
Iteration 25/25 | Loss: 0.00128250

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00128250
Iteration 2/1000 | Loss: 0.00002180
Iteration 3/1000 | Loss: 0.00001204
Iteration 4/1000 | Loss: 0.00001110
Iteration 5/1000 | Loss: 0.00001035
Iteration 6/1000 | Loss: 0.00001005
Iteration 7/1000 | Loss: 0.00000976
Iteration 8/1000 | Loss: 0.00000963
Iteration 9/1000 | Loss: 0.00000962
Iteration 10/1000 | Loss: 0.00000962
Iteration 11/1000 | Loss: 0.00000961
Iteration 12/1000 | Loss: 0.00000960
Iteration 13/1000 | Loss: 0.00000960
Iteration 14/1000 | Loss: 0.00000958
Iteration 15/1000 | Loss: 0.00000957
Iteration 16/1000 | Loss: 0.00000955
Iteration 17/1000 | Loss: 0.00000954
Iteration 18/1000 | Loss: 0.00000954
Iteration 19/1000 | Loss: 0.00000953
Iteration 20/1000 | Loss: 0.00000953
Iteration 21/1000 | Loss: 0.00000952
Iteration 22/1000 | Loss: 0.00000952
Iteration 23/1000 | Loss: 0.00000950
Iteration 24/1000 | Loss: 0.00000950
Iteration 25/1000 | Loss: 0.00000949
Iteration 26/1000 | Loss: 0.00000949
Iteration 27/1000 | Loss: 0.00000948
Iteration 28/1000 | Loss: 0.00000948
Iteration 29/1000 | Loss: 0.00000948
Iteration 30/1000 | Loss: 0.00000947
Iteration 31/1000 | Loss: 0.00000947
Iteration 32/1000 | Loss: 0.00000947
Iteration 33/1000 | Loss: 0.00000946
Iteration 34/1000 | Loss: 0.00000945
Iteration 35/1000 | Loss: 0.00000944
Iteration 36/1000 | Loss: 0.00000943
Iteration 37/1000 | Loss: 0.00000942
Iteration 38/1000 | Loss: 0.00000942
Iteration 39/1000 | Loss: 0.00000941
Iteration 40/1000 | Loss: 0.00000941
Iteration 41/1000 | Loss: 0.00000941
Iteration 42/1000 | Loss: 0.00000940
Iteration 43/1000 | Loss: 0.00000939
Iteration 44/1000 | Loss: 0.00000939
Iteration 45/1000 | Loss: 0.00000938
Iteration 46/1000 | Loss: 0.00000938
Iteration 47/1000 | Loss: 0.00000937
Iteration 48/1000 | Loss: 0.00000937
Iteration 49/1000 | Loss: 0.00000936
Iteration 50/1000 | Loss: 0.00000936
Iteration 51/1000 | Loss: 0.00000935
Iteration 52/1000 | Loss: 0.00000935
Iteration 53/1000 | Loss: 0.00000935
Iteration 54/1000 | Loss: 0.00000934
Iteration 55/1000 | Loss: 0.00000934
Iteration 56/1000 | Loss: 0.00000934
Iteration 57/1000 | Loss: 0.00000934
Iteration 58/1000 | Loss: 0.00000934
Iteration 59/1000 | Loss: 0.00000934
Iteration 60/1000 | Loss: 0.00000934
Iteration 61/1000 | Loss: 0.00000933
Iteration 62/1000 | Loss: 0.00000933
Iteration 63/1000 | Loss: 0.00000933
Iteration 64/1000 | Loss: 0.00000933
Iteration 65/1000 | Loss: 0.00000933
Iteration 66/1000 | Loss: 0.00000932
Iteration 67/1000 | Loss: 0.00000932
Iteration 68/1000 | Loss: 0.00000932
Iteration 69/1000 | Loss: 0.00000931
Iteration 70/1000 | Loss: 0.00000931
Iteration 71/1000 | Loss: 0.00000931
Iteration 72/1000 | Loss: 0.00000931
Iteration 73/1000 | Loss: 0.00000931
Iteration 74/1000 | Loss: 0.00000931
Iteration 75/1000 | Loss: 0.00000931
Iteration 76/1000 | Loss: 0.00000931
Iteration 77/1000 | Loss: 0.00000931
Iteration 78/1000 | Loss: 0.00000931
Iteration 79/1000 | Loss: 0.00000931
Iteration 80/1000 | Loss: 0.00000931
Iteration 81/1000 | Loss: 0.00000931
Iteration 82/1000 | Loss: 0.00000931
Iteration 83/1000 | Loss: 0.00000931
Iteration 84/1000 | Loss: 0.00000931
Iteration 85/1000 | Loss: 0.00000931
Iteration 86/1000 | Loss: 0.00000931
Iteration 87/1000 | Loss: 0.00000931
Iteration 88/1000 | Loss: 0.00000931
Iteration 89/1000 | Loss: 0.00000931
Iteration 90/1000 | Loss: 0.00000931
Iteration 91/1000 | Loss: 0.00000931
Iteration 92/1000 | Loss: 0.00000931
Iteration 93/1000 | Loss: 0.00000931
Iteration 94/1000 | Loss: 0.00000931
Iteration 95/1000 | Loss: 0.00000931
Iteration 96/1000 | Loss: 0.00000931
Iteration 97/1000 | Loss: 0.00000931
Iteration 98/1000 | Loss: 0.00000931
Iteration 99/1000 | Loss: 0.00000931
Iteration 100/1000 | Loss: 0.00000931
Iteration 101/1000 | Loss: 0.00000931
Iteration 102/1000 | Loss: 0.00000931
Iteration 103/1000 | Loss: 0.00000931
Iteration 104/1000 | Loss: 0.00000931
Iteration 105/1000 | Loss: 0.00000931
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 105. Stopping optimization.
Last 5 losses: [9.309068445872981e-06, 9.309068445872981e-06, 9.309068445872981e-06, 9.309068445872981e-06, 9.309068445872981e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.309068445872981e-06

Optimization complete. Final v2v error: 2.6143949031829834 mm

Highest mean error: 3.129006862640381 mm for frame 74

Lowest mean error: 2.5224976539611816 mm for frame 111

Saving results

Total time: 28.664209842681885
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_022/1001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_022/1001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_022/1001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00870033
Iteration 2/25 | Loss: 0.00110755
Iteration 3/25 | Loss: 0.00083798
Iteration 4/25 | Loss: 0.00079089
Iteration 5/25 | Loss: 0.00077937
Iteration 6/25 | Loss: 0.00077656
Iteration 7/25 | Loss: 0.00077583
Iteration 8/25 | Loss: 0.00077583
Iteration 9/25 | Loss: 0.00077583
Iteration 10/25 | Loss: 0.00077583
Iteration 11/25 | Loss: 0.00077583
Iteration 12/25 | Loss: 0.00077583
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0007758273277431726, 0.0007758273277431726, 0.0007758273277431726, 0.0007758273277431726, 0.0007758273277431726]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007758273277431726

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.62642848
Iteration 2/25 | Loss: 0.00136845
Iteration 3/25 | Loss: 0.00136845
Iteration 4/25 | Loss: 0.00136845
Iteration 5/25 | Loss: 0.00136845
Iteration 6/25 | Loss: 0.00136844
Iteration 7/25 | Loss: 0.00136844
Iteration 8/25 | Loss: 0.00136844
Iteration 9/25 | Loss: 0.00136844
Iteration 10/25 | Loss: 0.00136844
Iteration 11/25 | Loss: 0.00136844
Iteration 12/25 | Loss: 0.00136844
Iteration 13/25 | Loss: 0.00136844
Iteration 14/25 | Loss: 0.00136844
Iteration 15/25 | Loss: 0.00136844
Iteration 16/25 | Loss: 0.00136844
Iteration 17/25 | Loss: 0.00136844
Iteration 18/25 | Loss: 0.00136844
Iteration 19/25 | Loss: 0.00136844
Iteration 20/25 | Loss: 0.00136844
Iteration 21/25 | Loss: 0.00136844
Iteration 22/25 | Loss: 0.00136844
Iteration 23/25 | Loss: 0.00136844
Iteration 24/25 | Loss: 0.00136844
Iteration 25/25 | Loss: 0.00136844

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00136844
Iteration 2/1000 | Loss: 0.00002620
Iteration 3/1000 | Loss: 0.00001759
Iteration 4/1000 | Loss: 0.00001516
Iteration 5/1000 | Loss: 0.00001433
Iteration 6/1000 | Loss: 0.00001337
Iteration 7/1000 | Loss: 0.00001302
Iteration 8/1000 | Loss: 0.00001278
Iteration 9/1000 | Loss: 0.00001258
Iteration 10/1000 | Loss: 0.00001248
Iteration 11/1000 | Loss: 0.00001237
Iteration 12/1000 | Loss: 0.00001231
Iteration 13/1000 | Loss: 0.00001230
Iteration 14/1000 | Loss: 0.00001229
Iteration 15/1000 | Loss: 0.00001228
Iteration 16/1000 | Loss: 0.00001227
Iteration 17/1000 | Loss: 0.00001226
Iteration 18/1000 | Loss: 0.00001226
Iteration 19/1000 | Loss: 0.00001226
Iteration 20/1000 | Loss: 0.00001226
Iteration 21/1000 | Loss: 0.00001225
Iteration 22/1000 | Loss: 0.00001225
Iteration 23/1000 | Loss: 0.00001224
Iteration 24/1000 | Loss: 0.00001224
Iteration 25/1000 | Loss: 0.00001222
Iteration 26/1000 | Loss: 0.00001221
Iteration 27/1000 | Loss: 0.00001221
Iteration 28/1000 | Loss: 0.00001218
Iteration 29/1000 | Loss: 0.00001218
Iteration 30/1000 | Loss: 0.00001216
Iteration 31/1000 | Loss: 0.00001216
Iteration 32/1000 | Loss: 0.00001215
Iteration 33/1000 | Loss: 0.00001215
Iteration 34/1000 | Loss: 0.00001215
Iteration 35/1000 | Loss: 0.00001215
Iteration 36/1000 | Loss: 0.00001214
Iteration 37/1000 | Loss: 0.00001214
Iteration 38/1000 | Loss: 0.00001214
Iteration 39/1000 | Loss: 0.00001213
Iteration 40/1000 | Loss: 0.00001213
Iteration 41/1000 | Loss: 0.00001213
Iteration 42/1000 | Loss: 0.00001212
Iteration 43/1000 | Loss: 0.00001212
Iteration 44/1000 | Loss: 0.00001212
Iteration 45/1000 | Loss: 0.00001212
Iteration 46/1000 | Loss: 0.00001211
Iteration 47/1000 | Loss: 0.00001211
Iteration 48/1000 | Loss: 0.00001211
Iteration 49/1000 | Loss: 0.00001211
Iteration 50/1000 | Loss: 0.00001210
Iteration 51/1000 | Loss: 0.00001210
Iteration 52/1000 | Loss: 0.00001210
Iteration 53/1000 | Loss: 0.00001210
Iteration 54/1000 | Loss: 0.00001210
Iteration 55/1000 | Loss: 0.00001209
Iteration 56/1000 | Loss: 0.00001208
Iteration 57/1000 | Loss: 0.00001208
Iteration 58/1000 | Loss: 0.00001207
Iteration 59/1000 | Loss: 0.00001206
Iteration 60/1000 | Loss: 0.00001206
Iteration 61/1000 | Loss: 0.00001205
Iteration 62/1000 | Loss: 0.00001205
Iteration 63/1000 | Loss: 0.00001204
Iteration 64/1000 | Loss: 0.00001204
Iteration 65/1000 | Loss: 0.00001204
Iteration 66/1000 | Loss: 0.00001204
Iteration 67/1000 | Loss: 0.00001203
Iteration 68/1000 | Loss: 0.00001203
Iteration 69/1000 | Loss: 0.00001203
Iteration 70/1000 | Loss: 0.00001203
Iteration 71/1000 | Loss: 0.00001202
Iteration 72/1000 | Loss: 0.00001202
Iteration 73/1000 | Loss: 0.00001202
Iteration 74/1000 | Loss: 0.00001202
Iteration 75/1000 | Loss: 0.00001202
Iteration 76/1000 | Loss: 0.00001201
Iteration 77/1000 | Loss: 0.00001201
Iteration 78/1000 | Loss: 0.00001201
Iteration 79/1000 | Loss: 0.00001200
Iteration 80/1000 | Loss: 0.00001200
Iteration 81/1000 | Loss: 0.00001200
Iteration 82/1000 | Loss: 0.00001200
Iteration 83/1000 | Loss: 0.00001200
Iteration 84/1000 | Loss: 0.00001200
Iteration 85/1000 | Loss: 0.00001200
Iteration 86/1000 | Loss: 0.00001200
Iteration 87/1000 | Loss: 0.00001200
Iteration 88/1000 | Loss: 0.00001200
Iteration 89/1000 | Loss: 0.00001200
Iteration 90/1000 | Loss: 0.00001200
Iteration 91/1000 | Loss: 0.00001200
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 91. Stopping optimization.
Last 5 losses: [1.199566395371221e-05, 1.199566395371221e-05, 1.199566395371221e-05, 1.199566395371221e-05, 1.199566395371221e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.199566395371221e-05

Optimization complete. Final v2v error: 2.9535977840423584 mm

Highest mean error: 3.34226393699646 mm for frame 114

Lowest mean error: 2.7740025520324707 mm for frame 27

Saving results

Total time: 36.8084602355957
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_022/1002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_022/1002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_022/1002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00379038
Iteration 2/25 | Loss: 0.00086448
Iteration 3/25 | Loss: 0.00075001
Iteration 4/25 | Loss: 0.00073510
Iteration 5/25 | Loss: 0.00073134
Iteration 6/25 | Loss: 0.00072994
Iteration 7/25 | Loss: 0.00072959
Iteration 8/25 | Loss: 0.00072959
Iteration 9/25 | Loss: 0.00072959
Iteration 10/25 | Loss: 0.00072959
Iteration 11/25 | Loss: 0.00072959
Iteration 12/25 | Loss: 0.00072959
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0007295921677723527, 0.0007295921677723527, 0.0007295921677723527, 0.0007295921677723527, 0.0007295921677723527]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007295921677723527

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.56956279
Iteration 2/25 | Loss: 0.00123058
Iteration 3/25 | Loss: 0.00123057
Iteration 4/25 | Loss: 0.00123057
Iteration 5/25 | Loss: 0.00123057
Iteration 6/25 | Loss: 0.00123057
Iteration 7/25 | Loss: 0.00123057
Iteration 8/25 | Loss: 0.00123057
Iteration 9/25 | Loss: 0.00123057
Iteration 10/25 | Loss: 0.00123057
Iteration 11/25 | Loss: 0.00123057
Iteration 12/25 | Loss: 0.00123057
Iteration 13/25 | Loss: 0.00123057
Iteration 14/25 | Loss: 0.00123057
Iteration 15/25 | Loss: 0.00123057
Iteration 16/25 | Loss: 0.00123057
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0012305707205086946, 0.0012305707205086946, 0.0012305707205086946, 0.0012305707205086946, 0.0012305707205086946]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012305707205086946

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00123057
Iteration 2/1000 | Loss: 0.00002287
Iteration 3/1000 | Loss: 0.00001277
Iteration 4/1000 | Loss: 0.00001155
Iteration 5/1000 | Loss: 0.00001106
Iteration 6/1000 | Loss: 0.00001037
Iteration 7/1000 | Loss: 0.00001037
Iteration 8/1000 | Loss: 0.00001014
Iteration 9/1000 | Loss: 0.00000998
Iteration 10/1000 | Loss: 0.00000996
Iteration 11/1000 | Loss: 0.00000992
Iteration 12/1000 | Loss: 0.00000991
Iteration 13/1000 | Loss: 0.00000991
Iteration 14/1000 | Loss: 0.00000991
Iteration 15/1000 | Loss: 0.00000991
Iteration 16/1000 | Loss: 0.00000991
Iteration 17/1000 | Loss: 0.00000991
Iteration 18/1000 | Loss: 0.00000989
Iteration 19/1000 | Loss: 0.00000989
Iteration 20/1000 | Loss: 0.00000987
Iteration 21/1000 | Loss: 0.00000984
Iteration 22/1000 | Loss: 0.00000983
Iteration 23/1000 | Loss: 0.00000983
Iteration 24/1000 | Loss: 0.00000982
Iteration 25/1000 | Loss: 0.00000980
Iteration 26/1000 | Loss: 0.00000979
Iteration 27/1000 | Loss: 0.00000978
Iteration 28/1000 | Loss: 0.00000978
Iteration 29/1000 | Loss: 0.00000976
Iteration 30/1000 | Loss: 0.00000975
Iteration 31/1000 | Loss: 0.00000970
Iteration 32/1000 | Loss: 0.00000970
Iteration 33/1000 | Loss: 0.00000966
Iteration 34/1000 | Loss: 0.00000965
Iteration 35/1000 | Loss: 0.00000965
Iteration 36/1000 | Loss: 0.00000964
Iteration 37/1000 | Loss: 0.00000963
Iteration 38/1000 | Loss: 0.00000963
Iteration 39/1000 | Loss: 0.00000963
Iteration 40/1000 | Loss: 0.00000963
Iteration 41/1000 | Loss: 0.00000963
Iteration 42/1000 | Loss: 0.00000962
Iteration 43/1000 | Loss: 0.00000962
Iteration 44/1000 | Loss: 0.00000962
Iteration 45/1000 | Loss: 0.00000961
Iteration 46/1000 | Loss: 0.00000961
Iteration 47/1000 | Loss: 0.00000960
Iteration 48/1000 | Loss: 0.00000960
Iteration 49/1000 | Loss: 0.00000958
Iteration 50/1000 | Loss: 0.00000958
Iteration 51/1000 | Loss: 0.00000957
Iteration 52/1000 | Loss: 0.00000957
Iteration 53/1000 | Loss: 0.00000957
Iteration 54/1000 | Loss: 0.00000956
Iteration 55/1000 | Loss: 0.00000955
Iteration 56/1000 | Loss: 0.00000955
Iteration 57/1000 | Loss: 0.00000954
Iteration 58/1000 | Loss: 0.00000953
Iteration 59/1000 | Loss: 0.00000953
Iteration 60/1000 | Loss: 0.00000953
Iteration 61/1000 | Loss: 0.00000952
Iteration 62/1000 | Loss: 0.00000952
Iteration 63/1000 | Loss: 0.00000951
Iteration 64/1000 | Loss: 0.00000951
Iteration 65/1000 | Loss: 0.00000951
Iteration 66/1000 | Loss: 0.00000950
Iteration 67/1000 | Loss: 0.00000950
Iteration 68/1000 | Loss: 0.00000950
Iteration 69/1000 | Loss: 0.00000949
Iteration 70/1000 | Loss: 0.00000949
Iteration 71/1000 | Loss: 0.00000949
Iteration 72/1000 | Loss: 0.00000949
Iteration 73/1000 | Loss: 0.00000949
Iteration 74/1000 | Loss: 0.00000949
Iteration 75/1000 | Loss: 0.00000948
Iteration 76/1000 | Loss: 0.00000948
Iteration 77/1000 | Loss: 0.00000948
Iteration 78/1000 | Loss: 0.00000948
Iteration 79/1000 | Loss: 0.00000948
Iteration 80/1000 | Loss: 0.00000948
Iteration 81/1000 | Loss: 0.00000948
Iteration 82/1000 | Loss: 0.00000948
Iteration 83/1000 | Loss: 0.00000948
Iteration 84/1000 | Loss: 0.00000947
Iteration 85/1000 | Loss: 0.00000947
Iteration 86/1000 | Loss: 0.00000947
Iteration 87/1000 | Loss: 0.00000947
Iteration 88/1000 | Loss: 0.00000947
Iteration 89/1000 | Loss: 0.00000947
Iteration 90/1000 | Loss: 0.00000947
Iteration 91/1000 | Loss: 0.00000947
Iteration 92/1000 | Loss: 0.00000947
Iteration 93/1000 | Loss: 0.00000947
Iteration 94/1000 | Loss: 0.00000947
Iteration 95/1000 | Loss: 0.00000947
Iteration 96/1000 | Loss: 0.00000947
Iteration 97/1000 | Loss: 0.00000947
Iteration 98/1000 | Loss: 0.00000947
Iteration 99/1000 | Loss: 0.00000947
Iteration 100/1000 | Loss: 0.00000946
Iteration 101/1000 | Loss: 0.00000946
Iteration 102/1000 | Loss: 0.00000946
Iteration 103/1000 | Loss: 0.00000946
Iteration 104/1000 | Loss: 0.00000945
Iteration 105/1000 | Loss: 0.00000945
Iteration 106/1000 | Loss: 0.00000945
Iteration 107/1000 | Loss: 0.00000945
Iteration 108/1000 | Loss: 0.00000945
Iteration 109/1000 | Loss: 0.00000945
Iteration 110/1000 | Loss: 0.00000945
Iteration 111/1000 | Loss: 0.00000944
Iteration 112/1000 | Loss: 0.00000944
Iteration 113/1000 | Loss: 0.00000944
Iteration 114/1000 | Loss: 0.00000944
Iteration 115/1000 | Loss: 0.00000944
Iteration 116/1000 | Loss: 0.00000944
Iteration 117/1000 | Loss: 0.00000944
Iteration 118/1000 | Loss: 0.00000944
Iteration 119/1000 | Loss: 0.00000944
Iteration 120/1000 | Loss: 0.00000943
Iteration 121/1000 | Loss: 0.00000943
Iteration 122/1000 | Loss: 0.00000943
Iteration 123/1000 | Loss: 0.00000943
Iteration 124/1000 | Loss: 0.00000943
Iteration 125/1000 | Loss: 0.00000943
Iteration 126/1000 | Loss: 0.00000943
Iteration 127/1000 | Loss: 0.00000942
Iteration 128/1000 | Loss: 0.00000942
Iteration 129/1000 | Loss: 0.00000942
Iteration 130/1000 | Loss: 0.00000942
Iteration 131/1000 | Loss: 0.00000942
Iteration 132/1000 | Loss: 0.00000942
Iteration 133/1000 | Loss: 0.00000942
Iteration 134/1000 | Loss: 0.00000942
Iteration 135/1000 | Loss: 0.00000942
Iteration 136/1000 | Loss: 0.00000942
Iteration 137/1000 | Loss: 0.00000942
Iteration 138/1000 | Loss: 0.00000942
Iteration 139/1000 | Loss: 0.00000941
Iteration 140/1000 | Loss: 0.00000941
Iteration 141/1000 | Loss: 0.00000941
Iteration 142/1000 | Loss: 0.00000941
Iteration 143/1000 | Loss: 0.00000941
Iteration 144/1000 | Loss: 0.00000940
Iteration 145/1000 | Loss: 0.00000940
Iteration 146/1000 | Loss: 0.00000940
Iteration 147/1000 | Loss: 0.00000939
Iteration 148/1000 | Loss: 0.00000939
Iteration 149/1000 | Loss: 0.00000939
Iteration 150/1000 | Loss: 0.00000939
Iteration 151/1000 | Loss: 0.00000938
Iteration 152/1000 | Loss: 0.00000938
Iteration 153/1000 | Loss: 0.00000938
Iteration 154/1000 | Loss: 0.00000938
Iteration 155/1000 | Loss: 0.00000937
Iteration 156/1000 | Loss: 0.00000937
Iteration 157/1000 | Loss: 0.00000937
Iteration 158/1000 | Loss: 0.00000937
Iteration 159/1000 | Loss: 0.00000937
Iteration 160/1000 | Loss: 0.00000937
Iteration 161/1000 | Loss: 0.00000936
Iteration 162/1000 | Loss: 0.00000936
Iteration 163/1000 | Loss: 0.00000936
Iteration 164/1000 | Loss: 0.00000936
Iteration 165/1000 | Loss: 0.00000936
Iteration 166/1000 | Loss: 0.00000936
Iteration 167/1000 | Loss: 0.00000936
Iteration 168/1000 | Loss: 0.00000936
Iteration 169/1000 | Loss: 0.00000936
Iteration 170/1000 | Loss: 0.00000936
Iteration 171/1000 | Loss: 0.00000936
Iteration 172/1000 | Loss: 0.00000935
Iteration 173/1000 | Loss: 0.00000935
Iteration 174/1000 | Loss: 0.00000935
Iteration 175/1000 | Loss: 0.00000935
Iteration 176/1000 | Loss: 0.00000935
Iteration 177/1000 | Loss: 0.00000935
Iteration 178/1000 | Loss: 0.00000935
Iteration 179/1000 | Loss: 0.00000935
Iteration 180/1000 | Loss: 0.00000935
Iteration 181/1000 | Loss: 0.00000935
Iteration 182/1000 | Loss: 0.00000935
Iteration 183/1000 | Loss: 0.00000935
Iteration 184/1000 | Loss: 0.00000935
Iteration 185/1000 | Loss: 0.00000935
Iteration 186/1000 | Loss: 0.00000935
Iteration 187/1000 | Loss: 0.00000935
Iteration 188/1000 | Loss: 0.00000935
Iteration 189/1000 | Loss: 0.00000935
Iteration 190/1000 | Loss: 0.00000934
Iteration 191/1000 | Loss: 0.00000934
Iteration 192/1000 | Loss: 0.00000934
Iteration 193/1000 | Loss: 0.00000934
Iteration 194/1000 | Loss: 0.00000934
Iteration 195/1000 | Loss: 0.00000934
Iteration 196/1000 | Loss: 0.00000934
Iteration 197/1000 | Loss: 0.00000934
Iteration 198/1000 | Loss: 0.00000934
Iteration 199/1000 | Loss: 0.00000934
Iteration 200/1000 | Loss: 0.00000934
Iteration 201/1000 | Loss: 0.00000934
Iteration 202/1000 | Loss: 0.00000934
Iteration 203/1000 | Loss: 0.00000934
Iteration 204/1000 | Loss: 0.00000934
Iteration 205/1000 | Loss: 0.00000934
Iteration 206/1000 | Loss: 0.00000934
Iteration 207/1000 | Loss: 0.00000934
Iteration 208/1000 | Loss: 0.00000934
Iteration 209/1000 | Loss: 0.00000934
Iteration 210/1000 | Loss: 0.00000934
Iteration 211/1000 | Loss: 0.00000934
Iteration 212/1000 | Loss: 0.00000933
Iteration 213/1000 | Loss: 0.00000933
Iteration 214/1000 | Loss: 0.00000933
Iteration 215/1000 | Loss: 0.00000933
Iteration 216/1000 | Loss: 0.00000933
Iteration 217/1000 | Loss: 0.00000933
Iteration 218/1000 | Loss: 0.00000933
Iteration 219/1000 | Loss: 0.00000933
Iteration 220/1000 | Loss: 0.00000933
Iteration 221/1000 | Loss: 0.00000933
Iteration 222/1000 | Loss: 0.00000933
Iteration 223/1000 | Loss: 0.00000933
Iteration 224/1000 | Loss: 0.00000933
Iteration 225/1000 | Loss: 0.00000933
Iteration 226/1000 | Loss: 0.00000933
Iteration 227/1000 | Loss: 0.00000933
Iteration 228/1000 | Loss: 0.00000933
Iteration 229/1000 | Loss: 0.00000933
Iteration 230/1000 | Loss: 0.00000933
Iteration 231/1000 | Loss: 0.00000933
Iteration 232/1000 | Loss: 0.00000933
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 232. Stopping optimization.
Last 5 losses: [9.328868145530578e-06, 9.328868145530578e-06, 9.328868145530578e-06, 9.328868145530578e-06, 9.328868145530578e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.328868145530578e-06

Optimization complete. Final v2v error: 2.615563154220581 mm

Highest mean error: 3.0851967334747314 mm for frame 80

Lowest mean error: 2.494776725769043 mm for frame 102

Saving results

Total time: 40.807433128356934
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_022/1031/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_022/1031.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_022/1031
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01025765
Iteration 2/25 | Loss: 0.00137659
Iteration 3/25 | Loss: 0.00102199
Iteration 4/25 | Loss: 0.00096175
Iteration 5/25 | Loss: 0.00094923
Iteration 6/25 | Loss: 0.00094668
Iteration 7/25 | Loss: 0.00094652
Iteration 8/25 | Loss: 0.00094652
Iteration 9/25 | Loss: 0.00094652
Iteration 10/25 | Loss: 0.00094652
Iteration 11/25 | Loss: 0.00094652
Iteration 12/25 | Loss: 0.00094652
Iteration 13/25 | Loss: 0.00094652
Iteration 14/25 | Loss: 0.00094652
Iteration 15/25 | Loss: 0.00094652
Iteration 16/25 | Loss: 0.00094652
Iteration 17/25 | Loss: 0.00094652
Iteration 18/25 | Loss: 0.00094652
Iteration 19/25 | Loss: 0.00094652
Iteration 20/25 | Loss: 0.00094652
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0009465173934586346, 0.0009465173934586346, 0.0009465173934586346, 0.0009465173934586346, 0.0009465173934586346]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009465173934586346

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.59197235
Iteration 2/25 | Loss: 0.00155609
Iteration 3/25 | Loss: 0.00155601
Iteration 4/25 | Loss: 0.00155601
Iteration 5/25 | Loss: 0.00155601
Iteration 6/25 | Loss: 0.00155601
Iteration 7/25 | Loss: 0.00155601
Iteration 8/25 | Loss: 0.00155601
Iteration 9/25 | Loss: 0.00155601
Iteration 10/25 | Loss: 0.00155601
Iteration 11/25 | Loss: 0.00155601
Iteration 12/25 | Loss: 0.00155601
Iteration 13/25 | Loss: 0.00155601
Iteration 14/25 | Loss: 0.00155601
Iteration 15/25 | Loss: 0.00155601
Iteration 16/25 | Loss: 0.00155601
Iteration 17/25 | Loss: 0.00155601
Iteration 18/25 | Loss: 0.00155601
Iteration 19/25 | Loss: 0.00155601
Iteration 20/25 | Loss: 0.00155601
Iteration 21/25 | Loss: 0.00155601
Iteration 22/25 | Loss: 0.00155601
Iteration 23/25 | Loss: 0.00155601
Iteration 24/25 | Loss: 0.00155601
Iteration 25/25 | Loss: 0.00155601
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0015560067258775234, 0.0015560067258775234, 0.0015560067258775234, 0.0015560067258775234, 0.0015560067258775234]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015560067258775234

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00155601
Iteration 2/1000 | Loss: 0.00006052
Iteration 3/1000 | Loss: 0.00003975
Iteration 4/1000 | Loss: 0.00003472
Iteration 5/1000 | Loss: 0.00003274
Iteration 6/1000 | Loss: 0.00003165
Iteration 7/1000 | Loss: 0.00003085
Iteration 8/1000 | Loss: 0.00003036
Iteration 9/1000 | Loss: 0.00002982
Iteration 10/1000 | Loss: 0.00002947
Iteration 11/1000 | Loss: 0.00002923
Iteration 12/1000 | Loss: 0.00002907
Iteration 13/1000 | Loss: 0.00002891
Iteration 14/1000 | Loss: 0.00002886
Iteration 15/1000 | Loss: 0.00002879
Iteration 16/1000 | Loss: 0.00002868
Iteration 17/1000 | Loss: 0.00002864
Iteration 18/1000 | Loss: 0.00002862
Iteration 19/1000 | Loss: 0.00002861
Iteration 20/1000 | Loss: 0.00002858
Iteration 21/1000 | Loss: 0.00002856
Iteration 22/1000 | Loss: 0.00002856
Iteration 23/1000 | Loss: 0.00002851
Iteration 24/1000 | Loss: 0.00002851
Iteration 25/1000 | Loss: 0.00002850
Iteration 26/1000 | Loss: 0.00002848
Iteration 27/1000 | Loss: 0.00002848
Iteration 28/1000 | Loss: 0.00002848
Iteration 29/1000 | Loss: 0.00002847
Iteration 30/1000 | Loss: 0.00002847
Iteration 31/1000 | Loss: 0.00002846
Iteration 32/1000 | Loss: 0.00002846
Iteration 33/1000 | Loss: 0.00002846
Iteration 34/1000 | Loss: 0.00002846
Iteration 35/1000 | Loss: 0.00002846
Iteration 36/1000 | Loss: 0.00002846
Iteration 37/1000 | Loss: 0.00002846
Iteration 38/1000 | Loss: 0.00002846
Iteration 39/1000 | Loss: 0.00002846
Iteration 40/1000 | Loss: 0.00002846
Iteration 41/1000 | Loss: 0.00002845
Iteration 42/1000 | Loss: 0.00002845
Iteration 43/1000 | Loss: 0.00002845
Iteration 44/1000 | Loss: 0.00002843
Iteration 45/1000 | Loss: 0.00002843
Iteration 46/1000 | Loss: 0.00002843
Iteration 47/1000 | Loss: 0.00002842
Iteration 48/1000 | Loss: 0.00002842
Iteration 49/1000 | Loss: 0.00002841
Iteration 50/1000 | Loss: 0.00002841
Iteration 51/1000 | Loss: 0.00002841
Iteration 52/1000 | Loss: 0.00002840
Iteration 53/1000 | Loss: 0.00002839
Iteration 54/1000 | Loss: 0.00002838
Iteration 55/1000 | Loss: 0.00002837
Iteration 56/1000 | Loss: 0.00002837
Iteration 57/1000 | Loss: 0.00002836
Iteration 58/1000 | Loss: 0.00002836
Iteration 59/1000 | Loss: 0.00002836
Iteration 60/1000 | Loss: 0.00002835
Iteration 61/1000 | Loss: 0.00002834
Iteration 62/1000 | Loss: 0.00002833
Iteration 63/1000 | Loss: 0.00002832
Iteration 64/1000 | Loss: 0.00002830
Iteration 65/1000 | Loss: 0.00002829
Iteration 66/1000 | Loss: 0.00002828
Iteration 67/1000 | Loss: 0.00002828
Iteration 68/1000 | Loss: 0.00002827
Iteration 69/1000 | Loss: 0.00002827
Iteration 70/1000 | Loss: 0.00002826
Iteration 71/1000 | Loss: 0.00002826
Iteration 72/1000 | Loss: 0.00002826
Iteration 73/1000 | Loss: 0.00002826
Iteration 74/1000 | Loss: 0.00002826
Iteration 75/1000 | Loss: 0.00002826
Iteration 76/1000 | Loss: 0.00002825
Iteration 77/1000 | Loss: 0.00002825
Iteration 78/1000 | Loss: 0.00002825
Iteration 79/1000 | Loss: 0.00002824
Iteration 80/1000 | Loss: 0.00002823
Iteration 81/1000 | Loss: 0.00002823
Iteration 82/1000 | Loss: 0.00002823
Iteration 83/1000 | Loss: 0.00002823
Iteration 84/1000 | Loss: 0.00002822
Iteration 85/1000 | Loss: 0.00002822
Iteration 86/1000 | Loss: 0.00002820
Iteration 87/1000 | Loss: 0.00002820
Iteration 88/1000 | Loss: 0.00002820
Iteration 89/1000 | Loss: 0.00002819
Iteration 90/1000 | Loss: 0.00002819
Iteration 91/1000 | Loss: 0.00002819
Iteration 92/1000 | Loss: 0.00002818
Iteration 93/1000 | Loss: 0.00002818
Iteration 94/1000 | Loss: 0.00002818
Iteration 95/1000 | Loss: 0.00002817
Iteration 96/1000 | Loss: 0.00002817
Iteration 97/1000 | Loss: 0.00002817
Iteration 98/1000 | Loss: 0.00002816
Iteration 99/1000 | Loss: 0.00002816
Iteration 100/1000 | Loss: 0.00002816
Iteration 101/1000 | Loss: 0.00002815
Iteration 102/1000 | Loss: 0.00002815
Iteration 103/1000 | Loss: 0.00002815
Iteration 104/1000 | Loss: 0.00002815
Iteration 105/1000 | Loss: 0.00002814
Iteration 106/1000 | Loss: 0.00002814
Iteration 107/1000 | Loss: 0.00002814
Iteration 108/1000 | Loss: 0.00002814
Iteration 109/1000 | Loss: 0.00002813
Iteration 110/1000 | Loss: 0.00002813
Iteration 111/1000 | Loss: 0.00002813
Iteration 112/1000 | Loss: 0.00002813
Iteration 113/1000 | Loss: 0.00002812
Iteration 114/1000 | Loss: 0.00002812
Iteration 115/1000 | Loss: 0.00002812
Iteration 116/1000 | Loss: 0.00002812
Iteration 117/1000 | Loss: 0.00002811
Iteration 118/1000 | Loss: 0.00002811
Iteration 119/1000 | Loss: 0.00002811
Iteration 120/1000 | Loss: 0.00002811
Iteration 121/1000 | Loss: 0.00002811
Iteration 122/1000 | Loss: 0.00002810
Iteration 123/1000 | Loss: 0.00002810
Iteration 124/1000 | Loss: 0.00002810
Iteration 125/1000 | Loss: 0.00002810
Iteration 126/1000 | Loss: 0.00002810
Iteration 127/1000 | Loss: 0.00002810
Iteration 128/1000 | Loss: 0.00002810
Iteration 129/1000 | Loss: 0.00002810
Iteration 130/1000 | Loss: 0.00002810
Iteration 131/1000 | Loss: 0.00002809
Iteration 132/1000 | Loss: 0.00002809
Iteration 133/1000 | Loss: 0.00002809
Iteration 134/1000 | Loss: 0.00002809
Iteration 135/1000 | Loss: 0.00002808
Iteration 136/1000 | Loss: 0.00002808
Iteration 137/1000 | Loss: 0.00002808
Iteration 138/1000 | Loss: 0.00002808
Iteration 139/1000 | Loss: 0.00002808
Iteration 140/1000 | Loss: 0.00002808
Iteration 141/1000 | Loss: 0.00002807
Iteration 142/1000 | Loss: 0.00002807
Iteration 143/1000 | Loss: 0.00002807
Iteration 144/1000 | Loss: 0.00002807
Iteration 145/1000 | Loss: 0.00002807
Iteration 146/1000 | Loss: 0.00002807
Iteration 147/1000 | Loss: 0.00002807
Iteration 148/1000 | Loss: 0.00002807
Iteration 149/1000 | Loss: 0.00002807
Iteration 150/1000 | Loss: 0.00002806
Iteration 151/1000 | Loss: 0.00002806
Iteration 152/1000 | Loss: 0.00002806
Iteration 153/1000 | Loss: 0.00002806
Iteration 154/1000 | Loss: 0.00002806
Iteration 155/1000 | Loss: 0.00002806
Iteration 156/1000 | Loss: 0.00002806
Iteration 157/1000 | Loss: 0.00002806
Iteration 158/1000 | Loss: 0.00002806
Iteration 159/1000 | Loss: 0.00002806
Iteration 160/1000 | Loss: 0.00002805
Iteration 161/1000 | Loss: 0.00002805
Iteration 162/1000 | Loss: 0.00002805
Iteration 163/1000 | Loss: 0.00002805
Iteration 164/1000 | Loss: 0.00002805
Iteration 165/1000 | Loss: 0.00002805
Iteration 166/1000 | Loss: 0.00002805
Iteration 167/1000 | Loss: 0.00002805
Iteration 168/1000 | Loss: 0.00002805
Iteration 169/1000 | Loss: 0.00002805
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 169. Stopping optimization.
Last 5 losses: [2.805260373861529e-05, 2.805260373861529e-05, 2.805260373861529e-05, 2.805260373861529e-05, 2.805260373861529e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.805260373861529e-05

Optimization complete. Final v2v error: 4.212271213531494 mm

Highest mean error: 6.205833435058594 mm for frame 48

Lowest mean error: 3.06398344039917 mm for frame 107

Saving results

Total time: 53.311015367507935
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_022/1039/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_022/1039.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_022/1039
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00866220
Iteration 2/25 | Loss: 0.00096133
Iteration 3/25 | Loss: 0.00079128
Iteration 4/25 | Loss: 0.00076779
Iteration 5/25 | Loss: 0.00076069
Iteration 6/25 | Loss: 0.00075866
Iteration 7/25 | Loss: 0.00075848
Iteration 8/25 | Loss: 0.00075848
Iteration 9/25 | Loss: 0.00075848
Iteration 10/25 | Loss: 0.00075848
Iteration 11/25 | Loss: 0.00075848
Iteration 12/25 | Loss: 0.00075848
Iteration 13/25 | Loss: 0.00075848
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0007584753329865634, 0.0007584753329865634, 0.0007584753329865634, 0.0007584753329865634, 0.0007584753329865634]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007584753329865634

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.59236133
Iteration 2/25 | Loss: 0.00123625
Iteration 3/25 | Loss: 0.00123624
Iteration 4/25 | Loss: 0.00123624
Iteration 5/25 | Loss: 0.00123624
Iteration 6/25 | Loss: 0.00123624
Iteration 7/25 | Loss: 0.00123624
Iteration 8/25 | Loss: 0.00123624
Iteration 9/25 | Loss: 0.00123624
Iteration 10/25 | Loss: 0.00123624
Iteration 11/25 | Loss: 0.00123624
Iteration 12/25 | Loss: 0.00123624
Iteration 13/25 | Loss: 0.00123624
Iteration 14/25 | Loss: 0.00123624
Iteration 15/25 | Loss: 0.00123624
Iteration 16/25 | Loss: 0.00123624
Iteration 17/25 | Loss: 0.00123624
Iteration 18/25 | Loss: 0.00123624
Iteration 19/25 | Loss: 0.00123624
Iteration 20/25 | Loss: 0.00123624
Iteration 21/25 | Loss: 0.00123624
Iteration 22/25 | Loss: 0.00123624
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0012362358393147588, 0.0012362358393147588, 0.0012362358393147588, 0.0012362358393147588, 0.0012362358393147588]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012362358393147588

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00123624
Iteration 2/1000 | Loss: 0.00002652
Iteration 3/1000 | Loss: 0.00001904
Iteration 4/1000 | Loss: 0.00001693
Iteration 5/1000 | Loss: 0.00001570
Iteration 6/1000 | Loss: 0.00001507
Iteration 7/1000 | Loss: 0.00001463
Iteration 8/1000 | Loss: 0.00001432
Iteration 9/1000 | Loss: 0.00001430
Iteration 10/1000 | Loss: 0.00001410
Iteration 11/1000 | Loss: 0.00001391
Iteration 12/1000 | Loss: 0.00001367
Iteration 13/1000 | Loss: 0.00001349
Iteration 14/1000 | Loss: 0.00001343
Iteration 15/1000 | Loss: 0.00001335
Iteration 16/1000 | Loss: 0.00001335
Iteration 17/1000 | Loss: 0.00001334
Iteration 18/1000 | Loss: 0.00001333
Iteration 19/1000 | Loss: 0.00001332
Iteration 20/1000 | Loss: 0.00001332
Iteration 21/1000 | Loss: 0.00001332
Iteration 22/1000 | Loss: 0.00001332
Iteration 23/1000 | Loss: 0.00001331
Iteration 24/1000 | Loss: 0.00001331
Iteration 25/1000 | Loss: 0.00001331
Iteration 26/1000 | Loss: 0.00001331
Iteration 27/1000 | Loss: 0.00001330
Iteration 28/1000 | Loss: 0.00001330
Iteration 29/1000 | Loss: 0.00001330
Iteration 30/1000 | Loss: 0.00001329
Iteration 31/1000 | Loss: 0.00001328
Iteration 32/1000 | Loss: 0.00001328
Iteration 33/1000 | Loss: 0.00001328
Iteration 34/1000 | Loss: 0.00001328
Iteration 35/1000 | Loss: 0.00001327
Iteration 36/1000 | Loss: 0.00001324
Iteration 37/1000 | Loss: 0.00001323
Iteration 38/1000 | Loss: 0.00001322
Iteration 39/1000 | Loss: 0.00001321
Iteration 40/1000 | Loss: 0.00001318
Iteration 41/1000 | Loss: 0.00001318
Iteration 42/1000 | Loss: 0.00001318
Iteration 43/1000 | Loss: 0.00001318
Iteration 44/1000 | Loss: 0.00001317
Iteration 45/1000 | Loss: 0.00001317
Iteration 46/1000 | Loss: 0.00001315
Iteration 47/1000 | Loss: 0.00001315
Iteration 48/1000 | Loss: 0.00001315
Iteration 49/1000 | Loss: 0.00001315
Iteration 50/1000 | Loss: 0.00001315
Iteration 51/1000 | Loss: 0.00001315
Iteration 52/1000 | Loss: 0.00001315
Iteration 53/1000 | Loss: 0.00001315
Iteration 54/1000 | Loss: 0.00001315
Iteration 55/1000 | Loss: 0.00001315
Iteration 56/1000 | Loss: 0.00001315
Iteration 57/1000 | Loss: 0.00001315
Iteration 58/1000 | Loss: 0.00001315
Iteration 59/1000 | Loss: 0.00001315
Iteration 60/1000 | Loss: 0.00001315
Iteration 61/1000 | Loss: 0.00001314
Iteration 62/1000 | Loss: 0.00001314
Iteration 63/1000 | Loss: 0.00001314
Iteration 64/1000 | Loss: 0.00001314
Iteration 65/1000 | Loss: 0.00001313
Iteration 66/1000 | Loss: 0.00001313
Iteration 67/1000 | Loss: 0.00001313
Iteration 68/1000 | Loss: 0.00001312
Iteration 69/1000 | Loss: 0.00001312
Iteration 70/1000 | Loss: 0.00001312
Iteration 71/1000 | Loss: 0.00001312
Iteration 72/1000 | Loss: 0.00001312
Iteration 73/1000 | Loss: 0.00001312
Iteration 74/1000 | Loss: 0.00001312
Iteration 75/1000 | Loss: 0.00001312
Iteration 76/1000 | Loss: 0.00001312
Iteration 77/1000 | Loss: 0.00001312
Iteration 78/1000 | Loss: 0.00001311
Iteration 79/1000 | Loss: 0.00001311
Iteration 80/1000 | Loss: 0.00001311
Iteration 81/1000 | Loss: 0.00001311
Iteration 82/1000 | Loss: 0.00001311
Iteration 83/1000 | Loss: 0.00001311
Iteration 84/1000 | Loss: 0.00001311
Iteration 85/1000 | Loss: 0.00001310
Iteration 86/1000 | Loss: 0.00001310
Iteration 87/1000 | Loss: 0.00001310
Iteration 88/1000 | Loss: 0.00001310
Iteration 89/1000 | Loss: 0.00001310
Iteration 90/1000 | Loss: 0.00001310
Iteration 91/1000 | Loss: 0.00001310
Iteration 92/1000 | Loss: 0.00001310
Iteration 93/1000 | Loss: 0.00001310
Iteration 94/1000 | Loss: 0.00001309
Iteration 95/1000 | Loss: 0.00001309
Iteration 96/1000 | Loss: 0.00001309
Iteration 97/1000 | Loss: 0.00001309
Iteration 98/1000 | Loss: 0.00001309
Iteration 99/1000 | Loss: 0.00001309
Iteration 100/1000 | Loss: 0.00001309
Iteration 101/1000 | Loss: 0.00001309
Iteration 102/1000 | Loss: 0.00001309
Iteration 103/1000 | Loss: 0.00001308
Iteration 104/1000 | Loss: 0.00001308
Iteration 105/1000 | Loss: 0.00001308
Iteration 106/1000 | Loss: 0.00001308
Iteration 107/1000 | Loss: 0.00001308
Iteration 108/1000 | Loss: 0.00001308
Iteration 109/1000 | Loss: 0.00001308
Iteration 110/1000 | Loss: 0.00001308
Iteration 111/1000 | Loss: 0.00001308
Iteration 112/1000 | Loss: 0.00001308
Iteration 113/1000 | Loss: 0.00001308
Iteration 114/1000 | Loss: 0.00001308
Iteration 115/1000 | Loss: 0.00001308
Iteration 116/1000 | Loss: 0.00001308
Iteration 117/1000 | Loss: 0.00001308
Iteration 118/1000 | Loss: 0.00001308
Iteration 119/1000 | Loss: 0.00001308
Iteration 120/1000 | Loss: 0.00001308
Iteration 121/1000 | Loss: 0.00001308
Iteration 122/1000 | Loss: 0.00001308
Iteration 123/1000 | Loss: 0.00001308
Iteration 124/1000 | Loss: 0.00001308
Iteration 125/1000 | Loss: 0.00001308
Iteration 126/1000 | Loss: 0.00001308
Iteration 127/1000 | Loss: 0.00001308
Iteration 128/1000 | Loss: 0.00001308
Iteration 129/1000 | Loss: 0.00001307
Iteration 130/1000 | Loss: 0.00001307
Iteration 131/1000 | Loss: 0.00001307
Iteration 132/1000 | Loss: 0.00001307
Iteration 133/1000 | Loss: 0.00001307
Iteration 134/1000 | Loss: 0.00001307
Iteration 135/1000 | Loss: 0.00001307
Iteration 136/1000 | Loss: 0.00001307
Iteration 137/1000 | Loss: 0.00001307
Iteration 138/1000 | Loss: 0.00001307
Iteration 139/1000 | Loss: 0.00001307
Iteration 140/1000 | Loss: 0.00001307
Iteration 141/1000 | Loss: 0.00001307
Iteration 142/1000 | Loss: 0.00001307
Iteration 143/1000 | Loss: 0.00001307
Iteration 144/1000 | Loss: 0.00001307
Iteration 145/1000 | Loss: 0.00001307
Iteration 146/1000 | Loss: 0.00001307
Iteration 147/1000 | Loss: 0.00001307
Iteration 148/1000 | Loss: 0.00001307
Iteration 149/1000 | Loss: 0.00001307
Iteration 150/1000 | Loss: 0.00001307
Iteration 151/1000 | Loss: 0.00001307
Iteration 152/1000 | Loss: 0.00001307
Iteration 153/1000 | Loss: 0.00001307
Iteration 154/1000 | Loss: 0.00001307
Iteration 155/1000 | Loss: 0.00001307
Iteration 156/1000 | Loss: 0.00001307
Iteration 157/1000 | Loss: 0.00001307
Iteration 158/1000 | Loss: 0.00001307
Iteration 159/1000 | Loss: 0.00001307
Iteration 160/1000 | Loss: 0.00001307
Iteration 161/1000 | Loss: 0.00001307
Iteration 162/1000 | Loss: 0.00001307
Iteration 163/1000 | Loss: 0.00001307
Iteration 164/1000 | Loss: 0.00001307
Iteration 165/1000 | Loss: 0.00001307
Iteration 166/1000 | Loss: 0.00001307
Iteration 167/1000 | Loss: 0.00001307
Iteration 168/1000 | Loss: 0.00001307
Iteration 169/1000 | Loss: 0.00001307
Iteration 170/1000 | Loss: 0.00001307
Iteration 171/1000 | Loss: 0.00001307
Iteration 172/1000 | Loss: 0.00001307
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 172. Stopping optimization.
Last 5 losses: [1.307392813032493e-05, 1.307392813032493e-05, 1.307392813032493e-05, 1.307392813032493e-05, 1.307392813032493e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.307392813032493e-05

Optimization complete. Final v2v error: 3.032881259918213 mm

Highest mean error: 3.2389984130859375 mm for frame 29

Lowest mean error: 2.8962361812591553 mm for frame 79

Saving results

Total time: 37.97258114814758
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_022/1092/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_022/1092.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_022/1092
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01081677
Iteration 2/25 | Loss: 0.00396648
Iteration 3/25 | Loss: 0.00265325
Iteration 4/25 | Loss: 0.00172252
Iteration 5/25 | Loss: 0.00148662
Iteration 6/25 | Loss: 0.00130603
Iteration 7/25 | Loss: 0.00115203
Iteration 8/25 | Loss: 0.00106806
Iteration 9/25 | Loss: 0.00102136
Iteration 10/25 | Loss: 0.00099815
Iteration 11/25 | Loss: 0.00098857
Iteration 12/25 | Loss: 0.00098340
Iteration 13/25 | Loss: 0.00098369
Iteration 14/25 | Loss: 0.00097971
Iteration 15/25 | Loss: 0.00097701
Iteration 16/25 | Loss: 0.00098285
Iteration 17/25 | Loss: 0.00098187
Iteration 18/25 | Loss: 0.00097536
Iteration 19/25 | Loss: 0.00097213
Iteration 20/25 | Loss: 0.00096784
Iteration 21/25 | Loss: 0.00096457
Iteration 22/25 | Loss: 0.00096368
Iteration 23/25 | Loss: 0.00096577
Iteration 24/25 | Loss: 0.00096342
Iteration 25/25 | Loss: 0.00096113

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.64284444
Iteration 2/25 | Loss: 0.00278607
Iteration 3/25 | Loss: 0.00269289
Iteration 4/25 | Loss: 0.00269289
Iteration 5/25 | Loss: 0.00269289
Iteration 6/25 | Loss: 0.00269289
Iteration 7/25 | Loss: 0.00269289
Iteration 8/25 | Loss: 0.00269289
Iteration 9/25 | Loss: 0.00269289
Iteration 10/25 | Loss: 0.00269289
Iteration 11/25 | Loss: 0.00269289
Iteration 12/25 | Loss: 0.00269289
Iteration 13/25 | Loss: 0.00269289
Iteration 14/25 | Loss: 0.00269289
Iteration 15/25 | Loss: 0.00269289
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0026928854640573263, 0.0026928854640573263, 0.0026928854640573263, 0.0026928854640573263, 0.0026928854640573263]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0026928854640573263

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00269289
Iteration 2/1000 | Loss: 0.00023564
Iteration 3/1000 | Loss: 0.00035905
Iteration 4/1000 | Loss: 0.00105703
Iteration 5/1000 | Loss: 0.00009471
Iteration 6/1000 | Loss: 0.00007116
Iteration 7/1000 | Loss: 0.00037970
Iteration 8/1000 | Loss: 0.00039682
Iteration 9/1000 | Loss: 0.00092093
Iteration 10/1000 | Loss: 0.00045421
Iteration 11/1000 | Loss: 0.00134257
Iteration 12/1000 | Loss: 0.00009426
Iteration 13/1000 | Loss: 0.00007229
Iteration 14/1000 | Loss: 0.00042315
Iteration 15/1000 | Loss: 0.00041615
Iteration 16/1000 | Loss: 0.00037013
Iteration 17/1000 | Loss: 0.00029080
Iteration 18/1000 | Loss: 0.00043029
Iteration 19/1000 | Loss: 0.00005777
Iteration 20/1000 | Loss: 0.00005117
Iteration 21/1000 | Loss: 0.00004770
Iteration 22/1000 | Loss: 0.00043720
Iteration 23/1000 | Loss: 0.00006939
Iteration 24/1000 | Loss: 0.00005430
Iteration 25/1000 | Loss: 0.00004684
Iteration 26/1000 | Loss: 0.00004527
Iteration 27/1000 | Loss: 0.00143959
Iteration 28/1000 | Loss: 0.00007703
Iteration 29/1000 | Loss: 0.00005172
Iteration 30/1000 | Loss: 0.00004712
Iteration 31/1000 | Loss: 0.00071024
Iteration 32/1000 | Loss: 0.00105429
Iteration 33/1000 | Loss: 0.00006750
Iteration 34/1000 | Loss: 0.00004675
Iteration 35/1000 | Loss: 0.00051013
Iteration 36/1000 | Loss: 0.00227033
Iteration 37/1000 | Loss: 0.00009997
Iteration 38/1000 | Loss: 0.00005624
Iteration 39/1000 | Loss: 0.00004318
Iteration 40/1000 | Loss: 0.00003945
Iteration 41/1000 | Loss: 0.00003733
Iteration 42/1000 | Loss: 0.00003600
Iteration 43/1000 | Loss: 0.00003526
Iteration 44/1000 | Loss: 0.00003459
Iteration 45/1000 | Loss: 0.00003394
Iteration 46/1000 | Loss: 0.00003357
Iteration 47/1000 | Loss: 0.00003316
Iteration 48/1000 | Loss: 0.00003280
Iteration 49/1000 | Loss: 0.00003274
Iteration 50/1000 | Loss: 0.00003272
Iteration 51/1000 | Loss: 0.00003271
Iteration 52/1000 | Loss: 0.00003268
Iteration 53/1000 | Loss: 0.00003268
Iteration 54/1000 | Loss: 0.00003266
Iteration 55/1000 | Loss: 0.00003263
Iteration 56/1000 | Loss: 0.00003263
Iteration 57/1000 | Loss: 0.00003261
Iteration 58/1000 | Loss: 0.00003261
Iteration 59/1000 | Loss: 0.00003260
Iteration 60/1000 | Loss: 0.00003259
Iteration 61/1000 | Loss: 0.00003256
Iteration 62/1000 | Loss: 0.00003250
Iteration 63/1000 | Loss: 0.00003246
Iteration 64/1000 | Loss: 0.00003232
Iteration 65/1000 | Loss: 0.00102343
Iteration 66/1000 | Loss: 0.00003556
Iteration 67/1000 | Loss: 0.00003215
Iteration 68/1000 | Loss: 0.00003123
Iteration 69/1000 | Loss: 0.00003050
Iteration 70/1000 | Loss: 0.00002989
Iteration 71/1000 | Loss: 0.00002944
Iteration 72/1000 | Loss: 0.00002914
Iteration 73/1000 | Loss: 0.00002909
Iteration 74/1000 | Loss: 0.00002899
Iteration 75/1000 | Loss: 0.00002883
Iteration 76/1000 | Loss: 0.00002878
Iteration 77/1000 | Loss: 0.00002876
Iteration 78/1000 | Loss: 0.00002876
Iteration 79/1000 | Loss: 0.00002875
Iteration 80/1000 | Loss: 0.00002874
Iteration 81/1000 | Loss: 0.00002873
Iteration 82/1000 | Loss: 0.00002872
Iteration 83/1000 | Loss: 0.00002871
Iteration 84/1000 | Loss: 0.00002870
Iteration 85/1000 | Loss: 0.00002868
Iteration 86/1000 | Loss: 0.00002868
Iteration 87/1000 | Loss: 0.00002868
Iteration 88/1000 | Loss: 0.00002867
Iteration 89/1000 | Loss: 0.00002867
Iteration 90/1000 | Loss: 0.00002866
Iteration 91/1000 | Loss: 0.00002866
Iteration 92/1000 | Loss: 0.00002866
Iteration 93/1000 | Loss: 0.00002865
Iteration 94/1000 | Loss: 0.00002865
Iteration 95/1000 | Loss: 0.00002864
Iteration 96/1000 | Loss: 0.00002864
Iteration 97/1000 | Loss: 0.00002864
Iteration 98/1000 | Loss: 0.00002863
Iteration 99/1000 | Loss: 0.00002863
Iteration 100/1000 | Loss: 0.00002863
Iteration 101/1000 | Loss: 0.00002863
Iteration 102/1000 | Loss: 0.00002863
Iteration 103/1000 | Loss: 0.00002862
Iteration 104/1000 | Loss: 0.00002862
Iteration 105/1000 | Loss: 0.00002862
Iteration 106/1000 | Loss: 0.00002862
Iteration 107/1000 | Loss: 0.00002862
Iteration 108/1000 | Loss: 0.00002862
Iteration 109/1000 | Loss: 0.00002862
Iteration 110/1000 | Loss: 0.00002861
Iteration 111/1000 | Loss: 0.00002861
Iteration 112/1000 | Loss: 0.00002861
Iteration 113/1000 | Loss: 0.00002861
Iteration 114/1000 | Loss: 0.00002861
Iteration 115/1000 | Loss: 0.00002861
Iteration 116/1000 | Loss: 0.00002861
Iteration 117/1000 | Loss: 0.00002861
Iteration 118/1000 | Loss: 0.00002861
Iteration 119/1000 | Loss: 0.00002861
Iteration 120/1000 | Loss: 0.00002861
Iteration 121/1000 | Loss: 0.00002860
Iteration 122/1000 | Loss: 0.00002860
Iteration 123/1000 | Loss: 0.00002860
Iteration 124/1000 | Loss: 0.00002860
Iteration 125/1000 | Loss: 0.00002859
Iteration 126/1000 | Loss: 0.00002859
Iteration 127/1000 | Loss: 0.00002859
Iteration 128/1000 | Loss: 0.00002859
Iteration 129/1000 | Loss: 0.00002859
Iteration 130/1000 | Loss: 0.00002858
Iteration 131/1000 | Loss: 0.00002858
Iteration 132/1000 | Loss: 0.00002858
Iteration 133/1000 | Loss: 0.00002858
Iteration 134/1000 | Loss: 0.00002858
Iteration 135/1000 | Loss: 0.00002858
Iteration 136/1000 | Loss: 0.00002858
Iteration 137/1000 | Loss: 0.00002857
Iteration 138/1000 | Loss: 0.00002857
Iteration 139/1000 | Loss: 0.00002857
Iteration 140/1000 | Loss: 0.00002857
Iteration 141/1000 | Loss: 0.00002857
Iteration 142/1000 | Loss: 0.00002857
Iteration 143/1000 | Loss: 0.00002857
Iteration 144/1000 | Loss: 0.00002857
Iteration 145/1000 | Loss: 0.00002857
Iteration 146/1000 | Loss: 0.00002857
Iteration 147/1000 | Loss: 0.00002857
Iteration 148/1000 | Loss: 0.00002856
Iteration 149/1000 | Loss: 0.00002856
Iteration 150/1000 | Loss: 0.00002856
Iteration 151/1000 | Loss: 0.00002856
Iteration 152/1000 | Loss: 0.00002856
Iteration 153/1000 | Loss: 0.00002856
Iteration 154/1000 | Loss: 0.00002856
Iteration 155/1000 | Loss: 0.00002856
Iteration 156/1000 | Loss: 0.00002856
Iteration 157/1000 | Loss: 0.00002856
Iteration 158/1000 | Loss: 0.00002856
Iteration 159/1000 | Loss: 0.00002856
Iteration 160/1000 | Loss: 0.00002856
Iteration 161/1000 | Loss: 0.00002856
Iteration 162/1000 | Loss: 0.00002856
Iteration 163/1000 | Loss: 0.00002856
Iteration 164/1000 | Loss: 0.00002856
Iteration 165/1000 | Loss: 0.00002856
Iteration 166/1000 | Loss: 0.00002856
Iteration 167/1000 | Loss: 0.00002856
Iteration 168/1000 | Loss: 0.00002856
Iteration 169/1000 | Loss: 0.00002855
Iteration 170/1000 | Loss: 0.00002855
Iteration 171/1000 | Loss: 0.00002855
Iteration 172/1000 | Loss: 0.00002855
Iteration 173/1000 | Loss: 0.00002855
Iteration 174/1000 | Loss: 0.00002855
Iteration 175/1000 | Loss: 0.00002855
Iteration 176/1000 | Loss: 0.00002855
Iteration 177/1000 | Loss: 0.00002855
Iteration 178/1000 | Loss: 0.00002855
Iteration 179/1000 | Loss: 0.00002855
Iteration 180/1000 | Loss: 0.00002855
Iteration 181/1000 | Loss: 0.00002855
Iteration 182/1000 | Loss: 0.00002855
Iteration 183/1000 | Loss: 0.00002855
Iteration 184/1000 | Loss: 0.00002855
Iteration 185/1000 | Loss: 0.00002855
Iteration 186/1000 | Loss: 0.00002855
Iteration 187/1000 | Loss: 0.00002855
Iteration 188/1000 | Loss: 0.00002855
Iteration 189/1000 | Loss: 0.00002855
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 189. Stopping optimization.
Last 5 losses: [2.8553353331517428e-05, 2.8553353331517428e-05, 2.8553353331517428e-05, 2.8553353331517428e-05, 2.8553353331517428e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.8553353331517428e-05

Optimization complete. Final v2v error: 4.161354064941406 mm

Highest mean error: 11.45494556427002 mm for frame 147

Lowest mean error: 3.1204445362091064 mm for frame 12

Saving results

Total time: 144.327054977417
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_022/1091/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_022/1091.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_022/1091
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00402055
Iteration 2/25 | Loss: 0.00092923
Iteration 3/25 | Loss: 0.00077751
Iteration 4/25 | Loss: 0.00075563
Iteration 5/25 | Loss: 0.00074987
Iteration 6/25 | Loss: 0.00074748
Iteration 7/25 | Loss: 0.00074686
Iteration 8/25 | Loss: 0.00074675
Iteration 9/25 | Loss: 0.00074675
Iteration 10/25 | Loss: 0.00074675
Iteration 11/25 | Loss: 0.00074675
Iteration 12/25 | Loss: 0.00074675
Iteration 13/25 | Loss: 0.00074675
Iteration 14/25 | Loss: 0.00074675
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0007467464893124998, 0.0007467464893124998, 0.0007467464893124998, 0.0007467464893124998, 0.0007467464893124998]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007467464893124998

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.63201225
Iteration 2/25 | Loss: 0.00133858
Iteration 3/25 | Loss: 0.00133858
Iteration 4/25 | Loss: 0.00133858
Iteration 5/25 | Loss: 0.00133858
Iteration 6/25 | Loss: 0.00133858
Iteration 7/25 | Loss: 0.00133858
Iteration 8/25 | Loss: 0.00133858
Iteration 9/25 | Loss: 0.00133858
Iteration 10/25 | Loss: 0.00133858
Iteration 11/25 | Loss: 0.00133858
Iteration 12/25 | Loss: 0.00133858
Iteration 13/25 | Loss: 0.00133858
Iteration 14/25 | Loss: 0.00133858
Iteration 15/25 | Loss: 0.00133858
Iteration 16/25 | Loss: 0.00133858
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0013385799247771502, 0.0013385799247771502, 0.0013385799247771502, 0.0013385799247771502, 0.0013385799247771502]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013385799247771502

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00133858
Iteration 2/1000 | Loss: 0.00002746
Iteration 3/1000 | Loss: 0.00001705
Iteration 4/1000 | Loss: 0.00001311
Iteration 5/1000 | Loss: 0.00001244
Iteration 6/1000 | Loss: 0.00001182
Iteration 7/1000 | Loss: 0.00001149
Iteration 8/1000 | Loss: 0.00001123
Iteration 9/1000 | Loss: 0.00001106
Iteration 10/1000 | Loss: 0.00001101
Iteration 11/1000 | Loss: 0.00001099
Iteration 12/1000 | Loss: 0.00001099
Iteration 13/1000 | Loss: 0.00001098
Iteration 14/1000 | Loss: 0.00001098
Iteration 15/1000 | Loss: 0.00001097
Iteration 16/1000 | Loss: 0.00001097
Iteration 17/1000 | Loss: 0.00001096
Iteration 18/1000 | Loss: 0.00001095
Iteration 19/1000 | Loss: 0.00001094
Iteration 20/1000 | Loss: 0.00001093
Iteration 21/1000 | Loss: 0.00001092
Iteration 22/1000 | Loss: 0.00001092
Iteration 23/1000 | Loss: 0.00001090
Iteration 24/1000 | Loss: 0.00001090
Iteration 25/1000 | Loss: 0.00001089
Iteration 26/1000 | Loss: 0.00001089
Iteration 27/1000 | Loss: 0.00001089
Iteration 28/1000 | Loss: 0.00001088
Iteration 29/1000 | Loss: 0.00001088
Iteration 30/1000 | Loss: 0.00001087
Iteration 31/1000 | Loss: 0.00001087
Iteration 32/1000 | Loss: 0.00001087
Iteration 33/1000 | Loss: 0.00001086
Iteration 34/1000 | Loss: 0.00001086
Iteration 35/1000 | Loss: 0.00001086
Iteration 36/1000 | Loss: 0.00001085
Iteration 37/1000 | Loss: 0.00001085
Iteration 38/1000 | Loss: 0.00001084
Iteration 39/1000 | Loss: 0.00001084
Iteration 40/1000 | Loss: 0.00001084
Iteration 41/1000 | Loss: 0.00001084
Iteration 42/1000 | Loss: 0.00001083
Iteration 43/1000 | Loss: 0.00001082
Iteration 44/1000 | Loss: 0.00001081
Iteration 45/1000 | Loss: 0.00001081
Iteration 46/1000 | Loss: 0.00001080
Iteration 47/1000 | Loss: 0.00001080
Iteration 48/1000 | Loss: 0.00001078
Iteration 49/1000 | Loss: 0.00001078
Iteration 50/1000 | Loss: 0.00001077
Iteration 51/1000 | Loss: 0.00001077
Iteration 52/1000 | Loss: 0.00001076
Iteration 53/1000 | Loss: 0.00001076
Iteration 54/1000 | Loss: 0.00001075
Iteration 55/1000 | Loss: 0.00001075
Iteration 56/1000 | Loss: 0.00001075
Iteration 57/1000 | Loss: 0.00001074
Iteration 58/1000 | Loss: 0.00001074
Iteration 59/1000 | Loss: 0.00001073
Iteration 60/1000 | Loss: 0.00001073
Iteration 61/1000 | Loss: 0.00001073
Iteration 62/1000 | Loss: 0.00001073
Iteration 63/1000 | Loss: 0.00001072
Iteration 64/1000 | Loss: 0.00001072
Iteration 65/1000 | Loss: 0.00001072
Iteration 66/1000 | Loss: 0.00001071
Iteration 67/1000 | Loss: 0.00001070
Iteration 68/1000 | Loss: 0.00001070
Iteration 69/1000 | Loss: 0.00001070
Iteration 70/1000 | Loss: 0.00001070
Iteration 71/1000 | Loss: 0.00001070
Iteration 72/1000 | Loss: 0.00001070
Iteration 73/1000 | Loss: 0.00001069
Iteration 74/1000 | Loss: 0.00001069
Iteration 75/1000 | Loss: 0.00001069
Iteration 76/1000 | Loss: 0.00001068
Iteration 77/1000 | Loss: 0.00001068
Iteration 78/1000 | Loss: 0.00001067
Iteration 79/1000 | Loss: 0.00001067
Iteration 80/1000 | Loss: 0.00001067
Iteration 81/1000 | Loss: 0.00001066
Iteration 82/1000 | Loss: 0.00001066
Iteration 83/1000 | Loss: 0.00001066
Iteration 84/1000 | Loss: 0.00001066
Iteration 85/1000 | Loss: 0.00001066
Iteration 86/1000 | Loss: 0.00001066
Iteration 87/1000 | Loss: 0.00001066
Iteration 88/1000 | Loss: 0.00001066
Iteration 89/1000 | Loss: 0.00001066
Iteration 90/1000 | Loss: 0.00001066
Iteration 91/1000 | Loss: 0.00001066
Iteration 92/1000 | Loss: 0.00001065
Iteration 93/1000 | Loss: 0.00001065
Iteration 94/1000 | Loss: 0.00001065
Iteration 95/1000 | Loss: 0.00001064
Iteration 96/1000 | Loss: 0.00001064
Iteration 97/1000 | Loss: 0.00001064
Iteration 98/1000 | Loss: 0.00001064
Iteration 99/1000 | Loss: 0.00001064
Iteration 100/1000 | Loss: 0.00001064
Iteration 101/1000 | Loss: 0.00001064
Iteration 102/1000 | Loss: 0.00001063
Iteration 103/1000 | Loss: 0.00001063
Iteration 104/1000 | Loss: 0.00001063
Iteration 105/1000 | Loss: 0.00001063
Iteration 106/1000 | Loss: 0.00001063
Iteration 107/1000 | Loss: 0.00001063
Iteration 108/1000 | Loss: 0.00001063
Iteration 109/1000 | Loss: 0.00001063
Iteration 110/1000 | Loss: 0.00001063
Iteration 111/1000 | Loss: 0.00001062
Iteration 112/1000 | Loss: 0.00001062
Iteration 113/1000 | Loss: 0.00001062
Iteration 114/1000 | Loss: 0.00001062
Iteration 115/1000 | Loss: 0.00001062
Iteration 116/1000 | Loss: 0.00001062
Iteration 117/1000 | Loss: 0.00001062
Iteration 118/1000 | Loss: 0.00001062
Iteration 119/1000 | Loss: 0.00001062
Iteration 120/1000 | Loss: 0.00001062
Iteration 121/1000 | Loss: 0.00001062
Iteration 122/1000 | Loss: 0.00001062
Iteration 123/1000 | Loss: 0.00001062
Iteration 124/1000 | Loss: 0.00001062
Iteration 125/1000 | Loss: 0.00001061
Iteration 126/1000 | Loss: 0.00001061
Iteration 127/1000 | Loss: 0.00001061
Iteration 128/1000 | Loss: 0.00001061
Iteration 129/1000 | Loss: 0.00001061
Iteration 130/1000 | Loss: 0.00001061
Iteration 131/1000 | Loss: 0.00001061
Iteration 132/1000 | Loss: 0.00001061
Iteration 133/1000 | Loss: 0.00001061
Iteration 134/1000 | Loss: 0.00001061
Iteration 135/1000 | Loss: 0.00001061
Iteration 136/1000 | Loss: 0.00001061
Iteration 137/1000 | Loss: 0.00001060
Iteration 138/1000 | Loss: 0.00001060
Iteration 139/1000 | Loss: 0.00001060
Iteration 140/1000 | Loss: 0.00001060
Iteration 141/1000 | Loss: 0.00001060
Iteration 142/1000 | Loss: 0.00001060
Iteration 143/1000 | Loss: 0.00001060
Iteration 144/1000 | Loss: 0.00001060
Iteration 145/1000 | Loss: 0.00001060
Iteration 146/1000 | Loss: 0.00001060
Iteration 147/1000 | Loss: 0.00001060
Iteration 148/1000 | Loss: 0.00001060
Iteration 149/1000 | Loss: 0.00001060
Iteration 150/1000 | Loss: 0.00001060
Iteration 151/1000 | Loss: 0.00001060
Iteration 152/1000 | Loss: 0.00001059
Iteration 153/1000 | Loss: 0.00001059
Iteration 154/1000 | Loss: 0.00001059
Iteration 155/1000 | Loss: 0.00001059
Iteration 156/1000 | Loss: 0.00001059
Iteration 157/1000 | Loss: 0.00001059
Iteration 158/1000 | Loss: 0.00001059
Iteration 159/1000 | Loss: 0.00001059
Iteration 160/1000 | Loss: 0.00001058
Iteration 161/1000 | Loss: 0.00001058
Iteration 162/1000 | Loss: 0.00001058
Iteration 163/1000 | Loss: 0.00001058
Iteration 164/1000 | Loss: 0.00001058
Iteration 165/1000 | Loss: 0.00001058
Iteration 166/1000 | Loss: 0.00001058
Iteration 167/1000 | Loss: 0.00001058
Iteration 168/1000 | Loss: 0.00001058
Iteration 169/1000 | Loss: 0.00001058
Iteration 170/1000 | Loss: 0.00001058
Iteration 171/1000 | Loss: 0.00001058
Iteration 172/1000 | Loss: 0.00001057
Iteration 173/1000 | Loss: 0.00001057
Iteration 174/1000 | Loss: 0.00001057
Iteration 175/1000 | Loss: 0.00001057
Iteration 176/1000 | Loss: 0.00001057
Iteration 177/1000 | Loss: 0.00001057
Iteration 178/1000 | Loss: 0.00001057
Iteration 179/1000 | Loss: 0.00001057
Iteration 180/1000 | Loss: 0.00001057
Iteration 181/1000 | Loss: 0.00001057
Iteration 182/1000 | Loss: 0.00001057
Iteration 183/1000 | Loss: 0.00001057
Iteration 184/1000 | Loss: 0.00001057
Iteration 185/1000 | Loss: 0.00001057
Iteration 186/1000 | Loss: 0.00001056
Iteration 187/1000 | Loss: 0.00001056
Iteration 188/1000 | Loss: 0.00001056
Iteration 189/1000 | Loss: 0.00001056
Iteration 190/1000 | Loss: 0.00001056
Iteration 191/1000 | Loss: 0.00001056
Iteration 192/1000 | Loss: 0.00001056
Iteration 193/1000 | Loss: 0.00001056
Iteration 194/1000 | Loss: 0.00001056
Iteration 195/1000 | Loss: 0.00001056
Iteration 196/1000 | Loss: 0.00001056
Iteration 197/1000 | Loss: 0.00001056
Iteration 198/1000 | Loss: 0.00001056
Iteration 199/1000 | Loss: 0.00001056
Iteration 200/1000 | Loss: 0.00001056
Iteration 201/1000 | Loss: 0.00001056
Iteration 202/1000 | Loss: 0.00001056
Iteration 203/1000 | Loss: 0.00001056
Iteration 204/1000 | Loss: 0.00001055
Iteration 205/1000 | Loss: 0.00001055
Iteration 206/1000 | Loss: 0.00001055
Iteration 207/1000 | Loss: 0.00001055
Iteration 208/1000 | Loss: 0.00001055
Iteration 209/1000 | Loss: 0.00001055
Iteration 210/1000 | Loss: 0.00001055
Iteration 211/1000 | Loss: 0.00001055
Iteration 212/1000 | Loss: 0.00001055
Iteration 213/1000 | Loss: 0.00001055
Iteration 214/1000 | Loss: 0.00001055
Iteration 215/1000 | Loss: 0.00001055
Iteration 216/1000 | Loss: 0.00001055
Iteration 217/1000 | Loss: 0.00001054
Iteration 218/1000 | Loss: 0.00001054
Iteration 219/1000 | Loss: 0.00001054
Iteration 220/1000 | Loss: 0.00001054
Iteration 221/1000 | Loss: 0.00001054
Iteration 222/1000 | Loss: 0.00001054
Iteration 223/1000 | Loss: 0.00001054
Iteration 224/1000 | Loss: 0.00001054
Iteration 225/1000 | Loss: 0.00001054
Iteration 226/1000 | Loss: 0.00001054
Iteration 227/1000 | Loss: 0.00001054
Iteration 228/1000 | Loss: 0.00001054
Iteration 229/1000 | Loss: 0.00001054
Iteration 230/1000 | Loss: 0.00001054
Iteration 231/1000 | Loss: 0.00001054
Iteration 232/1000 | Loss: 0.00001054
Iteration 233/1000 | Loss: 0.00001054
Iteration 234/1000 | Loss: 0.00001054
Iteration 235/1000 | Loss: 0.00001054
Iteration 236/1000 | Loss: 0.00001054
Iteration 237/1000 | Loss: 0.00001054
Iteration 238/1000 | Loss: 0.00001054
Iteration 239/1000 | Loss: 0.00001054
Iteration 240/1000 | Loss: 0.00001054
Iteration 241/1000 | Loss: 0.00001054
Iteration 242/1000 | Loss: 0.00001053
Iteration 243/1000 | Loss: 0.00001053
Iteration 244/1000 | Loss: 0.00001053
Iteration 245/1000 | Loss: 0.00001053
Iteration 246/1000 | Loss: 0.00001053
Iteration 247/1000 | Loss: 0.00001053
Iteration 248/1000 | Loss: 0.00001053
Iteration 249/1000 | Loss: 0.00001053
Iteration 250/1000 | Loss: 0.00001053
Iteration 251/1000 | Loss: 0.00001053
Iteration 252/1000 | Loss: 0.00001053
Iteration 253/1000 | Loss: 0.00001053
Iteration 254/1000 | Loss: 0.00001053
Iteration 255/1000 | Loss: 0.00001053
Iteration 256/1000 | Loss: 0.00001053
Iteration 257/1000 | Loss: 0.00001053
Iteration 258/1000 | Loss: 0.00001053
Iteration 259/1000 | Loss: 0.00001053
Iteration 260/1000 | Loss: 0.00001053
Iteration 261/1000 | Loss: 0.00001053
Iteration 262/1000 | Loss: 0.00001053
Iteration 263/1000 | Loss: 0.00001053
Iteration 264/1000 | Loss: 0.00001053
Iteration 265/1000 | Loss: 0.00001053
Iteration 266/1000 | Loss: 0.00001053
Iteration 267/1000 | Loss: 0.00001053
Iteration 268/1000 | Loss: 0.00001053
Iteration 269/1000 | Loss: 0.00001053
Iteration 270/1000 | Loss: 0.00001053
Iteration 271/1000 | Loss: 0.00001053
Iteration 272/1000 | Loss: 0.00001053
Iteration 273/1000 | Loss: 0.00001053
Iteration 274/1000 | Loss: 0.00001053
Iteration 275/1000 | Loss: 0.00001053
Iteration 276/1000 | Loss: 0.00001053
Iteration 277/1000 | Loss: 0.00001053
Iteration 278/1000 | Loss: 0.00001053
Iteration 279/1000 | Loss: 0.00001053
Iteration 280/1000 | Loss: 0.00001053
Iteration 281/1000 | Loss: 0.00001053
Iteration 282/1000 | Loss: 0.00001053
Iteration 283/1000 | Loss: 0.00001053
Iteration 284/1000 | Loss: 0.00001053
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 284. Stopping optimization.
Last 5 losses: [1.0530038707656786e-05, 1.0530038707656786e-05, 1.0530038707656786e-05, 1.0530038707656786e-05, 1.0530038707656786e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0530038707656786e-05

Optimization complete. Final v2v error: 2.721665620803833 mm

Highest mean error: 3.9869401454925537 mm for frame 60

Lowest mean error: 2.462557315826416 mm for frame 107

Saving results

Total time: 40.9873526096344
