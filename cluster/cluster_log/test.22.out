Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=22, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 1232-1287
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_helen_posed_023/1075/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_helen_posed_023/1075.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_helen_posed_023/1075
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00553219
Iteration 2/25 | Loss: 0.00158337
Iteration 3/25 | Loss: 0.00132053
Iteration 4/25 | Loss: 0.00129982
Iteration 5/25 | Loss: 0.00129652
Iteration 6/25 | Loss: 0.00129581
Iteration 7/25 | Loss: 0.00129581
Iteration 8/25 | Loss: 0.00129581
Iteration 9/25 | Loss: 0.00129581
Iteration 10/25 | Loss: 0.00129581
Iteration 11/25 | Loss: 0.00129581
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012958148727193475, 0.0012958148727193475, 0.0012958148727193475, 0.0012958148727193475, 0.0012958148727193475]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012958148727193475

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.39753842
Iteration 2/25 | Loss: 0.00104271
Iteration 3/25 | Loss: 0.00104262
Iteration 4/25 | Loss: 0.00104262
Iteration 5/25 | Loss: 0.00104262
Iteration 6/25 | Loss: 0.00104262
Iteration 7/25 | Loss: 0.00104262
Iteration 8/25 | Loss: 0.00104262
Iteration 9/25 | Loss: 0.00104262
Iteration 10/25 | Loss: 0.00104262
Iteration 11/25 | Loss: 0.00104262
Iteration 12/25 | Loss: 0.00104262
Iteration 13/25 | Loss: 0.00104262
Iteration 14/25 | Loss: 0.00104262
Iteration 15/25 | Loss: 0.00104262
Iteration 16/25 | Loss: 0.00104262
Iteration 17/25 | Loss: 0.00104262
Iteration 18/25 | Loss: 0.00104262
Iteration 19/25 | Loss: 0.00104262
Iteration 20/25 | Loss: 0.00104262
Iteration 21/25 | Loss: 0.00104262
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.00104261573869735, 0.00104261573869735, 0.00104261573869735, 0.00104261573869735, 0.00104261573869735]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00104261573869735

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00104262
Iteration 2/1000 | Loss: 0.00004341
Iteration 3/1000 | Loss: 0.00002639
Iteration 4/1000 | Loss: 0.00002226
Iteration 5/1000 | Loss: 0.00002094
Iteration 6/1000 | Loss: 0.00002010
Iteration 7/1000 | Loss: 0.00001950
Iteration 8/1000 | Loss: 0.00001917
Iteration 9/1000 | Loss: 0.00001886
Iteration 10/1000 | Loss: 0.00001857
Iteration 11/1000 | Loss: 0.00001835
Iteration 12/1000 | Loss: 0.00001832
Iteration 13/1000 | Loss: 0.00001816
Iteration 14/1000 | Loss: 0.00001812
Iteration 15/1000 | Loss: 0.00001798
Iteration 16/1000 | Loss: 0.00001794
Iteration 17/1000 | Loss: 0.00001794
Iteration 18/1000 | Loss: 0.00001791
Iteration 19/1000 | Loss: 0.00001791
Iteration 20/1000 | Loss: 0.00001790
Iteration 21/1000 | Loss: 0.00001787
Iteration 22/1000 | Loss: 0.00001786
Iteration 23/1000 | Loss: 0.00001785
Iteration 24/1000 | Loss: 0.00001784
Iteration 25/1000 | Loss: 0.00001780
Iteration 26/1000 | Loss: 0.00001777
Iteration 27/1000 | Loss: 0.00001776
Iteration 28/1000 | Loss: 0.00001775
Iteration 29/1000 | Loss: 0.00001775
Iteration 30/1000 | Loss: 0.00001774
Iteration 31/1000 | Loss: 0.00001774
Iteration 32/1000 | Loss: 0.00001774
Iteration 33/1000 | Loss: 0.00001772
Iteration 34/1000 | Loss: 0.00001772
Iteration 35/1000 | Loss: 0.00001772
Iteration 36/1000 | Loss: 0.00001772
Iteration 37/1000 | Loss: 0.00001772
Iteration 38/1000 | Loss: 0.00001771
Iteration 39/1000 | Loss: 0.00001771
Iteration 40/1000 | Loss: 0.00001771
Iteration 41/1000 | Loss: 0.00001771
Iteration 42/1000 | Loss: 0.00001771
Iteration 43/1000 | Loss: 0.00001769
Iteration 44/1000 | Loss: 0.00001769
Iteration 45/1000 | Loss: 0.00001768
Iteration 46/1000 | Loss: 0.00001768
Iteration 47/1000 | Loss: 0.00001768
Iteration 48/1000 | Loss: 0.00001768
Iteration 49/1000 | Loss: 0.00001768
Iteration 50/1000 | Loss: 0.00001768
Iteration 51/1000 | Loss: 0.00001768
Iteration 52/1000 | Loss: 0.00001768
Iteration 53/1000 | Loss: 0.00001767
Iteration 54/1000 | Loss: 0.00001767
Iteration 55/1000 | Loss: 0.00001767
Iteration 56/1000 | Loss: 0.00001767
Iteration 57/1000 | Loss: 0.00001767
Iteration 58/1000 | Loss: 0.00001766
Iteration 59/1000 | Loss: 0.00001765
Iteration 60/1000 | Loss: 0.00001764
Iteration 61/1000 | Loss: 0.00001764
Iteration 62/1000 | Loss: 0.00001764
Iteration 63/1000 | Loss: 0.00001764
Iteration 64/1000 | Loss: 0.00001764
Iteration 65/1000 | Loss: 0.00001764
Iteration 66/1000 | Loss: 0.00001764
Iteration 67/1000 | Loss: 0.00001763
Iteration 68/1000 | Loss: 0.00001763
Iteration 69/1000 | Loss: 0.00001763
Iteration 70/1000 | Loss: 0.00001762
Iteration 71/1000 | Loss: 0.00001762
Iteration 72/1000 | Loss: 0.00001761
Iteration 73/1000 | Loss: 0.00001761
Iteration 74/1000 | Loss: 0.00001761
Iteration 75/1000 | Loss: 0.00001760
Iteration 76/1000 | Loss: 0.00001760
Iteration 77/1000 | Loss: 0.00001760
Iteration 78/1000 | Loss: 0.00001760
Iteration 79/1000 | Loss: 0.00001760
Iteration 80/1000 | Loss: 0.00001760
Iteration 81/1000 | Loss: 0.00001760
Iteration 82/1000 | Loss: 0.00001759
Iteration 83/1000 | Loss: 0.00001759
Iteration 84/1000 | Loss: 0.00001759
Iteration 85/1000 | Loss: 0.00001759
Iteration 86/1000 | Loss: 0.00001759
Iteration 87/1000 | Loss: 0.00001758
Iteration 88/1000 | Loss: 0.00001758
Iteration 89/1000 | Loss: 0.00001758
Iteration 90/1000 | Loss: 0.00001758
Iteration 91/1000 | Loss: 0.00001758
Iteration 92/1000 | Loss: 0.00001757
Iteration 93/1000 | Loss: 0.00001757
Iteration 94/1000 | Loss: 0.00001757
Iteration 95/1000 | Loss: 0.00001757
Iteration 96/1000 | Loss: 0.00001757
Iteration 97/1000 | Loss: 0.00001757
Iteration 98/1000 | Loss: 0.00001757
Iteration 99/1000 | Loss: 0.00001757
Iteration 100/1000 | Loss: 0.00001757
Iteration 101/1000 | Loss: 0.00001757
Iteration 102/1000 | Loss: 0.00001757
Iteration 103/1000 | Loss: 0.00001757
Iteration 104/1000 | Loss: 0.00001757
Iteration 105/1000 | Loss: 0.00001757
Iteration 106/1000 | Loss: 0.00001756
Iteration 107/1000 | Loss: 0.00001756
Iteration 108/1000 | Loss: 0.00001756
Iteration 109/1000 | Loss: 0.00001755
Iteration 110/1000 | Loss: 0.00001755
Iteration 111/1000 | Loss: 0.00001755
Iteration 112/1000 | Loss: 0.00001755
Iteration 113/1000 | Loss: 0.00001755
Iteration 114/1000 | Loss: 0.00001754
Iteration 115/1000 | Loss: 0.00001754
Iteration 116/1000 | Loss: 0.00001754
Iteration 117/1000 | Loss: 0.00001753
Iteration 118/1000 | Loss: 0.00001753
Iteration 119/1000 | Loss: 0.00001753
Iteration 120/1000 | Loss: 0.00001753
Iteration 121/1000 | Loss: 0.00001752
Iteration 122/1000 | Loss: 0.00001752
Iteration 123/1000 | Loss: 0.00001752
Iteration 124/1000 | Loss: 0.00001751
Iteration 125/1000 | Loss: 0.00001751
Iteration 126/1000 | Loss: 0.00001751
Iteration 127/1000 | Loss: 0.00001751
Iteration 128/1000 | Loss: 0.00001750
Iteration 129/1000 | Loss: 0.00001750
Iteration 130/1000 | Loss: 0.00001750
Iteration 131/1000 | Loss: 0.00001750
Iteration 132/1000 | Loss: 0.00001749
Iteration 133/1000 | Loss: 0.00001749
Iteration 134/1000 | Loss: 0.00001749
Iteration 135/1000 | Loss: 0.00001749
Iteration 136/1000 | Loss: 0.00001749
Iteration 137/1000 | Loss: 0.00001749
Iteration 138/1000 | Loss: 0.00001749
Iteration 139/1000 | Loss: 0.00001748
Iteration 140/1000 | Loss: 0.00001748
Iteration 141/1000 | Loss: 0.00001748
Iteration 142/1000 | Loss: 0.00001748
Iteration 143/1000 | Loss: 0.00001747
Iteration 144/1000 | Loss: 0.00001747
Iteration 145/1000 | Loss: 0.00001747
Iteration 146/1000 | Loss: 0.00001746
Iteration 147/1000 | Loss: 0.00001746
Iteration 148/1000 | Loss: 0.00001746
Iteration 149/1000 | Loss: 0.00001746
Iteration 150/1000 | Loss: 0.00001746
Iteration 151/1000 | Loss: 0.00001745
Iteration 152/1000 | Loss: 0.00001745
Iteration 153/1000 | Loss: 0.00001745
Iteration 154/1000 | Loss: 0.00001745
Iteration 155/1000 | Loss: 0.00001745
Iteration 156/1000 | Loss: 0.00001744
Iteration 157/1000 | Loss: 0.00001744
Iteration 158/1000 | Loss: 0.00001744
Iteration 159/1000 | Loss: 0.00001744
Iteration 160/1000 | Loss: 0.00001744
Iteration 161/1000 | Loss: 0.00001744
Iteration 162/1000 | Loss: 0.00001744
Iteration 163/1000 | Loss: 0.00001743
Iteration 164/1000 | Loss: 0.00001743
Iteration 165/1000 | Loss: 0.00001743
Iteration 166/1000 | Loss: 0.00001743
Iteration 167/1000 | Loss: 0.00001743
Iteration 168/1000 | Loss: 0.00001743
Iteration 169/1000 | Loss: 0.00001743
Iteration 170/1000 | Loss: 0.00001743
Iteration 171/1000 | Loss: 0.00001743
Iteration 172/1000 | Loss: 0.00001743
Iteration 173/1000 | Loss: 0.00001743
Iteration 174/1000 | Loss: 0.00001742
Iteration 175/1000 | Loss: 0.00001742
Iteration 176/1000 | Loss: 0.00001742
Iteration 177/1000 | Loss: 0.00001742
Iteration 178/1000 | Loss: 0.00001742
Iteration 179/1000 | Loss: 0.00001741
Iteration 180/1000 | Loss: 0.00001741
Iteration 181/1000 | Loss: 0.00001741
Iteration 182/1000 | Loss: 0.00001741
Iteration 183/1000 | Loss: 0.00001741
Iteration 184/1000 | Loss: 0.00001741
Iteration 185/1000 | Loss: 0.00001741
Iteration 186/1000 | Loss: 0.00001740
Iteration 187/1000 | Loss: 0.00001740
Iteration 188/1000 | Loss: 0.00001740
Iteration 189/1000 | Loss: 0.00001740
Iteration 190/1000 | Loss: 0.00001740
Iteration 191/1000 | Loss: 0.00001739
Iteration 192/1000 | Loss: 0.00001739
Iteration 193/1000 | Loss: 0.00001739
Iteration 194/1000 | Loss: 0.00001739
Iteration 195/1000 | Loss: 0.00001738
Iteration 196/1000 | Loss: 0.00001738
Iteration 197/1000 | Loss: 0.00001738
Iteration 198/1000 | Loss: 0.00001738
Iteration 199/1000 | Loss: 0.00001738
Iteration 200/1000 | Loss: 0.00001738
Iteration 201/1000 | Loss: 0.00001738
Iteration 202/1000 | Loss: 0.00001738
Iteration 203/1000 | Loss: 0.00001738
Iteration 204/1000 | Loss: 0.00001738
Iteration 205/1000 | Loss: 0.00001738
Iteration 206/1000 | Loss: 0.00001738
Iteration 207/1000 | Loss: 0.00001738
Iteration 208/1000 | Loss: 0.00001738
Iteration 209/1000 | Loss: 0.00001738
Iteration 210/1000 | Loss: 0.00001738
Iteration 211/1000 | Loss: 0.00001737
Iteration 212/1000 | Loss: 0.00001737
Iteration 213/1000 | Loss: 0.00001737
Iteration 214/1000 | Loss: 0.00001737
Iteration 215/1000 | Loss: 0.00001737
Iteration 216/1000 | Loss: 0.00001737
Iteration 217/1000 | Loss: 0.00001737
Iteration 218/1000 | Loss: 0.00001737
Iteration 219/1000 | Loss: 0.00001737
Iteration 220/1000 | Loss: 0.00001737
Iteration 221/1000 | Loss: 0.00001737
Iteration 222/1000 | Loss: 0.00001737
Iteration 223/1000 | Loss: 0.00001737
Iteration 224/1000 | Loss: 0.00001737
Iteration 225/1000 | Loss: 0.00001737
Iteration 226/1000 | Loss: 0.00001737
Iteration 227/1000 | Loss: 0.00001737
Iteration 228/1000 | Loss: 0.00001737
Iteration 229/1000 | Loss: 0.00001737
Iteration 230/1000 | Loss: 0.00001737
Iteration 231/1000 | Loss: 0.00001737
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 231. Stopping optimization.
Last 5 losses: [1.7372009097016416e-05, 1.7372009097016416e-05, 1.7372009097016416e-05, 1.7372009097016416e-05, 1.7372009097016416e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7372009097016416e-05

Optimization complete. Final v2v error: 3.4912633895874023 mm

Highest mean error: 5.227167129516602 mm for frame 191

Lowest mean error: 3.0190320014953613 mm for frame 147

Saving results

Total time: 52.7343852519989
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_helen_posed_023/1041/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_helen_posed_023/1041.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_helen_posed_023/1041
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00945271
Iteration 2/25 | Loss: 0.00365529
Iteration 3/25 | Loss: 0.00319021
Iteration 4/25 | Loss: 0.00187122
Iteration 5/25 | Loss: 0.00168057
Iteration 6/25 | Loss: 0.00159076
Iteration 7/25 | Loss: 0.00160593
Iteration 8/25 | Loss: 0.00165918
Iteration 9/25 | Loss: 0.00166588
Iteration 10/25 | Loss: 0.00157276
Iteration 11/25 | Loss: 0.00146853
Iteration 12/25 | Loss: 0.00148307
Iteration 13/25 | Loss: 0.00146610
Iteration 14/25 | Loss: 0.00151568
Iteration 15/25 | Loss: 0.00147726
Iteration 16/25 | Loss: 0.00146526
Iteration 17/25 | Loss: 0.00150504
Iteration 18/25 | Loss: 0.00149906
Iteration 19/25 | Loss: 0.00148816
Iteration 20/25 | Loss: 0.00150018
Iteration 21/25 | Loss: 0.00148294
Iteration 22/25 | Loss: 0.00145914
Iteration 23/25 | Loss: 0.00145478
Iteration 24/25 | Loss: 0.00146500
Iteration 25/25 | Loss: 0.00144184

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.28459132
Iteration 2/25 | Loss: 0.00214284
Iteration 3/25 | Loss: 0.00214284
Iteration 4/25 | Loss: 0.00214284
Iteration 5/25 | Loss: 0.00214284
Iteration 6/25 | Loss: 0.00214284
Iteration 7/25 | Loss: 0.00214284
Iteration 8/25 | Loss: 0.00214284
Iteration 9/25 | Loss: 0.00214283
Iteration 10/25 | Loss: 0.00214284
Iteration 11/25 | Loss: 0.00214284
Iteration 12/25 | Loss: 0.00214284
Iteration 13/25 | Loss: 0.00214283
Iteration 14/25 | Loss: 0.00214283
Iteration 15/25 | Loss: 0.00214283
Iteration 16/25 | Loss: 0.00214283
Iteration 17/25 | Loss: 0.00214283
Iteration 18/25 | Loss: 0.00214283
Iteration 19/25 | Loss: 0.00214283
Iteration 20/25 | Loss: 0.00214283
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0021428344771265984, 0.0021428344771265984, 0.0021428344771265984, 0.0021428344771265984, 0.0021428344771265984]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0021428344771265984

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00214283
Iteration 2/1000 | Loss: 0.00128369
Iteration 3/1000 | Loss: 0.00093649
Iteration 4/1000 | Loss: 0.00034763
Iteration 5/1000 | Loss: 0.00013497
Iteration 6/1000 | Loss: 0.00032205
Iteration 7/1000 | Loss: 0.00011520
Iteration 8/1000 | Loss: 0.00037630
Iteration 9/1000 | Loss: 0.00035813
Iteration 10/1000 | Loss: 0.00027729
Iteration 11/1000 | Loss: 0.00022161
Iteration 12/1000 | Loss: 0.00016864
Iteration 13/1000 | Loss: 0.00015080
Iteration 14/1000 | Loss: 0.00043562
Iteration 15/1000 | Loss: 0.00036839
Iteration 16/1000 | Loss: 0.00028286
Iteration 17/1000 | Loss: 0.00111443
Iteration 18/1000 | Loss: 0.00046804
Iteration 19/1000 | Loss: 0.00040183
Iteration 20/1000 | Loss: 0.00063390
Iteration 21/1000 | Loss: 0.00022660
Iteration 22/1000 | Loss: 0.00007544
Iteration 23/1000 | Loss: 0.00071624
Iteration 24/1000 | Loss: 0.00052891
Iteration 25/1000 | Loss: 0.00052697
Iteration 26/1000 | Loss: 0.00045614
Iteration 27/1000 | Loss: 0.00026375
Iteration 28/1000 | Loss: 0.00034285
Iteration 29/1000 | Loss: 0.00024139
Iteration 30/1000 | Loss: 0.00033584
Iteration 31/1000 | Loss: 0.00101269
Iteration 32/1000 | Loss: 0.00042928
Iteration 33/1000 | Loss: 0.00016960
Iteration 34/1000 | Loss: 0.00014746
Iteration 35/1000 | Loss: 0.00032098
Iteration 36/1000 | Loss: 0.00043408
Iteration 37/1000 | Loss: 0.00043736
Iteration 38/1000 | Loss: 0.00037317
Iteration 39/1000 | Loss: 0.00041094
Iteration 40/1000 | Loss: 0.00038586
Iteration 41/1000 | Loss: 0.00037730
Iteration 42/1000 | Loss: 0.00016537
Iteration 43/1000 | Loss: 0.00035896
Iteration 44/1000 | Loss: 0.00036273
Iteration 45/1000 | Loss: 0.00111257
Iteration 46/1000 | Loss: 0.00042666
Iteration 47/1000 | Loss: 0.00071010
Iteration 48/1000 | Loss: 0.00035833
Iteration 49/1000 | Loss: 0.00013201
Iteration 50/1000 | Loss: 0.00017076
Iteration 51/1000 | Loss: 0.00015733
Iteration 52/1000 | Loss: 0.00006955
Iteration 53/1000 | Loss: 0.00016983
Iteration 54/1000 | Loss: 0.00008629
Iteration 55/1000 | Loss: 0.00016747
Iteration 56/1000 | Loss: 0.00012142
Iteration 57/1000 | Loss: 0.00018057
Iteration 58/1000 | Loss: 0.00006271
Iteration 59/1000 | Loss: 0.00005599
Iteration 60/1000 | Loss: 0.00004818
Iteration 61/1000 | Loss: 0.00024976
Iteration 62/1000 | Loss: 0.00034649
Iteration 63/1000 | Loss: 0.00006326
Iteration 64/1000 | Loss: 0.00005306
Iteration 65/1000 | Loss: 0.00012132
Iteration 66/1000 | Loss: 0.00031801
Iteration 67/1000 | Loss: 0.00014722
Iteration 68/1000 | Loss: 0.00018437
Iteration 69/1000 | Loss: 0.00013660
Iteration 70/1000 | Loss: 0.00006460
Iteration 71/1000 | Loss: 0.00004629
Iteration 72/1000 | Loss: 0.00026797
Iteration 73/1000 | Loss: 0.00005314
Iteration 74/1000 | Loss: 0.00004575
Iteration 75/1000 | Loss: 0.00004292
Iteration 76/1000 | Loss: 0.00004055
Iteration 77/1000 | Loss: 0.00006559
Iteration 78/1000 | Loss: 0.00004463
Iteration 79/1000 | Loss: 0.00003921
Iteration 80/1000 | Loss: 0.00003643
Iteration 81/1000 | Loss: 0.00057191
Iteration 82/1000 | Loss: 0.00051771
Iteration 83/1000 | Loss: 0.00038515
Iteration 84/1000 | Loss: 0.00005459
Iteration 85/1000 | Loss: 0.00004327
Iteration 86/1000 | Loss: 0.00003887
Iteration 87/1000 | Loss: 0.00003614
Iteration 88/1000 | Loss: 0.00003401
Iteration 89/1000 | Loss: 0.00017581
Iteration 90/1000 | Loss: 0.00003857
Iteration 91/1000 | Loss: 0.00003211
Iteration 92/1000 | Loss: 0.00003052
Iteration 93/1000 | Loss: 0.00002958
Iteration 94/1000 | Loss: 0.00002904
Iteration 95/1000 | Loss: 0.00028479
Iteration 96/1000 | Loss: 0.00021381
Iteration 97/1000 | Loss: 0.00012244
Iteration 98/1000 | Loss: 0.00005331
Iteration 99/1000 | Loss: 0.00005224
Iteration 100/1000 | Loss: 0.00004701
Iteration 101/1000 | Loss: 0.00004286
Iteration 102/1000 | Loss: 0.00003189
Iteration 103/1000 | Loss: 0.00005235
Iteration 104/1000 | Loss: 0.00003242
Iteration 105/1000 | Loss: 0.00003356
Iteration 106/1000 | Loss: 0.00002967
Iteration 107/1000 | Loss: 0.00002892
Iteration 108/1000 | Loss: 0.00002816
Iteration 109/1000 | Loss: 0.00002788
Iteration 110/1000 | Loss: 0.00002783
Iteration 111/1000 | Loss: 0.00003698
Iteration 112/1000 | Loss: 0.00003224
Iteration 113/1000 | Loss: 0.00003614
Iteration 114/1000 | Loss: 0.00006105
Iteration 115/1000 | Loss: 0.00004933
Iteration 116/1000 | Loss: 0.00002765
Iteration 117/1000 | Loss: 0.00002746
Iteration 118/1000 | Loss: 0.00002719
Iteration 119/1000 | Loss: 0.00002716
Iteration 120/1000 | Loss: 0.00002715
Iteration 121/1000 | Loss: 0.00002713
Iteration 122/1000 | Loss: 0.00002713
Iteration 123/1000 | Loss: 0.00002708
Iteration 124/1000 | Loss: 0.00002708
Iteration 125/1000 | Loss: 0.00002706
Iteration 126/1000 | Loss: 0.00002705
Iteration 127/1000 | Loss: 0.00002705
Iteration 128/1000 | Loss: 0.00002704
Iteration 129/1000 | Loss: 0.00002704
Iteration 130/1000 | Loss: 0.00002704
Iteration 131/1000 | Loss: 0.00002704
Iteration 132/1000 | Loss: 0.00002704
Iteration 133/1000 | Loss: 0.00002704
Iteration 134/1000 | Loss: 0.00002704
Iteration 135/1000 | Loss: 0.00002704
Iteration 136/1000 | Loss: 0.00002703
Iteration 137/1000 | Loss: 0.00002703
Iteration 138/1000 | Loss: 0.00002702
Iteration 139/1000 | Loss: 0.00002702
Iteration 140/1000 | Loss: 0.00002702
Iteration 141/1000 | Loss: 0.00002702
Iteration 142/1000 | Loss: 0.00002702
Iteration 143/1000 | Loss: 0.00002702
Iteration 144/1000 | Loss: 0.00002701
Iteration 145/1000 | Loss: 0.00002701
Iteration 146/1000 | Loss: 0.00002701
Iteration 147/1000 | Loss: 0.00002701
Iteration 148/1000 | Loss: 0.00002701
Iteration 149/1000 | Loss: 0.00002701
Iteration 150/1000 | Loss: 0.00002701
Iteration 151/1000 | Loss: 0.00002701
Iteration 152/1000 | Loss: 0.00002701
Iteration 153/1000 | Loss: 0.00002701
Iteration 154/1000 | Loss: 0.00002701
Iteration 155/1000 | Loss: 0.00002700
Iteration 156/1000 | Loss: 0.00002700
Iteration 157/1000 | Loss: 0.00002700
Iteration 158/1000 | Loss: 0.00002700
Iteration 159/1000 | Loss: 0.00002700
Iteration 160/1000 | Loss: 0.00002700
Iteration 161/1000 | Loss: 0.00002700
Iteration 162/1000 | Loss: 0.00002700
Iteration 163/1000 | Loss: 0.00002700
Iteration 164/1000 | Loss: 0.00002700
Iteration 165/1000 | Loss: 0.00002699
Iteration 166/1000 | Loss: 0.00002699
Iteration 167/1000 | Loss: 0.00002699
Iteration 168/1000 | Loss: 0.00002699
Iteration 169/1000 | Loss: 0.00002699
Iteration 170/1000 | Loss: 0.00002699
Iteration 171/1000 | Loss: 0.00002699
Iteration 172/1000 | Loss: 0.00002699
Iteration 173/1000 | Loss: 0.00002699
Iteration 174/1000 | Loss: 0.00002698
Iteration 175/1000 | Loss: 0.00002698
Iteration 176/1000 | Loss: 0.00002698
Iteration 177/1000 | Loss: 0.00002698
Iteration 178/1000 | Loss: 0.00002698
Iteration 179/1000 | Loss: 0.00002698
Iteration 180/1000 | Loss: 0.00002698
Iteration 181/1000 | Loss: 0.00002698
Iteration 182/1000 | Loss: 0.00002698
Iteration 183/1000 | Loss: 0.00002697
Iteration 184/1000 | Loss: 0.00002696
Iteration 185/1000 | Loss: 0.00002696
Iteration 186/1000 | Loss: 0.00002695
Iteration 187/1000 | Loss: 0.00002695
Iteration 188/1000 | Loss: 0.00002695
Iteration 189/1000 | Loss: 0.00002694
Iteration 190/1000 | Loss: 0.00002694
Iteration 191/1000 | Loss: 0.00002693
Iteration 192/1000 | Loss: 0.00002693
Iteration 193/1000 | Loss: 0.00002693
Iteration 194/1000 | Loss: 0.00002693
Iteration 195/1000 | Loss: 0.00002693
Iteration 196/1000 | Loss: 0.00002693
Iteration 197/1000 | Loss: 0.00002692
Iteration 198/1000 | Loss: 0.00002692
Iteration 199/1000 | Loss: 0.00002692
Iteration 200/1000 | Loss: 0.00002691
Iteration 201/1000 | Loss: 0.00002691
Iteration 202/1000 | Loss: 0.00002690
Iteration 203/1000 | Loss: 0.00002689
Iteration 204/1000 | Loss: 0.00002689
Iteration 205/1000 | Loss: 0.00002689
Iteration 206/1000 | Loss: 0.00002689
Iteration 207/1000 | Loss: 0.00002688
Iteration 208/1000 | Loss: 0.00002688
Iteration 209/1000 | Loss: 0.00002688
Iteration 210/1000 | Loss: 0.00002687
Iteration 211/1000 | Loss: 0.00002687
Iteration 212/1000 | Loss: 0.00002686
Iteration 213/1000 | Loss: 0.00002686
Iteration 214/1000 | Loss: 0.00002685
Iteration 215/1000 | Loss: 0.00002685
Iteration 216/1000 | Loss: 0.00002685
Iteration 217/1000 | Loss: 0.00002685
Iteration 218/1000 | Loss: 0.00002685
Iteration 219/1000 | Loss: 0.00002685
Iteration 220/1000 | Loss: 0.00002685
Iteration 221/1000 | Loss: 0.00002685
Iteration 222/1000 | Loss: 0.00002684
Iteration 223/1000 | Loss: 0.00002684
Iteration 224/1000 | Loss: 0.00002684
Iteration 225/1000 | Loss: 0.00002683
Iteration 226/1000 | Loss: 0.00002682
Iteration 227/1000 | Loss: 0.00002682
Iteration 228/1000 | Loss: 0.00002682
Iteration 229/1000 | Loss: 0.00002681
Iteration 230/1000 | Loss: 0.00002681
Iteration 231/1000 | Loss: 0.00002681
Iteration 232/1000 | Loss: 0.00002681
Iteration 233/1000 | Loss: 0.00002681
Iteration 234/1000 | Loss: 0.00002681
Iteration 235/1000 | Loss: 0.00002680
Iteration 236/1000 | Loss: 0.00002680
Iteration 237/1000 | Loss: 0.00002680
Iteration 238/1000 | Loss: 0.00002680
Iteration 239/1000 | Loss: 0.00002680
Iteration 240/1000 | Loss: 0.00002680
Iteration 241/1000 | Loss: 0.00002680
Iteration 242/1000 | Loss: 0.00002680
Iteration 243/1000 | Loss: 0.00002679
Iteration 244/1000 | Loss: 0.00002679
Iteration 245/1000 | Loss: 0.00002679
Iteration 246/1000 | Loss: 0.00002678
Iteration 247/1000 | Loss: 0.00002678
Iteration 248/1000 | Loss: 0.00002678
Iteration 249/1000 | Loss: 0.00002678
Iteration 250/1000 | Loss: 0.00002678
Iteration 251/1000 | Loss: 0.00002677
Iteration 252/1000 | Loss: 0.00002677
Iteration 253/1000 | Loss: 0.00002676
Iteration 254/1000 | Loss: 0.00002676
Iteration 255/1000 | Loss: 0.00002675
Iteration 256/1000 | Loss: 0.00002675
Iteration 257/1000 | Loss: 0.00002674
Iteration 258/1000 | Loss: 0.00002674
Iteration 259/1000 | Loss: 0.00002674
Iteration 260/1000 | Loss: 0.00002674
Iteration 261/1000 | Loss: 0.00002674
Iteration 262/1000 | Loss: 0.00002673
Iteration 263/1000 | Loss: 0.00002673
Iteration 264/1000 | Loss: 0.00002673
Iteration 265/1000 | Loss: 0.00002673
Iteration 266/1000 | Loss: 0.00002672
Iteration 267/1000 | Loss: 0.00002672
Iteration 268/1000 | Loss: 0.00002671
Iteration 269/1000 | Loss: 0.00002671
Iteration 270/1000 | Loss: 0.00002671
Iteration 271/1000 | Loss: 0.00002671
Iteration 272/1000 | Loss: 0.00002671
Iteration 273/1000 | Loss: 0.00002671
Iteration 274/1000 | Loss: 0.00002671
Iteration 275/1000 | Loss: 0.00002670
Iteration 276/1000 | Loss: 0.00002670
Iteration 277/1000 | Loss: 0.00002669
Iteration 278/1000 | Loss: 0.00002669
Iteration 279/1000 | Loss: 0.00002669
Iteration 280/1000 | Loss: 0.00002669
Iteration 281/1000 | Loss: 0.00002668
Iteration 282/1000 | Loss: 0.00002668
Iteration 283/1000 | Loss: 0.00002668
Iteration 284/1000 | Loss: 0.00002668
Iteration 285/1000 | Loss: 0.00002667
Iteration 286/1000 | Loss: 0.00002667
Iteration 287/1000 | Loss: 0.00002667
Iteration 288/1000 | Loss: 0.00002665
Iteration 289/1000 | Loss: 0.00002664
Iteration 290/1000 | Loss: 0.00002664
Iteration 291/1000 | Loss: 0.00002663
Iteration 292/1000 | Loss: 0.00002663
Iteration 293/1000 | Loss: 0.00002663
Iteration 294/1000 | Loss: 0.00002663
Iteration 295/1000 | Loss: 0.00002663
Iteration 296/1000 | Loss: 0.00002663
Iteration 297/1000 | Loss: 0.00002663
Iteration 298/1000 | Loss: 0.00002662
Iteration 299/1000 | Loss: 0.00002662
Iteration 300/1000 | Loss: 0.00002660
Iteration 301/1000 | Loss: 0.00028147
Iteration 302/1000 | Loss: 0.00097215
Iteration 303/1000 | Loss: 0.00007699
Iteration 304/1000 | Loss: 0.00004411
Iteration 305/1000 | Loss: 0.00003347
Iteration 306/1000 | Loss: 0.00003019
Iteration 307/1000 | Loss: 0.00002823
Iteration 308/1000 | Loss: 0.00002660
Iteration 309/1000 | Loss: 0.00002562
Iteration 310/1000 | Loss: 0.00002505
Iteration 311/1000 | Loss: 0.00002452
Iteration 312/1000 | Loss: 0.00002405
Iteration 313/1000 | Loss: 0.00002370
Iteration 314/1000 | Loss: 0.00002351
Iteration 315/1000 | Loss: 0.00002337
Iteration 316/1000 | Loss: 0.00002317
Iteration 317/1000 | Loss: 0.00002315
Iteration 318/1000 | Loss: 0.00002310
Iteration 319/1000 | Loss: 0.00002309
Iteration 320/1000 | Loss: 0.00002304
Iteration 321/1000 | Loss: 0.00002297
Iteration 322/1000 | Loss: 0.00002296
Iteration 323/1000 | Loss: 0.00002290
Iteration 324/1000 | Loss: 0.00002290
Iteration 325/1000 | Loss: 0.00002289
Iteration 326/1000 | Loss: 0.00002289
Iteration 327/1000 | Loss: 0.00002289
Iteration 328/1000 | Loss: 0.00002289
Iteration 329/1000 | Loss: 0.00002286
Iteration 330/1000 | Loss: 0.00002286
Iteration 331/1000 | Loss: 0.00002286
Iteration 332/1000 | Loss: 0.00002285
Iteration 333/1000 | Loss: 0.00002283
Iteration 334/1000 | Loss: 0.00002282
Iteration 335/1000 | Loss: 0.00002281
Iteration 336/1000 | Loss: 0.00002280
Iteration 337/1000 | Loss: 0.00002275
Iteration 338/1000 | Loss: 0.00002274
Iteration 339/1000 | Loss: 0.00002274
Iteration 340/1000 | Loss: 0.00002274
Iteration 341/1000 | Loss: 0.00002274
Iteration 342/1000 | Loss: 0.00002274
Iteration 343/1000 | Loss: 0.00002274
Iteration 344/1000 | Loss: 0.00002274
Iteration 345/1000 | Loss: 0.00002274
Iteration 346/1000 | Loss: 0.00002274
Iteration 347/1000 | Loss: 0.00002273
Iteration 348/1000 | Loss: 0.00002272
Iteration 349/1000 | Loss: 0.00002272
Iteration 350/1000 | Loss: 0.00002272
Iteration 351/1000 | Loss: 0.00002272
Iteration 352/1000 | Loss: 0.00002272
Iteration 353/1000 | Loss: 0.00002272
Iteration 354/1000 | Loss: 0.00002272
Iteration 355/1000 | Loss: 0.00002272
Iteration 356/1000 | Loss: 0.00002272
Iteration 357/1000 | Loss: 0.00002272
Iteration 358/1000 | Loss: 0.00002272
Iteration 359/1000 | Loss: 0.00002272
Iteration 360/1000 | Loss: 0.00002272
Iteration 361/1000 | Loss: 0.00002272
Iteration 362/1000 | Loss: 0.00002272
Iteration 363/1000 | Loss: 0.00002272
Iteration 364/1000 | Loss: 0.00002271
Iteration 365/1000 | Loss: 0.00002270
Iteration 366/1000 | Loss: 0.00002270
Iteration 367/1000 | Loss: 0.00002270
Iteration 368/1000 | Loss: 0.00002269
Iteration 369/1000 | Loss: 0.00002269
Iteration 370/1000 | Loss: 0.00002269
Iteration 371/1000 | Loss: 0.00002268
Iteration 372/1000 | Loss: 0.00002268
Iteration 373/1000 | Loss: 0.00002267
Iteration 374/1000 | Loss: 0.00002267
Iteration 375/1000 | Loss: 0.00002267
Iteration 376/1000 | Loss: 0.00002266
Iteration 377/1000 | Loss: 0.00002266
Iteration 378/1000 | Loss: 0.00002265
Iteration 379/1000 | Loss: 0.00002265
Iteration 380/1000 | Loss: 0.00002265
Iteration 381/1000 | Loss: 0.00002265
Iteration 382/1000 | Loss: 0.00002264
Iteration 383/1000 | Loss: 0.00002264
Iteration 384/1000 | Loss: 0.00002264
Iteration 385/1000 | Loss: 0.00002263
Iteration 386/1000 | Loss: 0.00002263
Iteration 387/1000 | Loss: 0.00002263
Iteration 388/1000 | Loss: 0.00002262
Iteration 389/1000 | Loss: 0.00002261
Iteration 390/1000 | Loss: 0.00002261
Iteration 391/1000 | Loss: 0.00002261
Iteration 392/1000 | Loss: 0.00002261
Iteration 393/1000 | Loss: 0.00002261
Iteration 394/1000 | Loss: 0.00002261
Iteration 395/1000 | Loss: 0.00002261
Iteration 396/1000 | Loss: 0.00002261
Iteration 397/1000 | Loss: 0.00002261
Iteration 398/1000 | Loss: 0.00002261
Iteration 399/1000 | Loss: 0.00002261
Iteration 400/1000 | Loss: 0.00002261
Iteration 401/1000 | Loss: 0.00002260
Iteration 402/1000 | Loss: 0.00002260
Iteration 403/1000 | Loss: 0.00002259
Iteration 404/1000 | Loss: 0.00002259
Iteration 405/1000 | Loss: 0.00002259
Iteration 406/1000 | Loss: 0.00002259
Iteration 407/1000 | Loss: 0.00002259
Iteration 408/1000 | Loss: 0.00002259
Iteration 409/1000 | Loss: 0.00002258
Iteration 410/1000 | Loss: 0.00002258
Iteration 411/1000 | Loss: 0.00002258
Iteration 412/1000 | Loss: 0.00002258
Iteration 413/1000 | Loss: 0.00002258
Iteration 414/1000 | Loss: 0.00002258
Iteration 415/1000 | Loss: 0.00002258
Iteration 416/1000 | Loss: 0.00002258
Iteration 417/1000 | Loss: 0.00002258
Iteration 418/1000 | Loss: 0.00002258
Iteration 419/1000 | Loss: 0.00002258
Iteration 420/1000 | Loss: 0.00002258
Iteration 421/1000 | Loss: 0.00002258
Iteration 422/1000 | Loss: 0.00002258
Iteration 423/1000 | Loss: 0.00002257
Iteration 424/1000 | Loss: 0.00002257
Iteration 425/1000 | Loss: 0.00002257
Iteration 426/1000 | Loss: 0.00002257
Iteration 427/1000 | Loss: 0.00002257
Iteration 428/1000 | Loss: 0.00002257
Iteration 429/1000 | Loss: 0.00002257
Iteration 430/1000 | Loss: 0.00002257
Iteration 431/1000 | Loss: 0.00002257
Iteration 432/1000 | Loss: 0.00002257
Iteration 433/1000 | Loss: 0.00002257
Iteration 434/1000 | Loss: 0.00002257
Iteration 435/1000 | Loss: 0.00002256
Iteration 436/1000 | Loss: 0.00002256
Iteration 437/1000 | Loss: 0.00002256
Iteration 438/1000 | Loss: 0.00002256
Iteration 439/1000 | Loss: 0.00002256
Iteration 440/1000 | Loss: 0.00002256
Iteration 441/1000 | Loss: 0.00002256
Iteration 442/1000 | Loss: 0.00002256
Iteration 443/1000 | Loss: 0.00002256
Iteration 444/1000 | Loss: 0.00002256
Iteration 445/1000 | Loss: 0.00002256
Iteration 446/1000 | Loss: 0.00002256
Iteration 447/1000 | Loss: 0.00002256
Iteration 448/1000 | Loss: 0.00002256
Iteration 449/1000 | Loss: 0.00002256
Iteration 450/1000 | Loss: 0.00002256
Iteration 451/1000 | Loss: 0.00002256
Iteration 452/1000 | Loss: 0.00002255
Iteration 453/1000 | Loss: 0.00002255
Iteration 454/1000 | Loss: 0.00002255
Iteration 455/1000 | Loss: 0.00002255
Iteration 456/1000 | Loss: 0.00002255
Iteration 457/1000 | Loss: 0.00002255
Iteration 458/1000 | Loss: 0.00002255
Iteration 459/1000 | Loss: 0.00002255
Iteration 460/1000 | Loss: 0.00002255
Iteration 461/1000 | Loss: 0.00002255
Iteration 462/1000 | Loss: 0.00002255
Iteration 463/1000 | Loss: 0.00002255
Iteration 464/1000 | Loss: 0.00002255
Iteration 465/1000 | Loss: 0.00002255
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 465. Stopping optimization.
Last 5 losses: [2.255209801660385e-05, 2.255209801660385e-05, 2.255209801660385e-05, 2.255209801660385e-05, 2.255209801660385e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.255209801660385e-05

Optimization complete. Final v2v error: 3.7969870567321777 mm

Highest mean error: 5.8777995109558105 mm for frame 53

Lowest mean error: 3.558844566345215 mm for frame 151

Saving results

Total time: 254.8618266582489
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_helen_posed_023/1074/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_helen_posed_023/1074.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_helen_posed_023/1074
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00754509
Iteration 2/25 | Loss: 0.00170667
Iteration 3/25 | Loss: 0.00132932
Iteration 4/25 | Loss: 0.00129799
Iteration 5/25 | Loss: 0.00129347
Iteration 6/25 | Loss: 0.00129291
Iteration 7/25 | Loss: 0.00129291
Iteration 8/25 | Loss: 0.00129291
Iteration 9/25 | Loss: 0.00129291
Iteration 10/25 | Loss: 0.00129291
Iteration 11/25 | Loss: 0.00129291
Iteration 12/25 | Loss: 0.00129291
Iteration 13/25 | Loss: 0.00129291
Iteration 14/25 | Loss: 0.00129291
Iteration 15/25 | Loss: 0.00129291
Iteration 16/25 | Loss: 0.00129291
Iteration 17/25 | Loss: 0.00129291
Iteration 18/25 | Loss: 0.00129291
Iteration 19/25 | Loss: 0.00129291
Iteration 20/25 | Loss: 0.00129291
Iteration 21/25 | Loss: 0.00129291
Iteration 22/25 | Loss: 0.00129291
Iteration 23/25 | Loss: 0.00129291
Iteration 24/25 | Loss: 0.00129291
Iteration 25/25 | Loss: 0.00129291

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.18794799
Iteration 2/25 | Loss: 0.00103637
Iteration 3/25 | Loss: 0.00103637
Iteration 4/25 | Loss: 0.00103637
Iteration 5/25 | Loss: 0.00103637
Iteration 6/25 | Loss: 0.00103637
Iteration 7/25 | Loss: 0.00103637
Iteration 8/25 | Loss: 0.00103637
Iteration 9/25 | Loss: 0.00103637
Iteration 10/25 | Loss: 0.00103637
Iteration 11/25 | Loss: 0.00103637
Iteration 12/25 | Loss: 0.00103637
Iteration 13/25 | Loss: 0.00103637
Iteration 14/25 | Loss: 0.00103637
Iteration 15/25 | Loss: 0.00103637
Iteration 16/25 | Loss: 0.00103637
Iteration 17/25 | Loss: 0.00103637
Iteration 18/25 | Loss: 0.00103637
Iteration 19/25 | Loss: 0.00103637
Iteration 20/25 | Loss: 0.00103637
Iteration 21/25 | Loss: 0.00103637
Iteration 22/25 | Loss: 0.00103637
Iteration 23/25 | Loss: 0.00103637
Iteration 24/25 | Loss: 0.00103637
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.001036368077620864, 0.001036368077620864, 0.001036368077620864, 0.001036368077620864, 0.001036368077620864]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001036368077620864

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00103637
Iteration 2/1000 | Loss: 0.00004719
Iteration 3/1000 | Loss: 0.00003111
Iteration 4/1000 | Loss: 0.00002776
Iteration 5/1000 | Loss: 0.00002569
Iteration 6/1000 | Loss: 0.00002448
Iteration 7/1000 | Loss: 0.00002341
Iteration 8/1000 | Loss: 0.00002283
Iteration 9/1000 | Loss: 0.00002218
Iteration 10/1000 | Loss: 0.00002167
Iteration 11/1000 | Loss: 0.00002132
Iteration 12/1000 | Loss: 0.00002096
Iteration 13/1000 | Loss: 0.00002077
Iteration 14/1000 | Loss: 0.00002072
Iteration 15/1000 | Loss: 0.00002071
Iteration 16/1000 | Loss: 0.00002052
Iteration 17/1000 | Loss: 0.00002050
Iteration 18/1000 | Loss: 0.00002045
Iteration 19/1000 | Loss: 0.00002040
Iteration 20/1000 | Loss: 0.00002038
Iteration 21/1000 | Loss: 0.00002034
Iteration 22/1000 | Loss: 0.00002029
Iteration 23/1000 | Loss: 0.00002027
Iteration 24/1000 | Loss: 0.00002026
Iteration 25/1000 | Loss: 0.00002017
Iteration 26/1000 | Loss: 0.00002014
Iteration 27/1000 | Loss: 0.00002014
Iteration 28/1000 | Loss: 0.00002014
Iteration 29/1000 | Loss: 0.00002013
Iteration 30/1000 | Loss: 0.00002012
Iteration 31/1000 | Loss: 0.00002012
Iteration 32/1000 | Loss: 0.00002011
Iteration 33/1000 | Loss: 0.00002010
Iteration 34/1000 | Loss: 0.00002010
Iteration 35/1000 | Loss: 0.00002009
Iteration 36/1000 | Loss: 0.00002009
Iteration 37/1000 | Loss: 0.00002009
Iteration 38/1000 | Loss: 0.00002009
Iteration 39/1000 | Loss: 0.00002009
Iteration 40/1000 | Loss: 0.00002009
Iteration 41/1000 | Loss: 0.00002009
Iteration 42/1000 | Loss: 0.00002009
Iteration 43/1000 | Loss: 0.00002008
Iteration 44/1000 | Loss: 0.00002008
Iteration 45/1000 | Loss: 0.00002000
Iteration 46/1000 | Loss: 0.00001997
Iteration 47/1000 | Loss: 0.00001997
Iteration 48/1000 | Loss: 0.00001996
Iteration 49/1000 | Loss: 0.00001996
Iteration 50/1000 | Loss: 0.00001993
Iteration 51/1000 | Loss: 0.00001993
Iteration 52/1000 | Loss: 0.00001993
Iteration 53/1000 | Loss: 0.00001992
Iteration 54/1000 | Loss: 0.00001992
Iteration 55/1000 | Loss: 0.00001988
Iteration 56/1000 | Loss: 0.00001988
Iteration 57/1000 | Loss: 0.00001988
Iteration 58/1000 | Loss: 0.00001988
Iteration 59/1000 | Loss: 0.00001988
Iteration 60/1000 | Loss: 0.00001988
Iteration 61/1000 | Loss: 0.00001988
Iteration 62/1000 | Loss: 0.00001988
Iteration 63/1000 | Loss: 0.00001987
Iteration 64/1000 | Loss: 0.00001987
Iteration 65/1000 | Loss: 0.00001984
Iteration 66/1000 | Loss: 0.00001984
Iteration 67/1000 | Loss: 0.00001983
Iteration 68/1000 | Loss: 0.00001983
Iteration 69/1000 | Loss: 0.00001982
Iteration 70/1000 | Loss: 0.00001981
Iteration 71/1000 | Loss: 0.00001979
Iteration 72/1000 | Loss: 0.00001978
Iteration 73/1000 | Loss: 0.00001978
Iteration 74/1000 | Loss: 0.00001978
Iteration 75/1000 | Loss: 0.00001978
Iteration 76/1000 | Loss: 0.00001974
Iteration 77/1000 | Loss: 0.00001974
Iteration 78/1000 | Loss: 0.00001974
Iteration 79/1000 | Loss: 0.00001974
Iteration 80/1000 | Loss: 0.00001974
Iteration 81/1000 | Loss: 0.00001973
Iteration 82/1000 | Loss: 0.00001973
Iteration 83/1000 | Loss: 0.00001973
Iteration 84/1000 | Loss: 0.00001973
Iteration 85/1000 | Loss: 0.00001972
Iteration 86/1000 | Loss: 0.00001971
Iteration 87/1000 | Loss: 0.00001970
Iteration 88/1000 | Loss: 0.00001970
Iteration 89/1000 | Loss: 0.00001970
Iteration 90/1000 | Loss: 0.00001970
Iteration 91/1000 | Loss: 0.00001970
Iteration 92/1000 | Loss: 0.00001970
Iteration 93/1000 | Loss: 0.00001970
Iteration 94/1000 | Loss: 0.00001970
Iteration 95/1000 | Loss: 0.00001970
Iteration 96/1000 | Loss: 0.00001970
Iteration 97/1000 | Loss: 0.00001970
Iteration 98/1000 | Loss: 0.00001970
Iteration 99/1000 | Loss: 0.00001969
Iteration 100/1000 | Loss: 0.00001969
Iteration 101/1000 | Loss: 0.00001968
Iteration 102/1000 | Loss: 0.00001968
Iteration 103/1000 | Loss: 0.00001968
Iteration 104/1000 | Loss: 0.00001968
Iteration 105/1000 | Loss: 0.00001968
Iteration 106/1000 | Loss: 0.00001968
Iteration 107/1000 | Loss: 0.00001968
Iteration 108/1000 | Loss: 0.00001968
Iteration 109/1000 | Loss: 0.00001967
Iteration 110/1000 | Loss: 0.00001967
Iteration 111/1000 | Loss: 0.00001967
Iteration 112/1000 | Loss: 0.00001967
Iteration 113/1000 | Loss: 0.00001967
Iteration 114/1000 | Loss: 0.00001967
Iteration 115/1000 | Loss: 0.00001967
Iteration 116/1000 | Loss: 0.00001967
Iteration 117/1000 | Loss: 0.00001967
Iteration 118/1000 | Loss: 0.00001967
Iteration 119/1000 | Loss: 0.00001966
Iteration 120/1000 | Loss: 0.00001966
Iteration 121/1000 | Loss: 0.00001966
Iteration 122/1000 | Loss: 0.00001966
Iteration 123/1000 | Loss: 0.00001966
Iteration 124/1000 | Loss: 0.00001966
Iteration 125/1000 | Loss: 0.00001966
Iteration 126/1000 | Loss: 0.00001966
Iteration 127/1000 | Loss: 0.00001966
Iteration 128/1000 | Loss: 0.00001965
Iteration 129/1000 | Loss: 0.00001965
Iteration 130/1000 | Loss: 0.00001965
Iteration 131/1000 | Loss: 0.00001965
Iteration 132/1000 | Loss: 0.00001964
Iteration 133/1000 | Loss: 0.00001964
Iteration 134/1000 | Loss: 0.00001964
Iteration 135/1000 | Loss: 0.00001964
Iteration 136/1000 | Loss: 0.00001964
Iteration 137/1000 | Loss: 0.00001964
Iteration 138/1000 | Loss: 0.00001964
Iteration 139/1000 | Loss: 0.00001964
Iteration 140/1000 | Loss: 0.00001964
Iteration 141/1000 | Loss: 0.00001964
Iteration 142/1000 | Loss: 0.00001964
Iteration 143/1000 | Loss: 0.00001964
Iteration 144/1000 | Loss: 0.00001964
Iteration 145/1000 | Loss: 0.00001964
Iteration 146/1000 | Loss: 0.00001964
Iteration 147/1000 | Loss: 0.00001964
Iteration 148/1000 | Loss: 0.00001964
Iteration 149/1000 | Loss: 0.00001964
Iteration 150/1000 | Loss: 0.00001964
Iteration 151/1000 | Loss: 0.00001964
Iteration 152/1000 | Loss: 0.00001964
Iteration 153/1000 | Loss: 0.00001964
Iteration 154/1000 | Loss: 0.00001964
Iteration 155/1000 | Loss: 0.00001964
Iteration 156/1000 | Loss: 0.00001964
Iteration 157/1000 | Loss: 0.00001964
Iteration 158/1000 | Loss: 0.00001964
Iteration 159/1000 | Loss: 0.00001964
Iteration 160/1000 | Loss: 0.00001964
Iteration 161/1000 | Loss: 0.00001964
Iteration 162/1000 | Loss: 0.00001964
Iteration 163/1000 | Loss: 0.00001964
Iteration 164/1000 | Loss: 0.00001964
Iteration 165/1000 | Loss: 0.00001964
Iteration 166/1000 | Loss: 0.00001964
Iteration 167/1000 | Loss: 0.00001964
Iteration 168/1000 | Loss: 0.00001964
Iteration 169/1000 | Loss: 0.00001964
Iteration 170/1000 | Loss: 0.00001964
Iteration 171/1000 | Loss: 0.00001964
Iteration 172/1000 | Loss: 0.00001964
Iteration 173/1000 | Loss: 0.00001964
Iteration 174/1000 | Loss: 0.00001964
Iteration 175/1000 | Loss: 0.00001964
Iteration 176/1000 | Loss: 0.00001964
Iteration 177/1000 | Loss: 0.00001964
Iteration 178/1000 | Loss: 0.00001964
Iteration 179/1000 | Loss: 0.00001964
Iteration 180/1000 | Loss: 0.00001964
Iteration 181/1000 | Loss: 0.00001964
Iteration 182/1000 | Loss: 0.00001964
Iteration 183/1000 | Loss: 0.00001964
Iteration 184/1000 | Loss: 0.00001964
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 184. Stopping optimization.
Last 5 losses: [1.963776594493538e-05, 1.963776594493538e-05, 1.963776594493538e-05, 1.963776594493538e-05, 1.963776594493538e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.963776594493538e-05

Optimization complete. Final v2v error: 3.700803756713867 mm

Highest mean error: 4.509171485900879 mm for frame 44

Lowest mean error: 3.1829886436462402 mm for frame 181

Saving results

Total time: 46.57011818885803
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_helen_posed_023/1094/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_helen_posed_023/1094.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_helen_posed_023/1094
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00700287
Iteration 2/25 | Loss: 0.00181267
Iteration 3/25 | Loss: 0.00151283
Iteration 4/25 | Loss: 0.00144682
Iteration 5/25 | Loss: 0.00134854
Iteration 6/25 | Loss: 0.00133335
Iteration 7/25 | Loss: 0.00130183
Iteration 8/25 | Loss: 0.00130060
Iteration 9/25 | Loss: 0.00129222
Iteration 10/25 | Loss: 0.00128769
Iteration 11/25 | Loss: 0.00128338
Iteration 12/25 | Loss: 0.00128221
Iteration 13/25 | Loss: 0.00128137
Iteration 14/25 | Loss: 0.00128058
Iteration 15/25 | Loss: 0.00128013
Iteration 16/25 | Loss: 0.00127985
Iteration 17/25 | Loss: 0.00127967
Iteration 18/25 | Loss: 0.00127950
Iteration 19/25 | Loss: 0.00127943
Iteration 20/25 | Loss: 0.00127943
Iteration 21/25 | Loss: 0.00127943
Iteration 22/25 | Loss: 0.00127943
Iteration 23/25 | Loss: 0.00127943
Iteration 24/25 | Loss: 0.00127943
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0012794279027730227, 0.0012794279027730227, 0.0012794279027730227, 0.0012794279027730227, 0.0012794279027730227]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012794279027730227

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.64244318
Iteration 2/25 | Loss: 0.00370025
Iteration 3/25 | Loss: 0.00224760
Iteration 4/25 | Loss: 0.00224760
Iteration 5/25 | Loss: 0.00224760
Iteration 6/25 | Loss: 0.00224760
Iteration 7/25 | Loss: 0.00224760
Iteration 8/25 | Loss: 0.00224760
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 8. Stopping optimization.
Last 5 losses: [0.002247600583359599, 0.002247600583359599, 0.002247600583359599, 0.002247600583359599, 0.002247600583359599]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002247600583359599

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00224760
Iteration 2/1000 | Loss: 0.00032885
Iteration 3/1000 | Loss: 0.00100515
Iteration 4/1000 | Loss: 0.00053732
Iteration 5/1000 | Loss: 0.00009867
Iteration 6/1000 | Loss: 0.00367856
Iteration 7/1000 | Loss: 0.00243221
Iteration 8/1000 | Loss: 0.00429044
Iteration 9/1000 | Loss: 0.00310081
Iteration 10/1000 | Loss: 0.00415509
Iteration 11/1000 | Loss: 0.00788292
Iteration 12/1000 | Loss: 0.00616113
Iteration 13/1000 | Loss: 0.00248773
Iteration 14/1000 | Loss: 0.00331355
Iteration 15/1000 | Loss: 0.00291414
Iteration 16/1000 | Loss: 0.00187788
Iteration 17/1000 | Loss: 0.00221236
Iteration 18/1000 | Loss: 0.00274506
Iteration 19/1000 | Loss: 0.00323853
Iteration 20/1000 | Loss: 0.00203098
Iteration 21/1000 | Loss: 0.00343330
Iteration 22/1000 | Loss: 0.00214020
Iteration 23/1000 | Loss: 0.00298907
Iteration 24/1000 | Loss: 0.00183682
Iteration 25/1000 | Loss: 0.00150826
Iteration 26/1000 | Loss: 0.00188979
Iteration 27/1000 | Loss: 0.00059352
Iteration 28/1000 | Loss: 0.00033244
Iteration 29/1000 | Loss: 0.00061252
Iteration 30/1000 | Loss: 0.00049199
Iteration 31/1000 | Loss: 0.00016367
Iteration 32/1000 | Loss: 0.00084305
Iteration 33/1000 | Loss: 0.00016081
Iteration 34/1000 | Loss: 0.00051069
Iteration 35/1000 | Loss: 0.00068435
Iteration 36/1000 | Loss: 0.00078951
Iteration 37/1000 | Loss: 0.00056533
Iteration 38/1000 | Loss: 0.00030034
Iteration 39/1000 | Loss: 0.00056087
Iteration 40/1000 | Loss: 0.00017305
Iteration 41/1000 | Loss: 0.00037416
Iteration 42/1000 | Loss: 0.00004246
Iteration 43/1000 | Loss: 0.00003766
Iteration 44/1000 | Loss: 0.00016733
Iteration 45/1000 | Loss: 0.00024982
Iteration 46/1000 | Loss: 0.00003408
Iteration 47/1000 | Loss: 0.00003073
Iteration 48/1000 | Loss: 0.00031837
Iteration 49/1000 | Loss: 0.00003261
Iteration 50/1000 | Loss: 0.00006050
Iteration 51/1000 | Loss: 0.00002721
Iteration 52/1000 | Loss: 0.00002613
Iteration 53/1000 | Loss: 0.00002520
Iteration 54/1000 | Loss: 0.00002446
Iteration 55/1000 | Loss: 0.00002381
Iteration 56/1000 | Loss: 0.00002335
Iteration 57/1000 | Loss: 0.00002289
Iteration 58/1000 | Loss: 0.00005346
Iteration 59/1000 | Loss: 0.00006356
Iteration 60/1000 | Loss: 0.00004036
Iteration 61/1000 | Loss: 0.00002737
Iteration 62/1000 | Loss: 0.00002541
Iteration 63/1000 | Loss: 0.00002357
Iteration 64/1000 | Loss: 0.00002259
Iteration 65/1000 | Loss: 0.00002159
Iteration 66/1000 | Loss: 0.00002026
Iteration 67/1000 | Loss: 0.00001982
Iteration 68/1000 | Loss: 0.00001960
Iteration 69/1000 | Loss: 0.00001954
Iteration 70/1000 | Loss: 0.00001937
Iteration 71/1000 | Loss: 0.00001936
Iteration 72/1000 | Loss: 0.00001916
Iteration 73/1000 | Loss: 0.00001896
Iteration 74/1000 | Loss: 0.00001896
Iteration 75/1000 | Loss: 0.00001895
Iteration 76/1000 | Loss: 0.00001894
Iteration 77/1000 | Loss: 0.00001893
Iteration 78/1000 | Loss: 0.00001892
Iteration 79/1000 | Loss: 0.00001891
Iteration 80/1000 | Loss: 0.00001891
Iteration 81/1000 | Loss: 0.00001890
Iteration 82/1000 | Loss: 0.00001888
Iteration 83/1000 | Loss: 0.00001887
Iteration 84/1000 | Loss: 0.00001887
Iteration 85/1000 | Loss: 0.00001887
Iteration 86/1000 | Loss: 0.00001886
Iteration 87/1000 | Loss: 0.00001886
Iteration 88/1000 | Loss: 0.00001886
Iteration 89/1000 | Loss: 0.00001885
Iteration 90/1000 | Loss: 0.00001884
Iteration 91/1000 | Loss: 0.00001883
Iteration 92/1000 | Loss: 0.00001883
Iteration 93/1000 | Loss: 0.00001882
Iteration 94/1000 | Loss: 0.00001882
Iteration 95/1000 | Loss: 0.00001882
Iteration 96/1000 | Loss: 0.00001881
Iteration 97/1000 | Loss: 0.00001881
Iteration 98/1000 | Loss: 0.00001881
Iteration 99/1000 | Loss: 0.00001881
Iteration 100/1000 | Loss: 0.00001881
Iteration 101/1000 | Loss: 0.00001881
Iteration 102/1000 | Loss: 0.00001880
Iteration 103/1000 | Loss: 0.00001880
Iteration 104/1000 | Loss: 0.00001879
Iteration 105/1000 | Loss: 0.00001879
Iteration 106/1000 | Loss: 0.00001879
Iteration 107/1000 | Loss: 0.00001879
Iteration 108/1000 | Loss: 0.00001878
Iteration 109/1000 | Loss: 0.00001878
Iteration 110/1000 | Loss: 0.00001878
Iteration 111/1000 | Loss: 0.00001878
Iteration 112/1000 | Loss: 0.00001877
Iteration 113/1000 | Loss: 0.00001877
Iteration 114/1000 | Loss: 0.00001877
Iteration 115/1000 | Loss: 0.00001876
Iteration 116/1000 | Loss: 0.00001876
Iteration 117/1000 | Loss: 0.00001875
Iteration 118/1000 | Loss: 0.00001875
Iteration 119/1000 | Loss: 0.00001874
Iteration 120/1000 | Loss: 0.00001874
Iteration 121/1000 | Loss: 0.00001874
Iteration 122/1000 | Loss: 0.00001874
Iteration 123/1000 | Loss: 0.00001874
Iteration 124/1000 | Loss: 0.00001874
Iteration 125/1000 | Loss: 0.00001874
Iteration 126/1000 | Loss: 0.00001874
Iteration 127/1000 | Loss: 0.00001874
Iteration 128/1000 | Loss: 0.00001874
Iteration 129/1000 | Loss: 0.00001874
Iteration 130/1000 | Loss: 0.00001874
Iteration 131/1000 | Loss: 0.00001873
Iteration 132/1000 | Loss: 0.00001873
Iteration 133/1000 | Loss: 0.00001873
Iteration 134/1000 | Loss: 0.00001873
Iteration 135/1000 | Loss: 0.00001873
Iteration 136/1000 | Loss: 0.00001873
Iteration 137/1000 | Loss: 0.00001872
Iteration 138/1000 | Loss: 0.00001872
Iteration 139/1000 | Loss: 0.00001872
Iteration 140/1000 | Loss: 0.00001872
Iteration 141/1000 | Loss: 0.00001872
Iteration 142/1000 | Loss: 0.00001872
Iteration 143/1000 | Loss: 0.00001871
Iteration 144/1000 | Loss: 0.00001871
Iteration 145/1000 | Loss: 0.00001871
Iteration 146/1000 | Loss: 0.00001871
Iteration 147/1000 | Loss: 0.00001871
Iteration 148/1000 | Loss: 0.00001871
Iteration 149/1000 | Loss: 0.00001871
Iteration 150/1000 | Loss: 0.00001871
Iteration 151/1000 | Loss: 0.00001870
Iteration 152/1000 | Loss: 0.00001870
Iteration 153/1000 | Loss: 0.00001870
Iteration 154/1000 | Loss: 0.00001870
Iteration 155/1000 | Loss: 0.00001870
Iteration 156/1000 | Loss: 0.00001869
Iteration 157/1000 | Loss: 0.00001869
Iteration 158/1000 | Loss: 0.00001869
Iteration 159/1000 | Loss: 0.00001868
Iteration 160/1000 | Loss: 0.00001868
Iteration 161/1000 | Loss: 0.00001868
Iteration 162/1000 | Loss: 0.00001868
Iteration 163/1000 | Loss: 0.00001867
Iteration 164/1000 | Loss: 0.00001867
Iteration 165/1000 | Loss: 0.00001867
Iteration 166/1000 | Loss: 0.00001867
Iteration 167/1000 | Loss: 0.00001867
Iteration 168/1000 | Loss: 0.00001867
Iteration 169/1000 | Loss: 0.00001867
Iteration 170/1000 | Loss: 0.00001867
Iteration 171/1000 | Loss: 0.00001867
Iteration 172/1000 | Loss: 0.00001867
Iteration 173/1000 | Loss: 0.00001867
Iteration 174/1000 | Loss: 0.00001867
Iteration 175/1000 | Loss: 0.00001867
Iteration 176/1000 | Loss: 0.00001866
Iteration 177/1000 | Loss: 0.00001866
Iteration 178/1000 | Loss: 0.00001866
Iteration 179/1000 | Loss: 0.00001866
Iteration 180/1000 | Loss: 0.00001866
Iteration 181/1000 | Loss: 0.00001866
Iteration 182/1000 | Loss: 0.00001866
Iteration 183/1000 | Loss: 0.00001866
Iteration 184/1000 | Loss: 0.00001866
Iteration 185/1000 | Loss: 0.00001866
Iteration 186/1000 | Loss: 0.00001865
Iteration 187/1000 | Loss: 0.00001865
Iteration 188/1000 | Loss: 0.00001865
Iteration 189/1000 | Loss: 0.00001865
Iteration 190/1000 | Loss: 0.00001864
Iteration 191/1000 | Loss: 0.00001864
Iteration 192/1000 | Loss: 0.00001864
Iteration 193/1000 | Loss: 0.00001864
Iteration 194/1000 | Loss: 0.00001863
Iteration 195/1000 | Loss: 0.00001863
Iteration 196/1000 | Loss: 0.00001863
Iteration 197/1000 | Loss: 0.00001862
Iteration 198/1000 | Loss: 0.00001862
Iteration 199/1000 | Loss: 0.00001862
Iteration 200/1000 | Loss: 0.00001862
Iteration 201/1000 | Loss: 0.00001862
Iteration 202/1000 | Loss: 0.00001862
Iteration 203/1000 | Loss: 0.00001862
Iteration 204/1000 | Loss: 0.00001862
Iteration 205/1000 | Loss: 0.00001861
Iteration 206/1000 | Loss: 0.00001861
Iteration 207/1000 | Loss: 0.00001861
Iteration 208/1000 | Loss: 0.00001861
Iteration 209/1000 | Loss: 0.00001861
Iteration 210/1000 | Loss: 0.00001861
Iteration 211/1000 | Loss: 0.00001861
Iteration 212/1000 | Loss: 0.00001861
Iteration 213/1000 | Loss: 0.00001860
Iteration 214/1000 | Loss: 0.00001860
Iteration 215/1000 | Loss: 0.00001860
Iteration 216/1000 | Loss: 0.00001860
Iteration 217/1000 | Loss: 0.00001859
Iteration 218/1000 | Loss: 0.00001859
Iteration 219/1000 | Loss: 0.00001859
Iteration 220/1000 | Loss: 0.00001859
Iteration 221/1000 | Loss: 0.00001859
Iteration 222/1000 | Loss: 0.00001859
Iteration 223/1000 | Loss: 0.00001859
Iteration 224/1000 | Loss: 0.00001859
Iteration 225/1000 | Loss: 0.00001859
Iteration 226/1000 | Loss: 0.00001859
Iteration 227/1000 | Loss: 0.00001859
Iteration 228/1000 | Loss: 0.00001859
Iteration 229/1000 | Loss: 0.00001859
Iteration 230/1000 | Loss: 0.00001859
Iteration 231/1000 | Loss: 0.00001859
Iteration 232/1000 | Loss: 0.00001859
Iteration 233/1000 | Loss: 0.00001859
Iteration 234/1000 | Loss: 0.00001858
Iteration 235/1000 | Loss: 0.00001858
Iteration 236/1000 | Loss: 0.00001858
Iteration 237/1000 | Loss: 0.00001858
Iteration 238/1000 | Loss: 0.00001858
Iteration 239/1000 | Loss: 0.00001858
Iteration 240/1000 | Loss: 0.00001858
Iteration 241/1000 | Loss: 0.00001858
Iteration 242/1000 | Loss: 0.00001858
Iteration 243/1000 | Loss: 0.00001858
Iteration 244/1000 | Loss: 0.00001858
Iteration 245/1000 | Loss: 0.00001858
Iteration 246/1000 | Loss: 0.00001857
Iteration 247/1000 | Loss: 0.00001857
Iteration 248/1000 | Loss: 0.00001857
Iteration 249/1000 | Loss: 0.00001857
Iteration 250/1000 | Loss: 0.00001857
Iteration 251/1000 | Loss: 0.00001857
Iteration 252/1000 | Loss: 0.00001857
Iteration 253/1000 | Loss: 0.00001857
Iteration 254/1000 | Loss: 0.00001857
Iteration 255/1000 | Loss: 0.00001857
Iteration 256/1000 | Loss: 0.00001857
Iteration 257/1000 | Loss: 0.00001856
Iteration 258/1000 | Loss: 0.00001856
Iteration 259/1000 | Loss: 0.00001856
Iteration 260/1000 | Loss: 0.00001856
Iteration 261/1000 | Loss: 0.00001856
Iteration 262/1000 | Loss: 0.00029652
Iteration 263/1000 | Loss: 0.00004314
Iteration 264/1000 | Loss: 0.00001882
Iteration 265/1000 | Loss: 0.00007064
Iteration 266/1000 | Loss: 0.00004167
Iteration 267/1000 | Loss: 0.00003072
Iteration 268/1000 | Loss: 0.00001870
Iteration 269/1000 | Loss: 0.00001866
Iteration 270/1000 | Loss: 0.00001863
Iteration 271/1000 | Loss: 0.00001861
Iteration 272/1000 | Loss: 0.00001861
Iteration 273/1000 | Loss: 0.00001861
Iteration 274/1000 | Loss: 0.00001861
Iteration 275/1000 | Loss: 0.00001861
Iteration 276/1000 | Loss: 0.00001861
Iteration 277/1000 | Loss: 0.00001861
Iteration 278/1000 | Loss: 0.00001861
Iteration 279/1000 | Loss: 0.00001861
Iteration 280/1000 | Loss: 0.00001861
Iteration 281/1000 | Loss: 0.00001860
Iteration 282/1000 | Loss: 0.00001860
Iteration 283/1000 | Loss: 0.00001860
Iteration 284/1000 | Loss: 0.00001860
Iteration 285/1000 | Loss: 0.00001860
Iteration 286/1000 | Loss: 0.00001859
Iteration 287/1000 | Loss: 0.00001859
Iteration 288/1000 | Loss: 0.00001856
Iteration 289/1000 | Loss: 0.00001855
Iteration 290/1000 | Loss: 0.00001855
Iteration 291/1000 | Loss: 0.00001854
Iteration 292/1000 | Loss: 0.00001854
Iteration 293/1000 | Loss: 0.00001854
Iteration 294/1000 | Loss: 0.00001854
Iteration 295/1000 | Loss: 0.00001853
Iteration 296/1000 | Loss: 0.00001853
Iteration 297/1000 | Loss: 0.00001853
Iteration 298/1000 | Loss: 0.00001853
Iteration 299/1000 | Loss: 0.00001853
Iteration 300/1000 | Loss: 0.00001853
Iteration 301/1000 | Loss: 0.00001852
Iteration 302/1000 | Loss: 0.00001852
Iteration 303/1000 | Loss: 0.00001852
Iteration 304/1000 | Loss: 0.00001852
Iteration 305/1000 | Loss: 0.00001852
Iteration 306/1000 | Loss: 0.00001852
Iteration 307/1000 | Loss: 0.00001852
Iteration 308/1000 | Loss: 0.00001852
Iteration 309/1000 | Loss: 0.00001852
Iteration 310/1000 | Loss: 0.00001852
Iteration 311/1000 | Loss: 0.00001852
Iteration 312/1000 | Loss: 0.00001852
Iteration 313/1000 | Loss: 0.00001852
Iteration 314/1000 | Loss: 0.00001851
Iteration 315/1000 | Loss: 0.00001851
Iteration 316/1000 | Loss: 0.00001851
Iteration 317/1000 | Loss: 0.00001851
Iteration 318/1000 | Loss: 0.00001851
Iteration 319/1000 | Loss: 0.00001851
Iteration 320/1000 | Loss: 0.00001851
Iteration 321/1000 | Loss: 0.00001851
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 321. Stopping optimization.
Last 5 losses: [1.8514225303079e-05, 1.8514225303079e-05, 1.8514225303079e-05, 1.8514225303079e-05, 1.8514225303079e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8514225303079e-05

Optimization complete. Final v2v error: 3.1240592002868652 mm

Highest mean error: 11.681008338928223 mm for frame 2

Lowest mean error: 2.7559194564819336 mm for frame 78

Saving results

Total time: 179.6182460784912
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_helen_posed_023/1066/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_helen_posed_023/1066.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_helen_posed_023/1066
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00628432
Iteration 2/25 | Loss: 0.00149531
Iteration 3/25 | Loss: 0.00135365
Iteration 4/25 | Loss: 0.00134177
Iteration 5/25 | Loss: 0.00133818
Iteration 6/25 | Loss: 0.00133769
Iteration 7/25 | Loss: 0.00133769
Iteration 8/25 | Loss: 0.00133769
Iteration 9/25 | Loss: 0.00133769
Iteration 10/25 | Loss: 0.00133769
Iteration 11/25 | Loss: 0.00133769
Iteration 12/25 | Loss: 0.00133769
Iteration 13/25 | Loss: 0.00133769
Iteration 14/25 | Loss: 0.00133769
Iteration 15/25 | Loss: 0.00133769
Iteration 16/25 | Loss: 0.00133769
Iteration 17/25 | Loss: 0.00133769
Iteration 18/25 | Loss: 0.00133769
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0013376929564401507, 0.0013376929564401507, 0.0013376929564401507, 0.0013376929564401507, 0.0013376929564401507]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013376929564401507

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.28533959
Iteration 2/25 | Loss: 0.00151616
Iteration 3/25 | Loss: 0.00151613
Iteration 4/25 | Loss: 0.00151613
Iteration 5/25 | Loss: 0.00151613
Iteration 6/25 | Loss: 0.00151613
Iteration 7/25 | Loss: 0.00151613
Iteration 8/25 | Loss: 0.00151613
Iteration 9/25 | Loss: 0.00151613
Iteration 10/25 | Loss: 0.00151613
Iteration 11/25 | Loss: 0.00151613
Iteration 12/25 | Loss: 0.00151613
Iteration 13/25 | Loss: 0.00151613
Iteration 14/25 | Loss: 0.00151613
Iteration 15/25 | Loss: 0.00151613
Iteration 16/25 | Loss: 0.00151613
Iteration 17/25 | Loss: 0.00151613
Iteration 18/25 | Loss: 0.00151613
Iteration 19/25 | Loss: 0.00151613
Iteration 20/25 | Loss: 0.00151613
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0015161270275712013, 0.0015161270275712013, 0.0015161270275712013, 0.0015161270275712013, 0.0015161270275712013]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015161270275712013

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00151613
Iteration 2/1000 | Loss: 0.00003606
Iteration 3/1000 | Loss: 0.00002747
Iteration 4/1000 | Loss: 0.00002557
Iteration 5/1000 | Loss: 0.00002467
Iteration 6/1000 | Loss: 0.00002402
Iteration 7/1000 | Loss: 0.00002370
Iteration 8/1000 | Loss: 0.00002328
Iteration 9/1000 | Loss: 0.00002282
Iteration 10/1000 | Loss: 0.00002256
Iteration 11/1000 | Loss: 0.00002235
Iteration 12/1000 | Loss: 0.00002214
Iteration 13/1000 | Loss: 0.00002196
Iteration 14/1000 | Loss: 0.00002174
Iteration 15/1000 | Loss: 0.00002172
Iteration 16/1000 | Loss: 0.00002164
Iteration 17/1000 | Loss: 0.00002149
Iteration 18/1000 | Loss: 0.00002149
Iteration 19/1000 | Loss: 0.00002144
Iteration 20/1000 | Loss: 0.00002137
Iteration 21/1000 | Loss: 0.00002134
Iteration 22/1000 | Loss: 0.00002134
Iteration 23/1000 | Loss: 0.00002129
Iteration 24/1000 | Loss: 0.00002129
Iteration 25/1000 | Loss: 0.00002127
Iteration 26/1000 | Loss: 0.00002126
Iteration 27/1000 | Loss: 0.00002126
Iteration 28/1000 | Loss: 0.00002125
Iteration 29/1000 | Loss: 0.00002124
Iteration 30/1000 | Loss: 0.00002122
Iteration 31/1000 | Loss: 0.00002121
Iteration 32/1000 | Loss: 0.00002118
Iteration 33/1000 | Loss: 0.00002115
Iteration 34/1000 | Loss: 0.00002115
Iteration 35/1000 | Loss: 0.00002114
Iteration 36/1000 | Loss: 0.00002114
Iteration 37/1000 | Loss: 0.00002114
Iteration 38/1000 | Loss: 0.00002113
Iteration 39/1000 | Loss: 0.00002113
Iteration 40/1000 | Loss: 0.00002113
Iteration 41/1000 | Loss: 0.00002113
Iteration 42/1000 | Loss: 0.00002113
Iteration 43/1000 | Loss: 0.00002113
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 43. Stopping optimization.
Last 5 losses: [2.112671427312307e-05, 2.112671427312307e-05, 2.112671427312307e-05, 2.112671427312307e-05, 2.112671427312307e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.112671427312307e-05

Optimization complete. Final v2v error: 3.8367631435394287 mm

Highest mean error: 4.124485969543457 mm for frame 205

Lowest mean error: 3.446901559829712 mm for frame 235

Saving results

Total time: 39.695245027542114
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_helen_posed_023/1046/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_helen_posed_023/1046.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_helen_posed_023/1046
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01017960
Iteration 2/25 | Loss: 0.00148920
Iteration 3/25 | Loss: 0.00127607
Iteration 4/25 | Loss: 0.00124894
Iteration 5/25 | Loss: 0.00124197
Iteration 6/25 | Loss: 0.00124169
Iteration 7/25 | Loss: 0.00124169
Iteration 8/25 | Loss: 0.00124169
Iteration 9/25 | Loss: 0.00124169
Iteration 10/25 | Loss: 0.00124169
Iteration 11/25 | Loss: 0.00124167
Iteration 12/25 | Loss: 0.00124167
Iteration 13/25 | Loss: 0.00124167
Iteration 14/25 | Loss: 0.00124167
Iteration 15/25 | Loss: 0.00124167
Iteration 16/25 | Loss: 0.00124167
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0012416653335094452, 0.0012416653335094452, 0.0012416653335094452, 0.0012416653335094452, 0.0012416653335094452]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012416653335094452

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.50907755
Iteration 2/25 | Loss: 0.00153935
Iteration 3/25 | Loss: 0.00153935
Iteration 4/25 | Loss: 0.00153934
Iteration 5/25 | Loss: 0.00153934
Iteration 6/25 | Loss: 0.00153934
Iteration 7/25 | Loss: 0.00153934
Iteration 8/25 | Loss: 0.00153934
Iteration 9/25 | Loss: 0.00153934
Iteration 10/25 | Loss: 0.00153934
Iteration 11/25 | Loss: 0.00153934
Iteration 12/25 | Loss: 0.00153934
Iteration 13/25 | Loss: 0.00153934
Iteration 14/25 | Loss: 0.00153934
Iteration 15/25 | Loss: 0.00153934
Iteration 16/25 | Loss: 0.00153934
Iteration 17/25 | Loss: 0.00153934
Iteration 18/25 | Loss: 0.00153934
Iteration 19/25 | Loss: 0.00153934
Iteration 20/25 | Loss: 0.00153934
Iteration 21/25 | Loss: 0.00153934
Iteration 22/25 | Loss: 0.00153934
Iteration 23/25 | Loss: 0.00153934
Iteration 24/25 | Loss: 0.00153934
Iteration 25/25 | Loss: 0.00153934

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00153934
Iteration 2/1000 | Loss: 0.00003764
Iteration 3/1000 | Loss: 0.00002544
Iteration 4/1000 | Loss: 0.00002125
Iteration 5/1000 | Loss: 0.00001972
Iteration 6/1000 | Loss: 0.00001888
Iteration 7/1000 | Loss: 0.00001818
Iteration 8/1000 | Loss: 0.00001767
Iteration 9/1000 | Loss: 0.00001730
Iteration 10/1000 | Loss: 0.00001695
Iteration 11/1000 | Loss: 0.00001659
Iteration 12/1000 | Loss: 0.00001638
Iteration 13/1000 | Loss: 0.00001615
Iteration 14/1000 | Loss: 0.00001605
Iteration 15/1000 | Loss: 0.00001605
Iteration 16/1000 | Loss: 0.00001604
Iteration 17/1000 | Loss: 0.00001601
Iteration 18/1000 | Loss: 0.00001600
Iteration 19/1000 | Loss: 0.00001600
Iteration 20/1000 | Loss: 0.00001600
Iteration 21/1000 | Loss: 0.00001599
Iteration 22/1000 | Loss: 0.00001599
Iteration 23/1000 | Loss: 0.00001598
Iteration 24/1000 | Loss: 0.00001598
Iteration 25/1000 | Loss: 0.00001597
Iteration 26/1000 | Loss: 0.00001596
Iteration 27/1000 | Loss: 0.00001596
Iteration 28/1000 | Loss: 0.00001595
Iteration 29/1000 | Loss: 0.00001595
Iteration 30/1000 | Loss: 0.00001593
Iteration 31/1000 | Loss: 0.00001593
Iteration 32/1000 | Loss: 0.00001593
Iteration 33/1000 | Loss: 0.00001592
Iteration 34/1000 | Loss: 0.00001592
Iteration 35/1000 | Loss: 0.00001591
Iteration 36/1000 | Loss: 0.00001591
Iteration 37/1000 | Loss: 0.00001591
Iteration 38/1000 | Loss: 0.00001590
Iteration 39/1000 | Loss: 0.00001590
Iteration 40/1000 | Loss: 0.00001590
Iteration 41/1000 | Loss: 0.00001589
Iteration 42/1000 | Loss: 0.00001589
Iteration 43/1000 | Loss: 0.00001589
Iteration 44/1000 | Loss: 0.00001589
Iteration 45/1000 | Loss: 0.00001589
Iteration 46/1000 | Loss: 0.00001589
Iteration 47/1000 | Loss: 0.00001589
Iteration 48/1000 | Loss: 0.00001589
Iteration 49/1000 | Loss: 0.00001589
Iteration 50/1000 | Loss: 0.00001589
Iteration 51/1000 | Loss: 0.00001588
Iteration 52/1000 | Loss: 0.00001588
Iteration 53/1000 | Loss: 0.00001587
Iteration 54/1000 | Loss: 0.00001587
Iteration 55/1000 | Loss: 0.00001586
Iteration 56/1000 | Loss: 0.00001586
Iteration 57/1000 | Loss: 0.00001585
Iteration 58/1000 | Loss: 0.00001585
Iteration 59/1000 | Loss: 0.00001585
Iteration 60/1000 | Loss: 0.00001585
Iteration 61/1000 | Loss: 0.00001585
Iteration 62/1000 | Loss: 0.00001584
Iteration 63/1000 | Loss: 0.00001584
Iteration 64/1000 | Loss: 0.00001583
Iteration 65/1000 | Loss: 0.00001583
Iteration 66/1000 | Loss: 0.00001583
Iteration 67/1000 | Loss: 0.00001583
Iteration 68/1000 | Loss: 0.00001582
Iteration 69/1000 | Loss: 0.00001582
Iteration 70/1000 | Loss: 0.00001582
Iteration 71/1000 | Loss: 0.00001582
Iteration 72/1000 | Loss: 0.00001582
Iteration 73/1000 | Loss: 0.00001582
Iteration 74/1000 | Loss: 0.00001582
Iteration 75/1000 | Loss: 0.00001582
Iteration 76/1000 | Loss: 0.00001582
Iteration 77/1000 | Loss: 0.00001582
Iteration 78/1000 | Loss: 0.00001581
Iteration 79/1000 | Loss: 0.00001581
Iteration 80/1000 | Loss: 0.00001581
Iteration 81/1000 | Loss: 0.00001581
Iteration 82/1000 | Loss: 0.00001580
Iteration 83/1000 | Loss: 0.00001580
Iteration 84/1000 | Loss: 0.00001580
Iteration 85/1000 | Loss: 0.00001580
Iteration 86/1000 | Loss: 0.00001580
Iteration 87/1000 | Loss: 0.00001580
Iteration 88/1000 | Loss: 0.00001580
Iteration 89/1000 | Loss: 0.00001580
Iteration 90/1000 | Loss: 0.00001580
Iteration 91/1000 | Loss: 0.00001580
Iteration 92/1000 | Loss: 0.00001580
Iteration 93/1000 | Loss: 0.00001580
Iteration 94/1000 | Loss: 0.00001580
Iteration 95/1000 | Loss: 0.00001580
Iteration 96/1000 | Loss: 0.00001579
Iteration 97/1000 | Loss: 0.00001579
Iteration 98/1000 | Loss: 0.00001579
Iteration 99/1000 | Loss: 0.00001579
Iteration 100/1000 | Loss: 0.00001579
Iteration 101/1000 | Loss: 0.00001579
Iteration 102/1000 | Loss: 0.00001579
Iteration 103/1000 | Loss: 0.00001579
Iteration 104/1000 | Loss: 0.00001579
Iteration 105/1000 | Loss: 0.00001578
Iteration 106/1000 | Loss: 0.00001578
Iteration 107/1000 | Loss: 0.00001578
Iteration 108/1000 | Loss: 0.00001578
Iteration 109/1000 | Loss: 0.00001578
Iteration 110/1000 | Loss: 0.00001578
Iteration 111/1000 | Loss: 0.00001578
Iteration 112/1000 | Loss: 0.00001578
Iteration 113/1000 | Loss: 0.00001578
Iteration 114/1000 | Loss: 0.00001578
Iteration 115/1000 | Loss: 0.00001578
Iteration 116/1000 | Loss: 0.00001578
Iteration 117/1000 | Loss: 0.00001578
Iteration 118/1000 | Loss: 0.00001578
Iteration 119/1000 | Loss: 0.00001578
Iteration 120/1000 | Loss: 0.00001578
Iteration 121/1000 | Loss: 0.00001578
Iteration 122/1000 | Loss: 0.00001578
Iteration 123/1000 | Loss: 0.00001578
Iteration 124/1000 | Loss: 0.00001578
Iteration 125/1000 | Loss: 0.00001578
Iteration 126/1000 | Loss: 0.00001578
Iteration 127/1000 | Loss: 0.00001578
Iteration 128/1000 | Loss: 0.00001578
Iteration 129/1000 | Loss: 0.00001578
Iteration 130/1000 | Loss: 0.00001578
Iteration 131/1000 | Loss: 0.00001578
Iteration 132/1000 | Loss: 0.00001578
Iteration 133/1000 | Loss: 0.00001578
Iteration 134/1000 | Loss: 0.00001578
Iteration 135/1000 | Loss: 0.00001578
Iteration 136/1000 | Loss: 0.00001578
Iteration 137/1000 | Loss: 0.00001578
Iteration 138/1000 | Loss: 0.00001578
Iteration 139/1000 | Loss: 0.00001578
Iteration 140/1000 | Loss: 0.00001578
Iteration 141/1000 | Loss: 0.00001578
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 141. Stopping optimization.
Last 5 losses: [1.5779583918629214e-05, 1.5779583918629214e-05, 1.5779583918629214e-05, 1.5779583918629214e-05, 1.5779583918629214e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5779583918629214e-05

Optimization complete. Final v2v error: 3.435070753097534 mm

Highest mean error: 4.020914077758789 mm for frame 111

Lowest mean error: 2.861978530883789 mm for frame 169

Saving results

Total time: 35.319435596466064
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_helen_posed_023/1087/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_helen_posed_023/1087.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_helen_posed_023/1087
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00587205
Iteration 2/25 | Loss: 0.00173370
Iteration 3/25 | Loss: 0.00140467
Iteration 4/25 | Loss: 0.00137641
Iteration 5/25 | Loss: 0.00137200
Iteration 6/25 | Loss: 0.00137147
Iteration 7/25 | Loss: 0.00137147
Iteration 8/25 | Loss: 0.00137147
Iteration 9/25 | Loss: 0.00137147
Iteration 10/25 | Loss: 0.00137147
Iteration 11/25 | Loss: 0.00137147
Iteration 12/25 | Loss: 0.00137147
Iteration 13/25 | Loss: 0.00137147
Iteration 14/25 | Loss: 0.00137147
Iteration 15/25 | Loss: 0.00137147
Iteration 16/25 | Loss: 0.00137147
Iteration 17/25 | Loss: 0.00137147
Iteration 18/25 | Loss: 0.00137147
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0013714722590520978, 0.0013714722590520978, 0.0013714722590520978, 0.0013714722590520978, 0.0013714722590520978]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013714722590520978

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.28095984
Iteration 2/25 | Loss: 0.00123849
Iteration 3/25 | Loss: 0.00123846
Iteration 4/25 | Loss: 0.00123846
Iteration 5/25 | Loss: 0.00123846
Iteration 6/25 | Loss: 0.00123846
Iteration 7/25 | Loss: 0.00123846
Iteration 8/25 | Loss: 0.00123846
Iteration 9/25 | Loss: 0.00123846
Iteration 10/25 | Loss: 0.00123846
Iteration 11/25 | Loss: 0.00123846
Iteration 12/25 | Loss: 0.00123846
Iteration 13/25 | Loss: 0.00123846
Iteration 14/25 | Loss: 0.00123846
Iteration 15/25 | Loss: 0.00123846
Iteration 16/25 | Loss: 0.00123846
Iteration 17/25 | Loss: 0.00123846
Iteration 18/25 | Loss: 0.00123846
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.001238462165929377, 0.001238462165929377, 0.001238462165929377, 0.001238462165929377, 0.001238462165929377]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001238462165929377

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00123846
Iteration 2/1000 | Loss: 0.00005252
Iteration 3/1000 | Loss: 0.00002952
Iteration 4/1000 | Loss: 0.00002535
Iteration 5/1000 | Loss: 0.00002394
Iteration 6/1000 | Loss: 0.00002311
Iteration 7/1000 | Loss: 0.00002240
Iteration 8/1000 | Loss: 0.00002202
Iteration 9/1000 | Loss: 0.00002175
Iteration 10/1000 | Loss: 0.00002160
Iteration 11/1000 | Loss: 0.00002155
Iteration 12/1000 | Loss: 0.00002154
Iteration 13/1000 | Loss: 0.00002154
Iteration 14/1000 | Loss: 0.00002152
Iteration 15/1000 | Loss: 0.00002132
Iteration 16/1000 | Loss: 0.00002119
Iteration 17/1000 | Loss: 0.00002110
Iteration 18/1000 | Loss: 0.00002109
Iteration 19/1000 | Loss: 0.00002105
Iteration 20/1000 | Loss: 0.00002104
Iteration 21/1000 | Loss: 0.00002103
Iteration 22/1000 | Loss: 0.00002100
Iteration 23/1000 | Loss: 0.00002100
Iteration 24/1000 | Loss: 0.00002100
Iteration 25/1000 | Loss: 0.00002100
Iteration 26/1000 | Loss: 0.00002100
Iteration 27/1000 | Loss: 0.00002100
Iteration 28/1000 | Loss: 0.00002100
Iteration 29/1000 | Loss: 0.00002100
Iteration 30/1000 | Loss: 0.00002099
Iteration 31/1000 | Loss: 0.00002096
Iteration 32/1000 | Loss: 0.00002095
Iteration 33/1000 | Loss: 0.00002094
Iteration 34/1000 | Loss: 0.00002093
Iteration 35/1000 | Loss: 0.00002092
Iteration 36/1000 | Loss: 0.00002092
Iteration 37/1000 | Loss: 0.00002091
Iteration 38/1000 | Loss: 0.00002091
Iteration 39/1000 | Loss: 0.00002090
Iteration 40/1000 | Loss: 0.00002090
Iteration 41/1000 | Loss: 0.00002090
Iteration 42/1000 | Loss: 0.00002088
Iteration 43/1000 | Loss: 0.00002088
Iteration 44/1000 | Loss: 0.00002088
Iteration 45/1000 | Loss: 0.00002088
Iteration 46/1000 | Loss: 0.00002088
Iteration 47/1000 | Loss: 0.00002088
Iteration 48/1000 | Loss: 0.00002088
Iteration 49/1000 | Loss: 0.00002088
Iteration 50/1000 | Loss: 0.00002088
Iteration 51/1000 | Loss: 0.00002087
Iteration 52/1000 | Loss: 0.00002087
Iteration 53/1000 | Loss: 0.00002087
Iteration 54/1000 | Loss: 0.00002087
Iteration 55/1000 | Loss: 0.00002087
Iteration 56/1000 | Loss: 0.00002086
Iteration 57/1000 | Loss: 0.00002085
Iteration 58/1000 | Loss: 0.00002085
Iteration 59/1000 | Loss: 0.00002085
Iteration 60/1000 | Loss: 0.00002085
Iteration 61/1000 | Loss: 0.00002084
Iteration 62/1000 | Loss: 0.00002084
Iteration 63/1000 | Loss: 0.00002084
Iteration 64/1000 | Loss: 0.00002084
Iteration 65/1000 | Loss: 0.00002084
Iteration 66/1000 | Loss: 0.00002084
Iteration 67/1000 | Loss: 0.00002084
Iteration 68/1000 | Loss: 0.00002084
Iteration 69/1000 | Loss: 0.00002084
Iteration 70/1000 | Loss: 0.00002084
Iteration 71/1000 | Loss: 0.00002084
Iteration 72/1000 | Loss: 0.00002084
Iteration 73/1000 | Loss: 0.00002084
Iteration 74/1000 | Loss: 0.00002084
Iteration 75/1000 | Loss: 0.00002083
Iteration 76/1000 | Loss: 0.00002083
Iteration 77/1000 | Loss: 0.00002083
Iteration 78/1000 | Loss: 0.00002083
Iteration 79/1000 | Loss: 0.00002083
Iteration 80/1000 | Loss: 0.00002083
Iteration 81/1000 | Loss: 0.00002083
Iteration 82/1000 | Loss: 0.00002083
Iteration 83/1000 | Loss: 0.00002082
Iteration 84/1000 | Loss: 0.00002082
Iteration 85/1000 | Loss: 0.00002082
Iteration 86/1000 | Loss: 0.00002082
Iteration 87/1000 | Loss: 0.00002082
Iteration 88/1000 | Loss: 0.00002082
Iteration 89/1000 | Loss: 0.00002082
Iteration 90/1000 | Loss: 0.00002082
Iteration 91/1000 | Loss: 0.00002082
Iteration 92/1000 | Loss: 0.00002082
Iteration 93/1000 | Loss: 0.00002082
Iteration 94/1000 | Loss: 0.00002082
Iteration 95/1000 | Loss: 0.00002082
Iteration 96/1000 | Loss: 0.00002082
Iteration 97/1000 | Loss: 0.00002082
Iteration 98/1000 | Loss: 0.00002081
Iteration 99/1000 | Loss: 0.00002081
Iteration 100/1000 | Loss: 0.00002081
Iteration 101/1000 | Loss: 0.00002081
Iteration 102/1000 | Loss: 0.00002081
Iteration 103/1000 | Loss: 0.00002081
Iteration 104/1000 | Loss: 0.00002081
Iteration 105/1000 | Loss: 0.00002081
Iteration 106/1000 | Loss: 0.00002081
Iteration 107/1000 | Loss: 0.00002081
Iteration 108/1000 | Loss: 0.00002080
Iteration 109/1000 | Loss: 0.00002080
Iteration 110/1000 | Loss: 0.00002080
Iteration 111/1000 | Loss: 0.00002080
Iteration 112/1000 | Loss: 0.00002080
Iteration 113/1000 | Loss: 0.00002080
Iteration 114/1000 | Loss: 0.00002079
Iteration 115/1000 | Loss: 0.00002079
Iteration 116/1000 | Loss: 0.00002079
Iteration 117/1000 | Loss: 0.00002079
Iteration 118/1000 | Loss: 0.00002079
Iteration 119/1000 | Loss: 0.00002079
Iteration 120/1000 | Loss: 0.00002079
Iteration 121/1000 | Loss: 0.00002079
Iteration 122/1000 | Loss: 0.00002079
Iteration 123/1000 | Loss: 0.00002079
Iteration 124/1000 | Loss: 0.00002078
Iteration 125/1000 | Loss: 0.00002078
Iteration 126/1000 | Loss: 0.00002078
Iteration 127/1000 | Loss: 0.00002078
Iteration 128/1000 | Loss: 0.00002077
Iteration 129/1000 | Loss: 0.00002077
Iteration 130/1000 | Loss: 0.00002077
Iteration 131/1000 | Loss: 0.00002077
Iteration 132/1000 | Loss: 0.00002077
Iteration 133/1000 | Loss: 0.00002077
Iteration 134/1000 | Loss: 0.00002077
Iteration 135/1000 | Loss: 0.00002076
Iteration 136/1000 | Loss: 0.00002076
Iteration 137/1000 | Loss: 0.00002076
Iteration 138/1000 | Loss: 0.00002076
Iteration 139/1000 | Loss: 0.00002076
Iteration 140/1000 | Loss: 0.00002076
Iteration 141/1000 | Loss: 0.00002076
Iteration 142/1000 | Loss: 0.00002076
Iteration 143/1000 | Loss: 0.00002075
Iteration 144/1000 | Loss: 0.00002075
Iteration 145/1000 | Loss: 0.00002075
Iteration 146/1000 | Loss: 0.00002075
Iteration 147/1000 | Loss: 0.00002075
Iteration 148/1000 | Loss: 0.00002075
Iteration 149/1000 | Loss: 0.00002075
Iteration 150/1000 | Loss: 0.00002075
Iteration 151/1000 | Loss: 0.00002074
Iteration 152/1000 | Loss: 0.00002074
Iteration 153/1000 | Loss: 0.00002074
Iteration 154/1000 | Loss: 0.00002074
Iteration 155/1000 | Loss: 0.00002074
Iteration 156/1000 | Loss: 0.00002074
Iteration 157/1000 | Loss: 0.00002074
Iteration 158/1000 | Loss: 0.00002074
Iteration 159/1000 | Loss: 0.00002074
Iteration 160/1000 | Loss: 0.00002074
Iteration 161/1000 | Loss: 0.00002074
Iteration 162/1000 | Loss: 0.00002074
Iteration 163/1000 | Loss: 0.00002074
Iteration 164/1000 | Loss: 0.00002074
Iteration 165/1000 | Loss: 0.00002074
Iteration 166/1000 | Loss: 0.00002074
Iteration 167/1000 | Loss: 0.00002074
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 167. Stopping optimization.
Last 5 losses: [2.0740542822750285e-05, 2.0740542822750285e-05, 2.0740542822750285e-05, 2.0740542822750285e-05, 2.0740542822750285e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0740542822750285e-05

Optimization complete. Final v2v error: 3.880552053451538 mm

Highest mean error: 4.017056465148926 mm for frame 38

Lowest mean error: 3.443905830383301 mm for frame 4

Saving results

Total time: 37.25799298286438
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_helen_posed_023/1035/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_helen_posed_023/1035.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_helen_posed_023/1035
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00765423
Iteration 2/25 | Loss: 0.00142954
Iteration 3/25 | Loss: 0.00128760
Iteration 4/25 | Loss: 0.00127009
Iteration 5/25 | Loss: 0.00126588
Iteration 6/25 | Loss: 0.00126559
Iteration 7/25 | Loss: 0.00126559
Iteration 8/25 | Loss: 0.00126559
Iteration 9/25 | Loss: 0.00126559
Iteration 10/25 | Loss: 0.00126559
Iteration 11/25 | Loss: 0.00126559
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012655863538384438, 0.0012655863538384438, 0.0012655863538384438, 0.0012655863538384438, 0.0012655863538384438]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012655863538384438

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.11502099
Iteration 2/25 | Loss: 0.00109014
Iteration 3/25 | Loss: 0.00109014
Iteration 4/25 | Loss: 0.00109014
Iteration 5/25 | Loss: 0.00109014
Iteration 6/25 | Loss: 0.00109014
Iteration 7/25 | Loss: 0.00109014
Iteration 8/25 | Loss: 0.00109014
Iteration 9/25 | Loss: 0.00109014
Iteration 10/25 | Loss: 0.00109014
Iteration 11/25 | Loss: 0.00109014
Iteration 12/25 | Loss: 0.00109014
Iteration 13/25 | Loss: 0.00109014
Iteration 14/25 | Loss: 0.00109014
Iteration 15/25 | Loss: 0.00109014
Iteration 16/25 | Loss: 0.00109014
Iteration 17/25 | Loss: 0.00109014
Iteration 18/25 | Loss: 0.00109014
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.001090137055143714, 0.001090137055143714, 0.001090137055143714, 0.001090137055143714, 0.001090137055143714]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001090137055143714

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00109014
Iteration 2/1000 | Loss: 0.00004327
Iteration 3/1000 | Loss: 0.00002641
Iteration 4/1000 | Loss: 0.00002157
Iteration 5/1000 | Loss: 0.00001964
Iteration 6/1000 | Loss: 0.00001886
Iteration 7/1000 | Loss: 0.00001819
Iteration 8/1000 | Loss: 0.00001781
Iteration 9/1000 | Loss: 0.00001757
Iteration 10/1000 | Loss: 0.00001728
Iteration 11/1000 | Loss: 0.00001706
Iteration 12/1000 | Loss: 0.00001691
Iteration 13/1000 | Loss: 0.00001686
Iteration 14/1000 | Loss: 0.00001673
Iteration 15/1000 | Loss: 0.00001671
Iteration 16/1000 | Loss: 0.00001669
Iteration 17/1000 | Loss: 0.00001665
Iteration 18/1000 | Loss: 0.00001658
Iteration 19/1000 | Loss: 0.00001657
Iteration 20/1000 | Loss: 0.00001657
Iteration 21/1000 | Loss: 0.00001655
Iteration 22/1000 | Loss: 0.00001654
Iteration 23/1000 | Loss: 0.00001653
Iteration 24/1000 | Loss: 0.00001653
Iteration 25/1000 | Loss: 0.00001653
Iteration 26/1000 | Loss: 0.00001652
Iteration 27/1000 | Loss: 0.00001651
Iteration 28/1000 | Loss: 0.00001648
Iteration 29/1000 | Loss: 0.00001648
Iteration 30/1000 | Loss: 0.00001647
Iteration 31/1000 | Loss: 0.00001647
Iteration 32/1000 | Loss: 0.00001644
Iteration 33/1000 | Loss: 0.00001643
Iteration 34/1000 | Loss: 0.00001643
Iteration 35/1000 | Loss: 0.00001642
Iteration 36/1000 | Loss: 0.00001642
Iteration 37/1000 | Loss: 0.00001642
Iteration 38/1000 | Loss: 0.00001642
Iteration 39/1000 | Loss: 0.00001642
Iteration 40/1000 | Loss: 0.00001642
Iteration 41/1000 | Loss: 0.00001641
Iteration 42/1000 | Loss: 0.00001641
Iteration 43/1000 | Loss: 0.00001640
Iteration 44/1000 | Loss: 0.00001639
Iteration 45/1000 | Loss: 0.00001639
Iteration 46/1000 | Loss: 0.00001639
Iteration 47/1000 | Loss: 0.00001639
Iteration 48/1000 | Loss: 0.00001639
Iteration 49/1000 | Loss: 0.00001638
Iteration 50/1000 | Loss: 0.00001638
Iteration 51/1000 | Loss: 0.00001638
Iteration 52/1000 | Loss: 0.00001638
Iteration 53/1000 | Loss: 0.00001637
Iteration 54/1000 | Loss: 0.00001637
Iteration 55/1000 | Loss: 0.00001637
Iteration 56/1000 | Loss: 0.00001637
Iteration 57/1000 | Loss: 0.00001637
Iteration 58/1000 | Loss: 0.00001636
Iteration 59/1000 | Loss: 0.00001636
Iteration 60/1000 | Loss: 0.00001636
Iteration 61/1000 | Loss: 0.00001635
Iteration 62/1000 | Loss: 0.00001635
Iteration 63/1000 | Loss: 0.00001635
Iteration 64/1000 | Loss: 0.00001634
Iteration 65/1000 | Loss: 0.00001634
Iteration 66/1000 | Loss: 0.00001634
Iteration 67/1000 | Loss: 0.00001634
Iteration 68/1000 | Loss: 0.00001634
Iteration 69/1000 | Loss: 0.00001634
Iteration 70/1000 | Loss: 0.00001633
Iteration 71/1000 | Loss: 0.00001633
Iteration 72/1000 | Loss: 0.00001632
Iteration 73/1000 | Loss: 0.00001632
Iteration 74/1000 | Loss: 0.00001632
Iteration 75/1000 | Loss: 0.00001632
Iteration 76/1000 | Loss: 0.00001632
Iteration 77/1000 | Loss: 0.00001632
Iteration 78/1000 | Loss: 0.00001632
Iteration 79/1000 | Loss: 0.00001632
Iteration 80/1000 | Loss: 0.00001631
Iteration 81/1000 | Loss: 0.00001631
Iteration 82/1000 | Loss: 0.00001629
Iteration 83/1000 | Loss: 0.00001629
Iteration 84/1000 | Loss: 0.00001629
Iteration 85/1000 | Loss: 0.00001629
Iteration 86/1000 | Loss: 0.00001629
Iteration 87/1000 | Loss: 0.00001629
Iteration 88/1000 | Loss: 0.00001629
Iteration 89/1000 | Loss: 0.00001629
Iteration 90/1000 | Loss: 0.00001629
Iteration 91/1000 | Loss: 0.00001629
Iteration 92/1000 | Loss: 0.00001629
Iteration 93/1000 | Loss: 0.00001628
Iteration 94/1000 | Loss: 0.00001628
Iteration 95/1000 | Loss: 0.00001628
Iteration 96/1000 | Loss: 0.00001627
Iteration 97/1000 | Loss: 0.00001627
Iteration 98/1000 | Loss: 0.00001627
Iteration 99/1000 | Loss: 0.00001627
Iteration 100/1000 | Loss: 0.00001626
Iteration 101/1000 | Loss: 0.00001626
Iteration 102/1000 | Loss: 0.00001626
Iteration 103/1000 | Loss: 0.00001626
Iteration 104/1000 | Loss: 0.00001626
Iteration 105/1000 | Loss: 0.00001626
Iteration 106/1000 | Loss: 0.00001626
Iteration 107/1000 | Loss: 0.00001626
Iteration 108/1000 | Loss: 0.00001626
Iteration 109/1000 | Loss: 0.00001626
Iteration 110/1000 | Loss: 0.00001625
Iteration 111/1000 | Loss: 0.00001625
Iteration 112/1000 | Loss: 0.00001625
Iteration 113/1000 | Loss: 0.00001625
Iteration 114/1000 | Loss: 0.00001625
Iteration 115/1000 | Loss: 0.00001625
Iteration 116/1000 | Loss: 0.00001625
Iteration 117/1000 | Loss: 0.00001625
Iteration 118/1000 | Loss: 0.00001624
Iteration 119/1000 | Loss: 0.00001624
Iteration 120/1000 | Loss: 0.00001624
Iteration 121/1000 | Loss: 0.00001623
Iteration 122/1000 | Loss: 0.00001623
Iteration 123/1000 | Loss: 0.00001623
Iteration 124/1000 | Loss: 0.00001623
Iteration 125/1000 | Loss: 0.00001623
Iteration 126/1000 | Loss: 0.00001623
Iteration 127/1000 | Loss: 0.00001622
Iteration 128/1000 | Loss: 0.00001622
Iteration 129/1000 | Loss: 0.00001622
Iteration 130/1000 | Loss: 0.00001622
Iteration 131/1000 | Loss: 0.00001622
Iteration 132/1000 | Loss: 0.00001622
Iteration 133/1000 | Loss: 0.00001622
Iteration 134/1000 | Loss: 0.00001621
Iteration 135/1000 | Loss: 0.00001621
Iteration 136/1000 | Loss: 0.00001621
Iteration 137/1000 | Loss: 0.00001620
Iteration 138/1000 | Loss: 0.00001620
Iteration 139/1000 | Loss: 0.00001620
Iteration 140/1000 | Loss: 0.00001620
Iteration 141/1000 | Loss: 0.00001619
Iteration 142/1000 | Loss: 0.00001619
Iteration 143/1000 | Loss: 0.00001619
Iteration 144/1000 | Loss: 0.00001619
Iteration 145/1000 | Loss: 0.00001618
Iteration 146/1000 | Loss: 0.00001618
Iteration 147/1000 | Loss: 0.00001618
Iteration 148/1000 | Loss: 0.00001618
Iteration 149/1000 | Loss: 0.00001617
Iteration 150/1000 | Loss: 0.00001617
Iteration 151/1000 | Loss: 0.00001617
Iteration 152/1000 | Loss: 0.00001617
Iteration 153/1000 | Loss: 0.00001617
Iteration 154/1000 | Loss: 0.00001616
Iteration 155/1000 | Loss: 0.00001616
Iteration 156/1000 | Loss: 0.00001616
Iteration 157/1000 | Loss: 0.00001616
Iteration 158/1000 | Loss: 0.00001616
Iteration 159/1000 | Loss: 0.00001616
Iteration 160/1000 | Loss: 0.00001616
Iteration 161/1000 | Loss: 0.00001615
Iteration 162/1000 | Loss: 0.00001615
Iteration 163/1000 | Loss: 0.00001615
Iteration 164/1000 | Loss: 0.00001615
Iteration 165/1000 | Loss: 0.00001614
Iteration 166/1000 | Loss: 0.00001614
Iteration 167/1000 | Loss: 0.00001614
Iteration 168/1000 | Loss: 0.00001614
Iteration 169/1000 | Loss: 0.00001614
Iteration 170/1000 | Loss: 0.00001614
Iteration 171/1000 | Loss: 0.00001614
Iteration 172/1000 | Loss: 0.00001614
Iteration 173/1000 | Loss: 0.00001614
Iteration 174/1000 | Loss: 0.00001614
Iteration 175/1000 | Loss: 0.00001614
Iteration 176/1000 | Loss: 0.00001613
Iteration 177/1000 | Loss: 0.00001613
Iteration 178/1000 | Loss: 0.00001613
Iteration 179/1000 | Loss: 0.00001613
Iteration 180/1000 | Loss: 0.00001612
Iteration 181/1000 | Loss: 0.00001612
Iteration 182/1000 | Loss: 0.00001612
Iteration 183/1000 | Loss: 0.00001612
Iteration 184/1000 | Loss: 0.00001612
Iteration 185/1000 | Loss: 0.00001612
Iteration 186/1000 | Loss: 0.00001612
Iteration 187/1000 | Loss: 0.00001611
Iteration 188/1000 | Loss: 0.00001611
Iteration 189/1000 | Loss: 0.00001611
Iteration 190/1000 | Loss: 0.00001611
Iteration 191/1000 | Loss: 0.00001611
Iteration 192/1000 | Loss: 0.00001611
Iteration 193/1000 | Loss: 0.00001611
Iteration 194/1000 | Loss: 0.00001611
Iteration 195/1000 | Loss: 0.00001611
Iteration 196/1000 | Loss: 0.00001611
Iteration 197/1000 | Loss: 0.00001611
Iteration 198/1000 | Loss: 0.00001611
Iteration 199/1000 | Loss: 0.00001611
Iteration 200/1000 | Loss: 0.00001611
Iteration 201/1000 | Loss: 0.00001611
Iteration 202/1000 | Loss: 0.00001611
Iteration 203/1000 | Loss: 0.00001611
Iteration 204/1000 | Loss: 0.00001611
Iteration 205/1000 | Loss: 0.00001611
Iteration 206/1000 | Loss: 0.00001611
Iteration 207/1000 | Loss: 0.00001610
Iteration 208/1000 | Loss: 0.00001610
Iteration 209/1000 | Loss: 0.00001610
Iteration 210/1000 | Loss: 0.00001610
Iteration 211/1000 | Loss: 0.00001610
Iteration 212/1000 | Loss: 0.00001610
Iteration 213/1000 | Loss: 0.00001610
Iteration 214/1000 | Loss: 0.00001610
Iteration 215/1000 | Loss: 0.00001610
Iteration 216/1000 | Loss: 0.00001610
Iteration 217/1000 | Loss: 0.00001610
Iteration 218/1000 | Loss: 0.00001610
Iteration 219/1000 | Loss: 0.00001610
Iteration 220/1000 | Loss: 0.00001610
Iteration 221/1000 | Loss: 0.00001610
Iteration 222/1000 | Loss: 0.00001610
Iteration 223/1000 | Loss: 0.00001610
Iteration 224/1000 | Loss: 0.00001610
Iteration 225/1000 | Loss: 0.00001610
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 225. Stopping optimization.
Last 5 losses: [1.6095405953819863e-05, 1.6095405953819863e-05, 1.6095405953819863e-05, 1.6095405953819863e-05, 1.6095405953819863e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6095405953819863e-05

Optimization complete. Final v2v error: 3.3574740886688232 mm

Highest mean error: 4.418063640594482 mm for frame 210

Lowest mean error: 2.7757408618927 mm for frame 153

Saving results

Total time: 49.47404193878174
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_helen_posed_023/1093/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_helen_posed_023/1093.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_helen_posed_023/1093
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00794111
Iteration 2/25 | Loss: 0.00155489
Iteration 3/25 | Loss: 0.00136403
Iteration 4/25 | Loss: 0.00133055
Iteration 5/25 | Loss: 0.00131926
Iteration 6/25 | Loss: 0.00131626
Iteration 7/25 | Loss: 0.00131578
Iteration 8/25 | Loss: 0.00131418
Iteration 9/25 | Loss: 0.00131400
Iteration 10/25 | Loss: 0.00131494
Iteration 11/25 | Loss: 0.00131413
Iteration 12/25 | Loss: 0.00131400
Iteration 13/25 | Loss: 0.00131400
Iteration 14/25 | Loss: 0.00131396
Iteration 15/25 | Loss: 0.00131396
Iteration 16/25 | Loss: 0.00131396
Iteration 17/25 | Loss: 0.00131396
Iteration 18/25 | Loss: 0.00131396
Iteration 19/25 | Loss: 0.00131396
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0013139587827026844, 0.0013139587827026844, 0.0013139587827026844, 0.0013139587827026844, 0.0013139587827026844]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013139587827026844

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.41996396
Iteration 2/25 | Loss: 0.00189092
Iteration 3/25 | Loss: 0.00188948
Iteration 4/25 | Loss: 0.00188948
Iteration 5/25 | Loss: 0.00188948
Iteration 6/25 | Loss: 0.00188948
Iteration 7/25 | Loss: 0.00188948
Iteration 8/25 | Loss: 0.00188948
Iteration 9/25 | Loss: 0.00188948
Iteration 10/25 | Loss: 0.00188948
Iteration 11/25 | Loss: 0.00188948
Iteration 12/25 | Loss: 0.00188948
Iteration 13/25 | Loss: 0.00188948
Iteration 14/25 | Loss: 0.00188948
Iteration 15/25 | Loss: 0.00188948
Iteration 16/25 | Loss: 0.00188948
Iteration 17/25 | Loss: 0.00188948
Iteration 18/25 | Loss: 0.00188948
Iteration 19/25 | Loss: 0.00188948
Iteration 20/25 | Loss: 0.00188948
Iteration 21/25 | Loss: 0.00188948
Iteration 22/25 | Loss: 0.00188948
Iteration 23/25 | Loss: 0.00188948
Iteration 24/25 | Loss: 0.00188948
Iteration 25/25 | Loss: 0.00188948

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00188948
Iteration 2/1000 | Loss: 0.00007287
Iteration 3/1000 | Loss: 0.00003832
Iteration 4/1000 | Loss: 0.00002826
Iteration 5/1000 | Loss: 0.00002566
Iteration 6/1000 | Loss: 0.00002401
Iteration 7/1000 | Loss: 0.00002273
Iteration 8/1000 | Loss: 0.00002202
Iteration 9/1000 | Loss: 0.00002136
Iteration 10/1000 | Loss: 0.00002107
Iteration 11/1000 | Loss: 0.00002066
Iteration 12/1000 | Loss: 0.00002037
Iteration 13/1000 | Loss: 0.00002019
Iteration 14/1000 | Loss: 0.00002005
Iteration 15/1000 | Loss: 0.00001994
Iteration 16/1000 | Loss: 0.00001985
Iteration 17/1000 | Loss: 0.00001983
Iteration 18/1000 | Loss: 0.00001979
Iteration 19/1000 | Loss: 0.00001977
Iteration 20/1000 | Loss: 0.00001976
Iteration 21/1000 | Loss: 0.00001976
Iteration 22/1000 | Loss: 0.00001975
Iteration 23/1000 | Loss: 0.00001975
Iteration 24/1000 | Loss: 0.00001975
Iteration 25/1000 | Loss: 0.00001975
Iteration 26/1000 | Loss: 0.00001975
Iteration 27/1000 | Loss: 0.00001975
Iteration 28/1000 | Loss: 0.00001974
Iteration 29/1000 | Loss: 0.00001974
Iteration 30/1000 | Loss: 0.00001974
Iteration 31/1000 | Loss: 0.00001973
Iteration 32/1000 | Loss: 0.00001973
Iteration 33/1000 | Loss: 0.00001973
Iteration 34/1000 | Loss: 0.00001973
Iteration 35/1000 | Loss: 0.00001972
Iteration 36/1000 | Loss: 0.00001972
Iteration 37/1000 | Loss: 0.00001971
Iteration 38/1000 | Loss: 0.00001971
Iteration 39/1000 | Loss: 0.00001971
Iteration 40/1000 | Loss: 0.00001971
Iteration 41/1000 | Loss: 0.00001970
Iteration 42/1000 | Loss: 0.00001970
Iteration 43/1000 | Loss: 0.00001970
Iteration 44/1000 | Loss: 0.00001970
Iteration 45/1000 | Loss: 0.00001969
Iteration 46/1000 | Loss: 0.00001969
Iteration 47/1000 | Loss: 0.00001969
Iteration 48/1000 | Loss: 0.00001969
Iteration 49/1000 | Loss: 0.00001968
Iteration 50/1000 | Loss: 0.00001968
Iteration 51/1000 | Loss: 0.00001968
Iteration 52/1000 | Loss: 0.00001968
Iteration 53/1000 | Loss: 0.00001967
Iteration 54/1000 | Loss: 0.00001967
Iteration 55/1000 | Loss: 0.00001967
Iteration 56/1000 | Loss: 0.00001967
Iteration 57/1000 | Loss: 0.00001966
Iteration 58/1000 | Loss: 0.00001966
Iteration 59/1000 | Loss: 0.00001966
Iteration 60/1000 | Loss: 0.00001966
Iteration 61/1000 | Loss: 0.00001966
Iteration 62/1000 | Loss: 0.00001965
Iteration 63/1000 | Loss: 0.00001965
Iteration 64/1000 | Loss: 0.00001965
Iteration 65/1000 | Loss: 0.00001965
Iteration 66/1000 | Loss: 0.00001965
Iteration 67/1000 | Loss: 0.00001965
Iteration 68/1000 | Loss: 0.00001965
Iteration 69/1000 | Loss: 0.00001965
Iteration 70/1000 | Loss: 0.00001965
Iteration 71/1000 | Loss: 0.00001965
Iteration 72/1000 | Loss: 0.00001964
Iteration 73/1000 | Loss: 0.00001964
Iteration 74/1000 | Loss: 0.00001964
Iteration 75/1000 | Loss: 0.00001964
Iteration 76/1000 | Loss: 0.00001964
Iteration 77/1000 | Loss: 0.00001963
Iteration 78/1000 | Loss: 0.00001963
Iteration 79/1000 | Loss: 0.00001963
Iteration 80/1000 | Loss: 0.00001963
Iteration 81/1000 | Loss: 0.00001963
Iteration 82/1000 | Loss: 0.00001963
Iteration 83/1000 | Loss: 0.00001963
Iteration 84/1000 | Loss: 0.00001963
Iteration 85/1000 | Loss: 0.00001962
Iteration 86/1000 | Loss: 0.00001962
Iteration 87/1000 | Loss: 0.00001962
Iteration 88/1000 | Loss: 0.00001962
Iteration 89/1000 | Loss: 0.00001962
Iteration 90/1000 | Loss: 0.00001962
Iteration 91/1000 | Loss: 0.00001962
Iteration 92/1000 | Loss: 0.00001962
Iteration 93/1000 | Loss: 0.00001962
Iteration 94/1000 | Loss: 0.00001962
Iteration 95/1000 | Loss: 0.00001962
Iteration 96/1000 | Loss: 0.00001961
Iteration 97/1000 | Loss: 0.00001961
Iteration 98/1000 | Loss: 0.00001961
Iteration 99/1000 | Loss: 0.00001961
Iteration 100/1000 | Loss: 0.00001961
Iteration 101/1000 | Loss: 0.00001961
Iteration 102/1000 | Loss: 0.00001961
Iteration 103/1000 | Loss: 0.00001961
Iteration 104/1000 | Loss: 0.00001961
Iteration 105/1000 | Loss: 0.00001961
Iteration 106/1000 | Loss: 0.00001961
Iteration 107/1000 | Loss: 0.00001961
Iteration 108/1000 | Loss: 0.00001961
Iteration 109/1000 | Loss: 0.00001961
Iteration 110/1000 | Loss: 0.00001961
Iteration 111/1000 | Loss: 0.00001961
Iteration 112/1000 | Loss: 0.00001960
Iteration 113/1000 | Loss: 0.00001960
Iteration 114/1000 | Loss: 0.00001960
Iteration 115/1000 | Loss: 0.00001960
Iteration 116/1000 | Loss: 0.00001960
Iteration 117/1000 | Loss: 0.00001960
Iteration 118/1000 | Loss: 0.00001960
Iteration 119/1000 | Loss: 0.00001960
Iteration 120/1000 | Loss: 0.00001960
Iteration 121/1000 | Loss: 0.00001960
Iteration 122/1000 | Loss: 0.00001960
Iteration 123/1000 | Loss: 0.00001960
Iteration 124/1000 | Loss: 0.00001960
Iteration 125/1000 | Loss: 0.00001960
Iteration 126/1000 | Loss: 0.00001960
Iteration 127/1000 | Loss: 0.00001959
Iteration 128/1000 | Loss: 0.00001959
Iteration 129/1000 | Loss: 0.00001959
Iteration 130/1000 | Loss: 0.00001959
Iteration 131/1000 | Loss: 0.00001959
Iteration 132/1000 | Loss: 0.00001959
Iteration 133/1000 | Loss: 0.00001959
Iteration 134/1000 | Loss: 0.00001959
Iteration 135/1000 | Loss: 0.00001958
Iteration 136/1000 | Loss: 0.00001958
Iteration 137/1000 | Loss: 0.00001958
Iteration 138/1000 | Loss: 0.00001958
Iteration 139/1000 | Loss: 0.00001958
Iteration 140/1000 | Loss: 0.00001958
Iteration 141/1000 | Loss: 0.00001958
Iteration 142/1000 | Loss: 0.00001958
Iteration 143/1000 | Loss: 0.00001958
Iteration 144/1000 | Loss: 0.00001958
Iteration 145/1000 | Loss: 0.00001958
Iteration 146/1000 | Loss: 0.00001958
Iteration 147/1000 | Loss: 0.00001957
Iteration 148/1000 | Loss: 0.00001957
Iteration 149/1000 | Loss: 0.00001957
Iteration 150/1000 | Loss: 0.00001957
Iteration 151/1000 | Loss: 0.00001957
Iteration 152/1000 | Loss: 0.00001957
Iteration 153/1000 | Loss: 0.00001957
Iteration 154/1000 | Loss: 0.00001957
Iteration 155/1000 | Loss: 0.00001957
Iteration 156/1000 | Loss: 0.00001957
Iteration 157/1000 | Loss: 0.00001957
Iteration 158/1000 | Loss: 0.00001957
Iteration 159/1000 | Loss: 0.00001957
Iteration 160/1000 | Loss: 0.00001957
Iteration 161/1000 | Loss: 0.00001956
Iteration 162/1000 | Loss: 0.00001956
Iteration 163/1000 | Loss: 0.00001956
Iteration 164/1000 | Loss: 0.00001956
Iteration 165/1000 | Loss: 0.00001956
Iteration 166/1000 | Loss: 0.00001956
Iteration 167/1000 | Loss: 0.00001956
Iteration 168/1000 | Loss: 0.00001956
Iteration 169/1000 | Loss: 0.00001956
Iteration 170/1000 | Loss: 0.00001956
Iteration 171/1000 | Loss: 0.00001956
Iteration 172/1000 | Loss: 0.00001955
Iteration 173/1000 | Loss: 0.00001955
Iteration 174/1000 | Loss: 0.00001955
Iteration 175/1000 | Loss: 0.00001955
Iteration 176/1000 | Loss: 0.00001955
Iteration 177/1000 | Loss: 0.00001955
Iteration 178/1000 | Loss: 0.00001955
Iteration 179/1000 | Loss: 0.00001955
Iteration 180/1000 | Loss: 0.00001955
Iteration 181/1000 | Loss: 0.00001955
Iteration 182/1000 | Loss: 0.00001955
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 182. Stopping optimization.
Last 5 losses: [1.955273182829842e-05, 1.955273182829842e-05, 1.955273182829842e-05, 1.955273182829842e-05, 1.955273182829842e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.955273182829842e-05

Optimization complete. Final v2v error: 3.7615251541137695 mm

Highest mean error: 3.9766669273376465 mm for frame 71

Lowest mean error: 3.4919915199279785 mm for frame 116

Saving results

Total time: 59.20582318305969
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_helen_posed_023/1098/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_helen_posed_023/1098.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_helen_posed_023/1098
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00776558
Iteration 2/25 | Loss: 0.00138631
Iteration 3/25 | Loss: 0.00122018
Iteration 4/25 | Loss: 0.00120108
Iteration 5/25 | Loss: 0.00119764
Iteration 6/25 | Loss: 0.00119764
Iteration 7/25 | Loss: 0.00119764
Iteration 8/25 | Loss: 0.00119764
Iteration 9/25 | Loss: 0.00119764
Iteration 10/25 | Loss: 0.00119764
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0011976357782259583, 0.0011976357782259583, 0.0011976357782259583, 0.0011976357782259583, 0.0011976357782259583]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011976357782259583

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.29998946
Iteration 2/25 | Loss: 0.00125326
Iteration 3/25 | Loss: 0.00125326
Iteration 4/25 | Loss: 0.00125326
Iteration 5/25 | Loss: 0.00125326
Iteration 6/25 | Loss: 0.00125326
Iteration 7/25 | Loss: 0.00125326
Iteration 8/25 | Loss: 0.00125326
Iteration 9/25 | Loss: 0.00125326
Iteration 10/25 | Loss: 0.00125326
Iteration 11/25 | Loss: 0.00125326
Iteration 12/25 | Loss: 0.00125326
Iteration 13/25 | Loss: 0.00125326
Iteration 14/25 | Loss: 0.00125326
Iteration 15/25 | Loss: 0.00125326
Iteration 16/25 | Loss: 0.00125326
Iteration 17/25 | Loss: 0.00125326
Iteration 18/25 | Loss: 0.00125326
Iteration 19/25 | Loss: 0.00125326
Iteration 20/25 | Loss: 0.00125326
Iteration 21/25 | Loss: 0.00125326
Iteration 22/25 | Loss: 0.00125326
Iteration 23/25 | Loss: 0.00125326
Iteration 24/25 | Loss: 0.00125326
Iteration 25/25 | Loss: 0.00125326

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00125326
Iteration 2/1000 | Loss: 0.00002402
Iteration 3/1000 | Loss: 0.00001640
Iteration 4/1000 | Loss: 0.00001451
Iteration 5/1000 | Loss: 0.00001367
Iteration 6/1000 | Loss: 0.00001315
Iteration 7/1000 | Loss: 0.00001272
Iteration 8/1000 | Loss: 0.00001255
Iteration 9/1000 | Loss: 0.00001219
Iteration 10/1000 | Loss: 0.00001199
Iteration 11/1000 | Loss: 0.00001188
Iteration 12/1000 | Loss: 0.00001184
Iteration 13/1000 | Loss: 0.00001183
Iteration 14/1000 | Loss: 0.00001175
Iteration 15/1000 | Loss: 0.00001174
Iteration 16/1000 | Loss: 0.00001173
Iteration 17/1000 | Loss: 0.00001165
Iteration 18/1000 | Loss: 0.00001165
Iteration 19/1000 | Loss: 0.00001162
Iteration 20/1000 | Loss: 0.00001161
Iteration 21/1000 | Loss: 0.00001161
Iteration 22/1000 | Loss: 0.00001160
Iteration 23/1000 | Loss: 0.00001155
Iteration 24/1000 | Loss: 0.00001154
Iteration 25/1000 | Loss: 0.00001154
Iteration 26/1000 | Loss: 0.00001153
Iteration 27/1000 | Loss: 0.00001153
Iteration 28/1000 | Loss: 0.00001152
Iteration 29/1000 | Loss: 0.00001152
Iteration 30/1000 | Loss: 0.00001152
Iteration 31/1000 | Loss: 0.00001152
Iteration 32/1000 | Loss: 0.00001151
Iteration 33/1000 | Loss: 0.00001150
Iteration 34/1000 | Loss: 0.00001150
Iteration 35/1000 | Loss: 0.00001149
Iteration 36/1000 | Loss: 0.00001146
Iteration 37/1000 | Loss: 0.00001144
Iteration 38/1000 | Loss: 0.00001144
Iteration 39/1000 | Loss: 0.00001144
Iteration 40/1000 | Loss: 0.00001144
Iteration 41/1000 | Loss: 0.00001143
Iteration 42/1000 | Loss: 0.00001142
Iteration 43/1000 | Loss: 0.00001142
Iteration 44/1000 | Loss: 0.00001142
Iteration 45/1000 | Loss: 0.00001141
Iteration 46/1000 | Loss: 0.00001141
Iteration 47/1000 | Loss: 0.00001140
Iteration 48/1000 | Loss: 0.00001140
Iteration 49/1000 | Loss: 0.00001139
Iteration 50/1000 | Loss: 0.00001139
Iteration 51/1000 | Loss: 0.00001138
Iteration 52/1000 | Loss: 0.00001138
Iteration 53/1000 | Loss: 0.00001138
Iteration 54/1000 | Loss: 0.00001138
Iteration 55/1000 | Loss: 0.00001138
Iteration 56/1000 | Loss: 0.00001138
Iteration 57/1000 | Loss: 0.00001138
Iteration 58/1000 | Loss: 0.00001138
Iteration 59/1000 | Loss: 0.00001138
Iteration 60/1000 | Loss: 0.00001138
Iteration 61/1000 | Loss: 0.00001138
Iteration 62/1000 | Loss: 0.00001138
Iteration 63/1000 | Loss: 0.00001138
Iteration 64/1000 | Loss: 0.00001138
Iteration 65/1000 | Loss: 0.00001138
Iteration 66/1000 | Loss: 0.00001138
Iteration 67/1000 | Loss: 0.00001138
Iteration 68/1000 | Loss: 0.00001138
Iteration 69/1000 | Loss: 0.00001138
Iteration 70/1000 | Loss: 0.00001138
Iteration 71/1000 | Loss: 0.00001138
Iteration 72/1000 | Loss: 0.00001138
Iteration 73/1000 | Loss: 0.00001138
Iteration 74/1000 | Loss: 0.00001138
Iteration 75/1000 | Loss: 0.00001138
Iteration 76/1000 | Loss: 0.00001138
Iteration 77/1000 | Loss: 0.00001138
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 77. Stopping optimization.
Last 5 losses: [1.1375886060704943e-05, 1.1375886060704943e-05, 1.1375886060704943e-05, 1.1375886060704943e-05, 1.1375886060704943e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1375886060704943e-05

Optimization complete. Final v2v error: 2.879554033279419 mm

Highest mean error: 3.062115430831909 mm for frame 63

Lowest mean error: 2.668070077896118 mm for frame 234

Saving results

Total time: 33.76767945289612
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_helen_posed_023/1011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_helen_posed_023/1011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_helen_posed_023/1011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00493776
Iteration 2/25 | Loss: 0.00146317
Iteration 3/25 | Loss: 0.00131490
Iteration 4/25 | Loss: 0.00129832
Iteration 5/25 | Loss: 0.00129424
Iteration 6/25 | Loss: 0.00129424
Iteration 7/25 | Loss: 0.00129424
Iteration 8/25 | Loss: 0.00129424
Iteration 9/25 | Loss: 0.00129424
Iteration 10/25 | Loss: 0.00129424
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0012942359317094088, 0.0012942359317094088, 0.0012942359317094088, 0.0012942359317094088, 0.0012942359317094088]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012942359317094088

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.30382562
Iteration 2/25 | Loss: 0.00128571
Iteration 3/25 | Loss: 0.00128570
Iteration 4/25 | Loss: 0.00128570
Iteration 5/25 | Loss: 0.00128570
Iteration 6/25 | Loss: 0.00128570
Iteration 7/25 | Loss: 0.00128570
Iteration 8/25 | Loss: 0.00128570
Iteration 9/25 | Loss: 0.00128570
Iteration 10/25 | Loss: 0.00128570
Iteration 11/25 | Loss: 0.00128570
Iteration 12/25 | Loss: 0.00128570
Iteration 13/25 | Loss: 0.00128570
Iteration 14/25 | Loss: 0.00128570
Iteration 15/25 | Loss: 0.00128570
Iteration 16/25 | Loss: 0.00128570
Iteration 17/25 | Loss: 0.00128570
Iteration 18/25 | Loss: 0.00128570
Iteration 19/25 | Loss: 0.00128570
Iteration 20/25 | Loss: 0.00128570
Iteration 21/25 | Loss: 0.00128570
Iteration 22/25 | Loss: 0.00128570
Iteration 23/25 | Loss: 0.00128570
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0012856960529461503, 0.0012856960529461503, 0.0012856960529461503, 0.0012856960529461503, 0.0012856960529461503]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012856960529461503

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00128570
Iteration 2/1000 | Loss: 0.00004997
Iteration 3/1000 | Loss: 0.00003428
Iteration 4/1000 | Loss: 0.00003057
Iteration 5/1000 | Loss: 0.00002954
Iteration 6/1000 | Loss: 0.00002844
Iteration 7/1000 | Loss: 0.00002742
Iteration 8/1000 | Loss: 0.00002685
Iteration 9/1000 | Loss: 0.00002641
Iteration 10/1000 | Loss: 0.00002603
Iteration 11/1000 | Loss: 0.00002579
Iteration 12/1000 | Loss: 0.00002559
Iteration 13/1000 | Loss: 0.00002542
Iteration 14/1000 | Loss: 0.00002539
Iteration 15/1000 | Loss: 0.00002532
Iteration 16/1000 | Loss: 0.00002514
Iteration 17/1000 | Loss: 0.00002498
Iteration 18/1000 | Loss: 0.00002493
Iteration 19/1000 | Loss: 0.00002480
Iteration 20/1000 | Loss: 0.00002471
Iteration 21/1000 | Loss: 0.00002471
Iteration 22/1000 | Loss: 0.00002468
Iteration 23/1000 | Loss: 0.00002468
Iteration 24/1000 | Loss: 0.00002464
Iteration 25/1000 | Loss: 0.00002461
Iteration 26/1000 | Loss: 0.00002457
Iteration 27/1000 | Loss: 0.00002456
Iteration 28/1000 | Loss: 0.00002456
Iteration 29/1000 | Loss: 0.00002456
Iteration 30/1000 | Loss: 0.00002456
Iteration 31/1000 | Loss: 0.00002456
Iteration 32/1000 | Loss: 0.00002456
Iteration 33/1000 | Loss: 0.00002456
Iteration 34/1000 | Loss: 0.00002456
Iteration 35/1000 | Loss: 0.00002456
Iteration 36/1000 | Loss: 0.00002454
Iteration 37/1000 | Loss: 0.00002454
Iteration 38/1000 | Loss: 0.00002454
Iteration 39/1000 | Loss: 0.00002454
Iteration 40/1000 | Loss: 0.00002453
Iteration 41/1000 | Loss: 0.00002453
Iteration 42/1000 | Loss: 0.00002453
Iteration 43/1000 | Loss: 0.00002453
Iteration 44/1000 | Loss: 0.00002453
Iteration 45/1000 | Loss: 0.00002453
Iteration 46/1000 | Loss: 0.00002453
Iteration 47/1000 | Loss: 0.00002453
Iteration 48/1000 | Loss: 0.00002453
Iteration 49/1000 | Loss: 0.00002453
Iteration 50/1000 | Loss: 0.00002453
Iteration 51/1000 | Loss: 0.00002453
Iteration 52/1000 | Loss: 0.00002452
Iteration 53/1000 | Loss: 0.00002452
Iteration 54/1000 | Loss: 0.00002452
Iteration 55/1000 | Loss: 0.00002451
Iteration 56/1000 | Loss: 0.00002450
Iteration 57/1000 | Loss: 0.00002450
Iteration 58/1000 | Loss: 0.00002450
Iteration 59/1000 | Loss: 0.00002450
Iteration 60/1000 | Loss: 0.00002449
Iteration 61/1000 | Loss: 0.00002447
Iteration 62/1000 | Loss: 0.00002447
Iteration 63/1000 | Loss: 0.00002447
Iteration 64/1000 | Loss: 0.00002446
Iteration 65/1000 | Loss: 0.00002446
Iteration 66/1000 | Loss: 0.00002445
Iteration 67/1000 | Loss: 0.00002444
Iteration 68/1000 | Loss: 0.00002443
Iteration 69/1000 | Loss: 0.00002442
Iteration 70/1000 | Loss: 0.00002442
Iteration 71/1000 | Loss: 0.00002442
Iteration 72/1000 | Loss: 0.00002442
Iteration 73/1000 | Loss: 0.00002442
Iteration 74/1000 | Loss: 0.00002442
Iteration 75/1000 | Loss: 0.00002442
Iteration 76/1000 | Loss: 0.00002442
Iteration 77/1000 | Loss: 0.00002442
Iteration 78/1000 | Loss: 0.00002442
Iteration 79/1000 | Loss: 0.00002441
Iteration 80/1000 | Loss: 0.00002441
Iteration 81/1000 | Loss: 0.00002441
Iteration 82/1000 | Loss: 0.00002441
Iteration 83/1000 | Loss: 0.00002439
Iteration 84/1000 | Loss: 0.00002439
Iteration 85/1000 | Loss: 0.00002439
Iteration 86/1000 | Loss: 0.00002439
Iteration 87/1000 | Loss: 0.00002438
Iteration 88/1000 | Loss: 0.00002432
Iteration 89/1000 | Loss: 0.00002431
Iteration 90/1000 | Loss: 0.00002431
Iteration 91/1000 | Loss: 0.00002430
Iteration 92/1000 | Loss: 0.00002430
Iteration 93/1000 | Loss: 0.00002430
Iteration 94/1000 | Loss: 0.00002429
Iteration 95/1000 | Loss: 0.00002429
Iteration 96/1000 | Loss: 0.00002429
Iteration 97/1000 | Loss: 0.00002428
Iteration 98/1000 | Loss: 0.00002428
Iteration 99/1000 | Loss: 0.00002428
Iteration 100/1000 | Loss: 0.00002428
Iteration 101/1000 | Loss: 0.00002428
Iteration 102/1000 | Loss: 0.00002428
Iteration 103/1000 | Loss: 0.00002428
Iteration 104/1000 | Loss: 0.00002428
Iteration 105/1000 | Loss: 0.00002428
Iteration 106/1000 | Loss: 0.00002428
Iteration 107/1000 | Loss: 0.00002428
Iteration 108/1000 | Loss: 0.00002428
Iteration 109/1000 | Loss: 0.00002428
Iteration 110/1000 | Loss: 0.00002427
Iteration 111/1000 | Loss: 0.00002427
Iteration 112/1000 | Loss: 0.00002427
Iteration 113/1000 | Loss: 0.00002427
Iteration 114/1000 | Loss: 0.00002426
Iteration 115/1000 | Loss: 0.00002426
Iteration 116/1000 | Loss: 0.00002426
Iteration 117/1000 | Loss: 0.00002426
Iteration 118/1000 | Loss: 0.00002425
Iteration 119/1000 | Loss: 0.00002425
Iteration 120/1000 | Loss: 0.00002425
Iteration 121/1000 | Loss: 0.00002425
Iteration 122/1000 | Loss: 0.00002425
Iteration 123/1000 | Loss: 0.00002425
Iteration 124/1000 | Loss: 0.00002425
Iteration 125/1000 | Loss: 0.00002425
Iteration 126/1000 | Loss: 0.00002425
Iteration 127/1000 | Loss: 0.00002425
Iteration 128/1000 | Loss: 0.00002425
Iteration 129/1000 | Loss: 0.00002425
Iteration 130/1000 | Loss: 0.00002425
Iteration 131/1000 | Loss: 0.00002424
Iteration 132/1000 | Loss: 0.00002424
Iteration 133/1000 | Loss: 0.00002424
Iteration 134/1000 | Loss: 0.00002424
Iteration 135/1000 | Loss: 0.00002424
Iteration 136/1000 | Loss: 0.00002424
Iteration 137/1000 | Loss: 0.00002424
Iteration 138/1000 | Loss: 0.00002424
Iteration 139/1000 | Loss: 0.00002424
Iteration 140/1000 | Loss: 0.00002424
Iteration 141/1000 | Loss: 0.00002424
Iteration 142/1000 | Loss: 0.00002424
Iteration 143/1000 | Loss: 0.00002424
Iteration 144/1000 | Loss: 0.00002424
Iteration 145/1000 | Loss: 0.00002424
Iteration 146/1000 | Loss: 0.00002424
Iteration 147/1000 | Loss: 0.00002424
Iteration 148/1000 | Loss: 0.00002424
Iteration 149/1000 | Loss: 0.00002424
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 149. Stopping optimization.
Last 5 losses: [2.4244232918135822e-05, 2.4244232918135822e-05, 2.4244232918135822e-05, 2.4244232918135822e-05, 2.4244232918135822e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4244232918135822e-05

Optimization complete. Final v2v error: 4.1663713455200195 mm

Highest mean error: 4.83831787109375 mm for frame 179

Lowest mean error: 3.721311569213867 mm for frame 152

Saving results

Total time: 50.47473406791687
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_helen_posed_023/1017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_helen_posed_023/1017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_helen_posed_023/1017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00984575
Iteration 2/25 | Loss: 0.00357663
Iteration 3/25 | Loss: 0.00226675
Iteration 4/25 | Loss: 0.00195393
Iteration 5/25 | Loss: 0.00193900
Iteration 6/25 | Loss: 0.00195467
Iteration 7/25 | Loss: 0.00181122
Iteration 8/25 | Loss: 0.00170882
Iteration 9/25 | Loss: 0.00166490
Iteration 10/25 | Loss: 0.00164880
Iteration 11/25 | Loss: 0.00161823
Iteration 12/25 | Loss: 0.00159589
Iteration 13/25 | Loss: 0.00157065
Iteration 14/25 | Loss: 0.00156536
Iteration 15/25 | Loss: 0.00154653
Iteration 16/25 | Loss: 0.00153259
Iteration 17/25 | Loss: 0.00153459
Iteration 18/25 | Loss: 0.00153139
Iteration 19/25 | Loss: 0.00152478
Iteration 20/25 | Loss: 0.00152303
Iteration 21/25 | Loss: 0.00152225
Iteration 22/25 | Loss: 0.00152276
Iteration 23/25 | Loss: 0.00152034
Iteration 24/25 | Loss: 0.00152182
Iteration 25/25 | Loss: 0.00152325

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.30433726
Iteration 2/25 | Loss: 0.00641684
Iteration 3/25 | Loss: 0.00427559
Iteration 4/25 | Loss: 0.00427559
Iteration 5/25 | Loss: 0.00427559
Iteration 6/25 | Loss: 0.00427559
Iteration 7/25 | Loss: 0.00427559
Iteration 8/25 | Loss: 0.00427559
Iteration 9/25 | Loss: 0.00427559
Iteration 10/25 | Loss: 0.00427559
Iteration 11/25 | Loss: 0.00427559
Iteration 12/25 | Loss: 0.00427559
Iteration 13/25 | Loss: 0.00427559
Iteration 14/25 | Loss: 0.00427559
Iteration 15/25 | Loss: 0.00427559
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.004275585524737835, 0.004275585524737835, 0.004275585524737835, 0.004275585524737835, 0.004275585524737835]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.004275585524737835

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00427559
Iteration 2/1000 | Loss: 0.00312788
Iteration 3/1000 | Loss: 0.00233200
Iteration 4/1000 | Loss: 0.00048792
Iteration 5/1000 | Loss: 0.00048282
Iteration 6/1000 | Loss: 0.00076675
Iteration 7/1000 | Loss: 0.00045164
Iteration 8/1000 | Loss: 0.00039825
Iteration 9/1000 | Loss: 0.00026092
Iteration 10/1000 | Loss: 0.00036002
Iteration 11/1000 | Loss: 0.00022808
Iteration 12/1000 | Loss: 0.00022725
Iteration 13/1000 | Loss: 0.00019200
Iteration 14/1000 | Loss: 0.00051676
Iteration 15/1000 | Loss: 0.00019001
Iteration 16/1000 | Loss: 0.00046038
Iteration 17/1000 | Loss: 0.00023473
Iteration 18/1000 | Loss: 0.00018364
Iteration 19/1000 | Loss: 0.00068054
Iteration 20/1000 | Loss: 0.00135198
Iteration 21/1000 | Loss: 0.00167790
Iteration 22/1000 | Loss: 0.00051803
Iteration 23/1000 | Loss: 0.00025894
Iteration 24/1000 | Loss: 0.00023785
Iteration 25/1000 | Loss: 0.00033330
Iteration 26/1000 | Loss: 0.00016704
Iteration 27/1000 | Loss: 0.00018241
Iteration 28/1000 | Loss: 0.00017314
Iteration 29/1000 | Loss: 0.00016753
Iteration 30/1000 | Loss: 0.00015688
Iteration 31/1000 | Loss: 0.00042326
Iteration 32/1000 | Loss: 0.00021296
Iteration 33/1000 | Loss: 0.00019566
Iteration 34/1000 | Loss: 0.00022528
Iteration 35/1000 | Loss: 0.00015691
Iteration 36/1000 | Loss: 0.00015374
Iteration 37/1000 | Loss: 0.00026236
Iteration 38/1000 | Loss: 0.00022400
Iteration 39/1000 | Loss: 0.00046491
Iteration 40/1000 | Loss: 0.00062870
Iteration 41/1000 | Loss: 0.00016463
Iteration 42/1000 | Loss: 0.00052756
Iteration 43/1000 | Loss: 0.00016108
Iteration 44/1000 | Loss: 0.00018473
Iteration 45/1000 | Loss: 0.00015235
Iteration 46/1000 | Loss: 0.00018909
Iteration 47/1000 | Loss: 0.00017511
Iteration 48/1000 | Loss: 0.00019347
Iteration 49/1000 | Loss: 0.00016779
Iteration 50/1000 | Loss: 0.00037090
Iteration 51/1000 | Loss: 0.00074274
Iteration 52/1000 | Loss: 0.00037076
Iteration 53/1000 | Loss: 0.00037995
Iteration 54/1000 | Loss: 0.00021499
Iteration 55/1000 | Loss: 0.00024496
Iteration 56/1000 | Loss: 0.00014518
Iteration 57/1000 | Loss: 0.00017607
Iteration 58/1000 | Loss: 0.00014587
Iteration 59/1000 | Loss: 0.00014234
Iteration 60/1000 | Loss: 0.00015484
Iteration 61/1000 | Loss: 0.00015310
Iteration 62/1000 | Loss: 0.00015434
Iteration 63/1000 | Loss: 0.00052363
Iteration 64/1000 | Loss: 0.00021507
Iteration 65/1000 | Loss: 0.00022561
Iteration 66/1000 | Loss: 0.00050522
Iteration 67/1000 | Loss: 0.00048847
Iteration 68/1000 | Loss: 0.00040266
Iteration 69/1000 | Loss: 0.00014953
Iteration 70/1000 | Loss: 0.00039710
Iteration 71/1000 | Loss: 0.00050171
Iteration 72/1000 | Loss: 0.00019087
Iteration 73/1000 | Loss: 0.00016555
Iteration 74/1000 | Loss: 0.00019099
Iteration 75/1000 | Loss: 0.00013144
Iteration 76/1000 | Loss: 0.00014811
Iteration 77/1000 | Loss: 0.00016006
Iteration 78/1000 | Loss: 0.00014173
Iteration 79/1000 | Loss: 0.00015467
Iteration 80/1000 | Loss: 0.00016249
Iteration 81/1000 | Loss: 0.00023322
Iteration 82/1000 | Loss: 0.00012709
Iteration 83/1000 | Loss: 0.00017653
Iteration 84/1000 | Loss: 0.00041009
Iteration 85/1000 | Loss: 0.00017348
Iteration 86/1000 | Loss: 0.00012777
Iteration 87/1000 | Loss: 0.00054023
Iteration 88/1000 | Loss: 0.00037676
Iteration 89/1000 | Loss: 0.00041232
Iteration 90/1000 | Loss: 0.00059026
Iteration 91/1000 | Loss: 0.00013116
Iteration 92/1000 | Loss: 0.00015381
Iteration 93/1000 | Loss: 0.00011993
Iteration 94/1000 | Loss: 0.00011796
Iteration 95/1000 | Loss: 0.00011569
Iteration 96/1000 | Loss: 0.00011411
Iteration 97/1000 | Loss: 0.00011334
Iteration 98/1000 | Loss: 0.00043645
Iteration 99/1000 | Loss: 0.00014499
Iteration 100/1000 | Loss: 0.00012315
Iteration 101/1000 | Loss: 0.00012843
Iteration 102/1000 | Loss: 0.00011680
Iteration 103/1000 | Loss: 0.00011472
Iteration 104/1000 | Loss: 0.00011217
Iteration 105/1000 | Loss: 0.00011096
Iteration 106/1000 | Loss: 0.00010987
Iteration 107/1000 | Loss: 0.00010925
Iteration 108/1000 | Loss: 0.00010871
Iteration 109/1000 | Loss: 0.00010820
Iteration 110/1000 | Loss: 0.00010764
Iteration 111/1000 | Loss: 0.00018583
Iteration 112/1000 | Loss: 0.00011264
Iteration 113/1000 | Loss: 0.00010916
Iteration 114/1000 | Loss: 0.00010732
Iteration 115/1000 | Loss: 0.00018064
Iteration 116/1000 | Loss: 0.00010522
Iteration 117/1000 | Loss: 0.00016742
Iteration 118/1000 | Loss: 0.00032905
Iteration 119/1000 | Loss: 0.00016275
Iteration 120/1000 | Loss: 0.00010792
Iteration 121/1000 | Loss: 0.00010459
Iteration 122/1000 | Loss: 0.00010349
Iteration 123/1000 | Loss: 0.00010233
Iteration 124/1000 | Loss: 0.00010116
Iteration 125/1000 | Loss: 0.00010056
Iteration 126/1000 | Loss: 0.00021263
Iteration 127/1000 | Loss: 0.00010830
Iteration 128/1000 | Loss: 0.00010339
Iteration 129/1000 | Loss: 0.00010096
Iteration 130/1000 | Loss: 0.00017672
Iteration 131/1000 | Loss: 0.00010372
Iteration 132/1000 | Loss: 0.00010058
Iteration 133/1000 | Loss: 0.00009833
Iteration 134/1000 | Loss: 0.00009658
Iteration 135/1000 | Loss: 0.00009556
Iteration 136/1000 | Loss: 0.00009492
Iteration 137/1000 | Loss: 0.00009443
Iteration 138/1000 | Loss: 0.00019945
Iteration 139/1000 | Loss: 0.00010256
Iteration 140/1000 | Loss: 0.00009741
Iteration 141/1000 | Loss: 0.00009498
Iteration 142/1000 | Loss: 0.00009382
Iteration 143/1000 | Loss: 0.00018825
Iteration 144/1000 | Loss: 0.00010038
Iteration 145/1000 | Loss: 0.00019467
Iteration 146/1000 | Loss: 0.00020047
Iteration 147/1000 | Loss: 0.00019363
Iteration 148/1000 | Loss: 0.00011256
Iteration 149/1000 | Loss: 0.00009658
Iteration 150/1000 | Loss: 0.00009336
Iteration 151/1000 | Loss: 0.00019265
Iteration 152/1000 | Loss: 0.00012826
Iteration 153/1000 | Loss: 0.00009941
Iteration 154/1000 | Loss: 0.00009026
Iteration 155/1000 | Loss: 0.00008855
Iteration 156/1000 | Loss: 0.00008758
Iteration 157/1000 | Loss: 0.00008667
Iteration 158/1000 | Loss: 0.00008601
Iteration 159/1000 | Loss: 0.00008566
Iteration 160/1000 | Loss: 0.00018158
Iteration 161/1000 | Loss: 0.00009059
Iteration 162/1000 | Loss: 0.00008757
Iteration 163/1000 | Loss: 0.00008605
Iteration 164/1000 | Loss: 0.00008485
Iteration 165/1000 | Loss: 0.00008418
Iteration 166/1000 | Loss: 0.00008379
Iteration 167/1000 | Loss: 0.00008354
Iteration 168/1000 | Loss: 0.00008333
Iteration 169/1000 | Loss: 0.00008327
Iteration 170/1000 | Loss: 0.00008318
Iteration 171/1000 | Loss: 0.00008315
Iteration 172/1000 | Loss: 0.00008314
Iteration 173/1000 | Loss: 0.00008313
Iteration 174/1000 | Loss: 0.00008313
Iteration 175/1000 | Loss: 0.00008313
Iteration 176/1000 | Loss: 0.00008313
Iteration 177/1000 | Loss: 0.00008313
Iteration 178/1000 | Loss: 0.00008312
Iteration 179/1000 | Loss: 0.00008312
Iteration 180/1000 | Loss: 0.00008311
Iteration 181/1000 | Loss: 0.00008310
Iteration 182/1000 | Loss: 0.00008307
Iteration 183/1000 | Loss: 0.00008306
Iteration 184/1000 | Loss: 0.00008305
Iteration 185/1000 | Loss: 0.00008305
Iteration 186/1000 | Loss: 0.00008304
Iteration 187/1000 | Loss: 0.00008304
Iteration 188/1000 | Loss: 0.00008304
Iteration 189/1000 | Loss: 0.00008303
Iteration 190/1000 | Loss: 0.00008303
Iteration 191/1000 | Loss: 0.00008303
Iteration 192/1000 | Loss: 0.00008303
Iteration 193/1000 | Loss: 0.00008303
Iteration 194/1000 | Loss: 0.00008302
Iteration 195/1000 | Loss: 0.00008302
Iteration 196/1000 | Loss: 0.00008301
Iteration 197/1000 | Loss: 0.00008301
Iteration 198/1000 | Loss: 0.00008301
Iteration 199/1000 | Loss: 0.00008301
Iteration 200/1000 | Loss: 0.00008300
Iteration 201/1000 | Loss: 0.00008300
Iteration 202/1000 | Loss: 0.00008299
Iteration 203/1000 | Loss: 0.00008298
Iteration 204/1000 | Loss: 0.00008298
Iteration 205/1000 | Loss: 0.00008297
Iteration 206/1000 | Loss: 0.00008296
Iteration 207/1000 | Loss: 0.00008296
Iteration 208/1000 | Loss: 0.00008293
Iteration 209/1000 | Loss: 0.00008293
Iteration 210/1000 | Loss: 0.00008292
Iteration 211/1000 | Loss: 0.00008292
Iteration 212/1000 | Loss: 0.00008292
Iteration 213/1000 | Loss: 0.00008292
Iteration 214/1000 | Loss: 0.00008292
Iteration 215/1000 | Loss: 0.00008292
Iteration 216/1000 | Loss: 0.00008292
Iteration 217/1000 | Loss: 0.00008292
Iteration 218/1000 | Loss: 0.00008291
Iteration 219/1000 | Loss: 0.00008291
Iteration 220/1000 | Loss: 0.00008291
Iteration 221/1000 | Loss: 0.00008292
Iteration 222/1000 | Loss: 0.00008291
Iteration 223/1000 | Loss: 0.00008291
Iteration 224/1000 | Loss: 0.00008291
Iteration 225/1000 | Loss: 0.00008292
Iteration 226/1000 | Loss: 0.00008291
Iteration 227/1000 | Loss: 0.00008291
Iteration 228/1000 | Loss: 0.00008291
Iteration 229/1000 | Loss: 0.00008291
Iteration 230/1000 | Loss: 0.00008291
Iteration 231/1000 | Loss: 0.00008291
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 231. Stopping optimization.
Last 5 losses: [8.291499398183078e-05, 8.291499398183078e-05, 8.291499398183078e-05, 8.291499398183078e-05, 8.291499398183078e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.291499398183078e-05

Optimization complete. Final v2v error: 4.591617584228516 mm

Highest mean error: 10.752155303955078 mm for frame 210

Lowest mean error: 2.8266923427581787 mm for frame 173

Saving results

Total time: 321.1319818496704
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_helen_posed_023/1097/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_helen_posed_023/1097.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_helen_posed_023/1097
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00409628
Iteration 2/25 | Loss: 0.00127027
Iteration 3/25 | Loss: 0.00120468
Iteration 4/25 | Loss: 0.00119144
Iteration 5/25 | Loss: 0.00118809
Iteration 6/25 | Loss: 0.00118779
Iteration 7/25 | Loss: 0.00118779
Iteration 8/25 | Loss: 0.00118779
Iteration 9/25 | Loss: 0.00118779
Iteration 10/25 | Loss: 0.00118779
Iteration 11/25 | Loss: 0.00118779
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011877920478582382, 0.0011877920478582382, 0.0011877920478582382, 0.0011877920478582382, 0.0011877920478582382]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011877920478582382

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.30946171
Iteration 2/25 | Loss: 0.00113902
Iteration 3/25 | Loss: 0.00113902
Iteration 4/25 | Loss: 0.00113902
Iteration 5/25 | Loss: 0.00113902
Iteration 6/25 | Loss: 0.00113902
Iteration 7/25 | Loss: 0.00113902
Iteration 8/25 | Loss: 0.00113902
Iteration 9/25 | Loss: 0.00113902
Iteration 10/25 | Loss: 0.00113902
Iteration 11/25 | Loss: 0.00113902
Iteration 12/25 | Loss: 0.00113902
Iteration 13/25 | Loss: 0.00113902
Iteration 14/25 | Loss: 0.00113902
Iteration 15/25 | Loss: 0.00113902
Iteration 16/25 | Loss: 0.00113902
Iteration 17/25 | Loss: 0.00113902
Iteration 18/25 | Loss: 0.00113902
Iteration 19/25 | Loss: 0.00113902
Iteration 20/25 | Loss: 0.00113902
Iteration 21/25 | Loss: 0.00113902
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0011390155414119363, 0.0011390155414119363, 0.0011390155414119363, 0.0011390155414119363, 0.0011390155414119363]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011390155414119363

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00113902
Iteration 2/1000 | Loss: 0.00002191
Iteration 3/1000 | Loss: 0.00001618
Iteration 4/1000 | Loss: 0.00001520
Iteration 5/1000 | Loss: 0.00001462
Iteration 6/1000 | Loss: 0.00001390
Iteration 7/1000 | Loss: 0.00001389
Iteration 8/1000 | Loss: 0.00001359
Iteration 9/1000 | Loss: 0.00001326
Iteration 10/1000 | Loss: 0.00001301
Iteration 11/1000 | Loss: 0.00001279
Iteration 12/1000 | Loss: 0.00001276
Iteration 13/1000 | Loss: 0.00001275
Iteration 14/1000 | Loss: 0.00001270
Iteration 15/1000 | Loss: 0.00001256
Iteration 16/1000 | Loss: 0.00001246
Iteration 17/1000 | Loss: 0.00001236
Iteration 18/1000 | Loss: 0.00001236
Iteration 19/1000 | Loss: 0.00001234
Iteration 20/1000 | Loss: 0.00001233
Iteration 21/1000 | Loss: 0.00001233
Iteration 22/1000 | Loss: 0.00001231
Iteration 23/1000 | Loss: 0.00001229
Iteration 24/1000 | Loss: 0.00001223
Iteration 25/1000 | Loss: 0.00001218
Iteration 26/1000 | Loss: 0.00001218
Iteration 27/1000 | Loss: 0.00001217
Iteration 28/1000 | Loss: 0.00001217
Iteration 29/1000 | Loss: 0.00001217
Iteration 30/1000 | Loss: 0.00001217
Iteration 31/1000 | Loss: 0.00001217
Iteration 32/1000 | Loss: 0.00001216
Iteration 33/1000 | Loss: 0.00001215
Iteration 34/1000 | Loss: 0.00001215
Iteration 35/1000 | Loss: 0.00001214
Iteration 36/1000 | Loss: 0.00001214
Iteration 37/1000 | Loss: 0.00001214
Iteration 38/1000 | Loss: 0.00001213
Iteration 39/1000 | Loss: 0.00001213
Iteration 40/1000 | Loss: 0.00001213
Iteration 41/1000 | Loss: 0.00001212
Iteration 42/1000 | Loss: 0.00001212
Iteration 43/1000 | Loss: 0.00001212
Iteration 44/1000 | Loss: 0.00001211
Iteration 45/1000 | Loss: 0.00001211
Iteration 46/1000 | Loss: 0.00001211
Iteration 47/1000 | Loss: 0.00001211
Iteration 48/1000 | Loss: 0.00001211
Iteration 49/1000 | Loss: 0.00001211
Iteration 50/1000 | Loss: 0.00001211
Iteration 51/1000 | Loss: 0.00001210
Iteration 52/1000 | Loss: 0.00001210
Iteration 53/1000 | Loss: 0.00001210
Iteration 54/1000 | Loss: 0.00001210
Iteration 55/1000 | Loss: 0.00001209
Iteration 56/1000 | Loss: 0.00001209
Iteration 57/1000 | Loss: 0.00001209
Iteration 58/1000 | Loss: 0.00001209
Iteration 59/1000 | Loss: 0.00001209
Iteration 60/1000 | Loss: 0.00001208
Iteration 61/1000 | Loss: 0.00001208
Iteration 62/1000 | Loss: 0.00001208
Iteration 63/1000 | Loss: 0.00001208
Iteration 64/1000 | Loss: 0.00001207
Iteration 65/1000 | Loss: 0.00001207
Iteration 66/1000 | Loss: 0.00001207
Iteration 67/1000 | Loss: 0.00001206
Iteration 68/1000 | Loss: 0.00001206
Iteration 69/1000 | Loss: 0.00001206
Iteration 70/1000 | Loss: 0.00001205
Iteration 71/1000 | Loss: 0.00001205
Iteration 72/1000 | Loss: 0.00001204
Iteration 73/1000 | Loss: 0.00001204
Iteration 74/1000 | Loss: 0.00001204
Iteration 75/1000 | Loss: 0.00001203
Iteration 76/1000 | Loss: 0.00001203
Iteration 77/1000 | Loss: 0.00001203
Iteration 78/1000 | Loss: 0.00001202
Iteration 79/1000 | Loss: 0.00001202
Iteration 80/1000 | Loss: 0.00001202
Iteration 81/1000 | Loss: 0.00001202
Iteration 82/1000 | Loss: 0.00001202
Iteration 83/1000 | Loss: 0.00001202
Iteration 84/1000 | Loss: 0.00001202
Iteration 85/1000 | Loss: 0.00001201
Iteration 86/1000 | Loss: 0.00001201
Iteration 87/1000 | Loss: 0.00001201
Iteration 88/1000 | Loss: 0.00001201
Iteration 89/1000 | Loss: 0.00001200
Iteration 90/1000 | Loss: 0.00001200
Iteration 91/1000 | Loss: 0.00001200
Iteration 92/1000 | Loss: 0.00001200
Iteration 93/1000 | Loss: 0.00001200
Iteration 94/1000 | Loss: 0.00001200
Iteration 95/1000 | Loss: 0.00001200
Iteration 96/1000 | Loss: 0.00001200
Iteration 97/1000 | Loss: 0.00001200
Iteration 98/1000 | Loss: 0.00001200
Iteration 99/1000 | Loss: 0.00001199
Iteration 100/1000 | Loss: 0.00001199
Iteration 101/1000 | Loss: 0.00001199
Iteration 102/1000 | Loss: 0.00001199
Iteration 103/1000 | Loss: 0.00001198
Iteration 104/1000 | Loss: 0.00001198
Iteration 105/1000 | Loss: 0.00001198
Iteration 106/1000 | Loss: 0.00001198
Iteration 107/1000 | Loss: 0.00001198
Iteration 108/1000 | Loss: 0.00001198
Iteration 109/1000 | Loss: 0.00001198
Iteration 110/1000 | Loss: 0.00001197
Iteration 111/1000 | Loss: 0.00001197
Iteration 112/1000 | Loss: 0.00001197
Iteration 113/1000 | Loss: 0.00001197
Iteration 114/1000 | Loss: 0.00001197
Iteration 115/1000 | Loss: 0.00001197
Iteration 116/1000 | Loss: 0.00001197
Iteration 117/1000 | Loss: 0.00001197
Iteration 118/1000 | Loss: 0.00001197
Iteration 119/1000 | Loss: 0.00001197
Iteration 120/1000 | Loss: 0.00001197
Iteration 121/1000 | Loss: 0.00001197
Iteration 122/1000 | Loss: 0.00001197
Iteration 123/1000 | Loss: 0.00001197
Iteration 124/1000 | Loss: 0.00001197
Iteration 125/1000 | Loss: 0.00001197
Iteration 126/1000 | Loss: 0.00001197
Iteration 127/1000 | Loss: 0.00001197
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 127. Stopping optimization.
Last 5 losses: [1.1970860214205459e-05, 1.1970860214205459e-05, 1.1970860214205459e-05, 1.1970860214205459e-05, 1.1970860214205459e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1970860214205459e-05

Optimization complete. Final v2v error: 3.01762318611145 mm

Highest mean error: 3.0431222915649414 mm for frame 155

Lowest mean error: 2.9507076740264893 mm for frame 121

Saving results

Total time: 34.89597225189209
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_helen_posed_023/1062/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_helen_posed_023/1062.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_helen_posed_023/1062
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00766676
Iteration 2/25 | Loss: 0.00128522
Iteration 3/25 | Loss: 0.00118623
Iteration 4/25 | Loss: 0.00117764
Iteration 5/25 | Loss: 0.00117657
Iteration 6/25 | Loss: 0.00117643
Iteration 7/25 | Loss: 0.00117643
Iteration 8/25 | Loss: 0.00117643
Iteration 9/25 | Loss: 0.00117643
Iteration 10/25 | Loss: 0.00117643
Iteration 11/25 | Loss: 0.00117643
Iteration 12/25 | Loss: 0.00117643
Iteration 13/25 | Loss: 0.00117643
Iteration 14/25 | Loss: 0.00117643
Iteration 15/25 | Loss: 0.00117643
Iteration 16/25 | Loss: 0.00117643
Iteration 17/25 | Loss: 0.00117643
Iteration 18/25 | Loss: 0.00117643
Iteration 19/25 | Loss: 0.00117643
Iteration 20/25 | Loss: 0.00117643
Iteration 21/25 | Loss: 0.00117643
Iteration 22/25 | Loss: 0.00117643
Iteration 23/25 | Loss: 0.00117643
Iteration 24/25 | Loss: 0.00117643
Iteration 25/25 | Loss: 0.00117643

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.31059134
Iteration 2/25 | Loss: 0.00116180
Iteration 3/25 | Loss: 0.00116180
Iteration 4/25 | Loss: 0.00116180
Iteration 5/25 | Loss: 0.00116180
Iteration 6/25 | Loss: 0.00116180
Iteration 7/25 | Loss: 0.00116180
Iteration 8/25 | Loss: 0.00116180
Iteration 9/25 | Loss: 0.00116180
Iteration 10/25 | Loss: 0.00116180
Iteration 11/25 | Loss: 0.00116180
Iteration 12/25 | Loss: 0.00116180
Iteration 13/25 | Loss: 0.00116180
Iteration 14/25 | Loss: 0.00116180
Iteration 15/25 | Loss: 0.00116180
Iteration 16/25 | Loss: 0.00116180
Iteration 17/25 | Loss: 0.00116180
Iteration 18/25 | Loss: 0.00116180
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0011617975542321801, 0.0011617975542321801, 0.0011617975542321801, 0.0011617975542321801, 0.0011617975542321801]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011617975542321801

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00116180
Iteration 2/1000 | Loss: 0.00002022
Iteration 3/1000 | Loss: 0.00001500
Iteration 4/1000 | Loss: 0.00001350
Iteration 5/1000 | Loss: 0.00001253
Iteration 6/1000 | Loss: 0.00001205
Iteration 7/1000 | Loss: 0.00001163
Iteration 8/1000 | Loss: 0.00001120
Iteration 9/1000 | Loss: 0.00001084
Iteration 10/1000 | Loss: 0.00001067
Iteration 11/1000 | Loss: 0.00001053
Iteration 12/1000 | Loss: 0.00001051
Iteration 13/1000 | Loss: 0.00001048
Iteration 14/1000 | Loss: 0.00001048
Iteration 15/1000 | Loss: 0.00001047
Iteration 16/1000 | Loss: 0.00001045
Iteration 17/1000 | Loss: 0.00001044
Iteration 18/1000 | Loss: 0.00001043
Iteration 19/1000 | Loss: 0.00001035
Iteration 20/1000 | Loss: 0.00001032
Iteration 21/1000 | Loss: 0.00001032
Iteration 22/1000 | Loss: 0.00001031
Iteration 23/1000 | Loss: 0.00001029
Iteration 24/1000 | Loss: 0.00001029
Iteration 25/1000 | Loss: 0.00001023
Iteration 26/1000 | Loss: 0.00001017
Iteration 27/1000 | Loss: 0.00001016
Iteration 28/1000 | Loss: 0.00001015
Iteration 29/1000 | Loss: 0.00001014
Iteration 30/1000 | Loss: 0.00001014
Iteration 31/1000 | Loss: 0.00001013
Iteration 32/1000 | Loss: 0.00001012
Iteration 33/1000 | Loss: 0.00001012
Iteration 34/1000 | Loss: 0.00001011
Iteration 35/1000 | Loss: 0.00001010
Iteration 36/1000 | Loss: 0.00001009
Iteration 37/1000 | Loss: 0.00001008
Iteration 38/1000 | Loss: 0.00001007
Iteration 39/1000 | Loss: 0.00001007
Iteration 40/1000 | Loss: 0.00001007
Iteration 41/1000 | Loss: 0.00001006
Iteration 42/1000 | Loss: 0.00001006
Iteration 43/1000 | Loss: 0.00001005
Iteration 44/1000 | Loss: 0.00001005
Iteration 45/1000 | Loss: 0.00001004
Iteration 46/1000 | Loss: 0.00001004
Iteration 47/1000 | Loss: 0.00001003
Iteration 48/1000 | Loss: 0.00001003
Iteration 49/1000 | Loss: 0.00000999
Iteration 50/1000 | Loss: 0.00000997
Iteration 51/1000 | Loss: 0.00000997
Iteration 52/1000 | Loss: 0.00000996
Iteration 53/1000 | Loss: 0.00000996
Iteration 54/1000 | Loss: 0.00000996
Iteration 55/1000 | Loss: 0.00000995
Iteration 56/1000 | Loss: 0.00000995
Iteration 57/1000 | Loss: 0.00000994
Iteration 58/1000 | Loss: 0.00000994
Iteration 59/1000 | Loss: 0.00000994
Iteration 60/1000 | Loss: 0.00000994
Iteration 61/1000 | Loss: 0.00000993
Iteration 62/1000 | Loss: 0.00000993
Iteration 63/1000 | Loss: 0.00000993
Iteration 64/1000 | Loss: 0.00000992
Iteration 65/1000 | Loss: 0.00000992
Iteration 66/1000 | Loss: 0.00000992
Iteration 67/1000 | Loss: 0.00000991
Iteration 68/1000 | Loss: 0.00000991
Iteration 69/1000 | Loss: 0.00000991
Iteration 70/1000 | Loss: 0.00000990
Iteration 71/1000 | Loss: 0.00000990
Iteration 72/1000 | Loss: 0.00000989
Iteration 73/1000 | Loss: 0.00000989
Iteration 74/1000 | Loss: 0.00000989
Iteration 75/1000 | Loss: 0.00000989
Iteration 76/1000 | Loss: 0.00000989
Iteration 77/1000 | Loss: 0.00000988
Iteration 78/1000 | Loss: 0.00000988
Iteration 79/1000 | Loss: 0.00000988
Iteration 80/1000 | Loss: 0.00000988
Iteration 81/1000 | Loss: 0.00000988
Iteration 82/1000 | Loss: 0.00000987
Iteration 83/1000 | Loss: 0.00000986
Iteration 84/1000 | Loss: 0.00000986
Iteration 85/1000 | Loss: 0.00000986
Iteration 86/1000 | Loss: 0.00000984
Iteration 87/1000 | Loss: 0.00000984
Iteration 88/1000 | Loss: 0.00000984
Iteration 89/1000 | Loss: 0.00000984
Iteration 90/1000 | Loss: 0.00000983
Iteration 91/1000 | Loss: 0.00000983
Iteration 92/1000 | Loss: 0.00000982
Iteration 93/1000 | Loss: 0.00000981
Iteration 94/1000 | Loss: 0.00000980
Iteration 95/1000 | Loss: 0.00000980
Iteration 96/1000 | Loss: 0.00000979
Iteration 97/1000 | Loss: 0.00000979
Iteration 98/1000 | Loss: 0.00000978
Iteration 99/1000 | Loss: 0.00000978
Iteration 100/1000 | Loss: 0.00000978
Iteration 101/1000 | Loss: 0.00000978
Iteration 102/1000 | Loss: 0.00000977
Iteration 103/1000 | Loss: 0.00000977
Iteration 104/1000 | Loss: 0.00000976
Iteration 105/1000 | Loss: 0.00000976
Iteration 106/1000 | Loss: 0.00000975
Iteration 107/1000 | Loss: 0.00000975
Iteration 108/1000 | Loss: 0.00000975
Iteration 109/1000 | Loss: 0.00000974
Iteration 110/1000 | Loss: 0.00000974
Iteration 111/1000 | Loss: 0.00000974
Iteration 112/1000 | Loss: 0.00000974
Iteration 113/1000 | Loss: 0.00000974
Iteration 114/1000 | Loss: 0.00000974
Iteration 115/1000 | Loss: 0.00000974
Iteration 116/1000 | Loss: 0.00000974
Iteration 117/1000 | Loss: 0.00000973
Iteration 118/1000 | Loss: 0.00000973
Iteration 119/1000 | Loss: 0.00000973
Iteration 120/1000 | Loss: 0.00000973
Iteration 121/1000 | Loss: 0.00000973
Iteration 122/1000 | Loss: 0.00000973
Iteration 123/1000 | Loss: 0.00000972
Iteration 124/1000 | Loss: 0.00000972
Iteration 125/1000 | Loss: 0.00000972
Iteration 126/1000 | Loss: 0.00000972
Iteration 127/1000 | Loss: 0.00000972
Iteration 128/1000 | Loss: 0.00000972
Iteration 129/1000 | Loss: 0.00000972
Iteration 130/1000 | Loss: 0.00000972
Iteration 131/1000 | Loss: 0.00000972
Iteration 132/1000 | Loss: 0.00000971
Iteration 133/1000 | Loss: 0.00000971
Iteration 134/1000 | Loss: 0.00000971
Iteration 135/1000 | Loss: 0.00000971
Iteration 136/1000 | Loss: 0.00000971
Iteration 137/1000 | Loss: 0.00000971
Iteration 138/1000 | Loss: 0.00000970
Iteration 139/1000 | Loss: 0.00000970
Iteration 140/1000 | Loss: 0.00000970
Iteration 141/1000 | Loss: 0.00000970
Iteration 142/1000 | Loss: 0.00000970
Iteration 143/1000 | Loss: 0.00000970
Iteration 144/1000 | Loss: 0.00000970
Iteration 145/1000 | Loss: 0.00000970
Iteration 146/1000 | Loss: 0.00000969
Iteration 147/1000 | Loss: 0.00000969
Iteration 148/1000 | Loss: 0.00000969
Iteration 149/1000 | Loss: 0.00000969
Iteration 150/1000 | Loss: 0.00000969
Iteration 151/1000 | Loss: 0.00000969
Iteration 152/1000 | Loss: 0.00000969
Iteration 153/1000 | Loss: 0.00000968
Iteration 154/1000 | Loss: 0.00000968
Iteration 155/1000 | Loss: 0.00000968
Iteration 156/1000 | Loss: 0.00000968
Iteration 157/1000 | Loss: 0.00000968
Iteration 158/1000 | Loss: 0.00000968
Iteration 159/1000 | Loss: 0.00000968
Iteration 160/1000 | Loss: 0.00000968
Iteration 161/1000 | Loss: 0.00000968
Iteration 162/1000 | Loss: 0.00000968
Iteration 163/1000 | Loss: 0.00000968
Iteration 164/1000 | Loss: 0.00000968
Iteration 165/1000 | Loss: 0.00000968
Iteration 166/1000 | Loss: 0.00000968
Iteration 167/1000 | Loss: 0.00000967
Iteration 168/1000 | Loss: 0.00000967
Iteration 169/1000 | Loss: 0.00000967
Iteration 170/1000 | Loss: 0.00000967
Iteration 171/1000 | Loss: 0.00000967
Iteration 172/1000 | Loss: 0.00000966
Iteration 173/1000 | Loss: 0.00000966
Iteration 174/1000 | Loss: 0.00000966
Iteration 175/1000 | Loss: 0.00000966
Iteration 176/1000 | Loss: 0.00000966
Iteration 177/1000 | Loss: 0.00000966
Iteration 178/1000 | Loss: 0.00000966
Iteration 179/1000 | Loss: 0.00000966
Iteration 180/1000 | Loss: 0.00000966
Iteration 181/1000 | Loss: 0.00000966
Iteration 182/1000 | Loss: 0.00000965
Iteration 183/1000 | Loss: 0.00000965
Iteration 184/1000 | Loss: 0.00000965
Iteration 185/1000 | Loss: 0.00000965
Iteration 186/1000 | Loss: 0.00000965
Iteration 187/1000 | Loss: 0.00000965
Iteration 188/1000 | Loss: 0.00000965
Iteration 189/1000 | Loss: 0.00000965
Iteration 190/1000 | Loss: 0.00000965
Iteration 191/1000 | Loss: 0.00000965
Iteration 192/1000 | Loss: 0.00000964
Iteration 193/1000 | Loss: 0.00000964
Iteration 194/1000 | Loss: 0.00000964
Iteration 195/1000 | Loss: 0.00000964
Iteration 196/1000 | Loss: 0.00000964
Iteration 197/1000 | Loss: 0.00000963
Iteration 198/1000 | Loss: 0.00000963
Iteration 199/1000 | Loss: 0.00000963
Iteration 200/1000 | Loss: 0.00000963
Iteration 201/1000 | Loss: 0.00000963
Iteration 202/1000 | Loss: 0.00000962
Iteration 203/1000 | Loss: 0.00000962
Iteration 204/1000 | Loss: 0.00000962
Iteration 205/1000 | Loss: 0.00000962
Iteration 206/1000 | Loss: 0.00000962
Iteration 207/1000 | Loss: 0.00000962
Iteration 208/1000 | Loss: 0.00000962
Iteration 209/1000 | Loss: 0.00000962
Iteration 210/1000 | Loss: 0.00000962
Iteration 211/1000 | Loss: 0.00000962
Iteration 212/1000 | Loss: 0.00000962
Iteration 213/1000 | Loss: 0.00000962
Iteration 214/1000 | Loss: 0.00000962
Iteration 215/1000 | Loss: 0.00000962
Iteration 216/1000 | Loss: 0.00000962
Iteration 217/1000 | Loss: 0.00000962
Iteration 218/1000 | Loss: 0.00000962
Iteration 219/1000 | Loss: 0.00000962
Iteration 220/1000 | Loss: 0.00000962
Iteration 221/1000 | Loss: 0.00000962
Iteration 222/1000 | Loss: 0.00000962
Iteration 223/1000 | Loss: 0.00000962
Iteration 224/1000 | Loss: 0.00000962
Iteration 225/1000 | Loss: 0.00000962
Iteration 226/1000 | Loss: 0.00000962
Iteration 227/1000 | Loss: 0.00000962
Iteration 228/1000 | Loss: 0.00000962
Iteration 229/1000 | Loss: 0.00000962
Iteration 230/1000 | Loss: 0.00000962
Iteration 231/1000 | Loss: 0.00000962
Iteration 232/1000 | Loss: 0.00000962
Iteration 233/1000 | Loss: 0.00000962
Iteration 234/1000 | Loss: 0.00000962
Iteration 235/1000 | Loss: 0.00000962
Iteration 236/1000 | Loss: 0.00000962
Iteration 237/1000 | Loss: 0.00000962
Iteration 238/1000 | Loss: 0.00000962
Iteration 239/1000 | Loss: 0.00000962
Iteration 240/1000 | Loss: 0.00000962
Iteration 241/1000 | Loss: 0.00000962
Iteration 242/1000 | Loss: 0.00000962
Iteration 243/1000 | Loss: 0.00000962
Iteration 244/1000 | Loss: 0.00000962
Iteration 245/1000 | Loss: 0.00000962
Iteration 246/1000 | Loss: 0.00000962
Iteration 247/1000 | Loss: 0.00000962
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 247. Stopping optimization.
Last 5 losses: [9.61629211815307e-06, 9.61629211815307e-06, 9.61629211815307e-06, 9.61629211815307e-06, 9.61629211815307e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.61629211815307e-06

Optimization complete. Final v2v error: 2.6564695835113525 mm

Highest mean error: 2.83221173286438 mm for frame 80

Lowest mean error: 2.485199451446533 mm for frame 251

Saving results

Total time: 50.49739384651184
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_helen_posed_023/1050/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_helen_posed_023/1050.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_helen_posed_023/1050
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00546931
Iteration 2/25 | Loss: 0.00126200
Iteration 3/25 | Loss: 0.00120341
Iteration 4/25 | Loss: 0.00119450
Iteration 5/25 | Loss: 0.00119171
Iteration 6/25 | Loss: 0.00119145
Iteration 7/25 | Loss: 0.00119145
Iteration 8/25 | Loss: 0.00119145
Iteration 9/25 | Loss: 0.00119145
Iteration 10/25 | Loss: 0.00119145
Iteration 11/25 | Loss: 0.00119145
Iteration 12/25 | Loss: 0.00119145
Iteration 13/25 | Loss: 0.00119145
Iteration 14/25 | Loss: 0.00119145
Iteration 15/25 | Loss: 0.00119145
Iteration 16/25 | Loss: 0.00119145
Iteration 17/25 | Loss: 0.00119145
Iteration 18/25 | Loss: 0.00119145
Iteration 19/25 | Loss: 0.00119145
Iteration 20/25 | Loss: 0.00119145
Iteration 21/25 | Loss: 0.00119145
Iteration 22/25 | Loss: 0.00119145
Iteration 23/25 | Loss: 0.00119145
Iteration 24/25 | Loss: 0.00119145
Iteration 25/25 | Loss: 0.00119145

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.88668442
Iteration 2/25 | Loss: 0.00116493
Iteration 3/25 | Loss: 0.00116492
Iteration 4/25 | Loss: 0.00116491
Iteration 5/25 | Loss: 0.00116491
Iteration 6/25 | Loss: 0.00116491
Iteration 7/25 | Loss: 0.00116491
Iteration 8/25 | Loss: 0.00116491
Iteration 9/25 | Loss: 0.00116491
Iteration 10/25 | Loss: 0.00116491
Iteration 11/25 | Loss: 0.00116491
Iteration 12/25 | Loss: 0.00116491
Iteration 13/25 | Loss: 0.00116491
Iteration 14/25 | Loss: 0.00116491
Iteration 15/25 | Loss: 0.00116491
Iteration 16/25 | Loss: 0.00116491
Iteration 17/25 | Loss: 0.00116491
Iteration 18/25 | Loss: 0.00116491
Iteration 19/25 | Loss: 0.00116491
Iteration 20/25 | Loss: 0.00116491
Iteration 21/25 | Loss: 0.00116491
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0011649122461676598, 0.0011649122461676598, 0.0011649122461676598, 0.0011649122461676598, 0.0011649122461676598]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011649122461676598

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00116491
Iteration 2/1000 | Loss: 0.00002382
Iteration 3/1000 | Loss: 0.00001677
Iteration 4/1000 | Loss: 0.00001556
Iteration 5/1000 | Loss: 0.00001479
Iteration 6/1000 | Loss: 0.00001418
Iteration 7/1000 | Loss: 0.00001357
Iteration 8/1000 | Loss: 0.00001326
Iteration 9/1000 | Loss: 0.00001294
Iteration 10/1000 | Loss: 0.00001267
Iteration 11/1000 | Loss: 0.00001265
Iteration 12/1000 | Loss: 0.00001245
Iteration 13/1000 | Loss: 0.00001239
Iteration 14/1000 | Loss: 0.00001225
Iteration 15/1000 | Loss: 0.00001215
Iteration 16/1000 | Loss: 0.00001212
Iteration 17/1000 | Loss: 0.00001211
Iteration 18/1000 | Loss: 0.00001210
Iteration 19/1000 | Loss: 0.00001209
Iteration 20/1000 | Loss: 0.00001202
Iteration 21/1000 | Loss: 0.00001199
Iteration 22/1000 | Loss: 0.00001198
Iteration 23/1000 | Loss: 0.00001198
Iteration 24/1000 | Loss: 0.00001197
Iteration 25/1000 | Loss: 0.00001188
Iteration 26/1000 | Loss: 0.00001187
Iteration 27/1000 | Loss: 0.00001186
Iteration 28/1000 | Loss: 0.00001183
Iteration 29/1000 | Loss: 0.00001181
Iteration 30/1000 | Loss: 0.00001181
Iteration 31/1000 | Loss: 0.00001180
Iteration 32/1000 | Loss: 0.00001179
Iteration 33/1000 | Loss: 0.00001177
Iteration 34/1000 | Loss: 0.00001177
Iteration 35/1000 | Loss: 0.00001177
Iteration 36/1000 | Loss: 0.00001176
Iteration 37/1000 | Loss: 0.00001175
Iteration 38/1000 | Loss: 0.00001174
Iteration 39/1000 | Loss: 0.00001169
Iteration 40/1000 | Loss: 0.00001169
Iteration 41/1000 | Loss: 0.00001168
Iteration 42/1000 | Loss: 0.00001167
Iteration 43/1000 | Loss: 0.00001166
Iteration 44/1000 | Loss: 0.00001166
Iteration 45/1000 | Loss: 0.00001165
Iteration 46/1000 | Loss: 0.00001165
Iteration 47/1000 | Loss: 0.00001165
Iteration 48/1000 | Loss: 0.00001164
Iteration 49/1000 | Loss: 0.00001164
Iteration 50/1000 | Loss: 0.00001163
Iteration 51/1000 | Loss: 0.00001163
Iteration 52/1000 | Loss: 0.00001162
Iteration 53/1000 | Loss: 0.00001162
Iteration 54/1000 | Loss: 0.00001162
Iteration 55/1000 | Loss: 0.00001162
Iteration 56/1000 | Loss: 0.00001161
Iteration 57/1000 | Loss: 0.00001161
Iteration 58/1000 | Loss: 0.00001161
Iteration 59/1000 | Loss: 0.00001161
Iteration 60/1000 | Loss: 0.00001160
Iteration 61/1000 | Loss: 0.00001160
Iteration 62/1000 | Loss: 0.00001160
Iteration 63/1000 | Loss: 0.00001160
Iteration 64/1000 | Loss: 0.00001160
Iteration 65/1000 | Loss: 0.00001160
Iteration 66/1000 | Loss: 0.00001160
Iteration 67/1000 | Loss: 0.00001159
Iteration 68/1000 | Loss: 0.00001159
Iteration 69/1000 | Loss: 0.00001159
Iteration 70/1000 | Loss: 0.00001159
Iteration 71/1000 | Loss: 0.00001158
Iteration 72/1000 | Loss: 0.00001158
Iteration 73/1000 | Loss: 0.00001158
Iteration 74/1000 | Loss: 0.00001157
Iteration 75/1000 | Loss: 0.00001157
Iteration 76/1000 | Loss: 0.00001157
Iteration 77/1000 | Loss: 0.00001156
Iteration 78/1000 | Loss: 0.00001156
Iteration 79/1000 | Loss: 0.00001156
Iteration 80/1000 | Loss: 0.00001156
Iteration 81/1000 | Loss: 0.00001155
Iteration 82/1000 | Loss: 0.00001155
Iteration 83/1000 | Loss: 0.00001154
Iteration 84/1000 | Loss: 0.00001154
Iteration 85/1000 | Loss: 0.00001154
Iteration 86/1000 | Loss: 0.00001153
Iteration 87/1000 | Loss: 0.00001153
Iteration 88/1000 | Loss: 0.00001152
Iteration 89/1000 | Loss: 0.00001152
Iteration 90/1000 | Loss: 0.00001152
Iteration 91/1000 | Loss: 0.00001151
Iteration 92/1000 | Loss: 0.00001151
Iteration 93/1000 | Loss: 0.00001151
Iteration 94/1000 | Loss: 0.00001151
Iteration 95/1000 | Loss: 0.00001151
Iteration 96/1000 | Loss: 0.00001151
Iteration 97/1000 | Loss: 0.00001151
Iteration 98/1000 | Loss: 0.00001151
Iteration 99/1000 | Loss: 0.00001151
Iteration 100/1000 | Loss: 0.00001150
Iteration 101/1000 | Loss: 0.00001150
Iteration 102/1000 | Loss: 0.00001150
Iteration 103/1000 | Loss: 0.00001150
Iteration 104/1000 | Loss: 0.00001150
Iteration 105/1000 | Loss: 0.00001150
Iteration 106/1000 | Loss: 0.00001150
Iteration 107/1000 | Loss: 0.00001150
Iteration 108/1000 | Loss: 0.00001150
Iteration 109/1000 | Loss: 0.00001149
Iteration 110/1000 | Loss: 0.00001149
Iteration 111/1000 | Loss: 0.00001149
Iteration 112/1000 | Loss: 0.00001149
Iteration 113/1000 | Loss: 0.00001149
Iteration 114/1000 | Loss: 0.00001148
Iteration 115/1000 | Loss: 0.00001148
Iteration 116/1000 | Loss: 0.00001148
Iteration 117/1000 | Loss: 0.00001148
Iteration 118/1000 | Loss: 0.00001147
Iteration 119/1000 | Loss: 0.00001147
Iteration 120/1000 | Loss: 0.00001147
Iteration 121/1000 | Loss: 0.00001147
Iteration 122/1000 | Loss: 0.00001146
Iteration 123/1000 | Loss: 0.00001146
Iteration 124/1000 | Loss: 0.00001146
Iteration 125/1000 | Loss: 0.00001145
Iteration 126/1000 | Loss: 0.00001145
Iteration 127/1000 | Loss: 0.00001145
Iteration 128/1000 | Loss: 0.00001145
Iteration 129/1000 | Loss: 0.00001145
Iteration 130/1000 | Loss: 0.00001144
Iteration 131/1000 | Loss: 0.00001144
Iteration 132/1000 | Loss: 0.00001144
Iteration 133/1000 | Loss: 0.00001144
Iteration 134/1000 | Loss: 0.00001144
Iteration 135/1000 | Loss: 0.00001144
Iteration 136/1000 | Loss: 0.00001144
Iteration 137/1000 | Loss: 0.00001144
Iteration 138/1000 | Loss: 0.00001144
Iteration 139/1000 | Loss: 0.00001144
Iteration 140/1000 | Loss: 0.00001144
Iteration 141/1000 | Loss: 0.00001144
Iteration 142/1000 | Loss: 0.00001144
Iteration 143/1000 | Loss: 0.00001144
Iteration 144/1000 | Loss: 0.00001144
Iteration 145/1000 | Loss: 0.00001143
Iteration 146/1000 | Loss: 0.00001143
Iteration 147/1000 | Loss: 0.00001143
Iteration 148/1000 | Loss: 0.00001143
Iteration 149/1000 | Loss: 0.00001143
Iteration 150/1000 | Loss: 0.00001143
Iteration 151/1000 | Loss: 0.00001143
Iteration 152/1000 | Loss: 0.00001143
Iteration 153/1000 | Loss: 0.00001143
Iteration 154/1000 | Loss: 0.00001143
Iteration 155/1000 | Loss: 0.00001143
Iteration 156/1000 | Loss: 0.00001143
Iteration 157/1000 | Loss: 0.00001143
Iteration 158/1000 | Loss: 0.00001143
Iteration 159/1000 | Loss: 0.00001143
Iteration 160/1000 | Loss: 0.00001143
Iteration 161/1000 | Loss: 0.00001143
Iteration 162/1000 | Loss: 0.00001143
Iteration 163/1000 | Loss: 0.00001143
Iteration 164/1000 | Loss: 0.00001143
Iteration 165/1000 | Loss: 0.00001143
Iteration 166/1000 | Loss: 0.00001143
Iteration 167/1000 | Loss: 0.00001143
Iteration 168/1000 | Loss: 0.00001142
Iteration 169/1000 | Loss: 0.00001142
Iteration 170/1000 | Loss: 0.00001142
Iteration 171/1000 | Loss: 0.00001142
Iteration 172/1000 | Loss: 0.00001142
Iteration 173/1000 | Loss: 0.00001142
Iteration 174/1000 | Loss: 0.00001142
Iteration 175/1000 | Loss: 0.00001142
Iteration 176/1000 | Loss: 0.00001142
Iteration 177/1000 | Loss: 0.00001142
Iteration 178/1000 | Loss: 0.00001142
Iteration 179/1000 | Loss: 0.00001142
Iteration 180/1000 | Loss: 0.00001142
Iteration 181/1000 | Loss: 0.00001142
Iteration 182/1000 | Loss: 0.00001142
Iteration 183/1000 | Loss: 0.00001142
Iteration 184/1000 | Loss: 0.00001142
Iteration 185/1000 | Loss: 0.00001142
Iteration 186/1000 | Loss: 0.00001142
Iteration 187/1000 | Loss: 0.00001142
Iteration 188/1000 | Loss: 0.00001142
Iteration 189/1000 | Loss: 0.00001141
Iteration 190/1000 | Loss: 0.00001141
Iteration 191/1000 | Loss: 0.00001141
Iteration 192/1000 | Loss: 0.00001141
Iteration 193/1000 | Loss: 0.00001141
Iteration 194/1000 | Loss: 0.00001141
Iteration 195/1000 | Loss: 0.00001141
Iteration 196/1000 | Loss: 0.00001141
Iteration 197/1000 | Loss: 0.00001141
Iteration 198/1000 | Loss: 0.00001141
Iteration 199/1000 | Loss: 0.00001141
Iteration 200/1000 | Loss: 0.00001141
Iteration 201/1000 | Loss: 0.00001141
Iteration 202/1000 | Loss: 0.00001141
Iteration 203/1000 | Loss: 0.00001141
Iteration 204/1000 | Loss: 0.00001141
Iteration 205/1000 | Loss: 0.00001141
Iteration 206/1000 | Loss: 0.00001141
Iteration 207/1000 | Loss: 0.00001141
Iteration 208/1000 | Loss: 0.00001141
Iteration 209/1000 | Loss: 0.00001141
Iteration 210/1000 | Loss: 0.00001141
Iteration 211/1000 | Loss: 0.00001141
Iteration 212/1000 | Loss: 0.00001141
Iteration 213/1000 | Loss: 0.00001141
Iteration 214/1000 | Loss: 0.00001141
Iteration 215/1000 | Loss: 0.00001141
Iteration 216/1000 | Loss: 0.00001141
Iteration 217/1000 | Loss: 0.00001141
Iteration 218/1000 | Loss: 0.00001141
Iteration 219/1000 | Loss: 0.00001141
Iteration 220/1000 | Loss: 0.00001141
Iteration 221/1000 | Loss: 0.00001141
Iteration 222/1000 | Loss: 0.00001141
Iteration 223/1000 | Loss: 0.00001141
Iteration 224/1000 | Loss: 0.00001141
Iteration 225/1000 | Loss: 0.00001141
Iteration 226/1000 | Loss: 0.00001141
Iteration 227/1000 | Loss: 0.00001141
Iteration 228/1000 | Loss: 0.00001141
Iteration 229/1000 | Loss: 0.00001141
Iteration 230/1000 | Loss: 0.00001141
Iteration 231/1000 | Loss: 0.00001141
Iteration 232/1000 | Loss: 0.00001141
Iteration 233/1000 | Loss: 0.00001141
Iteration 234/1000 | Loss: 0.00001141
Iteration 235/1000 | Loss: 0.00001141
Iteration 236/1000 | Loss: 0.00001141
Iteration 237/1000 | Loss: 0.00001141
Iteration 238/1000 | Loss: 0.00001141
Iteration 239/1000 | Loss: 0.00001141
Iteration 240/1000 | Loss: 0.00001141
Iteration 241/1000 | Loss: 0.00001141
Iteration 242/1000 | Loss: 0.00001141
Iteration 243/1000 | Loss: 0.00001141
Iteration 244/1000 | Loss: 0.00001141
Iteration 245/1000 | Loss: 0.00001141
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 245. Stopping optimization.
Last 5 losses: [1.1410959814384114e-05, 1.1410959814384114e-05, 1.1410959814384114e-05, 1.1410959814384114e-05, 1.1410959814384114e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1410959814384114e-05

Optimization complete. Final v2v error: 2.90112566947937 mm

Highest mean error: 3.6236000061035156 mm for frame 64

Lowest mean error: 2.5629351139068604 mm for frame 129

Saving results

Total time: 44.01918601989746
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_helen_posed_023/1080/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_helen_posed_023/1080.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_helen_posed_023/1080
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00825072
Iteration 2/25 | Loss: 0.00179524
Iteration 3/25 | Loss: 0.00142174
Iteration 4/25 | Loss: 0.00138781
Iteration 5/25 | Loss: 0.00139580
Iteration 6/25 | Loss: 0.00137033
Iteration 7/25 | Loss: 0.00133236
Iteration 8/25 | Loss: 0.00132315
Iteration 9/25 | Loss: 0.00131446
Iteration 10/25 | Loss: 0.00132111
Iteration 11/25 | Loss: 0.00130712
Iteration 12/25 | Loss: 0.00130991
Iteration 13/25 | Loss: 0.00129901
Iteration 14/25 | Loss: 0.00130090
Iteration 15/25 | Loss: 0.00129660
Iteration 16/25 | Loss: 0.00129240
Iteration 17/25 | Loss: 0.00129334
Iteration 18/25 | Loss: 0.00129956
Iteration 19/25 | Loss: 0.00128970
Iteration 20/25 | Loss: 0.00128821
Iteration 21/25 | Loss: 0.00128786
Iteration 22/25 | Loss: 0.00128780
Iteration 23/25 | Loss: 0.00128780
Iteration 24/25 | Loss: 0.00128780
Iteration 25/25 | Loss: 0.00128780

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.59586239
Iteration 2/25 | Loss: 0.00093511
Iteration 3/25 | Loss: 0.00093510
Iteration 4/25 | Loss: 0.00093510
Iteration 5/25 | Loss: 0.00093509
Iteration 6/25 | Loss: 0.00093509
Iteration 7/25 | Loss: 0.00093509
Iteration 8/25 | Loss: 0.00093509
Iteration 9/25 | Loss: 0.00093509
Iteration 10/25 | Loss: 0.00093509
Iteration 11/25 | Loss: 0.00093509
Iteration 12/25 | Loss: 0.00093509
Iteration 13/25 | Loss: 0.00093509
Iteration 14/25 | Loss: 0.00093509
Iteration 15/25 | Loss: 0.00093509
Iteration 16/25 | Loss: 0.00093509
Iteration 17/25 | Loss: 0.00093509
Iteration 18/25 | Loss: 0.00093509
Iteration 19/25 | Loss: 0.00093509
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0009350928594358265, 0.0009350928594358265, 0.0009350928594358265, 0.0009350928594358265, 0.0009350928594358265]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009350928594358265

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00093509
Iteration 2/1000 | Loss: 0.00003140
Iteration 3/1000 | Loss: 0.00002265
Iteration 4/1000 | Loss: 0.00004125
Iteration 5/1000 | Loss: 0.00002047
Iteration 6/1000 | Loss: 0.00001941
Iteration 7/1000 | Loss: 0.00001888
Iteration 8/1000 | Loss: 0.00001840
Iteration 9/1000 | Loss: 0.00001797
Iteration 10/1000 | Loss: 0.00001772
Iteration 11/1000 | Loss: 0.00001753
Iteration 12/1000 | Loss: 0.00001742
Iteration 13/1000 | Loss: 0.00001731
Iteration 14/1000 | Loss: 0.00001725
Iteration 15/1000 | Loss: 0.00001723
Iteration 16/1000 | Loss: 0.00001723
Iteration 17/1000 | Loss: 0.00001722
Iteration 18/1000 | Loss: 0.00001722
Iteration 19/1000 | Loss: 0.00001721
Iteration 20/1000 | Loss: 0.00001720
Iteration 21/1000 | Loss: 0.00001719
Iteration 22/1000 | Loss: 0.00001718
Iteration 23/1000 | Loss: 0.00001717
Iteration 24/1000 | Loss: 0.00001714
Iteration 25/1000 | Loss: 0.00001713
Iteration 26/1000 | Loss: 0.00001713
Iteration 27/1000 | Loss: 0.00001713
Iteration 28/1000 | Loss: 0.00001713
Iteration 29/1000 | Loss: 0.00001713
Iteration 30/1000 | Loss: 0.00001713
Iteration 31/1000 | Loss: 0.00001713
Iteration 32/1000 | Loss: 0.00001713
Iteration 33/1000 | Loss: 0.00001713
Iteration 34/1000 | Loss: 0.00001713
Iteration 35/1000 | Loss: 0.00001713
Iteration 36/1000 | Loss: 0.00001712
Iteration 37/1000 | Loss: 0.00001711
Iteration 38/1000 | Loss: 0.00001711
Iteration 39/1000 | Loss: 0.00001710
Iteration 40/1000 | Loss: 0.00001710
Iteration 41/1000 | Loss: 0.00001710
Iteration 42/1000 | Loss: 0.00001710
Iteration 43/1000 | Loss: 0.00001709
Iteration 44/1000 | Loss: 0.00001709
Iteration 45/1000 | Loss: 0.00001709
Iteration 46/1000 | Loss: 0.00001708
Iteration 47/1000 | Loss: 0.00001708
Iteration 48/1000 | Loss: 0.00001707
Iteration 49/1000 | Loss: 0.00001707
Iteration 50/1000 | Loss: 0.00001707
Iteration 51/1000 | Loss: 0.00001705
Iteration 52/1000 | Loss: 0.00001705
Iteration 53/1000 | Loss: 0.00001705
Iteration 54/1000 | Loss: 0.00001705
Iteration 55/1000 | Loss: 0.00001705
Iteration 56/1000 | Loss: 0.00001705
Iteration 57/1000 | Loss: 0.00001705
Iteration 58/1000 | Loss: 0.00001705
Iteration 59/1000 | Loss: 0.00001705
Iteration 60/1000 | Loss: 0.00001704
Iteration 61/1000 | Loss: 0.00001704
Iteration 62/1000 | Loss: 0.00001704
Iteration 63/1000 | Loss: 0.00001704
Iteration 64/1000 | Loss: 0.00001703
Iteration 65/1000 | Loss: 0.00001703
Iteration 66/1000 | Loss: 0.00001702
Iteration 67/1000 | Loss: 0.00001702
Iteration 68/1000 | Loss: 0.00001702
Iteration 69/1000 | Loss: 0.00001702
Iteration 70/1000 | Loss: 0.00001702
Iteration 71/1000 | Loss: 0.00001701
Iteration 72/1000 | Loss: 0.00001701
Iteration 73/1000 | Loss: 0.00001701
Iteration 74/1000 | Loss: 0.00001701
Iteration 75/1000 | Loss: 0.00001701
Iteration 76/1000 | Loss: 0.00001700
Iteration 77/1000 | Loss: 0.00001700
Iteration 78/1000 | Loss: 0.00001700
Iteration 79/1000 | Loss: 0.00001700
Iteration 80/1000 | Loss: 0.00001700
Iteration 81/1000 | Loss: 0.00001700
Iteration 82/1000 | Loss: 0.00001699
Iteration 83/1000 | Loss: 0.00001699
Iteration 84/1000 | Loss: 0.00001699
Iteration 85/1000 | Loss: 0.00001698
Iteration 86/1000 | Loss: 0.00001698
Iteration 87/1000 | Loss: 0.00001698
Iteration 88/1000 | Loss: 0.00001698
Iteration 89/1000 | Loss: 0.00001697
Iteration 90/1000 | Loss: 0.00001697
Iteration 91/1000 | Loss: 0.00001697
Iteration 92/1000 | Loss: 0.00001696
Iteration 93/1000 | Loss: 0.00001696
Iteration 94/1000 | Loss: 0.00001695
Iteration 95/1000 | Loss: 0.00001695
Iteration 96/1000 | Loss: 0.00001695
Iteration 97/1000 | Loss: 0.00001694
Iteration 98/1000 | Loss: 0.00001694
Iteration 99/1000 | Loss: 0.00001694
Iteration 100/1000 | Loss: 0.00001693
Iteration 101/1000 | Loss: 0.00001693
Iteration 102/1000 | Loss: 0.00001693
Iteration 103/1000 | Loss: 0.00001693
Iteration 104/1000 | Loss: 0.00001693
Iteration 105/1000 | Loss: 0.00001692
Iteration 106/1000 | Loss: 0.00001692
Iteration 107/1000 | Loss: 0.00001692
Iteration 108/1000 | Loss: 0.00001692
Iteration 109/1000 | Loss: 0.00001691
Iteration 110/1000 | Loss: 0.00001691
Iteration 111/1000 | Loss: 0.00001691
Iteration 112/1000 | Loss: 0.00001691
Iteration 113/1000 | Loss: 0.00001691
Iteration 114/1000 | Loss: 0.00001690
Iteration 115/1000 | Loss: 0.00001690
Iteration 116/1000 | Loss: 0.00001690
Iteration 117/1000 | Loss: 0.00001690
Iteration 118/1000 | Loss: 0.00001690
Iteration 119/1000 | Loss: 0.00001690
Iteration 120/1000 | Loss: 0.00001690
Iteration 121/1000 | Loss: 0.00001690
Iteration 122/1000 | Loss: 0.00001690
Iteration 123/1000 | Loss: 0.00001690
Iteration 124/1000 | Loss: 0.00001690
Iteration 125/1000 | Loss: 0.00001690
Iteration 126/1000 | Loss: 0.00001690
Iteration 127/1000 | Loss: 0.00001689
Iteration 128/1000 | Loss: 0.00001689
Iteration 129/1000 | Loss: 0.00001689
Iteration 130/1000 | Loss: 0.00001688
Iteration 131/1000 | Loss: 0.00001688
Iteration 132/1000 | Loss: 0.00001688
Iteration 133/1000 | Loss: 0.00001687
Iteration 134/1000 | Loss: 0.00001687
Iteration 135/1000 | Loss: 0.00001687
Iteration 136/1000 | Loss: 0.00001687
Iteration 137/1000 | Loss: 0.00001687
Iteration 138/1000 | Loss: 0.00001687
Iteration 139/1000 | Loss: 0.00001687
Iteration 140/1000 | Loss: 0.00001687
Iteration 141/1000 | Loss: 0.00001687
Iteration 142/1000 | Loss: 0.00001687
Iteration 143/1000 | Loss: 0.00001687
Iteration 144/1000 | Loss: 0.00001687
Iteration 145/1000 | Loss: 0.00001687
Iteration 146/1000 | Loss: 0.00001686
Iteration 147/1000 | Loss: 0.00001686
Iteration 148/1000 | Loss: 0.00001686
Iteration 149/1000 | Loss: 0.00001686
Iteration 150/1000 | Loss: 0.00001685
Iteration 151/1000 | Loss: 0.00001685
Iteration 152/1000 | Loss: 0.00001685
Iteration 153/1000 | Loss: 0.00001685
Iteration 154/1000 | Loss: 0.00001685
Iteration 155/1000 | Loss: 0.00001685
Iteration 156/1000 | Loss: 0.00001684
Iteration 157/1000 | Loss: 0.00001684
Iteration 158/1000 | Loss: 0.00001684
Iteration 159/1000 | Loss: 0.00001684
Iteration 160/1000 | Loss: 0.00001684
Iteration 161/1000 | Loss: 0.00001684
Iteration 162/1000 | Loss: 0.00001683
Iteration 163/1000 | Loss: 0.00001683
Iteration 164/1000 | Loss: 0.00001683
Iteration 165/1000 | Loss: 0.00001683
Iteration 166/1000 | Loss: 0.00001683
Iteration 167/1000 | Loss: 0.00001683
Iteration 168/1000 | Loss: 0.00001683
Iteration 169/1000 | Loss: 0.00001683
Iteration 170/1000 | Loss: 0.00001683
Iteration 171/1000 | Loss: 0.00001683
Iteration 172/1000 | Loss: 0.00001683
Iteration 173/1000 | Loss: 0.00001683
Iteration 174/1000 | Loss: 0.00001683
Iteration 175/1000 | Loss: 0.00001683
Iteration 176/1000 | Loss: 0.00001683
Iteration 177/1000 | Loss: 0.00001683
Iteration 178/1000 | Loss: 0.00001683
Iteration 179/1000 | Loss: 0.00001683
Iteration 180/1000 | Loss: 0.00001683
Iteration 181/1000 | Loss: 0.00001683
Iteration 182/1000 | Loss: 0.00001683
Iteration 183/1000 | Loss: 0.00001682
Iteration 184/1000 | Loss: 0.00001682
Iteration 185/1000 | Loss: 0.00001682
Iteration 186/1000 | Loss: 0.00001682
Iteration 187/1000 | Loss: 0.00001682
Iteration 188/1000 | Loss: 0.00001682
Iteration 189/1000 | Loss: 0.00001682
Iteration 190/1000 | Loss: 0.00001682
Iteration 191/1000 | Loss: 0.00001682
Iteration 192/1000 | Loss: 0.00001682
Iteration 193/1000 | Loss: 0.00001682
Iteration 194/1000 | Loss: 0.00001682
Iteration 195/1000 | Loss: 0.00001682
Iteration 196/1000 | Loss: 0.00001682
Iteration 197/1000 | Loss: 0.00001682
Iteration 198/1000 | Loss: 0.00001682
Iteration 199/1000 | Loss: 0.00001682
Iteration 200/1000 | Loss: 0.00001682
Iteration 201/1000 | Loss: 0.00001682
Iteration 202/1000 | Loss: 0.00001682
Iteration 203/1000 | Loss: 0.00001682
Iteration 204/1000 | Loss: 0.00001682
Iteration 205/1000 | Loss: 0.00001682
Iteration 206/1000 | Loss: 0.00001682
Iteration 207/1000 | Loss: 0.00001682
Iteration 208/1000 | Loss: 0.00001682
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 208. Stopping optimization.
Last 5 losses: [1.681662251939997e-05, 1.681662251939997e-05, 1.681662251939997e-05, 1.681662251939997e-05, 1.681662251939997e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.681662251939997e-05

Optimization complete. Final v2v error: 3.4384589195251465 mm

Highest mean error: 3.7805099487304688 mm for frame 20

Lowest mean error: 3.0518550872802734 mm for frame 237

Saving results

Total time: 78.07493734359741
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_helen_posed_023/1019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_helen_posed_023/1019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_helen_posed_023/1019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00763381
Iteration 2/25 | Loss: 0.00157970
Iteration 3/25 | Loss: 0.00136167
Iteration 4/25 | Loss: 0.00134785
Iteration 5/25 | Loss: 0.00133762
Iteration 6/25 | Loss: 0.00133849
Iteration 7/25 | Loss: 0.00133449
Iteration 8/25 | Loss: 0.00133617
Iteration 9/25 | Loss: 0.00133352
Iteration 10/25 | Loss: 0.00133169
Iteration 11/25 | Loss: 0.00132936
Iteration 12/25 | Loss: 0.00132890
Iteration 13/25 | Loss: 0.00132876
Iteration 14/25 | Loss: 0.00132876
Iteration 15/25 | Loss: 0.00132876
Iteration 16/25 | Loss: 0.00132876
Iteration 17/25 | Loss: 0.00132876
Iteration 18/25 | Loss: 0.00132876
Iteration 19/25 | Loss: 0.00132876
Iteration 20/25 | Loss: 0.00132876
Iteration 21/25 | Loss: 0.00132876
Iteration 22/25 | Loss: 0.00132876
Iteration 23/25 | Loss: 0.00132876
Iteration 24/25 | Loss: 0.00132876
Iteration 25/25 | Loss: 0.00132876

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.31673253
Iteration 2/25 | Loss: 0.00103387
Iteration 3/25 | Loss: 0.00103385
Iteration 4/25 | Loss: 0.00103385
Iteration 5/25 | Loss: 0.00103384
Iteration 6/25 | Loss: 0.00103384
Iteration 7/25 | Loss: 0.00103384
Iteration 8/25 | Loss: 0.00103384
Iteration 9/25 | Loss: 0.00103384
Iteration 10/25 | Loss: 0.00103384
Iteration 11/25 | Loss: 0.00103384
Iteration 12/25 | Loss: 0.00103384
Iteration 13/25 | Loss: 0.00103384
Iteration 14/25 | Loss: 0.00103384
Iteration 15/25 | Loss: 0.00103384
Iteration 16/25 | Loss: 0.00103384
Iteration 17/25 | Loss: 0.00103384
Iteration 18/25 | Loss: 0.00103384
Iteration 19/25 | Loss: 0.00103384
Iteration 20/25 | Loss: 0.00103384
Iteration 21/25 | Loss: 0.00103384
Iteration 22/25 | Loss: 0.00103384
Iteration 23/25 | Loss: 0.00103384
Iteration 24/25 | Loss: 0.00103384
Iteration 25/25 | Loss: 0.00103384

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00103384
Iteration 2/1000 | Loss: 0.00004080
Iteration 3/1000 | Loss: 0.00002702
Iteration 4/1000 | Loss: 0.00002486
Iteration 5/1000 | Loss: 0.00002331
Iteration 6/1000 | Loss: 0.00002248
Iteration 7/1000 | Loss: 0.00002191
Iteration 8/1000 | Loss: 0.00002147
Iteration 9/1000 | Loss: 0.00002113
Iteration 10/1000 | Loss: 0.00002106
Iteration 11/1000 | Loss: 0.00002098
Iteration 12/1000 | Loss: 0.00002093
Iteration 13/1000 | Loss: 0.00002090
Iteration 14/1000 | Loss: 0.00002081
Iteration 15/1000 | Loss: 0.00002075
Iteration 16/1000 | Loss: 0.00002061
Iteration 17/1000 | Loss: 0.00002058
Iteration 18/1000 | Loss: 0.00002047
Iteration 19/1000 | Loss: 0.00002038
Iteration 20/1000 | Loss: 0.00002030
Iteration 21/1000 | Loss: 0.00002029
Iteration 22/1000 | Loss: 0.00002024
Iteration 23/1000 | Loss: 0.00002024
Iteration 24/1000 | Loss: 0.00002024
Iteration 25/1000 | Loss: 0.00002024
Iteration 26/1000 | Loss: 0.00002024
Iteration 27/1000 | Loss: 0.00002024
Iteration 28/1000 | Loss: 0.00002024
Iteration 29/1000 | Loss: 0.00002024
Iteration 30/1000 | Loss: 0.00002023
Iteration 31/1000 | Loss: 0.00002023
Iteration 32/1000 | Loss: 0.00002023
Iteration 33/1000 | Loss: 0.00002023
Iteration 34/1000 | Loss: 0.00002023
Iteration 35/1000 | Loss: 0.00002023
Iteration 36/1000 | Loss: 0.00002023
Iteration 37/1000 | Loss: 0.00002022
Iteration 38/1000 | Loss: 0.00002022
Iteration 39/1000 | Loss: 0.00002022
Iteration 40/1000 | Loss: 0.00002022
Iteration 41/1000 | Loss: 0.00002021
Iteration 42/1000 | Loss: 0.00002021
Iteration 43/1000 | Loss: 0.00002020
Iteration 44/1000 | Loss: 0.00002020
Iteration 45/1000 | Loss: 0.00002020
Iteration 46/1000 | Loss: 0.00002020
Iteration 47/1000 | Loss: 0.00002020
Iteration 48/1000 | Loss: 0.00002020
Iteration 49/1000 | Loss: 0.00002020
Iteration 50/1000 | Loss: 0.00002020
Iteration 51/1000 | Loss: 0.00002019
Iteration 52/1000 | Loss: 0.00002019
Iteration 53/1000 | Loss: 0.00002019
Iteration 54/1000 | Loss: 0.00002019
Iteration 55/1000 | Loss: 0.00002019
Iteration 56/1000 | Loss: 0.00002018
Iteration 57/1000 | Loss: 0.00002018
Iteration 58/1000 | Loss: 0.00002018
Iteration 59/1000 | Loss: 0.00002018
Iteration 60/1000 | Loss: 0.00002017
Iteration 61/1000 | Loss: 0.00002017
Iteration 62/1000 | Loss: 0.00002017
Iteration 63/1000 | Loss: 0.00002016
Iteration 64/1000 | Loss: 0.00002016
Iteration 65/1000 | Loss: 0.00002016
Iteration 66/1000 | Loss: 0.00002016
Iteration 67/1000 | Loss: 0.00002016
Iteration 68/1000 | Loss: 0.00002016
Iteration 69/1000 | Loss: 0.00002016
Iteration 70/1000 | Loss: 0.00002015
Iteration 71/1000 | Loss: 0.00002015
Iteration 72/1000 | Loss: 0.00002015
Iteration 73/1000 | Loss: 0.00002015
Iteration 74/1000 | Loss: 0.00002015
Iteration 75/1000 | Loss: 0.00002014
Iteration 76/1000 | Loss: 0.00002014
Iteration 77/1000 | Loss: 0.00002014
Iteration 78/1000 | Loss: 0.00002014
Iteration 79/1000 | Loss: 0.00002014
Iteration 80/1000 | Loss: 0.00002013
Iteration 81/1000 | Loss: 0.00002013
Iteration 82/1000 | Loss: 0.00002013
Iteration 83/1000 | Loss: 0.00002012
Iteration 84/1000 | Loss: 0.00002012
Iteration 85/1000 | Loss: 0.00002012
Iteration 86/1000 | Loss: 0.00002012
Iteration 87/1000 | Loss: 0.00002011
Iteration 88/1000 | Loss: 0.00002011
Iteration 89/1000 | Loss: 0.00002011
Iteration 90/1000 | Loss: 0.00002011
Iteration 91/1000 | Loss: 0.00002011
Iteration 92/1000 | Loss: 0.00002011
Iteration 93/1000 | Loss: 0.00002011
Iteration 94/1000 | Loss: 0.00002011
Iteration 95/1000 | Loss: 0.00002010
Iteration 96/1000 | Loss: 0.00002010
Iteration 97/1000 | Loss: 0.00002010
Iteration 98/1000 | Loss: 0.00002010
Iteration 99/1000 | Loss: 0.00002010
Iteration 100/1000 | Loss: 0.00002009
Iteration 101/1000 | Loss: 0.00002009
Iteration 102/1000 | Loss: 0.00002009
Iteration 103/1000 | Loss: 0.00002009
Iteration 104/1000 | Loss: 0.00002009
Iteration 105/1000 | Loss: 0.00002009
Iteration 106/1000 | Loss: 0.00002009
Iteration 107/1000 | Loss: 0.00002008
Iteration 108/1000 | Loss: 0.00002008
Iteration 109/1000 | Loss: 0.00002008
Iteration 110/1000 | Loss: 0.00002008
Iteration 111/1000 | Loss: 0.00002008
Iteration 112/1000 | Loss: 0.00002008
Iteration 113/1000 | Loss: 0.00002008
Iteration 114/1000 | Loss: 0.00002008
Iteration 115/1000 | Loss: 0.00002008
Iteration 116/1000 | Loss: 0.00002007
Iteration 117/1000 | Loss: 0.00002007
Iteration 118/1000 | Loss: 0.00002006
Iteration 119/1000 | Loss: 0.00002006
Iteration 120/1000 | Loss: 0.00002006
Iteration 121/1000 | Loss: 0.00002006
Iteration 122/1000 | Loss: 0.00002006
Iteration 123/1000 | Loss: 0.00002006
Iteration 124/1000 | Loss: 0.00002005
Iteration 125/1000 | Loss: 0.00002005
Iteration 126/1000 | Loss: 0.00002005
Iteration 127/1000 | Loss: 0.00002005
Iteration 128/1000 | Loss: 0.00002005
Iteration 129/1000 | Loss: 0.00002005
Iteration 130/1000 | Loss: 0.00002005
Iteration 131/1000 | Loss: 0.00002005
Iteration 132/1000 | Loss: 0.00002004
Iteration 133/1000 | Loss: 0.00002004
Iteration 134/1000 | Loss: 0.00002004
Iteration 135/1000 | Loss: 0.00002004
Iteration 136/1000 | Loss: 0.00002004
Iteration 137/1000 | Loss: 0.00002004
Iteration 138/1000 | Loss: 0.00002004
Iteration 139/1000 | Loss: 0.00002004
Iteration 140/1000 | Loss: 0.00002004
Iteration 141/1000 | Loss: 0.00002004
Iteration 142/1000 | Loss: 0.00002004
Iteration 143/1000 | Loss: 0.00002003
Iteration 144/1000 | Loss: 0.00002003
Iteration 145/1000 | Loss: 0.00002003
Iteration 146/1000 | Loss: 0.00002003
Iteration 147/1000 | Loss: 0.00002003
Iteration 148/1000 | Loss: 0.00002003
Iteration 149/1000 | Loss: 0.00002003
Iteration 150/1000 | Loss: 0.00002003
Iteration 151/1000 | Loss: 0.00002003
Iteration 152/1000 | Loss: 0.00002003
Iteration 153/1000 | Loss: 0.00002003
Iteration 154/1000 | Loss: 0.00002003
Iteration 155/1000 | Loss: 0.00002003
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 155. Stopping optimization.
Last 5 losses: [2.0030764062539674e-05, 2.0030764062539674e-05, 2.0030764062539674e-05, 2.0030764062539674e-05, 2.0030764062539674e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0030764062539674e-05

Optimization complete. Final v2v error: 3.662489414215088 mm

Highest mean error: 5.631465435028076 mm for frame 65

Lowest mean error: 3.226724863052368 mm for frame 194

Saving results

Total time: 59.64899969100952
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_helen_posed_023/1009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_helen_posed_023/1009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_helen_posed_023/1009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00762386
Iteration 2/25 | Loss: 0.00153377
Iteration 3/25 | Loss: 0.00135224
Iteration 4/25 | Loss: 0.00132046
Iteration 5/25 | Loss: 0.00131590
Iteration 6/25 | Loss: 0.00130819
Iteration 7/25 | Loss: 0.00130607
Iteration 8/25 | Loss: 0.00130161
Iteration 9/25 | Loss: 0.00130148
Iteration 10/25 | Loss: 0.00130146
Iteration 11/25 | Loss: 0.00130145
Iteration 12/25 | Loss: 0.00130145
Iteration 13/25 | Loss: 0.00130145
Iteration 14/25 | Loss: 0.00130145
Iteration 15/25 | Loss: 0.00130145
Iteration 16/25 | Loss: 0.00130145
Iteration 17/25 | Loss: 0.00130145
Iteration 18/25 | Loss: 0.00130145
Iteration 19/25 | Loss: 0.00130145
Iteration 20/25 | Loss: 0.00130145
Iteration 21/25 | Loss: 0.00130145
Iteration 22/25 | Loss: 0.00130144
Iteration 23/25 | Loss: 0.00130144
Iteration 24/25 | Loss: 0.00130144
Iteration 25/25 | Loss: 0.00130144

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.07434964
Iteration 2/25 | Loss: 0.00144206
Iteration 3/25 | Loss: 0.00143479
Iteration 4/25 | Loss: 0.00143479
Iteration 5/25 | Loss: 0.00143478
Iteration 6/25 | Loss: 0.00143478
Iteration 7/25 | Loss: 0.00143478
Iteration 8/25 | Loss: 0.00143478
Iteration 9/25 | Loss: 0.00143478
Iteration 10/25 | Loss: 0.00143478
Iteration 11/25 | Loss: 0.00143478
Iteration 12/25 | Loss: 0.00143478
Iteration 13/25 | Loss: 0.00143478
Iteration 14/25 | Loss: 0.00143478
Iteration 15/25 | Loss: 0.00143478
Iteration 16/25 | Loss: 0.00143478
Iteration 17/25 | Loss: 0.00143478
Iteration 18/25 | Loss: 0.00143478
Iteration 19/25 | Loss: 0.00143478
Iteration 20/25 | Loss: 0.00143478
Iteration 21/25 | Loss: 0.00143478
Iteration 22/25 | Loss: 0.00143478
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.001434781588613987, 0.001434781588613987, 0.001434781588613987, 0.001434781588613987, 0.001434781588613987]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001434781588613987

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00143478
Iteration 2/1000 | Loss: 0.00004854
Iteration 3/1000 | Loss: 0.00002622
Iteration 4/1000 | Loss: 0.00002342
Iteration 5/1000 | Loss: 0.00002248
Iteration 6/1000 | Loss: 0.00002190
Iteration 7/1000 | Loss: 0.00002158
Iteration 8/1000 | Loss: 0.00002123
Iteration 9/1000 | Loss: 0.00002100
Iteration 10/1000 | Loss: 0.00002067
Iteration 11/1000 | Loss: 0.00002054
Iteration 12/1000 | Loss: 0.00002046
Iteration 13/1000 | Loss: 0.00002043
Iteration 14/1000 | Loss: 0.00002042
Iteration 15/1000 | Loss: 0.00002038
Iteration 16/1000 | Loss: 0.00002036
Iteration 17/1000 | Loss: 0.00002036
Iteration 18/1000 | Loss: 0.00002035
Iteration 19/1000 | Loss: 0.00002029
Iteration 20/1000 | Loss: 0.00002027
Iteration 21/1000 | Loss: 0.00002025
Iteration 22/1000 | Loss: 0.00002024
Iteration 23/1000 | Loss: 0.00002023
Iteration 24/1000 | Loss: 0.00002018
Iteration 25/1000 | Loss: 0.00002016
Iteration 26/1000 | Loss: 0.00002016
Iteration 27/1000 | Loss: 0.00002015
Iteration 28/1000 | Loss: 0.00002015
Iteration 29/1000 | Loss: 0.00002006
Iteration 30/1000 | Loss: 0.00002006
Iteration 31/1000 | Loss: 0.00002005
Iteration 32/1000 | Loss: 0.00002003
Iteration 33/1000 | Loss: 0.00002002
Iteration 34/1000 | Loss: 0.00002002
Iteration 35/1000 | Loss: 0.00002002
Iteration 36/1000 | Loss: 0.00002002
Iteration 37/1000 | Loss: 0.00002001
Iteration 38/1000 | Loss: 0.00002000
Iteration 39/1000 | Loss: 0.00002000
Iteration 40/1000 | Loss: 0.00002000
Iteration 41/1000 | Loss: 0.00001999
Iteration 42/1000 | Loss: 0.00001999
Iteration 43/1000 | Loss: 0.00001999
Iteration 44/1000 | Loss: 0.00001999
Iteration 45/1000 | Loss: 0.00001999
Iteration 46/1000 | Loss: 0.00001998
Iteration 47/1000 | Loss: 0.00001998
Iteration 48/1000 | Loss: 0.00001998
Iteration 49/1000 | Loss: 0.00001998
Iteration 50/1000 | Loss: 0.00001998
Iteration 51/1000 | Loss: 0.00001998
Iteration 52/1000 | Loss: 0.00001998
Iteration 53/1000 | Loss: 0.00001998
Iteration 54/1000 | Loss: 0.00001998
Iteration 55/1000 | Loss: 0.00001997
Iteration 56/1000 | Loss: 0.00001997
Iteration 57/1000 | Loss: 0.00001997
Iteration 58/1000 | Loss: 0.00001996
Iteration 59/1000 | Loss: 0.00001996
Iteration 60/1000 | Loss: 0.00001996
Iteration 61/1000 | Loss: 0.00001996
Iteration 62/1000 | Loss: 0.00001995
Iteration 63/1000 | Loss: 0.00001995
Iteration 64/1000 | Loss: 0.00001995
Iteration 65/1000 | Loss: 0.00001995
Iteration 66/1000 | Loss: 0.00001995
Iteration 67/1000 | Loss: 0.00001995
Iteration 68/1000 | Loss: 0.00001995
Iteration 69/1000 | Loss: 0.00001995
Iteration 70/1000 | Loss: 0.00001994
Iteration 71/1000 | Loss: 0.00001994
Iteration 72/1000 | Loss: 0.00001994
Iteration 73/1000 | Loss: 0.00001994
Iteration 74/1000 | Loss: 0.00001994
Iteration 75/1000 | Loss: 0.00001994
Iteration 76/1000 | Loss: 0.00001994
Iteration 77/1000 | Loss: 0.00001994
Iteration 78/1000 | Loss: 0.00001994
Iteration 79/1000 | Loss: 0.00001994
Iteration 80/1000 | Loss: 0.00001994
Iteration 81/1000 | Loss: 0.00001994
Iteration 82/1000 | Loss: 0.00001993
Iteration 83/1000 | Loss: 0.00001993
Iteration 84/1000 | Loss: 0.00001993
Iteration 85/1000 | Loss: 0.00001993
Iteration 86/1000 | Loss: 0.00001993
Iteration 87/1000 | Loss: 0.00001993
Iteration 88/1000 | Loss: 0.00001993
Iteration 89/1000 | Loss: 0.00001993
Iteration 90/1000 | Loss: 0.00001993
Iteration 91/1000 | Loss: 0.00001993
Iteration 92/1000 | Loss: 0.00001993
Iteration 93/1000 | Loss: 0.00001993
Iteration 94/1000 | Loss: 0.00001993
Iteration 95/1000 | Loss: 0.00001993
Iteration 96/1000 | Loss: 0.00001993
Iteration 97/1000 | Loss: 0.00001993
Iteration 98/1000 | Loss: 0.00001992
Iteration 99/1000 | Loss: 0.00001992
Iteration 100/1000 | Loss: 0.00001992
Iteration 101/1000 | Loss: 0.00001992
Iteration 102/1000 | Loss: 0.00001991
Iteration 103/1000 | Loss: 0.00001991
Iteration 104/1000 | Loss: 0.00001991
Iteration 105/1000 | Loss: 0.00001991
Iteration 106/1000 | Loss: 0.00001990
Iteration 107/1000 | Loss: 0.00001990
Iteration 108/1000 | Loss: 0.00001990
Iteration 109/1000 | Loss: 0.00001990
Iteration 110/1000 | Loss: 0.00001990
Iteration 111/1000 | Loss: 0.00001990
Iteration 112/1000 | Loss: 0.00001989
Iteration 113/1000 | Loss: 0.00001989
Iteration 114/1000 | Loss: 0.00001989
Iteration 115/1000 | Loss: 0.00001989
Iteration 116/1000 | Loss: 0.00001989
Iteration 117/1000 | Loss: 0.00001989
Iteration 118/1000 | Loss: 0.00001989
Iteration 119/1000 | Loss: 0.00001989
Iteration 120/1000 | Loss: 0.00001988
Iteration 121/1000 | Loss: 0.00001988
Iteration 122/1000 | Loss: 0.00001988
Iteration 123/1000 | Loss: 0.00001988
Iteration 124/1000 | Loss: 0.00001988
Iteration 125/1000 | Loss: 0.00001988
Iteration 126/1000 | Loss: 0.00001988
Iteration 127/1000 | Loss: 0.00001988
Iteration 128/1000 | Loss: 0.00001988
Iteration 129/1000 | Loss: 0.00001988
Iteration 130/1000 | Loss: 0.00001988
Iteration 131/1000 | Loss: 0.00001988
Iteration 132/1000 | Loss: 0.00001988
Iteration 133/1000 | Loss: 0.00001988
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 133. Stopping optimization.
Last 5 losses: [1.9880966647178866e-05, 1.9880966647178866e-05, 1.9880966647178866e-05, 1.9880966647178866e-05, 1.9880966647178866e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9880966647178866e-05

Optimization complete. Final v2v error: 3.760465383529663 mm

Highest mean error: 4.159612655639648 mm for frame 186

Lowest mean error: 3.571404457092285 mm for frame 28

Saving results

Total time: 45.17546033859253
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_helen_posed_023/1032/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_helen_posed_023/1032.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_helen_posed_023/1032
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00639524
Iteration 2/25 | Loss: 0.00133520
Iteration 3/25 | Loss: 0.00125025
Iteration 4/25 | Loss: 0.00124456
Iteration 5/25 | Loss: 0.00124268
Iteration 6/25 | Loss: 0.00124264
Iteration 7/25 | Loss: 0.00124264
Iteration 8/25 | Loss: 0.00124264
Iteration 9/25 | Loss: 0.00124264
Iteration 10/25 | Loss: 0.00124264
Iteration 11/25 | Loss: 0.00124264
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012426447356119752, 0.0012426447356119752, 0.0012426447356119752, 0.0012426447356119752, 0.0012426447356119752]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012426447356119752

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.02892542
Iteration 2/25 | Loss: 0.00114721
Iteration 3/25 | Loss: 0.00114715
Iteration 4/25 | Loss: 0.00114715
Iteration 5/25 | Loss: 0.00114715
Iteration 6/25 | Loss: 0.00114714
Iteration 7/25 | Loss: 0.00114714
Iteration 8/25 | Loss: 0.00114714
Iteration 9/25 | Loss: 0.00114714
Iteration 10/25 | Loss: 0.00114714
Iteration 11/25 | Loss: 0.00114714
Iteration 12/25 | Loss: 0.00114714
Iteration 13/25 | Loss: 0.00114714
Iteration 14/25 | Loss: 0.00114714
Iteration 15/25 | Loss: 0.00114714
Iteration 16/25 | Loss: 0.00114714
Iteration 17/25 | Loss: 0.00114714
Iteration 18/25 | Loss: 0.00114714
Iteration 19/25 | Loss: 0.00114714
Iteration 20/25 | Loss: 0.00114714
Iteration 21/25 | Loss: 0.00114714
Iteration 22/25 | Loss: 0.00114714
Iteration 23/25 | Loss: 0.00114714
Iteration 24/25 | Loss: 0.00114714
Iteration 25/25 | Loss: 0.00114714

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00114714
Iteration 2/1000 | Loss: 0.00002343
Iteration 3/1000 | Loss: 0.00001822
Iteration 4/1000 | Loss: 0.00001710
Iteration 5/1000 | Loss: 0.00001627
Iteration 6/1000 | Loss: 0.00001573
Iteration 7/1000 | Loss: 0.00001528
Iteration 8/1000 | Loss: 0.00001496
Iteration 9/1000 | Loss: 0.00001457
Iteration 10/1000 | Loss: 0.00001429
Iteration 11/1000 | Loss: 0.00001423
Iteration 12/1000 | Loss: 0.00001406
Iteration 13/1000 | Loss: 0.00001388
Iteration 14/1000 | Loss: 0.00001385
Iteration 15/1000 | Loss: 0.00001379
Iteration 16/1000 | Loss: 0.00001370
Iteration 17/1000 | Loss: 0.00001367
Iteration 18/1000 | Loss: 0.00001365
Iteration 19/1000 | Loss: 0.00001365
Iteration 20/1000 | Loss: 0.00001364
Iteration 21/1000 | Loss: 0.00001362
Iteration 22/1000 | Loss: 0.00001360
Iteration 23/1000 | Loss: 0.00001357
Iteration 24/1000 | Loss: 0.00001356
Iteration 25/1000 | Loss: 0.00001354
Iteration 26/1000 | Loss: 0.00001350
Iteration 27/1000 | Loss: 0.00001348
Iteration 28/1000 | Loss: 0.00001347
Iteration 29/1000 | Loss: 0.00001347
Iteration 30/1000 | Loss: 0.00001346
Iteration 31/1000 | Loss: 0.00001346
Iteration 32/1000 | Loss: 0.00001345
Iteration 33/1000 | Loss: 0.00001345
Iteration 34/1000 | Loss: 0.00001345
Iteration 35/1000 | Loss: 0.00001344
Iteration 36/1000 | Loss: 0.00001343
Iteration 37/1000 | Loss: 0.00001343
Iteration 38/1000 | Loss: 0.00001343
Iteration 39/1000 | Loss: 0.00001343
Iteration 40/1000 | Loss: 0.00001343
Iteration 41/1000 | Loss: 0.00001342
Iteration 42/1000 | Loss: 0.00001341
Iteration 43/1000 | Loss: 0.00001338
Iteration 44/1000 | Loss: 0.00001337
Iteration 45/1000 | Loss: 0.00001337
Iteration 46/1000 | Loss: 0.00001337
Iteration 47/1000 | Loss: 0.00001337
Iteration 48/1000 | Loss: 0.00001337
Iteration 49/1000 | Loss: 0.00001337
Iteration 50/1000 | Loss: 0.00001337
Iteration 51/1000 | Loss: 0.00001337
Iteration 52/1000 | Loss: 0.00001337
Iteration 53/1000 | Loss: 0.00001337
Iteration 54/1000 | Loss: 0.00001337
Iteration 55/1000 | Loss: 0.00001337
Iteration 56/1000 | Loss: 0.00001337
Iteration 57/1000 | Loss: 0.00001336
Iteration 58/1000 | Loss: 0.00001336
Iteration 59/1000 | Loss: 0.00001336
Iteration 60/1000 | Loss: 0.00001336
Iteration 61/1000 | Loss: 0.00001336
Iteration 62/1000 | Loss: 0.00001335
Iteration 63/1000 | Loss: 0.00001335
Iteration 64/1000 | Loss: 0.00001335
Iteration 65/1000 | Loss: 0.00001335
Iteration 66/1000 | Loss: 0.00001334
Iteration 67/1000 | Loss: 0.00001334
Iteration 68/1000 | Loss: 0.00001333
Iteration 69/1000 | Loss: 0.00001332
Iteration 70/1000 | Loss: 0.00001332
Iteration 71/1000 | Loss: 0.00001332
Iteration 72/1000 | Loss: 0.00001332
Iteration 73/1000 | Loss: 0.00001332
Iteration 74/1000 | Loss: 0.00001331
Iteration 75/1000 | Loss: 0.00001331
Iteration 76/1000 | Loss: 0.00001331
Iteration 77/1000 | Loss: 0.00001331
Iteration 78/1000 | Loss: 0.00001330
Iteration 79/1000 | Loss: 0.00001330
Iteration 80/1000 | Loss: 0.00001330
Iteration 81/1000 | Loss: 0.00001329
Iteration 82/1000 | Loss: 0.00001329
Iteration 83/1000 | Loss: 0.00001329
Iteration 84/1000 | Loss: 0.00001329
Iteration 85/1000 | Loss: 0.00001329
Iteration 86/1000 | Loss: 0.00001329
Iteration 87/1000 | Loss: 0.00001329
Iteration 88/1000 | Loss: 0.00001329
Iteration 89/1000 | Loss: 0.00001329
Iteration 90/1000 | Loss: 0.00001329
Iteration 91/1000 | Loss: 0.00001329
Iteration 92/1000 | Loss: 0.00001329
Iteration 93/1000 | Loss: 0.00001329
Iteration 94/1000 | Loss: 0.00001329
Iteration 95/1000 | Loss: 0.00001329
Iteration 96/1000 | Loss: 0.00001329
Iteration 97/1000 | Loss: 0.00001329
Iteration 98/1000 | Loss: 0.00001329
Iteration 99/1000 | Loss: 0.00001329
Iteration 100/1000 | Loss: 0.00001329
Iteration 101/1000 | Loss: 0.00001329
Iteration 102/1000 | Loss: 0.00001329
Iteration 103/1000 | Loss: 0.00001329
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 103. Stopping optimization.
Last 5 losses: [1.3287033652886748e-05, 1.3287033652886748e-05, 1.3287033652886748e-05, 1.3287033652886748e-05, 1.3287033652886748e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3287033652886748e-05

Optimization complete. Final v2v error: 3.092751979827881 mm

Highest mean error: 3.6591055393218994 mm for frame 140

Lowest mean error: 2.872079372406006 mm for frame 17

Saving results

Total time: 34.6686155796051
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_helen_posed_023/1058/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_helen_posed_023/1058.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_helen_posed_023/1058
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00991422
Iteration 2/25 | Loss: 0.00230733
Iteration 3/25 | Loss: 0.00188589
Iteration 4/25 | Loss: 0.00181432
Iteration 5/25 | Loss: 0.00177401
Iteration 6/25 | Loss: 0.00167600
Iteration 7/25 | Loss: 0.00161287
Iteration 8/25 | Loss: 0.00156591
Iteration 9/25 | Loss: 0.00154123
Iteration 10/25 | Loss: 0.00153500
Iteration 11/25 | Loss: 0.00152945
Iteration 12/25 | Loss: 0.00151183
Iteration 13/25 | Loss: 0.00149732
Iteration 14/25 | Loss: 0.00148056
Iteration 15/25 | Loss: 0.00147639
Iteration 16/25 | Loss: 0.00148816
Iteration 17/25 | Loss: 0.00146946
Iteration 18/25 | Loss: 0.00146345
Iteration 19/25 | Loss: 0.00146214
Iteration 20/25 | Loss: 0.00146483
Iteration 21/25 | Loss: 0.00146253
Iteration 22/25 | Loss: 0.00146174
Iteration 23/25 | Loss: 0.00146159
Iteration 24/25 | Loss: 0.00146227
Iteration 25/25 | Loss: 0.00146131

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.27239919
Iteration 2/25 | Loss: 0.00405702
Iteration 3/25 | Loss: 0.00400989
Iteration 4/25 | Loss: 0.00400989
Iteration 5/25 | Loss: 0.00400989
Iteration 6/25 | Loss: 0.00400989
Iteration 7/25 | Loss: 0.00400989
Iteration 8/25 | Loss: 0.00400989
Iteration 9/25 | Loss: 0.00400989
Iteration 10/25 | Loss: 0.00400989
Iteration 11/25 | Loss: 0.00400989
Iteration 12/25 | Loss: 0.00400989
Iteration 13/25 | Loss: 0.00400989
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.00400988943874836, 0.00400988943874836, 0.00400988943874836, 0.00400988943874836, 0.00400988943874836]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00400988943874836

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00400989
Iteration 2/1000 | Loss: 0.00283978
Iteration 3/1000 | Loss: 0.00051367
Iteration 4/1000 | Loss: 0.00074239
Iteration 5/1000 | Loss: 0.00096710
Iteration 6/1000 | Loss: 0.00029780
Iteration 7/1000 | Loss: 0.00047346
Iteration 8/1000 | Loss: 0.00027745
Iteration 9/1000 | Loss: 0.00025960
Iteration 10/1000 | Loss: 0.00026641
Iteration 11/1000 | Loss: 0.00055463
Iteration 12/1000 | Loss: 0.00061094
Iteration 13/1000 | Loss: 0.00085186
Iteration 14/1000 | Loss: 0.00081203
Iteration 15/1000 | Loss: 0.00068615
Iteration 16/1000 | Loss: 0.00059123
Iteration 17/1000 | Loss: 0.00041535
Iteration 18/1000 | Loss: 0.00149245
Iteration 19/1000 | Loss: 0.00088126
Iteration 20/1000 | Loss: 0.00096050
Iteration 21/1000 | Loss: 0.00053055
Iteration 22/1000 | Loss: 0.00045319
Iteration 23/1000 | Loss: 0.00023355
Iteration 24/1000 | Loss: 0.00017978
Iteration 25/1000 | Loss: 0.00027450
Iteration 26/1000 | Loss: 0.00027455
Iteration 27/1000 | Loss: 0.00030042
Iteration 28/1000 | Loss: 0.00029861
Iteration 29/1000 | Loss: 0.00090244
Iteration 30/1000 | Loss: 0.00111205
Iteration 31/1000 | Loss: 0.00186111
Iteration 32/1000 | Loss: 0.00074241
Iteration 33/1000 | Loss: 0.00099898
Iteration 34/1000 | Loss: 0.00034552
Iteration 35/1000 | Loss: 0.00086163
Iteration 36/1000 | Loss: 0.00048488
Iteration 37/1000 | Loss: 0.00095976
Iteration 38/1000 | Loss: 0.00062132
Iteration 39/1000 | Loss: 0.00044886
Iteration 40/1000 | Loss: 0.00041151
Iteration 41/1000 | Loss: 0.00040877
Iteration 42/1000 | Loss: 0.00046383
Iteration 43/1000 | Loss: 0.00045181
Iteration 44/1000 | Loss: 0.00041233
Iteration 45/1000 | Loss: 0.00014224
Iteration 46/1000 | Loss: 0.00011833
Iteration 47/1000 | Loss: 0.00092524
Iteration 48/1000 | Loss: 0.00036842
Iteration 49/1000 | Loss: 0.00052638
Iteration 50/1000 | Loss: 0.00044368
Iteration 51/1000 | Loss: 0.00044038
Iteration 52/1000 | Loss: 0.00032972
Iteration 53/1000 | Loss: 0.00051539
Iteration 54/1000 | Loss: 0.00033397
Iteration 55/1000 | Loss: 0.00037915
Iteration 56/1000 | Loss: 0.00025506
Iteration 57/1000 | Loss: 0.00050854
Iteration 58/1000 | Loss: 0.00028757
Iteration 59/1000 | Loss: 0.00009440
Iteration 60/1000 | Loss: 0.00009998
Iteration 61/1000 | Loss: 0.00020396
Iteration 62/1000 | Loss: 0.00009528
Iteration 63/1000 | Loss: 0.00022486
Iteration 64/1000 | Loss: 0.00013176
Iteration 65/1000 | Loss: 0.00025196
Iteration 66/1000 | Loss: 0.00014071
Iteration 67/1000 | Loss: 0.00091423
Iteration 68/1000 | Loss: 0.00026523
Iteration 69/1000 | Loss: 0.00011678
Iteration 70/1000 | Loss: 0.00058678
Iteration 71/1000 | Loss: 0.00040680
Iteration 72/1000 | Loss: 0.00020572
Iteration 73/1000 | Loss: 0.00024243
Iteration 74/1000 | Loss: 0.00023883
Iteration 75/1000 | Loss: 0.00014486
Iteration 76/1000 | Loss: 0.00007659
Iteration 77/1000 | Loss: 0.00006978
Iteration 78/1000 | Loss: 0.00026077
Iteration 79/1000 | Loss: 0.00017962
Iteration 80/1000 | Loss: 0.00048575
Iteration 81/1000 | Loss: 0.00087802
Iteration 82/1000 | Loss: 0.00010760
Iteration 83/1000 | Loss: 0.00009022
Iteration 84/1000 | Loss: 0.00007832
Iteration 85/1000 | Loss: 0.00006767
Iteration 86/1000 | Loss: 0.00123796
Iteration 87/1000 | Loss: 0.00126774
Iteration 88/1000 | Loss: 0.00007650
Iteration 89/1000 | Loss: 0.00006276
Iteration 90/1000 | Loss: 0.00005562
Iteration 91/1000 | Loss: 0.00005124
Iteration 92/1000 | Loss: 0.00142360
Iteration 93/1000 | Loss: 0.00029399
Iteration 94/1000 | Loss: 0.00027942
Iteration 95/1000 | Loss: 0.00006543
Iteration 96/1000 | Loss: 0.00065445
Iteration 97/1000 | Loss: 0.00057523
Iteration 98/1000 | Loss: 0.00005392
Iteration 99/1000 | Loss: 0.00004586
Iteration 100/1000 | Loss: 0.00004257
Iteration 101/1000 | Loss: 0.00004746
Iteration 102/1000 | Loss: 0.00004131
Iteration 103/1000 | Loss: 0.00004294
Iteration 104/1000 | Loss: 0.00004010
Iteration 105/1000 | Loss: 0.00003724
Iteration 106/1000 | Loss: 0.00003987
Iteration 107/1000 | Loss: 0.00003649
Iteration 108/1000 | Loss: 0.00012195
Iteration 109/1000 | Loss: 0.00009295
Iteration 110/1000 | Loss: 0.00009685
Iteration 111/1000 | Loss: 0.00011101
Iteration 112/1000 | Loss: 0.00003746
Iteration 113/1000 | Loss: 0.00003633
Iteration 114/1000 | Loss: 0.00003503
Iteration 115/1000 | Loss: 0.00003626
Iteration 116/1000 | Loss: 0.00003364
Iteration 117/1000 | Loss: 0.00003317
Iteration 118/1000 | Loss: 0.00003287
Iteration 119/1000 | Loss: 0.00003267
Iteration 120/1000 | Loss: 0.00003260
Iteration 121/1000 | Loss: 0.00003258
Iteration 122/1000 | Loss: 0.00003258
Iteration 123/1000 | Loss: 0.00003257
Iteration 124/1000 | Loss: 0.00003257
Iteration 125/1000 | Loss: 0.00003256
Iteration 126/1000 | Loss: 0.00003256
Iteration 127/1000 | Loss: 0.00003255
Iteration 128/1000 | Loss: 0.00003255
Iteration 129/1000 | Loss: 0.00003254
Iteration 130/1000 | Loss: 0.00003784
Iteration 131/1000 | Loss: 0.00003415
Iteration 132/1000 | Loss: 0.00007368
Iteration 133/1000 | Loss: 0.00003664
Iteration 134/1000 | Loss: 0.00003223
Iteration 135/1000 | Loss: 0.00003328
Iteration 136/1000 | Loss: 0.00003215
Iteration 137/1000 | Loss: 0.00003215
Iteration 138/1000 | Loss: 0.00003215
Iteration 139/1000 | Loss: 0.00003215
Iteration 140/1000 | Loss: 0.00003215
Iteration 141/1000 | Loss: 0.00003214
Iteration 142/1000 | Loss: 0.00003214
Iteration 143/1000 | Loss: 0.00003214
Iteration 144/1000 | Loss: 0.00003214
Iteration 145/1000 | Loss: 0.00003214
Iteration 146/1000 | Loss: 0.00003214
Iteration 147/1000 | Loss: 0.00003214
Iteration 148/1000 | Loss: 0.00003214
Iteration 149/1000 | Loss: 0.00003202
Iteration 150/1000 | Loss: 0.00003200
Iteration 151/1000 | Loss: 0.00003195
Iteration 152/1000 | Loss: 0.00003194
Iteration 153/1000 | Loss: 0.00003193
Iteration 154/1000 | Loss: 0.00003192
Iteration 155/1000 | Loss: 0.00003191
Iteration 156/1000 | Loss: 0.00003190
Iteration 157/1000 | Loss: 0.00003189
Iteration 158/1000 | Loss: 0.00003189
Iteration 159/1000 | Loss: 0.00003188
Iteration 160/1000 | Loss: 0.00003186
Iteration 161/1000 | Loss: 0.00003186
Iteration 162/1000 | Loss: 0.00003185
Iteration 163/1000 | Loss: 0.00003185
Iteration 164/1000 | Loss: 0.00003183
Iteration 165/1000 | Loss: 0.00003182
Iteration 166/1000 | Loss: 0.00003181
Iteration 167/1000 | Loss: 0.00019039
Iteration 168/1000 | Loss: 0.00014549
Iteration 169/1000 | Loss: 0.00021775
Iteration 170/1000 | Loss: 0.00006865
Iteration 171/1000 | Loss: 0.00008600
Iteration 172/1000 | Loss: 0.00004797
Iteration 173/1000 | Loss: 0.00003524
Iteration 174/1000 | Loss: 0.00003395
Iteration 175/1000 | Loss: 0.00003340
Iteration 176/1000 | Loss: 0.00003561
Iteration 177/1000 | Loss: 0.00003293
Iteration 178/1000 | Loss: 0.00003289
Iteration 179/1000 | Loss: 0.00003287
Iteration 180/1000 | Loss: 0.00015708
Iteration 181/1000 | Loss: 0.00003421
Iteration 182/1000 | Loss: 0.00003586
Iteration 183/1000 | Loss: 0.00003405
Iteration 184/1000 | Loss: 0.00003213
Iteration 185/1000 | Loss: 0.00003303
Iteration 186/1000 | Loss: 0.00003135
Iteration 187/1000 | Loss: 0.00003174
Iteration 188/1000 | Loss: 0.00003111
Iteration 189/1000 | Loss: 0.00003113
Iteration 190/1000 | Loss: 0.00003108
Iteration 191/1000 | Loss: 0.00003106
Iteration 192/1000 | Loss: 0.00003106
Iteration 193/1000 | Loss: 0.00003106
Iteration 194/1000 | Loss: 0.00003106
Iteration 195/1000 | Loss: 0.00003106
Iteration 196/1000 | Loss: 0.00003105
Iteration 197/1000 | Loss: 0.00003105
Iteration 198/1000 | Loss: 0.00003105
Iteration 199/1000 | Loss: 0.00003105
Iteration 200/1000 | Loss: 0.00003105
Iteration 201/1000 | Loss: 0.00003105
Iteration 202/1000 | Loss: 0.00003105
Iteration 203/1000 | Loss: 0.00003105
Iteration 204/1000 | Loss: 0.00003105
Iteration 205/1000 | Loss: 0.00003104
Iteration 206/1000 | Loss: 0.00003104
Iteration 207/1000 | Loss: 0.00003104
Iteration 208/1000 | Loss: 0.00003100
Iteration 209/1000 | Loss: 0.00003129
Iteration 210/1000 | Loss: 0.00003098
Iteration 211/1000 | Loss: 0.00003097
Iteration 212/1000 | Loss: 0.00003097
Iteration 213/1000 | Loss: 0.00003097
Iteration 214/1000 | Loss: 0.00003097
Iteration 215/1000 | Loss: 0.00003097
Iteration 216/1000 | Loss: 0.00003097
Iteration 217/1000 | Loss: 0.00003097
Iteration 218/1000 | Loss: 0.00003097
Iteration 219/1000 | Loss: 0.00003096
Iteration 220/1000 | Loss: 0.00003096
Iteration 221/1000 | Loss: 0.00003096
Iteration 222/1000 | Loss: 0.00003096
Iteration 223/1000 | Loss: 0.00003096
Iteration 224/1000 | Loss: 0.00003096
Iteration 225/1000 | Loss: 0.00003095
Iteration 226/1000 | Loss: 0.00003095
Iteration 227/1000 | Loss: 0.00003095
Iteration 228/1000 | Loss: 0.00003095
Iteration 229/1000 | Loss: 0.00003095
Iteration 230/1000 | Loss: 0.00003095
Iteration 231/1000 | Loss: 0.00003095
Iteration 232/1000 | Loss: 0.00003094
Iteration 233/1000 | Loss: 0.00003094
Iteration 234/1000 | Loss: 0.00003094
Iteration 235/1000 | Loss: 0.00003093
Iteration 236/1000 | Loss: 0.00003096
Iteration 237/1000 | Loss: 0.00003093
Iteration 238/1000 | Loss: 0.00003092
Iteration 239/1000 | Loss: 0.00003092
Iteration 240/1000 | Loss: 0.00003092
Iteration 241/1000 | Loss: 0.00003092
Iteration 242/1000 | Loss: 0.00003092
Iteration 243/1000 | Loss: 0.00003092
Iteration 244/1000 | Loss: 0.00003092
Iteration 245/1000 | Loss: 0.00003092
Iteration 246/1000 | Loss: 0.00003092
Iteration 247/1000 | Loss: 0.00003092
Iteration 248/1000 | Loss: 0.00003092
Iteration 249/1000 | Loss: 0.00003092
Iteration 250/1000 | Loss: 0.00003092
Iteration 251/1000 | Loss: 0.00003091
Iteration 252/1000 | Loss: 0.00003091
Iteration 253/1000 | Loss: 0.00003091
Iteration 254/1000 | Loss: 0.00003091
Iteration 255/1000 | Loss: 0.00003091
Iteration 256/1000 | Loss: 0.00003091
Iteration 257/1000 | Loss: 0.00003090
Iteration 258/1000 | Loss: 0.00003090
Iteration 259/1000 | Loss: 0.00003090
Iteration 260/1000 | Loss: 0.00003089
Iteration 261/1000 | Loss: 0.00003089
Iteration 262/1000 | Loss: 0.00003089
Iteration 263/1000 | Loss: 0.00003089
Iteration 264/1000 | Loss: 0.00003089
Iteration 265/1000 | Loss: 0.00003089
Iteration 266/1000 | Loss: 0.00003089
Iteration 267/1000 | Loss: 0.00003089
Iteration 268/1000 | Loss: 0.00003088
Iteration 269/1000 | Loss: 0.00003088
Iteration 270/1000 | Loss: 0.00003088
Iteration 271/1000 | Loss: 0.00003088
Iteration 272/1000 | Loss: 0.00003088
Iteration 273/1000 | Loss: 0.00003088
Iteration 274/1000 | Loss: 0.00003088
Iteration 275/1000 | Loss: 0.00003088
Iteration 276/1000 | Loss: 0.00003088
Iteration 277/1000 | Loss: 0.00003088
Iteration 278/1000 | Loss: 0.00003088
Iteration 279/1000 | Loss: 0.00003088
Iteration 280/1000 | Loss: 0.00003088
Iteration 281/1000 | Loss: 0.00003088
Iteration 282/1000 | Loss: 0.00003088
Iteration 283/1000 | Loss: 0.00003088
Iteration 284/1000 | Loss: 0.00003088
Iteration 285/1000 | Loss: 0.00003088
Iteration 286/1000 | Loss: 0.00003088
Iteration 287/1000 | Loss: 0.00003088
Iteration 288/1000 | Loss: 0.00003088
Iteration 289/1000 | Loss: 0.00003087
Iteration 290/1000 | Loss: 0.00003087
Iteration 291/1000 | Loss: 0.00003087
Iteration 292/1000 | Loss: 0.00003087
Iteration 293/1000 | Loss: 0.00003087
Iteration 294/1000 | Loss: 0.00003087
Iteration 295/1000 | Loss: 0.00003087
Iteration 296/1000 | Loss: 0.00003087
Iteration 297/1000 | Loss: 0.00003087
Iteration 298/1000 | Loss: 0.00003087
Iteration 299/1000 | Loss: 0.00003087
Iteration 300/1000 | Loss: 0.00003087
Iteration 301/1000 | Loss: 0.00003087
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 301. Stopping optimization.
Last 5 losses: [3.08720045723021e-05, 3.08720045723021e-05, 3.08720045723021e-05, 3.08720045723021e-05, 3.08720045723021e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.08720045723021e-05

Optimization complete. Final v2v error: 3.939703941345215 mm

Highest mean error: 11.480512619018555 mm for frame 43

Lowest mean error: 3.0760421752929688 mm for frame 7

Saving results

Total time: 293.7483525276184
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_helen_posed_023/1044/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_helen_posed_023/1044.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_helen_posed_023/1044
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00969103
Iteration 2/25 | Loss: 0.00280326
Iteration 3/25 | Loss: 0.00247208
Iteration 4/25 | Loss: 0.00243497
Iteration 5/25 | Loss: 0.00210610
Iteration 6/25 | Loss: 0.00204682
Iteration 7/25 | Loss: 0.00201453
Iteration 8/25 | Loss: 0.00200742
Iteration 9/25 | Loss: 0.00198377
Iteration 10/25 | Loss: 0.00196823
Iteration 11/25 | Loss: 0.00198138
Iteration 12/25 | Loss: 0.00194916
Iteration 13/25 | Loss: 0.00194263
Iteration 14/25 | Loss: 0.00194544
Iteration 15/25 | Loss: 0.00194430
Iteration 16/25 | Loss: 0.00194197
Iteration 17/25 | Loss: 0.00193808
Iteration 18/25 | Loss: 0.00193707
Iteration 19/25 | Loss: 0.00193685
Iteration 20/25 | Loss: 0.00193684
Iteration 21/25 | Loss: 0.00193683
Iteration 22/25 | Loss: 0.00193683
Iteration 23/25 | Loss: 0.00193683
Iteration 24/25 | Loss: 0.00193683
Iteration 25/25 | Loss: 0.00193683

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.31148076
Iteration 2/25 | Loss: 0.00506028
Iteration 3/25 | Loss: 0.00506028
Iteration 4/25 | Loss: 0.00506028
Iteration 5/25 | Loss: 0.00506028
Iteration 6/25 | Loss: 0.00506028
Iteration 7/25 | Loss: 0.00506028
Iteration 8/25 | Loss: 0.00506028
Iteration 9/25 | Loss: 0.00506028
Iteration 10/25 | Loss: 0.00506028
Iteration 11/25 | Loss: 0.00506028
Iteration 12/25 | Loss: 0.00506028
Iteration 13/25 | Loss: 0.00506028
Iteration 14/25 | Loss: 0.00506028
Iteration 15/25 | Loss: 0.00506028
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.005060277413576841, 0.005060277413576841, 0.005060277413576841, 0.005060277413576841, 0.005060277413576841]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.005060277413576841

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00506028
Iteration 2/1000 | Loss: 0.00068969
Iteration 3/1000 | Loss: 0.00057595
Iteration 4/1000 | Loss: 0.00046266
Iteration 5/1000 | Loss: 0.00042512
Iteration 6/1000 | Loss: 0.00039542
Iteration 7/1000 | Loss: 0.00036457
Iteration 8/1000 | Loss: 0.00034408
Iteration 9/1000 | Loss: 0.00032292
Iteration 10/1000 | Loss: 0.00031077
Iteration 11/1000 | Loss: 0.00029969
Iteration 12/1000 | Loss: 0.00029057
Iteration 13/1000 | Loss: 0.00028645
Iteration 14/1000 | Loss: 0.00028320
Iteration 15/1000 | Loss: 0.00028072
Iteration 16/1000 | Loss: 0.00027899
Iteration 17/1000 | Loss: 0.00027743
Iteration 18/1000 | Loss: 0.00027645
Iteration 19/1000 | Loss: 0.00027551
Iteration 20/1000 | Loss: 0.00027477
Iteration 21/1000 | Loss: 0.00027407
Iteration 22/1000 | Loss: 0.00027360
Iteration 23/1000 | Loss: 0.00027324
Iteration 24/1000 | Loss: 0.00027298
Iteration 25/1000 | Loss: 0.00027269
Iteration 26/1000 | Loss: 0.00027252
Iteration 27/1000 | Loss: 0.00027238
Iteration 28/1000 | Loss: 0.00027233
Iteration 29/1000 | Loss: 0.00027231
Iteration 30/1000 | Loss: 0.00027230
Iteration 31/1000 | Loss: 0.00027229
Iteration 32/1000 | Loss: 0.00027229
Iteration 33/1000 | Loss: 0.00027227
Iteration 34/1000 | Loss: 0.00027227
Iteration 35/1000 | Loss: 0.00027227
Iteration 36/1000 | Loss: 0.00027227
Iteration 37/1000 | Loss: 0.00027227
Iteration 38/1000 | Loss: 0.00027227
Iteration 39/1000 | Loss: 0.00027227
Iteration 40/1000 | Loss: 0.00027226
Iteration 41/1000 | Loss: 0.00027226
Iteration 42/1000 | Loss: 0.00027226
Iteration 43/1000 | Loss: 0.00027226
Iteration 44/1000 | Loss: 0.00027226
Iteration 45/1000 | Loss: 0.00027224
Iteration 46/1000 | Loss: 0.00027224
Iteration 47/1000 | Loss: 0.00027224
Iteration 48/1000 | Loss: 0.00027224
Iteration 49/1000 | Loss: 0.00027224
Iteration 50/1000 | Loss: 0.00027223
Iteration 51/1000 | Loss: 0.00027223
Iteration 52/1000 | Loss: 0.00027223
Iteration 53/1000 | Loss: 0.00027223
Iteration 54/1000 | Loss: 0.00027222
Iteration 55/1000 | Loss: 0.00027222
Iteration 56/1000 | Loss: 0.00027222
Iteration 57/1000 | Loss: 0.00027222
Iteration 58/1000 | Loss: 0.00027222
Iteration 59/1000 | Loss: 0.00027222
Iteration 60/1000 | Loss: 0.00027222
Iteration 61/1000 | Loss: 0.00027222
Iteration 62/1000 | Loss: 0.00027222
Iteration 63/1000 | Loss: 0.00027222
Iteration 64/1000 | Loss: 0.00027222
Iteration 65/1000 | Loss: 0.00027222
Iteration 66/1000 | Loss: 0.00027222
Iteration 67/1000 | Loss: 0.00027222
Iteration 68/1000 | Loss: 0.00027222
Iteration 69/1000 | Loss: 0.00027222
Iteration 70/1000 | Loss: 0.00027222
Iteration 71/1000 | Loss: 0.00027222
Iteration 72/1000 | Loss: 0.00027222
Iteration 73/1000 | Loss: 0.00027222
Iteration 74/1000 | Loss: 0.00027222
Iteration 75/1000 | Loss: 0.00027222
Iteration 76/1000 | Loss: 0.00027222
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 76. Stopping optimization.
Last 5 losses: [0.00027221936034038663, 0.00027221936034038663, 0.00027221936034038663, 0.00027221936034038663, 0.00027221936034038663]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00027221936034038663

Optimization complete. Final v2v error: 9.71712589263916 mm

Highest mean error: 10.353350639343262 mm for frame 23

Lowest mean error: 5.527336597442627 mm for frame 0

Saving results

Total time: 73.4645643234253
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_helen_posed_023/1077/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_helen_posed_023/1077.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_helen_posed_023/1077
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00491791
Iteration 2/25 | Loss: 0.00151738
Iteration 3/25 | Loss: 0.00126581
Iteration 4/25 | Loss: 0.00123454
Iteration 5/25 | Loss: 0.00122968
Iteration 6/25 | Loss: 0.00122824
Iteration 7/25 | Loss: 0.00122824
Iteration 8/25 | Loss: 0.00122824
Iteration 9/25 | Loss: 0.00122824
Iteration 10/25 | Loss: 0.00122824
Iteration 11/25 | Loss: 0.00122824
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012282398529350758, 0.0012282398529350758, 0.0012282398529350758, 0.0012282398529350758, 0.0012282398529350758]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012282398529350758

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33519018
Iteration 2/25 | Loss: 0.00109138
Iteration 3/25 | Loss: 0.00109137
Iteration 4/25 | Loss: 0.00109137
Iteration 5/25 | Loss: 0.00109137
Iteration 6/25 | Loss: 0.00109137
Iteration 7/25 | Loss: 0.00109137
Iteration 8/25 | Loss: 0.00109137
Iteration 9/25 | Loss: 0.00109137
Iteration 10/25 | Loss: 0.00109137
Iteration 11/25 | Loss: 0.00109137
Iteration 12/25 | Loss: 0.00109137
Iteration 13/25 | Loss: 0.00109137
Iteration 14/25 | Loss: 0.00109137
Iteration 15/25 | Loss: 0.00109137
Iteration 16/25 | Loss: 0.00109137
Iteration 17/25 | Loss: 0.00109137
Iteration 18/25 | Loss: 0.00109137
Iteration 19/25 | Loss: 0.00109137
Iteration 20/25 | Loss: 0.00109137
Iteration 21/25 | Loss: 0.00109137
Iteration 22/25 | Loss: 0.00109137
Iteration 23/25 | Loss: 0.00109137
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0010913703590631485, 0.0010913703590631485, 0.0010913703590631485, 0.0010913703590631485, 0.0010913703590631485]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010913703590631485

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00109137
Iteration 2/1000 | Loss: 0.00002948
Iteration 3/1000 | Loss: 0.00001975
Iteration 4/1000 | Loss: 0.00001826
Iteration 5/1000 | Loss: 0.00001718
Iteration 6/1000 | Loss: 0.00001644
Iteration 7/1000 | Loss: 0.00001597
Iteration 8/1000 | Loss: 0.00001563
Iteration 9/1000 | Loss: 0.00001532
Iteration 10/1000 | Loss: 0.00001506
Iteration 11/1000 | Loss: 0.00001485
Iteration 12/1000 | Loss: 0.00001477
Iteration 13/1000 | Loss: 0.00001472
Iteration 14/1000 | Loss: 0.00001470
Iteration 15/1000 | Loss: 0.00001465
Iteration 16/1000 | Loss: 0.00001464
Iteration 17/1000 | Loss: 0.00001458
Iteration 18/1000 | Loss: 0.00001457
Iteration 19/1000 | Loss: 0.00001457
Iteration 20/1000 | Loss: 0.00001456
Iteration 21/1000 | Loss: 0.00001456
Iteration 22/1000 | Loss: 0.00001453
Iteration 23/1000 | Loss: 0.00001452
Iteration 24/1000 | Loss: 0.00001452
Iteration 25/1000 | Loss: 0.00001451
Iteration 26/1000 | Loss: 0.00001451
Iteration 27/1000 | Loss: 0.00001451
Iteration 28/1000 | Loss: 0.00001449
Iteration 29/1000 | Loss: 0.00001447
Iteration 30/1000 | Loss: 0.00001447
Iteration 31/1000 | Loss: 0.00001446
Iteration 32/1000 | Loss: 0.00001446
Iteration 33/1000 | Loss: 0.00001446
Iteration 34/1000 | Loss: 0.00001445
Iteration 35/1000 | Loss: 0.00001445
Iteration 36/1000 | Loss: 0.00001444
Iteration 37/1000 | Loss: 0.00001443
Iteration 38/1000 | Loss: 0.00001443
Iteration 39/1000 | Loss: 0.00001443
Iteration 40/1000 | Loss: 0.00001443
Iteration 41/1000 | Loss: 0.00001443
Iteration 42/1000 | Loss: 0.00001442
Iteration 43/1000 | Loss: 0.00001442
Iteration 44/1000 | Loss: 0.00001442
Iteration 45/1000 | Loss: 0.00001441
Iteration 46/1000 | Loss: 0.00001440
Iteration 47/1000 | Loss: 0.00001440
Iteration 48/1000 | Loss: 0.00001440
Iteration 49/1000 | Loss: 0.00001440
Iteration 50/1000 | Loss: 0.00001439
Iteration 51/1000 | Loss: 0.00001439
Iteration 52/1000 | Loss: 0.00001439
Iteration 53/1000 | Loss: 0.00001439
Iteration 54/1000 | Loss: 0.00001439
Iteration 55/1000 | Loss: 0.00001439
Iteration 56/1000 | Loss: 0.00001438
Iteration 57/1000 | Loss: 0.00001438
Iteration 58/1000 | Loss: 0.00001438
Iteration 59/1000 | Loss: 0.00001438
Iteration 60/1000 | Loss: 0.00001438
Iteration 61/1000 | Loss: 0.00001438
Iteration 62/1000 | Loss: 0.00001437
Iteration 63/1000 | Loss: 0.00001437
Iteration 64/1000 | Loss: 0.00001436
Iteration 65/1000 | Loss: 0.00001436
Iteration 66/1000 | Loss: 0.00001436
Iteration 67/1000 | Loss: 0.00001435
Iteration 68/1000 | Loss: 0.00001435
Iteration 69/1000 | Loss: 0.00001435
Iteration 70/1000 | Loss: 0.00001435
Iteration 71/1000 | Loss: 0.00001434
Iteration 72/1000 | Loss: 0.00001434
Iteration 73/1000 | Loss: 0.00001433
Iteration 74/1000 | Loss: 0.00001433
Iteration 75/1000 | Loss: 0.00001433
Iteration 76/1000 | Loss: 0.00001432
Iteration 77/1000 | Loss: 0.00001431
Iteration 78/1000 | Loss: 0.00001431
Iteration 79/1000 | Loss: 0.00001430
Iteration 80/1000 | Loss: 0.00001430
Iteration 81/1000 | Loss: 0.00001429
Iteration 82/1000 | Loss: 0.00001429
Iteration 83/1000 | Loss: 0.00001428
Iteration 84/1000 | Loss: 0.00001428
Iteration 85/1000 | Loss: 0.00001428
Iteration 86/1000 | Loss: 0.00001427
Iteration 87/1000 | Loss: 0.00001427
Iteration 88/1000 | Loss: 0.00001426
Iteration 89/1000 | Loss: 0.00001426
Iteration 90/1000 | Loss: 0.00001426
Iteration 91/1000 | Loss: 0.00001426
Iteration 92/1000 | Loss: 0.00001426
Iteration 93/1000 | Loss: 0.00001426
Iteration 94/1000 | Loss: 0.00001426
Iteration 95/1000 | Loss: 0.00001425
Iteration 96/1000 | Loss: 0.00001425
Iteration 97/1000 | Loss: 0.00001425
Iteration 98/1000 | Loss: 0.00001424
Iteration 99/1000 | Loss: 0.00001424
Iteration 100/1000 | Loss: 0.00001424
Iteration 101/1000 | Loss: 0.00001423
Iteration 102/1000 | Loss: 0.00001423
Iteration 103/1000 | Loss: 0.00001423
Iteration 104/1000 | Loss: 0.00001423
Iteration 105/1000 | Loss: 0.00001423
Iteration 106/1000 | Loss: 0.00001423
Iteration 107/1000 | Loss: 0.00001423
Iteration 108/1000 | Loss: 0.00001423
Iteration 109/1000 | Loss: 0.00001423
Iteration 110/1000 | Loss: 0.00001423
Iteration 111/1000 | Loss: 0.00001422
Iteration 112/1000 | Loss: 0.00001422
Iteration 113/1000 | Loss: 0.00001422
Iteration 114/1000 | Loss: 0.00001422
Iteration 115/1000 | Loss: 0.00001422
Iteration 116/1000 | Loss: 0.00001421
Iteration 117/1000 | Loss: 0.00001421
Iteration 118/1000 | Loss: 0.00001421
Iteration 119/1000 | Loss: 0.00001421
Iteration 120/1000 | Loss: 0.00001420
Iteration 121/1000 | Loss: 0.00001420
Iteration 122/1000 | Loss: 0.00001420
Iteration 123/1000 | Loss: 0.00001420
Iteration 124/1000 | Loss: 0.00001420
Iteration 125/1000 | Loss: 0.00001419
Iteration 126/1000 | Loss: 0.00001419
Iteration 127/1000 | Loss: 0.00001419
Iteration 128/1000 | Loss: 0.00001419
Iteration 129/1000 | Loss: 0.00001418
Iteration 130/1000 | Loss: 0.00001418
Iteration 131/1000 | Loss: 0.00001418
Iteration 132/1000 | Loss: 0.00001418
Iteration 133/1000 | Loss: 0.00001417
Iteration 134/1000 | Loss: 0.00001417
Iteration 135/1000 | Loss: 0.00001416
Iteration 136/1000 | Loss: 0.00001416
Iteration 137/1000 | Loss: 0.00001415
Iteration 138/1000 | Loss: 0.00001415
Iteration 139/1000 | Loss: 0.00001415
Iteration 140/1000 | Loss: 0.00001414
Iteration 141/1000 | Loss: 0.00001414
Iteration 142/1000 | Loss: 0.00001414
Iteration 143/1000 | Loss: 0.00001413
Iteration 144/1000 | Loss: 0.00001413
Iteration 145/1000 | Loss: 0.00001412
Iteration 146/1000 | Loss: 0.00001412
Iteration 147/1000 | Loss: 0.00001412
Iteration 148/1000 | Loss: 0.00001411
Iteration 149/1000 | Loss: 0.00001411
Iteration 150/1000 | Loss: 0.00001411
Iteration 151/1000 | Loss: 0.00001410
Iteration 152/1000 | Loss: 0.00001410
Iteration 153/1000 | Loss: 0.00001410
Iteration 154/1000 | Loss: 0.00001410
Iteration 155/1000 | Loss: 0.00001409
Iteration 156/1000 | Loss: 0.00001409
Iteration 157/1000 | Loss: 0.00001409
Iteration 158/1000 | Loss: 0.00001409
Iteration 159/1000 | Loss: 0.00001409
Iteration 160/1000 | Loss: 0.00001409
Iteration 161/1000 | Loss: 0.00001408
Iteration 162/1000 | Loss: 0.00001408
Iteration 163/1000 | Loss: 0.00001408
Iteration 164/1000 | Loss: 0.00001408
Iteration 165/1000 | Loss: 0.00001408
Iteration 166/1000 | Loss: 0.00001408
Iteration 167/1000 | Loss: 0.00001407
Iteration 168/1000 | Loss: 0.00001407
Iteration 169/1000 | Loss: 0.00001407
Iteration 170/1000 | Loss: 0.00001406
Iteration 171/1000 | Loss: 0.00001406
Iteration 172/1000 | Loss: 0.00001406
Iteration 173/1000 | Loss: 0.00001406
Iteration 174/1000 | Loss: 0.00001406
Iteration 175/1000 | Loss: 0.00001406
Iteration 176/1000 | Loss: 0.00001406
Iteration 177/1000 | Loss: 0.00001405
Iteration 178/1000 | Loss: 0.00001405
Iteration 179/1000 | Loss: 0.00001405
Iteration 180/1000 | Loss: 0.00001405
Iteration 181/1000 | Loss: 0.00001405
Iteration 182/1000 | Loss: 0.00001405
Iteration 183/1000 | Loss: 0.00001405
Iteration 184/1000 | Loss: 0.00001405
Iteration 185/1000 | Loss: 0.00001405
Iteration 186/1000 | Loss: 0.00001404
Iteration 187/1000 | Loss: 0.00001404
Iteration 188/1000 | Loss: 0.00001404
Iteration 189/1000 | Loss: 0.00001404
Iteration 190/1000 | Loss: 0.00001404
Iteration 191/1000 | Loss: 0.00001404
Iteration 192/1000 | Loss: 0.00001404
Iteration 193/1000 | Loss: 0.00001403
Iteration 194/1000 | Loss: 0.00001403
Iteration 195/1000 | Loss: 0.00001403
Iteration 196/1000 | Loss: 0.00001403
Iteration 197/1000 | Loss: 0.00001403
Iteration 198/1000 | Loss: 0.00001403
Iteration 199/1000 | Loss: 0.00001403
Iteration 200/1000 | Loss: 0.00001402
Iteration 201/1000 | Loss: 0.00001402
Iteration 202/1000 | Loss: 0.00001402
Iteration 203/1000 | Loss: 0.00001402
Iteration 204/1000 | Loss: 0.00001402
Iteration 205/1000 | Loss: 0.00001402
Iteration 206/1000 | Loss: 0.00001402
Iteration 207/1000 | Loss: 0.00001402
Iteration 208/1000 | Loss: 0.00001402
Iteration 209/1000 | Loss: 0.00001402
Iteration 210/1000 | Loss: 0.00001402
Iteration 211/1000 | Loss: 0.00001401
Iteration 212/1000 | Loss: 0.00001401
Iteration 213/1000 | Loss: 0.00001401
Iteration 214/1000 | Loss: 0.00001401
Iteration 215/1000 | Loss: 0.00001401
Iteration 216/1000 | Loss: 0.00001401
Iteration 217/1000 | Loss: 0.00001401
Iteration 218/1000 | Loss: 0.00001401
Iteration 219/1000 | Loss: 0.00001401
Iteration 220/1000 | Loss: 0.00001401
Iteration 221/1000 | Loss: 0.00001401
Iteration 222/1000 | Loss: 0.00001401
Iteration 223/1000 | Loss: 0.00001401
Iteration 224/1000 | Loss: 0.00001401
Iteration 225/1000 | Loss: 0.00001401
Iteration 226/1000 | Loss: 0.00001401
Iteration 227/1000 | Loss: 0.00001401
Iteration 228/1000 | Loss: 0.00001401
Iteration 229/1000 | Loss: 0.00001401
Iteration 230/1000 | Loss: 0.00001401
Iteration 231/1000 | Loss: 0.00001401
Iteration 232/1000 | Loss: 0.00001401
Iteration 233/1000 | Loss: 0.00001401
Iteration 234/1000 | Loss: 0.00001401
Iteration 235/1000 | Loss: 0.00001401
Iteration 236/1000 | Loss: 0.00001401
Iteration 237/1000 | Loss: 0.00001401
Iteration 238/1000 | Loss: 0.00001401
Iteration 239/1000 | Loss: 0.00001401
Iteration 240/1000 | Loss: 0.00001401
Iteration 241/1000 | Loss: 0.00001401
Iteration 242/1000 | Loss: 0.00001401
Iteration 243/1000 | Loss: 0.00001401
Iteration 244/1000 | Loss: 0.00001401
Iteration 245/1000 | Loss: 0.00001401
Iteration 246/1000 | Loss: 0.00001401
Iteration 247/1000 | Loss: 0.00001401
Iteration 248/1000 | Loss: 0.00001401
Iteration 249/1000 | Loss: 0.00001401
Iteration 250/1000 | Loss: 0.00001401
Iteration 251/1000 | Loss: 0.00001401
Iteration 252/1000 | Loss: 0.00001401
Iteration 253/1000 | Loss: 0.00001401
Iteration 254/1000 | Loss: 0.00001401
Iteration 255/1000 | Loss: 0.00001401
Iteration 256/1000 | Loss: 0.00001401
Iteration 257/1000 | Loss: 0.00001401
Iteration 258/1000 | Loss: 0.00001401
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 258. Stopping optimization.
Last 5 losses: [1.400938617734937e-05, 1.400938617734937e-05, 1.400938617734937e-05, 1.400938617734937e-05, 1.400938617734937e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.400938617734937e-05

Optimization complete. Final v2v error: 3.138617992401123 mm

Highest mean error: 4.268497467041016 mm for frame 92

Lowest mean error: 2.7941625118255615 mm for frame 26

Saving results

Total time: 45.59877061843872
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_helen_posed_023/1047/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_helen_posed_023/1047.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_helen_posed_023/1047
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00448357
Iteration 2/25 | Loss: 0.00129089
Iteration 3/25 | Loss: 0.00121180
Iteration 4/25 | Loss: 0.00120305
Iteration 5/25 | Loss: 0.00120101
Iteration 6/25 | Loss: 0.00120101
Iteration 7/25 | Loss: 0.00120101
Iteration 8/25 | Loss: 0.00120101
Iteration 9/25 | Loss: 0.00120101
Iteration 10/25 | Loss: 0.00120101
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0012010147329419851, 0.0012010147329419851, 0.0012010147329419851, 0.0012010147329419851, 0.0012010147329419851]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012010147329419851

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.31686914
Iteration 2/25 | Loss: 0.00133275
Iteration 3/25 | Loss: 0.00133275
Iteration 4/25 | Loss: 0.00133274
Iteration 5/25 | Loss: 0.00133274
Iteration 6/25 | Loss: 0.00133274
Iteration 7/25 | Loss: 0.00133274
Iteration 8/25 | Loss: 0.00133274
Iteration 9/25 | Loss: 0.00133274
Iteration 10/25 | Loss: 0.00133274
Iteration 11/25 | Loss: 0.00133274
Iteration 12/25 | Loss: 0.00133274
Iteration 13/25 | Loss: 0.00133274
Iteration 14/25 | Loss: 0.00133274
Iteration 15/25 | Loss: 0.00133274
Iteration 16/25 | Loss: 0.00133274
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0013327420456334949, 0.0013327420456334949, 0.0013327420456334949, 0.0013327420456334949, 0.0013327420456334949]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013327420456334949

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00133274
Iteration 2/1000 | Loss: 0.00002075
Iteration 3/1000 | Loss: 0.00001548
Iteration 4/1000 | Loss: 0.00001425
Iteration 5/1000 | Loss: 0.00001371
Iteration 6/1000 | Loss: 0.00001316
Iteration 7/1000 | Loss: 0.00001284
Iteration 8/1000 | Loss: 0.00001261
Iteration 9/1000 | Loss: 0.00001227
Iteration 10/1000 | Loss: 0.00001214
Iteration 11/1000 | Loss: 0.00001196
Iteration 12/1000 | Loss: 0.00001191
Iteration 13/1000 | Loss: 0.00001185
Iteration 14/1000 | Loss: 0.00001171
Iteration 15/1000 | Loss: 0.00001152
Iteration 16/1000 | Loss: 0.00001151
Iteration 17/1000 | Loss: 0.00001134
Iteration 18/1000 | Loss: 0.00001128
Iteration 19/1000 | Loss: 0.00001128
Iteration 20/1000 | Loss: 0.00001127
Iteration 21/1000 | Loss: 0.00001127
Iteration 22/1000 | Loss: 0.00001126
Iteration 23/1000 | Loss: 0.00001124
Iteration 24/1000 | Loss: 0.00001124
Iteration 25/1000 | Loss: 0.00001122
Iteration 26/1000 | Loss: 0.00001122
Iteration 27/1000 | Loss: 0.00001121
Iteration 28/1000 | Loss: 0.00001121
Iteration 29/1000 | Loss: 0.00001121
Iteration 30/1000 | Loss: 0.00001121
Iteration 31/1000 | Loss: 0.00001120
Iteration 32/1000 | Loss: 0.00001119
Iteration 33/1000 | Loss: 0.00001115
Iteration 34/1000 | Loss: 0.00001115
Iteration 35/1000 | Loss: 0.00001113
Iteration 36/1000 | Loss: 0.00001113
Iteration 37/1000 | Loss: 0.00001108
Iteration 38/1000 | Loss: 0.00001108
Iteration 39/1000 | Loss: 0.00001107
Iteration 40/1000 | Loss: 0.00001107
Iteration 41/1000 | Loss: 0.00001107
Iteration 42/1000 | Loss: 0.00001107
Iteration 43/1000 | Loss: 0.00001107
Iteration 44/1000 | Loss: 0.00001107
Iteration 45/1000 | Loss: 0.00001106
Iteration 46/1000 | Loss: 0.00001105
Iteration 47/1000 | Loss: 0.00001104
Iteration 48/1000 | Loss: 0.00001104
Iteration 49/1000 | Loss: 0.00001104
Iteration 50/1000 | Loss: 0.00001103
Iteration 51/1000 | Loss: 0.00001103
Iteration 52/1000 | Loss: 0.00001102
Iteration 53/1000 | Loss: 0.00001102
Iteration 54/1000 | Loss: 0.00001101
Iteration 55/1000 | Loss: 0.00001100
Iteration 56/1000 | Loss: 0.00001100
Iteration 57/1000 | Loss: 0.00001099
Iteration 58/1000 | Loss: 0.00001099
Iteration 59/1000 | Loss: 0.00001098
Iteration 60/1000 | Loss: 0.00001094
Iteration 61/1000 | Loss: 0.00001093
Iteration 62/1000 | Loss: 0.00001093
Iteration 63/1000 | Loss: 0.00001090
Iteration 64/1000 | Loss: 0.00001089
Iteration 65/1000 | Loss: 0.00001089
Iteration 66/1000 | Loss: 0.00001087
Iteration 67/1000 | Loss: 0.00001087
Iteration 68/1000 | Loss: 0.00001086
Iteration 69/1000 | Loss: 0.00001086
Iteration 70/1000 | Loss: 0.00001086
Iteration 71/1000 | Loss: 0.00001086
Iteration 72/1000 | Loss: 0.00001085
Iteration 73/1000 | Loss: 0.00001085
Iteration 74/1000 | Loss: 0.00001085
Iteration 75/1000 | Loss: 0.00001085
Iteration 76/1000 | Loss: 0.00001084
Iteration 77/1000 | Loss: 0.00001084
Iteration 78/1000 | Loss: 0.00001084
Iteration 79/1000 | Loss: 0.00001084
Iteration 80/1000 | Loss: 0.00001084
Iteration 81/1000 | Loss: 0.00001084
Iteration 82/1000 | Loss: 0.00001084
Iteration 83/1000 | Loss: 0.00001083
Iteration 84/1000 | Loss: 0.00001083
Iteration 85/1000 | Loss: 0.00001083
Iteration 86/1000 | Loss: 0.00001083
Iteration 87/1000 | Loss: 0.00001083
Iteration 88/1000 | Loss: 0.00001083
Iteration 89/1000 | Loss: 0.00001083
Iteration 90/1000 | Loss: 0.00001083
Iteration 91/1000 | Loss: 0.00001083
Iteration 92/1000 | Loss: 0.00001082
Iteration 93/1000 | Loss: 0.00001082
Iteration 94/1000 | Loss: 0.00001082
Iteration 95/1000 | Loss: 0.00001082
Iteration 96/1000 | Loss: 0.00001082
Iteration 97/1000 | Loss: 0.00001082
Iteration 98/1000 | Loss: 0.00001082
Iteration 99/1000 | Loss: 0.00001082
Iteration 100/1000 | Loss: 0.00001082
Iteration 101/1000 | Loss: 0.00001082
Iteration 102/1000 | Loss: 0.00001082
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 102. Stopping optimization.
Last 5 losses: [1.0818765986186918e-05, 1.0818765986186918e-05, 1.0818765986186918e-05, 1.0818765986186918e-05, 1.0818765986186918e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0818765986186918e-05

Optimization complete. Final v2v error: 2.822547197341919 mm

Highest mean error: 3.243638753890991 mm for frame 221

Lowest mean error: 2.598830461502075 mm for frame 133

Saving results

Total time: 40.91040110588074
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_helen_posed_023/1059/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_helen_posed_023/1059.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_helen_posed_023/1059
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00929512
Iteration 2/25 | Loss: 0.00331784
Iteration 3/25 | Loss: 0.00241407
Iteration 4/25 | Loss: 0.00228541
Iteration 5/25 | Loss: 0.00209999
Iteration 6/25 | Loss: 0.00196732
Iteration 7/25 | Loss: 0.00179281
Iteration 8/25 | Loss: 0.00162041
Iteration 9/25 | Loss: 0.00153854
Iteration 10/25 | Loss: 0.00149186
Iteration 11/25 | Loss: 0.00149283
Iteration 12/25 | Loss: 0.00148288
Iteration 13/25 | Loss: 0.00147783
Iteration 14/25 | Loss: 0.00147256
Iteration 15/25 | Loss: 0.00147059
Iteration 16/25 | Loss: 0.00146983
Iteration 17/25 | Loss: 0.00146928
Iteration 18/25 | Loss: 0.00146893
Iteration 19/25 | Loss: 0.00146878
Iteration 20/25 | Loss: 0.00146871
Iteration 21/25 | Loss: 0.00146871
Iteration 22/25 | Loss: 0.00146870
Iteration 23/25 | Loss: 0.00146870
Iteration 24/25 | Loss: 0.00146870
Iteration 25/25 | Loss: 0.00146870

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.29298151
Iteration 2/25 | Loss: 0.00262960
Iteration 3/25 | Loss: 0.00262960
Iteration 4/25 | Loss: 0.00262960
Iteration 5/25 | Loss: 0.00262960
Iteration 6/25 | Loss: 0.00262960
Iteration 7/25 | Loss: 0.00262960
Iteration 8/25 | Loss: 0.00262960
Iteration 9/25 | Loss: 0.00262960
Iteration 10/25 | Loss: 0.00262960
Iteration 11/25 | Loss: 0.00262960
Iteration 12/25 | Loss: 0.00262960
Iteration 13/25 | Loss: 0.00262960
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0026296013966202736, 0.0026296013966202736, 0.0026296013966202736, 0.0026296013966202736, 0.0026296013966202736]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0026296013966202736

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00262960
Iteration 2/1000 | Loss: 0.00140101
Iteration 3/1000 | Loss: 0.00017515
Iteration 4/1000 | Loss: 0.00012724
Iteration 5/1000 | Loss: 0.00008815
Iteration 6/1000 | Loss: 0.00007227
Iteration 7/1000 | Loss: 0.00005822
Iteration 8/1000 | Loss: 0.00004831
Iteration 9/1000 | Loss: 0.00004031
Iteration 10/1000 | Loss: 0.00003744
Iteration 11/1000 | Loss: 0.00022420
Iteration 12/1000 | Loss: 0.00069572
Iteration 13/1000 | Loss: 0.00018157
Iteration 14/1000 | Loss: 0.00004878
Iteration 15/1000 | Loss: 0.00003517
Iteration 16/1000 | Loss: 0.00002992
Iteration 17/1000 | Loss: 0.00002632
Iteration 18/1000 | Loss: 0.00002438
Iteration 19/1000 | Loss: 0.00002297
Iteration 20/1000 | Loss: 0.00002184
Iteration 21/1000 | Loss: 0.00002120
Iteration 22/1000 | Loss: 0.00002058
Iteration 23/1000 | Loss: 0.00002017
Iteration 24/1000 | Loss: 0.00001986
Iteration 25/1000 | Loss: 0.00001973
Iteration 26/1000 | Loss: 0.00001962
Iteration 27/1000 | Loss: 0.00001960
Iteration 28/1000 | Loss: 0.00001952
Iteration 29/1000 | Loss: 0.00001952
Iteration 30/1000 | Loss: 0.00001952
Iteration 31/1000 | Loss: 0.00001951
Iteration 32/1000 | Loss: 0.00001950
Iteration 33/1000 | Loss: 0.00001950
Iteration 34/1000 | Loss: 0.00001946
Iteration 35/1000 | Loss: 0.00001944
Iteration 36/1000 | Loss: 0.00001941
Iteration 37/1000 | Loss: 0.00001940
Iteration 38/1000 | Loss: 0.00001940
Iteration 39/1000 | Loss: 0.00001939
Iteration 40/1000 | Loss: 0.00001938
Iteration 41/1000 | Loss: 0.00001936
Iteration 42/1000 | Loss: 0.00001936
Iteration 43/1000 | Loss: 0.00001936
Iteration 44/1000 | Loss: 0.00001935
Iteration 45/1000 | Loss: 0.00001935
Iteration 46/1000 | Loss: 0.00001935
Iteration 47/1000 | Loss: 0.00001935
Iteration 48/1000 | Loss: 0.00001935
Iteration 49/1000 | Loss: 0.00001933
Iteration 50/1000 | Loss: 0.00001933
Iteration 51/1000 | Loss: 0.00001933
Iteration 52/1000 | Loss: 0.00001933
Iteration 53/1000 | Loss: 0.00001933
Iteration 54/1000 | Loss: 0.00001932
Iteration 55/1000 | Loss: 0.00001932
Iteration 56/1000 | Loss: 0.00001932
Iteration 57/1000 | Loss: 0.00001932
Iteration 58/1000 | Loss: 0.00001931
Iteration 59/1000 | Loss: 0.00001931
Iteration 60/1000 | Loss: 0.00001930
Iteration 61/1000 | Loss: 0.00001930
Iteration 62/1000 | Loss: 0.00001930
Iteration 63/1000 | Loss: 0.00001929
Iteration 64/1000 | Loss: 0.00001929
Iteration 65/1000 | Loss: 0.00001929
Iteration 66/1000 | Loss: 0.00001929
Iteration 67/1000 | Loss: 0.00001929
Iteration 68/1000 | Loss: 0.00001929
Iteration 69/1000 | Loss: 0.00001929
Iteration 70/1000 | Loss: 0.00001929
Iteration 71/1000 | Loss: 0.00001928
Iteration 72/1000 | Loss: 0.00001928
Iteration 73/1000 | Loss: 0.00001928
Iteration 74/1000 | Loss: 0.00001927
Iteration 75/1000 | Loss: 0.00001927
Iteration 76/1000 | Loss: 0.00001927
Iteration 77/1000 | Loss: 0.00001927
Iteration 78/1000 | Loss: 0.00001927
Iteration 79/1000 | Loss: 0.00001927
Iteration 80/1000 | Loss: 0.00001927
Iteration 81/1000 | Loss: 0.00001927
Iteration 82/1000 | Loss: 0.00001927
Iteration 83/1000 | Loss: 0.00001927
Iteration 84/1000 | Loss: 0.00001927
Iteration 85/1000 | Loss: 0.00001927
Iteration 86/1000 | Loss: 0.00001927
Iteration 87/1000 | Loss: 0.00001927
Iteration 88/1000 | Loss: 0.00001926
Iteration 89/1000 | Loss: 0.00001926
Iteration 90/1000 | Loss: 0.00001926
Iteration 91/1000 | Loss: 0.00001926
Iteration 92/1000 | Loss: 0.00001926
Iteration 93/1000 | Loss: 0.00001926
Iteration 94/1000 | Loss: 0.00001926
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 94. Stopping optimization.
Last 5 losses: [1.9264829461462796e-05, 1.9264829461462796e-05, 1.9264829461462796e-05, 1.9264829461462796e-05, 1.9264829461462796e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9264829461462796e-05

Optimization complete. Final v2v error: 3.765446186065674 mm

Highest mean error: 5.061757564544678 mm for frame 61

Lowest mean error: 3.207562208175659 mm for frame 153

Saving results

Total time: 79.28112435340881
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_helen_posed_023/1012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_helen_posed_023/1012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_helen_posed_023/1012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00377818
Iteration 2/25 | Loss: 0.00124207
Iteration 3/25 | Loss: 0.00118282
Iteration 4/25 | Loss: 0.00117615
Iteration 5/25 | Loss: 0.00117428
Iteration 6/25 | Loss: 0.00117419
Iteration 7/25 | Loss: 0.00117419
Iteration 8/25 | Loss: 0.00117419
Iteration 9/25 | Loss: 0.00117419
Iteration 10/25 | Loss: 0.00117419
Iteration 11/25 | Loss: 0.00117419
Iteration 12/25 | Loss: 0.00117419
Iteration 13/25 | Loss: 0.00117419
Iteration 14/25 | Loss: 0.00117419
Iteration 15/25 | Loss: 0.00117419
Iteration 16/25 | Loss: 0.00117419
Iteration 17/25 | Loss: 0.00117419
Iteration 18/25 | Loss: 0.00117419
Iteration 19/25 | Loss: 0.00117419
Iteration 20/25 | Loss: 0.00117419
Iteration 21/25 | Loss: 0.00117419
Iteration 22/25 | Loss: 0.00117419
Iteration 23/25 | Loss: 0.00117419
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0011741869384422898, 0.0011741869384422898, 0.0011741869384422898, 0.0011741869384422898, 0.0011741869384422898]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011741869384422898

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.61345172
Iteration 2/25 | Loss: 0.00125141
Iteration 3/25 | Loss: 0.00125141
Iteration 4/25 | Loss: 0.00125141
Iteration 5/25 | Loss: 0.00125141
Iteration 6/25 | Loss: 0.00125141
Iteration 7/25 | Loss: 0.00125140
Iteration 8/25 | Loss: 0.00125140
Iteration 9/25 | Loss: 0.00125140
Iteration 10/25 | Loss: 0.00125140
Iteration 11/25 | Loss: 0.00125140
Iteration 12/25 | Loss: 0.00125140
Iteration 13/25 | Loss: 0.00125140
Iteration 14/25 | Loss: 0.00125140
Iteration 15/25 | Loss: 0.00125140
Iteration 16/25 | Loss: 0.00125140
Iteration 17/25 | Loss: 0.00125140
Iteration 18/25 | Loss: 0.00125140
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.001251403708010912, 0.001251403708010912, 0.001251403708010912, 0.001251403708010912, 0.001251403708010912]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001251403708010912

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00125140
Iteration 2/1000 | Loss: 0.00002497
Iteration 3/1000 | Loss: 0.00001581
Iteration 4/1000 | Loss: 0.00001288
Iteration 5/1000 | Loss: 0.00001164
Iteration 6/1000 | Loss: 0.00001107
Iteration 7/1000 | Loss: 0.00001060
Iteration 8/1000 | Loss: 0.00001017
Iteration 9/1000 | Loss: 0.00000989
Iteration 10/1000 | Loss: 0.00000987
Iteration 11/1000 | Loss: 0.00000986
Iteration 12/1000 | Loss: 0.00000959
Iteration 13/1000 | Loss: 0.00000946
Iteration 14/1000 | Loss: 0.00000945
Iteration 15/1000 | Loss: 0.00000941
Iteration 16/1000 | Loss: 0.00000941
Iteration 17/1000 | Loss: 0.00000927
Iteration 18/1000 | Loss: 0.00000926
Iteration 19/1000 | Loss: 0.00000922
Iteration 20/1000 | Loss: 0.00000919
Iteration 21/1000 | Loss: 0.00000918
Iteration 22/1000 | Loss: 0.00000918
Iteration 23/1000 | Loss: 0.00000917
Iteration 24/1000 | Loss: 0.00000917
Iteration 25/1000 | Loss: 0.00000911
Iteration 26/1000 | Loss: 0.00000911
Iteration 27/1000 | Loss: 0.00000910
Iteration 28/1000 | Loss: 0.00000910
Iteration 29/1000 | Loss: 0.00000909
Iteration 30/1000 | Loss: 0.00000908
Iteration 31/1000 | Loss: 0.00000907
Iteration 32/1000 | Loss: 0.00000905
Iteration 33/1000 | Loss: 0.00000904
Iteration 34/1000 | Loss: 0.00000904
Iteration 35/1000 | Loss: 0.00000904
Iteration 36/1000 | Loss: 0.00000903
Iteration 37/1000 | Loss: 0.00000903
Iteration 38/1000 | Loss: 0.00000899
Iteration 39/1000 | Loss: 0.00000899
Iteration 40/1000 | Loss: 0.00000898
Iteration 41/1000 | Loss: 0.00000898
Iteration 42/1000 | Loss: 0.00000897
Iteration 43/1000 | Loss: 0.00000896
Iteration 44/1000 | Loss: 0.00000896
Iteration 45/1000 | Loss: 0.00000895
Iteration 46/1000 | Loss: 0.00000895
Iteration 47/1000 | Loss: 0.00000894
Iteration 48/1000 | Loss: 0.00000894
Iteration 49/1000 | Loss: 0.00000894
Iteration 50/1000 | Loss: 0.00000893
Iteration 51/1000 | Loss: 0.00000893
Iteration 52/1000 | Loss: 0.00000893
Iteration 53/1000 | Loss: 0.00000893
Iteration 54/1000 | Loss: 0.00000892
Iteration 55/1000 | Loss: 0.00000892
Iteration 56/1000 | Loss: 0.00000891
Iteration 57/1000 | Loss: 0.00000891
Iteration 58/1000 | Loss: 0.00000891
Iteration 59/1000 | Loss: 0.00000890
Iteration 60/1000 | Loss: 0.00000890
Iteration 61/1000 | Loss: 0.00000890
Iteration 62/1000 | Loss: 0.00000889
Iteration 63/1000 | Loss: 0.00000889
Iteration 64/1000 | Loss: 0.00000889
Iteration 65/1000 | Loss: 0.00000888
Iteration 66/1000 | Loss: 0.00000888
Iteration 67/1000 | Loss: 0.00000888
Iteration 68/1000 | Loss: 0.00000888
Iteration 69/1000 | Loss: 0.00000888
Iteration 70/1000 | Loss: 0.00000888
Iteration 71/1000 | Loss: 0.00000888
Iteration 72/1000 | Loss: 0.00000888
Iteration 73/1000 | Loss: 0.00000888
Iteration 74/1000 | Loss: 0.00000888
Iteration 75/1000 | Loss: 0.00000888
Iteration 76/1000 | Loss: 0.00000888
Iteration 77/1000 | Loss: 0.00000888
Iteration 78/1000 | Loss: 0.00000888
Iteration 79/1000 | Loss: 0.00000888
Iteration 80/1000 | Loss: 0.00000888
Iteration 81/1000 | Loss: 0.00000888
Iteration 82/1000 | Loss: 0.00000888
Iteration 83/1000 | Loss: 0.00000888
Iteration 84/1000 | Loss: 0.00000888
Iteration 85/1000 | Loss: 0.00000888
Iteration 86/1000 | Loss: 0.00000888
Iteration 87/1000 | Loss: 0.00000888
Iteration 88/1000 | Loss: 0.00000888
Iteration 89/1000 | Loss: 0.00000888
Iteration 90/1000 | Loss: 0.00000888
Iteration 91/1000 | Loss: 0.00000888
Iteration 92/1000 | Loss: 0.00000888
Iteration 93/1000 | Loss: 0.00000888
Iteration 94/1000 | Loss: 0.00000888
Iteration 95/1000 | Loss: 0.00000888
Iteration 96/1000 | Loss: 0.00000888
Iteration 97/1000 | Loss: 0.00000888
Iteration 98/1000 | Loss: 0.00000888
Iteration 99/1000 | Loss: 0.00000888
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 99. Stopping optimization.
Last 5 losses: [8.878389053279534e-06, 8.878389053279534e-06, 8.878389053279534e-06, 8.878389053279534e-06, 8.878389053279534e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.878389053279534e-06

Optimization complete. Final v2v error: 2.5849106311798096 mm

Highest mean error: 3.007213830947876 mm for frame 76

Lowest mean error: 2.4860386848449707 mm for frame 30

Saving results

Total time: 31.5537850856781
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_helen_posed_023/1064/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_helen_posed_023/1064.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_helen_posed_023/1064
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00802151
Iteration 2/25 | Loss: 0.00171063
Iteration 3/25 | Loss: 0.00136742
Iteration 4/25 | Loss: 0.00132583
Iteration 5/25 | Loss: 0.00131995
Iteration 6/25 | Loss: 0.00131835
Iteration 7/25 | Loss: 0.00131831
Iteration 8/25 | Loss: 0.00131831
Iteration 9/25 | Loss: 0.00131831
Iteration 10/25 | Loss: 0.00131831
Iteration 11/25 | Loss: 0.00131831
Iteration 12/25 | Loss: 0.00131831
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0013183066621422768, 0.0013183066621422768, 0.0013183066621422768, 0.0013183066621422768, 0.0013183066621422768]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013183066621422768

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35764492
Iteration 2/25 | Loss: 0.00100593
Iteration 3/25 | Loss: 0.00100592
Iteration 4/25 | Loss: 0.00100592
Iteration 5/25 | Loss: 0.00100592
Iteration 6/25 | Loss: 0.00100592
Iteration 7/25 | Loss: 0.00100592
Iteration 8/25 | Loss: 0.00100592
Iteration 9/25 | Loss: 0.00100592
Iteration 10/25 | Loss: 0.00100592
Iteration 11/25 | Loss: 0.00100592
Iteration 12/25 | Loss: 0.00100592
Iteration 13/25 | Loss: 0.00100592
Iteration 14/25 | Loss: 0.00100592
Iteration 15/25 | Loss: 0.00100592
Iteration 16/25 | Loss: 0.00100592
Iteration 17/25 | Loss: 0.00100592
Iteration 18/25 | Loss: 0.00100592
Iteration 19/25 | Loss: 0.00100592
Iteration 20/25 | Loss: 0.00100592
Iteration 21/25 | Loss: 0.00100592
Iteration 22/25 | Loss: 0.00100592
Iteration 23/25 | Loss: 0.00100592
Iteration 24/25 | Loss: 0.00100592
Iteration 25/25 | Loss: 0.00100592

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00100592
Iteration 2/1000 | Loss: 0.00005672
Iteration 3/1000 | Loss: 0.00003951
Iteration 4/1000 | Loss: 0.00003241
Iteration 5/1000 | Loss: 0.00003032
Iteration 6/1000 | Loss: 0.00002916
Iteration 7/1000 | Loss: 0.00002838
Iteration 8/1000 | Loss: 0.00002778
Iteration 9/1000 | Loss: 0.00002732
Iteration 10/1000 | Loss: 0.00002698
Iteration 11/1000 | Loss: 0.00002673
Iteration 12/1000 | Loss: 0.00002655
Iteration 13/1000 | Loss: 0.00002638
Iteration 14/1000 | Loss: 0.00002636
Iteration 15/1000 | Loss: 0.00002630
Iteration 16/1000 | Loss: 0.00002627
Iteration 17/1000 | Loss: 0.00002625
Iteration 18/1000 | Loss: 0.00002625
Iteration 19/1000 | Loss: 0.00002624
Iteration 20/1000 | Loss: 0.00002623
Iteration 21/1000 | Loss: 0.00002619
Iteration 22/1000 | Loss: 0.00002617
Iteration 23/1000 | Loss: 0.00002613
Iteration 24/1000 | Loss: 0.00002611
Iteration 25/1000 | Loss: 0.00002610
Iteration 26/1000 | Loss: 0.00002610
Iteration 27/1000 | Loss: 0.00002609
Iteration 28/1000 | Loss: 0.00002608
Iteration 29/1000 | Loss: 0.00002607
Iteration 30/1000 | Loss: 0.00002607
Iteration 31/1000 | Loss: 0.00002606
Iteration 32/1000 | Loss: 0.00002606
Iteration 33/1000 | Loss: 0.00002606
Iteration 34/1000 | Loss: 0.00002606
Iteration 35/1000 | Loss: 0.00002606
Iteration 36/1000 | Loss: 0.00002605
Iteration 37/1000 | Loss: 0.00002601
Iteration 38/1000 | Loss: 0.00002601
Iteration 39/1000 | Loss: 0.00002601
Iteration 40/1000 | Loss: 0.00002601
Iteration 41/1000 | Loss: 0.00002601
Iteration 42/1000 | Loss: 0.00002601
Iteration 43/1000 | Loss: 0.00002601
Iteration 44/1000 | Loss: 0.00002600
Iteration 45/1000 | Loss: 0.00002600
Iteration 46/1000 | Loss: 0.00002600
Iteration 47/1000 | Loss: 0.00002600
Iteration 48/1000 | Loss: 0.00002600
Iteration 49/1000 | Loss: 0.00002600
Iteration 50/1000 | Loss: 0.00002600
Iteration 51/1000 | Loss: 0.00002600
Iteration 52/1000 | Loss: 0.00002600
Iteration 53/1000 | Loss: 0.00002600
Iteration 54/1000 | Loss: 0.00002600
Iteration 55/1000 | Loss: 0.00002600
Iteration 56/1000 | Loss: 0.00002600
Iteration 57/1000 | Loss: 0.00002600
Iteration 58/1000 | Loss: 0.00002600
Iteration 59/1000 | Loss: 0.00002600
Iteration 60/1000 | Loss: 0.00002600
Iteration 61/1000 | Loss: 0.00002600
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 61. Stopping optimization.
Last 5 losses: [2.600406514829956e-05, 2.600406514829956e-05, 2.600406514829956e-05, 2.600406514829956e-05, 2.600406514829956e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.600406514829956e-05

Optimization complete. Final v2v error: 4.289586544036865 mm

Highest mean error: 5.142960548400879 mm for frame 36

Lowest mean error: 3.777841329574585 mm for frame 46

Saving results

Total time: 31.54697275161743
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_helen_posed_023/1023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_helen_posed_023/1023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_helen_posed_023/1023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00992891
Iteration 2/25 | Loss: 0.00256805
Iteration 3/25 | Loss: 0.00205017
Iteration 4/25 | Loss: 0.00196026
Iteration 5/25 | Loss: 0.00190304
Iteration 6/25 | Loss: 0.00168155
Iteration 7/25 | Loss: 0.00154481
Iteration 8/25 | Loss: 0.00142858
Iteration 9/25 | Loss: 0.00136971
Iteration 10/25 | Loss: 0.00135449
Iteration 11/25 | Loss: 0.00134352
Iteration 12/25 | Loss: 0.00134731
Iteration 13/25 | Loss: 0.00133121
Iteration 14/25 | Loss: 0.00132514
Iteration 15/25 | Loss: 0.00132321
Iteration 16/25 | Loss: 0.00132275
Iteration 17/25 | Loss: 0.00132273
Iteration 18/25 | Loss: 0.00132273
Iteration 19/25 | Loss: 0.00132273
Iteration 20/25 | Loss: 0.00132273
Iteration 21/25 | Loss: 0.00132273
Iteration 22/25 | Loss: 0.00132273
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0013227250892668962, 0.0013227250892668962, 0.0013227250892668962, 0.0013227250892668962, 0.0013227250892668962]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013227250892668962

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.29489601
Iteration 2/25 | Loss: 0.00104403
Iteration 3/25 | Loss: 0.00104402
Iteration 4/25 | Loss: 0.00104402
Iteration 5/25 | Loss: 0.00104402
Iteration 6/25 | Loss: 0.00104402
Iteration 7/25 | Loss: 0.00104402
Iteration 8/25 | Loss: 0.00104402
Iteration 9/25 | Loss: 0.00104402
Iteration 10/25 | Loss: 0.00104402
Iteration 11/25 | Loss: 0.00104402
Iteration 12/25 | Loss: 0.00104402
Iteration 13/25 | Loss: 0.00104402
Iteration 14/25 | Loss: 0.00104402
Iteration 15/25 | Loss: 0.00104402
Iteration 16/25 | Loss: 0.00104402
Iteration 17/25 | Loss: 0.00104402
Iteration 18/25 | Loss: 0.00104402
Iteration 19/25 | Loss: 0.00104402
Iteration 20/25 | Loss: 0.00104402
Iteration 21/25 | Loss: 0.00104402
Iteration 22/25 | Loss: 0.00104402
Iteration 23/25 | Loss: 0.00104402
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.001044024364091456, 0.001044024364091456, 0.001044024364091456, 0.001044024364091456, 0.001044024364091456]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001044024364091456

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00104402
Iteration 2/1000 | Loss: 0.00004137
Iteration 3/1000 | Loss: 0.00002998
Iteration 4/1000 | Loss: 0.00002836
Iteration 5/1000 | Loss: 0.00002730
Iteration 6/1000 | Loss: 0.00002656
Iteration 7/1000 | Loss: 0.00002608
Iteration 8/1000 | Loss: 0.00002555
Iteration 9/1000 | Loss: 0.00002518
Iteration 10/1000 | Loss: 0.00002497
Iteration 11/1000 | Loss: 0.00002480
Iteration 12/1000 | Loss: 0.00002456
Iteration 13/1000 | Loss: 0.00002441
Iteration 14/1000 | Loss: 0.00002431
Iteration 15/1000 | Loss: 0.00002428
Iteration 16/1000 | Loss: 0.00002426
Iteration 17/1000 | Loss: 0.00002426
Iteration 18/1000 | Loss: 0.00002425
Iteration 19/1000 | Loss: 0.00002422
Iteration 20/1000 | Loss: 0.00002421
Iteration 21/1000 | Loss: 0.00002421
Iteration 22/1000 | Loss: 0.00002421
Iteration 23/1000 | Loss: 0.00002420
Iteration 24/1000 | Loss: 0.00002419
Iteration 25/1000 | Loss: 0.00002418
Iteration 26/1000 | Loss: 0.00002418
Iteration 27/1000 | Loss: 0.00002417
Iteration 28/1000 | Loss: 0.00002417
Iteration 29/1000 | Loss: 0.00002417
Iteration 30/1000 | Loss: 0.00002416
Iteration 31/1000 | Loss: 0.00002416
Iteration 32/1000 | Loss: 0.00002416
Iteration 33/1000 | Loss: 0.00002416
Iteration 34/1000 | Loss: 0.00002416
Iteration 35/1000 | Loss: 0.00002416
Iteration 36/1000 | Loss: 0.00002415
Iteration 37/1000 | Loss: 0.00002415
Iteration 38/1000 | Loss: 0.00002415
Iteration 39/1000 | Loss: 0.00002415
Iteration 40/1000 | Loss: 0.00002414
Iteration 41/1000 | Loss: 0.00002414
Iteration 42/1000 | Loss: 0.00002414
Iteration 43/1000 | Loss: 0.00002414
Iteration 44/1000 | Loss: 0.00002413
Iteration 45/1000 | Loss: 0.00002413
Iteration 46/1000 | Loss: 0.00002412
Iteration 47/1000 | Loss: 0.00002412
Iteration 48/1000 | Loss: 0.00002412
Iteration 49/1000 | Loss: 0.00002412
Iteration 50/1000 | Loss: 0.00002411
Iteration 51/1000 | Loss: 0.00002411
Iteration 52/1000 | Loss: 0.00002411
Iteration 53/1000 | Loss: 0.00002411
Iteration 54/1000 | Loss: 0.00002411
Iteration 55/1000 | Loss: 0.00002411
Iteration 56/1000 | Loss: 0.00002411
Iteration 57/1000 | Loss: 0.00002411
Iteration 58/1000 | Loss: 0.00002410
Iteration 59/1000 | Loss: 0.00002410
Iteration 60/1000 | Loss: 0.00002410
Iteration 61/1000 | Loss: 0.00002409
Iteration 62/1000 | Loss: 0.00002409
Iteration 63/1000 | Loss: 0.00002408
Iteration 64/1000 | Loss: 0.00002408
Iteration 65/1000 | Loss: 0.00002408
Iteration 66/1000 | Loss: 0.00002408
Iteration 67/1000 | Loss: 0.00002408
Iteration 68/1000 | Loss: 0.00002408
Iteration 69/1000 | Loss: 0.00002408
Iteration 70/1000 | Loss: 0.00002408
Iteration 71/1000 | Loss: 0.00002408
Iteration 72/1000 | Loss: 0.00002408
Iteration 73/1000 | Loss: 0.00002408
Iteration 74/1000 | Loss: 0.00002407
Iteration 75/1000 | Loss: 0.00002407
Iteration 76/1000 | Loss: 0.00002407
Iteration 77/1000 | Loss: 0.00002406
Iteration 78/1000 | Loss: 0.00002405
Iteration 79/1000 | Loss: 0.00002405
Iteration 80/1000 | Loss: 0.00002405
Iteration 81/1000 | Loss: 0.00002405
Iteration 82/1000 | Loss: 0.00002405
Iteration 83/1000 | Loss: 0.00002405
Iteration 84/1000 | Loss: 0.00002405
Iteration 85/1000 | Loss: 0.00002405
Iteration 86/1000 | Loss: 0.00002404
Iteration 87/1000 | Loss: 0.00002404
Iteration 88/1000 | Loss: 0.00002404
Iteration 89/1000 | Loss: 0.00002404
Iteration 90/1000 | Loss: 0.00002404
Iteration 91/1000 | Loss: 0.00002404
Iteration 92/1000 | Loss: 0.00002403
Iteration 93/1000 | Loss: 0.00002403
Iteration 94/1000 | Loss: 0.00002403
Iteration 95/1000 | Loss: 0.00002403
Iteration 96/1000 | Loss: 0.00002403
Iteration 97/1000 | Loss: 0.00002403
Iteration 98/1000 | Loss: 0.00002403
Iteration 99/1000 | Loss: 0.00002402
Iteration 100/1000 | Loss: 0.00002402
Iteration 101/1000 | Loss: 0.00002402
Iteration 102/1000 | Loss: 0.00002401
Iteration 103/1000 | Loss: 0.00002401
Iteration 104/1000 | Loss: 0.00002401
Iteration 105/1000 | Loss: 0.00002401
Iteration 106/1000 | Loss: 0.00002401
Iteration 107/1000 | Loss: 0.00002400
Iteration 108/1000 | Loss: 0.00002400
Iteration 109/1000 | Loss: 0.00002400
Iteration 110/1000 | Loss: 0.00002400
Iteration 111/1000 | Loss: 0.00002400
Iteration 112/1000 | Loss: 0.00002400
Iteration 113/1000 | Loss: 0.00002400
Iteration 114/1000 | Loss: 0.00002399
Iteration 115/1000 | Loss: 0.00002399
Iteration 116/1000 | Loss: 0.00002399
Iteration 117/1000 | Loss: 0.00002399
Iteration 118/1000 | Loss: 0.00002399
Iteration 119/1000 | Loss: 0.00002399
Iteration 120/1000 | Loss: 0.00002399
Iteration 121/1000 | Loss: 0.00002399
Iteration 122/1000 | Loss: 0.00002399
Iteration 123/1000 | Loss: 0.00002399
Iteration 124/1000 | Loss: 0.00002399
Iteration 125/1000 | Loss: 0.00002399
Iteration 126/1000 | Loss: 0.00002399
Iteration 127/1000 | Loss: 0.00002399
Iteration 128/1000 | Loss: 0.00002399
Iteration 129/1000 | Loss: 0.00002399
Iteration 130/1000 | Loss: 0.00002399
Iteration 131/1000 | Loss: 0.00002399
Iteration 132/1000 | Loss: 0.00002399
Iteration 133/1000 | Loss: 0.00002399
Iteration 134/1000 | Loss: 0.00002399
Iteration 135/1000 | Loss: 0.00002399
Iteration 136/1000 | Loss: 0.00002399
Iteration 137/1000 | Loss: 0.00002399
Iteration 138/1000 | Loss: 0.00002399
Iteration 139/1000 | Loss: 0.00002399
Iteration 140/1000 | Loss: 0.00002399
Iteration 141/1000 | Loss: 0.00002399
Iteration 142/1000 | Loss: 0.00002399
Iteration 143/1000 | Loss: 0.00002399
Iteration 144/1000 | Loss: 0.00002399
Iteration 145/1000 | Loss: 0.00002399
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 145. Stopping optimization.
Last 5 losses: [2.3993730792426504e-05, 2.3993730792426504e-05, 2.3993730792426504e-05, 2.3993730792426504e-05, 2.3993730792426504e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3993730792426504e-05

Optimization complete. Final v2v error: 4.252376556396484 mm

Highest mean error: 4.3637495040893555 mm for frame 13

Lowest mean error: 4.143885612487793 mm for frame 183

Saving results

Total time: 63.074496269226074
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_helen_posed_023/1024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_helen_posed_023/1024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_helen_posed_023/1024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00573894
Iteration 2/25 | Loss: 0.00166489
Iteration 3/25 | Loss: 0.00139330
Iteration 4/25 | Loss: 0.00136607
Iteration 5/25 | Loss: 0.00136763
Iteration 6/25 | Loss: 0.00135739
Iteration 7/25 | Loss: 0.00135022
Iteration 8/25 | Loss: 0.00134701
Iteration 9/25 | Loss: 0.00134651
Iteration 10/25 | Loss: 0.00134634
Iteration 11/25 | Loss: 0.00134627
Iteration 12/25 | Loss: 0.00134624
Iteration 13/25 | Loss: 0.00134624
Iteration 14/25 | Loss: 0.00134623
Iteration 15/25 | Loss: 0.00134623
Iteration 16/25 | Loss: 0.00134622
Iteration 17/25 | Loss: 0.00134622
Iteration 18/25 | Loss: 0.00134622
Iteration 19/25 | Loss: 0.00134622
Iteration 20/25 | Loss: 0.00134622
Iteration 21/25 | Loss: 0.00134621
Iteration 22/25 | Loss: 0.00134621
Iteration 23/25 | Loss: 0.00134621
Iteration 24/25 | Loss: 0.00134621
Iteration 25/25 | Loss: 0.00134621

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.20710480
Iteration 2/25 | Loss: 0.00157245
Iteration 3/25 | Loss: 0.00157242
Iteration 4/25 | Loss: 0.00157241
Iteration 5/25 | Loss: 0.00157241
Iteration 6/25 | Loss: 0.00157241
Iteration 7/25 | Loss: 0.00157241
Iteration 8/25 | Loss: 0.00157241
Iteration 9/25 | Loss: 0.00157241
Iteration 10/25 | Loss: 0.00157241
Iteration 11/25 | Loss: 0.00157241
Iteration 12/25 | Loss: 0.00157241
Iteration 13/25 | Loss: 0.00157241
Iteration 14/25 | Loss: 0.00157241
Iteration 15/25 | Loss: 0.00157241
Iteration 16/25 | Loss: 0.00157241
Iteration 17/25 | Loss: 0.00157241
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0015724117401987314, 0.0015724117401987314, 0.0015724117401987314, 0.0015724117401987314, 0.0015724117401987314]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015724117401987314

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00157241
Iteration 2/1000 | Loss: 0.00012889
Iteration 3/1000 | Loss: 0.00008710
Iteration 4/1000 | Loss: 0.00007713
Iteration 5/1000 | Loss: 0.00007007
Iteration 6/1000 | Loss: 0.00006602
Iteration 7/1000 | Loss: 0.00006282
Iteration 8/1000 | Loss: 0.00042033
Iteration 9/1000 | Loss: 0.00023083
Iteration 10/1000 | Loss: 0.00020485
Iteration 11/1000 | Loss: 0.00006669
Iteration 12/1000 | Loss: 0.00006121
Iteration 13/1000 | Loss: 0.00039670
Iteration 14/1000 | Loss: 0.00005863
Iteration 15/1000 | Loss: 0.00005612
Iteration 16/1000 | Loss: 0.00005479
Iteration 17/1000 | Loss: 0.00005378
Iteration 18/1000 | Loss: 0.00005277
Iteration 19/1000 | Loss: 0.00005192
Iteration 20/1000 | Loss: 0.00072846
Iteration 21/1000 | Loss: 0.00037231
Iteration 22/1000 | Loss: 0.00005734
Iteration 23/1000 | Loss: 0.00036971
Iteration 24/1000 | Loss: 0.00020182
Iteration 25/1000 | Loss: 0.00004964
Iteration 26/1000 | Loss: 0.00004756
Iteration 27/1000 | Loss: 0.00024345
Iteration 28/1000 | Loss: 0.00004493
Iteration 29/1000 | Loss: 0.00004348
Iteration 30/1000 | Loss: 0.00052879
Iteration 31/1000 | Loss: 0.00015684
Iteration 32/1000 | Loss: 0.00004454
Iteration 33/1000 | Loss: 0.00004195
Iteration 34/1000 | Loss: 0.00052749
Iteration 35/1000 | Loss: 0.00022678
Iteration 36/1000 | Loss: 0.00004131
Iteration 37/1000 | Loss: 0.00051439
Iteration 38/1000 | Loss: 0.00019964
Iteration 39/1000 | Loss: 0.00004115
Iteration 40/1000 | Loss: 0.00004061
Iteration 41/1000 | Loss: 0.00052428
Iteration 42/1000 | Loss: 0.00006082
Iteration 43/1000 | Loss: 0.00004574
Iteration 44/1000 | Loss: 0.00004338
Iteration 45/1000 | Loss: 0.00033861
Iteration 46/1000 | Loss: 0.00004623
Iteration 47/1000 | Loss: 0.00003945
Iteration 48/1000 | Loss: 0.00003817
Iteration 49/1000 | Loss: 0.00003772
Iteration 50/1000 | Loss: 0.00003728
Iteration 51/1000 | Loss: 0.00003702
Iteration 52/1000 | Loss: 0.00003688
Iteration 53/1000 | Loss: 0.00003684
Iteration 54/1000 | Loss: 0.00003683
Iteration 55/1000 | Loss: 0.00003680
Iteration 56/1000 | Loss: 0.00003677
Iteration 57/1000 | Loss: 0.00003677
Iteration 58/1000 | Loss: 0.00003676
Iteration 59/1000 | Loss: 0.00003674
Iteration 60/1000 | Loss: 0.00003674
Iteration 61/1000 | Loss: 0.00003672
Iteration 62/1000 | Loss: 0.00003672
Iteration 63/1000 | Loss: 0.00003671
Iteration 64/1000 | Loss: 0.00003666
Iteration 65/1000 | Loss: 0.00003665
Iteration 66/1000 | Loss: 0.00003660
Iteration 67/1000 | Loss: 0.00003655
Iteration 68/1000 | Loss: 0.00003654
Iteration 69/1000 | Loss: 0.00003653
Iteration 70/1000 | Loss: 0.00003653
Iteration 71/1000 | Loss: 0.00003652
Iteration 72/1000 | Loss: 0.00003652
Iteration 73/1000 | Loss: 0.00003651
Iteration 74/1000 | Loss: 0.00003651
Iteration 75/1000 | Loss: 0.00003650
Iteration 76/1000 | Loss: 0.00003650
Iteration 77/1000 | Loss: 0.00003650
Iteration 78/1000 | Loss: 0.00003650
Iteration 79/1000 | Loss: 0.00003650
Iteration 80/1000 | Loss: 0.00003649
Iteration 81/1000 | Loss: 0.00003649
Iteration 82/1000 | Loss: 0.00003649
Iteration 83/1000 | Loss: 0.00003649
Iteration 84/1000 | Loss: 0.00003649
Iteration 85/1000 | Loss: 0.00003649
Iteration 86/1000 | Loss: 0.00003648
Iteration 87/1000 | Loss: 0.00003648
Iteration 88/1000 | Loss: 0.00003648
Iteration 89/1000 | Loss: 0.00003648
Iteration 90/1000 | Loss: 0.00003647
Iteration 91/1000 | Loss: 0.00003647
Iteration 92/1000 | Loss: 0.00003647
Iteration 93/1000 | Loss: 0.00003647
Iteration 94/1000 | Loss: 0.00003647
Iteration 95/1000 | Loss: 0.00003647
Iteration 96/1000 | Loss: 0.00003647
Iteration 97/1000 | Loss: 0.00003647
Iteration 98/1000 | Loss: 0.00003646
Iteration 99/1000 | Loss: 0.00003646
Iteration 100/1000 | Loss: 0.00003646
Iteration 101/1000 | Loss: 0.00003646
Iteration 102/1000 | Loss: 0.00003646
Iteration 103/1000 | Loss: 0.00003646
Iteration 104/1000 | Loss: 0.00003646
Iteration 105/1000 | Loss: 0.00003646
Iteration 106/1000 | Loss: 0.00003646
Iteration 107/1000 | Loss: 0.00003645
Iteration 108/1000 | Loss: 0.00003645
Iteration 109/1000 | Loss: 0.00003645
Iteration 110/1000 | Loss: 0.00003645
Iteration 111/1000 | Loss: 0.00003645
Iteration 112/1000 | Loss: 0.00003644
Iteration 113/1000 | Loss: 0.00003644
Iteration 114/1000 | Loss: 0.00003644
Iteration 115/1000 | Loss: 0.00003644
Iteration 116/1000 | Loss: 0.00003644
Iteration 117/1000 | Loss: 0.00003644
Iteration 118/1000 | Loss: 0.00003644
Iteration 119/1000 | Loss: 0.00003643
Iteration 120/1000 | Loss: 0.00003643
Iteration 121/1000 | Loss: 0.00003643
Iteration 122/1000 | Loss: 0.00003643
Iteration 123/1000 | Loss: 0.00003643
Iteration 124/1000 | Loss: 0.00003643
Iteration 125/1000 | Loss: 0.00003643
Iteration 126/1000 | Loss: 0.00003643
Iteration 127/1000 | Loss: 0.00003643
Iteration 128/1000 | Loss: 0.00003643
Iteration 129/1000 | Loss: 0.00003643
Iteration 130/1000 | Loss: 0.00003643
Iteration 131/1000 | Loss: 0.00003643
Iteration 132/1000 | Loss: 0.00003643
Iteration 133/1000 | Loss: 0.00003643
Iteration 134/1000 | Loss: 0.00003643
Iteration 135/1000 | Loss: 0.00003643
Iteration 136/1000 | Loss: 0.00003643
Iteration 137/1000 | Loss: 0.00003643
Iteration 138/1000 | Loss: 0.00003642
Iteration 139/1000 | Loss: 0.00003642
Iteration 140/1000 | Loss: 0.00003642
Iteration 141/1000 | Loss: 0.00003642
Iteration 142/1000 | Loss: 0.00003642
Iteration 143/1000 | Loss: 0.00003642
Iteration 144/1000 | Loss: 0.00003642
Iteration 145/1000 | Loss: 0.00003642
Iteration 146/1000 | Loss: 0.00003642
Iteration 147/1000 | Loss: 0.00003642
Iteration 148/1000 | Loss: 0.00003642
Iteration 149/1000 | Loss: 0.00003642
Iteration 150/1000 | Loss: 0.00003642
Iteration 151/1000 | Loss: 0.00003642
Iteration 152/1000 | Loss: 0.00003642
Iteration 153/1000 | Loss: 0.00003642
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 153. Stopping optimization.
Last 5 losses: [3.642016235971823e-05, 3.642016235971823e-05, 3.642016235971823e-05, 3.642016235971823e-05, 3.642016235971823e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.642016235971823e-05

Optimization complete. Final v2v error: 3.834477663040161 mm

Highest mean error: 11.209325790405273 mm for frame 93

Lowest mean error: 2.576852321624756 mm for frame 126

Saving results

Total time: 114.84843897819519
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_helen_posed_023/1065/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_helen_posed_023/1065.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_helen_posed_023/1065
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00461814
Iteration 2/25 | Loss: 0.00120938
Iteration 3/25 | Loss: 0.00116839
Iteration 4/25 | Loss: 0.00116078
Iteration 5/25 | Loss: 0.00115904
Iteration 6/25 | Loss: 0.00115894
Iteration 7/25 | Loss: 0.00115894
Iteration 8/25 | Loss: 0.00115894
Iteration 9/25 | Loss: 0.00115894
Iteration 10/25 | Loss: 0.00115894
Iteration 11/25 | Loss: 0.00115894
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011589418863877654, 0.0011589418863877654, 0.0011589418863877654, 0.0011589418863877654, 0.0011589418863877654]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011589418863877654

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.04654551
Iteration 2/25 | Loss: 0.00101733
Iteration 3/25 | Loss: 0.00101732
Iteration 4/25 | Loss: 0.00101732
Iteration 5/25 | Loss: 0.00101732
Iteration 6/25 | Loss: 0.00101732
Iteration 7/25 | Loss: 0.00101732
Iteration 8/25 | Loss: 0.00101732
Iteration 9/25 | Loss: 0.00101732
Iteration 10/25 | Loss: 0.00101732
Iteration 11/25 | Loss: 0.00101732
Iteration 12/25 | Loss: 0.00101732
Iteration 13/25 | Loss: 0.00101732
Iteration 14/25 | Loss: 0.00101732
Iteration 15/25 | Loss: 0.00101732
Iteration 16/25 | Loss: 0.00101732
Iteration 17/25 | Loss: 0.00101732
Iteration 18/25 | Loss: 0.00101732
Iteration 19/25 | Loss: 0.00101732
Iteration 20/25 | Loss: 0.00101732
Iteration 21/25 | Loss: 0.00101732
Iteration 22/25 | Loss: 0.00101732
Iteration 23/25 | Loss: 0.00101732
Iteration 24/25 | Loss: 0.00101732
Iteration 25/25 | Loss: 0.00101732

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00101732
Iteration 2/1000 | Loss: 0.00004284
Iteration 3/1000 | Loss: 0.00002806
Iteration 4/1000 | Loss: 0.00002333
Iteration 5/1000 | Loss: 0.00002222
Iteration 6/1000 | Loss: 0.00002089
Iteration 7/1000 | Loss: 0.00002014
Iteration 8/1000 | Loss: 0.00001950
Iteration 9/1000 | Loss: 0.00001905
Iteration 10/1000 | Loss: 0.00001854
Iteration 11/1000 | Loss: 0.00001821
Iteration 12/1000 | Loss: 0.00001792
Iteration 13/1000 | Loss: 0.00001765
Iteration 14/1000 | Loss: 0.00001763
Iteration 15/1000 | Loss: 0.00001734
Iteration 16/1000 | Loss: 0.00001719
Iteration 17/1000 | Loss: 0.00001719
Iteration 18/1000 | Loss: 0.00001719
Iteration 19/1000 | Loss: 0.00001718
Iteration 20/1000 | Loss: 0.00001718
Iteration 21/1000 | Loss: 0.00001718
Iteration 22/1000 | Loss: 0.00001718
Iteration 23/1000 | Loss: 0.00001718
Iteration 24/1000 | Loss: 0.00001718
Iteration 25/1000 | Loss: 0.00001718
Iteration 26/1000 | Loss: 0.00001718
Iteration 27/1000 | Loss: 0.00001717
Iteration 28/1000 | Loss: 0.00001717
Iteration 29/1000 | Loss: 0.00001717
Iteration 30/1000 | Loss: 0.00001717
Iteration 31/1000 | Loss: 0.00001717
Iteration 32/1000 | Loss: 0.00001717
Iteration 33/1000 | Loss: 0.00001717
Iteration 34/1000 | Loss: 0.00001717
Iteration 35/1000 | Loss: 0.00001717
Iteration 36/1000 | Loss: 0.00001717
Iteration 37/1000 | Loss: 0.00001717
Iteration 38/1000 | Loss: 0.00001717
Iteration 39/1000 | Loss: 0.00001716
Iteration 40/1000 | Loss: 0.00001716
Iteration 41/1000 | Loss: 0.00001716
Iteration 42/1000 | Loss: 0.00001716
Iteration 43/1000 | Loss: 0.00001716
Iteration 44/1000 | Loss: 0.00001716
Iteration 45/1000 | Loss: 0.00001716
Iteration 46/1000 | Loss: 0.00001715
Iteration 47/1000 | Loss: 0.00001715
Iteration 48/1000 | Loss: 0.00001714
Iteration 49/1000 | Loss: 0.00001714
Iteration 50/1000 | Loss: 0.00001714
Iteration 51/1000 | Loss: 0.00001714
Iteration 52/1000 | Loss: 0.00001714
Iteration 53/1000 | Loss: 0.00001713
Iteration 54/1000 | Loss: 0.00001713
Iteration 55/1000 | Loss: 0.00001713
Iteration 56/1000 | Loss: 0.00001713
Iteration 57/1000 | Loss: 0.00001713
Iteration 58/1000 | Loss: 0.00001712
Iteration 59/1000 | Loss: 0.00001712
Iteration 60/1000 | Loss: 0.00001711
Iteration 61/1000 | Loss: 0.00001711
Iteration 62/1000 | Loss: 0.00001711
Iteration 63/1000 | Loss: 0.00001711
Iteration 64/1000 | Loss: 0.00001711
Iteration 65/1000 | Loss: 0.00001711
Iteration 66/1000 | Loss: 0.00001711
Iteration 67/1000 | Loss: 0.00001711
Iteration 68/1000 | Loss: 0.00001711
Iteration 69/1000 | Loss: 0.00001711
Iteration 70/1000 | Loss: 0.00001710
Iteration 71/1000 | Loss: 0.00001710
Iteration 72/1000 | Loss: 0.00001710
Iteration 73/1000 | Loss: 0.00001710
Iteration 74/1000 | Loss: 0.00001709
Iteration 75/1000 | Loss: 0.00001709
Iteration 76/1000 | Loss: 0.00001709
Iteration 77/1000 | Loss: 0.00001709
Iteration 78/1000 | Loss: 0.00001709
Iteration 79/1000 | Loss: 0.00001709
Iteration 80/1000 | Loss: 0.00001709
Iteration 81/1000 | Loss: 0.00001709
Iteration 82/1000 | Loss: 0.00001709
Iteration 83/1000 | Loss: 0.00001709
Iteration 84/1000 | Loss: 0.00001708
Iteration 85/1000 | Loss: 0.00001708
Iteration 86/1000 | Loss: 0.00001708
Iteration 87/1000 | Loss: 0.00001707
Iteration 88/1000 | Loss: 0.00001707
Iteration 89/1000 | Loss: 0.00001706
Iteration 90/1000 | Loss: 0.00001705
Iteration 91/1000 | Loss: 0.00001705
Iteration 92/1000 | Loss: 0.00001704
Iteration 93/1000 | Loss: 0.00001704
Iteration 94/1000 | Loss: 0.00001704
Iteration 95/1000 | Loss: 0.00001704
Iteration 96/1000 | Loss: 0.00001703
Iteration 97/1000 | Loss: 0.00001703
Iteration 98/1000 | Loss: 0.00001703
Iteration 99/1000 | Loss: 0.00001703
Iteration 100/1000 | Loss: 0.00001703
Iteration 101/1000 | Loss: 0.00001702
Iteration 102/1000 | Loss: 0.00001701
Iteration 103/1000 | Loss: 0.00001701
Iteration 104/1000 | Loss: 0.00001700
Iteration 105/1000 | Loss: 0.00001700
Iteration 106/1000 | Loss: 0.00001699
Iteration 107/1000 | Loss: 0.00001699
Iteration 108/1000 | Loss: 0.00001699
Iteration 109/1000 | Loss: 0.00001699
Iteration 110/1000 | Loss: 0.00001698
Iteration 111/1000 | Loss: 0.00001698
Iteration 112/1000 | Loss: 0.00001698
Iteration 113/1000 | Loss: 0.00001697
Iteration 114/1000 | Loss: 0.00001697
Iteration 115/1000 | Loss: 0.00001697
Iteration 116/1000 | Loss: 0.00001696
Iteration 117/1000 | Loss: 0.00001696
Iteration 118/1000 | Loss: 0.00001696
Iteration 119/1000 | Loss: 0.00001696
Iteration 120/1000 | Loss: 0.00001696
Iteration 121/1000 | Loss: 0.00001696
Iteration 122/1000 | Loss: 0.00001696
Iteration 123/1000 | Loss: 0.00001696
Iteration 124/1000 | Loss: 0.00001696
Iteration 125/1000 | Loss: 0.00001696
Iteration 126/1000 | Loss: 0.00001696
Iteration 127/1000 | Loss: 0.00001696
Iteration 128/1000 | Loss: 0.00001696
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 128. Stopping optimization.
Last 5 losses: [1.696134677331429e-05, 1.696134677331429e-05, 1.696134677331429e-05, 1.696134677331429e-05, 1.696134677331429e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.696134677331429e-05

Optimization complete. Final v2v error: 3.50347638130188 mm

Highest mean error: 3.529437780380249 mm for frame 26

Lowest mean error: 3.4801907539367676 mm for frame 96

Saving results

Total time: 33.600627422332764
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_helen_posed_023/1069/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_helen_posed_023/1069.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_helen_posed_023/1069
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00803999
Iteration 2/25 | Loss: 0.00138441
Iteration 3/25 | Loss: 0.00127782
Iteration 4/25 | Loss: 0.00126936
Iteration 5/25 | Loss: 0.00126720
Iteration 6/25 | Loss: 0.00126649
Iteration 7/25 | Loss: 0.00126639
Iteration 8/25 | Loss: 0.00126639
Iteration 9/25 | Loss: 0.00126639
Iteration 10/25 | Loss: 0.00126639
Iteration 11/25 | Loss: 0.00126639
Iteration 12/25 | Loss: 0.00126639
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0012663877569139004, 0.0012663877569139004, 0.0012663877569139004, 0.0012663877569139004, 0.0012663877569139004]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012663877569139004

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.30871952
Iteration 2/25 | Loss: 0.00237989
Iteration 3/25 | Loss: 0.00237989
Iteration 4/25 | Loss: 0.00237989
Iteration 5/25 | Loss: 0.00237989
Iteration 6/25 | Loss: 0.00237989
Iteration 7/25 | Loss: 0.00237989
Iteration 8/25 | Loss: 0.00237989
Iteration 9/25 | Loss: 0.00237989
Iteration 10/25 | Loss: 0.00237989
Iteration 11/25 | Loss: 0.00237989
Iteration 12/25 | Loss: 0.00237989
Iteration 13/25 | Loss: 0.00237989
Iteration 14/25 | Loss: 0.00237989
Iteration 15/25 | Loss: 0.00237989
Iteration 16/25 | Loss: 0.00237989
Iteration 17/25 | Loss: 0.00237989
Iteration 18/25 | Loss: 0.00237989
Iteration 19/25 | Loss: 0.00237989
Iteration 20/25 | Loss: 0.00237989
Iteration 21/25 | Loss: 0.00237989
Iteration 22/25 | Loss: 0.00237989
Iteration 23/25 | Loss: 0.00237989
Iteration 24/25 | Loss: 0.00237989
Iteration 25/25 | Loss: 0.00237989
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.002379890065640211, 0.002379890065640211, 0.002379890065640211, 0.002379890065640211, 0.002379890065640211]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002379890065640211

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00237989
Iteration 2/1000 | Loss: 0.00015178
Iteration 3/1000 | Loss: 0.00009473
Iteration 4/1000 | Loss: 0.00007707
Iteration 5/1000 | Loss: 0.00012933
Iteration 6/1000 | Loss: 0.00011147
Iteration 7/1000 | Loss: 0.00010744
Iteration 8/1000 | Loss: 0.00022082
Iteration 9/1000 | Loss: 0.00011488
Iteration 10/1000 | Loss: 0.00007514
Iteration 11/1000 | Loss: 0.00010956
Iteration 12/1000 | Loss: 0.00012474
Iteration 13/1000 | Loss: 0.00006135
Iteration 14/1000 | Loss: 0.00019465
Iteration 15/1000 | Loss: 0.00015459
Iteration 16/1000 | Loss: 0.00024498
Iteration 17/1000 | Loss: 0.00016075
Iteration 18/1000 | Loss: 0.00017999
Iteration 19/1000 | Loss: 0.00016825
Iteration 20/1000 | Loss: 0.00012158
Iteration 21/1000 | Loss: 0.00015190
Iteration 22/1000 | Loss: 0.00012398
Iteration 23/1000 | Loss: 0.00013649
Iteration 24/1000 | Loss: 0.00012454
Iteration 25/1000 | Loss: 0.00013098
Iteration 26/1000 | Loss: 0.00005688
Iteration 27/1000 | Loss: 0.00005530
Iteration 28/1000 | Loss: 0.00005426
Iteration 29/1000 | Loss: 0.00013894
Iteration 30/1000 | Loss: 0.00011554
Iteration 31/1000 | Loss: 0.00011404
Iteration 32/1000 | Loss: 0.00025508
Iteration 33/1000 | Loss: 0.00022112
Iteration 34/1000 | Loss: 0.00008763
Iteration 35/1000 | Loss: 0.00008291
Iteration 36/1000 | Loss: 0.00023547
Iteration 37/1000 | Loss: 0.00006134
Iteration 38/1000 | Loss: 0.00005773
Iteration 39/1000 | Loss: 0.00010712
Iteration 40/1000 | Loss: 0.00009684
Iteration 41/1000 | Loss: 0.00006863
Iteration 42/1000 | Loss: 0.00007631
Iteration 43/1000 | Loss: 0.00006544
Iteration 44/1000 | Loss: 0.00006983
Iteration 45/1000 | Loss: 0.00007187
Iteration 46/1000 | Loss: 0.00016924
Iteration 47/1000 | Loss: 0.00013112
Iteration 48/1000 | Loss: 0.00005816
Iteration 49/1000 | Loss: 0.00018649
Iteration 50/1000 | Loss: 0.00014470
Iteration 51/1000 | Loss: 0.00015620
Iteration 52/1000 | Loss: 0.00006840
Iteration 53/1000 | Loss: 0.00007535
Iteration 54/1000 | Loss: 0.00015484
Iteration 55/1000 | Loss: 0.00008309
Iteration 56/1000 | Loss: 0.00011711
Iteration 57/1000 | Loss: 0.00011095
Iteration 58/1000 | Loss: 0.00013463
Iteration 59/1000 | Loss: 0.00011573
Iteration 60/1000 | Loss: 0.00012467
Iteration 61/1000 | Loss: 0.00011284
Iteration 62/1000 | Loss: 0.00012548
Iteration 63/1000 | Loss: 0.00015724
Iteration 64/1000 | Loss: 0.00015484
Iteration 65/1000 | Loss: 0.00008912
Iteration 66/1000 | Loss: 0.00014323
Iteration 67/1000 | Loss: 0.00005841
Iteration 68/1000 | Loss: 0.00005324
Iteration 69/1000 | Loss: 0.00018813
Iteration 70/1000 | Loss: 0.00008312
Iteration 71/1000 | Loss: 0.00012916
Iteration 72/1000 | Loss: 0.00013509
Iteration 73/1000 | Loss: 0.00027333
Iteration 74/1000 | Loss: 0.00012166
Iteration 75/1000 | Loss: 0.00009380
Iteration 76/1000 | Loss: 0.00018674
Iteration 77/1000 | Loss: 0.00012668
Iteration 78/1000 | Loss: 0.00014366
Iteration 79/1000 | Loss: 0.00006592
Iteration 80/1000 | Loss: 0.00010625
Iteration 81/1000 | Loss: 0.00017340
Iteration 82/1000 | Loss: 0.00005776
Iteration 83/1000 | Loss: 0.00007719
Iteration 84/1000 | Loss: 0.00005403
Iteration 85/1000 | Loss: 0.00005343
Iteration 86/1000 | Loss: 0.00005278
Iteration 87/1000 | Loss: 0.00005198
Iteration 88/1000 | Loss: 0.00005166
Iteration 89/1000 | Loss: 0.00005117
Iteration 90/1000 | Loss: 0.00005070
Iteration 91/1000 | Loss: 0.00005043
Iteration 92/1000 | Loss: 0.00005027
Iteration 93/1000 | Loss: 0.00004997
Iteration 94/1000 | Loss: 0.00004972
Iteration 95/1000 | Loss: 0.00004932
Iteration 96/1000 | Loss: 0.00004893
Iteration 97/1000 | Loss: 0.00004844
Iteration 98/1000 | Loss: 0.00004816
Iteration 99/1000 | Loss: 0.00004778
Iteration 100/1000 | Loss: 0.00004743
Iteration 101/1000 | Loss: 0.00004708
Iteration 102/1000 | Loss: 0.00004676
Iteration 103/1000 | Loss: 0.00004649
Iteration 104/1000 | Loss: 0.00004619
Iteration 105/1000 | Loss: 0.00004589
Iteration 106/1000 | Loss: 0.00004570
Iteration 107/1000 | Loss: 0.00004556
Iteration 108/1000 | Loss: 0.00004545
Iteration 109/1000 | Loss: 0.00004539
Iteration 110/1000 | Loss: 0.00004529
Iteration 111/1000 | Loss: 0.00004515
Iteration 112/1000 | Loss: 0.00004511
Iteration 113/1000 | Loss: 0.00004496
Iteration 114/1000 | Loss: 0.00004493
Iteration 115/1000 | Loss: 0.00004491
Iteration 116/1000 | Loss: 0.00004485
Iteration 117/1000 | Loss: 0.00004483
Iteration 118/1000 | Loss: 0.00004482
Iteration 119/1000 | Loss: 0.00004476
Iteration 120/1000 | Loss: 0.00004476
Iteration 121/1000 | Loss: 0.00004475
Iteration 122/1000 | Loss: 0.00004475
Iteration 123/1000 | Loss: 0.00004475
Iteration 124/1000 | Loss: 0.00004474
Iteration 125/1000 | Loss: 0.00004474
Iteration 126/1000 | Loss: 0.00004474
Iteration 127/1000 | Loss: 0.00004474
Iteration 128/1000 | Loss: 0.00004473
Iteration 129/1000 | Loss: 0.00004473
Iteration 130/1000 | Loss: 0.00004473
Iteration 131/1000 | Loss: 0.00004472
Iteration 132/1000 | Loss: 0.00004472
Iteration 133/1000 | Loss: 0.00004472
Iteration 134/1000 | Loss: 0.00004472
Iteration 135/1000 | Loss: 0.00004472
Iteration 136/1000 | Loss: 0.00004471
Iteration 137/1000 | Loss: 0.00004471
Iteration 138/1000 | Loss: 0.00004470
Iteration 139/1000 | Loss: 0.00004470
Iteration 140/1000 | Loss: 0.00004470
Iteration 141/1000 | Loss: 0.00004470
Iteration 142/1000 | Loss: 0.00004470
Iteration 143/1000 | Loss: 0.00004469
Iteration 144/1000 | Loss: 0.00004468
Iteration 145/1000 | Loss: 0.00004468
Iteration 146/1000 | Loss: 0.00004468
Iteration 147/1000 | Loss: 0.00004467
Iteration 148/1000 | Loss: 0.00004467
Iteration 149/1000 | Loss: 0.00004467
Iteration 150/1000 | Loss: 0.00004467
Iteration 151/1000 | Loss: 0.00004467
Iteration 152/1000 | Loss: 0.00004467
Iteration 153/1000 | Loss: 0.00004466
Iteration 154/1000 | Loss: 0.00004466
Iteration 155/1000 | Loss: 0.00004466
Iteration 156/1000 | Loss: 0.00004466
Iteration 157/1000 | Loss: 0.00004466
Iteration 158/1000 | Loss: 0.00004466
Iteration 159/1000 | Loss: 0.00004466
Iteration 160/1000 | Loss: 0.00004466
Iteration 161/1000 | Loss: 0.00004466
Iteration 162/1000 | Loss: 0.00004465
Iteration 163/1000 | Loss: 0.00004465
Iteration 164/1000 | Loss: 0.00004464
Iteration 165/1000 | Loss: 0.00004464
Iteration 166/1000 | Loss: 0.00004463
Iteration 167/1000 | Loss: 0.00004463
Iteration 168/1000 | Loss: 0.00004463
Iteration 169/1000 | Loss: 0.00004463
Iteration 170/1000 | Loss: 0.00004462
Iteration 171/1000 | Loss: 0.00004462
Iteration 172/1000 | Loss: 0.00004461
Iteration 173/1000 | Loss: 0.00004461
Iteration 174/1000 | Loss: 0.00004461
Iteration 175/1000 | Loss: 0.00004460
Iteration 176/1000 | Loss: 0.00004460
Iteration 177/1000 | Loss: 0.00004460
Iteration 178/1000 | Loss: 0.00004460
Iteration 179/1000 | Loss: 0.00004460
Iteration 180/1000 | Loss: 0.00004459
Iteration 181/1000 | Loss: 0.00004459
Iteration 182/1000 | Loss: 0.00004458
Iteration 183/1000 | Loss: 0.00004458
Iteration 184/1000 | Loss: 0.00004458
Iteration 185/1000 | Loss: 0.00004458
Iteration 186/1000 | Loss: 0.00004458
Iteration 187/1000 | Loss: 0.00004458
Iteration 188/1000 | Loss: 0.00004458
Iteration 189/1000 | Loss: 0.00004458
Iteration 190/1000 | Loss: 0.00004458
Iteration 191/1000 | Loss: 0.00004458
Iteration 192/1000 | Loss: 0.00004458
Iteration 193/1000 | Loss: 0.00004457
Iteration 194/1000 | Loss: 0.00004457
Iteration 195/1000 | Loss: 0.00004457
Iteration 196/1000 | Loss: 0.00004457
Iteration 197/1000 | Loss: 0.00004457
Iteration 198/1000 | Loss: 0.00004456
Iteration 199/1000 | Loss: 0.00004456
Iteration 200/1000 | Loss: 0.00004456
Iteration 201/1000 | Loss: 0.00004456
Iteration 202/1000 | Loss: 0.00004455
Iteration 203/1000 | Loss: 0.00004455
Iteration 204/1000 | Loss: 0.00004455
Iteration 205/1000 | Loss: 0.00004455
Iteration 206/1000 | Loss: 0.00004455
Iteration 207/1000 | Loss: 0.00004455
Iteration 208/1000 | Loss: 0.00004455
Iteration 209/1000 | Loss: 0.00004455
Iteration 210/1000 | Loss: 0.00004455
Iteration 211/1000 | Loss: 0.00004455
Iteration 212/1000 | Loss: 0.00004455
Iteration 213/1000 | Loss: 0.00004454
Iteration 214/1000 | Loss: 0.00004454
Iteration 215/1000 | Loss: 0.00004454
Iteration 216/1000 | Loss: 0.00004454
Iteration 217/1000 | Loss: 0.00004454
Iteration 218/1000 | Loss: 0.00004454
Iteration 219/1000 | Loss: 0.00004454
Iteration 220/1000 | Loss: 0.00004454
Iteration 221/1000 | Loss: 0.00004454
Iteration 222/1000 | Loss: 0.00004454
Iteration 223/1000 | Loss: 0.00004454
Iteration 224/1000 | Loss: 0.00004454
Iteration 225/1000 | Loss: 0.00004454
Iteration 226/1000 | Loss: 0.00004454
Iteration 227/1000 | Loss: 0.00004454
Iteration 228/1000 | Loss: 0.00004453
Iteration 229/1000 | Loss: 0.00004453
Iteration 230/1000 | Loss: 0.00004453
Iteration 231/1000 | Loss: 0.00004453
Iteration 232/1000 | Loss: 0.00004453
Iteration 233/1000 | Loss: 0.00004453
Iteration 234/1000 | Loss: 0.00004453
Iteration 235/1000 | Loss: 0.00004453
Iteration 236/1000 | Loss: 0.00004453
Iteration 237/1000 | Loss: 0.00004453
Iteration 238/1000 | Loss: 0.00004453
Iteration 239/1000 | Loss: 0.00004453
Iteration 240/1000 | Loss: 0.00004453
Iteration 241/1000 | Loss: 0.00004452
Iteration 242/1000 | Loss: 0.00004452
Iteration 243/1000 | Loss: 0.00004452
Iteration 244/1000 | Loss: 0.00004452
Iteration 245/1000 | Loss: 0.00004452
Iteration 246/1000 | Loss: 0.00004452
Iteration 247/1000 | Loss: 0.00004452
Iteration 248/1000 | Loss: 0.00004452
Iteration 249/1000 | Loss: 0.00004452
Iteration 250/1000 | Loss: 0.00004452
Iteration 251/1000 | Loss: 0.00004452
Iteration 252/1000 | Loss: 0.00004451
Iteration 253/1000 | Loss: 0.00004451
Iteration 254/1000 | Loss: 0.00004451
Iteration 255/1000 | Loss: 0.00004451
Iteration 256/1000 | Loss: 0.00004451
Iteration 257/1000 | Loss: 0.00004451
Iteration 258/1000 | Loss: 0.00004451
Iteration 259/1000 | Loss: 0.00004451
Iteration 260/1000 | Loss: 0.00004450
Iteration 261/1000 | Loss: 0.00004450
Iteration 262/1000 | Loss: 0.00004450
Iteration 263/1000 | Loss: 0.00004450
Iteration 264/1000 | Loss: 0.00004450
Iteration 265/1000 | Loss: 0.00004450
Iteration 266/1000 | Loss: 0.00004450
Iteration 267/1000 | Loss: 0.00004450
Iteration 268/1000 | Loss: 0.00004450
Iteration 269/1000 | Loss: 0.00004450
Iteration 270/1000 | Loss: 0.00004450
Iteration 271/1000 | Loss: 0.00004450
Iteration 272/1000 | Loss: 0.00004450
Iteration 273/1000 | Loss: 0.00004449
Iteration 274/1000 | Loss: 0.00004449
Iteration 275/1000 | Loss: 0.00004449
Iteration 276/1000 | Loss: 0.00004449
Iteration 277/1000 | Loss: 0.00004449
Iteration 278/1000 | Loss: 0.00004449
Iteration 279/1000 | Loss: 0.00004449
Iteration 280/1000 | Loss: 0.00004449
Iteration 281/1000 | Loss: 0.00004449
Iteration 282/1000 | Loss: 0.00004449
Iteration 283/1000 | Loss: 0.00004449
Iteration 284/1000 | Loss: 0.00004449
Iteration 285/1000 | Loss: 0.00004449
Iteration 286/1000 | Loss: 0.00004449
Iteration 287/1000 | Loss: 0.00004449
Iteration 288/1000 | Loss: 0.00004449
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 288. Stopping optimization.
Last 5 losses: [4.448596155270934e-05, 4.448596155270934e-05, 4.448596155270934e-05, 4.448596155270934e-05, 4.448596155270934e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.448596155270934e-05

Optimization complete. Final v2v error: 3.367396116256714 mm

Highest mean error: 11.272144317626953 mm for frame 85

Lowest mean error: 2.30690860748291 mm for frame 36

Saving results

Total time: 187.98996710777283
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_helen_posed_023/1021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_helen_posed_023/1021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_helen_posed_023/1021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01005740
Iteration 2/25 | Loss: 0.00229140
Iteration 3/25 | Loss: 0.00180501
Iteration 4/25 | Loss: 0.00161769
Iteration 5/25 | Loss: 0.00158784
Iteration 6/25 | Loss: 0.00148131
Iteration 7/25 | Loss: 0.00140214
Iteration 8/25 | Loss: 0.00130247
Iteration 9/25 | Loss: 0.00127686
Iteration 10/25 | Loss: 0.00126249
Iteration 11/25 | Loss: 0.00125779
Iteration 12/25 | Loss: 0.00125627
Iteration 13/25 | Loss: 0.00125669
Iteration 14/25 | Loss: 0.00125555
Iteration 15/25 | Loss: 0.00125566
Iteration 16/25 | Loss: 0.00125599
Iteration 17/25 | Loss: 0.00125484
Iteration 18/25 | Loss: 0.00125262
Iteration 19/25 | Loss: 0.00125243
Iteration 20/25 | Loss: 0.00125236
Iteration 21/25 | Loss: 0.00125236
Iteration 22/25 | Loss: 0.00125236
Iteration 23/25 | Loss: 0.00125235
Iteration 24/25 | Loss: 0.00125235
Iteration 25/25 | Loss: 0.00125235

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.26596487
Iteration 2/25 | Loss: 0.00110777
Iteration 3/25 | Loss: 0.00110777
Iteration 4/25 | Loss: 0.00110777
Iteration 5/25 | Loss: 0.00110777
Iteration 6/25 | Loss: 0.00110777
Iteration 7/25 | Loss: 0.00110777
Iteration 8/25 | Loss: 0.00110777
Iteration 9/25 | Loss: 0.00110777
Iteration 10/25 | Loss: 0.00110777
Iteration 11/25 | Loss: 0.00110777
Iteration 12/25 | Loss: 0.00110777
Iteration 13/25 | Loss: 0.00110777
Iteration 14/25 | Loss: 0.00110777
Iteration 15/25 | Loss: 0.00110777
Iteration 16/25 | Loss: 0.00110777
Iteration 17/25 | Loss: 0.00110777
Iteration 18/25 | Loss: 0.00110777
Iteration 19/25 | Loss: 0.00110777
Iteration 20/25 | Loss: 0.00110777
Iteration 21/25 | Loss: 0.00110777
Iteration 22/25 | Loss: 0.00110777
Iteration 23/25 | Loss: 0.00110777
Iteration 24/25 | Loss: 0.00110777
Iteration 25/25 | Loss: 0.00110777

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00110777
Iteration 2/1000 | Loss: 0.00003791
Iteration 3/1000 | Loss: 0.00002373
Iteration 4/1000 | Loss: 0.00002166
Iteration 5/1000 | Loss: 0.00002101
Iteration 6/1000 | Loss: 0.00002036
Iteration 7/1000 | Loss: 0.00001992
Iteration 8/1000 | Loss: 0.00001935
Iteration 9/1000 | Loss: 0.00001896
Iteration 10/1000 | Loss: 0.00001865
Iteration 11/1000 | Loss: 0.00001841
Iteration 12/1000 | Loss: 0.00001840
Iteration 13/1000 | Loss: 0.00001825
Iteration 14/1000 | Loss: 0.00001823
Iteration 15/1000 | Loss: 0.00001820
Iteration 16/1000 | Loss: 0.00001817
Iteration 17/1000 | Loss: 0.00001813
Iteration 18/1000 | Loss: 0.00001810
Iteration 19/1000 | Loss: 0.00001808
Iteration 20/1000 | Loss: 0.00001807
Iteration 21/1000 | Loss: 0.00001807
Iteration 22/1000 | Loss: 0.00001806
Iteration 23/1000 | Loss: 0.00001806
Iteration 24/1000 | Loss: 0.00001806
Iteration 25/1000 | Loss: 0.00001806
Iteration 26/1000 | Loss: 0.00001806
Iteration 27/1000 | Loss: 0.00001806
Iteration 28/1000 | Loss: 0.00001806
Iteration 29/1000 | Loss: 0.00001806
Iteration 30/1000 | Loss: 0.00001805
Iteration 31/1000 | Loss: 0.00001805
Iteration 32/1000 | Loss: 0.00001805
Iteration 33/1000 | Loss: 0.00001805
Iteration 34/1000 | Loss: 0.00001805
Iteration 35/1000 | Loss: 0.00001805
Iteration 36/1000 | Loss: 0.00001805
Iteration 37/1000 | Loss: 0.00001805
Iteration 38/1000 | Loss: 0.00001804
Iteration 39/1000 | Loss: 0.00001804
Iteration 40/1000 | Loss: 0.00001804
Iteration 41/1000 | Loss: 0.00001804
Iteration 42/1000 | Loss: 0.00001799
Iteration 43/1000 | Loss: 0.00001799
Iteration 44/1000 | Loss: 0.00001799
Iteration 45/1000 | Loss: 0.00001799
Iteration 46/1000 | Loss: 0.00001799
Iteration 47/1000 | Loss: 0.00001799
Iteration 48/1000 | Loss: 0.00001799
Iteration 49/1000 | Loss: 0.00001799
Iteration 50/1000 | Loss: 0.00001799
Iteration 51/1000 | Loss: 0.00001798
Iteration 52/1000 | Loss: 0.00001798
Iteration 53/1000 | Loss: 0.00001797
Iteration 54/1000 | Loss: 0.00001797
Iteration 55/1000 | Loss: 0.00001797
Iteration 56/1000 | Loss: 0.00001797
Iteration 57/1000 | Loss: 0.00003314
Iteration 58/1000 | Loss: 0.00002606
Iteration 59/1000 | Loss: 0.00001796
Iteration 60/1000 | Loss: 0.00001796
Iteration 61/1000 | Loss: 0.00001795
Iteration 62/1000 | Loss: 0.00001795
Iteration 63/1000 | Loss: 0.00001795
Iteration 64/1000 | Loss: 0.00001795
Iteration 65/1000 | Loss: 0.00001795
Iteration 66/1000 | Loss: 0.00001795
Iteration 67/1000 | Loss: 0.00001795
Iteration 68/1000 | Loss: 0.00001795
Iteration 69/1000 | Loss: 0.00001794
Iteration 70/1000 | Loss: 0.00001794
Iteration 71/1000 | Loss: 0.00001793
Iteration 72/1000 | Loss: 0.00001793
Iteration 73/1000 | Loss: 0.00001793
Iteration 74/1000 | Loss: 0.00001793
Iteration 75/1000 | Loss: 0.00001792
Iteration 76/1000 | Loss: 0.00001792
Iteration 77/1000 | Loss: 0.00001792
Iteration 78/1000 | Loss: 0.00001792
Iteration 79/1000 | Loss: 0.00001792
Iteration 80/1000 | Loss: 0.00001792
Iteration 81/1000 | Loss: 0.00001792
Iteration 82/1000 | Loss: 0.00001792
Iteration 83/1000 | Loss: 0.00001792
Iteration 84/1000 | Loss: 0.00001792
Iteration 85/1000 | Loss: 0.00001791
Iteration 86/1000 | Loss: 0.00001791
Iteration 87/1000 | Loss: 0.00001791
Iteration 88/1000 | Loss: 0.00001791
Iteration 89/1000 | Loss: 0.00001791
Iteration 90/1000 | Loss: 0.00001791
Iteration 91/1000 | Loss: 0.00001791
Iteration 92/1000 | Loss: 0.00001791
Iteration 93/1000 | Loss: 0.00001791
Iteration 94/1000 | Loss: 0.00001791
Iteration 95/1000 | Loss: 0.00001791
Iteration 96/1000 | Loss: 0.00001790
Iteration 97/1000 | Loss: 0.00001790
Iteration 98/1000 | Loss: 0.00001790
Iteration 99/1000 | Loss: 0.00001790
Iteration 100/1000 | Loss: 0.00001790
Iteration 101/1000 | Loss: 0.00001790
Iteration 102/1000 | Loss: 0.00001790
Iteration 103/1000 | Loss: 0.00001790
Iteration 104/1000 | Loss: 0.00001790
Iteration 105/1000 | Loss: 0.00001789
Iteration 106/1000 | Loss: 0.00001789
Iteration 107/1000 | Loss: 0.00001789
Iteration 108/1000 | Loss: 0.00001789
Iteration 109/1000 | Loss: 0.00001789
Iteration 110/1000 | Loss: 0.00001789
Iteration 111/1000 | Loss: 0.00001789
Iteration 112/1000 | Loss: 0.00001789
Iteration 113/1000 | Loss: 0.00001789
Iteration 114/1000 | Loss: 0.00001789
Iteration 115/1000 | Loss: 0.00001789
Iteration 116/1000 | Loss: 0.00001789
Iteration 117/1000 | Loss: 0.00001789
Iteration 118/1000 | Loss: 0.00001789
Iteration 119/1000 | Loss: 0.00001789
Iteration 120/1000 | Loss: 0.00001789
Iteration 121/1000 | Loss: 0.00001789
Iteration 122/1000 | Loss: 0.00001788
Iteration 123/1000 | Loss: 0.00001788
Iteration 124/1000 | Loss: 0.00001788
Iteration 125/1000 | Loss: 0.00001787
Iteration 126/1000 | Loss: 0.00001787
Iteration 127/1000 | Loss: 0.00001787
Iteration 128/1000 | Loss: 0.00001787
Iteration 129/1000 | Loss: 0.00001787
Iteration 130/1000 | Loss: 0.00001787
Iteration 131/1000 | Loss: 0.00001787
Iteration 132/1000 | Loss: 0.00001787
Iteration 133/1000 | Loss: 0.00001787
Iteration 134/1000 | Loss: 0.00001787
Iteration 135/1000 | Loss: 0.00001787
Iteration 136/1000 | Loss: 0.00001787
Iteration 137/1000 | Loss: 0.00001787
Iteration 138/1000 | Loss: 0.00001787
Iteration 139/1000 | Loss: 0.00001787
Iteration 140/1000 | Loss: 0.00001787
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 140. Stopping optimization.
Last 5 losses: [1.7868294889922254e-05, 1.7868294889922254e-05, 1.7868294889922254e-05, 1.7868294889922254e-05, 1.7868294889922254e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7868294889922254e-05

Optimization complete. Final v2v error: 3.5569229125976562 mm

Highest mean error: 4.774250507354736 mm for frame 61

Lowest mean error: 3.3926947116851807 mm for frame 3

Saving results

Total time: 61.07039952278137
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_helen_posed_023/1027/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_helen_posed_023/1027.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_helen_posed_023/1027
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00624317
Iteration 2/25 | Loss: 0.00124305
Iteration 3/25 | Loss: 0.00118286
Iteration 4/25 | Loss: 0.00117601
Iteration 5/25 | Loss: 0.00117354
Iteration 6/25 | Loss: 0.00117315
Iteration 7/25 | Loss: 0.00117315
Iteration 8/25 | Loss: 0.00117315
Iteration 9/25 | Loss: 0.00117315
Iteration 10/25 | Loss: 0.00117315
Iteration 11/25 | Loss: 0.00117315
Iteration 12/25 | Loss: 0.00117315
Iteration 13/25 | Loss: 0.00117315
Iteration 14/25 | Loss: 0.00117315
Iteration 15/25 | Loss: 0.00117315
Iteration 16/25 | Loss: 0.00117315
Iteration 17/25 | Loss: 0.00117315
Iteration 18/25 | Loss: 0.00117315
Iteration 19/25 | Loss: 0.00117315
Iteration 20/25 | Loss: 0.00117315
Iteration 21/25 | Loss: 0.00117315
Iteration 22/25 | Loss: 0.00117315
Iteration 23/25 | Loss: 0.00117315
Iteration 24/25 | Loss: 0.00117315
Iteration 25/25 | Loss: 0.00117315

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35223401
Iteration 2/25 | Loss: 0.00120774
Iteration 3/25 | Loss: 0.00120774
Iteration 4/25 | Loss: 0.00120774
Iteration 5/25 | Loss: 0.00120774
Iteration 6/25 | Loss: 0.00120774
Iteration 7/25 | Loss: 0.00120774
Iteration 8/25 | Loss: 0.00120774
Iteration 9/25 | Loss: 0.00120774
Iteration 10/25 | Loss: 0.00120774
Iteration 11/25 | Loss: 0.00120774
Iteration 12/25 | Loss: 0.00120774
Iteration 13/25 | Loss: 0.00120774
Iteration 14/25 | Loss: 0.00120774
Iteration 15/25 | Loss: 0.00120774
Iteration 16/25 | Loss: 0.00120774
Iteration 17/25 | Loss: 0.00120774
Iteration 18/25 | Loss: 0.00120774
Iteration 19/25 | Loss: 0.00120774
Iteration 20/25 | Loss: 0.00120774
Iteration 21/25 | Loss: 0.00120774
Iteration 22/25 | Loss: 0.00120774
Iteration 23/25 | Loss: 0.00120774
Iteration 24/25 | Loss: 0.00120774
Iteration 25/25 | Loss: 0.00120774

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00120774
Iteration 2/1000 | Loss: 0.00001906
Iteration 3/1000 | Loss: 0.00001296
Iteration 4/1000 | Loss: 0.00001190
Iteration 5/1000 | Loss: 0.00001131
Iteration 6/1000 | Loss: 0.00001084
Iteration 7/1000 | Loss: 0.00001040
Iteration 8/1000 | Loss: 0.00001009
Iteration 9/1000 | Loss: 0.00000983
Iteration 10/1000 | Loss: 0.00000961
Iteration 11/1000 | Loss: 0.00000957
Iteration 12/1000 | Loss: 0.00000955
Iteration 13/1000 | Loss: 0.00000940
Iteration 14/1000 | Loss: 0.00000934
Iteration 15/1000 | Loss: 0.00000928
Iteration 16/1000 | Loss: 0.00000928
Iteration 17/1000 | Loss: 0.00000926
Iteration 18/1000 | Loss: 0.00000924
Iteration 19/1000 | Loss: 0.00000924
Iteration 20/1000 | Loss: 0.00000924
Iteration 21/1000 | Loss: 0.00000923
Iteration 22/1000 | Loss: 0.00000920
Iteration 23/1000 | Loss: 0.00000920
Iteration 24/1000 | Loss: 0.00000918
Iteration 25/1000 | Loss: 0.00000917
Iteration 26/1000 | Loss: 0.00000912
Iteration 27/1000 | Loss: 0.00000912
Iteration 28/1000 | Loss: 0.00000912
Iteration 29/1000 | Loss: 0.00000911
Iteration 30/1000 | Loss: 0.00000910
Iteration 31/1000 | Loss: 0.00000910
Iteration 32/1000 | Loss: 0.00000909
Iteration 33/1000 | Loss: 0.00000908
Iteration 34/1000 | Loss: 0.00000908
Iteration 35/1000 | Loss: 0.00000908
Iteration 36/1000 | Loss: 0.00000907
Iteration 37/1000 | Loss: 0.00000907
Iteration 38/1000 | Loss: 0.00000907
Iteration 39/1000 | Loss: 0.00000907
Iteration 40/1000 | Loss: 0.00000907
Iteration 41/1000 | Loss: 0.00000907
Iteration 42/1000 | Loss: 0.00000907
Iteration 43/1000 | Loss: 0.00000907
Iteration 44/1000 | Loss: 0.00000907
Iteration 45/1000 | Loss: 0.00000907
Iteration 46/1000 | Loss: 0.00000907
Iteration 47/1000 | Loss: 0.00000906
Iteration 48/1000 | Loss: 0.00000906
Iteration 49/1000 | Loss: 0.00000905
Iteration 50/1000 | Loss: 0.00000904
Iteration 51/1000 | Loss: 0.00000904
Iteration 52/1000 | Loss: 0.00000904
Iteration 53/1000 | Loss: 0.00000904
Iteration 54/1000 | Loss: 0.00000904
Iteration 55/1000 | Loss: 0.00000904
Iteration 56/1000 | Loss: 0.00000904
Iteration 57/1000 | Loss: 0.00000904
Iteration 58/1000 | Loss: 0.00000904
Iteration 59/1000 | Loss: 0.00000903
Iteration 60/1000 | Loss: 0.00000903
Iteration 61/1000 | Loss: 0.00000902
Iteration 62/1000 | Loss: 0.00000902
Iteration 63/1000 | Loss: 0.00000902
Iteration 64/1000 | Loss: 0.00000902
Iteration 65/1000 | Loss: 0.00000902
Iteration 66/1000 | Loss: 0.00000902
Iteration 67/1000 | Loss: 0.00000901
Iteration 68/1000 | Loss: 0.00000900
Iteration 69/1000 | Loss: 0.00000900
Iteration 70/1000 | Loss: 0.00000899
Iteration 71/1000 | Loss: 0.00000899
Iteration 72/1000 | Loss: 0.00000898
Iteration 73/1000 | Loss: 0.00000897
Iteration 74/1000 | Loss: 0.00000897
Iteration 75/1000 | Loss: 0.00000897
Iteration 76/1000 | Loss: 0.00000897
Iteration 77/1000 | Loss: 0.00000896
Iteration 78/1000 | Loss: 0.00000896
Iteration 79/1000 | Loss: 0.00000896
Iteration 80/1000 | Loss: 0.00000896
Iteration 81/1000 | Loss: 0.00000896
Iteration 82/1000 | Loss: 0.00000895
Iteration 83/1000 | Loss: 0.00000895
Iteration 84/1000 | Loss: 0.00000895
Iteration 85/1000 | Loss: 0.00000895
Iteration 86/1000 | Loss: 0.00000894
Iteration 87/1000 | Loss: 0.00000894
Iteration 88/1000 | Loss: 0.00000894
Iteration 89/1000 | Loss: 0.00000894
Iteration 90/1000 | Loss: 0.00000894
Iteration 91/1000 | Loss: 0.00000893
Iteration 92/1000 | Loss: 0.00000893
Iteration 93/1000 | Loss: 0.00000893
Iteration 94/1000 | Loss: 0.00000893
Iteration 95/1000 | Loss: 0.00000893
Iteration 96/1000 | Loss: 0.00000893
Iteration 97/1000 | Loss: 0.00000893
Iteration 98/1000 | Loss: 0.00000893
Iteration 99/1000 | Loss: 0.00000893
Iteration 100/1000 | Loss: 0.00000893
Iteration 101/1000 | Loss: 0.00000893
Iteration 102/1000 | Loss: 0.00000893
Iteration 103/1000 | Loss: 0.00000893
Iteration 104/1000 | Loss: 0.00000893
Iteration 105/1000 | Loss: 0.00000893
Iteration 106/1000 | Loss: 0.00000893
Iteration 107/1000 | Loss: 0.00000893
Iteration 108/1000 | Loss: 0.00000893
Iteration 109/1000 | Loss: 0.00000893
Iteration 110/1000 | Loss: 0.00000893
Iteration 111/1000 | Loss: 0.00000893
Iteration 112/1000 | Loss: 0.00000893
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 112. Stopping optimization.
Last 5 losses: [8.929498108045664e-06, 8.929498108045664e-06, 8.929498108045664e-06, 8.929498108045664e-06, 8.929498108045664e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.929498108045664e-06

Optimization complete. Final v2v error: 2.575364828109741 mm

Highest mean error: 3.182910680770874 mm for frame 80

Lowest mean error: 2.3853578567504883 mm for frame 104

Saving results

Total time: 33.117841958999634
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_helen_posed_023/1004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_helen_posed_023/1004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_helen_posed_023/1004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01019076
Iteration 2/25 | Loss: 0.00249527
Iteration 3/25 | Loss: 0.00213431
Iteration 4/25 | Loss: 0.00196324
Iteration 5/25 | Loss: 0.00203149
Iteration 6/25 | Loss: 0.00206608
Iteration 7/25 | Loss: 0.00209997
Iteration 8/25 | Loss: 0.00201741
Iteration 9/25 | Loss: 0.00183849
Iteration 10/25 | Loss: 0.00173885
Iteration 11/25 | Loss: 0.00165166
Iteration 12/25 | Loss: 0.00161170
Iteration 13/25 | Loss: 0.00160013
Iteration 14/25 | Loss: 0.00160394
Iteration 15/25 | Loss: 0.00158300
Iteration 16/25 | Loss: 0.00157219
Iteration 17/25 | Loss: 0.00156106
Iteration 18/25 | Loss: 0.00156028
Iteration 19/25 | Loss: 0.00155237
Iteration 20/25 | Loss: 0.00154316
Iteration 21/25 | Loss: 0.00153371
Iteration 22/25 | Loss: 0.00154443
Iteration 23/25 | Loss: 0.00152546
Iteration 24/25 | Loss: 0.00151404
Iteration 25/25 | Loss: 0.00150711

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.14890850
Iteration 2/25 | Loss: 0.00215740
Iteration 3/25 | Loss: 0.00215736
Iteration 4/25 | Loss: 0.00215735
Iteration 5/25 | Loss: 0.00215735
Iteration 6/25 | Loss: 0.00215735
Iteration 7/25 | Loss: 0.00215735
Iteration 8/25 | Loss: 0.00215735
Iteration 9/25 | Loss: 0.00215735
Iteration 10/25 | Loss: 0.00215735
Iteration 11/25 | Loss: 0.00215735
Iteration 12/25 | Loss: 0.00215735
Iteration 13/25 | Loss: 0.00215735
Iteration 14/25 | Loss: 0.00215735
Iteration 15/25 | Loss: 0.00215735
Iteration 16/25 | Loss: 0.00215735
Iteration 17/25 | Loss: 0.00215735
Iteration 18/25 | Loss: 0.00215735
Iteration 19/25 | Loss: 0.00215735
Iteration 20/25 | Loss: 0.00215735
Iteration 21/25 | Loss: 0.00215735
Iteration 22/25 | Loss: 0.00215735
Iteration 23/25 | Loss: 0.00215735
Iteration 24/25 | Loss: 0.00215735
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.002157350303605199, 0.002157350303605199, 0.002157350303605199, 0.002157350303605199, 0.002157350303605199]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002157350303605199

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00215735
Iteration 2/1000 | Loss: 0.00797537
Iteration 3/1000 | Loss: 0.00779614
Iteration 4/1000 | Loss: 0.00701474
Iteration 5/1000 | Loss: 0.00874006
Iteration 6/1000 | Loss: 0.00188327
Iteration 7/1000 | Loss: 0.00053217
Iteration 8/1000 | Loss: 0.00323067
Iteration 9/1000 | Loss: 0.00410265
Iteration 10/1000 | Loss: 0.00456888
Iteration 11/1000 | Loss: 0.00702596
Iteration 12/1000 | Loss: 0.00255953
Iteration 13/1000 | Loss: 0.00199828
Iteration 14/1000 | Loss: 0.00371185
Iteration 15/1000 | Loss: 0.00179439
Iteration 16/1000 | Loss: 0.00177687
Iteration 17/1000 | Loss: 0.00131368
Iteration 18/1000 | Loss: 0.00138019
Iteration 19/1000 | Loss: 0.00213455
Iteration 20/1000 | Loss: 0.00128478
Iteration 21/1000 | Loss: 0.00160873
Iteration 22/1000 | Loss: 0.00188390
Iteration 23/1000 | Loss: 0.00290052
Iteration 24/1000 | Loss: 0.00271258
Iteration 25/1000 | Loss: 0.00224468
Iteration 26/1000 | Loss: 0.00119855
Iteration 27/1000 | Loss: 0.00130370
Iteration 28/1000 | Loss: 0.00096601
Iteration 29/1000 | Loss: 0.00168020
Iteration 30/1000 | Loss: 0.00220294
Iteration 31/1000 | Loss: 0.00048095
Iteration 32/1000 | Loss: 0.00028209
Iteration 33/1000 | Loss: 0.00013743
Iteration 34/1000 | Loss: 0.00060070
Iteration 35/1000 | Loss: 0.00071858
Iteration 36/1000 | Loss: 0.00068873
Iteration 37/1000 | Loss: 0.00047467
Iteration 38/1000 | Loss: 0.00158761
Iteration 39/1000 | Loss: 0.00173033
Iteration 40/1000 | Loss: 0.00054690
Iteration 41/1000 | Loss: 0.00047370
Iteration 42/1000 | Loss: 0.00045116
Iteration 43/1000 | Loss: 0.00054693
Iteration 44/1000 | Loss: 0.00036403
Iteration 45/1000 | Loss: 0.00030096
Iteration 46/1000 | Loss: 0.00021076
Iteration 47/1000 | Loss: 0.00046420
Iteration 48/1000 | Loss: 0.00110881
Iteration 49/1000 | Loss: 0.00084006
Iteration 50/1000 | Loss: 0.00197316
Iteration 51/1000 | Loss: 0.00087134
Iteration 52/1000 | Loss: 0.00049375
Iteration 53/1000 | Loss: 0.00040456
Iteration 54/1000 | Loss: 0.00038408
Iteration 55/1000 | Loss: 0.00037037
Iteration 56/1000 | Loss: 0.00153830
Iteration 57/1000 | Loss: 0.00021378
Iteration 58/1000 | Loss: 0.00029221
Iteration 59/1000 | Loss: 0.00027499
Iteration 60/1000 | Loss: 0.00025486
Iteration 61/1000 | Loss: 0.00007261
Iteration 62/1000 | Loss: 0.00011991
Iteration 63/1000 | Loss: 0.00158041
Iteration 64/1000 | Loss: 0.00027437
Iteration 65/1000 | Loss: 0.00044608
Iteration 66/1000 | Loss: 0.00007817
Iteration 67/1000 | Loss: 0.00013239
Iteration 68/1000 | Loss: 0.00037810
Iteration 69/1000 | Loss: 0.00020433
Iteration 70/1000 | Loss: 0.00020518
Iteration 71/1000 | Loss: 0.00005295
Iteration 72/1000 | Loss: 0.00034859
Iteration 73/1000 | Loss: 0.00059687
Iteration 74/1000 | Loss: 0.00006646
Iteration 75/1000 | Loss: 0.00020596
Iteration 76/1000 | Loss: 0.00018258
Iteration 77/1000 | Loss: 0.00021733
Iteration 78/1000 | Loss: 0.00023436
Iteration 79/1000 | Loss: 0.00009659
Iteration 80/1000 | Loss: 0.00007340
Iteration 81/1000 | Loss: 0.00007564
Iteration 82/1000 | Loss: 0.00006137
Iteration 83/1000 | Loss: 0.00005728
Iteration 84/1000 | Loss: 0.00005019
Iteration 85/1000 | Loss: 0.00004112
Iteration 86/1000 | Loss: 0.00011785
Iteration 87/1000 | Loss: 0.00011264
Iteration 88/1000 | Loss: 0.00011725
Iteration 89/1000 | Loss: 0.00004911
Iteration 90/1000 | Loss: 0.00004071
Iteration 91/1000 | Loss: 0.00003736
Iteration 92/1000 | Loss: 0.00026784
Iteration 93/1000 | Loss: 0.00003607
Iteration 94/1000 | Loss: 0.00003453
Iteration 95/1000 | Loss: 0.00018594
Iteration 96/1000 | Loss: 0.00005337
Iteration 97/1000 | Loss: 0.00017001
Iteration 98/1000 | Loss: 0.00007596
Iteration 99/1000 | Loss: 0.00003708
Iteration 100/1000 | Loss: 0.00003226
Iteration 101/1000 | Loss: 0.00003159
Iteration 102/1000 | Loss: 0.00003117
Iteration 103/1000 | Loss: 0.00032061
Iteration 104/1000 | Loss: 0.00006544
Iteration 105/1000 | Loss: 0.00004733
Iteration 106/1000 | Loss: 0.00005243
Iteration 107/1000 | Loss: 0.00004139
Iteration 108/1000 | Loss: 0.00003391
Iteration 109/1000 | Loss: 0.00003293
Iteration 110/1000 | Loss: 0.00003235
Iteration 111/1000 | Loss: 0.00003194
Iteration 112/1000 | Loss: 0.00003153
Iteration 113/1000 | Loss: 0.00018646
Iteration 114/1000 | Loss: 0.00008532
Iteration 115/1000 | Loss: 0.00018023
Iteration 116/1000 | Loss: 0.00009686
Iteration 117/1000 | Loss: 0.00004421
Iteration 118/1000 | Loss: 0.00003700
Iteration 119/1000 | Loss: 0.00003235
Iteration 120/1000 | Loss: 0.00004643
Iteration 121/1000 | Loss: 0.00016370
Iteration 122/1000 | Loss: 0.00004436
Iteration 123/1000 | Loss: 0.00003815
Iteration 124/1000 | Loss: 0.00003366
Iteration 125/1000 | Loss: 0.00003204
Iteration 126/1000 | Loss: 0.00003089
Iteration 127/1000 | Loss: 0.00003037
Iteration 128/1000 | Loss: 0.00003001
Iteration 129/1000 | Loss: 0.00013995
Iteration 130/1000 | Loss: 0.00007951
Iteration 131/1000 | Loss: 0.00013252
Iteration 132/1000 | Loss: 0.00006631
Iteration 133/1000 | Loss: 0.00007002
Iteration 134/1000 | Loss: 0.00005592
Iteration 135/1000 | Loss: 0.00006871
Iteration 136/1000 | Loss: 0.00003279
Iteration 137/1000 | Loss: 0.00003098
Iteration 138/1000 | Loss: 0.00014348
Iteration 139/1000 | Loss: 0.00031837
Iteration 140/1000 | Loss: 0.00025197
Iteration 141/1000 | Loss: 0.00024960
Iteration 142/1000 | Loss: 0.00006844
Iteration 143/1000 | Loss: 0.00007677
Iteration 144/1000 | Loss: 0.00003597
Iteration 145/1000 | Loss: 0.00004574
Iteration 146/1000 | Loss: 0.00004343
Iteration 147/1000 | Loss: 0.00017019
Iteration 148/1000 | Loss: 0.00018129
Iteration 149/1000 | Loss: 0.00005367
Iteration 150/1000 | Loss: 0.00013039
Iteration 151/1000 | Loss: 0.00004480
Iteration 152/1000 | Loss: 0.00016222
Iteration 153/1000 | Loss: 0.00009909
Iteration 154/1000 | Loss: 0.00015283
Iteration 155/1000 | Loss: 0.00016074
Iteration 156/1000 | Loss: 0.00016879
Iteration 157/1000 | Loss: 0.00015116
Iteration 158/1000 | Loss: 0.00005859
Iteration 159/1000 | Loss: 0.00007641
Iteration 160/1000 | Loss: 0.00016335
Iteration 161/1000 | Loss: 0.00010462
Iteration 162/1000 | Loss: 0.00016009
Iteration 163/1000 | Loss: 0.00004015
Iteration 164/1000 | Loss: 0.00003112
Iteration 165/1000 | Loss: 0.00008186
Iteration 166/1000 | Loss: 0.00003578
Iteration 167/1000 | Loss: 0.00010388
Iteration 168/1000 | Loss: 0.00012731
Iteration 169/1000 | Loss: 0.00014344
Iteration 170/1000 | Loss: 0.00014449
Iteration 171/1000 | Loss: 0.00013407
Iteration 172/1000 | Loss: 0.00004068
Iteration 173/1000 | Loss: 0.00003438
Iteration 174/1000 | Loss: 0.00003199
Iteration 175/1000 | Loss: 0.00003026
Iteration 176/1000 | Loss: 0.00002944
Iteration 177/1000 | Loss: 0.00015329
Iteration 178/1000 | Loss: 0.00008151
Iteration 179/1000 | Loss: 0.00003615
Iteration 180/1000 | Loss: 0.00003079
Iteration 181/1000 | Loss: 0.00014359
Iteration 182/1000 | Loss: 0.00012744
Iteration 183/1000 | Loss: 0.00014767
Iteration 184/1000 | Loss: 0.00003156
Iteration 185/1000 | Loss: 0.00002909
Iteration 186/1000 | Loss: 0.00002706
Iteration 187/1000 | Loss: 0.00002632
Iteration 188/1000 | Loss: 0.00002580
Iteration 189/1000 | Loss: 0.00002551
Iteration 190/1000 | Loss: 0.00002527
Iteration 191/1000 | Loss: 0.00003783
Iteration 192/1000 | Loss: 0.00003782
Iteration 193/1000 | Loss: 0.00004532
Iteration 194/1000 | Loss: 0.00004569
Iteration 195/1000 | Loss: 0.00003345
Iteration 196/1000 | Loss: 0.00002657
Iteration 197/1000 | Loss: 0.00002622
Iteration 198/1000 | Loss: 0.00002530
Iteration 199/1000 | Loss: 0.00003407
Iteration 200/1000 | Loss: 0.00003159
Iteration 201/1000 | Loss: 0.00003333
Iteration 202/1000 | Loss: 0.00003045
Iteration 203/1000 | Loss: 0.00003104
Iteration 204/1000 | Loss: 0.00002984
Iteration 205/1000 | Loss: 0.00002475
Iteration 206/1000 | Loss: 0.00004228
Iteration 207/1000 | Loss: 0.00003789
Iteration 208/1000 | Loss: 0.00002613
Iteration 209/1000 | Loss: 0.00004339
Iteration 210/1000 | Loss: 0.00003864
Iteration 211/1000 | Loss: 0.00003023
Iteration 212/1000 | Loss: 0.00002519
Iteration 213/1000 | Loss: 0.00004525
Iteration 214/1000 | Loss: 0.00004398
Iteration 215/1000 | Loss: 0.00002837
Iteration 216/1000 | Loss: 0.00002619
Iteration 217/1000 | Loss: 0.00003159
Iteration 218/1000 | Loss: 0.00004054
Iteration 219/1000 | Loss: 0.00003576
Iteration 220/1000 | Loss: 0.00002685
Iteration 221/1000 | Loss: 0.00005209
Iteration 222/1000 | Loss: 0.00005312
Iteration 223/1000 | Loss: 0.00005156
Iteration 224/1000 | Loss: 0.00004215
Iteration 225/1000 | Loss: 0.00002606
Iteration 226/1000 | Loss: 0.00003535
Iteration 227/1000 | Loss: 0.00004632
Iteration 228/1000 | Loss: 0.00004071
Iteration 229/1000 | Loss: 0.00005044
Iteration 230/1000 | Loss: 0.00003974
Iteration 231/1000 | Loss: 0.00005397
Iteration 232/1000 | Loss: 0.00004128
Iteration 233/1000 | Loss: 0.00005260
Iteration 234/1000 | Loss: 0.00004236
Iteration 235/1000 | Loss: 0.00005404
Iteration 236/1000 | Loss: 0.00005160
Iteration 237/1000 | Loss: 0.00003957
Iteration 238/1000 | Loss: 0.00003058
Iteration 239/1000 | Loss: 0.00002776
Iteration 240/1000 | Loss: 0.00002606
Iteration 241/1000 | Loss: 0.00002614
Iteration 242/1000 | Loss: 0.00002519
Iteration 243/1000 | Loss: 0.00002425
Iteration 244/1000 | Loss: 0.00002602
Iteration 245/1000 | Loss: 0.00002432
Iteration 246/1000 | Loss: 0.00002519
Iteration 247/1000 | Loss: 0.00002415
Iteration 248/1000 | Loss: 0.00002506
Iteration 249/1000 | Loss: 0.00002423
Iteration 250/1000 | Loss: 0.00002411
Iteration 251/1000 | Loss: 0.00002400
Iteration 252/1000 | Loss: 0.00002400
Iteration 253/1000 | Loss: 0.00002400
Iteration 254/1000 | Loss: 0.00002427
Iteration 255/1000 | Loss: 0.00002396
Iteration 256/1000 | Loss: 0.00002521
Iteration 257/1000 | Loss: 0.00002391
Iteration 258/1000 | Loss: 0.00002391
Iteration 259/1000 | Loss: 0.00002391
Iteration 260/1000 | Loss: 0.00002391
Iteration 261/1000 | Loss: 0.00002390
Iteration 262/1000 | Loss: 0.00002390
Iteration 263/1000 | Loss: 0.00002390
Iteration 264/1000 | Loss: 0.00002390
Iteration 265/1000 | Loss: 0.00002390
Iteration 266/1000 | Loss: 0.00002390
Iteration 267/1000 | Loss: 0.00002390
Iteration 268/1000 | Loss: 0.00002390
Iteration 269/1000 | Loss: 0.00002376
Iteration 270/1000 | Loss: 0.00002375
Iteration 271/1000 | Loss: 0.00002371
Iteration 272/1000 | Loss: 0.00003383
Iteration 273/1000 | Loss: 0.00002521
Iteration 274/1000 | Loss: 0.00003469
Iteration 275/1000 | Loss: 0.00003568
Iteration 276/1000 | Loss: 0.00002415
Iteration 277/1000 | Loss: 0.00003414
Iteration 278/1000 | Loss: 0.00002977
Iteration 279/1000 | Loss: 0.00003426
Iteration 280/1000 | Loss: 0.00002368
Iteration 281/1000 | Loss: 0.00002355
Iteration 282/1000 | Loss: 0.00002355
Iteration 283/1000 | Loss: 0.00002351
Iteration 284/1000 | Loss: 0.00002348
Iteration 285/1000 | Loss: 0.00002345
Iteration 286/1000 | Loss: 0.00002345
Iteration 287/1000 | Loss: 0.00002344
Iteration 288/1000 | Loss: 0.00002344
Iteration 289/1000 | Loss: 0.00002343
Iteration 290/1000 | Loss: 0.00002343
Iteration 291/1000 | Loss: 0.00002342
Iteration 292/1000 | Loss: 0.00002342
Iteration 293/1000 | Loss: 0.00002342
Iteration 294/1000 | Loss: 0.00002342
Iteration 295/1000 | Loss: 0.00002342
Iteration 296/1000 | Loss: 0.00002342
Iteration 297/1000 | Loss: 0.00002341
Iteration 298/1000 | Loss: 0.00002341
Iteration 299/1000 | Loss: 0.00002341
Iteration 300/1000 | Loss: 0.00002341
Iteration 301/1000 | Loss: 0.00002341
Iteration 302/1000 | Loss: 0.00002341
Iteration 303/1000 | Loss: 0.00002341
Iteration 304/1000 | Loss: 0.00002341
Iteration 305/1000 | Loss: 0.00002340
Iteration 306/1000 | Loss: 0.00002340
Iteration 307/1000 | Loss: 0.00002340
Iteration 308/1000 | Loss: 0.00002340
Iteration 309/1000 | Loss: 0.00002340
Iteration 310/1000 | Loss: 0.00002340
Iteration 311/1000 | Loss: 0.00002340
Iteration 312/1000 | Loss: 0.00002340
Iteration 313/1000 | Loss: 0.00002340
Iteration 314/1000 | Loss: 0.00002339
Iteration 315/1000 | Loss: 0.00002339
Iteration 316/1000 | Loss: 0.00002339
Iteration 317/1000 | Loss: 0.00002339
Iteration 318/1000 | Loss: 0.00002339
Iteration 319/1000 | Loss: 0.00002339
Iteration 320/1000 | Loss: 0.00002339
Iteration 321/1000 | Loss: 0.00002339
Iteration 322/1000 | Loss: 0.00002339
Iteration 323/1000 | Loss: 0.00002339
Iteration 324/1000 | Loss: 0.00002339
Iteration 325/1000 | Loss: 0.00002339
Iteration 326/1000 | Loss: 0.00002339
Iteration 327/1000 | Loss: 0.00002339
Iteration 328/1000 | Loss: 0.00002339
Iteration 329/1000 | Loss: 0.00002339
Iteration 330/1000 | Loss: 0.00002339
Iteration 331/1000 | Loss: 0.00002339
Iteration 332/1000 | Loss: 0.00002339
Iteration 333/1000 | Loss: 0.00002339
Iteration 334/1000 | Loss: 0.00002339
Iteration 335/1000 | Loss: 0.00002339
Iteration 336/1000 | Loss: 0.00002339
Iteration 337/1000 | Loss: 0.00002339
Iteration 338/1000 | Loss: 0.00002339
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 338. Stopping optimization.
Last 5 losses: [2.3386064640362747e-05, 2.3386064640362747e-05, 2.3386064640362747e-05, 2.3386064640362747e-05, 2.3386064640362747e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3386064640362747e-05

Optimization complete. Final v2v error: 3.8469018936157227 mm

Highest mean error: 15.044744491577148 mm for frame 241

Lowest mean error: 3.3079421520233154 mm for frame 260

Saving results

Total time: 481.8160583972931
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_helen_posed_023/1086/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_helen_posed_023/1086.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_helen_posed_023/1086
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00837192
Iteration 2/25 | Loss: 0.00161852
Iteration 3/25 | Loss: 0.00137801
Iteration 4/25 | Loss: 0.00133841
Iteration 5/25 | Loss: 0.00132894
Iteration 6/25 | Loss: 0.00132154
Iteration 7/25 | Loss: 0.00131477
Iteration 8/25 | Loss: 0.00126387
Iteration 9/25 | Loss: 0.00125597
Iteration 10/25 | Loss: 0.00124865
Iteration 11/25 | Loss: 0.00125145
Iteration 12/25 | Loss: 0.00124877
Iteration 13/25 | Loss: 0.00124922
Iteration 14/25 | Loss: 0.00122432
Iteration 15/25 | Loss: 0.00122049
Iteration 16/25 | Loss: 0.00122007
Iteration 17/25 | Loss: 0.00121992
Iteration 18/25 | Loss: 0.00121987
Iteration 19/25 | Loss: 0.00121986
Iteration 20/25 | Loss: 0.00121986
Iteration 21/25 | Loss: 0.00121986
Iteration 22/25 | Loss: 0.00121986
Iteration 23/25 | Loss: 0.00121986
Iteration 24/25 | Loss: 0.00121986
Iteration 25/25 | Loss: 0.00121986

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.88914406
Iteration 2/25 | Loss: 0.00072281
Iteration 3/25 | Loss: 0.00072280
Iteration 4/25 | Loss: 0.00072280
Iteration 5/25 | Loss: 0.00072280
Iteration 6/25 | Loss: 0.00072280
Iteration 7/25 | Loss: 0.00072280
Iteration 8/25 | Loss: 0.00072280
Iteration 9/25 | Loss: 0.00072280
Iteration 10/25 | Loss: 0.00072280
Iteration 11/25 | Loss: 0.00072280
Iteration 12/25 | Loss: 0.00072280
Iteration 13/25 | Loss: 0.00072280
Iteration 14/25 | Loss: 0.00072280
Iteration 15/25 | Loss: 0.00072280
Iteration 16/25 | Loss: 0.00072280
Iteration 17/25 | Loss: 0.00072280
Iteration 18/25 | Loss: 0.00072280
Iteration 19/25 | Loss: 0.00072280
Iteration 20/25 | Loss: 0.00072280
Iteration 21/25 | Loss: 0.00072280
Iteration 22/25 | Loss: 0.00072280
Iteration 23/25 | Loss: 0.00072280
Iteration 24/25 | Loss: 0.00072280
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0007227969472296536, 0.0007227969472296536, 0.0007227969472296536, 0.0007227969472296536, 0.0007227969472296536]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007227969472296536

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00072280
Iteration 2/1000 | Loss: 0.00003269
Iteration 3/1000 | Loss: 0.00002463
Iteration 4/1000 | Loss: 0.00002207
Iteration 5/1000 | Loss: 0.00002116
Iteration 6/1000 | Loss: 0.00002070
Iteration 7/1000 | Loss: 0.00002010
Iteration 8/1000 | Loss: 0.00001969
Iteration 9/1000 | Loss: 0.00001935
Iteration 10/1000 | Loss: 0.00001903
Iteration 11/1000 | Loss: 0.00001881
Iteration 12/1000 | Loss: 0.00001875
Iteration 13/1000 | Loss: 0.00001872
Iteration 14/1000 | Loss: 0.00001872
Iteration 15/1000 | Loss: 0.00001872
Iteration 16/1000 | Loss: 0.00001871
Iteration 17/1000 | Loss: 0.00001868
Iteration 18/1000 | Loss: 0.00001867
Iteration 19/1000 | Loss: 0.00001860
Iteration 20/1000 | Loss: 0.00001858
Iteration 21/1000 | Loss: 0.00001858
Iteration 22/1000 | Loss: 0.00001857
Iteration 23/1000 | Loss: 0.00001857
Iteration 24/1000 | Loss: 0.00001856
Iteration 25/1000 | Loss: 0.00001850
Iteration 26/1000 | Loss: 0.00001846
Iteration 27/1000 | Loss: 0.00001846
Iteration 28/1000 | Loss: 0.00001846
Iteration 29/1000 | Loss: 0.00001843
Iteration 30/1000 | Loss: 0.00001843
Iteration 31/1000 | Loss: 0.00001843
Iteration 32/1000 | Loss: 0.00001843
Iteration 33/1000 | Loss: 0.00001843
Iteration 34/1000 | Loss: 0.00001843
Iteration 35/1000 | Loss: 0.00001843
Iteration 36/1000 | Loss: 0.00001843
Iteration 37/1000 | Loss: 0.00001841
Iteration 38/1000 | Loss: 0.00001841
Iteration 39/1000 | Loss: 0.00001841
Iteration 40/1000 | Loss: 0.00001841
Iteration 41/1000 | Loss: 0.00001840
Iteration 42/1000 | Loss: 0.00001840
Iteration 43/1000 | Loss: 0.00001839
Iteration 44/1000 | Loss: 0.00001839
Iteration 45/1000 | Loss: 0.00001838
Iteration 46/1000 | Loss: 0.00001838
Iteration 47/1000 | Loss: 0.00001837
Iteration 48/1000 | Loss: 0.00001837
Iteration 49/1000 | Loss: 0.00001837
Iteration 50/1000 | Loss: 0.00001837
Iteration 51/1000 | Loss: 0.00001837
Iteration 52/1000 | Loss: 0.00001836
Iteration 53/1000 | Loss: 0.00001836
Iteration 54/1000 | Loss: 0.00001836
Iteration 55/1000 | Loss: 0.00001836
Iteration 56/1000 | Loss: 0.00001836
Iteration 57/1000 | Loss: 0.00001836
Iteration 58/1000 | Loss: 0.00001836
Iteration 59/1000 | Loss: 0.00001836
Iteration 60/1000 | Loss: 0.00001835
Iteration 61/1000 | Loss: 0.00001835
Iteration 62/1000 | Loss: 0.00001835
Iteration 63/1000 | Loss: 0.00001835
Iteration 64/1000 | Loss: 0.00001835
Iteration 65/1000 | Loss: 0.00001835
Iteration 66/1000 | Loss: 0.00001835
Iteration 67/1000 | Loss: 0.00001835
Iteration 68/1000 | Loss: 0.00001835
Iteration 69/1000 | Loss: 0.00001834
Iteration 70/1000 | Loss: 0.00001834
Iteration 71/1000 | Loss: 0.00001833
Iteration 72/1000 | Loss: 0.00001833
Iteration 73/1000 | Loss: 0.00001833
Iteration 74/1000 | Loss: 0.00001832
Iteration 75/1000 | Loss: 0.00001832
Iteration 76/1000 | Loss: 0.00001831
Iteration 77/1000 | Loss: 0.00001831
Iteration 78/1000 | Loss: 0.00001831
Iteration 79/1000 | Loss: 0.00001830
Iteration 80/1000 | Loss: 0.00001830
Iteration 81/1000 | Loss: 0.00001830
Iteration 82/1000 | Loss: 0.00001830
Iteration 83/1000 | Loss: 0.00001830
Iteration 84/1000 | Loss: 0.00001830
Iteration 85/1000 | Loss: 0.00001830
Iteration 86/1000 | Loss: 0.00001830
Iteration 87/1000 | Loss: 0.00001830
Iteration 88/1000 | Loss: 0.00001830
Iteration 89/1000 | Loss: 0.00001830
Iteration 90/1000 | Loss: 0.00001830
Iteration 91/1000 | Loss: 0.00001830
Iteration 92/1000 | Loss: 0.00001829
Iteration 93/1000 | Loss: 0.00001829
Iteration 94/1000 | Loss: 0.00001829
Iteration 95/1000 | Loss: 0.00001828
Iteration 96/1000 | Loss: 0.00001828
Iteration 97/1000 | Loss: 0.00001828
Iteration 98/1000 | Loss: 0.00001828
Iteration 99/1000 | Loss: 0.00001828
Iteration 100/1000 | Loss: 0.00001828
Iteration 101/1000 | Loss: 0.00001828
Iteration 102/1000 | Loss: 0.00001828
Iteration 103/1000 | Loss: 0.00001828
Iteration 104/1000 | Loss: 0.00001828
Iteration 105/1000 | Loss: 0.00001828
Iteration 106/1000 | Loss: 0.00001828
Iteration 107/1000 | Loss: 0.00001828
Iteration 108/1000 | Loss: 0.00001828
Iteration 109/1000 | Loss: 0.00001828
Iteration 110/1000 | Loss: 0.00001828
Iteration 111/1000 | Loss: 0.00001828
Iteration 112/1000 | Loss: 0.00001828
Iteration 113/1000 | Loss: 0.00001827
Iteration 114/1000 | Loss: 0.00001827
Iteration 115/1000 | Loss: 0.00001827
Iteration 116/1000 | Loss: 0.00001827
Iteration 117/1000 | Loss: 0.00001827
Iteration 118/1000 | Loss: 0.00001827
Iteration 119/1000 | Loss: 0.00001827
Iteration 120/1000 | Loss: 0.00001827
Iteration 121/1000 | Loss: 0.00001827
Iteration 122/1000 | Loss: 0.00001827
Iteration 123/1000 | Loss: 0.00001827
Iteration 124/1000 | Loss: 0.00001827
Iteration 125/1000 | Loss: 0.00001827
Iteration 126/1000 | Loss: 0.00001827
Iteration 127/1000 | Loss: 0.00001827
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 127. Stopping optimization.
Last 5 losses: [1.8273065506946295e-05, 1.8273065506946295e-05, 1.8273065506946295e-05, 1.8273065506946295e-05, 1.8273065506946295e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8273065506946295e-05

Optimization complete. Final v2v error: 3.6289443969726562 mm

Highest mean error: 3.8010854721069336 mm for frame 20

Lowest mean error: 3.510359525680542 mm for frame 0

Saving results

Total time: 56.179317235946655
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_helen_posed_023/1010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_helen_posed_023/1010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_helen_posed_023/1010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00815410
Iteration 2/25 | Loss: 0.00155284
Iteration 3/25 | Loss: 0.00135500
Iteration 4/25 | Loss: 0.00133687
Iteration 5/25 | Loss: 0.00133240
Iteration 6/25 | Loss: 0.00133189
Iteration 7/25 | Loss: 0.00133189
Iteration 8/25 | Loss: 0.00133189
Iteration 9/25 | Loss: 0.00133189
Iteration 10/25 | Loss: 0.00133189
Iteration 11/25 | Loss: 0.00133189
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001331886276602745, 0.001331886276602745, 0.001331886276602745, 0.001331886276602745, 0.001331886276602745]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001331886276602745

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.09784710
Iteration 2/25 | Loss: 0.00135571
Iteration 3/25 | Loss: 0.00135569
Iteration 4/25 | Loss: 0.00135569
Iteration 5/25 | Loss: 0.00135568
Iteration 6/25 | Loss: 0.00135568
Iteration 7/25 | Loss: 0.00135568
Iteration 8/25 | Loss: 0.00135568
Iteration 9/25 | Loss: 0.00135568
Iteration 10/25 | Loss: 0.00135568
Iteration 11/25 | Loss: 0.00135568
Iteration 12/25 | Loss: 0.00135568
Iteration 13/25 | Loss: 0.00135568
Iteration 14/25 | Loss: 0.00135568
Iteration 15/25 | Loss: 0.00135568
Iteration 16/25 | Loss: 0.00135568
Iteration 17/25 | Loss: 0.00135568
Iteration 18/25 | Loss: 0.00135568
Iteration 19/25 | Loss: 0.00135568
Iteration 20/25 | Loss: 0.00135568
Iteration 21/25 | Loss: 0.00135568
Iteration 22/25 | Loss: 0.00135568
Iteration 23/25 | Loss: 0.00135568
Iteration 24/25 | Loss: 0.00135568
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.001355682616122067, 0.001355682616122067, 0.001355682616122067, 0.001355682616122067, 0.001355682616122067]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001355682616122067

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00135568
Iteration 2/1000 | Loss: 0.00005419
Iteration 3/1000 | Loss: 0.00003631
Iteration 4/1000 | Loss: 0.00002772
Iteration 5/1000 | Loss: 0.00002557
Iteration 6/1000 | Loss: 0.00002456
Iteration 7/1000 | Loss: 0.00002381
Iteration 8/1000 | Loss: 0.00002340
Iteration 9/1000 | Loss: 0.00002296
Iteration 10/1000 | Loss: 0.00002276
Iteration 11/1000 | Loss: 0.00002260
Iteration 12/1000 | Loss: 0.00002239
Iteration 13/1000 | Loss: 0.00002221
Iteration 14/1000 | Loss: 0.00002214
Iteration 15/1000 | Loss: 0.00002214
Iteration 16/1000 | Loss: 0.00002201
Iteration 17/1000 | Loss: 0.00002199
Iteration 18/1000 | Loss: 0.00002198
Iteration 19/1000 | Loss: 0.00002197
Iteration 20/1000 | Loss: 0.00002189
Iteration 21/1000 | Loss: 0.00002186
Iteration 22/1000 | Loss: 0.00002185
Iteration 23/1000 | Loss: 0.00002185
Iteration 24/1000 | Loss: 0.00002184
Iteration 25/1000 | Loss: 0.00002183
Iteration 26/1000 | Loss: 0.00002183
Iteration 27/1000 | Loss: 0.00002182
Iteration 28/1000 | Loss: 0.00002182
Iteration 29/1000 | Loss: 0.00002182
Iteration 30/1000 | Loss: 0.00002181
Iteration 31/1000 | Loss: 0.00002181
Iteration 32/1000 | Loss: 0.00002180
Iteration 33/1000 | Loss: 0.00002180
Iteration 34/1000 | Loss: 0.00002180
Iteration 35/1000 | Loss: 0.00002179
Iteration 36/1000 | Loss: 0.00002179
Iteration 37/1000 | Loss: 0.00002179
Iteration 38/1000 | Loss: 0.00002179
Iteration 39/1000 | Loss: 0.00002178
Iteration 40/1000 | Loss: 0.00002178
Iteration 41/1000 | Loss: 0.00002178
Iteration 42/1000 | Loss: 0.00002178
Iteration 43/1000 | Loss: 0.00002177
Iteration 44/1000 | Loss: 0.00002176
Iteration 45/1000 | Loss: 0.00002176
Iteration 46/1000 | Loss: 0.00002176
Iteration 47/1000 | Loss: 0.00002175
Iteration 48/1000 | Loss: 0.00002175
Iteration 49/1000 | Loss: 0.00002175
Iteration 50/1000 | Loss: 0.00002174
Iteration 51/1000 | Loss: 0.00002172
Iteration 52/1000 | Loss: 0.00002172
Iteration 53/1000 | Loss: 0.00002171
Iteration 54/1000 | Loss: 0.00002168
Iteration 55/1000 | Loss: 0.00002168
Iteration 56/1000 | Loss: 0.00002166
Iteration 57/1000 | Loss: 0.00002166
Iteration 58/1000 | Loss: 0.00002166
Iteration 59/1000 | Loss: 0.00002165
Iteration 60/1000 | Loss: 0.00002165
Iteration 61/1000 | Loss: 0.00002165
Iteration 62/1000 | Loss: 0.00002164
Iteration 63/1000 | Loss: 0.00002164
Iteration 64/1000 | Loss: 0.00002164
Iteration 65/1000 | Loss: 0.00002163
Iteration 66/1000 | Loss: 0.00002163
Iteration 67/1000 | Loss: 0.00002163
Iteration 68/1000 | Loss: 0.00002162
Iteration 69/1000 | Loss: 0.00002162
Iteration 70/1000 | Loss: 0.00002162
Iteration 71/1000 | Loss: 0.00002162
Iteration 72/1000 | Loss: 0.00002162
Iteration 73/1000 | Loss: 0.00002162
Iteration 74/1000 | Loss: 0.00002162
Iteration 75/1000 | Loss: 0.00002162
Iteration 76/1000 | Loss: 0.00002160
Iteration 77/1000 | Loss: 0.00002160
Iteration 78/1000 | Loss: 0.00002160
Iteration 79/1000 | Loss: 0.00002159
Iteration 80/1000 | Loss: 0.00002159
Iteration 81/1000 | Loss: 0.00002159
Iteration 82/1000 | Loss: 0.00002159
Iteration 83/1000 | Loss: 0.00002159
Iteration 84/1000 | Loss: 0.00002159
Iteration 85/1000 | Loss: 0.00002159
Iteration 86/1000 | Loss: 0.00002159
Iteration 87/1000 | Loss: 0.00002159
Iteration 88/1000 | Loss: 0.00002159
Iteration 89/1000 | Loss: 0.00002159
Iteration 90/1000 | Loss: 0.00002159
Iteration 91/1000 | Loss: 0.00002159
Iteration 92/1000 | Loss: 0.00002158
Iteration 93/1000 | Loss: 0.00002158
Iteration 94/1000 | Loss: 0.00002157
Iteration 95/1000 | Loss: 0.00002157
Iteration 96/1000 | Loss: 0.00002157
Iteration 97/1000 | Loss: 0.00002156
Iteration 98/1000 | Loss: 0.00002156
Iteration 99/1000 | Loss: 0.00002156
Iteration 100/1000 | Loss: 0.00002156
Iteration 101/1000 | Loss: 0.00002156
Iteration 102/1000 | Loss: 0.00002156
Iteration 103/1000 | Loss: 0.00002156
Iteration 104/1000 | Loss: 0.00002155
Iteration 105/1000 | Loss: 0.00002155
Iteration 106/1000 | Loss: 0.00002155
Iteration 107/1000 | Loss: 0.00002155
Iteration 108/1000 | Loss: 0.00002154
Iteration 109/1000 | Loss: 0.00002154
Iteration 110/1000 | Loss: 0.00002154
Iteration 111/1000 | Loss: 0.00002154
Iteration 112/1000 | Loss: 0.00002153
Iteration 113/1000 | Loss: 0.00002153
Iteration 114/1000 | Loss: 0.00002153
Iteration 115/1000 | Loss: 0.00002152
Iteration 116/1000 | Loss: 0.00002152
Iteration 117/1000 | Loss: 0.00002152
Iteration 118/1000 | Loss: 0.00002152
Iteration 119/1000 | Loss: 0.00002151
Iteration 120/1000 | Loss: 0.00002150
Iteration 121/1000 | Loss: 0.00002149
Iteration 122/1000 | Loss: 0.00002149
Iteration 123/1000 | Loss: 0.00002149
Iteration 124/1000 | Loss: 0.00002149
Iteration 125/1000 | Loss: 0.00002149
Iteration 126/1000 | Loss: 0.00002148
Iteration 127/1000 | Loss: 0.00002148
Iteration 128/1000 | Loss: 0.00002148
Iteration 129/1000 | Loss: 0.00002148
Iteration 130/1000 | Loss: 0.00002148
Iteration 131/1000 | Loss: 0.00002148
Iteration 132/1000 | Loss: 0.00002148
Iteration 133/1000 | Loss: 0.00002148
Iteration 134/1000 | Loss: 0.00002148
Iteration 135/1000 | Loss: 0.00002148
Iteration 136/1000 | Loss: 0.00002148
Iteration 137/1000 | Loss: 0.00002148
Iteration 138/1000 | Loss: 0.00002148
Iteration 139/1000 | Loss: 0.00002148
Iteration 140/1000 | Loss: 0.00002148
Iteration 141/1000 | Loss: 0.00002148
Iteration 142/1000 | Loss: 0.00002148
Iteration 143/1000 | Loss: 0.00002148
Iteration 144/1000 | Loss: 0.00002148
Iteration 145/1000 | Loss: 0.00002147
Iteration 146/1000 | Loss: 0.00002147
Iteration 147/1000 | Loss: 0.00002147
Iteration 148/1000 | Loss: 0.00002146
Iteration 149/1000 | Loss: 0.00002146
Iteration 150/1000 | Loss: 0.00002146
Iteration 151/1000 | Loss: 0.00002146
Iteration 152/1000 | Loss: 0.00002146
Iteration 153/1000 | Loss: 0.00002146
Iteration 154/1000 | Loss: 0.00002146
Iteration 155/1000 | Loss: 0.00002145
Iteration 156/1000 | Loss: 0.00002145
Iteration 157/1000 | Loss: 0.00002145
Iteration 158/1000 | Loss: 0.00002145
Iteration 159/1000 | Loss: 0.00002145
Iteration 160/1000 | Loss: 0.00002145
Iteration 161/1000 | Loss: 0.00002145
Iteration 162/1000 | Loss: 0.00002144
Iteration 163/1000 | Loss: 0.00002144
Iteration 164/1000 | Loss: 0.00002144
Iteration 165/1000 | Loss: 0.00002144
Iteration 166/1000 | Loss: 0.00002144
Iteration 167/1000 | Loss: 0.00002144
Iteration 168/1000 | Loss: 0.00002144
Iteration 169/1000 | Loss: 0.00002143
Iteration 170/1000 | Loss: 0.00002143
Iteration 171/1000 | Loss: 0.00002143
Iteration 172/1000 | Loss: 0.00002143
Iteration 173/1000 | Loss: 0.00002143
Iteration 174/1000 | Loss: 0.00002143
Iteration 175/1000 | Loss: 0.00002142
Iteration 176/1000 | Loss: 0.00002142
Iteration 177/1000 | Loss: 0.00002142
Iteration 178/1000 | Loss: 0.00002142
Iteration 179/1000 | Loss: 0.00002142
Iteration 180/1000 | Loss: 0.00002142
Iteration 181/1000 | Loss: 0.00002142
Iteration 182/1000 | Loss: 0.00002142
Iteration 183/1000 | Loss: 0.00002142
Iteration 184/1000 | Loss: 0.00002142
Iteration 185/1000 | Loss: 0.00002142
Iteration 186/1000 | Loss: 0.00002141
Iteration 187/1000 | Loss: 0.00002141
Iteration 188/1000 | Loss: 0.00002141
Iteration 189/1000 | Loss: 0.00002141
Iteration 190/1000 | Loss: 0.00002141
Iteration 191/1000 | Loss: 0.00002140
Iteration 192/1000 | Loss: 0.00002140
Iteration 193/1000 | Loss: 0.00002140
Iteration 194/1000 | Loss: 0.00002140
Iteration 195/1000 | Loss: 0.00002140
Iteration 196/1000 | Loss: 0.00002140
Iteration 197/1000 | Loss: 0.00002140
Iteration 198/1000 | Loss: 0.00002140
Iteration 199/1000 | Loss: 0.00002140
Iteration 200/1000 | Loss: 0.00002140
Iteration 201/1000 | Loss: 0.00002140
Iteration 202/1000 | Loss: 0.00002139
Iteration 203/1000 | Loss: 0.00002139
Iteration 204/1000 | Loss: 0.00002139
Iteration 205/1000 | Loss: 0.00002139
Iteration 206/1000 | Loss: 0.00002139
Iteration 207/1000 | Loss: 0.00002139
Iteration 208/1000 | Loss: 0.00002139
Iteration 209/1000 | Loss: 0.00002138
Iteration 210/1000 | Loss: 0.00002138
Iteration 211/1000 | Loss: 0.00002138
Iteration 212/1000 | Loss: 0.00002138
Iteration 213/1000 | Loss: 0.00002138
Iteration 214/1000 | Loss: 0.00002138
Iteration 215/1000 | Loss: 0.00002138
Iteration 216/1000 | Loss: 0.00002138
Iteration 217/1000 | Loss: 0.00002138
Iteration 218/1000 | Loss: 0.00002138
Iteration 219/1000 | Loss: 0.00002137
Iteration 220/1000 | Loss: 0.00002137
Iteration 221/1000 | Loss: 0.00002137
Iteration 222/1000 | Loss: 0.00002137
Iteration 223/1000 | Loss: 0.00002137
Iteration 224/1000 | Loss: 0.00002137
Iteration 225/1000 | Loss: 0.00002137
Iteration 226/1000 | Loss: 0.00002137
Iteration 227/1000 | Loss: 0.00002137
Iteration 228/1000 | Loss: 0.00002137
Iteration 229/1000 | Loss: 0.00002137
Iteration 230/1000 | Loss: 0.00002137
Iteration 231/1000 | Loss: 0.00002137
Iteration 232/1000 | Loss: 0.00002137
Iteration 233/1000 | Loss: 0.00002136
Iteration 234/1000 | Loss: 0.00002136
Iteration 235/1000 | Loss: 0.00002136
Iteration 236/1000 | Loss: 0.00002136
Iteration 237/1000 | Loss: 0.00002136
Iteration 238/1000 | Loss: 0.00002136
Iteration 239/1000 | Loss: 0.00002136
Iteration 240/1000 | Loss: 0.00002136
Iteration 241/1000 | Loss: 0.00002135
Iteration 242/1000 | Loss: 0.00002135
Iteration 243/1000 | Loss: 0.00002135
Iteration 244/1000 | Loss: 0.00002135
Iteration 245/1000 | Loss: 0.00002135
Iteration 246/1000 | Loss: 0.00002135
Iteration 247/1000 | Loss: 0.00002135
Iteration 248/1000 | Loss: 0.00002135
Iteration 249/1000 | Loss: 0.00002134
Iteration 250/1000 | Loss: 0.00002134
Iteration 251/1000 | Loss: 0.00002134
Iteration 252/1000 | Loss: 0.00002134
Iteration 253/1000 | Loss: 0.00002134
Iteration 254/1000 | Loss: 0.00002134
Iteration 255/1000 | Loss: 0.00002134
Iteration 256/1000 | Loss: 0.00002134
Iteration 257/1000 | Loss: 0.00002134
Iteration 258/1000 | Loss: 0.00002134
Iteration 259/1000 | Loss: 0.00002134
Iteration 260/1000 | Loss: 0.00002134
Iteration 261/1000 | Loss: 0.00002134
Iteration 262/1000 | Loss: 0.00002134
Iteration 263/1000 | Loss: 0.00002134
Iteration 264/1000 | Loss: 0.00002134
Iteration 265/1000 | Loss: 0.00002134
Iteration 266/1000 | Loss: 0.00002134
Iteration 267/1000 | Loss: 0.00002134
Iteration 268/1000 | Loss: 0.00002134
Iteration 269/1000 | Loss: 0.00002134
Iteration 270/1000 | Loss: 0.00002134
Iteration 271/1000 | Loss: 0.00002134
Iteration 272/1000 | Loss: 0.00002134
Iteration 273/1000 | Loss: 0.00002134
Iteration 274/1000 | Loss: 0.00002134
Iteration 275/1000 | Loss: 0.00002134
Iteration 276/1000 | Loss: 0.00002134
Iteration 277/1000 | Loss: 0.00002134
Iteration 278/1000 | Loss: 0.00002134
Iteration 279/1000 | Loss: 0.00002134
Iteration 280/1000 | Loss: 0.00002134
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 280. Stopping optimization.
Last 5 losses: [2.1340843886719085e-05, 2.1340843886719085e-05, 2.1340843886719085e-05, 2.1340843886719085e-05, 2.1340843886719085e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1340843886719085e-05

Optimization complete. Final v2v error: 3.7134578227996826 mm

Highest mean error: 4.523257732391357 mm for frame 60

Lowest mean error: 3.1382946968078613 mm for frame 26

Saving results

Total time: 46.82775402069092
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_helen_posed_023/1007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_helen_posed_023/1007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_helen_posed_023/1007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01035639
Iteration 2/25 | Loss: 0.01035639
Iteration 3/25 | Loss: 0.00254401
Iteration 4/25 | Loss: 0.00191490
Iteration 5/25 | Loss: 0.00167524
Iteration 6/25 | Loss: 0.00156535
Iteration 7/25 | Loss: 0.00155323
Iteration 8/25 | Loss: 0.00142594
Iteration 9/25 | Loss: 0.00132074
Iteration 10/25 | Loss: 0.00128138
Iteration 11/25 | Loss: 0.00123687
Iteration 12/25 | Loss: 0.00121520
Iteration 13/25 | Loss: 0.00121019
Iteration 14/25 | Loss: 0.00120350
Iteration 15/25 | Loss: 0.00120383
Iteration 16/25 | Loss: 0.00120137
Iteration 17/25 | Loss: 0.00120087
Iteration 18/25 | Loss: 0.00120066
Iteration 19/25 | Loss: 0.00120056
Iteration 20/25 | Loss: 0.00120056
Iteration 21/25 | Loss: 0.00120055
Iteration 22/25 | Loss: 0.00120055
Iteration 23/25 | Loss: 0.00120055
Iteration 24/25 | Loss: 0.00120054
Iteration 25/25 | Loss: 0.00120054

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 6.65961599
Iteration 2/25 | Loss: 0.00146578
Iteration 3/25 | Loss: 0.00130164
Iteration 4/25 | Loss: 0.00130164
Iteration 5/25 | Loss: 0.00130164
Iteration 6/25 | Loss: 0.00130164
Iteration 7/25 | Loss: 0.00130164
Iteration 8/25 | Loss: 0.00130164
Iteration 9/25 | Loss: 0.00130163
Iteration 10/25 | Loss: 0.00130163
Iteration 11/25 | Loss: 0.00130163
Iteration 12/25 | Loss: 0.00130163
Iteration 13/25 | Loss: 0.00130163
Iteration 14/25 | Loss: 0.00130163
Iteration 15/25 | Loss: 0.00130163
Iteration 16/25 | Loss: 0.00130163
Iteration 17/25 | Loss: 0.00130163
Iteration 18/25 | Loss: 0.00130163
Iteration 19/25 | Loss: 0.00130163
Iteration 20/25 | Loss: 0.00130163
Iteration 21/25 | Loss: 0.00130163
Iteration 22/25 | Loss: 0.00130163
Iteration 23/25 | Loss: 0.00130163
Iteration 24/25 | Loss: 0.00130163
Iteration 25/25 | Loss: 0.00130163

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00130163
Iteration 2/1000 | Loss: 0.00005888
Iteration 3/1000 | Loss: 0.00002749
Iteration 4/1000 | Loss: 0.00012354
Iteration 5/1000 | Loss: 0.00004269
Iteration 6/1000 | Loss: 0.00022014
Iteration 7/1000 | Loss: 0.00015067
Iteration 8/1000 | Loss: 0.00002627
Iteration 9/1000 | Loss: 0.00031376
Iteration 10/1000 | Loss: 0.00002876
Iteration 11/1000 | Loss: 0.00003631
Iteration 12/1000 | Loss: 0.00004849
Iteration 13/1000 | Loss: 0.00002171
Iteration 14/1000 | Loss: 0.00003125
Iteration 15/1000 | Loss: 0.00002080
Iteration 16/1000 | Loss: 0.00131355
Iteration 17/1000 | Loss: 0.00052652
Iteration 18/1000 | Loss: 0.00001986
Iteration 19/1000 | Loss: 0.00017052
Iteration 20/1000 | Loss: 0.00010087
Iteration 21/1000 | Loss: 0.00003442
Iteration 22/1000 | Loss: 0.00002011
Iteration 23/1000 | Loss: 0.00003159
Iteration 24/1000 | Loss: 0.00006434
Iteration 25/1000 | Loss: 0.00003974
Iteration 26/1000 | Loss: 0.00002870
Iteration 27/1000 | Loss: 0.00001333
Iteration 28/1000 | Loss: 0.00007278
Iteration 29/1000 | Loss: 0.00005341
Iteration 30/1000 | Loss: 0.00001930
Iteration 31/1000 | Loss: 0.00001233
Iteration 32/1000 | Loss: 0.00003873
Iteration 33/1000 | Loss: 0.00011979
Iteration 34/1000 | Loss: 0.00002288
Iteration 35/1000 | Loss: 0.00004273
Iteration 36/1000 | Loss: 0.00001284
Iteration 37/1000 | Loss: 0.00002031
Iteration 38/1000 | Loss: 0.00001180
Iteration 39/1000 | Loss: 0.00001359
Iteration 40/1000 | Loss: 0.00001170
Iteration 41/1000 | Loss: 0.00001169
Iteration 42/1000 | Loss: 0.00001169
Iteration 43/1000 | Loss: 0.00001168
Iteration 44/1000 | Loss: 0.00001168
Iteration 45/1000 | Loss: 0.00001168
Iteration 46/1000 | Loss: 0.00001168
Iteration 47/1000 | Loss: 0.00001167
Iteration 48/1000 | Loss: 0.00005831
Iteration 49/1000 | Loss: 0.00004920
Iteration 50/1000 | Loss: 0.00005614
Iteration 51/1000 | Loss: 0.00001165
Iteration 52/1000 | Loss: 0.00001159
Iteration 53/1000 | Loss: 0.00001158
Iteration 54/1000 | Loss: 0.00001156
Iteration 55/1000 | Loss: 0.00001156
Iteration 56/1000 | Loss: 0.00001156
Iteration 57/1000 | Loss: 0.00001156
Iteration 58/1000 | Loss: 0.00001156
Iteration 59/1000 | Loss: 0.00001155
Iteration 60/1000 | Loss: 0.00001155
Iteration 61/1000 | Loss: 0.00001155
Iteration 62/1000 | Loss: 0.00001155
Iteration 63/1000 | Loss: 0.00001154
Iteration 64/1000 | Loss: 0.00001154
Iteration 65/1000 | Loss: 0.00001154
Iteration 66/1000 | Loss: 0.00001154
Iteration 67/1000 | Loss: 0.00001154
Iteration 68/1000 | Loss: 0.00001153
Iteration 69/1000 | Loss: 0.00001153
Iteration 70/1000 | Loss: 0.00001153
Iteration 71/1000 | Loss: 0.00001152
Iteration 72/1000 | Loss: 0.00001152
Iteration 73/1000 | Loss: 0.00001152
Iteration 74/1000 | Loss: 0.00001152
Iteration 75/1000 | Loss: 0.00001152
Iteration 76/1000 | Loss: 0.00001152
Iteration 77/1000 | Loss: 0.00001151
Iteration 78/1000 | Loss: 0.00001151
Iteration 79/1000 | Loss: 0.00001151
Iteration 80/1000 | Loss: 0.00001151
Iteration 81/1000 | Loss: 0.00001151
Iteration 82/1000 | Loss: 0.00001151
Iteration 83/1000 | Loss: 0.00001151
Iteration 84/1000 | Loss: 0.00001151
Iteration 85/1000 | Loss: 0.00001151
Iteration 86/1000 | Loss: 0.00001150
Iteration 87/1000 | Loss: 0.00001150
Iteration 88/1000 | Loss: 0.00001150
Iteration 89/1000 | Loss: 0.00001150
Iteration 90/1000 | Loss: 0.00001150
Iteration 91/1000 | Loss: 0.00001150
Iteration 92/1000 | Loss: 0.00001150
Iteration 93/1000 | Loss: 0.00001150
Iteration 94/1000 | Loss: 0.00001150
Iteration 95/1000 | Loss: 0.00001150
Iteration 96/1000 | Loss: 0.00001150
Iteration 97/1000 | Loss: 0.00001150
Iteration 98/1000 | Loss: 0.00001149
Iteration 99/1000 | Loss: 0.00001149
Iteration 100/1000 | Loss: 0.00001149
Iteration 101/1000 | Loss: 0.00003336
Iteration 102/1000 | Loss: 0.00001152
Iteration 103/1000 | Loss: 0.00001151
Iteration 104/1000 | Loss: 0.00001151
Iteration 105/1000 | Loss: 0.00001149
Iteration 106/1000 | Loss: 0.00002932
Iteration 107/1000 | Loss: 0.00001467
Iteration 108/1000 | Loss: 0.00001549
Iteration 109/1000 | Loss: 0.00001148
Iteration 110/1000 | Loss: 0.00001148
Iteration 111/1000 | Loss: 0.00001148
Iteration 112/1000 | Loss: 0.00001148
Iteration 113/1000 | Loss: 0.00001148
Iteration 114/1000 | Loss: 0.00001148
Iteration 115/1000 | Loss: 0.00001148
Iteration 116/1000 | Loss: 0.00001147
Iteration 117/1000 | Loss: 0.00001147
Iteration 118/1000 | Loss: 0.00001147
Iteration 119/1000 | Loss: 0.00001146
Iteration 120/1000 | Loss: 0.00001146
Iteration 121/1000 | Loss: 0.00001146
Iteration 122/1000 | Loss: 0.00001146
Iteration 123/1000 | Loss: 0.00001334
Iteration 124/1000 | Loss: 0.00001185
Iteration 125/1000 | Loss: 0.00001184
Iteration 126/1000 | Loss: 0.00001390
Iteration 127/1000 | Loss: 0.00001146
Iteration 128/1000 | Loss: 0.00001146
Iteration 129/1000 | Loss: 0.00001146
Iteration 130/1000 | Loss: 0.00001146
Iteration 131/1000 | Loss: 0.00001146
Iteration 132/1000 | Loss: 0.00001146
Iteration 133/1000 | Loss: 0.00001146
Iteration 134/1000 | Loss: 0.00001146
Iteration 135/1000 | Loss: 0.00001146
Iteration 136/1000 | Loss: 0.00001146
Iteration 137/1000 | Loss: 0.00001146
Iteration 138/1000 | Loss: 0.00001146
Iteration 139/1000 | Loss: 0.00001146
Iteration 140/1000 | Loss: 0.00001146
Iteration 141/1000 | Loss: 0.00001146
Iteration 142/1000 | Loss: 0.00001146
Iteration 143/1000 | Loss: 0.00001146
Iteration 144/1000 | Loss: 0.00001146
Iteration 145/1000 | Loss: 0.00001146
Iteration 146/1000 | Loss: 0.00001146
Iteration 147/1000 | Loss: 0.00001146
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 147. Stopping optimization.
Last 5 losses: [1.1458777407824527e-05, 1.1458777407824527e-05, 1.1458777407824527e-05, 1.1458777407824527e-05, 1.1458777407824527e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1458777407824527e-05

Optimization complete. Final v2v error: 2.9213755130767822 mm

Highest mean error: 3.4172399044036865 mm for frame 89

Lowest mean error: 2.6697580814361572 mm for frame 24

Saving results

Total time: 111.51982235908508
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_helen_posed_023/1089/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_helen_posed_023/1089.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_helen_posed_023/1089
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00546362
Iteration 2/25 | Loss: 0.00126122
Iteration 3/25 | Loss: 0.00119243
Iteration 4/25 | Loss: 0.00118170
Iteration 5/25 | Loss: 0.00117791
Iteration 6/25 | Loss: 0.00117712
Iteration 7/25 | Loss: 0.00117712
Iteration 8/25 | Loss: 0.00117712
Iteration 9/25 | Loss: 0.00117712
Iteration 10/25 | Loss: 0.00117712
Iteration 11/25 | Loss: 0.00117712
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011771194403991103, 0.0011771194403991103, 0.0011771194403991103, 0.0011771194403991103, 0.0011771194403991103]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011771194403991103

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.78211999
Iteration 2/25 | Loss: 0.00120230
Iteration 3/25 | Loss: 0.00120230
Iteration 4/25 | Loss: 0.00120230
Iteration 5/25 | Loss: 0.00120230
Iteration 6/25 | Loss: 0.00120230
Iteration 7/25 | Loss: 0.00120229
Iteration 8/25 | Loss: 0.00120229
Iteration 9/25 | Loss: 0.00120229
Iteration 10/25 | Loss: 0.00120229
Iteration 11/25 | Loss: 0.00120229
Iteration 12/25 | Loss: 0.00120229
Iteration 13/25 | Loss: 0.00120229
Iteration 14/25 | Loss: 0.00120229
Iteration 15/25 | Loss: 0.00120229
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0012022941373288631, 0.0012022941373288631, 0.0012022941373288631, 0.0012022941373288631, 0.0012022941373288631]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012022941373288631

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00120229
Iteration 2/1000 | Loss: 0.00002365
Iteration 3/1000 | Loss: 0.00001701
Iteration 4/1000 | Loss: 0.00001492
Iteration 5/1000 | Loss: 0.00001384
Iteration 6/1000 | Loss: 0.00001327
Iteration 7/1000 | Loss: 0.00001281
Iteration 8/1000 | Loss: 0.00001232
Iteration 9/1000 | Loss: 0.00001205
Iteration 10/1000 | Loss: 0.00001172
Iteration 11/1000 | Loss: 0.00001164
Iteration 12/1000 | Loss: 0.00001152
Iteration 13/1000 | Loss: 0.00001134
Iteration 14/1000 | Loss: 0.00001127
Iteration 15/1000 | Loss: 0.00001126
Iteration 16/1000 | Loss: 0.00001125
Iteration 17/1000 | Loss: 0.00001118
Iteration 18/1000 | Loss: 0.00001112
Iteration 19/1000 | Loss: 0.00001109
Iteration 20/1000 | Loss: 0.00001107
Iteration 21/1000 | Loss: 0.00001107
Iteration 22/1000 | Loss: 0.00001106
Iteration 23/1000 | Loss: 0.00001105
Iteration 24/1000 | Loss: 0.00001104
Iteration 25/1000 | Loss: 0.00001095
Iteration 26/1000 | Loss: 0.00001095
Iteration 27/1000 | Loss: 0.00001095
Iteration 28/1000 | Loss: 0.00001093
Iteration 29/1000 | Loss: 0.00001092
Iteration 30/1000 | Loss: 0.00001090
Iteration 31/1000 | Loss: 0.00001085
Iteration 32/1000 | Loss: 0.00001085
Iteration 33/1000 | Loss: 0.00001084
Iteration 34/1000 | Loss: 0.00001084
Iteration 35/1000 | Loss: 0.00001083
Iteration 36/1000 | Loss: 0.00001082
Iteration 37/1000 | Loss: 0.00001081
Iteration 38/1000 | Loss: 0.00001081
Iteration 39/1000 | Loss: 0.00001080
Iteration 40/1000 | Loss: 0.00001076
Iteration 41/1000 | Loss: 0.00001076
Iteration 42/1000 | Loss: 0.00001075
Iteration 43/1000 | Loss: 0.00001074
Iteration 44/1000 | Loss: 0.00001074
Iteration 45/1000 | Loss: 0.00001073
Iteration 46/1000 | Loss: 0.00001073
Iteration 47/1000 | Loss: 0.00001072
Iteration 48/1000 | Loss: 0.00001072
Iteration 49/1000 | Loss: 0.00001071
Iteration 50/1000 | Loss: 0.00001070
Iteration 51/1000 | Loss: 0.00001070
Iteration 52/1000 | Loss: 0.00001070
Iteration 53/1000 | Loss: 0.00001069
Iteration 54/1000 | Loss: 0.00001069
Iteration 55/1000 | Loss: 0.00001069
Iteration 56/1000 | Loss: 0.00001068
Iteration 57/1000 | Loss: 0.00001066
Iteration 58/1000 | Loss: 0.00001066
Iteration 59/1000 | Loss: 0.00001066
Iteration 60/1000 | Loss: 0.00001066
Iteration 61/1000 | Loss: 0.00001065
Iteration 62/1000 | Loss: 0.00001065
Iteration 63/1000 | Loss: 0.00001064
Iteration 64/1000 | Loss: 0.00001064
Iteration 65/1000 | Loss: 0.00001064
Iteration 66/1000 | Loss: 0.00001064
Iteration 67/1000 | Loss: 0.00001064
Iteration 68/1000 | Loss: 0.00001064
Iteration 69/1000 | Loss: 0.00001064
Iteration 70/1000 | Loss: 0.00001063
Iteration 71/1000 | Loss: 0.00001063
Iteration 72/1000 | Loss: 0.00001063
Iteration 73/1000 | Loss: 0.00001063
Iteration 74/1000 | Loss: 0.00001063
Iteration 75/1000 | Loss: 0.00001062
Iteration 76/1000 | Loss: 0.00001061
Iteration 77/1000 | Loss: 0.00001061
Iteration 78/1000 | Loss: 0.00001061
Iteration 79/1000 | Loss: 0.00001061
Iteration 80/1000 | Loss: 0.00001061
Iteration 81/1000 | Loss: 0.00001061
Iteration 82/1000 | Loss: 0.00001061
Iteration 83/1000 | Loss: 0.00001061
Iteration 84/1000 | Loss: 0.00001061
Iteration 85/1000 | Loss: 0.00001061
Iteration 86/1000 | Loss: 0.00001061
Iteration 87/1000 | Loss: 0.00001061
Iteration 88/1000 | Loss: 0.00001060
Iteration 89/1000 | Loss: 0.00001060
Iteration 90/1000 | Loss: 0.00001060
Iteration 91/1000 | Loss: 0.00001060
Iteration 92/1000 | Loss: 0.00001060
Iteration 93/1000 | Loss: 0.00001060
Iteration 94/1000 | Loss: 0.00001059
Iteration 95/1000 | Loss: 0.00001059
Iteration 96/1000 | Loss: 0.00001059
Iteration 97/1000 | Loss: 0.00001059
Iteration 98/1000 | Loss: 0.00001059
Iteration 99/1000 | Loss: 0.00001059
Iteration 100/1000 | Loss: 0.00001058
Iteration 101/1000 | Loss: 0.00001058
Iteration 102/1000 | Loss: 0.00001057
Iteration 103/1000 | Loss: 0.00001057
Iteration 104/1000 | Loss: 0.00001057
Iteration 105/1000 | Loss: 0.00001056
Iteration 106/1000 | Loss: 0.00001056
Iteration 107/1000 | Loss: 0.00001056
Iteration 108/1000 | Loss: 0.00001056
Iteration 109/1000 | Loss: 0.00001056
Iteration 110/1000 | Loss: 0.00001056
Iteration 111/1000 | Loss: 0.00001056
Iteration 112/1000 | Loss: 0.00001056
Iteration 113/1000 | Loss: 0.00001055
Iteration 114/1000 | Loss: 0.00001055
Iteration 115/1000 | Loss: 0.00001055
Iteration 116/1000 | Loss: 0.00001055
Iteration 117/1000 | Loss: 0.00001055
Iteration 118/1000 | Loss: 0.00001055
Iteration 119/1000 | Loss: 0.00001054
Iteration 120/1000 | Loss: 0.00001054
Iteration 121/1000 | Loss: 0.00001054
Iteration 122/1000 | Loss: 0.00001054
Iteration 123/1000 | Loss: 0.00001054
Iteration 124/1000 | Loss: 0.00001053
Iteration 125/1000 | Loss: 0.00001053
Iteration 126/1000 | Loss: 0.00001053
Iteration 127/1000 | Loss: 0.00001053
Iteration 128/1000 | Loss: 0.00001053
Iteration 129/1000 | Loss: 0.00001053
Iteration 130/1000 | Loss: 0.00001052
Iteration 131/1000 | Loss: 0.00001052
Iteration 132/1000 | Loss: 0.00001052
Iteration 133/1000 | Loss: 0.00001052
Iteration 134/1000 | Loss: 0.00001052
Iteration 135/1000 | Loss: 0.00001052
Iteration 136/1000 | Loss: 0.00001052
Iteration 137/1000 | Loss: 0.00001051
Iteration 138/1000 | Loss: 0.00001051
Iteration 139/1000 | Loss: 0.00001051
Iteration 140/1000 | Loss: 0.00001051
Iteration 141/1000 | Loss: 0.00001051
Iteration 142/1000 | Loss: 0.00001051
Iteration 143/1000 | Loss: 0.00001051
Iteration 144/1000 | Loss: 0.00001051
Iteration 145/1000 | Loss: 0.00001051
Iteration 146/1000 | Loss: 0.00001051
Iteration 147/1000 | Loss: 0.00001050
Iteration 148/1000 | Loss: 0.00001050
Iteration 149/1000 | Loss: 0.00001050
Iteration 150/1000 | Loss: 0.00001050
Iteration 151/1000 | Loss: 0.00001050
Iteration 152/1000 | Loss: 0.00001050
Iteration 153/1000 | Loss: 0.00001050
Iteration 154/1000 | Loss: 0.00001050
Iteration 155/1000 | Loss: 0.00001050
Iteration 156/1000 | Loss: 0.00001050
Iteration 157/1000 | Loss: 0.00001050
Iteration 158/1000 | Loss: 0.00001050
Iteration 159/1000 | Loss: 0.00001050
Iteration 160/1000 | Loss: 0.00001050
Iteration 161/1000 | Loss: 0.00001050
Iteration 162/1000 | Loss: 0.00001050
Iteration 163/1000 | Loss: 0.00001050
Iteration 164/1000 | Loss: 0.00001049
Iteration 165/1000 | Loss: 0.00001049
Iteration 166/1000 | Loss: 0.00001049
Iteration 167/1000 | Loss: 0.00001049
Iteration 168/1000 | Loss: 0.00001049
Iteration 169/1000 | Loss: 0.00001049
Iteration 170/1000 | Loss: 0.00001049
Iteration 171/1000 | Loss: 0.00001049
Iteration 172/1000 | Loss: 0.00001049
Iteration 173/1000 | Loss: 0.00001049
Iteration 174/1000 | Loss: 0.00001049
Iteration 175/1000 | Loss: 0.00001049
Iteration 176/1000 | Loss: 0.00001049
Iteration 177/1000 | Loss: 0.00001048
Iteration 178/1000 | Loss: 0.00001048
Iteration 179/1000 | Loss: 0.00001048
Iteration 180/1000 | Loss: 0.00001048
Iteration 181/1000 | Loss: 0.00001048
Iteration 182/1000 | Loss: 0.00001048
Iteration 183/1000 | Loss: 0.00001048
Iteration 184/1000 | Loss: 0.00001048
Iteration 185/1000 | Loss: 0.00001048
Iteration 186/1000 | Loss: 0.00001048
Iteration 187/1000 | Loss: 0.00001048
Iteration 188/1000 | Loss: 0.00001048
Iteration 189/1000 | Loss: 0.00001047
Iteration 190/1000 | Loss: 0.00001047
Iteration 191/1000 | Loss: 0.00001047
Iteration 192/1000 | Loss: 0.00001047
Iteration 193/1000 | Loss: 0.00001047
Iteration 194/1000 | Loss: 0.00001047
Iteration 195/1000 | Loss: 0.00001047
Iteration 196/1000 | Loss: 0.00001047
Iteration 197/1000 | Loss: 0.00001047
Iteration 198/1000 | Loss: 0.00001047
Iteration 199/1000 | Loss: 0.00001047
Iteration 200/1000 | Loss: 0.00001047
Iteration 201/1000 | Loss: 0.00001047
Iteration 202/1000 | Loss: 0.00001047
Iteration 203/1000 | Loss: 0.00001047
Iteration 204/1000 | Loss: 0.00001047
Iteration 205/1000 | Loss: 0.00001047
Iteration 206/1000 | Loss: 0.00001047
Iteration 207/1000 | Loss: 0.00001046
Iteration 208/1000 | Loss: 0.00001046
Iteration 209/1000 | Loss: 0.00001046
Iteration 210/1000 | Loss: 0.00001046
Iteration 211/1000 | Loss: 0.00001046
Iteration 212/1000 | Loss: 0.00001046
Iteration 213/1000 | Loss: 0.00001046
Iteration 214/1000 | Loss: 0.00001046
Iteration 215/1000 | Loss: 0.00001046
Iteration 216/1000 | Loss: 0.00001046
Iteration 217/1000 | Loss: 0.00001046
Iteration 218/1000 | Loss: 0.00001046
Iteration 219/1000 | Loss: 0.00001045
Iteration 220/1000 | Loss: 0.00001045
Iteration 221/1000 | Loss: 0.00001045
Iteration 222/1000 | Loss: 0.00001045
Iteration 223/1000 | Loss: 0.00001045
Iteration 224/1000 | Loss: 0.00001045
Iteration 225/1000 | Loss: 0.00001045
Iteration 226/1000 | Loss: 0.00001045
Iteration 227/1000 | Loss: 0.00001045
Iteration 228/1000 | Loss: 0.00001045
Iteration 229/1000 | Loss: 0.00001045
Iteration 230/1000 | Loss: 0.00001045
Iteration 231/1000 | Loss: 0.00001045
Iteration 232/1000 | Loss: 0.00001045
Iteration 233/1000 | Loss: 0.00001045
Iteration 234/1000 | Loss: 0.00001045
Iteration 235/1000 | Loss: 0.00001045
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 235. Stopping optimization.
Last 5 losses: [1.0453889444761444e-05, 1.0453889444761444e-05, 1.0453889444761444e-05, 1.0453889444761444e-05, 1.0453889444761444e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0453889444761444e-05

Optimization complete. Final v2v error: 2.8106558322906494 mm

Highest mean error: 3.1722464561462402 mm for frame 137

Lowest mean error: 2.626725196838379 mm for frame 155

Saving results

Total time: 44.902743101119995
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_helen_posed_023/1036/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_helen_posed_023/1036.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_helen_posed_023/1036
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00639499
Iteration 2/25 | Loss: 0.00141994
Iteration 3/25 | Loss: 0.00128646
Iteration 4/25 | Loss: 0.00126098
Iteration 5/25 | Loss: 0.00125931
Iteration 6/25 | Loss: 0.00125709
Iteration 7/25 | Loss: 0.00124482
Iteration 8/25 | Loss: 0.00123635
Iteration 9/25 | Loss: 0.00123395
Iteration 10/25 | Loss: 0.00123135
Iteration 11/25 | Loss: 0.00123055
Iteration 12/25 | Loss: 0.00123043
Iteration 13/25 | Loss: 0.00123041
Iteration 14/25 | Loss: 0.00123041
Iteration 15/25 | Loss: 0.00123041
Iteration 16/25 | Loss: 0.00123040
Iteration 17/25 | Loss: 0.00123040
Iteration 18/25 | Loss: 0.00123040
Iteration 19/25 | Loss: 0.00123040
Iteration 20/25 | Loss: 0.00123040
Iteration 21/25 | Loss: 0.00123040
Iteration 22/25 | Loss: 0.00123040
Iteration 23/25 | Loss: 0.00123040
Iteration 24/25 | Loss: 0.00123040
Iteration 25/25 | Loss: 0.00123039

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34676254
Iteration 2/25 | Loss: 0.00148730
Iteration 3/25 | Loss: 0.00148730
Iteration 4/25 | Loss: 0.00148730
Iteration 5/25 | Loss: 0.00148730
Iteration 6/25 | Loss: 0.00148730
Iteration 7/25 | Loss: 0.00148730
Iteration 8/25 | Loss: 0.00148730
Iteration 9/25 | Loss: 0.00148730
Iteration 10/25 | Loss: 0.00148730
Iteration 11/25 | Loss: 0.00148730
Iteration 12/25 | Loss: 0.00148730
Iteration 13/25 | Loss: 0.00148730
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.00148729735519737, 0.00148729735519737, 0.00148729735519737, 0.00148729735519737, 0.00148729735519737]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00148729735519737

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00148730
Iteration 2/1000 | Loss: 0.00005773
Iteration 3/1000 | Loss: 0.00003904
Iteration 4/1000 | Loss: 0.00003348
Iteration 5/1000 | Loss: 0.00003106
Iteration 6/1000 | Loss: 0.00002934
Iteration 7/1000 | Loss: 0.00002824
Iteration 8/1000 | Loss: 0.00002739
Iteration 9/1000 | Loss: 0.00002659
Iteration 10/1000 | Loss: 0.00002591
Iteration 11/1000 | Loss: 0.00008244
Iteration 12/1000 | Loss: 0.00002537
Iteration 13/1000 | Loss: 0.00002385
Iteration 14/1000 | Loss: 0.00002322
Iteration 15/1000 | Loss: 0.00002263
Iteration 16/1000 | Loss: 0.00002212
Iteration 17/1000 | Loss: 0.00002181
Iteration 18/1000 | Loss: 0.00002159
Iteration 19/1000 | Loss: 0.00002137
Iteration 20/1000 | Loss: 0.00002132
Iteration 21/1000 | Loss: 0.00002113
Iteration 22/1000 | Loss: 0.00002109
Iteration 23/1000 | Loss: 0.00002108
Iteration 24/1000 | Loss: 0.00002097
Iteration 25/1000 | Loss: 0.00002095
Iteration 26/1000 | Loss: 0.00002082
Iteration 27/1000 | Loss: 0.00002082
Iteration 28/1000 | Loss: 0.00002081
Iteration 29/1000 | Loss: 0.00002079
Iteration 30/1000 | Loss: 0.00002075
Iteration 31/1000 | Loss: 0.00002074
Iteration 32/1000 | Loss: 0.00002070
Iteration 33/1000 | Loss: 0.00002069
Iteration 34/1000 | Loss: 0.00002064
Iteration 35/1000 | Loss: 0.00002063
Iteration 36/1000 | Loss: 0.00002063
Iteration 37/1000 | Loss: 0.00002059
Iteration 38/1000 | Loss: 0.00002059
Iteration 39/1000 | Loss: 0.00002056
Iteration 40/1000 | Loss: 0.00002056
Iteration 41/1000 | Loss: 0.00002055
Iteration 42/1000 | Loss: 0.00002055
Iteration 43/1000 | Loss: 0.00002054
Iteration 44/1000 | Loss: 0.00002054
Iteration 45/1000 | Loss: 0.00002053
Iteration 46/1000 | Loss: 0.00002053
Iteration 47/1000 | Loss: 0.00002052
Iteration 48/1000 | Loss: 0.00002052
Iteration 49/1000 | Loss: 0.00002052
Iteration 50/1000 | Loss: 0.00002051
Iteration 51/1000 | Loss: 0.00002051
Iteration 52/1000 | Loss: 0.00002050
Iteration 53/1000 | Loss: 0.00002050
Iteration 54/1000 | Loss: 0.00048287
Iteration 55/1000 | Loss: 0.00002183
Iteration 56/1000 | Loss: 0.00002021
Iteration 57/1000 | Loss: 0.00001961
Iteration 58/1000 | Loss: 0.00001932
Iteration 59/1000 | Loss: 0.00001909
Iteration 60/1000 | Loss: 0.00001906
Iteration 61/1000 | Loss: 0.00001890
Iteration 62/1000 | Loss: 0.00001883
Iteration 63/1000 | Loss: 0.00001880
Iteration 64/1000 | Loss: 0.00001880
Iteration 65/1000 | Loss: 0.00001879
Iteration 66/1000 | Loss: 0.00001879
Iteration 67/1000 | Loss: 0.00001879
Iteration 68/1000 | Loss: 0.00001878
Iteration 69/1000 | Loss: 0.00001877
Iteration 70/1000 | Loss: 0.00001875
Iteration 71/1000 | Loss: 0.00001875
Iteration 72/1000 | Loss: 0.00001875
Iteration 73/1000 | Loss: 0.00001875
Iteration 74/1000 | Loss: 0.00001875
Iteration 75/1000 | Loss: 0.00001875
Iteration 76/1000 | Loss: 0.00001875
Iteration 77/1000 | Loss: 0.00001874
Iteration 78/1000 | Loss: 0.00001874
Iteration 79/1000 | Loss: 0.00001873
Iteration 80/1000 | Loss: 0.00001872
Iteration 81/1000 | Loss: 0.00001872
Iteration 82/1000 | Loss: 0.00001872
Iteration 83/1000 | Loss: 0.00001871
Iteration 84/1000 | Loss: 0.00001871
Iteration 85/1000 | Loss: 0.00001871
Iteration 86/1000 | Loss: 0.00001870
Iteration 87/1000 | Loss: 0.00001870
Iteration 88/1000 | Loss: 0.00001870
Iteration 89/1000 | Loss: 0.00001870
Iteration 90/1000 | Loss: 0.00001869
Iteration 91/1000 | Loss: 0.00001869
Iteration 92/1000 | Loss: 0.00001868
Iteration 93/1000 | Loss: 0.00001868
Iteration 94/1000 | Loss: 0.00001868
Iteration 95/1000 | Loss: 0.00001868
Iteration 96/1000 | Loss: 0.00001868
Iteration 97/1000 | Loss: 0.00001867
Iteration 98/1000 | Loss: 0.00001867
Iteration 99/1000 | Loss: 0.00001867
Iteration 100/1000 | Loss: 0.00001867
Iteration 101/1000 | Loss: 0.00001867
Iteration 102/1000 | Loss: 0.00001867
Iteration 103/1000 | Loss: 0.00001867
Iteration 104/1000 | Loss: 0.00001867
Iteration 105/1000 | Loss: 0.00001867
Iteration 106/1000 | Loss: 0.00001867
Iteration 107/1000 | Loss: 0.00001866
Iteration 108/1000 | Loss: 0.00001866
Iteration 109/1000 | Loss: 0.00001866
Iteration 110/1000 | Loss: 0.00001866
Iteration 111/1000 | Loss: 0.00001866
Iteration 112/1000 | Loss: 0.00001866
Iteration 113/1000 | Loss: 0.00001866
Iteration 114/1000 | Loss: 0.00001865
Iteration 115/1000 | Loss: 0.00001865
Iteration 116/1000 | Loss: 0.00001865
Iteration 117/1000 | Loss: 0.00001864
Iteration 118/1000 | Loss: 0.00001864
Iteration 119/1000 | Loss: 0.00001864
Iteration 120/1000 | Loss: 0.00001864
Iteration 121/1000 | Loss: 0.00001863
Iteration 122/1000 | Loss: 0.00001863
Iteration 123/1000 | Loss: 0.00001863
Iteration 124/1000 | Loss: 0.00001863
Iteration 125/1000 | Loss: 0.00001863
Iteration 126/1000 | Loss: 0.00001862
Iteration 127/1000 | Loss: 0.00001862
Iteration 128/1000 | Loss: 0.00001862
Iteration 129/1000 | Loss: 0.00001861
Iteration 130/1000 | Loss: 0.00001861
Iteration 131/1000 | Loss: 0.00001861
Iteration 132/1000 | Loss: 0.00001861
Iteration 133/1000 | Loss: 0.00001860
Iteration 134/1000 | Loss: 0.00001860
Iteration 135/1000 | Loss: 0.00001860
Iteration 136/1000 | Loss: 0.00001860
Iteration 137/1000 | Loss: 0.00001859
Iteration 138/1000 | Loss: 0.00001859
Iteration 139/1000 | Loss: 0.00001859
Iteration 140/1000 | Loss: 0.00001858
Iteration 141/1000 | Loss: 0.00001858
Iteration 142/1000 | Loss: 0.00001858
Iteration 143/1000 | Loss: 0.00001858
Iteration 144/1000 | Loss: 0.00001858
Iteration 145/1000 | Loss: 0.00001858
Iteration 146/1000 | Loss: 0.00001857
Iteration 147/1000 | Loss: 0.00001857
Iteration 148/1000 | Loss: 0.00001857
Iteration 149/1000 | Loss: 0.00001857
Iteration 150/1000 | Loss: 0.00001857
Iteration 151/1000 | Loss: 0.00001857
Iteration 152/1000 | Loss: 0.00001857
Iteration 153/1000 | Loss: 0.00001857
Iteration 154/1000 | Loss: 0.00001857
Iteration 155/1000 | Loss: 0.00001857
Iteration 156/1000 | Loss: 0.00001857
Iteration 157/1000 | Loss: 0.00001857
Iteration 158/1000 | Loss: 0.00001857
Iteration 159/1000 | Loss: 0.00001857
Iteration 160/1000 | Loss: 0.00001857
Iteration 161/1000 | Loss: 0.00001856
Iteration 162/1000 | Loss: 0.00001856
Iteration 163/1000 | Loss: 0.00001856
Iteration 164/1000 | Loss: 0.00001856
Iteration 165/1000 | Loss: 0.00001856
Iteration 166/1000 | Loss: 0.00001856
Iteration 167/1000 | Loss: 0.00001856
Iteration 168/1000 | Loss: 0.00001856
Iteration 169/1000 | Loss: 0.00001856
Iteration 170/1000 | Loss: 0.00001856
Iteration 171/1000 | Loss: 0.00001856
Iteration 172/1000 | Loss: 0.00001855
Iteration 173/1000 | Loss: 0.00001855
Iteration 174/1000 | Loss: 0.00001855
Iteration 175/1000 | Loss: 0.00001855
Iteration 176/1000 | Loss: 0.00001855
Iteration 177/1000 | Loss: 0.00001855
Iteration 178/1000 | Loss: 0.00001855
Iteration 179/1000 | Loss: 0.00001855
Iteration 180/1000 | Loss: 0.00001855
Iteration 181/1000 | Loss: 0.00001855
Iteration 182/1000 | Loss: 0.00001855
Iteration 183/1000 | Loss: 0.00001855
Iteration 184/1000 | Loss: 0.00001854
Iteration 185/1000 | Loss: 0.00001854
Iteration 186/1000 | Loss: 0.00001854
Iteration 187/1000 | Loss: 0.00001854
Iteration 188/1000 | Loss: 0.00001854
Iteration 189/1000 | Loss: 0.00001854
Iteration 190/1000 | Loss: 0.00001853
Iteration 191/1000 | Loss: 0.00001853
Iteration 192/1000 | Loss: 0.00001853
Iteration 193/1000 | Loss: 0.00001852
Iteration 194/1000 | Loss: 0.00001852
Iteration 195/1000 | Loss: 0.00001852
Iteration 196/1000 | Loss: 0.00001852
Iteration 197/1000 | Loss: 0.00001851
Iteration 198/1000 | Loss: 0.00001851
Iteration 199/1000 | Loss: 0.00001851
Iteration 200/1000 | Loss: 0.00001851
Iteration 201/1000 | Loss: 0.00001851
Iteration 202/1000 | Loss: 0.00001851
Iteration 203/1000 | Loss: 0.00001851
Iteration 204/1000 | Loss: 0.00001851
Iteration 205/1000 | Loss: 0.00001851
Iteration 206/1000 | Loss: 0.00001851
Iteration 207/1000 | Loss: 0.00001851
Iteration 208/1000 | Loss: 0.00001851
Iteration 209/1000 | Loss: 0.00001850
Iteration 210/1000 | Loss: 0.00001850
Iteration 211/1000 | Loss: 0.00001850
Iteration 212/1000 | Loss: 0.00001850
Iteration 213/1000 | Loss: 0.00001850
Iteration 214/1000 | Loss: 0.00001850
Iteration 215/1000 | Loss: 0.00001850
Iteration 216/1000 | Loss: 0.00001850
Iteration 217/1000 | Loss: 0.00001850
Iteration 218/1000 | Loss: 0.00001850
Iteration 219/1000 | Loss: 0.00001850
Iteration 220/1000 | Loss: 0.00001850
Iteration 221/1000 | Loss: 0.00001850
Iteration 222/1000 | Loss: 0.00001850
Iteration 223/1000 | Loss: 0.00001850
Iteration 224/1000 | Loss: 0.00001850
Iteration 225/1000 | Loss: 0.00001850
Iteration 226/1000 | Loss: 0.00001850
Iteration 227/1000 | Loss: 0.00001850
Iteration 228/1000 | Loss: 0.00001850
Iteration 229/1000 | Loss: 0.00001850
Iteration 230/1000 | Loss: 0.00001849
Iteration 231/1000 | Loss: 0.00001849
Iteration 232/1000 | Loss: 0.00001849
Iteration 233/1000 | Loss: 0.00001849
Iteration 234/1000 | Loss: 0.00001849
Iteration 235/1000 | Loss: 0.00001849
Iteration 236/1000 | Loss: 0.00001849
Iteration 237/1000 | Loss: 0.00001849
Iteration 238/1000 | Loss: 0.00001849
Iteration 239/1000 | Loss: 0.00001849
Iteration 240/1000 | Loss: 0.00001849
Iteration 241/1000 | Loss: 0.00001849
Iteration 242/1000 | Loss: 0.00001849
Iteration 243/1000 | Loss: 0.00001849
Iteration 244/1000 | Loss: 0.00001849
Iteration 245/1000 | Loss: 0.00001849
Iteration 246/1000 | Loss: 0.00001849
Iteration 247/1000 | Loss: 0.00001849
Iteration 248/1000 | Loss: 0.00001849
Iteration 249/1000 | Loss: 0.00001849
Iteration 250/1000 | Loss: 0.00001849
Iteration 251/1000 | Loss: 0.00001849
Iteration 252/1000 | Loss: 0.00001849
Iteration 253/1000 | Loss: 0.00001849
Iteration 254/1000 | Loss: 0.00001849
Iteration 255/1000 | Loss: 0.00001849
Iteration 256/1000 | Loss: 0.00001849
Iteration 257/1000 | Loss: 0.00001848
Iteration 258/1000 | Loss: 0.00001848
Iteration 259/1000 | Loss: 0.00001848
Iteration 260/1000 | Loss: 0.00001848
Iteration 261/1000 | Loss: 0.00001848
Iteration 262/1000 | Loss: 0.00001848
Iteration 263/1000 | Loss: 0.00001848
Iteration 264/1000 | Loss: 0.00001848
Iteration 265/1000 | Loss: 0.00001848
Iteration 266/1000 | Loss: 0.00001848
Iteration 267/1000 | Loss: 0.00001848
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 267. Stopping optimization.
Last 5 losses: [1.84846758202184e-05, 1.84846758202184e-05, 1.84846758202184e-05, 1.84846758202184e-05, 1.84846758202184e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.84846758202184e-05

Optimization complete. Final v2v error: 3.6477222442626953 mm

Highest mean error: 4.8886027336120605 mm for frame 63

Lowest mean error: 3.038170576095581 mm for frame 134

Saving results

Total time: 90.58027958869934
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_helen_posed_023/1054/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_helen_posed_023/1054.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_helen_posed_023/1054
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00880975
Iteration 2/25 | Loss: 0.00131288
Iteration 3/25 | Loss: 0.00122950
Iteration 4/25 | Loss: 0.00120653
Iteration 5/25 | Loss: 0.00120065
Iteration 6/25 | Loss: 0.00119955
Iteration 7/25 | Loss: 0.00119955
Iteration 8/25 | Loss: 0.00119955
Iteration 9/25 | Loss: 0.00119955
Iteration 10/25 | Loss: 0.00119955
Iteration 11/25 | Loss: 0.00119955
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001199551741592586, 0.001199551741592586, 0.001199551741592586, 0.001199551741592586, 0.001199551741592586]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001199551741592586

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.24899399
Iteration 2/25 | Loss: 0.00143602
Iteration 3/25 | Loss: 0.00143602
Iteration 4/25 | Loss: 0.00143602
Iteration 5/25 | Loss: 0.00143602
Iteration 6/25 | Loss: 0.00143602
Iteration 7/25 | Loss: 0.00143602
Iteration 8/25 | Loss: 0.00143602
Iteration 9/25 | Loss: 0.00143602
Iteration 10/25 | Loss: 0.00143602
Iteration 11/25 | Loss: 0.00143602
Iteration 12/25 | Loss: 0.00143602
Iteration 13/25 | Loss: 0.00143602
Iteration 14/25 | Loss: 0.00143602
Iteration 15/25 | Loss: 0.00143602
Iteration 16/25 | Loss: 0.00143602
Iteration 17/25 | Loss: 0.00143602
Iteration 18/25 | Loss: 0.00143602
Iteration 19/25 | Loss: 0.00143602
Iteration 20/25 | Loss: 0.00143602
Iteration 21/25 | Loss: 0.00143602
Iteration 22/25 | Loss: 0.00143602
Iteration 23/25 | Loss: 0.00143602
Iteration 24/25 | Loss: 0.00143602
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0014360191999003291, 0.0014360191999003291, 0.0014360191999003291, 0.0014360191999003291, 0.0014360191999003291]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014360191999003291

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00143602
Iteration 2/1000 | Loss: 0.00003619
Iteration 3/1000 | Loss: 0.00002463
Iteration 4/1000 | Loss: 0.00002141
Iteration 5/1000 | Loss: 0.00001957
Iteration 6/1000 | Loss: 0.00001794
Iteration 7/1000 | Loss: 0.00001707
Iteration 8/1000 | Loss: 0.00001656
Iteration 9/1000 | Loss: 0.00001632
Iteration 10/1000 | Loss: 0.00001609
Iteration 11/1000 | Loss: 0.00001608
Iteration 12/1000 | Loss: 0.00001606
Iteration 13/1000 | Loss: 0.00001599
Iteration 14/1000 | Loss: 0.00001598
Iteration 15/1000 | Loss: 0.00001597
Iteration 16/1000 | Loss: 0.00001596
Iteration 17/1000 | Loss: 0.00001586
Iteration 18/1000 | Loss: 0.00001585
Iteration 19/1000 | Loss: 0.00001585
Iteration 20/1000 | Loss: 0.00001583
Iteration 21/1000 | Loss: 0.00001582
Iteration 22/1000 | Loss: 0.00001582
Iteration 23/1000 | Loss: 0.00001576
Iteration 24/1000 | Loss: 0.00001575
Iteration 25/1000 | Loss: 0.00001574
Iteration 26/1000 | Loss: 0.00001574
Iteration 27/1000 | Loss: 0.00001569
Iteration 28/1000 | Loss: 0.00001569
Iteration 29/1000 | Loss: 0.00001569
Iteration 30/1000 | Loss: 0.00001569
Iteration 31/1000 | Loss: 0.00001569
Iteration 32/1000 | Loss: 0.00001568
Iteration 33/1000 | Loss: 0.00001566
Iteration 34/1000 | Loss: 0.00001565
Iteration 35/1000 | Loss: 0.00001565
Iteration 36/1000 | Loss: 0.00001563
Iteration 37/1000 | Loss: 0.00001563
Iteration 38/1000 | Loss: 0.00001562
Iteration 39/1000 | Loss: 0.00001562
Iteration 40/1000 | Loss: 0.00001561
Iteration 41/1000 | Loss: 0.00001561
Iteration 42/1000 | Loss: 0.00001561
Iteration 43/1000 | Loss: 0.00001561
Iteration 44/1000 | Loss: 0.00001561
Iteration 45/1000 | Loss: 0.00001561
Iteration 46/1000 | Loss: 0.00001560
Iteration 47/1000 | Loss: 0.00001560
Iteration 48/1000 | Loss: 0.00001560
Iteration 49/1000 | Loss: 0.00001559
Iteration 50/1000 | Loss: 0.00001556
Iteration 51/1000 | Loss: 0.00001556
Iteration 52/1000 | Loss: 0.00001556
Iteration 53/1000 | Loss: 0.00001555
Iteration 54/1000 | Loss: 0.00001555
Iteration 55/1000 | Loss: 0.00001554
Iteration 56/1000 | Loss: 0.00001554
Iteration 57/1000 | Loss: 0.00001552
Iteration 58/1000 | Loss: 0.00001551
Iteration 59/1000 | Loss: 0.00001551
Iteration 60/1000 | Loss: 0.00001551
Iteration 61/1000 | Loss: 0.00001550
Iteration 62/1000 | Loss: 0.00001550
Iteration 63/1000 | Loss: 0.00001550
Iteration 64/1000 | Loss: 0.00001550
Iteration 65/1000 | Loss: 0.00001550
Iteration 66/1000 | Loss: 0.00001549
Iteration 67/1000 | Loss: 0.00001549
Iteration 68/1000 | Loss: 0.00001549
Iteration 69/1000 | Loss: 0.00001549
Iteration 70/1000 | Loss: 0.00001549
Iteration 71/1000 | Loss: 0.00001549
Iteration 72/1000 | Loss: 0.00001549
Iteration 73/1000 | Loss: 0.00001549
Iteration 74/1000 | Loss: 0.00001549
Iteration 75/1000 | Loss: 0.00001549
Iteration 76/1000 | Loss: 0.00001548
Iteration 77/1000 | Loss: 0.00001548
Iteration 78/1000 | Loss: 0.00001548
Iteration 79/1000 | Loss: 0.00001548
Iteration 80/1000 | Loss: 0.00001547
Iteration 81/1000 | Loss: 0.00001547
Iteration 82/1000 | Loss: 0.00001547
Iteration 83/1000 | Loss: 0.00001547
Iteration 84/1000 | Loss: 0.00001547
Iteration 85/1000 | Loss: 0.00001546
Iteration 86/1000 | Loss: 0.00001546
Iteration 87/1000 | Loss: 0.00001546
Iteration 88/1000 | Loss: 0.00001545
Iteration 89/1000 | Loss: 0.00001545
Iteration 90/1000 | Loss: 0.00001545
Iteration 91/1000 | Loss: 0.00001545
Iteration 92/1000 | Loss: 0.00001545
Iteration 93/1000 | Loss: 0.00001545
Iteration 94/1000 | Loss: 0.00001545
Iteration 95/1000 | Loss: 0.00001544
Iteration 96/1000 | Loss: 0.00001544
Iteration 97/1000 | Loss: 0.00001544
Iteration 98/1000 | Loss: 0.00001544
Iteration 99/1000 | Loss: 0.00001544
Iteration 100/1000 | Loss: 0.00001544
Iteration 101/1000 | Loss: 0.00001544
Iteration 102/1000 | Loss: 0.00001544
Iteration 103/1000 | Loss: 0.00001544
Iteration 104/1000 | Loss: 0.00001544
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 104. Stopping optimization.
Last 5 losses: [1.544371116324328e-05, 1.544371116324328e-05, 1.544371116324328e-05, 1.544371116324328e-05, 1.544371116324328e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.544371116324328e-05

Optimization complete. Final v2v error: 3.3864991664886475 mm

Highest mean error: 3.8503828048706055 mm for frame 168

Lowest mean error: 2.8717875480651855 mm for frame 237

Saving results

Total time: 37.7616810798645
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_helen_posed_023/1015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_helen_posed_023/1015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_helen_posed_023/1015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00747412
Iteration 2/25 | Loss: 0.00149976
Iteration 3/25 | Loss: 0.00127324
Iteration 4/25 | Loss: 0.00125769
Iteration 5/25 | Loss: 0.00125618
Iteration 6/25 | Loss: 0.00125601
Iteration 7/25 | Loss: 0.00125601
Iteration 8/25 | Loss: 0.00125601
Iteration 9/25 | Loss: 0.00125601
Iteration 10/25 | Loss: 0.00125601
Iteration 11/25 | Loss: 0.00125601
Iteration 12/25 | Loss: 0.00125601
Iteration 13/25 | Loss: 0.00125601
Iteration 14/25 | Loss: 0.00125601
Iteration 15/25 | Loss: 0.00125601
Iteration 16/25 | Loss: 0.00125601
Iteration 17/25 | Loss: 0.00125601
Iteration 18/25 | Loss: 0.00125601
Iteration 19/25 | Loss: 0.00125601
Iteration 20/25 | Loss: 0.00125601
Iteration 21/25 | Loss: 0.00125601
Iteration 22/25 | Loss: 0.00125601
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0012560075847432017, 0.0012560075847432017, 0.0012560075847432017, 0.0012560075847432017, 0.0012560075847432017]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012560075847432017

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.28291130
Iteration 2/25 | Loss: 0.00102128
Iteration 3/25 | Loss: 0.00102126
Iteration 4/25 | Loss: 0.00102126
Iteration 5/25 | Loss: 0.00102126
Iteration 6/25 | Loss: 0.00102126
Iteration 7/25 | Loss: 0.00102126
Iteration 8/25 | Loss: 0.00102126
Iteration 9/25 | Loss: 0.00102126
Iteration 10/25 | Loss: 0.00102126
Iteration 11/25 | Loss: 0.00102126
Iteration 12/25 | Loss: 0.00102126
Iteration 13/25 | Loss: 0.00102126
Iteration 14/25 | Loss: 0.00102126
Iteration 15/25 | Loss: 0.00102126
Iteration 16/25 | Loss: 0.00102126
Iteration 17/25 | Loss: 0.00102126
Iteration 18/25 | Loss: 0.00102126
Iteration 19/25 | Loss: 0.00102126
Iteration 20/25 | Loss: 0.00102126
Iteration 21/25 | Loss: 0.00102126
Iteration 22/25 | Loss: 0.00102126
Iteration 23/25 | Loss: 0.00102126
Iteration 24/25 | Loss: 0.00102126
Iteration 25/25 | Loss: 0.00102126

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00102126
Iteration 2/1000 | Loss: 0.00004053
Iteration 3/1000 | Loss: 0.00002347
Iteration 4/1000 | Loss: 0.00001791
Iteration 5/1000 | Loss: 0.00001618
Iteration 6/1000 | Loss: 0.00001533
Iteration 7/1000 | Loss: 0.00001495
Iteration 8/1000 | Loss: 0.00001463
Iteration 9/1000 | Loss: 0.00001428
Iteration 10/1000 | Loss: 0.00001417
Iteration 11/1000 | Loss: 0.00001394
Iteration 12/1000 | Loss: 0.00001372
Iteration 13/1000 | Loss: 0.00001370
Iteration 14/1000 | Loss: 0.00001360
Iteration 15/1000 | Loss: 0.00001348
Iteration 16/1000 | Loss: 0.00001347
Iteration 17/1000 | Loss: 0.00001344
Iteration 18/1000 | Loss: 0.00001344
Iteration 19/1000 | Loss: 0.00001344
Iteration 20/1000 | Loss: 0.00001340
Iteration 21/1000 | Loss: 0.00001340
Iteration 22/1000 | Loss: 0.00001339
Iteration 23/1000 | Loss: 0.00001339
Iteration 24/1000 | Loss: 0.00001328
Iteration 25/1000 | Loss: 0.00001321
Iteration 26/1000 | Loss: 0.00001314
Iteration 27/1000 | Loss: 0.00001313
Iteration 28/1000 | Loss: 0.00001311
Iteration 29/1000 | Loss: 0.00001310
Iteration 30/1000 | Loss: 0.00001310
Iteration 31/1000 | Loss: 0.00001309
Iteration 32/1000 | Loss: 0.00001309
Iteration 33/1000 | Loss: 0.00001309
Iteration 34/1000 | Loss: 0.00001309
Iteration 35/1000 | Loss: 0.00001308
Iteration 36/1000 | Loss: 0.00001308
Iteration 37/1000 | Loss: 0.00001307
Iteration 38/1000 | Loss: 0.00001307
Iteration 39/1000 | Loss: 0.00001307
Iteration 40/1000 | Loss: 0.00001306
Iteration 41/1000 | Loss: 0.00001305
Iteration 42/1000 | Loss: 0.00001305
Iteration 43/1000 | Loss: 0.00001305
Iteration 44/1000 | Loss: 0.00001304
Iteration 45/1000 | Loss: 0.00001304
Iteration 46/1000 | Loss: 0.00001304
Iteration 47/1000 | Loss: 0.00001303
Iteration 48/1000 | Loss: 0.00001303
Iteration 49/1000 | Loss: 0.00001302
Iteration 50/1000 | Loss: 0.00001301
Iteration 51/1000 | Loss: 0.00001301
Iteration 52/1000 | Loss: 0.00001301
Iteration 53/1000 | Loss: 0.00001301
Iteration 54/1000 | Loss: 0.00001301
Iteration 55/1000 | Loss: 0.00001301
Iteration 56/1000 | Loss: 0.00001301
Iteration 57/1000 | Loss: 0.00001300
Iteration 58/1000 | Loss: 0.00001300
Iteration 59/1000 | Loss: 0.00001300
Iteration 60/1000 | Loss: 0.00001300
Iteration 61/1000 | Loss: 0.00001300
Iteration 62/1000 | Loss: 0.00001300
Iteration 63/1000 | Loss: 0.00001300
Iteration 64/1000 | Loss: 0.00001300
Iteration 65/1000 | Loss: 0.00001300
Iteration 66/1000 | Loss: 0.00001300
Iteration 67/1000 | Loss: 0.00001300
Iteration 68/1000 | Loss: 0.00001300
Iteration 69/1000 | Loss: 0.00001300
Iteration 70/1000 | Loss: 0.00001300
Iteration 71/1000 | Loss: 0.00001300
Iteration 72/1000 | Loss: 0.00001300
Iteration 73/1000 | Loss: 0.00001300
Iteration 74/1000 | Loss: 0.00001300
Iteration 75/1000 | Loss: 0.00001300
Iteration 76/1000 | Loss: 0.00001300
Iteration 77/1000 | Loss: 0.00001299
Iteration 78/1000 | Loss: 0.00001299
Iteration 79/1000 | Loss: 0.00001299
Iteration 80/1000 | Loss: 0.00001299
Iteration 81/1000 | Loss: 0.00001299
Iteration 82/1000 | Loss: 0.00001299
Iteration 83/1000 | Loss: 0.00001299
Iteration 84/1000 | Loss: 0.00001299
Iteration 85/1000 | Loss: 0.00001298
Iteration 86/1000 | Loss: 0.00001298
Iteration 87/1000 | Loss: 0.00001298
Iteration 88/1000 | Loss: 0.00001298
Iteration 89/1000 | Loss: 0.00001298
Iteration 90/1000 | Loss: 0.00001298
Iteration 91/1000 | Loss: 0.00001298
Iteration 92/1000 | Loss: 0.00001298
Iteration 93/1000 | Loss: 0.00001298
Iteration 94/1000 | Loss: 0.00001298
Iteration 95/1000 | Loss: 0.00001298
Iteration 96/1000 | Loss: 0.00001297
Iteration 97/1000 | Loss: 0.00001297
Iteration 98/1000 | Loss: 0.00001297
Iteration 99/1000 | Loss: 0.00001297
Iteration 100/1000 | Loss: 0.00001297
Iteration 101/1000 | Loss: 0.00001296
Iteration 102/1000 | Loss: 0.00001296
Iteration 103/1000 | Loss: 0.00001296
Iteration 104/1000 | Loss: 0.00001296
Iteration 105/1000 | Loss: 0.00001296
Iteration 106/1000 | Loss: 0.00001296
Iteration 107/1000 | Loss: 0.00001296
Iteration 108/1000 | Loss: 0.00001295
Iteration 109/1000 | Loss: 0.00001295
Iteration 110/1000 | Loss: 0.00001295
Iteration 111/1000 | Loss: 0.00001295
Iteration 112/1000 | Loss: 0.00001295
Iteration 113/1000 | Loss: 0.00001295
Iteration 114/1000 | Loss: 0.00001295
Iteration 115/1000 | Loss: 0.00001295
Iteration 116/1000 | Loss: 0.00001295
Iteration 117/1000 | Loss: 0.00001295
Iteration 118/1000 | Loss: 0.00001295
Iteration 119/1000 | Loss: 0.00001295
Iteration 120/1000 | Loss: 0.00001295
Iteration 121/1000 | Loss: 0.00001295
Iteration 122/1000 | Loss: 0.00001295
Iteration 123/1000 | Loss: 0.00001295
Iteration 124/1000 | Loss: 0.00001294
Iteration 125/1000 | Loss: 0.00001294
Iteration 126/1000 | Loss: 0.00001294
Iteration 127/1000 | Loss: 0.00001294
Iteration 128/1000 | Loss: 0.00001294
Iteration 129/1000 | Loss: 0.00001294
Iteration 130/1000 | Loss: 0.00001294
Iteration 131/1000 | Loss: 0.00001294
Iteration 132/1000 | Loss: 0.00001294
Iteration 133/1000 | Loss: 0.00001294
Iteration 134/1000 | Loss: 0.00001294
Iteration 135/1000 | Loss: 0.00001294
Iteration 136/1000 | Loss: 0.00001294
Iteration 137/1000 | Loss: 0.00001294
Iteration 138/1000 | Loss: 0.00001294
Iteration 139/1000 | Loss: 0.00001294
Iteration 140/1000 | Loss: 0.00001294
Iteration 141/1000 | Loss: 0.00001294
Iteration 142/1000 | Loss: 0.00001294
Iteration 143/1000 | Loss: 0.00001294
Iteration 144/1000 | Loss: 0.00001294
Iteration 145/1000 | Loss: 0.00001294
Iteration 146/1000 | Loss: 0.00001294
Iteration 147/1000 | Loss: 0.00001294
Iteration 148/1000 | Loss: 0.00001294
Iteration 149/1000 | Loss: 0.00001294
Iteration 150/1000 | Loss: 0.00001294
Iteration 151/1000 | Loss: 0.00001294
Iteration 152/1000 | Loss: 0.00001294
Iteration 153/1000 | Loss: 0.00001294
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 153. Stopping optimization.
Last 5 losses: [1.2937441169924568e-05, 1.2937441169924568e-05, 1.2937441169924568e-05, 1.2937441169924568e-05, 1.2937441169924568e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2937441169924568e-05

Optimization complete. Final v2v error: 3.0573153495788574 mm

Highest mean error: 3.4762909412384033 mm for frame 109

Lowest mean error: 2.7599997520446777 mm for frame 17

Saving results

Total time: 37.761807441711426
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_helen_posed_023/1053/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_helen_posed_023/1053.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_helen_posed_023/1053
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00677875
Iteration 2/25 | Loss: 0.00137802
Iteration 3/25 | Loss: 0.00126409
Iteration 4/25 | Loss: 0.00120097
Iteration 5/25 | Loss: 0.00119348
Iteration 6/25 | Loss: 0.00120656
Iteration 7/25 | Loss: 0.00118638
Iteration 8/25 | Loss: 0.00118974
Iteration 9/25 | Loss: 0.00118411
Iteration 10/25 | Loss: 0.00118406
Iteration 11/25 | Loss: 0.00118406
Iteration 12/25 | Loss: 0.00118406
Iteration 13/25 | Loss: 0.00118406
Iteration 14/25 | Loss: 0.00118406
Iteration 15/25 | Loss: 0.00118405
Iteration 16/25 | Loss: 0.00118405
Iteration 17/25 | Loss: 0.00118405
Iteration 18/25 | Loss: 0.00118405
Iteration 19/25 | Loss: 0.00118405
Iteration 20/25 | Loss: 0.00118405
Iteration 21/25 | Loss: 0.00118405
Iteration 22/25 | Loss: 0.00118405
Iteration 23/25 | Loss: 0.00118405
Iteration 24/25 | Loss: 0.00118405
Iteration 25/25 | Loss: 0.00118405

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.57713008
Iteration 2/25 | Loss: 0.00131236
Iteration 3/25 | Loss: 0.00131236
Iteration 4/25 | Loss: 0.00131236
Iteration 5/25 | Loss: 0.00131236
Iteration 6/25 | Loss: 0.00131236
Iteration 7/25 | Loss: 0.00131236
Iteration 8/25 | Loss: 0.00131236
Iteration 9/25 | Loss: 0.00131236
Iteration 10/25 | Loss: 0.00131236
Iteration 11/25 | Loss: 0.00131236
Iteration 12/25 | Loss: 0.00131236
Iteration 13/25 | Loss: 0.00131236
Iteration 14/25 | Loss: 0.00131236
Iteration 15/25 | Loss: 0.00131235
Iteration 16/25 | Loss: 0.00131235
Iteration 17/25 | Loss: 0.00131235
Iteration 18/25 | Loss: 0.00131235
Iteration 19/25 | Loss: 0.00131235
Iteration 20/25 | Loss: 0.00131235
Iteration 21/25 | Loss: 0.00131235
Iteration 22/25 | Loss: 0.00131235
Iteration 23/25 | Loss: 0.00131235
Iteration 24/25 | Loss: 0.00131235
Iteration 25/25 | Loss: 0.00131235

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00131235
Iteration 2/1000 | Loss: 0.00001762
Iteration 3/1000 | Loss: 0.00001422
Iteration 4/1000 | Loss: 0.00001292
Iteration 5/1000 | Loss: 0.00001227
Iteration 6/1000 | Loss: 0.00001183
Iteration 7/1000 | Loss: 0.00001146
Iteration 8/1000 | Loss: 0.00001114
Iteration 9/1000 | Loss: 0.00001092
Iteration 10/1000 | Loss: 0.00001081
Iteration 11/1000 | Loss: 0.00001073
Iteration 12/1000 | Loss: 0.00001071
Iteration 13/1000 | Loss: 0.00001060
Iteration 14/1000 | Loss: 0.00001059
Iteration 15/1000 | Loss: 0.00001059
Iteration 16/1000 | Loss: 0.00001054
Iteration 17/1000 | Loss: 0.00001054
Iteration 18/1000 | Loss: 0.00001053
Iteration 19/1000 | Loss: 0.00001053
Iteration 20/1000 | Loss: 0.00001053
Iteration 21/1000 | Loss: 0.00001053
Iteration 22/1000 | Loss: 0.00001052
Iteration 23/1000 | Loss: 0.00001047
Iteration 24/1000 | Loss: 0.00001046
Iteration 25/1000 | Loss: 0.00001044
Iteration 26/1000 | Loss: 0.00001042
Iteration 27/1000 | Loss: 0.00001042
Iteration 28/1000 | Loss: 0.00001040
Iteration 29/1000 | Loss: 0.00001040
Iteration 30/1000 | Loss: 0.00001040
Iteration 31/1000 | Loss: 0.00001038
Iteration 32/1000 | Loss: 0.00001038
Iteration 33/1000 | Loss: 0.00001037
Iteration 34/1000 | Loss: 0.00001037
Iteration 35/1000 | Loss: 0.00001037
Iteration 36/1000 | Loss: 0.00001036
Iteration 37/1000 | Loss: 0.00001036
Iteration 38/1000 | Loss: 0.00001035
Iteration 39/1000 | Loss: 0.00001035
Iteration 40/1000 | Loss: 0.00001034
Iteration 41/1000 | Loss: 0.00001033
Iteration 42/1000 | Loss: 0.00001033
Iteration 43/1000 | Loss: 0.00001032
Iteration 44/1000 | Loss: 0.00001032
Iteration 45/1000 | Loss: 0.00001031
Iteration 46/1000 | Loss: 0.00001030
Iteration 47/1000 | Loss: 0.00001030
Iteration 48/1000 | Loss: 0.00001029
Iteration 49/1000 | Loss: 0.00001029
Iteration 50/1000 | Loss: 0.00001029
Iteration 51/1000 | Loss: 0.00001029
Iteration 52/1000 | Loss: 0.00001029
Iteration 53/1000 | Loss: 0.00001029
Iteration 54/1000 | Loss: 0.00001029
Iteration 55/1000 | Loss: 0.00001028
Iteration 56/1000 | Loss: 0.00001028
Iteration 57/1000 | Loss: 0.00001027
Iteration 58/1000 | Loss: 0.00001027
Iteration 59/1000 | Loss: 0.00001027
Iteration 60/1000 | Loss: 0.00001027
Iteration 61/1000 | Loss: 0.00001027
Iteration 62/1000 | Loss: 0.00001026
Iteration 63/1000 | Loss: 0.00001026
Iteration 64/1000 | Loss: 0.00001026
Iteration 65/1000 | Loss: 0.00001026
Iteration 66/1000 | Loss: 0.00001026
Iteration 67/1000 | Loss: 0.00001026
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 67. Stopping optimization.
Last 5 losses: [1.026366589940153e-05, 1.026366589940153e-05, 1.026366589940153e-05, 1.026366589940153e-05, 1.026366589940153e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.026366589940153e-05

Optimization complete. Final v2v error: 2.7661526203155518 mm

Highest mean error: 3.076402187347412 mm for frame 158

Lowest mean error: 2.5430572032928467 mm for frame 40

Saving results

Total time: 43.48623085021973
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_helen_posed_023/1037/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_helen_posed_023/1037.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_helen_posed_023/1037
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01054158
Iteration 2/25 | Loss: 0.01054158
Iteration 3/25 | Loss: 0.01054158
Iteration 4/25 | Loss: 0.01054158
Iteration 5/25 | Loss: 0.00154868
Iteration 6/25 | Loss: 0.00135500
Iteration 7/25 | Loss: 0.00124744
Iteration 8/25 | Loss: 0.00124000
Iteration 9/25 | Loss: 0.00123195
Iteration 10/25 | Loss: 0.00124529
Iteration 11/25 | Loss: 0.00123014
Iteration 12/25 | Loss: 0.00122603
Iteration 13/25 | Loss: 0.00122513
Iteration 14/25 | Loss: 0.00122088
Iteration 15/25 | Loss: 0.00121988
Iteration 16/25 | Loss: 0.00121956
Iteration 17/25 | Loss: 0.00121956
Iteration 18/25 | Loss: 0.00121956
Iteration 19/25 | Loss: 0.00121956
Iteration 20/25 | Loss: 0.00121956
Iteration 21/25 | Loss: 0.00121956
Iteration 22/25 | Loss: 0.00121956
Iteration 23/25 | Loss: 0.00121956
Iteration 24/25 | Loss: 0.00121956
Iteration 25/25 | Loss: 0.00121956

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 8.38763905
Iteration 2/25 | Loss: 0.00127917
Iteration 3/25 | Loss: 0.00127917
Iteration 4/25 | Loss: 0.00127917
Iteration 5/25 | Loss: 0.00127917
Iteration 6/25 | Loss: 0.00127917
Iteration 7/25 | Loss: 0.00127916
Iteration 8/25 | Loss: 0.00127916
Iteration 9/25 | Loss: 0.00127916
Iteration 10/25 | Loss: 0.00127916
Iteration 11/25 | Loss: 0.00127916
Iteration 12/25 | Loss: 0.00127916
Iteration 13/25 | Loss: 0.00127916
Iteration 14/25 | Loss: 0.00127916
Iteration 15/25 | Loss: 0.00127916
Iteration 16/25 | Loss: 0.00127916
Iteration 17/25 | Loss: 0.00127916
Iteration 18/25 | Loss: 0.00127916
Iteration 19/25 | Loss: 0.00127916
Iteration 20/25 | Loss: 0.00127916
Iteration 21/25 | Loss: 0.00127916
Iteration 22/25 | Loss: 0.00127916
Iteration 23/25 | Loss: 0.00127916
Iteration 24/25 | Loss: 0.00127916
Iteration 25/25 | Loss: 0.00127916

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00127916
Iteration 2/1000 | Loss: 0.00002047
Iteration 3/1000 | Loss: 0.00001452
Iteration 4/1000 | Loss: 0.00006866
Iteration 5/1000 | Loss: 0.00001284
Iteration 6/1000 | Loss: 0.00010746
Iteration 7/1000 | Loss: 0.00002726
Iteration 8/1000 | Loss: 0.00001210
Iteration 9/1000 | Loss: 0.00001187
Iteration 10/1000 | Loss: 0.00003874
Iteration 11/1000 | Loss: 0.00001997
Iteration 12/1000 | Loss: 0.00001138
Iteration 13/1000 | Loss: 0.00001118
Iteration 14/1000 | Loss: 0.00001111
Iteration 15/1000 | Loss: 0.00003189
Iteration 16/1000 | Loss: 0.00018584
Iteration 17/1000 | Loss: 0.00001157
Iteration 18/1000 | Loss: 0.00001101
Iteration 19/1000 | Loss: 0.00001089
Iteration 20/1000 | Loss: 0.00002425
Iteration 21/1000 | Loss: 0.00001304
Iteration 22/1000 | Loss: 0.00001185
Iteration 23/1000 | Loss: 0.00001085
Iteration 24/1000 | Loss: 0.00001083
Iteration 25/1000 | Loss: 0.00001083
Iteration 26/1000 | Loss: 0.00001083
Iteration 27/1000 | Loss: 0.00001083
Iteration 28/1000 | Loss: 0.00001083
Iteration 29/1000 | Loss: 0.00001083
Iteration 30/1000 | Loss: 0.00001083
Iteration 31/1000 | Loss: 0.00001083
Iteration 32/1000 | Loss: 0.00001083
Iteration 33/1000 | Loss: 0.00001082
Iteration 34/1000 | Loss: 0.00001082
Iteration 35/1000 | Loss: 0.00001082
Iteration 36/1000 | Loss: 0.00001082
Iteration 37/1000 | Loss: 0.00001082
Iteration 38/1000 | Loss: 0.00001082
Iteration 39/1000 | Loss: 0.00001082
Iteration 40/1000 | Loss: 0.00001081
Iteration 41/1000 | Loss: 0.00001081
Iteration 42/1000 | Loss: 0.00001081
Iteration 43/1000 | Loss: 0.00001081
Iteration 44/1000 | Loss: 0.00001081
Iteration 45/1000 | Loss: 0.00001081
Iteration 46/1000 | Loss: 0.00001081
Iteration 47/1000 | Loss: 0.00001081
Iteration 48/1000 | Loss: 0.00001081
Iteration 49/1000 | Loss: 0.00001081
Iteration 50/1000 | Loss: 0.00001080
Iteration 51/1000 | Loss: 0.00001080
Iteration 52/1000 | Loss: 0.00001080
Iteration 53/1000 | Loss: 0.00001080
Iteration 54/1000 | Loss: 0.00001080
Iteration 55/1000 | Loss: 0.00001080
Iteration 56/1000 | Loss: 0.00001080
Iteration 57/1000 | Loss: 0.00001079
Iteration 58/1000 | Loss: 0.00001077
Iteration 59/1000 | Loss: 0.00001076
Iteration 60/1000 | Loss: 0.00001075
Iteration 61/1000 | Loss: 0.00001075
Iteration 62/1000 | Loss: 0.00001074
Iteration 63/1000 | Loss: 0.00001074
Iteration 64/1000 | Loss: 0.00001072
Iteration 65/1000 | Loss: 0.00001072
Iteration 66/1000 | Loss: 0.00001072
Iteration 67/1000 | Loss: 0.00001072
Iteration 68/1000 | Loss: 0.00001072
Iteration 69/1000 | Loss: 0.00001071
Iteration 70/1000 | Loss: 0.00001071
Iteration 71/1000 | Loss: 0.00001070
Iteration 72/1000 | Loss: 0.00002815
Iteration 73/1000 | Loss: 0.00001901
Iteration 74/1000 | Loss: 0.00002483
Iteration 75/1000 | Loss: 0.00001068
Iteration 76/1000 | Loss: 0.00001067
Iteration 77/1000 | Loss: 0.00001066
Iteration 78/1000 | Loss: 0.00001066
Iteration 79/1000 | Loss: 0.00001066
Iteration 80/1000 | Loss: 0.00001065
Iteration 81/1000 | Loss: 0.00001065
Iteration 82/1000 | Loss: 0.00001065
Iteration 83/1000 | Loss: 0.00001065
Iteration 84/1000 | Loss: 0.00001064
Iteration 85/1000 | Loss: 0.00001064
Iteration 86/1000 | Loss: 0.00001064
Iteration 87/1000 | Loss: 0.00001064
Iteration 88/1000 | Loss: 0.00001064
Iteration 89/1000 | Loss: 0.00001064
Iteration 90/1000 | Loss: 0.00001063
Iteration 91/1000 | Loss: 0.00001063
Iteration 92/1000 | Loss: 0.00001063
Iteration 93/1000 | Loss: 0.00001063
Iteration 94/1000 | Loss: 0.00001583
Iteration 95/1000 | Loss: 0.00001065
Iteration 96/1000 | Loss: 0.00001062
Iteration 97/1000 | Loss: 0.00001061
Iteration 98/1000 | Loss: 0.00001060
Iteration 99/1000 | Loss: 0.00001060
Iteration 100/1000 | Loss: 0.00001059
Iteration 101/1000 | Loss: 0.00001059
Iteration 102/1000 | Loss: 0.00001058
Iteration 103/1000 | Loss: 0.00001058
Iteration 104/1000 | Loss: 0.00001058
Iteration 105/1000 | Loss: 0.00001057
Iteration 106/1000 | Loss: 0.00001057
Iteration 107/1000 | Loss: 0.00001057
Iteration 108/1000 | Loss: 0.00001057
Iteration 109/1000 | Loss: 0.00001057
Iteration 110/1000 | Loss: 0.00001057
Iteration 111/1000 | Loss: 0.00001057
Iteration 112/1000 | Loss: 0.00001057
Iteration 113/1000 | Loss: 0.00001057
Iteration 114/1000 | Loss: 0.00001057
Iteration 115/1000 | Loss: 0.00001057
Iteration 116/1000 | Loss: 0.00001056
Iteration 117/1000 | Loss: 0.00001056
Iteration 118/1000 | Loss: 0.00001056
Iteration 119/1000 | Loss: 0.00001056
Iteration 120/1000 | Loss: 0.00001056
Iteration 121/1000 | Loss: 0.00001056
Iteration 122/1000 | Loss: 0.00001056
Iteration 123/1000 | Loss: 0.00001056
Iteration 124/1000 | Loss: 0.00001056
Iteration 125/1000 | Loss: 0.00001056
Iteration 126/1000 | Loss: 0.00001056
Iteration 127/1000 | Loss: 0.00001056
Iteration 128/1000 | Loss: 0.00001056
Iteration 129/1000 | Loss: 0.00001055
Iteration 130/1000 | Loss: 0.00001055
Iteration 131/1000 | Loss: 0.00001055
Iteration 132/1000 | Loss: 0.00001783
Iteration 133/1000 | Loss: 0.00001783
Iteration 134/1000 | Loss: 0.00001150
Iteration 135/1000 | Loss: 0.00006365
Iteration 136/1000 | Loss: 0.00001056
Iteration 137/1000 | Loss: 0.00001056
Iteration 138/1000 | Loss: 0.00001056
Iteration 139/1000 | Loss: 0.00001056
Iteration 140/1000 | Loss: 0.00001056
Iteration 141/1000 | Loss: 0.00001056
Iteration 142/1000 | Loss: 0.00001056
Iteration 143/1000 | Loss: 0.00001056
Iteration 144/1000 | Loss: 0.00001056
Iteration 145/1000 | Loss: 0.00001055
Iteration 146/1000 | Loss: 0.00001055
Iteration 147/1000 | Loss: 0.00001055
Iteration 148/1000 | Loss: 0.00001054
Iteration 149/1000 | Loss: 0.00001054
Iteration 150/1000 | Loss: 0.00001054
Iteration 151/1000 | Loss: 0.00001054
Iteration 152/1000 | Loss: 0.00001054
Iteration 153/1000 | Loss: 0.00001054
Iteration 154/1000 | Loss: 0.00001054
Iteration 155/1000 | Loss: 0.00001054
Iteration 156/1000 | Loss: 0.00001054
Iteration 157/1000 | Loss: 0.00001054
Iteration 158/1000 | Loss: 0.00001054
Iteration 159/1000 | Loss: 0.00001054
Iteration 160/1000 | Loss: 0.00001054
Iteration 161/1000 | Loss: 0.00001054
Iteration 162/1000 | Loss: 0.00001053
Iteration 163/1000 | Loss: 0.00001053
Iteration 164/1000 | Loss: 0.00001053
Iteration 165/1000 | Loss: 0.00001053
Iteration 166/1000 | Loss: 0.00001053
Iteration 167/1000 | Loss: 0.00001053
Iteration 168/1000 | Loss: 0.00001053
Iteration 169/1000 | Loss: 0.00001053
Iteration 170/1000 | Loss: 0.00001053
Iteration 171/1000 | Loss: 0.00001053
Iteration 172/1000 | Loss: 0.00001052
Iteration 173/1000 | Loss: 0.00001052
Iteration 174/1000 | Loss: 0.00001052
Iteration 175/1000 | Loss: 0.00001052
Iteration 176/1000 | Loss: 0.00001052
Iteration 177/1000 | Loss: 0.00001052
Iteration 178/1000 | Loss: 0.00001052
Iteration 179/1000 | Loss: 0.00001052
Iteration 180/1000 | Loss: 0.00001052
Iteration 181/1000 | Loss: 0.00001052
Iteration 182/1000 | Loss: 0.00001052
Iteration 183/1000 | Loss: 0.00001052
Iteration 184/1000 | Loss: 0.00001052
Iteration 185/1000 | Loss: 0.00001051
Iteration 186/1000 | Loss: 0.00001051
Iteration 187/1000 | Loss: 0.00001051
Iteration 188/1000 | Loss: 0.00001051
Iteration 189/1000 | Loss: 0.00001051
Iteration 190/1000 | Loss: 0.00001051
Iteration 191/1000 | Loss: 0.00001051
Iteration 192/1000 | Loss: 0.00001051
Iteration 193/1000 | Loss: 0.00001051
Iteration 194/1000 | Loss: 0.00001051
Iteration 195/1000 | Loss: 0.00001051
Iteration 196/1000 | Loss: 0.00001051
Iteration 197/1000 | Loss: 0.00001051
Iteration 198/1000 | Loss: 0.00001051
Iteration 199/1000 | Loss: 0.00001051
Iteration 200/1000 | Loss: 0.00001051
Iteration 201/1000 | Loss: 0.00001051
Iteration 202/1000 | Loss: 0.00001051
Iteration 203/1000 | Loss: 0.00001051
Iteration 204/1000 | Loss: 0.00001051
Iteration 205/1000 | Loss: 0.00001051
Iteration 206/1000 | Loss: 0.00001051
Iteration 207/1000 | Loss: 0.00001051
Iteration 208/1000 | Loss: 0.00001051
Iteration 209/1000 | Loss: 0.00001051
Iteration 210/1000 | Loss: 0.00001051
Iteration 211/1000 | Loss: 0.00001050
Iteration 212/1000 | Loss: 0.00001050
Iteration 213/1000 | Loss: 0.00001050
Iteration 214/1000 | Loss: 0.00001050
Iteration 215/1000 | Loss: 0.00001050
Iteration 216/1000 | Loss: 0.00001050
Iteration 217/1000 | Loss: 0.00001050
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 217. Stopping optimization.
Last 5 losses: [1.0504805686650798e-05, 1.0504805686650798e-05, 1.0504805686650798e-05, 1.0504805686650798e-05, 1.0504805686650798e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0504805686650798e-05

Optimization complete. Final v2v error: 2.7406585216522217 mm

Highest mean error: 3.090867757797241 mm for frame 119

Lowest mean error: 2.5497987270355225 mm for frame 161

Saving results

Total time: 77.14699959754944
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_helen_posed_023/1051/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_helen_posed_023/1051.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_helen_posed_023/1051
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00792796
Iteration 2/25 | Loss: 0.00162343
Iteration 3/25 | Loss: 0.00132108
Iteration 4/25 | Loss: 0.00129399
Iteration 5/25 | Loss: 0.00128618
Iteration 6/25 | Loss: 0.00128351
Iteration 7/25 | Loss: 0.00128315
Iteration 8/25 | Loss: 0.00128315
Iteration 9/25 | Loss: 0.00128315
Iteration 10/25 | Loss: 0.00128315
Iteration 11/25 | Loss: 0.00128315
Iteration 12/25 | Loss: 0.00128315
Iteration 13/25 | Loss: 0.00128315
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0012831531930714846, 0.0012831531930714846, 0.0012831531930714846, 0.0012831531930714846, 0.0012831531930714846]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012831531930714846

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.13886642
Iteration 2/25 | Loss: 0.00140888
Iteration 3/25 | Loss: 0.00140887
Iteration 4/25 | Loss: 0.00140887
Iteration 5/25 | Loss: 0.00140887
Iteration 6/25 | Loss: 0.00140887
Iteration 7/25 | Loss: 0.00140887
Iteration 8/25 | Loss: 0.00140887
Iteration 9/25 | Loss: 0.00140887
Iteration 10/25 | Loss: 0.00140887
Iteration 11/25 | Loss: 0.00140887
Iteration 12/25 | Loss: 0.00140887
Iteration 13/25 | Loss: 0.00140887
Iteration 14/25 | Loss: 0.00140887
Iteration 15/25 | Loss: 0.00140887
Iteration 16/25 | Loss: 0.00140887
Iteration 17/25 | Loss: 0.00140887
Iteration 18/25 | Loss: 0.00140887
Iteration 19/25 | Loss: 0.00140887
Iteration 20/25 | Loss: 0.00140887
Iteration 21/25 | Loss: 0.00140887
Iteration 22/25 | Loss: 0.00140887
Iteration 23/25 | Loss: 0.00140887
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0014088709140196443, 0.0014088709140196443, 0.0014088709140196443, 0.0014088709140196443, 0.0014088709140196443]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014088709140196443

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00140887
Iteration 2/1000 | Loss: 0.00008214
Iteration 3/1000 | Loss: 0.00005334
Iteration 4/1000 | Loss: 0.00003971
Iteration 5/1000 | Loss: 0.00003624
Iteration 6/1000 | Loss: 0.00003480
Iteration 7/1000 | Loss: 0.00003324
Iteration 8/1000 | Loss: 0.00003245
Iteration 9/1000 | Loss: 0.00003167
Iteration 10/1000 | Loss: 0.00003122
Iteration 11/1000 | Loss: 0.00003080
Iteration 12/1000 | Loss: 0.00003038
Iteration 13/1000 | Loss: 0.00003003
Iteration 14/1000 | Loss: 0.00002981
Iteration 15/1000 | Loss: 0.00002956
Iteration 16/1000 | Loss: 0.00002938
Iteration 17/1000 | Loss: 0.00002921
Iteration 18/1000 | Loss: 0.00002909
Iteration 19/1000 | Loss: 0.00002908
Iteration 20/1000 | Loss: 0.00002900
Iteration 21/1000 | Loss: 0.00002891
Iteration 22/1000 | Loss: 0.00002890
Iteration 23/1000 | Loss: 0.00002889
Iteration 24/1000 | Loss: 0.00002888
Iteration 25/1000 | Loss: 0.00002885
Iteration 26/1000 | Loss: 0.00002884
Iteration 27/1000 | Loss: 0.00002884
Iteration 28/1000 | Loss: 0.00002884
Iteration 29/1000 | Loss: 0.00002884
Iteration 30/1000 | Loss: 0.00002884
Iteration 31/1000 | Loss: 0.00002883
Iteration 32/1000 | Loss: 0.00002883
Iteration 33/1000 | Loss: 0.00002882
Iteration 34/1000 | Loss: 0.00002880
Iteration 35/1000 | Loss: 0.00002879
Iteration 36/1000 | Loss: 0.00002879
Iteration 37/1000 | Loss: 0.00002877
Iteration 38/1000 | Loss: 0.00002876
Iteration 39/1000 | Loss: 0.00002875
Iteration 40/1000 | Loss: 0.00002874
Iteration 41/1000 | Loss: 0.00002872
Iteration 42/1000 | Loss: 0.00002872
Iteration 43/1000 | Loss: 0.00002870
Iteration 44/1000 | Loss: 0.00002870
Iteration 45/1000 | Loss: 0.00002870
Iteration 46/1000 | Loss: 0.00002869
Iteration 47/1000 | Loss: 0.00002869
Iteration 48/1000 | Loss: 0.00002869
Iteration 49/1000 | Loss: 0.00002868
Iteration 50/1000 | Loss: 0.00002868
Iteration 51/1000 | Loss: 0.00002868
Iteration 52/1000 | Loss: 0.00002867
Iteration 53/1000 | Loss: 0.00002867
Iteration 54/1000 | Loss: 0.00002867
Iteration 55/1000 | Loss: 0.00002866
Iteration 56/1000 | Loss: 0.00002866
Iteration 57/1000 | Loss: 0.00002866
Iteration 58/1000 | Loss: 0.00002865
Iteration 59/1000 | Loss: 0.00002865
Iteration 60/1000 | Loss: 0.00002864
Iteration 61/1000 | Loss: 0.00002864
Iteration 62/1000 | Loss: 0.00002864
Iteration 63/1000 | Loss: 0.00002864
Iteration 64/1000 | Loss: 0.00002864
Iteration 65/1000 | Loss: 0.00002864
Iteration 66/1000 | Loss: 0.00002864
Iteration 67/1000 | Loss: 0.00002863
Iteration 68/1000 | Loss: 0.00002863
Iteration 69/1000 | Loss: 0.00002862
Iteration 70/1000 | Loss: 0.00002862
Iteration 71/1000 | Loss: 0.00002862
Iteration 72/1000 | Loss: 0.00002862
Iteration 73/1000 | Loss: 0.00002862
Iteration 74/1000 | Loss: 0.00002862
Iteration 75/1000 | Loss: 0.00002861
Iteration 76/1000 | Loss: 0.00002861
Iteration 77/1000 | Loss: 0.00002861
Iteration 78/1000 | Loss: 0.00002861
Iteration 79/1000 | Loss: 0.00002861
Iteration 80/1000 | Loss: 0.00002861
Iteration 81/1000 | Loss: 0.00002860
Iteration 82/1000 | Loss: 0.00002860
Iteration 83/1000 | Loss: 0.00002860
Iteration 84/1000 | Loss: 0.00002859
Iteration 85/1000 | Loss: 0.00002859
Iteration 86/1000 | Loss: 0.00002859
Iteration 87/1000 | Loss: 0.00002859
Iteration 88/1000 | Loss: 0.00002859
Iteration 89/1000 | Loss: 0.00002859
Iteration 90/1000 | Loss: 0.00002859
Iteration 91/1000 | Loss: 0.00002859
Iteration 92/1000 | Loss: 0.00002858
Iteration 93/1000 | Loss: 0.00002858
Iteration 94/1000 | Loss: 0.00002858
Iteration 95/1000 | Loss: 0.00002858
Iteration 96/1000 | Loss: 0.00002858
Iteration 97/1000 | Loss: 0.00002858
Iteration 98/1000 | Loss: 0.00002858
Iteration 99/1000 | Loss: 0.00002857
Iteration 100/1000 | Loss: 0.00002857
Iteration 101/1000 | Loss: 0.00002857
Iteration 102/1000 | Loss: 0.00002857
Iteration 103/1000 | Loss: 0.00002857
Iteration 104/1000 | Loss: 0.00002857
Iteration 105/1000 | Loss: 0.00002857
Iteration 106/1000 | Loss: 0.00002857
Iteration 107/1000 | Loss: 0.00002857
Iteration 108/1000 | Loss: 0.00002857
Iteration 109/1000 | Loss: 0.00002857
Iteration 110/1000 | Loss: 0.00002857
Iteration 111/1000 | Loss: 0.00002856
Iteration 112/1000 | Loss: 0.00002856
Iteration 113/1000 | Loss: 0.00002856
Iteration 114/1000 | Loss: 0.00002856
Iteration 115/1000 | Loss: 0.00002856
Iteration 116/1000 | Loss: 0.00002856
Iteration 117/1000 | Loss: 0.00002856
Iteration 118/1000 | Loss: 0.00002856
Iteration 119/1000 | Loss: 0.00002855
Iteration 120/1000 | Loss: 0.00002855
Iteration 121/1000 | Loss: 0.00002855
Iteration 122/1000 | Loss: 0.00002855
Iteration 123/1000 | Loss: 0.00002855
Iteration 124/1000 | Loss: 0.00002855
Iteration 125/1000 | Loss: 0.00002854
Iteration 126/1000 | Loss: 0.00002854
Iteration 127/1000 | Loss: 0.00002854
Iteration 128/1000 | Loss: 0.00002854
Iteration 129/1000 | Loss: 0.00002854
Iteration 130/1000 | Loss: 0.00002854
Iteration 131/1000 | Loss: 0.00002854
Iteration 132/1000 | Loss: 0.00002854
Iteration 133/1000 | Loss: 0.00002854
Iteration 134/1000 | Loss: 0.00002854
Iteration 135/1000 | Loss: 0.00002854
Iteration 136/1000 | Loss: 0.00002854
Iteration 137/1000 | Loss: 0.00002854
Iteration 138/1000 | Loss: 0.00002854
Iteration 139/1000 | Loss: 0.00002854
Iteration 140/1000 | Loss: 0.00002854
Iteration 141/1000 | Loss: 0.00002853
Iteration 142/1000 | Loss: 0.00002853
Iteration 143/1000 | Loss: 0.00002853
Iteration 144/1000 | Loss: 0.00002853
Iteration 145/1000 | Loss: 0.00002853
Iteration 146/1000 | Loss: 0.00002853
Iteration 147/1000 | Loss: 0.00002853
Iteration 148/1000 | Loss: 0.00002853
Iteration 149/1000 | Loss: 0.00002853
Iteration 150/1000 | Loss: 0.00002853
Iteration 151/1000 | Loss: 0.00002853
Iteration 152/1000 | Loss: 0.00002853
Iteration 153/1000 | Loss: 0.00002853
Iteration 154/1000 | Loss: 0.00002853
Iteration 155/1000 | Loss: 0.00002853
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 155. Stopping optimization.
Last 5 losses: [2.853426667570602e-05, 2.853426667570602e-05, 2.853426667570602e-05, 2.853426667570602e-05, 2.853426667570602e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.853426667570602e-05

Optimization complete. Final v2v error: 4.327911853790283 mm

Highest mean error: 5.248711109161377 mm for frame 158

Lowest mean error: 2.8297832012176514 mm for frame 197

Saving results

Total time: 49.62614154815674
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_48_us_2148/0023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_48_us_2148/0023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_48_us_2148/0023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00829962
Iteration 2/25 | Loss: 0.00205661
Iteration 3/25 | Loss: 0.00191284
Iteration 4/25 | Loss: 0.00187561
Iteration 5/25 | Loss: 0.00187442
Iteration 6/25 | Loss: 0.00187711
Iteration 7/25 | Loss: 0.00185968
Iteration 8/25 | Loss: 0.00186103
Iteration 9/25 | Loss: 0.00185770
Iteration 10/25 | Loss: 0.00185766
Iteration 11/25 | Loss: 0.00185766
Iteration 12/25 | Loss: 0.00185766
Iteration 13/25 | Loss: 0.00185766
Iteration 14/25 | Loss: 0.00185766
Iteration 15/25 | Loss: 0.00185766
Iteration 16/25 | Loss: 0.00185766
Iteration 17/25 | Loss: 0.00185766
Iteration 18/25 | Loss: 0.00185765
Iteration 19/25 | Loss: 0.00185765
Iteration 20/25 | Loss: 0.00185765
Iteration 21/25 | Loss: 0.00185765
Iteration 22/25 | Loss: 0.00185765
Iteration 23/25 | Loss: 0.00185765
Iteration 24/25 | Loss: 0.00185765
Iteration 25/25 | Loss: 0.00185765

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.97988820
Iteration 2/25 | Loss: 0.00224111
Iteration 3/25 | Loss: 0.00224110
Iteration 4/25 | Loss: 0.00224110
Iteration 5/25 | Loss: 0.00224110
Iteration 6/25 | Loss: 0.00224110
Iteration 7/25 | Loss: 0.00224110
Iteration 8/25 | Loss: 0.00224110
Iteration 9/25 | Loss: 0.00224110
Iteration 10/25 | Loss: 0.00224110
Iteration 11/25 | Loss: 0.00224110
Iteration 12/25 | Loss: 0.00224110
Iteration 13/25 | Loss: 0.00224110
Iteration 14/25 | Loss: 0.00224110
Iteration 15/25 | Loss: 0.00224110
Iteration 16/25 | Loss: 0.00224110
Iteration 17/25 | Loss: 0.00224110
Iteration 18/25 | Loss: 0.00224110
Iteration 19/25 | Loss: 0.00224110
Iteration 20/25 | Loss: 0.00224110
Iteration 21/25 | Loss: 0.00224110
Iteration 22/25 | Loss: 0.00224110
Iteration 23/25 | Loss: 0.00224110
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.002241103444248438, 0.002241103444248438, 0.002241103444248438, 0.002241103444248438, 0.002241103444248438]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002241103444248438

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00224110
Iteration 2/1000 | Loss: 0.00009053
Iteration 3/1000 | Loss: 0.00006055
Iteration 4/1000 | Loss: 0.00005213
Iteration 5/1000 | Loss: 0.00004849
Iteration 6/1000 | Loss: 0.00004712
Iteration 7/1000 | Loss: 0.00004644
Iteration 8/1000 | Loss: 0.00004551
Iteration 9/1000 | Loss: 0.00004484
Iteration 10/1000 | Loss: 0.00004448
Iteration 11/1000 | Loss: 0.00004425
Iteration 12/1000 | Loss: 0.00004405
Iteration 13/1000 | Loss: 0.00004402
Iteration 14/1000 | Loss: 0.00004399
Iteration 15/1000 | Loss: 0.00004392
Iteration 16/1000 | Loss: 0.00004391
Iteration 17/1000 | Loss: 0.00004390
Iteration 18/1000 | Loss: 0.00004389
Iteration 19/1000 | Loss: 0.00004389
Iteration 20/1000 | Loss: 0.00004389
Iteration 21/1000 | Loss: 0.00004389
Iteration 22/1000 | Loss: 0.00004389
Iteration 23/1000 | Loss: 0.00004388
Iteration 24/1000 | Loss: 0.00004388
Iteration 25/1000 | Loss: 0.00004388
Iteration 26/1000 | Loss: 0.00004388
Iteration 27/1000 | Loss: 0.00004388
Iteration 28/1000 | Loss: 0.00004388
Iteration 29/1000 | Loss: 0.00004388
Iteration 30/1000 | Loss: 0.00004387
Iteration 31/1000 | Loss: 0.00004387
Iteration 32/1000 | Loss: 0.00004387
Iteration 33/1000 | Loss: 0.00004386
Iteration 34/1000 | Loss: 0.00004386
Iteration 35/1000 | Loss: 0.00004386
Iteration 36/1000 | Loss: 0.00004386
Iteration 37/1000 | Loss: 0.00004385
Iteration 38/1000 | Loss: 0.00004385
Iteration 39/1000 | Loss: 0.00004385
Iteration 40/1000 | Loss: 0.00004384
Iteration 41/1000 | Loss: 0.00004384
Iteration 42/1000 | Loss: 0.00004383
Iteration 43/1000 | Loss: 0.00004383
Iteration 44/1000 | Loss: 0.00004383
Iteration 45/1000 | Loss: 0.00004383
Iteration 46/1000 | Loss: 0.00004383
Iteration 47/1000 | Loss: 0.00004383
Iteration 48/1000 | Loss: 0.00004383
Iteration 49/1000 | Loss: 0.00004383
Iteration 50/1000 | Loss: 0.00004383
Iteration 51/1000 | Loss: 0.00004383
Iteration 52/1000 | Loss: 0.00004382
Iteration 53/1000 | Loss: 0.00004382
Iteration 54/1000 | Loss: 0.00004382
Iteration 55/1000 | Loss: 0.00004382
Iteration 56/1000 | Loss: 0.00004382
Iteration 57/1000 | Loss: 0.00004382
Iteration 58/1000 | Loss: 0.00004382
Iteration 59/1000 | Loss: 0.00004382
Iteration 60/1000 | Loss: 0.00004381
Iteration 61/1000 | Loss: 0.00004381
Iteration 62/1000 | Loss: 0.00004381
Iteration 63/1000 | Loss: 0.00004381
Iteration 64/1000 | Loss: 0.00004381
Iteration 65/1000 | Loss: 0.00004381
Iteration 66/1000 | Loss: 0.00004381
Iteration 67/1000 | Loss: 0.00004381
Iteration 68/1000 | Loss: 0.00004381
Iteration 69/1000 | Loss: 0.00004381
Iteration 70/1000 | Loss: 0.00004381
Iteration 71/1000 | Loss: 0.00004381
Iteration 72/1000 | Loss: 0.00004381
Iteration 73/1000 | Loss: 0.00004381
Iteration 74/1000 | Loss: 0.00004381
Iteration 75/1000 | Loss: 0.00004381
Iteration 76/1000 | Loss: 0.00004381
Iteration 77/1000 | Loss: 0.00004380
Iteration 78/1000 | Loss: 0.00004380
Iteration 79/1000 | Loss: 0.00004380
Iteration 80/1000 | Loss: 0.00004380
Iteration 81/1000 | Loss: 0.00004380
Iteration 82/1000 | Loss: 0.00004380
Iteration 83/1000 | Loss: 0.00004380
Iteration 84/1000 | Loss: 0.00004380
Iteration 85/1000 | Loss: 0.00004380
Iteration 86/1000 | Loss: 0.00004380
Iteration 87/1000 | Loss: 0.00004380
Iteration 88/1000 | Loss: 0.00004380
Iteration 89/1000 | Loss: 0.00004380
Iteration 90/1000 | Loss: 0.00004380
Iteration 91/1000 | Loss: 0.00004380
Iteration 92/1000 | Loss: 0.00004380
Iteration 93/1000 | Loss: 0.00004380
Iteration 94/1000 | Loss: 0.00004380
Iteration 95/1000 | Loss: 0.00004380
Iteration 96/1000 | Loss: 0.00004380
Iteration 97/1000 | Loss: 0.00004380
Iteration 98/1000 | Loss: 0.00004380
Iteration 99/1000 | Loss: 0.00004380
Iteration 100/1000 | Loss: 0.00004380
Iteration 101/1000 | Loss: 0.00004380
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 101. Stopping optimization.
Last 5 losses: [4.380317477625795e-05, 4.380317477625795e-05, 4.380317477625795e-05, 4.380317477625795e-05, 4.380317477625795e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.380317477625795e-05

Optimization complete. Final v2v error: 5.824377059936523 mm

Highest mean error: 6.133805274963379 mm for frame 128

Lowest mean error: 5.474360942840576 mm for frame 204

Saving results

Total time: 46.057050943374634
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_48_us_2148/0022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_48_us_2148/0022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_48_us_2148/0022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00962185
Iteration 2/25 | Loss: 0.00208698
Iteration 3/25 | Loss: 0.00186043
Iteration 4/25 | Loss: 0.00183353
Iteration 5/25 | Loss: 0.00182893
Iteration 6/25 | Loss: 0.00182721
Iteration 7/25 | Loss: 0.00182649
Iteration 8/25 | Loss: 0.00182624
Iteration 9/25 | Loss: 0.00182611
Iteration 10/25 | Loss: 0.00182594
Iteration 11/25 | Loss: 0.00182812
Iteration 12/25 | Loss: 0.00182627
Iteration 13/25 | Loss: 0.00182466
Iteration 14/25 | Loss: 0.00182523
Iteration 15/25 | Loss: 0.00182549
Iteration 16/25 | Loss: 0.00182495
Iteration 17/25 | Loss: 0.00182467
Iteration 18/25 | Loss: 0.00182575
Iteration 19/25 | Loss: 0.00182596
Iteration 20/25 | Loss: 0.00182508
Iteration 21/25 | Loss: 0.00182463
Iteration 22/25 | Loss: 0.00182541
Iteration 23/25 | Loss: 0.00182627
Iteration 24/25 | Loss: 0.00182456
Iteration 25/25 | Loss: 0.00182525

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.49420559
Iteration 2/25 | Loss: 0.00224814
Iteration 3/25 | Loss: 0.00224814
Iteration 4/25 | Loss: 0.00224814
Iteration 5/25 | Loss: 0.00224814
Iteration 6/25 | Loss: 0.00224814
Iteration 7/25 | Loss: 0.00224814
Iteration 8/25 | Loss: 0.00224814
Iteration 9/25 | Loss: 0.00224814
Iteration 10/25 | Loss: 0.00224814
Iteration 11/25 | Loss: 0.00224814
Iteration 12/25 | Loss: 0.00224814
Iteration 13/25 | Loss: 0.00224814
Iteration 14/25 | Loss: 0.00224814
Iteration 15/25 | Loss: 0.00224814
Iteration 16/25 | Loss: 0.00224814
Iteration 17/25 | Loss: 0.00224814
Iteration 18/25 | Loss: 0.00224814
Iteration 19/25 | Loss: 0.00224814
Iteration 20/25 | Loss: 0.00224814
Iteration 21/25 | Loss: 0.00224814
Iteration 22/25 | Loss: 0.00224814
Iteration 23/25 | Loss: 0.00224814
Iteration 24/25 | Loss: 0.00224814
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0022481440100818872, 0.0022481440100818872, 0.0022481440100818872, 0.0022481440100818872, 0.0022481440100818872]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0022481440100818872

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00224814
Iteration 2/1000 | Loss: 0.00009759
Iteration 3/1000 | Loss: 0.00006396
Iteration 4/1000 | Loss: 0.00009964
Iteration 5/1000 | Loss: 0.00005580
Iteration 6/1000 | Loss: 0.00004913
Iteration 7/1000 | Loss: 0.00004530
Iteration 8/1000 | Loss: 0.00004376
Iteration 9/1000 | Loss: 0.00004243
Iteration 10/1000 | Loss: 0.00004146
Iteration 11/1000 | Loss: 0.00004085
Iteration 12/1000 | Loss: 0.00004026
Iteration 13/1000 | Loss: 0.00003986
Iteration 14/1000 | Loss: 0.00003962
Iteration 15/1000 | Loss: 0.00003943
Iteration 16/1000 | Loss: 0.00003928
Iteration 17/1000 | Loss: 0.00003923
Iteration 18/1000 | Loss: 0.00003919
Iteration 19/1000 | Loss: 0.00003914
Iteration 20/1000 | Loss: 0.00003906
Iteration 21/1000 | Loss: 0.00003905
Iteration 22/1000 | Loss: 0.00003902
Iteration 23/1000 | Loss: 0.00003894
Iteration 24/1000 | Loss: 0.00003891
Iteration 25/1000 | Loss: 0.00003890
Iteration 26/1000 | Loss: 0.00003889
Iteration 27/1000 | Loss: 0.00003889
Iteration 28/1000 | Loss: 0.00003888
Iteration 29/1000 | Loss: 0.00003885
Iteration 30/1000 | Loss: 0.00003885
Iteration 31/1000 | Loss: 0.00003885
Iteration 32/1000 | Loss: 0.00003885
Iteration 33/1000 | Loss: 0.00003884
Iteration 34/1000 | Loss: 0.00003884
Iteration 35/1000 | Loss: 0.00003884
Iteration 36/1000 | Loss: 0.00003884
Iteration 37/1000 | Loss: 0.00003883
Iteration 38/1000 | Loss: 0.00003883
Iteration 39/1000 | Loss: 0.00003882
Iteration 40/1000 | Loss: 0.00003882
Iteration 41/1000 | Loss: 0.00003881
Iteration 42/1000 | Loss: 0.00003881
Iteration 43/1000 | Loss: 0.00003881
Iteration 44/1000 | Loss: 0.00003881
Iteration 45/1000 | Loss: 0.00003880
Iteration 46/1000 | Loss: 0.00003880
Iteration 47/1000 | Loss: 0.00003880
Iteration 48/1000 | Loss: 0.00003879
Iteration 49/1000 | Loss: 0.00003879
Iteration 50/1000 | Loss: 0.00003878
Iteration 51/1000 | Loss: 0.00003878
Iteration 52/1000 | Loss: 0.00003878
Iteration 53/1000 | Loss: 0.00003878
Iteration 54/1000 | Loss: 0.00003878
Iteration 55/1000 | Loss: 0.00003877
Iteration 56/1000 | Loss: 0.00003877
Iteration 57/1000 | Loss: 0.00003877
Iteration 58/1000 | Loss: 0.00003876
Iteration 59/1000 | Loss: 0.00003876
Iteration 60/1000 | Loss: 0.00003876
Iteration 61/1000 | Loss: 0.00003876
Iteration 62/1000 | Loss: 0.00003875
Iteration 63/1000 | Loss: 0.00003875
Iteration 64/1000 | Loss: 0.00003875
Iteration 65/1000 | Loss: 0.00003875
Iteration 66/1000 | Loss: 0.00003875
Iteration 67/1000 | Loss: 0.00003875
Iteration 68/1000 | Loss: 0.00003875
Iteration 69/1000 | Loss: 0.00003874
Iteration 70/1000 | Loss: 0.00003874
Iteration 71/1000 | Loss: 0.00003874
Iteration 72/1000 | Loss: 0.00003874
Iteration 73/1000 | Loss: 0.00003874
Iteration 74/1000 | Loss: 0.00003874
Iteration 75/1000 | Loss: 0.00003874
Iteration 76/1000 | Loss: 0.00003873
Iteration 77/1000 | Loss: 0.00003873
Iteration 78/1000 | Loss: 0.00003873
Iteration 79/1000 | Loss: 0.00003873
Iteration 80/1000 | Loss: 0.00003873
Iteration 81/1000 | Loss: 0.00003873
Iteration 82/1000 | Loss: 0.00003872
Iteration 83/1000 | Loss: 0.00003872
Iteration 84/1000 | Loss: 0.00003872
Iteration 85/1000 | Loss: 0.00003872
Iteration 86/1000 | Loss: 0.00003871
Iteration 87/1000 | Loss: 0.00003871
Iteration 88/1000 | Loss: 0.00003871
Iteration 89/1000 | Loss: 0.00003871
Iteration 90/1000 | Loss: 0.00003871
Iteration 91/1000 | Loss: 0.00003871
Iteration 92/1000 | Loss: 0.00003870
Iteration 93/1000 | Loss: 0.00003870
Iteration 94/1000 | Loss: 0.00003870
Iteration 95/1000 | Loss: 0.00003870
Iteration 96/1000 | Loss: 0.00003870
Iteration 97/1000 | Loss: 0.00003870
Iteration 98/1000 | Loss: 0.00003870
Iteration 99/1000 | Loss: 0.00003870
Iteration 100/1000 | Loss: 0.00003870
Iteration 101/1000 | Loss: 0.00003870
Iteration 102/1000 | Loss: 0.00003869
Iteration 103/1000 | Loss: 0.00003869
Iteration 104/1000 | Loss: 0.00003869
Iteration 105/1000 | Loss: 0.00003869
Iteration 106/1000 | Loss: 0.00003869
Iteration 107/1000 | Loss: 0.00003869
Iteration 108/1000 | Loss: 0.00003869
Iteration 109/1000 | Loss: 0.00003869
Iteration 110/1000 | Loss: 0.00003868
Iteration 111/1000 | Loss: 0.00003868
Iteration 112/1000 | Loss: 0.00003868
Iteration 113/1000 | Loss: 0.00003868
Iteration 114/1000 | Loss: 0.00003868
Iteration 115/1000 | Loss: 0.00003868
Iteration 116/1000 | Loss: 0.00003867
Iteration 117/1000 | Loss: 0.00003867
Iteration 118/1000 | Loss: 0.00003867
Iteration 119/1000 | Loss: 0.00003867
Iteration 120/1000 | Loss: 0.00003867
Iteration 121/1000 | Loss: 0.00003867
Iteration 122/1000 | Loss: 0.00003867
Iteration 123/1000 | Loss: 0.00003867
Iteration 124/1000 | Loss: 0.00003867
Iteration 125/1000 | Loss: 0.00003867
Iteration 126/1000 | Loss: 0.00003866
Iteration 127/1000 | Loss: 0.00003866
Iteration 128/1000 | Loss: 0.00003866
Iteration 129/1000 | Loss: 0.00003866
Iteration 130/1000 | Loss: 0.00003865
Iteration 131/1000 | Loss: 0.00003865
Iteration 132/1000 | Loss: 0.00003865
Iteration 133/1000 | Loss: 0.00003865
Iteration 134/1000 | Loss: 0.00003865
Iteration 135/1000 | Loss: 0.00003865
Iteration 136/1000 | Loss: 0.00003865
Iteration 137/1000 | Loss: 0.00003864
Iteration 138/1000 | Loss: 0.00003864
Iteration 139/1000 | Loss: 0.00003864
Iteration 140/1000 | Loss: 0.00003864
Iteration 141/1000 | Loss: 0.00003863
Iteration 142/1000 | Loss: 0.00003863
Iteration 143/1000 | Loss: 0.00003863
Iteration 144/1000 | Loss: 0.00003863
Iteration 145/1000 | Loss: 0.00003863
Iteration 146/1000 | Loss: 0.00003863
Iteration 147/1000 | Loss: 0.00003863
Iteration 148/1000 | Loss: 0.00003862
Iteration 149/1000 | Loss: 0.00003862
Iteration 150/1000 | Loss: 0.00003862
Iteration 151/1000 | Loss: 0.00003862
Iteration 152/1000 | Loss: 0.00003862
Iteration 153/1000 | Loss: 0.00003862
Iteration 154/1000 | Loss: 0.00003862
Iteration 155/1000 | Loss: 0.00003862
Iteration 156/1000 | Loss: 0.00003862
Iteration 157/1000 | Loss: 0.00003862
Iteration 158/1000 | Loss: 0.00003862
Iteration 159/1000 | Loss: 0.00003862
Iteration 160/1000 | Loss: 0.00003862
Iteration 161/1000 | Loss: 0.00003862
Iteration 162/1000 | Loss: 0.00003861
Iteration 163/1000 | Loss: 0.00003861
Iteration 164/1000 | Loss: 0.00003861
Iteration 165/1000 | Loss: 0.00003861
Iteration 166/1000 | Loss: 0.00003861
Iteration 167/1000 | Loss: 0.00003861
Iteration 168/1000 | Loss: 0.00003861
Iteration 169/1000 | Loss: 0.00003861
Iteration 170/1000 | Loss: 0.00003861
Iteration 171/1000 | Loss: 0.00003861
Iteration 172/1000 | Loss: 0.00003861
Iteration 173/1000 | Loss: 0.00003861
Iteration 174/1000 | Loss: 0.00003861
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 174. Stopping optimization.
Last 5 losses: [3.861319419229403e-05, 3.861319419229403e-05, 3.861319419229403e-05, 3.861319419229403e-05, 3.861319419229403e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.861319419229403e-05

Optimization complete. Final v2v error: 5.445649147033691 mm

Highest mean error: 6.135902404785156 mm for frame 63

Lowest mean error: 4.9732346534729 mm for frame 6

Saving results

Total time: 83.32431292533875
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_48_us_2148/0021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_48_us_2148/0021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_48_us_2148/0021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00512177
Iteration 2/25 | Loss: 0.00214620
Iteration 3/25 | Loss: 0.00189858
Iteration 4/25 | Loss: 0.00186199
Iteration 5/25 | Loss: 0.00185407
Iteration 6/25 | Loss: 0.00185229
Iteration 7/25 | Loss: 0.00185183
Iteration 8/25 | Loss: 0.00185183
Iteration 9/25 | Loss: 0.00185183
Iteration 10/25 | Loss: 0.00185183
Iteration 11/25 | Loss: 0.00185183
Iteration 12/25 | Loss: 0.00185183
Iteration 13/25 | Loss: 0.00185183
Iteration 14/25 | Loss: 0.00185183
Iteration 15/25 | Loss: 0.00185183
Iteration 16/25 | Loss: 0.00185183
Iteration 17/25 | Loss: 0.00185183
Iteration 18/25 | Loss: 0.00185183
Iteration 19/25 | Loss: 0.00185183
Iteration 20/25 | Loss: 0.00185183
Iteration 21/25 | Loss: 0.00185183
Iteration 22/25 | Loss: 0.00185183
Iteration 23/25 | Loss: 0.00185183
Iteration 24/25 | Loss: 0.00185183
Iteration 25/25 | Loss: 0.00185183

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.49687064
Iteration 2/25 | Loss: 0.00264126
Iteration 3/25 | Loss: 0.00264126
Iteration 4/25 | Loss: 0.00264125
Iteration 5/25 | Loss: 0.00264125
Iteration 6/25 | Loss: 0.00264125
Iteration 7/25 | Loss: 0.00264125
Iteration 8/25 | Loss: 0.00264125
Iteration 9/25 | Loss: 0.00264125
Iteration 10/25 | Loss: 0.00264125
Iteration 11/25 | Loss: 0.00264125
Iteration 12/25 | Loss: 0.00264125
Iteration 13/25 | Loss: 0.00264125
Iteration 14/25 | Loss: 0.00264125
Iteration 15/25 | Loss: 0.00264125
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0026412527076900005, 0.0026412527076900005, 0.0026412527076900005, 0.0026412527076900005, 0.0026412527076900005]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0026412527076900005

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00264125
Iteration 2/1000 | Loss: 0.00011958
Iteration 3/1000 | Loss: 0.00006982
Iteration 4/1000 | Loss: 0.00005625
Iteration 5/1000 | Loss: 0.00004948
Iteration 6/1000 | Loss: 0.00004611
Iteration 7/1000 | Loss: 0.00004382
Iteration 8/1000 | Loss: 0.00004259
Iteration 9/1000 | Loss: 0.00004180
Iteration 10/1000 | Loss: 0.00004121
Iteration 11/1000 | Loss: 0.00004070
Iteration 12/1000 | Loss: 0.00004035
Iteration 13/1000 | Loss: 0.00003998
Iteration 14/1000 | Loss: 0.00003981
Iteration 15/1000 | Loss: 0.00003973
Iteration 16/1000 | Loss: 0.00003957
Iteration 17/1000 | Loss: 0.00003949
Iteration 18/1000 | Loss: 0.00003946
Iteration 19/1000 | Loss: 0.00003946
Iteration 20/1000 | Loss: 0.00003945
Iteration 21/1000 | Loss: 0.00003945
Iteration 22/1000 | Loss: 0.00003945
Iteration 23/1000 | Loss: 0.00003944
Iteration 24/1000 | Loss: 0.00003943
Iteration 25/1000 | Loss: 0.00003943
Iteration 26/1000 | Loss: 0.00003943
Iteration 27/1000 | Loss: 0.00003943
Iteration 28/1000 | Loss: 0.00003942
Iteration 29/1000 | Loss: 0.00003942
Iteration 30/1000 | Loss: 0.00003942
Iteration 31/1000 | Loss: 0.00003942
Iteration 32/1000 | Loss: 0.00003942
Iteration 33/1000 | Loss: 0.00003942
Iteration 34/1000 | Loss: 0.00003942
Iteration 35/1000 | Loss: 0.00003940
Iteration 36/1000 | Loss: 0.00003938
Iteration 37/1000 | Loss: 0.00003938
Iteration 38/1000 | Loss: 0.00003937
Iteration 39/1000 | Loss: 0.00003937
Iteration 40/1000 | Loss: 0.00003937
Iteration 41/1000 | Loss: 0.00003936
Iteration 42/1000 | Loss: 0.00003936
Iteration 43/1000 | Loss: 0.00003935
Iteration 44/1000 | Loss: 0.00003933
Iteration 45/1000 | Loss: 0.00003932
Iteration 46/1000 | Loss: 0.00003930
Iteration 47/1000 | Loss: 0.00003928
Iteration 48/1000 | Loss: 0.00003927
Iteration 49/1000 | Loss: 0.00003927
Iteration 50/1000 | Loss: 0.00003924
Iteration 51/1000 | Loss: 0.00003924
Iteration 52/1000 | Loss: 0.00003923
Iteration 53/1000 | Loss: 0.00003923
Iteration 54/1000 | Loss: 0.00003923
Iteration 55/1000 | Loss: 0.00003923
Iteration 56/1000 | Loss: 0.00003923
Iteration 57/1000 | Loss: 0.00003923
Iteration 58/1000 | Loss: 0.00003923
Iteration 59/1000 | Loss: 0.00003923
Iteration 60/1000 | Loss: 0.00003923
Iteration 61/1000 | Loss: 0.00003922
Iteration 62/1000 | Loss: 0.00003922
Iteration 63/1000 | Loss: 0.00003921
Iteration 64/1000 | Loss: 0.00003921
Iteration 65/1000 | Loss: 0.00003921
Iteration 66/1000 | Loss: 0.00003921
Iteration 67/1000 | Loss: 0.00003921
Iteration 68/1000 | Loss: 0.00003921
Iteration 69/1000 | Loss: 0.00003920
Iteration 70/1000 | Loss: 0.00003920
Iteration 71/1000 | Loss: 0.00003920
Iteration 72/1000 | Loss: 0.00003920
Iteration 73/1000 | Loss: 0.00003920
Iteration 74/1000 | Loss: 0.00003920
Iteration 75/1000 | Loss: 0.00003920
Iteration 76/1000 | Loss: 0.00003920
Iteration 77/1000 | Loss: 0.00003919
Iteration 78/1000 | Loss: 0.00003919
Iteration 79/1000 | Loss: 0.00003919
Iteration 80/1000 | Loss: 0.00003919
Iteration 81/1000 | Loss: 0.00003919
Iteration 82/1000 | Loss: 0.00003918
Iteration 83/1000 | Loss: 0.00003918
Iteration 84/1000 | Loss: 0.00003918
Iteration 85/1000 | Loss: 0.00003918
Iteration 86/1000 | Loss: 0.00003918
Iteration 87/1000 | Loss: 0.00003918
Iteration 88/1000 | Loss: 0.00003918
Iteration 89/1000 | Loss: 0.00003918
Iteration 90/1000 | Loss: 0.00003918
Iteration 91/1000 | Loss: 0.00003918
Iteration 92/1000 | Loss: 0.00003918
Iteration 93/1000 | Loss: 0.00003917
Iteration 94/1000 | Loss: 0.00003917
Iteration 95/1000 | Loss: 0.00003917
Iteration 96/1000 | Loss: 0.00003917
Iteration 97/1000 | Loss: 0.00003917
Iteration 98/1000 | Loss: 0.00003917
Iteration 99/1000 | Loss: 0.00003916
Iteration 100/1000 | Loss: 0.00003916
Iteration 101/1000 | Loss: 0.00003915
Iteration 102/1000 | Loss: 0.00003915
Iteration 103/1000 | Loss: 0.00003915
Iteration 104/1000 | Loss: 0.00003915
Iteration 105/1000 | Loss: 0.00003914
Iteration 106/1000 | Loss: 0.00003914
Iteration 107/1000 | Loss: 0.00003914
Iteration 108/1000 | Loss: 0.00003914
Iteration 109/1000 | Loss: 0.00003914
Iteration 110/1000 | Loss: 0.00003914
Iteration 111/1000 | Loss: 0.00003914
Iteration 112/1000 | Loss: 0.00003914
Iteration 113/1000 | Loss: 0.00003914
Iteration 114/1000 | Loss: 0.00003913
Iteration 115/1000 | Loss: 0.00003913
Iteration 116/1000 | Loss: 0.00003913
Iteration 117/1000 | Loss: 0.00003913
Iteration 118/1000 | Loss: 0.00003912
Iteration 119/1000 | Loss: 0.00003912
Iteration 120/1000 | Loss: 0.00003912
Iteration 121/1000 | Loss: 0.00003912
Iteration 122/1000 | Loss: 0.00003912
Iteration 123/1000 | Loss: 0.00003912
Iteration 124/1000 | Loss: 0.00003912
Iteration 125/1000 | Loss: 0.00003912
Iteration 126/1000 | Loss: 0.00003912
Iteration 127/1000 | Loss: 0.00003912
Iteration 128/1000 | Loss: 0.00003912
Iteration 129/1000 | Loss: 0.00003912
Iteration 130/1000 | Loss: 0.00003912
Iteration 131/1000 | Loss: 0.00003912
Iteration 132/1000 | Loss: 0.00003912
Iteration 133/1000 | Loss: 0.00003912
Iteration 134/1000 | Loss: 0.00003912
Iteration 135/1000 | Loss: 0.00003911
Iteration 136/1000 | Loss: 0.00003911
Iteration 137/1000 | Loss: 0.00003911
Iteration 138/1000 | Loss: 0.00003911
Iteration 139/1000 | Loss: 0.00003911
Iteration 140/1000 | Loss: 0.00003911
Iteration 141/1000 | Loss: 0.00003911
Iteration 142/1000 | Loss: 0.00003911
Iteration 143/1000 | Loss: 0.00003911
Iteration 144/1000 | Loss: 0.00003911
Iteration 145/1000 | Loss: 0.00003911
Iteration 146/1000 | Loss: 0.00003911
Iteration 147/1000 | Loss: 0.00003911
Iteration 148/1000 | Loss: 0.00003911
Iteration 149/1000 | Loss: 0.00003910
Iteration 150/1000 | Loss: 0.00003910
Iteration 151/1000 | Loss: 0.00003910
Iteration 152/1000 | Loss: 0.00003910
Iteration 153/1000 | Loss: 0.00003910
Iteration 154/1000 | Loss: 0.00003910
Iteration 155/1000 | Loss: 0.00003910
Iteration 156/1000 | Loss: 0.00003910
Iteration 157/1000 | Loss: 0.00003910
Iteration 158/1000 | Loss: 0.00003910
Iteration 159/1000 | Loss: 0.00003910
Iteration 160/1000 | Loss: 0.00003910
Iteration 161/1000 | Loss: 0.00003910
Iteration 162/1000 | Loss: 0.00003910
Iteration 163/1000 | Loss: 0.00003910
Iteration 164/1000 | Loss: 0.00003910
Iteration 165/1000 | Loss: 0.00003910
Iteration 166/1000 | Loss: 0.00003910
Iteration 167/1000 | Loss: 0.00003910
Iteration 168/1000 | Loss: 0.00003910
Iteration 169/1000 | Loss: 0.00003909
Iteration 170/1000 | Loss: 0.00003909
Iteration 171/1000 | Loss: 0.00003909
Iteration 172/1000 | Loss: 0.00003909
Iteration 173/1000 | Loss: 0.00003909
Iteration 174/1000 | Loss: 0.00003909
Iteration 175/1000 | Loss: 0.00003909
Iteration 176/1000 | Loss: 0.00003909
Iteration 177/1000 | Loss: 0.00003909
Iteration 178/1000 | Loss: 0.00003909
Iteration 179/1000 | Loss: 0.00003909
Iteration 180/1000 | Loss: 0.00003909
Iteration 181/1000 | Loss: 0.00003909
Iteration 182/1000 | Loss: 0.00003909
Iteration 183/1000 | Loss: 0.00003909
Iteration 184/1000 | Loss: 0.00003909
Iteration 185/1000 | Loss: 0.00003909
Iteration 186/1000 | Loss: 0.00003909
Iteration 187/1000 | Loss: 0.00003909
Iteration 188/1000 | Loss: 0.00003909
Iteration 189/1000 | Loss: 0.00003909
Iteration 190/1000 | Loss: 0.00003909
Iteration 191/1000 | Loss: 0.00003909
Iteration 192/1000 | Loss: 0.00003909
Iteration 193/1000 | Loss: 0.00003909
Iteration 194/1000 | Loss: 0.00003909
Iteration 195/1000 | Loss: 0.00003909
Iteration 196/1000 | Loss: 0.00003909
Iteration 197/1000 | Loss: 0.00003909
Iteration 198/1000 | Loss: 0.00003909
Iteration 199/1000 | Loss: 0.00003909
Iteration 200/1000 | Loss: 0.00003909
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 200. Stopping optimization.
Last 5 losses: [3.908639700966887e-05, 3.908639700966887e-05, 3.908639700966887e-05, 3.908639700966887e-05, 3.908639700966887e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.908639700966887e-05

Optimization complete. Final v2v error: 5.522224426269531 mm

Highest mean error: 5.793689727783203 mm for frame 73

Lowest mean error: 5.271281719207764 mm for frame 66

Saving results

Total time: 43.914193630218506
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_48_us_2148/0013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_48_us_2148/0013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_48_us_2148/0013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01076324
Iteration 2/25 | Loss: 0.00226342
Iteration 3/25 | Loss: 0.00193543
Iteration 4/25 | Loss: 0.00184054
Iteration 5/25 | Loss: 0.00179678
Iteration 6/25 | Loss: 0.00179224
Iteration 7/25 | Loss: 0.00177921
Iteration 8/25 | Loss: 0.00178423
Iteration 9/25 | Loss: 0.00177685
Iteration 10/25 | Loss: 0.00177498
Iteration 11/25 | Loss: 0.00177384
Iteration 12/25 | Loss: 0.00177278
Iteration 13/25 | Loss: 0.00177239
Iteration 14/25 | Loss: 0.00177190
Iteration 15/25 | Loss: 0.00177164
Iteration 16/25 | Loss: 0.00177142
Iteration 17/25 | Loss: 0.00177127
Iteration 18/25 | Loss: 0.00177124
Iteration 19/25 | Loss: 0.00177124
Iteration 20/25 | Loss: 0.00177124
Iteration 21/25 | Loss: 0.00177123
Iteration 22/25 | Loss: 0.00177123
Iteration 23/25 | Loss: 0.00177123
Iteration 24/25 | Loss: 0.00177123
Iteration 25/25 | Loss: 0.00177123

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.60554266
Iteration 2/25 | Loss: 0.00220300
Iteration 3/25 | Loss: 0.00220298
Iteration 4/25 | Loss: 0.00220298
Iteration 5/25 | Loss: 0.00220298
Iteration 6/25 | Loss: 0.00220298
Iteration 7/25 | Loss: 0.00220298
Iteration 8/25 | Loss: 0.00220298
Iteration 9/25 | Loss: 0.00220298
Iteration 10/25 | Loss: 0.00220298
Iteration 11/25 | Loss: 0.00220298
Iteration 12/25 | Loss: 0.00220298
Iteration 13/25 | Loss: 0.00220298
Iteration 14/25 | Loss: 0.00220298
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0022029830142855644, 0.0022029830142855644, 0.0022029830142855644, 0.0022029830142855644, 0.0022029830142855644]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0022029830142855644

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00220298
Iteration 2/1000 | Loss: 0.00011967
Iteration 3/1000 | Loss: 0.00008325
Iteration 4/1000 | Loss: 0.00007165
Iteration 5/1000 | Loss: 0.00005868
Iteration 6/1000 | Loss: 0.00013444
Iteration 7/1000 | Loss: 0.00005356
Iteration 8/1000 | Loss: 0.00005239
Iteration 9/1000 | Loss: 0.00111196
Iteration 10/1000 | Loss: 0.00006650
Iteration 11/1000 | Loss: 0.00005309
Iteration 12/1000 | Loss: 0.00004957
Iteration 13/1000 | Loss: 0.00004796
Iteration 14/1000 | Loss: 0.00004699
Iteration 15/1000 | Loss: 0.00004653
Iteration 16/1000 | Loss: 0.00004622
Iteration 17/1000 | Loss: 0.00004599
Iteration 18/1000 | Loss: 0.00006330
Iteration 19/1000 | Loss: 0.00004581
Iteration 20/1000 | Loss: 0.00004563
Iteration 21/1000 | Loss: 0.00004550
Iteration 22/1000 | Loss: 0.00004550
Iteration 23/1000 | Loss: 0.00004544
Iteration 24/1000 | Loss: 0.00004542
Iteration 25/1000 | Loss: 0.00004538
Iteration 26/1000 | Loss: 0.00004538
Iteration 27/1000 | Loss: 0.00004538
Iteration 28/1000 | Loss: 0.00004533
Iteration 29/1000 | Loss: 0.00004530
Iteration 30/1000 | Loss: 0.00004528
Iteration 31/1000 | Loss: 0.00004528
Iteration 32/1000 | Loss: 0.00004527
Iteration 33/1000 | Loss: 0.00004527
Iteration 34/1000 | Loss: 0.00004527
Iteration 35/1000 | Loss: 0.00004526
Iteration 36/1000 | Loss: 0.00004526
Iteration 37/1000 | Loss: 0.00004526
Iteration 38/1000 | Loss: 0.00004526
Iteration 39/1000 | Loss: 0.00004526
Iteration 40/1000 | Loss: 0.00004526
Iteration 41/1000 | Loss: 0.00004526
Iteration 42/1000 | Loss: 0.00004526
Iteration 43/1000 | Loss: 0.00004525
Iteration 44/1000 | Loss: 0.00004525
Iteration 45/1000 | Loss: 0.00004525
Iteration 46/1000 | Loss: 0.00004525
Iteration 47/1000 | Loss: 0.00004525
Iteration 48/1000 | Loss: 0.00004525
Iteration 49/1000 | Loss: 0.00004525
Iteration 50/1000 | Loss: 0.00004525
Iteration 51/1000 | Loss: 0.00004525
Iteration 52/1000 | Loss: 0.00004525
Iteration 53/1000 | Loss: 0.00004525
Iteration 54/1000 | Loss: 0.00004525
Iteration 55/1000 | Loss: 0.00004524
Iteration 56/1000 | Loss: 0.00004524
Iteration 57/1000 | Loss: 0.00004524
Iteration 58/1000 | Loss: 0.00004524
Iteration 59/1000 | Loss: 0.00004524
Iteration 60/1000 | Loss: 0.00004524
Iteration 61/1000 | Loss: 0.00004524
Iteration 62/1000 | Loss: 0.00004524
Iteration 63/1000 | Loss: 0.00004524
Iteration 64/1000 | Loss: 0.00004524
Iteration 65/1000 | Loss: 0.00004524
Iteration 66/1000 | Loss: 0.00004524
Iteration 67/1000 | Loss: 0.00004523
Iteration 68/1000 | Loss: 0.00004523
Iteration 69/1000 | Loss: 0.00004523
Iteration 70/1000 | Loss: 0.00004523
Iteration 71/1000 | Loss: 0.00004522
Iteration 72/1000 | Loss: 0.00004522
Iteration 73/1000 | Loss: 0.00004522
Iteration 74/1000 | Loss: 0.00004522
Iteration 75/1000 | Loss: 0.00004522
Iteration 76/1000 | Loss: 0.00004522
Iteration 77/1000 | Loss: 0.00004522
Iteration 78/1000 | Loss: 0.00004521
Iteration 79/1000 | Loss: 0.00004521
Iteration 80/1000 | Loss: 0.00004521
Iteration 81/1000 | Loss: 0.00004521
Iteration 82/1000 | Loss: 0.00004521
Iteration 83/1000 | Loss: 0.00004521
Iteration 84/1000 | Loss: 0.00004521
Iteration 85/1000 | Loss: 0.00004521
Iteration 86/1000 | Loss: 0.00004521
Iteration 87/1000 | Loss: 0.00004521
Iteration 88/1000 | Loss: 0.00004521
Iteration 89/1000 | Loss: 0.00004521
Iteration 90/1000 | Loss: 0.00004521
Iteration 91/1000 | Loss: 0.00004521
Iteration 92/1000 | Loss: 0.00004521
Iteration 93/1000 | Loss: 0.00004521
Iteration 94/1000 | Loss: 0.00004521
Iteration 95/1000 | Loss: 0.00004521
Iteration 96/1000 | Loss: 0.00004521
Iteration 97/1000 | Loss: 0.00004520
Iteration 98/1000 | Loss: 0.00004520
Iteration 99/1000 | Loss: 0.00004520
Iteration 100/1000 | Loss: 0.00004520
Iteration 101/1000 | Loss: 0.00004520
Iteration 102/1000 | Loss: 0.00004520
Iteration 103/1000 | Loss: 0.00006484
Iteration 104/1000 | Loss: 0.00004523
Iteration 105/1000 | Loss: 0.00004522
Iteration 106/1000 | Loss: 0.00005093
Iteration 107/1000 | Loss: 0.00004523
Iteration 108/1000 | Loss: 0.00004523
Iteration 109/1000 | Loss: 0.00004523
Iteration 110/1000 | Loss: 0.00004523
Iteration 111/1000 | Loss: 0.00004523
Iteration 112/1000 | Loss: 0.00004523
Iteration 113/1000 | Loss: 0.00004620
Iteration 114/1000 | Loss: 0.00004522
Iteration 115/1000 | Loss: 0.00004521
Iteration 116/1000 | Loss: 0.00004521
Iteration 117/1000 | Loss: 0.00004521
Iteration 118/1000 | Loss: 0.00004521
Iteration 119/1000 | Loss: 0.00004521
Iteration 120/1000 | Loss: 0.00004521
Iteration 121/1000 | Loss: 0.00004521
Iteration 122/1000 | Loss: 0.00004521
Iteration 123/1000 | Loss: 0.00004521
Iteration 124/1000 | Loss: 0.00004521
Iteration 125/1000 | Loss: 0.00004521
Iteration 126/1000 | Loss: 0.00004521
Iteration 127/1000 | Loss: 0.00004520
Iteration 128/1000 | Loss: 0.00004520
Iteration 129/1000 | Loss: 0.00004520
Iteration 130/1000 | Loss: 0.00004520
Iteration 131/1000 | Loss: 0.00004519
Iteration 132/1000 | Loss: 0.00004519
Iteration 133/1000 | Loss: 0.00004519
Iteration 134/1000 | Loss: 0.00004519
Iteration 135/1000 | Loss: 0.00004519
Iteration 136/1000 | Loss: 0.00004519
Iteration 137/1000 | Loss: 0.00004519
Iteration 138/1000 | Loss: 0.00004519
Iteration 139/1000 | Loss: 0.00004519
Iteration 140/1000 | Loss: 0.00004519
Iteration 141/1000 | Loss: 0.00004519
Iteration 142/1000 | Loss: 0.00004519
Iteration 143/1000 | Loss: 0.00004519
Iteration 144/1000 | Loss: 0.00004519
Iteration 145/1000 | Loss: 0.00004519
Iteration 146/1000 | Loss: 0.00004519
Iteration 147/1000 | Loss: 0.00004519
Iteration 148/1000 | Loss: 0.00004519
Iteration 149/1000 | Loss: 0.00004519
Iteration 150/1000 | Loss: 0.00004519
Iteration 151/1000 | Loss: 0.00004519
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 151. Stopping optimization.
Last 5 losses: [4.5191864046500996e-05, 4.5191864046500996e-05, 4.5191864046500996e-05, 4.5191864046500996e-05, 4.5191864046500996e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.5191864046500996e-05

Optimization complete. Final v2v error: 5.826809406280518 mm

Highest mean error: 12.443917274475098 mm for frame 24

Lowest mean error: 5.131720066070557 mm for frame 170

Saving results

Total time: 74.98933386802673
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_48_us_2148/0016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_48_us_2148/0016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_48_us_2148/0016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01103178
Iteration 2/25 | Loss: 0.00313985
Iteration 3/25 | Loss: 0.00243179
Iteration 4/25 | Loss: 0.00240250
Iteration 5/25 | Loss: 0.00234918
Iteration 6/25 | Loss: 0.00221885
Iteration 7/25 | Loss: 0.00218481
Iteration 8/25 | Loss: 0.00218557
Iteration 9/25 | Loss: 0.00217010
Iteration 10/25 | Loss: 0.00215966
Iteration 11/25 | Loss: 0.00214741
Iteration 12/25 | Loss: 0.00210820
Iteration 13/25 | Loss: 0.00210115
Iteration 14/25 | Loss: 0.00210240
Iteration 15/25 | Loss: 0.00209878
Iteration 16/25 | Loss: 0.00209902
Iteration 17/25 | Loss: 0.00209758
Iteration 18/25 | Loss: 0.00209713
Iteration 19/25 | Loss: 0.00209704
Iteration 20/25 | Loss: 0.00209703
Iteration 21/25 | Loss: 0.00209703
Iteration 22/25 | Loss: 0.00209703
Iteration 23/25 | Loss: 0.00209703
Iteration 24/25 | Loss: 0.00209702
Iteration 25/25 | Loss: 0.00209695

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.85537535
Iteration 2/25 | Loss: 0.00369749
Iteration 3/25 | Loss: 0.00338736
Iteration 4/25 | Loss: 0.00338736
Iteration 5/25 | Loss: 0.00338736
Iteration 6/25 | Loss: 0.00338736
Iteration 7/25 | Loss: 0.00338736
Iteration 8/25 | Loss: 0.00338735
Iteration 9/25 | Loss: 0.00338735
Iteration 10/25 | Loss: 0.00338736
Iteration 11/25 | Loss: 0.00338735
Iteration 12/25 | Loss: 0.00338735
Iteration 13/25 | Loss: 0.00338735
Iteration 14/25 | Loss: 0.00338736
Iteration 15/25 | Loss: 0.00338736
Iteration 16/25 | Loss: 0.00338736
Iteration 17/25 | Loss: 0.00338736
Iteration 18/25 | Loss: 0.00338736
Iteration 19/25 | Loss: 0.00338736
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0033873554784804583, 0.0033873554784804583, 0.0033873554784804583, 0.0033873554784804583, 0.0033873554784804583]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0033873554784804583

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00338736
Iteration 2/1000 | Loss: 0.00042070
Iteration 3/1000 | Loss: 0.00235291
Iteration 4/1000 | Loss: 0.00033029
Iteration 5/1000 | Loss: 0.00022599
Iteration 6/1000 | Loss: 0.00019503
Iteration 7/1000 | Loss: 0.00018252
Iteration 8/1000 | Loss: 0.00017584
Iteration 9/1000 | Loss: 0.00017207
Iteration 10/1000 | Loss: 0.00016959
Iteration 11/1000 | Loss: 0.00016729
Iteration 12/1000 | Loss: 0.00016579
Iteration 13/1000 | Loss: 0.00016460
Iteration 14/1000 | Loss: 0.01423071
Iteration 15/1000 | Loss: 0.00892423
Iteration 16/1000 | Loss: 0.00943131
Iteration 17/1000 | Loss: 0.00528837
Iteration 18/1000 | Loss: 0.00138285
Iteration 19/1000 | Loss: 0.00515509
Iteration 20/1000 | Loss: 0.00393769
Iteration 21/1000 | Loss: 0.00710588
Iteration 22/1000 | Loss: 0.00668181
Iteration 23/1000 | Loss: 0.00738886
Iteration 24/1000 | Loss: 0.00767485
Iteration 25/1000 | Loss: 0.00543325
Iteration 26/1000 | Loss: 0.00822561
Iteration 27/1000 | Loss: 0.00369270
Iteration 28/1000 | Loss: 0.00487507
Iteration 29/1000 | Loss: 0.00277860
Iteration 30/1000 | Loss: 0.00375798
Iteration 31/1000 | Loss: 0.00391224
Iteration 32/1000 | Loss: 0.00377899
Iteration 33/1000 | Loss: 0.00451287
Iteration 34/1000 | Loss: 0.00529049
Iteration 35/1000 | Loss: 0.00447981
Iteration 36/1000 | Loss: 0.00335393
Iteration 37/1000 | Loss: 0.00139115
Iteration 38/1000 | Loss: 0.00269102
Iteration 39/1000 | Loss: 0.00176106
Iteration 40/1000 | Loss: 0.00206259
Iteration 41/1000 | Loss: 0.00150084
Iteration 42/1000 | Loss: 0.00196627
Iteration 43/1000 | Loss: 0.00125722
Iteration 44/1000 | Loss: 0.00149333
Iteration 45/1000 | Loss: 0.00139929
Iteration 46/1000 | Loss: 0.00116121
Iteration 47/1000 | Loss: 0.00063505
Iteration 48/1000 | Loss: 0.00069993
Iteration 49/1000 | Loss: 0.00131031
Iteration 50/1000 | Loss: 0.00111933
Iteration 51/1000 | Loss: 0.00124498
Iteration 52/1000 | Loss: 0.00141384
Iteration 53/1000 | Loss: 0.00066485
Iteration 54/1000 | Loss: 0.00136848
Iteration 55/1000 | Loss: 0.00284003
Iteration 56/1000 | Loss: 0.00122062
Iteration 57/1000 | Loss: 0.00170396
Iteration 58/1000 | Loss: 0.00068112
Iteration 59/1000 | Loss: 0.00149913
Iteration 60/1000 | Loss: 0.00248224
Iteration 61/1000 | Loss: 0.00399736
Iteration 62/1000 | Loss: 0.00277530
Iteration 63/1000 | Loss: 0.00246051
Iteration 64/1000 | Loss: 0.00298171
Iteration 65/1000 | Loss: 0.00262055
Iteration 66/1000 | Loss: 0.00276461
Iteration 67/1000 | Loss: 0.00191422
Iteration 68/1000 | Loss: 0.00064869
Iteration 69/1000 | Loss: 0.00109788
Iteration 70/1000 | Loss: 0.00071488
Iteration 71/1000 | Loss: 0.00106187
Iteration 72/1000 | Loss: 0.00048871
Iteration 73/1000 | Loss: 0.00022748
Iteration 74/1000 | Loss: 0.00103404
Iteration 75/1000 | Loss: 0.00060896
Iteration 76/1000 | Loss: 0.00081701
Iteration 77/1000 | Loss: 0.00047992
Iteration 78/1000 | Loss: 0.00083808
Iteration 79/1000 | Loss: 0.00077731
Iteration 80/1000 | Loss: 0.00034198
Iteration 81/1000 | Loss: 0.00065534
Iteration 82/1000 | Loss: 0.00061044
Iteration 83/1000 | Loss: 0.00023108
Iteration 84/1000 | Loss: 0.00067971
Iteration 85/1000 | Loss: 0.00069499
Iteration 86/1000 | Loss: 0.00052834
Iteration 87/1000 | Loss: 0.00018415
Iteration 88/1000 | Loss: 0.00053100
Iteration 89/1000 | Loss: 0.00023363
Iteration 90/1000 | Loss: 0.00048528
Iteration 91/1000 | Loss: 0.00025987
Iteration 92/1000 | Loss: 0.00054508
Iteration 93/1000 | Loss: 0.00018022
Iteration 94/1000 | Loss: 0.00016938
Iteration 95/1000 | Loss: 0.00022394
Iteration 96/1000 | Loss: 0.00016572
Iteration 97/1000 | Loss: 0.00015772
Iteration 98/1000 | Loss: 0.00087424
Iteration 99/1000 | Loss: 0.00077127
Iteration 100/1000 | Loss: 0.00065662
Iteration 101/1000 | Loss: 0.00022231
Iteration 102/1000 | Loss: 0.00026856
Iteration 103/1000 | Loss: 0.00127859
Iteration 104/1000 | Loss: 0.00041740
Iteration 105/1000 | Loss: 0.00016024
Iteration 106/1000 | Loss: 0.00015016
Iteration 107/1000 | Loss: 0.00017150
Iteration 108/1000 | Loss: 0.00032962
Iteration 109/1000 | Loss: 0.00013859
Iteration 110/1000 | Loss: 0.00094480
Iteration 111/1000 | Loss: 0.00054339
Iteration 112/1000 | Loss: 0.00027806
Iteration 113/1000 | Loss: 0.00056481
Iteration 114/1000 | Loss: 0.00045505
Iteration 115/1000 | Loss: 0.00059782
Iteration 116/1000 | Loss: 0.00020037
Iteration 117/1000 | Loss: 0.00018400
Iteration 118/1000 | Loss: 0.00018286
Iteration 119/1000 | Loss: 0.00068084
Iteration 120/1000 | Loss: 0.00013924
Iteration 121/1000 | Loss: 0.00014214
Iteration 122/1000 | Loss: 0.00014591
Iteration 123/1000 | Loss: 0.00013198
Iteration 124/1000 | Loss: 0.00013210
Iteration 125/1000 | Loss: 0.00026995
Iteration 126/1000 | Loss: 0.00015208
Iteration 127/1000 | Loss: 0.00018506
Iteration 128/1000 | Loss: 0.00012948
Iteration 129/1000 | Loss: 0.00035106
Iteration 130/1000 | Loss: 0.00013926
Iteration 131/1000 | Loss: 0.00013499
Iteration 132/1000 | Loss: 0.00012091
Iteration 133/1000 | Loss: 0.00012163
Iteration 134/1000 | Loss: 0.00012062
Iteration 135/1000 | Loss: 0.00031321
Iteration 136/1000 | Loss: 0.00035171
Iteration 137/1000 | Loss: 0.00031826
Iteration 138/1000 | Loss: 0.00013996
Iteration 139/1000 | Loss: 0.00028622
Iteration 140/1000 | Loss: 0.00065209
Iteration 141/1000 | Loss: 0.00038251
Iteration 142/1000 | Loss: 0.00011642
Iteration 143/1000 | Loss: 0.00037746
Iteration 144/1000 | Loss: 0.00012675
Iteration 145/1000 | Loss: 0.00011802
Iteration 146/1000 | Loss: 0.00012122
Iteration 147/1000 | Loss: 0.00010280
Iteration 148/1000 | Loss: 0.00012409
Iteration 149/1000 | Loss: 0.00012187
Iteration 150/1000 | Loss: 0.00013404
Iteration 151/1000 | Loss: 0.00011678
Iteration 152/1000 | Loss: 0.00012584
Iteration 153/1000 | Loss: 0.00011617
Iteration 154/1000 | Loss: 0.00012867
Iteration 155/1000 | Loss: 0.00011974
Iteration 156/1000 | Loss: 0.00011711
Iteration 157/1000 | Loss: 0.00012759
Iteration 158/1000 | Loss: 0.00010781
Iteration 159/1000 | Loss: 0.00012885
Iteration 160/1000 | Loss: 0.00011112
Iteration 161/1000 | Loss: 0.00011327
Iteration 162/1000 | Loss: 0.00011459
Iteration 163/1000 | Loss: 0.00010927
Iteration 164/1000 | Loss: 0.00011485
Iteration 165/1000 | Loss: 0.00011860
Iteration 166/1000 | Loss: 0.00012053
Iteration 167/1000 | Loss: 0.00012481
Iteration 168/1000 | Loss: 0.00012746
Iteration 169/1000 | Loss: 0.00012016
Iteration 170/1000 | Loss: 0.00012022
Iteration 171/1000 | Loss: 0.00025204
Iteration 172/1000 | Loss: 0.00016109
Iteration 173/1000 | Loss: 0.00015701
Iteration 174/1000 | Loss: 0.00011596
Iteration 175/1000 | Loss: 0.00011579
Iteration 176/1000 | Loss: 0.00012725
Iteration 177/1000 | Loss: 0.00011781
Iteration 178/1000 | Loss: 0.00011797
Iteration 179/1000 | Loss: 0.00010978
Iteration 180/1000 | Loss: 0.00011317
Iteration 181/1000 | Loss: 0.00010539
Iteration 182/1000 | Loss: 0.00010763
Iteration 183/1000 | Loss: 0.00010325
Iteration 184/1000 | Loss: 0.00009546
Iteration 185/1000 | Loss: 0.00009271
Iteration 186/1000 | Loss: 0.00009846
Iteration 187/1000 | Loss: 0.00009949
Iteration 188/1000 | Loss: 0.00010379
Iteration 189/1000 | Loss: 0.00012227
Iteration 190/1000 | Loss: 0.00009980
Iteration 191/1000 | Loss: 0.00011818
Iteration 192/1000 | Loss: 0.00010327
Iteration 193/1000 | Loss: 0.00012121
Iteration 194/1000 | Loss: 0.00010449
Iteration 195/1000 | Loss: 0.00010412
Iteration 196/1000 | Loss: 0.00010197
Iteration 197/1000 | Loss: 0.00010126
Iteration 198/1000 | Loss: 0.00010100
Iteration 199/1000 | Loss: 0.00009947
Iteration 200/1000 | Loss: 0.00009924
Iteration 201/1000 | Loss: 0.00010425
Iteration 202/1000 | Loss: 0.00010019
Iteration 203/1000 | Loss: 0.00010356
Iteration 204/1000 | Loss: 0.00010292
Iteration 205/1000 | Loss: 0.00009310
Iteration 206/1000 | Loss: 0.00010038
Iteration 207/1000 | Loss: 0.00010326
Iteration 208/1000 | Loss: 0.00010337
Iteration 209/1000 | Loss: 0.00010313
Iteration 210/1000 | Loss: 0.00011591
Iteration 211/1000 | Loss: 0.00009799
Iteration 212/1000 | Loss: 0.00009246
Iteration 213/1000 | Loss: 0.00009061
Iteration 214/1000 | Loss: 0.00008920
Iteration 215/1000 | Loss: 0.00008847
Iteration 216/1000 | Loss: 0.00008790
Iteration 217/1000 | Loss: 0.00008774
Iteration 218/1000 | Loss: 0.00008770
Iteration 219/1000 | Loss: 0.00008769
Iteration 220/1000 | Loss: 0.00008769
Iteration 221/1000 | Loss: 0.00008769
Iteration 222/1000 | Loss: 0.00008768
Iteration 223/1000 | Loss: 0.00008768
Iteration 224/1000 | Loss: 0.00008766
Iteration 225/1000 | Loss: 0.00008766
Iteration 226/1000 | Loss: 0.00008766
Iteration 227/1000 | Loss: 0.00008754
Iteration 228/1000 | Loss: 0.00009420
Iteration 229/1000 | Loss: 0.00009420
Iteration 230/1000 | Loss: 0.00009371
Iteration 231/1000 | Loss: 0.00009188
Iteration 232/1000 | Loss: 0.00009042
Iteration 233/1000 | Loss: 0.00008991
Iteration 234/1000 | Loss: 0.00008942
Iteration 235/1000 | Loss: 0.00008915
Iteration 236/1000 | Loss: 0.00009353
Iteration 237/1000 | Loss: 0.00009102
Iteration 238/1000 | Loss: 0.00009230
Iteration 239/1000 | Loss: 0.00009247
Iteration 240/1000 | Loss: 0.00009061
Iteration 241/1000 | Loss: 0.00008903
Iteration 242/1000 | Loss: 0.00009725
Iteration 243/1000 | Loss: 0.00009106
Iteration 244/1000 | Loss: 0.00008937
Iteration 245/1000 | Loss: 0.00008841
Iteration 246/1000 | Loss: 0.00008760
Iteration 247/1000 | Loss: 0.00008725
Iteration 248/1000 | Loss: 0.00008723
Iteration 249/1000 | Loss: 0.00008706
Iteration 250/1000 | Loss: 0.00008705
Iteration 251/1000 | Loss: 0.00008704
Iteration 252/1000 | Loss: 0.00008704
Iteration 253/1000 | Loss: 0.00008704
Iteration 254/1000 | Loss: 0.00008703
Iteration 255/1000 | Loss: 0.00008702
Iteration 256/1000 | Loss: 0.00008702
Iteration 257/1000 | Loss: 0.00008699
Iteration 258/1000 | Loss: 0.00008698
Iteration 259/1000 | Loss: 0.00008697
Iteration 260/1000 | Loss: 0.00008696
Iteration 261/1000 | Loss: 0.00008695
Iteration 262/1000 | Loss: 0.00008695
Iteration 263/1000 | Loss: 0.00008694
Iteration 264/1000 | Loss: 0.00008693
Iteration 265/1000 | Loss: 0.00008691
Iteration 266/1000 | Loss: 0.00008690
Iteration 267/1000 | Loss: 0.00008690
Iteration 268/1000 | Loss: 0.00008689
Iteration 269/1000 | Loss: 0.00008685
Iteration 270/1000 | Loss: 0.00008685
Iteration 271/1000 | Loss: 0.00008684
Iteration 272/1000 | Loss: 0.00008684
Iteration 273/1000 | Loss: 0.00008683
Iteration 274/1000 | Loss: 0.00008683
Iteration 275/1000 | Loss: 0.00008682
Iteration 276/1000 | Loss: 0.00008682
Iteration 277/1000 | Loss: 0.00008682
Iteration 278/1000 | Loss: 0.00008682
Iteration 279/1000 | Loss: 0.00008682
Iteration 280/1000 | Loss: 0.00008682
Iteration 281/1000 | Loss: 0.00008682
Iteration 282/1000 | Loss: 0.00008682
Iteration 283/1000 | Loss: 0.00008682
Iteration 284/1000 | Loss: 0.00008682
Iteration 285/1000 | Loss: 0.00008681
Iteration 286/1000 | Loss: 0.00008681
Iteration 287/1000 | Loss: 0.00008681
Iteration 288/1000 | Loss: 0.00008681
Iteration 289/1000 | Loss: 0.00008681
Iteration 290/1000 | Loss: 0.00008681
Iteration 291/1000 | Loss: 0.00008681
Iteration 292/1000 | Loss: 0.00008681
Iteration 293/1000 | Loss: 0.00008680
Iteration 294/1000 | Loss: 0.00008680
Iteration 295/1000 | Loss: 0.00008679
Iteration 296/1000 | Loss: 0.00008679
Iteration 297/1000 | Loss: 0.00008679
Iteration 298/1000 | Loss: 0.00008679
Iteration 299/1000 | Loss: 0.00008678
Iteration 300/1000 | Loss: 0.00008678
Iteration 301/1000 | Loss: 0.00008678
Iteration 302/1000 | Loss: 0.00008678
Iteration 303/1000 | Loss: 0.00008678
Iteration 304/1000 | Loss: 0.00008678
Iteration 305/1000 | Loss: 0.00008678
Iteration 306/1000 | Loss: 0.00008678
Iteration 307/1000 | Loss: 0.00008677
Iteration 308/1000 | Loss: 0.00008677
Iteration 309/1000 | Loss: 0.00008677
Iteration 310/1000 | Loss: 0.00008677
Iteration 311/1000 | Loss: 0.00008677
Iteration 312/1000 | Loss: 0.00008676
Iteration 313/1000 | Loss: 0.00008676
Iteration 314/1000 | Loss: 0.00008676
Iteration 315/1000 | Loss: 0.00008676
Iteration 316/1000 | Loss: 0.00008676
Iteration 317/1000 | Loss: 0.00008676
Iteration 318/1000 | Loss: 0.00008676
Iteration 319/1000 | Loss: 0.00008676
Iteration 320/1000 | Loss: 0.00008675
Iteration 321/1000 | Loss: 0.00008675
Iteration 322/1000 | Loss: 0.00008675
Iteration 323/1000 | Loss: 0.00008675
Iteration 324/1000 | Loss: 0.00008675
Iteration 325/1000 | Loss: 0.00008674
Iteration 326/1000 | Loss: 0.00008674
Iteration 327/1000 | Loss: 0.00008674
Iteration 328/1000 | Loss: 0.00008673
Iteration 329/1000 | Loss: 0.00008673
Iteration 330/1000 | Loss: 0.00008673
Iteration 331/1000 | Loss: 0.00008673
Iteration 332/1000 | Loss: 0.00008672
Iteration 333/1000 | Loss: 0.00008672
Iteration 334/1000 | Loss: 0.00008672
Iteration 335/1000 | Loss: 0.00008672
Iteration 336/1000 | Loss: 0.00008672
Iteration 337/1000 | Loss: 0.00008672
Iteration 338/1000 | Loss: 0.00008672
Iteration 339/1000 | Loss: 0.00008672
Iteration 340/1000 | Loss: 0.00008672
Iteration 341/1000 | Loss: 0.00008672
Iteration 342/1000 | Loss: 0.00008672
Iteration 343/1000 | Loss: 0.00008672
Iteration 344/1000 | Loss: 0.00008672
Iteration 345/1000 | Loss: 0.00008672
Iteration 346/1000 | Loss: 0.00008672
Iteration 347/1000 | Loss: 0.00008672
Iteration 348/1000 | Loss: 0.00008672
Iteration 349/1000 | Loss: 0.00008672
Iteration 350/1000 | Loss: 0.00008672
Iteration 351/1000 | Loss: 0.00008672
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 351. Stopping optimization.
Last 5 losses: [8.671879186294973e-05, 8.671879186294973e-05, 8.671879186294973e-05, 8.671879186294973e-05, 8.671879186294973e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.671879186294973e-05

Optimization complete. Final v2v error: 7.207800388336182 mm

Highest mean error: 15.039972305297852 mm for frame 32

Lowest mean error: 5.622837543487549 mm for frame 109

Saving results

Total time: 425.1928188800812
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_48_us_2148/0001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_48_us_2148/0001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_48_us_2148/0001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00513036
Iteration 2/25 | Loss: 0.00193580
Iteration 3/25 | Loss: 0.00185854
Iteration 4/25 | Loss: 0.00184965
Iteration 5/25 | Loss: 0.00184644
Iteration 6/25 | Loss: 0.00184559
Iteration 7/25 | Loss: 0.00184559
Iteration 8/25 | Loss: 0.00184559
Iteration 9/25 | Loss: 0.00184559
Iteration 10/25 | Loss: 0.00184559
Iteration 11/25 | Loss: 0.00184559
Iteration 12/25 | Loss: 0.00184559
Iteration 13/25 | Loss: 0.00184559
Iteration 14/25 | Loss: 0.00184559
Iteration 15/25 | Loss: 0.00184559
Iteration 16/25 | Loss: 0.00184559
Iteration 17/25 | Loss: 0.00184559
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0018455932149663568, 0.0018455932149663568, 0.0018455932149663568, 0.0018455932149663568, 0.0018455932149663568]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0018455932149663568

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.63059139
Iteration 2/25 | Loss: 0.00228923
Iteration 3/25 | Loss: 0.00228922
Iteration 4/25 | Loss: 0.00228922
Iteration 5/25 | Loss: 0.00228922
Iteration 6/25 | Loss: 0.00228922
Iteration 7/25 | Loss: 0.00228921
Iteration 8/25 | Loss: 0.00228921
Iteration 9/25 | Loss: 0.00228921
Iteration 10/25 | Loss: 0.00228921
Iteration 11/25 | Loss: 0.00228921
Iteration 12/25 | Loss: 0.00228921
Iteration 13/25 | Loss: 0.00228921
Iteration 14/25 | Loss: 0.00228921
Iteration 15/25 | Loss: 0.00228921
Iteration 16/25 | Loss: 0.00228921
Iteration 17/25 | Loss: 0.00228921
Iteration 18/25 | Loss: 0.00228921
Iteration 19/25 | Loss: 0.00228921
Iteration 20/25 | Loss: 0.00228921
Iteration 21/25 | Loss: 0.00228921
Iteration 22/25 | Loss: 0.00228921
Iteration 23/25 | Loss: 0.00228921
Iteration 24/25 | Loss: 0.00228921
Iteration 25/25 | Loss: 0.00228921
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0022892141714692116, 0.0022892141714692116, 0.0022892141714692116, 0.0022892141714692116, 0.0022892141714692116]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0022892141714692116

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00228921
Iteration 2/1000 | Loss: 0.00008115
Iteration 3/1000 | Loss: 0.00006010
Iteration 4/1000 | Loss: 0.00005156
Iteration 5/1000 | Loss: 0.00004836
Iteration 6/1000 | Loss: 0.00004655
Iteration 7/1000 | Loss: 0.00004558
Iteration 8/1000 | Loss: 0.00004482
Iteration 9/1000 | Loss: 0.00004436
Iteration 10/1000 | Loss: 0.00004392
Iteration 11/1000 | Loss: 0.00004363
Iteration 12/1000 | Loss: 0.00004340
Iteration 13/1000 | Loss: 0.00004332
Iteration 14/1000 | Loss: 0.00004326
Iteration 15/1000 | Loss: 0.00004323
Iteration 16/1000 | Loss: 0.00004322
Iteration 17/1000 | Loss: 0.00004322
Iteration 18/1000 | Loss: 0.00004321
Iteration 19/1000 | Loss: 0.00004321
Iteration 20/1000 | Loss: 0.00004318
Iteration 21/1000 | Loss: 0.00004316
Iteration 22/1000 | Loss: 0.00004316
Iteration 23/1000 | Loss: 0.00004316
Iteration 24/1000 | Loss: 0.00004316
Iteration 25/1000 | Loss: 0.00004313
Iteration 26/1000 | Loss: 0.00004313
Iteration 27/1000 | Loss: 0.00004311
Iteration 28/1000 | Loss: 0.00004309
Iteration 29/1000 | Loss: 0.00004309
Iteration 30/1000 | Loss: 0.00004308
Iteration 31/1000 | Loss: 0.00004308
Iteration 32/1000 | Loss: 0.00004308
Iteration 33/1000 | Loss: 0.00004307
Iteration 34/1000 | Loss: 0.00004307
Iteration 35/1000 | Loss: 0.00004305
Iteration 36/1000 | Loss: 0.00004305
Iteration 37/1000 | Loss: 0.00004305
Iteration 38/1000 | Loss: 0.00004305
Iteration 39/1000 | Loss: 0.00004304
Iteration 40/1000 | Loss: 0.00004304
Iteration 41/1000 | Loss: 0.00004304
Iteration 42/1000 | Loss: 0.00004302
Iteration 43/1000 | Loss: 0.00004302
Iteration 44/1000 | Loss: 0.00004302
Iteration 45/1000 | Loss: 0.00004302
Iteration 46/1000 | Loss: 0.00004302
Iteration 47/1000 | Loss: 0.00004302
Iteration 48/1000 | Loss: 0.00004302
Iteration 49/1000 | Loss: 0.00004301
Iteration 50/1000 | Loss: 0.00004301
Iteration 51/1000 | Loss: 0.00004301
Iteration 52/1000 | Loss: 0.00004300
Iteration 53/1000 | Loss: 0.00004300
Iteration 54/1000 | Loss: 0.00004300
Iteration 55/1000 | Loss: 0.00004298
Iteration 56/1000 | Loss: 0.00004298
Iteration 57/1000 | Loss: 0.00004298
Iteration 58/1000 | Loss: 0.00004298
Iteration 59/1000 | Loss: 0.00004298
Iteration 60/1000 | Loss: 0.00004298
Iteration 61/1000 | Loss: 0.00004298
Iteration 62/1000 | Loss: 0.00004298
Iteration 63/1000 | Loss: 0.00004298
Iteration 64/1000 | Loss: 0.00004298
Iteration 65/1000 | Loss: 0.00004297
Iteration 66/1000 | Loss: 0.00004297
Iteration 67/1000 | Loss: 0.00004297
Iteration 68/1000 | Loss: 0.00004296
Iteration 69/1000 | Loss: 0.00004296
Iteration 70/1000 | Loss: 0.00004296
Iteration 71/1000 | Loss: 0.00004296
Iteration 72/1000 | Loss: 0.00004295
Iteration 73/1000 | Loss: 0.00004295
Iteration 74/1000 | Loss: 0.00004295
Iteration 75/1000 | Loss: 0.00004295
Iteration 76/1000 | Loss: 0.00004295
Iteration 77/1000 | Loss: 0.00004295
Iteration 78/1000 | Loss: 0.00004295
Iteration 79/1000 | Loss: 0.00004295
Iteration 80/1000 | Loss: 0.00004294
Iteration 81/1000 | Loss: 0.00004294
Iteration 82/1000 | Loss: 0.00004294
Iteration 83/1000 | Loss: 0.00004294
Iteration 84/1000 | Loss: 0.00004294
Iteration 85/1000 | Loss: 0.00004294
Iteration 86/1000 | Loss: 0.00004293
Iteration 87/1000 | Loss: 0.00004293
Iteration 88/1000 | Loss: 0.00004293
Iteration 89/1000 | Loss: 0.00004293
Iteration 90/1000 | Loss: 0.00004293
Iteration 91/1000 | Loss: 0.00004293
Iteration 92/1000 | Loss: 0.00004293
Iteration 93/1000 | Loss: 0.00004293
Iteration 94/1000 | Loss: 0.00004293
Iteration 95/1000 | Loss: 0.00004293
Iteration 96/1000 | Loss: 0.00004293
Iteration 97/1000 | Loss: 0.00004293
Iteration 98/1000 | Loss: 0.00004293
Iteration 99/1000 | Loss: 0.00004293
Iteration 100/1000 | Loss: 0.00004293
Iteration 101/1000 | Loss: 0.00004293
Iteration 102/1000 | Loss: 0.00004292
Iteration 103/1000 | Loss: 0.00004292
Iteration 104/1000 | Loss: 0.00004292
Iteration 105/1000 | Loss: 0.00004292
Iteration 106/1000 | Loss: 0.00004292
Iteration 107/1000 | Loss: 0.00004292
Iteration 108/1000 | Loss: 0.00004292
Iteration 109/1000 | Loss: 0.00004292
Iteration 110/1000 | Loss: 0.00004292
Iteration 111/1000 | Loss: 0.00004292
Iteration 112/1000 | Loss: 0.00004292
Iteration 113/1000 | Loss: 0.00004292
Iteration 114/1000 | Loss: 0.00004292
Iteration 115/1000 | Loss: 0.00004292
Iteration 116/1000 | Loss: 0.00004292
Iteration 117/1000 | Loss: 0.00004292
Iteration 118/1000 | Loss: 0.00004292
Iteration 119/1000 | Loss: 0.00004292
Iteration 120/1000 | Loss: 0.00004292
Iteration 121/1000 | Loss: 0.00004292
Iteration 122/1000 | Loss: 0.00004292
Iteration 123/1000 | Loss: 0.00004292
Iteration 124/1000 | Loss: 0.00004292
Iteration 125/1000 | Loss: 0.00004292
Iteration 126/1000 | Loss: 0.00004292
Iteration 127/1000 | Loss: 0.00004292
Iteration 128/1000 | Loss: 0.00004292
Iteration 129/1000 | Loss: 0.00004292
Iteration 130/1000 | Loss: 0.00004292
Iteration 131/1000 | Loss: 0.00004292
Iteration 132/1000 | Loss: 0.00004292
Iteration 133/1000 | Loss: 0.00004292
Iteration 134/1000 | Loss: 0.00004292
Iteration 135/1000 | Loss: 0.00004292
Iteration 136/1000 | Loss: 0.00004292
Iteration 137/1000 | Loss: 0.00004292
Iteration 138/1000 | Loss: 0.00004292
Iteration 139/1000 | Loss: 0.00004292
Iteration 140/1000 | Loss: 0.00004292
Iteration 141/1000 | Loss: 0.00004292
Iteration 142/1000 | Loss: 0.00004292
Iteration 143/1000 | Loss: 0.00004292
Iteration 144/1000 | Loss: 0.00004292
Iteration 145/1000 | Loss: 0.00004292
Iteration 146/1000 | Loss: 0.00004292
Iteration 147/1000 | Loss: 0.00004292
Iteration 148/1000 | Loss: 0.00004292
Iteration 149/1000 | Loss: 0.00004292
Iteration 150/1000 | Loss: 0.00004292
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 150. Stopping optimization.
Last 5 losses: [4.2918927647406235e-05, 4.2918927647406235e-05, 4.2918927647406235e-05, 4.2918927647406235e-05, 4.2918927647406235e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.2918927647406235e-05

Optimization complete. Final v2v error: 5.746694087982178 mm

Highest mean error: 6.200656890869141 mm for frame 88

Lowest mean error: 5.448018550872803 mm for frame 36

Saving results

Total time: 35.47314667701721
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_48_us_2148/0017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_48_us_2148/0017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_48_us_2148/0017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01095344
Iteration 2/25 | Loss: 0.01095344
Iteration 3/25 | Loss: 0.01095344
Iteration 4/25 | Loss: 0.01095344
Iteration 5/25 | Loss: 0.01095344
Iteration 6/25 | Loss: 0.01095344
Iteration 7/25 | Loss: 0.01095344
Iteration 8/25 | Loss: 0.01095344
Iteration 9/25 | Loss: 0.01095344
Iteration 10/25 | Loss: 0.01095343
Iteration 11/25 | Loss: 0.01095343
Iteration 12/25 | Loss: 0.01095343
Iteration 13/25 | Loss: 0.01095343
Iteration 14/25 | Loss: 0.01095343
Iteration 15/25 | Loss: 0.01095343
Iteration 16/25 | Loss: 0.01095343
Iteration 17/25 | Loss: 0.01095343
Iteration 18/25 | Loss: 0.01095343
Iteration 19/25 | Loss: 0.01095343
Iteration 20/25 | Loss: 0.01095343
Iteration 21/25 | Loss: 0.01095343
Iteration 22/25 | Loss: 0.01095343
Iteration 23/25 | Loss: 0.01095342
Iteration 24/25 | Loss: 0.01095342
Iteration 25/25 | Loss: 0.01095342

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.65879393
Iteration 2/25 | Loss: 0.09606154
Iteration 3/25 | Loss: 0.09562640
Iteration 4/25 | Loss: 0.09545003
Iteration 5/25 | Loss: 0.09545003
Iteration 6/25 | Loss: 0.09545002
Iteration 7/25 | Loss: 0.09545002
Iteration 8/25 | Loss: 0.09545001
Iteration 9/25 | Loss: 0.09545001
Iteration 10/25 | Loss: 0.09545001
Iteration 11/25 | Loss: 0.09545001
Iteration 12/25 | Loss: 0.09545001
Iteration 13/25 | Loss: 0.09545001
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0954500138759613, 0.0954500138759613, 0.0954500138759613, 0.0954500138759613, 0.0954500138759613]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0954500138759613

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.09545001
Iteration 2/1000 | Loss: 0.00669332
Iteration 3/1000 | Loss: 0.00638178
Iteration 4/1000 | Loss: 0.00262330
Iteration 5/1000 | Loss: 0.00101307
Iteration 6/1000 | Loss: 0.00138001
Iteration 7/1000 | Loss: 0.00401625
Iteration 8/1000 | Loss: 0.00505307
Iteration 9/1000 | Loss: 0.00274968
Iteration 10/1000 | Loss: 0.00625209
Iteration 11/1000 | Loss: 0.00500468
Iteration 12/1000 | Loss: 0.00049192
Iteration 13/1000 | Loss: 0.00105896
Iteration 14/1000 | Loss: 0.00057002
Iteration 15/1000 | Loss: 0.00113085
Iteration 16/1000 | Loss: 0.00035488
Iteration 17/1000 | Loss: 0.00072111
Iteration 18/1000 | Loss: 0.00015108
Iteration 19/1000 | Loss: 0.00036140
Iteration 20/1000 | Loss: 0.00087792
Iteration 21/1000 | Loss: 0.00027689
Iteration 22/1000 | Loss: 0.00012089
Iteration 23/1000 | Loss: 0.00079588
Iteration 24/1000 | Loss: 0.00111593
Iteration 25/1000 | Loss: 0.00050219
Iteration 26/1000 | Loss: 0.00031843
Iteration 27/1000 | Loss: 0.00077275
Iteration 28/1000 | Loss: 0.00009407
Iteration 29/1000 | Loss: 0.00008903
Iteration 30/1000 | Loss: 0.00082633
Iteration 31/1000 | Loss: 0.00020211
Iteration 32/1000 | Loss: 0.00014038
Iteration 33/1000 | Loss: 0.00085990
Iteration 34/1000 | Loss: 0.00009058
Iteration 35/1000 | Loss: 0.00027156
Iteration 36/1000 | Loss: 0.00009442
Iteration 37/1000 | Loss: 0.00040198
Iteration 38/1000 | Loss: 0.00021500
Iteration 39/1000 | Loss: 0.00076624
Iteration 40/1000 | Loss: 0.00215625
Iteration 41/1000 | Loss: 0.00222674
Iteration 42/1000 | Loss: 0.00189327
Iteration 43/1000 | Loss: 0.00160882
Iteration 44/1000 | Loss: 0.00246555
Iteration 45/1000 | Loss: 0.00185961
Iteration 46/1000 | Loss: 0.00144773
Iteration 47/1000 | Loss: 0.00245960
Iteration 48/1000 | Loss: 0.00113824
Iteration 49/1000 | Loss: 0.00121713
Iteration 50/1000 | Loss: 0.00086451
Iteration 51/1000 | Loss: 0.00065287
Iteration 52/1000 | Loss: 0.00057192
Iteration 53/1000 | Loss: 0.00078726
Iteration 54/1000 | Loss: 0.00120246
Iteration 55/1000 | Loss: 0.00088605
Iteration 56/1000 | Loss: 0.00158032
Iteration 57/1000 | Loss: 0.00136583
Iteration 58/1000 | Loss: 0.00116262
Iteration 59/1000 | Loss: 0.00158375
Iteration 60/1000 | Loss: 0.00043533
Iteration 61/1000 | Loss: 0.00016924
Iteration 62/1000 | Loss: 0.00033635
Iteration 63/1000 | Loss: 0.00058547
Iteration 64/1000 | Loss: 0.00019924
Iteration 65/1000 | Loss: 0.00019309
Iteration 66/1000 | Loss: 0.00007592
Iteration 67/1000 | Loss: 0.00014766
Iteration 68/1000 | Loss: 0.00028937
Iteration 69/1000 | Loss: 0.00007688
Iteration 70/1000 | Loss: 0.00007005
Iteration 71/1000 | Loss: 0.00006825
Iteration 72/1000 | Loss: 0.00006665
Iteration 73/1000 | Loss: 0.00016704
Iteration 74/1000 | Loss: 0.00006510
Iteration 75/1000 | Loss: 0.00006423
Iteration 76/1000 | Loss: 0.00006321
Iteration 77/1000 | Loss: 0.00006232
Iteration 78/1000 | Loss: 0.00008598
Iteration 79/1000 | Loss: 0.00020350
Iteration 80/1000 | Loss: 0.00008680
Iteration 81/1000 | Loss: 0.00010620
Iteration 82/1000 | Loss: 0.00006522
Iteration 83/1000 | Loss: 0.00014506
Iteration 84/1000 | Loss: 0.00006168
Iteration 85/1000 | Loss: 0.00008125
Iteration 86/1000 | Loss: 0.00007093
Iteration 87/1000 | Loss: 0.00006685
Iteration 88/1000 | Loss: 0.00007424
Iteration 89/1000 | Loss: 0.00006541
Iteration 90/1000 | Loss: 0.00006366
Iteration 91/1000 | Loss: 0.00006161
Iteration 92/1000 | Loss: 0.00006356
Iteration 93/1000 | Loss: 0.00006363
Iteration 94/1000 | Loss: 0.00006912
Iteration 95/1000 | Loss: 0.00006643
Iteration 96/1000 | Loss: 0.00006319
Iteration 97/1000 | Loss: 0.00006365
Iteration 98/1000 | Loss: 0.00006348
Iteration 99/1000 | Loss: 0.00006773
Iteration 100/1000 | Loss: 0.00006953
Iteration 101/1000 | Loss: 0.00007084
Iteration 102/1000 | Loss: 0.00007210
Iteration 103/1000 | Loss: 0.00007097
Iteration 104/1000 | Loss: 0.00006743
Iteration 105/1000 | Loss: 0.00007070
Iteration 106/1000 | Loss: 0.00007386
Iteration 107/1000 | Loss: 0.00007235
Iteration 108/1000 | Loss: 0.00007481
Iteration 109/1000 | Loss: 0.00007239
Iteration 110/1000 | Loss: 0.00007007
Iteration 111/1000 | Loss: 0.00006706
Iteration 112/1000 | Loss: 0.00006772
Iteration 113/1000 | Loss: 0.00006643
Iteration 114/1000 | Loss: 0.00007263
Iteration 115/1000 | Loss: 0.00007026
Iteration 116/1000 | Loss: 0.00007576
Iteration 117/1000 | Loss: 0.00007211
Iteration 118/1000 | Loss: 0.00007679
Iteration 119/1000 | Loss: 0.00007141
Iteration 120/1000 | Loss: 0.00007523
Iteration 121/1000 | Loss: 0.00007138
Iteration 122/1000 | Loss: 0.00007584
Iteration 123/1000 | Loss: 0.00007080
Iteration 124/1000 | Loss: 0.00007211
Iteration 125/1000 | Loss: 0.00006893
Iteration 126/1000 | Loss: 0.00007328
Iteration 127/1000 | Loss: 0.00006606
Iteration 128/1000 | Loss: 0.00007026
Iteration 129/1000 | Loss: 0.00006530
Iteration 130/1000 | Loss: 0.00006344
Iteration 131/1000 | Loss: 0.00006765
Iteration 132/1000 | Loss: 0.00006769
Iteration 133/1000 | Loss: 0.00007176
Iteration 134/1000 | Loss: 0.00006938
Iteration 135/1000 | Loss: 0.00006981
Iteration 136/1000 | Loss: 0.00007072
Iteration 137/1000 | Loss: 0.00006735
Iteration 138/1000 | Loss: 0.00006703
Iteration 139/1000 | Loss: 0.00006820
Iteration 140/1000 | Loss: 0.00007250
Iteration 141/1000 | Loss: 0.00007164
Iteration 142/1000 | Loss: 0.00007046
Iteration 143/1000 | Loss: 0.00006790
Iteration 144/1000 | Loss: 0.00006614
Iteration 145/1000 | Loss: 0.00006637
Iteration 146/1000 | Loss: 0.00006927
Iteration 147/1000 | Loss: 0.00007180
Iteration 148/1000 | Loss: 0.00006869
Iteration 149/1000 | Loss: 0.00007137
Iteration 150/1000 | Loss: 0.00006812
Iteration 151/1000 | Loss: 0.00006745
Iteration 152/1000 | Loss: 0.00006736
Iteration 153/1000 | Loss: 0.00006660
Iteration 154/1000 | Loss: 0.00006921
Iteration 155/1000 | Loss: 0.00007077
Iteration 156/1000 | Loss: 0.00006830
Iteration 157/1000 | Loss: 0.00006726
Iteration 158/1000 | Loss: 0.00006649
Iteration 159/1000 | Loss: 0.00007205
Iteration 160/1000 | Loss: 0.00007230
Iteration 161/1000 | Loss: 0.00006732
Iteration 162/1000 | Loss: 0.00007084
Iteration 163/1000 | Loss: 0.00007683
Iteration 164/1000 | Loss: 0.00007031
Iteration 165/1000 | Loss: 0.00006866
Iteration 166/1000 | Loss: 0.00006734
Iteration 167/1000 | Loss: 0.00006688
Iteration 168/1000 | Loss: 0.00006924
Iteration 169/1000 | Loss: 0.00007381
Iteration 170/1000 | Loss: 0.00006870
Iteration 171/1000 | Loss: 0.00006832
Iteration 172/1000 | Loss: 0.00007299
Iteration 173/1000 | Loss: 0.00007208
Iteration 174/1000 | Loss: 0.00006910
Iteration 175/1000 | Loss: 0.00006494
Iteration 176/1000 | Loss: 0.00006360
Iteration 177/1000 | Loss: 0.00006442
Iteration 178/1000 | Loss: 0.00006508
Iteration 179/1000 | Loss: 0.00006686
Iteration 180/1000 | Loss: 0.00006903
Iteration 181/1000 | Loss: 0.00006789
Iteration 182/1000 | Loss: 0.00006595
Iteration 183/1000 | Loss: 0.00006258
Iteration 184/1000 | Loss: 0.00006139
Iteration 185/1000 | Loss: 0.00006124
Iteration 186/1000 | Loss: 0.00006168
Iteration 187/1000 | Loss: 0.00006446
Iteration 188/1000 | Loss: 0.00006310
Iteration 189/1000 | Loss: 0.00006843
Iteration 190/1000 | Loss: 0.00006414
Iteration 191/1000 | Loss: 0.00006143
Iteration 192/1000 | Loss: 0.00006191
Iteration 193/1000 | Loss: 0.00006626
Iteration 194/1000 | Loss: 0.00006445
Iteration 195/1000 | Loss: 0.00006768
Iteration 196/1000 | Loss: 0.00006541
Iteration 197/1000 | Loss: 0.00006813
Iteration 198/1000 | Loss: 0.00006750
Iteration 199/1000 | Loss: 0.00006954
Iteration 200/1000 | Loss: 0.00006839
Iteration 201/1000 | Loss: 0.00006895
Iteration 202/1000 | Loss: 0.00006598
Iteration 203/1000 | Loss: 0.00006838
Iteration 204/1000 | Loss: 0.00006699
Iteration 205/1000 | Loss: 0.00006784
Iteration 206/1000 | Loss: 0.00006727
Iteration 207/1000 | Loss: 0.00007067
Iteration 208/1000 | Loss: 0.00006921
Iteration 209/1000 | Loss: 0.00007089
Iteration 210/1000 | Loss: 0.00006887
Iteration 211/1000 | Loss: 0.00007010
Iteration 212/1000 | Loss: 0.00007063
Iteration 213/1000 | Loss: 0.00006832
Iteration 214/1000 | Loss: 0.00006639
Iteration 215/1000 | Loss: 0.00006634
Iteration 216/1000 | Loss: 0.00006514
Iteration 217/1000 | Loss: 0.00006564
Iteration 218/1000 | Loss: 0.00006546
Iteration 219/1000 | Loss: 0.00006361
Iteration 220/1000 | Loss: 0.00006490
Iteration 221/1000 | Loss: 0.00006585
Iteration 222/1000 | Loss: 0.00006410
Iteration 223/1000 | Loss: 0.00006453
Iteration 224/1000 | Loss: 0.00006774
Iteration 225/1000 | Loss: 0.00006602
Iteration 226/1000 | Loss: 0.00006738
Iteration 227/1000 | Loss: 0.00006609
Iteration 228/1000 | Loss: 0.00006360
Iteration 229/1000 | Loss: 0.00006320
Iteration 230/1000 | Loss: 0.00006782
Iteration 231/1000 | Loss: 0.00006777
Iteration 232/1000 | Loss: 0.00006763
Iteration 233/1000 | Loss: 0.00006758
Iteration 234/1000 | Loss: 0.00006506
Iteration 235/1000 | Loss: 0.00006402
Iteration 236/1000 | Loss: 0.00006591
Iteration 237/1000 | Loss: 0.00006591
Iteration 238/1000 | Loss: 0.00006384
Iteration 239/1000 | Loss: 0.00006466
Iteration 240/1000 | Loss: 0.00006581
Iteration 241/1000 | Loss: 0.00006222
Iteration 242/1000 | Loss: 0.00006375
Iteration 243/1000 | Loss: 0.00006935
Iteration 244/1000 | Loss: 0.00006414
Iteration 245/1000 | Loss: 0.00006349
Iteration 246/1000 | Loss: 0.00006333
Iteration 247/1000 | Loss: 0.00006397
Iteration 248/1000 | Loss: 0.00006647
Iteration 249/1000 | Loss: 0.00006729
Iteration 250/1000 | Loss: 0.00006704
Iteration 251/1000 | Loss: 0.00006644
Iteration 252/1000 | Loss: 0.00006511
Iteration 253/1000 | Loss: 0.00006835
Iteration 254/1000 | Loss: 0.00006629
Iteration 255/1000 | Loss: 0.00006297
Iteration 256/1000 | Loss: 0.00006153
Iteration 257/1000 | Loss: 0.00006347
Iteration 258/1000 | Loss: 0.00006401
Iteration 259/1000 | Loss: 0.00006462
Iteration 260/1000 | Loss: 0.00006446
Iteration 261/1000 | Loss: 0.00006575
Iteration 262/1000 | Loss: 0.00006370
Iteration 263/1000 | Loss: 0.00006812
Iteration 264/1000 | Loss: 0.00006532
Iteration 265/1000 | Loss: 0.00006830
Iteration 266/1000 | Loss: 0.00006424
Iteration 267/1000 | Loss: 0.00006577
Iteration 268/1000 | Loss: 0.00006492
Iteration 269/1000 | Loss: 0.00006736
Iteration 270/1000 | Loss: 0.00006495
Iteration 271/1000 | Loss: 0.00006730
Iteration 272/1000 | Loss: 0.00006553
Iteration 273/1000 | Loss: 0.00006462
Iteration 274/1000 | Loss: 0.00006294
Iteration 275/1000 | Loss: 0.00006285
Iteration 276/1000 | Loss: 0.00006135
Iteration 277/1000 | Loss: 0.00006329
Iteration 278/1000 | Loss: 0.00006277
Iteration 279/1000 | Loss: 0.00006556
Iteration 280/1000 | Loss: 0.00006441
Iteration 281/1000 | Loss: 0.00006545
Iteration 282/1000 | Loss: 0.00006484
Iteration 283/1000 | Loss: 0.00006532
Iteration 284/1000 | Loss: 0.00006271
Iteration 285/1000 | Loss: 0.00006185
Iteration 286/1000 | Loss: 0.00006368
Iteration 287/1000 | Loss: 0.00006226
Iteration 288/1000 | Loss: 0.00006548
Iteration 289/1000 | Loss: 0.00006241
Iteration 290/1000 | Loss: 0.00006149
Iteration 291/1000 | Loss: 0.00006288
Iteration 292/1000 | Loss: 0.00006655
Iteration 293/1000 | Loss: 0.00006494
Iteration 294/1000 | Loss: 0.00006324
Iteration 295/1000 | Loss: 0.00006383
Iteration 296/1000 | Loss: 0.00006506
Iteration 297/1000 | Loss: 0.00006505
Iteration 298/1000 | Loss: 0.00006747
Iteration 299/1000 | Loss: 0.00006569
Iteration 300/1000 | Loss: 0.00006762
Iteration 301/1000 | Loss: 0.00006755
Iteration 302/1000 | Loss: 0.00006718
Iteration 303/1000 | Loss: 0.00006689
Iteration 304/1000 | Loss: 0.00006831
Iteration 305/1000 | Loss: 0.00006662
Iteration 306/1000 | Loss: 0.00006773
Iteration 307/1000 | Loss: 0.00006640
Iteration 308/1000 | Loss: 0.00006534
Iteration 309/1000 | Loss: 0.00006302
Iteration 310/1000 | Loss: 0.00006513
Iteration 311/1000 | Loss: 0.00006572
Iteration 312/1000 | Loss: 0.00006510
Iteration 313/1000 | Loss: 0.00006289
Iteration 314/1000 | Loss: 0.00006486
Iteration 315/1000 | Loss: 0.00006252
Iteration 316/1000 | Loss: 0.00006440
Iteration 317/1000 | Loss: 0.00006317
Iteration 318/1000 | Loss: 0.00006526
Iteration 319/1000 | Loss: 0.00006304
Iteration 320/1000 | Loss: 0.00006496
Iteration 321/1000 | Loss: 0.00006298
Iteration 322/1000 | Loss: 0.00006335
Iteration 323/1000 | Loss: 0.00006208
Iteration 324/1000 | Loss: 0.00006213
Iteration 325/1000 | Loss: 0.00006249
Iteration 326/1000 | Loss: 0.00006244
Iteration 327/1000 | Loss: 0.00006221
Iteration 328/1000 | Loss: 0.00006546
Iteration 329/1000 | Loss: 0.00006493
Iteration 330/1000 | Loss: 0.00006631
Iteration 331/1000 | Loss: 0.00006646
Iteration 332/1000 | Loss: 0.00006628
Iteration 333/1000 | Loss: 0.00006555
Iteration 334/1000 | Loss: 0.00006545
Iteration 335/1000 | Loss: 0.00006357
Iteration 336/1000 | Loss: 0.00006236
Iteration 337/1000 | Loss: 0.00006301
Iteration 338/1000 | Loss: 0.00006556
Iteration 339/1000 | Loss: 0.00006458
Iteration 340/1000 | Loss: 0.00006220
Iteration 341/1000 | Loss: 0.00006075
Iteration 342/1000 | Loss: 0.00006182
Iteration 343/1000 | Loss: 0.00006084
Iteration 344/1000 | Loss: 0.00006180
Iteration 345/1000 | Loss: 0.00006080
Iteration 346/1000 | Loss: 0.00006203
Iteration 347/1000 | Loss: 0.00006306
Iteration 348/1000 | Loss: 0.00006235
Iteration 349/1000 | Loss: 0.00006404
Iteration 350/1000 | Loss: 0.00006117
Iteration 351/1000 | Loss: 0.00006378
Iteration 352/1000 | Loss: 0.00006165
Iteration 353/1000 | Loss: 0.00006191
Iteration 354/1000 | Loss: 0.00006333
Iteration 355/1000 | Loss: 0.00006295
Iteration 356/1000 | Loss: 0.00006381
Iteration 357/1000 | Loss: 0.00006253
Iteration 358/1000 | Loss: 0.00006310
Iteration 359/1000 | Loss: 0.00006286
Iteration 360/1000 | Loss: 0.00006355
Iteration 361/1000 | Loss: 0.00006326
Iteration 362/1000 | Loss: 0.00006345
Iteration 363/1000 | Loss: 0.00006349
Iteration 364/1000 | Loss: 0.00006330
Iteration 365/1000 | Loss: 0.00006310
Iteration 366/1000 | Loss: 0.00006311
Iteration 367/1000 | Loss: 0.00006362
Iteration 368/1000 | Loss: 0.00006324
Iteration 369/1000 | Loss: 0.00006384
Iteration 370/1000 | Loss: 0.00006302
Iteration 371/1000 | Loss: 0.00006336
Iteration 372/1000 | Loss: 0.00006295
Iteration 373/1000 | Loss: 0.00006129
Iteration 374/1000 | Loss: 0.00006201
Iteration 375/1000 | Loss: 0.00006040
Iteration 376/1000 | Loss: 0.00006352
Iteration 377/1000 | Loss: 0.00006192
Iteration 378/1000 | Loss: 0.00006634
Iteration 379/1000 | Loss: 0.00006331
Iteration 380/1000 | Loss: 0.00006202
Iteration 381/1000 | Loss: 0.00006575
Iteration 382/1000 | Loss: 0.00006288
Iteration 383/1000 | Loss: 0.00006080
Iteration 384/1000 | Loss: 0.00006006
Iteration 385/1000 | Loss: 0.00006121
Iteration 386/1000 | Loss: 0.00006333
Iteration 387/1000 | Loss: 0.00006096
Iteration 388/1000 | Loss: 0.00005992
Iteration 389/1000 | Loss: 0.00005991
Iteration 390/1000 | Loss: 0.00005991
Iteration 391/1000 | Loss: 0.00005990
Iteration 392/1000 | Loss: 0.00005990
Iteration 393/1000 | Loss: 0.00005990
Iteration 394/1000 | Loss: 0.00005989
Iteration 395/1000 | Loss: 0.00005989
Iteration 396/1000 | Loss: 0.00005988
Iteration 397/1000 | Loss: 0.00005988
Iteration 398/1000 | Loss: 0.00005987
Iteration 399/1000 | Loss: 0.00005987
Iteration 400/1000 | Loss: 0.00005987
Iteration 401/1000 | Loss: 0.00005987
Iteration 402/1000 | Loss: 0.00005987
Iteration 403/1000 | Loss: 0.00005986
Iteration 404/1000 | Loss: 0.00005986
Iteration 405/1000 | Loss: 0.00005986
Iteration 406/1000 | Loss: 0.00005986
Iteration 407/1000 | Loss: 0.00005986
Iteration 408/1000 | Loss: 0.00005986
Iteration 409/1000 | Loss: 0.00005986
Iteration 410/1000 | Loss: 0.00005986
Iteration 411/1000 | Loss: 0.00005986
Iteration 412/1000 | Loss: 0.00005986
Iteration 413/1000 | Loss: 0.00005986
Iteration 414/1000 | Loss: 0.00005986
Iteration 415/1000 | Loss: 0.00005986
Iteration 416/1000 | Loss: 0.00005986
Iteration 417/1000 | Loss: 0.00005986
Iteration 418/1000 | Loss: 0.00005986
Iteration 419/1000 | Loss: 0.00005985
Iteration 420/1000 | Loss: 0.00005985
Iteration 421/1000 | Loss: 0.00005985
Iteration 422/1000 | Loss: 0.00005985
Iteration 423/1000 | Loss: 0.00005985
Iteration 424/1000 | Loss: 0.00005985
Iteration 425/1000 | Loss: 0.00005985
Iteration 426/1000 | Loss: 0.00005985
Iteration 427/1000 | Loss: 0.00005985
Iteration 428/1000 | Loss: 0.00005984
Iteration 429/1000 | Loss: 0.00005984
Iteration 430/1000 | Loss: 0.00005984
Iteration 431/1000 | Loss: 0.00005983
Iteration 432/1000 | Loss: 0.00005983
Iteration 433/1000 | Loss: 0.00005983
Iteration 434/1000 | Loss: 0.00005982
Iteration 435/1000 | Loss: 0.00005982
Iteration 436/1000 | Loss: 0.00005982
Iteration 437/1000 | Loss: 0.00005981
Iteration 438/1000 | Loss: 0.00005981
Iteration 439/1000 | Loss: 0.00005981
Iteration 440/1000 | Loss: 0.00005981
Iteration 441/1000 | Loss: 0.00006186
Iteration 442/1000 | Loss: 0.00006134
Iteration 443/1000 | Loss: 0.00005980
Iteration 444/1000 | Loss: 0.00005980
Iteration 445/1000 | Loss: 0.00005980
Iteration 446/1000 | Loss: 0.00005979
Iteration 447/1000 | Loss: 0.00005979
Iteration 448/1000 | Loss: 0.00005979
Iteration 449/1000 | Loss: 0.00005979
Iteration 450/1000 | Loss: 0.00005979
Iteration 451/1000 | Loss: 0.00006161
Iteration 452/1000 | Loss: 0.00006126
Iteration 453/1000 | Loss: 0.00006198
Iteration 454/1000 | Loss: 0.00006120
Iteration 455/1000 | Loss: 0.00006175
Iteration 456/1000 | Loss: 0.00006122
Iteration 457/1000 | Loss: 0.00006158
Iteration 458/1000 | Loss: 0.00006131
Iteration 459/1000 | Loss: 0.00006160
Iteration 460/1000 | Loss: 0.00006117
Iteration 461/1000 | Loss: 0.00005979
Iteration 462/1000 | Loss: 0.00005979
Iteration 463/1000 | Loss: 0.00005979
Iteration 464/1000 | Loss: 0.00005979
Iteration 465/1000 | Loss: 0.00005979
Iteration 466/1000 | Loss: 0.00005979
Iteration 467/1000 | Loss: 0.00005979
Iteration 468/1000 | Loss: 0.00005978
Iteration 469/1000 | Loss: 0.00005978
Iteration 470/1000 | Loss: 0.00005978
Iteration 471/1000 | Loss: 0.00005978
Iteration 472/1000 | Loss: 0.00005978
Iteration 473/1000 | Loss: 0.00005978
Iteration 474/1000 | Loss: 0.00005978
Iteration 475/1000 | Loss: 0.00005977
Iteration 476/1000 | Loss: 0.00005977
Iteration 477/1000 | Loss: 0.00005977
Iteration 478/1000 | Loss: 0.00005977
Iteration 479/1000 | Loss: 0.00005977
Iteration 480/1000 | Loss: 0.00005977
Iteration 481/1000 | Loss: 0.00005977
Iteration 482/1000 | Loss: 0.00005977
Iteration 483/1000 | Loss: 0.00005977
Iteration 484/1000 | Loss: 0.00005977
Iteration 485/1000 | Loss: 0.00005977
Iteration 486/1000 | Loss: 0.00005977
Iteration 487/1000 | Loss: 0.00005977
Iteration 488/1000 | Loss: 0.00005977
Iteration 489/1000 | Loss: 0.00005977
Iteration 490/1000 | Loss: 0.00005977
Iteration 491/1000 | Loss: 0.00005977
Iteration 492/1000 | Loss: 0.00005977
Iteration 493/1000 | Loss: 0.00005977
Iteration 494/1000 | Loss: 0.00005977
Iteration 495/1000 | Loss: 0.00005977
Iteration 496/1000 | Loss: 0.00005977
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 496. Stopping optimization.
Last 5 losses: [5.976589454803616e-05, 5.976589454803616e-05, 5.976589454803616e-05, 5.976589454803616e-05, 5.976589454803616e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 5.976589454803616e-05

Optimization complete. Final v2v error: 5.947707176208496 mm

Highest mean error: 12.73175048828125 mm for frame 143

Lowest mean error: 4.7056169509887695 mm for frame 118

Saving results

Total time: 642.0283811092377
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_48_us_2148/0012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_48_us_2148/0012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_48_us_2148/0012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01181400
Iteration 2/25 | Loss: 0.00231459
Iteration 3/25 | Loss: 0.00202917
Iteration 4/25 | Loss: 0.00200062
Iteration 5/25 | Loss: 0.00199161
Iteration 6/25 | Loss: 0.00198973
Iteration 7/25 | Loss: 0.00198973
Iteration 8/25 | Loss: 0.00198973
Iteration 9/25 | Loss: 0.00198973
Iteration 10/25 | Loss: 0.00198973
Iteration 11/25 | Loss: 0.00198973
Iteration 12/25 | Loss: 0.00198973
Iteration 13/25 | Loss: 0.00198973
Iteration 14/25 | Loss: 0.00198973
Iteration 15/25 | Loss: 0.00198973
Iteration 16/25 | Loss: 0.00198973
Iteration 17/25 | Loss: 0.00198973
Iteration 18/25 | Loss: 0.00198973
Iteration 19/25 | Loss: 0.00198973
Iteration 20/25 | Loss: 0.00198973
Iteration 21/25 | Loss: 0.00198973
Iteration 22/25 | Loss: 0.00198973
Iteration 23/25 | Loss: 0.00198973
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.001989725511521101, 0.001989725511521101, 0.001989725511521101, 0.001989725511521101, 0.001989725511521101]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001989725511521101

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.06138790
Iteration 2/25 | Loss: 0.00211934
Iteration 3/25 | Loss: 0.00211932
Iteration 4/25 | Loss: 0.00211932
Iteration 5/25 | Loss: 0.00211932
Iteration 6/25 | Loss: 0.00211932
Iteration 7/25 | Loss: 0.00211932
Iteration 8/25 | Loss: 0.00211932
Iteration 9/25 | Loss: 0.00211932
Iteration 10/25 | Loss: 0.00211932
Iteration 11/25 | Loss: 0.00211932
Iteration 12/25 | Loss: 0.00211932
Iteration 13/25 | Loss: 0.00211932
Iteration 14/25 | Loss: 0.00211932
Iteration 15/25 | Loss: 0.00211932
Iteration 16/25 | Loss: 0.00211932
Iteration 17/25 | Loss: 0.00211932
Iteration 18/25 | Loss: 0.00211932
Iteration 19/25 | Loss: 0.00211932
Iteration 20/25 | Loss: 0.00211932
Iteration 21/25 | Loss: 0.00211932
Iteration 22/25 | Loss: 0.00211932
Iteration 23/25 | Loss: 0.00211932
Iteration 24/25 | Loss: 0.00211932
Iteration 25/25 | Loss: 0.00211932

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00211932
Iteration 2/1000 | Loss: 0.00013926
Iteration 3/1000 | Loss: 0.00009485
Iteration 4/1000 | Loss: 0.00008000
Iteration 5/1000 | Loss: 0.00007353
Iteration 6/1000 | Loss: 0.00006852
Iteration 7/1000 | Loss: 0.00006591
Iteration 8/1000 | Loss: 0.00006450
Iteration 9/1000 | Loss: 0.00006344
Iteration 10/1000 | Loss: 0.00006286
Iteration 11/1000 | Loss: 0.00006233
Iteration 12/1000 | Loss: 0.00006204
Iteration 13/1000 | Loss: 0.00006191
Iteration 14/1000 | Loss: 0.00006177
Iteration 15/1000 | Loss: 0.00006175
Iteration 16/1000 | Loss: 0.00006170
Iteration 17/1000 | Loss: 0.00006170
Iteration 18/1000 | Loss: 0.00006167
Iteration 19/1000 | Loss: 0.00006160
Iteration 20/1000 | Loss: 0.00006156
Iteration 21/1000 | Loss: 0.00006153
Iteration 22/1000 | Loss: 0.00006153
Iteration 23/1000 | Loss: 0.00006152
Iteration 24/1000 | Loss: 0.00006152
Iteration 25/1000 | Loss: 0.00006150
Iteration 26/1000 | Loss: 0.00006149
Iteration 27/1000 | Loss: 0.00006149
Iteration 28/1000 | Loss: 0.00006149
Iteration 29/1000 | Loss: 0.00006148
Iteration 30/1000 | Loss: 0.00006148
Iteration 31/1000 | Loss: 0.00006148
Iteration 32/1000 | Loss: 0.00006147
Iteration 33/1000 | Loss: 0.00006147
Iteration 34/1000 | Loss: 0.00006144
Iteration 35/1000 | Loss: 0.00006143
Iteration 36/1000 | Loss: 0.00006142
Iteration 37/1000 | Loss: 0.00006142
Iteration 38/1000 | Loss: 0.00006142
Iteration 39/1000 | Loss: 0.00006142
Iteration 40/1000 | Loss: 0.00006142
Iteration 41/1000 | Loss: 0.00006142
Iteration 42/1000 | Loss: 0.00006142
Iteration 43/1000 | Loss: 0.00006142
Iteration 44/1000 | Loss: 0.00006140
Iteration 45/1000 | Loss: 0.00006140
Iteration 46/1000 | Loss: 0.00006139
Iteration 47/1000 | Loss: 0.00006139
Iteration 48/1000 | Loss: 0.00006139
Iteration 49/1000 | Loss: 0.00006139
Iteration 50/1000 | Loss: 0.00006139
Iteration 51/1000 | Loss: 0.00006138
Iteration 52/1000 | Loss: 0.00006138
Iteration 53/1000 | Loss: 0.00006138
Iteration 54/1000 | Loss: 0.00006138
Iteration 55/1000 | Loss: 0.00006138
Iteration 56/1000 | Loss: 0.00006138
Iteration 57/1000 | Loss: 0.00006138
Iteration 58/1000 | Loss: 0.00006138
Iteration 59/1000 | Loss: 0.00006138
Iteration 60/1000 | Loss: 0.00006137
Iteration 61/1000 | Loss: 0.00006137
Iteration 62/1000 | Loss: 0.00006137
Iteration 63/1000 | Loss: 0.00006137
Iteration 64/1000 | Loss: 0.00006137
Iteration 65/1000 | Loss: 0.00006137
Iteration 66/1000 | Loss: 0.00006137
Iteration 67/1000 | Loss: 0.00006137
Iteration 68/1000 | Loss: 0.00006136
Iteration 69/1000 | Loss: 0.00006136
Iteration 70/1000 | Loss: 0.00006136
Iteration 71/1000 | Loss: 0.00006136
Iteration 72/1000 | Loss: 0.00006136
Iteration 73/1000 | Loss: 0.00006135
Iteration 74/1000 | Loss: 0.00006135
Iteration 75/1000 | Loss: 0.00006135
Iteration 76/1000 | Loss: 0.00006135
Iteration 77/1000 | Loss: 0.00006134
Iteration 78/1000 | Loss: 0.00006134
Iteration 79/1000 | Loss: 0.00006134
Iteration 80/1000 | Loss: 0.00006134
Iteration 81/1000 | Loss: 0.00006134
Iteration 82/1000 | Loss: 0.00006133
Iteration 83/1000 | Loss: 0.00006133
Iteration 84/1000 | Loss: 0.00006133
Iteration 85/1000 | Loss: 0.00006133
Iteration 86/1000 | Loss: 0.00006133
Iteration 87/1000 | Loss: 0.00006133
Iteration 88/1000 | Loss: 0.00006133
Iteration 89/1000 | Loss: 0.00006132
Iteration 90/1000 | Loss: 0.00006132
Iteration 91/1000 | Loss: 0.00006132
Iteration 92/1000 | Loss: 0.00006132
Iteration 93/1000 | Loss: 0.00006132
Iteration 94/1000 | Loss: 0.00006132
Iteration 95/1000 | Loss: 0.00006132
Iteration 96/1000 | Loss: 0.00006132
Iteration 97/1000 | Loss: 0.00006132
Iteration 98/1000 | Loss: 0.00006131
Iteration 99/1000 | Loss: 0.00006131
Iteration 100/1000 | Loss: 0.00006131
Iteration 101/1000 | Loss: 0.00006131
Iteration 102/1000 | Loss: 0.00006131
Iteration 103/1000 | Loss: 0.00006130
Iteration 104/1000 | Loss: 0.00006130
Iteration 105/1000 | Loss: 0.00006130
Iteration 106/1000 | Loss: 0.00006129
Iteration 107/1000 | Loss: 0.00006129
Iteration 108/1000 | Loss: 0.00006129
Iteration 109/1000 | Loss: 0.00006129
Iteration 110/1000 | Loss: 0.00006129
Iteration 111/1000 | Loss: 0.00006129
Iteration 112/1000 | Loss: 0.00006129
Iteration 113/1000 | Loss: 0.00006129
Iteration 114/1000 | Loss: 0.00006129
Iteration 115/1000 | Loss: 0.00006128
Iteration 116/1000 | Loss: 0.00006128
Iteration 117/1000 | Loss: 0.00006128
Iteration 118/1000 | Loss: 0.00006128
Iteration 119/1000 | Loss: 0.00006128
Iteration 120/1000 | Loss: 0.00006128
Iteration 121/1000 | Loss: 0.00006128
Iteration 122/1000 | Loss: 0.00006128
Iteration 123/1000 | Loss: 0.00006128
Iteration 124/1000 | Loss: 0.00006128
Iteration 125/1000 | Loss: 0.00006128
Iteration 126/1000 | Loss: 0.00006128
Iteration 127/1000 | Loss: 0.00006128
Iteration 128/1000 | Loss: 0.00006128
Iteration 129/1000 | Loss: 0.00006128
Iteration 130/1000 | Loss: 0.00006128
Iteration 131/1000 | Loss: 0.00006128
Iteration 132/1000 | Loss: 0.00006128
Iteration 133/1000 | Loss: 0.00006128
Iteration 134/1000 | Loss: 0.00006128
Iteration 135/1000 | Loss: 0.00006128
Iteration 136/1000 | Loss: 0.00006128
Iteration 137/1000 | Loss: 0.00006128
Iteration 138/1000 | Loss: 0.00006128
Iteration 139/1000 | Loss: 0.00006128
Iteration 140/1000 | Loss: 0.00006128
Iteration 141/1000 | Loss: 0.00006128
Iteration 142/1000 | Loss: 0.00006128
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 142. Stopping optimization.
Last 5 losses: [6.128069071564823e-05, 6.128069071564823e-05, 6.128069071564823e-05, 6.128069071564823e-05, 6.128069071564823e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 6.128069071564823e-05

Optimization complete. Final v2v error: 6.583173751831055 mm

Highest mean error: 7.361902713775635 mm for frame 84

Lowest mean error: 5.997893333435059 mm for frame 39

Saving results

Total time: 43.37670540809631
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_48_us_2148/0006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_48_us_2148/0006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_48_us_2148/0006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00658513
Iteration 2/25 | Loss: 0.00203082
Iteration 3/25 | Loss: 0.00189324
Iteration 4/25 | Loss: 0.00187606
Iteration 5/25 | Loss: 0.00187058
Iteration 6/25 | Loss: 0.00186902
Iteration 7/25 | Loss: 0.00186902
Iteration 8/25 | Loss: 0.00186902
Iteration 9/25 | Loss: 0.00186902
Iteration 10/25 | Loss: 0.00186902
Iteration 11/25 | Loss: 0.00186902
Iteration 12/25 | Loss: 0.00186902
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0018690215656533837, 0.0018690215656533837, 0.0018690215656533837, 0.0018690215656533837, 0.0018690215656533837]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0018690215656533837

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.68410182
Iteration 2/25 | Loss: 0.00254603
Iteration 3/25 | Loss: 0.00254603
Iteration 4/25 | Loss: 0.00254603
Iteration 5/25 | Loss: 0.00254603
Iteration 6/25 | Loss: 0.00254603
Iteration 7/25 | Loss: 0.00254603
Iteration 8/25 | Loss: 0.00254603
Iteration 9/25 | Loss: 0.00254603
Iteration 10/25 | Loss: 0.00254603
Iteration 11/25 | Loss: 0.00254603
Iteration 12/25 | Loss: 0.00254603
Iteration 13/25 | Loss: 0.00254603
Iteration 14/25 | Loss: 0.00254603
Iteration 15/25 | Loss: 0.00254603
Iteration 16/25 | Loss: 0.00254603
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0025460293982177973, 0.0025460293982177973, 0.0025460293982177973, 0.0025460293982177973, 0.0025460293982177973]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0025460293982177973

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00254603
Iteration 2/1000 | Loss: 0.00010664
Iteration 3/1000 | Loss: 0.00007132
Iteration 4/1000 | Loss: 0.00006100
Iteration 5/1000 | Loss: 0.00005595
Iteration 6/1000 | Loss: 0.00005358
Iteration 7/1000 | Loss: 0.00005248
Iteration 8/1000 | Loss: 0.00005175
Iteration 9/1000 | Loss: 0.00005101
Iteration 10/1000 | Loss: 0.00005065
Iteration 11/1000 | Loss: 0.00005044
Iteration 12/1000 | Loss: 0.00005030
Iteration 13/1000 | Loss: 0.00005030
Iteration 14/1000 | Loss: 0.00005024
Iteration 15/1000 | Loss: 0.00005018
Iteration 16/1000 | Loss: 0.00005016
Iteration 17/1000 | Loss: 0.00005016
Iteration 18/1000 | Loss: 0.00005015
Iteration 19/1000 | Loss: 0.00005012
Iteration 20/1000 | Loss: 0.00005011
Iteration 21/1000 | Loss: 0.00005010
Iteration 22/1000 | Loss: 0.00005006
Iteration 23/1000 | Loss: 0.00005005
Iteration 24/1000 | Loss: 0.00005004
Iteration 25/1000 | Loss: 0.00004999
Iteration 26/1000 | Loss: 0.00004999
Iteration 27/1000 | Loss: 0.00004999
Iteration 28/1000 | Loss: 0.00004999
Iteration 29/1000 | Loss: 0.00004999
Iteration 30/1000 | Loss: 0.00004999
Iteration 31/1000 | Loss: 0.00004998
Iteration 32/1000 | Loss: 0.00004997
Iteration 33/1000 | Loss: 0.00004997
Iteration 34/1000 | Loss: 0.00004996
Iteration 35/1000 | Loss: 0.00004996
Iteration 36/1000 | Loss: 0.00004996
Iteration 37/1000 | Loss: 0.00004995
Iteration 38/1000 | Loss: 0.00004995
Iteration 39/1000 | Loss: 0.00004995
Iteration 40/1000 | Loss: 0.00004995
Iteration 41/1000 | Loss: 0.00004994
Iteration 42/1000 | Loss: 0.00004994
Iteration 43/1000 | Loss: 0.00004994
Iteration 44/1000 | Loss: 0.00004994
Iteration 45/1000 | Loss: 0.00004994
Iteration 46/1000 | Loss: 0.00004994
Iteration 47/1000 | Loss: 0.00004994
Iteration 48/1000 | Loss: 0.00004993
Iteration 49/1000 | Loss: 0.00004993
Iteration 50/1000 | Loss: 0.00004993
Iteration 51/1000 | Loss: 0.00004993
Iteration 52/1000 | Loss: 0.00004992
Iteration 53/1000 | Loss: 0.00004992
Iteration 54/1000 | Loss: 0.00004992
Iteration 55/1000 | Loss: 0.00004991
Iteration 56/1000 | Loss: 0.00004991
Iteration 57/1000 | Loss: 0.00004991
Iteration 58/1000 | Loss: 0.00004991
Iteration 59/1000 | Loss: 0.00004991
Iteration 60/1000 | Loss: 0.00004990
Iteration 61/1000 | Loss: 0.00004990
Iteration 62/1000 | Loss: 0.00004990
Iteration 63/1000 | Loss: 0.00004990
Iteration 64/1000 | Loss: 0.00004990
Iteration 65/1000 | Loss: 0.00004990
Iteration 66/1000 | Loss: 0.00004990
Iteration 67/1000 | Loss: 0.00004989
Iteration 68/1000 | Loss: 0.00004989
Iteration 69/1000 | Loss: 0.00004989
Iteration 70/1000 | Loss: 0.00004988
Iteration 71/1000 | Loss: 0.00004988
Iteration 72/1000 | Loss: 0.00004988
Iteration 73/1000 | Loss: 0.00004987
Iteration 74/1000 | Loss: 0.00004987
Iteration 75/1000 | Loss: 0.00004987
Iteration 76/1000 | Loss: 0.00004987
Iteration 77/1000 | Loss: 0.00004987
Iteration 78/1000 | Loss: 0.00004987
Iteration 79/1000 | Loss: 0.00004987
Iteration 80/1000 | Loss: 0.00004987
Iteration 81/1000 | Loss: 0.00004987
Iteration 82/1000 | Loss: 0.00004987
Iteration 83/1000 | Loss: 0.00004987
Iteration 84/1000 | Loss: 0.00004987
Iteration 85/1000 | Loss: 0.00004987
Iteration 86/1000 | Loss: 0.00004987
Iteration 87/1000 | Loss: 0.00004987
Iteration 88/1000 | Loss: 0.00004987
Iteration 89/1000 | Loss: 0.00004987
Iteration 90/1000 | Loss: 0.00004987
Iteration 91/1000 | Loss: 0.00004987
Iteration 92/1000 | Loss: 0.00004987
Iteration 93/1000 | Loss: 0.00004987
Iteration 94/1000 | Loss: 0.00004987
Iteration 95/1000 | Loss: 0.00004987
Iteration 96/1000 | Loss: 0.00004987
Iteration 97/1000 | Loss: 0.00004987
Iteration 98/1000 | Loss: 0.00004987
Iteration 99/1000 | Loss: 0.00004987
Iteration 100/1000 | Loss: 0.00004987
Iteration 101/1000 | Loss: 0.00004987
Iteration 102/1000 | Loss: 0.00004987
Iteration 103/1000 | Loss: 0.00004987
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 103. Stopping optimization.
Last 5 losses: [4.986735075362958e-05, 4.986735075362958e-05, 4.986735075362958e-05, 4.986735075362958e-05, 4.986735075362958e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.986735075362958e-05

Optimization complete. Final v2v error: 6.226919174194336 mm

Highest mean error: 6.8738837242126465 mm for frame 71

Lowest mean error: 5.784772872924805 mm for frame 170

Saving results

Total time: 36.810428857803345
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_48_us_2148/0024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_48_us_2148/0024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_48_us_2148/0024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00926205
Iteration 2/25 | Loss: 0.00224883
Iteration 3/25 | Loss: 0.00212536
Iteration 4/25 | Loss: 0.00208781
Iteration 5/25 | Loss: 0.00208094
Iteration 6/25 | Loss: 0.00205328
Iteration 7/25 | Loss: 0.00204788
Iteration 8/25 | Loss: 0.00204653
Iteration 9/25 | Loss: 0.00204600
Iteration 10/25 | Loss: 0.00204592
Iteration 11/25 | Loss: 0.00204592
Iteration 12/25 | Loss: 0.00204592
Iteration 13/25 | Loss: 0.00204592
Iteration 14/25 | Loss: 0.00204592
Iteration 15/25 | Loss: 0.00204592
Iteration 16/25 | Loss: 0.00204592
Iteration 17/25 | Loss: 0.00204592
Iteration 18/25 | Loss: 0.00204592
Iteration 19/25 | Loss: 0.00204592
Iteration 20/25 | Loss: 0.00204592
Iteration 21/25 | Loss: 0.00204592
Iteration 22/25 | Loss: 0.00204592
Iteration 23/25 | Loss: 0.00204592
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0020459231454879045, 0.0020459231454879045, 0.0020459231454879045, 0.0020459231454879045, 0.0020459231454879045]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0020459231454879045

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.50901043
Iteration 2/25 | Loss: 0.00303331
Iteration 3/25 | Loss: 0.00303331
Iteration 4/25 | Loss: 0.00303331
Iteration 5/25 | Loss: 0.00303331
Iteration 6/25 | Loss: 0.00303331
Iteration 7/25 | Loss: 0.00303331
Iteration 8/25 | Loss: 0.00303331
Iteration 9/25 | Loss: 0.00303331
Iteration 10/25 | Loss: 0.00303331
Iteration 11/25 | Loss: 0.00303331
Iteration 12/25 | Loss: 0.00303331
Iteration 13/25 | Loss: 0.00303331
Iteration 14/25 | Loss: 0.00303331
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.003033311339095235, 0.003033311339095235, 0.003033311339095235, 0.003033311339095235, 0.003033311339095235]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.003033311339095235

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00303331
Iteration 2/1000 | Loss: 0.00015886
Iteration 3/1000 | Loss: 0.00009444
Iteration 4/1000 | Loss: 0.00007542
Iteration 5/1000 | Loss: 0.00006939
Iteration 6/1000 | Loss: 0.00006469
Iteration 7/1000 | Loss: 0.00006232
Iteration 8/1000 | Loss: 0.00006040
Iteration 9/1000 | Loss: 0.00005915
Iteration 10/1000 | Loss: 0.00005829
Iteration 11/1000 | Loss: 0.00005750
Iteration 12/1000 | Loss: 0.00005682
Iteration 13/1000 | Loss: 0.00005650
Iteration 14/1000 | Loss: 0.00005629
Iteration 15/1000 | Loss: 0.00005617
Iteration 16/1000 | Loss: 0.00005608
Iteration 17/1000 | Loss: 0.00005601
Iteration 18/1000 | Loss: 0.00005597
Iteration 19/1000 | Loss: 0.00005596
Iteration 20/1000 | Loss: 0.00005596
Iteration 21/1000 | Loss: 0.00005595
Iteration 22/1000 | Loss: 0.00005594
Iteration 23/1000 | Loss: 0.00005594
Iteration 24/1000 | Loss: 0.00005593
Iteration 25/1000 | Loss: 0.00005593
Iteration 26/1000 | Loss: 0.00005593
Iteration 27/1000 | Loss: 0.00005591
Iteration 28/1000 | Loss: 0.00005591
Iteration 29/1000 | Loss: 0.00005591
Iteration 30/1000 | Loss: 0.00005591
Iteration 31/1000 | Loss: 0.00005591
Iteration 32/1000 | Loss: 0.00005591
Iteration 33/1000 | Loss: 0.00005590
Iteration 34/1000 | Loss: 0.00005590
Iteration 35/1000 | Loss: 0.00005590
Iteration 36/1000 | Loss: 0.00005589
Iteration 37/1000 | Loss: 0.00005589
Iteration 38/1000 | Loss: 0.00005589
Iteration 39/1000 | Loss: 0.00005588
Iteration 40/1000 | Loss: 0.00005588
Iteration 41/1000 | Loss: 0.00005587
Iteration 42/1000 | Loss: 0.00005587
Iteration 43/1000 | Loss: 0.00005587
Iteration 44/1000 | Loss: 0.00005587
Iteration 45/1000 | Loss: 0.00005586
Iteration 46/1000 | Loss: 0.00005586
Iteration 47/1000 | Loss: 0.00005586
Iteration 48/1000 | Loss: 0.00005586
Iteration 49/1000 | Loss: 0.00005586
Iteration 50/1000 | Loss: 0.00005586
Iteration 51/1000 | Loss: 0.00005586
Iteration 52/1000 | Loss: 0.00005585
Iteration 53/1000 | Loss: 0.00005585
Iteration 54/1000 | Loss: 0.00005585
Iteration 55/1000 | Loss: 0.00005585
Iteration 56/1000 | Loss: 0.00005585
Iteration 57/1000 | Loss: 0.00005585
Iteration 58/1000 | Loss: 0.00005584
Iteration 59/1000 | Loss: 0.00005584
Iteration 60/1000 | Loss: 0.00005584
Iteration 61/1000 | Loss: 0.00005584
Iteration 62/1000 | Loss: 0.00005584
Iteration 63/1000 | Loss: 0.00005584
Iteration 64/1000 | Loss: 0.00005584
Iteration 65/1000 | Loss: 0.00005583
Iteration 66/1000 | Loss: 0.00005583
Iteration 67/1000 | Loss: 0.00005583
Iteration 68/1000 | Loss: 0.00005583
Iteration 69/1000 | Loss: 0.00005582
Iteration 70/1000 | Loss: 0.00005582
Iteration 71/1000 | Loss: 0.00005582
Iteration 72/1000 | Loss: 0.00005582
Iteration 73/1000 | Loss: 0.00005581
Iteration 74/1000 | Loss: 0.00005581
Iteration 75/1000 | Loss: 0.00005581
Iteration 76/1000 | Loss: 0.00005581
Iteration 77/1000 | Loss: 0.00005581
Iteration 78/1000 | Loss: 0.00005581
Iteration 79/1000 | Loss: 0.00005581
Iteration 80/1000 | Loss: 0.00005581
Iteration 81/1000 | Loss: 0.00005581
Iteration 82/1000 | Loss: 0.00005581
Iteration 83/1000 | Loss: 0.00005581
Iteration 84/1000 | Loss: 0.00005580
Iteration 85/1000 | Loss: 0.00005580
Iteration 86/1000 | Loss: 0.00005580
Iteration 87/1000 | Loss: 0.00005580
Iteration 88/1000 | Loss: 0.00005580
Iteration 89/1000 | Loss: 0.00005580
Iteration 90/1000 | Loss: 0.00005580
Iteration 91/1000 | Loss: 0.00005580
Iteration 92/1000 | Loss: 0.00005580
Iteration 93/1000 | Loss: 0.00005580
Iteration 94/1000 | Loss: 0.00005580
Iteration 95/1000 | Loss: 0.00005580
Iteration 96/1000 | Loss: 0.00005580
Iteration 97/1000 | Loss: 0.00005579
Iteration 98/1000 | Loss: 0.00005579
Iteration 99/1000 | Loss: 0.00005579
Iteration 100/1000 | Loss: 0.00005579
Iteration 101/1000 | Loss: 0.00005579
Iteration 102/1000 | Loss: 0.00005579
Iteration 103/1000 | Loss: 0.00005579
Iteration 104/1000 | Loss: 0.00005579
Iteration 105/1000 | Loss: 0.00005579
Iteration 106/1000 | Loss: 0.00005579
Iteration 107/1000 | Loss: 0.00005579
Iteration 108/1000 | Loss: 0.00005579
Iteration 109/1000 | Loss: 0.00005578
Iteration 110/1000 | Loss: 0.00005578
Iteration 111/1000 | Loss: 0.00005578
Iteration 112/1000 | Loss: 0.00005578
Iteration 113/1000 | Loss: 0.00005578
Iteration 114/1000 | Loss: 0.00005578
Iteration 115/1000 | Loss: 0.00005578
Iteration 116/1000 | Loss: 0.00005578
Iteration 117/1000 | Loss: 0.00005578
Iteration 118/1000 | Loss: 0.00005577
Iteration 119/1000 | Loss: 0.00005577
Iteration 120/1000 | Loss: 0.00005577
Iteration 121/1000 | Loss: 0.00005577
Iteration 122/1000 | Loss: 0.00005577
Iteration 123/1000 | Loss: 0.00005577
Iteration 124/1000 | Loss: 0.00005577
Iteration 125/1000 | Loss: 0.00005577
Iteration 126/1000 | Loss: 0.00005577
Iteration 127/1000 | Loss: 0.00005577
Iteration 128/1000 | Loss: 0.00005577
Iteration 129/1000 | Loss: 0.00005577
Iteration 130/1000 | Loss: 0.00005577
Iteration 131/1000 | Loss: 0.00005576
Iteration 132/1000 | Loss: 0.00005576
Iteration 133/1000 | Loss: 0.00005576
Iteration 134/1000 | Loss: 0.00005576
Iteration 135/1000 | Loss: 0.00005576
Iteration 136/1000 | Loss: 0.00005576
Iteration 137/1000 | Loss: 0.00005576
Iteration 138/1000 | Loss: 0.00005576
Iteration 139/1000 | Loss: 0.00005576
Iteration 140/1000 | Loss: 0.00005576
Iteration 141/1000 | Loss: 0.00005576
Iteration 142/1000 | Loss: 0.00005576
Iteration 143/1000 | Loss: 0.00005576
Iteration 144/1000 | Loss: 0.00005576
Iteration 145/1000 | Loss: 0.00005576
Iteration 146/1000 | Loss: 0.00005576
Iteration 147/1000 | Loss: 0.00005576
Iteration 148/1000 | Loss: 0.00005575
Iteration 149/1000 | Loss: 0.00005575
Iteration 150/1000 | Loss: 0.00005575
Iteration 151/1000 | Loss: 0.00005575
Iteration 152/1000 | Loss: 0.00005575
Iteration 153/1000 | Loss: 0.00005575
Iteration 154/1000 | Loss: 0.00005575
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 154. Stopping optimization.
Last 5 losses: [5.57545390620362e-05, 5.57545390620362e-05, 5.57545390620362e-05, 5.57545390620362e-05, 5.57545390620362e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 5.57545390620362e-05

Optimization complete. Final v2v error: 6.333858489990234 mm

Highest mean error: 6.901456832885742 mm for frame 6

Lowest mean error: 5.76088809967041 mm for frame 192

Saving results

Total time: 53.95173263549805
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_48_us_2148/0002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_48_us_2148/0002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_48_us_2148/0002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00936197
Iteration 2/25 | Loss: 0.00205543
Iteration 3/25 | Loss: 0.00189925
Iteration 4/25 | Loss: 0.00188364
Iteration 5/25 | Loss: 0.00187931
Iteration 6/25 | Loss: 0.00187847
Iteration 7/25 | Loss: 0.00187847
Iteration 8/25 | Loss: 0.00187847
Iteration 9/25 | Loss: 0.00187847
Iteration 10/25 | Loss: 0.00187847
Iteration 11/25 | Loss: 0.00187847
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0018784706480801105, 0.0018784706480801105, 0.0018784706480801105, 0.0018784706480801105, 0.0018784706480801105]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0018784706480801105

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.49065936
Iteration 2/25 | Loss: 0.00244418
Iteration 3/25 | Loss: 0.00244418
Iteration 4/25 | Loss: 0.00244418
Iteration 5/25 | Loss: 0.00244418
Iteration 6/25 | Loss: 0.00244418
Iteration 7/25 | Loss: 0.00244418
Iteration 8/25 | Loss: 0.00244418
Iteration 9/25 | Loss: 0.00244417
Iteration 10/25 | Loss: 0.00244417
Iteration 11/25 | Loss: 0.00244417
Iteration 12/25 | Loss: 0.00244417
Iteration 13/25 | Loss: 0.00244417
Iteration 14/25 | Loss: 0.00244417
Iteration 15/25 | Loss: 0.00244417
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0024441746063530445, 0.0024441746063530445, 0.0024441746063530445, 0.0024441746063530445, 0.0024441746063530445]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0024441746063530445

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00244417
Iteration 2/1000 | Loss: 0.00010722
Iteration 3/1000 | Loss: 0.00007363
Iteration 4/1000 | Loss: 0.00006185
Iteration 5/1000 | Loss: 0.00005543
Iteration 6/1000 | Loss: 0.00005171
Iteration 7/1000 | Loss: 0.00004915
Iteration 8/1000 | Loss: 0.00004775
Iteration 9/1000 | Loss: 0.00004692
Iteration 10/1000 | Loss: 0.00004636
Iteration 11/1000 | Loss: 0.00004571
Iteration 12/1000 | Loss: 0.00004531
Iteration 13/1000 | Loss: 0.00004509
Iteration 14/1000 | Loss: 0.00004491
Iteration 15/1000 | Loss: 0.00004477
Iteration 16/1000 | Loss: 0.00004476
Iteration 17/1000 | Loss: 0.00004474
Iteration 18/1000 | Loss: 0.00004468
Iteration 19/1000 | Loss: 0.00004463
Iteration 20/1000 | Loss: 0.00004457
Iteration 21/1000 | Loss: 0.00004456
Iteration 22/1000 | Loss: 0.00004455
Iteration 23/1000 | Loss: 0.00004455
Iteration 24/1000 | Loss: 0.00004453
Iteration 25/1000 | Loss: 0.00004452
Iteration 26/1000 | Loss: 0.00004452
Iteration 27/1000 | Loss: 0.00004451
Iteration 28/1000 | Loss: 0.00004451
Iteration 29/1000 | Loss: 0.00004450
Iteration 30/1000 | Loss: 0.00004450
Iteration 31/1000 | Loss: 0.00004450
Iteration 32/1000 | Loss: 0.00004450
Iteration 33/1000 | Loss: 0.00004449
Iteration 34/1000 | Loss: 0.00004448
Iteration 35/1000 | Loss: 0.00004448
Iteration 36/1000 | Loss: 0.00004447
Iteration 37/1000 | Loss: 0.00004447
Iteration 38/1000 | Loss: 0.00004447
Iteration 39/1000 | Loss: 0.00004447
Iteration 40/1000 | Loss: 0.00004446
Iteration 41/1000 | Loss: 0.00004446
Iteration 42/1000 | Loss: 0.00004446
Iteration 43/1000 | Loss: 0.00004445
Iteration 44/1000 | Loss: 0.00004444
Iteration 45/1000 | Loss: 0.00004444
Iteration 46/1000 | Loss: 0.00004444
Iteration 47/1000 | Loss: 0.00004444
Iteration 48/1000 | Loss: 0.00004444
Iteration 49/1000 | Loss: 0.00004444
Iteration 50/1000 | Loss: 0.00004444
Iteration 51/1000 | Loss: 0.00004444
Iteration 52/1000 | Loss: 0.00004443
Iteration 53/1000 | Loss: 0.00004443
Iteration 54/1000 | Loss: 0.00004443
Iteration 55/1000 | Loss: 0.00004443
Iteration 56/1000 | Loss: 0.00004443
Iteration 57/1000 | Loss: 0.00004443
Iteration 58/1000 | Loss: 0.00004442
Iteration 59/1000 | Loss: 0.00004442
Iteration 60/1000 | Loss: 0.00004442
Iteration 61/1000 | Loss: 0.00004442
Iteration 62/1000 | Loss: 0.00004442
Iteration 63/1000 | Loss: 0.00004442
Iteration 64/1000 | Loss: 0.00004441
Iteration 65/1000 | Loss: 0.00004441
Iteration 66/1000 | Loss: 0.00004441
Iteration 67/1000 | Loss: 0.00004441
Iteration 68/1000 | Loss: 0.00004441
Iteration 69/1000 | Loss: 0.00004441
Iteration 70/1000 | Loss: 0.00004441
Iteration 71/1000 | Loss: 0.00004441
Iteration 72/1000 | Loss: 0.00004441
Iteration 73/1000 | Loss: 0.00004440
Iteration 74/1000 | Loss: 0.00004440
Iteration 75/1000 | Loss: 0.00004440
Iteration 76/1000 | Loss: 0.00004440
Iteration 77/1000 | Loss: 0.00004439
Iteration 78/1000 | Loss: 0.00004439
Iteration 79/1000 | Loss: 0.00004439
Iteration 80/1000 | Loss: 0.00004438
Iteration 81/1000 | Loss: 0.00004438
Iteration 82/1000 | Loss: 0.00004438
Iteration 83/1000 | Loss: 0.00004437
Iteration 84/1000 | Loss: 0.00004437
Iteration 85/1000 | Loss: 0.00004437
Iteration 86/1000 | Loss: 0.00004437
Iteration 87/1000 | Loss: 0.00004437
Iteration 88/1000 | Loss: 0.00004437
Iteration 89/1000 | Loss: 0.00004437
Iteration 90/1000 | Loss: 0.00004437
Iteration 91/1000 | Loss: 0.00004436
Iteration 92/1000 | Loss: 0.00004436
Iteration 93/1000 | Loss: 0.00004436
Iteration 94/1000 | Loss: 0.00004436
Iteration 95/1000 | Loss: 0.00004436
Iteration 96/1000 | Loss: 0.00004435
Iteration 97/1000 | Loss: 0.00004435
Iteration 98/1000 | Loss: 0.00004435
Iteration 99/1000 | Loss: 0.00004435
Iteration 100/1000 | Loss: 0.00004435
Iteration 101/1000 | Loss: 0.00004435
Iteration 102/1000 | Loss: 0.00004435
Iteration 103/1000 | Loss: 0.00004435
Iteration 104/1000 | Loss: 0.00004434
Iteration 105/1000 | Loss: 0.00004434
Iteration 106/1000 | Loss: 0.00004434
Iteration 107/1000 | Loss: 0.00004434
Iteration 108/1000 | Loss: 0.00004434
Iteration 109/1000 | Loss: 0.00004434
Iteration 110/1000 | Loss: 0.00004434
Iteration 111/1000 | Loss: 0.00004434
Iteration 112/1000 | Loss: 0.00004434
Iteration 113/1000 | Loss: 0.00004433
Iteration 114/1000 | Loss: 0.00004433
Iteration 115/1000 | Loss: 0.00004433
Iteration 116/1000 | Loss: 0.00004433
Iteration 117/1000 | Loss: 0.00004433
Iteration 118/1000 | Loss: 0.00004433
Iteration 119/1000 | Loss: 0.00004433
Iteration 120/1000 | Loss: 0.00004433
Iteration 121/1000 | Loss: 0.00004433
Iteration 122/1000 | Loss: 0.00004433
Iteration 123/1000 | Loss: 0.00004433
Iteration 124/1000 | Loss: 0.00004433
Iteration 125/1000 | Loss: 0.00004433
Iteration 126/1000 | Loss: 0.00004433
Iteration 127/1000 | Loss: 0.00004432
Iteration 128/1000 | Loss: 0.00004432
Iteration 129/1000 | Loss: 0.00004432
Iteration 130/1000 | Loss: 0.00004432
Iteration 131/1000 | Loss: 0.00004432
Iteration 132/1000 | Loss: 0.00004432
Iteration 133/1000 | Loss: 0.00004432
Iteration 134/1000 | Loss: 0.00004432
Iteration 135/1000 | Loss: 0.00004432
Iteration 136/1000 | Loss: 0.00004432
Iteration 137/1000 | Loss: 0.00004432
Iteration 138/1000 | Loss: 0.00004432
Iteration 139/1000 | Loss: 0.00004432
Iteration 140/1000 | Loss: 0.00004432
Iteration 141/1000 | Loss: 0.00004432
Iteration 142/1000 | Loss: 0.00004432
Iteration 143/1000 | Loss: 0.00004432
Iteration 144/1000 | Loss: 0.00004432
Iteration 145/1000 | Loss: 0.00004432
Iteration 146/1000 | Loss: 0.00004432
Iteration 147/1000 | Loss: 0.00004432
Iteration 148/1000 | Loss: 0.00004432
Iteration 149/1000 | Loss: 0.00004432
Iteration 150/1000 | Loss: 0.00004432
Iteration 151/1000 | Loss: 0.00004432
Iteration 152/1000 | Loss: 0.00004432
Iteration 153/1000 | Loss: 0.00004432
Iteration 154/1000 | Loss: 0.00004432
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 154. Stopping optimization.
Last 5 losses: [4.43245553469751e-05, 4.43245553469751e-05, 4.43245553469751e-05, 4.43245553469751e-05, 4.43245553469751e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.43245553469751e-05

Optimization complete. Final v2v error: 5.749058246612549 mm

Highest mean error: 6.010542869567871 mm for frame 26

Lowest mean error: 5.482731342315674 mm for frame 0

Saving results

Total time: 38.89336681365967
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_48_us_2148/0005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_48_us_2148/0005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_48_us_2148/0005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01166843
Iteration 2/25 | Loss: 0.00273446
Iteration 3/25 | Loss: 0.00217577
Iteration 4/25 | Loss: 0.00187936
Iteration 5/25 | Loss: 0.00184054
Iteration 6/25 | Loss: 0.00183621
Iteration 7/25 | Loss: 0.00182344
Iteration 8/25 | Loss: 0.00182884
Iteration 9/25 | Loss: 0.00183077
Iteration 10/25 | Loss: 0.00182513
Iteration 11/25 | Loss: 0.00180841
Iteration 12/25 | Loss: 0.00179750
Iteration 13/25 | Loss: 0.00179097
Iteration 14/25 | Loss: 0.00178624
Iteration 15/25 | Loss: 0.00180115
Iteration 16/25 | Loss: 0.00181082
Iteration 17/25 | Loss: 0.00179485
Iteration 18/25 | Loss: 0.00177951
Iteration 19/25 | Loss: 0.00175731
Iteration 20/25 | Loss: 0.00175509
Iteration 21/25 | Loss: 0.00175361
Iteration 22/25 | Loss: 0.00175434
Iteration 23/25 | Loss: 0.00174696
Iteration 24/25 | Loss: 0.00174366
Iteration 25/25 | Loss: 0.00173753

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.71083665
Iteration 2/25 | Loss: 0.00465251
Iteration 3/25 | Loss: 0.00289054
Iteration 4/25 | Loss: 0.00289041
Iteration 5/25 | Loss: 0.00289041
Iteration 6/25 | Loss: 0.00289041
Iteration 7/25 | Loss: 0.00289041
Iteration 8/25 | Loss: 0.00289041
Iteration 9/25 | Loss: 0.00289041
Iteration 10/25 | Loss: 0.00289041
Iteration 11/25 | Loss: 0.00289041
Iteration 12/25 | Loss: 0.00289041
Iteration 13/25 | Loss: 0.00289041
Iteration 14/25 | Loss: 0.00289041
Iteration 15/25 | Loss: 0.00289041
Iteration 16/25 | Loss: 0.00289041
Iteration 17/25 | Loss: 0.00289041
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0028904052451252937, 0.0028904052451252937, 0.0028904052451252937, 0.0028904052451252937, 0.0028904052451252937]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0028904052451252937

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00289041
Iteration 2/1000 | Loss: 0.00093885
Iteration 3/1000 | Loss: 0.00032560
Iteration 4/1000 | Loss: 0.00074316
Iteration 5/1000 | Loss: 0.00116171
Iteration 6/1000 | Loss: 0.00062646
Iteration 7/1000 | Loss: 0.00170884
Iteration 8/1000 | Loss: 0.00125914
Iteration 9/1000 | Loss: 0.00181196
Iteration 10/1000 | Loss: 0.00073422
Iteration 11/1000 | Loss: 0.00031743
Iteration 12/1000 | Loss: 0.00021001
Iteration 13/1000 | Loss: 0.00011845
Iteration 14/1000 | Loss: 0.00013834
Iteration 15/1000 | Loss: 0.00069622
Iteration 16/1000 | Loss: 0.00050385
Iteration 17/1000 | Loss: 0.00077771
Iteration 18/1000 | Loss: 0.00117839
Iteration 19/1000 | Loss: 0.00026627
Iteration 20/1000 | Loss: 0.00021495
Iteration 21/1000 | Loss: 0.00029694
Iteration 22/1000 | Loss: 0.00028449
Iteration 23/1000 | Loss: 0.00015112
Iteration 24/1000 | Loss: 0.00013943
Iteration 25/1000 | Loss: 0.00023277
Iteration 26/1000 | Loss: 0.00011749
Iteration 27/1000 | Loss: 0.00015746
Iteration 28/1000 | Loss: 0.00025446
Iteration 29/1000 | Loss: 0.00020713
Iteration 30/1000 | Loss: 0.00013872
Iteration 31/1000 | Loss: 0.00016493
Iteration 32/1000 | Loss: 0.00038866
Iteration 33/1000 | Loss: 0.00030386
Iteration 34/1000 | Loss: 0.00024954
Iteration 35/1000 | Loss: 0.00022081
Iteration 36/1000 | Loss: 0.00022875
Iteration 37/1000 | Loss: 0.00020288
Iteration 38/1000 | Loss: 0.00022755
Iteration 39/1000 | Loss: 0.00020924
Iteration 40/1000 | Loss: 0.00021676
Iteration 41/1000 | Loss: 0.00023301
Iteration 42/1000 | Loss: 0.00021594
Iteration 43/1000 | Loss: 0.00015262
Iteration 44/1000 | Loss: 0.00018700
Iteration 45/1000 | Loss: 0.00015862
Iteration 46/1000 | Loss: 0.00022047
Iteration 47/1000 | Loss: 0.00017024
Iteration 48/1000 | Loss: 0.00020431
Iteration 49/1000 | Loss: 0.00020565
Iteration 50/1000 | Loss: 0.00020955
Iteration 51/1000 | Loss: 0.00031005
Iteration 52/1000 | Loss: 0.00022328
Iteration 53/1000 | Loss: 0.00021079
Iteration 54/1000 | Loss: 0.00015906
Iteration 55/1000 | Loss: 0.00017873
Iteration 56/1000 | Loss: 0.00017866
Iteration 57/1000 | Loss: 0.00019820
Iteration 58/1000 | Loss: 0.00016495
Iteration 59/1000 | Loss: 0.00020733
Iteration 60/1000 | Loss: 0.00016208
Iteration 61/1000 | Loss: 0.00013954
Iteration 62/1000 | Loss: 0.00021547
Iteration 63/1000 | Loss: 0.00020671
Iteration 64/1000 | Loss: 0.00020779
Iteration 65/1000 | Loss: 0.00017225
Iteration 66/1000 | Loss: 0.00014502
Iteration 67/1000 | Loss: 0.00018384
Iteration 68/1000 | Loss: 0.00015281
Iteration 69/1000 | Loss: 0.00013816
Iteration 70/1000 | Loss: 0.00013659
Iteration 71/1000 | Loss: 0.00018544
Iteration 72/1000 | Loss: 0.00016106
Iteration 73/1000 | Loss: 0.00024381
Iteration 74/1000 | Loss: 0.00013741
Iteration 75/1000 | Loss: 0.00020227
Iteration 76/1000 | Loss: 0.00021764
Iteration 77/1000 | Loss: 0.00015702
Iteration 78/1000 | Loss: 0.00021101
Iteration 79/1000 | Loss: 0.00015499
Iteration 80/1000 | Loss: 0.00021771
Iteration 81/1000 | Loss: 0.00019945
Iteration 82/1000 | Loss: 0.00020078
Iteration 83/1000 | Loss: 0.00019014
Iteration 84/1000 | Loss: 0.00019995
Iteration 85/1000 | Loss: 0.00324211
Iteration 86/1000 | Loss: 0.00023735
Iteration 87/1000 | Loss: 0.00011570
Iteration 88/1000 | Loss: 0.00031014
Iteration 89/1000 | Loss: 0.00023336
Iteration 90/1000 | Loss: 0.00025527
Iteration 91/1000 | Loss: 0.00011532
Iteration 92/1000 | Loss: 0.00010296
Iteration 93/1000 | Loss: 0.00011343
Iteration 94/1000 | Loss: 0.00066531
Iteration 95/1000 | Loss: 0.00025501
Iteration 96/1000 | Loss: 0.00012805
Iteration 97/1000 | Loss: 0.00032670
Iteration 98/1000 | Loss: 0.00011765
Iteration 99/1000 | Loss: 0.00017967
Iteration 100/1000 | Loss: 0.00018689
Iteration 101/1000 | Loss: 0.00018134
Iteration 102/1000 | Loss: 0.00021262
Iteration 103/1000 | Loss: 0.00022764
Iteration 104/1000 | Loss: 0.00009327
Iteration 105/1000 | Loss: 0.00009053
Iteration 106/1000 | Loss: 0.00012129
Iteration 107/1000 | Loss: 0.00018195
Iteration 108/1000 | Loss: 0.00031167
Iteration 109/1000 | Loss: 0.00019152
Iteration 110/1000 | Loss: 0.00023533
Iteration 111/1000 | Loss: 0.00010688
Iteration 112/1000 | Loss: 0.00019344
Iteration 113/1000 | Loss: 0.00018311
Iteration 114/1000 | Loss: 0.00019961
Iteration 115/1000 | Loss: 0.00018939
Iteration 116/1000 | Loss: 0.00018014
Iteration 117/1000 | Loss: 0.00018909
Iteration 118/1000 | Loss: 0.00018351
Iteration 119/1000 | Loss: 0.00017131
Iteration 120/1000 | Loss: 0.00010976
Iteration 121/1000 | Loss: 0.00032843
Iteration 122/1000 | Loss: 0.00018753
Iteration 123/1000 | Loss: 0.00024421
Iteration 124/1000 | Loss: 0.00022041
Iteration 125/1000 | Loss: 0.00017035
Iteration 126/1000 | Loss: 0.00017496
Iteration 127/1000 | Loss: 0.00021227
Iteration 128/1000 | Loss: 0.00016200
Iteration 129/1000 | Loss: 0.00255823
Iteration 130/1000 | Loss: 0.00034162
Iteration 131/1000 | Loss: 0.00020303
Iteration 132/1000 | Loss: 0.00017153
Iteration 133/1000 | Loss: 0.00017253
Iteration 134/1000 | Loss: 0.00019715
Iteration 135/1000 | Loss: 0.00030607
Iteration 136/1000 | Loss: 0.00106662
Iteration 137/1000 | Loss: 0.00018712
Iteration 138/1000 | Loss: 0.00017007
Iteration 139/1000 | Loss: 0.00016465
Iteration 140/1000 | Loss: 0.00014713
Iteration 141/1000 | Loss: 0.00023815
Iteration 142/1000 | Loss: 0.00020088
Iteration 143/1000 | Loss: 0.00019689
Iteration 144/1000 | Loss: 0.00016909
Iteration 145/1000 | Loss: 0.00018547
Iteration 146/1000 | Loss: 0.00016306
Iteration 147/1000 | Loss: 0.00020199
Iteration 148/1000 | Loss: 0.00023859
Iteration 149/1000 | Loss: 0.00019843
Iteration 150/1000 | Loss: 0.00017676
Iteration 151/1000 | Loss: 0.00018916
Iteration 152/1000 | Loss: 0.00017610
Iteration 153/1000 | Loss: 0.00019704
Iteration 154/1000 | Loss: 0.00016876
Iteration 155/1000 | Loss: 0.00018714
Iteration 156/1000 | Loss: 0.00016168
Iteration 157/1000 | Loss: 0.00123168
Iteration 158/1000 | Loss: 0.00022025
Iteration 159/1000 | Loss: 0.00007471
Iteration 160/1000 | Loss: 0.00007804
Iteration 161/1000 | Loss: 0.00027363
Iteration 162/1000 | Loss: 0.00025064
Iteration 163/1000 | Loss: 0.00007335
Iteration 164/1000 | Loss: 0.00006145
Iteration 165/1000 | Loss: 0.00006953
Iteration 166/1000 | Loss: 0.00006751
Iteration 167/1000 | Loss: 0.00007597
Iteration 168/1000 | Loss: 0.00005755
Iteration 169/1000 | Loss: 0.00006455
Iteration 170/1000 | Loss: 0.00005687
Iteration 171/1000 | Loss: 0.00007167
Iteration 172/1000 | Loss: 0.00006761
Iteration 173/1000 | Loss: 0.00034750
Iteration 174/1000 | Loss: 0.00055650
Iteration 175/1000 | Loss: 0.00040504
Iteration 176/1000 | Loss: 0.00028848
Iteration 177/1000 | Loss: 0.00007696
Iteration 178/1000 | Loss: 0.00007718
Iteration 179/1000 | Loss: 0.00007103
Iteration 180/1000 | Loss: 0.00006685
Iteration 181/1000 | Loss: 0.00006578
Iteration 182/1000 | Loss: 0.00008001
Iteration 183/1000 | Loss: 0.00006182
Iteration 184/1000 | Loss: 0.00005831
Iteration 185/1000 | Loss: 0.00005685
Iteration 186/1000 | Loss: 0.00005547
Iteration 187/1000 | Loss: 0.00005459
Iteration 188/1000 | Loss: 0.00005367
Iteration 189/1000 | Loss: 0.00005296
Iteration 190/1000 | Loss: 0.00005247
Iteration 191/1000 | Loss: 0.00005210
Iteration 192/1000 | Loss: 0.00005173
Iteration 193/1000 | Loss: 0.00005148
Iteration 194/1000 | Loss: 0.00005123
Iteration 195/1000 | Loss: 0.00005107
Iteration 196/1000 | Loss: 0.00005106
Iteration 197/1000 | Loss: 0.00005101
Iteration 198/1000 | Loss: 0.00005100
Iteration 199/1000 | Loss: 0.00005099
Iteration 200/1000 | Loss: 0.00005083
Iteration 201/1000 | Loss: 0.00005081
Iteration 202/1000 | Loss: 0.00005081
Iteration 203/1000 | Loss: 0.00005080
Iteration 204/1000 | Loss: 0.00005080
Iteration 205/1000 | Loss: 0.00005080
Iteration 206/1000 | Loss: 0.00005079
Iteration 207/1000 | Loss: 0.00005079
Iteration 208/1000 | Loss: 0.00005079
Iteration 209/1000 | Loss: 0.00005078
Iteration 210/1000 | Loss: 0.00005078
Iteration 211/1000 | Loss: 0.00005074
Iteration 212/1000 | Loss: 0.00005074
Iteration 213/1000 | Loss: 0.00005073
Iteration 214/1000 | Loss: 0.00005072
Iteration 215/1000 | Loss: 0.00005072
Iteration 216/1000 | Loss: 0.00005071
Iteration 217/1000 | Loss: 0.00005069
Iteration 218/1000 | Loss: 0.00005068
Iteration 219/1000 | Loss: 0.00005068
Iteration 220/1000 | Loss: 0.00005068
Iteration 221/1000 | Loss: 0.00005068
Iteration 222/1000 | Loss: 0.00005068
Iteration 223/1000 | Loss: 0.00005067
Iteration 224/1000 | Loss: 0.00005067
Iteration 225/1000 | Loss: 0.00005067
Iteration 226/1000 | Loss: 0.00005067
Iteration 227/1000 | Loss: 0.00005067
Iteration 228/1000 | Loss: 0.00005067
Iteration 229/1000 | Loss: 0.00005067
Iteration 230/1000 | Loss: 0.00005067
Iteration 231/1000 | Loss: 0.00005067
Iteration 232/1000 | Loss: 0.00005067
Iteration 233/1000 | Loss: 0.00005066
Iteration 234/1000 | Loss: 0.00005066
Iteration 235/1000 | Loss: 0.00005066
Iteration 236/1000 | Loss: 0.00005066
Iteration 237/1000 | Loss: 0.00005066
Iteration 238/1000 | Loss: 0.00005065
Iteration 239/1000 | Loss: 0.00005065
Iteration 240/1000 | Loss: 0.00005064
Iteration 241/1000 | Loss: 0.00005064
Iteration 242/1000 | Loss: 0.00005064
Iteration 243/1000 | Loss: 0.00005064
Iteration 244/1000 | Loss: 0.00005063
Iteration 245/1000 | Loss: 0.00005063
Iteration 246/1000 | Loss: 0.00005063
Iteration 247/1000 | Loss: 0.00005063
Iteration 248/1000 | Loss: 0.00005062
Iteration 249/1000 | Loss: 0.00005062
Iteration 250/1000 | Loss: 0.00005062
Iteration 251/1000 | Loss: 0.00005062
Iteration 252/1000 | Loss: 0.00005062
Iteration 253/1000 | Loss: 0.00005062
Iteration 254/1000 | Loss: 0.00005061
Iteration 255/1000 | Loss: 0.00005061
Iteration 256/1000 | Loss: 0.00005061
Iteration 257/1000 | Loss: 0.00005061
Iteration 258/1000 | Loss: 0.00005061
Iteration 259/1000 | Loss: 0.00005061
Iteration 260/1000 | Loss: 0.00005061
Iteration 261/1000 | Loss: 0.00005061
Iteration 262/1000 | Loss: 0.00005061
Iteration 263/1000 | Loss: 0.00005061
Iteration 264/1000 | Loss: 0.00005061
Iteration 265/1000 | Loss: 0.00005061
Iteration 266/1000 | Loss: 0.00005061
Iteration 267/1000 | Loss: 0.00005061
Iteration 268/1000 | Loss: 0.00005061
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 268. Stopping optimization.
Last 5 losses: [5.061054980615154e-05, 5.061054980615154e-05, 5.061054980615154e-05, 5.061054980615154e-05, 5.061054980615154e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 5.061054980615154e-05

Optimization complete. Final v2v error: 5.700413227081299 mm

Highest mean error: 16.65493392944336 mm for frame 196

Lowest mean error: 5.099861145019531 mm for frame 50

Saving results

Total time: 366.5832211971283
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_48_us_2148/0003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_48_us_2148/0003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_48_us_2148/0003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01205834
Iteration 2/25 | Loss: 0.01205834
Iteration 3/25 | Loss: 0.00335338
Iteration 4/25 | Loss: 0.00267128
Iteration 5/25 | Loss: 0.00261197
Iteration 6/25 | Loss: 0.00246439
Iteration 7/25 | Loss: 0.00242472
Iteration 8/25 | Loss: 0.00239806
Iteration 9/25 | Loss: 0.00239237
Iteration 10/25 | Loss: 0.00236896
Iteration 11/25 | Loss: 0.00237037
Iteration 12/25 | Loss: 0.00236628
Iteration 13/25 | Loss: 0.00236349
Iteration 14/25 | Loss: 0.00236223
Iteration 15/25 | Loss: 0.00236083
Iteration 16/25 | Loss: 0.00236065
Iteration 17/25 | Loss: 0.00236051
Iteration 18/25 | Loss: 0.00236288
Iteration 19/25 | Loss: 0.00236089
Iteration 20/25 | Loss: 0.00236462
Iteration 21/25 | Loss: 0.00235981
Iteration 22/25 | Loss: 0.00235958
Iteration 23/25 | Loss: 0.00235943
Iteration 24/25 | Loss: 0.00235942
Iteration 25/25 | Loss: 0.00235942

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46155584
Iteration 2/25 | Loss: 0.01011136
Iteration 3/25 | Loss: 0.01011136
Iteration 4/25 | Loss: 0.00948367
Iteration 5/25 | Loss: 0.00948366
Iteration 6/25 | Loss: 0.00948366
Iteration 7/25 | Loss: 0.00948366
Iteration 8/25 | Loss: 0.00948366
Iteration 9/25 | Loss: 0.00948366
Iteration 10/25 | Loss: 0.00948366
Iteration 11/25 | Loss: 0.00948366
Iteration 12/25 | Loss: 0.00948366
Iteration 13/25 | Loss: 0.00948366
Iteration 14/25 | Loss: 0.00948366
Iteration 15/25 | Loss: 0.00948366
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.009483658708631992, 0.009483658708631992, 0.009483658708631992, 0.009483658708631992, 0.009483658708631992]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.009483658708631992

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00948366
Iteration 2/1000 | Loss: 0.00189574
Iteration 3/1000 | Loss: 0.00204442
Iteration 4/1000 | Loss: 0.00313915
Iteration 5/1000 | Loss: 0.00297495
Iteration 6/1000 | Loss: 0.00296384
Iteration 7/1000 | Loss: 0.00137679
Iteration 8/1000 | Loss: 0.00182139
Iteration 9/1000 | Loss: 0.00298276
Iteration 10/1000 | Loss: 0.00094369
Iteration 11/1000 | Loss: 0.00191206
Iteration 12/1000 | Loss: 0.00147933
Iteration 13/1000 | Loss: 0.00170478
Iteration 14/1000 | Loss: 0.00168095
Iteration 15/1000 | Loss: 0.00057704
Iteration 16/1000 | Loss: 0.00047184
Iteration 17/1000 | Loss: 0.00064681
Iteration 18/1000 | Loss: 0.00179101
Iteration 19/1000 | Loss: 0.00141884
Iteration 20/1000 | Loss: 0.00066837
Iteration 21/1000 | Loss: 0.00055151
Iteration 22/1000 | Loss: 0.00042608
Iteration 23/1000 | Loss: 0.00041552
Iteration 24/1000 | Loss: 0.00041003
Iteration 25/1000 | Loss: 0.00075062
Iteration 26/1000 | Loss: 0.00074574
Iteration 27/1000 | Loss: 0.00063059
Iteration 28/1000 | Loss: 0.00040445
Iteration 29/1000 | Loss: 0.00040131
Iteration 30/1000 | Loss: 0.00039922
Iteration 31/1000 | Loss: 0.00054893
Iteration 32/1000 | Loss: 0.00039693
Iteration 33/1000 | Loss: 0.00039527
Iteration 34/1000 | Loss: 0.00039405
Iteration 35/1000 | Loss: 0.00039332
Iteration 36/1000 | Loss: 0.00046491
Iteration 37/1000 | Loss: 0.00039269
Iteration 38/1000 | Loss: 0.00039245
Iteration 39/1000 | Loss: 0.00039216
Iteration 40/1000 | Loss: 0.00039193
Iteration 41/1000 | Loss: 0.00039193
Iteration 42/1000 | Loss: 0.00052669
Iteration 43/1000 | Loss: 0.00062171
Iteration 44/1000 | Loss: 0.00079776
Iteration 45/1000 | Loss: 0.00069395
Iteration 46/1000 | Loss: 0.00044689
Iteration 47/1000 | Loss: 0.00039365
Iteration 48/1000 | Loss: 0.00039208
Iteration 49/1000 | Loss: 0.00051322
Iteration 50/1000 | Loss: 0.00052194
Iteration 51/1000 | Loss: 0.00039198
Iteration 52/1000 | Loss: 0.00039167
Iteration 53/1000 | Loss: 0.00039162
Iteration 54/1000 | Loss: 0.00039161
Iteration 55/1000 | Loss: 0.00039161
Iteration 56/1000 | Loss: 0.00039161
Iteration 57/1000 | Loss: 0.00039161
Iteration 58/1000 | Loss: 0.00039160
Iteration 59/1000 | Loss: 0.00039159
Iteration 60/1000 | Loss: 0.00039159
Iteration 61/1000 | Loss: 0.00039159
Iteration 62/1000 | Loss: 0.00039159
Iteration 63/1000 | Loss: 0.00039158
Iteration 64/1000 | Loss: 0.00039158
Iteration 65/1000 | Loss: 0.00039158
Iteration 66/1000 | Loss: 0.00039158
Iteration 67/1000 | Loss: 0.00039157
Iteration 68/1000 | Loss: 0.00039154
Iteration 69/1000 | Loss: 0.00039153
Iteration 70/1000 | Loss: 0.00039153
Iteration 71/1000 | Loss: 0.00039152
Iteration 72/1000 | Loss: 0.00039152
Iteration 73/1000 | Loss: 0.00039151
Iteration 74/1000 | Loss: 0.00039151
Iteration 75/1000 | Loss: 0.00039151
Iteration 76/1000 | Loss: 0.00039151
Iteration 77/1000 | Loss: 0.00039151
Iteration 78/1000 | Loss: 0.00039151
Iteration 79/1000 | Loss: 0.00039150
Iteration 80/1000 | Loss: 0.00039150
Iteration 81/1000 | Loss: 0.00039150
Iteration 82/1000 | Loss: 0.00039150
Iteration 83/1000 | Loss: 0.00039150
Iteration 84/1000 | Loss: 0.00039150
Iteration 85/1000 | Loss: 0.00039150
Iteration 86/1000 | Loss: 0.00039150
Iteration 87/1000 | Loss: 0.00039150
Iteration 88/1000 | Loss: 0.00039150
Iteration 89/1000 | Loss: 0.00039149
Iteration 90/1000 | Loss: 0.00039149
Iteration 91/1000 | Loss: 0.00039149
Iteration 92/1000 | Loss: 0.00039149
Iteration 93/1000 | Loss: 0.00039149
Iteration 94/1000 | Loss: 0.00039148
Iteration 95/1000 | Loss: 0.00039147
Iteration 96/1000 | Loss: 0.00039147
Iteration 97/1000 | Loss: 0.00039147
Iteration 98/1000 | Loss: 0.00039147
Iteration 99/1000 | Loss: 0.00039147
Iteration 100/1000 | Loss: 0.00039147
Iteration 101/1000 | Loss: 0.00039147
Iteration 102/1000 | Loss: 0.00039147
Iteration 103/1000 | Loss: 0.00039147
Iteration 104/1000 | Loss: 0.00039147
Iteration 105/1000 | Loss: 0.00039147
Iteration 106/1000 | Loss: 0.00039147
Iteration 107/1000 | Loss: 0.00039147
Iteration 108/1000 | Loss: 0.00039146
Iteration 109/1000 | Loss: 0.00039146
Iteration 110/1000 | Loss: 0.00039145
Iteration 111/1000 | Loss: 0.00039145
Iteration 112/1000 | Loss: 0.00039145
Iteration 113/1000 | Loss: 0.00039145
Iteration 114/1000 | Loss: 0.00039145
Iteration 115/1000 | Loss: 0.00039145
Iteration 116/1000 | Loss: 0.00039145
Iteration 117/1000 | Loss: 0.00039145
Iteration 118/1000 | Loss: 0.00039145
Iteration 119/1000 | Loss: 0.00039145
Iteration 120/1000 | Loss: 0.00039145
Iteration 121/1000 | Loss: 0.00039145
Iteration 122/1000 | Loss: 0.00039144
Iteration 123/1000 | Loss: 0.00039144
Iteration 124/1000 | Loss: 0.00039144
Iteration 125/1000 | Loss: 0.00039144
Iteration 126/1000 | Loss: 0.00039144
Iteration 127/1000 | Loss: 0.00039143
Iteration 128/1000 | Loss: 0.00039143
Iteration 129/1000 | Loss: 0.00039143
Iteration 130/1000 | Loss: 0.00039143
Iteration 131/1000 | Loss: 0.00039143
Iteration 132/1000 | Loss: 0.00039143
Iteration 133/1000 | Loss: 0.00039143
Iteration 134/1000 | Loss: 0.00039143
Iteration 135/1000 | Loss: 0.00039143
Iteration 136/1000 | Loss: 0.00039143
Iteration 137/1000 | Loss: 0.00039143
Iteration 138/1000 | Loss: 0.00039142
Iteration 139/1000 | Loss: 0.00039142
Iteration 140/1000 | Loss: 0.00039142
Iteration 141/1000 | Loss: 0.00039141
Iteration 142/1000 | Loss: 0.00039141
Iteration 143/1000 | Loss: 0.00039141
Iteration 144/1000 | Loss: 0.00039141
Iteration 145/1000 | Loss: 0.00039141
Iteration 146/1000 | Loss: 0.00039141
Iteration 147/1000 | Loss: 0.00039141
Iteration 148/1000 | Loss: 0.00039141
Iteration 149/1000 | Loss: 0.00039141
Iteration 150/1000 | Loss: 0.00039141
Iteration 151/1000 | Loss: 0.00039140
Iteration 152/1000 | Loss: 0.00039140
Iteration 153/1000 | Loss: 0.00039140
Iteration 154/1000 | Loss: 0.00039140
Iteration 155/1000 | Loss: 0.00039140
Iteration 156/1000 | Loss: 0.00039140
Iteration 157/1000 | Loss: 0.00039140
Iteration 158/1000 | Loss: 0.00039140
Iteration 159/1000 | Loss: 0.00039140
Iteration 160/1000 | Loss: 0.00039140
Iteration 161/1000 | Loss: 0.00039140
Iteration 162/1000 | Loss: 0.00039140
Iteration 163/1000 | Loss: 0.00039140
Iteration 164/1000 | Loss: 0.00039140
Iteration 165/1000 | Loss: 0.00039140
Iteration 166/1000 | Loss: 0.00039140
Iteration 167/1000 | Loss: 0.00039140
Iteration 168/1000 | Loss: 0.00039140
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 168. Stopping optimization.
Last 5 losses: [0.0003914036205969751, 0.0003914036205969751, 0.0003914036205969751, 0.0003914036205969751, 0.0003914036205969751]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0003914036205969751

Optimization complete. Final v2v error: 11.233116149902344 mm

Highest mean error: 16.404460906982422 mm for frame 188

Lowest mean error: 6.716551303863525 mm for frame 55

Saving results

Total time: 134.75531315803528
