Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=49, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 2744-2799
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_025/1072/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_025/1072.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_025/1072
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00678938
Iteration 2/25 | Loss: 0.00174706
Iteration 3/25 | Loss: 0.00147885
Iteration 4/25 | Loss: 0.00140262
Iteration 5/25 | Loss: 0.00137427
Iteration 6/25 | Loss: 0.00135497
Iteration 7/25 | Loss: 0.00134962
Iteration 8/25 | Loss: 0.00135734
Iteration 9/25 | Loss: 0.00135296
Iteration 10/25 | Loss: 0.00134726
Iteration 11/25 | Loss: 0.00134141
Iteration 12/25 | Loss: 0.00133884
Iteration 13/25 | Loss: 0.00133708
Iteration 14/25 | Loss: 0.00133964
Iteration 15/25 | Loss: 0.00133625
Iteration 16/25 | Loss: 0.00133451
Iteration 17/25 | Loss: 0.00133344
Iteration 18/25 | Loss: 0.00133326
Iteration 19/25 | Loss: 0.00133323
Iteration 20/25 | Loss: 0.00133323
Iteration 21/25 | Loss: 0.00133323
Iteration 22/25 | Loss: 0.00133323
Iteration 23/25 | Loss: 0.00133323
Iteration 24/25 | Loss: 0.00133322
Iteration 25/25 | Loss: 0.00133322

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.18151140
Iteration 2/25 | Loss: 0.00139611
Iteration 3/25 | Loss: 0.00139606
Iteration 4/25 | Loss: 0.00139606
Iteration 5/25 | Loss: 0.00139606
Iteration 6/25 | Loss: 0.00139606
Iteration 7/25 | Loss: 0.00139606
Iteration 8/25 | Loss: 0.00139606
Iteration 9/25 | Loss: 0.00139606
Iteration 10/25 | Loss: 0.00139606
Iteration 11/25 | Loss: 0.00139606
Iteration 12/25 | Loss: 0.00139606
Iteration 13/25 | Loss: 0.00139606
Iteration 14/25 | Loss: 0.00139606
Iteration 15/25 | Loss: 0.00139606
Iteration 16/25 | Loss: 0.00139606
Iteration 17/25 | Loss: 0.00139606
Iteration 18/25 | Loss: 0.00139606
Iteration 19/25 | Loss: 0.00139606
Iteration 20/25 | Loss: 0.00139606
Iteration 21/25 | Loss: 0.00139606
Iteration 22/25 | Loss: 0.00139606
Iteration 23/25 | Loss: 0.00139606
Iteration 24/25 | Loss: 0.00139606
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0013960568467155099, 0.0013960568467155099, 0.0013960568467155099, 0.0013960568467155099, 0.0013960568467155099]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013960568467155099

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00139606
Iteration 2/1000 | Loss: 0.00003614
Iteration 3/1000 | Loss: 0.00002562
Iteration 4/1000 | Loss: 0.00002287
Iteration 5/1000 | Loss: 0.00002134
Iteration 6/1000 | Loss: 0.00002069
Iteration 7/1000 | Loss: 0.00002014
Iteration 8/1000 | Loss: 0.00001956
Iteration 9/1000 | Loss: 0.00001904
Iteration 10/1000 | Loss: 0.00001859
Iteration 11/1000 | Loss: 0.00001830
Iteration 12/1000 | Loss: 0.00008926
Iteration 13/1000 | Loss: 0.00001923
Iteration 14/1000 | Loss: 0.00001802
Iteration 15/1000 | Loss: 0.00001733
Iteration 16/1000 | Loss: 0.00001670
Iteration 17/1000 | Loss: 0.00001636
Iteration 18/1000 | Loss: 0.00001629
Iteration 19/1000 | Loss: 0.00001619
Iteration 20/1000 | Loss: 0.00001619
Iteration 21/1000 | Loss: 0.00001603
Iteration 22/1000 | Loss: 0.00001589
Iteration 23/1000 | Loss: 0.00001588
Iteration 24/1000 | Loss: 0.00001585
Iteration 25/1000 | Loss: 0.00001584
Iteration 26/1000 | Loss: 0.00001583
Iteration 27/1000 | Loss: 0.00001583
Iteration 28/1000 | Loss: 0.00001582
Iteration 29/1000 | Loss: 0.00001582
Iteration 30/1000 | Loss: 0.00001582
Iteration 31/1000 | Loss: 0.00001581
Iteration 32/1000 | Loss: 0.00001581
Iteration 33/1000 | Loss: 0.00001578
Iteration 34/1000 | Loss: 0.00001573
Iteration 35/1000 | Loss: 0.00001570
Iteration 36/1000 | Loss: 0.00001570
Iteration 37/1000 | Loss: 0.00001570
Iteration 38/1000 | Loss: 0.00001566
Iteration 39/1000 | Loss: 0.00001566
Iteration 40/1000 | Loss: 0.00001565
Iteration 41/1000 | Loss: 0.00001565
Iteration 42/1000 | Loss: 0.00001564
Iteration 43/1000 | Loss: 0.00001562
Iteration 44/1000 | Loss: 0.00001562
Iteration 45/1000 | Loss: 0.00001562
Iteration 46/1000 | Loss: 0.00001561
Iteration 47/1000 | Loss: 0.00001561
Iteration 48/1000 | Loss: 0.00001561
Iteration 49/1000 | Loss: 0.00001556
Iteration 50/1000 | Loss: 0.00001556
Iteration 51/1000 | Loss: 0.00001556
Iteration 52/1000 | Loss: 0.00001556
Iteration 53/1000 | Loss: 0.00001556
Iteration 54/1000 | Loss: 0.00001556
Iteration 55/1000 | Loss: 0.00001556
Iteration 56/1000 | Loss: 0.00001556
Iteration 57/1000 | Loss: 0.00001555
Iteration 58/1000 | Loss: 0.00001555
Iteration 59/1000 | Loss: 0.00001555
Iteration 60/1000 | Loss: 0.00001555
Iteration 61/1000 | Loss: 0.00001555
Iteration 62/1000 | Loss: 0.00001555
Iteration 63/1000 | Loss: 0.00001555
Iteration 64/1000 | Loss: 0.00001555
Iteration 65/1000 | Loss: 0.00001554
Iteration 66/1000 | Loss: 0.00001554
Iteration 67/1000 | Loss: 0.00001554
Iteration 68/1000 | Loss: 0.00001554
Iteration 69/1000 | Loss: 0.00001554
Iteration 70/1000 | Loss: 0.00001554
Iteration 71/1000 | Loss: 0.00001554
Iteration 72/1000 | Loss: 0.00001553
Iteration 73/1000 | Loss: 0.00001553
Iteration 74/1000 | Loss: 0.00001553
Iteration 75/1000 | Loss: 0.00001553
Iteration 76/1000 | Loss: 0.00001553
Iteration 77/1000 | Loss: 0.00001553
Iteration 78/1000 | Loss: 0.00001553
Iteration 79/1000 | Loss: 0.00001553
Iteration 80/1000 | Loss: 0.00001552
Iteration 81/1000 | Loss: 0.00001552
Iteration 82/1000 | Loss: 0.00001552
Iteration 83/1000 | Loss: 0.00001552
Iteration 84/1000 | Loss: 0.00001552
Iteration 85/1000 | Loss: 0.00001552
Iteration 86/1000 | Loss: 0.00001552
Iteration 87/1000 | Loss: 0.00001552
Iteration 88/1000 | Loss: 0.00001551
Iteration 89/1000 | Loss: 0.00001551
Iteration 90/1000 | Loss: 0.00001551
Iteration 91/1000 | Loss: 0.00001551
Iteration 92/1000 | Loss: 0.00001550
Iteration 93/1000 | Loss: 0.00001550
Iteration 94/1000 | Loss: 0.00001550
Iteration 95/1000 | Loss: 0.00001550
Iteration 96/1000 | Loss: 0.00001550
Iteration 97/1000 | Loss: 0.00001550
Iteration 98/1000 | Loss: 0.00001550
Iteration 99/1000 | Loss: 0.00001550
Iteration 100/1000 | Loss: 0.00001549
Iteration 101/1000 | Loss: 0.00001549
Iteration 102/1000 | Loss: 0.00001549
Iteration 103/1000 | Loss: 0.00001549
Iteration 104/1000 | Loss: 0.00001549
Iteration 105/1000 | Loss: 0.00001549
Iteration 106/1000 | Loss: 0.00001549
Iteration 107/1000 | Loss: 0.00001549
Iteration 108/1000 | Loss: 0.00001549
Iteration 109/1000 | Loss: 0.00001549
Iteration 110/1000 | Loss: 0.00001548
Iteration 111/1000 | Loss: 0.00001548
Iteration 112/1000 | Loss: 0.00001548
Iteration 113/1000 | Loss: 0.00001548
Iteration 114/1000 | Loss: 0.00001548
Iteration 115/1000 | Loss: 0.00001548
Iteration 116/1000 | Loss: 0.00001548
Iteration 117/1000 | Loss: 0.00001548
Iteration 118/1000 | Loss: 0.00001548
Iteration 119/1000 | Loss: 0.00001548
Iteration 120/1000 | Loss: 0.00001548
Iteration 121/1000 | Loss: 0.00001547
Iteration 122/1000 | Loss: 0.00001547
Iteration 123/1000 | Loss: 0.00001547
Iteration 124/1000 | Loss: 0.00001547
Iteration 125/1000 | Loss: 0.00001547
Iteration 126/1000 | Loss: 0.00001547
Iteration 127/1000 | Loss: 0.00001547
Iteration 128/1000 | Loss: 0.00001547
Iteration 129/1000 | Loss: 0.00001547
Iteration 130/1000 | Loss: 0.00001547
Iteration 131/1000 | Loss: 0.00001547
Iteration 132/1000 | Loss: 0.00001547
Iteration 133/1000 | Loss: 0.00001546
Iteration 134/1000 | Loss: 0.00001546
Iteration 135/1000 | Loss: 0.00001546
Iteration 136/1000 | Loss: 0.00001546
Iteration 137/1000 | Loss: 0.00001546
Iteration 138/1000 | Loss: 0.00001546
Iteration 139/1000 | Loss: 0.00001546
Iteration 140/1000 | Loss: 0.00001546
Iteration 141/1000 | Loss: 0.00001546
Iteration 142/1000 | Loss: 0.00001546
Iteration 143/1000 | Loss: 0.00001546
Iteration 144/1000 | Loss: 0.00001546
Iteration 145/1000 | Loss: 0.00001546
Iteration 146/1000 | Loss: 0.00001546
Iteration 147/1000 | Loss: 0.00001546
Iteration 148/1000 | Loss: 0.00001546
Iteration 149/1000 | Loss: 0.00001546
Iteration 150/1000 | Loss: 0.00001546
Iteration 151/1000 | Loss: 0.00001546
Iteration 152/1000 | Loss: 0.00001546
Iteration 153/1000 | Loss: 0.00001546
Iteration 154/1000 | Loss: 0.00001546
Iteration 155/1000 | Loss: 0.00001546
Iteration 156/1000 | Loss: 0.00001546
Iteration 157/1000 | Loss: 0.00001546
Iteration 158/1000 | Loss: 0.00001546
Iteration 159/1000 | Loss: 0.00001546
Iteration 160/1000 | Loss: 0.00001546
Iteration 161/1000 | Loss: 0.00001546
Iteration 162/1000 | Loss: 0.00001546
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 162. Stopping optimization.
Last 5 losses: [1.545612030895427e-05, 1.545612030895427e-05, 1.545612030895427e-05, 1.545612030895427e-05, 1.545612030895427e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.545612030895427e-05

Optimization complete. Final v2v error: 3.292881727218628 mm

Highest mean error: 3.791534185409546 mm for frame 145

Lowest mean error: 2.8500964641571045 mm for frame 133

Saving results

Total time: 74.87192559242249
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_025/1030/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_025/1030.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_025/1030
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00830530
Iteration 2/25 | Loss: 0.00127344
Iteration 3/25 | Loss: 0.00121981
Iteration 4/25 | Loss: 0.00120924
Iteration 5/25 | Loss: 0.00120609
Iteration 6/25 | Loss: 0.00120546
Iteration 7/25 | Loss: 0.00120546
Iteration 8/25 | Loss: 0.00120546
Iteration 9/25 | Loss: 0.00120546
Iteration 10/25 | Loss: 0.00120546
Iteration 11/25 | Loss: 0.00120546
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012054602848365903, 0.0012054602848365903, 0.0012054602848365903, 0.0012054602848365903, 0.0012054602848365903]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012054602848365903

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.75371695
Iteration 2/25 | Loss: 0.00149325
Iteration 3/25 | Loss: 0.00149325
Iteration 4/25 | Loss: 0.00149325
Iteration 5/25 | Loss: 0.00149325
Iteration 6/25 | Loss: 0.00149325
Iteration 7/25 | Loss: 0.00149325
Iteration 8/25 | Loss: 0.00149325
Iteration 9/25 | Loss: 0.00149325
Iteration 10/25 | Loss: 0.00149325
Iteration 11/25 | Loss: 0.00149325
Iteration 12/25 | Loss: 0.00149325
Iteration 13/25 | Loss: 0.00149325
Iteration 14/25 | Loss: 0.00149325
Iteration 15/25 | Loss: 0.00149325
Iteration 16/25 | Loss: 0.00149325
Iteration 17/25 | Loss: 0.00149325
Iteration 18/25 | Loss: 0.00149325
Iteration 19/25 | Loss: 0.00149325
Iteration 20/25 | Loss: 0.00149325
Iteration 21/25 | Loss: 0.00149325
Iteration 22/25 | Loss: 0.00149325
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0014932488556951284, 0.0014932488556951284, 0.0014932488556951284, 0.0014932488556951284, 0.0014932488556951284]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014932488556951284

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00149325
Iteration 2/1000 | Loss: 0.00002013
Iteration 3/1000 | Loss: 0.00001494
Iteration 4/1000 | Loss: 0.00001346
Iteration 5/1000 | Loss: 0.00001275
Iteration 6/1000 | Loss: 0.00001220
Iteration 7/1000 | Loss: 0.00001186
Iteration 8/1000 | Loss: 0.00001156
Iteration 9/1000 | Loss: 0.00001128
Iteration 10/1000 | Loss: 0.00001106
Iteration 11/1000 | Loss: 0.00001101
Iteration 12/1000 | Loss: 0.00001098
Iteration 13/1000 | Loss: 0.00001096
Iteration 14/1000 | Loss: 0.00001096
Iteration 15/1000 | Loss: 0.00001091
Iteration 16/1000 | Loss: 0.00001087
Iteration 17/1000 | Loss: 0.00001074
Iteration 18/1000 | Loss: 0.00001067
Iteration 19/1000 | Loss: 0.00001064
Iteration 20/1000 | Loss: 0.00001063
Iteration 21/1000 | Loss: 0.00001063
Iteration 22/1000 | Loss: 0.00001062
Iteration 23/1000 | Loss: 0.00001057
Iteration 24/1000 | Loss: 0.00001053
Iteration 25/1000 | Loss: 0.00001052
Iteration 26/1000 | Loss: 0.00001052
Iteration 27/1000 | Loss: 0.00001051
Iteration 28/1000 | Loss: 0.00001050
Iteration 29/1000 | Loss: 0.00001049
Iteration 30/1000 | Loss: 0.00001049
Iteration 31/1000 | Loss: 0.00001048
Iteration 32/1000 | Loss: 0.00001048
Iteration 33/1000 | Loss: 0.00001047
Iteration 34/1000 | Loss: 0.00001047
Iteration 35/1000 | Loss: 0.00001045
Iteration 36/1000 | Loss: 0.00001044
Iteration 37/1000 | Loss: 0.00001042
Iteration 38/1000 | Loss: 0.00001042
Iteration 39/1000 | Loss: 0.00001041
Iteration 40/1000 | Loss: 0.00001040
Iteration 41/1000 | Loss: 0.00001039
Iteration 42/1000 | Loss: 0.00001038
Iteration 43/1000 | Loss: 0.00001036
Iteration 44/1000 | Loss: 0.00001035
Iteration 45/1000 | Loss: 0.00001034
Iteration 46/1000 | Loss: 0.00001033
Iteration 47/1000 | Loss: 0.00001033
Iteration 48/1000 | Loss: 0.00001033
Iteration 49/1000 | Loss: 0.00001032
Iteration 50/1000 | Loss: 0.00001032
Iteration 51/1000 | Loss: 0.00001031
Iteration 52/1000 | Loss: 0.00001031
Iteration 53/1000 | Loss: 0.00001031
Iteration 54/1000 | Loss: 0.00001030
Iteration 55/1000 | Loss: 0.00001030
Iteration 56/1000 | Loss: 0.00001030
Iteration 57/1000 | Loss: 0.00001030
Iteration 58/1000 | Loss: 0.00001029
Iteration 59/1000 | Loss: 0.00001029
Iteration 60/1000 | Loss: 0.00001028
Iteration 61/1000 | Loss: 0.00001028
Iteration 62/1000 | Loss: 0.00001025
Iteration 63/1000 | Loss: 0.00001025
Iteration 64/1000 | Loss: 0.00001025
Iteration 65/1000 | Loss: 0.00001024
Iteration 66/1000 | Loss: 0.00001024
Iteration 67/1000 | Loss: 0.00001024
Iteration 68/1000 | Loss: 0.00001024
Iteration 69/1000 | Loss: 0.00001023
Iteration 70/1000 | Loss: 0.00001023
Iteration 71/1000 | Loss: 0.00001021
Iteration 72/1000 | Loss: 0.00001021
Iteration 73/1000 | Loss: 0.00001021
Iteration 74/1000 | Loss: 0.00001021
Iteration 75/1000 | Loss: 0.00001021
Iteration 76/1000 | Loss: 0.00001021
Iteration 77/1000 | Loss: 0.00001021
Iteration 78/1000 | Loss: 0.00001021
Iteration 79/1000 | Loss: 0.00001021
Iteration 80/1000 | Loss: 0.00001021
Iteration 81/1000 | Loss: 0.00001020
Iteration 82/1000 | Loss: 0.00001020
Iteration 83/1000 | Loss: 0.00001020
Iteration 84/1000 | Loss: 0.00001020
Iteration 85/1000 | Loss: 0.00001019
Iteration 86/1000 | Loss: 0.00001019
Iteration 87/1000 | Loss: 0.00001018
Iteration 88/1000 | Loss: 0.00001018
Iteration 89/1000 | Loss: 0.00001017
Iteration 90/1000 | Loss: 0.00001016
Iteration 91/1000 | Loss: 0.00001016
Iteration 92/1000 | Loss: 0.00001016
Iteration 93/1000 | Loss: 0.00001016
Iteration 94/1000 | Loss: 0.00001016
Iteration 95/1000 | Loss: 0.00001016
Iteration 96/1000 | Loss: 0.00001016
Iteration 97/1000 | Loss: 0.00001016
Iteration 98/1000 | Loss: 0.00001016
Iteration 99/1000 | Loss: 0.00001015
Iteration 100/1000 | Loss: 0.00001015
Iteration 101/1000 | Loss: 0.00001015
Iteration 102/1000 | Loss: 0.00001015
Iteration 103/1000 | Loss: 0.00001014
Iteration 104/1000 | Loss: 0.00001014
Iteration 105/1000 | Loss: 0.00001014
Iteration 106/1000 | Loss: 0.00001013
Iteration 107/1000 | Loss: 0.00001013
Iteration 108/1000 | Loss: 0.00001013
Iteration 109/1000 | Loss: 0.00001013
Iteration 110/1000 | Loss: 0.00001013
Iteration 111/1000 | Loss: 0.00001013
Iteration 112/1000 | Loss: 0.00001013
Iteration 113/1000 | Loss: 0.00001013
Iteration 114/1000 | Loss: 0.00001013
Iteration 115/1000 | Loss: 0.00001013
Iteration 116/1000 | Loss: 0.00001013
Iteration 117/1000 | Loss: 0.00001013
Iteration 118/1000 | Loss: 0.00001013
Iteration 119/1000 | Loss: 0.00001012
Iteration 120/1000 | Loss: 0.00001012
Iteration 121/1000 | Loss: 0.00001012
Iteration 122/1000 | Loss: 0.00001012
Iteration 123/1000 | Loss: 0.00001011
Iteration 124/1000 | Loss: 0.00001011
Iteration 125/1000 | Loss: 0.00001011
Iteration 126/1000 | Loss: 0.00001011
Iteration 127/1000 | Loss: 0.00001011
Iteration 128/1000 | Loss: 0.00001011
Iteration 129/1000 | Loss: 0.00001011
Iteration 130/1000 | Loss: 0.00001011
Iteration 131/1000 | Loss: 0.00001011
Iteration 132/1000 | Loss: 0.00001011
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 132. Stopping optimization.
Last 5 losses: [1.0107346497534309e-05, 1.0107346497534309e-05, 1.0107346497534309e-05, 1.0107346497534309e-05, 1.0107346497534309e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0107346497534309e-05

Optimization complete. Final v2v error: 2.733922004699707 mm

Highest mean error: 3.4169368743896484 mm for frame 84

Lowest mean error: 2.553856134414673 mm for frame 128

Saving results

Total time: 38.011157512664795
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_025/1061/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_025/1061.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_025/1061
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01050889
Iteration 2/25 | Loss: 0.00445833
Iteration 3/25 | Loss: 0.00514878
Iteration 4/25 | Loss: 0.00249345
Iteration 5/25 | Loss: 0.00230916
Iteration 6/25 | Loss: 0.00201215
Iteration 7/25 | Loss: 0.00199173
Iteration 8/25 | Loss: 0.00186303
Iteration 9/25 | Loss: 0.00177217
Iteration 10/25 | Loss: 0.00168294
Iteration 11/25 | Loss: 0.00169077
Iteration 12/25 | Loss: 0.00152484
Iteration 13/25 | Loss: 0.00151405
Iteration 14/25 | Loss: 0.00146517
Iteration 15/25 | Loss: 0.00144704
Iteration 16/25 | Loss: 0.00143088
Iteration 17/25 | Loss: 0.00141504
Iteration 18/25 | Loss: 0.00141211
Iteration 19/25 | Loss: 0.00140204
Iteration 20/25 | Loss: 0.00140185
Iteration 21/25 | Loss: 0.00139603
Iteration 22/25 | Loss: 0.00139416
Iteration 23/25 | Loss: 0.00139510
Iteration 24/25 | Loss: 0.00138951
Iteration 25/25 | Loss: 0.00138998

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.54194504
Iteration 2/25 | Loss: 0.00164613
Iteration 3/25 | Loss: 0.00164613
Iteration 4/25 | Loss: 0.00164613
Iteration 5/25 | Loss: 0.00164613
Iteration 6/25 | Loss: 0.00164613
Iteration 7/25 | Loss: 0.00164613
Iteration 8/25 | Loss: 0.00164613
Iteration 9/25 | Loss: 0.00164613
Iteration 10/25 | Loss: 0.00164613
Iteration 11/25 | Loss: 0.00164613
Iteration 12/25 | Loss: 0.00164613
Iteration 13/25 | Loss: 0.00164613
Iteration 14/25 | Loss: 0.00164613
Iteration 15/25 | Loss: 0.00164613
Iteration 16/25 | Loss: 0.00164613
Iteration 17/25 | Loss: 0.00164613
Iteration 18/25 | Loss: 0.00164613
Iteration 19/25 | Loss: 0.00164613
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0016461263876408339, 0.0016461263876408339, 0.0016461263876408339, 0.0016461263876408339, 0.0016461263876408339]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0016461263876408339

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00164613
Iteration 2/1000 | Loss: 0.00012904
Iteration 3/1000 | Loss: 0.00009040
Iteration 4/1000 | Loss: 0.00007602
Iteration 5/1000 | Loss: 0.00006659
Iteration 6/1000 | Loss: 0.00006148
Iteration 7/1000 | Loss: 0.00005946
Iteration 8/1000 | Loss: 0.00005702
Iteration 9/1000 | Loss: 0.00005552
Iteration 10/1000 | Loss: 0.00005447
Iteration 11/1000 | Loss: 0.00005354
Iteration 12/1000 | Loss: 0.00005270
Iteration 13/1000 | Loss: 0.00005212
Iteration 14/1000 | Loss: 0.00005178
Iteration 15/1000 | Loss: 0.00005150
Iteration 16/1000 | Loss: 0.00019964
Iteration 17/1000 | Loss: 0.00018596
Iteration 18/1000 | Loss: 0.00012743
Iteration 19/1000 | Loss: 0.00005778
Iteration 20/1000 | Loss: 0.00005315
Iteration 21/1000 | Loss: 0.00004948
Iteration 22/1000 | Loss: 0.00004612
Iteration 23/1000 | Loss: 0.00004473
Iteration 24/1000 | Loss: 0.00004400
Iteration 25/1000 | Loss: 0.00004343
Iteration 26/1000 | Loss: 0.00004307
Iteration 27/1000 | Loss: 0.00004281
Iteration 28/1000 | Loss: 0.00004254
Iteration 29/1000 | Loss: 0.00004239
Iteration 30/1000 | Loss: 0.00004235
Iteration 31/1000 | Loss: 0.00004235
Iteration 32/1000 | Loss: 0.00004233
Iteration 33/1000 | Loss: 0.00004220
Iteration 34/1000 | Loss: 0.00004217
Iteration 35/1000 | Loss: 0.00004205
Iteration 36/1000 | Loss: 0.00004203
Iteration 37/1000 | Loss: 0.00004203
Iteration 38/1000 | Loss: 0.00004185
Iteration 39/1000 | Loss: 0.00004148
Iteration 40/1000 | Loss: 0.00004065
Iteration 41/1000 | Loss: 0.00003938
Iteration 42/1000 | Loss: 0.00003870
Iteration 43/1000 | Loss: 0.00003807
Iteration 44/1000 | Loss: 0.00003773
Iteration 45/1000 | Loss: 0.00003747
Iteration 46/1000 | Loss: 0.00003722
Iteration 47/1000 | Loss: 0.00003700
Iteration 48/1000 | Loss: 0.00003689
Iteration 49/1000 | Loss: 0.00003686
Iteration 50/1000 | Loss: 0.00003684
Iteration 51/1000 | Loss: 0.00003684
Iteration 52/1000 | Loss: 0.00003684
Iteration 53/1000 | Loss: 0.00003682
Iteration 54/1000 | Loss: 0.00003678
Iteration 55/1000 | Loss: 0.00003678
Iteration 56/1000 | Loss: 0.00003676
Iteration 57/1000 | Loss: 0.00003675
Iteration 58/1000 | Loss: 0.00003675
Iteration 59/1000 | Loss: 0.00003675
Iteration 60/1000 | Loss: 0.00003674
Iteration 61/1000 | Loss: 0.00003674
Iteration 62/1000 | Loss: 0.00003673
Iteration 63/1000 | Loss: 0.00003672
Iteration 64/1000 | Loss: 0.00003672
Iteration 65/1000 | Loss: 0.00003672
Iteration 66/1000 | Loss: 0.00003669
Iteration 67/1000 | Loss: 0.00003669
Iteration 68/1000 | Loss: 0.00003664
Iteration 69/1000 | Loss: 0.00003663
Iteration 70/1000 | Loss: 0.00003660
Iteration 71/1000 | Loss: 0.00003659
Iteration 72/1000 | Loss: 0.00003658
Iteration 73/1000 | Loss: 0.00003657
Iteration 74/1000 | Loss: 0.00003656
Iteration 75/1000 | Loss: 0.00003654
Iteration 76/1000 | Loss: 0.00003654
Iteration 77/1000 | Loss: 0.00003654
Iteration 78/1000 | Loss: 0.00003654
Iteration 79/1000 | Loss: 0.00003654
Iteration 80/1000 | Loss: 0.00003654
Iteration 81/1000 | Loss: 0.00003654
Iteration 82/1000 | Loss: 0.00003654
Iteration 83/1000 | Loss: 0.00003654
Iteration 84/1000 | Loss: 0.00003653
Iteration 85/1000 | Loss: 0.00003653
Iteration 86/1000 | Loss: 0.00003652
Iteration 87/1000 | Loss: 0.00003652
Iteration 88/1000 | Loss: 0.00003651
Iteration 89/1000 | Loss: 0.00003651
Iteration 90/1000 | Loss: 0.00003651
Iteration 91/1000 | Loss: 0.00003651
Iteration 92/1000 | Loss: 0.00003651
Iteration 93/1000 | Loss: 0.00003651
Iteration 94/1000 | Loss: 0.00003650
Iteration 95/1000 | Loss: 0.00003650
Iteration 96/1000 | Loss: 0.00003650
Iteration 97/1000 | Loss: 0.00003650
Iteration 98/1000 | Loss: 0.00003650
Iteration 99/1000 | Loss: 0.00003650
Iteration 100/1000 | Loss: 0.00003650
Iteration 101/1000 | Loss: 0.00003650
Iteration 102/1000 | Loss: 0.00003649
Iteration 103/1000 | Loss: 0.00003649
Iteration 104/1000 | Loss: 0.00003649
Iteration 105/1000 | Loss: 0.00003649
Iteration 106/1000 | Loss: 0.00003649
Iteration 107/1000 | Loss: 0.00003649
Iteration 108/1000 | Loss: 0.00003648
Iteration 109/1000 | Loss: 0.00003648
Iteration 110/1000 | Loss: 0.00003648
Iteration 111/1000 | Loss: 0.00003648
Iteration 112/1000 | Loss: 0.00003648
Iteration 113/1000 | Loss: 0.00003648
Iteration 114/1000 | Loss: 0.00003648
Iteration 115/1000 | Loss: 0.00003648
Iteration 116/1000 | Loss: 0.00003648
Iteration 117/1000 | Loss: 0.00003648
Iteration 118/1000 | Loss: 0.00003648
Iteration 119/1000 | Loss: 0.00003647
Iteration 120/1000 | Loss: 0.00003647
Iteration 121/1000 | Loss: 0.00003647
Iteration 122/1000 | Loss: 0.00003647
Iteration 123/1000 | Loss: 0.00003647
Iteration 124/1000 | Loss: 0.00003647
Iteration 125/1000 | Loss: 0.00003647
Iteration 126/1000 | Loss: 0.00003646
Iteration 127/1000 | Loss: 0.00003646
Iteration 128/1000 | Loss: 0.00003646
Iteration 129/1000 | Loss: 0.00003646
Iteration 130/1000 | Loss: 0.00003646
Iteration 131/1000 | Loss: 0.00003646
Iteration 132/1000 | Loss: 0.00003646
Iteration 133/1000 | Loss: 0.00003646
Iteration 134/1000 | Loss: 0.00003646
Iteration 135/1000 | Loss: 0.00003646
Iteration 136/1000 | Loss: 0.00003646
Iteration 137/1000 | Loss: 0.00003645
Iteration 138/1000 | Loss: 0.00003645
Iteration 139/1000 | Loss: 0.00003645
Iteration 140/1000 | Loss: 0.00003645
Iteration 141/1000 | Loss: 0.00003645
Iteration 142/1000 | Loss: 0.00003645
Iteration 143/1000 | Loss: 0.00003645
Iteration 144/1000 | Loss: 0.00003645
Iteration 145/1000 | Loss: 0.00003645
Iteration 146/1000 | Loss: 0.00003645
Iteration 147/1000 | Loss: 0.00003645
Iteration 148/1000 | Loss: 0.00003644
Iteration 149/1000 | Loss: 0.00003644
Iteration 150/1000 | Loss: 0.00003644
Iteration 151/1000 | Loss: 0.00003644
Iteration 152/1000 | Loss: 0.00003644
Iteration 153/1000 | Loss: 0.00003644
Iteration 154/1000 | Loss: 0.00003644
Iteration 155/1000 | Loss: 0.00003644
Iteration 156/1000 | Loss: 0.00003644
Iteration 157/1000 | Loss: 0.00003644
Iteration 158/1000 | Loss: 0.00003644
Iteration 159/1000 | Loss: 0.00003644
Iteration 160/1000 | Loss: 0.00003644
Iteration 161/1000 | Loss: 0.00003644
Iteration 162/1000 | Loss: 0.00003643
Iteration 163/1000 | Loss: 0.00003643
Iteration 164/1000 | Loss: 0.00003643
Iteration 165/1000 | Loss: 0.00003643
Iteration 166/1000 | Loss: 0.00003643
Iteration 167/1000 | Loss: 0.00003643
Iteration 168/1000 | Loss: 0.00003642
Iteration 169/1000 | Loss: 0.00003642
Iteration 170/1000 | Loss: 0.00003642
Iteration 171/1000 | Loss: 0.00003642
Iteration 172/1000 | Loss: 0.00003642
Iteration 173/1000 | Loss: 0.00003642
Iteration 174/1000 | Loss: 0.00003642
Iteration 175/1000 | Loss: 0.00003642
Iteration 176/1000 | Loss: 0.00003642
Iteration 177/1000 | Loss: 0.00003642
Iteration 178/1000 | Loss: 0.00003641
Iteration 179/1000 | Loss: 0.00003641
Iteration 180/1000 | Loss: 0.00003641
Iteration 181/1000 | Loss: 0.00003641
Iteration 182/1000 | Loss: 0.00003641
Iteration 183/1000 | Loss: 0.00003641
Iteration 184/1000 | Loss: 0.00003641
Iteration 185/1000 | Loss: 0.00003641
Iteration 186/1000 | Loss: 0.00003641
Iteration 187/1000 | Loss: 0.00003640
Iteration 188/1000 | Loss: 0.00003640
Iteration 189/1000 | Loss: 0.00003640
Iteration 190/1000 | Loss: 0.00003640
Iteration 191/1000 | Loss: 0.00003640
Iteration 192/1000 | Loss: 0.00003640
Iteration 193/1000 | Loss: 0.00003640
Iteration 194/1000 | Loss: 0.00003640
Iteration 195/1000 | Loss: 0.00003640
Iteration 196/1000 | Loss: 0.00003640
Iteration 197/1000 | Loss: 0.00003640
Iteration 198/1000 | Loss: 0.00003640
Iteration 199/1000 | Loss: 0.00003640
Iteration 200/1000 | Loss: 0.00003640
Iteration 201/1000 | Loss: 0.00003640
Iteration 202/1000 | Loss: 0.00003640
Iteration 203/1000 | Loss: 0.00003640
Iteration 204/1000 | Loss: 0.00003640
Iteration 205/1000 | Loss: 0.00003640
Iteration 206/1000 | Loss: 0.00003640
Iteration 207/1000 | Loss: 0.00003640
Iteration 208/1000 | Loss: 0.00003640
Iteration 209/1000 | Loss: 0.00003640
Iteration 210/1000 | Loss: 0.00003640
Iteration 211/1000 | Loss: 0.00003640
Iteration 212/1000 | Loss: 0.00003640
Iteration 213/1000 | Loss: 0.00003640
Iteration 214/1000 | Loss: 0.00003640
Iteration 215/1000 | Loss: 0.00003640
Iteration 216/1000 | Loss: 0.00003640
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 216. Stopping optimization.
Last 5 losses: [3.6395093047758564e-05, 3.6395093047758564e-05, 3.6395093047758564e-05, 3.6395093047758564e-05, 3.6395093047758564e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.6395093047758564e-05

Optimization complete. Final v2v error: 4.212124347686768 mm

Highest mean error: 11.43311595916748 mm for frame 69

Lowest mean error: 3.709839344024658 mm for frame 56

Saving results

Total time: 121.786874294281
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_025/1082/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_025/1082.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_025/1082
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00767607
Iteration 2/25 | Loss: 0.00167048
Iteration 3/25 | Loss: 0.00143059
Iteration 4/25 | Loss: 0.00140757
Iteration 5/25 | Loss: 0.00140386
Iteration 6/25 | Loss: 0.00140362
Iteration 7/25 | Loss: 0.00140362
Iteration 8/25 | Loss: 0.00140362
Iteration 9/25 | Loss: 0.00140362
Iteration 10/25 | Loss: 0.00140362
Iteration 11/25 | Loss: 0.00140362
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0014036246575415134, 0.0014036246575415134, 0.0014036246575415134, 0.0014036246575415134, 0.0014036246575415134]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014036246575415134

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.10778904
Iteration 2/25 | Loss: 0.00148427
Iteration 3/25 | Loss: 0.00148423
Iteration 4/25 | Loss: 0.00148423
Iteration 5/25 | Loss: 0.00148423
Iteration 6/25 | Loss: 0.00148423
Iteration 7/25 | Loss: 0.00148423
Iteration 8/25 | Loss: 0.00148423
Iteration 9/25 | Loss: 0.00148423
Iteration 10/25 | Loss: 0.00148423
Iteration 11/25 | Loss: 0.00148423
Iteration 12/25 | Loss: 0.00148423
Iteration 13/25 | Loss: 0.00148423
Iteration 14/25 | Loss: 0.00148423
Iteration 15/25 | Loss: 0.00148423
Iteration 16/25 | Loss: 0.00148423
Iteration 17/25 | Loss: 0.00148423
Iteration 18/25 | Loss: 0.00148423
Iteration 19/25 | Loss: 0.00148423
Iteration 20/25 | Loss: 0.00148423
Iteration 21/25 | Loss: 0.00148423
Iteration 22/25 | Loss: 0.00148423
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0014842255041003227, 0.0014842255041003227, 0.0014842255041003227, 0.0014842255041003227, 0.0014842255041003227]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014842255041003227

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00148423
Iteration 2/1000 | Loss: 0.00004562
Iteration 3/1000 | Loss: 0.00003283
Iteration 4/1000 | Loss: 0.00003005
Iteration 5/1000 | Loss: 0.00002858
Iteration 6/1000 | Loss: 0.00002788
Iteration 7/1000 | Loss: 0.00002725
Iteration 8/1000 | Loss: 0.00002680
Iteration 9/1000 | Loss: 0.00002628
Iteration 10/1000 | Loss: 0.00002577
Iteration 11/1000 | Loss: 0.00002537
Iteration 12/1000 | Loss: 0.00002502
Iteration 13/1000 | Loss: 0.00002474
Iteration 14/1000 | Loss: 0.00002446
Iteration 15/1000 | Loss: 0.00002427
Iteration 16/1000 | Loss: 0.00002411
Iteration 17/1000 | Loss: 0.00002397
Iteration 18/1000 | Loss: 0.00002385
Iteration 19/1000 | Loss: 0.00002381
Iteration 20/1000 | Loss: 0.00002380
Iteration 21/1000 | Loss: 0.00002380
Iteration 22/1000 | Loss: 0.00002380
Iteration 23/1000 | Loss: 0.00002380
Iteration 24/1000 | Loss: 0.00002379
Iteration 25/1000 | Loss: 0.00002379
Iteration 26/1000 | Loss: 0.00002378
Iteration 27/1000 | Loss: 0.00002378
Iteration 28/1000 | Loss: 0.00002378
Iteration 29/1000 | Loss: 0.00002377
Iteration 30/1000 | Loss: 0.00002377
Iteration 31/1000 | Loss: 0.00002377
Iteration 32/1000 | Loss: 0.00002376
Iteration 33/1000 | Loss: 0.00002376
Iteration 34/1000 | Loss: 0.00002375
Iteration 35/1000 | Loss: 0.00002375
Iteration 36/1000 | Loss: 0.00002374
Iteration 37/1000 | Loss: 0.00002373
Iteration 38/1000 | Loss: 0.00002373
Iteration 39/1000 | Loss: 0.00002372
Iteration 40/1000 | Loss: 0.00002372
Iteration 41/1000 | Loss: 0.00002372
Iteration 42/1000 | Loss: 0.00002371
Iteration 43/1000 | Loss: 0.00002371
Iteration 44/1000 | Loss: 0.00002371
Iteration 45/1000 | Loss: 0.00002371
Iteration 46/1000 | Loss: 0.00002371
Iteration 47/1000 | Loss: 0.00002370
Iteration 48/1000 | Loss: 0.00002370
Iteration 49/1000 | Loss: 0.00002370
Iteration 50/1000 | Loss: 0.00002369
Iteration 51/1000 | Loss: 0.00002369
Iteration 52/1000 | Loss: 0.00002369
Iteration 53/1000 | Loss: 0.00002368
Iteration 54/1000 | Loss: 0.00002368
Iteration 55/1000 | Loss: 0.00002368
Iteration 56/1000 | Loss: 0.00002368
Iteration 57/1000 | Loss: 0.00002368
Iteration 58/1000 | Loss: 0.00002367
Iteration 59/1000 | Loss: 0.00002367
Iteration 60/1000 | Loss: 0.00002367
Iteration 61/1000 | Loss: 0.00002367
Iteration 62/1000 | Loss: 0.00002366
Iteration 63/1000 | Loss: 0.00002366
Iteration 64/1000 | Loss: 0.00002366
Iteration 65/1000 | Loss: 0.00002366
Iteration 66/1000 | Loss: 0.00002366
Iteration 67/1000 | Loss: 0.00002366
Iteration 68/1000 | Loss: 0.00002366
Iteration 69/1000 | Loss: 0.00002366
Iteration 70/1000 | Loss: 0.00002366
Iteration 71/1000 | Loss: 0.00002366
Iteration 72/1000 | Loss: 0.00002365
Iteration 73/1000 | Loss: 0.00002365
Iteration 74/1000 | Loss: 0.00002365
Iteration 75/1000 | Loss: 0.00002365
Iteration 76/1000 | Loss: 0.00002364
Iteration 77/1000 | Loss: 0.00002364
Iteration 78/1000 | Loss: 0.00002364
Iteration 79/1000 | Loss: 0.00002364
Iteration 80/1000 | Loss: 0.00002364
Iteration 81/1000 | Loss: 0.00002363
Iteration 82/1000 | Loss: 0.00002363
Iteration 83/1000 | Loss: 0.00002363
Iteration 84/1000 | Loss: 0.00002363
Iteration 85/1000 | Loss: 0.00002363
Iteration 86/1000 | Loss: 0.00002362
Iteration 87/1000 | Loss: 0.00002362
Iteration 88/1000 | Loss: 0.00002362
Iteration 89/1000 | Loss: 0.00002362
Iteration 90/1000 | Loss: 0.00002362
Iteration 91/1000 | Loss: 0.00002362
Iteration 92/1000 | Loss: 0.00002362
Iteration 93/1000 | Loss: 0.00002362
Iteration 94/1000 | Loss: 0.00002362
Iteration 95/1000 | Loss: 0.00002361
Iteration 96/1000 | Loss: 0.00002361
Iteration 97/1000 | Loss: 0.00002361
Iteration 98/1000 | Loss: 0.00002361
Iteration 99/1000 | Loss: 0.00002361
Iteration 100/1000 | Loss: 0.00002361
Iteration 101/1000 | Loss: 0.00002361
Iteration 102/1000 | Loss: 0.00002361
Iteration 103/1000 | Loss: 0.00002360
Iteration 104/1000 | Loss: 0.00002360
Iteration 105/1000 | Loss: 0.00002360
Iteration 106/1000 | Loss: 0.00002360
Iteration 107/1000 | Loss: 0.00002360
Iteration 108/1000 | Loss: 0.00002359
Iteration 109/1000 | Loss: 0.00002359
Iteration 110/1000 | Loss: 0.00002359
Iteration 111/1000 | Loss: 0.00002359
Iteration 112/1000 | Loss: 0.00002359
Iteration 113/1000 | Loss: 0.00002359
Iteration 114/1000 | Loss: 0.00002359
Iteration 115/1000 | Loss: 0.00002359
Iteration 116/1000 | Loss: 0.00002359
Iteration 117/1000 | Loss: 0.00002359
Iteration 118/1000 | Loss: 0.00002359
Iteration 119/1000 | Loss: 0.00002359
Iteration 120/1000 | Loss: 0.00002358
Iteration 121/1000 | Loss: 0.00002358
Iteration 122/1000 | Loss: 0.00002358
Iteration 123/1000 | Loss: 0.00002358
Iteration 124/1000 | Loss: 0.00002358
Iteration 125/1000 | Loss: 0.00002358
Iteration 126/1000 | Loss: 0.00002358
Iteration 127/1000 | Loss: 0.00002358
Iteration 128/1000 | Loss: 0.00002358
Iteration 129/1000 | Loss: 0.00002358
Iteration 130/1000 | Loss: 0.00002357
Iteration 131/1000 | Loss: 0.00002357
Iteration 132/1000 | Loss: 0.00002357
Iteration 133/1000 | Loss: 0.00002357
Iteration 134/1000 | Loss: 0.00002357
Iteration 135/1000 | Loss: 0.00002357
Iteration 136/1000 | Loss: 0.00002357
Iteration 137/1000 | Loss: 0.00002357
Iteration 138/1000 | Loss: 0.00002356
Iteration 139/1000 | Loss: 0.00002356
Iteration 140/1000 | Loss: 0.00002356
Iteration 141/1000 | Loss: 0.00002356
Iteration 142/1000 | Loss: 0.00002356
Iteration 143/1000 | Loss: 0.00002356
Iteration 144/1000 | Loss: 0.00002356
Iteration 145/1000 | Loss: 0.00002356
Iteration 146/1000 | Loss: 0.00002356
Iteration 147/1000 | Loss: 0.00002356
Iteration 148/1000 | Loss: 0.00002356
Iteration 149/1000 | Loss: 0.00002356
Iteration 150/1000 | Loss: 0.00002356
Iteration 151/1000 | Loss: 0.00002355
Iteration 152/1000 | Loss: 0.00002355
Iteration 153/1000 | Loss: 0.00002355
Iteration 154/1000 | Loss: 0.00002355
Iteration 155/1000 | Loss: 0.00002355
Iteration 156/1000 | Loss: 0.00002355
Iteration 157/1000 | Loss: 0.00002355
Iteration 158/1000 | Loss: 0.00002355
Iteration 159/1000 | Loss: 0.00002355
Iteration 160/1000 | Loss: 0.00002355
Iteration 161/1000 | Loss: 0.00002355
Iteration 162/1000 | Loss: 0.00002355
Iteration 163/1000 | Loss: 0.00002355
Iteration 164/1000 | Loss: 0.00002355
Iteration 165/1000 | Loss: 0.00002355
Iteration 166/1000 | Loss: 0.00002355
Iteration 167/1000 | Loss: 0.00002355
Iteration 168/1000 | Loss: 0.00002355
Iteration 169/1000 | Loss: 0.00002355
Iteration 170/1000 | Loss: 0.00002355
Iteration 171/1000 | Loss: 0.00002355
Iteration 172/1000 | Loss: 0.00002355
Iteration 173/1000 | Loss: 0.00002355
Iteration 174/1000 | Loss: 0.00002355
Iteration 175/1000 | Loss: 0.00002355
Iteration 176/1000 | Loss: 0.00002355
Iteration 177/1000 | Loss: 0.00002355
Iteration 178/1000 | Loss: 0.00002355
Iteration 179/1000 | Loss: 0.00002355
Iteration 180/1000 | Loss: 0.00002355
Iteration 181/1000 | Loss: 0.00002355
Iteration 182/1000 | Loss: 0.00002355
Iteration 183/1000 | Loss: 0.00002355
Iteration 184/1000 | Loss: 0.00002355
Iteration 185/1000 | Loss: 0.00002355
Iteration 186/1000 | Loss: 0.00002355
Iteration 187/1000 | Loss: 0.00002355
Iteration 188/1000 | Loss: 0.00002355
Iteration 189/1000 | Loss: 0.00002355
Iteration 190/1000 | Loss: 0.00002355
Iteration 191/1000 | Loss: 0.00002355
Iteration 192/1000 | Loss: 0.00002355
Iteration 193/1000 | Loss: 0.00002355
Iteration 194/1000 | Loss: 0.00002355
Iteration 195/1000 | Loss: 0.00002355
Iteration 196/1000 | Loss: 0.00002355
Iteration 197/1000 | Loss: 0.00002355
Iteration 198/1000 | Loss: 0.00002355
Iteration 199/1000 | Loss: 0.00002355
Iteration 200/1000 | Loss: 0.00002355
Iteration 201/1000 | Loss: 0.00002355
Iteration 202/1000 | Loss: 0.00002355
Iteration 203/1000 | Loss: 0.00002355
Iteration 204/1000 | Loss: 0.00002355
Iteration 205/1000 | Loss: 0.00002355
Iteration 206/1000 | Loss: 0.00002355
Iteration 207/1000 | Loss: 0.00002355
Iteration 208/1000 | Loss: 0.00002355
Iteration 209/1000 | Loss: 0.00002355
Iteration 210/1000 | Loss: 0.00002355
Iteration 211/1000 | Loss: 0.00002355
Iteration 212/1000 | Loss: 0.00002355
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 212. Stopping optimization.
Last 5 losses: [2.3546901502413675e-05, 2.3546901502413675e-05, 2.3546901502413675e-05, 2.3546901502413675e-05, 2.3546901502413675e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3546901502413675e-05

Optimization complete. Final v2v error: 3.92277455329895 mm

Highest mean error: 5.004348278045654 mm for frame 207

Lowest mean error: 3.4240832328796387 mm for frame 64

Saving results

Total time: 51.802133083343506
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_025/1067/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_025/1067.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_025/1067
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00992703
Iteration 2/25 | Loss: 0.00185884
Iteration 3/25 | Loss: 0.00154400
Iteration 4/25 | Loss: 0.00155847
Iteration 5/25 | Loss: 0.00147133
Iteration 6/25 | Loss: 0.00145455
Iteration 7/25 | Loss: 0.00136641
Iteration 8/25 | Loss: 0.00133226
Iteration 9/25 | Loss: 0.00130291
Iteration 10/25 | Loss: 0.00131332
Iteration 11/25 | Loss: 0.00130739
Iteration 12/25 | Loss: 0.00129417
Iteration 13/25 | Loss: 0.00130724
Iteration 14/25 | Loss: 0.00129360
Iteration 15/25 | Loss: 0.00129130
Iteration 16/25 | Loss: 0.00128930
Iteration 17/25 | Loss: 0.00129004
Iteration 18/25 | Loss: 0.00127599
Iteration 19/25 | Loss: 0.00126839
Iteration 20/25 | Loss: 0.00126705
Iteration 21/25 | Loss: 0.00126880
Iteration 22/25 | Loss: 0.00126989
Iteration 23/25 | Loss: 0.00126892
Iteration 24/25 | Loss: 0.00126904
Iteration 25/25 | Loss: 0.00126879

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.29806316
Iteration 2/25 | Loss: 0.00159941
Iteration 3/25 | Loss: 0.00158220
Iteration 4/25 | Loss: 0.00158220
Iteration 5/25 | Loss: 0.00158220
Iteration 6/25 | Loss: 0.00158220
Iteration 7/25 | Loss: 0.00158219
Iteration 8/25 | Loss: 0.00158219
Iteration 9/25 | Loss: 0.00158219
Iteration 10/25 | Loss: 0.00158219
Iteration 11/25 | Loss: 0.00158219
Iteration 12/25 | Loss: 0.00158219
Iteration 13/25 | Loss: 0.00158219
Iteration 14/25 | Loss: 0.00158219
Iteration 15/25 | Loss: 0.00158219
Iteration 16/25 | Loss: 0.00158219
Iteration 17/25 | Loss: 0.00158219
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0015821935376152396, 0.0015821935376152396, 0.0015821935376152396, 0.0015821935376152396, 0.0015821935376152396]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015821935376152396

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00158219
Iteration 2/1000 | Loss: 0.00007502
Iteration 3/1000 | Loss: 0.00003989
Iteration 4/1000 | Loss: 0.00007892
Iteration 5/1000 | Loss: 0.00009465
Iteration 6/1000 | Loss: 0.00003777
Iteration 7/1000 | Loss: 0.00004642
Iteration 8/1000 | Loss: 0.00004950
Iteration 9/1000 | Loss: 0.00004975
Iteration 10/1000 | Loss: 0.00004992
Iteration 11/1000 | Loss: 0.00013380
Iteration 12/1000 | Loss: 0.00006142
Iteration 13/1000 | Loss: 0.00004795
Iteration 14/1000 | Loss: 0.00005246
Iteration 15/1000 | Loss: 0.00019112
Iteration 16/1000 | Loss: 0.00012994
Iteration 17/1000 | Loss: 0.00005190
Iteration 18/1000 | Loss: 0.00004903
Iteration 19/1000 | Loss: 0.00005175
Iteration 20/1000 | Loss: 0.00004171
Iteration 21/1000 | Loss: 0.00004700
Iteration 22/1000 | Loss: 0.00004354
Iteration 23/1000 | Loss: 0.00004588
Iteration 24/1000 | Loss: 0.00004622
Iteration 25/1000 | Loss: 0.00005814
Iteration 26/1000 | Loss: 0.00044593
Iteration 27/1000 | Loss: 0.00020067
Iteration 28/1000 | Loss: 0.00008022
Iteration 29/1000 | Loss: 0.00004011
Iteration 30/1000 | Loss: 0.00011294
Iteration 31/1000 | Loss: 0.00005566
Iteration 32/1000 | Loss: 0.00007326
Iteration 33/1000 | Loss: 0.00004708
Iteration 34/1000 | Loss: 0.00006690
Iteration 35/1000 | Loss: 0.00003987
Iteration 36/1000 | Loss: 0.00004190
Iteration 37/1000 | Loss: 0.00003960
Iteration 38/1000 | Loss: 0.00007497
Iteration 39/1000 | Loss: 0.00004879
Iteration 40/1000 | Loss: 0.00005013
Iteration 41/1000 | Loss: 0.00004494
Iteration 42/1000 | Loss: 0.00003090
Iteration 43/1000 | Loss: 0.00004766
Iteration 44/1000 | Loss: 0.00004335
Iteration 45/1000 | Loss: 0.00003566
Iteration 46/1000 | Loss: 0.00003528
Iteration 47/1000 | Loss: 0.00004517
Iteration 48/1000 | Loss: 0.00003934
Iteration 49/1000 | Loss: 0.00002759
Iteration 50/1000 | Loss: 0.00005086
Iteration 51/1000 | Loss: 0.00003931
Iteration 52/1000 | Loss: 0.00004296
Iteration 53/1000 | Loss: 0.00004093
Iteration 54/1000 | Loss: 0.00003923
Iteration 55/1000 | Loss: 0.00005666
Iteration 56/1000 | Loss: 0.00005188
Iteration 57/1000 | Loss: 0.00004489
Iteration 58/1000 | Loss: 0.00004420
Iteration 59/1000 | Loss: 0.00004385
Iteration 60/1000 | Loss: 0.00004246
Iteration 61/1000 | Loss: 0.00004016
Iteration 62/1000 | Loss: 0.00004344
Iteration 63/1000 | Loss: 0.00004436
Iteration 64/1000 | Loss: 0.00006933
Iteration 65/1000 | Loss: 0.00005115
Iteration 66/1000 | Loss: 0.00003756
Iteration 67/1000 | Loss: 0.00003621
Iteration 68/1000 | Loss: 0.00003817
Iteration 69/1000 | Loss: 0.00003796
Iteration 70/1000 | Loss: 0.00004571
Iteration 71/1000 | Loss: 0.00005284
Iteration 72/1000 | Loss: 0.00004513
Iteration 73/1000 | Loss: 0.00006052
Iteration 74/1000 | Loss: 0.00006947
Iteration 75/1000 | Loss: 0.00003745
Iteration 76/1000 | Loss: 0.00003230
Iteration 77/1000 | Loss: 0.00003472
Iteration 78/1000 | Loss: 0.00003942
Iteration 79/1000 | Loss: 0.00004100
Iteration 80/1000 | Loss: 0.00004123
Iteration 81/1000 | Loss: 0.00007855
Iteration 82/1000 | Loss: 0.00004173
Iteration 83/1000 | Loss: 0.00003599
Iteration 84/1000 | Loss: 0.00003993
Iteration 85/1000 | Loss: 0.00004639
Iteration 86/1000 | Loss: 0.00003707
Iteration 87/1000 | Loss: 0.00002747
Iteration 88/1000 | Loss: 0.00003823
Iteration 89/1000 | Loss: 0.00004320
Iteration 90/1000 | Loss: 0.00003739
Iteration 91/1000 | Loss: 0.00005904
Iteration 92/1000 | Loss: 0.00003806
Iteration 93/1000 | Loss: 0.00004215
Iteration 94/1000 | Loss: 0.00005296
Iteration 95/1000 | Loss: 0.00004249
Iteration 96/1000 | Loss: 0.00004774
Iteration 97/1000 | Loss: 0.00002138
Iteration 98/1000 | Loss: 0.00005125
Iteration 99/1000 | Loss: 0.00005031
Iteration 100/1000 | Loss: 0.00005065
Iteration 101/1000 | Loss: 0.00004521
Iteration 102/1000 | Loss: 0.00003807
Iteration 103/1000 | Loss: 0.00003482
Iteration 104/1000 | Loss: 0.00003655
Iteration 105/1000 | Loss: 0.00003352
Iteration 106/1000 | Loss: 0.00005742
Iteration 107/1000 | Loss: 0.00003523
Iteration 108/1000 | Loss: 0.00003707
Iteration 109/1000 | Loss: 0.00003210
Iteration 110/1000 | Loss: 0.00003959
Iteration 111/1000 | Loss: 0.00003196
Iteration 112/1000 | Loss: 0.00003279
Iteration 113/1000 | Loss: 0.00008370
Iteration 114/1000 | Loss: 0.00006025
Iteration 115/1000 | Loss: 0.00004160
Iteration 116/1000 | Loss: 0.00004116
Iteration 117/1000 | Loss: 0.00003231
Iteration 118/1000 | Loss: 0.00003384
Iteration 119/1000 | Loss: 0.00002890
Iteration 120/1000 | Loss: 0.00002836
Iteration 121/1000 | Loss: 0.00002793
Iteration 122/1000 | Loss: 0.00002496
Iteration 123/1000 | Loss: 0.00004270
Iteration 124/1000 | Loss: 0.00003845
Iteration 125/1000 | Loss: 0.00003789
Iteration 126/1000 | Loss: 0.00003812
Iteration 127/1000 | Loss: 0.00003968
Iteration 128/1000 | Loss: 0.00004018
Iteration 129/1000 | Loss: 0.00003691
Iteration 130/1000 | Loss: 0.00003829
Iteration 131/1000 | Loss: 0.00003443
Iteration 132/1000 | Loss: 0.00003701
Iteration 133/1000 | Loss: 0.00005533
Iteration 134/1000 | Loss: 0.00003515
Iteration 135/1000 | Loss: 0.00004981
Iteration 136/1000 | Loss: 0.00003581
Iteration 137/1000 | Loss: 0.00004151
Iteration 138/1000 | Loss: 0.00002888
Iteration 139/1000 | Loss: 0.00003149
Iteration 140/1000 | Loss: 0.00003202
Iteration 141/1000 | Loss: 0.00003634
Iteration 142/1000 | Loss: 0.00004829
Iteration 143/1000 | Loss: 0.00003874
Iteration 144/1000 | Loss: 0.00007613
Iteration 145/1000 | Loss: 0.00013930
Iteration 146/1000 | Loss: 0.00005604
Iteration 147/1000 | Loss: 0.00003764
Iteration 148/1000 | Loss: 0.00004315
Iteration 149/1000 | Loss: 0.00002814
Iteration 150/1000 | Loss: 0.00002112
Iteration 151/1000 | Loss: 0.00001840
Iteration 152/1000 | Loss: 0.00002566
Iteration 153/1000 | Loss: 0.00001483
Iteration 154/1000 | Loss: 0.00001750
Iteration 155/1000 | Loss: 0.00001555
Iteration 156/1000 | Loss: 0.00001446
Iteration 157/1000 | Loss: 0.00001446
Iteration 158/1000 | Loss: 0.00001444
Iteration 159/1000 | Loss: 0.00001443
Iteration 160/1000 | Loss: 0.00001441
Iteration 161/1000 | Loss: 0.00001440
Iteration 162/1000 | Loss: 0.00001424
Iteration 163/1000 | Loss: 0.00001423
Iteration 164/1000 | Loss: 0.00001414
Iteration 165/1000 | Loss: 0.00001414
Iteration 166/1000 | Loss: 0.00001414
Iteration 167/1000 | Loss: 0.00001413
Iteration 168/1000 | Loss: 0.00001413
Iteration 169/1000 | Loss: 0.00001413
Iteration 170/1000 | Loss: 0.00001413
Iteration 171/1000 | Loss: 0.00001413
Iteration 172/1000 | Loss: 0.00001413
Iteration 173/1000 | Loss: 0.00001413
Iteration 174/1000 | Loss: 0.00001413
Iteration 175/1000 | Loss: 0.00001413
Iteration 176/1000 | Loss: 0.00001413
Iteration 177/1000 | Loss: 0.00001412
Iteration 178/1000 | Loss: 0.00004958
Iteration 179/1000 | Loss: 0.00001528
Iteration 180/1000 | Loss: 0.00001608
Iteration 181/1000 | Loss: 0.00002179
Iteration 182/1000 | Loss: 0.00001407
Iteration 183/1000 | Loss: 0.00001406
Iteration 184/1000 | Loss: 0.00001405
Iteration 185/1000 | Loss: 0.00001405
Iteration 186/1000 | Loss: 0.00001404
Iteration 187/1000 | Loss: 0.00001404
Iteration 188/1000 | Loss: 0.00001403
Iteration 189/1000 | Loss: 0.00001403
Iteration 190/1000 | Loss: 0.00001403
Iteration 191/1000 | Loss: 0.00001402
Iteration 192/1000 | Loss: 0.00001402
Iteration 193/1000 | Loss: 0.00001401
Iteration 194/1000 | Loss: 0.00001401
Iteration 195/1000 | Loss: 0.00001401
Iteration 196/1000 | Loss: 0.00001400
Iteration 197/1000 | Loss: 0.00001400
Iteration 198/1000 | Loss: 0.00001399
Iteration 199/1000 | Loss: 0.00001396
Iteration 200/1000 | Loss: 0.00001395
Iteration 201/1000 | Loss: 0.00001394
Iteration 202/1000 | Loss: 0.00001394
Iteration 203/1000 | Loss: 0.00001393
Iteration 204/1000 | Loss: 0.00001393
Iteration 205/1000 | Loss: 0.00001392
Iteration 206/1000 | Loss: 0.00001391
Iteration 207/1000 | Loss: 0.00001391
Iteration 208/1000 | Loss: 0.00001389
Iteration 209/1000 | Loss: 0.00001388
Iteration 210/1000 | Loss: 0.00001387
Iteration 211/1000 | Loss: 0.00001386
Iteration 212/1000 | Loss: 0.00001384
Iteration 213/1000 | Loss: 0.00001384
Iteration 214/1000 | Loss: 0.00001384
Iteration 215/1000 | Loss: 0.00001384
Iteration 216/1000 | Loss: 0.00001384
Iteration 217/1000 | Loss: 0.00001384
Iteration 218/1000 | Loss: 0.00001383
Iteration 219/1000 | Loss: 0.00001383
Iteration 220/1000 | Loss: 0.00001383
Iteration 221/1000 | Loss: 0.00001383
Iteration 222/1000 | Loss: 0.00001383
Iteration 223/1000 | Loss: 0.00001383
Iteration 224/1000 | Loss: 0.00001383
Iteration 225/1000 | Loss: 0.00001383
Iteration 226/1000 | Loss: 0.00001382
Iteration 227/1000 | Loss: 0.00001382
Iteration 228/1000 | Loss: 0.00001381
Iteration 229/1000 | Loss: 0.00001381
Iteration 230/1000 | Loss: 0.00001380
Iteration 231/1000 | Loss: 0.00001380
Iteration 232/1000 | Loss: 0.00001380
Iteration 233/1000 | Loss: 0.00001380
Iteration 234/1000 | Loss: 0.00001380
Iteration 235/1000 | Loss: 0.00001380
Iteration 236/1000 | Loss: 0.00001379
Iteration 237/1000 | Loss: 0.00001379
Iteration 238/1000 | Loss: 0.00001379
Iteration 239/1000 | Loss: 0.00001378
Iteration 240/1000 | Loss: 0.00001378
Iteration 241/1000 | Loss: 0.00001378
Iteration 242/1000 | Loss: 0.00001378
Iteration 243/1000 | Loss: 0.00005785
Iteration 244/1000 | Loss: 0.00001401
Iteration 245/1000 | Loss: 0.00001377
Iteration 246/1000 | Loss: 0.00001376
Iteration 247/1000 | Loss: 0.00001376
Iteration 248/1000 | Loss: 0.00001376
Iteration 249/1000 | Loss: 0.00001375
Iteration 250/1000 | Loss: 0.00001375
Iteration 251/1000 | Loss: 0.00001375
Iteration 252/1000 | Loss: 0.00001375
Iteration 253/1000 | Loss: 0.00001375
Iteration 254/1000 | Loss: 0.00001375
Iteration 255/1000 | Loss: 0.00001375
Iteration 256/1000 | Loss: 0.00001375
Iteration 257/1000 | Loss: 0.00001375
Iteration 258/1000 | Loss: 0.00001375
Iteration 259/1000 | Loss: 0.00001374
Iteration 260/1000 | Loss: 0.00001374
Iteration 261/1000 | Loss: 0.00001374
Iteration 262/1000 | Loss: 0.00001374
Iteration 263/1000 | Loss: 0.00001374
Iteration 264/1000 | Loss: 0.00001374
Iteration 265/1000 | Loss: 0.00001374
Iteration 266/1000 | Loss: 0.00001374
Iteration 267/1000 | Loss: 0.00001374
Iteration 268/1000 | Loss: 0.00001374
Iteration 269/1000 | Loss: 0.00001374
Iteration 270/1000 | Loss: 0.00001374
Iteration 271/1000 | Loss: 0.00001374
Iteration 272/1000 | Loss: 0.00001374
Iteration 273/1000 | Loss: 0.00001374
Iteration 274/1000 | Loss: 0.00001374
Iteration 275/1000 | Loss: 0.00001374
Iteration 276/1000 | Loss: 0.00001374
Iteration 277/1000 | Loss: 0.00001374
Iteration 278/1000 | Loss: 0.00001374
Iteration 279/1000 | Loss: 0.00001374
Iteration 280/1000 | Loss: 0.00001374
Iteration 281/1000 | Loss: 0.00001374
Iteration 282/1000 | Loss: 0.00001374
Iteration 283/1000 | Loss: 0.00001374
Iteration 284/1000 | Loss: 0.00001374
Iteration 285/1000 | Loss: 0.00001374
Iteration 286/1000 | Loss: 0.00001374
Iteration 287/1000 | Loss: 0.00001374
Iteration 288/1000 | Loss: 0.00001374
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 288. Stopping optimization.
Last 5 losses: [1.373947634419892e-05, 1.373947634419892e-05, 1.373947634419892e-05, 1.373947634419892e-05, 1.373947634419892e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.373947634419892e-05

Optimization complete. Final v2v error: 3.0326359272003174 mm

Highest mean error: 5.159070014953613 mm for frame 47

Lowest mean error: 2.6204261779785156 mm for frame 130

Saving results

Total time: 278.4936888217926
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_025/1057/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_025/1057.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_025/1057
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00591068
Iteration 2/25 | Loss: 0.00140768
Iteration 3/25 | Loss: 0.00130597
Iteration 4/25 | Loss: 0.00128183
Iteration 5/25 | Loss: 0.00127234
Iteration 6/25 | Loss: 0.00126954
Iteration 7/25 | Loss: 0.00126874
Iteration 8/25 | Loss: 0.00126874
Iteration 9/25 | Loss: 0.00126874
Iteration 10/25 | Loss: 0.00126874
Iteration 11/25 | Loss: 0.00126874
Iteration 12/25 | Loss: 0.00126874
Iteration 13/25 | Loss: 0.00126874
Iteration 14/25 | Loss: 0.00126874
Iteration 15/25 | Loss: 0.00126874
Iteration 16/25 | Loss: 0.00126874
Iteration 17/25 | Loss: 0.00126874
Iteration 18/25 | Loss: 0.00126874
Iteration 19/25 | Loss: 0.00126874
Iteration 20/25 | Loss: 0.00126874
Iteration 21/25 | Loss: 0.00126874
Iteration 22/25 | Loss: 0.00126874
Iteration 23/25 | Loss: 0.00126874
Iteration 24/25 | Loss: 0.00126874
Iteration 25/25 | Loss: 0.00126874

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.22035575
Iteration 2/25 | Loss: 0.00261452
Iteration 3/25 | Loss: 0.00261451
Iteration 4/25 | Loss: 0.00261451
Iteration 5/25 | Loss: 0.00261451
Iteration 6/25 | Loss: 0.00261451
Iteration 7/25 | Loss: 0.00261451
Iteration 8/25 | Loss: 0.00261451
Iteration 9/25 | Loss: 0.00261451
Iteration 10/25 | Loss: 0.00261451
Iteration 11/25 | Loss: 0.00261451
Iteration 12/25 | Loss: 0.00261451
Iteration 13/25 | Loss: 0.00261451
Iteration 14/25 | Loss: 0.00261451
Iteration 15/25 | Loss: 0.00261451
Iteration 16/25 | Loss: 0.00261451
Iteration 17/25 | Loss: 0.00261451
Iteration 18/25 | Loss: 0.00261451
Iteration 19/25 | Loss: 0.00261451
Iteration 20/25 | Loss: 0.00261451
Iteration 21/25 | Loss: 0.00261451
Iteration 22/25 | Loss: 0.00261451
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.002614510478451848, 0.002614510478451848, 0.002614510478451848, 0.002614510478451848, 0.002614510478451848]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002614510478451848

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00261451
Iteration 2/1000 | Loss: 0.00007029
Iteration 3/1000 | Loss: 0.00004628
Iteration 4/1000 | Loss: 0.00003163
Iteration 5/1000 | Loss: 0.00002779
Iteration 6/1000 | Loss: 0.00002553
Iteration 7/1000 | Loss: 0.00002418
Iteration 8/1000 | Loss: 0.00002325
Iteration 9/1000 | Loss: 0.00002259
Iteration 10/1000 | Loss: 0.00002200
Iteration 11/1000 | Loss: 0.00002190
Iteration 12/1000 | Loss: 0.00002153
Iteration 13/1000 | Loss: 0.00002124
Iteration 14/1000 | Loss: 0.00002103
Iteration 15/1000 | Loss: 0.00002087
Iteration 16/1000 | Loss: 0.00002086
Iteration 17/1000 | Loss: 0.00002085
Iteration 18/1000 | Loss: 0.00002078
Iteration 19/1000 | Loss: 0.00002077
Iteration 20/1000 | Loss: 0.00002075
Iteration 21/1000 | Loss: 0.00002070
Iteration 22/1000 | Loss: 0.00002055
Iteration 23/1000 | Loss: 0.00002055
Iteration 24/1000 | Loss: 0.00002050
Iteration 25/1000 | Loss: 0.00002049
Iteration 26/1000 | Loss: 0.00002049
Iteration 27/1000 | Loss: 0.00002048
Iteration 28/1000 | Loss: 0.00002048
Iteration 29/1000 | Loss: 0.00002045
Iteration 30/1000 | Loss: 0.00002044
Iteration 31/1000 | Loss: 0.00002044
Iteration 32/1000 | Loss: 0.00002043
Iteration 33/1000 | Loss: 0.00002043
Iteration 34/1000 | Loss: 0.00002043
Iteration 35/1000 | Loss: 0.00002043
Iteration 36/1000 | Loss: 0.00002042
Iteration 37/1000 | Loss: 0.00002042
Iteration 38/1000 | Loss: 0.00002041
Iteration 39/1000 | Loss: 0.00002041
Iteration 40/1000 | Loss: 0.00002035
Iteration 41/1000 | Loss: 0.00002031
Iteration 42/1000 | Loss: 0.00002031
Iteration 43/1000 | Loss: 0.00002030
Iteration 44/1000 | Loss: 0.00002030
Iteration 45/1000 | Loss: 0.00002030
Iteration 46/1000 | Loss: 0.00002029
Iteration 47/1000 | Loss: 0.00002029
Iteration 48/1000 | Loss: 0.00002028
Iteration 49/1000 | Loss: 0.00002027
Iteration 50/1000 | Loss: 0.00002027
Iteration 51/1000 | Loss: 0.00002024
Iteration 52/1000 | Loss: 0.00002024
Iteration 53/1000 | Loss: 0.00002024
Iteration 54/1000 | Loss: 0.00002023
Iteration 55/1000 | Loss: 0.00002023
Iteration 56/1000 | Loss: 0.00002023
Iteration 57/1000 | Loss: 0.00002023
Iteration 58/1000 | Loss: 0.00002023
Iteration 59/1000 | Loss: 0.00002023
Iteration 60/1000 | Loss: 0.00002023
Iteration 61/1000 | Loss: 0.00002023
Iteration 62/1000 | Loss: 0.00002023
Iteration 63/1000 | Loss: 0.00002023
Iteration 64/1000 | Loss: 0.00002023
Iteration 65/1000 | Loss: 0.00002023
Iteration 66/1000 | Loss: 0.00002023
Iteration 67/1000 | Loss: 0.00002023
Iteration 68/1000 | Loss: 0.00002023
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 68. Stopping optimization.
Last 5 losses: [2.023238812398631e-05, 2.023238812398631e-05, 2.023238812398631e-05, 2.023238812398631e-05, 2.023238812398631e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.023238812398631e-05

Optimization complete. Final v2v error: 3.868518590927124 mm

Highest mean error: 4.4014482498168945 mm for frame 93

Lowest mean error: 3.267197370529175 mm for frame 58

Saving results

Total time: 37.680957078933716
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_025/1025/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_025/1025.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_025/1025
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00827659
Iteration 2/25 | Loss: 0.00132184
Iteration 3/25 | Loss: 0.00122654
Iteration 4/25 | Loss: 0.00121679
Iteration 5/25 | Loss: 0.00121487
Iteration 6/25 | Loss: 0.00121482
Iteration 7/25 | Loss: 0.00121482
Iteration 8/25 | Loss: 0.00121482
Iteration 9/25 | Loss: 0.00121482
Iteration 10/25 | Loss: 0.00121482
Iteration 11/25 | Loss: 0.00121482
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012148157693445683, 0.0012148157693445683, 0.0012148157693445683, 0.0012148157693445683, 0.0012148157693445683]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012148157693445683

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.28030038
Iteration 2/25 | Loss: 0.00130213
Iteration 3/25 | Loss: 0.00130211
Iteration 4/25 | Loss: 0.00130211
Iteration 5/25 | Loss: 0.00130211
Iteration 6/25 | Loss: 0.00130211
Iteration 7/25 | Loss: 0.00130211
Iteration 8/25 | Loss: 0.00130211
Iteration 9/25 | Loss: 0.00130211
Iteration 10/25 | Loss: 0.00130211
Iteration 11/25 | Loss: 0.00130211
Iteration 12/25 | Loss: 0.00130211
Iteration 13/25 | Loss: 0.00130211
Iteration 14/25 | Loss: 0.00130211
Iteration 15/25 | Loss: 0.00130211
Iteration 16/25 | Loss: 0.00130211
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0013021057238802314, 0.0013021057238802314, 0.0013021057238802314, 0.0013021057238802314, 0.0013021057238802314]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013021057238802314

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00130211
Iteration 2/1000 | Loss: 0.00002093
Iteration 3/1000 | Loss: 0.00001394
Iteration 4/1000 | Loss: 0.00001245
Iteration 5/1000 | Loss: 0.00001142
Iteration 6/1000 | Loss: 0.00001094
Iteration 7/1000 | Loss: 0.00001044
Iteration 8/1000 | Loss: 0.00001009
Iteration 9/1000 | Loss: 0.00000990
Iteration 10/1000 | Loss: 0.00000961
Iteration 11/1000 | Loss: 0.00000952
Iteration 12/1000 | Loss: 0.00000951
Iteration 13/1000 | Loss: 0.00000950
Iteration 14/1000 | Loss: 0.00000934
Iteration 15/1000 | Loss: 0.00000934
Iteration 16/1000 | Loss: 0.00000933
Iteration 17/1000 | Loss: 0.00000932
Iteration 18/1000 | Loss: 0.00000926
Iteration 19/1000 | Loss: 0.00000925
Iteration 20/1000 | Loss: 0.00000924
Iteration 21/1000 | Loss: 0.00000923
Iteration 22/1000 | Loss: 0.00000911
Iteration 23/1000 | Loss: 0.00000907
Iteration 24/1000 | Loss: 0.00000906
Iteration 25/1000 | Loss: 0.00000906
Iteration 26/1000 | Loss: 0.00000906
Iteration 27/1000 | Loss: 0.00000906
Iteration 28/1000 | Loss: 0.00000905
Iteration 29/1000 | Loss: 0.00000905
Iteration 30/1000 | Loss: 0.00000905
Iteration 31/1000 | Loss: 0.00000904
Iteration 32/1000 | Loss: 0.00000903
Iteration 33/1000 | Loss: 0.00000901
Iteration 34/1000 | Loss: 0.00000901
Iteration 35/1000 | Loss: 0.00000901
Iteration 36/1000 | Loss: 0.00000901
Iteration 37/1000 | Loss: 0.00000900
Iteration 38/1000 | Loss: 0.00000900
Iteration 39/1000 | Loss: 0.00000899
Iteration 40/1000 | Loss: 0.00000899
Iteration 41/1000 | Loss: 0.00000898
Iteration 42/1000 | Loss: 0.00000898
Iteration 43/1000 | Loss: 0.00000898
Iteration 44/1000 | Loss: 0.00000897
Iteration 45/1000 | Loss: 0.00000897
Iteration 46/1000 | Loss: 0.00000896
Iteration 47/1000 | Loss: 0.00000896
Iteration 48/1000 | Loss: 0.00000896
Iteration 49/1000 | Loss: 0.00000896
Iteration 50/1000 | Loss: 0.00000896
Iteration 51/1000 | Loss: 0.00000896
Iteration 52/1000 | Loss: 0.00000895
Iteration 53/1000 | Loss: 0.00000895
Iteration 54/1000 | Loss: 0.00000895
Iteration 55/1000 | Loss: 0.00000895
Iteration 56/1000 | Loss: 0.00000895
Iteration 57/1000 | Loss: 0.00000895
Iteration 58/1000 | Loss: 0.00000895
Iteration 59/1000 | Loss: 0.00000895
Iteration 60/1000 | Loss: 0.00000895
Iteration 61/1000 | Loss: 0.00000894
Iteration 62/1000 | Loss: 0.00000894
Iteration 63/1000 | Loss: 0.00000894
Iteration 64/1000 | Loss: 0.00000894
Iteration 65/1000 | Loss: 0.00000894
Iteration 66/1000 | Loss: 0.00000894
Iteration 67/1000 | Loss: 0.00000894
Iteration 68/1000 | Loss: 0.00000894
Iteration 69/1000 | Loss: 0.00000893
Iteration 70/1000 | Loss: 0.00000893
Iteration 71/1000 | Loss: 0.00000890
Iteration 72/1000 | Loss: 0.00000890
Iteration 73/1000 | Loss: 0.00000890
Iteration 74/1000 | Loss: 0.00000890
Iteration 75/1000 | Loss: 0.00000889
Iteration 76/1000 | Loss: 0.00000889
Iteration 77/1000 | Loss: 0.00000889
Iteration 78/1000 | Loss: 0.00000889
Iteration 79/1000 | Loss: 0.00000889
Iteration 80/1000 | Loss: 0.00000889
Iteration 81/1000 | Loss: 0.00000889
Iteration 82/1000 | Loss: 0.00000889
Iteration 83/1000 | Loss: 0.00000889
Iteration 84/1000 | Loss: 0.00000889
Iteration 85/1000 | Loss: 0.00000889
Iteration 86/1000 | Loss: 0.00000889
Iteration 87/1000 | Loss: 0.00000888
Iteration 88/1000 | Loss: 0.00000888
Iteration 89/1000 | Loss: 0.00000888
Iteration 90/1000 | Loss: 0.00000888
Iteration 91/1000 | Loss: 0.00000888
Iteration 92/1000 | Loss: 0.00000887
Iteration 93/1000 | Loss: 0.00000887
Iteration 94/1000 | Loss: 0.00000886
Iteration 95/1000 | Loss: 0.00000886
Iteration 96/1000 | Loss: 0.00000886
Iteration 97/1000 | Loss: 0.00000885
Iteration 98/1000 | Loss: 0.00000884
Iteration 99/1000 | Loss: 0.00000884
Iteration 100/1000 | Loss: 0.00000884
Iteration 101/1000 | Loss: 0.00000884
Iteration 102/1000 | Loss: 0.00000884
Iteration 103/1000 | Loss: 0.00000884
Iteration 104/1000 | Loss: 0.00000884
Iteration 105/1000 | Loss: 0.00000884
Iteration 106/1000 | Loss: 0.00000884
Iteration 107/1000 | Loss: 0.00000884
Iteration 108/1000 | Loss: 0.00000883
Iteration 109/1000 | Loss: 0.00000883
Iteration 110/1000 | Loss: 0.00000882
Iteration 111/1000 | Loss: 0.00000882
Iteration 112/1000 | Loss: 0.00000881
Iteration 113/1000 | Loss: 0.00000881
Iteration 114/1000 | Loss: 0.00000880
Iteration 115/1000 | Loss: 0.00000880
Iteration 116/1000 | Loss: 0.00000880
Iteration 117/1000 | Loss: 0.00000880
Iteration 118/1000 | Loss: 0.00000880
Iteration 119/1000 | Loss: 0.00000880
Iteration 120/1000 | Loss: 0.00000879
Iteration 121/1000 | Loss: 0.00000879
Iteration 122/1000 | Loss: 0.00000878
Iteration 123/1000 | Loss: 0.00000878
Iteration 124/1000 | Loss: 0.00000877
Iteration 125/1000 | Loss: 0.00000877
Iteration 126/1000 | Loss: 0.00000877
Iteration 127/1000 | Loss: 0.00000877
Iteration 128/1000 | Loss: 0.00000876
Iteration 129/1000 | Loss: 0.00000876
Iteration 130/1000 | Loss: 0.00000876
Iteration 131/1000 | Loss: 0.00000876
Iteration 132/1000 | Loss: 0.00000875
Iteration 133/1000 | Loss: 0.00000875
Iteration 134/1000 | Loss: 0.00000875
Iteration 135/1000 | Loss: 0.00000875
Iteration 136/1000 | Loss: 0.00000875
Iteration 137/1000 | Loss: 0.00000875
Iteration 138/1000 | Loss: 0.00000874
Iteration 139/1000 | Loss: 0.00000874
Iteration 140/1000 | Loss: 0.00000874
Iteration 141/1000 | Loss: 0.00000874
Iteration 142/1000 | Loss: 0.00000874
Iteration 143/1000 | Loss: 0.00000874
Iteration 144/1000 | Loss: 0.00000874
Iteration 145/1000 | Loss: 0.00000874
Iteration 146/1000 | Loss: 0.00000873
Iteration 147/1000 | Loss: 0.00000873
Iteration 148/1000 | Loss: 0.00000873
Iteration 149/1000 | Loss: 0.00000873
Iteration 150/1000 | Loss: 0.00000873
Iteration 151/1000 | Loss: 0.00000872
Iteration 152/1000 | Loss: 0.00000872
Iteration 153/1000 | Loss: 0.00000872
Iteration 154/1000 | Loss: 0.00000872
Iteration 155/1000 | Loss: 0.00000871
Iteration 156/1000 | Loss: 0.00000871
Iteration 157/1000 | Loss: 0.00000871
Iteration 158/1000 | Loss: 0.00000871
Iteration 159/1000 | Loss: 0.00000871
Iteration 160/1000 | Loss: 0.00000870
Iteration 161/1000 | Loss: 0.00000870
Iteration 162/1000 | Loss: 0.00000870
Iteration 163/1000 | Loss: 0.00000870
Iteration 164/1000 | Loss: 0.00000870
Iteration 165/1000 | Loss: 0.00000869
Iteration 166/1000 | Loss: 0.00000869
Iteration 167/1000 | Loss: 0.00000869
Iteration 168/1000 | Loss: 0.00000869
Iteration 169/1000 | Loss: 0.00000869
Iteration 170/1000 | Loss: 0.00000868
Iteration 171/1000 | Loss: 0.00000868
Iteration 172/1000 | Loss: 0.00000868
Iteration 173/1000 | Loss: 0.00000868
Iteration 174/1000 | Loss: 0.00000868
Iteration 175/1000 | Loss: 0.00000868
Iteration 176/1000 | Loss: 0.00000868
Iteration 177/1000 | Loss: 0.00000868
Iteration 178/1000 | Loss: 0.00000868
Iteration 179/1000 | Loss: 0.00000868
Iteration 180/1000 | Loss: 0.00000868
Iteration 181/1000 | Loss: 0.00000868
Iteration 182/1000 | Loss: 0.00000868
Iteration 183/1000 | Loss: 0.00000868
Iteration 184/1000 | Loss: 0.00000868
Iteration 185/1000 | Loss: 0.00000868
Iteration 186/1000 | Loss: 0.00000868
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 186. Stopping optimization.
Last 5 losses: [8.684539352543652e-06, 8.684539352543652e-06, 8.684539352543652e-06, 8.684539352543652e-06, 8.684539352543652e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.684539352543652e-06

Optimization complete. Final v2v error: 2.5623185634613037 mm

Highest mean error: 2.8125905990600586 mm for frame 26

Lowest mean error: 2.439694881439209 mm for frame 106

Saving results

Total time: 38.43250584602356
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_025/1055/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_025/1055.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_025/1055
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00399773
Iteration 2/25 | Loss: 0.00132566
Iteration 3/25 | Loss: 0.00123596
Iteration 4/25 | Loss: 0.00122846
Iteration 5/25 | Loss: 0.00122710
Iteration 6/25 | Loss: 0.00122710
Iteration 7/25 | Loss: 0.00122710
Iteration 8/25 | Loss: 0.00122710
Iteration 9/25 | Loss: 0.00122710
Iteration 10/25 | Loss: 0.00122710
Iteration 11/25 | Loss: 0.00122710
Iteration 12/25 | Loss: 0.00122710
Iteration 13/25 | Loss: 0.00122710
Iteration 14/25 | Loss: 0.00122710
Iteration 15/25 | Loss: 0.00122710
Iteration 16/25 | Loss: 0.00122710
Iteration 17/25 | Loss: 0.00122710
Iteration 18/25 | Loss: 0.00122710
Iteration 19/25 | Loss: 0.00122710
Iteration 20/25 | Loss: 0.00122710
Iteration 21/25 | Loss: 0.00122710
Iteration 22/25 | Loss: 0.00122710
Iteration 23/25 | Loss: 0.00122710
Iteration 24/25 | Loss: 0.00122710
Iteration 25/25 | Loss: 0.00122710

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.54291320
Iteration 2/25 | Loss: 0.00134818
Iteration 3/25 | Loss: 0.00134816
Iteration 4/25 | Loss: 0.00134816
Iteration 5/25 | Loss: 0.00134816
Iteration 6/25 | Loss: 0.00134816
Iteration 7/25 | Loss: 0.00134816
Iteration 8/25 | Loss: 0.00134816
Iteration 9/25 | Loss: 0.00134816
Iteration 10/25 | Loss: 0.00134816
Iteration 11/25 | Loss: 0.00134816
Iteration 12/25 | Loss: 0.00134816
Iteration 13/25 | Loss: 0.00134816
Iteration 14/25 | Loss: 0.00134816
Iteration 15/25 | Loss: 0.00134816
Iteration 16/25 | Loss: 0.00134816
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0013481592759490013, 0.0013481592759490013, 0.0013481592759490013, 0.0013481592759490013, 0.0013481592759490013]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013481592759490013

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00134816
Iteration 2/1000 | Loss: 0.00002264
Iteration 3/1000 | Loss: 0.00001489
Iteration 4/1000 | Loss: 0.00001353
Iteration 5/1000 | Loss: 0.00001246
Iteration 6/1000 | Loss: 0.00001188
Iteration 7/1000 | Loss: 0.00001157
Iteration 8/1000 | Loss: 0.00001154
Iteration 9/1000 | Loss: 0.00001117
Iteration 10/1000 | Loss: 0.00001076
Iteration 11/1000 | Loss: 0.00001058
Iteration 12/1000 | Loss: 0.00001037
Iteration 13/1000 | Loss: 0.00001035
Iteration 14/1000 | Loss: 0.00001023
Iteration 15/1000 | Loss: 0.00001021
Iteration 16/1000 | Loss: 0.00001020
Iteration 17/1000 | Loss: 0.00001016
Iteration 18/1000 | Loss: 0.00001012
Iteration 19/1000 | Loss: 0.00001009
Iteration 20/1000 | Loss: 0.00001008
Iteration 21/1000 | Loss: 0.00001008
Iteration 22/1000 | Loss: 0.00001006
Iteration 23/1000 | Loss: 0.00001005
Iteration 24/1000 | Loss: 0.00001005
Iteration 25/1000 | Loss: 0.00001004
Iteration 26/1000 | Loss: 0.00001004
Iteration 27/1000 | Loss: 0.00001004
Iteration 28/1000 | Loss: 0.00001002
Iteration 29/1000 | Loss: 0.00001000
Iteration 30/1000 | Loss: 0.00000999
Iteration 31/1000 | Loss: 0.00000999
Iteration 32/1000 | Loss: 0.00000998
Iteration 33/1000 | Loss: 0.00000998
Iteration 34/1000 | Loss: 0.00000997
Iteration 35/1000 | Loss: 0.00000996
Iteration 36/1000 | Loss: 0.00000996
Iteration 37/1000 | Loss: 0.00000995
Iteration 38/1000 | Loss: 0.00000993
Iteration 39/1000 | Loss: 0.00000993
Iteration 40/1000 | Loss: 0.00000992
Iteration 41/1000 | Loss: 0.00000991
Iteration 42/1000 | Loss: 0.00000991
Iteration 43/1000 | Loss: 0.00000990
Iteration 44/1000 | Loss: 0.00000990
Iteration 45/1000 | Loss: 0.00000989
Iteration 46/1000 | Loss: 0.00000989
Iteration 47/1000 | Loss: 0.00000988
Iteration 48/1000 | Loss: 0.00000988
Iteration 49/1000 | Loss: 0.00000987
Iteration 50/1000 | Loss: 0.00000987
Iteration 51/1000 | Loss: 0.00000987
Iteration 52/1000 | Loss: 0.00000987
Iteration 53/1000 | Loss: 0.00000986
Iteration 54/1000 | Loss: 0.00000986
Iteration 55/1000 | Loss: 0.00000986
Iteration 56/1000 | Loss: 0.00000986
Iteration 57/1000 | Loss: 0.00000986
Iteration 58/1000 | Loss: 0.00000985
Iteration 59/1000 | Loss: 0.00000984
Iteration 60/1000 | Loss: 0.00000983
Iteration 61/1000 | Loss: 0.00000982
Iteration 62/1000 | Loss: 0.00000982
Iteration 63/1000 | Loss: 0.00000982
Iteration 64/1000 | Loss: 0.00000981
Iteration 65/1000 | Loss: 0.00000981
Iteration 66/1000 | Loss: 0.00000981
Iteration 67/1000 | Loss: 0.00000980
Iteration 68/1000 | Loss: 0.00000979
Iteration 69/1000 | Loss: 0.00000976
Iteration 70/1000 | Loss: 0.00000976
Iteration 71/1000 | Loss: 0.00000976
Iteration 72/1000 | Loss: 0.00000975
Iteration 73/1000 | Loss: 0.00000974
Iteration 74/1000 | Loss: 0.00000974
Iteration 75/1000 | Loss: 0.00000972
Iteration 76/1000 | Loss: 0.00000972
Iteration 77/1000 | Loss: 0.00000971
Iteration 78/1000 | Loss: 0.00000971
Iteration 79/1000 | Loss: 0.00000971
Iteration 80/1000 | Loss: 0.00000971
Iteration 81/1000 | Loss: 0.00000970
Iteration 82/1000 | Loss: 0.00000970
Iteration 83/1000 | Loss: 0.00000970
Iteration 84/1000 | Loss: 0.00000970
Iteration 85/1000 | Loss: 0.00000968
Iteration 86/1000 | Loss: 0.00000968
Iteration 87/1000 | Loss: 0.00000967
Iteration 88/1000 | Loss: 0.00000967
Iteration 89/1000 | Loss: 0.00000967
Iteration 90/1000 | Loss: 0.00000967
Iteration 91/1000 | Loss: 0.00000966
Iteration 92/1000 | Loss: 0.00000966
Iteration 93/1000 | Loss: 0.00000966
Iteration 94/1000 | Loss: 0.00000966
Iteration 95/1000 | Loss: 0.00000966
Iteration 96/1000 | Loss: 0.00000966
Iteration 97/1000 | Loss: 0.00000965
Iteration 98/1000 | Loss: 0.00000965
Iteration 99/1000 | Loss: 0.00000964
Iteration 100/1000 | Loss: 0.00000964
Iteration 101/1000 | Loss: 0.00000964
Iteration 102/1000 | Loss: 0.00000963
Iteration 103/1000 | Loss: 0.00000963
Iteration 104/1000 | Loss: 0.00000963
Iteration 105/1000 | Loss: 0.00000963
Iteration 106/1000 | Loss: 0.00000963
Iteration 107/1000 | Loss: 0.00000963
Iteration 108/1000 | Loss: 0.00000963
Iteration 109/1000 | Loss: 0.00000963
Iteration 110/1000 | Loss: 0.00000962
Iteration 111/1000 | Loss: 0.00000962
Iteration 112/1000 | Loss: 0.00000961
Iteration 113/1000 | Loss: 0.00000961
Iteration 114/1000 | Loss: 0.00000961
Iteration 115/1000 | Loss: 0.00000961
Iteration 116/1000 | Loss: 0.00000960
Iteration 117/1000 | Loss: 0.00000960
Iteration 118/1000 | Loss: 0.00000959
Iteration 119/1000 | Loss: 0.00000959
Iteration 120/1000 | Loss: 0.00000959
Iteration 121/1000 | Loss: 0.00000959
Iteration 122/1000 | Loss: 0.00000959
Iteration 123/1000 | Loss: 0.00000959
Iteration 124/1000 | Loss: 0.00000959
Iteration 125/1000 | Loss: 0.00000958
Iteration 126/1000 | Loss: 0.00000958
Iteration 127/1000 | Loss: 0.00000958
Iteration 128/1000 | Loss: 0.00000958
Iteration 129/1000 | Loss: 0.00000957
Iteration 130/1000 | Loss: 0.00000956
Iteration 131/1000 | Loss: 0.00000956
Iteration 132/1000 | Loss: 0.00000956
Iteration 133/1000 | Loss: 0.00000956
Iteration 134/1000 | Loss: 0.00000955
Iteration 135/1000 | Loss: 0.00000955
Iteration 136/1000 | Loss: 0.00000955
Iteration 137/1000 | Loss: 0.00000954
Iteration 138/1000 | Loss: 0.00000954
Iteration 139/1000 | Loss: 0.00000953
Iteration 140/1000 | Loss: 0.00000953
Iteration 141/1000 | Loss: 0.00000953
Iteration 142/1000 | Loss: 0.00000953
Iteration 143/1000 | Loss: 0.00000953
Iteration 144/1000 | Loss: 0.00000953
Iteration 145/1000 | Loss: 0.00000952
Iteration 146/1000 | Loss: 0.00000952
Iteration 147/1000 | Loss: 0.00000952
Iteration 148/1000 | Loss: 0.00000952
Iteration 149/1000 | Loss: 0.00000952
Iteration 150/1000 | Loss: 0.00000952
Iteration 151/1000 | Loss: 0.00000952
Iteration 152/1000 | Loss: 0.00000951
Iteration 153/1000 | Loss: 0.00000951
Iteration 154/1000 | Loss: 0.00000951
Iteration 155/1000 | Loss: 0.00000951
Iteration 156/1000 | Loss: 0.00000950
Iteration 157/1000 | Loss: 0.00000950
Iteration 158/1000 | Loss: 0.00000950
Iteration 159/1000 | Loss: 0.00000950
Iteration 160/1000 | Loss: 0.00000950
Iteration 161/1000 | Loss: 0.00000950
Iteration 162/1000 | Loss: 0.00000950
Iteration 163/1000 | Loss: 0.00000950
Iteration 164/1000 | Loss: 0.00000949
Iteration 165/1000 | Loss: 0.00000949
Iteration 166/1000 | Loss: 0.00000949
Iteration 167/1000 | Loss: 0.00000949
Iteration 168/1000 | Loss: 0.00000949
Iteration 169/1000 | Loss: 0.00000949
Iteration 170/1000 | Loss: 0.00000949
Iteration 171/1000 | Loss: 0.00000949
Iteration 172/1000 | Loss: 0.00000949
Iteration 173/1000 | Loss: 0.00000949
Iteration 174/1000 | Loss: 0.00000949
Iteration 175/1000 | Loss: 0.00000949
Iteration 176/1000 | Loss: 0.00000949
Iteration 177/1000 | Loss: 0.00000949
Iteration 178/1000 | Loss: 0.00000949
Iteration 179/1000 | Loss: 0.00000948
Iteration 180/1000 | Loss: 0.00000948
Iteration 181/1000 | Loss: 0.00000948
Iteration 182/1000 | Loss: 0.00000948
Iteration 183/1000 | Loss: 0.00000948
Iteration 184/1000 | Loss: 0.00000948
Iteration 185/1000 | Loss: 0.00000948
Iteration 186/1000 | Loss: 0.00000947
Iteration 187/1000 | Loss: 0.00000947
Iteration 188/1000 | Loss: 0.00000947
Iteration 189/1000 | Loss: 0.00000947
Iteration 190/1000 | Loss: 0.00000947
Iteration 191/1000 | Loss: 0.00000947
Iteration 192/1000 | Loss: 0.00000946
Iteration 193/1000 | Loss: 0.00000946
Iteration 194/1000 | Loss: 0.00000946
Iteration 195/1000 | Loss: 0.00000946
Iteration 196/1000 | Loss: 0.00000946
Iteration 197/1000 | Loss: 0.00000946
Iteration 198/1000 | Loss: 0.00000946
Iteration 199/1000 | Loss: 0.00000946
Iteration 200/1000 | Loss: 0.00000946
Iteration 201/1000 | Loss: 0.00000946
Iteration 202/1000 | Loss: 0.00000946
Iteration 203/1000 | Loss: 0.00000946
Iteration 204/1000 | Loss: 0.00000946
Iteration 205/1000 | Loss: 0.00000946
Iteration 206/1000 | Loss: 0.00000946
Iteration 207/1000 | Loss: 0.00000946
Iteration 208/1000 | Loss: 0.00000945
Iteration 209/1000 | Loss: 0.00000945
Iteration 210/1000 | Loss: 0.00000945
Iteration 211/1000 | Loss: 0.00000945
Iteration 212/1000 | Loss: 0.00000945
Iteration 213/1000 | Loss: 0.00000945
Iteration 214/1000 | Loss: 0.00000945
Iteration 215/1000 | Loss: 0.00000945
Iteration 216/1000 | Loss: 0.00000945
Iteration 217/1000 | Loss: 0.00000945
Iteration 218/1000 | Loss: 0.00000945
Iteration 219/1000 | Loss: 0.00000945
Iteration 220/1000 | Loss: 0.00000945
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 220. Stopping optimization.
Last 5 losses: [9.452210179006215e-06, 9.452210179006215e-06, 9.452210179006215e-06, 9.452210179006215e-06, 9.452210179006215e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.452210179006215e-06

Optimization complete. Final v2v error: 2.6424448490142822 mm

Highest mean error: 2.7868893146514893 mm for frame 266

Lowest mean error: 2.4940831661224365 mm for frame 43

Saving results

Total time: 48.63249349594116
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_025/1013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_025/1013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_025/1013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00382506
Iteration 2/25 | Loss: 0.00126688
Iteration 3/25 | Loss: 0.00120898
Iteration 4/25 | Loss: 0.00120227
Iteration 5/25 | Loss: 0.00120032
Iteration 6/25 | Loss: 0.00119969
Iteration 7/25 | Loss: 0.00119967
Iteration 8/25 | Loss: 0.00119967
Iteration 9/25 | Loss: 0.00119967
Iteration 10/25 | Loss: 0.00119967
Iteration 11/25 | Loss: 0.00119967
Iteration 12/25 | Loss: 0.00119967
Iteration 13/25 | Loss: 0.00119967
Iteration 14/25 | Loss: 0.00119967
Iteration 15/25 | Loss: 0.00119967
Iteration 16/25 | Loss: 0.00119967
Iteration 17/25 | Loss: 0.00119967
Iteration 18/25 | Loss: 0.00119967
Iteration 19/25 | Loss: 0.00119967
Iteration 20/25 | Loss: 0.00119967
Iteration 21/25 | Loss: 0.00119967
Iteration 22/25 | Loss: 0.00119967
Iteration 23/25 | Loss: 0.00119967
Iteration 24/25 | Loss: 0.00119967
Iteration 25/25 | Loss: 0.00119967

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.62057173
Iteration 2/25 | Loss: 0.00153439
Iteration 3/25 | Loss: 0.00153439
Iteration 4/25 | Loss: 0.00153439
Iteration 5/25 | Loss: 0.00153438
Iteration 6/25 | Loss: 0.00153438
Iteration 7/25 | Loss: 0.00153438
Iteration 8/25 | Loss: 0.00153438
Iteration 9/25 | Loss: 0.00153438
Iteration 10/25 | Loss: 0.00153438
Iteration 11/25 | Loss: 0.00153438
Iteration 12/25 | Loss: 0.00153438
Iteration 13/25 | Loss: 0.00153438
Iteration 14/25 | Loss: 0.00153438
Iteration 15/25 | Loss: 0.00153438
Iteration 16/25 | Loss: 0.00153438
Iteration 17/25 | Loss: 0.00153438
Iteration 18/25 | Loss: 0.00153438
Iteration 19/25 | Loss: 0.00153438
Iteration 20/25 | Loss: 0.00153438
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0015343830455094576, 0.0015343830455094576, 0.0015343830455094576, 0.0015343830455094576, 0.0015343830455094576]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015343830455094576

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00153438
Iteration 2/1000 | Loss: 0.00002628
Iteration 3/1000 | Loss: 0.00001524
Iteration 4/1000 | Loss: 0.00001248
Iteration 5/1000 | Loss: 0.00001145
Iteration 6/1000 | Loss: 0.00001080
Iteration 7/1000 | Loss: 0.00001034
Iteration 8/1000 | Loss: 0.00000993
Iteration 9/1000 | Loss: 0.00000974
Iteration 10/1000 | Loss: 0.00000972
Iteration 11/1000 | Loss: 0.00000955
Iteration 12/1000 | Loss: 0.00000931
Iteration 13/1000 | Loss: 0.00000922
Iteration 14/1000 | Loss: 0.00000915
Iteration 15/1000 | Loss: 0.00000912
Iteration 16/1000 | Loss: 0.00000909
Iteration 17/1000 | Loss: 0.00000909
Iteration 18/1000 | Loss: 0.00000908
Iteration 19/1000 | Loss: 0.00000907
Iteration 20/1000 | Loss: 0.00000907
Iteration 21/1000 | Loss: 0.00000900
Iteration 22/1000 | Loss: 0.00000897
Iteration 23/1000 | Loss: 0.00000895
Iteration 24/1000 | Loss: 0.00000891
Iteration 25/1000 | Loss: 0.00000888
Iteration 26/1000 | Loss: 0.00000888
Iteration 27/1000 | Loss: 0.00000888
Iteration 28/1000 | Loss: 0.00000887
Iteration 29/1000 | Loss: 0.00000887
Iteration 30/1000 | Loss: 0.00000887
Iteration 31/1000 | Loss: 0.00000887
Iteration 32/1000 | Loss: 0.00000887
Iteration 33/1000 | Loss: 0.00000886
Iteration 34/1000 | Loss: 0.00000883
Iteration 35/1000 | Loss: 0.00000880
Iteration 36/1000 | Loss: 0.00000880
Iteration 37/1000 | Loss: 0.00000880
Iteration 38/1000 | Loss: 0.00000879
Iteration 39/1000 | Loss: 0.00000879
Iteration 40/1000 | Loss: 0.00000877
Iteration 41/1000 | Loss: 0.00000876
Iteration 42/1000 | Loss: 0.00000875
Iteration 43/1000 | Loss: 0.00000875
Iteration 44/1000 | Loss: 0.00000875
Iteration 45/1000 | Loss: 0.00000874
Iteration 46/1000 | Loss: 0.00000874
Iteration 47/1000 | Loss: 0.00000874
Iteration 48/1000 | Loss: 0.00000873
Iteration 49/1000 | Loss: 0.00000873
Iteration 50/1000 | Loss: 0.00000873
Iteration 51/1000 | Loss: 0.00000872
Iteration 52/1000 | Loss: 0.00000872
Iteration 53/1000 | Loss: 0.00000872
Iteration 54/1000 | Loss: 0.00000872
Iteration 55/1000 | Loss: 0.00000871
Iteration 56/1000 | Loss: 0.00000871
Iteration 57/1000 | Loss: 0.00000871
Iteration 58/1000 | Loss: 0.00000871
Iteration 59/1000 | Loss: 0.00000870
Iteration 60/1000 | Loss: 0.00000870
Iteration 61/1000 | Loss: 0.00000870
Iteration 62/1000 | Loss: 0.00000870
Iteration 63/1000 | Loss: 0.00000870
Iteration 64/1000 | Loss: 0.00000869
Iteration 65/1000 | Loss: 0.00000869
Iteration 66/1000 | Loss: 0.00000869
Iteration 67/1000 | Loss: 0.00000869
Iteration 68/1000 | Loss: 0.00000868
Iteration 69/1000 | Loss: 0.00000868
Iteration 70/1000 | Loss: 0.00000868
Iteration 71/1000 | Loss: 0.00000868
Iteration 72/1000 | Loss: 0.00000868
Iteration 73/1000 | Loss: 0.00000868
Iteration 74/1000 | Loss: 0.00000867
Iteration 75/1000 | Loss: 0.00000867
Iteration 76/1000 | Loss: 0.00000867
Iteration 77/1000 | Loss: 0.00000866
Iteration 78/1000 | Loss: 0.00000866
Iteration 79/1000 | Loss: 0.00000866
Iteration 80/1000 | Loss: 0.00000865
Iteration 81/1000 | Loss: 0.00000865
Iteration 82/1000 | Loss: 0.00000865
Iteration 83/1000 | Loss: 0.00000865
Iteration 84/1000 | Loss: 0.00000865
Iteration 85/1000 | Loss: 0.00000865
Iteration 86/1000 | Loss: 0.00000865
Iteration 87/1000 | Loss: 0.00000865
Iteration 88/1000 | Loss: 0.00000865
Iteration 89/1000 | Loss: 0.00000865
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 89. Stopping optimization.
Last 5 losses: [8.6521431512665e-06, 8.6521431512665e-06, 8.6521431512665e-06, 8.6521431512665e-06, 8.6521431512665e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.6521431512665e-06

Optimization complete. Final v2v error: 2.5551347732543945 mm

Highest mean error: 3.002260446548462 mm for frame 66

Lowest mean error: 2.4368114471435547 mm for frame 115

Saving results

Total time: 34.9167640209198
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_025/1056/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_025/1056.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_025/1056
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01019460
Iteration 2/25 | Loss: 0.00251420
Iteration 3/25 | Loss: 0.00212245
Iteration 4/25 | Loss: 0.00205852
Iteration 5/25 | Loss: 0.00202724
Iteration 6/25 | Loss: 0.00192297
Iteration 7/25 | Loss: 0.00171906
Iteration 8/25 | Loss: 0.00159233
Iteration 9/25 | Loss: 0.00152754
Iteration 10/25 | Loss: 0.00150733
Iteration 11/25 | Loss: 0.00150573
Iteration 12/25 | Loss: 0.00146501
Iteration 13/25 | Loss: 0.00146331
Iteration 14/25 | Loss: 0.00147334
Iteration 15/25 | Loss: 0.00147032
Iteration 16/25 | Loss: 0.00144683
Iteration 17/25 | Loss: 0.00144126
Iteration 18/25 | Loss: 0.00143980
Iteration 19/25 | Loss: 0.00143916
Iteration 20/25 | Loss: 0.00143886
Iteration 21/25 | Loss: 0.00143874
Iteration 22/25 | Loss: 0.00143865
Iteration 23/25 | Loss: 0.00143856
Iteration 24/25 | Loss: 0.00143847
Iteration 25/25 | Loss: 0.00143846

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.27180099
Iteration 2/25 | Loss: 0.00353336
Iteration 3/25 | Loss: 0.00224628
Iteration 4/25 | Loss: 0.00224627
Iteration 5/25 | Loss: 0.00224627
Iteration 6/25 | Loss: 0.00224627
Iteration 7/25 | Loss: 0.00224627
Iteration 8/25 | Loss: 0.00224627
Iteration 9/25 | Loss: 0.00224627
Iteration 10/25 | Loss: 0.00224627
Iteration 11/25 | Loss: 0.00224627
Iteration 12/25 | Loss: 0.00224627
Iteration 13/25 | Loss: 0.00224627
Iteration 14/25 | Loss: 0.00224627
Iteration 15/25 | Loss: 0.00224627
Iteration 16/25 | Loss: 0.00224627
Iteration 17/25 | Loss: 0.00224627
Iteration 18/25 | Loss: 0.00224627
Iteration 19/25 | Loss: 0.00224627
Iteration 20/25 | Loss: 0.00224627
Iteration 21/25 | Loss: 0.00224627
Iteration 22/25 | Loss: 0.00224627
Iteration 23/25 | Loss: 0.00224627
Iteration 24/25 | Loss: 0.00224627
Iteration 25/25 | Loss: 0.00224627

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00224627
Iteration 2/1000 | Loss: 0.00106816
Iteration 3/1000 | Loss: 0.00043193
Iteration 4/1000 | Loss: 0.00100839
Iteration 5/1000 | Loss: 0.00052306
Iteration 6/1000 | Loss: 0.00017464
Iteration 7/1000 | Loss: 0.00015723
Iteration 8/1000 | Loss: 0.00015392
Iteration 9/1000 | Loss: 0.00029993
Iteration 10/1000 | Loss: 0.00031323
Iteration 11/1000 | Loss: 0.00015755
Iteration 12/1000 | Loss: 0.00012574
Iteration 13/1000 | Loss: 0.00054632
Iteration 14/1000 | Loss: 0.00037586
Iteration 15/1000 | Loss: 0.00024568
Iteration 16/1000 | Loss: 0.00012703
Iteration 17/1000 | Loss: 0.00049809
Iteration 18/1000 | Loss: 0.00095461
Iteration 19/1000 | Loss: 0.00577981
Iteration 20/1000 | Loss: 0.01284247
Iteration 21/1000 | Loss: 0.00492451
Iteration 22/1000 | Loss: 0.00171628
Iteration 23/1000 | Loss: 0.00206305
Iteration 24/1000 | Loss: 0.00032567
Iteration 25/1000 | Loss: 0.00098213
Iteration 26/1000 | Loss: 0.00011773
Iteration 27/1000 | Loss: 0.00032107
Iteration 28/1000 | Loss: 0.00014888
Iteration 29/1000 | Loss: 0.00007521
Iteration 30/1000 | Loss: 0.00013716
Iteration 31/1000 | Loss: 0.00014448
Iteration 32/1000 | Loss: 0.00035301
Iteration 33/1000 | Loss: 0.00004399
Iteration 34/1000 | Loss: 0.00008904
Iteration 35/1000 | Loss: 0.00017488
Iteration 36/1000 | Loss: 0.00006362
Iteration 37/1000 | Loss: 0.00016724
Iteration 38/1000 | Loss: 0.00008270
Iteration 39/1000 | Loss: 0.00045370
Iteration 40/1000 | Loss: 0.00001775
Iteration 41/1000 | Loss: 0.00007020
Iteration 42/1000 | Loss: 0.00001495
Iteration 43/1000 | Loss: 0.00023169
Iteration 44/1000 | Loss: 0.00117979
Iteration 45/1000 | Loss: 0.00024857
Iteration 46/1000 | Loss: 0.00021814
Iteration 47/1000 | Loss: 0.00007530
Iteration 48/1000 | Loss: 0.00009842
Iteration 49/1000 | Loss: 0.00007035
Iteration 50/1000 | Loss: 0.00002384
Iteration 51/1000 | Loss: 0.00001125
Iteration 52/1000 | Loss: 0.00005164
Iteration 53/1000 | Loss: 0.00013855
Iteration 54/1000 | Loss: 0.00003388
Iteration 55/1000 | Loss: 0.00001582
Iteration 56/1000 | Loss: 0.00001279
Iteration 57/1000 | Loss: 0.00008491
Iteration 58/1000 | Loss: 0.00001883
Iteration 59/1000 | Loss: 0.00000956
Iteration 60/1000 | Loss: 0.00015334
Iteration 61/1000 | Loss: 0.00024576
Iteration 62/1000 | Loss: 0.00002435
Iteration 63/1000 | Loss: 0.00006743
Iteration 64/1000 | Loss: 0.00001041
Iteration 65/1000 | Loss: 0.00010743
Iteration 66/1000 | Loss: 0.00003717
Iteration 67/1000 | Loss: 0.00010128
Iteration 68/1000 | Loss: 0.00026046
Iteration 69/1000 | Loss: 0.00000978
Iteration 70/1000 | Loss: 0.00002702
Iteration 71/1000 | Loss: 0.00001034
Iteration 72/1000 | Loss: 0.00002417
Iteration 73/1000 | Loss: 0.00000917
Iteration 74/1000 | Loss: 0.00001497
Iteration 75/1000 | Loss: 0.00000909
Iteration 76/1000 | Loss: 0.00000909
Iteration 77/1000 | Loss: 0.00000909
Iteration 78/1000 | Loss: 0.00000909
Iteration 79/1000 | Loss: 0.00000909
Iteration 80/1000 | Loss: 0.00000908
Iteration 81/1000 | Loss: 0.00000908
Iteration 82/1000 | Loss: 0.00000908
Iteration 83/1000 | Loss: 0.00000908
Iteration 84/1000 | Loss: 0.00000908
Iteration 85/1000 | Loss: 0.00000908
Iteration 86/1000 | Loss: 0.00000908
Iteration 87/1000 | Loss: 0.00000906
Iteration 88/1000 | Loss: 0.00000904
Iteration 89/1000 | Loss: 0.00000904
Iteration 90/1000 | Loss: 0.00000904
Iteration 91/1000 | Loss: 0.00000904
Iteration 92/1000 | Loss: 0.00000904
Iteration 93/1000 | Loss: 0.00000904
Iteration 94/1000 | Loss: 0.00000904
Iteration 95/1000 | Loss: 0.00000904
Iteration 96/1000 | Loss: 0.00000904
Iteration 97/1000 | Loss: 0.00000904
Iteration 98/1000 | Loss: 0.00000904
Iteration 99/1000 | Loss: 0.00000904
Iteration 100/1000 | Loss: 0.00000903
Iteration 101/1000 | Loss: 0.00000902
Iteration 102/1000 | Loss: 0.00000902
Iteration 103/1000 | Loss: 0.00000902
Iteration 104/1000 | Loss: 0.00000900
Iteration 105/1000 | Loss: 0.00002226
Iteration 106/1000 | Loss: 0.00001466
Iteration 107/1000 | Loss: 0.00001474
Iteration 108/1000 | Loss: 0.00001256
Iteration 109/1000 | Loss: 0.00001762
Iteration 110/1000 | Loss: 0.00001466
Iteration 111/1000 | Loss: 0.00040634
Iteration 112/1000 | Loss: 0.00004474
Iteration 113/1000 | Loss: 0.00000930
Iteration 114/1000 | Loss: 0.00003031
Iteration 115/1000 | Loss: 0.00001055
Iteration 116/1000 | Loss: 0.00000915
Iteration 117/1000 | Loss: 0.00001060
Iteration 118/1000 | Loss: 0.00001060
Iteration 119/1000 | Loss: 0.00000898
Iteration 120/1000 | Loss: 0.00000897
Iteration 121/1000 | Loss: 0.00000897
Iteration 122/1000 | Loss: 0.00000897
Iteration 123/1000 | Loss: 0.00000897
Iteration 124/1000 | Loss: 0.00000897
Iteration 125/1000 | Loss: 0.00000897
Iteration 126/1000 | Loss: 0.00000897
Iteration 127/1000 | Loss: 0.00000897
Iteration 128/1000 | Loss: 0.00000896
Iteration 129/1000 | Loss: 0.00000896
Iteration 130/1000 | Loss: 0.00000896
Iteration 131/1000 | Loss: 0.00000896
Iteration 132/1000 | Loss: 0.00000896
Iteration 133/1000 | Loss: 0.00000896
Iteration 134/1000 | Loss: 0.00000896
Iteration 135/1000 | Loss: 0.00000895
Iteration 136/1000 | Loss: 0.00000895
Iteration 137/1000 | Loss: 0.00000895
Iteration 138/1000 | Loss: 0.00004444
Iteration 139/1000 | Loss: 0.00002047
Iteration 140/1000 | Loss: 0.00001031
Iteration 141/1000 | Loss: 0.00000899
Iteration 142/1000 | Loss: 0.00000898
Iteration 143/1000 | Loss: 0.00000898
Iteration 144/1000 | Loss: 0.00000898
Iteration 145/1000 | Loss: 0.00000898
Iteration 146/1000 | Loss: 0.00002796
Iteration 147/1000 | Loss: 0.00001056
Iteration 148/1000 | Loss: 0.00001770
Iteration 149/1000 | Loss: 0.00001131
Iteration 150/1000 | Loss: 0.00000903
Iteration 151/1000 | Loss: 0.00000902
Iteration 152/1000 | Loss: 0.00000901
Iteration 153/1000 | Loss: 0.00000899
Iteration 154/1000 | Loss: 0.00000899
Iteration 155/1000 | Loss: 0.00000899
Iteration 156/1000 | Loss: 0.00000899
Iteration 157/1000 | Loss: 0.00000899
Iteration 158/1000 | Loss: 0.00000899
Iteration 159/1000 | Loss: 0.00000899
Iteration 160/1000 | Loss: 0.00000899
Iteration 161/1000 | Loss: 0.00000899
Iteration 162/1000 | Loss: 0.00000899
Iteration 163/1000 | Loss: 0.00000899
Iteration 164/1000 | Loss: 0.00000899
Iteration 165/1000 | Loss: 0.00000899
Iteration 166/1000 | Loss: 0.00000899
Iteration 167/1000 | Loss: 0.00000899
Iteration 168/1000 | Loss: 0.00000899
Iteration 169/1000 | Loss: 0.00000899
Iteration 170/1000 | Loss: 0.00000899
Iteration 171/1000 | Loss: 0.00000899
Iteration 172/1000 | Loss: 0.00000899
Iteration 173/1000 | Loss: 0.00000899
Iteration 174/1000 | Loss: 0.00000899
Iteration 175/1000 | Loss: 0.00000899
Iteration 176/1000 | Loss: 0.00000899
Iteration 177/1000 | Loss: 0.00000899
Iteration 178/1000 | Loss: 0.00000899
Iteration 179/1000 | Loss: 0.00000899
Iteration 180/1000 | Loss: 0.00000899
Iteration 181/1000 | Loss: 0.00000899
Iteration 182/1000 | Loss: 0.00000899
Iteration 183/1000 | Loss: 0.00000899
Iteration 184/1000 | Loss: 0.00000899
Iteration 185/1000 | Loss: 0.00000899
Iteration 186/1000 | Loss: 0.00000899
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 186. Stopping optimization.
Last 5 losses: [8.987064575194381e-06, 8.987064575194381e-06, 8.987064575194381e-06, 8.987064575194381e-06, 8.987064575194381e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.987064575194381e-06

Optimization complete. Final v2v error: 2.5908308029174805 mm

Highest mean error: 3.1064810752868652 mm for frame 53

Lowest mean error: 2.476638078689575 mm for frame 93

Saving results

Total time: 171.97742199897766
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_025/1099/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_025/1099.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_025/1099
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00531780
Iteration 2/25 | Loss: 0.00158301
Iteration 3/25 | Loss: 0.00131125
Iteration 4/25 | Loss: 0.00128403
Iteration 5/25 | Loss: 0.00127703
Iteration 6/25 | Loss: 0.00127423
Iteration 7/25 | Loss: 0.00126793
Iteration 8/25 | Loss: 0.00126562
Iteration 9/25 | Loss: 0.00126505
Iteration 10/25 | Loss: 0.00126227
Iteration 11/25 | Loss: 0.00126146
Iteration 12/25 | Loss: 0.00126538
Iteration 13/25 | Loss: 0.00126741
Iteration 14/25 | Loss: 0.00126493
Iteration 15/25 | Loss: 0.00126288
Iteration 16/25 | Loss: 0.00126354
Iteration 17/25 | Loss: 0.00126002
Iteration 18/25 | Loss: 0.00125915
Iteration 19/25 | Loss: 0.00125837
Iteration 20/25 | Loss: 0.00125852
Iteration 21/25 | Loss: 0.00125815
Iteration 22/25 | Loss: 0.00125815
Iteration 23/25 | Loss: 0.00125815
Iteration 24/25 | Loss: 0.00125815
Iteration 25/25 | Loss: 0.00125815

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.50913525
Iteration 2/25 | Loss: 0.00165613
Iteration 3/25 | Loss: 0.00165614
Iteration 4/25 | Loss: 0.00165479
Iteration 5/25 | Loss: 0.00165479
Iteration 6/25 | Loss: 0.00165479
Iteration 7/25 | Loss: 0.00165479
Iteration 8/25 | Loss: 0.00165479
Iteration 9/25 | Loss: 0.00165479
Iteration 10/25 | Loss: 0.00165479
Iteration 11/25 | Loss: 0.00165479
Iteration 12/25 | Loss: 0.00165479
Iteration 13/25 | Loss: 0.00165479
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0016547917621210217, 0.0016547917621210217, 0.0016547917621210217, 0.0016547917621210217, 0.0016547917621210217]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0016547917621210217

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00165479
Iteration 2/1000 | Loss: 0.00003201
Iteration 3/1000 | Loss: 0.00002138
Iteration 4/1000 | Loss: 0.00001770
Iteration 5/1000 | Loss: 0.00001670
Iteration 6/1000 | Loss: 0.00001597
Iteration 7/1000 | Loss: 0.00001550
Iteration 8/1000 | Loss: 0.00001514
Iteration 9/1000 | Loss: 0.00001476
Iteration 10/1000 | Loss: 0.00001442
Iteration 11/1000 | Loss: 0.00001424
Iteration 12/1000 | Loss: 0.00001412
Iteration 13/1000 | Loss: 0.00001408
Iteration 14/1000 | Loss: 0.00001403
Iteration 15/1000 | Loss: 0.00001403
Iteration 16/1000 | Loss: 0.00001403
Iteration 17/1000 | Loss: 0.00001402
Iteration 18/1000 | Loss: 0.00001401
Iteration 19/1000 | Loss: 0.00001400
Iteration 20/1000 | Loss: 0.00001399
Iteration 21/1000 | Loss: 0.00001398
Iteration 22/1000 | Loss: 0.00001396
Iteration 23/1000 | Loss: 0.00001395
Iteration 24/1000 | Loss: 0.00001395
Iteration 25/1000 | Loss: 0.00001394
Iteration 26/1000 | Loss: 0.00001394
Iteration 27/1000 | Loss: 0.00001393
Iteration 28/1000 | Loss: 0.00001393
Iteration 29/1000 | Loss: 0.00001391
Iteration 30/1000 | Loss: 0.00001390
Iteration 31/1000 | Loss: 0.00001388
Iteration 32/1000 | Loss: 0.00001387
Iteration 33/1000 | Loss: 0.00001387
Iteration 34/1000 | Loss: 0.00001387
Iteration 35/1000 | Loss: 0.00001386
Iteration 36/1000 | Loss: 0.00001384
Iteration 37/1000 | Loss: 0.00001383
Iteration 38/1000 | Loss: 0.00001383
Iteration 39/1000 | Loss: 0.00001383
Iteration 40/1000 | Loss: 0.00001383
Iteration 41/1000 | Loss: 0.00001382
Iteration 42/1000 | Loss: 0.00001382
Iteration 43/1000 | Loss: 0.00001382
Iteration 44/1000 | Loss: 0.00001382
Iteration 45/1000 | Loss: 0.00001381
Iteration 46/1000 | Loss: 0.00001380
Iteration 47/1000 | Loss: 0.00001379
Iteration 48/1000 | Loss: 0.00001379
Iteration 49/1000 | Loss: 0.00001378
Iteration 50/1000 | Loss: 0.00001378
Iteration 51/1000 | Loss: 0.00001378
Iteration 52/1000 | Loss: 0.00001378
Iteration 53/1000 | Loss: 0.00001377
Iteration 54/1000 | Loss: 0.00001377
Iteration 55/1000 | Loss: 0.00001377
Iteration 56/1000 | Loss: 0.00001377
Iteration 57/1000 | Loss: 0.00001377
Iteration 58/1000 | Loss: 0.00001377
Iteration 59/1000 | Loss: 0.00001377
Iteration 60/1000 | Loss: 0.00001376
Iteration 61/1000 | Loss: 0.00001376
Iteration 62/1000 | Loss: 0.00001376
Iteration 63/1000 | Loss: 0.00001373
Iteration 64/1000 | Loss: 0.00001373
Iteration 65/1000 | Loss: 0.00001373
Iteration 66/1000 | Loss: 0.00001373
Iteration 67/1000 | Loss: 0.00001373
Iteration 68/1000 | Loss: 0.00001373
Iteration 69/1000 | Loss: 0.00001373
Iteration 70/1000 | Loss: 0.00001373
Iteration 71/1000 | Loss: 0.00001372
Iteration 72/1000 | Loss: 0.00001372
Iteration 73/1000 | Loss: 0.00001371
Iteration 74/1000 | Loss: 0.00001371
Iteration 75/1000 | Loss: 0.00001371
Iteration 76/1000 | Loss: 0.00001370
Iteration 77/1000 | Loss: 0.00001370
Iteration 78/1000 | Loss: 0.00001370
Iteration 79/1000 | Loss: 0.00001370
Iteration 80/1000 | Loss: 0.00001369
Iteration 81/1000 | Loss: 0.00001369
Iteration 82/1000 | Loss: 0.00001369
Iteration 83/1000 | Loss: 0.00001369
Iteration 84/1000 | Loss: 0.00001368
Iteration 85/1000 | Loss: 0.00001367
Iteration 86/1000 | Loss: 0.00001367
Iteration 87/1000 | Loss: 0.00001367
Iteration 88/1000 | Loss: 0.00001366
Iteration 89/1000 | Loss: 0.00001366
Iteration 90/1000 | Loss: 0.00001366
Iteration 91/1000 | Loss: 0.00001366
Iteration 92/1000 | Loss: 0.00001366
Iteration 93/1000 | Loss: 0.00001366
Iteration 94/1000 | Loss: 0.00001366
Iteration 95/1000 | Loss: 0.00001365
Iteration 96/1000 | Loss: 0.00001365
Iteration 97/1000 | Loss: 0.00001365
Iteration 98/1000 | Loss: 0.00001365
Iteration 99/1000 | Loss: 0.00001364
Iteration 100/1000 | Loss: 0.00001364
Iteration 101/1000 | Loss: 0.00001364
Iteration 102/1000 | Loss: 0.00001364
Iteration 103/1000 | Loss: 0.00001364
Iteration 104/1000 | Loss: 0.00001363
Iteration 105/1000 | Loss: 0.00001363
Iteration 106/1000 | Loss: 0.00001363
Iteration 107/1000 | Loss: 0.00001363
Iteration 108/1000 | Loss: 0.00001363
Iteration 109/1000 | Loss: 0.00001363
Iteration 110/1000 | Loss: 0.00001362
Iteration 111/1000 | Loss: 0.00001362
Iteration 112/1000 | Loss: 0.00001362
Iteration 113/1000 | Loss: 0.00001362
Iteration 114/1000 | Loss: 0.00001362
Iteration 115/1000 | Loss: 0.00001362
Iteration 116/1000 | Loss: 0.00001362
Iteration 117/1000 | Loss: 0.00001361
Iteration 118/1000 | Loss: 0.00001361
Iteration 119/1000 | Loss: 0.00001361
Iteration 120/1000 | Loss: 0.00001361
Iteration 121/1000 | Loss: 0.00001361
Iteration 122/1000 | Loss: 0.00001361
Iteration 123/1000 | Loss: 0.00001361
Iteration 124/1000 | Loss: 0.00001361
Iteration 125/1000 | Loss: 0.00001361
Iteration 126/1000 | Loss: 0.00001360
Iteration 127/1000 | Loss: 0.00001360
Iteration 128/1000 | Loss: 0.00001360
Iteration 129/1000 | Loss: 0.00001360
Iteration 130/1000 | Loss: 0.00001360
Iteration 131/1000 | Loss: 0.00001360
Iteration 132/1000 | Loss: 0.00001360
Iteration 133/1000 | Loss: 0.00001360
Iteration 134/1000 | Loss: 0.00001360
Iteration 135/1000 | Loss: 0.00001360
Iteration 136/1000 | Loss: 0.00001359
Iteration 137/1000 | Loss: 0.00001359
Iteration 138/1000 | Loss: 0.00001359
Iteration 139/1000 | Loss: 0.00001359
Iteration 140/1000 | Loss: 0.00001359
Iteration 141/1000 | Loss: 0.00001359
Iteration 142/1000 | Loss: 0.00001359
Iteration 143/1000 | Loss: 0.00001359
Iteration 144/1000 | Loss: 0.00001359
Iteration 145/1000 | Loss: 0.00001359
Iteration 146/1000 | Loss: 0.00001359
Iteration 147/1000 | Loss: 0.00001359
Iteration 148/1000 | Loss: 0.00001359
Iteration 149/1000 | Loss: 0.00001359
Iteration 150/1000 | Loss: 0.00001359
Iteration 151/1000 | Loss: 0.00001359
Iteration 152/1000 | Loss: 0.00001359
Iteration 153/1000 | Loss: 0.00001359
Iteration 154/1000 | Loss: 0.00001359
Iteration 155/1000 | Loss: 0.00001359
Iteration 156/1000 | Loss: 0.00001359
Iteration 157/1000 | Loss: 0.00001359
Iteration 158/1000 | Loss: 0.00001359
Iteration 159/1000 | Loss: 0.00001359
Iteration 160/1000 | Loss: 0.00001359
Iteration 161/1000 | Loss: 0.00001359
Iteration 162/1000 | Loss: 0.00001359
Iteration 163/1000 | Loss: 0.00001359
Iteration 164/1000 | Loss: 0.00001359
Iteration 165/1000 | Loss: 0.00001359
Iteration 166/1000 | Loss: 0.00001359
Iteration 167/1000 | Loss: 0.00001359
Iteration 168/1000 | Loss: 0.00001359
Iteration 169/1000 | Loss: 0.00001359
Iteration 170/1000 | Loss: 0.00001359
Iteration 171/1000 | Loss: 0.00001359
Iteration 172/1000 | Loss: 0.00001359
Iteration 173/1000 | Loss: 0.00001359
Iteration 174/1000 | Loss: 0.00001359
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 174. Stopping optimization.
Last 5 losses: [1.3588022738986183e-05, 1.3588022738986183e-05, 1.3588022738986183e-05, 1.3588022738986183e-05, 1.3588022738986183e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3588022738986183e-05

Optimization complete. Final v2v error: 3.11784029006958 mm

Highest mean error: 3.879607915878296 mm for frame 67

Lowest mean error: 2.823591709136963 mm for frame 238

Saving results

Total time: 74.44416379928589
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_025/1083/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_025/1083.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_025/1083
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00814983
Iteration 2/25 | Loss: 0.00151350
Iteration 3/25 | Loss: 0.00128026
Iteration 4/25 | Loss: 0.00125842
Iteration 5/25 | Loss: 0.00125594
Iteration 6/25 | Loss: 0.00125584
Iteration 7/25 | Loss: 0.00125584
Iteration 8/25 | Loss: 0.00125584
Iteration 9/25 | Loss: 0.00125584
Iteration 10/25 | Loss: 0.00125584
Iteration 11/25 | Loss: 0.00125584
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012558366870507598, 0.0012558366870507598, 0.0012558366870507598, 0.0012558366870507598, 0.0012558366870507598]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012558366870507598

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.91405261
Iteration 2/25 | Loss: 0.00083600
Iteration 3/25 | Loss: 0.00083599
Iteration 4/25 | Loss: 0.00083599
Iteration 5/25 | Loss: 0.00083599
Iteration 6/25 | Loss: 0.00083599
Iteration 7/25 | Loss: 0.00083599
Iteration 8/25 | Loss: 0.00083599
Iteration 9/25 | Loss: 0.00083599
Iteration 10/25 | Loss: 0.00083599
Iteration 11/25 | Loss: 0.00083599
Iteration 12/25 | Loss: 0.00083599
Iteration 13/25 | Loss: 0.00083599
Iteration 14/25 | Loss: 0.00083599
Iteration 15/25 | Loss: 0.00083599
Iteration 16/25 | Loss: 0.00083599
Iteration 17/25 | Loss: 0.00083599
Iteration 18/25 | Loss: 0.00083599
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0008359909406863153, 0.0008359909406863153, 0.0008359909406863153, 0.0008359909406863153, 0.0008359909406863153]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008359909406863153

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00083599
Iteration 2/1000 | Loss: 0.00003205
Iteration 3/1000 | Loss: 0.00002345
Iteration 4/1000 | Loss: 0.00002080
Iteration 5/1000 | Loss: 0.00001991
Iteration 6/1000 | Loss: 0.00001935
Iteration 7/1000 | Loss: 0.00001893
Iteration 8/1000 | Loss: 0.00001853
Iteration 9/1000 | Loss: 0.00001818
Iteration 10/1000 | Loss: 0.00001786
Iteration 11/1000 | Loss: 0.00001764
Iteration 12/1000 | Loss: 0.00001752
Iteration 13/1000 | Loss: 0.00001736
Iteration 14/1000 | Loss: 0.00001732
Iteration 15/1000 | Loss: 0.00001725
Iteration 16/1000 | Loss: 0.00001725
Iteration 17/1000 | Loss: 0.00001724
Iteration 18/1000 | Loss: 0.00001714
Iteration 19/1000 | Loss: 0.00001713
Iteration 20/1000 | Loss: 0.00001713
Iteration 21/1000 | Loss: 0.00001713
Iteration 22/1000 | Loss: 0.00001713
Iteration 23/1000 | Loss: 0.00001713
Iteration 24/1000 | Loss: 0.00001713
Iteration 25/1000 | Loss: 0.00001713
Iteration 26/1000 | Loss: 0.00001713
Iteration 27/1000 | Loss: 0.00001712
Iteration 28/1000 | Loss: 0.00001712
Iteration 29/1000 | Loss: 0.00001712
Iteration 30/1000 | Loss: 0.00001712
Iteration 31/1000 | Loss: 0.00001712
Iteration 32/1000 | Loss: 0.00001712
Iteration 33/1000 | Loss: 0.00001712
Iteration 34/1000 | Loss: 0.00001712
Iteration 35/1000 | Loss: 0.00001712
Iteration 36/1000 | Loss: 0.00001712
Iteration 37/1000 | Loss: 0.00001711
Iteration 38/1000 | Loss: 0.00001711
Iteration 39/1000 | Loss: 0.00001710
Iteration 40/1000 | Loss: 0.00001709
Iteration 41/1000 | Loss: 0.00001709
Iteration 42/1000 | Loss: 0.00001703
Iteration 43/1000 | Loss: 0.00001703
Iteration 44/1000 | Loss: 0.00001702
Iteration 45/1000 | Loss: 0.00001702
Iteration 46/1000 | Loss: 0.00001701
Iteration 47/1000 | Loss: 0.00001701
Iteration 48/1000 | Loss: 0.00001700
Iteration 49/1000 | Loss: 0.00001700
Iteration 50/1000 | Loss: 0.00001700
Iteration 51/1000 | Loss: 0.00001700
Iteration 52/1000 | Loss: 0.00001700
Iteration 53/1000 | Loss: 0.00001700
Iteration 54/1000 | Loss: 0.00001700
Iteration 55/1000 | Loss: 0.00001700
Iteration 56/1000 | Loss: 0.00001700
Iteration 57/1000 | Loss: 0.00001699
Iteration 58/1000 | Loss: 0.00001699
Iteration 59/1000 | Loss: 0.00001699
Iteration 60/1000 | Loss: 0.00001699
Iteration 61/1000 | Loss: 0.00001699
Iteration 62/1000 | Loss: 0.00001699
Iteration 63/1000 | Loss: 0.00001698
Iteration 64/1000 | Loss: 0.00001698
Iteration 65/1000 | Loss: 0.00001698
Iteration 66/1000 | Loss: 0.00001697
Iteration 67/1000 | Loss: 0.00001697
Iteration 68/1000 | Loss: 0.00001696
Iteration 69/1000 | Loss: 0.00001691
Iteration 70/1000 | Loss: 0.00001691
Iteration 71/1000 | Loss: 0.00001690
Iteration 72/1000 | Loss: 0.00001689
Iteration 73/1000 | Loss: 0.00001689
Iteration 74/1000 | Loss: 0.00001689
Iteration 75/1000 | Loss: 0.00001689
Iteration 76/1000 | Loss: 0.00001689
Iteration 77/1000 | Loss: 0.00001688
Iteration 78/1000 | Loss: 0.00001688
Iteration 79/1000 | Loss: 0.00001687
Iteration 80/1000 | Loss: 0.00001687
Iteration 81/1000 | Loss: 0.00001687
Iteration 82/1000 | Loss: 0.00001686
Iteration 83/1000 | Loss: 0.00001686
Iteration 84/1000 | Loss: 0.00001686
Iteration 85/1000 | Loss: 0.00001686
Iteration 86/1000 | Loss: 0.00001685
Iteration 87/1000 | Loss: 0.00001685
Iteration 88/1000 | Loss: 0.00001685
Iteration 89/1000 | Loss: 0.00001685
Iteration 90/1000 | Loss: 0.00001685
Iteration 91/1000 | Loss: 0.00001684
Iteration 92/1000 | Loss: 0.00001684
Iteration 93/1000 | Loss: 0.00001684
Iteration 94/1000 | Loss: 0.00001684
Iteration 95/1000 | Loss: 0.00001684
Iteration 96/1000 | Loss: 0.00001684
Iteration 97/1000 | Loss: 0.00001684
Iteration 98/1000 | Loss: 0.00001684
Iteration 99/1000 | Loss: 0.00001684
Iteration 100/1000 | Loss: 0.00001684
Iteration 101/1000 | Loss: 0.00001683
Iteration 102/1000 | Loss: 0.00001683
Iteration 103/1000 | Loss: 0.00001683
Iteration 104/1000 | Loss: 0.00001683
Iteration 105/1000 | Loss: 0.00001683
Iteration 106/1000 | Loss: 0.00001683
Iteration 107/1000 | Loss: 0.00001683
Iteration 108/1000 | Loss: 0.00001683
Iteration 109/1000 | Loss: 0.00001683
Iteration 110/1000 | Loss: 0.00001683
Iteration 111/1000 | Loss: 0.00001683
Iteration 112/1000 | Loss: 0.00001683
Iteration 113/1000 | Loss: 0.00001682
Iteration 114/1000 | Loss: 0.00001682
Iteration 115/1000 | Loss: 0.00001682
Iteration 116/1000 | Loss: 0.00001682
Iteration 117/1000 | Loss: 0.00001682
Iteration 118/1000 | Loss: 0.00001682
Iteration 119/1000 | Loss: 0.00001682
Iteration 120/1000 | Loss: 0.00001682
Iteration 121/1000 | Loss: 0.00001682
Iteration 122/1000 | Loss: 0.00001682
Iteration 123/1000 | Loss: 0.00001682
Iteration 124/1000 | Loss: 0.00001682
Iteration 125/1000 | Loss: 0.00001682
Iteration 126/1000 | Loss: 0.00001682
Iteration 127/1000 | Loss: 0.00001682
Iteration 128/1000 | Loss: 0.00001682
Iteration 129/1000 | Loss: 0.00001682
Iteration 130/1000 | Loss: 0.00001682
Iteration 131/1000 | Loss: 0.00001682
Iteration 132/1000 | Loss: 0.00001682
Iteration 133/1000 | Loss: 0.00001682
Iteration 134/1000 | Loss: 0.00001682
Iteration 135/1000 | Loss: 0.00001682
Iteration 136/1000 | Loss: 0.00001682
Iteration 137/1000 | Loss: 0.00001682
Iteration 138/1000 | Loss: 0.00001682
Iteration 139/1000 | Loss: 0.00001682
Iteration 140/1000 | Loss: 0.00001682
Iteration 141/1000 | Loss: 0.00001682
Iteration 142/1000 | Loss: 0.00001682
Iteration 143/1000 | Loss: 0.00001682
Iteration 144/1000 | Loss: 0.00001682
Iteration 145/1000 | Loss: 0.00001682
Iteration 146/1000 | Loss: 0.00001682
Iteration 147/1000 | Loss: 0.00001682
Iteration 148/1000 | Loss: 0.00001682
Iteration 149/1000 | Loss: 0.00001682
Iteration 150/1000 | Loss: 0.00001682
Iteration 151/1000 | Loss: 0.00001682
Iteration 152/1000 | Loss: 0.00001682
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 152. Stopping optimization.
Last 5 losses: [1.6817204596009105e-05, 1.6817204596009105e-05, 1.6817204596009105e-05, 1.6817204596009105e-05, 1.6817204596009105e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6817204596009105e-05

Optimization complete. Final v2v error: 3.4568281173706055 mm

Highest mean error: 3.8094229698181152 mm for frame 30

Lowest mean error: 3.271932363510132 mm for frame 141

Saving results

Total time: 37.70047855377197
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_025/1078/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_025/1078.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_025/1078
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00752578
Iteration 2/25 | Loss: 0.00160493
Iteration 3/25 | Loss: 0.00132676
Iteration 4/25 | Loss: 0.00129698
Iteration 5/25 | Loss: 0.00129190
Iteration 6/25 | Loss: 0.00129086
Iteration 7/25 | Loss: 0.00129086
Iteration 8/25 | Loss: 0.00129086
Iteration 9/25 | Loss: 0.00129086
Iteration 10/25 | Loss: 0.00129086
Iteration 11/25 | Loss: 0.00129086
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012908588396385312, 0.0012908588396385312, 0.0012908588396385312, 0.0012908588396385312, 0.0012908588396385312]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012908588396385312

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.00636530
Iteration 2/25 | Loss: 0.00144202
Iteration 3/25 | Loss: 0.00144193
Iteration 4/25 | Loss: 0.00144193
Iteration 5/25 | Loss: 0.00144193
Iteration 6/25 | Loss: 0.00144193
Iteration 7/25 | Loss: 0.00144193
Iteration 8/25 | Loss: 0.00144193
Iteration 9/25 | Loss: 0.00144193
Iteration 10/25 | Loss: 0.00144193
Iteration 11/25 | Loss: 0.00144193
Iteration 12/25 | Loss: 0.00144193
Iteration 13/25 | Loss: 0.00144193
Iteration 14/25 | Loss: 0.00144193
Iteration 15/25 | Loss: 0.00144193
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0014419254148378968, 0.0014419254148378968, 0.0014419254148378968, 0.0014419254148378968, 0.0014419254148378968]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014419254148378968

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00144193
Iteration 2/1000 | Loss: 0.00003643
Iteration 3/1000 | Loss: 0.00002408
Iteration 4/1000 | Loss: 0.00002148
Iteration 5/1000 | Loss: 0.00002026
Iteration 6/1000 | Loss: 0.00001931
Iteration 7/1000 | Loss: 0.00001884
Iteration 8/1000 | Loss: 0.00001838
Iteration 9/1000 | Loss: 0.00001809
Iteration 10/1000 | Loss: 0.00001780
Iteration 11/1000 | Loss: 0.00001754
Iteration 12/1000 | Loss: 0.00001747
Iteration 13/1000 | Loss: 0.00001733
Iteration 14/1000 | Loss: 0.00001715
Iteration 15/1000 | Loss: 0.00001706
Iteration 16/1000 | Loss: 0.00001703
Iteration 17/1000 | Loss: 0.00001701
Iteration 18/1000 | Loss: 0.00001699
Iteration 19/1000 | Loss: 0.00001698
Iteration 20/1000 | Loss: 0.00001698
Iteration 21/1000 | Loss: 0.00001692
Iteration 22/1000 | Loss: 0.00001691
Iteration 23/1000 | Loss: 0.00001690
Iteration 24/1000 | Loss: 0.00001690
Iteration 25/1000 | Loss: 0.00001688
Iteration 26/1000 | Loss: 0.00001687
Iteration 27/1000 | Loss: 0.00001687
Iteration 28/1000 | Loss: 0.00001683
Iteration 29/1000 | Loss: 0.00001680
Iteration 30/1000 | Loss: 0.00001679
Iteration 31/1000 | Loss: 0.00001679
Iteration 32/1000 | Loss: 0.00001678
Iteration 33/1000 | Loss: 0.00001678
Iteration 34/1000 | Loss: 0.00001677
Iteration 35/1000 | Loss: 0.00001676
Iteration 36/1000 | Loss: 0.00001673
Iteration 37/1000 | Loss: 0.00001672
Iteration 38/1000 | Loss: 0.00001672
Iteration 39/1000 | Loss: 0.00001672
Iteration 40/1000 | Loss: 0.00001671
Iteration 41/1000 | Loss: 0.00001670
Iteration 42/1000 | Loss: 0.00001670
Iteration 43/1000 | Loss: 0.00001666
Iteration 44/1000 | Loss: 0.00001665
Iteration 45/1000 | Loss: 0.00001665
Iteration 46/1000 | Loss: 0.00001664
Iteration 47/1000 | Loss: 0.00001663
Iteration 48/1000 | Loss: 0.00001663
Iteration 49/1000 | Loss: 0.00001662
Iteration 50/1000 | Loss: 0.00001662
Iteration 51/1000 | Loss: 0.00001661
Iteration 52/1000 | Loss: 0.00001661
Iteration 53/1000 | Loss: 0.00001660
Iteration 54/1000 | Loss: 0.00001660
Iteration 55/1000 | Loss: 0.00001660
Iteration 56/1000 | Loss: 0.00001659
Iteration 57/1000 | Loss: 0.00001659
Iteration 58/1000 | Loss: 0.00001658
Iteration 59/1000 | Loss: 0.00001658
Iteration 60/1000 | Loss: 0.00001658
Iteration 61/1000 | Loss: 0.00001658
Iteration 62/1000 | Loss: 0.00001657
Iteration 63/1000 | Loss: 0.00001657
Iteration 64/1000 | Loss: 0.00001656
Iteration 65/1000 | Loss: 0.00001656
Iteration 66/1000 | Loss: 0.00001655
Iteration 67/1000 | Loss: 0.00001655
Iteration 68/1000 | Loss: 0.00001655
Iteration 69/1000 | Loss: 0.00001655
Iteration 70/1000 | Loss: 0.00001654
Iteration 71/1000 | Loss: 0.00001654
Iteration 72/1000 | Loss: 0.00001654
Iteration 73/1000 | Loss: 0.00001653
Iteration 74/1000 | Loss: 0.00001653
Iteration 75/1000 | Loss: 0.00001653
Iteration 76/1000 | Loss: 0.00001652
Iteration 77/1000 | Loss: 0.00001652
Iteration 78/1000 | Loss: 0.00001652
Iteration 79/1000 | Loss: 0.00001652
Iteration 80/1000 | Loss: 0.00001652
Iteration 81/1000 | Loss: 0.00001651
Iteration 82/1000 | Loss: 0.00001651
Iteration 83/1000 | Loss: 0.00001651
Iteration 84/1000 | Loss: 0.00001650
Iteration 85/1000 | Loss: 0.00001650
Iteration 86/1000 | Loss: 0.00001650
Iteration 87/1000 | Loss: 0.00001650
Iteration 88/1000 | Loss: 0.00001650
Iteration 89/1000 | Loss: 0.00001650
Iteration 90/1000 | Loss: 0.00001650
Iteration 91/1000 | Loss: 0.00001650
Iteration 92/1000 | Loss: 0.00001649
Iteration 93/1000 | Loss: 0.00001649
Iteration 94/1000 | Loss: 0.00001649
Iteration 95/1000 | Loss: 0.00001649
Iteration 96/1000 | Loss: 0.00001649
Iteration 97/1000 | Loss: 0.00001649
Iteration 98/1000 | Loss: 0.00001649
Iteration 99/1000 | Loss: 0.00001649
Iteration 100/1000 | Loss: 0.00001648
Iteration 101/1000 | Loss: 0.00001648
Iteration 102/1000 | Loss: 0.00001648
Iteration 103/1000 | Loss: 0.00001647
Iteration 104/1000 | Loss: 0.00001647
Iteration 105/1000 | Loss: 0.00001647
Iteration 106/1000 | Loss: 0.00001646
Iteration 107/1000 | Loss: 0.00001646
Iteration 108/1000 | Loss: 0.00001645
Iteration 109/1000 | Loss: 0.00001645
Iteration 110/1000 | Loss: 0.00001644
Iteration 111/1000 | Loss: 0.00001644
Iteration 112/1000 | Loss: 0.00001644
Iteration 113/1000 | Loss: 0.00001644
Iteration 114/1000 | Loss: 0.00001643
Iteration 115/1000 | Loss: 0.00001643
Iteration 116/1000 | Loss: 0.00001643
Iteration 117/1000 | Loss: 0.00001642
Iteration 118/1000 | Loss: 0.00001642
Iteration 119/1000 | Loss: 0.00001642
Iteration 120/1000 | Loss: 0.00001642
Iteration 121/1000 | Loss: 0.00001642
Iteration 122/1000 | Loss: 0.00001642
Iteration 123/1000 | Loss: 0.00001642
Iteration 124/1000 | Loss: 0.00001641
Iteration 125/1000 | Loss: 0.00001641
Iteration 126/1000 | Loss: 0.00001641
Iteration 127/1000 | Loss: 0.00001641
Iteration 128/1000 | Loss: 0.00001640
Iteration 129/1000 | Loss: 0.00001640
Iteration 130/1000 | Loss: 0.00001640
Iteration 131/1000 | Loss: 0.00001640
Iteration 132/1000 | Loss: 0.00001640
Iteration 133/1000 | Loss: 0.00001640
Iteration 134/1000 | Loss: 0.00001639
Iteration 135/1000 | Loss: 0.00001639
Iteration 136/1000 | Loss: 0.00001639
Iteration 137/1000 | Loss: 0.00001638
Iteration 138/1000 | Loss: 0.00001638
Iteration 139/1000 | Loss: 0.00001638
Iteration 140/1000 | Loss: 0.00001638
Iteration 141/1000 | Loss: 0.00001637
Iteration 142/1000 | Loss: 0.00001637
Iteration 143/1000 | Loss: 0.00001637
Iteration 144/1000 | Loss: 0.00001636
Iteration 145/1000 | Loss: 0.00001636
Iteration 146/1000 | Loss: 0.00001636
Iteration 147/1000 | Loss: 0.00001636
Iteration 148/1000 | Loss: 0.00001636
Iteration 149/1000 | Loss: 0.00001636
Iteration 150/1000 | Loss: 0.00001636
Iteration 151/1000 | Loss: 0.00001635
Iteration 152/1000 | Loss: 0.00001635
Iteration 153/1000 | Loss: 0.00001635
Iteration 154/1000 | Loss: 0.00001634
Iteration 155/1000 | Loss: 0.00001634
Iteration 156/1000 | Loss: 0.00001634
Iteration 157/1000 | Loss: 0.00001633
Iteration 158/1000 | Loss: 0.00001633
Iteration 159/1000 | Loss: 0.00001632
Iteration 160/1000 | Loss: 0.00001632
Iteration 161/1000 | Loss: 0.00001632
Iteration 162/1000 | Loss: 0.00001632
Iteration 163/1000 | Loss: 0.00001632
Iteration 164/1000 | Loss: 0.00001632
Iteration 165/1000 | Loss: 0.00001632
Iteration 166/1000 | Loss: 0.00001631
Iteration 167/1000 | Loss: 0.00001631
Iteration 168/1000 | Loss: 0.00001631
Iteration 169/1000 | Loss: 0.00001631
Iteration 170/1000 | Loss: 0.00001630
Iteration 171/1000 | Loss: 0.00001630
Iteration 172/1000 | Loss: 0.00001630
Iteration 173/1000 | Loss: 0.00001629
Iteration 174/1000 | Loss: 0.00001629
Iteration 175/1000 | Loss: 0.00001629
Iteration 176/1000 | Loss: 0.00001629
Iteration 177/1000 | Loss: 0.00001629
Iteration 178/1000 | Loss: 0.00001629
Iteration 179/1000 | Loss: 0.00001629
Iteration 180/1000 | Loss: 0.00001628
Iteration 181/1000 | Loss: 0.00001628
Iteration 182/1000 | Loss: 0.00001628
Iteration 183/1000 | Loss: 0.00001628
Iteration 184/1000 | Loss: 0.00001628
Iteration 185/1000 | Loss: 0.00001628
Iteration 186/1000 | Loss: 0.00001628
Iteration 187/1000 | Loss: 0.00001628
Iteration 188/1000 | Loss: 0.00001628
Iteration 189/1000 | Loss: 0.00001628
Iteration 190/1000 | Loss: 0.00001628
Iteration 191/1000 | Loss: 0.00001628
Iteration 192/1000 | Loss: 0.00001628
Iteration 193/1000 | Loss: 0.00001628
Iteration 194/1000 | Loss: 0.00001627
Iteration 195/1000 | Loss: 0.00001627
Iteration 196/1000 | Loss: 0.00001627
Iteration 197/1000 | Loss: 0.00001627
Iteration 198/1000 | Loss: 0.00001627
Iteration 199/1000 | Loss: 0.00001627
Iteration 200/1000 | Loss: 0.00001627
Iteration 201/1000 | Loss: 0.00001627
Iteration 202/1000 | Loss: 0.00001627
Iteration 203/1000 | Loss: 0.00001627
Iteration 204/1000 | Loss: 0.00001627
Iteration 205/1000 | Loss: 0.00001627
Iteration 206/1000 | Loss: 0.00001627
Iteration 207/1000 | Loss: 0.00001627
Iteration 208/1000 | Loss: 0.00001627
Iteration 209/1000 | Loss: 0.00001626
Iteration 210/1000 | Loss: 0.00001626
Iteration 211/1000 | Loss: 0.00001626
Iteration 212/1000 | Loss: 0.00001626
Iteration 213/1000 | Loss: 0.00001626
Iteration 214/1000 | Loss: 0.00001626
Iteration 215/1000 | Loss: 0.00001626
Iteration 216/1000 | Loss: 0.00001626
Iteration 217/1000 | Loss: 0.00001626
Iteration 218/1000 | Loss: 0.00001626
Iteration 219/1000 | Loss: 0.00001626
Iteration 220/1000 | Loss: 0.00001626
Iteration 221/1000 | Loss: 0.00001626
Iteration 222/1000 | Loss: 0.00001626
Iteration 223/1000 | Loss: 0.00001626
Iteration 224/1000 | Loss: 0.00001626
Iteration 225/1000 | Loss: 0.00001626
Iteration 226/1000 | Loss: 0.00001625
Iteration 227/1000 | Loss: 0.00001625
Iteration 228/1000 | Loss: 0.00001625
Iteration 229/1000 | Loss: 0.00001625
Iteration 230/1000 | Loss: 0.00001625
Iteration 231/1000 | Loss: 0.00001625
Iteration 232/1000 | Loss: 0.00001625
Iteration 233/1000 | Loss: 0.00001625
Iteration 234/1000 | Loss: 0.00001625
Iteration 235/1000 | Loss: 0.00001625
Iteration 236/1000 | Loss: 0.00001625
Iteration 237/1000 | Loss: 0.00001624
Iteration 238/1000 | Loss: 0.00001624
Iteration 239/1000 | Loss: 0.00001624
Iteration 240/1000 | Loss: 0.00001624
Iteration 241/1000 | Loss: 0.00001624
Iteration 242/1000 | Loss: 0.00001624
Iteration 243/1000 | Loss: 0.00001624
Iteration 244/1000 | Loss: 0.00001624
Iteration 245/1000 | Loss: 0.00001624
Iteration 246/1000 | Loss: 0.00001624
Iteration 247/1000 | Loss: 0.00001624
Iteration 248/1000 | Loss: 0.00001624
Iteration 249/1000 | Loss: 0.00001624
Iteration 250/1000 | Loss: 0.00001624
Iteration 251/1000 | Loss: 0.00001624
Iteration 252/1000 | Loss: 0.00001624
Iteration 253/1000 | Loss: 0.00001623
Iteration 254/1000 | Loss: 0.00001623
Iteration 255/1000 | Loss: 0.00001623
Iteration 256/1000 | Loss: 0.00001623
Iteration 257/1000 | Loss: 0.00001623
Iteration 258/1000 | Loss: 0.00001623
Iteration 259/1000 | Loss: 0.00001623
Iteration 260/1000 | Loss: 0.00001623
Iteration 261/1000 | Loss: 0.00001623
Iteration 262/1000 | Loss: 0.00001623
Iteration 263/1000 | Loss: 0.00001623
Iteration 264/1000 | Loss: 0.00001623
Iteration 265/1000 | Loss: 0.00001623
Iteration 266/1000 | Loss: 0.00001623
Iteration 267/1000 | Loss: 0.00001623
Iteration 268/1000 | Loss: 0.00001623
Iteration 269/1000 | Loss: 0.00001623
Iteration 270/1000 | Loss: 0.00001623
Iteration 271/1000 | Loss: 0.00001623
Iteration 272/1000 | Loss: 0.00001623
Iteration 273/1000 | Loss: 0.00001622
Iteration 274/1000 | Loss: 0.00001622
Iteration 275/1000 | Loss: 0.00001622
Iteration 276/1000 | Loss: 0.00001622
Iteration 277/1000 | Loss: 0.00001622
Iteration 278/1000 | Loss: 0.00001622
Iteration 279/1000 | Loss: 0.00001622
Iteration 280/1000 | Loss: 0.00001622
Iteration 281/1000 | Loss: 0.00001622
Iteration 282/1000 | Loss: 0.00001622
Iteration 283/1000 | Loss: 0.00001622
Iteration 284/1000 | Loss: 0.00001622
Iteration 285/1000 | Loss: 0.00001622
Iteration 286/1000 | Loss: 0.00001622
Iteration 287/1000 | Loss: 0.00001622
Iteration 288/1000 | Loss: 0.00001622
Iteration 289/1000 | Loss: 0.00001622
Iteration 290/1000 | Loss: 0.00001622
Iteration 291/1000 | Loss: 0.00001622
Iteration 292/1000 | Loss: 0.00001622
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 292. Stopping optimization.
Last 5 losses: [1.6215830328292213e-05, 1.6215830328292213e-05, 1.6215830328292213e-05, 1.6215830328292213e-05, 1.6215830328292213e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6215830328292213e-05

Optimization complete. Final v2v error: 3.320073366165161 mm

Highest mean error: 4.815064907073975 mm for frame 161

Lowest mean error: 2.6424365043640137 mm for frame 189

Saving results

Total time: 53.580010652542114
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_025/1028/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_025/1028.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_025/1028
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00413805
Iteration 2/25 | Loss: 0.00131109
Iteration 3/25 | Loss: 0.00122346
Iteration 4/25 | Loss: 0.00121368
Iteration 5/25 | Loss: 0.00121082
Iteration 6/25 | Loss: 0.00121008
Iteration 7/25 | Loss: 0.00121008
Iteration 8/25 | Loss: 0.00121008
Iteration 9/25 | Loss: 0.00121008
Iteration 10/25 | Loss: 0.00121008
Iteration 11/25 | Loss: 0.00121008
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012100808089599013, 0.0012100808089599013, 0.0012100808089599013, 0.0012100808089599013, 0.0012100808089599013]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012100808089599013

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.31278801
Iteration 2/25 | Loss: 0.00144885
Iteration 3/25 | Loss: 0.00144885
Iteration 4/25 | Loss: 0.00144885
Iteration 5/25 | Loss: 0.00144885
Iteration 6/25 | Loss: 0.00144884
Iteration 7/25 | Loss: 0.00144884
Iteration 8/25 | Loss: 0.00144884
Iteration 9/25 | Loss: 0.00144884
Iteration 10/25 | Loss: 0.00144884
Iteration 11/25 | Loss: 0.00144884
Iteration 12/25 | Loss: 0.00144884
Iteration 13/25 | Loss: 0.00144884
Iteration 14/25 | Loss: 0.00144884
Iteration 15/25 | Loss: 0.00144884
Iteration 16/25 | Loss: 0.00144884
Iteration 17/25 | Loss: 0.00144884
Iteration 18/25 | Loss: 0.00144884
Iteration 19/25 | Loss: 0.00144884
Iteration 20/25 | Loss: 0.00144884
Iteration 21/25 | Loss: 0.00144884
Iteration 22/25 | Loss: 0.00144884
Iteration 23/25 | Loss: 0.00144884
Iteration 24/25 | Loss: 0.00144884
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0014488422311842442, 0.0014488422311842442, 0.0014488422311842442, 0.0014488422311842442, 0.0014488422311842442]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014488422311842442

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00144884
Iteration 2/1000 | Loss: 0.00002510
Iteration 3/1000 | Loss: 0.00001773
Iteration 4/1000 | Loss: 0.00001456
Iteration 5/1000 | Loss: 0.00001367
Iteration 6/1000 | Loss: 0.00001311
Iteration 7/1000 | Loss: 0.00001268
Iteration 8/1000 | Loss: 0.00001218
Iteration 9/1000 | Loss: 0.00001191
Iteration 10/1000 | Loss: 0.00001165
Iteration 11/1000 | Loss: 0.00001151
Iteration 12/1000 | Loss: 0.00001146
Iteration 13/1000 | Loss: 0.00001135
Iteration 14/1000 | Loss: 0.00001125
Iteration 15/1000 | Loss: 0.00001116
Iteration 16/1000 | Loss: 0.00001109
Iteration 17/1000 | Loss: 0.00001106
Iteration 18/1000 | Loss: 0.00001105
Iteration 19/1000 | Loss: 0.00001104
Iteration 20/1000 | Loss: 0.00001094
Iteration 21/1000 | Loss: 0.00001094
Iteration 22/1000 | Loss: 0.00001091
Iteration 23/1000 | Loss: 0.00001091
Iteration 24/1000 | Loss: 0.00001086
Iteration 25/1000 | Loss: 0.00001081
Iteration 26/1000 | Loss: 0.00001079
Iteration 27/1000 | Loss: 0.00001078
Iteration 28/1000 | Loss: 0.00001078
Iteration 29/1000 | Loss: 0.00001077
Iteration 30/1000 | Loss: 0.00001076
Iteration 31/1000 | Loss: 0.00001076
Iteration 32/1000 | Loss: 0.00001075
Iteration 33/1000 | Loss: 0.00001075
Iteration 34/1000 | Loss: 0.00001075
Iteration 35/1000 | Loss: 0.00001074
Iteration 36/1000 | Loss: 0.00001071
Iteration 37/1000 | Loss: 0.00001070
Iteration 38/1000 | Loss: 0.00001070
Iteration 39/1000 | Loss: 0.00001069
Iteration 40/1000 | Loss: 0.00001069
Iteration 41/1000 | Loss: 0.00001069
Iteration 42/1000 | Loss: 0.00001068
Iteration 43/1000 | Loss: 0.00001068
Iteration 44/1000 | Loss: 0.00001065
Iteration 45/1000 | Loss: 0.00001065
Iteration 46/1000 | Loss: 0.00001064
Iteration 47/1000 | Loss: 0.00001063
Iteration 48/1000 | Loss: 0.00001063
Iteration 49/1000 | Loss: 0.00001063
Iteration 50/1000 | Loss: 0.00001061
Iteration 51/1000 | Loss: 0.00001061
Iteration 52/1000 | Loss: 0.00001060
Iteration 53/1000 | Loss: 0.00001060
Iteration 54/1000 | Loss: 0.00001060
Iteration 55/1000 | Loss: 0.00001060
Iteration 56/1000 | Loss: 0.00001060
Iteration 57/1000 | Loss: 0.00001060
Iteration 58/1000 | Loss: 0.00001060
Iteration 59/1000 | Loss: 0.00001060
Iteration 60/1000 | Loss: 0.00001059
Iteration 61/1000 | Loss: 0.00001059
Iteration 62/1000 | Loss: 0.00001058
Iteration 63/1000 | Loss: 0.00001058
Iteration 64/1000 | Loss: 0.00001058
Iteration 65/1000 | Loss: 0.00001057
Iteration 66/1000 | Loss: 0.00001057
Iteration 67/1000 | Loss: 0.00001056
Iteration 68/1000 | Loss: 0.00001056
Iteration 69/1000 | Loss: 0.00001056
Iteration 70/1000 | Loss: 0.00001056
Iteration 71/1000 | Loss: 0.00001056
Iteration 72/1000 | Loss: 0.00001056
Iteration 73/1000 | Loss: 0.00001056
Iteration 74/1000 | Loss: 0.00001056
Iteration 75/1000 | Loss: 0.00001056
Iteration 76/1000 | Loss: 0.00001056
Iteration 77/1000 | Loss: 0.00001056
Iteration 78/1000 | Loss: 0.00001055
Iteration 79/1000 | Loss: 0.00001055
Iteration 80/1000 | Loss: 0.00001055
Iteration 81/1000 | Loss: 0.00001055
Iteration 82/1000 | Loss: 0.00001054
Iteration 83/1000 | Loss: 0.00001054
Iteration 84/1000 | Loss: 0.00001054
Iteration 85/1000 | Loss: 0.00001053
Iteration 86/1000 | Loss: 0.00001053
Iteration 87/1000 | Loss: 0.00001053
Iteration 88/1000 | Loss: 0.00001052
Iteration 89/1000 | Loss: 0.00001052
Iteration 90/1000 | Loss: 0.00001052
Iteration 91/1000 | Loss: 0.00001052
Iteration 92/1000 | Loss: 0.00001052
Iteration 93/1000 | Loss: 0.00001052
Iteration 94/1000 | Loss: 0.00001052
Iteration 95/1000 | Loss: 0.00001052
Iteration 96/1000 | Loss: 0.00001051
Iteration 97/1000 | Loss: 0.00001051
Iteration 98/1000 | Loss: 0.00001051
Iteration 99/1000 | Loss: 0.00001051
Iteration 100/1000 | Loss: 0.00001050
Iteration 101/1000 | Loss: 0.00001050
Iteration 102/1000 | Loss: 0.00001050
Iteration 103/1000 | Loss: 0.00001050
Iteration 104/1000 | Loss: 0.00001050
Iteration 105/1000 | Loss: 0.00001049
Iteration 106/1000 | Loss: 0.00001049
Iteration 107/1000 | Loss: 0.00001049
Iteration 108/1000 | Loss: 0.00001049
Iteration 109/1000 | Loss: 0.00001049
Iteration 110/1000 | Loss: 0.00001049
Iteration 111/1000 | Loss: 0.00001049
Iteration 112/1000 | Loss: 0.00001049
Iteration 113/1000 | Loss: 0.00001049
Iteration 114/1000 | Loss: 0.00001048
Iteration 115/1000 | Loss: 0.00001048
Iteration 116/1000 | Loss: 0.00001048
Iteration 117/1000 | Loss: 0.00001048
Iteration 118/1000 | Loss: 0.00001047
Iteration 119/1000 | Loss: 0.00001047
Iteration 120/1000 | Loss: 0.00001047
Iteration 121/1000 | Loss: 0.00001047
Iteration 122/1000 | Loss: 0.00001047
Iteration 123/1000 | Loss: 0.00001047
Iteration 124/1000 | Loss: 0.00001047
Iteration 125/1000 | Loss: 0.00001047
Iteration 126/1000 | Loss: 0.00001047
Iteration 127/1000 | Loss: 0.00001047
Iteration 128/1000 | Loss: 0.00001047
Iteration 129/1000 | Loss: 0.00001047
Iteration 130/1000 | Loss: 0.00001046
Iteration 131/1000 | Loss: 0.00001046
Iteration 132/1000 | Loss: 0.00001046
Iteration 133/1000 | Loss: 0.00001046
Iteration 134/1000 | Loss: 0.00001046
Iteration 135/1000 | Loss: 0.00001046
Iteration 136/1000 | Loss: 0.00001046
Iteration 137/1000 | Loss: 0.00001046
Iteration 138/1000 | Loss: 0.00001046
Iteration 139/1000 | Loss: 0.00001046
Iteration 140/1000 | Loss: 0.00001046
Iteration 141/1000 | Loss: 0.00001046
Iteration 142/1000 | Loss: 0.00001046
Iteration 143/1000 | Loss: 0.00001046
Iteration 144/1000 | Loss: 0.00001046
Iteration 145/1000 | Loss: 0.00001046
Iteration 146/1000 | Loss: 0.00001046
Iteration 147/1000 | Loss: 0.00001046
Iteration 148/1000 | Loss: 0.00001045
Iteration 149/1000 | Loss: 0.00001045
Iteration 150/1000 | Loss: 0.00001045
Iteration 151/1000 | Loss: 0.00001045
Iteration 152/1000 | Loss: 0.00001045
Iteration 153/1000 | Loss: 0.00001045
Iteration 154/1000 | Loss: 0.00001045
Iteration 155/1000 | Loss: 0.00001045
Iteration 156/1000 | Loss: 0.00001045
Iteration 157/1000 | Loss: 0.00001045
Iteration 158/1000 | Loss: 0.00001045
Iteration 159/1000 | Loss: 0.00001044
Iteration 160/1000 | Loss: 0.00001044
Iteration 161/1000 | Loss: 0.00001044
Iteration 162/1000 | Loss: 0.00001044
Iteration 163/1000 | Loss: 0.00001044
Iteration 164/1000 | Loss: 0.00001044
Iteration 165/1000 | Loss: 0.00001044
Iteration 166/1000 | Loss: 0.00001044
Iteration 167/1000 | Loss: 0.00001044
Iteration 168/1000 | Loss: 0.00001044
Iteration 169/1000 | Loss: 0.00001044
Iteration 170/1000 | Loss: 0.00001043
Iteration 171/1000 | Loss: 0.00001043
Iteration 172/1000 | Loss: 0.00001043
Iteration 173/1000 | Loss: 0.00001043
Iteration 174/1000 | Loss: 0.00001043
Iteration 175/1000 | Loss: 0.00001043
Iteration 176/1000 | Loss: 0.00001043
Iteration 177/1000 | Loss: 0.00001043
Iteration 178/1000 | Loss: 0.00001043
Iteration 179/1000 | Loss: 0.00001043
Iteration 180/1000 | Loss: 0.00001043
Iteration 181/1000 | Loss: 0.00001043
Iteration 182/1000 | Loss: 0.00001043
Iteration 183/1000 | Loss: 0.00001043
Iteration 184/1000 | Loss: 0.00001043
Iteration 185/1000 | Loss: 0.00001043
Iteration 186/1000 | Loss: 0.00001043
Iteration 187/1000 | Loss: 0.00001043
Iteration 188/1000 | Loss: 0.00001043
Iteration 189/1000 | Loss: 0.00001042
Iteration 190/1000 | Loss: 0.00001042
Iteration 191/1000 | Loss: 0.00001042
Iteration 192/1000 | Loss: 0.00001042
Iteration 193/1000 | Loss: 0.00001042
Iteration 194/1000 | Loss: 0.00001042
Iteration 195/1000 | Loss: 0.00001042
Iteration 196/1000 | Loss: 0.00001042
Iteration 197/1000 | Loss: 0.00001042
Iteration 198/1000 | Loss: 0.00001042
Iteration 199/1000 | Loss: 0.00001042
Iteration 200/1000 | Loss: 0.00001042
Iteration 201/1000 | Loss: 0.00001042
Iteration 202/1000 | Loss: 0.00001042
Iteration 203/1000 | Loss: 0.00001042
Iteration 204/1000 | Loss: 0.00001042
Iteration 205/1000 | Loss: 0.00001042
Iteration 206/1000 | Loss: 0.00001042
Iteration 207/1000 | Loss: 0.00001042
Iteration 208/1000 | Loss: 0.00001042
Iteration 209/1000 | Loss: 0.00001042
Iteration 210/1000 | Loss: 0.00001042
Iteration 211/1000 | Loss: 0.00001042
Iteration 212/1000 | Loss: 0.00001042
Iteration 213/1000 | Loss: 0.00001042
Iteration 214/1000 | Loss: 0.00001042
Iteration 215/1000 | Loss: 0.00001042
Iteration 216/1000 | Loss: 0.00001042
Iteration 217/1000 | Loss: 0.00001042
Iteration 218/1000 | Loss: 0.00001042
Iteration 219/1000 | Loss: 0.00001042
Iteration 220/1000 | Loss: 0.00001042
Iteration 221/1000 | Loss: 0.00001042
Iteration 222/1000 | Loss: 0.00001042
Iteration 223/1000 | Loss: 0.00001042
Iteration 224/1000 | Loss: 0.00001042
Iteration 225/1000 | Loss: 0.00001042
Iteration 226/1000 | Loss: 0.00001042
Iteration 227/1000 | Loss: 0.00001042
Iteration 228/1000 | Loss: 0.00001042
Iteration 229/1000 | Loss: 0.00001042
Iteration 230/1000 | Loss: 0.00001042
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 230. Stopping optimization.
Last 5 losses: [1.0417420526209753e-05, 1.0417420526209753e-05, 1.0417420526209753e-05, 1.0417420526209753e-05, 1.0417420526209753e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0417420526209753e-05

Optimization complete. Final v2v error: 2.791821002960205 mm

Highest mean error: 3.27066969871521 mm for frame 57

Lowest mean error: 2.544907808303833 mm for frame 124

Saving results

Total time: 43.24173951148987
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_025/1071/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_025/1071.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_025/1071
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01011601
Iteration 2/25 | Loss: 0.00206454
Iteration 3/25 | Loss: 0.00153662
Iteration 4/25 | Loss: 0.00157181
Iteration 5/25 | Loss: 0.00143053
Iteration 6/25 | Loss: 0.00135773
Iteration 7/25 | Loss: 0.00128455
Iteration 8/25 | Loss: 0.00125362
Iteration 9/25 | Loss: 0.00122733
Iteration 10/25 | Loss: 0.00121480
Iteration 11/25 | Loss: 0.00120191
Iteration 12/25 | Loss: 0.00120758
Iteration 13/25 | Loss: 0.00119823
Iteration 14/25 | Loss: 0.00119865
Iteration 15/25 | Loss: 0.00119694
Iteration 16/25 | Loss: 0.00119694
Iteration 17/25 | Loss: 0.00119694
Iteration 18/25 | Loss: 0.00119694
Iteration 19/25 | Loss: 0.00119693
Iteration 20/25 | Loss: 0.00119693
Iteration 21/25 | Loss: 0.00119693
Iteration 22/25 | Loss: 0.00119693
Iteration 23/25 | Loss: 0.00119693
Iteration 24/25 | Loss: 0.00119693
Iteration 25/25 | Loss: 0.00119692

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.27361226
Iteration 2/25 | Loss: 0.00142323
Iteration 3/25 | Loss: 0.00142322
Iteration 4/25 | Loss: 0.00135921
Iteration 5/25 | Loss: 0.00135921
Iteration 6/25 | Loss: 0.00135921
Iteration 7/25 | Loss: 0.00135921
Iteration 8/25 | Loss: 0.00135921
Iteration 9/25 | Loss: 0.00135921
Iteration 10/25 | Loss: 0.00135921
Iteration 11/25 | Loss: 0.00135921
Iteration 12/25 | Loss: 0.00135921
Iteration 13/25 | Loss: 0.00135921
Iteration 14/25 | Loss: 0.00135921
Iteration 15/25 | Loss: 0.00135921
Iteration 16/25 | Loss: 0.00135921
Iteration 17/25 | Loss: 0.00135921
Iteration 18/25 | Loss: 0.00135921
Iteration 19/25 | Loss: 0.00135921
Iteration 20/25 | Loss: 0.00135921
Iteration 21/25 | Loss: 0.00135921
Iteration 22/25 | Loss: 0.00135921
Iteration 23/25 | Loss: 0.00135921
Iteration 24/25 | Loss: 0.00135921
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0013592095347121358, 0.0013592095347121358, 0.0013592095347121358, 0.0013592095347121358, 0.0013592095347121358]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013592095347121358

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00135921
Iteration 2/1000 | Loss: 0.00049255
Iteration 3/1000 | Loss: 0.00030807
Iteration 4/1000 | Loss: 0.00001365
Iteration 5/1000 | Loss: 0.00010980
Iteration 6/1000 | Loss: 0.00002390
Iteration 7/1000 | Loss: 0.00002913
Iteration 8/1000 | Loss: 0.00001772
Iteration 9/1000 | Loss: 0.00001919
Iteration 10/1000 | Loss: 0.00008859
Iteration 11/1000 | Loss: 0.00001131
Iteration 12/1000 | Loss: 0.00001109
Iteration 13/1000 | Loss: 0.00001106
Iteration 14/1000 | Loss: 0.00001090
Iteration 15/1000 | Loss: 0.00002510
Iteration 16/1000 | Loss: 0.00004474
Iteration 17/1000 | Loss: 0.00001400
Iteration 18/1000 | Loss: 0.00002495
Iteration 19/1000 | Loss: 0.00002790
Iteration 20/1000 | Loss: 0.00001073
Iteration 21/1000 | Loss: 0.00001444
Iteration 22/1000 | Loss: 0.00002829
Iteration 23/1000 | Loss: 0.00003014
Iteration 24/1000 | Loss: 0.00001382
Iteration 25/1000 | Loss: 0.00001100
Iteration 26/1000 | Loss: 0.00001611
Iteration 27/1000 | Loss: 0.00002083
Iteration 28/1000 | Loss: 0.00001140
Iteration 29/1000 | Loss: 0.00001438
Iteration 30/1000 | Loss: 0.00001071
Iteration 31/1000 | Loss: 0.00001082
Iteration 32/1000 | Loss: 0.00001456
Iteration 33/1000 | Loss: 0.00001067
Iteration 34/1000 | Loss: 0.00001130
Iteration 35/1000 | Loss: 0.00001119
Iteration 36/1000 | Loss: 0.00001040
Iteration 37/1000 | Loss: 0.00001039
Iteration 38/1000 | Loss: 0.00001039
Iteration 39/1000 | Loss: 0.00001038
Iteration 40/1000 | Loss: 0.00001038
Iteration 41/1000 | Loss: 0.00002036
Iteration 42/1000 | Loss: 0.00002469
Iteration 43/1000 | Loss: 0.00001032
Iteration 44/1000 | Loss: 0.00001029
Iteration 45/1000 | Loss: 0.00001028
Iteration 46/1000 | Loss: 0.00001027
Iteration 47/1000 | Loss: 0.00001027
Iteration 48/1000 | Loss: 0.00001027
Iteration 49/1000 | Loss: 0.00001027
Iteration 50/1000 | Loss: 0.00001027
Iteration 51/1000 | Loss: 0.00001027
Iteration 52/1000 | Loss: 0.00001027
Iteration 53/1000 | Loss: 0.00001026
Iteration 54/1000 | Loss: 0.00001855
Iteration 55/1000 | Loss: 0.00001026
Iteration 56/1000 | Loss: 0.00001025
Iteration 57/1000 | Loss: 0.00001024
Iteration 58/1000 | Loss: 0.00001024
Iteration 59/1000 | Loss: 0.00001023
Iteration 60/1000 | Loss: 0.00001023
Iteration 61/1000 | Loss: 0.00001022
Iteration 62/1000 | Loss: 0.00001021
Iteration 63/1000 | Loss: 0.00001020
Iteration 64/1000 | Loss: 0.00001020
Iteration 65/1000 | Loss: 0.00001223
Iteration 66/1000 | Loss: 0.00001016
Iteration 67/1000 | Loss: 0.00001016
Iteration 68/1000 | Loss: 0.00001016
Iteration 69/1000 | Loss: 0.00001016
Iteration 70/1000 | Loss: 0.00001016
Iteration 71/1000 | Loss: 0.00001016
Iteration 72/1000 | Loss: 0.00001016
Iteration 73/1000 | Loss: 0.00001016
Iteration 74/1000 | Loss: 0.00001016
Iteration 75/1000 | Loss: 0.00001016
Iteration 76/1000 | Loss: 0.00001016
Iteration 77/1000 | Loss: 0.00001016
Iteration 78/1000 | Loss: 0.00001016
Iteration 79/1000 | Loss: 0.00001016
Iteration 80/1000 | Loss: 0.00001016
Iteration 81/1000 | Loss: 0.00001016
Iteration 82/1000 | Loss: 0.00001016
Iteration 83/1000 | Loss: 0.00001016
Iteration 84/1000 | Loss: 0.00001016
Iteration 85/1000 | Loss: 0.00001016
Iteration 86/1000 | Loss: 0.00001016
Iteration 87/1000 | Loss: 0.00001016
Iteration 88/1000 | Loss: 0.00001016
Iteration 89/1000 | Loss: 0.00001016
Iteration 90/1000 | Loss: 0.00001016
Iteration 91/1000 | Loss: 0.00001016
Iteration 92/1000 | Loss: 0.00001016
Iteration 93/1000 | Loss: 0.00001016
Iteration 94/1000 | Loss: 0.00001016
Iteration 95/1000 | Loss: 0.00001016
Iteration 96/1000 | Loss: 0.00001016
Iteration 97/1000 | Loss: 0.00001016
Iteration 98/1000 | Loss: 0.00001016
Iteration 99/1000 | Loss: 0.00001016
Iteration 100/1000 | Loss: 0.00001016
Iteration 101/1000 | Loss: 0.00001016
Iteration 102/1000 | Loss: 0.00001016
Iteration 103/1000 | Loss: 0.00001016
Iteration 104/1000 | Loss: 0.00001016
Iteration 105/1000 | Loss: 0.00001016
Iteration 106/1000 | Loss: 0.00001016
Iteration 107/1000 | Loss: 0.00001016
Iteration 108/1000 | Loss: 0.00001016
Iteration 109/1000 | Loss: 0.00001016
Iteration 110/1000 | Loss: 0.00001016
Iteration 111/1000 | Loss: 0.00001016
Iteration 112/1000 | Loss: 0.00001016
Iteration 113/1000 | Loss: 0.00001016
Iteration 114/1000 | Loss: 0.00001016
Iteration 115/1000 | Loss: 0.00001016
Iteration 116/1000 | Loss: 0.00001016
Iteration 117/1000 | Loss: 0.00001016
Iteration 118/1000 | Loss: 0.00001016
Iteration 119/1000 | Loss: 0.00001016
Iteration 120/1000 | Loss: 0.00001016
Iteration 121/1000 | Loss: 0.00001016
Iteration 122/1000 | Loss: 0.00001016
Iteration 123/1000 | Loss: 0.00001016
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 123. Stopping optimization.
Last 5 losses: [1.015797897707671e-05, 1.015797897707671e-05, 1.015797897707671e-05, 1.015797897707671e-05, 1.015797897707671e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.015797897707671e-05

Optimization complete. Final v2v error: 2.7801311016082764 mm

Highest mean error: 3.0137596130371094 mm for frame 99

Lowest mean error: 2.674410343170166 mm for frame 19

Saving results

Total time: 78.68238949775696
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_025/1085/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_025/1085.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_025/1085
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00388894
Iteration 2/25 | Loss: 0.00136187
Iteration 3/25 | Loss: 0.00125760
Iteration 4/25 | Loss: 0.00124659
Iteration 5/25 | Loss: 0.00124323
Iteration 6/25 | Loss: 0.00124230
Iteration 7/25 | Loss: 0.00124195
Iteration 8/25 | Loss: 0.00124195
Iteration 9/25 | Loss: 0.00124195
Iteration 10/25 | Loss: 0.00124195
Iteration 11/25 | Loss: 0.00124195
Iteration 12/25 | Loss: 0.00124195
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0012419461272656918, 0.0012419461272656918, 0.0012419461272656918, 0.0012419461272656918, 0.0012419461272656918]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012419461272656918

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39467609
Iteration 2/25 | Loss: 0.00165288
Iteration 3/25 | Loss: 0.00165288
Iteration 4/25 | Loss: 0.00165288
Iteration 5/25 | Loss: 0.00165288
Iteration 6/25 | Loss: 0.00165288
Iteration 7/25 | Loss: 0.00165288
Iteration 8/25 | Loss: 0.00165288
Iteration 9/25 | Loss: 0.00165288
Iteration 10/25 | Loss: 0.00165288
Iteration 11/25 | Loss: 0.00165288
Iteration 12/25 | Loss: 0.00165288
Iteration 13/25 | Loss: 0.00165288
Iteration 14/25 | Loss: 0.00165288
Iteration 15/25 | Loss: 0.00165288
Iteration 16/25 | Loss: 0.00165288
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0016528755659237504, 0.0016528755659237504, 0.0016528755659237504, 0.0016528755659237504, 0.0016528755659237504]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0016528755659237504

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00165288
Iteration 2/1000 | Loss: 0.00003487
Iteration 3/1000 | Loss: 0.00002380
Iteration 4/1000 | Loss: 0.00001738
Iteration 5/1000 | Loss: 0.00001557
Iteration 6/1000 | Loss: 0.00001444
Iteration 7/1000 | Loss: 0.00001385
Iteration 8/1000 | Loss: 0.00001343
Iteration 9/1000 | Loss: 0.00001304
Iteration 10/1000 | Loss: 0.00001278
Iteration 11/1000 | Loss: 0.00001262
Iteration 12/1000 | Loss: 0.00001238
Iteration 13/1000 | Loss: 0.00001233
Iteration 14/1000 | Loss: 0.00001218
Iteration 15/1000 | Loss: 0.00001207
Iteration 16/1000 | Loss: 0.00001206
Iteration 17/1000 | Loss: 0.00001202
Iteration 18/1000 | Loss: 0.00001200
Iteration 19/1000 | Loss: 0.00001199
Iteration 20/1000 | Loss: 0.00001198
Iteration 21/1000 | Loss: 0.00001197
Iteration 22/1000 | Loss: 0.00001197
Iteration 23/1000 | Loss: 0.00001197
Iteration 24/1000 | Loss: 0.00001196
Iteration 25/1000 | Loss: 0.00001196
Iteration 26/1000 | Loss: 0.00001195
Iteration 27/1000 | Loss: 0.00001195
Iteration 28/1000 | Loss: 0.00001194
Iteration 29/1000 | Loss: 0.00001193
Iteration 30/1000 | Loss: 0.00001191
Iteration 31/1000 | Loss: 0.00001190
Iteration 32/1000 | Loss: 0.00001186
Iteration 33/1000 | Loss: 0.00001185
Iteration 34/1000 | Loss: 0.00001181
Iteration 35/1000 | Loss: 0.00001178
Iteration 36/1000 | Loss: 0.00001177
Iteration 37/1000 | Loss: 0.00001177
Iteration 38/1000 | Loss: 0.00001176
Iteration 39/1000 | Loss: 0.00001175
Iteration 40/1000 | Loss: 0.00001175
Iteration 41/1000 | Loss: 0.00001174
Iteration 42/1000 | Loss: 0.00001172
Iteration 43/1000 | Loss: 0.00001170
Iteration 44/1000 | Loss: 0.00001170
Iteration 45/1000 | Loss: 0.00001168
Iteration 46/1000 | Loss: 0.00001168
Iteration 47/1000 | Loss: 0.00001168
Iteration 48/1000 | Loss: 0.00001167
Iteration 49/1000 | Loss: 0.00001167
Iteration 50/1000 | Loss: 0.00001167
Iteration 51/1000 | Loss: 0.00001166
Iteration 52/1000 | Loss: 0.00001166
Iteration 53/1000 | Loss: 0.00001166
Iteration 54/1000 | Loss: 0.00001165
Iteration 55/1000 | Loss: 0.00001165
Iteration 56/1000 | Loss: 0.00001165
Iteration 57/1000 | Loss: 0.00001164
Iteration 58/1000 | Loss: 0.00001164
Iteration 59/1000 | Loss: 0.00001164
Iteration 60/1000 | Loss: 0.00001163
Iteration 61/1000 | Loss: 0.00001163
Iteration 62/1000 | Loss: 0.00001163
Iteration 63/1000 | Loss: 0.00001163
Iteration 64/1000 | Loss: 0.00001162
Iteration 65/1000 | Loss: 0.00001162
Iteration 66/1000 | Loss: 0.00001162
Iteration 67/1000 | Loss: 0.00001162
Iteration 68/1000 | Loss: 0.00001162
Iteration 69/1000 | Loss: 0.00001161
Iteration 70/1000 | Loss: 0.00001161
Iteration 71/1000 | Loss: 0.00001161
Iteration 72/1000 | Loss: 0.00001161
Iteration 73/1000 | Loss: 0.00001161
Iteration 74/1000 | Loss: 0.00001161
Iteration 75/1000 | Loss: 0.00001160
Iteration 76/1000 | Loss: 0.00001160
Iteration 77/1000 | Loss: 0.00001160
Iteration 78/1000 | Loss: 0.00001160
Iteration 79/1000 | Loss: 0.00001160
Iteration 80/1000 | Loss: 0.00001159
Iteration 81/1000 | Loss: 0.00001159
Iteration 82/1000 | Loss: 0.00001159
Iteration 83/1000 | Loss: 0.00001159
Iteration 84/1000 | Loss: 0.00001159
Iteration 85/1000 | Loss: 0.00001159
Iteration 86/1000 | Loss: 0.00001159
Iteration 87/1000 | Loss: 0.00001158
Iteration 88/1000 | Loss: 0.00001158
Iteration 89/1000 | Loss: 0.00001158
Iteration 90/1000 | Loss: 0.00001158
Iteration 91/1000 | Loss: 0.00001157
Iteration 92/1000 | Loss: 0.00001157
Iteration 93/1000 | Loss: 0.00001156
Iteration 94/1000 | Loss: 0.00001156
Iteration 95/1000 | Loss: 0.00001156
Iteration 96/1000 | Loss: 0.00001156
Iteration 97/1000 | Loss: 0.00001156
Iteration 98/1000 | Loss: 0.00001156
Iteration 99/1000 | Loss: 0.00001156
Iteration 100/1000 | Loss: 0.00001156
Iteration 101/1000 | Loss: 0.00001156
Iteration 102/1000 | Loss: 0.00001155
Iteration 103/1000 | Loss: 0.00001155
Iteration 104/1000 | Loss: 0.00001155
Iteration 105/1000 | Loss: 0.00001155
Iteration 106/1000 | Loss: 0.00001155
Iteration 107/1000 | Loss: 0.00001155
Iteration 108/1000 | Loss: 0.00001155
Iteration 109/1000 | Loss: 0.00001155
Iteration 110/1000 | Loss: 0.00001155
Iteration 111/1000 | Loss: 0.00001154
Iteration 112/1000 | Loss: 0.00001154
Iteration 113/1000 | Loss: 0.00001154
Iteration 114/1000 | Loss: 0.00001154
Iteration 115/1000 | Loss: 0.00001154
Iteration 116/1000 | Loss: 0.00001154
Iteration 117/1000 | Loss: 0.00001154
Iteration 118/1000 | Loss: 0.00001153
Iteration 119/1000 | Loss: 0.00001153
Iteration 120/1000 | Loss: 0.00001153
Iteration 121/1000 | Loss: 0.00001153
Iteration 122/1000 | Loss: 0.00001153
Iteration 123/1000 | Loss: 0.00001153
Iteration 124/1000 | Loss: 0.00001153
Iteration 125/1000 | Loss: 0.00001153
Iteration 126/1000 | Loss: 0.00001153
Iteration 127/1000 | Loss: 0.00001153
Iteration 128/1000 | Loss: 0.00001153
Iteration 129/1000 | Loss: 0.00001153
Iteration 130/1000 | Loss: 0.00001152
Iteration 131/1000 | Loss: 0.00001152
Iteration 132/1000 | Loss: 0.00001152
Iteration 133/1000 | Loss: 0.00001152
Iteration 134/1000 | Loss: 0.00001152
Iteration 135/1000 | Loss: 0.00001152
Iteration 136/1000 | Loss: 0.00001152
Iteration 137/1000 | Loss: 0.00001152
Iteration 138/1000 | Loss: 0.00001152
Iteration 139/1000 | Loss: 0.00001151
Iteration 140/1000 | Loss: 0.00001151
Iteration 141/1000 | Loss: 0.00001151
Iteration 142/1000 | Loss: 0.00001151
Iteration 143/1000 | Loss: 0.00001151
Iteration 144/1000 | Loss: 0.00001151
Iteration 145/1000 | Loss: 0.00001151
Iteration 146/1000 | Loss: 0.00001151
Iteration 147/1000 | Loss: 0.00001151
Iteration 148/1000 | Loss: 0.00001151
Iteration 149/1000 | Loss: 0.00001151
Iteration 150/1000 | Loss: 0.00001150
Iteration 151/1000 | Loss: 0.00001150
Iteration 152/1000 | Loss: 0.00001150
Iteration 153/1000 | Loss: 0.00001150
Iteration 154/1000 | Loss: 0.00001150
Iteration 155/1000 | Loss: 0.00001150
Iteration 156/1000 | Loss: 0.00001150
Iteration 157/1000 | Loss: 0.00001150
Iteration 158/1000 | Loss: 0.00001150
Iteration 159/1000 | Loss: 0.00001150
Iteration 160/1000 | Loss: 0.00001150
Iteration 161/1000 | Loss: 0.00001150
Iteration 162/1000 | Loss: 0.00001149
Iteration 163/1000 | Loss: 0.00001149
Iteration 164/1000 | Loss: 0.00001149
Iteration 165/1000 | Loss: 0.00001148
Iteration 166/1000 | Loss: 0.00001148
Iteration 167/1000 | Loss: 0.00001148
Iteration 168/1000 | Loss: 0.00001148
Iteration 169/1000 | Loss: 0.00001148
Iteration 170/1000 | Loss: 0.00001148
Iteration 171/1000 | Loss: 0.00001148
Iteration 172/1000 | Loss: 0.00001147
Iteration 173/1000 | Loss: 0.00001147
Iteration 174/1000 | Loss: 0.00001147
Iteration 175/1000 | Loss: 0.00001147
Iteration 176/1000 | Loss: 0.00001147
Iteration 177/1000 | Loss: 0.00001147
Iteration 178/1000 | Loss: 0.00001147
Iteration 179/1000 | Loss: 0.00001147
Iteration 180/1000 | Loss: 0.00001146
Iteration 181/1000 | Loss: 0.00001146
Iteration 182/1000 | Loss: 0.00001146
Iteration 183/1000 | Loss: 0.00001146
Iteration 184/1000 | Loss: 0.00001146
Iteration 185/1000 | Loss: 0.00001146
Iteration 186/1000 | Loss: 0.00001146
Iteration 187/1000 | Loss: 0.00001145
Iteration 188/1000 | Loss: 0.00001145
Iteration 189/1000 | Loss: 0.00001145
Iteration 190/1000 | Loss: 0.00001145
Iteration 191/1000 | Loss: 0.00001144
Iteration 192/1000 | Loss: 0.00001144
Iteration 193/1000 | Loss: 0.00001144
Iteration 194/1000 | Loss: 0.00001144
Iteration 195/1000 | Loss: 0.00001144
Iteration 196/1000 | Loss: 0.00001143
Iteration 197/1000 | Loss: 0.00001143
Iteration 198/1000 | Loss: 0.00001143
Iteration 199/1000 | Loss: 0.00001143
Iteration 200/1000 | Loss: 0.00001143
Iteration 201/1000 | Loss: 0.00001143
Iteration 202/1000 | Loss: 0.00001143
Iteration 203/1000 | Loss: 0.00001143
Iteration 204/1000 | Loss: 0.00001143
Iteration 205/1000 | Loss: 0.00001143
Iteration 206/1000 | Loss: 0.00001143
Iteration 207/1000 | Loss: 0.00001143
Iteration 208/1000 | Loss: 0.00001143
Iteration 209/1000 | Loss: 0.00001143
Iteration 210/1000 | Loss: 0.00001143
Iteration 211/1000 | Loss: 0.00001142
Iteration 212/1000 | Loss: 0.00001142
Iteration 213/1000 | Loss: 0.00001142
Iteration 214/1000 | Loss: 0.00001142
Iteration 215/1000 | Loss: 0.00001142
Iteration 216/1000 | Loss: 0.00001142
Iteration 217/1000 | Loss: 0.00001142
Iteration 218/1000 | Loss: 0.00001142
Iteration 219/1000 | Loss: 0.00001142
Iteration 220/1000 | Loss: 0.00001142
Iteration 221/1000 | Loss: 0.00001142
Iteration 222/1000 | Loss: 0.00001142
Iteration 223/1000 | Loss: 0.00001142
Iteration 224/1000 | Loss: 0.00001142
Iteration 225/1000 | Loss: 0.00001142
Iteration 226/1000 | Loss: 0.00001142
Iteration 227/1000 | Loss: 0.00001142
Iteration 228/1000 | Loss: 0.00001141
Iteration 229/1000 | Loss: 0.00001141
Iteration 230/1000 | Loss: 0.00001141
Iteration 231/1000 | Loss: 0.00001141
Iteration 232/1000 | Loss: 0.00001141
Iteration 233/1000 | Loss: 0.00001141
Iteration 234/1000 | Loss: 0.00001141
Iteration 235/1000 | Loss: 0.00001141
Iteration 236/1000 | Loss: 0.00001141
Iteration 237/1000 | Loss: 0.00001141
Iteration 238/1000 | Loss: 0.00001140
Iteration 239/1000 | Loss: 0.00001140
Iteration 240/1000 | Loss: 0.00001140
Iteration 241/1000 | Loss: 0.00001140
Iteration 242/1000 | Loss: 0.00001140
Iteration 243/1000 | Loss: 0.00001140
Iteration 244/1000 | Loss: 0.00001140
Iteration 245/1000 | Loss: 0.00001140
Iteration 246/1000 | Loss: 0.00001140
Iteration 247/1000 | Loss: 0.00001140
Iteration 248/1000 | Loss: 0.00001140
Iteration 249/1000 | Loss: 0.00001139
Iteration 250/1000 | Loss: 0.00001139
Iteration 251/1000 | Loss: 0.00001139
Iteration 252/1000 | Loss: 0.00001139
Iteration 253/1000 | Loss: 0.00001139
Iteration 254/1000 | Loss: 0.00001139
Iteration 255/1000 | Loss: 0.00001139
Iteration 256/1000 | Loss: 0.00001139
Iteration 257/1000 | Loss: 0.00001139
Iteration 258/1000 | Loss: 0.00001138
Iteration 259/1000 | Loss: 0.00001138
Iteration 260/1000 | Loss: 0.00001138
Iteration 261/1000 | Loss: 0.00001138
Iteration 262/1000 | Loss: 0.00001138
Iteration 263/1000 | Loss: 0.00001138
Iteration 264/1000 | Loss: 0.00001138
Iteration 265/1000 | Loss: 0.00001138
Iteration 266/1000 | Loss: 0.00001138
Iteration 267/1000 | Loss: 0.00001137
Iteration 268/1000 | Loss: 0.00001137
Iteration 269/1000 | Loss: 0.00001137
Iteration 270/1000 | Loss: 0.00001137
Iteration 271/1000 | Loss: 0.00001137
Iteration 272/1000 | Loss: 0.00001137
Iteration 273/1000 | Loss: 0.00001137
Iteration 274/1000 | Loss: 0.00001137
Iteration 275/1000 | Loss: 0.00001137
Iteration 276/1000 | Loss: 0.00001137
Iteration 277/1000 | Loss: 0.00001137
Iteration 278/1000 | Loss: 0.00001137
Iteration 279/1000 | Loss: 0.00001137
Iteration 280/1000 | Loss: 0.00001137
Iteration 281/1000 | Loss: 0.00001137
Iteration 282/1000 | Loss: 0.00001137
Iteration 283/1000 | Loss: 0.00001137
Iteration 284/1000 | Loss: 0.00001137
Iteration 285/1000 | Loss: 0.00001137
Iteration 286/1000 | Loss: 0.00001137
Iteration 287/1000 | Loss: 0.00001137
Iteration 288/1000 | Loss: 0.00001137
Iteration 289/1000 | Loss: 0.00001137
Iteration 290/1000 | Loss: 0.00001137
Iteration 291/1000 | Loss: 0.00001136
Iteration 292/1000 | Loss: 0.00001136
Iteration 293/1000 | Loss: 0.00001136
Iteration 294/1000 | Loss: 0.00001136
Iteration 295/1000 | Loss: 0.00001136
Iteration 296/1000 | Loss: 0.00001136
Iteration 297/1000 | Loss: 0.00001136
Iteration 298/1000 | Loss: 0.00001136
Iteration 299/1000 | Loss: 0.00001136
Iteration 300/1000 | Loss: 0.00001136
Iteration 301/1000 | Loss: 0.00001136
Iteration 302/1000 | Loss: 0.00001136
Iteration 303/1000 | Loss: 0.00001136
Iteration 304/1000 | Loss: 0.00001136
Iteration 305/1000 | Loss: 0.00001136
Iteration 306/1000 | Loss: 0.00001136
Iteration 307/1000 | Loss: 0.00001136
Iteration 308/1000 | Loss: 0.00001136
Iteration 309/1000 | Loss: 0.00001136
Iteration 310/1000 | Loss: 0.00001136
Iteration 311/1000 | Loss: 0.00001136
Iteration 312/1000 | Loss: 0.00001136
Iteration 313/1000 | Loss: 0.00001136
Iteration 314/1000 | Loss: 0.00001136
Iteration 315/1000 | Loss: 0.00001136
Iteration 316/1000 | Loss: 0.00001136
Iteration 317/1000 | Loss: 0.00001135
Iteration 318/1000 | Loss: 0.00001135
Iteration 319/1000 | Loss: 0.00001135
Iteration 320/1000 | Loss: 0.00001135
Iteration 321/1000 | Loss: 0.00001135
Iteration 322/1000 | Loss: 0.00001135
Iteration 323/1000 | Loss: 0.00001135
Iteration 324/1000 | Loss: 0.00001135
Iteration 325/1000 | Loss: 0.00001135
Iteration 326/1000 | Loss: 0.00001135
Iteration 327/1000 | Loss: 0.00001135
Iteration 328/1000 | Loss: 0.00001135
Iteration 329/1000 | Loss: 0.00001135
Iteration 330/1000 | Loss: 0.00001135
Iteration 331/1000 | Loss: 0.00001135
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 331. Stopping optimization.
Last 5 losses: [1.1352798537700437e-05, 1.1352798537700437e-05, 1.1352798537700437e-05, 1.1352798537700437e-05, 1.1352798537700437e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1352798537700437e-05

Optimization complete. Final v2v error: 2.8326828479766846 mm

Highest mean error: 4.20146369934082 mm for frame 55

Lowest mean error: 2.4419376850128174 mm for frame 26

Saving results

Total time: 50.22183084487915
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_025/1045/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_025/1045.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_025/1045
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00849085
Iteration 2/25 | Loss: 0.00151220
Iteration 3/25 | Loss: 0.00133829
Iteration 4/25 | Loss: 0.00129516
Iteration 5/25 | Loss: 0.00128923
Iteration 6/25 | Loss: 0.00128764
Iteration 7/25 | Loss: 0.00128716
Iteration 8/25 | Loss: 0.00128716
Iteration 9/25 | Loss: 0.00128716
Iteration 10/25 | Loss: 0.00128716
Iteration 11/25 | Loss: 0.00128716
Iteration 12/25 | Loss: 0.00128716
Iteration 13/25 | Loss: 0.00128716
Iteration 14/25 | Loss: 0.00128710
Iteration 15/25 | Loss: 0.00128710
Iteration 16/25 | Loss: 0.00128710
Iteration 17/25 | Loss: 0.00128710
Iteration 18/25 | Loss: 0.00128710
Iteration 19/25 | Loss: 0.00128710
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0012870999053120613, 0.0012870999053120613, 0.0012870999053120613, 0.0012870999053120613, 0.0012870999053120613]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012870999053120613

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.21905625
Iteration 2/25 | Loss: 0.00163825
Iteration 3/25 | Loss: 0.00163825
Iteration 4/25 | Loss: 0.00163825
Iteration 5/25 | Loss: 0.00163825
Iteration 6/25 | Loss: 0.00163825
Iteration 7/25 | Loss: 0.00163825
Iteration 8/25 | Loss: 0.00163825
Iteration 9/25 | Loss: 0.00163825
Iteration 10/25 | Loss: 0.00163825
Iteration 11/25 | Loss: 0.00163825
Iteration 12/25 | Loss: 0.00163825
Iteration 13/25 | Loss: 0.00163825
Iteration 14/25 | Loss: 0.00163825
Iteration 15/25 | Loss: 0.00163825
Iteration 16/25 | Loss: 0.00163825
Iteration 17/25 | Loss: 0.00163825
Iteration 18/25 | Loss: 0.00163825
Iteration 19/25 | Loss: 0.00163825
Iteration 20/25 | Loss: 0.00163825
Iteration 21/25 | Loss: 0.00163825
Iteration 22/25 | Loss: 0.00163825
Iteration 23/25 | Loss: 0.00163825
Iteration 24/25 | Loss: 0.00163825
Iteration 25/25 | Loss: 0.00163825

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00163825
Iteration 2/1000 | Loss: 0.00009967
Iteration 3/1000 | Loss: 0.00006544
Iteration 4/1000 | Loss: 0.00003751
Iteration 5/1000 | Loss: 0.00003309
Iteration 6/1000 | Loss: 0.00003067
Iteration 7/1000 | Loss: 0.00002948
Iteration 8/1000 | Loss: 0.00002869
Iteration 9/1000 | Loss: 0.00002810
Iteration 10/1000 | Loss: 0.00002785
Iteration 11/1000 | Loss: 0.00002760
Iteration 12/1000 | Loss: 0.00002739
Iteration 13/1000 | Loss: 0.00002717
Iteration 14/1000 | Loss: 0.00002698
Iteration 15/1000 | Loss: 0.00002678
Iteration 16/1000 | Loss: 0.00002676
Iteration 17/1000 | Loss: 0.00002676
Iteration 18/1000 | Loss: 0.00002664
Iteration 19/1000 | Loss: 0.00002658
Iteration 20/1000 | Loss: 0.00002657
Iteration 21/1000 | Loss: 0.00002656
Iteration 22/1000 | Loss: 0.00002656
Iteration 23/1000 | Loss: 0.00002655
Iteration 24/1000 | Loss: 0.00002654
Iteration 25/1000 | Loss: 0.00002654
Iteration 26/1000 | Loss: 0.00002652
Iteration 27/1000 | Loss: 0.00002652
Iteration 28/1000 | Loss: 0.00002651
Iteration 29/1000 | Loss: 0.00002650
Iteration 30/1000 | Loss: 0.00002650
Iteration 31/1000 | Loss: 0.00002649
Iteration 32/1000 | Loss: 0.00002645
Iteration 33/1000 | Loss: 0.00002645
Iteration 34/1000 | Loss: 0.00002644
Iteration 35/1000 | Loss: 0.00002644
Iteration 36/1000 | Loss: 0.00002644
Iteration 37/1000 | Loss: 0.00002642
Iteration 38/1000 | Loss: 0.00002640
Iteration 39/1000 | Loss: 0.00002640
Iteration 40/1000 | Loss: 0.00002639
Iteration 41/1000 | Loss: 0.00002638
Iteration 42/1000 | Loss: 0.00002637
Iteration 43/1000 | Loss: 0.00002637
Iteration 44/1000 | Loss: 0.00002636
Iteration 45/1000 | Loss: 0.00002636
Iteration 46/1000 | Loss: 0.00002635
Iteration 47/1000 | Loss: 0.00002635
Iteration 48/1000 | Loss: 0.00002635
Iteration 49/1000 | Loss: 0.00002635
Iteration 50/1000 | Loss: 0.00002634
Iteration 51/1000 | Loss: 0.00002634
Iteration 52/1000 | Loss: 0.00002634
Iteration 53/1000 | Loss: 0.00002634
Iteration 54/1000 | Loss: 0.00002634
Iteration 55/1000 | Loss: 0.00002634
Iteration 56/1000 | Loss: 0.00002634
Iteration 57/1000 | Loss: 0.00002633
Iteration 58/1000 | Loss: 0.00002633
Iteration 59/1000 | Loss: 0.00002633
Iteration 60/1000 | Loss: 0.00002633
Iteration 61/1000 | Loss: 0.00002632
Iteration 62/1000 | Loss: 0.00002632
Iteration 63/1000 | Loss: 0.00002631
Iteration 64/1000 | Loss: 0.00002631
Iteration 65/1000 | Loss: 0.00002631
Iteration 66/1000 | Loss: 0.00002631
Iteration 67/1000 | Loss: 0.00002631
Iteration 68/1000 | Loss: 0.00002630
Iteration 69/1000 | Loss: 0.00002630
Iteration 70/1000 | Loss: 0.00002630
Iteration 71/1000 | Loss: 0.00002630
Iteration 72/1000 | Loss: 0.00002628
Iteration 73/1000 | Loss: 0.00002627
Iteration 74/1000 | Loss: 0.00002626
Iteration 75/1000 | Loss: 0.00002626
Iteration 76/1000 | Loss: 0.00002626
Iteration 77/1000 | Loss: 0.00002626
Iteration 78/1000 | Loss: 0.00002625
Iteration 79/1000 | Loss: 0.00002625
Iteration 80/1000 | Loss: 0.00002625
Iteration 81/1000 | Loss: 0.00002625
Iteration 82/1000 | Loss: 0.00002624
Iteration 83/1000 | Loss: 0.00002624
Iteration 84/1000 | Loss: 0.00002624
Iteration 85/1000 | Loss: 0.00002623
Iteration 86/1000 | Loss: 0.00002623
Iteration 87/1000 | Loss: 0.00002623
Iteration 88/1000 | Loss: 0.00002623
Iteration 89/1000 | Loss: 0.00002623
Iteration 90/1000 | Loss: 0.00002622
Iteration 91/1000 | Loss: 0.00002622
Iteration 92/1000 | Loss: 0.00002622
Iteration 93/1000 | Loss: 0.00002622
Iteration 94/1000 | Loss: 0.00002621
Iteration 95/1000 | Loss: 0.00002621
Iteration 96/1000 | Loss: 0.00002621
Iteration 97/1000 | Loss: 0.00002621
Iteration 98/1000 | Loss: 0.00002621
Iteration 99/1000 | Loss: 0.00002621
Iteration 100/1000 | Loss: 0.00002620
Iteration 101/1000 | Loss: 0.00002620
Iteration 102/1000 | Loss: 0.00002620
Iteration 103/1000 | Loss: 0.00002620
Iteration 104/1000 | Loss: 0.00002620
Iteration 105/1000 | Loss: 0.00002620
Iteration 106/1000 | Loss: 0.00002619
Iteration 107/1000 | Loss: 0.00002619
Iteration 108/1000 | Loss: 0.00002619
Iteration 109/1000 | Loss: 0.00002619
Iteration 110/1000 | Loss: 0.00002619
Iteration 111/1000 | Loss: 0.00002619
Iteration 112/1000 | Loss: 0.00002618
Iteration 113/1000 | Loss: 0.00002618
Iteration 114/1000 | Loss: 0.00002618
Iteration 115/1000 | Loss: 0.00002618
Iteration 116/1000 | Loss: 0.00002618
Iteration 117/1000 | Loss: 0.00002618
Iteration 118/1000 | Loss: 0.00002617
Iteration 119/1000 | Loss: 0.00002617
Iteration 120/1000 | Loss: 0.00002617
Iteration 121/1000 | Loss: 0.00002617
Iteration 122/1000 | Loss: 0.00002617
Iteration 123/1000 | Loss: 0.00002616
Iteration 124/1000 | Loss: 0.00002616
Iteration 125/1000 | Loss: 0.00002616
Iteration 126/1000 | Loss: 0.00002616
Iteration 127/1000 | Loss: 0.00002616
Iteration 128/1000 | Loss: 0.00002616
Iteration 129/1000 | Loss: 0.00002616
Iteration 130/1000 | Loss: 0.00002616
Iteration 131/1000 | Loss: 0.00002616
Iteration 132/1000 | Loss: 0.00002615
Iteration 133/1000 | Loss: 0.00002615
Iteration 134/1000 | Loss: 0.00002615
Iteration 135/1000 | Loss: 0.00002615
Iteration 136/1000 | Loss: 0.00002615
Iteration 137/1000 | Loss: 0.00002614
Iteration 138/1000 | Loss: 0.00002614
Iteration 139/1000 | Loss: 0.00002614
Iteration 140/1000 | Loss: 0.00002614
Iteration 141/1000 | Loss: 0.00002614
Iteration 142/1000 | Loss: 0.00002614
Iteration 143/1000 | Loss: 0.00002614
Iteration 144/1000 | Loss: 0.00002614
Iteration 145/1000 | Loss: 0.00002614
Iteration 146/1000 | Loss: 0.00002614
Iteration 147/1000 | Loss: 0.00002614
Iteration 148/1000 | Loss: 0.00002614
Iteration 149/1000 | Loss: 0.00002614
Iteration 150/1000 | Loss: 0.00002614
Iteration 151/1000 | Loss: 0.00002614
Iteration 152/1000 | Loss: 0.00002614
Iteration 153/1000 | Loss: 0.00002614
Iteration 154/1000 | Loss: 0.00002614
Iteration 155/1000 | Loss: 0.00002614
Iteration 156/1000 | Loss: 0.00002614
Iteration 157/1000 | Loss: 0.00002614
Iteration 158/1000 | Loss: 0.00002614
Iteration 159/1000 | Loss: 0.00002614
Iteration 160/1000 | Loss: 0.00002614
Iteration 161/1000 | Loss: 0.00002614
Iteration 162/1000 | Loss: 0.00002614
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 162. Stopping optimization.
Last 5 losses: [2.6136316591873765e-05, 2.6136316591873765e-05, 2.6136316591873765e-05, 2.6136316591873765e-05, 2.6136316591873765e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.6136316591873765e-05

Optimization complete. Final v2v error: 4.302768707275391 mm

Highest mean error: 4.749209403991699 mm for frame 78

Lowest mean error: 3.363726854324341 mm for frame 6

Saving results

Total time: 42.9345269203186
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_025/1070/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_025/1070.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_025/1070
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00675255
Iteration 2/25 | Loss: 0.00168847
Iteration 3/25 | Loss: 0.00144510
Iteration 4/25 | Loss: 0.00140798
Iteration 5/25 | Loss: 0.00140042
Iteration 6/25 | Loss: 0.00139873
Iteration 7/25 | Loss: 0.00139873
Iteration 8/25 | Loss: 0.00139873
Iteration 9/25 | Loss: 0.00139873
Iteration 10/25 | Loss: 0.00139873
Iteration 11/25 | Loss: 0.00139873
Iteration 12/25 | Loss: 0.00139873
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0013987282291054726, 0.0013987282291054726, 0.0013987282291054726, 0.0013987282291054726, 0.0013987282291054726]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013987282291054726

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.09234476
Iteration 2/25 | Loss: 0.00193229
Iteration 3/25 | Loss: 0.00193229
Iteration 4/25 | Loss: 0.00193229
Iteration 5/25 | Loss: 0.00193229
Iteration 6/25 | Loss: 0.00193229
Iteration 7/25 | Loss: 0.00193229
Iteration 8/25 | Loss: 0.00193229
Iteration 9/25 | Loss: 0.00193229
Iteration 10/25 | Loss: 0.00193229
Iteration 11/25 | Loss: 0.00193229
Iteration 12/25 | Loss: 0.00193229
Iteration 13/25 | Loss: 0.00193229
Iteration 14/25 | Loss: 0.00193229
Iteration 15/25 | Loss: 0.00193229
Iteration 16/25 | Loss: 0.00193229
Iteration 17/25 | Loss: 0.00193229
Iteration 18/25 | Loss: 0.00193229
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0019322867738083005, 0.0019322867738083005, 0.0019322867738083005, 0.0019322867738083005, 0.0019322867738083005]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0019322867738083005

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00193229
Iteration 2/1000 | Loss: 0.00007663
Iteration 3/1000 | Loss: 0.00004588
Iteration 4/1000 | Loss: 0.00003789
Iteration 5/1000 | Loss: 0.00003486
Iteration 6/1000 | Loss: 0.00003244
Iteration 7/1000 | Loss: 0.00003101
Iteration 8/1000 | Loss: 0.00002999
Iteration 9/1000 | Loss: 0.00002945
Iteration 10/1000 | Loss: 0.00002904
Iteration 11/1000 | Loss: 0.00002872
Iteration 12/1000 | Loss: 0.00002840
Iteration 13/1000 | Loss: 0.00002815
Iteration 14/1000 | Loss: 0.00002795
Iteration 15/1000 | Loss: 0.00002775
Iteration 16/1000 | Loss: 0.00002770
Iteration 17/1000 | Loss: 0.00002762
Iteration 18/1000 | Loss: 0.00002758
Iteration 19/1000 | Loss: 0.00002758
Iteration 20/1000 | Loss: 0.00002756
Iteration 21/1000 | Loss: 0.00002753
Iteration 22/1000 | Loss: 0.00002751
Iteration 23/1000 | Loss: 0.00002750
Iteration 24/1000 | Loss: 0.00002749
Iteration 25/1000 | Loss: 0.00002748
Iteration 26/1000 | Loss: 0.00002748
Iteration 27/1000 | Loss: 0.00002748
Iteration 28/1000 | Loss: 0.00002745
Iteration 29/1000 | Loss: 0.00002744
Iteration 30/1000 | Loss: 0.00002739
Iteration 31/1000 | Loss: 0.00002739
Iteration 32/1000 | Loss: 0.00002734
Iteration 33/1000 | Loss: 0.00002734
Iteration 34/1000 | Loss: 0.00002731
Iteration 35/1000 | Loss: 0.00002730
Iteration 36/1000 | Loss: 0.00002730
Iteration 37/1000 | Loss: 0.00002730
Iteration 38/1000 | Loss: 0.00002729
Iteration 39/1000 | Loss: 0.00002729
Iteration 40/1000 | Loss: 0.00002728
Iteration 41/1000 | Loss: 0.00002728
Iteration 42/1000 | Loss: 0.00002728
Iteration 43/1000 | Loss: 0.00002727
Iteration 44/1000 | Loss: 0.00002726
Iteration 45/1000 | Loss: 0.00002725
Iteration 46/1000 | Loss: 0.00002725
Iteration 47/1000 | Loss: 0.00002725
Iteration 48/1000 | Loss: 0.00002724
Iteration 49/1000 | Loss: 0.00002724
Iteration 50/1000 | Loss: 0.00002724
Iteration 51/1000 | Loss: 0.00002723
Iteration 52/1000 | Loss: 0.00002723
Iteration 53/1000 | Loss: 0.00002722
Iteration 54/1000 | Loss: 0.00002722
Iteration 55/1000 | Loss: 0.00002722
Iteration 56/1000 | Loss: 0.00002721
Iteration 57/1000 | Loss: 0.00002720
Iteration 58/1000 | Loss: 0.00002720
Iteration 59/1000 | Loss: 0.00002719
Iteration 60/1000 | Loss: 0.00002719
Iteration 61/1000 | Loss: 0.00002719
Iteration 62/1000 | Loss: 0.00002716
Iteration 63/1000 | Loss: 0.00002715
Iteration 64/1000 | Loss: 0.00002715
Iteration 65/1000 | Loss: 0.00002714
Iteration 66/1000 | Loss: 0.00002714
Iteration 67/1000 | Loss: 0.00002714
Iteration 68/1000 | Loss: 0.00002713
Iteration 69/1000 | Loss: 0.00002713
Iteration 70/1000 | Loss: 0.00002713
Iteration 71/1000 | Loss: 0.00002712
Iteration 72/1000 | Loss: 0.00002712
Iteration 73/1000 | Loss: 0.00002712
Iteration 74/1000 | Loss: 0.00002712
Iteration 75/1000 | Loss: 0.00002711
Iteration 76/1000 | Loss: 0.00002711
Iteration 77/1000 | Loss: 0.00002711
Iteration 78/1000 | Loss: 0.00002711
Iteration 79/1000 | Loss: 0.00002711
Iteration 80/1000 | Loss: 0.00002711
Iteration 81/1000 | Loss: 0.00002711
Iteration 82/1000 | Loss: 0.00002711
Iteration 83/1000 | Loss: 0.00002711
Iteration 84/1000 | Loss: 0.00002711
Iteration 85/1000 | Loss: 0.00002711
Iteration 86/1000 | Loss: 0.00002711
Iteration 87/1000 | Loss: 0.00002710
Iteration 88/1000 | Loss: 0.00002710
Iteration 89/1000 | Loss: 0.00002710
Iteration 90/1000 | Loss: 0.00002709
Iteration 91/1000 | Loss: 0.00002709
Iteration 92/1000 | Loss: 0.00002709
Iteration 93/1000 | Loss: 0.00002709
Iteration 94/1000 | Loss: 0.00002708
Iteration 95/1000 | Loss: 0.00002708
Iteration 96/1000 | Loss: 0.00002708
Iteration 97/1000 | Loss: 0.00002708
Iteration 98/1000 | Loss: 0.00002708
Iteration 99/1000 | Loss: 0.00002708
Iteration 100/1000 | Loss: 0.00002707
Iteration 101/1000 | Loss: 0.00002707
Iteration 102/1000 | Loss: 0.00002707
Iteration 103/1000 | Loss: 0.00002707
Iteration 104/1000 | Loss: 0.00002707
Iteration 105/1000 | Loss: 0.00002707
Iteration 106/1000 | Loss: 0.00002707
Iteration 107/1000 | Loss: 0.00002707
Iteration 108/1000 | Loss: 0.00002707
Iteration 109/1000 | Loss: 0.00002707
Iteration 110/1000 | Loss: 0.00002707
Iteration 111/1000 | Loss: 0.00002707
Iteration 112/1000 | Loss: 0.00002707
Iteration 113/1000 | Loss: 0.00002707
Iteration 114/1000 | Loss: 0.00002707
Iteration 115/1000 | Loss: 0.00002707
Iteration 116/1000 | Loss: 0.00002707
Iteration 117/1000 | Loss: 0.00002707
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 117. Stopping optimization.
Last 5 losses: [2.7067804694524966e-05, 2.7067804694524966e-05, 2.7067804694524966e-05, 2.7067804694524966e-05, 2.7067804694524966e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.7067804694524966e-05

Optimization complete. Final v2v error: 4.385108947753906 mm

Highest mean error: 4.772276878356934 mm for frame 2

Lowest mean error: 3.9023327827453613 mm for frame 187

Saving results

Total time: 47.2066695690155
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_025/1049/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_025/1049.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_025/1049
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00870723
Iteration 2/25 | Loss: 0.00157883
Iteration 3/25 | Loss: 0.00128037
Iteration 4/25 | Loss: 0.00122628
Iteration 5/25 | Loss: 0.00122115
Iteration 6/25 | Loss: 0.00121930
Iteration 7/25 | Loss: 0.00121581
Iteration 8/25 | Loss: 0.00121451
Iteration 9/25 | Loss: 0.00121399
Iteration 10/25 | Loss: 0.00121329
Iteration 11/25 | Loss: 0.00121293
Iteration 12/25 | Loss: 0.00121280
Iteration 13/25 | Loss: 0.00121280
Iteration 14/25 | Loss: 0.00121280
Iteration 15/25 | Loss: 0.00121280
Iteration 16/25 | Loss: 0.00121280
Iteration 17/25 | Loss: 0.00121280
Iteration 18/25 | Loss: 0.00121280
Iteration 19/25 | Loss: 0.00121279
Iteration 20/25 | Loss: 0.00121279
Iteration 21/25 | Loss: 0.00121279
Iteration 22/25 | Loss: 0.00121279
Iteration 23/25 | Loss: 0.00121279
Iteration 24/25 | Loss: 0.00121279
Iteration 25/25 | Loss: 0.00121279

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.98805392
Iteration 2/25 | Loss: 0.00142515
Iteration 3/25 | Loss: 0.00139976
Iteration 4/25 | Loss: 0.00139976
Iteration 5/25 | Loss: 0.00139976
Iteration 6/25 | Loss: 0.00139976
Iteration 7/25 | Loss: 0.00139976
Iteration 8/25 | Loss: 0.00139976
Iteration 9/25 | Loss: 0.00139976
Iteration 10/25 | Loss: 0.00139976
Iteration 11/25 | Loss: 0.00139976
Iteration 12/25 | Loss: 0.00139976
Iteration 13/25 | Loss: 0.00139976
Iteration 14/25 | Loss: 0.00139976
Iteration 15/25 | Loss: 0.00139976
Iteration 16/25 | Loss: 0.00139976
Iteration 17/25 | Loss: 0.00139976
Iteration 18/25 | Loss: 0.00139976
Iteration 19/25 | Loss: 0.00139976
Iteration 20/25 | Loss: 0.00139976
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0013997550122439861, 0.0013997550122439861, 0.0013997550122439861, 0.0013997550122439861, 0.0013997550122439861]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013997550122439861

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00139976
Iteration 2/1000 | Loss: 0.00005068
Iteration 3/1000 | Loss: 0.00004506
Iteration 4/1000 | Loss: 0.00006194
Iteration 5/1000 | Loss: 0.00001967
Iteration 6/1000 | Loss: 0.00021632
Iteration 7/1000 | Loss: 0.00001780
Iteration 8/1000 | Loss: 0.00009917
Iteration 9/1000 | Loss: 0.00001340
Iteration 10/1000 | Loss: 0.00003551
Iteration 11/1000 | Loss: 0.00001283
Iteration 12/1000 | Loss: 0.00001248
Iteration 13/1000 | Loss: 0.00003620
Iteration 14/1000 | Loss: 0.00001208
Iteration 15/1000 | Loss: 0.00001180
Iteration 16/1000 | Loss: 0.00007303
Iteration 17/1000 | Loss: 0.00001496
Iteration 18/1000 | Loss: 0.00001372
Iteration 19/1000 | Loss: 0.00001154
Iteration 20/1000 | Loss: 0.00001143
Iteration 21/1000 | Loss: 0.00001139
Iteration 22/1000 | Loss: 0.00001138
Iteration 23/1000 | Loss: 0.00001124
Iteration 24/1000 | Loss: 0.00001119
Iteration 25/1000 | Loss: 0.00001116
Iteration 26/1000 | Loss: 0.00001116
Iteration 27/1000 | Loss: 0.00001115
Iteration 28/1000 | Loss: 0.00001115
Iteration 29/1000 | Loss: 0.00003723
Iteration 30/1000 | Loss: 0.00004037
Iteration 31/1000 | Loss: 0.00003658
Iteration 32/1000 | Loss: 0.00001248
Iteration 33/1000 | Loss: 0.00001107
Iteration 34/1000 | Loss: 0.00001106
Iteration 35/1000 | Loss: 0.00001105
Iteration 36/1000 | Loss: 0.00001105
Iteration 37/1000 | Loss: 0.00001105
Iteration 38/1000 | Loss: 0.00001105
Iteration 39/1000 | Loss: 0.00001105
Iteration 40/1000 | Loss: 0.00001105
Iteration 41/1000 | Loss: 0.00001105
Iteration 42/1000 | Loss: 0.00001105
Iteration 43/1000 | Loss: 0.00001101
Iteration 44/1000 | Loss: 0.00001101
Iteration 45/1000 | Loss: 0.00001101
Iteration 46/1000 | Loss: 0.00001100
Iteration 47/1000 | Loss: 0.00001100
Iteration 48/1000 | Loss: 0.00001099
Iteration 49/1000 | Loss: 0.00001099
Iteration 50/1000 | Loss: 0.00001099
Iteration 51/1000 | Loss: 0.00001569
Iteration 52/1000 | Loss: 0.00001096
Iteration 53/1000 | Loss: 0.00001096
Iteration 54/1000 | Loss: 0.00001096
Iteration 55/1000 | Loss: 0.00001096
Iteration 56/1000 | Loss: 0.00001096
Iteration 57/1000 | Loss: 0.00001096
Iteration 58/1000 | Loss: 0.00001096
Iteration 59/1000 | Loss: 0.00001096
Iteration 60/1000 | Loss: 0.00001096
Iteration 61/1000 | Loss: 0.00001096
Iteration 62/1000 | Loss: 0.00001095
Iteration 63/1000 | Loss: 0.00001093
Iteration 64/1000 | Loss: 0.00001093
Iteration 65/1000 | Loss: 0.00001092
Iteration 66/1000 | Loss: 0.00001092
Iteration 67/1000 | Loss: 0.00001092
Iteration 68/1000 | Loss: 0.00001092
Iteration 69/1000 | Loss: 0.00001092
Iteration 70/1000 | Loss: 0.00001092
Iteration 71/1000 | Loss: 0.00001091
Iteration 72/1000 | Loss: 0.00001091
Iteration 73/1000 | Loss: 0.00001091
Iteration 74/1000 | Loss: 0.00001090
Iteration 75/1000 | Loss: 0.00001089
Iteration 76/1000 | Loss: 0.00001089
Iteration 77/1000 | Loss: 0.00001089
Iteration 78/1000 | Loss: 0.00001088
Iteration 79/1000 | Loss: 0.00001088
Iteration 80/1000 | Loss: 0.00001088
Iteration 81/1000 | Loss: 0.00001088
Iteration 82/1000 | Loss: 0.00001088
Iteration 83/1000 | Loss: 0.00001088
Iteration 84/1000 | Loss: 0.00001088
Iteration 85/1000 | Loss: 0.00001088
Iteration 86/1000 | Loss: 0.00001087
Iteration 87/1000 | Loss: 0.00001087
Iteration 88/1000 | Loss: 0.00001087
Iteration 89/1000 | Loss: 0.00001087
Iteration 90/1000 | Loss: 0.00001086
Iteration 91/1000 | Loss: 0.00001086
Iteration 92/1000 | Loss: 0.00001086
Iteration 93/1000 | Loss: 0.00001086
Iteration 94/1000 | Loss: 0.00001085
Iteration 95/1000 | Loss: 0.00001085
Iteration 96/1000 | Loss: 0.00001085
Iteration 97/1000 | Loss: 0.00001085
Iteration 98/1000 | Loss: 0.00001085
Iteration 99/1000 | Loss: 0.00001085
Iteration 100/1000 | Loss: 0.00001085
Iteration 101/1000 | Loss: 0.00001085
Iteration 102/1000 | Loss: 0.00001084
Iteration 103/1000 | Loss: 0.00001084
Iteration 104/1000 | Loss: 0.00001084
Iteration 105/1000 | Loss: 0.00001083
Iteration 106/1000 | Loss: 0.00001083
Iteration 107/1000 | Loss: 0.00001083
Iteration 108/1000 | Loss: 0.00001083
Iteration 109/1000 | Loss: 0.00001083
Iteration 110/1000 | Loss: 0.00001083
Iteration 111/1000 | Loss: 0.00001083
Iteration 112/1000 | Loss: 0.00001083
Iteration 113/1000 | Loss: 0.00001083
Iteration 114/1000 | Loss: 0.00001083
Iteration 115/1000 | Loss: 0.00001083
Iteration 116/1000 | Loss: 0.00001083
Iteration 117/1000 | Loss: 0.00001083
Iteration 118/1000 | Loss: 0.00001083
Iteration 119/1000 | Loss: 0.00001083
Iteration 120/1000 | Loss: 0.00001083
Iteration 121/1000 | Loss: 0.00001083
Iteration 122/1000 | Loss: 0.00001083
Iteration 123/1000 | Loss: 0.00001083
Iteration 124/1000 | Loss: 0.00001083
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 124. Stopping optimization.
Last 5 losses: [1.082592825696338e-05, 1.082592825696338e-05, 1.082592825696338e-05, 1.082592825696338e-05, 1.082592825696338e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.082592825696338e-05

Optimization complete. Final v2v error: 2.8502345085144043 mm

Highest mean error: 3.2240164279937744 mm for frame 38

Lowest mean error: 2.608273506164551 mm for frame 224

Saving results

Total time: 73.47029876708984
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_025/1068/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_025/1068.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_025/1068
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00927123
Iteration 2/25 | Loss: 0.00184957
Iteration 3/25 | Loss: 0.00163930
Iteration 4/25 | Loss: 0.00158890
Iteration 5/25 | Loss: 0.00155181
Iteration 6/25 | Loss: 0.00154685
Iteration 7/25 | Loss: 0.00152312
Iteration 8/25 | Loss: 0.00152694
Iteration 9/25 | Loss: 0.00151889
Iteration 10/25 | Loss: 0.00151363
Iteration 11/25 | Loss: 0.00151126
Iteration 12/25 | Loss: 0.00150997
Iteration 13/25 | Loss: 0.00150802
Iteration 14/25 | Loss: 0.00150228
Iteration 15/25 | Loss: 0.00150190
Iteration 16/25 | Loss: 0.00150168
Iteration 17/25 | Loss: 0.00150160
Iteration 18/25 | Loss: 0.00150158
Iteration 19/25 | Loss: 0.00150158
Iteration 20/25 | Loss: 0.00150158
Iteration 21/25 | Loss: 0.00150158
Iteration 22/25 | Loss: 0.00150158
Iteration 23/25 | Loss: 0.00150158
Iteration 24/25 | Loss: 0.00150158
Iteration 25/25 | Loss: 0.00150158

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.81889999
Iteration 2/25 | Loss: 0.00291863
Iteration 3/25 | Loss: 0.00291863
Iteration 4/25 | Loss: 0.00263584
Iteration 5/25 | Loss: 0.00263581
Iteration 6/25 | Loss: 0.00263581
Iteration 7/25 | Loss: 0.00263581
Iteration 8/25 | Loss: 0.00263581
Iteration 9/25 | Loss: 0.00263581
Iteration 10/25 | Loss: 0.00263581
Iteration 11/25 | Loss: 0.00263581
Iteration 12/25 | Loss: 0.00263581
Iteration 13/25 | Loss: 0.00263581
Iteration 14/25 | Loss: 0.00263581
Iteration 15/25 | Loss: 0.00263581
Iteration 16/25 | Loss: 0.00263581
Iteration 17/25 | Loss: 0.00263581
Iteration 18/25 | Loss: 0.00263581
Iteration 19/25 | Loss: 0.00263581
Iteration 20/25 | Loss: 0.00263581
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0026358107570558786, 0.0026358107570558786, 0.0026358107570558786, 0.0026358107570558786, 0.0026358107570558786]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0026358107570558786

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00263581
Iteration 2/1000 | Loss: 0.00027365
Iteration 3/1000 | Loss: 0.00114117
Iteration 4/1000 | Loss: 0.00091936
Iteration 5/1000 | Loss: 0.00013759
Iteration 6/1000 | Loss: 0.00081342
Iteration 7/1000 | Loss: 0.00043352
Iteration 8/1000 | Loss: 0.00042595
Iteration 9/1000 | Loss: 0.00016095
Iteration 10/1000 | Loss: 0.00026286
Iteration 11/1000 | Loss: 0.00008437
Iteration 12/1000 | Loss: 0.00012061
Iteration 13/1000 | Loss: 0.00006426
Iteration 14/1000 | Loss: 0.00005906
Iteration 15/1000 | Loss: 0.00005550
Iteration 16/1000 | Loss: 0.00017001
Iteration 17/1000 | Loss: 0.00067360
Iteration 18/1000 | Loss: 0.00076988
Iteration 19/1000 | Loss: 0.00126770
Iteration 20/1000 | Loss: 0.00087966
Iteration 21/1000 | Loss: 0.00052905
Iteration 22/1000 | Loss: 0.00100037
Iteration 23/1000 | Loss: 0.00047957
Iteration 24/1000 | Loss: 0.00098141
Iteration 25/1000 | Loss: 0.00029735
Iteration 26/1000 | Loss: 0.00037388
Iteration 27/1000 | Loss: 0.00005645
Iteration 28/1000 | Loss: 0.00004929
Iteration 29/1000 | Loss: 0.00048009
Iteration 30/1000 | Loss: 0.00033517
Iteration 31/1000 | Loss: 0.00032809
Iteration 32/1000 | Loss: 0.00018769
Iteration 33/1000 | Loss: 0.00021413
Iteration 34/1000 | Loss: 0.00061032
Iteration 35/1000 | Loss: 0.00005545
Iteration 36/1000 | Loss: 0.00004381
Iteration 37/1000 | Loss: 0.00003810
Iteration 38/1000 | Loss: 0.00003579
Iteration 39/1000 | Loss: 0.00029479
Iteration 40/1000 | Loss: 0.00003896
Iteration 41/1000 | Loss: 0.00003469
Iteration 42/1000 | Loss: 0.00003256
Iteration 43/1000 | Loss: 0.00003169
Iteration 44/1000 | Loss: 0.00003111
Iteration 45/1000 | Loss: 0.00003069
Iteration 46/1000 | Loss: 0.00003026
Iteration 47/1000 | Loss: 0.00002994
Iteration 48/1000 | Loss: 0.00002968
Iteration 49/1000 | Loss: 0.00002938
Iteration 50/1000 | Loss: 0.00002929
Iteration 51/1000 | Loss: 0.00002925
Iteration 52/1000 | Loss: 0.00002923
Iteration 53/1000 | Loss: 0.00002921
Iteration 54/1000 | Loss: 0.00002920
Iteration 55/1000 | Loss: 0.00002919
Iteration 56/1000 | Loss: 0.00002919
Iteration 57/1000 | Loss: 0.00002919
Iteration 58/1000 | Loss: 0.00002919
Iteration 59/1000 | Loss: 0.00002919
Iteration 60/1000 | Loss: 0.00002918
Iteration 61/1000 | Loss: 0.00002914
Iteration 62/1000 | Loss: 0.00002912
Iteration 63/1000 | Loss: 0.00002910
Iteration 64/1000 | Loss: 0.00002909
Iteration 65/1000 | Loss: 0.00002907
Iteration 66/1000 | Loss: 0.00002907
Iteration 67/1000 | Loss: 0.00002905
Iteration 68/1000 | Loss: 0.00002904
Iteration 69/1000 | Loss: 0.00002903
Iteration 70/1000 | Loss: 0.00002896
Iteration 71/1000 | Loss: 0.00002894
Iteration 72/1000 | Loss: 0.00002893
Iteration 73/1000 | Loss: 0.00002893
Iteration 74/1000 | Loss: 0.00002893
Iteration 75/1000 | Loss: 0.00002892
Iteration 76/1000 | Loss: 0.00002892
Iteration 77/1000 | Loss: 0.00002892
Iteration 78/1000 | Loss: 0.00002891
Iteration 79/1000 | Loss: 0.00002891
Iteration 80/1000 | Loss: 0.00002891
Iteration 81/1000 | Loss: 0.00002890
Iteration 82/1000 | Loss: 0.00002890
Iteration 83/1000 | Loss: 0.00002890
Iteration 84/1000 | Loss: 0.00002890
Iteration 85/1000 | Loss: 0.00002889
Iteration 86/1000 | Loss: 0.00002889
Iteration 87/1000 | Loss: 0.00002889
Iteration 88/1000 | Loss: 0.00002888
Iteration 89/1000 | Loss: 0.00002888
Iteration 90/1000 | Loss: 0.00002888
Iteration 91/1000 | Loss: 0.00002887
Iteration 92/1000 | Loss: 0.00002887
Iteration 93/1000 | Loss: 0.00002886
Iteration 94/1000 | Loss: 0.00002886
Iteration 95/1000 | Loss: 0.00002883
Iteration 96/1000 | Loss: 0.00002883
Iteration 97/1000 | Loss: 0.00002881
Iteration 98/1000 | Loss: 0.00002881
Iteration 99/1000 | Loss: 0.00002880
Iteration 100/1000 | Loss: 0.00002880
Iteration 101/1000 | Loss: 0.00002879
Iteration 102/1000 | Loss: 0.00002878
Iteration 103/1000 | Loss: 0.00002875
Iteration 104/1000 | Loss: 0.00002874
Iteration 105/1000 | Loss: 0.00002873
Iteration 106/1000 | Loss: 0.00002873
Iteration 107/1000 | Loss: 0.00002873
Iteration 108/1000 | Loss: 0.00002872
Iteration 109/1000 | Loss: 0.00002872
Iteration 110/1000 | Loss: 0.00002871
Iteration 111/1000 | Loss: 0.00002871
Iteration 112/1000 | Loss: 0.00002871
Iteration 113/1000 | Loss: 0.00002871
Iteration 114/1000 | Loss: 0.00002871
Iteration 115/1000 | Loss: 0.00002871
Iteration 116/1000 | Loss: 0.00002870
Iteration 117/1000 | Loss: 0.00002870
Iteration 118/1000 | Loss: 0.00002869
Iteration 119/1000 | Loss: 0.00002869
Iteration 120/1000 | Loss: 0.00002869
Iteration 121/1000 | Loss: 0.00002869
Iteration 122/1000 | Loss: 0.00002868
Iteration 123/1000 | Loss: 0.00002868
Iteration 124/1000 | Loss: 0.00002868
Iteration 125/1000 | Loss: 0.00002867
Iteration 126/1000 | Loss: 0.00002867
Iteration 127/1000 | Loss: 0.00002867
Iteration 128/1000 | Loss: 0.00002867
Iteration 129/1000 | Loss: 0.00002866
Iteration 130/1000 | Loss: 0.00002865
Iteration 131/1000 | Loss: 0.00002865
Iteration 132/1000 | Loss: 0.00002865
Iteration 133/1000 | Loss: 0.00002864
Iteration 134/1000 | Loss: 0.00002864
Iteration 135/1000 | Loss: 0.00002864
Iteration 136/1000 | Loss: 0.00002864
Iteration 137/1000 | Loss: 0.00002864
Iteration 138/1000 | Loss: 0.00002863
Iteration 139/1000 | Loss: 0.00002863
Iteration 140/1000 | Loss: 0.00002862
Iteration 141/1000 | Loss: 0.00002862
Iteration 142/1000 | Loss: 0.00002862
Iteration 143/1000 | Loss: 0.00002862
Iteration 144/1000 | Loss: 0.00002861
Iteration 145/1000 | Loss: 0.00002861
Iteration 146/1000 | Loss: 0.00002861
Iteration 147/1000 | Loss: 0.00002860
Iteration 148/1000 | Loss: 0.00002860
Iteration 149/1000 | Loss: 0.00002860
Iteration 150/1000 | Loss: 0.00002860
Iteration 151/1000 | Loss: 0.00002860
Iteration 152/1000 | Loss: 0.00002859
Iteration 153/1000 | Loss: 0.00002859
Iteration 154/1000 | Loss: 0.00002859
Iteration 155/1000 | Loss: 0.00002859
Iteration 156/1000 | Loss: 0.00002859
Iteration 157/1000 | Loss: 0.00002859
Iteration 158/1000 | Loss: 0.00002859
Iteration 159/1000 | Loss: 0.00002859
Iteration 160/1000 | Loss: 0.00002859
Iteration 161/1000 | Loss: 0.00002859
Iteration 162/1000 | Loss: 0.00002859
Iteration 163/1000 | Loss: 0.00002859
Iteration 164/1000 | Loss: 0.00002858
Iteration 165/1000 | Loss: 0.00002858
Iteration 166/1000 | Loss: 0.00002858
Iteration 167/1000 | Loss: 0.00002858
Iteration 168/1000 | Loss: 0.00002857
Iteration 169/1000 | Loss: 0.00002857
Iteration 170/1000 | Loss: 0.00002857
Iteration 171/1000 | Loss: 0.00002857
Iteration 172/1000 | Loss: 0.00002856
Iteration 173/1000 | Loss: 0.00002856
Iteration 174/1000 | Loss: 0.00002856
Iteration 175/1000 | Loss: 0.00002856
Iteration 176/1000 | Loss: 0.00002855
Iteration 177/1000 | Loss: 0.00002855
Iteration 178/1000 | Loss: 0.00002855
Iteration 179/1000 | Loss: 0.00002855
Iteration 180/1000 | Loss: 0.00002855
Iteration 181/1000 | Loss: 0.00002855
Iteration 182/1000 | Loss: 0.00002855
Iteration 183/1000 | Loss: 0.00002855
Iteration 184/1000 | Loss: 0.00002855
Iteration 185/1000 | Loss: 0.00002855
Iteration 186/1000 | Loss: 0.00002854
Iteration 187/1000 | Loss: 0.00002854
Iteration 188/1000 | Loss: 0.00002854
Iteration 189/1000 | Loss: 0.00002854
Iteration 190/1000 | Loss: 0.00002854
Iteration 191/1000 | Loss: 0.00002853
Iteration 192/1000 | Loss: 0.00002853
Iteration 193/1000 | Loss: 0.00002853
Iteration 194/1000 | Loss: 0.00002853
Iteration 195/1000 | Loss: 0.00002853
Iteration 196/1000 | Loss: 0.00002853
Iteration 197/1000 | Loss: 0.00002852
Iteration 198/1000 | Loss: 0.00002852
Iteration 199/1000 | Loss: 0.00002852
Iteration 200/1000 | Loss: 0.00002852
Iteration 201/1000 | Loss: 0.00002852
Iteration 202/1000 | Loss: 0.00002852
Iteration 203/1000 | Loss: 0.00002852
Iteration 204/1000 | Loss: 0.00002852
Iteration 205/1000 | Loss: 0.00002852
Iteration 206/1000 | Loss: 0.00002851
Iteration 207/1000 | Loss: 0.00002851
Iteration 208/1000 | Loss: 0.00002851
Iteration 209/1000 | Loss: 0.00002851
Iteration 210/1000 | Loss: 0.00002851
Iteration 211/1000 | Loss: 0.00002851
Iteration 212/1000 | Loss: 0.00002851
Iteration 213/1000 | Loss: 0.00002851
Iteration 214/1000 | Loss: 0.00002851
Iteration 215/1000 | Loss: 0.00002851
Iteration 216/1000 | Loss: 0.00002851
Iteration 217/1000 | Loss: 0.00002851
Iteration 218/1000 | Loss: 0.00002851
Iteration 219/1000 | Loss: 0.00002851
Iteration 220/1000 | Loss: 0.00002851
Iteration 221/1000 | Loss: 0.00002850
Iteration 222/1000 | Loss: 0.00002850
Iteration 223/1000 | Loss: 0.00002850
Iteration 224/1000 | Loss: 0.00002850
Iteration 225/1000 | Loss: 0.00002850
Iteration 226/1000 | Loss: 0.00002850
Iteration 227/1000 | Loss: 0.00002850
Iteration 228/1000 | Loss: 0.00002850
Iteration 229/1000 | Loss: 0.00002850
Iteration 230/1000 | Loss: 0.00002850
Iteration 231/1000 | Loss: 0.00002850
Iteration 232/1000 | Loss: 0.00002850
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 232. Stopping optimization.
Last 5 losses: [2.8501688575488515e-05, 2.8501688575488515e-05, 2.8501688575488515e-05, 2.8501688575488515e-05, 2.8501688575488515e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.8501688575488515e-05

Optimization complete. Final v2v error: 4.420986175537109 mm

Highest mean error: 5.25730037689209 mm for frame 105

Lowest mean error: 3.2752764225006104 mm for frame 208

Saving results

Total time: 136.1906979084015
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_025/1079/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_025/1079.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_025/1079
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00437247
Iteration 2/25 | Loss: 0.00130145
Iteration 3/25 | Loss: 0.00124541
Iteration 4/25 | Loss: 0.00123369
Iteration 5/25 | Loss: 0.00123024
Iteration 6/25 | Loss: 0.00122974
Iteration 7/25 | Loss: 0.00122974
Iteration 8/25 | Loss: 0.00122974
Iteration 9/25 | Loss: 0.00122974
Iteration 10/25 | Loss: 0.00122974
Iteration 11/25 | Loss: 0.00122974
Iteration 12/25 | Loss: 0.00122974
Iteration 13/25 | Loss: 0.00122974
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0012297354405745864, 0.0012297354405745864, 0.0012297354405745864, 0.0012297354405745864, 0.0012297354405745864]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012297354405745864

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37274635
Iteration 2/25 | Loss: 0.00153511
Iteration 3/25 | Loss: 0.00153511
Iteration 4/25 | Loss: 0.00153511
Iteration 5/25 | Loss: 0.00153511
Iteration 6/25 | Loss: 0.00153511
Iteration 7/25 | Loss: 0.00153511
Iteration 8/25 | Loss: 0.00153511
Iteration 9/25 | Loss: 0.00153511
Iteration 10/25 | Loss: 0.00153510
Iteration 11/25 | Loss: 0.00153510
Iteration 12/25 | Loss: 0.00153510
Iteration 13/25 | Loss: 0.00153510
Iteration 14/25 | Loss: 0.00153510
Iteration 15/25 | Loss: 0.00153510
Iteration 16/25 | Loss: 0.00153510
Iteration 17/25 | Loss: 0.00153510
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.001535104471258819, 0.001535104471258819, 0.001535104471258819, 0.001535104471258819, 0.001535104471258819]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001535104471258819

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00153510
Iteration 2/1000 | Loss: 0.00002530
Iteration 3/1000 | Loss: 0.00001747
Iteration 4/1000 | Loss: 0.00001617
Iteration 5/1000 | Loss: 0.00001553
Iteration 6/1000 | Loss: 0.00001515
Iteration 7/1000 | Loss: 0.00001460
Iteration 8/1000 | Loss: 0.00001434
Iteration 9/1000 | Loss: 0.00001401
Iteration 10/1000 | Loss: 0.00001372
Iteration 11/1000 | Loss: 0.00001359
Iteration 12/1000 | Loss: 0.00001352
Iteration 13/1000 | Loss: 0.00001337
Iteration 14/1000 | Loss: 0.00001334
Iteration 15/1000 | Loss: 0.00001324
Iteration 16/1000 | Loss: 0.00001313
Iteration 17/1000 | Loss: 0.00001309
Iteration 18/1000 | Loss: 0.00001306
Iteration 19/1000 | Loss: 0.00001305
Iteration 20/1000 | Loss: 0.00001305
Iteration 21/1000 | Loss: 0.00001304
Iteration 22/1000 | Loss: 0.00001304
Iteration 23/1000 | Loss: 0.00001303
Iteration 24/1000 | Loss: 0.00001296
Iteration 25/1000 | Loss: 0.00001293
Iteration 26/1000 | Loss: 0.00001293
Iteration 27/1000 | Loss: 0.00001293
Iteration 28/1000 | Loss: 0.00001292
Iteration 29/1000 | Loss: 0.00001291
Iteration 30/1000 | Loss: 0.00001291
Iteration 31/1000 | Loss: 0.00001290
Iteration 32/1000 | Loss: 0.00001290
Iteration 33/1000 | Loss: 0.00001289
Iteration 34/1000 | Loss: 0.00001289
Iteration 35/1000 | Loss: 0.00001288
Iteration 36/1000 | Loss: 0.00001288
Iteration 37/1000 | Loss: 0.00001288
Iteration 38/1000 | Loss: 0.00001288
Iteration 39/1000 | Loss: 0.00001287
Iteration 40/1000 | Loss: 0.00001287
Iteration 41/1000 | Loss: 0.00001286
Iteration 42/1000 | Loss: 0.00001286
Iteration 43/1000 | Loss: 0.00001285
Iteration 44/1000 | Loss: 0.00001285
Iteration 45/1000 | Loss: 0.00001285
Iteration 46/1000 | Loss: 0.00001285
Iteration 47/1000 | Loss: 0.00001285
Iteration 48/1000 | Loss: 0.00001285
Iteration 49/1000 | Loss: 0.00001285
Iteration 50/1000 | Loss: 0.00001285
Iteration 51/1000 | Loss: 0.00001285
Iteration 52/1000 | Loss: 0.00001285
Iteration 53/1000 | Loss: 0.00001284
Iteration 54/1000 | Loss: 0.00001284
Iteration 55/1000 | Loss: 0.00001284
Iteration 56/1000 | Loss: 0.00001283
Iteration 57/1000 | Loss: 0.00001283
Iteration 58/1000 | Loss: 0.00001282
Iteration 59/1000 | Loss: 0.00001281
Iteration 60/1000 | Loss: 0.00001280
Iteration 61/1000 | Loss: 0.00001280
Iteration 62/1000 | Loss: 0.00001280
Iteration 63/1000 | Loss: 0.00001279
Iteration 64/1000 | Loss: 0.00001279
Iteration 65/1000 | Loss: 0.00001279
Iteration 66/1000 | Loss: 0.00001279
Iteration 67/1000 | Loss: 0.00001278
Iteration 68/1000 | Loss: 0.00001278
Iteration 69/1000 | Loss: 0.00001278
Iteration 70/1000 | Loss: 0.00001278
Iteration 71/1000 | Loss: 0.00001278
Iteration 72/1000 | Loss: 0.00001278
Iteration 73/1000 | Loss: 0.00001277
Iteration 74/1000 | Loss: 0.00001277
Iteration 75/1000 | Loss: 0.00001277
Iteration 76/1000 | Loss: 0.00001277
Iteration 77/1000 | Loss: 0.00001277
Iteration 78/1000 | Loss: 0.00001277
Iteration 79/1000 | Loss: 0.00001276
Iteration 80/1000 | Loss: 0.00001276
Iteration 81/1000 | Loss: 0.00001276
Iteration 82/1000 | Loss: 0.00001276
Iteration 83/1000 | Loss: 0.00001276
Iteration 84/1000 | Loss: 0.00001276
Iteration 85/1000 | Loss: 0.00001275
Iteration 86/1000 | Loss: 0.00001275
Iteration 87/1000 | Loss: 0.00001275
Iteration 88/1000 | Loss: 0.00001275
Iteration 89/1000 | Loss: 0.00001275
Iteration 90/1000 | Loss: 0.00001275
Iteration 91/1000 | Loss: 0.00001275
Iteration 92/1000 | Loss: 0.00001275
Iteration 93/1000 | Loss: 0.00001275
Iteration 94/1000 | Loss: 0.00001275
Iteration 95/1000 | Loss: 0.00001275
Iteration 96/1000 | Loss: 0.00001275
Iteration 97/1000 | Loss: 0.00001274
Iteration 98/1000 | Loss: 0.00001274
Iteration 99/1000 | Loss: 0.00001274
Iteration 100/1000 | Loss: 0.00001274
Iteration 101/1000 | Loss: 0.00001274
Iteration 102/1000 | Loss: 0.00001274
Iteration 103/1000 | Loss: 0.00001274
Iteration 104/1000 | Loss: 0.00001274
Iteration 105/1000 | Loss: 0.00001273
Iteration 106/1000 | Loss: 0.00001273
Iteration 107/1000 | Loss: 0.00001273
Iteration 108/1000 | Loss: 0.00001273
Iteration 109/1000 | Loss: 0.00001273
Iteration 110/1000 | Loss: 0.00001272
Iteration 111/1000 | Loss: 0.00001272
Iteration 112/1000 | Loss: 0.00001272
Iteration 113/1000 | Loss: 0.00001272
Iteration 114/1000 | Loss: 0.00001272
Iteration 115/1000 | Loss: 0.00001272
Iteration 116/1000 | Loss: 0.00001271
Iteration 117/1000 | Loss: 0.00001271
Iteration 118/1000 | Loss: 0.00001271
Iteration 119/1000 | Loss: 0.00001271
Iteration 120/1000 | Loss: 0.00001271
Iteration 121/1000 | Loss: 0.00001271
Iteration 122/1000 | Loss: 0.00001271
Iteration 123/1000 | Loss: 0.00001271
Iteration 124/1000 | Loss: 0.00001271
Iteration 125/1000 | Loss: 0.00001271
Iteration 126/1000 | Loss: 0.00001270
Iteration 127/1000 | Loss: 0.00001270
Iteration 128/1000 | Loss: 0.00001270
Iteration 129/1000 | Loss: 0.00001270
Iteration 130/1000 | Loss: 0.00001270
Iteration 131/1000 | Loss: 0.00001270
Iteration 132/1000 | Loss: 0.00001270
Iteration 133/1000 | Loss: 0.00001270
Iteration 134/1000 | Loss: 0.00001270
Iteration 135/1000 | Loss: 0.00001270
Iteration 136/1000 | Loss: 0.00001270
Iteration 137/1000 | Loss: 0.00001270
Iteration 138/1000 | Loss: 0.00001270
Iteration 139/1000 | Loss: 0.00001270
Iteration 140/1000 | Loss: 0.00001270
Iteration 141/1000 | Loss: 0.00001270
Iteration 142/1000 | Loss: 0.00001269
Iteration 143/1000 | Loss: 0.00001269
Iteration 144/1000 | Loss: 0.00001269
Iteration 145/1000 | Loss: 0.00001269
Iteration 146/1000 | Loss: 0.00001269
Iteration 147/1000 | Loss: 0.00001269
Iteration 148/1000 | Loss: 0.00001269
Iteration 149/1000 | Loss: 0.00001269
Iteration 150/1000 | Loss: 0.00001269
Iteration 151/1000 | Loss: 0.00001269
Iteration 152/1000 | Loss: 0.00001269
Iteration 153/1000 | Loss: 0.00001269
Iteration 154/1000 | Loss: 0.00001269
Iteration 155/1000 | Loss: 0.00001269
Iteration 156/1000 | Loss: 0.00001269
Iteration 157/1000 | Loss: 0.00001269
Iteration 158/1000 | Loss: 0.00001269
Iteration 159/1000 | Loss: 0.00001269
Iteration 160/1000 | Loss: 0.00001269
Iteration 161/1000 | Loss: 0.00001269
Iteration 162/1000 | Loss: 0.00001269
Iteration 163/1000 | Loss: 0.00001269
Iteration 164/1000 | Loss: 0.00001269
Iteration 165/1000 | Loss: 0.00001269
Iteration 166/1000 | Loss: 0.00001269
Iteration 167/1000 | Loss: 0.00001269
Iteration 168/1000 | Loss: 0.00001269
Iteration 169/1000 | Loss: 0.00001269
Iteration 170/1000 | Loss: 0.00001269
Iteration 171/1000 | Loss: 0.00001269
Iteration 172/1000 | Loss: 0.00001269
Iteration 173/1000 | Loss: 0.00001269
Iteration 174/1000 | Loss: 0.00001269
Iteration 175/1000 | Loss: 0.00001269
Iteration 176/1000 | Loss: 0.00001269
Iteration 177/1000 | Loss: 0.00001269
Iteration 178/1000 | Loss: 0.00001269
Iteration 179/1000 | Loss: 0.00001269
Iteration 180/1000 | Loss: 0.00001269
Iteration 181/1000 | Loss: 0.00001269
Iteration 182/1000 | Loss: 0.00001269
Iteration 183/1000 | Loss: 0.00001269
Iteration 184/1000 | Loss: 0.00001269
Iteration 185/1000 | Loss: 0.00001269
Iteration 186/1000 | Loss: 0.00001269
Iteration 187/1000 | Loss: 0.00001269
Iteration 188/1000 | Loss: 0.00001269
Iteration 189/1000 | Loss: 0.00001269
Iteration 190/1000 | Loss: 0.00001269
Iteration 191/1000 | Loss: 0.00001269
Iteration 192/1000 | Loss: 0.00001269
Iteration 193/1000 | Loss: 0.00001269
Iteration 194/1000 | Loss: 0.00001269
Iteration 195/1000 | Loss: 0.00001269
Iteration 196/1000 | Loss: 0.00001269
Iteration 197/1000 | Loss: 0.00001269
Iteration 198/1000 | Loss: 0.00001269
Iteration 199/1000 | Loss: 0.00001269
Iteration 200/1000 | Loss: 0.00001269
Iteration 201/1000 | Loss: 0.00001269
Iteration 202/1000 | Loss: 0.00001269
Iteration 203/1000 | Loss: 0.00001269
Iteration 204/1000 | Loss: 0.00001269
Iteration 205/1000 | Loss: 0.00001269
Iteration 206/1000 | Loss: 0.00001269
Iteration 207/1000 | Loss: 0.00001269
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 207. Stopping optimization.
Last 5 losses: [1.2685683941526804e-05, 1.2685683941526804e-05, 1.2685683941526804e-05, 1.2685683941526804e-05, 1.2685683941526804e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2685683941526804e-05

Optimization complete. Final v2v error: 3.0718367099761963 mm

Highest mean error: 3.560947895050049 mm for frame 78

Lowest mean error: 2.9491021633148193 mm for frame 30

Saving results

Total time: 39.665345668792725
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_025/1020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_025/1020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_025/1020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00522251
Iteration 2/25 | Loss: 0.00138106
Iteration 3/25 | Loss: 0.00127487
Iteration 4/25 | Loss: 0.00126470
Iteration 5/25 | Loss: 0.00126258
Iteration 6/25 | Loss: 0.00126223
Iteration 7/25 | Loss: 0.00126223
Iteration 8/25 | Loss: 0.00126223
Iteration 9/25 | Loss: 0.00126223
Iteration 10/25 | Loss: 0.00126223
Iteration 11/25 | Loss: 0.00126223
Iteration 12/25 | Loss: 0.00126223
Iteration 13/25 | Loss: 0.00126223
Iteration 14/25 | Loss: 0.00126223
Iteration 15/25 | Loss: 0.00126223
Iteration 16/25 | Loss: 0.00126223
Iteration 17/25 | Loss: 0.00126223
Iteration 18/25 | Loss: 0.00126223
Iteration 19/25 | Loss: 0.00126223
Iteration 20/25 | Loss: 0.00126223
Iteration 21/25 | Loss: 0.00126223
Iteration 22/25 | Loss: 0.00126223
Iteration 23/25 | Loss: 0.00126223
Iteration 24/25 | Loss: 0.00126223
Iteration 25/25 | Loss: 0.00126223

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.35298681
Iteration 2/25 | Loss: 0.00125751
Iteration 3/25 | Loss: 0.00125750
Iteration 4/25 | Loss: 0.00125750
Iteration 5/25 | Loss: 0.00125750
Iteration 6/25 | Loss: 0.00125750
Iteration 7/25 | Loss: 0.00125750
Iteration 8/25 | Loss: 0.00125750
Iteration 9/25 | Loss: 0.00125750
Iteration 10/25 | Loss: 0.00125750
Iteration 11/25 | Loss: 0.00125750
Iteration 12/25 | Loss: 0.00125750
Iteration 13/25 | Loss: 0.00125750
Iteration 14/25 | Loss: 0.00125750
Iteration 15/25 | Loss: 0.00125750
Iteration 16/25 | Loss: 0.00125750
Iteration 17/25 | Loss: 0.00125750
Iteration 18/25 | Loss: 0.00125750
Iteration 19/25 | Loss: 0.00125750
Iteration 20/25 | Loss: 0.00125750
Iteration 21/25 | Loss: 0.00125750
Iteration 22/25 | Loss: 0.00125750
Iteration 23/25 | Loss: 0.00125750
Iteration 24/25 | Loss: 0.00125750
Iteration 25/25 | Loss: 0.00125750
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0012574985157698393, 0.0012574985157698393, 0.0012574985157698393, 0.0012574985157698393, 0.0012574985157698393]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012574985157698393

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00125750
Iteration 2/1000 | Loss: 0.00002618
Iteration 3/1000 | Loss: 0.00001922
Iteration 4/1000 | Loss: 0.00001743
Iteration 5/1000 | Loss: 0.00001659
Iteration 6/1000 | Loss: 0.00001600
Iteration 7/1000 | Loss: 0.00001545
Iteration 8/1000 | Loss: 0.00001509
Iteration 9/1000 | Loss: 0.00001465
Iteration 10/1000 | Loss: 0.00001434
Iteration 11/1000 | Loss: 0.00001410
Iteration 12/1000 | Loss: 0.00001404
Iteration 13/1000 | Loss: 0.00001389
Iteration 14/1000 | Loss: 0.00001387
Iteration 15/1000 | Loss: 0.00001378
Iteration 16/1000 | Loss: 0.00001378
Iteration 17/1000 | Loss: 0.00001378
Iteration 18/1000 | Loss: 0.00001365
Iteration 19/1000 | Loss: 0.00001362
Iteration 20/1000 | Loss: 0.00001359
Iteration 21/1000 | Loss: 0.00001359
Iteration 22/1000 | Loss: 0.00001358
Iteration 23/1000 | Loss: 0.00001357
Iteration 24/1000 | Loss: 0.00001357
Iteration 25/1000 | Loss: 0.00001356
Iteration 26/1000 | Loss: 0.00001355
Iteration 27/1000 | Loss: 0.00001354
Iteration 28/1000 | Loss: 0.00001354
Iteration 29/1000 | Loss: 0.00001354
Iteration 30/1000 | Loss: 0.00001353
Iteration 31/1000 | Loss: 0.00001352
Iteration 32/1000 | Loss: 0.00001352
Iteration 33/1000 | Loss: 0.00001350
Iteration 34/1000 | Loss: 0.00001346
Iteration 35/1000 | Loss: 0.00001346
Iteration 36/1000 | Loss: 0.00001344
Iteration 37/1000 | Loss: 0.00001343
Iteration 38/1000 | Loss: 0.00001343
Iteration 39/1000 | Loss: 0.00001343
Iteration 40/1000 | Loss: 0.00001342
Iteration 41/1000 | Loss: 0.00001341
Iteration 42/1000 | Loss: 0.00001340
Iteration 43/1000 | Loss: 0.00001340
Iteration 44/1000 | Loss: 0.00001340
Iteration 45/1000 | Loss: 0.00001340
Iteration 46/1000 | Loss: 0.00001340
Iteration 47/1000 | Loss: 0.00001340
Iteration 48/1000 | Loss: 0.00001340
Iteration 49/1000 | Loss: 0.00001339
Iteration 50/1000 | Loss: 0.00001339
Iteration 51/1000 | Loss: 0.00001339
Iteration 52/1000 | Loss: 0.00001335
Iteration 53/1000 | Loss: 0.00001335
Iteration 54/1000 | Loss: 0.00001335
Iteration 55/1000 | Loss: 0.00001334
Iteration 56/1000 | Loss: 0.00001332
Iteration 57/1000 | Loss: 0.00001332
Iteration 58/1000 | Loss: 0.00001332
Iteration 59/1000 | Loss: 0.00001332
Iteration 60/1000 | Loss: 0.00001332
Iteration 61/1000 | Loss: 0.00001332
Iteration 62/1000 | Loss: 0.00001331
Iteration 63/1000 | Loss: 0.00001331
Iteration 64/1000 | Loss: 0.00001331
Iteration 65/1000 | Loss: 0.00001331
Iteration 66/1000 | Loss: 0.00001331
Iteration 67/1000 | Loss: 0.00001331
Iteration 68/1000 | Loss: 0.00001330
Iteration 69/1000 | Loss: 0.00001330
Iteration 70/1000 | Loss: 0.00001330
Iteration 71/1000 | Loss: 0.00001330
Iteration 72/1000 | Loss: 0.00001330
Iteration 73/1000 | Loss: 0.00001330
Iteration 74/1000 | Loss: 0.00001329
Iteration 75/1000 | Loss: 0.00001329
Iteration 76/1000 | Loss: 0.00001328
Iteration 77/1000 | Loss: 0.00001328
Iteration 78/1000 | Loss: 0.00001328
Iteration 79/1000 | Loss: 0.00001327
Iteration 80/1000 | Loss: 0.00001327
Iteration 81/1000 | Loss: 0.00001327
Iteration 82/1000 | Loss: 0.00001326
Iteration 83/1000 | Loss: 0.00001326
Iteration 84/1000 | Loss: 0.00001326
Iteration 85/1000 | Loss: 0.00001326
Iteration 86/1000 | Loss: 0.00001326
Iteration 87/1000 | Loss: 0.00001325
Iteration 88/1000 | Loss: 0.00001325
Iteration 89/1000 | Loss: 0.00001325
Iteration 90/1000 | Loss: 0.00001325
Iteration 91/1000 | Loss: 0.00001325
Iteration 92/1000 | Loss: 0.00001325
Iteration 93/1000 | Loss: 0.00001325
Iteration 94/1000 | Loss: 0.00001325
Iteration 95/1000 | Loss: 0.00001325
Iteration 96/1000 | Loss: 0.00001325
Iteration 97/1000 | Loss: 0.00001325
Iteration 98/1000 | Loss: 0.00001325
Iteration 99/1000 | Loss: 0.00001325
Iteration 100/1000 | Loss: 0.00001325
Iteration 101/1000 | Loss: 0.00001325
Iteration 102/1000 | Loss: 0.00001325
Iteration 103/1000 | Loss: 0.00001325
Iteration 104/1000 | Loss: 0.00001325
Iteration 105/1000 | Loss: 0.00001324
Iteration 106/1000 | Loss: 0.00001324
Iteration 107/1000 | Loss: 0.00001324
Iteration 108/1000 | Loss: 0.00001324
Iteration 109/1000 | Loss: 0.00001324
Iteration 110/1000 | Loss: 0.00001324
Iteration 111/1000 | Loss: 0.00001324
Iteration 112/1000 | Loss: 0.00001324
Iteration 113/1000 | Loss: 0.00001324
Iteration 114/1000 | Loss: 0.00001324
Iteration 115/1000 | Loss: 0.00001324
Iteration 116/1000 | Loss: 0.00001324
Iteration 117/1000 | Loss: 0.00001324
Iteration 118/1000 | Loss: 0.00001324
Iteration 119/1000 | Loss: 0.00001323
Iteration 120/1000 | Loss: 0.00001323
Iteration 121/1000 | Loss: 0.00001323
Iteration 122/1000 | Loss: 0.00001323
Iteration 123/1000 | Loss: 0.00001323
Iteration 124/1000 | Loss: 0.00001323
Iteration 125/1000 | Loss: 0.00001323
Iteration 126/1000 | Loss: 0.00001323
Iteration 127/1000 | Loss: 0.00001323
Iteration 128/1000 | Loss: 0.00001323
Iteration 129/1000 | Loss: 0.00001323
Iteration 130/1000 | Loss: 0.00001323
Iteration 131/1000 | Loss: 0.00001323
Iteration 132/1000 | Loss: 0.00001323
Iteration 133/1000 | Loss: 0.00001323
Iteration 134/1000 | Loss: 0.00001323
Iteration 135/1000 | Loss: 0.00001323
Iteration 136/1000 | Loss: 0.00001323
Iteration 137/1000 | Loss: 0.00001323
Iteration 138/1000 | Loss: 0.00001323
Iteration 139/1000 | Loss: 0.00001323
Iteration 140/1000 | Loss: 0.00001323
Iteration 141/1000 | Loss: 0.00001322
Iteration 142/1000 | Loss: 0.00001322
Iteration 143/1000 | Loss: 0.00001322
Iteration 144/1000 | Loss: 0.00001322
Iteration 145/1000 | Loss: 0.00001322
Iteration 146/1000 | Loss: 0.00001322
Iteration 147/1000 | Loss: 0.00001322
Iteration 148/1000 | Loss: 0.00001322
Iteration 149/1000 | Loss: 0.00001322
Iteration 150/1000 | Loss: 0.00001322
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 150. Stopping optimization.
Last 5 losses: [1.322360822086921e-05, 1.322360822086921e-05, 1.322360822086921e-05, 1.322360822086921e-05, 1.322360822086921e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.322360822086921e-05

Optimization complete. Final v2v error: 3.0620057582855225 mm

Highest mean error: 3.698516607284546 mm for frame 68

Lowest mean error: 2.6807937622070312 mm for frame 28

Saving results

Total time: 39.64980363845825
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_025/1090/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_025/1090.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_025/1090
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00815305
Iteration 2/25 | Loss: 0.00131754
Iteration 3/25 | Loss: 0.00121624
Iteration 4/25 | Loss: 0.00120266
Iteration 5/25 | Loss: 0.00120072
Iteration 6/25 | Loss: 0.00120072
Iteration 7/25 | Loss: 0.00120072
Iteration 8/25 | Loss: 0.00120072
Iteration 9/25 | Loss: 0.00120072
Iteration 10/25 | Loss: 0.00120072
Iteration 11/25 | Loss: 0.00120072
Iteration 12/25 | Loss: 0.00120072
Iteration 13/25 | Loss: 0.00120072
Iteration 14/25 | Loss: 0.00120072
Iteration 15/25 | Loss: 0.00120072
Iteration 16/25 | Loss: 0.00120072
Iteration 17/25 | Loss: 0.00120072
Iteration 18/25 | Loss: 0.00120072
Iteration 19/25 | Loss: 0.00120072
Iteration 20/25 | Loss: 0.00120072
Iteration 21/25 | Loss: 0.00120072
Iteration 22/25 | Loss: 0.00120072
Iteration 23/25 | Loss: 0.00120072
Iteration 24/25 | Loss: 0.00120072
Iteration 25/25 | Loss: 0.00120072

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.28320515
Iteration 2/25 | Loss: 0.00140472
Iteration 3/25 | Loss: 0.00140472
Iteration 4/25 | Loss: 0.00140472
Iteration 5/25 | Loss: 0.00140472
Iteration 6/25 | Loss: 0.00140472
Iteration 7/25 | Loss: 0.00140472
Iteration 8/25 | Loss: 0.00140472
Iteration 9/25 | Loss: 0.00140472
Iteration 10/25 | Loss: 0.00140472
Iteration 11/25 | Loss: 0.00140472
Iteration 12/25 | Loss: 0.00140472
Iteration 13/25 | Loss: 0.00140472
Iteration 14/25 | Loss: 0.00140472
Iteration 15/25 | Loss: 0.00140472
Iteration 16/25 | Loss: 0.00140472
Iteration 17/25 | Loss: 0.00140472
Iteration 18/25 | Loss: 0.00140472
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0014047159347683191, 0.0014047159347683191, 0.0014047159347683191, 0.0014047159347683191, 0.0014047159347683191]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014047159347683191

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00140472
Iteration 2/1000 | Loss: 0.00001949
Iteration 3/1000 | Loss: 0.00001456
Iteration 4/1000 | Loss: 0.00001327
Iteration 5/1000 | Loss: 0.00001237
Iteration 6/1000 | Loss: 0.00001171
Iteration 7/1000 | Loss: 0.00001132
Iteration 8/1000 | Loss: 0.00001104
Iteration 9/1000 | Loss: 0.00001067
Iteration 10/1000 | Loss: 0.00001063
Iteration 11/1000 | Loss: 0.00001050
Iteration 12/1000 | Loss: 0.00001048
Iteration 13/1000 | Loss: 0.00001039
Iteration 14/1000 | Loss: 0.00001036
Iteration 15/1000 | Loss: 0.00001036
Iteration 16/1000 | Loss: 0.00001035
Iteration 17/1000 | Loss: 0.00001027
Iteration 18/1000 | Loss: 0.00001027
Iteration 19/1000 | Loss: 0.00001019
Iteration 20/1000 | Loss: 0.00001011
Iteration 21/1000 | Loss: 0.00001005
Iteration 22/1000 | Loss: 0.00000993
Iteration 23/1000 | Loss: 0.00000984
Iteration 24/1000 | Loss: 0.00000983
Iteration 25/1000 | Loss: 0.00000975
Iteration 26/1000 | Loss: 0.00000974
Iteration 27/1000 | Loss: 0.00000973
Iteration 28/1000 | Loss: 0.00000973
Iteration 29/1000 | Loss: 0.00000972
Iteration 30/1000 | Loss: 0.00000971
Iteration 31/1000 | Loss: 0.00000969
Iteration 32/1000 | Loss: 0.00000969
Iteration 33/1000 | Loss: 0.00000969
Iteration 34/1000 | Loss: 0.00000968
Iteration 35/1000 | Loss: 0.00000967
Iteration 36/1000 | Loss: 0.00000967
Iteration 37/1000 | Loss: 0.00000966
Iteration 38/1000 | Loss: 0.00000965
Iteration 39/1000 | Loss: 0.00000965
Iteration 40/1000 | Loss: 0.00000964
Iteration 41/1000 | Loss: 0.00000963
Iteration 42/1000 | Loss: 0.00000963
Iteration 43/1000 | Loss: 0.00000962
Iteration 44/1000 | Loss: 0.00000961
Iteration 45/1000 | Loss: 0.00000960
Iteration 46/1000 | Loss: 0.00000960
Iteration 47/1000 | Loss: 0.00000960
Iteration 48/1000 | Loss: 0.00000960
Iteration 49/1000 | Loss: 0.00000959
Iteration 50/1000 | Loss: 0.00000959
Iteration 51/1000 | Loss: 0.00000959
Iteration 52/1000 | Loss: 0.00000959
Iteration 53/1000 | Loss: 0.00000958
Iteration 54/1000 | Loss: 0.00000957
Iteration 55/1000 | Loss: 0.00000954
Iteration 56/1000 | Loss: 0.00000954
Iteration 57/1000 | Loss: 0.00000953
Iteration 58/1000 | Loss: 0.00000953
Iteration 59/1000 | Loss: 0.00000950
Iteration 60/1000 | Loss: 0.00000950
Iteration 61/1000 | Loss: 0.00000949
Iteration 62/1000 | Loss: 0.00000948
Iteration 63/1000 | Loss: 0.00000948
Iteration 64/1000 | Loss: 0.00000947
Iteration 65/1000 | Loss: 0.00000947
Iteration 66/1000 | Loss: 0.00000946
Iteration 67/1000 | Loss: 0.00000945
Iteration 68/1000 | Loss: 0.00000945
Iteration 69/1000 | Loss: 0.00000944
Iteration 70/1000 | Loss: 0.00000943
Iteration 71/1000 | Loss: 0.00000943
Iteration 72/1000 | Loss: 0.00000943
Iteration 73/1000 | Loss: 0.00000942
Iteration 74/1000 | Loss: 0.00000942
Iteration 75/1000 | Loss: 0.00000942
Iteration 76/1000 | Loss: 0.00000941
Iteration 77/1000 | Loss: 0.00000941
Iteration 78/1000 | Loss: 0.00000941
Iteration 79/1000 | Loss: 0.00000941
Iteration 80/1000 | Loss: 0.00000941
Iteration 81/1000 | Loss: 0.00000941
Iteration 82/1000 | Loss: 0.00000940
Iteration 83/1000 | Loss: 0.00000940
Iteration 84/1000 | Loss: 0.00000940
Iteration 85/1000 | Loss: 0.00000940
Iteration 86/1000 | Loss: 0.00000940
Iteration 87/1000 | Loss: 0.00000940
Iteration 88/1000 | Loss: 0.00000940
Iteration 89/1000 | Loss: 0.00000940
Iteration 90/1000 | Loss: 0.00000939
Iteration 91/1000 | Loss: 0.00000939
Iteration 92/1000 | Loss: 0.00000939
Iteration 93/1000 | Loss: 0.00000939
Iteration 94/1000 | Loss: 0.00000939
Iteration 95/1000 | Loss: 0.00000938
Iteration 96/1000 | Loss: 0.00000938
Iteration 97/1000 | Loss: 0.00000938
Iteration 98/1000 | Loss: 0.00000938
Iteration 99/1000 | Loss: 0.00000938
Iteration 100/1000 | Loss: 0.00000938
Iteration 101/1000 | Loss: 0.00000938
Iteration 102/1000 | Loss: 0.00000938
Iteration 103/1000 | Loss: 0.00000937
Iteration 104/1000 | Loss: 0.00000937
Iteration 105/1000 | Loss: 0.00000937
Iteration 106/1000 | Loss: 0.00000937
Iteration 107/1000 | Loss: 0.00000937
Iteration 108/1000 | Loss: 0.00000937
Iteration 109/1000 | Loss: 0.00000937
Iteration 110/1000 | Loss: 0.00000937
Iteration 111/1000 | Loss: 0.00000936
Iteration 112/1000 | Loss: 0.00000936
Iteration 113/1000 | Loss: 0.00000936
Iteration 114/1000 | Loss: 0.00000936
Iteration 115/1000 | Loss: 0.00000935
Iteration 116/1000 | Loss: 0.00000935
Iteration 117/1000 | Loss: 0.00000935
Iteration 118/1000 | Loss: 0.00000935
Iteration 119/1000 | Loss: 0.00000935
Iteration 120/1000 | Loss: 0.00000935
Iteration 121/1000 | Loss: 0.00000935
Iteration 122/1000 | Loss: 0.00000935
Iteration 123/1000 | Loss: 0.00000934
Iteration 124/1000 | Loss: 0.00000934
Iteration 125/1000 | Loss: 0.00000934
Iteration 126/1000 | Loss: 0.00000934
Iteration 127/1000 | Loss: 0.00000934
Iteration 128/1000 | Loss: 0.00000934
Iteration 129/1000 | Loss: 0.00000934
Iteration 130/1000 | Loss: 0.00000933
Iteration 131/1000 | Loss: 0.00000933
Iteration 132/1000 | Loss: 0.00000933
Iteration 133/1000 | Loss: 0.00000932
Iteration 134/1000 | Loss: 0.00000932
Iteration 135/1000 | Loss: 0.00000932
Iteration 136/1000 | Loss: 0.00000932
Iteration 137/1000 | Loss: 0.00000932
Iteration 138/1000 | Loss: 0.00000932
Iteration 139/1000 | Loss: 0.00000932
Iteration 140/1000 | Loss: 0.00000932
Iteration 141/1000 | Loss: 0.00000932
Iteration 142/1000 | Loss: 0.00000932
Iteration 143/1000 | Loss: 0.00000932
Iteration 144/1000 | Loss: 0.00000932
Iteration 145/1000 | Loss: 0.00000932
Iteration 146/1000 | Loss: 0.00000932
Iteration 147/1000 | Loss: 0.00000931
Iteration 148/1000 | Loss: 0.00000931
Iteration 149/1000 | Loss: 0.00000931
Iteration 150/1000 | Loss: 0.00000931
Iteration 151/1000 | Loss: 0.00000931
Iteration 152/1000 | Loss: 0.00000931
Iteration 153/1000 | Loss: 0.00000931
Iteration 154/1000 | Loss: 0.00000931
Iteration 155/1000 | Loss: 0.00000931
Iteration 156/1000 | Loss: 0.00000931
Iteration 157/1000 | Loss: 0.00000931
Iteration 158/1000 | Loss: 0.00000931
Iteration 159/1000 | Loss: 0.00000931
Iteration 160/1000 | Loss: 0.00000931
Iteration 161/1000 | Loss: 0.00000931
Iteration 162/1000 | Loss: 0.00000930
Iteration 163/1000 | Loss: 0.00000930
Iteration 164/1000 | Loss: 0.00000930
Iteration 165/1000 | Loss: 0.00000930
Iteration 166/1000 | Loss: 0.00000930
Iteration 167/1000 | Loss: 0.00000930
Iteration 168/1000 | Loss: 0.00000930
Iteration 169/1000 | Loss: 0.00000930
Iteration 170/1000 | Loss: 0.00000930
Iteration 171/1000 | Loss: 0.00000930
Iteration 172/1000 | Loss: 0.00000930
Iteration 173/1000 | Loss: 0.00000930
Iteration 174/1000 | Loss: 0.00000930
Iteration 175/1000 | Loss: 0.00000930
Iteration 176/1000 | Loss: 0.00000930
Iteration 177/1000 | Loss: 0.00000930
Iteration 178/1000 | Loss: 0.00000930
Iteration 179/1000 | Loss: 0.00000930
Iteration 180/1000 | Loss: 0.00000930
Iteration 181/1000 | Loss: 0.00000930
Iteration 182/1000 | Loss: 0.00000929
Iteration 183/1000 | Loss: 0.00000929
Iteration 184/1000 | Loss: 0.00000929
Iteration 185/1000 | Loss: 0.00000929
Iteration 186/1000 | Loss: 0.00000929
Iteration 187/1000 | Loss: 0.00000929
Iteration 188/1000 | Loss: 0.00000929
Iteration 189/1000 | Loss: 0.00000929
Iteration 190/1000 | Loss: 0.00000929
Iteration 191/1000 | Loss: 0.00000929
Iteration 192/1000 | Loss: 0.00000929
Iteration 193/1000 | Loss: 0.00000929
Iteration 194/1000 | Loss: 0.00000929
Iteration 195/1000 | Loss: 0.00000929
Iteration 196/1000 | Loss: 0.00000929
Iteration 197/1000 | Loss: 0.00000928
Iteration 198/1000 | Loss: 0.00000928
Iteration 199/1000 | Loss: 0.00000928
Iteration 200/1000 | Loss: 0.00000928
Iteration 201/1000 | Loss: 0.00000928
Iteration 202/1000 | Loss: 0.00000928
Iteration 203/1000 | Loss: 0.00000928
Iteration 204/1000 | Loss: 0.00000928
Iteration 205/1000 | Loss: 0.00000928
Iteration 206/1000 | Loss: 0.00000927
Iteration 207/1000 | Loss: 0.00000927
Iteration 208/1000 | Loss: 0.00000927
Iteration 209/1000 | Loss: 0.00000927
Iteration 210/1000 | Loss: 0.00000927
Iteration 211/1000 | Loss: 0.00000927
Iteration 212/1000 | Loss: 0.00000927
Iteration 213/1000 | Loss: 0.00000927
Iteration 214/1000 | Loss: 0.00000927
Iteration 215/1000 | Loss: 0.00000927
Iteration 216/1000 | Loss: 0.00000927
Iteration 217/1000 | Loss: 0.00000927
Iteration 218/1000 | Loss: 0.00000927
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 218. Stopping optimization.
Last 5 losses: [9.27146083995467e-06, 9.27146083995467e-06, 9.27146083995467e-06, 9.27146083995467e-06, 9.27146083995467e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.27146083995467e-06

Optimization complete. Final v2v error: 2.6201601028442383 mm

Highest mean error: 2.817426919937134 mm for frame 68

Lowest mean error: 2.432234287261963 mm for frame 243

Saving results

Total time: 52.44039964675903
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_025/1018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_025/1018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_025/1018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00228855
Iteration 2/25 | Loss: 0.00128626
Iteration 3/25 | Loss: 0.00121400
Iteration 4/25 | Loss: 0.00119940
Iteration 5/25 | Loss: 0.00119314
Iteration 6/25 | Loss: 0.00119075
Iteration 7/25 | Loss: 0.00119075
Iteration 8/25 | Loss: 0.00119075
Iteration 9/25 | Loss: 0.00119075
Iteration 10/25 | Loss: 0.00119075
Iteration 11/25 | Loss: 0.00119075
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001190745853818953, 0.001190745853818953, 0.001190745853818953, 0.001190745853818953, 0.001190745853818953]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001190745853818953

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.23255956
Iteration 2/25 | Loss: 0.00262032
Iteration 3/25 | Loss: 0.00262032
Iteration 4/25 | Loss: 0.00262032
Iteration 5/25 | Loss: 0.00262032
Iteration 6/25 | Loss: 0.00262032
Iteration 7/25 | Loss: 0.00262032
Iteration 8/25 | Loss: 0.00262032
Iteration 9/25 | Loss: 0.00262032
Iteration 10/25 | Loss: 0.00262032
Iteration 11/25 | Loss: 0.00262032
Iteration 12/25 | Loss: 0.00262032
Iteration 13/25 | Loss: 0.00262032
Iteration 14/25 | Loss: 0.00262032
Iteration 15/25 | Loss: 0.00262032
Iteration 16/25 | Loss: 0.00262032
Iteration 17/25 | Loss: 0.00262032
Iteration 18/25 | Loss: 0.00262032
Iteration 19/25 | Loss: 0.00262032
Iteration 20/25 | Loss: 0.00262032
Iteration 21/25 | Loss: 0.00262032
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0026203207671642303, 0.0026203207671642303, 0.0026203207671642303, 0.0026203207671642303, 0.0026203207671642303]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0026203207671642303

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00262032
Iteration 2/1000 | Loss: 0.00004114
Iteration 3/1000 | Loss: 0.00002468
Iteration 4/1000 | Loss: 0.00001818
Iteration 5/1000 | Loss: 0.00001624
Iteration 6/1000 | Loss: 0.00001516
Iteration 7/1000 | Loss: 0.00001441
Iteration 8/1000 | Loss: 0.00001390
Iteration 9/1000 | Loss: 0.00001365
Iteration 10/1000 | Loss: 0.00001344
Iteration 11/1000 | Loss: 0.00001322
Iteration 12/1000 | Loss: 0.00001299
Iteration 13/1000 | Loss: 0.00001289
Iteration 14/1000 | Loss: 0.00001272
Iteration 15/1000 | Loss: 0.00001270
Iteration 16/1000 | Loss: 0.00001266
Iteration 17/1000 | Loss: 0.00001264
Iteration 18/1000 | Loss: 0.00001263
Iteration 19/1000 | Loss: 0.00001262
Iteration 20/1000 | Loss: 0.00001262
Iteration 21/1000 | Loss: 0.00001261
Iteration 22/1000 | Loss: 0.00001261
Iteration 23/1000 | Loss: 0.00001261
Iteration 24/1000 | Loss: 0.00001260
Iteration 25/1000 | Loss: 0.00001260
Iteration 26/1000 | Loss: 0.00001260
Iteration 27/1000 | Loss: 0.00001259
Iteration 28/1000 | Loss: 0.00001257
Iteration 29/1000 | Loss: 0.00001256
Iteration 30/1000 | Loss: 0.00001256
Iteration 31/1000 | Loss: 0.00001256
Iteration 32/1000 | Loss: 0.00001255
Iteration 33/1000 | Loss: 0.00001252
Iteration 34/1000 | Loss: 0.00001251
Iteration 35/1000 | Loss: 0.00001251
Iteration 36/1000 | Loss: 0.00001251
Iteration 37/1000 | Loss: 0.00001250
Iteration 38/1000 | Loss: 0.00001250
Iteration 39/1000 | Loss: 0.00001248
Iteration 40/1000 | Loss: 0.00001248
Iteration 41/1000 | Loss: 0.00001247
Iteration 42/1000 | Loss: 0.00001247
Iteration 43/1000 | Loss: 0.00001245
Iteration 44/1000 | Loss: 0.00001244
Iteration 45/1000 | Loss: 0.00001244
Iteration 46/1000 | Loss: 0.00001243
Iteration 47/1000 | Loss: 0.00001243
Iteration 48/1000 | Loss: 0.00001242
Iteration 49/1000 | Loss: 0.00001242
Iteration 50/1000 | Loss: 0.00001241
Iteration 51/1000 | Loss: 0.00001241
Iteration 52/1000 | Loss: 0.00001240
Iteration 53/1000 | Loss: 0.00001240
Iteration 54/1000 | Loss: 0.00001240
Iteration 55/1000 | Loss: 0.00001239
Iteration 56/1000 | Loss: 0.00001238
Iteration 57/1000 | Loss: 0.00001238
Iteration 58/1000 | Loss: 0.00001238
Iteration 59/1000 | Loss: 0.00001238
Iteration 60/1000 | Loss: 0.00001238
Iteration 61/1000 | Loss: 0.00001238
Iteration 62/1000 | Loss: 0.00001238
Iteration 63/1000 | Loss: 0.00001238
Iteration 64/1000 | Loss: 0.00001238
Iteration 65/1000 | Loss: 0.00001238
Iteration 66/1000 | Loss: 0.00001238
Iteration 67/1000 | Loss: 0.00001238
Iteration 68/1000 | Loss: 0.00001237
Iteration 69/1000 | Loss: 0.00001236
Iteration 70/1000 | Loss: 0.00001235
Iteration 71/1000 | Loss: 0.00001235
Iteration 72/1000 | Loss: 0.00001235
Iteration 73/1000 | Loss: 0.00001234
Iteration 74/1000 | Loss: 0.00001234
Iteration 75/1000 | Loss: 0.00001234
Iteration 76/1000 | Loss: 0.00001234
Iteration 77/1000 | Loss: 0.00001234
Iteration 78/1000 | Loss: 0.00001234
Iteration 79/1000 | Loss: 0.00001233
Iteration 80/1000 | Loss: 0.00001233
Iteration 81/1000 | Loss: 0.00001233
Iteration 82/1000 | Loss: 0.00001233
Iteration 83/1000 | Loss: 0.00001233
Iteration 84/1000 | Loss: 0.00001232
Iteration 85/1000 | Loss: 0.00001232
Iteration 86/1000 | Loss: 0.00001232
Iteration 87/1000 | Loss: 0.00001232
Iteration 88/1000 | Loss: 0.00001232
Iteration 89/1000 | Loss: 0.00001232
Iteration 90/1000 | Loss: 0.00001231
Iteration 91/1000 | Loss: 0.00001231
Iteration 92/1000 | Loss: 0.00001231
Iteration 93/1000 | Loss: 0.00001231
Iteration 94/1000 | Loss: 0.00001231
Iteration 95/1000 | Loss: 0.00001231
Iteration 96/1000 | Loss: 0.00001231
Iteration 97/1000 | Loss: 0.00001230
Iteration 98/1000 | Loss: 0.00001230
Iteration 99/1000 | Loss: 0.00001229
Iteration 100/1000 | Loss: 0.00001229
Iteration 101/1000 | Loss: 0.00001229
Iteration 102/1000 | Loss: 0.00001229
Iteration 103/1000 | Loss: 0.00001228
Iteration 104/1000 | Loss: 0.00001228
Iteration 105/1000 | Loss: 0.00001228
Iteration 106/1000 | Loss: 0.00001227
Iteration 107/1000 | Loss: 0.00001227
Iteration 108/1000 | Loss: 0.00001227
Iteration 109/1000 | Loss: 0.00001227
Iteration 110/1000 | Loss: 0.00001226
Iteration 111/1000 | Loss: 0.00001226
Iteration 112/1000 | Loss: 0.00001226
Iteration 113/1000 | Loss: 0.00001226
Iteration 114/1000 | Loss: 0.00001226
Iteration 115/1000 | Loss: 0.00001226
Iteration 116/1000 | Loss: 0.00001226
Iteration 117/1000 | Loss: 0.00001225
Iteration 118/1000 | Loss: 0.00001225
Iteration 119/1000 | Loss: 0.00001225
Iteration 120/1000 | Loss: 0.00001225
Iteration 121/1000 | Loss: 0.00001225
Iteration 122/1000 | Loss: 0.00001225
Iteration 123/1000 | Loss: 0.00001225
Iteration 124/1000 | Loss: 0.00001225
Iteration 125/1000 | Loss: 0.00001225
Iteration 126/1000 | Loss: 0.00001225
Iteration 127/1000 | Loss: 0.00001225
Iteration 128/1000 | Loss: 0.00001224
Iteration 129/1000 | Loss: 0.00001224
Iteration 130/1000 | Loss: 0.00001224
Iteration 131/1000 | Loss: 0.00001224
Iteration 132/1000 | Loss: 0.00001224
Iteration 133/1000 | Loss: 0.00001223
Iteration 134/1000 | Loss: 0.00001223
Iteration 135/1000 | Loss: 0.00001223
Iteration 136/1000 | Loss: 0.00001223
Iteration 137/1000 | Loss: 0.00001223
Iteration 138/1000 | Loss: 0.00001223
Iteration 139/1000 | Loss: 0.00001223
Iteration 140/1000 | Loss: 0.00001222
Iteration 141/1000 | Loss: 0.00001222
Iteration 142/1000 | Loss: 0.00001222
Iteration 143/1000 | Loss: 0.00001222
Iteration 144/1000 | Loss: 0.00001222
Iteration 145/1000 | Loss: 0.00001222
Iteration 146/1000 | Loss: 0.00001221
Iteration 147/1000 | Loss: 0.00001221
Iteration 148/1000 | Loss: 0.00001221
Iteration 149/1000 | Loss: 0.00001220
Iteration 150/1000 | Loss: 0.00001220
Iteration 151/1000 | Loss: 0.00001219
Iteration 152/1000 | Loss: 0.00001219
Iteration 153/1000 | Loss: 0.00001219
Iteration 154/1000 | Loss: 0.00001219
Iteration 155/1000 | Loss: 0.00001219
Iteration 156/1000 | Loss: 0.00001219
Iteration 157/1000 | Loss: 0.00001219
Iteration 158/1000 | Loss: 0.00001219
Iteration 159/1000 | Loss: 0.00001219
Iteration 160/1000 | Loss: 0.00001219
Iteration 161/1000 | Loss: 0.00001218
Iteration 162/1000 | Loss: 0.00001218
Iteration 163/1000 | Loss: 0.00001218
Iteration 164/1000 | Loss: 0.00001218
Iteration 165/1000 | Loss: 0.00001218
Iteration 166/1000 | Loss: 0.00001218
Iteration 167/1000 | Loss: 0.00001218
Iteration 168/1000 | Loss: 0.00001218
Iteration 169/1000 | Loss: 0.00001218
Iteration 170/1000 | Loss: 0.00001217
Iteration 171/1000 | Loss: 0.00001217
Iteration 172/1000 | Loss: 0.00001217
Iteration 173/1000 | Loss: 0.00001217
Iteration 174/1000 | Loss: 0.00001217
Iteration 175/1000 | Loss: 0.00001217
Iteration 176/1000 | Loss: 0.00001217
Iteration 177/1000 | Loss: 0.00001216
Iteration 178/1000 | Loss: 0.00001216
Iteration 179/1000 | Loss: 0.00001216
Iteration 180/1000 | Loss: 0.00001216
Iteration 181/1000 | Loss: 0.00001216
Iteration 182/1000 | Loss: 0.00001216
Iteration 183/1000 | Loss: 0.00001216
Iteration 184/1000 | Loss: 0.00001216
Iteration 185/1000 | Loss: 0.00001216
Iteration 186/1000 | Loss: 0.00001215
Iteration 187/1000 | Loss: 0.00001215
Iteration 188/1000 | Loss: 0.00001215
Iteration 189/1000 | Loss: 0.00001215
Iteration 190/1000 | Loss: 0.00001215
Iteration 191/1000 | Loss: 0.00001215
Iteration 192/1000 | Loss: 0.00001215
Iteration 193/1000 | Loss: 0.00001215
Iteration 194/1000 | Loss: 0.00001215
Iteration 195/1000 | Loss: 0.00001215
Iteration 196/1000 | Loss: 0.00001215
Iteration 197/1000 | Loss: 0.00001215
Iteration 198/1000 | Loss: 0.00001215
Iteration 199/1000 | Loss: 0.00001215
Iteration 200/1000 | Loss: 0.00001215
Iteration 201/1000 | Loss: 0.00001215
Iteration 202/1000 | Loss: 0.00001215
Iteration 203/1000 | Loss: 0.00001215
Iteration 204/1000 | Loss: 0.00001215
Iteration 205/1000 | Loss: 0.00001215
Iteration 206/1000 | Loss: 0.00001214
Iteration 207/1000 | Loss: 0.00001214
Iteration 208/1000 | Loss: 0.00001214
Iteration 209/1000 | Loss: 0.00001214
Iteration 210/1000 | Loss: 0.00001214
Iteration 211/1000 | Loss: 0.00001214
Iteration 212/1000 | Loss: 0.00001214
Iteration 213/1000 | Loss: 0.00001214
Iteration 214/1000 | Loss: 0.00001214
Iteration 215/1000 | Loss: 0.00001214
Iteration 216/1000 | Loss: 0.00001214
Iteration 217/1000 | Loss: 0.00001214
Iteration 218/1000 | Loss: 0.00001214
Iteration 219/1000 | Loss: 0.00001214
Iteration 220/1000 | Loss: 0.00001214
Iteration 221/1000 | Loss: 0.00001214
Iteration 222/1000 | Loss: 0.00001214
Iteration 223/1000 | Loss: 0.00001214
Iteration 224/1000 | Loss: 0.00001214
Iteration 225/1000 | Loss: 0.00001214
Iteration 226/1000 | Loss: 0.00001214
Iteration 227/1000 | Loss: 0.00001214
Iteration 228/1000 | Loss: 0.00001214
Iteration 229/1000 | Loss: 0.00001214
Iteration 230/1000 | Loss: 0.00001214
Iteration 231/1000 | Loss: 0.00001214
Iteration 232/1000 | Loss: 0.00001214
Iteration 233/1000 | Loss: 0.00001214
Iteration 234/1000 | Loss: 0.00001214
Iteration 235/1000 | Loss: 0.00001214
Iteration 236/1000 | Loss: 0.00001214
Iteration 237/1000 | Loss: 0.00001214
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 237. Stopping optimization.
Last 5 losses: [1.2138996680732816e-05, 1.2138996680732816e-05, 1.2138996680732816e-05, 1.2138996680732816e-05, 1.2138996680732816e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2138996680732816e-05

Optimization complete. Final v2v error: 3.0165843963623047 mm

Highest mean error: 3.42814564704895 mm for frame 42

Lowest mean error: 2.77712345123291 mm for frame 188

Saving results

Total time: 44.92181730270386
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_025/1034/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_025/1034.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_025/1034
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00591671
Iteration 2/25 | Loss: 0.00153788
Iteration 3/25 | Loss: 0.00132012
Iteration 4/25 | Loss: 0.00129622
Iteration 5/25 | Loss: 0.00129263
Iteration 6/25 | Loss: 0.00129144
Iteration 7/25 | Loss: 0.00129141
Iteration 8/25 | Loss: 0.00129141
Iteration 9/25 | Loss: 0.00129141
Iteration 10/25 | Loss: 0.00129141
Iteration 11/25 | Loss: 0.00129141
Iteration 12/25 | Loss: 0.00129141
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0012914142571389675, 0.0012914142571389675, 0.0012914142571389675, 0.0012914142571389675, 0.0012914142571389675]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012914142571389675

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.10528100
Iteration 2/25 | Loss: 0.00126305
Iteration 3/25 | Loss: 0.00126304
Iteration 4/25 | Loss: 0.00126304
Iteration 5/25 | Loss: 0.00126304
Iteration 6/25 | Loss: 0.00126304
Iteration 7/25 | Loss: 0.00126304
Iteration 8/25 | Loss: 0.00126304
Iteration 9/25 | Loss: 0.00126304
Iteration 10/25 | Loss: 0.00126304
Iteration 11/25 | Loss: 0.00126304
Iteration 12/25 | Loss: 0.00126304
Iteration 13/25 | Loss: 0.00126304
Iteration 14/25 | Loss: 0.00126304
Iteration 15/25 | Loss: 0.00126304
Iteration 16/25 | Loss: 0.00126304
Iteration 17/25 | Loss: 0.00126304
Iteration 18/25 | Loss: 0.00126304
Iteration 19/25 | Loss: 0.00126304
Iteration 20/25 | Loss: 0.00126304
Iteration 21/25 | Loss: 0.00126304
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0012630423298105597, 0.0012630423298105597, 0.0012630423298105597, 0.0012630423298105597, 0.0012630423298105597]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012630423298105597

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00126304
Iteration 2/1000 | Loss: 0.00004426
Iteration 3/1000 | Loss: 0.00002943
Iteration 4/1000 | Loss: 0.00002063
Iteration 5/1000 | Loss: 0.00001865
Iteration 6/1000 | Loss: 0.00001783
Iteration 7/1000 | Loss: 0.00001737
Iteration 8/1000 | Loss: 0.00001708
Iteration 9/1000 | Loss: 0.00001676
Iteration 10/1000 | Loss: 0.00001655
Iteration 11/1000 | Loss: 0.00001635
Iteration 12/1000 | Loss: 0.00001618
Iteration 13/1000 | Loss: 0.00001617
Iteration 14/1000 | Loss: 0.00001617
Iteration 15/1000 | Loss: 0.00001617
Iteration 16/1000 | Loss: 0.00001616
Iteration 17/1000 | Loss: 0.00001610
Iteration 18/1000 | Loss: 0.00001602
Iteration 19/1000 | Loss: 0.00001600
Iteration 20/1000 | Loss: 0.00001598
Iteration 21/1000 | Loss: 0.00001598
Iteration 22/1000 | Loss: 0.00001591
Iteration 23/1000 | Loss: 0.00001591
Iteration 24/1000 | Loss: 0.00001591
Iteration 25/1000 | Loss: 0.00001589
Iteration 26/1000 | Loss: 0.00001588
Iteration 27/1000 | Loss: 0.00001586
Iteration 28/1000 | Loss: 0.00001586
Iteration 29/1000 | Loss: 0.00001586
Iteration 30/1000 | Loss: 0.00001586
Iteration 31/1000 | Loss: 0.00001586
Iteration 32/1000 | Loss: 0.00001586
Iteration 33/1000 | Loss: 0.00001586
Iteration 34/1000 | Loss: 0.00001586
Iteration 35/1000 | Loss: 0.00001584
Iteration 36/1000 | Loss: 0.00001578
Iteration 37/1000 | Loss: 0.00001575
Iteration 38/1000 | Loss: 0.00001574
Iteration 39/1000 | Loss: 0.00001572
Iteration 40/1000 | Loss: 0.00001572
Iteration 41/1000 | Loss: 0.00001572
Iteration 42/1000 | Loss: 0.00001571
Iteration 43/1000 | Loss: 0.00001571
Iteration 44/1000 | Loss: 0.00001566
Iteration 45/1000 | Loss: 0.00001565
Iteration 46/1000 | Loss: 0.00001564
Iteration 47/1000 | Loss: 0.00001564
Iteration 48/1000 | Loss: 0.00001563
Iteration 49/1000 | Loss: 0.00001563
Iteration 50/1000 | Loss: 0.00001562
Iteration 51/1000 | Loss: 0.00001562
Iteration 52/1000 | Loss: 0.00001562
Iteration 53/1000 | Loss: 0.00001562
Iteration 54/1000 | Loss: 0.00001562
Iteration 55/1000 | Loss: 0.00001561
Iteration 56/1000 | Loss: 0.00001561
Iteration 57/1000 | Loss: 0.00001560
Iteration 58/1000 | Loss: 0.00001560
Iteration 59/1000 | Loss: 0.00001560
Iteration 60/1000 | Loss: 0.00001559
Iteration 61/1000 | Loss: 0.00001559
Iteration 62/1000 | Loss: 0.00001559
Iteration 63/1000 | Loss: 0.00001559
Iteration 64/1000 | Loss: 0.00001559
Iteration 65/1000 | Loss: 0.00001559
Iteration 66/1000 | Loss: 0.00001559
Iteration 67/1000 | Loss: 0.00001558
Iteration 68/1000 | Loss: 0.00001558
Iteration 69/1000 | Loss: 0.00001558
Iteration 70/1000 | Loss: 0.00001558
Iteration 71/1000 | Loss: 0.00001558
Iteration 72/1000 | Loss: 0.00001558
Iteration 73/1000 | Loss: 0.00001558
Iteration 74/1000 | Loss: 0.00001558
Iteration 75/1000 | Loss: 0.00001558
Iteration 76/1000 | Loss: 0.00001557
Iteration 77/1000 | Loss: 0.00001557
Iteration 78/1000 | Loss: 0.00001557
Iteration 79/1000 | Loss: 0.00001557
Iteration 80/1000 | Loss: 0.00001556
Iteration 81/1000 | Loss: 0.00001556
Iteration 82/1000 | Loss: 0.00001556
Iteration 83/1000 | Loss: 0.00001556
Iteration 84/1000 | Loss: 0.00001556
Iteration 85/1000 | Loss: 0.00001555
Iteration 86/1000 | Loss: 0.00001555
Iteration 87/1000 | Loss: 0.00001555
Iteration 88/1000 | Loss: 0.00001555
Iteration 89/1000 | Loss: 0.00001555
Iteration 90/1000 | Loss: 0.00001555
Iteration 91/1000 | Loss: 0.00001555
Iteration 92/1000 | Loss: 0.00001555
Iteration 93/1000 | Loss: 0.00001555
Iteration 94/1000 | Loss: 0.00001555
Iteration 95/1000 | Loss: 0.00001555
Iteration 96/1000 | Loss: 0.00001555
Iteration 97/1000 | Loss: 0.00001554
Iteration 98/1000 | Loss: 0.00001554
Iteration 99/1000 | Loss: 0.00001554
Iteration 100/1000 | Loss: 0.00001554
Iteration 101/1000 | Loss: 0.00001554
Iteration 102/1000 | Loss: 0.00001553
Iteration 103/1000 | Loss: 0.00001553
Iteration 104/1000 | Loss: 0.00001553
Iteration 105/1000 | Loss: 0.00001553
Iteration 106/1000 | Loss: 0.00001553
Iteration 107/1000 | Loss: 0.00001553
Iteration 108/1000 | Loss: 0.00001553
Iteration 109/1000 | Loss: 0.00001553
Iteration 110/1000 | Loss: 0.00001553
Iteration 111/1000 | Loss: 0.00001553
Iteration 112/1000 | Loss: 0.00001553
Iteration 113/1000 | Loss: 0.00001553
Iteration 114/1000 | Loss: 0.00001552
Iteration 115/1000 | Loss: 0.00001552
Iteration 116/1000 | Loss: 0.00001552
Iteration 117/1000 | Loss: 0.00001552
Iteration 118/1000 | Loss: 0.00001552
Iteration 119/1000 | Loss: 0.00001552
Iteration 120/1000 | Loss: 0.00001552
Iteration 121/1000 | Loss: 0.00001552
Iteration 122/1000 | Loss: 0.00001552
Iteration 123/1000 | Loss: 0.00001552
Iteration 124/1000 | Loss: 0.00001552
Iteration 125/1000 | Loss: 0.00001552
Iteration 126/1000 | Loss: 0.00001552
Iteration 127/1000 | Loss: 0.00001552
Iteration 128/1000 | Loss: 0.00001552
Iteration 129/1000 | Loss: 0.00001552
Iteration 130/1000 | Loss: 0.00001552
Iteration 131/1000 | Loss: 0.00001552
Iteration 132/1000 | Loss: 0.00001552
Iteration 133/1000 | Loss: 0.00001552
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 133. Stopping optimization.
Last 5 losses: [1.552455978526268e-05, 1.552455978526268e-05, 1.552455978526268e-05, 1.552455978526268e-05, 1.552455978526268e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.552455978526268e-05

Optimization complete. Final v2v error: 3.2074551582336426 mm

Highest mean error: 4.7735724449157715 mm for frame 55

Lowest mean error: 2.845122814178467 mm for frame 139

Saving results

Total time: 38.28932976722717
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_025/1008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_025/1008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_025/1008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00396249
Iteration 2/25 | Loss: 0.00133267
Iteration 3/25 | Loss: 0.00124329
Iteration 4/25 | Loss: 0.00123263
Iteration 5/25 | Loss: 0.00122944
Iteration 6/25 | Loss: 0.00122944
Iteration 7/25 | Loss: 0.00122944
Iteration 8/25 | Loss: 0.00122944
Iteration 9/25 | Loss: 0.00122944
Iteration 10/25 | Loss: 0.00122944
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0012294373009353876, 0.0012294373009353876, 0.0012294373009353876, 0.0012294373009353876, 0.0012294373009353876]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012294373009353876

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.24078560
Iteration 2/25 | Loss: 0.00170185
Iteration 3/25 | Loss: 0.00170185
Iteration 4/25 | Loss: 0.00170185
Iteration 5/25 | Loss: 0.00170185
Iteration 6/25 | Loss: 0.00170185
Iteration 7/25 | Loss: 0.00170185
Iteration 8/25 | Loss: 0.00170185
Iteration 9/25 | Loss: 0.00170185
Iteration 10/25 | Loss: 0.00170185
Iteration 11/25 | Loss: 0.00170185
Iteration 12/25 | Loss: 0.00170185
Iteration 13/25 | Loss: 0.00170185
Iteration 14/25 | Loss: 0.00170185
Iteration 15/25 | Loss: 0.00170185
Iteration 16/25 | Loss: 0.00170185
Iteration 17/25 | Loss: 0.00170185
Iteration 18/25 | Loss: 0.00170185
Iteration 19/25 | Loss: 0.00170185
Iteration 20/25 | Loss: 0.00170185
Iteration 21/25 | Loss: 0.00170185
Iteration 22/25 | Loss: 0.00170185
Iteration 23/25 | Loss: 0.00170185
Iteration 24/25 | Loss: 0.00170185
Iteration 25/25 | Loss: 0.00170185

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00170185
Iteration 2/1000 | Loss: 0.00004539
Iteration 3/1000 | Loss: 0.00003127
Iteration 4/1000 | Loss: 0.00002857
Iteration 5/1000 | Loss: 0.00002671
Iteration 6/1000 | Loss: 0.00002537
Iteration 7/1000 | Loss: 0.00002454
Iteration 8/1000 | Loss: 0.00002398
Iteration 9/1000 | Loss: 0.00002353
Iteration 10/1000 | Loss: 0.00002307
Iteration 11/1000 | Loss: 0.00002281
Iteration 12/1000 | Loss: 0.00002249
Iteration 13/1000 | Loss: 0.00002220
Iteration 14/1000 | Loss: 0.00002194
Iteration 15/1000 | Loss: 0.00002190
Iteration 16/1000 | Loss: 0.00002185
Iteration 17/1000 | Loss: 0.00002180
Iteration 18/1000 | Loss: 0.00002178
Iteration 19/1000 | Loss: 0.00002171
Iteration 20/1000 | Loss: 0.00002167
Iteration 21/1000 | Loss: 0.00002163
Iteration 22/1000 | Loss: 0.00002163
Iteration 23/1000 | Loss: 0.00002161
Iteration 24/1000 | Loss: 0.00002160
Iteration 25/1000 | Loss: 0.00002160
Iteration 26/1000 | Loss: 0.00002159
Iteration 27/1000 | Loss: 0.00002158
Iteration 28/1000 | Loss: 0.00002158
Iteration 29/1000 | Loss: 0.00002158
Iteration 30/1000 | Loss: 0.00002158
Iteration 31/1000 | Loss: 0.00002157
Iteration 32/1000 | Loss: 0.00002156
Iteration 33/1000 | Loss: 0.00002156
Iteration 34/1000 | Loss: 0.00002156
Iteration 35/1000 | Loss: 0.00002156
Iteration 36/1000 | Loss: 0.00002154
Iteration 37/1000 | Loss: 0.00002154
Iteration 38/1000 | Loss: 0.00002154
Iteration 39/1000 | Loss: 0.00002153
Iteration 40/1000 | Loss: 0.00002153
Iteration 41/1000 | Loss: 0.00002153
Iteration 42/1000 | Loss: 0.00002153
Iteration 43/1000 | Loss: 0.00002153
Iteration 44/1000 | Loss: 0.00002153
Iteration 45/1000 | Loss: 0.00002153
Iteration 46/1000 | Loss: 0.00002153
Iteration 47/1000 | Loss: 0.00002153
Iteration 48/1000 | Loss: 0.00002153
Iteration 49/1000 | Loss: 0.00002152
Iteration 50/1000 | Loss: 0.00002152
Iteration 51/1000 | Loss: 0.00002152
Iteration 52/1000 | Loss: 0.00002151
Iteration 53/1000 | Loss: 0.00002151
Iteration 54/1000 | Loss: 0.00002151
Iteration 55/1000 | Loss: 0.00002150
Iteration 56/1000 | Loss: 0.00002150
Iteration 57/1000 | Loss: 0.00002150
Iteration 58/1000 | Loss: 0.00002149
Iteration 59/1000 | Loss: 0.00002149
Iteration 60/1000 | Loss: 0.00002149
Iteration 61/1000 | Loss: 0.00002149
Iteration 62/1000 | Loss: 0.00002149
Iteration 63/1000 | Loss: 0.00002149
Iteration 64/1000 | Loss: 0.00002148
Iteration 65/1000 | Loss: 0.00002148
Iteration 66/1000 | Loss: 0.00002148
Iteration 67/1000 | Loss: 0.00002148
Iteration 68/1000 | Loss: 0.00002148
Iteration 69/1000 | Loss: 0.00002148
Iteration 70/1000 | Loss: 0.00002148
Iteration 71/1000 | Loss: 0.00002148
Iteration 72/1000 | Loss: 0.00002147
Iteration 73/1000 | Loss: 0.00002147
Iteration 74/1000 | Loss: 0.00002147
Iteration 75/1000 | Loss: 0.00002147
Iteration 76/1000 | Loss: 0.00002146
Iteration 77/1000 | Loss: 0.00002146
Iteration 78/1000 | Loss: 0.00002146
Iteration 79/1000 | Loss: 0.00002146
Iteration 80/1000 | Loss: 0.00002146
Iteration 81/1000 | Loss: 0.00002146
Iteration 82/1000 | Loss: 0.00002146
Iteration 83/1000 | Loss: 0.00002146
Iteration 84/1000 | Loss: 0.00002145
Iteration 85/1000 | Loss: 0.00002145
Iteration 86/1000 | Loss: 0.00002145
Iteration 87/1000 | Loss: 0.00002145
Iteration 88/1000 | Loss: 0.00002144
Iteration 89/1000 | Loss: 0.00002144
Iteration 90/1000 | Loss: 0.00002144
Iteration 91/1000 | Loss: 0.00002143
Iteration 92/1000 | Loss: 0.00002143
Iteration 93/1000 | Loss: 0.00002143
Iteration 94/1000 | Loss: 0.00002143
Iteration 95/1000 | Loss: 0.00002142
Iteration 96/1000 | Loss: 0.00002142
Iteration 97/1000 | Loss: 0.00002141
Iteration 98/1000 | Loss: 0.00002141
Iteration 99/1000 | Loss: 0.00002141
Iteration 100/1000 | Loss: 0.00002141
Iteration 101/1000 | Loss: 0.00002140
Iteration 102/1000 | Loss: 0.00002140
Iteration 103/1000 | Loss: 0.00002140
Iteration 104/1000 | Loss: 0.00002139
Iteration 105/1000 | Loss: 0.00002139
Iteration 106/1000 | Loss: 0.00002139
Iteration 107/1000 | Loss: 0.00002139
Iteration 108/1000 | Loss: 0.00002138
Iteration 109/1000 | Loss: 0.00002138
Iteration 110/1000 | Loss: 0.00002137
Iteration 111/1000 | Loss: 0.00002137
Iteration 112/1000 | Loss: 0.00002137
Iteration 113/1000 | Loss: 0.00002137
Iteration 114/1000 | Loss: 0.00002136
Iteration 115/1000 | Loss: 0.00002136
Iteration 116/1000 | Loss: 0.00002136
Iteration 117/1000 | Loss: 0.00002135
Iteration 118/1000 | Loss: 0.00002135
Iteration 119/1000 | Loss: 0.00002133
Iteration 120/1000 | Loss: 0.00002133
Iteration 121/1000 | Loss: 0.00002131
Iteration 122/1000 | Loss: 0.00002131
Iteration 123/1000 | Loss: 0.00002131
Iteration 124/1000 | Loss: 0.00002130
Iteration 125/1000 | Loss: 0.00002130
Iteration 126/1000 | Loss: 0.00002130
Iteration 127/1000 | Loss: 0.00002130
Iteration 128/1000 | Loss: 0.00002129
Iteration 129/1000 | Loss: 0.00002129
Iteration 130/1000 | Loss: 0.00002129
Iteration 131/1000 | Loss: 0.00002129
Iteration 132/1000 | Loss: 0.00002129
Iteration 133/1000 | Loss: 0.00002129
Iteration 134/1000 | Loss: 0.00002128
Iteration 135/1000 | Loss: 0.00002128
Iteration 136/1000 | Loss: 0.00002128
Iteration 137/1000 | Loss: 0.00002128
Iteration 138/1000 | Loss: 0.00002128
Iteration 139/1000 | Loss: 0.00002128
Iteration 140/1000 | Loss: 0.00002128
Iteration 141/1000 | Loss: 0.00002128
Iteration 142/1000 | Loss: 0.00002127
Iteration 143/1000 | Loss: 0.00002127
Iteration 144/1000 | Loss: 0.00002127
Iteration 145/1000 | Loss: 0.00002127
Iteration 146/1000 | Loss: 0.00002127
Iteration 147/1000 | Loss: 0.00002127
Iteration 148/1000 | Loss: 0.00002127
Iteration 149/1000 | Loss: 0.00002127
Iteration 150/1000 | Loss: 0.00002126
Iteration 151/1000 | Loss: 0.00002126
Iteration 152/1000 | Loss: 0.00002126
Iteration 153/1000 | Loss: 0.00002126
Iteration 154/1000 | Loss: 0.00002126
Iteration 155/1000 | Loss: 0.00002126
Iteration 156/1000 | Loss: 0.00002126
Iteration 157/1000 | Loss: 0.00002126
Iteration 158/1000 | Loss: 0.00002126
Iteration 159/1000 | Loss: 0.00002126
Iteration 160/1000 | Loss: 0.00002126
Iteration 161/1000 | Loss: 0.00002126
Iteration 162/1000 | Loss: 0.00002126
Iteration 163/1000 | Loss: 0.00002126
Iteration 164/1000 | Loss: 0.00002126
Iteration 165/1000 | Loss: 0.00002125
Iteration 166/1000 | Loss: 0.00002125
Iteration 167/1000 | Loss: 0.00002125
Iteration 168/1000 | Loss: 0.00002125
Iteration 169/1000 | Loss: 0.00002125
Iteration 170/1000 | Loss: 0.00002125
Iteration 171/1000 | Loss: 0.00002125
Iteration 172/1000 | Loss: 0.00002125
Iteration 173/1000 | Loss: 0.00002125
Iteration 174/1000 | Loss: 0.00002125
Iteration 175/1000 | Loss: 0.00002125
Iteration 176/1000 | Loss: 0.00002125
Iteration 177/1000 | Loss: 0.00002125
Iteration 178/1000 | Loss: 0.00002125
Iteration 179/1000 | Loss: 0.00002125
Iteration 180/1000 | Loss: 0.00002125
Iteration 181/1000 | Loss: 0.00002125
Iteration 182/1000 | Loss: 0.00002125
Iteration 183/1000 | Loss: 0.00002125
Iteration 184/1000 | Loss: 0.00002125
Iteration 185/1000 | Loss: 0.00002125
Iteration 186/1000 | Loss: 0.00002125
Iteration 187/1000 | Loss: 0.00002125
Iteration 188/1000 | Loss: 0.00002125
Iteration 189/1000 | Loss: 0.00002125
Iteration 190/1000 | Loss: 0.00002125
Iteration 191/1000 | Loss: 0.00002125
Iteration 192/1000 | Loss: 0.00002125
Iteration 193/1000 | Loss: 0.00002125
Iteration 194/1000 | Loss: 0.00002125
Iteration 195/1000 | Loss: 0.00002125
Iteration 196/1000 | Loss: 0.00002125
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 196. Stopping optimization.
Last 5 losses: [2.124909769918304e-05, 2.124909769918304e-05, 2.124909769918304e-05, 2.124909769918304e-05, 2.124909769918304e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.124909769918304e-05

Optimization complete. Final v2v error: 3.779512882232666 mm

Highest mean error: 4.2448344230651855 mm for frame 113

Lowest mean error: 3.312459945678711 mm for frame 210

Saving results

Total time: 49.16327786445618
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_025/1052/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_025/1052.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_025/1052
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00410029
Iteration 2/25 | Loss: 0.00128037
Iteration 3/25 | Loss: 0.00122276
Iteration 4/25 | Loss: 0.00121332
Iteration 5/25 | Loss: 0.00121013
Iteration 6/25 | Loss: 0.00120938
Iteration 7/25 | Loss: 0.00120938
Iteration 8/25 | Loss: 0.00120938
Iteration 9/25 | Loss: 0.00120938
Iteration 10/25 | Loss: 0.00120938
Iteration 11/25 | Loss: 0.00120938
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001209384179674089, 0.001209384179674089, 0.001209384179674089, 0.001209384179674089, 0.001209384179674089]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001209384179674089

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.85044694
Iteration 2/25 | Loss: 0.00155341
Iteration 3/25 | Loss: 0.00155340
Iteration 4/25 | Loss: 0.00155340
Iteration 5/25 | Loss: 0.00155340
Iteration 6/25 | Loss: 0.00155340
Iteration 7/25 | Loss: 0.00155340
Iteration 8/25 | Loss: 0.00155340
Iteration 9/25 | Loss: 0.00155340
Iteration 10/25 | Loss: 0.00155340
Iteration 11/25 | Loss: 0.00155340
Iteration 12/25 | Loss: 0.00155340
Iteration 13/25 | Loss: 0.00155340
Iteration 14/25 | Loss: 0.00155340
Iteration 15/25 | Loss: 0.00155340
Iteration 16/25 | Loss: 0.00155340
Iteration 17/25 | Loss: 0.00155340
Iteration 18/25 | Loss: 0.00155340
Iteration 19/25 | Loss: 0.00155340
Iteration 20/25 | Loss: 0.00155340
Iteration 21/25 | Loss: 0.00155340
Iteration 22/25 | Loss: 0.00155340
Iteration 23/25 | Loss: 0.00155340
Iteration 24/25 | Loss: 0.00155340
Iteration 25/25 | Loss: 0.00155340
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0015534003032371402, 0.0015534003032371402, 0.0015534003032371402, 0.0015534003032371402, 0.0015534003032371402]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015534003032371402

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00155340
Iteration 2/1000 | Loss: 0.00002713
Iteration 3/1000 | Loss: 0.00001797
Iteration 4/1000 | Loss: 0.00001498
Iteration 5/1000 | Loss: 0.00001400
Iteration 6/1000 | Loss: 0.00001337
Iteration 7/1000 | Loss: 0.00001279
Iteration 8/1000 | Loss: 0.00001224
Iteration 9/1000 | Loss: 0.00001215
Iteration 10/1000 | Loss: 0.00001203
Iteration 11/1000 | Loss: 0.00001180
Iteration 12/1000 | Loss: 0.00001156
Iteration 13/1000 | Loss: 0.00001154
Iteration 14/1000 | Loss: 0.00001141
Iteration 15/1000 | Loss: 0.00001129
Iteration 16/1000 | Loss: 0.00001119
Iteration 17/1000 | Loss: 0.00001115
Iteration 18/1000 | Loss: 0.00001114
Iteration 19/1000 | Loss: 0.00001113
Iteration 20/1000 | Loss: 0.00001112
Iteration 21/1000 | Loss: 0.00001110
Iteration 22/1000 | Loss: 0.00001108
Iteration 23/1000 | Loss: 0.00001106
Iteration 24/1000 | Loss: 0.00001105
Iteration 25/1000 | Loss: 0.00001105
Iteration 26/1000 | Loss: 0.00001104
Iteration 27/1000 | Loss: 0.00001104
Iteration 28/1000 | Loss: 0.00001103
Iteration 29/1000 | Loss: 0.00001102
Iteration 30/1000 | Loss: 0.00001102
Iteration 31/1000 | Loss: 0.00001101
Iteration 32/1000 | Loss: 0.00001100
Iteration 33/1000 | Loss: 0.00001100
Iteration 34/1000 | Loss: 0.00001099
Iteration 35/1000 | Loss: 0.00001099
Iteration 36/1000 | Loss: 0.00001099
Iteration 37/1000 | Loss: 0.00001095
Iteration 38/1000 | Loss: 0.00001093
Iteration 39/1000 | Loss: 0.00001091
Iteration 40/1000 | Loss: 0.00001091
Iteration 41/1000 | Loss: 0.00001090
Iteration 42/1000 | Loss: 0.00001089
Iteration 43/1000 | Loss: 0.00001089
Iteration 44/1000 | Loss: 0.00001088
Iteration 45/1000 | Loss: 0.00001088
Iteration 46/1000 | Loss: 0.00001087
Iteration 47/1000 | Loss: 0.00001086
Iteration 48/1000 | Loss: 0.00001085
Iteration 49/1000 | Loss: 0.00001085
Iteration 50/1000 | Loss: 0.00001085
Iteration 51/1000 | Loss: 0.00001084
Iteration 52/1000 | Loss: 0.00001084
Iteration 53/1000 | Loss: 0.00001084
Iteration 54/1000 | Loss: 0.00001083
Iteration 55/1000 | Loss: 0.00001083
Iteration 56/1000 | Loss: 0.00001083
Iteration 57/1000 | Loss: 0.00001082
Iteration 58/1000 | Loss: 0.00001082
Iteration 59/1000 | Loss: 0.00001080
Iteration 60/1000 | Loss: 0.00001080
Iteration 61/1000 | Loss: 0.00001080
Iteration 62/1000 | Loss: 0.00001080
Iteration 63/1000 | Loss: 0.00001079
Iteration 64/1000 | Loss: 0.00001079
Iteration 65/1000 | Loss: 0.00001079
Iteration 66/1000 | Loss: 0.00001079
Iteration 67/1000 | Loss: 0.00001079
Iteration 68/1000 | Loss: 0.00001079
Iteration 69/1000 | Loss: 0.00001079
Iteration 70/1000 | Loss: 0.00001076
Iteration 71/1000 | Loss: 0.00001075
Iteration 72/1000 | Loss: 0.00001075
Iteration 73/1000 | Loss: 0.00001075
Iteration 74/1000 | Loss: 0.00001074
Iteration 75/1000 | Loss: 0.00001074
Iteration 76/1000 | Loss: 0.00001073
Iteration 77/1000 | Loss: 0.00001073
Iteration 78/1000 | Loss: 0.00001073
Iteration 79/1000 | Loss: 0.00001072
Iteration 80/1000 | Loss: 0.00001072
Iteration 81/1000 | Loss: 0.00001071
Iteration 82/1000 | Loss: 0.00001071
Iteration 83/1000 | Loss: 0.00001071
Iteration 84/1000 | Loss: 0.00001071
Iteration 85/1000 | Loss: 0.00001070
Iteration 86/1000 | Loss: 0.00001070
Iteration 87/1000 | Loss: 0.00001070
Iteration 88/1000 | Loss: 0.00001070
Iteration 89/1000 | Loss: 0.00001070
Iteration 90/1000 | Loss: 0.00001070
Iteration 91/1000 | Loss: 0.00001070
Iteration 92/1000 | Loss: 0.00001070
Iteration 93/1000 | Loss: 0.00001069
Iteration 94/1000 | Loss: 0.00001069
Iteration 95/1000 | Loss: 0.00001069
Iteration 96/1000 | Loss: 0.00001069
Iteration 97/1000 | Loss: 0.00001069
Iteration 98/1000 | Loss: 0.00001069
Iteration 99/1000 | Loss: 0.00001069
Iteration 100/1000 | Loss: 0.00001069
Iteration 101/1000 | Loss: 0.00001068
Iteration 102/1000 | Loss: 0.00001068
Iteration 103/1000 | Loss: 0.00001068
Iteration 104/1000 | Loss: 0.00001068
Iteration 105/1000 | Loss: 0.00001068
Iteration 106/1000 | Loss: 0.00001068
Iteration 107/1000 | Loss: 0.00001068
Iteration 108/1000 | Loss: 0.00001068
Iteration 109/1000 | Loss: 0.00001067
Iteration 110/1000 | Loss: 0.00001067
Iteration 111/1000 | Loss: 0.00001067
Iteration 112/1000 | Loss: 0.00001067
Iteration 113/1000 | Loss: 0.00001067
Iteration 114/1000 | Loss: 0.00001067
Iteration 115/1000 | Loss: 0.00001067
Iteration 116/1000 | Loss: 0.00001067
Iteration 117/1000 | Loss: 0.00001067
Iteration 118/1000 | Loss: 0.00001067
Iteration 119/1000 | Loss: 0.00001066
Iteration 120/1000 | Loss: 0.00001066
Iteration 121/1000 | Loss: 0.00001066
Iteration 122/1000 | Loss: 0.00001065
Iteration 123/1000 | Loss: 0.00001065
Iteration 124/1000 | Loss: 0.00001065
Iteration 125/1000 | Loss: 0.00001065
Iteration 126/1000 | Loss: 0.00001065
Iteration 127/1000 | Loss: 0.00001065
Iteration 128/1000 | Loss: 0.00001065
Iteration 129/1000 | Loss: 0.00001065
Iteration 130/1000 | Loss: 0.00001064
Iteration 131/1000 | Loss: 0.00001064
Iteration 132/1000 | Loss: 0.00001064
Iteration 133/1000 | Loss: 0.00001064
Iteration 134/1000 | Loss: 0.00001064
Iteration 135/1000 | Loss: 0.00001064
Iteration 136/1000 | Loss: 0.00001064
Iteration 137/1000 | Loss: 0.00001064
Iteration 138/1000 | Loss: 0.00001064
Iteration 139/1000 | Loss: 0.00001064
Iteration 140/1000 | Loss: 0.00001064
Iteration 141/1000 | Loss: 0.00001064
Iteration 142/1000 | Loss: 0.00001063
Iteration 143/1000 | Loss: 0.00001063
Iteration 144/1000 | Loss: 0.00001063
Iteration 145/1000 | Loss: 0.00001063
Iteration 146/1000 | Loss: 0.00001063
Iteration 147/1000 | Loss: 0.00001063
Iteration 148/1000 | Loss: 0.00001063
Iteration 149/1000 | Loss: 0.00001063
Iteration 150/1000 | Loss: 0.00001063
Iteration 151/1000 | Loss: 0.00001063
Iteration 152/1000 | Loss: 0.00001062
Iteration 153/1000 | Loss: 0.00001062
Iteration 154/1000 | Loss: 0.00001062
Iteration 155/1000 | Loss: 0.00001062
Iteration 156/1000 | Loss: 0.00001062
Iteration 157/1000 | Loss: 0.00001062
Iteration 158/1000 | Loss: 0.00001062
Iteration 159/1000 | Loss: 0.00001062
Iteration 160/1000 | Loss: 0.00001062
Iteration 161/1000 | Loss: 0.00001062
Iteration 162/1000 | Loss: 0.00001062
Iteration 163/1000 | Loss: 0.00001062
Iteration 164/1000 | Loss: 0.00001062
Iteration 165/1000 | Loss: 0.00001061
Iteration 166/1000 | Loss: 0.00001061
Iteration 167/1000 | Loss: 0.00001061
Iteration 168/1000 | Loss: 0.00001061
Iteration 169/1000 | Loss: 0.00001061
Iteration 170/1000 | Loss: 0.00001061
Iteration 171/1000 | Loss: 0.00001061
Iteration 172/1000 | Loss: 0.00001061
Iteration 173/1000 | Loss: 0.00001061
Iteration 174/1000 | Loss: 0.00001061
Iteration 175/1000 | Loss: 0.00001061
Iteration 176/1000 | Loss: 0.00001061
Iteration 177/1000 | Loss: 0.00001061
Iteration 178/1000 | Loss: 0.00001061
Iteration 179/1000 | Loss: 0.00001061
Iteration 180/1000 | Loss: 0.00001060
Iteration 181/1000 | Loss: 0.00001060
Iteration 182/1000 | Loss: 0.00001060
Iteration 183/1000 | Loss: 0.00001060
Iteration 184/1000 | Loss: 0.00001060
Iteration 185/1000 | Loss: 0.00001060
Iteration 186/1000 | Loss: 0.00001060
Iteration 187/1000 | Loss: 0.00001060
Iteration 188/1000 | Loss: 0.00001060
Iteration 189/1000 | Loss: 0.00001059
Iteration 190/1000 | Loss: 0.00001059
Iteration 191/1000 | Loss: 0.00001059
Iteration 192/1000 | Loss: 0.00001059
Iteration 193/1000 | Loss: 0.00001059
Iteration 194/1000 | Loss: 0.00001059
Iteration 195/1000 | Loss: 0.00001059
Iteration 196/1000 | Loss: 0.00001059
Iteration 197/1000 | Loss: 0.00001059
Iteration 198/1000 | Loss: 0.00001059
Iteration 199/1000 | Loss: 0.00001059
Iteration 200/1000 | Loss: 0.00001059
Iteration 201/1000 | Loss: 0.00001059
Iteration 202/1000 | Loss: 0.00001059
Iteration 203/1000 | Loss: 0.00001058
Iteration 204/1000 | Loss: 0.00001058
Iteration 205/1000 | Loss: 0.00001058
Iteration 206/1000 | Loss: 0.00001058
Iteration 207/1000 | Loss: 0.00001058
Iteration 208/1000 | Loss: 0.00001058
Iteration 209/1000 | Loss: 0.00001058
Iteration 210/1000 | Loss: 0.00001058
Iteration 211/1000 | Loss: 0.00001058
Iteration 212/1000 | Loss: 0.00001058
Iteration 213/1000 | Loss: 0.00001058
Iteration 214/1000 | Loss: 0.00001058
Iteration 215/1000 | Loss: 0.00001057
Iteration 216/1000 | Loss: 0.00001057
Iteration 217/1000 | Loss: 0.00001057
Iteration 218/1000 | Loss: 0.00001057
Iteration 219/1000 | Loss: 0.00001057
Iteration 220/1000 | Loss: 0.00001057
Iteration 221/1000 | Loss: 0.00001057
Iteration 222/1000 | Loss: 0.00001057
Iteration 223/1000 | Loss: 0.00001057
Iteration 224/1000 | Loss: 0.00001057
Iteration 225/1000 | Loss: 0.00001057
Iteration 226/1000 | Loss: 0.00001057
Iteration 227/1000 | Loss: 0.00001057
Iteration 228/1000 | Loss: 0.00001057
Iteration 229/1000 | Loss: 0.00001057
Iteration 230/1000 | Loss: 0.00001057
Iteration 231/1000 | Loss: 0.00001057
Iteration 232/1000 | Loss: 0.00001057
Iteration 233/1000 | Loss: 0.00001057
Iteration 234/1000 | Loss: 0.00001056
Iteration 235/1000 | Loss: 0.00001056
Iteration 236/1000 | Loss: 0.00001056
Iteration 237/1000 | Loss: 0.00001056
Iteration 238/1000 | Loss: 0.00001056
Iteration 239/1000 | Loss: 0.00001056
Iteration 240/1000 | Loss: 0.00001056
Iteration 241/1000 | Loss: 0.00001056
Iteration 242/1000 | Loss: 0.00001056
Iteration 243/1000 | Loss: 0.00001056
Iteration 244/1000 | Loss: 0.00001056
Iteration 245/1000 | Loss: 0.00001056
Iteration 246/1000 | Loss: 0.00001056
Iteration 247/1000 | Loss: 0.00001056
Iteration 248/1000 | Loss: 0.00001056
Iteration 249/1000 | Loss: 0.00001056
Iteration 250/1000 | Loss: 0.00001056
Iteration 251/1000 | Loss: 0.00001056
Iteration 252/1000 | Loss: 0.00001056
Iteration 253/1000 | Loss: 0.00001056
Iteration 254/1000 | Loss: 0.00001056
Iteration 255/1000 | Loss: 0.00001056
Iteration 256/1000 | Loss: 0.00001056
Iteration 257/1000 | Loss: 0.00001056
Iteration 258/1000 | Loss: 0.00001056
Iteration 259/1000 | Loss: 0.00001056
Iteration 260/1000 | Loss: 0.00001056
Iteration 261/1000 | Loss: 0.00001056
Iteration 262/1000 | Loss: 0.00001056
Iteration 263/1000 | Loss: 0.00001056
Iteration 264/1000 | Loss: 0.00001056
Iteration 265/1000 | Loss: 0.00001056
Iteration 266/1000 | Loss: 0.00001056
Iteration 267/1000 | Loss: 0.00001056
Iteration 268/1000 | Loss: 0.00001056
Iteration 269/1000 | Loss: 0.00001056
Iteration 270/1000 | Loss: 0.00001056
Iteration 271/1000 | Loss: 0.00001056
Iteration 272/1000 | Loss: 0.00001056
Iteration 273/1000 | Loss: 0.00001056
Iteration 274/1000 | Loss: 0.00001056
Iteration 275/1000 | Loss: 0.00001056
Iteration 276/1000 | Loss: 0.00001056
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 276. Stopping optimization.
Last 5 losses: [1.0558939720795024e-05, 1.0558939720795024e-05, 1.0558939720795024e-05, 1.0558939720795024e-05, 1.0558939720795024e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0558939720795024e-05

Optimization complete. Final v2v error: 2.767951726913452 mm

Highest mean error: 4.38702917098999 mm for frame 61

Lowest mean error: 2.5356497764587402 mm for frame 82

Saving results

Total time: 45.685633182525635
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_025/1029/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_025/1029.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_025/1029
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00817177
Iteration 2/25 | Loss: 0.00130133
Iteration 3/25 | Loss: 0.00121709
Iteration 4/25 | Loss: 0.00120806
Iteration 5/25 | Loss: 0.00120639
Iteration 6/25 | Loss: 0.00120639
Iteration 7/25 | Loss: 0.00120639
Iteration 8/25 | Loss: 0.00120639
Iteration 9/25 | Loss: 0.00120639
Iteration 10/25 | Loss: 0.00120639
Iteration 11/25 | Loss: 0.00120639
Iteration 12/25 | Loss: 0.00120639
Iteration 13/25 | Loss: 0.00120639
Iteration 14/25 | Loss: 0.00120639
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0012063928879797459, 0.0012063928879797459, 0.0012063928879797459, 0.0012063928879797459, 0.0012063928879797459]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012063928879797459

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.28301275
Iteration 2/25 | Loss: 0.00129125
Iteration 3/25 | Loss: 0.00129123
Iteration 4/25 | Loss: 0.00129123
Iteration 5/25 | Loss: 0.00129123
Iteration 6/25 | Loss: 0.00129123
Iteration 7/25 | Loss: 0.00129123
Iteration 8/25 | Loss: 0.00129123
Iteration 9/25 | Loss: 0.00129123
Iteration 10/25 | Loss: 0.00129123
Iteration 11/25 | Loss: 0.00129123
Iteration 12/25 | Loss: 0.00129123
Iteration 13/25 | Loss: 0.00129123
Iteration 14/25 | Loss: 0.00129123
Iteration 15/25 | Loss: 0.00129123
Iteration 16/25 | Loss: 0.00129123
Iteration 17/25 | Loss: 0.00129123
Iteration 18/25 | Loss: 0.00129123
Iteration 19/25 | Loss: 0.00129123
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0012912257807329297, 0.0012912257807329297, 0.0012912257807329297, 0.0012912257807329297, 0.0012912257807329297]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012912257807329297

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00129123
Iteration 2/1000 | Loss: 0.00001957
Iteration 3/1000 | Loss: 0.00001435
Iteration 4/1000 | Loss: 0.00001277
Iteration 5/1000 | Loss: 0.00001178
Iteration 6/1000 | Loss: 0.00001132
Iteration 7/1000 | Loss: 0.00001075
Iteration 8/1000 | Loss: 0.00001049
Iteration 9/1000 | Loss: 0.00001045
Iteration 10/1000 | Loss: 0.00001018
Iteration 11/1000 | Loss: 0.00000996
Iteration 12/1000 | Loss: 0.00000982
Iteration 13/1000 | Loss: 0.00000981
Iteration 14/1000 | Loss: 0.00000980
Iteration 15/1000 | Loss: 0.00000964
Iteration 16/1000 | Loss: 0.00000962
Iteration 17/1000 | Loss: 0.00000957
Iteration 18/1000 | Loss: 0.00000957
Iteration 19/1000 | Loss: 0.00000956
Iteration 20/1000 | Loss: 0.00000948
Iteration 21/1000 | Loss: 0.00000947
Iteration 22/1000 | Loss: 0.00000941
Iteration 23/1000 | Loss: 0.00000940
Iteration 24/1000 | Loss: 0.00000938
Iteration 25/1000 | Loss: 0.00000938
Iteration 26/1000 | Loss: 0.00000937
Iteration 27/1000 | Loss: 0.00000937
Iteration 28/1000 | Loss: 0.00000937
Iteration 29/1000 | Loss: 0.00000937
Iteration 30/1000 | Loss: 0.00000936
Iteration 31/1000 | Loss: 0.00000936
Iteration 32/1000 | Loss: 0.00000936
Iteration 33/1000 | Loss: 0.00000936
Iteration 34/1000 | Loss: 0.00000935
Iteration 35/1000 | Loss: 0.00000935
Iteration 36/1000 | Loss: 0.00000935
Iteration 37/1000 | Loss: 0.00000935
Iteration 38/1000 | Loss: 0.00000934
Iteration 39/1000 | Loss: 0.00000934
Iteration 40/1000 | Loss: 0.00000933
Iteration 41/1000 | Loss: 0.00000933
Iteration 42/1000 | Loss: 0.00000932
Iteration 43/1000 | Loss: 0.00000932
Iteration 44/1000 | Loss: 0.00000931
Iteration 45/1000 | Loss: 0.00000931
Iteration 46/1000 | Loss: 0.00000931
Iteration 47/1000 | Loss: 0.00000931
Iteration 48/1000 | Loss: 0.00000931
Iteration 49/1000 | Loss: 0.00000930
Iteration 50/1000 | Loss: 0.00000930
Iteration 51/1000 | Loss: 0.00000929
Iteration 52/1000 | Loss: 0.00000929
Iteration 53/1000 | Loss: 0.00000929
Iteration 54/1000 | Loss: 0.00000928
Iteration 55/1000 | Loss: 0.00000928
Iteration 56/1000 | Loss: 0.00000928
Iteration 57/1000 | Loss: 0.00000927
Iteration 58/1000 | Loss: 0.00000927
Iteration 59/1000 | Loss: 0.00000926
Iteration 60/1000 | Loss: 0.00000926
Iteration 61/1000 | Loss: 0.00000925
Iteration 62/1000 | Loss: 0.00000922
Iteration 63/1000 | Loss: 0.00000922
Iteration 64/1000 | Loss: 0.00000922
Iteration 65/1000 | Loss: 0.00000921
Iteration 66/1000 | Loss: 0.00000921
Iteration 67/1000 | Loss: 0.00000921
Iteration 68/1000 | Loss: 0.00000921
Iteration 69/1000 | Loss: 0.00000921
Iteration 70/1000 | Loss: 0.00000921
Iteration 71/1000 | Loss: 0.00000921
Iteration 72/1000 | Loss: 0.00000921
Iteration 73/1000 | Loss: 0.00000920
Iteration 74/1000 | Loss: 0.00000920
Iteration 75/1000 | Loss: 0.00000916
Iteration 76/1000 | Loss: 0.00000915
Iteration 77/1000 | Loss: 0.00000915
Iteration 78/1000 | Loss: 0.00000913
Iteration 79/1000 | Loss: 0.00000912
Iteration 80/1000 | Loss: 0.00000912
Iteration 81/1000 | Loss: 0.00000912
Iteration 82/1000 | Loss: 0.00000911
Iteration 83/1000 | Loss: 0.00000910
Iteration 84/1000 | Loss: 0.00000910
Iteration 85/1000 | Loss: 0.00000910
Iteration 86/1000 | Loss: 0.00000909
Iteration 87/1000 | Loss: 0.00000909
Iteration 88/1000 | Loss: 0.00000908
Iteration 89/1000 | Loss: 0.00000908
Iteration 90/1000 | Loss: 0.00000908
Iteration 91/1000 | Loss: 0.00000908
Iteration 92/1000 | Loss: 0.00000907
Iteration 93/1000 | Loss: 0.00000907
Iteration 94/1000 | Loss: 0.00000907
Iteration 95/1000 | Loss: 0.00000907
Iteration 96/1000 | Loss: 0.00000907
Iteration 97/1000 | Loss: 0.00000906
Iteration 98/1000 | Loss: 0.00000906
Iteration 99/1000 | Loss: 0.00000905
Iteration 100/1000 | Loss: 0.00000905
Iteration 101/1000 | Loss: 0.00000905
Iteration 102/1000 | Loss: 0.00000905
Iteration 103/1000 | Loss: 0.00000905
Iteration 104/1000 | Loss: 0.00000905
Iteration 105/1000 | Loss: 0.00000905
Iteration 106/1000 | Loss: 0.00000905
Iteration 107/1000 | Loss: 0.00000904
Iteration 108/1000 | Loss: 0.00000904
Iteration 109/1000 | Loss: 0.00000904
Iteration 110/1000 | Loss: 0.00000904
Iteration 111/1000 | Loss: 0.00000904
Iteration 112/1000 | Loss: 0.00000904
Iteration 113/1000 | Loss: 0.00000904
Iteration 114/1000 | Loss: 0.00000903
Iteration 115/1000 | Loss: 0.00000903
Iteration 116/1000 | Loss: 0.00000902
Iteration 117/1000 | Loss: 0.00000902
Iteration 118/1000 | Loss: 0.00000902
Iteration 119/1000 | Loss: 0.00000902
Iteration 120/1000 | Loss: 0.00000902
Iteration 121/1000 | Loss: 0.00000902
Iteration 122/1000 | Loss: 0.00000902
Iteration 123/1000 | Loss: 0.00000902
Iteration 124/1000 | Loss: 0.00000902
Iteration 125/1000 | Loss: 0.00000902
Iteration 126/1000 | Loss: 0.00000902
Iteration 127/1000 | Loss: 0.00000902
Iteration 128/1000 | Loss: 0.00000901
Iteration 129/1000 | Loss: 0.00000901
Iteration 130/1000 | Loss: 0.00000901
Iteration 131/1000 | Loss: 0.00000901
Iteration 132/1000 | Loss: 0.00000901
Iteration 133/1000 | Loss: 0.00000901
Iteration 134/1000 | Loss: 0.00000901
Iteration 135/1000 | Loss: 0.00000900
Iteration 136/1000 | Loss: 0.00000900
Iteration 137/1000 | Loss: 0.00000900
Iteration 138/1000 | Loss: 0.00000900
Iteration 139/1000 | Loss: 0.00000900
Iteration 140/1000 | Loss: 0.00000900
Iteration 141/1000 | Loss: 0.00000900
Iteration 142/1000 | Loss: 0.00000900
Iteration 143/1000 | Loss: 0.00000900
Iteration 144/1000 | Loss: 0.00000900
Iteration 145/1000 | Loss: 0.00000900
Iteration 146/1000 | Loss: 0.00000900
Iteration 147/1000 | Loss: 0.00000900
Iteration 148/1000 | Loss: 0.00000900
Iteration 149/1000 | Loss: 0.00000900
Iteration 150/1000 | Loss: 0.00000900
Iteration 151/1000 | Loss: 0.00000900
Iteration 152/1000 | Loss: 0.00000899
Iteration 153/1000 | Loss: 0.00000899
Iteration 154/1000 | Loss: 0.00000899
Iteration 155/1000 | Loss: 0.00000899
Iteration 156/1000 | Loss: 0.00000899
Iteration 157/1000 | Loss: 0.00000899
Iteration 158/1000 | Loss: 0.00000899
Iteration 159/1000 | Loss: 0.00000898
Iteration 160/1000 | Loss: 0.00000898
Iteration 161/1000 | Loss: 0.00000898
Iteration 162/1000 | Loss: 0.00000898
Iteration 163/1000 | Loss: 0.00000898
Iteration 164/1000 | Loss: 0.00000898
Iteration 165/1000 | Loss: 0.00000898
Iteration 166/1000 | Loss: 0.00000898
Iteration 167/1000 | Loss: 0.00000898
Iteration 168/1000 | Loss: 0.00000898
Iteration 169/1000 | Loss: 0.00000898
Iteration 170/1000 | Loss: 0.00000898
Iteration 171/1000 | Loss: 0.00000898
Iteration 172/1000 | Loss: 0.00000898
Iteration 173/1000 | Loss: 0.00000898
Iteration 174/1000 | Loss: 0.00000898
Iteration 175/1000 | Loss: 0.00000898
Iteration 176/1000 | Loss: 0.00000897
Iteration 177/1000 | Loss: 0.00000897
Iteration 178/1000 | Loss: 0.00000897
Iteration 179/1000 | Loss: 0.00000897
Iteration 180/1000 | Loss: 0.00000897
Iteration 181/1000 | Loss: 0.00000897
Iteration 182/1000 | Loss: 0.00000897
Iteration 183/1000 | Loss: 0.00000897
Iteration 184/1000 | Loss: 0.00000897
Iteration 185/1000 | Loss: 0.00000897
Iteration 186/1000 | Loss: 0.00000897
Iteration 187/1000 | Loss: 0.00000897
Iteration 188/1000 | Loss: 0.00000896
Iteration 189/1000 | Loss: 0.00000896
Iteration 190/1000 | Loss: 0.00000896
Iteration 191/1000 | Loss: 0.00000896
Iteration 192/1000 | Loss: 0.00000896
Iteration 193/1000 | Loss: 0.00000896
Iteration 194/1000 | Loss: 0.00000896
Iteration 195/1000 | Loss: 0.00000896
Iteration 196/1000 | Loss: 0.00000896
Iteration 197/1000 | Loss: 0.00000896
Iteration 198/1000 | Loss: 0.00000896
Iteration 199/1000 | Loss: 0.00000896
Iteration 200/1000 | Loss: 0.00000896
Iteration 201/1000 | Loss: 0.00000896
Iteration 202/1000 | Loss: 0.00000896
Iteration 203/1000 | Loss: 0.00000896
Iteration 204/1000 | Loss: 0.00000896
Iteration 205/1000 | Loss: 0.00000896
Iteration 206/1000 | Loss: 0.00000896
Iteration 207/1000 | Loss: 0.00000896
Iteration 208/1000 | Loss: 0.00000896
Iteration 209/1000 | Loss: 0.00000896
Iteration 210/1000 | Loss: 0.00000896
Iteration 211/1000 | Loss: 0.00000896
Iteration 212/1000 | Loss: 0.00000896
Iteration 213/1000 | Loss: 0.00000896
Iteration 214/1000 | Loss: 0.00000896
Iteration 215/1000 | Loss: 0.00000896
Iteration 216/1000 | Loss: 0.00000896
Iteration 217/1000 | Loss: 0.00000896
Iteration 218/1000 | Loss: 0.00000896
Iteration 219/1000 | Loss: 0.00000896
Iteration 220/1000 | Loss: 0.00000896
Iteration 221/1000 | Loss: 0.00000896
Iteration 222/1000 | Loss: 0.00000896
Iteration 223/1000 | Loss: 0.00000896
Iteration 224/1000 | Loss: 0.00000896
Iteration 225/1000 | Loss: 0.00000896
Iteration 226/1000 | Loss: 0.00000896
Iteration 227/1000 | Loss: 0.00000896
Iteration 228/1000 | Loss: 0.00000896
Iteration 229/1000 | Loss: 0.00000896
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 229. Stopping optimization.
Last 5 losses: [8.95728499017423e-06, 8.95728499017423e-06, 8.95728499017423e-06, 8.95728499017423e-06, 8.95728499017423e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.95728499017423e-06

Optimization complete. Final v2v error: 2.6034443378448486 mm

Highest mean error: 2.745725393295288 mm for frame 28

Lowest mean error: 2.51924204826355 mm for frame 40

Saving results

Total time: 41.415289878845215
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_025/1006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_025/1006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_025/1006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00778972
Iteration 2/25 | Loss: 0.00155503
Iteration 3/25 | Loss: 0.00129368
Iteration 4/25 | Loss: 0.00126502
Iteration 5/25 | Loss: 0.00126020
Iteration 6/25 | Loss: 0.00125881
Iteration 7/25 | Loss: 0.00125843
Iteration 8/25 | Loss: 0.00125830
Iteration 9/25 | Loss: 0.00125830
Iteration 10/25 | Loss: 0.00125830
Iteration 11/25 | Loss: 0.00125830
Iteration 12/25 | Loss: 0.00125830
Iteration 13/25 | Loss: 0.00125830
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0012582977069541812, 0.0012582977069541812, 0.0012582977069541812, 0.0012582977069541812, 0.0012582977069541812]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012582977069541812

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.52311254
Iteration 2/25 | Loss: 0.00158723
Iteration 3/25 | Loss: 0.00158723
Iteration 4/25 | Loss: 0.00158723
Iteration 5/25 | Loss: 0.00158723
Iteration 6/25 | Loss: 0.00158722
Iteration 7/25 | Loss: 0.00158722
Iteration 8/25 | Loss: 0.00158722
Iteration 9/25 | Loss: 0.00158722
Iteration 10/25 | Loss: 0.00158722
Iteration 11/25 | Loss: 0.00158722
Iteration 12/25 | Loss: 0.00158722
Iteration 13/25 | Loss: 0.00158722
Iteration 14/25 | Loss: 0.00158722
Iteration 15/25 | Loss: 0.00158722
Iteration 16/25 | Loss: 0.00158722
Iteration 17/25 | Loss: 0.00158722
Iteration 18/25 | Loss: 0.00158722
Iteration 19/25 | Loss: 0.00158722
Iteration 20/25 | Loss: 0.00158722
Iteration 21/25 | Loss: 0.00158722
Iteration 22/25 | Loss: 0.00158722
Iteration 23/25 | Loss: 0.00158722
Iteration 24/25 | Loss: 0.00158722
Iteration 25/25 | Loss: 0.00158722
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0015872225631028414, 0.0015872225631028414, 0.0015872225631028414, 0.0015872225631028414, 0.0015872225631028414]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015872225631028414

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00158722
Iteration 2/1000 | Loss: 0.00002378
Iteration 3/1000 | Loss: 0.00001949
Iteration 4/1000 | Loss: 0.00015606
Iteration 5/1000 | Loss: 0.00002343
Iteration 6/1000 | Loss: 0.00001955
Iteration 7/1000 | Loss: 0.00001824
Iteration 8/1000 | Loss: 0.00014906
Iteration 9/1000 | Loss: 0.00001939
Iteration 10/1000 | Loss: 0.00001771
Iteration 11/1000 | Loss: 0.00017607
Iteration 12/1000 | Loss: 0.00009496
Iteration 13/1000 | Loss: 0.00015157
Iteration 14/1000 | Loss: 0.00009039
Iteration 15/1000 | Loss: 0.00001941
Iteration 16/1000 | Loss: 0.00014736
Iteration 17/1000 | Loss: 0.00011467
Iteration 18/1000 | Loss: 0.00001839
Iteration 19/1000 | Loss: 0.00015154
Iteration 20/1000 | Loss: 0.00012697
Iteration 21/1000 | Loss: 0.00001787
Iteration 22/1000 | Loss: 0.00001876
Iteration 23/1000 | Loss: 0.00001616
Iteration 24/1000 | Loss: 0.00001614
Iteration 25/1000 | Loss: 0.00001612
Iteration 26/1000 | Loss: 0.00001594
Iteration 27/1000 | Loss: 0.00001566
Iteration 28/1000 | Loss: 0.00001698
Iteration 29/1000 | Loss: 0.00001586
Iteration 30/1000 | Loss: 0.00001582
Iteration 31/1000 | Loss: 0.00001548
Iteration 32/1000 | Loss: 0.00001517
Iteration 33/1000 | Loss: 0.00001515
Iteration 34/1000 | Loss: 0.00001515
Iteration 35/1000 | Loss: 0.00001514
Iteration 36/1000 | Loss: 0.00001514
Iteration 37/1000 | Loss: 0.00001513
Iteration 38/1000 | Loss: 0.00001513
Iteration 39/1000 | Loss: 0.00001513
Iteration 40/1000 | Loss: 0.00001513
Iteration 41/1000 | Loss: 0.00001513
Iteration 42/1000 | Loss: 0.00001512
Iteration 43/1000 | Loss: 0.00001511
Iteration 44/1000 | Loss: 0.00001510
Iteration 45/1000 | Loss: 0.00001508
Iteration 46/1000 | Loss: 0.00001507
Iteration 47/1000 | Loss: 0.00001505
Iteration 48/1000 | Loss: 0.00001504
Iteration 49/1000 | Loss: 0.00001502
Iteration 50/1000 | Loss: 0.00001499
Iteration 51/1000 | Loss: 0.00001498
Iteration 52/1000 | Loss: 0.00001497
Iteration 53/1000 | Loss: 0.00001496
Iteration 54/1000 | Loss: 0.00001496
Iteration 55/1000 | Loss: 0.00001495
Iteration 56/1000 | Loss: 0.00001494
Iteration 57/1000 | Loss: 0.00001493
Iteration 58/1000 | Loss: 0.00001492
Iteration 59/1000 | Loss: 0.00001491
Iteration 60/1000 | Loss: 0.00001491
Iteration 61/1000 | Loss: 0.00001490
Iteration 62/1000 | Loss: 0.00001490
Iteration 63/1000 | Loss: 0.00001489
Iteration 64/1000 | Loss: 0.00001489
Iteration 65/1000 | Loss: 0.00001488
Iteration 66/1000 | Loss: 0.00001488
Iteration 67/1000 | Loss: 0.00001488
Iteration 68/1000 | Loss: 0.00001487
Iteration 69/1000 | Loss: 0.00001487
Iteration 70/1000 | Loss: 0.00001487
Iteration 71/1000 | Loss: 0.00001487
Iteration 72/1000 | Loss: 0.00001487
Iteration 73/1000 | Loss: 0.00001487
Iteration 74/1000 | Loss: 0.00001486
Iteration 75/1000 | Loss: 0.00001486
Iteration 76/1000 | Loss: 0.00001486
Iteration 77/1000 | Loss: 0.00001486
Iteration 78/1000 | Loss: 0.00001486
Iteration 79/1000 | Loss: 0.00001486
Iteration 80/1000 | Loss: 0.00001485
Iteration 81/1000 | Loss: 0.00001485
Iteration 82/1000 | Loss: 0.00001485
Iteration 83/1000 | Loss: 0.00001485
Iteration 84/1000 | Loss: 0.00001485
Iteration 85/1000 | Loss: 0.00001485
Iteration 86/1000 | Loss: 0.00001485
Iteration 87/1000 | Loss: 0.00001485
Iteration 88/1000 | Loss: 0.00001485
Iteration 89/1000 | Loss: 0.00001485
Iteration 90/1000 | Loss: 0.00001484
Iteration 91/1000 | Loss: 0.00001484
Iteration 92/1000 | Loss: 0.00001484
Iteration 93/1000 | Loss: 0.00001484
Iteration 94/1000 | Loss: 0.00001484
Iteration 95/1000 | Loss: 0.00001484
Iteration 96/1000 | Loss: 0.00001484
Iteration 97/1000 | Loss: 0.00001484
Iteration 98/1000 | Loss: 0.00001484
Iteration 99/1000 | Loss: 0.00001484
Iteration 100/1000 | Loss: 0.00001484
Iteration 101/1000 | Loss: 0.00001483
Iteration 102/1000 | Loss: 0.00001483
Iteration 103/1000 | Loss: 0.00001483
Iteration 104/1000 | Loss: 0.00001483
Iteration 105/1000 | Loss: 0.00001483
Iteration 106/1000 | Loss: 0.00001483
Iteration 107/1000 | Loss: 0.00001482
Iteration 108/1000 | Loss: 0.00001482
Iteration 109/1000 | Loss: 0.00001482
Iteration 110/1000 | Loss: 0.00001482
Iteration 111/1000 | Loss: 0.00001482
Iteration 112/1000 | Loss: 0.00001482
Iteration 113/1000 | Loss: 0.00001481
Iteration 114/1000 | Loss: 0.00001481
Iteration 115/1000 | Loss: 0.00001481
Iteration 116/1000 | Loss: 0.00001481
Iteration 117/1000 | Loss: 0.00001481
Iteration 118/1000 | Loss: 0.00001481
Iteration 119/1000 | Loss: 0.00001480
Iteration 120/1000 | Loss: 0.00001480
Iteration 121/1000 | Loss: 0.00001480
Iteration 122/1000 | Loss: 0.00001480
Iteration 123/1000 | Loss: 0.00001480
Iteration 124/1000 | Loss: 0.00001480
Iteration 125/1000 | Loss: 0.00001480
Iteration 126/1000 | Loss: 0.00001480
Iteration 127/1000 | Loss: 0.00001480
Iteration 128/1000 | Loss: 0.00001480
Iteration 129/1000 | Loss: 0.00001480
Iteration 130/1000 | Loss: 0.00001480
Iteration 131/1000 | Loss: 0.00001480
Iteration 132/1000 | Loss: 0.00001480
Iteration 133/1000 | Loss: 0.00001480
Iteration 134/1000 | Loss: 0.00001480
Iteration 135/1000 | Loss: 0.00001480
Iteration 136/1000 | Loss: 0.00001480
Iteration 137/1000 | Loss: 0.00001480
Iteration 138/1000 | Loss: 0.00001480
Iteration 139/1000 | Loss: 0.00001480
Iteration 140/1000 | Loss: 0.00001480
Iteration 141/1000 | Loss: 0.00001480
Iteration 142/1000 | Loss: 0.00001480
Iteration 143/1000 | Loss: 0.00001480
Iteration 144/1000 | Loss: 0.00001480
Iteration 145/1000 | Loss: 0.00001480
Iteration 146/1000 | Loss: 0.00001480
Iteration 147/1000 | Loss: 0.00001480
Iteration 148/1000 | Loss: 0.00001480
Iteration 149/1000 | Loss: 0.00001480
Iteration 150/1000 | Loss: 0.00001480
Iteration 151/1000 | Loss: 0.00001480
Iteration 152/1000 | Loss: 0.00001480
Iteration 153/1000 | Loss: 0.00001480
Iteration 154/1000 | Loss: 0.00001480
Iteration 155/1000 | Loss: 0.00001480
Iteration 156/1000 | Loss: 0.00001480
Iteration 157/1000 | Loss: 0.00001480
Iteration 158/1000 | Loss: 0.00001480
Iteration 159/1000 | Loss: 0.00001479
Iteration 160/1000 | Loss: 0.00001479
Iteration 161/1000 | Loss: 0.00001479
Iteration 162/1000 | Loss: 0.00001479
Iteration 163/1000 | Loss: 0.00001479
Iteration 164/1000 | Loss: 0.00001479
Iteration 165/1000 | Loss: 0.00001479
Iteration 166/1000 | Loss: 0.00001479
Iteration 167/1000 | Loss: 0.00001479
Iteration 168/1000 | Loss: 0.00001479
Iteration 169/1000 | Loss: 0.00001479
Iteration 170/1000 | Loss: 0.00001479
Iteration 171/1000 | Loss: 0.00001479
Iteration 172/1000 | Loss: 0.00001479
Iteration 173/1000 | Loss: 0.00001479
Iteration 174/1000 | Loss: 0.00001479
Iteration 175/1000 | Loss: 0.00001479
Iteration 176/1000 | Loss: 0.00001479
Iteration 177/1000 | Loss: 0.00001479
Iteration 178/1000 | Loss: 0.00001479
Iteration 179/1000 | Loss: 0.00001479
Iteration 180/1000 | Loss: 0.00001479
Iteration 181/1000 | Loss: 0.00001479
Iteration 182/1000 | Loss: 0.00001479
Iteration 183/1000 | Loss: 0.00001479
Iteration 184/1000 | Loss: 0.00001479
Iteration 185/1000 | Loss: 0.00001479
Iteration 186/1000 | Loss: 0.00001479
Iteration 187/1000 | Loss: 0.00001479
Iteration 188/1000 | Loss: 0.00001479
Iteration 189/1000 | Loss: 0.00001479
Iteration 190/1000 | Loss: 0.00001479
Iteration 191/1000 | Loss: 0.00001479
Iteration 192/1000 | Loss: 0.00001479
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 192. Stopping optimization.
Last 5 losses: [1.4793102309340611e-05, 1.4793102309340611e-05, 1.4793102309340611e-05, 1.4793102309340611e-05, 1.4793102309340611e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4793102309340611e-05

Optimization complete. Final v2v error: 3.1615490913391113 mm

Highest mean error: 6.988204002380371 mm for frame 57

Lowest mean error: 2.816742181777954 mm for frame 160

Saving results

Total time: 73.8648567199707
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_025/1033/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_025/1033.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_025/1033
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00514199
Iteration 2/25 | Loss: 0.00135166
Iteration 3/25 | Loss: 0.00126225
Iteration 4/25 | Loss: 0.00125196
Iteration 5/25 | Loss: 0.00125035
Iteration 6/25 | Loss: 0.00125035
Iteration 7/25 | Loss: 0.00125035
Iteration 8/25 | Loss: 0.00125035
Iteration 9/25 | Loss: 0.00125035
Iteration 10/25 | Loss: 0.00125035
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0012503501493483782, 0.0012503501493483782, 0.0012503501493483782, 0.0012503501493483782, 0.0012503501493483782]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012503501493483782

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.86757517
Iteration 2/25 | Loss: 0.00153754
Iteration 3/25 | Loss: 0.00153753
Iteration 4/25 | Loss: 0.00153753
Iteration 5/25 | Loss: 0.00153753
Iteration 6/25 | Loss: 0.00153753
Iteration 7/25 | Loss: 0.00153753
Iteration 8/25 | Loss: 0.00153753
Iteration 9/25 | Loss: 0.00153753
Iteration 10/25 | Loss: 0.00153753
Iteration 11/25 | Loss: 0.00153753
Iteration 12/25 | Loss: 0.00153753
Iteration 13/25 | Loss: 0.00153753
Iteration 14/25 | Loss: 0.00153753
Iteration 15/25 | Loss: 0.00153753
Iteration 16/25 | Loss: 0.00153753
Iteration 17/25 | Loss: 0.00153753
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0015375282382592559, 0.0015375282382592559, 0.0015375282382592559, 0.0015375282382592559, 0.0015375282382592559]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015375282382592559

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00153753
Iteration 2/1000 | Loss: 0.00002699
Iteration 3/1000 | Loss: 0.00002011
Iteration 4/1000 | Loss: 0.00001868
Iteration 5/1000 | Loss: 0.00001771
Iteration 6/1000 | Loss: 0.00001708
Iteration 7/1000 | Loss: 0.00001665
Iteration 8/1000 | Loss: 0.00001606
Iteration 9/1000 | Loss: 0.00001571
Iteration 10/1000 | Loss: 0.00001538
Iteration 11/1000 | Loss: 0.00001510
Iteration 12/1000 | Loss: 0.00001500
Iteration 13/1000 | Loss: 0.00001491
Iteration 14/1000 | Loss: 0.00001478
Iteration 15/1000 | Loss: 0.00001471
Iteration 16/1000 | Loss: 0.00001465
Iteration 17/1000 | Loss: 0.00001464
Iteration 18/1000 | Loss: 0.00001459
Iteration 19/1000 | Loss: 0.00001455
Iteration 20/1000 | Loss: 0.00001455
Iteration 21/1000 | Loss: 0.00001454
Iteration 22/1000 | Loss: 0.00001450
Iteration 23/1000 | Loss: 0.00001449
Iteration 24/1000 | Loss: 0.00001448
Iteration 25/1000 | Loss: 0.00001447
Iteration 26/1000 | Loss: 0.00001447
Iteration 27/1000 | Loss: 0.00001447
Iteration 28/1000 | Loss: 0.00001446
Iteration 29/1000 | Loss: 0.00001446
Iteration 30/1000 | Loss: 0.00001446
Iteration 31/1000 | Loss: 0.00001446
Iteration 32/1000 | Loss: 0.00001445
Iteration 33/1000 | Loss: 0.00001445
Iteration 34/1000 | Loss: 0.00001445
Iteration 35/1000 | Loss: 0.00001445
Iteration 36/1000 | Loss: 0.00001445
Iteration 37/1000 | Loss: 0.00001445
Iteration 38/1000 | Loss: 0.00001445
Iteration 39/1000 | Loss: 0.00001444
Iteration 40/1000 | Loss: 0.00001444
Iteration 41/1000 | Loss: 0.00001444
Iteration 42/1000 | Loss: 0.00001444
Iteration 43/1000 | Loss: 0.00001443
Iteration 44/1000 | Loss: 0.00001443
Iteration 45/1000 | Loss: 0.00001440
Iteration 46/1000 | Loss: 0.00001440
Iteration 47/1000 | Loss: 0.00001440
Iteration 48/1000 | Loss: 0.00001439
Iteration 49/1000 | Loss: 0.00001435
Iteration 50/1000 | Loss: 0.00001435
Iteration 51/1000 | Loss: 0.00001435
Iteration 52/1000 | Loss: 0.00001434
Iteration 53/1000 | Loss: 0.00001434
Iteration 54/1000 | Loss: 0.00001433
Iteration 55/1000 | Loss: 0.00001433
Iteration 56/1000 | Loss: 0.00001433
Iteration 57/1000 | Loss: 0.00001433
Iteration 58/1000 | Loss: 0.00001432
Iteration 59/1000 | Loss: 0.00001432
Iteration 60/1000 | Loss: 0.00001431
Iteration 61/1000 | Loss: 0.00001431
Iteration 62/1000 | Loss: 0.00001431
Iteration 63/1000 | Loss: 0.00001431
Iteration 64/1000 | Loss: 0.00001431
Iteration 65/1000 | Loss: 0.00001431
Iteration 66/1000 | Loss: 0.00001431
Iteration 67/1000 | Loss: 0.00001431
Iteration 68/1000 | Loss: 0.00001431
Iteration 69/1000 | Loss: 0.00001431
Iteration 70/1000 | Loss: 0.00001431
Iteration 71/1000 | Loss: 0.00001430
Iteration 72/1000 | Loss: 0.00001430
Iteration 73/1000 | Loss: 0.00001430
Iteration 74/1000 | Loss: 0.00001430
Iteration 75/1000 | Loss: 0.00001430
Iteration 76/1000 | Loss: 0.00001430
Iteration 77/1000 | Loss: 0.00001430
Iteration 78/1000 | Loss: 0.00001429
Iteration 79/1000 | Loss: 0.00001429
Iteration 80/1000 | Loss: 0.00001429
Iteration 81/1000 | Loss: 0.00001428
Iteration 82/1000 | Loss: 0.00001428
Iteration 83/1000 | Loss: 0.00001428
Iteration 84/1000 | Loss: 0.00001427
Iteration 85/1000 | Loss: 0.00001427
Iteration 86/1000 | Loss: 0.00001427
Iteration 87/1000 | Loss: 0.00001426
Iteration 88/1000 | Loss: 0.00001426
Iteration 89/1000 | Loss: 0.00001426
Iteration 90/1000 | Loss: 0.00001425
Iteration 91/1000 | Loss: 0.00001425
Iteration 92/1000 | Loss: 0.00001425
Iteration 93/1000 | Loss: 0.00001424
Iteration 94/1000 | Loss: 0.00001424
Iteration 95/1000 | Loss: 0.00001424
Iteration 96/1000 | Loss: 0.00001424
Iteration 97/1000 | Loss: 0.00001424
Iteration 98/1000 | Loss: 0.00001424
Iteration 99/1000 | Loss: 0.00001424
Iteration 100/1000 | Loss: 0.00001424
Iteration 101/1000 | Loss: 0.00001424
Iteration 102/1000 | Loss: 0.00001424
Iteration 103/1000 | Loss: 0.00001424
Iteration 104/1000 | Loss: 0.00001424
Iteration 105/1000 | Loss: 0.00001424
Iteration 106/1000 | Loss: 0.00001423
Iteration 107/1000 | Loss: 0.00001423
Iteration 108/1000 | Loss: 0.00001423
Iteration 109/1000 | Loss: 0.00001423
Iteration 110/1000 | Loss: 0.00001423
Iteration 111/1000 | Loss: 0.00001423
Iteration 112/1000 | Loss: 0.00001423
Iteration 113/1000 | Loss: 0.00001423
Iteration 114/1000 | Loss: 0.00001422
Iteration 115/1000 | Loss: 0.00001422
Iteration 116/1000 | Loss: 0.00001422
Iteration 117/1000 | Loss: 0.00001422
Iteration 118/1000 | Loss: 0.00001422
Iteration 119/1000 | Loss: 0.00001422
Iteration 120/1000 | Loss: 0.00001422
Iteration 121/1000 | Loss: 0.00001422
Iteration 122/1000 | Loss: 0.00001422
Iteration 123/1000 | Loss: 0.00001422
Iteration 124/1000 | Loss: 0.00001421
Iteration 125/1000 | Loss: 0.00001421
Iteration 126/1000 | Loss: 0.00001421
Iteration 127/1000 | Loss: 0.00001421
Iteration 128/1000 | Loss: 0.00001421
Iteration 129/1000 | Loss: 0.00001421
Iteration 130/1000 | Loss: 0.00001421
Iteration 131/1000 | Loss: 0.00001421
Iteration 132/1000 | Loss: 0.00001421
Iteration 133/1000 | Loss: 0.00001421
Iteration 134/1000 | Loss: 0.00001421
Iteration 135/1000 | Loss: 0.00001421
Iteration 136/1000 | Loss: 0.00001421
Iteration 137/1000 | Loss: 0.00001421
Iteration 138/1000 | Loss: 0.00001421
Iteration 139/1000 | Loss: 0.00001421
Iteration 140/1000 | Loss: 0.00001421
Iteration 141/1000 | Loss: 0.00001421
Iteration 142/1000 | Loss: 0.00001421
Iteration 143/1000 | Loss: 0.00001421
Iteration 144/1000 | Loss: 0.00001421
Iteration 145/1000 | Loss: 0.00001421
Iteration 146/1000 | Loss: 0.00001421
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 146. Stopping optimization.
Last 5 losses: [1.4206513696990442e-05, 1.4206513696990442e-05, 1.4206513696990442e-05, 1.4206513696990442e-05, 1.4206513696990442e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4206513696990442e-05

Optimization complete. Final v2v error: 3.2166008949279785 mm

Highest mean error: 3.7349772453308105 mm for frame 74

Lowest mean error: 2.9015862941741943 mm for frame 217

Saving results

Total time: 44.33996868133545
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_025/1005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_025/1005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_025/1005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00581509
Iteration 2/25 | Loss: 0.00133329
Iteration 3/25 | Loss: 0.00126257
Iteration 4/25 | Loss: 0.00124634
Iteration 5/25 | Loss: 0.00124107
Iteration 6/25 | Loss: 0.00123980
Iteration 7/25 | Loss: 0.00123980
Iteration 8/25 | Loss: 0.00123980
Iteration 9/25 | Loss: 0.00123980
Iteration 10/25 | Loss: 0.00123980
Iteration 11/25 | Loss: 0.00123980
Iteration 12/25 | Loss: 0.00123980
Iteration 13/25 | Loss: 0.00123980
Iteration 14/25 | Loss: 0.00123980
Iteration 15/25 | Loss: 0.00123980
Iteration 16/25 | Loss: 0.00123980
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.001239800127223134, 0.001239800127223134, 0.001239800127223134, 0.001239800127223134, 0.001239800127223134]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001239800127223134

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.08844709
Iteration 2/25 | Loss: 0.00149496
Iteration 3/25 | Loss: 0.00149496
Iteration 4/25 | Loss: 0.00149496
Iteration 5/25 | Loss: 0.00149496
Iteration 6/25 | Loss: 0.00149496
Iteration 7/25 | Loss: 0.00149496
Iteration 8/25 | Loss: 0.00149496
Iteration 9/25 | Loss: 0.00149496
Iteration 10/25 | Loss: 0.00149496
Iteration 11/25 | Loss: 0.00149496
Iteration 12/25 | Loss: 0.00149496
Iteration 13/25 | Loss: 0.00149496
Iteration 14/25 | Loss: 0.00149496
Iteration 15/25 | Loss: 0.00149496
Iteration 16/25 | Loss: 0.00149496
Iteration 17/25 | Loss: 0.00149496
Iteration 18/25 | Loss: 0.00149496
Iteration 19/25 | Loss: 0.00149496
Iteration 20/25 | Loss: 0.00149496
Iteration 21/25 | Loss: 0.00149496
Iteration 22/25 | Loss: 0.00149496
Iteration 23/25 | Loss: 0.00149496
Iteration 24/25 | Loss: 0.00149496
Iteration 25/25 | Loss: 0.00149496

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00149496
Iteration 2/1000 | Loss: 0.00002783
Iteration 3/1000 | Loss: 0.00002154
Iteration 4/1000 | Loss: 0.00002007
Iteration 5/1000 | Loss: 0.00001945
Iteration 6/1000 | Loss: 0.00001884
Iteration 7/1000 | Loss: 0.00001831
Iteration 8/1000 | Loss: 0.00001815
Iteration 9/1000 | Loss: 0.00001789
Iteration 10/1000 | Loss: 0.00001756
Iteration 11/1000 | Loss: 0.00001733
Iteration 12/1000 | Loss: 0.00001708
Iteration 13/1000 | Loss: 0.00001686
Iteration 14/1000 | Loss: 0.00001673
Iteration 15/1000 | Loss: 0.00001669
Iteration 16/1000 | Loss: 0.00001668
Iteration 17/1000 | Loss: 0.00001668
Iteration 18/1000 | Loss: 0.00001668
Iteration 19/1000 | Loss: 0.00001667
Iteration 20/1000 | Loss: 0.00001666
Iteration 21/1000 | Loss: 0.00001662
Iteration 22/1000 | Loss: 0.00001662
Iteration 23/1000 | Loss: 0.00001662
Iteration 24/1000 | Loss: 0.00001658
Iteration 25/1000 | Loss: 0.00001658
Iteration 26/1000 | Loss: 0.00001655
Iteration 27/1000 | Loss: 0.00001654
Iteration 28/1000 | Loss: 0.00001649
Iteration 29/1000 | Loss: 0.00001648
Iteration 30/1000 | Loss: 0.00001645
Iteration 31/1000 | Loss: 0.00001644
Iteration 32/1000 | Loss: 0.00001644
Iteration 33/1000 | Loss: 0.00001643
Iteration 34/1000 | Loss: 0.00001643
Iteration 35/1000 | Loss: 0.00001642
Iteration 36/1000 | Loss: 0.00001641
Iteration 37/1000 | Loss: 0.00001641
Iteration 38/1000 | Loss: 0.00001641
Iteration 39/1000 | Loss: 0.00001640
Iteration 40/1000 | Loss: 0.00001640
Iteration 41/1000 | Loss: 0.00001639
Iteration 42/1000 | Loss: 0.00001639
Iteration 43/1000 | Loss: 0.00001639
Iteration 44/1000 | Loss: 0.00001638
Iteration 45/1000 | Loss: 0.00001637
Iteration 46/1000 | Loss: 0.00001637
Iteration 47/1000 | Loss: 0.00001637
Iteration 48/1000 | Loss: 0.00001637
Iteration 49/1000 | Loss: 0.00001637
Iteration 50/1000 | Loss: 0.00001637
Iteration 51/1000 | Loss: 0.00001636
Iteration 52/1000 | Loss: 0.00001636
Iteration 53/1000 | Loss: 0.00001636
Iteration 54/1000 | Loss: 0.00001635
Iteration 55/1000 | Loss: 0.00001635
Iteration 56/1000 | Loss: 0.00001634
Iteration 57/1000 | Loss: 0.00001634
Iteration 58/1000 | Loss: 0.00001634
Iteration 59/1000 | Loss: 0.00001634
Iteration 60/1000 | Loss: 0.00001634
Iteration 61/1000 | Loss: 0.00001634
Iteration 62/1000 | Loss: 0.00001634
Iteration 63/1000 | Loss: 0.00001634
Iteration 64/1000 | Loss: 0.00001634
Iteration 65/1000 | Loss: 0.00001634
Iteration 66/1000 | Loss: 0.00001634
Iteration 67/1000 | Loss: 0.00001634
Iteration 68/1000 | Loss: 0.00001634
Iteration 69/1000 | Loss: 0.00001634
Iteration 70/1000 | Loss: 0.00001634
Iteration 71/1000 | Loss: 0.00001633
Iteration 72/1000 | Loss: 0.00001633
Iteration 73/1000 | Loss: 0.00001633
Iteration 74/1000 | Loss: 0.00001633
Iteration 75/1000 | Loss: 0.00001633
Iteration 76/1000 | Loss: 0.00001633
Iteration 77/1000 | Loss: 0.00001632
Iteration 78/1000 | Loss: 0.00001632
Iteration 79/1000 | Loss: 0.00001632
Iteration 80/1000 | Loss: 0.00001631
Iteration 81/1000 | Loss: 0.00001631
Iteration 82/1000 | Loss: 0.00001631
Iteration 83/1000 | Loss: 0.00001631
Iteration 84/1000 | Loss: 0.00001631
Iteration 85/1000 | Loss: 0.00001631
Iteration 86/1000 | Loss: 0.00001631
Iteration 87/1000 | Loss: 0.00001630
Iteration 88/1000 | Loss: 0.00001630
Iteration 89/1000 | Loss: 0.00001630
Iteration 90/1000 | Loss: 0.00001630
Iteration 91/1000 | Loss: 0.00001630
Iteration 92/1000 | Loss: 0.00001630
Iteration 93/1000 | Loss: 0.00001630
Iteration 94/1000 | Loss: 0.00001630
Iteration 95/1000 | Loss: 0.00001630
Iteration 96/1000 | Loss: 0.00001630
Iteration 97/1000 | Loss: 0.00001629
Iteration 98/1000 | Loss: 0.00001629
Iteration 99/1000 | Loss: 0.00001629
Iteration 100/1000 | Loss: 0.00001629
Iteration 101/1000 | Loss: 0.00001629
Iteration 102/1000 | Loss: 0.00001629
Iteration 103/1000 | Loss: 0.00001629
Iteration 104/1000 | Loss: 0.00001629
Iteration 105/1000 | Loss: 0.00001628
Iteration 106/1000 | Loss: 0.00001628
Iteration 107/1000 | Loss: 0.00001628
Iteration 108/1000 | Loss: 0.00001628
Iteration 109/1000 | Loss: 0.00001628
Iteration 110/1000 | Loss: 0.00001628
Iteration 111/1000 | Loss: 0.00001628
Iteration 112/1000 | Loss: 0.00001628
Iteration 113/1000 | Loss: 0.00001628
Iteration 114/1000 | Loss: 0.00001628
Iteration 115/1000 | Loss: 0.00001628
Iteration 116/1000 | Loss: 0.00001628
Iteration 117/1000 | Loss: 0.00001628
Iteration 118/1000 | Loss: 0.00001628
Iteration 119/1000 | Loss: 0.00001628
Iteration 120/1000 | Loss: 0.00001628
Iteration 121/1000 | Loss: 0.00001628
Iteration 122/1000 | Loss: 0.00001628
Iteration 123/1000 | Loss: 0.00001628
Iteration 124/1000 | Loss: 0.00001627
Iteration 125/1000 | Loss: 0.00001627
Iteration 126/1000 | Loss: 0.00001627
Iteration 127/1000 | Loss: 0.00001627
Iteration 128/1000 | Loss: 0.00001627
Iteration 129/1000 | Loss: 0.00001627
Iteration 130/1000 | Loss: 0.00001627
Iteration 131/1000 | Loss: 0.00001627
Iteration 132/1000 | Loss: 0.00001627
Iteration 133/1000 | Loss: 0.00001627
Iteration 134/1000 | Loss: 0.00001627
Iteration 135/1000 | Loss: 0.00001627
Iteration 136/1000 | Loss: 0.00001626
Iteration 137/1000 | Loss: 0.00001626
Iteration 138/1000 | Loss: 0.00001626
Iteration 139/1000 | Loss: 0.00001626
Iteration 140/1000 | Loss: 0.00001626
Iteration 141/1000 | Loss: 0.00001626
Iteration 142/1000 | Loss: 0.00001626
Iteration 143/1000 | Loss: 0.00001626
Iteration 144/1000 | Loss: 0.00001626
Iteration 145/1000 | Loss: 0.00001626
Iteration 146/1000 | Loss: 0.00001626
Iteration 147/1000 | Loss: 0.00001626
Iteration 148/1000 | Loss: 0.00001626
Iteration 149/1000 | Loss: 0.00001626
Iteration 150/1000 | Loss: 0.00001625
Iteration 151/1000 | Loss: 0.00001625
Iteration 152/1000 | Loss: 0.00001625
Iteration 153/1000 | Loss: 0.00001625
Iteration 154/1000 | Loss: 0.00001625
Iteration 155/1000 | Loss: 0.00001625
Iteration 156/1000 | Loss: 0.00001625
Iteration 157/1000 | Loss: 0.00001625
Iteration 158/1000 | Loss: 0.00001625
Iteration 159/1000 | Loss: 0.00001625
Iteration 160/1000 | Loss: 0.00001625
Iteration 161/1000 | Loss: 0.00001625
Iteration 162/1000 | Loss: 0.00001625
Iteration 163/1000 | Loss: 0.00001625
Iteration 164/1000 | Loss: 0.00001625
Iteration 165/1000 | Loss: 0.00001625
Iteration 166/1000 | Loss: 0.00001625
Iteration 167/1000 | Loss: 0.00001625
Iteration 168/1000 | Loss: 0.00001625
Iteration 169/1000 | Loss: 0.00001625
Iteration 170/1000 | Loss: 0.00001625
Iteration 171/1000 | Loss: 0.00001625
Iteration 172/1000 | Loss: 0.00001625
Iteration 173/1000 | Loss: 0.00001625
Iteration 174/1000 | Loss: 0.00001625
Iteration 175/1000 | Loss: 0.00001625
Iteration 176/1000 | Loss: 0.00001625
Iteration 177/1000 | Loss: 0.00001625
Iteration 178/1000 | Loss: 0.00001625
Iteration 179/1000 | Loss: 0.00001625
Iteration 180/1000 | Loss: 0.00001625
Iteration 181/1000 | Loss: 0.00001625
Iteration 182/1000 | Loss: 0.00001625
Iteration 183/1000 | Loss: 0.00001625
Iteration 184/1000 | Loss: 0.00001625
Iteration 185/1000 | Loss: 0.00001625
Iteration 186/1000 | Loss: 0.00001625
Iteration 187/1000 | Loss: 0.00001625
Iteration 188/1000 | Loss: 0.00001625
Iteration 189/1000 | Loss: 0.00001625
Iteration 190/1000 | Loss: 0.00001625
Iteration 191/1000 | Loss: 0.00001625
Iteration 192/1000 | Loss: 0.00001625
Iteration 193/1000 | Loss: 0.00001625
Iteration 194/1000 | Loss: 0.00001625
Iteration 195/1000 | Loss: 0.00001625
Iteration 196/1000 | Loss: 0.00001625
Iteration 197/1000 | Loss: 0.00001625
Iteration 198/1000 | Loss: 0.00001625
Iteration 199/1000 | Loss: 0.00001625
Iteration 200/1000 | Loss: 0.00001625
Iteration 201/1000 | Loss: 0.00001625
Iteration 202/1000 | Loss: 0.00001625
Iteration 203/1000 | Loss: 0.00001625
Iteration 204/1000 | Loss: 0.00001625
Iteration 205/1000 | Loss: 0.00001625
Iteration 206/1000 | Loss: 0.00001625
Iteration 207/1000 | Loss: 0.00001625
Iteration 208/1000 | Loss: 0.00001625
Iteration 209/1000 | Loss: 0.00001625
Iteration 210/1000 | Loss: 0.00001625
Iteration 211/1000 | Loss: 0.00001625
Iteration 212/1000 | Loss: 0.00001625
Iteration 213/1000 | Loss: 0.00001625
Iteration 214/1000 | Loss: 0.00001625
Iteration 215/1000 | Loss: 0.00001625
Iteration 216/1000 | Loss: 0.00001625
Iteration 217/1000 | Loss: 0.00001625
Iteration 218/1000 | Loss: 0.00001625
Iteration 219/1000 | Loss: 0.00001625
Iteration 220/1000 | Loss: 0.00001625
Iteration 221/1000 | Loss: 0.00001625
Iteration 222/1000 | Loss: 0.00001625
Iteration 223/1000 | Loss: 0.00001625
Iteration 224/1000 | Loss: 0.00001625
Iteration 225/1000 | Loss: 0.00001625
Iteration 226/1000 | Loss: 0.00001625
Iteration 227/1000 | Loss: 0.00001625
Iteration 228/1000 | Loss: 0.00001625
Iteration 229/1000 | Loss: 0.00001625
Iteration 230/1000 | Loss: 0.00001625
Iteration 231/1000 | Loss: 0.00001625
Iteration 232/1000 | Loss: 0.00001625
Iteration 233/1000 | Loss: 0.00001625
Iteration 234/1000 | Loss: 0.00001625
Iteration 235/1000 | Loss: 0.00001625
Iteration 236/1000 | Loss: 0.00001625
Iteration 237/1000 | Loss: 0.00001625
Iteration 238/1000 | Loss: 0.00001625
Iteration 239/1000 | Loss: 0.00001625
Iteration 240/1000 | Loss: 0.00001625
Iteration 241/1000 | Loss: 0.00001625
Iteration 242/1000 | Loss: 0.00001625
Iteration 243/1000 | Loss: 0.00001625
Iteration 244/1000 | Loss: 0.00001625
Iteration 245/1000 | Loss: 0.00001625
Iteration 246/1000 | Loss: 0.00001625
Iteration 247/1000 | Loss: 0.00001625
Iteration 248/1000 | Loss: 0.00001625
Iteration 249/1000 | Loss: 0.00001625
Iteration 250/1000 | Loss: 0.00001625
Iteration 251/1000 | Loss: 0.00001625
Iteration 252/1000 | Loss: 0.00001625
Iteration 253/1000 | Loss: 0.00001625
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 253. Stopping optimization.
Last 5 losses: [1.625095865165349e-05, 1.625095865165349e-05, 1.625095865165349e-05, 1.625095865165349e-05, 1.625095865165349e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.625095865165349e-05

Optimization complete. Final v2v error: 3.4598946571350098 mm

Highest mean error: 3.7320289611816406 mm for frame 95

Lowest mean error: 3.2605018615722656 mm for frame 123

Saving results

Total time: 42.23259782791138
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_025/1043/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_025/1043.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_025/1043
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00362169
Iteration 2/25 | Loss: 0.00124533
Iteration 3/25 | Loss: 0.00118319
Iteration 4/25 | Loss: 0.00117651
Iteration 5/25 | Loss: 0.00117490
Iteration 6/25 | Loss: 0.00117490
Iteration 7/25 | Loss: 0.00117490
Iteration 8/25 | Loss: 0.00117490
Iteration 9/25 | Loss: 0.00117490
Iteration 10/25 | Loss: 0.00117490
Iteration 11/25 | Loss: 0.00117490
Iteration 12/25 | Loss: 0.00117490
Iteration 13/25 | Loss: 0.00117490
Iteration 14/25 | Loss: 0.00117490
Iteration 15/25 | Loss: 0.00117490
Iteration 16/25 | Loss: 0.00117490
Iteration 17/25 | Loss: 0.00117490
Iteration 18/25 | Loss: 0.00117490
Iteration 19/25 | Loss: 0.00117490
Iteration 20/25 | Loss: 0.00117490
Iteration 21/25 | Loss: 0.00117490
Iteration 22/25 | Loss: 0.00117490
Iteration 23/25 | Loss: 0.00117490
Iteration 24/25 | Loss: 0.00117490
Iteration 25/25 | Loss: 0.00117490
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0011749000987038016, 0.0011749000987038016, 0.0011749000987038016, 0.0011749000987038016, 0.0011749000987038016]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011749000987038016

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36163878
Iteration 2/25 | Loss: 0.00148986
Iteration 3/25 | Loss: 0.00148985
Iteration 4/25 | Loss: 0.00148985
Iteration 5/25 | Loss: 0.00148985
Iteration 6/25 | Loss: 0.00148985
Iteration 7/25 | Loss: 0.00148985
Iteration 8/25 | Loss: 0.00148985
Iteration 9/25 | Loss: 0.00148985
Iteration 10/25 | Loss: 0.00148985
Iteration 11/25 | Loss: 0.00148985
Iteration 12/25 | Loss: 0.00148985
Iteration 13/25 | Loss: 0.00148985
Iteration 14/25 | Loss: 0.00148985
Iteration 15/25 | Loss: 0.00148985
Iteration 16/25 | Loss: 0.00148985
Iteration 17/25 | Loss: 0.00148985
Iteration 18/25 | Loss: 0.00148985
Iteration 19/25 | Loss: 0.00148985
Iteration 20/25 | Loss: 0.00148985
Iteration 21/25 | Loss: 0.00148985
Iteration 22/25 | Loss: 0.00148985
Iteration 23/25 | Loss: 0.00148985
Iteration 24/25 | Loss: 0.00148985
Iteration 25/25 | Loss: 0.00148985

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00148985
Iteration 2/1000 | Loss: 0.00002195
Iteration 3/1000 | Loss: 0.00001519
Iteration 4/1000 | Loss: 0.00001211
Iteration 5/1000 | Loss: 0.00001119
Iteration 6/1000 | Loss: 0.00001057
Iteration 7/1000 | Loss: 0.00001005
Iteration 8/1000 | Loss: 0.00000977
Iteration 9/1000 | Loss: 0.00000949
Iteration 10/1000 | Loss: 0.00000921
Iteration 11/1000 | Loss: 0.00000913
Iteration 12/1000 | Loss: 0.00000911
Iteration 13/1000 | Loss: 0.00000902
Iteration 14/1000 | Loss: 0.00000898
Iteration 15/1000 | Loss: 0.00000895
Iteration 16/1000 | Loss: 0.00000894
Iteration 17/1000 | Loss: 0.00000891
Iteration 18/1000 | Loss: 0.00000891
Iteration 19/1000 | Loss: 0.00000889
Iteration 20/1000 | Loss: 0.00000889
Iteration 21/1000 | Loss: 0.00000878
Iteration 22/1000 | Loss: 0.00000874
Iteration 23/1000 | Loss: 0.00000869
Iteration 24/1000 | Loss: 0.00000865
Iteration 25/1000 | Loss: 0.00000863
Iteration 26/1000 | Loss: 0.00000861
Iteration 27/1000 | Loss: 0.00000857
Iteration 28/1000 | Loss: 0.00000854
Iteration 29/1000 | Loss: 0.00000853
Iteration 30/1000 | Loss: 0.00000853
Iteration 31/1000 | Loss: 0.00000852
Iteration 32/1000 | Loss: 0.00000852
Iteration 33/1000 | Loss: 0.00000851
Iteration 34/1000 | Loss: 0.00000851
Iteration 35/1000 | Loss: 0.00000851
Iteration 36/1000 | Loss: 0.00000850
Iteration 37/1000 | Loss: 0.00000850
Iteration 38/1000 | Loss: 0.00000844
Iteration 39/1000 | Loss: 0.00000844
Iteration 40/1000 | Loss: 0.00000843
Iteration 41/1000 | Loss: 0.00000842
Iteration 42/1000 | Loss: 0.00000842
Iteration 43/1000 | Loss: 0.00000842
Iteration 44/1000 | Loss: 0.00000841
Iteration 45/1000 | Loss: 0.00000840
Iteration 46/1000 | Loss: 0.00000840
Iteration 47/1000 | Loss: 0.00000839
Iteration 48/1000 | Loss: 0.00000838
Iteration 49/1000 | Loss: 0.00000837
Iteration 50/1000 | Loss: 0.00000837
Iteration 51/1000 | Loss: 0.00000837
Iteration 52/1000 | Loss: 0.00000837
Iteration 53/1000 | Loss: 0.00000836
Iteration 54/1000 | Loss: 0.00000836
Iteration 55/1000 | Loss: 0.00000835
Iteration 56/1000 | Loss: 0.00000835
Iteration 57/1000 | Loss: 0.00000835
Iteration 58/1000 | Loss: 0.00000834
Iteration 59/1000 | Loss: 0.00000834
Iteration 60/1000 | Loss: 0.00000833
Iteration 61/1000 | Loss: 0.00000833
Iteration 62/1000 | Loss: 0.00000832
Iteration 63/1000 | Loss: 0.00000832
Iteration 64/1000 | Loss: 0.00000831
Iteration 65/1000 | Loss: 0.00000831
Iteration 66/1000 | Loss: 0.00000830
Iteration 67/1000 | Loss: 0.00000830
Iteration 68/1000 | Loss: 0.00000830
Iteration 69/1000 | Loss: 0.00000830
Iteration 70/1000 | Loss: 0.00000830
Iteration 71/1000 | Loss: 0.00000830
Iteration 72/1000 | Loss: 0.00000828
Iteration 73/1000 | Loss: 0.00000828
Iteration 74/1000 | Loss: 0.00000828
Iteration 75/1000 | Loss: 0.00000827
Iteration 76/1000 | Loss: 0.00000827
Iteration 77/1000 | Loss: 0.00000827
Iteration 78/1000 | Loss: 0.00000826
Iteration 79/1000 | Loss: 0.00000826
Iteration 80/1000 | Loss: 0.00000826
Iteration 81/1000 | Loss: 0.00000826
Iteration 82/1000 | Loss: 0.00000825
Iteration 83/1000 | Loss: 0.00000824
Iteration 84/1000 | Loss: 0.00000824
Iteration 85/1000 | Loss: 0.00000824
Iteration 86/1000 | Loss: 0.00000824
Iteration 87/1000 | Loss: 0.00000824
Iteration 88/1000 | Loss: 0.00000823
Iteration 89/1000 | Loss: 0.00000823
Iteration 90/1000 | Loss: 0.00000823
Iteration 91/1000 | Loss: 0.00000823
Iteration 92/1000 | Loss: 0.00000823
Iteration 93/1000 | Loss: 0.00000823
Iteration 94/1000 | Loss: 0.00000823
Iteration 95/1000 | Loss: 0.00000823
Iteration 96/1000 | Loss: 0.00000823
Iteration 97/1000 | Loss: 0.00000823
Iteration 98/1000 | Loss: 0.00000823
Iteration 99/1000 | Loss: 0.00000822
Iteration 100/1000 | Loss: 0.00000822
Iteration 101/1000 | Loss: 0.00000822
Iteration 102/1000 | Loss: 0.00000822
Iteration 103/1000 | Loss: 0.00000822
Iteration 104/1000 | Loss: 0.00000822
Iteration 105/1000 | Loss: 0.00000822
Iteration 106/1000 | Loss: 0.00000822
Iteration 107/1000 | Loss: 0.00000822
Iteration 108/1000 | Loss: 0.00000822
Iteration 109/1000 | Loss: 0.00000822
Iteration 110/1000 | Loss: 0.00000822
Iteration 111/1000 | Loss: 0.00000822
Iteration 112/1000 | Loss: 0.00000821
Iteration 113/1000 | Loss: 0.00000821
Iteration 114/1000 | Loss: 0.00000821
Iteration 115/1000 | Loss: 0.00000821
Iteration 116/1000 | Loss: 0.00000821
Iteration 117/1000 | Loss: 0.00000821
Iteration 118/1000 | Loss: 0.00000821
Iteration 119/1000 | Loss: 0.00000820
Iteration 120/1000 | Loss: 0.00000820
Iteration 121/1000 | Loss: 0.00000820
Iteration 122/1000 | Loss: 0.00000820
Iteration 123/1000 | Loss: 0.00000820
Iteration 124/1000 | Loss: 0.00000820
Iteration 125/1000 | Loss: 0.00000820
Iteration 126/1000 | Loss: 0.00000820
Iteration 127/1000 | Loss: 0.00000819
Iteration 128/1000 | Loss: 0.00000819
Iteration 129/1000 | Loss: 0.00000819
Iteration 130/1000 | Loss: 0.00000819
Iteration 131/1000 | Loss: 0.00000819
Iteration 132/1000 | Loss: 0.00000819
Iteration 133/1000 | Loss: 0.00000819
Iteration 134/1000 | Loss: 0.00000819
Iteration 135/1000 | Loss: 0.00000819
Iteration 136/1000 | Loss: 0.00000819
Iteration 137/1000 | Loss: 0.00000819
Iteration 138/1000 | Loss: 0.00000818
Iteration 139/1000 | Loss: 0.00000818
Iteration 140/1000 | Loss: 0.00000818
Iteration 141/1000 | Loss: 0.00000818
Iteration 142/1000 | Loss: 0.00000818
Iteration 143/1000 | Loss: 0.00000818
Iteration 144/1000 | Loss: 0.00000818
Iteration 145/1000 | Loss: 0.00000817
Iteration 146/1000 | Loss: 0.00000817
Iteration 147/1000 | Loss: 0.00000817
Iteration 148/1000 | Loss: 0.00000817
Iteration 149/1000 | Loss: 0.00000816
Iteration 150/1000 | Loss: 0.00000816
Iteration 151/1000 | Loss: 0.00000816
Iteration 152/1000 | Loss: 0.00000815
Iteration 153/1000 | Loss: 0.00000815
Iteration 154/1000 | Loss: 0.00000814
Iteration 155/1000 | Loss: 0.00000814
Iteration 156/1000 | Loss: 0.00000814
Iteration 157/1000 | Loss: 0.00000814
Iteration 158/1000 | Loss: 0.00000814
Iteration 159/1000 | Loss: 0.00000814
Iteration 160/1000 | Loss: 0.00000813
Iteration 161/1000 | Loss: 0.00000813
Iteration 162/1000 | Loss: 0.00000813
Iteration 163/1000 | Loss: 0.00000813
Iteration 164/1000 | Loss: 0.00000813
Iteration 165/1000 | Loss: 0.00000813
Iteration 166/1000 | Loss: 0.00000812
Iteration 167/1000 | Loss: 0.00000812
Iteration 168/1000 | Loss: 0.00000812
Iteration 169/1000 | Loss: 0.00000812
Iteration 170/1000 | Loss: 0.00000812
Iteration 171/1000 | Loss: 0.00000812
Iteration 172/1000 | Loss: 0.00000811
Iteration 173/1000 | Loss: 0.00000811
Iteration 174/1000 | Loss: 0.00000811
Iteration 175/1000 | Loss: 0.00000811
Iteration 176/1000 | Loss: 0.00000811
Iteration 177/1000 | Loss: 0.00000811
Iteration 178/1000 | Loss: 0.00000811
Iteration 179/1000 | Loss: 0.00000811
Iteration 180/1000 | Loss: 0.00000811
Iteration 181/1000 | Loss: 0.00000811
Iteration 182/1000 | Loss: 0.00000811
Iteration 183/1000 | Loss: 0.00000811
Iteration 184/1000 | Loss: 0.00000811
Iteration 185/1000 | Loss: 0.00000811
Iteration 186/1000 | Loss: 0.00000811
Iteration 187/1000 | Loss: 0.00000810
Iteration 188/1000 | Loss: 0.00000810
Iteration 189/1000 | Loss: 0.00000810
Iteration 190/1000 | Loss: 0.00000810
Iteration 191/1000 | Loss: 0.00000810
Iteration 192/1000 | Loss: 0.00000810
Iteration 193/1000 | Loss: 0.00000810
Iteration 194/1000 | Loss: 0.00000810
Iteration 195/1000 | Loss: 0.00000810
Iteration 196/1000 | Loss: 0.00000810
Iteration 197/1000 | Loss: 0.00000810
Iteration 198/1000 | Loss: 0.00000810
Iteration 199/1000 | Loss: 0.00000810
Iteration 200/1000 | Loss: 0.00000810
Iteration 201/1000 | Loss: 0.00000809
Iteration 202/1000 | Loss: 0.00000809
Iteration 203/1000 | Loss: 0.00000809
Iteration 204/1000 | Loss: 0.00000809
Iteration 205/1000 | Loss: 0.00000809
Iteration 206/1000 | Loss: 0.00000809
Iteration 207/1000 | Loss: 0.00000809
Iteration 208/1000 | Loss: 0.00000809
Iteration 209/1000 | Loss: 0.00000809
Iteration 210/1000 | Loss: 0.00000809
Iteration 211/1000 | Loss: 0.00000809
Iteration 212/1000 | Loss: 0.00000809
Iteration 213/1000 | Loss: 0.00000809
Iteration 214/1000 | Loss: 0.00000809
Iteration 215/1000 | Loss: 0.00000809
Iteration 216/1000 | Loss: 0.00000809
Iteration 217/1000 | Loss: 0.00000809
Iteration 218/1000 | Loss: 0.00000808
Iteration 219/1000 | Loss: 0.00000808
Iteration 220/1000 | Loss: 0.00000808
Iteration 221/1000 | Loss: 0.00000808
Iteration 222/1000 | Loss: 0.00000808
Iteration 223/1000 | Loss: 0.00000808
Iteration 224/1000 | Loss: 0.00000808
Iteration 225/1000 | Loss: 0.00000807
Iteration 226/1000 | Loss: 0.00000807
Iteration 227/1000 | Loss: 0.00000807
Iteration 228/1000 | Loss: 0.00000807
Iteration 229/1000 | Loss: 0.00000807
Iteration 230/1000 | Loss: 0.00000807
Iteration 231/1000 | Loss: 0.00000807
Iteration 232/1000 | Loss: 0.00000807
Iteration 233/1000 | Loss: 0.00000807
Iteration 234/1000 | Loss: 0.00000807
Iteration 235/1000 | Loss: 0.00000807
Iteration 236/1000 | Loss: 0.00000807
Iteration 237/1000 | Loss: 0.00000807
Iteration 238/1000 | Loss: 0.00000807
Iteration 239/1000 | Loss: 0.00000807
Iteration 240/1000 | Loss: 0.00000807
Iteration 241/1000 | Loss: 0.00000807
Iteration 242/1000 | Loss: 0.00000807
Iteration 243/1000 | Loss: 0.00000807
Iteration 244/1000 | Loss: 0.00000807
Iteration 245/1000 | Loss: 0.00000807
Iteration 246/1000 | Loss: 0.00000807
Iteration 247/1000 | Loss: 0.00000807
Iteration 248/1000 | Loss: 0.00000807
Iteration 249/1000 | Loss: 0.00000807
Iteration 250/1000 | Loss: 0.00000807
Iteration 251/1000 | Loss: 0.00000807
Iteration 252/1000 | Loss: 0.00000807
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 252. Stopping optimization.
Last 5 losses: [8.070947842497844e-06, 8.070947842497844e-06, 8.070947842497844e-06, 8.070947842497844e-06, 8.070947842497844e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.070947842497844e-06

Optimization complete. Final v2v error: 2.4765868186950684 mm

Highest mean error: 2.5697720050811768 mm for frame 120

Lowest mean error: 2.4298226833343506 mm for frame 146

Saving results

Total time: 44.92780041694641
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_025/1026/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_025/1026.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_025/1026
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00472280
Iteration 2/25 | Loss: 0.00135396
Iteration 3/25 | Loss: 0.00126553
Iteration 4/25 | Loss: 0.00125216
Iteration 5/25 | Loss: 0.00124602
Iteration 6/25 | Loss: 0.00124529
Iteration 7/25 | Loss: 0.00124529
Iteration 8/25 | Loss: 0.00124529
Iteration 9/25 | Loss: 0.00124529
Iteration 10/25 | Loss: 0.00124529
Iteration 11/25 | Loss: 0.00124529
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001245294464752078, 0.001245294464752078, 0.001245294464752078, 0.001245294464752078, 0.001245294464752078]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001245294464752078

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.71968287
Iteration 2/25 | Loss: 0.00145591
Iteration 3/25 | Loss: 0.00145591
Iteration 4/25 | Loss: 0.00145591
Iteration 5/25 | Loss: 0.00145591
Iteration 6/25 | Loss: 0.00145591
Iteration 7/25 | Loss: 0.00145591
Iteration 8/25 | Loss: 0.00145591
Iteration 9/25 | Loss: 0.00145591
Iteration 10/25 | Loss: 0.00145591
Iteration 11/25 | Loss: 0.00145591
Iteration 12/25 | Loss: 0.00145591
Iteration 13/25 | Loss: 0.00145591
Iteration 14/25 | Loss: 0.00145591
Iteration 15/25 | Loss: 0.00145591
Iteration 16/25 | Loss: 0.00145591
Iteration 17/25 | Loss: 0.00145591
Iteration 18/25 | Loss: 0.00145591
Iteration 19/25 | Loss: 0.00145591
Iteration 20/25 | Loss: 0.00145591
Iteration 21/25 | Loss: 0.00145591
Iteration 22/25 | Loss: 0.00145591
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0014559070114046335, 0.0014559070114046335, 0.0014559070114046335, 0.0014559070114046335, 0.0014559070114046335]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014559070114046335

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00145591
Iteration 2/1000 | Loss: 0.00003442
Iteration 3/1000 | Loss: 0.00002657
Iteration 4/1000 | Loss: 0.00002418
Iteration 5/1000 | Loss: 0.00002309
Iteration 6/1000 | Loss: 0.00002224
Iteration 7/1000 | Loss: 0.00002160
Iteration 8/1000 | Loss: 0.00002115
Iteration 9/1000 | Loss: 0.00002071
Iteration 10/1000 | Loss: 0.00002038
Iteration 11/1000 | Loss: 0.00002009
Iteration 12/1000 | Loss: 0.00001990
Iteration 13/1000 | Loss: 0.00001969
Iteration 14/1000 | Loss: 0.00001959
Iteration 15/1000 | Loss: 0.00001958
Iteration 16/1000 | Loss: 0.00001952
Iteration 17/1000 | Loss: 0.00001938
Iteration 18/1000 | Loss: 0.00001921
Iteration 19/1000 | Loss: 0.00001917
Iteration 20/1000 | Loss: 0.00001917
Iteration 21/1000 | Loss: 0.00001912
Iteration 22/1000 | Loss: 0.00001910
Iteration 23/1000 | Loss: 0.00001909
Iteration 24/1000 | Loss: 0.00001908
Iteration 25/1000 | Loss: 0.00001905
Iteration 26/1000 | Loss: 0.00001900
Iteration 27/1000 | Loss: 0.00001900
Iteration 28/1000 | Loss: 0.00001898
Iteration 29/1000 | Loss: 0.00001898
Iteration 30/1000 | Loss: 0.00001898
Iteration 31/1000 | Loss: 0.00001897
Iteration 32/1000 | Loss: 0.00001897
Iteration 33/1000 | Loss: 0.00001889
Iteration 34/1000 | Loss: 0.00001886
Iteration 35/1000 | Loss: 0.00001885
Iteration 36/1000 | Loss: 0.00001881
Iteration 37/1000 | Loss: 0.00001877
Iteration 38/1000 | Loss: 0.00001875
Iteration 39/1000 | Loss: 0.00001874
Iteration 40/1000 | Loss: 0.00001874
Iteration 41/1000 | Loss: 0.00001873
Iteration 42/1000 | Loss: 0.00001873
Iteration 43/1000 | Loss: 0.00001872
Iteration 44/1000 | Loss: 0.00001872
Iteration 45/1000 | Loss: 0.00001872
Iteration 46/1000 | Loss: 0.00001871
Iteration 47/1000 | Loss: 0.00001871
Iteration 48/1000 | Loss: 0.00001870
Iteration 49/1000 | Loss: 0.00001870
Iteration 50/1000 | Loss: 0.00001870
Iteration 51/1000 | Loss: 0.00001869
Iteration 52/1000 | Loss: 0.00001869
Iteration 53/1000 | Loss: 0.00001869
Iteration 54/1000 | Loss: 0.00001869
Iteration 55/1000 | Loss: 0.00001868
Iteration 56/1000 | Loss: 0.00001867
Iteration 57/1000 | Loss: 0.00001867
Iteration 58/1000 | Loss: 0.00001867
Iteration 59/1000 | Loss: 0.00001867
Iteration 60/1000 | Loss: 0.00001867
Iteration 61/1000 | Loss: 0.00001867
Iteration 62/1000 | Loss: 0.00001867
Iteration 63/1000 | Loss: 0.00001867
Iteration 64/1000 | Loss: 0.00001867
Iteration 65/1000 | Loss: 0.00001866
Iteration 66/1000 | Loss: 0.00001866
Iteration 67/1000 | Loss: 0.00001866
Iteration 68/1000 | Loss: 0.00001865
Iteration 69/1000 | Loss: 0.00001865
Iteration 70/1000 | Loss: 0.00001865
Iteration 71/1000 | Loss: 0.00001865
Iteration 72/1000 | Loss: 0.00001864
Iteration 73/1000 | Loss: 0.00001861
Iteration 74/1000 | Loss: 0.00001861
Iteration 75/1000 | Loss: 0.00001859
Iteration 76/1000 | Loss: 0.00001857
Iteration 77/1000 | Loss: 0.00001857
Iteration 78/1000 | Loss: 0.00001857
Iteration 79/1000 | Loss: 0.00001857
Iteration 80/1000 | Loss: 0.00001857
Iteration 81/1000 | Loss: 0.00001857
Iteration 82/1000 | Loss: 0.00001856
Iteration 83/1000 | Loss: 0.00001856
Iteration 84/1000 | Loss: 0.00001856
Iteration 85/1000 | Loss: 0.00001856
Iteration 86/1000 | Loss: 0.00001856
Iteration 87/1000 | Loss: 0.00001855
Iteration 88/1000 | Loss: 0.00001854
Iteration 89/1000 | Loss: 0.00001853
Iteration 90/1000 | Loss: 0.00001851
Iteration 91/1000 | Loss: 0.00001851
Iteration 92/1000 | Loss: 0.00001850
Iteration 93/1000 | Loss: 0.00001850
Iteration 94/1000 | Loss: 0.00001850
Iteration 95/1000 | Loss: 0.00001849
Iteration 96/1000 | Loss: 0.00001848
Iteration 97/1000 | Loss: 0.00001847
Iteration 98/1000 | Loss: 0.00001846
Iteration 99/1000 | Loss: 0.00001846
Iteration 100/1000 | Loss: 0.00001845
Iteration 101/1000 | Loss: 0.00001845
Iteration 102/1000 | Loss: 0.00001844
Iteration 103/1000 | Loss: 0.00001844
Iteration 104/1000 | Loss: 0.00001844
Iteration 105/1000 | Loss: 0.00001843
Iteration 106/1000 | Loss: 0.00001842
Iteration 107/1000 | Loss: 0.00001842
Iteration 108/1000 | Loss: 0.00001842
Iteration 109/1000 | Loss: 0.00001842
Iteration 110/1000 | Loss: 0.00001841
Iteration 111/1000 | Loss: 0.00001841
Iteration 112/1000 | Loss: 0.00001841
Iteration 113/1000 | Loss: 0.00001841
Iteration 114/1000 | Loss: 0.00001840
Iteration 115/1000 | Loss: 0.00001840
Iteration 116/1000 | Loss: 0.00001840
Iteration 117/1000 | Loss: 0.00001839
Iteration 118/1000 | Loss: 0.00001839
Iteration 119/1000 | Loss: 0.00001839
Iteration 120/1000 | Loss: 0.00001839
Iteration 121/1000 | Loss: 0.00001839
Iteration 122/1000 | Loss: 0.00001839
Iteration 123/1000 | Loss: 0.00001838
Iteration 124/1000 | Loss: 0.00001838
Iteration 125/1000 | Loss: 0.00001838
Iteration 126/1000 | Loss: 0.00001838
Iteration 127/1000 | Loss: 0.00001837
Iteration 128/1000 | Loss: 0.00001837
Iteration 129/1000 | Loss: 0.00001837
Iteration 130/1000 | Loss: 0.00001837
Iteration 131/1000 | Loss: 0.00001837
Iteration 132/1000 | Loss: 0.00001837
Iteration 133/1000 | Loss: 0.00001837
Iteration 134/1000 | Loss: 0.00001837
Iteration 135/1000 | Loss: 0.00001837
Iteration 136/1000 | Loss: 0.00001837
Iteration 137/1000 | Loss: 0.00001837
Iteration 138/1000 | Loss: 0.00001837
Iteration 139/1000 | Loss: 0.00001836
Iteration 140/1000 | Loss: 0.00001836
Iteration 141/1000 | Loss: 0.00001836
Iteration 142/1000 | Loss: 0.00001836
Iteration 143/1000 | Loss: 0.00001836
Iteration 144/1000 | Loss: 0.00001836
Iteration 145/1000 | Loss: 0.00001836
Iteration 146/1000 | Loss: 0.00001836
Iteration 147/1000 | Loss: 0.00001836
Iteration 148/1000 | Loss: 0.00001836
Iteration 149/1000 | Loss: 0.00001836
Iteration 150/1000 | Loss: 0.00001835
Iteration 151/1000 | Loss: 0.00001835
Iteration 152/1000 | Loss: 0.00001835
Iteration 153/1000 | Loss: 0.00001835
Iteration 154/1000 | Loss: 0.00001835
Iteration 155/1000 | Loss: 0.00001835
Iteration 156/1000 | Loss: 0.00001835
Iteration 157/1000 | Loss: 0.00001835
Iteration 158/1000 | Loss: 0.00001835
Iteration 159/1000 | Loss: 0.00001834
Iteration 160/1000 | Loss: 0.00001834
Iteration 161/1000 | Loss: 0.00001834
Iteration 162/1000 | Loss: 0.00001834
Iteration 163/1000 | Loss: 0.00001834
Iteration 164/1000 | Loss: 0.00001834
Iteration 165/1000 | Loss: 0.00001834
Iteration 166/1000 | Loss: 0.00001834
Iteration 167/1000 | Loss: 0.00001834
Iteration 168/1000 | Loss: 0.00001834
Iteration 169/1000 | Loss: 0.00001834
Iteration 170/1000 | Loss: 0.00001833
Iteration 171/1000 | Loss: 0.00001833
Iteration 172/1000 | Loss: 0.00001833
Iteration 173/1000 | Loss: 0.00001833
Iteration 174/1000 | Loss: 0.00001833
Iteration 175/1000 | Loss: 0.00001833
Iteration 176/1000 | Loss: 0.00001833
Iteration 177/1000 | Loss: 0.00001833
Iteration 178/1000 | Loss: 0.00001833
Iteration 179/1000 | Loss: 0.00001833
Iteration 180/1000 | Loss: 0.00001832
Iteration 181/1000 | Loss: 0.00001832
Iteration 182/1000 | Loss: 0.00001832
Iteration 183/1000 | Loss: 0.00001832
Iteration 184/1000 | Loss: 0.00001832
Iteration 185/1000 | Loss: 0.00001832
Iteration 186/1000 | Loss: 0.00001832
Iteration 187/1000 | Loss: 0.00001832
Iteration 188/1000 | Loss: 0.00001832
Iteration 189/1000 | Loss: 0.00001832
Iteration 190/1000 | Loss: 0.00001832
Iteration 191/1000 | Loss: 0.00001832
Iteration 192/1000 | Loss: 0.00001832
Iteration 193/1000 | Loss: 0.00001832
Iteration 194/1000 | Loss: 0.00001832
Iteration 195/1000 | Loss: 0.00001832
Iteration 196/1000 | Loss: 0.00001832
Iteration 197/1000 | Loss: 0.00001832
Iteration 198/1000 | Loss: 0.00001832
Iteration 199/1000 | Loss: 0.00001832
Iteration 200/1000 | Loss: 0.00001832
Iteration 201/1000 | Loss: 0.00001832
Iteration 202/1000 | Loss: 0.00001832
Iteration 203/1000 | Loss: 0.00001832
Iteration 204/1000 | Loss: 0.00001832
Iteration 205/1000 | Loss: 0.00001832
Iteration 206/1000 | Loss: 0.00001832
Iteration 207/1000 | Loss: 0.00001832
Iteration 208/1000 | Loss: 0.00001832
Iteration 209/1000 | Loss: 0.00001832
Iteration 210/1000 | Loss: 0.00001832
Iteration 211/1000 | Loss: 0.00001832
Iteration 212/1000 | Loss: 0.00001832
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 212. Stopping optimization.
Last 5 losses: [1.8316217392566614e-05, 1.8316217392566614e-05, 1.8316217392566614e-05, 1.8316217392566614e-05, 1.8316217392566614e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8316217392566614e-05

Optimization complete. Final v2v error: 3.6776959896087646 mm

Highest mean error: 4.363966464996338 mm for frame 260

Lowest mean error: 3.5154027938842773 mm for frame 206

Saving results

Total time: 59.116755962371826
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_025/1063/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_025/1063.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_025/1063
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00998634
Iteration 2/25 | Loss: 0.00165933
Iteration 3/25 | Loss: 0.00141947
Iteration 4/25 | Loss: 0.00137109
Iteration 5/25 | Loss: 0.00135653
Iteration 6/25 | Loss: 0.00135256
Iteration 7/25 | Loss: 0.00135256
Iteration 8/25 | Loss: 0.00135256
Iteration 9/25 | Loss: 0.00135256
Iteration 10/25 | Loss: 0.00135256
Iteration 11/25 | Loss: 0.00135256
Iteration 12/25 | Loss: 0.00135256
Iteration 13/25 | Loss: 0.00135256
Iteration 14/25 | Loss: 0.00135256
Iteration 15/25 | Loss: 0.00135256
Iteration 16/25 | Loss: 0.00135256
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0013525557005777955, 0.0013525557005777955, 0.0013525557005777955, 0.0013525557005777955, 0.0013525557005777955]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013525557005777955

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.90087920
Iteration 2/25 | Loss: 0.00174257
Iteration 3/25 | Loss: 0.00174237
Iteration 4/25 | Loss: 0.00174237
Iteration 5/25 | Loss: 0.00174237
Iteration 6/25 | Loss: 0.00174237
Iteration 7/25 | Loss: 0.00174237
Iteration 8/25 | Loss: 0.00174237
Iteration 9/25 | Loss: 0.00174237
Iteration 10/25 | Loss: 0.00174237
Iteration 11/25 | Loss: 0.00174237
Iteration 12/25 | Loss: 0.00174237
Iteration 13/25 | Loss: 0.00174237
Iteration 14/25 | Loss: 0.00174237
Iteration 15/25 | Loss: 0.00174237
Iteration 16/25 | Loss: 0.00174237
Iteration 17/25 | Loss: 0.00174237
Iteration 18/25 | Loss: 0.00174237
Iteration 19/25 | Loss: 0.00174237
Iteration 20/25 | Loss: 0.00174237
Iteration 21/25 | Loss: 0.00174237
Iteration 22/25 | Loss: 0.00174237
Iteration 23/25 | Loss: 0.00174237
Iteration 24/25 | Loss: 0.00174237
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0017423658864572644, 0.0017423658864572644, 0.0017423658864572644, 0.0017423658864572644, 0.0017423658864572644]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0017423658864572644

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00174237
Iteration 2/1000 | Loss: 0.00008436
Iteration 3/1000 | Loss: 0.00006254
Iteration 4/1000 | Loss: 0.00005425
Iteration 5/1000 | Loss: 0.00005186
Iteration 6/1000 | Loss: 0.00005004
Iteration 7/1000 | Loss: 0.00004875
Iteration 8/1000 | Loss: 0.00004725
Iteration 9/1000 | Loss: 0.00004635
Iteration 10/1000 | Loss: 0.00004549
Iteration 11/1000 | Loss: 0.00004474
Iteration 12/1000 | Loss: 0.00004416
Iteration 13/1000 | Loss: 0.00004368
Iteration 14/1000 | Loss: 0.00004328
Iteration 15/1000 | Loss: 0.00004296
Iteration 16/1000 | Loss: 0.00004270
Iteration 17/1000 | Loss: 0.00004251
Iteration 18/1000 | Loss: 0.00004233
Iteration 19/1000 | Loss: 0.00004219
Iteration 20/1000 | Loss: 0.00004218
Iteration 21/1000 | Loss: 0.00004209
Iteration 22/1000 | Loss: 0.00004204
Iteration 23/1000 | Loss: 0.00004188
Iteration 24/1000 | Loss: 0.00004181
Iteration 25/1000 | Loss: 0.00004176
Iteration 26/1000 | Loss: 0.00004176
Iteration 27/1000 | Loss: 0.00004173
Iteration 28/1000 | Loss: 0.00004169
Iteration 29/1000 | Loss: 0.00004169
Iteration 30/1000 | Loss: 0.00004169
Iteration 31/1000 | Loss: 0.00004167
Iteration 32/1000 | Loss: 0.00004166
Iteration 33/1000 | Loss: 0.00004166
Iteration 34/1000 | Loss: 0.00004166
Iteration 35/1000 | Loss: 0.00004165
Iteration 36/1000 | Loss: 0.00004165
Iteration 37/1000 | Loss: 0.00004165
Iteration 38/1000 | Loss: 0.00004164
Iteration 39/1000 | Loss: 0.00004164
Iteration 40/1000 | Loss: 0.00004163
Iteration 41/1000 | Loss: 0.00004163
Iteration 42/1000 | Loss: 0.00004163
Iteration 43/1000 | Loss: 0.00004162
Iteration 44/1000 | Loss: 0.00004162
Iteration 45/1000 | Loss: 0.00004162
Iteration 46/1000 | Loss: 0.00004161
Iteration 47/1000 | Loss: 0.00004161
Iteration 48/1000 | Loss: 0.00004161
Iteration 49/1000 | Loss: 0.00004160
Iteration 50/1000 | Loss: 0.00004159
Iteration 51/1000 | Loss: 0.00004158
Iteration 52/1000 | Loss: 0.00004158
Iteration 53/1000 | Loss: 0.00004157
Iteration 54/1000 | Loss: 0.00004157
Iteration 55/1000 | Loss: 0.00004156
Iteration 56/1000 | Loss: 0.00004155
Iteration 57/1000 | Loss: 0.00004155
Iteration 58/1000 | Loss: 0.00004154
Iteration 59/1000 | Loss: 0.00004154
Iteration 60/1000 | Loss: 0.00004154
Iteration 61/1000 | Loss: 0.00004153
Iteration 62/1000 | Loss: 0.00004153
Iteration 63/1000 | Loss: 0.00004153
Iteration 64/1000 | Loss: 0.00004153
Iteration 65/1000 | Loss: 0.00004152
Iteration 66/1000 | Loss: 0.00004152
Iteration 67/1000 | Loss: 0.00004152
Iteration 68/1000 | Loss: 0.00004152
Iteration 69/1000 | Loss: 0.00004152
Iteration 70/1000 | Loss: 0.00004152
Iteration 71/1000 | Loss: 0.00004152
Iteration 72/1000 | Loss: 0.00004152
Iteration 73/1000 | Loss: 0.00004151
Iteration 74/1000 | Loss: 0.00004151
Iteration 75/1000 | Loss: 0.00004150
Iteration 76/1000 | Loss: 0.00004150
Iteration 77/1000 | Loss: 0.00004150
Iteration 78/1000 | Loss: 0.00004150
Iteration 79/1000 | Loss: 0.00004150
Iteration 80/1000 | Loss: 0.00004149
Iteration 81/1000 | Loss: 0.00004149
Iteration 82/1000 | Loss: 0.00004149
Iteration 83/1000 | Loss: 0.00004149
Iteration 84/1000 | Loss: 0.00004149
Iteration 85/1000 | Loss: 0.00004149
Iteration 86/1000 | Loss: 0.00004149
Iteration 87/1000 | Loss: 0.00004149
Iteration 88/1000 | Loss: 0.00004149
Iteration 89/1000 | Loss: 0.00004149
Iteration 90/1000 | Loss: 0.00004149
Iteration 91/1000 | Loss: 0.00004148
Iteration 92/1000 | Loss: 0.00004148
Iteration 93/1000 | Loss: 0.00004148
Iteration 94/1000 | Loss: 0.00004148
Iteration 95/1000 | Loss: 0.00004148
Iteration 96/1000 | Loss: 0.00004148
Iteration 97/1000 | Loss: 0.00004148
Iteration 98/1000 | Loss: 0.00004147
Iteration 99/1000 | Loss: 0.00004147
Iteration 100/1000 | Loss: 0.00004147
Iteration 101/1000 | Loss: 0.00004147
Iteration 102/1000 | Loss: 0.00004147
Iteration 103/1000 | Loss: 0.00004147
Iteration 104/1000 | Loss: 0.00004147
Iteration 105/1000 | Loss: 0.00004147
Iteration 106/1000 | Loss: 0.00004147
Iteration 107/1000 | Loss: 0.00004146
Iteration 108/1000 | Loss: 0.00004145
Iteration 109/1000 | Loss: 0.00004145
Iteration 110/1000 | Loss: 0.00004145
Iteration 111/1000 | Loss: 0.00004145
Iteration 112/1000 | Loss: 0.00004145
Iteration 113/1000 | Loss: 0.00004145
Iteration 114/1000 | Loss: 0.00004145
Iteration 115/1000 | Loss: 0.00004145
Iteration 116/1000 | Loss: 0.00004144
Iteration 117/1000 | Loss: 0.00004144
Iteration 118/1000 | Loss: 0.00004144
Iteration 119/1000 | Loss: 0.00004144
Iteration 120/1000 | Loss: 0.00004143
Iteration 121/1000 | Loss: 0.00004143
Iteration 122/1000 | Loss: 0.00004143
Iteration 123/1000 | Loss: 0.00004143
Iteration 124/1000 | Loss: 0.00004143
Iteration 125/1000 | Loss: 0.00004143
Iteration 126/1000 | Loss: 0.00004143
Iteration 127/1000 | Loss: 0.00004142
Iteration 128/1000 | Loss: 0.00004142
Iteration 129/1000 | Loss: 0.00004142
Iteration 130/1000 | Loss: 0.00004142
Iteration 131/1000 | Loss: 0.00004142
Iteration 132/1000 | Loss: 0.00004142
Iteration 133/1000 | Loss: 0.00004142
Iteration 134/1000 | Loss: 0.00004142
Iteration 135/1000 | Loss: 0.00004142
Iteration 136/1000 | Loss: 0.00004142
Iteration 137/1000 | Loss: 0.00004142
Iteration 138/1000 | Loss: 0.00004141
Iteration 139/1000 | Loss: 0.00004141
Iteration 140/1000 | Loss: 0.00004140
Iteration 141/1000 | Loss: 0.00004140
Iteration 142/1000 | Loss: 0.00004140
Iteration 143/1000 | Loss: 0.00004140
Iteration 144/1000 | Loss: 0.00004140
Iteration 145/1000 | Loss: 0.00004140
Iteration 146/1000 | Loss: 0.00004140
Iteration 147/1000 | Loss: 0.00004140
Iteration 148/1000 | Loss: 0.00004140
Iteration 149/1000 | Loss: 0.00004140
Iteration 150/1000 | Loss: 0.00004139
Iteration 151/1000 | Loss: 0.00004139
Iteration 152/1000 | Loss: 0.00004139
Iteration 153/1000 | Loss: 0.00004138
Iteration 154/1000 | Loss: 0.00004138
Iteration 155/1000 | Loss: 0.00004138
Iteration 156/1000 | Loss: 0.00004138
Iteration 157/1000 | Loss: 0.00004138
Iteration 158/1000 | Loss: 0.00004137
Iteration 159/1000 | Loss: 0.00004137
Iteration 160/1000 | Loss: 0.00004137
Iteration 161/1000 | Loss: 0.00004137
Iteration 162/1000 | Loss: 0.00004137
Iteration 163/1000 | Loss: 0.00004137
Iteration 164/1000 | Loss: 0.00004137
Iteration 165/1000 | Loss: 0.00004137
Iteration 166/1000 | Loss: 0.00004137
Iteration 167/1000 | Loss: 0.00004137
Iteration 168/1000 | Loss: 0.00004137
Iteration 169/1000 | Loss: 0.00004137
Iteration 170/1000 | Loss: 0.00004137
Iteration 171/1000 | Loss: 0.00004137
Iteration 172/1000 | Loss: 0.00004137
Iteration 173/1000 | Loss: 0.00004137
Iteration 174/1000 | Loss: 0.00004137
Iteration 175/1000 | Loss: 0.00004137
Iteration 176/1000 | Loss: 0.00004137
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 176. Stopping optimization.
Last 5 losses: [4.1366183722857386e-05, 4.1366183722857386e-05, 4.1366183722857386e-05, 4.1366183722857386e-05, 4.1366183722857386e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.1366183722857386e-05

Optimization complete. Final v2v error: 5.304436206817627 mm

Highest mean error: 5.770752906799316 mm for frame 182

Lowest mean error: 4.840485095977783 mm for frame 150

Saving results

Total time: 61.09620642662048
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_025/1095/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_025/1095.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_025/1095
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01054953
Iteration 2/25 | Loss: 0.00243925
Iteration 3/25 | Loss: 0.00183577
Iteration 4/25 | Loss: 0.00180661
Iteration 5/25 | Loss: 0.00189451
Iteration 6/25 | Loss: 0.00187715
Iteration 7/25 | Loss: 0.00184380
Iteration 8/25 | Loss: 0.00177956
Iteration 9/25 | Loss: 0.00163506
Iteration 10/25 | Loss: 0.00157132
Iteration 11/25 | Loss: 0.00147000
Iteration 12/25 | Loss: 0.00142001
Iteration 13/25 | Loss: 0.00138635
Iteration 14/25 | Loss: 0.00135455
Iteration 15/25 | Loss: 0.00134132
Iteration 16/25 | Loss: 0.00133085
Iteration 17/25 | Loss: 0.00132670
Iteration 18/25 | Loss: 0.00132813
Iteration 19/25 | Loss: 0.00132166
Iteration 20/25 | Loss: 0.00131623
Iteration 21/25 | Loss: 0.00130925
Iteration 22/25 | Loss: 0.00130953
Iteration 23/25 | Loss: 0.00130858
Iteration 24/25 | Loss: 0.00130544
Iteration 25/25 | Loss: 0.00130458

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.96507341
Iteration 2/25 | Loss: 0.00097612
Iteration 3/25 | Loss: 0.00095043
Iteration 4/25 | Loss: 0.00095043
Iteration 5/25 | Loss: 0.00095043
Iteration 6/25 | Loss: 0.00095043
Iteration 7/25 | Loss: 0.00095043
Iteration 8/25 | Loss: 0.00095043
Iteration 9/25 | Loss: 0.00095043
Iteration 10/25 | Loss: 0.00095043
Iteration 11/25 | Loss: 0.00095043
Iteration 12/25 | Loss: 0.00095043
Iteration 13/25 | Loss: 0.00095043
Iteration 14/25 | Loss: 0.00095043
Iteration 15/25 | Loss: 0.00095043
Iteration 16/25 | Loss: 0.00095043
Iteration 17/25 | Loss: 0.00095043
Iteration 18/25 | Loss: 0.00095043
Iteration 19/25 | Loss: 0.00095043
Iteration 20/25 | Loss: 0.00095043
Iteration 21/25 | Loss: 0.00095043
Iteration 22/25 | Loss: 0.00095043
Iteration 23/25 | Loss: 0.00095043
Iteration 24/25 | Loss: 0.00095043
Iteration 25/25 | Loss: 0.00095043

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00095043
Iteration 2/1000 | Loss: 0.00006572
Iteration 3/1000 | Loss: 0.00003566
Iteration 4/1000 | Loss: 0.00002197
Iteration 5/1000 | Loss: 0.00003621
Iteration 6/1000 | Loss: 0.00002135
Iteration 7/1000 | Loss: 0.00003494
Iteration 8/1000 | Loss: 0.00002676
Iteration 9/1000 | Loss: 0.00002353
Iteration 10/1000 | Loss: 0.00002548
Iteration 11/1000 | Loss: 0.00003696
Iteration 12/1000 | Loss: 0.00002683
Iteration 13/1000 | Loss: 0.00032759
Iteration 14/1000 | Loss: 0.00025044
Iteration 15/1000 | Loss: 0.00029853
Iteration 16/1000 | Loss: 0.00016993
Iteration 17/1000 | Loss: 0.00004147
Iteration 18/1000 | Loss: 0.00003959
Iteration 19/1000 | Loss: 0.00005453
Iteration 20/1000 | Loss: 0.00017531
Iteration 21/1000 | Loss: 0.00020036
Iteration 22/1000 | Loss: 0.00019314
Iteration 23/1000 | Loss: 0.00021584
Iteration 24/1000 | Loss: 0.00027928
Iteration 25/1000 | Loss: 0.00005803
Iteration 26/1000 | Loss: 0.00003783
Iteration 27/1000 | Loss: 0.00004171
Iteration 28/1000 | Loss: 0.00003480
Iteration 29/1000 | Loss: 0.00003827
Iteration 30/1000 | Loss: 0.00003324
Iteration 31/1000 | Loss: 0.00003655
Iteration 32/1000 | Loss: 0.00003351
Iteration 33/1000 | Loss: 0.00003654
Iteration 34/1000 | Loss: 0.00003443
Iteration 35/1000 | Loss: 0.00002909
Iteration 36/1000 | Loss: 0.00002025
Iteration 37/1000 | Loss: 0.00003304
Iteration 38/1000 | Loss: 0.00002801
Iteration 39/1000 | Loss: 0.00003302
Iteration 40/1000 | Loss: 0.00003675
Iteration 41/1000 | Loss: 0.00003483
Iteration 42/1000 | Loss: 0.00003646
Iteration 43/1000 | Loss: 0.00003057
Iteration 44/1000 | Loss: 0.00003624
Iteration 45/1000 | Loss: 0.00002785
Iteration 46/1000 | Loss: 0.00002999
Iteration 47/1000 | Loss: 0.00003315
Iteration 48/1000 | Loss: 0.00002981
Iteration 49/1000 | Loss: 0.00003630
Iteration 50/1000 | Loss: 0.00003428
Iteration 51/1000 | Loss: 0.00003400
Iteration 52/1000 | Loss: 0.00003281
Iteration 53/1000 | Loss: 0.00002149
Iteration 54/1000 | Loss: 0.00001956
Iteration 55/1000 | Loss: 0.00001883
Iteration 56/1000 | Loss: 0.00001801
Iteration 57/1000 | Loss: 0.00001821
Iteration 58/1000 | Loss: 0.00001710
Iteration 59/1000 | Loss: 0.00001697
Iteration 60/1000 | Loss: 0.00001676
Iteration 61/1000 | Loss: 0.00001645
Iteration 62/1000 | Loss: 0.00001627
Iteration 63/1000 | Loss: 0.00001617
Iteration 64/1000 | Loss: 0.00001613
Iteration 65/1000 | Loss: 0.00001605
Iteration 66/1000 | Loss: 0.00001603
Iteration 67/1000 | Loss: 0.00001602
Iteration 68/1000 | Loss: 0.00001602
Iteration 69/1000 | Loss: 0.00001601
Iteration 70/1000 | Loss: 0.00001601
Iteration 71/1000 | Loss: 0.00001600
Iteration 72/1000 | Loss: 0.00001600
Iteration 73/1000 | Loss: 0.00001600
Iteration 74/1000 | Loss: 0.00001599
Iteration 75/1000 | Loss: 0.00001599
Iteration 76/1000 | Loss: 0.00001598
Iteration 77/1000 | Loss: 0.00001598
Iteration 78/1000 | Loss: 0.00001592
Iteration 79/1000 | Loss: 0.00001588
Iteration 80/1000 | Loss: 0.00001588
Iteration 81/1000 | Loss: 0.00001587
Iteration 82/1000 | Loss: 0.00001587
Iteration 83/1000 | Loss: 0.00001586
Iteration 84/1000 | Loss: 0.00001585
Iteration 85/1000 | Loss: 0.00001585
Iteration 86/1000 | Loss: 0.00001582
Iteration 87/1000 | Loss: 0.00001577
Iteration 88/1000 | Loss: 0.00001577
Iteration 89/1000 | Loss: 0.00001577
Iteration 90/1000 | Loss: 0.00001576
Iteration 91/1000 | Loss: 0.00001576
Iteration 92/1000 | Loss: 0.00001576
Iteration 93/1000 | Loss: 0.00001576
Iteration 94/1000 | Loss: 0.00001576
Iteration 95/1000 | Loss: 0.00001576
Iteration 96/1000 | Loss: 0.00001576
Iteration 97/1000 | Loss: 0.00001576
Iteration 98/1000 | Loss: 0.00001576
Iteration 99/1000 | Loss: 0.00001576
Iteration 100/1000 | Loss: 0.00001575
Iteration 101/1000 | Loss: 0.00001575
Iteration 102/1000 | Loss: 0.00001575
Iteration 103/1000 | Loss: 0.00001575
Iteration 104/1000 | Loss: 0.00001575
Iteration 105/1000 | Loss: 0.00001575
Iteration 106/1000 | Loss: 0.00001575
Iteration 107/1000 | Loss: 0.00001575
Iteration 108/1000 | Loss: 0.00001574
Iteration 109/1000 | Loss: 0.00001573
Iteration 110/1000 | Loss: 0.00001569
Iteration 111/1000 | Loss: 0.00001569
Iteration 112/1000 | Loss: 0.00001567
Iteration 113/1000 | Loss: 0.00001567
Iteration 114/1000 | Loss: 0.00001567
Iteration 115/1000 | Loss: 0.00001567
Iteration 116/1000 | Loss: 0.00001567
Iteration 117/1000 | Loss: 0.00001566
Iteration 118/1000 | Loss: 0.00001566
Iteration 119/1000 | Loss: 0.00001566
Iteration 120/1000 | Loss: 0.00001565
Iteration 121/1000 | Loss: 0.00001565
Iteration 122/1000 | Loss: 0.00001565
Iteration 123/1000 | Loss: 0.00001565
Iteration 124/1000 | Loss: 0.00001565
Iteration 125/1000 | Loss: 0.00001565
Iteration 126/1000 | Loss: 0.00001565
Iteration 127/1000 | Loss: 0.00001564
Iteration 128/1000 | Loss: 0.00001563
Iteration 129/1000 | Loss: 0.00001563
Iteration 130/1000 | Loss: 0.00001563
Iteration 131/1000 | Loss: 0.00001563
Iteration 132/1000 | Loss: 0.00001563
Iteration 133/1000 | Loss: 0.00001563
Iteration 134/1000 | Loss: 0.00001563
Iteration 135/1000 | Loss: 0.00001563
Iteration 136/1000 | Loss: 0.00001563
Iteration 137/1000 | Loss: 0.00001563
Iteration 138/1000 | Loss: 0.00001563
Iteration 139/1000 | Loss: 0.00001563
Iteration 140/1000 | Loss: 0.00001562
Iteration 141/1000 | Loss: 0.00001562
Iteration 142/1000 | Loss: 0.00001562
Iteration 143/1000 | Loss: 0.00001561
Iteration 144/1000 | Loss: 0.00001560
Iteration 145/1000 | Loss: 0.00001559
Iteration 146/1000 | Loss: 0.00001559
Iteration 147/1000 | Loss: 0.00001559
Iteration 148/1000 | Loss: 0.00001559
Iteration 149/1000 | Loss: 0.00001559
Iteration 150/1000 | Loss: 0.00001558
Iteration 151/1000 | Loss: 0.00001558
Iteration 152/1000 | Loss: 0.00001557
Iteration 153/1000 | Loss: 0.00001557
Iteration 154/1000 | Loss: 0.00001556
Iteration 155/1000 | Loss: 0.00001556
Iteration 156/1000 | Loss: 0.00001556
Iteration 157/1000 | Loss: 0.00001556
Iteration 158/1000 | Loss: 0.00001555
Iteration 159/1000 | Loss: 0.00001555
Iteration 160/1000 | Loss: 0.00001555
Iteration 161/1000 | Loss: 0.00001555
Iteration 162/1000 | Loss: 0.00001554
Iteration 163/1000 | Loss: 0.00001554
Iteration 164/1000 | Loss: 0.00001553
Iteration 165/1000 | Loss: 0.00001549
Iteration 166/1000 | Loss: 0.00001548
Iteration 167/1000 | Loss: 0.00001546
Iteration 168/1000 | Loss: 0.00001546
Iteration 169/1000 | Loss: 0.00001546
Iteration 170/1000 | Loss: 0.00001545
Iteration 171/1000 | Loss: 0.00001545
Iteration 172/1000 | Loss: 0.00001545
Iteration 173/1000 | Loss: 0.00001545
Iteration 174/1000 | Loss: 0.00001545
Iteration 175/1000 | Loss: 0.00001545
Iteration 176/1000 | Loss: 0.00032145
Iteration 177/1000 | Loss: 0.00005646
Iteration 178/1000 | Loss: 0.00018157
Iteration 179/1000 | Loss: 0.00022166
Iteration 180/1000 | Loss: 0.00014961
Iteration 181/1000 | Loss: 0.00015846
Iteration 182/1000 | Loss: 0.00001835
Iteration 183/1000 | Loss: 0.00001695
Iteration 184/1000 | Loss: 0.00002534
Iteration 185/1000 | Loss: 0.00020592
Iteration 186/1000 | Loss: 0.00016642
Iteration 187/1000 | Loss: 0.00003756
Iteration 188/1000 | Loss: 0.00014128
Iteration 189/1000 | Loss: 0.00019920
Iteration 190/1000 | Loss: 0.00019058
Iteration 191/1000 | Loss: 0.00002980
Iteration 192/1000 | Loss: 0.00004544
Iteration 193/1000 | Loss: 0.00003354
Iteration 194/1000 | Loss: 0.00002967
Iteration 195/1000 | Loss: 0.00001853
Iteration 196/1000 | Loss: 0.00001780
Iteration 197/1000 | Loss: 0.00001735
Iteration 198/1000 | Loss: 0.00001699
Iteration 199/1000 | Loss: 0.00001676
Iteration 200/1000 | Loss: 0.00001652
Iteration 201/1000 | Loss: 0.00001627
Iteration 202/1000 | Loss: 0.00001619
Iteration 203/1000 | Loss: 0.00003759
Iteration 204/1000 | Loss: 0.00002804
Iteration 205/1000 | Loss: 0.00003907
Iteration 206/1000 | Loss: 0.00001856
Iteration 207/1000 | Loss: 0.00001714
Iteration 208/1000 | Loss: 0.00001654
Iteration 209/1000 | Loss: 0.00001592
Iteration 210/1000 | Loss: 0.00001561
Iteration 211/1000 | Loss: 0.00001534
Iteration 212/1000 | Loss: 0.00001525
Iteration 213/1000 | Loss: 0.00001507
Iteration 214/1000 | Loss: 0.00001500
Iteration 215/1000 | Loss: 0.00001493
Iteration 216/1000 | Loss: 0.00001492
Iteration 217/1000 | Loss: 0.00001492
Iteration 218/1000 | Loss: 0.00001492
Iteration 219/1000 | Loss: 0.00001492
Iteration 220/1000 | Loss: 0.00001492
Iteration 221/1000 | Loss: 0.00001492
Iteration 222/1000 | Loss: 0.00001492
Iteration 223/1000 | Loss: 0.00001492
Iteration 224/1000 | Loss: 0.00001492
Iteration 225/1000 | Loss: 0.00001492
Iteration 226/1000 | Loss: 0.00001491
Iteration 227/1000 | Loss: 0.00001491
Iteration 228/1000 | Loss: 0.00001491
Iteration 229/1000 | Loss: 0.00001489
Iteration 230/1000 | Loss: 0.00001487
Iteration 231/1000 | Loss: 0.00001486
Iteration 232/1000 | Loss: 0.00001481
Iteration 233/1000 | Loss: 0.00001480
Iteration 234/1000 | Loss: 0.00001479
Iteration 235/1000 | Loss: 0.00001478
Iteration 236/1000 | Loss: 0.00001476
Iteration 237/1000 | Loss: 0.00001475
Iteration 238/1000 | Loss: 0.00001474
Iteration 239/1000 | Loss: 0.00001474
Iteration 240/1000 | Loss: 0.00001473
Iteration 241/1000 | Loss: 0.00001472
Iteration 242/1000 | Loss: 0.00001472
Iteration 243/1000 | Loss: 0.00001471
Iteration 244/1000 | Loss: 0.00001470
Iteration 245/1000 | Loss: 0.00001470
Iteration 246/1000 | Loss: 0.00001468
Iteration 247/1000 | Loss: 0.00001468
Iteration 248/1000 | Loss: 0.00001468
Iteration 249/1000 | Loss: 0.00001468
Iteration 250/1000 | Loss: 0.00001468
Iteration 251/1000 | Loss: 0.00001468
Iteration 252/1000 | Loss: 0.00001468
Iteration 253/1000 | Loss: 0.00001468
Iteration 254/1000 | Loss: 0.00001467
Iteration 255/1000 | Loss: 0.00001467
Iteration 256/1000 | Loss: 0.00001467
Iteration 257/1000 | Loss: 0.00001467
Iteration 258/1000 | Loss: 0.00001467
Iteration 259/1000 | Loss: 0.00001467
Iteration 260/1000 | Loss: 0.00001467
Iteration 261/1000 | Loss: 0.00001466
Iteration 262/1000 | Loss: 0.00001466
Iteration 263/1000 | Loss: 0.00001465
Iteration 264/1000 | Loss: 0.00001465
Iteration 265/1000 | Loss: 0.00001465
Iteration 266/1000 | Loss: 0.00001465
Iteration 267/1000 | Loss: 0.00001464
Iteration 268/1000 | Loss: 0.00001464
Iteration 269/1000 | Loss: 0.00001464
Iteration 270/1000 | Loss: 0.00001463
Iteration 271/1000 | Loss: 0.00001463
Iteration 272/1000 | Loss: 0.00001463
Iteration 273/1000 | Loss: 0.00001462
Iteration 274/1000 | Loss: 0.00001462
Iteration 275/1000 | Loss: 0.00001462
Iteration 276/1000 | Loss: 0.00001461
Iteration 277/1000 | Loss: 0.00001460
Iteration 278/1000 | Loss: 0.00001460
Iteration 279/1000 | Loss: 0.00001459
Iteration 280/1000 | Loss: 0.00001459
Iteration 281/1000 | Loss: 0.00001458
Iteration 282/1000 | Loss: 0.00001457
Iteration 283/1000 | Loss: 0.00001456
Iteration 284/1000 | Loss: 0.00001455
Iteration 285/1000 | Loss: 0.00001455
Iteration 286/1000 | Loss: 0.00001455
Iteration 287/1000 | Loss: 0.00001455
Iteration 288/1000 | Loss: 0.00001455
Iteration 289/1000 | Loss: 0.00001455
Iteration 290/1000 | Loss: 0.00001455
Iteration 291/1000 | Loss: 0.00001455
Iteration 292/1000 | Loss: 0.00001455
Iteration 293/1000 | Loss: 0.00001455
Iteration 294/1000 | Loss: 0.00001455
Iteration 295/1000 | Loss: 0.00002684
Iteration 296/1000 | Loss: 0.00001760
Iteration 297/1000 | Loss: 0.00001574
Iteration 298/1000 | Loss: 0.00001493
Iteration 299/1000 | Loss: 0.00001457
Iteration 300/1000 | Loss: 0.00001440
Iteration 301/1000 | Loss: 0.00001435
Iteration 302/1000 | Loss: 0.00001434
Iteration 303/1000 | Loss: 0.00001434
Iteration 304/1000 | Loss: 0.00001434
Iteration 305/1000 | Loss: 0.00001434
Iteration 306/1000 | Loss: 0.00001434
Iteration 307/1000 | Loss: 0.00001434
Iteration 308/1000 | Loss: 0.00001433
Iteration 309/1000 | Loss: 0.00001428
Iteration 310/1000 | Loss: 0.00001428
Iteration 311/1000 | Loss: 0.00001427
Iteration 312/1000 | Loss: 0.00001426
Iteration 313/1000 | Loss: 0.00001426
Iteration 314/1000 | Loss: 0.00001425
Iteration 315/1000 | Loss: 0.00001425
Iteration 316/1000 | Loss: 0.00001425
Iteration 317/1000 | Loss: 0.00001425
Iteration 318/1000 | Loss: 0.00001425
Iteration 319/1000 | Loss: 0.00001425
Iteration 320/1000 | Loss: 0.00001425
Iteration 321/1000 | Loss: 0.00001425
Iteration 322/1000 | Loss: 0.00001425
Iteration 323/1000 | Loss: 0.00001425
Iteration 324/1000 | Loss: 0.00001425
Iteration 325/1000 | Loss: 0.00001425
Iteration 326/1000 | Loss: 0.00001425
Iteration 327/1000 | Loss: 0.00001425
Iteration 328/1000 | Loss: 0.00001425
Iteration 329/1000 | Loss: 0.00001425
Iteration 330/1000 | Loss: 0.00001425
Iteration 331/1000 | Loss: 0.00001425
Iteration 332/1000 | Loss: 0.00001425
Iteration 333/1000 | Loss: 0.00001425
Iteration 334/1000 | Loss: 0.00001425
Iteration 335/1000 | Loss: 0.00001425
Iteration 336/1000 | Loss: 0.00001425
Iteration 337/1000 | Loss: 0.00001425
Iteration 338/1000 | Loss: 0.00001425
Iteration 339/1000 | Loss: 0.00001425
Iteration 340/1000 | Loss: 0.00001425
Iteration 341/1000 | Loss: 0.00001425
Iteration 342/1000 | Loss: 0.00001425
Iteration 343/1000 | Loss: 0.00001425
Iteration 344/1000 | Loss: 0.00001425
Iteration 345/1000 | Loss: 0.00001425
Iteration 346/1000 | Loss: 0.00001425
Iteration 347/1000 | Loss: 0.00001425
Iteration 348/1000 | Loss: 0.00001425
Iteration 349/1000 | Loss: 0.00001425
Iteration 350/1000 | Loss: 0.00001425
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 350. Stopping optimization.
Last 5 losses: [1.4245061720430385e-05, 1.4245061720430385e-05, 1.4245061720430385e-05, 1.4245061720430385e-05, 1.4245061720430385e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4245061720430385e-05

Optimization complete. Final v2v error: 3.208409547805786 mm

Highest mean error: 5.285829544067383 mm for frame 40

Lowest mean error: 2.9938371181488037 mm for frame 9

Saving results

Total time: 226.24122858047485
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_025/1088/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_025/1088.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_025/1088
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00739439
Iteration 2/25 | Loss: 0.00192395
Iteration 3/25 | Loss: 0.00144380
Iteration 4/25 | Loss: 0.00141335
Iteration 5/25 | Loss: 0.00141082
Iteration 6/25 | Loss: 0.00141082
Iteration 7/25 | Loss: 0.00141082
Iteration 8/25 | Loss: 0.00141082
Iteration 9/25 | Loss: 0.00141082
Iteration 10/25 | Loss: 0.00141082
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0014108188915997744, 0.0014108188915997744, 0.0014108188915997744, 0.0014108188915997744, 0.0014108188915997744]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014108188915997744

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.27059197
Iteration 2/25 | Loss: 0.00122379
Iteration 3/25 | Loss: 0.00122378
Iteration 4/25 | Loss: 0.00122378
Iteration 5/25 | Loss: 0.00122377
Iteration 6/25 | Loss: 0.00122377
Iteration 7/25 | Loss: 0.00122377
Iteration 8/25 | Loss: 0.00122377
Iteration 9/25 | Loss: 0.00122377
Iteration 10/25 | Loss: 0.00122377
Iteration 11/25 | Loss: 0.00122377
Iteration 12/25 | Loss: 0.00122377
Iteration 13/25 | Loss: 0.00122377
Iteration 14/25 | Loss: 0.00122377
Iteration 15/25 | Loss: 0.00122377
Iteration 16/25 | Loss: 0.00122377
Iteration 17/25 | Loss: 0.00122377
Iteration 18/25 | Loss: 0.00122377
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0012237724149599671, 0.0012237724149599671, 0.0012237724149599671, 0.0012237724149599671, 0.0012237724149599671]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012237724149599671

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00122377
Iteration 2/1000 | Loss: 0.00004378
Iteration 3/1000 | Loss: 0.00003203
Iteration 4/1000 | Loss: 0.00002955
Iteration 5/1000 | Loss: 0.00002850
Iteration 6/1000 | Loss: 0.00002759
Iteration 7/1000 | Loss: 0.00002720
Iteration 8/1000 | Loss: 0.00002680
Iteration 9/1000 | Loss: 0.00002638
Iteration 10/1000 | Loss: 0.00002608
Iteration 11/1000 | Loss: 0.00002590
Iteration 12/1000 | Loss: 0.00002571
Iteration 13/1000 | Loss: 0.00002549
Iteration 14/1000 | Loss: 0.00002539
Iteration 15/1000 | Loss: 0.00002530
Iteration 16/1000 | Loss: 0.00002529
Iteration 17/1000 | Loss: 0.00002528
Iteration 18/1000 | Loss: 0.00002528
Iteration 19/1000 | Loss: 0.00002528
Iteration 20/1000 | Loss: 0.00002528
Iteration 21/1000 | Loss: 0.00002528
Iteration 22/1000 | Loss: 0.00002527
Iteration 23/1000 | Loss: 0.00002527
Iteration 24/1000 | Loss: 0.00002526
Iteration 25/1000 | Loss: 0.00002525
Iteration 26/1000 | Loss: 0.00002525
Iteration 27/1000 | Loss: 0.00002525
Iteration 28/1000 | Loss: 0.00002524
Iteration 29/1000 | Loss: 0.00002524
Iteration 30/1000 | Loss: 0.00002522
Iteration 31/1000 | Loss: 0.00002522
Iteration 32/1000 | Loss: 0.00002521
Iteration 33/1000 | Loss: 0.00002521
Iteration 34/1000 | Loss: 0.00002521
Iteration 35/1000 | Loss: 0.00002521
Iteration 36/1000 | Loss: 0.00002521
Iteration 37/1000 | Loss: 0.00002521
Iteration 38/1000 | Loss: 0.00002521
Iteration 39/1000 | Loss: 0.00002521
Iteration 40/1000 | Loss: 0.00002521
Iteration 41/1000 | Loss: 0.00002521
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 41. Stopping optimization.
Last 5 losses: [2.5212928449036554e-05, 2.5212928449036554e-05, 2.5212928449036554e-05, 2.5212928449036554e-05, 2.5212928449036554e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.5212928449036554e-05

Optimization complete. Final v2v error: 4.245837688446045 mm

Highest mean error: 4.596335411071777 mm for frame 2

Lowest mean error: 4.002712726593018 mm for frame 161

Saving results

Total time: 33.22095322608948
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_025/1096/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_025/1096.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_025/1096
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00383284
Iteration 2/25 | Loss: 0.00131659
Iteration 3/25 | Loss: 0.00123913
Iteration 4/25 | Loss: 0.00122222
Iteration 5/25 | Loss: 0.00121535
Iteration 6/25 | Loss: 0.00121353
Iteration 7/25 | Loss: 0.00121297
Iteration 8/25 | Loss: 0.00121279
Iteration 9/25 | Loss: 0.00121279
Iteration 10/25 | Loss: 0.00121279
Iteration 11/25 | Loss: 0.00121279
Iteration 12/25 | Loss: 0.00121279
Iteration 13/25 | Loss: 0.00121279
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0012127907248213887, 0.0012127907248213887, 0.0012127907248213887, 0.0012127907248213887, 0.0012127907248213887]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012127907248213887

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39294326
Iteration 2/25 | Loss: 0.00210884
Iteration 3/25 | Loss: 0.00210884
Iteration 4/25 | Loss: 0.00210884
Iteration 5/25 | Loss: 0.00210883
Iteration 6/25 | Loss: 0.00210883
Iteration 7/25 | Loss: 0.00210883
Iteration 8/25 | Loss: 0.00210883
Iteration 9/25 | Loss: 0.00210883
Iteration 10/25 | Loss: 0.00210883
Iteration 11/25 | Loss: 0.00210883
Iteration 12/25 | Loss: 0.00210883
Iteration 13/25 | Loss: 0.00210883
Iteration 14/25 | Loss: 0.00210883
Iteration 15/25 | Loss: 0.00210883
Iteration 16/25 | Loss: 0.00210883
Iteration 17/25 | Loss: 0.00210883
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0021088330540806055, 0.0021088330540806055, 0.0021088330540806055, 0.0021088330540806055, 0.0021088330540806055]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0021088330540806055

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00210883
Iteration 2/1000 | Loss: 0.00005714
Iteration 3/1000 | Loss: 0.00004082
Iteration 4/1000 | Loss: 0.00003205
Iteration 5/1000 | Loss: 0.00002825
Iteration 6/1000 | Loss: 0.00002624
Iteration 7/1000 | Loss: 0.00002478
Iteration 8/1000 | Loss: 0.00002375
Iteration 9/1000 | Loss: 0.00002301
Iteration 10/1000 | Loss: 0.00002244
Iteration 11/1000 | Loss: 0.00002204
Iteration 12/1000 | Loss: 0.00002172
Iteration 13/1000 | Loss: 0.00002145
Iteration 14/1000 | Loss: 0.00002124
Iteration 15/1000 | Loss: 0.00002116
Iteration 16/1000 | Loss: 0.00002110
Iteration 17/1000 | Loss: 0.00002104
Iteration 18/1000 | Loss: 0.00002090
Iteration 19/1000 | Loss: 0.00002089
Iteration 20/1000 | Loss: 0.00002084
Iteration 21/1000 | Loss: 0.00002083
Iteration 22/1000 | Loss: 0.00002082
Iteration 23/1000 | Loss: 0.00002077
Iteration 24/1000 | Loss: 0.00002076
Iteration 25/1000 | Loss: 0.00002074
Iteration 26/1000 | Loss: 0.00002073
Iteration 27/1000 | Loss: 0.00002072
Iteration 28/1000 | Loss: 0.00002070
Iteration 29/1000 | Loss: 0.00002069
Iteration 30/1000 | Loss: 0.00002068
Iteration 31/1000 | Loss: 0.00002067
Iteration 32/1000 | Loss: 0.00002067
Iteration 33/1000 | Loss: 0.00002066
Iteration 34/1000 | Loss: 0.00002065
Iteration 35/1000 | Loss: 0.00002060
Iteration 36/1000 | Loss: 0.00002060
Iteration 37/1000 | Loss: 0.00002060
Iteration 38/1000 | Loss: 0.00002059
Iteration 39/1000 | Loss: 0.00002058
Iteration 40/1000 | Loss: 0.00002058
Iteration 41/1000 | Loss: 0.00002058
Iteration 42/1000 | Loss: 0.00002057
Iteration 43/1000 | Loss: 0.00002057
Iteration 44/1000 | Loss: 0.00002056
Iteration 45/1000 | Loss: 0.00002056
Iteration 46/1000 | Loss: 0.00002056
Iteration 47/1000 | Loss: 0.00002055
Iteration 48/1000 | Loss: 0.00002055
Iteration 49/1000 | Loss: 0.00002055
Iteration 50/1000 | Loss: 0.00002054
Iteration 51/1000 | Loss: 0.00002054
Iteration 52/1000 | Loss: 0.00002054
Iteration 53/1000 | Loss: 0.00002053
Iteration 54/1000 | Loss: 0.00002053
Iteration 55/1000 | Loss: 0.00002053
Iteration 56/1000 | Loss: 0.00002053
Iteration 57/1000 | Loss: 0.00002053
Iteration 58/1000 | Loss: 0.00002053
Iteration 59/1000 | Loss: 0.00002053
Iteration 60/1000 | Loss: 0.00002053
Iteration 61/1000 | Loss: 0.00002053
Iteration 62/1000 | Loss: 0.00002053
Iteration 63/1000 | Loss: 0.00002053
Iteration 64/1000 | Loss: 0.00002052
Iteration 65/1000 | Loss: 0.00002052
Iteration 66/1000 | Loss: 0.00002052
Iteration 67/1000 | Loss: 0.00002052
Iteration 68/1000 | Loss: 0.00002052
Iteration 69/1000 | Loss: 0.00002052
Iteration 70/1000 | Loss: 0.00002052
Iteration 71/1000 | Loss: 0.00002052
Iteration 72/1000 | Loss: 0.00002052
Iteration 73/1000 | Loss: 0.00002052
Iteration 74/1000 | Loss: 0.00002052
Iteration 75/1000 | Loss: 0.00002051
Iteration 76/1000 | Loss: 0.00002051
Iteration 77/1000 | Loss: 0.00002051
Iteration 78/1000 | Loss: 0.00002051
Iteration 79/1000 | Loss: 0.00002051
Iteration 80/1000 | Loss: 0.00002051
Iteration 81/1000 | Loss: 0.00002051
Iteration 82/1000 | Loss: 0.00002051
Iteration 83/1000 | Loss: 0.00002051
Iteration 84/1000 | Loss: 0.00002051
Iteration 85/1000 | Loss: 0.00002050
Iteration 86/1000 | Loss: 0.00002050
Iteration 87/1000 | Loss: 0.00002050
Iteration 88/1000 | Loss: 0.00002050
Iteration 89/1000 | Loss: 0.00002050
Iteration 90/1000 | Loss: 0.00002049
Iteration 91/1000 | Loss: 0.00002049
Iteration 92/1000 | Loss: 0.00002049
Iteration 93/1000 | Loss: 0.00002049
Iteration 94/1000 | Loss: 0.00002049
Iteration 95/1000 | Loss: 0.00002049
Iteration 96/1000 | Loss: 0.00002049
Iteration 97/1000 | Loss: 0.00002049
Iteration 98/1000 | Loss: 0.00002049
Iteration 99/1000 | Loss: 0.00002049
Iteration 100/1000 | Loss: 0.00002049
Iteration 101/1000 | Loss: 0.00002049
Iteration 102/1000 | Loss: 0.00002049
Iteration 103/1000 | Loss: 0.00002048
Iteration 104/1000 | Loss: 0.00002048
Iteration 105/1000 | Loss: 0.00002048
Iteration 106/1000 | Loss: 0.00002048
Iteration 107/1000 | Loss: 0.00002048
Iteration 108/1000 | Loss: 0.00002048
Iteration 109/1000 | Loss: 0.00002048
Iteration 110/1000 | Loss: 0.00002048
Iteration 111/1000 | Loss: 0.00002048
Iteration 112/1000 | Loss: 0.00002047
Iteration 113/1000 | Loss: 0.00002047
Iteration 114/1000 | Loss: 0.00002047
Iteration 115/1000 | Loss: 0.00002047
Iteration 116/1000 | Loss: 0.00002047
Iteration 117/1000 | Loss: 0.00002047
Iteration 118/1000 | Loss: 0.00002047
Iteration 119/1000 | Loss: 0.00002047
Iteration 120/1000 | Loss: 0.00002046
Iteration 121/1000 | Loss: 0.00002046
Iteration 122/1000 | Loss: 0.00002046
Iteration 123/1000 | Loss: 0.00002046
Iteration 124/1000 | Loss: 0.00002046
Iteration 125/1000 | Loss: 0.00002046
Iteration 126/1000 | Loss: 0.00002045
Iteration 127/1000 | Loss: 0.00002045
Iteration 128/1000 | Loss: 0.00002045
Iteration 129/1000 | Loss: 0.00002045
Iteration 130/1000 | Loss: 0.00002044
Iteration 131/1000 | Loss: 0.00002044
Iteration 132/1000 | Loss: 0.00002044
Iteration 133/1000 | Loss: 0.00002044
Iteration 134/1000 | Loss: 0.00002044
Iteration 135/1000 | Loss: 0.00002044
Iteration 136/1000 | Loss: 0.00002044
Iteration 137/1000 | Loss: 0.00002044
Iteration 138/1000 | Loss: 0.00002043
Iteration 139/1000 | Loss: 0.00002043
Iteration 140/1000 | Loss: 0.00002043
Iteration 141/1000 | Loss: 0.00002043
Iteration 142/1000 | Loss: 0.00002043
Iteration 143/1000 | Loss: 0.00002043
Iteration 144/1000 | Loss: 0.00002043
Iteration 145/1000 | Loss: 0.00002043
Iteration 146/1000 | Loss: 0.00002043
Iteration 147/1000 | Loss: 0.00002043
Iteration 148/1000 | Loss: 0.00002043
Iteration 149/1000 | Loss: 0.00002042
Iteration 150/1000 | Loss: 0.00002042
Iteration 151/1000 | Loss: 0.00002042
Iteration 152/1000 | Loss: 0.00002042
Iteration 153/1000 | Loss: 0.00002042
Iteration 154/1000 | Loss: 0.00002042
Iteration 155/1000 | Loss: 0.00002041
Iteration 156/1000 | Loss: 0.00002041
Iteration 157/1000 | Loss: 0.00002041
Iteration 158/1000 | Loss: 0.00002041
Iteration 159/1000 | Loss: 0.00002041
Iteration 160/1000 | Loss: 0.00002041
Iteration 161/1000 | Loss: 0.00002041
Iteration 162/1000 | Loss: 0.00002041
Iteration 163/1000 | Loss: 0.00002041
Iteration 164/1000 | Loss: 0.00002041
Iteration 165/1000 | Loss: 0.00002041
Iteration 166/1000 | Loss: 0.00002040
Iteration 167/1000 | Loss: 0.00002040
Iteration 168/1000 | Loss: 0.00002040
Iteration 169/1000 | Loss: 0.00002040
Iteration 170/1000 | Loss: 0.00002040
Iteration 171/1000 | Loss: 0.00002040
Iteration 172/1000 | Loss: 0.00002040
Iteration 173/1000 | Loss: 0.00002040
Iteration 174/1000 | Loss: 0.00002040
Iteration 175/1000 | Loss: 0.00002040
Iteration 176/1000 | Loss: 0.00002040
Iteration 177/1000 | Loss: 0.00002040
Iteration 178/1000 | Loss: 0.00002040
Iteration 179/1000 | Loss: 0.00002040
Iteration 180/1000 | Loss: 0.00002040
Iteration 181/1000 | Loss: 0.00002039
Iteration 182/1000 | Loss: 0.00002039
Iteration 183/1000 | Loss: 0.00002039
Iteration 184/1000 | Loss: 0.00002039
Iteration 185/1000 | Loss: 0.00002039
Iteration 186/1000 | Loss: 0.00002039
Iteration 187/1000 | Loss: 0.00002039
Iteration 188/1000 | Loss: 0.00002039
Iteration 189/1000 | Loss: 0.00002039
Iteration 190/1000 | Loss: 0.00002039
Iteration 191/1000 | Loss: 0.00002038
Iteration 192/1000 | Loss: 0.00002038
Iteration 193/1000 | Loss: 0.00002038
Iteration 194/1000 | Loss: 0.00002038
Iteration 195/1000 | Loss: 0.00002038
Iteration 196/1000 | Loss: 0.00002038
Iteration 197/1000 | Loss: 0.00002037
Iteration 198/1000 | Loss: 0.00002037
Iteration 199/1000 | Loss: 0.00002037
Iteration 200/1000 | Loss: 0.00002037
Iteration 201/1000 | Loss: 0.00002037
Iteration 202/1000 | Loss: 0.00002037
Iteration 203/1000 | Loss: 0.00002037
Iteration 204/1000 | Loss: 0.00002037
Iteration 205/1000 | Loss: 0.00002037
Iteration 206/1000 | Loss: 0.00002037
Iteration 207/1000 | Loss: 0.00002037
Iteration 208/1000 | Loss: 0.00002037
Iteration 209/1000 | Loss: 0.00002037
Iteration 210/1000 | Loss: 0.00002037
Iteration 211/1000 | Loss: 0.00002037
Iteration 212/1000 | Loss: 0.00002037
Iteration 213/1000 | Loss: 0.00002037
Iteration 214/1000 | Loss: 0.00002036
Iteration 215/1000 | Loss: 0.00002036
Iteration 216/1000 | Loss: 0.00002036
Iteration 217/1000 | Loss: 0.00002036
Iteration 218/1000 | Loss: 0.00002036
Iteration 219/1000 | Loss: 0.00002035
Iteration 220/1000 | Loss: 0.00002035
Iteration 221/1000 | Loss: 0.00002035
Iteration 222/1000 | Loss: 0.00002035
Iteration 223/1000 | Loss: 0.00002035
Iteration 224/1000 | Loss: 0.00002035
Iteration 225/1000 | Loss: 0.00002035
Iteration 226/1000 | Loss: 0.00002035
Iteration 227/1000 | Loss: 0.00002035
Iteration 228/1000 | Loss: 0.00002035
Iteration 229/1000 | Loss: 0.00002035
Iteration 230/1000 | Loss: 0.00002035
Iteration 231/1000 | Loss: 0.00002034
Iteration 232/1000 | Loss: 0.00002034
Iteration 233/1000 | Loss: 0.00002034
Iteration 234/1000 | Loss: 0.00002034
Iteration 235/1000 | Loss: 0.00002034
Iteration 236/1000 | Loss: 0.00002034
Iteration 237/1000 | Loss: 0.00002034
Iteration 238/1000 | Loss: 0.00002034
Iteration 239/1000 | Loss: 0.00002034
Iteration 240/1000 | Loss: 0.00002034
Iteration 241/1000 | Loss: 0.00002034
Iteration 242/1000 | Loss: 0.00002034
Iteration 243/1000 | Loss: 0.00002033
Iteration 244/1000 | Loss: 0.00002033
Iteration 245/1000 | Loss: 0.00002033
Iteration 246/1000 | Loss: 0.00002033
Iteration 247/1000 | Loss: 0.00002033
Iteration 248/1000 | Loss: 0.00002033
Iteration 249/1000 | Loss: 0.00002033
Iteration 250/1000 | Loss: 0.00002033
Iteration 251/1000 | Loss: 0.00002033
Iteration 252/1000 | Loss: 0.00002033
Iteration 253/1000 | Loss: 0.00002033
Iteration 254/1000 | Loss: 0.00002033
Iteration 255/1000 | Loss: 0.00002032
Iteration 256/1000 | Loss: 0.00002032
Iteration 257/1000 | Loss: 0.00002032
Iteration 258/1000 | Loss: 0.00002032
Iteration 259/1000 | Loss: 0.00002032
Iteration 260/1000 | Loss: 0.00002032
Iteration 261/1000 | Loss: 0.00002032
Iteration 262/1000 | Loss: 0.00002032
Iteration 263/1000 | Loss: 0.00002032
Iteration 264/1000 | Loss: 0.00002032
Iteration 265/1000 | Loss: 0.00002031
Iteration 266/1000 | Loss: 0.00002031
Iteration 267/1000 | Loss: 0.00002031
Iteration 268/1000 | Loss: 0.00002031
Iteration 269/1000 | Loss: 0.00002031
Iteration 270/1000 | Loss: 0.00002031
Iteration 271/1000 | Loss: 0.00002031
Iteration 272/1000 | Loss: 0.00002031
Iteration 273/1000 | Loss: 0.00002030
Iteration 274/1000 | Loss: 0.00002030
Iteration 275/1000 | Loss: 0.00002030
Iteration 276/1000 | Loss: 0.00002030
Iteration 277/1000 | Loss: 0.00002030
Iteration 278/1000 | Loss: 0.00002030
Iteration 279/1000 | Loss: 0.00002030
Iteration 280/1000 | Loss: 0.00002030
Iteration 281/1000 | Loss: 0.00002030
Iteration 282/1000 | Loss: 0.00002030
Iteration 283/1000 | Loss: 0.00002030
Iteration 284/1000 | Loss: 0.00002030
Iteration 285/1000 | Loss: 0.00002030
Iteration 286/1000 | Loss: 0.00002030
Iteration 287/1000 | Loss: 0.00002030
Iteration 288/1000 | Loss: 0.00002030
Iteration 289/1000 | Loss: 0.00002030
Iteration 290/1000 | Loss: 0.00002030
Iteration 291/1000 | Loss: 0.00002030
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 291. Stopping optimization.
Last 5 losses: [2.030074938375037e-05, 2.030074938375037e-05, 2.030074938375037e-05, 2.030074938375037e-05, 2.030074938375037e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.030074938375037e-05

Optimization complete. Final v2v error: 3.6689188480377197 mm

Highest mean error: 5.283751010894775 mm for frame 45

Lowest mean error: 2.5423762798309326 mm for frame 33

Saving results

Total time: 51.83004403114319
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_025/1076/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_025/1076.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_025/1076
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00847649
Iteration 2/25 | Loss: 0.00162893
Iteration 3/25 | Loss: 0.00139545
Iteration 4/25 | Loss: 0.00131131
Iteration 5/25 | Loss: 0.00130305
Iteration 6/25 | Loss: 0.00129082
Iteration 7/25 | Loss: 0.00126674
Iteration 8/25 | Loss: 0.00124829
Iteration 9/25 | Loss: 0.00124414
Iteration 10/25 | Loss: 0.00124041
Iteration 11/25 | Loss: 0.00123945
Iteration 12/25 | Loss: 0.00123892
Iteration 13/25 | Loss: 0.00123873
Iteration 14/25 | Loss: 0.00123866
Iteration 15/25 | Loss: 0.00123866
Iteration 16/25 | Loss: 0.00123866
Iteration 17/25 | Loss: 0.00123866
Iteration 18/25 | Loss: 0.00123866
Iteration 19/25 | Loss: 0.00123865
Iteration 20/25 | Loss: 0.00123865
Iteration 21/25 | Loss: 0.00123865
Iteration 22/25 | Loss: 0.00123865
Iteration 23/25 | Loss: 0.00123865
Iteration 24/25 | Loss: 0.00123865
Iteration 25/25 | Loss: 0.00123865

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 8.79459476
Iteration 2/25 | Loss: 0.00156169
Iteration 3/25 | Loss: 0.00156169
Iteration 4/25 | Loss: 0.00156169
Iteration 5/25 | Loss: 0.00156169
Iteration 6/25 | Loss: 0.00156169
Iteration 7/25 | Loss: 0.00156169
Iteration 8/25 | Loss: 0.00156169
Iteration 9/25 | Loss: 0.00156169
Iteration 10/25 | Loss: 0.00156169
Iteration 11/25 | Loss: 0.00156169
Iteration 12/25 | Loss: 0.00156169
Iteration 13/25 | Loss: 0.00156169
Iteration 14/25 | Loss: 0.00156169
Iteration 15/25 | Loss: 0.00156169
Iteration 16/25 | Loss: 0.00156169
Iteration 17/25 | Loss: 0.00156169
Iteration 18/25 | Loss: 0.00156169
Iteration 19/25 | Loss: 0.00156169
Iteration 20/25 | Loss: 0.00156169
Iteration 21/25 | Loss: 0.00156169
Iteration 22/25 | Loss: 0.00156169
Iteration 23/25 | Loss: 0.00156169
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0015616884920746088, 0.0015616884920746088, 0.0015616884920746088, 0.0015616884920746088, 0.0015616884920746088]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015616884920746088

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00156169
Iteration 2/1000 | Loss: 0.00002192
Iteration 3/1000 | Loss: 0.00001862
Iteration 4/1000 | Loss: 0.00002286
Iteration 5/1000 | Loss: 0.00002342
Iteration 6/1000 | Loss: 0.00001451
Iteration 7/1000 | Loss: 0.00003690
Iteration 8/1000 | Loss: 0.00004757
Iteration 9/1000 | Loss: 0.00001360
Iteration 10/1000 | Loss: 0.00001350
Iteration 11/1000 | Loss: 0.00002392
Iteration 12/1000 | Loss: 0.00001950
Iteration 13/1000 | Loss: 0.00001304
Iteration 14/1000 | Loss: 0.00001293
Iteration 15/1000 | Loss: 0.00001378
Iteration 16/1000 | Loss: 0.00001280
Iteration 17/1000 | Loss: 0.00001278
Iteration 18/1000 | Loss: 0.00001278
Iteration 19/1000 | Loss: 0.00001276
Iteration 20/1000 | Loss: 0.00001275
Iteration 21/1000 | Loss: 0.00001275
Iteration 22/1000 | Loss: 0.00001274
Iteration 23/1000 | Loss: 0.00001273
Iteration 24/1000 | Loss: 0.00004307
Iteration 25/1000 | Loss: 0.00002376
Iteration 26/1000 | Loss: 0.00001319
Iteration 27/1000 | Loss: 0.00001260
Iteration 28/1000 | Loss: 0.00002165
Iteration 29/1000 | Loss: 0.00001260
Iteration 30/1000 | Loss: 0.00001260
Iteration 31/1000 | Loss: 0.00001260
Iteration 32/1000 | Loss: 0.00001258
Iteration 33/1000 | Loss: 0.00001258
Iteration 34/1000 | Loss: 0.00001257
Iteration 35/1000 | Loss: 0.00001256
Iteration 36/1000 | Loss: 0.00001256
Iteration 37/1000 | Loss: 0.00001256
Iteration 38/1000 | Loss: 0.00001256
Iteration 39/1000 | Loss: 0.00001256
Iteration 40/1000 | Loss: 0.00001256
Iteration 41/1000 | Loss: 0.00001256
Iteration 42/1000 | Loss: 0.00001255
Iteration 43/1000 | Loss: 0.00001255
Iteration 44/1000 | Loss: 0.00001255
Iteration 45/1000 | Loss: 0.00001255
Iteration 46/1000 | Loss: 0.00001253
Iteration 47/1000 | Loss: 0.00001252
Iteration 48/1000 | Loss: 0.00001251
Iteration 49/1000 | Loss: 0.00001251
Iteration 50/1000 | Loss: 0.00001250
Iteration 51/1000 | Loss: 0.00001246
Iteration 52/1000 | Loss: 0.00001246
Iteration 53/1000 | Loss: 0.00001246
Iteration 54/1000 | Loss: 0.00001246
Iteration 55/1000 | Loss: 0.00001243
Iteration 56/1000 | Loss: 0.00001243
Iteration 57/1000 | Loss: 0.00001810
Iteration 58/1000 | Loss: 0.00001353
Iteration 59/1000 | Loss: 0.00001246
Iteration 60/1000 | Loss: 0.00001246
Iteration 61/1000 | Loss: 0.00001246
Iteration 62/1000 | Loss: 0.00001237
Iteration 63/1000 | Loss: 0.00001237
Iteration 64/1000 | Loss: 0.00001237
Iteration 65/1000 | Loss: 0.00001237
Iteration 66/1000 | Loss: 0.00001237
Iteration 67/1000 | Loss: 0.00001237
Iteration 68/1000 | Loss: 0.00001237
Iteration 69/1000 | Loss: 0.00001237
Iteration 70/1000 | Loss: 0.00001237
Iteration 71/1000 | Loss: 0.00001237
Iteration 72/1000 | Loss: 0.00001237
Iteration 73/1000 | Loss: 0.00001237
Iteration 74/1000 | Loss: 0.00001237
Iteration 75/1000 | Loss: 0.00001237
Iteration 76/1000 | Loss: 0.00001237
Iteration 77/1000 | Loss: 0.00001237
Iteration 78/1000 | Loss: 0.00001237
Iteration 79/1000 | Loss: 0.00001237
Iteration 80/1000 | Loss: 0.00001237
Iteration 81/1000 | Loss: 0.00001237
Iteration 82/1000 | Loss: 0.00001237
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 82. Stopping optimization.
Last 5 losses: [1.2368625903036445e-05, 1.2368625903036445e-05, 1.2368625903036445e-05, 1.2368625903036445e-05, 1.2368625903036445e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2368625903036445e-05

Optimization complete. Final v2v error: 2.9988505840301514 mm

Highest mean error: 3.3545589447021484 mm for frame 189

Lowest mean error: 2.754664659500122 mm for frame 101

Saving results

Total time: 66.43971538543701
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_025/1075/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_025/1075.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_025/1075
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01076011
Iteration 2/25 | Loss: 0.00301388
Iteration 3/25 | Loss: 0.00219436
Iteration 4/25 | Loss: 0.00204425
Iteration 5/25 | Loss: 0.00175592
Iteration 6/25 | Loss: 0.00167259
Iteration 7/25 | Loss: 0.00165496
Iteration 8/25 | Loss: 0.00164762
Iteration 9/25 | Loss: 0.00164641
Iteration 10/25 | Loss: 0.00164377
Iteration 11/25 | Loss: 0.00164343
Iteration 12/25 | Loss: 0.00164317
Iteration 13/25 | Loss: 0.00164296
Iteration 14/25 | Loss: 0.00164284
Iteration 15/25 | Loss: 0.00164282
Iteration 16/25 | Loss: 0.00164282
Iteration 17/25 | Loss: 0.00164282
Iteration 18/25 | Loss: 0.00164282
Iteration 19/25 | Loss: 0.00164282
Iteration 20/25 | Loss: 0.00164282
Iteration 21/25 | Loss: 0.00164282
Iteration 22/25 | Loss: 0.00164281
Iteration 23/25 | Loss: 0.00164281
Iteration 24/25 | Loss: 0.00164281
Iteration 25/25 | Loss: 0.00164281

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.14995790
Iteration 2/25 | Loss: 0.00273414
Iteration 3/25 | Loss: 0.00273413
Iteration 4/25 | Loss: 0.00273413
Iteration 5/25 | Loss: 0.00273413
Iteration 6/25 | Loss: 0.00273413
Iteration 7/25 | Loss: 0.00273413
Iteration 8/25 | Loss: 0.00273413
Iteration 9/25 | Loss: 0.00273413
Iteration 10/25 | Loss: 0.00273413
Iteration 11/25 | Loss: 0.00273413
Iteration 12/25 | Loss: 0.00273413
Iteration 13/25 | Loss: 0.00273413
Iteration 14/25 | Loss: 0.00273413
Iteration 15/25 | Loss: 0.00273413
Iteration 16/25 | Loss: 0.00273413
Iteration 17/25 | Loss: 0.00273413
Iteration 18/25 | Loss: 0.00273413
Iteration 19/25 | Loss: 0.00273413
Iteration 20/25 | Loss: 0.00273413
Iteration 21/25 | Loss: 0.00273413
Iteration 22/25 | Loss: 0.00273413
Iteration 23/25 | Loss: 0.00273413
Iteration 24/25 | Loss: 0.00273413
Iteration 25/25 | Loss: 0.00273413

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00273413
Iteration 2/1000 | Loss: 0.00029510
Iteration 3/1000 | Loss: 0.00023179
Iteration 4/1000 | Loss: 0.00020349
Iteration 5/1000 | Loss: 0.00018523
Iteration 6/1000 | Loss: 0.00017500
Iteration 7/1000 | Loss: 0.00016706
Iteration 8/1000 | Loss: 0.00016077
Iteration 9/1000 | Loss: 0.00015648
Iteration 10/1000 | Loss: 0.00015290
Iteration 11/1000 | Loss: 0.00014930
Iteration 12/1000 | Loss: 0.00014739
Iteration 13/1000 | Loss: 0.00014499
Iteration 14/1000 | Loss: 0.00014371
Iteration 15/1000 | Loss: 0.00014280
Iteration 16/1000 | Loss: 0.00014152
Iteration 17/1000 | Loss: 0.00014065
Iteration 18/1000 | Loss: 0.00014015
Iteration 19/1000 | Loss: 0.00013978
Iteration 20/1000 | Loss: 0.00013950
Iteration 21/1000 | Loss: 0.00013923
Iteration 22/1000 | Loss: 0.00013896
Iteration 23/1000 | Loss: 0.00013885
Iteration 24/1000 | Loss: 0.00013874
Iteration 25/1000 | Loss: 0.00013870
Iteration 26/1000 | Loss: 0.00013870
Iteration 27/1000 | Loss: 0.00013861
Iteration 28/1000 | Loss: 0.00013857
Iteration 29/1000 | Loss: 0.00013857
Iteration 30/1000 | Loss: 0.00013854
Iteration 31/1000 | Loss: 0.00013849
Iteration 32/1000 | Loss: 0.00013846
Iteration 33/1000 | Loss: 0.00013846
Iteration 34/1000 | Loss: 0.00013846
Iteration 35/1000 | Loss: 0.00013846
Iteration 36/1000 | Loss: 0.00013846
Iteration 37/1000 | Loss: 0.00013846
Iteration 38/1000 | Loss: 0.00013846
Iteration 39/1000 | Loss: 0.00013846
Iteration 40/1000 | Loss: 0.00013846
Iteration 41/1000 | Loss: 0.00013846
Iteration 42/1000 | Loss: 0.00013846
Iteration 43/1000 | Loss: 0.00013845
Iteration 44/1000 | Loss: 0.00013845
Iteration 45/1000 | Loss: 0.00013844
Iteration 46/1000 | Loss: 0.00013840
Iteration 47/1000 | Loss: 0.00013835
Iteration 48/1000 | Loss: 0.00013835
Iteration 49/1000 | Loss: 0.00013827
Iteration 50/1000 | Loss: 0.00013824
Iteration 51/1000 | Loss: 0.00013823
Iteration 52/1000 | Loss: 0.00013817
Iteration 53/1000 | Loss: 0.00013815
Iteration 54/1000 | Loss: 0.00013809
Iteration 55/1000 | Loss: 0.00013807
Iteration 56/1000 | Loss: 0.00013806
Iteration 57/1000 | Loss: 0.00013806
Iteration 58/1000 | Loss: 0.00013805
Iteration 59/1000 | Loss: 0.00013803
Iteration 60/1000 | Loss: 0.00013803
Iteration 61/1000 | Loss: 0.00013803
Iteration 62/1000 | Loss: 0.00013803
Iteration 63/1000 | Loss: 0.00013802
Iteration 64/1000 | Loss: 0.00013802
Iteration 65/1000 | Loss: 0.00013802
Iteration 66/1000 | Loss: 0.00013802
Iteration 67/1000 | Loss: 0.00013802
Iteration 68/1000 | Loss: 0.00013802
Iteration 69/1000 | Loss: 0.00013802
Iteration 70/1000 | Loss: 0.00013801
Iteration 71/1000 | Loss: 0.00013801
Iteration 72/1000 | Loss: 0.00013801
Iteration 73/1000 | Loss: 0.00013801
Iteration 74/1000 | Loss: 0.00013801
Iteration 75/1000 | Loss: 0.00013800
Iteration 76/1000 | Loss: 0.00013799
Iteration 77/1000 | Loss: 0.00013798
Iteration 78/1000 | Loss: 0.00013798
Iteration 79/1000 | Loss: 0.00013798
Iteration 80/1000 | Loss: 0.00013798
Iteration 81/1000 | Loss: 0.00013798
Iteration 82/1000 | Loss: 0.00013798
Iteration 83/1000 | Loss: 0.00013798
Iteration 84/1000 | Loss: 0.00013798
Iteration 85/1000 | Loss: 0.00013798
Iteration 86/1000 | Loss: 0.00013797
Iteration 87/1000 | Loss: 0.00013797
Iteration 88/1000 | Loss: 0.00013797
Iteration 89/1000 | Loss: 0.00013796
Iteration 90/1000 | Loss: 0.00013796
Iteration 91/1000 | Loss: 0.00013795
Iteration 92/1000 | Loss: 0.00013795
Iteration 93/1000 | Loss: 0.00013795
Iteration 94/1000 | Loss: 0.00013795
Iteration 95/1000 | Loss: 0.00013795
Iteration 96/1000 | Loss: 0.00013795
Iteration 97/1000 | Loss: 0.00013795
Iteration 98/1000 | Loss: 0.00013795
Iteration 99/1000 | Loss: 0.00013795
Iteration 100/1000 | Loss: 0.00013795
Iteration 101/1000 | Loss: 0.00013795
Iteration 102/1000 | Loss: 0.00013795
Iteration 103/1000 | Loss: 0.00013795
Iteration 104/1000 | Loss: 0.00013795
Iteration 105/1000 | Loss: 0.00013795
Iteration 106/1000 | Loss: 0.00013795
Iteration 107/1000 | Loss: 0.00013795
Iteration 108/1000 | Loss: 0.00013795
Iteration 109/1000 | Loss: 0.00013795
Iteration 110/1000 | Loss: 0.00013795
Iteration 111/1000 | Loss: 0.00013794
Iteration 112/1000 | Loss: 0.00013794
Iteration 113/1000 | Loss: 0.00013794
Iteration 114/1000 | Loss: 0.00013794
Iteration 115/1000 | Loss: 0.00013794
Iteration 116/1000 | Loss: 0.00013794
Iteration 117/1000 | Loss: 0.00013794
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 117. Stopping optimization.
Last 5 losses: [0.0001379444292979315, 0.0001379444292979315, 0.0001379444292979315, 0.0001379444292979315, 0.0001379444292979315]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0001379444292979315

Optimization complete. Final v2v error: 6.94398307800293 mm

Highest mean error: 10.974292755126953 mm for frame 110

Lowest mean error: 4.860481262207031 mm for frame 67

Saving results

Total time: 81.98901438713074
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_025/1041/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_025/1041.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_025/1041
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01005951
Iteration 2/25 | Loss: 0.00235696
Iteration 3/25 | Loss: 0.00307659
Iteration 4/25 | Loss: 0.00239511
Iteration 5/25 | Loss: 0.00194900
Iteration 6/25 | Loss: 0.00173457
Iteration 7/25 | Loss: 0.00154173
Iteration 8/25 | Loss: 0.00146079
Iteration 9/25 | Loss: 0.00140926
Iteration 10/25 | Loss: 0.00136586
Iteration 11/25 | Loss: 0.00133321
Iteration 12/25 | Loss: 0.00134670
Iteration 13/25 | Loss: 0.00134585
Iteration 14/25 | Loss: 0.00132817
Iteration 15/25 | Loss: 0.00134141
Iteration 16/25 | Loss: 0.00133562
Iteration 17/25 | Loss: 0.00130661
Iteration 18/25 | Loss: 0.00129671
Iteration 19/25 | Loss: 0.00129625
Iteration 20/25 | Loss: 0.00129574
Iteration 21/25 | Loss: 0.00129424
Iteration 22/25 | Loss: 0.00129548
Iteration 23/25 | Loss: 0.00129487
Iteration 24/25 | Loss: 0.00129553
Iteration 25/25 | Loss: 0.00129190

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36227453
Iteration 2/25 | Loss: 0.00168521
Iteration 3/25 | Loss: 0.00168521
Iteration 4/25 | Loss: 0.00168521
Iteration 5/25 | Loss: 0.00168521
Iteration 6/25 | Loss: 0.00168521
Iteration 7/25 | Loss: 0.00168521
Iteration 8/25 | Loss: 0.00168521
Iteration 9/25 | Loss: 0.00168521
Iteration 10/25 | Loss: 0.00168521
Iteration 11/25 | Loss: 0.00168521
Iteration 12/25 | Loss: 0.00168521
Iteration 13/25 | Loss: 0.00168521
Iteration 14/25 | Loss: 0.00168521
Iteration 15/25 | Loss: 0.00168521
Iteration 16/25 | Loss: 0.00168521
Iteration 17/25 | Loss: 0.00168521
Iteration 18/25 | Loss: 0.00168521
Iteration 19/25 | Loss: 0.00168521
Iteration 20/25 | Loss: 0.00168521
Iteration 21/25 | Loss: 0.00168521
Iteration 22/25 | Loss: 0.00168521
Iteration 23/25 | Loss: 0.00168521
Iteration 24/25 | Loss: 0.00168521
Iteration 25/25 | Loss: 0.00168521

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00168521
Iteration 2/1000 | Loss: 0.00023044
Iteration 3/1000 | Loss: 0.00081127
Iteration 4/1000 | Loss: 0.00012818
Iteration 5/1000 | Loss: 0.00004191
Iteration 6/1000 | Loss: 0.00003428
Iteration 7/1000 | Loss: 0.00003117
Iteration 8/1000 | Loss: 0.00016747
Iteration 9/1000 | Loss: 0.00097138
Iteration 10/1000 | Loss: 0.00032619
Iteration 11/1000 | Loss: 0.00029701
Iteration 12/1000 | Loss: 0.00026277
Iteration 13/1000 | Loss: 0.00039054
Iteration 14/1000 | Loss: 0.00030348
Iteration 15/1000 | Loss: 0.00076353
Iteration 16/1000 | Loss: 0.00043767
Iteration 17/1000 | Loss: 0.00028798
Iteration 18/1000 | Loss: 0.00034133
Iteration 19/1000 | Loss: 0.00032599
Iteration 20/1000 | Loss: 0.00024169
Iteration 21/1000 | Loss: 0.00023966
Iteration 22/1000 | Loss: 0.00012651
Iteration 23/1000 | Loss: 0.00007009
Iteration 24/1000 | Loss: 0.00002904
Iteration 25/1000 | Loss: 0.00010064
Iteration 26/1000 | Loss: 0.00008246
Iteration 27/1000 | Loss: 0.00008040
Iteration 28/1000 | Loss: 0.00010580
Iteration 29/1000 | Loss: 0.00055948
Iteration 30/1000 | Loss: 0.00013529
Iteration 31/1000 | Loss: 0.00009107
Iteration 32/1000 | Loss: 0.00008094
Iteration 33/1000 | Loss: 0.00007530
Iteration 34/1000 | Loss: 0.00010654
Iteration 35/1000 | Loss: 0.00007898
Iteration 36/1000 | Loss: 0.00002926
Iteration 37/1000 | Loss: 0.00011863
Iteration 38/1000 | Loss: 0.00019948
Iteration 39/1000 | Loss: 0.00009973
Iteration 40/1000 | Loss: 0.00006306
Iteration 41/1000 | Loss: 0.00020274
Iteration 42/1000 | Loss: 0.00003327
Iteration 43/1000 | Loss: 0.00017043
Iteration 44/1000 | Loss: 0.00009362
Iteration 45/1000 | Loss: 0.00007805
Iteration 46/1000 | Loss: 0.00002566
Iteration 47/1000 | Loss: 0.00002379
Iteration 48/1000 | Loss: 0.00002218
Iteration 49/1000 | Loss: 0.00002170
Iteration 50/1000 | Loss: 0.00002130
Iteration 51/1000 | Loss: 0.00002095
Iteration 52/1000 | Loss: 0.00002060
Iteration 53/1000 | Loss: 0.00002035
Iteration 54/1000 | Loss: 0.00002010
Iteration 55/1000 | Loss: 0.00001992
Iteration 56/1000 | Loss: 0.00001979
Iteration 57/1000 | Loss: 0.00001977
Iteration 58/1000 | Loss: 0.00001975
Iteration 59/1000 | Loss: 0.00001974
Iteration 60/1000 | Loss: 0.00001973
Iteration 61/1000 | Loss: 0.00001966
Iteration 62/1000 | Loss: 0.00001958
Iteration 63/1000 | Loss: 0.00001956
Iteration 64/1000 | Loss: 0.00001955
Iteration 65/1000 | Loss: 0.00001951
Iteration 66/1000 | Loss: 0.00001946
Iteration 67/1000 | Loss: 0.00001945
Iteration 68/1000 | Loss: 0.00001944
Iteration 69/1000 | Loss: 0.00001944
Iteration 70/1000 | Loss: 0.00001943
Iteration 71/1000 | Loss: 0.00001942
Iteration 72/1000 | Loss: 0.00001942
Iteration 73/1000 | Loss: 0.00001942
Iteration 74/1000 | Loss: 0.00001942
Iteration 75/1000 | Loss: 0.00001942
Iteration 76/1000 | Loss: 0.00001942
Iteration 77/1000 | Loss: 0.00001942
Iteration 78/1000 | Loss: 0.00001941
Iteration 79/1000 | Loss: 0.00001941
Iteration 80/1000 | Loss: 0.00001941
Iteration 81/1000 | Loss: 0.00001941
Iteration 82/1000 | Loss: 0.00001941
Iteration 83/1000 | Loss: 0.00001940
Iteration 84/1000 | Loss: 0.00001940
Iteration 85/1000 | Loss: 0.00001940
Iteration 86/1000 | Loss: 0.00001940
Iteration 87/1000 | Loss: 0.00001940
Iteration 88/1000 | Loss: 0.00001940
Iteration 89/1000 | Loss: 0.00001940
Iteration 90/1000 | Loss: 0.00001940
Iteration 91/1000 | Loss: 0.00001940
Iteration 92/1000 | Loss: 0.00001939
Iteration 93/1000 | Loss: 0.00001939
Iteration 94/1000 | Loss: 0.00001939
Iteration 95/1000 | Loss: 0.00001939
Iteration 96/1000 | Loss: 0.00001939
Iteration 97/1000 | Loss: 0.00001939
Iteration 98/1000 | Loss: 0.00001939
Iteration 99/1000 | Loss: 0.00001939
Iteration 100/1000 | Loss: 0.00001939
Iteration 101/1000 | Loss: 0.00001939
Iteration 102/1000 | Loss: 0.00001938
Iteration 103/1000 | Loss: 0.00001938
Iteration 104/1000 | Loss: 0.00001938
Iteration 105/1000 | Loss: 0.00001938
Iteration 106/1000 | Loss: 0.00001938
Iteration 107/1000 | Loss: 0.00001937
Iteration 108/1000 | Loss: 0.00001937
Iteration 109/1000 | Loss: 0.00001937
Iteration 110/1000 | Loss: 0.00001937
Iteration 111/1000 | Loss: 0.00001937
Iteration 112/1000 | Loss: 0.00001937
Iteration 113/1000 | Loss: 0.00001936
Iteration 114/1000 | Loss: 0.00001936
Iteration 115/1000 | Loss: 0.00001936
Iteration 116/1000 | Loss: 0.00001935
Iteration 117/1000 | Loss: 0.00001935
Iteration 118/1000 | Loss: 0.00001935
Iteration 119/1000 | Loss: 0.00001935
Iteration 120/1000 | Loss: 0.00001934
Iteration 121/1000 | Loss: 0.00001934
Iteration 122/1000 | Loss: 0.00001934
Iteration 123/1000 | Loss: 0.00001934
Iteration 124/1000 | Loss: 0.00001933
Iteration 125/1000 | Loss: 0.00001933
Iteration 126/1000 | Loss: 0.00001933
Iteration 127/1000 | Loss: 0.00001933
Iteration 128/1000 | Loss: 0.00001933
Iteration 129/1000 | Loss: 0.00001932
Iteration 130/1000 | Loss: 0.00001932
Iteration 131/1000 | Loss: 0.00001932
Iteration 132/1000 | Loss: 0.00001932
Iteration 133/1000 | Loss: 0.00001932
Iteration 134/1000 | Loss: 0.00001932
Iteration 135/1000 | Loss: 0.00001932
Iteration 136/1000 | Loss: 0.00001932
Iteration 137/1000 | Loss: 0.00001931
Iteration 138/1000 | Loss: 0.00001931
Iteration 139/1000 | Loss: 0.00001931
Iteration 140/1000 | Loss: 0.00001931
Iteration 141/1000 | Loss: 0.00001930
Iteration 142/1000 | Loss: 0.00001930
Iteration 143/1000 | Loss: 0.00001930
Iteration 144/1000 | Loss: 0.00001930
Iteration 145/1000 | Loss: 0.00001929
Iteration 146/1000 | Loss: 0.00001929
Iteration 147/1000 | Loss: 0.00001929
Iteration 148/1000 | Loss: 0.00001929
Iteration 149/1000 | Loss: 0.00001929
Iteration 150/1000 | Loss: 0.00001928
Iteration 151/1000 | Loss: 0.00001928
Iteration 152/1000 | Loss: 0.00001928
Iteration 153/1000 | Loss: 0.00001928
Iteration 154/1000 | Loss: 0.00001927
Iteration 155/1000 | Loss: 0.00001927
Iteration 156/1000 | Loss: 0.00001927
Iteration 157/1000 | Loss: 0.00001927
Iteration 158/1000 | Loss: 0.00001927
Iteration 159/1000 | Loss: 0.00001926
Iteration 160/1000 | Loss: 0.00001926
Iteration 161/1000 | Loss: 0.00001926
Iteration 162/1000 | Loss: 0.00001926
Iteration 163/1000 | Loss: 0.00001926
Iteration 164/1000 | Loss: 0.00001926
Iteration 165/1000 | Loss: 0.00001926
Iteration 166/1000 | Loss: 0.00001926
Iteration 167/1000 | Loss: 0.00001925
Iteration 168/1000 | Loss: 0.00001925
Iteration 169/1000 | Loss: 0.00001925
Iteration 170/1000 | Loss: 0.00001925
Iteration 171/1000 | Loss: 0.00001925
Iteration 172/1000 | Loss: 0.00001925
Iteration 173/1000 | Loss: 0.00001925
Iteration 174/1000 | Loss: 0.00001925
Iteration 175/1000 | Loss: 0.00001925
Iteration 176/1000 | Loss: 0.00001925
Iteration 177/1000 | Loss: 0.00001925
Iteration 178/1000 | Loss: 0.00001925
Iteration 179/1000 | Loss: 0.00001925
Iteration 180/1000 | Loss: 0.00001925
Iteration 181/1000 | Loss: 0.00001925
Iteration 182/1000 | Loss: 0.00001925
Iteration 183/1000 | Loss: 0.00001925
Iteration 184/1000 | Loss: 0.00001924
Iteration 185/1000 | Loss: 0.00001924
Iteration 186/1000 | Loss: 0.00001924
Iteration 187/1000 | Loss: 0.00001924
Iteration 188/1000 | Loss: 0.00001924
Iteration 189/1000 | Loss: 0.00001924
Iteration 190/1000 | Loss: 0.00001924
Iteration 191/1000 | Loss: 0.00001924
Iteration 192/1000 | Loss: 0.00001924
Iteration 193/1000 | Loss: 0.00001924
Iteration 194/1000 | Loss: 0.00001924
Iteration 195/1000 | Loss: 0.00001924
Iteration 196/1000 | Loss: 0.00001924
Iteration 197/1000 | Loss: 0.00001924
Iteration 198/1000 | Loss: 0.00001924
Iteration 199/1000 | Loss: 0.00001924
Iteration 200/1000 | Loss: 0.00001924
Iteration 201/1000 | Loss: 0.00001924
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 201. Stopping optimization.
Last 5 losses: [1.924196294567082e-05, 1.924196294567082e-05, 1.924196294567082e-05, 1.924196294567082e-05, 1.924196294567082e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.924196294567082e-05

Optimization complete. Final v2v error: 3.5680840015411377 mm

Highest mean error: 11.842458724975586 mm for frame 107

Lowest mean error: 2.9588656425476074 mm for frame 170

Saving results

Total time: 157.65037441253662
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_025/1074/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_025/1074.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_025/1074
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00797927
Iteration 2/25 | Loss: 0.00208801
Iteration 3/25 | Loss: 0.00161941
Iteration 4/25 | Loss: 0.00147434
Iteration 5/25 | Loss: 0.00147015
Iteration 6/25 | Loss: 0.00136671
Iteration 7/25 | Loss: 0.00135636
Iteration 8/25 | Loss: 0.00135517
Iteration 9/25 | Loss: 0.00135492
Iteration 10/25 | Loss: 0.00135492
Iteration 11/25 | Loss: 0.00135492
Iteration 12/25 | Loss: 0.00135492
Iteration 13/25 | Loss: 0.00135492
Iteration 14/25 | Loss: 0.00135492
Iteration 15/25 | Loss: 0.00135492
Iteration 16/25 | Loss: 0.00135492
Iteration 17/25 | Loss: 0.00135492
Iteration 18/25 | Loss: 0.00135492
Iteration 19/25 | Loss: 0.00135492
Iteration 20/25 | Loss: 0.00135492
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.001354919746518135, 0.001354919746518135, 0.001354919746518135, 0.001354919746518135, 0.001354919746518135]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001354919746518135

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.21892226
Iteration 2/25 | Loss: 0.00103627
Iteration 3/25 | Loss: 0.00103625
Iteration 4/25 | Loss: 0.00103625
Iteration 5/25 | Loss: 0.00103625
Iteration 6/25 | Loss: 0.00103625
Iteration 7/25 | Loss: 0.00103625
Iteration 8/25 | Loss: 0.00103625
Iteration 9/25 | Loss: 0.00103625
Iteration 10/25 | Loss: 0.00103625
Iteration 11/25 | Loss: 0.00103625
Iteration 12/25 | Loss: 0.00103625
Iteration 13/25 | Loss: 0.00103625
Iteration 14/25 | Loss: 0.00103625
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0010362478205934167, 0.0010362478205934167, 0.0010362478205934167, 0.0010362478205934167, 0.0010362478205934167]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010362478205934167

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00103625
Iteration 2/1000 | Loss: 0.00004278
Iteration 3/1000 | Loss: 0.00002375
Iteration 4/1000 | Loss: 0.00002024
Iteration 5/1000 | Loss: 0.00001917
Iteration 6/1000 | Loss: 0.00001862
Iteration 7/1000 | Loss: 0.00001819
Iteration 8/1000 | Loss: 0.00001790
Iteration 9/1000 | Loss: 0.00001761
Iteration 10/1000 | Loss: 0.00001743
Iteration 11/1000 | Loss: 0.00001728
Iteration 12/1000 | Loss: 0.00001726
Iteration 13/1000 | Loss: 0.00001712
Iteration 14/1000 | Loss: 0.00001703
Iteration 15/1000 | Loss: 0.00001702
Iteration 16/1000 | Loss: 0.00001700
Iteration 17/1000 | Loss: 0.00001699
Iteration 18/1000 | Loss: 0.00001693
Iteration 19/1000 | Loss: 0.00001692
Iteration 20/1000 | Loss: 0.00001687
Iteration 21/1000 | Loss: 0.00001687
Iteration 22/1000 | Loss: 0.00001683
Iteration 23/1000 | Loss: 0.00001681
Iteration 24/1000 | Loss: 0.00001681
Iteration 25/1000 | Loss: 0.00001680
Iteration 26/1000 | Loss: 0.00001680
Iteration 27/1000 | Loss: 0.00001679
Iteration 28/1000 | Loss: 0.00001678
Iteration 29/1000 | Loss: 0.00001676
Iteration 30/1000 | Loss: 0.00001676
Iteration 31/1000 | Loss: 0.00001675
Iteration 32/1000 | Loss: 0.00001675
Iteration 33/1000 | Loss: 0.00001675
Iteration 34/1000 | Loss: 0.00001675
Iteration 35/1000 | Loss: 0.00001675
Iteration 36/1000 | Loss: 0.00001675
Iteration 37/1000 | Loss: 0.00001675
Iteration 38/1000 | Loss: 0.00001675
Iteration 39/1000 | Loss: 0.00001675
Iteration 40/1000 | Loss: 0.00001675
Iteration 41/1000 | Loss: 0.00001673
Iteration 42/1000 | Loss: 0.00001673
Iteration 43/1000 | Loss: 0.00001672
Iteration 44/1000 | Loss: 0.00001672
Iteration 45/1000 | Loss: 0.00001672
Iteration 46/1000 | Loss: 0.00001672
Iteration 47/1000 | Loss: 0.00001672
Iteration 48/1000 | Loss: 0.00001672
Iteration 49/1000 | Loss: 0.00001672
Iteration 50/1000 | Loss: 0.00001672
Iteration 51/1000 | Loss: 0.00001672
Iteration 52/1000 | Loss: 0.00001671
Iteration 53/1000 | Loss: 0.00001671
Iteration 54/1000 | Loss: 0.00001670
Iteration 55/1000 | Loss: 0.00001669
Iteration 56/1000 | Loss: 0.00001669
Iteration 57/1000 | Loss: 0.00001669
Iteration 58/1000 | Loss: 0.00001669
Iteration 59/1000 | Loss: 0.00001669
Iteration 60/1000 | Loss: 0.00001669
Iteration 61/1000 | Loss: 0.00001669
Iteration 62/1000 | Loss: 0.00001669
Iteration 63/1000 | Loss: 0.00001669
Iteration 64/1000 | Loss: 0.00001669
Iteration 65/1000 | Loss: 0.00001669
Iteration 66/1000 | Loss: 0.00001668
Iteration 67/1000 | Loss: 0.00001668
Iteration 68/1000 | Loss: 0.00001668
Iteration 69/1000 | Loss: 0.00001668
Iteration 70/1000 | Loss: 0.00001668
Iteration 71/1000 | Loss: 0.00001668
Iteration 72/1000 | Loss: 0.00001667
Iteration 73/1000 | Loss: 0.00001667
Iteration 74/1000 | Loss: 0.00001666
Iteration 75/1000 | Loss: 0.00001666
Iteration 76/1000 | Loss: 0.00001666
Iteration 77/1000 | Loss: 0.00001666
Iteration 78/1000 | Loss: 0.00001665
Iteration 79/1000 | Loss: 0.00001665
Iteration 80/1000 | Loss: 0.00001665
Iteration 81/1000 | Loss: 0.00001665
Iteration 82/1000 | Loss: 0.00001665
Iteration 83/1000 | Loss: 0.00001664
Iteration 84/1000 | Loss: 0.00001664
Iteration 85/1000 | Loss: 0.00001664
Iteration 86/1000 | Loss: 0.00001664
Iteration 87/1000 | Loss: 0.00001664
Iteration 88/1000 | Loss: 0.00001663
Iteration 89/1000 | Loss: 0.00001663
Iteration 90/1000 | Loss: 0.00001663
Iteration 91/1000 | Loss: 0.00001663
Iteration 92/1000 | Loss: 0.00001663
Iteration 93/1000 | Loss: 0.00001663
Iteration 94/1000 | Loss: 0.00001663
Iteration 95/1000 | Loss: 0.00001663
Iteration 96/1000 | Loss: 0.00001663
Iteration 97/1000 | Loss: 0.00001663
Iteration 98/1000 | Loss: 0.00001663
Iteration 99/1000 | Loss: 0.00001663
Iteration 100/1000 | Loss: 0.00001663
Iteration 101/1000 | Loss: 0.00001663
Iteration 102/1000 | Loss: 0.00001663
Iteration 103/1000 | Loss: 0.00001663
Iteration 104/1000 | Loss: 0.00001663
Iteration 105/1000 | Loss: 0.00001663
Iteration 106/1000 | Loss: 0.00001663
Iteration 107/1000 | Loss: 0.00001663
Iteration 108/1000 | Loss: 0.00001663
Iteration 109/1000 | Loss: 0.00001663
Iteration 110/1000 | Loss: 0.00001663
Iteration 111/1000 | Loss: 0.00001663
Iteration 112/1000 | Loss: 0.00001663
Iteration 113/1000 | Loss: 0.00001663
Iteration 114/1000 | Loss: 0.00001663
Iteration 115/1000 | Loss: 0.00001663
Iteration 116/1000 | Loss: 0.00001663
Iteration 117/1000 | Loss: 0.00001663
Iteration 118/1000 | Loss: 0.00001663
Iteration 119/1000 | Loss: 0.00001663
Iteration 120/1000 | Loss: 0.00001663
Iteration 121/1000 | Loss: 0.00001663
Iteration 122/1000 | Loss: 0.00001663
Iteration 123/1000 | Loss: 0.00001663
Iteration 124/1000 | Loss: 0.00001663
Iteration 125/1000 | Loss: 0.00001663
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 125. Stopping optimization.
Last 5 losses: [1.662583053985145e-05, 1.662583053985145e-05, 1.662583053985145e-05, 1.662583053985145e-05, 1.662583053985145e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.662583053985145e-05

Optimization complete. Final v2v error: 3.422593355178833 mm

Highest mean error: 3.7148351669311523 mm for frame 9

Lowest mean error: 3.3035762310028076 mm for frame 66

Saving results

Total time: 39.89192795753479
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_025/1094/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_025/1094.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_025/1094
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00607465
Iteration 2/25 | Loss: 0.00159361
Iteration 3/25 | Loss: 0.00138708
Iteration 4/25 | Loss: 0.00136520
Iteration 5/25 | Loss: 0.00135963
Iteration 6/25 | Loss: 0.00135946
Iteration 7/25 | Loss: 0.00135946
Iteration 8/25 | Loss: 0.00135946
Iteration 9/25 | Loss: 0.00135946
Iteration 10/25 | Loss: 0.00135946
Iteration 11/25 | Loss: 0.00135946
Iteration 12/25 | Loss: 0.00135946
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0013594608753919601, 0.0013594608753919601, 0.0013594608753919601, 0.0013594608753919601, 0.0013594608753919601]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013594608753919601

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.24178493
Iteration 2/25 | Loss: 0.00174435
Iteration 3/25 | Loss: 0.00174434
Iteration 4/25 | Loss: 0.00174434
Iteration 5/25 | Loss: 0.00174434
Iteration 6/25 | Loss: 0.00174434
Iteration 7/25 | Loss: 0.00174434
Iteration 8/25 | Loss: 0.00174434
Iteration 9/25 | Loss: 0.00174434
Iteration 10/25 | Loss: 0.00174434
Iteration 11/25 | Loss: 0.00174434
Iteration 12/25 | Loss: 0.00174434
Iteration 13/25 | Loss: 0.00174434
Iteration 14/25 | Loss: 0.00174434
Iteration 15/25 | Loss: 0.00174434
Iteration 16/25 | Loss: 0.00174434
Iteration 17/25 | Loss: 0.00174434
Iteration 18/25 | Loss: 0.00174434
Iteration 19/25 | Loss: 0.00174434
Iteration 20/25 | Loss: 0.00174434
Iteration 21/25 | Loss: 0.00174434
Iteration 22/25 | Loss: 0.00174434
Iteration 23/25 | Loss: 0.00174434
Iteration 24/25 | Loss: 0.00174434
Iteration 25/25 | Loss: 0.00174434

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00174434
Iteration 2/1000 | Loss: 0.00004323
Iteration 3/1000 | Loss: 0.00003465
Iteration 4/1000 | Loss: 0.00003270
Iteration 5/1000 | Loss: 0.00003158
Iteration 6/1000 | Loss: 0.00003065
Iteration 7/1000 | Loss: 0.00003003
Iteration 8/1000 | Loss: 0.00002965
Iteration 9/1000 | Loss: 0.00002932
Iteration 10/1000 | Loss: 0.00002900
Iteration 11/1000 | Loss: 0.00002877
Iteration 12/1000 | Loss: 0.00002855
Iteration 13/1000 | Loss: 0.00002840
Iteration 14/1000 | Loss: 0.00002829
Iteration 15/1000 | Loss: 0.00002818
Iteration 16/1000 | Loss: 0.00002817
Iteration 17/1000 | Loss: 0.00002815
Iteration 18/1000 | Loss: 0.00002814
Iteration 19/1000 | Loss: 0.00002814
Iteration 20/1000 | Loss: 0.00002813
Iteration 21/1000 | Loss: 0.00002813
Iteration 22/1000 | Loss: 0.00002813
Iteration 23/1000 | Loss: 0.00002812
Iteration 24/1000 | Loss: 0.00002812
Iteration 25/1000 | Loss: 0.00002811
Iteration 26/1000 | Loss: 0.00002811
Iteration 27/1000 | Loss: 0.00002810
Iteration 28/1000 | Loss: 0.00002810
Iteration 29/1000 | Loss: 0.00002810
Iteration 30/1000 | Loss: 0.00002810
Iteration 31/1000 | Loss: 0.00002810
Iteration 32/1000 | Loss: 0.00002810
Iteration 33/1000 | Loss: 0.00002810
Iteration 34/1000 | Loss: 0.00002810
Iteration 35/1000 | Loss: 0.00002810
Iteration 36/1000 | Loss: 0.00002810
Iteration 37/1000 | Loss: 0.00002810
Iteration 38/1000 | Loss: 0.00002810
Iteration 39/1000 | Loss: 0.00002810
Iteration 40/1000 | Loss: 0.00002810
Iteration 41/1000 | Loss: 0.00002810
Iteration 42/1000 | Loss: 0.00002808
Iteration 43/1000 | Loss: 0.00002808
Iteration 44/1000 | Loss: 0.00002808
Iteration 45/1000 | Loss: 0.00002808
Iteration 46/1000 | Loss: 0.00002808
Iteration 47/1000 | Loss: 0.00002808
Iteration 48/1000 | Loss: 0.00002808
Iteration 49/1000 | Loss: 0.00002808
Iteration 50/1000 | Loss: 0.00002807
Iteration 51/1000 | Loss: 0.00002807
Iteration 52/1000 | Loss: 0.00002807
Iteration 53/1000 | Loss: 0.00002807
Iteration 54/1000 | Loss: 0.00002807
Iteration 55/1000 | Loss: 0.00002807
Iteration 56/1000 | Loss: 0.00002807
Iteration 57/1000 | Loss: 0.00002807
Iteration 58/1000 | Loss: 0.00002807
Iteration 59/1000 | Loss: 0.00002806
Iteration 60/1000 | Loss: 0.00002805
Iteration 61/1000 | Loss: 0.00002805
Iteration 62/1000 | Loss: 0.00002805
Iteration 63/1000 | Loss: 0.00002805
Iteration 64/1000 | Loss: 0.00002805
Iteration 65/1000 | Loss: 0.00002804
Iteration 66/1000 | Loss: 0.00002804
Iteration 67/1000 | Loss: 0.00002803
Iteration 68/1000 | Loss: 0.00002803
Iteration 69/1000 | Loss: 0.00002803
Iteration 70/1000 | Loss: 0.00002803
Iteration 71/1000 | Loss: 0.00002803
Iteration 72/1000 | Loss: 0.00002802
Iteration 73/1000 | Loss: 0.00002802
Iteration 74/1000 | Loss: 0.00002802
Iteration 75/1000 | Loss: 0.00002802
Iteration 76/1000 | Loss: 0.00002802
Iteration 77/1000 | Loss: 0.00002802
Iteration 78/1000 | Loss: 0.00002802
Iteration 79/1000 | Loss: 0.00002802
Iteration 80/1000 | Loss: 0.00002801
Iteration 81/1000 | Loss: 0.00002801
Iteration 82/1000 | Loss: 0.00002800
Iteration 83/1000 | Loss: 0.00002800
Iteration 84/1000 | Loss: 0.00002800
Iteration 85/1000 | Loss: 0.00002800
Iteration 86/1000 | Loss: 0.00002799
Iteration 87/1000 | Loss: 0.00002799
Iteration 88/1000 | Loss: 0.00002798
Iteration 89/1000 | Loss: 0.00002797
Iteration 90/1000 | Loss: 0.00002794
Iteration 91/1000 | Loss: 0.00002793
Iteration 92/1000 | Loss: 0.00002792
Iteration 93/1000 | Loss: 0.00002792
Iteration 94/1000 | Loss: 0.00002792
Iteration 95/1000 | Loss: 0.00002792
Iteration 96/1000 | Loss: 0.00002791
Iteration 97/1000 | Loss: 0.00002791
Iteration 98/1000 | Loss: 0.00002791
Iteration 99/1000 | Loss: 0.00002790
Iteration 100/1000 | Loss: 0.00002790
Iteration 101/1000 | Loss: 0.00002790
Iteration 102/1000 | Loss: 0.00002790
Iteration 103/1000 | Loss: 0.00002789
Iteration 104/1000 | Loss: 0.00002789
Iteration 105/1000 | Loss: 0.00002789
Iteration 106/1000 | Loss: 0.00002789
Iteration 107/1000 | Loss: 0.00002789
Iteration 108/1000 | Loss: 0.00002789
Iteration 109/1000 | Loss: 0.00002789
Iteration 110/1000 | Loss: 0.00002789
Iteration 111/1000 | Loss: 0.00002788
Iteration 112/1000 | Loss: 0.00002788
Iteration 113/1000 | Loss: 0.00002787
Iteration 114/1000 | Loss: 0.00002787
Iteration 115/1000 | Loss: 0.00002786
Iteration 116/1000 | Loss: 0.00002786
Iteration 117/1000 | Loss: 0.00002786
Iteration 118/1000 | Loss: 0.00002786
Iteration 119/1000 | Loss: 0.00002785
Iteration 120/1000 | Loss: 0.00002785
Iteration 121/1000 | Loss: 0.00002785
Iteration 122/1000 | Loss: 0.00002785
Iteration 123/1000 | Loss: 0.00002785
Iteration 124/1000 | Loss: 0.00002785
Iteration 125/1000 | Loss: 0.00002785
Iteration 126/1000 | Loss: 0.00002785
Iteration 127/1000 | Loss: 0.00002785
Iteration 128/1000 | Loss: 0.00002785
Iteration 129/1000 | Loss: 0.00002784
Iteration 130/1000 | Loss: 0.00002784
Iteration 131/1000 | Loss: 0.00002783
Iteration 132/1000 | Loss: 0.00002783
Iteration 133/1000 | Loss: 0.00002783
Iteration 134/1000 | Loss: 0.00002783
Iteration 135/1000 | Loss: 0.00002783
Iteration 136/1000 | Loss: 0.00002782
Iteration 137/1000 | Loss: 0.00002782
Iteration 138/1000 | Loss: 0.00002782
Iteration 139/1000 | Loss: 0.00002782
Iteration 140/1000 | Loss: 0.00002782
Iteration 141/1000 | Loss: 0.00002782
Iteration 142/1000 | Loss: 0.00002782
Iteration 143/1000 | Loss: 0.00002782
Iteration 144/1000 | Loss: 0.00002782
Iteration 145/1000 | Loss: 0.00002782
Iteration 146/1000 | Loss: 0.00002782
Iteration 147/1000 | Loss: 0.00002782
Iteration 148/1000 | Loss: 0.00002781
Iteration 149/1000 | Loss: 0.00002781
Iteration 150/1000 | Loss: 0.00002781
Iteration 151/1000 | Loss: 0.00002781
Iteration 152/1000 | Loss: 0.00002781
Iteration 153/1000 | Loss: 0.00002781
Iteration 154/1000 | Loss: 0.00002781
Iteration 155/1000 | Loss: 0.00002781
Iteration 156/1000 | Loss: 0.00002781
Iteration 157/1000 | Loss: 0.00002781
Iteration 158/1000 | Loss: 0.00002781
Iteration 159/1000 | Loss: 0.00002781
Iteration 160/1000 | Loss: 0.00002781
Iteration 161/1000 | Loss: 0.00002781
Iteration 162/1000 | Loss: 0.00002781
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 162. Stopping optimization.
Last 5 losses: [2.7811245672637597e-05, 2.7811245672637597e-05, 2.7811245672637597e-05, 2.7811245672637597e-05, 2.7811245672637597e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.7811245672637597e-05

Optimization complete. Final v2v error: 4.4017252922058105 mm

Highest mean error: 4.829761028289795 mm for frame 38

Lowest mean error: 4.017419815063477 mm for frame 0

Saving results

Total time: 45.02852463722229
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_025/1066/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_025/1066.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_025/1066
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00568354
Iteration 2/25 | Loss: 0.00127808
Iteration 3/25 | Loss: 0.00121155
Iteration 4/25 | Loss: 0.00120243
Iteration 5/25 | Loss: 0.00119934
Iteration 6/25 | Loss: 0.00119864
Iteration 7/25 | Loss: 0.00119864
Iteration 8/25 | Loss: 0.00119864
Iteration 9/25 | Loss: 0.00119864
Iteration 10/25 | Loss: 0.00119864
Iteration 11/25 | Loss: 0.00119864
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001198642305098474, 0.001198642305098474, 0.001198642305098474, 0.001198642305098474, 0.001198642305098474]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001198642305098474

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.77508187
Iteration 2/25 | Loss: 0.00144214
Iteration 3/25 | Loss: 0.00144214
Iteration 4/25 | Loss: 0.00144214
Iteration 5/25 | Loss: 0.00144214
Iteration 6/25 | Loss: 0.00144214
Iteration 7/25 | Loss: 0.00144214
Iteration 8/25 | Loss: 0.00144214
Iteration 9/25 | Loss: 0.00144214
Iteration 10/25 | Loss: 0.00144214
Iteration 11/25 | Loss: 0.00144214
Iteration 12/25 | Loss: 0.00144214
Iteration 13/25 | Loss: 0.00144214
Iteration 14/25 | Loss: 0.00144214
Iteration 15/25 | Loss: 0.00144213
Iteration 16/25 | Loss: 0.00144213
Iteration 17/25 | Loss: 0.00144213
Iteration 18/25 | Loss: 0.00144213
Iteration 19/25 | Loss: 0.00144213
Iteration 20/25 | Loss: 0.00144213
Iteration 21/25 | Loss: 0.00144213
Iteration 22/25 | Loss: 0.00144213
Iteration 23/25 | Loss: 0.00144213
Iteration 24/25 | Loss: 0.00144213
Iteration 25/25 | Loss: 0.00144213

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00144213
Iteration 2/1000 | Loss: 0.00002021
Iteration 3/1000 | Loss: 0.00001420
Iteration 4/1000 | Loss: 0.00001270
Iteration 5/1000 | Loss: 0.00001201
Iteration 6/1000 | Loss: 0.00001137
Iteration 7/1000 | Loss: 0.00001093
Iteration 8/1000 | Loss: 0.00001073
Iteration 9/1000 | Loss: 0.00001034
Iteration 10/1000 | Loss: 0.00001013
Iteration 11/1000 | Loss: 0.00001012
Iteration 12/1000 | Loss: 0.00001011
Iteration 13/1000 | Loss: 0.00000998
Iteration 14/1000 | Loss: 0.00000997
Iteration 15/1000 | Loss: 0.00000997
Iteration 16/1000 | Loss: 0.00000983
Iteration 17/1000 | Loss: 0.00000979
Iteration 18/1000 | Loss: 0.00000975
Iteration 19/1000 | Loss: 0.00000975
Iteration 20/1000 | Loss: 0.00000972
Iteration 21/1000 | Loss: 0.00000972
Iteration 22/1000 | Loss: 0.00000971
Iteration 23/1000 | Loss: 0.00000967
Iteration 24/1000 | Loss: 0.00000965
Iteration 25/1000 | Loss: 0.00000964
Iteration 26/1000 | Loss: 0.00000959
Iteration 27/1000 | Loss: 0.00000954
Iteration 28/1000 | Loss: 0.00000953
Iteration 29/1000 | Loss: 0.00000951
Iteration 30/1000 | Loss: 0.00000946
Iteration 31/1000 | Loss: 0.00000946
Iteration 32/1000 | Loss: 0.00000946
Iteration 33/1000 | Loss: 0.00000944
Iteration 34/1000 | Loss: 0.00000942
Iteration 35/1000 | Loss: 0.00000942
Iteration 36/1000 | Loss: 0.00000941
Iteration 37/1000 | Loss: 0.00000941
Iteration 38/1000 | Loss: 0.00000941
Iteration 39/1000 | Loss: 0.00000941
Iteration 40/1000 | Loss: 0.00000941
Iteration 41/1000 | Loss: 0.00000940
Iteration 42/1000 | Loss: 0.00000940
Iteration 43/1000 | Loss: 0.00000940
Iteration 44/1000 | Loss: 0.00000940
Iteration 45/1000 | Loss: 0.00000940
Iteration 46/1000 | Loss: 0.00000940
Iteration 47/1000 | Loss: 0.00000940
Iteration 48/1000 | Loss: 0.00000940
Iteration 49/1000 | Loss: 0.00000940
Iteration 50/1000 | Loss: 0.00000939
Iteration 51/1000 | Loss: 0.00000939
Iteration 52/1000 | Loss: 0.00000938
Iteration 53/1000 | Loss: 0.00000938
Iteration 54/1000 | Loss: 0.00000938
Iteration 55/1000 | Loss: 0.00000937
Iteration 56/1000 | Loss: 0.00000937
Iteration 57/1000 | Loss: 0.00000937
Iteration 58/1000 | Loss: 0.00000937
Iteration 59/1000 | Loss: 0.00000936
Iteration 60/1000 | Loss: 0.00000936
Iteration 61/1000 | Loss: 0.00000936
Iteration 62/1000 | Loss: 0.00000936
Iteration 63/1000 | Loss: 0.00000936
Iteration 64/1000 | Loss: 0.00000936
Iteration 65/1000 | Loss: 0.00000935
Iteration 66/1000 | Loss: 0.00000935
Iteration 67/1000 | Loss: 0.00000935
Iteration 68/1000 | Loss: 0.00000934
Iteration 69/1000 | Loss: 0.00000934
Iteration 70/1000 | Loss: 0.00000934
Iteration 71/1000 | Loss: 0.00000934
Iteration 72/1000 | Loss: 0.00000934
Iteration 73/1000 | Loss: 0.00000933
Iteration 74/1000 | Loss: 0.00000933
Iteration 75/1000 | Loss: 0.00000932
Iteration 76/1000 | Loss: 0.00000932
Iteration 77/1000 | Loss: 0.00000932
Iteration 78/1000 | Loss: 0.00000932
Iteration 79/1000 | Loss: 0.00000932
Iteration 80/1000 | Loss: 0.00000932
Iteration 81/1000 | Loss: 0.00000932
Iteration 82/1000 | Loss: 0.00000932
Iteration 83/1000 | Loss: 0.00000932
Iteration 84/1000 | Loss: 0.00000932
Iteration 85/1000 | Loss: 0.00000932
Iteration 86/1000 | Loss: 0.00000932
Iteration 87/1000 | Loss: 0.00000932
Iteration 88/1000 | Loss: 0.00000932
Iteration 89/1000 | Loss: 0.00000931
Iteration 90/1000 | Loss: 0.00000931
Iteration 91/1000 | Loss: 0.00000931
Iteration 92/1000 | Loss: 0.00000931
Iteration 93/1000 | Loss: 0.00000930
Iteration 94/1000 | Loss: 0.00000930
Iteration 95/1000 | Loss: 0.00000929
Iteration 96/1000 | Loss: 0.00000929
Iteration 97/1000 | Loss: 0.00000929
Iteration 98/1000 | Loss: 0.00000929
Iteration 99/1000 | Loss: 0.00000929
Iteration 100/1000 | Loss: 0.00000929
Iteration 101/1000 | Loss: 0.00000929
Iteration 102/1000 | Loss: 0.00000929
Iteration 103/1000 | Loss: 0.00000929
Iteration 104/1000 | Loss: 0.00000928
Iteration 105/1000 | Loss: 0.00000928
Iteration 106/1000 | Loss: 0.00000928
Iteration 107/1000 | Loss: 0.00000928
Iteration 108/1000 | Loss: 0.00000928
Iteration 109/1000 | Loss: 0.00000928
Iteration 110/1000 | Loss: 0.00000928
Iteration 111/1000 | Loss: 0.00000928
Iteration 112/1000 | Loss: 0.00000927
Iteration 113/1000 | Loss: 0.00000927
Iteration 114/1000 | Loss: 0.00000927
Iteration 115/1000 | Loss: 0.00000927
Iteration 116/1000 | Loss: 0.00000927
Iteration 117/1000 | Loss: 0.00000927
Iteration 118/1000 | Loss: 0.00000926
Iteration 119/1000 | Loss: 0.00000926
Iteration 120/1000 | Loss: 0.00000926
Iteration 121/1000 | Loss: 0.00000926
Iteration 122/1000 | Loss: 0.00000926
Iteration 123/1000 | Loss: 0.00000926
Iteration 124/1000 | Loss: 0.00000926
Iteration 125/1000 | Loss: 0.00000926
Iteration 126/1000 | Loss: 0.00000926
Iteration 127/1000 | Loss: 0.00000925
Iteration 128/1000 | Loss: 0.00000925
Iteration 129/1000 | Loss: 0.00000925
Iteration 130/1000 | Loss: 0.00000925
Iteration 131/1000 | Loss: 0.00000924
Iteration 132/1000 | Loss: 0.00000924
Iteration 133/1000 | Loss: 0.00000924
Iteration 134/1000 | Loss: 0.00000924
Iteration 135/1000 | Loss: 0.00000924
Iteration 136/1000 | Loss: 0.00000924
Iteration 137/1000 | Loss: 0.00000924
Iteration 138/1000 | Loss: 0.00000924
Iteration 139/1000 | Loss: 0.00000924
Iteration 140/1000 | Loss: 0.00000924
Iteration 141/1000 | Loss: 0.00000924
Iteration 142/1000 | Loss: 0.00000924
Iteration 143/1000 | Loss: 0.00000924
Iteration 144/1000 | Loss: 0.00000924
Iteration 145/1000 | Loss: 0.00000924
Iteration 146/1000 | Loss: 0.00000924
Iteration 147/1000 | Loss: 0.00000924
Iteration 148/1000 | Loss: 0.00000924
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 148. Stopping optimization.
Last 5 losses: [9.239014616468921e-06, 9.239014616468921e-06, 9.239014616468921e-06, 9.239014616468921e-06, 9.239014616468921e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.239014616468921e-06

Optimization complete. Final v2v error: 2.6626501083374023 mm

Highest mean error: 2.953183174133301 mm for frame 108

Lowest mean error: 2.5265021324157715 mm for frame 157

Saving results

Total time: 38.46263766288757
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_025/1046/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_025/1046.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_025/1046
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00899657
Iteration 2/25 | Loss: 0.00168045
Iteration 3/25 | Loss: 0.00144666
Iteration 4/25 | Loss: 0.00142404
Iteration 5/25 | Loss: 0.00142027
Iteration 6/25 | Loss: 0.00141916
Iteration 7/25 | Loss: 0.00141910
Iteration 8/25 | Loss: 0.00141910
Iteration 9/25 | Loss: 0.00141910
Iteration 10/25 | Loss: 0.00141910
Iteration 11/25 | Loss: 0.00141910
Iteration 12/25 | Loss: 0.00141910
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0014190974179655313, 0.0014190974179655313, 0.0014190974179655313, 0.0014190974179655313, 0.0014190974179655313]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014190974179655313

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.52752006
Iteration 2/25 | Loss: 0.00149160
Iteration 3/25 | Loss: 0.00149160
Iteration 4/25 | Loss: 0.00149160
Iteration 5/25 | Loss: 0.00149160
Iteration 6/25 | Loss: 0.00149160
Iteration 7/25 | Loss: 0.00149160
Iteration 8/25 | Loss: 0.00149160
Iteration 9/25 | Loss: 0.00149160
Iteration 10/25 | Loss: 0.00149160
Iteration 11/25 | Loss: 0.00149160
Iteration 12/25 | Loss: 0.00149160
Iteration 13/25 | Loss: 0.00149160
Iteration 14/25 | Loss: 0.00149160
Iteration 15/25 | Loss: 0.00149160
Iteration 16/25 | Loss: 0.00149160
Iteration 17/25 | Loss: 0.00149160
Iteration 18/25 | Loss: 0.00149160
Iteration 19/25 | Loss: 0.00149160
Iteration 20/25 | Loss: 0.00149160
Iteration 21/25 | Loss: 0.00149160
Iteration 22/25 | Loss: 0.00149160
Iteration 23/25 | Loss: 0.00149160
Iteration 24/25 | Loss: 0.00149160
Iteration 25/25 | Loss: 0.00149160

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00149160
Iteration 2/1000 | Loss: 0.00005455
Iteration 3/1000 | Loss: 0.00003475
Iteration 4/1000 | Loss: 0.00002845
Iteration 5/1000 | Loss: 0.00002719
Iteration 6/1000 | Loss: 0.00002604
Iteration 7/1000 | Loss: 0.00002549
Iteration 8/1000 | Loss: 0.00002503
Iteration 9/1000 | Loss: 0.00002468
Iteration 10/1000 | Loss: 0.00002443
Iteration 11/1000 | Loss: 0.00002421
Iteration 12/1000 | Loss: 0.00002406
Iteration 13/1000 | Loss: 0.00002394
Iteration 14/1000 | Loss: 0.00002384
Iteration 15/1000 | Loss: 0.00002383
Iteration 16/1000 | Loss: 0.00002378
Iteration 17/1000 | Loss: 0.00002373
Iteration 18/1000 | Loss: 0.00002360
Iteration 19/1000 | Loss: 0.00002351
Iteration 20/1000 | Loss: 0.00002350
Iteration 21/1000 | Loss: 0.00002350
Iteration 22/1000 | Loss: 0.00002350
Iteration 23/1000 | Loss: 0.00002348
Iteration 24/1000 | Loss: 0.00002348
Iteration 25/1000 | Loss: 0.00002347
Iteration 26/1000 | Loss: 0.00002347
Iteration 27/1000 | Loss: 0.00002347
Iteration 28/1000 | Loss: 0.00002346
Iteration 29/1000 | Loss: 0.00002346
Iteration 30/1000 | Loss: 0.00002345
Iteration 31/1000 | Loss: 0.00002345
Iteration 32/1000 | Loss: 0.00002345
Iteration 33/1000 | Loss: 0.00002344
Iteration 34/1000 | Loss: 0.00002344
Iteration 35/1000 | Loss: 0.00002344
Iteration 36/1000 | Loss: 0.00002343
Iteration 37/1000 | Loss: 0.00002343
Iteration 38/1000 | Loss: 0.00002343
Iteration 39/1000 | Loss: 0.00002343
Iteration 40/1000 | Loss: 0.00002343
Iteration 41/1000 | Loss: 0.00002343
Iteration 42/1000 | Loss: 0.00002342
Iteration 43/1000 | Loss: 0.00002342
Iteration 44/1000 | Loss: 0.00002342
Iteration 45/1000 | Loss: 0.00002342
Iteration 46/1000 | Loss: 0.00002342
Iteration 47/1000 | Loss: 0.00002342
Iteration 48/1000 | Loss: 0.00002342
Iteration 49/1000 | Loss: 0.00002342
Iteration 50/1000 | Loss: 0.00002341
Iteration 51/1000 | Loss: 0.00002341
Iteration 52/1000 | Loss: 0.00002341
Iteration 53/1000 | Loss: 0.00002340
Iteration 54/1000 | Loss: 0.00002340
Iteration 55/1000 | Loss: 0.00002340
Iteration 56/1000 | Loss: 0.00002340
Iteration 57/1000 | Loss: 0.00002340
Iteration 58/1000 | Loss: 0.00002340
Iteration 59/1000 | Loss: 0.00002340
Iteration 60/1000 | Loss: 0.00002340
Iteration 61/1000 | Loss: 0.00002340
Iteration 62/1000 | Loss: 0.00002340
Iteration 63/1000 | Loss: 0.00002340
Iteration 64/1000 | Loss: 0.00002340
Iteration 65/1000 | Loss: 0.00002339
Iteration 66/1000 | Loss: 0.00002339
Iteration 67/1000 | Loss: 0.00002339
Iteration 68/1000 | Loss: 0.00002339
Iteration 69/1000 | Loss: 0.00002338
Iteration 70/1000 | Loss: 0.00002338
Iteration 71/1000 | Loss: 0.00002338
Iteration 72/1000 | Loss: 0.00002338
Iteration 73/1000 | Loss: 0.00002338
Iteration 74/1000 | Loss: 0.00002337
Iteration 75/1000 | Loss: 0.00002337
Iteration 76/1000 | Loss: 0.00002337
Iteration 77/1000 | Loss: 0.00002337
Iteration 78/1000 | Loss: 0.00002336
Iteration 79/1000 | Loss: 0.00002336
Iteration 80/1000 | Loss: 0.00002336
Iteration 81/1000 | Loss: 0.00002336
Iteration 82/1000 | Loss: 0.00002336
Iteration 83/1000 | Loss: 0.00002335
Iteration 84/1000 | Loss: 0.00002335
Iteration 85/1000 | Loss: 0.00002335
Iteration 86/1000 | Loss: 0.00002334
Iteration 87/1000 | Loss: 0.00002334
Iteration 88/1000 | Loss: 0.00002334
Iteration 89/1000 | Loss: 0.00002334
Iteration 90/1000 | Loss: 0.00002334
Iteration 91/1000 | Loss: 0.00002334
Iteration 92/1000 | Loss: 0.00002334
Iteration 93/1000 | Loss: 0.00002333
Iteration 94/1000 | Loss: 0.00002333
Iteration 95/1000 | Loss: 0.00002333
Iteration 96/1000 | Loss: 0.00002333
Iteration 97/1000 | Loss: 0.00002333
Iteration 98/1000 | Loss: 0.00002332
Iteration 99/1000 | Loss: 0.00002332
Iteration 100/1000 | Loss: 0.00002332
Iteration 101/1000 | Loss: 0.00002332
Iteration 102/1000 | Loss: 0.00002332
Iteration 103/1000 | Loss: 0.00002332
Iteration 104/1000 | Loss: 0.00002331
Iteration 105/1000 | Loss: 0.00002331
Iteration 106/1000 | Loss: 0.00002331
Iteration 107/1000 | Loss: 0.00002331
Iteration 108/1000 | Loss: 0.00002331
Iteration 109/1000 | Loss: 0.00002331
Iteration 110/1000 | Loss: 0.00002331
Iteration 111/1000 | Loss: 0.00002330
Iteration 112/1000 | Loss: 0.00002330
Iteration 113/1000 | Loss: 0.00002330
Iteration 114/1000 | Loss: 0.00002330
Iteration 115/1000 | Loss: 0.00002330
Iteration 116/1000 | Loss: 0.00002330
Iteration 117/1000 | Loss: 0.00002330
Iteration 118/1000 | Loss: 0.00002330
Iteration 119/1000 | Loss: 0.00002330
Iteration 120/1000 | Loss: 0.00002330
Iteration 121/1000 | Loss: 0.00002330
Iteration 122/1000 | Loss: 0.00002330
Iteration 123/1000 | Loss: 0.00002329
Iteration 124/1000 | Loss: 0.00002329
Iteration 125/1000 | Loss: 0.00002329
Iteration 126/1000 | Loss: 0.00002329
Iteration 127/1000 | Loss: 0.00002329
Iteration 128/1000 | Loss: 0.00002329
Iteration 129/1000 | Loss: 0.00002329
Iteration 130/1000 | Loss: 0.00002328
Iteration 131/1000 | Loss: 0.00002328
Iteration 132/1000 | Loss: 0.00002328
Iteration 133/1000 | Loss: 0.00002328
Iteration 134/1000 | Loss: 0.00002328
Iteration 135/1000 | Loss: 0.00002328
Iteration 136/1000 | Loss: 0.00002328
Iteration 137/1000 | Loss: 0.00002328
Iteration 138/1000 | Loss: 0.00002328
Iteration 139/1000 | Loss: 0.00002328
Iteration 140/1000 | Loss: 0.00002328
Iteration 141/1000 | Loss: 0.00002328
Iteration 142/1000 | Loss: 0.00002328
Iteration 143/1000 | Loss: 0.00002328
Iteration 144/1000 | Loss: 0.00002328
Iteration 145/1000 | Loss: 0.00002328
Iteration 146/1000 | Loss: 0.00002328
Iteration 147/1000 | Loss: 0.00002327
Iteration 148/1000 | Loss: 0.00002327
Iteration 149/1000 | Loss: 0.00002327
Iteration 150/1000 | Loss: 0.00002327
Iteration 151/1000 | Loss: 0.00002327
Iteration 152/1000 | Loss: 0.00002327
Iteration 153/1000 | Loss: 0.00002327
Iteration 154/1000 | Loss: 0.00002327
Iteration 155/1000 | Loss: 0.00002327
Iteration 156/1000 | Loss: 0.00002327
Iteration 157/1000 | Loss: 0.00002327
Iteration 158/1000 | Loss: 0.00002327
Iteration 159/1000 | Loss: 0.00002327
Iteration 160/1000 | Loss: 0.00002327
Iteration 161/1000 | Loss: 0.00002327
Iteration 162/1000 | Loss: 0.00002327
Iteration 163/1000 | Loss: 0.00002327
Iteration 164/1000 | Loss: 0.00002327
Iteration 165/1000 | Loss: 0.00002327
Iteration 166/1000 | Loss: 0.00002327
Iteration 167/1000 | Loss: 0.00002327
Iteration 168/1000 | Loss: 0.00002327
Iteration 169/1000 | Loss: 0.00002327
Iteration 170/1000 | Loss: 0.00002327
Iteration 171/1000 | Loss: 0.00002327
Iteration 172/1000 | Loss: 0.00002327
Iteration 173/1000 | Loss: 0.00002327
Iteration 174/1000 | Loss: 0.00002327
Iteration 175/1000 | Loss: 0.00002327
Iteration 176/1000 | Loss: 0.00002327
Iteration 177/1000 | Loss: 0.00002327
Iteration 178/1000 | Loss: 0.00002327
Iteration 179/1000 | Loss: 0.00002327
Iteration 180/1000 | Loss: 0.00002327
Iteration 181/1000 | Loss: 0.00002327
Iteration 182/1000 | Loss: 0.00002327
Iteration 183/1000 | Loss: 0.00002327
Iteration 184/1000 | Loss: 0.00002327
Iteration 185/1000 | Loss: 0.00002327
Iteration 186/1000 | Loss: 0.00002327
Iteration 187/1000 | Loss: 0.00002327
Iteration 188/1000 | Loss: 0.00002327
Iteration 189/1000 | Loss: 0.00002327
Iteration 190/1000 | Loss: 0.00002327
Iteration 191/1000 | Loss: 0.00002327
Iteration 192/1000 | Loss: 0.00002327
Iteration 193/1000 | Loss: 0.00002327
Iteration 194/1000 | Loss: 0.00002327
Iteration 195/1000 | Loss: 0.00002327
Iteration 196/1000 | Loss: 0.00002327
Iteration 197/1000 | Loss: 0.00002327
Iteration 198/1000 | Loss: 0.00002327
Iteration 199/1000 | Loss: 0.00002327
Iteration 200/1000 | Loss: 0.00002327
Iteration 201/1000 | Loss: 0.00002327
Iteration 202/1000 | Loss: 0.00002327
Iteration 203/1000 | Loss: 0.00002327
Iteration 204/1000 | Loss: 0.00002327
Iteration 205/1000 | Loss: 0.00002327
Iteration 206/1000 | Loss: 0.00002327
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 206. Stopping optimization.
Last 5 losses: [2.3274264094652608e-05, 2.3274264094652608e-05, 2.3274264094652608e-05, 2.3274264094652608e-05, 2.3274264094652608e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3274264094652608e-05

Optimization complete. Final v2v error: 4.085877895355225 mm

Highest mean error: 4.5658159255981445 mm for frame 19

Lowest mean error: 3.678635358810425 mm for frame 42

Saving results

Total time: 41.561317920684814
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_025/1087/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_025/1087.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_025/1087
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00394694
Iteration 2/25 | Loss: 0.00132093
Iteration 3/25 | Loss: 0.00122836
Iteration 4/25 | Loss: 0.00121806
Iteration 5/25 | Loss: 0.00121634
Iteration 6/25 | Loss: 0.00121634
Iteration 7/25 | Loss: 0.00121634
Iteration 8/25 | Loss: 0.00121634
Iteration 9/25 | Loss: 0.00121634
Iteration 10/25 | Loss: 0.00121634
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0012163417413830757, 0.0012163417413830757, 0.0012163417413830757, 0.0012163417413830757, 0.0012163417413830757]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012163417413830757

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.28213799
Iteration 2/25 | Loss: 0.00130894
Iteration 3/25 | Loss: 0.00130894
Iteration 4/25 | Loss: 0.00130894
Iteration 5/25 | Loss: 0.00130894
Iteration 6/25 | Loss: 0.00130893
Iteration 7/25 | Loss: 0.00130893
Iteration 8/25 | Loss: 0.00130893
Iteration 9/25 | Loss: 0.00130893
Iteration 10/25 | Loss: 0.00130893
Iteration 11/25 | Loss: 0.00130893
Iteration 12/25 | Loss: 0.00130893
Iteration 13/25 | Loss: 0.00130893
Iteration 14/25 | Loss: 0.00130893
Iteration 15/25 | Loss: 0.00130893
Iteration 16/25 | Loss: 0.00130893
Iteration 17/25 | Loss: 0.00130893
Iteration 18/25 | Loss: 0.00130893
Iteration 19/25 | Loss: 0.00130893
Iteration 20/25 | Loss: 0.00130893
Iteration 21/25 | Loss: 0.00130893
Iteration 22/25 | Loss: 0.00130893
Iteration 23/25 | Loss: 0.00130893
Iteration 24/25 | Loss: 0.00130893
Iteration 25/25 | Loss: 0.00130893

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00130893
Iteration 2/1000 | Loss: 0.00002533
Iteration 3/1000 | Loss: 0.00001918
Iteration 4/1000 | Loss: 0.00001680
Iteration 5/1000 | Loss: 0.00001546
Iteration 6/1000 | Loss: 0.00001469
Iteration 7/1000 | Loss: 0.00001400
Iteration 8/1000 | Loss: 0.00001365
Iteration 9/1000 | Loss: 0.00001331
Iteration 10/1000 | Loss: 0.00001288
Iteration 11/1000 | Loss: 0.00001265
Iteration 12/1000 | Loss: 0.00001263
Iteration 13/1000 | Loss: 0.00001254
Iteration 14/1000 | Loss: 0.00001251
Iteration 15/1000 | Loss: 0.00001250
Iteration 16/1000 | Loss: 0.00001246
Iteration 17/1000 | Loss: 0.00001244
Iteration 18/1000 | Loss: 0.00001231
Iteration 19/1000 | Loss: 0.00001223
Iteration 20/1000 | Loss: 0.00001220
Iteration 21/1000 | Loss: 0.00001219
Iteration 22/1000 | Loss: 0.00001218
Iteration 23/1000 | Loss: 0.00001218
Iteration 24/1000 | Loss: 0.00001214
Iteration 25/1000 | Loss: 0.00001214
Iteration 26/1000 | Loss: 0.00001211
Iteration 27/1000 | Loss: 0.00001206
Iteration 28/1000 | Loss: 0.00001201
Iteration 29/1000 | Loss: 0.00001201
Iteration 30/1000 | Loss: 0.00001200
Iteration 31/1000 | Loss: 0.00001199
Iteration 32/1000 | Loss: 0.00001185
Iteration 33/1000 | Loss: 0.00001180
Iteration 34/1000 | Loss: 0.00001179
Iteration 35/1000 | Loss: 0.00001175
Iteration 36/1000 | Loss: 0.00001174
Iteration 37/1000 | Loss: 0.00001174
Iteration 38/1000 | Loss: 0.00001174
Iteration 39/1000 | Loss: 0.00001173
Iteration 40/1000 | Loss: 0.00001173
Iteration 41/1000 | Loss: 0.00001173
Iteration 42/1000 | Loss: 0.00001173
Iteration 43/1000 | Loss: 0.00001173
Iteration 44/1000 | Loss: 0.00001171
Iteration 45/1000 | Loss: 0.00001169
Iteration 46/1000 | Loss: 0.00001169
Iteration 47/1000 | Loss: 0.00001169
Iteration 48/1000 | Loss: 0.00001166
Iteration 49/1000 | Loss: 0.00001165
Iteration 50/1000 | Loss: 0.00001165
Iteration 51/1000 | Loss: 0.00001164
Iteration 52/1000 | Loss: 0.00001164
Iteration 53/1000 | Loss: 0.00001164
Iteration 54/1000 | Loss: 0.00001163
Iteration 55/1000 | Loss: 0.00001163
Iteration 56/1000 | Loss: 0.00001163
Iteration 57/1000 | Loss: 0.00001162
Iteration 58/1000 | Loss: 0.00001160
Iteration 59/1000 | Loss: 0.00001160
Iteration 60/1000 | Loss: 0.00001160
Iteration 61/1000 | Loss: 0.00001160
Iteration 62/1000 | Loss: 0.00001160
Iteration 63/1000 | Loss: 0.00001160
Iteration 64/1000 | Loss: 0.00001160
Iteration 65/1000 | Loss: 0.00001160
Iteration 66/1000 | Loss: 0.00001160
Iteration 67/1000 | Loss: 0.00001160
Iteration 68/1000 | Loss: 0.00001159
Iteration 69/1000 | Loss: 0.00001159
Iteration 70/1000 | Loss: 0.00001159
Iteration 71/1000 | Loss: 0.00001159
Iteration 72/1000 | Loss: 0.00001159
Iteration 73/1000 | Loss: 0.00001159
Iteration 74/1000 | Loss: 0.00001159
Iteration 75/1000 | Loss: 0.00001158
Iteration 76/1000 | Loss: 0.00001158
Iteration 77/1000 | Loss: 0.00001158
Iteration 78/1000 | Loss: 0.00001158
Iteration 79/1000 | Loss: 0.00001158
Iteration 80/1000 | Loss: 0.00001157
Iteration 81/1000 | Loss: 0.00001156
Iteration 82/1000 | Loss: 0.00001156
Iteration 83/1000 | Loss: 0.00001155
Iteration 84/1000 | Loss: 0.00001155
Iteration 85/1000 | Loss: 0.00001154
Iteration 86/1000 | Loss: 0.00001154
Iteration 87/1000 | Loss: 0.00001152
Iteration 88/1000 | Loss: 0.00001152
Iteration 89/1000 | Loss: 0.00001152
Iteration 90/1000 | Loss: 0.00001152
Iteration 91/1000 | Loss: 0.00001152
Iteration 92/1000 | Loss: 0.00001152
Iteration 93/1000 | Loss: 0.00001151
Iteration 94/1000 | Loss: 0.00001151
Iteration 95/1000 | Loss: 0.00001149
Iteration 96/1000 | Loss: 0.00001149
Iteration 97/1000 | Loss: 0.00001149
Iteration 98/1000 | Loss: 0.00001149
Iteration 99/1000 | Loss: 0.00001149
Iteration 100/1000 | Loss: 0.00001149
Iteration 101/1000 | Loss: 0.00001149
Iteration 102/1000 | Loss: 0.00001148
Iteration 103/1000 | Loss: 0.00001148
Iteration 104/1000 | Loss: 0.00001147
Iteration 105/1000 | Loss: 0.00001146
Iteration 106/1000 | Loss: 0.00001146
Iteration 107/1000 | Loss: 0.00001146
Iteration 108/1000 | Loss: 0.00001145
Iteration 109/1000 | Loss: 0.00001145
Iteration 110/1000 | Loss: 0.00001145
Iteration 111/1000 | Loss: 0.00001145
Iteration 112/1000 | Loss: 0.00001145
Iteration 113/1000 | Loss: 0.00001145
Iteration 114/1000 | Loss: 0.00001145
Iteration 115/1000 | Loss: 0.00001145
Iteration 116/1000 | Loss: 0.00001145
Iteration 117/1000 | Loss: 0.00001145
Iteration 118/1000 | Loss: 0.00001144
Iteration 119/1000 | Loss: 0.00001144
Iteration 120/1000 | Loss: 0.00001144
Iteration 121/1000 | Loss: 0.00001144
Iteration 122/1000 | Loss: 0.00001144
Iteration 123/1000 | Loss: 0.00001144
Iteration 124/1000 | Loss: 0.00001143
Iteration 125/1000 | Loss: 0.00001143
Iteration 126/1000 | Loss: 0.00001143
Iteration 127/1000 | Loss: 0.00001143
Iteration 128/1000 | Loss: 0.00001143
Iteration 129/1000 | Loss: 0.00001143
Iteration 130/1000 | Loss: 0.00001143
Iteration 131/1000 | Loss: 0.00001143
Iteration 132/1000 | Loss: 0.00001143
Iteration 133/1000 | Loss: 0.00001143
Iteration 134/1000 | Loss: 0.00001143
Iteration 135/1000 | Loss: 0.00001142
Iteration 136/1000 | Loss: 0.00001142
Iteration 137/1000 | Loss: 0.00001142
Iteration 138/1000 | Loss: 0.00001142
Iteration 139/1000 | Loss: 0.00001142
Iteration 140/1000 | Loss: 0.00001142
Iteration 141/1000 | Loss: 0.00001142
Iteration 142/1000 | Loss: 0.00001142
Iteration 143/1000 | Loss: 0.00001142
Iteration 144/1000 | Loss: 0.00001141
Iteration 145/1000 | Loss: 0.00001141
Iteration 146/1000 | Loss: 0.00001141
Iteration 147/1000 | Loss: 0.00001141
Iteration 148/1000 | Loss: 0.00001141
Iteration 149/1000 | Loss: 0.00001141
Iteration 150/1000 | Loss: 0.00001141
Iteration 151/1000 | Loss: 0.00001141
Iteration 152/1000 | Loss: 0.00001141
Iteration 153/1000 | Loss: 0.00001141
Iteration 154/1000 | Loss: 0.00001141
Iteration 155/1000 | Loss: 0.00001141
Iteration 156/1000 | Loss: 0.00001141
Iteration 157/1000 | Loss: 0.00001140
Iteration 158/1000 | Loss: 0.00001140
Iteration 159/1000 | Loss: 0.00001140
Iteration 160/1000 | Loss: 0.00001140
Iteration 161/1000 | Loss: 0.00001140
Iteration 162/1000 | Loss: 0.00001140
Iteration 163/1000 | Loss: 0.00001140
Iteration 164/1000 | Loss: 0.00001140
Iteration 165/1000 | Loss: 0.00001140
Iteration 166/1000 | Loss: 0.00001140
Iteration 167/1000 | Loss: 0.00001140
Iteration 168/1000 | Loss: 0.00001140
Iteration 169/1000 | Loss: 0.00001140
Iteration 170/1000 | Loss: 0.00001140
Iteration 171/1000 | Loss: 0.00001140
Iteration 172/1000 | Loss: 0.00001140
Iteration 173/1000 | Loss: 0.00001140
Iteration 174/1000 | Loss: 0.00001140
Iteration 175/1000 | Loss: 0.00001140
Iteration 176/1000 | Loss: 0.00001140
Iteration 177/1000 | Loss: 0.00001139
Iteration 178/1000 | Loss: 0.00001139
Iteration 179/1000 | Loss: 0.00001139
Iteration 180/1000 | Loss: 0.00001139
Iteration 181/1000 | Loss: 0.00001139
Iteration 182/1000 | Loss: 0.00001139
Iteration 183/1000 | Loss: 0.00001139
Iteration 184/1000 | Loss: 0.00001139
Iteration 185/1000 | Loss: 0.00001139
Iteration 186/1000 | Loss: 0.00001139
Iteration 187/1000 | Loss: 0.00001139
Iteration 188/1000 | Loss: 0.00001139
Iteration 189/1000 | Loss: 0.00001139
Iteration 190/1000 | Loss: 0.00001139
Iteration 191/1000 | Loss: 0.00001139
Iteration 192/1000 | Loss: 0.00001139
Iteration 193/1000 | Loss: 0.00001139
Iteration 194/1000 | Loss: 0.00001139
Iteration 195/1000 | Loss: 0.00001139
Iteration 196/1000 | Loss: 0.00001139
Iteration 197/1000 | Loss: 0.00001139
Iteration 198/1000 | Loss: 0.00001139
Iteration 199/1000 | Loss: 0.00001139
Iteration 200/1000 | Loss: 0.00001139
Iteration 201/1000 | Loss: 0.00001139
Iteration 202/1000 | Loss: 0.00001139
Iteration 203/1000 | Loss: 0.00001139
Iteration 204/1000 | Loss: 0.00001139
Iteration 205/1000 | Loss: 0.00001139
Iteration 206/1000 | Loss: 0.00001139
Iteration 207/1000 | Loss: 0.00001139
Iteration 208/1000 | Loss: 0.00001139
Iteration 209/1000 | Loss: 0.00001139
Iteration 210/1000 | Loss: 0.00001139
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 210. Stopping optimization.
Last 5 losses: [1.1386889127606992e-05, 1.1386889127606992e-05, 1.1386889127606992e-05, 1.1386889127606992e-05, 1.1386889127606992e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1386889127606992e-05

Optimization complete. Final v2v error: 2.9020440578460693 mm

Highest mean error: 3.2050187587738037 mm for frame 106

Lowest mean error: 2.6513583660125732 mm for frame 2

Saving results

Total time: 45.726457834243774
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_025/1035/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_025/1035.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_025/1035
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00788891
Iteration 2/25 | Loss: 0.00223443
Iteration 3/25 | Loss: 0.00155610
Iteration 4/25 | Loss: 0.00149568
Iteration 5/25 | Loss: 0.00146915
Iteration 6/25 | Loss: 0.00146023
Iteration 7/25 | Loss: 0.00145835
Iteration 8/25 | Loss: 0.00145792
Iteration 9/25 | Loss: 0.00145780
Iteration 10/25 | Loss: 0.00145780
Iteration 11/25 | Loss: 0.00145780
Iteration 12/25 | Loss: 0.00145780
Iteration 13/25 | Loss: 0.00145780
Iteration 14/25 | Loss: 0.00145780
Iteration 15/25 | Loss: 0.00145780
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0014577952679246664, 0.0014577952679246664, 0.0014577952679246664, 0.0014577952679246664, 0.0014577952679246664]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014577952679246664

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.25687039
Iteration 2/25 | Loss: 0.00136808
Iteration 3/25 | Loss: 0.00136808
Iteration 4/25 | Loss: 0.00136808
Iteration 5/25 | Loss: 0.00136808
Iteration 6/25 | Loss: 0.00136808
Iteration 7/25 | Loss: 0.00136808
Iteration 8/25 | Loss: 0.00136808
Iteration 9/25 | Loss: 0.00136807
Iteration 10/25 | Loss: 0.00136807
Iteration 11/25 | Loss: 0.00136807
Iteration 12/25 | Loss: 0.00136807
Iteration 13/25 | Loss: 0.00136807
Iteration 14/25 | Loss: 0.00136807
Iteration 15/25 | Loss: 0.00136807
Iteration 16/25 | Loss: 0.00136807
Iteration 17/25 | Loss: 0.00136807
Iteration 18/25 | Loss: 0.00136807
Iteration 19/25 | Loss: 0.00136807
Iteration 20/25 | Loss: 0.00136807
Iteration 21/25 | Loss: 0.00136807
Iteration 22/25 | Loss: 0.00136807
Iteration 23/25 | Loss: 0.00136807
Iteration 24/25 | Loss: 0.00136807
Iteration 25/25 | Loss: 0.00136807

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00136807
Iteration 2/1000 | Loss: 0.00006562
Iteration 3/1000 | Loss: 0.00003868
Iteration 4/1000 | Loss: 0.00003362
Iteration 5/1000 | Loss: 0.00003173
Iteration 6/1000 | Loss: 0.00003028
Iteration 7/1000 | Loss: 0.00002959
Iteration 8/1000 | Loss: 0.00002900
Iteration 9/1000 | Loss: 0.00002871
Iteration 10/1000 | Loss: 0.00002845
Iteration 11/1000 | Loss: 0.00002824
Iteration 12/1000 | Loss: 0.00002823
Iteration 13/1000 | Loss: 0.00002812
Iteration 14/1000 | Loss: 0.00002812
Iteration 15/1000 | Loss: 0.00002811
Iteration 16/1000 | Loss: 0.00002809
Iteration 17/1000 | Loss: 0.00002809
Iteration 18/1000 | Loss: 0.00002809
Iteration 19/1000 | Loss: 0.00002808
Iteration 20/1000 | Loss: 0.00002808
Iteration 21/1000 | Loss: 0.00002807
Iteration 22/1000 | Loss: 0.00002801
Iteration 23/1000 | Loss: 0.00002796
Iteration 24/1000 | Loss: 0.00002796
Iteration 25/1000 | Loss: 0.00002793
Iteration 26/1000 | Loss: 0.00002793
Iteration 27/1000 | Loss: 0.00002792
Iteration 28/1000 | Loss: 0.00002792
Iteration 29/1000 | Loss: 0.00002791
Iteration 30/1000 | Loss: 0.00002790
Iteration 31/1000 | Loss: 0.00002790
Iteration 32/1000 | Loss: 0.00002789
Iteration 33/1000 | Loss: 0.00002789
Iteration 34/1000 | Loss: 0.00002787
Iteration 35/1000 | Loss: 0.00002787
Iteration 36/1000 | Loss: 0.00002787
Iteration 37/1000 | Loss: 0.00002785
Iteration 38/1000 | Loss: 0.00002781
Iteration 39/1000 | Loss: 0.00002779
Iteration 40/1000 | Loss: 0.00002778
Iteration 41/1000 | Loss: 0.00002777
Iteration 42/1000 | Loss: 0.00002777
Iteration 43/1000 | Loss: 0.00002776
Iteration 44/1000 | Loss: 0.00002776
Iteration 45/1000 | Loss: 0.00002776
Iteration 46/1000 | Loss: 0.00002775
Iteration 47/1000 | Loss: 0.00002775
Iteration 48/1000 | Loss: 0.00002775
Iteration 49/1000 | Loss: 0.00002775
Iteration 50/1000 | Loss: 0.00002775
Iteration 51/1000 | Loss: 0.00002775
Iteration 52/1000 | Loss: 0.00002775
Iteration 53/1000 | Loss: 0.00002774
Iteration 54/1000 | Loss: 0.00002774
Iteration 55/1000 | Loss: 0.00002774
Iteration 56/1000 | Loss: 0.00002774
Iteration 57/1000 | Loss: 0.00002774
Iteration 58/1000 | Loss: 0.00002774
Iteration 59/1000 | Loss: 0.00002773
Iteration 60/1000 | Loss: 0.00002772
Iteration 61/1000 | Loss: 0.00002772
Iteration 62/1000 | Loss: 0.00002772
Iteration 63/1000 | Loss: 0.00002772
Iteration 64/1000 | Loss: 0.00002772
Iteration 65/1000 | Loss: 0.00002772
Iteration 66/1000 | Loss: 0.00002771
Iteration 67/1000 | Loss: 0.00002771
Iteration 68/1000 | Loss: 0.00002771
Iteration 69/1000 | Loss: 0.00002771
Iteration 70/1000 | Loss: 0.00002770
Iteration 71/1000 | Loss: 0.00002770
Iteration 72/1000 | Loss: 0.00002770
Iteration 73/1000 | Loss: 0.00002770
Iteration 74/1000 | Loss: 0.00002770
Iteration 75/1000 | Loss: 0.00002770
Iteration 76/1000 | Loss: 0.00002770
Iteration 77/1000 | Loss: 0.00002769
Iteration 78/1000 | Loss: 0.00002769
Iteration 79/1000 | Loss: 0.00002769
Iteration 80/1000 | Loss: 0.00002769
Iteration 81/1000 | Loss: 0.00002769
Iteration 82/1000 | Loss: 0.00002769
Iteration 83/1000 | Loss: 0.00002769
Iteration 84/1000 | Loss: 0.00002769
Iteration 85/1000 | Loss: 0.00002769
Iteration 86/1000 | Loss: 0.00002769
Iteration 87/1000 | Loss: 0.00002769
Iteration 88/1000 | Loss: 0.00002769
Iteration 89/1000 | Loss: 0.00002769
Iteration 90/1000 | Loss: 0.00002769
Iteration 91/1000 | Loss: 0.00002769
Iteration 92/1000 | Loss: 0.00002768
Iteration 93/1000 | Loss: 0.00002768
Iteration 94/1000 | Loss: 0.00002768
Iteration 95/1000 | Loss: 0.00002768
Iteration 96/1000 | Loss: 0.00002768
Iteration 97/1000 | Loss: 0.00002768
Iteration 98/1000 | Loss: 0.00002768
Iteration 99/1000 | Loss: 0.00002768
Iteration 100/1000 | Loss: 0.00002768
Iteration 101/1000 | Loss: 0.00002768
Iteration 102/1000 | Loss: 0.00002768
Iteration 103/1000 | Loss: 0.00002768
Iteration 104/1000 | Loss: 0.00002768
Iteration 105/1000 | Loss: 0.00002768
Iteration 106/1000 | Loss: 0.00002768
Iteration 107/1000 | Loss: 0.00002768
Iteration 108/1000 | Loss: 0.00002768
Iteration 109/1000 | Loss: 0.00002768
Iteration 110/1000 | Loss: 0.00002768
Iteration 111/1000 | Loss: 0.00002768
Iteration 112/1000 | Loss: 0.00002768
Iteration 113/1000 | Loss: 0.00002768
Iteration 114/1000 | Loss: 0.00002768
Iteration 115/1000 | Loss: 0.00002768
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 115. Stopping optimization.
Last 5 losses: [2.7676855097524822e-05, 2.7676855097524822e-05, 2.7676855097524822e-05, 2.7676855097524822e-05, 2.7676855097524822e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.7676855097524822e-05

Optimization complete. Final v2v error: 4.562154769897461 mm

Highest mean error: 4.931215286254883 mm for frame 223

Lowest mean error: 4.005580902099609 mm for frame 43

Saving results

Total time: 47.24586033821106
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_025/1093/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_025/1093.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_025/1093
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00535041
Iteration 2/25 | Loss: 0.00139383
Iteration 3/25 | Loss: 0.00126965
Iteration 4/25 | Loss: 0.00124632
Iteration 5/25 | Loss: 0.00124086
Iteration 6/25 | Loss: 0.00123998
Iteration 7/25 | Loss: 0.00123998
Iteration 8/25 | Loss: 0.00123998
Iteration 9/25 | Loss: 0.00123998
Iteration 10/25 | Loss: 0.00123998
Iteration 11/25 | Loss: 0.00123998
Iteration 12/25 | Loss: 0.00123998
Iteration 13/25 | Loss: 0.00123998
Iteration 14/25 | Loss: 0.00123998
Iteration 15/25 | Loss: 0.00123998
Iteration 16/25 | Loss: 0.00123998
Iteration 17/25 | Loss: 0.00123998
Iteration 18/25 | Loss: 0.00123998
Iteration 19/25 | Loss: 0.00123998
Iteration 20/25 | Loss: 0.00123998
Iteration 21/25 | Loss: 0.00123998
Iteration 22/25 | Loss: 0.00123998
Iteration 23/25 | Loss: 0.00123998
Iteration 24/25 | Loss: 0.00123998
Iteration 25/25 | Loss: 0.00123998

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.88651681
Iteration 2/25 | Loss: 0.00138251
Iteration 3/25 | Loss: 0.00138250
Iteration 4/25 | Loss: 0.00138250
Iteration 5/25 | Loss: 0.00138250
Iteration 6/25 | Loss: 0.00138250
Iteration 7/25 | Loss: 0.00138250
Iteration 8/25 | Loss: 0.00138250
Iteration 9/25 | Loss: 0.00138250
Iteration 10/25 | Loss: 0.00138250
Iteration 11/25 | Loss: 0.00138250
Iteration 12/25 | Loss: 0.00138250
Iteration 13/25 | Loss: 0.00138250
Iteration 14/25 | Loss: 0.00138250
Iteration 15/25 | Loss: 0.00138250
Iteration 16/25 | Loss: 0.00138250
Iteration 17/25 | Loss: 0.00138250
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0013825020287185907, 0.0013825020287185907, 0.0013825020287185907, 0.0013825020287185907, 0.0013825020287185907]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013825020287185907

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00138250
Iteration 2/1000 | Loss: 0.00002599
Iteration 3/1000 | Loss: 0.00002079
Iteration 4/1000 | Loss: 0.00001936
Iteration 5/1000 | Loss: 0.00001843
Iteration 6/1000 | Loss: 0.00001761
Iteration 7/1000 | Loss: 0.00001707
Iteration 8/1000 | Loss: 0.00001666
Iteration 9/1000 | Loss: 0.00001624
Iteration 10/1000 | Loss: 0.00001600
Iteration 11/1000 | Loss: 0.00001572
Iteration 12/1000 | Loss: 0.00001546
Iteration 13/1000 | Loss: 0.00001535
Iteration 14/1000 | Loss: 0.00001533
Iteration 15/1000 | Loss: 0.00001529
Iteration 16/1000 | Loss: 0.00001526
Iteration 17/1000 | Loss: 0.00001523
Iteration 18/1000 | Loss: 0.00001522
Iteration 19/1000 | Loss: 0.00001522
Iteration 20/1000 | Loss: 0.00001521
Iteration 21/1000 | Loss: 0.00001513
Iteration 22/1000 | Loss: 0.00001504
Iteration 23/1000 | Loss: 0.00001499
Iteration 24/1000 | Loss: 0.00001498
Iteration 25/1000 | Loss: 0.00001493
Iteration 26/1000 | Loss: 0.00001493
Iteration 27/1000 | Loss: 0.00001491
Iteration 28/1000 | Loss: 0.00001490
Iteration 29/1000 | Loss: 0.00001490
Iteration 30/1000 | Loss: 0.00001489
Iteration 31/1000 | Loss: 0.00001488
Iteration 32/1000 | Loss: 0.00001488
Iteration 33/1000 | Loss: 0.00001487
Iteration 34/1000 | Loss: 0.00001486
Iteration 35/1000 | Loss: 0.00001486
Iteration 36/1000 | Loss: 0.00001486
Iteration 37/1000 | Loss: 0.00001485
Iteration 38/1000 | Loss: 0.00001485
Iteration 39/1000 | Loss: 0.00001484
Iteration 40/1000 | Loss: 0.00001484
Iteration 41/1000 | Loss: 0.00001483
Iteration 42/1000 | Loss: 0.00001483
Iteration 43/1000 | Loss: 0.00001482
Iteration 44/1000 | Loss: 0.00001482
Iteration 45/1000 | Loss: 0.00001481
Iteration 46/1000 | Loss: 0.00001481
Iteration 47/1000 | Loss: 0.00001481
Iteration 48/1000 | Loss: 0.00001480
Iteration 49/1000 | Loss: 0.00001480
Iteration 50/1000 | Loss: 0.00001479
Iteration 51/1000 | Loss: 0.00001478
Iteration 52/1000 | Loss: 0.00001478
Iteration 53/1000 | Loss: 0.00001476
Iteration 54/1000 | Loss: 0.00001476
Iteration 55/1000 | Loss: 0.00001476
Iteration 56/1000 | Loss: 0.00001476
Iteration 57/1000 | Loss: 0.00001476
Iteration 58/1000 | Loss: 0.00001476
Iteration 59/1000 | Loss: 0.00001476
Iteration 60/1000 | Loss: 0.00001476
Iteration 61/1000 | Loss: 0.00001476
Iteration 62/1000 | Loss: 0.00001476
Iteration 63/1000 | Loss: 0.00001476
Iteration 64/1000 | Loss: 0.00001476
Iteration 65/1000 | Loss: 0.00001475
Iteration 66/1000 | Loss: 0.00001475
Iteration 67/1000 | Loss: 0.00001475
Iteration 68/1000 | Loss: 0.00001475
Iteration 69/1000 | Loss: 0.00001475
Iteration 70/1000 | Loss: 0.00001475
Iteration 71/1000 | Loss: 0.00001475
Iteration 72/1000 | Loss: 0.00001475
Iteration 73/1000 | Loss: 0.00001475
Iteration 74/1000 | Loss: 0.00001475
Iteration 75/1000 | Loss: 0.00001474
Iteration 76/1000 | Loss: 0.00001474
Iteration 77/1000 | Loss: 0.00001474
Iteration 78/1000 | Loss: 0.00001472
Iteration 79/1000 | Loss: 0.00001472
Iteration 80/1000 | Loss: 0.00001472
Iteration 81/1000 | Loss: 0.00001472
Iteration 82/1000 | Loss: 0.00001471
Iteration 83/1000 | Loss: 0.00001471
Iteration 84/1000 | Loss: 0.00001471
Iteration 85/1000 | Loss: 0.00001470
Iteration 86/1000 | Loss: 0.00001469
Iteration 87/1000 | Loss: 0.00001469
Iteration 88/1000 | Loss: 0.00001469
Iteration 89/1000 | Loss: 0.00001469
Iteration 90/1000 | Loss: 0.00001469
Iteration 91/1000 | Loss: 0.00001469
Iteration 92/1000 | Loss: 0.00001469
Iteration 93/1000 | Loss: 0.00001469
Iteration 94/1000 | Loss: 0.00001469
Iteration 95/1000 | Loss: 0.00001468
Iteration 96/1000 | Loss: 0.00001468
Iteration 97/1000 | Loss: 0.00001468
Iteration 98/1000 | Loss: 0.00001468
Iteration 99/1000 | Loss: 0.00001467
Iteration 100/1000 | Loss: 0.00001467
Iteration 101/1000 | Loss: 0.00001467
Iteration 102/1000 | Loss: 0.00001467
Iteration 103/1000 | Loss: 0.00001466
Iteration 104/1000 | Loss: 0.00001466
Iteration 105/1000 | Loss: 0.00001466
Iteration 106/1000 | Loss: 0.00001466
Iteration 107/1000 | Loss: 0.00001466
Iteration 108/1000 | Loss: 0.00001466
Iteration 109/1000 | Loss: 0.00001466
Iteration 110/1000 | Loss: 0.00001465
Iteration 111/1000 | Loss: 0.00001465
Iteration 112/1000 | Loss: 0.00001465
Iteration 113/1000 | Loss: 0.00001465
Iteration 114/1000 | Loss: 0.00001465
Iteration 115/1000 | Loss: 0.00001465
Iteration 116/1000 | Loss: 0.00001465
Iteration 117/1000 | Loss: 0.00001465
Iteration 118/1000 | Loss: 0.00001465
Iteration 119/1000 | Loss: 0.00001464
Iteration 120/1000 | Loss: 0.00001464
Iteration 121/1000 | Loss: 0.00001464
Iteration 122/1000 | Loss: 0.00001464
Iteration 123/1000 | Loss: 0.00001464
Iteration 124/1000 | Loss: 0.00001464
Iteration 125/1000 | Loss: 0.00001464
Iteration 126/1000 | Loss: 0.00001463
Iteration 127/1000 | Loss: 0.00001463
Iteration 128/1000 | Loss: 0.00001463
Iteration 129/1000 | Loss: 0.00001463
Iteration 130/1000 | Loss: 0.00001463
Iteration 131/1000 | Loss: 0.00001463
Iteration 132/1000 | Loss: 0.00001463
Iteration 133/1000 | Loss: 0.00001463
Iteration 134/1000 | Loss: 0.00001462
Iteration 135/1000 | Loss: 0.00001462
Iteration 136/1000 | Loss: 0.00001462
Iteration 137/1000 | Loss: 0.00001462
Iteration 138/1000 | Loss: 0.00001462
Iteration 139/1000 | Loss: 0.00001462
Iteration 140/1000 | Loss: 0.00001462
Iteration 141/1000 | Loss: 0.00001462
Iteration 142/1000 | Loss: 0.00001462
Iteration 143/1000 | Loss: 0.00001462
Iteration 144/1000 | Loss: 0.00001462
Iteration 145/1000 | Loss: 0.00001462
Iteration 146/1000 | Loss: 0.00001462
Iteration 147/1000 | Loss: 0.00001462
Iteration 148/1000 | Loss: 0.00001462
Iteration 149/1000 | Loss: 0.00001462
Iteration 150/1000 | Loss: 0.00001462
Iteration 151/1000 | Loss: 0.00001462
Iteration 152/1000 | Loss: 0.00001462
Iteration 153/1000 | Loss: 0.00001462
Iteration 154/1000 | Loss: 0.00001462
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 154. Stopping optimization.
Last 5 losses: [1.4620193724113051e-05, 1.4620193724113051e-05, 1.4620193724113051e-05, 1.4620193724113051e-05, 1.4620193724113051e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4620193724113051e-05

Optimization complete. Final v2v error: 3.288515090942383 mm

Highest mean error: 3.7499382495880127 mm for frame 123

Lowest mean error: 2.9546191692352295 mm for frame 83

Saving results

Total time: 41.541223764419556
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_025/1098/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_025/1098.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_025/1098
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00873360
Iteration 2/25 | Loss: 0.00174149
Iteration 3/25 | Loss: 0.00143050
Iteration 4/25 | Loss: 0.00142767
Iteration 5/25 | Loss: 0.00140472
Iteration 6/25 | Loss: 0.00136703
Iteration 7/25 | Loss: 0.00138131
Iteration 8/25 | Loss: 0.00134711
Iteration 9/25 | Loss: 0.00134441
Iteration 10/25 | Loss: 0.00132514
Iteration 11/25 | Loss: 0.00132014
Iteration 12/25 | Loss: 0.00132355
Iteration 13/25 | Loss: 0.00132055
Iteration 14/25 | Loss: 0.00132335
Iteration 15/25 | Loss: 0.00132056
Iteration 16/25 | Loss: 0.00132188
Iteration 17/25 | Loss: 0.00131859
Iteration 18/25 | Loss: 0.00131670
Iteration 19/25 | Loss: 0.00131458
Iteration 20/25 | Loss: 0.00131193
Iteration 21/25 | Loss: 0.00131032
Iteration 22/25 | Loss: 0.00130993
Iteration 23/25 | Loss: 0.00130972
Iteration 24/25 | Loss: 0.00130966
Iteration 25/25 | Loss: 0.00130965

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.59252357
Iteration 2/25 | Loss: 0.00222421
Iteration 3/25 | Loss: 0.00222412
Iteration 4/25 | Loss: 0.00222412
Iteration 5/25 | Loss: 0.00222412
Iteration 6/25 | Loss: 0.00222412
Iteration 7/25 | Loss: 0.00222411
Iteration 8/25 | Loss: 0.00222411
Iteration 9/25 | Loss: 0.00222411
Iteration 10/25 | Loss: 0.00222411
Iteration 11/25 | Loss: 0.00222411
Iteration 12/25 | Loss: 0.00222411
Iteration 13/25 | Loss: 0.00222411
Iteration 14/25 | Loss: 0.00222411
Iteration 15/25 | Loss: 0.00222411
Iteration 16/25 | Loss: 0.00222411
Iteration 17/25 | Loss: 0.00222411
Iteration 18/25 | Loss: 0.00222411
Iteration 19/25 | Loss: 0.00222411
Iteration 20/25 | Loss: 0.00222411
Iteration 21/25 | Loss: 0.00222411
Iteration 22/25 | Loss: 0.00222411
Iteration 23/25 | Loss: 0.00222411
Iteration 24/25 | Loss: 0.00222411
Iteration 25/25 | Loss: 0.00222411

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00222411
Iteration 2/1000 | Loss: 0.00012277
Iteration 3/1000 | Loss: 0.00019167
Iteration 4/1000 | Loss: 0.00007432
Iteration 5/1000 | Loss: 0.00018208
Iteration 6/1000 | Loss: 0.00019878
Iteration 7/1000 | Loss: 0.00021085
Iteration 8/1000 | Loss: 0.00008517
Iteration 9/1000 | Loss: 0.00008301
Iteration 10/1000 | Loss: 0.00004294
Iteration 11/1000 | Loss: 0.00020253
Iteration 12/1000 | Loss: 0.00018970
Iteration 13/1000 | Loss: 0.00023423
Iteration 14/1000 | Loss: 0.00019236
Iteration 15/1000 | Loss: 0.00005700
Iteration 16/1000 | Loss: 0.00007494
Iteration 17/1000 | Loss: 0.00007672
Iteration 18/1000 | Loss: 0.00004736
Iteration 19/1000 | Loss: 0.00004430
Iteration 20/1000 | Loss: 0.00008437
Iteration 21/1000 | Loss: 0.00003578
Iteration 22/1000 | Loss: 0.00006792
Iteration 23/1000 | Loss: 0.00003985
Iteration 24/1000 | Loss: 0.00004619
Iteration 25/1000 | Loss: 0.00004621
Iteration 26/1000 | Loss: 0.00004692
Iteration 27/1000 | Loss: 0.00003658
Iteration 28/1000 | Loss: 0.00003458
Iteration 29/1000 | Loss: 0.00004474
Iteration 30/1000 | Loss: 0.00004591
Iteration 31/1000 | Loss: 0.00004391
Iteration 32/1000 | Loss: 0.00004602
Iteration 33/1000 | Loss: 0.00004685
Iteration 34/1000 | Loss: 0.00004606
Iteration 35/1000 | Loss: 0.00004015
Iteration 36/1000 | Loss: 0.00003927
Iteration 37/1000 | Loss: 0.00004490
Iteration 38/1000 | Loss: 0.00004733
Iteration 39/1000 | Loss: 0.00004148
Iteration 40/1000 | Loss: 0.00004704
Iteration 41/1000 | Loss: 0.00003774
Iteration 42/1000 | Loss: 0.00003378
Iteration 43/1000 | Loss: 0.00004090
Iteration 44/1000 | Loss: 0.00004794
Iteration 45/1000 | Loss: 0.00003761
Iteration 46/1000 | Loss: 0.00004765
Iteration 47/1000 | Loss: 0.00004325
Iteration 48/1000 | Loss: 0.00004538
Iteration 49/1000 | Loss: 0.00004072
Iteration 50/1000 | Loss: 0.00004470
Iteration 51/1000 | Loss: 0.00004025
Iteration 52/1000 | Loss: 0.00004111
Iteration 53/1000 | Loss: 0.00003792
Iteration 54/1000 | Loss: 0.00004212
Iteration 55/1000 | Loss: 0.00003601
Iteration 56/1000 | Loss: 0.00004628
Iteration 57/1000 | Loss: 0.00003977
Iteration 58/1000 | Loss: 0.00003954
Iteration 59/1000 | Loss: 0.00003952
Iteration 60/1000 | Loss: 0.00004944
Iteration 61/1000 | Loss: 0.00004500
Iteration 62/1000 | Loss: 0.00004319
Iteration 63/1000 | Loss: 0.00004220
Iteration 64/1000 | Loss: 0.00003760
Iteration 65/1000 | Loss: 0.00004327
Iteration 66/1000 | Loss: 0.00004095
Iteration 67/1000 | Loss: 0.00004052
Iteration 68/1000 | Loss: 0.00004015
Iteration 69/1000 | Loss: 0.00004006
Iteration 70/1000 | Loss: 0.00004877
Iteration 71/1000 | Loss: 0.00004026
Iteration 72/1000 | Loss: 0.00003398
Iteration 73/1000 | Loss: 0.00004856
Iteration 74/1000 | Loss: 0.00003158
Iteration 75/1000 | Loss: 0.00003048
Iteration 76/1000 | Loss: 0.00002972
Iteration 77/1000 | Loss: 0.00002923
Iteration 78/1000 | Loss: 0.00002895
Iteration 79/1000 | Loss: 0.00002886
Iteration 80/1000 | Loss: 0.00002885
Iteration 81/1000 | Loss: 0.00002885
Iteration 82/1000 | Loss: 0.00002884
Iteration 83/1000 | Loss: 0.00002883
Iteration 84/1000 | Loss: 0.00002881
Iteration 85/1000 | Loss: 0.00002880
Iteration 86/1000 | Loss: 0.00002874
Iteration 87/1000 | Loss: 0.00002855
Iteration 88/1000 | Loss: 0.00002831
Iteration 89/1000 | Loss: 0.00002801
Iteration 90/1000 | Loss: 0.00002797
Iteration 91/1000 | Loss: 0.00002792
Iteration 92/1000 | Loss: 0.00002777
Iteration 93/1000 | Loss: 0.00002775
Iteration 94/1000 | Loss: 0.00002772
Iteration 95/1000 | Loss: 0.00002771
Iteration 96/1000 | Loss: 0.00002771
Iteration 97/1000 | Loss: 0.00002770
Iteration 98/1000 | Loss: 0.00002769
Iteration 99/1000 | Loss: 0.00002769
Iteration 100/1000 | Loss: 0.00002758
Iteration 101/1000 | Loss: 0.00002743
Iteration 102/1000 | Loss: 0.00002740
Iteration 103/1000 | Loss: 0.00002729
Iteration 104/1000 | Loss: 0.00002729
Iteration 105/1000 | Loss: 0.00002728
Iteration 106/1000 | Loss: 0.00002726
Iteration 107/1000 | Loss: 0.00002726
Iteration 108/1000 | Loss: 0.00002723
Iteration 109/1000 | Loss: 0.00002722
Iteration 110/1000 | Loss: 0.00002721
Iteration 111/1000 | Loss: 0.00002721
Iteration 112/1000 | Loss: 0.00002720
Iteration 113/1000 | Loss: 0.00002720
Iteration 114/1000 | Loss: 0.00002719
Iteration 115/1000 | Loss: 0.00002719
Iteration 116/1000 | Loss: 0.00002719
Iteration 117/1000 | Loss: 0.00002719
Iteration 118/1000 | Loss: 0.00002719
Iteration 119/1000 | Loss: 0.00002718
Iteration 120/1000 | Loss: 0.00002718
Iteration 121/1000 | Loss: 0.00002718
Iteration 122/1000 | Loss: 0.00002717
Iteration 123/1000 | Loss: 0.00002717
Iteration 124/1000 | Loss: 0.00002717
Iteration 125/1000 | Loss: 0.00002717
Iteration 126/1000 | Loss: 0.00002716
Iteration 127/1000 | Loss: 0.00002716
Iteration 128/1000 | Loss: 0.00002715
Iteration 129/1000 | Loss: 0.00002715
Iteration 130/1000 | Loss: 0.00002714
Iteration 131/1000 | Loss: 0.00002714
Iteration 132/1000 | Loss: 0.00002714
Iteration 133/1000 | Loss: 0.00002713
Iteration 134/1000 | Loss: 0.00002713
Iteration 135/1000 | Loss: 0.00002713
Iteration 136/1000 | Loss: 0.00002712
Iteration 137/1000 | Loss: 0.00002712
Iteration 138/1000 | Loss: 0.00002712
Iteration 139/1000 | Loss: 0.00002711
Iteration 140/1000 | Loss: 0.00002711
Iteration 141/1000 | Loss: 0.00002711
Iteration 142/1000 | Loss: 0.00002710
Iteration 143/1000 | Loss: 0.00002710
Iteration 144/1000 | Loss: 0.00002710
Iteration 145/1000 | Loss: 0.00002709
Iteration 146/1000 | Loss: 0.00002709
Iteration 147/1000 | Loss: 0.00002709
Iteration 148/1000 | Loss: 0.00002708
Iteration 149/1000 | Loss: 0.00002708
Iteration 150/1000 | Loss: 0.00002708
Iteration 151/1000 | Loss: 0.00002707
Iteration 152/1000 | Loss: 0.00002707
Iteration 153/1000 | Loss: 0.00002707
Iteration 154/1000 | Loss: 0.00002706
Iteration 155/1000 | Loss: 0.00002706
Iteration 156/1000 | Loss: 0.00002706
Iteration 157/1000 | Loss: 0.00002705
Iteration 158/1000 | Loss: 0.00002705
Iteration 159/1000 | Loss: 0.00002705
Iteration 160/1000 | Loss: 0.00002705
Iteration 161/1000 | Loss: 0.00002704
Iteration 162/1000 | Loss: 0.00002704
Iteration 163/1000 | Loss: 0.00002704
Iteration 164/1000 | Loss: 0.00002704
Iteration 165/1000 | Loss: 0.00002704
Iteration 166/1000 | Loss: 0.00002704
Iteration 167/1000 | Loss: 0.00002704
Iteration 168/1000 | Loss: 0.00002704
Iteration 169/1000 | Loss: 0.00002704
Iteration 170/1000 | Loss: 0.00002703
Iteration 171/1000 | Loss: 0.00002703
Iteration 172/1000 | Loss: 0.00002703
Iteration 173/1000 | Loss: 0.00002703
Iteration 174/1000 | Loss: 0.00002703
Iteration 175/1000 | Loss: 0.00002703
Iteration 176/1000 | Loss: 0.00002703
Iteration 177/1000 | Loss: 0.00002702
Iteration 178/1000 | Loss: 0.00002702
Iteration 179/1000 | Loss: 0.00002702
Iteration 180/1000 | Loss: 0.00002702
Iteration 181/1000 | Loss: 0.00002702
Iteration 182/1000 | Loss: 0.00002702
Iteration 183/1000 | Loss: 0.00002702
Iteration 184/1000 | Loss: 0.00002702
Iteration 185/1000 | Loss: 0.00002702
Iteration 186/1000 | Loss: 0.00002702
Iteration 187/1000 | Loss: 0.00002702
Iteration 188/1000 | Loss: 0.00002702
Iteration 189/1000 | Loss: 0.00002702
Iteration 190/1000 | Loss: 0.00002702
Iteration 191/1000 | Loss: 0.00002702
Iteration 192/1000 | Loss: 0.00002702
Iteration 193/1000 | Loss: 0.00002702
Iteration 194/1000 | Loss: 0.00002702
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 194. Stopping optimization.
Last 5 losses: [2.7017817046726122e-05, 2.7017817046726122e-05, 2.7017817046726122e-05, 2.7017817046726122e-05, 2.7017817046726122e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.7017817046726122e-05

Optimization complete. Final v2v error: 4.273952007293701 mm

Highest mean error: 6.841540813446045 mm for frame 43

Lowest mean error: 3.0028326511383057 mm for frame 81

Saving results

Total time: 195.04773831367493
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_025/1011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_025/1011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_025/1011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00364697
Iteration 2/25 | Loss: 0.00127651
Iteration 3/25 | Loss: 0.00121106
Iteration 4/25 | Loss: 0.00120080
Iteration 5/25 | Loss: 0.00119745
Iteration 6/25 | Loss: 0.00119739
Iteration 7/25 | Loss: 0.00119739
Iteration 8/25 | Loss: 0.00119739
Iteration 9/25 | Loss: 0.00119739
Iteration 10/25 | Loss: 0.00119739
Iteration 11/25 | Loss: 0.00119739
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011973908403888345, 0.0011973908403888345, 0.0011973908403888345, 0.0011973908403888345, 0.0011973908403888345]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011973908403888345

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.55749559
Iteration 2/25 | Loss: 0.00152936
Iteration 3/25 | Loss: 0.00152936
Iteration 4/25 | Loss: 0.00152936
Iteration 5/25 | Loss: 0.00152936
Iteration 6/25 | Loss: 0.00152936
Iteration 7/25 | Loss: 0.00152936
Iteration 8/25 | Loss: 0.00152936
Iteration 9/25 | Loss: 0.00152936
Iteration 10/25 | Loss: 0.00152936
Iteration 11/25 | Loss: 0.00152936
Iteration 12/25 | Loss: 0.00152936
Iteration 13/25 | Loss: 0.00152936
Iteration 14/25 | Loss: 0.00152936
Iteration 15/25 | Loss: 0.00152936
Iteration 16/25 | Loss: 0.00152936
Iteration 17/25 | Loss: 0.00152936
Iteration 18/25 | Loss: 0.00152936
Iteration 19/25 | Loss: 0.00152936
Iteration 20/25 | Loss: 0.00152936
Iteration 21/25 | Loss: 0.00152936
Iteration 22/25 | Loss: 0.00152936
Iteration 23/25 | Loss: 0.00152936
Iteration 24/25 | Loss: 0.00152936
Iteration 25/25 | Loss: 0.00152936

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00152936
Iteration 2/1000 | Loss: 0.00001605
Iteration 3/1000 | Loss: 0.00001278
Iteration 4/1000 | Loss: 0.00001181
Iteration 5/1000 | Loss: 0.00001111
Iteration 6/1000 | Loss: 0.00001061
Iteration 7/1000 | Loss: 0.00001022
Iteration 8/1000 | Loss: 0.00000989
Iteration 9/1000 | Loss: 0.00000971
Iteration 10/1000 | Loss: 0.00000959
Iteration 11/1000 | Loss: 0.00000958
Iteration 12/1000 | Loss: 0.00000955
Iteration 13/1000 | Loss: 0.00000952
Iteration 14/1000 | Loss: 0.00000948
Iteration 15/1000 | Loss: 0.00000947
Iteration 16/1000 | Loss: 0.00000938
Iteration 17/1000 | Loss: 0.00000937
Iteration 18/1000 | Loss: 0.00000937
Iteration 19/1000 | Loss: 0.00000936
Iteration 20/1000 | Loss: 0.00000935
Iteration 21/1000 | Loss: 0.00000920
Iteration 22/1000 | Loss: 0.00000920
Iteration 23/1000 | Loss: 0.00000913
Iteration 24/1000 | Loss: 0.00000912
Iteration 25/1000 | Loss: 0.00000912
Iteration 26/1000 | Loss: 0.00000909
Iteration 27/1000 | Loss: 0.00000903
Iteration 28/1000 | Loss: 0.00000903
Iteration 29/1000 | Loss: 0.00000902
Iteration 30/1000 | Loss: 0.00000897
Iteration 31/1000 | Loss: 0.00000896
Iteration 32/1000 | Loss: 0.00000896
Iteration 33/1000 | Loss: 0.00000896
Iteration 34/1000 | Loss: 0.00000896
Iteration 35/1000 | Loss: 0.00000896
Iteration 36/1000 | Loss: 0.00000896
Iteration 37/1000 | Loss: 0.00000896
Iteration 38/1000 | Loss: 0.00000895
Iteration 39/1000 | Loss: 0.00000895
Iteration 40/1000 | Loss: 0.00000894
Iteration 41/1000 | Loss: 0.00000894
Iteration 42/1000 | Loss: 0.00000893
Iteration 43/1000 | Loss: 0.00000892
Iteration 44/1000 | Loss: 0.00000892
Iteration 45/1000 | Loss: 0.00000892
Iteration 46/1000 | Loss: 0.00000891
Iteration 47/1000 | Loss: 0.00000890
Iteration 48/1000 | Loss: 0.00000890
Iteration 49/1000 | Loss: 0.00000890
Iteration 50/1000 | Loss: 0.00000890
Iteration 51/1000 | Loss: 0.00000890
Iteration 52/1000 | Loss: 0.00000889
Iteration 53/1000 | Loss: 0.00000889
Iteration 54/1000 | Loss: 0.00000888
Iteration 55/1000 | Loss: 0.00000887
Iteration 56/1000 | Loss: 0.00000887
Iteration 57/1000 | Loss: 0.00000886
Iteration 58/1000 | Loss: 0.00000886
Iteration 59/1000 | Loss: 0.00000886
Iteration 60/1000 | Loss: 0.00000886
Iteration 61/1000 | Loss: 0.00000885
Iteration 62/1000 | Loss: 0.00000885
Iteration 63/1000 | Loss: 0.00000885
Iteration 64/1000 | Loss: 0.00000885
Iteration 65/1000 | Loss: 0.00000884
Iteration 66/1000 | Loss: 0.00000884
Iteration 67/1000 | Loss: 0.00000884
Iteration 68/1000 | Loss: 0.00000884
Iteration 69/1000 | Loss: 0.00000884
Iteration 70/1000 | Loss: 0.00000883
Iteration 71/1000 | Loss: 0.00000883
Iteration 72/1000 | Loss: 0.00000883
Iteration 73/1000 | Loss: 0.00000883
Iteration 74/1000 | Loss: 0.00000882
Iteration 75/1000 | Loss: 0.00000882
Iteration 76/1000 | Loss: 0.00000882
Iteration 77/1000 | Loss: 0.00000882
Iteration 78/1000 | Loss: 0.00000882
Iteration 79/1000 | Loss: 0.00000881
Iteration 80/1000 | Loss: 0.00000880
Iteration 81/1000 | Loss: 0.00000880
Iteration 82/1000 | Loss: 0.00000880
Iteration 83/1000 | Loss: 0.00000880
Iteration 84/1000 | Loss: 0.00000879
Iteration 85/1000 | Loss: 0.00000879
Iteration 86/1000 | Loss: 0.00000879
Iteration 87/1000 | Loss: 0.00000879
Iteration 88/1000 | Loss: 0.00000879
Iteration 89/1000 | Loss: 0.00000879
Iteration 90/1000 | Loss: 0.00000878
Iteration 91/1000 | Loss: 0.00000878
Iteration 92/1000 | Loss: 0.00000878
Iteration 93/1000 | Loss: 0.00000878
Iteration 94/1000 | Loss: 0.00000878
Iteration 95/1000 | Loss: 0.00000877
Iteration 96/1000 | Loss: 0.00000877
Iteration 97/1000 | Loss: 0.00000876
Iteration 98/1000 | Loss: 0.00000876
Iteration 99/1000 | Loss: 0.00000876
Iteration 100/1000 | Loss: 0.00000876
Iteration 101/1000 | Loss: 0.00000876
Iteration 102/1000 | Loss: 0.00000876
Iteration 103/1000 | Loss: 0.00000876
Iteration 104/1000 | Loss: 0.00000876
Iteration 105/1000 | Loss: 0.00000875
Iteration 106/1000 | Loss: 0.00000874
Iteration 107/1000 | Loss: 0.00000874
Iteration 108/1000 | Loss: 0.00000874
Iteration 109/1000 | Loss: 0.00000874
Iteration 110/1000 | Loss: 0.00000873
Iteration 111/1000 | Loss: 0.00000873
Iteration 112/1000 | Loss: 0.00000873
Iteration 113/1000 | Loss: 0.00000873
Iteration 114/1000 | Loss: 0.00000872
Iteration 115/1000 | Loss: 0.00000872
Iteration 116/1000 | Loss: 0.00000872
Iteration 117/1000 | Loss: 0.00000872
Iteration 118/1000 | Loss: 0.00000872
Iteration 119/1000 | Loss: 0.00000872
Iteration 120/1000 | Loss: 0.00000872
Iteration 121/1000 | Loss: 0.00000871
Iteration 122/1000 | Loss: 0.00000871
Iteration 123/1000 | Loss: 0.00000871
Iteration 124/1000 | Loss: 0.00000871
Iteration 125/1000 | Loss: 0.00000871
Iteration 126/1000 | Loss: 0.00000871
Iteration 127/1000 | Loss: 0.00000871
Iteration 128/1000 | Loss: 0.00000871
Iteration 129/1000 | Loss: 0.00000871
Iteration 130/1000 | Loss: 0.00000871
Iteration 131/1000 | Loss: 0.00000870
Iteration 132/1000 | Loss: 0.00000870
Iteration 133/1000 | Loss: 0.00000870
Iteration 134/1000 | Loss: 0.00000870
Iteration 135/1000 | Loss: 0.00000870
Iteration 136/1000 | Loss: 0.00000869
Iteration 137/1000 | Loss: 0.00000869
Iteration 138/1000 | Loss: 0.00000869
Iteration 139/1000 | Loss: 0.00000869
Iteration 140/1000 | Loss: 0.00000869
Iteration 141/1000 | Loss: 0.00000869
Iteration 142/1000 | Loss: 0.00000869
Iteration 143/1000 | Loss: 0.00000869
Iteration 144/1000 | Loss: 0.00000869
Iteration 145/1000 | Loss: 0.00000868
Iteration 146/1000 | Loss: 0.00000868
Iteration 147/1000 | Loss: 0.00000868
Iteration 148/1000 | Loss: 0.00000868
Iteration 149/1000 | Loss: 0.00000868
Iteration 150/1000 | Loss: 0.00000868
Iteration 151/1000 | Loss: 0.00000868
Iteration 152/1000 | Loss: 0.00000868
Iteration 153/1000 | Loss: 0.00000868
Iteration 154/1000 | Loss: 0.00000868
Iteration 155/1000 | Loss: 0.00000868
Iteration 156/1000 | Loss: 0.00000868
Iteration 157/1000 | Loss: 0.00000868
Iteration 158/1000 | Loss: 0.00000868
Iteration 159/1000 | Loss: 0.00000868
Iteration 160/1000 | Loss: 0.00000867
Iteration 161/1000 | Loss: 0.00000867
Iteration 162/1000 | Loss: 0.00000867
Iteration 163/1000 | Loss: 0.00000867
Iteration 164/1000 | Loss: 0.00000867
Iteration 165/1000 | Loss: 0.00000867
Iteration 166/1000 | Loss: 0.00000867
Iteration 167/1000 | Loss: 0.00000867
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 167. Stopping optimization.
Last 5 losses: [8.674554919707589e-06, 8.674554919707589e-06, 8.674554919707589e-06, 8.674554919707589e-06, 8.674554919707589e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.674554919707589e-06

Optimization complete. Final v2v error: 2.5721309185028076 mm

Highest mean error: 2.965193510055542 mm for frame 136

Lowest mean error: 2.4839823246002197 mm for frame 7

Saving results

Total time: 44.10914850234985
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_025/1017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_025/1017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_025/1017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01014632
Iteration 2/25 | Loss: 0.00291426
Iteration 3/25 | Loss: 0.00215088
Iteration 4/25 | Loss: 0.00191854
Iteration 5/25 | Loss: 0.00183212
Iteration 6/25 | Loss: 0.00180418
Iteration 7/25 | Loss: 0.00171343
Iteration 8/25 | Loss: 0.00179975
Iteration 9/25 | Loss: 0.00201835
Iteration 10/25 | Loss: 0.00184034
Iteration 11/25 | Loss: 0.00153236
Iteration 12/25 | Loss: 0.00133570
Iteration 13/25 | Loss: 0.00128375
Iteration 14/25 | Loss: 0.00126812
Iteration 15/25 | Loss: 0.00127013
Iteration 16/25 | Loss: 0.00125485
Iteration 17/25 | Loss: 0.00124915
Iteration 18/25 | Loss: 0.00124746
Iteration 19/25 | Loss: 0.00124778
Iteration 20/25 | Loss: 0.00124670
Iteration 21/25 | Loss: 0.00124343
Iteration 22/25 | Loss: 0.00124270
Iteration 23/25 | Loss: 0.00124222
Iteration 24/25 | Loss: 0.00124156
Iteration 25/25 | Loss: 0.00124323

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.52173328
Iteration 2/25 | Loss: 0.00122714
Iteration 3/25 | Loss: 0.00122714
Iteration 4/25 | Loss: 0.00122714
Iteration 5/25 | Loss: 0.00122714
Iteration 6/25 | Loss: 0.00122714
Iteration 7/25 | Loss: 0.00122714
Iteration 8/25 | Loss: 0.00122713
Iteration 9/25 | Loss: 0.00122713
Iteration 10/25 | Loss: 0.00122713
Iteration 11/25 | Loss: 0.00122713
Iteration 12/25 | Loss: 0.00122713
Iteration 13/25 | Loss: 0.00122713
Iteration 14/25 | Loss: 0.00122713
Iteration 15/25 | Loss: 0.00122713
Iteration 16/25 | Loss: 0.00122713
Iteration 17/25 | Loss: 0.00122713
Iteration 18/25 | Loss: 0.00122713
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0012271342566236854, 0.0012271342566236854, 0.0012271342566236854, 0.0012271342566236854, 0.0012271342566236854]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012271342566236854

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00122713
Iteration 2/1000 | Loss: 0.00004345
Iteration 3/1000 | Loss: 0.00005155
Iteration 4/1000 | Loss: 0.00004879
Iteration 5/1000 | Loss: 0.00003875
Iteration 6/1000 | Loss: 0.00002771
Iteration 7/1000 | Loss: 0.00002210
Iteration 8/1000 | Loss: 0.00002954
Iteration 9/1000 | Loss: 0.00002466
Iteration 10/1000 | Loss: 0.00002580
Iteration 11/1000 | Loss: 0.00003043
Iteration 12/1000 | Loss: 0.00006142
Iteration 13/1000 | Loss: 0.00003723
Iteration 14/1000 | Loss: 0.00003191
Iteration 15/1000 | Loss: 0.00003099
Iteration 16/1000 | Loss: 0.00021080
Iteration 17/1000 | Loss: 0.00002919
Iteration 18/1000 | Loss: 0.00004682
Iteration 19/1000 | Loss: 0.00002747
Iteration 20/1000 | Loss: 0.00001742
Iteration 21/1000 | Loss: 0.00002064
Iteration 22/1000 | Loss: 0.00002534
Iteration 23/1000 | Loss: 0.00003322
Iteration 24/1000 | Loss: 0.00002742
Iteration 25/1000 | Loss: 0.00003437
Iteration 26/1000 | Loss: 0.00002729
Iteration 27/1000 | Loss: 0.00003303
Iteration 28/1000 | Loss: 0.00002628
Iteration 29/1000 | Loss: 0.00002654
Iteration 30/1000 | Loss: 0.00001664
Iteration 31/1000 | Loss: 0.00001572
Iteration 32/1000 | Loss: 0.00001511
Iteration 33/1000 | Loss: 0.00001810
Iteration 34/1000 | Loss: 0.00001541
Iteration 35/1000 | Loss: 0.00001469
Iteration 36/1000 | Loss: 0.00001452
Iteration 37/1000 | Loss: 0.00001445
Iteration 38/1000 | Loss: 0.00001442
Iteration 39/1000 | Loss: 0.00001419
Iteration 40/1000 | Loss: 0.00001408
Iteration 41/1000 | Loss: 0.00001398
Iteration 42/1000 | Loss: 0.00001398
Iteration 43/1000 | Loss: 0.00001397
Iteration 44/1000 | Loss: 0.00001396
Iteration 45/1000 | Loss: 0.00001396
Iteration 46/1000 | Loss: 0.00001395
Iteration 47/1000 | Loss: 0.00002472
Iteration 48/1000 | Loss: 0.00001388
Iteration 49/1000 | Loss: 0.00001387
Iteration 50/1000 | Loss: 0.00001387
Iteration 51/1000 | Loss: 0.00001387
Iteration 52/1000 | Loss: 0.00001387
Iteration 53/1000 | Loss: 0.00001386
Iteration 54/1000 | Loss: 0.00001386
Iteration 55/1000 | Loss: 0.00001386
Iteration 56/1000 | Loss: 0.00001385
Iteration 57/1000 | Loss: 0.00001385
Iteration 58/1000 | Loss: 0.00001385
Iteration 59/1000 | Loss: 0.00001385
Iteration 60/1000 | Loss: 0.00001385
Iteration 61/1000 | Loss: 0.00001385
Iteration 62/1000 | Loss: 0.00001385
Iteration 63/1000 | Loss: 0.00001385
Iteration 64/1000 | Loss: 0.00001384
Iteration 65/1000 | Loss: 0.00001383
Iteration 66/1000 | Loss: 0.00001383
Iteration 67/1000 | Loss: 0.00001383
Iteration 68/1000 | Loss: 0.00001382
Iteration 69/1000 | Loss: 0.00001382
Iteration 70/1000 | Loss: 0.00001381
Iteration 71/1000 | Loss: 0.00001381
Iteration 72/1000 | Loss: 0.00001380
Iteration 73/1000 | Loss: 0.00001379
Iteration 74/1000 | Loss: 0.00001377
Iteration 75/1000 | Loss: 0.00001375
Iteration 76/1000 | Loss: 0.00001374
Iteration 77/1000 | Loss: 0.00001373
Iteration 78/1000 | Loss: 0.00001373
Iteration 79/1000 | Loss: 0.00001372
Iteration 80/1000 | Loss: 0.00001372
Iteration 81/1000 | Loss: 0.00001371
Iteration 82/1000 | Loss: 0.00001371
Iteration 83/1000 | Loss: 0.00001370
Iteration 84/1000 | Loss: 0.00001369
Iteration 85/1000 | Loss: 0.00001368
Iteration 86/1000 | Loss: 0.00001368
Iteration 87/1000 | Loss: 0.00001361
Iteration 88/1000 | Loss: 0.00001358
Iteration 89/1000 | Loss: 0.00001357
Iteration 90/1000 | Loss: 0.00001356
Iteration 91/1000 | Loss: 0.00001356
Iteration 92/1000 | Loss: 0.00001355
Iteration 93/1000 | Loss: 0.00001355
Iteration 94/1000 | Loss: 0.00001355
Iteration 95/1000 | Loss: 0.00001355
Iteration 96/1000 | Loss: 0.00001355
Iteration 97/1000 | Loss: 0.00001352
Iteration 98/1000 | Loss: 0.00001352
Iteration 99/1000 | Loss: 0.00001351
Iteration 100/1000 | Loss: 0.00001351
Iteration 101/1000 | Loss: 0.00001351
Iteration 102/1000 | Loss: 0.00001351
Iteration 103/1000 | Loss: 0.00001350
Iteration 104/1000 | Loss: 0.00001350
Iteration 105/1000 | Loss: 0.00001350
Iteration 106/1000 | Loss: 0.00001350
Iteration 107/1000 | Loss: 0.00001349
Iteration 108/1000 | Loss: 0.00001349
Iteration 109/1000 | Loss: 0.00001349
Iteration 110/1000 | Loss: 0.00001348
Iteration 111/1000 | Loss: 0.00001348
Iteration 112/1000 | Loss: 0.00001347
Iteration 113/1000 | Loss: 0.00001347
Iteration 114/1000 | Loss: 0.00001346
Iteration 115/1000 | Loss: 0.00001346
Iteration 116/1000 | Loss: 0.00001346
Iteration 117/1000 | Loss: 0.00001345
Iteration 118/1000 | Loss: 0.00001345
Iteration 119/1000 | Loss: 0.00001345
Iteration 120/1000 | Loss: 0.00001344
Iteration 121/1000 | Loss: 0.00001344
Iteration 122/1000 | Loss: 0.00001344
Iteration 123/1000 | Loss: 0.00001344
Iteration 124/1000 | Loss: 0.00001344
Iteration 125/1000 | Loss: 0.00001344
Iteration 126/1000 | Loss: 0.00001344
Iteration 127/1000 | Loss: 0.00001343
Iteration 128/1000 | Loss: 0.00001343
Iteration 129/1000 | Loss: 0.00001343
Iteration 130/1000 | Loss: 0.00001343
Iteration 131/1000 | Loss: 0.00001343
Iteration 132/1000 | Loss: 0.00001343
Iteration 133/1000 | Loss: 0.00001342
Iteration 134/1000 | Loss: 0.00001342
Iteration 135/1000 | Loss: 0.00001342
Iteration 136/1000 | Loss: 0.00001342
Iteration 137/1000 | Loss: 0.00001342
Iteration 138/1000 | Loss: 0.00001342
Iteration 139/1000 | Loss: 0.00001342
Iteration 140/1000 | Loss: 0.00001342
Iteration 141/1000 | Loss: 0.00001342
Iteration 142/1000 | Loss: 0.00001342
Iteration 143/1000 | Loss: 0.00001342
Iteration 144/1000 | Loss: 0.00001342
Iteration 145/1000 | Loss: 0.00001342
Iteration 146/1000 | Loss: 0.00001342
Iteration 147/1000 | Loss: 0.00001342
Iteration 148/1000 | Loss: 0.00001342
Iteration 149/1000 | Loss: 0.00001342
Iteration 150/1000 | Loss: 0.00001342
Iteration 151/1000 | Loss: 0.00001341
Iteration 152/1000 | Loss: 0.00001341
Iteration 153/1000 | Loss: 0.00001341
Iteration 154/1000 | Loss: 0.00001341
Iteration 155/1000 | Loss: 0.00001341
Iteration 156/1000 | Loss: 0.00001341
Iteration 157/1000 | Loss: 0.00001341
Iteration 158/1000 | Loss: 0.00001341
Iteration 159/1000 | Loss: 0.00001341
Iteration 160/1000 | Loss: 0.00001341
Iteration 161/1000 | Loss: 0.00001341
Iteration 162/1000 | Loss: 0.00001341
Iteration 163/1000 | Loss: 0.00001341
Iteration 164/1000 | Loss: 0.00001341
Iteration 165/1000 | Loss: 0.00001341
Iteration 166/1000 | Loss: 0.00001341
Iteration 167/1000 | Loss: 0.00001341
Iteration 168/1000 | Loss: 0.00001341
Iteration 169/1000 | Loss: 0.00001341
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 169. Stopping optimization.
Last 5 losses: [1.3414942259259988e-05, 1.3414942259259988e-05, 1.3414942259259988e-05, 1.3414942259259988e-05, 1.3414942259259988e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3414942259259988e-05

Optimization complete. Final v2v error: 3.1806280612945557 mm

Highest mean error: 4.296355724334717 mm for frame 37

Lowest mean error: 3.0577895641326904 mm for frame 203

Saving results

Total time: 130.00156807899475
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_025/1097/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_025/1097.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_025/1097
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00833187
Iteration 2/25 | Loss: 0.00159687
Iteration 3/25 | Loss: 0.00135999
Iteration 4/25 | Loss: 0.00133852
Iteration 5/25 | Loss: 0.00133464
Iteration 6/25 | Loss: 0.00133354
Iteration 7/25 | Loss: 0.00133354
Iteration 8/25 | Loss: 0.00133354
Iteration 9/25 | Loss: 0.00133354
Iteration 10/25 | Loss: 0.00133354
Iteration 11/25 | Loss: 0.00133354
Iteration 12/25 | Loss: 0.00133354
Iteration 13/25 | Loss: 0.00133354
Iteration 14/25 | Loss: 0.00133354
Iteration 15/25 | Loss: 0.00133354
Iteration 16/25 | Loss: 0.00133354
Iteration 17/25 | Loss: 0.00133354
Iteration 18/25 | Loss: 0.00133354
Iteration 19/25 | Loss: 0.00133354
Iteration 20/25 | Loss: 0.00133354
Iteration 21/25 | Loss: 0.00133354
Iteration 22/25 | Loss: 0.00133354
Iteration 23/25 | Loss: 0.00133354
Iteration 24/25 | Loss: 0.00133354
Iteration 25/25 | Loss: 0.00133354

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.30551922
Iteration 2/25 | Loss: 0.00141899
Iteration 3/25 | Loss: 0.00141899
Iteration 4/25 | Loss: 0.00141899
Iteration 5/25 | Loss: 0.00141898
Iteration 6/25 | Loss: 0.00141898
Iteration 7/25 | Loss: 0.00141898
Iteration 8/25 | Loss: 0.00141898
Iteration 9/25 | Loss: 0.00141898
Iteration 10/25 | Loss: 0.00141898
Iteration 11/25 | Loss: 0.00141898
Iteration 12/25 | Loss: 0.00141898
Iteration 13/25 | Loss: 0.00141898
Iteration 14/25 | Loss: 0.00141898
Iteration 15/25 | Loss: 0.00141898
Iteration 16/25 | Loss: 0.00141898
Iteration 17/25 | Loss: 0.00141898
Iteration 18/25 | Loss: 0.00141898
Iteration 19/25 | Loss: 0.00141898
Iteration 20/25 | Loss: 0.00141898
Iteration 21/25 | Loss: 0.00141898
Iteration 22/25 | Loss: 0.00141898
Iteration 23/25 | Loss: 0.00141898
Iteration 24/25 | Loss: 0.00141898
Iteration 25/25 | Loss: 0.00141898
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.00141898391302675, 0.00141898391302675, 0.00141898391302675, 0.00141898391302675, 0.00141898391302675]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00141898391302675

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00141898
Iteration 2/1000 | Loss: 0.00004557
Iteration 3/1000 | Loss: 0.00003047
Iteration 4/1000 | Loss: 0.00002741
Iteration 5/1000 | Loss: 0.00002615
Iteration 6/1000 | Loss: 0.00002521
Iteration 7/1000 | Loss: 0.00002460
Iteration 8/1000 | Loss: 0.00002413
Iteration 9/1000 | Loss: 0.00002378
Iteration 10/1000 | Loss: 0.00002349
Iteration 11/1000 | Loss: 0.00002320
Iteration 12/1000 | Loss: 0.00002306
Iteration 13/1000 | Loss: 0.00002288
Iteration 14/1000 | Loss: 0.00002268
Iteration 15/1000 | Loss: 0.00002255
Iteration 16/1000 | Loss: 0.00002247
Iteration 17/1000 | Loss: 0.00002239
Iteration 18/1000 | Loss: 0.00002234
Iteration 19/1000 | Loss: 0.00002232
Iteration 20/1000 | Loss: 0.00002232
Iteration 21/1000 | Loss: 0.00002231
Iteration 22/1000 | Loss: 0.00002231
Iteration 23/1000 | Loss: 0.00002231
Iteration 24/1000 | Loss: 0.00002227
Iteration 25/1000 | Loss: 0.00002227
Iteration 26/1000 | Loss: 0.00002226
Iteration 27/1000 | Loss: 0.00002223
Iteration 28/1000 | Loss: 0.00002223
Iteration 29/1000 | Loss: 0.00002223
Iteration 30/1000 | Loss: 0.00002222
Iteration 31/1000 | Loss: 0.00002222
Iteration 32/1000 | Loss: 0.00002222
Iteration 33/1000 | Loss: 0.00002222
Iteration 34/1000 | Loss: 0.00002222
Iteration 35/1000 | Loss: 0.00002222
Iteration 36/1000 | Loss: 0.00002222
Iteration 37/1000 | Loss: 0.00002222
Iteration 38/1000 | Loss: 0.00002222
Iteration 39/1000 | Loss: 0.00002221
Iteration 40/1000 | Loss: 0.00002218
Iteration 41/1000 | Loss: 0.00002217
Iteration 42/1000 | Loss: 0.00002217
Iteration 43/1000 | Loss: 0.00002217
Iteration 44/1000 | Loss: 0.00002216
Iteration 45/1000 | Loss: 0.00002214
Iteration 46/1000 | Loss: 0.00002213
Iteration 47/1000 | Loss: 0.00002212
Iteration 48/1000 | Loss: 0.00002212
Iteration 49/1000 | Loss: 0.00002212
Iteration 50/1000 | Loss: 0.00002210
Iteration 51/1000 | Loss: 0.00002209
Iteration 52/1000 | Loss: 0.00002209
Iteration 53/1000 | Loss: 0.00002209
Iteration 54/1000 | Loss: 0.00002208
Iteration 55/1000 | Loss: 0.00002208
Iteration 56/1000 | Loss: 0.00002207
Iteration 57/1000 | Loss: 0.00002207
Iteration 58/1000 | Loss: 0.00002207
Iteration 59/1000 | Loss: 0.00002206
Iteration 60/1000 | Loss: 0.00002205
Iteration 61/1000 | Loss: 0.00002205
Iteration 62/1000 | Loss: 0.00002205
Iteration 63/1000 | Loss: 0.00002205
Iteration 64/1000 | Loss: 0.00002205
Iteration 65/1000 | Loss: 0.00002204
Iteration 66/1000 | Loss: 0.00002204
Iteration 67/1000 | Loss: 0.00002204
Iteration 68/1000 | Loss: 0.00002204
Iteration 69/1000 | Loss: 0.00002203
Iteration 70/1000 | Loss: 0.00002203
Iteration 71/1000 | Loss: 0.00002203
Iteration 72/1000 | Loss: 0.00002202
Iteration 73/1000 | Loss: 0.00002201
Iteration 74/1000 | Loss: 0.00002201
Iteration 75/1000 | Loss: 0.00002201
Iteration 76/1000 | Loss: 0.00002201
Iteration 77/1000 | Loss: 0.00002201
Iteration 78/1000 | Loss: 0.00002201
Iteration 79/1000 | Loss: 0.00002201
Iteration 80/1000 | Loss: 0.00002201
Iteration 81/1000 | Loss: 0.00002201
Iteration 82/1000 | Loss: 0.00002201
Iteration 83/1000 | Loss: 0.00002200
Iteration 84/1000 | Loss: 0.00002200
Iteration 85/1000 | Loss: 0.00002200
Iteration 86/1000 | Loss: 0.00002200
Iteration 87/1000 | Loss: 0.00002199
Iteration 88/1000 | Loss: 0.00002199
Iteration 89/1000 | Loss: 0.00002199
Iteration 90/1000 | Loss: 0.00002198
Iteration 91/1000 | Loss: 0.00002198
Iteration 92/1000 | Loss: 0.00002198
Iteration 93/1000 | Loss: 0.00002198
Iteration 94/1000 | Loss: 0.00002198
Iteration 95/1000 | Loss: 0.00002198
Iteration 96/1000 | Loss: 0.00002198
Iteration 97/1000 | Loss: 0.00002198
Iteration 98/1000 | Loss: 0.00002197
Iteration 99/1000 | Loss: 0.00002197
Iteration 100/1000 | Loss: 0.00002197
Iteration 101/1000 | Loss: 0.00002197
Iteration 102/1000 | Loss: 0.00002197
Iteration 103/1000 | Loss: 0.00002197
Iteration 104/1000 | Loss: 0.00002197
Iteration 105/1000 | Loss: 0.00002197
Iteration 106/1000 | Loss: 0.00002197
Iteration 107/1000 | Loss: 0.00002196
Iteration 108/1000 | Loss: 0.00002196
Iteration 109/1000 | Loss: 0.00002196
Iteration 110/1000 | Loss: 0.00002196
Iteration 111/1000 | Loss: 0.00002196
Iteration 112/1000 | Loss: 0.00002196
Iteration 113/1000 | Loss: 0.00002196
Iteration 114/1000 | Loss: 0.00002196
Iteration 115/1000 | Loss: 0.00002196
Iteration 116/1000 | Loss: 0.00002196
Iteration 117/1000 | Loss: 0.00002196
Iteration 118/1000 | Loss: 0.00002196
Iteration 119/1000 | Loss: 0.00002196
Iteration 120/1000 | Loss: 0.00002196
Iteration 121/1000 | Loss: 0.00002196
Iteration 122/1000 | Loss: 0.00002195
Iteration 123/1000 | Loss: 0.00002195
Iteration 124/1000 | Loss: 0.00002195
Iteration 125/1000 | Loss: 0.00002195
Iteration 126/1000 | Loss: 0.00002195
Iteration 127/1000 | Loss: 0.00002195
Iteration 128/1000 | Loss: 0.00002195
Iteration 129/1000 | Loss: 0.00002195
Iteration 130/1000 | Loss: 0.00002195
Iteration 131/1000 | Loss: 0.00002195
Iteration 132/1000 | Loss: 0.00002195
Iteration 133/1000 | Loss: 0.00002195
Iteration 134/1000 | Loss: 0.00002195
Iteration 135/1000 | Loss: 0.00002195
Iteration 136/1000 | Loss: 0.00002195
Iteration 137/1000 | Loss: 0.00002195
Iteration 138/1000 | Loss: 0.00002195
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 138. Stopping optimization.
Last 5 losses: [2.1948188077658415e-05, 2.1948188077658415e-05, 2.1948188077658415e-05, 2.1948188077658415e-05, 2.1948188077658415e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1948188077658415e-05

Optimization complete. Final v2v error: 3.8311307430267334 mm

Highest mean error: 5.424508571624756 mm for frame 149

Lowest mean error: 2.7293548583984375 mm for frame 6

Saving results

Total time: 42.62823295593262
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_025/1062/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_025/1062.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_025/1062
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00578678
Iteration 2/25 | Loss: 0.00139441
Iteration 3/25 | Loss: 0.00129748
Iteration 4/25 | Loss: 0.00127180
Iteration 5/25 | Loss: 0.00126150
Iteration 6/25 | Loss: 0.00125989
Iteration 7/25 | Loss: 0.00125989
Iteration 8/25 | Loss: 0.00125989
Iteration 9/25 | Loss: 0.00125989
Iteration 10/25 | Loss: 0.00125989
Iteration 11/25 | Loss: 0.00125989
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.00125989131629467, 0.00125989131629467, 0.00125989131629467, 0.00125989131629467, 0.00125989131629467]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00125989131629467

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.60251760
Iteration 2/25 | Loss: 0.00166294
Iteration 3/25 | Loss: 0.00166294
Iteration 4/25 | Loss: 0.00166293
Iteration 5/25 | Loss: 0.00166293
Iteration 6/25 | Loss: 0.00166293
Iteration 7/25 | Loss: 0.00166293
Iteration 8/25 | Loss: 0.00166293
Iteration 9/25 | Loss: 0.00166293
Iteration 10/25 | Loss: 0.00166293
Iteration 11/25 | Loss: 0.00166293
Iteration 12/25 | Loss: 0.00166293
Iteration 13/25 | Loss: 0.00166293
Iteration 14/25 | Loss: 0.00166293
Iteration 15/25 | Loss: 0.00166293
Iteration 16/25 | Loss: 0.00166293
Iteration 17/25 | Loss: 0.00166293
Iteration 18/25 | Loss: 0.00166293
Iteration 19/25 | Loss: 0.00166293
Iteration 20/25 | Loss: 0.00166293
Iteration 21/25 | Loss: 0.00166293
Iteration 22/25 | Loss: 0.00166293
Iteration 23/25 | Loss: 0.00166293
Iteration 24/25 | Loss: 0.00166293
Iteration 25/25 | Loss: 0.00166293

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00166293
Iteration 2/1000 | Loss: 0.00002831
Iteration 3/1000 | Loss: 0.00002376
Iteration 4/1000 | Loss: 0.00002269
Iteration 5/1000 | Loss: 0.00002200
Iteration 6/1000 | Loss: 0.00002132
Iteration 7/1000 | Loss: 0.00002098
Iteration 8/1000 | Loss: 0.00002044
Iteration 9/1000 | Loss: 0.00002013
Iteration 10/1000 | Loss: 0.00001981
Iteration 11/1000 | Loss: 0.00001949
Iteration 12/1000 | Loss: 0.00001925
Iteration 13/1000 | Loss: 0.00001903
Iteration 14/1000 | Loss: 0.00001902
Iteration 15/1000 | Loss: 0.00001899
Iteration 16/1000 | Loss: 0.00001894
Iteration 17/1000 | Loss: 0.00001893
Iteration 18/1000 | Loss: 0.00001893
Iteration 19/1000 | Loss: 0.00001892
Iteration 20/1000 | Loss: 0.00001892
Iteration 21/1000 | Loss: 0.00001891
Iteration 22/1000 | Loss: 0.00001890
Iteration 23/1000 | Loss: 0.00001888
Iteration 24/1000 | Loss: 0.00001887
Iteration 25/1000 | Loss: 0.00001884
Iteration 26/1000 | Loss: 0.00001884
Iteration 27/1000 | Loss: 0.00001884
Iteration 28/1000 | Loss: 0.00001883
Iteration 29/1000 | Loss: 0.00001882
Iteration 30/1000 | Loss: 0.00001882
Iteration 31/1000 | Loss: 0.00001880
Iteration 32/1000 | Loss: 0.00001880
Iteration 33/1000 | Loss: 0.00001880
Iteration 34/1000 | Loss: 0.00001879
Iteration 35/1000 | Loss: 0.00001879
Iteration 36/1000 | Loss: 0.00001878
Iteration 37/1000 | Loss: 0.00001878
Iteration 38/1000 | Loss: 0.00001877
Iteration 39/1000 | Loss: 0.00001874
Iteration 40/1000 | Loss: 0.00001874
Iteration 41/1000 | Loss: 0.00001873
Iteration 42/1000 | Loss: 0.00001873
Iteration 43/1000 | Loss: 0.00001872
Iteration 44/1000 | Loss: 0.00001872
Iteration 45/1000 | Loss: 0.00001872
Iteration 46/1000 | Loss: 0.00001871
Iteration 47/1000 | Loss: 0.00001871
Iteration 48/1000 | Loss: 0.00001871
Iteration 49/1000 | Loss: 0.00001870
Iteration 50/1000 | Loss: 0.00001870
Iteration 51/1000 | Loss: 0.00001869
Iteration 52/1000 | Loss: 0.00001869
Iteration 53/1000 | Loss: 0.00001869
Iteration 54/1000 | Loss: 0.00001869
Iteration 55/1000 | Loss: 0.00001869
Iteration 56/1000 | Loss: 0.00001868
Iteration 57/1000 | Loss: 0.00001868
Iteration 58/1000 | Loss: 0.00001868
Iteration 59/1000 | Loss: 0.00001868
Iteration 60/1000 | Loss: 0.00001867
Iteration 61/1000 | Loss: 0.00001867
Iteration 62/1000 | Loss: 0.00001867
Iteration 63/1000 | Loss: 0.00001866
Iteration 64/1000 | Loss: 0.00001866
Iteration 65/1000 | Loss: 0.00001866
Iteration 66/1000 | Loss: 0.00001866
Iteration 67/1000 | Loss: 0.00001865
Iteration 68/1000 | Loss: 0.00001865
Iteration 69/1000 | Loss: 0.00001865
Iteration 70/1000 | Loss: 0.00001864
Iteration 71/1000 | Loss: 0.00001864
Iteration 72/1000 | Loss: 0.00001864
Iteration 73/1000 | Loss: 0.00001864
Iteration 74/1000 | Loss: 0.00001863
Iteration 75/1000 | Loss: 0.00001863
Iteration 76/1000 | Loss: 0.00001862
Iteration 77/1000 | Loss: 0.00001862
Iteration 78/1000 | Loss: 0.00001862
Iteration 79/1000 | Loss: 0.00001861
Iteration 80/1000 | Loss: 0.00001861
Iteration 81/1000 | Loss: 0.00001861
Iteration 82/1000 | Loss: 0.00001860
Iteration 83/1000 | Loss: 0.00001860
Iteration 84/1000 | Loss: 0.00001860
Iteration 85/1000 | Loss: 0.00001860
Iteration 86/1000 | Loss: 0.00001860
Iteration 87/1000 | Loss: 0.00001859
Iteration 88/1000 | Loss: 0.00001859
Iteration 89/1000 | Loss: 0.00001859
Iteration 90/1000 | Loss: 0.00001859
Iteration 91/1000 | Loss: 0.00001859
Iteration 92/1000 | Loss: 0.00001859
Iteration 93/1000 | Loss: 0.00001859
Iteration 94/1000 | Loss: 0.00001859
Iteration 95/1000 | Loss: 0.00001858
Iteration 96/1000 | Loss: 0.00001858
Iteration 97/1000 | Loss: 0.00001858
Iteration 98/1000 | Loss: 0.00001858
Iteration 99/1000 | Loss: 0.00001858
Iteration 100/1000 | Loss: 0.00001858
Iteration 101/1000 | Loss: 0.00001858
Iteration 102/1000 | Loss: 0.00001858
Iteration 103/1000 | Loss: 0.00001858
Iteration 104/1000 | Loss: 0.00001858
Iteration 105/1000 | Loss: 0.00001858
Iteration 106/1000 | Loss: 0.00001858
Iteration 107/1000 | Loss: 0.00001858
Iteration 108/1000 | Loss: 0.00001858
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 108. Stopping optimization.
Last 5 losses: [1.858083669503685e-05, 1.858083669503685e-05, 1.858083669503685e-05, 1.858083669503685e-05, 1.858083669503685e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.858083669503685e-05

Optimization complete. Final v2v error: 3.6892082691192627 mm

Highest mean error: 4.140646457672119 mm for frame 162

Lowest mean error: 3.3420088291168213 mm for frame 29

Saving results

Total time: 41.13289999961853
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_025/1050/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_025/1050.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_025/1050
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00804754
Iteration 2/25 | Loss: 0.00159505
Iteration 3/25 | Loss: 0.00133493
Iteration 4/25 | Loss: 0.00130089
Iteration 5/25 | Loss: 0.00130407
Iteration 6/25 | Loss: 0.00127800
Iteration 7/25 | Loss: 0.00127274
Iteration 8/25 | Loss: 0.00127214
Iteration 9/25 | Loss: 0.00127203
Iteration 10/25 | Loss: 0.00127198
Iteration 11/25 | Loss: 0.00127198
Iteration 12/25 | Loss: 0.00127198
Iteration 13/25 | Loss: 0.00127198
Iteration 14/25 | Loss: 0.00127198
Iteration 15/25 | Loss: 0.00127198
Iteration 16/25 | Loss: 0.00127198
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0012719797668978572, 0.0012719797668978572, 0.0012719797668978572, 0.0012719797668978572, 0.0012719797668978572]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012719797668978572

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.30266476
Iteration 2/25 | Loss: 0.00131390
Iteration 3/25 | Loss: 0.00131389
Iteration 4/25 | Loss: 0.00131389
Iteration 5/25 | Loss: 0.00131389
Iteration 6/25 | Loss: 0.00131389
Iteration 7/25 | Loss: 0.00131388
Iteration 8/25 | Loss: 0.00131388
Iteration 9/25 | Loss: 0.00131388
Iteration 10/25 | Loss: 0.00131388
Iteration 11/25 | Loss: 0.00131388
Iteration 12/25 | Loss: 0.00131388
Iteration 13/25 | Loss: 0.00131388
Iteration 14/25 | Loss: 0.00131388
Iteration 15/25 | Loss: 0.00131388
Iteration 16/25 | Loss: 0.00131388
Iteration 17/25 | Loss: 0.00131388
Iteration 18/25 | Loss: 0.00131388
Iteration 19/25 | Loss: 0.00131388
Iteration 20/25 | Loss: 0.00131388
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0013138834619894624, 0.0013138834619894624, 0.0013138834619894624, 0.0013138834619894624, 0.0013138834619894624]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013138834619894624

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00131388
Iteration 2/1000 | Loss: 0.00003752
Iteration 3/1000 | Loss: 0.00002308
Iteration 4/1000 | Loss: 0.00001994
Iteration 5/1000 | Loss: 0.00001895
Iteration 6/1000 | Loss: 0.00001810
Iteration 7/1000 | Loss: 0.00001757
Iteration 8/1000 | Loss: 0.00001721
Iteration 9/1000 | Loss: 0.00001691
Iteration 10/1000 | Loss: 0.00001661
Iteration 11/1000 | Loss: 0.00001631
Iteration 12/1000 | Loss: 0.00001611
Iteration 13/1000 | Loss: 0.00020156
Iteration 14/1000 | Loss: 0.00002147
Iteration 15/1000 | Loss: 0.00001806
Iteration 16/1000 | Loss: 0.00001692
Iteration 17/1000 | Loss: 0.00001589
Iteration 18/1000 | Loss: 0.00001541
Iteration 19/1000 | Loss: 0.00001515
Iteration 20/1000 | Loss: 0.00001498
Iteration 21/1000 | Loss: 0.00001496
Iteration 22/1000 | Loss: 0.00001494
Iteration 23/1000 | Loss: 0.00001491
Iteration 24/1000 | Loss: 0.00001489
Iteration 25/1000 | Loss: 0.00001488
Iteration 26/1000 | Loss: 0.00001488
Iteration 27/1000 | Loss: 0.00001487
Iteration 28/1000 | Loss: 0.00001487
Iteration 29/1000 | Loss: 0.00001486
Iteration 30/1000 | Loss: 0.00001486
Iteration 31/1000 | Loss: 0.00001486
Iteration 32/1000 | Loss: 0.00001484
Iteration 33/1000 | Loss: 0.00001484
Iteration 34/1000 | Loss: 0.00001483
Iteration 35/1000 | Loss: 0.00001482
Iteration 36/1000 | Loss: 0.00001478
Iteration 37/1000 | Loss: 0.00001476
Iteration 38/1000 | Loss: 0.00001475
Iteration 39/1000 | Loss: 0.00001475
Iteration 40/1000 | Loss: 0.00001475
Iteration 41/1000 | Loss: 0.00001474
Iteration 42/1000 | Loss: 0.00001474
Iteration 43/1000 | Loss: 0.00001473
Iteration 44/1000 | Loss: 0.00001473
Iteration 45/1000 | Loss: 0.00001473
Iteration 46/1000 | Loss: 0.00001472
Iteration 47/1000 | Loss: 0.00001472
Iteration 48/1000 | Loss: 0.00001471
Iteration 49/1000 | Loss: 0.00001470
Iteration 50/1000 | Loss: 0.00001470
Iteration 51/1000 | Loss: 0.00001469
Iteration 52/1000 | Loss: 0.00001469
Iteration 53/1000 | Loss: 0.00001468
Iteration 54/1000 | Loss: 0.00001468
Iteration 55/1000 | Loss: 0.00001468
Iteration 56/1000 | Loss: 0.00001467
Iteration 57/1000 | Loss: 0.00001466
Iteration 58/1000 | Loss: 0.00001466
Iteration 59/1000 | Loss: 0.00001465
Iteration 60/1000 | Loss: 0.00001465
Iteration 61/1000 | Loss: 0.00001464
Iteration 62/1000 | Loss: 0.00001464
Iteration 63/1000 | Loss: 0.00001464
Iteration 64/1000 | Loss: 0.00001463
Iteration 65/1000 | Loss: 0.00001463
Iteration 66/1000 | Loss: 0.00001462
Iteration 67/1000 | Loss: 0.00001462
Iteration 68/1000 | Loss: 0.00001462
Iteration 69/1000 | Loss: 0.00001462
Iteration 70/1000 | Loss: 0.00001462
Iteration 71/1000 | Loss: 0.00001462
Iteration 72/1000 | Loss: 0.00001462
Iteration 73/1000 | Loss: 0.00001461
Iteration 74/1000 | Loss: 0.00001461
Iteration 75/1000 | Loss: 0.00001461
Iteration 76/1000 | Loss: 0.00001461
Iteration 77/1000 | Loss: 0.00001461
Iteration 78/1000 | Loss: 0.00001461
Iteration 79/1000 | Loss: 0.00001460
Iteration 80/1000 | Loss: 0.00001460
Iteration 81/1000 | Loss: 0.00001460
Iteration 82/1000 | Loss: 0.00001460
Iteration 83/1000 | Loss: 0.00001460
Iteration 84/1000 | Loss: 0.00001460
Iteration 85/1000 | Loss: 0.00001460
Iteration 86/1000 | Loss: 0.00001460
Iteration 87/1000 | Loss: 0.00001460
Iteration 88/1000 | Loss: 0.00001460
Iteration 89/1000 | Loss: 0.00001460
Iteration 90/1000 | Loss: 0.00001460
Iteration 91/1000 | Loss: 0.00001460
Iteration 92/1000 | Loss: 0.00001460
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 92. Stopping optimization.
Last 5 losses: [1.4603199815610424e-05, 1.4603199815610424e-05, 1.4603199815610424e-05, 1.4603199815610424e-05, 1.4603199815610424e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4603199815610424e-05

Optimization complete. Final v2v error: 3.2446987628936768 mm

Highest mean error: 4.068043231964111 mm for frame 174

Lowest mean error: 2.753591775894165 mm for frame 239

Saving results

Total time: 57.94143223762512
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_025/1080/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_025/1080.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_025/1080
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01034951
Iteration 2/25 | Loss: 0.00260640
Iteration 3/25 | Loss: 0.00189993
Iteration 4/25 | Loss: 0.00194706
Iteration 5/25 | Loss: 0.00177656
Iteration 6/25 | Loss: 0.00161991
Iteration 7/25 | Loss: 0.00153662
Iteration 8/25 | Loss: 0.00154842
Iteration 9/25 | Loss: 0.00150253
Iteration 10/25 | Loss: 0.00150014
Iteration 11/25 | Loss: 0.00149895
Iteration 12/25 | Loss: 0.00147660
Iteration 13/25 | Loss: 0.00145377
Iteration 14/25 | Loss: 0.00142835
Iteration 15/25 | Loss: 0.00141082
Iteration 16/25 | Loss: 0.00143076
Iteration 17/25 | Loss: 0.00140066
Iteration 18/25 | Loss: 0.00138766
Iteration 19/25 | Loss: 0.00137506
Iteration 20/25 | Loss: 0.00137140
Iteration 21/25 | Loss: 0.00135560
Iteration 22/25 | Loss: 0.00134844
Iteration 23/25 | Loss: 0.00133516
Iteration 24/25 | Loss: 0.00133221
Iteration 25/25 | Loss: 0.00132741

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32320237
Iteration 2/25 | Loss: 0.00275970
Iteration 3/25 | Loss: 0.00262894
Iteration 4/25 | Loss: 0.00262894
Iteration 5/25 | Loss: 0.00262894
Iteration 6/25 | Loss: 0.00262894
Iteration 7/25 | Loss: 0.00262894
Iteration 8/25 | Loss: 0.00262894
Iteration 9/25 | Loss: 0.00262894
Iteration 10/25 | Loss: 0.00262894
Iteration 11/25 | Loss: 0.00262894
Iteration 12/25 | Loss: 0.00262894
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.002628941321745515, 0.002628941321745515, 0.002628941321745515, 0.002628941321745515, 0.002628941321745515]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002628941321745515

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00262894
Iteration 2/1000 | Loss: 0.00190518
Iteration 3/1000 | Loss: 0.00449251
Iteration 4/1000 | Loss: 0.00346983
Iteration 5/1000 | Loss: 0.00245347
Iteration 6/1000 | Loss: 0.00279560
Iteration 7/1000 | Loss: 0.00281784
Iteration 8/1000 | Loss: 0.00260268
Iteration 9/1000 | Loss: 0.00436284
Iteration 10/1000 | Loss: 0.00377344
Iteration 11/1000 | Loss: 0.00332749
Iteration 12/1000 | Loss: 0.00391808
Iteration 13/1000 | Loss: 0.00297658
Iteration 14/1000 | Loss: 0.00511598
Iteration 15/1000 | Loss: 0.00378390
Iteration 16/1000 | Loss: 0.00337539
Iteration 17/1000 | Loss: 0.00373223
Iteration 18/1000 | Loss: 0.00469995
Iteration 19/1000 | Loss: 0.00453439
Iteration 20/1000 | Loss: 0.00345439
Iteration 21/1000 | Loss: 0.00304714
Iteration 22/1000 | Loss: 0.00345499
Iteration 23/1000 | Loss: 0.00354945
Iteration 24/1000 | Loss: 0.00395377
Iteration 25/1000 | Loss: 0.00471373
Iteration 26/1000 | Loss: 0.00436111
Iteration 27/1000 | Loss: 0.00415632
Iteration 28/1000 | Loss: 0.00409751
Iteration 29/1000 | Loss: 0.00445713
Iteration 30/1000 | Loss: 0.00405180
Iteration 31/1000 | Loss: 0.00392193
Iteration 32/1000 | Loss: 0.00475096
Iteration 33/1000 | Loss: 0.00389358
Iteration 34/1000 | Loss: 0.00451080
Iteration 35/1000 | Loss: 0.00414763
Iteration 36/1000 | Loss: 0.00508960
Iteration 37/1000 | Loss: 0.00544484
Iteration 38/1000 | Loss: 0.00453783
Iteration 39/1000 | Loss: 0.00528335
Iteration 40/1000 | Loss: 0.00427537
Iteration 41/1000 | Loss: 0.00486427
Iteration 42/1000 | Loss: 0.00416217
Iteration 43/1000 | Loss: 0.00462981
Iteration 44/1000 | Loss: 0.00416575
Iteration 45/1000 | Loss: 0.00412761
Iteration 46/1000 | Loss: 0.00483848
Iteration 47/1000 | Loss: 0.00428693
Iteration 48/1000 | Loss: 0.00382658
Iteration 49/1000 | Loss: 0.00394013
Iteration 50/1000 | Loss: 0.00347610
Iteration 51/1000 | Loss: 0.00400423
Iteration 52/1000 | Loss: 0.00484166
Iteration 53/1000 | Loss: 0.00453500
Iteration 54/1000 | Loss: 0.00380886
Iteration 55/1000 | Loss: 0.00449540
Iteration 56/1000 | Loss: 0.00492002
Iteration 57/1000 | Loss: 0.00446909
Iteration 58/1000 | Loss: 0.00449773
Iteration 59/1000 | Loss: 0.00444226
Iteration 60/1000 | Loss: 0.00591851
Iteration 61/1000 | Loss: 0.00494882
Iteration 62/1000 | Loss: 0.00453705
Iteration 63/1000 | Loss: 0.00429074
Iteration 64/1000 | Loss: 0.00474624
Iteration 65/1000 | Loss: 0.00447695
Iteration 66/1000 | Loss: 0.00466835
Iteration 67/1000 | Loss: 0.00393049
Iteration 68/1000 | Loss: 0.00337855
Iteration 69/1000 | Loss: 0.00419891
Iteration 70/1000 | Loss: 0.00391527
Iteration 71/1000 | Loss: 0.00744186
Iteration 72/1000 | Loss: 0.00414416
Iteration 73/1000 | Loss: 0.00382601
Iteration 74/1000 | Loss: 0.00468571
Iteration 75/1000 | Loss: 0.00431616
Iteration 76/1000 | Loss: 0.00437647
Iteration 77/1000 | Loss: 0.00464321
Iteration 78/1000 | Loss: 0.00446467
Iteration 79/1000 | Loss: 0.00491677
Iteration 80/1000 | Loss: 0.00441832
Iteration 81/1000 | Loss: 0.00485654
Iteration 82/1000 | Loss: 0.00492238
Iteration 83/1000 | Loss: 0.00404597
Iteration 84/1000 | Loss: 0.00470002
Iteration 85/1000 | Loss: 0.00479591
Iteration 86/1000 | Loss: 0.00382540
Iteration 87/1000 | Loss: 0.00336268
Iteration 88/1000 | Loss: 0.00401791
Iteration 89/1000 | Loss: 0.00454123
Iteration 90/1000 | Loss: 0.00452113
Iteration 91/1000 | Loss: 0.00478399
Iteration 92/1000 | Loss: 0.00441710
Iteration 93/1000 | Loss: 0.00429771
Iteration 94/1000 | Loss: 0.00475063
Iteration 95/1000 | Loss: 0.00392954
Iteration 96/1000 | Loss: 0.00477255
Iteration 97/1000 | Loss: 0.00422841
Iteration 98/1000 | Loss: 0.00530120
Iteration 99/1000 | Loss: 0.00545900
Iteration 100/1000 | Loss: 0.00429721
Iteration 101/1000 | Loss: 0.00341635
Iteration 102/1000 | Loss: 0.00369819
Iteration 103/1000 | Loss: 0.00393209
Iteration 104/1000 | Loss: 0.00409311
Iteration 105/1000 | Loss: 0.00483742
Iteration 106/1000 | Loss: 0.00406165
Iteration 107/1000 | Loss: 0.00441348
Iteration 108/1000 | Loss: 0.00422695
Iteration 109/1000 | Loss: 0.00375561
Iteration 110/1000 | Loss: 0.00426291
Iteration 111/1000 | Loss: 0.00371416
Iteration 112/1000 | Loss: 0.00390924
Iteration 113/1000 | Loss: 0.00507129
Iteration 114/1000 | Loss: 0.00401040
Iteration 115/1000 | Loss: 0.00422742
Iteration 116/1000 | Loss: 0.00451211
Iteration 117/1000 | Loss: 0.00397076
Iteration 118/1000 | Loss: 0.00486299
Iteration 119/1000 | Loss: 0.00363784
Iteration 120/1000 | Loss: 0.00420391
Iteration 121/1000 | Loss: 0.00383825
Iteration 122/1000 | Loss: 0.00401163
Iteration 123/1000 | Loss: 0.00388631
Iteration 124/1000 | Loss: 0.00436907
Iteration 125/1000 | Loss: 0.00492099
Iteration 126/1000 | Loss: 0.00340351
Iteration 127/1000 | Loss: 0.00401096
Iteration 128/1000 | Loss: 0.00459818
Iteration 129/1000 | Loss: 0.00524690
Iteration 130/1000 | Loss: 0.00528744
Iteration 131/1000 | Loss: 0.00468053
Iteration 132/1000 | Loss: 0.00513481
Iteration 133/1000 | Loss: 0.00642285
Iteration 134/1000 | Loss: 0.00442779
Iteration 135/1000 | Loss: 0.00463436
Iteration 136/1000 | Loss: 0.00441386
Iteration 137/1000 | Loss: 0.00415995
Iteration 138/1000 | Loss: 0.00439274
Iteration 139/1000 | Loss: 0.00439610
Iteration 140/1000 | Loss: 0.00553982
Iteration 141/1000 | Loss: 0.00532227
Iteration 142/1000 | Loss: 0.00471019
Iteration 143/1000 | Loss: 0.00507395
Iteration 144/1000 | Loss: 0.00423361
Iteration 145/1000 | Loss: 0.00485357
Iteration 146/1000 | Loss: 0.00392693
Iteration 147/1000 | Loss: 0.00442845
Iteration 148/1000 | Loss: 0.00281645
Iteration 149/1000 | Loss: 0.00424686
Iteration 150/1000 | Loss: 0.00584791
Iteration 151/1000 | Loss: 0.00340294
Iteration 152/1000 | Loss: 0.00377967
Iteration 153/1000 | Loss: 0.00403444
Iteration 154/1000 | Loss: 0.00254627
Iteration 155/1000 | Loss: 0.00352347
Iteration 156/1000 | Loss: 0.00395212
Iteration 157/1000 | Loss: 0.00318314
Iteration 158/1000 | Loss: 0.00262402
Iteration 159/1000 | Loss: 0.00257238
Iteration 160/1000 | Loss: 0.00268406
Iteration 161/1000 | Loss: 0.00247775
Iteration 162/1000 | Loss: 0.00294016
Iteration 163/1000 | Loss: 0.00256420
Iteration 164/1000 | Loss: 0.00312990
Iteration 165/1000 | Loss: 0.00277295
Iteration 166/1000 | Loss: 0.00262276
Iteration 167/1000 | Loss: 0.00216403
Iteration 168/1000 | Loss: 0.00223589
Iteration 169/1000 | Loss: 0.00265756
Iteration 170/1000 | Loss: 0.00264935
Iteration 171/1000 | Loss: 0.00316927
Iteration 172/1000 | Loss: 0.00175515
Iteration 173/1000 | Loss: 0.00209146
Iteration 174/1000 | Loss: 0.00201204
Iteration 175/1000 | Loss: 0.00207792
Iteration 176/1000 | Loss: 0.00215250
Iteration 177/1000 | Loss: 0.00253592
Iteration 178/1000 | Loss: 0.00252592
Iteration 179/1000 | Loss: 0.00159390
Iteration 180/1000 | Loss: 0.00164280
Iteration 181/1000 | Loss: 0.00155580
Iteration 182/1000 | Loss: 0.00199982
Iteration 183/1000 | Loss: 0.00181888
Iteration 184/1000 | Loss: 0.00297497
Iteration 185/1000 | Loss: 0.00186591
Iteration 186/1000 | Loss: 0.00183095
Iteration 187/1000 | Loss: 0.00152141
Iteration 188/1000 | Loss: 0.00140859
Iteration 189/1000 | Loss: 0.00201968
Iteration 190/1000 | Loss: 0.00162434
Iteration 191/1000 | Loss: 0.00158788
Iteration 192/1000 | Loss: 0.00180684
Iteration 193/1000 | Loss: 0.00151295
Iteration 194/1000 | Loss: 0.00197978
Iteration 195/1000 | Loss: 0.00211100
Iteration 196/1000 | Loss: 0.00148821
Iteration 197/1000 | Loss: 0.00169192
Iteration 198/1000 | Loss: 0.00162068
Iteration 199/1000 | Loss: 0.00161842
Iteration 200/1000 | Loss: 0.00184158
Iteration 201/1000 | Loss: 0.00176286
Iteration 202/1000 | Loss: 0.00166574
Iteration 203/1000 | Loss: 0.00161260
Iteration 204/1000 | Loss: 0.00208100
Iteration 205/1000 | Loss: 0.00172915
Iteration 206/1000 | Loss: 0.00179182
Iteration 207/1000 | Loss: 0.00213708
Iteration 208/1000 | Loss: 0.00258501
Iteration 209/1000 | Loss: 0.00201957
Iteration 210/1000 | Loss: 0.00251982
Iteration 211/1000 | Loss: 0.00167166
Iteration 212/1000 | Loss: 0.00138613
Iteration 213/1000 | Loss: 0.00184106
Iteration 214/1000 | Loss: 0.00151018
Iteration 215/1000 | Loss: 0.00198248
Iteration 216/1000 | Loss: 0.00158524
Iteration 217/1000 | Loss: 0.00154071
Iteration 218/1000 | Loss: 0.00183483
Iteration 219/1000 | Loss: 0.00157021
Iteration 220/1000 | Loss: 0.00160012
Iteration 221/1000 | Loss: 0.00147028
Iteration 222/1000 | Loss: 0.00134512
Iteration 223/1000 | Loss: 0.00134050
Iteration 224/1000 | Loss: 0.00139749
Iteration 225/1000 | Loss: 0.00141816
Iteration 226/1000 | Loss: 0.00161062
Iteration 227/1000 | Loss: 0.00162759
Iteration 228/1000 | Loss: 0.00163251
Iteration 229/1000 | Loss: 0.00144915
Iteration 230/1000 | Loss: 0.00167370
Iteration 231/1000 | Loss: 0.00148232
Iteration 232/1000 | Loss: 0.00142630
Iteration 233/1000 | Loss: 0.00139436
Iteration 234/1000 | Loss: 0.00171276
Iteration 235/1000 | Loss: 0.00225179
Iteration 236/1000 | Loss: 0.00233093
Iteration 237/1000 | Loss: 0.00233507
Iteration 238/1000 | Loss: 0.00203816
Iteration 239/1000 | Loss: 0.00127088
Iteration 240/1000 | Loss: 0.00160803
Iteration 241/1000 | Loss: 0.00130945
Iteration 242/1000 | Loss: 0.00175522
Iteration 243/1000 | Loss: 0.00352565
Iteration 244/1000 | Loss: 0.00171365
Iteration 245/1000 | Loss: 0.00140629
Iteration 246/1000 | Loss: 0.00142493
Iteration 247/1000 | Loss: 0.00245586
Iteration 248/1000 | Loss: 0.00175185
Iteration 249/1000 | Loss: 0.00194639
Iteration 250/1000 | Loss: 0.00169068
Iteration 251/1000 | Loss: 0.00177029
Iteration 252/1000 | Loss: 0.00145903
Iteration 253/1000 | Loss: 0.00181393
Iteration 254/1000 | Loss: 0.00210812
Iteration 255/1000 | Loss: 0.00196556
Iteration 256/1000 | Loss: 0.00202016
Iteration 257/1000 | Loss: 0.00320836
Iteration 258/1000 | Loss: 0.00428828
Iteration 259/1000 | Loss: 0.00222756
Iteration 260/1000 | Loss: 0.00144876
Iteration 261/1000 | Loss: 0.00148099
Iteration 262/1000 | Loss: 0.00159329
Iteration 263/1000 | Loss: 0.00181811
Iteration 264/1000 | Loss: 0.00149136
Iteration 265/1000 | Loss: 0.00191868
Iteration 266/1000 | Loss: 0.00172917
Iteration 267/1000 | Loss: 0.00178719
Iteration 268/1000 | Loss: 0.00268541
Iteration 269/1000 | Loss: 0.00330238
Iteration 270/1000 | Loss: 0.00171015
Iteration 271/1000 | Loss: 0.00120204
Iteration 272/1000 | Loss: 0.00144274
Iteration 273/1000 | Loss: 0.00143893
Iteration 274/1000 | Loss: 0.00150289
Iteration 275/1000 | Loss: 0.00259217
Iteration 276/1000 | Loss: 0.00267453
Iteration 277/1000 | Loss: 0.00300409
Iteration 278/1000 | Loss: 0.00329991
Iteration 279/1000 | Loss: 0.00207530
Iteration 280/1000 | Loss: 0.00227093
Iteration 281/1000 | Loss: 0.00161510
Iteration 282/1000 | Loss: 0.00133491
Iteration 283/1000 | Loss: 0.00133534
Iteration 284/1000 | Loss: 0.00138043
Iteration 285/1000 | Loss: 0.00137025
Iteration 286/1000 | Loss: 0.00099796
Iteration 287/1000 | Loss: 0.00117337
Iteration 288/1000 | Loss: 0.00213030
Iteration 289/1000 | Loss: 0.00210175
Iteration 290/1000 | Loss: 0.00089755
Iteration 291/1000 | Loss: 0.00109890
Iteration 292/1000 | Loss: 0.00100211
Iteration 293/1000 | Loss: 0.00151994
Iteration 294/1000 | Loss: 0.00146201
Iteration 295/1000 | Loss: 0.00131982
Iteration 296/1000 | Loss: 0.00266607
Iteration 297/1000 | Loss: 0.00121818
Iteration 298/1000 | Loss: 0.00158209
Iteration 299/1000 | Loss: 0.00127208
Iteration 300/1000 | Loss: 0.00097125
Iteration 301/1000 | Loss: 0.00115717
Iteration 302/1000 | Loss: 0.00118150
Iteration 303/1000 | Loss: 0.00179596
Iteration 304/1000 | Loss: 0.00142415
Iteration 305/1000 | Loss: 0.00137925
Iteration 306/1000 | Loss: 0.00119659
Iteration 307/1000 | Loss: 0.00159185
Iteration 308/1000 | Loss: 0.00170117
Iteration 309/1000 | Loss: 0.00415394
Iteration 310/1000 | Loss: 0.00119162
Iteration 311/1000 | Loss: 0.00076645
Iteration 312/1000 | Loss: 0.00131023
Iteration 313/1000 | Loss: 0.00080748
Iteration 314/1000 | Loss: 0.00113933
Iteration 315/1000 | Loss: 0.00097675
Iteration 316/1000 | Loss: 0.00107503
Iteration 317/1000 | Loss: 0.00089778
Iteration 318/1000 | Loss: 0.00089411
Iteration 319/1000 | Loss: 0.00098347
Iteration 320/1000 | Loss: 0.00115886
Iteration 321/1000 | Loss: 0.00086259
Iteration 322/1000 | Loss: 0.00077958
Iteration 323/1000 | Loss: 0.00086860
Iteration 324/1000 | Loss: 0.00168591
Iteration 325/1000 | Loss: 0.00117806
Iteration 326/1000 | Loss: 0.00117399
Iteration 327/1000 | Loss: 0.00100855
Iteration 328/1000 | Loss: 0.00082923
Iteration 329/1000 | Loss: 0.00073622
Iteration 330/1000 | Loss: 0.00087024
Iteration 331/1000 | Loss: 0.00127508
Iteration 332/1000 | Loss: 0.00115823
Iteration 333/1000 | Loss: 0.00124969
Iteration 334/1000 | Loss: 0.00100298
Iteration 335/1000 | Loss: 0.00104644
Iteration 336/1000 | Loss: 0.00095495
Iteration 337/1000 | Loss: 0.00108153
Iteration 338/1000 | Loss: 0.00104950
Iteration 339/1000 | Loss: 0.00083497
Iteration 340/1000 | Loss: 0.00077491
Iteration 341/1000 | Loss: 0.00112359
Iteration 342/1000 | Loss: 0.00109951
Iteration 343/1000 | Loss: 0.00086445
Iteration 344/1000 | Loss: 0.00101980
Iteration 345/1000 | Loss: 0.00099207
Iteration 346/1000 | Loss: 0.00101181
Iteration 347/1000 | Loss: 0.00086346
Iteration 348/1000 | Loss: 0.00092001
Iteration 349/1000 | Loss: 0.00125438
Iteration 350/1000 | Loss: 0.00075004
Iteration 351/1000 | Loss: 0.00080600
Iteration 352/1000 | Loss: 0.00059265
Iteration 353/1000 | Loss: 0.00079734
Iteration 354/1000 | Loss: 0.00084091
Iteration 355/1000 | Loss: 0.00115757
Iteration 356/1000 | Loss: 0.00069458
Iteration 357/1000 | Loss: 0.00138219
Iteration 358/1000 | Loss: 0.00090846
Iteration 359/1000 | Loss: 0.00062874
Iteration 360/1000 | Loss: 0.00091385
Iteration 361/1000 | Loss: 0.00113205
Iteration 362/1000 | Loss: 0.00093702
Iteration 363/1000 | Loss: 0.00147134
Iteration 364/1000 | Loss: 0.00160124
Iteration 365/1000 | Loss: 0.00088562
Iteration 366/1000 | Loss: 0.00070859
Iteration 367/1000 | Loss: 0.00045613
Iteration 368/1000 | Loss: 0.00077731
Iteration 369/1000 | Loss: 0.00080239
Iteration 370/1000 | Loss: 0.00076603
Iteration 371/1000 | Loss: 0.00083579
Iteration 372/1000 | Loss: 0.00092621
Iteration 373/1000 | Loss: 0.00088473
Iteration 374/1000 | Loss: 0.00092646
Iteration 375/1000 | Loss: 0.00090129
Iteration 376/1000 | Loss: 0.00092739
Iteration 377/1000 | Loss: 0.00093743
Iteration 378/1000 | Loss: 0.00094512
Iteration 379/1000 | Loss: 0.00046045
Iteration 380/1000 | Loss: 0.00072088
Iteration 381/1000 | Loss: 0.00052080
Iteration 382/1000 | Loss: 0.00081228
Iteration 383/1000 | Loss: 0.00106946
Iteration 384/1000 | Loss: 0.00070746
Iteration 385/1000 | Loss: 0.00064076
Iteration 386/1000 | Loss: 0.00056983
Iteration 387/1000 | Loss: 0.00049525
Iteration 388/1000 | Loss: 0.00056220
Iteration 389/1000 | Loss: 0.00052406
Iteration 390/1000 | Loss: 0.00064765
Iteration 391/1000 | Loss: 0.00061772
Iteration 392/1000 | Loss: 0.00057282
Iteration 393/1000 | Loss: 0.00047717
Iteration 394/1000 | Loss: 0.00052926
Iteration 395/1000 | Loss: 0.00038168
Iteration 396/1000 | Loss: 0.00062377
Iteration 397/1000 | Loss: 0.00146915
Iteration 398/1000 | Loss: 0.00083659
Iteration 399/1000 | Loss: 0.00040530
Iteration 400/1000 | Loss: 0.00044967
Iteration 401/1000 | Loss: 0.00027302
Iteration 402/1000 | Loss: 0.00020413
Iteration 403/1000 | Loss: 0.00022039
Iteration 404/1000 | Loss: 0.00023182
Iteration 405/1000 | Loss: 0.00045625
Iteration 406/1000 | Loss: 0.00028074
Iteration 407/1000 | Loss: 0.00022316
Iteration 408/1000 | Loss: 0.00023987
Iteration 409/1000 | Loss: 0.00036422
Iteration 410/1000 | Loss: 0.00020041
Iteration 411/1000 | Loss: 0.00017001
Iteration 412/1000 | Loss: 0.00025774
Iteration 413/1000 | Loss: 0.00026582
Iteration 414/1000 | Loss: 0.00029738
Iteration 415/1000 | Loss: 0.00035966
Iteration 416/1000 | Loss: 0.00035453
Iteration 417/1000 | Loss: 0.00066794
Iteration 418/1000 | Loss: 0.00050228
Iteration 419/1000 | Loss: 0.00027018
Iteration 420/1000 | Loss: 0.00031692
Iteration 421/1000 | Loss: 0.00020178
Iteration 422/1000 | Loss: 0.00019733
Iteration 423/1000 | Loss: 0.00015578
Iteration 424/1000 | Loss: 0.00013280
Iteration 425/1000 | Loss: 0.00024054
Iteration 426/1000 | Loss: 0.00022617
Iteration 427/1000 | Loss: 0.00028040
Iteration 428/1000 | Loss: 0.00027506
Iteration 429/1000 | Loss: 0.00028010
Iteration 430/1000 | Loss: 0.00026800
Iteration 431/1000 | Loss: 0.00031560
Iteration 432/1000 | Loss: 0.00027070
Iteration 433/1000 | Loss: 0.00028296
Iteration 434/1000 | Loss: 0.00028352
Iteration 435/1000 | Loss: 0.00031779
Iteration 436/1000 | Loss: 0.00027454
Iteration 437/1000 | Loss: 0.00030217
Iteration 438/1000 | Loss: 0.00027042
Iteration 439/1000 | Loss: 0.00041285
Iteration 440/1000 | Loss: 0.00029146
Iteration 441/1000 | Loss: 0.00031211
Iteration 442/1000 | Loss: 0.00029126
Iteration 443/1000 | Loss: 0.00015540
Iteration 444/1000 | Loss: 0.00014177
Iteration 445/1000 | Loss: 0.00012046
Iteration 446/1000 | Loss: 0.00012285
Iteration 447/1000 | Loss: 0.00016526
Iteration 448/1000 | Loss: 0.00007359
Iteration 449/1000 | Loss: 0.00015402
Iteration 450/1000 | Loss: 0.00011503
Iteration 451/1000 | Loss: 0.00011899
Iteration 452/1000 | Loss: 0.00005280
Iteration 453/1000 | Loss: 0.00015455
Iteration 454/1000 | Loss: 0.00008346
Iteration 455/1000 | Loss: 0.00005696
Iteration 456/1000 | Loss: 0.00005986
Iteration 457/1000 | Loss: 0.00005099
Iteration 458/1000 | Loss: 0.00006192
Iteration 459/1000 | Loss: 0.00008036
Iteration 460/1000 | Loss: 0.00008651
Iteration 461/1000 | Loss: 0.00007494
Iteration 462/1000 | Loss: 0.00008859
Iteration 463/1000 | Loss: 0.00010378
Iteration 464/1000 | Loss: 0.00012515
Iteration 465/1000 | Loss: 0.00011024
Iteration 466/1000 | Loss: 0.00012794
Iteration 467/1000 | Loss: 0.00010769
Iteration 468/1000 | Loss: 0.00012489
Iteration 469/1000 | Loss: 0.00010572
Iteration 470/1000 | Loss: 0.00011702
Iteration 471/1000 | Loss: 0.00015264
Iteration 472/1000 | Loss: 0.00016072
Iteration 473/1000 | Loss: 0.00011442
Iteration 474/1000 | Loss: 0.00012299
Iteration 475/1000 | Loss: 0.00010408
Iteration 476/1000 | Loss: 0.00010401
Iteration 477/1000 | Loss: 0.00013168
Iteration 478/1000 | Loss: 0.00010616
Iteration 479/1000 | Loss: 0.00012258
Iteration 480/1000 | Loss: 0.00009743
Iteration 481/1000 | Loss: 0.00024336
Iteration 482/1000 | Loss: 0.00010420
Iteration 483/1000 | Loss: 0.00010497
Iteration 484/1000 | Loss: 0.00010650
Iteration 485/1000 | Loss: 0.00017798
Iteration 486/1000 | Loss: 0.00004741
Iteration 487/1000 | Loss: 0.00018019
Iteration 488/1000 | Loss: 0.00020534
Iteration 489/1000 | Loss: 0.00014362
Iteration 490/1000 | Loss: 0.00012200
Iteration 491/1000 | Loss: 0.00011674
Iteration 492/1000 | Loss: 0.00010177
Iteration 493/1000 | Loss: 0.00011869
Iteration 494/1000 | Loss: 0.00013136
Iteration 495/1000 | Loss: 0.00012819
Iteration 496/1000 | Loss: 0.00013867
Iteration 497/1000 | Loss: 0.00015392
Iteration 498/1000 | Loss: 0.00018851
Iteration 499/1000 | Loss: 0.00007805
Iteration 500/1000 | Loss: 0.00009714
Iteration 501/1000 | Loss: 0.00023951
Iteration 502/1000 | Loss: 0.00006309
Iteration 503/1000 | Loss: 0.00003630
Iteration 504/1000 | Loss: 0.00004611
Iteration 505/1000 | Loss: 0.00004137
Iteration 506/1000 | Loss: 0.00004314
Iteration 507/1000 | Loss: 0.00004079
Iteration 508/1000 | Loss: 0.00004013
Iteration 509/1000 | Loss: 0.00002799
Iteration 510/1000 | Loss: 0.00003527
Iteration 511/1000 | Loss: 0.00004124
Iteration 512/1000 | Loss: 0.00004218
Iteration 513/1000 | Loss: 0.00011440
Iteration 514/1000 | Loss: 0.00008776
Iteration 515/1000 | Loss: 0.00004188
Iteration 516/1000 | Loss: 0.00007499
Iteration 517/1000 | Loss: 0.00004140
Iteration 518/1000 | Loss: 0.00003890
Iteration 519/1000 | Loss: 0.00004402
Iteration 520/1000 | Loss: 0.00004395
Iteration 521/1000 | Loss: 0.00004473
Iteration 522/1000 | Loss: 0.00004186
Iteration 523/1000 | Loss: 0.00004178
Iteration 524/1000 | Loss: 0.00004049
Iteration 525/1000 | Loss: 0.00004252
Iteration 526/1000 | Loss: 0.00004045
Iteration 527/1000 | Loss: 0.00004070
Iteration 528/1000 | Loss: 0.00004014
Iteration 529/1000 | Loss: 0.00004042
Iteration 530/1000 | Loss: 0.00004221
Iteration 531/1000 | Loss: 0.00005475
Iteration 532/1000 | Loss: 0.00004140
Iteration 533/1000 | Loss: 0.00003604
Iteration 534/1000 | Loss: 0.00005146
Iteration 535/1000 | Loss: 0.00003285
Iteration 536/1000 | Loss: 0.00004241
Iteration 537/1000 | Loss: 0.00003345
Iteration 538/1000 | Loss: 0.00004693
Iteration 539/1000 | Loss: 0.00004022
Iteration 540/1000 | Loss: 0.00003782
Iteration 541/1000 | Loss: 0.00008980
Iteration 542/1000 | Loss: 0.00005831
Iteration 543/1000 | Loss: 0.00004620
Iteration 544/1000 | Loss: 0.00003957
Iteration 545/1000 | Loss: 0.00003819
Iteration 546/1000 | Loss: 0.00004126
Iteration 547/1000 | Loss: 0.00006836
Iteration 548/1000 | Loss: 0.00004440
Iteration 549/1000 | Loss: 0.00006628
Iteration 550/1000 | Loss: 0.00005549
Iteration 551/1000 | Loss: 0.00007245
Iteration 552/1000 | Loss: 0.00004471
Iteration 553/1000 | Loss: 0.00003678
Iteration 554/1000 | Loss: 0.00005027
Iteration 555/1000 | Loss: 0.00016873
Iteration 556/1000 | Loss: 0.00054801
Iteration 557/1000 | Loss: 0.00004893
Iteration 558/1000 | Loss: 0.00003617
Iteration 559/1000 | Loss: 0.00007546
Iteration 560/1000 | Loss: 0.00014700
Iteration 561/1000 | Loss: 0.00007916
Iteration 562/1000 | Loss: 0.00006936
Iteration 563/1000 | Loss: 0.00006448
Iteration 564/1000 | Loss: 0.00003918
Iteration 565/1000 | Loss: 0.00004027
Iteration 566/1000 | Loss: 0.00004573
Iteration 567/1000 | Loss: 0.00004149
Iteration 568/1000 | Loss: 0.00003223
Iteration 569/1000 | Loss: 0.00005689
Iteration 570/1000 | Loss: 0.00004913
Iteration 571/1000 | Loss: 0.00004277
Iteration 572/1000 | Loss: 0.00002692
Iteration 573/1000 | Loss: 0.00003807
Iteration 574/1000 | Loss: 0.00005021
Iteration 575/1000 | Loss: 0.00003780
Iteration 576/1000 | Loss: 0.00004384
Iteration 577/1000 | Loss: 0.00003717
Iteration 578/1000 | Loss: 0.00004017
Iteration 579/1000 | Loss: 0.00003673
Iteration 580/1000 | Loss: 0.00003496
Iteration 581/1000 | Loss: 0.00003591
Iteration 582/1000 | Loss: 0.00004314
Iteration 583/1000 | Loss: 0.00003864
Iteration 584/1000 | Loss: 0.00003357
Iteration 585/1000 | Loss: 0.00004247
Iteration 586/1000 | Loss: 0.00003457
Iteration 587/1000 | Loss: 0.00004393
Iteration 588/1000 | Loss: 0.00003969
Iteration 589/1000 | Loss: 0.00003601
Iteration 590/1000 | Loss: 0.00003936
Iteration 591/1000 | Loss: 0.00003055
Iteration 592/1000 | Loss: 0.00003870
Iteration 593/1000 | Loss: 0.00003257
Iteration 594/1000 | Loss: 0.00003218
Iteration 595/1000 | Loss: 0.00004144
Iteration 596/1000 | Loss: 0.00003940
Iteration 597/1000 | Loss: 0.00003715
Iteration 598/1000 | Loss: 0.00003864
Iteration 599/1000 | Loss: 0.00003095
Iteration 600/1000 | Loss: 0.00003842
Iteration 601/1000 | Loss: 0.00004268
Iteration 602/1000 | Loss: 0.00002030
Iteration 603/1000 | Loss: 0.00003626
Iteration 604/1000 | Loss: 0.00002208
Iteration 605/1000 | Loss: 0.00001217
Iteration 606/1000 | Loss: 0.00001056
Iteration 607/1000 | Loss: 0.00000983
Iteration 608/1000 | Loss: 0.00000924
Iteration 609/1000 | Loss: 0.00000883
Iteration 610/1000 | Loss: 0.00000859
Iteration 611/1000 | Loss: 0.00000837
Iteration 612/1000 | Loss: 0.00000817
Iteration 613/1000 | Loss: 0.00000798
Iteration 614/1000 | Loss: 0.00000791
Iteration 615/1000 | Loss: 0.00000788
Iteration 616/1000 | Loss: 0.00000786
Iteration 617/1000 | Loss: 0.00000784
Iteration 618/1000 | Loss: 0.00000780
Iteration 619/1000 | Loss: 0.00000780
Iteration 620/1000 | Loss: 0.00000779
Iteration 621/1000 | Loss: 0.00000778
Iteration 622/1000 | Loss: 0.00000775
Iteration 623/1000 | Loss: 0.00000774
Iteration 624/1000 | Loss: 0.00000773
Iteration 625/1000 | Loss: 0.00000770
Iteration 626/1000 | Loss: 0.00000769
Iteration 627/1000 | Loss: 0.00000769
Iteration 628/1000 | Loss: 0.00000765
Iteration 629/1000 | Loss: 0.00000765
Iteration 630/1000 | Loss: 0.00000765
Iteration 631/1000 | Loss: 0.00000765
Iteration 632/1000 | Loss: 0.00000765
Iteration 633/1000 | Loss: 0.00000765
Iteration 634/1000 | Loss: 0.00000765
Iteration 635/1000 | Loss: 0.00000765
Iteration 636/1000 | Loss: 0.00000764
Iteration 637/1000 | Loss: 0.00000764
Iteration 638/1000 | Loss: 0.00000764
Iteration 639/1000 | Loss: 0.00000764
Iteration 640/1000 | Loss: 0.00000763
Iteration 641/1000 | Loss: 0.00000762
Iteration 642/1000 | Loss: 0.00000762
Iteration 643/1000 | Loss: 0.00000761
Iteration 644/1000 | Loss: 0.00000761
Iteration 645/1000 | Loss: 0.00000761
Iteration 646/1000 | Loss: 0.00000761
Iteration 647/1000 | Loss: 0.00000759
Iteration 648/1000 | Loss: 0.00000759
Iteration 649/1000 | Loss: 0.00000758
Iteration 650/1000 | Loss: 0.00000757
Iteration 651/1000 | Loss: 0.00000757
Iteration 652/1000 | Loss: 0.00000757
Iteration 653/1000 | Loss: 0.00000757
Iteration 654/1000 | Loss: 0.00000757
Iteration 655/1000 | Loss: 0.00000757
Iteration 656/1000 | Loss: 0.00000756
Iteration 657/1000 | Loss: 0.00000756
Iteration 658/1000 | Loss: 0.00000755
Iteration 659/1000 | Loss: 0.00000754
Iteration 660/1000 | Loss: 0.00000754
Iteration 661/1000 | Loss: 0.00000754
Iteration 662/1000 | Loss: 0.00000754
Iteration 663/1000 | Loss: 0.00000753
Iteration 664/1000 | Loss: 0.00000753
Iteration 665/1000 | Loss: 0.00000753
Iteration 666/1000 | Loss: 0.00000753
Iteration 667/1000 | Loss: 0.00000752
Iteration 668/1000 | Loss: 0.00000752
Iteration 669/1000 | Loss: 0.00000752
Iteration 670/1000 | Loss: 0.00000752
Iteration 671/1000 | Loss: 0.00000751
Iteration 672/1000 | Loss: 0.00000751
Iteration 673/1000 | Loss: 0.00000751
Iteration 674/1000 | Loss: 0.00000751
Iteration 675/1000 | Loss: 0.00000751
Iteration 676/1000 | Loss: 0.00000751
Iteration 677/1000 | Loss: 0.00000751
Iteration 678/1000 | Loss: 0.00000751
Iteration 679/1000 | Loss: 0.00000751
Iteration 680/1000 | Loss: 0.00000750
Iteration 681/1000 | Loss: 0.00000750
Iteration 682/1000 | Loss: 0.00000750
Iteration 683/1000 | Loss: 0.00000750
Iteration 684/1000 | Loss: 0.00000750
Iteration 685/1000 | Loss: 0.00000750
Iteration 686/1000 | Loss: 0.00000750
Iteration 687/1000 | Loss: 0.00000750
Iteration 688/1000 | Loss: 0.00000750
Iteration 689/1000 | Loss: 0.00000750
Iteration 690/1000 | Loss: 0.00000750
Iteration 691/1000 | Loss: 0.00000750
Iteration 692/1000 | Loss: 0.00000749
Iteration 693/1000 | Loss: 0.00000749
Iteration 694/1000 | Loss: 0.00000749
Iteration 695/1000 | Loss: 0.00000749
Iteration 696/1000 | Loss: 0.00000748
Iteration 697/1000 | Loss: 0.00000748
Iteration 698/1000 | Loss: 0.00000748
Iteration 699/1000 | Loss: 0.00000748
Iteration 700/1000 | Loss: 0.00000748
Iteration 701/1000 | Loss: 0.00000748
Iteration 702/1000 | Loss: 0.00000748
Iteration 703/1000 | Loss: 0.00000748
Iteration 704/1000 | Loss: 0.00000748
Iteration 705/1000 | Loss: 0.00000748
Iteration 706/1000 | Loss: 0.00000747
Iteration 707/1000 | Loss: 0.00000747
Iteration 708/1000 | Loss: 0.00000747
Iteration 709/1000 | Loss: 0.00000747
Iteration 710/1000 | Loss: 0.00000747
Iteration 711/1000 | Loss: 0.00000747
Iteration 712/1000 | Loss: 0.00000747
Iteration 713/1000 | Loss: 0.00000747
Iteration 714/1000 | Loss: 0.00000747
Iteration 715/1000 | Loss: 0.00000747
Iteration 716/1000 | Loss: 0.00000747
Iteration 717/1000 | Loss: 0.00000747
Iteration 718/1000 | Loss: 0.00000746
Iteration 719/1000 | Loss: 0.00000746
Iteration 720/1000 | Loss: 0.00000746
Iteration 721/1000 | Loss: 0.00000746
Iteration 722/1000 | Loss: 0.00000745
Iteration 723/1000 | Loss: 0.00000745
Iteration 724/1000 | Loss: 0.00000745
Iteration 725/1000 | Loss: 0.00000745
Iteration 726/1000 | Loss: 0.00000745
Iteration 727/1000 | Loss: 0.00000745
Iteration 728/1000 | Loss: 0.00000745
Iteration 729/1000 | Loss: 0.00000745
Iteration 730/1000 | Loss: 0.00000745
Iteration 731/1000 | Loss: 0.00000745
Iteration 732/1000 | Loss: 0.00000745
Iteration 733/1000 | Loss: 0.00000745
Iteration 734/1000 | Loss: 0.00000745
Iteration 735/1000 | Loss: 0.00000745
Iteration 736/1000 | Loss: 0.00000745
Iteration 737/1000 | Loss: 0.00000745
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 737. Stopping optimization.
Last 5 losses: [7.449139502568869e-06, 7.449139502568869e-06, 7.449139502568869e-06, 7.449139502568869e-06, 7.449139502568869e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 7.449139502568869e-06

Optimization complete. Final v2v error: 2.3871963024139404 mm

Highest mean error: 3.3097212314605713 mm for frame 68

Lowest mean error: 2.2717466354370117 mm for frame 5

Saving results

Total time: 973.5760271549225
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_025/1019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_025/1019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_025/1019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00429586
Iteration 2/25 | Loss: 0.00135205
Iteration 3/25 | Loss: 0.00124147
Iteration 4/25 | Loss: 0.00122723
Iteration 5/25 | Loss: 0.00122417
Iteration 6/25 | Loss: 0.00122342
Iteration 7/25 | Loss: 0.00122342
Iteration 8/25 | Loss: 0.00122342
Iteration 9/25 | Loss: 0.00122342
Iteration 10/25 | Loss: 0.00122342
Iteration 11/25 | Loss: 0.00122342
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012234157184138894, 0.0012234157184138894, 0.0012234157184138894, 0.0012234157184138894, 0.0012234157184138894]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012234157184138894

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.43704700
Iteration 2/25 | Loss: 0.00143364
Iteration 3/25 | Loss: 0.00143364
Iteration 4/25 | Loss: 0.00143363
Iteration 5/25 | Loss: 0.00143363
Iteration 6/25 | Loss: 0.00143363
Iteration 7/25 | Loss: 0.00143363
Iteration 8/25 | Loss: 0.00143363
Iteration 9/25 | Loss: 0.00143363
Iteration 10/25 | Loss: 0.00143363
Iteration 11/25 | Loss: 0.00143363
Iteration 12/25 | Loss: 0.00143363
Iteration 13/25 | Loss: 0.00143363
Iteration 14/25 | Loss: 0.00143363
Iteration 15/25 | Loss: 0.00143363
Iteration 16/25 | Loss: 0.00143363
Iteration 17/25 | Loss: 0.00143363
Iteration 18/25 | Loss: 0.00143363
Iteration 19/25 | Loss: 0.00143363
Iteration 20/25 | Loss: 0.00143363
Iteration 21/25 | Loss: 0.00143363
Iteration 22/25 | Loss: 0.00143363
Iteration 23/25 | Loss: 0.00143363
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0014336317544803023, 0.0014336317544803023, 0.0014336317544803023, 0.0014336317544803023, 0.0014336317544803023]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014336317544803023

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00143363
Iteration 2/1000 | Loss: 0.00002738
Iteration 3/1000 | Loss: 0.00001979
Iteration 4/1000 | Loss: 0.00001630
Iteration 5/1000 | Loss: 0.00001518
Iteration 6/1000 | Loss: 0.00001444
Iteration 7/1000 | Loss: 0.00001391
Iteration 8/1000 | Loss: 0.00001344
Iteration 9/1000 | Loss: 0.00001316
Iteration 10/1000 | Loss: 0.00001282
Iteration 11/1000 | Loss: 0.00001252
Iteration 12/1000 | Loss: 0.00001237
Iteration 13/1000 | Loss: 0.00001236
Iteration 14/1000 | Loss: 0.00001231
Iteration 15/1000 | Loss: 0.00001220
Iteration 16/1000 | Loss: 0.00001214
Iteration 17/1000 | Loss: 0.00001205
Iteration 18/1000 | Loss: 0.00001205
Iteration 19/1000 | Loss: 0.00001203
Iteration 20/1000 | Loss: 0.00001199
Iteration 21/1000 | Loss: 0.00001197
Iteration 22/1000 | Loss: 0.00001197
Iteration 23/1000 | Loss: 0.00001196
Iteration 24/1000 | Loss: 0.00001196
Iteration 25/1000 | Loss: 0.00001195
Iteration 26/1000 | Loss: 0.00001194
Iteration 27/1000 | Loss: 0.00001191
Iteration 28/1000 | Loss: 0.00001191
Iteration 29/1000 | Loss: 0.00001189
Iteration 30/1000 | Loss: 0.00001189
Iteration 31/1000 | Loss: 0.00001188
Iteration 32/1000 | Loss: 0.00001185
Iteration 33/1000 | Loss: 0.00001184
Iteration 34/1000 | Loss: 0.00001184
Iteration 35/1000 | Loss: 0.00001183
Iteration 36/1000 | Loss: 0.00001183
Iteration 37/1000 | Loss: 0.00001178
Iteration 38/1000 | Loss: 0.00001178
Iteration 39/1000 | Loss: 0.00001178
Iteration 40/1000 | Loss: 0.00001178
Iteration 41/1000 | Loss: 0.00001178
Iteration 42/1000 | Loss: 0.00001175
Iteration 43/1000 | Loss: 0.00001175
Iteration 44/1000 | Loss: 0.00001174
Iteration 45/1000 | Loss: 0.00001174
Iteration 46/1000 | Loss: 0.00001173
Iteration 47/1000 | Loss: 0.00001173
Iteration 48/1000 | Loss: 0.00001170
Iteration 49/1000 | Loss: 0.00001170
Iteration 50/1000 | Loss: 0.00001167
Iteration 51/1000 | Loss: 0.00001167
Iteration 52/1000 | Loss: 0.00001167
Iteration 53/1000 | Loss: 0.00001167
Iteration 54/1000 | Loss: 0.00001166
Iteration 55/1000 | Loss: 0.00001166
Iteration 56/1000 | Loss: 0.00001166
Iteration 57/1000 | Loss: 0.00001166
Iteration 58/1000 | Loss: 0.00001166
Iteration 59/1000 | Loss: 0.00001164
Iteration 60/1000 | Loss: 0.00001163
Iteration 61/1000 | Loss: 0.00001163
Iteration 62/1000 | Loss: 0.00001163
Iteration 63/1000 | Loss: 0.00001162
Iteration 64/1000 | Loss: 0.00001162
Iteration 65/1000 | Loss: 0.00001161
Iteration 66/1000 | Loss: 0.00001161
Iteration 67/1000 | Loss: 0.00001160
Iteration 68/1000 | Loss: 0.00001160
Iteration 69/1000 | Loss: 0.00001160
Iteration 70/1000 | Loss: 0.00001159
Iteration 71/1000 | Loss: 0.00001159
Iteration 72/1000 | Loss: 0.00001158
Iteration 73/1000 | Loss: 0.00001158
Iteration 74/1000 | Loss: 0.00001158
Iteration 75/1000 | Loss: 0.00001157
Iteration 76/1000 | Loss: 0.00001157
Iteration 77/1000 | Loss: 0.00001157
Iteration 78/1000 | Loss: 0.00001157
Iteration 79/1000 | Loss: 0.00001157
Iteration 80/1000 | Loss: 0.00001156
Iteration 81/1000 | Loss: 0.00001156
Iteration 82/1000 | Loss: 0.00001156
Iteration 83/1000 | Loss: 0.00001155
Iteration 84/1000 | Loss: 0.00001155
Iteration 85/1000 | Loss: 0.00001154
Iteration 86/1000 | Loss: 0.00001154
Iteration 87/1000 | Loss: 0.00001154
Iteration 88/1000 | Loss: 0.00001154
Iteration 89/1000 | Loss: 0.00001153
Iteration 90/1000 | Loss: 0.00001153
Iteration 91/1000 | Loss: 0.00001153
Iteration 92/1000 | Loss: 0.00001152
Iteration 93/1000 | Loss: 0.00001152
Iteration 94/1000 | Loss: 0.00001152
Iteration 95/1000 | Loss: 0.00001152
Iteration 96/1000 | Loss: 0.00001151
Iteration 97/1000 | Loss: 0.00001151
Iteration 98/1000 | Loss: 0.00001151
Iteration 99/1000 | Loss: 0.00001151
Iteration 100/1000 | Loss: 0.00001150
Iteration 101/1000 | Loss: 0.00001150
Iteration 102/1000 | Loss: 0.00001150
Iteration 103/1000 | Loss: 0.00001149
Iteration 104/1000 | Loss: 0.00001149
Iteration 105/1000 | Loss: 0.00001149
Iteration 106/1000 | Loss: 0.00001149
Iteration 107/1000 | Loss: 0.00001149
Iteration 108/1000 | Loss: 0.00001149
Iteration 109/1000 | Loss: 0.00001149
Iteration 110/1000 | Loss: 0.00001149
Iteration 111/1000 | Loss: 0.00001148
Iteration 112/1000 | Loss: 0.00001148
Iteration 113/1000 | Loss: 0.00001148
Iteration 114/1000 | Loss: 0.00001148
Iteration 115/1000 | Loss: 0.00001148
Iteration 116/1000 | Loss: 0.00001148
Iteration 117/1000 | Loss: 0.00001148
Iteration 118/1000 | Loss: 0.00001148
Iteration 119/1000 | Loss: 0.00001148
Iteration 120/1000 | Loss: 0.00001148
Iteration 121/1000 | Loss: 0.00001148
Iteration 122/1000 | Loss: 0.00001148
Iteration 123/1000 | Loss: 0.00001148
Iteration 124/1000 | Loss: 0.00001148
Iteration 125/1000 | Loss: 0.00001148
Iteration 126/1000 | Loss: 0.00001147
Iteration 127/1000 | Loss: 0.00001147
Iteration 128/1000 | Loss: 0.00001147
Iteration 129/1000 | Loss: 0.00001147
Iteration 130/1000 | Loss: 0.00001147
Iteration 131/1000 | Loss: 0.00001147
Iteration 132/1000 | Loss: 0.00001147
Iteration 133/1000 | Loss: 0.00001147
Iteration 134/1000 | Loss: 0.00001146
Iteration 135/1000 | Loss: 0.00001146
Iteration 136/1000 | Loss: 0.00001146
Iteration 137/1000 | Loss: 0.00001146
Iteration 138/1000 | Loss: 0.00001146
Iteration 139/1000 | Loss: 0.00001146
Iteration 140/1000 | Loss: 0.00001146
Iteration 141/1000 | Loss: 0.00001146
Iteration 142/1000 | Loss: 0.00001146
Iteration 143/1000 | Loss: 0.00001146
Iteration 144/1000 | Loss: 0.00001145
Iteration 145/1000 | Loss: 0.00001145
Iteration 146/1000 | Loss: 0.00001145
Iteration 147/1000 | Loss: 0.00001144
Iteration 148/1000 | Loss: 0.00001144
Iteration 149/1000 | Loss: 0.00001144
Iteration 150/1000 | Loss: 0.00001144
Iteration 151/1000 | Loss: 0.00001144
Iteration 152/1000 | Loss: 0.00001144
Iteration 153/1000 | Loss: 0.00001144
Iteration 154/1000 | Loss: 0.00001144
Iteration 155/1000 | Loss: 0.00001144
Iteration 156/1000 | Loss: 0.00001144
Iteration 157/1000 | Loss: 0.00001144
Iteration 158/1000 | Loss: 0.00001144
Iteration 159/1000 | Loss: 0.00001144
Iteration 160/1000 | Loss: 0.00001143
Iteration 161/1000 | Loss: 0.00001143
Iteration 162/1000 | Loss: 0.00001143
Iteration 163/1000 | Loss: 0.00001143
Iteration 164/1000 | Loss: 0.00001143
Iteration 165/1000 | Loss: 0.00001143
Iteration 166/1000 | Loss: 0.00001143
Iteration 167/1000 | Loss: 0.00001143
Iteration 168/1000 | Loss: 0.00001143
Iteration 169/1000 | Loss: 0.00001143
Iteration 170/1000 | Loss: 0.00001143
Iteration 171/1000 | Loss: 0.00001143
Iteration 172/1000 | Loss: 0.00001143
Iteration 173/1000 | Loss: 0.00001143
Iteration 174/1000 | Loss: 0.00001143
Iteration 175/1000 | Loss: 0.00001143
Iteration 176/1000 | Loss: 0.00001143
Iteration 177/1000 | Loss: 0.00001143
Iteration 178/1000 | Loss: 0.00001143
Iteration 179/1000 | Loss: 0.00001143
Iteration 180/1000 | Loss: 0.00001143
Iteration 181/1000 | Loss: 0.00001143
Iteration 182/1000 | Loss: 0.00001143
Iteration 183/1000 | Loss: 0.00001143
Iteration 184/1000 | Loss: 0.00001143
Iteration 185/1000 | Loss: 0.00001143
Iteration 186/1000 | Loss: 0.00001143
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 186. Stopping optimization.
Last 5 losses: [1.1433081454015337e-05, 1.1433081454015337e-05, 1.1433081454015337e-05, 1.1433081454015337e-05, 1.1433081454015337e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1433081454015337e-05

Optimization complete. Final v2v error: 2.916532278060913 mm

Highest mean error: 3.6123931407928467 mm for frame 89

Lowest mean error: 2.5954091548919678 mm for frame 40

Saving results

Total time: 42.4050452709198
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_025/1009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_025/1009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_025/1009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01052651
Iteration 2/25 | Loss: 0.01052651
Iteration 3/25 | Loss: 0.00181869
Iteration 4/25 | Loss: 0.00140951
Iteration 5/25 | Loss: 0.00130844
Iteration 6/25 | Loss: 0.00128828
Iteration 7/25 | Loss: 0.00130539
Iteration 8/25 | Loss: 0.00130535
Iteration 9/25 | Loss: 0.00129041
Iteration 10/25 | Loss: 0.00128143
Iteration 11/25 | Loss: 0.00126215
Iteration 12/25 | Loss: 0.00125310
Iteration 13/25 | Loss: 0.00125267
Iteration 14/25 | Loss: 0.00124633
Iteration 15/25 | Loss: 0.00124614
Iteration 16/25 | Loss: 0.00124160
Iteration 17/25 | Loss: 0.00124040
Iteration 18/25 | Loss: 0.00123951
Iteration 19/25 | Loss: 0.00123916
Iteration 20/25 | Loss: 0.00124258
Iteration 21/25 | Loss: 0.00124161
Iteration 22/25 | Loss: 0.00123991
Iteration 23/25 | Loss: 0.00124062
Iteration 24/25 | Loss: 0.00124044
Iteration 25/25 | Loss: 0.00123876

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 6.39686489
Iteration 2/25 | Loss: 0.00155565
Iteration 3/25 | Loss: 0.00153988
Iteration 4/25 | Loss: 0.00153988
Iteration 5/25 | Loss: 0.00153988
Iteration 6/25 | Loss: 0.00153988
Iteration 7/25 | Loss: 0.00153988
Iteration 8/25 | Loss: 0.00153988
Iteration 9/25 | Loss: 0.00153988
Iteration 10/25 | Loss: 0.00153988
Iteration 11/25 | Loss: 0.00153988
Iteration 12/25 | Loss: 0.00153988
Iteration 13/25 | Loss: 0.00153988
Iteration 14/25 | Loss: 0.00153988
Iteration 15/25 | Loss: 0.00153988
Iteration 16/25 | Loss: 0.00153988
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.001539875054731965, 0.001539875054731965, 0.001539875054731965, 0.001539875054731965, 0.001539875054731965]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001539875054731965

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00153988
Iteration 2/1000 | Loss: 0.00021944
Iteration 3/1000 | Loss: 0.00020741
Iteration 4/1000 | Loss: 0.00002907
Iteration 5/1000 | Loss: 0.00016005
Iteration 6/1000 | Loss: 0.00016361
Iteration 7/1000 | Loss: 0.00014862
Iteration 8/1000 | Loss: 0.00001993
Iteration 9/1000 | Loss: 0.00017947
Iteration 10/1000 | Loss: 0.00002735
Iteration 11/1000 | Loss: 0.00004344
Iteration 12/1000 | Loss: 0.00002025
Iteration 13/1000 | Loss: 0.00016228
Iteration 14/1000 | Loss: 0.00014728
Iteration 15/1000 | Loss: 0.00015728
Iteration 16/1000 | Loss: 0.00014371
Iteration 17/1000 | Loss: 0.00017774
Iteration 18/1000 | Loss: 0.00010456
Iteration 19/1000 | Loss: 0.00010592
Iteration 20/1000 | Loss: 0.00009040
Iteration 21/1000 | Loss: 0.00012307
Iteration 22/1000 | Loss: 0.00007905
Iteration 23/1000 | Loss: 0.00011961
Iteration 24/1000 | Loss: 0.00010848
Iteration 25/1000 | Loss: 0.00010927
Iteration 26/1000 | Loss: 0.00010774
Iteration 27/1000 | Loss: 0.00011410
Iteration 28/1000 | Loss: 0.00003085
Iteration 29/1000 | Loss: 0.00009375
Iteration 30/1000 | Loss: 0.00007370
Iteration 31/1000 | Loss: 0.00004513
Iteration 32/1000 | Loss: 0.00002002
Iteration 33/1000 | Loss: 0.00001928
Iteration 34/1000 | Loss: 0.00012119
Iteration 35/1000 | Loss: 0.00002410
Iteration 36/1000 | Loss: 0.00010073
Iteration 37/1000 | Loss: 0.00005524
Iteration 38/1000 | Loss: 0.00002057
Iteration 39/1000 | Loss: 0.00001953
Iteration 40/1000 | Loss: 0.00001879
Iteration 41/1000 | Loss: 0.00004695
Iteration 42/1000 | Loss: 0.00013653
Iteration 43/1000 | Loss: 0.00021157
Iteration 44/1000 | Loss: 0.00018368
Iteration 45/1000 | Loss: 0.00016525
Iteration 46/1000 | Loss: 0.00011489
Iteration 47/1000 | Loss: 0.00005460
Iteration 48/1000 | Loss: 0.00011429
Iteration 49/1000 | Loss: 0.00010800
Iteration 50/1000 | Loss: 0.00011865
Iteration 51/1000 | Loss: 0.00002030
Iteration 52/1000 | Loss: 0.00002024
Iteration 53/1000 | Loss: 0.00001701
Iteration 54/1000 | Loss: 0.00001599
Iteration 55/1000 | Loss: 0.00001533
Iteration 56/1000 | Loss: 0.00001480
Iteration 57/1000 | Loss: 0.00001451
Iteration 58/1000 | Loss: 0.00001428
Iteration 59/1000 | Loss: 0.00001532
Iteration 60/1000 | Loss: 0.00001446
Iteration 61/1000 | Loss: 0.00001391
Iteration 62/1000 | Loss: 0.00001421
Iteration 63/1000 | Loss: 0.00001384
Iteration 64/1000 | Loss: 0.00001384
Iteration 65/1000 | Loss: 0.00001384
Iteration 66/1000 | Loss: 0.00001384
Iteration 67/1000 | Loss: 0.00001384
Iteration 68/1000 | Loss: 0.00001384
Iteration 69/1000 | Loss: 0.00001384
Iteration 70/1000 | Loss: 0.00001384
Iteration 71/1000 | Loss: 0.00001384
Iteration 72/1000 | Loss: 0.00001383
Iteration 73/1000 | Loss: 0.00001381
Iteration 74/1000 | Loss: 0.00001380
Iteration 75/1000 | Loss: 0.00001379
Iteration 76/1000 | Loss: 0.00001378
Iteration 77/1000 | Loss: 0.00001519
Iteration 78/1000 | Loss: 0.00001519
Iteration 79/1000 | Loss: 0.00001384
Iteration 80/1000 | Loss: 0.00001369
Iteration 81/1000 | Loss: 0.00001368
Iteration 82/1000 | Loss: 0.00001368
Iteration 83/1000 | Loss: 0.00001367
Iteration 84/1000 | Loss: 0.00001367
Iteration 85/1000 | Loss: 0.00001367
Iteration 86/1000 | Loss: 0.00001367
Iteration 87/1000 | Loss: 0.00001367
Iteration 88/1000 | Loss: 0.00001367
Iteration 89/1000 | Loss: 0.00001367
Iteration 90/1000 | Loss: 0.00001366
Iteration 91/1000 | Loss: 0.00001366
Iteration 92/1000 | Loss: 0.00001366
Iteration 93/1000 | Loss: 0.00001366
Iteration 94/1000 | Loss: 0.00001366
Iteration 95/1000 | Loss: 0.00001366
Iteration 96/1000 | Loss: 0.00001366
Iteration 97/1000 | Loss: 0.00001366
Iteration 98/1000 | Loss: 0.00001366
Iteration 99/1000 | Loss: 0.00001366
Iteration 100/1000 | Loss: 0.00001366
Iteration 101/1000 | Loss: 0.00001366
Iteration 102/1000 | Loss: 0.00001366
Iteration 103/1000 | Loss: 0.00001366
Iteration 104/1000 | Loss: 0.00001366
Iteration 105/1000 | Loss: 0.00001366
Iteration 106/1000 | Loss: 0.00001365
Iteration 107/1000 | Loss: 0.00001365
Iteration 108/1000 | Loss: 0.00001365
Iteration 109/1000 | Loss: 0.00001365
Iteration 110/1000 | Loss: 0.00001365
Iteration 111/1000 | Loss: 0.00001365
Iteration 112/1000 | Loss: 0.00001364
Iteration 113/1000 | Loss: 0.00001364
Iteration 114/1000 | Loss: 0.00001364
Iteration 115/1000 | Loss: 0.00001364
Iteration 116/1000 | Loss: 0.00001364
Iteration 117/1000 | Loss: 0.00001364
Iteration 118/1000 | Loss: 0.00001364
Iteration 119/1000 | Loss: 0.00001364
Iteration 120/1000 | Loss: 0.00001364
Iteration 121/1000 | Loss: 0.00001364
Iteration 122/1000 | Loss: 0.00001363
Iteration 123/1000 | Loss: 0.00001363
Iteration 124/1000 | Loss: 0.00001363
Iteration 125/1000 | Loss: 0.00001363
Iteration 126/1000 | Loss: 0.00001363
Iteration 127/1000 | Loss: 0.00001363
Iteration 128/1000 | Loss: 0.00001363
Iteration 129/1000 | Loss: 0.00001363
Iteration 130/1000 | Loss: 0.00001363
Iteration 131/1000 | Loss: 0.00001363
Iteration 132/1000 | Loss: 0.00001363
Iteration 133/1000 | Loss: 0.00001363
Iteration 134/1000 | Loss: 0.00001363
Iteration 135/1000 | Loss: 0.00001363
Iteration 136/1000 | Loss: 0.00001363
Iteration 137/1000 | Loss: 0.00001362
Iteration 138/1000 | Loss: 0.00001362
Iteration 139/1000 | Loss: 0.00001362
Iteration 140/1000 | Loss: 0.00001362
Iteration 141/1000 | Loss: 0.00001362
Iteration 142/1000 | Loss: 0.00001362
Iteration 143/1000 | Loss: 0.00001362
Iteration 144/1000 | Loss: 0.00001362
Iteration 145/1000 | Loss: 0.00001362
Iteration 146/1000 | Loss: 0.00001361
Iteration 147/1000 | Loss: 0.00001361
Iteration 148/1000 | Loss: 0.00001361
Iteration 149/1000 | Loss: 0.00001361
Iteration 150/1000 | Loss: 0.00001361
Iteration 151/1000 | Loss: 0.00001361
Iteration 152/1000 | Loss: 0.00001361
Iteration 153/1000 | Loss: 0.00001361
Iteration 154/1000 | Loss: 0.00001361
Iteration 155/1000 | Loss: 0.00001361
Iteration 156/1000 | Loss: 0.00001361
Iteration 157/1000 | Loss: 0.00001361
Iteration 158/1000 | Loss: 0.00001361
Iteration 159/1000 | Loss: 0.00001361
Iteration 160/1000 | Loss: 0.00001361
Iteration 161/1000 | Loss: 0.00001742
Iteration 162/1000 | Loss: 0.00001361
Iteration 163/1000 | Loss: 0.00001361
Iteration 164/1000 | Loss: 0.00001361
Iteration 165/1000 | Loss: 0.00001361
Iteration 166/1000 | Loss: 0.00001361
Iteration 167/1000 | Loss: 0.00001361
Iteration 168/1000 | Loss: 0.00001361
Iteration 169/1000 | Loss: 0.00001361
Iteration 170/1000 | Loss: 0.00001360
Iteration 171/1000 | Loss: 0.00001360
Iteration 172/1000 | Loss: 0.00001360
Iteration 173/1000 | Loss: 0.00001360
Iteration 174/1000 | Loss: 0.00001360
Iteration 175/1000 | Loss: 0.00001360
Iteration 176/1000 | Loss: 0.00001360
Iteration 177/1000 | Loss: 0.00001360
Iteration 178/1000 | Loss: 0.00001360
Iteration 179/1000 | Loss: 0.00001360
Iteration 180/1000 | Loss: 0.00001360
Iteration 181/1000 | Loss: 0.00001360
Iteration 182/1000 | Loss: 0.00001360
Iteration 183/1000 | Loss: 0.00001360
Iteration 184/1000 | Loss: 0.00001360
Iteration 185/1000 | Loss: 0.00001360
Iteration 186/1000 | Loss: 0.00001360
Iteration 187/1000 | Loss: 0.00001360
Iteration 188/1000 | Loss: 0.00001360
Iteration 189/1000 | Loss: 0.00001360
Iteration 190/1000 | Loss: 0.00001360
Iteration 191/1000 | Loss: 0.00001360
Iteration 192/1000 | Loss: 0.00001360
Iteration 193/1000 | Loss: 0.00001360
Iteration 194/1000 | Loss: 0.00001360
Iteration 195/1000 | Loss: 0.00001360
Iteration 196/1000 | Loss: 0.00001360
Iteration 197/1000 | Loss: 0.00001360
Iteration 198/1000 | Loss: 0.00001360
Iteration 199/1000 | Loss: 0.00001360
Iteration 200/1000 | Loss: 0.00001360
Iteration 201/1000 | Loss: 0.00001360
Iteration 202/1000 | Loss: 0.00001360
Iteration 203/1000 | Loss: 0.00001360
Iteration 204/1000 | Loss: 0.00001360
Iteration 205/1000 | Loss: 0.00001360
Iteration 206/1000 | Loss: 0.00001360
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 206. Stopping optimization.
Last 5 losses: [1.3597799807030242e-05, 1.3597799807030242e-05, 1.3597799807030242e-05, 1.3597799807030242e-05, 1.3597799807030242e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3597799807030242e-05

Optimization complete. Final v2v error: 3.1731321811676025 mm

Highest mean error: 4.047086715698242 mm for frame 157

Lowest mean error: 2.697739839553833 mm for frame 208

Saving results

Total time: 164.6896390914917
