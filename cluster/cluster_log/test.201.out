Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=201, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 11256-11311
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_023/1076/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_023/1076.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_023/1076
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00466618
Iteration 2/25 | Loss: 0.00132615
Iteration 3/25 | Loss: 0.00116843
Iteration 4/25 | Loss: 0.00115297
Iteration 5/25 | Loss: 0.00115072
Iteration 6/25 | Loss: 0.00115044
Iteration 7/25 | Loss: 0.00115044
Iteration 8/25 | Loss: 0.00115044
Iteration 9/25 | Loss: 0.00115044
Iteration 10/25 | Loss: 0.00115044
Iteration 11/25 | Loss: 0.00115044
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011504361173138022, 0.0011504361173138022, 0.0011504361173138022, 0.0011504361173138022, 0.0011504361173138022]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011504361173138022

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36519587
Iteration 2/25 | Loss: 0.00075343
Iteration 3/25 | Loss: 0.00075343
Iteration 4/25 | Loss: 0.00075343
Iteration 5/25 | Loss: 0.00075343
Iteration 6/25 | Loss: 0.00075342
Iteration 7/25 | Loss: 0.00075342
Iteration 8/25 | Loss: 0.00075342
Iteration 9/25 | Loss: 0.00075342
Iteration 10/25 | Loss: 0.00075342
Iteration 11/25 | Loss: 0.00075342
Iteration 12/25 | Loss: 0.00075342
Iteration 13/25 | Loss: 0.00075342
Iteration 14/25 | Loss: 0.00075342
Iteration 15/25 | Loss: 0.00075342
Iteration 16/25 | Loss: 0.00075342
Iteration 17/25 | Loss: 0.00075342
Iteration 18/25 | Loss: 0.00075342
Iteration 19/25 | Loss: 0.00075342
Iteration 20/25 | Loss: 0.00075342
Iteration 21/25 | Loss: 0.00075342
Iteration 22/25 | Loss: 0.00075342
Iteration 23/25 | Loss: 0.00075342
Iteration 24/25 | Loss: 0.00075342
Iteration 25/25 | Loss: 0.00075342

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00075342
Iteration 2/1000 | Loss: 0.00002603
Iteration 3/1000 | Loss: 0.00001724
Iteration 4/1000 | Loss: 0.00001449
Iteration 5/1000 | Loss: 0.00001331
Iteration 6/1000 | Loss: 0.00001268
Iteration 7/1000 | Loss: 0.00001223
Iteration 8/1000 | Loss: 0.00001184
Iteration 9/1000 | Loss: 0.00001164
Iteration 10/1000 | Loss: 0.00001140
Iteration 11/1000 | Loss: 0.00001134
Iteration 12/1000 | Loss: 0.00001129
Iteration 13/1000 | Loss: 0.00001127
Iteration 14/1000 | Loss: 0.00001126
Iteration 15/1000 | Loss: 0.00001123
Iteration 16/1000 | Loss: 0.00001122
Iteration 17/1000 | Loss: 0.00001122
Iteration 18/1000 | Loss: 0.00001122
Iteration 19/1000 | Loss: 0.00001121
Iteration 20/1000 | Loss: 0.00001116
Iteration 21/1000 | Loss: 0.00001115
Iteration 22/1000 | Loss: 0.00001112
Iteration 23/1000 | Loss: 0.00001111
Iteration 24/1000 | Loss: 0.00001110
Iteration 25/1000 | Loss: 0.00001108
Iteration 26/1000 | Loss: 0.00001107
Iteration 27/1000 | Loss: 0.00001106
Iteration 28/1000 | Loss: 0.00001105
Iteration 29/1000 | Loss: 0.00001105
Iteration 30/1000 | Loss: 0.00001104
Iteration 31/1000 | Loss: 0.00001099
Iteration 32/1000 | Loss: 0.00001099
Iteration 33/1000 | Loss: 0.00001098
Iteration 34/1000 | Loss: 0.00001098
Iteration 35/1000 | Loss: 0.00001097
Iteration 36/1000 | Loss: 0.00001097
Iteration 37/1000 | Loss: 0.00001097
Iteration 38/1000 | Loss: 0.00001096
Iteration 39/1000 | Loss: 0.00001096
Iteration 40/1000 | Loss: 0.00001096
Iteration 41/1000 | Loss: 0.00001096
Iteration 42/1000 | Loss: 0.00001096
Iteration 43/1000 | Loss: 0.00001096
Iteration 44/1000 | Loss: 0.00001096
Iteration 45/1000 | Loss: 0.00001095
Iteration 46/1000 | Loss: 0.00001095
Iteration 47/1000 | Loss: 0.00001095
Iteration 48/1000 | Loss: 0.00001095
Iteration 49/1000 | Loss: 0.00001094
Iteration 50/1000 | Loss: 0.00001094
Iteration 51/1000 | Loss: 0.00001093
Iteration 52/1000 | Loss: 0.00001093
Iteration 53/1000 | Loss: 0.00001093
Iteration 54/1000 | Loss: 0.00001092
Iteration 55/1000 | Loss: 0.00001092
Iteration 56/1000 | Loss: 0.00001092
Iteration 57/1000 | Loss: 0.00001091
Iteration 58/1000 | Loss: 0.00001091
Iteration 59/1000 | Loss: 0.00001091
Iteration 60/1000 | Loss: 0.00001091
Iteration 61/1000 | Loss: 0.00001091
Iteration 62/1000 | Loss: 0.00001090
Iteration 63/1000 | Loss: 0.00001090
Iteration 64/1000 | Loss: 0.00001090
Iteration 65/1000 | Loss: 0.00001089
Iteration 66/1000 | Loss: 0.00001089
Iteration 67/1000 | Loss: 0.00001089
Iteration 68/1000 | Loss: 0.00001089
Iteration 69/1000 | Loss: 0.00001088
Iteration 70/1000 | Loss: 0.00001088
Iteration 71/1000 | Loss: 0.00001088
Iteration 72/1000 | Loss: 0.00001088
Iteration 73/1000 | Loss: 0.00001086
Iteration 74/1000 | Loss: 0.00001086
Iteration 75/1000 | Loss: 0.00001086
Iteration 76/1000 | Loss: 0.00001086
Iteration 77/1000 | Loss: 0.00001086
Iteration 78/1000 | Loss: 0.00001086
Iteration 79/1000 | Loss: 0.00001086
Iteration 80/1000 | Loss: 0.00001085
Iteration 81/1000 | Loss: 0.00001085
Iteration 82/1000 | Loss: 0.00001083
Iteration 83/1000 | Loss: 0.00001083
Iteration 84/1000 | Loss: 0.00001082
Iteration 85/1000 | Loss: 0.00001082
Iteration 86/1000 | Loss: 0.00001082
Iteration 87/1000 | Loss: 0.00001081
Iteration 88/1000 | Loss: 0.00001081
Iteration 89/1000 | Loss: 0.00001081
Iteration 90/1000 | Loss: 0.00001081
Iteration 91/1000 | Loss: 0.00001080
Iteration 92/1000 | Loss: 0.00001080
Iteration 93/1000 | Loss: 0.00001080
Iteration 94/1000 | Loss: 0.00001080
Iteration 95/1000 | Loss: 0.00001079
Iteration 96/1000 | Loss: 0.00001079
Iteration 97/1000 | Loss: 0.00001079
Iteration 98/1000 | Loss: 0.00001079
Iteration 99/1000 | Loss: 0.00001078
Iteration 100/1000 | Loss: 0.00001078
Iteration 101/1000 | Loss: 0.00001078
Iteration 102/1000 | Loss: 0.00001078
Iteration 103/1000 | Loss: 0.00001077
Iteration 104/1000 | Loss: 0.00001077
Iteration 105/1000 | Loss: 0.00001077
Iteration 106/1000 | Loss: 0.00001077
Iteration 107/1000 | Loss: 0.00001077
Iteration 108/1000 | Loss: 0.00001076
Iteration 109/1000 | Loss: 0.00001076
Iteration 110/1000 | Loss: 0.00001076
Iteration 111/1000 | Loss: 0.00001076
Iteration 112/1000 | Loss: 0.00001076
Iteration 113/1000 | Loss: 0.00001076
Iteration 114/1000 | Loss: 0.00001076
Iteration 115/1000 | Loss: 0.00001075
Iteration 116/1000 | Loss: 0.00001075
Iteration 117/1000 | Loss: 0.00001075
Iteration 118/1000 | Loss: 0.00001074
Iteration 119/1000 | Loss: 0.00001074
Iteration 120/1000 | Loss: 0.00001074
Iteration 121/1000 | Loss: 0.00001074
Iteration 122/1000 | Loss: 0.00001074
Iteration 123/1000 | Loss: 0.00001074
Iteration 124/1000 | Loss: 0.00001074
Iteration 125/1000 | Loss: 0.00001074
Iteration 126/1000 | Loss: 0.00001074
Iteration 127/1000 | Loss: 0.00001074
Iteration 128/1000 | Loss: 0.00001074
Iteration 129/1000 | Loss: 0.00001074
Iteration 130/1000 | Loss: 0.00001073
Iteration 131/1000 | Loss: 0.00001073
Iteration 132/1000 | Loss: 0.00001073
Iteration 133/1000 | Loss: 0.00001073
Iteration 134/1000 | Loss: 0.00001073
Iteration 135/1000 | Loss: 0.00001073
Iteration 136/1000 | Loss: 0.00001073
Iteration 137/1000 | Loss: 0.00001073
Iteration 138/1000 | Loss: 0.00001073
Iteration 139/1000 | Loss: 0.00001072
Iteration 140/1000 | Loss: 0.00001072
Iteration 141/1000 | Loss: 0.00001072
Iteration 142/1000 | Loss: 0.00001072
Iteration 143/1000 | Loss: 0.00001072
Iteration 144/1000 | Loss: 0.00001072
Iteration 145/1000 | Loss: 0.00001072
Iteration 146/1000 | Loss: 0.00001072
Iteration 147/1000 | Loss: 0.00001071
Iteration 148/1000 | Loss: 0.00001071
Iteration 149/1000 | Loss: 0.00001071
Iteration 150/1000 | Loss: 0.00001071
Iteration 151/1000 | Loss: 0.00001071
Iteration 152/1000 | Loss: 0.00001071
Iteration 153/1000 | Loss: 0.00001071
Iteration 154/1000 | Loss: 0.00001071
Iteration 155/1000 | Loss: 0.00001070
Iteration 156/1000 | Loss: 0.00001070
Iteration 157/1000 | Loss: 0.00001070
Iteration 158/1000 | Loss: 0.00001070
Iteration 159/1000 | Loss: 0.00001069
Iteration 160/1000 | Loss: 0.00001069
Iteration 161/1000 | Loss: 0.00001069
Iteration 162/1000 | Loss: 0.00001069
Iteration 163/1000 | Loss: 0.00001069
Iteration 164/1000 | Loss: 0.00001069
Iteration 165/1000 | Loss: 0.00001069
Iteration 166/1000 | Loss: 0.00001068
Iteration 167/1000 | Loss: 0.00001068
Iteration 168/1000 | Loss: 0.00001068
Iteration 169/1000 | Loss: 0.00001068
Iteration 170/1000 | Loss: 0.00001068
Iteration 171/1000 | Loss: 0.00001068
Iteration 172/1000 | Loss: 0.00001068
Iteration 173/1000 | Loss: 0.00001067
Iteration 174/1000 | Loss: 0.00001067
Iteration 175/1000 | Loss: 0.00001067
Iteration 176/1000 | Loss: 0.00001067
Iteration 177/1000 | Loss: 0.00001067
Iteration 178/1000 | Loss: 0.00001067
Iteration 179/1000 | Loss: 0.00001066
Iteration 180/1000 | Loss: 0.00001066
Iteration 181/1000 | Loss: 0.00001066
Iteration 182/1000 | Loss: 0.00001066
Iteration 183/1000 | Loss: 0.00001066
Iteration 184/1000 | Loss: 0.00001066
Iteration 185/1000 | Loss: 0.00001066
Iteration 186/1000 | Loss: 0.00001065
Iteration 187/1000 | Loss: 0.00001065
Iteration 188/1000 | Loss: 0.00001065
Iteration 189/1000 | Loss: 0.00001065
Iteration 190/1000 | Loss: 0.00001065
Iteration 191/1000 | Loss: 0.00001065
Iteration 192/1000 | Loss: 0.00001065
Iteration 193/1000 | Loss: 0.00001065
Iteration 194/1000 | Loss: 0.00001065
Iteration 195/1000 | Loss: 0.00001065
Iteration 196/1000 | Loss: 0.00001065
Iteration 197/1000 | Loss: 0.00001064
Iteration 198/1000 | Loss: 0.00001064
Iteration 199/1000 | Loss: 0.00001064
Iteration 200/1000 | Loss: 0.00001064
Iteration 201/1000 | Loss: 0.00001063
Iteration 202/1000 | Loss: 0.00001063
Iteration 203/1000 | Loss: 0.00001063
Iteration 204/1000 | Loss: 0.00001063
Iteration 205/1000 | Loss: 0.00001063
Iteration 206/1000 | Loss: 0.00001063
Iteration 207/1000 | Loss: 0.00001063
Iteration 208/1000 | Loss: 0.00001062
Iteration 209/1000 | Loss: 0.00001062
Iteration 210/1000 | Loss: 0.00001062
Iteration 211/1000 | Loss: 0.00001062
Iteration 212/1000 | Loss: 0.00001062
Iteration 213/1000 | Loss: 0.00001062
Iteration 214/1000 | Loss: 0.00001062
Iteration 215/1000 | Loss: 0.00001062
Iteration 216/1000 | Loss: 0.00001062
Iteration 217/1000 | Loss: 0.00001062
Iteration 218/1000 | Loss: 0.00001062
Iteration 219/1000 | Loss: 0.00001062
Iteration 220/1000 | Loss: 0.00001062
Iteration 221/1000 | Loss: 0.00001062
Iteration 222/1000 | Loss: 0.00001062
Iteration 223/1000 | Loss: 0.00001062
Iteration 224/1000 | Loss: 0.00001062
Iteration 225/1000 | Loss: 0.00001061
Iteration 226/1000 | Loss: 0.00001061
Iteration 227/1000 | Loss: 0.00001061
Iteration 228/1000 | Loss: 0.00001061
Iteration 229/1000 | Loss: 0.00001061
Iteration 230/1000 | Loss: 0.00001061
Iteration 231/1000 | Loss: 0.00001061
Iteration 232/1000 | Loss: 0.00001061
Iteration 233/1000 | Loss: 0.00001060
Iteration 234/1000 | Loss: 0.00001060
Iteration 235/1000 | Loss: 0.00001060
Iteration 236/1000 | Loss: 0.00001060
Iteration 237/1000 | Loss: 0.00001060
Iteration 238/1000 | Loss: 0.00001060
Iteration 239/1000 | Loss: 0.00001060
Iteration 240/1000 | Loss: 0.00001060
Iteration 241/1000 | Loss: 0.00001060
Iteration 242/1000 | Loss: 0.00001060
Iteration 243/1000 | Loss: 0.00001060
Iteration 244/1000 | Loss: 0.00001060
Iteration 245/1000 | Loss: 0.00001060
Iteration 246/1000 | Loss: 0.00001060
Iteration 247/1000 | Loss: 0.00001060
Iteration 248/1000 | Loss: 0.00001060
Iteration 249/1000 | Loss: 0.00001060
Iteration 250/1000 | Loss: 0.00001060
Iteration 251/1000 | Loss: 0.00001060
Iteration 252/1000 | Loss: 0.00001060
Iteration 253/1000 | Loss: 0.00001060
Iteration 254/1000 | Loss: 0.00001060
Iteration 255/1000 | Loss: 0.00001060
Iteration 256/1000 | Loss: 0.00001060
Iteration 257/1000 | Loss: 0.00001060
Iteration 258/1000 | Loss: 0.00001060
Iteration 259/1000 | Loss: 0.00001060
Iteration 260/1000 | Loss: 0.00001060
Iteration 261/1000 | Loss: 0.00001060
Iteration 262/1000 | Loss: 0.00001060
Iteration 263/1000 | Loss: 0.00001060
Iteration 264/1000 | Loss: 0.00001060
Iteration 265/1000 | Loss: 0.00001060
Iteration 266/1000 | Loss: 0.00001060
Iteration 267/1000 | Loss: 0.00001060
Iteration 268/1000 | Loss: 0.00001060
Iteration 269/1000 | Loss: 0.00001060
Iteration 270/1000 | Loss: 0.00001060
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 270. Stopping optimization.
Last 5 losses: [1.0604491762933321e-05, 1.0604491762933321e-05, 1.0604491762933321e-05, 1.0604491762933321e-05, 1.0604491762933321e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0604491762933321e-05

Optimization complete. Final v2v error: 2.7382988929748535 mm

Highest mean error: 3.4214391708374023 mm for frame 83

Lowest mean error: 2.4529056549072266 mm for frame 156

Saving results

Total time: 45.24505019187927
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_023/1075/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_023/1075.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_023/1075
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00760196
Iteration 2/25 | Loss: 0.00184738
Iteration 3/25 | Loss: 0.00139783
Iteration 4/25 | Loss: 0.00132199
Iteration 5/25 | Loss: 0.00130842
Iteration 6/25 | Loss: 0.00132743
Iteration 7/25 | Loss: 0.00126868
Iteration 8/25 | Loss: 0.00123906
Iteration 9/25 | Loss: 0.00123710
Iteration 10/25 | Loss: 0.00123363
Iteration 11/25 | Loss: 0.00122663
Iteration 12/25 | Loss: 0.00122636
Iteration 13/25 | Loss: 0.00122679
Iteration 14/25 | Loss: 0.00122580
Iteration 15/25 | Loss: 0.00122621
Iteration 16/25 | Loss: 0.00122553
Iteration 17/25 | Loss: 0.00122599
Iteration 18/25 | Loss: 0.00122595
Iteration 19/25 | Loss: 0.00122620
Iteration 20/25 | Loss: 0.00122598
Iteration 21/25 | Loss: 0.00122633
Iteration 22/25 | Loss: 0.00122707
Iteration 23/25 | Loss: 0.00122678
Iteration 24/25 | Loss: 0.00122614
Iteration 25/25 | Loss: 0.00122637

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33216500
Iteration 2/25 | Loss: 0.00075062
Iteration 3/25 | Loss: 0.00075061
Iteration 4/25 | Loss: 0.00075061
Iteration 5/25 | Loss: 0.00075061
Iteration 6/25 | Loss: 0.00075061
Iteration 7/25 | Loss: 0.00075061
Iteration 8/25 | Loss: 0.00075061
Iteration 9/25 | Loss: 0.00075061
Iteration 10/25 | Loss: 0.00075061
Iteration 11/25 | Loss: 0.00075061
Iteration 12/25 | Loss: 0.00075061
Iteration 13/25 | Loss: 0.00075061
Iteration 14/25 | Loss: 0.00075060
Iteration 15/25 | Loss: 0.00075061
Iteration 16/25 | Loss: 0.00075060
Iteration 17/25 | Loss: 0.00075060
Iteration 18/25 | Loss: 0.00075060
Iteration 19/25 | Loss: 0.00075060
Iteration 20/25 | Loss: 0.00075060
Iteration 21/25 | Loss: 0.00075060
Iteration 22/25 | Loss: 0.00075060
Iteration 23/25 | Loss: 0.00075060
Iteration 24/25 | Loss: 0.00075060
Iteration 25/25 | Loss: 0.00075060

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00075060
Iteration 2/1000 | Loss: 0.00075034
Iteration 3/1000 | Loss: 0.00053150
Iteration 4/1000 | Loss: 0.00008207
Iteration 5/1000 | Loss: 0.00007427
Iteration 6/1000 | Loss: 0.00006991
Iteration 7/1000 | Loss: 0.00007798
Iteration 8/1000 | Loss: 0.00005738
Iteration 9/1000 | Loss: 0.00005610
Iteration 10/1000 | Loss: 0.00006427
Iteration 11/1000 | Loss: 0.00004925
Iteration 12/1000 | Loss: 0.00004289
Iteration 13/1000 | Loss: 0.00005009
Iteration 14/1000 | Loss: 0.00003535
Iteration 15/1000 | Loss: 0.00003984
Iteration 16/1000 | Loss: 0.00005257
Iteration 17/1000 | Loss: 0.00003487
Iteration 18/1000 | Loss: 0.00003159
Iteration 19/1000 | Loss: 0.00002844
Iteration 20/1000 | Loss: 0.00003149
Iteration 21/1000 | Loss: 0.00002638
Iteration 22/1000 | Loss: 0.00003251
Iteration 23/1000 | Loss: 0.00002536
Iteration 24/1000 | Loss: 0.00002396
Iteration 25/1000 | Loss: 0.00002486
Iteration 26/1000 | Loss: 0.00002442
Iteration 27/1000 | Loss: 0.00003544
Iteration 28/1000 | Loss: 0.00002737
Iteration 29/1000 | Loss: 0.00002441
Iteration 30/1000 | Loss: 0.00002855
Iteration 31/1000 | Loss: 0.00002630
Iteration 32/1000 | Loss: 0.00002278
Iteration 33/1000 | Loss: 0.00002125
Iteration 34/1000 | Loss: 0.00002047
Iteration 35/1000 | Loss: 0.00001999
Iteration 36/1000 | Loss: 0.00001952
Iteration 37/1000 | Loss: 0.00001925
Iteration 38/1000 | Loss: 0.00001893
Iteration 39/1000 | Loss: 0.00001866
Iteration 40/1000 | Loss: 0.00001839
Iteration 41/1000 | Loss: 0.00001830
Iteration 42/1000 | Loss: 0.00001810
Iteration 43/1000 | Loss: 0.00001793
Iteration 44/1000 | Loss: 0.00001792
Iteration 45/1000 | Loss: 0.00001792
Iteration 46/1000 | Loss: 0.00001792
Iteration 47/1000 | Loss: 0.00001791
Iteration 48/1000 | Loss: 0.00001791
Iteration 49/1000 | Loss: 0.00001789
Iteration 50/1000 | Loss: 0.00001788
Iteration 51/1000 | Loss: 0.00001784
Iteration 52/1000 | Loss: 0.00001784
Iteration 53/1000 | Loss: 0.00001784
Iteration 54/1000 | Loss: 0.00001783
Iteration 55/1000 | Loss: 0.00001781
Iteration 56/1000 | Loss: 0.00001779
Iteration 57/1000 | Loss: 0.00001779
Iteration 58/1000 | Loss: 0.00001779
Iteration 59/1000 | Loss: 0.00001779
Iteration 60/1000 | Loss: 0.00001779
Iteration 61/1000 | Loss: 0.00001779
Iteration 62/1000 | Loss: 0.00001778
Iteration 63/1000 | Loss: 0.00001778
Iteration 64/1000 | Loss: 0.00001778
Iteration 65/1000 | Loss: 0.00001778
Iteration 66/1000 | Loss: 0.00001778
Iteration 67/1000 | Loss: 0.00001778
Iteration 68/1000 | Loss: 0.00001778
Iteration 69/1000 | Loss: 0.00001778
Iteration 70/1000 | Loss: 0.00001778
Iteration 71/1000 | Loss: 0.00001778
Iteration 72/1000 | Loss: 0.00001778
Iteration 73/1000 | Loss: 0.00001777
Iteration 74/1000 | Loss: 0.00001777
Iteration 75/1000 | Loss: 0.00001777
Iteration 76/1000 | Loss: 0.00001777
Iteration 77/1000 | Loss: 0.00001776
Iteration 78/1000 | Loss: 0.00001776
Iteration 79/1000 | Loss: 0.00001776
Iteration 80/1000 | Loss: 0.00001775
Iteration 81/1000 | Loss: 0.00001775
Iteration 82/1000 | Loss: 0.00001775
Iteration 83/1000 | Loss: 0.00001774
Iteration 84/1000 | Loss: 0.00001774
Iteration 85/1000 | Loss: 0.00001773
Iteration 86/1000 | Loss: 0.00001773
Iteration 87/1000 | Loss: 0.00001772
Iteration 88/1000 | Loss: 0.00001772
Iteration 89/1000 | Loss: 0.00001772
Iteration 90/1000 | Loss: 0.00001772
Iteration 91/1000 | Loss: 0.00001771
Iteration 92/1000 | Loss: 0.00001771
Iteration 93/1000 | Loss: 0.00001771
Iteration 94/1000 | Loss: 0.00001770
Iteration 95/1000 | Loss: 0.00001770
Iteration 96/1000 | Loss: 0.00001770
Iteration 97/1000 | Loss: 0.00001770
Iteration 98/1000 | Loss: 0.00001769
Iteration 99/1000 | Loss: 0.00001769
Iteration 100/1000 | Loss: 0.00001768
Iteration 101/1000 | Loss: 0.00001768
Iteration 102/1000 | Loss: 0.00001767
Iteration 103/1000 | Loss: 0.00001767
Iteration 104/1000 | Loss: 0.00001767
Iteration 105/1000 | Loss: 0.00001767
Iteration 106/1000 | Loss: 0.00001767
Iteration 107/1000 | Loss: 0.00001767
Iteration 108/1000 | Loss: 0.00001767
Iteration 109/1000 | Loss: 0.00001767
Iteration 110/1000 | Loss: 0.00001766
Iteration 111/1000 | Loss: 0.00001766
Iteration 112/1000 | Loss: 0.00001766
Iteration 113/1000 | Loss: 0.00001766
Iteration 114/1000 | Loss: 0.00001766
Iteration 115/1000 | Loss: 0.00001765
Iteration 116/1000 | Loss: 0.00001765
Iteration 117/1000 | Loss: 0.00001765
Iteration 118/1000 | Loss: 0.00001764
Iteration 119/1000 | Loss: 0.00001764
Iteration 120/1000 | Loss: 0.00001764
Iteration 121/1000 | Loss: 0.00001764
Iteration 122/1000 | Loss: 0.00001763
Iteration 123/1000 | Loss: 0.00001763
Iteration 124/1000 | Loss: 0.00001763
Iteration 125/1000 | Loss: 0.00001762
Iteration 126/1000 | Loss: 0.00001762
Iteration 127/1000 | Loss: 0.00001762
Iteration 128/1000 | Loss: 0.00001762
Iteration 129/1000 | Loss: 0.00001762
Iteration 130/1000 | Loss: 0.00001761
Iteration 131/1000 | Loss: 0.00001761
Iteration 132/1000 | Loss: 0.00001761
Iteration 133/1000 | Loss: 0.00001761
Iteration 134/1000 | Loss: 0.00001760
Iteration 135/1000 | Loss: 0.00001760
Iteration 136/1000 | Loss: 0.00001759
Iteration 137/1000 | Loss: 0.00001759
Iteration 138/1000 | Loss: 0.00001759
Iteration 139/1000 | Loss: 0.00001758
Iteration 140/1000 | Loss: 0.00001758
Iteration 141/1000 | Loss: 0.00001758
Iteration 142/1000 | Loss: 0.00001758
Iteration 143/1000 | Loss: 0.00001758
Iteration 144/1000 | Loss: 0.00001757
Iteration 145/1000 | Loss: 0.00001757
Iteration 146/1000 | Loss: 0.00001757
Iteration 147/1000 | Loss: 0.00001757
Iteration 148/1000 | Loss: 0.00001757
Iteration 149/1000 | Loss: 0.00001757
Iteration 150/1000 | Loss: 0.00001757
Iteration 151/1000 | Loss: 0.00001756
Iteration 152/1000 | Loss: 0.00001756
Iteration 153/1000 | Loss: 0.00001755
Iteration 154/1000 | Loss: 0.00001755
Iteration 155/1000 | Loss: 0.00001755
Iteration 156/1000 | Loss: 0.00001755
Iteration 157/1000 | Loss: 0.00001755
Iteration 158/1000 | Loss: 0.00001754
Iteration 159/1000 | Loss: 0.00001754
Iteration 160/1000 | Loss: 0.00001754
Iteration 161/1000 | Loss: 0.00001754
Iteration 162/1000 | Loss: 0.00001754
Iteration 163/1000 | Loss: 0.00001754
Iteration 164/1000 | Loss: 0.00001754
Iteration 165/1000 | Loss: 0.00001754
Iteration 166/1000 | Loss: 0.00001754
Iteration 167/1000 | Loss: 0.00001754
Iteration 168/1000 | Loss: 0.00001753
Iteration 169/1000 | Loss: 0.00001753
Iteration 170/1000 | Loss: 0.00001753
Iteration 171/1000 | Loss: 0.00001753
Iteration 172/1000 | Loss: 0.00001753
Iteration 173/1000 | Loss: 0.00001753
Iteration 174/1000 | Loss: 0.00001753
Iteration 175/1000 | Loss: 0.00001753
Iteration 176/1000 | Loss: 0.00001752
Iteration 177/1000 | Loss: 0.00001752
Iteration 178/1000 | Loss: 0.00001752
Iteration 179/1000 | Loss: 0.00001752
Iteration 180/1000 | Loss: 0.00001752
Iteration 181/1000 | Loss: 0.00001752
Iteration 182/1000 | Loss: 0.00001752
Iteration 183/1000 | Loss: 0.00001752
Iteration 184/1000 | Loss: 0.00001752
Iteration 185/1000 | Loss: 0.00001752
Iteration 186/1000 | Loss: 0.00001752
Iteration 187/1000 | Loss: 0.00001751
Iteration 188/1000 | Loss: 0.00001751
Iteration 189/1000 | Loss: 0.00001751
Iteration 190/1000 | Loss: 0.00001751
Iteration 191/1000 | Loss: 0.00001751
Iteration 192/1000 | Loss: 0.00001751
Iteration 193/1000 | Loss: 0.00001751
Iteration 194/1000 | Loss: 0.00001751
Iteration 195/1000 | Loss: 0.00001750
Iteration 196/1000 | Loss: 0.00001750
Iteration 197/1000 | Loss: 0.00001750
Iteration 198/1000 | Loss: 0.00001750
Iteration 199/1000 | Loss: 0.00001750
Iteration 200/1000 | Loss: 0.00001750
Iteration 201/1000 | Loss: 0.00001750
Iteration 202/1000 | Loss: 0.00001750
Iteration 203/1000 | Loss: 0.00001750
Iteration 204/1000 | Loss: 0.00001750
Iteration 205/1000 | Loss: 0.00001750
Iteration 206/1000 | Loss: 0.00001750
Iteration 207/1000 | Loss: 0.00001750
Iteration 208/1000 | Loss: 0.00001750
Iteration 209/1000 | Loss: 0.00001750
Iteration 210/1000 | Loss: 0.00001750
Iteration 211/1000 | Loss: 0.00001750
Iteration 212/1000 | Loss: 0.00001750
Iteration 213/1000 | Loss: 0.00001750
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 213. Stopping optimization.
Last 5 losses: [1.7499240129836835e-05, 1.7499240129836835e-05, 1.7499240129836835e-05, 1.7499240129836835e-05, 1.7499240129836835e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7499240129836835e-05

Optimization complete. Final v2v error: 3.444418430328369 mm

Highest mean error: 5.588517189025879 mm for frame 219

Lowest mean error: 2.733532667160034 mm for frame 77

Saving results

Total time: 137.6092402935028
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_023/1041/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_023/1041.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_023/1041
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00477357
Iteration 2/25 | Loss: 0.00129302
Iteration 3/25 | Loss: 0.00118804
Iteration 4/25 | Loss: 0.00117272
Iteration 5/25 | Loss: 0.00116603
Iteration 6/25 | Loss: 0.00116481
Iteration 7/25 | Loss: 0.00116481
Iteration 8/25 | Loss: 0.00116481
Iteration 9/25 | Loss: 0.00116481
Iteration 10/25 | Loss: 0.00116481
Iteration 11/25 | Loss: 0.00116481
Iteration 12/25 | Loss: 0.00116481
Iteration 13/25 | Loss: 0.00116481
Iteration 14/25 | Loss: 0.00116481
Iteration 15/25 | Loss: 0.00116481
Iteration 16/25 | Loss: 0.00116481
Iteration 17/25 | Loss: 0.00116481
Iteration 18/25 | Loss: 0.00116481
Iteration 19/25 | Loss: 0.00116481
Iteration 20/25 | Loss: 0.00116481
Iteration 21/25 | Loss: 0.00116481
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0011648052604869008, 0.0011648052604869008, 0.0011648052604869008, 0.0011648052604869008, 0.0011648052604869008]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011648052604869008

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.79812592
Iteration 2/25 | Loss: 0.00084174
Iteration 3/25 | Loss: 0.00084174
Iteration 4/25 | Loss: 0.00084174
Iteration 5/25 | Loss: 0.00084174
Iteration 6/25 | Loss: 0.00084174
Iteration 7/25 | Loss: 0.00084174
Iteration 8/25 | Loss: 0.00084174
Iteration 9/25 | Loss: 0.00084174
Iteration 10/25 | Loss: 0.00084174
Iteration 11/25 | Loss: 0.00084174
Iteration 12/25 | Loss: 0.00084174
Iteration 13/25 | Loss: 0.00084174
Iteration 14/25 | Loss: 0.00084174
Iteration 15/25 | Loss: 0.00084174
Iteration 16/25 | Loss: 0.00084174
Iteration 17/25 | Loss: 0.00084174
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0008417396456934512, 0.0008417396456934512, 0.0008417396456934512, 0.0008417396456934512, 0.0008417396456934512]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008417396456934512

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00084174
Iteration 2/1000 | Loss: 0.00003806
Iteration 3/1000 | Loss: 0.00002421
Iteration 4/1000 | Loss: 0.00002170
Iteration 5/1000 | Loss: 0.00002046
Iteration 6/1000 | Loss: 0.00001966
Iteration 7/1000 | Loss: 0.00001912
Iteration 8/1000 | Loss: 0.00001867
Iteration 9/1000 | Loss: 0.00001829
Iteration 10/1000 | Loss: 0.00001800
Iteration 11/1000 | Loss: 0.00001775
Iteration 12/1000 | Loss: 0.00001756
Iteration 13/1000 | Loss: 0.00001738
Iteration 14/1000 | Loss: 0.00001728
Iteration 15/1000 | Loss: 0.00001727
Iteration 16/1000 | Loss: 0.00001724
Iteration 17/1000 | Loss: 0.00001723
Iteration 18/1000 | Loss: 0.00001710
Iteration 19/1000 | Loss: 0.00001703
Iteration 20/1000 | Loss: 0.00001703
Iteration 21/1000 | Loss: 0.00001701
Iteration 22/1000 | Loss: 0.00001700
Iteration 23/1000 | Loss: 0.00001700
Iteration 24/1000 | Loss: 0.00001700
Iteration 25/1000 | Loss: 0.00001700
Iteration 26/1000 | Loss: 0.00001700
Iteration 27/1000 | Loss: 0.00001700
Iteration 28/1000 | Loss: 0.00001700
Iteration 29/1000 | Loss: 0.00001700
Iteration 30/1000 | Loss: 0.00001699
Iteration 31/1000 | Loss: 0.00001699
Iteration 32/1000 | Loss: 0.00001699
Iteration 33/1000 | Loss: 0.00001699
Iteration 34/1000 | Loss: 0.00001699
Iteration 35/1000 | Loss: 0.00001697
Iteration 36/1000 | Loss: 0.00001697
Iteration 37/1000 | Loss: 0.00001697
Iteration 38/1000 | Loss: 0.00001696
Iteration 39/1000 | Loss: 0.00001695
Iteration 40/1000 | Loss: 0.00001694
Iteration 41/1000 | Loss: 0.00001694
Iteration 42/1000 | Loss: 0.00001694
Iteration 43/1000 | Loss: 0.00001693
Iteration 44/1000 | Loss: 0.00001693
Iteration 45/1000 | Loss: 0.00001693
Iteration 46/1000 | Loss: 0.00001692
Iteration 47/1000 | Loss: 0.00001692
Iteration 48/1000 | Loss: 0.00001687
Iteration 49/1000 | Loss: 0.00001684
Iteration 50/1000 | Loss: 0.00001683
Iteration 51/1000 | Loss: 0.00001683
Iteration 52/1000 | Loss: 0.00001682
Iteration 53/1000 | Loss: 0.00001680
Iteration 54/1000 | Loss: 0.00001680
Iteration 55/1000 | Loss: 0.00001680
Iteration 56/1000 | Loss: 0.00001680
Iteration 57/1000 | Loss: 0.00001680
Iteration 58/1000 | Loss: 0.00001680
Iteration 59/1000 | Loss: 0.00001679
Iteration 60/1000 | Loss: 0.00001679
Iteration 61/1000 | Loss: 0.00001679
Iteration 62/1000 | Loss: 0.00001679
Iteration 63/1000 | Loss: 0.00001679
Iteration 64/1000 | Loss: 0.00001679
Iteration 65/1000 | Loss: 0.00001679
Iteration 66/1000 | Loss: 0.00001679
Iteration 67/1000 | Loss: 0.00001676
Iteration 68/1000 | Loss: 0.00001675
Iteration 69/1000 | Loss: 0.00001673
Iteration 70/1000 | Loss: 0.00001673
Iteration 71/1000 | Loss: 0.00001672
Iteration 72/1000 | Loss: 0.00001672
Iteration 73/1000 | Loss: 0.00001671
Iteration 74/1000 | Loss: 0.00001671
Iteration 75/1000 | Loss: 0.00001670
Iteration 76/1000 | Loss: 0.00001670
Iteration 77/1000 | Loss: 0.00001669
Iteration 78/1000 | Loss: 0.00001669
Iteration 79/1000 | Loss: 0.00001669
Iteration 80/1000 | Loss: 0.00001669
Iteration 81/1000 | Loss: 0.00001669
Iteration 82/1000 | Loss: 0.00001669
Iteration 83/1000 | Loss: 0.00001668
Iteration 84/1000 | Loss: 0.00001668
Iteration 85/1000 | Loss: 0.00001668
Iteration 86/1000 | Loss: 0.00001668
Iteration 87/1000 | Loss: 0.00001667
Iteration 88/1000 | Loss: 0.00001667
Iteration 89/1000 | Loss: 0.00001667
Iteration 90/1000 | Loss: 0.00001667
Iteration 91/1000 | Loss: 0.00001667
Iteration 92/1000 | Loss: 0.00001667
Iteration 93/1000 | Loss: 0.00001667
Iteration 94/1000 | Loss: 0.00001667
Iteration 95/1000 | Loss: 0.00001667
Iteration 96/1000 | Loss: 0.00001666
Iteration 97/1000 | Loss: 0.00001666
Iteration 98/1000 | Loss: 0.00001666
Iteration 99/1000 | Loss: 0.00001666
Iteration 100/1000 | Loss: 0.00001665
Iteration 101/1000 | Loss: 0.00001665
Iteration 102/1000 | Loss: 0.00001665
Iteration 103/1000 | Loss: 0.00001665
Iteration 104/1000 | Loss: 0.00001665
Iteration 105/1000 | Loss: 0.00001665
Iteration 106/1000 | Loss: 0.00001665
Iteration 107/1000 | Loss: 0.00001665
Iteration 108/1000 | Loss: 0.00001665
Iteration 109/1000 | Loss: 0.00001665
Iteration 110/1000 | Loss: 0.00001665
Iteration 111/1000 | Loss: 0.00001665
Iteration 112/1000 | Loss: 0.00001664
Iteration 113/1000 | Loss: 0.00001664
Iteration 114/1000 | Loss: 0.00001664
Iteration 115/1000 | Loss: 0.00001664
Iteration 116/1000 | Loss: 0.00001664
Iteration 117/1000 | Loss: 0.00001663
Iteration 118/1000 | Loss: 0.00001663
Iteration 119/1000 | Loss: 0.00001663
Iteration 120/1000 | Loss: 0.00001663
Iteration 121/1000 | Loss: 0.00001663
Iteration 122/1000 | Loss: 0.00001662
Iteration 123/1000 | Loss: 0.00001662
Iteration 124/1000 | Loss: 0.00001662
Iteration 125/1000 | Loss: 0.00001662
Iteration 126/1000 | Loss: 0.00001662
Iteration 127/1000 | Loss: 0.00001662
Iteration 128/1000 | Loss: 0.00001662
Iteration 129/1000 | Loss: 0.00001662
Iteration 130/1000 | Loss: 0.00001662
Iteration 131/1000 | Loss: 0.00001661
Iteration 132/1000 | Loss: 0.00001661
Iteration 133/1000 | Loss: 0.00001661
Iteration 134/1000 | Loss: 0.00001661
Iteration 135/1000 | Loss: 0.00001661
Iteration 136/1000 | Loss: 0.00001661
Iteration 137/1000 | Loss: 0.00001661
Iteration 138/1000 | Loss: 0.00001660
Iteration 139/1000 | Loss: 0.00001660
Iteration 140/1000 | Loss: 0.00001660
Iteration 141/1000 | Loss: 0.00001660
Iteration 142/1000 | Loss: 0.00001660
Iteration 143/1000 | Loss: 0.00001660
Iteration 144/1000 | Loss: 0.00001660
Iteration 145/1000 | Loss: 0.00001659
Iteration 146/1000 | Loss: 0.00001659
Iteration 147/1000 | Loss: 0.00001659
Iteration 148/1000 | Loss: 0.00001659
Iteration 149/1000 | Loss: 0.00001659
Iteration 150/1000 | Loss: 0.00001658
Iteration 151/1000 | Loss: 0.00001658
Iteration 152/1000 | Loss: 0.00001658
Iteration 153/1000 | Loss: 0.00001658
Iteration 154/1000 | Loss: 0.00001658
Iteration 155/1000 | Loss: 0.00001658
Iteration 156/1000 | Loss: 0.00001658
Iteration 157/1000 | Loss: 0.00001657
Iteration 158/1000 | Loss: 0.00001657
Iteration 159/1000 | Loss: 0.00001657
Iteration 160/1000 | Loss: 0.00001657
Iteration 161/1000 | Loss: 0.00001657
Iteration 162/1000 | Loss: 0.00001657
Iteration 163/1000 | Loss: 0.00001657
Iteration 164/1000 | Loss: 0.00001656
Iteration 165/1000 | Loss: 0.00001656
Iteration 166/1000 | Loss: 0.00001656
Iteration 167/1000 | Loss: 0.00001656
Iteration 168/1000 | Loss: 0.00001655
Iteration 169/1000 | Loss: 0.00001655
Iteration 170/1000 | Loss: 0.00001655
Iteration 171/1000 | Loss: 0.00001655
Iteration 172/1000 | Loss: 0.00001655
Iteration 173/1000 | Loss: 0.00001655
Iteration 174/1000 | Loss: 0.00001655
Iteration 175/1000 | Loss: 0.00001655
Iteration 176/1000 | Loss: 0.00001655
Iteration 177/1000 | Loss: 0.00001655
Iteration 178/1000 | Loss: 0.00001655
Iteration 179/1000 | Loss: 0.00001655
Iteration 180/1000 | Loss: 0.00001655
Iteration 181/1000 | Loss: 0.00001654
Iteration 182/1000 | Loss: 0.00001653
Iteration 183/1000 | Loss: 0.00001653
Iteration 184/1000 | Loss: 0.00001653
Iteration 185/1000 | Loss: 0.00001653
Iteration 186/1000 | Loss: 0.00001653
Iteration 187/1000 | Loss: 0.00001653
Iteration 188/1000 | Loss: 0.00001652
Iteration 189/1000 | Loss: 0.00001652
Iteration 190/1000 | Loss: 0.00001652
Iteration 191/1000 | Loss: 0.00001652
Iteration 192/1000 | Loss: 0.00001652
Iteration 193/1000 | Loss: 0.00001652
Iteration 194/1000 | Loss: 0.00001652
Iteration 195/1000 | Loss: 0.00001652
Iteration 196/1000 | Loss: 0.00001652
Iteration 197/1000 | Loss: 0.00001652
Iteration 198/1000 | Loss: 0.00001652
Iteration 199/1000 | Loss: 0.00001652
Iteration 200/1000 | Loss: 0.00001651
Iteration 201/1000 | Loss: 0.00001650
Iteration 202/1000 | Loss: 0.00001650
Iteration 203/1000 | Loss: 0.00001650
Iteration 204/1000 | Loss: 0.00001649
Iteration 205/1000 | Loss: 0.00001649
Iteration 206/1000 | Loss: 0.00001649
Iteration 207/1000 | Loss: 0.00001649
Iteration 208/1000 | Loss: 0.00001649
Iteration 209/1000 | Loss: 0.00001649
Iteration 210/1000 | Loss: 0.00001649
Iteration 211/1000 | Loss: 0.00001649
Iteration 212/1000 | Loss: 0.00001649
Iteration 213/1000 | Loss: 0.00001649
Iteration 214/1000 | Loss: 0.00001649
Iteration 215/1000 | Loss: 0.00001649
Iteration 216/1000 | Loss: 0.00001648
Iteration 217/1000 | Loss: 0.00001648
Iteration 218/1000 | Loss: 0.00001648
Iteration 219/1000 | Loss: 0.00001648
Iteration 220/1000 | Loss: 0.00001648
Iteration 221/1000 | Loss: 0.00001648
Iteration 222/1000 | Loss: 0.00001647
Iteration 223/1000 | Loss: 0.00001647
Iteration 224/1000 | Loss: 0.00001646
Iteration 225/1000 | Loss: 0.00001646
Iteration 226/1000 | Loss: 0.00001646
Iteration 227/1000 | Loss: 0.00001646
Iteration 228/1000 | Loss: 0.00001645
Iteration 229/1000 | Loss: 0.00001645
Iteration 230/1000 | Loss: 0.00001645
Iteration 231/1000 | Loss: 0.00001645
Iteration 232/1000 | Loss: 0.00001644
Iteration 233/1000 | Loss: 0.00001644
Iteration 234/1000 | Loss: 0.00001644
Iteration 235/1000 | Loss: 0.00001644
Iteration 236/1000 | Loss: 0.00001644
Iteration 237/1000 | Loss: 0.00001644
Iteration 238/1000 | Loss: 0.00001644
Iteration 239/1000 | Loss: 0.00001644
Iteration 240/1000 | Loss: 0.00001644
Iteration 241/1000 | Loss: 0.00001644
Iteration 242/1000 | Loss: 0.00001644
Iteration 243/1000 | Loss: 0.00001644
Iteration 244/1000 | Loss: 0.00001644
Iteration 245/1000 | Loss: 0.00001644
Iteration 246/1000 | Loss: 0.00001644
Iteration 247/1000 | Loss: 0.00001644
Iteration 248/1000 | Loss: 0.00001644
Iteration 249/1000 | Loss: 0.00001643
Iteration 250/1000 | Loss: 0.00001643
Iteration 251/1000 | Loss: 0.00001643
Iteration 252/1000 | Loss: 0.00001643
Iteration 253/1000 | Loss: 0.00001643
Iteration 254/1000 | Loss: 0.00001643
Iteration 255/1000 | Loss: 0.00001643
Iteration 256/1000 | Loss: 0.00001643
Iteration 257/1000 | Loss: 0.00001643
Iteration 258/1000 | Loss: 0.00001643
Iteration 259/1000 | Loss: 0.00001643
Iteration 260/1000 | Loss: 0.00001643
Iteration 261/1000 | Loss: 0.00001643
Iteration 262/1000 | Loss: 0.00001643
Iteration 263/1000 | Loss: 0.00001643
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 263. Stopping optimization.
Last 5 losses: [1.6434181816293858e-05, 1.6434181816293858e-05, 1.6434181816293858e-05, 1.6434181816293858e-05, 1.6434181816293858e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6434181816293858e-05

Optimization complete. Final v2v error: 3.4264562129974365 mm

Highest mean error: 4.117700576782227 mm for frame 9

Lowest mean error: 3.1987104415893555 mm for frame 220

Saving results

Total time: 58.29914736747742
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_023/1074/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_023/1074.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_023/1074
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00842228
Iteration 2/25 | Loss: 0.00137916
Iteration 3/25 | Loss: 0.00122469
Iteration 4/25 | Loss: 0.00114257
Iteration 5/25 | Loss: 0.00113968
Iteration 6/25 | Loss: 0.00113899
Iteration 7/25 | Loss: 0.00113886
Iteration 8/25 | Loss: 0.00113886
Iteration 9/25 | Loss: 0.00113886
Iteration 10/25 | Loss: 0.00113886
Iteration 11/25 | Loss: 0.00113886
Iteration 12/25 | Loss: 0.00113886
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0011388579150661826, 0.0011388579150661826, 0.0011388579150661826, 0.0011388579150661826, 0.0011388579150661826]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011388579150661826

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 6.48196173
Iteration 2/25 | Loss: 0.00091515
Iteration 3/25 | Loss: 0.00089471
Iteration 4/25 | Loss: 0.00089471
Iteration 5/25 | Loss: 0.00089471
Iteration 6/25 | Loss: 0.00089471
Iteration 7/25 | Loss: 0.00089471
Iteration 8/25 | Loss: 0.00089471
Iteration 9/25 | Loss: 0.00089471
Iteration 10/25 | Loss: 0.00089471
Iteration 11/25 | Loss: 0.00089471
Iteration 12/25 | Loss: 0.00089470
Iteration 13/25 | Loss: 0.00089470
Iteration 14/25 | Loss: 0.00089470
Iteration 15/25 | Loss: 0.00089470
Iteration 16/25 | Loss: 0.00089470
Iteration 17/25 | Loss: 0.00089470
Iteration 18/25 | Loss: 0.00089470
Iteration 19/25 | Loss: 0.00089470
Iteration 20/25 | Loss: 0.00089470
Iteration 21/25 | Loss: 0.00089470
Iteration 22/25 | Loss: 0.00089470
Iteration 23/25 | Loss: 0.00089470
Iteration 24/25 | Loss: 0.00089470
Iteration 25/25 | Loss: 0.00089470

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00089470
Iteration 2/1000 | Loss: 0.00001737
Iteration 3/1000 | Loss: 0.00001311
Iteration 4/1000 | Loss: 0.00001189
Iteration 5/1000 | Loss: 0.00001135
Iteration 6/1000 | Loss: 0.00001111
Iteration 7/1000 | Loss: 0.00001108
Iteration 8/1000 | Loss: 0.00001085
Iteration 9/1000 | Loss: 0.00001083
Iteration 10/1000 | Loss: 0.00001055
Iteration 11/1000 | Loss: 0.00001053
Iteration 12/1000 | Loss: 0.00001044
Iteration 13/1000 | Loss: 0.00001031
Iteration 14/1000 | Loss: 0.00001028
Iteration 15/1000 | Loss: 0.00001026
Iteration 16/1000 | Loss: 0.00001025
Iteration 17/1000 | Loss: 0.00001025
Iteration 18/1000 | Loss: 0.00001025
Iteration 19/1000 | Loss: 0.00001024
Iteration 20/1000 | Loss: 0.00001024
Iteration 21/1000 | Loss: 0.00001024
Iteration 22/1000 | Loss: 0.00001023
Iteration 23/1000 | Loss: 0.00001023
Iteration 24/1000 | Loss: 0.00001023
Iteration 25/1000 | Loss: 0.00001022
Iteration 26/1000 | Loss: 0.00001021
Iteration 27/1000 | Loss: 0.00001021
Iteration 28/1000 | Loss: 0.00001020
Iteration 29/1000 | Loss: 0.00001020
Iteration 30/1000 | Loss: 0.00001020
Iteration 31/1000 | Loss: 0.00001019
Iteration 32/1000 | Loss: 0.00001019
Iteration 33/1000 | Loss: 0.00001018
Iteration 34/1000 | Loss: 0.00001018
Iteration 35/1000 | Loss: 0.00001017
Iteration 36/1000 | Loss: 0.00001017
Iteration 37/1000 | Loss: 0.00001016
Iteration 38/1000 | Loss: 0.00001016
Iteration 39/1000 | Loss: 0.00001015
Iteration 40/1000 | Loss: 0.00001015
Iteration 41/1000 | Loss: 0.00001015
Iteration 42/1000 | Loss: 0.00001015
Iteration 43/1000 | Loss: 0.00001015
Iteration 44/1000 | Loss: 0.00001015
Iteration 45/1000 | Loss: 0.00001014
Iteration 46/1000 | Loss: 0.00001014
Iteration 47/1000 | Loss: 0.00001014
Iteration 48/1000 | Loss: 0.00001014
Iteration 49/1000 | Loss: 0.00001012
Iteration 50/1000 | Loss: 0.00001012
Iteration 51/1000 | Loss: 0.00001012
Iteration 52/1000 | Loss: 0.00001012
Iteration 53/1000 | Loss: 0.00001011
Iteration 54/1000 | Loss: 0.00001011
Iteration 55/1000 | Loss: 0.00001011
Iteration 56/1000 | Loss: 0.00001010
Iteration 57/1000 | Loss: 0.00001010
Iteration 58/1000 | Loss: 0.00001008
Iteration 59/1000 | Loss: 0.00001008
Iteration 60/1000 | Loss: 0.00001007
Iteration 61/1000 | Loss: 0.00001006
Iteration 62/1000 | Loss: 0.00001003
Iteration 63/1000 | Loss: 0.00001003
Iteration 64/1000 | Loss: 0.00001003
Iteration 65/1000 | Loss: 0.00001003
Iteration 66/1000 | Loss: 0.00001003
Iteration 67/1000 | Loss: 0.00001003
Iteration 68/1000 | Loss: 0.00001002
Iteration 69/1000 | Loss: 0.00001002
Iteration 70/1000 | Loss: 0.00001002
Iteration 71/1000 | Loss: 0.00001002
Iteration 72/1000 | Loss: 0.00001001
Iteration 73/1000 | Loss: 0.00001001
Iteration 74/1000 | Loss: 0.00000999
Iteration 75/1000 | Loss: 0.00000998
Iteration 76/1000 | Loss: 0.00000997
Iteration 77/1000 | Loss: 0.00000997
Iteration 78/1000 | Loss: 0.00000997
Iteration 79/1000 | Loss: 0.00000997
Iteration 80/1000 | Loss: 0.00000997
Iteration 81/1000 | Loss: 0.00000996
Iteration 82/1000 | Loss: 0.00000996
Iteration 83/1000 | Loss: 0.00000995
Iteration 84/1000 | Loss: 0.00000995
Iteration 85/1000 | Loss: 0.00000995
Iteration 86/1000 | Loss: 0.00000994
Iteration 87/1000 | Loss: 0.00000994
Iteration 88/1000 | Loss: 0.00000993
Iteration 89/1000 | Loss: 0.00000993
Iteration 90/1000 | Loss: 0.00000993
Iteration 91/1000 | Loss: 0.00000993
Iteration 92/1000 | Loss: 0.00000993
Iteration 93/1000 | Loss: 0.00000993
Iteration 94/1000 | Loss: 0.00000993
Iteration 95/1000 | Loss: 0.00000993
Iteration 96/1000 | Loss: 0.00000992
Iteration 97/1000 | Loss: 0.00000992
Iteration 98/1000 | Loss: 0.00000992
Iteration 99/1000 | Loss: 0.00000992
Iteration 100/1000 | Loss: 0.00000992
Iteration 101/1000 | Loss: 0.00000992
Iteration 102/1000 | Loss: 0.00000992
Iteration 103/1000 | Loss: 0.00000992
Iteration 104/1000 | Loss: 0.00000992
Iteration 105/1000 | Loss: 0.00000992
Iteration 106/1000 | Loss: 0.00000992
Iteration 107/1000 | Loss: 0.00000992
Iteration 108/1000 | Loss: 0.00000992
Iteration 109/1000 | Loss: 0.00000992
Iteration 110/1000 | Loss: 0.00000992
Iteration 111/1000 | Loss: 0.00000992
Iteration 112/1000 | Loss: 0.00000992
Iteration 113/1000 | Loss: 0.00000992
Iteration 114/1000 | Loss: 0.00000992
Iteration 115/1000 | Loss: 0.00000992
Iteration 116/1000 | Loss: 0.00000992
Iteration 117/1000 | Loss: 0.00000992
Iteration 118/1000 | Loss: 0.00000992
Iteration 119/1000 | Loss: 0.00000992
Iteration 120/1000 | Loss: 0.00000992
Iteration 121/1000 | Loss: 0.00000992
Iteration 122/1000 | Loss: 0.00000992
Iteration 123/1000 | Loss: 0.00000992
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 123. Stopping optimization.
Last 5 losses: [9.918793693941552e-06, 9.918793693941552e-06, 9.918793693941552e-06, 9.918793693941552e-06, 9.918793693941552e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.918793693941552e-06

Optimization complete. Final v2v error: 2.6899020671844482 mm

Highest mean error: 2.9978744983673096 mm for frame 93

Lowest mean error: 2.493283987045288 mm for frame 187

Saving results

Total time: 38.954455614089966
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_023/1094/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_023/1094.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_023/1094
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00513988
Iteration 2/25 | Loss: 0.00134571
Iteration 3/25 | Loss: 0.00122407
Iteration 4/25 | Loss: 0.00120473
Iteration 5/25 | Loss: 0.00119581
Iteration 6/25 | Loss: 0.00119409
Iteration 7/25 | Loss: 0.00119409
Iteration 8/25 | Loss: 0.00119409
Iteration 9/25 | Loss: 0.00119409
Iteration 10/25 | Loss: 0.00119409
Iteration 11/25 | Loss: 0.00119409
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001194093027152121, 0.001194093027152121, 0.001194093027152121, 0.001194093027152121, 0.001194093027152121]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001194093027152121

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.76844668
Iteration 2/25 | Loss: 0.00075442
Iteration 3/25 | Loss: 0.00075442
Iteration 4/25 | Loss: 0.00075442
Iteration 5/25 | Loss: 0.00075442
Iteration 6/25 | Loss: 0.00075442
Iteration 7/25 | Loss: 0.00075442
Iteration 8/25 | Loss: 0.00075442
Iteration 9/25 | Loss: 0.00075442
Iteration 10/25 | Loss: 0.00075442
Iteration 11/25 | Loss: 0.00075442
Iteration 12/25 | Loss: 0.00075442
Iteration 13/25 | Loss: 0.00075442
Iteration 14/25 | Loss: 0.00075442
Iteration 15/25 | Loss: 0.00075442
Iteration 16/25 | Loss: 0.00075442
Iteration 17/25 | Loss: 0.00075442
Iteration 18/25 | Loss: 0.00075442
Iteration 19/25 | Loss: 0.00075442
Iteration 20/25 | Loss: 0.00075442
Iteration 21/25 | Loss: 0.00075442
Iteration 22/25 | Loss: 0.00075442
Iteration 23/25 | Loss: 0.00075442
Iteration 24/25 | Loss: 0.00075442
Iteration 25/25 | Loss: 0.00075442

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00075442
Iteration 2/1000 | Loss: 0.00003535
Iteration 3/1000 | Loss: 0.00002791
Iteration 4/1000 | Loss: 0.00002600
Iteration 5/1000 | Loss: 0.00002504
Iteration 6/1000 | Loss: 0.00002436
Iteration 7/1000 | Loss: 0.00002380
Iteration 8/1000 | Loss: 0.00002345
Iteration 9/1000 | Loss: 0.00002301
Iteration 10/1000 | Loss: 0.00002268
Iteration 11/1000 | Loss: 0.00002244
Iteration 12/1000 | Loss: 0.00002217
Iteration 13/1000 | Loss: 0.00002208
Iteration 14/1000 | Loss: 0.00002189
Iteration 15/1000 | Loss: 0.00002170
Iteration 16/1000 | Loss: 0.00002169
Iteration 17/1000 | Loss: 0.00002169
Iteration 18/1000 | Loss: 0.00002160
Iteration 19/1000 | Loss: 0.00002157
Iteration 20/1000 | Loss: 0.00002157
Iteration 21/1000 | Loss: 0.00002156
Iteration 22/1000 | Loss: 0.00002145
Iteration 23/1000 | Loss: 0.00002144
Iteration 24/1000 | Loss: 0.00002144
Iteration 25/1000 | Loss: 0.00002143
Iteration 26/1000 | Loss: 0.00002140
Iteration 27/1000 | Loss: 0.00002136
Iteration 28/1000 | Loss: 0.00002131
Iteration 29/1000 | Loss: 0.00002128
Iteration 30/1000 | Loss: 0.00002125
Iteration 31/1000 | Loss: 0.00002122
Iteration 32/1000 | Loss: 0.00002122
Iteration 33/1000 | Loss: 0.00002121
Iteration 34/1000 | Loss: 0.00002120
Iteration 35/1000 | Loss: 0.00002120
Iteration 36/1000 | Loss: 0.00002118
Iteration 37/1000 | Loss: 0.00002118
Iteration 38/1000 | Loss: 0.00002118
Iteration 39/1000 | Loss: 0.00002117
Iteration 40/1000 | Loss: 0.00002116
Iteration 41/1000 | Loss: 0.00002116
Iteration 42/1000 | Loss: 0.00002115
Iteration 43/1000 | Loss: 0.00002115
Iteration 44/1000 | Loss: 0.00002114
Iteration 45/1000 | Loss: 0.00002114
Iteration 46/1000 | Loss: 0.00002113
Iteration 47/1000 | Loss: 0.00002113
Iteration 48/1000 | Loss: 0.00002113
Iteration 49/1000 | Loss: 0.00002113
Iteration 50/1000 | Loss: 0.00002112
Iteration 51/1000 | Loss: 0.00002112
Iteration 52/1000 | Loss: 0.00002112
Iteration 53/1000 | Loss: 0.00002112
Iteration 54/1000 | Loss: 0.00002112
Iteration 55/1000 | Loss: 0.00002111
Iteration 56/1000 | Loss: 0.00002111
Iteration 57/1000 | Loss: 0.00002110
Iteration 58/1000 | Loss: 0.00002109
Iteration 59/1000 | Loss: 0.00002109
Iteration 60/1000 | Loss: 0.00002109
Iteration 61/1000 | Loss: 0.00002109
Iteration 62/1000 | Loss: 0.00002109
Iteration 63/1000 | Loss: 0.00002109
Iteration 64/1000 | Loss: 0.00002109
Iteration 65/1000 | Loss: 0.00002109
Iteration 66/1000 | Loss: 0.00002109
Iteration 67/1000 | Loss: 0.00002108
Iteration 68/1000 | Loss: 0.00002108
Iteration 69/1000 | Loss: 0.00002108
Iteration 70/1000 | Loss: 0.00002108
Iteration 71/1000 | Loss: 0.00002108
Iteration 72/1000 | Loss: 0.00002108
Iteration 73/1000 | Loss: 0.00002107
Iteration 74/1000 | Loss: 0.00002107
Iteration 75/1000 | Loss: 0.00002107
Iteration 76/1000 | Loss: 0.00002106
Iteration 77/1000 | Loss: 0.00002106
Iteration 78/1000 | Loss: 0.00002105
Iteration 79/1000 | Loss: 0.00002105
Iteration 80/1000 | Loss: 0.00002104
Iteration 81/1000 | Loss: 0.00002104
Iteration 82/1000 | Loss: 0.00002104
Iteration 83/1000 | Loss: 0.00002104
Iteration 84/1000 | Loss: 0.00002104
Iteration 85/1000 | Loss: 0.00002104
Iteration 86/1000 | Loss: 0.00002104
Iteration 87/1000 | Loss: 0.00002103
Iteration 88/1000 | Loss: 0.00002103
Iteration 89/1000 | Loss: 0.00002103
Iteration 90/1000 | Loss: 0.00002103
Iteration 91/1000 | Loss: 0.00002102
Iteration 92/1000 | Loss: 0.00002102
Iteration 93/1000 | Loss: 0.00002102
Iteration 94/1000 | Loss: 0.00002102
Iteration 95/1000 | Loss: 0.00002102
Iteration 96/1000 | Loss: 0.00002101
Iteration 97/1000 | Loss: 0.00002101
Iteration 98/1000 | Loss: 0.00002101
Iteration 99/1000 | Loss: 0.00002101
Iteration 100/1000 | Loss: 0.00002101
Iteration 101/1000 | Loss: 0.00002101
Iteration 102/1000 | Loss: 0.00002100
Iteration 103/1000 | Loss: 0.00002100
Iteration 104/1000 | Loss: 0.00002099
Iteration 105/1000 | Loss: 0.00002099
Iteration 106/1000 | Loss: 0.00002098
Iteration 107/1000 | Loss: 0.00002098
Iteration 108/1000 | Loss: 0.00002098
Iteration 109/1000 | Loss: 0.00002097
Iteration 110/1000 | Loss: 0.00002097
Iteration 111/1000 | Loss: 0.00002097
Iteration 112/1000 | Loss: 0.00002097
Iteration 113/1000 | Loss: 0.00002096
Iteration 114/1000 | Loss: 0.00002096
Iteration 115/1000 | Loss: 0.00002096
Iteration 116/1000 | Loss: 0.00002096
Iteration 117/1000 | Loss: 0.00002095
Iteration 118/1000 | Loss: 0.00002095
Iteration 119/1000 | Loss: 0.00002095
Iteration 120/1000 | Loss: 0.00002095
Iteration 121/1000 | Loss: 0.00002095
Iteration 122/1000 | Loss: 0.00002095
Iteration 123/1000 | Loss: 0.00002095
Iteration 124/1000 | Loss: 0.00002095
Iteration 125/1000 | Loss: 0.00002095
Iteration 126/1000 | Loss: 0.00002095
Iteration 127/1000 | Loss: 0.00002095
Iteration 128/1000 | Loss: 0.00002095
Iteration 129/1000 | Loss: 0.00002095
Iteration 130/1000 | Loss: 0.00002095
Iteration 131/1000 | Loss: 0.00002095
Iteration 132/1000 | Loss: 0.00002095
Iteration 133/1000 | Loss: 0.00002094
Iteration 134/1000 | Loss: 0.00002093
Iteration 135/1000 | Loss: 0.00002093
Iteration 136/1000 | Loss: 0.00002093
Iteration 137/1000 | Loss: 0.00002093
Iteration 138/1000 | Loss: 0.00002093
Iteration 139/1000 | Loss: 0.00002093
Iteration 140/1000 | Loss: 0.00002093
Iteration 141/1000 | Loss: 0.00002093
Iteration 142/1000 | Loss: 0.00002093
Iteration 143/1000 | Loss: 0.00002093
Iteration 144/1000 | Loss: 0.00002093
Iteration 145/1000 | Loss: 0.00002093
Iteration 146/1000 | Loss: 0.00002092
Iteration 147/1000 | Loss: 0.00002092
Iteration 148/1000 | Loss: 0.00002092
Iteration 149/1000 | Loss: 0.00002092
Iteration 150/1000 | Loss: 0.00002092
Iteration 151/1000 | Loss: 0.00002091
Iteration 152/1000 | Loss: 0.00002091
Iteration 153/1000 | Loss: 0.00002091
Iteration 154/1000 | Loss: 0.00002091
Iteration 155/1000 | Loss: 0.00002091
Iteration 156/1000 | Loss: 0.00002091
Iteration 157/1000 | Loss: 0.00002091
Iteration 158/1000 | Loss: 0.00002091
Iteration 159/1000 | Loss: 0.00002091
Iteration 160/1000 | Loss: 0.00002090
Iteration 161/1000 | Loss: 0.00002090
Iteration 162/1000 | Loss: 0.00002090
Iteration 163/1000 | Loss: 0.00002090
Iteration 164/1000 | Loss: 0.00002090
Iteration 165/1000 | Loss: 0.00002089
Iteration 166/1000 | Loss: 0.00002089
Iteration 167/1000 | Loss: 0.00002089
Iteration 168/1000 | Loss: 0.00002089
Iteration 169/1000 | Loss: 0.00002089
Iteration 170/1000 | Loss: 0.00002089
Iteration 171/1000 | Loss: 0.00002089
Iteration 172/1000 | Loss: 0.00002089
Iteration 173/1000 | Loss: 0.00002089
Iteration 174/1000 | Loss: 0.00002089
Iteration 175/1000 | Loss: 0.00002089
Iteration 176/1000 | Loss: 0.00002088
Iteration 177/1000 | Loss: 0.00002088
Iteration 178/1000 | Loss: 0.00002088
Iteration 179/1000 | Loss: 0.00002088
Iteration 180/1000 | Loss: 0.00002088
Iteration 181/1000 | Loss: 0.00002088
Iteration 182/1000 | Loss: 0.00002088
Iteration 183/1000 | Loss: 0.00002088
Iteration 184/1000 | Loss: 0.00002088
Iteration 185/1000 | Loss: 0.00002088
Iteration 186/1000 | Loss: 0.00002088
Iteration 187/1000 | Loss: 0.00002088
Iteration 188/1000 | Loss: 0.00002088
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 188. Stopping optimization.
Last 5 losses: [2.088451583404094e-05, 2.088451583404094e-05, 2.088451583404094e-05, 2.088451583404094e-05, 2.088451583404094e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.088451583404094e-05

Optimization complete. Final v2v error: 3.887561559677124 mm

Highest mean error: 4.222701072692871 mm for frame 5

Lowest mean error: 3.6621551513671875 mm for frame 50

Saving results

Total time: 54.49654507637024
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_023/1066/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_023/1066.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_023/1066
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00907688
Iteration 2/25 | Loss: 0.00129434
Iteration 3/25 | Loss: 0.00119974
Iteration 4/25 | Loss: 0.00117925
Iteration 5/25 | Loss: 0.00117218
Iteration 6/25 | Loss: 0.00117059
Iteration 7/25 | Loss: 0.00117058
Iteration 8/25 | Loss: 0.00117058
Iteration 9/25 | Loss: 0.00117058
Iteration 10/25 | Loss: 0.00117058
Iteration 11/25 | Loss: 0.00117058
Iteration 12/25 | Loss: 0.00117058
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0011705781798809767, 0.0011705781798809767, 0.0011705781798809767, 0.0011705781798809767, 0.0011705781798809767]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011705781798809767

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35978353
Iteration 2/25 | Loss: 0.00084141
Iteration 3/25 | Loss: 0.00084140
Iteration 4/25 | Loss: 0.00084140
Iteration 5/25 | Loss: 0.00084140
Iteration 6/25 | Loss: 0.00084139
Iteration 7/25 | Loss: 0.00084139
Iteration 8/25 | Loss: 0.00084139
Iteration 9/25 | Loss: 0.00084139
Iteration 10/25 | Loss: 0.00084139
Iteration 11/25 | Loss: 0.00084139
Iteration 12/25 | Loss: 0.00084139
Iteration 13/25 | Loss: 0.00084139
Iteration 14/25 | Loss: 0.00084139
Iteration 15/25 | Loss: 0.00084139
Iteration 16/25 | Loss: 0.00084139
Iteration 17/25 | Loss: 0.00084139
Iteration 18/25 | Loss: 0.00084139
Iteration 19/25 | Loss: 0.00084139
Iteration 20/25 | Loss: 0.00084139
Iteration 21/25 | Loss: 0.00084139
Iteration 22/25 | Loss: 0.00084139
Iteration 23/25 | Loss: 0.00084139
Iteration 24/25 | Loss: 0.00084139
Iteration 25/25 | Loss: 0.00084139

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00084139
Iteration 2/1000 | Loss: 0.00004448
Iteration 3/1000 | Loss: 0.00002946
Iteration 4/1000 | Loss: 0.00002364
Iteration 5/1000 | Loss: 0.00002144
Iteration 6/1000 | Loss: 0.00002046
Iteration 7/1000 | Loss: 0.00001985
Iteration 8/1000 | Loss: 0.00001936
Iteration 9/1000 | Loss: 0.00001887
Iteration 10/1000 | Loss: 0.00001860
Iteration 11/1000 | Loss: 0.00001860
Iteration 12/1000 | Loss: 0.00001838
Iteration 13/1000 | Loss: 0.00001825
Iteration 14/1000 | Loss: 0.00001821
Iteration 15/1000 | Loss: 0.00001817
Iteration 16/1000 | Loss: 0.00001816
Iteration 17/1000 | Loss: 0.00001815
Iteration 18/1000 | Loss: 0.00001815
Iteration 19/1000 | Loss: 0.00001814
Iteration 20/1000 | Loss: 0.00001813
Iteration 21/1000 | Loss: 0.00001811
Iteration 22/1000 | Loss: 0.00001810
Iteration 23/1000 | Loss: 0.00001809
Iteration 24/1000 | Loss: 0.00001808
Iteration 25/1000 | Loss: 0.00001803
Iteration 26/1000 | Loss: 0.00001803
Iteration 27/1000 | Loss: 0.00001802
Iteration 28/1000 | Loss: 0.00001802
Iteration 29/1000 | Loss: 0.00001801
Iteration 30/1000 | Loss: 0.00001800
Iteration 31/1000 | Loss: 0.00001796
Iteration 32/1000 | Loss: 0.00001796
Iteration 33/1000 | Loss: 0.00001790
Iteration 34/1000 | Loss: 0.00001790
Iteration 35/1000 | Loss: 0.00001789
Iteration 36/1000 | Loss: 0.00001789
Iteration 37/1000 | Loss: 0.00001788
Iteration 38/1000 | Loss: 0.00001785
Iteration 39/1000 | Loss: 0.00001785
Iteration 40/1000 | Loss: 0.00001785
Iteration 41/1000 | Loss: 0.00001784
Iteration 42/1000 | Loss: 0.00001784
Iteration 43/1000 | Loss: 0.00001783
Iteration 44/1000 | Loss: 0.00001781
Iteration 45/1000 | Loss: 0.00001781
Iteration 46/1000 | Loss: 0.00001781
Iteration 47/1000 | Loss: 0.00001781
Iteration 48/1000 | Loss: 0.00001781
Iteration 49/1000 | Loss: 0.00001781
Iteration 50/1000 | Loss: 0.00001781
Iteration 51/1000 | Loss: 0.00001781
Iteration 52/1000 | Loss: 0.00001780
Iteration 53/1000 | Loss: 0.00001780
Iteration 54/1000 | Loss: 0.00001780
Iteration 55/1000 | Loss: 0.00001780
Iteration 56/1000 | Loss: 0.00001780
Iteration 57/1000 | Loss: 0.00001780
Iteration 58/1000 | Loss: 0.00001780
Iteration 59/1000 | Loss: 0.00001780
Iteration 60/1000 | Loss: 0.00001780
Iteration 61/1000 | Loss: 0.00001780
Iteration 62/1000 | Loss: 0.00001780
Iteration 63/1000 | Loss: 0.00001780
Iteration 64/1000 | Loss: 0.00001779
Iteration 65/1000 | Loss: 0.00001779
Iteration 66/1000 | Loss: 0.00001779
Iteration 67/1000 | Loss: 0.00001779
Iteration 68/1000 | Loss: 0.00001779
Iteration 69/1000 | Loss: 0.00001778
Iteration 70/1000 | Loss: 0.00001777
Iteration 71/1000 | Loss: 0.00001777
Iteration 72/1000 | Loss: 0.00001777
Iteration 73/1000 | Loss: 0.00001777
Iteration 74/1000 | Loss: 0.00001776
Iteration 75/1000 | Loss: 0.00001775
Iteration 76/1000 | Loss: 0.00001775
Iteration 77/1000 | Loss: 0.00001774
Iteration 78/1000 | Loss: 0.00001773
Iteration 79/1000 | Loss: 0.00001773
Iteration 80/1000 | Loss: 0.00001773
Iteration 81/1000 | Loss: 0.00001772
Iteration 82/1000 | Loss: 0.00001772
Iteration 83/1000 | Loss: 0.00001772
Iteration 84/1000 | Loss: 0.00001772
Iteration 85/1000 | Loss: 0.00001772
Iteration 86/1000 | Loss: 0.00001772
Iteration 87/1000 | Loss: 0.00001772
Iteration 88/1000 | Loss: 0.00001772
Iteration 89/1000 | Loss: 0.00001772
Iteration 90/1000 | Loss: 0.00001772
Iteration 91/1000 | Loss: 0.00001772
Iteration 92/1000 | Loss: 0.00001771
Iteration 93/1000 | Loss: 0.00001771
Iteration 94/1000 | Loss: 0.00001771
Iteration 95/1000 | Loss: 0.00001769
Iteration 96/1000 | Loss: 0.00001769
Iteration 97/1000 | Loss: 0.00001769
Iteration 98/1000 | Loss: 0.00001769
Iteration 99/1000 | Loss: 0.00001769
Iteration 100/1000 | Loss: 0.00001769
Iteration 101/1000 | Loss: 0.00001768
Iteration 102/1000 | Loss: 0.00001768
Iteration 103/1000 | Loss: 0.00001768
Iteration 104/1000 | Loss: 0.00001768
Iteration 105/1000 | Loss: 0.00001768
Iteration 106/1000 | Loss: 0.00001768
Iteration 107/1000 | Loss: 0.00001768
Iteration 108/1000 | Loss: 0.00001768
Iteration 109/1000 | Loss: 0.00001768
Iteration 110/1000 | Loss: 0.00001768
Iteration 111/1000 | Loss: 0.00001768
Iteration 112/1000 | Loss: 0.00001768
Iteration 113/1000 | Loss: 0.00001768
Iteration 114/1000 | Loss: 0.00001767
Iteration 115/1000 | Loss: 0.00001767
Iteration 116/1000 | Loss: 0.00001767
Iteration 117/1000 | Loss: 0.00001765
Iteration 118/1000 | Loss: 0.00001765
Iteration 119/1000 | Loss: 0.00001765
Iteration 120/1000 | Loss: 0.00001765
Iteration 121/1000 | Loss: 0.00001765
Iteration 122/1000 | Loss: 0.00001765
Iteration 123/1000 | Loss: 0.00001765
Iteration 124/1000 | Loss: 0.00001765
Iteration 125/1000 | Loss: 0.00001765
Iteration 126/1000 | Loss: 0.00001765
Iteration 127/1000 | Loss: 0.00001765
Iteration 128/1000 | Loss: 0.00001765
Iteration 129/1000 | Loss: 0.00001765
Iteration 130/1000 | Loss: 0.00001765
Iteration 131/1000 | Loss: 0.00001765
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 131. Stopping optimization.
Last 5 losses: [1.7646960259298794e-05, 1.7646960259298794e-05, 1.7646960259298794e-05, 1.7646960259298794e-05, 1.7646960259298794e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7646960259298794e-05

Optimization complete. Final v2v error: 3.49240779876709 mm

Highest mean error: 5.433511734008789 mm for frame 69

Lowest mean error: 3.016172409057617 mm for frame 95

Saving results

Total time: 36.79434061050415
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_023/1046/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_023/1046.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_023/1046
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00758338
Iteration 2/25 | Loss: 0.00126964
Iteration 3/25 | Loss: 0.00117904
Iteration 4/25 | Loss: 0.00115830
Iteration 5/25 | Loss: 0.00117519
Iteration 6/25 | Loss: 0.00115642
Iteration 7/25 | Loss: 0.00113815
Iteration 8/25 | Loss: 0.00112336
Iteration 9/25 | Loss: 0.00111887
Iteration 10/25 | Loss: 0.00111756
Iteration 11/25 | Loss: 0.00112146
Iteration 12/25 | Loss: 0.00111739
Iteration 13/25 | Loss: 0.00111648
Iteration 14/25 | Loss: 0.00111445
Iteration 15/25 | Loss: 0.00111479
Iteration 16/25 | Loss: 0.00111511
Iteration 17/25 | Loss: 0.00111472
Iteration 18/25 | Loss: 0.00111504
Iteration 19/25 | Loss: 0.00111465
Iteration 20/25 | Loss: 0.00111422
Iteration 21/25 | Loss: 0.00111444
Iteration 22/25 | Loss: 0.00111452
Iteration 23/25 | Loss: 0.00111457
Iteration 24/25 | Loss: 0.00111439
Iteration 25/25 | Loss: 0.00111437

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35345757
Iteration 2/25 | Loss: 0.00087899
Iteration 3/25 | Loss: 0.00087899
Iteration 4/25 | Loss: 0.00087899
Iteration 5/25 | Loss: 0.00087899
Iteration 6/25 | Loss: 0.00087899
Iteration 7/25 | Loss: 0.00087899
Iteration 8/25 | Loss: 0.00087899
Iteration 9/25 | Loss: 0.00087899
Iteration 10/25 | Loss: 0.00087899
Iteration 11/25 | Loss: 0.00087898
Iteration 12/25 | Loss: 0.00087898
Iteration 13/25 | Loss: 0.00087898
Iteration 14/25 | Loss: 0.00087898
Iteration 15/25 | Loss: 0.00087898
Iteration 16/25 | Loss: 0.00087898
Iteration 17/25 | Loss: 0.00087898
Iteration 18/25 | Loss: 0.00087898
Iteration 19/25 | Loss: 0.00087898
Iteration 20/25 | Loss: 0.00087898
Iteration 21/25 | Loss: 0.00087898
Iteration 22/25 | Loss: 0.00087898
Iteration 23/25 | Loss: 0.00087898
Iteration 24/25 | Loss: 0.00087898
Iteration 25/25 | Loss: 0.00087898

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00087898
Iteration 2/1000 | Loss: 0.00002734
Iteration 3/1000 | Loss: 0.00002980
Iteration 4/1000 | Loss: 0.00002103
Iteration 5/1000 | Loss: 0.00003754
Iteration 6/1000 | Loss: 0.00001994
Iteration 7/1000 | Loss: 0.00001744
Iteration 8/1000 | Loss: 0.00001652
Iteration 9/1000 | Loss: 0.00001579
Iteration 10/1000 | Loss: 0.00001528
Iteration 11/1000 | Loss: 0.00001452
Iteration 12/1000 | Loss: 0.00001411
Iteration 13/1000 | Loss: 0.00001385
Iteration 14/1000 | Loss: 0.00001371
Iteration 15/1000 | Loss: 0.00001340
Iteration 16/1000 | Loss: 0.00001314
Iteration 17/1000 | Loss: 0.00001300
Iteration 18/1000 | Loss: 0.00001292
Iteration 19/1000 | Loss: 0.00001282
Iteration 20/1000 | Loss: 0.00001282
Iteration 21/1000 | Loss: 0.00001281
Iteration 22/1000 | Loss: 0.00001280
Iteration 23/1000 | Loss: 0.00001279
Iteration 24/1000 | Loss: 0.00001278
Iteration 25/1000 | Loss: 0.00001277
Iteration 26/1000 | Loss: 0.00001277
Iteration 27/1000 | Loss: 0.00001277
Iteration 28/1000 | Loss: 0.00001276
Iteration 29/1000 | Loss: 0.00001276
Iteration 30/1000 | Loss: 0.00001276
Iteration 31/1000 | Loss: 0.00001275
Iteration 32/1000 | Loss: 0.00001275
Iteration 33/1000 | Loss: 0.00001275
Iteration 34/1000 | Loss: 0.00001275
Iteration 35/1000 | Loss: 0.00001274
Iteration 36/1000 | Loss: 0.00001274
Iteration 37/1000 | Loss: 0.00001274
Iteration 38/1000 | Loss: 0.00001274
Iteration 39/1000 | Loss: 0.00001274
Iteration 40/1000 | Loss: 0.00001274
Iteration 41/1000 | Loss: 0.00001273
Iteration 42/1000 | Loss: 0.00001273
Iteration 43/1000 | Loss: 0.00001272
Iteration 44/1000 | Loss: 0.00001272
Iteration 45/1000 | Loss: 0.00001271
Iteration 46/1000 | Loss: 0.00001271
Iteration 47/1000 | Loss: 0.00001270
Iteration 48/1000 | Loss: 0.00001268
Iteration 49/1000 | Loss: 0.00001267
Iteration 50/1000 | Loss: 0.00001266
Iteration 51/1000 | Loss: 0.00001265
Iteration 52/1000 | Loss: 0.00001265
Iteration 53/1000 | Loss: 0.00001265
Iteration 54/1000 | Loss: 0.00001264
Iteration 55/1000 | Loss: 0.00001264
Iteration 56/1000 | Loss: 0.00001264
Iteration 57/1000 | Loss: 0.00001264
Iteration 58/1000 | Loss: 0.00001263
Iteration 59/1000 | Loss: 0.00001263
Iteration 60/1000 | Loss: 0.00001263
Iteration 61/1000 | Loss: 0.00001263
Iteration 62/1000 | Loss: 0.00001262
Iteration 63/1000 | Loss: 0.00001262
Iteration 64/1000 | Loss: 0.00001262
Iteration 65/1000 | Loss: 0.00001262
Iteration 66/1000 | Loss: 0.00001261
Iteration 67/1000 | Loss: 0.00001261
Iteration 68/1000 | Loss: 0.00001261
Iteration 69/1000 | Loss: 0.00001260
Iteration 70/1000 | Loss: 0.00001260
Iteration 71/1000 | Loss: 0.00001260
Iteration 72/1000 | Loss: 0.00001260
Iteration 73/1000 | Loss: 0.00001260
Iteration 74/1000 | Loss: 0.00001259
Iteration 75/1000 | Loss: 0.00001259
Iteration 76/1000 | Loss: 0.00001259
Iteration 77/1000 | Loss: 0.00001259
Iteration 78/1000 | Loss: 0.00001259
Iteration 79/1000 | Loss: 0.00001258
Iteration 80/1000 | Loss: 0.00001258
Iteration 81/1000 | Loss: 0.00001258
Iteration 82/1000 | Loss: 0.00001257
Iteration 83/1000 | Loss: 0.00001257
Iteration 84/1000 | Loss: 0.00001255
Iteration 85/1000 | Loss: 0.00001255
Iteration 86/1000 | Loss: 0.00001255
Iteration 87/1000 | Loss: 0.00001254
Iteration 88/1000 | Loss: 0.00001254
Iteration 89/1000 | Loss: 0.00001251
Iteration 90/1000 | Loss: 0.00001251
Iteration 91/1000 | Loss: 0.00001251
Iteration 92/1000 | Loss: 0.00001251
Iteration 93/1000 | Loss: 0.00001251
Iteration 94/1000 | Loss: 0.00001250
Iteration 95/1000 | Loss: 0.00001250
Iteration 96/1000 | Loss: 0.00001249
Iteration 97/1000 | Loss: 0.00001249
Iteration 98/1000 | Loss: 0.00001249
Iteration 99/1000 | Loss: 0.00001249
Iteration 100/1000 | Loss: 0.00001249
Iteration 101/1000 | Loss: 0.00001249
Iteration 102/1000 | Loss: 0.00001249
Iteration 103/1000 | Loss: 0.00001248
Iteration 104/1000 | Loss: 0.00001248
Iteration 105/1000 | Loss: 0.00001248
Iteration 106/1000 | Loss: 0.00001248
Iteration 107/1000 | Loss: 0.00001248
Iteration 108/1000 | Loss: 0.00001248
Iteration 109/1000 | Loss: 0.00001247
Iteration 110/1000 | Loss: 0.00001247
Iteration 111/1000 | Loss: 0.00001247
Iteration 112/1000 | Loss: 0.00001247
Iteration 113/1000 | Loss: 0.00001246
Iteration 114/1000 | Loss: 0.00001246
Iteration 115/1000 | Loss: 0.00001246
Iteration 116/1000 | Loss: 0.00001245
Iteration 117/1000 | Loss: 0.00001245
Iteration 118/1000 | Loss: 0.00001245
Iteration 119/1000 | Loss: 0.00001245
Iteration 120/1000 | Loss: 0.00001245
Iteration 121/1000 | Loss: 0.00001245
Iteration 122/1000 | Loss: 0.00001244
Iteration 123/1000 | Loss: 0.00001244
Iteration 124/1000 | Loss: 0.00001244
Iteration 125/1000 | Loss: 0.00001244
Iteration 126/1000 | Loss: 0.00001244
Iteration 127/1000 | Loss: 0.00001244
Iteration 128/1000 | Loss: 0.00001244
Iteration 129/1000 | Loss: 0.00001243
Iteration 130/1000 | Loss: 0.00001243
Iteration 131/1000 | Loss: 0.00001243
Iteration 132/1000 | Loss: 0.00001242
Iteration 133/1000 | Loss: 0.00001242
Iteration 134/1000 | Loss: 0.00001242
Iteration 135/1000 | Loss: 0.00001242
Iteration 136/1000 | Loss: 0.00001241
Iteration 137/1000 | Loss: 0.00001241
Iteration 138/1000 | Loss: 0.00001240
Iteration 139/1000 | Loss: 0.00001240
Iteration 140/1000 | Loss: 0.00001239
Iteration 141/1000 | Loss: 0.00001239
Iteration 142/1000 | Loss: 0.00001239
Iteration 143/1000 | Loss: 0.00001239
Iteration 144/1000 | Loss: 0.00001238
Iteration 145/1000 | Loss: 0.00001238
Iteration 146/1000 | Loss: 0.00001238
Iteration 147/1000 | Loss: 0.00001238
Iteration 148/1000 | Loss: 0.00001238
Iteration 149/1000 | Loss: 0.00001237
Iteration 150/1000 | Loss: 0.00001237
Iteration 151/1000 | Loss: 0.00001237
Iteration 152/1000 | Loss: 0.00001237
Iteration 153/1000 | Loss: 0.00001237
Iteration 154/1000 | Loss: 0.00001237
Iteration 155/1000 | Loss: 0.00001237
Iteration 156/1000 | Loss: 0.00001237
Iteration 157/1000 | Loss: 0.00001236
Iteration 158/1000 | Loss: 0.00001236
Iteration 159/1000 | Loss: 0.00001236
Iteration 160/1000 | Loss: 0.00001236
Iteration 161/1000 | Loss: 0.00001236
Iteration 162/1000 | Loss: 0.00001236
Iteration 163/1000 | Loss: 0.00001236
Iteration 164/1000 | Loss: 0.00001236
Iteration 165/1000 | Loss: 0.00001236
Iteration 166/1000 | Loss: 0.00001236
Iteration 167/1000 | Loss: 0.00001236
Iteration 168/1000 | Loss: 0.00001235
Iteration 169/1000 | Loss: 0.00001235
Iteration 170/1000 | Loss: 0.00001235
Iteration 171/1000 | Loss: 0.00001235
Iteration 172/1000 | Loss: 0.00001235
Iteration 173/1000 | Loss: 0.00001235
Iteration 174/1000 | Loss: 0.00001235
Iteration 175/1000 | Loss: 0.00001235
Iteration 176/1000 | Loss: 0.00001235
Iteration 177/1000 | Loss: 0.00001235
Iteration 178/1000 | Loss: 0.00001235
Iteration 179/1000 | Loss: 0.00001235
Iteration 180/1000 | Loss: 0.00001235
Iteration 181/1000 | Loss: 0.00001235
Iteration 182/1000 | Loss: 0.00001235
Iteration 183/1000 | Loss: 0.00001235
Iteration 184/1000 | Loss: 0.00001234
Iteration 185/1000 | Loss: 0.00001234
Iteration 186/1000 | Loss: 0.00001234
Iteration 187/1000 | Loss: 0.00001234
Iteration 188/1000 | Loss: 0.00001234
Iteration 189/1000 | Loss: 0.00001234
Iteration 190/1000 | Loss: 0.00001234
Iteration 191/1000 | Loss: 0.00001234
Iteration 192/1000 | Loss: 0.00001234
Iteration 193/1000 | Loss: 0.00001234
Iteration 194/1000 | Loss: 0.00001234
Iteration 195/1000 | Loss: 0.00001234
Iteration 196/1000 | Loss: 0.00001234
Iteration 197/1000 | Loss: 0.00001234
Iteration 198/1000 | Loss: 0.00001234
Iteration 199/1000 | Loss: 0.00001233
Iteration 200/1000 | Loss: 0.00001233
Iteration 201/1000 | Loss: 0.00001233
Iteration 202/1000 | Loss: 0.00001233
Iteration 203/1000 | Loss: 0.00001233
Iteration 204/1000 | Loss: 0.00001233
Iteration 205/1000 | Loss: 0.00001233
Iteration 206/1000 | Loss: 0.00001233
Iteration 207/1000 | Loss: 0.00001233
Iteration 208/1000 | Loss: 0.00001233
Iteration 209/1000 | Loss: 0.00001233
Iteration 210/1000 | Loss: 0.00001233
Iteration 211/1000 | Loss: 0.00001233
Iteration 212/1000 | Loss: 0.00001233
Iteration 213/1000 | Loss: 0.00001233
Iteration 214/1000 | Loss: 0.00001233
Iteration 215/1000 | Loss: 0.00001232
Iteration 216/1000 | Loss: 0.00001232
Iteration 217/1000 | Loss: 0.00001232
Iteration 218/1000 | Loss: 0.00001232
Iteration 219/1000 | Loss: 0.00001232
Iteration 220/1000 | Loss: 0.00001232
Iteration 221/1000 | Loss: 0.00001232
Iteration 222/1000 | Loss: 0.00001232
Iteration 223/1000 | Loss: 0.00001232
Iteration 224/1000 | Loss: 0.00001232
Iteration 225/1000 | Loss: 0.00001232
Iteration 226/1000 | Loss: 0.00001231
Iteration 227/1000 | Loss: 0.00001231
Iteration 228/1000 | Loss: 0.00001231
Iteration 229/1000 | Loss: 0.00001231
Iteration 230/1000 | Loss: 0.00001231
Iteration 231/1000 | Loss: 0.00001231
Iteration 232/1000 | Loss: 0.00001231
Iteration 233/1000 | Loss: 0.00001231
Iteration 234/1000 | Loss: 0.00001231
Iteration 235/1000 | Loss: 0.00001231
Iteration 236/1000 | Loss: 0.00001231
Iteration 237/1000 | Loss: 0.00001231
Iteration 238/1000 | Loss: 0.00001231
Iteration 239/1000 | Loss: 0.00001231
Iteration 240/1000 | Loss: 0.00001231
Iteration 241/1000 | Loss: 0.00001231
Iteration 242/1000 | Loss: 0.00001231
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 242. Stopping optimization.
Last 5 losses: [1.2313990737311542e-05, 1.2313990737311542e-05, 1.2313990737311542e-05, 1.2313990737311542e-05, 1.2313990737311542e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2313990737311542e-05

Optimization complete. Final v2v error: 2.9950637817382812 mm

Highest mean error: 4.1722846031188965 mm for frame 95

Lowest mean error: 2.489323377609253 mm for frame 203

Saving results

Total time: 99.35073852539062
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_023/1087/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_023/1087.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_023/1087
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00821854
Iteration 2/25 | Loss: 0.00120523
Iteration 3/25 | Loss: 0.00114433
Iteration 4/25 | Loss: 0.00113345
Iteration 5/25 | Loss: 0.00113105
Iteration 6/25 | Loss: 0.00113105
Iteration 7/25 | Loss: 0.00113105
Iteration 8/25 | Loss: 0.00113105
Iteration 9/25 | Loss: 0.00113105
Iteration 10/25 | Loss: 0.00113105
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0011310460977256298, 0.0011310460977256298, 0.0011310460977256298, 0.0011310460977256298, 0.0011310460977256298]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011310460977256298

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.02224541
Iteration 2/25 | Loss: 0.00087663
Iteration 3/25 | Loss: 0.00087662
Iteration 4/25 | Loss: 0.00087662
Iteration 5/25 | Loss: 0.00087661
Iteration 6/25 | Loss: 0.00087661
Iteration 7/25 | Loss: 0.00087661
Iteration 8/25 | Loss: 0.00087661
Iteration 9/25 | Loss: 0.00087661
Iteration 10/25 | Loss: 0.00087661
Iteration 11/25 | Loss: 0.00087661
Iteration 12/25 | Loss: 0.00087661
Iteration 13/25 | Loss: 0.00087661
Iteration 14/25 | Loss: 0.00087661
Iteration 15/25 | Loss: 0.00087661
Iteration 16/25 | Loss: 0.00087661
Iteration 17/25 | Loss: 0.00087661
Iteration 18/25 | Loss: 0.00087661
Iteration 19/25 | Loss: 0.00087661
Iteration 20/25 | Loss: 0.00087661
Iteration 21/25 | Loss: 0.00087661
Iteration 22/25 | Loss: 0.00087661
Iteration 23/25 | Loss: 0.00087661
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0008766124956309795, 0.0008766124956309795, 0.0008766124956309795, 0.0008766124956309795, 0.0008766124956309795]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008766124956309795

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00087661
Iteration 2/1000 | Loss: 0.00002345
Iteration 3/1000 | Loss: 0.00001849
Iteration 4/1000 | Loss: 0.00001717
Iteration 5/1000 | Loss: 0.00001627
Iteration 6/1000 | Loss: 0.00001549
Iteration 7/1000 | Loss: 0.00001506
Iteration 8/1000 | Loss: 0.00001471
Iteration 9/1000 | Loss: 0.00001433
Iteration 10/1000 | Loss: 0.00001432
Iteration 11/1000 | Loss: 0.00001431
Iteration 12/1000 | Loss: 0.00001427
Iteration 13/1000 | Loss: 0.00001413
Iteration 14/1000 | Loss: 0.00001410
Iteration 15/1000 | Loss: 0.00001395
Iteration 16/1000 | Loss: 0.00001394
Iteration 17/1000 | Loss: 0.00001393
Iteration 18/1000 | Loss: 0.00001388
Iteration 19/1000 | Loss: 0.00001388
Iteration 20/1000 | Loss: 0.00001379
Iteration 21/1000 | Loss: 0.00001374
Iteration 22/1000 | Loss: 0.00001373
Iteration 23/1000 | Loss: 0.00001372
Iteration 24/1000 | Loss: 0.00001369
Iteration 25/1000 | Loss: 0.00001367
Iteration 26/1000 | Loss: 0.00001366
Iteration 27/1000 | Loss: 0.00001365
Iteration 28/1000 | Loss: 0.00001364
Iteration 29/1000 | Loss: 0.00001363
Iteration 30/1000 | Loss: 0.00001354
Iteration 31/1000 | Loss: 0.00001351
Iteration 32/1000 | Loss: 0.00001350
Iteration 33/1000 | Loss: 0.00001345
Iteration 34/1000 | Loss: 0.00001344
Iteration 35/1000 | Loss: 0.00001344
Iteration 36/1000 | Loss: 0.00001343
Iteration 37/1000 | Loss: 0.00001343
Iteration 38/1000 | Loss: 0.00001341
Iteration 39/1000 | Loss: 0.00001341
Iteration 40/1000 | Loss: 0.00001340
Iteration 41/1000 | Loss: 0.00001338
Iteration 42/1000 | Loss: 0.00001338
Iteration 43/1000 | Loss: 0.00001337
Iteration 44/1000 | Loss: 0.00001337
Iteration 45/1000 | Loss: 0.00001337
Iteration 46/1000 | Loss: 0.00001337
Iteration 47/1000 | Loss: 0.00001336
Iteration 48/1000 | Loss: 0.00001335
Iteration 49/1000 | Loss: 0.00001335
Iteration 50/1000 | Loss: 0.00001333
Iteration 51/1000 | Loss: 0.00001333
Iteration 52/1000 | Loss: 0.00001332
Iteration 53/1000 | Loss: 0.00001332
Iteration 54/1000 | Loss: 0.00001332
Iteration 55/1000 | Loss: 0.00001331
Iteration 56/1000 | Loss: 0.00001331
Iteration 57/1000 | Loss: 0.00001330
Iteration 58/1000 | Loss: 0.00001330
Iteration 59/1000 | Loss: 0.00001330
Iteration 60/1000 | Loss: 0.00001329
Iteration 61/1000 | Loss: 0.00001329
Iteration 62/1000 | Loss: 0.00001328
Iteration 63/1000 | Loss: 0.00001328
Iteration 64/1000 | Loss: 0.00001327
Iteration 65/1000 | Loss: 0.00001327
Iteration 66/1000 | Loss: 0.00001327
Iteration 67/1000 | Loss: 0.00001327
Iteration 68/1000 | Loss: 0.00001326
Iteration 69/1000 | Loss: 0.00001326
Iteration 70/1000 | Loss: 0.00001326
Iteration 71/1000 | Loss: 0.00001326
Iteration 72/1000 | Loss: 0.00001325
Iteration 73/1000 | Loss: 0.00001325
Iteration 74/1000 | Loss: 0.00001324
Iteration 75/1000 | Loss: 0.00001324
Iteration 76/1000 | Loss: 0.00001323
Iteration 77/1000 | Loss: 0.00001323
Iteration 78/1000 | Loss: 0.00001323
Iteration 79/1000 | Loss: 0.00001323
Iteration 80/1000 | Loss: 0.00001322
Iteration 81/1000 | Loss: 0.00001322
Iteration 82/1000 | Loss: 0.00001322
Iteration 83/1000 | Loss: 0.00001321
Iteration 84/1000 | Loss: 0.00001321
Iteration 85/1000 | Loss: 0.00001321
Iteration 86/1000 | Loss: 0.00001320
Iteration 87/1000 | Loss: 0.00001320
Iteration 88/1000 | Loss: 0.00001319
Iteration 89/1000 | Loss: 0.00001319
Iteration 90/1000 | Loss: 0.00001319
Iteration 91/1000 | Loss: 0.00001319
Iteration 92/1000 | Loss: 0.00001319
Iteration 93/1000 | Loss: 0.00001319
Iteration 94/1000 | Loss: 0.00001319
Iteration 95/1000 | Loss: 0.00001319
Iteration 96/1000 | Loss: 0.00001319
Iteration 97/1000 | Loss: 0.00001319
Iteration 98/1000 | Loss: 0.00001319
Iteration 99/1000 | Loss: 0.00001318
Iteration 100/1000 | Loss: 0.00001318
Iteration 101/1000 | Loss: 0.00001318
Iteration 102/1000 | Loss: 0.00001318
Iteration 103/1000 | Loss: 0.00001318
Iteration 104/1000 | Loss: 0.00001318
Iteration 105/1000 | Loss: 0.00001317
Iteration 106/1000 | Loss: 0.00001317
Iteration 107/1000 | Loss: 0.00001317
Iteration 108/1000 | Loss: 0.00001317
Iteration 109/1000 | Loss: 0.00001317
Iteration 110/1000 | Loss: 0.00001317
Iteration 111/1000 | Loss: 0.00001317
Iteration 112/1000 | Loss: 0.00001317
Iteration 113/1000 | Loss: 0.00001317
Iteration 114/1000 | Loss: 0.00001317
Iteration 115/1000 | Loss: 0.00001317
Iteration 116/1000 | Loss: 0.00001317
Iteration 117/1000 | Loss: 0.00001317
Iteration 118/1000 | Loss: 0.00001317
Iteration 119/1000 | Loss: 0.00001317
Iteration 120/1000 | Loss: 0.00001317
Iteration 121/1000 | Loss: 0.00001317
Iteration 122/1000 | Loss: 0.00001317
Iteration 123/1000 | Loss: 0.00001317
Iteration 124/1000 | Loss: 0.00001317
Iteration 125/1000 | Loss: 0.00001317
Iteration 126/1000 | Loss: 0.00001317
Iteration 127/1000 | Loss: 0.00001317
Iteration 128/1000 | Loss: 0.00001317
Iteration 129/1000 | Loss: 0.00001317
Iteration 130/1000 | Loss: 0.00001317
Iteration 131/1000 | Loss: 0.00001317
Iteration 132/1000 | Loss: 0.00001317
Iteration 133/1000 | Loss: 0.00001317
Iteration 134/1000 | Loss: 0.00001317
Iteration 135/1000 | Loss: 0.00001317
Iteration 136/1000 | Loss: 0.00001317
Iteration 137/1000 | Loss: 0.00001317
Iteration 138/1000 | Loss: 0.00001317
Iteration 139/1000 | Loss: 0.00001317
Iteration 140/1000 | Loss: 0.00001317
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 140. Stopping optimization.
Last 5 losses: [1.316667203354882e-05, 1.316667203354882e-05, 1.316667203354882e-05, 1.316667203354882e-05, 1.316667203354882e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.316667203354882e-05

Optimization complete. Final v2v error: 3.1008479595184326 mm

Highest mean error: 3.7593753337860107 mm for frame 140

Lowest mean error: 2.6751489639282227 mm for frame 21

Saving results

Total time: 43.35726070404053
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_023/1035/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_023/1035.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_023/1035
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00348670
Iteration 2/25 | Loss: 0.00118320
Iteration 3/25 | Loss: 0.00110302
Iteration 4/25 | Loss: 0.00109297
Iteration 5/25 | Loss: 0.00108983
Iteration 6/25 | Loss: 0.00108872
Iteration 7/25 | Loss: 0.00108872
Iteration 8/25 | Loss: 0.00108872
Iteration 9/25 | Loss: 0.00108872
Iteration 10/25 | Loss: 0.00108872
Iteration 11/25 | Loss: 0.00108872
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010887205135077238, 0.0010887205135077238, 0.0010887205135077238, 0.0010887205135077238, 0.0010887205135077238]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010887205135077238

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36809719
Iteration 2/25 | Loss: 0.00099123
Iteration 3/25 | Loss: 0.00099123
Iteration 4/25 | Loss: 0.00099122
Iteration 5/25 | Loss: 0.00099122
Iteration 6/25 | Loss: 0.00099122
Iteration 7/25 | Loss: 0.00099122
Iteration 8/25 | Loss: 0.00099122
Iteration 9/25 | Loss: 0.00099122
Iteration 10/25 | Loss: 0.00099122
Iteration 11/25 | Loss: 0.00099122
Iteration 12/25 | Loss: 0.00099122
Iteration 13/25 | Loss: 0.00099122
Iteration 14/25 | Loss: 0.00099122
Iteration 15/25 | Loss: 0.00099122
Iteration 16/25 | Loss: 0.00099122
Iteration 17/25 | Loss: 0.00099122
Iteration 18/25 | Loss: 0.00099122
Iteration 19/25 | Loss: 0.00099122
Iteration 20/25 | Loss: 0.00099122
Iteration 21/25 | Loss: 0.00099122
Iteration 22/25 | Loss: 0.00099122
Iteration 23/25 | Loss: 0.00099122
Iteration 24/25 | Loss: 0.00099122
Iteration 25/25 | Loss: 0.00099122

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00099122
Iteration 2/1000 | Loss: 0.00002265
Iteration 3/1000 | Loss: 0.00001525
Iteration 4/1000 | Loss: 0.00001370
Iteration 5/1000 | Loss: 0.00001282
Iteration 6/1000 | Loss: 0.00001235
Iteration 7/1000 | Loss: 0.00001201
Iteration 8/1000 | Loss: 0.00001184
Iteration 9/1000 | Loss: 0.00001167
Iteration 10/1000 | Loss: 0.00001151
Iteration 11/1000 | Loss: 0.00001151
Iteration 12/1000 | Loss: 0.00001150
Iteration 13/1000 | Loss: 0.00001146
Iteration 14/1000 | Loss: 0.00001145
Iteration 15/1000 | Loss: 0.00001141
Iteration 16/1000 | Loss: 0.00001141
Iteration 17/1000 | Loss: 0.00001136
Iteration 18/1000 | Loss: 0.00001136
Iteration 19/1000 | Loss: 0.00001132
Iteration 20/1000 | Loss: 0.00001129
Iteration 21/1000 | Loss: 0.00001129
Iteration 22/1000 | Loss: 0.00001129
Iteration 23/1000 | Loss: 0.00001129
Iteration 24/1000 | Loss: 0.00001128
Iteration 25/1000 | Loss: 0.00001128
Iteration 26/1000 | Loss: 0.00001128
Iteration 27/1000 | Loss: 0.00001128
Iteration 28/1000 | Loss: 0.00001127
Iteration 29/1000 | Loss: 0.00001119
Iteration 30/1000 | Loss: 0.00001119
Iteration 31/1000 | Loss: 0.00001115
Iteration 32/1000 | Loss: 0.00001115
Iteration 33/1000 | Loss: 0.00001114
Iteration 34/1000 | Loss: 0.00001113
Iteration 35/1000 | Loss: 0.00001112
Iteration 36/1000 | Loss: 0.00001112
Iteration 37/1000 | Loss: 0.00001111
Iteration 38/1000 | Loss: 0.00001110
Iteration 39/1000 | Loss: 0.00001110
Iteration 40/1000 | Loss: 0.00001109
Iteration 41/1000 | Loss: 0.00001109
Iteration 42/1000 | Loss: 0.00001109
Iteration 43/1000 | Loss: 0.00001108
Iteration 44/1000 | Loss: 0.00001107
Iteration 45/1000 | Loss: 0.00001107
Iteration 46/1000 | Loss: 0.00001107
Iteration 47/1000 | Loss: 0.00001106
Iteration 48/1000 | Loss: 0.00001106
Iteration 49/1000 | Loss: 0.00001106
Iteration 50/1000 | Loss: 0.00001105
Iteration 51/1000 | Loss: 0.00001105
Iteration 52/1000 | Loss: 0.00001104
Iteration 53/1000 | Loss: 0.00001104
Iteration 54/1000 | Loss: 0.00001104
Iteration 55/1000 | Loss: 0.00001103
Iteration 56/1000 | Loss: 0.00001103
Iteration 57/1000 | Loss: 0.00001102
Iteration 58/1000 | Loss: 0.00001102
Iteration 59/1000 | Loss: 0.00001102
Iteration 60/1000 | Loss: 0.00001101
Iteration 61/1000 | Loss: 0.00001101
Iteration 62/1000 | Loss: 0.00001101
Iteration 63/1000 | Loss: 0.00001101
Iteration 64/1000 | Loss: 0.00001100
Iteration 65/1000 | Loss: 0.00001100
Iteration 66/1000 | Loss: 0.00001100
Iteration 67/1000 | Loss: 0.00001100
Iteration 68/1000 | Loss: 0.00001100
Iteration 69/1000 | Loss: 0.00001100
Iteration 70/1000 | Loss: 0.00001100
Iteration 71/1000 | Loss: 0.00001100
Iteration 72/1000 | Loss: 0.00001099
Iteration 73/1000 | Loss: 0.00001099
Iteration 74/1000 | Loss: 0.00001099
Iteration 75/1000 | Loss: 0.00001099
Iteration 76/1000 | Loss: 0.00001099
Iteration 77/1000 | Loss: 0.00001099
Iteration 78/1000 | Loss: 0.00001099
Iteration 79/1000 | Loss: 0.00001099
Iteration 80/1000 | Loss: 0.00001099
Iteration 81/1000 | Loss: 0.00001099
Iteration 82/1000 | Loss: 0.00001099
Iteration 83/1000 | Loss: 0.00001099
Iteration 84/1000 | Loss: 0.00001098
Iteration 85/1000 | Loss: 0.00001098
Iteration 86/1000 | Loss: 0.00001098
Iteration 87/1000 | Loss: 0.00001098
Iteration 88/1000 | Loss: 0.00001098
Iteration 89/1000 | Loss: 0.00001098
Iteration 90/1000 | Loss: 0.00001098
Iteration 91/1000 | Loss: 0.00001098
Iteration 92/1000 | Loss: 0.00001098
Iteration 93/1000 | Loss: 0.00001098
Iteration 94/1000 | Loss: 0.00001098
Iteration 95/1000 | Loss: 0.00001098
Iteration 96/1000 | Loss: 0.00001098
Iteration 97/1000 | Loss: 0.00001098
Iteration 98/1000 | Loss: 0.00001098
Iteration 99/1000 | Loss: 0.00001098
Iteration 100/1000 | Loss: 0.00001098
Iteration 101/1000 | Loss: 0.00001098
Iteration 102/1000 | Loss: 0.00001098
Iteration 103/1000 | Loss: 0.00001098
Iteration 104/1000 | Loss: 0.00001098
Iteration 105/1000 | Loss: 0.00001098
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 105. Stopping optimization.
Last 5 losses: [1.0984775144606829e-05, 1.0984775144606829e-05, 1.0984775144606829e-05, 1.0984775144606829e-05, 1.0984775144606829e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0984775144606829e-05

Optimization complete. Final v2v error: 2.826058864593506 mm

Highest mean error: 3.6637613773345947 mm for frame 25

Lowest mean error: 2.3501338958740234 mm for frame 132

Saving results

Total time: 31.61428689956665
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_023/1093/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_023/1093.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_023/1093
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00881844
Iteration 2/25 | Loss: 0.00146017
Iteration 3/25 | Loss: 0.00126292
Iteration 4/25 | Loss: 0.00124932
Iteration 5/25 | Loss: 0.00124192
Iteration 6/25 | Loss: 0.00124607
Iteration 7/25 | Loss: 0.00123759
Iteration 8/25 | Loss: 0.00123564
Iteration 9/25 | Loss: 0.00123516
Iteration 10/25 | Loss: 0.00123487
Iteration 11/25 | Loss: 0.00123897
Iteration 12/25 | Loss: 0.00123821
Iteration 13/25 | Loss: 0.00123868
Iteration 14/25 | Loss: 0.00123737
Iteration 15/25 | Loss: 0.00123879
Iteration 16/25 | Loss: 0.00124199
Iteration 17/25 | Loss: 0.00124308
Iteration 18/25 | Loss: 0.00123675
Iteration 19/25 | Loss: 0.00123879
Iteration 20/25 | Loss: 0.00123878
Iteration 21/25 | Loss: 0.00123789
Iteration 22/25 | Loss: 0.00123467
Iteration 23/25 | Loss: 0.00123466
Iteration 24/25 | Loss: 0.00123466
Iteration 25/25 | Loss: 0.00123466

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 13.35212803
Iteration 2/25 | Loss: 0.00073101
Iteration 3/25 | Loss: 0.00073090
Iteration 4/25 | Loss: 0.00073090
Iteration 5/25 | Loss: 0.00073090
Iteration 6/25 | Loss: 0.00073090
Iteration 7/25 | Loss: 0.00073090
Iteration 8/25 | Loss: 0.00073090
Iteration 9/25 | Loss: 0.00073089
Iteration 10/25 | Loss: 0.00073089
Iteration 11/25 | Loss: 0.00073089
Iteration 12/25 | Loss: 0.00073089
Iteration 13/25 | Loss: 0.00073089
Iteration 14/25 | Loss: 0.00073089
Iteration 15/25 | Loss: 0.00073089
Iteration 16/25 | Loss: 0.00073089
Iteration 17/25 | Loss: 0.00073089
Iteration 18/25 | Loss: 0.00073089
Iteration 19/25 | Loss: 0.00073089
Iteration 20/25 | Loss: 0.00073089
Iteration 21/25 | Loss: 0.00073089
Iteration 22/25 | Loss: 0.00073089
Iteration 23/25 | Loss: 0.00073089
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.000730894273146987, 0.000730894273146987, 0.000730894273146987, 0.000730894273146987, 0.000730894273146987]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000730894273146987

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00073089
Iteration 2/1000 | Loss: 0.00020412
Iteration 3/1000 | Loss: 0.00021123
Iteration 4/1000 | Loss: 0.00016766
Iteration 5/1000 | Loss: 0.00020119
Iteration 6/1000 | Loss: 0.00020765
Iteration 7/1000 | Loss: 0.00015561
Iteration 8/1000 | Loss: 0.00018618
Iteration 9/1000 | Loss: 0.00003381
Iteration 10/1000 | Loss: 0.00016185
Iteration 11/1000 | Loss: 0.00014330
Iteration 12/1000 | Loss: 0.00020138
Iteration 13/1000 | Loss: 0.00017956
Iteration 14/1000 | Loss: 0.00013235
Iteration 15/1000 | Loss: 0.00002838
Iteration 16/1000 | Loss: 0.00002612
Iteration 17/1000 | Loss: 0.00002517
Iteration 18/1000 | Loss: 0.00002412
Iteration 19/1000 | Loss: 0.00002332
Iteration 20/1000 | Loss: 0.00002271
Iteration 21/1000 | Loss: 0.00003327
Iteration 22/1000 | Loss: 0.00002380
Iteration 23/1000 | Loss: 0.00002256
Iteration 24/1000 | Loss: 0.00002195
Iteration 25/1000 | Loss: 0.00002184
Iteration 26/1000 | Loss: 0.00002176
Iteration 27/1000 | Loss: 0.00002171
Iteration 28/1000 | Loss: 0.00002170
Iteration 29/1000 | Loss: 0.00002163
Iteration 30/1000 | Loss: 0.00002148
Iteration 31/1000 | Loss: 0.00002140
Iteration 32/1000 | Loss: 0.00002139
Iteration 33/1000 | Loss: 0.00002139
Iteration 34/1000 | Loss: 0.00002136
Iteration 35/1000 | Loss: 0.00002134
Iteration 36/1000 | Loss: 0.00002132
Iteration 37/1000 | Loss: 0.00002132
Iteration 38/1000 | Loss: 0.00002132
Iteration 39/1000 | Loss: 0.00002131
Iteration 40/1000 | Loss: 0.00002131
Iteration 41/1000 | Loss: 0.00002131
Iteration 42/1000 | Loss: 0.00002130
Iteration 43/1000 | Loss: 0.00002130
Iteration 44/1000 | Loss: 0.00002130
Iteration 45/1000 | Loss: 0.00002130
Iteration 46/1000 | Loss: 0.00002129
Iteration 47/1000 | Loss: 0.00002129
Iteration 48/1000 | Loss: 0.00002129
Iteration 49/1000 | Loss: 0.00002129
Iteration 50/1000 | Loss: 0.00002129
Iteration 51/1000 | Loss: 0.00002129
Iteration 52/1000 | Loss: 0.00002129
Iteration 53/1000 | Loss: 0.00002129
Iteration 54/1000 | Loss: 0.00002129
Iteration 55/1000 | Loss: 0.00002128
Iteration 56/1000 | Loss: 0.00002128
Iteration 57/1000 | Loss: 0.00002128
Iteration 58/1000 | Loss: 0.00002128
Iteration 59/1000 | Loss: 0.00002128
Iteration 60/1000 | Loss: 0.00002128
Iteration 61/1000 | Loss: 0.00002128
Iteration 62/1000 | Loss: 0.00002128
Iteration 63/1000 | Loss: 0.00002128
Iteration 64/1000 | Loss: 0.00002128
Iteration 65/1000 | Loss: 0.00002128
Iteration 66/1000 | Loss: 0.00002128
Iteration 67/1000 | Loss: 0.00002128
Iteration 68/1000 | Loss: 0.00002128
Iteration 69/1000 | Loss: 0.00002128
Iteration 70/1000 | Loss: 0.00002128
Iteration 71/1000 | Loss: 0.00002128
Iteration 72/1000 | Loss: 0.00002128
Iteration 73/1000 | Loss: 0.00002128
Iteration 74/1000 | Loss: 0.00002128
Iteration 75/1000 | Loss: 0.00002128
Iteration 76/1000 | Loss: 0.00002128
Iteration 77/1000 | Loss: 0.00002128
Iteration 78/1000 | Loss: 0.00002128
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 78. Stopping optimization.
Last 5 losses: [2.12821141758468e-05, 2.12821141758468e-05, 2.12821141758468e-05, 2.12821141758468e-05, 2.12821141758468e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.12821141758468e-05

Optimization complete. Final v2v error: 3.8540987968444824 mm

Highest mean error: 4.806962013244629 mm for frame 112

Lowest mean error: 3.266162395477295 mm for frame 206

Saving results

Total time: 88.84965062141418
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_023/1098/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_023/1098.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_023/1098
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00994331
Iteration 2/25 | Loss: 0.00994330
Iteration 3/25 | Loss: 0.00308020
Iteration 4/25 | Loss: 0.00219076
Iteration 5/25 | Loss: 0.00196220
Iteration 6/25 | Loss: 0.00176667
Iteration 7/25 | Loss: 0.00177232
Iteration 8/25 | Loss: 0.00167016
Iteration 9/25 | Loss: 0.00158686
Iteration 10/25 | Loss: 0.00155880
Iteration 11/25 | Loss: 0.00155721
Iteration 12/25 | Loss: 0.00153823
Iteration 13/25 | Loss: 0.00150594
Iteration 14/25 | Loss: 0.00146763
Iteration 15/25 | Loss: 0.00145690
Iteration 16/25 | Loss: 0.00143740
Iteration 17/25 | Loss: 0.00143485
Iteration 18/25 | Loss: 0.00143045
Iteration 19/25 | Loss: 0.00141897
Iteration 20/25 | Loss: 0.00141443
Iteration 21/25 | Loss: 0.00141059
Iteration 22/25 | Loss: 0.00141573
Iteration 23/25 | Loss: 0.00140207
Iteration 24/25 | Loss: 0.00140115
Iteration 25/25 | Loss: 0.00140201

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35583758
Iteration 2/25 | Loss: 0.00379683
Iteration 3/25 | Loss: 0.00326546
Iteration 4/25 | Loss: 0.00326545
Iteration 5/25 | Loss: 0.00326545
Iteration 6/25 | Loss: 0.00326545
Iteration 7/25 | Loss: 0.00326544
Iteration 8/25 | Loss: 0.00326544
Iteration 9/25 | Loss: 0.00326544
Iteration 10/25 | Loss: 0.00326544
Iteration 11/25 | Loss: 0.00326544
Iteration 12/25 | Loss: 0.00326544
Iteration 13/25 | Loss: 0.00326544
Iteration 14/25 | Loss: 0.00326544
Iteration 15/25 | Loss: 0.00326544
Iteration 16/25 | Loss: 0.00326544
Iteration 17/25 | Loss: 0.00326544
Iteration 18/25 | Loss: 0.00326544
Iteration 19/25 | Loss: 0.00326544
Iteration 20/25 | Loss: 0.00326544
Iteration 21/25 | Loss: 0.00326544
Iteration 22/25 | Loss: 0.00326544
Iteration 23/25 | Loss: 0.00326544
Iteration 24/25 | Loss: 0.00326544
Iteration 25/25 | Loss: 0.00326544

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00326544
Iteration 2/1000 | Loss: 0.00042466
Iteration 3/1000 | Loss: 0.00086642
Iteration 4/1000 | Loss: 0.00049894
Iteration 5/1000 | Loss: 0.00048497
Iteration 6/1000 | Loss: 0.00032008
Iteration 7/1000 | Loss: 0.00019036
Iteration 8/1000 | Loss: 0.00020087
Iteration 9/1000 | Loss: 0.00015859
Iteration 10/1000 | Loss: 0.00047735
Iteration 11/1000 | Loss: 0.00014878
Iteration 12/1000 | Loss: 0.00054659
Iteration 13/1000 | Loss: 0.00058777
Iteration 14/1000 | Loss: 0.00021655
Iteration 15/1000 | Loss: 0.00019243
Iteration 16/1000 | Loss: 0.00077249
Iteration 17/1000 | Loss: 0.00109829
Iteration 18/1000 | Loss: 0.00017829
Iteration 19/1000 | Loss: 0.00018110
Iteration 20/1000 | Loss: 0.00013900
Iteration 21/1000 | Loss: 0.00039822
Iteration 22/1000 | Loss: 0.00013846
Iteration 23/1000 | Loss: 0.00013346
Iteration 24/1000 | Loss: 0.00012921
Iteration 25/1000 | Loss: 0.00012767
Iteration 26/1000 | Loss: 0.00034640
Iteration 27/1000 | Loss: 0.00021598
Iteration 28/1000 | Loss: 0.00021241
Iteration 29/1000 | Loss: 0.00017874
Iteration 30/1000 | Loss: 0.00021227
Iteration 31/1000 | Loss: 0.00017697
Iteration 32/1000 | Loss: 0.00018192
Iteration 33/1000 | Loss: 0.00013061
Iteration 34/1000 | Loss: 0.00020328
Iteration 35/1000 | Loss: 0.00012383
Iteration 36/1000 | Loss: 0.00034736
Iteration 37/1000 | Loss: 0.00019223
Iteration 38/1000 | Loss: 0.00011931
Iteration 39/1000 | Loss: 0.00012516
Iteration 40/1000 | Loss: 0.00019009
Iteration 41/1000 | Loss: 0.00011070
Iteration 42/1000 | Loss: 0.00011523
Iteration 43/1000 | Loss: 0.00030596
Iteration 44/1000 | Loss: 0.00021486
Iteration 45/1000 | Loss: 0.00031367
Iteration 46/1000 | Loss: 0.00022145
Iteration 47/1000 | Loss: 0.00031395
Iteration 48/1000 | Loss: 0.00017988
Iteration 49/1000 | Loss: 0.00028147
Iteration 50/1000 | Loss: 0.00019555
Iteration 51/1000 | Loss: 0.00022038
Iteration 52/1000 | Loss: 0.00042894
Iteration 53/1000 | Loss: 0.00037438
Iteration 54/1000 | Loss: 0.00034468
Iteration 55/1000 | Loss: 0.00033702
Iteration 56/1000 | Loss: 0.00014539
Iteration 57/1000 | Loss: 0.00012585
Iteration 58/1000 | Loss: 0.00010934
Iteration 59/1000 | Loss: 0.00010847
Iteration 60/1000 | Loss: 0.00012292
Iteration 61/1000 | Loss: 0.00011071
Iteration 62/1000 | Loss: 0.00010993
Iteration 63/1000 | Loss: 0.00010732
Iteration 64/1000 | Loss: 0.00033066
Iteration 65/1000 | Loss: 0.00042820
Iteration 66/1000 | Loss: 0.00029824
Iteration 67/1000 | Loss: 0.00014138
Iteration 68/1000 | Loss: 0.00025101
Iteration 69/1000 | Loss: 0.00027354
Iteration 70/1000 | Loss: 0.00023842
Iteration 71/1000 | Loss: 0.00034917
Iteration 72/1000 | Loss: 0.00025524
Iteration 73/1000 | Loss: 0.00021871
Iteration 74/1000 | Loss: 0.00016821
Iteration 75/1000 | Loss: 0.00018943
Iteration 76/1000 | Loss: 0.00059130
Iteration 77/1000 | Loss: 0.00013396
Iteration 78/1000 | Loss: 0.00026275
Iteration 79/1000 | Loss: 0.00021290
Iteration 80/1000 | Loss: 0.00016640
Iteration 81/1000 | Loss: 0.00012372
Iteration 82/1000 | Loss: 0.00031069
Iteration 83/1000 | Loss: 0.00019743
Iteration 84/1000 | Loss: 0.00012322
Iteration 85/1000 | Loss: 0.00020087
Iteration 86/1000 | Loss: 0.00010435
Iteration 87/1000 | Loss: 0.00010114
Iteration 88/1000 | Loss: 0.00010939
Iteration 89/1000 | Loss: 0.00010988
Iteration 90/1000 | Loss: 0.00011610
Iteration 91/1000 | Loss: 0.00012866
Iteration 92/1000 | Loss: 0.00011775
Iteration 93/1000 | Loss: 0.00017240
Iteration 94/1000 | Loss: 0.00012450
Iteration 95/1000 | Loss: 0.00055713
Iteration 96/1000 | Loss: 0.00011159
Iteration 97/1000 | Loss: 0.00010284
Iteration 98/1000 | Loss: 0.00009883
Iteration 99/1000 | Loss: 0.00009738
Iteration 100/1000 | Loss: 0.00011111
Iteration 101/1000 | Loss: 0.00009821
Iteration 102/1000 | Loss: 0.00009666
Iteration 103/1000 | Loss: 0.00019705
Iteration 104/1000 | Loss: 0.00011061
Iteration 105/1000 | Loss: 0.00009769
Iteration 106/1000 | Loss: 0.00009628
Iteration 107/1000 | Loss: 0.00009468
Iteration 108/1000 | Loss: 0.00009385
Iteration 109/1000 | Loss: 0.00009316
Iteration 110/1000 | Loss: 0.00019171
Iteration 111/1000 | Loss: 0.00011283
Iteration 112/1000 | Loss: 0.00015619
Iteration 113/1000 | Loss: 0.00067503
Iteration 114/1000 | Loss: 0.00018027
Iteration 115/1000 | Loss: 0.00021335
Iteration 116/1000 | Loss: 0.00010298
Iteration 117/1000 | Loss: 0.00011218
Iteration 118/1000 | Loss: 0.00009471
Iteration 119/1000 | Loss: 0.00009237
Iteration 120/1000 | Loss: 0.00049072
Iteration 121/1000 | Loss: 0.00010777
Iteration 122/1000 | Loss: 0.00017463
Iteration 123/1000 | Loss: 0.00016393
Iteration 124/1000 | Loss: 0.00033336
Iteration 125/1000 | Loss: 0.00027374
Iteration 126/1000 | Loss: 0.00013116
Iteration 127/1000 | Loss: 0.00009866
Iteration 128/1000 | Loss: 0.00009348
Iteration 129/1000 | Loss: 0.00008997
Iteration 130/1000 | Loss: 0.00008730
Iteration 131/1000 | Loss: 0.00008525
Iteration 132/1000 | Loss: 0.00008438
Iteration 133/1000 | Loss: 0.00025936
Iteration 134/1000 | Loss: 0.00041736
Iteration 135/1000 | Loss: 0.00017789
Iteration 136/1000 | Loss: 0.00041064
Iteration 137/1000 | Loss: 0.00020539
Iteration 138/1000 | Loss: 0.00012433
Iteration 139/1000 | Loss: 0.00023805
Iteration 140/1000 | Loss: 0.00025500
Iteration 141/1000 | Loss: 0.00022371
Iteration 142/1000 | Loss: 0.00028647
Iteration 143/1000 | Loss: 0.00010760
Iteration 144/1000 | Loss: 0.00010616
Iteration 145/1000 | Loss: 0.00010785
Iteration 146/1000 | Loss: 0.00008363
Iteration 147/1000 | Loss: 0.00008157
Iteration 148/1000 | Loss: 0.00014819
Iteration 149/1000 | Loss: 0.00007936
Iteration 150/1000 | Loss: 0.00007838
Iteration 151/1000 | Loss: 0.00007769
Iteration 152/1000 | Loss: 0.00007736
Iteration 153/1000 | Loss: 0.00014685
Iteration 154/1000 | Loss: 0.00023843
Iteration 155/1000 | Loss: 0.00007896
Iteration 156/1000 | Loss: 0.00007712
Iteration 157/1000 | Loss: 0.00007670
Iteration 158/1000 | Loss: 0.00014260
Iteration 159/1000 | Loss: 0.00007947
Iteration 160/1000 | Loss: 0.00009713
Iteration 161/1000 | Loss: 0.00007651
Iteration 162/1000 | Loss: 0.00007642
Iteration 163/1000 | Loss: 0.00007630
Iteration 164/1000 | Loss: 0.00007622
Iteration 165/1000 | Loss: 0.00007617
Iteration 166/1000 | Loss: 0.00007617
Iteration 167/1000 | Loss: 0.00007616
Iteration 168/1000 | Loss: 0.00007616
Iteration 169/1000 | Loss: 0.00007616
Iteration 170/1000 | Loss: 0.00007615
Iteration 171/1000 | Loss: 0.00007615
Iteration 172/1000 | Loss: 0.00007615
Iteration 173/1000 | Loss: 0.00007614
Iteration 174/1000 | Loss: 0.00007614
Iteration 175/1000 | Loss: 0.00007614
Iteration 176/1000 | Loss: 0.00007614
Iteration 177/1000 | Loss: 0.00007613
Iteration 178/1000 | Loss: 0.00007613
Iteration 179/1000 | Loss: 0.00007613
Iteration 180/1000 | Loss: 0.00007613
Iteration 181/1000 | Loss: 0.00007613
Iteration 182/1000 | Loss: 0.00007612
Iteration 183/1000 | Loss: 0.00007612
Iteration 184/1000 | Loss: 0.00007611
Iteration 185/1000 | Loss: 0.00007611
Iteration 186/1000 | Loss: 0.00007610
Iteration 187/1000 | Loss: 0.00007610
Iteration 188/1000 | Loss: 0.00007610
Iteration 189/1000 | Loss: 0.00007609
Iteration 190/1000 | Loss: 0.00007609
Iteration 191/1000 | Loss: 0.00007609
Iteration 192/1000 | Loss: 0.00007609
Iteration 193/1000 | Loss: 0.00007608
Iteration 194/1000 | Loss: 0.00007608
Iteration 195/1000 | Loss: 0.00007608
Iteration 196/1000 | Loss: 0.00007608
Iteration 197/1000 | Loss: 0.00007608
Iteration 198/1000 | Loss: 0.00007608
Iteration 199/1000 | Loss: 0.00007608
Iteration 200/1000 | Loss: 0.00007608
Iteration 201/1000 | Loss: 0.00007608
Iteration 202/1000 | Loss: 0.00007607
Iteration 203/1000 | Loss: 0.00007607
Iteration 204/1000 | Loss: 0.00007607
Iteration 205/1000 | Loss: 0.00007607
Iteration 206/1000 | Loss: 0.00007607
Iteration 207/1000 | Loss: 0.00007606
Iteration 208/1000 | Loss: 0.00007606
Iteration 209/1000 | Loss: 0.00007605
Iteration 210/1000 | Loss: 0.00007605
Iteration 211/1000 | Loss: 0.00007605
Iteration 212/1000 | Loss: 0.00007605
Iteration 213/1000 | Loss: 0.00007605
Iteration 214/1000 | Loss: 0.00007605
Iteration 215/1000 | Loss: 0.00007605
Iteration 216/1000 | Loss: 0.00007604
Iteration 217/1000 | Loss: 0.00007604
Iteration 218/1000 | Loss: 0.00007604
Iteration 219/1000 | Loss: 0.00007604
Iteration 220/1000 | Loss: 0.00007604
Iteration 221/1000 | Loss: 0.00007603
Iteration 222/1000 | Loss: 0.00007603
Iteration 223/1000 | Loss: 0.00007603
Iteration 224/1000 | Loss: 0.00007603
Iteration 225/1000 | Loss: 0.00007603
Iteration 226/1000 | Loss: 0.00007603
Iteration 227/1000 | Loss: 0.00007603
Iteration 228/1000 | Loss: 0.00007603
Iteration 229/1000 | Loss: 0.00007602
Iteration 230/1000 | Loss: 0.00007602
Iteration 231/1000 | Loss: 0.00007602
Iteration 232/1000 | Loss: 0.00007602
Iteration 233/1000 | Loss: 0.00007602
Iteration 234/1000 | Loss: 0.00007602
Iteration 235/1000 | Loss: 0.00007601
Iteration 236/1000 | Loss: 0.00007601
Iteration 237/1000 | Loss: 0.00007601
Iteration 238/1000 | Loss: 0.00007601
Iteration 239/1000 | Loss: 0.00007601
Iteration 240/1000 | Loss: 0.00007601
Iteration 241/1000 | Loss: 0.00007601
Iteration 242/1000 | Loss: 0.00007600
Iteration 243/1000 | Loss: 0.00007600
Iteration 244/1000 | Loss: 0.00007600
Iteration 245/1000 | Loss: 0.00007600
Iteration 246/1000 | Loss: 0.00007600
Iteration 247/1000 | Loss: 0.00007599
Iteration 248/1000 | Loss: 0.00007599
Iteration 249/1000 | Loss: 0.00007599
Iteration 250/1000 | Loss: 0.00007599
Iteration 251/1000 | Loss: 0.00007599
Iteration 252/1000 | Loss: 0.00007598
Iteration 253/1000 | Loss: 0.00007598
Iteration 254/1000 | Loss: 0.00007598
Iteration 255/1000 | Loss: 0.00007598
Iteration 256/1000 | Loss: 0.00007598
Iteration 257/1000 | Loss: 0.00007597
Iteration 258/1000 | Loss: 0.00007597
Iteration 259/1000 | Loss: 0.00007597
Iteration 260/1000 | Loss: 0.00007597
Iteration 261/1000 | Loss: 0.00007596
Iteration 262/1000 | Loss: 0.00007596
Iteration 263/1000 | Loss: 0.00007596
Iteration 264/1000 | Loss: 0.00007596
Iteration 265/1000 | Loss: 0.00007596
Iteration 266/1000 | Loss: 0.00007596
Iteration 267/1000 | Loss: 0.00007596
Iteration 268/1000 | Loss: 0.00007596
Iteration 269/1000 | Loss: 0.00007595
Iteration 270/1000 | Loss: 0.00007595
Iteration 271/1000 | Loss: 0.00007595
Iteration 272/1000 | Loss: 0.00007595
Iteration 273/1000 | Loss: 0.00007595
Iteration 274/1000 | Loss: 0.00007595
Iteration 275/1000 | Loss: 0.00007595
Iteration 276/1000 | Loss: 0.00007594
Iteration 277/1000 | Loss: 0.00007594
Iteration 278/1000 | Loss: 0.00007594
Iteration 279/1000 | Loss: 0.00007594
Iteration 280/1000 | Loss: 0.00007594
Iteration 281/1000 | Loss: 0.00007594
Iteration 282/1000 | Loss: 0.00007593
Iteration 283/1000 | Loss: 0.00007593
Iteration 284/1000 | Loss: 0.00007593
Iteration 285/1000 | Loss: 0.00007593
Iteration 286/1000 | Loss: 0.00007593
Iteration 287/1000 | Loss: 0.00007593
Iteration 288/1000 | Loss: 0.00007593
Iteration 289/1000 | Loss: 0.00007593
Iteration 290/1000 | Loss: 0.00007593
Iteration 291/1000 | Loss: 0.00007593
Iteration 292/1000 | Loss: 0.00007593
Iteration 293/1000 | Loss: 0.00007593
Iteration 294/1000 | Loss: 0.00007593
Iteration 295/1000 | Loss: 0.00007592
Iteration 296/1000 | Loss: 0.00007592
Iteration 297/1000 | Loss: 0.00007592
Iteration 298/1000 | Loss: 0.00007592
Iteration 299/1000 | Loss: 0.00007592
Iteration 300/1000 | Loss: 0.00007592
Iteration 301/1000 | Loss: 0.00007592
Iteration 302/1000 | Loss: 0.00007592
Iteration 303/1000 | Loss: 0.00007592
Iteration 304/1000 | Loss: 0.00007592
Iteration 305/1000 | Loss: 0.00007592
Iteration 306/1000 | Loss: 0.00007592
Iteration 307/1000 | Loss: 0.00007592
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 307. Stopping optimization.
Last 5 losses: [7.591790199512616e-05, 7.591790199512616e-05, 7.591790199512616e-05, 7.591790199512616e-05, 7.591790199512616e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 7.591790199512616e-05

Optimization complete. Final v2v error: 4.427116870880127 mm

Highest mean error: 11.53206729888916 mm for frame 187

Lowest mean error: 2.5439815521240234 mm for frame 167

Saving results

Total time: 325.0133812427521
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_023/1011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_023/1011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_023/1011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00974375
Iteration 2/25 | Loss: 0.00166166
Iteration 3/25 | Loss: 0.00138314
Iteration 4/25 | Loss: 0.00134893
Iteration 5/25 | Loss: 0.00134194
Iteration 6/25 | Loss: 0.00134074
Iteration 7/25 | Loss: 0.00134074
Iteration 8/25 | Loss: 0.00134074
Iteration 9/25 | Loss: 0.00134074
Iteration 10/25 | Loss: 0.00134074
Iteration 11/25 | Loss: 0.00134074
Iteration 12/25 | Loss: 0.00134074
Iteration 13/25 | Loss: 0.00134074
Iteration 14/25 | Loss: 0.00134074
Iteration 15/25 | Loss: 0.00134074
Iteration 16/25 | Loss: 0.00134074
Iteration 17/25 | Loss: 0.00134074
Iteration 18/25 | Loss: 0.00134074
Iteration 19/25 | Loss: 0.00134074
Iteration 20/25 | Loss: 0.00134074
Iteration 21/25 | Loss: 0.00134074
Iteration 22/25 | Loss: 0.00134074
Iteration 23/25 | Loss: 0.00134074
Iteration 24/25 | Loss: 0.00134074
Iteration 25/25 | Loss: 0.00134074
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0013407351216301322, 0.0013407351216301322, 0.0013407351216301322, 0.0013407351216301322, 0.0013407351216301322]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013407351216301322

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.61294770
Iteration 2/25 | Loss: 0.00093606
Iteration 3/25 | Loss: 0.00093604
Iteration 4/25 | Loss: 0.00093604
Iteration 5/25 | Loss: 0.00093604
Iteration 6/25 | Loss: 0.00093604
Iteration 7/25 | Loss: 0.00093604
Iteration 8/25 | Loss: 0.00093604
Iteration 9/25 | Loss: 0.00093604
Iteration 10/25 | Loss: 0.00093604
Iteration 11/25 | Loss: 0.00093604
Iteration 12/25 | Loss: 0.00093604
Iteration 13/25 | Loss: 0.00093604
Iteration 14/25 | Loss: 0.00093604
Iteration 15/25 | Loss: 0.00093604
Iteration 16/25 | Loss: 0.00093604
Iteration 17/25 | Loss: 0.00093604
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0009360360563732684, 0.0009360360563732684, 0.0009360360563732684, 0.0009360360563732684, 0.0009360360563732684]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009360360563732684

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00093604
Iteration 2/1000 | Loss: 0.00007392
Iteration 3/1000 | Loss: 0.00004821
Iteration 4/1000 | Loss: 0.00003984
Iteration 5/1000 | Loss: 0.00003754
Iteration 6/1000 | Loss: 0.00003619
Iteration 7/1000 | Loss: 0.00003514
Iteration 8/1000 | Loss: 0.00003453
Iteration 9/1000 | Loss: 0.00003402
Iteration 10/1000 | Loss: 0.00003361
Iteration 11/1000 | Loss: 0.00003328
Iteration 12/1000 | Loss: 0.00003307
Iteration 13/1000 | Loss: 0.00003287
Iteration 14/1000 | Loss: 0.00003283
Iteration 15/1000 | Loss: 0.00003282
Iteration 16/1000 | Loss: 0.00003272
Iteration 17/1000 | Loss: 0.00003269
Iteration 18/1000 | Loss: 0.00003267
Iteration 19/1000 | Loss: 0.00003265
Iteration 20/1000 | Loss: 0.00003265
Iteration 21/1000 | Loss: 0.00003265
Iteration 22/1000 | Loss: 0.00003262
Iteration 23/1000 | Loss: 0.00003262
Iteration 24/1000 | Loss: 0.00003260
Iteration 25/1000 | Loss: 0.00003259
Iteration 26/1000 | Loss: 0.00003259
Iteration 27/1000 | Loss: 0.00003255
Iteration 28/1000 | Loss: 0.00003255
Iteration 29/1000 | Loss: 0.00003255
Iteration 30/1000 | Loss: 0.00003255
Iteration 31/1000 | Loss: 0.00003255
Iteration 32/1000 | Loss: 0.00003254
Iteration 33/1000 | Loss: 0.00003254
Iteration 34/1000 | Loss: 0.00003252
Iteration 35/1000 | Loss: 0.00003252
Iteration 36/1000 | Loss: 0.00003250
Iteration 37/1000 | Loss: 0.00003250
Iteration 38/1000 | Loss: 0.00003250
Iteration 39/1000 | Loss: 0.00003250
Iteration 40/1000 | Loss: 0.00003250
Iteration 41/1000 | Loss: 0.00003250
Iteration 42/1000 | Loss: 0.00003250
Iteration 43/1000 | Loss: 0.00003249
Iteration 44/1000 | Loss: 0.00003249
Iteration 45/1000 | Loss: 0.00003249
Iteration 46/1000 | Loss: 0.00003249
Iteration 47/1000 | Loss: 0.00003249
Iteration 48/1000 | Loss: 0.00003248
Iteration 49/1000 | Loss: 0.00003247
Iteration 50/1000 | Loss: 0.00003247
Iteration 51/1000 | Loss: 0.00003247
Iteration 52/1000 | Loss: 0.00003247
Iteration 53/1000 | Loss: 0.00003246
Iteration 54/1000 | Loss: 0.00003246
Iteration 55/1000 | Loss: 0.00003246
Iteration 56/1000 | Loss: 0.00003246
Iteration 57/1000 | Loss: 0.00003246
Iteration 58/1000 | Loss: 0.00003246
Iteration 59/1000 | Loss: 0.00003246
Iteration 60/1000 | Loss: 0.00003246
Iteration 61/1000 | Loss: 0.00003246
Iteration 62/1000 | Loss: 0.00003246
Iteration 63/1000 | Loss: 0.00003245
Iteration 64/1000 | Loss: 0.00003245
Iteration 65/1000 | Loss: 0.00003244
Iteration 66/1000 | Loss: 0.00003243
Iteration 67/1000 | Loss: 0.00003243
Iteration 68/1000 | Loss: 0.00003243
Iteration 69/1000 | Loss: 0.00003242
Iteration 70/1000 | Loss: 0.00003242
Iteration 71/1000 | Loss: 0.00003242
Iteration 72/1000 | Loss: 0.00003242
Iteration 73/1000 | Loss: 0.00003242
Iteration 74/1000 | Loss: 0.00003242
Iteration 75/1000 | Loss: 0.00003242
Iteration 76/1000 | Loss: 0.00003242
Iteration 77/1000 | Loss: 0.00003241
Iteration 78/1000 | Loss: 0.00003241
Iteration 79/1000 | Loss: 0.00003241
Iteration 80/1000 | Loss: 0.00003240
Iteration 81/1000 | Loss: 0.00003240
Iteration 82/1000 | Loss: 0.00003240
Iteration 83/1000 | Loss: 0.00003240
Iteration 84/1000 | Loss: 0.00003240
Iteration 85/1000 | Loss: 0.00003240
Iteration 86/1000 | Loss: 0.00003240
Iteration 87/1000 | Loss: 0.00003240
Iteration 88/1000 | Loss: 0.00003239
Iteration 89/1000 | Loss: 0.00003239
Iteration 90/1000 | Loss: 0.00003239
Iteration 91/1000 | Loss: 0.00003239
Iteration 92/1000 | Loss: 0.00003238
Iteration 93/1000 | Loss: 0.00003238
Iteration 94/1000 | Loss: 0.00003238
Iteration 95/1000 | Loss: 0.00003238
Iteration 96/1000 | Loss: 0.00003238
Iteration 97/1000 | Loss: 0.00003238
Iteration 98/1000 | Loss: 0.00003238
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 98. Stopping optimization.
Last 5 losses: [3.2380339689552784e-05, 3.2380339689552784e-05, 3.2380339689552784e-05, 3.2380339689552784e-05, 3.2380339689552784e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.2380339689552784e-05

Optimization complete. Final v2v error: 4.561186790466309 mm

Highest mean error: 6.175042629241943 mm for frame 48

Lowest mean error: 3.369318962097168 mm for frame 75

Saving results

Total time: 41.03873157501221
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_023/1017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_023/1017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_023/1017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00981937
Iteration 2/25 | Loss: 0.00981937
Iteration 3/25 | Loss: 0.00400364
Iteration 4/25 | Loss: 0.00245100
Iteration 5/25 | Loss: 0.00215916
Iteration 6/25 | Loss: 0.00192472
Iteration 7/25 | Loss: 0.00175268
Iteration 8/25 | Loss: 0.00169678
Iteration 9/25 | Loss: 0.00160457
Iteration 10/25 | Loss: 0.00157029
Iteration 11/25 | Loss: 0.00153753
Iteration 12/25 | Loss: 0.00151568
Iteration 13/25 | Loss: 0.00149845
Iteration 14/25 | Loss: 0.00147849
Iteration 15/25 | Loss: 0.00145874
Iteration 16/25 | Loss: 0.00146074
Iteration 17/25 | Loss: 0.00145532
Iteration 18/25 | Loss: 0.00144254
Iteration 19/25 | Loss: 0.00144353
Iteration 20/25 | Loss: 0.00144074
Iteration 21/25 | Loss: 0.00143155
Iteration 22/25 | Loss: 0.00143298
Iteration 23/25 | Loss: 0.00142196
Iteration 24/25 | Loss: 0.00141734
Iteration 25/25 | Loss: 0.00141683

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38762152
Iteration 2/25 | Loss: 0.00320825
Iteration 3/25 | Loss: 0.00251961
Iteration 4/25 | Loss: 0.00251961
Iteration 5/25 | Loss: 0.00251961
Iteration 6/25 | Loss: 0.00251961
Iteration 7/25 | Loss: 0.00251961
Iteration 8/25 | Loss: 0.00251961
Iteration 9/25 | Loss: 0.00251961
Iteration 10/25 | Loss: 0.00251961
Iteration 11/25 | Loss: 0.00251961
Iteration 12/25 | Loss: 0.00251961
Iteration 13/25 | Loss: 0.00251961
Iteration 14/25 | Loss: 0.00251961
Iteration 15/25 | Loss: 0.00251961
Iteration 16/25 | Loss: 0.00251961
Iteration 17/25 | Loss: 0.00251961
Iteration 18/25 | Loss: 0.00251961
Iteration 19/25 | Loss: 0.00251961
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.002519605215638876, 0.002519605215638876, 0.002519605215638876, 0.002519605215638876, 0.002519605215638876]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002519605215638876

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00251961
Iteration 2/1000 | Loss: 0.00104893
Iteration 3/1000 | Loss: 0.00070529
Iteration 4/1000 | Loss: 0.00047223
Iteration 5/1000 | Loss: 0.00089689
Iteration 6/1000 | Loss: 0.00072168
Iteration 7/1000 | Loss: 0.00027841
Iteration 8/1000 | Loss: 0.00087889
Iteration 9/1000 | Loss: 0.00037066
Iteration 10/1000 | Loss: 0.00025314
Iteration 11/1000 | Loss: 0.00024924
Iteration 12/1000 | Loss: 0.00050113
Iteration 13/1000 | Loss: 0.00014949
Iteration 14/1000 | Loss: 0.00033490
Iteration 15/1000 | Loss: 0.00019684
Iteration 16/1000 | Loss: 0.00029725
Iteration 17/1000 | Loss: 0.00024466
Iteration 18/1000 | Loss: 0.00040134
Iteration 19/1000 | Loss: 0.00015096
Iteration 20/1000 | Loss: 0.00104162
Iteration 21/1000 | Loss: 0.00037385
Iteration 22/1000 | Loss: 0.00030774
Iteration 23/1000 | Loss: 0.00062438
Iteration 24/1000 | Loss: 0.00236943
Iteration 25/1000 | Loss: 0.00341771
Iteration 26/1000 | Loss: 0.00245388
Iteration 27/1000 | Loss: 0.00236902
Iteration 28/1000 | Loss: 0.00063722
Iteration 29/1000 | Loss: 0.00094784
Iteration 30/1000 | Loss: 0.00028757
Iteration 31/1000 | Loss: 0.00069354
Iteration 32/1000 | Loss: 0.00076484
Iteration 33/1000 | Loss: 0.00020310
Iteration 34/1000 | Loss: 0.00046802
Iteration 35/1000 | Loss: 0.00047153
Iteration 36/1000 | Loss: 0.00043037
Iteration 37/1000 | Loss: 0.00068849
Iteration 38/1000 | Loss: 0.00006448
Iteration 39/1000 | Loss: 0.00035608
Iteration 40/1000 | Loss: 0.00016762
Iteration 41/1000 | Loss: 0.00009033
Iteration 42/1000 | Loss: 0.00006911
Iteration 43/1000 | Loss: 0.00009002
Iteration 44/1000 | Loss: 0.00016826
Iteration 45/1000 | Loss: 0.00006286
Iteration 46/1000 | Loss: 0.00004503
Iteration 47/1000 | Loss: 0.00014775
Iteration 48/1000 | Loss: 0.00005954
Iteration 49/1000 | Loss: 0.00005420
Iteration 50/1000 | Loss: 0.00007110
Iteration 51/1000 | Loss: 0.00003534
Iteration 52/1000 | Loss: 0.00012916
Iteration 53/1000 | Loss: 0.00004935
Iteration 54/1000 | Loss: 0.00002764
Iteration 55/1000 | Loss: 0.00016969
Iteration 56/1000 | Loss: 0.00008684
Iteration 57/1000 | Loss: 0.00009142
Iteration 58/1000 | Loss: 0.00003533
Iteration 59/1000 | Loss: 0.00002612
Iteration 60/1000 | Loss: 0.00002590
Iteration 61/1000 | Loss: 0.00003576
Iteration 62/1000 | Loss: 0.00032665
Iteration 63/1000 | Loss: 0.00004940
Iteration 64/1000 | Loss: 0.00002572
Iteration 65/1000 | Loss: 0.00003385
Iteration 66/1000 | Loss: 0.00002901
Iteration 67/1000 | Loss: 0.00002531
Iteration 68/1000 | Loss: 0.00002531
Iteration 69/1000 | Loss: 0.00002589
Iteration 70/1000 | Loss: 0.00002981
Iteration 71/1000 | Loss: 0.00002523
Iteration 72/1000 | Loss: 0.00002523
Iteration 73/1000 | Loss: 0.00002522
Iteration 74/1000 | Loss: 0.00002522
Iteration 75/1000 | Loss: 0.00002522
Iteration 76/1000 | Loss: 0.00002521
Iteration 77/1000 | Loss: 0.00002521
Iteration 78/1000 | Loss: 0.00002519
Iteration 79/1000 | Loss: 0.00002669
Iteration 80/1000 | Loss: 0.00009988
Iteration 81/1000 | Loss: 0.00002925
Iteration 82/1000 | Loss: 0.00008163
Iteration 83/1000 | Loss: 0.00002503
Iteration 84/1000 | Loss: 0.00002494
Iteration 85/1000 | Loss: 0.00002493
Iteration 86/1000 | Loss: 0.00002490
Iteration 87/1000 | Loss: 0.00002489
Iteration 88/1000 | Loss: 0.00003858
Iteration 89/1000 | Loss: 0.00002824
Iteration 90/1000 | Loss: 0.00002861
Iteration 91/1000 | Loss: 0.00009029
Iteration 92/1000 | Loss: 0.00007603
Iteration 93/1000 | Loss: 0.00004148
Iteration 94/1000 | Loss: 0.00002722
Iteration 95/1000 | Loss: 0.00002414
Iteration 96/1000 | Loss: 0.00005593
Iteration 97/1000 | Loss: 0.00003196
Iteration 98/1000 | Loss: 0.00002951
Iteration 99/1000 | Loss: 0.00005481
Iteration 100/1000 | Loss: 0.00002566
Iteration 101/1000 | Loss: 0.00002587
Iteration 102/1000 | Loss: 0.00002408
Iteration 103/1000 | Loss: 0.00002376
Iteration 104/1000 | Loss: 0.00002376
Iteration 105/1000 | Loss: 0.00002376
Iteration 106/1000 | Loss: 0.00002376
Iteration 107/1000 | Loss: 0.00002376
Iteration 108/1000 | Loss: 0.00002376
Iteration 109/1000 | Loss: 0.00002375
Iteration 110/1000 | Loss: 0.00002375
Iteration 111/1000 | Loss: 0.00003905
Iteration 112/1000 | Loss: 0.00009335
Iteration 113/1000 | Loss: 0.00023240
Iteration 114/1000 | Loss: 0.00016215
Iteration 115/1000 | Loss: 0.00078297
Iteration 116/1000 | Loss: 0.00011085
Iteration 117/1000 | Loss: 0.00006686
Iteration 118/1000 | Loss: 0.00003025
Iteration 119/1000 | Loss: 0.00002696
Iteration 120/1000 | Loss: 0.00004513
Iteration 121/1000 | Loss: 0.00002284
Iteration 122/1000 | Loss: 0.00004770
Iteration 123/1000 | Loss: 0.00006462
Iteration 124/1000 | Loss: 0.00002119
Iteration 125/1000 | Loss: 0.00002861
Iteration 126/1000 | Loss: 0.00002392
Iteration 127/1000 | Loss: 0.00002412
Iteration 128/1000 | Loss: 0.00003985
Iteration 129/1000 | Loss: 0.00007030
Iteration 130/1000 | Loss: 0.00007057
Iteration 131/1000 | Loss: 0.00002036
Iteration 132/1000 | Loss: 0.00003009
Iteration 133/1000 | Loss: 0.00003009
Iteration 134/1000 | Loss: 0.00038793
Iteration 135/1000 | Loss: 0.00002503
Iteration 136/1000 | Loss: 0.00002659
Iteration 137/1000 | Loss: 0.00003694
Iteration 138/1000 | Loss: 0.00003033
Iteration 139/1000 | Loss: 0.00003572
Iteration 140/1000 | Loss: 0.00001886
Iteration 141/1000 | Loss: 0.00003357
Iteration 142/1000 | Loss: 0.00001894
Iteration 143/1000 | Loss: 0.00002073
Iteration 144/1000 | Loss: 0.00002569
Iteration 145/1000 | Loss: 0.00010463
Iteration 146/1000 | Loss: 0.00005917
Iteration 147/1000 | Loss: 0.00003263
Iteration 148/1000 | Loss: 0.00001822
Iteration 149/1000 | Loss: 0.00001822
Iteration 150/1000 | Loss: 0.00001816
Iteration 151/1000 | Loss: 0.00001816
Iteration 152/1000 | Loss: 0.00001815
Iteration 153/1000 | Loss: 0.00001814
Iteration 154/1000 | Loss: 0.00001814
Iteration 155/1000 | Loss: 0.00001813
Iteration 156/1000 | Loss: 0.00001813
Iteration 157/1000 | Loss: 0.00002079
Iteration 158/1000 | Loss: 0.00003157
Iteration 159/1000 | Loss: 0.00001809
Iteration 160/1000 | Loss: 0.00001809
Iteration 161/1000 | Loss: 0.00001809
Iteration 162/1000 | Loss: 0.00001809
Iteration 163/1000 | Loss: 0.00001809
Iteration 164/1000 | Loss: 0.00001809
Iteration 165/1000 | Loss: 0.00001809
Iteration 166/1000 | Loss: 0.00001809
Iteration 167/1000 | Loss: 0.00001809
Iteration 168/1000 | Loss: 0.00001809
Iteration 169/1000 | Loss: 0.00001808
Iteration 170/1000 | Loss: 0.00001807
Iteration 171/1000 | Loss: 0.00001807
Iteration 172/1000 | Loss: 0.00001807
Iteration 173/1000 | Loss: 0.00001807
Iteration 174/1000 | Loss: 0.00001807
Iteration 175/1000 | Loss: 0.00001807
Iteration 176/1000 | Loss: 0.00001807
Iteration 177/1000 | Loss: 0.00001807
Iteration 178/1000 | Loss: 0.00001807
Iteration 179/1000 | Loss: 0.00001807
Iteration 180/1000 | Loss: 0.00001807
Iteration 181/1000 | Loss: 0.00001807
Iteration 182/1000 | Loss: 0.00001807
Iteration 183/1000 | Loss: 0.00001807
Iteration 184/1000 | Loss: 0.00001807
Iteration 185/1000 | Loss: 0.00001807
Iteration 186/1000 | Loss: 0.00001807
Iteration 187/1000 | Loss: 0.00001807
Iteration 188/1000 | Loss: 0.00001807
Iteration 189/1000 | Loss: 0.00001807
Iteration 190/1000 | Loss: 0.00001806
Iteration 191/1000 | Loss: 0.00001806
Iteration 192/1000 | Loss: 0.00001806
Iteration 193/1000 | Loss: 0.00001806
Iteration 194/1000 | Loss: 0.00001806
Iteration 195/1000 | Loss: 0.00001806
Iteration 196/1000 | Loss: 0.00001806
Iteration 197/1000 | Loss: 0.00001806
Iteration 198/1000 | Loss: 0.00001806
Iteration 199/1000 | Loss: 0.00002340
Iteration 200/1000 | Loss: 0.00001805
Iteration 201/1000 | Loss: 0.00001804
Iteration 202/1000 | Loss: 0.00001804
Iteration 203/1000 | Loss: 0.00001804
Iteration 204/1000 | Loss: 0.00001804
Iteration 205/1000 | Loss: 0.00001804
Iteration 206/1000 | Loss: 0.00001804
Iteration 207/1000 | Loss: 0.00001803
Iteration 208/1000 | Loss: 0.00001803
Iteration 209/1000 | Loss: 0.00001803
Iteration 210/1000 | Loss: 0.00001803
Iteration 211/1000 | Loss: 0.00001803
Iteration 212/1000 | Loss: 0.00001803
Iteration 213/1000 | Loss: 0.00001803
Iteration 214/1000 | Loss: 0.00001803
Iteration 215/1000 | Loss: 0.00001802
Iteration 216/1000 | Loss: 0.00001802
Iteration 217/1000 | Loss: 0.00001802
Iteration 218/1000 | Loss: 0.00001802
Iteration 219/1000 | Loss: 0.00001802
Iteration 220/1000 | Loss: 0.00001802
Iteration 221/1000 | Loss: 0.00001802
Iteration 222/1000 | Loss: 0.00001802
Iteration 223/1000 | Loss: 0.00001802
Iteration 224/1000 | Loss: 0.00001802
Iteration 225/1000 | Loss: 0.00001802
Iteration 226/1000 | Loss: 0.00001801
Iteration 227/1000 | Loss: 0.00001801
Iteration 228/1000 | Loss: 0.00001801
Iteration 229/1000 | Loss: 0.00001801
Iteration 230/1000 | Loss: 0.00001801
Iteration 231/1000 | Loss: 0.00001801
Iteration 232/1000 | Loss: 0.00001801
Iteration 233/1000 | Loss: 0.00001800
Iteration 234/1000 | Loss: 0.00001800
Iteration 235/1000 | Loss: 0.00001800
Iteration 236/1000 | Loss: 0.00001800
Iteration 237/1000 | Loss: 0.00001800
Iteration 238/1000 | Loss: 0.00001800
Iteration 239/1000 | Loss: 0.00001800
Iteration 240/1000 | Loss: 0.00001800
Iteration 241/1000 | Loss: 0.00001800
Iteration 242/1000 | Loss: 0.00001799
Iteration 243/1000 | Loss: 0.00001799
Iteration 244/1000 | Loss: 0.00001799
Iteration 245/1000 | Loss: 0.00001799
Iteration 246/1000 | Loss: 0.00001799
Iteration 247/1000 | Loss: 0.00001799
Iteration 248/1000 | Loss: 0.00001799
Iteration 249/1000 | Loss: 0.00001799
Iteration 250/1000 | Loss: 0.00001799
Iteration 251/1000 | Loss: 0.00002077
Iteration 252/1000 | Loss: 0.00001808
Iteration 253/1000 | Loss: 0.00005484
Iteration 254/1000 | Loss: 0.00001888
Iteration 255/1000 | Loss: 0.00002035
Iteration 256/1000 | Loss: 0.00001955
Iteration 257/1000 | Loss: 0.00001801
Iteration 258/1000 | Loss: 0.00001800
Iteration 259/1000 | Loss: 0.00001800
Iteration 260/1000 | Loss: 0.00001800
Iteration 261/1000 | Loss: 0.00001799
Iteration 262/1000 | Loss: 0.00001799
Iteration 263/1000 | Loss: 0.00001799
Iteration 264/1000 | Loss: 0.00001799
Iteration 265/1000 | Loss: 0.00001799
Iteration 266/1000 | Loss: 0.00001799
Iteration 267/1000 | Loss: 0.00001799
Iteration 268/1000 | Loss: 0.00001798
Iteration 269/1000 | Loss: 0.00001798
Iteration 270/1000 | Loss: 0.00001798
Iteration 271/1000 | Loss: 0.00001798
Iteration 272/1000 | Loss: 0.00001798
Iteration 273/1000 | Loss: 0.00001798
Iteration 274/1000 | Loss: 0.00001798
Iteration 275/1000 | Loss: 0.00001798
Iteration 276/1000 | Loss: 0.00001797
Iteration 277/1000 | Loss: 0.00001797
Iteration 278/1000 | Loss: 0.00001797
Iteration 279/1000 | Loss: 0.00001796
Iteration 280/1000 | Loss: 0.00001796
Iteration 281/1000 | Loss: 0.00001796
Iteration 282/1000 | Loss: 0.00001796
Iteration 283/1000 | Loss: 0.00001796
Iteration 284/1000 | Loss: 0.00001796
Iteration 285/1000 | Loss: 0.00001796
Iteration 286/1000 | Loss: 0.00001796
Iteration 287/1000 | Loss: 0.00001796
Iteration 288/1000 | Loss: 0.00001796
Iteration 289/1000 | Loss: 0.00001796
Iteration 290/1000 | Loss: 0.00001796
Iteration 291/1000 | Loss: 0.00001796
Iteration 292/1000 | Loss: 0.00001796
Iteration 293/1000 | Loss: 0.00001796
Iteration 294/1000 | Loss: 0.00001796
Iteration 295/1000 | Loss: 0.00001796
Iteration 296/1000 | Loss: 0.00001796
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 296. Stopping optimization.
Last 5 losses: [1.7955964722204953e-05, 1.7955964722204953e-05, 1.7955964722204953e-05, 1.7955964722204953e-05, 1.7955964722204953e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7955964722204953e-05

Optimization complete. Final v2v error: 3.126385450363159 mm

Highest mean error: 10.728635787963867 mm for frame 172

Lowest mean error: 2.7081758975982666 mm for frame 40

Saving results

Total time: 269.5174717903137
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_023/1097/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_023/1097.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_023/1097
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00405477
Iteration 2/25 | Loss: 0.00122500
Iteration 3/25 | Loss: 0.00114823
Iteration 4/25 | Loss: 0.00113951
Iteration 5/25 | Loss: 0.00113738
Iteration 6/25 | Loss: 0.00113688
Iteration 7/25 | Loss: 0.00113685
Iteration 8/25 | Loss: 0.00113685
Iteration 9/25 | Loss: 0.00113685
Iteration 10/25 | Loss: 0.00113685
Iteration 11/25 | Loss: 0.00113685
Iteration 12/25 | Loss: 0.00113685
Iteration 13/25 | Loss: 0.00113685
Iteration 14/25 | Loss: 0.00113685
Iteration 15/25 | Loss: 0.00113685
Iteration 16/25 | Loss: 0.00113685
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0011368499835953116, 0.0011368499835953116, 0.0011368499835953116, 0.0011368499835953116, 0.0011368499835953116]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011368499835953116

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44472444
Iteration 2/25 | Loss: 0.00085233
Iteration 3/25 | Loss: 0.00085233
Iteration 4/25 | Loss: 0.00085233
Iteration 5/25 | Loss: 0.00085233
Iteration 6/25 | Loss: 0.00085233
Iteration 7/25 | Loss: 0.00085233
Iteration 8/25 | Loss: 0.00085233
Iteration 9/25 | Loss: 0.00085233
Iteration 10/25 | Loss: 0.00085233
Iteration 11/25 | Loss: 0.00085233
Iteration 12/25 | Loss: 0.00085233
Iteration 13/25 | Loss: 0.00085233
Iteration 14/25 | Loss: 0.00085233
Iteration 15/25 | Loss: 0.00085233
Iteration 16/25 | Loss: 0.00085233
Iteration 17/25 | Loss: 0.00085233
Iteration 18/25 | Loss: 0.00085233
Iteration 19/25 | Loss: 0.00085233
Iteration 20/25 | Loss: 0.00085233
Iteration 21/25 | Loss: 0.00085233
Iteration 22/25 | Loss: 0.00085233
Iteration 23/25 | Loss: 0.00085233
Iteration 24/25 | Loss: 0.00085233
Iteration 25/25 | Loss: 0.00085233

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00085233
Iteration 2/1000 | Loss: 0.00002297
Iteration 3/1000 | Loss: 0.00001442
Iteration 4/1000 | Loss: 0.00001280
Iteration 5/1000 | Loss: 0.00001201
Iteration 6/1000 | Loss: 0.00001143
Iteration 7/1000 | Loss: 0.00001107
Iteration 8/1000 | Loss: 0.00001078
Iteration 9/1000 | Loss: 0.00001073
Iteration 10/1000 | Loss: 0.00001072
Iteration 11/1000 | Loss: 0.00001067
Iteration 12/1000 | Loss: 0.00001065
Iteration 13/1000 | Loss: 0.00001064
Iteration 14/1000 | Loss: 0.00001055
Iteration 15/1000 | Loss: 0.00001052
Iteration 16/1000 | Loss: 0.00001033
Iteration 17/1000 | Loss: 0.00001024
Iteration 18/1000 | Loss: 0.00001023
Iteration 19/1000 | Loss: 0.00001016
Iteration 20/1000 | Loss: 0.00001015
Iteration 21/1000 | Loss: 0.00001014
Iteration 22/1000 | Loss: 0.00001010
Iteration 23/1000 | Loss: 0.00001002
Iteration 24/1000 | Loss: 0.00001001
Iteration 25/1000 | Loss: 0.00001000
Iteration 26/1000 | Loss: 0.00000999
Iteration 27/1000 | Loss: 0.00000999
Iteration 28/1000 | Loss: 0.00000998
Iteration 29/1000 | Loss: 0.00000998
Iteration 30/1000 | Loss: 0.00000996
Iteration 31/1000 | Loss: 0.00000996
Iteration 32/1000 | Loss: 0.00000996
Iteration 33/1000 | Loss: 0.00000996
Iteration 34/1000 | Loss: 0.00000995
Iteration 35/1000 | Loss: 0.00000993
Iteration 36/1000 | Loss: 0.00000992
Iteration 37/1000 | Loss: 0.00000992
Iteration 38/1000 | Loss: 0.00000991
Iteration 39/1000 | Loss: 0.00000991
Iteration 40/1000 | Loss: 0.00000991
Iteration 41/1000 | Loss: 0.00000991
Iteration 42/1000 | Loss: 0.00000991
Iteration 43/1000 | Loss: 0.00000990
Iteration 44/1000 | Loss: 0.00000989
Iteration 45/1000 | Loss: 0.00000989
Iteration 46/1000 | Loss: 0.00000988
Iteration 47/1000 | Loss: 0.00000988
Iteration 48/1000 | Loss: 0.00000987
Iteration 49/1000 | Loss: 0.00000987
Iteration 50/1000 | Loss: 0.00000987
Iteration 51/1000 | Loss: 0.00000987
Iteration 52/1000 | Loss: 0.00000987
Iteration 53/1000 | Loss: 0.00000987
Iteration 54/1000 | Loss: 0.00000987
Iteration 55/1000 | Loss: 0.00000987
Iteration 56/1000 | Loss: 0.00000987
Iteration 57/1000 | Loss: 0.00000987
Iteration 58/1000 | Loss: 0.00000986
Iteration 59/1000 | Loss: 0.00000986
Iteration 60/1000 | Loss: 0.00000986
Iteration 61/1000 | Loss: 0.00000986
Iteration 62/1000 | Loss: 0.00000986
Iteration 63/1000 | Loss: 0.00000986
Iteration 64/1000 | Loss: 0.00000986
Iteration 65/1000 | Loss: 0.00000986
Iteration 66/1000 | Loss: 0.00000986
Iteration 67/1000 | Loss: 0.00000986
Iteration 68/1000 | Loss: 0.00000986
Iteration 69/1000 | Loss: 0.00000986
Iteration 70/1000 | Loss: 0.00000985
Iteration 71/1000 | Loss: 0.00000985
Iteration 72/1000 | Loss: 0.00000985
Iteration 73/1000 | Loss: 0.00000985
Iteration 74/1000 | Loss: 0.00000985
Iteration 75/1000 | Loss: 0.00000985
Iteration 76/1000 | Loss: 0.00000985
Iteration 77/1000 | Loss: 0.00000985
Iteration 78/1000 | Loss: 0.00000985
Iteration 79/1000 | Loss: 0.00000984
Iteration 80/1000 | Loss: 0.00000984
Iteration 81/1000 | Loss: 0.00000984
Iteration 82/1000 | Loss: 0.00000984
Iteration 83/1000 | Loss: 0.00000984
Iteration 84/1000 | Loss: 0.00000984
Iteration 85/1000 | Loss: 0.00000983
Iteration 86/1000 | Loss: 0.00000983
Iteration 87/1000 | Loss: 0.00000983
Iteration 88/1000 | Loss: 0.00000982
Iteration 89/1000 | Loss: 0.00000982
Iteration 90/1000 | Loss: 0.00000982
Iteration 91/1000 | Loss: 0.00000982
Iteration 92/1000 | Loss: 0.00000982
Iteration 93/1000 | Loss: 0.00000982
Iteration 94/1000 | Loss: 0.00000981
Iteration 95/1000 | Loss: 0.00000981
Iteration 96/1000 | Loss: 0.00000981
Iteration 97/1000 | Loss: 0.00000981
Iteration 98/1000 | Loss: 0.00000981
Iteration 99/1000 | Loss: 0.00000980
Iteration 100/1000 | Loss: 0.00000980
Iteration 101/1000 | Loss: 0.00000980
Iteration 102/1000 | Loss: 0.00000979
Iteration 103/1000 | Loss: 0.00000979
Iteration 104/1000 | Loss: 0.00000979
Iteration 105/1000 | Loss: 0.00000979
Iteration 106/1000 | Loss: 0.00000979
Iteration 107/1000 | Loss: 0.00000979
Iteration 108/1000 | Loss: 0.00000979
Iteration 109/1000 | Loss: 0.00000979
Iteration 110/1000 | Loss: 0.00000979
Iteration 111/1000 | Loss: 0.00000979
Iteration 112/1000 | Loss: 0.00000979
Iteration 113/1000 | Loss: 0.00000979
Iteration 114/1000 | Loss: 0.00000979
Iteration 115/1000 | Loss: 0.00000979
Iteration 116/1000 | Loss: 0.00000979
Iteration 117/1000 | Loss: 0.00000979
Iteration 118/1000 | Loss: 0.00000979
Iteration 119/1000 | Loss: 0.00000979
Iteration 120/1000 | Loss: 0.00000979
Iteration 121/1000 | Loss: 0.00000979
Iteration 122/1000 | Loss: 0.00000979
Iteration 123/1000 | Loss: 0.00000979
Iteration 124/1000 | Loss: 0.00000979
Iteration 125/1000 | Loss: 0.00000979
Iteration 126/1000 | Loss: 0.00000979
Iteration 127/1000 | Loss: 0.00000979
Iteration 128/1000 | Loss: 0.00000979
Iteration 129/1000 | Loss: 0.00000979
Iteration 130/1000 | Loss: 0.00000979
Iteration 131/1000 | Loss: 0.00000979
Iteration 132/1000 | Loss: 0.00000979
Iteration 133/1000 | Loss: 0.00000979
Iteration 134/1000 | Loss: 0.00000979
Iteration 135/1000 | Loss: 0.00000979
Iteration 136/1000 | Loss: 0.00000979
Iteration 137/1000 | Loss: 0.00000979
Iteration 138/1000 | Loss: 0.00000979
Iteration 139/1000 | Loss: 0.00000979
Iteration 140/1000 | Loss: 0.00000979
Iteration 141/1000 | Loss: 0.00000979
Iteration 142/1000 | Loss: 0.00000979
Iteration 143/1000 | Loss: 0.00000979
Iteration 144/1000 | Loss: 0.00000979
Iteration 145/1000 | Loss: 0.00000979
Iteration 146/1000 | Loss: 0.00000979
Iteration 147/1000 | Loss: 0.00000979
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 147. Stopping optimization.
Last 5 losses: [9.789244359126315e-06, 9.789244359126315e-06, 9.789244359126315e-06, 9.789244359126315e-06, 9.789244359126315e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.789244359126315e-06

Optimization complete. Final v2v error: 2.6577582359313965 mm

Highest mean error: 3.5145375728607178 mm for frame 74

Lowest mean error: 2.429145336151123 mm for frame 107

Saving results

Total time: 34.55734181404114
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_023/1062/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_023/1062.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_023/1062
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00437233
Iteration 2/25 | Loss: 0.00121896
Iteration 3/25 | Loss: 0.00115795
Iteration 4/25 | Loss: 0.00115270
Iteration 5/25 | Loss: 0.00115125
Iteration 6/25 | Loss: 0.00115125
Iteration 7/25 | Loss: 0.00115125
Iteration 8/25 | Loss: 0.00115125
Iteration 9/25 | Loss: 0.00115125
Iteration 10/25 | Loss: 0.00115125
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0011512464843690395, 0.0011512464843690395, 0.0011512464843690395, 0.0011512464843690395, 0.0011512464843690395]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011512464843690395

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.00581956
Iteration 2/25 | Loss: 0.00059359
Iteration 3/25 | Loss: 0.00059359
Iteration 4/25 | Loss: 0.00059359
Iteration 5/25 | Loss: 0.00059359
Iteration 6/25 | Loss: 0.00059359
Iteration 7/25 | Loss: 0.00059359
Iteration 8/25 | Loss: 0.00059359
Iteration 9/25 | Loss: 0.00059359
Iteration 10/25 | Loss: 0.00059359
Iteration 11/25 | Loss: 0.00059359
Iteration 12/25 | Loss: 0.00059359
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0005935851368121803, 0.0005935851368121803, 0.0005935851368121803, 0.0005935851368121803, 0.0005935851368121803]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005935851368121803

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00059359
Iteration 2/1000 | Loss: 0.00003910
Iteration 3/1000 | Loss: 0.00002270
Iteration 4/1000 | Loss: 0.00001825
Iteration 5/1000 | Loss: 0.00001706
Iteration 6/1000 | Loss: 0.00001638
Iteration 7/1000 | Loss: 0.00001568
Iteration 8/1000 | Loss: 0.00001504
Iteration 9/1000 | Loss: 0.00001465
Iteration 10/1000 | Loss: 0.00001432
Iteration 11/1000 | Loss: 0.00001403
Iteration 12/1000 | Loss: 0.00001396
Iteration 13/1000 | Loss: 0.00001375
Iteration 14/1000 | Loss: 0.00001352
Iteration 15/1000 | Loss: 0.00001350
Iteration 16/1000 | Loss: 0.00001341
Iteration 17/1000 | Loss: 0.00001341
Iteration 18/1000 | Loss: 0.00001341
Iteration 19/1000 | Loss: 0.00001341
Iteration 20/1000 | Loss: 0.00001341
Iteration 21/1000 | Loss: 0.00001341
Iteration 22/1000 | Loss: 0.00001341
Iteration 23/1000 | Loss: 0.00001341
Iteration 24/1000 | Loss: 0.00001341
Iteration 25/1000 | Loss: 0.00001341
Iteration 26/1000 | Loss: 0.00001341
Iteration 27/1000 | Loss: 0.00001341
Iteration 28/1000 | Loss: 0.00001340
Iteration 29/1000 | Loss: 0.00001340
Iteration 30/1000 | Loss: 0.00001340
Iteration 31/1000 | Loss: 0.00001339
Iteration 32/1000 | Loss: 0.00001339
Iteration 33/1000 | Loss: 0.00001339
Iteration 34/1000 | Loss: 0.00001338
Iteration 35/1000 | Loss: 0.00001338
Iteration 36/1000 | Loss: 0.00001338
Iteration 37/1000 | Loss: 0.00001338
Iteration 38/1000 | Loss: 0.00001338
Iteration 39/1000 | Loss: 0.00001338
Iteration 40/1000 | Loss: 0.00001337
Iteration 41/1000 | Loss: 0.00001336
Iteration 42/1000 | Loss: 0.00001335
Iteration 43/1000 | Loss: 0.00001335
Iteration 44/1000 | Loss: 0.00001335
Iteration 45/1000 | Loss: 0.00001335
Iteration 46/1000 | Loss: 0.00001335
Iteration 47/1000 | Loss: 0.00001335
Iteration 48/1000 | Loss: 0.00001334
Iteration 49/1000 | Loss: 0.00001334
Iteration 50/1000 | Loss: 0.00001334
Iteration 51/1000 | Loss: 0.00001334
Iteration 52/1000 | Loss: 0.00001334
Iteration 53/1000 | Loss: 0.00001334
Iteration 54/1000 | Loss: 0.00001334
Iteration 55/1000 | Loss: 0.00001334
Iteration 56/1000 | Loss: 0.00001334
Iteration 57/1000 | Loss: 0.00001334
Iteration 58/1000 | Loss: 0.00001333
Iteration 59/1000 | Loss: 0.00001333
Iteration 60/1000 | Loss: 0.00001332
Iteration 61/1000 | Loss: 0.00001332
Iteration 62/1000 | Loss: 0.00001331
Iteration 63/1000 | Loss: 0.00001330
Iteration 64/1000 | Loss: 0.00001330
Iteration 65/1000 | Loss: 0.00001330
Iteration 66/1000 | Loss: 0.00001330
Iteration 67/1000 | Loss: 0.00001329
Iteration 68/1000 | Loss: 0.00001329
Iteration 69/1000 | Loss: 0.00001329
Iteration 70/1000 | Loss: 0.00001329
Iteration 71/1000 | Loss: 0.00001329
Iteration 72/1000 | Loss: 0.00001328
Iteration 73/1000 | Loss: 0.00001328
Iteration 74/1000 | Loss: 0.00001328
Iteration 75/1000 | Loss: 0.00001328
Iteration 76/1000 | Loss: 0.00001328
Iteration 77/1000 | Loss: 0.00001328
Iteration 78/1000 | Loss: 0.00001328
Iteration 79/1000 | Loss: 0.00001328
Iteration 80/1000 | Loss: 0.00001328
Iteration 81/1000 | Loss: 0.00001328
Iteration 82/1000 | Loss: 0.00001328
Iteration 83/1000 | Loss: 0.00001328
Iteration 84/1000 | Loss: 0.00001328
Iteration 85/1000 | Loss: 0.00001328
Iteration 86/1000 | Loss: 0.00001328
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 86. Stopping optimization.
Last 5 losses: [1.3279541235533543e-05, 1.3279541235533543e-05, 1.3279541235533543e-05, 1.3279541235533543e-05, 1.3279541235533543e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3279541235533543e-05

Optimization complete. Final v2v error: 3.136570930480957 mm

Highest mean error: 3.165745258331299 mm for frame 104

Lowest mean error: 3.0977046489715576 mm for frame 16

Saving results

Total time: 29.22622585296631
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_023/1050/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_023/1050.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_023/1050
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00918217
Iteration 2/25 | Loss: 0.00142432
Iteration 3/25 | Loss: 0.00129897
Iteration 4/25 | Loss: 0.00128555
Iteration 5/25 | Loss: 0.00128125
Iteration 6/25 | Loss: 0.00128120
Iteration 7/25 | Loss: 0.00128120
Iteration 8/25 | Loss: 0.00128120
Iteration 9/25 | Loss: 0.00128120
Iteration 10/25 | Loss: 0.00128120
Iteration 11/25 | Loss: 0.00128120
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012812025379389524, 0.0012812025379389524, 0.0012812025379389524, 0.0012812025379389524, 0.0012812025379389524]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012812025379389524

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.99116731
Iteration 2/25 | Loss: 0.00099242
Iteration 3/25 | Loss: 0.00099242
Iteration 4/25 | Loss: 0.00099242
Iteration 5/25 | Loss: 0.00099242
Iteration 6/25 | Loss: 0.00099241
Iteration 7/25 | Loss: 0.00099241
Iteration 8/25 | Loss: 0.00099241
Iteration 9/25 | Loss: 0.00099241
Iteration 10/25 | Loss: 0.00099241
Iteration 11/25 | Loss: 0.00099241
Iteration 12/25 | Loss: 0.00099241
Iteration 13/25 | Loss: 0.00099241
Iteration 14/25 | Loss: 0.00099241
Iteration 15/25 | Loss: 0.00099241
Iteration 16/25 | Loss: 0.00099241
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0009924137266352773, 0.0009924137266352773, 0.0009924137266352773, 0.0009924137266352773, 0.0009924137266352773]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009924137266352773

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00099241
Iteration 2/1000 | Loss: 0.00005502
Iteration 3/1000 | Loss: 0.00003337
Iteration 4/1000 | Loss: 0.00002622
Iteration 5/1000 | Loss: 0.00002481
Iteration 6/1000 | Loss: 0.00002388
Iteration 7/1000 | Loss: 0.00002338
Iteration 8/1000 | Loss: 0.00002290
Iteration 9/1000 | Loss: 0.00002260
Iteration 10/1000 | Loss: 0.00002229
Iteration 11/1000 | Loss: 0.00002201
Iteration 12/1000 | Loss: 0.00002179
Iteration 13/1000 | Loss: 0.00002164
Iteration 14/1000 | Loss: 0.00002149
Iteration 15/1000 | Loss: 0.00002142
Iteration 16/1000 | Loss: 0.00002142
Iteration 17/1000 | Loss: 0.00002138
Iteration 18/1000 | Loss: 0.00002137
Iteration 19/1000 | Loss: 0.00002136
Iteration 20/1000 | Loss: 0.00002136
Iteration 21/1000 | Loss: 0.00002136
Iteration 22/1000 | Loss: 0.00002136
Iteration 23/1000 | Loss: 0.00002135
Iteration 24/1000 | Loss: 0.00002135
Iteration 25/1000 | Loss: 0.00002135
Iteration 26/1000 | Loss: 0.00002133
Iteration 27/1000 | Loss: 0.00002133
Iteration 28/1000 | Loss: 0.00002133
Iteration 29/1000 | Loss: 0.00002133
Iteration 30/1000 | Loss: 0.00002133
Iteration 31/1000 | Loss: 0.00002132
Iteration 32/1000 | Loss: 0.00002132
Iteration 33/1000 | Loss: 0.00002132
Iteration 34/1000 | Loss: 0.00002132
Iteration 35/1000 | Loss: 0.00002132
Iteration 36/1000 | Loss: 0.00002132
Iteration 37/1000 | Loss: 0.00002132
Iteration 38/1000 | Loss: 0.00002130
Iteration 39/1000 | Loss: 0.00002130
Iteration 40/1000 | Loss: 0.00002130
Iteration 41/1000 | Loss: 0.00002130
Iteration 42/1000 | Loss: 0.00002129
Iteration 43/1000 | Loss: 0.00002129
Iteration 44/1000 | Loss: 0.00002128
Iteration 45/1000 | Loss: 0.00002128
Iteration 46/1000 | Loss: 0.00002128
Iteration 47/1000 | Loss: 0.00002127
Iteration 48/1000 | Loss: 0.00002126
Iteration 49/1000 | Loss: 0.00002125
Iteration 50/1000 | Loss: 0.00002125
Iteration 51/1000 | Loss: 0.00002125
Iteration 52/1000 | Loss: 0.00002122
Iteration 53/1000 | Loss: 0.00002122
Iteration 54/1000 | Loss: 0.00002121
Iteration 55/1000 | Loss: 0.00002121
Iteration 56/1000 | Loss: 0.00002120
Iteration 57/1000 | Loss: 0.00002120
Iteration 58/1000 | Loss: 0.00002120
Iteration 59/1000 | Loss: 0.00002120
Iteration 60/1000 | Loss: 0.00002119
Iteration 61/1000 | Loss: 0.00002119
Iteration 62/1000 | Loss: 0.00002119
Iteration 63/1000 | Loss: 0.00002119
Iteration 64/1000 | Loss: 0.00002118
Iteration 65/1000 | Loss: 0.00002118
Iteration 66/1000 | Loss: 0.00002118
Iteration 67/1000 | Loss: 0.00002118
Iteration 68/1000 | Loss: 0.00002117
Iteration 69/1000 | Loss: 0.00002117
Iteration 70/1000 | Loss: 0.00002117
Iteration 71/1000 | Loss: 0.00002117
Iteration 72/1000 | Loss: 0.00002117
Iteration 73/1000 | Loss: 0.00002116
Iteration 74/1000 | Loss: 0.00002116
Iteration 75/1000 | Loss: 0.00002116
Iteration 76/1000 | Loss: 0.00002116
Iteration 77/1000 | Loss: 0.00002116
Iteration 78/1000 | Loss: 0.00002116
Iteration 79/1000 | Loss: 0.00002116
Iteration 80/1000 | Loss: 0.00002116
Iteration 81/1000 | Loss: 0.00002116
Iteration 82/1000 | Loss: 0.00002116
Iteration 83/1000 | Loss: 0.00002116
Iteration 84/1000 | Loss: 0.00002116
Iteration 85/1000 | Loss: 0.00002115
Iteration 86/1000 | Loss: 0.00002115
Iteration 87/1000 | Loss: 0.00002115
Iteration 88/1000 | Loss: 0.00002115
Iteration 89/1000 | Loss: 0.00002114
Iteration 90/1000 | Loss: 0.00002114
Iteration 91/1000 | Loss: 0.00002113
Iteration 92/1000 | Loss: 0.00002113
Iteration 93/1000 | Loss: 0.00002112
Iteration 94/1000 | Loss: 0.00002112
Iteration 95/1000 | Loss: 0.00002111
Iteration 96/1000 | Loss: 0.00002111
Iteration 97/1000 | Loss: 0.00002111
Iteration 98/1000 | Loss: 0.00002111
Iteration 99/1000 | Loss: 0.00002111
Iteration 100/1000 | Loss: 0.00002110
Iteration 101/1000 | Loss: 0.00002110
Iteration 102/1000 | Loss: 0.00002110
Iteration 103/1000 | Loss: 0.00002109
Iteration 104/1000 | Loss: 0.00002109
Iteration 105/1000 | Loss: 0.00002109
Iteration 106/1000 | Loss: 0.00002109
Iteration 107/1000 | Loss: 0.00002109
Iteration 108/1000 | Loss: 0.00002109
Iteration 109/1000 | Loss: 0.00002108
Iteration 110/1000 | Loss: 0.00002108
Iteration 111/1000 | Loss: 0.00002108
Iteration 112/1000 | Loss: 0.00002108
Iteration 113/1000 | Loss: 0.00002108
Iteration 114/1000 | Loss: 0.00002108
Iteration 115/1000 | Loss: 0.00002108
Iteration 116/1000 | Loss: 0.00002108
Iteration 117/1000 | Loss: 0.00002107
Iteration 118/1000 | Loss: 0.00002107
Iteration 119/1000 | Loss: 0.00002107
Iteration 120/1000 | Loss: 0.00002106
Iteration 121/1000 | Loss: 0.00002106
Iteration 122/1000 | Loss: 0.00002106
Iteration 123/1000 | Loss: 0.00002106
Iteration 124/1000 | Loss: 0.00002105
Iteration 125/1000 | Loss: 0.00002105
Iteration 126/1000 | Loss: 0.00002105
Iteration 127/1000 | Loss: 0.00002105
Iteration 128/1000 | Loss: 0.00002104
Iteration 129/1000 | Loss: 0.00002104
Iteration 130/1000 | Loss: 0.00002104
Iteration 131/1000 | Loss: 0.00002104
Iteration 132/1000 | Loss: 0.00002104
Iteration 133/1000 | Loss: 0.00002104
Iteration 134/1000 | Loss: 0.00002104
Iteration 135/1000 | Loss: 0.00002103
Iteration 136/1000 | Loss: 0.00002103
Iteration 137/1000 | Loss: 0.00002103
Iteration 138/1000 | Loss: 0.00002103
Iteration 139/1000 | Loss: 0.00002103
Iteration 140/1000 | Loss: 0.00002103
Iteration 141/1000 | Loss: 0.00002103
Iteration 142/1000 | Loss: 0.00002103
Iteration 143/1000 | Loss: 0.00002102
Iteration 144/1000 | Loss: 0.00002102
Iteration 145/1000 | Loss: 0.00002102
Iteration 146/1000 | Loss: 0.00002102
Iteration 147/1000 | Loss: 0.00002102
Iteration 148/1000 | Loss: 0.00002102
Iteration 149/1000 | Loss: 0.00002102
Iteration 150/1000 | Loss: 0.00002102
Iteration 151/1000 | Loss: 0.00002101
Iteration 152/1000 | Loss: 0.00002101
Iteration 153/1000 | Loss: 0.00002101
Iteration 154/1000 | Loss: 0.00002101
Iteration 155/1000 | Loss: 0.00002101
Iteration 156/1000 | Loss: 0.00002101
Iteration 157/1000 | Loss: 0.00002100
Iteration 158/1000 | Loss: 0.00002100
Iteration 159/1000 | Loss: 0.00002100
Iteration 160/1000 | Loss: 0.00002100
Iteration 161/1000 | Loss: 0.00002100
Iteration 162/1000 | Loss: 0.00002100
Iteration 163/1000 | Loss: 0.00002100
Iteration 164/1000 | Loss: 0.00002100
Iteration 165/1000 | Loss: 0.00002100
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 165. Stopping optimization.
Last 5 losses: [2.1000891138100997e-05, 2.1000891138100997e-05, 2.1000891138100997e-05, 2.1000891138100997e-05, 2.1000891138100997e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1000891138100997e-05

Optimization complete. Final v2v error: 3.758868455886841 mm

Highest mean error: 4.469799518585205 mm for frame 81

Lowest mean error: 3.2706186771392822 mm for frame 0

Saving results

Total time: 39.965189695358276
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_023/1080/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_023/1080.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_023/1080
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00793415
Iteration 2/25 | Loss: 0.00117927
Iteration 3/25 | Loss: 0.00111628
Iteration 4/25 | Loss: 0.00111160
Iteration 5/25 | Loss: 0.00110999
Iteration 6/25 | Loss: 0.00110999
Iteration 7/25 | Loss: 0.00110999
Iteration 8/25 | Loss: 0.00110999
Iteration 9/25 | Loss: 0.00110999
Iteration 10/25 | Loss: 0.00110999
Iteration 11/25 | Loss: 0.00110999
Iteration 12/25 | Loss: 0.00110999
Iteration 13/25 | Loss: 0.00110999
Iteration 14/25 | Loss: 0.00110999
Iteration 15/25 | Loss: 0.00110999
Iteration 16/25 | Loss: 0.00110999
Iteration 17/25 | Loss: 0.00110999
Iteration 18/25 | Loss: 0.00110999
Iteration 19/25 | Loss: 0.00110999
Iteration 20/25 | Loss: 0.00110999
Iteration 21/25 | Loss: 0.00110999
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0011099866824224591, 0.0011099866824224591, 0.0011099866824224591, 0.0011099866824224591, 0.0011099866824224591]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011099866824224591

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36516166
Iteration 2/25 | Loss: 0.00083155
Iteration 3/25 | Loss: 0.00083155
Iteration 4/25 | Loss: 0.00083154
Iteration 5/25 | Loss: 0.00083154
Iteration 6/25 | Loss: 0.00083154
Iteration 7/25 | Loss: 0.00083154
Iteration 8/25 | Loss: 0.00083154
Iteration 9/25 | Loss: 0.00083154
Iteration 10/25 | Loss: 0.00083154
Iteration 11/25 | Loss: 0.00083154
Iteration 12/25 | Loss: 0.00083154
Iteration 13/25 | Loss: 0.00083154
Iteration 14/25 | Loss: 0.00083154
Iteration 15/25 | Loss: 0.00083154
Iteration 16/25 | Loss: 0.00083154
Iteration 17/25 | Loss: 0.00083154
Iteration 18/25 | Loss: 0.00083154
Iteration 19/25 | Loss: 0.00083154
Iteration 20/25 | Loss: 0.00083154
Iteration 21/25 | Loss: 0.00083154
Iteration 22/25 | Loss: 0.00083154
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0008315419545397162, 0.0008315419545397162, 0.0008315419545397162, 0.0008315419545397162, 0.0008315419545397162]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008315419545397162

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00083154
Iteration 2/1000 | Loss: 0.00002168
Iteration 3/1000 | Loss: 0.00001450
Iteration 4/1000 | Loss: 0.00001289
Iteration 5/1000 | Loss: 0.00001195
Iteration 6/1000 | Loss: 0.00001137
Iteration 7/1000 | Loss: 0.00001086
Iteration 8/1000 | Loss: 0.00001062
Iteration 9/1000 | Loss: 0.00001038
Iteration 10/1000 | Loss: 0.00001021
Iteration 11/1000 | Loss: 0.00001010
Iteration 12/1000 | Loss: 0.00001009
Iteration 13/1000 | Loss: 0.00001009
Iteration 14/1000 | Loss: 0.00001008
Iteration 15/1000 | Loss: 0.00001007
Iteration 16/1000 | Loss: 0.00001005
Iteration 17/1000 | Loss: 0.00001003
Iteration 18/1000 | Loss: 0.00001002
Iteration 19/1000 | Loss: 0.00001002
Iteration 20/1000 | Loss: 0.00001002
Iteration 21/1000 | Loss: 0.00000994
Iteration 22/1000 | Loss: 0.00000990
Iteration 23/1000 | Loss: 0.00000989
Iteration 24/1000 | Loss: 0.00000988
Iteration 25/1000 | Loss: 0.00000987
Iteration 26/1000 | Loss: 0.00000984
Iteration 27/1000 | Loss: 0.00000984
Iteration 28/1000 | Loss: 0.00000983
Iteration 29/1000 | Loss: 0.00000983
Iteration 30/1000 | Loss: 0.00000983
Iteration 31/1000 | Loss: 0.00000983
Iteration 32/1000 | Loss: 0.00000983
Iteration 33/1000 | Loss: 0.00000983
Iteration 34/1000 | Loss: 0.00000983
Iteration 35/1000 | Loss: 0.00000983
Iteration 36/1000 | Loss: 0.00000983
Iteration 37/1000 | Loss: 0.00000983
Iteration 38/1000 | Loss: 0.00000983
Iteration 39/1000 | Loss: 0.00000982
Iteration 40/1000 | Loss: 0.00000982
Iteration 41/1000 | Loss: 0.00000982
Iteration 42/1000 | Loss: 0.00000981
Iteration 43/1000 | Loss: 0.00000981
Iteration 44/1000 | Loss: 0.00000980
Iteration 45/1000 | Loss: 0.00000980
Iteration 46/1000 | Loss: 0.00000980
Iteration 47/1000 | Loss: 0.00000979
Iteration 48/1000 | Loss: 0.00000979
Iteration 49/1000 | Loss: 0.00000979
Iteration 50/1000 | Loss: 0.00000979
Iteration 51/1000 | Loss: 0.00000979
Iteration 52/1000 | Loss: 0.00000979
Iteration 53/1000 | Loss: 0.00000978
Iteration 54/1000 | Loss: 0.00000978
Iteration 55/1000 | Loss: 0.00000975
Iteration 56/1000 | Loss: 0.00000975
Iteration 57/1000 | Loss: 0.00000975
Iteration 58/1000 | Loss: 0.00000975
Iteration 59/1000 | Loss: 0.00000975
Iteration 60/1000 | Loss: 0.00000975
Iteration 61/1000 | Loss: 0.00000975
Iteration 62/1000 | Loss: 0.00000974
Iteration 63/1000 | Loss: 0.00000974
Iteration 64/1000 | Loss: 0.00000973
Iteration 65/1000 | Loss: 0.00000973
Iteration 66/1000 | Loss: 0.00000972
Iteration 67/1000 | Loss: 0.00000972
Iteration 68/1000 | Loss: 0.00000972
Iteration 69/1000 | Loss: 0.00000972
Iteration 70/1000 | Loss: 0.00000971
Iteration 71/1000 | Loss: 0.00000971
Iteration 72/1000 | Loss: 0.00000970
Iteration 73/1000 | Loss: 0.00000970
Iteration 74/1000 | Loss: 0.00000969
Iteration 75/1000 | Loss: 0.00000969
Iteration 76/1000 | Loss: 0.00000969
Iteration 77/1000 | Loss: 0.00000968
Iteration 78/1000 | Loss: 0.00000966
Iteration 79/1000 | Loss: 0.00000966
Iteration 80/1000 | Loss: 0.00000966
Iteration 81/1000 | Loss: 0.00000966
Iteration 82/1000 | Loss: 0.00000966
Iteration 83/1000 | Loss: 0.00000966
Iteration 84/1000 | Loss: 0.00000965
Iteration 85/1000 | Loss: 0.00000965
Iteration 86/1000 | Loss: 0.00000965
Iteration 87/1000 | Loss: 0.00000964
Iteration 88/1000 | Loss: 0.00000964
Iteration 89/1000 | Loss: 0.00000963
Iteration 90/1000 | Loss: 0.00000963
Iteration 91/1000 | Loss: 0.00000962
Iteration 92/1000 | Loss: 0.00000962
Iteration 93/1000 | Loss: 0.00000961
Iteration 94/1000 | Loss: 0.00000961
Iteration 95/1000 | Loss: 0.00000961
Iteration 96/1000 | Loss: 0.00000960
Iteration 97/1000 | Loss: 0.00000960
Iteration 98/1000 | Loss: 0.00000958
Iteration 99/1000 | Loss: 0.00000958
Iteration 100/1000 | Loss: 0.00000958
Iteration 101/1000 | Loss: 0.00000958
Iteration 102/1000 | Loss: 0.00000958
Iteration 103/1000 | Loss: 0.00000958
Iteration 104/1000 | Loss: 0.00000958
Iteration 105/1000 | Loss: 0.00000958
Iteration 106/1000 | Loss: 0.00000958
Iteration 107/1000 | Loss: 0.00000958
Iteration 108/1000 | Loss: 0.00000957
Iteration 109/1000 | Loss: 0.00000957
Iteration 110/1000 | Loss: 0.00000957
Iteration 111/1000 | Loss: 0.00000957
Iteration 112/1000 | Loss: 0.00000957
Iteration 113/1000 | Loss: 0.00000957
Iteration 114/1000 | Loss: 0.00000957
Iteration 115/1000 | Loss: 0.00000957
Iteration 116/1000 | Loss: 0.00000957
Iteration 117/1000 | Loss: 0.00000957
Iteration 118/1000 | Loss: 0.00000957
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 118. Stopping optimization.
Last 5 losses: [9.572795534040779e-06, 9.572795534040779e-06, 9.572795534040779e-06, 9.572795534040779e-06, 9.572795534040779e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.572795534040779e-06

Optimization complete. Final v2v error: 2.6558310985565186 mm

Highest mean error: 2.8442165851593018 mm for frame 85

Lowest mean error: 2.5024826526641846 mm for frame 11

Saving results

Total time: 33.408886432647705
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_023/1019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_023/1019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_023/1019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01018905
Iteration 2/25 | Loss: 0.00281534
Iteration 3/25 | Loss: 0.00214465
Iteration 4/25 | Loss: 0.00209257
Iteration 5/25 | Loss: 0.00198512
Iteration 6/25 | Loss: 0.00184972
Iteration 7/25 | Loss: 0.00164912
Iteration 8/25 | Loss: 0.00159646
Iteration 9/25 | Loss: 0.00152760
Iteration 10/25 | Loss: 0.00151434
Iteration 11/25 | Loss: 0.00149387
Iteration 12/25 | Loss: 0.00146420
Iteration 13/25 | Loss: 0.00145813
Iteration 14/25 | Loss: 0.00146063
Iteration 15/25 | Loss: 0.00145301
Iteration 16/25 | Loss: 0.00145493
Iteration 17/25 | Loss: 0.00144903
Iteration 18/25 | Loss: 0.00143875
Iteration 19/25 | Loss: 0.00143542
Iteration 20/25 | Loss: 0.00143100
Iteration 21/25 | Loss: 0.00142985
Iteration 22/25 | Loss: 0.00142836
Iteration 23/25 | Loss: 0.00142635
Iteration 24/25 | Loss: 0.00142116
Iteration 25/25 | Loss: 0.00142055

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.47357607
Iteration 2/25 | Loss: 0.00373332
Iteration 3/25 | Loss: 0.00283618
Iteration 4/25 | Loss: 0.00283618
Iteration 5/25 | Loss: 0.00283618
Iteration 6/25 | Loss: 0.00283618
Iteration 7/25 | Loss: 0.00283618
Iteration 8/25 | Loss: 0.00283618
Iteration 9/25 | Loss: 0.00283618
Iteration 10/25 | Loss: 0.00283618
Iteration 11/25 | Loss: 0.00283618
Iteration 12/25 | Loss: 0.00283618
Iteration 13/25 | Loss: 0.00283618
Iteration 14/25 | Loss: 0.00283618
Iteration 15/25 | Loss: 0.00283618
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.002836180618032813, 0.002836180618032813, 0.002836180618032813, 0.002836180618032813, 0.002836180618032813]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002836180618032813

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00283618
Iteration 2/1000 | Loss: 0.00169677
Iteration 3/1000 | Loss: 0.00054372
Iteration 4/1000 | Loss: 0.00030753
Iteration 5/1000 | Loss: 0.00024349
Iteration 6/1000 | Loss: 0.00020635
Iteration 7/1000 | Loss: 0.00039386
Iteration 8/1000 | Loss: 0.00013666
Iteration 9/1000 | Loss: 0.00013056
Iteration 10/1000 | Loss: 0.00080130
Iteration 11/1000 | Loss: 0.00012409
Iteration 12/1000 | Loss: 0.00054233
Iteration 13/1000 | Loss: 0.00012233
Iteration 14/1000 | Loss: 0.00011672
Iteration 15/1000 | Loss: 0.00011371
Iteration 16/1000 | Loss: 0.00011179
Iteration 17/1000 | Loss: 0.00041441
Iteration 18/1000 | Loss: 0.00011038
Iteration 19/1000 | Loss: 0.00010929
Iteration 20/1000 | Loss: 0.00183860
Iteration 21/1000 | Loss: 0.00087870
Iteration 22/1000 | Loss: 0.00121228
Iteration 23/1000 | Loss: 0.00061403
Iteration 24/1000 | Loss: 0.00197836
Iteration 25/1000 | Loss: 0.00011779
Iteration 26/1000 | Loss: 0.00010987
Iteration 27/1000 | Loss: 0.00010430
Iteration 28/1000 | Loss: 0.00043207
Iteration 29/1000 | Loss: 0.00009828
Iteration 30/1000 | Loss: 0.00009603
Iteration 31/1000 | Loss: 0.00009505
Iteration 32/1000 | Loss: 0.00009412
Iteration 33/1000 | Loss: 0.00028304
Iteration 34/1000 | Loss: 0.00010167
Iteration 35/1000 | Loss: 0.00009199
Iteration 36/1000 | Loss: 0.00009077
Iteration 37/1000 | Loss: 0.00009003
Iteration 38/1000 | Loss: 0.00008917
Iteration 39/1000 | Loss: 0.00008851
Iteration 40/1000 | Loss: 0.00008792
Iteration 41/1000 | Loss: 0.00049688
Iteration 42/1000 | Loss: 0.00148584
Iteration 43/1000 | Loss: 0.00094265
Iteration 44/1000 | Loss: 0.00044726
Iteration 45/1000 | Loss: 0.00030390
Iteration 46/1000 | Loss: 0.00015022
Iteration 47/1000 | Loss: 0.00014547
Iteration 48/1000 | Loss: 0.00014375
Iteration 49/1000 | Loss: 0.00008990
Iteration 50/1000 | Loss: 0.00103533
Iteration 51/1000 | Loss: 0.00109443
Iteration 52/1000 | Loss: 0.00072078
Iteration 53/1000 | Loss: 0.00012990
Iteration 54/1000 | Loss: 0.00105478
Iteration 55/1000 | Loss: 0.00037367
Iteration 56/1000 | Loss: 0.00037745
Iteration 57/1000 | Loss: 0.00027273
Iteration 58/1000 | Loss: 0.00012428
Iteration 59/1000 | Loss: 0.00010711
Iteration 60/1000 | Loss: 0.00008448
Iteration 61/1000 | Loss: 0.00008128
Iteration 62/1000 | Loss: 0.00007752
Iteration 63/1000 | Loss: 0.00007416
Iteration 64/1000 | Loss: 0.00007106
Iteration 65/1000 | Loss: 0.00006903
Iteration 66/1000 | Loss: 0.00006798
Iteration 67/1000 | Loss: 0.00006715
Iteration 68/1000 | Loss: 0.00006630
Iteration 69/1000 | Loss: 0.00006557
Iteration 70/1000 | Loss: 0.00006493
Iteration 71/1000 | Loss: 0.00006431
Iteration 72/1000 | Loss: 0.00006384
Iteration 73/1000 | Loss: 0.00006344
Iteration 74/1000 | Loss: 0.00006311
Iteration 75/1000 | Loss: 0.00006289
Iteration 76/1000 | Loss: 0.00006265
Iteration 77/1000 | Loss: 0.00006250
Iteration 78/1000 | Loss: 0.00006249
Iteration 79/1000 | Loss: 0.00006230
Iteration 80/1000 | Loss: 0.00006208
Iteration 81/1000 | Loss: 0.00006187
Iteration 82/1000 | Loss: 0.00006183
Iteration 83/1000 | Loss: 0.00006178
Iteration 84/1000 | Loss: 0.00006170
Iteration 85/1000 | Loss: 0.00006167
Iteration 86/1000 | Loss: 0.00006167
Iteration 87/1000 | Loss: 0.00006165
Iteration 88/1000 | Loss: 0.00006160
Iteration 89/1000 | Loss: 0.00006160
Iteration 90/1000 | Loss: 0.00006159
Iteration 91/1000 | Loss: 0.00006159
Iteration 92/1000 | Loss: 0.00006158
Iteration 93/1000 | Loss: 0.00006158
Iteration 94/1000 | Loss: 0.00006158
Iteration 95/1000 | Loss: 0.00006158
Iteration 96/1000 | Loss: 0.00006158
Iteration 97/1000 | Loss: 0.00006157
Iteration 98/1000 | Loss: 0.00006157
Iteration 99/1000 | Loss: 0.00006157
Iteration 100/1000 | Loss: 0.00006157
Iteration 101/1000 | Loss: 0.00006157
Iteration 102/1000 | Loss: 0.00006157
Iteration 103/1000 | Loss: 0.00006156
Iteration 104/1000 | Loss: 0.00006156
Iteration 105/1000 | Loss: 0.00006155
Iteration 106/1000 | Loss: 0.00006155
Iteration 107/1000 | Loss: 0.00006155
Iteration 108/1000 | Loss: 0.00006154
Iteration 109/1000 | Loss: 0.00006153
Iteration 110/1000 | Loss: 0.00006153
Iteration 111/1000 | Loss: 0.00006152
Iteration 112/1000 | Loss: 0.00006152
Iteration 113/1000 | Loss: 0.00006151
Iteration 114/1000 | Loss: 0.00006151
Iteration 115/1000 | Loss: 0.00006150
Iteration 116/1000 | Loss: 0.00006150
Iteration 117/1000 | Loss: 0.00006150
Iteration 118/1000 | Loss: 0.00006150
Iteration 119/1000 | Loss: 0.00006149
Iteration 120/1000 | Loss: 0.00006149
Iteration 121/1000 | Loss: 0.00006149
Iteration 122/1000 | Loss: 0.00006149
Iteration 123/1000 | Loss: 0.00006149
Iteration 124/1000 | Loss: 0.00006149
Iteration 125/1000 | Loss: 0.00006149
Iteration 126/1000 | Loss: 0.00006149
Iteration 127/1000 | Loss: 0.00006148
Iteration 128/1000 | Loss: 0.00006148
Iteration 129/1000 | Loss: 0.00006147
Iteration 130/1000 | Loss: 0.00006147
Iteration 131/1000 | Loss: 0.00006146
Iteration 132/1000 | Loss: 0.00006146
Iteration 133/1000 | Loss: 0.00006146
Iteration 134/1000 | Loss: 0.00006145
Iteration 135/1000 | Loss: 0.00006145
Iteration 136/1000 | Loss: 0.00006145
Iteration 137/1000 | Loss: 0.00006145
Iteration 138/1000 | Loss: 0.00006144
Iteration 139/1000 | Loss: 0.00006144
Iteration 140/1000 | Loss: 0.00006143
Iteration 141/1000 | Loss: 0.00006143
Iteration 142/1000 | Loss: 0.00006143
Iteration 143/1000 | Loss: 0.00006143
Iteration 144/1000 | Loss: 0.00006143
Iteration 145/1000 | Loss: 0.00006143
Iteration 146/1000 | Loss: 0.00006142
Iteration 147/1000 | Loss: 0.00006142
Iteration 148/1000 | Loss: 0.00006142
Iteration 149/1000 | Loss: 0.00006142
Iteration 150/1000 | Loss: 0.00006142
Iteration 151/1000 | Loss: 0.00006142
Iteration 152/1000 | Loss: 0.00006142
Iteration 153/1000 | Loss: 0.00006142
Iteration 154/1000 | Loss: 0.00006142
Iteration 155/1000 | Loss: 0.00006142
Iteration 156/1000 | Loss: 0.00006142
Iteration 157/1000 | Loss: 0.00006142
Iteration 158/1000 | Loss: 0.00006142
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 158. Stopping optimization.
Last 5 losses: [6.142161146271974e-05, 6.142161146271974e-05, 6.142161146271974e-05, 6.142161146271974e-05, 6.142161146271974e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 6.142161146271974e-05

Optimization complete. Final v2v error: 4.564174652099609 mm

Highest mean error: 12.321436882019043 mm for frame 46

Lowest mean error: 3.2210240364074707 mm for frame 13

Saving results

Total time: 171.48079586029053
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_023/1009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_023/1009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_023/1009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00795608
Iteration 2/25 | Loss: 0.00183684
Iteration 3/25 | Loss: 0.00134467
Iteration 4/25 | Loss: 0.00128512
Iteration 5/25 | Loss: 0.00127864
Iteration 6/25 | Loss: 0.00127757
Iteration 7/25 | Loss: 0.00127721
Iteration 8/25 | Loss: 0.00127721
Iteration 9/25 | Loss: 0.00127721
Iteration 10/25 | Loss: 0.00127721
Iteration 11/25 | Loss: 0.00127721
Iteration 12/25 | Loss: 0.00127721
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.001277206582017243, 0.001277206582017243, 0.001277206582017243, 0.001277206582017243, 0.001277206582017243]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001277206582017243

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.22569442
Iteration 2/25 | Loss: 0.00064772
Iteration 3/25 | Loss: 0.00064769
Iteration 4/25 | Loss: 0.00064769
Iteration 5/25 | Loss: 0.00064768
Iteration 6/25 | Loss: 0.00064768
Iteration 7/25 | Loss: 0.00064768
Iteration 8/25 | Loss: 0.00064768
Iteration 9/25 | Loss: 0.00064768
Iteration 10/25 | Loss: 0.00064768
Iteration 11/25 | Loss: 0.00064768
Iteration 12/25 | Loss: 0.00064768
Iteration 13/25 | Loss: 0.00064768
Iteration 14/25 | Loss: 0.00064768
Iteration 15/25 | Loss: 0.00064768
Iteration 16/25 | Loss: 0.00064768
Iteration 17/25 | Loss: 0.00064768
Iteration 18/25 | Loss: 0.00064768
Iteration 19/25 | Loss: 0.00064768
Iteration 20/25 | Loss: 0.00064768
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.000647683278657496, 0.000647683278657496, 0.000647683278657496, 0.000647683278657496, 0.000647683278657496]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000647683278657496

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00064768
Iteration 2/1000 | Loss: 0.00004547
Iteration 3/1000 | Loss: 0.00002885
Iteration 4/1000 | Loss: 0.00002527
Iteration 5/1000 | Loss: 0.00002367
Iteration 6/1000 | Loss: 0.00002275
Iteration 7/1000 | Loss: 0.00002246
Iteration 8/1000 | Loss: 0.00002205
Iteration 9/1000 | Loss: 0.00002168
Iteration 10/1000 | Loss: 0.00002142
Iteration 11/1000 | Loss: 0.00002117
Iteration 12/1000 | Loss: 0.00002099
Iteration 13/1000 | Loss: 0.00002095
Iteration 14/1000 | Loss: 0.00002092
Iteration 15/1000 | Loss: 0.00002088
Iteration 16/1000 | Loss: 0.00002087
Iteration 17/1000 | Loss: 0.00002086
Iteration 18/1000 | Loss: 0.00002082
Iteration 19/1000 | Loss: 0.00002072
Iteration 20/1000 | Loss: 0.00002071
Iteration 21/1000 | Loss: 0.00002069
Iteration 22/1000 | Loss: 0.00002069
Iteration 23/1000 | Loss: 0.00002066
Iteration 24/1000 | Loss: 0.00002066
Iteration 25/1000 | Loss: 0.00002066
Iteration 26/1000 | Loss: 0.00002066
Iteration 27/1000 | Loss: 0.00002066
Iteration 28/1000 | Loss: 0.00002066
Iteration 29/1000 | Loss: 0.00002066
Iteration 30/1000 | Loss: 0.00002066
Iteration 31/1000 | Loss: 0.00002066
Iteration 32/1000 | Loss: 0.00002066
Iteration 33/1000 | Loss: 0.00002066
Iteration 34/1000 | Loss: 0.00002066
Iteration 35/1000 | Loss: 0.00002066
Iteration 36/1000 | Loss: 0.00002066
Iteration 37/1000 | Loss: 0.00002066
Iteration 38/1000 | Loss: 0.00002066
Iteration 39/1000 | Loss: 0.00002066
Iteration 40/1000 | Loss: 0.00002066
Iteration 41/1000 | Loss: 0.00002066
Iteration 42/1000 | Loss: 0.00002066
Iteration 43/1000 | Loss: 0.00002066
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 43. Stopping optimization.
Last 5 losses: [2.0656094420701265e-05, 2.0656094420701265e-05, 2.0656094420701265e-05, 2.0656094420701265e-05, 2.0656094420701265e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0656094420701265e-05

Optimization complete. Final v2v error: 3.8051536083221436 mm

Highest mean error: 4.122154712677002 mm for frame 112

Lowest mean error: 3.5241048336029053 mm for frame 45

Saving results

Total time: 31.38451075553894
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_023/1032/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_023/1032.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_023/1032
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00421424
Iteration 2/25 | Loss: 0.00155645
Iteration 3/25 | Loss: 0.00126583
Iteration 4/25 | Loss: 0.00122359
Iteration 5/25 | Loss: 0.00121500
Iteration 6/25 | Loss: 0.00121315
Iteration 7/25 | Loss: 0.00121289
Iteration 8/25 | Loss: 0.00121289
Iteration 9/25 | Loss: 0.00121289
Iteration 10/25 | Loss: 0.00121289
Iteration 11/25 | Loss: 0.00121289
Iteration 12/25 | Loss: 0.00121289
Iteration 13/25 | Loss: 0.00121289
Iteration 14/25 | Loss: 0.00121289
Iteration 15/25 | Loss: 0.00121289
Iteration 16/25 | Loss: 0.00121289
Iteration 17/25 | Loss: 0.00121289
Iteration 18/25 | Loss: 0.00121289
Iteration 19/25 | Loss: 0.00121289
Iteration 20/25 | Loss: 0.00121289
Iteration 21/25 | Loss: 0.00121289
Iteration 22/25 | Loss: 0.00121289
Iteration 23/25 | Loss: 0.00121289
Iteration 24/25 | Loss: 0.00121289
Iteration 25/25 | Loss: 0.00121289

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36836982
Iteration 2/25 | Loss: 0.00057913
Iteration 3/25 | Loss: 0.00057912
Iteration 4/25 | Loss: 0.00057912
Iteration 5/25 | Loss: 0.00057912
Iteration 6/25 | Loss: 0.00057912
Iteration 7/25 | Loss: 0.00057912
Iteration 8/25 | Loss: 0.00057912
Iteration 9/25 | Loss: 0.00057912
Iteration 10/25 | Loss: 0.00057912
Iteration 11/25 | Loss: 0.00057912
Iteration 12/25 | Loss: 0.00057912
Iteration 13/25 | Loss: 0.00057912
Iteration 14/25 | Loss: 0.00057912
Iteration 15/25 | Loss: 0.00057912
Iteration 16/25 | Loss: 0.00057912
Iteration 17/25 | Loss: 0.00057912
Iteration 18/25 | Loss: 0.00057912
Iteration 19/25 | Loss: 0.00057912
Iteration 20/25 | Loss: 0.00057912
Iteration 21/25 | Loss: 0.00057912
Iteration 22/25 | Loss: 0.00057912
Iteration 23/25 | Loss: 0.00057912
Iteration 24/25 | Loss: 0.00057912
Iteration 25/25 | Loss: 0.00057912

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00057912
Iteration 2/1000 | Loss: 0.00003450
Iteration 3/1000 | Loss: 0.00002506
Iteration 4/1000 | Loss: 0.00002345
Iteration 5/1000 | Loss: 0.00002276
Iteration 6/1000 | Loss: 0.00002203
Iteration 7/1000 | Loss: 0.00002135
Iteration 8/1000 | Loss: 0.00002088
Iteration 9/1000 | Loss: 0.00002051
Iteration 10/1000 | Loss: 0.00002017
Iteration 11/1000 | Loss: 0.00001993
Iteration 12/1000 | Loss: 0.00001985
Iteration 13/1000 | Loss: 0.00001972
Iteration 14/1000 | Loss: 0.00001971
Iteration 15/1000 | Loss: 0.00001969
Iteration 16/1000 | Loss: 0.00001965
Iteration 17/1000 | Loss: 0.00001963
Iteration 18/1000 | Loss: 0.00001962
Iteration 19/1000 | Loss: 0.00001962
Iteration 20/1000 | Loss: 0.00001961
Iteration 21/1000 | Loss: 0.00001960
Iteration 22/1000 | Loss: 0.00001959
Iteration 23/1000 | Loss: 0.00001959
Iteration 24/1000 | Loss: 0.00001958
Iteration 25/1000 | Loss: 0.00001958
Iteration 26/1000 | Loss: 0.00001958
Iteration 27/1000 | Loss: 0.00001958
Iteration 28/1000 | Loss: 0.00001957
Iteration 29/1000 | Loss: 0.00001957
Iteration 30/1000 | Loss: 0.00001957
Iteration 31/1000 | Loss: 0.00001957
Iteration 32/1000 | Loss: 0.00001957
Iteration 33/1000 | Loss: 0.00001956
Iteration 34/1000 | Loss: 0.00001956
Iteration 35/1000 | Loss: 0.00001956
Iteration 36/1000 | Loss: 0.00001955
Iteration 37/1000 | Loss: 0.00001955
Iteration 38/1000 | Loss: 0.00001955
Iteration 39/1000 | Loss: 0.00001954
Iteration 40/1000 | Loss: 0.00001953
Iteration 41/1000 | Loss: 0.00001953
Iteration 42/1000 | Loss: 0.00001952
Iteration 43/1000 | Loss: 0.00001952
Iteration 44/1000 | Loss: 0.00001951
Iteration 45/1000 | Loss: 0.00001951
Iteration 46/1000 | Loss: 0.00001951
Iteration 47/1000 | Loss: 0.00001950
Iteration 48/1000 | Loss: 0.00001950
Iteration 49/1000 | Loss: 0.00001949
Iteration 50/1000 | Loss: 0.00001949
Iteration 51/1000 | Loss: 0.00001949
Iteration 52/1000 | Loss: 0.00001948
Iteration 53/1000 | Loss: 0.00001948
Iteration 54/1000 | Loss: 0.00001948
Iteration 55/1000 | Loss: 0.00001947
Iteration 56/1000 | Loss: 0.00001947
Iteration 57/1000 | Loss: 0.00001947
Iteration 58/1000 | Loss: 0.00001947
Iteration 59/1000 | Loss: 0.00001947
Iteration 60/1000 | Loss: 0.00001947
Iteration 61/1000 | Loss: 0.00001947
Iteration 62/1000 | Loss: 0.00001947
Iteration 63/1000 | Loss: 0.00001947
Iteration 64/1000 | Loss: 0.00001947
Iteration 65/1000 | Loss: 0.00001947
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 65. Stopping optimization.
Last 5 losses: [1.9472337953629903e-05, 1.9472337953629903e-05, 1.9472337953629903e-05, 1.9472337953629903e-05, 1.9472337953629903e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9472337953629903e-05

Optimization complete. Final v2v error: 3.6903252601623535 mm

Highest mean error: 4.057466506958008 mm for frame 62

Lowest mean error: 3.2428019046783447 mm for frame 0

Saving results

Total time: 34.64059519767761
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_023/1058/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_023/1058.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_023/1058
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00925136
Iteration 2/25 | Loss: 0.00163175
Iteration 3/25 | Loss: 0.00143919
Iteration 4/25 | Loss: 0.00141507
Iteration 5/25 | Loss: 0.00140189
Iteration 6/25 | Loss: 0.00139166
Iteration 7/25 | Loss: 0.00138593
Iteration 8/25 | Loss: 0.00138736
Iteration 9/25 | Loss: 0.00138492
Iteration 10/25 | Loss: 0.00137598
Iteration 11/25 | Loss: 0.00137383
Iteration 12/25 | Loss: 0.00137307
Iteration 13/25 | Loss: 0.00137243
Iteration 14/25 | Loss: 0.00137284
Iteration 15/25 | Loss: 0.00137094
Iteration 16/25 | Loss: 0.00137039
Iteration 17/25 | Loss: 0.00137026
Iteration 18/25 | Loss: 0.00137025
Iteration 19/25 | Loss: 0.00137025
Iteration 20/25 | Loss: 0.00137024
Iteration 21/25 | Loss: 0.00137024
Iteration 22/25 | Loss: 0.00137024
Iteration 23/25 | Loss: 0.00137024
Iteration 24/25 | Loss: 0.00137024
Iteration 25/25 | Loss: 0.00137024

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35167432
Iteration 2/25 | Loss: 0.00195994
Iteration 3/25 | Loss: 0.00195994
Iteration 4/25 | Loss: 0.00195994
Iteration 5/25 | Loss: 0.00195993
Iteration 6/25 | Loss: 0.00195993
Iteration 7/25 | Loss: 0.00195993
Iteration 8/25 | Loss: 0.00195993
Iteration 9/25 | Loss: 0.00195993
Iteration 10/25 | Loss: 0.00195993
Iteration 11/25 | Loss: 0.00195993
Iteration 12/25 | Loss: 0.00195993
Iteration 13/25 | Loss: 0.00195993
Iteration 14/25 | Loss: 0.00195993
Iteration 15/25 | Loss: 0.00195993
Iteration 16/25 | Loss: 0.00195993
Iteration 17/25 | Loss: 0.00195993
Iteration 18/25 | Loss: 0.00195993
Iteration 19/25 | Loss: 0.00195993
Iteration 20/25 | Loss: 0.00195993
Iteration 21/25 | Loss: 0.00195993
Iteration 22/25 | Loss: 0.00195993
Iteration 23/25 | Loss: 0.00195993
Iteration 24/25 | Loss: 0.00195993
Iteration 25/25 | Loss: 0.00195993

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00195993
Iteration 2/1000 | Loss: 0.00020434
Iteration 3/1000 | Loss: 0.00071350
Iteration 4/1000 | Loss: 0.00048986
Iteration 5/1000 | Loss: 0.00012263
Iteration 6/1000 | Loss: 0.00013988
Iteration 7/1000 | Loss: 0.00009513
Iteration 8/1000 | Loss: 0.00008497
Iteration 9/1000 | Loss: 0.00032695
Iteration 10/1000 | Loss: 0.00008895
Iteration 11/1000 | Loss: 0.00014417
Iteration 12/1000 | Loss: 0.00008110
Iteration 13/1000 | Loss: 0.00053118
Iteration 14/1000 | Loss: 0.00060248
Iteration 15/1000 | Loss: 0.00008643
Iteration 16/1000 | Loss: 0.00049174
Iteration 17/1000 | Loss: 0.00052395
Iteration 18/1000 | Loss: 0.00031163
Iteration 19/1000 | Loss: 0.00008557
Iteration 20/1000 | Loss: 0.00069615
Iteration 21/1000 | Loss: 0.00026976
Iteration 22/1000 | Loss: 0.00025941
Iteration 23/1000 | Loss: 0.00009582
Iteration 24/1000 | Loss: 0.00007899
Iteration 25/1000 | Loss: 0.00050210
Iteration 26/1000 | Loss: 0.00022820
Iteration 27/1000 | Loss: 0.00014864
Iteration 28/1000 | Loss: 0.00006899
Iteration 29/1000 | Loss: 0.00013961
Iteration 30/1000 | Loss: 0.00006887
Iteration 31/1000 | Loss: 0.00006623
Iteration 32/1000 | Loss: 0.00006386
Iteration 33/1000 | Loss: 0.00020306
Iteration 34/1000 | Loss: 0.00065888
Iteration 35/1000 | Loss: 0.00046130
Iteration 36/1000 | Loss: 0.00050633
Iteration 37/1000 | Loss: 0.00018994
Iteration 38/1000 | Loss: 0.00009838
Iteration 39/1000 | Loss: 0.00006134
Iteration 40/1000 | Loss: 0.00005913
Iteration 41/1000 | Loss: 0.00005731
Iteration 42/1000 | Loss: 0.00007891
Iteration 43/1000 | Loss: 0.00053923
Iteration 44/1000 | Loss: 0.00065041
Iteration 45/1000 | Loss: 0.00173317
Iteration 46/1000 | Loss: 0.00148993
Iteration 47/1000 | Loss: 0.00077171
Iteration 48/1000 | Loss: 0.00011366
Iteration 49/1000 | Loss: 0.00008244
Iteration 50/1000 | Loss: 0.00006676
Iteration 51/1000 | Loss: 0.00005655
Iteration 52/1000 | Loss: 0.00034326
Iteration 53/1000 | Loss: 0.00006193
Iteration 54/1000 | Loss: 0.00004454
Iteration 55/1000 | Loss: 0.00004058
Iteration 56/1000 | Loss: 0.00003716
Iteration 57/1000 | Loss: 0.00003467
Iteration 58/1000 | Loss: 0.00003271
Iteration 59/1000 | Loss: 0.00003132
Iteration 60/1000 | Loss: 0.00003003
Iteration 61/1000 | Loss: 0.00002861
Iteration 62/1000 | Loss: 0.00002743
Iteration 63/1000 | Loss: 0.00002670
Iteration 64/1000 | Loss: 0.00002621
Iteration 65/1000 | Loss: 0.00002582
Iteration 66/1000 | Loss: 0.00002559
Iteration 67/1000 | Loss: 0.00002535
Iteration 68/1000 | Loss: 0.00002526
Iteration 69/1000 | Loss: 0.00002521
Iteration 70/1000 | Loss: 0.00002520
Iteration 71/1000 | Loss: 0.00002520
Iteration 72/1000 | Loss: 0.00002520
Iteration 73/1000 | Loss: 0.00002519
Iteration 74/1000 | Loss: 0.00002519
Iteration 75/1000 | Loss: 0.00002519
Iteration 76/1000 | Loss: 0.00002518
Iteration 77/1000 | Loss: 0.00002518
Iteration 78/1000 | Loss: 0.00002517
Iteration 79/1000 | Loss: 0.00002517
Iteration 80/1000 | Loss: 0.00002517
Iteration 81/1000 | Loss: 0.00002517
Iteration 82/1000 | Loss: 0.00002516
Iteration 83/1000 | Loss: 0.00002516
Iteration 84/1000 | Loss: 0.00002516
Iteration 85/1000 | Loss: 0.00002516
Iteration 86/1000 | Loss: 0.00002515
Iteration 87/1000 | Loss: 0.00002515
Iteration 88/1000 | Loss: 0.00002515
Iteration 89/1000 | Loss: 0.00002514
Iteration 90/1000 | Loss: 0.00002514
Iteration 91/1000 | Loss: 0.00002513
Iteration 92/1000 | Loss: 0.00002513
Iteration 93/1000 | Loss: 0.00002512
Iteration 94/1000 | Loss: 0.00002511
Iteration 95/1000 | Loss: 0.00002511
Iteration 96/1000 | Loss: 0.00002511
Iteration 97/1000 | Loss: 0.00002510
Iteration 98/1000 | Loss: 0.00002510
Iteration 99/1000 | Loss: 0.00002509
Iteration 100/1000 | Loss: 0.00002508
Iteration 101/1000 | Loss: 0.00002507
Iteration 102/1000 | Loss: 0.00002507
Iteration 103/1000 | Loss: 0.00002507
Iteration 104/1000 | Loss: 0.00002505
Iteration 105/1000 | Loss: 0.00002505
Iteration 106/1000 | Loss: 0.00002505
Iteration 107/1000 | Loss: 0.00002505
Iteration 108/1000 | Loss: 0.00002505
Iteration 109/1000 | Loss: 0.00002505
Iteration 110/1000 | Loss: 0.00002505
Iteration 111/1000 | Loss: 0.00002505
Iteration 112/1000 | Loss: 0.00002505
Iteration 113/1000 | Loss: 0.00002505
Iteration 114/1000 | Loss: 0.00002504
Iteration 115/1000 | Loss: 0.00002504
Iteration 116/1000 | Loss: 0.00002503
Iteration 117/1000 | Loss: 0.00002503
Iteration 118/1000 | Loss: 0.00002503
Iteration 119/1000 | Loss: 0.00002503
Iteration 120/1000 | Loss: 0.00002503
Iteration 121/1000 | Loss: 0.00002502
Iteration 122/1000 | Loss: 0.00002502
Iteration 123/1000 | Loss: 0.00002502
Iteration 124/1000 | Loss: 0.00002502
Iteration 125/1000 | Loss: 0.00002502
Iteration 126/1000 | Loss: 0.00002502
Iteration 127/1000 | Loss: 0.00002502
Iteration 128/1000 | Loss: 0.00002502
Iteration 129/1000 | Loss: 0.00002502
Iteration 130/1000 | Loss: 0.00002502
Iteration 131/1000 | Loss: 0.00002501
Iteration 132/1000 | Loss: 0.00002501
Iteration 133/1000 | Loss: 0.00002501
Iteration 134/1000 | Loss: 0.00002501
Iteration 135/1000 | Loss: 0.00002501
Iteration 136/1000 | Loss: 0.00002501
Iteration 137/1000 | Loss: 0.00002501
Iteration 138/1000 | Loss: 0.00002501
Iteration 139/1000 | Loss: 0.00002501
Iteration 140/1000 | Loss: 0.00002501
Iteration 141/1000 | Loss: 0.00002500
Iteration 142/1000 | Loss: 0.00002500
Iteration 143/1000 | Loss: 0.00002500
Iteration 144/1000 | Loss: 0.00002500
Iteration 145/1000 | Loss: 0.00002500
Iteration 146/1000 | Loss: 0.00002500
Iteration 147/1000 | Loss: 0.00002500
Iteration 148/1000 | Loss: 0.00002500
Iteration 149/1000 | Loss: 0.00002499
Iteration 150/1000 | Loss: 0.00002499
Iteration 151/1000 | Loss: 0.00002499
Iteration 152/1000 | Loss: 0.00002498
Iteration 153/1000 | Loss: 0.00002498
Iteration 154/1000 | Loss: 0.00002498
Iteration 155/1000 | Loss: 0.00002497
Iteration 156/1000 | Loss: 0.00002497
Iteration 157/1000 | Loss: 0.00002496
Iteration 158/1000 | Loss: 0.00002496
Iteration 159/1000 | Loss: 0.00002496
Iteration 160/1000 | Loss: 0.00002496
Iteration 161/1000 | Loss: 0.00002495
Iteration 162/1000 | Loss: 0.00002495
Iteration 163/1000 | Loss: 0.00002495
Iteration 164/1000 | Loss: 0.00002495
Iteration 165/1000 | Loss: 0.00002495
Iteration 166/1000 | Loss: 0.00002494
Iteration 167/1000 | Loss: 0.00002494
Iteration 168/1000 | Loss: 0.00002494
Iteration 169/1000 | Loss: 0.00002494
Iteration 170/1000 | Loss: 0.00002494
Iteration 171/1000 | Loss: 0.00002494
Iteration 172/1000 | Loss: 0.00002494
Iteration 173/1000 | Loss: 0.00002494
Iteration 174/1000 | Loss: 0.00002494
Iteration 175/1000 | Loss: 0.00002494
Iteration 176/1000 | Loss: 0.00002494
Iteration 177/1000 | Loss: 0.00002494
Iteration 178/1000 | Loss: 0.00002494
Iteration 179/1000 | Loss: 0.00002494
Iteration 180/1000 | Loss: 0.00002494
Iteration 181/1000 | Loss: 0.00002494
Iteration 182/1000 | Loss: 0.00002494
Iteration 183/1000 | Loss: 0.00002494
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 183. Stopping optimization.
Last 5 losses: [2.4937233320088126e-05, 2.4937233320088126e-05, 2.4937233320088126e-05, 2.4937233320088126e-05, 2.4937233320088126e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4937233320088126e-05

Optimization complete. Final v2v error: 4.1527419090271 mm

Highest mean error: 4.936083793640137 mm for frame 116

Lowest mean error: 3.3402860164642334 mm for frame 203

Saving results

Total time: 152.8733217716217
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_023/1044/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_023/1044.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_023/1044
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00408787
Iteration 2/25 | Loss: 0.00121956
Iteration 3/25 | Loss: 0.00113892
Iteration 4/25 | Loss: 0.00113094
Iteration 5/25 | Loss: 0.00112863
Iteration 6/25 | Loss: 0.00112836
Iteration 7/25 | Loss: 0.00112836
Iteration 8/25 | Loss: 0.00112836
Iteration 9/25 | Loss: 0.00112836
Iteration 10/25 | Loss: 0.00112836
Iteration 11/25 | Loss: 0.00112836
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011283592320978642, 0.0011283592320978642, 0.0011283592320978642, 0.0011283592320978642, 0.0011283592320978642]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011283592320978642

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.52198875
Iteration 2/25 | Loss: 0.00080088
Iteration 3/25 | Loss: 0.00080088
Iteration 4/25 | Loss: 0.00080088
Iteration 5/25 | Loss: 0.00080088
Iteration 6/25 | Loss: 0.00080088
Iteration 7/25 | Loss: 0.00080088
Iteration 8/25 | Loss: 0.00080087
Iteration 9/25 | Loss: 0.00080087
Iteration 10/25 | Loss: 0.00080087
Iteration 11/25 | Loss: 0.00080087
Iteration 12/25 | Loss: 0.00080087
Iteration 13/25 | Loss: 0.00080087
Iteration 14/25 | Loss: 0.00080087
Iteration 15/25 | Loss: 0.00080087
Iteration 16/25 | Loss: 0.00080087
Iteration 17/25 | Loss: 0.00080087
Iteration 18/25 | Loss: 0.00080087
Iteration 19/25 | Loss: 0.00080087
Iteration 20/25 | Loss: 0.00080087
Iteration 21/25 | Loss: 0.00080087
Iteration 22/25 | Loss: 0.00080087
Iteration 23/25 | Loss: 0.00080087
Iteration 24/25 | Loss: 0.00080087
Iteration 25/25 | Loss: 0.00080087

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00080087
Iteration 2/1000 | Loss: 0.00002394
Iteration 3/1000 | Loss: 0.00001642
Iteration 4/1000 | Loss: 0.00001445
Iteration 5/1000 | Loss: 0.00001342
Iteration 6/1000 | Loss: 0.00001265
Iteration 7/1000 | Loss: 0.00001219
Iteration 8/1000 | Loss: 0.00001181
Iteration 9/1000 | Loss: 0.00001162
Iteration 10/1000 | Loss: 0.00001161
Iteration 11/1000 | Loss: 0.00001139
Iteration 12/1000 | Loss: 0.00001120
Iteration 13/1000 | Loss: 0.00001118
Iteration 14/1000 | Loss: 0.00001114
Iteration 15/1000 | Loss: 0.00001113
Iteration 16/1000 | Loss: 0.00001112
Iteration 17/1000 | Loss: 0.00001112
Iteration 18/1000 | Loss: 0.00001112
Iteration 19/1000 | Loss: 0.00001111
Iteration 20/1000 | Loss: 0.00001110
Iteration 21/1000 | Loss: 0.00001109
Iteration 22/1000 | Loss: 0.00001109
Iteration 23/1000 | Loss: 0.00001108
Iteration 24/1000 | Loss: 0.00001107
Iteration 25/1000 | Loss: 0.00001106
Iteration 26/1000 | Loss: 0.00001105
Iteration 27/1000 | Loss: 0.00001104
Iteration 28/1000 | Loss: 0.00001100
Iteration 29/1000 | Loss: 0.00001100
Iteration 30/1000 | Loss: 0.00001098
Iteration 31/1000 | Loss: 0.00001098
Iteration 32/1000 | Loss: 0.00001098
Iteration 33/1000 | Loss: 0.00001097
Iteration 34/1000 | Loss: 0.00001097
Iteration 35/1000 | Loss: 0.00001097
Iteration 36/1000 | Loss: 0.00001096
Iteration 37/1000 | Loss: 0.00001095
Iteration 38/1000 | Loss: 0.00001095
Iteration 39/1000 | Loss: 0.00001095
Iteration 40/1000 | Loss: 0.00001095
Iteration 41/1000 | Loss: 0.00001095
Iteration 42/1000 | Loss: 0.00001095
Iteration 43/1000 | Loss: 0.00001094
Iteration 44/1000 | Loss: 0.00001094
Iteration 45/1000 | Loss: 0.00001094
Iteration 46/1000 | Loss: 0.00001094
Iteration 47/1000 | Loss: 0.00001093
Iteration 48/1000 | Loss: 0.00001092
Iteration 49/1000 | Loss: 0.00001092
Iteration 50/1000 | Loss: 0.00001092
Iteration 51/1000 | Loss: 0.00001091
Iteration 52/1000 | Loss: 0.00001090
Iteration 53/1000 | Loss: 0.00001090
Iteration 54/1000 | Loss: 0.00001090
Iteration 55/1000 | Loss: 0.00001090
Iteration 56/1000 | Loss: 0.00001090
Iteration 57/1000 | Loss: 0.00001090
Iteration 58/1000 | Loss: 0.00001090
Iteration 59/1000 | Loss: 0.00001089
Iteration 60/1000 | Loss: 0.00001089
Iteration 61/1000 | Loss: 0.00001089
Iteration 62/1000 | Loss: 0.00001089
Iteration 63/1000 | Loss: 0.00001088
Iteration 64/1000 | Loss: 0.00001088
Iteration 65/1000 | Loss: 0.00001088
Iteration 66/1000 | Loss: 0.00001088
Iteration 67/1000 | Loss: 0.00001087
Iteration 68/1000 | Loss: 0.00001087
Iteration 69/1000 | Loss: 0.00001087
Iteration 70/1000 | Loss: 0.00001087
Iteration 71/1000 | Loss: 0.00001086
Iteration 72/1000 | Loss: 0.00001086
Iteration 73/1000 | Loss: 0.00001086
Iteration 74/1000 | Loss: 0.00001086
Iteration 75/1000 | Loss: 0.00001086
Iteration 76/1000 | Loss: 0.00001085
Iteration 77/1000 | Loss: 0.00001085
Iteration 78/1000 | Loss: 0.00001085
Iteration 79/1000 | Loss: 0.00001085
Iteration 80/1000 | Loss: 0.00001084
Iteration 81/1000 | Loss: 0.00001084
Iteration 82/1000 | Loss: 0.00001084
Iteration 83/1000 | Loss: 0.00001083
Iteration 84/1000 | Loss: 0.00001083
Iteration 85/1000 | Loss: 0.00001082
Iteration 86/1000 | Loss: 0.00001082
Iteration 87/1000 | Loss: 0.00001082
Iteration 88/1000 | Loss: 0.00001082
Iteration 89/1000 | Loss: 0.00001082
Iteration 90/1000 | Loss: 0.00001081
Iteration 91/1000 | Loss: 0.00001081
Iteration 92/1000 | Loss: 0.00001081
Iteration 93/1000 | Loss: 0.00001081
Iteration 94/1000 | Loss: 0.00001081
Iteration 95/1000 | Loss: 0.00001081
Iteration 96/1000 | Loss: 0.00001081
Iteration 97/1000 | Loss: 0.00001081
Iteration 98/1000 | Loss: 0.00001081
Iteration 99/1000 | Loss: 0.00001080
Iteration 100/1000 | Loss: 0.00001080
Iteration 101/1000 | Loss: 0.00001080
Iteration 102/1000 | Loss: 0.00001080
Iteration 103/1000 | Loss: 0.00001080
Iteration 104/1000 | Loss: 0.00001079
Iteration 105/1000 | Loss: 0.00001079
Iteration 106/1000 | Loss: 0.00001079
Iteration 107/1000 | Loss: 0.00001079
Iteration 108/1000 | Loss: 0.00001079
Iteration 109/1000 | Loss: 0.00001078
Iteration 110/1000 | Loss: 0.00001078
Iteration 111/1000 | Loss: 0.00001078
Iteration 112/1000 | Loss: 0.00001078
Iteration 113/1000 | Loss: 0.00001077
Iteration 114/1000 | Loss: 0.00001077
Iteration 115/1000 | Loss: 0.00001077
Iteration 116/1000 | Loss: 0.00001076
Iteration 117/1000 | Loss: 0.00001076
Iteration 118/1000 | Loss: 0.00001075
Iteration 119/1000 | Loss: 0.00001075
Iteration 120/1000 | Loss: 0.00001075
Iteration 121/1000 | Loss: 0.00001074
Iteration 122/1000 | Loss: 0.00001074
Iteration 123/1000 | Loss: 0.00001074
Iteration 124/1000 | Loss: 0.00001074
Iteration 125/1000 | Loss: 0.00001074
Iteration 126/1000 | Loss: 0.00001073
Iteration 127/1000 | Loss: 0.00001073
Iteration 128/1000 | Loss: 0.00001073
Iteration 129/1000 | Loss: 0.00001072
Iteration 130/1000 | Loss: 0.00001072
Iteration 131/1000 | Loss: 0.00001072
Iteration 132/1000 | Loss: 0.00001072
Iteration 133/1000 | Loss: 0.00001072
Iteration 134/1000 | Loss: 0.00001071
Iteration 135/1000 | Loss: 0.00001071
Iteration 136/1000 | Loss: 0.00001071
Iteration 137/1000 | Loss: 0.00001070
Iteration 138/1000 | Loss: 0.00001070
Iteration 139/1000 | Loss: 0.00001070
Iteration 140/1000 | Loss: 0.00001069
Iteration 141/1000 | Loss: 0.00001069
Iteration 142/1000 | Loss: 0.00001069
Iteration 143/1000 | Loss: 0.00001068
Iteration 144/1000 | Loss: 0.00001068
Iteration 145/1000 | Loss: 0.00001068
Iteration 146/1000 | Loss: 0.00001068
Iteration 147/1000 | Loss: 0.00001068
Iteration 148/1000 | Loss: 0.00001067
Iteration 149/1000 | Loss: 0.00001067
Iteration 150/1000 | Loss: 0.00001067
Iteration 151/1000 | Loss: 0.00001067
Iteration 152/1000 | Loss: 0.00001067
Iteration 153/1000 | Loss: 0.00001067
Iteration 154/1000 | Loss: 0.00001067
Iteration 155/1000 | Loss: 0.00001067
Iteration 156/1000 | Loss: 0.00001066
Iteration 157/1000 | Loss: 0.00001066
Iteration 158/1000 | Loss: 0.00001066
Iteration 159/1000 | Loss: 0.00001066
Iteration 160/1000 | Loss: 0.00001066
Iteration 161/1000 | Loss: 0.00001066
Iteration 162/1000 | Loss: 0.00001066
Iteration 163/1000 | Loss: 0.00001066
Iteration 164/1000 | Loss: 0.00001066
Iteration 165/1000 | Loss: 0.00001066
Iteration 166/1000 | Loss: 0.00001066
Iteration 167/1000 | Loss: 0.00001066
Iteration 168/1000 | Loss: 0.00001066
Iteration 169/1000 | Loss: 0.00001066
Iteration 170/1000 | Loss: 0.00001065
Iteration 171/1000 | Loss: 0.00001065
Iteration 172/1000 | Loss: 0.00001065
Iteration 173/1000 | Loss: 0.00001065
Iteration 174/1000 | Loss: 0.00001065
Iteration 175/1000 | Loss: 0.00001065
Iteration 176/1000 | Loss: 0.00001064
Iteration 177/1000 | Loss: 0.00001064
Iteration 178/1000 | Loss: 0.00001064
Iteration 179/1000 | Loss: 0.00001064
Iteration 180/1000 | Loss: 0.00001064
Iteration 181/1000 | Loss: 0.00001064
Iteration 182/1000 | Loss: 0.00001064
Iteration 183/1000 | Loss: 0.00001063
Iteration 184/1000 | Loss: 0.00001063
Iteration 185/1000 | Loss: 0.00001063
Iteration 186/1000 | Loss: 0.00001063
Iteration 187/1000 | Loss: 0.00001063
Iteration 188/1000 | Loss: 0.00001063
Iteration 189/1000 | Loss: 0.00001063
Iteration 190/1000 | Loss: 0.00001063
Iteration 191/1000 | Loss: 0.00001063
Iteration 192/1000 | Loss: 0.00001063
Iteration 193/1000 | Loss: 0.00001063
Iteration 194/1000 | Loss: 0.00001063
Iteration 195/1000 | Loss: 0.00001063
Iteration 196/1000 | Loss: 0.00001062
Iteration 197/1000 | Loss: 0.00001062
Iteration 198/1000 | Loss: 0.00001062
Iteration 199/1000 | Loss: 0.00001062
Iteration 200/1000 | Loss: 0.00001062
Iteration 201/1000 | Loss: 0.00001062
Iteration 202/1000 | Loss: 0.00001062
Iteration 203/1000 | Loss: 0.00001062
Iteration 204/1000 | Loss: 0.00001061
Iteration 205/1000 | Loss: 0.00001061
Iteration 206/1000 | Loss: 0.00001061
Iteration 207/1000 | Loss: 0.00001061
Iteration 208/1000 | Loss: 0.00001061
Iteration 209/1000 | Loss: 0.00001061
Iteration 210/1000 | Loss: 0.00001061
Iteration 211/1000 | Loss: 0.00001060
Iteration 212/1000 | Loss: 0.00001060
Iteration 213/1000 | Loss: 0.00001060
Iteration 214/1000 | Loss: 0.00001060
Iteration 215/1000 | Loss: 0.00001060
Iteration 216/1000 | Loss: 0.00001060
Iteration 217/1000 | Loss: 0.00001060
Iteration 218/1000 | Loss: 0.00001060
Iteration 219/1000 | Loss: 0.00001060
Iteration 220/1000 | Loss: 0.00001060
Iteration 221/1000 | Loss: 0.00001060
Iteration 222/1000 | Loss: 0.00001060
Iteration 223/1000 | Loss: 0.00001059
Iteration 224/1000 | Loss: 0.00001059
Iteration 225/1000 | Loss: 0.00001059
Iteration 226/1000 | Loss: 0.00001059
Iteration 227/1000 | Loss: 0.00001059
Iteration 228/1000 | Loss: 0.00001059
Iteration 229/1000 | Loss: 0.00001059
Iteration 230/1000 | Loss: 0.00001059
Iteration 231/1000 | Loss: 0.00001059
Iteration 232/1000 | Loss: 0.00001059
Iteration 233/1000 | Loss: 0.00001059
Iteration 234/1000 | Loss: 0.00001059
Iteration 235/1000 | Loss: 0.00001059
Iteration 236/1000 | Loss: 0.00001058
Iteration 237/1000 | Loss: 0.00001058
Iteration 238/1000 | Loss: 0.00001058
Iteration 239/1000 | Loss: 0.00001058
Iteration 240/1000 | Loss: 0.00001058
Iteration 241/1000 | Loss: 0.00001058
Iteration 242/1000 | Loss: 0.00001058
Iteration 243/1000 | Loss: 0.00001058
Iteration 244/1000 | Loss: 0.00001058
Iteration 245/1000 | Loss: 0.00001058
Iteration 246/1000 | Loss: 0.00001058
Iteration 247/1000 | Loss: 0.00001058
Iteration 248/1000 | Loss: 0.00001058
Iteration 249/1000 | Loss: 0.00001057
Iteration 250/1000 | Loss: 0.00001057
Iteration 251/1000 | Loss: 0.00001057
Iteration 252/1000 | Loss: 0.00001057
Iteration 253/1000 | Loss: 0.00001057
Iteration 254/1000 | Loss: 0.00001057
Iteration 255/1000 | Loss: 0.00001056
Iteration 256/1000 | Loss: 0.00001056
Iteration 257/1000 | Loss: 0.00001056
Iteration 258/1000 | Loss: 0.00001056
Iteration 259/1000 | Loss: 0.00001056
Iteration 260/1000 | Loss: 0.00001056
Iteration 261/1000 | Loss: 0.00001056
Iteration 262/1000 | Loss: 0.00001056
Iteration 263/1000 | Loss: 0.00001056
Iteration 264/1000 | Loss: 0.00001056
Iteration 265/1000 | Loss: 0.00001055
Iteration 266/1000 | Loss: 0.00001055
Iteration 267/1000 | Loss: 0.00001055
Iteration 268/1000 | Loss: 0.00001055
Iteration 269/1000 | Loss: 0.00001055
Iteration 270/1000 | Loss: 0.00001055
Iteration 271/1000 | Loss: 0.00001055
Iteration 272/1000 | Loss: 0.00001055
Iteration 273/1000 | Loss: 0.00001054
Iteration 274/1000 | Loss: 0.00001054
Iteration 275/1000 | Loss: 0.00001054
Iteration 276/1000 | Loss: 0.00001054
Iteration 277/1000 | Loss: 0.00001054
Iteration 278/1000 | Loss: 0.00001054
Iteration 279/1000 | Loss: 0.00001054
Iteration 280/1000 | Loss: 0.00001054
Iteration 281/1000 | Loss: 0.00001054
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 281. Stopping optimization.
Last 5 losses: [1.0541692972765304e-05, 1.0541692972765304e-05, 1.0541692972765304e-05, 1.0541692972765304e-05, 1.0541692972765304e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0541692972765304e-05

Optimization complete. Final v2v error: 2.7236340045928955 mm

Highest mean error: 4.08135986328125 mm for frame 167

Lowest mean error: 2.4675991535186768 mm for frame 140

Saving results

Total time: 49.993438959121704
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_023/1077/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_023/1077.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_023/1077
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00404815
Iteration 2/25 | Loss: 0.00136040
Iteration 3/25 | Loss: 0.00115953
Iteration 4/25 | Loss: 0.00114390
Iteration 5/25 | Loss: 0.00114181
Iteration 6/25 | Loss: 0.00114121
Iteration 7/25 | Loss: 0.00114121
Iteration 8/25 | Loss: 0.00114121
Iteration 9/25 | Loss: 0.00114121
Iteration 10/25 | Loss: 0.00114121
Iteration 11/25 | Loss: 0.00114121
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011412141611799598, 0.0011412141611799598, 0.0011412141611799598, 0.0011412141611799598, 0.0011412141611799598]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011412141611799598

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35302639
Iteration 2/25 | Loss: 0.00064963
Iteration 3/25 | Loss: 0.00064963
Iteration 4/25 | Loss: 0.00064962
Iteration 5/25 | Loss: 0.00064962
Iteration 6/25 | Loss: 0.00064962
Iteration 7/25 | Loss: 0.00064962
Iteration 8/25 | Loss: 0.00064962
Iteration 9/25 | Loss: 0.00064962
Iteration 10/25 | Loss: 0.00064962
Iteration 11/25 | Loss: 0.00064962
Iteration 12/25 | Loss: 0.00064962
Iteration 13/25 | Loss: 0.00064962
Iteration 14/25 | Loss: 0.00064962
Iteration 15/25 | Loss: 0.00064962
Iteration 16/25 | Loss: 0.00064962
Iteration 17/25 | Loss: 0.00064962
Iteration 18/25 | Loss: 0.00064962
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0006496220594272017, 0.0006496220594272017, 0.0006496220594272017, 0.0006496220594272017, 0.0006496220594272017]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006496220594272017

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00064962
Iteration 2/1000 | Loss: 0.00002592
Iteration 3/1000 | Loss: 0.00001793
Iteration 4/1000 | Loss: 0.00001576
Iteration 5/1000 | Loss: 0.00001489
Iteration 6/1000 | Loss: 0.00001426
Iteration 7/1000 | Loss: 0.00001363
Iteration 8/1000 | Loss: 0.00001330
Iteration 9/1000 | Loss: 0.00001310
Iteration 10/1000 | Loss: 0.00001289
Iteration 11/1000 | Loss: 0.00001283
Iteration 12/1000 | Loss: 0.00001277
Iteration 13/1000 | Loss: 0.00001273
Iteration 14/1000 | Loss: 0.00001270
Iteration 15/1000 | Loss: 0.00001262
Iteration 16/1000 | Loss: 0.00001260
Iteration 17/1000 | Loss: 0.00001260
Iteration 18/1000 | Loss: 0.00001256
Iteration 19/1000 | Loss: 0.00001256
Iteration 20/1000 | Loss: 0.00001252
Iteration 21/1000 | Loss: 0.00001245
Iteration 22/1000 | Loss: 0.00001243
Iteration 23/1000 | Loss: 0.00001241
Iteration 24/1000 | Loss: 0.00001239
Iteration 25/1000 | Loss: 0.00001239
Iteration 26/1000 | Loss: 0.00001238
Iteration 27/1000 | Loss: 0.00001238
Iteration 28/1000 | Loss: 0.00001238
Iteration 29/1000 | Loss: 0.00001238
Iteration 30/1000 | Loss: 0.00001237
Iteration 31/1000 | Loss: 0.00001236
Iteration 32/1000 | Loss: 0.00001235
Iteration 33/1000 | Loss: 0.00001235
Iteration 34/1000 | Loss: 0.00001234
Iteration 35/1000 | Loss: 0.00001234
Iteration 36/1000 | Loss: 0.00001233
Iteration 37/1000 | Loss: 0.00001233
Iteration 38/1000 | Loss: 0.00001232
Iteration 39/1000 | Loss: 0.00001232
Iteration 40/1000 | Loss: 0.00001231
Iteration 41/1000 | Loss: 0.00001231
Iteration 42/1000 | Loss: 0.00001231
Iteration 43/1000 | Loss: 0.00001231
Iteration 44/1000 | Loss: 0.00001230
Iteration 45/1000 | Loss: 0.00001230
Iteration 46/1000 | Loss: 0.00001230
Iteration 47/1000 | Loss: 0.00001228
Iteration 48/1000 | Loss: 0.00001227
Iteration 49/1000 | Loss: 0.00001227
Iteration 50/1000 | Loss: 0.00001226
Iteration 51/1000 | Loss: 0.00001226
Iteration 52/1000 | Loss: 0.00001225
Iteration 53/1000 | Loss: 0.00001225
Iteration 54/1000 | Loss: 0.00001221
Iteration 55/1000 | Loss: 0.00001221
Iteration 56/1000 | Loss: 0.00001221
Iteration 57/1000 | Loss: 0.00001221
Iteration 58/1000 | Loss: 0.00001221
Iteration 59/1000 | Loss: 0.00001221
Iteration 60/1000 | Loss: 0.00001221
Iteration 61/1000 | Loss: 0.00001221
Iteration 62/1000 | Loss: 0.00001220
Iteration 63/1000 | Loss: 0.00001220
Iteration 64/1000 | Loss: 0.00001220
Iteration 65/1000 | Loss: 0.00001220
Iteration 66/1000 | Loss: 0.00001220
Iteration 67/1000 | Loss: 0.00001218
Iteration 68/1000 | Loss: 0.00001217
Iteration 69/1000 | Loss: 0.00001216
Iteration 70/1000 | Loss: 0.00001216
Iteration 71/1000 | Loss: 0.00001215
Iteration 72/1000 | Loss: 0.00001215
Iteration 73/1000 | Loss: 0.00001214
Iteration 74/1000 | Loss: 0.00001213
Iteration 75/1000 | Loss: 0.00001213
Iteration 76/1000 | Loss: 0.00001213
Iteration 77/1000 | Loss: 0.00001212
Iteration 78/1000 | Loss: 0.00001212
Iteration 79/1000 | Loss: 0.00001212
Iteration 80/1000 | Loss: 0.00001211
Iteration 81/1000 | Loss: 0.00001211
Iteration 82/1000 | Loss: 0.00001210
Iteration 83/1000 | Loss: 0.00001210
Iteration 84/1000 | Loss: 0.00001210
Iteration 85/1000 | Loss: 0.00001209
Iteration 86/1000 | Loss: 0.00001209
Iteration 87/1000 | Loss: 0.00001208
Iteration 88/1000 | Loss: 0.00001207
Iteration 89/1000 | Loss: 0.00001207
Iteration 90/1000 | Loss: 0.00001206
Iteration 91/1000 | Loss: 0.00001206
Iteration 92/1000 | Loss: 0.00001206
Iteration 93/1000 | Loss: 0.00001206
Iteration 94/1000 | Loss: 0.00001206
Iteration 95/1000 | Loss: 0.00001205
Iteration 96/1000 | Loss: 0.00001205
Iteration 97/1000 | Loss: 0.00001205
Iteration 98/1000 | Loss: 0.00001205
Iteration 99/1000 | Loss: 0.00001204
Iteration 100/1000 | Loss: 0.00001204
Iteration 101/1000 | Loss: 0.00001204
Iteration 102/1000 | Loss: 0.00001204
Iteration 103/1000 | Loss: 0.00001204
Iteration 104/1000 | Loss: 0.00001204
Iteration 105/1000 | Loss: 0.00001204
Iteration 106/1000 | Loss: 0.00001204
Iteration 107/1000 | Loss: 0.00001203
Iteration 108/1000 | Loss: 0.00001203
Iteration 109/1000 | Loss: 0.00001203
Iteration 110/1000 | Loss: 0.00001203
Iteration 111/1000 | Loss: 0.00001203
Iteration 112/1000 | Loss: 0.00001203
Iteration 113/1000 | Loss: 0.00001203
Iteration 114/1000 | Loss: 0.00001203
Iteration 115/1000 | Loss: 0.00001202
Iteration 116/1000 | Loss: 0.00001202
Iteration 117/1000 | Loss: 0.00001202
Iteration 118/1000 | Loss: 0.00001202
Iteration 119/1000 | Loss: 0.00001202
Iteration 120/1000 | Loss: 0.00001202
Iteration 121/1000 | Loss: 0.00001201
Iteration 122/1000 | Loss: 0.00001201
Iteration 123/1000 | Loss: 0.00001201
Iteration 124/1000 | Loss: 0.00001201
Iteration 125/1000 | Loss: 0.00001201
Iteration 126/1000 | Loss: 0.00001201
Iteration 127/1000 | Loss: 0.00001201
Iteration 128/1000 | Loss: 0.00001201
Iteration 129/1000 | Loss: 0.00001201
Iteration 130/1000 | Loss: 0.00001201
Iteration 131/1000 | Loss: 0.00001201
Iteration 132/1000 | Loss: 0.00001201
Iteration 133/1000 | Loss: 0.00001200
Iteration 134/1000 | Loss: 0.00001200
Iteration 135/1000 | Loss: 0.00001200
Iteration 136/1000 | Loss: 0.00001200
Iteration 137/1000 | Loss: 0.00001200
Iteration 138/1000 | Loss: 0.00001200
Iteration 139/1000 | Loss: 0.00001200
Iteration 140/1000 | Loss: 0.00001200
Iteration 141/1000 | Loss: 0.00001200
Iteration 142/1000 | Loss: 0.00001200
Iteration 143/1000 | Loss: 0.00001200
Iteration 144/1000 | Loss: 0.00001200
Iteration 145/1000 | Loss: 0.00001200
Iteration 146/1000 | Loss: 0.00001200
Iteration 147/1000 | Loss: 0.00001200
Iteration 148/1000 | Loss: 0.00001199
Iteration 149/1000 | Loss: 0.00001199
Iteration 150/1000 | Loss: 0.00001199
Iteration 151/1000 | Loss: 0.00001199
Iteration 152/1000 | Loss: 0.00001199
Iteration 153/1000 | Loss: 0.00001199
Iteration 154/1000 | Loss: 0.00001199
Iteration 155/1000 | Loss: 0.00001199
Iteration 156/1000 | Loss: 0.00001199
Iteration 157/1000 | Loss: 0.00001198
Iteration 158/1000 | Loss: 0.00001198
Iteration 159/1000 | Loss: 0.00001198
Iteration 160/1000 | Loss: 0.00001198
Iteration 161/1000 | Loss: 0.00001198
Iteration 162/1000 | Loss: 0.00001198
Iteration 163/1000 | Loss: 0.00001198
Iteration 164/1000 | Loss: 0.00001198
Iteration 165/1000 | Loss: 0.00001198
Iteration 166/1000 | Loss: 0.00001198
Iteration 167/1000 | Loss: 0.00001198
Iteration 168/1000 | Loss: 0.00001198
Iteration 169/1000 | Loss: 0.00001198
Iteration 170/1000 | Loss: 0.00001198
Iteration 171/1000 | Loss: 0.00001198
Iteration 172/1000 | Loss: 0.00001198
Iteration 173/1000 | Loss: 0.00001198
Iteration 174/1000 | Loss: 0.00001198
Iteration 175/1000 | Loss: 0.00001197
Iteration 176/1000 | Loss: 0.00001197
Iteration 177/1000 | Loss: 0.00001197
Iteration 178/1000 | Loss: 0.00001197
Iteration 179/1000 | Loss: 0.00001197
Iteration 180/1000 | Loss: 0.00001197
Iteration 181/1000 | Loss: 0.00001197
Iteration 182/1000 | Loss: 0.00001197
Iteration 183/1000 | Loss: 0.00001197
Iteration 184/1000 | Loss: 0.00001197
Iteration 185/1000 | Loss: 0.00001197
Iteration 186/1000 | Loss: 0.00001197
Iteration 187/1000 | Loss: 0.00001197
Iteration 188/1000 | Loss: 0.00001197
Iteration 189/1000 | Loss: 0.00001197
Iteration 190/1000 | Loss: 0.00001197
Iteration 191/1000 | Loss: 0.00001197
Iteration 192/1000 | Loss: 0.00001196
Iteration 193/1000 | Loss: 0.00001196
Iteration 194/1000 | Loss: 0.00001196
Iteration 195/1000 | Loss: 0.00001196
Iteration 196/1000 | Loss: 0.00001196
Iteration 197/1000 | Loss: 0.00001196
Iteration 198/1000 | Loss: 0.00001196
Iteration 199/1000 | Loss: 0.00001196
Iteration 200/1000 | Loss: 0.00001196
Iteration 201/1000 | Loss: 0.00001196
Iteration 202/1000 | Loss: 0.00001196
Iteration 203/1000 | Loss: 0.00001196
Iteration 204/1000 | Loss: 0.00001196
Iteration 205/1000 | Loss: 0.00001196
Iteration 206/1000 | Loss: 0.00001196
Iteration 207/1000 | Loss: 0.00001196
Iteration 208/1000 | Loss: 0.00001195
Iteration 209/1000 | Loss: 0.00001195
Iteration 210/1000 | Loss: 0.00001195
Iteration 211/1000 | Loss: 0.00001195
Iteration 212/1000 | Loss: 0.00001195
Iteration 213/1000 | Loss: 0.00001195
Iteration 214/1000 | Loss: 0.00001195
Iteration 215/1000 | Loss: 0.00001194
Iteration 216/1000 | Loss: 0.00001194
Iteration 217/1000 | Loss: 0.00001194
Iteration 218/1000 | Loss: 0.00001194
Iteration 219/1000 | Loss: 0.00001194
Iteration 220/1000 | Loss: 0.00001193
Iteration 221/1000 | Loss: 0.00001193
Iteration 222/1000 | Loss: 0.00001193
Iteration 223/1000 | Loss: 0.00001192
Iteration 224/1000 | Loss: 0.00001192
Iteration 225/1000 | Loss: 0.00001192
Iteration 226/1000 | Loss: 0.00001192
Iteration 227/1000 | Loss: 0.00001192
Iteration 228/1000 | Loss: 0.00001192
Iteration 229/1000 | Loss: 0.00001192
Iteration 230/1000 | Loss: 0.00001191
Iteration 231/1000 | Loss: 0.00001191
Iteration 232/1000 | Loss: 0.00001191
Iteration 233/1000 | Loss: 0.00001191
Iteration 234/1000 | Loss: 0.00001191
Iteration 235/1000 | Loss: 0.00001191
Iteration 236/1000 | Loss: 0.00001191
Iteration 237/1000 | Loss: 0.00001191
Iteration 238/1000 | Loss: 0.00001191
Iteration 239/1000 | Loss: 0.00001190
Iteration 240/1000 | Loss: 0.00001190
Iteration 241/1000 | Loss: 0.00001190
Iteration 242/1000 | Loss: 0.00001190
Iteration 243/1000 | Loss: 0.00001190
Iteration 244/1000 | Loss: 0.00001190
Iteration 245/1000 | Loss: 0.00001190
Iteration 246/1000 | Loss: 0.00001189
Iteration 247/1000 | Loss: 0.00001189
Iteration 248/1000 | Loss: 0.00001189
Iteration 249/1000 | Loss: 0.00001188
Iteration 250/1000 | Loss: 0.00001188
Iteration 251/1000 | Loss: 0.00001188
Iteration 252/1000 | Loss: 0.00001188
Iteration 253/1000 | Loss: 0.00001188
Iteration 254/1000 | Loss: 0.00001188
Iteration 255/1000 | Loss: 0.00001188
Iteration 256/1000 | Loss: 0.00001188
Iteration 257/1000 | Loss: 0.00001188
Iteration 258/1000 | Loss: 0.00001188
Iteration 259/1000 | Loss: 0.00001188
Iteration 260/1000 | Loss: 0.00001188
Iteration 261/1000 | Loss: 0.00001187
Iteration 262/1000 | Loss: 0.00001187
Iteration 263/1000 | Loss: 0.00001187
Iteration 264/1000 | Loss: 0.00001187
Iteration 265/1000 | Loss: 0.00001187
Iteration 266/1000 | Loss: 0.00001187
Iteration 267/1000 | Loss: 0.00001187
Iteration 268/1000 | Loss: 0.00001187
Iteration 269/1000 | Loss: 0.00001187
Iteration 270/1000 | Loss: 0.00001187
Iteration 271/1000 | Loss: 0.00001186
Iteration 272/1000 | Loss: 0.00001186
Iteration 273/1000 | Loss: 0.00001186
Iteration 274/1000 | Loss: 0.00001186
Iteration 275/1000 | Loss: 0.00001186
Iteration 276/1000 | Loss: 0.00001186
Iteration 277/1000 | Loss: 0.00001186
Iteration 278/1000 | Loss: 0.00001186
Iteration 279/1000 | Loss: 0.00001186
Iteration 280/1000 | Loss: 0.00001186
Iteration 281/1000 | Loss: 0.00001186
Iteration 282/1000 | Loss: 0.00001186
Iteration 283/1000 | Loss: 0.00001186
Iteration 284/1000 | Loss: 0.00001186
Iteration 285/1000 | Loss: 0.00001186
Iteration 286/1000 | Loss: 0.00001186
Iteration 287/1000 | Loss: 0.00001186
Iteration 288/1000 | Loss: 0.00001186
Iteration 289/1000 | Loss: 0.00001185
Iteration 290/1000 | Loss: 0.00001185
Iteration 291/1000 | Loss: 0.00001185
Iteration 292/1000 | Loss: 0.00001185
Iteration 293/1000 | Loss: 0.00001185
Iteration 294/1000 | Loss: 0.00001185
Iteration 295/1000 | Loss: 0.00001185
Iteration 296/1000 | Loss: 0.00001185
Iteration 297/1000 | Loss: 0.00001185
Iteration 298/1000 | Loss: 0.00001185
Iteration 299/1000 | Loss: 0.00001185
Iteration 300/1000 | Loss: 0.00001185
Iteration 301/1000 | Loss: 0.00001184
Iteration 302/1000 | Loss: 0.00001184
Iteration 303/1000 | Loss: 0.00001184
Iteration 304/1000 | Loss: 0.00001184
Iteration 305/1000 | Loss: 0.00001184
Iteration 306/1000 | Loss: 0.00001184
Iteration 307/1000 | Loss: 0.00001184
Iteration 308/1000 | Loss: 0.00001184
Iteration 309/1000 | Loss: 0.00001184
Iteration 310/1000 | Loss: 0.00001184
Iteration 311/1000 | Loss: 0.00001184
Iteration 312/1000 | Loss: 0.00001184
Iteration 313/1000 | Loss: 0.00001184
Iteration 314/1000 | Loss: 0.00001184
Iteration 315/1000 | Loss: 0.00001184
Iteration 316/1000 | Loss: 0.00001184
Iteration 317/1000 | Loss: 0.00001184
Iteration 318/1000 | Loss: 0.00001184
Iteration 319/1000 | Loss: 0.00001184
Iteration 320/1000 | Loss: 0.00001184
Iteration 321/1000 | Loss: 0.00001184
Iteration 322/1000 | Loss: 0.00001184
Iteration 323/1000 | Loss: 0.00001184
Iteration 324/1000 | Loss: 0.00001184
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 324. Stopping optimization.
Last 5 losses: [1.1835659279313404e-05, 1.1835659279313404e-05, 1.1835659279313404e-05, 1.1835659279313404e-05, 1.1835659279313404e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1835659279313404e-05

Optimization complete. Final v2v error: 2.9509894847869873 mm

Highest mean error: 3.063037872314453 mm for frame 85

Lowest mean error: 2.8475942611694336 mm for frame 121

Saving results

Total time: 47.53605270385742
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_023/1047/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_023/1047.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_023/1047
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00589647
Iteration 2/25 | Loss: 0.00142804
Iteration 3/25 | Loss: 0.00130968
Iteration 4/25 | Loss: 0.00116805
Iteration 5/25 | Loss: 0.00116403
Iteration 6/25 | Loss: 0.00116000
Iteration 7/25 | Loss: 0.00115027
Iteration 8/25 | Loss: 0.00114864
Iteration 9/25 | Loss: 0.00114731
Iteration 10/25 | Loss: 0.00114651
Iteration 11/25 | Loss: 0.00114609
Iteration 12/25 | Loss: 0.00114807
Iteration 13/25 | Loss: 0.00114428
Iteration 14/25 | Loss: 0.00114333
Iteration 15/25 | Loss: 0.00114302
Iteration 16/25 | Loss: 0.00114299
Iteration 17/25 | Loss: 0.00114299
Iteration 18/25 | Loss: 0.00114299
Iteration 19/25 | Loss: 0.00114298
Iteration 20/25 | Loss: 0.00114298
Iteration 21/25 | Loss: 0.00114298
Iteration 22/25 | Loss: 0.00114298
Iteration 23/25 | Loss: 0.00114298
Iteration 24/25 | Loss: 0.00114298
Iteration 25/25 | Loss: 0.00114298

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.20109391
Iteration 2/25 | Loss: 0.00093363
Iteration 3/25 | Loss: 0.00083485
Iteration 4/25 | Loss: 0.00083484
Iteration 5/25 | Loss: 0.00083484
Iteration 6/25 | Loss: 0.00083484
Iteration 7/25 | Loss: 0.00083484
Iteration 8/25 | Loss: 0.00083484
Iteration 9/25 | Loss: 0.00083484
Iteration 10/25 | Loss: 0.00083484
Iteration 11/25 | Loss: 0.00083484
Iteration 12/25 | Loss: 0.00083484
Iteration 13/25 | Loss: 0.00083484
Iteration 14/25 | Loss: 0.00083484
Iteration 15/25 | Loss: 0.00083484
Iteration 16/25 | Loss: 0.00083484
Iteration 17/25 | Loss: 0.00083484
Iteration 18/25 | Loss: 0.00083484
Iteration 19/25 | Loss: 0.00083484
Iteration 20/25 | Loss: 0.00083484
Iteration 21/25 | Loss: 0.00083484
Iteration 22/25 | Loss: 0.00083484
Iteration 23/25 | Loss: 0.00083484
Iteration 24/25 | Loss: 0.00083484
Iteration 25/25 | Loss: 0.00083484

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00083484
Iteration 2/1000 | Loss: 0.00002380
Iteration 3/1000 | Loss: 0.00001623
Iteration 4/1000 | Loss: 0.00001411
Iteration 5/1000 | Loss: 0.00001339
Iteration 6/1000 | Loss: 0.00001287
Iteration 7/1000 | Loss: 0.00001248
Iteration 8/1000 | Loss: 0.00001231
Iteration 9/1000 | Loss: 0.00001221
Iteration 10/1000 | Loss: 0.00001200
Iteration 11/1000 | Loss: 0.00001182
Iteration 12/1000 | Loss: 0.00001177
Iteration 13/1000 | Loss: 0.00001174
Iteration 14/1000 | Loss: 0.00001173
Iteration 15/1000 | Loss: 0.00001169
Iteration 16/1000 | Loss: 0.00001168
Iteration 17/1000 | Loss: 0.00001167
Iteration 18/1000 | Loss: 0.00001167
Iteration 19/1000 | Loss: 0.00001167
Iteration 20/1000 | Loss: 0.00001167
Iteration 21/1000 | Loss: 0.00001166
Iteration 22/1000 | Loss: 0.00001166
Iteration 23/1000 | Loss: 0.00001166
Iteration 24/1000 | Loss: 0.00001165
Iteration 25/1000 | Loss: 0.00001165
Iteration 26/1000 | Loss: 0.00001164
Iteration 27/1000 | Loss: 0.00001164
Iteration 28/1000 | Loss: 0.00001164
Iteration 29/1000 | Loss: 0.00001163
Iteration 30/1000 | Loss: 0.00001163
Iteration 31/1000 | Loss: 0.00001163
Iteration 32/1000 | Loss: 0.00001163
Iteration 33/1000 | Loss: 0.00001163
Iteration 34/1000 | Loss: 0.00001163
Iteration 35/1000 | Loss: 0.00001163
Iteration 36/1000 | Loss: 0.00001163
Iteration 37/1000 | Loss: 0.00001163
Iteration 38/1000 | Loss: 0.00001163
Iteration 39/1000 | Loss: 0.00001163
Iteration 40/1000 | Loss: 0.00001163
Iteration 41/1000 | Loss: 0.00001162
Iteration 42/1000 | Loss: 0.00001162
Iteration 43/1000 | Loss: 0.00001162
Iteration 44/1000 | Loss: 0.00001159
Iteration 45/1000 | Loss: 0.00001159
Iteration 46/1000 | Loss: 0.00001159
Iteration 47/1000 | Loss: 0.00001158
Iteration 48/1000 | Loss: 0.00001158
Iteration 49/1000 | Loss: 0.00001158
Iteration 50/1000 | Loss: 0.00001158
Iteration 51/1000 | Loss: 0.00001158
Iteration 52/1000 | Loss: 0.00001158
Iteration 53/1000 | Loss: 0.00001158
Iteration 54/1000 | Loss: 0.00001158
Iteration 55/1000 | Loss: 0.00001158
Iteration 56/1000 | Loss: 0.00001158
Iteration 57/1000 | Loss: 0.00001158
Iteration 58/1000 | Loss: 0.00001157
Iteration 59/1000 | Loss: 0.00001157
Iteration 60/1000 | Loss: 0.00001157
Iteration 61/1000 | Loss: 0.00001157
Iteration 62/1000 | Loss: 0.00001157
Iteration 63/1000 | Loss: 0.00001157
Iteration 64/1000 | Loss: 0.00001157
Iteration 65/1000 | Loss: 0.00001157
Iteration 66/1000 | Loss: 0.00001157
Iteration 67/1000 | Loss: 0.00001157
Iteration 68/1000 | Loss: 0.00001157
Iteration 69/1000 | Loss: 0.00001156
Iteration 70/1000 | Loss: 0.00001156
Iteration 71/1000 | Loss: 0.00001155
Iteration 72/1000 | Loss: 0.00001155
Iteration 73/1000 | Loss: 0.00001154
Iteration 74/1000 | Loss: 0.00001154
Iteration 75/1000 | Loss: 0.00001154
Iteration 76/1000 | Loss: 0.00001154
Iteration 77/1000 | Loss: 0.00001153
Iteration 78/1000 | Loss: 0.00001153
Iteration 79/1000 | Loss: 0.00001153
Iteration 80/1000 | Loss: 0.00001152
Iteration 81/1000 | Loss: 0.00001152
Iteration 82/1000 | Loss: 0.00001152
Iteration 83/1000 | Loss: 0.00001152
Iteration 84/1000 | Loss: 0.00001152
Iteration 85/1000 | Loss: 0.00001152
Iteration 86/1000 | Loss: 0.00001152
Iteration 87/1000 | Loss: 0.00001151
Iteration 88/1000 | Loss: 0.00001151
Iteration 89/1000 | Loss: 0.00001150
Iteration 90/1000 | Loss: 0.00001150
Iteration 91/1000 | Loss: 0.00001149
Iteration 92/1000 | Loss: 0.00001149
Iteration 93/1000 | Loss: 0.00001149
Iteration 94/1000 | Loss: 0.00001149
Iteration 95/1000 | Loss: 0.00001149
Iteration 96/1000 | Loss: 0.00001148
Iteration 97/1000 | Loss: 0.00001148
Iteration 98/1000 | Loss: 0.00001148
Iteration 99/1000 | Loss: 0.00001148
Iteration 100/1000 | Loss: 0.00001147
Iteration 101/1000 | Loss: 0.00001147
Iteration 102/1000 | Loss: 0.00001147
Iteration 103/1000 | Loss: 0.00001147
Iteration 104/1000 | Loss: 0.00001147
Iteration 105/1000 | Loss: 0.00001146
Iteration 106/1000 | Loss: 0.00001146
Iteration 107/1000 | Loss: 0.00001146
Iteration 108/1000 | Loss: 0.00001145
Iteration 109/1000 | Loss: 0.00001145
Iteration 110/1000 | Loss: 0.00001145
Iteration 111/1000 | Loss: 0.00001145
Iteration 112/1000 | Loss: 0.00001145
Iteration 113/1000 | Loss: 0.00001144
Iteration 114/1000 | Loss: 0.00001144
Iteration 115/1000 | Loss: 0.00001144
Iteration 116/1000 | Loss: 0.00001143
Iteration 117/1000 | Loss: 0.00001143
Iteration 118/1000 | Loss: 0.00001143
Iteration 119/1000 | Loss: 0.00001142
Iteration 120/1000 | Loss: 0.00001142
Iteration 121/1000 | Loss: 0.00001142
Iteration 122/1000 | Loss: 0.00001142
Iteration 123/1000 | Loss: 0.00001142
Iteration 124/1000 | Loss: 0.00001142
Iteration 125/1000 | Loss: 0.00001142
Iteration 126/1000 | Loss: 0.00001142
Iteration 127/1000 | Loss: 0.00001142
Iteration 128/1000 | Loss: 0.00001142
Iteration 129/1000 | Loss: 0.00001142
Iteration 130/1000 | Loss: 0.00001142
Iteration 131/1000 | Loss: 0.00001142
Iteration 132/1000 | Loss: 0.00001142
Iteration 133/1000 | Loss: 0.00001142
Iteration 134/1000 | Loss: 0.00001142
Iteration 135/1000 | Loss: 0.00001142
Iteration 136/1000 | Loss: 0.00001142
Iteration 137/1000 | Loss: 0.00001142
Iteration 138/1000 | Loss: 0.00001142
Iteration 139/1000 | Loss: 0.00001142
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 139. Stopping optimization.
Last 5 losses: [1.1418251233408228e-05, 1.1418251233408228e-05, 1.1418251233408228e-05, 1.1418251233408228e-05, 1.1418251233408228e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1418251233408228e-05

Optimization complete. Final v2v error: 2.8762848377227783 mm

Highest mean error: 3.2717339992523193 mm for frame 66

Lowest mean error: 2.5896475315093994 mm for frame 128

Saving results

Total time: 57.93985176086426
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_023/1059/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_023/1059.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_023/1059
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00922915
Iteration 2/25 | Loss: 0.00180251
Iteration 3/25 | Loss: 0.00145091
Iteration 4/25 | Loss: 0.00140988
Iteration 5/25 | Loss: 0.00140402
Iteration 6/25 | Loss: 0.00140310
Iteration 7/25 | Loss: 0.00140310
Iteration 8/25 | Loss: 0.00140310
Iteration 9/25 | Loss: 0.00140310
Iteration 10/25 | Loss: 0.00140310
Iteration 11/25 | Loss: 0.00140310
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0014031013706699014, 0.0014031013706699014, 0.0014031013706699014, 0.0014031013706699014, 0.0014031013706699014]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014031013706699014

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.75613123
Iteration 2/25 | Loss: 0.00069738
Iteration 3/25 | Loss: 0.00069738
Iteration 4/25 | Loss: 0.00069738
Iteration 5/25 | Loss: 0.00069738
Iteration 6/25 | Loss: 0.00069738
Iteration 7/25 | Loss: 0.00069738
Iteration 8/25 | Loss: 0.00069738
Iteration 9/25 | Loss: 0.00069738
Iteration 10/25 | Loss: 0.00069738
Iteration 11/25 | Loss: 0.00069738
Iteration 12/25 | Loss: 0.00069738
Iteration 13/25 | Loss: 0.00069738
Iteration 14/25 | Loss: 0.00069738
Iteration 15/25 | Loss: 0.00069738
Iteration 16/25 | Loss: 0.00069738
Iteration 17/25 | Loss: 0.00069738
Iteration 18/25 | Loss: 0.00069738
Iteration 19/25 | Loss: 0.00069738
Iteration 20/25 | Loss: 0.00069738
Iteration 21/25 | Loss: 0.00069738
Iteration 22/25 | Loss: 0.00069738
Iteration 23/25 | Loss: 0.00069738
Iteration 24/25 | Loss: 0.00069738
Iteration 25/25 | Loss: 0.00069738

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00069738
Iteration 2/1000 | Loss: 0.00005836
Iteration 3/1000 | Loss: 0.00004099
Iteration 4/1000 | Loss: 0.00003681
Iteration 5/1000 | Loss: 0.00003567
Iteration 6/1000 | Loss: 0.00003467
Iteration 7/1000 | Loss: 0.00003418
Iteration 8/1000 | Loss: 0.00003386
Iteration 9/1000 | Loss: 0.00003362
Iteration 10/1000 | Loss: 0.00003338
Iteration 11/1000 | Loss: 0.00003328
Iteration 12/1000 | Loss: 0.00003328
Iteration 13/1000 | Loss: 0.00003312
Iteration 14/1000 | Loss: 0.00003301
Iteration 15/1000 | Loss: 0.00003292
Iteration 16/1000 | Loss: 0.00003285
Iteration 17/1000 | Loss: 0.00003285
Iteration 18/1000 | Loss: 0.00003278
Iteration 19/1000 | Loss: 0.00003276
Iteration 20/1000 | Loss: 0.00003264
Iteration 21/1000 | Loss: 0.00003264
Iteration 22/1000 | Loss: 0.00003264
Iteration 23/1000 | Loss: 0.00003263
Iteration 24/1000 | Loss: 0.00003263
Iteration 25/1000 | Loss: 0.00003263
Iteration 26/1000 | Loss: 0.00003263
Iteration 27/1000 | Loss: 0.00003263
Iteration 28/1000 | Loss: 0.00003263
Iteration 29/1000 | Loss: 0.00003263
Iteration 30/1000 | Loss: 0.00003263
Iteration 31/1000 | Loss: 0.00003263
Iteration 32/1000 | Loss: 0.00003263
Iteration 33/1000 | Loss: 0.00003262
Iteration 34/1000 | Loss: 0.00003262
Iteration 35/1000 | Loss: 0.00003262
Iteration 36/1000 | Loss: 0.00003262
Iteration 37/1000 | Loss: 0.00003262
Iteration 38/1000 | Loss: 0.00003261
Iteration 39/1000 | Loss: 0.00003255
Iteration 40/1000 | Loss: 0.00003247
Iteration 41/1000 | Loss: 0.00003246
Iteration 42/1000 | Loss: 0.00003246
Iteration 43/1000 | Loss: 0.00003246
Iteration 44/1000 | Loss: 0.00003246
Iteration 45/1000 | Loss: 0.00003246
Iteration 46/1000 | Loss: 0.00003246
Iteration 47/1000 | Loss: 0.00003246
Iteration 48/1000 | Loss: 0.00003245
Iteration 49/1000 | Loss: 0.00003245
Iteration 50/1000 | Loss: 0.00003245
Iteration 51/1000 | Loss: 0.00003245
Iteration 52/1000 | Loss: 0.00003244
Iteration 53/1000 | Loss: 0.00003244
Iteration 54/1000 | Loss: 0.00003243
Iteration 55/1000 | Loss: 0.00003243
Iteration 56/1000 | Loss: 0.00003242
Iteration 57/1000 | Loss: 0.00003242
Iteration 58/1000 | Loss: 0.00003242
Iteration 59/1000 | Loss: 0.00003242
Iteration 60/1000 | Loss: 0.00003241
Iteration 61/1000 | Loss: 0.00003241
Iteration 62/1000 | Loss: 0.00003241
Iteration 63/1000 | Loss: 0.00003240
Iteration 64/1000 | Loss: 0.00003240
Iteration 65/1000 | Loss: 0.00003239
Iteration 66/1000 | Loss: 0.00003239
Iteration 67/1000 | Loss: 0.00003239
Iteration 68/1000 | Loss: 0.00003239
Iteration 69/1000 | Loss: 0.00003239
Iteration 70/1000 | Loss: 0.00003238
Iteration 71/1000 | Loss: 0.00003238
Iteration 72/1000 | Loss: 0.00003238
Iteration 73/1000 | Loss: 0.00003238
Iteration 74/1000 | Loss: 0.00003238
Iteration 75/1000 | Loss: 0.00003238
Iteration 76/1000 | Loss: 0.00003238
Iteration 77/1000 | Loss: 0.00003238
Iteration 78/1000 | Loss: 0.00003237
Iteration 79/1000 | Loss: 0.00003237
Iteration 80/1000 | Loss: 0.00003237
Iteration 81/1000 | Loss: 0.00003237
Iteration 82/1000 | Loss: 0.00003237
Iteration 83/1000 | Loss: 0.00003236
Iteration 84/1000 | Loss: 0.00003236
Iteration 85/1000 | Loss: 0.00003236
Iteration 86/1000 | Loss: 0.00003235
Iteration 87/1000 | Loss: 0.00003235
Iteration 88/1000 | Loss: 0.00003235
Iteration 89/1000 | Loss: 0.00003235
Iteration 90/1000 | Loss: 0.00003235
Iteration 91/1000 | Loss: 0.00003235
Iteration 92/1000 | Loss: 0.00003235
Iteration 93/1000 | Loss: 0.00003235
Iteration 94/1000 | Loss: 0.00003234
Iteration 95/1000 | Loss: 0.00003234
Iteration 96/1000 | Loss: 0.00003234
Iteration 97/1000 | Loss: 0.00003234
Iteration 98/1000 | Loss: 0.00003234
Iteration 99/1000 | Loss: 0.00003234
Iteration 100/1000 | Loss: 0.00003234
Iteration 101/1000 | Loss: 0.00003234
Iteration 102/1000 | Loss: 0.00003234
Iteration 103/1000 | Loss: 0.00003234
Iteration 104/1000 | Loss: 0.00003234
Iteration 105/1000 | Loss: 0.00003233
Iteration 106/1000 | Loss: 0.00003233
Iteration 107/1000 | Loss: 0.00003233
Iteration 108/1000 | Loss: 0.00003233
Iteration 109/1000 | Loss: 0.00003232
Iteration 110/1000 | Loss: 0.00003232
Iteration 111/1000 | Loss: 0.00003232
Iteration 112/1000 | Loss: 0.00003231
Iteration 113/1000 | Loss: 0.00003231
Iteration 114/1000 | Loss: 0.00003231
Iteration 115/1000 | Loss: 0.00003231
Iteration 116/1000 | Loss: 0.00003231
Iteration 117/1000 | Loss: 0.00003231
Iteration 118/1000 | Loss: 0.00003231
Iteration 119/1000 | Loss: 0.00003231
Iteration 120/1000 | Loss: 0.00003231
Iteration 121/1000 | Loss: 0.00003231
Iteration 122/1000 | Loss: 0.00003230
Iteration 123/1000 | Loss: 0.00003230
Iteration 124/1000 | Loss: 0.00003230
Iteration 125/1000 | Loss: 0.00003230
Iteration 126/1000 | Loss: 0.00003230
Iteration 127/1000 | Loss: 0.00003230
Iteration 128/1000 | Loss: 0.00003230
Iteration 129/1000 | Loss: 0.00003229
Iteration 130/1000 | Loss: 0.00003229
Iteration 131/1000 | Loss: 0.00003229
Iteration 132/1000 | Loss: 0.00003229
Iteration 133/1000 | Loss: 0.00003229
Iteration 134/1000 | Loss: 0.00003229
Iteration 135/1000 | Loss: 0.00003229
Iteration 136/1000 | Loss: 0.00003229
Iteration 137/1000 | Loss: 0.00003229
Iteration 138/1000 | Loss: 0.00003229
Iteration 139/1000 | Loss: 0.00003229
Iteration 140/1000 | Loss: 0.00003228
Iteration 141/1000 | Loss: 0.00003228
Iteration 142/1000 | Loss: 0.00003228
Iteration 143/1000 | Loss: 0.00003228
Iteration 144/1000 | Loss: 0.00003228
Iteration 145/1000 | Loss: 0.00003228
Iteration 146/1000 | Loss: 0.00003228
Iteration 147/1000 | Loss: 0.00003228
Iteration 148/1000 | Loss: 0.00003228
Iteration 149/1000 | Loss: 0.00003228
Iteration 150/1000 | Loss: 0.00003228
Iteration 151/1000 | Loss: 0.00003228
Iteration 152/1000 | Loss: 0.00003228
Iteration 153/1000 | Loss: 0.00003228
Iteration 154/1000 | Loss: 0.00003228
Iteration 155/1000 | Loss: 0.00003228
Iteration 156/1000 | Loss: 0.00003228
Iteration 157/1000 | Loss: 0.00003228
Iteration 158/1000 | Loss: 0.00003228
Iteration 159/1000 | Loss: 0.00003228
Iteration 160/1000 | Loss: 0.00003228
Iteration 161/1000 | Loss: 0.00003228
Iteration 162/1000 | Loss: 0.00003228
Iteration 163/1000 | Loss: 0.00003228
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 163. Stopping optimization.
Last 5 losses: [3.228445348213427e-05, 3.228445348213427e-05, 3.228445348213427e-05, 3.228445348213427e-05, 3.228445348213427e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.228445348213427e-05

Optimization complete. Final v2v error: 4.661245346069336 mm

Highest mean error: 4.912858486175537 mm for frame 35

Lowest mean error: 4.274415016174316 mm for frame 0

Saving results

Total time: 42.214470624923706
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_023/1012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_023/1012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_023/1012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00813729
Iteration 2/25 | Loss: 0.00121282
Iteration 3/25 | Loss: 0.00112297
Iteration 4/25 | Loss: 0.00111371
Iteration 5/25 | Loss: 0.00111169
Iteration 6/25 | Loss: 0.00111169
Iteration 7/25 | Loss: 0.00111169
Iteration 8/25 | Loss: 0.00111169
Iteration 9/25 | Loss: 0.00111169
Iteration 10/25 | Loss: 0.00111169
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.001111691235564649, 0.001111691235564649, 0.001111691235564649, 0.001111691235564649, 0.001111691235564649]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001111691235564649

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36040545
Iteration 2/25 | Loss: 0.00081309
Iteration 3/25 | Loss: 0.00081309
Iteration 4/25 | Loss: 0.00081309
Iteration 5/25 | Loss: 0.00081309
Iteration 6/25 | Loss: 0.00081309
Iteration 7/25 | Loss: 0.00081309
Iteration 8/25 | Loss: 0.00081309
Iteration 9/25 | Loss: 0.00081309
Iteration 10/25 | Loss: 0.00081309
Iteration 11/25 | Loss: 0.00081308
Iteration 12/25 | Loss: 0.00081308
Iteration 13/25 | Loss: 0.00081308
Iteration 14/25 | Loss: 0.00081308
Iteration 15/25 | Loss: 0.00081308
Iteration 16/25 | Loss: 0.00081308
Iteration 17/25 | Loss: 0.00081308
Iteration 18/25 | Loss: 0.00081308
Iteration 19/25 | Loss: 0.00081308
Iteration 20/25 | Loss: 0.00081308
Iteration 21/25 | Loss: 0.00081308
Iteration 22/25 | Loss: 0.00081308
Iteration 23/25 | Loss: 0.00081308
Iteration 24/25 | Loss: 0.00081308
Iteration 25/25 | Loss: 0.00081308

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00081308
Iteration 2/1000 | Loss: 0.00002081
Iteration 3/1000 | Loss: 0.00001331
Iteration 4/1000 | Loss: 0.00001201
Iteration 5/1000 | Loss: 0.00001128
Iteration 6/1000 | Loss: 0.00001062
Iteration 7/1000 | Loss: 0.00001030
Iteration 8/1000 | Loss: 0.00001009
Iteration 9/1000 | Loss: 0.00000981
Iteration 10/1000 | Loss: 0.00000977
Iteration 11/1000 | Loss: 0.00000976
Iteration 12/1000 | Loss: 0.00000975
Iteration 13/1000 | Loss: 0.00000973
Iteration 14/1000 | Loss: 0.00000972
Iteration 15/1000 | Loss: 0.00000966
Iteration 16/1000 | Loss: 0.00000966
Iteration 17/1000 | Loss: 0.00000961
Iteration 18/1000 | Loss: 0.00000959
Iteration 19/1000 | Loss: 0.00000958
Iteration 20/1000 | Loss: 0.00000958
Iteration 21/1000 | Loss: 0.00000955
Iteration 22/1000 | Loss: 0.00000953
Iteration 23/1000 | Loss: 0.00000953
Iteration 24/1000 | Loss: 0.00000951
Iteration 25/1000 | Loss: 0.00000951
Iteration 26/1000 | Loss: 0.00000950
Iteration 27/1000 | Loss: 0.00000949
Iteration 28/1000 | Loss: 0.00000948
Iteration 29/1000 | Loss: 0.00000948
Iteration 30/1000 | Loss: 0.00000948
Iteration 31/1000 | Loss: 0.00000947
Iteration 32/1000 | Loss: 0.00000947
Iteration 33/1000 | Loss: 0.00000945
Iteration 34/1000 | Loss: 0.00000942
Iteration 35/1000 | Loss: 0.00000942
Iteration 36/1000 | Loss: 0.00000942
Iteration 37/1000 | Loss: 0.00000941
Iteration 38/1000 | Loss: 0.00000941
Iteration 39/1000 | Loss: 0.00000939
Iteration 40/1000 | Loss: 0.00000939
Iteration 41/1000 | Loss: 0.00000938
Iteration 42/1000 | Loss: 0.00000938
Iteration 43/1000 | Loss: 0.00000938
Iteration 44/1000 | Loss: 0.00000938
Iteration 45/1000 | Loss: 0.00000938
Iteration 46/1000 | Loss: 0.00000938
Iteration 47/1000 | Loss: 0.00000937
Iteration 48/1000 | Loss: 0.00000936
Iteration 49/1000 | Loss: 0.00000935
Iteration 50/1000 | Loss: 0.00000935
Iteration 51/1000 | Loss: 0.00000935
Iteration 52/1000 | Loss: 0.00000934
Iteration 53/1000 | Loss: 0.00000934
Iteration 54/1000 | Loss: 0.00000934
Iteration 55/1000 | Loss: 0.00000934
Iteration 56/1000 | Loss: 0.00000934
Iteration 57/1000 | Loss: 0.00000933
Iteration 58/1000 | Loss: 0.00000933
Iteration 59/1000 | Loss: 0.00000932
Iteration 60/1000 | Loss: 0.00000931
Iteration 61/1000 | Loss: 0.00000931
Iteration 62/1000 | Loss: 0.00000931
Iteration 63/1000 | Loss: 0.00000930
Iteration 64/1000 | Loss: 0.00000928
Iteration 65/1000 | Loss: 0.00000928
Iteration 66/1000 | Loss: 0.00000927
Iteration 67/1000 | Loss: 0.00000926
Iteration 68/1000 | Loss: 0.00000925
Iteration 69/1000 | Loss: 0.00000925
Iteration 70/1000 | Loss: 0.00000924
Iteration 71/1000 | Loss: 0.00000922
Iteration 72/1000 | Loss: 0.00000922
Iteration 73/1000 | Loss: 0.00000919
Iteration 74/1000 | Loss: 0.00000918
Iteration 75/1000 | Loss: 0.00000917
Iteration 76/1000 | Loss: 0.00000917
Iteration 77/1000 | Loss: 0.00000917
Iteration 78/1000 | Loss: 0.00000917
Iteration 79/1000 | Loss: 0.00000917
Iteration 80/1000 | Loss: 0.00000917
Iteration 81/1000 | Loss: 0.00000917
Iteration 82/1000 | Loss: 0.00000916
Iteration 83/1000 | Loss: 0.00000916
Iteration 84/1000 | Loss: 0.00000915
Iteration 85/1000 | Loss: 0.00000915
Iteration 86/1000 | Loss: 0.00000915
Iteration 87/1000 | Loss: 0.00000915
Iteration 88/1000 | Loss: 0.00000915
Iteration 89/1000 | Loss: 0.00000914
Iteration 90/1000 | Loss: 0.00000914
Iteration 91/1000 | Loss: 0.00000914
Iteration 92/1000 | Loss: 0.00000914
Iteration 93/1000 | Loss: 0.00000913
Iteration 94/1000 | Loss: 0.00000913
Iteration 95/1000 | Loss: 0.00000913
Iteration 96/1000 | Loss: 0.00000912
Iteration 97/1000 | Loss: 0.00000912
Iteration 98/1000 | Loss: 0.00000912
Iteration 99/1000 | Loss: 0.00000912
Iteration 100/1000 | Loss: 0.00000912
Iteration 101/1000 | Loss: 0.00000911
Iteration 102/1000 | Loss: 0.00000911
Iteration 103/1000 | Loss: 0.00000911
Iteration 104/1000 | Loss: 0.00000911
Iteration 105/1000 | Loss: 0.00000911
Iteration 106/1000 | Loss: 0.00000911
Iteration 107/1000 | Loss: 0.00000911
Iteration 108/1000 | Loss: 0.00000911
Iteration 109/1000 | Loss: 0.00000910
Iteration 110/1000 | Loss: 0.00000910
Iteration 111/1000 | Loss: 0.00000910
Iteration 112/1000 | Loss: 0.00000909
Iteration 113/1000 | Loss: 0.00000909
Iteration 114/1000 | Loss: 0.00000909
Iteration 115/1000 | Loss: 0.00000909
Iteration 116/1000 | Loss: 0.00000908
Iteration 117/1000 | Loss: 0.00000908
Iteration 118/1000 | Loss: 0.00000908
Iteration 119/1000 | Loss: 0.00000907
Iteration 120/1000 | Loss: 0.00000907
Iteration 121/1000 | Loss: 0.00000906
Iteration 122/1000 | Loss: 0.00000906
Iteration 123/1000 | Loss: 0.00000906
Iteration 124/1000 | Loss: 0.00000906
Iteration 125/1000 | Loss: 0.00000906
Iteration 126/1000 | Loss: 0.00000906
Iteration 127/1000 | Loss: 0.00000906
Iteration 128/1000 | Loss: 0.00000906
Iteration 129/1000 | Loss: 0.00000906
Iteration 130/1000 | Loss: 0.00000905
Iteration 131/1000 | Loss: 0.00000905
Iteration 132/1000 | Loss: 0.00000905
Iteration 133/1000 | Loss: 0.00000905
Iteration 134/1000 | Loss: 0.00000905
Iteration 135/1000 | Loss: 0.00000905
Iteration 136/1000 | Loss: 0.00000905
Iteration 137/1000 | Loss: 0.00000904
Iteration 138/1000 | Loss: 0.00000904
Iteration 139/1000 | Loss: 0.00000904
Iteration 140/1000 | Loss: 0.00000904
Iteration 141/1000 | Loss: 0.00000904
Iteration 142/1000 | Loss: 0.00000904
Iteration 143/1000 | Loss: 0.00000904
Iteration 144/1000 | Loss: 0.00000904
Iteration 145/1000 | Loss: 0.00000903
Iteration 146/1000 | Loss: 0.00000903
Iteration 147/1000 | Loss: 0.00000903
Iteration 148/1000 | Loss: 0.00000903
Iteration 149/1000 | Loss: 0.00000903
Iteration 150/1000 | Loss: 0.00000902
Iteration 151/1000 | Loss: 0.00000902
Iteration 152/1000 | Loss: 0.00000902
Iteration 153/1000 | Loss: 0.00000902
Iteration 154/1000 | Loss: 0.00000902
Iteration 155/1000 | Loss: 0.00000902
Iteration 156/1000 | Loss: 0.00000902
Iteration 157/1000 | Loss: 0.00000902
Iteration 158/1000 | Loss: 0.00000902
Iteration 159/1000 | Loss: 0.00000902
Iteration 160/1000 | Loss: 0.00000902
Iteration 161/1000 | Loss: 0.00000902
Iteration 162/1000 | Loss: 0.00000902
Iteration 163/1000 | Loss: 0.00000902
Iteration 164/1000 | Loss: 0.00000902
Iteration 165/1000 | Loss: 0.00000902
Iteration 166/1000 | Loss: 0.00000902
Iteration 167/1000 | Loss: 0.00000902
Iteration 168/1000 | Loss: 0.00000902
Iteration 169/1000 | Loss: 0.00000902
Iteration 170/1000 | Loss: 0.00000902
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 170. Stopping optimization.
Last 5 losses: [9.023557140608318e-06, 9.023557140608318e-06, 9.023557140608318e-06, 9.023557140608318e-06, 9.023557140608318e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.023557140608318e-06

Optimization complete. Final v2v error: 2.5719165802001953 mm

Highest mean error: 2.7311906814575195 mm for frame 107

Lowest mean error: 2.4347310066223145 mm for frame 203

Saving results

Total time: 39.071192026138306
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_023/1064/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_023/1064.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_023/1064
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00998464
Iteration 2/25 | Loss: 0.00242142
Iteration 3/25 | Loss: 0.00165151
Iteration 4/25 | Loss: 0.00148621
Iteration 5/25 | Loss: 0.00141692
Iteration 6/25 | Loss: 0.00134295
Iteration 7/25 | Loss: 0.00139930
Iteration 8/25 | Loss: 0.00130029
Iteration 9/25 | Loss: 0.00124348
Iteration 10/25 | Loss: 0.00123019
Iteration 11/25 | Loss: 0.00122744
Iteration 12/25 | Loss: 0.00122517
Iteration 13/25 | Loss: 0.00122174
Iteration 14/25 | Loss: 0.00121967
Iteration 15/25 | Loss: 0.00121893
Iteration 16/25 | Loss: 0.00122004
Iteration 17/25 | Loss: 0.00121897
Iteration 18/25 | Loss: 0.00121906
Iteration 19/25 | Loss: 0.00121811
Iteration 20/25 | Loss: 0.00121978
Iteration 21/25 | Loss: 0.00121801
Iteration 22/25 | Loss: 0.00121927
Iteration 23/25 | Loss: 0.00121998
Iteration 24/25 | Loss: 0.00122065
Iteration 25/25 | Loss: 0.00121910

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.49820888
Iteration 2/25 | Loss: 0.00116338
Iteration 3/25 | Loss: 0.00103708
Iteration 4/25 | Loss: 0.00103708
Iteration 5/25 | Loss: 0.00103708
Iteration 6/25 | Loss: 0.00103708
Iteration 7/25 | Loss: 0.00103708
Iteration 8/25 | Loss: 0.00103708
Iteration 9/25 | Loss: 0.00103708
Iteration 10/25 | Loss: 0.00103708
Iteration 11/25 | Loss: 0.00103707
Iteration 12/25 | Loss: 0.00103707
Iteration 13/25 | Loss: 0.00103707
Iteration 14/25 | Loss: 0.00103707
Iteration 15/25 | Loss: 0.00103707
Iteration 16/25 | Loss: 0.00103707
Iteration 17/25 | Loss: 0.00103707
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0010370747186243534, 0.0010370747186243534, 0.0010370747186243534, 0.0010370747186243534, 0.0010370747186243534]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010370747186243534

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00103707
Iteration 2/1000 | Loss: 0.00037263
Iteration 3/1000 | Loss: 0.00009472
Iteration 4/1000 | Loss: 0.00008512
Iteration 5/1000 | Loss: 0.00017408
Iteration 6/1000 | Loss: 0.00020533
Iteration 7/1000 | Loss: 0.00020680
Iteration 8/1000 | Loss: 0.00005158
Iteration 9/1000 | Loss: 0.00005258
Iteration 10/1000 | Loss: 0.00014724
Iteration 11/1000 | Loss: 0.00005882
Iteration 12/1000 | Loss: 0.00006990
Iteration 13/1000 | Loss: 0.00019291
Iteration 14/1000 | Loss: 0.00006873
Iteration 15/1000 | Loss: 0.00005529
Iteration 16/1000 | Loss: 0.00004287
Iteration 17/1000 | Loss: 0.00007162
Iteration 18/1000 | Loss: 0.00004186
Iteration 19/1000 | Loss: 0.00006105
Iteration 20/1000 | Loss: 0.00033024
Iteration 21/1000 | Loss: 0.00026242
Iteration 22/1000 | Loss: 0.00061413
Iteration 23/1000 | Loss: 0.00031241
Iteration 24/1000 | Loss: 0.00010255
Iteration 25/1000 | Loss: 0.00025756
Iteration 26/1000 | Loss: 0.00019937
Iteration 27/1000 | Loss: 0.00009068
Iteration 28/1000 | Loss: 0.00014808
Iteration 29/1000 | Loss: 0.00005349
Iteration 30/1000 | Loss: 0.00004729
Iteration 31/1000 | Loss: 0.00004222
Iteration 32/1000 | Loss: 0.00003441
Iteration 33/1000 | Loss: 0.00004228
Iteration 34/1000 | Loss: 0.00004657
Iteration 35/1000 | Loss: 0.00004138
Iteration 36/1000 | Loss: 0.00007097
Iteration 37/1000 | Loss: 0.00006000
Iteration 38/1000 | Loss: 0.00004175
Iteration 39/1000 | Loss: 0.00006115
Iteration 40/1000 | Loss: 0.00003206
Iteration 41/1000 | Loss: 0.00004053
Iteration 42/1000 | Loss: 0.00003907
Iteration 43/1000 | Loss: 0.00003886
Iteration 44/1000 | Loss: 0.00006790
Iteration 45/1000 | Loss: 0.00002269
Iteration 46/1000 | Loss: 0.00002818
Iteration 47/1000 | Loss: 0.00004109
Iteration 48/1000 | Loss: 0.00010385
Iteration 49/1000 | Loss: 0.00011044
Iteration 50/1000 | Loss: 0.00001971
Iteration 51/1000 | Loss: 0.00001934
Iteration 52/1000 | Loss: 0.00003394
Iteration 53/1000 | Loss: 0.00011398
Iteration 54/1000 | Loss: 0.00001963
Iteration 55/1000 | Loss: 0.00005100
Iteration 56/1000 | Loss: 0.00002151
Iteration 57/1000 | Loss: 0.00002148
Iteration 58/1000 | Loss: 0.00002947
Iteration 59/1000 | Loss: 0.00013308
Iteration 60/1000 | Loss: 0.00003267
Iteration 61/1000 | Loss: 0.00002110
Iteration 62/1000 | Loss: 0.00002083
Iteration 63/1000 | Loss: 0.00001956
Iteration 64/1000 | Loss: 0.00001800
Iteration 65/1000 | Loss: 0.00001895
Iteration 66/1000 | Loss: 0.00001874
Iteration 67/1000 | Loss: 0.00040351
Iteration 68/1000 | Loss: 0.00032090
Iteration 69/1000 | Loss: 0.00004387
Iteration 70/1000 | Loss: 0.00002293
Iteration 71/1000 | Loss: 0.00001951
Iteration 72/1000 | Loss: 0.00004723
Iteration 73/1000 | Loss: 0.00002492
Iteration 74/1000 | Loss: 0.00001971
Iteration 75/1000 | Loss: 0.00002704
Iteration 76/1000 | Loss: 0.00001520
Iteration 77/1000 | Loss: 0.00001717
Iteration 78/1000 | Loss: 0.00001456
Iteration 79/1000 | Loss: 0.00001712
Iteration 80/1000 | Loss: 0.00001652
Iteration 81/1000 | Loss: 0.00001439
Iteration 82/1000 | Loss: 0.00001438
Iteration 83/1000 | Loss: 0.00001438
Iteration 84/1000 | Loss: 0.00001438
Iteration 85/1000 | Loss: 0.00001438
Iteration 86/1000 | Loss: 0.00001510
Iteration 87/1000 | Loss: 0.00001436
Iteration 88/1000 | Loss: 0.00001436
Iteration 89/1000 | Loss: 0.00001436
Iteration 90/1000 | Loss: 0.00001436
Iteration 91/1000 | Loss: 0.00001436
Iteration 92/1000 | Loss: 0.00001436
Iteration 93/1000 | Loss: 0.00001435
Iteration 94/1000 | Loss: 0.00001647
Iteration 95/1000 | Loss: 0.00001976
Iteration 96/1000 | Loss: 0.00001492
Iteration 97/1000 | Loss: 0.00001426
Iteration 98/1000 | Loss: 0.00001426
Iteration 99/1000 | Loss: 0.00001426
Iteration 100/1000 | Loss: 0.00001425
Iteration 101/1000 | Loss: 0.00001425
Iteration 102/1000 | Loss: 0.00001425
Iteration 103/1000 | Loss: 0.00001425
Iteration 104/1000 | Loss: 0.00001425
Iteration 105/1000 | Loss: 0.00001425
Iteration 106/1000 | Loss: 0.00001425
Iteration 107/1000 | Loss: 0.00001425
Iteration 108/1000 | Loss: 0.00001425
Iteration 109/1000 | Loss: 0.00001425
Iteration 110/1000 | Loss: 0.00001425
Iteration 111/1000 | Loss: 0.00001424
Iteration 112/1000 | Loss: 0.00001424
Iteration 113/1000 | Loss: 0.00001423
Iteration 114/1000 | Loss: 0.00001423
Iteration 115/1000 | Loss: 0.00001423
Iteration 116/1000 | Loss: 0.00001422
Iteration 117/1000 | Loss: 0.00001422
Iteration 118/1000 | Loss: 0.00001422
Iteration 119/1000 | Loss: 0.00001422
Iteration 120/1000 | Loss: 0.00001422
Iteration 121/1000 | Loss: 0.00001422
Iteration 122/1000 | Loss: 0.00001422
Iteration 123/1000 | Loss: 0.00001421
Iteration 124/1000 | Loss: 0.00001494
Iteration 125/1000 | Loss: 0.00002917
Iteration 126/1000 | Loss: 0.00002386
Iteration 127/1000 | Loss: 0.00001496
Iteration 128/1000 | Loss: 0.00001536
Iteration 129/1000 | Loss: 0.00001514
Iteration 130/1000 | Loss: 0.00001524
Iteration 131/1000 | Loss: 0.00001845
Iteration 132/1000 | Loss: 0.00001915
Iteration 133/1000 | Loss: 0.00001433
Iteration 134/1000 | Loss: 0.00001543
Iteration 135/1000 | Loss: 0.00001407
Iteration 136/1000 | Loss: 0.00001407
Iteration 137/1000 | Loss: 0.00001407
Iteration 138/1000 | Loss: 0.00001407
Iteration 139/1000 | Loss: 0.00001407
Iteration 140/1000 | Loss: 0.00001407
Iteration 141/1000 | Loss: 0.00001407
Iteration 142/1000 | Loss: 0.00001407
Iteration 143/1000 | Loss: 0.00001407
Iteration 144/1000 | Loss: 0.00001407
Iteration 145/1000 | Loss: 0.00001407
Iteration 146/1000 | Loss: 0.00001407
Iteration 147/1000 | Loss: 0.00001407
Iteration 148/1000 | Loss: 0.00001407
Iteration 149/1000 | Loss: 0.00001407
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 149. Stopping optimization.
Last 5 losses: [1.4066611583984923e-05, 1.4066611583984923e-05, 1.4066611583984923e-05, 1.4066611583984923e-05, 1.4066611583984923e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4066611583984923e-05

Optimization complete. Final v2v error: 3.129065752029419 mm

Highest mean error: 6.052702903747559 mm for frame 167

Lowest mean error: 2.7727229595184326 mm for frame 12

Saving results

Total time: 201.3069212436676
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_023/1023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_023/1023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_023/1023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00790284
Iteration 2/25 | Loss: 0.00136059
Iteration 3/25 | Loss: 0.00115023
Iteration 4/25 | Loss: 0.00113522
Iteration 5/25 | Loss: 0.00113000
Iteration 6/25 | Loss: 0.00112834
Iteration 7/25 | Loss: 0.00112822
Iteration 8/25 | Loss: 0.00112822
Iteration 9/25 | Loss: 0.00112822
Iteration 10/25 | Loss: 0.00112822
Iteration 11/25 | Loss: 0.00112822
Iteration 12/25 | Loss: 0.00112822
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0011282231425866485, 0.0011282231425866485, 0.0011282231425866485, 0.0011282231425866485, 0.0011282231425866485]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011282231425866485

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.19510853
Iteration 2/25 | Loss: 0.00092116
Iteration 3/25 | Loss: 0.00092115
Iteration 4/25 | Loss: 0.00092115
Iteration 5/25 | Loss: 0.00092115
Iteration 6/25 | Loss: 0.00092115
Iteration 7/25 | Loss: 0.00092115
Iteration 8/25 | Loss: 0.00092115
Iteration 9/25 | Loss: 0.00092115
Iteration 10/25 | Loss: 0.00092115
Iteration 11/25 | Loss: 0.00092115
Iteration 12/25 | Loss: 0.00092115
Iteration 13/25 | Loss: 0.00092115
Iteration 14/25 | Loss: 0.00092115
Iteration 15/25 | Loss: 0.00092115
Iteration 16/25 | Loss: 0.00092115
Iteration 17/25 | Loss: 0.00092115
Iteration 18/25 | Loss: 0.00092115
Iteration 19/25 | Loss: 0.00092115
Iteration 20/25 | Loss: 0.00092115
Iteration 21/25 | Loss: 0.00092115
Iteration 22/25 | Loss: 0.00092115
Iteration 23/25 | Loss: 0.00092115
Iteration 24/25 | Loss: 0.00092115
Iteration 25/25 | Loss: 0.00092115

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00092115
Iteration 2/1000 | Loss: 0.00003240
Iteration 3/1000 | Loss: 0.00002158
Iteration 4/1000 | Loss: 0.00001832
Iteration 5/1000 | Loss: 0.00001664
Iteration 6/1000 | Loss: 0.00001546
Iteration 7/1000 | Loss: 0.00001486
Iteration 8/1000 | Loss: 0.00001434
Iteration 9/1000 | Loss: 0.00001391
Iteration 10/1000 | Loss: 0.00001363
Iteration 11/1000 | Loss: 0.00001338
Iteration 12/1000 | Loss: 0.00001319
Iteration 13/1000 | Loss: 0.00001317
Iteration 14/1000 | Loss: 0.00001317
Iteration 15/1000 | Loss: 0.00001310
Iteration 16/1000 | Loss: 0.00001307
Iteration 17/1000 | Loss: 0.00001306
Iteration 18/1000 | Loss: 0.00001306
Iteration 19/1000 | Loss: 0.00001305
Iteration 20/1000 | Loss: 0.00001304
Iteration 21/1000 | Loss: 0.00001301
Iteration 22/1000 | Loss: 0.00001299
Iteration 23/1000 | Loss: 0.00001298
Iteration 24/1000 | Loss: 0.00001294
Iteration 25/1000 | Loss: 0.00001294
Iteration 26/1000 | Loss: 0.00001292
Iteration 27/1000 | Loss: 0.00001287
Iteration 28/1000 | Loss: 0.00001285
Iteration 29/1000 | Loss: 0.00001281
Iteration 30/1000 | Loss: 0.00001277
Iteration 31/1000 | Loss: 0.00001276
Iteration 32/1000 | Loss: 0.00001276
Iteration 33/1000 | Loss: 0.00001275
Iteration 34/1000 | Loss: 0.00001274
Iteration 35/1000 | Loss: 0.00001273
Iteration 36/1000 | Loss: 0.00001269
Iteration 37/1000 | Loss: 0.00001269
Iteration 38/1000 | Loss: 0.00001269
Iteration 39/1000 | Loss: 0.00001268
Iteration 40/1000 | Loss: 0.00001267
Iteration 41/1000 | Loss: 0.00001267
Iteration 42/1000 | Loss: 0.00001267
Iteration 43/1000 | Loss: 0.00001266
Iteration 44/1000 | Loss: 0.00001266
Iteration 45/1000 | Loss: 0.00001265
Iteration 46/1000 | Loss: 0.00001265
Iteration 47/1000 | Loss: 0.00001265
Iteration 48/1000 | Loss: 0.00001264
Iteration 49/1000 | Loss: 0.00001263
Iteration 50/1000 | Loss: 0.00001263
Iteration 51/1000 | Loss: 0.00001263
Iteration 52/1000 | Loss: 0.00001261
Iteration 53/1000 | Loss: 0.00001257
Iteration 54/1000 | Loss: 0.00001257
Iteration 55/1000 | Loss: 0.00001257
Iteration 56/1000 | Loss: 0.00001256
Iteration 57/1000 | Loss: 0.00001256
Iteration 58/1000 | Loss: 0.00001256
Iteration 59/1000 | Loss: 0.00001256
Iteration 60/1000 | Loss: 0.00001256
Iteration 61/1000 | Loss: 0.00001256
Iteration 62/1000 | Loss: 0.00001256
Iteration 63/1000 | Loss: 0.00001255
Iteration 64/1000 | Loss: 0.00001255
Iteration 65/1000 | Loss: 0.00001254
Iteration 66/1000 | Loss: 0.00001253
Iteration 67/1000 | Loss: 0.00001253
Iteration 68/1000 | Loss: 0.00001252
Iteration 69/1000 | Loss: 0.00001252
Iteration 70/1000 | Loss: 0.00001252
Iteration 71/1000 | Loss: 0.00001252
Iteration 72/1000 | Loss: 0.00001252
Iteration 73/1000 | Loss: 0.00001252
Iteration 74/1000 | Loss: 0.00001252
Iteration 75/1000 | Loss: 0.00001252
Iteration 76/1000 | Loss: 0.00001252
Iteration 77/1000 | Loss: 0.00001252
Iteration 78/1000 | Loss: 0.00001251
Iteration 79/1000 | Loss: 0.00001251
Iteration 80/1000 | Loss: 0.00001251
Iteration 81/1000 | Loss: 0.00001251
Iteration 82/1000 | Loss: 0.00001251
Iteration 83/1000 | Loss: 0.00001250
Iteration 84/1000 | Loss: 0.00001250
Iteration 85/1000 | Loss: 0.00001249
Iteration 86/1000 | Loss: 0.00001249
Iteration 87/1000 | Loss: 0.00001249
Iteration 88/1000 | Loss: 0.00001249
Iteration 89/1000 | Loss: 0.00001248
Iteration 90/1000 | Loss: 0.00001248
Iteration 91/1000 | Loss: 0.00001248
Iteration 92/1000 | Loss: 0.00001248
Iteration 93/1000 | Loss: 0.00001247
Iteration 94/1000 | Loss: 0.00001247
Iteration 95/1000 | Loss: 0.00001247
Iteration 96/1000 | Loss: 0.00001247
Iteration 97/1000 | Loss: 0.00001247
Iteration 98/1000 | Loss: 0.00001247
Iteration 99/1000 | Loss: 0.00001247
Iteration 100/1000 | Loss: 0.00001246
Iteration 101/1000 | Loss: 0.00001246
Iteration 102/1000 | Loss: 0.00001246
Iteration 103/1000 | Loss: 0.00001246
Iteration 104/1000 | Loss: 0.00001246
Iteration 105/1000 | Loss: 0.00001246
Iteration 106/1000 | Loss: 0.00001246
Iteration 107/1000 | Loss: 0.00001246
Iteration 108/1000 | Loss: 0.00001245
Iteration 109/1000 | Loss: 0.00001245
Iteration 110/1000 | Loss: 0.00001245
Iteration 111/1000 | Loss: 0.00001245
Iteration 112/1000 | Loss: 0.00001245
Iteration 113/1000 | Loss: 0.00001245
Iteration 114/1000 | Loss: 0.00001244
Iteration 115/1000 | Loss: 0.00001244
Iteration 116/1000 | Loss: 0.00001244
Iteration 117/1000 | Loss: 0.00001243
Iteration 118/1000 | Loss: 0.00001243
Iteration 119/1000 | Loss: 0.00001243
Iteration 120/1000 | Loss: 0.00001242
Iteration 121/1000 | Loss: 0.00001242
Iteration 122/1000 | Loss: 0.00001242
Iteration 123/1000 | Loss: 0.00001242
Iteration 124/1000 | Loss: 0.00001241
Iteration 125/1000 | Loss: 0.00001241
Iteration 126/1000 | Loss: 0.00001241
Iteration 127/1000 | Loss: 0.00001241
Iteration 128/1000 | Loss: 0.00001241
Iteration 129/1000 | Loss: 0.00001241
Iteration 130/1000 | Loss: 0.00001241
Iteration 131/1000 | Loss: 0.00001240
Iteration 132/1000 | Loss: 0.00001240
Iteration 133/1000 | Loss: 0.00001240
Iteration 134/1000 | Loss: 0.00001240
Iteration 135/1000 | Loss: 0.00001240
Iteration 136/1000 | Loss: 0.00001240
Iteration 137/1000 | Loss: 0.00001240
Iteration 138/1000 | Loss: 0.00001240
Iteration 139/1000 | Loss: 0.00001240
Iteration 140/1000 | Loss: 0.00001240
Iteration 141/1000 | Loss: 0.00001240
Iteration 142/1000 | Loss: 0.00001240
Iteration 143/1000 | Loss: 0.00001240
Iteration 144/1000 | Loss: 0.00001240
Iteration 145/1000 | Loss: 0.00001239
Iteration 146/1000 | Loss: 0.00001239
Iteration 147/1000 | Loss: 0.00001239
Iteration 148/1000 | Loss: 0.00001239
Iteration 149/1000 | Loss: 0.00001238
Iteration 150/1000 | Loss: 0.00001238
Iteration 151/1000 | Loss: 0.00001238
Iteration 152/1000 | Loss: 0.00001238
Iteration 153/1000 | Loss: 0.00001238
Iteration 154/1000 | Loss: 0.00001238
Iteration 155/1000 | Loss: 0.00001238
Iteration 156/1000 | Loss: 0.00001238
Iteration 157/1000 | Loss: 0.00001237
Iteration 158/1000 | Loss: 0.00001237
Iteration 159/1000 | Loss: 0.00001237
Iteration 160/1000 | Loss: 0.00001237
Iteration 161/1000 | Loss: 0.00001237
Iteration 162/1000 | Loss: 0.00001237
Iteration 163/1000 | Loss: 0.00001237
Iteration 164/1000 | Loss: 0.00001237
Iteration 165/1000 | Loss: 0.00001236
Iteration 166/1000 | Loss: 0.00001236
Iteration 167/1000 | Loss: 0.00001236
Iteration 168/1000 | Loss: 0.00001236
Iteration 169/1000 | Loss: 0.00001235
Iteration 170/1000 | Loss: 0.00001235
Iteration 171/1000 | Loss: 0.00001235
Iteration 172/1000 | Loss: 0.00001235
Iteration 173/1000 | Loss: 0.00001235
Iteration 174/1000 | Loss: 0.00001235
Iteration 175/1000 | Loss: 0.00001235
Iteration 176/1000 | Loss: 0.00001235
Iteration 177/1000 | Loss: 0.00001235
Iteration 178/1000 | Loss: 0.00001235
Iteration 179/1000 | Loss: 0.00001235
Iteration 180/1000 | Loss: 0.00001235
Iteration 181/1000 | Loss: 0.00001235
Iteration 182/1000 | Loss: 0.00001235
Iteration 183/1000 | Loss: 0.00001235
Iteration 184/1000 | Loss: 0.00001235
Iteration 185/1000 | Loss: 0.00001235
Iteration 186/1000 | Loss: 0.00001235
Iteration 187/1000 | Loss: 0.00001235
Iteration 188/1000 | Loss: 0.00001235
Iteration 189/1000 | Loss: 0.00001235
Iteration 190/1000 | Loss: 0.00001235
Iteration 191/1000 | Loss: 0.00001235
Iteration 192/1000 | Loss: 0.00001235
Iteration 193/1000 | Loss: 0.00001235
Iteration 194/1000 | Loss: 0.00001235
Iteration 195/1000 | Loss: 0.00001235
Iteration 196/1000 | Loss: 0.00001235
Iteration 197/1000 | Loss: 0.00001235
Iteration 198/1000 | Loss: 0.00001235
Iteration 199/1000 | Loss: 0.00001235
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 199. Stopping optimization.
Last 5 losses: [1.2351113582553808e-05, 1.2351113582553808e-05, 1.2351113582553808e-05, 1.2351113582553808e-05, 1.2351113582553808e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2351113582553808e-05

Optimization complete. Final v2v error: 2.8978559970855713 mm

Highest mean error: 4.258659362792969 mm for frame 76

Lowest mean error: 2.4159536361694336 mm for frame 110

Saving results

Total time: 43.305479288101196
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_023/1024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_023/1024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_023/1024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00589823
Iteration 2/25 | Loss: 0.00165375
Iteration 3/25 | Loss: 0.00135377
Iteration 4/25 | Loss: 0.00132892
Iteration 5/25 | Loss: 0.00132432
Iteration 6/25 | Loss: 0.00132375
Iteration 7/25 | Loss: 0.00132375
Iteration 8/25 | Loss: 0.00132375
Iteration 9/25 | Loss: 0.00132375
Iteration 10/25 | Loss: 0.00132375
Iteration 11/25 | Loss: 0.00132375
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013237482635304332, 0.0013237482635304332, 0.0013237482635304332, 0.0013237482635304332, 0.0013237482635304332]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013237482635304332

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33564854
Iteration 2/25 | Loss: 0.00091224
Iteration 3/25 | Loss: 0.00091220
Iteration 4/25 | Loss: 0.00091220
Iteration 5/25 | Loss: 0.00091220
Iteration 6/25 | Loss: 0.00091220
Iteration 7/25 | Loss: 0.00091220
Iteration 8/25 | Loss: 0.00091220
Iteration 9/25 | Loss: 0.00091220
Iteration 10/25 | Loss: 0.00091220
Iteration 11/25 | Loss: 0.00091220
Iteration 12/25 | Loss: 0.00091220
Iteration 13/25 | Loss: 0.00091220
Iteration 14/25 | Loss: 0.00091220
Iteration 15/25 | Loss: 0.00091220
Iteration 16/25 | Loss: 0.00091220
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0009122015908360481, 0.0009122015908360481, 0.0009122015908360481, 0.0009122015908360481, 0.0009122015908360481]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009122015908360481

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00091220
Iteration 2/1000 | Loss: 0.00005248
Iteration 3/1000 | Loss: 0.00003045
Iteration 4/1000 | Loss: 0.00002569
Iteration 5/1000 | Loss: 0.00002418
Iteration 6/1000 | Loss: 0.00002319
Iteration 7/1000 | Loss: 0.00002240
Iteration 8/1000 | Loss: 0.00002208
Iteration 9/1000 | Loss: 0.00002183
Iteration 10/1000 | Loss: 0.00002178
Iteration 11/1000 | Loss: 0.00002173
Iteration 12/1000 | Loss: 0.00002171
Iteration 13/1000 | Loss: 0.00002165
Iteration 14/1000 | Loss: 0.00002165
Iteration 15/1000 | Loss: 0.00002165
Iteration 16/1000 | Loss: 0.00002165
Iteration 17/1000 | Loss: 0.00002164
Iteration 18/1000 | Loss: 0.00002164
Iteration 19/1000 | Loss: 0.00002164
Iteration 20/1000 | Loss: 0.00002164
Iteration 21/1000 | Loss: 0.00002164
Iteration 22/1000 | Loss: 0.00002164
Iteration 23/1000 | Loss: 0.00002164
Iteration 24/1000 | Loss: 0.00002163
Iteration 25/1000 | Loss: 0.00002148
Iteration 26/1000 | Loss: 0.00002145
Iteration 27/1000 | Loss: 0.00002141
Iteration 28/1000 | Loss: 0.00002133
Iteration 29/1000 | Loss: 0.00002131
Iteration 30/1000 | Loss: 0.00002129
Iteration 31/1000 | Loss: 0.00002128
Iteration 32/1000 | Loss: 0.00002128
Iteration 33/1000 | Loss: 0.00002126
Iteration 34/1000 | Loss: 0.00002122
Iteration 35/1000 | Loss: 0.00002122
Iteration 36/1000 | Loss: 0.00002121
Iteration 37/1000 | Loss: 0.00002121
Iteration 38/1000 | Loss: 0.00002121
Iteration 39/1000 | Loss: 0.00002120
Iteration 40/1000 | Loss: 0.00002120
Iteration 41/1000 | Loss: 0.00002120
Iteration 42/1000 | Loss: 0.00002120
Iteration 43/1000 | Loss: 0.00002119
Iteration 44/1000 | Loss: 0.00002117
Iteration 45/1000 | Loss: 0.00002115
Iteration 46/1000 | Loss: 0.00002115
Iteration 47/1000 | Loss: 0.00002115
Iteration 48/1000 | Loss: 0.00002115
Iteration 49/1000 | Loss: 0.00002115
Iteration 50/1000 | Loss: 0.00002115
Iteration 51/1000 | Loss: 0.00002114
Iteration 52/1000 | Loss: 0.00002114
Iteration 53/1000 | Loss: 0.00002113
Iteration 54/1000 | Loss: 0.00002112
Iteration 55/1000 | Loss: 0.00002112
Iteration 56/1000 | Loss: 0.00002111
Iteration 57/1000 | Loss: 0.00002111
Iteration 58/1000 | Loss: 0.00002111
Iteration 59/1000 | Loss: 0.00002111
Iteration 60/1000 | Loss: 0.00002111
Iteration 61/1000 | Loss: 0.00002111
Iteration 62/1000 | Loss: 0.00002111
Iteration 63/1000 | Loss: 0.00002111
Iteration 64/1000 | Loss: 0.00002110
Iteration 65/1000 | Loss: 0.00002110
Iteration 66/1000 | Loss: 0.00002109
Iteration 67/1000 | Loss: 0.00002108
Iteration 68/1000 | Loss: 0.00002108
Iteration 69/1000 | Loss: 0.00002108
Iteration 70/1000 | Loss: 0.00002108
Iteration 71/1000 | Loss: 0.00002107
Iteration 72/1000 | Loss: 0.00002107
Iteration 73/1000 | Loss: 0.00002107
Iteration 74/1000 | Loss: 0.00002107
Iteration 75/1000 | Loss: 0.00002107
Iteration 76/1000 | Loss: 0.00002107
Iteration 77/1000 | Loss: 0.00002106
Iteration 78/1000 | Loss: 0.00002106
Iteration 79/1000 | Loss: 0.00002106
Iteration 80/1000 | Loss: 0.00002106
Iteration 81/1000 | Loss: 0.00002105
Iteration 82/1000 | Loss: 0.00002105
Iteration 83/1000 | Loss: 0.00002105
Iteration 84/1000 | Loss: 0.00002105
Iteration 85/1000 | Loss: 0.00002104
Iteration 86/1000 | Loss: 0.00002104
Iteration 87/1000 | Loss: 0.00002104
Iteration 88/1000 | Loss: 0.00002104
Iteration 89/1000 | Loss: 0.00002104
Iteration 90/1000 | Loss: 0.00002104
Iteration 91/1000 | Loss: 0.00002104
Iteration 92/1000 | Loss: 0.00002104
Iteration 93/1000 | Loss: 0.00002104
Iteration 94/1000 | Loss: 0.00002103
Iteration 95/1000 | Loss: 0.00002103
Iteration 96/1000 | Loss: 0.00002102
Iteration 97/1000 | Loss: 0.00002102
Iteration 98/1000 | Loss: 0.00002102
Iteration 99/1000 | Loss: 0.00002102
Iteration 100/1000 | Loss: 0.00002102
Iteration 101/1000 | Loss: 0.00002101
Iteration 102/1000 | Loss: 0.00002101
Iteration 103/1000 | Loss: 0.00002101
Iteration 104/1000 | Loss: 0.00002101
Iteration 105/1000 | Loss: 0.00002100
Iteration 106/1000 | Loss: 0.00002100
Iteration 107/1000 | Loss: 0.00002100
Iteration 108/1000 | Loss: 0.00002099
Iteration 109/1000 | Loss: 0.00002099
Iteration 110/1000 | Loss: 0.00002099
Iteration 111/1000 | Loss: 0.00002099
Iteration 112/1000 | Loss: 0.00002099
Iteration 113/1000 | Loss: 0.00002099
Iteration 114/1000 | Loss: 0.00002099
Iteration 115/1000 | Loss: 0.00002099
Iteration 116/1000 | Loss: 0.00002099
Iteration 117/1000 | Loss: 0.00002099
Iteration 118/1000 | Loss: 0.00002098
Iteration 119/1000 | Loss: 0.00002098
Iteration 120/1000 | Loss: 0.00002098
Iteration 121/1000 | Loss: 0.00002098
Iteration 122/1000 | Loss: 0.00002098
Iteration 123/1000 | Loss: 0.00002098
Iteration 124/1000 | Loss: 0.00002098
Iteration 125/1000 | Loss: 0.00002098
Iteration 126/1000 | Loss: 0.00002098
Iteration 127/1000 | Loss: 0.00002098
Iteration 128/1000 | Loss: 0.00002098
Iteration 129/1000 | Loss: 0.00002098
Iteration 130/1000 | Loss: 0.00002098
Iteration 131/1000 | Loss: 0.00002098
Iteration 132/1000 | Loss: 0.00002098
Iteration 133/1000 | Loss: 0.00002098
Iteration 134/1000 | Loss: 0.00002098
Iteration 135/1000 | Loss: 0.00002098
Iteration 136/1000 | Loss: 0.00002098
Iteration 137/1000 | Loss: 0.00002098
Iteration 138/1000 | Loss: 0.00002098
Iteration 139/1000 | Loss: 0.00002098
Iteration 140/1000 | Loss: 0.00002098
Iteration 141/1000 | Loss: 0.00002098
Iteration 142/1000 | Loss: 0.00002098
Iteration 143/1000 | Loss: 0.00002098
Iteration 144/1000 | Loss: 0.00002098
Iteration 145/1000 | Loss: 0.00002098
Iteration 146/1000 | Loss: 0.00002098
Iteration 147/1000 | Loss: 0.00002098
Iteration 148/1000 | Loss: 0.00002098
Iteration 149/1000 | Loss: 0.00002098
Iteration 150/1000 | Loss: 0.00002098
Iteration 151/1000 | Loss: 0.00002098
Iteration 152/1000 | Loss: 0.00002098
Iteration 153/1000 | Loss: 0.00002098
Iteration 154/1000 | Loss: 0.00002098
Iteration 155/1000 | Loss: 0.00002098
Iteration 156/1000 | Loss: 0.00002098
Iteration 157/1000 | Loss: 0.00002098
Iteration 158/1000 | Loss: 0.00002098
Iteration 159/1000 | Loss: 0.00002098
Iteration 160/1000 | Loss: 0.00002098
Iteration 161/1000 | Loss: 0.00002098
Iteration 162/1000 | Loss: 0.00002098
Iteration 163/1000 | Loss: 0.00002098
Iteration 164/1000 | Loss: 0.00002098
Iteration 165/1000 | Loss: 0.00002098
Iteration 166/1000 | Loss: 0.00002098
Iteration 167/1000 | Loss: 0.00002098
Iteration 168/1000 | Loss: 0.00002098
Iteration 169/1000 | Loss: 0.00002098
Iteration 170/1000 | Loss: 0.00002098
Iteration 171/1000 | Loss: 0.00002098
Iteration 172/1000 | Loss: 0.00002098
Iteration 173/1000 | Loss: 0.00002098
Iteration 174/1000 | Loss: 0.00002098
Iteration 175/1000 | Loss: 0.00002098
Iteration 176/1000 | Loss: 0.00002098
Iteration 177/1000 | Loss: 0.00002098
Iteration 178/1000 | Loss: 0.00002098
Iteration 179/1000 | Loss: 0.00002098
Iteration 180/1000 | Loss: 0.00002098
Iteration 181/1000 | Loss: 0.00002098
Iteration 182/1000 | Loss: 0.00002098
Iteration 183/1000 | Loss: 0.00002098
Iteration 184/1000 | Loss: 0.00002098
Iteration 185/1000 | Loss: 0.00002098
Iteration 186/1000 | Loss: 0.00002098
Iteration 187/1000 | Loss: 0.00002098
Iteration 188/1000 | Loss: 0.00002098
Iteration 189/1000 | Loss: 0.00002098
Iteration 190/1000 | Loss: 0.00002098
Iteration 191/1000 | Loss: 0.00002098
Iteration 192/1000 | Loss: 0.00002098
Iteration 193/1000 | Loss: 0.00002098
Iteration 194/1000 | Loss: 0.00002098
Iteration 195/1000 | Loss: 0.00002098
Iteration 196/1000 | Loss: 0.00002098
Iteration 197/1000 | Loss: 0.00002098
Iteration 198/1000 | Loss: 0.00002098
Iteration 199/1000 | Loss: 0.00002098
Iteration 200/1000 | Loss: 0.00002098
Iteration 201/1000 | Loss: 0.00002098
Iteration 202/1000 | Loss: 0.00002098
Iteration 203/1000 | Loss: 0.00002098
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 203. Stopping optimization.
Last 5 losses: [2.09756017284235e-05, 2.09756017284235e-05, 2.09756017284235e-05, 2.09756017284235e-05, 2.09756017284235e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.09756017284235e-05

Optimization complete. Final v2v error: 3.8930230140686035 mm

Highest mean error: 4.030352592468262 mm for frame 84

Lowest mean error: 3.445470094680786 mm for frame 4

Saving results

Total time: 37.45168924331665
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_023/1065/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_023/1065.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_023/1065
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00273622
Iteration 2/25 | Loss: 0.00130550
Iteration 3/25 | Loss: 0.00114786
Iteration 4/25 | Loss: 0.00112948
Iteration 5/25 | Loss: 0.00112476
Iteration 6/25 | Loss: 0.00112279
Iteration 7/25 | Loss: 0.00112221
Iteration 8/25 | Loss: 0.00112207
Iteration 9/25 | Loss: 0.00112207
Iteration 10/25 | Loss: 0.00112207
Iteration 11/25 | Loss: 0.00112207
Iteration 12/25 | Loss: 0.00112207
Iteration 13/25 | Loss: 0.00112207
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0011220723390579224, 0.0011220723390579224, 0.0011220723390579224, 0.0011220723390579224, 0.0011220723390579224]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011220723390579224

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33075047
Iteration 2/25 | Loss: 0.00098649
Iteration 3/25 | Loss: 0.00098649
Iteration 4/25 | Loss: 0.00098649
Iteration 5/25 | Loss: 0.00098649
Iteration 6/25 | Loss: 0.00098649
Iteration 7/25 | Loss: 0.00098649
Iteration 8/25 | Loss: 0.00098649
Iteration 9/25 | Loss: 0.00098649
Iteration 10/25 | Loss: 0.00098649
Iteration 11/25 | Loss: 0.00098649
Iteration 12/25 | Loss: 0.00098649
Iteration 13/25 | Loss: 0.00098649
Iteration 14/25 | Loss: 0.00098649
Iteration 15/25 | Loss: 0.00098649
Iteration 16/25 | Loss: 0.00098649
Iteration 17/25 | Loss: 0.00098649
Iteration 18/25 | Loss: 0.00098649
Iteration 19/25 | Loss: 0.00098649
Iteration 20/25 | Loss: 0.00098649
Iteration 21/25 | Loss: 0.00098649
Iteration 22/25 | Loss: 0.00098649
Iteration 23/25 | Loss: 0.00098649
Iteration 24/25 | Loss: 0.00098649
Iteration 25/25 | Loss: 0.00098649
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0009864919120445848, 0.0009864919120445848, 0.0009864919120445848, 0.0009864919120445848, 0.0009864919120445848]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009864919120445848

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00098649
Iteration 2/1000 | Loss: 0.00004246
Iteration 3/1000 | Loss: 0.00002621
Iteration 4/1000 | Loss: 0.00001835
Iteration 5/1000 | Loss: 0.00001677
Iteration 6/1000 | Loss: 0.00001577
Iteration 7/1000 | Loss: 0.00001508
Iteration 8/1000 | Loss: 0.00001456
Iteration 9/1000 | Loss: 0.00001420
Iteration 10/1000 | Loss: 0.00001391
Iteration 11/1000 | Loss: 0.00001370
Iteration 12/1000 | Loss: 0.00001356
Iteration 13/1000 | Loss: 0.00001349
Iteration 14/1000 | Loss: 0.00001343
Iteration 15/1000 | Loss: 0.00001335
Iteration 16/1000 | Loss: 0.00001333
Iteration 17/1000 | Loss: 0.00001330
Iteration 18/1000 | Loss: 0.00001330
Iteration 19/1000 | Loss: 0.00001328
Iteration 20/1000 | Loss: 0.00001327
Iteration 21/1000 | Loss: 0.00001325
Iteration 22/1000 | Loss: 0.00001325
Iteration 23/1000 | Loss: 0.00001320
Iteration 24/1000 | Loss: 0.00001318
Iteration 25/1000 | Loss: 0.00001317
Iteration 26/1000 | Loss: 0.00001317
Iteration 27/1000 | Loss: 0.00001316
Iteration 28/1000 | Loss: 0.00001316
Iteration 29/1000 | Loss: 0.00001316
Iteration 30/1000 | Loss: 0.00001316
Iteration 31/1000 | Loss: 0.00001315
Iteration 32/1000 | Loss: 0.00001315
Iteration 33/1000 | Loss: 0.00001315
Iteration 34/1000 | Loss: 0.00001315
Iteration 35/1000 | Loss: 0.00001315
Iteration 36/1000 | Loss: 0.00001315
Iteration 37/1000 | Loss: 0.00001315
Iteration 38/1000 | Loss: 0.00001315
Iteration 39/1000 | Loss: 0.00001315
Iteration 40/1000 | Loss: 0.00001314
Iteration 41/1000 | Loss: 0.00001314
Iteration 42/1000 | Loss: 0.00001314
Iteration 43/1000 | Loss: 0.00001314
Iteration 44/1000 | Loss: 0.00001314
Iteration 45/1000 | Loss: 0.00001314
Iteration 46/1000 | Loss: 0.00001313
Iteration 47/1000 | Loss: 0.00001313
Iteration 48/1000 | Loss: 0.00001313
Iteration 49/1000 | Loss: 0.00001313
Iteration 50/1000 | Loss: 0.00001313
Iteration 51/1000 | Loss: 0.00001312
Iteration 52/1000 | Loss: 0.00001312
Iteration 53/1000 | Loss: 0.00001312
Iteration 54/1000 | Loss: 0.00001312
Iteration 55/1000 | Loss: 0.00001312
Iteration 56/1000 | Loss: 0.00001312
Iteration 57/1000 | Loss: 0.00001312
Iteration 58/1000 | Loss: 0.00001312
Iteration 59/1000 | Loss: 0.00001312
Iteration 60/1000 | Loss: 0.00001312
Iteration 61/1000 | Loss: 0.00001312
Iteration 62/1000 | Loss: 0.00001312
Iteration 63/1000 | Loss: 0.00001312
Iteration 64/1000 | Loss: 0.00001311
Iteration 65/1000 | Loss: 0.00001311
Iteration 66/1000 | Loss: 0.00001311
Iteration 67/1000 | Loss: 0.00001311
Iteration 68/1000 | Loss: 0.00001311
Iteration 69/1000 | Loss: 0.00001311
Iteration 70/1000 | Loss: 0.00001311
Iteration 71/1000 | Loss: 0.00001311
Iteration 72/1000 | Loss: 0.00001311
Iteration 73/1000 | Loss: 0.00001311
Iteration 74/1000 | Loss: 0.00001311
Iteration 75/1000 | Loss: 0.00001311
Iteration 76/1000 | Loss: 0.00001310
Iteration 77/1000 | Loss: 0.00001310
Iteration 78/1000 | Loss: 0.00001310
Iteration 79/1000 | Loss: 0.00001310
Iteration 80/1000 | Loss: 0.00001310
Iteration 81/1000 | Loss: 0.00001310
Iteration 82/1000 | Loss: 0.00001310
Iteration 83/1000 | Loss: 0.00001310
Iteration 84/1000 | Loss: 0.00001310
Iteration 85/1000 | Loss: 0.00001310
Iteration 86/1000 | Loss: 0.00001309
Iteration 87/1000 | Loss: 0.00001309
Iteration 88/1000 | Loss: 0.00001309
Iteration 89/1000 | Loss: 0.00001309
Iteration 90/1000 | Loss: 0.00001309
Iteration 91/1000 | Loss: 0.00001309
Iteration 92/1000 | Loss: 0.00001309
Iteration 93/1000 | Loss: 0.00001309
Iteration 94/1000 | Loss: 0.00001309
Iteration 95/1000 | Loss: 0.00001309
Iteration 96/1000 | Loss: 0.00001309
Iteration 97/1000 | Loss: 0.00001309
Iteration 98/1000 | Loss: 0.00001309
Iteration 99/1000 | Loss: 0.00001309
Iteration 100/1000 | Loss: 0.00001309
Iteration 101/1000 | Loss: 0.00001309
Iteration 102/1000 | Loss: 0.00001309
Iteration 103/1000 | Loss: 0.00001309
Iteration 104/1000 | Loss: 0.00001309
Iteration 105/1000 | Loss: 0.00001309
Iteration 106/1000 | Loss: 0.00001309
Iteration 107/1000 | Loss: 0.00001309
Iteration 108/1000 | Loss: 0.00001309
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 108. Stopping optimization.
Last 5 losses: [1.3094713722239248e-05, 1.3094713722239248e-05, 1.3094713722239248e-05, 1.3094713722239248e-05, 1.3094713722239248e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3094713722239248e-05

Optimization complete. Final v2v error: 3.1005232334136963 mm

Highest mean error: 3.4967286586761475 mm for frame 83

Lowest mean error: 2.729245185852051 mm for frame 4

Saving results

Total time: 36.42405295372009
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_023/1069/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_023/1069.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_023/1069
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00991177
Iteration 2/25 | Loss: 0.00295846
Iteration 3/25 | Loss: 0.00186054
Iteration 4/25 | Loss: 0.00177234
Iteration 5/25 | Loss: 0.00164869
Iteration 6/25 | Loss: 0.00155897
Iteration 7/25 | Loss: 0.00155687
Iteration 8/25 | Loss: 0.00150792
Iteration 9/25 | Loss: 0.00146212
Iteration 10/25 | Loss: 0.00140264
Iteration 11/25 | Loss: 0.00139380
Iteration 12/25 | Loss: 0.00138921
Iteration 13/25 | Loss: 0.00138579
Iteration 14/25 | Loss: 0.00138889
Iteration 15/25 | Loss: 0.00137861
Iteration 16/25 | Loss: 0.00138015
Iteration 17/25 | Loss: 0.00137256
Iteration 18/25 | Loss: 0.00138350
Iteration 19/25 | Loss: 0.00137639
Iteration 20/25 | Loss: 0.00136527
Iteration 21/25 | Loss: 0.00135651
Iteration 22/25 | Loss: 0.00135483
Iteration 23/25 | Loss: 0.00135460
Iteration 24/25 | Loss: 0.00135456
Iteration 25/25 | Loss: 0.00135456

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35614383
Iteration 2/25 | Loss: 0.00258953
Iteration 3/25 | Loss: 0.00257116
Iteration 4/25 | Loss: 0.00257116
Iteration 5/25 | Loss: 0.00257116
Iteration 6/25 | Loss: 0.00257116
Iteration 7/25 | Loss: 0.00257116
Iteration 8/25 | Loss: 0.00257116
Iteration 9/25 | Loss: 0.00257116
Iteration 10/25 | Loss: 0.00257116
Iteration 11/25 | Loss: 0.00257116
Iteration 12/25 | Loss: 0.00257116
Iteration 13/25 | Loss: 0.00257116
Iteration 14/25 | Loss: 0.00257116
Iteration 15/25 | Loss: 0.00257116
Iteration 16/25 | Loss: 0.00257116
Iteration 17/25 | Loss: 0.00257116
Iteration 18/25 | Loss: 0.00257116
Iteration 19/25 | Loss: 0.00257116
Iteration 20/25 | Loss: 0.00257116
Iteration 21/25 | Loss: 0.00257116
Iteration 22/25 | Loss: 0.00257116
Iteration 23/25 | Loss: 0.00257116
Iteration 24/25 | Loss: 0.00257116
Iteration 25/25 | Loss: 0.00257116

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00257116
Iteration 2/1000 | Loss: 0.00036689
Iteration 3/1000 | Loss: 0.00027750
Iteration 4/1000 | Loss: 0.00088367
Iteration 5/1000 | Loss: 0.00023948
Iteration 6/1000 | Loss: 0.00039978
Iteration 7/1000 | Loss: 0.00016704
Iteration 8/1000 | Loss: 0.00045152
Iteration 9/1000 | Loss: 0.00040371
Iteration 10/1000 | Loss: 0.00040007
Iteration 11/1000 | Loss: 0.00021948
Iteration 12/1000 | Loss: 0.00019225
Iteration 13/1000 | Loss: 0.00031572
Iteration 14/1000 | Loss: 0.00086247
Iteration 15/1000 | Loss: 0.00327061
Iteration 16/1000 | Loss: 0.00044344
Iteration 17/1000 | Loss: 0.00054277
Iteration 18/1000 | Loss: 0.00078137
Iteration 19/1000 | Loss: 0.00039221
Iteration 20/1000 | Loss: 0.00029330
Iteration 21/1000 | Loss: 0.00031535
Iteration 22/1000 | Loss: 0.00081200
Iteration 23/1000 | Loss: 0.00057449
Iteration 24/1000 | Loss: 0.00036880
Iteration 25/1000 | Loss: 0.00023410
Iteration 26/1000 | Loss: 0.00010007
Iteration 27/1000 | Loss: 0.00068904
Iteration 28/1000 | Loss: 0.00008710
Iteration 29/1000 | Loss: 0.00012264
Iteration 30/1000 | Loss: 0.00009015
Iteration 31/1000 | Loss: 0.00006880
Iteration 32/1000 | Loss: 0.00008723
Iteration 33/1000 | Loss: 0.00054456
Iteration 34/1000 | Loss: 0.00301669
Iteration 35/1000 | Loss: 0.00057868
Iteration 36/1000 | Loss: 0.00012737
Iteration 37/1000 | Loss: 0.00022354
Iteration 38/1000 | Loss: 0.00032522
Iteration 39/1000 | Loss: 0.00007778
Iteration 40/1000 | Loss: 0.00015399
Iteration 41/1000 | Loss: 0.00010859
Iteration 42/1000 | Loss: 0.00009745
Iteration 43/1000 | Loss: 0.00009404
Iteration 44/1000 | Loss: 0.00003368
Iteration 45/1000 | Loss: 0.00012521
Iteration 46/1000 | Loss: 0.00077298
Iteration 47/1000 | Loss: 0.00004941
Iteration 48/1000 | Loss: 0.00003514
Iteration 49/1000 | Loss: 0.00004365
Iteration 50/1000 | Loss: 0.00011022
Iteration 51/1000 | Loss: 0.00004737
Iteration 52/1000 | Loss: 0.00002820
Iteration 53/1000 | Loss: 0.00004557
Iteration 54/1000 | Loss: 0.00027075
Iteration 55/1000 | Loss: 0.00006920
Iteration 56/1000 | Loss: 0.00002765
Iteration 57/1000 | Loss: 0.00006730
Iteration 58/1000 | Loss: 0.00007184
Iteration 59/1000 | Loss: 0.00005903
Iteration 60/1000 | Loss: 0.00004462
Iteration 61/1000 | Loss: 0.00003976
Iteration 62/1000 | Loss: 0.00004645
Iteration 63/1000 | Loss: 0.00002849
Iteration 64/1000 | Loss: 0.00002691
Iteration 65/1000 | Loss: 0.00005929
Iteration 66/1000 | Loss: 0.00002707
Iteration 67/1000 | Loss: 0.00005267
Iteration 68/1000 | Loss: 0.00006519
Iteration 69/1000 | Loss: 0.00002647
Iteration 70/1000 | Loss: 0.00004891
Iteration 71/1000 | Loss: 0.00002598
Iteration 72/1000 | Loss: 0.00010809
Iteration 73/1000 | Loss: 0.00002846
Iteration 74/1000 | Loss: 0.00002812
Iteration 75/1000 | Loss: 0.00002812
Iteration 76/1000 | Loss: 0.00002876
Iteration 77/1000 | Loss: 0.00002554
Iteration 78/1000 | Loss: 0.00002554
Iteration 79/1000 | Loss: 0.00002554
Iteration 80/1000 | Loss: 0.00002553
Iteration 81/1000 | Loss: 0.00002553
Iteration 82/1000 | Loss: 0.00002553
Iteration 83/1000 | Loss: 0.00002553
Iteration 84/1000 | Loss: 0.00002553
Iteration 85/1000 | Loss: 0.00002553
Iteration 86/1000 | Loss: 0.00002987
Iteration 87/1000 | Loss: 0.00002707
Iteration 88/1000 | Loss: 0.00002548
Iteration 89/1000 | Loss: 0.00002547
Iteration 90/1000 | Loss: 0.00002547
Iteration 91/1000 | Loss: 0.00002547
Iteration 92/1000 | Loss: 0.00004138
Iteration 93/1000 | Loss: 0.00002548
Iteration 94/1000 | Loss: 0.00002548
Iteration 95/1000 | Loss: 0.00002545
Iteration 96/1000 | Loss: 0.00002545
Iteration 97/1000 | Loss: 0.00002545
Iteration 98/1000 | Loss: 0.00002545
Iteration 99/1000 | Loss: 0.00002545
Iteration 100/1000 | Loss: 0.00002545
Iteration 101/1000 | Loss: 0.00002545
Iteration 102/1000 | Loss: 0.00002545
Iteration 103/1000 | Loss: 0.00002545
Iteration 104/1000 | Loss: 0.00002545
Iteration 105/1000 | Loss: 0.00002545
Iteration 106/1000 | Loss: 0.00002544
Iteration 107/1000 | Loss: 0.00002544
Iteration 108/1000 | Loss: 0.00002544
Iteration 109/1000 | Loss: 0.00002544
Iteration 110/1000 | Loss: 0.00002543
Iteration 111/1000 | Loss: 0.00002543
Iteration 112/1000 | Loss: 0.00002543
Iteration 113/1000 | Loss: 0.00002543
Iteration 114/1000 | Loss: 0.00002542
Iteration 115/1000 | Loss: 0.00002542
Iteration 116/1000 | Loss: 0.00002542
Iteration 117/1000 | Loss: 0.00002542
Iteration 118/1000 | Loss: 0.00002542
Iteration 119/1000 | Loss: 0.00002542
Iteration 120/1000 | Loss: 0.00002542
Iteration 121/1000 | Loss: 0.00002542
Iteration 122/1000 | Loss: 0.00002542
Iteration 123/1000 | Loss: 0.00002541
Iteration 124/1000 | Loss: 0.00002541
Iteration 125/1000 | Loss: 0.00002541
Iteration 126/1000 | Loss: 0.00002541
Iteration 127/1000 | Loss: 0.00002541
Iteration 128/1000 | Loss: 0.00002541
Iteration 129/1000 | Loss: 0.00002540
Iteration 130/1000 | Loss: 0.00002540
Iteration 131/1000 | Loss: 0.00002540
Iteration 132/1000 | Loss: 0.00002540
Iteration 133/1000 | Loss: 0.00002540
Iteration 134/1000 | Loss: 0.00002540
Iteration 135/1000 | Loss: 0.00002540
Iteration 136/1000 | Loss: 0.00002539
Iteration 137/1000 | Loss: 0.00002539
Iteration 138/1000 | Loss: 0.00002539
Iteration 139/1000 | Loss: 0.00002538
Iteration 140/1000 | Loss: 0.00002538
Iteration 141/1000 | Loss: 0.00002538
Iteration 142/1000 | Loss: 0.00002538
Iteration 143/1000 | Loss: 0.00002538
Iteration 144/1000 | Loss: 0.00004572
Iteration 145/1000 | Loss: 0.00002540
Iteration 146/1000 | Loss: 0.00002535
Iteration 147/1000 | Loss: 0.00002535
Iteration 148/1000 | Loss: 0.00002535
Iteration 149/1000 | Loss: 0.00002535
Iteration 150/1000 | Loss: 0.00002534
Iteration 151/1000 | Loss: 0.00002534
Iteration 152/1000 | Loss: 0.00002534
Iteration 153/1000 | Loss: 0.00002534
Iteration 154/1000 | Loss: 0.00002534
Iteration 155/1000 | Loss: 0.00002533
Iteration 156/1000 | Loss: 0.00003937
Iteration 157/1000 | Loss: 0.00002533
Iteration 158/1000 | Loss: 0.00002531
Iteration 159/1000 | Loss: 0.00002530
Iteration 160/1000 | Loss: 0.00002530
Iteration 161/1000 | Loss: 0.00002530
Iteration 162/1000 | Loss: 0.00002530
Iteration 163/1000 | Loss: 0.00002530
Iteration 164/1000 | Loss: 0.00002530
Iteration 165/1000 | Loss: 0.00002530
Iteration 166/1000 | Loss: 0.00002530
Iteration 167/1000 | Loss: 0.00002530
Iteration 168/1000 | Loss: 0.00002530
Iteration 169/1000 | Loss: 0.00002530
Iteration 170/1000 | Loss: 0.00002529
Iteration 171/1000 | Loss: 0.00002529
Iteration 172/1000 | Loss: 0.00002529
Iteration 173/1000 | Loss: 0.00002529
Iteration 174/1000 | Loss: 0.00002529
Iteration 175/1000 | Loss: 0.00002529
Iteration 176/1000 | Loss: 0.00002529
Iteration 177/1000 | Loss: 0.00002529
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 177. Stopping optimization.
Last 5 losses: [2.529385528760031e-05, 2.529385528760031e-05, 2.529385528760031e-05, 2.529385528760031e-05, 2.529385528760031e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.529385528760031e-05

Optimization complete. Final v2v error: 3.7005441188812256 mm

Highest mean error: 10.610719680786133 mm for frame 168

Lowest mean error: 3.4509057998657227 mm for frame 8

Saving results

Total time: 183.40937495231628
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_023/1021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_023/1021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_023/1021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00761210
Iteration 2/25 | Loss: 0.00138301
Iteration 3/25 | Loss: 0.00123836
Iteration 4/25 | Loss: 0.00123348
Iteration 5/25 | Loss: 0.00123244
Iteration 6/25 | Loss: 0.00123244
Iteration 7/25 | Loss: 0.00123244
Iteration 8/25 | Loss: 0.00123244
Iteration 9/25 | Loss: 0.00123244
Iteration 10/25 | Loss: 0.00123244
Iteration 11/25 | Loss: 0.00123244
Iteration 12/25 | Loss: 0.00123244
Iteration 13/25 | Loss: 0.00123244
Iteration 14/25 | Loss: 0.00123244
Iteration 15/25 | Loss: 0.00123244
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0012324405834078789, 0.0012324405834078789, 0.0012324405834078789, 0.0012324405834078789, 0.0012324405834078789]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012324405834078789

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.27761507
Iteration 2/25 | Loss: 0.00070916
Iteration 3/25 | Loss: 0.00070914
Iteration 4/25 | Loss: 0.00070914
Iteration 5/25 | Loss: 0.00070914
Iteration 6/25 | Loss: 0.00070914
Iteration 7/25 | Loss: 0.00070914
Iteration 8/25 | Loss: 0.00070914
Iteration 9/25 | Loss: 0.00070914
Iteration 10/25 | Loss: 0.00070914
Iteration 11/25 | Loss: 0.00070914
Iteration 12/25 | Loss: 0.00070914
Iteration 13/25 | Loss: 0.00070914
Iteration 14/25 | Loss: 0.00070914
Iteration 15/25 | Loss: 0.00070914
Iteration 16/25 | Loss: 0.00070914
Iteration 17/25 | Loss: 0.00070914
Iteration 18/25 | Loss: 0.00070914
Iteration 19/25 | Loss: 0.00070914
Iteration 20/25 | Loss: 0.00070914
Iteration 21/25 | Loss: 0.00070914
Iteration 22/25 | Loss: 0.00070914
Iteration 23/25 | Loss: 0.00070914
Iteration 24/25 | Loss: 0.00070914
Iteration 25/25 | Loss: 0.00070914

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00070914
Iteration 2/1000 | Loss: 0.00003382
Iteration 3/1000 | Loss: 0.00002333
Iteration 4/1000 | Loss: 0.00002079
Iteration 5/1000 | Loss: 0.00001960
Iteration 6/1000 | Loss: 0.00001901
Iteration 7/1000 | Loss: 0.00001857
Iteration 8/1000 | Loss: 0.00001818
Iteration 9/1000 | Loss: 0.00001793
Iteration 10/1000 | Loss: 0.00001769
Iteration 11/1000 | Loss: 0.00001755
Iteration 12/1000 | Loss: 0.00001751
Iteration 13/1000 | Loss: 0.00001748
Iteration 14/1000 | Loss: 0.00001744
Iteration 15/1000 | Loss: 0.00001743
Iteration 16/1000 | Loss: 0.00001741
Iteration 17/1000 | Loss: 0.00001741
Iteration 18/1000 | Loss: 0.00001733
Iteration 19/1000 | Loss: 0.00001731
Iteration 20/1000 | Loss: 0.00001727
Iteration 21/1000 | Loss: 0.00001727
Iteration 22/1000 | Loss: 0.00001725
Iteration 23/1000 | Loss: 0.00001725
Iteration 24/1000 | Loss: 0.00001724
Iteration 25/1000 | Loss: 0.00001724
Iteration 26/1000 | Loss: 0.00001724
Iteration 27/1000 | Loss: 0.00001724
Iteration 28/1000 | Loss: 0.00001723
Iteration 29/1000 | Loss: 0.00001722
Iteration 30/1000 | Loss: 0.00001719
Iteration 31/1000 | Loss: 0.00001719
Iteration 32/1000 | Loss: 0.00001719
Iteration 33/1000 | Loss: 0.00001718
Iteration 34/1000 | Loss: 0.00001718
Iteration 35/1000 | Loss: 0.00001718
Iteration 36/1000 | Loss: 0.00001718
Iteration 37/1000 | Loss: 0.00001718
Iteration 38/1000 | Loss: 0.00001718
Iteration 39/1000 | Loss: 0.00001718
Iteration 40/1000 | Loss: 0.00001718
Iteration 41/1000 | Loss: 0.00001718
Iteration 42/1000 | Loss: 0.00001718
Iteration 43/1000 | Loss: 0.00001717
Iteration 44/1000 | Loss: 0.00001717
Iteration 45/1000 | Loss: 0.00001717
Iteration 46/1000 | Loss: 0.00001714
Iteration 47/1000 | Loss: 0.00001714
Iteration 48/1000 | Loss: 0.00001714
Iteration 49/1000 | Loss: 0.00001714
Iteration 50/1000 | Loss: 0.00001713
Iteration 51/1000 | Loss: 0.00001713
Iteration 52/1000 | Loss: 0.00001713
Iteration 53/1000 | Loss: 0.00001711
Iteration 54/1000 | Loss: 0.00001711
Iteration 55/1000 | Loss: 0.00001710
Iteration 56/1000 | Loss: 0.00001709
Iteration 57/1000 | Loss: 0.00001709
Iteration 58/1000 | Loss: 0.00001708
Iteration 59/1000 | Loss: 0.00001708
Iteration 60/1000 | Loss: 0.00001708
Iteration 61/1000 | Loss: 0.00001707
Iteration 62/1000 | Loss: 0.00001707
Iteration 63/1000 | Loss: 0.00001707
Iteration 64/1000 | Loss: 0.00001707
Iteration 65/1000 | Loss: 0.00001707
Iteration 66/1000 | Loss: 0.00001707
Iteration 67/1000 | Loss: 0.00001707
Iteration 68/1000 | Loss: 0.00001707
Iteration 69/1000 | Loss: 0.00001706
Iteration 70/1000 | Loss: 0.00001706
Iteration 71/1000 | Loss: 0.00001705
Iteration 72/1000 | Loss: 0.00001705
Iteration 73/1000 | Loss: 0.00001705
Iteration 74/1000 | Loss: 0.00001704
Iteration 75/1000 | Loss: 0.00001704
Iteration 76/1000 | Loss: 0.00001704
Iteration 77/1000 | Loss: 0.00001704
Iteration 78/1000 | Loss: 0.00001704
Iteration 79/1000 | Loss: 0.00001702
Iteration 80/1000 | Loss: 0.00001702
Iteration 81/1000 | Loss: 0.00001701
Iteration 82/1000 | Loss: 0.00001701
Iteration 83/1000 | Loss: 0.00001701
Iteration 84/1000 | Loss: 0.00001697
Iteration 85/1000 | Loss: 0.00001697
Iteration 86/1000 | Loss: 0.00001697
Iteration 87/1000 | Loss: 0.00001697
Iteration 88/1000 | Loss: 0.00001697
Iteration 89/1000 | Loss: 0.00001697
Iteration 90/1000 | Loss: 0.00001697
Iteration 91/1000 | Loss: 0.00001697
Iteration 92/1000 | Loss: 0.00001697
Iteration 93/1000 | Loss: 0.00001697
Iteration 94/1000 | Loss: 0.00001697
Iteration 95/1000 | Loss: 0.00001697
Iteration 96/1000 | Loss: 0.00001696
Iteration 97/1000 | Loss: 0.00001696
Iteration 98/1000 | Loss: 0.00001696
Iteration 99/1000 | Loss: 0.00001695
Iteration 100/1000 | Loss: 0.00001695
Iteration 101/1000 | Loss: 0.00001695
Iteration 102/1000 | Loss: 0.00001694
Iteration 103/1000 | Loss: 0.00001694
Iteration 104/1000 | Loss: 0.00001694
Iteration 105/1000 | Loss: 0.00001694
Iteration 106/1000 | Loss: 0.00001694
Iteration 107/1000 | Loss: 0.00001694
Iteration 108/1000 | Loss: 0.00001694
Iteration 109/1000 | Loss: 0.00001694
Iteration 110/1000 | Loss: 0.00001694
Iteration 111/1000 | Loss: 0.00001694
Iteration 112/1000 | Loss: 0.00001694
Iteration 113/1000 | Loss: 0.00001693
Iteration 114/1000 | Loss: 0.00001693
Iteration 115/1000 | Loss: 0.00001693
Iteration 116/1000 | Loss: 0.00001693
Iteration 117/1000 | Loss: 0.00001693
Iteration 118/1000 | Loss: 0.00001693
Iteration 119/1000 | Loss: 0.00001693
Iteration 120/1000 | Loss: 0.00001693
Iteration 121/1000 | Loss: 0.00001693
Iteration 122/1000 | Loss: 0.00001693
Iteration 123/1000 | Loss: 0.00001693
Iteration 124/1000 | Loss: 0.00001693
Iteration 125/1000 | Loss: 0.00001693
Iteration 126/1000 | Loss: 0.00001693
Iteration 127/1000 | Loss: 0.00001693
Iteration 128/1000 | Loss: 0.00001693
Iteration 129/1000 | Loss: 0.00001693
Iteration 130/1000 | Loss: 0.00001693
Iteration 131/1000 | Loss: 0.00001693
Iteration 132/1000 | Loss: 0.00001693
Iteration 133/1000 | Loss: 0.00001693
Iteration 134/1000 | Loss: 0.00001693
Iteration 135/1000 | Loss: 0.00001693
Iteration 136/1000 | Loss: 0.00001693
Iteration 137/1000 | Loss: 0.00001693
Iteration 138/1000 | Loss: 0.00001693
Iteration 139/1000 | Loss: 0.00001693
Iteration 140/1000 | Loss: 0.00001693
Iteration 141/1000 | Loss: 0.00001693
Iteration 142/1000 | Loss: 0.00001693
Iteration 143/1000 | Loss: 0.00001693
Iteration 144/1000 | Loss: 0.00001693
Iteration 145/1000 | Loss: 0.00001693
Iteration 146/1000 | Loss: 0.00001693
Iteration 147/1000 | Loss: 0.00001693
Iteration 148/1000 | Loss: 0.00001693
Iteration 149/1000 | Loss: 0.00001693
Iteration 150/1000 | Loss: 0.00001693
Iteration 151/1000 | Loss: 0.00001693
Iteration 152/1000 | Loss: 0.00001693
Iteration 153/1000 | Loss: 0.00001693
Iteration 154/1000 | Loss: 0.00001693
Iteration 155/1000 | Loss: 0.00001693
Iteration 156/1000 | Loss: 0.00001693
Iteration 157/1000 | Loss: 0.00001693
Iteration 158/1000 | Loss: 0.00001693
Iteration 159/1000 | Loss: 0.00001693
Iteration 160/1000 | Loss: 0.00001693
Iteration 161/1000 | Loss: 0.00001693
Iteration 162/1000 | Loss: 0.00001693
Iteration 163/1000 | Loss: 0.00001693
Iteration 164/1000 | Loss: 0.00001693
Iteration 165/1000 | Loss: 0.00001693
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 165. Stopping optimization.
Last 5 losses: [1.692898695182521e-05, 1.692898695182521e-05, 1.692898695182521e-05, 1.692898695182521e-05, 1.692898695182521e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.692898695182521e-05

Optimization complete. Final v2v error: 3.4662628173828125 mm

Highest mean error: 3.6830952167510986 mm for frame 30

Lowest mean error: 3.334550142288208 mm for frame 38

Saving results

Total time: 35.75078845024109
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_023/1027/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_023/1027.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_023/1027
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00841980
Iteration 2/25 | Loss: 0.00136393
Iteration 3/25 | Loss: 0.00114776
Iteration 4/25 | Loss: 0.00112548
Iteration 5/25 | Loss: 0.00111966
Iteration 6/25 | Loss: 0.00111883
Iteration 7/25 | Loss: 0.00111883
Iteration 8/25 | Loss: 0.00111883
Iteration 9/25 | Loss: 0.00111883
Iteration 10/25 | Loss: 0.00111883
Iteration 11/25 | Loss: 0.00111883
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001118827029131353, 0.001118827029131353, 0.001118827029131353, 0.001118827029131353, 0.001118827029131353]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001118827029131353

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.69372082
Iteration 2/25 | Loss: 0.00084927
Iteration 3/25 | Loss: 0.00084927
Iteration 4/25 | Loss: 0.00084927
Iteration 5/25 | Loss: 0.00084927
Iteration 6/25 | Loss: 0.00084927
Iteration 7/25 | Loss: 0.00084927
Iteration 8/25 | Loss: 0.00084927
Iteration 9/25 | Loss: 0.00084927
Iteration 10/25 | Loss: 0.00084927
Iteration 11/25 | Loss: 0.00084927
Iteration 12/25 | Loss: 0.00084927
Iteration 13/25 | Loss: 0.00084927
Iteration 14/25 | Loss: 0.00084927
Iteration 15/25 | Loss: 0.00084927
Iteration 16/25 | Loss: 0.00084927
Iteration 17/25 | Loss: 0.00084927
Iteration 18/25 | Loss: 0.00084927
Iteration 19/25 | Loss: 0.00084927
Iteration 20/25 | Loss: 0.00084927
Iteration 21/25 | Loss: 0.00084927
Iteration 22/25 | Loss: 0.00084927
Iteration 23/25 | Loss: 0.00084927
Iteration 24/25 | Loss: 0.00084927
Iteration 25/25 | Loss: 0.00084927

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00084927
Iteration 2/1000 | Loss: 0.00002195
Iteration 3/1000 | Loss: 0.00001556
Iteration 4/1000 | Loss: 0.00001376
Iteration 5/1000 | Loss: 0.00001305
Iteration 6/1000 | Loss: 0.00001245
Iteration 7/1000 | Loss: 0.00001216
Iteration 8/1000 | Loss: 0.00001187
Iteration 9/1000 | Loss: 0.00001161
Iteration 10/1000 | Loss: 0.00001145
Iteration 11/1000 | Loss: 0.00001145
Iteration 12/1000 | Loss: 0.00001138
Iteration 13/1000 | Loss: 0.00001138
Iteration 14/1000 | Loss: 0.00001134
Iteration 15/1000 | Loss: 0.00001134
Iteration 16/1000 | Loss: 0.00001133
Iteration 17/1000 | Loss: 0.00001132
Iteration 18/1000 | Loss: 0.00001131
Iteration 19/1000 | Loss: 0.00001130
Iteration 20/1000 | Loss: 0.00001129
Iteration 21/1000 | Loss: 0.00001122
Iteration 22/1000 | Loss: 0.00001121
Iteration 23/1000 | Loss: 0.00001119
Iteration 24/1000 | Loss: 0.00001118
Iteration 25/1000 | Loss: 0.00001117
Iteration 26/1000 | Loss: 0.00001116
Iteration 27/1000 | Loss: 0.00001114
Iteration 28/1000 | Loss: 0.00001114
Iteration 29/1000 | Loss: 0.00001113
Iteration 30/1000 | Loss: 0.00001113
Iteration 31/1000 | Loss: 0.00001108
Iteration 32/1000 | Loss: 0.00001107
Iteration 33/1000 | Loss: 0.00001106
Iteration 34/1000 | Loss: 0.00001106
Iteration 35/1000 | Loss: 0.00001105
Iteration 36/1000 | Loss: 0.00001102
Iteration 37/1000 | Loss: 0.00001102
Iteration 38/1000 | Loss: 0.00001101
Iteration 39/1000 | Loss: 0.00001101
Iteration 40/1000 | Loss: 0.00001099
Iteration 41/1000 | Loss: 0.00001098
Iteration 42/1000 | Loss: 0.00001097
Iteration 43/1000 | Loss: 0.00001096
Iteration 44/1000 | Loss: 0.00001090
Iteration 45/1000 | Loss: 0.00001090
Iteration 46/1000 | Loss: 0.00001089
Iteration 47/1000 | Loss: 0.00001088
Iteration 48/1000 | Loss: 0.00001087
Iteration 49/1000 | Loss: 0.00001086
Iteration 50/1000 | Loss: 0.00001086
Iteration 51/1000 | Loss: 0.00001086
Iteration 52/1000 | Loss: 0.00001085
Iteration 53/1000 | Loss: 0.00001085
Iteration 54/1000 | Loss: 0.00001085
Iteration 55/1000 | Loss: 0.00001085
Iteration 56/1000 | Loss: 0.00001085
Iteration 57/1000 | Loss: 0.00001084
Iteration 58/1000 | Loss: 0.00001084
Iteration 59/1000 | Loss: 0.00001084
Iteration 60/1000 | Loss: 0.00001084
Iteration 61/1000 | Loss: 0.00001084
Iteration 62/1000 | Loss: 0.00001084
Iteration 63/1000 | Loss: 0.00001084
Iteration 64/1000 | Loss: 0.00001084
Iteration 65/1000 | Loss: 0.00001083
Iteration 66/1000 | Loss: 0.00001083
Iteration 67/1000 | Loss: 0.00001082
Iteration 68/1000 | Loss: 0.00001082
Iteration 69/1000 | Loss: 0.00001082
Iteration 70/1000 | Loss: 0.00001081
Iteration 71/1000 | Loss: 0.00001081
Iteration 72/1000 | Loss: 0.00001080
Iteration 73/1000 | Loss: 0.00001080
Iteration 74/1000 | Loss: 0.00001080
Iteration 75/1000 | Loss: 0.00001079
Iteration 76/1000 | Loss: 0.00001079
Iteration 77/1000 | Loss: 0.00001078
Iteration 78/1000 | Loss: 0.00001078
Iteration 79/1000 | Loss: 0.00001078
Iteration 80/1000 | Loss: 0.00001077
Iteration 81/1000 | Loss: 0.00001077
Iteration 82/1000 | Loss: 0.00001077
Iteration 83/1000 | Loss: 0.00001076
Iteration 84/1000 | Loss: 0.00001076
Iteration 85/1000 | Loss: 0.00001076
Iteration 86/1000 | Loss: 0.00001076
Iteration 87/1000 | Loss: 0.00001075
Iteration 88/1000 | Loss: 0.00001075
Iteration 89/1000 | Loss: 0.00001075
Iteration 90/1000 | Loss: 0.00001074
Iteration 91/1000 | Loss: 0.00001074
Iteration 92/1000 | Loss: 0.00001074
Iteration 93/1000 | Loss: 0.00001074
Iteration 94/1000 | Loss: 0.00001073
Iteration 95/1000 | Loss: 0.00001072
Iteration 96/1000 | Loss: 0.00001072
Iteration 97/1000 | Loss: 0.00001071
Iteration 98/1000 | Loss: 0.00001071
Iteration 99/1000 | Loss: 0.00001070
Iteration 100/1000 | Loss: 0.00001070
Iteration 101/1000 | Loss: 0.00001070
Iteration 102/1000 | Loss: 0.00001070
Iteration 103/1000 | Loss: 0.00001069
Iteration 104/1000 | Loss: 0.00001069
Iteration 105/1000 | Loss: 0.00001068
Iteration 106/1000 | Loss: 0.00001067
Iteration 107/1000 | Loss: 0.00001067
Iteration 108/1000 | Loss: 0.00001067
Iteration 109/1000 | Loss: 0.00001067
Iteration 110/1000 | Loss: 0.00001066
Iteration 111/1000 | Loss: 0.00001066
Iteration 112/1000 | Loss: 0.00001066
Iteration 113/1000 | Loss: 0.00001066
Iteration 114/1000 | Loss: 0.00001066
Iteration 115/1000 | Loss: 0.00001065
Iteration 116/1000 | Loss: 0.00001065
Iteration 117/1000 | Loss: 0.00001065
Iteration 118/1000 | Loss: 0.00001064
Iteration 119/1000 | Loss: 0.00001064
Iteration 120/1000 | Loss: 0.00001064
Iteration 121/1000 | Loss: 0.00001064
Iteration 122/1000 | Loss: 0.00001064
Iteration 123/1000 | Loss: 0.00001063
Iteration 124/1000 | Loss: 0.00001063
Iteration 125/1000 | Loss: 0.00001063
Iteration 126/1000 | Loss: 0.00001063
Iteration 127/1000 | Loss: 0.00001063
Iteration 128/1000 | Loss: 0.00001063
Iteration 129/1000 | Loss: 0.00001063
Iteration 130/1000 | Loss: 0.00001062
Iteration 131/1000 | Loss: 0.00001062
Iteration 132/1000 | Loss: 0.00001062
Iteration 133/1000 | Loss: 0.00001062
Iteration 134/1000 | Loss: 0.00001061
Iteration 135/1000 | Loss: 0.00001061
Iteration 136/1000 | Loss: 0.00001061
Iteration 137/1000 | Loss: 0.00001061
Iteration 138/1000 | Loss: 0.00001061
Iteration 139/1000 | Loss: 0.00001061
Iteration 140/1000 | Loss: 0.00001060
Iteration 141/1000 | Loss: 0.00001060
Iteration 142/1000 | Loss: 0.00001060
Iteration 143/1000 | Loss: 0.00001059
Iteration 144/1000 | Loss: 0.00001059
Iteration 145/1000 | Loss: 0.00001059
Iteration 146/1000 | Loss: 0.00001059
Iteration 147/1000 | Loss: 0.00001059
Iteration 148/1000 | Loss: 0.00001059
Iteration 149/1000 | Loss: 0.00001059
Iteration 150/1000 | Loss: 0.00001059
Iteration 151/1000 | Loss: 0.00001059
Iteration 152/1000 | Loss: 0.00001059
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 152. Stopping optimization.
Last 5 losses: [1.0594631930871401e-05, 1.0594631930871401e-05, 1.0594631930871401e-05, 1.0594631930871401e-05, 1.0594631930871401e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0594631930871401e-05

Optimization complete. Final v2v error: 2.802717685699463 mm

Highest mean error: 3.108071804046631 mm for frame 190

Lowest mean error: 2.518644332885742 mm for frame 0

Saving results

Total time: 42.67807626724243
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_023/1004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_023/1004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_023/1004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00921783
Iteration 2/25 | Loss: 0.00176244
Iteration 3/25 | Loss: 0.00139605
Iteration 4/25 | Loss: 0.00132268
Iteration 5/25 | Loss: 0.00130431
Iteration 6/25 | Loss: 0.00129934
Iteration 7/25 | Loss: 0.00129728
Iteration 8/25 | Loss: 0.00129647
Iteration 9/25 | Loss: 0.00129597
Iteration 10/25 | Loss: 0.00129802
Iteration 11/25 | Loss: 0.00129334
Iteration 12/25 | Loss: 0.00129187
Iteration 13/25 | Loss: 0.00129117
Iteration 14/25 | Loss: 0.00129099
Iteration 15/25 | Loss: 0.00129090
Iteration 16/25 | Loss: 0.00129088
Iteration 17/25 | Loss: 0.00129088
Iteration 18/25 | Loss: 0.00129087
Iteration 19/25 | Loss: 0.00129087
Iteration 20/25 | Loss: 0.00129087
Iteration 21/25 | Loss: 0.00129087
Iteration 22/25 | Loss: 0.00129087
Iteration 23/25 | Loss: 0.00129087
Iteration 24/25 | Loss: 0.00129087
Iteration 25/25 | Loss: 0.00129087

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32254374
Iteration 2/25 | Loss: 0.00263164
Iteration 3/25 | Loss: 0.00263163
Iteration 4/25 | Loss: 0.00263163
Iteration 5/25 | Loss: 0.00263163
Iteration 6/25 | Loss: 0.00263163
Iteration 7/25 | Loss: 0.00263163
Iteration 8/25 | Loss: 0.00263163
Iteration 9/25 | Loss: 0.00263163
Iteration 10/25 | Loss: 0.00263163
Iteration 11/25 | Loss: 0.00263163
Iteration 12/25 | Loss: 0.00263163
Iteration 13/25 | Loss: 0.00263163
Iteration 14/25 | Loss: 0.00263163
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0026316260918974876, 0.0026316260918974876, 0.0026316260918974876, 0.0026316260918974876, 0.0026316260918974876]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0026316260918974876

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00263163
Iteration 2/1000 | Loss: 0.00022812
Iteration 3/1000 | Loss: 0.00015538
Iteration 4/1000 | Loss: 0.00013182
Iteration 5/1000 | Loss: 0.00012195
Iteration 6/1000 | Loss: 0.00011727
Iteration 7/1000 | Loss: 0.00011350
Iteration 8/1000 | Loss: 0.00011024
Iteration 9/1000 | Loss: 0.00010718
Iteration 10/1000 | Loss: 0.00010532
Iteration 11/1000 | Loss: 0.00010373
Iteration 12/1000 | Loss: 0.00019414
Iteration 13/1000 | Loss: 0.00016857
Iteration 14/1000 | Loss: 0.00018144
Iteration 15/1000 | Loss: 0.00010435
Iteration 16/1000 | Loss: 0.00010070
Iteration 17/1000 | Loss: 0.00009902
Iteration 18/1000 | Loss: 0.00072924
Iteration 19/1000 | Loss: 0.01182532
Iteration 20/1000 | Loss: 0.00486350
Iteration 21/1000 | Loss: 0.00029971
Iteration 22/1000 | Loss: 0.00014894
Iteration 23/1000 | Loss: 0.00009146
Iteration 24/1000 | Loss: 0.00006898
Iteration 25/1000 | Loss: 0.00005875
Iteration 26/1000 | Loss: 0.00019589
Iteration 27/1000 | Loss: 0.00004711
Iteration 28/1000 | Loss: 0.00003970
Iteration 29/1000 | Loss: 0.00003488
Iteration 30/1000 | Loss: 0.00003158
Iteration 31/1000 | Loss: 0.00002923
Iteration 32/1000 | Loss: 0.00002760
Iteration 33/1000 | Loss: 0.00002620
Iteration 34/1000 | Loss: 0.00002510
Iteration 35/1000 | Loss: 0.00002450
Iteration 36/1000 | Loss: 0.00002408
Iteration 37/1000 | Loss: 0.00002375
Iteration 38/1000 | Loss: 0.00002342
Iteration 39/1000 | Loss: 0.00002314
Iteration 40/1000 | Loss: 0.00002293
Iteration 41/1000 | Loss: 0.00002279
Iteration 42/1000 | Loss: 0.00002271
Iteration 43/1000 | Loss: 0.00002266
Iteration 44/1000 | Loss: 0.00002262
Iteration 45/1000 | Loss: 0.00002259
Iteration 46/1000 | Loss: 0.00002254
Iteration 47/1000 | Loss: 0.00002250
Iteration 48/1000 | Loss: 0.00002249
Iteration 49/1000 | Loss: 0.00002249
Iteration 50/1000 | Loss: 0.00002248
Iteration 51/1000 | Loss: 0.00002248
Iteration 52/1000 | Loss: 0.00002240
Iteration 53/1000 | Loss: 0.00002234
Iteration 54/1000 | Loss: 0.00002231
Iteration 55/1000 | Loss: 0.00002230
Iteration 56/1000 | Loss: 0.00002229
Iteration 57/1000 | Loss: 0.00002229
Iteration 58/1000 | Loss: 0.00002228
Iteration 59/1000 | Loss: 0.00002227
Iteration 60/1000 | Loss: 0.00002226
Iteration 61/1000 | Loss: 0.00002225
Iteration 62/1000 | Loss: 0.00002223
Iteration 63/1000 | Loss: 0.00002222
Iteration 64/1000 | Loss: 0.00002222
Iteration 65/1000 | Loss: 0.00002222
Iteration 66/1000 | Loss: 0.00002221
Iteration 67/1000 | Loss: 0.00002221
Iteration 68/1000 | Loss: 0.00002221
Iteration 69/1000 | Loss: 0.00002220
Iteration 70/1000 | Loss: 0.00002218
Iteration 71/1000 | Loss: 0.00002218
Iteration 72/1000 | Loss: 0.00002218
Iteration 73/1000 | Loss: 0.00002218
Iteration 74/1000 | Loss: 0.00002218
Iteration 75/1000 | Loss: 0.00002217
Iteration 76/1000 | Loss: 0.00002217
Iteration 77/1000 | Loss: 0.00002216
Iteration 78/1000 | Loss: 0.00002216
Iteration 79/1000 | Loss: 0.00002216
Iteration 80/1000 | Loss: 0.00002216
Iteration 81/1000 | Loss: 0.00002215
Iteration 82/1000 | Loss: 0.00002215
Iteration 83/1000 | Loss: 0.00002215
Iteration 84/1000 | Loss: 0.00002215
Iteration 85/1000 | Loss: 0.00002215
Iteration 86/1000 | Loss: 0.00002215
Iteration 87/1000 | Loss: 0.00002215
Iteration 88/1000 | Loss: 0.00002214
Iteration 89/1000 | Loss: 0.00002214
Iteration 90/1000 | Loss: 0.00002214
Iteration 91/1000 | Loss: 0.00002214
Iteration 92/1000 | Loss: 0.00002214
Iteration 93/1000 | Loss: 0.00002214
Iteration 94/1000 | Loss: 0.00002213
Iteration 95/1000 | Loss: 0.00002213
Iteration 96/1000 | Loss: 0.00002213
Iteration 97/1000 | Loss: 0.00002213
Iteration 98/1000 | Loss: 0.00002213
Iteration 99/1000 | Loss: 0.00002213
Iteration 100/1000 | Loss: 0.00002213
Iteration 101/1000 | Loss: 0.00002213
Iteration 102/1000 | Loss: 0.00002213
Iteration 103/1000 | Loss: 0.00002213
Iteration 104/1000 | Loss: 0.00002213
Iteration 105/1000 | Loss: 0.00002212
Iteration 106/1000 | Loss: 0.00002212
Iteration 107/1000 | Loss: 0.00002212
Iteration 108/1000 | Loss: 0.00002212
Iteration 109/1000 | Loss: 0.00002212
Iteration 110/1000 | Loss: 0.00002212
Iteration 111/1000 | Loss: 0.00002211
Iteration 112/1000 | Loss: 0.00002211
Iteration 113/1000 | Loss: 0.00002211
Iteration 114/1000 | Loss: 0.00002210
Iteration 115/1000 | Loss: 0.00002210
Iteration 116/1000 | Loss: 0.00002210
Iteration 117/1000 | Loss: 0.00002209
Iteration 118/1000 | Loss: 0.00002209
Iteration 119/1000 | Loss: 0.00002209
Iteration 120/1000 | Loss: 0.00002209
Iteration 121/1000 | Loss: 0.00002208
Iteration 122/1000 | Loss: 0.00002208
Iteration 123/1000 | Loss: 0.00002208
Iteration 124/1000 | Loss: 0.00002208
Iteration 125/1000 | Loss: 0.00002208
Iteration 126/1000 | Loss: 0.00002208
Iteration 127/1000 | Loss: 0.00002207
Iteration 128/1000 | Loss: 0.00002207
Iteration 129/1000 | Loss: 0.00002207
Iteration 130/1000 | Loss: 0.00002207
Iteration 131/1000 | Loss: 0.00002206
Iteration 132/1000 | Loss: 0.00002206
Iteration 133/1000 | Loss: 0.00002206
Iteration 134/1000 | Loss: 0.00002206
Iteration 135/1000 | Loss: 0.00002206
Iteration 136/1000 | Loss: 0.00002206
Iteration 137/1000 | Loss: 0.00002205
Iteration 138/1000 | Loss: 0.00002205
Iteration 139/1000 | Loss: 0.00002205
Iteration 140/1000 | Loss: 0.00002205
Iteration 141/1000 | Loss: 0.00002205
Iteration 142/1000 | Loss: 0.00002205
Iteration 143/1000 | Loss: 0.00002205
Iteration 144/1000 | Loss: 0.00002205
Iteration 145/1000 | Loss: 0.00002205
Iteration 146/1000 | Loss: 0.00002205
Iteration 147/1000 | Loss: 0.00002205
Iteration 148/1000 | Loss: 0.00002204
Iteration 149/1000 | Loss: 0.00002204
Iteration 150/1000 | Loss: 0.00002204
Iteration 151/1000 | Loss: 0.00002203
Iteration 152/1000 | Loss: 0.00002203
Iteration 153/1000 | Loss: 0.00002203
Iteration 154/1000 | Loss: 0.00002203
Iteration 155/1000 | Loss: 0.00002203
Iteration 156/1000 | Loss: 0.00002202
Iteration 157/1000 | Loss: 0.00002202
Iteration 158/1000 | Loss: 0.00002202
Iteration 159/1000 | Loss: 0.00002201
Iteration 160/1000 | Loss: 0.00002201
Iteration 161/1000 | Loss: 0.00002201
Iteration 162/1000 | Loss: 0.00002201
Iteration 163/1000 | Loss: 0.00002201
Iteration 164/1000 | Loss: 0.00002201
Iteration 165/1000 | Loss: 0.00002200
Iteration 166/1000 | Loss: 0.00002200
Iteration 167/1000 | Loss: 0.00002200
Iteration 168/1000 | Loss: 0.00002200
Iteration 169/1000 | Loss: 0.00002200
Iteration 170/1000 | Loss: 0.00002200
Iteration 171/1000 | Loss: 0.00002200
Iteration 172/1000 | Loss: 0.00002199
Iteration 173/1000 | Loss: 0.00002199
Iteration 174/1000 | Loss: 0.00002199
Iteration 175/1000 | Loss: 0.00002199
Iteration 176/1000 | Loss: 0.00002199
Iteration 177/1000 | Loss: 0.00002198
Iteration 178/1000 | Loss: 0.00002198
Iteration 179/1000 | Loss: 0.00002198
Iteration 180/1000 | Loss: 0.00002198
Iteration 181/1000 | Loss: 0.00002197
Iteration 182/1000 | Loss: 0.00002197
Iteration 183/1000 | Loss: 0.00002197
Iteration 184/1000 | Loss: 0.00002197
Iteration 185/1000 | Loss: 0.00002197
Iteration 186/1000 | Loss: 0.00002197
Iteration 187/1000 | Loss: 0.00002197
Iteration 188/1000 | Loss: 0.00002197
Iteration 189/1000 | Loss: 0.00002197
Iteration 190/1000 | Loss: 0.00002196
Iteration 191/1000 | Loss: 0.00002196
Iteration 192/1000 | Loss: 0.00002196
Iteration 193/1000 | Loss: 0.00002196
Iteration 194/1000 | Loss: 0.00002196
Iteration 195/1000 | Loss: 0.00002196
Iteration 196/1000 | Loss: 0.00002196
Iteration 197/1000 | Loss: 0.00002196
Iteration 198/1000 | Loss: 0.00002196
Iteration 199/1000 | Loss: 0.00002196
Iteration 200/1000 | Loss: 0.00002196
Iteration 201/1000 | Loss: 0.00002196
Iteration 202/1000 | Loss: 0.00002196
Iteration 203/1000 | Loss: 0.00002196
Iteration 204/1000 | Loss: 0.00002196
Iteration 205/1000 | Loss: 0.00002196
Iteration 206/1000 | Loss: 0.00002196
Iteration 207/1000 | Loss: 0.00002196
Iteration 208/1000 | Loss: 0.00002196
Iteration 209/1000 | Loss: 0.00002196
Iteration 210/1000 | Loss: 0.00002196
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 210. Stopping optimization.
Last 5 losses: [2.1959054720355198e-05, 2.1959054720355198e-05, 2.1959054720355198e-05, 2.1959054720355198e-05, 2.1959054720355198e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1959054720355198e-05

Optimization complete. Final v2v error: 3.8913521766662598 mm

Highest mean error: 4.236210823059082 mm for frame 8

Lowest mean error: 3.645339012145996 mm for frame 151

Saving results

Total time: 102.38588786125183
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_023/1086/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_023/1086.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_023/1086
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00947471
Iteration 2/25 | Loss: 0.00219883
Iteration 3/25 | Loss: 0.00161904
Iteration 4/25 | Loss: 0.00153490
Iteration 5/25 | Loss: 0.00146317
Iteration 6/25 | Loss: 0.00145580
Iteration 7/25 | Loss: 0.00137927
Iteration 8/25 | Loss: 0.00131532
Iteration 9/25 | Loss: 0.00128579
Iteration 10/25 | Loss: 0.00127983
Iteration 11/25 | Loss: 0.00127684
Iteration 12/25 | Loss: 0.00127347
Iteration 13/25 | Loss: 0.00127119
Iteration 14/25 | Loss: 0.00130720
Iteration 15/25 | Loss: 0.00130740
Iteration 16/25 | Loss: 0.00130274
Iteration 17/25 | Loss: 0.00128572
Iteration 18/25 | Loss: 0.00126927
Iteration 19/25 | Loss: 0.00125833
Iteration 20/25 | Loss: 0.00124947
Iteration 21/25 | Loss: 0.00124344
Iteration 22/25 | Loss: 0.00124493
Iteration 23/25 | Loss: 0.00123859
Iteration 24/25 | Loss: 0.00124225
Iteration 25/25 | Loss: 0.00124348

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35588348
Iteration 2/25 | Loss: 0.00107198
Iteration 3/25 | Loss: 0.00107198
Iteration 4/25 | Loss: 0.00107198
Iteration 5/25 | Loss: 0.00107198
Iteration 6/25 | Loss: 0.00107197
Iteration 7/25 | Loss: 0.00107197
Iteration 8/25 | Loss: 0.00107197
Iteration 9/25 | Loss: 0.00107197
Iteration 10/25 | Loss: 0.00107197
Iteration 11/25 | Loss: 0.00107197
Iteration 12/25 | Loss: 0.00107197
Iteration 13/25 | Loss: 0.00107197
Iteration 14/25 | Loss: 0.00107197
Iteration 15/25 | Loss: 0.00107197
Iteration 16/25 | Loss: 0.00107197
Iteration 17/25 | Loss: 0.00107197
Iteration 18/25 | Loss: 0.00107197
Iteration 19/25 | Loss: 0.00107197
Iteration 20/25 | Loss: 0.00107197
Iteration 21/25 | Loss: 0.00107197
Iteration 22/25 | Loss: 0.00107197
Iteration 23/25 | Loss: 0.00107197
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.00107197358738631, 0.00107197358738631, 0.00107197358738631, 0.00107197358738631, 0.00107197358738631]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00107197358738631

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00107197
Iteration 2/1000 | Loss: 0.00009437
Iteration 3/1000 | Loss: 0.00005846
Iteration 4/1000 | Loss: 0.00004626
Iteration 5/1000 | Loss: 0.00004170
Iteration 6/1000 | Loss: 0.00003973
Iteration 7/1000 | Loss: 0.00003790
Iteration 8/1000 | Loss: 0.00003663
Iteration 9/1000 | Loss: 0.00021554
Iteration 10/1000 | Loss: 0.00003800
Iteration 11/1000 | Loss: 0.00003479
Iteration 12/1000 | Loss: 0.00003307
Iteration 13/1000 | Loss: 0.00020513
Iteration 14/1000 | Loss: 0.00003779
Iteration 15/1000 | Loss: 0.00003185
Iteration 16/1000 | Loss: 0.00002917
Iteration 17/1000 | Loss: 0.00002817
Iteration 18/1000 | Loss: 0.00002720
Iteration 19/1000 | Loss: 0.00002650
Iteration 20/1000 | Loss: 0.00002602
Iteration 21/1000 | Loss: 0.00002544
Iteration 22/1000 | Loss: 0.00002496
Iteration 23/1000 | Loss: 0.00002466
Iteration 24/1000 | Loss: 0.00002434
Iteration 25/1000 | Loss: 0.00002408
Iteration 26/1000 | Loss: 0.00002399
Iteration 27/1000 | Loss: 0.00002394
Iteration 28/1000 | Loss: 0.00002392
Iteration 29/1000 | Loss: 0.00002390
Iteration 30/1000 | Loss: 0.00002389
Iteration 31/1000 | Loss: 0.00002388
Iteration 32/1000 | Loss: 0.00002378
Iteration 33/1000 | Loss: 0.00002378
Iteration 34/1000 | Loss: 0.00002375
Iteration 35/1000 | Loss: 0.00002375
Iteration 36/1000 | Loss: 0.00002374
Iteration 37/1000 | Loss: 0.00002369
Iteration 38/1000 | Loss: 0.00002368
Iteration 39/1000 | Loss: 0.00002367
Iteration 40/1000 | Loss: 0.00002364
Iteration 41/1000 | Loss: 0.00002362
Iteration 42/1000 | Loss: 0.00002361
Iteration 43/1000 | Loss: 0.00002360
Iteration 44/1000 | Loss: 0.00002359
Iteration 45/1000 | Loss: 0.00002358
Iteration 46/1000 | Loss: 0.00002358
Iteration 47/1000 | Loss: 0.00002357
Iteration 48/1000 | Loss: 0.00002357
Iteration 49/1000 | Loss: 0.00002355
Iteration 50/1000 | Loss: 0.00002353
Iteration 51/1000 | Loss: 0.00002353
Iteration 52/1000 | Loss: 0.00002353
Iteration 53/1000 | Loss: 0.00002352
Iteration 54/1000 | Loss: 0.00002352
Iteration 55/1000 | Loss: 0.00002351
Iteration 56/1000 | Loss: 0.00002350
Iteration 57/1000 | Loss: 0.00002350
Iteration 58/1000 | Loss: 0.00002350
Iteration 59/1000 | Loss: 0.00002349
Iteration 60/1000 | Loss: 0.00002349
Iteration 61/1000 | Loss: 0.00002349
Iteration 62/1000 | Loss: 0.00002348
Iteration 63/1000 | Loss: 0.00002348
Iteration 64/1000 | Loss: 0.00002347
Iteration 65/1000 | Loss: 0.00002347
Iteration 66/1000 | Loss: 0.00002347
Iteration 67/1000 | Loss: 0.00002346
Iteration 68/1000 | Loss: 0.00002346
Iteration 69/1000 | Loss: 0.00002346
Iteration 70/1000 | Loss: 0.00002345
Iteration 71/1000 | Loss: 0.00002344
Iteration 72/1000 | Loss: 0.00002344
Iteration 73/1000 | Loss: 0.00002343
Iteration 74/1000 | Loss: 0.00002343
Iteration 75/1000 | Loss: 0.00002343
Iteration 76/1000 | Loss: 0.00002342
Iteration 77/1000 | Loss: 0.00002342
Iteration 78/1000 | Loss: 0.00002341
Iteration 79/1000 | Loss: 0.00002341
Iteration 80/1000 | Loss: 0.00002341
Iteration 81/1000 | Loss: 0.00002340
Iteration 82/1000 | Loss: 0.00002339
Iteration 83/1000 | Loss: 0.00002339
Iteration 84/1000 | Loss: 0.00002339
Iteration 85/1000 | Loss: 0.00002339
Iteration 86/1000 | Loss: 0.00002339
Iteration 87/1000 | Loss: 0.00002339
Iteration 88/1000 | Loss: 0.00002339
Iteration 89/1000 | Loss: 0.00002339
Iteration 90/1000 | Loss: 0.00002339
Iteration 91/1000 | Loss: 0.00002338
Iteration 92/1000 | Loss: 0.00002338
Iteration 93/1000 | Loss: 0.00002338
Iteration 94/1000 | Loss: 0.00002337
Iteration 95/1000 | Loss: 0.00002336
Iteration 96/1000 | Loss: 0.00002336
Iteration 97/1000 | Loss: 0.00002336
Iteration 98/1000 | Loss: 0.00002336
Iteration 99/1000 | Loss: 0.00002336
Iteration 100/1000 | Loss: 0.00002335
Iteration 101/1000 | Loss: 0.00002335
Iteration 102/1000 | Loss: 0.00002335
Iteration 103/1000 | Loss: 0.00002335
Iteration 104/1000 | Loss: 0.00002335
Iteration 105/1000 | Loss: 0.00002335
Iteration 106/1000 | Loss: 0.00002335
Iteration 107/1000 | Loss: 0.00002335
Iteration 108/1000 | Loss: 0.00002334
Iteration 109/1000 | Loss: 0.00002334
Iteration 110/1000 | Loss: 0.00002334
Iteration 111/1000 | Loss: 0.00002334
Iteration 112/1000 | Loss: 0.00002334
Iteration 113/1000 | Loss: 0.00002334
Iteration 114/1000 | Loss: 0.00002334
Iteration 115/1000 | Loss: 0.00002334
Iteration 116/1000 | Loss: 0.00002333
Iteration 117/1000 | Loss: 0.00002333
Iteration 118/1000 | Loss: 0.00002333
Iteration 119/1000 | Loss: 0.00002333
Iteration 120/1000 | Loss: 0.00002333
Iteration 121/1000 | Loss: 0.00002333
Iteration 122/1000 | Loss: 0.00002332
Iteration 123/1000 | Loss: 0.00002332
Iteration 124/1000 | Loss: 0.00002332
Iteration 125/1000 | Loss: 0.00002332
Iteration 126/1000 | Loss: 0.00002332
Iteration 127/1000 | Loss: 0.00002332
Iteration 128/1000 | Loss: 0.00002332
Iteration 129/1000 | Loss: 0.00002332
Iteration 130/1000 | Loss: 0.00002331
Iteration 131/1000 | Loss: 0.00002331
Iteration 132/1000 | Loss: 0.00002331
Iteration 133/1000 | Loss: 0.00002331
Iteration 134/1000 | Loss: 0.00002330
Iteration 135/1000 | Loss: 0.00002330
Iteration 136/1000 | Loss: 0.00002330
Iteration 137/1000 | Loss: 0.00002330
Iteration 138/1000 | Loss: 0.00002330
Iteration 139/1000 | Loss: 0.00002330
Iteration 140/1000 | Loss: 0.00002329
Iteration 141/1000 | Loss: 0.00002329
Iteration 142/1000 | Loss: 0.00002329
Iteration 143/1000 | Loss: 0.00002328
Iteration 144/1000 | Loss: 0.00002328
Iteration 145/1000 | Loss: 0.00002328
Iteration 146/1000 | Loss: 0.00002327
Iteration 147/1000 | Loss: 0.00002327
Iteration 148/1000 | Loss: 0.00002327
Iteration 149/1000 | Loss: 0.00002326
Iteration 150/1000 | Loss: 0.00002326
Iteration 151/1000 | Loss: 0.00002326
Iteration 152/1000 | Loss: 0.00002326
Iteration 153/1000 | Loss: 0.00002325
Iteration 154/1000 | Loss: 0.00002325
Iteration 155/1000 | Loss: 0.00002325
Iteration 156/1000 | Loss: 0.00002324
Iteration 157/1000 | Loss: 0.00002324
Iteration 158/1000 | Loss: 0.00002324
Iteration 159/1000 | Loss: 0.00002324
Iteration 160/1000 | Loss: 0.00002323
Iteration 161/1000 | Loss: 0.00002323
Iteration 162/1000 | Loss: 0.00002323
Iteration 163/1000 | Loss: 0.00002323
Iteration 164/1000 | Loss: 0.00002323
Iteration 165/1000 | Loss: 0.00002322
Iteration 166/1000 | Loss: 0.00002322
Iteration 167/1000 | Loss: 0.00002322
Iteration 168/1000 | Loss: 0.00002322
Iteration 169/1000 | Loss: 0.00002322
Iteration 170/1000 | Loss: 0.00002322
Iteration 171/1000 | Loss: 0.00002322
Iteration 172/1000 | Loss: 0.00002322
Iteration 173/1000 | Loss: 0.00002322
Iteration 174/1000 | Loss: 0.00002321
Iteration 175/1000 | Loss: 0.00002321
Iteration 176/1000 | Loss: 0.00002321
Iteration 177/1000 | Loss: 0.00002321
Iteration 178/1000 | Loss: 0.00002320
Iteration 179/1000 | Loss: 0.00002320
Iteration 180/1000 | Loss: 0.00002320
Iteration 181/1000 | Loss: 0.00002320
Iteration 182/1000 | Loss: 0.00002320
Iteration 183/1000 | Loss: 0.00002320
Iteration 184/1000 | Loss: 0.00002320
Iteration 185/1000 | Loss: 0.00002320
Iteration 186/1000 | Loss: 0.00002319
Iteration 187/1000 | Loss: 0.00002319
Iteration 188/1000 | Loss: 0.00002319
Iteration 189/1000 | Loss: 0.00002319
Iteration 190/1000 | Loss: 0.00002319
Iteration 191/1000 | Loss: 0.00002319
Iteration 192/1000 | Loss: 0.00002319
Iteration 193/1000 | Loss: 0.00002318
Iteration 194/1000 | Loss: 0.00002318
Iteration 195/1000 | Loss: 0.00002318
Iteration 196/1000 | Loss: 0.00002318
Iteration 197/1000 | Loss: 0.00002318
Iteration 198/1000 | Loss: 0.00002318
Iteration 199/1000 | Loss: 0.00002318
Iteration 200/1000 | Loss: 0.00002318
Iteration 201/1000 | Loss: 0.00002318
Iteration 202/1000 | Loss: 0.00002318
Iteration 203/1000 | Loss: 0.00002318
Iteration 204/1000 | Loss: 0.00002318
Iteration 205/1000 | Loss: 0.00002318
Iteration 206/1000 | Loss: 0.00002318
Iteration 207/1000 | Loss: 0.00002318
Iteration 208/1000 | Loss: 0.00002317
Iteration 209/1000 | Loss: 0.00002317
Iteration 210/1000 | Loss: 0.00002317
Iteration 211/1000 | Loss: 0.00002317
Iteration 212/1000 | Loss: 0.00002317
Iteration 213/1000 | Loss: 0.00002317
Iteration 214/1000 | Loss: 0.00002317
Iteration 215/1000 | Loss: 0.00002317
Iteration 216/1000 | Loss: 0.00002317
Iteration 217/1000 | Loss: 0.00002317
Iteration 218/1000 | Loss: 0.00002317
Iteration 219/1000 | Loss: 0.00002317
Iteration 220/1000 | Loss: 0.00002317
Iteration 221/1000 | Loss: 0.00002317
Iteration 222/1000 | Loss: 0.00002317
Iteration 223/1000 | Loss: 0.00002317
Iteration 224/1000 | Loss: 0.00002317
Iteration 225/1000 | Loss: 0.00002317
Iteration 226/1000 | Loss: 0.00002317
Iteration 227/1000 | Loss: 0.00002317
Iteration 228/1000 | Loss: 0.00002317
Iteration 229/1000 | Loss: 0.00002317
Iteration 230/1000 | Loss: 0.00002317
Iteration 231/1000 | Loss: 0.00002317
Iteration 232/1000 | Loss: 0.00002317
Iteration 233/1000 | Loss: 0.00002317
Iteration 234/1000 | Loss: 0.00002317
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 234. Stopping optimization.
Last 5 losses: [2.3171096472651698e-05, 2.3171096472651698e-05, 2.3171096472651698e-05, 2.3171096472651698e-05, 2.3171096472651698e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3171096472651698e-05

Optimization complete. Final v2v error: 3.854395866394043 mm

Highest mean error: 5.692375183105469 mm for frame 46

Lowest mean error: 2.5799262523651123 mm for frame 5

Saving results

Total time: 97.95169281959534
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_023/1010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_023/1010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_023/1010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00981965
Iteration 2/25 | Loss: 0.00435739
Iteration 3/25 | Loss: 0.00338543
Iteration 4/25 | Loss: 0.00265060
Iteration 5/25 | Loss: 0.00231510
Iteration 6/25 | Loss: 0.00222172
Iteration 7/25 | Loss: 0.00215929
Iteration 8/25 | Loss: 0.00206608
Iteration 9/25 | Loss: 0.00198863
Iteration 10/25 | Loss: 0.00183966
Iteration 11/25 | Loss: 0.00173434
Iteration 12/25 | Loss: 0.00166751
Iteration 13/25 | Loss: 0.00164040
Iteration 14/25 | Loss: 0.00162620
Iteration 15/25 | Loss: 0.00161243
Iteration 16/25 | Loss: 0.00160793
Iteration 17/25 | Loss: 0.00159806
Iteration 18/25 | Loss: 0.00158875
Iteration 19/25 | Loss: 0.00158755
Iteration 20/25 | Loss: 0.00157640
Iteration 21/25 | Loss: 0.00156305
Iteration 22/25 | Loss: 0.00156198
Iteration 23/25 | Loss: 0.00155331
Iteration 24/25 | Loss: 0.00154523
Iteration 25/25 | Loss: 0.00154457

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32155812
Iteration 2/25 | Loss: 0.00354826
Iteration 3/25 | Loss: 0.00305121
Iteration 4/25 | Loss: 0.00305121
Iteration 5/25 | Loss: 0.00305121
Iteration 6/25 | Loss: 0.00305121
Iteration 7/25 | Loss: 0.00305121
Iteration 8/25 | Loss: 0.00305121
Iteration 9/25 | Loss: 0.00305121
Iteration 10/25 | Loss: 0.00305121
Iteration 11/25 | Loss: 0.00305121
Iteration 12/25 | Loss: 0.00305121
Iteration 13/25 | Loss: 0.00305121
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0030512085650116205, 0.0030512085650116205, 0.0030512085650116205, 0.0030512085650116205, 0.0030512085650116205]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0030512085650116205

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00305121
Iteration 2/1000 | Loss: 0.00380965
Iteration 3/1000 | Loss: 0.00102907
Iteration 4/1000 | Loss: 0.00061395
Iteration 5/1000 | Loss: 0.00112906
Iteration 6/1000 | Loss: 0.00026648
Iteration 7/1000 | Loss: 0.00038997
Iteration 8/1000 | Loss: 0.00068654
Iteration 9/1000 | Loss: 0.00097724
Iteration 10/1000 | Loss: 0.00312459
Iteration 11/1000 | Loss: 0.00168797
Iteration 12/1000 | Loss: 0.00323174
Iteration 13/1000 | Loss: 0.00142260
Iteration 14/1000 | Loss: 0.00022989
Iteration 15/1000 | Loss: 0.00012094
Iteration 16/1000 | Loss: 0.00198453
Iteration 17/1000 | Loss: 0.00049879
Iteration 18/1000 | Loss: 0.00117784
Iteration 19/1000 | Loss: 0.00014716
Iteration 20/1000 | Loss: 0.00011353
Iteration 21/1000 | Loss: 0.00073660
Iteration 22/1000 | Loss: 0.00122206
Iteration 23/1000 | Loss: 0.00188255
Iteration 24/1000 | Loss: 0.00038329
Iteration 25/1000 | Loss: 0.00129883
Iteration 26/1000 | Loss: 0.00149078
Iteration 27/1000 | Loss: 0.00030788
Iteration 28/1000 | Loss: 0.00121609
Iteration 29/1000 | Loss: 0.00037003
Iteration 30/1000 | Loss: 0.00042490
Iteration 31/1000 | Loss: 0.00016194
Iteration 32/1000 | Loss: 0.00014780
Iteration 33/1000 | Loss: 0.00012942
Iteration 34/1000 | Loss: 0.00009452
Iteration 35/1000 | Loss: 0.00035008
Iteration 36/1000 | Loss: 0.00009449
Iteration 37/1000 | Loss: 0.00008855
Iteration 38/1000 | Loss: 0.00114418
Iteration 39/1000 | Loss: 0.00022430
Iteration 40/1000 | Loss: 0.00008999
Iteration 41/1000 | Loss: 0.00008431
Iteration 42/1000 | Loss: 0.00034919
Iteration 43/1000 | Loss: 0.00008115
Iteration 44/1000 | Loss: 0.00007999
Iteration 45/1000 | Loss: 0.00014263
Iteration 46/1000 | Loss: 0.00021558
Iteration 47/1000 | Loss: 0.00008916
Iteration 48/1000 | Loss: 0.00008113
Iteration 49/1000 | Loss: 0.00007815
Iteration 50/1000 | Loss: 0.00013677
Iteration 51/1000 | Loss: 0.00007790
Iteration 52/1000 | Loss: 0.00013744
Iteration 53/1000 | Loss: 0.00007737
Iteration 54/1000 | Loss: 0.00007703
Iteration 55/1000 | Loss: 0.00008369
Iteration 56/1000 | Loss: 0.00014897
Iteration 57/1000 | Loss: 0.00013999
Iteration 58/1000 | Loss: 0.00025326
Iteration 59/1000 | Loss: 0.00047256
Iteration 60/1000 | Loss: 0.00158385
Iteration 61/1000 | Loss: 0.00218805
Iteration 62/1000 | Loss: 0.00222495
Iteration 63/1000 | Loss: 0.00013462
Iteration 64/1000 | Loss: 0.00010900
Iteration 65/1000 | Loss: 0.00014947
Iteration 66/1000 | Loss: 0.00010296
Iteration 67/1000 | Loss: 0.00011926
Iteration 68/1000 | Loss: 0.00011765
Iteration 69/1000 | Loss: 0.00020496
Iteration 70/1000 | Loss: 0.00064350
Iteration 71/1000 | Loss: 0.00006883
Iteration 72/1000 | Loss: 0.00021121
Iteration 73/1000 | Loss: 0.00027754
Iteration 74/1000 | Loss: 0.00009579
Iteration 75/1000 | Loss: 0.00008680
Iteration 76/1000 | Loss: 0.00006445
Iteration 77/1000 | Loss: 0.00008585
Iteration 78/1000 | Loss: 0.00018575
Iteration 79/1000 | Loss: 0.00006336
Iteration 80/1000 | Loss: 0.00018501
Iteration 81/1000 | Loss: 0.00010599
Iteration 82/1000 | Loss: 0.00010763
Iteration 83/1000 | Loss: 0.00010438
Iteration 84/1000 | Loss: 0.00006368
Iteration 85/1000 | Loss: 0.00013199
Iteration 86/1000 | Loss: 0.00006128
Iteration 87/1000 | Loss: 0.00006035
Iteration 88/1000 | Loss: 0.00014132
Iteration 89/1000 | Loss: 0.00006004
Iteration 90/1000 | Loss: 0.00013003
Iteration 91/1000 | Loss: 0.00005977
Iteration 92/1000 | Loss: 0.00005972
Iteration 93/1000 | Loss: 0.00005968
Iteration 94/1000 | Loss: 0.00005962
Iteration 95/1000 | Loss: 0.00005961
Iteration 96/1000 | Loss: 0.00005960
Iteration 97/1000 | Loss: 0.00005960
Iteration 98/1000 | Loss: 0.00005960
Iteration 99/1000 | Loss: 0.00005959
Iteration 100/1000 | Loss: 0.00005948
Iteration 101/1000 | Loss: 0.00005947
Iteration 102/1000 | Loss: 0.00005946
Iteration 103/1000 | Loss: 0.00005946
Iteration 104/1000 | Loss: 0.00005946
Iteration 105/1000 | Loss: 0.00005945
Iteration 106/1000 | Loss: 0.00005945
Iteration 107/1000 | Loss: 0.00005941
Iteration 108/1000 | Loss: 0.00011818
Iteration 109/1000 | Loss: 0.00027906
Iteration 110/1000 | Loss: 0.00047960
Iteration 111/1000 | Loss: 0.00064426
Iteration 112/1000 | Loss: 0.00079768
Iteration 113/1000 | Loss: 0.00066773
Iteration 114/1000 | Loss: 0.00021945
Iteration 115/1000 | Loss: 0.00012466
Iteration 116/1000 | Loss: 0.00048212
Iteration 117/1000 | Loss: 0.00006024
Iteration 118/1000 | Loss: 0.00036382
Iteration 119/1000 | Loss: 0.00013241
Iteration 120/1000 | Loss: 0.00112312
Iteration 121/1000 | Loss: 0.00006410
Iteration 122/1000 | Loss: 0.00004564
Iteration 123/1000 | Loss: 0.00004388
Iteration 124/1000 | Loss: 0.00011973
Iteration 125/1000 | Loss: 0.00004554
Iteration 126/1000 | Loss: 0.00013569
Iteration 127/1000 | Loss: 0.00007316
Iteration 128/1000 | Loss: 0.00006666
Iteration 129/1000 | Loss: 0.00004551
Iteration 130/1000 | Loss: 0.00004078
Iteration 131/1000 | Loss: 0.00008496
Iteration 132/1000 | Loss: 0.00004019
Iteration 133/1000 | Loss: 0.00004001
Iteration 134/1000 | Loss: 0.00003997
Iteration 135/1000 | Loss: 0.00003996
Iteration 136/1000 | Loss: 0.00008913
Iteration 137/1000 | Loss: 0.00004500
Iteration 138/1000 | Loss: 0.00007392
Iteration 139/1000 | Loss: 0.00012801
Iteration 140/1000 | Loss: 0.00004791
Iteration 141/1000 | Loss: 0.00003992
Iteration 142/1000 | Loss: 0.00003985
Iteration 143/1000 | Loss: 0.00003984
Iteration 144/1000 | Loss: 0.00003984
Iteration 145/1000 | Loss: 0.00003984
Iteration 146/1000 | Loss: 0.00003984
Iteration 147/1000 | Loss: 0.00003984
Iteration 148/1000 | Loss: 0.00003984
Iteration 149/1000 | Loss: 0.00003983
Iteration 150/1000 | Loss: 0.00003983
Iteration 151/1000 | Loss: 0.00003983
Iteration 152/1000 | Loss: 0.00003983
Iteration 153/1000 | Loss: 0.00003983
Iteration 154/1000 | Loss: 0.00003983
Iteration 155/1000 | Loss: 0.00003983
Iteration 156/1000 | Loss: 0.00003983
Iteration 157/1000 | Loss: 0.00003982
Iteration 158/1000 | Loss: 0.00003982
Iteration 159/1000 | Loss: 0.00003981
Iteration 160/1000 | Loss: 0.00003981
Iteration 161/1000 | Loss: 0.00003980
Iteration 162/1000 | Loss: 0.00003980
Iteration 163/1000 | Loss: 0.00003978
Iteration 164/1000 | Loss: 0.00003978
Iteration 165/1000 | Loss: 0.00003978
Iteration 166/1000 | Loss: 0.00003977
Iteration 167/1000 | Loss: 0.00003977
Iteration 168/1000 | Loss: 0.00006947
Iteration 169/1000 | Loss: 0.00004217
Iteration 170/1000 | Loss: 0.00004826
Iteration 171/1000 | Loss: 0.00003975
Iteration 172/1000 | Loss: 0.00003975
Iteration 173/1000 | Loss: 0.00003975
Iteration 174/1000 | Loss: 0.00003975
Iteration 175/1000 | Loss: 0.00003974
Iteration 176/1000 | Loss: 0.00003974
Iteration 177/1000 | Loss: 0.00003974
Iteration 178/1000 | Loss: 0.00003974
Iteration 179/1000 | Loss: 0.00003974
Iteration 180/1000 | Loss: 0.00003974
Iteration 181/1000 | Loss: 0.00003974
Iteration 182/1000 | Loss: 0.00003974
Iteration 183/1000 | Loss: 0.00003973
Iteration 184/1000 | Loss: 0.00003973
Iteration 185/1000 | Loss: 0.00003973
Iteration 186/1000 | Loss: 0.00003973
Iteration 187/1000 | Loss: 0.00003973
Iteration 188/1000 | Loss: 0.00003973
Iteration 189/1000 | Loss: 0.00003973
Iteration 190/1000 | Loss: 0.00003973
Iteration 191/1000 | Loss: 0.00003973
Iteration 192/1000 | Loss: 0.00003973
Iteration 193/1000 | Loss: 0.00003972
Iteration 194/1000 | Loss: 0.00003972
Iteration 195/1000 | Loss: 0.00003972
Iteration 196/1000 | Loss: 0.00003971
Iteration 197/1000 | Loss: 0.00003971
Iteration 198/1000 | Loss: 0.00003971
Iteration 199/1000 | Loss: 0.00003971
Iteration 200/1000 | Loss: 0.00003970
Iteration 201/1000 | Loss: 0.00003970
Iteration 202/1000 | Loss: 0.00003970
Iteration 203/1000 | Loss: 0.00003970
Iteration 204/1000 | Loss: 0.00003970
Iteration 205/1000 | Loss: 0.00003970
Iteration 206/1000 | Loss: 0.00003969
Iteration 207/1000 | Loss: 0.00003969
Iteration 208/1000 | Loss: 0.00003969
Iteration 209/1000 | Loss: 0.00003969
Iteration 210/1000 | Loss: 0.00003969
Iteration 211/1000 | Loss: 0.00003969
Iteration 212/1000 | Loss: 0.00003968
Iteration 213/1000 | Loss: 0.00003968
Iteration 214/1000 | Loss: 0.00003968
Iteration 215/1000 | Loss: 0.00003968
Iteration 216/1000 | Loss: 0.00003967
Iteration 217/1000 | Loss: 0.00003967
Iteration 218/1000 | Loss: 0.00003967
Iteration 219/1000 | Loss: 0.00003967
Iteration 220/1000 | Loss: 0.00003967
Iteration 221/1000 | Loss: 0.00003967
Iteration 222/1000 | Loss: 0.00003966
Iteration 223/1000 | Loss: 0.00003966
Iteration 224/1000 | Loss: 0.00003966
Iteration 225/1000 | Loss: 0.00003966
Iteration 226/1000 | Loss: 0.00003966
Iteration 227/1000 | Loss: 0.00003966
Iteration 228/1000 | Loss: 0.00003966
Iteration 229/1000 | Loss: 0.00003966
Iteration 230/1000 | Loss: 0.00003966
Iteration 231/1000 | Loss: 0.00003965
Iteration 232/1000 | Loss: 0.00003965
Iteration 233/1000 | Loss: 0.00003965
Iteration 234/1000 | Loss: 0.00003965
Iteration 235/1000 | Loss: 0.00003965
Iteration 236/1000 | Loss: 0.00003965
Iteration 237/1000 | Loss: 0.00003965
Iteration 238/1000 | Loss: 0.00003965
Iteration 239/1000 | Loss: 0.00003964
Iteration 240/1000 | Loss: 0.00003964
Iteration 241/1000 | Loss: 0.00003963
Iteration 242/1000 | Loss: 0.00003963
Iteration 243/1000 | Loss: 0.00003963
Iteration 244/1000 | Loss: 0.00010314
Iteration 245/1000 | Loss: 0.00003966
Iteration 246/1000 | Loss: 0.00008465
Iteration 247/1000 | Loss: 0.00010293
Iteration 248/1000 | Loss: 0.00070139
Iteration 249/1000 | Loss: 0.00016076
Iteration 250/1000 | Loss: 0.00013884
Iteration 251/1000 | Loss: 0.00005343
Iteration 252/1000 | Loss: 0.00003975
Iteration 253/1000 | Loss: 0.00003965
Iteration 254/1000 | Loss: 0.00003964
Iteration 255/1000 | Loss: 0.00003963
Iteration 256/1000 | Loss: 0.00003962
Iteration 257/1000 | Loss: 0.00003961
Iteration 258/1000 | Loss: 0.00003960
Iteration 259/1000 | Loss: 0.00003960
Iteration 260/1000 | Loss: 0.00003960
Iteration 261/1000 | Loss: 0.00003959
Iteration 262/1000 | Loss: 0.00003959
Iteration 263/1000 | Loss: 0.00003959
Iteration 264/1000 | Loss: 0.00003959
Iteration 265/1000 | Loss: 0.00003958
Iteration 266/1000 | Loss: 0.00003958
Iteration 267/1000 | Loss: 0.00007010
Iteration 268/1000 | Loss: 0.00003962
Iteration 269/1000 | Loss: 0.00003957
Iteration 270/1000 | Loss: 0.00003957
Iteration 271/1000 | Loss: 0.00003957
Iteration 272/1000 | Loss: 0.00003957
Iteration 273/1000 | Loss: 0.00003956
Iteration 274/1000 | Loss: 0.00003956
Iteration 275/1000 | Loss: 0.00003956
Iteration 276/1000 | Loss: 0.00003956
Iteration 277/1000 | Loss: 0.00003956
Iteration 278/1000 | Loss: 0.00003956
Iteration 279/1000 | Loss: 0.00003956
Iteration 280/1000 | Loss: 0.00003956
Iteration 281/1000 | Loss: 0.00003956
Iteration 282/1000 | Loss: 0.00003956
Iteration 283/1000 | Loss: 0.00003956
Iteration 284/1000 | Loss: 0.00003955
Iteration 285/1000 | Loss: 0.00003955
Iteration 286/1000 | Loss: 0.00003955
Iteration 287/1000 | Loss: 0.00003955
Iteration 288/1000 | Loss: 0.00003955
Iteration 289/1000 | Loss: 0.00003955
Iteration 290/1000 | Loss: 0.00003955
Iteration 291/1000 | Loss: 0.00003955
Iteration 292/1000 | Loss: 0.00003954
Iteration 293/1000 | Loss: 0.00003954
Iteration 294/1000 | Loss: 0.00003954
Iteration 295/1000 | Loss: 0.00003954
Iteration 296/1000 | Loss: 0.00003954
Iteration 297/1000 | Loss: 0.00003954
Iteration 298/1000 | Loss: 0.00003954
Iteration 299/1000 | Loss: 0.00003954
Iteration 300/1000 | Loss: 0.00003954
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 300. Stopping optimization.
Last 5 losses: [3.954463318223134e-05, 3.954463318223134e-05, 3.954463318223134e-05, 3.954463318223134e-05, 3.954463318223134e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.954463318223134e-05

Optimization complete. Final v2v error: 4.109386920928955 mm

Highest mean error: 11.45827865600586 mm for frame 120

Lowest mean error: 3.378291368484497 mm for frame 15

Saving results

Total time: 282.7416911125183
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_023/1007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_023/1007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_023/1007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01053448
Iteration 2/25 | Loss: 0.00165337
Iteration 3/25 | Loss: 0.00129879
Iteration 4/25 | Loss: 0.00123882
Iteration 5/25 | Loss: 0.00121069
Iteration 6/25 | Loss: 0.00119956
Iteration 7/25 | Loss: 0.00119510
Iteration 8/25 | Loss: 0.00119282
Iteration 9/25 | Loss: 0.00119238
Iteration 10/25 | Loss: 0.00119234
Iteration 11/25 | Loss: 0.00119234
Iteration 12/25 | Loss: 0.00119234
Iteration 13/25 | Loss: 0.00119234
Iteration 14/25 | Loss: 0.00119233
Iteration 15/25 | Loss: 0.00119233
Iteration 16/25 | Loss: 0.00119233
Iteration 17/25 | Loss: 0.00119233
Iteration 18/25 | Loss: 0.00119233
Iteration 19/25 | Loss: 0.00119233
Iteration 20/25 | Loss: 0.00119233
Iteration 21/25 | Loss: 0.00119233
Iteration 22/25 | Loss: 0.00119233
Iteration 23/25 | Loss: 0.00119233
Iteration 24/25 | Loss: 0.00119233
Iteration 25/25 | Loss: 0.00119233

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35124695
Iteration 2/25 | Loss: 0.00099275
Iteration 3/25 | Loss: 0.00099274
Iteration 4/25 | Loss: 0.00099274
Iteration 5/25 | Loss: 0.00099274
Iteration 6/25 | Loss: 0.00095568
Iteration 7/25 | Loss: 0.00095568
Iteration 8/25 | Loss: 0.00095568
Iteration 9/25 | Loss: 0.00095568
Iteration 10/25 | Loss: 0.00095568
Iteration 11/25 | Loss: 0.00095568
Iteration 12/25 | Loss: 0.00095568
Iteration 13/25 | Loss: 0.00095568
Iteration 14/25 | Loss: 0.00095568
Iteration 15/25 | Loss: 0.00095568
Iteration 16/25 | Loss: 0.00095568
Iteration 17/25 | Loss: 0.00095568
Iteration 18/25 | Loss: 0.00095568
Iteration 19/25 | Loss: 0.00095568
Iteration 20/25 | Loss: 0.00095568
Iteration 21/25 | Loss: 0.00095568
Iteration 22/25 | Loss: 0.00095568
Iteration 23/25 | Loss: 0.00095568
Iteration 24/25 | Loss: 0.00095568
Iteration 25/25 | Loss: 0.00095568

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00095568
Iteration 2/1000 | Loss: 0.00002842
Iteration 3/1000 | Loss: 0.00001835
Iteration 4/1000 | Loss: 0.00001632
Iteration 5/1000 | Loss: 0.00001556
Iteration 6/1000 | Loss: 0.00001520
Iteration 7/1000 | Loss: 0.00007124
Iteration 8/1000 | Loss: 0.00003153
Iteration 9/1000 | Loss: 0.00001697
Iteration 10/1000 | Loss: 0.00001462
Iteration 11/1000 | Loss: 0.00001441
Iteration 12/1000 | Loss: 0.00001439
Iteration 13/1000 | Loss: 0.00001434
Iteration 14/1000 | Loss: 0.00009653
Iteration 15/1000 | Loss: 0.00004974
Iteration 16/1000 | Loss: 0.00007072
Iteration 17/1000 | Loss: 0.00007013
Iteration 18/1000 | Loss: 0.00011716
Iteration 19/1000 | Loss: 0.00001593
Iteration 20/1000 | Loss: 0.00001435
Iteration 21/1000 | Loss: 0.00001396
Iteration 22/1000 | Loss: 0.00001386
Iteration 23/1000 | Loss: 0.00001382
Iteration 24/1000 | Loss: 0.00001380
Iteration 25/1000 | Loss: 0.00001380
Iteration 26/1000 | Loss: 0.00001379
Iteration 27/1000 | Loss: 0.00001378
Iteration 28/1000 | Loss: 0.00001376
Iteration 29/1000 | Loss: 0.00001375
Iteration 30/1000 | Loss: 0.00001375
Iteration 31/1000 | Loss: 0.00001373
Iteration 32/1000 | Loss: 0.00001373
Iteration 33/1000 | Loss: 0.00001369
Iteration 34/1000 | Loss: 0.00001369
Iteration 35/1000 | Loss: 0.00001367
Iteration 36/1000 | Loss: 0.00001366
Iteration 37/1000 | Loss: 0.00001365
Iteration 38/1000 | Loss: 0.00001365
Iteration 39/1000 | Loss: 0.00001365
Iteration 40/1000 | Loss: 0.00001365
Iteration 41/1000 | Loss: 0.00001365
Iteration 42/1000 | Loss: 0.00001364
Iteration 43/1000 | Loss: 0.00001364
Iteration 44/1000 | Loss: 0.00001364
Iteration 45/1000 | Loss: 0.00001362
Iteration 46/1000 | Loss: 0.00001362
Iteration 47/1000 | Loss: 0.00001361
Iteration 48/1000 | Loss: 0.00001361
Iteration 49/1000 | Loss: 0.00001361
Iteration 50/1000 | Loss: 0.00001361
Iteration 51/1000 | Loss: 0.00001361
Iteration 52/1000 | Loss: 0.00001360
Iteration 53/1000 | Loss: 0.00001360
Iteration 54/1000 | Loss: 0.00001360
Iteration 55/1000 | Loss: 0.00001360
Iteration 56/1000 | Loss: 0.00001359
Iteration 57/1000 | Loss: 0.00001359
Iteration 58/1000 | Loss: 0.00001359
Iteration 59/1000 | Loss: 0.00001359
Iteration 60/1000 | Loss: 0.00001358
Iteration 61/1000 | Loss: 0.00001358
Iteration 62/1000 | Loss: 0.00001358
Iteration 63/1000 | Loss: 0.00001358
Iteration 64/1000 | Loss: 0.00001358
Iteration 65/1000 | Loss: 0.00001357
Iteration 66/1000 | Loss: 0.00001357
Iteration 67/1000 | Loss: 0.00001357
Iteration 68/1000 | Loss: 0.00001357
Iteration 69/1000 | Loss: 0.00001357
Iteration 70/1000 | Loss: 0.00001357
Iteration 71/1000 | Loss: 0.00001357
Iteration 72/1000 | Loss: 0.00001357
Iteration 73/1000 | Loss: 0.00001357
Iteration 74/1000 | Loss: 0.00001356
Iteration 75/1000 | Loss: 0.00001355
Iteration 76/1000 | Loss: 0.00001355
Iteration 77/1000 | Loss: 0.00001355
Iteration 78/1000 | Loss: 0.00001355
Iteration 79/1000 | Loss: 0.00001354
Iteration 80/1000 | Loss: 0.00001354
Iteration 81/1000 | Loss: 0.00001354
Iteration 82/1000 | Loss: 0.00001354
Iteration 83/1000 | Loss: 0.00001353
Iteration 84/1000 | Loss: 0.00001350
Iteration 85/1000 | Loss: 0.00001348
Iteration 86/1000 | Loss: 0.00001347
Iteration 87/1000 | Loss: 0.00001346
Iteration 88/1000 | Loss: 0.00001344
Iteration 89/1000 | Loss: 0.00001343
Iteration 90/1000 | Loss: 0.00001341
Iteration 91/1000 | Loss: 0.00001339
Iteration 92/1000 | Loss: 0.00001339
Iteration 93/1000 | Loss: 0.00001338
Iteration 94/1000 | Loss: 0.00001338
Iteration 95/1000 | Loss: 0.00001338
Iteration 96/1000 | Loss: 0.00001338
Iteration 97/1000 | Loss: 0.00001337
Iteration 98/1000 | Loss: 0.00001337
Iteration 99/1000 | Loss: 0.00001337
Iteration 100/1000 | Loss: 0.00001337
Iteration 101/1000 | Loss: 0.00001337
Iteration 102/1000 | Loss: 0.00001336
Iteration 103/1000 | Loss: 0.00001336
Iteration 104/1000 | Loss: 0.00001336
Iteration 105/1000 | Loss: 0.00001335
Iteration 106/1000 | Loss: 0.00001335
Iteration 107/1000 | Loss: 0.00001335
Iteration 108/1000 | Loss: 0.00001335
Iteration 109/1000 | Loss: 0.00001334
Iteration 110/1000 | Loss: 0.00001334
Iteration 111/1000 | Loss: 0.00001334
Iteration 112/1000 | Loss: 0.00001334
Iteration 113/1000 | Loss: 0.00001334
Iteration 114/1000 | Loss: 0.00001334
Iteration 115/1000 | Loss: 0.00001334
Iteration 116/1000 | Loss: 0.00001334
Iteration 117/1000 | Loss: 0.00001333
Iteration 118/1000 | Loss: 0.00001333
Iteration 119/1000 | Loss: 0.00001333
Iteration 120/1000 | Loss: 0.00001333
Iteration 121/1000 | Loss: 0.00001333
Iteration 122/1000 | Loss: 0.00001332
Iteration 123/1000 | Loss: 0.00001332
Iteration 124/1000 | Loss: 0.00001332
Iteration 125/1000 | Loss: 0.00001332
Iteration 126/1000 | Loss: 0.00001332
Iteration 127/1000 | Loss: 0.00001332
Iteration 128/1000 | Loss: 0.00001331
Iteration 129/1000 | Loss: 0.00001331
Iteration 130/1000 | Loss: 0.00001331
Iteration 131/1000 | Loss: 0.00001331
Iteration 132/1000 | Loss: 0.00001331
Iteration 133/1000 | Loss: 0.00001331
Iteration 134/1000 | Loss: 0.00001331
Iteration 135/1000 | Loss: 0.00001331
Iteration 136/1000 | Loss: 0.00001330
Iteration 137/1000 | Loss: 0.00001330
Iteration 138/1000 | Loss: 0.00001330
Iteration 139/1000 | Loss: 0.00001330
Iteration 140/1000 | Loss: 0.00001329
Iteration 141/1000 | Loss: 0.00001329
Iteration 142/1000 | Loss: 0.00001329
Iteration 143/1000 | Loss: 0.00001329
Iteration 144/1000 | Loss: 0.00001329
Iteration 145/1000 | Loss: 0.00001329
Iteration 146/1000 | Loss: 0.00001329
Iteration 147/1000 | Loss: 0.00001329
Iteration 148/1000 | Loss: 0.00001329
Iteration 149/1000 | Loss: 0.00001329
Iteration 150/1000 | Loss: 0.00001328
Iteration 151/1000 | Loss: 0.00001328
Iteration 152/1000 | Loss: 0.00001328
Iteration 153/1000 | Loss: 0.00001328
Iteration 154/1000 | Loss: 0.00001328
Iteration 155/1000 | Loss: 0.00001328
Iteration 156/1000 | Loss: 0.00001328
Iteration 157/1000 | Loss: 0.00001327
Iteration 158/1000 | Loss: 0.00001327
Iteration 159/1000 | Loss: 0.00001327
Iteration 160/1000 | Loss: 0.00001327
Iteration 161/1000 | Loss: 0.00001327
Iteration 162/1000 | Loss: 0.00001327
Iteration 163/1000 | Loss: 0.00001327
Iteration 164/1000 | Loss: 0.00001327
Iteration 165/1000 | Loss: 0.00001327
Iteration 166/1000 | Loss: 0.00001327
Iteration 167/1000 | Loss: 0.00001326
Iteration 168/1000 | Loss: 0.00001326
Iteration 169/1000 | Loss: 0.00001326
Iteration 170/1000 | Loss: 0.00001326
Iteration 171/1000 | Loss: 0.00001326
Iteration 172/1000 | Loss: 0.00001326
Iteration 173/1000 | Loss: 0.00001326
Iteration 174/1000 | Loss: 0.00001326
Iteration 175/1000 | Loss: 0.00001326
Iteration 176/1000 | Loss: 0.00001326
Iteration 177/1000 | Loss: 0.00001326
Iteration 178/1000 | Loss: 0.00001326
Iteration 179/1000 | Loss: 0.00001326
Iteration 180/1000 | Loss: 0.00001326
Iteration 181/1000 | Loss: 0.00001326
Iteration 182/1000 | Loss: 0.00001325
Iteration 183/1000 | Loss: 0.00001325
Iteration 184/1000 | Loss: 0.00001325
Iteration 185/1000 | Loss: 0.00001325
Iteration 186/1000 | Loss: 0.00001325
Iteration 187/1000 | Loss: 0.00001325
Iteration 188/1000 | Loss: 0.00001325
Iteration 189/1000 | Loss: 0.00001325
Iteration 190/1000 | Loss: 0.00001325
Iteration 191/1000 | Loss: 0.00001325
Iteration 192/1000 | Loss: 0.00001325
Iteration 193/1000 | Loss: 0.00001325
Iteration 194/1000 | Loss: 0.00001325
Iteration 195/1000 | Loss: 0.00001325
Iteration 196/1000 | Loss: 0.00001325
Iteration 197/1000 | Loss: 0.00001325
Iteration 198/1000 | Loss: 0.00001325
Iteration 199/1000 | Loss: 0.00001325
Iteration 200/1000 | Loss: 0.00001324
Iteration 201/1000 | Loss: 0.00001324
Iteration 202/1000 | Loss: 0.00001324
Iteration 203/1000 | Loss: 0.00001324
Iteration 204/1000 | Loss: 0.00001324
Iteration 205/1000 | Loss: 0.00001324
Iteration 206/1000 | Loss: 0.00001324
Iteration 207/1000 | Loss: 0.00001324
Iteration 208/1000 | Loss: 0.00001324
Iteration 209/1000 | Loss: 0.00001324
Iteration 210/1000 | Loss: 0.00001324
Iteration 211/1000 | Loss: 0.00001324
Iteration 212/1000 | Loss: 0.00001324
Iteration 213/1000 | Loss: 0.00001323
Iteration 214/1000 | Loss: 0.00001323
Iteration 215/1000 | Loss: 0.00001323
Iteration 216/1000 | Loss: 0.00001323
Iteration 217/1000 | Loss: 0.00001323
Iteration 218/1000 | Loss: 0.00001323
Iteration 219/1000 | Loss: 0.00001323
Iteration 220/1000 | Loss: 0.00001323
Iteration 221/1000 | Loss: 0.00001323
Iteration 222/1000 | Loss: 0.00001323
Iteration 223/1000 | Loss: 0.00001323
Iteration 224/1000 | Loss: 0.00001323
Iteration 225/1000 | Loss: 0.00001323
Iteration 226/1000 | Loss: 0.00001323
Iteration 227/1000 | Loss: 0.00001323
Iteration 228/1000 | Loss: 0.00001323
Iteration 229/1000 | Loss: 0.00001323
Iteration 230/1000 | Loss: 0.00001323
Iteration 231/1000 | Loss: 0.00001323
Iteration 232/1000 | Loss: 0.00001323
Iteration 233/1000 | Loss: 0.00001323
Iteration 234/1000 | Loss: 0.00001323
Iteration 235/1000 | Loss: 0.00001323
Iteration 236/1000 | Loss: 0.00001323
Iteration 237/1000 | Loss: 0.00001323
Iteration 238/1000 | Loss: 0.00001323
Iteration 239/1000 | Loss: 0.00001323
Iteration 240/1000 | Loss: 0.00001323
Iteration 241/1000 | Loss: 0.00001323
Iteration 242/1000 | Loss: 0.00001323
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 242. Stopping optimization.
Last 5 losses: [1.3225959264673293e-05, 1.3225959264673293e-05, 1.3225959264673293e-05, 1.3225959264673293e-05, 1.3225959264673293e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3225959264673293e-05

Optimization complete. Final v2v error: 3.1166296005249023 mm

Highest mean error: 3.774563789367676 mm for frame 20

Lowest mean error: 2.9265923500061035 mm for frame 50

Saving results

Total time: 61.65236473083496
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_023/1089/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_023/1089.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_023/1089
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00445261
Iteration 2/25 | Loss: 0.00134049
Iteration 3/25 | Loss: 0.00119164
Iteration 4/25 | Loss: 0.00117556
Iteration 5/25 | Loss: 0.00117066
Iteration 6/25 | Loss: 0.00117034
Iteration 7/25 | Loss: 0.00117034
Iteration 8/25 | Loss: 0.00117034
Iteration 9/25 | Loss: 0.00117034
Iteration 10/25 | Loss: 0.00117034
Iteration 11/25 | Loss: 0.00117034
Iteration 12/25 | Loss: 0.00117034
Iteration 13/25 | Loss: 0.00117034
Iteration 14/25 | Loss: 0.00117034
Iteration 15/25 | Loss: 0.00117034
Iteration 16/25 | Loss: 0.00117034
Iteration 17/25 | Loss: 0.00117034
Iteration 18/25 | Loss: 0.00117034
Iteration 19/25 | Loss: 0.00117034
Iteration 20/25 | Loss: 0.00117034
Iteration 21/25 | Loss: 0.00117034
Iteration 22/25 | Loss: 0.00117034
Iteration 23/25 | Loss: 0.00117034
Iteration 24/25 | Loss: 0.00117034
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0011703424388542771, 0.0011703424388542771, 0.0011703424388542771, 0.0011703424388542771, 0.0011703424388542771]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011703424388542771

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37739539
Iteration 2/25 | Loss: 0.00086943
Iteration 3/25 | Loss: 0.00086942
Iteration 4/25 | Loss: 0.00086942
Iteration 5/25 | Loss: 0.00086942
Iteration 6/25 | Loss: 0.00086942
Iteration 7/25 | Loss: 0.00086942
Iteration 8/25 | Loss: 0.00086942
Iteration 9/25 | Loss: 0.00086942
Iteration 10/25 | Loss: 0.00086942
Iteration 11/25 | Loss: 0.00086942
Iteration 12/25 | Loss: 0.00086942
Iteration 13/25 | Loss: 0.00086942
Iteration 14/25 | Loss: 0.00086942
Iteration 15/25 | Loss: 0.00086942
Iteration 16/25 | Loss: 0.00086942
Iteration 17/25 | Loss: 0.00086942
Iteration 18/25 | Loss: 0.00086942
Iteration 19/25 | Loss: 0.00086942
Iteration 20/25 | Loss: 0.00086942
Iteration 21/25 | Loss: 0.00086942
Iteration 22/25 | Loss: 0.00086942
Iteration 23/25 | Loss: 0.00086942
Iteration 24/25 | Loss: 0.00086942
Iteration 25/25 | Loss: 0.00086942

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00086942
Iteration 2/1000 | Loss: 0.00002983
Iteration 3/1000 | Loss: 0.00002369
Iteration 4/1000 | Loss: 0.00002221
Iteration 5/1000 | Loss: 0.00002129
Iteration 6/1000 | Loss: 0.00002063
Iteration 7/1000 | Loss: 0.00002019
Iteration 8/1000 | Loss: 0.00001975
Iteration 9/1000 | Loss: 0.00001936
Iteration 10/1000 | Loss: 0.00001915
Iteration 11/1000 | Loss: 0.00001902
Iteration 12/1000 | Loss: 0.00001888
Iteration 13/1000 | Loss: 0.00001888
Iteration 14/1000 | Loss: 0.00001878
Iteration 15/1000 | Loss: 0.00001878
Iteration 16/1000 | Loss: 0.00001878
Iteration 17/1000 | Loss: 0.00001878
Iteration 18/1000 | Loss: 0.00001876
Iteration 19/1000 | Loss: 0.00001875
Iteration 20/1000 | Loss: 0.00001875
Iteration 21/1000 | Loss: 0.00001872
Iteration 22/1000 | Loss: 0.00001872
Iteration 23/1000 | Loss: 0.00001872
Iteration 24/1000 | Loss: 0.00001871
Iteration 25/1000 | Loss: 0.00001871
Iteration 26/1000 | Loss: 0.00001869
Iteration 27/1000 | Loss: 0.00001868
Iteration 28/1000 | Loss: 0.00001868
Iteration 29/1000 | Loss: 0.00001868
Iteration 30/1000 | Loss: 0.00001868
Iteration 31/1000 | Loss: 0.00001868
Iteration 32/1000 | Loss: 0.00001868
Iteration 33/1000 | Loss: 0.00001867
Iteration 34/1000 | Loss: 0.00001867
Iteration 35/1000 | Loss: 0.00001867
Iteration 36/1000 | Loss: 0.00001867
Iteration 37/1000 | Loss: 0.00001867
Iteration 38/1000 | Loss: 0.00001867
Iteration 39/1000 | Loss: 0.00001865
Iteration 40/1000 | Loss: 0.00001865
Iteration 41/1000 | Loss: 0.00001865
Iteration 42/1000 | Loss: 0.00001864
Iteration 43/1000 | Loss: 0.00001864
Iteration 44/1000 | Loss: 0.00001863
Iteration 45/1000 | Loss: 0.00001863
Iteration 46/1000 | Loss: 0.00001863
Iteration 47/1000 | Loss: 0.00001862
Iteration 48/1000 | Loss: 0.00001862
Iteration 49/1000 | Loss: 0.00001862
Iteration 50/1000 | Loss: 0.00001861
Iteration 51/1000 | Loss: 0.00001861
Iteration 52/1000 | Loss: 0.00001860
Iteration 53/1000 | Loss: 0.00001860
Iteration 54/1000 | Loss: 0.00001860
Iteration 55/1000 | Loss: 0.00001859
Iteration 56/1000 | Loss: 0.00001859
Iteration 57/1000 | Loss: 0.00001858
Iteration 58/1000 | Loss: 0.00001858
Iteration 59/1000 | Loss: 0.00001858
Iteration 60/1000 | Loss: 0.00001856
Iteration 61/1000 | Loss: 0.00001856
Iteration 62/1000 | Loss: 0.00001856
Iteration 63/1000 | Loss: 0.00001856
Iteration 64/1000 | Loss: 0.00001856
Iteration 65/1000 | Loss: 0.00001856
Iteration 66/1000 | Loss: 0.00001856
Iteration 67/1000 | Loss: 0.00001855
Iteration 68/1000 | Loss: 0.00001854
Iteration 69/1000 | Loss: 0.00001852
Iteration 70/1000 | Loss: 0.00001852
Iteration 71/1000 | Loss: 0.00001851
Iteration 72/1000 | Loss: 0.00001851
Iteration 73/1000 | Loss: 0.00001850
Iteration 74/1000 | Loss: 0.00001848
Iteration 75/1000 | Loss: 0.00001846
Iteration 76/1000 | Loss: 0.00001846
Iteration 77/1000 | Loss: 0.00001846
Iteration 78/1000 | Loss: 0.00001845
Iteration 79/1000 | Loss: 0.00001844
Iteration 80/1000 | Loss: 0.00001843
Iteration 81/1000 | Loss: 0.00001843
Iteration 82/1000 | Loss: 0.00001843
Iteration 83/1000 | Loss: 0.00001843
Iteration 84/1000 | Loss: 0.00001842
Iteration 85/1000 | Loss: 0.00001842
Iteration 86/1000 | Loss: 0.00001842
Iteration 87/1000 | Loss: 0.00001842
Iteration 88/1000 | Loss: 0.00001842
Iteration 89/1000 | Loss: 0.00001842
Iteration 90/1000 | Loss: 0.00001840
Iteration 91/1000 | Loss: 0.00001840
Iteration 92/1000 | Loss: 0.00001839
Iteration 93/1000 | Loss: 0.00001838
Iteration 94/1000 | Loss: 0.00001838
Iteration 95/1000 | Loss: 0.00001838
Iteration 96/1000 | Loss: 0.00001837
Iteration 97/1000 | Loss: 0.00001837
Iteration 98/1000 | Loss: 0.00001837
Iteration 99/1000 | Loss: 0.00001837
Iteration 100/1000 | Loss: 0.00001836
Iteration 101/1000 | Loss: 0.00001836
Iteration 102/1000 | Loss: 0.00001836
Iteration 103/1000 | Loss: 0.00001836
Iteration 104/1000 | Loss: 0.00001836
Iteration 105/1000 | Loss: 0.00001836
Iteration 106/1000 | Loss: 0.00001835
Iteration 107/1000 | Loss: 0.00001835
Iteration 108/1000 | Loss: 0.00001835
Iteration 109/1000 | Loss: 0.00001835
Iteration 110/1000 | Loss: 0.00001835
Iteration 111/1000 | Loss: 0.00001835
Iteration 112/1000 | Loss: 0.00001835
Iteration 113/1000 | Loss: 0.00001835
Iteration 114/1000 | Loss: 0.00001835
Iteration 115/1000 | Loss: 0.00001835
Iteration 116/1000 | Loss: 0.00001835
Iteration 117/1000 | Loss: 0.00001835
Iteration 118/1000 | Loss: 0.00001834
Iteration 119/1000 | Loss: 0.00001834
Iteration 120/1000 | Loss: 0.00001834
Iteration 121/1000 | Loss: 0.00001834
Iteration 122/1000 | Loss: 0.00001834
Iteration 123/1000 | Loss: 0.00001833
Iteration 124/1000 | Loss: 0.00001833
Iteration 125/1000 | Loss: 0.00001833
Iteration 126/1000 | Loss: 0.00001833
Iteration 127/1000 | Loss: 0.00001833
Iteration 128/1000 | Loss: 0.00001833
Iteration 129/1000 | Loss: 0.00001833
Iteration 130/1000 | Loss: 0.00001833
Iteration 131/1000 | Loss: 0.00001833
Iteration 132/1000 | Loss: 0.00001833
Iteration 133/1000 | Loss: 0.00001833
Iteration 134/1000 | Loss: 0.00001833
Iteration 135/1000 | Loss: 0.00001832
Iteration 136/1000 | Loss: 0.00001832
Iteration 137/1000 | Loss: 0.00001832
Iteration 138/1000 | Loss: 0.00001832
Iteration 139/1000 | Loss: 0.00001832
Iteration 140/1000 | Loss: 0.00001832
Iteration 141/1000 | Loss: 0.00001832
Iteration 142/1000 | Loss: 0.00001832
Iteration 143/1000 | Loss: 0.00001832
Iteration 144/1000 | Loss: 0.00001832
Iteration 145/1000 | Loss: 0.00001832
Iteration 146/1000 | Loss: 0.00001832
Iteration 147/1000 | Loss: 0.00001832
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 147. Stopping optimization.
Last 5 losses: [1.831976078392472e-05, 1.831976078392472e-05, 1.831976078392472e-05, 1.831976078392472e-05, 1.831976078392472e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.831976078392472e-05

Optimization complete. Final v2v error: 3.632998466491699 mm

Highest mean error: 4.1922430992126465 mm for frame 143

Lowest mean error: 3.2563059329986572 mm for frame 40

Saving results

Total time: 42.490437746047974
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_023/1036/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_023/1036.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_023/1036
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00770604
Iteration 2/25 | Loss: 0.00128646
Iteration 3/25 | Loss: 0.00116263
Iteration 4/25 | Loss: 0.00113720
Iteration 5/25 | Loss: 0.00113603
Iteration 6/25 | Loss: 0.00113434
Iteration 7/25 | Loss: 0.00113163
Iteration 8/25 | Loss: 0.00113075
Iteration 9/25 | Loss: 0.00113041
Iteration 10/25 | Loss: 0.00112963
Iteration 11/25 | Loss: 0.00112951
Iteration 12/25 | Loss: 0.00112951
Iteration 13/25 | Loss: 0.00112947
Iteration 14/25 | Loss: 0.00112947
Iteration 15/25 | Loss: 0.00112947
Iteration 16/25 | Loss: 0.00112947
Iteration 17/25 | Loss: 0.00112947
Iteration 18/25 | Loss: 0.00112947
Iteration 19/25 | Loss: 0.00112947
Iteration 20/25 | Loss: 0.00112947
Iteration 21/25 | Loss: 0.00112947
Iteration 22/25 | Loss: 0.00112947
Iteration 23/25 | Loss: 0.00112947
Iteration 24/25 | Loss: 0.00112947
Iteration 25/25 | Loss: 0.00112947

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.41535878
Iteration 2/25 | Loss: 0.00095675
Iteration 3/25 | Loss: 0.00089314
Iteration 4/25 | Loss: 0.00089314
Iteration 5/25 | Loss: 0.00089314
Iteration 6/25 | Loss: 0.00089314
Iteration 7/25 | Loss: 0.00089314
Iteration 8/25 | Loss: 0.00089314
Iteration 9/25 | Loss: 0.00089314
Iteration 10/25 | Loss: 0.00089314
Iteration 11/25 | Loss: 0.00089314
Iteration 12/25 | Loss: 0.00089314
Iteration 13/25 | Loss: 0.00089313
Iteration 14/25 | Loss: 0.00089313
Iteration 15/25 | Loss: 0.00089313
Iteration 16/25 | Loss: 0.00089313
Iteration 17/25 | Loss: 0.00089313
Iteration 18/25 | Loss: 0.00089313
Iteration 19/25 | Loss: 0.00089313
Iteration 20/25 | Loss: 0.00089313
Iteration 21/25 | Loss: 0.00089313
Iteration 22/25 | Loss: 0.00089313
Iteration 23/25 | Loss: 0.00089313
Iteration 24/25 | Loss: 0.00089313
Iteration 25/25 | Loss: 0.00089313

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00089313
Iteration 2/1000 | Loss: 0.00007348
Iteration 3/1000 | Loss: 0.00010892
Iteration 4/1000 | Loss: 0.00001431
Iteration 5/1000 | Loss: 0.00002841
Iteration 6/1000 | Loss: 0.00005533
Iteration 7/1000 | Loss: 0.00001820
Iteration 8/1000 | Loss: 0.00001266
Iteration 9/1000 | Loss: 0.00001233
Iteration 10/1000 | Loss: 0.00001203
Iteration 11/1000 | Loss: 0.00002786
Iteration 12/1000 | Loss: 0.00001167
Iteration 13/1000 | Loss: 0.00001166
Iteration 14/1000 | Loss: 0.00001147
Iteration 15/1000 | Loss: 0.00001134
Iteration 16/1000 | Loss: 0.00001125
Iteration 17/1000 | Loss: 0.00001125
Iteration 18/1000 | Loss: 0.00001123
Iteration 19/1000 | Loss: 0.00002852
Iteration 20/1000 | Loss: 0.00001757
Iteration 21/1000 | Loss: 0.00001111
Iteration 22/1000 | Loss: 0.00001098
Iteration 23/1000 | Loss: 0.00001098
Iteration 24/1000 | Loss: 0.00001098
Iteration 25/1000 | Loss: 0.00001098
Iteration 26/1000 | Loss: 0.00001098
Iteration 27/1000 | Loss: 0.00001098
Iteration 28/1000 | Loss: 0.00001098
Iteration 29/1000 | Loss: 0.00001098
Iteration 30/1000 | Loss: 0.00001097
Iteration 31/1000 | Loss: 0.00001097
Iteration 32/1000 | Loss: 0.00001097
Iteration 33/1000 | Loss: 0.00001097
Iteration 34/1000 | Loss: 0.00001097
Iteration 35/1000 | Loss: 0.00001097
Iteration 36/1000 | Loss: 0.00001097
Iteration 37/1000 | Loss: 0.00001097
Iteration 38/1000 | Loss: 0.00001097
Iteration 39/1000 | Loss: 0.00001096
Iteration 40/1000 | Loss: 0.00001096
Iteration 41/1000 | Loss: 0.00001096
Iteration 42/1000 | Loss: 0.00001096
Iteration 43/1000 | Loss: 0.00001096
Iteration 44/1000 | Loss: 0.00001096
Iteration 45/1000 | Loss: 0.00001095
Iteration 46/1000 | Loss: 0.00001095
Iteration 47/1000 | Loss: 0.00001095
Iteration 48/1000 | Loss: 0.00001095
Iteration 49/1000 | Loss: 0.00001095
Iteration 50/1000 | Loss: 0.00001094
Iteration 51/1000 | Loss: 0.00001094
Iteration 52/1000 | Loss: 0.00001094
Iteration 53/1000 | Loss: 0.00001094
Iteration 54/1000 | Loss: 0.00001094
Iteration 55/1000 | Loss: 0.00001094
Iteration 56/1000 | Loss: 0.00001094
Iteration 57/1000 | Loss: 0.00001094
Iteration 58/1000 | Loss: 0.00001743
Iteration 59/1000 | Loss: 0.00001090
Iteration 60/1000 | Loss: 0.00003374
Iteration 61/1000 | Loss: 0.00002134
Iteration 62/1000 | Loss: 0.00001148
Iteration 63/1000 | Loss: 0.00002701
Iteration 64/1000 | Loss: 0.00001095
Iteration 65/1000 | Loss: 0.00001087
Iteration 66/1000 | Loss: 0.00001087
Iteration 67/1000 | Loss: 0.00001086
Iteration 68/1000 | Loss: 0.00001086
Iteration 69/1000 | Loss: 0.00001084
Iteration 70/1000 | Loss: 0.00001084
Iteration 71/1000 | Loss: 0.00001084
Iteration 72/1000 | Loss: 0.00001084
Iteration 73/1000 | Loss: 0.00001083
Iteration 74/1000 | Loss: 0.00001083
Iteration 75/1000 | Loss: 0.00001083
Iteration 76/1000 | Loss: 0.00001082
Iteration 77/1000 | Loss: 0.00001082
Iteration 78/1000 | Loss: 0.00001082
Iteration 79/1000 | Loss: 0.00001081
Iteration 80/1000 | Loss: 0.00001081
Iteration 81/1000 | Loss: 0.00001081
Iteration 82/1000 | Loss: 0.00001081
Iteration 83/1000 | Loss: 0.00001080
Iteration 84/1000 | Loss: 0.00001080
Iteration 85/1000 | Loss: 0.00001080
Iteration 86/1000 | Loss: 0.00001080
Iteration 87/1000 | Loss: 0.00001080
Iteration 88/1000 | Loss: 0.00001080
Iteration 89/1000 | Loss: 0.00001080
Iteration 90/1000 | Loss: 0.00001079
Iteration 91/1000 | Loss: 0.00001079
Iteration 92/1000 | Loss: 0.00001079
Iteration 93/1000 | Loss: 0.00001079
Iteration 94/1000 | Loss: 0.00001079
Iteration 95/1000 | Loss: 0.00001079
Iteration 96/1000 | Loss: 0.00001079
Iteration 97/1000 | Loss: 0.00001079
Iteration 98/1000 | Loss: 0.00001079
Iteration 99/1000 | Loss: 0.00001078
Iteration 100/1000 | Loss: 0.00001078
Iteration 101/1000 | Loss: 0.00001078
Iteration 102/1000 | Loss: 0.00001078
Iteration 103/1000 | Loss: 0.00001077
Iteration 104/1000 | Loss: 0.00001077
Iteration 105/1000 | Loss: 0.00001077
Iteration 106/1000 | Loss: 0.00001077
Iteration 107/1000 | Loss: 0.00001077
Iteration 108/1000 | Loss: 0.00001077
Iteration 109/1000 | Loss: 0.00001077
Iteration 110/1000 | Loss: 0.00001077
Iteration 111/1000 | Loss: 0.00001077
Iteration 112/1000 | Loss: 0.00001077
Iteration 113/1000 | Loss: 0.00001076
Iteration 114/1000 | Loss: 0.00001076
Iteration 115/1000 | Loss: 0.00001076
Iteration 116/1000 | Loss: 0.00001076
Iteration 117/1000 | Loss: 0.00001076
Iteration 118/1000 | Loss: 0.00001076
Iteration 119/1000 | Loss: 0.00001076
Iteration 120/1000 | Loss: 0.00001076
Iteration 121/1000 | Loss: 0.00001076
Iteration 122/1000 | Loss: 0.00001076
Iteration 123/1000 | Loss: 0.00001076
Iteration 124/1000 | Loss: 0.00001076
Iteration 125/1000 | Loss: 0.00001075
Iteration 126/1000 | Loss: 0.00001075
Iteration 127/1000 | Loss: 0.00001075
Iteration 128/1000 | Loss: 0.00001075
Iteration 129/1000 | Loss: 0.00001075
Iteration 130/1000 | Loss: 0.00001075
Iteration 131/1000 | Loss: 0.00001074
Iteration 132/1000 | Loss: 0.00001074
Iteration 133/1000 | Loss: 0.00001074
Iteration 134/1000 | Loss: 0.00001074
Iteration 135/1000 | Loss: 0.00001074
Iteration 136/1000 | Loss: 0.00001074
Iteration 137/1000 | Loss: 0.00001074
Iteration 138/1000 | Loss: 0.00001074
Iteration 139/1000 | Loss: 0.00001074
Iteration 140/1000 | Loss: 0.00001074
Iteration 141/1000 | Loss: 0.00001074
Iteration 142/1000 | Loss: 0.00001074
Iteration 143/1000 | Loss: 0.00001074
Iteration 144/1000 | Loss: 0.00001938
Iteration 145/1000 | Loss: 0.00006808
Iteration 146/1000 | Loss: 0.00001083
Iteration 147/1000 | Loss: 0.00002123
Iteration 148/1000 | Loss: 0.00011566
Iteration 149/1000 | Loss: 0.00002447
Iteration 150/1000 | Loss: 0.00001100
Iteration 151/1000 | Loss: 0.00001076
Iteration 152/1000 | Loss: 0.00001076
Iteration 153/1000 | Loss: 0.00001076
Iteration 154/1000 | Loss: 0.00001073
Iteration 155/1000 | Loss: 0.00001073
Iteration 156/1000 | Loss: 0.00001073
Iteration 157/1000 | Loss: 0.00001073
Iteration 158/1000 | Loss: 0.00001073
Iteration 159/1000 | Loss: 0.00001073
Iteration 160/1000 | Loss: 0.00001073
Iteration 161/1000 | Loss: 0.00001073
Iteration 162/1000 | Loss: 0.00001072
Iteration 163/1000 | Loss: 0.00001072
Iteration 164/1000 | Loss: 0.00001072
Iteration 165/1000 | Loss: 0.00001072
Iteration 166/1000 | Loss: 0.00001072
Iteration 167/1000 | Loss: 0.00001072
Iteration 168/1000 | Loss: 0.00001072
Iteration 169/1000 | Loss: 0.00001072
Iteration 170/1000 | Loss: 0.00001072
Iteration 171/1000 | Loss: 0.00001072
Iteration 172/1000 | Loss: 0.00001072
Iteration 173/1000 | Loss: 0.00001072
Iteration 174/1000 | Loss: 0.00001072
Iteration 175/1000 | Loss: 0.00001072
Iteration 176/1000 | Loss: 0.00001072
Iteration 177/1000 | Loss: 0.00001072
Iteration 178/1000 | Loss: 0.00001072
Iteration 179/1000 | Loss: 0.00001072
Iteration 180/1000 | Loss: 0.00001072
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 180. Stopping optimization.
Last 5 losses: [1.0715460120991338e-05, 1.0715460120991338e-05, 1.0715460120991338e-05, 1.0715460120991338e-05, 1.0715460120991338e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0715460120991338e-05

Optimization complete. Final v2v error: 2.8076295852661133 mm

Highest mean error: 3.508234739303589 mm for frame 43

Lowest mean error: 2.501067876815796 mm for frame 20

Saving results

Total time: 75.43399333953857
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_023/1054/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_023/1054.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_023/1054
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00390404
Iteration 2/25 | Loss: 0.00124573
Iteration 3/25 | Loss: 0.00114610
Iteration 4/25 | Loss: 0.00112674
Iteration 5/25 | Loss: 0.00111950
Iteration 6/25 | Loss: 0.00111791
Iteration 7/25 | Loss: 0.00111791
Iteration 8/25 | Loss: 0.00111791
Iteration 9/25 | Loss: 0.00111791
Iteration 10/25 | Loss: 0.00111791
Iteration 11/25 | Loss: 0.00111791
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011179115390405059, 0.0011179115390405059, 0.0011179115390405059, 0.0011179115390405059, 0.0011179115390405059]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011179115390405059

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34195697
Iteration 2/25 | Loss: 0.00105589
Iteration 3/25 | Loss: 0.00105589
Iteration 4/25 | Loss: 0.00105589
Iteration 5/25 | Loss: 0.00105589
Iteration 6/25 | Loss: 0.00105589
Iteration 7/25 | Loss: 0.00105588
Iteration 8/25 | Loss: 0.00105588
Iteration 9/25 | Loss: 0.00105588
Iteration 10/25 | Loss: 0.00105588
Iteration 11/25 | Loss: 0.00105588
Iteration 12/25 | Loss: 0.00105588
Iteration 13/25 | Loss: 0.00105588
Iteration 14/25 | Loss: 0.00105588
Iteration 15/25 | Loss: 0.00105588
Iteration 16/25 | Loss: 0.00105588
Iteration 17/25 | Loss: 0.00105588
Iteration 18/25 | Loss: 0.00105588
Iteration 19/25 | Loss: 0.00105588
Iteration 20/25 | Loss: 0.00105588
Iteration 21/25 | Loss: 0.00105588
Iteration 22/25 | Loss: 0.00105588
Iteration 23/25 | Loss: 0.00105588
Iteration 24/25 | Loss: 0.00105588
Iteration 25/25 | Loss: 0.00105588
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0010558842914178967, 0.0010558842914178967, 0.0010558842914178967, 0.0010558842914178967, 0.0010558842914178967]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010558842914178967

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00105588
Iteration 2/1000 | Loss: 0.00003221
Iteration 3/1000 | Loss: 0.00002234
Iteration 4/1000 | Loss: 0.00002024
Iteration 5/1000 | Loss: 0.00001901
Iteration 6/1000 | Loss: 0.00001793
Iteration 7/1000 | Loss: 0.00001738
Iteration 8/1000 | Loss: 0.00001702
Iteration 9/1000 | Loss: 0.00001672
Iteration 10/1000 | Loss: 0.00001635
Iteration 11/1000 | Loss: 0.00001628
Iteration 12/1000 | Loss: 0.00001618
Iteration 13/1000 | Loss: 0.00001614
Iteration 14/1000 | Loss: 0.00001607
Iteration 15/1000 | Loss: 0.00001594
Iteration 16/1000 | Loss: 0.00001590
Iteration 17/1000 | Loss: 0.00001590
Iteration 18/1000 | Loss: 0.00001587
Iteration 19/1000 | Loss: 0.00001586
Iteration 20/1000 | Loss: 0.00001586
Iteration 21/1000 | Loss: 0.00001586
Iteration 22/1000 | Loss: 0.00001585
Iteration 23/1000 | Loss: 0.00001585
Iteration 24/1000 | Loss: 0.00001584
Iteration 25/1000 | Loss: 0.00001584
Iteration 26/1000 | Loss: 0.00001583
Iteration 27/1000 | Loss: 0.00001583
Iteration 28/1000 | Loss: 0.00001582
Iteration 29/1000 | Loss: 0.00001581
Iteration 30/1000 | Loss: 0.00001581
Iteration 31/1000 | Loss: 0.00001580
Iteration 32/1000 | Loss: 0.00001580
Iteration 33/1000 | Loss: 0.00001580
Iteration 34/1000 | Loss: 0.00001579
Iteration 35/1000 | Loss: 0.00001579
Iteration 36/1000 | Loss: 0.00001578
Iteration 37/1000 | Loss: 0.00001578
Iteration 38/1000 | Loss: 0.00001578
Iteration 39/1000 | Loss: 0.00001578
Iteration 40/1000 | Loss: 0.00001578
Iteration 41/1000 | Loss: 0.00001578
Iteration 42/1000 | Loss: 0.00001578
Iteration 43/1000 | Loss: 0.00001578
Iteration 44/1000 | Loss: 0.00001578
Iteration 45/1000 | Loss: 0.00001578
Iteration 46/1000 | Loss: 0.00001578
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 46. Stopping optimization.
Last 5 losses: [1.5783158232807182e-05, 1.5783158232807182e-05, 1.5783158232807182e-05, 1.5783158232807182e-05, 1.5783158232807182e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5783158232807182e-05

Optimization complete. Final v2v error: 3.296029567718506 mm

Highest mean error: 4.169984340667725 mm for frame 95

Lowest mean error: 2.7596442699432373 mm for frame 73

Saving results

Total time: 32.3510537147522
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_023/1015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_023/1015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_023/1015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00530582
Iteration 2/25 | Loss: 0.00136934
Iteration 3/25 | Loss: 0.00125550
Iteration 4/25 | Loss: 0.00124852
Iteration 5/25 | Loss: 0.00124655
Iteration 6/25 | Loss: 0.00124650
Iteration 7/25 | Loss: 0.00124650
Iteration 8/25 | Loss: 0.00124650
Iteration 9/25 | Loss: 0.00124650
Iteration 10/25 | Loss: 0.00124650
Iteration 11/25 | Loss: 0.00124650
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012464969186112285, 0.0012464969186112285, 0.0012464969186112285, 0.0012464969186112285, 0.0012464969186112285]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012464969186112285

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.47836328
Iteration 2/25 | Loss: 0.00061198
Iteration 3/25 | Loss: 0.00061197
Iteration 4/25 | Loss: 0.00061197
Iteration 5/25 | Loss: 0.00061197
Iteration 6/25 | Loss: 0.00061197
Iteration 7/25 | Loss: 0.00061197
Iteration 8/25 | Loss: 0.00061197
Iteration 9/25 | Loss: 0.00061197
Iteration 10/25 | Loss: 0.00061197
Iteration 11/25 | Loss: 0.00061197
Iteration 12/25 | Loss: 0.00061197
Iteration 13/25 | Loss: 0.00061197
Iteration 14/25 | Loss: 0.00061197
Iteration 15/25 | Loss: 0.00061197
Iteration 16/25 | Loss: 0.00061197
Iteration 17/25 | Loss: 0.00061197
Iteration 18/25 | Loss: 0.00061197
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0006119697354733944, 0.0006119697354733944, 0.0006119697354733944, 0.0006119697354733944, 0.0006119697354733944]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006119697354733944

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00061197
Iteration 2/1000 | Loss: 0.00004929
Iteration 3/1000 | Loss: 0.00003534
Iteration 4/1000 | Loss: 0.00002863
Iteration 5/1000 | Loss: 0.00002654
Iteration 6/1000 | Loss: 0.00002534
Iteration 7/1000 | Loss: 0.00002464
Iteration 8/1000 | Loss: 0.00002403
Iteration 9/1000 | Loss: 0.00002355
Iteration 10/1000 | Loss: 0.00002317
Iteration 11/1000 | Loss: 0.00002286
Iteration 12/1000 | Loss: 0.00002270
Iteration 13/1000 | Loss: 0.00002264
Iteration 14/1000 | Loss: 0.00002261
Iteration 15/1000 | Loss: 0.00002246
Iteration 16/1000 | Loss: 0.00002237
Iteration 17/1000 | Loss: 0.00002227
Iteration 18/1000 | Loss: 0.00002226
Iteration 19/1000 | Loss: 0.00002225
Iteration 20/1000 | Loss: 0.00002222
Iteration 21/1000 | Loss: 0.00002222
Iteration 22/1000 | Loss: 0.00002221
Iteration 23/1000 | Loss: 0.00002220
Iteration 24/1000 | Loss: 0.00002220
Iteration 25/1000 | Loss: 0.00002219
Iteration 26/1000 | Loss: 0.00002218
Iteration 27/1000 | Loss: 0.00002218
Iteration 28/1000 | Loss: 0.00002217
Iteration 29/1000 | Loss: 0.00002217
Iteration 30/1000 | Loss: 0.00002217
Iteration 31/1000 | Loss: 0.00002217
Iteration 32/1000 | Loss: 0.00002217
Iteration 33/1000 | Loss: 0.00002217
Iteration 34/1000 | Loss: 0.00002217
Iteration 35/1000 | Loss: 0.00002216
Iteration 36/1000 | Loss: 0.00002216
Iteration 37/1000 | Loss: 0.00002216
Iteration 38/1000 | Loss: 0.00002215
Iteration 39/1000 | Loss: 0.00002215
Iteration 40/1000 | Loss: 0.00002215
Iteration 41/1000 | Loss: 0.00002214
Iteration 42/1000 | Loss: 0.00002214
Iteration 43/1000 | Loss: 0.00002214
Iteration 44/1000 | Loss: 0.00002214
Iteration 45/1000 | Loss: 0.00002214
Iteration 46/1000 | Loss: 0.00002214
Iteration 47/1000 | Loss: 0.00002214
Iteration 48/1000 | Loss: 0.00002214
Iteration 49/1000 | Loss: 0.00002214
Iteration 50/1000 | Loss: 0.00002214
Iteration 51/1000 | Loss: 0.00002214
Iteration 52/1000 | Loss: 0.00002214
Iteration 53/1000 | Loss: 0.00002214
Iteration 54/1000 | Loss: 0.00002214
Iteration 55/1000 | Loss: 0.00002214
Iteration 56/1000 | Loss: 0.00002214
Iteration 57/1000 | Loss: 0.00002214
Iteration 58/1000 | Loss: 0.00002214
Iteration 59/1000 | Loss: 0.00002214
Iteration 60/1000 | Loss: 0.00002214
Iteration 61/1000 | Loss: 0.00002214
Iteration 62/1000 | Loss: 0.00002214
Iteration 63/1000 | Loss: 0.00002214
Iteration 64/1000 | Loss: 0.00002214
Iteration 65/1000 | Loss: 0.00002214
Iteration 66/1000 | Loss: 0.00002214
Iteration 67/1000 | Loss: 0.00002214
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 67. Stopping optimization.
Last 5 losses: [2.2136313418741338e-05, 2.2136313418741338e-05, 2.2136313418741338e-05, 2.2136313418741338e-05, 2.2136313418741338e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2136313418741338e-05

Optimization complete. Final v2v error: 3.937150001525879 mm

Highest mean error: 4.342922687530518 mm for frame 78

Lowest mean error: 3.649498462677002 mm for frame 13

Saving results

Total time: 31.199288606643677
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_023/1053/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_023/1053.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_023/1053
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00555208
Iteration 2/25 | Loss: 0.00134964
Iteration 3/25 | Loss: 0.00119575
Iteration 4/25 | Loss: 0.00117216
Iteration 5/25 | Loss: 0.00116874
Iteration 6/25 | Loss: 0.00116656
Iteration 7/25 | Loss: 0.00116605
Iteration 8/25 | Loss: 0.00116598
Iteration 9/25 | Loss: 0.00116598
Iteration 10/25 | Loss: 0.00116598
Iteration 11/25 | Loss: 0.00116597
Iteration 12/25 | Loss: 0.00116597
Iteration 13/25 | Loss: 0.00116597
Iteration 14/25 | Loss: 0.00116597
Iteration 15/25 | Loss: 0.00116597
Iteration 16/25 | Loss: 0.00116597
Iteration 17/25 | Loss: 0.00116597
Iteration 18/25 | Loss: 0.00116597
Iteration 19/25 | Loss: 0.00116596
Iteration 20/25 | Loss: 0.00116596
Iteration 21/25 | Loss: 0.00116596
Iteration 22/25 | Loss: 0.00116596
Iteration 23/25 | Loss: 0.00116596
Iteration 24/25 | Loss: 0.00116596
Iteration 25/25 | Loss: 0.00116596

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.22871161
Iteration 2/25 | Loss: 0.00085030
Iteration 3/25 | Loss: 0.00080142
Iteration 4/25 | Loss: 0.00080142
Iteration 5/25 | Loss: 0.00080142
Iteration 6/25 | Loss: 0.00080142
Iteration 7/25 | Loss: 0.00080142
Iteration 8/25 | Loss: 0.00080142
Iteration 9/25 | Loss: 0.00080142
Iteration 10/25 | Loss: 0.00080142
Iteration 11/25 | Loss: 0.00080142
Iteration 12/25 | Loss: 0.00080141
Iteration 13/25 | Loss: 0.00080141
Iteration 14/25 | Loss: 0.00080141
Iteration 15/25 | Loss: 0.00080141
Iteration 16/25 | Loss: 0.00080141
Iteration 17/25 | Loss: 0.00080141
Iteration 18/25 | Loss: 0.00080141
Iteration 19/25 | Loss: 0.00080141
Iteration 20/25 | Loss: 0.00080141
Iteration 21/25 | Loss: 0.00080141
Iteration 22/25 | Loss: 0.00080141
Iteration 23/25 | Loss: 0.00080141
Iteration 24/25 | Loss: 0.00080141
Iteration 25/25 | Loss: 0.00080141

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00080141
Iteration 2/1000 | Loss: 0.00009973
Iteration 3/1000 | Loss: 0.00001660
Iteration 4/1000 | Loss: 0.00002294
Iteration 5/1000 | Loss: 0.00001865
Iteration 6/1000 | Loss: 0.00004510
Iteration 7/1000 | Loss: 0.00014913
Iteration 8/1000 | Loss: 0.00047237
Iteration 9/1000 | Loss: 0.00001753
Iteration 10/1000 | Loss: 0.00001368
Iteration 11/1000 | Loss: 0.00001342
Iteration 12/1000 | Loss: 0.00001327
Iteration 13/1000 | Loss: 0.00001326
Iteration 14/1000 | Loss: 0.00001326
Iteration 15/1000 | Loss: 0.00002591
Iteration 16/1000 | Loss: 0.00001305
Iteration 17/1000 | Loss: 0.00001299
Iteration 18/1000 | Loss: 0.00001286
Iteration 19/1000 | Loss: 0.00001280
Iteration 20/1000 | Loss: 0.00001275
Iteration 21/1000 | Loss: 0.00001272
Iteration 22/1000 | Loss: 0.00001271
Iteration 23/1000 | Loss: 0.00001271
Iteration 24/1000 | Loss: 0.00001271
Iteration 25/1000 | Loss: 0.00001271
Iteration 26/1000 | Loss: 0.00001270
Iteration 27/1000 | Loss: 0.00001270
Iteration 28/1000 | Loss: 0.00001270
Iteration 29/1000 | Loss: 0.00001270
Iteration 30/1000 | Loss: 0.00001270
Iteration 31/1000 | Loss: 0.00001269
Iteration 32/1000 | Loss: 0.00001269
Iteration 33/1000 | Loss: 0.00001268
Iteration 34/1000 | Loss: 0.00001268
Iteration 35/1000 | Loss: 0.00001267
Iteration 36/1000 | Loss: 0.00001267
Iteration 37/1000 | Loss: 0.00001266
Iteration 38/1000 | Loss: 0.00001266
Iteration 39/1000 | Loss: 0.00001265
Iteration 40/1000 | Loss: 0.00001265
Iteration 41/1000 | Loss: 0.00001264
Iteration 42/1000 | Loss: 0.00001264
Iteration 43/1000 | Loss: 0.00001263
Iteration 44/1000 | Loss: 0.00001263
Iteration 45/1000 | Loss: 0.00001262
Iteration 46/1000 | Loss: 0.00001262
Iteration 47/1000 | Loss: 0.00001262
Iteration 48/1000 | Loss: 0.00001262
Iteration 49/1000 | Loss: 0.00001262
Iteration 50/1000 | Loss: 0.00001261
Iteration 51/1000 | Loss: 0.00001261
Iteration 52/1000 | Loss: 0.00001261
Iteration 53/1000 | Loss: 0.00001260
Iteration 54/1000 | Loss: 0.00001260
Iteration 55/1000 | Loss: 0.00001259
Iteration 56/1000 | Loss: 0.00001259
Iteration 57/1000 | Loss: 0.00001259
Iteration 58/1000 | Loss: 0.00001259
Iteration 59/1000 | Loss: 0.00001259
Iteration 60/1000 | Loss: 0.00001259
Iteration 61/1000 | Loss: 0.00001258
Iteration 62/1000 | Loss: 0.00001258
Iteration 63/1000 | Loss: 0.00001258
Iteration 64/1000 | Loss: 0.00001258
Iteration 65/1000 | Loss: 0.00001258
Iteration 66/1000 | Loss: 0.00003072
Iteration 67/1000 | Loss: 0.00001267
Iteration 68/1000 | Loss: 0.00001254
Iteration 69/1000 | Loss: 0.00001254
Iteration 70/1000 | Loss: 0.00001254
Iteration 71/1000 | Loss: 0.00001254
Iteration 72/1000 | Loss: 0.00001254
Iteration 73/1000 | Loss: 0.00001254
Iteration 74/1000 | Loss: 0.00001254
Iteration 75/1000 | Loss: 0.00001254
Iteration 76/1000 | Loss: 0.00001253
Iteration 77/1000 | Loss: 0.00001253
Iteration 78/1000 | Loss: 0.00001253
Iteration 79/1000 | Loss: 0.00001253
Iteration 80/1000 | Loss: 0.00001253
Iteration 81/1000 | Loss: 0.00001253
Iteration 82/1000 | Loss: 0.00001253
Iteration 83/1000 | Loss: 0.00001253
Iteration 84/1000 | Loss: 0.00001253
Iteration 85/1000 | Loss: 0.00001253
Iteration 86/1000 | Loss: 0.00001253
Iteration 87/1000 | Loss: 0.00001253
Iteration 88/1000 | Loss: 0.00001253
Iteration 89/1000 | Loss: 0.00001252
Iteration 90/1000 | Loss: 0.00001252
Iteration 91/1000 | Loss: 0.00001252
Iteration 92/1000 | Loss: 0.00001252
Iteration 93/1000 | Loss: 0.00001252
Iteration 94/1000 | Loss: 0.00001252
Iteration 95/1000 | Loss: 0.00001251
Iteration 96/1000 | Loss: 0.00001251
Iteration 97/1000 | Loss: 0.00001251
Iteration 98/1000 | Loss: 0.00001251
Iteration 99/1000 | Loss: 0.00001251
Iteration 100/1000 | Loss: 0.00001251
Iteration 101/1000 | Loss: 0.00001251
Iteration 102/1000 | Loss: 0.00001251
Iteration 103/1000 | Loss: 0.00001251
Iteration 104/1000 | Loss: 0.00001251
Iteration 105/1000 | Loss: 0.00001251
Iteration 106/1000 | Loss: 0.00001251
Iteration 107/1000 | Loss: 0.00001251
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 107. Stopping optimization.
Last 5 losses: [1.2507336577982642e-05, 1.2507336577982642e-05, 1.2507336577982642e-05, 1.2507336577982642e-05, 1.2507336577982642e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2507336577982642e-05

Optimization complete. Final v2v error: 2.9983890056610107 mm

Highest mean error: 3.5202646255493164 mm for frame 213

Lowest mean error: 2.745997667312622 mm for frame 172

Saving results

Total time: 51.74033570289612
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_023/1037/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_023/1037.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_023/1037
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00598808
Iteration 2/25 | Loss: 0.00118493
Iteration 3/25 | Loss: 0.00112831
Iteration 4/25 | Loss: 0.00111976
Iteration 5/25 | Loss: 0.00111661
Iteration 6/25 | Loss: 0.00111661
Iteration 7/25 | Loss: 0.00111661
Iteration 8/25 | Loss: 0.00111661
Iteration 9/25 | Loss: 0.00111661
Iteration 10/25 | Loss: 0.00111661
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0011166105978190899, 0.0011166105978190899, 0.0011166105978190899, 0.0011166105978190899, 0.0011166105978190899]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011166105978190899

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.90214014
Iteration 2/25 | Loss: 0.00086933
Iteration 3/25 | Loss: 0.00086933
Iteration 4/25 | Loss: 0.00086933
Iteration 5/25 | Loss: 0.00086933
Iteration 6/25 | Loss: 0.00086933
Iteration 7/25 | Loss: 0.00086933
Iteration 8/25 | Loss: 0.00086933
Iteration 9/25 | Loss: 0.00086933
Iteration 10/25 | Loss: 0.00086933
Iteration 11/25 | Loss: 0.00086933
Iteration 12/25 | Loss: 0.00086933
Iteration 13/25 | Loss: 0.00086933
Iteration 14/25 | Loss: 0.00086933
Iteration 15/25 | Loss: 0.00086933
Iteration 16/25 | Loss: 0.00086933
Iteration 17/25 | Loss: 0.00086933
Iteration 18/25 | Loss: 0.00086933
Iteration 19/25 | Loss: 0.00086933
Iteration 20/25 | Loss: 0.00086933
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.000869331939611584, 0.000869331939611584, 0.000869331939611584, 0.000869331939611584, 0.000869331939611584]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000869331939611584

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00086933
Iteration 2/1000 | Loss: 0.00002012
Iteration 3/1000 | Loss: 0.00001435
Iteration 4/1000 | Loss: 0.00001269
Iteration 5/1000 | Loss: 0.00001202
Iteration 6/1000 | Loss: 0.00001163
Iteration 7/1000 | Loss: 0.00001127
Iteration 8/1000 | Loss: 0.00001119
Iteration 9/1000 | Loss: 0.00001108
Iteration 10/1000 | Loss: 0.00001098
Iteration 11/1000 | Loss: 0.00001078
Iteration 12/1000 | Loss: 0.00001069
Iteration 13/1000 | Loss: 0.00001064
Iteration 14/1000 | Loss: 0.00001057
Iteration 15/1000 | Loss: 0.00001056
Iteration 16/1000 | Loss: 0.00001056
Iteration 17/1000 | Loss: 0.00001055
Iteration 18/1000 | Loss: 0.00001049
Iteration 19/1000 | Loss: 0.00001047
Iteration 20/1000 | Loss: 0.00001032
Iteration 21/1000 | Loss: 0.00001031
Iteration 22/1000 | Loss: 0.00001022
Iteration 23/1000 | Loss: 0.00001022
Iteration 24/1000 | Loss: 0.00001020
Iteration 25/1000 | Loss: 0.00001017
Iteration 26/1000 | Loss: 0.00001016
Iteration 27/1000 | Loss: 0.00001015
Iteration 28/1000 | Loss: 0.00001015
Iteration 29/1000 | Loss: 0.00001014
Iteration 30/1000 | Loss: 0.00001014
Iteration 31/1000 | Loss: 0.00001014
Iteration 32/1000 | Loss: 0.00001013
Iteration 33/1000 | Loss: 0.00001013
Iteration 34/1000 | Loss: 0.00001012
Iteration 35/1000 | Loss: 0.00001010
Iteration 36/1000 | Loss: 0.00001009
Iteration 37/1000 | Loss: 0.00001009
Iteration 38/1000 | Loss: 0.00001009
Iteration 39/1000 | Loss: 0.00001009
Iteration 40/1000 | Loss: 0.00001009
Iteration 41/1000 | Loss: 0.00001009
Iteration 42/1000 | Loss: 0.00001009
Iteration 43/1000 | Loss: 0.00001009
Iteration 44/1000 | Loss: 0.00001009
Iteration 45/1000 | Loss: 0.00001008
Iteration 46/1000 | Loss: 0.00001008
Iteration 47/1000 | Loss: 0.00001007
Iteration 48/1000 | Loss: 0.00001006
Iteration 49/1000 | Loss: 0.00001005
Iteration 50/1000 | Loss: 0.00001005
Iteration 51/1000 | Loss: 0.00001004
Iteration 52/1000 | Loss: 0.00001004
Iteration 53/1000 | Loss: 0.00001003
Iteration 54/1000 | Loss: 0.00001002
Iteration 55/1000 | Loss: 0.00001002
Iteration 56/1000 | Loss: 0.00001002
Iteration 57/1000 | Loss: 0.00001001
Iteration 58/1000 | Loss: 0.00001001
Iteration 59/1000 | Loss: 0.00001001
Iteration 60/1000 | Loss: 0.00001000
Iteration 61/1000 | Loss: 0.00001000
Iteration 62/1000 | Loss: 0.00000999
Iteration 63/1000 | Loss: 0.00000998
Iteration 64/1000 | Loss: 0.00000998
Iteration 65/1000 | Loss: 0.00000998
Iteration 66/1000 | Loss: 0.00000998
Iteration 67/1000 | Loss: 0.00000998
Iteration 68/1000 | Loss: 0.00000998
Iteration 69/1000 | Loss: 0.00000997
Iteration 70/1000 | Loss: 0.00000997
Iteration 71/1000 | Loss: 0.00000996
Iteration 72/1000 | Loss: 0.00000996
Iteration 73/1000 | Loss: 0.00000996
Iteration 74/1000 | Loss: 0.00000996
Iteration 75/1000 | Loss: 0.00000996
Iteration 76/1000 | Loss: 0.00000995
Iteration 77/1000 | Loss: 0.00000995
Iteration 78/1000 | Loss: 0.00000994
Iteration 79/1000 | Loss: 0.00000994
Iteration 80/1000 | Loss: 0.00000993
Iteration 81/1000 | Loss: 0.00000992
Iteration 82/1000 | Loss: 0.00000991
Iteration 83/1000 | Loss: 0.00000990
Iteration 84/1000 | Loss: 0.00000990
Iteration 85/1000 | Loss: 0.00000990
Iteration 86/1000 | Loss: 0.00000989
Iteration 87/1000 | Loss: 0.00000988
Iteration 88/1000 | Loss: 0.00000988
Iteration 89/1000 | Loss: 0.00000987
Iteration 90/1000 | Loss: 0.00000987
Iteration 91/1000 | Loss: 0.00000987
Iteration 92/1000 | Loss: 0.00000987
Iteration 93/1000 | Loss: 0.00000987
Iteration 94/1000 | Loss: 0.00000986
Iteration 95/1000 | Loss: 0.00000986
Iteration 96/1000 | Loss: 0.00000986
Iteration 97/1000 | Loss: 0.00000986
Iteration 98/1000 | Loss: 0.00000986
Iteration 99/1000 | Loss: 0.00000986
Iteration 100/1000 | Loss: 0.00000986
Iteration 101/1000 | Loss: 0.00000986
Iteration 102/1000 | Loss: 0.00000986
Iteration 103/1000 | Loss: 0.00000986
Iteration 104/1000 | Loss: 0.00000986
Iteration 105/1000 | Loss: 0.00000986
Iteration 106/1000 | Loss: 0.00000986
Iteration 107/1000 | Loss: 0.00000986
Iteration 108/1000 | Loss: 0.00000986
Iteration 109/1000 | Loss: 0.00000986
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 109. Stopping optimization.
Last 5 losses: [9.863469131232705e-06, 9.863469131232705e-06, 9.863469131232705e-06, 9.863469131232705e-06, 9.863469131232705e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.863469131232705e-06

Optimization complete. Final v2v error: 2.7078707218170166 mm

Highest mean error: 2.917224645614624 mm for frame 99

Lowest mean error: 2.5524954795837402 mm for frame 123

Saving results

Total time: 32.96597218513489
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_023/1051/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_023/1051.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_023/1051
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00950409
Iteration 2/25 | Loss: 0.00189214
Iteration 3/25 | Loss: 0.00144215
Iteration 4/25 | Loss: 0.00142822
Iteration 5/25 | Loss: 0.00142553
Iteration 6/25 | Loss: 0.00142519
Iteration 7/25 | Loss: 0.00142519
Iteration 8/25 | Loss: 0.00142519
Iteration 9/25 | Loss: 0.00142519
Iteration 10/25 | Loss: 0.00142519
Iteration 11/25 | Loss: 0.00142519
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0014251888496801257, 0.0014251888496801257, 0.0014251888496801257, 0.0014251888496801257, 0.0014251888496801257]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014251888496801257

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.22700489
Iteration 2/25 | Loss: 0.00103710
Iteration 3/25 | Loss: 0.00103710
Iteration 4/25 | Loss: 0.00103710
Iteration 5/25 | Loss: 0.00103710
Iteration 6/25 | Loss: 0.00103710
Iteration 7/25 | Loss: 0.00103710
Iteration 8/25 | Loss: 0.00103710
Iteration 9/25 | Loss: 0.00103710
Iteration 10/25 | Loss: 0.00103710
Iteration 11/25 | Loss: 0.00103710
Iteration 12/25 | Loss: 0.00103710
Iteration 13/25 | Loss: 0.00103710
Iteration 14/25 | Loss: 0.00103710
Iteration 15/25 | Loss: 0.00103710
Iteration 16/25 | Loss: 0.00103710
Iteration 17/25 | Loss: 0.00103710
Iteration 18/25 | Loss: 0.00103710
Iteration 19/25 | Loss: 0.00103710
Iteration 20/25 | Loss: 0.00103710
Iteration 21/25 | Loss: 0.00103710
Iteration 22/25 | Loss: 0.00103710
Iteration 23/25 | Loss: 0.00103710
Iteration 24/25 | Loss: 0.00103710
Iteration 25/25 | Loss: 0.00103710

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00103710
Iteration 2/1000 | Loss: 0.00007455
Iteration 3/1000 | Loss: 0.00005209
Iteration 4/1000 | Loss: 0.00004155
Iteration 5/1000 | Loss: 0.00003914
Iteration 6/1000 | Loss: 0.00003755
Iteration 7/1000 | Loss: 0.00003660
Iteration 8/1000 | Loss: 0.00003583
Iteration 9/1000 | Loss: 0.00003502
Iteration 10/1000 | Loss: 0.00003440
Iteration 11/1000 | Loss: 0.00003377
Iteration 12/1000 | Loss: 0.00003335
Iteration 13/1000 | Loss: 0.00003300
Iteration 14/1000 | Loss: 0.00003276
Iteration 15/1000 | Loss: 0.00003250
Iteration 16/1000 | Loss: 0.00003229
Iteration 17/1000 | Loss: 0.00003210
Iteration 18/1000 | Loss: 0.00003193
Iteration 19/1000 | Loss: 0.00003188
Iteration 20/1000 | Loss: 0.00003188
Iteration 21/1000 | Loss: 0.00003185
Iteration 22/1000 | Loss: 0.00003183
Iteration 23/1000 | Loss: 0.00003170
Iteration 24/1000 | Loss: 0.00003170
Iteration 25/1000 | Loss: 0.00003166
Iteration 26/1000 | Loss: 0.00003150
Iteration 27/1000 | Loss: 0.00003148
Iteration 28/1000 | Loss: 0.00003148
Iteration 29/1000 | Loss: 0.00003148
Iteration 30/1000 | Loss: 0.00003135
Iteration 31/1000 | Loss: 0.00003132
Iteration 32/1000 | Loss: 0.00003131
Iteration 33/1000 | Loss: 0.00003130
Iteration 34/1000 | Loss: 0.00003129
Iteration 35/1000 | Loss: 0.00003126
Iteration 36/1000 | Loss: 0.00003124
Iteration 37/1000 | Loss: 0.00003121
Iteration 38/1000 | Loss: 0.00003120
Iteration 39/1000 | Loss: 0.00003120
Iteration 40/1000 | Loss: 0.00003120
Iteration 41/1000 | Loss: 0.00003119
Iteration 42/1000 | Loss: 0.00003118
Iteration 43/1000 | Loss: 0.00003118
Iteration 44/1000 | Loss: 0.00003117
Iteration 45/1000 | Loss: 0.00003116
Iteration 46/1000 | Loss: 0.00003116
Iteration 47/1000 | Loss: 0.00003116
Iteration 48/1000 | Loss: 0.00003116
Iteration 49/1000 | Loss: 0.00003114
Iteration 50/1000 | Loss: 0.00003108
Iteration 51/1000 | Loss: 0.00003108
Iteration 52/1000 | Loss: 0.00003108
Iteration 53/1000 | Loss: 0.00003108
Iteration 54/1000 | Loss: 0.00003108
Iteration 55/1000 | Loss: 0.00003108
Iteration 56/1000 | Loss: 0.00003108
Iteration 57/1000 | Loss: 0.00003108
Iteration 58/1000 | Loss: 0.00003108
Iteration 59/1000 | Loss: 0.00003108
Iteration 60/1000 | Loss: 0.00003108
Iteration 61/1000 | Loss: 0.00003107
Iteration 62/1000 | Loss: 0.00003107
Iteration 63/1000 | Loss: 0.00003107
Iteration 64/1000 | Loss: 0.00003107
Iteration 65/1000 | Loss: 0.00003107
Iteration 66/1000 | Loss: 0.00003107
Iteration 67/1000 | Loss: 0.00003107
Iteration 68/1000 | Loss: 0.00003107
Iteration 69/1000 | Loss: 0.00003107
Iteration 70/1000 | Loss: 0.00003107
Iteration 71/1000 | Loss: 0.00003107
Iteration 72/1000 | Loss: 0.00003106
Iteration 73/1000 | Loss: 0.00003106
Iteration 74/1000 | Loss: 0.00003105
Iteration 75/1000 | Loss: 0.00003105
Iteration 76/1000 | Loss: 0.00003105
Iteration 77/1000 | Loss: 0.00003104
Iteration 78/1000 | Loss: 0.00003104
Iteration 79/1000 | Loss: 0.00003104
Iteration 80/1000 | Loss: 0.00003104
Iteration 81/1000 | Loss: 0.00003104
Iteration 82/1000 | Loss: 0.00003104
Iteration 83/1000 | Loss: 0.00003104
Iteration 84/1000 | Loss: 0.00003104
Iteration 85/1000 | Loss: 0.00003104
Iteration 86/1000 | Loss: 0.00003103
Iteration 87/1000 | Loss: 0.00003103
Iteration 88/1000 | Loss: 0.00003103
Iteration 89/1000 | Loss: 0.00003103
Iteration 90/1000 | Loss: 0.00003103
Iteration 91/1000 | Loss: 0.00003102
Iteration 92/1000 | Loss: 0.00003102
Iteration 93/1000 | Loss: 0.00003102
Iteration 94/1000 | Loss: 0.00003101
Iteration 95/1000 | Loss: 0.00003101
Iteration 96/1000 | Loss: 0.00003101
Iteration 97/1000 | Loss: 0.00003101
Iteration 98/1000 | Loss: 0.00003101
Iteration 99/1000 | Loss: 0.00003100
Iteration 100/1000 | Loss: 0.00003100
Iteration 101/1000 | Loss: 0.00003100
Iteration 102/1000 | Loss: 0.00003100
Iteration 103/1000 | Loss: 0.00003100
Iteration 104/1000 | Loss: 0.00003100
Iteration 105/1000 | Loss: 0.00003100
Iteration 106/1000 | Loss: 0.00003100
Iteration 107/1000 | Loss: 0.00003100
Iteration 108/1000 | Loss: 0.00003100
Iteration 109/1000 | Loss: 0.00003099
Iteration 110/1000 | Loss: 0.00003099
Iteration 111/1000 | Loss: 0.00003099
Iteration 112/1000 | Loss: 0.00003099
Iteration 113/1000 | Loss: 0.00003099
Iteration 114/1000 | Loss: 0.00003099
Iteration 115/1000 | Loss: 0.00003099
Iteration 116/1000 | Loss: 0.00003099
Iteration 117/1000 | Loss: 0.00003098
Iteration 118/1000 | Loss: 0.00003098
Iteration 119/1000 | Loss: 0.00003098
Iteration 120/1000 | Loss: 0.00003097
Iteration 121/1000 | Loss: 0.00003097
Iteration 122/1000 | Loss: 0.00003097
Iteration 123/1000 | Loss: 0.00003097
Iteration 124/1000 | Loss: 0.00003097
Iteration 125/1000 | Loss: 0.00003097
Iteration 126/1000 | Loss: 0.00003097
Iteration 127/1000 | Loss: 0.00003097
Iteration 128/1000 | Loss: 0.00003097
Iteration 129/1000 | Loss: 0.00003096
Iteration 130/1000 | Loss: 0.00003096
Iteration 131/1000 | Loss: 0.00003096
Iteration 132/1000 | Loss: 0.00003096
Iteration 133/1000 | Loss: 0.00003096
Iteration 134/1000 | Loss: 0.00003096
Iteration 135/1000 | Loss: 0.00003096
Iteration 136/1000 | Loss: 0.00003095
Iteration 137/1000 | Loss: 0.00003095
Iteration 138/1000 | Loss: 0.00003095
Iteration 139/1000 | Loss: 0.00003094
Iteration 140/1000 | Loss: 0.00003094
Iteration 141/1000 | Loss: 0.00003094
Iteration 142/1000 | Loss: 0.00003094
Iteration 143/1000 | Loss: 0.00003094
Iteration 144/1000 | Loss: 0.00003094
Iteration 145/1000 | Loss: 0.00003094
Iteration 146/1000 | Loss: 0.00003094
Iteration 147/1000 | Loss: 0.00003094
Iteration 148/1000 | Loss: 0.00003094
Iteration 149/1000 | Loss: 0.00003094
Iteration 150/1000 | Loss: 0.00003094
Iteration 151/1000 | Loss: 0.00003094
Iteration 152/1000 | Loss: 0.00003094
Iteration 153/1000 | Loss: 0.00003094
Iteration 154/1000 | Loss: 0.00003094
Iteration 155/1000 | Loss: 0.00003093
Iteration 156/1000 | Loss: 0.00003093
Iteration 157/1000 | Loss: 0.00003093
Iteration 158/1000 | Loss: 0.00003093
Iteration 159/1000 | Loss: 0.00003093
Iteration 160/1000 | Loss: 0.00003093
Iteration 161/1000 | Loss: 0.00003093
Iteration 162/1000 | Loss: 0.00003093
Iteration 163/1000 | Loss: 0.00003093
Iteration 164/1000 | Loss: 0.00003093
Iteration 165/1000 | Loss: 0.00003093
Iteration 166/1000 | Loss: 0.00003093
Iteration 167/1000 | Loss: 0.00003093
Iteration 168/1000 | Loss: 0.00003093
Iteration 169/1000 | Loss: 0.00003093
Iteration 170/1000 | Loss: 0.00003093
Iteration 171/1000 | Loss: 0.00003093
Iteration 172/1000 | Loss: 0.00003093
Iteration 173/1000 | Loss: 0.00003093
Iteration 174/1000 | Loss: 0.00003093
Iteration 175/1000 | Loss: 0.00003093
Iteration 176/1000 | Loss: 0.00003093
Iteration 177/1000 | Loss: 0.00003093
Iteration 178/1000 | Loss: 0.00003093
Iteration 179/1000 | Loss: 0.00003093
Iteration 180/1000 | Loss: 0.00003093
Iteration 181/1000 | Loss: 0.00003093
Iteration 182/1000 | Loss: 0.00003093
Iteration 183/1000 | Loss: 0.00003093
Iteration 184/1000 | Loss: 0.00003093
Iteration 185/1000 | Loss: 0.00003093
Iteration 186/1000 | Loss: 0.00003093
Iteration 187/1000 | Loss: 0.00003093
Iteration 188/1000 | Loss: 0.00003093
Iteration 189/1000 | Loss: 0.00003093
Iteration 190/1000 | Loss: 0.00003093
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 190. Stopping optimization.
Last 5 losses: [3.0931096262065694e-05, 3.0931096262065694e-05, 3.0931096262065694e-05, 3.0931096262065694e-05, 3.0931096262065694e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.0931096262065694e-05

Optimization complete. Final v2v error: 4.539204120635986 mm

Highest mean error: 5.202789783477783 mm for frame 90

Lowest mean error: 4.116387844085693 mm for frame 59

Saving results

Total time: 50.64572882652283
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_004/1042/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_004/1042.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_004/1042
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01063497
Iteration 2/25 | Loss: 0.00171196
Iteration 3/25 | Loss: 0.00132027
Iteration 4/25 | Loss: 0.00123913
Iteration 5/25 | Loss: 0.00123610
Iteration 6/25 | Loss: 0.00120917
Iteration 7/25 | Loss: 0.00118895
Iteration 8/25 | Loss: 0.00118123
Iteration 9/25 | Loss: 0.00118359
Iteration 10/25 | Loss: 0.00117815
Iteration 11/25 | Loss: 0.00118090
Iteration 12/25 | Loss: 0.00117772
Iteration 13/25 | Loss: 0.00117769
Iteration 14/25 | Loss: 0.00117769
Iteration 15/25 | Loss: 0.00118032
Iteration 16/25 | Loss: 0.00117775
Iteration 17/25 | Loss: 0.00117767
Iteration 18/25 | Loss: 0.00117767
Iteration 19/25 | Loss: 0.00117767
Iteration 20/25 | Loss: 0.00117766
Iteration 21/25 | Loss: 0.00117766
Iteration 22/25 | Loss: 0.00117766
Iteration 23/25 | Loss: 0.00117766
Iteration 24/25 | Loss: 0.00117766
Iteration 25/25 | Loss: 0.00117766

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 9.07780552
Iteration 2/25 | Loss: 0.00100027
Iteration 3/25 | Loss: 0.00100005
Iteration 4/25 | Loss: 0.00100005
Iteration 5/25 | Loss: 0.00100005
Iteration 6/25 | Loss: 0.00100005
Iteration 7/25 | Loss: 0.00100005
Iteration 8/25 | Loss: 0.00100005
Iteration 9/25 | Loss: 0.00100005
Iteration 10/25 | Loss: 0.00100005
Iteration 11/25 | Loss: 0.00100005
Iteration 12/25 | Loss: 0.00100005
Iteration 13/25 | Loss: 0.00100005
Iteration 14/25 | Loss: 0.00100005
Iteration 15/25 | Loss: 0.00100005
Iteration 16/25 | Loss: 0.00100005
Iteration 17/25 | Loss: 0.00100005
Iteration 18/25 | Loss: 0.00100005
Iteration 19/25 | Loss: 0.00100005
Iteration 20/25 | Loss: 0.00100005
Iteration 21/25 | Loss: 0.00100005
Iteration 22/25 | Loss: 0.00100005
Iteration 23/25 | Loss: 0.00100005
Iteration 24/25 | Loss: 0.00100005
Iteration 25/25 | Loss: 0.00100005

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00100005
Iteration 2/1000 | Loss: 0.00009799
Iteration 3/1000 | Loss: 0.00010159
Iteration 4/1000 | Loss: 0.00006595
Iteration 5/1000 | Loss: 0.00004777
Iteration 6/1000 | Loss: 0.00004254
Iteration 7/1000 | Loss: 0.00004062
Iteration 8/1000 | Loss: 0.00003936
Iteration 9/1000 | Loss: 0.00010918
Iteration 10/1000 | Loss: 0.00003833
Iteration 11/1000 | Loss: 0.00003754
Iteration 12/1000 | Loss: 0.00003702
Iteration 13/1000 | Loss: 0.00003665
Iteration 14/1000 | Loss: 0.00010643
Iteration 15/1000 | Loss: 0.00003656
Iteration 16/1000 | Loss: 0.00003606
Iteration 17/1000 | Loss: 0.00003580
Iteration 18/1000 | Loss: 0.00003573
Iteration 19/1000 | Loss: 0.00008946
Iteration 20/1000 | Loss: 0.00004075
Iteration 21/1000 | Loss: 0.00007579
Iteration 22/1000 | Loss: 0.00005499
Iteration 23/1000 | Loss: 0.00007592
Iteration 24/1000 | Loss: 0.00023452
Iteration 25/1000 | Loss: 0.00003534
Iteration 26/1000 | Loss: 0.00003533
Iteration 27/1000 | Loss: 0.00003531
Iteration 28/1000 | Loss: 0.00003531
Iteration 29/1000 | Loss: 0.00003531
Iteration 30/1000 | Loss: 0.00003530
Iteration 31/1000 | Loss: 0.00003530
Iteration 32/1000 | Loss: 0.00003529
Iteration 33/1000 | Loss: 0.00003529
Iteration 34/1000 | Loss: 0.00003529
Iteration 35/1000 | Loss: 0.00003528
Iteration 36/1000 | Loss: 0.00003528
Iteration 37/1000 | Loss: 0.00003528
Iteration 38/1000 | Loss: 0.00003527
Iteration 39/1000 | Loss: 0.00003527
Iteration 40/1000 | Loss: 0.00003527
Iteration 41/1000 | Loss: 0.00003526
Iteration 42/1000 | Loss: 0.00003525
Iteration 43/1000 | Loss: 0.00003525
Iteration 44/1000 | Loss: 0.00007427
Iteration 45/1000 | Loss: 0.00028274
Iteration 46/1000 | Loss: 0.00003548
Iteration 47/1000 | Loss: 0.00003519
Iteration 48/1000 | Loss: 0.00003518
Iteration 49/1000 | Loss: 0.00003514
Iteration 50/1000 | Loss: 0.00003514
Iteration 51/1000 | Loss: 0.00003512
Iteration 52/1000 | Loss: 0.00003512
Iteration 53/1000 | Loss: 0.00003510
Iteration 54/1000 | Loss: 0.00008086
Iteration 55/1000 | Loss: 0.00014447
Iteration 56/1000 | Loss: 0.00003510
Iteration 57/1000 | Loss: 0.00008200
Iteration 58/1000 | Loss: 0.00004232
Iteration 59/1000 | Loss: 0.00006664
Iteration 60/1000 | Loss: 0.00005194
Iteration 61/1000 | Loss: 0.00005165
Iteration 62/1000 | Loss: 0.00003500
Iteration 63/1000 | Loss: 0.00003493
Iteration 64/1000 | Loss: 0.00003492
Iteration 65/1000 | Loss: 0.00003491
Iteration 66/1000 | Loss: 0.00003491
Iteration 67/1000 | Loss: 0.00003491
Iteration 68/1000 | Loss: 0.00003491
Iteration 69/1000 | Loss: 0.00003491
Iteration 70/1000 | Loss: 0.00003491
Iteration 71/1000 | Loss: 0.00003491
Iteration 72/1000 | Loss: 0.00003490
Iteration 73/1000 | Loss: 0.00003490
Iteration 74/1000 | Loss: 0.00003489
Iteration 75/1000 | Loss: 0.00003489
Iteration 76/1000 | Loss: 0.00003488
Iteration 77/1000 | Loss: 0.00003488
Iteration 78/1000 | Loss: 0.00003488
Iteration 79/1000 | Loss: 0.00003487
Iteration 80/1000 | Loss: 0.00003484
Iteration 81/1000 | Loss: 0.00003483
Iteration 82/1000 | Loss: 0.00003483
Iteration 83/1000 | Loss: 0.00003482
Iteration 84/1000 | Loss: 0.00003482
Iteration 85/1000 | Loss: 0.00003482
Iteration 86/1000 | Loss: 0.00003481
Iteration 87/1000 | Loss: 0.00003481
Iteration 88/1000 | Loss: 0.00003481
Iteration 89/1000 | Loss: 0.00003480
Iteration 90/1000 | Loss: 0.00003480
Iteration 91/1000 | Loss: 0.00003479
Iteration 92/1000 | Loss: 0.00003478
Iteration 93/1000 | Loss: 0.00003477
Iteration 94/1000 | Loss: 0.00003477
Iteration 95/1000 | Loss: 0.00003477
Iteration 96/1000 | Loss: 0.00003476
Iteration 97/1000 | Loss: 0.00003476
Iteration 98/1000 | Loss: 0.00003476
Iteration 99/1000 | Loss: 0.00003475
Iteration 100/1000 | Loss: 0.00003475
Iteration 101/1000 | Loss: 0.00003475
Iteration 102/1000 | Loss: 0.00003474
Iteration 103/1000 | Loss: 0.00003474
Iteration 104/1000 | Loss: 0.00003474
Iteration 105/1000 | Loss: 0.00003474
Iteration 106/1000 | Loss: 0.00003474
Iteration 107/1000 | Loss: 0.00003473
Iteration 108/1000 | Loss: 0.00003472
Iteration 109/1000 | Loss: 0.00003472
Iteration 110/1000 | Loss: 0.00003472
Iteration 111/1000 | Loss: 0.00003472
Iteration 112/1000 | Loss: 0.00003471
Iteration 113/1000 | Loss: 0.00003471
Iteration 114/1000 | Loss: 0.00003471
Iteration 115/1000 | Loss: 0.00003471
Iteration 116/1000 | Loss: 0.00003470
Iteration 117/1000 | Loss: 0.00003470
Iteration 118/1000 | Loss: 0.00003470
Iteration 119/1000 | Loss: 0.00003470
Iteration 120/1000 | Loss: 0.00003470
Iteration 121/1000 | Loss: 0.00003469
Iteration 122/1000 | Loss: 0.00003469
Iteration 123/1000 | Loss: 0.00003469
Iteration 124/1000 | Loss: 0.00003469
Iteration 125/1000 | Loss: 0.00003469
Iteration 126/1000 | Loss: 0.00003469
Iteration 127/1000 | Loss: 0.00003469
Iteration 128/1000 | Loss: 0.00003469
Iteration 129/1000 | Loss: 0.00003468
Iteration 130/1000 | Loss: 0.00003468
Iteration 131/1000 | Loss: 0.00003468
Iteration 132/1000 | Loss: 0.00003468
Iteration 133/1000 | Loss: 0.00003468
Iteration 134/1000 | Loss: 0.00003468
Iteration 135/1000 | Loss: 0.00003468
Iteration 136/1000 | Loss: 0.00003468
Iteration 137/1000 | Loss: 0.00003468
Iteration 138/1000 | Loss: 0.00003468
Iteration 139/1000 | Loss: 0.00003468
Iteration 140/1000 | Loss: 0.00003468
Iteration 141/1000 | Loss: 0.00003468
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 141. Stopping optimization.
Last 5 losses: [3.4675304050324485e-05, 3.4675304050324485e-05, 3.4675304050324485e-05, 3.4675304050324485e-05, 3.4675304050324485e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.4675304050324485e-05

Optimization complete. Final v2v error: 4.666433334350586 mm

Highest mean error: 7.251336097717285 mm for frame 99

Lowest mean error: 3.129638910293579 mm for frame 140

Saving results

Total time: 83.83642029762268
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_004/1022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_004/1022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_004/1022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00435594
Iteration 2/25 | Loss: 0.00116803
Iteration 3/25 | Loss: 0.00109421
Iteration 4/25 | Loss: 0.00107549
Iteration 5/25 | Loss: 0.00106927
Iteration 6/25 | Loss: 0.00106796
Iteration 7/25 | Loss: 0.00106782
Iteration 8/25 | Loss: 0.00106782
Iteration 9/25 | Loss: 0.00106782
Iteration 10/25 | Loss: 0.00106782
Iteration 11/25 | Loss: 0.00106782
Iteration 12/25 | Loss: 0.00106782
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0010678169783204794, 0.0010678169783204794, 0.0010678169783204794, 0.0010678169783204794, 0.0010678169783204794]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010678169783204794

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44960153
Iteration 2/25 | Loss: 0.00074414
Iteration 3/25 | Loss: 0.00074414
Iteration 4/25 | Loss: 0.00074414
Iteration 5/25 | Loss: 0.00074414
Iteration 6/25 | Loss: 0.00074413
Iteration 7/25 | Loss: 0.00074413
Iteration 8/25 | Loss: 0.00074413
Iteration 9/25 | Loss: 0.00074413
Iteration 10/25 | Loss: 0.00074413
Iteration 11/25 | Loss: 0.00074413
Iteration 12/25 | Loss: 0.00074413
Iteration 13/25 | Loss: 0.00074413
Iteration 14/25 | Loss: 0.00074413
Iteration 15/25 | Loss: 0.00074413
Iteration 16/25 | Loss: 0.00074413
Iteration 17/25 | Loss: 0.00074413
Iteration 18/25 | Loss: 0.00074413
Iteration 19/25 | Loss: 0.00074413
Iteration 20/25 | Loss: 0.00074413
Iteration 21/25 | Loss: 0.00074413
Iteration 22/25 | Loss: 0.00074413
Iteration 23/25 | Loss: 0.00074413
Iteration 24/25 | Loss: 0.00074413
Iteration 25/25 | Loss: 0.00074413

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00074413
Iteration 2/1000 | Loss: 0.00002423
Iteration 3/1000 | Loss: 0.00001554
Iteration 4/1000 | Loss: 0.00001435
Iteration 5/1000 | Loss: 0.00001390
Iteration 6/1000 | Loss: 0.00001355
Iteration 7/1000 | Loss: 0.00001324
Iteration 8/1000 | Loss: 0.00001308
Iteration 9/1000 | Loss: 0.00001308
Iteration 10/1000 | Loss: 0.00001291
Iteration 11/1000 | Loss: 0.00001270
Iteration 12/1000 | Loss: 0.00001268
Iteration 13/1000 | Loss: 0.00001260
Iteration 14/1000 | Loss: 0.00001260
Iteration 15/1000 | Loss: 0.00001259
Iteration 16/1000 | Loss: 0.00001253
Iteration 17/1000 | Loss: 0.00001252
Iteration 18/1000 | Loss: 0.00001247
Iteration 19/1000 | Loss: 0.00001245
Iteration 20/1000 | Loss: 0.00001244
Iteration 21/1000 | Loss: 0.00001242
Iteration 22/1000 | Loss: 0.00001241
Iteration 23/1000 | Loss: 0.00001241
Iteration 24/1000 | Loss: 0.00001240
Iteration 25/1000 | Loss: 0.00001238
Iteration 26/1000 | Loss: 0.00001237
Iteration 27/1000 | Loss: 0.00001236
Iteration 28/1000 | Loss: 0.00001236
Iteration 29/1000 | Loss: 0.00001236
Iteration 30/1000 | Loss: 0.00001236
Iteration 31/1000 | Loss: 0.00001236
Iteration 32/1000 | Loss: 0.00001235
Iteration 33/1000 | Loss: 0.00001235
Iteration 34/1000 | Loss: 0.00001235
Iteration 35/1000 | Loss: 0.00001235
Iteration 36/1000 | Loss: 0.00001234
Iteration 37/1000 | Loss: 0.00001232
Iteration 38/1000 | Loss: 0.00001231
Iteration 39/1000 | Loss: 0.00001230
Iteration 40/1000 | Loss: 0.00001230
Iteration 41/1000 | Loss: 0.00001230
Iteration 42/1000 | Loss: 0.00001230
Iteration 43/1000 | Loss: 0.00001229
Iteration 44/1000 | Loss: 0.00001229
Iteration 45/1000 | Loss: 0.00001229
Iteration 46/1000 | Loss: 0.00001228
Iteration 47/1000 | Loss: 0.00001228
Iteration 48/1000 | Loss: 0.00001228
Iteration 49/1000 | Loss: 0.00001227
Iteration 50/1000 | Loss: 0.00001227
Iteration 51/1000 | Loss: 0.00001226
Iteration 52/1000 | Loss: 0.00001226
Iteration 53/1000 | Loss: 0.00001226
Iteration 54/1000 | Loss: 0.00001226
Iteration 55/1000 | Loss: 0.00001226
Iteration 56/1000 | Loss: 0.00001226
Iteration 57/1000 | Loss: 0.00001226
Iteration 58/1000 | Loss: 0.00001225
Iteration 59/1000 | Loss: 0.00001225
Iteration 60/1000 | Loss: 0.00001225
Iteration 61/1000 | Loss: 0.00001224
Iteration 62/1000 | Loss: 0.00001224
Iteration 63/1000 | Loss: 0.00001223
Iteration 64/1000 | Loss: 0.00001223
Iteration 65/1000 | Loss: 0.00001223
Iteration 66/1000 | Loss: 0.00001222
Iteration 67/1000 | Loss: 0.00001222
Iteration 68/1000 | Loss: 0.00001222
Iteration 69/1000 | Loss: 0.00001221
Iteration 70/1000 | Loss: 0.00001221
Iteration 71/1000 | Loss: 0.00001221
Iteration 72/1000 | Loss: 0.00001220
Iteration 73/1000 | Loss: 0.00001220
Iteration 74/1000 | Loss: 0.00001220
Iteration 75/1000 | Loss: 0.00001219
Iteration 76/1000 | Loss: 0.00001219
Iteration 77/1000 | Loss: 0.00001219
Iteration 78/1000 | Loss: 0.00001219
Iteration 79/1000 | Loss: 0.00001219
Iteration 80/1000 | Loss: 0.00001219
Iteration 81/1000 | Loss: 0.00001218
Iteration 82/1000 | Loss: 0.00001218
Iteration 83/1000 | Loss: 0.00001218
Iteration 84/1000 | Loss: 0.00001218
Iteration 85/1000 | Loss: 0.00001217
Iteration 86/1000 | Loss: 0.00001217
Iteration 87/1000 | Loss: 0.00001217
Iteration 88/1000 | Loss: 0.00001216
Iteration 89/1000 | Loss: 0.00001216
Iteration 90/1000 | Loss: 0.00001215
Iteration 91/1000 | Loss: 0.00001214
Iteration 92/1000 | Loss: 0.00001214
Iteration 93/1000 | Loss: 0.00001214
Iteration 94/1000 | Loss: 0.00001213
Iteration 95/1000 | Loss: 0.00001213
Iteration 96/1000 | Loss: 0.00001213
Iteration 97/1000 | Loss: 0.00001213
Iteration 98/1000 | Loss: 0.00001213
Iteration 99/1000 | Loss: 0.00001213
Iteration 100/1000 | Loss: 0.00001212
Iteration 101/1000 | Loss: 0.00001212
Iteration 102/1000 | Loss: 0.00001212
Iteration 103/1000 | Loss: 0.00001212
Iteration 104/1000 | Loss: 0.00001212
Iteration 105/1000 | Loss: 0.00001211
Iteration 106/1000 | Loss: 0.00001211
Iteration 107/1000 | Loss: 0.00001211
Iteration 108/1000 | Loss: 0.00001211
Iteration 109/1000 | Loss: 0.00001211
Iteration 110/1000 | Loss: 0.00001211
Iteration 111/1000 | Loss: 0.00001211
Iteration 112/1000 | Loss: 0.00001210
Iteration 113/1000 | Loss: 0.00001210
Iteration 114/1000 | Loss: 0.00001210
Iteration 115/1000 | Loss: 0.00001209
Iteration 116/1000 | Loss: 0.00001209
Iteration 117/1000 | Loss: 0.00001209
Iteration 118/1000 | Loss: 0.00001209
Iteration 119/1000 | Loss: 0.00001209
Iteration 120/1000 | Loss: 0.00001208
Iteration 121/1000 | Loss: 0.00001208
Iteration 122/1000 | Loss: 0.00001208
Iteration 123/1000 | Loss: 0.00001207
Iteration 124/1000 | Loss: 0.00001207
Iteration 125/1000 | Loss: 0.00001207
Iteration 126/1000 | Loss: 0.00001207
Iteration 127/1000 | Loss: 0.00001207
Iteration 128/1000 | Loss: 0.00001207
Iteration 129/1000 | Loss: 0.00001207
Iteration 130/1000 | Loss: 0.00001206
Iteration 131/1000 | Loss: 0.00001206
Iteration 132/1000 | Loss: 0.00001206
Iteration 133/1000 | Loss: 0.00001206
Iteration 134/1000 | Loss: 0.00001206
Iteration 135/1000 | Loss: 0.00001206
Iteration 136/1000 | Loss: 0.00001206
Iteration 137/1000 | Loss: 0.00001206
Iteration 138/1000 | Loss: 0.00001206
Iteration 139/1000 | Loss: 0.00001206
Iteration 140/1000 | Loss: 0.00001206
Iteration 141/1000 | Loss: 0.00001206
Iteration 142/1000 | Loss: 0.00001206
Iteration 143/1000 | Loss: 0.00001206
Iteration 144/1000 | Loss: 0.00001206
Iteration 145/1000 | Loss: 0.00001205
Iteration 146/1000 | Loss: 0.00001205
Iteration 147/1000 | Loss: 0.00001205
Iteration 148/1000 | Loss: 0.00001205
Iteration 149/1000 | Loss: 0.00001205
Iteration 150/1000 | Loss: 0.00001205
Iteration 151/1000 | Loss: 0.00001205
Iteration 152/1000 | Loss: 0.00001205
Iteration 153/1000 | Loss: 0.00001205
Iteration 154/1000 | Loss: 0.00001205
Iteration 155/1000 | Loss: 0.00001204
Iteration 156/1000 | Loss: 0.00001204
Iteration 157/1000 | Loss: 0.00001204
Iteration 158/1000 | Loss: 0.00001204
Iteration 159/1000 | Loss: 0.00001204
Iteration 160/1000 | Loss: 0.00001204
Iteration 161/1000 | Loss: 0.00001204
Iteration 162/1000 | Loss: 0.00001204
Iteration 163/1000 | Loss: 0.00001204
Iteration 164/1000 | Loss: 0.00001204
Iteration 165/1000 | Loss: 0.00001204
Iteration 166/1000 | Loss: 0.00001204
Iteration 167/1000 | Loss: 0.00001204
Iteration 168/1000 | Loss: 0.00001204
Iteration 169/1000 | Loss: 0.00001204
Iteration 170/1000 | Loss: 0.00001203
Iteration 171/1000 | Loss: 0.00001203
Iteration 172/1000 | Loss: 0.00001203
Iteration 173/1000 | Loss: 0.00001203
Iteration 174/1000 | Loss: 0.00001203
Iteration 175/1000 | Loss: 0.00001203
Iteration 176/1000 | Loss: 0.00001203
Iteration 177/1000 | Loss: 0.00001203
Iteration 178/1000 | Loss: 0.00001203
Iteration 179/1000 | Loss: 0.00001203
Iteration 180/1000 | Loss: 0.00001203
Iteration 181/1000 | Loss: 0.00001203
Iteration 182/1000 | Loss: 0.00001203
Iteration 183/1000 | Loss: 0.00001202
Iteration 184/1000 | Loss: 0.00001202
Iteration 185/1000 | Loss: 0.00001202
Iteration 186/1000 | Loss: 0.00001202
Iteration 187/1000 | Loss: 0.00001202
Iteration 188/1000 | Loss: 0.00001202
Iteration 189/1000 | Loss: 0.00001202
Iteration 190/1000 | Loss: 0.00001202
Iteration 191/1000 | Loss: 0.00001202
Iteration 192/1000 | Loss: 0.00001202
Iteration 193/1000 | Loss: 0.00001202
Iteration 194/1000 | Loss: 0.00001202
Iteration 195/1000 | Loss: 0.00001202
Iteration 196/1000 | Loss: 0.00001202
Iteration 197/1000 | Loss: 0.00001202
Iteration 198/1000 | Loss: 0.00001202
Iteration 199/1000 | Loss: 0.00001202
Iteration 200/1000 | Loss: 0.00001201
Iteration 201/1000 | Loss: 0.00001201
Iteration 202/1000 | Loss: 0.00001201
Iteration 203/1000 | Loss: 0.00001201
Iteration 204/1000 | Loss: 0.00001201
Iteration 205/1000 | Loss: 0.00001201
Iteration 206/1000 | Loss: 0.00001201
Iteration 207/1000 | Loss: 0.00001201
Iteration 208/1000 | Loss: 0.00001201
Iteration 209/1000 | Loss: 0.00001201
Iteration 210/1000 | Loss: 0.00001201
Iteration 211/1000 | Loss: 0.00001201
Iteration 212/1000 | Loss: 0.00001201
Iteration 213/1000 | Loss: 0.00001201
Iteration 214/1000 | Loss: 0.00001201
Iteration 215/1000 | Loss: 0.00001201
Iteration 216/1000 | Loss: 0.00001201
Iteration 217/1000 | Loss: 0.00001201
Iteration 218/1000 | Loss: 0.00001201
Iteration 219/1000 | Loss: 0.00001201
Iteration 220/1000 | Loss: 0.00001201
Iteration 221/1000 | Loss: 0.00001201
Iteration 222/1000 | Loss: 0.00001201
Iteration 223/1000 | Loss: 0.00001201
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 223. Stopping optimization.
Last 5 losses: [1.2012164916086476e-05, 1.2012164916086476e-05, 1.2012164916086476e-05, 1.2012164916086476e-05, 1.2012164916086476e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2012164916086476e-05

Optimization complete. Final v2v error: 2.958364248275757 mm

Highest mean error: 3.4867377281188965 mm for frame 77

Lowest mean error: 2.8234119415283203 mm for frame 29

Saving results

Total time: 39.20763087272644
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_004/1000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_004/1000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_004/1000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01033999
Iteration 2/25 | Loss: 0.01033999
Iteration 3/25 | Loss: 0.01033998
Iteration 4/25 | Loss: 0.01033998
Iteration 5/25 | Loss: 0.00271813
Iteration 6/25 | Loss: 0.00170941
Iteration 7/25 | Loss: 0.00138608
Iteration 8/25 | Loss: 0.00137596
Iteration 9/25 | Loss: 0.00131752
Iteration 10/25 | Loss: 0.00123005
Iteration 11/25 | Loss: 0.00119747
Iteration 12/25 | Loss: 0.00117984
Iteration 13/25 | Loss: 0.00116335
Iteration 14/25 | Loss: 0.00115439
Iteration 15/25 | Loss: 0.00116422
Iteration 16/25 | Loss: 0.00115540
Iteration 17/25 | Loss: 0.00115093
Iteration 18/25 | Loss: 0.00114971
Iteration 19/25 | Loss: 0.00115614
Iteration 20/25 | Loss: 0.00114640
Iteration 21/25 | Loss: 0.00115231
Iteration 22/25 | Loss: 0.00115556
Iteration 23/25 | Loss: 0.00114947
Iteration 24/25 | Loss: 0.00114758
Iteration 25/25 | Loss: 0.00114826

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.40697289
Iteration 2/25 | Loss: 0.00216539
Iteration 3/25 | Loss: 0.00172238
Iteration 4/25 | Loss: 0.00172238
Iteration 5/25 | Loss: 0.00172238
Iteration 6/25 | Loss: 0.00172238
Iteration 7/25 | Loss: 0.00172238
Iteration 8/25 | Loss: 0.00172238
Iteration 9/25 | Loss: 0.00172238
Iteration 10/25 | Loss: 0.00172238
Iteration 11/25 | Loss: 0.00172237
Iteration 12/25 | Loss: 0.00172237
Iteration 13/25 | Loss: 0.00172237
Iteration 14/25 | Loss: 0.00172237
Iteration 15/25 | Loss: 0.00172237
Iteration 16/25 | Loss: 0.00172237
Iteration 17/25 | Loss: 0.00172237
Iteration 18/25 | Loss: 0.00172237
Iteration 19/25 | Loss: 0.00172237
Iteration 20/25 | Loss: 0.00172237
Iteration 21/25 | Loss: 0.00172237
Iteration 22/25 | Loss: 0.00172237
Iteration 23/25 | Loss: 0.00172237
Iteration 24/25 | Loss: 0.00172237
Iteration 25/25 | Loss: 0.00172237

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00172237
Iteration 2/1000 | Loss: 0.00060520
Iteration 3/1000 | Loss: 0.00110613
Iteration 4/1000 | Loss: 0.00061475
Iteration 5/1000 | Loss: 0.00052879
Iteration 6/1000 | Loss: 0.00020470
Iteration 7/1000 | Loss: 0.00059535
Iteration 8/1000 | Loss: 0.00033347
Iteration 9/1000 | Loss: 0.00036391
Iteration 10/1000 | Loss: 0.00032211
Iteration 11/1000 | Loss: 0.00044206
Iteration 12/1000 | Loss: 0.00046914
Iteration 13/1000 | Loss: 0.00029210
Iteration 14/1000 | Loss: 0.00086685
Iteration 15/1000 | Loss: 0.00062396
Iteration 16/1000 | Loss: 0.00070032
Iteration 17/1000 | Loss: 0.00040215
Iteration 18/1000 | Loss: 0.00033588
Iteration 19/1000 | Loss: 0.00037777
Iteration 20/1000 | Loss: 0.00116873
Iteration 21/1000 | Loss: 0.00081250
Iteration 22/1000 | Loss: 0.00025066
Iteration 23/1000 | Loss: 0.00026713
Iteration 24/1000 | Loss: 0.00037179
Iteration 25/1000 | Loss: 0.00029813
Iteration 26/1000 | Loss: 0.00029485
Iteration 27/1000 | Loss: 0.00037495
Iteration 28/1000 | Loss: 0.00059057
Iteration 29/1000 | Loss: 0.00029661
Iteration 30/1000 | Loss: 0.00043291
Iteration 31/1000 | Loss: 0.00024202
Iteration 32/1000 | Loss: 0.00039627
Iteration 33/1000 | Loss: 0.00019671
Iteration 34/1000 | Loss: 0.00037022
Iteration 35/1000 | Loss: 0.00022098
Iteration 36/1000 | Loss: 0.00046084
Iteration 37/1000 | Loss: 0.00023323
Iteration 38/1000 | Loss: 0.00021589
Iteration 39/1000 | Loss: 0.00031287
Iteration 40/1000 | Loss: 0.00023893
Iteration 41/1000 | Loss: 0.00013746
Iteration 42/1000 | Loss: 0.00032547
Iteration 43/1000 | Loss: 0.00020875
Iteration 44/1000 | Loss: 0.00008194
Iteration 45/1000 | Loss: 0.00016404
Iteration 46/1000 | Loss: 0.00017248
Iteration 47/1000 | Loss: 0.00008492
Iteration 48/1000 | Loss: 0.00030561
Iteration 49/1000 | Loss: 0.00036232
Iteration 50/1000 | Loss: 0.00022366
Iteration 51/1000 | Loss: 0.00017024
Iteration 52/1000 | Loss: 0.00045074
Iteration 53/1000 | Loss: 0.00067466
Iteration 54/1000 | Loss: 0.00036329
Iteration 55/1000 | Loss: 0.00032876
Iteration 56/1000 | Loss: 0.00060246
Iteration 57/1000 | Loss: 0.00058457
Iteration 58/1000 | Loss: 0.00096500
Iteration 59/1000 | Loss: 0.00063432
Iteration 60/1000 | Loss: 0.00008872
Iteration 61/1000 | Loss: 0.00012783
Iteration 62/1000 | Loss: 0.00020408
Iteration 63/1000 | Loss: 0.00024199
Iteration 64/1000 | Loss: 0.00008811
Iteration 65/1000 | Loss: 0.00013477
Iteration 66/1000 | Loss: 0.00015324
Iteration 67/1000 | Loss: 0.00012106
Iteration 68/1000 | Loss: 0.00010637
Iteration 69/1000 | Loss: 0.00028151
Iteration 70/1000 | Loss: 0.00056874
Iteration 71/1000 | Loss: 0.00049342
Iteration 72/1000 | Loss: 0.00018378
Iteration 73/1000 | Loss: 0.00033956
Iteration 74/1000 | Loss: 0.00046858
Iteration 75/1000 | Loss: 0.00052149
Iteration 76/1000 | Loss: 0.00056592
Iteration 77/1000 | Loss: 0.00053029
Iteration 78/1000 | Loss: 0.00053202
Iteration 79/1000 | Loss: 0.00018003
Iteration 80/1000 | Loss: 0.00018329
Iteration 81/1000 | Loss: 0.00016715
Iteration 82/1000 | Loss: 0.00018000
Iteration 83/1000 | Loss: 0.00025611
Iteration 84/1000 | Loss: 0.00009577
Iteration 85/1000 | Loss: 0.00011343
Iteration 86/1000 | Loss: 0.00012739
Iteration 87/1000 | Loss: 0.00013243
Iteration 88/1000 | Loss: 0.00011060
Iteration 89/1000 | Loss: 0.00012265
Iteration 90/1000 | Loss: 0.00010230
Iteration 91/1000 | Loss: 0.00011436
Iteration 92/1000 | Loss: 0.00023234
Iteration 93/1000 | Loss: 0.00027273
Iteration 94/1000 | Loss: 0.00006086
Iteration 95/1000 | Loss: 0.00018994
Iteration 96/1000 | Loss: 0.00024202
Iteration 97/1000 | Loss: 0.00013871
Iteration 98/1000 | Loss: 0.00006050
Iteration 99/1000 | Loss: 0.00008535
Iteration 100/1000 | Loss: 0.00009718
Iteration 101/1000 | Loss: 0.00008605
Iteration 102/1000 | Loss: 0.00008924
Iteration 103/1000 | Loss: 0.00024575
Iteration 104/1000 | Loss: 0.00005613
Iteration 105/1000 | Loss: 0.00008927
Iteration 106/1000 | Loss: 0.00004580
Iteration 107/1000 | Loss: 0.00004899
Iteration 108/1000 | Loss: 0.00024175
Iteration 109/1000 | Loss: 0.00012048
Iteration 110/1000 | Loss: 0.00030847
Iteration 111/1000 | Loss: 0.00030611
Iteration 112/1000 | Loss: 0.00004605
Iteration 113/1000 | Loss: 0.00012925
Iteration 114/1000 | Loss: 0.00118285
Iteration 115/1000 | Loss: 0.00057982
Iteration 116/1000 | Loss: 0.00176031
Iteration 117/1000 | Loss: 0.00014716
Iteration 118/1000 | Loss: 0.00014771
Iteration 119/1000 | Loss: 0.00005509
Iteration 120/1000 | Loss: 0.00014525
Iteration 121/1000 | Loss: 0.00004648
Iteration 122/1000 | Loss: 0.00007659
Iteration 123/1000 | Loss: 0.00032761
Iteration 124/1000 | Loss: 0.00006444
Iteration 125/1000 | Loss: 0.00012262
Iteration 126/1000 | Loss: 0.00003949
Iteration 127/1000 | Loss: 0.00025448
Iteration 128/1000 | Loss: 0.00008145
Iteration 129/1000 | Loss: 0.00019446
Iteration 130/1000 | Loss: 0.00005883
Iteration 131/1000 | Loss: 0.00006175
Iteration 132/1000 | Loss: 0.00004639
Iteration 133/1000 | Loss: 0.00004851
Iteration 134/1000 | Loss: 0.00009062
Iteration 135/1000 | Loss: 0.00003746
Iteration 136/1000 | Loss: 0.00004868
Iteration 137/1000 | Loss: 0.00004753
Iteration 138/1000 | Loss: 0.00022480
Iteration 139/1000 | Loss: 0.00004946
Iteration 140/1000 | Loss: 0.00003361
Iteration 141/1000 | Loss: 0.00004918
Iteration 142/1000 | Loss: 0.00011952
Iteration 143/1000 | Loss: 0.00032463
Iteration 144/1000 | Loss: 0.00030382
Iteration 145/1000 | Loss: 0.00010747
Iteration 146/1000 | Loss: 0.00030520
Iteration 147/1000 | Loss: 0.00021370
Iteration 148/1000 | Loss: 0.00022406
Iteration 149/1000 | Loss: 0.00004386
Iteration 150/1000 | Loss: 0.00011325
Iteration 151/1000 | Loss: 0.00010361
Iteration 152/1000 | Loss: 0.00075998
Iteration 153/1000 | Loss: 0.00158340
Iteration 154/1000 | Loss: 0.00010753
Iteration 155/1000 | Loss: 0.00014063
Iteration 156/1000 | Loss: 0.00012186
Iteration 157/1000 | Loss: 0.00008879
Iteration 158/1000 | Loss: 0.00011985
Iteration 159/1000 | Loss: 0.00004977
Iteration 160/1000 | Loss: 0.00019765
Iteration 161/1000 | Loss: 0.00087791
Iteration 162/1000 | Loss: 0.00146767
Iteration 163/1000 | Loss: 0.00136508
Iteration 164/1000 | Loss: 0.00062939
Iteration 165/1000 | Loss: 0.00042098
Iteration 166/1000 | Loss: 0.00006435
Iteration 167/1000 | Loss: 0.00019520
Iteration 168/1000 | Loss: 0.00032454
Iteration 169/1000 | Loss: 0.00020219
Iteration 170/1000 | Loss: 0.00003847
Iteration 171/1000 | Loss: 0.00009593
Iteration 172/1000 | Loss: 0.00003253
Iteration 173/1000 | Loss: 0.00025764
Iteration 174/1000 | Loss: 0.00013414
Iteration 175/1000 | Loss: 0.00020665
Iteration 176/1000 | Loss: 0.00015257
Iteration 177/1000 | Loss: 0.00003731
Iteration 178/1000 | Loss: 0.00002871
Iteration 179/1000 | Loss: 0.00003186
Iteration 180/1000 | Loss: 0.00005184
Iteration 181/1000 | Loss: 0.00002632
Iteration 182/1000 | Loss: 0.00006751
Iteration 183/1000 | Loss: 0.00008796
Iteration 184/1000 | Loss: 0.00016307
Iteration 185/1000 | Loss: 0.00003475
Iteration 186/1000 | Loss: 0.00008253
Iteration 187/1000 | Loss: 0.00007139
Iteration 188/1000 | Loss: 0.00009367
Iteration 189/1000 | Loss: 0.00021266
Iteration 190/1000 | Loss: 0.00006469
Iteration 191/1000 | Loss: 0.00002575
Iteration 192/1000 | Loss: 0.00013173
Iteration 193/1000 | Loss: 0.00006309
Iteration 194/1000 | Loss: 0.00002549
Iteration 195/1000 | Loss: 0.00002890
Iteration 196/1000 | Loss: 0.00002943
Iteration 197/1000 | Loss: 0.00002274
Iteration 198/1000 | Loss: 0.00015614
Iteration 199/1000 | Loss: 0.00008084
Iteration 200/1000 | Loss: 0.00014954
Iteration 201/1000 | Loss: 0.00002652
Iteration 202/1000 | Loss: 0.00004046
Iteration 203/1000 | Loss: 0.00011797
Iteration 204/1000 | Loss: 0.00015300
Iteration 205/1000 | Loss: 0.00002986
Iteration 206/1000 | Loss: 0.00012690
Iteration 207/1000 | Loss: 0.00015126
Iteration 208/1000 | Loss: 0.00002328
Iteration 209/1000 | Loss: 0.00007006
Iteration 210/1000 | Loss: 0.00015862
Iteration 211/1000 | Loss: 0.00002056
Iteration 212/1000 | Loss: 0.00003183
Iteration 213/1000 | Loss: 0.00001970
Iteration 214/1000 | Loss: 0.00001935
Iteration 215/1000 | Loss: 0.00001925
Iteration 216/1000 | Loss: 0.00001911
Iteration 217/1000 | Loss: 0.00001910
Iteration 218/1000 | Loss: 0.00007939
Iteration 219/1000 | Loss: 0.00002717
Iteration 220/1000 | Loss: 0.00002204
Iteration 221/1000 | Loss: 0.00002030
Iteration 222/1000 | Loss: 0.00001890
Iteration 223/1000 | Loss: 0.00008178
Iteration 224/1000 | Loss: 0.00004327
Iteration 225/1000 | Loss: 0.00001784
Iteration 226/1000 | Loss: 0.00009265
Iteration 227/1000 | Loss: 0.00001766
Iteration 228/1000 | Loss: 0.00001751
Iteration 229/1000 | Loss: 0.00001749
Iteration 230/1000 | Loss: 0.00001734
Iteration 231/1000 | Loss: 0.00001733
Iteration 232/1000 | Loss: 0.00001725
Iteration 233/1000 | Loss: 0.00001723
Iteration 234/1000 | Loss: 0.00001723
Iteration 235/1000 | Loss: 0.00001722
Iteration 236/1000 | Loss: 0.00001722
Iteration 237/1000 | Loss: 0.00001721
Iteration 238/1000 | Loss: 0.00001721
Iteration 239/1000 | Loss: 0.00001720
Iteration 240/1000 | Loss: 0.00001720
Iteration 241/1000 | Loss: 0.00001720
Iteration 242/1000 | Loss: 0.00001720
Iteration 243/1000 | Loss: 0.00001719
Iteration 244/1000 | Loss: 0.00001719
Iteration 245/1000 | Loss: 0.00001719
Iteration 246/1000 | Loss: 0.00001719
Iteration 247/1000 | Loss: 0.00001719
Iteration 248/1000 | Loss: 0.00001719
Iteration 249/1000 | Loss: 0.00001719
Iteration 250/1000 | Loss: 0.00001719
Iteration 251/1000 | Loss: 0.00001718
Iteration 252/1000 | Loss: 0.00001718
Iteration 253/1000 | Loss: 0.00001718
Iteration 254/1000 | Loss: 0.00001718
Iteration 255/1000 | Loss: 0.00001718
Iteration 256/1000 | Loss: 0.00001718
Iteration 257/1000 | Loss: 0.00001718
Iteration 258/1000 | Loss: 0.00001718
Iteration 259/1000 | Loss: 0.00001718
Iteration 260/1000 | Loss: 0.00001717
Iteration 261/1000 | Loss: 0.00001717
Iteration 262/1000 | Loss: 0.00001717
Iteration 263/1000 | Loss: 0.00001717
Iteration 264/1000 | Loss: 0.00001717
Iteration 265/1000 | Loss: 0.00001717
Iteration 266/1000 | Loss: 0.00001717
Iteration 267/1000 | Loss: 0.00001717
Iteration 268/1000 | Loss: 0.00001717
Iteration 269/1000 | Loss: 0.00001717
Iteration 270/1000 | Loss: 0.00001717
Iteration 271/1000 | Loss: 0.00001716
Iteration 272/1000 | Loss: 0.00001716
Iteration 273/1000 | Loss: 0.00001716
Iteration 274/1000 | Loss: 0.00001716
Iteration 275/1000 | Loss: 0.00001716
Iteration 276/1000 | Loss: 0.00001716
Iteration 277/1000 | Loss: 0.00001716
Iteration 278/1000 | Loss: 0.00001716
Iteration 279/1000 | Loss: 0.00001716
Iteration 280/1000 | Loss: 0.00001716
Iteration 281/1000 | Loss: 0.00001716
Iteration 282/1000 | Loss: 0.00001716
Iteration 283/1000 | Loss: 0.00001716
Iteration 284/1000 | Loss: 0.00001716
Iteration 285/1000 | Loss: 0.00001715
Iteration 286/1000 | Loss: 0.00001715
Iteration 287/1000 | Loss: 0.00001715
Iteration 288/1000 | Loss: 0.00001715
Iteration 289/1000 | Loss: 0.00001715
Iteration 290/1000 | Loss: 0.00001714
Iteration 291/1000 | Loss: 0.00001714
Iteration 292/1000 | Loss: 0.00001714
Iteration 293/1000 | Loss: 0.00001714
Iteration 294/1000 | Loss: 0.00001714
Iteration 295/1000 | Loss: 0.00001714
Iteration 296/1000 | Loss: 0.00001714
Iteration 297/1000 | Loss: 0.00001714
Iteration 298/1000 | Loss: 0.00001714
Iteration 299/1000 | Loss: 0.00001713
Iteration 300/1000 | Loss: 0.00001713
Iteration 301/1000 | Loss: 0.00001713
Iteration 302/1000 | Loss: 0.00001713
Iteration 303/1000 | Loss: 0.00001712
Iteration 304/1000 | Loss: 0.00001712
Iteration 305/1000 | Loss: 0.00001712
Iteration 306/1000 | Loss: 0.00001712
Iteration 307/1000 | Loss: 0.00001712
Iteration 308/1000 | Loss: 0.00001711
Iteration 309/1000 | Loss: 0.00001711
Iteration 310/1000 | Loss: 0.00001711
Iteration 311/1000 | Loss: 0.00001711
Iteration 312/1000 | Loss: 0.00001711
Iteration 313/1000 | Loss: 0.00001711
Iteration 314/1000 | Loss: 0.00001711
Iteration 315/1000 | Loss: 0.00001711
Iteration 316/1000 | Loss: 0.00001710
Iteration 317/1000 | Loss: 0.00001710
Iteration 318/1000 | Loss: 0.00001710
Iteration 319/1000 | Loss: 0.00001710
Iteration 320/1000 | Loss: 0.00001710
Iteration 321/1000 | Loss: 0.00001710
Iteration 322/1000 | Loss: 0.00001710
Iteration 323/1000 | Loss: 0.00001709
Iteration 324/1000 | Loss: 0.00001709
Iteration 325/1000 | Loss: 0.00001709
Iteration 326/1000 | Loss: 0.00001709
Iteration 327/1000 | Loss: 0.00001708
Iteration 328/1000 | Loss: 0.00001708
Iteration 329/1000 | Loss: 0.00001707
Iteration 330/1000 | Loss: 0.00001707
Iteration 331/1000 | Loss: 0.00001707
Iteration 332/1000 | Loss: 0.00001707
Iteration 333/1000 | Loss: 0.00001707
Iteration 334/1000 | Loss: 0.00001707
Iteration 335/1000 | Loss: 0.00001707
Iteration 336/1000 | Loss: 0.00001707
Iteration 337/1000 | Loss: 0.00001707
Iteration 338/1000 | Loss: 0.00001707
Iteration 339/1000 | Loss: 0.00001707
Iteration 340/1000 | Loss: 0.00001706
Iteration 341/1000 | Loss: 0.00001706
Iteration 342/1000 | Loss: 0.00001706
Iteration 343/1000 | Loss: 0.00001706
Iteration 344/1000 | Loss: 0.00001706
Iteration 345/1000 | Loss: 0.00001705
Iteration 346/1000 | Loss: 0.00001705
Iteration 347/1000 | Loss: 0.00001705
Iteration 348/1000 | Loss: 0.00001705
Iteration 349/1000 | Loss: 0.00001705
Iteration 350/1000 | Loss: 0.00001705
Iteration 351/1000 | Loss: 0.00001705
Iteration 352/1000 | Loss: 0.00001705
Iteration 353/1000 | Loss: 0.00001705
Iteration 354/1000 | Loss: 0.00001705
Iteration 355/1000 | Loss: 0.00001705
Iteration 356/1000 | Loss: 0.00001705
Iteration 357/1000 | Loss: 0.00001705
Iteration 358/1000 | Loss: 0.00001705
Iteration 359/1000 | Loss: 0.00001705
Iteration 360/1000 | Loss: 0.00001705
Iteration 361/1000 | Loss: 0.00001704
Iteration 362/1000 | Loss: 0.00001704
Iteration 363/1000 | Loss: 0.00001704
Iteration 364/1000 | Loss: 0.00001704
Iteration 365/1000 | Loss: 0.00001704
Iteration 366/1000 | Loss: 0.00001704
Iteration 367/1000 | Loss: 0.00001704
Iteration 368/1000 | Loss: 0.00001703
Iteration 369/1000 | Loss: 0.00001703
Iteration 370/1000 | Loss: 0.00001703
Iteration 371/1000 | Loss: 0.00001703
Iteration 372/1000 | Loss: 0.00001703
Iteration 373/1000 | Loss: 0.00001703
Iteration 374/1000 | Loss: 0.00001703
Iteration 375/1000 | Loss: 0.00001703
Iteration 376/1000 | Loss: 0.00001703
Iteration 377/1000 | Loss: 0.00001703
Iteration 378/1000 | Loss: 0.00001703
Iteration 379/1000 | Loss: 0.00001702
Iteration 380/1000 | Loss: 0.00001702
Iteration 381/1000 | Loss: 0.00001702
Iteration 382/1000 | Loss: 0.00001702
Iteration 383/1000 | Loss: 0.00001702
Iteration 384/1000 | Loss: 0.00001702
Iteration 385/1000 | Loss: 0.00001702
Iteration 386/1000 | Loss: 0.00001702
Iteration 387/1000 | Loss: 0.00001702
Iteration 388/1000 | Loss: 0.00001702
Iteration 389/1000 | Loss: 0.00001702
Iteration 390/1000 | Loss: 0.00001702
Iteration 391/1000 | Loss: 0.00001702
Iteration 392/1000 | Loss: 0.00001702
Iteration 393/1000 | Loss: 0.00001702
Iteration 394/1000 | Loss: 0.00001702
Iteration 395/1000 | Loss: 0.00001702
Iteration 396/1000 | Loss: 0.00001702
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 396. Stopping optimization.
Last 5 losses: [1.7016027413774282e-05, 1.7016027413774282e-05, 1.7016027413774282e-05, 1.7016027413774282e-05, 1.7016027413774282e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7016027413774282e-05

Optimization complete. Final v2v error: 2.988125801086426 mm

Highest mean error: 10.58911418914795 mm for frame 74

Lowest mean error: 2.565675973892212 mm for frame 44

Saving results

Total time: 419.08985471725464
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_004/1014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_004/1014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_004/1014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01078681
Iteration 2/25 | Loss: 0.01078681
Iteration 3/25 | Loss: 0.01078681
Iteration 4/25 | Loss: 0.00615022
Iteration 5/25 | Loss: 0.00358149
Iteration 6/25 | Loss: 0.00328733
Iteration 7/25 | Loss: 0.00263485
Iteration 8/25 | Loss: 0.00283337
Iteration 9/25 | Loss: 0.00198264
Iteration 10/25 | Loss: 0.00175978
Iteration 11/25 | Loss: 0.00153769
Iteration 12/25 | Loss: 0.00142187
Iteration 13/25 | Loss: 0.00137113
Iteration 14/25 | Loss: 0.00134915
Iteration 15/25 | Loss: 0.00132588
Iteration 16/25 | Loss: 0.00132067
Iteration 17/25 | Loss: 0.00131539
Iteration 18/25 | Loss: 0.00130661
Iteration 19/25 | Loss: 0.00130806
Iteration 20/25 | Loss: 0.00130102
Iteration 21/25 | Loss: 0.00129885
Iteration 22/25 | Loss: 0.00129701
Iteration 23/25 | Loss: 0.00129650
Iteration 24/25 | Loss: 0.00129569
Iteration 25/25 | Loss: 0.00129700

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.56399989
Iteration 2/25 | Loss: 0.00096575
Iteration 3/25 | Loss: 0.00093444
Iteration 4/25 | Loss: 0.00093444
Iteration 5/25 | Loss: 0.00093443
Iteration 6/25 | Loss: 0.00093443
Iteration 7/25 | Loss: 0.00093443
Iteration 8/25 | Loss: 0.00093443
Iteration 9/25 | Loss: 0.00093443
Iteration 10/25 | Loss: 0.00093443
Iteration 11/25 | Loss: 0.00093443
Iteration 12/25 | Loss: 0.00093443
Iteration 13/25 | Loss: 0.00093443
Iteration 14/25 | Loss: 0.00093443
Iteration 15/25 | Loss: 0.00093443
Iteration 16/25 | Loss: 0.00093443
Iteration 17/25 | Loss: 0.00093443
Iteration 18/25 | Loss: 0.00093443
Iteration 19/25 | Loss: 0.00093443
Iteration 20/25 | Loss: 0.00093443
Iteration 21/25 | Loss: 0.00093443
Iteration 22/25 | Loss: 0.00093443
Iteration 23/25 | Loss: 0.00093443
Iteration 24/25 | Loss: 0.00093443
Iteration 25/25 | Loss: 0.00093443

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00093443
Iteration 2/1000 | Loss: 0.00012910
Iteration 3/1000 | Loss: 0.00008622
Iteration 4/1000 | Loss: 0.00006178
Iteration 5/1000 | Loss: 0.00005454
Iteration 6/1000 | Loss: 0.00016375
Iteration 7/1000 | Loss: 0.00005011
Iteration 8/1000 | Loss: 0.00004875
Iteration 9/1000 | Loss: 0.00005242
Iteration 10/1000 | Loss: 0.00004681
Iteration 11/1000 | Loss: 0.00005620
Iteration 12/1000 | Loss: 0.00004581
Iteration 13/1000 | Loss: 0.00038874
Iteration 14/1000 | Loss: 0.00615971
Iteration 15/1000 | Loss: 0.00791954
Iteration 16/1000 | Loss: 0.00099560
Iteration 17/1000 | Loss: 0.00027025
Iteration 18/1000 | Loss: 0.00006442
Iteration 19/1000 | Loss: 0.00005328
Iteration 20/1000 | Loss: 0.00094756
Iteration 21/1000 | Loss: 0.00595568
Iteration 22/1000 | Loss: 0.00136201
Iteration 23/1000 | Loss: 0.00090665
Iteration 24/1000 | Loss: 0.00016448
Iteration 25/1000 | Loss: 0.00006738
Iteration 26/1000 | Loss: 0.00005848
Iteration 27/1000 | Loss: 0.00018245
Iteration 28/1000 | Loss: 0.00009148
Iteration 29/1000 | Loss: 0.00003338
Iteration 30/1000 | Loss: 0.00004737
Iteration 31/1000 | Loss: 0.00012392
Iteration 32/1000 | Loss: 0.00006637
Iteration 33/1000 | Loss: 0.00002497
Iteration 34/1000 | Loss: 0.00006001
Iteration 35/1000 | Loss: 0.00004373
Iteration 36/1000 | Loss: 0.00003915
Iteration 37/1000 | Loss: 0.00009767
Iteration 38/1000 | Loss: 0.00020152
Iteration 39/1000 | Loss: 0.00006972
Iteration 40/1000 | Loss: 0.00031251
Iteration 41/1000 | Loss: 0.00002104
Iteration 42/1000 | Loss: 0.00004768
Iteration 43/1000 | Loss: 0.00001888
Iteration 44/1000 | Loss: 0.00001851
Iteration 45/1000 | Loss: 0.00001818
Iteration 46/1000 | Loss: 0.00001817
Iteration 47/1000 | Loss: 0.00001799
Iteration 48/1000 | Loss: 0.00001788
Iteration 49/1000 | Loss: 0.00001784
Iteration 50/1000 | Loss: 0.00001783
Iteration 51/1000 | Loss: 0.00001782
Iteration 52/1000 | Loss: 0.00001777
Iteration 53/1000 | Loss: 0.00001777
Iteration 54/1000 | Loss: 0.00001777
Iteration 55/1000 | Loss: 0.00001777
Iteration 56/1000 | Loss: 0.00001777
Iteration 57/1000 | Loss: 0.00001777
Iteration 58/1000 | Loss: 0.00001777
Iteration 59/1000 | Loss: 0.00001776
Iteration 60/1000 | Loss: 0.00001776
Iteration 61/1000 | Loss: 0.00001775
Iteration 62/1000 | Loss: 0.00001773
Iteration 63/1000 | Loss: 0.00001773
Iteration 64/1000 | Loss: 0.00001773
Iteration 65/1000 | Loss: 0.00001773
Iteration 66/1000 | Loss: 0.00001771
Iteration 67/1000 | Loss: 0.00001771
Iteration 68/1000 | Loss: 0.00001771
Iteration 69/1000 | Loss: 0.00001771
Iteration 70/1000 | Loss: 0.00001771
Iteration 71/1000 | Loss: 0.00001771
Iteration 72/1000 | Loss: 0.00001770
Iteration 73/1000 | Loss: 0.00001770
Iteration 74/1000 | Loss: 0.00001770
Iteration 75/1000 | Loss: 0.00001770
Iteration 76/1000 | Loss: 0.00001770
Iteration 77/1000 | Loss: 0.00001769
Iteration 78/1000 | Loss: 0.00001768
Iteration 79/1000 | Loss: 0.00001768
Iteration 80/1000 | Loss: 0.00001768
Iteration 81/1000 | Loss: 0.00001768
Iteration 82/1000 | Loss: 0.00001768
Iteration 83/1000 | Loss: 0.00001768
Iteration 84/1000 | Loss: 0.00001767
Iteration 85/1000 | Loss: 0.00001767
Iteration 86/1000 | Loss: 0.00001767
Iteration 87/1000 | Loss: 0.00001767
Iteration 88/1000 | Loss: 0.00001767
Iteration 89/1000 | Loss: 0.00001767
Iteration 90/1000 | Loss: 0.00001767
Iteration 91/1000 | Loss: 0.00001767
Iteration 92/1000 | Loss: 0.00001766
Iteration 93/1000 | Loss: 0.00001766
Iteration 94/1000 | Loss: 0.00001766
Iteration 95/1000 | Loss: 0.00001765
Iteration 96/1000 | Loss: 0.00001765
Iteration 97/1000 | Loss: 0.00001765
Iteration 98/1000 | Loss: 0.00001764
Iteration 99/1000 | Loss: 0.00001764
Iteration 100/1000 | Loss: 0.00001764
Iteration 101/1000 | Loss: 0.00001763
Iteration 102/1000 | Loss: 0.00001763
Iteration 103/1000 | Loss: 0.00001763
Iteration 104/1000 | Loss: 0.00001763
Iteration 105/1000 | Loss: 0.00001763
Iteration 106/1000 | Loss: 0.00001763
Iteration 107/1000 | Loss: 0.00001763
Iteration 108/1000 | Loss: 0.00001763
Iteration 109/1000 | Loss: 0.00001763
Iteration 110/1000 | Loss: 0.00001763
Iteration 111/1000 | Loss: 0.00001763
Iteration 112/1000 | Loss: 0.00001763
Iteration 113/1000 | Loss: 0.00001762
Iteration 114/1000 | Loss: 0.00001762
Iteration 115/1000 | Loss: 0.00001762
Iteration 116/1000 | Loss: 0.00001762
Iteration 117/1000 | Loss: 0.00001762
Iteration 118/1000 | Loss: 0.00001762
Iteration 119/1000 | Loss: 0.00001762
Iteration 120/1000 | Loss: 0.00001761
Iteration 121/1000 | Loss: 0.00001761
Iteration 122/1000 | Loss: 0.00001761
Iteration 123/1000 | Loss: 0.00001761
Iteration 124/1000 | Loss: 0.00001761
Iteration 125/1000 | Loss: 0.00001761
Iteration 126/1000 | Loss: 0.00001761
Iteration 127/1000 | Loss: 0.00001761
Iteration 128/1000 | Loss: 0.00001761
Iteration 129/1000 | Loss: 0.00001761
Iteration 130/1000 | Loss: 0.00001761
Iteration 131/1000 | Loss: 0.00001761
Iteration 132/1000 | Loss: 0.00001761
Iteration 133/1000 | Loss: 0.00001761
Iteration 134/1000 | Loss: 0.00001761
Iteration 135/1000 | Loss: 0.00001761
Iteration 136/1000 | Loss: 0.00001761
Iteration 137/1000 | Loss: 0.00001761
Iteration 138/1000 | Loss: 0.00001761
Iteration 139/1000 | Loss: 0.00001761
Iteration 140/1000 | Loss: 0.00001761
Iteration 141/1000 | Loss: 0.00001761
Iteration 142/1000 | Loss: 0.00001761
Iteration 143/1000 | Loss: 0.00001760
Iteration 144/1000 | Loss: 0.00001760
Iteration 145/1000 | Loss: 0.00001760
Iteration 146/1000 | Loss: 0.00001760
Iteration 147/1000 | Loss: 0.00001760
Iteration 148/1000 | Loss: 0.00001760
Iteration 149/1000 | Loss: 0.00001760
Iteration 150/1000 | Loss: 0.00001760
Iteration 151/1000 | Loss: 0.00001760
Iteration 152/1000 | Loss: 0.00001760
Iteration 153/1000 | Loss: 0.00001760
Iteration 154/1000 | Loss: 0.00001760
Iteration 155/1000 | Loss: 0.00001760
Iteration 156/1000 | Loss: 0.00001760
Iteration 157/1000 | Loss: 0.00001760
Iteration 158/1000 | Loss: 0.00001760
Iteration 159/1000 | Loss: 0.00001760
Iteration 160/1000 | Loss: 0.00001760
Iteration 161/1000 | Loss: 0.00001760
Iteration 162/1000 | Loss: 0.00001760
Iteration 163/1000 | Loss: 0.00001760
Iteration 164/1000 | Loss: 0.00001760
Iteration 165/1000 | Loss: 0.00001760
Iteration 166/1000 | Loss: 0.00001760
Iteration 167/1000 | Loss: 0.00001760
Iteration 168/1000 | Loss: 0.00001760
Iteration 169/1000 | Loss: 0.00001760
Iteration 170/1000 | Loss: 0.00001760
Iteration 171/1000 | Loss: 0.00001760
Iteration 172/1000 | Loss: 0.00001760
Iteration 173/1000 | Loss: 0.00001760
Iteration 174/1000 | Loss: 0.00001760
Iteration 175/1000 | Loss: 0.00001760
Iteration 176/1000 | Loss: 0.00001760
Iteration 177/1000 | Loss: 0.00001760
Iteration 178/1000 | Loss: 0.00001760
Iteration 179/1000 | Loss: 0.00001760
Iteration 180/1000 | Loss: 0.00001760
Iteration 181/1000 | Loss: 0.00001760
Iteration 182/1000 | Loss: 0.00001760
Iteration 183/1000 | Loss: 0.00001760
Iteration 184/1000 | Loss: 0.00001760
Iteration 185/1000 | Loss: 0.00001760
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 185. Stopping optimization.
Last 5 losses: [1.760207123879809e-05, 1.760207123879809e-05, 1.760207123879809e-05, 1.760207123879809e-05, 1.760207123879809e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.760207123879809e-05

Optimization complete. Final v2v error: 3.59950590133667 mm

Highest mean error: 3.8911919593811035 mm for frame 167

Lowest mean error: 3.48252010345459 mm for frame 58

Saving results

Total time: 132.22475004196167
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_004/1084/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_004/1084.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_004/1084
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00525816
Iteration 2/25 | Loss: 0.00116987
Iteration 3/25 | Loss: 0.00111255
Iteration 4/25 | Loss: 0.00110608
Iteration 5/25 | Loss: 0.00110394
Iteration 6/25 | Loss: 0.00110394
Iteration 7/25 | Loss: 0.00110394
Iteration 8/25 | Loss: 0.00110394
Iteration 9/25 | Loss: 0.00110394
Iteration 10/25 | Loss: 0.00110394
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0011039399541914463, 0.0011039399541914463, 0.0011039399541914463, 0.0011039399541914463, 0.0011039399541914463]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011039399541914463

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 10.27922344
Iteration 2/25 | Loss: 0.00068838
Iteration 3/25 | Loss: 0.00068836
Iteration 4/25 | Loss: 0.00068836
Iteration 5/25 | Loss: 0.00068836
Iteration 6/25 | Loss: 0.00068836
Iteration 7/25 | Loss: 0.00068836
Iteration 8/25 | Loss: 0.00068836
Iteration 9/25 | Loss: 0.00068836
Iteration 10/25 | Loss: 0.00068836
Iteration 11/25 | Loss: 0.00068836
Iteration 12/25 | Loss: 0.00068836
Iteration 13/25 | Loss: 0.00068836
Iteration 14/25 | Loss: 0.00068836
Iteration 15/25 | Loss: 0.00068836
Iteration 16/25 | Loss: 0.00068836
Iteration 17/25 | Loss: 0.00068836
Iteration 18/25 | Loss: 0.00068836
Iteration 19/25 | Loss: 0.00068836
Iteration 20/25 | Loss: 0.00068836
Iteration 21/25 | Loss: 0.00068836
Iteration 22/25 | Loss: 0.00068836
Iteration 23/25 | Loss: 0.00068836
Iteration 24/25 | Loss: 0.00068836
Iteration 25/25 | Loss: 0.00068836

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00068836
Iteration 2/1000 | Loss: 0.00002268
Iteration 3/1000 | Loss: 0.00001495
Iteration 4/1000 | Loss: 0.00001405
Iteration 5/1000 | Loss: 0.00001345
Iteration 6/1000 | Loss: 0.00001308
Iteration 7/1000 | Loss: 0.00001286
Iteration 8/1000 | Loss: 0.00001262
Iteration 9/1000 | Loss: 0.00001242
Iteration 10/1000 | Loss: 0.00001229
Iteration 11/1000 | Loss: 0.00001228
Iteration 12/1000 | Loss: 0.00001216
Iteration 13/1000 | Loss: 0.00001215
Iteration 14/1000 | Loss: 0.00001211
Iteration 15/1000 | Loss: 0.00001202
Iteration 16/1000 | Loss: 0.00001202
Iteration 17/1000 | Loss: 0.00001195
Iteration 18/1000 | Loss: 0.00001195
Iteration 19/1000 | Loss: 0.00001194
Iteration 20/1000 | Loss: 0.00001194
Iteration 21/1000 | Loss: 0.00001193
Iteration 22/1000 | Loss: 0.00001192
Iteration 23/1000 | Loss: 0.00001192
Iteration 24/1000 | Loss: 0.00001192
Iteration 25/1000 | Loss: 0.00001191
Iteration 26/1000 | Loss: 0.00001190
Iteration 27/1000 | Loss: 0.00001190
Iteration 28/1000 | Loss: 0.00001190
Iteration 29/1000 | Loss: 0.00001189
Iteration 30/1000 | Loss: 0.00001189
Iteration 31/1000 | Loss: 0.00001189
Iteration 32/1000 | Loss: 0.00001188
Iteration 33/1000 | Loss: 0.00001188
Iteration 34/1000 | Loss: 0.00001187
Iteration 35/1000 | Loss: 0.00001187
Iteration 36/1000 | Loss: 0.00001186
Iteration 37/1000 | Loss: 0.00001186
Iteration 38/1000 | Loss: 0.00001186
Iteration 39/1000 | Loss: 0.00001185
Iteration 40/1000 | Loss: 0.00001185
Iteration 41/1000 | Loss: 0.00001185
Iteration 42/1000 | Loss: 0.00001184
Iteration 43/1000 | Loss: 0.00001183
Iteration 44/1000 | Loss: 0.00001180
Iteration 45/1000 | Loss: 0.00001180
Iteration 46/1000 | Loss: 0.00001179
Iteration 47/1000 | Loss: 0.00001179
Iteration 48/1000 | Loss: 0.00001176
Iteration 49/1000 | Loss: 0.00001176
Iteration 50/1000 | Loss: 0.00001176
Iteration 51/1000 | Loss: 0.00001176
Iteration 52/1000 | Loss: 0.00001175
Iteration 53/1000 | Loss: 0.00001175
Iteration 54/1000 | Loss: 0.00001175
Iteration 55/1000 | Loss: 0.00001174
Iteration 56/1000 | Loss: 0.00001173
Iteration 57/1000 | Loss: 0.00001172
Iteration 58/1000 | Loss: 0.00001171
Iteration 59/1000 | Loss: 0.00001171
Iteration 60/1000 | Loss: 0.00001171
Iteration 61/1000 | Loss: 0.00001170
Iteration 62/1000 | Loss: 0.00001170
Iteration 63/1000 | Loss: 0.00001167
Iteration 64/1000 | Loss: 0.00001167
Iteration 65/1000 | Loss: 0.00001167
Iteration 66/1000 | Loss: 0.00001167
Iteration 67/1000 | Loss: 0.00001166
Iteration 68/1000 | Loss: 0.00001166
Iteration 69/1000 | Loss: 0.00001166
Iteration 70/1000 | Loss: 0.00001165
Iteration 71/1000 | Loss: 0.00001164
Iteration 72/1000 | Loss: 0.00001163
Iteration 73/1000 | Loss: 0.00001162
Iteration 74/1000 | Loss: 0.00001162
Iteration 75/1000 | Loss: 0.00001161
Iteration 76/1000 | Loss: 0.00001161
Iteration 77/1000 | Loss: 0.00001161
Iteration 78/1000 | Loss: 0.00001161
Iteration 79/1000 | Loss: 0.00001161
Iteration 80/1000 | Loss: 0.00001161
Iteration 81/1000 | Loss: 0.00001160
Iteration 82/1000 | Loss: 0.00001160
Iteration 83/1000 | Loss: 0.00001159
Iteration 84/1000 | Loss: 0.00001159
Iteration 85/1000 | Loss: 0.00001158
Iteration 86/1000 | Loss: 0.00001158
Iteration 87/1000 | Loss: 0.00001158
Iteration 88/1000 | Loss: 0.00001158
Iteration 89/1000 | Loss: 0.00001158
Iteration 90/1000 | Loss: 0.00001158
Iteration 91/1000 | Loss: 0.00001158
Iteration 92/1000 | Loss: 0.00001157
Iteration 93/1000 | Loss: 0.00001157
Iteration 94/1000 | Loss: 0.00001157
Iteration 95/1000 | Loss: 0.00001157
Iteration 96/1000 | Loss: 0.00001157
Iteration 97/1000 | Loss: 0.00001157
Iteration 98/1000 | Loss: 0.00001157
Iteration 99/1000 | Loss: 0.00001157
Iteration 100/1000 | Loss: 0.00001157
Iteration 101/1000 | Loss: 0.00001157
Iteration 102/1000 | Loss: 0.00001156
Iteration 103/1000 | Loss: 0.00001156
Iteration 104/1000 | Loss: 0.00001156
Iteration 105/1000 | Loss: 0.00001156
Iteration 106/1000 | Loss: 0.00001155
Iteration 107/1000 | Loss: 0.00001155
Iteration 108/1000 | Loss: 0.00001155
Iteration 109/1000 | Loss: 0.00001154
Iteration 110/1000 | Loss: 0.00001153
Iteration 111/1000 | Loss: 0.00001152
Iteration 112/1000 | Loss: 0.00001151
Iteration 113/1000 | Loss: 0.00001151
Iteration 114/1000 | Loss: 0.00001151
Iteration 115/1000 | Loss: 0.00001150
Iteration 116/1000 | Loss: 0.00001150
Iteration 117/1000 | Loss: 0.00001150
Iteration 118/1000 | Loss: 0.00001150
Iteration 119/1000 | Loss: 0.00001150
Iteration 120/1000 | Loss: 0.00001150
Iteration 121/1000 | Loss: 0.00001150
Iteration 122/1000 | Loss: 0.00001149
Iteration 123/1000 | Loss: 0.00001149
Iteration 124/1000 | Loss: 0.00001149
Iteration 125/1000 | Loss: 0.00001149
Iteration 126/1000 | Loss: 0.00001149
Iteration 127/1000 | Loss: 0.00001149
Iteration 128/1000 | Loss: 0.00001148
Iteration 129/1000 | Loss: 0.00001148
Iteration 130/1000 | Loss: 0.00001147
Iteration 131/1000 | Loss: 0.00001147
Iteration 132/1000 | Loss: 0.00001147
Iteration 133/1000 | Loss: 0.00001147
Iteration 134/1000 | Loss: 0.00001147
Iteration 135/1000 | Loss: 0.00001146
Iteration 136/1000 | Loss: 0.00001146
Iteration 137/1000 | Loss: 0.00001146
Iteration 138/1000 | Loss: 0.00001146
Iteration 139/1000 | Loss: 0.00001146
Iteration 140/1000 | Loss: 0.00001146
Iteration 141/1000 | Loss: 0.00001146
Iteration 142/1000 | Loss: 0.00001146
Iteration 143/1000 | Loss: 0.00001145
Iteration 144/1000 | Loss: 0.00001145
Iteration 145/1000 | Loss: 0.00001145
Iteration 146/1000 | Loss: 0.00001145
Iteration 147/1000 | Loss: 0.00001145
Iteration 148/1000 | Loss: 0.00001145
Iteration 149/1000 | Loss: 0.00001145
Iteration 150/1000 | Loss: 0.00001145
Iteration 151/1000 | Loss: 0.00001145
Iteration 152/1000 | Loss: 0.00001145
Iteration 153/1000 | Loss: 0.00001145
Iteration 154/1000 | Loss: 0.00001145
Iteration 155/1000 | Loss: 0.00001145
Iteration 156/1000 | Loss: 0.00001144
Iteration 157/1000 | Loss: 0.00001144
Iteration 158/1000 | Loss: 0.00001144
Iteration 159/1000 | Loss: 0.00001144
Iteration 160/1000 | Loss: 0.00001144
Iteration 161/1000 | Loss: 0.00001144
Iteration 162/1000 | Loss: 0.00001144
Iteration 163/1000 | Loss: 0.00001144
Iteration 164/1000 | Loss: 0.00001144
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 164. Stopping optimization.
Last 5 losses: [1.1439540685387328e-05, 1.1439540685387328e-05, 1.1439540685387328e-05, 1.1439540685387328e-05, 1.1439540685387328e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1439540685387328e-05

Optimization complete. Final v2v error: 2.870713949203491 mm

Highest mean error: 3.0791566371917725 mm for frame 42

Lowest mean error: 2.650359630584717 mm for frame 31

Saving results

Total time: 43.444254636764526
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_004/1048/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_004/1048.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_004/1048
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00803841
Iteration 2/25 | Loss: 0.00155141
Iteration 3/25 | Loss: 0.00125877
Iteration 4/25 | Loss: 0.00124200
Iteration 5/25 | Loss: 0.00123910
Iteration 6/25 | Loss: 0.00123910
Iteration 7/25 | Loss: 0.00123910
Iteration 8/25 | Loss: 0.00123910
Iteration 9/25 | Loss: 0.00123910
Iteration 10/25 | Loss: 0.00123910
Iteration 11/25 | Loss: 0.00123910
Iteration 12/25 | Loss: 0.00123910
Iteration 13/25 | Loss: 0.00123910
Iteration 14/25 | Loss: 0.00123910
Iteration 15/25 | Loss: 0.00123910
Iteration 16/25 | Loss: 0.00123910
Iteration 17/25 | Loss: 0.00123910
Iteration 18/25 | Loss: 0.00123910
Iteration 19/25 | Loss: 0.00123910
Iteration 20/25 | Loss: 0.00123910
Iteration 21/25 | Loss: 0.00123910
Iteration 22/25 | Loss: 0.00123910
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0012390969786792994, 0.0012390969786792994, 0.0012390969786792994, 0.0012390969786792994, 0.0012390969786792994]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012390969786792994

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.24710929
Iteration 2/25 | Loss: 0.00058999
Iteration 3/25 | Loss: 0.00058999
Iteration 4/25 | Loss: 0.00058999
Iteration 5/25 | Loss: 0.00058999
Iteration 6/25 | Loss: 0.00058999
Iteration 7/25 | Loss: 0.00058999
Iteration 8/25 | Loss: 0.00058999
Iteration 9/25 | Loss: 0.00058999
Iteration 10/25 | Loss: 0.00058999
Iteration 11/25 | Loss: 0.00058999
Iteration 12/25 | Loss: 0.00058999
Iteration 13/25 | Loss: 0.00058999
Iteration 14/25 | Loss: 0.00058999
Iteration 15/25 | Loss: 0.00058999
Iteration 16/25 | Loss: 0.00058999
Iteration 17/25 | Loss: 0.00058999
Iteration 18/25 | Loss: 0.00058999
Iteration 19/25 | Loss: 0.00058999
Iteration 20/25 | Loss: 0.00058999
Iteration 21/25 | Loss: 0.00058999
Iteration 22/25 | Loss: 0.00058999
Iteration 23/25 | Loss: 0.00058999
Iteration 24/25 | Loss: 0.00058999
Iteration 25/25 | Loss: 0.00058999

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00058999
Iteration 2/1000 | Loss: 0.00003039
Iteration 3/1000 | Loss: 0.00002422
Iteration 4/1000 | Loss: 0.00002218
Iteration 5/1000 | Loss: 0.00002109
Iteration 6/1000 | Loss: 0.00002047
Iteration 7/1000 | Loss: 0.00001992
Iteration 8/1000 | Loss: 0.00001957
Iteration 9/1000 | Loss: 0.00001933
Iteration 10/1000 | Loss: 0.00001916
Iteration 11/1000 | Loss: 0.00001912
Iteration 12/1000 | Loss: 0.00001906
Iteration 13/1000 | Loss: 0.00001905
Iteration 14/1000 | Loss: 0.00001897
Iteration 15/1000 | Loss: 0.00001897
Iteration 16/1000 | Loss: 0.00001895
Iteration 17/1000 | Loss: 0.00001895
Iteration 18/1000 | Loss: 0.00001894
Iteration 19/1000 | Loss: 0.00001894
Iteration 20/1000 | Loss: 0.00001894
Iteration 21/1000 | Loss: 0.00001893
Iteration 22/1000 | Loss: 0.00001892
Iteration 23/1000 | Loss: 0.00001892
Iteration 24/1000 | Loss: 0.00001892
Iteration 25/1000 | Loss: 0.00001891
Iteration 26/1000 | Loss: 0.00001891
Iteration 27/1000 | Loss: 0.00001891
Iteration 28/1000 | Loss: 0.00001890
Iteration 29/1000 | Loss: 0.00001890
Iteration 30/1000 | Loss: 0.00001890
Iteration 31/1000 | Loss: 0.00001890
Iteration 32/1000 | Loss: 0.00001890
Iteration 33/1000 | Loss: 0.00001890
Iteration 34/1000 | Loss: 0.00001890
Iteration 35/1000 | Loss: 0.00001890
Iteration 36/1000 | Loss: 0.00001890
Iteration 37/1000 | Loss: 0.00001890
Iteration 38/1000 | Loss: 0.00001890
Iteration 39/1000 | Loss: 0.00001890
Iteration 40/1000 | Loss: 0.00001889
Iteration 41/1000 | Loss: 0.00001889
Iteration 42/1000 | Loss: 0.00001889
Iteration 43/1000 | Loss: 0.00001889
Iteration 44/1000 | Loss: 0.00001889
Iteration 45/1000 | Loss: 0.00001889
Iteration 46/1000 | Loss: 0.00001889
Iteration 47/1000 | Loss: 0.00001889
Iteration 48/1000 | Loss: 0.00001889
Iteration 49/1000 | Loss: 0.00001889
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 49. Stopping optimization.
Last 5 losses: [1.8893548258347437e-05, 1.8893548258347437e-05, 1.8893548258347437e-05, 1.8893548258347437e-05, 1.8893548258347437e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8893548258347437e-05

Optimization complete. Final v2v error: 3.6268362998962402 mm

Highest mean error: 3.969292163848877 mm for frame 37

Lowest mean error: 3.3715105056762695 mm for frame 214

Saving results

Total time: 28.448176860809326
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_004/1016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_004/1016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_004/1016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00413343
Iteration 2/25 | Loss: 0.00113897
Iteration 3/25 | Loss: 0.00107836
Iteration 4/25 | Loss: 0.00107167
Iteration 5/25 | Loss: 0.00106967
Iteration 6/25 | Loss: 0.00106945
Iteration 7/25 | Loss: 0.00106945
Iteration 8/25 | Loss: 0.00106945
Iteration 9/25 | Loss: 0.00106945
Iteration 10/25 | Loss: 0.00106945
Iteration 11/25 | Loss: 0.00106945
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010694516822695732, 0.0010694516822695732, 0.0010694516822695732, 0.0010694516822695732, 0.0010694516822695732]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010694516822695732

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38693166
Iteration 2/25 | Loss: 0.00069170
Iteration 3/25 | Loss: 0.00069170
Iteration 4/25 | Loss: 0.00069170
Iteration 5/25 | Loss: 0.00069170
Iteration 6/25 | Loss: 0.00069170
Iteration 7/25 | Loss: 0.00069170
Iteration 8/25 | Loss: 0.00069170
Iteration 9/25 | Loss: 0.00069170
Iteration 10/25 | Loss: 0.00069170
Iteration 11/25 | Loss: 0.00069170
Iteration 12/25 | Loss: 0.00069170
Iteration 13/25 | Loss: 0.00069170
Iteration 14/25 | Loss: 0.00069170
Iteration 15/25 | Loss: 0.00069170
Iteration 16/25 | Loss: 0.00069170
Iteration 17/25 | Loss: 0.00069170
Iteration 18/25 | Loss: 0.00069170
Iteration 19/25 | Loss: 0.00069170
Iteration 20/25 | Loss: 0.00069170
Iteration 21/25 | Loss: 0.00069170
Iteration 22/25 | Loss: 0.00069170
Iteration 23/25 | Loss: 0.00069170
Iteration 24/25 | Loss: 0.00069170
Iteration 25/25 | Loss: 0.00069170

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00069170
Iteration 2/1000 | Loss: 0.00002061
Iteration 3/1000 | Loss: 0.00001282
Iteration 4/1000 | Loss: 0.00001176
Iteration 5/1000 | Loss: 0.00001118
Iteration 6/1000 | Loss: 0.00001096
Iteration 7/1000 | Loss: 0.00001072
Iteration 8/1000 | Loss: 0.00001070
Iteration 9/1000 | Loss: 0.00001047
Iteration 10/1000 | Loss: 0.00001025
Iteration 11/1000 | Loss: 0.00001021
Iteration 12/1000 | Loss: 0.00001019
Iteration 13/1000 | Loss: 0.00001016
Iteration 14/1000 | Loss: 0.00001006
Iteration 15/1000 | Loss: 0.00000996
Iteration 16/1000 | Loss: 0.00000996
Iteration 17/1000 | Loss: 0.00000996
Iteration 18/1000 | Loss: 0.00000993
Iteration 19/1000 | Loss: 0.00000992
Iteration 20/1000 | Loss: 0.00000988
Iteration 21/1000 | Loss: 0.00000986
Iteration 22/1000 | Loss: 0.00000986
Iteration 23/1000 | Loss: 0.00000985
Iteration 24/1000 | Loss: 0.00000985
Iteration 25/1000 | Loss: 0.00000985
Iteration 26/1000 | Loss: 0.00000984
Iteration 27/1000 | Loss: 0.00000984
Iteration 28/1000 | Loss: 0.00000983
Iteration 29/1000 | Loss: 0.00000983
Iteration 30/1000 | Loss: 0.00000983
Iteration 31/1000 | Loss: 0.00000982
Iteration 32/1000 | Loss: 0.00000982
Iteration 33/1000 | Loss: 0.00000982
Iteration 34/1000 | Loss: 0.00000982
Iteration 35/1000 | Loss: 0.00000982
Iteration 36/1000 | Loss: 0.00000981
Iteration 37/1000 | Loss: 0.00000980
Iteration 38/1000 | Loss: 0.00000980
Iteration 39/1000 | Loss: 0.00000980
Iteration 40/1000 | Loss: 0.00000980
Iteration 41/1000 | Loss: 0.00000979
Iteration 42/1000 | Loss: 0.00000976
Iteration 43/1000 | Loss: 0.00000976
Iteration 44/1000 | Loss: 0.00000976
Iteration 45/1000 | Loss: 0.00000976
Iteration 46/1000 | Loss: 0.00000975
Iteration 47/1000 | Loss: 0.00000975
Iteration 48/1000 | Loss: 0.00000975
Iteration 49/1000 | Loss: 0.00000974
Iteration 50/1000 | Loss: 0.00000974
Iteration 51/1000 | Loss: 0.00000974
Iteration 52/1000 | Loss: 0.00000974
Iteration 53/1000 | Loss: 0.00000974
Iteration 54/1000 | Loss: 0.00000974
Iteration 55/1000 | Loss: 0.00000974
Iteration 56/1000 | Loss: 0.00000973
Iteration 57/1000 | Loss: 0.00000973
Iteration 58/1000 | Loss: 0.00000973
Iteration 59/1000 | Loss: 0.00000973
Iteration 60/1000 | Loss: 0.00000973
Iteration 61/1000 | Loss: 0.00000973
Iteration 62/1000 | Loss: 0.00000972
Iteration 63/1000 | Loss: 0.00000972
Iteration 64/1000 | Loss: 0.00000972
Iteration 65/1000 | Loss: 0.00000972
Iteration 66/1000 | Loss: 0.00000972
Iteration 67/1000 | Loss: 0.00000972
Iteration 68/1000 | Loss: 0.00000972
Iteration 69/1000 | Loss: 0.00000972
Iteration 70/1000 | Loss: 0.00000972
Iteration 71/1000 | Loss: 0.00000972
Iteration 72/1000 | Loss: 0.00000972
Iteration 73/1000 | Loss: 0.00000972
Iteration 74/1000 | Loss: 0.00000972
Iteration 75/1000 | Loss: 0.00000972
Iteration 76/1000 | Loss: 0.00000971
Iteration 77/1000 | Loss: 0.00000971
Iteration 78/1000 | Loss: 0.00000971
Iteration 79/1000 | Loss: 0.00000971
Iteration 80/1000 | Loss: 0.00000971
Iteration 81/1000 | Loss: 0.00000971
Iteration 82/1000 | Loss: 0.00000971
Iteration 83/1000 | Loss: 0.00000971
Iteration 84/1000 | Loss: 0.00000971
Iteration 85/1000 | Loss: 0.00000971
Iteration 86/1000 | Loss: 0.00000971
Iteration 87/1000 | Loss: 0.00000971
Iteration 88/1000 | Loss: 0.00000971
Iteration 89/1000 | Loss: 0.00000971
Iteration 90/1000 | Loss: 0.00000971
Iteration 91/1000 | Loss: 0.00000970
Iteration 92/1000 | Loss: 0.00000970
Iteration 93/1000 | Loss: 0.00000970
Iteration 94/1000 | Loss: 0.00000970
Iteration 95/1000 | Loss: 0.00000970
Iteration 96/1000 | Loss: 0.00000969
Iteration 97/1000 | Loss: 0.00000969
Iteration 98/1000 | Loss: 0.00000969
Iteration 99/1000 | Loss: 0.00000969
Iteration 100/1000 | Loss: 0.00000969
Iteration 101/1000 | Loss: 0.00000969
Iteration 102/1000 | Loss: 0.00000969
Iteration 103/1000 | Loss: 0.00000969
Iteration 104/1000 | Loss: 0.00000968
Iteration 105/1000 | Loss: 0.00000968
Iteration 106/1000 | Loss: 0.00000968
Iteration 107/1000 | Loss: 0.00000968
Iteration 108/1000 | Loss: 0.00000968
Iteration 109/1000 | Loss: 0.00000968
Iteration 110/1000 | Loss: 0.00000968
Iteration 111/1000 | Loss: 0.00000967
Iteration 112/1000 | Loss: 0.00000967
Iteration 113/1000 | Loss: 0.00000967
Iteration 114/1000 | Loss: 0.00000967
Iteration 115/1000 | Loss: 0.00000967
Iteration 116/1000 | Loss: 0.00000967
Iteration 117/1000 | Loss: 0.00000967
Iteration 118/1000 | Loss: 0.00000967
Iteration 119/1000 | Loss: 0.00000967
Iteration 120/1000 | Loss: 0.00000967
Iteration 121/1000 | Loss: 0.00000967
Iteration 122/1000 | Loss: 0.00000967
Iteration 123/1000 | Loss: 0.00000967
Iteration 124/1000 | Loss: 0.00000966
Iteration 125/1000 | Loss: 0.00000966
Iteration 126/1000 | Loss: 0.00000966
Iteration 127/1000 | Loss: 0.00000966
Iteration 128/1000 | Loss: 0.00000966
Iteration 129/1000 | Loss: 0.00000966
Iteration 130/1000 | Loss: 0.00000965
Iteration 131/1000 | Loss: 0.00000965
Iteration 132/1000 | Loss: 0.00000965
Iteration 133/1000 | Loss: 0.00000965
Iteration 134/1000 | Loss: 0.00000965
Iteration 135/1000 | Loss: 0.00000965
Iteration 136/1000 | Loss: 0.00000965
Iteration 137/1000 | Loss: 0.00000965
Iteration 138/1000 | Loss: 0.00000964
Iteration 139/1000 | Loss: 0.00000964
Iteration 140/1000 | Loss: 0.00000964
Iteration 141/1000 | Loss: 0.00000964
Iteration 142/1000 | Loss: 0.00000964
Iteration 143/1000 | Loss: 0.00000964
Iteration 144/1000 | Loss: 0.00000964
Iteration 145/1000 | Loss: 0.00000964
Iteration 146/1000 | Loss: 0.00000964
Iteration 147/1000 | Loss: 0.00000964
Iteration 148/1000 | Loss: 0.00000964
Iteration 149/1000 | Loss: 0.00000964
Iteration 150/1000 | Loss: 0.00000964
Iteration 151/1000 | Loss: 0.00000964
Iteration 152/1000 | Loss: 0.00000963
Iteration 153/1000 | Loss: 0.00000963
Iteration 154/1000 | Loss: 0.00000963
Iteration 155/1000 | Loss: 0.00000963
Iteration 156/1000 | Loss: 0.00000963
Iteration 157/1000 | Loss: 0.00000963
Iteration 158/1000 | Loss: 0.00000963
Iteration 159/1000 | Loss: 0.00000963
Iteration 160/1000 | Loss: 0.00000963
Iteration 161/1000 | Loss: 0.00000963
Iteration 162/1000 | Loss: 0.00000963
Iteration 163/1000 | Loss: 0.00000962
Iteration 164/1000 | Loss: 0.00000962
Iteration 165/1000 | Loss: 0.00000962
Iteration 166/1000 | Loss: 0.00000962
Iteration 167/1000 | Loss: 0.00000962
Iteration 168/1000 | Loss: 0.00000962
Iteration 169/1000 | Loss: 0.00000961
Iteration 170/1000 | Loss: 0.00000961
Iteration 171/1000 | Loss: 0.00000961
Iteration 172/1000 | Loss: 0.00000961
Iteration 173/1000 | Loss: 0.00000961
Iteration 174/1000 | Loss: 0.00000961
Iteration 175/1000 | Loss: 0.00000961
Iteration 176/1000 | Loss: 0.00000961
Iteration 177/1000 | Loss: 0.00000960
Iteration 178/1000 | Loss: 0.00000960
Iteration 179/1000 | Loss: 0.00000960
Iteration 180/1000 | Loss: 0.00000960
Iteration 181/1000 | Loss: 0.00000960
Iteration 182/1000 | Loss: 0.00000960
Iteration 183/1000 | Loss: 0.00000960
Iteration 184/1000 | Loss: 0.00000960
Iteration 185/1000 | Loss: 0.00000959
Iteration 186/1000 | Loss: 0.00000959
Iteration 187/1000 | Loss: 0.00000959
Iteration 188/1000 | Loss: 0.00000959
Iteration 189/1000 | Loss: 0.00000959
Iteration 190/1000 | Loss: 0.00000959
Iteration 191/1000 | Loss: 0.00000959
Iteration 192/1000 | Loss: 0.00000959
Iteration 193/1000 | Loss: 0.00000959
Iteration 194/1000 | Loss: 0.00000959
Iteration 195/1000 | Loss: 0.00000959
Iteration 196/1000 | Loss: 0.00000959
Iteration 197/1000 | Loss: 0.00000958
Iteration 198/1000 | Loss: 0.00000958
Iteration 199/1000 | Loss: 0.00000958
Iteration 200/1000 | Loss: 0.00000958
Iteration 201/1000 | Loss: 0.00000958
Iteration 202/1000 | Loss: 0.00000958
Iteration 203/1000 | Loss: 0.00000958
Iteration 204/1000 | Loss: 0.00000958
Iteration 205/1000 | Loss: 0.00000958
Iteration 206/1000 | Loss: 0.00000958
Iteration 207/1000 | Loss: 0.00000958
Iteration 208/1000 | Loss: 0.00000958
Iteration 209/1000 | Loss: 0.00000958
Iteration 210/1000 | Loss: 0.00000958
Iteration 211/1000 | Loss: 0.00000958
Iteration 212/1000 | Loss: 0.00000958
Iteration 213/1000 | Loss: 0.00000958
Iteration 214/1000 | Loss: 0.00000958
Iteration 215/1000 | Loss: 0.00000958
Iteration 216/1000 | Loss: 0.00000958
Iteration 217/1000 | Loss: 0.00000958
Iteration 218/1000 | Loss: 0.00000958
Iteration 219/1000 | Loss: 0.00000958
Iteration 220/1000 | Loss: 0.00000958
Iteration 221/1000 | Loss: 0.00000958
Iteration 222/1000 | Loss: 0.00000958
Iteration 223/1000 | Loss: 0.00000958
Iteration 224/1000 | Loss: 0.00000958
Iteration 225/1000 | Loss: 0.00000958
Iteration 226/1000 | Loss: 0.00000958
Iteration 227/1000 | Loss: 0.00000958
Iteration 228/1000 | Loss: 0.00000958
Iteration 229/1000 | Loss: 0.00000958
Iteration 230/1000 | Loss: 0.00000958
Iteration 231/1000 | Loss: 0.00000958
Iteration 232/1000 | Loss: 0.00000958
Iteration 233/1000 | Loss: 0.00000958
Iteration 234/1000 | Loss: 0.00000958
Iteration 235/1000 | Loss: 0.00000958
Iteration 236/1000 | Loss: 0.00000958
Iteration 237/1000 | Loss: 0.00000958
Iteration 238/1000 | Loss: 0.00000958
Iteration 239/1000 | Loss: 0.00000958
Iteration 240/1000 | Loss: 0.00000958
Iteration 241/1000 | Loss: 0.00000958
Iteration 242/1000 | Loss: 0.00000958
Iteration 243/1000 | Loss: 0.00000958
Iteration 244/1000 | Loss: 0.00000958
Iteration 245/1000 | Loss: 0.00000958
Iteration 246/1000 | Loss: 0.00000958
Iteration 247/1000 | Loss: 0.00000958
Iteration 248/1000 | Loss: 0.00000958
Iteration 249/1000 | Loss: 0.00000958
Iteration 250/1000 | Loss: 0.00000958
Iteration 251/1000 | Loss: 0.00000958
Iteration 252/1000 | Loss: 0.00000958
Iteration 253/1000 | Loss: 0.00000958
Iteration 254/1000 | Loss: 0.00000958
Iteration 255/1000 | Loss: 0.00000958
Iteration 256/1000 | Loss: 0.00000958
Iteration 257/1000 | Loss: 0.00000958
Iteration 258/1000 | Loss: 0.00000958
Iteration 259/1000 | Loss: 0.00000958
Iteration 260/1000 | Loss: 0.00000958
Iteration 261/1000 | Loss: 0.00000958
Iteration 262/1000 | Loss: 0.00000958
Iteration 263/1000 | Loss: 0.00000958
Iteration 264/1000 | Loss: 0.00000958
Iteration 265/1000 | Loss: 0.00000958
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 265. Stopping optimization.
Last 5 losses: [9.575935109751299e-06, 9.575935109751299e-06, 9.575935109751299e-06, 9.575935109751299e-06, 9.575935109751299e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.575935109751299e-06

Optimization complete. Final v2v error: 2.6652371883392334 mm

Highest mean error: 2.7491204738616943 mm for frame 20

Lowest mean error: 2.561166763305664 mm for frame 128

Saving results

Total time: 37.90620708465576
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_004/1073/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_004/1073.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_004/1073
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01018140
Iteration 2/25 | Loss: 0.00146349
Iteration 3/25 | Loss: 0.00124100
Iteration 4/25 | Loss: 0.00121311
Iteration 5/25 | Loss: 0.00116459
Iteration 6/25 | Loss: 0.00111113
Iteration 7/25 | Loss: 0.00109840
Iteration 8/25 | Loss: 0.00109041
Iteration 9/25 | Loss: 0.00108821
Iteration 10/25 | Loss: 0.00109019
Iteration 11/25 | Loss: 0.00108915
Iteration 12/25 | Loss: 0.00108792
Iteration 13/25 | Loss: 0.00109313
Iteration 14/25 | Loss: 0.00109209
Iteration 15/25 | Loss: 0.00109239
Iteration 16/25 | Loss: 0.00108960
Iteration 17/25 | Loss: 0.00108776
Iteration 18/25 | Loss: 0.00108855
Iteration 19/25 | Loss: 0.00108991
Iteration 20/25 | Loss: 0.00108773
Iteration 21/25 | Loss: 0.00108656
Iteration 22/25 | Loss: 0.00108631
Iteration 23/25 | Loss: 0.00108616
Iteration 24/25 | Loss: 0.00108606
Iteration 25/25 | Loss: 0.00108571

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.66638565
Iteration 2/25 | Loss: 0.00072973
Iteration 3/25 | Loss: 0.00072973
Iteration 4/25 | Loss: 0.00072973
Iteration 5/25 | Loss: 0.00072973
Iteration 6/25 | Loss: 0.00072973
Iteration 7/25 | Loss: 0.00072973
Iteration 8/25 | Loss: 0.00072973
Iteration 9/25 | Loss: 0.00072973
Iteration 10/25 | Loss: 0.00072973
Iteration 11/25 | Loss: 0.00072973
Iteration 12/25 | Loss: 0.00072973
Iteration 13/25 | Loss: 0.00072973
Iteration 14/25 | Loss: 0.00072973
Iteration 15/25 | Loss: 0.00072973
Iteration 16/25 | Loss: 0.00072973
Iteration 17/25 | Loss: 0.00072973
Iteration 18/25 | Loss: 0.00072973
Iteration 19/25 | Loss: 0.00072973
Iteration 20/25 | Loss: 0.00072973
Iteration 21/25 | Loss: 0.00072973
Iteration 22/25 | Loss: 0.00072973
Iteration 23/25 | Loss: 0.00072973
Iteration 24/25 | Loss: 0.00072973
Iteration 25/25 | Loss: 0.00072973
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.000729726511053741, 0.000729726511053741, 0.000729726511053741, 0.000729726511053741, 0.000729726511053741]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000729726511053741

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00072973
Iteration 2/1000 | Loss: 0.00002301
Iteration 3/1000 | Loss: 0.00001719
Iteration 4/1000 | Loss: 0.00001579
Iteration 5/1000 | Loss: 0.00001487
Iteration 6/1000 | Loss: 0.00001441
Iteration 7/1000 | Loss: 0.00001406
Iteration 8/1000 | Loss: 0.00015567
Iteration 9/1000 | Loss: 0.00002473
Iteration 10/1000 | Loss: 0.00015143
Iteration 11/1000 | Loss: 0.00015971
Iteration 12/1000 | Loss: 0.00015573
Iteration 13/1000 | Loss: 0.00030643
Iteration 14/1000 | Loss: 0.00010211
Iteration 15/1000 | Loss: 0.00023607
Iteration 16/1000 | Loss: 0.00017116
Iteration 17/1000 | Loss: 0.00016329
Iteration 18/1000 | Loss: 0.00032122
Iteration 19/1000 | Loss: 0.00055294
Iteration 20/1000 | Loss: 0.00002100
Iteration 21/1000 | Loss: 0.00001696
Iteration 22/1000 | Loss: 0.00001548
Iteration 23/1000 | Loss: 0.00001458
Iteration 24/1000 | Loss: 0.00001410
Iteration 25/1000 | Loss: 0.00001366
Iteration 26/1000 | Loss: 0.00001336
Iteration 27/1000 | Loss: 0.00001316
Iteration 28/1000 | Loss: 0.00001313
Iteration 29/1000 | Loss: 0.00001297
Iteration 30/1000 | Loss: 0.00001283
Iteration 31/1000 | Loss: 0.00001781
Iteration 32/1000 | Loss: 0.00001312
Iteration 33/1000 | Loss: 0.00001269
Iteration 34/1000 | Loss: 0.00001256
Iteration 35/1000 | Loss: 0.00001253
Iteration 36/1000 | Loss: 0.00001246
Iteration 37/1000 | Loss: 0.00001244
Iteration 38/1000 | Loss: 0.00001240
Iteration 39/1000 | Loss: 0.00001239
Iteration 40/1000 | Loss: 0.00001238
Iteration 41/1000 | Loss: 0.00001234
Iteration 42/1000 | Loss: 0.00001232
Iteration 43/1000 | Loss: 0.00001228
Iteration 44/1000 | Loss: 0.00001228
Iteration 45/1000 | Loss: 0.00001228
Iteration 46/1000 | Loss: 0.00001228
Iteration 47/1000 | Loss: 0.00001228
Iteration 48/1000 | Loss: 0.00001228
Iteration 49/1000 | Loss: 0.00001228
Iteration 50/1000 | Loss: 0.00001228
Iteration 51/1000 | Loss: 0.00001227
Iteration 52/1000 | Loss: 0.00001227
Iteration 53/1000 | Loss: 0.00001227
Iteration 54/1000 | Loss: 0.00001227
Iteration 55/1000 | Loss: 0.00001226
Iteration 56/1000 | Loss: 0.00001226
Iteration 57/1000 | Loss: 0.00001226
Iteration 58/1000 | Loss: 0.00001225
Iteration 59/1000 | Loss: 0.00001225
Iteration 60/1000 | Loss: 0.00001224
Iteration 61/1000 | Loss: 0.00001223
Iteration 62/1000 | Loss: 0.00001223
Iteration 63/1000 | Loss: 0.00001223
Iteration 64/1000 | Loss: 0.00001223
Iteration 65/1000 | Loss: 0.00001223
Iteration 66/1000 | Loss: 0.00001223
Iteration 67/1000 | Loss: 0.00001223
Iteration 68/1000 | Loss: 0.00001223
Iteration 69/1000 | Loss: 0.00001223
Iteration 70/1000 | Loss: 0.00001223
Iteration 71/1000 | Loss: 0.00001222
Iteration 72/1000 | Loss: 0.00001222
Iteration 73/1000 | Loss: 0.00001222
Iteration 74/1000 | Loss: 0.00001221
Iteration 75/1000 | Loss: 0.00001220
Iteration 76/1000 | Loss: 0.00001219
Iteration 77/1000 | Loss: 0.00001219
Iteration 78/1000 | Loss: 0.00001218
Iteration 79/1000 | Loss: 0.00001218
Iteration 80/1000 | Loss: 0.00001218
Iteration 81/1000 | Loss: 0.00001217
Iteration 82/1000 | Loss: 0.00006609
Iteration 83/1000 | Loss: 0.00004702
Iteration 84/1000 | Loss: 0.00001232
Iteration 85/1000 | Loss: 0.00001212
Iteration 86/1000 | Loss: 0.00001209
Iteration 87/1000 | Loss: 0.00001208
Iteration 88/1000 | Loss: 0.00001208
Iteration 89/1000 | Loss: 0.00001207
Iteration 90/1000 | Loss: 0.00001207
Iteration 91/1000 | Loss: 0.00001207
Iteration 92/1000 | Loss: 0.00001207
Iteration 93/1000 | Loss: 0.00001206
Iteration 94/1000 | Loss: 0.00001205
Iteration 95/1000 | Loss: 0.00001205
Iteration 96/1000 | Loss: 0.00001205
Iteration 97/1000 | Loss: 0.00001205
Iteration 98/1000 | Loss: 0.00001204
Iteration 99/1000 | Loss: 0.00001204
Iteration 100/1000 | Loss: 0.00001204
Iteration 101/1000 | Loss: 0.00001203
Iteration 102/1000 | Loss: 0.00001203
Iteration 103/1000 | Loss: 0.00001202
Iteration 104/1000 | Loss: 0.00001202
Iteration 105/1000 | Loss: 0.00001202
Iteration 106/1000 | Loss: 0.00001202
Iteration 107/1000 | Loss: 0.00001202
Iteration 108/1000 | Loss: 0.00001202
Iteration 109/1000 | Loss: 0.00001202
Iteration 110/1000 | Loss: 0.00001202
Iteration 111/1000 | Loss: 0.00001202
Iteration 112/1000 | Loss: 0.00001202
Iteration 113/1000 | Loss: 0.00001201
Iteration 114/1000 | Loss: 0.00001201
Iteration 115/1000 | Loss: 0.00001201
Iteration 116/1000 | Loss: 0.00001201
Iteration 117/1000 | Loss: 0.00001201
Iteration 118/1000 | Loss: 0.00001200
Iteration 119/1000 | Loss: 0.00001200
Iteration 120/1000 | Loss: 0.00001200
Iteration 121/1000 | Loss: 0.00001200
Iteration 122/1000 | Loss: 0.00001200
Iteration 123/1000 | Loss: 0.00001200
Iteration 124/1000 | Loss: 0.00001199
Iteration 125/1000 | Loss: 0.00001199
Iteration 126/1000 | Loss: 0.00001199
Iteration 127/1000 | Loss: 0.00001198
Iteration 128/1000 | Loss: 0.00001198
Iteration 129/1000 | Loss: 0.00001198
Iteration 130/1000 | Loss: 0.00001198
Iteration 131/1000 | Loss: 0.00001198
Iteration 132/1000 | Loss: 0.00001198
Iteration 133/1000 | Loss: 0.00001197
Iteration 134/1000 | Loss: 0.00001197
Iteration 135/1000 | Loss: 0.00001197
Iteration 136/1000 | Loss: 0.00001197
Iteration 137/1000 | Loss: 0.00001197
Iteration 138/1000 | Loss: 0.00001197
Iteration 139/1000 | Loss: 0.00001196
Iteration 140/1000 | Loss: 0.00001196
Iteration 141/1000 | Loss: 0.00001196
Iteration 142/1000 | Loss: 0.00001196
Iteration 143/1000 | Loss: 0.00001196
Iteration 144/1000 | Loss: 0.00001196
Iteration 145/1000 | Loss: 0.00001196
Iteration 146/1000 | Loss: 0.00001196
Iteration 147/1000 | Loss: 0.00001196
Iteration 148/1000 | Loss: 0.00001196
Iteration 149/1000 | Loss: 0.00001196
Iteration 150/1000 | Loss: 0.00001196
Iteration 151/1000 | Loss: 0.00001196
Iteration 152/1000 | Loss: 0.00001196
Iteration 153/1000 | Loss: 0.00001196
Iteration 154/1000 | Loss: 0.00001196
Iteration 155/1000 | Loss: 0.00001196
Iteration 156/1000 | Loss: 0.00001196
Iteration 157/1000 | Loss: 0.00001196
Iteration 158/1000 | Loss: 0.00001196
Iteration 159/1000 | Loss: 0.00001196
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 159. Stopping optimization.
Last 5 losses: [1.195685126731405e-05, 1.195685126731405e-05, 1.195685126731405e-05, 1.195685126731405e-05, 1.195685126731405e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.195685126731405e-05

Optimization complete. Final v2v error: 2.8025496006011963 mm

Highest mean error: 5.085755825042725 mm for frame 9

Lowest mean error: 2.556919574737549 mm for frame 28

Saving results

Total time: 119.89191603660583
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_004/1081/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_004/1081.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_004/1081
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00631544
Iteration 2/25 | Loss: 0.00137300
Iteration 3/25 | Loss: 0.00118358
Iteration 4/25 | Loss: 0.00115191
Iteration 5/25 | Loss: 0.00113769
Iteration 6/25 | Loss: 0.00113463
Iteration 7/25 | Loss: 0.00113352
Iteration 8/25 | Loss: 0.00113287
Iteration 9/25 | Loss: 0.00113256
Iteration 10/25 | Loss: 0.00112805
Iteration 11/25 | Loss: 0.00112667
Iteration 12/25 | Loss: 0.00112616
Iteration 13/25 | Loss: 0.00113265
Iteration 14/25 | Loss: 0.00112538
Iteration 15/25 | Loss: 0.00112370
Iteration 16/25 | Loss: 0.00112339
Iteration 17/25 | Loss: 0.00112337
Iteration 18/25 | Loss: 0.00112337
Iteration 19/25 | Loss: 0.00112335
Iteration 20/25 | Loss: 0.00112335
Iteration 21/25 | Loss: 0.00112335
Iteration 22/25 | Loss: 0.00112335
Iteration 23/25 | Loss: 0.00112335
Iteration 24/25 | Loss: 0.00112335
Iteration 25/25 | Loss: 0.00112335

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.67580032
Iteration 2/25 | Loss: 0.00102653
Iteration 3/25 | Loss: 0.00102652
Iteration 4/25 | Loss: 0.00102652
Iteration 5/25 | Loss: 0.00102652
Iteration 6/25 | Loss: 0.00102652
Iteration 7/25 | Loss: 0.00102652
Iteration 8/25 | Loss: 0.00102652
Iteration 9/25 | Loss: 0.00102652
Iteration 10/25 | Loss: 0.00102652
Iteration 11/25 | Loss: 0.00102652
Iteration 12/25 | Loss: 0.00102652
Iteration 13/25 | Loss: 0.00102652
Iteration 14/25 | Loss: 0.00102652
Iteration 15/25 | Loss: 0.00102652
Iteration 16/25 | Loss: 0.00102652
Iteration 17/25 | Loss: 0.00102652
Iteration 18/25 | Loss: 0.00102652
Iteration 19/25 | Loss: 0.00102652
Iteration 20/25 | Loss: 0.00102652
Iteration 21/25 | Loss: 0.00102652
Iteration 22/25 | Loss: 0.00102652
Iteration 23/25 | Loss: 0.00102652
Iteration 24/25 | Loss: 0.00102652
Iteration 25/25 | Loss: 0.00102652

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00102652
Iteration 2/1000 | Loss: 0.00006109
Iteration 3/1000 | Loss: 0.00004054
Iteration 4/1000 | Loss: 0.00002992
Iteration 5/1000 | Loss: 0.00002777
Iteration 6/1000 | Loss: 0.00002656
Iteration 7/1000 | Loss: 0.00002524
Iteration 8/1000 | Loss: 0.00022818
Iteration 9/1000 | Loss: 0.00030202
Iteration 10/1000 | Loss: 0.00002983
Iteration 11/1000 | Loss: 0.00002485
Iteration 12/1000 | Loss: 0.00002377
Iteration 13/1000 | Loss: 0.00002341
Iteration 14/1000 | Loss: 0.00021968
Iteration 15/1000 | Loss: 0.00003668
Iteration 16/1000 | Loss: 0.00002755
Iteration 17/1000 | Loss: 0.00002378
Iteration 18/1000 | Loss: 0.00002299
Iteration 19/1000 | Loss: 0.00002278
Iteration 20/1000 | Loss: 0.00002259
Iteration 21/1000 | Loss: 0.00002244
Iteration 22/1000 | Loss: 0.00002237
Iteration 23/1000 | Loss: 0.00002237
Iteration 24/1000 | Loss: 0.00002237
Iteration 25/1000 | Loss: 0.00002221
Iteration 26/1000 | Loss: 0.00002205
Iteration 27/1000 | Loss: 0.00002204
Iteration 28/1000 | Loss: 0.00002191
Iteration 29/1000 | Loss: 0.00002189
Iteration 30/1000 | Loss: 0.00002189
Iteration 31/1000 | Loss: 0.00002175
Iteration 32/1000 | Loss: 0.00002175
Iteration 33/1000 | Loss: 0.00002174
Iteration 34/1000 | Loss: 0.00002174
Iteration 35/1000 | Loss: 0.00002173
Iteration 36/1000 | Loss: 0.00002173
Iteration 37/1000 | Loss: 0.00002173
Iteration 38/1000 | Loss: 0.00002172
Iteration 39/1000 | Loss: 0.00002172
Iteration 40/1000 | Loss: 0.00002172
Iteration 41/1000 | Loss: 0.00002172
Iteration 42/1000 | Loss: 0.00002172
Iteration 43/1000 | Loss: 0.00002172
Iteration 44/1000 | Loss: 0.00002172
Iteration 45/1000 | Loss: 0.00002172
Iteration 46/1000 | Loss: 0.00002172
Iteration 47/1000 | Loss: 0.00002172
Iteration 48/1000 | Loss: 0.00002172
Iteration 49/1000 | Loss: 0.00002172
Iteration 50/1000 | Loss: 0.00002172
Iteration 51/1000 | Loss: 0.00002171
Iteration 52/1000 | Loss: 0.00002171
Iteration 53/1000 | Loss: 0.00002171
Iteration 54/1000 | Loss: 0.00002171
Iteration 55/1000 | Loss: 0.00002171
Iteration 56/1000 | Loss: 0.00002171
Iteration 57/1000 | Loss: 0.00002170
Iteration 58/1000 | Loss: 0.00002170
Iteration 59/1000 | Loss: 0.00002169
Iteration 60/1000 | Loss: 0.00002169
Iteration 61/1000 | Loss: 0.00002169
Iteration 62/1000 | Loss: 0.00002168
Iteration 63/1000 | Loss: 0.00002168
Iteration 64/1000 | Loss: 0.00002167
Iteration 65/1000 | Loss: 0.00002167
Iteration 66/1000 | Loss: 0.00002167
Iteration 67/1000 | Loss: 0.00002167
Iteration 68/1000 | Loss: 0.00002166
Iteration 69/1000 | Loss: 0.00002166
Iteration 70/1000 | Loss: 0.00002166
Iteration 71/1000 | Loss: 0.00002166
Iteration 72/1000 | Loss: 0.00002166
Iteration 73/1000 | Loss: 0.00002166
Iteration 74/1000 | Loss: 0.00002166
Iteration 75/1000 | Loss: 0.00002165
Iteration 76/1000 | Loss: 0.00002165
Iteration 77/1000 | Loss: 0.00002165
Iteration 78/1000 | Loss: 0.00002164
Iteration 79/1000 | Loss: 0.00002164
Iteration 80/1000 | Loss: 0.00002164
Iteration 81/1000 | Loss: 0.00002164
Iteration 82/1000 | Loss: 0.00002163
Iteration 83/1000 | Loss: 0.00002163
Iteration 84/1000 | Loss: 0.00002163
Iteration 85/1000 | Loss: 0.00002163
Iteration 86/1000 | Loss: 0.00002162
Iteration 87/1000 | Loss: 0.00002162
Iteration 88/1000 | Loss: 0.00002162
Iteration 89/1000 | Loss: 0.00002161
Iteration 90/1000 | Loss: 0.00002161
Iteration 91/1000 | Loss: 0.00002161
Iteration 92/1000 | Loss: 0.00002160
Iteration 93/1000 | Loss: 0.00002160
Iteration 94/1000 | Loss: 0.00002159
Iteration 95/1000 | Loss: 0.00002159
Iteration 96/1000 | Loss: 0.00002159
Iteration 97/1000 | Loss: 0.00002158
Iteration 98/1000 | Loss: 0.00002158
Iteration 99/1000 | Loss: 0.00002158
Iteration 100/1000 | Loss: 0.00002158
Iteration 101/1000 | Loss: 0.00002158
Iteration 102/1000 | Loss: 0.00002158
Iteration 103/1000 | Loss: 0.00002158
Iteration 104/1000 | Loss: 0.00002157
Iteration 105/1000 | Loss: 0.00002157
Iteration 106/1000 | Loss: 0.00002157
Iteration 107/1000 | Loss: 0.00002156
Iteration 108/1000 | Loss: 0.00002156
Iteration 109/1000 | Loss: 0.00002154
Iteration 110/1000 | Loss: 0.00002154
Iteration 111/1000 | Loss: 0.00002154
Iteration 112/1000 | Loss: 0.00002154
Iteration 113/1000 | Loss: 0.00002154
Iteration 114/1000 | Loss: 0.00002154
Iteration 115/1000 | Loss: 0.00002153
Iteration 116/1000 | Loss: 0.00002153
Iteration 117/1000 | Loss: 0.00002153
Iteration 118/1000 | Loss: 0.00002153
Iteration 119/1000 | Loss: 0.00002153
Iteration 120/1000 | Loss: 0.00002153
Iteration 121/1000 | Loss: 0.00002153
Iteration 122/1000 | Loss: 0.00002153
Iteration 123/1000 | Loss: 0.00002152
Iteration 124/1000 | Loss: 0.00002152
Iteration 125/1000 | Loss: 0.00002152
Iteration 126/1000 | Loss: 0.00002151
Iteration 127/1000 | Loss: 0.00002151
Iteration 128/1000 | Loss: 0.00002151
Iteration 129/1000 | Loss: 0.00002151
Iteration 130/1000 | Loss: 0.00002151
Iteration 131/1000 | Loss: 0.00002150
Iteration 132/1000 | Loss: 0.00002150
Iteration 133/1000 | Loss: 0.00002150
Iteration 134/1000 | Loss: 0.00002150
Iteration 135/1000 | Loss: 0.00002150
Iteration 136/1000 | Loss: 0.00002150
Iteration 137/1000 | Loss: 0.00002150
Iteration 138/1000 | Loss: 0.00002150
Iteration 139/1000 | Loss: 0.00002150
Iteration 140/1000 | Loss: 0.00002150
Iteration 141/1000 | Loss: 0.00002149
Iteration 142/1000 | Loss: 0.00002149
Iteration 143/1000 | Loss: 0.00002149
Iteration 144/1000 | Loss: 0.00002149
Iteration 145/1000 | Loss: 0.00002149
Iteration 146/1000 | Loss: 0.00002149
Iteration 147/1000 | Loss: 0.00002149
Iteration 148/1000 | Loss: 0.00002149
Iteration 149/1000 | Loss: 0.00002149
Iteration 150/1000 | Loss: 0.00002149
Iteration 151/1000 | Loss: 0.00002149
Iteration 152/1000 | Loss: 0.00002149
Iteration 153/1000 | Loss: 0.00002148
Iteration 154/1000 | Loss: 0.00002148
Iteration 155/1000 | Loss: 0.00002148
Iteration 156/1000 | Loss: 0.00002148
Iteration 157/1000 | Loss: 0.00002148
Iteration 158/1000 | Loss: 0.00002148
Iteration 159/1000 | Loss: 0.00002148
Iteration 160/1000 | Loss: 0.00002148
Iteration 161/1000 | Loss: 0.00002148
Iteration 162/1000 | Loss: 0.00002148
Iteration 163/1000 | Loss: 0.00002148
Iteration 164/1000 | Loss: 0.00002148
Iteration 165/1000 | Loss: 0.00002148
Iteration 166/1000 | Loss: 0.00002148
Iteration 167/1000 | Loss: 0.00002148
Iteration 168/1000 | Loss: 0.00002148
Iteration 169/1000 | Loss: 0.00002148
Iteration 170/1000 | Loss: 0.00002148
Iteration 171/1000 | Loss: 0.00002148
Iteration 172/1000 | Loss: 0.00002148
Iteration 173/1000 | Loss: 0.00002148
Iteration 174/1000 | Loss: 0.00002148
Iteration 175/1000 | Loss: 0.00002148
Iteration 176/1000 | Loss: 0.00002148
Iteration 177/1000 | Loss: 0.00002148
Iteration 178/1000 | Loss: 0.00002148
Iteration 179/1000 | Loss: 0.00002148
Iteration 180/1000 | Loss: 0.00002148
Iteration 181/1000 | Loss: 0.00002148
Iteration 182/1000 | Loss: 0.00002148
Iteration 183/1000 | Loss: 0.00002148
Iteration 184/1000 | Loss: 0.00002148
Iteration 185/1000 | Loss: 0.00002148
Iteration 186/1000 | Loss: 0.00002148
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 186. Stopping optimization.
Last 5 losses: [2.1477926566149108e-05, 2.1477926566149108e-05, 2.1477926566149108e-05, 2.1477926566149108e-05, 2.1477926566149108e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1477926566149108e-05

Optimization complete. Final v2v error: 3.8121707439422607 mm

Highest mean error: 4.846600532531738 mm for frame 59

Lowest mean error: 3.117466449737549 mm for frame 80

Saving results

Total time: 74.4219708442688
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_004/1038/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_004/1038.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_004/1038
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00891537
Iteration 2/25 | Loss: 0.00124640
Iteration 3/25 | Loss: 0.00112241
Iteration 4/25 | Loss: 0.00110106
Iteration 5/25 | Loss: 0.00109378
Iteration 6/25 | Loss: 0.00109183
Iteration 7/25 | Loss: 0.00109180
Iteration 8/25 | Loss: 0.00109180
Iteration 9/25 | Loss: 0.00109170
Iteration 10/25 | Loss: 0.00109170
Iteration 11/25 | Loss: 0.00109170
Iteration 12/25 | Loss: 0.00109170
Iteration 13/25 | Loss: 0.00109170
Iteration 14/25 | Loss: 0.00109170
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0010916981846094131, 0.0010916981846094131, 0.0010916981846094131, 0.0010916981846094131, 0.0010916981846094131]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010916981846094131

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38488472
Iteration 2/25 | Loss: 0.00071061
Iteration 3/25 | Loss: 0.00071059
Iteration 4/25 | Loss: 0.00071059
Iteration 5/25 | Loss: 0.00071059
Iteration 6/25 | Loss: 0.00071059
Iteration 7/25 | Loss: 0.00071059
Iteration 8/25 | Loss: 0.00071059
Iteration 9/25 | Loss: 0.00071059
Iteration 10/25 | Loss: 0.00071059
Iteration 11/25 | Loss: 0.00071059
Iteration 12/25 | Loss: 0.00071059
Iteration 13/25 | Loss: 0.00071059
Iteration 14/25 | Loss: 0.00071059
Iteration 15/25 | Loss: 0.00071059
Iteration 16/25 | Loss: 0.00071059
Iteration 17/25 | Loss: 0.00071059
Iteration 18/25 | Loss: 0.00071059
Iteration 19/25 | Loss: 0.00071059
Iteration 20/25 | Loss: 0.00071059
Iteration 21/25 | Loss: 0.00071059
Iteration 22/25 | Loss: 0.00071059
Iteration 23/25 | Loss: 0.00071059
Iteration 24/25 | Loss: 0.00071059
Iteration 25/25 | Loss: 0.00071059

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00071059
Iteration 2/1000 | Loss: 0.00003191
Iteration 3/1000 | Loss: 0.00002257
Iteration 4/1000 | Loss: 0.00002003
Iteration 5/1000 | Loss: 0.00001889
Iteration 6/1000 | Loss: 0.00001804
Iteration 7/1000 | Loss: 0.00001760
Iteration 8/1000 | Loss: 0.00001717
Iteration 9/1000 | Loss: 0.00001679
Iteration 10/1000 | Loss: 0.00001661
Iteration 11/1000 | Loss: 0.00001660
Iteration 12/1000 | Loss: 0.00001644
Iteration 13/1000 | Loss: 0.00001642
Iteration 14/1000 | Loss: 0.00001629
Iteration 15/1000 | Loss: 0.00001628
Iteration 16/1000 | Loss: 0.00001627
Iteration 17/1000 | Loss: 0.00001626
Iteration 18/1000 | Loss: 0.00001618
Iteration 19/1000 | Loss: 0.00001617
Iteration 20/1000 | Loss: 0.00001617
Iteration 21/1000 | Loss: 0.00001616
Iteration 22/1000 | Loss: 0.00001615
Iteration 23/1000 | Loss: 0.00001611
Iteration 24/1000 | Loss: 0.00001610
Iteration 25/1000 | Loss: 0.00001609
Iteration 26/1000 | Loss: 0.00001602
Iteration 27/1000 | Loss: 0.00001599
Iteration 28/1000 | Loss: 0.00001599
Iteration 29/1000 | Loss: 0.00001598
Iteration 30/1000 | Loss: 0.00001598
Iteration 31/1000 | Loss: 0.00001597
Iteration 32/1000 | Loss: 0.00001596
Iteration 33/1000 | Loss: 0.00001596
Iteration 34/1000 | Loss: 0.00001595
Iteration 35/1000 | Loss: 0.00001594
Iteration 36/1000 | Loss: 0.00001594
Iteration 37/1000 | Loss: 0.00001593
Iteration 38/1000 | Loss: 0.00001593
Iteration 39/1000 | Loss: 0.00001592
Iteration 40/1000 | Loss: 0.00001592
Iteration 41/1000 | Loss: 0.00001591
Iteration 42/1000 | Loss: 0.00001591
Iteration 43/1000 | Loss: 0.00001591
Iteration 44/1000 | Loss: 0.00001590
Iteration 45/1000 | Loss: 0.00001590
Iteration 46/1000 | Loss: 0.00001589
Iteration 47/1000 | Loss: 0.00001589
Iteration 48/1000 | Loss: 0.00001588
Iteration 49/1000 | Loss: 0.00001588
Iteration 50/1000 | Loss: 0.00001587
Iteration 51/1000 | Loss: 0.00001587
Iteration 52/1000 | Loss: 0.00001587
Iteration 53/1000 | Loss: 0.00001586
Iteration 54/1000 | Loss: 0.00001586
Iteration 55/1000 | Loss: 0.00001584
Iteration 56/1000 | Loss: 0.00001584
Iteration 57/1000 | Loss: 0.00001584
Iteration 58/1000 | Loss: 0.00001583
Iteration 59/1000 | Loss: 0.00001583
Iteration 60/1000 | Loss: 0.00001583
Iteration 61/1000 | Loss: 0.00001583
Iteration 62/1000 | Loss: 0.00001583
Iteration 63/1000 | Loss: 0.00001583
Iteration 64/1000 | Loss: 0.00001582
Iteration 65/1000 | Loss: 0.00001582
Iteration 66/1000 | Loss: 0.00001581
Iteration 67/1000 | Loss: 0.00001581
Iteration 68/1000 | Loss: 0.00001581
Iteration 69/1000 | Loss: 0.00001580
Iteration 70/1000 | Loss: 0.00001580
Iteration 71/1000 | Loss: 0.00001579
Iteration 72/1000 | Loss: 0.00001579
Iteration 73/1000 | Loss: 0.00001579
Iteration 74/1000 | Loss: 0.00001579
Iteration 75/1000 | Loss: 0.00001578
Iteration 76/1000 | Loss: 0.00001578
Iteration 77/1000 | Loss: 0.00001577
Iteration 78/1000 | Loss: 0.00001577
Iteration 79/1000 | Loss: 0.00001577
Iteration 80/1000 | Loss: 0.00001577
Iteration 81/1000 | Loss: 0.00001576
Iteration 82/1000 | Loss: 0.00001576
Iteration 83/1000 | Loss: 0.00001576
Iteration 84/1000 | Loss: 0.00001576
Iteration 85/1000 | Loss: 0.00001576
Iteration 86/1000 | Loss: 0.00001575
Iteration 87/1000 | Loss: 0.00001575
Iteration 88/1000 | Loss: 0.00001575
Iteration 89/1000 | Loss: 0.00001575
Iteration 90/1000 | Loss: 0.00001575
Iteration 91/1000 | Loss: 0.00001574
Iteration 92/1000 | Loss: 0.00001573
Iteration 93/1000 | Loss: 0.00001573
Iteration 94/1000 | Loss: 0.00001573
Iteration 95/1000 | Loss: 0.00001572
Iteration 96/1000 | Loss: 0.00001572
Iteration 97/1000 | Loss: 0.00001572
Iteration 98/1000 | Loss: 0.00001572
Iteration 99/1000 | Loss: 0.00001572
Iteration 100/1000 | Loss: 0.00001572
Iteration 101/1000 | Loss: 0.00001572
Iteration 102/1000 | Loss: 0.00001572
Iteration 103/1000 | Loss: 0.00001571
Iteration 104/1000 | Loss: 0.00001571
Iteration 105/1000 | Loss: 0.00001571
Iteration 106/1000 | Loss: 0.00001571
Iteration 107/1000 | Loss: 0.00001570
Iteration 108/1000 | Loss: 0.00001570
Iteration 109/1000 | Loss: 0.00001570
Iteration 110/1000 | Loss: 0.00001570
Iteration 111/1000 | Loss: 0.00001570
Iteration 112/1000 | Loss: 0.00001570
Iteration 113/1000 | Loss: 0.00001569
Iteration 114/1000 | Loss: 0.00001569
Iteration 115/1000 | Loss: 0.00001569
Iteration 116/1000 | Loss: 0.00001569
Iteration 117/1000 | Loss: 0.00001569
Iteration 118/1000 | Loss: 0.00001569
Iteration 119/1000 | Loss: 0.00001569
Iteration 120/1000 | Loss: 0.00001569
Iteration 121/1000 | Loss: 0.00001569
Iteration 122/1000 | Loss: 0.00001569
Iteration 123/1000 | Loss: 0.00001568
Iteration 124/1000 | Loss: 0.00001568
Iteration 125/1000 | Loss: 0.00001568
Iteration 126/1000 | Loss: 0.00001568
Iteration 127/1000 | Loss: 0.00001568
Iteration 128/1000 | Loss: 0.00001568
Iteration 129/1000 | Loss: 0.00001568
Iteration 130/1000 | Loss: 0.00001568
Iteration 131/1000 | Loss: 0.00001568
Iteration 132/1000 | Loss: 0.00001568
Iteration 133/1000 | Loss: 0.00001568
Iteration 134/1000 | Loss: 0.00001568
Iteration 135/1000 | Loss: 0.00001567
Iteration 136/1000 | Loss: 0.00001567
Iteration 137/1000 | Loss: 0.00001567
Iteration 138/1000 | Loss: 0.00001567
Iteration 139/1000 | Loss: 0.00001566
Iteration 140/1000 | Loss: 0.00001566
Iteration 141/1000 | Loss: 0.00001566
Iteration 142/1000 | Loss: 0.00001566
Iteration 143/1000 | Loss: 0.00001566
Iteration 144/1000 | Loss: 0.00001566
Iteration 145/1000 | Loss: 0.00001565
Iteration 146/1000 | Loss: 0.00001565
Iteration 147/1000 | Loss: 0.00001565
Iteration 148/1000 | Loss: 0.00001564
Iteration 149/1000 | Loss: 0.00001564
Iteration 150/1000 | Loss: 0.00001564
Iteration 151/1000 | Loss: 0.00001564
Iteration 152/1000 | Loss: 0.00001564
Iteration 153/1000 | Loss: 0.00001563
Iteration 154/1000 | Loss: 0.00001563
Iteration 155/1000 | Loss: 0.00001563
Iteration 156/1000 | Loss: 0.00001563
Iteration 157/1000 | Loss: 0.00001563
Iteration 158/1000 | Loss: 0.00001563
Iteration 159/1000 | Loss: 0.00001563
Iteration 160/1000 | Loss: 0.00001562
Iteration 161/1000 | Loss: 0.00001562
Iteration 162/1000 | Loss: 0.00001562
Iteration 163/1000 | Loss: 0.00001562
Iteration 164/1000 | Loss: 0.00001562
Iteration 165/1000 | Loss: 0.00001562
Iteration 166/1000 | Loss: 0.00001561
Iteration 167/1000 | Loss: 0.00001561
Iteration 168/1000 | Loss: 0.00001561
Iteration 169/1000 | Loss: 0.00001561
Iteration 170/1000 | Loss: 0.00001561
Iteration 171/1000 | Loss: 0.00001560
Iteration 172/1000 | Loss: 0.00001560
Iteration 173/1000 | Loss: 0.00001560
Iteration 174/1000 | Loss: 0.00001560
Iteration 175/1000 | Loss: 0.00001560
Iteration 176/1000 | Loss: 0.00001560
Iteration 177/1000 | Loss: 0.00001559
Iteration 178/1000 | Loss: 0.00001559
Iteration 179/1000 | Loss: 0.00001559
Iteration 180/1000 | Loss: 0.00001559
Iteration 181/1000 | Loss: 0.00001559
Iteration 182/1000 | Loss: 0.00001559
Iteration 183/1000 | Loss: 0.00001559
Iteration 184/1000 | Loss: 0.00001559
Iteration 185/1000 | Loss: 0.00001559
Iteration 186/1000 | Loss: 0.00001559
Iteration 187/1000 | Loss: 0.00001558
Iteration 188/1000 | Loss: 0.00001558
Iteration 189/1000 | Loss: 0.00001558
Iteration 190/1000 | Loss: 0.00001558
Iteration 191/1000 | Loss: 0.00001558
Iteration 192/1000 | Loss: 0.00001557
Iteration 193/1000 | Loss: 0.00001557
Iteration 194/1000 | Loss: 0.00001557
Iteration 195/1000 | Loss: 0.00001557
Iteration 196/1000 | Loss: 0.00001557
Iteration 197/1000 | Loss: 0.00001557
Iteration 198/1000 | Loss: 0.00001557
Iteration 199/1000 | Loss: 0.00001557
Iteration 200/1000 | Loss: 0.00001556
Iteration 201/1000 | Loss: 0.00001556
Iteration 202/1000 | Loss: 0.00001556
Iteration 203/1000 | Loss: 0.00001556
Iteration 204/1000 | Loss: 0.00001556
Iteration 205/1000 | Loss: 0.00001556
Iteration 206/1000 | Loss: 0.00001556
Iteration 207/1000 | Loss: 0.00001556
Iteration 208/1000 | Loss: 0.00001556
Iteration 209/1000 | Loss: 0.00001556
Iteration 210/1000 | Loss: 0.00001556
Iteration 211/1000 | Loss: 0.00001556
Iteration 212/1000 | Loss: 0.00001556
Iteration 213/1000 | Loss: 0.00001556
Iteration 214/1000 | Loss: 0.00001555
Iteration 215/1000 | Loss: 0.00001555
Iteration 216/1000 | Loss: 0.00001555
Iteration 217/1000 | Loss: 0.00001555
Iteration 218/1000 | Loss: 0.00001555
Iteration 219/1000 | Loss: 0.00001555
Iteration 220/1000 | Loss: 0.00001555
Iteration 221/1000 | Loss: 0.00001555
Iteration 222/1000 | Loss: 0.00001555
Iteration 223/1000 | Loss: 0.00001555
Iteration 224/1000 | Loss: 0.00001555
Iteration 225/1000 | Loss: 0.00001555
Iteration 226/1000 | Loss: 0.00001555
Iteration 227/1000 | Loss: 0.00001555
Iteration 228/1000 | Loss: 0.00001555
Iteration 229/1000 | Loss: 0.00001555
Iteration 230/1000 | Loss: 0.00001555
Iteration 231/1000 | Loss: 0.00001555
Iteration 232/1000 | Loss: 0.00001554
Iteration 233/1000 | Loss: 0.00001554
Iteration 234/1000 | Loss: 0.00001554
Iteration 235/1000 | Loss: 0.00001554
Iteration 236/1000 | Loss: 0.00001554
Iteration 237/1000 | Loss: 0.00001554
Iteration 238/1000 | Loss: 0.00001554
Iteration 239/1000 | Loss: 0.00001554
Iteration 240/1000 | Loss: 0.00001554
Iteration 241/1000 | Loss: 0.00001554
Iteration 242/1000 | Loss: 0.00001554
Iteration 243/1000 | Loss: 0.00001554
Iteration 244/1000 | Loss: 0.00001554
Iteration 245/1000 | Loss: 0.00001554
Iteration 246/1000 | Loss: 0.00001554
Iteration 247/1000 | Loss: 0.00001554
Iteration 248/1000 | Loss: 0.00001554
Iteration 249/1000 | Loss: 0.00001554
Iteration 250/1000 | Loss: 0.00001554
Iteration 251/1000 | Loss: 0.00001554
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 251. Stopping optimization.
Last 5 losses: [1.5535335478489287e-05, 1.5535335478489287e-05, 1.5535335478489287e-05, 1.5535335478489287e-05, 1.5535335478489287e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5535335478489287e-05

Optimization complete. Final v2v error: 3.3176023960113525 mm

Highest mean error: 4.985384464263916 mm for frame 69

Lowest mean error: 2.8790524005889893 mm for frame 50

Saving results

Total time: 46.56147742271423
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_004/1001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_004/1001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_004/1001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00825768
Iteration 2/25 | Loss: 0.00165048
Iteration 3/25 | Loss: 0.00124803
Iteration 4/25 | Loss: 0.00122160
Iteration 5/25 | Loss: 0.00121792
Iteration 6/25 | Loss: 0.00121065
Iteration 7/25 | Loss: 0.00120586
Iteration 8/25 | Loss: 0.00120560
Iteration 9/25 | Loss: 0.00120552
Iteration 10/25 | Loss: 0.00120552
Iteration 11/25 | Loss: 0.00120552
Iteration 12/25 | Loss: 0.00120551
Iteration 13/25 | Loss: 0.00120551
Iteration 14/25 | Loss: 0.00120551
Iteration 15/25 | Loss: 0.00120551
Iteration 16/25 | Loss: 0.00120551
Iteration 17/25 | Loss: 0.00120551
Iteration 18/25 | Loss: 0.00120551
Iteration 19/25 | Loss: 0.00120551
Iteration 20/25 | Loss: 0.00120551
Iteration 21/25 | Loss: 0.00120551
Iteration 22/25 | Loss: 0.00120551
Iteration 23/25 | Loss: 0.00120551
Iteration 24/25 | Loss: 0.00120551
Iteration 25/25 | Loss: 0.00120551

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.26900756
Iteration 2/25 | Loss: 0.00080864
Iteration 3/25 | Loss: 0.00080864
Iteration 4/25 | Loss: 0.00080864
Iteration 5/25 | Loss: 0.00080864
Iteration 6/25 | Loss: 0.00080863
Iteration 7/25 | Loss: 0.00080863
Iteration 8/25 | Loss: 0.00080863
Iteration 9/25 | Loss: 0.00080863
Iteration 10/25 | Loss: 0.00080863
Iteration 11/25 | Loss: 0.00080863
Iteration 12/25 | Loss: 0.00080863
Iteration 13/25 | Loss: 0.00080863
Iteration 14/25 | Loss: 0.00080863
Iteration 15/25 | Loss: 0.00080863
Iteration 16/25 | Loss: 0.00080863
Iteration 17/25 | Loss: 0.00080863
Iteration 18/25 | Loss: 0.00080863
Iteration 19/25 | Loss: 0.00080863
Iteration 20/25 | Loss: 0.00080863
Iteration 21/25 | Loss: 0.00080863
Iteration 22/25 | Loss: 0.00080863
Iteration 23/25 | Loss: 0.00080863
Iteration 24/25 | Loss: 0.00080863
Iteration 25/25 | Loss: 0.00080863

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00080863
Iteration 2/1000 | Loss: 0.00003211
Iteration 3/1000 | Loss: 0.00002184
Iteration 4/1000 | Loss: 0.00001959
Iteration 5/1000 | Loss: 0.00001862
Iteration 6/1000 | Loss: 0.00001799
Iteration 7/1000 | Loss: 0.00001774
Iteration 8/1000 | Loss: 0.00001738
Iteration 9/1000 | Loss: 0.00001706
Iteration 10/1000 | Loss: 0.00001684
Iteration 11/1000 | Loss: 0.00001669
Iteration 12/1000 | Loss: 0.00001665
Iteration 13/1000 | Loss: 0.00001664
Iteration 14/1000 | Loss: 0.00001660
Iteration 15/1000 | Loss: 0.00001659
Iteration 16/1000 | Loss: 0.00001659
Iteration 17/1000 | Loss: 0.00001659
Iteration 18/1000 | Loss: 0.00001659
Iteration 19/1000 | Loss: 0.00001659
Iteration 20/1000 | Loss: 0.00001659
Iteration 21/1000 | Loss: 0.00001658
Iteration 22/1000 | Loss: 0.00001658
Iteration 23/1000 | Loss: 0.00001658
Iteration 24/1000 | Loss: 0.00001658
Iteration 25/1000 | Loss: 0.00001658
Iteration 26/1000 | Loss: 0.00001657
Iteration 27/1000 | Loss: 0.00001657
Iteration 28/1000 | Loss: 0.00001655
Iteration 29/1000 | Loss: 0.00001653
Iteration 30/1000 | Loss: 0.00001653
Iteration 31/1000 | Loss: 0.00001653
Iteration 32/1000 | Loss: 0.00001653
Iteration 33/1000 | Loss: 0.00001652
Iteration 34/1000 | Loss: 0.00001652
Iteration 35/1000 | Loss: 0.00001652
Iteration 36/1000 | Loss: 0.00001652
Iteration 37/1000 | Loss: 0.00001652
Iteration 38/1000 | Loss: 0.00001652
Iteration 39/1000 | Loss: 0.00001651
Iteration 40/1000 | Loss: 0.00001651
Iteration 41/1000 | Loss: 0.00001651
Iteration 42/1000 | Loss: 0.00001651
Iteration 43/1000 | Loss: 0.00001651
Iteration 44/1000 | Loss: 0.00001651
Iteration 45/1000 | Loss: 0.00001650
Iteration 46/1000 | Loss: 0.00001650
Iteration 47/1000 | Loss: 0.00001650
Iteration 48/1000 | Loss: 0.00001648
Iteration 49/1000 | Loss: 0.00001648
Iteration 50/1000 | Loss: 0.00001648
Iteration 51/1000 | Loss: 0.00001648
Iteration 52/1000 | Loss: 0.00001648
Iteration 53/1000 | Loss: 0.00001648
Iteration 54/1000 | Loss: 0.00001647
Iteration 55/1000 | Loss: 0.00001647
Iteration 56/1000 | Loss: 0.00001647
Iteration 57/1000 | Loss: 0.00001647
Iteration 58/1000 | Loss: 0.00001647
Iteration 59/1000 | Loss: 0.00001647
Iteration 60/1000 | Loss: 0.00001647
Iteration 61/1000 | Loss: 0.00001647
Iteration 62/1000 | Loss: 0.00001647
Iteration 63/1000 | Loss: 0.00001647
Iteration 64/1000 | Loss: 0.00001646
Iteration 65/1000 | Loss: 0.00001646
Iteration 66/1000 | Loss: 0.00001646
Iteration 67/1000 | Loss: 0.00001646
Iteration 68/1000 | Loss: 0.00001646
Iteration 69/1000 | Loss: 0.00001646
Iteration 70/1000 | Loss: 0.00001646
Iteration 71/1000 | Loss: 0.00001646
Iteration 72/1000 | Loss: 0.00001646
Iteration 73/1000 | Loss: 0.00001646
Iteration 74/1000 | Loss: 0.00001645
Iteration 75/1000 | Loss: 0.00001645
Iteration 76/1000 | Loss: 0.00001645
Iteration 77/1000 | Loss: 0.00001645
Iteration 78/1000 | Loss: 0.00001645
Iteration 79/1000 | Loss: 0.00001645
Iteration 80/1000 | Loss: 0.00001645
Iteration 81/1000 | Loss: 0.00001644
Iteration 82/1000 | Loss: 0.00001644
Iteration 83/1000 | Loss: 0.00001643
Iteration 84/1000 | Loss: 0.00001643
Iteration 85/1000 | Loss: 0.00001643
Iteration 86/1000 | Loss: 0.00001643
Iteration 87/1000 | Loss: 0.00001643
Iteration 88/1000 | Loss: 0.00001642
Iteration 89/1000 | Loss: 0.00001642
Iteration 90/1000 | Loss: 0.00001642
Iteration 91/1000 | Loss: 0.00001642
Iteration 92/1000 | Loss: 0.00001642
Iteration 93/1000 | Loss: 0.00001642
Iteration 94/1000 | Loss: 0.00001642
Iteration 95/1000 | Loss: 0.00001642
Iteration 96/1000 | Loss: 0.00001642
Iteration 97/1000 | Loss: 0.00001642
Iteration 98/1000 | Loss: 0.00001642
Iteration 99/1000 | Loss: 0.00001641
Iteration 100/1000 | Loss: 0.00001641
Iteration 101/1000 | Loss: 0.00001641
Iteration 102/1000 | Loss: 0.00001641
Iteration 103/1000 | Loss: 0.00001640
Iteration 104/1000 | Loss: 0.00001640
Iteration 105/1000 | Loss: 0.00001640
Iteration 106/1000 | Loss: 0.00001640
Iteration 107/1000 | Loss: 0.00001640
Iteration 108/1000 | Loss: 0.00001640
Iteration 109/1000 | Loss: 0.00001640
Iteration 110/1000 | Loss: 0.00001639
Iteration 111/1000 | Loss: 0.00001639
Iteration 112/1000 | Loss: 0.00001639
Iteration 113/1000 | Loss: 0.00001639
Iteration 114/1000 | Loss: 0.00001639
Iteration 115/1000 | Loss: 0.00001638
Iteration 116/1000 | Loss: 0.00001638
Iteration 117/1000 | Loss: 0.00001638
Iteration 118/1000 | Loss: 0.00001638
Iteration 119/1000 | Loss: 0.00001637
Iteration 120/1000 | Loss: 0.00001637
Iteration 121/1000 | Loss: 0.00001637
Iteration 122/1000 | Loss: 0.00001637
Iteration 123/1000 | Loss: 0.00001637
Iteration 124/1000 | Loss: 0.00001637
Iteration 125/1000 | Loss: 0.00001637
Iteration 126/1000 | Loss: 0.00001637
Iteration 127/1000 | Loss: 0.00001637
Iteration 128/1000 | Loss: 0.00001637
Iteration 129/1000 | Loss: 0.00001637
Iteration 130/1000 | Loss: 0.00001637
Iteration 131/1000 | Loss: 0.00001637
Iteration 132/1000 | Loss: 0.00001636
Iteration 133/1000 | Loss: 0.00001636
Iteration 134/1000 | Loss: 0.00001636
Iteration 135/1000 | Loss: 0.00001636
Iteration 136/1000 | Loss: 0.00001636
Iteration 137/1000 | Loss: 0.00001636
Iteration 138/1000 | Loss: 0.00001636
Iteration 139/1000 | Loss: 0.00001636
Iteration 140/1000 | Loss: 0.00001636
Iteration 141/1000 | Loss: 0.00001636
Iteration 142/1000 | Loss: 0.00001636
Iteration 143/1000 | Loss: 0.00001636
Iteration 144/1000 | Loss: 0.00001635
Iteration 145/1000 | Loss: 0.00001635
Iteration 146/1000 | Loss: 0.00001635
Iteration 147/1000 | Loss: 0.00001635
Iteration 148/1000 | Loss: 0.00001635
Iteration 149/1000 | Loss: 0.00001635
Iteration 150/1000 | Loss: 0.00001635
Iteration 151/1000 | Loss: 0.00001635
Iteration 152/1000 | Loss: 0.00001635
Iteration 153/1000 | Loss: 0.00001634
Iteration 154/1000 | Loss: 0.00001634
Iteration 155/1000 | Loss: 0.00001634
Iteration 156/1000 | Loss: 0.00001634
Iteration 157/1000 | Loss: 0.00001634
Iteration 158/1000 | Loss: 0.00001634
Iteration 159/1000 | Loss: 0.00001634
Iteration 160/1000 | Loss: 0.00001634
Iteration 161/1000 | Loss: 0.00001634
Iteration 162/1000 | Loss: 0.00001633
Iteration 163/1000 | Loss: 0.00001633
Iteration 164/1000 | Loss: 0.00001633
Iteration 165/1000 | Loss: 0.00001633
Iteration 166/1000 | Loss: 0.00001633
Iteration 167/1000 | Loss: 0.00001633
Iteration 168/1000 | Loss: 0.00001633
Iteration 169/1000 | Loss: 0.00001633
Iteration 170/1000 | Loss: 0.00001633
Iteration 171/1000 | Loss: 0.00001633
Iteration 172/1000 | Loss: 0.00001633
Iteration 173/1000 | Loss: 0.00001633
Iteration 174/1000 | Loss: 0.00001633
Iteration 175/1000 | Loss: 0.00001632
Iteration 176/1000 | Loss: 0.00001632
Iteration 177/1000 | Loss: 0.00001632
Iteration 178/1000 | Loss: 0.00001632
Iteration 179/1000 | Loss: 0.00001632
Iteration 180/1000 | Loss: 0.00001632
Iteration 181/1000 | Loss: 0.00001632
Iteration 182/1000 | Loss: 0.00001632
Iteration 183/1000 | Loss: 0.00001632
Iteration 184/1000 | Loss: 0.00001632
Iteration 185/1000 | Loss: 0.00001632
Iteration 186/1000 | Loss: 0.00001632
Iteration 187/1000 | Loss: 0.00001632
Iteration 188/1000 | Loss: 0.00001632
Iteration 189/1000 | Loss: 0.00001632
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 189. Stopping optimization.
Last 5 losses: [1.631712439120747e-05, 1.631712439120747e-05, 1.631712439120747e-05, 1.631712439120747e-05, 1.631712439120747e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.631712439120747e-05

Optimization complete. Final v2v error: 3.4033639430999756 mm

Highest mean error: 4.100829124450684 mm for frame 195

Lowest mean error: 3.0573558807373047 mm for frame 232

Saving results

Total time: 48.30176520347595
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_004/1002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_004/1002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_004/1002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00608251
Iteration 2/25 | Loss: 0.00137749
Iteration 3/25 | Loss: 0.00118366
Iteration 4/25 | Loss: 0.00116282
Iteration 5/25 | Loss: 0.00115043
Iteration 6/25 | Loss: 0.00115175
Iteration 7/25 | Loss: 0.00115032
Iteration 8/25 | Loss: 0.00114558
Iteration 9/25 | Loss: 0.00114141
Iteration 10/25 | Loss: 0.00113996
Iteration 11/25 | Loss: 0.00113924
Iteration 12/25 | Loss: 0.00113900
Iteration 13/25 | Loss: 0.00113885
Iteration 14/25 | Loss: 0.00113874
Iteration 15/25 | Loss: 0.00113874
Iteration 16/25 | Loss: 0.00113874
Iteration 17/25 | Loss: 0.00113874
Iteration 18/25 | Loss: 0.00113874
Iteration 19/25 | Loss: 0.00113874
Iteration 20/25 | Loss: 0.00113873
Iteration 21/25 | Loss: 0.00113873
Iteration 22/25 | Loss: 0.00113873
Iteration 23/25 | Loss: 0.00113873
Iteration 24/25 | Loss: 0.00113873
Iteration 25/25 | Loss: 0.00113873

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.38909411
Iteration 2/25 | Loss: 0.00081018
Iteration 3/25 | Loss: 0.00081016
Iteration 4/25 | Loss: 0.00081016
Iteration 5/25 | Loss: 0.00081016
Iteration 6/25 | Loss: 0.00081016
Iteration 7/25 | Loss: 0.00081016
Iteration 8/25 | Loss: 0.00081016
Iteration 9/25 | Loss: 0.00081016
Iteration 10/25 | Loss: 0.00081016
Iteration 11/25 | Loss: 0.00081016
Iteration 12/25 | Loss: 0.00081016
Iteration 13/25 | Loss: 0.00081016
Iteration 14/25 | Loss: 0.00081016
Iteration 15/25 | Loss: 0.00081016
Iteration 16/25 | Loss: 0.00081016
Iteration 17/25 | Loss: 0.00081016
Iteration 18/25 | Loss: 0.00081016
Iteration 19/25 | Loss: 0.00081016
Iteration 20/25 | Loss: 0.00081016
Iteration 21/25 | Loss: 0.00081016
Iteration 22/25 | Loss: 0.00081016
Iteration 23/25 | Loss: 0.00081016
Iteration 24/25 | Loss: 0.00081016
Iteration 25/25 | Loss: 0.00081016

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00081016
Iteration 2/1000 | Loss: 0.00004643
Iteration 3/1000 | Loss: 0.00003147
Iteration 4/1000 | Loss: 0.00002429
Iteration 5/1000 | Loss: 0.00002269
Iteration 6/1000 | Loss: 0.00002137
Iteration 7/1000 | Loss: 0.00002037
Iteration 8/1000 | Loss: 0.00001978
Iteration 9/1000 | Loss: 0.00001941
Iteration 10/1000 | Loss: 0.00001903
Iteration 11/1000 | Loss: 0.00001867
Iteration 12/1000 | Loss: 0.00001843
Iteration 13/1000 | Loss: 0.00001824
Iteration 14/1000 | Loss: 0.00001807
Iteration 15/1000 | Loss: 0.00001797
Iteration 16/1000 | Loss: 0.00001797
Iteration 17/1000 | Loss: 0.00001791
Iteration 18/1000 | Loss: 0.00001787
Iteration 19/1000 | Loss: 0.00001785
Iteration 20/1000 | Loss: 0.00001785
Iteration 21/1000 | Loss: 0.00001784
Iteration 22/1000 | Loss: 0.00001783
Iteration 23/1000 | Loss: 0.00001783
Iteration 24/1000 | Loss: 0.00001781
Iteration 25/1000 | Loss: 0.00001779
Iteration 26/1000 | Loss: 0.00001778
Iteration 27/1000 | Loss: 0.00001772
Iteration 28/1000 | Loss: 0.00001771
Iteration 29/1000 | Loss: 0.00001769
Iteration 30/1000 | Loss: 0.00001768
Iteration 31/1000 | Loss: 0.00001766
Iteration 32/1000 | Loss: 0.00001759
Iteration 33/1000 | Loss: 0.00001757
Iteration 34/1000 | Loss: 0.00001757
Iteration 35/1000 | Loss: 0.00002658
Iteration 36/1000 | Loss: 0.00001863
Iteration 37/1000 | Loss: 0.00001796
Iteration 38/1000 | Loss: 0.00001757
Iteration 39/1000 | Loss: 0.00001739
Iteration 40/1000 | Loss: 0.00001738
Iteration 41/1000 | Loss: 0.00001737
Iteration 42/1000 | Loss: 0.00001736
Iteration 43/1000 | Loss: 0.00001735
Iteration 44/1000 | Loss: 0.00001735
Iteration 45/1000 | Loss: 0.00001734
Iteration 46/1000 | Loss: 0.00001734
Iteration 47/1000 | Loss: 0.00001733
Iteration 48/1000 | Loss: 0.00001732
Iteration 49/1000 | Loss: 0.00001731
Iteration 50/1000 | Loss: 0.00001731
Iteration 51/1000 | Loss: 0.00001731
Iteration 52/1000 | Loss: 0.00001731
Iteration 53/1000 | Loss: 0.00001730
Iteration 54/1000 | Loss: 0.00001730
Iteration 55/1000 | Loss: 0.00001730
Iteration 56/1000 | Loss: 0.00001729
Iteration 57/1000 | Loss: 0.00001729
Iteration 58/1000 | Loss: 0.00001729
Iteration 59/1000 | Loss: 0.00001729
Iteration 60/1000 | Loss: 0.00001729
Iteration 61/1000 | Loss: 0.00001729
Iteration 62/1000 | Loss: 0.00001729
Iteration 63/1000 | Loss: 0.00001728
Iteration 64/1000 | Loss: 0.00001728
Iteration 65/1000 | Loss: 0.00001728
Iteration 66/1000 | Loss: 0.00001727
Iteration 67/1000 | Loss: 0.00001727
Iteration 68/1000 | Loss: 0.00001727
Iteration 69/1000 | Loss: 0.00001727
Iteration 70/1000 | Loss: 0.00001727
Iteration 71/1000 | Loss: 0.00001727
Iteration 72/1000 | Loss: 0.00001727
Iteration 73/1000 | Loss: 0.00001726
Iteration 74/1000 | Loss: 0.00001726
Iteration 75/1000 | Loss: 0.00001726
Iteration 76/1000 | Loss: 0.00001726
Iteration 77/1000 | Loss: 0.00001726
Iteration 78/1000 | Loss: 0.00001726
Iteration 79/1000 | Loss: 0.00001725
Iteration 80/1000 | Loss: 0.00001725
Iteration 81/1000 | Loss: 0.00001725
Iteration 82/1000 | Loss: 0.00001725
Iteration 83/1000 | Loss: 0.00001725
Iteration 84/1000 | Loss: 0.00001725
Iteration 85/1000 | Loss: 0.00001725
Iteration 86/1000 | Loss: 0.00001725
Iteration 87/1000 | Loss: 0.00001725
Iteration 88/1000 | Loss: 0.00001724
Iteration 89/1000 | Loss: 0.00001724
Iteration 90/1000 | Loss: 0.00001724
Iteration 91/1000 | Loss: 0.00001723
Iteration 92/1000 | Loss: 0.00001723
Iteration 93/1000 | Loss: 0.00001723
Iteration 94/1000 | Loss: 0.00001722
Iteration 95/1000 | Loss: 0.00001722
Iteration 96/1000 | Loss: 0.00001722
Iteration 97/1000 | Loss: 0.00001721
Iteration 98/1000 | Loss: 0.00001721
Iteration 99/1000 | Loss: 0.00001720
Iteration 100/1000 | Loss: 0.00001720
Iteration 101/1000 | Loss: 0.00001719
Iteration 102/1000 | Loss: 0.00001719
Iteration 103/1000 | Loss: 0.00001719
Iteration 104/1000 | Loss: 0.00001718
Iteration 105/1000 | Loss: 0.00001717
Iteration 106/1000 | Loss: 0.00001717
Iteration 107/1000 | Loss: 0.00001717
Iteration 108/1000 | Loss: 0.00001716
Iteration 109/1000 | Loss: 0.00001716
Iteration 110/1000 | Loss: 0.00001716
Iteration 111/1000 | Loss: 0.00001716
Iteration 112/1000 | Loss: 0.00001716
Iteration 113/1000 | Loss: 0.00001715
Iteration 114/1000 | Loss: 0.00001715
Iteration 115/1000 | Loss: 0.00001715
Iteration 116/1000 | Loss: 0.00001714
Iteration 117/1000 | Loss: 0.00001714
Iteration 118/1000 | Loss: 0.00001714
Iteration 119/1000 | Loss: 0.00001714
Iteration 120/1000 | Loss: 0.00001714
Iteration 121/1000 | Loss: 0.00001714
Iteration 122/1000 | Loss: 0.00001714
Iteration 123/1000 | Loss: 0.00001714
Iteration 124/1000 | Loss: 0.00001713
Iteration 125/1000 | Loss: 0.00001713
Iteration 126/1000 | Loss: 0.00001713
Iteration 127/1000 | Loss: 0.00001713
Iteration 128/1000 | Loss: 0.00001713
Iteration 129/1000 | Loss: 0.00001712
Iteration 130/1000 | Loss: 0.00001712
Iteration 131/1000 | Loss: 0.00001712
Iteration 132/1000 | Loss: 0.00001711
Iteration 133/1000 | Loss: 0.00001711
Iteration 134/1000 | Loss: 0.00001710
Iteration 135/1000 | Loss: 0.00001710
Iteration 136/1000 | Loss: 0.00001709
Iteration 137/1000 | Loss: 0.00001709
Iteration 138/1000 | Loss: 0.00001708
Iteration 139/1000 | Loss: 0.00001708
Iteration 140/1000 | Loss: 0.00001708
Iteration 141/1000 | Loss: 0.00001708
Iteration 142/1000 | Loss: 0.00001708
Iteration 143/1000 | Loss: 0.00001707
Iteration 144/1000 | Loss: 0.00001707
Iteration 145/1000 | Loss: 0.00001707
Iteration 146/1000 | Loss: 0.00001707
Iteration 147/1000 | Loss: 0.00001707
Iteration 148/1000 | Loss: 0.00001707
Iteration 149/1000 | Loss: 0.00001707
Iteration 150/1000 | Loss: 0.00001707
Iteration 151/1000 | Loss: 0.00001706
Iteration 152/1000 | Loss: 0.00001706
Iteration 153/1000 | Loss: 0.00001706
Iteration 154/1000 | Loss: 0.00001706
Iteration 155/1000 | Loss: 0.00001706
Iteration 156/1000 | Loss: 0.00001706
Iteration 157/1000 | Loss: 0.00001706
Iteration 158/1000 | Loss: 0.00001706
Iteration 159/1000 | Loss: 0.00001706
Iteration 160/1000 | Loss: 0.00001706
Iteration 161/1000 | Loss: 0.00001706
Iteration 162/1000 | Loss: 0.00001706
Iteration 163/1000 | Loss: 0.00001705
Iteration 164/1000 | Loss: 0.00001705
Iteration 165/1000 | Loss: 0.00001705
Iteration 166/1000 | Loss: 0.00001705
Iteration 167/1000 | Loss: 0.00001705
Iteration 168/1000 | Loss: 0.00001705
Iteration 169/1000 | Loss: 0.00001705
Iteration 170/1000 | Loss: 0.00001705
Iteration 171/1000 | Loss: 0.00001705
Iteration 172/1000 | Loss: 0.00001705
Iteration 173/1000 | Loss: 0.00001705
Iteration 174/1000 | Loss: 0.00001705
Iteration 175/1000 | Loss: 0.00001705
Iteration 176/1000 | Loss: 0.00001705
Iteration 177/1000 | Loss: 0.00001705
Iteration 178/1000 | Loss: 0.00001704
Iteration 179/1000 | Loss: 0.00001704
Iteration 180/1000 | Loss: 0.00001704
Iteration 181/1000 | Loss: 0.00001704
Iteration 182/1000 | Loss: 0.00001704
Iteration 183/1000 | Loss: 0.00001704
Iteration 184/1000 | Loss: 0.00001704
Iteration 185/1000 | Loss: 0.00001704
Iteration 186/1000 | Loss: 0.00001704
Iteration 187/1000 | Loss: 0.00001704
Iteration 188/1000 | Loss: 0.00001704
Iteration 189/1000 | Loss: 0.00001704
Iteration 190/1000 | Loss: 0.00001704
Iteration 191/1000 | Loss: 0.00001704
Iteration 192/1000 | Loss: 0.00001704
Iteration 193/1000 | Loss: 0.00001704
Iteration 194/1000 | Loss: 0.00001704
Iteration 195/1000 | Loss: 0.00001703
Iteration 196/1000 | Loss: 0.00001703
Iteration 197/1000 | Loss: 0.00001703
Iteration 198/1000 | Loss: 0.00001703
Iteration 199/1000 | Loss: 0.00001703
Iteration 200/1000 | Loss: 0.00001703
Iteration 201/1000 | Loss: 0.00001703
Iteration 202/1000 | Loss: 0.00001703
Iteration 203/1000 | Loss: 0.00001703
Iteration 204/1000 | Loss: 0.00001703
Iteration 205/1000 | Loss: 0.00001703
Iteration 206/1000 | Loss: 0.00001703
Iteration 207/1000 | Loss: 0.00001703
Iteration 208/1000 | Loss: 0.00001703
Iteration 209/1000 | Loss: 0.00001703
Iteration 210/1000 | Loss: 0.00001703
Iteration 211/1000 | Loss: 0.00001703
Iteration 212/1000 | Loss: 0.00001703
Iteration 213/1000 | Loss: 0.00001703
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 213. Stopping optimization.
Last 5 losses: [1.7027859939844348e-05, 1.7027859939844348e-05, 1.7027859939844348e-05, 1.7027859939844348e-05, 1.7027859939844348e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7027859939844348e-05

Optimization complete. Final v2v error: 3.437136650085449 mm

Highest mean error: 5.875197410583496 mm for frame 166

Lowest mean error: 2.6574618816375732 mm for frame 192

Saving results

Total time: 77.29452443122864
