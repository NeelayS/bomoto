Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=210, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 11760-11815
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_022/1045/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_022/1045.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_022/1045
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01082426
Iteration 2/25 | Loss: 0.01082426
Iteration 3/25 | Loss: 0.00274296
Iteration 4/25 | Loss: 0.00214007
Iteration 5/25 | Loss: 0.00211963
Iteration 6/25 | Loss: 0.00267320
Iteration 7/25 | Loss: 0.00189542
Iteration 8/25 | Loss: 0.00158610
Iteration 9/25 | Loss: 0.00149801
Iteration 10/25 | Loss: 0.00143556
Iteration 11/25 | Loss: 0.00142169
Iteration 12/25 | Loss: 0.00143442
Iteration 13/25 | Loss: 0.00141412
Iteration 14/25 | Loss: 0.00138536
Iteration 15/25 | Loss: 0.00137649
Iteration 16/25 | Loss: 0.00136243
Iteration 17/25 | Loss: 0.00135666
Iteration 18/25 | Loss: 0.00136091
Iteration 19/25 | Loss: 0.00135416
Iteration 20/25 | Loss: 0.00134786
Iteration 21/25 | Loss: 0.00134514
Iteration 22/25 | Loss: 0.00134548
Iteration 23/25 | Loss: 0.00135920
Iteration 24/25 | Loss: 0.00136317
Iteration 25/25 | Loss: 0.00136421

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.24107575
Iteration 2/25 | Loss: 0.00245816
Iteration 3/25 | Loss: 0.00245814
Iteration 4/25 | Loss: 0.00245814
Iteration 5/25 | Loss: 0.00245814
Iteration 6/25 | Loss: 0.00245814
Iteration 7/25 | Loss: 0.00245813
Iteration 8/25 | Loss: 0.00245813
Iteration 9/25 | Loss: 0.00245813
Iteration 10/25 | Loss: 0.00245813
Iteration 11/25 | Loss: 0.00245813
Iteration 12/25 | Loss: 0.00245813
Iteration 13/25 | Loss: 0.00245813
Iteration 14/25 | Loss: 0.00245813
Iteration 15/25 | Loss: 0.00245813
Iteration 16/25 | Loss: 0.00245813
Iteration 17/25 | Loss: 0.00245813
Iteration 18/25 | Loss: 0.00245813
Iteration 19/25 | Loss: 0.00245813
Iteration 20/25 | Loss: 0.00245813
Iteration 21/25 | Loss: 0.00245813
Iteration 22/25 | Loss: 0.00245813
Iteration 23/25 | Loss: 0.00245813
Iteration 24/25 | Loss: 0.00245813
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0024581323377788067, 0.0024581323377788067, 0.0024581323377788067, 0.0024581323377788067, 0.0024581323377788067]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0024581323377788067

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00245813
Iteration 2/1000 | Loss: 0.00171120
Iteration 3/1000 | Loss: 0.00135601
Iteration 4/1000 | Loss: 0.00158694
Iteration 5/1000 | Loss: 0.00183966
Iteration 6/1000 | Loss: 0.00136156
Iteration 7/1000 | Loss: 0.00103496
Iteration 8/1000 | Loss: 0.00092085
Iteration 9/1000 | Loss: 0.00119220
Iteration 10/1000 | Loss: 0.00111805
Iteration 11/1000 | Loss: 0.00143329
Iteration 12/1000 | Loss: 0.00154975
Iteration 13/1000 | Loss: 0.00120533
Iteration 14/1000 | Loss: 0.00111911
Iteration 15/1000 | Loss: 0.00119226
Iteration 16/1000 | Loss: 0.00080174
Iteration 17/1000 | Loss: 0.00078921
Iteration 18/1000 | Loss: 0.00082707
Iteration 19/1000 | Loss: 0.00089631
Iteration 20/1000 | Loss: 0.00129224
Iteration 21/1000 | Loss: 0.00108587
Iteration 22/1000 | Loss: 0.00078250
Iteration 23/1000 | Loss: 0.00053528
Iteration 24/1000 | Loss: 0.00099053
Iteration 25/1000 | Loss: 0.00117312
Iteration 26/1000 | Loss: 0.00111076
Iteration 27/1000 | Loss: 0.00150374
Iteration 28/1000 | Loss: 0.00151808
Iteration 29/1000 | Loss: 0.00138335
Iteration 30/1000 | Loss: 0.00182320
Iteration 31/1000 | Loss: 0.00152426
Iteration 32/1000 | Loss: 0.00119883
Iteration 33/1000 | Loss: 0.00101114
Iteration 34/1000 | Loss: 0.00099900
Iteration 35/1000 | Loss: 0.00097467
Iteration 36/1000 | Loss: 0.00058676
Iteration 37/1000 | Loss: 0.00114404
Iteration 38/1000 | Loss: 0.00122807
Iteration 39/1000 | Loss: 0.00104337
Iteration 40/1000 | Loss: 0.00082821
Iteration 41/1000 | Loss: 0.00066436
Iteration 42/1000 | Loss: 0.00157482
Iteration 43/1000 | Loss: 0.00092537
Iteration 44/1000 | Loss: 0.00059703
Iteration 45/1000 | Loss: 0.00061159
Iteration 46/1000 | Loss: 0.00068874
Iteration 47/1000 | Loss: 0.00094046
Iteration 48/1000 | Loss: 0.00096598
Iteration 49/1000 | Loss: 0.00052108
Iteration 50/1000 | Loss: 0.00137559
Iteration 51/1000 | Loss: 0.00094670
Iteration 52/1000 | Loss: 0.00101491
Iteration 53/1000 | Loss: 0.00044387
Iteration 54/1000 | Loss: 0.00086512
Iteration 55/1000 | Loss: 0.00140584
Iteration 56/1000 | Loss: 0.00101627
Iteration 57/1000 | Loss: 0.00060658
Iteration 58/1000 | Loss: 0.00040505
Iteration 59/1000 | Loss: 0.00134491
Iteration 60/1000 | Loss: 0.00096945
Iteration 61/1000 | Loss: 0.00081964
Iteration 62/1000 | Loss: 0.00062184
Iteration 63/1000 | Loss: 0.00046990
Iteration 64/1000 | Loss: 0.00030196
Iteration 65/1000 | Loss: 0.00023280
Iteration 66/1000 | Loss: 0.00022136
Iteration 67/1000 | Loss: 0.00017934
Iteration 68/1000 | Loss: 0.00084089
Iteration 69/1000 | Loss: 0.00018444
Iteration 70/1000 | Loss: 0.00012543
Iteration 71/1000 | Loss: 0.00044170
Iteration 72/1000 | Loss: 0.00020047
Iteration 73/1000 | Loss: 0.00031879
Iteration 74/1000 | Loss: 0.00026489
Iteration 75/1000 | Loss: 0.00032564
Iteration 76/1000 | Loss: 0.00030276
Iteration 77/1000 | Loss: 0.00016503
Iteration 78/1000 | Loss: 0.00021825
Iteration 79/1000 | Loss: 0.00012626
Iteration 80/1000 | Loss: 0.00050802
Iteration 81/1000 | Loss: 0.00104501
Iteration 82/1000 | Loss: 0.00067144
Iteration 83/1000 | Loss: 0.00050623
Iteration 84/1000 | Loss: 0.00024304
Iteration 85/1000 | Loss: 0.00015566
Iteration 86/1000 | Loss: 0.00019903
Iteration 87/1000 | Loss: 0.00016017
Iteration 88/1000 | Loss: 0.00022236
Iteration 89/1000 | Loss: 0.00030407
Iteration 90/1000 | Loss: 0.00024180
Iteration 91/1000 | Loss: 0.00016705
Iteration 92/1000 | Loss: 0.00029720
Iteration 93/1000 | Loss: 0.00046338
Iteration 94/1000 | Loss: 0.00031365
Iteration 95/1000 | Loss: 0.00017034
Iteration 96/1000 | Loss: 0.00068018
Iteration 97/1000 | Loss: 0.00054378
Iteration 98/1000 | Loss: 0.00039669
Iteration 99/1000 | Loss: 0.00040761
Iteration 100/1000 | Loss: 0.00009095
Iteration 101/1000 | Loss: 0.00018776
Iteration 102/1000 | Loss: 0.00041102
Iteration 103/1000 | Loss: 0.00016516
Iteration 104/1000 | Loss: 0.00062652
Iteration 105/1000 | Loss: 0.00022684
Iteration 106/1000 | Loss: 0.00018892
Iteration 107/1000 | Loss: 0.00009825
Iteration 108/1000 | Loss: 0.00028008
Iteration 109/1000 | Loss: 0.00013563
Iteration 110/1000 | Loss: 0.00005825
Iteration 111/1000 | Loss: 0.00012769
Iteration 112/1000 | Loss: 0.00054218
Iteration 113/1000 | Loss: 0.00075343
Iteration 114/1000 | Loss: 0.00039841
Iteration 115/1000 | Loss: 0.00035700
Iteration 116/1000 | Loss: 0.00027183
Iteration 117/1000 | Loss: 0.00027830
Iteration 118/1000 | Loss: 0.00023734
Iteration 119/1000 | Loss: 0.00015879
Iteration 120/1000 | Loss: 0.00010735
Iteration 121/1000 | Loss: 0.00035328
Iteration 122/1000 | Loss: 0.00025330
Iteration 123/1000 | Loss: 0.00018399
Iteration 124/1000 | Loss: 0.00017509
Iteration 125/1000 | Loss: 0.00044082
Iteration 126/1000 | Loss: 0.00037032
Iteration 127/1000 | Loss: 0.00027628
Iteration 128/1000 | Loss: 0.00020624
Iteration 129/1000 | Loss: 0.00030473
Iteration 130/1000 | Loss: 0.00027712
Iteration 131/1000 | Loss: 0.00020384
Iteration 132/1000 | Loss: 0.00015160
Iteration 133/1000 | Loss: 0.00026442
Iteration 134/1000 | Loss: 0.00010093
Iteration 135/1000 | Loss: 0.00010012
Iteration 136/1000 | Loss: 0.00008628
Iteration 137/1000 | Loss: 0.00024304
Iteration 138/1000 | Loss: 0.00019817
Iteration 139/1000 | Loss: 0.00033323
Iteration 140/1000 | Loss: 0.00019773
Iteration 141/1000 | Loss: 0.00017385
Iteration 142/1000 | Loss: 0.00015817
Iteration 143/1000 | Loss: 0.00016785
Iteration 144/1000 | Loss: 0.00013900
Iteration 145/1000 | Loss: 0.00013563
Iteration 146/1000 | Loss: 0.00015840
Iteration 147/1000 | Loss: 0.00022430
Iteration 148/1000 | Loss: 0.00019229
Iteration 149/1000 | Loss: 0.00028093
Iteration 150/1000 | Loss: 0.00030824
Iteration 151/1000 | Loss: 0.00039139
Iteration 152/1000 | Loss: 0.00024862
Iteration 153/1000 | Loss: 0.00018863
Iteration 154/1000 | Loss: 0.00013493
Iteration 155/1000 | Loss: 0.00020073
Iteration 156/1000 | Loss: 0.00025719
Iteration 157/1000 | Loss: 0.00025961
Iteration 158/1000 | Loss: 0.00026690
Iteration 159/1000 | Loss: 0.00010758
Iteration 160/1000 | Loss: 0.00012283
Iteration 161/1000 | Loss: 0.00013822
Iteration 162/1000 | Loss: 0.00014517
Iteration 163/1000 | Loss: 0.00020026
Iteration 164/1000 | Loss: 0.00034608
Iteration 165/1000 | Loss: 0.00009502
Iteration 166/1000 | Loss: 0.00012260
Iteration 167/1000 | Loss: 0.00004852
Iteration 168/1000 | Loss: 0.00037160
Iteration 169/1000 | Loss: 0.00040803
Iteration 170/1000 | Loss: 0.00042787
Iteration 171/1000 | Loss: 0.00025249
Iteration 172/1000 | Loss: 0.00012136
Iteration 173/1000 | Loss: 0.00008808
Iteration 174/1000 | Loss: 0.00009359
Iteration 175/1000 | Loss: 0.00027414
Iteration 176/1000 | Loss: 0.00027671
Iteration 177/1000 | Loss: 0.00035783
Iteration 178/1000 | Loss: 0.00024824
Iteration 179/1000 | Loss: 0.00017647
Iteration 180/1000 | Loss: 0.00034449
Iteration 181/1000 | Loss: 0.00024235
Iteration 182/1000 | Loss: 0.00039398
Iteration 183/1000 | Loss: 0.00018162
Iteration 184/1000 | Loss: 0.00052622
Iteration 185/1000 | Loss: 0.00030122
Iteration 186/1000 | Loss: 0.00014017
Iteration 187/1000 | Loss: 0.00033811
Iteration 188/1000 | Loss: 0.00056370
Iteration 189/1000 | Loss: 0.00012024
Iteration 190/1000 | Loss: 0.00030761
Iteration 191/1000 | Loss: 0.00017557
Iteration 192/1000 | Loss: 0.00024083
Iteration 193/1000 | Loss: 0.00018893
Iteration 194/1000 | Loss: 0.00016065
Iteration 195/1000 | Loss: 0.00035236
Iteration 196/1000 | Loss: 0.00034371
Iteration 197/1000 | Loss: 0.00017258
Iteration 198/1000 | Loss: 0.00013278
Iteration 199/1000 | Loss: 0.00015370
Iteration 200/1000 | Loss: 0.00008749
Iteration 201/1000 | Loss: 0.00022197
Iteration 202/1000 | Loss: 0.00018210
Iteration 203/1000 | Loss: 0.00031212
Iteration 204/1000 | Loss: 0.00022713
Iteration 205/1000 | Loss: 0.00014269
Iteration 206/1000 | Loss: 0.00013158
Iteration 207/1000 | Loss: 0.00011723
Iteration 208/1000 | Loss: 0.00015486
Iteration 209/1000 | Loss: 0.00017777
Iteration 210/1000 | Loss: 0.00014704
Iteration 211/1000 | Loss: 0.00016672
Iteration 212/1000 | Loss: 0.00026857
Iteration 213/1000 | Loss: 0.00041714
Iteration 214/1000 | Loss: 0.00036686
Iteration 215/1000 | Loss: 0.00026807
Iteration 216/1000 | Loss: 0.00057355
Iteration 217/1000 | Loss: 0.00066036
Iteration 218/1000 | Loss: 0.00092579
Iteration 219/1000 | Loss: 0.00056995
Iteration 220/1000 | Loss: 0.00016939
Iteration 221/1000 | Loss: 0.00008333
Iteration 222/1000 | Loss: 0.00007714
Iteration 223/1000 | Loss: 0.00017049
Iteration 224/1000 | Loss: 0.00021979
Iteration 225/1000 | Loss: 0.00006150
Iteration 226/1000 | Loss: 0.00006037
Iteration 227/1000 | Loss: 0.00004902
Iteration 228/1000 | Loss: 0.00004424
Iteration 229/1000 | Loss: 0.00003934
Iteration 230/1000 | Loss: 0.00003732
Iteration 231/1000 | Loss: 0.00011419
Iteration 232/1000 | Loss: 0.00035073
Iteration 233/1000 | Loss: 0.00013739
Iteration 234/1000 | Loss: 0.00004168
Iteration 235/1000 | Loss: 0.00006173
Iteration 236/1000 | Loss: 0.00008594
Iteration 237/1000 | Loss: 0.00005695
Iteration 238/1000 | Loss: 0.00013458
Iteration 239/1000 | Loss: 0.00010868
Iteration 240/1000 | Loss: 0.00018584
Iteration 241/1000 | Loss: 0.00004676
Iteration 242/1000 | Loss: 0.00004039
Iteration 243/1000 | Loss: 0.00017680
Iteration 244/1000 | Loss: 0.00056666
Iteration 245/1000 | Loss: 0.00025229
Iteration 246/1000 | Loss: 0.00070995
Iteration 247/1000 | Loss: 0.00042944
Iteration 248/1000 | Loss: 0.00060730
Iteration 249/1000 | Loss: 0.00042358
Iteration 250/1000 | Loss: 0.00059352
Iteration 251/1000 | Loss: 0.00035101
Iteration 252/1000 | Loss: 0.00048212
Iteration 253/1000 | Loss: 0.00016111
Iteration 254/1000 | Loss: 0.00013528
Iteration 255/1000 | Loss: 0.00058206
Iteration 256/1000 | Loss: 0.00024228
Iteration 257/1000 | Loss: 0.00051504
Iteration 258/1000 | Loss: 0.00019655
Iteration 259/1000 | Loss: 0.00017058
Iteration 260/1000 | Loss: 0.00004680
Iteration 261/1000 | Loss: 0.00003514
Iteration 262/1000 | Loss: 0.00016020
Iteration 263/1000 | Loss: 0.00003665
Iteration 264/1000 | Loss: 0.00005134
Iteration 265/1000 | Loss: 0.00004347
Iteration 266/1000 | Loss: 0.00004185
Iteration 267/1000 | Loss: 0.00003263
Iteration 268/1000 | Loss: 0.00003492
Iteration 269/1000 | Loss: 0.00004415
Iteration 270/1000 | Loss: 0.00005011
Iteration 271/1000 | Loss: 0.00004573
Iteration 272/1000 | Loss: 0.00004861
Iteration 273/1000 | Loss: 0.00005515
Iteration 274/1000 | Loss: 0.00005240
Iteration 275/1000 | Loss: 0.00004981
Iteration 276/1000 | Loss: 0.00004804
Iteration 277/1000 | Loss: 0.00002788
Iteration 278/1000 | Loss: 0.00002744
Iteration 279/1000 | Loss: 0.00003845
Iteration 280/1000 | Loss: 0.00006622
Iteration 281/1000 | Loss: 0.00004676
Iteration 282/1000 | Loss: 0.00006279
Iteration 283/1000 | Loss: 0.00004637
Iteration 284/1000 | Loss: 0.00005693
Iteration 285/1000 | Loss: 0.00005199
Iteration 286/1000 | Loss: 0.00005578
Iteration 287/1000 | Loss: 0.00004690
Iteration 288/1000 | Loss: 0.00004666
Iteration 289/1000 | Loss: 0.00005380
Iteration 290/1000 | Loss: 0.00005276
Iteration 291/1000 | Loss: 0.00005431
Iteration 292/1000 | Loss: 0.00004139
Iteration 293/1000 | Loss: 0.00005774
Iteration 294/1000 | Loss: 0.00012506
Iteration 295/1000 | Loss: 0.00018945
Iteration 296/1000 | Loss: 0.00003887
Iteration 297/1000 | Loss: 0.00004893
Iteration 298/1000 | Loss: 0.00003521
Iteration 299/1000 | Loss: 0.00002883
Iteration 300/1000 | Loss: 0.00002648
Iteration 301/1000 | Loss: 0.00004074
Iteration 302/1000 | Loss: 0.00003976
Iteration 303/1000 | Loss: 0.00004703
Iteration 304/1000 | Loss: 0.00003333
Iteration 305/1000 | Loss: 0.00003922
Iteration 306/1000 | Loss: 0.00003898
Iteration 307/1000 | Loss: 0.00004033
Iteration 308/1000 | Loss: 0.00004245
Iteration 309/1000 | Loss: 0.00004006
Iteration 310/1000 | Loss: 0.00004950
Iteration 311/1000 | Loss: 0.00003206
Iteration 312/1000 | Loss: 0.00002820
Iteration 313/1000 | Loss: 0.00002721
Iteration 314/1000 | Loss: 0.00004214
Iteration 315/1000 | Loss: 0.00004068
Iteration 316/1000 | Loss: 0.00004232
Iteration 317/1000 | Loss: 0.00004240
Iteration 318/1000 | Loss: 0.00003528
Iteration 319/1000 | Loss: 0.00004207
Iteration 320/1000 | Loss: 0.00004071
Iteration 321/1000 | Loss: 0.00004176
Iteration 322/1000 | Loss: 0.00003620
Iteration 323/1000 | Loss: 0.00003622
Iteration 324/1000 | Loss: 0.00003631
Iteration 325/1000 | Loss: 0.00003484
Iteration 326/1000 | Loss: 0.00004727
Iteration 327/1000 | Loss: 0.00003038
Iteration 328/1000 | Loss: 0.00002772
Iteration 329/1000 | Loss: 0.00002638
Iteration 330/1000 | Loss: 0.00002559
Iteration 331/1000 | Loss: 0.00002528
Iteration 332/1000 | Loss: 0.00002504
Iteration 333/1000 | Loss: 0.00002499
Iteration 334/1000 | Loss: 0.00002486
Iteration 335/1000 | Loss: 0.00002484
Iteration 336/1000 | Loss: 0.00002483
Iteration 337/1000 | Loss: 0.00002483
Iteration 338/1000 | Loss: 0.00002483
Iteration 339/1000 | Loss: 0.00002482
Iteration 340/1000 | Loss: 0.00002482
Iteration 341/1000 | Loss: 0.00002482
Iteration 342/1000 | Loss: 0.00002482
Iteration 343/1000 | Loss: 0.00002482
Iteration 344/1000 | Loss: 0.00002482
Iteration 345/1000 | Loss: 0.00002482
Iteration 346/1000 | Loss: 0.00002482
Iteration 347/1000 | Loss: 0.00002482
Iteration 348/1000 | Loss: 0.00002481
Iteration 349/1000 | Loss: 0.00002481
Iteration 350/1000 | Loss: 0.00002481
Iteration 351/1000 | Loss: 0.00002481
Iteration 352/1000 | Loss: 0.00002478
Iteration 353/1000 | Loss: 0.00002478
Iteration 354/1000 | Loss: 0.00002478
Iteration 355/1000 | Loss: 0.00002478
Iteration 356/1000 | Loss: 0.00002478
Iteration 357/1000 | Loss: 0.00002478
Iteration 358/1000 | Loss: 0.00002478
Iteration 359/1000 | Loss: 0.00002477
Iteration 360/1000 | Loss: 0.00002477
Iteration 361/1000 | Loss: 0.00002477
Iteration 362/1000 | Loss: 0.00002476
Iteration 363/1000 | Loss: 0.00002476
Iteration 364/1000 | Loss: 0.00002476
Iteration 365/1000 | Loss: 0.00002476
Iteration 366/1000 | Loss: 0.00002476
Iteration 367/1000 | Loss: 0.00002475
Iteration 368/1000 | Loss: 0.00002475
Iteration 369/1000 | Loss: 0.00002475
Iteration 370/1000 | Loss: 0.00002475
Iteration 371/1000 | Loss: 0.00002475
Iteration 372/1000 | Loss: 0.00002475
Iteration 373/1000 | Loss: 0.00002475
Iteration 374/1000 | Loss: 0.00002475
Iteration 375/1000 | Loss: 0.00002474
Iteration 376/1000 | Loss: 0.00002474
Iteration 377/1000 | Loss: 0.00002474
Iteration 378/1000 | Loss: 0.00002474
Iteration 379/1000 | Loss: 0.00002474
Iteration 380/1000 | Loss: 0.00002474
Iteration 381/1000 | Loss: 0.00002474
Iteration 382/1000 | Loss: 0.00002474
Iteration 383/1000 | Loss: 0.00002474
Iteration 384/1000 | Loss: 0.00002473
Iteration 385/1000 | Loss: 0.00002473
Iteration 386/1000 | Loss: 0.00002473
Iteration 387/1000 | Loss: 0.00002473
Iteration 388/1000 | Loss: 0.00002473
Iteration 389/1000 | Loss: 0.00002473
Iteration 390/1000 | Loss: 0.00002473
Iteration 391/1000 | Loss: 0.00002473
Iteration 392/1000 | Loss: 0.00002473
Iteration 393/1000 | Loss: 0.00002473
Iteration 394/1000 | Loss: 0.00002473
Iteration 395/1000 | Loss: 0.00002472
Iteration 396/1000 | Loss: 0.00002472
Iteration 397/1000 | Loss: 0.00002472
Iteration 398/1000 | Loss: 0.00002472
Iteration 399/1000 | Loss: 0.00002472
Iteration 400/1000 | Loss: 0.00002471
Iteration 401/1000 | Loss: 0.00002471
Iteration 402/1000 | Loss: 0.00002471
Iteration 403/1000 | Loss: 0.00002471
Iteration 404/1000 | Loss: 0.00002471
Iteration 405/1000 | Loss: 0.00002471
Iteration 406/1000 | Loss: 0.00002471
Iteration 407/1000 | Loss: 0.00002471
Iteration 408/1000 | Loss: 0.00002471
Iteration 409/1000 | Loss: 0.00002471
Iteration 410/1000 | Loss: 0.00002471
Iteration 411/1000 | Loss: 0.00002470
Iteration 412/1000 | Loss: 0.00002470
Iteration 413/1000 | Loss: 0.00002469
Iteration 414/1000 | Loss: 0.00002469
Iteration 415/1000 | Loss: 0.00002469
Iteration 416/1000 | Loss: 0.00002469
Iteration 417/1000 | Loss: 0.00002469
Iteration 418/1000 | Loss: 0.00002468
Iteration 419/1000 | Loss: 0.00002468
Iteration 420/1000 | Loss: 0.00002468
Iteration 421/1000 | Loss: 0.00002468
Iteration 422/1000 | Loss: 0.00002468
Iteration 423/1000 | Loss: 0.00002468
Iteration 424/1000 | Loss: 0.00002467
Iteration 425/1000 | Loss: 0.00002467
Iteration 426/1000 | Loss: 0.00002467
Iteration 427/1000 | Loss: 0.00002467
Iteration 428/1000 | Loss: 0.00002466
Iteration 429/1000 | Loss: 0.00002466
Iteration 430/1000 | Loss: 0.00002466
Iteration 431/1000 | Loss: 0.00002466
Iteration 432/1000 | Loss: 0.00002466
Iteration 433/1000 | Loss: 0.00002466
Iteration 434/1000 | Loss: 0.00002465
Iteration 435/1000 | Loss: 0.00002465
Iteration 436/1000 | Loss: 0.00002465
Iteration 437/1000 | Loss: 0.00002465
Iteration 438/1000 | Loss: 0.00002465
Iteration 439/1000 | Loss: 0.00002465
Iteration 440/1000 | Loss: 0.00002465
Iteration 441/1000 | Loss: 0.00002464
Iteration 442/1000 | Loss: 0.00002464
Iteration 443/1000 | Loss: 0.00002464
Iteration 444/1000 | Loss: 0.00002464
Iteration 445/1000 | Loss: 0.00002464
Iteration 446/1000 | Loss: 0.00002464
Iteration 447/1000 | Loss: 0.00002463
Iteration 448/1000 | Loss: 0.00002463
Iteration 449/1000 | Loss: 0.00002463
Iteration 450/1000 | Loss: 0.00002463
Iteration 451/1000 | Loss: 0.00002463
Iteration 452/1000 | Loss: 0.00002463
Iteration 453/1000 | Loss: 0.00002462
Iteration 454/1000 | Loss: 0.00002462
Iteration 455/1000 | Loss: 0.00002462
Iteration 456/1000 | Loss: 0.00002462
Iteration 457/1000 | Loss: 0.00002462
Iteration 458/1000 | Loss: 0.00002462
Iteration 459/1000 | Loss: 0.00002462
Iteration 460/1000 | Loss: 0.00002462
Iteration 461/1000 | Loss: 0.00002462
Iteration 462/1000 | Loss: 0.00002462
Iteration 463/1000 | Loss: 0.00002462
Iteration 464/1000 | Loss: 0.00002462
Iteration 465/1000 | Loss: 0.00002462
Iteration 466/1000 | Loss: 0.00002462
Iteration 467/1000 | Loss: 0.00002462
Iteration 468/1000 | Loss: 0.00002462
Iteration 469/1000 | Loss: 0.00002462
Iteration 470/1000 | Loss: 0.00002462
Iteration 471/1000 | Loss: 0.00002461
Iteration 472/1000 | Loss: 0.00002461
Iteration 473/1000 | Loss: 0.00002461
Iteration 474/1000 | Loss: 0.00002461
Iteration 475/1000 | Loss: 0.00002461
Iteration 476/1000 | Loss: 0.00002461
Iteration 477/1000 | Loss: 0.00002461
Iteration 478/1000 | Loss: 0.00002461
Iteration 479/1000 | Loss: 0.00002461
Iteration 480/1000 | Loss: 0.00002461
Iteration 481/1000 | Loss: 0.00002461
Iteration 482/1000 | Loss: 0.00002461
Iteration 483/1000 | Loss: 0.00002460
Iteration 484/1000 | Loss: 0.00002460
Iteration 485/1000 | Loss: 0.00002460
Iteration 486/1000 | Loss: 0.00002460
Iteration 487/1000 | Loss: 0.00002460
Iteration 488/1000 | Loss: 0.00002460
Iteration 489/1000 | Loss: 0.00002460
Iteration 490/1000 | Loss: 0.00002460
Iteration 491/1000 | Loss: 0.00002460
Iteration 492/1000 | Loss: 0.00002460
Iteration 493/1000 | Loss: 0.00002460
Iteration 494/1000 | Loss: 0.00002460
Iteration 495/1000 | Loss: 0.00002460
Iteration 496/1000 | Loss: 0.00002460
Iteration 497/1000 | Loss: 0.00002460
Iteration 498/1000 | Loss: 0.00002460
Iteration 499/1000 | Loss: 0.00002460
Iteration 500/1000 | Loss: 0.00002460
Iteration 501/1000 | Loss: 0.00002460
Iteration 502/1000 | Loss: 0.00002460
Iteration 503/1000 | Loss: 0.00002460
Iteration 504/1000 | Loss: 0.00002460
Iteration 505/1000 | Loss: 0.00002460
Iteration 506/1000 | Loss: 0.00002460
Iteration 507/1000 | Loss: 0.00002460
Iteration 508/1000 | Loss: 0.00002460
Iteration 509/1000 | Loss: 0.00002460
Iteration 510/1000 | Loss: 0.00002460
Iteration 511/1000 | Loss: 0.00002460
Iteration 512/1000 | Loss: 0.00002460
Iteration 513/1000 | Loss: 0.00002460
Iteration 514/1000 | Loss: 0.00002460
Iteration 515/1000 | Loss: 0.00002460
Iteration 516/1000 | Loss: 0.00002460
Iteration 517/1000 | Loss: 0.00002460
Iteration 518/1000 | Loss: 0.00002460
Iteration 519/1000 | Loss: 0.00002460
Iteration 520/1000 | Loss: 0.00002460
Iteration 521/1000 | Loss: 0.00002460
Iteration 522/1000 | Loss: 0.00002460
Iteration 523/1000 | Loss: 0.00002460
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 523. Stopping optimization.
Last 5 losses: [2.460327959852293e-05, 2.460327959852293e-05, 2.460327959852293e-05, 2.460327959852293e-05, 2.460327959852293e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.460327959852293e-05

Optimization complete. Final v2v error: 3.9751203060150146 mm

Highest mean error: 4.903176784515381 mm for frame 189

Lowest mean error: 3.648757219314575 mm for frame 0

Saving results

Total time: 609.1293880939484
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_022/1070/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_022/1070.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_022/1070
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00873377
Iteration 2/25 | Loss: 0.00224520
Iteration 3/25 | Loss: 0.00180986
Iteration 4/25 | Loss: 0.00140544
Iteration 5/25 | Loss: 0.00140384
Iteration 6/25 | Loss: 0.00135615
Iteration 7/25 | Loss: 0.00128584
Iteration 8/25 | Loss: 0.00123667
Iteration 9/25 | Loss: 0.00122142
Iteration 10/25 | Loss: 0.00119058
Iteration 11/25 | Loss: 0.00117978
Iteration 12/25 | Loss: 0.00116183
Iteration 13/25 | Loss: 0.00116080
Iteration 14/25 | Loss: 0.00115492
Iteration 15/25 | Loss: 0.00115253
Iteration 16/25 | Loss: 0.00115554
Iteration 17/25 | Loss: 0.00115585
Iteration 18/25 | Loss: 0.00115277
Iteration 19/25 | Loss: 0.00115078
Iteration 20/25 | Loss: 0.00114801
Iteration 21/25 | Loss: 0.00114774
Iteration 22/25 | Loss: 0.00114762
Iteration 23/25 | Loss: 0.00114761
Iteration 24/25 | Loss: 0.00114761
Iteration 25/25 | Loss: 0.00114761

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44927001
Iteration 2/25 | Loss: 0.00147393
Iteration 3/25 | Loss: 0.00147393
Iteration 4/25 | Loss: 0.00147393
Iteration 5/25 | Loss: 0.00147393
Iteration 6/25 | Loss: 0.00147393
Iteration 7/25 | Loss: 0.00147393
Iteration 8/25 | Loss: 0.00147393
Iteration 9/25 | Loss: 0.00147392
Iteration 10/25 | Loss: 0.00147392
Iteration 11/25 | Loss: 0.00147392
Iteration 12/25 | Loss: 0.00147392
Iteration 13/25 | Loss: 0.00147392
Iteration 14/25 | Loss: 0.00147392
Iteration 15/25 | Loss: 0.00147392
Iteration 16/25 | Loss: 0.00147392
Iteration 17/25 | Loss: 0.00147392
Iteration 18/25 | Loss: 0.00147392
Iteration 19/25 | Loss: 0.00147392
Iteration 20/25 | Loss: 0.00147392
Iteration 21/25 | Loss: 0.00147392
Iteration 22/25 | Loss: 0.00147392
Iteration 23/25 | Loss: 0.00147392
Iteration 24/25 | Loss: 0.00147392
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0014739230973646045, 0.0014739230973646045, 0.0014739230973646045, 0.0014739230973646045, 0.0014739230973646045]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014739230973646045

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00147392
Iteration 2/1000 | Loss: 0.00010821
Iteration 3/1000 | Loss: 0.00007647
Iteration 4/1000 | Loss: 0.00006228
Iteration 5/1000 | Loss: 0.00005444
Iteration 6/1000 | Loss: 0.00005038
Iteration 7/1000 | Loss: 0.00004766
Iteration 8/1000 | Loss: 0.00004531
Iteration 9/1000 | Loss: 0.00004397
Iteration 10/1000 | Loss: 0.00004301
Iteration 11/1000 | Loss: 0.00004224
Iteration 12/1000 | Loss: 0.00004174
Iteration 13/1000 | Loss: 0.00004141
Iteration 14/1000 | Loss: 0.00004119
Iteration 15/1000 | Loss: 0.00004095
Iteration 16/1000 | Loss: 0.00004071
Iteration 17/1000 | Loss: 0.00004049
Iteration 18/1000 | Loss: 0.00004024
Iteration 19/1000 | Loss: 0.00003996
Iteration 20/1000 | Loss: 0.00003964
Iteration 21/1000 | Loss: 0.00003923
Iteration 22/1000 | Loss: 0.00014619
Iteration 23/1000 | Loss: 0.00004120
Iteration 24/1000 | Loss: 0.00003827
Iteration 25/1000 | Loss: 0.00003716
Iteration 26/1000 | Loss: 0.00003604
Iteration 27/1000 | Loss: 0.00003507
Iteration 28/1000 | Loss: 0.00003460
Iteration 29/1000 | Loss: 0.00003418
Iteration 30/1000 | Loss: 0.00003381
Iteration 31/1000 | Loss: 0.00003346
Iteration 32/1000 | Loss: 0.00003321
Iteration 33/1000 | Loss: 0.00003304
Iteration 34/1000 | Loss: 0.00003293
Iteration 35/1000 | Loss: 0.00003293
Iteration 36/1000 | Loss: 0.00003292
Iteration 37/1000 | Loss: 0.00003292
Iteration 38/1000 | Loss: 0.00003291
Iteration 39/1000 | Loss: 0.00003291
Iteration 40/1000 | Loss: 0.00003284
Iteration 41/1000 | Loss: 0.00003281
Iteration 42/1000 | Loss: 0.00003280
Iteration 43/1000 | Loss: 0.00003277
Iteration 44/1000 | Loss: 0.00003273
Iteration 45/1000 | Loss: 0.00003266
Iteration 46/1000 | Loss: 0.00003263
Iteration 47/1000 | Loss: 0.00003261
Iteration 48/1000 | Loss: 0.00003259
Iteration 49/1000 | Loss: 0.00003258
Iteration 50/1000 | Loss: 0.00003258
Iteration 51/1000 | Loss: 0.00003257
Iteration 52/1000 | Loss: 0.00003254
Iteration 53/1000 | Loss: 0.00003254
Iteration 54/1000 | Loss: 0.00003253
Iteration 55/1000 | Loss: 0.00003253
Iteration 56/1000 | Loss: 0.00003252
Iteration 57/1000 | Loss: 0.00003247
Iteration 58/1000 | Loss: 0.00003247
Iteration 59/1000 | Loss: 0.00003245
Iteration 60/1000 | Loss: 0.00003245
Iteration 61/1000 | Loss: 0.00003244
Iteration 62/1000 | Loss: 0.00003243
Iteration 63/1000 | Loss: 0.00003243
Iteration 64/1000 | Loss: 0.00003243
Iteration 65/1000 | Loss: 0.00003242
Iteration 66/1000 | Loss: 0.00003242
Iteration 67/1000 | Loss: 0.00003241
Iteration 68/1000 | Loss: 0.00003241
Iteration 69/1000 | Loss: 0.00003241
Iteration 70/1000 | Loss: 0.00003241
Iteration 71/1000 | Loss: 0.00003240
Iteration 72/1000 | Loss: 0.00003240
Iteration 73/1000 | Loss: 0.00003238
Iteration 74/1000 | Loss: 0.00003236
Iteration 75/1000 | Loss: 0.00003235
Iteration 76/1000 | Loss: 0.00003235
Iteration 77/1000 | Loss: 0.00003235
Iteration 78/1000 | Loss: 0.00003234
Iteration 79/1000 | Loss: 0.00003232
Iteration 80/1000 | Loss: 0.00003232
Iteration 81/1000 | Loss: 0.00003231
Iteration 82/1000 | Loss: 0.00003231
Iteration 83/1000 | Loss: 0.00003231
Iteration 84/1000 | Loss: 0.00003230
Iteration 85/1000 | Loss: 0.00003230
Iteration 86/1000 | Loss: 0.00003229
Iteration 87/1000 | Loss: 0.00003229
Iteration 88/1000 | Loss: 0.00003227
Iteration 89/1000 | Loss: 0.00003227
Iteration 90/1000 | Loss: 0.00003226
Iteration 91/1000 | Loss: 0.00003225
Iteration 92/1000 | Loss: 0.00003225
Iteration 93/1000 | Loss: 0.00003225
Iteration 94/1000 | Loss: 0.00003224
Iteration 95/1000 | Loss: 0.00003224
Iteration 96/1000 | Loss: 0.00003224
Iteration 97/1000 | Loss: 0.00003224
Iteration 98/1000 | Loss: 0.00003224
Iteration 99/1000 | Loss: 0.00003224
Iteration 100/1000 | Loss: 0.00003224
Iteration 101/1000 | Loss: 0.00003224
Iteration 102/1000 | Loss: 0.00003224
Iteration 103/1000 | Loss: 0.00003224
Iteration 104/1000 | Loss: 0.00003223
Iteration 105/1000 | Loss: 0.00003223
Iteration 106/1000 | Loss: 0.00003223
Iteration 107/1000 | Loss: 0.00003223
Iteration 108/1000 | Loss: 0.00003223
Iteration 109/1000 | Loss: 0.00003223
Iteration 110/1000 | Loss: 0.00003222
Iteration 111/1000 | Loss: 0.00003222
Iteration 112/1000 | Loss: 0.00003221
Iteration 113/1000 | Loss: 0.00003221
Iteration 114/1000 | Loss: 0.00003221
Iteration 115/1000 | Loss: 0.00003221
Iteration 116/1000 | Loss: 0.00003220
Iteration 117/1000 | Loss: 0.00003219
Iteration 118/1000 | Loss: 0.00003219
Iteration 119/1000 | Loss: 0.00003219
Iteration 120/1000 | Loss: 0.00003219
Iteration 121/1000 | Loss: 0.00003219
Iteration 122/1000 | Loss: 0.00003219
Iteration 123/1000 | Loss: 0.00003219
Iteration 124/1000 | Loss: 0.00003219
Iteration 125/1000 | Loss: 0.00003219
Iteration 126/1000 | Loss: 0.00003218
Iteration 127/1000 | Loss: 0.00003218
Iteration 128/1000 | Loss: 0.00003218
Iteration 129/1000 | Loss: 0.00003218
Iteration 130/1000 | Loss: 0.00003218
Iteration 131/1000 | Loss: 0.00003217
Iteration 132/1000 | Loss: 0.00003217
Iteration 133/1000 | Loss: 0.00003217
Iteration 134/1000 | Loss: 0.00003217
Iteration 135/1000 | Loss: 0.00003216
Iteration 136/1000 | Loss: 0.00003216
Iteration 137/1000 | Loss: 0.00003216
Iteration 138/1000 | Loss: 0.00003216
Iteration 139/1000 | Loss: 0.00003216
Iteration 140/1000 | Loss: 0.00003215
Iteration 141/1000 | Loss: 0.00003215
Iteration 142/1000 | Loss: 0.00003215
Iteration 143/1000 | Loss: 0.00003215
Iteration 144/1000 | Loss: 0.00003215
Iteration 145/1000 | Loss: 0.00003215
Iteration 146/1000 | Loss: 0.00003215
Iteration 147/1000 | Loss: 0.00003214
Iteration 148/1000 | Loss: 0.00003214
Iteration 149/1000 | Loss: 0.00003214
Iteration 150/1000 | Loss: 0.00003214
Iteration 151/1000 | Loss: 0.00003214
Iteration 152/1000 | Loss: 0.00003214
Iteration 153/1000 | Loss: 0.00003214
Iteration 154/1000 | Loss: 0.00003214
Iteration 155/1000 | Loss: 0.00003214
Iteration 156/1000 | Loss: 0.00003214
Iteration 157/1000 | Loss: 0.00003214
Iteration 158/1000 | Loss: 0.00003213
Iteration 159/1000 | Loss: 0.00003213
Iteration 160/1000 | Loss: 0.00003213
Iteration 161/1000 | Loss: 0.00003213
Iteration 162/1000 | Loss: 0.00003213
Iteration 163/1000 | Loss: 0.00003213
Iteration 164/1000 | Loss: 0.00003213
Iteration 165/1000 | Loss: 0.00003213
Iteration 166/1000 | Loss: 0.00003213
Iteration 167/1000 | Loss: 0.00003213
Iteration 168/1000 | Loss: 0.00003213
Iteration 169/1000 | Loss: 0.00003213
Iteration 170/1000 | Loss: 0.00003213
Iteration 171/1000 | Loss: 0.00003213
Iteration 172/1000 | Loss: 0.00003213
Iteration 173/1000 | Loss: 0.00003213
Iteration 174/1000 | Loss: 0.00003213
Iteration 175/1000 | Loss: 0.00003213
Iteration 176/1000 | Loss: 0.00003212
Iteration 177/1000 | Loss: 0.00003212
Iteration 178/1000 | Loss: 0.00003212
Iteration 179/1000 | Loss: 0.00003212
Iteration 180/1000 | Loss: 0.00003212
Iteration 181/1000 | Loss: 0.00003212
Iteration 182/1000 | Loss: 0.00003212
Iteration 183/1000 | Loss: 0.00003212
Iteration 184/1000 | Loss: 0.00003212
Iteration 185/1000 | Loss: 0.00003212
Iteration 186/1000 | Loss: 0.00003212
Iteration 187/1000 | Loss: 0.00003211
Iteration 188/1000 | Loss: 0.00003211
Iteration 189/1000 | Loss: 0.00003211
Iteration 190/1000 | Loss: 0.00003211
Iteration 191/1000 | Loss: 0.00003211
Iteration 192/1000 | Loss: 0.00003211
Iteration 193/1000 | Loss: 0.00003211
Iteration 194/1000 | Loss: 0.00003211
Iteration 195/1000 | Loss: 0.00003211
Iteration 196/1000 | Loss: 0.00003211
Iteration 197/1000 | Loss: 0.00003211
Iteration 198/1000 | Loss: 0.00003211
Iteration 199/1000 | Loss: 0.00003211
Iteration 200/1000 | Loss: 0.00003211
Iteration 201/1000 | Loss: 0.00003211
Iteration 202/1000 | Loss: 0.00003211
Iteration 203/1000 | Loss: 0.00003211
Iteration 204/1000 | Loss: 0.00003211
Iteration 205/1000 | Loss: 0.00003211
Iteration 206/1000 | Loss: 0.00003211
Iteration 207/1000 | Loss: 0.00003211
Iteration 208/1000 | Loss: 0.00003211
Iteration 209/1000 | Loss: 0.00003211
Iteration 210/1000 | Loss: 0.00003211
Iteration 211/1000 | Loss: 0.00003211
Iteration 212/1000 | Loss: 0.00003211
Iteration 213/1000 | Loss: 0.00003211
Iteration 214/1000 | Loss: 0.00003211
Iteration 215/1000 | Loss: 0.00003211
Iteration 216/1000 | Loss: 0.00003211
Iteration 217/1000 | Loss: 0.00003211
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 217. Stopping optimization.
Last 5 losses: [3.2109030144056305e-05, 3.2109030144056305e-05, 3.2109030144056305e-05, 3.2109030144056305e-05, 3.2109030144056305e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.2109030144056305e-05

Optimization complete. Final v2v error: 3.3213798999786377 mm

Highest mean error: 11.140519142150879 mm for frame 61

Lowest mean error: 2.5569188594818115 mm for frame 215

Saving results

Total time: 120.60885739326477
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_022/1049/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_022/1049.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_022/1049
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00863123
Iteration 2/25 | Loss: 0.00154480
Iteration 3/25 | Loss: 0.00130087
Iteration 4/25 | Loss: 0.00126581
Iteration 5/25 | Loss: 0.00124588
Iteration 6/25 | Loss: 0.00123915
Iteration 7/25 | Loss: 0.00123655
Iteration 8/25 | Loss: 0.00123609
Iteration 9/25 | Loss: 0.00124003
Iteration 10/25 | Loss: 0.00123958
Iteration 11/25 | Loss: 0.00123969
Iteration 12/25 | Loss: 0.00123798
Iteration 13/25 | Loss: 0.00123725
Iteration 14/25 | Loss: 0.00123714
Iteration 15/25 | Loss: 0.00123710
Iteration 16/25 | Loss: 0.00123710
Iteration 17/25 | Loss: 0.00123710
Iteration 18/25 | Loss: 0.00123710
Iteration 19/25 | Loss: 0.00123710
Iteration 20/25 | Loss: 0.00123710
Iteration 21/25 | Loss: 0.00123710
Iteration 22/25 | Loss: 0.00123710
Iteration 23/25 | Loss: 0.00123710
Iteration 24/25 | Loss: 0.00123710
Iteration 25/25 | Loss: 0.00123710

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.26725745
Iteration 2/25 | Loss: 0.00098439
Iteration 3/25 | Loss: 0.00098439
Iteration 4/25 | Loss: 0.00098439
Iteration 5/25 | Loss: 0.00098439
Iteration 6/25 | Loss: 0.00098439
Iteration 7/25 | Loss: 0.00098439
Iteration 8/25 | Loss: 0.00098439
Iteration 9/25 | Loss: 0.00098439
Iteration 10/25 | Loss: 0.00098439
Iteration 11/25 | Loss: 0.00098439
Iteration 12/25 | Loss: 0.00098439
Iteration 13/25 | Loss: 0.00098439
Iteration 14/25 | Loss: 0.00098439
Iteration 15/25 | Loss: 0.00098439
Iteration 16/25 | Loss: 0.00098439
Iteration 17/25 | Loss: 0.00098439
Iteration 18/25 | Loss: 0.00098439
Iteration 19/25 | Loss: 0.00098439
Iteration 20/25 | Loss: 0.00098439
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0009843892185017467, 0.0009843892185017467, 0.0009843892185017467, 0.0009843892185017467, 0.0009843892185017467]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009843892185017467

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00098439
Iteration 2/1000 | Loss: 0.00007118
Iteration 3/1000 | Loss: 0.00008837
Iteration 4/1000 | Loss: 0.00004180
Iteration 5/1000 | Loss: 0.00002811
Iteration 6/1000 | Loss: 0.00011874
Iteration 7/1000 | Loss: 0.00002932
Iteration 8/1000 | Loss: 0.00002687
Iteration 9/1000 | Loss: 0.00002564
Iteration 10/1000 | Loss: 0.00002512
Iteration 11/1000 | Loss: 0.00002464
Iteration 12/1000 | Loss: 0.00002417
Iteration 13/1000 | Loss: 0.00002374
Iteration 14/1000 | Loss: 0.00012688
Iteration 15/1000 | Loss: 0.00003305
Iteration 16/1000 | Loss: 0.00002769
Iteration 17/1000 | Loss: 0.00002462
Iteration 18/1000 | Loss: 0.00002355
Iteration 19/1000 | Loss: 0.00002312
Iteration 20/1000 | Loss: 0.00002295
Iteration 21/1000 | Loss: 0.00002286
Iteration 22/1000 | Loss: 0.00002283
Iteration 23/1000 | Loss: 0.00002282
Iteration 24/1000 | Loss: 0.00002282
Iteration 25/1000 | Loss: 0.00002281
Iteration 26/1000 | Loss: 0.00002281
Iteration 27/1000 | Loss: 0.00002281
Iteration 28/1000 | Loss: 0.00002278
Iteration 29/1000 | Loss: 0.00002277
Iteration 30/1000 | Loss: 0.00002277
Iteration 31/1000 | Loss: 0.00002277
Iteration 32/1000 | Loss: 0.00002277
Iteration 33/1000 | Loss: 0.00002277
Iteration 34/1000 | Loss: 0.00002277
Iteration 35/1000 | Loss: 0.00002277
Iteration 36/1000 | Loss: 0.00002277
Iteration 37/1000 | Loss: 0.00002273
Iteration 38/1000 | Loss: 0.00002268
Iteration 39/1000 | Loss: 0.00002263
Iteration 40/1000 | Loss: 0.00002256
Iteration 41/1000 | Loss: 0.00002253
Iteration 42/1000 | Loss: 0.00002251
Iteration 43/1000 | Loss: 0.00002251
Iteration 44/1000 | Loss: 0.00002251
Iteration 45/1000 | Loss: 0.00002251
Iteration 46/1000 | Loss: 0.00002251
Iteration 47/1000 | Loss: 0.00002251
Iteration 48/1000 | Loss: 0.00002251
Iteration 49/1000 | Loss: 0.00002251
Iteration 50/1000 | Loss: 0.00002251
Iteration 51/1000 | Loss: 0.00002251
Iteration 52/1000 | Loss: 0.00002251
Iteration 53/1000 | Loss: 0.00002250
Iteration 54/1000 | Loss: 0.00002250
Iteration 55/1000 | Loss: 0.00002250
Iteration 56/1000 | Loss: 0.00002250
Iteration 57/1000 | Loss: 0.00002250
Iteration 58/1000 | Loss: 0.00002250
Iteration 59/1000 | Loss: 0.00002248
Iteration 60/1000 | Loss: 0.00002248
Iteration 61/1000 | Loss: 0.00002247
Iteration 62/1000 | Loss: 0.00002247
Iteration 63/1000 | Loss: 0.00002246
Iteration 64/1000 | Loss: 0.00002245
Iteration 65/1000 | Loss: 0.00002245
Iteration 66/1000 | Loss: 0.00002245
Iteration 67/1000 | Loss: 0.00002244
Iteration 68/1000 | Loss: 0.00002244
Iteration 69/1000 | Loss: 0.00002243
Iteration 70/1000 | Loss: 0.00002243
Iteration 71/1000 | Loss: 0.00002243
Iteration 72/1000 | Loss: 0.00002242
Iteration 73/1000 | Loss: 0.00002242
Iteration 74/1000 | Loss: 0.00002242
Iteration 75/1000 | Loss: 0.00002241
Iteration 76/1000 | Loss: 0.00002241
Iteration 77/1000 | Loss: 0.00002241
Iteration 78/1000 | Loss: 0.00002241
Iteration 79/1000 | Loss: 0.00002240
Iteration 80/1000 | Loss: 0.00002240
Iteration 81/1000 | Loss: 0.00002240
Iteration 82/1000 | Loss: 0.00002240
Iteration 83/1000 | Loss: 0.00002240
Iteration 84/1000 | Loss: 0.00002239
Iteration 85/1000 | Loss: 0.00002239
Iteration 86/1000 | Loss: 0.00002239
Iteration 87/1000 | Loss: 0.00002238
Iteration 88/1000 | Loss: 0.00002238
Iteration 89/1000 | Loss: 0.00002238
Iteration 90/1000 | Loss: 0.00002237
Iteration 91/1000 | Loss: 0.00002237
Iteration 92/1000 | Loss: 0.00002236
Iteration 93/1000 | Loss: 0.00002236
Iteration 94/1000 | Loss: 0.00002236
Iteration 95/1000 | Loss: 0.00002235
Iteration 96/1000 | Loss: 0.00002233
Iteration 97/1000 | Loss: 0.00002233
Iteration 98/1000 | Loss: 0.00014336
Iteration 99/1000 | Loss: 0.00007338
Iteration 100/1000 | Loss: 0.00017362
Iteration 101/1000 | Loss: 0.00007311
Iteration 102/1000 | Loss: 0.00017069
Iteration 103/1000 | Loss: 0.00011989
Iteration 104/1000 | Loss: 0.00045581
Iteration 105/1000 | Loss: 0.00037813
Iteration 106/1000 | Loss: 0.00012950
Iteration 107/1000 | Loss: 0.00004525
Iteration 108/1000 | Loss: 0.00003245
Iteration 109/1000 | Loss: 0.00003652
Iteration 110/1000 | Loss: 0.00002983
Iteration 111/1000 | Loss: 0.00002791
Iteration 112/1000 | Loss: 0.00002621
Iteration 113/1000 | Loss: 0.00002503
Iteration 114/1000 | Loss: 0.00002404
Iteration 115/1000 | Loss: 0.00002365
Iteration 116/1000 | Loss: 0.00002342
Iteration 117/1000 | Loss: 0.00002338
Iteration 118/1000 | Loss: 0.00002318
Iteration 119/1000 | Loss: 0.00002304
Iteration 120/1000 | Loss: 0.00002301
Iteration 121/1000 | Loss: 0.00002296
Iteration 122/1000 | Loss: 0.00002283
Iteration 123/1000 | Loss: 0.00002276
Iteration 124/1000 | Loss: 0.00002270
Iteration 125/1000 | Loss: 0.00002270
Iteration 126/1000 | Loss: 0.00002259
Iteration 127/1000 | Loss: 0.00002258
Iteration 128/1000 | Loss: 0.00002258
Iteration 129/1000 | Loss: 0.00002258
Iteration 130/1000 | Loss: 0.00002257
Iteration 131/1000 | Loss: 0.00002257
Iteration 132/1000 | Loss: 0.00002256
Iteration 133/1000 | Loss: 0.00002256
Iteration 134/1000 | Loss: 0.00002255
Iteration 135/1000 | Loss: 0.00002255
Iteration 136/1000 | Loss: 0.00002254
Iteration 137/1000 | Loss: 0.00002253
Iteration 138/1000 | Loss: 0.00002253
Iteration 139/1000 | Loss: 0.00002253
Iteration 140/1000 | Loss: 0.00002252
Iteration 141/1000 | Loss: 0.00002252
Iteration 142/1000 | Loss: 0.00002251
Iteration 143/1000 | Loss: 0.00002251
Iteration 144/1000 | Loss: 0.00002250
Iteration 145/1000 | Loss: 0.00002250
Iteration 146/1000 | Loss: 0.00002250
Iteration 147/1000 | Loss: 0.00002249
Iteration 148/1000 | Loss: 0.00002249
Iteration 149/1000 | Loss: 0.00002249
Iteration 150/1000 | Loss: 0.00002248
Iteration 151/1000 | Loss: 0.00002248
Iteration 152/1000 | Loss: 0.00002248
Iteration 153/1000 | Loss: 0.00002248
Iteration 154/1000 | Loss: 0.00002247
Iteration 155/1000 | Loss: 0.00002247
Iteration 156/1000 | Loss: 0.00002246
Iteration 157/1000 | Loss: 0.00002246
Iteration 158/1000 | Loss: 0.00002246
Iteration 159/1000 | Loss: 0.00002246
Iteration 160/1000 | Loss: 0.00002246
Iteration 161/1000 | Loss: 0.00002245
Iteration 162/1000 | Loss: 0.00002245
Iteration 163/1000 | Loss: 0.00002245
Iteration 164/1000 | Loss: 0.00002245
Iteration 165/1000 | Loss: 0.00002244
Iteration 166/1000 | Loss: 0.00002244
Iteration 167/1000 | Loss: 0.00002241
Iteration 168/1000 | Loss: 0.00002241
Iteration 169/1000 | Loss: 0.00002239
Iteration 170/1000 | Loss: 0.00002238
Iteration 171/1000 | Loss: 0.00002238
Iteration 172/1000 | Loss: 0.00002237
Iteration 173/1000 | Loss: 0.00002237
Iteration 174/1000 | Loss: 0.00002236
Iteration 175/1000 | Loss: 0.00002236
Iteration 176/1000 | Loss: 0.00002235
Iteration 177/1000 | Loss: 0.00002235
Iteration 178/1000 | Loss: 0.00002235
Iteration 179/1000 | Loss: 0.00002234
Iteration 180/1000 | Loss: 0.00002234
Iteration 181/1000 | Loss: 0.00002234
Iteration 182/1000 | Loss: 0.00002233
Iteration 183/1000 | Loss: 0.00002233
Iteration 184/1000 | Loss: 0.00002232
Iteration 185/1000 | Loss: 0.00002232
Iteration 186/1000 | Loss: 0.00002232
Iteration 187/1000 | Loss: 0.00002232
Iteration 188/1000 | Loss: 0.00002231
Iteration 189/1000 | Loss: 0.00002231
Iteration 190/1000 | Loss: 0.00002231
Iteration 191/1000 | Loss: 0.00002230
Iteration 192/1000 | Loss: 0.00002230
Iteration 193/1000 | Loss: 0.00002230
Iteration 194/1000 | Loss: 0.00002230
Iteration 195/1000 | Loss: 0.00002230
Iteration 196/1000 | Loss: 0.00002229
Iteration 197/1000 | Loss: 0.00002229
Iteration 198/1000 | Loss: 0.00002228
Iteration 199/1000 | Loss: 0.00002228
Iteration 200/1000 | Loss: 0.00002228
Iteration 201/1000 | Loss: 0.00002228
Iteration 202/1000 | Loss: 0.00002228
Iteration 203/1000 | Loss: 0.00002228
Iteration 204/1000 | Loss: 0.00002228
Iteration 205/1000 | Loss: 0.00002228
Iteration 206/1000 | Loss: 0.00002228
Iteration 207/1000 | Loss: 0.00002228
Iteration 208/1000 | Loss: 0.00002227
Iteration 209/1000 | Loss: 0.00002227
Iteration 210/1000 | Loss: 0.00002227
Iteration 211/1000 | Loss: 0.00002226
Iteration 212/1000 | Loss: 0.00002226
Iteration 213/1000 | Loss: 0.00002226
Iteration 214/1000 | Loss: 0.00002226
Iteration 215/1000 | Loss: 0.00002226
Iteration 216/1000 | Loss: 0.00002225
Iteration 217/1000 | Loss: 0.00002225
Iteration 218/1000 | Loss: 0.00002225
Iteration 219/1000 | Loss: 0.00002225
Iteration 220/1000 | Loss: 0.00002225
Iteration 221/1000 | Loss: 0.00002225
Iteration 222/1000 | Loss: 0.00002225
Iteration 223/1000 | Loss: 0.00002225
Iteration 224/1000 | Loss: 0.00002225
Iteration 225/1000 | Loss: 0.00002225
Iteration 226/1000 | Loss: 0.00002225
Iteration 227/1000 | Loss: 0.00002225
Iteration 228/1000 | Loss: 0.00002225
Iteration 229/1000 | Loss: 0.00002225
Iteration 230/1000 | Loss: 0.00002225
Iteration 231/1000 | Loss: 0.00002225
Iteration 232/1000 | Loss: 0.00002224
Iteration 233/1000 | Loss: 0.00002224
Iteration 234/1000 | Loss: 0.00002224
Iteration 235/1000 | Loss: 0.00002224
Iteration 236/1000 | Loss: 0.00002224
Iteration 237/1000 | Loss: 0.00002224
Iteration 238/1000 | Loss: 0.00002224
Iteration 239/1000 | Loss: 0.00002224
Iteration 240/1000 | Loss: 0.00002224
Iteration 241/1000 | Loss: 0.00002224
Iteration 242/1000 | Loss: 0.00002224
Iteration 243/1000 | Loss: 0.00002224
Iteration 244/1000 | Loss: 0.00002224
Iteration 245/1000 | Loss: 0.00002224
Iteration 246/1000 | Loss: 0.00002224
Iteration 247/1000 | Loss: 0.00002223
Iteration 248/1000 | Loss: 0.00002223
Iteration 249/1000 | Loss: 0.00002223
Iteration 250/1000 | Loss: 0.00002223
Iteration 251/1000 | Loss: 0.00002223
Iteration 252/1000 | Loss: 0.00002223
Iteration 253/1000 | Loss: 0.00002223
Iteration 254/1000 | Loss: 0.00002223
Iteration 255/1000 | Loss: 0.00002223
Iteration 256/1000 | Loss: 0.00002223
Iteration 257/1000 | Loss: 0.00002223
Iteration 258/1000 | Loss: 0.00002222
Iteration 259/1000 | Loss: 0.00002222
Iteration 260/1000 | Loss: 0.00002222
Iteration 261/1000 | Loss: 0.00002222
Iteration 262/1000 | Loss: 0.00002222
Iteration 263/1000 | Loss: 0.00002222
Iteration 264/1000 | Loss: 0.00002221
Iteration 265/1000 | Loss: 0.00002221
Iteration 266/1000 | Loss: 0.00002221
Iteration 267/1000 | Loss: 0.00002220
Iteration 268/1000 | Loss: 0.00002220
Iteration 269/1000 | Loss: 0.00002220
Iteration 270/1000 | Loss: 0.00002220
Iteration 271/1000 | Loss: 0.00002220
Iteration 272/1000 | Loss: 0.00002220
Iteration 273/1000 | Loss: 0.00002220
Iteration 274/1000 | Loss: 0.00002219
Iteration 275/1000 | Loss: 0.00002219
Iteration 276/1000 | Loss: 0.00002219
Iteration 277/1000 | Loss: 0.00002219
Iteration 278/1000 | Loss: 0.00002219
Iteration 279/1000 | Loss: 0.00002219
Iteration 280/1000 | Loss: 0.00002219
Iteration 281/1000 | Loss: 0.00002219
Iteration 282/1000 | Loss: 0.00002219
Iteration 283/1000 | Loss: 0.00002219
Iteration 284/1000 | Loss: 0.00002219
Iteration 285/1000 | Loss: 0.00002219
Iteration 286/1000 | Loss: 0.00002219
Iteration 287/1000 | Loss: 0.00002219
Iteration 288/1000 | Loss: 0.00002219
Iteration 289/1000 | Loss: 0.00002219
Iteration 290/1000 | Loss: 0.00002219
Iteration 291/1000 | Loss: 0.00002219
Iteration 292/1000 | Loss: 0.00002219
Iteration 293/1000 | Loss: 0.00002219
Iteration 294/1000 | Loss: 0.00002219
Iteration 295/1000 | Loss: 0.00002219
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 295. Stopping optimization.
Last 5 losses: [2.2188401999301277e-05, 2.2188401999301277e-05, 2.2188401999301277e-05, 2.2188401999301277e-05, 2.2188401999301277e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2188401999301277e-05

Optimization complete. Final v2v error: 3.9416966438293457 mm

Highest mean error: 7.387136459350586 mm for frame 162

Lowest mean error: 3.483628034591675 mm for frame 30

Saving results

Total time: 125.53584909439087
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_022/1068/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_022/1068.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_022/1068
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00491488
Iteration 2/25 | Loss: 0.00143954
Iteration 3/25 | Loss: 0.00116786
Iteration 4/25 | Loss: 0.00113058
Iteration 5/25 | Loss: 0.00112447
Iteration 6/25 | Loss: 0.00112263
Iteration 7/25 | Loss: 0.00112261
Iteration 8/25 | Loss: 0.00112261
Iteration 9/25 | Loss: 0.00112261
Iteration 10/25 | Loss: 0.00112261
Iteration 11/25 | Loss: 0.00112261
Iteration 12/25 | Loss: 0.00112261
Iteration 13/25 | Loss: 0.00112261
Iteration 14/25 | Loss: 0.00112261
Iteration 15/25 | Loss: 0.00112261
Iteration 16/25 | Loss: 0.00112261
Iteration 17/25 | Loss: 0.00112261
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0011226149508729577, 0.0011226149508729577, 0.0011226149508729577, 0.0011226149508729577, 0.0011226149508729577]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011226149508729577

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37647736
Iteration 2/25 | Loss: 0.00071571
Iteration 3/25 | Loss: 0.00071570
Iteration 4/25 | Loss: 0.00071570
Iteration 5/25 | Loss: 0.00071570
Iteration 6/25 | Loss: 0.00071570
Iteration 7/25 | Loss: 0.00071570
Iteration 8/25 | Loss: 0.00071570
Iteration 9/25 | Loss: 0.00071570
Iteration 10/25 | Loss: 0.00071570
Iteration 11/25 | Loss: 0.00071570
Iteration 12/25 | Loss: 0.00071570
Iteration 13/25 | Loss: 0.00071570
Iteration 14/25 | Loss: 0.00071570
Iteration 15/25 | Loss: 0.00071570
Iteration 16/25 | Loss: 0.00071570
Iteration 17/25 | Loss: 0.00071570
Iteration 18/25 | Loss: 0.00071570
Iteration 19/25 | Loss: 0.00071570
Iteration 20/25 | Loss: 0.00071570
Iteration 21/25 | Loss: 0.00071570
Iteration 22/25 | Loss: 0.00071570
Iteration 23/25 | Loss: 0.00071570
Iteration 24/25 | Loss: 0.00071570
Iteration 25/25 | Loss: 0.00071570
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0007156985811889172, 0.0007156985811889172, 0.0007156985811889172, 0.0007156985811889172, 0.0007156985811889172]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007156985811889172

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00071570
Iteration 2/1000 | Loss: 0.00002801
Iteration 3/1000 | Loss: 0.00001834
Iteration 4/1000 | Loss: 0.00001661
Iteration 5/1000 | Loss: 0.00001581
Iteration 6/1000 | Loss: 0.00001520
Iteration 7/1000 | Loss: 0.00001474
Iteration 8/1000 | Loss: 0.00001440
Iteration 9/1000 | Loss: 0.00001433
Iteration 10/1000 | Loss: 0.00001424
Iteration 11/1000 | Loss: 0.00001420
Iteration 12/1000 | Loss: 0.00001402
Iteration 13/1000 | Loss: 0.00001395
Iteration 14/1000 | Loss: 0.00001385
Iteration 15/1000 | Loss: 0.00001379
Iteration 16/1000 | Loss: 0.00001378
Iteration 17/1000 | Loss: 0.00001377
Iteration 18/1000 | Loss: 0.00001377
Iteration 19/1000 | Loss: 0.00001376
Iteration 20/1000 | Loss: 0.00001375
Iteration 21/1000 | Loss: 0.00001375
Iteration 22/1000 | Loss: 0.00001373
Iteration 23/1000 | Loss: 0.00001371
Iteration 24/1000 | Loss: 0.00001370
Iteration 25/1000 | Loss: 0.00001370
Iteration 26/1000 | Loss: 0.00001368
Iteration 27/1000 | Loss: 0.00001368
Iteration 28/1000 | Loss: 0.00001365
Iteration 29/1000 | Loss: 0.00001365
Iteration 30/1000 | Loss: 0.00001364
Iteration 31/1000 | Loss: 0.00001363
Iteration 32/1000 | Loss: 0.00001362
Iteration 33/1000 | Loss: 0.00001362
Iteration 34/1000 | Loss: 0.00001361
Iteration 35/1000 | Loss: 0.00001361
Iteration 36/1000 | Loss: 0.00001361
Iteration 37/1000 | Loss: 0.00001361
Iteration 38/1000 | Loss: 0.00001361
Iteration 39/1000 | Loss: 0.00001361
Iteration 40/1000 | Loss: 0.00001361
Iteration 41/1000 | Loss: 0.00001361
Iteration 42/1000 | Loss: 0.00001361
Iteration 43/1000 | Loss: 0.00001361
Iteration 44/1000 | Loss: 0.00001361
Iteration 45/1000 | Loss: 0.00001361
Iteration 46/1000 | Loss: 0.00001360
Iteration 47/1000 | Loss: 0.00001360
Iteration 48/1000 | Loss: 0.00001360
Iteration 49/1000 | Loss: 0.00001360
Iteration 50/1000 | Loss: 0.00001359
Iteration 51/1000 | Loss: 0.00001359
Iteration 52/1000 | Loss: 0.00001358
Iteration 53/1000 | Loss: 0.00001358
Iteration 54/1000 | Loss: 0.00001357
Iteration 55/1000 | Loss: 0.00001357
Iteration 56/1000 | Loss: 0.00001357
Iteration 57/1000 | Loss: 0.00001357
Iteration 58/1000 | Loss: 0.00001357
Iteration 59/1000 | Loss: 0.00001357
Iteration 60/1000 | Loss: 0.00001357
Iteration 61/1000 | Loss: 0.00001356
Iteration 62/1000 | Loss: 0.00001356
Iteration 63/1000 | Loss: 0.00001356
Iteration 64/1000 | Loss: 0.00001356
Iteration 65/1000 | Loss: 0.00001356
Iteration 66/1000 | Loss: 0.00001355
Iteration 67/1000 | Loss: 0.00001355
Iteration 68/1000 | Loss: 0.00001355
Iteration 69/1000 | Loss: 0.00001355
Iteration 70/1000 | Loss: 0.00001355
Iteration 71/1000 | Loss: 0.00001354
Iteration 72/1000 | Loss: 0.00001354
Iteration 73/1000 | Loss: 0.00001354
Iteration 74/1000 | Loss: 0.00001354
Iteration 75/1000 | Loss: 0.00001354
Iteration 76/1000 | Loss: 0.00001354
Iteration 77/1000 | Loss: 0.00001354
Iteration 78/1000 | Loss: 0.00001353
Iteration 79/1000 | Loss: 0.00001353
Iteration 80/1000 | Loss: 0.00001353
Iteration 81/1000 | Loss: 0.00001352
Iteration 82/1000 | Loss: 0.00001352
Iteration 83/1000 | Loss: 0.00001352
Iteration 84/1000 | Loss: 0.00001352
Iteration 85/1000 | Loss: 0.00001352
Iteration 86/1000 | Loss: 0.00001352
Iteration 87/1000 | Loss: 0.00001352
Iteration 88/1000 | Loss: 0.00001351
Iteration 89/1000 | Loss: 0.00001351
Iteration 90/1000 | Loss: 0.00001351
Iteration 91/1000 | Loss: 0.00001350
Iteration 92/1000 | Loss: 0.00001349
Iteration 93/1000 | Loss: 0.00001349
Iteration 94/1000 | Loss: 0.00001349
Iteration 95/1000 | Loss: 0.00001349
Iteration 96/1000 | Loss: 0.00001349
Iteration 97/1000 | Loss: 0.00001349
Iteration 98/1000 | Loss: 0.00001349
Iteration 99/1000 | Loss: 0.00001349
Iteration 100/1000 | Loss: 0.00001348
Iteration 101/1000 | Loss: 0.00001348
Iteration 102/1000 | Loss: 0.00001348
Iteration 103/1000 | Loss: 0.00001348
Iteration 104/1000 | Loss: 0.00001348
Iteration 105/1000 | Loss: 0.00001348
Iteration 106/1000 | Loss: 0.00001348
Iteration 107/1000 | Loss: 0.00001347
Iteration 108/1000 | Loss: 0.00001347
Iteration 109/1000 | Loss: 0.00001347
Iteration 110/1000 | Loss: 0.00001346
Iteration 111/1000 | Loss: 0.00001346
Iteration 112/1000 | Loss: 0.00001346
Iteration 113/1000 | Loss: 0.00001346
Iteration 114/1000 | Loss: 0.00001346
Iteration 115/1000 | Loss: 0.00001346
Iteration 116/1000 | Loss: 0.00001346
Iteration 117/1000 | Loss: 0.00001346
Iteration 118/1000 | Loss: 0.00001346
Iteration 119/1000 | Loss: 0.00001345
Iteration 120/1000 | Loss: 0.00001345
Iteration 121/1000 | Loss: 0.00001345
Iteration 122/1000 | Loss: 0.00001345
Iteration 123/1000 | Loss: 0.00001345
Iteration 124/1000 | Loss: 0.00001345
Iteration 125/1000 | Loss: 0.00001344
Iteration 126/1000 | Loss: 0.00001344
Iteration 127/1000 | Loss: 0.00001344
Iteration 128/1000 | Loss: 0.00001344
Iteration 129/1000 | Loss: 0.00001344
Iteration 130/1000 | Loss: 0.00001344
Iteration 131/1000 | Loss: 0.00001344
Iteration 132/1000 | Loss: 0.00001344
Iteration 133/1000 | Loss: 0.00001344
Iteration 134/1000 | Loss: 0.00001344
Iteration 135/1000 | Loss: 0.00001344
Iteration 136/1000 | Loss: 0.00001344
Iteration 137/1000 | Loss: 0.00001344
Iteration 138/1000 | Loss: 0.00001344
Iteration 139/1000 | Loss: 0.00001344
Iteration 140/1000 | Loss: 0.00001344
Iteration 141/1000 | Loss: 0.00001344
Iteration 142/1000 | Loss: 0.00001344
Iteration 143/1000 | Loss: 0.00001344
Iteration 144/1000 | Loss: 0.00001344
Iteration 145/1000 | Loss: 0.00001344
Iteration 146/1000 | Loss: 0.00001344
Iteration 147/1000 | Loss: 0.00001344
Iteration 148/1000 | Loss: 0.00001344
Iteration 149/1000 | Loss: 0.00001344
Iteration 150/1000 | Loss: 0.00001344
Iteration 151/1000 | Loss: 0.00001344
Iteration 152/1000 | Loss: 0.00001344
Iteration 153/1000 | Loss: 0.00001344
Iteration 154/1000 | Loss: 0.00001344
Iteration 155/1000 | Loss: 0.00001344
Iteration 156/1000 | Loss: 0.00001344
Iteration 157/1000 | Loss: 0.00001344
Iteration 158/1000 | Loss: 0.00001344
Iteration 159/1000 | Loss: 0.00001344
Iteration 160/1000 | Loss: 0.00001344
Iteration 161/1000 | Loss: 0.00001344
Iteration 162/1000 | Loss: 0.00001344
Iteration 163/1000 | Loss: 0.00001344
Iteration 164/1000 | Loss: 0.00001344
Iteration 165/1000 | Loss: 0.00001344
Iteration 166/1000 | Loss: 0.00001344
Iteration 167/1000 | Loss: 0.00001344
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 167. Stopping optimization.
Last 5 losses: [1.3435339496936649e-05, 1.3435339496936649e-05, 1.3435339496936649e-05, 1.3435339496936649e-05, 1.3435339496936649e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3435339496936649e-05

Optimization complete. Final v2v error: 3.060642719268799 mm

Highest mean error: 4.233893871307373 mm for frame 92

Lowest mean error: 2.718994379043579 mm for frame 28

Saving results

Total time: 37.508392095565796
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_022/1079/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_022/1079.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_022/1079
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01017997
Iteration 2/25 | Loss: 0.00356579
Iteration 3/25 | Loss: 0.00247844
Iteration 4/25 | Loss: 0.00203311
Iteration 5/25 | Loss: 0.00192892
Iteration 6/25 | Loss: 0.00169224
Iteration 7/25 | Loss: 0.00156038
Iteration 8/25 | Loss: 0.00138332
Iteration 9/25 | Loss: 0.00133159
Iteration 10/25 | Loss: 0.00129970
Iteration 11/25 | Loss: 0.00125353
Iteration 12/25 | Loss: 0.00124068
Iteration 13/25 | Loss: 0.00123507
Iteration 14/25 | Loss: 0.00123525
Iteration 15/25 | Loss: 0.00123105
Iteration 16/25 | Loss: 0.00122997
Iteration 17/25 | Loss: 0.00122902
Iteration 18/25 | Loss: 0.00122788
Iteration 19/25 | Loss: 0.00122729
Iteration 20/25 | Loss: 0.00122989
Iteration 21/25 | Loss: 0.00122518
Iteration 22/25 | Loss: 0.00122469
Iteration 23/25 | Loss: 0.00122458
Iteration 24/25 | Loss: 0.00122455
Iteration 25/25 | Loss: 0.00122454

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36893559
Iteration 2/25 | Loss: 0.00112661
Iteration 3/25 | Loss: 0.00112661
Iteration 4/25 | Loss: 0.00112661
Iteration 5/25 | Loss: 0.00112660
Iteration 6/25 | Loss: 0.00112660
Iteration 7/25 | Loss: 0.00112660
Iteration 8/25 | Loss: 0.00112660
Iteration 9/25 | Loss: 0.00112660
Iteration 10/25 | Loss: 0.00112660
Iteration 11/25 | Loss: 0.00112660
Iteration 12/25 | Loss: 0.00112660
Iteration 13/25 | Loss: 0.00112660
Iteration 14/25 | Loss: 0.00112660
Iteration 15/25 | Loss: 0.00112660
Iteration 16/25 | Loss: 0.00112660
Iteration 17/25 | Loss: 0.00112660
Iteration 18/25 | Loss: 0.00112660
Iteration 19/25 | Loss: 0.00112660
Iteration 20/25 | Loss: 0.00112660
Iteration 21/25 | Loss: 0.00112660
Iteration 22/25 | Loss: 0.00112660
Iteration 23/25 | Loss: 0.00112660
Iteration 24/25 | Loss: 0.00112660
Iteration 25/25 | Loss: 0.00112660
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0011266019428148866, 0.0011266019428148866, 0.0011266019428148866, 0.0011266019428148866, 0.0011266019428148866]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011266019428148866

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00112660
Iteration 2/1000 | Loss: 0.00010563
Iteration 3/1000 | Loss: 0.00008347
Iteration 4/1000 | Loss: 0.00007456
Iteration 5/1000 | Loss: 0.00006982
Iteration 6/1000 | Loss: 0.00006738
Iteration 7/1000 | Loss: 0.00006515
Iteration 8/1000 | Loss: 0.00006317
Iteration 9/1000 | Loss: 0.00006156
Iteration 10/1000 | Loss: 0.00439458
Iteration 11/1000 | Loss: 0.00102637
Iteration 12/1000 | Loss: 0.00013354
Iteration 13/1000 | Loss: 0.00006785
Iteration 14/1000 | Loss: 0.00004815
Iteration 15/1000 | Loss: 0.00003982
Iteration 16/1000 | Loss: 0.00003303
Iteration 17/1000 | Loss: 0.00002847
Iteration 18/1000 | Loss: 0.00002450
Iteration 19/1000 | Loss: 0.00002264
Iteration 20/1000 | Loss: 0.00002062
Iteration 21/1000 | Loss: 0.00001896
Iteration 22/1000 | Loss: 0.00001770
Iteration 23/1000 | Loss: 0.00001683
Iteration 24/1000 | Loss: 0.00001611
Iteration 25/1000 | Loss: 0.00001555
Iteration 26/1000 | Loss: 0.00001528
Iteration 27/1000 | Loss: 0.00001506
Iteration 28/1000 | Loss: 0.00001495
Iteration 29/1000 | Loss: 0.00001494
Iteration 30/1000 | Loss: 0.00001483
Iteration 31/1000 | Loss: 0.00001480
Iteration 32/1000 | Loss: 0.00001480
Iteration 33/1000 | Loss: 0.00001479
Iteration 34/1000 | Loss: 0.00001477
Iteration 35/1000 | Loss: 0.00001475
Iteration 36/1000 | Loss: 0.00001472
Iteration 37/1000 | Loss: 0.00001472
Iteration 38/1000 | Loss: 0.00001471
Iteration 39/1000 | Loss: 0.00001471
Iteration 40/1000 | Loss: 0.00001470
Iteration 41/1000 | Loss: 0.00001470
Iteration 42/1000 | Loss: 0.00001470
Iteration 43/1000 | Loss: 0.00001470
Iteration 44/1000 | Loss: 0.00001470
Iteration 45/1000 | Loss: 0.00001470
Iteration 46/1000 | Loss: 0.00001470
Iteration 47/1000 | Loss: 0.00001469
Iteration 48/1000 | Loss: 0.00001469
Iteration 49/1000 | Loss: 0.00001469
Iteration 50/1000 | Loss: 0.00001469
Iteration 51/1000 | Loss: 0.00001469
Iteration 52/1000 | Loss: 0.00001469
Iteration 53/1000 | Loss: 0.00001469
Iteration 54/1000 | Loss: 0.00001468
Iteration 55/1000 | Loss: 0.00001468
Iteration 56/1000 | Loss: 0.00001468
Iteration 57/1000 | Loss: 0.00001468
Iteration 58/1000 | Loss: 0.00001468
Iteration 59/1000 | Loss: 0.00001467
Iteration 60/1000 | Loss: 0.00001467
Iteration 61/1000 | Loss: 0.00001467
Iteration 62/1000 | Loss: 0.00001466
Iteration 63/1000 | Loss: 0.00001466
Iteration 64/1000 | Loss: 0.00001466
Iteration 65/1000 | Loss: 0.00001465
Iteration 66/1000 | Loss: 0.00001465
Iteration 67/1000 | Loss: 0.00001465
Iteration 68/1000 | Loss: 0.00001465
Iteration 69/1000 | Loss: 0.00001465
Iteration 70/1000 | Loss: 0.00001465
Iteration 71/1000 | Loss: 0.00001464
Iteration 72/1000 | Loss: 0.00001464
Iteration 73/1000 | Loss: 0.00001464
Iteration 74/1000 | Loss: 0.00001463
Iteration 75/1000 | Loss: 0.00001463
Iteration 76/1000 | Loss: 0.00001463
Iteration 77/1000 | Loss: 0.00001463
Iteration 78/1000 | Loss: 0.00001463
Iteration 79/1000 | Loss: 0.00001463
Iteration 80/1000 | Loss: 0.00001463
Iteration 81/1000 | Loss: 0.00001463
Iteration 82/1000 | Loss: 0.00001463
Iteration 83/1000 | Loss: 0.00001463
Iteration 84/1000 | Loss: 0.00001463
Iteration 85/1000 | Loss: 0.00001463
Iteration 86/1000 | Loss: 0.00001463
Iteration 87/1000 | Loss: 0.00001463
Iteration 88/1000 | Loss: 0.00001462
Iteration 89/1000 | Loss: 0.00001462
Iteration 90/1000 | Loss: 0.00001462
Iteration 91/1000 | Loss: 0.00001462
Iteration 92/1000 | Loss: 0.00001462
Iteration 93/1000 | Loss: 0.00001462
Iteration 94/1000 | Loss: 0.00001462
Iteration 95/1000 | Loss: 0.00001461
Iteration 96/1000 | Loss: 0.00001461
Iteration 97/1000 | Loss: 0.00001461
Iteration 98/1000 | Loss: 0.00001461
Iteration 99/1000 | Loss: 0.00001461
Iteration 100/1000 | Loss: 0.00001461
Iteration 101/1000 | Loss: 0.00001461
Iteration 102/1000 | Loss: 0.00001461
Iteration 103/1000 | Loss: 0.00001461
Iteration 104/1000 | Loss: 0.00001461
Iteration 105/1000 | Loss: 0.00001461
Iteration 106/1000 | Loss: 0.00001461
Iteration 107/1000 | Loss: 0.00001461
Iteration 108/1000 | Loss: 0.00001460
Iteration 109/1000 | Loss: 0.00001460
Iteration 110/1000 | Loss: 0.00001460
Iteration 111/1000 | Loss: 0.00001460
Iteration 112/1000 | Loss: 0.00001460
Iteration 113/1000 | Loss: 0.00001460
Iteration 114/1000 | Loss: 0.00001460
Iteration 115/1000 | Loss: 0.00001460
Iteration 116/1000 | Loss: 0.00001460
Iteration 117/1000 | Loss: 0.00001460
Iteration 118/1000 | Loss: 0.00001460
Iteration 119/1000 | Loss: 0.00001460
Iteration 120/1000 | Loss: 0.00001460
Iteration 121/1000 | Loss: 0.00001460
Iteration 122/1000 | Loss: 0.00001460
Iteration 123/1000 | Loss: 0.00001460
Iteration 124/1000 | Loss: 0.00001460
Iteration 125/1000 | Loss: 0.00001459
Iteration 126/1000 | Loss: 0.00001459
Iteration 127/1000 | Loss: 0.00001459
Iteration 128/1000 | Loss: 0.00001459
Iteration 129/1000 | Loss: 0.00001459
Iteration 130/1000 | Loss: 0.00001459
Iteration 131/1000 | Loss: 0.00001459
Iteration 132/1000 | Loss: 0.00001459
Iteration 133/1000 | Loss: 0.00001459
Iteration 134/1000 | Loss: 0.00001459
Iteration 135/1000 | Loss: 0.00001459
Iteration 136/1000 | Loss: 0.00001459
Iteration 137/1000 | Loss: 0.00001459
Iteration 138/1000 | Loss: 0.00001459
Iteration 139/1000 | Loss: 0.00001459
Iteration 140/1000 | Loss: 0.00001459
Iteration 141/1000 | Loss: 0.00001459
Iteration 142/1000 | Loss: 0.00001459
Iteration 143/1000 | Loss: 0.00001459
Iteration 144/1000 | Loss: 0.00001459
Iteration 145/1000 | Loss: 0.00001459
Iteration 146/1000 | Loss: 0.00001459
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 146. Stopping optimization.
Last 5 losses: [1.4588518752134405e-05, 1.4588518752134405e-05, 1.4588518752134405e-05, 1.4588518752134405e-05, 1.4588518752134405e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4588518752134405e-05

Optimization complete. Final v2v error: 3.26133131980896 mm

Highest mean error: 3.9200851917266846 mm for frame 45

Lowest mean error: 3.005213737487793 mm for frame 239

Saving results

Total time: 102.84722590446472
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_022/1020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_022/1020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_022/1020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01002430
Iteration 2/25 | Loss: 0.00188727
Iteration 3/25 | Loss: 0.00174631
Iteration 4/25 | Loss: 0.00138084
Iteration 5/25 | Loss: 0.00128192
Iteration 6/25 | Loss: 0.00130676
Iteration 7/25 | Loss: 0.00125642
Iteration 8/25 | Loss: 0.00124130
Iteration 9/25 | Loss: 0.00124433
Iteration 10/25 | Loss: 0.00121689
Iteration 11/25 | Loss: 0.00120530
Iteration 12/25 | Loss: 0.00122228
Iteration 13/25 | Loss: 0.00120928
Iteration 14/25 | Loss: 0.00117785
Iteration 15/25 | Loss: 0.00116293
Iteration 16/25 | Loss: 0.00115733
Iteration 17/25 | Loss: 0.00115779
Iteration 18/25 | Loss: 0.00115866
Iteration 19/25 | Loss: 0.00115729
Iteration 20/25 | Loss: 0.00115922
Iteration 21/25 | Loss: 0.00115894
Iteration 22/25 | Loss: 0.00116027
Iteration 23/25 | Loss: 0.00115820
Iteration 24/25 | Loss: 0.00115945
Iteration 25/25 | Loss: 0.00115768

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37126601
Iteration 2/25 | Loss: 0.00101200
Iteration 3/25 | Loss: 0.00101200
Iteration 4/25 | Loss: 0.00101200
Iteration 5/25 | Loss: 0.00101200
Iteration 6/25 | Loss: 0.00101200
Iteration 7/25 | Loss: 0.00101200
Iteration 8/25 | Loss: 0.00101200
Iteration 9/25 | Loss: 0.00101200
Iteration 10/25 | Loss: 0.00101200
Iteration 11/25 | Loss: 0.00101200
Iteration 12/25 | Loss: 0.00101200
Iteration 13/25 | Loss: 0.00101200
Iteration 14/25 | Loss: 0.00101200
Iteration 15/25 | Loss: 0.00101200
Iteration 16/25 | Loss: 0.00101200
Iteration 17/25 | Loss: 0.00101200
Iteration 18/25 | Loss: 0.00101200
Iteration 19/25 | Loss: 0.00101200
Iteration 20/25 | Loss: 0.00101200
Iteration 21/25 | Loss: 0.00101200
Iteration 22/25 | Loss: 0.00101200
Iteration 23/25 | Loss: 0.00101200
Iteration 24/25 | Loss: 0.00101200
Iteration 25/25 | Loss: 0.00101200

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00101200
Iteration 2/1000 | Loss: 0.00055979
Iteration 3/1000 | Loss: 0.00021900
Iteration 4/1000 | Loss: 0.00008178
Iteration 5/1000 | Loss: 0.00003887
Iteration 6/1000 | Loss: 0.00009241
Iteration 7/1000 | Loss: 0.00007257
Iteration 8/1000 | Loss: 0.00005340
Iteration 9/1000 | Loss: 0.00005438
Iteration 10/1000 | Loss: 0.00005204
Iteration 11/1000 | Loss: 0.00007472
Iteration 12/1000 | Loss: 0.00006829
Iteration 13/1000 | Loss: 0.00006806
Iteration 14/1000 | Loss: 0.00007180
Iteration 15/1000 | Loss: 0.00006671
Iteration 16/1000 | Loss: 0.00007616
Iteration 17/1000 | Loss: 0.00005875
Iteration 18/1000 | Loss: 0.00007877
Iteration 19/1000 | Loss: 0.00008500
Iteration 20/1000 | Loss: 0.00005803
Iteration 21/1000 | Loss: 0.00007350
Iteration 22/1000 | Loss: 0.00007310
Iteration 23/1000 | Loss: 0.00006591
Iteration 24/1000 | Loss: 0.00007387
Iteration 25/1000 | Loss: 0.00007084
Iteration 26/1000 | Loss: 0.00007620
Iteration 27/1000 | Loss: 0.00007542
Iteration 28/1000 | Loss: 0.00007547
Iteration 29/1000 | Loss: 0.00007535
Iteration 30/1000 | Loss: 0.00008480
Iteration 31/1000 | Loss: 0.00007937
Iteration 32/1000 | Loss: 0.00005541
Iteration 33/1000 | Loss: 0.00004734
Iteration 34/1000 | Loss: 0.00005389
Iteration 35/1000 | Loss: 0.00006191
Iteration 36/1000 | Loss: 0.00005882
Iteration 37/1000 | Loss: 0.00005430
Iteration 38/1000 | Loss: 0.00006539
Iteration 39/1000 | Loss: 0.00006529
Iteration 40/1000 | Loss: 0.00005810
Iteration 41/1000 | Loss: 0.00004832
Iteration 42/1000 | Loss: 0.00005768
Iteration 43/1000 | Loss: 0.00005539
Iteration 44/1000 | Loss: 0.00005754
Iteration 45/1000 | Loss: 0.00006090
Iteration 46/1000 | Loss: 0.00008891
Iteration 47/1000 | Loss: 0.00003270
Iteration 48/1000 | Loss: 0.00005448
Iteration 49/1000 | Loss: 0.00004523
Iteration 50/1000 | Loss: 0.00002367
Iteration 51/1000 | Loss: 0.00002091
Iteration 52/1000 | Loss: 0.00001984
Iteration 53/1000 | Loss: 0.00001942
Iteration 54/1000 | Loss: 0.00001903
Iteration 55/1000 | Loss: 0.00001873
Iteration 56/1000 | Loss: 0.00001848
Iteration 57/1000 | Loss: 0.00029430
Iteration 58/1000 | Loss: 0.00040310
Iteration 59/1000 | Loss: 0.00001880
Iteration 60/1000 | Loss: 0.00001617
Iteration 61/1000 | Loss: 0.00001521
Iteration 62/1000 | Loss: 0.00001421
Iteration 63/1000 | Loss: 0.00001360
Iteration 64/1000 | Loss: 0.00001318
Iteration 65/1000 | Loss: 0.00001287
Iteration 66/1000 | Loss: 0.00001265
Iteration 67/1000 | Loss: 0.00001250
Iteration 68/1000 | Loss: 0.00001248
Iteration 69/1000 | Loss: 0.00001246
Iteration 70/1000 | Loss: 0.00001246
Iteration 71/1000 | Loss: 0.00001246
Iteration 72/1000 | Loss: 0.00001245
Iteration 73/1000 | Loss: 0.00001244
Iteration 74/1000 | Loss: 0.00001244
Iteration 75/1000 | Loss: 0.00001243
Iteration 76/1000 | Loss: 0.00001242
Iteration 77/1000 | Loss: 0.00001239
Iteration 78/1000 | Loss: 0.00001238
Iteration 79/1000 | Loss: 0.00001237
Iteration 80/1000 | Loss: 0.00001237
Iteration 81/1000 | Loss: 0.00001236
Iteration 82/1000 | Loss: 0.00001236
Iteration 83/1000 | Loss: 0.00001235
Iteration 84/1000 | Loss: 0.00001235
Iteration 85/1000 | Loss: 0.00001234
Iteration 86/1000 | Loss: 0.00001234
Iteration 87/1000 | Loss: 0.00001233
Iteration 88/1000 | Loss: 0.00001233
Iteration 89/1000 | Loss: 0.00001233
Iteration 90/1000 | Loss: 0.00001233
Iteration 91/1000 | Loss: 0.00001233
Iteration 92/1000 | Loss: 0.00001232
Iteration 93/1000 | Loss: 0.00001232
Iteration 94/1000 | Loss: 0.00001232
Iteration 95/1000 | Loss: 0.00001232
Iteration 96/1000 | Loss: 0.00001232
Iteration 97/1000 | Loss: 0.00001231
Iteration 98/1000 | Loss: 0.00001231
Iteration 99/1000 | Loss: 0.00001231
Iteration 100/1000 | Loss: 0.00001230
Iteration 101/1000 | Loss: 0.00001230
Iteration 102/1000 | Loss: 0.00001230
Iteration 103/1000 | Loss: 0.00001230
Iteration 104/1000 | Loss: 0.00001230
Iteration 105/1000 | Loss: 0.00001230
Iteration 106/1000 | Loss: 0.00001229
Iteration 107/1000 | Loss: 0.00001229
Iteration 108/1000 | Loss: 0.00001229
Iteration 109/1000 | Loss: 0.00001229
Iteration 110/1000 | Loss: 0.00001229
Iteration 111/1000 | Loss: 0.00001229
Iteration 112/1000 | Loss: 0.00001228
Iteration 113/1000 | Loss: 0.00001228
Iteration 114/1000 | Loss: 0.00001228
Iteration 115/1000 | Loss: 0.00001228
Iteration 116/1000 | Loss: 0.00001228
Iteration 117/1000 | Loss: 0.00001227
Iteration 118/1000 | Loss: 0.00001227
Iteration 119/1000 | Loss: 0.00001227
Iteration 120/1000 | Loss: 0.00001227
Iteration 121/1000 | Loss: 0.00001227
Iteration 122/1000 | Loss: 0.00001227
Iteration 123/1000 | Loss: 0.00001227
Iteration 124/1000 | Loss: 0.00001226
Iteration 125/1000 | Loss: 0.00001226
Iteration 126/1000 | Loss: 0.00001226
Iteration 127/1000 | Loss: 0.00001226
Iteration 128/1000 | Loss: 0.00001226
Iteration 129/1000 | Loss: 0.00001226
Iteration 130/1000 | Loss: 0.00001226
Iteration 131/1000 | Loss: 0.00001226
Iteration 132/1000 | Loss: 0.00001226
Iteration 133/1000 | Loss: 0.00001226
Iteration 134/1000 | Loss: 0.00001226
Iteration 135/1000 | Loss: 0.00001226
Iteration 136/1000 | Loss: 0.00001226
Iteration 137/1000 | Loss: 0.00001226
Iteration 138/1000 | Loss: 0.00001226
Iteration 139/1000 | Loss: 0.00001226
Iteration 140/1000 | Loss: 0.00001225
Iteration 141/1000 | Loss: 0.00001225
Iteration 142/1000 | Loss: 0.00001225
Iteration 143/1000 | Loss: 0.00001225
Iteration 144/1000 | Loss: 0.00001225
Iteration 145/1000 | Loss: 0.00001225
Iteration 146/1000 | Loss: 0.00001225
Iteration 147/1000 | Loss: 0.00001225
Iteration 148/1000 | Loss: 0.00001225
Iteration 149/1000 | Loss: 0.00001225
Iteration 150/1000 | Loss: 0.00001225
Iteration 151/1000 | Loss: 0.00001224
Iteration 152/1000 | Loss: 0.00001224
Iteration 153/1000 | Loss: 0.00001224
Iteration 154/1000 | Loss: 0.00001224
Iteration 155/1000 | Loss: 0.00001224
Iteration 156/1000 | Loss: 0.00001224
Iteration 157/1000 | Loss: 0.00001224
Iteration 158/1000 | Loss: 0.00001224
Iteration 159/1000 | Loss: 0.00001224
Iteration 160/1000 | Loss: 0.00001224
Iteration 161/1000 | Loss: 0.00001224
Iteration 162/1000 | Loss: 0.00001224
Iteration 163/1000 | Loss: 0.00001224
Iteration 164/1000 | Loss: 0.00001224
Iteration 165/1000 | Loss: 0.00001224
Iteration 166/1000 | Loss: 0.00001224
Iteration 167/1000 | Loss: 0.00001224
Iteration 168/1000 | Loss: 0.00001224
Iteration 169/1000 | Loss: 0.00001224
Iteration 170/1000 | Loss: 0.00001224
Iteration 171/1000 | Loss: 0.00001224
Iteration 172/1000 | Loss: 0.00001224
Iteration 173/1000 | Loss: 0.00001224
Iteration 174/1000 | Loss: 0.00001224
Iteration 175/1000 | Loss: 0.00001224
Iteration 176/1000 | Loss: 0.00001224
Iteration 177/1000 | Loss: 0.00001224
Iteration 178/1000 | Loss: 0.00001224
Iteration 179/1000 | Loss: 0.00001224
Iteration 180/1000 | Loss: 0.00001224
Iteration 181/1000 | Loss: 0.00001224
Iteration 182/1000 | Loss: 0.00001224
Iteration 183/1000 | Loss: 0.00001224
Iteration 184/1000 | Loss: 0.00001224
Iteration 185/1000 | Loss: 0.00001224
Iteration 186/1000 | Loss: 0.00001224
Iteration 187/1000 | Loss: 0.00001224
Iteration 188/1000 | Loss: 0.00001224
Iteration 189/1000 | Loss: 0.00001224
Iteration 190/1000 | Loss: 0.00001224
Iteration 191/1000 | Loss: 0.00001224
Iteration 192/1000 | Loss: 0.00001224
Iteration 193/1000 | Loss: 0.00001224
Iteration 194/1000 | Loss: 0.00001224
Iteration 195/1000 | Loss: 0.00001224
Iteration 196/1000 | Loss: 0.00001224
Iteration 197/1000 | Loss: 0.00001224
Iteration 198/1000 | Loss: 0.00001224
Iteration 199/1000 | Loss: 0.00001224
Iteration 200/1000 | Loss: 0.00001224
Iteration 201/1000 | Loss: 0.00001224
Iteration 202/1000 | Loss: 0.00001224
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 202. Stopping optimization.
Last 5 losses: [1.2235064787091687e-05, 1.2235064787091687e-05, 1.2235064787091687e-05, 1.2235064787091687e-05, 1.2235064787091687e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2235064787091687e-05

Optimization complete. Final v2v error: 2.954690933227539 mm

Highest mean error: 4.199723243713379 mm for frame 67

Lowest mean error: 2.6502184867858887 mm for frame 0

Saving results

Total time: 151.43059730529785
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_022/1090/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_022/1090.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_022/1090
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00884598
Iteration 2/25 | Loss: 0.00145212
Iteration 3/25 | Loss: 0.00121341
Iteration 4/25 | Loss: 0.00119710
Iteration 5/25 | Loss: 0.00119012
Iteration 6/25 | Loss: 0.00119469
Iteration 7/25 | Loss: 0.00118659
Iteration 8/25 | Loss: 0.00118416
Iteration 9/25 | Loss: 0.00118380
Iteration 10/25 | Loss: 0.00118355
Iteration 11/25 | Loss: 0.00118341
Iteration 12/25 | Loss: 0.00118766
Iteration 13/25 | Loss: 0.00118705
Iteration 14/25 | Loss: 0.00118785
Iteration 15/25 | Loss: 0.00118771
Iteration 16/25 | Loss: 0.00118371
Iteration 17/25 | Loss: 0.00118343
Iteration 18/25 | Loss: 0.00118772
Iteration 19/25 | Loss: 0.00118235
Iteration 20/25 | Loss: 0.00118192
Iteration 21/25 | Loss: 0.00118178
Iteration 22/25 | Loss: 0.00118177
Iteration 23/25 | Loss: 0.00118177
Iteration 24/25 | Loss: 0.00118177
Iteration 25/25 | Loss: 0.00118176

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 13.34416008
Iteration 2/25 | Loss: 0.00068765
Iteration 3/25 | Loss: 0.00068754
Iteration 4/25 | Loss: 0.00068754
Iteration 5/25 | Loss: 0.00068754
Iteration 6/25 | Loss: 0.00068753
Iteration 7/25 | Loss: 0.00068753
Iteration 8/25 | Loss: 0.00068753
Iteration 9/25 | Loss: 0.00068753
Iteration 10/25 | Loss: 0.00068753
Iteration 11/25 | Loss: 0.00068753
Iteration 12/25 | Loss: 0.00068753
Iteration 13/25 | Loss: 0.00068753
Iteration 14/25 | Loss: 0.00068753
Iteration 15/25 | Loss: 0.00068753
Iteration 16/25 | Loss: 0.00068753
Iteration 17/25 | Loss: 0.00068753
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0006875328253954649, 0.0006875328253954649, 0.0006875328253954649, 0.0006875328253954649, 0.0006875328253954649]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006875328253954649

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00068753
Iteration 2/1000 | Loss: 0.00004208
Iteration 3/1000 | Loss: 0.00002867
Iteration 4/1000 | Loss: 0.00002542
Iteration 5/1000 | Loss: 0.00002408
Iteration 6/1000 | Loss: 0.00002300
Iteration 7/1000 | Loss: 0.00002235
Iteration 8/1000 | Loss: 0.00002176
Iteration 9/1000 | Loss: 0.00002135
Iteration 10/1000 | Loss: 0.00002102
Iteration 11/1000 | Loss: 0.00002082
Iteration 12/1000 | Loss: 0.00002077
Iteration 13/1000 | Loss: 0.00002062
Iteration 14/1000 | Loss: 0.00002061
Iteration 15/1000 | Loss: 0.00002058
Iteration 16/1000 | Loss: 0.00002054
Iteration 17/1000 | Loss: 0.00002053
Iteration 18/1000 | Loss: 0.00002051
Iteration 19/1000 | Loss: 0.00002051
Iteration 20/1000 | Loss: 0.00002050
Iteration 21/1000 | Loss: 0.00002050
Iteration 22/1000 | Loss: 0.00002049
Iteration 23/1000 | Loss: 0.00002049
Iteration 24/1000 | Loss: 0.00002048
Iteration 25/1000 | Loss: 0.00002048
Iteration 26/1000 | Loss: 0.00002047
Iteration 27/1000 | Loss: 0.00002046
Iteration 28/1000 | Loss: 0.00002046
Iteration 29/1000 | Loss: 0.00002046
Iteration 30/1000 | Loss: 0.00002045
Iteration 31/1000 | Loss: 0.00002045
Iteration 32/1000 | Loss: 0.00002043
Iteration 33/1000 | Loss: 0.00002041
Iteration 34/1000 | Loss: 0.00002041
Iteration 35/1000 | Loss: 0.00002041
Iteration 36/1000 | Loss: 0.00002040
Iteration 37/1000 | Loss: 0.00002040
Iteration 38/1000 | Loss: 0.00002040
Iteration 39/1000 | Loss: 0.00002039
Iteration 40/1000 | Loss: 0.00002039
Iteration 41/1000 | Loss: 0.00002039
Iteration 42/1000 | Loss: 0.00002039
Iteration 43/1000 | Loss: 0.00002039
Iteration 44/1000 | Loss: 0.00002039
Iteration 45/1000 | Loss: 0.00002038
Iteration 46/1000 | Loss: 0.00002038
Iteration 47/1000 | Loss: 0.00002038
Iteration 48/1000 | Loss: 0.00002038
Iteration 49/1000 | Loss: 0.00002038
Iteration 50/1000 | Loss: 0.00002038
Iteration 51/1000 | Loss: 0.00002038
Iteration 52/1000 | Loss: 0.00002038
Iteration 53/1000 | Loss: 0.00002038
Iteration 54/1000 | Loss: 0.00002038
Iteration 55/1000 | Loss: 0.00002037
Iteration 56/1000 | Loss: 0.00002037
Iteration 57/1000 | Loss: 0.00002037
Iteration 58/1000 | Loss: 0.00002037
Iteration 59/1000 | Loss: 0.00002037
Iteration 60/1000 | Loss: 0.00002037
Iteration 61/1000 | Loss: 0.00002036
Iteration 62/1000 | Loss: 0.00002036
Iteration 63/1000 | Loss: 0.00002036
Iteration 64/1000 | Loss: 0.00002036
Iteration 65/1000 | Loss: 0.00002036
Iteration 66/1000 | Loss: 0.00002036
Iteration 67/1000 | Loss: 0.00002036
Iteration 68/1000 | Loss: 0.00002036
Iteration 69/1000 | Loss: 0.00002036
Iteration 70/1000 | Loss: 0.00002035
Iteration 71/1000 | Loss: 0.00002035
Iteration 72/1000 | Loss: 0.00002035
Iteration 73/1000 | Loss: 0.00002034
Iteration 74/1000 | Loss: 0.00002034
Iteration 75/1000 | Loss: 0.00002033
Iteration 76/1000 | Loss: 0.00002033
Iteration 77/1000 | Loss: 0.00002033
Iteration 78/1000 | Loss: 0.00002033
Iteration 79/1000 | Loss: 0.00002033
Iteration 80/1000 | Loss: 0.00002033
Iteration 81/1000 | Loss: 0.00002032
Iteration 82/1000 | Loss: 0.00002032
Iteration 83/1000 | Loss: 0.00002032
Iteration 84/1000 | Loss: 0.00002032
Iteration 85/1000 | Loss: 0.00002031
Iteration 86/1000 | Loss: 0.00002031
Iteration 87/1000 | Loss: 0.00002031
Iteration 88/1000 | Loss: 0.00002030
Iteration 89/1000 | Loss: 0.00002030
Iteration 90/1000 | Loss: 0.00002030
Iteration 91/1000 | Loss: 0.00002029
Iteration 92/1000 | Loss: 0.00002029
Iteration 93/1000 | Loss: 0.00002029
Iteration 94/1000 | Loss: 0.00002029
Iteration 95/1000 | Loss: 0.00002029
Iteration 96/1000 | Loss: 0.00002029
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 96. Stopping optimization.
Last 5 losses: [2.0292023691581562e-05, 2.0292023691581562e-05, 2.0292023691581562e-05, 2.0292023691581562e-05, 2.0292023691581562e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0292023691581562e-05

Optimization complete. Final v2v error: 3.7627663612365723 mm

Highest mean error: 4.723332405090332 mm for frame 30

Lowest mean error: 3.1637752056121826 mm for frame 207

Saving results

Total time: 69.85543084144592
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_022/1018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_022/1018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_022/1018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00798178
Iteration 2/25 | Loss: 0.00139768
Iteration 3/25 | Loss: 0.00122926
Iteration 4/25 | Loss: 0.00120395
Iteration 5/25 | Loss: 0.00119447
Iteration 6/25 | Loss: 0.00119155
Iteration 7/25 | Loss: 0.00119103
Iteration 8/25 | Loss: 0.00119103
Iteration 9/25 | Loss: 0.00119103
Iteration 10/25 | Loss: 0.00119103
Iteration 11/25 | Loss: 0.00119103
Iteration 12/25 | Loss: 0.00119103
Iteration 13/25 | Loss: 0.00119103
Iteration 14/25 | Loss: 0.00119103
Iteration 15/25 | Loss: 0.00119103
Iteration 16/25 | Loss: 0.00119103
Iteration 17/25 | Loss: 0.00119103
Iteration 18/25 | Loss: 0.00119103
Iteration 19/25 | Loss: 0.00119103
Iteration 20/25 | Loss: 0.00119103
Iteration 21/25 | Loss: 0.00119103
Iteration 22/25 | Loss: 0.00119103
Iteration 23/25 | Loss: 0.00119103
Iteration 24/25 | Loss: 0.00119103
Iteration 25/25 | Loss: 0.00119103

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.08037972
Iteration 2/25 | Loss: 0.00117036
Iteration 3/25 | Loss: 0.00117036
Iteration 4/25 | Loss: 0.00117036
Iteration 5/25 | Loss: 0.00117036
Iteration 6/25 | Loss: 0.00117036
Iteration 7/25 | Loss: 0.00117036
Iteration 8/25 | Loss: 0.00117036
Iteration 9/25 | Loss: 0.00117036
Iteration 10/25 | Loss: 0.00117036
Iteration 11/25 | Loss: 0.00117036
Iteration 12/25 | Loss: 0.00117036
Iteration 13/25 | Loss: 0.00117036
Iteration 14/25 | Loss: 0.00117036
Iteration 15/25 | Loss: 0.00117036
Iteration 16/25 | Loss: 0.00117036
Iteration 17/25 | Loss: 0.00117036
Iteration 18/25 | Loss: 0.00117036
Iteration 19/25 | Loss: 0.00117036
Iteration 20/25 | Loss: 0.00117036
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0011703596683219075, 0.0011703596683219075, 0.0011703596683219075, 0.0011703596683219075, 0.0011703596683219075]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011703596683219075

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00117036
Iteration 2/1000 | Loss: 0.00004859
Iteration 3/1000 | Loss: 0.00002623
Iteration 4/1000 | Loss: 0.00002268
Iteration 5/1000 | Loss: 0.00002102
Iteration 6/1000 | Loss: 0.00002018
Iteration 7/1000 | Loss: 0.00001951
Iteration 8/1000 | Loss: 0.00001901
Iteration 9/1000 | Loss: 0.00001870
Iteration 10/1000 | Loss: 0.00001846
Iteration 11/1000 | Loss: 0.00001825
Iteration 12/1000 | Loss: 0.00001816
Iteration 13/1000 | Loss: 0.00001814
Iteration 14/1000 | Loss: 0.00001810
Iteration 15/1000 | Loss: 0.00001808
Iteration 16/1000 | Loss: 0.00001806
Iteration 17/1000 | Loss: 0.00001803
Iteration 18/1000 | Loss: 0.00001799
Iteration 19/1000 | Loss: 0.00001799
Iteration 20/1000 | Loss: 0.00001794
Iteration 21/1000 | Loss: 0.00001790
Iteration 22/1000 | Loss: 0.00001790
Iteration 23/1000 | Loss: 0.00001789
Iteration 24/1000 | Loss: 0.00001789
Iteration 25/1000 | Loss: 0.00001789
Iteration 26/1000 | Loss: 0.00001788
Iteration 27/1000 | Loss: 0.00001786
Iteration 28/1000 | Loss: 0.00001786
Iteration 29/1000 | Loss: 0.00001785
Iteration 30/1000 | Loss: 0.00001784
Iteration 31/1000 | Loss: 0.00001784
Iteration 32/1000 | Loss: 0.00001783
Iteration 33/1000 | Loss: 0.00001783
Iteration 34/1000 | Loss: 0.00001782
Iteration 35/1000 | Loss: 0.00001781
Iteration 36/1000 | Loss: 0.00001781
Iteration 37/1000 | Loss: 0.00001781
Iteration 38/1000 | Loss: 0.00001780
Iteration 39/1000 | Loss: 0.00001780
Iteration 40/1000 | Loss: 0.00001777
Iteration 41/1000 | Loss: 0.00001777
Iteration 42/1000 | Loss: 0.00001776
Iteration 43/1000 | Loss: 0.00001776
Iteration 44/1000 | Loss: 0.00001775
Iteration 45/1000 | Loss: 0.00001775
Iteration 46/1000 | Loss: 0.00001774
Iteration 47/1000 | Loss: 0.00001773
Iteration 48/1000 | Loss: 0.00001773
Iteration 49/1000 | Loss: 0.00001772
Iteration 50/1000 | Loss: 0.00001772
Iteration 51/1000 | Loss: 0.00001772
Iteration 52/1000 | Loss: 0.00001772
Iteration 53/1000 | Loss: 0.00001772
Iteration 54/1000 | Loss: 0.00001772
Iteration 55/1000 | Loss: 0.00001772
Iteration 56/1000 | Loss: 0.00001772
Iteration 57/1000 | Loss: 0.00001772
Iteration 58/1000 | Loss: 0.00001771
Iteration 59/1000 | Loss: 0.00001771
Iteration 60/1000 | Loss: 0.00001770
Iteration 61/1000 | Loss: 0.00001770
Iteration 62/1000 | Loss: 0.00001770
Iteration 63/1000 | Loss: 0.00001769
Iteration 64/1000 | Loss: 0.00001769
Iteration 65/1000 | Loss: 0.00001769
Iteration 66/1000 | Loss: 0.00001768
Iteration 67/1000 | Loss: 0.00001768
Iteration 68/1000 | Loss: 0.00001768
Iteration 69/1000 | Loss: 0.00001768
Iteration 70/1000 | Loss: 0.00001768
Iteration 71/1000 | Loss: 0.00001768
Iteration 72/1000 | Loss: 0.00001767
Iteration 73/1000 | Loss: 0.00001767
Iteration 74/1000 | Loss: 0.00001767
Iteration 75/1000 | Loss: 0.00001767
Iteration 76/1000 | Loss: 0.00001767
Iteration 77/1000 | Loss: 0.00001767
Iteration 78/1000 | Loss: 0.00001767
Iteration 79/1000 | Loss: 0.00001766
Iteration 80/1000 | Loss: 0.00001766
Iteration 81/1000 | Loss: 0.00001766
Iteration 82/1000 | Loss: 0.00001766
Iteration 83/1000 | Loss: 0.00001766
Iteration 84/1000 | Loss: 0.00001765
Iteration 85/1000 | Loss: 0.00001765
Iteration 86/1000 | Loss: 0.00001765
Iteration 87/1000 | Loss: 0.00001765
Iteration 88/1000 | Loss: 0.00001765
Iteration 89/1000 | Loss: 0.00001765
Iteration 90/1000 | Loss: 0.00001765
Iteration 91/1000 | Loss: 0.00001764
Iteration 92/1000 | Loss: 0.00001764
Iteration 93/1000 | Loss: 0.00001764
Iteration 94/1000 | Loss: 0.00001764
Iteration 95/1000 | Loss: 0.00001764
Iteration 96/1000 | Loss: 0.00001764
Iteration 97/1000 | Loss: 0.00001763
Iteration 98/1000 | Loss: 0.00001763
Iteration 99/1000 | Loss: 0.00001763
Iteration 100/1000 | Loss: 0.00001763
Iteration 101/1000 | Loss: 0.00001763
Iteration 102/1000 | Loss: 0.00001763
Iteration 103/1000 | Loss: 0.00001763
Iteration 104/1000 | Loss: 0.00001763
Iteration 105/1000 | Loss: 0.00001763
Iteration 106/1000 | Loss: 0.00001763
Iteration 107/1000 | Loss: 0.00001763
Iteration 108/1000 | Loss: 0.00001763
Iteration 109/1000 | Loss: 0.00001763
Iteration 110/1000 | Loss: 0.00001763
Iteration 111/1000 | Loss: 0.00001763
Iteration 112/1000 | Loss: 0.00001762
Iteration 113/1000 | Loss: 0.00001762
Iteration 114/1000 | Loss: 0.00001762
Iteration 115/1000 | Loss: 0.00001762
Iteration 116/1000 | Loss: 0.00001762
Iteration 117/1000 | Loss: 0.00001762
Iteration 118/1000 | Loss: 0.00001762
Iteration 119/1000 | Loss: 0.00001762
Iteration 120/1000 | Loss: 0.00001762
Iteration 121/1000 | Loss: 0.00001762
Iteration 122/1000 | Loss: 0.00001761
Iteration 123/1000 | Loss: 0.00001761
Iteration 124/1000 | Loss: 0.00001761
Iteration 125/1000 | Loss: 0.00001761
Iteration 126/1000 | Loss: 0.00001761
Iteration 127/1000 | Loss: 0.00001761
Iteration 128/1000 | Loss: 0.00001760
Iteration 129/1000 | Loss: 0.00001760
Iteration 130/1000 | Loss: 0.00001760
Iteration 131/1000 | Loss: 0.00001760
Iteration 132/1000 | Loss: 0.00001760
Iteration 133/1000 | Loss: 0.00001759
Iteration 134/1000 | Loss: 0.00001759
Iteration 135/1000 | Loss: 0.00001759
Iteration 136/1000 | Loss: 0.00001759
Iteration 137/1000 | Loss: 0.00001759
Iteration 138/1000 | Loss: 0.00001759
Iteration 139/1000 | Loss: 0.00001758
Iteration 140/1000 | Loss: 0.00001758
Iteration 141/1000 | Loss: 0.00001758
Iteration 142/1000 | Loss: 0.00001758
Iteration 143/1000 | Loss: 0.00001758
Iteration 144/1000 | Loss: 0.00001758
Iteration 145/1000 | Loss: 0.00001757
Iteration 146/1000 | Loss: 0.00001757
Iteration 147/1000 | Loss: 0.00001757
Iteration 148/1000 | Loss: 0.00001757
Iteration 149/1000 | Loss: 0.00001757
Iteration 150/1000 | Loss: 0.00001757
Iteration 151/1000 | Loss: 0.00001757
Iteration 152/1000 | Loss: 0.00001757
Iteration 153/1000 | Loss: 0.00001756
Iteration 154/1000 | Loss: 0.00001756
Iteration 155/1000 | Loss: 0.00001756
Iteration 156/1000 | Loss: 0.00001756
Iteration 157/1000 | Loss: 0.00001756
Iteration 158/1000 | Loss: 0.00001756
Iteration 159/1000 | Loss: 0.00001756
Iteration 160/1000 | Loss: 0.00001756
Iteration 161/1000 | Loss: 0.00001756
Iteration 162/1000 | Loss: 0.00001756
Iteration 163/1000 | Loss: 0.00001756
Iteration 164/1000 | Loss: 0.00001756
Iteration 165/1000 | Loss: 0.00001756
Iteration 166/1000 | Loss: 0.00001756
Iteration 167/1000 | Loss: 0.00001755
Iteration 168/1000 | Loss: 0.00001755
Iteration 169/1000 | Loss: 0.00001755
Iteration 170/1000 | Loss: 0.00001755
Iteration 171/1000 | Loss: 0.00001755
Iteration 172/1000 | Loss: 0.00001755
Iteration 173/1000 | Loss: 0.00001755
Iteration 174/1000 | Loss: 0.00001755
Iteration 175/1000 | Loss: 0.00001755
Iteration 176/1000 | Loss: 0.00001755
Iteration 177/1000 | Loss: 0.00001755
Iteration 178/1000 | Loss: 0.00001755
Iteration 179/1000 | Loss: 0.00001755
Iteration 180/1000 | Loss: 0.00001755
Iteration 181/1000 | Loss: 0.00001755
Iteration 182/1000 | Loss: 0.00001755
Iteration 183/1000 | Loss: 0.00001755
Iteration 184/1000 | Loss: 0.00001755
Iteration 185/1000 | Loss: 0.00001755
Iteration 186/1000 | Loss: 0.00001755
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 186. Stopping optimization.
Last 5 losses: [1.7546082744956948e-05, 1.7546082744956948e-05, 1.7546082744956948e-05, 1.7546082744956948e-05, 1.7546082744956948e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7546082744956948e-05

Optimization complete. Final v2v error: 3.5568902492523193 mm

Highest mean error: 4.212575435638428 mm for frame 108

Lowest mean error: 2.982017755508423 mm for frame 215

Saving results

Total time: 47.45476531982422
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_022/1034/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_022/1034.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_022/1034
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00815731
Iteration 2/25 | Loss: 0.00230326
Iteration 3/25 | Loss: 0.00179876
Iteration 4/25 | Loss: 0.00167075
Iteration 5/25 | Loss: 0.00142257
Iteration 6/25 | Loss: 0.00128598
Iteration 7/25 | Loss: 0.00127962
Iteration 8/25 | Loss: 0.00127871
Iteration 9/25 | Loss: 0.00127819
Iteration 10/25 | Loss: 0.00127769
Iteration 11/25 | Loss: 0.00127676
Iteration 12/25 | Loss: 0.00127626
Iteration 13/25 | Loss: 0.00127569
Iteration 14/25 | Loss: 0.00127546
Iteration 15/25 | Loss: 0.00127546
Iteration 16/25 | Loss: 0.00127546
Iteration 17/25 | Loss: 0.00127546
Iteration 18/25 | Loss: 0.00127546
Iteration 19/25 | Loss: 0.00127546
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0012754598865285516, 0.0012754598865285516, 0.0012754598865285516, 0.0012754598865285516, 0.0012754598865285516]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012754598865285516

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34639883
Iteration 2/25 | Loss: 0.00063502
Iteration 3/25 | Loss: 0.00063501
Iteration 4/25 | Loss: 0.00063501
Iteration 5/25 | Loss: 0.00063501
Iteration 6/25 | Loss: 0.00063501
Iteration 7/25 | Loss: 0.00063501
Iteration 8/25 | Loss: 0.00063501
Iteration 9/25 | Loss: 0.00063500
Iteration 10/25 | Loss: 0.00063500
Iteration 11/25 | Loss: 0.00063500
Iteration 12/25 | Loss: 0.00063500
Iteration 13/25 | Loss: 0.00063500
Iteration 14/25 | Loss: 0.00063500
Iteration 15/25 | Loss: 0.00063500
Iteration 16/25 | Loss: 0.00063500
Iteration 17/25 | Loss: 0.00063500
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0006350044859573245, 0.0006350044859573245, 0.0006350044859573245, 0.0006350044859573245, 0.0006350044859573245]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006350044859573245

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00063500
Iteration 2/1000 | Loss: 0.00003796
Iteration 3/1000 | Loss: 0.00002313
Iteration 4/1000 | Loss: 0.00002068
Iteration 5/1000 | Loss: 0.00001983
Iteration 6/1000 | Loss: 0.00001927
Iteration 7/1000 | Loss: 0.00001895
Iteration 8/1000 | Loss: 0.00001863
Iteration 9/1000 | Loss: 0.00001845
Iteration 10/1000 | Loss: 0.00001841
Iteration 11/1000 | Loss: 0.00001840
Iteration 12/1000 | Loss: 0.00001840
Iteration 13/1000 | Loss: 0.00001839
Iteration 14/1000 | Loss: 0.00001839
Iteration 15/1000 | Loss: 0.00001838
Iteration 16/1000 | Loss: 0.00001838
Iteration 17/1000 | Loss: 0.00001830
Iteration 18/1000 | Loss: 0.00001830
Iteration 19/1000 | Loss: 0.00001826
Iteration 20/1000 | Loss: 0.00001825
Iteration 21/1000 | Loss: 0.00001824
Iteration 22/1000 | Loss: 0.00001816
Iteration 23/1000 | Loss: 0.00001814
Iteration 24/1000 | Loss: 0.00001813
Iteration 25/1000 | Loss: 0.00001813
Iteration 26/1000 | Loss: 0.00001812
Iteration 27/1000 | Loss: 0.00001812
Iteration 28/1000 | Loss: 0.00001812
Iteration 29/1000 | Loss: 0.00001812
Iteration 30/1000 | Loss: 0.00001811
Iteration 31/1000 | Loss: 0.00001811
Iteration 32/1000 | Loss: 0.00001811
Iteration 33/1000 | Loss: 0.00001811
Iteration 34/1000 | Loss: 0.00001810
Iteration 35/1000 | Loss: 0.00001809
Iteration 36/1000 | Loss: 0.00001802
Iteration 37/1000 | Loss: 0.00001802
Iteration 38/1000 | Loss: 0.00001802
Iteration 39/1000 | Loss: 0.00001802
Iteration 40/1000 | Loss: 0.00001802
Iteration 41/1000 | Loss: 0.00001802
Iteration 42/1000 | Loss: 0.00001802
Iteration 43/1000 | Loss: 0.00001802
Iteration 44/1000 | Loss: 0.00001802
Iteration 45/1000 | Loss: 0.00001801
Iteration 46/1000 | Loss: 0.00001801
Iteration 47/1000 | Loss: 0.00001800
Iteration 48/1000 | Loss: 0.00001800
Iteration 49/1000 | Loss: 0.00001800
Iteration 50/1000 | Loss: 0.00001799
Iteration 51/1000 | Loss: 0.00001799
Iteration 52/1000 | Loss: 0.00001799
Iteration 53/1000 | Loss: 0.00001799
Iteration 54/1000 | Loss: 0.00001798
Iteration 55/1000 | Loss: 0.00001798
Iteration 56/1000 | Loss: 0.00001798
Iteration 57/1000 | Loss: 0.00001798
Iteration 58/1000 | Loss: 0.00001798
Iteration 59/1000 | Loss: 0.00001798
Iteration 60/1000 | Loss: 0.00001798
Iteration 61/1000 | Loss: 0.00001798
Iteration 62/1000 | Loss: 0.00001798
Iteration 63/1000 | Loss: 0.00001798
Iteration 64/1000 | Loss: 0.00001797
Iteration 65/1000 | Loss: 0.00001797
Iteration 66/1000 | Loss: 0.00001797
Iteration 67/1000 | Loss: 0.00001797
Iteration 68/1000 | Loss: 0.00001797
Iteration 69/1000 | Loss: 0.00001796
Iteration 70/1000 | Loss: 0.00001796
Iteration 71/1000 | Loss: 0.00001796
Iteration 72/1000 | Loss: 0.00001795
Iteration 73/1000 | Loss: 0.00001795
Iteration 74/1000 | Loss: 0.00001795
Iteration 75/1000 | Loss: 0.00001795
Iteration 76/1000 | Loss: 0.00001794
Iteration 77/1000 | Loss: 0.00001793
Iteration 78/1000 | Loss: 0.00001793
Iteration 79/1000 | Loss: 0.00001793
Iteration 80/1000 | Loss: 0.00001793
Iteration 81/1000 | Loss: 0.00001792
Iteration 82/1000 | Loss: 0.00001792
Iteration 83/1000 | Loss: 0.00001791
Iteration 84/1000 | Loss: 0.00001791
Iteration 85/1000 | Loss: 0.00001791
Iteration 86/1000 | Loss: 0.00001791
Iteration 87/1000 | Loss: 0.00001790
Iteration 88/1000 | Loss: 0.00001790
Iteration 89/1000 | Loss: 0.00001789
Iteration 90/1000 | Loss: 0.00001789
Iteration 91/1000 | Loss: 0.00001789
Iteration 92/1000 | Loss: 0.00001789
Iteration 93/1000 | Loss: 0.00001789
Iteration 94/1000 | Loss: 0.00001788
Iteration 95/1000 | Loss: 0.00001788
Iteration 96/1000 | Loss: 0.00001788
Iteration 97/1000 | Loss: 0.00001788
Iteration 98/1000 | Loss: 0.00001788
Iteration 99/1000 | Loss: 0.00001788
Iteration 100/1000 | Loss: 0.00001787
Iteration 101/1000 | Loss: 0.00001787
Iteration 102/1000 | Loss: 0.00001787
Iteration 103/1000 | Loss: 0.00001787
Iteration 104/1000 | Loss: 0.00001787
Iteration 105/1000 | Loss: 0.00001787
Iteration 106/1000 | Loss: 0.00001787
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 106. Stopping optimization.
Last 5 losses: [1.7874455807032064e-05, 1.7874455807032064e-05, 1.7874455807032064e-05, 1.7874455807032064e-05, 1.7874455807032064e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7874455807032064e-05

Optimization complete. Final v2v error: 3.615022659301758 mm

Highest mean error: 3.8174638748168945 mm for frame 220

Lowest mean error: 3.4322378635406494 mm for frame 33

Saving results

Total time: 51.16772818565369
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_022/1008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_022/1008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_022/1008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01004361
Iteration 2/25 | Loss: 0.00181350
Iteration 3/25 | Loss: 0.00155140
Iteration 4/25 | Loss: 0.00142481
Iteration 5/25 | Loss: 0.00132397
Iteration 6/25 | Loss: 0.00139942
Iteration 7/25 | Loss: 0.00131637
Iteration 8/25 | Loss: 0.00129953
Iteration 9/25 | Loss: 0.00145766
Iteration 10/25 | Loss: 0.00122943
Iteration 11/25 | Loss: 0.00119078
Iteration 12/25 | Loss: 0.00118340
Iteration 13/25 | Loss: 0.00118122
Iteration 14/25 | Loss: 0.00118996
Iteration 15/25 | Loss: 0.00116945
Iteration 16/25 | Loss: 0.00116661
Iteration 17/25 | Loss: 0.00115532
Iteration 18/25 | Loss: 0.00114831
Iteration 19/25 | Loss: 0.00113769
Iteration 20/25 | Loss: 0.00114269
Iteration 21/25 | Loss: 0.00113763
Iteration 22/25 | Loss: 0.00113601
Iteration 23/25 | Loss: 0.00113481
Iteration 24/25 | Loss: 0.00113017
Iteration 25/25 | Loss: 0.00113348

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36506569
Iteration 2/25 | Loss: 0.00121642
Iteration 3/25 | Loss: 0.00120177
Iteration 4/25 | Loss: 0.00120177
Iteration 5/25 | Loss: 0.00120177
Iteration 6/25 | Loss: 0.00120177
Iteration 7/25 | Loss: 0.00120177
Iteration 8/25 | Loss: 0.00120177
Iteration 9/25 | Loss: 0.00120176
Iteration 10/25 | Loss: 0.00120176
Iteration 11/25 | Loss: 0.00120176
Iteration 12/25 | Loss: 0.00120176
Iteration 13/25 | Loss: 0.00120176
Iteration 14/25 | Loss: 0.00120176
Iteration 15/25 | Loss: 0.00120176
Iteration 16/25 | Loss: 0.00120176
Iteration 17/25 | Loss: 0.00120176
Iteration 18/25 | Loss: 0.00120176
Iteration 19/25 | Loss: 0.00120176
Iteration 20/25 | Loss: 0.00120176
Iteration 21/25 | Loss: 0.00120176
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0012017630506306887, 0.0012017630506306887, 0.0012017630506306887, 0.0012017630506306887, 0.0012017630506306887]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012017630506306887

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00120176
Iteration 2/1000 | Loss: 0.00020616
Iteration 3/1000 | Loss: 0.00024851
Iteration 4/1000 | Loss: 0.00007416
Iteration 5/1000 | Loss: 0.00024269
Iteration 6/1000 | Loss: 0.00005379
Iteration 7/1000 | Loss: 0.00019498
Iteration 8/1000 | Loss: 0.00003727
Iteration 9/1000 | Loss: 0.00003075
Iteration 10/1000 | Loss: 0.00006519
Iteration 11/1000 | Loss: 0.00005017
Iteration 12/1000 | Loss: 0.00003365
Iteration 13/1000 | Loss: 0.00004233
Iteration 14/1000 | Loss: 0.00006000
Iteration 15/1000 | Loss: 0.00005516
Iteration 16/1000 | Loss: 0.00005642
Iteration 17/1000 | Loss: 0.00005857
Iteration 18/1000 | Loss: 0.00006752
Iteration 19/1000 | Loss: 0.00005562
Iteration 20/1000 | Loss: 0.00003899
Iteration 21/1000 | Loss: 0.00005385
Iteration 22/1000 | Loss: 0.00006145
Iteration 23/1000 | Loss: 0.00005731
Iteration 24/1000 | Loss: 0.00005860
Iteration 25/1000 | Loss: 0.00004437
Iteration 26/1000 | Loss: 0.00006001
Iteration 27/1000 | Loss: 0.00005160
Iteration 28/1000 | Loss: 0.00004666
Iteration 29/1000 | Loss: 0.00006119
Iteration 30/1000 | Loss: 0.00006049
Iteration 31/1000 | Loss: 0.00004967
Iteration 32/1000 | Loss: 0.00008243
Iteration 33/1000 | Loss: 0.00007963
Iteration 34/1000 | Loss: 0.00003518
Iteration 35/1000 | Loss: 0.00004058
Iteration 36/1000 | Loss: 0.00006094
Iteration 37/1000 | Loss: 0.00003319
Iteration 38/1000 | Loss: 0.00004186
Iteration 39/1000 | Loss: 0.00004619
Iteration 40/1000 | Loss: 0.00003491
Iteration 41/1000 | Loss: 0.00004149
Iteration 42/1000 | Loss: 0.00003343
Iteration 43/1000 | Loss: 0.00003651
Iteration 44/1000 | Loss: 0.00004100
Iteration 45/1000 | Loss: 0.00003725
Iteration 46/1000 | Loss: 0.00004024
Iteration 47/1000 | Loss: 0.00003548
Iteration 48/1000 | Loss: 0.00004100
Iteration 49/1000 | Loss: 0.00003722
Iteration 50/1000 | Loss: 0.00004135
Iteration 51/1000 | Loss: 0.00003645
Iteration 52/1000 | Loss: 0.00003986
Iteration 53/1000 | Loss: 0.00003604
Iteration 54/1000 | Loss: 0.00003902
Iteration 55/1000 | Loss: 0.00003581
Iteration 56/1000 | Loss: 0.00005005
Iteration 57/1000 | Loss: 0.00003435
Iteration 58/1000 | Loss: 0.00003759
Iteration 59/1000 | Loss: 0.00003307
Iteration 60/1000 | Loss: 0.00003560
Iteration 61/1000 | Loss: 0.00004045
Iteration 62/1000 | Loss: 0.00003558
Iteration 63/1000 | Loss: 0.00002228
Iteration 64/1000 | Loss: 0.00003987
Iteration 65/1000 | Loss: 0.00003680
Iteration 66/1000 | Loss: 0.00003870
Iteration 67/1000 | Loss: 0.00003194
Iteration 68/1000 | Loss: 0.00003977
Iteration 69/1000 | Loss: 0.00004067
Iteration 70/1000 | Loss: 0.00004027
Iteration 71/1000 | Loss: 0.00004388
Iteration 72/1000 | Loss: 0.00003977
Iteration 73/1000 | Loss: 0.00004337
Iteration 74/1000 | Loss: 0.00004855
Iteration 75/1000 | Loss: 0.00005984
Iteration 76/1000 | Loss: 0.00004153
Iteration 77/1000 | Loss: 0.00004070
Iteration 78/1000 | Loss: 0.00003876
Iteration 79/1000 | Loss: 0.00004172
Iteration 80/1000 | Loss: 0.00003869
Iteration 81/1000 | Loss: 0.00003103
Iteration 82/1000 | Loss: 0.00003128
Iteration 83/1000 | Loss: 0.00002089
Iteration 84/1000 | Loss: 0.00004259
Iteration 85/1000 | Loss: 0.00003636
Iteration 86/1000 | Loss: 0.00003936
Iteration 87/1000 | Loss: 0.00004364
Iteration 88/1000 | Loss: 0.00003910
Iteration 89/1000 | Loss: 0.00002894
Iteration 90/1000 | Loss: 0.00002678
Iteration 91/1000 | Loss: 0.00004062
Iteration 92/1000 | Loss: 0.00002883
Iteration 93/1000 | Loss: 0.00005886
Iteration 94/1000 | Loss: 0.00002621
Iteration 95/1000 | Loss: 0.00002692
Iteration 96/1000 | Loss: 0.00003836
Iteration 97/1000 | Loss: 0.00003746
Iteration 98/1000 | Loss: 0.00004144
Iteration 99/1000 | Loss: 0.00002872
Iteration 100/1000 | Loss: 0.00006051
Iteration 101/1000 | Loss: 0.00032729
Iteration 102/1000 | Loss: 0.00032727
Iteration 103/1000 | Loss: 0.00028877
Iteration 104/1000 | Loss: 0.00026996
Iteration 105/1000 | Loss: 0.00005170
Iteration 106/1000 | Loss: 0.00020194
Iteration 107/1000 | Loss: 0.00002554
Iteration 108/1000 | Loss: 0.00003200
Iteration 109/1000 | Loss: 0.00025566
Iteration 110/1000 | Loss: 0.00007775
Iteration 111/1000 | Loss: 0.00006185
Iteration 112/1000 | Loss: 0.00003591
Iteration 113/1000 | Loss: 0.00006656
Iteration 114/1000 | Loss: 0.00006327
Iteration 115/1000 | Loss: 0.00004902
Iteration 116/1000 | Loss: 0.00005415
Iteration 117/1000 | Loss: 0.00004286
Iteration 118/1000 | Loss: 0.00005288
Iteration 119/1000 | Loss: 0.00004136
Iteration 120/1000 | Loss: 0.00003950
Iteration 121/1000 | Loss: 0.00005673
Iteration 122/1000 | Loss: 0.00029892
Iteration 123/1000 | Loss: 0.00043144
Iteration 124/1000 | Loss: 0.00007912
Iteration 125/1000 | Loss: 0.00013699
Iteration 126/1000 | Loss: 0.00002252
Iteration 127/1000 | Loss: 0.00002051
Iteration 128/1000 | Loss: 0.00001899
Iteration 129/1000 | Loss: 0.00028849
Iteration 130/1000 | Loss: 0.00033807
Iteration 131/1000 | Loss: 0.00011683
Iteration 132/1000 | Loss: 0.00031764
Iteration 133/1000 | Loss: 0.00006238
Iteration 134/1000 | Loss: 0.00006210
Iteration 135/1000 | Loss: 0.00002815
Iteration 136/1000 | Loss: 0.00079219
Iteration 137/1000 | Loss: 0.00035820
Iteration 138/1000 | Loss: 0.00052421
Iteration 139/1000 | Loss: 0.00006651
Iteration 140/1000 | Loss: 0.00004138
Iteration 141/1000 | Loss: 0.00004567
Iteration 142/1000 | Loss: 0.00002101
Iteration 143/1000 | Loss: 0.00003222
Iteration 144/1000 | Loss: 0.00001884
Iteration 145/1000 | Loss: 0.00002339
Iteration 146/1000 | Loss: 0.00001763
Iteration 147/1000 | Loss: 0.00001707
Iteration 148/1000 | Loss: 0.00001666
Iteration 149/1000 | Loss: 0.00027392
Iteration 150/1000 | Loss: 0.00028721
Iteration 151/1000 | Loss: 0.00019293
Iteration 152/1000 | Loss: 0.00011094
Iteration 153/1000 | Loss: 0.00013616
Iteration 154/1000 | Loss: 0.00002351
Iteration 155/1000 | Loss: 0.00002316
Iteration 156/1000 | Loss: 0.00001646
Iteration 157/1000 | Loss: 0.00001583
Iteration 158/1000 | Loss: 0.00001536
Iteration 159/1000 | Loss: 0.00003990
Iteration 160/1000 | Loss: 0.00001494
Iteration 161/1000 | Loss: 0.00001481
Iteration 162/1000 | Loss: 0.00001481
Iteration 163/1000 | Loss: 0.00001480
Iteration 164/1000 | Loss: 0.00001477
Iteration 165/1000 | Loss: 0.00001476
Iteration 166/1000 | Loss: 0.00001475
Iteration 167/1000 | Loss: 0.00001475
Iteration 168/1000 | Loss: 0.00001475
Iteration 169/1000 | Loss: 0.00001475
Iteration 170/1000 | Loss: 0.00001475
Iteration 171/1000 | Loss: 0.00001474
Iteration 172/1000 | Loss: 0.00001474
Iteration 173/1000 | Loss: 0.00001473
Iteration 174/1000 | Loss: 0.00001471
Iteration 175/1000 | Loss: 0.00001469
Iteration 176/1000 | Loss: 0.00001469
Iteration 177/1000 | Loss: 0.00001469
Iteration 178/1000 | Loss: 0.00001468
Iteration 179/1000 | Loss: 0.00001468
Iteration 180/1000 | Loss: 0.00001468
Iteration 181/1000 | Loss: 0.00001468
Iteration 182/1000 | Loss: 0.00001468
Iteration 183/1000 | Loss: 0.00001468
Iteration 184/1000 | Loss: 0.00001467
Iteration 185/1000 | Loss: 0.00001467
Iteration 186/1000 | Loss: 0.00001466
Iteration 187/1000 | Loss: 0.00001466
Iteration 188/1000 | Loss: 0.00001466
Iteration 189/1000 | Loss: 0.00001466
Iteration 190/1000 | Loss: 0.00001466
Iteration 191/1000 | Loss: 0.00001466
Iteration 192/1000 | Loss: 0.00001466
Iteration 193/1000 | Loss: 0.00001466
Iteration 194/1000 | Loss: 0.00001466
Iteration 195/1000 | Loss: 0.00001466
Iteration 196/1000 | Loss: 0.00001466
Iteration 197/1000 | Loss: 0.00001465
Iteration 198/1000 | Loss: 0.00001465
Iteration 199/1000 | Loss: 0.00001465
Iteration 200/1000 | Loss: 0.00001465
Iteration 201/1000 | Loss: 0.00001464
Iteration 202/1000 | Loss: 0.00001464
Iteration 203/1000 | Loss: 0.00001464
Iteration 204/1000 | Loss: 0.00001464
Iteration 205/1000 | Loss: 0.00001464
Iteration 206/1000 | Loss: 0.00003271
Iteration 207/1000 | Loss: 0.00001461
Iteration 208/1000 | Loss: 0.00001460
Iteration 209/1000 | Loss: 0.00001460
Iteration 210/1000 | Loss: 0.00001460
Iteration 211/1000 | Loss: 0.00001460
Iteration 212/1000 | Loss: 0.00001460
Iteration 213/1000 | Loss: 0.00001460
Iteration 214/1000 | Loss: 0.00001460
Iteration 215/1000 | Loss: 0.00001460
Iteration 216/1000 | Loss: 0.00001460
Iteration 217/1000 | Loss: 0.00001460
Iteration 218/1000 | Loss: 0.00001460
Iteration 219/1000 | Loss: 0.00001460
Iteration 220/1000 | Loss: 0.00001460
Iteration 221/1000 | Loss: 0.00001460
Iteration 222/1000 | Loss: 0.00001460
Iteration 223/1000 | Loss: 0.00001459
Iteration 224/1000 | Loss: 0.00001459
Iteration 225/1000 | Loss: 0.00001459
Iteration 226/1000 | Loss: 0.00001459
Iteration 227/1000 | Loss: 0.00001459
Iteration 228/1000 | Loss: 0.00001459
Iteration 229/1000 | Loss: 0.00001458
Iteration 230/1000 | Loss: 0.00001458
Iteration 231/1000 | Loss: 0.00001458
Iteration 232/1000 | Loss: 0.00001458
Iteration 233/1000 | Loss: 0.00001458
Iteration 234/1000 | Loss: 0.00001458
Iteration 235/1000 | Loss: 0.00001458
Iteration 236/1000 | Loss: 0.00001458
Iteration 237/1000 | Loss: 0.00001458
Iteration 238/1000 | Loss: 0.00001458
Iteration 239/1000 | Loss: 0.00001458
Iteration 240/1000 | Loss: 0.00001457
Iteration 241/1000 | Loss: 0.00001457
Iteration 242/1000 | Loss: 0.00001457
Iteration 243/1000 | Loss: 0.00001457
Iteration 244/1000 | Loss: 0.00001457
Iteration 245/1000 | Loss: 0.00001457
Iteration 246/1000 | Loss: 0.00001456
Iteration 247/1000 | Loss: 0.00001456
Iteration 248/1000 | Loss: 0.00001456
Iteration 249/1000 | Loss: 0.00001456
Iteration 250/1000 | Loss: 0.00001456
Iteration 251/1000 | Loss: 0.00001456
Iteration 252/1000 | Loss: 0.00001456
Iteration 253/1000 | Loss: 0.00001456
Iteration 254/1000 | Loss: 0.00001456
Iteration 255/1000 | Loss: 0.00001456
Iteration 256/1000 | Loss: 0.00001456
Iteration 257/1000 | Loss: 0.00001456
Iteration 258/1000 | Loss: 0.00001456
Iteration 259/1000 | Loss: 0.00001456
Iteration 260/1000 | Loss: 0.00001455
Iteration 261/1000 | Loss: 0.00001455
Iteration 262/1000 | Loss: 0.00001455
Iteration 263/1000 | Loss: 0.00001455
Iteration 264/1000 | Loss: 0.00001455
Iteration 265/1000 | Loss: 0.00001455
Iteration 266/1000 | Loss: 0.00001455
Iteration 267/1000 | Loss: 0.00001455
Iteration 268/1000 | Loss: 0.00001455
Iteration 269/1000 | Loss: 0.00001455
Iteration 270/1000 | Loss: 0.00001455
Iteration 271/1000 | Loss: 0.00001454
Iteration 272/1000 | Loss: 0.00001454
Iteration 273/1000 | Loss: 0.00001454
Iteration 274/1000 | Loss: 0.00001454
Iteration 275/1000 | Loss: 0.00001454
Iteration 276/1000 | Loss: 0.00001454
Iteration 277/1000 | Loss: 0.00001454
Iteration 278/1000 | Loss: 0.00001454
Iteration 279/1000 | Loss: 0.00001454
Iteration 280/1000 | Loss: 0.00001453
Iteration 281/1000 | Loss: 0.00001453
Iteration 282/1000 | Loss: 0.00001453
Iteration 283/1000 | Loss: 0.00001453
Iteration 284/1000 | Loss: 0.00001453
Iteration 285/1000 | Loss: 0.00001453
Iteration 286/1000 | Loss: 0.00001453
Iteration 287/1000 | Loss: 0.00001453
Iteration 288/1000 | Loss: 0.00001453
Iteration 289/1000 | Loss: 0.00001453
Iteration 290/1000 | Loss: 0.00001453
Iteration 291/1000 | Loss: 0.00001453
Iteration 292/1000 | Loss: 0.00001452
Iteration 293/1000 | Loss: 0.00001452
Iteration 294/1000 | Loss: 0.00001452
Iteration 295/1000 | Loss: 0.00001452
Iteration 296/1000 | Loss: 0.00001452
Iteration 297/1000 | Loss: 0.00001452
Iteration 298/1000 | Loss: 0.00001452
Iteration 299/1000 | Loss: 0.00001452
Iteration 300/1000 | Loss: 0.00001452
Iteration 301/1000 | Loss: 0.00001452
Iteration 302/1000 | Loss: 0.00001451
Iteration 303/1000 | Loss: 0.00001451
Iteration 304/1000 | Loss: 0.00001451
Iteration 305/1000 | Loss: 0.00001451
Iteration 306/1000 | Loss: 0.00001451
Iteration 307/1000 | Loss: 0.00001451
Iteration 308/1000 | Loss: 0.00001451
Iteration 309/1000 | Loss: 0.00001451
Iteration 310/1000 | Loss: 0.00001451
Iteration 311/1000 | Loss: 0.00001451
Iteration 312/1000 | Loss: 0.00001451
Iteration 313/1000 | Loss: 0.00001451
Iteration 314/1000 | Loss: 0.00001451
Iteration 315/1000 | Loss: 0.00001451
Iteration 316/1000 | Loss: 0.00001450
Iteration 317/1000 | Loss: 0.00001450
Iteration 318/1000 | Loss: 0.00001450
Iteration 319/1000 | Loss: 0.00001450
Iteration 320/1000 | Loss: 0.00001450
Iteration 321/1000 | Loss: 0.00001450
Iteration 322/1000 | Loss: 0.00001450
Iteration 323/1000 | Loss: 0.00001450
Iteration 324/1000 | Loss: 0.00001450
Iteration 325/1000 | Loss: 0.00001450
Iteration 326/1000 | Loss: 0.00001450
Iteration 327/1000 | Loss: 0.00001450
Iteration 328/1000 | Loss: 0.00001450
Iteration 329/1000 | Loss: 0.00001449
Iteration 330/1000 | Loss: 0.00001449
Iteration 331/1000 | Loss: 0.00001449
Iteration 332/1000 | Loss: 0.00001449
Iteration 333/1000 | Loss: 0.00001449
Iteration 334/1000 | Loss: 0.00001449
Iteration 335/1000 | Loss: 0.00001449
Iteration 336/1000 | Loss: 0.00001449
Iteration 337/1000 | Loss: 0.00001449
Iteration 338/1000 | Loss: 0.00001449
Iteration 339/1000 | Loss: 0.00001449
Iteration 340/1000 | Loss: 0.00001449
Iteration 341/1000 | Loss: 0.00001449
Iteration 342/1000 | Loss: 0.00001449
Iteration 343/1000 | Loss: 0.00001449
Iteration 344/1000 | Loss: 0.00001448
Iteration 345/1000 | Loss: 0.00001448
Iteration 346/1000 | Loss: 0.00001448
Iteration 347/1000 | Loss: 0.00001448
Iteration 348/1000 | Loss: 0.00001448
Iteration 349/1000 | Loss: 0.00001448
Iteration 350/1000 | Loss: 0.00001448
Iteration 351/1000 | Loss: 0.00001448
Iteration 352/1000 | Loss: 0.00001448
Iteration 353/1000 | Loss: 0.00001448
Iteration 354/1000 | Loss: 0.00001448
Iteration 355/1000 | Loss: 0.00001448
Iteration 356/1000 | Loss: 0.00001448
Iteration 357/1000 | Loss: 0.00001448
Iteration 358/1000 | Loss: 0.00001448
Iteration 359/1000 | Loss: 0.00001448
Iteration 360/1000 | Loss: 0.00001448
Iteration 361/1000 | Loss: 0.00001448
Iteration 362/1000 | Loss: 0.00001448
Iteration 363/1000 | Loss: 0.00001448
Iteration 364/1000 | Loss: 0.00001448
Iteration 365/1000 | Loss: 0.00001448
Iteration 366/1000 | Loss: 0.00001448
Iteration 367/1000 | Loss: 0.00001447
Iteration 368/1000 | Loss: 0.00001447
Iteration 369/1000 | Loss: 0.00001447
Iteration 370/1000 | Loss: 0.00001447
Iteration 371/1000 | Loss: 0.00001447
Iteration 372/1000 | Loss: 0.00001447
Iteration 373/1000 | Loss: 0.00001447
Iteration 374/1000 | Loss: 0.00001447
Iteration 375/1000 | Loss: 0.00001447
Iteration 376/1000 | Loss: 0.00001447
Iteration 377/1000 | Loss: 0.00001447
Iteration 378/1000 | Loss: 0.00001447
Iteration 379/1000 | Loss: 0.00001447
Iteration 380/1000 | Loss: 0.00001447
Iteration 381/1000 | Loss: 0.00001447
Iteration 382/1000 | Loss: 0.00001447
Iteration 383/1000 | Loss: 0.00001447
Iteration 384/1000 | Loss: 0.00001447
Iteration 385/1000 | Loss: 0.00001447
Iteration 386/1000 | Loss: 0.00001447
Iteration 387/1000 | Loss: 0.00001447
Iteration 388/1000 | Loss: 0.00001447
Iteration 389/1000 | Loss: 0.00001447
Iteration 390/1000 | Loss: 0.00001447
Iteration 391/1000 | Loss: 0.00001447
Iteration 392/1000 | Loss: 0.00001447
Iteration 393/1000 | Loss: 0.00001447
Iteration 394/1000 | Loss: 0.00001447
Iteration 395/1000 | Loss: 0.00001447
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 395. Stopping optimization.
Last 5 losses: [1.4474952877208125e-05, 1.4474952877208125e-05, 1.4474952877208125e-05, 1.4474952877208125e-05, 1.4474952877208125e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4474952877208125e-05

Optimization complete. Final v2v error: 3.177333116531372 mm

Highest mean error: 5.01433801651001 mm for frame 56

Lowest mean error: 2.631197452545166 mm for frame 21

Saving results

Total time: 285.75735330581665
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_022/1052/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_022/1052.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_022/1052
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00793102
Iteration 2/25 | Loss: 0.00133298
Iteration 3/25 | Loss: 0.00113236
Iteration 4/25 | Loss: 0.00110285
Iteration 5/25 | Loss: 0.00109707
Iteration 6/25 | Loss: 0.00109544
Iteration 7/25 | Loss: 0.00109544
Iteration 8/25 | Loss: 0.00109544
Iteration 9/25 | Loss: 0.00109544
Iteration 10/25 | Loss: 0.00109544
Iteration 11/25 | Loss: 0.00109544
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010954374447464943, 0.0010954374447464943, 0.0010954374447464943, 0.0010954374447464943, 0.0010954374447464943]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010954374447464943

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.68026400
Iteration 2/25 | Loss: 0.00078622
Iteration 3/25 | Loss: 0.00078616
Iteration 4/25 | Loss: 0.00078615
Iteration 5/25 | Loss: 0.00078615
Iteration 6/25 | Loss: 0.00078615
Iteration 7/25 | Loss: 0.00078615
Iteration 8/25 | Loss: 0.00078615
Iteration 9/25 | Loss: 0.00078615
Iteration 10/25 | Loss: 0.00078615
Iteration 11/25 | Loss: 0.00078615
Iteration 12/25 | Loss: 0.00078615
Iteration 13/25 | Loss: 0.00078615
Iteration 14/25 | Loss: 0.00078615
Iteration 15/25 | Loss: 0.00078615
Iteration 16/25 | Loss: 0.00078615
Iteration 17/25 | Loss: 0.00078615
Iteration 18/25 | Loss: 0.00078615
Iteration 19/25 | Loss: 0.00078615
Iteration 20/25 | Loss: 0.00078615
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0007861508056521416, 0.0007861508056521416, 0.0007861508056521416, 0.0007861508056521416, 0.0007861508056521416]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007861508056521416

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00078615
Iteration 2/1000 | Loss: 0.00004033
Iteration 3/1000 | Loss: 0.00002599
Iteration 4/1000 | Loss: 0.00002224
Iteration 5/1000 | Loss: 0.00002090
Iteration 6/1000 | Loss: 0.00001991
Iteration 7/1000 | Loss: 0.00001923
Iteration 8/1000 | Loss: 0.00001875
Iteration 9/1000 | Loss: 0.00001846
Iteration 10/1000 | Loss: 0.00001815
Iteration 11/1000 | Loss: 0.00001788
Iteration 12/1000 | Loss: 0.00001783
Iteration 13/1000 | Loss: 0.00001778
Iteration 14/1000 | Loss: 0.00001773
Iteration 15/1000 | Loss: 0.00001771
Iteration 16/1000 | Loss: 0.00001770
Iteration 17/1000 | Loss: 0.00001768
Iteration 18/1000 | Loss: 0.00001764
Iteration 19/1000 | Loss: 0.00001762
Iteration 20/1000 | Loss: 0.00001750
Iteration 21/1000 | Loss: 0.00001745
Iteration 22/1000 | Loss: 0.00001745
Iteration 23/1000 | Loss: 0.00001744
Iteration 24/1000 | Loss: 0.00001744
Iteration 25/1000 | Loss: 0.00001743
Iteration 26/1000 | Loss: 0.00001742
Iteration 27/1000 | Loss: 0.00001741
Iteration 28/1000 | Loss: 0.00001739
Iteration 29/1000 | Loss: 0.00001736
Iteration 30/1000 | Loss: 0.00001734
Iteration 31/1000 | Loss: 0.00001733
Iteration 32/1000 | Loss: 0.00001731
Iteration 33/1000 | Loss: 0.00001731
Iteration 34/1000 | Loss: 0.00001730
Iteration 35/1000 | Loss: 0.00001727
Iteration 36/1000 | Loss: 0.00001725
Iteration 37/1000 | Loss: 0.00001724
Iteration 38/1000 | Loss: 0.00001721
Iteration 39/1000 | Loss: 0.00001718
Iteration 40/1000 | Loss: 0.00001718
Iteration 41/1000 | Loss: 0.00001717
Iteration 42/1000 | Loss: 0.00001717
Iteration 43/1000 | Loss: 0.00001717
Iteration 44/1000 | Loss: 0.00001716
Iteration 45/1000 | Loss: 0.00001716
Iteration 46/1000 | Loss: 0.00001716
Iteration 47/1000 | Loss: 0.00001716
Iteration 48/1000 | Loss: 0.00001716
Iteration 49/1000 | Loss: 0.00001715
Iteration 50/1000 | Loss: 0.00001715
Iteration 51/1000 | Loss: 0.00001715
Iteration 52/1000 | Loss: 0.00001715
Iteration 53/1000 | Loss: 0.00001714
Iteration 54/1000 | Loss: 0.00001714
Iteration 55/1000 | Loss: 0.00001714
Iteration 56/1000 | Loss: 0.00001714
Iteration 57/1000 | Loss: 0.00001714
Iteration 58/1000 | Loss: 0.00001714
Iteration 59/1000 | Loss: 0.00001713
Iteration 60/1000 | Loss: 0.00001713
Iteration 61/1000 | Loss: 0.00001712
Iteration 62/1000 | Loss: 0.00001712
Iteration 63/1000 | Loss: 0.00001712
Iteration 64/1000 | Loss: 0.00001712
Iteration 65/1000 | Loss: 0.00001712
Iteration 66/1000 | Loss: 0.00001712
Iteration 67/1000 | Loss: 0.00001712
Iteration 68/1000 | Loss: 0.00001712
Iteration 69/1000 | Loss: 0.00001712
Iteration 70/1000 | Loss: 0.00001711
Iteration 71/1000 | Loss: 0.00001711
Iteration 72/1000 | Loss: 0.00001711
Iteration 73/1000 | Loss: 0.00001711
Iteration 74/1000 | Loss: 0.00001711
Iteration 75/1000 | Loss: 0.00001711
Iteration 76/1000 | Loss: 0.00001711
Iteration 77/1000 | Loss: 0.00001711
Iteration 78/1000 | Loss: 0.00001710
Iteration 79/1000 | Loss: 0.00001710
Iteration 80/1000 | Loss: 0.00001710
Iteration 81/1000 | Loss: 0.00001710
Iteration 82/1000 | Loss: 0.00001710
Iteration 83/1000 | Loss: 0.00001710
Iteration 84/1000 | Loss: 0.00001710
Iteration 85/1000 | Loss: 0.00001710
Iteration 86/1000 | Loss: 0.00001710
Iteration 87/1000 | Loss: 0.00001710
Iteration 88/1000 | Loss: 0.00001709
Iteration 89/1000 | Loss: 0.00001709
Iteration 90/1000 | Loss: 0.00001709
Iteration 91/1000 | Loss: 0.00001709
Iteration 92/1000 | Loss: 0.00001709
Iteration 93/1000 | Loss: 0.00001709
Iteration 94/1000 | Loss: 0.00001708
Iteration 95/1000 | Loss: 0.00001708
Iteration 96/1000 | Loss: 0.00001708
Iteration 97/1000 | Loss: 0.00001708
Iteration 98/1000 | Loss: 0.00001707
Iteration 99/1000 | Loss: 0.00001707
Iteration 100/1000 | Loss: 0.00001707
Iteration 101/1000 | Loss: 0.00001707
Iteration 102/1000 | Loss: 0.00001707
Iteration 103/1000 | Loss: 0.00001707
Iteration 104/1000 | Loss: 0.00001707
Iteration 105/1000 | Loss: 0.00001707
Iteration 106/1000 | Loss: 0.00001707
Iteration 107/1000 | Loss: 0.00001707
Iteration 108/1000 | Loss: 0.00001706
Iteration 109/1000 | Loss: 0.00001706
Iteration 110/1000 | Loss: 0.00001706
Iteration 111/1000 | Loss: 0.00001706
Iteration 112/1000 | Loss: 0.00001706
Iteration 113/1000 | Loss: 0.00001706
Iteration 114/1000 | Loss: 0.00001706
Iteration 115/1000 | Loss: 0.00001706
Iteration 116/1000 | Loss: 0.00001705
Iteration 117/1000 | Loss: 0.00001705
Iteration 118/1000 | Loss: 0.00001705
Iteration 119/1000 | Loss: 0.00001705
Iteration 120/1000 | Loss: 0.00001705
Iteration 121/1000 | Loss: 0.00001705
Iteration 122/1000 | Loss: 0.00001704
Iteration 123/1000 | Loss: 0.00001704
Iteration 124/1000 | Loss: 0.00001704
Iteration 125/1000 | Loss: 0.00001704
Iteration 126/1000 | Loss: 0.00001704
Iteration 127/1000 | Loss: 0.00001704
Iteration 128/1000 | Loss: 0.00001704
Iteration 129/1000 | Loss: 0.00001704
Iteration 130/1000 | Loss: 0.00001704
Iteration 131/1000 | Loss: 0.00001704
Iteration 132/1000 | Loss: 0.00001704
Iteration 133/1000 | Loss: 0.00001704
Iteration 134/1000 | Loss: 0.00001704
Iteration 135/1000 | Loss: 0.00001704
Iteration 136/1000 | Loss: 0.00001704
Iteration 137/1000 | Loss: 0.00001704
Iteration 138/1000 | Loss: 0.00001704
Iteration 139/1000 | Loss: 0.00001704
Iteration 140/1000 | Loss: 0.00001704
Iteration 141/1000 | Loss: 0.00001704
Iteration 142/1000 | Loss: 0.00001704
Iteration 143/1000 | Loss: 0.00001704
Iteration 144/1000 | Loss: 0.00001704
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 144. Stopping optimization.
Last 5 losses: [1.7035814380506054e-05, 1.7035814380506054e-05, 1.7035814380506054e-05, 1.7035814380506054e-05, 1.7035814380506054e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7035814380506054e-05

Optimization complete. Final v2v error: 3.3880350589752197 mm

Highest mean error: 4.586743354797363 mm for frame 130

Lowest mean error: 2.5327467918395996 mm for frame 41

Saving results

Total time: 45.15251636505127
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_022/1029/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_022/1029.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_022/1029
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00794086
Iteration 2/25 | Loss: 0.00135191
Iteration 3/25 | Loss: 0.00110922
Iteration 4/25 | Loss: 0.00109442
Iteration 5/25 | Loss: 0.00108926
Iteration 6/25 | Loss: 0.00108790
Iteration 7/25 | Loss: 0.00108790
Iteration 8/25 | Loss: 0.00108790
Iteration 9/25 | Loss: 0.00108790
Iteration 10/25 | Loss: 0.00108790
Iteration 11/25 | Loss: 0.00108790
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010879004839807749, 0.0010879004839807749, 0.0010879004839807749, 0.0010879004839807749, 0.0010879004839807749]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010879004839807749

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.11317790
Iteration 2/25 | Loss: 0.00084231
Iteration 3/25 | Loss: 0.00084231
Iteration 4/25 | Loss: 0.00084231
Iteration 5/25 | Loss: 0.00084231
Iteration 6/25 | Loss: 0.00084230
Iteration 7/25 | Loss: 0.00084230
Iteration 8/25 | Loss: 0.00084230
Iteration 9/25 | Loss: 0.00084230
Iteration 10/25 | Loss: 0.00084230
Iteration 11/25 | Loss: 0.00084230
Iteration 12/25 | Loss: 0.00084230
Iteration 13/25 | Loss: 0.00084230
Iteration 14/25 | Loss: 0.00084230
Iteration 15/25 | Loss: 0.00084230
Iteration 16/25 | Loss: 0.00084230
Iteration 17/25 | Loss: 0.00084230
Iteration 18/25 | Loss: 0.00084230
Iteration 19/25 | Loss: 0.00084230
Iteration 20/25 | Loss: 0.00084230
Iteration 21/25 | Loss: 0.00084230
Iteration 22/25 | Loss: 0.00084230
Iteration 23/25 | Loss: 0.00084230
Iteration 24/25 | Loss: 0.00084230
Iteration 25/25 | Loss: 0.00084230

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00084230
Iteration 2/1000 | Loss: 0.00004020
Iteration 3/1000 | Loss: 0.00002739
Iteration 4/1000 | Loss: 0.00002169
Iteration 5/1000 | Loss: 0.00001924
Iteration 6/1000 | Loss: 0.00001770
Iteration 7/1000 | Loss: 0.00001681
Iteration 8/1000 | Loss: 0.00001612
Iteration 9/1000 | Loss: 0.00001559
Iteration 10/1000 | Loss: 0.00001521
Iteration 11/1000 | Loss: 0.00001484
Iteration 12/1000 | Loss: 0.00001453
Iteration 13/1000 | Loss: 0.00001434
Iteration 14/1000 | Loss: 0.00001430
Iteration 15/1000 | Loss: 0.00001416
Iteration 16/1000 | Loss: 0.00001412
Iteration 17/1000 | Loss: 0.00001412
Iteration 18/1000 | Loss: 0.00001410
Iteration 19/1000 | Loss: 0.00001398
Iteration 20/1000 | Loss: 0.00001391
Iteration 21/1000 | Loss: 0.00001384
Iteration 22/1000 | Loss: 0.00001383
Iteration 23/1000 | Loss: 0.00001382
Iteration 24/1000 | Loss: 0.00001381
Iteration 25/1000 | Loss: 0.00001380
Iteration 26/1000 | Loss: 0.00001379
Iteration 27/1000 | Loss: 0.00001378
Iteration 28/1000 | Loss: 0.00001378
Iteration 29/1000 | Loss: 0.00001377
Iteration 30/1000 | Loss: 0.00001377
Iteration 31/1000 | Loss: 0.00001377
Iteration 32/1000 | Loss: 0.00001376
Iteration 33/1000 | Loss: 0.00001376
Iteration 34/1000 | Loss: 0.00001376
Iteration 35/1000 | Loss: 0.00001376
Iteration 36/1000 | Loss: 0.00001376
Iteration 37/1000 | Loss: 0.00001375
Iteration 38/1000 | Loss: 0.00001375
Iteration 39/1000 | Loss: 0.00001375
Iteration 40/1000 | Loss: 0.00001375
Iteration 41/1000 | Loss: 0.00001375
Iteration 42/1000 | Loss: 0.00001374
Iteration 43/1000 | Loss: 0.00001374
Iteration 44/1000 | Loss: 0.00001374
Iteration 45/1000 | Loss: 0.00001373
Iteration 46/1000 | Loss: 0.00001373
Iteration 47/1000 | Loss: 0.00001373
Iteration 48/1000 | Loss: 0.00001372
Iteration 49/1000 | Loss: 0.00001372
Iteration 50/1000 | Loss: 0.00001372
Iteration 51/1000 | Loss: 0.00001372
Iteration 52/1000 | Loss: 0.00001371
Iteration 53/1000 | Loss: 0.00001371
Iteration 54/1000 | Loss: 0.00001371
Iteration 55/1000 | Loss: 0.00001371
Iteration 56/1000 | Loss: 0.00001370
Iteration 57/1000 | Loss: 0.00001369
Iteration 58/1000 | Loss: 0.00001369
Iteration 59/1000 | Loss: 0.00001367
Iteration 60/1000 | Loss: 0.00001364
Iteration 61/1000 | Loss: 0.00001364
Iteration 62/1000 | Loss: 0.00001364
Iteration 63/1000 | Loss: 0.00001364
Iteration 64/1000 | Loss: 0.00001364
Iteration 65/1000 | Loss: 0.00001362
Iteration 66/1000 | Loss: 0.00001361
Iteration 67/1000 | Loss: 0.00001361
Iteration 68/1000 | Loss: 0.00001359
Iteration 69/1000 | Loss: 0.00001359
Iteration 70/1000 | Loss: 0.00001358
Iteration 71/1000 | Loss: 0.00001358
Iteration 72/1000 | Loss: 0.00001358
Iteration 73/1000 | Loss: 0.00001358
Iteration 74/1000 | Loss: 0.00001358
Iteration 75/1000 | Loss: 0.00001357
Iteration 76/1000 | Loss: 0.00001357
Iteration 77/1000 | Loss: 0.00001357
Iteration 78/1000 | Loss: 0.00001357
Iteration 79/1000 | Loss: 0.00001357
Iteration 80/1000 | Loss: 0.00001357
Iteration 81/1000 | Loss: 0.00001357
Iteration 82/1000 | Loss: 0.00001356
Iteration 83/1000 | Loss: 0.00001356
Iteration 84/1000 | Loss: 0.00001356
Iteration 85/1000 | Loss: 0.00001355
Iteration 86/1000 | Loss: 0.00001355
Iteration 87/1000 | Loss: 0.00001354
Iteration 88/1000 | Loss: 0.00001354
Iteration 89/1000 | Loss: 0.00001354
Iteration 90/1000 | Loss: 0.00001353
Iteration 91/1000 | Loss: 0.00001353
Iteration 92/1000 | Loss: 0.00001353
Iteration 93/1000 | Loss: 0.00001353
Iteration 94/1000 | Loss: 0.00001352
Iteration 95/1000 | Loss: 0.00001352
Iteration 96/1000 | Loss: 0.00001352
Iteration 97/1000 | Loss: 0.00001352
Iteration 98/1000 | Loss: 0.00001352
Iteration 99/1000 | Loss: 0.00001351
Iteration 100/1000 | Loss: 0.00001351
Iteration 101/1000 | Loss: 0.00001351
Iteration 102/1000 | Loss: 0.00001351
Iteration 103/1000 | Loss: 0.00001351
Iteration 104/1000 | Loss: 0.00001351
Iteration 105/1000 | Loss: 0.00001350
Iteration 106/1000 | Loss: 0.00001350
Iteration 107/1000 | Loss: 0.00001350
Iteration 108/1000 | Loss: 0.00001350
Iteration 109/1000 | Loss: 0.00001350
Iteration 110/1000 | Loss: 0.00001350
Iteration 111/1000 | Loss: 0.00001350
Iteration 112/1000 | Loss: 0.00001350
Iteration 113/1000 | Loss: 0.00001350
Iteration 114/1000 | Loss: 0.00001350
Iteration 115/1000 | Loss: 0.00001350
Iteration 116/1000 | Loss: 0.00001350
Iteration 117/1000 | Loss: 0.00001349
Iteration 118/1000 | Loss: 0.00001349
Iteration 119/1000 | Loss: 0.00001349
Iteration 120/1000 | Loss: 0.00001348
Iteration 121/1000 | Loss: 0.00001348
Iteration 122/1000 | Loss: 0.00001348
Iteration 123/1000 | Loss: 0.00001348
Iteration 124/1000 | Loss: 0.00001348
Iteration 125/1000 | Loss: 0.00001347
Iteration 126/1000 | Loss: 0.00001347
Iteration 127/1000 | Loss: 0.00001347
Iteration 128/1000 | Loss: 0.00001346
Iteration 129/1000 | Loss: 0.00001346
Iteration 130/1000 | Loss: 0.00001346
Iteration 131/1000 | Loss: 0.00001346
Iteration 132/1000 | Loss: 0.00001346
Iteration 133/1000 | Loss: 0.00001346
Iteration 134/1000 | Loss: 0.00001346
Iteration 135/1000 | Loss: 0.00001345
Iteration 136/1000 | Loss: 0.00001345
Iteration 137/1000 | Loss: 0.00001345
Iteration 138/1000 | Loss: 0.00001345
Iteration 139/1000 | Loss: 0.00001345
Iteration 140/1000 | Loss: 0.00001345
Iteration 141/1000 | Loss: 0.00001344
Iteration 142/1000 | Loss: 0.00001344
Iteration 143/1000 | Loss: 0.00001344
Iteration 144/1000 | Loss: 0.00001344
Iteration 145/1000 | Loss: 0.00001344
Iteration 146/1000 | Loss: 0.00001344
Iteration 147/1000 | Loss: 0.00001344
Iteration 148/1000 | Loss: 0.00001344
Iteration 149/1000 | Loss: 0.00001343
Iteration 150/1000 | Loss: 0.00001343
Iteration 151/1000 | Loss: 0.00001343
Iteration 152/1000 | Loss: 0.00001343
Iteration 153/1000 | Loss: 0.00001342
Iteration 154/1000 | Loss: 0.00001342
Iteration 155/1000 | Loss: 0.00001342
Iteration 156/1000 | Loss: 0.00001342
Iteration 157/1000 | Loss: 0.00001342
Iteration 158/1000 | Loss: 0.00001342
Iteration 159/1000 | Loss: 0.00001342
Iteration 160/1000 | Loss: 0.00001342
Iteration 161/1000 | Loss: 0.00001342
Iteration 162/1000 | Loss: 0.00001341
Iteration 163/1000 | Loss: 0.00001341
Iteration 164/1000 | Loss: 0.00001341
Iteration 165/1000 | Loss: 0.00001341
Iteration 166/1000 | Loss: 0.00001341
Iteration 167/1000 | Loss: 0.00001341
Iteration 168/1000 | Loss: 0.00001341
Iteration 169/1000 | Loss: 0.00001341
Iteration 170/1000 | Loss: 0.00001340
Iteration 171/1000 | Loss: 0.00001340
Iteration 172/1000 | Loss: 0.00001340
Iteration 173/1000 | Loss: 0.00001340
Iteration 174/1000 | Loss: 0.00001340
Iteration 175/1000 | Loss: 0.00001340
Iteration 176/1000 | Loss: 0.00001340
Iteration 177/1000 | Loss: 0.00001340
Iteration 178/1000 | Loss: 0.00001340
Iteration 179/1000 | Loss: 0.00001340
Iteration 180/1000 | Loss: 0.00001340
Iteration 181/1000 | Loss: 0.00001340
Iteration 182/1000 | Loss: 0.00001340
Iteration 183/1000 | Loss: 0.00001340
Iteration 184/1000 | Loss: 0.00001340
Iteration 185/1000 | Loss: 0.00001340
Iteration 186/1000 | Loss: 0.00001340
Iteration 187/1000 | Loss: 0.00001340
Iteration 188/1000 | Loss: 0.00001340
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 188. Stopping optimization.
Last 5 losses: [1.3401412616076414e-05, 1.3401412616076414e-05, 1.3401412616076414e-05, 1.3401412616076414e-05, 1.3401412616076414e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3401412616076414e-05

Optimization complete. Final v2v error: 2.9815726280212402 mm

Highest mean error: 4.140254497528076 mm for frame 82

Lowest mean error: 2.283766746520996 mm for frame 196

Saving results

Total time: 52.03049945831299
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_022/1006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_022/1006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_022/1006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00362185
Iteration 2/25 | Loss: 0.00111016
Iteration 3/25 | Loss: 0.00104594
Iteration 4/25 | Loss: 0.00103633
Iteration 5/25 | Loss: 0.00103391
Iteration 6/25 | Loss: 0.00103391
Iteration 7/25 | Loss: 0.00103391
Iteration 8/25 | Loss: 0.00103391
Iteration 9/25 | Loss: 0.00103391
Iteration 10/25 | Loss: 0.00103391
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0010339085711166263, 0.0010339085711166263, 0.0010339085711166263, 0.0010339085711166263, 0.0010339085711166263]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010339085711166263

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36865640
Iteration 2/25 | Loss: 0.00077558
Iteration 3/25 | Loss: 0.00077558
Iteration 4/25 | Loss: 0.00077558
Iteration 5/25 | Loss: 0.00077558
Iteration 6/25 | Loss: 0.00077558
Iteration 7/25 | Loss: 0.00077558
Iteration 8/25 | Loss: 0.00077558
Iteration 9/25 | Loss: 0.00077558
Iteration 10/25 | Loss: 0.00077558
Iteration 11/25 | Loss: 0.00077558
Iteration 12/25 | Loss: 0.00077558
Iteration 13/25 | Loss: 0.00077558
Iteration 14/25 | Loss: 0.00077558
Iteration 15/25 | Loss: 0.00077558
Iteration 16/25 | Loss: 0.00077558
Iteration 17/25 | Loss: 0.00077558
Iteration 18/25 | Loss: 0.00077558
Iteration 19/25 | Loss: 0.00077558
Iteration 20/25 | Loss: 0.00077558
Iteration 21/25 | Loss: 0.00077558
Iteration 22/25 | Loss: 0.00077558
Iteration 23/25 | Loss: 0.00077558
Iteration 24/25 | Loss: 0.00077558
Iteration 25/25 | Loss: 0.00077558

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00077558
Iteration 2/1000 | Loss: 0.00001551
Iteration 3/1000 | Loss: 0.00001100
Iteration 4/1000 | Loss: 0.00001011
Iteration 5/1000 | Loss: 0.00000949
Iteration 6/1000 | Loss: 0.00000908
Iteration 7/1000 | Loss: 0.00000901
Iteration 8/1000 | Loss: 0.00000898
Iteration 9/1000 | Loss: 0.00000877
Iteration 10/1000 | Loss: 0.00000855
Iteration 11/1000 | Loss: 0.00000843
Iteration 12/1000 | Loss: 0.00000843
Iteration 13/1000 | Loss: 0.00000842
Iteration 14/1000 | Loss: 0.00000841
Iteration 15/1000 | Loss: 0.00000840
Iteration 16/1000 | Loss: 0.00000836
Iteration 17/1000 | Loss: 0.00000835
Iteration 18/1000 | Loss: 0.00000830
Iteration 19/1000 | Loss: 0.00000830
Iteration 20/1000 | Loss: 0.00000829
Iteration 21/1000 | Loss: 0.00000824
Iteration 22/1000 | Loss: 0.00000820
Iteration 23/1000 | Loss: 0.00000819
Iteration 24/1000 | Loss: 0.00000818
Iteration 25/1000 | Loss: 0.00000817
Iteration 26/1000 | Loss: 0.00000816
Iteration 27/1000 | Loss: 0.00000811
Iteration 28/1000 | Loss: 0.00000805
Iteration 29/1000 | Loss: 0.00000805
Iteration 30/1000 | Loss: 0.00000805
Iteration 31/1000 | Loss: 0.00000805
Iteration 32/1000 | Loss: 0.00000804
Iteration 33/1000 | Loss: 0.00000804
Iteration 34/1000 | Loss: 0.00000803
Iteration 35/1000 | Loss: 0.00000802
Iteration 36/1000 | Loss: 0.00000801
Iteration 37/1000 | Loss: 0.00000801
Iteration 38/1000 | Loss: 0.00000801
Iteration 39/1000 | Loss: 0.00000800
Iteration 40/1000 | Loss: 0.00000800
Iteration 41/1000 | Loss: 0.00000800
Iteration 42/1000 | Loss: 0.00000799
Iteration 43/1000 | Loss: 0.00000799
Iteration 44/1000 | Loss: 0.00000799
Iteration 45/1000 | Loss: 0.00000799
Iteration 46/1000 | Loss: 0.00000798
Iteration 47/1000 | Loss: 0.00000798
Iteration 48/1000 | Loss: 0.00000798
Iteration 49/1000 | Loss: 0.00000798
Iteration 50/1000 | Loss: 0.00000798
Iteration 51/1000 | Loss: 0.00000797
Iteration 52/1000 | Loss: 0.00000797
Iteration 53/1000 | Loss: 0.00000797
Iteration 54/1000 | Loss: 0.00000797
Iteration 55/1000 | Loss: 0.00000797
Iteration 56/1000 | Loss: 0.00000797
Iteration 57/1000 | Loss: 0.00000796
Iteration 58/1000 | Loss: 0.00000795
Iteration 59/1000 | Loss: 0.00000794
Iteration 60/1000 | Loss: 0.00000794
Iteration 61/1000 | Loss: 0.00000794
Iteration 62/1000 | Loss: 0.00000794
Iteration 63/1000 | Loss: 0.00000794
Iteration 64/1000 | Loss: 0.00000794
Iteration 65/1000 | Loss: 0.00000794
Iteration 66/1000 | Loss: 0.00000793
Iteration 67/1000 | Loss: 0.00000793
Iteration 68/1000 | Loss: 0.00000791
Iteration 69/1000 | Loss: 0.00000790
Iteration 70/1000 | Loss: 0.00000789
Iteration 71/1000 | Loss: 0.00000789
Iteration 72/1000 | Loss: 0.00000789
Iteration 73/1000 | Loss: 0.00000788
Iteration 74/1000 | Loss: 0.00000788
Iteration 75/1000 | Loss: 0.00000788
Iteration 76/1000 | Loss: 0.00000788
Iteration 77/1000 | Loss: 0.00000787
Iteration 78/1000 | Loss: 0.00000787
Iteration 79/1000 | Loss: 0.00000786
Iteration 80/1000 | Loss: 0.00000786
Iteration 81/1000 | Loss: 0.00000786
Iteration 82/1000 | Loss: 0.00000786
Iteration 83/1000 | Loss: 0.00000785
Iteration 84/1000 | Loss: 0.00000784
Iteration 85/1000 | Loss: 0.00000784
Iteration 86/1000 | Loss: 0.00000783
Iteration 87/1000 | Loss: 0.00000783
Iteration 88/1000 | Loss: 0.00000782
Iteration 89/1000 | Loss: 0.00000782
Iteration 90/1000 | Loss: 0.00000782
Iteration 91/1000 | Loss: 0.00000781
Iteration 92/1000 | Loss: 0.00000781
Iteration 93/1000 | Loss: 0.00000781
Iteration 94/1000 | Loss: 0.00000781
Iteration 95/1000 | Loss: 0.00000781
Iteration 96/1000 | Loss: 0.00000781
Iteration 97/1000 | Loss: 0.00000781
Iteration 98/1000 | Loss: 0.00000781
Iteration 99/1000 | Loss: 0.00000781
Iteration 100/1000 | Loss: 0.00000781
Iteration 101/1000 | Loss: 0.00000781
Iteration 102/1000 | Loss: 0.00000780
Iteration 103/1000 | Loss: 0.00000780
Iteration 104/1000 | Loss: 0.00000780
Iteration 105/1000 | Loss: 0.00000780
Iteration 106/1000 | Loss: 0.00000779
Iteration 107/1000 | Loss: 0.00000779
Iteration 108/1000 | Loss: 0.00000779
Iteration 109/1000 | Loss: 0.00000778
Iteration 110/1000 | Loss: 0.00000778
Iteration 111/1000 | Loss: 0.00000778
Iteration 112/1000 | Loss: 0.00000777
Iteration 113/1000 | Loss: 0.00000777
Iteration 114/1000 | Loss: 0.00000777
Iteration 115/1000 | Loss: 0.00000776
Iteration 116/1000 | Loss: 0.00000776
Iteration 117/1000 | Loss: 0.00000776
Iteration 118/1000 | Loss: 0.00000776
Iteration 119/1000 | Loss: 0.00000776
Iteration 120/1000 | Loss: 0.00000776
Iteration 121/1000 | Loss: 0.00000775
Iteration 122/1000 | Loss: 0.00000775
Iteration 123/1000 | Loss: 0.00000775
Iteration 124/1000 | Loss: 0.00000774
Iteration 125/1000 | Loss: 0.00000774
Iteration 126/1000 | Loss: 0.00000774
Iteration 127/1000 | Loss: 0.00000774
Iteration 128/1000 | Loss: 0.00000774
Iteration 129/1000 | Loss: 0.00000774
Iteration 130/1000 | Loss: 0.00000774
Iteration 131/1000 | Loss: 0.00000774
Iteration 132/1000 | Loss: 0.00000774
Iteration 133/1000 | Loss: 0.00000774
Iteration 134/1000 | Loss: 0.00000774
Iteration 135/1000 | Loss: 0.00000774
Iteration 136/1000 | Loss: 0.00000774
Iteration 137/1000 | Loss: 0.00000773
Iteration 138/1000 | Loss: 0.00000773
Iteration 139/1000 | Loss: 0.00000773
Iteration 140/1000 | Loss: 0.00000772
Iteration 141/1000 | Loss: 0.00000772
Iteration 142/1000 | Loss: 0.00000771
Iteration 143/1000 | Loss: 0.00000771
Iteration 144/1000 | Loss: 0.00000770
Iteration 145/1000 | Loss: 0.00000770
Iteration 146/1000 | Loss: 0.00000770
Iteration 147/1000 | Loss: 0.00000770
Iteration 148/1000 | Loss: 0.00000769
Iteration 149/1000 | Loss: 0.00000769
Iteration 150/1000 | Loss: 0.00000769
Iteration 151/1000 | Loss: 0.00000769
Iteration 152/1000 | Loss: 0.00000769
Iteration 153/1000 | Loss: 0.00000769
Iteration 154/1000 | Loss: 0.00000769
Iteration 155/1000 | Loss: 0.00000769
Iteration 156/1000 | Loss: 0.00000769
Iteration 157/1000 | Loss: 0.00000769
Iteration 158/1000 | Loss: 0.00000769
Iteration 159/1000 | Loss: 0.00000769
Iteration 160/1000 | Loss: 0.00000769
Iteration 161/1000 | Loss: 0.00000769
Iteration 162/1000 | Loss: 0.00000769
Iteration 163/1000 | Loss: 0.00000769
Iteration 164/1000 | Loss: 0.00000769
Iteration 165/1000 | Loss: 0.00000769
Iteration 166/1000 | Loss: 0.00000769
Iteration 167/1000 | Loss: 0.00000769
Iteration 168/1000 | Loss: 0.00000769
Iteration 169/1000 | Loss: 0.00000769
Iteration 170/1000 | Loss: 0.00000769
Iteration 171/1000 | Loss: 0.00000769
Iteration 172/1000 | Loss: 0.00000769
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 172. Stopping optimization.
Last 5 losses: [7.688740879530087e-06, 7.688740879530087e-06, 7.688740879530087e-06, 7.688740879530087e-06, 7.688740879530087e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 7.688740879530087e-06

Optimization complete. Final v2v error: 2.401130437850952 mm

Highest mean error: 2.5087099075317383 mm for frame 138

Lowest mean error: 2.327120780944824 mm for frame 150

Saving results

Total time: 35.913392305374146
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_022/1033/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_022/1033.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_022/1033
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00851306
Iteration 2/25 | Loss: 0.00117541
Iteration 3/25 | Loss: 0.00110215
Iteration 4/25 | Loss: 0.00109233
Iteration 5/25 | Loss: 0.00108953
Iteration 6/25 | Loss: 0.00108889
Iteration 7/25 | Loss: 0.00108889
Iteration 8/25 | Loss: 0.00108889
Iteration 9/25 | Loss: 0.00108889
Iteration 10/25 | Loss: 0.00108889
Iteration 11/25 | Loss: 0.00108889
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010888893157243729, 0.0010888893157243729, 0.0010888893157243729, 0.0010888893157243729, 0.0010888893157243729]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010888893157243729

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45892799
Iteration 2/25 | Loss: 0.00092057
Iteration 3/25 | Loss: 0.00092057
Iteration 4/25 | Loss: 0.00092056
Iteration 5/25 | Loss: 0.00092056
Iteration 6/25 | Loss: 0.00092056
Iteration 7/25 | Loss: 0.00092056
Iteration 8/25 | Loss: 0.00092056
Iteration 9/25 | Loss: 0.00092056
Iteration 10/25 | Loss: 0.00092056
Iteration 11/25 | Loss: 0.00092056
Iteration 12/25 | Loss: 0.00092056
Iteration 13/25 | Loss: 0.00092056
Iteration 14/25 | Loss: 0.00092056
Iteration 15/25 | Loss: 0.00092056
Iteration 16/25 | Loss: 0.00092056
Iteration 17/25 | Loss: 0.00092056
Iteration 18/25 | Loss: 0.00092056
Iteration 19/25 | Loss: 0.00092056
Iteration 20/25 | Loss: 0.00092056
Iteration 21/25 | Loss: 0.00092056
Iteration 22/25 | Loss: 0.00092056
Iteration 23/25 | Loss: 0.00092056
Iteration 24/25 | Loss: 0.00092056
Iteration 25/25 | Loss: 0.00092056

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00092056
Iteration 2/1000 | Loss: 0.00002665
Iteration 3/1000 | Loss: 0.00001779
Iteration 4/1000 | Loss: 0.00001415
Iteration 5/1000 | Loss: 0.00001277
Iteration 6/1000 | Loss: 0.00001211
Iteration 7/1000 | Loss: 0.00001152
Iteration 8/1000 | Loss: 0.00001121
Iteration 9/1000 | Loss: 0.00001082
Iteration 10/1000 | Loss: 0.00001063
Iteration 11/1000 | Loss: 0.00001063
Iteration 12/1000 | Loss: 0.00001061
Iteration 13/1000 | Loss: 0.00001060
Iteration 14/1000 | Loss: 0.00001059
Iteration 15/1000 | Loss: 0.00001058
Iteration 16/1000 | Loss: 0.00001051
Iteration 17/1000 | Loss: 0.00001047
Iteration 18/1000 | Loss: 0.00001046
Iteration 19/1000 | Loss: 0.00001039
Iteration 20/1000 | Loss: 0.00001038
Iteration 21/1000 | Loss: 0.00001037
Iteration 22/1000 | Loss: 0.00001036
Iteration 23/1000 | Loss: 0.00001034
Iteration 24/1000 | Loss: 0.00001033
Iteration 25/1000 | Loss: 0.00001031
Iteration 26/1000 | Loss: 0.00001030
Iteration 27/1000 | Loss: 0.00001029
Iteration 28/1000 | Loss: 0.00001028
Iteration 29/1000 | Loss: 0.00001028
Iteration 30/1000 | Loss: 0.00001027
Iteration 31/1000 | Loss: 0.00001027
Iteration 32/1000 | Loss: 0.00001025
Iteration 33/1000 | Loss: 0.00001023
Iteration 34/1000 | Loss: 0.00001023
Iteration 35/1000 | Loss: 0.00001022
Iteration 36/1000 | Loss: 0.00001019
Iteration 37/1000 | Loss: 0.00001019
Iteration 38/1000 | Loss: 0.00001017
Iteration 39/1000 | Loss: 0.00001017
Iteration 40/1000 | Loss: 0.00001016
Iteration 41/1000 | Loss: 0.00001016
Iteration 42/1000 | Loss: 0.00001016
Iteration 43/1000 | Loss: 0.00001012
Iteration 44/1000 | Loss: 0.00001011
Iteration 45/1000 | Loss: 0.00001011
Iteration 46/1000 | Loss: 0.00001010
Iteration 47/1000 | Loss: 0.00001010
Iteration 48/1000 | Loss: 0.00001010
Iteration 49/1000 | Loss: 0.00001009
Iteration 50/1000 | Loss: 0.00001008
Iteration 51/1000 | Loss: 0.00001008
Iteration 52/1000 | Loss: 0.00001008
Iteration 53/1000 | Loss: 0.00001007
Iteration 54/1000 | Loss: 0.00001007
Iteration 55/1000 | Loss: 0.00001007
Iteration 56/1000 | Loss: 0.00001006
Iteration 57/1000 | Loss: 0.00001006
Iteration 58/1000 | Loss: 0.00001006
Iteration 59/1000 | Loss: 0.00001006
Iteration 60/1000 | Loss: 0.00001006
Iteration 61/1000 | Loss: 0.00001005
Iteration 62/1000 | Loss: 0.00001005
Iteration 63/1000 | Loss: 0.00001005
Iteration 64/1000 | Loss: 0.00001004
Iteration 65/1000 | Loss: 0.00001003
Iteration 66/1000 | Loss: 0.00001003
Iteration 67/1000 | Loss: 0.00001003
Iteration 68/1000 | Loss: 0.00001003
Iteration 69/1000 | Loss: 0.00001003
Iteration 70/1000 | Loss: 0.00001003
Iteration 71/1000 | Loss: 0.00001002
Iteration 72/1000 | Loss: 0.00001002
Iteration 73/1000 | Loss: 0.00001002
Iteration 74/1000 | Loss: 0.00001002
Iteration 75/1000 | Loss: 0.00001002
Iteration 76/1000 | Loss: 0.00001002
Iteration 77/1000 | Loss: 0.00001002
Iteration 78/1000 | Loss: 0.00001001
Iteration 79/1000 | Loss: 0.00001001
Iteration 80/1000 | Loss: 0.00001001
Iteration 81/1000 | Loss: 0.00001001
Iteration 82/1000 | Loss: 0.00001001
Iteration 83/1000 | Loss: 0.00001001
Iteration 84/1000 | Loss: 0.00001001
Iteration 85/1000 | Loss: 0.00001001
Iteration 86/1000 | Loss: 0.00001000
Iteration 87/1000 | Loss: 0.00001000
Iteration 88/1000 | Loss: 0.00001000
Iteration 89/1000 | Loss: 0.00001000
Iteration 90/1000 | Loss: 0.00001000
Iteration 91/1000 | Loss: 0.00000999
Iteration 92/1000 | Loss: 0.00000999
Iteration 93/1000 | Loss: 0.00000999
Iteration 94/1000 | Loss: 0.00000999
Iteration 95/1000 | Loss: 0.00000999
Iteration 96/1000 | Loss: 0.00000999
Iteration 97/1000 | Loss: 0.00000999
Iteration 98/1000 | Loss: 0.00000998
Iteration 99/1000 | Loss: 0.00000998
Iteration 100/1000 | Loss: 0.00000998
Iteration 101/1000 | Loss: 0.00000998
Iteration 102/1000 | Loss: 0.00000997
Iteration 103/1000 | Loss: 0.00000997
Iteration 104/1000 | Loss: 0.00000997
Iteration 105/1000 | Loss: 0.00000997
Iteration 106/1000 | Loss: 0.00000997
Iteration 107/1000 | Loss: 0.00000997
Iteration 108/1000 | Loss: 0.00000997
Iteration 109/1000 | Loss: 0.00000997
Iteration 110/1000 | Loss: 0.00000997
Iteration 111/1000 | Loss: 0.00000997
Iteration 112/1000 | Loss: 0.00000996
Iteration 113/1000 | Loss: 0.00000996
Iteration 114/1000 | Loss: 0.00000996
Iteration 115/1000 | Loss: 0.00000996
Iteration 116/1000 | Loss: 0.00000996
Iteration 117/1000 | Loss: 0.00000996
Iteration 118/1000 | Loss: 0.00000996
Iteration 119/1000 | Loss: 0.00000995
Iteration 120/1000 | Loss: 0.00000995
Iteration 121/1000 | Loss: 0.00000995
Iteration 122/1000 | Loss: 0.00000995
Iteration 123/1000 | Loss: 0.00000995
Iteration 124/1000 | Loss: 0.00000995
Iteration 125/1000 | Loss: 0.00000995
Iteration 126/1000 | Loss: 0.00000995
Iteration 127/1000 | Loss: 0.00000995
Iteration 128/1000 | Loss: 0.00000995
Iteration 129/1000 | Loss: 0.00000995
Iteration 130/1000 | Loss: 0.00000995
Iteration 131/1000 | Loss: 0.00000995
Iteration 132/1000 | Loss: 0.00000995
Iteration 133/1000 | Loss: 0.00000995
Iteration 134/1000 | Loss: 0.00000995
Iteration 135/1000 | Loss: 0.00000995
Iteration 136/1000 | Loss: 0.00000995
Iteration 137/1000 | Loss: 0.00000995
Iteration 138/1000 | Loss: 0.00000995
Iteration 139/1000 | Loss: 0.00000995
Iteration 140/1000 | Loss: 0.00000995
Iteration 141/1000 | Loss: 0.00000995
Iteration 142/1000 | Loss: 0.00000995
Iteration 143/1000 | Loss: 0.00000995
Iteration 144/1000 | Loss: 0.00000995
Iteration 145/1000 | Loss: 0.00000995
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 145. Stopping optimization.
Last 5 losses: [9.945653800969012e-06, 9.945653800969012e-06, 9.945653800969012e-06, 9.945653800969012e-06, 9.945653800969012e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.945653800969012e-06

Optimization complete. Final v2v error: 2.6502015590667725 mm

Highest mean error: 3.309997320175171 mm for frame 46

Lowest mean error: 2.443124771118164 mm for frame 21

Saving results

Total time: 35.97082257270813
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_022/1005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_022/1005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_022/1005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00461056
Iteration 2/25 | Loss: 0.00131637
Iteration 3/25 | Loss: 0.00112126
Iteration 4/25 | Loss: 0.00110421
Iteration 5/25 | Loss: 0.00110167
Iteration 6/25 | Loss: 0.00110079
Iteration 7/25 | Loss: 0.00110079
Iteration 8/25 | Loss: 0.00110079
Iteration 9/25 | Loss: 0.00110079
Iteration 10/25 | Loss: 0.00110079
Iteration 11/25 | Loss: 0.00110079
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001100789988413453, 0.001100789988413453, 0.001100789988413453, 0.001100789988413453, 0.001100789988413453]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001100789988413453

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36288595
Iteration 2/25 | Loss: 0.00075191
Iteration 3/25 | Loss: 0.00075191
Iteration 4/25 | Loss: 0.00075191
Iteration 5/25 | Loss: 0.00075191
Iteration 6/25 | Loss: 0.00075191
Iteration 7/25 | Loss: 0.00075191
Iteration 8/25 | Loss: 0.00075191
Iteration 9/25 | Loss: 0.00075190
Iteration 10/25 | Loss: 0.00075190
Iteration 11/25 | Loss: 0.00075190
Iteration 12/25 | Loss: 0.00075190
Iteration 13/25 | Loss: 0.00075190
Iteration 14/25 | Loss: 0.00075190
Iteration 15/25 | Loss: 0.00075190
Iteration 16/25 | Loss: 0.00075190
Iteration 17/25 | Loss: 0.00075190
Iteration 18/25 | Loss: 0.00075190
Iteration 19/25 | Loss: 0.00075190
Iteration 20/25 | Loss: 0.00075190
Iteration 21/25 | Loss: 0.00075190
Iteration 22/25 | Loss: 0.00075190
Iteration 23/25 | Loss: 0.00075190
Iteration 24/25 | Loss: 0.00075190
Iteration 25/25 | Loss: 0.00075190

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00075190
Iteration 2/1000 | Loss: 0.00002790
Iteration 3/1000 | Loss: 0.00001595
Iteration 4/1000 | Loss: 0.00001341
Iteration 5/1000 | Loss: 0.00001239
Iteration 6/1000 | Loss: 0.00001181
Iteration 7/1000 | Loss: 0.00001145
Iteration 8/1000 | Loss: 0.00001117
Iteration 9/1000 | Loss: 0.00001095
Iteration 10/1000 | Loss: 0.00001079
Iteration 11/1000 | Loss: 0.00001074
Iteration 12/1000 | Loss: 0.00001066
Iteration 13/1000 | Loss: 0.00001062
Iteration 14/1000 | Loss: 0.00001062
Iteration 15/1000 | Loss: 0.00001060
Iteration 16/1000 | Loss: 0.00001058
Iteration 17/1000 | Loss: 0.00001056
Iteration 18/1000 | Loss: 0.00001055
Iteration 19/1000 | Loss: 0.00001055
Iteration 20/1000 | Loss: 0.00001054
Iteration 21/1000 | Loss: 0.00001054
Iteration 22/1000 | Loss: 0.00001053
Iteration 23/1000 | Loss: 0.00001052
Iteration 24/1000 | Loss: 0.00001051
Iteration 25/1000 | Loss: 0.00001050
Iteration 26/1000 | Loss: 0.00001050
Iteration 27/1000 | Loss: 0.00001049
Iteration 28/1000 | Loss: 0.00001049
Iteration 29/1000 | Loss: 0.00001048
Iteration 30/1000 | Loss: 0.00001048
Iteration 31/1000 | Loss: 0.00001048
Iteration 32/1000 | Loss: 0.00001047
Iteration 33/1000 | Loss: 0.00001047
Iteration 34/1000 | Loss: 0.00001047
Iteration 35/1000 | Loss: 0.00001043
Iteration 36/1000 | Loss: 0.00001043
Iteration 37/1000 | Loss: 0.00001043
Iteration 38/1000 | Loss: 0.00001041
Iteration 39/1000 | Loss: 0.00001040
Iteration 40/1000 | Loss: 0.00001040
Iteration 41/1000 | Loss: 0.00001040
Iteration 42/1000 | Loss: 0.00001040
Iteration 43/1000 | Loss: 0.00001039
Iteration 44/1000 | Loss: 0.00001039
Iteration 45/1000 | Loss: 0.00001039
Iteration 46/1000 | Loss: 0.00001039
Iteration 47/1000 | Loss: 0.00001038
Iteration 48/1000 | Loss: 0.00001038
Iteration 49/1000 | Loss: 0.00001038
Iteration 50/1000 | Loss: 0.00001038
Iteration 51/1000 | Loss: 0.00001038
Iteration 52/1000 | Loss: 0.00001038
Iteration 53/1000 | Loss: 0.00001037
Iteration 54/1000 | Loss: 0.00001037
Iteration 55/1000 | Loss: 0.00001037
Iteration 56/1000 | Loss: 0.00001037
Iteration 57/1000 | Loss: 0.00001037
Iteration 58/1000 | Loss: 0.00001036
Iteration 59/1000 | Loss: 0.00001036
Iteration 60/1000 | Loss: 0.00001036
Iteration 61/1000 | Loss: 0.00001036
Iteration 62/1000 | Loss: 0.00001036
Iteration 63/1000 | Loss: 0.00001036
Iteration 64/1000 | Loss: 0.00001036
Iteration 65/1000 | Loss: 0.00001036
Iteration 66/1000 | Loss: 0.00001036
Iteration 67/1000 | Loss: 0.00001035
Iteration 68/1000 | Loss: 0.00001035
Iteration 69/1000 | Loss: 0.00001035
Iteration 70/1000 | Loss: 0.00001035
Iteration 71/1000 | Loss: 0.00001035
Iteration 72/1000 | Loss: 0.00001035
Iteration 73/1000 | Loss: 0.00001034
Iteration 74/1000 | Loss: 0.00001034
Iteration 75/1000 | Loss: 0.00001034
Iteration 76/1000 | Loss: 0.00001034
Iteration 77/1000 | Loss: 0.00001034
Iteration 78/1000 | Loss: 0.00001033
Iteration 79/1000 | Loss: 0.00001033
Iteration 80/1000 | Loss: 0.00001033
Iteration 81/1000 | Loss: 0.00001033
Iteration 82/1000 | Loss: 0.00001032
Iteration 83/1000 | Loss: 0.00001032
Iteration 84/1000 | Loss: 0.00001032
Iteration 85/1000 | Loss: 0.00001032
Iteration 86/1000 | Loss: 0.00001032
Iteration 87/1000 | Loss: 0.00001032
Iteration 88/1000 | Loss: 0.00001032
Iteration 89/1000 | Loss: 0.00001032
Iteration 90/1000 | Loss: 0.00001032
Iteration 91/1000 | Loss: 0.00001031
Iteration 92/1000 | Loss: 0.00001031
Iteration 93/1000 | Loss: 0.00001031
Iteration 94/1000 | Loss: 0.00001031
Iteration 95/1000 | Loss: 0.00001031
Iteration 96/1000 | Loss: 0.00001031
Iteration 97/1000 | Loss: 0.00001030
Iteration 98/1000 | Loss: 0.00001030
Iteration 99/1000 | Loss: 0.00001030
Iteration 100/1000 | Loss: 0.00001030
Iteration 101/1000 | Loss: 0.00001030
Iteration 102/1000 | Loss: 0.00001030
Iteration 103/1000 | Loss: 0.00001030
Iteration 104/1000 | Loss: 0.00001030
Iteration 105/1000 | Loss: 0.00001029
Iteration 106/1000 | Loss: 0.00001029
Iteration 107/1000 | Loss: 0.00001029
Iteration 108/1000 | Loss: 0.00001029
Iteration 109/1000 | Loss: 0.00001029
Iteration 110/1000 | Loss: 0.00001029
Iteration 111/1000 | Loss: 0.00001029
Iteration 112/1000 | Loss: 0.00001028
Iteration 113/1000 | Loss: 0.00001028
Iteration 114/1000 | Loss: 0.00001028
Iteration 115/1000 | Loss: 0.00001028
Iteration 116/1000 | Loss: 0.00001028
Iteration 117/1000 | Loss: 0.00001028
Iteration 118/1000 | Loss: 0.00001028
Iteration 119/1000 | Loss: 0.00001028
Iteration 120/1000 | Loss: 0.00001028
Iteration 121/1000 | Loss: 0.00001028
Iteration 122/1000 | Loss: 0.00001028
Iteration 123/1000 | Loss: 0.00001028
Iteration 124/1000 | Loss: 0.00001028
Iteration 125/1000 | Loss: 0.00001027
Iteration 126/1000 | Loss: 0.00001027
Iteration 127/1000 | Loss: 0.00001027
Iteration 128/1000 | Loss: 0.00001027
Iteration 129/1000 | Loss: 0.00001027
Iteration 130/1000 | Loss: 0.00001026
Iteration 131/1000 | Loss: 0.00001026
Iteration 132/1000 | Loss: 0.00001026
Iteration 133/1000 | Loss: 0.00001026
Iteration 134/1000 | Loss: 0.00001025
Iteration 135/1000 | Loss: 0.00001025
Iteration 136/1000 | Loss: 0.00001025
Iteration 137/1000 | Loss: 0.00001025
Iteration 138/1000 | Loss: 0.00001025
Iteration 139/1000 | Loss: 0.00001025
Iteration 140/1000 | Loss: 0.00001025
Iteration 141/1000 | Loss: 0.00001025
Iteration 142/1000 | Loss: 0.00001024
Iteration 143/1000 | Loss: 0.00001024
Iteration 144/1000 | Loss: 0.00001024
Iteration 145/1000 | Loss: 0.00001024
Iteration 146/1000 | Loss: 0.00001024
Iteration 147/1000 | Loss: 0.00001024
Iteration 148/1000 | Loss: 0.00001024
Iteration 149/1000 | Loss: 0.00001024
Iteration 150/1000 | Loss: 0.00001023
Iteration 151/1000 | Loss: 0.00001023
Iteration 152/1000 | Loss: 0.00001023
Iteration 153/1000 | Loss: 0.00001023
Iteration 154/1000 | Loss: 0.00001023
Iteration 155/1000 | Loss: 0.00001022
Iteration 156/1000 | Loss: 0.00001022
Iteration 157/1000 | Loss: 0.00001022
Iteration 158/1000 | Loss: 0.00001022
Iteration 159/1000 | Loss: 0.00001022
Iteration 160/1000 | Loss: 0.00001022
Iteration 161/1000 | Loss: 0.00001022
Iteration 162/1000 | Loss: 0.00001022
Iteration 163/1000 | Loss: 0.00001022
Iteration 164/1000 | Loss: 0.00001022
Iteration 165/1000 | Loss: 0.00001022
Iteration 166/1000 | Loss: 0.00001021
Iteration 167/1000 | Loss: 0.00001021
Iteration 168/1000 | Loss: 0.00001021
Iteration 169/1000 | Loss: 0.00001021
Iteration 170/1000 | Loss: 0.00001021
Iteration 171/1000 | Loss: 0.00001021
Iteration 172/1000 | Loss: 0.00001021
Iteration 173/1000 | Loss: 0.00001021
Iteration 174/1000 | Loss: 0.00001021
Iteration 175/1000 | Loss: 0.00001021
Iteration 176/1000 | Loss: 0.00001020
Iteration 177/1000 | Loss: 0.00001020
Iteration 178/1000 | Loss: 0.00001020
Iteration 179/1000 | Loss: 0.00001020
Iteration 180/1000 | Loss: 0.00001020
Iteration 181/1000 | Loss: 0.00001020
Iteration 182/1000 | Loss: 0.00001020
Iteration 183/1000 | Loss: 0.00001020
Iteration 184/1000 | Loss: 0.00001019
Iteration 185/1000 | Loss: 0.00001019
Iteration 186/1000 | Loss: 0.00001019
Iteration 187/1000 | Loss: 0.00001019
Iteration 188/1000 | Loss: 0.00001019
Iteration 189/1000 | Loss: 0.00001019
Iteration 190/1000 | Loss: 0.00001019
Iteration 191/1000 | Loss: 0.00001019
Iteration 192/1000 | Loss: 0.00001018
Iteration 193/1000 | Loss: 0.00001018
Iteration 194/1000 | Loss: 0.00001018
Iteration 195/1000 | Loss: 0.00001018
Iteration 196/1000 | Loss: 0.00001018
Iteration 197/1000 | Loss: 0.00001018
Iteration 198/1000 | Loss: 0.00001017
Iteration 199/1000 | Loss: 0.00001017
Iteration 200/1000 | Loss: 0.00001017
Iteration 201/1000 | Loss: 0.00001017
Iteration 202/1000 | Loss: 0.00001017
Iteration 203/1000 | Loss: 0.00001017
Iteration 204/1000 | Loss: 0.00001017
Iteration 205/1000 | Loss: 0.00001017
Iteration 206/1000 | Loss: 0.00001016
Iteration 207/1000 | Loss: 0.00001016
Iteration 208/1000 | Loss: 0.00001016
Iteration 209/1000 | Loss: 0.00001016
Iteration 210/1000 | Loss: 0.00001016
Iteration 211/1000 | Loss: 0.00001016
Iteration 212/1000 | Loss: 0.00001016
Iteration 213/1000 | Loss: 0.00001016
Iteration 214/1000 | Loss: 0.00001016
Iteration 215/1000 | Loss: 0.00001016
Iteration 216/1000 | Loss: 0.00001016
Iteration 217/1000 | Loss: 0.00001016
Iteration 218/1000 | Loss: 0.00001016
Iteration 219/1000 | Loss: 0.00001016
Iteration 220/1000 | Loss: 0.00001016
Iteration 221/1000 | Loss: 0.00001016
Iteration 222/1000 | Loss: 0.00001016
Iteration 223/1000 | Loss: 0.00001016
Iteration 224/1000 | Loss: 0.00001016
Iteration 225/1000 | Loss: 0.00001016
Iteration 226/1000 | Loss: 0.00001016
Iteration 227/1000 | Loss: 0.00001016
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 227. Stopping optimization.
Last 5 losses: [1.015808720694622e-05, 1.015808720694622e-05, 1.015808720694622e-05, 1.015808720694622e-05, 1.015808720694622e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.015808720694622e-05

Optimization complete. Final v2v error: 2.679171323776245 mm

Highest mean error: 3.478256940841675 mm for frame 79

Lowest mean error: 2.367076873779297 mm for frame 163

Saving results

Total time: 40.32696008682251
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_022/1043/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_022/1043.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_022/1043
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01020100
Iteration 2/25 | Loss: 0.01020100
Iteration 3/25 | Loss: 0.00294855
Iteration 4/25 | Loss: 0.00205555
Iteration 5/25 | Loss: 0.00184233
Iteration 6/25 | Loss: 0.00167417
Iteration 7/25 | Loss: 0.00165454
Iteration 8/25 | Loss: 0.00156685
Iteration 9/25 | Loss: 0.00151462
Iteration 10/25 | Loss: 0.00151431
Iteration 11/25 | Loss: 0.00147536
Iteration 12/25 | Loss: 0.00145985
Iteration 13/25 | Loss: 0.00143003
Iteration 14/25 | Loss: 0.00141473
Iteration 15/25 | Loss: 0.00142258
Iteration 16/25 | Loss: 0.00140843
Iteration 17/25 | Loss: 0.00140814
Iteration 18/25 | Loss: 0.00140400
Iteration 19/25 | Loss: 0.00140434
Iteration 20/25 | Loss: 0.00140261
Iteration 21/25 | Loss: 0.00140276
Iteration 22/25 | Loss: 0.00139838
Iteration 23/25 | Loss: 0.00139320
Iteration 24/25 | Loss: 0.00139097
Iteration 25/25 | Loss: 0.00139049

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.31045282
Iteration 2/25 | Loss: 0.00520512
Iteration 3/25 | Loss: 0.00319789
Iteration 4/25 | Loss: 0.00319789
Iteration 5/25 | Loss: 0.00319789
Iteration 6/25 | Loss: 0.00319789
Iteration 7/25 | Loss: 0.00319789
Iteration 8/25 | Loss: 0.00319789
Iteration 9/25 | Loss: 0.00319789
Iteration 10/25 | Loss: 0.00319789
Iteration 11/25 | Loss: 0.00319789
Iteration 12/25 | Loss: 0.00319789
Iteration 13/25 | Loss: 0.00319789
Iteration 14/25 | Loss: 0.00319789
Iteration 15/25 | Loss: 0.00319789
Iteration 16/25 | Loss: 0.00319789
Iteration 17/25 | Loss: 0.00319789
Iteration 18/25 | Loss: 0.00319789
Iteration 19/25 | Loss: 0.00319789
Iteration 20/25 | Loss: 0.00319789
Iteration 21/25 | Loss: 0.00319789
Iteration 22/25 | Loss: 0.00319789
Iteration 23/25 | Loss: 0.00319789
Iteration 24/25 | Loss: 0.00319789
Iteration 25/25 | Loss: 0.00319789

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00319789
Iteration 2/1000 | Loss: 0.00069611
Iteration 3/1000 | Loss: 0.00227933
Iteration 4/1000 | Loss: 0.00047782
Iteration 5/1000 | Loss: 0.00063618
Iteration 6/1000 | Loss: 0.00068964
Iteration 7/1000 | Loss: 0.00059099
Iteration 8/1000 | Loss: 0.00085380
Iteration 9/1000 | Loss: 0.00034185
Iteration 10/1000 | Loss: 0.00084862
Iteration 11/1000 | Loss: 0.00023545
Iteration 12/1000 | Loss: 0.00030213
Iteration 13/1000 | Loss: 0.00019669
Iteration 14/1000 | Loss: 0.00160251
Iteration 15/1000 | Loss: 0.00059052
Iteration 16/1000 | Loss: 0.00069459
Iteration 17/1000 | Loss: 0.00024217
Iteration 18/1000 | Loss: 0.00018775
Iteration 19/1000 | Loss: 0.00136148
Iteration 20/1000 | Loss: 0.00525362
Iteration 21/1000 | Loss: 0.00521162
Iteration 22/1000 | Loss: 0.00681468
Iteration 23/1000 | Loss: 0.00312425
Iteration 24/1000 | Loss: 0.00077288
Iteration 25/1000 | Loss: 0.00141516
Iteration 26/1000 | Loss: 0.00103680
Iteration 27/1000 | Loss: 0.00161290
Iteration 28/1000 | Loss: 0.00019002
Iteration 29/1000 | Loss: 0.00149313
Iteration 30/1000 | Loss: 0.00223871
Iteration 31/1000 | Loss: 0.00193108
Iteration 32/1000 | Loss: 0.00161060
Iteration 33/1000 | Loss: 0.00043330
Iteration 34/1000 | Loss: 0.00151142
Iteration 35/1000 | Loss: 0.00118578
Iteration 36/1000 | Loss: 0.00051787
Iteration 37/1000 | Loss: 0.00029780
Iteration 38/1000 | Loss: 0.00051857
Iteration 39/1000 | Loss: 0.00077002
Iteration 40/1000 | Loss: 0.00010762
Iteration 41/1000 | Loss: 0.00046559
Iteration 42/1000 | Loss: 0.00009242
Iteration 43/1000 | Loss: 0.00025187
Iteration 44/1000 | Loss: 0.00049921
Iteration 45/1000 | Loss: 0.00008200
Iteration 46/1000 | Loss: 0.00015958
Iteration 47/1000 | Loss: 0.00023992
Iteration 48/1000 | Loss: 0.00010872
Iteration 49/1000 | Loss: 0.00053324
Iteration 50/1000 | Loss: 0.00050022
Iteration 51/1000 | Loss: 0.00064226
Iteration 52/1000 | Loss: 0.00024137
Iteration 53/1000 | Loss: 0.00007476
Iteration 54/1000 | Loss: 0.00007341
Iteration 55/1000 | Loss: 0.00047858
Iteration 56/1000 | Loss: 0.00008137
Iteration 57/1000 | Loss: 0.00007948
Iteration 58/1000 | Loss: 0.00007171
Iteration 59/1000 | Loss: 0.00024308
Iteration 60/1000 | Loss: 0.00007106
Iteration 61/1000 | Loss: 0.00022581
Iteration 62/1000 | Loss: 0.00059506
Iteration 63/1000 | Loss: 0.00021064
Iteration 64/1000 | Loss: 0.00008680
Iteration 65/1000 | Loss: 0.00009008
Iteration 66/1000 | Loss: 0.00007910
Iteration 67/1000 | Loss: 0.00007010
Iteration 68/1000 | Loss: 0.00006983
Iteration 69/1000 | Loss: 0.00022692
Iteration 70/1000 | Loss: 0.00006984
Iteration 71/1000 | Loss: 0.00007191
Iteration 72/1000 | Loss: 0.00006956
Iteration 73/1000 | Loss: 0.00006920
Iteration 74/1000 | Loss: 0.00006893
Iteration 75/1000 | Loss: 0.00006878
Iteration 76/1000 | Loss: 0.00006875
Iteration 77/1000 | Loss: 0.00006875
Iteration 78/1000 | Loss: 0.00006869
Iteration 79/1000 | Loss: 0.00006863
Iteration 80/1000 | Loss: 0.00006858
Iteration 81/1000 | Loss: 0.00006858
Iteration 82/1000 | Loss: 0.00006844
Iteration 83/1000 | Loss: 0.00006834
Iteration 84/1000 | Loss: 0.00006820
Iteration 85/1000 | Loss: 0.00020729
Iteration 86/1000 | Loss: 0.00006938
Iteration 87/1000 | Loss: 0.00006734
Iteration 88/1000 | Loss: 0.00006659
Iteration 89/1000 | Loss: 0.00006529
Iteration 90/1000 | Loss: 0.00006405
Iteration 91/1000 | Loss: 0.00045359
Iteration 92/1000 | Loss: 0.00006570
Iteration 93/1000 | Loss: 0.00006420
Iteration 94/1000 | Loss: 0.00018540
Iteration 95/1000 | Loss: 0.00007985
Iteration 96/1000 | Loss: 0.00006025
Iteration 97/1000 | Loss: 0.00005940
Iteration 98/1000 | Loss: 0.00026699
Iteration 99/1000 | Loss: 0.00026065
Iteration 100/1000 | Loss: 0.00143356
Iteration 101/1000 | Loss: 0.00219982
Iteration 102/1000 | Loss: 0.00151570
Iteration 103/1000 | Loss: 0.00035654
Iteration 104/1000 | Loss: 0.00044920
Iteration 105/1000 | Loss: 0.00017530
Iteration 106/1000 | Loss: 0.00023779
Iteration 107/1000 | Loss: 0.00021858
Iteration 108/1000 | Loss: 0.00025413
Iteration 109/1000 | Loss: 0.00017447
Iteration 110/1000 | Loss: 0.00016601
Iteration 111/1000 | Loss: 0.00021509
Iteration 112/1000 | Loss: 0.00004910
Iteration 113/1000 | Loss: 0.00004017
Iteration 114/1000 | Loss: 0.00003646
Iteration 115/1000 | Loss: 0.00032238
Iteration 116/1000 | Loss: 0.00020361
Iteration 117/1000 | Loss: 0.00020322
Iteration 118/1000 | Loss: 0.00014841
Iteration 119/1000 | Loss: 0.00003227
Iteration 120/1000 | Loss: 0.00003121
Iteration 121/1000 | Loss: 0.00003049
Iteration 122/1000 | Loss: 0.00022211
Iteration 123/1000 | Loss: 0.00002982
Iteration 124/1000 | Loss: 0.00002938
Iteration 125/1000 | Loss: 0.00002905
Iteration 126/1000 | Loss: 0.00002883
Iteration 127/1000 | Loss: 0.00002880
Iteration 128/1000 | Loss: 0.00002865
Iteration 129/1000 | Loss: 0.00002850
Iteration 130/1000 | Loss: 0.00002850
Iteration 131/1000 | Loss: 0.00002832
Iteration 132/1000 | Loss: 0.00002810
Iteration 133/1000 | Loss: 0.00002809
Iteration 134/1000 | Loss: 0.00002808
Iteration 135/1000 | Loss: 0.00002807
Iteration 136/1000 | Loss: 0.00010759
Iteration 137/1000 | Loss: 0.00013356
Iteration 138/1000 | Loss: 0.00018826
Iteration 139/1000 | Loss: 0.00003215
Iteration 140/1000 | Loss: 0.00012294
Iteration 141/1000 | Loss: 0.00017252
Iteration 142/1000 | Loss: 0.00040851
Iteration 143/1000 | Loss: 0.00032180
Iteration 144/1000 | Loss: 0.00003161
Iteration 145/1000 | Loss: 0.00002901
Iteration 146/1000 | Loss: 0.00002834
Iteration 147/1000 | Loss: 0.00002790
Iteration 148/1000 | Loss: 0.00002738
Iteration 149/1000 | Loss: 0.00002685
Iteration 150/1000 | Loss: 0.00002656
Iteration 151/1000 | Loss: 0.00002636
Iteration 152/1000 | Loss: 0.00016210
Iteration 153/1000 | Loss: 0.00002917
Iteration 154/1000 | Loss: 0.00002623
Iteration 155/1000 | Loss: 0.00002618
Iteration 156/1000 | Loss: 0.00002618
Iteration 157/1000 | Loss: 0.00002617
Iteration 158/1000 | Loss: 0.00002616
Iteration 159/1000 | Loss: 0.00002614
Iteration 160/1000 | Loss: 0.00002614
Iteration 161/1000 | Loss: 0.00002613
Iteration 162/1000 | Loss: 0.00002613
Iteration 163/1000 | Loss: 0.00002612
Iteration 164/1000 | Loss: 0.00002612
Iteration 165/1000 | Loss: 0.00002611
Iteration 166/1000 | Loss: 0.00002607
Iteration 167/1000 | Loss: 0.00002604
Iteration 168/1000 | Loss: 0.00002601
Iteration 169/1000 | Loss: 0.00002600
Iteration 170/1000 | Loss: 0.00002599
Iteration 171/1000 | Loss: 0.00002589
Iteration 172/1000 | Loss: 0.00002585
Iteration 173/1000 | Loss: 0.00002584
Iteration 174/1000 | Loss: 0.00002580
Iteration 175/1000 | Loss: 0.00002580
Iteration 176/1000 | Loss: 0.00002580
Iteration 177/1000 | Loss: 0.00002580
Iteration 178/1000 | Loss: 0.00002579
Iteration 179/1000 | Loss: 0.00002579
Iteration 180/1000 | Loss: 0.00002578
Iteration 181/1000 | Loss: 0.00002578
Iteration 182/1000 | Loss: 0.00002578
Iteration 183/1000 | Loss: 0.00002578
Iteration 184/1000 | Loss: 0.00002577
Iteration 185/1000 | Loss: 0.00002577
Iteration 186/1000 | Loss: 0.00002577
Iteration 187/1000 | Loss: 0.00002577
Iteration 188/1000 | Loss: 0.00002576
Iteration 189/1000 | Loss: 0.00002576
Iteration 190/1000 | Loss: 0.00002576
Iteration 191/1000 | Loss: 0.00002576
Iteration 192/1000 | Loss: 0.00002575
Iteration 193/1000 | Loss: 0.00002575
Iteration 194/1000 | Loss: 0.00002575
Iteration 195/1000 | Loss: 0.00002575
Iteration 196/1000 | Loss: 0.00002574
Iteration 197/1000 | Loss: 0.00002574
Iteration 198/1000 | Loss: 0.00002574
Iteration 199/1000 | Loss: 0.00002573
Iteration 200/1000 | Loss: 0.00002573
Iteration 201/1000 | Loss: 0.00002572
Iteration 202/1000 | Loss: 0.00002572
Iteration 203/1000 | Loss: 0.00002572
Iteration 204/1000 | Loss: 0.00002572
Iteration 205/1000 | Loss: 0.00002571
Iteration 206/1000 | Loss: 0.00002571
Iteration 207/1000 | Loss: 0.00002571
Iteration 208/1000 | Loss: 0.00002571
Iteration 209/1000 | Loss: 0.00002571
Iteration 210/1000 | Loss: 0.00002571
Iteration 211/1000 | Loss: 0.00002571
Iteration 212/1000 | Loss: 0.00002571
Iteration 213/1000 | Loss: 0.00002571
Iteration 214/1000 | Loss: 0.00002570
Iteration 215/1000 | Loss: 0.00002570
Iteration 216/1000 | Loss: 0.00002570
Iteration 217/1000 | Loss: 0.00002570
Iteration 218/1000 | Loss: 0.00002570
Iteration 219/1000 | Loss: 0.00002570
Iteration 220/1000 | Loss: 0.00002569
Iteration 221/1000 | Loss: 0.00002569
Iteration 222/1000 | Loss: 0.00002569
Iteration 223/1000 | Loss: 0.00002569
Iteration 224/1000 | Loss: 0.00002569
Iteration 225/1000 | Loss: 0.00002569
Iteration 226/1000 | Loss: 0.00002569
Iteration 227/1000 | Loss: 0.00002569
Iteration 228/1000 | Loss: 0.00002569
Iteration 229/1000 | Loss: 0.00002569
Iteration 230/1000 | Loss: 0.00002569
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 230. Stopping optimization.
Last 5 losses: [2.5690431357361376e-05, 2.5690431357361376e-05, 2.5690431357361376e-05, 2.5690431357361376e-05, 2.5690431357361376e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.5690431357361376e-05

Optimization complete. Final v2v error: 3.304995059967041 mm

Highest mean error: 11.492828369140625 mm for frame 205

Lowest mean error: 2.723257064819336 mm for frame 185

Saving results

Total time: 298.23971724510193
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_022/1026/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_022/1026.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_022/1026
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01017650
Iteration 2/25 | Loss: 0.00153652
Iteration 3/25 | Loss: 0.00144730
Iteration 4/25 | Loss: 0.00116956
Iteration 5/25 | Loss: 0.00115250
Iteration 6/25 | Loss: 0.00114777
Iteration 7/25 | Loss: 0.00114637
Iteration 8/25 | Loss: 0.00115257
Iteration 9/25 | Loss: 0.00115892
Iteration 10/25 | Loss: 0.00115616
Iteration 11/25 | Loss: 0.00115806
Iteration 12/25 | Loss: 0.00116044
Iteration 13/25 | Loss: 0.00114928
Iteration 14/25 | Loss: 0.00114736
Iteration 15/25 | Loss: 0.00113965
Iteration 16/25 | Loss: 0.00113460
Iteration 17/25 | Loss: 0.00113244
Iteration 18/25 | Loss: 0.00113058
Iteration 19/25 | Loss: 0.00112934
Iteration 20/25 | Loss: 0.00112818
Iteration 21/25 | Loss: 0.00112759
Iteration 22/25 | Loss: 0.00112746
Iteration 23/25 | Loss: 0.00112741
Iteration 24/25 | Loss: 0.00112741
Iteration 25/25 | Loss: 0.00112740

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42058098
Iteration 2/25 | Loss: 0.00087687
Iteration 3/25 | Loss: 0.00087686
Iteration 4/25 | Loss: 0.00087686
Iteration 5/25 | Loss: 0.00087686
Iteration 6/25 | Loss: 0.00087686
Iteration 7/25 | Loss: 0.00087686
Iteration 8/25 | Loss: 0.00087686
Iteration 9/25 | Loss: 0.00087686
Iteration 10/25 | Loss: 0.00087686
Iteration 11/25 | Loss: 0.00087686
Iteration 12/25 | Loss: 0.00087686
Iteration 13/25 | Loss: 0.00087686
Iteration 14/25 | Loss: 0.00087686
Iteration 15/25 | Loss: 0.00087686
Iteration 16/25 | Loss: 0.00087686
Iteration 17/25 | Loss: 0.00087686
Iteration 18/25 | Loss: 0.00087686
Iteration 19/25 | Loss: 0.00087686
Iteration 20/25 | Loss: 0.00087686
Iteration 21/25 | Loss: 0.00087686
Iteration 22/25 | Loss: 0.00087686
Iteration 23/25 | Loss: 0.00087686
Iteration 24/25 | Loss: 0.00087686
Iteration 25/25 | Loss: 0.00087686

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00087686
Iteration 2/1000 | Loss: 0.00002675
Iteration 3/1000 | Loss: 0.00001689
Iteration 4/1000 | Loss: 0.00001464
Iteration 5/1000 | Loss: 0.00001363
Iteration 6/1000 | Loss: 0.00001304
Iteration 7/1000 | Loss: 0.00001250
Iteration 8/1000 | Loss: 0.00001209
Iteration 9/1000 | Loss: 0.00001192
Iteration 10/1000 | Loss: 0.00001184
Iteration 11/1000 | Loss: 0.00001182
Iteration 12/1000 | Loss: 0.00001180
Iteration 13/1000 | Loss: 0.00001167
Iteration 14/1000 | Loss: 0.00001152
Iteration 15/1000 | Loss: 0.00001147
Iteration 16/1000 | Loss: 0.00001146
Iteration 17/1000 | Loss: 0.00001146
Iteration 18/1000 | Loss: 0.00001144
Iteration 19/1000 | Loss: 0.00001143
Iteration 20/1000 | Loss: 0.00001142
Iteration 21/1000 | Loss: 0.00001141
Iteration 22/1000 | Loss: 0.00001141
Iteration 23/1000 | Loss: 0.00001140
Iteration 24/1000 | Loss: 0.00001139
Iteration 25/1000 | Loss: 0.00001138
Iteration 26/1000 | Loss: 0.00001137
Iteration 27/1000 | Loss: 0.00001137
Iteration 28/1000 | Loss: 0.00001137
Iteration 29/1000 | Loss: 0.00001136
Iteration 30/1000 | Loss: 0.00001136
Iteration 31/1000 | Loss: 0.00001136
Iteration 32/1000 | Loss: 0.00001136
Iteration 33/1000 | Loss: 0.00001136
Iteration 34/1000 | Loss: 0.00001135
Iteration 35/1000 | Loss: 0.00001135
Iteration 36/1000 | Loss: 0.00001134
Iteration 37/1000 | Loss: 0.00001134
Iteration 38/1000 | Loss: 0.00001134
Iteration 39/1000 | Loss: 0.00001133
Iteration 40/1000 | Loss: 0.00001129
Iteration 41/1000 | Loss: 0.00001129
Iteration 42/1000 | Loss: 0.00001129
Iteration 43/1000 | Loss: 0.00001129
Iteration 44/1000 | Loss: 0.00001129
Iteration 45/1000 | Loss: 0.00001126
Iteration 46/1000 | Loss: 0.00001126
Iteration 47/1000 | Loss: 0.00001125
Iteration 48/1000 | Loss: 0.00001125
Iteration 49/1000 | Loss: 0.00001125
Iteration 50/1000 | Loss: 0.00001125
Iteration 51/1000 | Loss: 0.00001125
Iteration 52/1000 | Loss: 0.00001125
Iteration 53/1000 | Loss: 0.00001124
Iteration 54/1000 | Loss: 0.00001124
Iteration 55/1000 | Loss: 0.00001124
Iteration 56/1000 | Loss: 0.00001123
Iteration 57/1000 | Loss: 0.00001123
Iteration 58/1000 | Loss: 0.00001123
Iteration 59/1000 | Loss: 0.00001123
Iteration 60/1000 | Loss: 0.00001122
Iteration 61/1000 | Loss: 0.00001122
Iteration 62/1000 | Loss: 0.00001122
Iteration 63/1000 | Loss: 0.00001122
Iteration 64/1000 | Loss: 0.00001122
Iteration 65/1000 | Loss: 0.00001122
Iteration 66/1000 | Loss: 0.00001122
Iteration 67/1000 | Loss: 0.00001122
Iteration 68/1000 | Loss: 0.00001122
Iteration 69/1000 | Loss: 0.00001121
Iteration 70/1000 | Loss: 0.00001121
Iteration 71/1000 | Loss: 0.00001120
Iteration 72/1000 | Loss: 0.00001120
Iteration 73/1000 | Loss: 0.00001119
Iteration 74/1000 | Loss: 0.00001119
Iteration 75/1000 | Loss: 0.00001119
Iteration 76/1000 | Loss: 0.00001118
Iteration 77/1000 | Loss: 0.00001118
Iteration 78/1000 | Loss: 0.00001118
Iteration 79/1000 | Loss: 0.00001118
Iteration 80/1000 | Loss: 0.00001118
Iteration 81/1000 | Loss: 0.00001118
Iteration 82/1000 | Loss: 0.00001118
Iteration 83/1000 | Loss: 0.00001118
Iteration 84/1000 | Loss: 0.00001118
Iteration 85/1000 | Loss: 0.00001118
Iteration 86/1000 | Loss: 0.00001117
Iteration 87/1000 | Loss: 0.00001117
Iteration 88/1000 | Loss: 0.00001116
Iteration 89/1000 | Loss: 0.00001116
Iteration 90/1000 | Loss: 0.00001116
Iteration 91/1000 | Loss: 0.00001116
Iteration 92/1000 | Loss: 0.00001115
Iteration 93/1000 | Loss: 0.00001115
Iteration 94/1000 | Loss: 0.00001115
Iteration 95/1000 | Loss: 0.00001115
Iteration 96/1000 | Loss: 0.00001114
Iteration 97/1000 | Loss: 0.00001114
Iteration 98/1000 | Loss: 0.00001114
Iteration 99/1000 | Loss: 0.00001113
Iteration 100/1000 | Loss: 0.00001113
Iteration 101/1000 | Loss: 0.00001113
Iteration 102/1000 | Loss: 0.00001113
Iteration 103/1000 | Loss: 0.00001113
Iteration 104/1000 | Loss: 0.00001113
Iteration 105/1000 | Loss: 0.00001113
Iteration 106/1000 | Loss: 0.00001113
Iteration 107/1000 | Loss: 0.00001113
Iteration 108/1000 | Loss: 0.00001112
Iteration 109/1000 | Loss: 0.00001112
Iteration 110/1000 | Loss: 0.00001112
Iteration 111/1000 | Loss: 0.00001112
Iteration 112/1000 | Loss: 0.00001112
Iteration 113/1000 | Loss: 0.00001112
Iteration 114/1000 | Loss: 0.00001112
Iteration 115/1000 | Loss: 0.00001111
Iteration 116/1000 | Loss: 0.00001111
Iteration 117/1000 | Loss: 0.00001110
Iteration 118/1000 | Loss: 0.00001110
Iteration 119/1000 | Loss: 0.00001110
Iteration 120/1000 | Loss: 0.00001110
Iteration 121/1000 | Loss: 0.00001110
Iteration 122/1000 | Loss: 0.00001110
Iteration 123/1000 | Loss: 0.00001109
Iteration 124/1000 | Loss: 0.00001109
Iteration 125/1000 | Loss: 0.00001109
Iteration 126/1000 | Loss: 0.00001109
Iteration 127/1000 | Loss: 0.00001109
Iteration 128/1000 | Loss: 0.00001109
Iteration 129/1000 | Loss: 0.00001108
Iteration 130/1000 | Loss: 0.00001108
Iteration 131/1000 | Loss: 0.00001108
Iteration 132/1000 | Loss: 0.00001108
Iteration 133/1000 | Loss: 0.00001107
Iteration 134/1000 | Loss: 0.00001107
Iteration 135/1000 | Loss: 0.00001107
Iteration 136/1000 | Loss: 0.00001107
Iteration 137/1000 | Loss: 0.00001107
Iteration 138/1000 | Loss: 0.00001107
Iteration 139/1000 | Loss: 0.00001107
Iteration 140/1000 | Loss: 0.00001107
Iteration 141/1000 | Loss: 0.00001107
Iteration 142/1000 | Loss: 0.00001107
Iteration 143/1000 | Loss: 0.00001107
Iteration 144/1000 | Loss: 0.00001107
Iteration 145/1000 | Loss: 0.00001107
Iteration 146/1000 | Loss: 0.00001107
Iteration 147/1000 | Loss: 0.00001107
Iteration 148/1000 | Loss: 0.00001107
Iteration 149/1000 | Loss: 0.00001107
Iteration 150/1000 | Loss: 0.00001107
Iteration 151/1000 | Loss: 0.00001107
Iteration 152/1000 | Loss: 0.00001107
Iteration 153/1000 | Loss: 0.00001107
Iteration 154/1000 | Loss: 0.00001107
Iteration 155/1000 | Loss: 0.00001107
Iteration 156/1000 | Loss: 0.00001107
Iteration 157/1000 | Loss: 0.00001107
Iteration 158/1000 | Loss: 0.00001107
Iteration 159/1000 | Loss: 0.00001107
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 159. Stopping optimization.
Last 5 losses: [1.106961371988291e-05, 1.106961371988291e-05, 1.106961371988291e-05, 1.106961371988291e-05, 1.106961371988291e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.106961371988291e-05

Optimization complete. Final v2v error: 2.838716745376587 mm

Highest mean error: 3.3627114295959473 mm for frame 77

Lowest mean error: 2.542804718017578 mm for frame 109

Saving results

Total time: 65.03861165046692
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_022/1063/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_022/1063.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_022/1063
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00414301
Iteration 2/25 | Loss: 0.00113527
Iteration 3/25 | Loss: 0.00108035
Iteration 4/25 | Loss: 0.00107320
Iteration 5/25 | Loss: 0.00107178
Iteration 6/25 | Loss: 0.00107178
Iteration 7/25 | Loss: 0.00107178
Iteration 8/25 | Loss: 0.00107174
Iteration 9/25 | Loss: 0.00107174
Iteration 10/25 | Loss: 0.00107174
Iteration 11/25 | Loss: 0.00107174
Iteration 12/25 | Loss: 0.00107174
Iteration 13/25 | Loss: 0.00107174
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0010717425029724836, 0.0010717425029724836, 0.0010717425029724836, 0.0010717425029724836, 0.0010717425029724836]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010717425029724836

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.46610951
Iteration 2/25 | Loss: 0.00068621
Iteration 3/25 | Loss: 0.00068621
Iteration 4/25 | Loss: 0.00068621
Iteration 5/25 | Loss: 0.00068621
Iteration 6/25 | Loss: 0.00068621
Iteration 7/25 | Loss: 0.00068621
Iteration 8/25 | Loss: 0.00068621
Iteration 9/25 | Loss: 0.00068621
Iteration 10/25 | Loss: 0.00068621
Iteration 11/25 | Loss: 0.00068621
Iteration 12/25 | Loss: 0.00068621
Iteration 13/25 | Loss: 0.00068621
Iteration 14/25 | Loss: 0.00068621
Iteration 15/25 | Loss: 0.00068621
Iteration 16/25 | Loss: 0.00068621
Iteration 17/25 | Loss: 0.00068621
Iteration 18/25 | Loss: 0.00068621
Iteration 19/25 | Loss: 0.00068621
Iteration 20/25 | Loss: 0.00068621
Iteration 21/25 | Loss: 0.00068621
Iteration 22/25 | Loss: 0.00068621
Iteration 23/25 | Loss: 0.00068621
Iteration 24/25 | Loss: 0.00068621
Iteration 25/25 | Loss: 0.00068621
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0006862056907266378, 0.0006862056907266378, 0.0006862056907266378, 0.0006862056907266378, 0.0006862056907266378]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006862056907266378

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00068621
Iteration 2/1000 | Loss: 0.00002296
Iteration 3/1000 | Loss: 0.00001620
Iteration 4/1000 | Loss: 0.00001490
Iteration 5/1000 | Loss: 0.00001425
Iteration 6/1000 | Loss: 0.00001346
Iteration 7/1000 | Loss: 0.00001309
Iteration 8/1000 | Loss: 0.00001270
Iteration 9/1000 | Loss: 0.00001246
Iteration 10/1000 | Loss: 0.00001237
Iteration 11/1000 | Loss: 0.00001233
Iteration 12/1000 | Loss: 0.00001211
Iteration 13/1000 | Loss: 0.00001205
Iteration 14/1000 | Loss: 0.00001198
Iteration 15/1000 | Loss: 0.00001192
Iteration 16/1000 | Loss: 0.00001177
Iteration 17/1000 | Loss: 0.00001175
Iteration 18/1000 | Loss: 0.00001166
Iteration 19/1000 | Loss: 0.00001158
Iteration 20/1000 | Loss: 0.00001158
Iteration 21/1000 | Loss: 0.00001157
Iteration 22/1000 | Loss: 0.00001157
Iteration 23/1000 | Loss: 0.00001157
Iteration 24/1000 | Loss: 0.00001156
Iteration 25/1000 | Loss: 0.00001156
Iteration 26/1000 | Loss: 0.00001156
Iteration 27/1000 | Loss: 0.00001155
Iteration 28/1000 | Loss: 0.00001155
Iteration 29/1000 | Loss: 0.00001155
Iteration 30/1000 | Loss: 0.00001154
Iteration 31/1000 | Loss: 0.00001153
Iteration 32/1000 | Loss: 0.00001152
Iteration 33/1000 | Loss: 0.00001152
Iteration 34/1000 | Loss: 0.00001152
Iteration 35/1000 | Loss: 0.00001152
Iteration 36/1000 | Loss: 0.00001152
Iteration 37/1000 | Loss: 0.00001152
Iteration 38/1000 | Loss: 0.00001152
Iteration 39/1000 | Loss: 0.00001151
Iteration 40/1000 | Loss: 0.00001150
Iteration 41/1000 | Loss: 0.00001150
Iteration 42/1000 | Loss: 0.00001149
Iteration 43/1000 | Loss: 0.00001148
Iteration 44/1000 | Loss: 0.00001148
Iteration 45/1000 | Loss: 0.00001148
Iteration 46/1000 | Loss: 0.00001147
Iteration 47/1000 | Loss: 0.00001147
Iteration 48/1000 | Loss: 0.00001146
Iteration 49/1000 | Loss: 0.00001146
Iteration 50/1000 | Loss: 0.00001146
Iteration 51/1000 | Loss: 0.00001145
Iteration 52/1000 | Loss: 0.00001144
Iteration 53/1000 | Loss: 0.00001143
Iteration 54/1000 | Loss: 0.00001140
Iteration 55/1000 | Loss: 0.00001139
Iteration 56/1000 | Loss: 0.00001139
Iteration 57/1000 | Loss: 0.00001138
Iteration 58/1000 | Loss: 0.00001138
Iteration 59/1000 | Loss: 0.00001137
Iteration 60/1000 | Loss: 0.00001137
Iteration 61/1000 | Loss: 0.00001134
Iteration 62/1000 | Loss: 0.00001134
Iteration 63/1000 | Loss: 0.00001134
Iteration 64/1000 | Loss: 0.00001134
Iteration 65/1000 | Loss: 0.00001134
Iteration 66/1000 | Loss: 0.00001134
Iteration 67/1000 | Loss: 0.00001134
Iteration 68/1000 | Loss: 0.00001134
Iteration 69/1000 | Loss: 0.00001134
Iteration 70/1000 | Loss: 0.00001133
Iteration 71/1000 | Loss: 0.00001133
Iteration 72/1000 | Loss: 0.00001133
Iteration 73/1000 | Loss: 0.00001131
Iteration 74/1000 | Loss: 0.00001131
Iteration 75/1000 | Loss: 0.00001130
Iteration 76/1000 | Loss: 0.00001130
Iteration 77/1000 | Loss: 0.00001130
Iteration 78/1000 | Loss: 0.00001130
Iteration 79/1000 | Loss: 0.00001129
Iteration 80/1000 | Loss: 0.00001129
Iteration 81/1000 | Loss: 0.00001128
Iteration 82/1000 | Loss: 0.00001128
Iteration 83/1000 | Loss: 0.00001128
Iteration 84/1000 | Loss: 0.00001127
Iteration 85/1000 | Loss: 0.00001127
Iteration 86/1000 | Loss: 0.00001126
Iteration 87/1000 | Loss: 0.00001126
Iteration 88/1000 | Loss: 0.00001125
Iteration 89/1000 | Loss: 0.00001125
Iteration 90/1000 | Loss: 0.00001125
Iteration 91/1000 | Loss: 0.00001124
Iteration 92/1000 | Loss: 0.00001124
Iteration 93/1000 | Loss: 0.00001124
Iteration 94/1000 | Loss: 0.00001124
Iteration 95/1000 | Loss: 0.00001123
Iteration 96/1000 | Loss: 0.00001123
Iteration 97/1000 | Loss: 0.00001123
Iteration 98/1000 | Loss: 0.00001123
Iteration 99/1000 | Loss: 0.00001123
Iteration 100/1000 | Loss: 0.00001123
Iteration 101/1000 | Loss: 0.00001122
Iteration 102/1000 | Loss: 0.00001122
Iteration 103/1000 | Loss: 0.00001122
Iteration 104/1000 | Loss: 0.00001122
Iteration 105/1000 | Loss: 0.00001121
Iteration 106/1000 | Loss: 0.00001121
Iteration 107/1000 | Loss: 0.00001121
Iteration 108/1000 | Loss: 0.00001121
Iteration 109/1000 | Loss: 0.00001121
Iteration 110/1000 | Loss: 0.00001121
Iteration 111/1000 | Loss: 0.00001121
Iteration 112/1000 | Loss: 0.00001121
Iteration 113/1000 | Loss: 0.00001121
Iteration 114/1000 | Loss: 0.00001121
Iteration 115/1000 | Loss: 0.00001121
Iteration 116/1000 | Loss: 0.00001120
Iteration 117/1000 | Loss: 0.00001120
Iteration 118/1000 | Loss: 0.00001120
Iteration 119/1000 | Loss: 0.00001120
Iteration 120/1000 | Loss: 0.00001120
Iteration 121/1000 | Loss: 0.00001120
Iteration 122/1000 | Loss: 0.00001120
Iteration 123/1000 | Loss: 0.00001120
Iteration 124/1000 | Loss: 0.00001119
Iteration 125/1000 | Loss: 0.00001119
Iteration 126/1000 | Loss: 0.00001119
Iteration 127/1000 | Loss: 0.00001119
Iteration 128/1000 | Loss: 0.00001119
Iteration 129/1000 | Loss: 0.00001119
Iteration 130/1000 | Loss: 0.00001119
Iteration 131/1000 | Loss: 0.00001119
Iteration 132/1000 | Loss: 0.00001119
Iteration 133/1000 | Loss: 0.00001119
Iteration 134/1000 | Loss: 0.00001119
Iteration 135/1000 | Loss: 0.00001119
Iteration 136/1000 | Loss: 0.00001119
Iteration 137/1000 | Loss: 0.00001119
Iteration 138/1000 | Loss: 0.00001119
Iteration 139/1000 | Loss: 0.00001119
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 139. Stopping optimization.
Last 5 losses: [1.1193166756129358e-05, 1.1193166756129358e-05, 1.1193166756129358e-05, 1.1193166756129358e-05, 1.1193166756129358e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1193166756129358e-05

Optimization complete. Final v2v error: 2.8974151611328125 mm

Highest mean error: 3.0770509243011475 mm for frame 68

Lowest mean error: 2.724698066711426 mm for frame 6

Saving results

Total time: 42.9711058139801
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_022/1095/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_022/1095.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_022/1095
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00496364
Iteration 2/25 | Loss: 0.00113692
Iteration 3/25 | Loss: 0.00106587
Iteration 4/25 | Loss: 0.00105430
Iteration 5/25 | Loss: 0.00105028
Iteration 6/25 | Loss: 0.00104948
Iteration 7/25 | Loss: 0.00104948
Iteration 8/25 | Loss: 0.00104948
Iteration 9/25 | Loss: 0.00104948
Iteration 10/25 | Loss: 0.00104948
Iteration 11/25 | Loss: 0.00104948
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010494764428585768, 0.0010494764428585768, 0.0010494764428585768, 0.0010494764428585768, 0.0010494764428585768]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010494764428585768

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.49293733
Iteration 2/25 | Loss: 0.00074313
Iteration 3/25 | Loss: 0.00074313
Iteration 4/25 | Loss: 0.00074313
Iteration 5/25 | Loss: 0.00074312
Iteration 6/25 | Loss: 0.00074312
Iteration 7/25 | Loss: 0.00074312
Iteration 8/25 | Loss: 0.00074312
Iteration 9/25 | Loss: 0.00074312
Iteration 10/25 | Loss: 0.00074312
Iteration 11/25 | Loss: 0.00074312
Iteration 12/25 | Loss: 0.00074312
Iteration 13/25 | Loss: 0.00074312
Iteration 14/25 | Loss: 0.00074312
Iteration 15/25 | Loss: 0.00074312
Iteration 16/25 | Loss: 0.00074312
Iteration 17/25 | Loss: 0.00074312
Iteration 18/25 | Loss: 0.00074312
Iteration 19/25 | Loss: 0.00074312
Iteration 20/25 | Loss: 0.00074312
Iteration 21/25 | Loss: 0.00074312
Iteration 22/25 | Loss: 0.00074312
Iteration 23/25 | Loss: 0.00074312
Iteration 24/25 | Loss: 0.00074312
Iteration 25/25 | Loss: 0.00074312

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00074312
Iteration 2/1000 | Loss: 0.00001797
Iteration 3/1000 | Loss: 0.00001291
Iteration 4/1000 | Loss: 0.00001185
Iteration 5/1000 | Loss: 0.00001130
Iteration 6/1000 | Loss: 0.00001085
Iteration 7/1000 | Loss: 0.00001042
Iteration 8/1000 | Loss: 0.00001024
Iteration 9/1000 | Loss: 0.00001015
Iteration 10/1000 | Loss: 0.00000996
Iteration 11/1000 | Loss: 0.00000986
Iteration 12/1000 | Loss: 0.00000984
Iteration 13/1000 | Loss: 0.00000984
Iteration 14/1000 | Loss: 0.00000980
Iteration 15/1000 | Loss: 0.00000977
Iteration 16/1000 | Loss: 0.00000968
Iteration 17/1000 | Loss: 0.00000968
Iteration 18/1000 | Loss: 0.00000966
Iteration 19/1000 | Loss: 0.00000965
Iteration 20/1000 | Loss: 0.00000955
Iteration 21/1000 | Loss: 0.00000955
Iteration 22/1000 | Loss: 0.00000954
Iteration 23/1000 | Loss: 0.00000954
Iteration 24/1000 | Loss: 0.00000953
Iteration 25/1000 | Loss: 0.00000952
Iteration 26/1000 | Loss: 0.00000951
Iteration 27/1000 | Loss: 0.00000951
Iteration 28/1000 | Loss: 0.00000951
Iteration 29/1000 | Loss: 0.00000950
Iteration 30/1000 | Loss: 0.00000950
Iteration 31/1000 | Loss: 0.00000949
Iteration 32/1000 | Loss: 0.00000948
Iteration 33/1000 | Loss: 0.00000944
Iteration 34/1000 | Loss: 0.00000944
Iteration 35/1000 | Loss: 0.00000944
Iteration 36/1000 | Loss: 0.00000940
Iteration 37/1000 | Loss: 0.00000939
Iteration 38/1000 | Loss: 0.00000939
Iteration 39/1000 | Loss: 0.00000938
Iteration 40/1000 | Loss: 0.00000937
Iteration 41/1000 | Loss: 0.00000937
Iteration 42/1000 | Loss: 0.00000937
Iteration 43/1000 | Loss: 0.00000937
Iteration 44/1000 | Loss: 0.00000937
Iteration 45/1000 | Loss: 0.00000937
Iteration 46/1000 | Loss: 0.00000937
Iteration 47/1000 | Loss: 0.00000937
Iteration 48/1000 | Loss: 0.00000937
Iteration 49/1000 | Loss: 0.00000936
Iteration 50/1000 | Loss: 0.00000936
Iteration 51/1000 | Loss: 0.00000935
Iteration 52/1000 | Loss: 0.00000934
Iteration 53/1000 | Loss: 0.00000934
Iteration 54/1000 | Loss: 0.00000934
Iteration 55/1000 | Loss: 0.00000933
Iteration 56/1000 | Loss: 0.00000933
Iteration 57/1000 | Loss: 0.00000933
Iteration 58/1000 | Loss: 0.00000933
Iteration 59/1000 | Loss: 0.00000933
Iteration 60/1000 | Loss: 0.00000932
Iteration 61/1000 | Loss: 0.00000932
Iteration 62/1000 | Loss: 0.00000931
Iteration 63/1000 | Loss: 0.00000931
Iteration 64/1000 | Loss: 0.00000930
Iteration 65/1000 | Loss: 0.00000929
Iteration 66/1000 | Loss: 0.00000929
Iteration 67/1000 | Loss: 0.00000929
Iteration 68/1000 | Loss: 0.00000929
Iteration 69/1000 | Loss: 0.00000928
Iteration 70/1000 | Loss: 0.00000927
Iteration 71/1000 | Loss: 0.00000927
Iteration 72/1000 | Loss: 0.00000927
Iteration 73/1000 | Loss: 0.00000926
Iteration 74/1000 | Loss: 0.00000925
Iteration 75/1000 | Loss: 0.00000925
Iteration 76/1000 | Loss: 0.00000925
Iteration 77/1000 | Loss: 0.00000924
Iteration 78/1000 | Loss: 0.00000924
Iteration 79/1000 | Loss: 0.00000924
Iteration 80/1000 | Loss: 0.00000924
Iteration 81/1000 | Loss: 0.00000924
Iteration 82/1000 | Loss: 0.00000923
Iteration 83/1000 | Loss: 0.00000923
Iteration 84/1000 | Loss: 0.00000922
Iteration 85/1000 | Loss: 0.00000922
Iteration 86/1000 | Loss: 0.00000922
Iteration 87/1000 | Loss: 0.00000921
Iteration 88/1000 | Loss: 0.00000921
Iteration 89/1000 | Loss: 0.00000921
Iteration 90/1000 | Loss: 0.00000921
Iteration 91/1000 | Loss: 0.00000921
Iteration 92/1000 | Loss: 0.00000921
Iteration 93/1000 | Loss: 0.00000921
Iteration 94/1000 | Loss: 0.00000921
Iteration 95/1000 | Loss: 0.00000921
Iteration 96/1000 | Loss: 0.00000921
Iteration 97/1000 | Loss: 0.00000920
Iteration 98/1000 | Loss: 0.00000920
Iteration 99/1000 | Loss: 0.00000920
Iteration 100/1000 | Loss: 0.00000920
Iteration 101/1000 | Loss: 0.00000920
Iteration 102/1000 | Loss: 0.00000920
Iteration 103/1000 | Loss: 0.00000920
Iteration 104/1000 | Loss: 0.00000919
Iteration 105/1000 | Loss: 0.00000919
Iteration 106/1000 | Loss: 0.00000918
Iteration 107/1000 | Loss: 0.00000918
Iteration 108/1000 | Loss: 0.00000918
Iteration 109/1000 | Loss: 0.00000918
Iteration 110/1000 | Loss: 0.00000918
Iteration 111/1000 | Loss: 0.00000918
Iteration 112/1000 | Loss: 0.00000918
Iteration 113/1000 | Loss: 0.00000918
Iteration 114/1000 | Loss: 0.00000918
Iteration 115/1000 | Loss: 0.00000917
Iteration 116/1000 | Loss: 0.00000916
Iteration 117/1000 | Loss: 0.00000916
Iteration 118/1000 | Loss: 0.00000916
Iteration 119/1000 | Loss: 0.00000916
Iteration 120/1000 | Loss: 0.00000916
Iteration 121/1000 | Loss: 0.00000916
Iteration 122/1000 | Loss: 0.00000916
Iteration 123/1000 | Loss: 0.00000916
Iteration 124/1000 | Loss: 0.00000916
Iteration 125/1000 | Loss: 0.00000916
Iteration 126/1000 | Loss: 0.00000916
Iteration 127/1000 | Loss: 0.00000916
Iteration 128/1000 | Loss: 0.00000915
Iteration 129/1000 | Loss: 0.00000915
Iteration 130/1000 | Loss: 0.00000915
Iteration 131/1000 | Loss: 0.00000915
Iteration 132/1000 | Loss: 0.00000914
Iteration 133/1000 | Loss: 0.00000914
Iteration 134/1000 | Loss: 0.00000914
Iteration 135/1000 | Loss: 0.00000914
Iteration 136/1000 | Loss: 0.00000914
Iteration 137/1000 | Loss: 0.00000913
Iteration 138/1000 | Loss: 0.00000913
Iteration 139/1000 | Loss: 0.00000913
Iteration 140/1000 | Loss: 0.00000913
Iteration 141/1000 | Loss: 0.00000913
Iteration 142/1000 | Loss: 0.00000913
Iteration 143/1000 | Loss: 0.00000913
Iteration 144/1000 | Loss: 0.00000913
Iteration 145/1000 | Loss: 0.00000913
Iteration 146/1000 | Loss: 0.00000912
Iteration 147/1000 | Loss: 0.00000912
Iteration 148/1000 | Loss: 0.00000912
Iteration 149/1000 | Loss: 0.00000912
Iteration 150/1000 | Loss: 0.00000912
Iteration 151/1000 | Loss: 0.00000912
Iteration 152/1000 | Loss: 0.00000911
Iteration 153/1000 | Loss: 0.00000911
Iteration 154/1000 | Loss: 0.00000911
Iteration 155/1000 | Loss: 0.00000911
Iteration 156/1000 | Loss: 0.00000911
Iteration 157/1000 | Loss: 0.00000911
Iteration 158/1000 | Loss: 0.00000911
Iteration 159/1000 | Loss: 0.00000911
Iteration 160/1000 | Loss: 0.00000911
Iteration 161/1000 | Loss: 0.00000911
Iteration 162/1000 | Loss: 0.00000911
Iteration 163/1000 | Loss: 0.00000911
Iteration 164/1000 | Loss: 0.00000911
Iteration 165/1000 | Loss: 0.00000911
Iteration 166/1000 | Loss: 0.00000910
Iteration 167/1000 | Loss: 0.00000910
Iteration 168/1000 | Loss: 0.00000910
Iteration 169/1000 | Loss: 0.00000910
Iteration 170/1000 | Loss: 0.00000910
Iteration 171/1000 | Loss: 0.00000910
Iteration 172/1000 | Loss: 0.00000909
Iteration 173/1000 | Loss: 0.00000909
Iteration 174/1000 | Loss: 0.00000909
Iteration 175/1000 | Loss: 0.00000909
Iteration 176/1000 | Loss: 0.00000909
Iteration 177/1000 | Loss: 0.00000909
Iteration 178/1000 | Loss: 0.00000909
Iteration 179/1000 | Loss: 0.00000909
Iteration 180/1000 | Loss: 0.00000909
Iteration 181/1000 | Loss: 0.00000909
Iteration 182/1000 | Loss: 0.00000909
Iteration 183/1000 | Loss: 0.00000909
Iteration 184/1000 | Loss: 0.00000909
Iteration 185/1000 | Loss: 0.00000909
Iteration 186/1000 | Loss: 0.00000909
Iteration 187/1000 | Loss: 0.00000909
Iteration 188/1000 | Loss: 0.00000909
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 188. Stopping optimization.
Last 5 losses: [9.09107802726794e-06, 9.09107802726794e-06, 9.09107802726794e-06, 9.09107802726794e-06, 9.09107802726794e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.09107802726794e-06

Optimization complete. Final v2v error: 2.6207199096679688 mm

Highest mean error: 2.84717059135437 mm for frame 79

Lowest mean error: 2.4886934757232666 mm for frame 98

Saving results

Total time: 38.85871982574463
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_022/1088/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_022/1088.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_022/1088
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00768397
Iteration 2/25 | Loss: 0.00136988
Iteration 3/25 | Loss: 0.00116367
Iteration 4/25 | Loss: 0.00109925
Iteration 5/25 | Loss: 0.00108619
Iteration 6/25 | Loss: 0.00107991
Iteration 7/25 | Loss: 0.00106815
Iteration 8/25 | Loss: 0.00107921
Iteration 9/25 | Loss: 0.00106735
Iteration 10/25 | Loss: 0.00106676
Iteration 11/25 | Loss: 0.00106674
Iteration 12/25 | Loss: 0.00106674
Iteration 13/25 | Loss: 0.00106674
Iteration 14/25 | Loss: 0.00106674
Iteration 15/25 | Loss: 0.00106674
Iteration 16/25 | Loss: 0.00106674
Iteration 17/25 | Loss: 0.00106674
Iteration 18/25 | Loss: 0.00106674
Iteration 19/25 | Loss: 0.00106673
Iteration 20/25 | Loss: 0.00106673
Iteration 21/25 | Loss: 0.00106673
Iteration 22/25 | Loss: 0.00106673
Iteration 23/25 | Loss: 0.00106673
Iteration 24/25 | Loss: 0.00106673
Iteration 25/25 | Loss: 0.00106673

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.38987374
Iteration 2/25 | Loss: 0.00126844
Iteration 3/25 | Loss: 0.00126842
Iteration 4/25 | Loss: 0.00126841
Iteration 5/25 | Loss: 0.00126841
Iteration 6/25 | Loss: 0.00126841
Iteration 7/25 | Loss: 0.00126841
Iteration 8/25 | Loss: 0.00126841
Iteration 9/25 | Loss: 0.00126841
Iteration 10/25 | Loss: 0.00126841
Iteration 11/25 | Loss: 0.00126841
Iteration 12/25 | Loss: 0.00126841
Iteration 13/25 | Loss: 0.00126841
Iteration 14/25 | Loss: 0.00126841
Iteration 15/25 | Loss: 0.00126841
Iteration 16/25 | Loss: 0.00126841
Iteration 17/25 | Loss: 0.00126841
Iteration 18/25 | Loss: 0.00126841
Iteration 19/25 | Loss: 0.00126841
Iteration 20/25 | Loss: 0.00126841
Iteration 21/25 | Loss: 0.00126841
Iteration 22/25 | Loss: 0.00126841
Iteration 23/25 | Loss: 0.00126841
Iteration 24/25 | Loss: 0.00126841
Iteration 25/25 | Loss: 0.00126841

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00126841
Iteration 2/1000 | Loss: 0.00008267
Iteration 3/1000 | Loss: 0.00001895
Iteration 4/1000 | Loss: 0.00001643
Iteration 5/1000 | Loss: 0.00001588
Iteration 6/1000 | Loss: 0.00001500
Iteration 7/1000 | Loss: 0.00001450
Iteration 8/1000 | Loss: 0.00024240
Iteration 9/1000 | Loss: 0.00001421
Iteration 10/1000 | Loss: 0.00001387
Iteration 11/1000 | Loss: 0.00001379
Iteration 12/1000 | Loss: 0.00001379
Iteration 13/1000 | Loss: 0.00001379
Iteration 14/1000 | Loss: 0.00001354
Iteration 15/1000 | Loss: 0.00001348
Iteration 16/1000 | Loss: 0.00001347
Iteration 17/1000 | Loss: 0.00001343
Iteration 18/1000 | Loss: 0.00001333
Iteration 19/1000 | Loss: 0.00001328
Iteration 20/1000 | Loss: 0.00001325
Iteration 21/1000 | Loss: 0.00001324
Iteration 22/1000 | Loss: 0.00001324
Iteration 23/1000 | Loss: 0.00001323
Iteration 24/1000 | Loss: 0.00001323
Iteration 25/1000 | Loss: 0.00001323
Iteration 26/1000 | Loss: 0.00001322
Iteration 27/1000 | Loss: 0.00001322
Iteration 28/1000 | Loss: 0.00001317
Iteration 29/1000 | Loss: 0.00001317
Iteration 30/1000 | Loss: 0.00001317
Iteration 31/1000 | Loss: 0.00001316
Iteration 32/1000 | Loss: 0.00001316
Iteration 33/1000 | Loss: 0.00001316
Iteration 34/1000 | Loss: 0.00001316
Iteration 35/1000 | Loss: 0.00001309
Iteration 36/1000 | Loss: 0.00001309
Iteration 37/1000 | Loss: 0.00001308
Iteration 38/1000 | Loss: 0.00001307
Iteration 39/1000 | Loss: 0.00001307
Iteration 40/1000 | Loss: 0.00001306
Iteration 41/1000 | Loss: 0.00001306
Iteration 42/1000 | Loss: 0.00001305
Iteration 43/1000 | Loss: 0.00001305
Iteration 44/1000 | Loss: 0.00001305
Iteration 45/1000 | Loss: 0.00001305
Iteration 46/1000 | Loss: 0.00001304
Iteration 47/1000 | Loss: 0.00001304
Iteration 48/1000 | Loss: 0.00001304
Iteration 49/1000 | Loss: 0.00001304
Iteration 50/1000 | Loss: 0.00001304
Iteration 51/1000 | Loss: 0.00001303
Iteration 52/1000 | Loss: 0.00001303
Iteration 53/1000 | Loss: 0.00001303
Iteration 54/1000 | Loss: 0.00001303
Iteration 55/1000 | Loss: 0.00001302
Iteration 56/1000 | Loss: 0.00001302
Iteration 57/1000 | Loss: 0.00001302
Iteration 58/1000 | Loss: 0.00001302
Iteration 59/1000 | Loss: 0.00001302
Iteration 60/1000 | Loss: 0.00001301
Iteration 61/1000 | Loss: 0.00001301
Iteration 62/1000 | Loss: 0.00001301
Iteration 63/1000 | Loss: 0.00001301
Iteration 64/1000 | Loss: 0.00001301
Iteration 65/1000 | Loss: 0.00001301
Iteration 66/1000 | Loss: 0.00001300
Iteration 67/1000 | Loss: 0.00001300
Iteration 68/1000 | Loss: 0.00001300
Iteration 69/1000 | Loss: 0.00001300
Iteration 70/1000 | Loss: 0.00001300
Iteration 71/1000 | Loss: 0.00001299
Iteration 72/1000 | Loss: 0.00001299
Iteration 73/1000 | Loss: 0.00001299
Iteration 74/1000 | Loss: 0.00001299
Iteration 75/1000 | Loss: 0.00001299
Iteration 76/1000 | Loss: 0.00001299
Iteration 77/1000 | Loss: 0.00001298
Iteration 78/1000 | Loss: 0.00001298
Iteration 79/1000 | Loss: 0.00001298
Iteration 80/1000 | Loss: 0.00001298
Iteration 81/1000 | Loss: 0.00001298
Iteration 82/1000 | Loss: 0.00001298
Iteration 83/1000 | Loss: 0.00001298
Iteration 84/1000 | Loss: 0.00001298
Iteration 85/1000 | Loss: 0.00001298
Iteration 86/1000 | Loss: 0.00001298
Iteration 87/1000 | Loss: 0.00001298
Iteration 88/1000 | Loss: 0.00001298
Iteration 89/1000 | Loss: 0.00001298
Iteration 90/1000 | Loss: 0.00001297
Iteration 91/1000 | Loss: 0.00001297
Iteration 92/1000 | Loss: 0.00001297
Iteration 93/1000 | Loss: 0.00001297
Iteration 94/1000 | Loss: 0.00001297
Iteration 95/1000 | Loss: 0.00001297
Iteration 96/1000 | Loss: 0.00001296
Iteration 97/1000 | Loss: 0.00001296
Iteration 98/1000 | Loss: 0.00001296
Iteration 99/1000 | Loss: 0.00001296
Iteration 100/1000 | Loss: 0.00001296
Iteration 101/1000 | Loss: 0.00001296
Iteration 102/1000 | Loss: 0.00001296
Iteration 103/1000 | Loss: 0.00001296
Iteration 104/1000 | Loss: 0.00001296
Iteration 105/1000 | Loss: 0.00001296
Iteration 106/1000 | Loss: 0.00001296
Iteration 107/1000 | Loss: 0.00001295
Iteration 108/1000 | Loss: 0.00001295
Iteration 109/1000 | Loss: 0.00001295
Iteration 110/1000 | Loss: 0.00001295
Iteration 111/1000 | Loss: 0.00001295
Iteration 112/1000 | Loss: 0.00001294
Iteration 113/1000 | Loss: 0.00001294
Iteration 114/1000 | Loss: 0.00001294
Iteration 115/1000 | Loss: 0.00001294
Iteration 116/1000 | Loss: 0.00001294
Iteration 117/1000 | Loss: 0.00001294
Iteration 118/1000 | Loss: 0.00001294
Iteration 119/1000 | Loss: 0.00001294
Iteration 120/1000 | Loss: 0.00001294
Iteration 121/1000 | Loss: 0.00001294
Iteration 122/1000 | Loss: 0.00001294
Iteration 123/1000 | Loss: 0.00001294
Iteration 124/1000 | Loss: 0.00001294
Iteration 125/1000 | Loss: 0.00001293
Iteration 126/1000 | Loss: 0.00001293
Iteration 127/1000 | Loss: 0.00001293
Iteration 128/1000 | Loss: 0.00001293
Iteration 129/1000 | Loss: 0.00001293
Iteration 130/1000 | Loss: 0.00001293
Iteration 131/1000 | Loss: 0.00001293
Iteration 132/1000 | Loss: 0.00001293
Iteration 133/1000 | Loss: 0.00001293
Iteration 134/1000 | Loss: 0.00001293
Iteration 135/1000 | Loss: 0.00001293
Iteration 136/1000 | Loss: 0.00001293
Iteration 137/1000 | Loss: 0.00001293
Iteration 138/1000 | Loss: 0.00001292
Iteration 139/1000 | Loss: 0.00001292
Iteration 140/1000 | Loss: 0.00001292
Iteration 141/1000 | Loss: 0.00001292
Iteration 142/1000 | Loss: 0.00001292
Iteration 143/1000 | Loss: 0.00001292
Iteration 144/1000 | Loss: 0.00001291
Iteration 145/1000 | Loss: 0.00001291
Iteration 146/1000 | Loss: 0.00001291
Iteration 147/1000 | Loss: 0.00001291
Iteration 148/1000 | Loss: 0.00001291
Iteration 149/1000 | Loss: 0.00001291
Iteration 150/1000 | Loss: 0.00001291
Iteration 151/1000 | Loss: 0.00001291
Iteration 152/1000 | Loss: 0.00001291
Iteration 153/1000 | Loss: 0.00001291
Iteration 154/1000 | Loss: 0.00001291
Iteration 155/1000 | Loss: 0.00001291
Iteration 156/1000 | Loss: 0.00001291
Iteration 157/1000 | Loss: 0.00001291
Iteration 158/1000 | Loss: 0.00001291
Iteration 159/1000 | Loss: 0.00001291
Iteration 160/1000 | Loss: 0.00001290
Iteration 161/1000 | Loss: 0.00001290
Iteration 162/1000 | Loss: 0.00001290
Iteration 163/1000 | Loss: 0.00001290
Iteration 164/1000 | Loss: 0.00001289
Iteration 165/1000 | Loss: 0.00001289
Iteration 166/1000 | Loss: 0.00001289
Iteration 167/1000 | Loss: 0.00001289
Iteration 168/1000 | Loss: 0.00001289
Iteration 169/1000 | Loss: 0.00001289
Iteration 170/1000 | Loss: 0.00001289
Iteration 171/1000 | Loss: 0.00001289
Iteration 172/1000 | Loss: 0.00001289
Iteration 173/1000 | Loss: 0.00001289
Iteration 174/1000 | Loss: 0.00001289
Iteration 175/1000 | Loss: 0.00001289
Iteration 176/1000 | Loss: 0.00001288
Iteration 177/1000 | Loss: 0.00001288
Iteration 178/1000 | Loss: 0.00001288
Iteration 179/1000 | Loss: 0.00001288
Iteration 180/1000 | Loss: 0.00001288
Iteration 181/1000 | Loss: 0.00001288
Iteration 182/1000 | Loss: 0.00001288
Iteration 183/1000 | Loss: 0.00001288
Iteration 184/1000 | Loss: 0.00001288
Iteration 185/1000 | Loss: 0.00001288
Iteration 186/1000 | Loss: 0.00001288
Iteration 187/1000 | Loss: 0.00001288
Iteration 188/1000 | Loss: 0.00001288
Iteration 189/1000 | Loss: 0.00001288
Iteration 190/1000 | Loss: 0.00001288
Iteration 191/1000 | Loss: 0.00001288
Iteration 192/1000 | Loss: 0.00001288
Iteration 193/1000 | Loss: 0.00001288
Iteration 194/1000 | Loss: 0.00001287
Iteration 195/1000 | Loss: 0.00001287
Iteration 196/1000 | Loss: 0.00001287
Iteration 197/1000 | Loss: 0.00001287
Iteration 198/1000 | Loss: 0.00001287
Iteration 199/1000 | Loss: 0.00001287
Iteration 200/1000 | Loss: 0.00001287
Iteration 201/1000 | Loss: 0.00001287
Iteration 202/1000 | Loss: 0.00001287
Iteration 203/1000 | Loss: 0.00001287
Iteration 204/1000 | Loss: 0.00001287
Iteration 205/1000 | Loss: 0.00001287
Iteration 206/1000 | Loss: 0.00001287
Iteration 207/1000 | Loss: 0.00001287
Iteration 208/1000 | Loss: 0.00001287
Iteration 209/1000 | Loss: 0.00001287
Iteration 210/1000 | Loss: 0.00001287
Iteration 211/1000 | Loss: 0.00001286
Iteration 212/1000 | Loss: 0.00001286
Iteration 213/1000 | Loss: 0.00001286
Iteration 214/1000 | Loss: 0.00001286
Iteration 215/1000 | Loss: 0.00001286
Iteration 216/1000 | Loss: 0.00001286
Iteration 217/1000 | Loss: 0.00001286
Iteration 218/1000 | Loss: 0.00001286
Iteration 219/1000 | Loss: 0.00001286
Iteration 220/1000 | Loss: 0.00001286
Iteration 221/1000 | Loss: 0.00001286
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 221. Stopping optimization.
Last 5 losses: [1.2862706171290483e-05, 1.2862706171290483e-05, 1.2862706171290483e-05, 1.2862706171290483e-05, 1.2862706171290483e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2862706171290483e-05

Optimization complete. Final v2v error: 3.0858681201934814 mm

Highest mean error: 3.610588788986206 mm for frame 53

Lowest mean error: 2.728407144546509 mm for frame 162

Saving results

Total time: 54.006447076797485
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_022/1096/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_022/1096.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_022/1096
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00819580
Iteration 2/25 | Loss: 0.00137645
Iteration 3/25 | Loss: 0.00116833
Iteration 4/25 | Loss: 0.00114826
Iteration 5/25 | Loss: 0.00114665
Iteration 6/25 | Loss: 0.00114665
Iteration 7/25 | Loss: 0.00114665
Iteration 8/25 | Loss: 0.00114665
Iteration 9/25 | Loss: 0.00114665
Iteration 10/25 | Loss: 0.00114665
Iteration 11/25 | Loss: 0.00114665
Iteration 12/25 | Loss: 0.00114665
Iteration 13/25 | Loss: 0.00114665
Iteration 14/25 | Loss: 0.00114665
Iteration 15/25 | Loss: 0.00114665
Iteration 16/25 | Loss: 0.00114665
Iteration 17/25 | Loss: 0.00114665
Iteration 18/25 | Loss: 0.00114665
Iteration 19/25 | Loss: 0.00114665
Iteration 20/25 | Loss: 0.00114665
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0011466502910479903, 0.0011466502910479903, 0.0011466502910479903, 0.0011466502910479903, 0.0011466502910479903]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011466502910479903

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.97334039
Iteration 2/25 | Loss: 0.00043363
Iteration 3/25 | Loss: 0.00043362
Iteration 4/25 | Loss: 0.00043362
Iteration 5/25 | Loss: 0.00043362
Iteration 6/25 | Loss: 0.00043362
Iteration 7/25 | Loss: 0.00043362
Iteration 8/25 | Loss: 0.00043362
Iteration 9/25 | Loss: 0.00043362
Iteration 10/25 | Loss: 0.00043362
Iteration 11/25 | Loss: 0.00043362
Iteration 12/25 | Loss: 0.00043362
Iteration 13/25 | Loss: 0.00043362
Iteration 14/25 | Loss: 0.00043362
Iteration 15/25 | Loss: 0.00043362
Iteration 16/25 | Loss: 0.00043362
Iteration 17/25 | Loss: 0.00043362
Iteration 18/25 | Loss: 0.00043362
Iteration 19/25 | Loss: 0.00043362
Iteration 20/25 | Loss: 0.00043362
Iteration 21/25 | Loss: 0.00043362
Iteration 22/25 | Loss: 0.00043362
Iteration 23/25 | Loss: 0.00043362
Iteration 24/25 | Loss: 0.00043362
Iteration 25/25 | Loss: 0.00043362

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00043362
Iteration 2/1000 | Loss: 0.00003220
Iteration 3/1000 | Loss: 0.00002350
Iteration 4/1000 | Loss: 0.00002166
Iteration 5/1000 | Loss: 0.00002091
Iteration 6/1000 | Loss: 0.00002042
Iteration 7/1000 | Loss: 0.00001982
Iteration 8/1000 | Loss: 0.00001948
Iteration 9/1000 | Loss: 0.00001930
Iteration 10/1000 | Loss: 0.00001910
Iteration 11/1000 | Loss: 0.00001899
Iteration 12/1000 | Loss: 0.00001896
Iteration 13/1000 | Loss: 0.00001895
Iteration 14/1000 | Loss: 0.00001893
Iteration 15/1000 | Loss: 0.00001893
Iteration 16/1000 | Loss: 0.00001892
Iteration 17/1000 | Loss: 0.00001891
Iteration 18/1000 | Loss: 0.00001890
Iteration 19/1000 | Loss: 0.00001890
Iteration 20/1000 | Loss: 0.00001889
Iteration 21/1000 | Loss: 0.00001887
Iteration 22/1000 | Loss: 0.00001886
Iteration 23/1000 | Loss: 0.00001886
Iteration 24/1000 | Loss: 0.00001886
Iteration 25/1000 | Loss: 0.00001875
Iteration 26/1000 | Loss: 0.00001872
Iteration 27/1000 | Loss: 0.00001872
Iteration 28/1000 | Loss: 0.00001868
Iteration 29/1000 | Loss: 0.00001867
Iteration 30/1000 | Loss: 0.00001867
Iteration 31/1000 | Loss: 0.00001867
Iteration 32/1000 | Loss: 0.00001866
Iteration 33/1000 | Loss: 0.00001864
Iteration 34/1000 | Loss: 0.00001864
Iteration 35/1000 | Loss: 0.00001864
Iteration 36/1000 | Loss: 0.00001864
Iteration 37/1000 | Loss: 0.00001864
Iteration 38/1000 | Loss: 0.00001864
Iteration 39/1000 | Loss: 0.00001863
Iteration 40/1000 | Loss: 0.00001862
Iteration 41/1000 | Loss: 0.00001862
Iteration 42/1000 | Loss: 0.00001861
Iteration 43/1000 | Loss: 0.00001861
Iteration 44/1000 | Loss: 0.00001860
Iteration 45/1000 | Loss: 0.00001860
Iteration 46/1000 | Loss: 0.00001860
Iteration 47/1000 | Loss: 0.00001859
Iteration 48/1000 | Loss: 0.00001859
Iteration 49/1000 | Loss: 0.00001858
Iteration 50/1000 | Loss: 0.00001857
Iteration 51/1000 | Loss: 0.00001856
Iteration 52/1000 | Loss: 0.00001856
Iteration 53/1000 | Loss: 0.00001856
Iteration 54/1000 | Loss: 0.00001856
Iteration 55/1000 | Loss: 0.00001856
Iteration 56/1000 | Loss: 0.00001856
Iteration 57/1000 | Loss: 0.00001856
Iteration 58/1000 | Loss: 0.00001856
Iteration 59/1000 | Loss: 0.00001856
Iteration 60/1000 | Loss: 0.00001856
Iteration 61/1000 | Loss: 0.00001856
Iteration 62/1000 | Loss: 0.00001855
Iteration 63/1000 | Loss: 0.00001855
Iteration 64/1000 | Loss: 0.00001854
Iteration 65/1000 | Loss: 0.00001854
Iteration 66/1000 | Loss: 0.00001854
Iteration 67/1000 | Loss: 0.00001854
Iteration 68/1000 | Loss: 0.00001854
Iteration 69/1000 | Loss: 0.00001854
Iteration 70/1000 | Loss: 0.00001853
Iteration 71/1000 | Loss: 0.00001853
Iteration 72/1000 | Loss: 0.00001853
Iteration 73/1000 | Loss: 0.00001853
Iteration 74/1000 | Loss: 0.00001852
Iteration 75/1000 | Loss: 0.00001850
Iteration 76/1000 | Loss: 0.00001850
Iteration 77/1000 | Loss: 0.00001850
Iteration 78/1000 | Loss: 0.00001850
Iteration 79/1000 | Loss: 0.00001850
Iteration 80/1000 | Loss: 0.00001850
Iteration 81/1000 | Loss: 0.00001849
Iteration 82/1000 | Loss: 0.00001849
Iteration 83/1000 | Loss: 0.00001849
Iteration 84/1000 | Loss: 0.00001849
Iteration 85/1000 | Loss: 0.00001849
Iteration 86/1000 | Loss: 0.00001847
Iteration 87/1000 | Loss: 0.00001847
Iteration 88/1000 | Loss: 0.00001847
Iteration 89/1000 | Loss: 0.00001847
Iteration 90/1000 | Loss: 0.00001847
Iteration 91/1000 | Loss: 0.00001846
Iteration 92/1000 | Loss: 0.00001846
Iteration 93/1000 | Loss: 0.00001846
Iteration 94/1000 | Loss: 0.00001845
Iteration 95/1000 | Loss: 0.00001845
Iteration 96/1000 | Loss: 0.00001845
Iteration 97/1000 | Loss: 0.00001845
Iteration 98/1000 | Loss: 0.00001844
Iteration 99/1000 | Loss: 0.00001844
Iteration 100/1000 | Loss: 0.00001844
Iteration 101/1000 | Loss: 0.00001844
Iteration 102/1000 | Loss: 0.00001844
Iteration 103/1000 | Loss: 0.00001843
Iteration 104/1000 | Loss: 0.00001843
Iteration 105/1000 | Loss: 0.00001842
Iteration 106/1000 | Loss: 0.00001841
Iteration 107/1000 | Loss: 0.00001840
Iteration 108/1000 | Loss: 0.00001840
Iteration 109/1000 | Loss: 0.00001839
Iteration 110/1000 | Loss: 0.00001839
Iteration 111/1000 | Loss: 0.00001839
Iteration 112/1000 | Loss: 0.00001839
Iteration 113/1000 | Loss: 0.00001839
Iteration 114/1000 | Loss: 0.00001839
Iteration 115/1000 | Loss: 0.00001839
Iteration 116/1000 | Loss: 0.00001838
Iteration 117/1000 | Loss: 0.00001837
Iteration 118/1000 | Loss: 0.00001837
Iteration 119/1000 | Loss: 0.00001837
Iteration 120/1000 | Loss: 0.00001837
Iteration 121/1000 | Loss: 0.00001837
Iteration 122/1000 | Loss: 0.00001836
Iteration 123/1000 | Loss: 0.00001835
Iteration 124/1000 | Loss: 0.00001835
Iteration 125/1000 | Loss: 0.00001835
Iteration 126/1000 | Loss: 0.00001835
Iteration 127/1000 | Loss: 0.00001834
Iteration 128/1000 | Loss: 0.00001834
Iteration 129/1000 | Loss: 0.00001834
Iteration 130/1000 | Loss: 0.00001834
Iteration 131/1000 | Loss: 0.00001834
Iteration 132/1000 | Loss: 0.00001834
Iteration 133/1000 | Loss: 0.00001834
Iteration 134/1000 | Loss: 0.00001834
Iteration 135/1000 | Loss: 0.00001834
Iteration 136/1000 | Loss: 0.00001834
Iteration 137/1000 | Loss: 0.00001834
Iteration 138/1000 | Loss: 0.00001834
Iteration 139/1000 | Loss: 0.00001834
Iteration 140/1000 | Loss: 0.00001834
Iteration 141/1000 | Loss: 0.00001833
Iteration 142/1000 | Loss: 0.00001833
Iteration 143/1000 | Loss: 0.00001833
Iteration 144/1000 | Loss: 0.00001833
Iteration 145/1000 | Loss: 0.00001833
Iteration 146/1000 | Loss: 0.00001833
Iteration 147/1000 | Loss: 0.00001833
Iteration 148/1000 | Loss: 0.00001833
Iteration 149/1000 | Loss: 0.00001833
Iteration 150/1000 | Loss: 0.00001833
Iteration 151/1000 | Loss: 0.00001833
Iteration 152/1000 | Loss: 0.00001833
Iteration 153/1000 | Loss: 0.00001833
Iteration 154/1000 | Loss: 0.00001833
Iteration 155/1000 | Loss: 0.00001833
Iteration 156/1000 | Loss: 0.00001833
Iteration 157/1000 | Loss: 0.00001833
Iteration 158/1000 | Loss: 0.00001833
Iteration 159/1000 | Loss: 0.00001833
Iteration 160/1000 | Loss: 0.00001833
Iteration 161/1000 | Loss: 0.00001833
Iteration 162/1000 | Loss: 0.00001833
Iteration 163/1000 | Loss: 0.00001833
Iteration 164/1000 | Loss: 0.00001833
Iteration 165/1000 | Loss: 0.00001833
Iteration 166/1000 | Loss: 0.00001833
Iteration 167/1000 | Loss: 0.00001833
Iteration 168/1000 | Loss: 0.00001833
Iteration 169/1000 | Loss: 0.00001833
Iteration 170/1000 | Loss: 0.00001833
Iteration 171/1000 | Loss: 0.00001833
Iteration 172/1000 | Loss: 0.00001833
Iteration 173/1000 | Loss: 0.00001833
Iteration 174/1000 | Loss: 0.00001833
Iteration 175/1000 | Loss: 0.00001833
Iteration 176/1000 | Loss: 0.00001833
Iteration 177/1000 | Loss: 0.00001833
Iteration 178/1000 | Loss: 0.00001833
Iteration 179/1000 | Loss: 0.00001833
Iteration 180/1000 | Loss: 0.00001833
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 180. Stopping optimization.
Last 5 losses: [1.832595262385439e-05, 1.832595262385439e-05, 1.832595262385439e-05, 1.832595262385439e-05, 1.832595262385439e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.832595262385439e-05

Optimization complete. Final v2v error: 3.534148693084717 mm

Highest mean error: 3.6608176231384277 mm for frame 108

Lowest mean error: 3.417865037918091 mm for frame 37

Saving results

Total time: 37.40146255493164
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_022/1076/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_022/1076.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_022/1076
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00771853
Iteration 2/25 | Loss: 0.00159882
Iteration 3/25 | Loss: 0.00124410
Iteration 4/25 | Loss: 0.00119413
Iteration 5/25 | Loss: 0.00121571
Iteration 6/25 | Loss: 0.00119890
Iteration 7/25 | Loss: 0.00119087
Iteration 8/25 | Loss: 0.00118492
Iteration 9/25 | Loss: 0.00117727
Iteration 10/25 | Loss: 0.00117542
Iteration 11/25 | Loss: 0.00117518
Iteration 12/25 | Loss: 0.00117516
Iteration 13/25 | Loss: 0.00117516
Iteration 14/25 | Loss: 0.00117516
Iteration 15/25 | Loss: 0.00117516
Iteration 16/25 | Loss: 0.00117516
Iteration 17/25 | Loss: 0.00117515
Iteration 18/25 | Loss: 0.00117515
Iteration 19/25 | Loss: 0.00117515
Iteration 20/25 | Loss: 0.00117515
Iteration 21/25 | Loss: 0.00117515
Iteration 22/25 | Loss: 0.00117515
Iteration 23/25 | Loss: 0.00117515
Iteration 24/25 | Loss: 0.00117515
Iteration 25/25 | Loss: 0.00117515

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.49331093
Iteration 2/25 | Loss: 0.00061746
Iteration 3/25 | Loss: 0.00061744
Iteration 4/25 | Loss: 0.00061744
Iteration 5/25 | Loss: 0.00061744
Iteration 6/25 | Loss: 0.00061744
Iteration 7/25 | Loss: 0.00061744
Iteration 8/25 | Loss: 0.00061744
Iteration 9/25 | Loss: 0.00061744
Iteration 10/25 | Loss: 0.00061744
Iteration 11/25 | Loss: 0.00061744
Iteration 12/25 | Loss: 0.00061744
Iteration 13/25 | Loss: 0.00061744
Iteration 14/25 | Loss: 0.00061744
Iteration 15/25 | Loss: 0.00061744
Iteration 16/25 | Loss: 0.00061744
Iteration 17/25 | Loss: 0.00061744
Iteration 18/25 | Loss: 0.00061744
Iteration 19/25 | Loss: 0.00061744
Iteration 20/25 | Loss: 0.00061744
Iteration 21/25 | Loss: 0.00061744
Iteration 22/25 | Loss: 0.00061744
Iteration 23/25 | Loss: 0.00061744
Iteration 24/25 | Loss: 0.00061744
Iteration 25/25 | Loss: 0.00061744
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0006174404406920075, 0.0006174404406920075, 0.0006174404406920075, 0.0006174404406920075, 0.0006174404406920075]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006174404406920075

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00061744
Iteration 2/1000 | Loss: 0.00002483
Iteration 3/1000 | Loss: 0.00014499
Iteration 4/1000 | Loss: 0.00002139
Iteration 5/1000 | Loss: 0.00001737
Iteration 6/1000 | Loss: 0.00001667
Iteration 7/1000 | Loss: 0.00018274
Iteration 8/1000 | Loss: 0.00052969
Iteration 9/1000 | Loss: 0.00012495
Iteration 10/1000 | Loss: 0.00059259
Iteration 11/1000 | Loss: 0.00003880
Iteration 12/1000 | Loss: 0.00030954
Iteration 13/1000 | Loss: 0.00058767
Iteration 14/1000 | Loss: 0.00001849
Iteration 15/1000 | Loss: 0.00001613
Iteration 16/1000 | Loss: 0.00001559
Iteration 17/1000 | Loss: 0.00001527
Iteration 18/1000 | Loss: 0.00001518
Iteration 19/1000 | Loss: 0.00001517
Iteration 20/1000 | Loss: 0.00001511
Iteration 21/1000 | Loss: 0.00001507
Iteration 22/1000 | Loss: 0.00001503
Iteration 23/1000 | Loss: 0.00001502
Iteration 24/1000 | Loss: 0.00001501
Iteration 25/1000 | Loss: 0.00001499
Iteration 26/1000 | Loss: 0.00001499
Iteration 27/1000 | Loss: 0.00001492
Iteration 28/1000 | Loss: 0.00001489
Iteration 29/1000 | Loss: 0.00001488
Iteration 30/1000 | Loss: 0.00001488
Iteration 31/1000 | Loss: 0.00001487
Iteration 32/1000 | Loss: 0.00001487
Iteration 33/1000 | Loss: 0.00001486
Iteration 34/1000 | Loss: 0.00001486
Iteration 35/1000 | Loss: 0.00001485
Iteration 36/1000 | Loss: 0.00001485
Iteration 37/1000 | Loss: 0.00001485
Iteration 38/1000 | Loss: 0.00001484
Iteration 39/1000 | Loss: 0.00001483
Iteration 40/1000 | Loss: 0.00001482
Iteration 41/1000 | Loss: 0.00001482
Iteration 42/1000 | Loss: 0.00001482
Iteration 43/1000 | Loss: 0.00001482
Iteration 44/1000 | Loss: 0.00001482
Iteration 45/1000 | Loss: 0.00001481
Iteration 46/1000 | Loss: 0.00001481
Iteration 47/1000 | Loss: 0.00001481
Iteration 48/1000 | Loss: 0.00001481
Iteration 49/1000 | Loss: 0.00001481
Iteration 50/1000 | Loss: 0.00001480
Iteration 51/1000 | Loss: 0.00001480
Iteration 52/1000 | Loss: 0.00001480
Iteration 53/1000 | Loss: 0.00001480
Iteration 54/1000 | Loss: 0.00001480
Iteration 55/1000 | Loss: 0.00001480
Iteration 56/1000 | Loss: 0.00001479
Iteration 57/1000 | Loss: 0.00001479
Iteration 58/1000 | Loss: 0.00001478
Iteration 59/1000 | Loss: 0.00001478
Iteration 60/1000 | Loss: 0.00001478
Iteration 61/1000 | Loss: 0.00001478
Iteration 62/1000 | Loss: 0.00001477
Iteration 63/1000 | Loss: 0.00001477
Iteration 64/1000 | Loss: 0.00001477
Iteration 65/1000 | Loss: 0.00001476
Iteration 66/1000 | Loss: 0.00001476
Iteration 67/1000 | Loss: 0.00001476
Iteration 68/1000 | Loss: 0.00001476
Iteration 69/1000 | Loss: 0.00001475
Iteration 70/1000 | Loss: 0.00001475
Iteration 71/1000 | Loss: 0.00001475
Iteration 72/1000 | Loss: 0.00001475
Iteration 73/1000 | Loss: 0.00001475
Iteration 74/1000 | Loss: 0.00001474
Iteration 75/1000 | Loss: 0.00001474
Iteration 76/1000 | Loss: 0.00001474
Iteration 77/1000 | Loss: 0.00001474
Iteration 78/1000 | Loss: 0.00001474
Iteration 79/1000 | Loss: 0.00001474
Iteration 80/1000 | Loss: 0.00001474
Iteration 81/1000 | Loss: 0.00001474
Iteration 82/1000 | Loss: 0.00001473
Iteration 83/1000 | Loss: 0.00001473
Iteration 84/1000 | Loss: 0.00001473
Iteration 85/1000 | Loss: 0.00001473
Iteration 86/1000 | Loss: 0.00001473
Iteration 87/1000 | Loss: 0.00001473
Iteration 88/1000 | Loss: 0.00001473
Iteration 89/1000 | Loss: 0.00001473
Iteration 90/1000 | Loss: 0.00001472
Iteration 91/1000 | Loss: 0.00001472
Iteration 92/1000 | Loss: 0.00001472
Iteration 93/1000 | Loss: 0.00001472
Iteration 94/1000 | Loss: 0.00001472
Iteration 95/1000 | Loss: 0.00001472
Iteration 96/1000 | Loss: 0.00001472
Iteration 97/1000 | Loss: 0.00001472
Iteration 98/1000 | Loss: 0.00001472
Iteration 99/1000 | Loss: 0.00001472
Iteration 100/1000 | Loss: 0.00001472
Iteration 101/1000 | Loss: 0.00001472
Iteration 102/1000 | Loss: 0.00012138
Iteration 103/1000 | Loss: 0.00001486
Iteration 104/1000 | Loss: 0.00001472
Iteration 105/1000 | Loss: 0.00001472
Iteration 106/1000 | Loss: 0.00001469
Iteration 107/1000 | Loss: 0.00001469
Iteration 108/1000 | Loss: 0.00001468
Iteration 109/1000 | Loss: 0.00001468
Iteration 110/1000 | Loss: 0.00001467
Iteration 111/1000 | Loss: 0.00001467
Iteration 112/1000 | Loss: 0.00001467
Iteration 113/1000 | Loss: 0.00001467
Iteration 114/1000 | Loss: 0.00001466
Iteration 115/1000 | Loss: 0.00001466
Iteration 116/1000 | Loss: 0.00001466
Iteration 117/1000 | Loss: 0.00001466
Iteration 118/1000 | Loss: 0.00001466
Iteration 119/1000 | Loss: 0.00001466
Iteration 120/1000 | Loss: 0.00001466
Iteration 121/1000 | Loss: 0.00001466
Iteration 122/1000 | Loss: 0.00001466
Iteration 123/1000 | Loss: 0.00001466
Iteration 124/1000 | Loss: 0.00001466
Iteration 125/1000 | Loss: 0.00001465
Iteration 126/1000 | Loss: 0.00001465
Iteration 127/1000 | Loss: 0.00001465
Iteration 128/1000 | Loss: 0.00001465
Iteration 129/1000 | Loss: 0.00001465
Iteration 130/1000 | Loss: 0.00001464
Iteration 131/1000 | Loss: 0.00001464
Iteration 132/1000 | Loss: 0.00001464
Iteration 133/1000 | Loss: 0.00001464
Iteration 134/1000 | Loss: 0.00001464
Iteration 135/1000 | Loss: 0.00001464
Iteration 136/1000 | Loss: 0.00001464
Iteration 137/1000 | Loss: 0.00001464
Iteration 138/1000 | Loss: 0.00001464
Iteration 139/1000 | Loss: 0.00001464
Iteration 140/1000 | Loss: 0.00001463
Iteration 141/1000 | Loss: 0.00001463
Iteration 142/1000 | Loss: 0.00001463
Iteration 143/1000 | Loss: 0.00001463
Iteration 144/1000 | Loss: 0.00001463
Iteration 145/1000 | Loss: 0.00001463
Iteration 146/1000 | Loss: 0.00001463
Iteration 147/1000 | Loss: 0.00001463
Iteration 148/1000 | Loss: 0.00001463
Iteration 149/1000 | Loss: 0.00001463
Iteration 150/1000 | Loss: 0.00001463
Iteration 151/1000 | Loss: 0.00001463
Iteration 152/1000 | Loss: 0.00001463
Iteration 153/1000 | Loss: 0.00001463
Iteration 154/1000 | Loss: 0.00001463
Iteration 155/1000 | Loss: 0.00001463
Iteration 156/1000 | Loss: 0.00001463
Iteration 157/1000 | Loss: 0.00001463
Iteration 158/1000 | Loss: 0.00001463
Iteration 159/1000 | Loss: 0.00001462
Iteration 160/1000 | Loss: 0.00001462
Iteration 161/1000 | Loss: 0.00001462
Iteration 162/1000 | Loss: 0.00001462
Iteration 163/1000 | Loss: 0.00001462
Iteration 164/1000 | Loss: 0.00001462
Iteration 165/1000 | Loss: 0.00001462
Iteration 166/1000 | Loss: 0.00001462
Iteration 167/1000 | Loss: 0.00001462
Iteration 168/1000 | Loss: 0.00001462
Iteration 169/1000 | Loss: 0.00001462
Iteration 170/1000 | Loss: 0.00001462
Iteration 171/1000 | Loss: 0.00001462
Iteration 172/1000 | Loss: 0.00001462
Iteration 173/1000 | Loss: 0.00001461
Iteration 174/1000 | Loss: 0.00001461
Iteration 175/1000 | Loss: 0.00001461
Iteration 176/1000 | Loss: 0.00001461
Iteration 177/1000 | Loss: 0.00001461
Iteration 178/1000 | Loss: 0.00001461
Iteration 179/1000 | Loss: 0.00001461
Iteration 180/1000 | Loss: 0.00001461
Iteration 181/1000 | Loss: 0.00001461
Iteration 182/1000 | Loss: 0.00001461
Iteration 183/1000 | Loss: 0.00001461
Iteration 184/1000 | Loss: 0.00001461
Iteration 185/1000 | Loss: 0.00001461
Iteration 186/1000 | Loss: 0.00001461
Iteration 187/1000 | Loss: 0.00001460
Iteration 188/1000 | Loss: 0.00001460
Iteration 189/1000 | Loss: 0.00001460
Iteration 190/1000 | Loss: 0.00014989
Iteration 191/1000 | Loss: 0.00014989
Iteration 192/1000 | Loss: 0.00002243
Iteration 193/1000 | Loss: 0.00006551
Iteration 194/1000 | Loss: 0.00003106
Iteration 195/1000 | Loss: 0.00001513
Iteration 196/1000 | Loss: 0.00002141
Iteration 197/1000 | Loss: 0.00001480
Iteration 198/1000 | Loss: 0.00001471
Iteration 199/1000 | Loss: 0.00001470
Iteration 200/1000 | Loss: 0.00001470
Iteration 201/1000 | Loss: 0.00001469
Iteration 202/1000 | Loss: 0.00001459
Iteration 203/1000 | Loss: 0.00001459
Iteration 204/1000 | Loss: 0.00001458
Iteration 205/1000 | Loss: 0.00001458
Iteration 206/1000 | Loss: 0.00001458
Iteration 207/1000 | Loss: 0.00001457
Iteration 208/1000 | Loss: 0.00001457
Iteration 209/1000 | Loss: 0.00001457
Iteration 210/1000 | Loss: 0.00001456
Iteration 211/1000 | Loss: 0.00001456
Iteration 212/1000 | Loss: 0.00001456
Iteration 213/1000 | Loss: 0.00001456
Iteration 214/1000 | Loss: 0.00001456
Iteration 215/1000 | Loss: 0.00001455
Iteration 216/1000 | Loss: 0.00001455
Iteration 217/1000 | Loss: 0.00001455
Iteration 218/1000 | Loss: 0.00001455
Iteration 219/1000 | Loss: 0.00001455
Iteration 220/1000 | Loss: 0.00001455
Iteration 221/1000 | Loss: 0.00001455
Iteration 222/1000 | Loss: 0.00001455
Iteration 223/1000 | Loss: 0.00001455
Iteration 224/1000 | Loss: 0.00001455
Iteration 225/1000 | Loss: 0.00001454
Iteration 226/1000 | Loss: 0.00001454
Iteration 227/1000 | Loss: 0.00001454
Iteration 228/1000 | Loss: 0.00001454
Iteration 229/1000 | Loss: 0.00001454
Iteration 230/1000 | Loss: 0.00001454
Iteration 231/1000 | Loss: 0.00001454
Iteration 232/1000 | Loss: 0.00001454
Iteration 233/1000 | Loss: 0.00001454
Iteration 234/1000 | Loss: 0.00001454
Iteration 235/1000 | Loss: 0.00001454
Iteration 236/1000 | Loss: 0.00001454
Iteration 237/1000 | Loss: 0.00001454
Iteration 238/1000 | Loss: 0.00001454
Iteration 239/1000 | Loss: 0.00001454
Iteration 240/1000 | Loss: 0.00001454
Iteration 241/1000 | Loss: 0.00001454
Iteration 242/1000 | Loss: 0.00001454
Iteration 243/1000 | Loss: 0.00001454
Iteration 244/1000 | Loss: 0.00001454
Iteration 245/1000 | Loss: 0.00001454
Iteration 246/1000 | Loss: 0.00001454
Iteration 247/1000 | Loss: 0.00001454
Iteration 248/1000 | Loss: 0.00001454
Iteration 249/1000 | Loss: 0.00001454
Iteration 250/1000 | Loss: 0.00001454
Iteration 251/1000 | Loss: 0.00001454
Iteration 252/1000 | Loss: 0.00001454
Iteration 253/1000 | Loss: 0.00001454
Iteration 254/1000 | Loss: 0.00001454
Iteration 255/1000 | Loss: 0.00001454
Iteration 256/1000 | Loss: 0.00001454
Iteration 257/1000 | Loss: 0.00001454
Iteration 258/1000 | Loss: 0.00001454
Iteration 259/1000 | Loss: 0.00001454
Iteration 260/1000 | Loss: 0.00001454
Iteration 261/1000 | Loss: 0.00001454
Iteration 262/1000 | Loss: 0.00001454
Iteration 263/1000 | Loss: 0.00001454
Iteration 264/1000 | Loss: 0.00001454
Iteration 265/1000 | Loss: 0.00001454
Iteration 266/1000 | Loss: 0.00001454
Iteration 267/1000 | Loss: 0.00001454
Iteration 268/1000 | Loss: 0.00001454
Iteration 269/1000 | Loss: 0.00001454
Iteration 270/1000 | Loss: 0.00001454
Iteration 271/1000 | Loss: 0.00001454
Iteration 272/1000 | Loss: 0.00001454
Iteration 273/1000 | Loss: 0.00001454
Iteration 274/1000 | Loss: 0.00001454
Iteration 275/1000 | Loss: 0.00001454
Iteration 276/1000 | Loss: 0.00001454
Iteration 277/1000 | Loss: 0.00001454
Iteration 278/1000 | Loss: 0.00001454
Iteration 279/1000 | Loss: 0.00001454
Iteration 280/1000 | Loss: 0.00001454
Iteration 281/1000 | Loss: 0.00001454
Iteration 282/1000 | Loss: 0.00001454
Iteration 283/1000 | Loss: 0.00001454
Iteration 284/1000 | Loss: 0.00001454
Iteration 285/1000 | Loss: 0.00001454
Iteration 286/1000 | Loss: 0.00001454
Iteration 287/1000 | Loss: 0.00001454
Iteration 288/1000 | Loss: 0.00001454
Iteration 289/1000 | Loss: 0.00001454
Iteration 290/1000 | Loss: 0.00001454
Iteration 291/1000 | Loss: 0.00001454
Iteration 292/1000 | Loss: 0.00001454
Iteration 293/1000 | Loss: 0.00001454
Iteration 294/1000 | Loss: 0.00001454
Iteration 295/1000 | Loss: 0.00001454
Iteration 296/1000 | Loss: 0.00001454
Iteration 297/1000 | Loss: 0.00001454
Iteration 298/1000 | Loss: 0.00001454
Iteration 299/1000 | Loss: 0.00001454
Iteration 300/1000 | Loss: 0.00001454
Iteration 301/1000 | Loss: 0.00001454
Iteration 302/1000 | Loss: 0.00001454
Iteration 303/1000 | Loss: 0.00001454
Iteration 304/1000 | Loss: 0.00001454
Iteration 305/1000 | Loss: 0.00001454
Iteration 306/1000 | Loss: 0.00001454
Iteration 307/1000 | Loss: 0.00001454
Iteration 308/1000 | Loss: 0.00001454
Iteration 309/1000 | Loss: 0.00001454
Iteration 310/1000 | Loss: 0.00001454
Iteration 311/1000 | Loss: 0.00001454
Iteration 312/1000 | Loss: 0.00001454
Iteration 313/1000 | Loss: 0.00001454
Iteration 314/1000 | Loss: 0.00001454
Iteration 315/1000 | Loss: 0.00001454
Iteration 316/1000 | Loss: 0.00001454
Iteration 317/1000 | Loss: 0.00001454
Iteration 318/1000 | Loss: 0.00001454
Iteration 319/1000 | Loss: 0.00001454
Iteration 320/1000 | Loss: 0.00001454
Iteration 321/1000 | Loss: 0.00001454
Iteration 322/1000 | Loss: 0.00001454
Iteration 323/1000 | Loss: 0.00001454
Iteration 324/1000 | Loss: 0.00001454
Iteration 325/1000 | Loss: 0.00001454
Iteration 326/1000 | Loss: 0.00001454
Iteration 327/1000 | Loss: 0.00001454
Iteration 328/1000 | Loss: 0.00001454
Iteration 329/1000 | Loss: 0.00001454
Iteration 330/1000 | Loss: 0.00001454
Iteration 331/1000 | Loss: 0.00001454
Iteration 332/1000 | Loss: 0.00001454
Iteration 333/1000 | Loss: 0.00001454
Iteration 334/1000 | Loss: 0.00001454
Iteration 335/1000 | Loss: 0.00001454
Iteration 336/1000 | Loss: 0.00001454
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 336. Stopping optimization.
Last 5 losses: [1.4538287359755486e-05, 1.4538287359755486e-05, 1.4538287359755486e-05, 1.4538287359755486e-05, 1.4538287359755486e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4538287359755486e-05

Optimization complete. Final v2v error: 3.20660400390625 mm

Highest mean error: 3.579735279083252 mm for frame 62

Lowest mean error: 2.9485411643981934 mm for frame 228

Saving results

Total time: 87.76448059082031
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_022/1075/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_022/1075.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_022/1075
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00731608
Iteration 2/25 | Loss: 0.00127617
Iteration 3/25 | Loss: 0.00121435
Iteration 4/25 | Loss: 0.00120799
Iteration 5/25 | Loss: 0.00120597
Iteration 6/25 | Loss: 0.00120597
Iteration 7/25 | Loss: 0.00120597
Iteration 8/25 | Loss: 0.00120597
Iteration 9/25 | Loss: 0.00120597
Iteration 10/25 | Loss: 0.00120597
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0012059744913130999, 0.0012059744913130999, 0.0012059744913130999, 0.0012059744913130999, 0.0012059744913130999]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012059744913130999

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.17506862
Iteration 2/25 | Loss: 0.00079191
Iteration 3/25 | Loss: 0.00079190
Iteration 4/25 | Loss: 0.00079190
Iteration 5/25 | Loss: 0.00079190
Iteration 6/25 | Loss: 0.00079190
Iteration 7/25 | Loss: 0.00079190
Iteration 8/25 | Loss: 0.00079190
Iteration 9/25 | Loss: 0.00079190
Iteration 10/25 | Loss: 0.00079190
Iteration 11/25 | Loss: 0.00079190
Iteration 12/25 | Loss: 0.00079190
Iteration 13/25 | Loss: 0.00079190
Iteration 14/25 | Loss: 0.00079190
Iteration 15/25 | Loss: 0.00079190
Iteration 16/25 | Loss: 0.00079190
Iteration 17/25 | Loss: 0.00079190
Iteration 18/25 | Loss: 0.00079190
Iteration 19/25 | Loss: 0.00079190
Iteration 20/25 | Loss: 0.00079190
Iteration 21/25 | Loss: 0.00079190
Iteration 22/25 | Loss: 0.00079190
Iteration 23/25 | Loss: 0.00079190
Iteration 24/25 | Loss: 0.00079190
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0007918959599919617, 0.0007918959599919617, 0.0007918959599919617, 0.0007918959599919617, 0.0007918959599919617]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007918959599919617

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00079190
Iteration 2/1000 | Loss: 0.00003376
Iteration 3/1000 | Loss: 0.00002287
Iteration 4/1000 | Loss: 0.00002044
Iteration 5/1000 | Loss: 0.00001956
Iteration 6/1000 | Loss: 0.00001910
Iteration 7/1000 | Loss: 0.00001884
Iteration 8/1000 | Loss: 0.00001849
Iteration 9/1000 | Loss: 0.00001824
Iteration 10/1000 | Loss: 0.00001798
Iteration 11/1000 | Loss: 0.00001776
Iteration 12/1000 | Loss: 0.00001761
Iteration 13/1000 | Loss: 0.00001758
Iteration 14/1000 | Loss: 0.00001754
Iteration 15/1000 | Loss: 0.00001753
Iteration 16/1000 | Loss: 0.00001753
Iteration 17/1000 | Loss: 0.00001749
Iteration 18/1000 | Loss: 0.00001735
Iteration 19/1000 | Loss: 0.00001724
Iteration 20/1000 | Loss: 0.00001715
Iteration 21/1000 | Loss: 0.00001714
Iteration 22/1000 | Loss: 0.00001713
Iteration 23/1000 | Loss: 0.00001712
Iteration 24/1000 | Loss: 0.00001712
Iteration 25/1000 | Loss: 0.00001707
Iteration 26/1000 | Loss: 0.00001707
Iteration 27/1000 | Loss: 0.00001705
Iteration 28/1000 | Loss: 0.00001705
Iteration 29/1000 | Loss: 0.00001705
Iteration 30/1000 | Loss: 0.00001703
Iteration 31/1000 | Loss: 0.00001702
Iteration 32/1000 | Loss: 0.00001701
Iteration 33/1000 | Loss: 0.00001701
Iteration 34/1000 | Loss: 0.00001701
Iteration 35/1000 | Loss: 0.00001701
Iteration 36/1000 | Loss: 0.00001701
Iteration 37/1000 | Loss: 0.00001701
Iteration 38/1000 | Loss: 0.00001701
Iteration 39/1000 | Loss: 0.00001701
Iteration 40/1000 | Loss: 0.00001701
Iteration 41/1000 | Loss: 0.00001701
Iteration 42/1000 | Loss: 0.00001700
Iteration 43/1000 | Loss: 0.00001700
Iteration 44/1000 | Loss: 0.00001697
Iteration 45/1000 | Loss: 0.00001696
Iteration 46/1000 | Loss: 0.00001696
Iteration 47/1000 | Loss: 0.00001695
Iteration 48/1000 | Loss: 0.00001695
Iteration 49/1000 | Loss: 0.00001695
Iteration 50/1000 | Loss: 0.00001694
Iteration 51/1000 | Loss: 0.00001694
Iteration 52/1000 | Loss: 0.00001694
Iteration 53/1000 | Loss: 0.00001694
Iteration 54/1000 | Loss: 0.00001694
Iteration 55/1000 | Loss: 0.00001693
Iteration 56/1000 | Loss: 0.00001693
Iteration 57/1000 | Loss: 0.00001692
Iteration 58/1000 | Loss: 0.00001692
Iteration 59/1000 | Loss: 0.00001691
Iteration 60/1000 | Loss: 0.00001691
Iteration 61/1000 | Loss: 0.00001691
Iteration 62/1000 | Loss: 0.00001691
Iteration 63/1000 | Loss: 0.00001691
Iteration 64/1000 | Loss: 0.00001690
Iteration 65/1000 | Loss: 0.00001690
Iteration 66/1000 | Loss: 0.00001690
Iteration 67/1000 | Loss: 0.00001690
Iteration 68/1000 | Loss: 0.00001690
Iteration 69/1000 | Loss: 0.00001690
Iteration 70/1000 | Loss: 0.00001690
Iteration 71/1000 | Loss: 0.00001690
Iteration 72/1000 | Loss: 0.00001690
Iteration 73/1000 | Loss: 0.00001690
Iteration 74/1000 | Loss: 0.00001690
Iteration 75/1000 | Loss: 0.00001690
Iteration 76/1000 | Loss: 0.00001690
Iteration 77/1000 | Loss: 0.00001689
Iteration 78/1000 | Loss: 0.00001689
Iteration 79/1000 | Loss: 0.00001689
Iteration 80/1000 | Loss: 0.00001689
Iteration 81/1000 | Loss: 0.00001689
Iteration 82/1000 | Loss: 0.00001688
Iteration 83/1000 | Loss: 0.00001688
Iteration 84/1000 | Loss: 0.00001688
Iteration 85/1000 | Loss: 0.00001688
Iteration 86/1000 | Loss: 0.00001688
Iteration 87/1000 | Loss: 0.00001688
Iteration 88/1000 | Loss: 0.00001688
Iteration 89/1000 | Loss: 0.00001688
Iteration 90/1000 | Loss: 0.00001688
Iteration 91/1000 | Loss: 0.00001688
Iteration 92/1000 | Loss: 0.00001688
Iteration 93/1000 | Loss: 0.00001687
Iteration 94/1000 | Loss: 0.00001687
Iteration 95/1000 | Loss: 0.00001687
Iteration 96/1000 | Loss: 0.00001687
Iteration 97/1000 | Loss: 0.00001686
Iteration 98/1000 | Loss: 0.00001686
Iteration 99/1000 | Loss: 0.00001686
Iteration 100/1000 | Loss: 0.00001686
Iteration 101/1000 | Loss: 0.00001686
Iteration 102/1000 | Loss: 0.00001686
Iteration 103/1000 | Loss: 0.00001686
Iteration 104/1000 | Loss: 0.00001686
Iteration 105/1000 | Loss: 0.00001686
Iteration 106/1000 | Loss: 0.00001686
Iteration 107/1000 | Loss: 0.00001685
Iteration 108/1000 | Loss: 0.00001685
Iteration 109/1000 | Loss: 0.00001685
Iteration 110/1000 | Loss: 0.00001685
Iteration 111/1000 | Loss: 0.00001685
Iteration 112/1000 | Loss: 0.00001684
Iteration 113/1000 | Loss: 0.00001684
Iteration 114/1000 | Loss: 0.00001684
Iteration 115/1000 | Loss: 0.00001684
Iteration 116/1000 | Loss: 0.00001684
Iteration 117/1000 | Loss: 0.00001684
Iteration 118/1000 | Loss: 0.00001684
Iteration 119/1000 | Loss: 0.00001684
Iteration 120/1000 | Loss: 0.00001684
Iteration 121/1000 | Loss: 0.00001684
Iteration 122/1000 | Loss: 0.00001684
Iteration 123/1000 | Loss: 0.00001684
Iteration 124/1000 | Loss: 0.00001684
Iteration 125/1000 | Loss: 0.00001684
Iteration 126/1000 | Loss: 0.00001684
Iteration 127/1000 | Loss: 0.00001684
Iteration 128/1000 | Loss: 0.00001684
Iteration 129/1000 | Loss: 0.00001684
Iteration 130/1000 | Loss: 0.00001684
Iteration 131/1000 | Loss: 0.00001683
Iteration 132/1000 | Loss: 0.00001683
Iteration 133/1000 | Loss: 0.00001683
Iteration 134/1000 | Loss: 0.00001683
Iteration 135/1000 | Loss: 0.00001683
Iteration 136/1000 | Loss: 0.00001683
Iteration 137/1000 | Loss: 0.00001683
Iteration 138/1000 | Loss: 0.00001683
Iteration 139/1000 | Loss: 0.00001683
Iteration 140/1000 | Loss: 0.00001683
Iteration 141/1000 | Loss: 0.00001683
Iteration 142/1000 | Loss: 0.00001683
Iteration 143/1000 | Loss: 0.00001683
Iteration 144/1000 | Loss: 0.00001683
Iteration 145/1000 | Loss: 0.00001683
Iteration 146/1000 | Loss: 0.00001682
Iteration 147/1000 | Loss: 0.00001682
Iteration 148/1000 | Loss: 0.00001682
Iteration 149/1000 | Loss: 0.00001682
Iteration 150/1000 | Loss: 0.00001682
Iteration 151/1000 | Loss: 0.00001682
Iteration 152/1000 | Loss: 0.00001682
Iteration 153/1000 | Loss: 0.00001682
Iteration 154/1000 | Loss: 0.00001682
Iteration 155/1000 | Loss: 0.00001682
Iteration 156/1000 | Loss: 0.00001682
Iteration 157/1000 | Loss: 0.00001682
Iteration 158/1000 | Loss: 0.00001682
Iteration 159/1000 | Loss: 0.00001682
Iteration 160/1000 | Loss: 0.00001682
Iteration 161/1000 | Loss: 0.00001682
Iteration 162/1000 | Loss: 0.00001682
Iteration 163/1000 | Loss: 0.00001682
Iteration 164/1000 | Loss: 0.00001682
Iteration 165/1000 | Loss: 0.00001682
Iteration 166/1000 | Loss: 0.00001682
Iteration 167/1000 | Loss: 0.00001682
Iteration 168/1000 | Loss: 0.00001682
Iteration 169/1000 | Loss: 0.00001681
Iteration 170/1000 | Loss: 0.00001681
Iteration 171/1000 | Loss: 0.00001681
Iteration 172/1000 | Loss: 0.00001681
Iteration 173/1000 | Loss: 0.00001681
Iteration 174/1000 | Loss: 0.00001681
Iteration 175/1000 | Loss: 0.00001681
Iteration 176/1000 | Loss: 0.00001681
Iteration 177/1000 | Loss: 0.00001681
Iteration 178/1000 | Loss: 0.00001681
Iteration 179/1000 | Loss: 0.00001681
Iteration 180/1000 | Loss: 0.00001681
Iteration 181/1000 | Loss: 0.00001681
Iteration 182/1000 | Loss: 0.00001681
Iteration 183/1000 | Loss: 0.00001681
Iteration 184/1000 | Loss: 0.00001681
Iteration 185/1000 | Loss: 0.00001681
Iteration 186/1000 | Loss: 0.00001681
Iteration 187/1000 | Loss: 0.00001681
Iteration 188/1000 | Loss: 0.00001681
Iteration 189/1000 | Loss: 0.00001681
Iteration 190/1000 | Loss: 0.00001681
Iteration 191/1000 | Loss: 0.00001681
Iteration 192/1000 | Loss: 0.00001681
Iteration 193/1000 | Loss: 0.00001681
Iteration 194/1000 | Loss: 0.00001681
Iteration 195/1000 | Loss: 0.00001681
Iteration 196/1000 | Loss: 0.00001681
Iteration 197/1000 | Loss: 0.00001681
Iteration 198/1000 | Loss: 0.00001681
Iteration 199/1000 | Loss: 0.00001681
Iteration 200/1000 | Loss: 0.00001681
Iteration 201/1000 | Loss: 0.00001681
Iteration 202/1000 | Loss: 0.00001681
Iteration 203/1000 | Loss: 0.00001681
Iteration 204/1000 | Loss: 0.00001681
Iteration 205/1000 | Loss: 0.00001681
Iteration 206/1000 | Loss: 0.00001681
Iteration 207/1000 | Loss: 0.00001681
Iteration 208/1000 | Loss: 0.00001681
Iteration 209/1000 | Loss: 0.00001681
Iteration 210/1000 | Loss: 0.00001681
Iteration 211/1000 | Loss: 0.00001681
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 211. Stopping optimization.
Last 5 losses: [1.6805275663500652e-05, 1.6805275663500652e-05, 1.6805275663500652e-05, 1.6805275663500652e-05, 1.6805275663500652e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6805275663500652e-05

Optimization complete. Final v2v error: 3.4137487411499023 mm

Highest mean error: 3.6001529693603516 mm for frame 10

Lowest mean error: 3.1421761512756348 mm for frame 132

Saving results

Total time: 41.054428815841675
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_022/1041/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_022/1041.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_022/1041
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00947702
Iteration 2/25 | Loss: 0.00219405
Iteration 3/25 | Loss: 0.00165296
Iteration 4/25 | Loss: 0.00158577
Iteration 5/25 | Loss: 0.00141181
Iteration 6/25 | Loss: 0.00137641
Iteration 7/25 | Loss: 0.00137609
Iteration 8/25 | Loss: 0.00136462
Iteration 9/25 | Loss: 0.00127671
Iteration 10/25 | Loss: 0.00123538
Iteration 11/25 | Loss: 0.00125455
Iteration 12/25 | Loss: 0.00123646
Iteration 13/25 | Loss: 0.00121643
Iteration 14/25 | Loss: 0.00123452
Iteration 15/25 | Loss: 0.00123962
Iteration 16/25 | Loss: 0.00124375
Iteration 17/25 | Loss: 0.00121213
Iteration 18/25 | Loss: 0.00119489
Iteration 19/25 | Loss: 0.00118485
Iteration 20/25 | Loss: 0.00117894
Iteration 21/25 | Loss: 0.00117663
Iteration 22/25 | Loss: 0.00117667
Iteration 23/25 | Loss: 0.00117677
Iteration 24/25 | Loss: 0.00117616
Iteration 25/25 | Loss: 0.00117603

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33836508
Iteration 2/25 | Loss: 0.00097895
Iteration 3/25 | Loss: 0.00097895
Iteration 4/25 | Loss: 0.00097895
Iteration 5/25 | Loss: 0.00097895
Iteration 6/25 | Loss: 0.00097895
Iteration 7/25 | Loss: 0.00097895
Iteration 8/25 | Loss: 0.00097895
Iteration 9/25 | Loss: 0.00097895
Iteration 10/25 | Loss: 0.00097895
Iteration 11/25 | Loss: 0.00097895
Iteration 12/25 | Loss: 0.00097895
Iteration 13/25 | Loss: 0.00097895
Iteration 14/25 | Loss: 0.00097895
Iteration 15/25 | Loss: 0.00097895
Iteration 16/25 | Loss: 0.00097895
Iteration 17/25 | Loss: 0.00097895
Iteration 18/25 | Loss: 0.00097895
Iteration 19/25 | Loss: 0.00097895
Iteration 20/25 | Loss: 0.00097895
Iteration 21/25 | Loss: 0.00097895
Iteration 22/25 | Loss: 0.00097895
Iteration 23/25 | Loss: 0.00097895
Iteration 24/25 | Loss: 0.00097895
Iteration 25/25 | Loss: 0.00097895

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00097895
Iteration 2/1000 | Loss: 0.00008931
Iteration 3/1000 | Loss: 0.00005730
Iteration 4/1000 | Loss: 0.00004663
Iteration 5/1000 | Loss: 0.00006416
Iteration 6/1000 | Loss: 0.00004524
Iteration 7/1000 | Loss: 0.00004802
Iteration 8/1000 | Loss: 0.00003849
Iteration 9/1000 | Loss: 0.00004252
Iteration 10/1000 | Loss: 0.00006265
Iteration 11/1000 | Loss: 0.00006539
Iteration 12/1000 | Loss: 0.00006051
Iteration 13/1000 | Loss: 0.00006179
Iteration 14/1000 | Loss: 0.00005745
Iteration 15/1000 | Loss: 0.00006073
Iteration 16/1000 | Loss: 0.00007945
Iteration 17/1000 | Loss: 0.00004513
Iteration 18/1000 | Loss: 0.00006975
Iteration 19/1000 | Loss: 0.00005253
Iteration 20/1000 | Loss: 0.00024346
Iteration 21/1000 | Loss: 0.00005034
Iteration 22/1000 | Loss: 0.00005748
Iteration 23/1000 | Loss: 0.00005190
Iteration 24/1000 | Loss: 0.00004796
Iteration 25/1000 | Loss: 0.00005110
Iteration 26/1000 | Loss: 0.00004715
Iteration 27/1000 | Loss: 0.00005366
Iteration 28/1000 | Loss: 0.00004346
Iteration 29/1000 | Loss: 0.00005119
Iteration 30/1000 | Loss: 0.00004851
Iteration 31/1000 | Loss: 0.00005291
Iteration 32/1000 | Loss: 0.00004717
Iteration 33/1000 | Loss: 0.00005636
Iteration 34/1000 | Loss: 0.00004692
Iteration 35/1000 | Loss: 0.00004121
Iteration 36/1000 | Loss: 0.00004697
Iteration 37/1000 | Loss: 0.00004168
Iteration 38/1000 | Loss: 0.00004539
Iteration 39/1000 | Loss: 0.00004111
Iteration 40/1000 | Loss: 0.00004491
Iteration 41/1000 | Loss: 0.00003646
Iteration 42/1000 | Loss: 0.00004652
Iteration 43/1000 | Loss: 0.00005010
Iteration 44/1000 | Loss: 0.00004837
Iteration 45/1000 | Loss: 0.00005133
Iteration 46/1000 | Loss: 0.00005166
Iteration 47/1000 | Loss: 0.00005142
Iteration 48/1000 | Loss: 0.00005198
Iteration 49/1000 | Loss: 0.00002874
Iteration 50/1000 | Loss: 0.00004730
Iteration 51/1000 | Loss: 0.00005736
Iteration 52/1000 | Loss: 0.00003896
Iteration 53/1000 | Loss: 0.00003336
Iteration 54/1000 | Loss: 0.00003040
Iteration 55/1000 | Loss: 0.00002875
Iteration 56/1000 | Loss: 0.00002787
Iteration 57/1000 | Loss: 0.00002730
Iteration 58/1000 | Loss: 0.00002690
Iteration 59/1000 | Loss: 0.00002659
Iteration 60/1000 | Loss: 0.00002641
Iteration 61/1000 | Loss: 0.00002622
Iteration 62/1000 | Loss: 0.00002621
Iteration 63/1000 | Loss: 0.00002609
Iteration 64/1000 | Loss: 0.00002607
Iteration 65/1000 | Loss: 0.00002605
Iteration 66/1000 | Loss: 0.00002605
Iteration 67/1000 | Loss: 0.00002603
Iteration 68/1000 | Loss: 0.00002591
Iteration 69/1000 | Loss: 0.00002583
Iteration 70/1000 | Loss: 0.00002582
Iteration 71/1000 | Loss: 0.00002581
Iteration 72/1000 | Loss: 0.00002580
Iteration 73/1000 | Loss: 0.00002575
Iteration 74/1000 | Loss: 0.00002575
Iteration 75/1000 | Loss: 0.00003639
Iteration 76/1000 | Loss: 0.00002571
Iteration 77/1000 | Loss: 0.00002567
Iteration 78/1000 | Loss: 0.00002565
Iteration 79/1000 | Loss: 0.00002565
Iteration 80/1000 | Loss: 0.00002565
Iteration 81/1000 | Loss: 0.00002565
Iteration 82/1000 | Loss: 0.00002565
Iteration 83/1000 | Loss: 0.00002565
Iteration 84/1000 | Loss: 0.00002565
Iteration 85/1000 | Loss: 0.00002565
Iteration 86/1000 | Loss: 0.00002564
Iteration 87/1000 | Loss: 0.00002564
Iteration 88/1000 | Loss: 0.00002564
Iteration 89/1000 | Loss: 0.00002564
Iteration 90/1000 | Loss: 0.00002563
Iteration 91/1000 | Loss: 0.00002563
Iteration 92/1000 | Loss: 0.00066725
Iteration 93/1000 | Loss: 0.00079817
Iteration 94/1000 | Loss: 0.00029939
Iteration 95/1000 | Loss: 0.00077695
Iteration 96/1000 | Loss: 0.00025840
Iteration 97/1000 | Loss: 0.00073647
Iteration 98/1000 | Loss: 0.00022228
Iteration 99/1000 | Loss: 0.00057467
Iteration 100/1000 | Loss: 0.00021480
Iteration 101/1000 | Loss: 0.00040243
Iteration 102/1000 | Loss: 0.00042012
Iteration 103/1000 | Loss: 0.00017568
Iteration 104/1000 | Loss: 0.00004281
Iteration 105/1000 | Loss: 0.00002805
Iteration 106/1000 | Loss: 0.00004783
Iteration 107/1000 | Loss: 0.00002559
Iteration 108/1000 | Loss: 0.00003340
Iteration 109/1000 | Loss: 0.00002460
Iteration 110/1000 | Loss: 0.00005281
Iteration 111/1000 | Loss: 0.00002388
Iteration 112/1000 | Loss: 0.00002357
Iteration 113/1000 | Loss: 0.00002335
Iteration 114/1000 | Loss: 0.00002328
Iteration 115/1000 | Loss: 0.00002318
Iteration 116/1000 | Loss: 0.00002303
Iteration 117/1000 | Loss: 0.00002299
Iteration 118/1000 | Loss: 0.00002298
Iteration 119/1000 | Loss: 0.00002298
Iteration 120/1000 | Loss: 0.00002298
Iteration 121/1000 | Loss: 0.00002298
Iteration 122/1000 | Loss: 0.00002298
Iteration 123/1000 | Loss: 0.00002298
Iteration 124/1000 | Loss: 0.00002297
Iteration 125/1000 | Loss: 0.00002296
Iteration 126/1000 | Loss: 0.00002296
Iteration 127/1000 | Loss: 0.00002295
Iteration 128/1000 | Loss: 0.00002294
Iteration 129/1000 | Loss: 0.00002294
Iteration 130/1000 | Loss: 0.00002293
Iteration 131/1000 | Loss: 0.00002292
Iteration 132/1000 | Loss: 0.00002292
Iteration 133/1000 | Loss: 0.00002291
Iteration 134/1000 | Loss: 0.00002291
Iteration 135/1000 | Loss: 0.00002290
Iteration 136/1000 | Loss: 0.00002289
Iteration 137/1000 | Loss: 0.00002289
Iteration 138/1000 | Loss: 0.00002288
Iteration 139/1000 | Loss: 0.00002288
Iteration 140/1000 | Loss: 0.00002287
Iteration 141/1000 | Loss: 0.00002287
Iteration 142/1000 | Loss: 0.00002287
Iteration 143/1000 | Loss: 0.00002286
Iteration 144/1000 | Loss: 0.00002286
Iteration 145/1000 | Loss: 0.00002285
Iteration 146/1000 | Loss: 0.00002285
Iteration 147/1000 | Loss: 0.00002397
Iteration 148/1000 | Loss: 0.00002284
Iteration 149/1000 | Loss: 0.00002284
Iteration 150/1000 | Loss: 0.00002283
Iteration 151/1000 | Loss: 0.00002283
Iteration 152/1000 | Loss: 0.00002283
Iteration 153/1000 | Loss: 0.00002283
Iteration 154/1000 | Loss: 0.00002283
Iteration 155/1000 | Loss: 0.00002283
Iteration 156/1000 | Loss: 0.00002283
Iteration 157/1000 | Loss: 0.00002282
Iteration 158/1000 | Loss: 0.00002282
Iteration 159/1000 | Loss: 0.00002282
Iteration 160/1000 | Loss: 0.00002282
Iteration 161/1000 | Loss: 0.00002282
Iteration 162/1000 | Loss: 0.00002282
Iteration 163/1000 | Loss: 0.00002282
Iteration 164/1000 | Loss: 0.00002282
Iteration 165/1000 | Loss: 0.00002282
Iteration 166/1000 | Loss: 0.00002282
Iteration 167/1000 | Loss: 0.00002282
Iteration 168/1000 | Loss: 0.00002282
Iteration 169/1000 | Loss: 0.00002282
Iteration 170/1000 | Loss: 0.00002282
Iteration 171/1000 | Loss: 0.00002282
Iteration 172/1000 | Loss: 0.00002282
Iteration 173/1000 | Loss: 0.00002281
Iteration 174/1000 | Loss: 0.00002281
Iteration 175/1000 | Loss: 0.00002281
Iteration 176/1000 | Loss: 0.00002281
Iteration 177/1000 | Loss: 0.00002281
Iteration 178/1000 | Loss: 0.00002281
Iteration 179/1000 | Loss: 0.00002281
Iteration 180/1000 | Loss: 0.00002281
Iteration 181/1000 | Loss: 0.00002281
Iteration 182/1000 | Loss: 0.00002281
Iteration 183/1000 | Loss: 0.00002281
Iteration 184/1000 | Loss: 0.00002281
Iteration 185/1000 | Loss: 0.00002281
Iteration 186/1000 | Loss: 0.00002280
Iteration 187/1000 | Loss: 0.00002280
Iteration 188/1000 | Loss: 0.00002280
Iteration 189/1000 | Loss: 0.00002280
Iteration 190/1000 | Loss: 0.00002280
Iteration 191/1000 | Loss: 0.00002280
Iteration 192/1000 | Loss: 0.00002280
Iteration 193/1000 | Loss: 0.00002280
Iteration 194/1000 | Loss: 0.00002280
Iteration 195/1000 | Loss: 0.00002280
Iteration 196/1000 | Loss: 0.00002280
Iteration 197/1000 | Loss: 0.00002280
Iteration 198/1000 | Loss: 0.00002280
Iteration 199/1000 | Loss: 0.00002280
Iteration 200/1000 | Loss: 0.00002280
Iteration 201/1000 | Loss: 0.00002280
Iteration 202/1000 | Loss: 0.00002280
Iteration 203/1000 | Loss: 0.00002280
Iteration 204/1000 | Loss: 0.00002280
Iteration 205/1000 | Loss: 0.00002280
Iteration 206/1000 | Loss: 0.00002280
Iteration 207/1000 | Loss: 0.00002280
Iteration 208/1000 | Loss: 0.00002280
Iteration 209/1000 | Loss: 0.00002280
Iteration 210/1000 | Loss: 0.00002280
Iteration 211/1000 | Loss: 0.00002280
Iteration 212/1000 | Loss: 0.00002280
Iteration 213/1000 | Loss: 0.00002280
Iteration 214/1000 | Loss: 0.00002280
Iteration 215/1000 | Loss: 0.00002280
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 215. Stopping optimization.
Last 5 losses: [2.2802805688115768e-05, 2.2802805688115768e-05, 2.2802805688115768e-05, 2.2802805688115768e-05, 2.2802805688115768e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2802805688115768e-05

Optimization complete. Final v2v error: 3.6542856693267822 mm

Highest mean error: 10.560259819030762 mm for frame 21

Lowest mean error: 2.437094211578369 mm for frame 1

Saving results

Total time: 177.33217597007751
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_022/1074/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_022/1074.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_022/1074
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00427399
Iteration 2/25 | Loss: 0.00139513
Iteration 3/25 | Loss: 0.00114590
Iteration 4/25 | Loss: 0.00112167
Iteration 5/25 | Loss: 0.00111896
Iteration 6/25 | Loss: 0.00111812
Iteration 7/25 | Loss: 0.00111812
Iteration 8/25 | Loss: 0.00111812
Iteration 9/25 | Loss: 0.00111812
Iteration 10/25 | Loss: 0.00111812
Iteration 11/25 | Loss: 0.00111812
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011181175941601396, 0.0011181175941601396, 0.0011181175941601396, 0.0011181175941601396, 0.0011181175941601396]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011181175941601396

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 6.95778608
Iteration 2/25 | Loss: 0.00073877
Iteration 3/25 | Loss: 0.00073877
Iteration 4/25 | Loss: 0.00073876
Iteration 5/25 | Loss: 0.00073876
Iteration 6/25 | Loss: 0.00073876
Iteration 7/25 | Loss: 0.00073876
Iteration 8/25 | Loss: 0.00073876
Iteration 9/25 | Loss: 0.00073876
Iteration 10/25 | Loss: 0.00073876
Iteration 11/25 | Loss: 0.00073876
Iteration 12/25 | Loss: 0.00073876
Iteration 13/25 | Loss: 0.00073876
Iteration 14/25 | Loss: 0.00073876
Iteration 15/25 | Loss: 0.00073876
Iteration 16/25 | Loss: 0.00073876
Iteration 17/25 | Loss: 0.00073876
Iteration 18/25 | Loss: 0.00073876
Iteration 19/25 | Loss: 0.00073876
Iteration 20/25 | Loss: 0.00073876
Iteration 21/25 | Loss: 0.00073876
Iteration 22/25 | Loss: 0.00073876
Iteration 23/25 | Loss: 0.00073876
Iteration 24/25 | Loss: 0.00073876
Iteration 25/25 | Loss: 0.00073876

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00073876
Iteration 2/1000 | Loss: 0.00002580
Iteration 3/1000 | Loss: 0.00001724
Iteration 4/1000 | Loss: 0.00001557
Iteration 5/1000 | Loss: 0.00001448
Iteration 6/1000 | Loss: 0.00001389
Iteration 7/1000 | Loss: 0.00001348
Iteration 8/1000 | Loss: 0.00001302
Iteration 9/1000 | Loss: 0.00001284
Iteration 10/1000 | Loss: 0.00001261
Iteration 11/1000 | Loss: 0.00001259
Iteration 12/1000 | Loss: 0.00001258
Iteration 13/1000 | Loss: 0.00001245
Iteration 14/1000 | Loss: 0.00001241
Iteration 15/1000 | Loss: 0.00001241
Iteration 16/1000 | Loss: 0.00001241
Iteration 17/1000 | Loss: 0.00001240
Iteration 18/1000 | Loss: 0.00001237
Iteration 19/1000 | Loss: 0.00001236
Iteration 20/1000 | Loss: 0.00001231
Iteration 21/1000 | Loss: 0.00001230
Iteration 22/1000 | Loss: 0.00001229
Iteration 23/1000 | Loss: 0.00001229
Iteration 24/1000 | Loss: 0.00001224
Iteration 25/1000 | Loss: 0.00001224
Iteration 26/1000 | Loss: 0.00001223
Iteration 27/1000 | Loss: 0.00001223
Iteration 28/1000 | Loss: 0.00001223
Iteration 29/1000 | Loss: 0.00001222
Iteration 30/1000 | Loss: 0.00001218
Iteration 31/1000 | Loss: 0.00001218
Iteration 32/1000 | Loss: 0.00001217
Iteration 33/1000 | Loss: 0.00001217
Iteration 34/1000 | Loss: 0.00001216
Iteration 35/1000 | Loss: 0.00001214
Iteration 36/1000 | Loss: 0.00001213
Iteration 37/1000 | Loss: 0.00001212
Iteration 38/1000 | Loss: 0.00001212
Iteration 39/1000 | Loss: 0.00001212
Iteration 40/1000 | Loss: 0.00001211
Iteration 41/1000 | Loss: 0.00001210
Iteration 42/1000 | Loss: 0.00001209
Iteration 43/1000 | Loss: 0.00001209
Iteration 44/1000 | Loss: 0.00001209
Iteration 45/1000 | Loss: 0.00001208
Iteration 46/1000 | Loss: 0.00001208
Iteration 47/1000 | Loss: 0.00001208
Iteration 48/1000 | Loss: 0.00001207
Iteration 49/1000 | Loss: 0.00001207
Iteration 50/1000 | Loss: 0.00001206
Iteration 51/1000 | Loss: 0.00001206
Iteration 52/1000 | Loss: 0.00001205
Iteration 53/1000 | Loss: 0.00001205
Iteration 54/1000 | Loss: 0.00001205
Iteration 55/1000 | Loss: 0.00001204
Iteration 56/1000 | Loss: 0.00001204
Iteration 57/1000 | Loss: 0.00001203
Iteration 58/1000 | Loss: 0.00001202
Iteration 59/1000 | Loss: 0.00001202
Iteration 60/1000 | Loss: 0.00001202
Iteration 61/1000 | Loss: 0.00001202
Iteration 62/1000 | Loss: 0.00001202
Iteration 63/1000 | Loss: 0.00001202
Iteration 64/1000 | Loss: 0.00001202
Iteration 65/1000 | Loss: 0.00001201
Iteration 66/1000 | Loss: 0.00001201
Iteration 67/1000 | Loss: 0.00001201
Iteration 68/1000 | Loss: 0.00001201
Iteration 69/1000 | Loss: 0.00001200
Iteration 70/1000 | Loss: 0.00001200
Iteration 71/1000 | Loss: 0.00001199
Iteration 72/1000 | Loss: 0.00001199
Iteration 73/1000 | Loss: 0.00001198
Iteration 74/1000 | Loss: 0.00001198
Iteration 75/1000 | Loss: 0.00001198
Iteration 76/1000 | Loss: 0.00001197
Iteration 77/1000 | Loss: 0.00001197
Iteration 78/1000 | Loss: 0.00001197
Iteration 79/1000 | Loss: 0.00001197
Iteration 80/1000 | Loss: 0.00001196
Iteration 81/1000 | Loss: 0.00001196
Iteration 82/1000 | Loss: 0.00001196
Iteration 83/1000 | Loss: 0.00001196
Iteration 84/1000 | Loss: 0.00001195
Iteration 85/1000 | Loss: 0.00001195
Iteration 86/1000 | Loss: 0.00001195
Iteration 87/1000 | Loss: 0.00001195
Iteration 88/1000 | Loss: 0.00001194
Iteration 89/1000 | Loss: 0.00001194
Iteration 90/1000 | Loss: 0.00001194
Iteration 91/1000 | Loss: 0.00001194
Iteration 92/1000 | Loss: 0.00001194
Iteration 93/1000 | Loss: 0.00001194
Iteration 94/1000 | Loss: 0.00001193
Iteration 95/1000 | Loss: 0.00001193
Iteration 96/1000 | Loss: 0.00001193
Iteration 97/1000 | Loss: 0.00001193
Iteration 98/1000 | Loss: 0.00001193
Iteration 99/1000 | Loss: 0.00001193
Iteration 100/1000 | Loss: 0.00001193
Iteration 101/1000 | Loss: 0.00001193
Iteration 102/1000 | Loss: 0.00001193
Iteration 103/1000 | Loss: 0.00001192
Iteration 104/1000 | Loss: 0.00001192
Iteration 105/1000 | Loss: 0.00001192
Iteration 106/1000 | Loss: 0.00001192
Iteration 107/1000 | Loss: 0.00001192
Iteration 108/1000 | Loss: 0.00001192
Iteration 109/1000 | Loss: 0.00001192
Iteration 110/1000 | Loss: 0.00001192
Iteration 111/1000 | Loss: 0.00001192
Iteration 112/1000 | Loss: 0.00001192
Iteration 113/1000 | Loss: 0.00001192
Iteration 114/1000 | Loss: 0.00001192
Iteration 115/1000 | Loss: 0.00001192
Iteration 116/1000 | Loss: 0.00001192
Iteration 117/1000 | Loss: 0.00001192
Iteration 118/1000 | Loss: 0.00001192
Iteration 119/1000 | Loss: 0.00001192
Iteration 120/1000 | Loss: 0.00001192
Iteration 121/1000 | Loss: 0.00001192
Iteration 122/1000 | Loss: 0.00001192
Iteration 123/1000 | Loss: 0.00001192
Iteration 124/1000 | Loss: 0.00001192
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 124. Stopping optimization.
Last 5 losses: [1.1923746569664218e-05, 1.1923746569664218e-05, 1.1923746569664218e-05, 1.1923746569664218e-05, 1.1923746569664218e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1923746569664218e-05

Optimization complete. Final v2v error: 2.9296579360961914 mm

Highest mean error: 3.670773983001709 mm for frame 75

Lowest mean error: 2.590891122817993 mm for frame 109

Saving results

Total time: 34.37706708908081
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_022/1094/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_022/1094.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_022/1094
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00384615
Iteration 2/25 | Loss: 0.00117156
Iteration 3/25 | Loss: 0.00108162
Iteration 4/25 | Loss: 0.00107003
Iteration 5/25 | Loss: 0.00106693
Iteration 6/25 | Loss: 0.00106629
Iteration 7/25 | Loss: 0.00106629
Iteration 8/25 | Loss: 0.00106629
Iteration 9/25 | Loss: 0.00106629
Iteration 10/25 | Loss: 0.00106629
Iteration 11/25 | Loss: 0.00106629
Iteration 12/25 | Loss: 0.00106629
Iteration 13/25 | Loss: 0.00106629
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0010662934510037303, 0.0010662934510037303, 0.0010662934510037303, 0.0010662934510037303, 0.0010662934510037303]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010662934510037303

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33813596
Iteration 2/25 | Loss: 0.00093966
Iteration 3/25 | Loss: 0.00093965
Iteration 4/25 | Loss: 0.00093965
Iteration 5/25 | Loss: 0.00093965
Iteration 6/25 | Loss: 0.00093965
Iteration 7/25 | Loss: 0.00093965
Iteration 8/25 | Loss: 0.00093965
Iteration 9/25 | Loss: 0.00093965
Iteration 10/25 | Loss: 0.00093965
Iteration 11/25 | Loss: 0.00093965
Iteration 12/25 | Loss: 0.00093965
Iteration 13/25 | Loss: 0.00093965
Iteration 14/25 | Loss: 0.00093965
Iteration 15/25 | Loss: 0.00093965
Iteration 16/25 | Loss: 0.00093965
Iteration 17/25 | Loss: 0.00093965
Iteration 18/25 | Loss: 0.00093965
Iteration 19/25 | Loss: 0.00093965
Iteration 20/25 | Loss: 0.00093965
Iteration 21/25 | Loss: 0.00093965
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.000939645862672478, 0.000939645862672478, 0.000939645862672478, 0.000939645862672478, 0.000939645862672478]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000939645862672478

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00093965
Iteration 2/1000 | Loss: 0.00004360
Iteration 3/1000 | Loss: 0.00003026
Iteration 4/1000 | Loss: 0.00002183
Iteration 5/1000 | Loss: 0.00001854
Iteration 6/1000 | Loss: 0.00001695
Iteration 7/1000 | Loss: 0.00001562
Iteration 8/1000 | Loss: 0.00001499
Iteration 9/1000 | Loss: 0.00001455
Iteration 10/1000 | Loss: 0.00001421
Iteration 11/1000 | Loss: 0.00001402
Iteration 12/1000 | Loss: 0.00001396
Iteration 13/1000 | Loss: 0.00001377
Iteration 14/1000 | Loss: 0.00001368
Iteration 15/1000 | Loss: 0.00001352
Iteration 16/1000 | Loss: 0.00001350
Iteration 17/1000 | Loss: 0.00001346
Iteration 18/1000 | Loss: 0.00001342
Iteration 19/1000 | Loss: 0.00001341
Iteration 20/1000 | Loss: 0.00001340
Iteration 21/1000 | Loss: 0.00001340
Iteration 22/1000 | Loss: 0.00001339
Iteration 23/1000 | Loss: 0.00001337
Iteration 24/1000 | Loss: 0.00001335
Iteration 25/1000 | Loss: 0.00001335
Iteration 26/1000 | Loss: 0.00001334
Iteration 27/1000 | Loss: 0.00001333
Iteration 28/1000 | Loss: 0.00001332
Iteration 29/1000 | Loss: 0.00001331
Iteration 30/1000 | Loss: 0.00001330
Iteration 31/1000 | Loss: 0.00001330
Iteration 32/1000 | Loss: 0.00001326
Iteration 33/1000 | Loss: 0.00001326
Iteration 34/1000 | Loss: 0.00001324
Iteration 35/1000 | Loss: 0.00001323
Iteration 36/1000 | Loss: 0.00001323
Iteration 37/1000 | Loss: 0.00001323
Iteration 38/1000 | Loss: 0.00001323
Iteration 39/1000 | Loss: 0.00001323
Iteration 40/1000 | Loss: 0.00001321
Iteration 41/1000 | Loss: 0.00001321
Iteration 42/1000 | Loss: 0.00001321
Iteration 43/1000 | Loss: 0.00001321
Iteration 44/1000 | Loss: 0.00001321
Iteration 45/1000 | Loss: 0.00001321
Iteration 46/1000 | Loss: 0.00001321
Iteration 47/1000 | Loss: 0.00001320
Iteration 48/1000 | Loss: 0.00001320
Iteration 49/1000 | Loss: 0.00001320
Iteration 50/1000 | Loss: 0.00001320
Iteration 51/1000 | Loss: 0.00001320
Iteration 52/1000 | Loss: 0.00001320
Iteration 53/1000 | Loss: 0.00001320
Iteration 54/1000 | Loss: 0.00001319
Iteration 55/1000 | Loss: 0.00001319
Iteration 56/1000 | Loss: 0.00001319
Iteration 57/1000 | Loss: 0.00001318
Iteration 58/1000 | Loss: 0.00001318
Iteration 59/1000 | Loss: 0.00001318
Iteration 60/1000 | Loss: 0.00001317
Iteration 61/1000 | Loss: 0.00001317
Iteration 62/1000 | Loss: 0.00001317
Iteration 63/1000 | Loss: 0.00001317
Iteration 64/1000 | Loss: 0.00001317
Iteration 65/1000 | Loss: 0.00001317
Iteration 66/1000 | Loss: 0.00001317
Iteration 67/1000 | Loss: 0.00001317
Iteration 68/1000 | Loss: 0.00001317
Iteration 69/1000 | Loss: 0.00001316
Iteration 70/1000 | Loss: 0.00001316
Iteration 71/1000 | Loss: 0.00001316
Iteration 72/1000 | Loss: 0.00001316
Iteration 73/1000 | Loss: 0.00001316
Iteration 74/1000 | Loss: 0.00001315
Iteration 75/1000 | Loss: 0.00001315
Iteration 76/1000 | Loss: 0.00001315
Iteration 77/1000 | Loss: 0.00001315
Iteration 78/1000 | Loss: 0.00001315
Iteration 79/1000 | Loss: 0.00001315
Iteration 80/1000 | Loss: 0.00001314
Iteration 81/1000 | Loss: 0.00001314
Iteration 82/1000 | Loss: 0.00001314
Iteration 83/1000 | Loss: 0.00001314
Iteration 84/1000 | Loss: 0.00001314
Iteration 85/1000 | Loss: 0.00001314
Iteration 86/1000 | Loss: 0.00001314
Iteration 87/1000 | Loss: 0.00001314
Iteration 88/1000 | Loss: 0.00001314
Iteration 89/1000 | Loss: 0.00001313
Iteration 90/1000 | Loss: 0.00001313
Iteration 91/1000 | Loss: 0.00001313
Iteration 92/1000 | Loss: 0.00001313
Iteration 93/1000 | Loss: 0.00001313
Iteration 94/1000 | Loss: 0.00001313
Iteration 95/1000 | Loss: 0.00001313
Iteration 96/1000 | Loss: 0.00001312
Iteration 97/1000 | Loss: 0.00001312
Iteration 98/1000 | Loss: 0.00001312
Iteration 99/1000 | Loss: 0.00001312
Iteration 100/1000 | Loss: 0.00001312
Iteration 101/1000 | Loss: 0.00001312
Iteration 102/1000 | Loss: 0.00001312
Iteration 103/1000 | Loss: 0.00001312
Iteration 104/1000 | Loss: 0.00001311
Iteration 105/1000 | Loss: 0.00001311
Iteration 106/1000 | Loss: 0.00001311
Iteration 107/1000 | Loss: 0.00001311
Iteration 108/1000 | Loss: 0.00001310
Iteration 109/1000 | Loss: 0.00001310
Iteration 110/1000 | Loss: 0.00001310
Iteration 111/1000 | Loss: 0.00001310
Iteration 112/1000 | Loss: 0.00001309
Iteration 113/1000 | Loss: 0.00001309
Iteration 114/1000 | Loss: 0.00001309
Iteration 115/1000 | Loss: 0.00001309
Iteration 116/1000 | Loss: 0.00001309
Iteration 117/1000 | Loss: 0.00001309
Iteration 118/1000 | Loss: 0.00001309
Iteration 119/1000 | Loss: 0.00001308
Iteration 120/1000 | Loss: 0.00001308
Iteration 121/1000 | Loss: 0.00001308
Iteration 122/1000 | Loss: 0.00001308
Iteration 123/1000 | Loss: 0.00001308
Iteration 124/1000 | Loss: 0.00001307
Iteration 125/1000 | Loss: 0.00001307
Iteration 126/1000 | Loss: 0.00001306
Iteration 127/1000 | Loss: 0.00001306
Iteration 128/1000 | Loss: 0.00001306
Iteration 129/1000 | Loss: 0.00001306
Iteration 130/1000 | Loss: 0.00001306
Iteration 131/1000 | Loss: 0.00001305
Iteration 132/1000 | Loss: 0.00001305
Iteration 133/1000 | Loss: 0.00001305
Iteration 134/1000 | Loss: 0.00001305
Iteration 135/1000 | Loss: 0.00001305
Iteration 136/1000 | Loss: 0.00001305
Iteration 137/1000 | Loss: 0.00001304
Iteration 138/1000 | Loss: 0.00001304
Iteration 139/1000 | Loss: 0.00001304
Iteration 140/1000 | Loss: 0.00001304
Iteration 141/1000 | Loss: 0.00001304
Iteration 142/1000 | Loss: 0.00001304
Iteration 143/1000 | Loss: 0.00001303
Iteration 144/1000 | Loss: 0.00001303
Iteration 145/1000 | Loss: 0.00001303
Iteration 146/1000 | Loss: 0.00001303
Iteration 147/1000 | Loss: 0.00001302
Iteration 148/1000 | Loss: 0.00001302
Iteration 149/1000 | Loss: 0.00001302
Iteration 150/1000 | Loss: 0.00001302
Iteration 151/1000 | Loss: 0.00001302
Iteration 152/1000 | Loss: 0.00001302
Iteration 153/1000 | Loss: 0.00001302
Iteration 154/1000 | Loss: 0.00001302
Iteration 155/1000 | Loss: 0.00001302
Iteration 156/1000 | Loss: 0.00001302
Iteration 157/1000 | Loss: 0.00001301
Iteration 158/1000 | Loss: 0.00001301
Iteration 159/1000 | Loss: 0.00001301
Iteration 160/1000 | Loss: 0.00001301
Iteration 161/1000 | Loss: 0.00001301
Iteration 162/1000 | Loss: 0.00001301
Iteration 163/1000 | Loss: 0.00001301
Iteration 164/1000 | Loss: 0.00001301
Iteration 165/1000 | Loss: 0.00001301
Iteration 166/1000 | Loss: 0.00001301
Iteration 167/1000 | Loss: 0.00001301
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 167. Stopping optimization.
Last 5 losses: [1.3012298950343393e-05, 1.3012298950343393e-05, 1.3012298950343393e-05, 1.3012298950343393e-05, 1.3012298950343393e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3012298950343393e-05

Optimization complete. Final v2v error: 3.0439813137054443 mm

Highest mean error: 3.8958067893981934 mm for frame 23

Lowest mean error: 2.5456466674804688 mm for frame 10

Saving results

Total time: 42.20058465003967
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_022/1066/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_022/1066.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_022/1066
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00857863
Iteration 2/25 | Loss: 0.00117867
Iteration 3/25 | Loss: 0.00108921
Iteration 4/25 | Loss: 0.00107960
Iteration 5/25 | Loss: 0.00107642
Iteration 6/25 | Loss: 0.00107563
Iteration 7/25 | Loss: 0.00107563
Iteration 8/25 | Loss: 0.00107563
Iteration 9/25 | Loss: 0.00107563
Iteration 10/25 | Loss: 0.00107563
Iteration 11/25 | Loss: 0.00107563
Iteration 12/25 | Loss: 0.00107563
Iteration 13/25 | Loss: 0.00107563
Iteration 14/25 | Loss: 0.00107563
Iteration 15/25 | Loss: 0.00107563
Iteration 16/25 | Loss: 0.00107563
Iteration 17/25 | Loss: 0.00107563
Iteration 18/25 | Loss: 0.00107563
Iteration 19/25 | Loss: 0.00107563
Iteration 20/25 | Loss: 0.00107563
Iteration 21/25 | Loss: 0.00107563
Iteration 22/25 | Loss: 0.00107563
Iteration 23/25 | Loss: 0.00107563
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0010756270494312048, 0.0010756270494312048, 0.0010756270494312048, 0.0010756270494312048, 0.0010756270494312048]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010756270494312048

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.77821016
Iteration 2/25 | Loss: 0.00088433
Iteration 3/25 | Loss: 0.00088433
Iteration 4/25 | Loss: 0.00088433
Iteration 5/25 | Loss: 0.00088433
Iteration 6/25 | Loss: 0.00088433
Iteration 7/25 | Loss: 0.00088433
Iteration 8/25 | Loss: 0.00088433
Iteration 9/25 | Loss: 0.00088433
Iteration 10/25 | Loss: 0.00088433
Iteration 11/25 | Loss: 0.00088433
Iteration 12/25 | Loss: 0.00088433
Iteration 13/25 | Loss: 0.00088433
Iteration 14/25 | Loss: 0.00088433
Iteration 15/25 | Loss: 0.00088433
Iteration 16/25 | Loss: 0.00088433
Iteration 17/25 | Loss: 0.00088433
Iteration 18/25 | Loss: 0.00088433
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0008843272225931287, 0.0008843272225931287, 0.0008843272225931287, 0.0008843272225931287, 0.0008843272225931287]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008843272225931287

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00088433
Iteration 2/1000 | Loss: 0.00001942
Iteration 3/1000 | Loss: 0.00001289
Iteration 4/1000 | Loss: 0.00001175
Iteration 5/1000 | Loss: 0.00001107
Iteration 6/1000 | Loss: 0.00001062
Iteration 7/1000 | Loss: 0.00001021
Iteration 8/1000 | Loss: 0.00001006
Iteration 9/1000 | Loss: 0.00000995
Iteration 10/1000 | Loss: 0.00000994
Iteration 11/1000 | Loss: 0.00000994
Iteration 12/1000 | Loss: 0.00000986
Iteration 13/1000 | Loss: 0.00000968
Iteration 14/1000 | Loss: 0.00000963
Iteration 15/1000 | Loss: 0.00000955
Iteration 16/1000 | Loss: 0.00000953
Iteration 17/1000 | Loss: 0.00000953
Iteration 18/1000 | Loss: 0.00000950
Iteration 19/1000 | Loss: 0.00000949
Iteration 20/1000 | Loss: 0.00000948
Iteration 21/1000 | Loss: 0.00000948
Iteration 22/1000 | Loss: 0.00000948
Iteration 23/1000 | Loss: 0.00000947
Iteration 24/1000 | Loss: 0.00000946
Iteration 25/1000 | Loss: 0.00000946
Iteration 26/1000 | Loss: 0.00000946
Iteration 27/1000 | Loss: 0.00000945
Iteration 28/1000 | Loss: 0.00000945
Iteration 29/1000 | Loss: 0.00000943
Iteration 30/1000 | Loss: 0.00000940
Iteration 31/1000 | Loss: 0.00000935
Iteration 32/1000 | Loss: 0.00000934
Iteration 33/1000 | Loss: 0.00000934
Iteration 34/1000 | Loss: 0.00000933
Iteration 35/1000 | Loss: 0.00000932
Iteration 36/1000 | Loss: 0.00000931
Iteration 37/1000 | Loss: 0.00000930
Iteration 38/1000 | Loss: 0.00000930
Iteration 39/1000 | Loss: 0.00000929
Iteration 40/1000 | Loss: 0.00000929
Iteration 41/1000 | Loss: 0.00000929
Iteration 42/1000 | Loss: 0.00000928
Iteration 43/1000 | Loss: 0.00000928
Iteration 44/1000 | Loss: 0.00000927
Iteration 45/1000 | Loss: 0.00000926
Iteration 46/1000 | Loss: 0.00000926
Iteration 47/1000 | Loss: 0.00000925
Iteration 48/1000 | Loss: 0.00000925
Iteration 49/1000 | Loss: 0.00000925
Iteration 50/1000 | Loss: 0.00000925
Iteration 51/1000 | Loss: 0.00000924
Iteration 52/1000 | Loss: 0.00000924
Iteration 53/1000 | Loss: 0.00000924
Iteration 54/1000 | Loss: 0.00000924
Iteration 55/1000 | Loss: 0.00000923
Iteration 56/1000 | Loss: 0.00000923
Iteration 57/1000 | Loss: 0.00000923
Iteration 58/1000 | Loss: 0.00000922
Iteration 59/1000 | Loss: 0.00000922
Iteration 60/1000 | Loss: 0.00000922
Iteration 61/1000 | Loss: 0.00000921
Iteration 62/1000 | Loss: 0.00000921
Iteration 63/1000 | Loss: 0.00000921
Iteration 64/1000 | Loss: 0.00000920
Iteration 65/1000 | Loss: 0.00000919
Iteration 66/1000 | Loss: 0.00000919
Iteration 67/1000 | Loss: 0.00000919
Iteration 68/1000 | Loss: 0.00000918
Iteration 69/1000 | Loss: 0.00000918
Iteration 70/1000 | Loss: 0.00000918
Iteration 71/1000 | Loss: 0.00000918
Iteration 72/1000 | Loss: 0.00000918
Iteration 73/1000 | Loss: 0.00000918
Iteration 74/1000 | Loss: 0.00000917
Iteration 75/1000 | Loss: 0.00000917
Iteration 76/1000 | Loss: 0.00000917
Iteration 77/1000 | Loss: 0.00000917
Iteration 78/1000 | Loss: 0.00000917
Iteration 79/1000 | Loss: 0.00000917
Iteration 80/1000 | Loss: 0.00000917
Iteration 81/1000 | Loss: 0.00000915
Iteration 82/1000 | Loss: 0.00000915
Iteration 83/1000 | Loss: 0.00000915
Iteration 84/1000 | Loss: 0.00000915
Iteration 85/1000 | Loss: 0.00000915
Iteration 86/1000 | Loss: 0.00000915
Iteration 87/1000 | Loss: 0.00000915
Iteration 88/1000 | Loss: 0.00000914
Iteration 89/1000 | Loss: 0.00000914
Iteration 90/1000 | Loss: 0.00000914
Iteration 91/1000 | Loss: 0.00000914
Iteration 92/1000 | Loss: 0.00000914
Iteration 93/1000 | Loss: 0.00000914
Iteration 94/1000 | Loss: 0.00000914
Iteration 95/1000 | Loss: 0.00000914
Iteration 96/1000 | Loss: 0.00000914
Iteration 97/1000 | Loss: 0.00000914
Iteration 98/1000 | Loss: 0.00000914
Iteration 99/1000 | Loss: 0.00000914
Iteration 100/1000 | Loss: 0.00000913
Iteration 101/1000 | Loss: 0.00000913
Iteration 102/1000 | Loss: 0.00000913
Iteration 103/1000 | Loss: 0.00000913
Iteration 104/1000 | Loss: 0.00000913
Iteration 105/1000 | Loss: 0.00000913
Iteration 106/1000 | Loss: 0.00000913
Iteration 107/1000 | Loss: 0.00000913
Iteration 108/1000 | Loss: 0.00000912
Iteration 109/1000 | Loss: 0.00000912
Iteration 110/1000 | Loss: 0.00000912
Iteration 111/1000 | Loss: 0.00000912
Iteration 112/1000 | Loss: 0.00000911
Iteration 113/1000 | Loss: 0.00000911
Iteration 114/1000 | Loss: 0.00000911
Iteration 115/1000 | Loss: 0.00000910
Iteration 116/1000 | Loss: 0.00000910
Iteration 117/1000 | Loss: 0.00000910
Iteration 118/1000 | Loss: 0.00000910
Iteration 119/1000 | Loss: 0.00000910
Iteration 120/1000 | Loss: 0.00000910
Iteration 121/1000 | Loss: 0.00000909
Iteration 122/1000 | Loss: 0.00000909
Iteration 123/1000 | Loss: 0.00000909
Iteration 124/1000 | Loss: 0.00000909
Iteration 125/1000 | Loss: 0.00000909
Iteration 126/1000 | Loss: 0.00000909
Iteration 127/1000 | Loss: 0.00000908
Iteration 128/1000 | Loss: 0.00000908
Iteration 129/1000 | Loss: 0.00000908
Iteration 130/1000 | Loss: 0.00000908
Iteration 131/1000 | Loss: 0.00000908
Iteration 132/1000 | Loss: 0.00000908
Iteration 133/1000 | Loss: 0.00000908
Iteration 134/1000 | Loss: 0.00000908
Iteration 135/1000 | Loss: 0.00000907
Iteration 136/1000 | Loss: 0.00000907
Iteration 137/1000 | Loss: 0.00000907
Iteration 138/1000 | Loss: 0.00000907
Iteration 139/1000 | Loss: 0.00000907
Iteration 140/1000 | Loss: 0.00000907
Iteration 141/1000 | Loss: 0.00000907
Iteration 142/1000 | Loss: 0.00000907
Iteration 143/1000 | Loss: 0.00000906
Iteration 144/1000 | Loss: 0.00000906
Iteration 145/1000 | Loss: 0.00000906
Iteration 146/1000 | Loss: 0.00000906
Iteration 147/1000 | Loss: 0.00000906
Iteration 148/1000 | Loss: 0.00000906
Iteration 149/1000 | Loss: 0.00000905
Iteration 150/1000 | Loss: 0.00000905
Iteration 151/1000 | Loss: 0.00000905
Iteration 152/1000 | Loss: 0.00000905
Iteration 153/1000 | Loss: 0.00000904
Iteration 154/1000 | Loss: 0.00000904
Iteration 155/1000 | Loss: 0.00000904
Iteration 156/1000 | Loss: 0.00000904
Iteration 157/1000 | Loss: 0.00000904
Iteration 158/1000 | Loss: 0.00000904
Iteration 159/1000 | Loss: 0.00000904
Iteration 160/1000 | Loss: 0.00000903
Iteration 161/1000 | Loss: 0.00000903
Iteration 162/1000 | Loss: 0.00000903
Iteration 163/1000 | Loss: 0.00000903
Iteration 164/1000 | Loss: 0.00000903
Iteration 165/1000 | Loss: 0.00000903
Iteration 166/1000 | Loss: 0.00000903
Iteration 167/1000 | Loss: 0.00000903
Iteration 168/1000 | Loss: 0.00000903
Iteration 169/1000 | Loss: 0.00000903
Iteration 170/1000 | Loss: 0.00000903
Iteration 171/1000 | Loss: 0.00000903
Iteration 172/1000 | Loss: 0.00000902
Iteration 173/1000 | Loss: 0.00000902
Iteration 174/1000 | Loss: 0.00000902
Iteration 175/1000 | Loss: 0.00000902
Iteration 176/1000 | Loss: 0.00000902
Iteration 177/1000 | Loss: 0.00000902
Iteration 178/1000 | Loss: 0.00000901
Iteration 179/1000 | Loss: 0.00000901
Iteration 180/1000 | Loss: 0.00000901
Iteration 181/1000 | Loss: 0.00000901
Iteration 182/1000 | Loss: 0.00000901
Iteration 183/1000 | Loss: 0.00000901
Iteration 184/1000 | Loss: 0.00000901
Iteration 185/1000 | Loss: 0.00000901
Iteration 186/1000 | Loss: 0.00000901
Iteration 187/1000 | Loss: 0.00000901
Iteration 188/1000 | Loss: 0.00000901
Iteration 189/1000 | Loss: 0.00000901
Iteration 190/1000 | Loss: 0.00000901
Iteration 191/1000 | Loss: 0.00000901
Iteration 192/1000 | Loss: 0.00000901
Iteration 193/1000 | Loss: 0.00000900
Iteration 194/1000 | Loss: 0.00000900
Iteration 195/1000 | Loss: 0.00000900
Iteration 196/1000 | Loss: 0.00000900
Iteration 197/1000 | Loss: 0.00000900
Iteration 198/1000 | Loss: 0.00000900
Iteration 199/1000 | Loss: 0.00000900
Iteration 200/1000 | Loss: 0.00000900
Iteration 201/1000 | Loss: 0.00000900
Iteration 202/1000 | Loss: 0.00000900
Iteration 203/1000 | Loss: 0.00000900
Iteration 204/1000 | Loss: 0.00000900
Iteration 205/1000 | Loss: 0.00000900
Iteration 206/1000 | Loss: 0.00000900
Iteration 207/1000 | Loss: 0.00000900
Iteration 208/1000 | Loss: 0.00000900
Iteration 209/1000 | Loss: 0.00000900
Iteration 210/1000 | Loss: 0.00000900
Iteration 211/1000 | Loss: 0.00000900
Iteration 212/1000 | Loss: 0.00000900
Iteration 213/1000 | Loss: 0.00000899
Iteration 214/1000 | Loss: 0.00000899
Iteration 215/1000 | Loss: 0.00000899
Iteration 216/1000 | Loss: 0.00000899
Iteration 217/1000 | Loss: 0.00000899
Iteration 218/1000 | Loss: 0.00000899
Iteration 219/1000 | Loss: 0.00000899
Iteration 220/1000 | Loss: 0.00000899
Iteration 221/1000 | Loss: 0.00000899
Iteration 222/1000 | Loss: 0.00000899
Iteration 223/1000 | Loss: 0.00000899
Iteration 224/1000 | Loss: 0.00000899
Iteration 225/1000 | Loss: 0.00000899
Iteration 226/1000 | Loss: 0.00000899
Iteration 227/1000 | Loss: 0.00000899
Iteration 228/1000 | Loss: 0.00000899
Iteration 229/1000 | Loss: 0.00000899
Iteration 230/1000 | Loss: 0.00000899
Iteration 231/1000 | Loss: 0.00000899
Iteration 232/1000 | Loss: 0.00000899
Iteration 233/1000 | Loss: 0.00000899
Iteration 234/1000 | Loss: 0.00000899
Iteration 235/1000 | Loss: 0.00000898
Iteration 236/1000 | Loss: 0.00000898
Iteration 237/1000 | Loss: 0.00000898
Iteration 238/1000 | Loss: 0.00000898
Iteration 239/1000 | Loss: 0.00000898
Iteration 240/1000 | Loss: 0.00000898
Iteration 241/1000 | Loss: 0.00000898
Iteration 242/1000 | Loss: 0.00000898
Iteration 243/1000 | Loss: 0.00000898
Iteration 244/1000 | Loss: 0.00000898
Iteration 245/1000 | Loss: 0.00000898
Iteration 246/1000 | Loss: 0.00000898
Iteration 247/1000 | Loss: 0.00000898
Iteration 248/1000 | Loss: 0.00000898
Iteration 249/1000 | Loss: 0.00000898
Iteration 250/1000 | Loss: 0.00000898
Iteration 251/1000 | Loss: 0.00000898
Iteration 252/1000 | Loss: 0.00000897
Iteration 253/1000 | Loss: 0.00000897
Iteration 254/1000 | Loss: 0.00000897
Iteration 255/1000 | Loss: 0.00000897
Iteration 256/1000 | Loss: 0.00000897
Iteration 257/1000 | Loss: 0.00000897
Iteration 258/1000 | Loss: 0.00000897
Iteration 259/1000 | Loss: 0.00000897
Iteration 260/1000 | Loss: 0.00000897
Iteration 261/1000 | Loss: 0.00000897
Iteration 262/1000 | Loss: 0.00000897
Iteration 263/1000 | Loss: 0.00000897
Iteration 264/1000 | Loss: 0.00000897
Iteration 265/1000 | Loss: 0.00000897
Iteration 266/1000 | Loss: 0.00000896
Iteration 267/1000 | Loss: 0.00000896
Iteration 268/1000 | Loss: 0.00000896
Iteration 269/1000 | Loss: 0.00000896
Iteration 270/1000 | Loss: 0.00000896
Iteration 271/1000 | Loss: 0.00000896
Iteration 272/1000 | Loss: 0.00000896
Iteration 273/1000 | Loss: 0.00000896
Iteration 274/1000 | Loss: 0.00000896
Iteration 275/1000 | Loss: 0.00000896
Iteration 276/1000 | Loss: 0.00000896
Iteration 277/1000 | Loss: 0.00000896
Iteration 278/1000 | Loss: 0.00000896
Iteration 279/1000 | Loss: 0.00000896
Iteration 280/1000 | Loss: 0.00000896
Iteration 281/1000 | Loss: 0.00000896
Iteration 282/1000 | Loss: 0.00000896
Iteration 283/1000 | Loss: 0.00000896
Iteration 284/1000 | Loss: 0.00000896
Iteration 285/1000 | Loss: 0.00000896
Iteration 286/1000 | Loss: 0.00000896
Iteration 287/1000 | Loss: 0.00000896
Iteration 288/1000 | Loss: 0.00000896
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 288. Stopping optimization.
Last 5 losses: [8.962467290984932e-06, 8.962467290984932e-06, 8.962467290984932e-06, 8.962467290984932e-06, 8.962467290984932e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.962467290984932e-06

Optimization complete. Final v2v error: 2.552506685256958 mm

Highest mean error: 3.2004406452178955 mm for frame 51

Lowest mean error: 2.325120449066162 mm for frame 101

Saving results

Total time: 42.05139946937561
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_022/1046/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_022/1046.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_022/1046
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00837590
Iteration 2/25 | Loss: 0.00185708
Iteration 3/25 | Loss: 0.00133835
Iteration 4/25 | Loss: 0.00123360
Iteration 5/25 | Loss: 0.00121798
Iteration 6/25 | Loss: 0.00121559
Iteration 7/25 | Loss: 0.00121544
Iteration 8/25 | Loss: 0.00121544
Iteration 9/25 | Loss: 0.00121544
Iteration 10/25 | Loss: 0.00121544
Iteration 11/25 | Loss: 0.00121544
Iteration 12/25 | Loss: 0.00121544
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.001215443597175181, 0.001215443597175181, 0.001215443597175181, 0.001215443597175181, 0.001215443597175181]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001215443597175181

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34591436
Iteration 2/25 | Loss: 0.00065720
Iteration 3/25 | Loss: 0.00065720
Iteration 4/25 | Loss: 0.00065720
Iteration 5/25 | Loss: 0.00065720
Iteration 6/25 | Loss: 0.00065720
Iteration 7/25 | Loss: 0.00065720
Iteration 8/25 | Loss: 0.00065720
Iteration 9/25 | Loss: 0.00065720
Iteration 10/25 | Loss: 0.00065720
Iteration 11/25 | Loss: 0.00065720
Iteration 12/25 | Loss: 0.00065720
Iteration 13/25 | Loss: 0.00065720
Iteration 14/25 | Loss: 0.00065720
Iteration 15/25 | Loss: 0.00065720
Iteration 16/25 | Loss: 0.00065720
Iteration 17/25 | Loss: 0.00065720
Iteration 18/25 | Loss: 0.00065720
Iteration 19/25 | Loss: 0.00065720
Iteration 20/25 | Loss: 0.00065720
Iteration 21/25 | Loss: 0.00065720
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.000657196156680584, 0.000657196156680584, 0.000657196156680584, 0.000657196156680584, 0.000657196156680584]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000657196156680584

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00065720
Iteration 2/1000 | Loss: 0.00005405
Iteration 3/1000 | Loss: 0.00003616
Iteration 4/1000 | Loss: 0.00003132
Iteration 5/1000 | Loss: 0.00002952
Iteration 6/1000 | Loss: 0.00002861
Iteration 7/1000 | Loss: 0.00002783
Iteration 8/1000 | Loss: 0.00002733
Iteration 9/1000 | Loss: 0.00002698
Iteration 10/1000 | Loss: 0.00002661
Iteration 11/1000 | Loss: 0.00002637
Iteration 12/1000 | Loss: 0.00002616
Iteration 13/1000 | Loss: 0.00002606
Iteration 14/1000 | Loss: 0.00002603
Iteration 15/1000 | Loss: 0.00002602
Iteration 16/1000 | Loss: 0.00002588
Iteration 17/1000 | Loss: 0.00002574
Iteration 18/1000 | Loss: 0.00002572
Iteration 19/1000 | Loss: 0.00002572
Iteration 20/1000 | Loss: 0.00002571
Iteration 21/1000 | Loss: 0.00002570
Iteration 22/1000 | Loss: 0.00002570
Iteration 23/1000 | Loss: 0.00002569
Iteration 24/1000 | Loss: 0.00002569
Iteration 25/1000 | Loss: 0.00002569
Iteration 26/1000 | Loss: 0.00002569
Iteration 27/1000 | Loss: 0.00002568
Iteration 28/1000 | Loss: 0.00002568
Iteration 29/1000 | Loss: 0.00002567
Iteration 30/1000 | Loss: 0.00002567
Iteration 31/1000 | Loss: 0.00002566
Iteration 32/1000 | Loss: 0.00002566
Iteration 33/1000 | Loss: 0.00002566
Iteration 34/1000 | Loss: 0.00002566
Iteration 35/1000 | Loss: 0.00002565
Iteration 36/1000 | Loss: 0.00002565
Iteration 37/1000 | Loss: 0.00002565
Iteration 38/1000 | Loss: 0.00002565
Iteration 39/1000 | Loss: 0.00002565
Iteration 40/1000 | Loss: 0.00002564
Iteration 41/1000 | Loss: 0.00002564
Iteration 42/1000 | Loss: 0.00002564
Iteration 43/1000 | Loss: 0.00002564
Iteration 44/1000 | Loss: 0.00002564
Iteration 45/1000 | Loss: 0.00002564
Iteration 46/1000 | Loss: 0.00002564
Iteration 47/1000 | Loss: 0.00002564
Iteration 48/1000 | Loss: 0.00002563
Iteration 49/1000 | Loss: 0.00002563
Iteration 50/1000 | Loss: 0.00002563
Iteration 51/1000 | Loss: 0.00002563
Iteration 52/1000 | Loss: 0.00002563
Iteration 53/1000 | Loss: 0.00002563
Iteration 54/1000 | Loss: 0.00002563
Iteration 55/1000 | Loss: 0.00002563
Iteration 56/1000 | Loss: 0.00002563
Iteration 57/1000 | Loss: 0.00002563
Iteration 58/1000 | Loss: 0.00002563
Iteration 59/1000 | Loss: 0.00002562
Iteration 60/1000 | Loss: 0.00002562
Iteration 61/1000 | Loss: 0.00002562
Iteration 62/1000 | Loss: 0.00002562
Iteration 63/1000 | Loss: 0.00002562
Iteration 64/1000 | Loss: 0.00002562
Iteration 65/1000 | Loss: 0.00002562
Iteration 66/1000 | Loss: 0.00002561
Iteration 67/1000 | Loss: 0.00002561
Iteration 68/1000 | Loss: 0.00002561
Iteration 69/1000 | Loss: 0.00002561
Iteration 70/1000 | Loss: 0.00002561
Iteration 71/1000 | Loss: 0.00002560
Iteration 72/1000 | Loss: 0.00002560
Iteration 73/1000 | Loss: 0.00002560
Iteration 74/1000 | Loss: 0.00002560
Iteration 75/1000 | Loss: 0.00002560
Iteration 76/1000 | Loss: 0.00002560
Iteration 77/1000 | Loss: 0.00002560
Iteration 78/1000 | Loss: 0.00002559
Iteration 79/1000 | Loss: 0.00002559
Iteration 80/1000 | Loss: 0.00002559
Iteration 81/1000 | Loss: 0.00002559
Iteration 82/1000 | Loss: 0.00002558
Iteration 83/1000 | Loss: 0.00002558
Iteration 84/1000 | Loss: 0.00002558
Iteration 85/1000 | Loss: 0.00002558
Iteration 86/1000 | Loss: 0.00002558
Iteration 87/1000 | Loss: 0.00002558
Iteration 88/1000 | Loss: 0.00002558
Iteration 89/1000 | Loss: 0.00002558
Iteration 90/1000 | Loss: 0.00002558
Iteration 91/1000 | Loss: 0.00002558
Iteration 92/1000 | Loss: 0.00002558
Iteration 93/1000 | Loss: 0.00002558
Iteration 94/1000 | Loss: 0.00002558
Iteration 95/1000 | Loss: 0.00002558
Iteration 96/1000 | Loss: 0.00002558
Iteration 97/1000 | Loss: 0.00002558
Iteration 98/1000 | Loss: 0.00002558
Iteration 99/1000 | Loss: 0.00002558
Iteration 100/1000 | Loss: 0.00002558
Iteration 101/1000 | Loss: 0.00002557
Iteration 102/1000 | Loss: 0.00002557
Iteration 103/1000 | Loss: 0.00002557
Iteration 104/1000 | Loss: 0.00002557
Iteration 105/1000 | Loss: 0.00002557
Iteration 106/1000 | Loss: 0.00002557
Iteration 107/1000 | Loss: 0.00002557
Iteration 108/1000 | Loss: 0.00002557
Iteration 109/1000 | Loss: 0.00002557
Iteration 110/1000 | Loss: 0.00002557
Iteration 111/1000 | Loss: 0.00002557
Iteration 112/1000 | Loss: 0.00002557
Iteration 113/1000 | Loss: 0.00002557
Iteration 114/1000 | Loss: 0.00002557
Iteration 115/1000 | Loss: 0.00002557
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 115. Stopping optimization.
Last 5 losses: [2.5571571313776076e-05, 2.5571571313776076e-05, 2.5571571313776076e-05, 2.5571571313776076e-05, 2.5571571313776076e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.5571571313776076e-05

Optimization complete. Final v2v error: 4.260695457458496 mm

Highest mean error: 4.8481125831604 mm for frame 29

Lowest mean error: 3.517289876937866 mm for frame 127

Saving results

Total time: 37.324374198913574
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_022/1087/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_022/1087.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_022/1087
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00574653
Iteration 2/25 | Loss: 0.00137502
Iteration 3/25 | Loss: 0.00122170
Iteration 4/25 | Loss: 0.00118363
Iteration 5/25 | Loss: 0.00117356
Iteration 6/25 | Loss: 0.00117044
Iteration 7/25 | Loss: 0.00116826
Iteration 8/25 | Loss: 0.00116940
Iteration 9/25 | Loss: 0.00116657
Iteration 10/25 | Loss: 0.00116522
Iteration 11/25 | Loss: 0.00116796
Iteration 12/25 | Loss: 0.00116509
Iteration 13/25 | Loss: 0.00116123
Iteration 14/25 | Loss: 0.00116008
Iteration 15/25 | Loss: 0.00115931
Iteration 16/25 | Loss: 0.00115923
Iteration 17/25 | Loss: 0.00115921
Iteration 18/25 | Loss: 0.00115921
Iteration 19/25 | Loss: 0.00115920
Iteration 20/25 | Loss: 0.00115920
Iteration 21/25 | Loss: 0.00115920
Iteration 22/25 | Loss: 0.00115920
Iteration 23/25 | Loss: 0.00115920
Iteration 24/25 | Loss: 0.00115920
Iteration 25/25 | Loss: 0.00115920

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.16365695
Iteration 2/25 | Loss: 0.00080907
Iteration 3/25 | Loss: 0.00080901
Iteration 4/25 | Loss: 0.00080901
Iteration 5/25 | Loss: 0.00080901
Iteration 6/25 | Loss: 0.00080901
Iteration 7/25 | Loss: 0.00080901
Iteration 8/25 | Loss: 0.00080901
Iteration 9/25 | Loss: 0.00080901
Iteration 10/25 | Loss: 0.00080901
Iteration 11/25 | Loss: 0.00080901
Iteration 12/25 | Loss: 0.00080901
Iteration 13/25 | Loss: 0.00080901
Iteration 14/25 | Loss: 0.00080901
Iteration 15/25 | Loss: 0.00080901
Iteration 16/25 | Loss: 0.00080901
Iteration 17/25 | Loss: 0.00080901
Iteration 18/25 | Loss: 0.00080901
Iteration 19/25 | Loss: 0.00080901
Iteration 20/25 | Loss: 0.00080901
Iteration 21/25 | Loss: 0.00080901
Iteration 22/25 | Loss: 0.00080901
Iteration 23/25 | Loss: 0.00080901
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0008090061601251364, 0.0008090061601251364, 0.0008090061601251364, 0.0008090061601251364, 0.0008090061601251364]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008090061601251364

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00080901
Iteration 2/1000 | Loss: 0.00004525
Iteration 3/1000 | Loss: 0.00002976
Iteration 4/1000 | Loss: 0.00002609
Iteration 5/1000 | Loss: 0.00002490
Iteration 6/1000 | Loss: 0.00002409
Iteration 7/1000 | Loss: 0.00002358
Iteration 8/1000 | Loss: 0.00002304
Iteration 9/1000 | Loss: 0.00002270
Iteration 10/1000 | Loss: 0.00002244
Iteration 11/1000 | Loss: 0.00002228
Iteration 12/1000 | Loss: 0.00002218
Iteration 13/1000 | Loss: 0.00002207
Iteration 14/1000 | Loss: 0.00002190
Iteration 15/1000 | Loss: 0.00002187
Iteration 16/1000 | Loss: 0.00002185
Iteration 17/1000 | Loss: 0.00002169
Iteration 18/1000 | Loss: 0.00002156
Iteration 19/1000 | Loss: 0.00002156
Iteration 20/1000 | Loss: 0.00002155
Iteration 21/1000 | Loss: 0.00002146
Iteration 22/1000 | Loss: 0.00002146
Iteration 23/1000 | Loss: 0.00002145
Iteration 24/1000 | Loss: 0.00002144
Iteration 25/1000 | Loss: 0.00002143
Iteration 26/1000 | Loss: 0.00002143
Iteration 27/1000 | Loss: 0.00002142
Iteration 28/1000 | Loss: 0.00002142
Iteration 29/1000 | Loss: 0.00002141
Iteration 30/1000 | Loss: 0.00002141
Iteration 31/1000 | Loss: 0.00002141
Iteration 32/1000 | Loss: 0.00002141
Iteration 33/1000 | Loss: 0.00002141
Iteration 34/1000 | Loss: 0.00002141
Iteration 35/1000 | Loss: 0.00002141
Iteration 36/1000 | Loss: 0.00002141
Iteration 37/1000 | Loss: 0.00002141
Iteration 38/1000 | Loss: 0.00002141
Iteration 39/1000 | Loss: 0.00002141
Iteration 40/1000 | Loss: 0.00002140
Iteration 41/1000 | Loss: 0.00002139
Iteration 42/1000 | Loss: 0.00002138
Iteration 43/1000 | Loss: 0.00002138
Iteration 44/1000 | Loss: 0.00002138
Iteration 45/1000 | Loss: 0.00002137
Iteration 46/1000 | Loss: 0.00002137
Iteration 47/1000 | Loss: 0.00002137
Iteration 48/1000 | Loss: 0.00002136
Iteration 49/1000 | Loss: 0.00002136
Iteration 50/1000 | Loss: 0.00002136
Iteration 51/1000 | Loss: 0.00002136
Iteration 52/1000 | Loss: 0.00002136
Iteration 53/1000 | Loss: 0.00002136
Iteration 54/1000 | Loss: 0.00002136
Iteration 55/1000 | Loss: 0.00002135
Iteration 56/1000 | Loss: 0.00002135
Iteration 57/1000 | Loss: 0.00002135
Iteration 58/1000 | Loss: 0.00002135
Iteration 59/1000 | Loss: 0.00002134
Iteration 60/1000 | Loss: 0.00002134
Iteration 61/1000 | Loss: 0.00002134
Iteration 62/1000 | Loss: 0.00002133
Iteration 63/1000 | Loss: 0.00002133
Iteration 64/1000 | Loss: 0.00002132
Iteration 65/1000 | Loss: 0.00002132
Iteration 66/1000 | Loss: 0.00002131
Iteration 67/1000 | Loss: 0.00002131
Iteration 68/1000 | Loss: 0.00002131
Iteration 69/1000 | Loss: 0.00002131
Iteration 70/1000 | Loss: 0.00002131
Iteration 71/1000 | Loss: 0.00002130
Iteration 72/1000 | Loss: 0.00002130
Iteration 73/1000 | Loss: 0.00002130
Iteration 74/1000 | Loss: 0.00002130
Iteration 75/1000 | Loss: 0.00002130
Iteration 76/1000 | Loss: 0.00002129
Iteration 77/1000 | Loss: 0.00002129
Iteration 78/1000 | Loss: 0.00002128
Iteration 79/1000 | Loss: 0.00002128
Iteration 80/1000 | Loss: 0.00002128
Iteration 81/1000 | Loss: 0.00002128
Iteration 82/1000 | Loss: 0.00002128
Iteration 83/1000 | Loss: 0.00002128
Iteration 84/1000 | Loss: 0.00002127
Iteration 85/1000 | Loss: 0.00002127
Iteration 86/1000 | Loss: 0.00002127
Iteration 87/1000 | Loss: 0.00002126
Iteration 88/1000 | Loss: 0.00002126
Iteration 89/1000 | Loss: 0.00002126
Iteration 90/1000 | Loss: 0.00002126
Iteration 91/1000 | Loss: 0.00002125
Iteration 92/1000 | Loss: 0.00002125
Iteration 93/1000 | Loss: 0.00002125
Iteration 94/1000 | Loss: 0.00002125
Iteration 95/1000 | Loss: 0.00002125
Iteration 96/1000 | Loss: 0.00002125
Iteration 97/1000 | Loss: 0.00002125
Iteration 98/1000 | Loss: 0.00002125
Iteration 99/1000 | Loss: 0.00002124
Iteration 100/1000 | Loss: 0.00002124
Iteration 101/1000 | Loss: 0.00002124
Iteration 102/1000 | Loss: 0.00002123
Iteration 103/1000 | Loss: 0.00002123
Iteration 104/1000 | Loss: 0.00002122
Iteration 105/1000 | Loss: 0.00002122
Iteration 106/1000 | Loss: 0.00002122
Iteration 107/1000 | Loss: 0.00002122
Iteration 108/1000 | Loss: 0.00002122
Iteration 109/1000 | Loss: 0.00002122
Iteration 110/1000 | Loss: 0.00002122
Iteration 111/1000 | Loss: 0.00002122
Iteration 112/1000 | Loss: 0.00002121
Iteration 113/1000 | Loss: 0.00002121
Iteration 114/1000 | Loss: 0.00002121
Iteration 115/1000 | Loss: 0.00002121
Iteration 116/1000 | Loss: 0.00002121
Iteration 117/1000 | Loss: 0.00002121
Iteration 118/1000 | Loss: 0.00002120
Iteration 119/1000 | Loss: 0.00002120
Iteration 120/1000 | Loss: 0.00002120
Iteration 121/1000 | Loss: 0.00002120
Iteration 122/1000 | Loss: 0.00002120
Iteration 123/1000 | Loss: 0.00002119
Iteration 124/1000 | Loss: 0.00002119
Iteration 125/1000 | Loss: 0.00002119
Iteration 126/1000 | Loss: 0.00002119
Iteration 127/1000 | Loss: 0.00002119
Iteration 128/1000 | Loss: 0.00002119
Iteration 129/1000 | Loss: 0.00002118
Iteration 130/1000 | Loss: 0.00002118
Iteration 131/1000 | Loss: 0.00002118
Iteration 132/1000 | Loss: 0.00002117
Iteration 133/1000 | Loss: 0.00002117
Iteration 134/1000 | Loss: 0.00002117
Iteration 135/1000 | Loss: 0.00002117
Iteration 136/1000 | Loss: 0.00002117
Iteration 137/1000 | Loss: 0.00002117
Iteration 138/1000 | Loss: 0.00002117
Iteration 139/1000 | Loss: 0.00002117
Iteration 140/1000 | Loss: 0.00002117
Iteration 141/1000 | Loss: 0.00002116
Iteration 142/1000 | Loss: 0.00002116
Iteration 143/1000 | Loss: 0.00002116
Iteration 144/1000 | Loss: 0.00002116
Iteration 145/1000 | Loss: 0.00002116
Iteration 146/1000 | Loss: 0.00002116
Iteration 147/1000 | Loss: 0.00002116
Iteration 148/1000 | Loss: 0.00002116
Iteration 149/1000 | Loss: 0.00002116
Iteration 150/1000 | Loss: 0.00002116
Iteration 151/1000 | Loss: 0.00002116
Iteration 152/1000 | Loss: 0.00002116
Iteration 153/1000 | Loss: 0.00002116
Iteration 154/1000 | Loss: 0.00002116
Iteration 155/1000 | Loss: 0.00002116
Iteration 156/1000 | Loss: 0.00002116
Iteration 157/1000 | Loss: 0.00002116
Iteration 158/1000 | Loss: 0.00002116
Iteration 159/1000 | Loss: 0.00002116
Iteration 160/1000 | Loss: 0.00002116
Iteration 161/1000 | Loss: 0.00002116
Iteration 162/1000 | Loss: 0.00002116
Iteration 163/1000 | Loss: 0.00002116
Iteration 164/1000 | Loss: 0.00002116
Iteration 165/1000 | Loss: 0.00002116
Iteration 166/1000 | Loss: 0.00002116
Iteration 167/1000 | Loss: 0.00002116
Iteration 168/1000 | Loss: 0.00002116
Iteration 169/1000 | Loss: 0.00002116
Iteration 170/1000 | Loss: 0.00002116
Iteration 171/1000 | Loss: 0.00002116
Iteration 172/1000 | Loss: 0.00002116
Iteration 173/1000 | Loss: 0.00002116
Iteration 174/1000 | Loss: 0.00002116
Iteration 175/1000 | Loss: 0.00002116
Iteration 176/1000 | Loss: 0.00002116
Iteration 177/1000 | Loss: 0.00002116
Iteration 178/1000 | Loss: 0.00002116
Iteration 179/1000 | Loss: 0.00002116
Iteration 180/1000 | Loss: 0.00002116
Iteration 181/1000 | Loss: 0.00002116
Iteration 182/1000 | Loss: 0.00002116
Iteration 183/1000 | Loss: 0.00002116
Iteration 184/1000 | Loss: 0.00002116
Iteration 185/1000 | Loss: 0.00002116
Iteration 186/1000 | Loss: 0.00002116
Iteration 187/1000 | Loss: 0.00002116
Iteration 188/1000 | Loss: 0.00002116
Iteration 189/1000 | Loss: 0.00002116
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 189. Stopping optimization.
Last 5 losses: [2.115586175932549e-05, 2.115586175932549e-05, 2.115586175932549e-05, 2.115586175932549e-05, 2.115586175932549e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.115586175932549e-05

Optimization complete. Final v2v error: 3.7403440475463867 mm

Highest mean error: 5.841810703277588 mm for frame 114

Lowest mean error: 3.1029305458068848 mm for frame 0

Saving results

Total time: 62.73914647102356
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_022/1035/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_022/1035.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_022/1035
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00547694
Iteration 2/25 | Loss: 0.00115175
Iteration 3/25 | Loss: 0.00108239
Iteration 4/25 | Loss: 0.00107191
Iteration 5/25 | Loss: 0.00106799
Iteration 6/25 | Loss: 0.00106717
Iteration 7/25 | Loss: 0.00106717
Iteration 8/25 | Loss: 0.00106717
Iteration 9/25 | Loss: 0.00106717
Iteration 10/25 | Loss: 0.00106717
Iteration 11/25 | Loss: 0.00106717
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010671659838408232, 0.0010671659838408232, 0.0010671659838408232, 0.0010671659838408232, 0.0010671659838408232]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010671659838408232

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.82576990
Iteration 2/25 | Loss: 0.00077414
Iteration 3/25 | Loss: 0.00077413
Iteration 4/25 | Loss: 0.00077413
Iteration 5/25 | Loss: 0.00077413
Iteration 6/25 | Loss: 0.00077413
Iteration 7/25 | Loss: 0.00077413
Iteration 8/25 | Loss: 0.00077413
Iteration 9/25 | Loss: 0.00077413
Iteration 10/25 | Loss: 0.00077413
Iteration 11/25 | Loss: 0.00077413
Iteration 12/25 | Loss: 0.00077413
Iteration 13/25 | Loss: 0.00077413
Iteration 14/25 | Loss: 0.00077413
Iteration 15/25 | Loss: 0.00077413
Iteration 16/25 | Loss: 0.00077413
Iteration 17/25 | Loss: 0.00077413
Iteration 18/25 | Loss: 0.00077413
Iteration 19/25 | Loss: 0.00077413
Iteration 20/25 | Loss: 0.00077413
Iteration 21/25 | Loss: 0.00077413
Iteration 22/25 | Loss: 0.00077413
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0007741291192360222, 0.0007741291192360222, 0.0007741291192360222, 0.0007741291192360222, 0.0007741291192360222]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007741291192360222

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00077413
Iteration 2/1000 | Loss: 0.00001938
Iteration 3/1000 | Loss: 0.00001440
Iteration 4/1000 | Loss: 0.00001311
Iteration 5/1000 | Loss: 0.00001242
Iteration 6/1000 | Loss: 0.00001199
Iteration 7/1000 | Loss: 0.00001154
Iteration 8/1000 | Loss: 0.00001126
Iteration 9/1000 | Loss: 0.00001098
Iteration 10/1000 | Loss: 0.00001078
Iteration 11/1000 | Loss: 0.00001063
Iteration 12/1000 | Loss: 0.00001051
Iteration 13/1000 | Loss: 0.00001050
Iteration 14/1000 | Loss: 0.00001047
Iteration 15/1000 | Loss: 0.00001045
Iteration 16/1000 | Loss: 0.00001043
Iteration 17/1000 | Loss: 0.00001043
Iteration 18/1000 | Loss: 0.00001038
Iteration 19/1000 | Loss: 0.00001037
Iteration 20/1000 | Loss: 0.00001037
Iteration 21/1000 | Loss: 0.00001037
Iteration 22/1000 | Loss: 0.00001037
Iteration 23/1000 | Loss: 0.00001033
Iteration 24/1000 | Loss: 0.00001032
Iteration 25/1000 | Loss: 0.00001031
Iteration 26/1000 | Loss: 0.00001030
Iteration 27/1000 | Loss: 0.00001030
Iteration 28/1000 | Loss: 0.00001030
Iteration 29/1000 | Loss: 0.00001030
Iteration 30/1000 | Loss: 0.00001029
Iteration 31/1000 | Loss: 0.00001027
Iteration 32/1000 | Loss: 0.00001027
Iteration 33/1000 | Loss: 0.00001027
Iteration 34/1000 | Loss: 0.00001027
Iteration 35/1000 | Loss: 0.00001026
Iteration 36/1000 | Loss: 0.00001026
Iteration 37/1000 | Loss: 0.00001026
Iteration 38/1000 | Loss: 0.00001026
Iteration 39/1000 | Loss: 0.00001026
Iteration 40/1000 | Loss: 0.00001026
Iteration 41/1000 | Loss: 0.00001026
Iteration 42/1000 | Loss: 0.00001026
Iteration 43/1000 | Loss: 0.00001026
Iteration 44/1000 | Loss: 0.00001025
Iteration 45/1000 | Loss: 0.00001023
Iteration 46/1000 | Loss: 0.00001023
Iteration 47/1000 | Loss: 0.00001022
Iteration 48/1000 | Loss: 0.00001022
Iteration 49/1000 | Loss: 0.00001021
Iteration 50/1000 | Loss: 0.00001020
Iteration 51/1000 | Loss: 0.00001019
Iteration 52/1000 | Loss: 0.00001017
Iteration 53/1000 | Loss: 0.00001017
Iteration 54/1000 | Loss: 0.00001017
Iteration 55/1000 | Loss: 0.00001016
Iteration 56/1000 | Loss: 0.00001016
Iteration 57/1000 | Loss: 0.00001016
Iteration 58/1000 | Loss: 0.00001015
Iteration 59/1000 | Loss: 0.00001014
Iteration 60/1000 | Loss: 0.00001014
Iteration 61/1000 | Loss: 0.00001014
Iteration 62/1000 | Loss: 0.00001014
Iteration 63/1000 | Loss: 0.00001013
Iteration 64/1000 | Loss: 0.00001013
Iteration 65/1000 | Loss: 0.00001012
Iteration 66/1000 | Loss: 0.00001012
Iteration 67/1000 | Loss: 0.00001011
Iteration 68/1000 | Loss: 0.00001011
Iteration 69/1000 | Loss: 0.00001011
Iteration 70/1000 | Loss: 0.00001010
Iteration 71/1000 | Loss: 0.00001010
Iteration 72/1000 | Loss: 0.00001009
Iteration 73/1000 | Loss: 0.00001009
Iteration 74/1000 | Loss: 0.00001008
Iteration 75/1000 | Loss: 0.00001007
Iteration 76/1000 | Loss: 0.00001007
Iteration 77/1000 | Loss: 0.00001006
Iteration 78/1000 | Loss: 0.00001006
Iteration 79/1000 | Loss: 0.00001006
Iteration 80/1000 | Loss: 0.00001006
Iteration 81/1000 | Loss: 0.00001006
Iteration 82/1000 | Loss: 0.00001006
Iteration 83/1000 | Loss: 0.00001006
Iteration 84/1000 | Loss: 0.00001006
Iteration 85/1000 | Loss: 0.00001006
Iteration 86/1000 | Loss: 0.00001003
Iteration 87/1000 | Loss: 0.00001003
Iteration 88/1000 | Loss: 0.00001003
Iteration 89/1000 | Loss: 0.00001003
Iteration 90/1000 | Loss: 0.00001003
Iteration 91/1000 | Loss: 0.00001003
Iteration 92/1000 | Loss: 0.00001003
Iteration 93/1000 | Loss: 0.00001003
Iteration 94/1000 | Loss: 0.00001003
Iteration 95/1000 | Loss: 0.00001003
Iteration 96/1000 | Loss: 0.00001003
Iteration 97/1000 | Loss: 0.00001003
Iteration 98/1000 | Loss: 0.00001003
Iteration 99/1000 | Loss: 0.00001002
Iteration 100/1000 | Loss: 0.00001002
Iteration 101/1000 | Loss: 0.00001002
Iteration 102/1000 | Loss: 0.00001002
Iteration 103/1000 | Loss: 0.00001002
Iteration 104/1000 | Loss: 0.00001002
Iteration 105/1000 | Loss: 0.00001002
Iteration 106/1000 | Loss: 0.00001002
Iteration 107/1000 | Loss: 0.00001002
Iteration 108/1000 | Loss: 0.00001002
Iteration 109/1000 | Loss: 0.00001002
Iteration 110/1000 | Loss: 0.00001002
Iteration 111/1000 | Loss: 0.00001002
Iteration 112/1000 | Loss: 0.00001002
Iteration 113/1000 | Loss: 0.00001002
Iteration 114/1000 | Loss: 0.00001002
Iteration 115/1000 | Loss: 0.00001002
Iteration 116/1000 | Loss: 0.00001002
Iteration 117/1000 | Loss: 0.00001002
Iteration 118/1000 | Loss: 0.00001002
Iteration 119/1000 | Loss: 0.00001002
Iteration 120/1000 | Loss: 0.00001002
Iteration 121/1000 | Loss: 0.00001002
Iteration 122/1000 | Loss: 0.00001002
Iteration 123/1000 | Loss: 0.00001002
Iteration 124/1000 | Loss: 0.00001002
Iteration 125/1000 | Loss: 0.00001002
Iteration 126/1000 | Loss: 0.00001002
Iteration 127/1000 | Loss: 0.00001002
Iteration 128/1000 | Loss: 0.00001002
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 128. Stopping optimization.
Last 5 losses: [1.001729197014356e-05, 1.001729197014356e-05, 1.001729197014356e-05, 1.001729197014356e-05, 1.001729197014356e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.001729197014356e-05

Optimization complete. Final v2v error: 2.739520311355591 mm

Highest mean error: 3.1195333003997803 mm for frame 137

Lowest mean error: 2.549806594848633 mm for frame 155

Saving results

Total time: 36.5311644077301
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_022/1093/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_022/1093.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_022/1093
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00996359
Iteration 2/25 | Loss: 0.00137209
Iteration 3/25 | Loss: 0.00115140
Iteration 4/25 | Loss: 0.00113221
Iteration 5/25 | Loss: 0.00112799
Iteration 6/25 | Loss: 0.00112796
Iteration 7/25 | Loss: 0.00112796
Iteration 8/25 | Loss: 0.00112796
Iteration 9/25 | Loss: 0.00112796
Iteration 10/25 | Loss: 0.00112796
Iteration 11/25 | Loss: 0.00112796
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011279638856649399, 0.0011279638856649399, 0.0011279638856649399, 0.0011279638856649399, 0.0011279638856649399]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011279638856649399

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33133495
Iteration 2/25 | Loss: 0.00071747
Iteration 3/25 | Loss: 0.00071747
Iteration 4/25 | Loss: 0.00071747
Iteration 5/25 | Loss: 0.00071747
Iteration 6/25 | Loss: 0.00071747
Iteration 7/25 | Loss: 0.00071747
Iteration 8/25 | Loss: 0.00071747
Iteration 9/25 | Loss: 0.00071747
Iteration 10/25 | Loss: 0.00071747
Iteration 11/25 | Loss: 0.00071747
Iteration 12/25 | Loss: 0.00071747
Iteration 13/25 | Loss: 0.00071747
Iteration 14/25 | Loss: 0.00071747
Iteration 15/25 | Loss: 0.00071747
Iteration 16/25 | Loss: 0.00071747
Iteration 17/25 | Loss: 0.00071747
Iteration 18/25 | Loss: 0.00071747
Iteration 19/25 | Loss: 0.00071747
Iteration 20/25 | Loss: 0.00071747
Iteration 21/25 | Loss: 0.00071747
Iteration 22/25 | Loss: 0.00071747
Iteration 23/25 | Loss: 0.00071747
Iteration 24/25 | Loss: 0.00071747
Iteration 25/25 | Loss: 0.00071747

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00071747
Iteration 2/1000 | Loss: 0.00002140
Iteration 3/1000 | Loss: 0.00001536
Iteration 4/1000 | Loss: 0.00001456
Iteration 5/1000 | Loss: 0.00001401
Iteration 6/1000 | Loss: 0.00001359
Iteration 7/1000 | Loss: 0.00001334
Iteration 8/1000 | Loss: 0.00001317
Iteration 9/1000 | Loss: 0.00001303
Iteration 10/1000 | Loss: 0.00001280
Iteration 11/1000 | Loss: 0.00001275
Iteration 12/1000 | Loss: 0.00001261
Iteration 13/1000 | Loss: 0.00001257
Iteration 14/1000 | Loss: 0.00001256
Iteration 15/1000 | Loss: 0.00001253
Iteration 16/1000 | Loss: 0.00001245
Iteration 17/1000 | Loss: 0.00001245
Iteration 18/1000 | Loss: 0.00001244
Iteration 19/1000 | Loss: 0.00001244
Iteration 20/1000 | Loss: 0.00001244
Iteration 21/1000 | Loss: 0.00001239
Iteration 22/1000 | Loss: 0.00001238
Iteration 23/1000 | Loss: 0.00001238
Iteration 24/1000 | Loss: 0.00001236
Iteration 25/1000 | Loss: 0.00001233
Iteration 26/1000 | Loss: 0.00001233
Iteration 27/1000 | Loss: 0.00001233
Iteration 28/1000 | Loss: 0.00001233
Iteration 29/1000 | Loss: 0.00001233
Iteration 30/1000 | Loss: 0.00001233
Iteration 31/1000 | Loss: 0.00001232
Iteration 32/1000 | Loss: 0.00001232
Iteration 33/1000 | Loss: 0.00001232
Iteration 34/1000 | Loss: 0.00001232
Iteration 35/1000 | Loss: 0.00001232
Iteration 36/1000 | Loss: 0.00001232
Iteration 37/1000 | Loss: 0.00001232
Iteration 38/1000 | Loss: 0.00001232
Iteration 39/1000 | Loss: 0.00001231
Iteration 40/1000 | Loss: 0.00001230
Iteration 41/1000 | Loss: 0.00001230
Iteration 42/1000 | Loss: 0.00001230
Iteration 43/1000 | Loss: 0.00001229
Iteration 44/1000 | Loss: 0.00001228
Iteration 45/1000 | Loss: 0.00001228
Iteration 46/1000 | Loss: 0.00001228
Iteration 47/1000 | Loss: 0.00001228
Iteration 48/1000 | Loss: 0.00001228
Iteration 49/1000 | Loss: 0.00001228
Iteration 50/1000 | Loss: 0.00001228
Iteration 51/1000 | Loss: 0.00001227
Iteration 52/1000 | Loss: 0.00001227
Iteration 53/1000 | Loss: 0.00001227
Iteration 54/1000 | Loss: 0.00001227
Iteration 55/1000 | Loss: 0.00001226
Iteration 56/1000 | Loss: 0.00001226
Iteration 57/1000 | Loss: 0.00001225
Iteration 58/1000 | Loss: 0.00001225
Iteration 59/1000 | Loss: 0.00001224
Iteration 60/1000 | Loss: 0.00001224
Iteration 61/1000 | Loss: 0.00001223
Iteration 62/1000 | Loss: 0.00001223
Iteration 63/1000 | Loss: 0.00001223
Iteration 64/1000 | Loss: 0.00001223
Iteration 65/1000 | Loss: 0.00001223
Iteration 66/1000 | Loss: 0.00001223
Iteration 67/1000 | Loss: 0.00001223
Iteration 68/1000 | Loss: 0.00001223
Iteration 69/1000 | Loss: 0.00001221
Iteration 70/1000 | Loss: 0.00001221
Iteration 71/1000 | Loss: 0.00001220
Iteration 72/1000 | Loss: 0.00001220
Iteration 73/1000 | Loss: 0.00001220
Iteration 74/1000 | Loss: 0.00001219
Iteration 75/1000 | Loss: 0.00001219
Iteration 76/1000 | Loss: 0.00001219
Iteration 77/1000 | Loss: 0.00001218
Iteration 78/1000 | Loss: 0.00001218
Iteration 79/1000 | Loss: 0.00001218
Iteration 80/1000 | Loss: 0.00001218
Iteration 81/1000 | Loss: 0.00001218
Iteration 82/1000 | Loss: 0.00001218
Iteration 83/1000 | Loss: 0.00001218
Iteration 84/1000 | Loss: 0.00001218
Iteration 85/1000 | Loss: 0.00001218
Iteration 86/1000 | Loss: 0.00001218
Iteration 87/1000 | Loss: 0.00001218
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 87. Stopping optimization.
Last 5 losses: [1.2180620615254156e-05, 1.2180620615254156e-05, 1.2180620615254156e-05, 1.2180620615254156e-05, 1.2180620615254156e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2180620615254156e-05

Optimization complete. Final v2v error: 3.0044970512390137 mm

Highest mean error: 3.186922073364258 mm for frame 181

Lowest mean error: 2.904771089553833 mm for frame 144

Saving results

Total time: 30.643802881240845
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_022/1098/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_022/1098.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_022/1098
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00958468
Iteration 2/25 | Loss: 0.00249166
Iteration 3/25 | Loss: 0.00173708
Iteration 4/25 | Loss: 0.00157925
Iteration 5/25 | Loss: 0.00155750
Iteration 6/25 | Loss: 0.00145306
Iteration 7/25 | Loss: 0.00139960
Iteration 8/25 | Loss: 0.00135941
Iteration 9/25 | Loss: 0.00134649
Iteration 10/25 | Loss: 0.00135174
Iteration 11/25 | Loss: 0.00130951
Iteration 12/25 | Loss: 0.00130896
Iteration 13/25 | Loss: 0.00129347
Iteration 14/25 | Loss: 0.00129103
Iteration 15/25 | Loss: 0.00128477
Iteration 16/25 | Loss: 0.00126941
Iteration 17/25 | Loss: 0.00126882
Iteration 18/25 | Loss: 0.00126684
Iteration 19/25 | Loss: 0.00125729
Iteration 20/25 | Loss: 0.00125446
Iteration 21/25 | Loss: 0.00124745
Iteration 22/25 | Loss: 0.00123903
Iteration 23/25 | Loss: 0.00123848
Iteration 24/25 | Loss: 0.00123859
Iteration 25/25 | Loss: 0.00123463

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36819303
Iteration 2/25 | Loss: 0.00192585
Iteration 3/25 | Loss: 0.00159834
Iteration 4/25 | Loss: 0.00159834
Iteration 5/25 | Loss: 0.00159834
Iteration 6/25 | Loss: 0.00159834
Iteration 7/25 | Loss: 0.00159834
Iteration 8/25 | Loss: 0.00159834
Iteration 9/25 | Loss: 0.00159834
Iteration 10/25 | Loss: 0.00159834
Iteration 11/25 | Loss: 0.00159834
Iteration 12/25 | Loss: 0.00159834
Iteration 13/25 | Loss: 0.00159834
Iteration 14/25 | Loss: 0.00159834
Iteration 15/25 | Loss: 0.00159834
Iteration 16/25 | Loss: 0.00159834
Iteration 17/25 | Loss: 0.00159833
Iteration 18/25 | Loss: 0.00159834
Iteration 19/25 | Loss: 0.00159833
Iteration 20/25 | Loss: 0.00159833
Iteration 21/25 | Loss: 0.00159833
Iteration 22/25 | Loss: 0.00159833
Iteration 23/25 | Loss: 0.00159833
Iteration 24/25 | Loss: 0.00159833
Iteration 25/25 | Loss: 0.00159833

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00159833
Iteration 2/1000 | Loss: 0.00036942
Iteration 3/1000 | Loss: 0.00053121
Iteration 4/1000 | Loss: 0.00026359
Iteration 5/1000 | Loss: 0.00040502
Iteration 6/1000 | Loss: 0.00020452
Iteration 7/1000 | Loss: 0.00041852
Iteration 8/1000 | Loss: 0.00040283
Iteration 9/1000 | Loss: 0.00035804
Iteration 10/1000 | Loss: 0.00031247
Iteration 11/1000 | Loss: 0.00044840
Iteration 12/1000 | Loss: 0.00033257
Iteration 13/1000 | Loss: 0.00033309
Iteration 14/1000 | Loss: 0.00034123
Iteration 15/1000 | Loss: 0.00052180
Iteration 16/1000 | Loss: 0.00025798
Iteration 17/1000 | Loss: 0.00036118
Iteration 18/1000 | Loss: 0.00024823
Iteration 19/1000 | Loss: 0.00021816
Iteration 20/1000 | Loss: 0.00031697
Iteration 21/1000 | Loss: 0.00031615
Iteration 22/1000 | Loss: 0.00021860
Iteration 23/1000 | Loss: 0.00025363
Iteration 24/1000 | Loss: 0.00010460
Iteration 25/1000 | Loss: 0.00009229
Iteration 26/1000 | Loss: 0.00008298
Iteration 27/1000 | Loss: 0.00019260
Iteration 28/1000 | Loss: 0.00059513
Iteration 29/1000 | Loss: 0.00054152
Iteration 30/1000 | Loss: 0.00039890
Iteration 31/1000 | Loss: 0.00057001
Iteration 32/1000 | Loss: 0.00036155
Iteration 33/1000 | Loss: 0.00023069
Iteration 34/1000 | Loss: 0.00034796
Iteration 35/1000 | Loss: 0.00012337
Iteration 36/1000 | Loss: 0.00008839
Iteration 37/1000 | Loss: 0.00008942
Iteration 38/1000 | Loss: 0.00015921
Iteration 39/1000 | Loss: 0.00024462
Iteration 40/1000 | Loss: 0.00071426
Iteration 41/1000 | Loss: 0.00027736
Iteration 42/1000 | Loss: 0.00025828
Iteration 43/1000 | Loss: 0.00034791
Iteration 44/1000 | Loss: 0.00020592
Iteration 45/1000 | Loss: 0.00028410
Iteration 46/1000 | Loss: 0.00017525
Iteration 47/1000 | Loss: 0.00008222
Iteration 48/1000 | Loss: 0.00020052
Iteration 49/1000 | Loss: 0.00018630
Iteration 50/1000 | Loss: 0.00020075
Iteration 51/1000 | Loss: 0.00010281
Iteration 52/1000 | Loss: 0.00016650
Iteration 53/1000 | Loss: 0.00010769
Iteration 54/1000 | Loss: 0.00012857
Iteration 55/1000 | Loss: 0.00009608
Iteration 56/1000 | Loss: 0.00008266
Iteration 57/1000 | Loss: 0.00008630
Iteration 58/1000 | Loss: 0.00020109
Iteration 59/1000 | Loss: 0.00016050
Iteration 60/1000 | Loss: 0.00009182
Iteration 61/1000 | Loss: 0.00025694
Iteration 62/1000 | Loss: 0.00021904
Iteration 63/1000 | Loss: 0.00022898
Iteration 64/1000 | Loss: 0.00018207
Iteration 65/1000 | Loss: 0.00007035
Iteration 66/1000 | Loss: 0.00026717
Iteration 67/1000 | Loss: 0.00026715
Iteration 68/1000 | Loss: 0.00025823
Iteration 69/1000 | Loss: 0.00017234
Iteration 70/1000 | Loss: 0.00017244
Iteration 71/1000 | Loss: 0.00018327
Iteration 72/1000 | Loss: 0.00006347
Iteration 73/1000 | Loss: 0.00007087
Iteration 74/1000 | Loss: 0.00007094
Iteration 75/1000 | Loss: 0.00007650
Iteration 76/1000 | Loss: 0.00007916
Iteration 77/1000 | Loss: 0.00031599
Iteration 78/1000 | Loss: 0.00016526
Iteration 79/1000 | Loss: 0.00026147
Iteration 80/1000 | Loss: 0.00020352
Iteration 81/1000 | Loss: 0.00027216
Iteration 82/1000 | Loss: 0.00067896
Iteration 83/1000 | Loss: 0.00070422
Iteration 84/1000 | Loss: 0.00030201
Iteration 85/1000 | Loss: 0.00030664
Iteration 86/1000 | Loss: 0.00059495
Iteration 87/1000 | Loss: 0.00020280
Iteration 88/1000 | Loss: 0.00052334
Iteration 89/1000 | Loss: 0.00008339
Iteration 90/1000 | Loss: 0.00007816
Iteration 91/1000 | Loss: 0.00008300
Iteration 92/1000 | Loss: 0.00007486
Iteration 93/1000 | Loss: 0.00006511
Iteration 94/1000 | Loss: 0.00006807
Iteration 95/1000 | Loss: 0.00006805
Iteration 96/1000 | Loss: 0.00006278
Iteration 97/1000 | Loss: 0.00005621
Iteration 98/1000 | Loss: 0.00005551
Iteration 99/1000 | Loss: 0.00060501
Iteration 100/1000 | Loss: 0.00025326
Iteration 101/1000 | Loss: 0.00074713
Iteration 102/1000 | Loss: 0.00043229
Iteration 103/1000 | Loss: 0.00072782
Iteration 104/1000 | Loss: 0.00033649
Iteration 105/1000 | Loss: 0.00081365
Iteration 106/1000 | Loss: 0.00183249
Iteration 107/1000 | Loss: 0.00202861
Iteration 108/1000 | Loss: 0.00167898
Iteration 109/1000 | Loss: 0.00186543
Iteration 110/1000 | Loss: 0.00044466
Iteration 111/1000 | Loss: 0.00018552
Iteration 112/1000 | Loss: 0.00048503
Iteration 113/1000 | Loss: 0.00012651
Iteration 114/1000 | Loss: 0.00005738
Iteration 115/1000 | Loss: 0.00004958
Iteration 116/1000 | Loss: 0.00009532
Iteration 117/1000 | Loss: 0.00004444
Iteration 118/1000 | Loss: 0.00004261
Iteration 119/1000 | Loss: 0.00004143
Iteration 120/1000 | Loss: 0.00038009
Iteration 121/1000 | Loss: 0.00007184
Iteration 122/1000 | Loss: 0.00035422
Iteration 123/1000 | Loss: 0.00024813
Iteration 124/1000 | Loss: 0.00004956
Iteration 125/1000 | Loss: 0.00004057
Iteration 126/1000 | Loss: 0.00021803
Iteration 127/1000 | Loss: 0.00003825
Iteration 128/1000 | Loss: 0.00003618
Iteration 129/1000 | Loss: 0.00003463
Iteration 130/1000 | Loss: 0.00003350
Iteration 131/1000 | Loss: 0.00003281
Iteration 132/1000 | Loss: 0.00025378
Iteration 133/1000 | Loss: 0.00024983
Iteration 134/1000 | Loss: 0.00003222
Iteration 135/1000 | Loss: 0.00003191
Iteration 136/1000 | Loss: 0.00003170
Iteration 137/1000 | Loss: 0.00056571
Iteration 138/1000 | Loss: 0.00025925
Iteration 139/1000 | Loss: 0.00026548
Iteration 140/1000 | Loss: 0.00024864
Iteration 141/1000 | Loss: 0.00016316
Iteration 142/1000 | Loss: 0.00019657
Iteration 143/1000 | Loss: 0.00026698
Iteration 144/1000 | Loss: 0.00023038
Iteration 145/1000 | Loss: 0.00011359
Iteration 146/1000 | Loss: 0.00003581
Iteration 147/1000 | Loss: 0.00028257
Iteration 148/1000 | Loss: 0.00026004
Iteration 149/1000 | Loss: 0.00008194
Iteration 150/1000 | Loss: 0.00012136
Iteration 151/1000 | Loss: 0.00018042
Iteration 152/1000 | Loss: 0.00020933
Iteration 153/1000 | Loss: 0.00003813
Iteration 154/1000 | Loss: 0.00003382
Iteration 155/1000 | Loss: 0.00003282
Iteration 156/1000 | Loss: 0.00003174
Iteration 157/1000 | Loss: 0.00031761
Iteration 158/1000 | Loss: 0.00012112
Iteration 159/1000 | Loss: 0.00004495
Iteration 160/1000 | Loss: 0.00003442
Iteration 161/1000 | Loss: 0.00003131
Iteration 162/1000 | Loss: 0.00017291
Iteration 163/1000 | Loss: 0.00034462
Iteration 164/1000 | Loss: 0.00015215
Iteration 165/1000 | Loss: 0.00004374
Iteration 166/1000 | Loss: 0.00020547
Iteration 167/1000 | Loss: 0.00011501
Iteration 168/1000 | Loss: 0.00019289
Iteration 169/1000 | Loss: 0.00004176
Iteration 170/1000 | Loss: 0.00015999
Iteration 171/1000 | Loss: 0.00004181
Iteration 172/1000 | Loss: 0.00003512
Iteration 173/1000 | Loss: 0.00003206
Iteration 174/1000 | Loss: 0.00003029
Iteration 175/1000 | Loss: 0.00002897
Iteration 176/1000 | Loss: 0.00002866
Iteration 177/1000 | Loss: 0.00002825
Iteration 178/1000 | Loss: 0.00002816
Iteration 179/1000 | Loss: 0.00002790
Iteration 180/1000 | Loss: 0.00002761
Iteration 181/1000 | Loss: 0.00002742
Iteration 182/1000 | Loss: 0.00002739
Iteration 183/1000 | Loss: 0.00002738
Iteration 184/1000 | Loss: 0.00002736
Iteration 185/1000 | Loss: 0.00002735
Iteration 186/1000 | Loss: 0.00002734
Iteration 187/1000 | Loss: 0.00002734
Iteration 188/1000 | Loss: 0.00002734
Iteration 189/1000 | Loss: 0.00002733
Iteration 190/1000 | Loss: 0.00002733
Iteration 191/1000 | Loss: 0.00002733
Iteration 192/1000 | Loss: 0.00002733
Iteration 193/1000 | Loss: 0.00002732
Iteration 194/1000 | Loss: 0.00002732
Iteration 195/1000 | Loss: 0.00002732
Iteration 196/1000 | Loss: 0.00002731
Iteration 197/1000 | Loss: 0.00002731
Iteration 198/1000 | Loss: 0.00002731
Iteration 199/1000 | Loss: 0.00002731
Iteration 200/1000 | Loss: 0.00002731
Iteration 201/1000 | Loss: 0.00002731
Iteration 202/1000 | Loss: 0.00002731
Iteration 203/1000 | Loss: 0.00002731
Iteration 204/1000 | Loss: 0.00002731
Iteration 205/1000 | Loss: 0.00002731
Iteration 206/1000 | Loss: 0.00002731
Iteration 207/1000 | Loss: 0.00002731
Iteration 208/1000 | Loss: 0.00002730
Iteration 209/1000 | Loss: 0.00002730
Iteration 210/1000 | Loss: 0.00002730
Iteration 211/1000 | Loss: 0.00002730
Iteration 212/1000 | Loss: 0.00002730
Iteration 213/1000 | Loss: 0.00002729
Iteration 214/1000 | Loss: 0.00002729
Iteration 215/1000 | Loss: 0.00002729
Iteration 216/1000 | Loss: 0.00002728
Iteration 217/1000 | Loss: 0.00002728
Iteration 218/1000 | Loss: 0.00002727
Iteration 219/1000 | Loss: 0.00002727
Iteration 220/1000 | Loss: 0.00002727
Iteration 221/1000 | Loss: 0.00002726
Iteration 222/1000 | Loss: 0.00002726
Iteration 223/1000 | Loss: 0.00002725
Iteration 224/1000 | Loss: 0.00002725
Iteration 225/1000 | Loss: 0.00002725
Iteration 226/1000 | Loss: 0.00002724
Iteration 227/1000 | Loss: 0.00002724
Iteration 228/1000 | Loss: 0.00002724
Iteration 229/1000 | Loss: 0.00002724
Iteration 230/1000 | Loss: 0.00002724
Iteration 231/1000 | Loss: 0.00002723
Iteration 232/1000 | Loss: 0.00002723
Iteration 233/1000 | Loss: 0.00002723
Iteration 234/1000 | Loss: 0.00002723
Iteration 235/1000 | Loss: 0.00002723
Iteration 236/1000 | Loss: 0.00002722
Iteration 237/1000 | Loss: 0.00002722
Iteration 238/1000 | Loss: 0.00002722
Iteration 239/1000 | Loss: 0.00002722
Iteration 240/1000 | Loss: 0.00002722
Iteration 241/1000 | Loss: 0.00002722
Iteration 242/1000 | Loss: 0.00002722
Iteration 243/1000 | Loss: 0.00002721
Iteration 244/1000 | Loss: 0.00002721
Iteration 245/1000 | Loss: 0.00002721
Iteration 246/1000 | Loss: 0.00002721
Iteration 247/1000 | Loss: 0.00002721
Iteration 248/1000 | Loss: 0.00002721
Iteration 249/1000 | Loss: 0.00002721
Iteration 250/1000 | Loss: 0.00002721
Iteration 251/1000 | Loss: 0.00002721
Iteration 252/1000 | Loss: 0.00002721
Iteration 253/1000 | Loss: 0.00002720
Iteration 254/1000 | Loss: 0.00002720
Iteration 255/1000 | Loss: 0.00002720
Iteration 256/1000 | Loss: 0.00002720
Iteration 257/1000 | Loss: 0.00002720
Iteration 258/1000 | Loss: 0.00002720
Iteration 259/1000 | Loss: 0.00002720
Iteration 260/1000 | Loss: 0.00002720
Iteration 261/1000 | Loss: 0.00002720
Iteration 262/1000 | Loss: 0.00002720
Iteration 263/1000 | Loss: 0.00002719
Iteration 264/1000 | Loss: 0.00002719
Iteration 265/1000 | Loss: 0.00002719
Iteration 266/1000 | Loss: 0.00002719
Iteration 267/1000 | Loss: 0.00002719
Iteration 268/1000 | Loss: 0.00002719
Iteration 269/1000 | Loss: 0.00002719
Iteration 270/1000 | Loss: 0.00002719
Iteration 271/1000 | Loss: 0.00002719
Iteration 272/1000 | Loss: 0.00002719
Iteration 273/1000 | Loss: 0.00002718
Iteration 274/1000 | Loss: 0.00002718
Iteration 275/1000 | Loss: 0.00002718
Iteration 276/1000 | Loss: 0.00002718
Iteration 277/1000 | Loss: 0.00002718
Iteration 278/1000 | Loss: 0.00002718
Iteration 279/1000 | Loss: 0.00002718
Iteration 280/1000 | Loss: 0.00002718
Iteration 281/1000 | Loss: 0.00002718
Iteration 282/1000 | Loss: 0.00002718
Iteration 283/1000 | Loss: 0.00002718
Iteration 284/1000 | Loss: 0.00002718
Iteration 285/1000 | Loss: 0.00002718
Iteration 286/1000 | Loss: 0.00002718
Iteration 287/1000 | Loss: 0.00002718
Iteration 288/1000 | Loss: 0.00002718
Iteration 289/1000 | Loss: 0.00002718
Iteration 290/1000 | Loss: 0.00002718
Iteration 291/1000 | Loss: 0.00002718
Iteration 292/1000 | Loss: 0.00002718
Iteration 293/1000 | Loss: 0.00002718
Iteration 294/1000 | Loss: 0.00002718
Iteration 295/1000 | Loss: 0.00002718
Iteration 296/1000 | Loss: 0.00002718
Iteration 297/1000 | Loss: 0.00002718
Iteration 298/1000 | Loss: 0.00002718
Iteration 299/1000 | Loss: 0.00002718
Iteration 300/1000 | Loss: 0.00002718
Iteration 301/1000 | Loss: 0.00002718
Iteration 302/1000 | Loss: 0.00002718
Iteration 303/1000 | Loss: 0.00002718
Iteration 304/1000 | Loss: 0.00002718
Iteration 305/1000 | Loss: 0.00002718
Iteration 306/1000 | Loss: 0.00002718
Iteration 307/1000 | Loss: 0.00002718
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 307. Stopping optimization.
Last 5 losses: [2.7175705326953903e-05, 2.7175705326953903e-05, 2.7175705326953903e-05, 2.7175705326953903e-05, 2.7175705326953903e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.7175705326953903e-05

Optimization complete. Final v2v error: 4.1374616622924805 mm

Highest mean error: 11.050086975097656 mm for frame 0

Lowest mean error: 3.0980589389801025 mm for frame 165

Saving results

Total time: 318.29197454452515
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_022/1011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_022/1011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_022/1011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00846817
Iteration 2/25 | Loss: 0.00115550
Iteration 3/25 | Loss: 0.00107421
Iteration 4/25 | Loss: 0.00106624
Iteration 5/25 | Loss: 0.00106338
Iteration 6/25 | Loss: 0.00106287
Iteration 7/25 | Loss: 0.00106287
Iteration 8/25 | Loss: 0.00106287
Iteration 9/25 | Loss: 0.00106287
Iteration 10/25 | Loss: 0.00106287
Iteration 11/25 | Loss: 0.00106287
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001062871073372662, 0.001062871073372662, 0.001062871073372662, 0.001062871073372662, 0.001062871073372662]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001062871073372662

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.68123472
Iteration 2/25 | Loss: 0.00085414
Iteration 3/25 | Loss: 0.00085414
Iteration 4/25 | Loss: 0.00085413
Iteration 5/25 | Loss: 0.00085413
Iteration 6/25 | Loss: 0.00085413
Iteration 7/25 | Loss: 0.00085413
Iteration 8/25 | Loss: 0.00085413
Iteration 9/25 | Loss: 0.00085413
Iteration 10/25 | Loss: 0.00085413
Iteration 11/25 | Loss: 0.00085413
Iteration 12/25 | Loss: 0.00085413
Iteration 13/25 | Loss: 0.00085413
Iteration 14/25 | Loss: 0.00085413
Iteration 15/25 | Loss: 0.00085413
Iteration 16/25 | Loss: 0.00085413
Iteration 17/25 | Loss: 0.00085413
Iteration 18/25 | Loss: 0.00085413
Iteration 19/25 | Loss: 0.00085413
Iteration 20/25 | Loss: 0.00085413
Iteration 21/25 | Loss: 0.00085413
Iteration 22/25 | Loss: 0.00085413
Iteration 23/25 | Loss: 0.00085413
Iteration 24/25 | Loss: 0.00085413
Iteration 25/25 | Loss: 0.00085413

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00085413
Iteration 2/1000 | Loss: 0.00001704
Iteration 3/1000 | Loss: 0.00001204
Iteration 4/1000 | Loss: 0.00001079
Iteration 5/1000 | Loss: 0.00001023
Iteration 6/1000 | Loss: 0.00000989
Iteration 7/1000 | Loss: 0.00000956
Iteration 8/1000 | Loss: 0.00000941
Iteration 9/1000 | Loss: 0.00000938
Iteration 10/1000 | Loss: 0.00000937
Iteration 11/1000 | Loss: 0.00000929
Iteration 12/1000 | Loss: 0.00000928
Iteration 13/1000 | Loss: 0.00000913
Iteration 14/1000 | Loss: 0.00000909
Iteration 15/1000 | Loss: 0.00000908
Iteration 16/1000 | Loss: 0.00000907
Iteration 17/1000 | Loss: 0.00000897
Iteration 18/1000 | Loss: 0.00000895
Iteration 19/1000 | Loss: 0.00000893
Iteration 20/1000 | Loss: 0.00000893
Iteration 21/1000 | Loss: 0.00000892
Iteration 22/1000 | Loss: 0.00000891
Iteration 23/1000 | Loss: 0.00000890
Iteration 24/1000 | Loss: 0.00000889
Iteration 25/1000 | Loss: 0.00000883
Iteration 26/1000 | Loss: 0.00000883
Iteration 27/1000 | Loss: 0.00000882
Iteration 28/1000 | Loss: 0.00000882
Iteration 29/1000 | Loss: 0.00000881
Iteration 30/1000 | Loss: 0.00000881
Iteration 31/1000 | Loss: 0.00000880
Iteration 32/1000 | Loss: 0.00000880
Iteration 33/1000 | Loss: 0.00000880
Iteration 34/1000 | Loss: 0.00000879
Iteration 35/1000 | Loss: 0.00000879
Iteration 36/1000 | Loss: 0.00000879
Iteration 37/1000 | Loss: 0.00000879
Iteration 38/1000 | Loss: 0.00000878
Iteration 39/1000 | Loss: 0.00000878
Iteration 40/1000 | Loss: 0.00000876
Iteration 41/1000 | Loss: 0.00000876
Iteration 42/1000 | Loss: 0.00000876
Iteration 43/1000 | Loss: 0.00000876
Iteration 44/1000 | Loss: 0.00000875
Iteration 45/1000 | Loss: 0.00000875
Iteration 46/1000 | Loss: 0.00000875
Iteration 47/1000 | Loss: 0.00000875
Iteration 48/1000 | Loss: 0.00000875
Iteration 49/1000 | Loss: 0.00000875
Iteration 50/1000 | Loss: 0.00000874
Iteration 51/1000 | Loss: 0.00000874
Iteration 52/1000 | Loss: 0.00000874
Iteration 53/1000 | Loss: 0.00000872
Iteration 54/1000 | Loss: 0.00000872
Iteration 55/1000 | Loss: 0.00000872
Iteration 56/1000 | Loss: 0.00000871
Iteration 57/1000 | Loss: 0.00000871
Iteration 58/1000 | Loss: 0.00000871
Iteration 59/1000 | Loss: 0.00000870
Iteration 60/1000 | Loss: 0.00000870
Iteration 61/1000 | Loss: 0.00000870
Iteration 62/1000 | Loss: 0.00000870
Iteration 63/1000 | Loss: 0.00000870
Iteration 64/1000 | Loss: 0.00000869
Iteration 65/1000 | Loss: 0.00000869
Iteration 66/1000 | Loss: 0.00000869
Iteration 67/1000 | Loss: 0.00000868
Iteration 68/1000 | Loss: 0.00000868
Iteration 69/1000 | Loss: 0.00000868
Iteration 70/1000 | Loss: 0.00000868
Iteration 71/1000 | Loss: 0.00000867
Iteration 72/1000 | Loss: 0.00000867
Iteration 73/1000 | Loss: 0.00000867
Iteration 74/1000 | Loss: 0.00000867
Iteration 75/1000 | Loss: 0.00000867
Iteration 76/1000 | Loss: 0.00000867
Iteration 77/1000 | Loss: 0.00000867
Iteration 78/1000 | Loss: 0.00000867
Iteration 79/1000 | Loss: 0.00000866
Iteration 80/1000 | Loss: 0.00000866
Iteration 81/1000 | Loss: 0.00000866
Iteration 82/1000 | Loss: 0.00000866
Iteration 83/1000 | Loss: 0.00000866
Iteration 84/1000 | Loss: 0.00000866
Iteration 85/1000 | Loss: 0.00000866
Iteration 86/1000 | Loss: 0.00000866
Iteration 87/1000 | Loss: 0.00000866
Iteration 88/1000 | Loss: 0.00000865
Iteration 89/1000 | Loss: 0.00000865
Iteration 90/1000 | Loss: 0.00000865
Iteration 91/1000 | Loss: 0.00000865
Iteration 92/1000 | Loss: 0.00000865
Iteration 93/1000 | Loss: 0.00000865
Iteration 94/1000 | Loss: 0.00000865
Iteration 95/1000 | Loss: 0.00000865
Iteration 96/1000 | Loss: 0.00000865
Iteration 97/1000 | Loss: 0.00000865
Iteration 98/1000 | Loss: 0.00000865
Iteration 99/1000 | Loss: 0.00000865
Iteration 100/1000 | Loss: 0.00000865
Iteration 101/1000 | Loss: 0.00000864
Iteration 102/1000 | Loss: 0.00000864
Iteration 103/1000 | Loss: 0.00000864
Iteration 104/1000 | Loss: 0.00000864
Iteration 105/1000 | Loss: 0.00000864
Iteration 106/1000 | Loss: 0.00000864
Iteration 107/1000 | Loss: 0.00000864
Iteration 108/1000 | Loss: 0.00000864
Iteration 109/1000 | Loss: 0.00000863
Iteration 110/1000 | Loss: 0.00000863
Iteration 111/1000 | Loss: 0.00000863
Iteration 112/1000 | Loss: 0.00000863
Iteration 113/1000 | Loss: 0.00000863
Iteration 114/1000 | Loss: 0.00000863
Iteration 115/1000 | Loss: 0.00000863
Iteration 116/1000 | Loss: 0.00000863
Iteration 117/1000 | Loss: 0.00000862
Iteration 118/1000 | Loss: 0.00000862
Iteration 119/1000 | Loss: 0.00000862
Iteration 120/1000 | Loss: 0.00000862
Iteration 121/1000 | Loss: 0.00000862
Iteration 122/1000 | Loss: 0.00000862
Iteration 123/1000 | Loss: 0.00000862
Iteration 124/1000 | Loss: 0.00000862
Iteration 125/1000 | Loss: 0.00000862
Iteration 126/1000 | Loss: 0.00000861
Iteration 127/1000 | Loss: 0.00000861
Iteration 128/1000 | Loss: 0.00000861
Iteration 129/1000 | Loss: 0.00000861
Iteration 130/1000 | Loss: 0.00000861
Iteration 131/1000 | Loss: 0.00000861
Iteration 132/1000 | Loss: 0.00000861
Iteration 133/1000 | Loss: 0.00000860
Iteration 134/1000 | Loss: 0.00000860
Iteration 135/1000 | Loss: 0.00000860
Iteration 136/1000 | Loss: 0.00000860
Iteration 137/1000 | Loss: 0.00000859
Iteration 138/1000 | Loss: 0.00000859
Iteration 139/1000 | Loss: 0.00000859
Iteration 140/1000 | Loss: 0.00000859
Iteration 141/1000 | Loss: 0.00000859
Iteration 142/1000 | Loss: 0.00000859
Iteration 143/1000 | Loss: 0.00000859
Iteration 144/1000 | Loss: 0.00000859
Iteration 145/1000 | Loss: 0.00000859
Iteration 146/1000 | Loss: 0.00000859
Iteration 147/1000 | Loss: 0.00000859
Iteration 148/1000 | Loss: 0.00000859
Iteration 149/1000 | Loss: 0.00000859
Iteration 150/1000 | Loss: 0.00000859
Iteration 151/1000 | Loss: 0.00000859
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 151. Stopping optimization.
Last 5 losses: [8.585980140196625e-06, 8.585980140196625e-06, 8.585980140196625e-06, 8.585980140196625e-06, 8.585980140196625e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.585980140196625e-06

Optimization complete. Final v2v error: 2.51985239982605 mm

Highest mean error: 2.8869097232818604 mm for frame 57

Lowest mean error: 2.3210926055908203 mm for frame 128

Saving results

Total time: 34.44797134399414
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_022/1017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_022/1017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_022/1017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00795771
Iteration 2/25 | Loss: 0.00126509
Iteration 3/25 | Loss: 0.00107650
Iteration 4/25 | Loss: 0.00106108
Iteration 5/25 | Loss: 0.00105816
Iteration 6/25 | Loss: 0.00105778
Iteration 7/25 | Loss: 0.00105778
Iteration 8/25 | Loss: 0.00105778
Iteration 9/25 | Loss: 0.00105778
Iteration 10/25 | Loss: 0.00105778
Iteration 11/25 | Loss: 0.00105778
Iteration 12/25 | Loss: 0.00105778
Iteration 13/25 | Loss: 0.00105778
Iteration 14/25 | Loss: 0.00105778
Iteration 15/25 | Loss: 0.00105778
Iteration 16/25 | Loss: 0.00105778
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0010577809298411012, 0.0010577809298411012, 0.0010577809298411012, 0.0010577809298411012, 0.0010577809298411012]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010577809298411012

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35765994
Iteration 2/25 | Loss: 0.00079557
Iteration 3/25 | Loss: 0.00079557
Iteration 4/25 | Loss: 0.00079556
Iteration 5/25 | Loss: 0.00079556
Iteration 6/25 | Loss: 0.00079556
Iteration 7/25 | Loss: 0.00079556
Iteration 8/25 | Loss: 0.00079556
Iteration 9/25 | Loss: 0.00079556
Iteration 10/25 | Loss: 0.00079556
Iteration 11/25 | Loss: 0.00079556
Iteration 12/25 | Loss: 0.00079556
Iteration 13/25 | Loss: 0.00079556
Iteration 14/25 | Loss: 0.00079556
Iteration 15/25 | Loss: 0.00079556
Iteration 16/25 | Loss: 0.00079556
Iteration 17/25 | Loss: 0.00079556
Iteration 18/25 | Loss: 0.00079556
Iteration 19/25 | Loss: 0.00079556
Iteration 20/25 | Loss: 0.00079556
Iteration 21/25 | Loss: 0.00079556
Iteration 22/25 | Loss: 0.00079556
Iteration 23/25 | Loss: 0.00079556
Iteration 24/25 | Loss: 0.00079556
Iteration 25/25 | Loss: 0.00079556

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00079556
Iteration 2/1000 | Loss: 0.00002075
Iteration 3/1000 | Loss: 0.00001348
Iteration 4/1000 | Loss: 0.00001214
Iteration 5/1000 | Loss: 0.00001122
Iteration 6/1000 | Loss: 0.00001060
Iteration 7/1000 | Loss: 0.00001019
Iteration 8/1000 | Loss: 0.00000995
Iteration 9/1000 | Loss: 0.00000985
Iteration 10/1000 | Loss: 0.00000981
Iteration 11/1000 | Loss: 0.00000953
Iteration 12/1000 | Loss: 0.00000942
Iteration 13/1000 | Loss: 0.00000940
Iteration 14/1000 | Loss: 0.00000939
Iteration 15/1000 | Loss: 0.00000938
Iteration 16/1000 | Loss: 0.00000937
Iteration 17/1000 | Loss: 0.00000937
Iteration 18/1000 | Loss: 0.00000935
Iteration 19/1000 | Loss: 0.00000935
Iteration 20/1000 | Loss: 0.00000934
Iteration 21/1000 | Loss: 0.00000931
Iteration 22/1000 | Loss: 0.00000928
Iteration 23/1000 | Loss: 0.00000923
Iteration 24/1000 | Loss: 0.00000919
Iteration 25/1000 | Loss: 0.00000917
Iteration 26/1000 | Loss: 0.00000917
Iteration 27/1000 | Loss: 0.00000916
Iteration 28/1000 | Loss: 0.00000911
Iteration 29/1000 | Loss: 0.00000910
Iteration 30/1000 | Loss: 0.00000909
Iteration 31/1000 | Loss: 0.00000909
Iteration 32/1000 | Loss: 0.00000908
Iteration 33/1000 | Loss: 0.00000908
Iteration 34/1000 | Loss: 0.00000907
Iteration 35/1000 | Loss: 0.00000907
Iteration 36/1000 | Loss: 0.00000907
Iteration 37/1000 | Loss: 0.00000907
Iteration 38/1000 | Loss: 0.00000907
Iteration 39/1000 | Loss: 0.00000907
Iteration 40/1000 | Loss: 0.00000907
Iteration 41/1000 | Loss: 0.00000906
Iteration 42/1000 | Loss: 0.00000906
Iteration 43/1000 | Loss: 0.00000906
Iteration 44/1000 | Loss: 0.00000906
Iteration 45/1000 | Loss: 0.00000906
Iteration 46/1000 | Loss: 0.00000906
Iteration 47/1000 | Loss: 0.00000906
Iteration 48/1000 | Loss: 0.00000905
Iteration 49/1000 | Loss: 0.00000905
Iteration 50/1000 | Loss: 0.00000902
Iteration 51/1000 | Loss: 0.00000901
Iteration 52/1000 | Loss: 0.00000901
Iteration 53/1000 | Loss: 0.00000901
Iteration 54/1000 | Loss: 0.00000901
Iteration 55/1000 | Loss: 0.00000900
Iteration 56/1000 | Loss: 0.00000900
Iteration 57/1000 | Loss: 0.00000899
Iteration 58/1000 | Loss: 0.00000899
Iteration 59/1000 | Loss: 0.00000898
Iteration 60/1000 | Loss: 0.00000898
Iteration 61/1000 | Loss: 0.00000897
Iteration 62/1000 | Loss: 0.00000897
Iteration 63/1000 | Loss: 0.00000897
Iteration 64/1000 | Loss: 0.00000897
Iteration 65/1000 | Loss: 0.00000897
Iteration 66/1000 | Loss: 0.00000897
Iteration 67/1000 | Loss: 0.00000896
Iteration 68/1000 | Loss: 0.00000896
Iteration 69/1000 | Loss: 0.00000896
Iteration 70/1000 | Loss: 0.00000896
Iteration 71/1000 | Loss: 0.00000896
Iteration 72/1000 | Loss: 0.00000896
Iteration 73/1000 | Loss: 0.00000896
Iteration 74/1000 | Loss: 0.00000895
Iteration 75/1000 | Loss: 0.00000893
Iteration 76/1000 | Loss: 0.00000893
Iteration 77/1000 | Loss: 0.00000893
Iteration 78/1000 | Loss: 0.00000893
Iteration 79/1000 | Loss: 0.00000893
Iteration 80/1000 | Loss: 0.00000892
Iteration 81/1000 | Loss: 0.00000892
Iteration 82/1000 | Loss: 0.00000892
Iteration 83/1000 | Loss: 0.00000892
Iteration 84/1000 | Loss: 0.00000892
Iteration 85/1000 | Loss: 0.00000892
Iteration 86/1000 | Loss: 0.00000892
Iteration 87/1000 | Loss: 0.00000891
Iteration 88/1000 | Loss: 0.00000891
Iteration 89/1000 | Loss: 0.00000890
Iteration 90/1000 | Loss: 0.00000890
Iteration 91/1000 | Loss: 0.00000889
Iteration 92/1000 | Loss: 0.00000889
Iteration 93/1000 | Loss: 0.00000889
Iteration 94/1000 | Loss: 0.00000889
Iteration 95/1000 | Loss: 0.00000889
Iteration 96/1000 | Loss: 0.00000889
Iteration 97/1000 | Loss: 0.00000889
Iteration 98/1000 | Loss: 0.00000888
Iteration 99/1000 | Loss: 0.00000888
Iteration 100/1000 | Loss: 0.00000888
Iteration 101/1000 | Loss: 0.00000887
Iteration 102/1000 | Loss: 0.00000887
Iteration 103/1000 | Loss: 0.00000887
Iteration 104/1000 | Loss: 0.00000887
Iteration 105/1000 | Loss: 0.00000887
Iteration 106/1000 | Loss: 0.00000886
Iteration 107/1000 | Loss: 0.00000886
Iteration 108/1000 | Loss: 0.00000886
Iteration 109/1000 | Loss: 0.00000886
Iteration 110/1000 | Loss: 0.00000886
Iteration 111/1000 | Loss: 0.00000886
Iteration 112/1000 | Loss: 0.00000886
Iteration 113/1000 | Loss: 0.00000886
Iteration 114/1000 | Loss: 0.00000885
Iteration 115/1000 | Loss: 0.00000885
Iteration 116/1000 | Loss: 0.00000885
Iteration 117/1000 | Loss: 0.00000885
Iteration 118/1000 | Loss: 0.00000885
Iteration 119/1000 | Loss: 0.00000885
Iteration 120/1000 | Loss: 0.00000885
Iteration 121/1000 | Loss: 0.00000884
Iteration 122/1000 | Loss: 0.00000884
Iteration 123/1000 | Loss: 0.00000884
Iteration 124/1000 | Loss: 0.00000884
Iteration 125/1000 | Loss: 0.00000884
Iteration 126/1000 | Loss: 0.00000884
Iteration 127/1000 | Loss: 0.00000883
Iteration 128/1000 | Loss: 0.00000883
Iteration 129/1000 | Loss: 0.00000882
Iteration 130/1000 | Loss: 0.00000882
Iteration 131/1000 | Loss: 0.00000881
Iteration 132/1000 | Loss: 0.00000881
Iteration 133/1000 | Loss: 0.00000881
Iteration 134/1000 | Loss: 0.00000880
Iteration 135/1000 | Loss: 0.00000880
Iteration 136/1000 | Loss: 0.00000879
Iteration 137/1000 | Loss: 0.00000879
Iteration 138/1000 | Loss: 0.00000879
Iteration 139/1000 | Loss: 0.00000879
Iteration 140/1000 | Loss: 0.00000879
Iteration 141/1000 | Loss: 0.00000879
Iteration 142/1000 | Loss: 0.00000879
Iteration 143/1000 | Loss: 0.00000879
Iteration 144/1000 | Loss: 0.00000879
Iteration 145/1000 | Loss: 0.00000878
Iteration 146/1000 | Loss: 0.00000878
Iteration 147/1000 | Loss: 0.00000878
Iteration 148/1000 | Loss: 0.00000878
Iteration 149/1000 | Loss: 0.00000878
Iteration 150/1000 | Loss: 0.00000877
Iteration 151/1000 | Loss: 0.00000877
Iteration 152/1000 | Loss: 0.00000877
Iteration 153/1000 | Loss: 0.00000876
Iteration 154/1000 | Loss: 0.00000876
Iteration 155/1000 | Loss: 0.00000876
Iteration 156/1000 | Loss: 0.00000876
Iteration 157/1000 | Loss: 0.00000876
Iteration 158/1000 | Loss: 0.00000876
Iteration 159/1000 | Loss: 0.00000875
Iteration 160/1000 | Loss: 0.00000875
Iteration 161/1000 | Loss: 0.00000875
Iteration 162/1000 | Loss: 0.00000875
Iteration 163/1000 | Loss: 0.00000875
Iteration 164/1000 | Loss: 0.00000875
Iteration 165/1000 | Loss: 0.00000875
Iteration 166/1000 | Loss: 0.00000875
Iteration 167/1000 | Loss: 0.00000874
Iteration 168/1000 | Loss: 0.00000874
Iteration 169/1000 | Loss: 0.00000874
Iteration 170/1000 | Loss: 0.00000874
Iteration 171/1000 | Loss: 0.00000874
Iteration 172/1000 | Loss: 0.00000874
Iteration 173/1000 | Loss: 0.00000874
Iteration 174/1000 | Loss: 0.00000874
Iteration 175/1000 | Loss: 0.00000873
Iteration 176/1000 | Loss: 0.00000873
Iteration 177/1000 | Loss: 0.00000873
Iteration 178/1000 | Loss: 0.00000873
Iteration 179/1000 | Loss: 0.00000873
Iteration 180/1000 | Loss: 0.00000873
Iteration 181/1000 | Loss: 0.00000873
Iteration 182/1000 | Loss: 0.00000873
Iteration 183/1000 | Loss: 0.00000873
Iteration 184/1000 | Loss: 0.00000873
Iteration 185/1000 | Loss: 0.00000873
Iteration 186/1000 | Loss: 0.00000873
Iteration 187/1000 | Loss: 0.00000873
Iteration 188/1000 | Loss: 0.00000872
Iteration 189/1000 | Loss: 0.00000872
Iteration 190/1000 | Loss: 0.00000872
Iteration 191/1000 | Loss: 0.00000872
Iteration 192/1000 | Loss: 0.00000872
Iteration 193/1000 | Loss: 0.00000871
Iteration 194/1000 | Loss: 0.00000871
Iteration 195/1000 | Loss: 0.00000871
Iteration 196/1000 | Loss: 0.00000871
Iteration 197/1000 | Loss: 0.00000871
Iteration 198/1000 | Loss: 0.00000871
Iteration 199/1000 | Loss: 0.00000871
Iteration 200/1000 | Loss: 0.00000871
Iteration 201/1000 | Loss: 0.00000871
Iteration 202/1000 | Loss: 0.00000871
Iteration 203/1000 | Loss: 0.00000871
Iteration 204/1000 | Loss: 0.00000871
Iteration 205/1000 | Loss: 0.00000871
Iteration 206/1000 | Loss: 0.00000871
Iteration 207/1000 | Loss: 0.00000871
Iteration 208/1000 | Loss: 0.00000871
Iteration 209/1000 | Loss: 0.00000871
Iteration 210/1000 | Loss: 0.00000871
Iteration 211/1000 | Loss: 0.00000870
Iteration 212/1000 | Loss: 0.00000870
Iteration 213/1000 | Loss: 0.00000870
Iteration 214/1000 | Loss: 0.00000870
Iteration 215/1000 | Loss: 0.00000870
Iteration 216/1000 | Loss: 0.00000870
Iteration 217/1000 | Loss: 0.00000870
Iteration 218/1000 | Loss: 0.00000870
Iteration 219/1000 | Loss: 0.00000870
Iteration 220/1000 | Loss: 0.00000870
Iteration 221/1000 | Loss: 0.00000869
Iteration 222/1000 | Loss: 0.00000869
Iteration 223/1000 | Loss: 0.00000869
Iteration 224/1000 | Loss: 0.00000869
Iteration 225/1000 | Loss: 0.00000869
Iteration 226/1000 | Loss: 0.00000869
Iteration 227/1000 | Loss: 0.00000869
Iteration 228/1000 | Loss: 0.00000869
Iteration 229/1000 | Loss: 0.00000869
Iteration 230/1000 | Loss: 0.00000869
Iteration 231/1000 | Loss: 0.00000869
Iteration 232/1000 | Loss: 0.00000869
Iteration 233/1000 | Loss: 0.00000869
Iteration 234/1000 | Loss: 0.00000869
Iteration 235/1000 | Loss: 0.00000869
Iteration 236/1000 | Loss: 0.00000869
Iteration 237/1000 | Loss: 0.00000869
Iteration 238/1000 | Loss: 0.00000869
Iteration 239/1000 | Loss: 0.00000869
Iteration 240/1000 | Loss: 0.00000868
Iteration 241/1000 | Loss: 0.00000868
Iteration 242/1000 | Loss: 0.00000868
Iteration 243/1000 | Loss: 0.00000868
Iteration 244/1000 | Loss: 0.00000868
Iteration 245/1000 | Loss: 0.00000868
Iteration 246/1000 | Loss: 0.00000868
Iteration 247/1000 | Loss: 0.00000867
Iteration 248/1000 | Loss: 0.00000867
Iteration 249/1000 | Loss: 0.00000867
Iteration 250/1000 | Loss: 0.00000867
Iteration 251/1000 | Loss: 0.00000867
Iteration 252/1000 | Loss: 0.00000867
Iteration 253/1000 | Loss: 0.00000867
Iteration 254/1000 | Loss: 0.00000867
Iteration 255/1000 | Loss: 0.00000867
Iteration 256/1000 | Loss: 0.00000867
Iteration 257/1000 | Loss: 0.00000867
Iteration 258/1000 | Loss: 0.00000866
Iteration 259/1000 | Loss: 0.00000866
Iteration 260/1000 | Loss: 0.00000866
Iteration 261/1000 | Loss: 0.00000866
Iteration 262/1000 | Loss: 0.00000866
Iteration 263/1000 | Loss: 0.00000866
Iteration 264/1000 | Loss: 0.00000866
Iteration 265/1000 | Loss: 0.00000866
Iteration 266/1000 | Loss: 0.00000866
Iteration 267/1000 | Loss: 0.00000866
Iteration 268/1000 | Loss: 0.00000865
Iteration 269/1000 | Loss: 0.00000865
Iteration 270/1000 | Loss: 0.00000865
Iteration 271/1000 | Loss: 0.00000865
Iteration 272/1000 | Loss: 0.00000865
Iteration 273/1000 | Loss: 0.00000865
Iteration 274/1000 | Loss: 0.00000864
Iteration 275/1000 | Loss: 0.00000864
Iteration 276/1000 | Loss: 0.00000864
Iteration 277/1000 | Loss: 0.00000864
Iteration 278/1000 | Loss: 0.00000864
Iteration 279/1000 | Loss: 0.00000864
Iteration 280/1000 | Loss: 0.00000864
Iteration 281/1000 | Loss: 0.00000864
Iteration 282/1000 | Loss: 0.00000864
Iteration 283/1000 | Loss: 0.00000864
Iteration 284/1000 | Loss: 0.00000864
Iteration 285/1000 | Loss: 0.00000864
Iteration 286/1000 | Loss: 0.00000864
Iteration 287/1000 | Loss: 0.00000864
Iteration 288/1000 | Loss: 0.00000864
Iteration 289/1000 | Loss: 0.00000864
Iteration 290/1000 | Loss: 0.00000864
Iteration 291/1000 | Loss: 0.00000864
Iteration 292/1000 | Loss: 0.00000864
Iteration 293/1000 | Loss: 0.00000864
Iteration 294/1000 | Loss: 0.00000864
Iteration 295/1000 | Loss: 0.00000864
Iteration 296/1000 | Loss: 0.00000864
Iteration 297/1000 | Loss: 0.00000864
Iteration 298/1000 | Loss: 0.00000864
Iteration 299/1000 | Loss: 0.00000864
Iteration 300/1000 | Loss: 0.00000864
Iteration 301/1000 | Loss: 0.00000864
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 301. Stopping optimization.
Last 5 losses: [8.637776772957295e-06, 8.637776772957295e-06, 8.637776772957295e-06, 8.637776772957295e-06, 8.637776772957295e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.637776772957295e-06

Optimization complete. Final v2v error: 2.502438545227051 mm

Highest mean error: 2.682903528213501 mm for frame 85

Lowest mean error: 2.3455116748809814 mm for frame 11

Saving results

Total time: 45.64865684509277
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_022/1097/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_022/1097.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_022/1097
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00730619
Iteration 2/25 | Loss: 0.00159091
Iteration 3/25 | Loss: 0.00136052
Iteration 4/25 | Loss: 0.00128604
Iteration 5/25 | Loss: 0.00129069
Iteration 6/25 | Loss: 0.00128317
Iteration 7/25 | Loss: 0.00121116
Iteration 8/25 | Loss: 0.00118844
Iteration 9/25 | Loss: 0.00117383
Iteration 10/25 | Loss: 0.00117071
Iteration 11/25 | Loss: 0.00117103
Iteration 12/25 | Loss: 0.00116122
Iteration 13/25 | Loss: 0.00115597
Iteration 14/25 | Loss: 0.00115478
Iteration 15/25 | Loss: 0.00115453
Iteration 16/25 | Loss: 0.00115453
Iteration 17/25 | Loss: 0.00115453
Iteration 18/25 | Loss: 0.00115453
Iteration 19/25 | Loss: 0.00115453
Iteration 20/25 | Loss: 0.00115452
Iteration 21/25 | Loss: 0.00115452
Iteration 22/25 | Loss: 0.00115452
Iteration 23/25 | Loss: 0.00115452
Iteration 24/25 | Loss: 0.00115452
Iteration 25/25 | Loss: 0.00115452

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.07925081
Iteration 2/25 | Loss: 0.00093411
Iteration 3/25 | Loss: 0.00093405
Iteration 4/25 | Loss: 0.00093405
Iteration 5/25 | Loss: 0.00093405
Iteration 6/25 | Loss: 0.00093405
Iteration 7/25 | Loss: 0.00093405
Iteration 8/25 | Loss: 0.00093405
Iteration 9/25 | Loss: 0.00093405
Iteration 10/25 | Loss: 0.00093405
Iteration 11/25 | Loss: 0.00093405
Iteration 12/25 | Loss: 0.00093405
Iteration 13/25 | Loss: 0.00093405
Iteration 14/25 | Loss: 0.00093405
Iteration 15/25 | Loss: 0.00093405
Iteration 16/25 | Loss: 0.00093405
Iteration 17/25 | Loss: 0.00093405
Iteration 18/25 | Loss: 0.00093405
Iteration 19/25 | Loss: 0.00093405
Iteration 20/25 | Loss: 0.00093405
Iteration 21/25 | Loss: 0.00093405
Iteration 22/25 | Loss: 0.00093405
Iteration 23/25 | Loss: 0.00093405
Iteration 24/25 | Loss: 0.00093405
Iteration 25/25 | Loss: 0.00093405

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00093405
Iteration 2/1000 | Loss: 0.00005532
Iteration 3/1000 | Loss: 0.00003482
Iteration 4/1000 | Loss: 0.00002843
Iteration 5/1000 | Loss: 0.00002569
Iteration 6/1000 | Loss: 0.00002388
Iteration 7/1000 | Loss: 0.00011962
Iteration 8/1000 | Loss: 0.00005061
Iteration 9/1000 | Loss: 0.00003920
Iteration 10/1000 | Loss: 0.00002304
Iteration 11/1000 | Loss: 0.00002112
Iteration 12/1000 | Loss: 0.00001986
Iteration 13/1000 | Loss: 0.00001894
Iteration 14/1000 | Loss: 0.00001861
Iteration 15/1000 | Loss: 0.00001860
Iteration 16/1000 | Loss: 0.00001827
Iteration 17/1000 | Loss: 0.00001798
Iteration 18/1000 | Loss: 0.00001773
Iteration 19/1000 | Loss: 0.00001760
Iteration 20/1000 | Loss: 0.00001760
Iteration 21/1000 | Loss: 0.00001759
Iteration 22/1000 | Loss: 0.00001757
Iteration 23/1000 | Loss: 0.00001755
Iteration 24/1000 | Loss: 0.00001751
Iteration 25/1000 | Loss: 0.00001747
Iteration 26/1000 | Loss: 0.00001744
Iteration 27/1000 | Loss: 0.00001743
Iteration 28/1000 | Loss: 0.00001741
Iteration 29/1000 | Loss: 0.00001740
Iteration 30/1000 | Loss: 0.00001738
Iteration 31/1000 | Loss: 0.00001737
Iteration 32/1000 | Loss: 0.00001735
Iteration 33/1000 | Loss: 0.00001732
Iteration 34/1000 | Loss: 0.00001729
Iteration 35/1000 | Loss: 0.00001729
Iteration 36/1000 | Loss: 0.00001728
Iteration 37/1000 | Loss: 0.00001728
Iteration 38/1000 | Loss: 0.00001727
Iteration 39/1000 | Loss: 0.00001727
Iteration 40/1000 | Loss: 0.00001726
Iteration 41/1000 | Loss: 0.00001726
Iteration 42/1000 | Loss: 0.00001726
Iteration 43/1000 | Loss: 0.00001725
Iteration 44/1000 | Loss: 0.00001725
Iteration 45/1000 | Loss: 0.00001725
Iteration 46/1000 | Loss: 0.00001724
Iteration 47/1000 | Loss: 0.00001724
Iteration 48/1000 | Loss: 0.00001724
Iteration 49/1000 | Loss: 0.00001724
Iteration 50/1000 | Loss: 0.00001724
Iteration 51/1000 | Loss: 0.00001724
Iteration 52/1000 | Loss: 0.00001724
Iteration 53/1000 | Loss: 0.00001724
Iteration 54/1000 | Loss: 0.00001724
Iteration 55/1000 | Loss: 0.00001723
Iteration 56/1000 | Loss: 0.00001723
Iteration 57/1000 | Loss: 0.00001723
Iteration 58/1000 | Loss: 0.00001722
Iteration 59/1000 | Loss: 0.00001722
Iteration 60/1000 | Loss: 0.00001722
Iteration 61/1000 | Loss: 0.00001722
Iteration 62/1000 | Loss: 0.00001722
Iteration 63/1000 | Loss: 0.00001721
Iteration 64/1000 | Loss: 0.00001721
Iteration 65/1000 | Loss: 0.00001721
Iteration 66/1000 | Loss: 0.00001720
Iteration 67/1000 | Loss: 0.00001720
Iteration 68/1000 | Loss: 0.00001720
Iteration 69/1000 | Loss: 0.00001720
Iteration 70/1000 | Loss: 0.00001720
Iteration 71/1000 | Loss: 0.00001719
Iteration 72/1000 | Loss: 0.00001719
Iteration 73/1000 | Loss: 0.00001719
Iteration 74/1000 | Loss: 0.00001718
Iteration 75/1000 | Loss: 0.00001718
Iteration 76/1000 | Loss: 0.00001718
Iteration 77/1000 | Loss: 0.00001717
Iteration 78/1000 | Loss: 0.00001717
Iteration 79/1000 | Loss: 0.00001717
Iteration 80/1000 | Loss: 0.00001716
Iteration 81/1000 | Loss: 0.00001716
Iteration 82/1000 | Loss: 0.00001716
Iteration 83/1000 | Loss: 0.00001716
Iteration 84/1000 | Loss: 0.00001716
Iteration 85/1000 | Loss: 0.00001715
Iteration 86/1000 | Loss: 0.00001715
Iteration 87/1000 | Loss: 0.00001715
Iteration 88/1000 | Loss: 0.00001714
Iteration 89/1000 | Loss: 0.00001714
Iteration 90/1000 | Loss: 0.00001714
Iteration 91/1000 | Loss: 0.00001714
Iteration 92/1000 | Loss: 0.00001714
Iteration 93/1000 | Loss: 0.00001714
Iteration 94/1000 | Loss: 0.00001713
Iteration 95/1000 | Loss: 0.00001713
Iteration 96/1000 | Loss: 0.00001713
Iteration 97/1000 | Loss: 0.00001713
Iteration 98/1000 | Loss: 0.00001712
Iteration 99/1000 | Loss: 0.00001712
Iteration 100/1000 | Loss: 0.00001712
Iteration 101/1000 | Loss: 0.00001712
Iteration 102/1000 | Loss: 0.00001711
Iteration 103/1000 | Loss: 0.00001711
Iteration 104/1000 | Loss: 0.00001711
Iteration 105/1000 | Loss: 0.00001711
Iteration 106/1000 | Loss: 0.00001710
Iteration 107/1000 | Loss: 0.00001710
Iteration 108/1000 | Loss: 0.00001710
Iteration 109/1000 | Loss: 0.00001710
Iteration 110/1000 | Loss: 0.00001710
Iteration 111/1000 | Loss: 0.00001709
Iteration 112/1000 | Loss: 0.00001709
Iteration 113/1000 | Loss: 0.00001709
Iteration 114/1000 | Loss: 0.00001709
Iteration 115/1000 | Loss: 0.00001709
Iteration 116/1000 | Loss: 0.00001708
Iteration 117/1000 | Loss: 0.00001708
Iteration 118/1000 | Loss: 0.00001708
Iteration 119/1000 | Loss: 0.00001708
Iteration 120/1000 | Loss: 0.00001708
Iteration 121/1000 | Loss: 0.00001708
Iteration 122/1000 | Loss: 0.00001707
Iteration 123/1000 | Loss: 0.00001707
Iteration 124/1000 | Loss: 0.00001707
Iteration 125/1000 | Loss: 0.00001707
Iteration 126/1000 | Loss: 0.00001707
Iteration 127/1000 | Loss: 0.00001707
Iteration 128/1000 | Loss: 0.00001707
Iteration 129/1000 | Loss: 0.00001707
Iteration 130/1000 | Loss: 0.00001707
Iteration 131/1000 | Loss: 0.00001706
Iteration 132/1000 | Loss: 0.00001706
Iteration 133/1000 | Loss: 0.00001706
Iteration 134/1000 | Loss: 0.00001706
Iteration 135/1000 | Loss: 0.00001706
Iteration 136/1000 | Loss: 0.00001706
Iteration 137/1000 | Loss: 0.00001706
Iteration 138/1000 | Loss: 0.00001706
Iteration 139/1000 | Loss: 0.00001706
Iteration 140/1000 | Loss: 0.00001706
Iteration 141/1000 | Loss: 0.00001706
Iteration 142/1000 | Loss: 0.00001706
Iteration 143/1000 | Loss: 0.00001705
Iteration 144/1000 | Loss: 0.00001705
Iteration 145/1000 | Loss: 0.00001705
Iteration 146/1000 | Loss: 0.00001705
Iteration 147/1000 | Loss: 0.00001705
Iteration 148/1000 | Loss: 0.00001705
Iteration 149/1000 | Loss: 0.00001705
Iteration 150/1000 | Loss: 0.00001705
Iteration 151/1000 | Loss: 0.00001705
Iteration 152/1000 | Loss: 0.00001705
Iteration 153/1000 | Loss: 0.00001705
Iteration 154/1000 | Loss: 0.00001705
Iteration 155/1000 | Loss: 0.00001705
Iteration 156/1000 | Loss: 0.00001705
Iteration 157/1000 | Loss: 0.00001704
Iteration 158/1000 | Loss: 0.00001704
Iteration 159/1000 | Loss: 0.00001704
Iteration 160/1000 | Loss: 0.00001704
Iteration 161/1000 | Loss: 0.00001704
Iteration 162/1000 | Loss: 0.00001704
Iteration 163/1000 | Loss: 0.00001704
Iteration 164/1000 | Loss: 0.00001704
Iteration 165/1000 | Loss: 0.00001703
Iteration 166/1000 | Loss: 0.00001703
Iteration 167/1000 | Loss: 0.00001703
Iteration 168/1000 | Loss: 0.00001703
Iteration 169/1000 | Loss: 0.00001703
Iteration 170/1000 | Loss: 0.00001703
Iteration 171/1000 | Loss: 0.00001703
Iteration 172/1000 | Loss: 0.00001703
Iteration 173/1000 | Loss: 0.00001703
Iteration 174/1000 | Loss: 0.00001703
Iteration 175/1000 | Loss: 0.00001703
Iteration 176/1000 | Loss: 0.00001703
Iteration 177/1000 | Loss: 0.00001703
Iteration 178/1000 | Loss: 0.00001703
Iteration 179/1000 | Loss: 0.00001703
Iteration 180/1000 | Loss: 0.00001702
Iteration 181/1000 | Loss: 0.00001702
Iteration 182/1000 | Loss: 0.00001702
Iteration 183/1000 | Loss: 0.00001702
Iteration 184/1000 | Loss: 0.00001702
Iteration 185/1000 | Loss: 0.00001702
Iteration 186/1000 | Loss: 0.00001701
Iteration 187/1000 | Loss: 0.00001701
Iteration 188/1000 | Loss: 0.00001701
Iteration 189/1000 | Loss: 0.00001701
Iteration 190/1000 | Loss: 0.00001701
Iteration 191/1000 | Loss: 0.00001700
Iteration 192/1000 | Loss: 0.00001700
Iteration 193/1000 | Loss: 0.00001700
Iteration 194/1000 | Loss: 0.00001700
Iteration 195/1000 | Loss: 0.00001699
Iteration 196/1000 | Loss: 0.00001699
Iteration 197/1000 | Loss: 0.00001699
Iteration 198/1000 | Loss: 0.00001699
Iteration 199/1000 | Loss: 0.00001698
Iteration 200/1000 | Loss: 0.00001698
Iteration 201/1000 | Loss: 0.00001698
Iteration 202/1000 | Loss: 0.00001698
Iteration 203/1000 | Loss: 0.00001698
Iteration 204/1000 | Loss: 0.00001698
Iteration 205/1000 | Loss: 0.00001698
Iteration 206/1000 | Loss: 0.00001698
Iteration 207/1000 | Loss: 0.00001697
Iteration 208/1000 | Loss: 0.00001697
Iteration 209/1000 | Loss: 0.00001697
Iteration 210/1000 | Loss: 0.00001697
Iteration 211/1000 | Loss: 0.00001697
Iteration 212/1000 | Loss: 0.00001697
Iteration 213/1000 | Loss: 0.00001697
Iteration 214/1000 | Loss: 0.00001697
Iteration 215/1000 | Loss: 0.00001697
Iteration 216/1000 | Loss: 0.00001697
Iteration 217/1000 | Loss: 0.00001697
Iteration 218/1000 | Loss: 0.00001697
Iteration 219/1000 | Loss: 0.00001697
Iteration 220/1000 | Loss: 0.00001697
Iteration 221/1000 | Loss: 0.00001696
Iteration 222/1000 | Loss: 0.00001696
Iteration 223/1000 | Loss: 0.00001696
Iteration 224/1000 | Loss: 0.00001696
Iteration 225/1000 | Loss: 0.00001696
Iteration 226/1000 | Loss: 0.00001696
Iteration 227/1000 | Loss: 0.00001696
Iteration 228/1000 | Loss: 0.00001696
Iteration 229/1000 | Loss: 0.00001696
Iteration 230/1000 | Loss: 0.00001696
Iteration 231/1000 | Loss: 0.00001696
Iteration 232/1000 | Loss: 0.00001696
Iteration 233/1000 | Loss: 0.00001696
Iteration 234/1000 | Loss: 0.00001696
Iteration 235/1000 | Loss: 0.00001695
Iteration 236/1000 | Loss: 0.00001695
Iteration 237/1000 | Loss: 0.00001695
Iteration 238/1000 | Loss: 0.00001695
Iteration 239/1000 | Loss: 0.00001695
Iteration 240/1000 | Loss: 0.00001695
Iteration 241/1000 | Loss: 0.00001695
Iteration 242/1000 | Loss: 0.00001694
Iteration 243/1000 | Loss: 0.00001694
Iteration 244/1000 | Loss: 0.00001694
Iteration 245/1000 | Loss: 0.00001694
Iteration 246/1000 | Loss: 0.00001694
Iteration 247/1000 | Loss: 0.00001694
Iteration 248/1000 | Loss: 0.00001694
Iteration 249/1000 | Loss: 0.00001694
Iteration 250/1000 | Loss: 0.00001694
Iteration 251/1000 | Loss: 0.00001694
Iteration 252/1000 | Loss: 0.00001694
Iteration 253/1000 | Loss: 0.00001694
Iteration 254/1000 | Loss: 0.00001694
Iteration 255/1000 | Loss: 0.00001694
Iteration 256/1000 | Loss: 0.00001694
Iteration 257/1000 | Loss: 0.00001694
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 257. Stopping optimization.
Last 5 losses: [1.694437742116861e-05, 1.694437742116861e-05, 1.694437742116861e-05, 1.694437742116861e-05, 1.694437742116861e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.694437742116861e-05

Optimization complete. Final v2v error: 3.4080841541290283 mm

Highest mean error: 4.528214454650879 mm for frame 107

Lowest mean error: 2.601591110229492 mm for frame 148

Saving results

Total time: 81.84027028083801
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_022/1062/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_022/1062.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_022/1062
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01015502
Iteration 2/25 | Loss: 0.00145479
Iteration 3/25 | Loss: 0.00122644
Iteration 4/25 | Loss: 0.00120328
Iteration 5/25 | Loss: 0.00114633
Iteration 6/25 | Loss: 0.00112061
Iteration 7/25 | Loss: 0.00111346
Iteration 8/25 | Loss: 0.00110957
Iteration 9/25 | Loss: 0.00110983
Iteration 10/25 | Loss: 0.00110792
Iteration 11/25 | Loss: 0.00110686
Iteration 12/25 | Loss: 0.00110886
Iteration 13/25 | Loss: 0.00110775
Iteration 14/25 | Loss: 0.00111133
Iteration 15/25 | Loss: 0.00110890
Iteration 16/25 | Loss: 0.00110601
Iteration 17/25 | Loss: 0.00110664
Iteration 18/25 | Loss: 0.00110560
Iteration 19/25 | Loss: 0.00110412
Iteration 20/25 | Loss: 0.00110383
Iteration 21/25 | Loss: 0.00110356
Iteration 22/25 | Loss: 0.00110333
Iteration 23/25 | Loss: 0.00110295
Iteration 24/25 | Loss: 0.00110361
Iteration 25/25 | Loss: 0.00110232

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.64684963
Iteration 2/25 | Loss: 0.00084485
Iteration 3/25 | Loss: 0.00084485
Iteration 4/25 | Loss: 0.00084485
Iteration 5/25 | Loss: 0.00084485
Iteration 6/25 | Loss: 0.00084485
Iteration 7/25 | Loss: 0.00084485
Iteration 8/25 | Loss: 0.00084485
Iteration 9/25 | Loss: 0.00084485
Iteration 10/25 | Loss: 0.00084485
Iteration 11/25 | Loss: 0.00084485
Iteration 12/25 | Loss: 0.00084485
Iteration 13/25 | Loss: 0.00084485
Iteration 14/25 | Loss: 0.00084485
Iteration 15/25 | Loss: 0.00084485
Iteration 16/25 | Loss: 0.00084485
Iteration 17/25 | Loss: 0.00084485
Iteration 18/25 | Loss: 0.00084485
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0008448488661088049, 0.0008448488661088049, 0.0008448488661088049, 0.0008448488661088049, 0.0008448488661088049]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008448488661088049

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00084485
Iteration 2/1000 | Loss: 0.00002254
Iteration 3/1000 | Loss: 0.00001627
Iteration 4/1000 | Loss: 0.00001480
Iteration 5/1000 | Loss: 0.00001649
Iteration 6/1000 | Loss: 0.00001351
Iteration 7/1000 | Loss: 0.00001285
Iteration 8/1000 | Loss: 0.00001247
Iteration 9/1000 | Loss: 0.00001215
Iteration 10/1000 | Loss: 0.00001187
Iteration 11/1000 | Loss: 0.00001163
Iteration 12/1000 | Loss: 0.00001154
Iteration 13/1000 | Loss: 0.00001148
Iteration 14/1000 | Loss: 0.00001147
Iteration 15/1000 | Loss: 0.00001142
Iteration 16/1000 | Loss: 0.00001141
Iteration 17/1000 | Loss: 0.00001139
Iteration 18/1000 | Loss: 0.00001138
Iteration 19/1000 | Loss: 0.00001138
Iteration 20/1000 | Loss: 0.00001135
Iteration 21/1000 | Loss: 0.00001134
Iteration 22/1000 | Loss: 0.00001134
Iteration 23/1000 | Loss: 0.00001132
Iteration 24/1000 | Loss: 0.00001132
Iteration 25/1000 | Loss: 0.00001131
Iteration 26/1000 | Loss: 0.00001130
Iteration 27/1000 | Loss: 0.00001129
Iteration 28/1000 | Loss: 0.00001125
Iteration 29/1000 | Loss: 0.00001125
Iteration 30/1000 | Loss: 0.00001123
Iteration 31/1000 | Loss: 0.00001123
Iteration 32/1000 | Loss: 0.00001123
Iteration 33/1000 | Loss: 0.00001122
Iteration 34/1000 | Loss: 0.00001122
Iteration 35/1000 | Loss: 0.00001122
Iteration 36/1000 | Loss: 0.00001122
Iteration 37/1000 | Loss: 0.00001121
Iteration 38/1000 | Loss: 0.00001121
Iteration 39/1000 | Loss: 0.00001121
Iteration 40/1000 | Loss: 0.00001121
Iteration 41/1000 | Loss: 0.00001120
Iteration 42/1000 | Loss: 0.00001120
Iteration 43/1000 | Loss: 0.00001120
Iteration 44/1000 | Loss: 0.00001119
Iteration 45/1000 | Loss: 0.00001119
Iteration 46/1000 | Loss: 0.00001119
Iteration 47/1000 | Loss: 0.00001119
Iteration 48/1000 | Loss: 0.00001119
Iteration 49/1000 | Loss: 0.00001118
Iteration 50/1000 | Loss: 0.00001118
Iteration 51/1000 | Loss: 0.00001118
Iteration 52/1000 | Loss: 0.00001115
Iteration 53/1000 | Loss: 0.00001115
Iteration 54/1000 | Loss: 0.00001115
Iteration 55/1000 | Loss: 0.00001115
Iteration 56/1000 | Loss: 0.00001115
Iteration 57/1000 | Loss: 0.00001115
Iteration 58/1000 | Loss: 0.00001115
Iteration 59/1000 | Loss: 0.00001114
Iteration 60/1000 | Loss: 0.00001114
Iteration 61/1000 | Loss: 0.00001114
Iteration 62/1000 | Loss: 0.00001114
Iteration 63/1000 | Loss: 0.00001113
Iteration 64/1000 | Loss: 0.00001113
Iteration 65/1000 | Loss: 0.00001113
Iteration 66/1000 | Loss: 0.00001113
Iteration 67/1000 | Loss: 0.00001112
Iteration 68/1000 | Loss: 0.00001112
Iteration 69/1000 | Loss: 0.00001112
Iteration 70/1000 | Loss: 0.00001112
Iteration 71/1000 | Loss: 0.00001112
Iteration 72/1000 | Loss: 0.00001112
Iteration 73/1000 | Loss: 0.00001112
Iteration 74/1000 | Loss: 0.00001111
Iteration 75/1000 | Loss: 0.00001111
Iteration 76/1000 | Loss: 0.00001111
Iteration 77/1000 | Loss: 0.00001111
Iteration 78/1000 | Loss: 0.00001110
Iteration 79/1000 | Loss: 0.00001110
Iteration 80/1000 | Loss: 0.00001110
Iteration 81/1000 | Loss: 0.00001110
Iteration 82/1000 | Loss: 0.00001110
Iteration 83/1000 | Loss: 0.00001110
Iteration 84/1000 | Loss: 0.00001110
Iteration 85/1000 | Loss: 0.00001110
Iteration 86/1000 | Loss: 0.00001110
Iteration 87/1000 | Loss: 0.00001109
Iteration 88/1000 | Loss: 0.00001109
Iteration 89/1000 | Loss: 0.00001109
Iteration 90/1000 | Loss: 0.00001109
Iteration 91/1000 | Loss: 0.00001109
Iteration 92/1000 | Loss: 0.00001108
Iteration 93/1000 | Loss: 0.00001108
Iteration 94/1000 | Loss: 0.00001108
Iteration 95/1000 | Loss: 0.00001108
Iteration 96/1000 | Loss: 0.00001108
Iteration 97/1000 | Loss: 0.00001108
Iteration 98/1000 | Loss: 0.00001108
Iteration 99/1000 | Loss: 0.00001108
Iteration 100/1000 | Loss: 0.00001107
Iteration 101/1000 | Loss: 0.00001107
Iteration 102/1000 | Loss: 0.00001107
Iteration 103/1000 | Loss: 0.00001107
Iteration 104/1000 | Loss: 0.00001107
Iteration 105/1000 | Loss: 0.00001107
Iteration 106/1000 | Loss: 0.00001107
Iteration 107/1000 | Loss: 0.00001107
Iteration 108/1000 | Loss: 0.00001107
Iteration 109/1000 | Loss: 0.00001107
Iteration 110/1000 | Loss: 0.00001107
Iteration 111/1000 | Loss: 0.00001107
Iteration 112/1000 | Loss: 0.00001107
Iteration 113/1000 | Loss: 0.00001107
Iteration 114/1000 | Loss: 0.00001107
Iteration 115/1000 | Loss: 0.00001107
Iteration 116/1000 | Loss: 0.00001107
Iteration 117/1000 | Loss: 0.00001107
Iteration 118/1000 | Loss: 0.00001107
Iteration 119/1000 | Loss: 0.00001107
Iteration 120/1000 | Loss: 0.00001107
Iteration 121/1000 | Loss: 0.00001107
Iteration 122/1000 | Loss: 0.00001107
Iteration 123/1000 | Loss: 0.00001107
Iteration 124/1000 | Loss: 0.00001107
Iteration 125/1000 | Loss: 0.00001107
Iteration 126/1000 | Loss: 0.00001107
Iteration 127/1000 | Loss: 0.00001107
Iteration 128/1000 | Loss: 0.00001107
Iteration 129/1000 | Loss: 0.00001107
Iteration 130/1000 | Loss: 0.00001107
Iteration 131/1000 | Loss: 0.00001107
Iteration 132/1000 | Loss: 0.00001107
Iteration 133/1000 | Loss: 0.00001107
Iteration 134/1000 | Loss: 0.00001107
Iteration 135/1000 | Loss: 0.00001107
Iteration 136/1000 | Loss: 0.00001107
Iteration 137/1000 | Loss: 0.00001107
Iteration 138/1000 | Loss: 0.00001107
Iteration 139/1000 | Loss: 0.00001107
Iteration 140/1000 | Loss: 0.00001107
Iteration 141/1000 | Loss: 0.00001107
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 141. Stopping optimization.
Last 5 losses: [1.1066124898206908e-05, 1.1066124898206908e-05, 1.1066124898206908e-05, 1.1066124898206908e-05, 1.1066124898206908e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1066124898206908e-05

Optimization complete. Final v2v error: 2.805326461791992 mm

Highest mean error: 4.887556076049805 mm for frame 11

Lowest mean error: 2.58770489692688 mm for frame 249

Saving results

Total time: 85.47329425811768
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_022/1050/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_022/1050.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_022/1050
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00794234
Iteration 2/25 | Loss: 0.00116244
Iteration 3/25 | Loss: 0.00109060
Iteration 4/25 | Loss: 0.00107427
Iteration 5/25 | Loss: 0.00106922
Iteration 6/25 | Loss: 0.00106912
Iteration 7/25 | Loss: 0.00106912
Iteration 8/25 | Loss: 0.00106912
Iteration 9/25 | Loss: 0.00106912
Iteration 10/25 | Loss: 0.00106912
Iteration 11/25 | Loss: 0.00106912
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010691197821870446, 0.0010691197821870446, 0.0010691197821870446, 0.0010691197821870446, 0.0010691197821870446]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010691197821870446

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38978934
Iteration 2/25 | Loss: 0.00077263
Iteration 3/25 | Loss: 0.00077263
Iteration 4/25 | Loss: 0.00077263
Iteration 5/25 | Loss: 0.00077263
Iteration 6/25 | Loss: 0.00077263
Iteration 7/25 | Loss: 0.00077263
Iteration 8/25 | Loss: 0.00077263
Iteration 9/25 | Loss: 0.00077263
Iteration 10/25 | Loss: 0.00077263
Iteration 11/25 | Loss: 0.00077263
Iteration 12/25 | Loss: 0.00077263
Iteration 13/25 | Loss: 0.00077263
Iteration 14/25 | Loss: 0.00077263
Iteration 15/25 | Loss: 0.00077263
Iteration 16/25 | Loss: 0.00077263
Iteration 17/25 | Loss: 0.00077263
Iteration 18/25 | Loss: 0.00077263
Iteration 19/25 | Loss: 0.00077263
Iteration 20/25 | Loss: 0.00077263
Iteration 21/25 | Loss: 0.00077263
Iteration 22/25 | Loss: 0.00077263
Iteration 23/25 | Loss: 0.00077263
Iteration 24/25 | Loss: 0.00077263
Iteration 25/25 | Loss: 0.00077263

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00077263
Iteration 2/1000 | Loss: 0.00002264
Iteration 3/1000 | Loss: 0.00001537
Iteration 4/1000 | Loss: 0.00001388
Iteration 5/1000 | Loss: 0.00001307
Iteration 6/1000 | Loss: 0.00001259
Iteration 7/1000 | Loss: 0.00001214
Iteration 8/1000 | Loss: 0.00001185
Iteration 9/1000 | Loss: 0.00001167
Iteration 10/1000 | Loss: 0.00001140
Iteration 11/1000 | Loss: 0.00001125
Iteration 12/1000 | Loss: 0.00001121
Iteration 13/1000 | Loss: 0.00001120
Iteration 14/1000 | Loss: 0.00001117
Iteration 15/1000 | Loss: 0.00001115
Iteration 16/1000 | Loss: 0.00001103
Iteration 17/1000 | Loss: 0.00001095
Iteration 18/1000 | Loss: 0.00001084
Iteration 19/1000 | Loss: 0.00001084
Iteration 20/1000 | Loss: 0.00001083
Iteration 21/1000 | Loss: 0.00001082
Iteration 22/1000 | Loss: 0.00001081
Iteration 23/1000 | Loss: 0.00001081
Iteration 24/1000 | Loss: 0.00001080
Iteration 25/1000 | Loss: 0.00001079
Iteration 26/1000 | Loss: 0.00001077
Iteration 27/1000 | Loss: 0.00001076
Iteration 28/1000 | Loss: 0.00001076
Iteration 29/1000 | Loss: 0.00001074
Iteration 30/1000 | Loss: 0.00001074
Iteration 31/1000 | Loss: 0.00001073
Iteration 32/1000 | Loss: 0.00001073
Iteration 33/1000 | Loss: 0.00001072
Iteration 34/1000 | Loss: 0.00001072
Iteration 35/1000 | Loss: 0.00001071
Iteration 36/1000 | Loss: 0.00001070
Iteration 37/1000 | Loss: 0.00001070
Iteration 38/1000 | Loss: 0.00001069
Iteration 39/1000 | Loss: 0.00001068
Iteration 40/1000 | Loss: 0.00001068
Iteration 41/1000 | Loss: 0.00001068
Iteration 42/1000 | Loss: 0.00001067
Iteration 43/1000 | Loss: 0.00001067
Iteration 44/1000 | Loss: 0.00001067
Iteration 45/1000 | Loss: 0.00001067
Iteration 46/1000 | Loss: 0.00001066
Iteration 47/1000 | Loss: 0.00001066
Iteration 48/1000 | Loss: 0.00001065
Iteration 49/1000 | Loss: 0.00001065
Iteration 50/1000 | Loss: 0.00001065
Iteration 51/1000 | Loss: 0.00001064
Iteration 52/1000 | Loss: 0.00001063
Iteration 53/1000 | Loss: 0.00001063
Iteration 54/1000 | Loss: 0.00001062
Iteration 55/1000 | Loss: 0.00001061
Iteration 56/1000 | Loss: 0.00001060
Iteration 57/1000 | Loss: 0.00001060
Iteration 58/1000 | Loss: 0.00001060
Iteration 59/1000 | Loss: 0.00001059
Iteration 60/1000 | Loss: 0.00001058
Iteration 61/1000 | Loss: 0.00001058
Iteration 62/1000 | Loss: 0.00001058
Iteration 63/1000 | Loss: 0.00001058
Iteration 64/1000 | Loss: 0.00001058
Iteration 65/1000 | Loss: 0.00001058
Iteration 66/1000 | Loss: 0.00001057
Iteration 67/1000 | Loss: 0.00001055
Iteration 68/1000 | Loss: 0.00001055
Iteration 69/1000 | Loss: 0.00001055
Iteration 70/1000 | Loss: 0.00001054
Iteration 71/1000 | Loss: 0.00001054
Iteration 72/1000 | Loss: 0.00001054
Iteration 73/1000 | Loss: 0.00001054
Iteration 74/1000 | Loss: 0.00001054
Iteration 75/1000 | Loss: 0.00001054
Iteration 76/1000 | Loss: 0.00001054
Iteration 77/1000 | Loss: 0.00001054
Iteration 78/1000 | Loss: 0.00001053
Iteration 79/1000 | Loss: 0.00001053
Iteration 80/1000 | Loss: 0.00001052
Iteration 81/1000 | Loss: 0.00001052
Iteration 82/1000 | Loss: 0.00001052
Iteration 83/1000 | Loss: 0.00001052
Iteration 84/1000 | Loss: 0.00001051
Iteration 85/1000 | Loss: 0.00001051
Iteration 86/1000 | Loss: 0.00001051
Iteration 87/1000 | Loss: 0.00001051
Iteration 88/1000 | Loss: 0.00001051
Iteration 89/1000 | Loss: 0.00001051
Iteration 90/1000 | Loss: 0.00001051
Iteration 91/1000 | Loss: 0.00001050
Iteration 92/1000 | Loss: 0.00001050
Iteration 93/1000 | Loss: 0.00001050
Iteration 94/1000 | Loss: 0.00001049
Iteration 95/1000 | Loss: 0.00001049
Iteration 96/1000 | Loss: 0.00001049
Iteration 97/1000 | Loss: 0.00001049
Iteration 98/1000 | Loss: 0.00001049
Iteration 99/1000 | Loss: 0.00001048
Iteration 100/1000 | Loss: 0.00001048
Iteration 101/1000 | Loss: 0.00001048
Iteration 102/1000 | Loss: 0.00001048
Iteration 103/1000 | Loss: 0.00001048
Iteration 104/1000 | Loss: 0.00001048
Iteration 105/1000 | Loss: 0.00001047
Iteration 106/1000 | Loss: 0.00001047
Iteration 107/1000 | Loss: 0.00001046
Iteration 108/1000 | Loss: 0.00001046
Iteration 109/1000 | Loss: 0.00001046
Iteration 110/1000 | Loss: 0.00001046
Iteration 111/1000 | Loss: 0.00001046
Iteration 112/1000 | Loss: 0.00001046
Iteration 113/1000 | Loss: 0.00001046
Iteration 114/1000 | Loss: 0.00001045
Iteration 115/1000 | Loss: 0.00001045
Iteration 116/1000 | Loss: 0.00001045
Iteration 117/1000 | Loss: 0.00001045
Iteration 118/1000 | Loss: 0.00001045
Iteration 119/1000 | Loss: 0.00001045
Iteration 120/1000 | Loss: 0.00001045
Iteration 121/1000 | Loss: 0.00001044
Iteration 122/1000 | Loss: 0.00001044
Iteration 123/1000 | Loss: 0.00001043
Iteration 124/1000 | Loss: 0.00001043
Iteration 125/1000 | Loss: 0.00001043
Iteration 126/1000 | Loss: 0.00001043
Iteration 127/1000 | Loss: 0.00001043
Iteration 128/1000 | Loss: 0.00001043
Iteration 129/1000 | Loss: 0.00001043
Iteration 130/1000 | Loss: 0.00001043
Iteration 131/1000 | Loss: 0.00001042
Iteration 132/1000 | Loss: 0.00001042
Iteration 133/1000 | Loss: 0.00001042
Iteration 134/1000 | Loss: 0.00001042
Iteration 135/1000 | Loss: 0.00001042
Iteration 136/1000 | Loss: 0.00001042
Iteration 137/1000 | Loss: 0.00001042
Iteration 138/1000 | Loss: 0.00001042
Iteration 139/1000 | Loss: 0.00001042
Iteration 140/1000 | Loss: 0.00001042
Iteration 141/1000 | Loss: 0.00001042
Iteration 142/1000 | Loss: 0.00001042
Iteration 143/1000 | Loss: 0.00001042
Iteration 144/1000 | Loss: 0.00001042
Iteration 145/1000 | Loss: 0.00001042
Iteration 146/1000 | Loss: 0.00001041
Iteration 147/1000 | Loss: 0.00001041
Iteration 148/1000 | Loss: 0.00001041
Iteration 149/1000 | Loss: 0.00001041
Iteration 150/1000 | Loss: 0.00001041
Iteration 151/1000 | Loss: 0.00001041
Iteration 152/1000 | Loss: 0.00001041
Iteration 153/1000 | Loss: 0.00001041
Iteration 154/1000 | Loss: 0.00001041
Iteration 155/1000 | Loss: 0.00001041
Iteration 156/1000 | Loss: 0.00001041
Iteration 157/1000 | Loss: 0.00001041
Iteration 158/1000 | Loss: 0.00001041
Iteration 159/1000 | Loss: 0.00001041
Iteration 160/1000 | Loss: 0.00001041
Iteration 161/1000 | Loss: 0.00001041
Iteration 162/1000 | Loss: 0.00001041
Iteration 163/1000 | Loss: 0.00001041
Iteration 164/1000 | Loss: 0.00001041
Iteration 165/1000 | Loss: 0.00001041
Iteration 166/1000 | Loss: 0.00001041
Iteration 167/1000 | Loss: 0.00001041
Iteration 168/1000 | Loss: 0.00001041
Iteration 169/1000 | Loss: 0.00001041
Iteration 170/1000 | Loss: 0.00001041
Iteration 171/1000 | Loss: 0.00001041
Iteration 172/1000 | Loss: 0.00001041
Iteration 173/1000 | Loss: 0.00001041
Iteration 174/1000 | Loss: 0.00001041
Iteration 175/1000 | Loss: 0.00001041
Iteration 176/1000 | Loss: 0.00001041
Iteration 177/1000 | Loss: 0.00001041
Iteration 178/1000 | Loss: 0.00001041
Iteration 179/1000 | Loss: 0.00001041
Iteration 180/1000 | Loss: 0.00001041
Iteration 181/1000 | Loss: 0.00001041
Iteration 182/1000 | Loss: 0.00001041
Iteration 183/1000 | Loss: 0.00001041
Iteration 184/1000 | Loss: 0.00001041
Iteration 185/1000 | Loss: 0.00001041
Iteration 186/1000 | Loss: 0.00001041
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 186. Stopping optimization.
Last 5 losses: [1.0407771696918644e-05, 1.0407771696918644e-05, 1.0407771696918644e-05, 1.0407771696918644e-05, 1.0407771696918644e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0407771696918644e-05

Optimization complete. Final v2v error: 2.7707607746124268 mm

Highest mean error: 3.014037847518921 mm for frame 64

Lowest mean error: 2.652489423751831 mm for frame 145

Saving results

Total time: 39.30713963508606
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_022/1080/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_022/1080.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_022/1080
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00427336
Iteration 2/25 | Loss: 0.00124317
Iteration 3/25 | Loss: 0.00112032
Iteration 4/25 | Loss: 0.00110347
Iteration 5/25 | Loss: 0.00109931
Iteration 6/25 | Loss: 0.00109811
Iteration 7/25 | Loss: 0.00109773
Iteration 8/25 | Loss: 0.00109767
Iteration 9/25 | Loss: 0.00109767
Iteration 10/25 | Loss: 0.00109767
Iteration 11/25 | Loss: 0.00109767
Iteration 12/25 | Loss: 0.00109767
Iteration 13/25 | Loss: 0.00109767
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0010976716876029968, 0.0010976716876029968, 0.0010976716876029968, 0.0010976716876029968, 0.0010976716876029968]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010976716876029968

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46040225
Iteration 2/25 | Loss: 0.00084798
Iteration 3/25 | Loss: 0.00084798
Iteration 4/25 | Loss: 0.00084798
Iteration 5/25 | Loss: 0.00084798
Iteration 6/25 | Loss: 0.00084798
Iteration 7/25 | Loss: 0.00084798
Iteration 8/25 | Loss: 0.00084798
Iteration 9/25 | Loss: 0.00084798
Iteration 10/25 | Loss: 0.00084798
Iteration 11/25 | Loss: 0.00084798
Iteration 12/25 | Loss: 0.00084798
Iteration 13/25 | Loss: 0.00084798
Iteration 14/25 | Loss: 0.00084798
Iteration 15/25 | Loss: 0.00084798
Iteration 16/25 | Loss: 0.00084798
Iteration 17/25 | Loss: 0.00084798
Iteration 18/25 | Loss: 0.00084798
Iteration 19/25 | Loss: 0.00084798
Iteration 20/25 | Loss: 0.00084798
Iteration 21/25 | Loss: 0.00084798
Iteration 22/25 | Loss: 0.00084798
Iteration 23/25 | Loss: 0.00084798
Iteration 24/25 | Loss: 0.00084798
Iteration 25/25 | Loss: 0.00084798

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00084798
Iteration 2/1000 | Loss: 0.00003431
Iteration 3/1000 | Loss: 0.00002212
Iteration 4/1000 | Loss: 0.00001769
Iteration 5/1000 | Loss: 0.00001594
Iteration 6/1000 | Loss: 0.00001536
Iteration 7/1000 | Loss: 0.00001484
Iteration 8/1000 | Loss: 0.00001445
Iteration 9/1000 | Loss: 0.00001405
Iteration 10/1000 | Loss: 0.00001378
Iteration 11/1000 | Loss: 0.00001370
Iteration 12/1000 | Loss: 0.00001354
Iteration 13/1000 | Loss: 0.00001334
Iteration 14/1000 | Loss: 0.00001332
Iteration 15/1000 | Loss: 0.00001326
Iteration 16/1000 | Loss: 0.00001324
Iteration 17/1000 | Loss: 0.00001323
Iteration 18/1000 | Loss: 0.00001321
Iteration 19/1000 | Loss: 0.00001315
Iteration 20/1000 | Loss: 0.00001312
Iteration 21/1000 | Loss: 0.00001308
Iteration 22/1000 | Loss: 0.00001305
Iteration 23/1000 | Loss: 0.00001304
Iteration 24/1000 | Loss: 0.00001298
Iteration 25/1000 | Loss: 0.00001297
Iteration 26/1000 | Loss: 0.00001297
Iteration 27/1000 | Loss: 0.00001293
Iteration 28/1000 | Loss: 0.00001292
Iteration 29/1000 | Loss: 0.00001291
Iteration 30/1000 | Loss: 0.00001290
Iteration 31/1000 | Loss: 0.00001290
Iteration 32/1000 | Loss: 0.00001289
Iteration 33/1000 | Loss: 0.00001288
Iteration 34/1000 | Loss: 0.00001288
Iteration 35/1000 | Loss: 0.00001287
Iteration 36/1000 | Loss: 0.00001286
Iteration 37/1000 | Loss: 0.00001286
Iteration 38/1000 | Loss: 0.00001282
Iteration 39/1000 | Loss: 0.00001282
Iteration 40/1000 | Loss: 0.00001280
Iteration 41/1000 | Loss: 0.00001280
Iteration 42/1000 | Loss: 0.00001278
Iteration 43/1000 | Loss: 0.00001278
Iteration 44/1000 | Loss: 0.00001278
Iteration 45/1000 | Loss: 0.00001278
Iteration 46/1000 | Loss: 0.00001277
Iteration 47/1000 | Loss: 0.00001277
Iteration 48/1000 | Loss: 0.00001276
Iteration 49/1000 | Loss: 0.00001276
Iteration 50/1000 | Loss: 0.00001276
Iteration 51/1000 | Loss: 0.00001276
Iteration 52/1000 | Loss: 0.00001275
Iteration 53/1000 | Loss: 0.00001275
Iteration 54/1000 | Loss: 0.00001274
Iteration 55/1000 | Loss: 0.00001274
Iteration 56/1000 | Loss: 0.00001274
Iteration 57/1000 | Loss: 0.00001273
Iteration 58/1000 | Loss: 0.00001273
Iteration 59/1000 | Loss: 0.00001273
Iteration 60/1000 | Loss: 0.00001272
Iteration 61/1000 | Loss: 0.00001272
Iteration 62/1000 | Loss: 0.00001272
Iteration 63/1000 | Loss: 0.00001272
Iteration 64/1000 | Loss: 0.00001271
Iteration 65/1000 | Loss: 0.00001271
Iteration 66/1000 | Loss: 0.00001270
Iteration 67/1000 | Loss: 0.00001270
Iteration 68/1000 | Loss: 0.00001270
Iteration 69/1000 | Loss: 0.00001270
Iteration 70/1000 | Loss: 0.00001270
Iteration 71/1000 | Loss: 0.00001269
Iteration 72/1000 | Loss: 0.00001269
Iteration 73/1000 | Loss: 0.00001269
Iteration 74/1000 | Loss: 0.00001269
Iteration 75/1000 | Loss: 0.00001269
Iteration 76/1000 | Loss: 0.00001269
Iteration 77/1000 | Loss: 0.00001269
Iteration 78/1000 | Loss: 0.00001269
Iteration 79/1000 | Loss: 0.00001268
Iteration 80/1000 | Loss: 0.00001268
Iteration 81/1000 | Loss: 0.00001267
Iteration 82/1000 | Loss: 0.00001267
Iteration 83/1000 | Loss: 0.00001267
Iteration 84/1000 | Loss: 0.00001266
Iteration 85/1000 | Loss: 0.00001266
Iteration 86/1000 | Loss: 0.00001265
Iteration 87/1000 | Loss: 0.00001265
Iteration 88/1000 | Loss: 0.00001265
Iteration 89/1000 | Loss: 0.00001264
Iteration 90/1000 | Loss: 0.00001264
Iteration 91/1000 | Loss: 0.00001264
Iteration 92/1000 | Loss: 0.00001263
Iteration 93/1000 | Loss: 0.00001263
Iteration 94/1000 | Loss: 0.00001262
Iteration 95/1000 | Loss: 0.00001262
Iteration 96/1000 | Loss: 0.00001262
Iteration 97/1000 | Loss: 0.00001262
Iteration 98/1000 | Loss: 0.00001262
Iteration 99/1000 | Loss: 0.00001262
Iteration 100/1000 | Loss: 0.00001262
Iteration 101/1000 | Loss: 0.00001262
Iteration 102/1000 | Loss: 0.00001262
Iteration 103/1000 | Loss: 0.00001262
Iteration 104/1000 | Loss: 0.00001261
Iteration 105/1000 | Loss: 0.00001261
Iteration 106/1000 | Loss: 0.00001261
Iteration 107/1000 | Loss: 0.00001261
Iteration 108/1000 | Loss: 0.00001261
Iteration 109/1000 | Loss: 0.00001261
Iteration 110/1000 | Loss: 0.00001261
Iteration 111/1000 | Loss: 0.00001261
Iteration 112/1000 | Loss: 0.00001261
Iteration 113/1000 | Loss: 0.00001260
Iteration 114/1000 | Loss: 0.00001260
Iteration 115/1000 | Loss: 0.00001260
Iteration 116/1000 | Loss: 0.00001260
Iteration 117/1000 | Loss: 0.00001260
Iteration 118/1000 | Loss: 0.00001260
Iteration 119/1000 | Loss: 0.00001260
Iteration 120/1000 | Loss: 0.00001259
Iteration 121/1000 | Loss: 0.00001259
Iteration 122/1000 | Loss: 0.00001259
Iteration 123/1000 | Loss: 0.00001259
Iteration 124/1000 | Loss: 0.00001259
Iteration 125/1000 | Loss: 0.00001259
Iteration 126/1000 | Loss: 0.00001259
Iteration 127/1000 | Loss: 0.00001259
Iteration 128/1000 | Loss: 0.00001259
Iteration 129/1000 | Loss: 0.00001259
Iteration 130/1000 | Loss: 0.00001258
Iteration 131/1000 | Loss: 0.00001258
Iteration 132/1000 | Loss: 0.00001258
Iteration 133/1000 | Loss: 0.00001258
Iteration 134/1000 | Loss: 0.00001258
Iteration 135/1000 | Loss: 0.00001258
Iteration 136/1000 | Loss: 0.00001258
Iteration 137/1000 | Loss: 0.00001257
Iteration 138/1000 | Loss: 0.00001257
Iteration 139/1000 | Loss: 0.00001257
Iteration 140/1000 | Loss: 0.00001257
Iteration 141/1000 | Loss: 0.00001256
Iteration 142/1000 | Loss: 0.00001256
Iteration 143/1000 | Loss: 0.00001256
Iteration 144/1000 | Loss: 0.00001255
Iteration 145/1000 | Loss: 0.00001255
Iteration 146/1000 | Loss: 0.00001255
Iteration 147/1000 | Loss: 0.00001255
Iteration 148/1000 | Loss: 0.00001254
Iteration 149/1000 | Loss: 0.00001254
Iteration 150/1000 | Loss: 0.00001254
Iteration 151/1000 | Loss: 0.00001254
Iteration 152/1000 | Loss: 0.00001254
Iteration 153/1000 | Loss: 0.00001254
Iteration 154/1000 | Loss: 0.00001254
Iteration 155/1000 | Loss: 0.00001253
Iteration 156/1000 | Loss: 0.00001253
Iteration 157/1000 | Loss: 0.00001253
Iteration 158/1000 | Loss: 0.00001253
Iteration 159/1000 | Loss: 0.00001253
Iteration 160/1000 | Loss: 0.00001253
Iteration 161/1000 | Loss: 0.00001253
Iteration 162/1000 | Loss: 0.00001253
Iteration 163/1000 | Loss: 0.00001253
Iteration 164/1000 | Loss: 0.00001253
Iteration 165/1000 | Loss: 0.00001253
Iteration 166/1000 | Loss: 0.00001253
Iteration 167/1000 | Loss: 0.00001253
Iteration 168/1000 | Loss: 0.00001253
Iteration 169/1000 | Loss: 0.00001253
Iteration 170/1000 | Loss: 0.00001252
Iteration 171/1000 | Loss: 0.00001252
Iteration 172/1000 | Loss: 0.00001252
Iteration 173/1000 | Loss: 0.00001252
Iteration 174/1000 | Loss: 0.00001252
Iteration 175/1000 | Loss: 0.00001252
Iteration 176/1000 | Loss: 0.00001252
Iteration 177/1000 | Loss: 0.00001252
Iteration 178/1000 | Loss: 0.00001252
Iteration 179/1000 | Loss: 0.00001252
Iteration 180/1000 | Loss: 0.00001252
Iteration 181/1000 | Loss: 0.00001252
Iteration 182/1000 | Loss: 0.00001252
Iteration 183/1000 | Loss: 0.00001252
Iteration 184/1000 | Loss: 0.00001252
Iteration 185/1000 | Loss: 0.00001252
Iteration 186/1000 | Loss: 0.00001252
Iteration 187/1000 | Loss: 0.00001252
Iteration 188/1000 | Loss: 0.00001252
Iteration 189/1000 | Loss: 0.00001252
Iteration 190/1000 | Loss: 0.00001252
Iteration 191/1000 | Loss: 0.00001251
Iteration 192/1000 | Loss: 0.00001251
Iteration 193/1000 | Loss: 0.00001251
Iteration 194/1000 | Loss: 0.00001251
Iteration 195/1000 | Loss: 0.00001251
Iteration 196/1000 | Loss: 0.00001251
Iteration 197/1000 | Loss: 0.00001251
Iteration 198/1000 | Loss: 0.00001251
Iteration 199/1000 | Loss: 0.00001251
Iteration 200/1000 | Loss: 0.00001251
Iteration 201/1000 | Loss: 0.00001250
Iteration 202/1000 | Loss: 0.00001250
Iteration 203/1000 | Loss: 0.00001250
Iteration 204/1000 | Loss: 0.00001250
Iteration 205/1000 | Loss: 0.00001250
Iteration 206/1000 | Loss: 0.00001250
Iteration 207/1000 | Loss: 0.00001250
Iteration 208/1000 | Loss: 0.00001250
Iteration 209/1000 | Loss: 0.00001250
Iteration 210/1000 | Loss: 0.00001250
Iteration 211/1000 | Loss: 0.00001250
Iteration 212/1000 | Loss: 0.00001250
Iteration 213/1000 | Loss: 0.00001250
Iteration 214/1000 | Loss: 0.00001250
Iteration 215/1000 | Loss: 0.00001250
Iteration 216/1000 | Loss: 0.00001250
Iteration 217/1000 | Loss: 0.00001250
Iteration 218/1000 | Loss: 0.00001250
Iteration 219/1000 | Loss: 0.00001250
Iteration 220/1000 | Loss: 0.00001250
Iteration 221/1000 | Loss: 0.00001250
Iteration 222/1000 | Loss: 0.00001250
Iteration 223/1000 | Loss: 0.00001250
Iteration 224/1000 | Loss: 0.00001250
Iteration 225/1000 | Loss: 0.00001250
Iteration 226/1000 | Loss: 0.00001250
Iteration 227/1000 | Loss: 0.00001250
Iteration 228/1000 | Loss: 0.00001250
Iteration 229/1000 | Loss: 0.00001250
Iteration 230/1000 | Loss: 0.00001250
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 230. Stopping optimization.
Last 5 losses: [1.2495544979174156e-05, 1.2495544979174156e-05, 1.2495544979174156e-05, 1.2495544979174156e-05, 1.2495544979174156e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2495544979174156e-05

Optimization complete. Final v2v error: 2.9796061515808105 mm

Highest mean error: 4.367217063903809 mm for frame 46

Lowest mean error: 2.617861270904541 mm for frame 86

Saving results

Total time: 47.989747047424316
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_022/1019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_022/1019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_022/1019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00801999
Iteration 2/25 | Loss: 0.00150697
Iteration 3/25 | Loss: 0.00127308
Iteration 4/25 | Loss: 0.00125912
Iteration 5/25 | Loss: 0.00125664
Iteration 6/25 | Loss: 0.00125664
Iteration 7/25 | Loss: 0.00125664
Iteration 8/25 | Loss: 0.00125664
Iteration 9/25 | Loss: 0.00125664
Iteration 10/25 | Loss: 0.00125664
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.001256642397493124, 0.001256642397493124, 0.001256642397493124, 0.001256642397493124, 0.001256642397493124]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001256642397493124

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.22966921
Iteration 2/25 | Loss: 0.00063608
Iteration 3/25 | Loss: 0.00063608
Iteration 4/25 | Loss: 0.00063608
Iteration 5/25 | Loss: 0.00063608
Iteration 6/25 | Loss: 0.00063608
Iteration 7/25 | Loss: 0.00063608
Iteration 8/25 | Loss: 0.00063608
Iteration 9/25 | Loss: 0.00063608
Iteration 10/25 | Loss: 0.00063607
Iteration 11/25 | Loss: 0.00063607
Iteration 12/25 | Loss: 0.00063607
Iteration 13/25 | Loss: 0.00063607
Iteration 14/25 | Loss: 0.00063607
Iteration 15/25 | Loss: 0.00063607
Iteration 16/25 | Loss: 0.00063607
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0006360748084262013, 0.0006360748084262013, 0.0006360748084262013, 0.0006360748084262013, 0.0006360748084262013]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006360748084262013

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00063607
Iteration 2/1000 | Loss: 0.00003093
Iteration 3/1000 | Loss: 0.00002401
Iteration 4/1000 | Loss: 0.00002169
Iteration 5/1000 | Loss: 0.00002081
Iteration 6/1000 | Loss: 0.00002026
Iteration 7/1000 | Loss: 0.00001964
Iteration 8/1000 | Loss: 0.00001932
Iteration 9/1000 | Loss: 0.00001918
Iteration 10/1000 | Loss: 0.00001902
Iteration 11/1000 | Loss: 0.00001900
Iteration 12/1000 | Loss: 0.00001900
Iteration 13/1000 | Loss: 0.00001892
Iteration 14/1000 | Loss: 0.00001887
Iteration 15/1000 | Loss: 0.00001887
Iteration 16/1000 | Loss: 0.00001886
Iteration 17/1000 | Loss: 0.00001886
Iteration 18/1000 | Loss: 0.00001884
Iteration 19/1000 | Loss: 0.00001883
Iteration 20/1000 | Loss: 0.00001883
Iteration 21/1000 | Loss: 0.00001883
Iteration 22/1000 | Loss: 0.00001883
Iteration 23/1000 | Loss: 0.00001882
Iteration 24/1000 | Loss: 0.00001881
Iteration 25/1000 | Loss: 0.00001880
Iteration 26/1000 | Loss: 0.00001880
Iteration 27/1000 | Loss: 0.00001878
Iteration 28/1000 | Loss: 0.00001878
Iteration 29/1000 | Loss: 0.00001878
Iteration 30/1000 | Loss: 0.00001878
Iteration 31/1000 | Loss: 0.00001878
Iteration 32/1000 | Loss: 0.00001878
Iteration 33/1000 | Loss: 0.00001878
Iteration 34/1000 | Loss: 0.00001878
Iteration 35/1000 | Loss: 0.00001877
Iteration 36/1000 | Loss: 0.00001877
Iteration 37/1000 | Loss: 0.00001876
Iteration 38/1000 | Loss: 0.00001876
Iteration 39/1000 | Loss: 0.00001876
Iteration 40/1000 | Loss: 0.00001876
Iteration 41/1000 | Loss: 0.00001876
Iteration 42/1000 | Loss: 0.00001876
Iteration 43/1000 | Loss: 0.00001876
Iteration 44/1000 | Loss: 0.00001876
Iteration 45/1000 | Loss: 0.00001876
Iteration 46/1000 | Loss: 0.00001876
Iteration 47/1000 | Loss: 0.00001875
Iteration 48/1000 | Loss: 0.00001875
Iteration 49/1000 | Loss: 0.00001875
Iteration 50/1000 | Loss: 0.00001875
Iteration 51/1000 | Loss: 0.00001875
Iteration 52/1000 | Loss: 0.00001875
Iteration 53/1000 | Loss: 0.00001875
Iteration 54/1000 | Loss: 0.00001875
Iteration 55/1000 | Loss: 0.00001874
Iteration 56/1000 | Loss: 0.00001874
Iteration 57/1000 | Loss: 0.00001874
Iteration 58/1000 | Loss: 0.00001873
Iteration 59/1000 | Loss: 0.00001873
Iteration 60/1000 | Loss: 0.00001873
Iteration 61/1000 | Loss: 0.00001873
Iteration 62/1000 | Loss: 0.00001873
Iteration 63/1000 | Loss: 0.00001873
Iteration 64/1000 | Loss: 0.00001873
Iteration 65/1000 | Loss: 0.00001873
Iteration 66/1000 | Loss: 0.00001873
Iteration 67/1000 | Loss: 0.00001873
Iteration 68/1000 | Loss: 0.00001873
Iteration 69/1000 | Loss: 0.00001873
Iteration 70/1000 | Loss: 0.00001873
Iteration 71/1000 | Loss: 0.00001873
Iteration 72/1000 | Loss: 0.00001873
Iteration 73/1000 | Loss: 0.00001873
Iteration 74/1000 | Loss: 0.00001873
Iteration 75/1000 | Loss: 0.00001873
Iteration 76/1000 | Loss: 0.00001873
Iteration 77/1000 | Loss: 0.00001873
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 77. Stopping optimization.
Last 5 losses: [1.872523716883734e-05, 1.872523716883734e-05, 1.872523716883734e-05, 1.872523716883734e-05, 1.872523716883734e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.872523716883734e-05

Optimization complete. Final v2v error: 3.6220760345458984 mm

Highest mean error: 3.9539401531219482 mm for frame 37

Lowest mean error: 3.3764991760253906 mm for frame 214

Saving results

Total time: 29.980947732925415
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_022/1009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_022/1009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_022/1009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00474866
Iteration 2/25 | Loss: 0.00134706
Iteration 3/25 | Loss: 0.00113340
Iteration 4/25 | Loss: 0.00111812
Iteration 5/25 | Loss: 0.00111541
Iteration 6/25 | Loss: 0.00111470
Iteration 7/25 | Loss: 0.00111470
Iteration 8/25 | Loss: 0.00111470
Iteration 9/25 | Loss: 0.00111470
Iteration 10/25 | Loss: 0.00111470
Iteration 11/25 | Loss: 0.00111470
Iteration 12/25 | Loss: 0.00111470
Iteration 13/25 | Loss: 0.00111470
Iteration 14/25 | Loss: 0.00111470
Iteration 15/25 | Loss: 0.00111470
Iteration 16/25 | Loss: 0.00111470
Iteration 17/25 | Loss: 0.00111470
Iteration 18/25 | Loss: 0.00111470
Iteration 19/25 | Loss: 0.00111470
Iteration 20/25 | Loss: 0.00111470
Iteration 21/25 | Loss: 0.00111470
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0011147035984322429, 0.0011147035984322429, 0.0011147035984322429, 0.0011147035984322429, 0.0011147035984322429]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011147035984322429

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37023115
Iteration 2/25 | Loss: 0.00076433
Iteration 3/25 | Loss: 0.00076433
Iteration 4/25 | Loss: 0.00076433
Iteration 5/25 | Loss: 0.00076433
Iteration 6/25 | Loss: 0.00076433
Iteration 7/25 | Loss: 0.00076433
Iteration 8/25 | Loss: 0.00076433
Iteration 9/25 | Loss: 0.00076433
Iteration 10/25 | Loss: 0.00076433
Iteration 11/25 | Loss: 0.00076433
Iteration 12/25 | Loss: 0.00076433
Iteration 13/25 | Loss: 0.00076433
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0007643253193236887, 0.0007643253193236887, 0.0007643253193236887, 0.0007643253193236887, 0.0007643253193236887]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007643253193236887

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00076433
Iteration 2/1000 | Loss: 0.00003734
Iteration 3/1000 | Loss: 0.00002072
Iteration 4/1000 | Loss: 0.00001785
Iteration 5/1000 | Loss: 0.00001658
Iteration 6/1000 | Loss: 0.00001582
Iteration 7/1000 | Loss: 0.00001529
Iteration 8/1000 | Loss: 0.00001484
Iteration 9/1000 | Loss: 0.00001458
Iteration 10/1000 | Loss: 0.00001433
Iteration 11/1000 | Loss: 0.00001419
Iteration 12/1000 | Loss: 0.00001418
Iteration 13/1000 | Loss: 0.00001413
Iteration 14/1000 | Loss: 0.00001413
Iteration 15/1000 | Loss: 0.00001411
Iteration 16/1000 | Loss: 0.00001411
Iteration 17/1000 | Loss: 0.00001410
Iteration 18/1000 | Loss: 0.00001410
Iteration 19/1000 | Loss: 0.00001409
Iteration 20/1000 | Loss: 0.00001409
Iteration 21/1000 | Loss: 0.00001408
Iteration 22/1000 | Loss: 0.00001406
Iteration 23/1000 | Loss: 0.00001405
Iteration 24/1000 | Loss: 0.00001405
Iteration 25/1000 | Loss: 0.00001403
Iteration 26/1000 | Loss: 0.00001402
Iteration 27/1000 | Loss: 0.00001400
Iteration 28/1000 | Loss: 0.00001399
Iteration 29/1000 | Loss: 0.00001397
Iteration 30/1000 | Loss: 0.00001396
Iteration 31/1000 | Loss: 0.00001396
Iteration 32/1000 | Loss: 0.00001395
Iteration 33/1000 | Loss: 0.00001394
Iteration 34/1000 | Loss: 0.00001394
Iteration 35/1000 | Loss: 0.00001393
Iteration 36/1000 | Loss: 0.00001393
Iteration 37/1000 | Loss: 0.00001392
Iteration 38/1000 | Loss: 0.00001392
Iteration 39/1000 | Loss: 0.00001392
Iteration 40/1000 | Loss: 0.00001392
Iteration 41/1000 | Loss: 0.00001391
Iteration 42/1000 | Loss: 0.00001391
Iteration 43/1000 | Loss: 0.00001391
Iteration 44/1000 | Loss: 0.00001391
Iteration 45/1000 | Loss: 0.00001390
Iteration 46/1000 | Loss: 0.00001390
Iteration 47/1000 | Loss: 0.00001390
Iteration 48/1000 | Loss: 0.00001389
Iteration 49/1000 | Loss: 0.00001389
Iteration 50/1000 | Loss: 0.00001389
Iteration 51/1000 | Loss: 0.00001388
Iteration 52/1000 | Loss: 0.00001388
Iteration 53/1000 | Loss: 0.00001387
Iteration 54/1000 | Loss: 0.00001387
Iteration 55/1000 | Loss: 0.00001387
Iteration 56/1000 | Loss: 0.00001386
Iteration 57/1000 | Loss: 0.00001386
Iteration 58/1000 | Loss: 0.00001386
Iteration 59/1000 | Loss: 0.00001385
Iteration 60/1000 | Loss: 0.00001385
Iteration 61/1000 | Loss: 0.00001385
Iteration 62/1000 | Loss: 0.00001385
Iteration 63/1000 | Loss: 0.00001385
Iteration 64/1000 | Loss: 0.00001384
Iteration 65/1000 | Loss: 0.00001384
Iteration 66/1000 | Loss: 0.00001384
Iteration 67/1000 | Loss: 0.00001383
Iteration 68/1000 | Loss: 0.00001383
Iteration 69/1000 | Loss: 0.00001383
Iteration 70/1000 | Loss: 0.00001383
Iteration 71/1000 | Loss: 0.00001382
Iteration 72/1000 | Loss: 0.00001382
Iteration 73/1000 | Loss: 0.00001382
Iteration 74/1000 | Loss: 0.00001382
Iteration 75/1000 | Loss: 0.00001382
Iteration 76/1000 | Loss: 0.00001382
Iteration 77/1000 | Loss: 0.00001382
Iteration 78/1000 | Loss: 0.00001382
Iteration 79/1000 | Loss: 0.00001382
Iteration 80/1000 | Loss: 0.00001382
Iteration 81/1000 | Loss: 0.00001382
Iteration 82/1000 | Loss: 0.00001382
Iteration 83/1000 | Loss: 0.00001381
Iteration 84/1000 | Loss: 0.00001381
Iteration 85/1000 | Loss: 0.00001381
Iteration 86/1000 | Loss: 0.00001381
Iteration 87/1000 | Loss: 0.00001381
Iteration 88/1000 | Loss: 0.00001381
Iteration 89/1000 | Loss: 0.00001380
Iteration 90/1000 | Loss: 0.00001379
Iteration 91/1000 | Loss: 0.00001379
Iteration 92/1000 | Loss: 0.00001379
Iteration 93/1000 | Loss: 0.00001379
Iteration 94/1000 | Loss: 0.00001379
Iteration 95/1000 | Loss: 0.00001379
Iteration 96/1000 | Loss: 0.00001378
Iteration 97/1000 | Loss: 0.00001378
Iteration 98/1000 | Loss: 0.00001378
Iteration 99/1000 | Loss: 0.00001378
Iteration 100/1000 | Loss: 0.00001377
Iteration 101/1000 | Loss: 0.00001377
Iteration 102/1000 | Loss: 0.00001376
Iteration 103/1000 | Loss: 0.00001376
Iteration 104/1000 | Loss: 0.00001376
Iteration 105/1000 | Loss: 0.00001376
Iteration 106/1000 | Loss: 0.00001376
Iteration 107/1000 | Loss: 0.00001375
Iteration 108/1000 | Loss: 0.00001375
Iteration 109/1000 | Loss: 0.00001375
Iteration 110/1000 | Loss: 0.00001374
Iteration 111/1000 | Loss: 0.00001374
Iteration 112/1000 | Loss: 0.00001374
Iteration 113/1000 | Loss: 0.00001374
Iteration 114/1000 | Loss: 0.00001373
Iteration 115/1000 | Loss: 0.00001373
Iteration 116/1000 | Loss: 0.00001373
Iteration 117/1000 | Loss: 0.00001373
Iteration 118/1000 | Loss: 0.00001373
Iteration 119/1000 | Loss: 0.00001373
Iteration 120/1000 | Loss: 0.00001373
Iteration 121/1000 | Loss: 0.00001373
Iteration 122/1000 | Loss: 0.00001372
Iteration 123/1000 | Loss: 0.00001372
Iteration 124/1000 | Loss: 0.00001372
Iteration 125/1000 | Loss: 0.00001372
Iteration 126/1000 | Loss: 0.00001372
Iteration 127/1000 | Loss: 0.00001371
Iteration 128/1000 | Loss: 0.00001371
Iteration 129/1000 | Loss: 0.00001371
Iteration 130/1000 | Loss: 0.00001371
Iteration 131/1000 | Loss: 0.00001371
Iteration 132/1000 | Loss: 0.00001371
Iteration 133/1000 | Loss: 0.00001371
Iteration 134/1000 | Loss: 0.00001371
Iteration 135/1000 | Loss: 0.00001370
Iteration 136/1000 | Loss: 0.00001370
Iteration 137/1000 | Loss: 0.00001370
Iteration 138/1000 | Loss: 0.00001370
Iteration 139/1000 | Loss: 0.00001370
Iteration 140/1000 | Loss: 0.00001369
Iteration 141/1000 | Loss: 0.00001368
Iteration 142/1000 | Loss: 0.00001368
Iteration 143/1000 | Loss: 0.00001368
Iteration 144/1000 | Loss: 0.00001368
Iteration 145/1000 | Loss: 0.00001368
Iteration 146/1000 | Loss: 0.00001368
Iteration 147/1000 | Loss: 0.00001367
Iteration 148/1000 | Loss: 0.00001367
Iteration 149/1000 | Loss: 0.00001367
Iteration 150/1000 | Loss: 0.00001366
Iteration 151/1000 | Loss: 0.00001366
Iteration 152/1000 | Loss: 0.00001366
Iteration 153/1000 | Loss: 0.00001366
Iteration 154/1000 | Loss: 0.00001366
Iteration 155/1000 | Loss: 0.00001366
Iteration 156/1000 | Loss: 0.00001366
Iteration 157/1000 | Loss: 0.00001365
Iteration 158/1000 | Loss: 0.00001365
Iteration 159/1000 | Loss: 0.00001365
Iteration 160/1000 | Loss: 0.00001365
Iteration 161/1000 | Loss: 0.00001365
Iteration 162/1000 | Loss: 0.00001365
Iteration 163/1000 | Loss: 0.00001365
Iteration 164/1000 | Loss: 0.00001365
Iteration 165/1000 | Loss: 0.00001365
Iteration 166/1000 | Loss: 0.00001365
Iteration 167/1000 | Loss: 0.00001365
Iteration 168/1000 | Loss: 0.00001365
Iteration 169/1000 | Loss: 0.00001364
Iteration 170/1000 | Loss: 0.00001364
Iteration 171/1000 | Loss: 0.00001364
Iteration 172/1000 | Loss: 0.00001364
Iteration 173/1000 | Loss: 0.00001364
Iteration 174/1000 | Loss: 0.00001364
Iteration 175/1000 | Loss: 0.00001363
Iteration 176/1000 | Loss: 0.00001363
Iteration 177/1000 | Loss: 0.00001363
Iteration 178/1000 | Loss: 0.00001362
Iteration 179/1000 | Loss: 0.00001362
Iteration 180/1000 | Loss: 0.00001362
Iteration 181/1000 | Loss: 0.00001362
Iteration 182/1000 | Loss: 0.00001362
Iteration 183/1000 | Loss: 0.00001362
Iteration 184/1000 | Loss: 0.00001362
Iteration 185/1000 | Loss: 0.00001362
Iteration 186/1000 | Loss: 0.00001362
Iteration 187/1000 | Loss: 0.00001362
Iteration 188/1000 | Loss: 0.00001362
Iteration 189/1000 | Loss: 0.00001361
Iteration 190/1000 | Loss: 0.00001361
Iteration 191/1000 | Loss: 0.00001361
Iteration 192/1000 | Loss: 0.00001360
Iteration 193/1000 | Loss: 0.00001360
Iteration 194/1000 | Loss: 0.00001360
Iteration 195/1000 | Loss: 0.00001360
Iteration 196/1000 | Loss: 0.00001360
Iteration 197/1000 | Loss: 0.00001360
Iteration 198/1000 | Loss: 0.00001360
Iteration 199/1000 | Loss: 0.00001360
Iteration 200/1000 | Loss: 0.00001360
Iteration 201/1000 | Loss: 0.00001360
Iteration 202/1000 | Loss: 0.00001360
Iteration 203/1000 | Loss: 0.00001360
Iteration 204/1000 | Loss: 0.00001360
Iteration 205/1000 | Loss: 0.00001359
Iteration 206/1000 | Loss: 0.00001359
Iteration 207/1000 | Loss: 0.00001359
Iteration 208/1000 | Loss: 0.00001359
Iteration 209/1000 | Loss: 0.00001359
Iteration 210/1000 | Loss: 0.00001359
Iteration 211/1000 | Loss: 0.00001359
Iteration 212/1000 | Loss: 0.00001359
Iteration 213/1000 | Loss: 0.00001359
Iteration 214/1000 | Loss: 0.00001359
Iteration 215/1000 | Loss: 0.00001359
Iteration 216/1000 | Loss: 0.00001359
Iteration 217/1000 | Loss: 0.00001358
Iteration 218/1000 | Loss: 0.00001358
Iteration 219/1000 | Loss: 0.00001358
Iteration 220/1000 | Loss: 0.00001358
Iteration 221/1000 | Loss: 0.00001358
Iteration 222/1000 | Loss: 0.00001358
Iteration 223/1000 | Loss: 0.00001358
Iteration 224/1000 | Loss: 0.00001358
Iteration 225/1000 | Loss: 0.00001358
Iteration 226/1000 | Loss: 0.00001358
Iteration 227/1000 | Loss: 0.00001358
Iteration 228/1000 | Loss: 0.00001358
Iteration 229/1000 | Loss: 0.00001358
Iteration 230/1000 | Loss: 0.00001358
Iteration 231/1000 | Loss: 0.00001358
Iteration 232/1000 | Loss: 0.00001358
Iteration 233/1000 | Loss: 0.00001358
Iteration 234/1000 | Loss: 0.00001358
Iteration 235/1000 | Loss: 0.00001358
Iteration 236/1000 | Loss: 0.00001358
Iteration 237/1000 | Loss: 0.00001358
Iteration 238/1000 | Loss: 0.00001358
Iteration 239/1000 | Loss: 0.00001358
Iteration 240/1000 | Loss: 0.00001358
Iteration 241/1000 | Loss: 0.00001358
Iteration 242/1000 | Loss: 0.00001358
Iteration 243/1000 | Loss: 0.00001358
Iteration 244/1000 | Loss: 0.00001358
Iteration 245/1000 | Loss: 0.00001358
Iteration 246/1000 | Loss: 0.00001358
Iteration 247/1000 | Loss: 0.00001358
Iteration 248/1000 | Loss: 0.00001358
Iteration 249/1000 | Loss: 0.00001358
Iteration 250/1000 | Loss: 0.00001358
Iteration 251/1000 | Loss: 0.00001358
Iteration 252/1000 | Loss: 0.00001358
Iteration 253/1000 | Loss: 0.00001358
Iteration 254/1000 | Loss: 0.00001358
Iteration 255/1000 | Loss: 0.00001358
Iteration 256/1000 | Loss: 0.00001358
Iteration 257/1000 | Loss: 0.00001358
Iteration 258/1000 | Loss: 0.00001358
Iteration 259/1000 | Loss: 0.00001358
Iteration 260/1000 | Loss: 0.00001358
Iteration 261/1000 | Loss: 0.00001358
Iteration 262/1000 | Loss: 0.00001358
Iteration 263/1000 | Loss: 0.00001358
Iteration 264/1000 | Loss: 0.00001358
Iteration 265/1000 | Loss: 0.00001358
Iteration 266/1000 | Loss: 0.00001358
Iteration 267/1000 | Loss: 0.00001358
Iteration 268/1000 | Loss: 0.00001358
Iteration 269/1000 | Loss: 0.00001358
Iteration 270/1000 | Loss: 0.00001358
Iteration 271/1000 | Loss: 0.00001358
Iteration 272/1000 | Loss: 0.00001358
Iteration 273/1000 | Loss: 0.00001358
Iteration 274/1000 | Loss: 0.00001358
Iteration 275/1000 | Loss: 0.00001358
Iteration 276/1000 | Loss: 0.00001358
Iteration 277/1000 | Loss: 0.00001358
Iteration 278/1000 | Loss: 0.00001358
Iteration 279/1000 | Loss: 0.00001358
Iteration 280/1000 | Loss: 0.00001358
Iteration 281/1000 | Loss: 0.00001358
Iteration 282/1000 | Loss: 0.00001358
Iteration 283/1000 | Loss: 0.00001358
Iteration 284/1000 | Loss: 0.00001358
Iteration 285/1000 | Loss: 0.00001358
Iteration 286/1000 | Loss: 0.00001358
Iteration 287/1000 | Loss: 0.00001358
Iteration 288/1000 | Loss: 0.00001358
Iteration 289/1000 | Loss: 0.00001358
Iteration 290/1000 | Loss: 0.00001358
Iteration 291/1000 | Loss: 0.00001358
Iteration 292/1000 | Loss: 0.00001358
Iteration 293/1000 | Loss: 0.00001358
Iteration 294/1000 | Loss: 0.00001358
Iteration 295/1000 | Loss: 0.00001358
Iteration 296/1000 | Loss: 0.00001358
Iteration 297/1000 | Loss: 0.00001358
Iteration 298/1000 | Loss: 0.00001358
Iteration 299/1000 | Loss: 0.00001358
Iteration 300/1000 | Loss: 0.00001358
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 300. Stopping optimization.
Last 5 losses: [1.3583871805167291e-05, 1.3583871805167291e-05, 1.3583871805167291e-05, 1.3583871805167291e-05, 1.3583871805167291e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3583871805167291e-05

Optimization complete. Final v2v error: 2.9421513080596924 mm

Highest mean error: 4.518489837646484 mm for frame 73

Lowest mean error: 2.4964120388031006 mm for frame 163

Saving results

Total time: 43.3096182346344
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_022/1032/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_022/1032.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_022/1032
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00411183
Iteration 2/25 | Loss: 0.00119704
Iteration 3/25 | Loss: 0.00111744
Iteration 4/25 | Loss: 0.00111352
Iteration 5/25 | Loss: 0.00111285
Iteration 6/25 | Loss: 0.00111285
Iteration 7/25 | Loss: 0.00111285
Iteration 8/25 | Loss: 0.00111285
Iteration 9/25 | Loss: 0.00111285
Iteration 10/25 | Loss: 0.00111285
Iteration 11/25 | Loss: 0.00111285
Iteration 12/25 | Loss: 0.00111285
Iteration 13/25 | Loss: 0.00111285
Iteration 14/25 | Loss: 0.00111285
Iteration 15/25 | Loss: 0.00111285
Iteration 16/25 | Loss: 0.00111285
Iteration 17/25 | Loss: 0.00111285
Iteration 18/25 | Loss: 0.00111285
Iteration 19/25 | Loss: 0.00111285
Iteration 20/25 | Loss: 0.00111285
Iteration 21/25 | Loss: 0.00111285
Iteration 22/25 | Loss: 0.00111285
Iteration 23/25 | Loss: 0.00111285
Iteration 24/25 | Loss: 0.00111285
Iteration 25/25 | Loss: 0.00111285

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36462343
Iteration 2/25 | Loss: 0.00076106
Iteration 3/25 | Loss: 0.00076105
Iteration 4/25 | Loss: 0.00076105
Iteration 5/25 | Loss: 0.00076105
Iteration 6/25 | Loss: 0.00076105
Iteration 7/25 | Loss: 0.00076105
Iteration 8/25 | Loss: 0.00076105
Iteration 9/25 | Loss: 0.00076105
Iteration 10/25 | Loss: 0.00076105
Iteration 11/25 | Loss: 0.00076105
Iteration 12/25 | Loss: 0.00076105
Iteration 13/25 | Loss: 0.00076105
Iteration 14/25 | Loss: 0.00076105
Iteration 15/25 | Loss: 0.00076105
Iteration 16/25 | Loss: 0.00076105
Iteration 17/25 | Loss: 0.00076105
Iteration 18/25 | Loss: 0.00076105
Iteration 19/25 | Loss: 0.00076105
Iteration 20/25 | Loss: 0.00076105
Iteration 21/25 | Loss: 0.00076105
Iteration 22/25 | Loss: 0.00076105
Iteration 23/25 | Loss: 0.00076105
Iteration 24/25 | Loss: 0.00076105
Iteration 25/25 | Loss: 0.00076105

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00076105
Iteration 2/1000 | Loss: 0.00002223
Iteration 3/1000 | Loss: 0.00001589
Iteration 4/1000 | Loss: 0.00001461
Iteration 5/1000 | Loss: 0.00001385
Iteration 6/1000 | Loss: 0.00001353
Iteration 7/1000 | Loss: 0.00001343
Iteration 8/1000 | Loss: 0.00001323
Iteration 9/1000 | Loss: 0.00001319
Iteration 10/1000 | Loss: 0.00001303
Iteration 11/1000 | Loss: 0.00001294
Iteration 12/1000 | Loss: 0.00001289
Iteration 13/1000 | Loss: 0.00001279
Iteration 14/1000 | Loss: 0.00001276
Iteration 15/1000 | Loss: 0.00001276
Iteration 16/1000 | Loss: 0.00001274
Iteration 17/1000 | Loss: 0.00001273
Iteration 18/1000 | Loss: 0.00001272
Iteration 19/1000 | Loss: 0.00001270
Iteration 20/1000 | Loss: 0.00001268
Iteration 21/1000 | Loss: 0.00001266
Iteration 22/1000 | Loss: 0.00001263
Iteration 23/1000 | Loss: 0.00001262
Iteration 24/1000 | Loss: 0.00001261
Iteration 25/1000 | Loss: 0.00001260
Iteration 26/1000 | Loss: 0.00001260
Iteration 27/1000 | Loss: 0.00001260
Iteration 28/1000 | Loss: 0.00001256
Iteration 29/1000 | Loss: 0.00001256
Iteration 30/1000 | Loss: 0.00001256
Iteration 31/1000 | Loss: 0.00001256
Iteration 32/1000 | Loss: 0.00001256
Iteration 33/1000 | Loss: 0.00001256
Iteration 34/1000 | Loss: 0.00001255
Iteration 35/1000 | Loss: 0.00001255
Iteration 36/1000 | Loss: 0.00001255
Iteration 37/1000 | Loss: 0.00001255
Iteration 38/1000 | Loss: 0.00001255
Iteration 39/1000 | Loss: 0.00001255
Iteration 40/1000 | Loss: 0.00001254
Iteration 41/1000 | Loss: 0.00001254
Iteration 42/1000 | Loss: 0.00001254
Iteration 43/1000 | Loss: 0.00001253
Iteration 44/1000 | Loss: 0.00001253
Iteration 45/1000 | Loss: 0.00001253
Iteration 46/1000 | Loss: 0.00001252
Iteration 47/1000 | Loss: 0.00001252
Iteration 48/1000 | Loss: 0.00001252
Iteration 49/1000 | Loss: 0.00001252
Iteration 50/1000 | Loss: 0.00001252
Iteration 51/1000 | Loss: 0.00001251
Iteration 52/1000 | Loss: 0.00001251
Iteration 53/1000 | Loss: 0.00001250
Iteration 54/1000 | Loss: 0.00001250
Iteration 55/1000 | Loss: 0.00001250
Iteration 56/1000 | Loss: 0.00001250
Iteration 57/1000 | Loss: 0.00001249
Iteration 58/1000 | Loss: 0.00001249
Iteration 59/1000 | Loss: 0.00001248
Iteration 60/1000 | Loss: 0.00001248
Iteration 61/1000 | Loss: 0.00001248
Iteration 62/1000 | Loss: 0.00001247
Iteration 63/1000 | Loss: 0.00001247
Iteration 64/1000 | Loss: 0.00001247
Iteration 65/1000 | Loss: 0.00001247
Iteration 66/1000 | Loss: 0.00001247
Iteration 67/1000 | Loss: 0.00001247
Iteration 68/1000 | Loss: 0.00001247
Iteration 69/1000 | Loss: 0.00001247
Iteration 70/1000 | Loss: 0.00001247
Iteration 71/1000 | Loss: 0.00001247
Iteration 72/1000 | Loss: 0.00001246
Iteration 73/1000 | Loss: 0.00001246
Iteration 74/1000 | Loss: 0.00001246
Iteration 75/1000 | Loss: 0.00001245
Iteration 76/1000 | Loss: 0.00001244
Iteration 77/1000 | Loss: 0.00001244
Iteration 78/1000 | Loss: 0.00001244
Iteration 79/1000 | Loss: 0.00001244
Iteration 80/1000 | Loss: 0.00001243
Iteration 81/1000 | Loss: 0.00001243
Iteration 82/1000 | Loss: 0.00001242
Iteration 83/1000 | Loss: 0.00001242
Iteration 84/1000 | Loss: 0.00001242
Iteration 85/1000 | Loss: 0.00001241
Iteration 86/1000 | Loss: 0.00001241
Iteration 87/1000 | Loss: 0.00001241
Iteration 88/1000 | Loss: 0.00001241
Iteration 89/1000 | Loss: 0.00001241
Iteration 90/1000 | Loss: 0.00001241
Iteration 91/1000 | Loss: 0.00001241
Iteration 92/1000 | Loss: 0.00001241
Iteration 93/1000 | Loss: 0.00001241
Iteration 94/1000 | Loss: 0.00001240
Iteration 95/1000 | Loss: 0.00001240
Iteration 96/1000 | Loss: 0.00001240
Iteration 97/1000 | Loss: 0.00001240
Iteration 98/1000 | Loss: 0.00001240
Iteration 99/1000 | Loss: 0.00001240
Iteration 100/1000 | Loss: 0.00001240
Iteration 101/1000 | Loss: 0.00001240
Iteration 102/1000 | Loss: 0.00001240
Iteration 103/1000 | Loss: 0.00001240
Iteration 104/1000 | Loss: 0.00001239
Iteration 105/1000 | Loss: 0.00001239
Iteration 106/1000 | Loss: 0.00001239
Iteration 107/1000 | Loss: 0.00001238
Iteration 108/1000 | Loss: 0.00001238
Iteration 109/1000 | Loss: 0.00001238
Iteration 110/1000 | Loss: 0.00001238
Iteration 111/1000 | Loss: 0.00001238
Iteration 112/1000 | Loss: 0.00001238
Iteration 113/1000 | Loss: 0.00001238
Iteration 114/1000 | Loss: 0.00001238
Iteration 115/1000 | Loss: 0.00001238
Iteration 116/1000 | Loss: 0.00001238
Iteration 117/1000 | Loss: 0.00001237
Iteration 118/1000 | Loss: 0.00001237
Iteration 119/1000 | Loss: 0.00001237
Iteration 120/1000 | Loss: 0.00001237
Iteration 121/1000 | Loss: 0.00001237
Iteration 122/1000 | Loss: 0.00001237
Iteration 123/1000 | Loss: 0.00001237
Iteration 124/1000 | Loss: 0.00001237
Iteration 125/1000 | Loss: 0.00001237
Iteration 126/1000 | Loss: 0.00001237
Iteration 127/1000 | Loss: 0.00001237
Iteration 128/1000 | Loss: 0.00001237
Iteration 129/1000 | Loss: 0.00001237
Iteration 130/1000 | Loss: 0.00001237
Iteration 131/1000 | Loss: 0.00001237
Iteration 132/1000 | Loss: 0.00001237
Iteration 133/1000 | Loss: 0.00001237
Iteration 134/1000 | Loss: 0.00001237
Iteration 135/1000 | Loss: 0.00001237
Iteration 136/1000 | Loss: 0.00001237
Iteration 137/1000 | Loss: 0.00001237
Iteration 138/1000 | Loss: 0.00001237
Iteration 139/1000 | Loss: 0.00001237
Iteration 140/1000 | Loss: 0.00001237
Iteration 141/1000 | Loss: 0.00001237
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 141. Stopping optimization.
Last 5 losses: [1.2374131074466277e-05, 1.2374131074466277e-05, 1.2374131074466277e-05, 1.2374131074466277e-05, 1.2374131074466277e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2374131074466277e-05

Optimization complete. Final v2v error: 2.986152172088623 mm

Highest mean error: 3.3396217823028564 mm for frame 74

Lowest mean error: 2.6570193767547607 mm for frame 203

Saving results

Total time: 33.664644956588745
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_022/1058/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_022/1058.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_022/1058
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00417968
Iteration 2/25 | Loss: 0.00116487
Iteration 3/25 | Loss: 0.00110209
Iteration 4/25 | Loss: 0.00108769
Iteration 5/25 | Loss: 0.00108349
Iteration 6/25 | Loss: 0.00108317
Iteration 7/25 | Loss: 0.00108317
Iteration 8/25 | Loss: 0.00108317
Iteration 9/25 | Loss: 0.00108317
Iteration 10/25 | Loss: 0.00108317
Iteration 11/25 | Loss: 0.00108317
Iteration 12/25 | Loss: 0.00108317
Iteration 13/25 | Loss: 0.00108317
Iteration 14/25 | Loss: 0.00108317
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0010831740219146013, 0.0010831740219146013, 0.0010831740219146013, 0.0010831740219146013, 0.0010831740219146013]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010831740219146013

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36208451
Iteration 2/25 | Loss: 0.00076079
Iteration 3/25 | Loss: 0.00076079
Iteration 4/25 | Loss: 0.00076079
Iteration 5/25 | Loss: 0.00076079
Iteration 6/25 | Loss: 0.00076079
Iteration 7/25 | Loss: 0.00076079
Iteration 8/25 | Loss: 0.00076079
Iteration 9/25 | Loss: 0.00076079
Iteration 10/25 | Loss: 0.00076079
Iteration 11/25 | Loss: 0.00076079
Iteration 12/25 | Loss: 0.00076079
Iteration 13/25 | Loss: 0.00076079
Iteration 14/25 | Loss: 0.00076079
Iteration 15/25 | Loss: 0.00076079
Iteration 16/25 | Loss: 0.00076079
Iteration 17/25 | Loss: 0.00076079
Iteration 18/25 | Loss: 0.00076079
Iteration 19/25 | Loss: 0.00076079
Iteration 20/25 | Loss: 0.00076079
Iteration 21/25 | Loss: 0.00076079
Iteration 22/25 | Loss: 0.00076079
Iteration 23/25 | Loss: 0.00076079
Iteration 24/25 | Loss: 0.00076079
Iteration 25/25 | Loss: 0.00076079

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00076079
Iteration 2/1000 | Loss: 0.00002116
Iteration 3/1000 | Loss: 0.00001588
Iteration 4/1000 | Loss: 0.00001483
Iteration 5/1000 | Loss: 0.00001401
Iteration 6/1000 | Loss: 0.00001356
Iteration 7/1000 | Loss: 0.00001329
Iteration 8/1000 | Loss: 0.00001328
Iteration 9/1000 | Loss: 0.00001301
Iteration 10/1000 | Loss: 0.00001278
Iteration 11/1000 | Loss: 0.00001274
Iteration 12/1000 | Loss: 0.00001270
Iteration 13/1000 | Loss: 0.00001267
Iteration 14/1000 | Loss: 0.00001262
Iteration 15/1000 | Loss: 0.00001261
Iteration 16/1000 | Loss: 0.00001261
Iteration 17/1000 | Loss: 0.00001257
Iteration 18/1000 | Loss: 0.00001256
Iteration 19/1000 | Loss: 0.00001254
Iteration 20/1000 | Loss: 0.00001254
Iteration 21/1000 | Loss: 0.00001253
Iteration 22/1000 | Loss: 0.00001253
Iteration 23/1000 | Loss: 0.00001252
Iteration 24/1000 | Loss: 0.00001245
Iteration 25/1000 | Loss: 0.00001244
Iteration 26/1000 | Loss: 0.00001237
Iteration 27/1000 | Loss: 0.00001236
Iteration 28/1000 | Loss: 0.00001233
Iteration 29/1000 | Loss: 0.00001232
Iteration 30/1000 | Loss: 0.00001231
Iteration 31/1000 | Loss: 0.00001231
Iteration 32/1000 | Loss: 0.00001227
Iteration 33/1000 | Loss: 0.00001227
Iteration 34/1000 | Loss: 0.00001227
Iteration 35/1000 | Loss: 0.00001227
Iteration 36/1000 | Loss: 0.00001226
Iteration 37/1000 | Loss: 0.00001226
Iteration 38/1000 | Loss: 0.00001225
Iteration 39/1000 | Loss: 0.00001225
Iteration 40/1000 | Loss: 0.00001224
Iteration 41/1000 | Loss: 0.00001224
Iteration 42/1000 | Loss: 0.00001224
Iteration 43/1000 | Loss: 0.00001224
Iteration 44/1000 | Loss: 0.00001223
Iteration 45/1000 | Loss: 0.00001223
Iteration 46/1000 | Loss: 0.00001222
Iteration 47/1000 | Loss: 0.00001222
Iteration 48/1000 | Loss: 0.00001221
Iteration 49/1000 | Loss: 0.00001220
Iteration 50/1000 | Loss: 0.00001220
Iteration 51/1000 | Loss: 0.00001218
Iteration 52/1000 | Loss: 0.00001217
Iteration 53/1000 | Loss: 0.00001217
Iteration 54/1000 | Loss: 0.00001215
Iteration 55/1000 | Loss: 0.00001215
Iteration 56/1000 | Loss: 0.00001214
Iteration 57/1000 | Loss: 0.00001214
Iteration 58/1000 | Loss: 0.00001209
Iteration 59/1000 | Loss: 0.00001209
Iteration 60/1000 | Loss: 0.00001209
Iteration 61/1000 | Loss: 0.00001204
Iteration 62/1000 | Loss: 0.00001204
Iteration 63/1000 | Loss: 0.00001204
Iteration 64/1000 | Loss: 0.00001204
Iteration 65/1000 | Loss: 0.00001204
Iteration 66/1000 | Loss: 0.00001204
Iteration 67/1000 | Loss: 0.00001204
Iteration 68/1000 | Loss: 0.00001204
Iteration 69/1000 | Loss: 0.00001204
Iteration 70/1000 | Loss: 0.00001204
Iteration 71/1000 | Loss: 0.00001204
Iteration 72/1000 | Loss: 0.00001203
Iteration 73/1000 | Loss: 0.00001203
Iteration 74/1000 | Loss: 0.00001203
Iteration 75/1000 | Loss: 0.00001203
Iteration 76/1000 | Loss: 0.00001203
Iteration 77/1000 | Loss: 0.00001203
Iteration 78/1000 | Loss: 0.00001200
Iteration 79/1000 | Loss: 0.00001199
Iteration 80/1000 | Loss: 0.00001199
Iteration 81/1000 | Loss: 0.00001198
Iteration 82/1000 | Loss: 0.00001198
Iteration 83/1000 | Loss: 0.00001198
Iteration 84/1000 | Loss: 0.00001198
Iteration 85/1000 | Loss: 0.00001198
Iteration 86/1000 | Loss: 0.00001197
Iteration 87/1000 | Loss: 0.00001196
Iteration 88/1000 | Loss: 0.00001195
Iteration 89/1000 | Loss: 0.00001195
Iteration 90/1000 | Loss: 0.00001195
Iteration 91/1000 | Loss: 0.00001194
Iteration 92/1000 | Loss: 0.00001194
Iteration 93/1000 | Loss: 0.00001194
Iteration 94/1000 | Loss: 0.00001194
Iteration 95/1000 | Loss: 0.00001194
Iteration 96/1000 | Loss: 0.00001194
Iteration 97/1000 | Loss: 0.00001193
Iteration 98/1000 | Loss: 0.00001193
Iteration 99/1000 | Loss: 0.00001193
Iteration 100/1000 | Loss: 0.00001193
Iteration 101/1000 | Loss: 0.00001193
Iteration 102/1000 | Loss: 0.00001193
Iteration 103/1000 | Loss: 0.00001193
Iteration 104/1000 | Loss: 0.00001193
Iteration 105/1000 | Loss: 0.00001193
Iteration 106/1000 | Loss: 0.00001193
Iteration 107/1000 | Loss: 0.00001191
Iteration 108/1000 | Loss: 0.00001191
Iteration 109/1000 | Loss: 0.00001190
Iteration 110/1000 | Loss: 0.00001190
Iteration 111/1000 | Loss: 0.00001190
Iteration 112/1000 | Loss: 0.00001190
Iteration 113/1000 | Loss: 0.00001190
Iteration 114/1000 | Loss: 0.00001190
Iteration 115/1000 | Loss: 0.00001190
Iteration 116/1000 | Loss: 0.00001190
Iteration 117/1000 | Loss: 0.00001189
Iteration 118/1000 | Loss: 0.00001189
Iteration 119/1000 | Loss: 0.00001189
Iteration 120/1000 | Loss: 0.00001189
Iteration 121/1000 | Loss: 0.00001189
Iteration 122/1000 | Loss: 0.00001189
Iteration 123/1000 | Loss: 0.00001189
Iteration 124/1000 | Loss: 0.00001189
Iteration 125/1000 | Loss: 0.00001189
Iteration 126/1000 | Loss: 0.00001189
Iteration 127/1000 | Loss: 0.00001189
Iteration 128/1000 | Loss: 0.00001188
Iteration 129/1000 | Loss: 0.00001188
Iteration 130/1000 | Loss: 0.00001188
Iteration 131/1000 | Loss: 0.00001188
Iteration 132/1000 | Loss: 0.00001187
Iteration 133/1000 | Loss: 0.00001187
Iteration 134/1000 | Loss: 0.00001187
Iteration 135/1000 | Loss: 0.00001187
Iteration 136/1000 | Loss: 0.00001187
Iteration 137/1000 | Loss: 0.00001187
Iteration 138/1000 | Loss: 0.00001187
Iteration 139/1000 | Loss: 0.00001187
Iteration 140/1000 | Loss: 0.00001187
Iteration 141/1000 | Loss: 0.00001187
Iteration 142/1000 | Loss: 0.00001187
Iteration 143/1000 | Loss: 0.00001187
Iteration 144/1000 | Loss: 0.00001187
Iteration 145/1000 | Loss: 0.00001186
Iteration 146/1000 | Loss: 0.00001186
Iteration 147/1000 | Loss: 0.00001186
Iteration 148/1000 | Loss: 0.00001186
Iteration 149/1000 | Loss: 0.00001186
Iteration 150/1000 | Loss: 0.00001186
Iteration 151/1000 | Loss: 0.00001186
Iteration 152/1000 | Loss: 0.00001186
Iteration 153/1000 | Loss: 0.00001186
Iteration 154/1000 | Loss: 0.00001186
Iteration 155/1000 | Loss: 0.00001185
Iteration 156/1000 | Loss: 0.00001185
Iteration 157/1000 | Loss: 0.00001185
Iteration 158/1000 | Loss: 0.00001185
Iteration 159/1000 | Loss: 0.00001184
Iteration 160/1000 | Loss: 0.00001184
Iteration 161/1000 | Loss: 0.00001184
Iteration 162/1000 | Loss: 0.00001184
Iteration 163/1000 | Loss: 0.00001184
Iteration 164/1000 | Loss: 0.00001184
Iteration 165/1000 | Loss: 0.00001184
Iteration 166/1000 | Loss: 0.00001184
Iteration 167/1000 | Loss: 0.00001184
Iteration 168/1000 | Loss: 0.00001184
Iteration 169/1000 | Loss: 0.00001184
Iteration 170/1000 | Loss: 0.00001184
Iteration 171/1000 | Loss: 0.00001184
Iteration 172/1000 | Loss: 0.00001184
Iteration 173/1000 | Loss: 0.00001184
Iteration 174/1000 | Loss: 0.00001184
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 174. Stopping optimization.
Last 5 losses: [1.1838156751764473e-05, 1.1838156751764473e-05, 1.1838156751764473e-05, 1.1838156751764473e-05, 1.1838156751764473e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1838156751764473e-05

Optimization complete. Final v2v error: 2.9547617435455322 mm

Highest mean error: 3.02984619140625 mm for frame 174

Lowest mean error: 2.863407850265503 mm for frame 49

Saving results

Total time: 43.56684327125549
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_022/1044/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_022/1044.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_022/1044
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00565477
Iteration 2/25 | Loss: 0.00127506
Iteration 3/25 | Loss: 0.00116174
Iteration 4/25 | Loss: 0.00114075
Iteration 5/25 | Loss: 0.00113385
Iteration 6/25 | Loss: 0.00113221
Iteration 7/25 | Loss: 0.00113205
Iteration 8/25 | Loss: 0.00113205
Iteration 9/25 | Loss: 0.00113205
Iteration 10/25 | Loss: 0.00113205
Iteration 11/25 | Loss: 0.00113205
Iteration 12/25 | Loss: 0.00113205
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0011320450576022267, 0.0011320450576022267, 0.0011320450576022267, 0.0011320450576022267, 0.0011320450576022267]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011320450576022267

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 6.79781723
Iteration 2/25 | Loss: 0.00096727
Iteration 3/25 | Loss: 0.00096726
Iteration 4/25 | Loss: 0.00096726
Iteration 5/25 | Loss: 0.00096726
Iteration 6/25 | Loss: 0.00096726
Iteration 7/25 | Loss: 0.00096726
Iteration 8/25 | Loss: 0.00096726
Iteration 9/25 | Loss: 0.00096726
Iteration 10/25 | Loss: 0.00096726
Iteration 11/25 | Loss: 0.00096726
Iteration 12/25 | Loss: 0.00096726
Iteration 13/25 | Loss: 0.00096726
Iteration 14/25 | Loss: 0.00096726
Iteration 15/25 | Loss: 0.00096726
Iteration 16/25 | Loss: 0.00096726
Iteration 17/25 | Loss: 0.00096726
Iteration 18/25 | Loss: 0.00096726
Iteration 19/25 | Loss: 0.00096726
Iteration 20/25 | Loss: 0.00096726
Iteration 21/25 | Loss: 0.00096726
Iteration 22/25 | Loss: 0.00096726
Iteration 23/25 | Loss: 0.00096726
Iteration 24/25 | Loss: 0.00096726
Iteration 25/25 | Loss: 0.00096726
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.000967262196354568, 0.000967262196354568, 0.000967262196354568, 0.000967262196354568, 0.000967262196354568]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000967262196354568

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00096726
Iteration 2/1000 | Loss: 0.00003872
Iteration 3/1000 | Loss: 0.00002611
Iteration 4/1000 | Loss: 0.00002070
Iteration 5/1000 | Loss: 0.00001944
Iteration 6/1000 | Loss: 0.00001871
Iteration 7/1000 | Loss: 0.00001829
Iteration 8/1000 | Loss: 0.00001790
Iteration 9/1000 | Loss: 0.00001759
Iteration 10/1000 | Loss: 0.00001750
Iteration 11/1000 | Loss: 0.00001735
Iteration 12/1000 | Loss: 0.00001728
Iteration 13/1000 | Loss: 0.00001720
Iteration 14/1000 | Loss: 0.00001711
Iteration 15/1000 | Loss: 0.00001706
Iteration 16/1000 | Loss: 0.00001705
Iteration 17/1000 | Loss: 0.00001705
Iteration 18/1000 | Loss: 0.00001700
Iteration 19/1000 | Loss: 0.00001699
Iteration 20/1000 | Loss: 0.00001696
Iteration 21/1000 | Loss: 0.00001695
Iteration 22/1000 | Loss: 0.00001694
Iteration 23/1000 | Loss: 0.00001693
Iteration 24/1000 | Loss: 0.00001691
Iteration 25/1000 | Loss: 0.00001690
Iteration 26/1000 | Loss: 0.00001690
Iteration 27/1000 | Loss: 0.00001690
Iteration 28/1000 | Loss: 0.00001689
Iteration 29/1000 | Loss: 0.00001682
Iteration 30/1000 | Loss: 0.00001681
Iteration 31/1000 | Loss: 0.00001676
Iteration 32/1000 | Loss: 0.00001674
Iteration 33/1000 | Loss: 0.00001673
Iteration 34/1000 | Loss: 0.00001673
Iteration 35/1000 | Loss: 0.00001673
Iteration 36/1000 | Loss: 0.00001673
Iteration 37/1000 | Loss: 0.00001672
Iteration 38/1000 | Loss: 0.00001672
Iteration 39/1000 | Loss: 0.00001671
Iteration 40/1000 | Loss: 0.00001671
Iteration 41/1000 | Loss: 0.00001670
Iteration 42/1000 | Loss: 0.00001669
Iteration 43/1000 | Loss: 0.00001669
Iteration 44/1000 | Loss: 0.00001669
Iteration 45/1000 | Loss: 0.00001669
Iteration 46/1000 | Loss: 0.00001668
Iteration 47/1000 | Loss: 0.00001668
Iteration 48/1000 | Loss: 0.00001667
Iteration 49/1000 | Loss: 0.00001667
Iteration 50/1000 | Loss: 0.00001667
Iteration 51/1000 | Loss: 0.00001666
Iteration 52/1000 | Loss: 0.00001666
Iteration 53/1000 | Loss: 0.00001666
Iteration 54/1000 | Loss: 0.00001666
Iteration 55/1000 | Loss: 0.00001666
Iteration 56/1000 | Loss: 0.00001666
Iteration 57/1000 | Loss: 0.00001665
Iteration 58/1000 | Loss: 0.00001665
Iteration 59/1000 | Loss: 0.00001665
Iteration 60/1000 | Loss: 0.00001665
Iteration 61/1000 | Loss: 0.00001664
Iteration 62/1000 | Loss: 0.00001664
Iteration 63/1000 | Loss: 0.00001664
Iteration 64/1000 | Loss: 0.00001664
Iteration 65/1000 | Loss: 0.00001664
Iteration 66/1000 | Loss: 0.00001663
Iteration 67/1000 | Loss: 0.00001663
Iteration 68/1000 | Loss: 0.00001663
Iteration 69/1000 | Loss: 0.00001662
Iteration 70/1000 | Loss: 0.00001662
Iteration 71/1000 | Loss: 0.00001662
Iteration 72/1000 | Loss: 0.00001662
Iteration 73/1000 | Loss: 0.00001661
Iteration 74/1000 | Loss: 0.00001661
Iteration 75/1000 | Loss: 0.00001661
Iteration 76/1000 | Loss: 0.00001661
Iteration 77/1000 | Loss: 0.00001660
Iteration 78/1000 | Loss: 0.00001660
Iteration 79/1000 | Loss: 0.00001660
Iteration 80/1000 | Loss: 0.00001659
Iteration 81/1000 | Loss: 0.00001659
Iteration 82/1000 | Loss: 0.00001659
Iteration 83/1000 | Loss: 0.00001659
Iteration 84/1000 | Loss: 0.00001659
Iteration 85/1000 | Loss: 0.00001659
Iteration 86/1000 | Loss: 0.00001658
Iteration 87/1000 | Loss: 0.00001658
Iteration 88/1000 | Loss: 0.00001658
Iteration 89/1000 | Loss: 0.00001657
Iteration 90/1000 | Loss: 0.00001657
Iteration 91/1000 | Loss: 0.00001657
Iteration 92/1000 | Loss: 0.00001657
Iteration 93/1000 | Loss: 0.00001656
Iteration 94/1000 | Loss: 0.00001656
Iteration 95/1000 | Loss: 0.00001656
Iteration 96/1000 | Loss: 0.00001656
Iteration 97/1000 | Loss: 0.00001656
Iteration 98/1000 | Loss: 0.00001655
Iteration 99/1000 | Loss: 0.00001655
Iteration 100/1000 | Loss: 0.00001655
Iteration 101/1000 | Loss: 0.00001654
Iteration 102/1000 | Loss: 0.00001654
Iteration 103/1000 | Loss: 0.00001654
Iteration 104/1000 | Loss: 0.00001653
Iteration 105/1000 | Loss: 0.00001653
Iteration 106/1000 | Loss: 0.00001653
Iteration 107/1000 | Loss: 0.00001652
Iteration 108/1000 | Loss: 0.00001652
Iteration 109/1000 | Loss: 0.00001651
Iteration 110/1000 | Loss: 0.00001651
Iteration 111/1000 | Loss: 0.00001651
Iteration 112/1000 | Loss: 0.00001651
Iteration 113/1000 | Loss: 0.00001650
Iteration 114/1000 | Loss: 0.00001650
Iteration 115/1000 | Loss: 0.00001649
Iteration 116/1000 | Loss: 0.00001649
Iteration 117/1000 | Loss: 0.00001649
Iteration 118/1000 | Loss: 0.00001649
Iteration 119/1000 | Loss: 0.00001649
Iteration 120/1000 | Loss: 0.00001649
Iteration 121/1000 | Loss: 0.00001649
Iteration 122/1000 | Loss: 0.00001648
Iteration 123/1000 | Loss: 0.00001648
Iteration 124/1000 | Loss: 0.00001648
Iteration 125/1000 | Loss: 0.00001648
Iteration 126/1000 | Loss: 0.00001647
Iteration 127/1000 | Loss: 0.00001647
Iteration 128/1000 | Loss: 0.00001647
Iteration 129/1000 | Loss: 0.00001647
Iteration 130/1000 | Loss: 0.00001647
Iteration 131/1000 | Loss: 0.00001647
Iteration 132/1000 | Loss: 0.00001647
Iteration 133/1000 | Loss: 0.00001646
Iteration 134/1000 | Loss: 0.00001646
Iteration 135/1000 | Loss: 0.00001646
Iteration 136/1000 | Loss: 0.00001646
Iteration 137/1000 | Loss: 0.00001646
Iteration 138/1000 | Loss: 0.00001646
Iteration 139/1000 | Loss: 0.00001646
Iteration 140/1000 | Loss: 0.00001646
Iteration 141/1000 | Loss: 0.00001646
Iteration 142/1000 | Loss: 0.00001646
Iteration 143/1000 | Loss: 0.00001645
Iteration 144/1000 | Loss: 0.00001645
Iteration 145/1000 | Loss: 0.00001645
Iteration 146/1000 | Loss: 0.00001645
Iteration 147/1000 | Loss: 0.00001645
Iteration 148/1000 | Loss: 0.00001644
Iteration 149/1000 | Loss: 0.00001644
Iteration 150/1000 | Loss: 0.00001644
Iteration 151/1000 | Loss: 0.00001644
Iteration 152/1000 | Loss: 0.00001644
Iteration 153/1000 | Loss: 0.00001644
Iteration 154/1000 | Loss: 0.00001644
Iteration 155/1000 | Loss: 0.00001643
Iteration 156/1000 | Loss: 0.00001643
Iteration 157/1000 | Loss: 0.00001643
Iteration 158/1000 | Loss: 0.00001643
Iteration 159/1000 | Loss: 0.00001643
Iteration 160/1000 | Loss: 0.00001642
Iteration 161/1000 | Loss: 0.00001642
Iteration 162/1000 | Loss: 0.00001642
Iteration 163/1000 | Loss: 0.00001642
Iteration 164/1000 | Loss: 0.00001642
Iteration 165/1000 | Loss: 0.00001642
Iteration 166/1000 | Loss: 0.00001642
Iteration 167/1000 | Loss: 0.00001642
Iteration 168/1000 | Loss: 0.00001642
Iteration 169/1000 | Loss: 0.00001642
Iteration 170/1000 | Loss: 0.00001642
Iteration 171/1000 | Loss: 0.00001642
Iteration 172/1000 | Loss: 0.00001642
Iteration 173/1000 | Loss: 0.00001642
Iteration 174/1000 | Loss: 0.00001642
Iteration 175/1000 | Loss: 0.00001641
Iteration 176/1000 | Loss: 0.00001641
Iteration 177/1000 | Loss: 0.00001641
Iteration 178/1000 | Loss: 0.00001641
Iteration 179/1000 | Loss: 0.00001641
Iteration 180/1000 | Loss: 0.00001641
Iteration 181/1000 | Loss: 0.00001641
Iteration 182/1000 | Loss: 0.00001641
Iteration 183/1000 | Loss: 0.00001641
Iteration 184/1000 | Loss: 0.00001641
Iteration 185/1000 | Loss: 0.00001641
Iteration 186/1000 | Loss: 0.00001641
Iteration 187/1000 | Loss: 0.00001641
Iteration 188/1000 | Loss: 0.00001641
Iteration 189/1000 | Loss: 0.00001641
Iteration 190/1000 | Loss: 0.00001641
Iteration 191/1000 | Loss: 0.00001641
Iteration 192/1000 | Loss: 0.00001641
Iteration 193/1000 | Loss: 0.00001641
Iteration 194/1000 | Loss: 0.00001641
Iteration 195/1000 | Loss: 0.00001641
Iteration 196/1000 | Loss: 0.00001641
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 196. Stopping optimization.
Last 5 losses: [1.640644586586859e-05, 1.640644586586859e-05, 1.640644586586859e-05, 1.640644586586859e-05, 1.640644586586859e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.640644586586859e-05

Optimization complete. Final v2v error: 3.362248659133911 mm

Highest mean error: 4.850090503692627 mm for frame 97

Lowest mean error: 2.710198402404785 mm for frame 43

Saving results

Total time: 41.84920859336853
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_022/1077/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_022/1077.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_022/1077
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00481606
Iteration 2/25 | Loss: 0.00121755
Iteration 3/25 | Loss: 0.00111788
Iteration 4/25 | Loss: 0.00110472
Iteration 5/25 | Loss: 0.00110138
Iteration 6/25 | Loss: 0.00110104
Iteration 7/25 | Loss: 0.00110104
Iteration 8/25 | Loss: 0.00110104
Iteration 9/25 | Loss: 0.00110104
Iteration 10/25 | Loss: 0.00110104
Iteration 11/25 | Loss: 0.00110104
Iteration 12/25 | Loss: 0.00110104
Iteration 13/25 | Loss: 0.00110104
Iteration 14/25 | Loss: 0.00110104
Iteration 15/25 | Loss: 0.00110104
Iteration 16/25 | Loss: 0.00110104
Iteration 17/25 | Loss: 0.00110104
Iteration 18/25 | Loss: 0.00110104
Iteration 19/25 | Loss: 0.00110104
Iteration 20/25 | Loss: 0.00110104
Iteration 21/25 | Loss: 0.00110104
Iteration 22/25 | Loss: 0.00110104
Iteration 23/25 | Loss: 0.00110104
Iteration 24/25 | Loss: 0.00110104
Iteration 25/25 | Loss: 0.00110104

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.66050005
Iteration 2/25 | Loss: 0.00080423
Iteration 3/25 | Loss: 0.00080422
Iteration 4/25 | Loss: 0.00080422
Iteration 5/25 | Loss: 0.00080422
Iteration 6/25 | Loss: 0.00080422
Iteration 7/25 | Loss: 0.00080422
Iteration 8/25 | Loss: 0.00080422
Iteration 9/25 | Loss: 0.00080422
Iteration 10/25 | Loss: 0.00080422
Iteration 11/25 | Loss: 0.00080422
Iteration 12/25 | Loss: 0.00080422
Iteration 13/25 | Loss: 0.00080422
Iteration 14/25 | Loss: 0.00080422
Iteration 15/25 | Loss: 0.00080422
Iteration 16/25 | Loss: 0.00080422
Iteration 17/25 | Loss: 0.00080422
Iteration 18/25 | Loss: 0.00080422
Iteration 19/25 | Loss: 0.00080422
Iteration 20/25 | Loss: 0.00080422
Iteration 21/25 | Loss: 0.00080422
Iteration 22/25 | Loss: 0.00080422
Iteration 23/25 | Loss: 0.00080422
Iteration 24/25 | Loss: 0.00080422
Iteration 25/25 | Loss: 0.00080422

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00080422
Iteration 2/1000 | Loss: 0.00001896
Iteration 3/1000 | Loss: 0.00001453
Iteration 4/1000 | Loss: 0.00001356
Iteration 5/1000 | Loss: 0.00001298
Iteration 6/1000 | Loss: 0.00001265
Iteration 7/1000 | Loss: 0.00001230
Iteration 8/1000 | Loss: 0.00001202
Iteration 9/1000 | Loss: 0.00001176
Iteration 10/1000 | Loss: 0.00001174
Iteration 11/1000 | Loss: 0.00001158
Iteration 12/1000 | Loss: 0.00001153
Iteration 13/1000 | Loss: 0.00001151
Iteration 14/1000 | Loss: 0.00001147
Iteration 15/1000 | Loss: 0.00001146
Iteration 16/1000 | Loss: 0.00001146
Iteration 17/1000 | Loss: 0.00001145
Iteration 18/1000 | Loss: 0.00001144
Iteration 19/1000 | Loss: 0.00001137
Iteration 20/1000 | Loss: 0.00001134
Iteration 21/1000 | Loss: 0.00001133
Iteration 22/1000 | Loss: 0.00001132
Iteration 23/1000 | Loss: 0.00001132
Iteration 24/1000 | Loss: 0.00001131
Iteration 25/1000 | Loss: 0.00001130
Iteration 26/1000 | Loss: 0.00001129
Iteration 27/1000 | Loss: 0.00001128
Iteration 28/1000 | Loss: 0.00001128
Iteration 29/1000 | Loss: 0.00001127
Iteration 30/1000 | Loss: 0.00001127
Iteration 31/1000 | Loss: 0.00001127
Iteration 32/1000 | Loss: 0.00001127
Iteration 33/1000 | Loss: 0.00001127
Iteration 34/1000 | Loss: 0.00001127
Iteration 35/1000 | Loss: 0.00001127
Iteration 36/1000 | Loss: 0.00001127
Iteration 37/1000 | Loss: 0.00001127
Iteration 38/1000 | Loss: 0.00001127
Iteration 39/1000 | Loss: 0.00001127
Iteration 40/1000 | Loss: 0.00001125
Iteration 41/1000 | Loss: 0.00001123
Iteration 42/1000 | Loss: 0.00001121
Iteration 43/1000 | Loss: 0.00001120
Iteration 44/1000 | Loss: 0.00001119
Iteration 45/1000 | Loss: 0.00001119
Iteration 46/1000 | Loss: 0.00001118
Iteration 47/1000 | Loss: 0.00001118
Iteration 48/1000 | Loss: 0.00001118
Iteration 49/1000 | Loss: 0.00001118
Iteration 50/1000 | Loss: 0.00001117
Iteration 51/1000 | Loss: 0.00001117
Iteration 52/1000 | Loss: 0.00001117
Iteration 53/1000 | Loss: 0.00001117
Iteration 54/1000 | Loss: 0.00001117
Iteration 55/1000 | Loss: 0.00001116
Iteration 56/1000 | Loss: 0.00001114
Iteration 57/1000 | Loss: 0.00001114
Iteration 58/1000 | Loss: 0.00001113
Iteration 59/1000 | Loss: 0.00001113
Iteration 60/1000 | Loss: 0.00001113
Iteration 61/1000 | Loss: 0.00001113
Iteration 62/1000 | Loss: 0.00001113
Iteration 63/1000 | Loss: 0.00001113
Iteration 64/1000 | Loss: 0.00001112
Iteration 65/1000 | Loss: 0.00001112
Iteration 66/1000 | Loss: 0.00001112
Iteration 67/1000 | Loss: 0.00001112
Iteration 68/1000 | Loss: 0.00001112
Iteration 69/1000 | Loss: 0.00001112
Iteration 70/1000 | Loss: 0.00001111
Iteration 71/1000 | Loss: 0.00001111
Iteration 72/1000 | Loss: 0.00001110
Iteration 73/1000 | Loss: 0.00001110
Iteration 74/1000 | Loss: 0.00001110
Iteration 75/1000 | Loss: 0.00001110
Iteration 76/1000 | Loss: 0.00001110
Iteration 77/1000 | Loss: 0.00001110
Iteration 78/1000 | Loss: 0.00001110
Iteration 79/1000 | Loss: 0.00001109
Iteration 80/1000 | Loss: 0.00001109
Iteration 81/1000 | Loss: 0.00001109
Iteration 82/1000 | Loss: 0.00001109
Iteration 83/1000 | Loss: 0.00001109
Iteration 84/1000 | Loss: 0.00001109
Iteration 85/1000 | Loss: 0.00001108
Iteration 86/1000 | Loss: 0.00001108
Iteration 87/1000 | Loss: 0.00001107
Iteration 88/1000 | Loss: 0.00001107
Iteration 89/1000 | Loss: 0.00001107
Iteration 90/1000 | Loss: 0.00001107
Iteration 91/1000 | Loss: 0.00001107
Iteration 92/1000 | Loss: 0.00001107
Iteration 93/1000 | Loss: 0.00001106
Iteration 94/1000 | Loss: 0.00001106
Iteration 95/1000 | Loss: 0.00001106
Iteration 96/1000 | Loss: 0.00001106
Iteration 97/1000 | Loss: 0.00001106
Iteration 98/1000 | Loss: 0.00001106
Iteration 99/1000 | Loss: 0.00001106
Iteration 100/1000 | Loss: 0.00001106
Iteration 101/1000 | Loss: 0.00001105
Iteration 102/1000 | Loss: 0.00001105
Iteration 103/1000 | Loss: 0.00001104
Iteration 104/1000 | Loss: 0.00001104
Iteration 105/1000 | Loss: 0.00001104
Iteration 106/1000 | Loss: 0.00001104
Iteration 107/1000 | Loss: 0.00001104
Iteration 108/1000 | Loss: 0.00001104
Iteration 109/1000 | Loss: 0.00001104
Iteration 110/1000 | Loss: 0.00001103
Iteration 111/1000 | Loss: 0.00001103
Iteration 112/1000 | Loss: 0.00001103
Iteration 113/1000 | Loss: 0.00001103
Iteration 114/1000 | Loss: 0.00001103
Iteration 115/1000 | Loss: 0.00001103
Iteration 116/1000 | Loss: 0.00001103
Iteration 117/1000 | Loss: 0.00001102
Iteration 118/1000 | Loss: 0.00001102
Iteration 119/1000 | Loss: 0.00001102
Iteration 120/1000 | Loss: 0.00001102
Iteration 121/1000 | Loss: 0.00001101
Iteration 122/1000 | Loss: 0.00001101
Iteration 123/1000 | Loss: 0.00001100
Iteration 124/1000 | Loss: 0.00001100
Iteration 125/1000 | Loss: 0.00001100
Iteration 126/1000 | Loss: 0.00001100
Iteration 127/1000 | Loss: 0.00001100
Iteration 128/1000 | Loss: 0.00001100
Iteration 129/1000 | Loss: 0.00001100
Iteration 130/1000 | Loss: 0.00001099
Iteration 131/1000 | Loss: 0.00001099
Iteration 132/1000 | Loss: 0.00001099
Iteration 133/1000 | Loss: 0.00001099
Iteration 134/1000 | Loss: 0.00001098
Iteration 135/1000 | Loss: 0.00001098
Iteration 136/1000 | Loss: 0.00001098
Iteration 137/1000 | Loss: 0.00001098
Iteration 138/1000 | Loss: 0.00001097
Iteration 139/1000 | Loss: 0.00001097
Iteration 140/1000 | Loss: 0.00001097
Iteration 141/1000 | Loss: 0.00001097
Iteration 142/1000 | Loss: 0.00001097
Iteration 143/1000 | Loss: 0.00001097
Iteration 144/1000 | Loss: 0.00001097
Iteration 145/1000 | Loss: 0.00001097
Iteration 146/1000 | Loss: 0.00001096
Iteration 147/1000 | Loss: 0.00001096
Iteration 148/1000 | Loss: 0.00001096
Iteration 149/1000 | Loss: 0.00001095
Iteration 150/1000 | Loss: 0.00001095
Iteration 151/1000 | Loss: 0.00001095
Iteration 152/1000 | Loss: 0.00001095
Iteration 153/1000 | Loss: 0.00001095
Iteration 154/1000 | Loss: 0.00001095
Iteration 155/1000 | Loss: 0.00001095
Iteration 156/1000 | Loss: 0.00001095
Iteration 157/1000 | Loss: 0.00001095
Iteration 158/1000 | Loss: 0.00001095
Iteration 159/1000 | Loss: 0.00001095
Iteration 160/1000 | Loss: 0.00001094
Iteration 161/1000 | Loss: 0.00001094
Iteration 162/1000 | Loss: 0.00001094
Iteration 163/1000 | Loss: 0.00001093
Iteration 164/1000 | Loss: 0.00001093
Iteration 165/1000 | Loss: 0.00001093
Iteration 166/1000 | Loss: 0.00001093
Iteration 167/1000 | Loss: 0.00001093
Iteration 168/1000 | Loss: 0.00001093
Iteration 169/1000 | Loss: 0.00001093
Iteration 170/1000 | Loss: 0.00001093
Iteration 171/1000 | Loss: 0.00001093
Iteration 172/1000 | Loss: 0.00001093
Iteration 173/1000 | Loss: 0.00001093
Iteration 174/1000 | Loss: 0.00001093
Iteration 175/1000 | Loss: 0.00001093
Iteration 176/1000 | Loss: 0.00001092
Iteration 177/1000 | Loss: 0.00001092
Iteration 178/1000 | Loss: 0.00001092
Iteration 179/1000 | Loss: 0.00001091
Iteration 180/1000 | Loss: 0.00001091
Iteration 181/1000 | Loss: 0.00001091
Iteration 182/1000 | Loss: 0.00001091
Iteration 183/1000 | Loss: 0.00001091
Iteration 184/1000 | Loss: 0.00001091
Iteration 185/1000 | Loss: 0.00001091
Iteration 186/1000 | Loss: 0.00001091
Iteration 187/1000 | Loss: 0.00001091
Iteration 188/1000 | Loss: 0.00001091
Iteration 189/1000 | Loss: 0.00001091
Iteration 190/1000 | Loss: 0.00001091
Iteration 191/1000 | Loss: 0.00001091
Iteration 192/1000 | Loss: 0.00001091
Iteration 193/1000 | Loss: 0.00001091
Iteration 194/1000 | Loss: 0.00001091
Iteration 195/1000 | Loss: 0.00001090
Iteration 196/1000 | Loss: 0.00001090
Iteration 197/1000 | Loss: 0.00001090
Iteration 198/1000 | Loss: 0.00001090
Iteration 199/1000 | Loss: 0.00001090
Iteration 200/1000 | Loss: 0.00001090
Iteration 201/1000 | Loss: 0.00001090
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 201. Stopping optimization.
Last 5 losses: [1.090465684683295e-05, 1.090465684683295e-05, 1.090465684683295e-05, 1.090465684683295e-05, 1.090465684683295e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.090465684683295e-05

Optimization complete. Final v2v error: 2.8243112564086914 mm

Highest mean error: 3.1078712940216064 mm for frame 211

Lowest mean error: 2.548997402191162 mm for frame 126

Saving results

Total time: 44.4697208404541
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_022/1047/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_022/1047.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_022/1047
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00736602
Iteration 2/25 | Loss: 0.00174084
Iteration 3/25 | Loss: 0.00128432
Iteration 4/25 | Loss: 0.00122817
Iteration 5/25 | Loss: 0.00120870
Iteration 6/25 | Loss: 0.00119251
Iteration 7/25 | Loss: 0.00118460
Iteration 8/25 | Loss: 0.00118358
Iteration 9/25 | Loss: 0.00118335
Iteration 10/25 | Loss: 0.00118328
Iteration 11/25 | Loss: 0.00118327
Iteration 12/25 | Loss: 0.00118327
Iteration 13/25 | Loss: 0.00118327
Iteration 14/25 | Loss: 0.00118327
Iteration 15/25 | Loss: 0.00118327
Iteration 16/25 | Loss: 0.00118327
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0011832687305286527, 0.0011832687305286527, 0.0011832687305286527, 0.0011832687305286527, 0.0011832687305286527]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011832687305286527

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.05984855
Iteration 2/25 | Loss: 0.00054613
Iteration 3/25 | Loss: 0.00054608
Iteration 4/25 | Loss: 0.00054608
Iteration 5/25 | Loss: 0.00054608
Iteration 6/25 | Loss: 0.00054608
Iteration 7/25 | Loss: 0.00054608
Iteration 8/25 | Loss: 0.00054608
Iteration 9/25 | Loss: 0.00054608
Iteration 10/25 | Loss: 0.00054608
Iteration 11/25 | Loss: 0.00054608
Iteration 12/25 | Loss: 0.00054608
Iteration 13/25 | Loss: 0.00054608
Iteration 14/25 | Loss: 0.00054608
Iteration 15/25 | Loss: 0.00054608
Iteration 16/25 | Loss: 0.00054608
Iteration 17/25 | Loss: 0.00054608
Iteration 18/25 | Loss: 0.00054608
Iteration 19/25 | Loss: 0.00054608
Iteration 20/25 | Loss: 0.00054608
Iteration 21/25 | Loss: 0.00054608
Iteration 22/25 | Loss: 0.00054608
Iteration 23/25 | Loss: 0.00054608
Iteration 24/25 | Loss: 0.00054608
Iteration 25/25 | Loss: 0.00054608

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00054608
Iteration 2/1000 | Loss: 0.00003739
Iteration 3/1000 | Loss: 0.00002327
Iteration 4/1000 | Loss: 0.00002017
Iteration 5/1000 | Loss: 0.00001905
Iteration 6/1000 | Loss: 0.00001822
Iteration 7/1000 | Loss: 0.00001773
Iteration 8/1000 | Loss: 0.00001750
Iteration 9/1000 | Loss: 0.00001742
Iteration 10/1000 | Loss: 0.00001726
Iteration 11/1000 | Loss: 0.00001720
Iteration 12/1000 | Loss: 0.00001718
Iteration 13/1000 | Loss: 0.00001715
Iteration 14/1000 | Loss: 0.00001714
Iteration 15/1000 | Loss: 0.00001714
Iteration 16/1000 | Loss: 0.00001714
Iteration 17/1000 | Loss: 0.00001713
Iteration 18/1000 | Loss: 0.00001711
Iteration 19/1000 | Loss: 0.00001710
Iteration 20/1000 | Loss: 0.00001710
Iteration 21/1000 | Loss: 0.00001709
Iteration 22/1000 | Loss: 0.00001709
Iteration 23/1000 | Loss: 0.00001708
Iteration 24/1000 | Loss: 0.00001708
Iteration 25/1000 | Loss: 0.00001705
Iteration 26/1000 | Loss: 0.00001702
Iteration 27/1000 | Loss: 0.00001701
Iteration 28/1000 | Loss: 0.00001701
Iteration 29/1000 | Loss: 0.00001701
Iteration 30/1000 | Loss: 0.00001700
Iteration 31/1000 | Loss: 0.00001699
Iteration 32/1000 | Loss: 0.00001696
Iteration 33/1000 | Loss: 0.00001695
Iteration 34/1000 | Loss: 0.00001695
Iteration 35/1000 | Loss: 0.00001695
Iteration 36/1000 | Loss: 0.00001694
Iteration 37/1000 | Loss: 0.00001693
Iteration 38/1000 | Loss: 0.00001692
Iteration 39/1000 | Loss: 0.00001692
Iteration 40/1000 | Loss: 0.00001692
Iteration 41/1000 | Loss: 0.00001691
Iteration 42/1000 | Loss: 0.00001691
Iteration 43/1000 | Loss: 0.00001691
Iteration 44/1000 | Loss: 0.00001691
Iteration 45/1000 | Loss: 0.00001690
Iteration 46/1000 | Loss: 0.00001688
Iteration 47/1000 | Loss: 0.00001686
Iteration 48/1000 | Loss: 0.00001684
Iteration 49/1000 | Loss: 0.00001683
Iteration 50/1000 | Loss: 0.00001683
Iteration 51/1000 | Loss: 0.00001683
Iteration 52/1000 | Loss: 0.00001683
Iteration 53/1000 | Loss: 0.00001683
Iteration 54/1000 | Loss: 0.00001682
Iteration 55/1000 | Loss: 0.00001682
Iteration 56/1000 | Loss: 0.00001682
Iteration 57/1000 | Loss: 0.00001682
Iteration 58/1000 | Loss: 0.00001681
Iteration 59/1000 | Loss: 0.00001681
Iteration 60/1000 | Loss: 0.00001681
Iteration 61/1000 | Loss: 0.00001681
Iteration 62/1000 | Loss: 0.00001680
Iteration 63/1000 | Loss: 0.00001680
Iteration 64/1000 | Loss: 0.00001680
Iteration 65/1000 | Loss: 0.00001680
Iteration 66/1000 | Loss: 0.00001680
Iteration 67/1000 | Loss: 0.00001680
Iteration 68/1000 | Loss: 0.00001680
Iteration 69/1000 | Loss: 0.00001680
Iteration 70/1000 | Loss: 0.00001679
Iteration 71/1000 | Loss: 0.00001679
Iteration 72/1000 | Loss: 0.00001679
Iteration 73/1000 | Loss: 0.00001679
Iteration 74/1000 | Loss: 0.00001678
Iteration 75/1000 | Loss: 0.00001678
Iteration 76/1000 | Loss: 0.00001678
Iteration 77/1000 | Loss: 0.00001678
Iteration 78/1000 | Loss: 0.00001678
Iteration 79/1000 | Loss: 0.00001678
Iteration 80/1000 | Loss: 0.00001678
Iteration 81/1000 | Loss: 0.00001678
Iteration 82/1000 | Loss: 0.00001678
Iteration 83/1000 | Loss: 0.00001678
Iteration 84/1000 | Loss: 0.00001678
Iteration 85/1000 | Loss: 0.00001678
Iteration 86/1000 | Loss: 0.00001678
Iteration 87/1000 | Loss: 0.00001677
Iteration 88/1000 | Loss: 0.00001677
Iteration 89/1000 | Loss: 0.00001677
Iteration 90/1000 | Loss: 0.00001677
Iteration 91/1000 | Loss: 0.00001677
Iteration 92/1000 | Loss: 0.00001677
Iteration 93/1000 | Loss: 0.00001677
Iteration 94/1000 | Loss: 0.00001677
Iteration 95/1000 | Loss: 0.00001677
Iteration 96/1000 | Loss: 0.00001677
Iteration 97/1000 | Loss: 0.00001677
Iteration 98/1000 | Loss: 0.00001677
Iteration 99/1000 | Loss: 0.00001677
Iteration 100/1000 | Loss: 0.00001677
Iteration 101/1000 | Loss: 0.00001676
Iteration 102/1000 | Loss: 0.00001676
Iteration 103/1000 | Loss: 0.00001676
Iteration 104/1000 | Loss: 0.00001676
Iteration 105/1000 | Loss: 0.00001676
Iteration 106/1000 | Loss: 0.00001676
Iteration 107/1000 | Loss: 0.00001675
Iteration 108/1000 | Loss: 0.00001675
Iteration 109/1000 | Loss: 0.00001675
Iteration 110/1000 | Loss: 0.00001675
Iteration 111/1000 | Loss: 0.00001675
Iteration 112/1000 | Loss: 0.00001675
Iteration 113/1000 | Loss: 0.00001675
Iteration 114/1000 | Loss: 0.00001675
Iteration 115/1000 | Loss: 0.00001675
Iteration 116/1000 | Loss: 0.00001675
Iteration 117/1000 | Loss: 0.00001675
Iteration 118/1000 | Loss: 0.00001675
Iteration 119/1000 | Loss: 0.00001675
Iteration 120/1000 | Loss: 0.00001675
Iteration 121/1000 | Loss: 0.00001674
Iteration 122/1000 | Loss: 0.00001674
Iteration 123/1000 | Loss: 0.00001674
Iteration 124/1000 | Loss: 0.00001674
Iteration 125/1000 | Loss: 0.00001674
Iteration 126/1000 | Loss: 0.00001674
Iteration 127/1000 | Loss: 0.00001674
Iteration 128/1000 | Loss: 0.00001674
Iteration 129/1000 | Loss: 0.00001674
Iteration 130/1000 | Loss: 0.00001674
Iteration 131/1000 | Loss: 0.00001674
Iteration 132/1000 | Loss: 0.00001674
Iteration 133/1000 | Loss: 0.00001674
Iteration 134/1000 | Loss: 0.00001674
Iteration 135/1000 | Loss: 0.00001674
Iteration 136/1000 | Loss: 0.00001674
Iteration 137/1000 | Loss: 0.00001674
Iteration 138/1000 | Loss: 0.00001674
Iteration 139/1000 | Loss: 0.00001674
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 139. Stopping optimization.
Last 5 losses: [1.6736838006181642e-05, 1.6736838006181642e-05, 1.6736838006181642e-05, 1.6736838006181642e-05, 1.6736838006181642e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6736838006181642e-05

Optimization complete. Final v2v error: 3.457822322845459 mm

Highest mean error: 3.862816095352173 mm for frame 112

Lowest mean error: 3.153141736984253 mm for frame 226

Saving results

Total time: 47.751821994781494
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_022/1059/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_022/1059.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_022/1059
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00758767
Iteration 2/25 | Loss: 0.00132509
Iteration 3/25 | Loss: 0.00120160
Iteration 4/25 | Loss: 0.00117855
Iteration 5/25 | Loss: 0.00117036
Iteration 6/25 | Loss: 0.00116866
Iteration 7/25 | Loss: 0.00116866
Iteration 8/25 | Loss: 0.00116866
Iteration 9/25 | Loss: 0.00116866
Iteration 10/25 | Loss: 0.00116866
Iteration 11/25 | Loss: 0.00116866
Iteration 12/25 | Loss: 0.00116866
Iteration 13/25 | Loss: 0.00116866
Iteration 14/25 | Loss: 0.00116866
Iteration 15/25 | Loss: 0.00116866
Iteration 16/25 | Loss: 0.00116866
Iteration 17/25 | Loss: 0.00116866
Iteration 18/25 | Loss: 0.00116866
Iteration 19/25 | Loss: 0.00116866
Iteration 20/25 | Loss: 0.00116866
Iteration 21/25 | Loss: 0.00116866
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0011686639627441764, 0.0011686639627441764, 0.0011686639627441764, 0.0011686639627441764, 0.0011686639627441764]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011686639627441764

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34567559
Iteration 2/25 | Loss: 0.00094350
Iteration 3/25 | Loss: 0.00094350
Iteration 4/25 | Loss: 0.00094350
Iteration 5/25 | Loss: 0.00094350
Iteration 6/25 | Loss: 0.00094350
Iteration 7/25 | Loss: 0.00094350
Iteration 8/25 | Loss: 0.00094350
Iteration 9/25 | Loss: 0.00094350
Iteration 10/25 | Loss: 0.00094350
Iteration 11/25 | Loss: 0.00094350
Iteration 12/25 | Loss: 0.00094349
Iteration 13/25 | Loss: 0.00094349
Iteration 14/25 | Loss: 0.00094349
Iteration 15/25 | Loss: 0.00094349
Iteration 16/25 | Loss: 0.00094349
Iteration 17/25 | Loss: 0.00094349
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0009434949606657028, 0.0009434949606657028, 0.0009434949606657028, 0.0009434949606657028, 0.0009434949606657028]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009434949606657028

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00094349
Iteration 2/1000 | Loss: 0.00005685
Iteration 3/1000 | Loss: 0.00003783
Iteration 4/1000 | Loss: 0.00003170
Iteration 5/1000 | Loss: 0.00002952
Iteration 6/1000 | Loss: 0.00002809
Iteration 7/1000 | Loss: 0.00002737
Iteration 8/1000 | Loss: 0.00002680
Iteration 9/1000 | Loss: 0.00002633
Iteration 10/1000 | Loss: 0.00002606
Iteration 11/1000 | Loss: 0.00002581
Iteration 12/1000 | Loss: 0.00002575
Iteration 13/1000 | Loss: 0.00002559
Iteration 14/1000 | Loss: 0.00002558
Iteration 15/1000 | Loss: 0.00002553
Iteration 16/1000 | Loss: 0.00002551
Iteration 17/1000 | Loss: 0.00002550
Iteration 18/1000 | Loss: 0.00002546
Iteration 19/1000 | Loss: 0.00002544
Iteration 20/1000 | Loss: 0.00002538
Iteration 21/1000 | Loss: 0.00002531
Iteration 22/1000 | Loss: 0.00002523
Iteration 23/1000 | Loss: 0.00002519
Iteration 24/1000 | Loss: 0.00002518
Iteration 25/1000 | Loss: 0.00002517
Iteration 26/1000 | Loss: 0.00002516
Iteration 27/1000 | Loss: 0.00002516
Iteration 28/1000 | Loss: 0.00002515
Iteration 29/1000 | Loss: 0.00002515
Iteration 30/1000 | Loss: 0.00002514
Iteration 31/1000 | Loss: 0.00002514
Iteration 32/1000 | Loss: 0.00002513
Iteration 33/1000 | Loss: 0.00002513
Iteration 34/1000 | Loss: 0.00002512
Iteration 35/1000 | Loss: 0.00002512
Iteration 36/1000 | Loss: 0.00002511
Iteration 37/1000 | Loss: 0.00002510
Iteration 38/1000 | Loss: 0.00002510
Iteration 39/1000 | Loss: 0.00002508
Iteration 40/1000 | Loss: 0.00002508
Iteration 41/1000 | Loss: 0.00002508
Iteration 42/1000 | Loss: 0.00002508
Iteration 43/1000 | Loss: 0.00002507
Iteration 44/1000 | Loss: 0.00002507
Iteration 45/1000 | Loss: 0.00002507
Iteration 46/1000 | Loss: 0.00002506
Iteration 47/1000 | Loss: 0.00002506
Iteration 48/1000 | Loss: 0.00002505
Iteration 49/1000 | Loss: 0.00002505
Iteration 50/1000 | Loss: 0.00002505
Iteration 51/1000 | Loss: 0.00002505
Iteration 52/1000 | Loss: 0.00002505
Iteration 53/1000 | Loss: 0.00002505
Iteration 54/1000 | Loss: 0.00002504
Iteration 55/1000 | Loss: 0.00002504
Iteration 56/1000 | Loss: 0.00002504
Iteration 57/1000 | Loss: 0.00002504
Iteration 58/1000 | Loss: 0.00002503
Iteration 59/1000 | Loss: 0.00002503
Iteration 60/1000 | Loss: 0.00002503
Iteration 61/1000 | Loss: 0.00002502
Iteration 62/1000 | Loss: 0.00002502
Iteration 63/1000 | Loss: 0.00002502
Iteration 64/1000 | Loss: 0.00002502
Iteration 65/1000 | Loss: 0.00002501
Iteration 66/1000 | Loss: 0.00002501
Iteration 67/1000 | Loss: 0.00002501
Iteration 68/1000 | Loss: 0.00002500
Iteration 69/1000 | Loss: 0.00002500
Iteration 70/1000 | Loss: 0.00002500
Iteration 71/1000 | Loss: 0.00002499
Iteration 72/1000 | Loss: 0.00002499
Iteration 73/1000 | Loss: 0.00002499
Iteration 74/1000 | Loss: 0.00002499
Iteration 75/1000 | Loss: 0.00002499
Iteration 76/1000 | Loss: 0.00002499
Iteration 77/1000 | Loss: 0.00002499
Iteration 78/1000 | Loss: 0.00002499
Iteration 79/1000 | Loss: 0.00002498
Iteration 80/1000 | Loss: 0.00002498
Iteration 81/1000 | Loss: 0.00002498
Iteration 82/1000 | Loss: 0.00002498
Iteration 83/1000 | Loss: 0.00002498
Iteration 84/1000 | Loss: 0.00002498
Iteration 85/1000 | Loss: 0.00002498
Iteration 86/1000 | Loss: 0.00002498
Iteration 87/1000 | Loss: 0.00002498
Iteration 88/1000 | Loss: 0.00002498
Iteration 89/1000 | Loss: 0.00002498
Iteration 90/1000 | Loss: 0.00002498
Iteration 91/1000 | Loss: 0.00002498
Iteration 92/1000 | Loss: 0.00002498
Iteration 93/1000 | Loss: 0.00002498
Iteration 94/1000 | Loss: 0.00002498
Iteration 95/1000 | Loss: 0.00002498
Iteration 96/1000 | Loss: 0.00002498
Iteration 97/1000 | Loss: 0.00002498
Iteration 98/1000 | Loss: 0.00002498
Iteration 99/1000 | Loss: 0.00002498
Iteration 100/1000 | Loss: 0.00002498
Iteration 101/1000 | Loss: 0.00002498
Iteration 102/1000 | Loss: 0.00002498
Iteration 103/1000 | Loss: 0.00002498
Iteration 104/1000 | Loss: 0.00002498
Iteration 105/1000 | Loss: 0.00002498
Iteration 106/1000 | Loss: 0.00002498
Iteration 107/1000 | Loss: 0.00002498
Iteration 108/1000 | Loss: 0.00002498
Iteration 109/1000 | Loss: 0.00002498
Iteration 110/1000 | Loss: 0.00002498
Iteration 111/1000 | Loss: 0.00002498
Iteration 112/1000 | Loss: 0.00002498
Iteration 113/1000 | Loss: 0.00002498
Iteration 114/1000 | Loss: 0.00002498
Iteration 115/1000 | Loss: 0.00002498
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 115. Stopping optimization.
Last 5 losses: [2.498100911907386e-05, 2.498100911907386e-05, 2.498100911907386e-05, 2.498100911907386e-05, 2.498100911907386e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.498100911907386e-05

Optimization complete. Final v2v error: 4.140235424041748 mm

Highest mean error: 5.179214954376221 mm for frame 21

Lowest mean error: 3.2251687049865723 mm for frame 130

Saving results

Total time: 37.67006039619446
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_022/1012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_022/1012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_022/1012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01036799
Iteration 2/25 | Loss: 0.00143913
Iteration 3/25 | Loss: 0.00117462
Iteration 4/25 | Loss: 0.00113734
Iteration 5/25 | Loss: 0.00115558
Iteration 6/25 | Loss: 0.00113619
Iteration 7/25 | Loss: 0.00112776
Iteration 8/25 | Loss: 0.00112551
Iteration 9/25 | Loss: 0.00111720
Iteration 10/25 | Loss: 0.00111401
Iteration 11/25 | Loss: 0.00111085
Iteration 12/25 | Loss: 0.00111001
Iteration 13/25 | Loss: 0.00110970
Iteration 14/25 | Loss: 0.00110247
Iteration 15/25 | Loss: 0.00110032
Iteration 16/25 | Loss: 0.00109993
Iteration 17/25 | Loss: 0.00109370
Iteration 18/25 | Loss: 0.00109199
Iteration 19/25 | Loss: 0.00109153
Iteration 20/25 | Loss: 0.00109137
Iteration 21/25 | Loss: 0.00109135
Iteration 22/25 | Loss: 0.00109134
Iteration 23/25 | Loss: 0.00109133
Iteration 24/25 | Loss: 0.00109132
Iteration 25/25 | Loss: 0.00109132

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.38939118
Iteration 2/25 | Loss: 0.00082621
Iteration 3/25 | Loss: 0.00082620
Iteration 4/25 | Loss: 0.00082620
Iteration 5/25 | Loss: 0.00082620
Iteration 6/25 | Loss: 0.00082620
Iteration 7/25 | Loss: 0.00082620
Iteration 8/25 | Loss: 0.00082620
Iteration 9/25 | Loss: 0.00082620
Iteration 10/25 | Loss: 0.00082620
Iteration 11/25 | Loss: 0.00082620
Iteration 12/25 | Loss: 0.00082620
Iteration 13/25 | Loss: 0.00082620
Iteration 14/25 | Loss: 0.00082620
Iteration 15/25 | Loss: 0.00082620
Iteration 16/25 | Loss: 0.00082620
Iteration 17/25 | Loss: 0.00082620
Iteration 18/25 | Loss: 0.00082620
Iteration 19/25 | Loss: 0.00082620
Iteration 20/25 | Loss: 0.00082620
Iteration 21/25 | Loss: 0.00082620
Iteration 22/25 | Loss: 0.00082620
Iteration 23/25 | Loss: 0.00082620
Iteration 24/25 | Loss: 0.00082620
Iteration 25/25 | Loss: 0.00082620

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00082620
Iteration 2/1000 | Loss: 0.00002098
Iteration 3/1000 | Loss: 0.00001478
Iteration 4/1000 | Loss: 0.00001349
Iteration 5/1000 | Loss: 0.00001266
Iteration 6/1000 | Loss: 0.00001212
Iteration 7/1000 | Loss: 0.00001176
Iteration 8/1000 | Loss: 0.00001148
Iteration 9/1000 | Loss: 0.00001119
Iteration 10/1000 | Loss: 0.00001110
Iteration 11/1000 | Loss: 0.00001092
Iteration 12/1000 | Loss: 0.00001090
Iteration 13/1000 | Loss: 0.00001085
Iteration 14/1000 | Loss: 0.00001081
Iteration 15/1000 | Loss: 0.00001080
Iteration 16/1000 | Loss: 0.00001080
Iteration 17/1000 | Loss: 0.00001079
Iteration 18/1000 | Loss: 0.00001077
Iteration 19/1000 | Loss: 0.00001077
Iteration 20/1000 | Loss: 0.00001075
Iteration 21/1000 | Loss: 0.00001075
Iteration 22/1000 | Loss: 0.00001074
Iteration 23/1000 | Loss: 0.00001074
Iteration 24/1000 | Loss: 0.00001073
Iteration 25/1000 | Loss: 0.00001073
Iteration 26/1000 | Loss: 0.00001073
Iteration 27/1000 | Loss: 0.00001072
Iteration 28/1000 | Loss: 0.00001072
Iteration 29/1000 | Loss: 0.00001071
Iteration 30/1000 | Loss: 0.00001071
Iteration 31/1000 | Loss: 0.00001071
Iteration 32/1000 | Loss: 0.00001071
Iteration 33/1000 | Loss: 0.00001071
Iteration 34/1000 | Loss: 0.00001071
Iteration 35/1000 | Loss: 0.00001070
Iteration 36/1000 | Loss: 0.00001070
Iteration 37/1000 | Loss: 0.00001070
Iteration 38/1000 | Loss: 0.00001070
Iteration 39/1000 | Loss: 0.00001069
Iteration 40/1000 | Loss: 0.00001069
Iteration 41/1000 | Loss: 0.00001069
Iteration 42/1000 | Loss: 0.00001069
Iteration 43/1000 | Loss: 0.00001068
Iteration 44/1000 | Loss: 0.00001068
Iteration 45/1000 | Loss: 0.00001066
Iteration 46/1000 | Loss: 0.00001066
Iteration 47/1000 | Loss: 0.00001066
Iteration 48/1000 | Loss: 0.00001066
Iteration 49/1000 | Loss: 0.00001066
Iteration 50/1000 | Loss: 0.00001066
Iteration 51/1000 | Loss: 0.00001066
Iteration 52/1000 | Loss: 0.00001066
Iteration 53/1000 | Loss: 0.00001066
Iteration 54/1000 | Loss: 0.00001066
Iteration 55/1000 | Loss: 0.00001066
Iteration 56/1000 | Loss: 0.00001065
Iteration 57/1000 | Loss: 0.00001064
Iteration 58/1000 | Loss: 0.00001064
Iteration 59/1000 | Loss: 0.00001064
Iteration 60/1000 | Loss: 0.00001063
Iteration 61/1000 | Loss: 0.00001063
Iteration 62/1000 | Loss: 0.00001063
Iteration 63/1000 | Loss: 0.00001063
Iteration 64/1000 | Loss: 0.00001063
Iteration 65/1000 | Loss: 0.00001063
Iteration 66/1000 | Loss: 0.00001063
Iteration 67/1000 | Loss: 0.00001063
Iteration 68/1000 | Loss: 0.00001063
Iteration 69/1000 | Loss: 0.00001063
Iteration 70/1000 | Loss: 0.00001063
Iteration 71/1000 | Loss: 0.00001063
Iteration 72/1000 | Loss: 0.00001063
Iteration 73/1000 | Loss: 0.00001063
Iteration 74/1000 | Loss: 0.00001063
Iteration 75/1000 | Loss: 0.00001063
Iteration 76/1000 | Loss: 0.00001063
Iteration 77/1000 | Loss: 0.00001063
Iteration 78/1000 | Loss: 0.00001063
Iteration 79/1000 | Loss: 0.00001063
Iteration 80/1000 | Loss: 0.00001063
Iteration 81/1000 | Loss: 0.00001063
Iteration 82/1000 | Loss: 0.00001063
Iteration 83/1000 | Loss: 0.00001063
Iteration 84/1000 | Loss: 0.00001063
Iteration 85/1000 | Loss: 0.00001063
Iteration 86/1000 | Loss: 0.00001063
Iteration 87/1000 | Loss: 0.00001063
Iteration 88/1000 | Loss: 0.00001063
Iteration 89/1000 | Loss: 0.00001063
Iteration 90/1000 | Loss: 0.00001063
Iteration 91/1000 | Loss: 0.00001063
Iteration 92/1000 | Loss: 0.00001063
Iteration 93/1000 | Loss: 0.00001063
Iteration 94/1000 | Loss: 0.00001063
Iteration 95/1000 | Loss: 0.00001063
Iteration 96/1000 | Loss: 0.00001063
Iteration 97/1000 | Loss: 0.00001063
Iteration 98/1000 | Loss: 0.00001063
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 98. Stopping optimization.
Last 5 losses: [1.0632467819959857e-05, 1.0632467819959857e-05, 1.0632467819959857e-05, 1.0632467819959857e-05, 1.0632467819959857e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0632467819959857e-05

Optimization complete. Final v2v error: 2.7906572818756104 mm

Highest mean error: 3.07877779006958 mm for frame 64

Lowest mean error: 2.5813183784484863 mm for frame 208

Saving results

Total time: 67.537344455719
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_022/1064/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_022/1064.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_022/1064
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00970194
Iteration 2/25 | Loss: 0.00970194
Iteration 3/25 | Loss: 0.00970194
Iteration 4/25 | Loss: 0.00348274
Iteration 5/25 | Loss: 0.00229786
Iteration 6/25 | Loss: 0.00224142
Iteration 7/25 | Loss: 0.00172096
Iteration 8/25 | Loss: 0.00161631
Iteration 9/25 | Loss: 0.00153698
Iteration 10/25 | Loss: 0.00146042
Iteration 11/25 | Loss: 0.00151144
Iteration 12/25 | Loss: 0.00139907
Iteration 13/25 | Loss: 0.00135933
Iteration 14/25 | Loss: 0.00135693
Iteration 15/25 | Loss: 0.00130738
Iteration 16/25 | Loss: 0.00130025
Iteration 17/25 | Loss: 0.00128670
Iteration 18/25 | Loss: 0.00127509
Iteration 19/25 | Loss: 0.00127474
Iteration 20/25 | Loss: 0.00127620
Iteration 21/25 | Loss: 0.00127335
Iteration 22/25 | Loss: 0.00127086
Iteration 23/25 | Loss: 0.00126823
Iteration 24/25 | Loss: 0.00126789
Iteration 25/25 | Loss: 0.00126782

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33983469
Iteration 2/25 | Loss: 0.00150268
Iteration 3/25 | Loss: 0.00115150
Iteration 4/25 | Loss: 0.00115150
Iteration 5/25 | Loss: 0.00115150
Iteration 6/25 | Loss: 0.00115150
Iteration 7/25 | Loss: 0.00115150
Iteration 8/25 | Loss: 0.00115150
Iteration 9/25 | Loss: 0.00115150
Iteration 10/25 | Loss: 0.00115150
Iteration 11/25 | Loss: 0.00115150
Iteration 12/25 | Loss: 0.00115150
Iteration 13/25 | Loss: 0.00115150
Iteration 14/25 | Loss: 0.00115150
Iteration 15/25 | Loss: 0.00115150
Iteration 16/25 | Loss: 0.00115150
Iteration 17/25 | Loss: 0.00115150
Iteration 18/25 | Loss: 0.00115150
Iteration 19/25 | Loss: 0.00115150
Iteration 20/25 | Loss: 0.00115150
Iteration 21/25 | Loss: 0.00115150
Iteration 22/25 | Loss: 0.00115150
Iteration 23/25 | Loss: 0.00115150
Iteration 24/25 | Loss: 0.00115150
Iteration 25/25 | Loss: 0.00115150

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00115150
Iteration 2/1000 | Loss: 0.00108359
Iteration 3/1000 | Loss: 0.00076203
Iteration 4/1000 | Loss: 0.00118071
Iteration 5/1000 | Loss: 0.00080768
Iteration 6/1000 | Loss: 0.00262365
Iteration 7/1000 | Loss: 0.00013819
Iteration 8/1000 | Loss: 0.00009515
Iteration 9/1000 | Loss: 0.00054436
Iteration 10/1000 | Loss: 0.00014871
Iteration 11/1000 | Loss: 0.00014733
Iteration 12/1000 | Loss: 0.00015799
Iteration 13/1000 | Loss: 0.00008651
Iteration 14/1000 | Loss: 0.00074395
Iteration 15/1000 | Loss: 0.00074333
Iteration 16/1000 | Loss: 0.00051378
Iteration 17/1000 | Loss: 0.00028265
Iteration 18/1000 | Loss: 0.00040821
Iteration 19/1000 | Loss: 0.00012053
Iteration 20/1000 | Loss: 0.00013832
Iteration 21/1000 | Loss: 0.00013690
Iteration 22/1000 | Loss: 0.00013715
Iteration 23/1000 | Loss: 0.00012211
Iteration 24/1000 | Loss: 0.00031458
Iteration 25/1000 | Loss: 0.00026580
Iteration 26/1000 | Loss: 0.00014535
Iteration 27/1000 | Loss: 0.00017161
Iteration 28/1000 | Loss: 0.00026995
Iteration 29/1000 | Loss: 0.00072978
Iteration 30/1000 | Loss: 0.00099011
Iteration 31/1000 | Loss: 0.00218476
Iteration 32/1000 | Loss: 0.00267413
Iteration 33/1000 | Loss: 0.00055940
Iteration 34/1000 | Loss: 0.00036347
Iteration 35/1000 | Loss: 0.00009662
Iteration 36/1000 | Loss: 0.00013619
Iteration 37/1000 | Loss: 0.00005806
Iteration 38/1000 | Loss: 0.00024054
Iteration 39/1000 | Loss: 0.00003975
Iteration 40/1000 | Loss: 0.00003203
Iteration 41/1000 | Loss: 0.00003761
Iteration 42/1000 | Loss: 0.00002657
Iteration 43/1000 | Loss: 0.00023147
Iteration 44/1000 | Loss: 0.00017969
Iteration 45/1000 | Loss: 0.00002045
Iteration 46/1000 | Loss: 0.00001902
Iteration 47/1000 | Loss: 0.00020439
Iteration 48/1000 | Loss: 0.00001711
Iteration 49/1000 | Loss: 0.00001603
Iteration 50/1000 | Loss: 0.00015789
Iteration 51/1000 | Loss: 0.00001537
Iteration 52/1000 | Loss: 0.00001495
Iteration 53/1000 | Loss: 0.00001465
Iteration 54/1000 | Loss: 0.00001441
Iteration 55/1000 | Loss: 0.00001422
Iteration 56/1000 | Loss: 0.00001420
Iteration 57/1000 | Loss: 0.00001420
Iteration 58/1000 | Loss: 0.00001415
Iteration 59/1000 | Loss: 0.00015017
Iteration 60/1000 | Loss: 0.00001537
Iteration 61/1000 | Loss: 0.00001407
Iteration 62/1000 | Loss: 0.00001391
Iteration 63/1000 | Loss: 0.00001390
Iteration 64/1000 | Loss: 0.00001390
Iteration 65/1000 | Loss: 0.00001390
Iteration 66/1000 | Loss: 0.00001390
Iteration 67/1000 | Loss: 0.00001390
Iteration 68/1000 | Loss: 0.00001390
Iteration 69/1000 | Loss: 0.00001390
Iteration 70/1000 | Loss: 0.00001390
Iteration 71/1000 | Loss: 0.00001389
Iteration 72/1000 | Loss: 0.00001389
Iteration 73/1000 | Loss: 0.00001389
Iteration 74/1000 | Loss: 0.00001389
Iteration 75/1000 | Loss: 0.00001389
Iteration 76/1000 | Loss: 0.00001389
Iteration 77/1000 | Loss: 0.00001387
Iteration 78/1000 | Loss: 0.00001387
Iteration 79/1000 | Loss: 0.00001387
Iteration 80/1000 | Loss: 0.00001387
Iteration 81/1000 | Loss: 0.00001386
Iteration 82/1000 | Loss: 0.00001386
Iteration 83/1000 | Loss: 0.00001386
Iteration 84/1000 | Loss: 0.00001385
Iteration 85/1000 | Loss: 0.00001385
Iteration 86/1000 | Loss: 0.00001385
Iteration 87/1000 | Loss: 0.00001385
Iteration 88/1000 | Loss: 0.00001385
Iteration 89/1000 | Loss: 0.00001385
Iteration 90/1000 | Loss: 0.00001385
Iteration 91/1000 | Loss: 0.00001385
Iteration 92/1000 | Loss: 0.00001384
Iteration 93/1000 | Loss: 0.00001384
Iteration 94/1000 | Loss: 0.00001384
Iteration 95/1000 | Loss: 0.00001384
Iteration 96/1000 | Loss: 0.00001384
Iteration 97/1000 | Loss: 0.00001383
Iteration 98/1000 | Loss: 0.00001383
Iteration 99/1000 | Loss: 0.00001383
Iteration 100/1000 | Loss: 0.00001383
Iteration 101/1000 | Loss: 0.00001383
Iteration 102/1000 | Loss: 0.00001383
Iteration 103/1000 | Loss: 0.00001383
Iteration 104/1000 | Loss: 0.00001383
Iteration 105/1000 | Loss: 0.00001383
Iteration 106/1000 | Loss: 0.00001382
Iteration 107/1000 | Loss: 0.00001382
Iteration 108/1000 | Loss: 0.00001382
Iteration 109/1000 | Loss: 0.00001382
Iteration 110/1000 | Loss: 0.00001381
Iteration 111/1000 | Loss: 0.00001381
Iteration 112/1000 | Loss: 0.00001381
Iteration 113/1000 | Loss: 0.00001381
Iteration 114/1000 | Loss: 0.00001381
Iteration 115/1000 | Loss: 0.00001381
Iteration 116/1000 | Loss: 0.00001381
Iteration 117/1000 | Loss: 0.00001381
Iteration 118/1000 | Loss: 0.00001381
Iteration 119/1000 | Loss: 0.00001381
Iteration 120/1000 | Loss: 0.00001381
Iteration 121/1000 | Loss: 0.00001381
Iteration 122/1000 | Loss: 0.00001381
Iteration 123/1000 | Loss: 0.00001381
Iteration 124/1000 | Loss: 0.00001381
Iteration 125/1000 | Loss: 0.00001381
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 125. Stopping optimization.
Last 5 losses: [1.3809358279104345e-05, 1.3809358279104345e-05, 1.3809358279104345e-05, 1.3809358279104345e-05, 1.3809358279104345e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3809358279104345e-05

Optimization complete. Final v2v error: 3.1409878730773926 mm

Highest mean error: 4.351101875305176 mm for frame 48

Lowest mean error: 2.8614652156829834 mm for frame 133

Saving results

Total time: 146.54140162467957
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_022/1023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_022/1023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_022/1023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01018640
Iteration 2/25 | Loss: 0.00206530
Iteration 3/25 | Loss: 0.00145665
Iteration 4/25 | Loss: 0.00130754
Iteration 5/25 | Loss: 0.00125313
Iteration 6/25 | Loss: 0.00124782
Iteration 7/25 | Loss: 0.00124609
Iteration 8/25 | Loss: 0.00124061
Iteration 9/25 | Loss: 0.00124053
Iteration 10/25 | Loss: 0.00123849
Iteration 11/25 | Loss: 0.00123599
Iteration 12/25 | Loss: 0.00123499
Iteration 13/25 | Loss: 0.00123491
Iteration 14/25 | Loss: 0.00123489
Iteration 15/25 | Loss: 0.00123489
Iteration 16/25 | Loss: 0.00123489
Iteration 17/25 | Loss: 0.00123488
Iteration 18/25 | Loss: 0.00123488
Iteration 19/25 | Loss: 0.00123488
Iteration 20/25 | Loss: 0.00123488
Iteration 21/25 | Loss: 0.00123488
Iteration 22/25 | Loss: 0.00123488
Iteration 23/25 | Loss: 0.00123488
Iteration 24/25 | Loss: 0.00123488
Iteration 25/25 | Loss: 0.00123488

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.25752521
Iteration 2/25 | Loss: 0.00160751
Iteration 3/25 | Loss: 0.00160751
Iteration 4/25 | Loss: 0.00160751
Iteration 5/25 | Loss: 0.00160751
Iteration 6/25 | Loss: 0.00160751
Iteration 7/25 | Loss: 0.00160751
Iteration 8/25 | Loss: 0.00160751
Iteration 9/25 | Loss: 0.00160751
Iteration 10/25 | Loss: 0.00160750
Iteration 11/25 | Loss: 0.00160750
Iteration 12/25 | Loss: 0.00160750
Iteration 13/25 | Loss: 0.00160750
Iteration 14/25 | Loss: 0.00160750
Iteration 15/25 | Loss: 0.00160750
Iteration 16/25 | Loss: 0.00160750
Iteration 17/25 | Loss: 0.00160750
Iteration 18/25 | Loss: 0.00160750
Iteration 19/25 | Loss: 0.00160750
Iteration 20/25 | Loss: 0.00160750
Iteration 21/25 | Loss: 0.00160750
Iteration 22/25 | Loss: 0.00160750
Iteration 23/25 | Loss: 0.00160750
Iteration 24/25 | Loss: 0.00160750
Iteration 25/25 | Loss: 0.00160750

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00160750
Iteration 2/1000 | Loss: 0.00009913
Iteration 3/1000 | Loss: 0.00007551
Iteration 4/1000 | Loss: 0.00006089
Iteration 5/1000 | Loss: 0.00004359
Iteration 6/1000 | Loss: 0.00003113
Iteration 7/1000 | Loss: 0.00002708
Iteration 8/1000 | Loss: 0.00002488
Iteration 9/1000 | Loss: 0.00013613
Iteration 10/1000 | Loss: 0.00005354
Iteration 11/1000 | Loss: 0.00003782
Iteration 12/1000 | Loss: 0.00005469
Iteration 13/1000 | Loss: 0.00002950
Iteration 14/1000 | Loss: 0.00002325
Iteration 15/1000 | Loss: 0.00002173
Iteration 16/1000 | Loss: 0.00002102
Iteration 17/1000 | Loss: 0.00002032
Iteration 18/1000 | Loss: 0.00002006
Iteration 19/1000 | Loss: 0.00007203
Iteration 20/1000 | Loss: 0.00003174
Iteration 21/1000 | Loss: 0.00002191
Iteration 22/1000 | Loss: 0.00002053
Iteration 23/1000 | Loss: 0.00001998
Iteration 24/1000 | Loss: 0.00001947
Iteration 25/1000 | Loss: 0.00001923
Iteration 26/1000 | Loss: 0.00001918
Iteration 27/1000 | Loss: 0.00001918
Iteration 28/1000 | Loss: 0.00001902
Iteration 29/1000 | Loss: 0.00001897
Iteration 30/1000 | Loss: 0.00001897
Iteration 31/1000 | Loss: 0.00001895
Iteration 32/1000 | Loss: 0.00001893
Iteration 33/1000 | Loss: 0.00001893
Iteration 34/1000 | Loss: 0.00001893
Iteration 35/1000 | Loss: 0.00001893
Iteration 36/1000 | Loss: 0.00001892
Iteration 37/1000 | Loss: 0.00001892
Iteration 38/1000 | Loss: 0.00001891
Iteration 39/1000 | Loss: 0.00001891
Iteration 40/1000 | Loss: 0.00001891
Iteration 41/1000 | Loss: 0.00001890
Iteration 42/1000 | Loss: 0.00001890
Iteration 43/1000 | Loss: 0.00001889
Iteration 44/1000 | Loss: 0.00001889
Iteration 45/1000 | Loss: 0.00001888
Iteration 46/1000 | Loss: 0.00001888
Iteration 47/1000 | Loss: 0.00001887
Iteration 48/1000 | Loss: 0.00001887
Iteration 49/1000 | Loss: 0.00001886
Iteration 50/1000 | Loss: 0.00001886
Iteration 51/1000 | Loss: 0.00001886
Iteration 52/1000 | Loss: 0.00001885
Iteration 53/1000 | Loss: 0.00001885
Iteration 54/1000 | Loss: 0.00001885
Iteration 55/1000 | Loss: 0.00001885
Iteration 56/1000 | Loss: 0.00001884
Iteration 57/1000 | Loss: 0.00001884
Iteration 58/1000 | Loss: 0.00001884
Iteration 59/1000 | Loss: 0.00001882
Iteration 60/1000 | Loss: 0.00001881
Iteration 61/1000 | Loss: 0.00001879
Iteration 62/1000 | Loss: 0.00001878
Iteration 63/1000 | Loss: 0.00001878
Iteration 64/1000 | Loss: 0.00001877
Iteration 65/1000 | Loss: 0.00001877
Iteration 66/1000 | Loss: 0.00001876
Iteration 67/1000 | Loss: 0.00001876
Iteration 68/1000 | Loss: 0.00001875
Iteration 69/1000 | Loss: 0.00001875
Iteration 70/1000 | Loss: 0.00001874
Iteration 71/1000 | Loss: 0.00001874
Iteration 72/1000 | Loss: 0.00001873
Iteration 73/1000 | Loss: 0.00001872
Iteration 74/1000 | Loss: 0.00001872
Iteration 75/1000 | Loss: 0.00001872
Iteration 76/1000 | Loss: 0.00001871
Iteration 77/1000 | Loss: 0.00001871
Iteration 78/1000 | Loss: 0.00001871
Iteration 79/1000 | Loss: 0.00001870
Iteration 80/1000 | Loss: 0.00001869
Iteration 81/1000 | Loss: 0.00001869
Iteration 82/1000 | Loss: 0.00001868
Iteration 83/1000 | Loss: 0.00001868
Iteration 84/1000 | Loss: 0.00001868
Iteration 85/1000 | Loss: 0.00001868
Iteration 86/1000 | Loss: 0.00001868
Iteration 87/1000 | Loss: 0.00001867
Iteration 88/1000 | Loss: 0.00001867
Iteration 89/1000 | Loss: 0.00001867
Iteration 90/1000 | Loss: 0.00001867
Iteration 91/1000 | Loss: 0.00001867
Iteration 92/1000 | Loss: 0.00001867
Iteration 93/1000 | Loss: 0.00001867
Iteration 94/1000 | Loss: 0.00001867
Iteration 95/1000 | Loss: 0.00001867
Iteration 96/1000 | Loss: 0.00001866
Iteration 97/1000 | Loss: 0.00001865
Iteration 98/1000 | Loss: 0.00001865
Iteration 99/1000 | Loss: 0.00001865
Iteration 100/1000 | Loss: 0.00001865
Iteration 101/1000 | Loss: 0.00001864
Iteration 102/1000 | Loss: 0.00001864
Iteration 103/1000 | Loss: 0.00001864
Iteration 104/1000 | Loss: 0.00001864
Iteration 105/1000 | Loss: 0.00001864
Iteration 106/1000 | Loss: 0.00001864
Iteration 107/1000 | Loss: 0.00001864
Iteration 108/1000 | Loss: 0.00001864
Iteration 109/1000 | Loss: 0.00001863
Iteration 110/1000 | Loss: 0.00001863
Iteration 111/1000 | Loss: 0.00001863
Iteration 112/1000 | Loss: 0.00001863
Iteration 113/1000 | Loss: 0.00001861
Iteration 114/1000 | Loss: 0.00007251
Iteration 115/1000 | Loss: 0.00003202
Iteration 116/1000 | Loss: 0.00002629
Iteration 117/1000 | Loss: 0.00002068
Iteration 118/1000 | Loss: 0.00001904
Iteration 119/1000 | Loss: 0.00001862
Iteration 120/1000 | Loss: 0.00001845
Iteration 121/1000 | Loss: 0.00001841
Iteration 122/1000 | Loss: 0.00001841
Iteration 123/1000 | Loss: 0.00001841
Iteration 124/1000 | Loss: 0.00001841
Iteration 125/1000 | Loss: 0.00001841
Iteration 126/1000 | Loss: 0.00001841
Iteration 127/1000 | Loss: 0.00001841
Iteration 128/1000 | Loss: 0.00001841
Iteration 129/1000 | Loss: 0.00001840
Iteration 130/1000 | Loss: 0.00001839
Iteration 131/1000 | Loss: 0.00001839
Iteration 132/1000 | Loss: 0.00001839
Iteration 133/1000 | Loss: 0.00001839
Iteration 134/1000 | Loss: 0.00001839
Iteration 135/1000 | Loss: 0.00001839
Iteration 136/1000 | Loss: 0.00001839
Iteration 137/1000 | Loss: 0.00001839
Iteration 138/1000 | Loss: 0.00001839
Iteration 139/1000 | Loss: 0.00001838
Iteration 140/1000 | Loss: 0.00001838
Iteration 141/1000 | Loss: 0.00001836
Iteration 142/1000 | Loss: 0.00001834
Iteration 143/1000 | Loss: 0.00001833
Iteration 144/1000 | Loss: 0.00001833
Iteration 145/1000 | Loss: 0.00001832
Iteration 146/1000 | Loss: 0.00001831
Iteration 147/1000 | Loss: 0.00001830
Iteration 148/1000 | Loss: 0.00001830
Iteration 149/1000 | Loss: 0.00001830
Iteration 150/1000 | Loss: 0.00001830
Iteration 151/1000 | Loss: 0.00001829
Iteration 152/1000 | Loss: 0.00001829
Iteration 153/1000 | Loss: 0.00001826
Iteration 154/1000 | Loss: 0.00001817
Iteration 155/1000 | Loss: 0.00001816
Iteration 156/1000 | Loss: 0.00001815
Iteration 157/1000 | Loss: 0.00001815
Iteration 158/1000 | Loss: 0.00001814
Iteration 159/1000 | Loss: 0.00001814
Iteration 160/1000 | Loss: 0.00001814
Iteration 161/1000 | Loss: 0.00001813
Iteration 162/1000 | Loss: 0.00001813
Iteration 163/1000 | Loss: 0.00001813
Iteration 164/1000 | Loss: 0.00001812
Iteration 165/1000 | Loss: 0.00001812
Iteration 166/1000 | Loss: 0.00001812
Iteration 167/1000 | Loss: 0.00001812
Iteration 168/1000 | Loss: 0.00001811
Iteration 169/1000 | Loss: 0.00001811
Iteration 170/1000 | Loss: 0.00001811
Iteration 171/1000 | Loss: 0.00001811
Iteration 172/1000 | Loss: 0.00001811
Iteration 173/1000 | Loss: 0.00001811
Iteration 174/1000 | Loss: 0.00001811
Iteration 175/1000 | Loss: 0.00001811
Iteration 176/1000 | Loss: 0.00001810
Iteration 177/1000 | Loss: 0.00001810
Iteration 178/1000 | Loss: 0.00001809
Iteration 179/1000 | Loss: 0.00001809
Iteration 180/1000 | Loss: 0.00001809
Iteration 181/1000 | Loss: 0.00001809
Iteration 182/1000 | Loss: 0.00001809
Iteration 183/1000 | Loss: 0.00001809
Iteration 184/1000 | Loss: 0.00001809
Iteration 185/1000 | Loss: 0.00001809
Iteration 186/1000 | Loss: 0.00001807
Iteration 187/1000 | Loss: 0.00001807
Iteration 188/1000 | Loss: 0.00001807
Iteration 189/1000 | Loss: 0.00001807
Iteration 190/1000 | Loss: 0.00001807
Iteration 191/1000 | Loss: 0.00001806
Iteration 192/1000 | Loss: 0.00001806
Iteration 193/1000 | Loss: 0.00001806
Iteration 194/1000 | Loss: 0.00001806
Iteration 195/1000 | Loss: 0.00001806
Iteration 196/1000 | Loss: 0.00001805
Iteration 197/1000 | Loss: 0.00001805
Iteration 198/1000 | Loss: 0.00001805
Iteration 199/1000 | Loss: 0.00001805
Iteration 200/1000 | Loss: 0.00001805
Iteration 201/1000 | Loss: 0.00001805
Iteration 202/1000 | Loss: 0.00001805
Iteration 203/1000 | Loss: 0.00001805
Iteration 204/1000 | Loss: 0.00001804
Iteration 205/1000 | Loss: 0.00001804
Iteration 206/1000 | Loss: 0.00001804
Iteration 207/1000 | Loss: 0.00001804
Iteration 208/1000 | Loss: 0.00001804
Iteration 209/1000 | Loss: 0.00001804
Iteration 210/1000 | Loss: 0.00001804
Iteration 211/1000 | Loss: 0.00001804
Iteration 212/1000 | Loss: 0.00001804
Iteration 213/1000 | Loss: 0.00001803
Iteration 214/1000 | Loss: 0.00001803
Iteration 215/1000 | Loss: 0.00001803
Iteration 216/1000 | Loss: 0.00001803
Iteration 217/1000 | Loss: 0.00001803
Iteration 218/1000 | Loss: 0.00001803
Iteration 219/1000 | Loss: 0.00001803
Iteration 220/1000 | Loss: 0.00001803
Iteration 221/1000 | Loss: 0.00001802
Iteration 222/1000 | Loss: 0.00001802
Iteration 223/1000 | Loss: 0.00001802
Iteration 224/1000 | Loss: 0.00001802
Iteration 225/1000 | Loss: 0.00001802
Iteration 226/1000 | Loss: 0.00001802
Iteration 227/1000 | Loss: 0.00001802
Iteration 228/1000 | Loss: 0.00001802
Iteration 229/1000 | Loss: 0.00001802
Iteration 230/1000 | Loss: 0.00001802
Iteration 231/1000 | Loss: 0.00001802
Iteration 232/1000 | Loss: 0.00001802
Iteration 233/1000 | Loss: 0.00001802
Iteration 234/1000 | Loss: 0.00001802
Iteration 235/1000 | Loss: 0.00001802
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 235. Stopping optimization.
Last 5 losses: [1.801803955459036e-05, 1.801803955459036e-05, 1.801803955459036e-05, 1.801803955459036e-05, 1.801803955459036e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.801803955459036e-05

Optimization complete. Final v2v error: 3.4977638721466064 mm

Highest mean error: 5.12733793258667 mm for frame 218

Lowest mean error: 3.091060161590576 mm for frame 66

Saving results

Total time: 103.06918239593506
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_022/1024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_022/1024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_022/1024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00797694
Iteration 2/25 | Loss: 0.00116173
Iteration 3/25 | Loss: 0.00108727
Iteration 4/25 | Loss: 0.00107049
Iteration 5/25 | Loss: 0.00106517
Iteration 6/25 | Loss: 0.00106455
Iteration 7/25 | Loss: 0.00106452
Iteration 8/25 | Loss: 0.00106452
Iteration 9/25 | Loss: 0.00106452
Iteration 10/25 | Loss: 0.00106452
Iteration 11/25 | Loss: 0.00106452
Iteration 12/25 | Loss: 0.00106452
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.001064523821696639, 0.001064523821696639, 0.001064523821696639, 0.001064523821696639, 0.001064523821696639]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001064523821696639

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.40420544
Iteration 2/25 | Loss: 0.00078298
Iteration 3/25 | Loss: 0.00078297
Iteration 4/25 | Loss: 0.00078297
Iteration 5/25 | Loss: 0.00078297
Iteration 6/25 | Loss: 0.00078297
Iteration 7/25 | Loss: 0.00078297
Iteration 8/25 | Loss: 0.00078297
Iteration 9/25 | Loss: 0.00078297
Iteration 10/25 | Loss: 0.00078297
Iteration 11/25 | Loss: 0.00078297
Iteration 12/25 | Loss: 0.00078297
Iteration 13/25 | Loss: 0.00078297
Iteration 14/25 | Loss: 0.00078297
Iteration 15/25 | Loss: 0.00078297
Iteration 16/25 | Loss: 0.00078297
Iteration 17/25 | Loss: 0.00078297
Iteration 18/25 | Loss: 0.00078297
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0007829694077372551, 0.0007829694077372551, 0.0007829694077372551, 0.0007829694077372551, 0.0007829694077372551]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007829694077372551

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00078297
Iteration 2/1000 | Loss: 0.00002515
Iteration 3/1000 | Loss: 0.00001777
Iteration 4/1000 | Loss: 0.00001553
Iteration 5/1000 | Loss: 0.00001457
Iteration 6/1000 | Loss: 0.00001390
Iteration 7/1000 | Loss: 0.00001340
Iteration 8/1000 | Loss: 0.00001310
Iteration 9/1000 | Loss: 0.00001287
Iteration 10/1000 | Loss: 0.00001259
Iteration 11/1000 | Loss: 0.00001243
Iteration 12/1000 | Loss: 0.00001243
Iteration 13/1000 | Loss: 0.00001242
Iteration 14/1000 | Loss: 0.00001237
Iteration 15/1000 | Loss: 0.00001236
Iteration 16/1000 | Loss: 0.00001235
Iteration 17/1000 | Loss: 0.00001234
Iteration 18/1000 | Loss: 0.00001232
Iteration 19/1000 | Loss: 0.00001231
Iteration 20/1000 | Loss: 0.00001224
Iteration 21/1000 | Loss: 0.00001221
Iteration 22/1000 | Loss: 0.00001216
Iteration 23/1000 | Loss: 0.00001215
Iteration 24/1000 | Loss: 0.00001214
Iteration 25/1000 | Loss: 0.00001208
Iteration 26/1000 | Loss: 0.00001208
Iteration 27/1000 | Loss: 0.00001207
Iteration 28/1000 | Loss: 0.00001207
Iteration 29/1000 | Loss: 0.00001207
Iteration 30/1000 | Loss: 0.00001206
Iteration 31/1000 | Loss: 0.00001205
Iteration 32/1000 | Loss: 0.00001204
Iteration 33/1000 | Loss: 0.00001203
Iteration 34/1000 | Loss: 0.00001203
Iteration 35/1000 | Loss: 0.00001202
Iteration 36/1000 | Loss: 0.00001202
Iteration 37/1000 | Loss: 0.00001202
Iteration 38/1000 | Loss: 0.00001202
Iteration 39/1000 | Loss: 0.00001202
Iteration 40/1000 | Loss: 0.00001201
Iteration 41/1000 | Loss: 0.00001201
Iteration 42/1000 | Loss: 0.00001201
Iteration 43/1000 | Loss: 0.00001201
Iteration 44/1000 | Loss: 0.00001201
Iteration 45/1000 | Loss: 0.00001201
Iteration 46/1000 | Loss: 0.00001200
Iteration 47/1000 | Loss: 0.00001200
Iteration 48/1000 | Loss: 0.00001199
Iteration 49/1000 | Loss: 0.00001198
Iteration 50/1000 | Loss: 0.00001198
Iteration 51/1000 | Loss: 0.00001197
Iteration 52/1000 | Loss: 0.00001196
Iteration 53/1000 | Loss: 0.00001194
Iteration 54/1000 | Loss: 0.00001193
Iteration 55/1000 | Loss: 0.00001192
Iteration 56/1000 | Loss: 0.00001192
Iteration 57/1000 | Loss: 0.00001192
Iteration 58/1000 | Loss: 0.00001192
Iteration 59/1000 | Loss: 0.00001191
Iteration 60/1000 | Loss: 0.00001190
Iteration 61/1000 | Loss: 0.00001190
Iteration 62/1000 | Loss: 0.00001189
Iteration 63/1000 | Loss: 0.00001188
Iteration 64/1000 | Loss: 0.00001188
Iteration 65/1000 | Loss: 0.00001188
Iteration 66/1000 | Loss: 0.00001187
Iteration 67/1000 | Loss: 0.00001187
Iteration 68/1000 | Loss: 0.00001187
Iteration 69/1000 | Loss: 0.00001186
Iteration 70/1000 | Loss: 0.00001186
Iteration 71/1000 | Loss: 0.00001185
Iteration 72/1000 | Loss: 0.00001185
Iteration 73/1000 | Loss: 0.00001185
Iteration 74/1000 | Loss: 0.00001184
Iteration 75/1000 | Loss: 0.00001184
Iteration 76/1000 | Loss: 0.00001184
Iteration 77/1000 | Loss: 0.00001184
Iteration 78/1000 | Loss: 0.00001184
Iteration 79/1000 | Loss: 0.00001184
Iteration 80/1000 | Loss: 0.00001183
Iteration 81/1000 | Loss: 0.00001183
Iteration 82/1000 | Loss: 0.00001183
Iteration 83/1000 | Loss: 0.00001182
Iteration 84/1000 | Loss: 0.00001182
Iteration 85/1000 | Loss: 0.00001182
Iteration 86/1000 | Loss: 0.00001182
Iteration 87/1000 | Loss: 0.00001181
Iteration 88/1000 | Loss: 0.00001181
Iteration 89/1000 | Loss: 0.00001181
Iteration 90/1000 | Loss: 0.00001181
Iteration 91/1000 | Loss: 0.00001180
Iteration 92/1000 | Loss: 0.00001180
Iteration 93/1000 | Loss: 0.00001180
Iteration 94/1000 | Loss: 0.00001180
Iteration 95/1000 | Loss: 0.00001180
Iteration 96/1000 | Loss: 0.00001179
Iteration 97/1000 | Loss: 0.00001179
Iteration 98/1000 | Loss: 0.00001179
Iteration 99/1000 | Loss: 0.00001178
Iteration 100/1000 | Loss: 0.00001178
Iteration 101/1000 | Loss: 0.00001178
Iteration 102/1000 | Loss: 0.00001178
Iteration 103/1000 | Loss: 0.00001178
Iteration 104/1000 | Loss: 0.00001178
Iteration 105/1000 | Loss: 0.00001178
Iteration 106/1000 | Loss: 0.00001178
Iteration 107/1000 | Loss: 0.00001177
Iteration 108/1000 | Loss: 0.00001177
Iteration 109/1000 | Loss: 0.00001177
Iteration 110/1000 | Loss: 0.00001177
Iteration 111/1000 | Loss: 0.00001176
Iteration 112/1000 | Loss: 0.00001176
Iteration 113/1000 | Loss: 0.00001176
Iteration 114/1000 | Loss: 0.00001175
Iteration 115/1000 | Loss: 0.00001175
Iteration 116/1000 | Loss: 0.00001175
Iteration 117/1000 | Loss: 0.00001175
Iteration 118/1000 | Loss: 0.00001174
Iteration 119/1000 | Loss: 0.00001174
Iteration 120/1000 | Loss: 0.00001173
Iteration 121/1000 | Loss: 0.00001173
Iteration 122/1000 | Loss: 0.00001173
Iteration 123/1000 | Loss: 0.00001172
Iteration 124/1000 | Loss: 0.00001172
Iteration 125/1000 | Loss: 0.00001172
Iteration 126/1000 | Loss: 0.00001172
Iteration 127/1000 | Loss: 0.00001172
Iteration 128/1000 | Loss: 0.00001172
Iteration 129/1000 | Loss: 0.00001171
Iteration 130/1000 | Loss: 0.00001171
Iteration 131/1000 | Loss: 0.00001171
Iteration 132/1000 | Loss: 0.00001171
Iteration 133/1000 | Loss: 0.00001171
Iteration 134/1000 | Loss: 0.00001171
Iteration 135/1000 | Loss: 0.00001171
Iteration 136/1000 | Loss: 0.00001171
Iteration 137/1000 | Loss: 0.00001171
Iteration 138/1000 | Loss: 0.00001170
Iteration 139/1000 | Loss: 0.00001170
Iteration 140/1000 | Loss: 0.00001170
Iteration 141/1000 | Loss: 0.00001170
Iteration 142/1000 | Loss: 0.00001170
Iteration 143/1000 | Loss: 0.00001170
Iteration 144/1000 | Loss: 0.00001170
Iteration 145/1000 | Loss: 0.00001170
Iteration 146/1000 | Loss: 0.00001169
Iteration 147/1000 | Loss: 0.00001169
Iteration 148/1000 | Loss: 0.00001169
Iteration 149/1000 | Loss: 0.00001169
Iteration 150/1000 | Loss: 0.00001169
Iteration 151/1000 | Loss: 0.00001169
Iteration 152/1000 | Loss: 0.00001169
Iteration 153/1000 | Loss: 0.00001169
Iteration 154/1000 | Loss: 0.00001169
Iteration 155/1000 | Loss: 0.00001169
Iteration 156/1000 | Loss: 0.00001169
Iteration 157/1000 | Loss: 0.00001168
Iteration 158/1000 | Loss: 0.00001168
Iteration 159/1000 | Loss: 0.00001168
Iteration 160/1000 | Loss: 0.00001168
Iteration 161/1000 | Loss: 0.00001168
Iteration 162/1000 | Loss: 0.00001168
Iteration 163/1000 | Loss: 0.00001167
Iteration 164/1000 | Loss: 0.00001167
Iteration 165/1000 | Loss: 0.00001167
Iteration 166/1000 | Loss: 0.00001167
Iteration 167/1000 | Loss: 0.00001167
Iteration 168/1000 | Loss: 0.00001167
Iteration 169/1000 | Loss: 0.00001167
Iteration 170/1000 | Loss: 0.00001167
Iteration 171/1000 | Loss: 0.00001167
Iteration 172/1000 | Loss: 0.00001167
Iteration 173/1000 | Loss: 0.00001167
Iteration 174/1000 | Loss: 0.00001167
Iteration 175/1000 | Loss: 0.00001167
Iteration 176/1000 | Loss: 0.00001167
Iteration 177/1000 | Loss: 0.00001167
Iteration 178/1000 | Loss: 0.00001167
Iteration 179/1000 | Loss: 0.00001167
Iteration 180/1000 | Loss: 0.00001167
Iteration 181/1000 | Loss: 0.00001167
Iteration 182/1000 | Loss: 0.00001167
Iteration 183/1000 | Loss: 0.00001167
Iteration 184/1000 | Loss: 0.00001167
Iteration 185/1000 | Loss: 0.00001167
Iteration 186/1000 | Loss: 0.00001167
Iteration 187/1000 | Loss: 0.00001167
Iteration 188/1000 | Loss: 0.00001167
Iteration 189/1000 | Loss: 0.00001167
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 189. Stopping optimization.
Last 5 losses: [1.1665268175420351e-05, 1.1665268175420351e-05, 1.1665268175420351e-05, 1.1665268175420351e-05, 1.1665268175420351e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1665268175420351e-05

Optimization complete. Final v2v error: 2.9619534015655518 mm

Highest mean error: 3.1947567462921143 mm for frame 154

Lowest mean error: 2.788008213043213 mm for frame 65

Saving results

Total time: 45.75841665267944
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_022/1065/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_022/1065.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_022/1065
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00402130
Iteration 2/25 | Loss: 0.00120246
Iteration 3/25 | Loss: 0.00110412
Iteration 4/25 | Loss: 0.00109040
Iteration 5/25 | Loss: 0.00108655
Iteration 6/25 | Loss: 0.00108547
Iteration 7/25 | Loss: 0.00108516
Iteration 8/25 | Loss: 0.00108516
Iteration 9/25 | Loss: 0.00108516
Iteration 10/25 | Loss: 0.00108516
Iteration 11/25 | Loss: 0.00108516
Iteration 12/25 | Loss: 0.00108516
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.001085155876353383, 0.001085155876353383, 0.001085155876353383, 0.001085155876353383, 0.001085155876353383]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001085155876353383

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32597101
Iteration 2/25 | Loss: 0.00105403
Iteration 3/25 | Loss: 0.00105400
Iteration 4/25 | Loss: 0.00105400
Iteration 5/25 | Loss: 0.00105400
Iteration 6/25 | Loss: 0.00105400
Iteration 7/25 | Loss: 0.00105400
Iteration 8/25 | Loss: 0.00105400
Iteration 9/25 | Loss: 0.00105400
Iteration 10/25 | Loss: 0.00105400
Iteration 11/25 | Loss: 0.00105400
Iteration 12/25 | Loss: 0.00105400
Iteration 13/25 | Loss: 0.00105400
Iteration 14/25 | Loss: 0.00105400
Iteration 15/25 | Loss: 0.00105400
Iteration 16/25 | Loss: 0.00105400
Iteration 17/25 | Loss: 0.00105400
Iteration 18/25 | Loss: 0.00105400
Iteration 19/25 | Loss: 0.00105400
Iteration 20/25 | Loss: 0.00105400
Iteration 21/25 | Loss: 0.00105400
Iteration 22/25 | Loss: 0.00105400
Iteration 23/25 | Loss: 0.00105400
Iteration 24/25 | Loss: 0.00105400
Iteration 25/25 | Loss: 0.00105400

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00105400
Iteration 2/1000 | Loss: 0.00002554
Iteration 3/1000 | Loss: 0.00001371
Iteration 4/1000 | Loss: 0.00001217
Iteration 5/1000 | Loss: 0.00001145
Iteration 6/1000 | Loss: 0.00001070
Iteration 7/1000 | Loss: 0.00001035
Iteration 8/1000 | Loss: 0.00001010
Iteration 9/1000 | Loss: 0.00000992
Iteration 10/1000 | Loss: 0.00000986
Iteration 11/1000 | Loss: 0.00000980
Iteration 12/1000 | Loss: 0.00000971
Iteration 13/1000 | Loss: 0.00000970
Iteration 14/1000 | Loss: 0.00000965
Iteration 15/1000 | Loss: 0.00000965
Iteration 16/1000 | Loss: 0.00000960
Iteration 17/1000 | Loss: 0.00000960
Iteration 18/1000 | Loss: 0.00000959
Iteration 19/1000 | Loss: 0.00000959
Iteration 20/1000 | Loss: 0.00000958
Iteration 21/1000 | Loss: 0.00000958
Iteration 22/1000 | Loss: 0.00000958
Iteration 23/1000 | Loss: 0.00000958
Iteration 24/1000 | Loss: 0.00000958
Iteration 25/1000 | Loss: 0.00000956
Iteration 26/1000 | Loss: 0.00000956
Iteration 27/1000 | Loss: 0.00000956
Iteration 28/1000 | Loss: 0.00000956
Iteration 29/1000 | Loss: 0.00000956
Iteration 30/1000 | Loss: 0.00000956
Iteration 31/1000 | Loss: 0.00000955
Iteration 32/1000 | Loss: 0.00000955
Iteration 33/1000 | Loss: 0.00000955
Iteration 34/1000 | Loss: 0.00000955
Iteration 35/1000 | Loss: 0.00000955
Iteration 36/1000 | Loss: 0.00000954
Iteration 37/1000 | Loss: 0.00000953
Iteration 38/1000 | Loss: 0.00000953
Iteration 39/1000 | Loss: 0.00000953
Iteration 40/1000 | Loss: 0.00000952
Iteration 41/1000 | Loss: 0.00000952
Iteration 42/1000 | Loss: 0.00000951
Iteration 43/1000 | Loss: 0.00000950
Iteration 44/1000 | Loss: 0.00000950
Iteration 45/1000 | Loss: 0.00000950
Iteration 46/1000 | Loss: 0.00000947
Iteration 47/1000 | Loss: 0.00000942
Iteration 48/1000 | Loss: 0.00000942
Iteration 49/1000 | Loss: 0.00000939
Iteration 50/1000 | Loss: 0.00000938
Iteration 51/1000 | Loss: 0.00000938
Iteration 52/1000 | Loss: 0.00000938
Iteration 53/1000 | Loss: 0.00000937
Iteration 54/1000 | Loss: 0.00000937
Iteration 55/1000 | Loss: 0.00000936
Iteration 56/1000 | Loss: 0.00000936
Iteration 57/1000 | Loss: 0.00000936
Iteration 58/1000 | Loss: 0.00000936
Iteration 59/1000 | Loss: 0.00000936
Iteration 60/1000 | Loss: 0.00000936
Iteration 61/1000 | Loss: 0.00000936
Iteration 62/1000 | Loss: 0.00000936
Iteration 63/1000 | Loss: 0.00000936
Iteration 64/1000 | Loss: 0.00000936
Iteration 65/1000 | Loss: 0.00000936
Iteration 66/1000 | Loss: 0.00000936
Iteration 67/1000 | Loss: 0.00000936
Iteration 68/1000 | Loss: 0.00000935
Iteration 69/1000 | Loss: 0.00000935
Iteration 70/1000 | Loss: 0.00000935
Iteration 71/1000 | Loss: 0.00000935
Iteration 72/1000 | Loss: 0.00000935
Iteration 73/1000 | Loss: 0.00000935
Iteration 74/1000 | Loss: 0.00000935
Iteration 75/1000 | Loss: 0.00000935
Iteration 76/1000 | Loss: 0.00000935
Iteration 77/1000 | Loss: 0.00000935
Iteration 78/1000 | Loss: 0.00000935
Iteration 79/1000 | Loss: 0.00000935
Iteration 80/1000 | Loss: 0.00000935
Iteration 81/1000 | Loss: 0.00000935
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 81. Stopping optimization.
Last 5 losses: [9.35282059799647e-06, 9.35282059799647e-06, 9.35282059799647e-06, 9.35282059799647e-06, 9.35282059799647e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.35282059799647e-06

Optimization complete. Final v2v error: 2.621535301208496 mm

Highest mean error: 2.8930070400238037 mm for frame 84

Lowest mean error: 2.4126360416412354 mm for frame 12

Saving results

Total time: 31.854324102401733
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_022/1069/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_022/1069.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_022/1069
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00411681
Iteration 2/25 | Loss: 0.00148681
Iteration 3/25 | Loss: 0.00116638
Iteration 4/25 | Loss: 0.00111401
Iteration 5/25 | Loss: 0.00110836
Iteration 6/25 | Loss: 0.00110705
Iteration 7/25 | Loss: 0.00110685
Iteration 8/25 | Loss: 0.00110685
Iteration 9/25 | Loss: 0.00110685
Iteration 10/25 | Loss: 0.00110685
Iteration 11/25 | Loss: 0.00110685
Iteration 12/25 | Loss: 0.00110685
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.001106851501390338, 0.001106851501390338, 0.001106851501390338, 0.001106851501390338, 0.001106851501390338]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001106851501390338

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32265961
Iteration 2/25 | Loss: 0.00071556
Iteration 3/25 | Loss: 0.00071556
Iteration 4/25 | Loss: 0.00071556
Iteration 5/25 | Loss: 0.00071556
Iteration 6/25 | Loss: 0.00071556
Iteration 7/25 | Loss: 0.00071556
Iteration 8/25 | Loss: 0.00071556
Iteration 9/25 | Loss: 0.00071556
Iteration 10/25 | Loss: 0.00071556
Iteration 11/25 | Loss: 0.00071556
Iteration 12/25 | Loss: 0.00071556
Iteration 13/25 | Loss: 0.00071556
Iteration 14/25 | Loss: 0.00071556
Iteration 15/25 | Loss: 0.00071556
Iteration 16/25 | Loss: 0.00071556
Iteration 17/25 | Loss: 0.00071556
Iteration 18/25 | Loss: 0.00071556
Iteration 19/25 | Loss: 0.00071556
Iteration 20/25 | Loss: 0.00071556
Iteration 21/25 | Loss: 0.00071556
Iteration 22/25 | Loss: 0.00071556
Iteration 23/25 | Loss: 0.00071556
Iteration 24/25 | Loss: 0.00071556
Iteration 25/25 | Loss: 0.00071556

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00071556
Iteration 2/1000 | Loss: 0.00003616
Iteration 3/1000 | Loss: 0.00002287
Iteration 4/1000 | Loss: 0.00001760
Iteration 5/1000 | Loss: 0.00001614
Iteration 6/1000 | Loss: 0.00001529
Iteration 7/1000 | Loss: 0.00001482
Iteration 8/1000 | Loss: 0.00001428
Iteration 9/1000 | Loss: 0.00001394
Iteration 10/1000 | Loss: 0.00001371
Iteration 11/1000 | Loss: 0.00001348
Iteration 12/1000 | Loss: 0.00001341
Iteration 13/1000 | Loss: 0.00001340
Iteration 14/1000 | Loss: 0.00001334
Iteration 15/1000 | Loss: 0.00001326
Iteration 16/1000 | Loss: 0.00001318
Iteration 17/1000 | Loss: 0.00001317
Iteration 18/1000 | Loss: 0.00001316
Iteration 19/1000 | Loss: 0.00001316
Iteration 20/1000 | Loss: 0.00001316
Iteration 21/1000 | Loss: 0.00001315
Iteration 22/1000 | Loss: 0.00001315
Iteration 23/1000 | Loss: 0.00001314
Iteration 24/1000 | Loss: 0.00001314
Iteration 25/1000 | Loss: 0.00001313
Iteration 26/1000 | Loss: 0.00001313
Iteration 27/1000 | Loss: 0.00001313
Iteration 28/1000 | Loss: 0.00001312
Iteration 29/1000 | Loss: 0.00001312
Iteration 30/1000 | Loss: 0.00001312
Iteration 31/1000 | Loss: 0.00001311
Iteration 32/1000 | Loss: 0.00001311
Iteration 33/1000 | Loss: 0.00001307
Iteration 34/1000 | Loss: 0.00001307
Iteration 35/1000 | Loss: 0.00001302
Iteration 36/1000 | Loss: 0.00001302
Iteration 37/1000 | Loss: 0.00001301
Iteration 38/1000 | Loss: 0.00001300
Iteration 39/1000 | Loss: 0.00001300
Iteration 40/1000 | Loss: 0.00001299
Iteration 41/1000 | Loss: 0.00001299
Iteration 42/1000 | Loss: 0.00001298
Iteration 43/1000 | Loss: 0.00001297
Iteration 44/1000 | Loss: 0.00001296
Iteration 45/1000 | Loss: 0.00001296
Iteration 46/1000 | Loss: 0.00001295
Iteration 47/1000 | Loss: 0.00001294
Iteration 48/1000 | Loss: 0.00001294
Iteration 49/1000 | Loss: 0.00001293
Iteration 50/1000 | Loss: 0.00001293
Iteration 51/1000 | Loss: 0.00001293
Iteration 52/1000 | Loss: 0.00001293
Iteration 53/1000 | Loss: 0.00001293
Iteration 54/1000 | Loss: 0.00001293
Iteration 55/1000 | Loss: 0.00001293
Iteration 56/1000 | Loss: 0.00001293
Iteration 57/1000 | Loss: 0.00001292
Iteration 58/1000 | Loss: 0.00001292
Iteration 59/1000 | Loss: 0.00001292
Iteration 60/1000 | Loss: 0.00001292
Iteration 61/1000 | Loss: 0.00001291
Iteration 62/1000 | Loss: 0.00001291
Iteration 63/1000 | Loss: 0.00001291
Iteration 64/1000 | Loss: 0.00001290
Iteration 65/1000 | Loss: 0.00001290
Iteration 66/1000 | Loss: 0.00001290
Iteration 67/1000 | Loss: 0.00001290
Iteration 68/1000 | Loss: 0.00001290
Iteration 69/1000 | Loss: 0.00001290
Iteration 70/1000 | Loss: 0.00001290
Iteration 71/1000 | Loss: 0.00001290
Iteration 72/1000 | Loss: 0.00001290
Iteration 73/1000 | Loss: 0.00001290
Iteration 74/1000 | Loss: 0.00001290
Iteration 75/1000 | Loss: 0.00001290
Iteration 76/1000 | Loss: 0.00001290
Iteration 77/1000 | Loss: 0.00001290
Iteration 78/1000 | Loss: 0.00001290
Iteration 79/1000 | Loss: 0.00001290
Iteration 80/1000 | Loss: 0.00001290
Iteration 81/1000 | Loss: 0.00001290
Iteration 82/1000 | Loss: 0.00001290
Iteration 83/1000 | Loss: 0.00001290
Iteration 84/1000 | Loss: 0.00001290
Iteration 85/1000 | Loss: 0.00001290
Iteration 86/1000 | Loss: 0.00001290
Iteration 87/1000 | Loss: 0.00001290
Iteration 88/1000 | Loss: 0.00001290
Iteration 89/1000 | Loss: 0.00001290
Iteration 90/1000 | Loss: 0.00001290
Iteration 91/1000 | Loss: 0.00001290
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 91. Stopping optimization.
Last 5 losses: [1.2901113223051652e-05, 1.2901113223051652e-05, 1.2901113223051652e-05, 1.2901113223051652e-05, 1.2901113223051652e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2901113223051652e-05

Optimization complete. Final v2v error: 3.0595862865448 mm

Highest mean error: 3.845025062561035 mm for frame 72

Lowest mean error: 2.5829193592071533 mm for frame 13

Saving results

Total time: 35.27061986923218
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_022/1021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_022/1021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_022/1021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00948848
Iteration 2/25 | Loss: 0.00165366
Iteration 3/25 | Loss: 0.00132030
Iteration 4/25 | Loss: 0.00129802
Iteration 5/25 | Loss: 0.00129197
Iteration 6/25 | Loss: 0.00129150
Iteration 7/25 | Loss: 0.00129150
Iteration 8/25 | Loss: 0.00129150
Iteration 9/25 | Loss: 0.00129150
Iteration 10/25 | Loss: 0.00129150
Iteration 11/25 | Loss: 0.00129150
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012915031984448433, 0.0012915031984448433, 0.0012915031984448433, 0.0012915031984448433, 0.0012915031984448433]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012915031984448433

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.81766397
Iteration 2/25 | Loss: 0.00094061
Iteration 3/25 | Loss: 0.00094061
Iteration 4/25 | Loss: 0.00094061
Iteration 5/25 | Loss: 0.00094061
Iteration 6/25 | Loss: 0.00094061
Iteration 7/25 | Loss: 0.00094061
Iteration 8/25 | Loss: 0.00094061
Iteration 9/25 | Loss: 0.00094061
Iteration 10/25 | Loss: 0.00094061
Iteration 11/25 | Loss: 0.00094061
Iteration 12/25 | Loss: 0.00094061
Iteration 13/25 | Loss: 0.00094060
Iteration 14/25 | Loss: 0.00094060
Iteration 15/25 | Loss: 0.00094060
Iteration 16/25 | Loss: 0.00094060
Iteration 17/25 | Loss: 0.00094060
Iteration 18/25 | Loss: 0.00094060
Iteration 19/25 | Loss: 0.00094060
Iteration 20/25 | Loss: 0.00094060
Iteration 21/25 | Loss: 0.00094060
Iteration 22/25 | Loss: 0.00094060
Iteration 23/25 | Loss: 0.00094060
Iteration 24/25 | Loss: 0.00094060
Iteration 25/25 | Loss: 0.00094060

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00094060
Iteration 2/1000 | Loss: 0.00006344
Iteration 3/1000 | Loss: 0.00004333
Iteration 4/1000 | Loss: 0.00003688
Iteration 5/1000 | Loss: 0.00003496
Iteration 6/1000 | Loss: 0.00003404
Iteration 7/1000 | Loss: 0.00003307
Iteration 8/1000 | Loss: 0.00003264
Iteration 9/1000 | Loss: 0.00003211
Iteration 10/1000 | Loss: 0.00003178
Iteration 11/1000 | Loss: 0.00003146
Iteration 12/1000 | Loss: 0.00003115
Iteration 13/1000 | Loss: 0.00003085
Iteration 14/1000 | Loss: 0.00003063
Iteration 15/1000 | Loss: 0.00003039
Iteration 16/1000 | Loss: 0.00003015
Iteration 17/1000 | Loss: 0.00002994
Iteration 18/1000 | Loss: 0.00002984
Iteration 19/1000 | Loss: 0.00002981
Iteration 20/1000 | Loss: 0.00002981
Iteration 21/1000 | Loss: 0.00002980
Iteration 22/1000 | Loss: 0.00002980
Iteration 23/1000 | Loss: 0.00002976
Iteration 24/1000 | Loss: 0.00002972
Iteration 25/1000 | Loss: 0.00002968
Iteration 26/1000 | Loss: 0.00002965
Iteration 27/1000 | Loss: 0.00002964
Iteration 28/1000 | Loss: 0.00002964
Iteration 29/1000 | Loss: 0.00002963
Iteration 30/1000 | Loss: 0.00002963
Iteration 31/1000 | Loss: 0.00002960
Iteration 32/1000 | Loss: 0.00002960
Iteration 33/1000 | Loss: 0.00002960
Iteration 34/1000 | Loss: 0.00002959
Iteration 35/1000 | Loss: 0.00002956
Iteration 36/1000 | Loss: 0.00002956
Iteration 37/1000 | Loss: 0.00002952
Iteration 38/1000 | Loss: 0.00002951
Iteration 39/1000 | Loss: 0.00002949
Iteration 40/1000 | Loss: 0.00002949
Iteration 41/1000 | Loss: 0.00002949
Iteration 42/1000 | Loss: 0.00002949
Iteration 43/1000 | Loss: 0.00002949
Iteration 44/1000 | Loss: 0.00002949
Iteration 45/1000 | Loss: 0.00002949
Iteration 46/1000 | Loss: 0.00002949
Iteration 47/1000 | Loss: 0.00002949
Iteration 48/1000 | Loss: 0.00002948
Iteration 49/1000 | Loss: 0.00002948
Iteration 50/1000 | Loss: 0.00002948
Iteration 51/1000 | Loss: 0.00002948
Iteration 52/1000 | Loss: 0.00002947
Iteration 53/1000 | Loss: 0.00002947
Iteration 54/1000 | Loss: 0.00002947
Iteration 55/1000 | Loss: 0.00002946
Iteration 56/1000 | Loss: 0.00002946
Iteration 57/1000 | Loss: 0.00002946
Iteration 58/1000 | Loss: 0.00002946
Iteration 59/1000 | Loss: 0.00002946
Iteration 60/1000 | Loss: 0.00002946
Iteration 61/1000 | Loss: 0.00002945
Iteration 62/1000 | Loss: 0.00002945
Iteration 63/1000 | Loss: 0.00002945
Iteration 64/1000 | Loss: 0.00002945
Iteration 65/1000 | Loss: 0.00002945
Iteration 66/1000 | Loss: 0.00002945
Iteration 67/1000 | Loss: 0.00002945
Iteration 68/1000 | Loss: 0.00002945
Iteration 69/1000 | Loss: 0.00002944
Iteration 70/1000 | Loss: 0.00002944
Iteration 71/1000 | Loss: 0.00002944
Iteration 72/1000 | Loss: 0.00002943
Iteration 73/1000 | Loss: 0.00002943
Iteration 74/1000 | Loss: 0.00002943
Iteration 75/1000 | Loss: 0.00002942
Iteration 76/1000 | Loss: 0.00002942
Iteration 77/1000 | Loss: 0.00002942
Iteration 78/1000 | Loss: 0.00002942
Iteration 79/1000 | Loss: 0.00002942
Iteration 80/1000 | Loss: 0.00002941
Iteration 81/1000 | Loss: 0.00002941
Iteration 82/1000 | Loss: 0.00002941
Iteration 83/1000 | Loss: 0.00002941
Iteration 84/1000 | Loss: 0.00002940
Iteration 85/1000 | Loss: 0.00002940
Iteration 86/1000 | Loss: 0.00002940
Iteration 87/1000 | Loss: 0.00002940
Iteration 88/1000 | Loss: 0.00002939
Iteration 89/1000 | Loss: 0.00002939
Iteration 90/1000 | Loss: 0.00002939
Iteration 91/1000 | Loss: 0.00002939
Iteration 92/1000 | Loss: 0.00002939
Iteration 93/1000 | Loss: 0.00002939
Iteration 94/1000 | Loss: 0.00002939
Iteration 95/1000 | Loss: 0.00002939
Iteration 96/1000 | Loss: 0.00002938
Iteration 97/1000 | Loss: 0.00002938
Iteration 98/1000 | Loss: 0.00002938
Iteration 99/1000 | Loss: 0.00002938
Iteration 100/1000 | Loss: 0.00002938
Iteration 101/1000 | Loss: 0.00002938
Iteration 102/1000 | Loss: 0.00002938
Iteration 103/1000 | Loss: 0.00002937
Iteration 104/1000 | Loss: 0.00002937
Iteration 105/1000 | Loss: 0.00002937
Iteration 106/1000 | Loss: 0.00002937
Iteration 107/1000 | Loss: 0.00002937
Iteration 108/1000 | Loss: 0.00002937
Iteration 109/1000 | Loss: 0.00002936
Iteration 110/1000 | Loss: 0.00002936
Iteration 111/1000 | Loss: 0.00002936
Iteration 112/1000 | Loss: 0.00002936
Iteration 113/1000 | Loss: 0.00002936
Iteration 114/1000 | Loss: 0.00002936
Iteration 115/1000 | Loss: 0.00002936
Iteration 116/1000 | Loss: 0.00002935
Iteration 117/1000 | Loss: 0.00002935
Iteration 118/1000 | Loss: 0.00002935
Iteration 119/1000 | Loss: 0.00002935
Iteration 120/1000 | Loss: 0.00002935
Iteration 121/1000 | Loss: 0.00002935
Iteration 122/1000 | Loss: 0.00002935
Iteration 123/1000 | Loss: 0.00002935
Iteration 124/1000 | Loss: 0.00002935
Iteration 125/1000 | Loss: 0.00002935
Iteration 126/1000 | Loss: 0.00002935
Iteration 127/1000 | Loss: 0.00002935
Iteration 128/1000 | Loss: 0.00002935
Iteration 129/1000 | Loss: 0.00002935
Iteration 130/1000 | Loss: 0.00002935
Iteration 131/1000 | Loss: 0.00002935
Iteration 132/1000 | Loss: 0.00002935
Iteration 133/1000 | Loss: 0.00002935
Iteration 134/1000 | Loss: 0.00002934
Iteration 135/1000 | Loss: 0.00002934
Iteration 136/1000 | Loss: 0.00002934
Iteration 137/1000 | Loss: 0.00002934
Iteration 138/1000 | Loss: 0.00002934
Iteration 139/1000 | Loss: 0.00002934
Iteration 140/1000 | Loss: 0.00002934
Iteration 141/1000 | Loss: 0.00002934
Iteration 142/1000 | Loss: 0.00002934
Iteration 143/1000 | Loss: 0.00002934
Iteration 144/1000 | Loss: 0.00002934
Iteration 145/1000 | Loss: 0.00002934
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 145. Stopping optimization.
Last 5 losses: [2.9342314519453794e-05, 2.9342314519453794e-05, 2.9342314519453794e-05, 2.9342314519453794e-05, 2.9342314519453794e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.9342314519453794e-05

Optimization complete. Final v2v error: 4.486428737640381 mm

Highest mean error: 5.532844543457031 mm for frame 88

Lowest mean error: 3.4960198402404785 mm for frame 0

Saving results

Total time: 52.255921602249146
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_022/1027/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_022/1027.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_022/1027
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01007821
Iteration 2/25 | Loss: 0.00241985
Iteration 3/25 | Loss: 0.00174311
Iteration 4/25 | Loss: 0.00151586
Iteration 5/25 | Loss: 0.00147658
Iteration 6/25 | Loss: 0.00144725
Iteration 7/25 | Loss: 0.00137322
Iteration 8/25 | Loss: 0.00134848
Iteration 9/25 | Loss: 0.00133687
Iteration 10/25 | Loss: 0.00128788
Iteration 11/25 | Loss: 0.00127731
Iteration 12/25 | Loss: 0.00127152
Iteration 13/25 | Loss: 0.00128713
Iteration 14/25 | Loss: 0.00127295
Iteration 15/25 | Loss: 0.00126389
Iteration 16/25 | Loss: 0.00125485
Iteration 17/25 | Loss: 0.00125864
Iteration 18/25 | Loss: 0.00125928
Iteration 19/25 | Loss: 0.00125032
Iteration 20/25 | Loss: 0.00126053
Iteration 21/25 | Loss: 0.00125454
Iteration 22/25 | Loss: 0.00124620
Iteration 23/25 | Loss: 0.00124473
Iteration 24/25 | Loss: 0.00123821
Iteration 25/25 | Loss: 0.00125922

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34936440
Iteration 2/25 | Loss: 0.00197261
Iteration 3/25 | Loss: 0.00197023
Iteration 4/25 | Loss: 0.00197023
Iteration 5/25 | Loss: 0.00197023
Iteration 6/25 | Loss: 0.00197023
Iteration 7/25 | Loss: 0.00197023
Iteration 8/25 | Loss: 0.00197023
Iteration 9/25 | Loss: 0.00197023
Iteration 10/25 | Loss: 0.00197023
Iteration 11/25 | Loss: 0.00197023
Iteration 12/25 | Loss: 0.00197023
Iteration 13/25 | Loss: 0.00197023
Iteration 14/25 | Loss: 0.00197023
Iteration 15/25 | Loss: 0.00197023
Iteration 16/25 | Loss: 0.00197023
Iteration 17/25 | Loss: 0.00197023
Iteration 18/25 | Loss: 0.00197023
Iteration 19/25 | Loss: 0.00197023
Iteration 20/25 | Loss: 0.00197023
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.001970230136066675, 0.001970230136066675, 0.001970230136066675, 0.001970230136066675, 0.001970230136066675]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001970230136066675

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00197023
Iteration 2/1000 | Loss: 0.00042048
Iteration 3/1000 | Loss: 0.00057498
Iteration 4/1000 | Loss: 0.00039355
Iteration 5/1000 | Loss: 0.00027786
Iteration 6/1000 | Loss: 0.00026453
Iteration 7/1000 | Loss: 0.00022231
Iteration 8/1000 | Loss: 0.00023884
Iteration 9/1000 | Loss: 0.00026288
Iteration 10/1000 | Loss: 0.00022015
Iteration 11/1000 | Loss: 0.00016960
Iteration 12/1000 | Loss: 0.00022080
Iteration 13/1000 | Loss: 0.00010859
Iteration 14/1000 | Loss: 0.00038795
Iteration 15/1000 | Loss: 0.00033095
Iteration 16/1000 | Loss: 0.00013813
Iteration 17/1000 | Loss: 0.00019021
Iteration 18/1000 | Loss: 0.00011411
Iteration 19/1000 | Loss: 0.00022172
Iteration 20/1000 | Loss: 0.00011569
Iteration 21/1000 | Loss: 0.00008780
Iteration 22/1000 | Loss: 0.00025775
Iteration 23/1000 | Loss: 0.00025219
Iteration 24/1000 | Loss: 0.00011833
Iteration 25/1000 | Loss: 0.00013747
Iteration 26/1000 | Loss: 0.00013691
Iteration 27/1000 | Loss: 0.00014561
Iteration 28/1000 | Loss: 0.00012872
Iteration 29/1000 | Loss: 0.00038691
Iteration 30/1000 | Loss: 0.00050266
Iteration 31/1000 | Loss: 0.00057894
Iteration 32/1000 | Loss: 0.00048635
Iteration 33/1000 | Loss: 0.00031231
Iteration 34/1000 | Loss: 0.00013116
Iteration 35/1000 | Loss: 0.00007673
Iteration 36/1000 | Loss: 0.00011163
Iteration 37/1000 | Loss: 0.00007797
Iteration 38/1000 | Loss: 0.00007058
Iteration 39/1000 | Loss: 0.00006698
Iteration 40/1000 | Loss: 0.00006543
Iteration 41/1000 | Loss: 0.00006410
Iteration 42/1000 | Loss: 0.00024125
Iteration 43/1000 | Loss: 0.00025249
Iteration 44/1000 | Loss: 0.00044253
Iteration 45/1000 | Loss: 0.00343805
Iteration 46/1000 | Loss: 0.00434683
Iteration 47/1000 | Loss: 0.00412587
Iteration 48/1000 | Loss: 0.00234940
Iteration 49/1000 | Loss: 0.00031937
Iteration 50/1000 | Loss: 0.00038669
Iteration 51/1000 | Loss: 0.00145474
Iteration 52/1000 | Loss: 0.00093037
Iteration 53/1000 | Loss: 0.00016975
Iteration 54/1000 | Loss: 0.00031947
Iteration 55/1000 | Loss: 0.00023262
Iteration 56/1000 | Loss: 0.00026296
Iteration 57/1000 | Loss: 0.00019906
Iteration 58/1000 | Loss: 0.00020726
Iteration 59/1000 | Loss: 0.00033664
Iteration 60/1000 | Loss: 0.00022518
Iteration 61/1000 | Loss: 0.00024748
Iteration 62/1000 | Loss: 0.00023889
Iteration 63/1000 | Loss: 0.00113039
Iteration 64/1000 | Loss: 0.00007734
Iteration 65/1000 | Loss: 0.00004230
Iteration 66/1000 | Loss: 0.00022423
Iteration 67/1000 | Loss: 0.00040390
Iteration 68/1000 | Loss: 0.00054712
Iteration 69/1000 | Loss: 0.00046193
Iteration 70/1000 | Loss: 0.00028581
Iteration 71/1000 | Loss: 0.00168939
Iteration 72/1000 | Loss: 0.00033637
Iteration 73/1000 | Loss: 0.00094750
Iteration 74/1000 | Loss: 0.00030570
Iteration 75/1000 | Loss: 0.00004306
Iteration 76/1000 | Loss: 0.00139692
Iteration 77/1000 | Loss: 0.00083125
Iteration 78/1000 | Loss: 0.00095106
Iteration 79/1000 | Loss: 0.00113375
Iteration 80/1000 | Loss: 0.00067578
Iteration 81/1000 | Loss: 0.00007429
Iteration 82/1000 | Loss: 0.00003369
Iteration 83/1000 | Loss: 0.00002802
Iteration 84/1000 | Loss: 0.00002500
Iteration 85/1000 | Loss: 0.00002307
Iteration 86/1000 | Loss: 0.00018990
Iteration 87/1000 | Loss: 0.00021731
Iteration 88/1000 | Loss: 0.00001959
Iteration 89/1000 | Loss: 0.00001795
Iteration 90/1000 | Loss: 0.00001682
Iteration 91/1000 | Loss: 0.00023809
Iteration 92/1000 | Loss: 0.00025741
Iteration 93/1000 | Loss: 0.00016903
Iteration 94/1000 | Loss: 0.00010557
Iteration 95/1000 | Loss: 0.00007731
Iteration 96/1000 | Loss: 0.00008915
Iteration 97/1000 | Loss: 0.00002621
Iteration 98/1000 | Loss: 0.00013411
Iteration 99/1000 | Loss: 0.00021703
Iteration 100/1000 | Loss: 0.00002365
Iteration 101/1000 | Loss: 0.00001781
Iteration 102/1000 | Loss: 0.00001479
Iteration 103/1000 | Loss: 0.00013783
Iteration 104/1000 | Loss: 0.00005331
Iteration 105/1000 | Loss: 0.00014004
Iteration 106/1000 | Loss: 0.00001383
Iteration 107/1000 | Loss: 0.00001310
Iteration 108/1000 | Loss: 0.00001272
Iteration 109/1000 | Loss: 0.00001240
Iteration 110/1000 | Loss: 0.00001232
Iteration 111/1000 | Loss: 0.00001231
Iteration 112/1000 | Loss: 0.00001231
Iteration 113/1000 | Loss: 0.00001220
Iteration 114/1000 | Loss: 0.00001208
Iteration 115/1000 | Loss: 0.00026058
Iteration 116/1000 | Loss: 0.00019019
Iteration 117/1000 | Loss: 0.00012489
Iteration 118/1000 | Loss: 0.00015660
Iteration 119/1000 | Loss: 0.00011564
Iteration 120/1000 | Loss: 0.00001785
Iteration 121/1000 | Loss: 0.00001501
Iteration 122/1000 | Loss: 0.00018587
Iteration 123/1000 | Loss: 0.00005205
Iteration 124/1000 | Loss: 0.00002237
Iteration 125/1000 | Loss: 0.00031263
Iteration 126/1000 | Loss: 0.00023879
Iteration 127/1000 | Loss: 0.00014084
Iteration 128/1000 | Loss: 0.00008269
Iteration 129/1000 | Loss: 0.00034789
Iteration 130/1000 | Loss: 0.00003151
Iteration 131/1000 | Loss: 0.00001988
Iteration 132/1000 | Loss: 0.00023145
Iteration 133/1000 | Loss: 0.00001927
Iteration 134/1000 | Loss: 0.00001657
Iteration 135/1000 | Loss: 0.00001513
Iteration 136/1000 | Loss: 0.00021369
Iteration 137/1000 | Loss: 0.00008822
Iteration 138/1000 | Loss: 0.00004670
Iteration 139/1000 | Loss: 0.00009671
Iteration 140/1000 | Loss: 0.00001407
Iteration 141/1000 | Loss: 0.00001334
Iteration 142/1000 | Loss: 0.00001306
Iteration 143/1000 | Loss: 0.00001269
Iteration 144/1000 | Loss: 0.00001242
Iteration 145/1000 | Loss: 0.00001197
Iteration 146/1000 | Loss: 0.00001154
Iteration 147/1000 | Loss: 0.00001126
Iteration 148/1000 | Loss: 0.00001098
Iteration 149/1000 | Loss: 0.00001098
Iteration 150/1000 | Loss: 0.00001097
Iteration 151/1000 | Loss: 0.00001097
Iteration 152/1000 | Loss: 0.00001096
Iteration 153/1000 | Loss: 0.00001095
Iteration 154/1000 | Loss: 0.00001093
Iteration 155/1000 | Loss: 0.00001092
Iteration 156/1000 | Loss: 0.00001092
Iteration 157/1000 | Loss: 0.00001091
Iteration 158/1000 | Loss: 0.00001090
Iteration 159/1000 | Loss: 0.00001090
Iteration 160/1000 | Loss: 0.00001089
Iteration 161/1000 | Loss: 0.00001089
Iteration 162/1000 | Loss: 0.00001086
Iteration 163/1000 | Loss: 0.00001086
Iteration 164/1000 | Loss: 0.00001086
Iteration 165/1000 | Loss: 0.00001086
Iteration 166/1000 | Loss: 0.00001086
Iteration 167/1000 | Loss: 0.00001086
Iteration 168/1000 | Loss: 0.00001086
Iteration 169/1000 | Loss: 0.00001086
Iteration 170/1000 | Loss: 0.00001086
Iteration 171/1000 | Loss: 0.00001085
Iteration 172/1000 | Loss: 0.00001085
Iteration 173/1000 | Loss: 0.00001085
Iteration 174/1000 | Loss: 0.00001085
Iteration 175/1000 | Loss: 0.00001085
Iteration 176/1000 | Loss: 0.00001085
Iteration 177/1000 | Loss: 0.00001085
Iteration 178/1000 | Loss: 0.00001085
Iteration 179/1000 | Loss: 0.00001084
Iteration 180/1000 | Loss: 0.00001083
Iteration 181/1000 | Loss: 0.00001083
Iteration 182/1000 | Loss: 0.00001083
Iteration 183/1000 | Loss: 0.00001083
Iteration 184/1000 | Loss: 0.00001083
Iteration 185/1000 | Loss: 0.00001083
Iteration 186/1000 | Loss: 0.00001083
Iteration 187/1000 | Loss: 0.00001083
Iteration 188/1000 | Loss: 0.00001083
Iteration 189/1000 | Loss: 0.00001083
Iteration 190/1000 | Loss: 0.00001082
Iteration 191/1000 | Loss: 0.00001082
Iteration 192/1000 | Loss: 0.00001082
Iteration 193/1000 | Loss: 0.00001082
Iteration 194/1000 | Loss: 0.00001082
Iteration 195/1000 | Loss: 0.00001082
Iteration 196/1000 | Loss: 0.00001082
Iteration 197/1000 | Loss: 0.00001082
Iteration 198/1000 | Loss: 0.00001082
Iteration 199/1000 | Loss: 0.00001082
Iteration 200/1000 | Loss: 0.00001082
Iteration 201/1000 | Loss: 0.00001081
Iteration 202/1000 | Loss: 0.00001081
Iteration 203/1000 | Loss: 0.00001081
Iteration 204/1000 | Loss: 0.00001081
Iteration 205/1000 | Loss: 0.00001081
Iteration 206/1000 | Loss: 0.00001080
Iteration 207/1000 | Loss: 0.00001080
Iteration 208/1000 | Loss: 0.00001079
Iteration 209/1000 | Loss: 0.00001079
Iteration 210/1000 | Loss: 0.00001079
Iteration 211/1000 | Loss: 0.00001079
Iteration 212/1000 | Loss: 0.00001078
Iteration 213/1000 | Loss: 0.00001078
Iteration 214/1000 | Loss: 0.00001078
Iteration 215/1000 | Loss: 0.00001078
Iteration 216/1000 | Loss: 0.00001078
Iteration 217/1000 | Loss: 0.00001078
Iteration 218/1000 | Loss: 0.00001078
Iteration 219/1000 | Loss: 0.00001078
Iteration 220/1000 | Loss: 0.00001078
Iteration 221/1000 | Loss: 0.00001077
Iteration 222/1000 | Loss: 0.00001077
Iteration 223/1000 | Loss: 0.00001077
Iteration 224/1000 | Loss: 0.00001077
Iteration 225/1000 | Loss: 0.00001077
Iteration 226/1000 | Loss: 0.00001077
Iteration 227/1000 | Loss: 0.00001077
Iteration 228/1000 | Loss: 0.00001077
Iteration 229/1000 | Loss: 0.00001076
Iteration 230/1000 | Loss: 0.00001076
Iteration 231/1000 | Loss: 0.00001076
Iteration 232/1000 | Loss: 0.00001076
Iteration 233/1000 | Loss: 0.00001076
Iteration 234/1000 | Loss: 0.00001076
Iteration 235/1000 | Loss: 0.00001076
Iteration 236/1000 | Loss: 0.00001076
Iteration 237/1000 | Loss: 0.00001076
Iteration 238/1000 | Loss: 0.00001076
Iteration 239/1000 | Loss: 0.00001076
Iteration 240/1000 | Loss: 0.00001076
Iteration 241/1000 | Loss: 0.00001076
Iteration 242/1000 | Loss: 0.00001076
Iteration 243/1000 | Loss: 0.00001076
Iteration 244/1000 | Loss: 0.00001076
Iteration 245/1000 | Loss: 0.00001076
Iteration 246/1000 | Loss: 0.00001076
Iteration 247/1000 | Loss: 0.00001076
Iteration 248/1000 | Loss: 0.00001076
Iteration 249/1000 | Loss: 0.00001076
Iteration 250/1000 | Loss: 0.00001076
Iteration 251/1000 | Loss: 0.00001076
Iteration 252/1000 | Loss: 0.00001076
Iteration 253/1000 | Loss: 0.00001076
Iteration 254/1000 | Loss: 0.00001076
Iteration 255/1000 | Loss: 0.00001076
Iteration 256/1000 | Loss: 0.00001076
Iteration 257/1000 | Loss: 0.00001076
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 257. Stopping optimization.
Last 5 losses: [1.0761911653389689e-05, 1.0761911653389689e-05, 1.0761911653389689e-05, 1.0761911653389689e-05, 1.0761911653389689e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0761911653389689e-05

Optimization complete. Final v2v error: 2.7051143646240234 mm

Highest mean error: 4.904623031616211 mm for frame 55

Lowest mean error: 2.418161392211914 mm for frame 29

Saving results

Total time: 254.36139249801636
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_022/1004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_022/1004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_022/1004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00883332
Iteration 2/25 | Loss: 0.00151202
Iteration 3/25 | Loss: 0.00126953
Iteration 4/25 | Loss: 0.00124002
Iteration 5/25 | Loss: 0.00123888
Iteration 6/25 | Loss: 0.00124215
Iteration 7/25 | Loss: 0.00121585
Iteration 8/25 | Loss: 0.00120145
Iteration 9/25 | Loss: 0.00119174
Iteration 10/25 | Loss: 0.00117923
Iteration 11/25 | Loss: 0.00117774
Iteration 12/25 | Loss: 0.00117953
Iteration 13/25 | Loss: 0.00117597
Iteration 14/25 | Loss: 0.00117979
Iteration 15/25 | Loss: 0.00117558
Iteration 16/25 | Loss: 0.00117022
Iteration 17/25 | Loss: 0.00116601
Iteration 18/25 | Loss: 0.00116256
Iteration 19/25 | Loss: 0.00116159
Iteration 20/25 | Loss: 0.00116115
Iteration 21/25 | Loss: 0.00116104
Iteration 22/25 | Loss: 0.00116102
Iteration 23/25 | Loss: 0.00116102
Iteration 24/25 | Loss: 0.00116102
Iteration 25/25 | Loss: 0.00116102

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.24958515
Iteration 2/25 | Loss: 0.00102082
Iteration 3/25 | Loss: 0.00102081
Iteration 4/25 | Loss: 0.00102081
Iteration 5/25 | Loss: 0.00102081
Iteration 6/25 | Loss: 0.00102081
Iteration 7/25 | Loss: 0.00102081
Iteration 8/25 | Loss: 0.00102081
Iteration 9/25 | Loss: 0.00102081
Iteration 10/25 | Loss: 0.00102081
Iteration 11/25 | Loss: 0.00102081
Iteration 12/25 | Loss: 0.00102081
Iteration 13/25 | Loss: 0.00102081
Iteration 14/25 | Loss: 0.00102081
Iteration 15/25 | Loss: 0.00102081
Iteration 16/25 | Loss: 0.00102081
Iteration 17/25 | Loss: 0.00102081
Iteration 18/25 | Loss: 0.00102081
Iteration 19/25 | Loss: 0.00102081
Iteration 20/25 | Loss: 0.00102081
Iteration 21/25 | Loss: 0.00102081
Iteration 22/25 | Loss: 0.00102081
Iteration 23/25 | Loss: 0.00102081
Iteration 24/25 | Loss: 0.00102081
Iteration 25/25 | Loss: 0.00102081

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00102081
Iteration 2/1000 | Loss: 0.00010531
Iteration 3/1000 | Loss: 0.00016283
Iteration 4/1000 | Loss: 0.00003393
Iteration 5/1000 | Loss: 0.00002600
Iteration 6/1000 | Loss: 0.00002380
Iteration 7/1000 | Loss: 0.00002299
Iteration 8/1000 | Loss: 0.00002219
Iteration 9/1000 | Loss: 0.00002147
Iteration 10/1000 | Loss: 0.00002092
Iteration 11/1000 | Loss: 0.00002067
Iteration 12/1000 | Loss: 0.00002047
Iteration 13/1000 | Loss: 0.00002018
Iteration 14/1000 | Loss: 0.00002010
Iteration 15/1000 | Loss: 0.00002005
Iteration 16/1000 | Loss: 0.00002004
Iteration 17/1000 | Loss: 0.00001986
Iteration 18/1000 | Loss: 0.00001980
Iteration 19/1000 | Loss: 0.00001967
Iteration 20/1000 | Loss: 0.00001962
Iteration 21/1000 | Loss: 0.00001957
Iteration 22/1000 | Loss: 0.00001957
Iteration 23/1000 | Loss: 0.00001955
Iteration 24/1000 | Loss: 0.00001954
Iteration 25/1000 | Loss: 0.00001954
Iteration 26/1000 | Loss: 0.00001953
Iteration 27/1000 | Loss: 0.00001952
Iteration 28/1000 | Loss: 0.00001951
Iteration 29/1000 | Loss: 0.00001950
Iteration 30/1000 | Loss: 0.00001949
Iteration 31/1000 | Loss: 0.00001947
Iteration 32/1000 | Loss: 0.00001946
Iteration 33/1000 | Loss: 0.00001946
Iteration 34/1000 | Loss: 0.00001946
Iteration 35/1000 | Loss: 0.00001946
Iteration 36/1000 | Loss: 0.00001945
Iteration 37/1000 | Loss: 0.00001945
Iteration 38/1000 | Loss: 0.00001945
Iteration 39/1000 | Loss: 0.00001945
Iteration 40/1000 | Loss: 0.00001945
Iteration 41/1000 | Loss: 0.00001945
Iteration 42/1000 | Loss: 0.00001945
Iteration 43/1000 | Loss: 0.00001945
Iteration 44/1000 | Loss: 0.00001945
Iteration 45/1000 | Loss: 0.00001945
Iteration 46/1000 | Loss: 0.00001943
Iteration 47/1000 | Loss: 0.00001943
Iteration 48/1000 | Loss: 0.00001942
Iteration 49/1000 | Loss: 0.00001942
Iteration 50/1000 | Loss: 0.00001942
Iteration 51/1000 | Loss: 0.00001942
Iteration 52/1000 | Loss: 0.00001942
Iteration 53/1000 | Loss: 0.00001942
Iteration 54/1000 | Loss: 0.00001942
Iteration 55/1000 | Loss: 0.00001942
Iteration 56/1000 | Loss: 0.00001942
Iteration 57/1000 | Loss: 0.00001942
Iteration 58/1000 | Loss: 0.00001941
Iteration 59/1000 | Loss: 0.00001941
Iteration 60/1000 | Loss: 0.00001941
Iteration 61/1000 | Loss: 0.00001941
Iteration 62/1000 | Loss: 0.00001941
Iteration 63/1000 | Loss: 0.00001941
Iteration 64/1000 | Loss: 0.00001940
Iteration 65/1000 | Loss: 0.00001940
Iteration 66/1000 | Loss: 0.00001940
Iteration 67/1000 | Loss: 0.00001940
Iteration 68/1000 | Loss: 0.00001940
Iteration 69/1000 | Loss: 0.00001940
Iteration 70/1000 | Loss: 0.00001940
Iteration 71/1000 | Loss: 0.00001940
Iteration 72/1000 | Loss: 0.00001940
Iteration 73/1000 | Loss: 0.00001939
Iteration 74/1000 | Loss: 0.00001939
Iteration 75/1000 | Loss: 0.00001939
Iteration 76/1000 | Loss: 0.00001939
Iteration 77/1000 | Loss: 0.00001939
Iteration 78/1000 | Loss: 0.00001938
Iteration 79/1000 | Loss: 0.00001938
Iteration 80/1000 | Loss: 0.00001938
Iteration 81/1000 | Loss: 0.00001938
Iteration 82/1000 | Loss: 0.00001938
Iteration 83/1000 | Loss: 0.00001938
Iteration 84/1000 | Loss: 0.00001938
Iteration 85/1000 | Loss: 0.00001937
Iteration 86/1000 | Loss: 0.00001937
Iteration 87/1000 | Loss: 0.00001937
Iteration 88/1000 | Loss: 0.00001937
Iteration 89/1000 | Loss: 0.00001937
Iteration 90/1000 | Loss: 0.00001937
Iteration 91/1000 | Loss: 0.00001937
Iteration 92/1000 | Loss: 0.00001937
Iteration 93/1000 | Loss: 0.00001937
Iteration 94/1000 | Loss: 0.00001937
Iteration 95/1000 | Loss: 0.00001937
Iteration 96/1000 | Loss: 0.00001937
Iteration 97/1000 | Loss: 0.00001937
Iteration 98/1000 | Loss: 0.00001937
Iteration 99/1000 | Loss: 0.00001937
Iteration 100/1000 | Loss: 0.00001937
Iteration 101/1000 | Loss: 0.00001937
Iteration 102/1000 | Loss: 0.00001937
Iteration 103/1000 | Loss: 0.00001937
Iteration 104/1000 | Loss: 0.00001937
Iteration 105/1000 | Loss: 0.00001937
Iteration 106/1000 | Loss: 0.00001937
Iteration 107/1000 | Loss: 0.00001937
Iteration 108/1000 | Loss: 0.00001937
Iteration 109/1000 | Loss: 0.00001937
Iteration 110/1000 | Loss: 0.00001937
Iteration 111/1000 | Loss: 0.00001937
Iteration 112/1000 | Loss: 0.00001937
Iteration 113/1000 | Loss: 0.00001937
Iteration 114/1000 | Loss: 0.00001937
Iteration 115/1000 | Loss: 0.00001937
Iteration 116/1000 | Loss: 0.00001937
Iteration 117/1000 | Loss: 0.00001937
Iteration 118/1000 | Loss: 0.00001937
Iteration 119/1000 | Loss: 0.00001937
Iteration 120/1000 | Loss: 0.00001937
Iteration 121/1000 | Loss: 0.00001937
Iteration 122/1000 | Loss: 0.00001937
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 122. Stopping optimization.
Last 5 losses: [1.9370951122255065e-05, 1.9370951122255065e-05, 1.9370951122255065e-05, 1.9370951122255065e-05, 1.9370951122255065e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9370951122255065e-05

Optimization complete. Final v2v error: 3.663370370864868 mm

Highest mean error: 5.669455051422119 mm for frame 106

Lowest mean error: 3.114799976348877 mm for frame 70

Saving results

Total time: 72.64205408096313
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_022/1086/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_022/1086.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_022/1086
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00535280
Iteration 2/25 | Loss: 0.00114832
Iteration 3/25 | Loss: 0.00108989
Iteration 4/25 | Loss: 0.00108064
Iteration 5/25 | Loss: 0.00107754
Iteration 6/25 | Loss: 0.00107702
Iteration 7/25 | Loss: 0.00107702
Iteration 8/25 | Loss: 0.00107702
Iteration 9/25 | Loss: 0.00107702
Iteration 10/25 | Loss: 0.00107702
Iteration 11/25 | Loss: 0.00107702
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010770214721560478, 0.0010770214721560478, 0.0010770214721560478, 0.0010770214721560478, 0.0010770214721560478]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010770214721560478

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 6.38290691
Iteration 2/25 | Loss: 0.00077386
Iteration 3/25 | Loss: 0.00077385
Iteration 4/25 | Loss: 0.00077385
Iteration 5/25 | Loss: 0.00077385
Iteration 6/25 | Loss: 0.00077385
Iteration 7/25 | Loss: 0.00077384
Iteration 8/25 | Loss: 0.00077384
Iteration 9/25 | Loss: 0.00077384
Iteration 10/25 | Loss: 0.00077384
Iteration 11/25 | Loss: 0.00077384
Iteration 12/25 | Loss: 0.00077384
Iteration 13/25 | Loss: 0.00077384
Iteration 14/25 | Loss: 0.00077384
Iteration 15/25 | Loss: 0.00077384
Iteration 16/25 | Loss: 0.00077384
Iteration 17/25 | Loss: 0.00077384
Iteration 18/25 | Loss: 0.00077384
Iteration 19/25 | Loss: 0.00077384
Iteration 20/25 | Loss: 0.00077384
Iteration 21/25 | Loss: 0.00077384
Iteration 22/25 | Loss: 0.00077384
Iteration 23/25 | Loss: 0.00077384
Iteration 24/25 | Loss: 0.00077384
Iteration 25/25 | Loss: 0.00077384

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00077384
Iteration 2/1000 | Loss: 0.00001956
Iteration 3/1000 | Loss: 0.00001450
Iteration 4/1000 | Loss: 0.00001328
Iteration 5/1000 | Loss: 0.00001266
Iteration 6/1000 | Loss: 0.00001226
Iteration 7/1000 | Loss: 0.00001180
Iteration 8/1000 | Loss: 0.00001152
Iteration 9/1000 | Loss: 0.00001131
Iteration 10/1000 | Loss: 0.00001108
Iteration 11/1000 | Loss: 0.00001100
Iteration 12/1000 | Loss: 0.00001093
Iteration 13/1000 | Loss: 0.00001090
Iteration 14/1000 | Loss: 0.00001090
Iteration 15/1000 | Loss: 0.00001088
Iteration 16/1000 | Loss: 0.00001087
Iteration 17/1000 | Loss: 0.00001079
Iteration 18/1000 | Loss: 0.00001077
Iteration 19/1000 | Loss: 0.00001076
Iteration 20/1000 | Loss: 0.00001075
Iteration 21/1000 | Loss: 0.00001075
Iteration 22/1000 | Loss: 0.00001074
Iteration 23/1000 | Loss: 0.00001067
Iteration 24/1000 | Loss: 0.00001064
Iteration 25/1000 | Loss: 0.00001063
Iteration 26/1000 | Loss: 0.00001062
Iteration 27/1000 | Loss: 0.00001062
Iteration 28/1000 | Loss: 0.00001062
Iteration 29/1000 | Loss: 0.00001062
Iteration 30/1000 | Loss: 0.00001061
Iteration 31/1000 | Loss: 0.00001061
Iteration 32/1000 | Loss: 0.00001061
Iteration 33/1000 | Loss: 0.00001060
Iteration 34/1000 | Loss: 0.00001060
Iteration 35/1000 | Loss: 0.00001059
Iteration 36/1000 | Loss: 0.00001059
Iteration 37/1000 | Loss: 0.00001059
Iteration 38/1000 | Loss: 0.00001058
Iteration 39/1000 | Loss: 0.00001058
Iteration 40/1000 | Loss: 0.00001058
Iteration 41/1000 | Loss: 0.00001057
Iteration 42/1000 | Loss: 0.00001056
Iteration 43/1000 | Loss: 0.00001056
Iteration 44/1000 | Loss: 0.00001056
Iteration 45/1000 | Loss: 0.00001055
Iteration 46/1000 | Loss: 0.00001054
Iteration 47/1000 | Loss: 0.00001054
Iteration 48/1000 | Loss: 0.00001053
Iteration 49/1000 | Loss: 0.00001053
Iteration 50/1000 | Loss: 0.00001052
Iteration 51/1000 | Loss: 0.00001052
Iteration 52/1000 | Loss: 0.00001051
Iteration 53/1000 | Loss: 0.00001050
Iteration 54/1000 | Loss: 0.00001049
Iteration 55/1000 | Loss: 0.00001046
Iteration 56/1000 | Loss: 0.00001045
Iteration 57/1000 | Loss: 0.00001044
Iteration 58/1000 | Loss: 0.00001044
Iteration 59/1000 | Loss: 0.00001044
Iteration 60/1000 | Loss: 0.00001043
Iteration 61/1000 | Loss: 0.00001043
Iteration 62/1000 | Loss: 0.00001042
Iteration 63/1000 | Loss: 0.00001042
Iteration 64/1000 | Loss: 0.00001041
Iteration 65/1000 | Loss: 0.00001041
Iteration 66/1000 | Loss: 0.00001041
Iteration 67/1000 | Loss: 0.00001041
Iteration 68/1000 | Loss: 0.00001041
Iteration 69/1000 | Loss: 0.00001040
Iteration 70/1000 | Loss: 0.00001040
Iteration 71/1000 | Loss: 0.00001040
Iteration 72/1000 | Loss: 0.00001039
Iteration 73/1000 | Loss: 0.00001038
Iteration 74/1000 | Loss: 0.00001037
Iteration 75/1000 | Loss: 0.00001037
Iteration 76/1000 | Loss: 0.00001037
Iteration 77/1000 | Loss: 0.00001037
Iteration 78/1000 | Loss: 0.00001037
Iteration 79/1000 | Loss: 0.00001037
Iteration 80/1000 | Loss: 0.00001037
Iteration 81/1000 | Loss: 0.00001036
Iteration 82/1000 | Loss: 0.00001035
Iteration 83/1000 | Loss: 0.00001035
Iteration 84/1000 | Loss: 0.00001035
Iteration 85/1000 | Loss: 0.00001035
Iteration 86/1000 | Loss: 0.00001035
Iteration 87/1000 | Loss: 0.00001035
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 87. Stopping optimization.
Last 5 losses: [1.0349580406909809e-05, 1.0349580406909809e-05, 1.0349580406909809e-05, 1.0349580406909809e-05, 1.0349580406909809e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0349580406909809e-05

Optimization complete. Final v2v error: 2.748337745666504 mm

Highest mean error: 3.568197250366211 mm for frame 60

Lowest mean error: 2.476285457611084 mm for frame 28

Saving results

Total time: 34.23117184638977
