Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=293, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 16408-16463
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_39_us_1277/0013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_39_us_1277/0013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_39_us_1277/0013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00880167
Iteration 2/25 | Loss: 0.00127652
Iteration 3/25 | Loss: 0.00116897
Iteration 4/25 | Loss: 0.00116431
Iteration 5/25 | Loss: 0.00116300
Iteration 6/25 | Loss: 0.00116300
Iteration 7/25 | Loss: 0.00116300
Iteration 8/25 | Loss: 0.00116300
Iteration 9/25 | Loss: 0.00116300
Iteration 10/25 | Loss: 0.00116300
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.001163002336397767, 0.001163002336397767, 0.001163002336397767, 0.001163002336397767, 0.001163002336397767]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001163002336397767

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.30837452
Iteration 2/25 | Loss: 0.00100592
Iteration 3/25 | Loss: 0.00100592
Iteration 4/25 | Loss: 0.00100592
Iteration 5/25 | Loss: 0.00100592
Iteration 6/25 | Loss: 0.00100592
Iteration 7/25 | Loss: 0.00100592
Iteration 8/25 | Loss: 0.00100592
Iteration 9/25 | Loss: 0.00100592
Iteration 10/25 | Loss: 0.00100592
Iteration 11/25 | Loss: 0.00100592
Iteration 12/25 | Loss: 0.00100592
Iteration 13/25 | Loss: 0.00100592
Iteration 14/25 | Loss: 0.00100592
Iteration 15/25 | Loss: 0.00100592
Iteration 16/25 | Loss: 0.00100592
Iteration 17/25 | Loss: 0.00100592
Iteration 18/25 | Loss: 0.00100592
Iteration 19/25 | Loss: 0.00100592
Iteration 20/25 | Loss: 0.00100592
Iteration 21/25 | Loss: 0.00100592
Iteration 22/25 | Loss: 0.00100592
Iteration 23/25 | Loss: 0.00100592
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0010059208143502474, 0.0010059208143502474, 0.0010059208143502474, 0.0010059208143502474, 0.0010059208143502474]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010059208143502474

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00100592
Iteration 2/1000 | Loss: 0.00004427
Iteration 3/1000 | Loss: 0.00002559
Iteration 4/1000 | Loss: 0.00002037
Iteration 5/1000 | Loss: 0.00001917
Iteration 6/1000 | Loss: 0.00001825
Iteration 7/1000 | Loss: 0.00001764
Iteration 8/1000 | Loss: 0.00001725
Iteration 9/1000 | Loss: 0.00001694
Iteration 10/1000 | Loss: 0.00001676
Iteration 11/1000 | Loss: 0.00001671
Iteration 12/1000 | Loss: 0.00001653
Iteration 13/1000 | Loss: 0.00001641
Iteration 14/1000 | Loss: 0.00001641
Iteration 15/1000 | Loss: 0.00001636
Iteration 16/1000 | Loss: 0.00001636
Iteration 17/1000 | Loss: 0.00001636
Iteration 18/1000 | Loss: 0.00001636
Iteration 19/1000 | Loss: 0.00001636
Iteration 20/1000 | Loss: 0.00001636
Iteration 21/1000 | Loss: 0.00001634
Iteration 22/1000 | Loss: 0.00001633
Iteration 23/1000 | Loss: 0.00001632
Iteration 24/1000 | Loss: 0.00001631
Iteration 25/1000 | Loss: 0.00001631
Iteration 26/1000 | Loss: 0.00001630
Iteration 27/1000 | Loss: 0.00001630
Iteration 28/1000 | Loss: 0.00001629
Iteration 29/1000 | Loss: 0.00001629
Iteration 30/1000 | Loss: 0.00001628
Iteration 31/1000 | Loss: 0.00001628
Iteration 32/1000 | Loss: 0.00001628
Iteration 33/1000 | Loss: 0.00001627
Iteration 34/1000 | Loss: 0.00001627
Iteration 35/1000 | Loss: 0.00001627
Iteration 36/1000 | Loss: 0.00001627
Iteration 37/1000 | Loss: 0.00001627
Iteration 38/1000 | Loss: 0.00001626
Iteration 39/1000 | Loss: 0.00001626
Iteration 40/1000 | Loss: 0.00001625
Iteration 41/1000 | Loss: 0.00001624
Iteration 42/1000 | Loss: 0.00001623
Iteration 43/1000 | Loss: 0.00001623
Iteration 44/1000 | Loss: 0.00001622
Iteration 45/1000 | Loss: 0.00001622
Iteration 46/1000 | Loss: 0.00001622
Iteration 47/1000 | Loss: 0.00001622
Iteration 48/1000 | Loss: 0.00001622
Iteration 49/1000 | Loss: 0.00001622
Iteration 50/1000 | Loss: 0.00001622
Iteration 51/1000 | Loss: 0.00001622
Iteration 52/1000 | Loss: 0.00001622
Iteration 53/1000 | Loss: 0.00001621
Iteration 54/1000 | Loss: 0.00001620
Iteration 55/1000 | Loss: 0.00001619
Iteration 56/1000 | Loss: 0.00001619
Iteration 57/1000 | Loss: 0.00001619
Iteration 58/1000 | Loss: 0.00001619
Iteration 59/1000 | Loss: 0.00001618
Iteration 60/1000 | Loss: 0.00001618
Iteration 61/1000 | Loss: 0.00001617
Iteration 62/1000 | Loss: 0.00001617
Iteration 63/1000 | Loss: 0.00001617
Iteration 64/1000 | Loss: 0.00001616
Iteration 65/1000 | Loss: 0.00001616
Iteration 66/1000 | Loss: 0.00001616
Iteration 67/1000 | Loss: 0.00001616
Iteration 68/1000 | Loss: 0.00001616
Iteration 69/1000 | Loss: 0.00001616
Iteration 70/1000 | Loss: 0.00001616
Iteration 71/1000 | Loss: 0.00001615
Iteration 72/1000 | Loss: 0.00001615
Iteration 73/1000 | Loss: 0.00001615
Iteration 74/1000 | Loss: 0.00001615
Iteration 75/1000 | Loss: 0.00001615
Iteration 76/1000 | Loss: 0.00001614
Iteration 77/1000 | Loss: 0.00001614
Iteration 78/1000 | Loss: 0.00001613
Iteration 79/1000 | Loss: 0.00001613
Iteration 80/1000 | Loss: 0.00001613
Iteration 81/1000 | Loss: 0.00001613
Iteration 82/1000 | Loss: 0.00001612
Iteration 83/1000 | Loss: 0.00001612
Iteration 84/1000 | Loss: 0.00001612
Iteration 85/1000 | Loss: 0.00001612
Iteration 86/1000 | Loss: 0.00001612
Iteration 87/1000 | Loss: 0.00001611
Iteration 88/1000 | Loss: 0.00001611
Iteration 89/1000 | Loss: 0.00001611
Iteration 90/1000 | Loss: 0.00001611
Iteration 91/1000 | Loss: 0.00001610
Iteration 92/1000 | Loss: 0.00001610
Iteration 93/1000 | Loss: 0.00001610
Iteration 94/1000 | Loss: 0.00001610
Iteration 95/1000 | Loss: 0.00001610
Iteration 96/1000 | Loss: 0.00001610
Iteration 97/1000 | Loss: 0.00001609
Iteration 98/1000 | Loss: 0.00001609
Iteration 99/1000 | Loss: 0.00001609
Iteration 100/1000 | Loss: 0.00001609
Iteration 101/1000 | Loss: 0.00001609
Iteration 102/1000 | Loss: 0.00001609
Iteration 103/1000 | Loss: 0.00001609
Iteration 104/1000 | Loss: 0.00001609
Iteration 105/1000 | Loss: 0.00001609
Iteration 106/1000 | Loss: 0.00001608
Iteration 107/1000 | Loss: 0.00001608
Iteration 108/1000 | Loss: 0.00001608
Iteration 109/1000 | Loss: 0.00001608
Iteration 110/1000 | Loss: 0.00001608
Iteration 111/1000 | Loss: 0.00001608
Iteration 112/1000 | Loss: 0.00001608
Iteration 113/1000 | Loss: 0.00001608
Iteration 114/1000 | Loss: 0.00001608
Iteration 115/1000 | Loss: 0.00001608
Iteration 116/1000 | Loss: 0.00001607
Iteration 117/1000 | Loss: 0.00001607
Iteration 118/1000 | Loss: 0.00001607
Iteration 119/1000 | Loss: 0.00001607
Iteration 120/1000 | Loss: 0.00001607
Iteration 121/1000 | Loss: 0.00001607
Iteration 122/1000 | Loss: 0.00001607
Iteration 123/1000 | Loss: 0.00001607
Iteration 124/1000 | Loss: 0.00001607
Iteration 125/1000 | Loss: 0.00001607
Iteration 126/1000 | Loss: 0.00001606
Iteration 127/1000 | Loss: 0.00001606
Iteration 128/1000 | Loss: 0.00001606
Iteration 129/1000 | Loss: 0.00001606
Iteration 130/1000 | Loss: 0.00001606
Iteration 131/1000 | Loss: 0.00001606
Iteration 132/1000 | Loss: 0.00001606
Iteration 133/1000 | Loss: 0.00001606
Iteration 134/1000 | Loss: 0.00001606
Iteration 135/1000 | Loss: 0.00001606
Iteration 136/1000 | Loss: 0.00001606
Iteration 137/1000 | Loss: 0.00001606
Iteration 138/1000 | Loss: 0.00001605
Iteration 139/1000 | Loss: 0.00001605
Iteration 140/1000 | Loss: 0.00001605
Iteration 141/1000 | Loss: 0.00001605
Iteration 142/1000 | Loss: 0.00001605
Iteration 143/1000 | Loss: 0.00001605
Iteration 144/1000 | Loss: 0.00001605
Iteration 145/1000 | Loss: 0.00001605
Iteration 146/1000 | Loss: 0.00001605
Iteration 147/1000 | Loss: 0.00001605
Iteration 148/1000 | Loss: 0.00001605
Iteration 149/1000 | Loss: 0.00001605
Iteration 150/1000 | Loss: 0.00001605
Iteration 151/1000 | Loss: 0.00001605
Iteration 152/1000 | Loss: 0.00001605
Iteration 153/1000 | Loss: 0.00001605
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 153. Stopping optimization.
Last 5 losses: [1.6051439160946757e-05, 1.6051439160946757e-05, 1.6051439160946757e-05, 1.6051439160946757e-05, 1.6051439160946757e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6051439160946757e-05

Optimization complete. Final v2v error: 3.4266843795776367 mm

Highest mean error: 3.7478814125061035 mm for frame 56

Lowest mean error: 3.1263766288757324 mm for frame 4

Saving results

Total time: 36.473883628845215
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_39_us_1277/0005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_39_us_1277/0005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_39_us_1277/0005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00895556
Iteration 2/25 | Loss: 0.00154040
Iteration 3/25 | Loss: 0.00127374
Iteration 4/25 | Loss: 0.00125297
Iteration 5/25 | Loss: 0.00124910
Iteration 6/25 | Loss: 0.00124879
Iteration 7/25 | Loss: 0.00124879
Iteration 8/25 | Loss: 0.00124879
Iteration 9/25 | Loss: 0.00124879
Iteration 10/25 | Loss: 0.00124879
Iteration 11/25 | Loss: 0.00124879
Iteration 12/25 | Loss: 0.00124879
Iteration 13/25 | Loss: 0.00124879
Iteration 14/25 | Loss: 0.00124879
Iteration 15/25 | Loss: 0.00124879
Iteration 16/25 | Loss: 0.00124879
Iteration 17/25 | Loss: 0.00124879
Iteration 18/25 | Loss: 0.00124879
Iteration 19/25 | Loss: 0.00124879
Iteration 20/25 | Loss: 0.00124879
Iteration 21/25 | Loss: 0.00124879
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0012487912317737937, 0.0012487912317737937, 0.0012487912317737937, 0.0012487912317737937, 0.0012487912317737937]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012487912317737937

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.27286494
Iteration 2/25 | Loss: 0.00078454
Iteration 3/25 | Loss: 0.00078453
Iteration 4/25 | Loss: 0.00078453
Iteration 5/25 | Loss: 0.00078453
Iteration 6/25 | Loss: 0.00078453
Iteration 7/25 | Loss: 0.00078453
Iteration 8/25 | Loss: 0.00078453
Iteration 9/25 | Loss: 0.00078453
Iteration 10/25 | Loss: 0.00078453
Iteration 11/25 | Loss: 0.00078453
Iteration 12/25 | Loss: 0.00078453
Iteration 13/25 | Loss: 0.00078453
Iteration 14/25 | Loss: 0.00078453
Iteration 15/25 | Loss: 0.00078453
Iteration 16/25 | Loss: 0.00078453
Iteration 17/25 | Loss: 0.00078453
Iteration 18/25 | Loss: 0.00078453
Iteration 19/25 | Loss: 0.00078453
Iteration 20/25 | Loss: 0.00078453
Iteration 21/25 | Loss: 0.00078453
Iteration 22/25 | Loss: 0.00078453
Iteration 23/25 | Loss: 0.00078453
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.000784530711825937, 0.000784530711825937, 0.000784530711825937, 0.000784530711825937, 0.000784530711825937]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000784530711825937

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00078453
Iteration 2/1000 | Loss: 0.00004236
Iteration 3/1000 | Loss: 0.00002779
Iteration 4/1000 | Loss: 0.00002394
Iteration 5/1000 | Loss: 0.00002204
Iteration 6/1000 | Loss: 0.00002128
Iteration 7/1000 | Loss: 0.00002075
Iteration 8/1000 | Loss: 0.00002033
Iteration 9/1000 | Loss: 0.00002008
Iteration 10/1000 | Loss: 0.00001995
Iteration 11/1000 | Loss: 0.00001995
Iteration 12/1000 | Loss: 0.00001995
Iteration 13/1000 | Loss: 0.00001992
Iteration 14/1000 | Loss: 0.00001985
Iteration 15/1000 | Loss: 0.00001974
Iteration 16/1000 | Loss: 0.00001974
Iteration 17/1000 | Loss: 0.00001974
Iteration 18/1000 | Loss: 0.00001972
Iteration 19/1000 | Loss: 0.00001969
Iteration 20/1000 | Loss: 0.00001969
Iteration 21/1000 | Loss: 0.00001969
Iteration 22/1000 | Loss: 0.00001968
Iteration 23/1000 | Loss: 0.00001968
Iteration 24/1000 | Loss: 0.00001965
Iteration 25/1000 | Loss: 0.00001965
Iteration 26/1000 | Loss: 0.00001963
Iteration 27/1000 | Loss: 0.00001963
Iteration 28/1000 | Loss: 0.00001962
Iteration 29/1000 | Loss: 0.00001962
Iteration 30/1000 | Loss: 0.00001962
Iteration 31/1000 | Loss: 0.00001962
Iteration 32/1000 | Loss: 0.00001961
Iteration 33/1000 | Loss: 0.00001960
Iteration 34/1000 | Loss: 0.00001960
Iteration 35/1000 | Loss: 0.00001959
Iteration 36/1000 | Loss: 0.00001959
Iteration 37/1000 | Loss: 0.00001958
Iteration 38/1000 | Loss: 0.00001958
Iteration 39/1000 | Loss: 0.00001958
Iteration 40/1000 | Loss: 0.00001958
Iteration 41/1000 | Loss: 0.00001958
Iteration 42/1000 | Loss: 0.00001958
Iteration 43/1000 | Loss: 0.00001958
Iteration 44/1000 | Loss: 0.00001958
Iteration 45/1000 | Loss: 0.00001958
Iteration 46/1000 | Loss: 0.00001958
Iteration 47/1000 | Loss: 0.00001957
Iteration 48/1000 | Loss: 0.00001957
Iteration 49/1000 | Loss: 0.00001957
Iteration 50/1000 | Loss: 0.00001957
Iteration 51/1000 | Loss: 0.00001957
Iteration 52/1000 | Loss: 0.00001957
Iteration 53/1000 | Loss: 0.00001956
Iteration 54/1000 | Loss: 0.00001956
Iteration 55/1000 | Loss: 0.00001956
Iteration 56/1000 | Loss: 0.00001956
Iteration 57/1000 | Loss: 0.00001956
Iteration 58/1000 | Loss: 0.00001956
Iteration 59/1000 | Loss: 0.00001956
Iteration 60/1000 | Loss: 0.00001956
Iteration 61/1000 | Loss: 0.00001956
Iteration 62/1000 | Loss: 0.00001956
Iteration 63/1000 | Loss: 0.00001956
Iteration 64/1000 | Loss: 0.00001956
Iteration 65/1000 | Loss: 0.00001956
Iteration 66/1000 | Loss: 0.00001955
Iteration 67/1000 | Loss: 0.00001955
Iteration 68/1000 | Loss: 0.00001955
Iteration 69/1000 | Loss: 0.00001955
Iteration 70/1000 | Loss: 0.00001955
Iteration 71/1000 | Loss: 0.00001955
Iteration 72/1000 | Loss: 0.00001955
Iteration 73/1000 | Loss: 0.00001955
Iteration 74/1000 | Loss: 0.00001955
Iteration 75/1000 | Loss: 0.00001955
Iteration 76/1000 | Loss: 0.00001954
Iteration 77/1000 | Loss: 0.00001954
Iteration 78/1000 | Loss: 0.00001953
Iteration 79/1000 | Loss: 0.00001953
Iteration 80/1000 | Loss: 0.00001953
Iteration 81/1000 | Loss: 0.00001953
Iteration 82/1000 | Loss: 0.00001953
Iteration 83/1000 | Loss: 0.00001953
Iteration 84/1000 | Loss: 0.00001953
Iteration 85/1000 | Loss: 0.00001953
Iteration 86/1000 | Loss: 0.00001952
Iteration 87/1000 | Loss: 0.00001952
Iteration 88/1000 | Loss: 0.00001952
Iteration 89/1000 | Loss: 0.00001952
Iteration 90/1000 | Loss: 0.00001952
Iteration 91/1000 | Loss: 0.00001952
Iteration 92/1000 | Loss: 0.00001952
Iteration 93/1000 | Loss: 0.00001952
Iteration 94/1000 | Loss: 0.00001952
Iteration 95/1000 | Loss: 0.00001952
Iteration 96/1000 | Loss: 0.00001952
Iteration 97/1000 | Loss: 0.00001952
Iteration 98/1000 | Loss: 0.00001952
Iteration 99/1000 | Loss: 0.00001952
Iteration 100/1000 | Loss: 0.00001952
Iteration 101/1000 | Loss: 0.00001952
Iteration 102/1000 | Loss: 0.00001952
Iteration 103/1000 | Loss: 0.00001952
Iteration 104/1000 | Loss: 0.00001952
Iteration 105/1000 | Loss: 0.00001952
Iteration 106/1000 | Loss: 0.00001952
Iteration 107/1000 | Loss: 0.00001952
Iteration 108/1000 | Loss: 0.00001952
Iteration 109/1000 | Loss: 0.00001952
Iteration 110/1000 | Loss: 0.00001952
Iteration 111/1000 | Loss: 0.00001952
Iteration 112/1000 | Loss: 0.00001952
Iteration 113/1000 | Loss: 0.00001952
Iteration 114/1000 | Loss: 0.00001952
Iteration 115/1000 | Loss: 0.00001952
Iteration 116/1000 | Loss: 0.00001952
Iteration 117/1000 | Loss: 0.00001952
Iteration 118/1000 | Loss: 0.00001952
Iteration 119/1000 | Loss: 0.00001952
Iteration 120/1000 | Loss: 0.00001952
Iteration 121/1000 | Loss: 0.00001952
Iteration 122/1000 | Loss: 0.00001952
Iteration 123/1000 | Loss: 0.00001952
Iteration 124/1000 | Loss: 0.00001952
Iteration 125/1000 | Loss: 0.00001952
Iteration 126/1000 | Loss: 0.00001952
Iteration 127/1000 | Loss: 0.00001952
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 127. Stopping optimization.
Last 5 losses: [1.951808008016087e-05, 1.951808008016087e-05, 1.951808008016087e-05, 1.951808008016087e-05, 1.951808008016087e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.951808008016087e-05

Optimization complete. Final v2v error: 3.8646938800811768 mm

Highest mean error: 4.15510892868042 mm for frame 113

Lowest mean error: 3.555377960205078 mm for frame 103

Saving results

Total time: 30.925443410873413
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_39_us_1277/0010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_39_us_1277/0010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_39_us_1277/0010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00827487
Iteration 2/25 | Loss: 0.00173163
Iteration 3/25 | Loss: 0.00143109
Iteration 4/25 | Loss: 0.00133478
Iteration 5/25 | Loss: 0.00135623
Iteration 6/25 | Loss: 0.00130146
Iteration 7/25 | Loss: 0.00129315
Iteration 8/25 | Loss: 0.00126479
Iteration 9/25 | Loss: 0.00126068
Iteration 10/25 | Loss: 0.00125834
Iteration 11/25 | Loss: 0.00124835
Iteration 12/25 | Loss: 0.00124637
Iteration 13/25 | Loss: 0.00125590
Iteration 14/25 | Loss: 0.00123710
Iteration 15/25 | Loss: 0.00123794
Iteration 16/25 | Loss: 0.00123553
Iteration 17/25 | Loss: 0.00123477
Iteration 18/25 | Loss: 0.00123618
Iteration 19/25 | Loss: 0.00123283
Iteration 20/25 | Loss: 0.00122978
Iteration 21/25 | Loss: 0.00122902
Iteration 22/25 | Loss: 0.00122861
Iteration 23/25 | Loss: 0.00123103
Iteration 24/25 | Loss: 0.00123115
Iteration 25/25 | Loss: 0.00123003

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.55053937
Iteration 2/25 | Loss: 0.00136021
Iteration 3/25 | Loss: 0.00136021
Iteration 4/25 | Loss: 0.00136021
Iteration 5/25 | Loss: 0.00136020
Iteration 6/25 | Loss: 0.00136020
Iteration 7/25 | Loss: 0.00136020
Iteration 8/25 | Loss: 0.00136020
Iteration 9/25 | Loss: 0.00136020
Iteration 10/25 | Loss: 0.00136020
Iteration 11/25 | Loss: 0.00136020
Iteration 12/25 | Loss: 0.00134880
Iteration 13/25 | Loss: 0.00134880
Iteration 14/25 | Loss: 0.00134880
Iteration 15/25 | Loss: 0.00134880
Iteration 16/25 | Loss: 0.00134880
Iteration 17/25 | Loss: 0.00134880
Iteration 18/25 | Loss: 0.00134880
Iteration 19/25 | Loss: 0.00134880
Iteration 20/25 | Loss: 0.00134880
Iteration 21/25 | Loss: 0.00134880
Iteration 22/25 | Loss: 0.00134880
Iteration 23/25 | Loss: 0.00134880
Iteration 24/25 | Loss: 0.00134880
Iteration 25/25 | Loss: 0.00134880

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00134880
Iteration 2/1000 | Loss: 0.00011382
Iteration 3/1000 | Loss: 0.00020152
Iteration 4/1000 | Loss: 0.00005208
Iteration 5/1000 | Loss: 0.00007418
Iteration 6/1000 | Loss: 0.00007436
Iteration 7/1000 | Loss: 0.00003532
Iteration 8/1000 | Loss: 0.00003302
Iteration 9/1000 | Loss: 0.00003110
Iteration 10/1000 | Loss: 0.00003047
Iteration 11/1000 | Loss: 0.00002979
Iteration 12/1000 | Loss: 0.00002928
Iteration 13/1000 | Loss: 0.00002892
Iteration 14/1000 | Loss: 0.00049551
Iteration 15/1000 | Loss: 0.00081965
Iteration 16/1000 | Loss: 0.00004445
Iteration 17/1000 | Loss: 0.00003370
Iteration 18/1000 | Loss: 0.00002930
Iteration 19/1000 | Loss: 0.00002600
Iteration 20/1000 | Loss: 0.00002361
Iteration 21/1000 | Loss: 0.00002268
Iteration 22/1000 | Loss: 0.00002216
Iteration 23/1000 | Loss: 0.00002190
Iteration 24/1000 | Loss: 0.00002171
Iteration 25/1000 | Loss: 0.00002156
Iteration 26/1000 | Loss: 0.00002139
Iteration 27/1000 | Loss: 0.00002140
Iteration 28/1000 | Loss: 0.00002139
Iteration 29/1000 | Loss: 0.00002122
Iteration 30/1000 | Loss: 0.00002122
Iteration 31/1000 | Loss: 0.00002121
Iteration 32/1000 | Loss: 0.00002121
Iteration 33/1000 | Loss: 0.00002118
Iteration 34/1000 | Loss: 0.00002114
Iteration 35/1000 | Loss: 0.00002114
Iteration 36/1000 | Loss: 0.00002111
Iteration 37/1000 | Loss: 0.00002106
Iteration 38/1000 | Loss: 0.00002106
Iteration 39/1000 | Loss: 0.00002106
Iteration 40/1000 | Loss: 0.00002105
Iteration 41/1000 | Loss: 0.00002105
Iteration 42/1000 | Loss: 0.00002105
Iteration 43/1000 | Loss: 0.00002105
Iteration 44/1000 | Loss: 0.00002105
Iteration 45/1000 | Loss: 0.00002105
Iteration 46/1000 | Loss: 0.00002105
Iteration 47/1000 | Loss: 0.00002105
Iteration 48/1000 | Loss: 0.00002105
Iteration 49/1000 | Loss: 0.00002104
Iteration 50/1000 | Loss: 0.00002104
Iteration 51/1000 | Loss: 0.00002104
Iteration 52/1000 | Loss: 0.00002103
Iteration 53/1000 | Loss: 0.00002103
Iteration 54/1000 | Loss: 0.00002103
Iteration 55/1000 | Loss: 0.00002103
Iteration 56/1000 | Loss: 0.00002103
Iteration 57/1000 | Loss: 0.00002103
Iteration 58/1000 | Loss: 0.00002102
Iteration 59/1000 | Loss: 0.00002102
Iteration 60/1000 | Loss: 0.00002102
Iteration 61/1000 | Loss: 0.00002102
Iteration 62/1000 | Loss: 0.00002102
Iteration 63/1000 | Loss: 0.00002102
Iteration 64/1000 | Loss: 0.00002102
Iteration 65/1000 | Loss: 0.00002102
Iteration 66/1000 | Loss: 0.00002102
Iteration 67/1000 | Loss: 0.00002102
Iteration 68/1000 | Loss: 0.00002102
Iteration 69/1000 | Loss: 0.00002101
Iteration 70/1000 | Loss: 0.00002101
Iteration 71/1000 | Loss: 0.00002101
Iteration 72/1000 | Loss: 0.00002101
Iteration 73/1000 | Loss: 0.00002101
Iteration 74/1000 | Loss: 0.00002101
Iteration 75/1000 | Loss: 0.00002107
Iteration 76/1000 | Loss: 0.00002106
Iteration 77/1000 | Loss: 0.00002105
Iteration 78/1000 | Loss: 0.00002105
Iteration 79/1000 | Loss: 0.00002105
Iteration 80/1000 | Loss: 0.00002104
Iteration 81/1000 | Loss: 0.00002104
Iteration 82/1000 | Loss: 0.00002104
Iteration 83/1000 | Loss: 0.00002104
Iteration 84/1000 | Loss: 0.00002104
Iteration 85/1000 | Loss: 0.00002103
Iteration 86/1000 | Loss: 0.00002103
Iteration 87/1000 | Loss: 0.00002103
Iteration 88/1000 | Loss: 0.00002103
Iteration 89/1000 | Loss: 0.00002103
Iteration 90/1000 | Loss: 0.00002103
Iteration 91/1000 | Loss: 0.00002103
Iteration 92/1000 | Loss: 0.00002103
Iteration 93/1000 | Loss: 0.00002102
Iteration 94/1000 | Loss: 0.00002102
Iteration 95/1000 | Loss: 0.00002102
Iteration 96/1000 | Loss: 0.00002102
Iteration 97/1000 | Loss: 0.00002102
Iteration 98/1000 | Loss: 0.00002102
Iteration 99/1000 | Loss: 0.00002102
Iteration 100/1000 | Loss: 0.00002102
Iteration 101/1000 | Loss: 0.00002102
Iteration 102/1000 | Loss: 0.00002102
Iteration 103/1000 | Loss: 0.00002102
Iteration 104/1000 | Loss: 0.00002102
Iteration 105/1000 | Loss: 0.00002102
Iteration 106/1000 | Loss: 0.00002102
Iteration 107/1000 | Loss: 0.00002102
Iteration 108/1000 | Loss: 0.00002102
Iteration 109/1000 | Loss: 0.00002102
Iteration 110/1000 | Loss: 0.00002102
Iteration 111/1000 | Loss: 0.00002102
Iteration 112/1000 | Loss: 0.00002102
Iteration 113/1000 | Loss: 0.00002102
Iteration 114/1000 | Loss: 0.00002102
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 114. Stopping optimization.
Last 5 losses: [2.1018884581280872e-05, 2.1018884581280872e-05, 2.1018884581280872e-05, 2.1018884581280872e-05, 2.1018884581280872e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1018884581280872e-05

Optimization complete. Final v2v error: 3.794095277786255 mm

Highest mean error: 5.022407054901123 mm for frame 72

Lowest mean error: 3.156050205230713 mm for frame 118

Saving results

Total time: 107.23362255096436
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_39_us_1277/0004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_39_us_1277/0004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_39_us_1277/0004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00494856
Iteration 2/25 | Loss: 0.00131962
Iteration 3/25 | Loss: 0.00122610
Iteration 4/25 | Loss: 0.00121712
Iteration 5/25 | Loss: 0.00121417
Iteration 6/25 | Loss: 0.00121417
Iteration 7/25 | Loss: 0.00121417
Iteration 8/25 | Loss: 0.00121417
Iteration 9/25 | Loss: 0.00121417
Iteration 10/25 | Loss: 0.00121417
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0012141745537519455, 0.0012141745537519455, 0.0012141745537519455, 0.0012141745537519455, 0.0012141745537519455]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012141745537519455

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33766842
Iteration 2/25 | Loss: 0.00101473
Iteration 3/25 | Loss: 0.00101473
Iteration 4/25 | Loss: 0.00101473
Iteration 5/25 | Loss: 0.00101473
Iteration 6/25 | Loss: 0.00101473
Iteration 7/25 | Loss: 0.00101473
Iteration 8/25 | Loss: 0.00101473
Iteration 9/25 | Loss: 0.00101473
Iteration 10/25 | Loss: 0.00101473
Iteration 11/25 | Loss: 0.00101473
Iteration 12/25 | Loss: 0.00101473
Iteration 13/25 | Loss: 0.00101473
Iteration 14/25 | Loss: 0.00101473
Iteration 15/25 | Loss: 0.00101473
Iteration 16/25 | Loss: 0.00101473
Iteration 17/25 | Loss: 0.00101473
Iteration 18/25 | Loss: 0.00101473
Iteration 19/25 | Loss: 0.00101473
Iteration 20/25 | Loss: 0.00101473
Iteration 21/25 | Loss: 0.00101473
Iteration 22/25 | Loss: 0.00101473
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0010147270513698459, 0.0010147270513698459, 0.0010147270513698459, 0.0010147270513698459, 0.0010147270513698459]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010147270513698459

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00101473
Iteration 2/1000 | Loss: 0.00004796
Iteration 3/1000 | Loss: 0.00002935
Iteration 4/1000 | Loss: 0.00002646
Iteration 5/1000 | Loss: 0.00002491
Iteration 6/1000 | Loss: 0.00002409
Iteration 7/1000 | Loss: 0.00002364
Iteration 8/1000 | Loss: 0.00002336
Iteration 9/1000 | Loss: 0.00002311
Iteration 10/1000 | Loss: 0.00002289
Iteration 11/1000 | Loss: 0.00002288
Iteration 12/1000 | Loss: 0.00002281
Iteration 13/1000 | Loss: 0.00002281
Iteration 14/1000 | Loss: 0.00002278
Iteration 15/1000 | Loss: 0.00002278
Iteration 16/1000 | Loss: 0.00002278
Iteration 17/1000 | Loss: 0.00002277
Iteration 18/1000 | Loss: 0.00002277
Iteration 19/1000 | Loss: 0.00002276
Iteration 20/1000 | Loss: 0.00002276
Iteration 21/1000 | Loss: 0.00002275
Iteration 22/1000 | Loss: 0.00002271
Iteration 23/1000 | Loss: 0.00002264
Iteration 24/1000 | Loss: 0.00002254
Iteration 25/1000 | Loss: 0.00002249
Iteration 26/1000 | Loss: 0.00002233
Iteration 27/1000 | Loss: 0.00002232
Iteration 28/1000 | Loss: 0.00002228
Iteration 29/1000 | Loss: 0.00002228
Iteration 30/1000 | Loss: 0.00002227
Iteration 31/1000 | Loss: 0.00002221
Iteration 32/1000 | Loss: 0.00002219
Iteration 33/1000 | Loss: 0.00002219
Iteration 34/1000 | Loss: 0.00002217
Iteration 35/1000 | Loss: 0.00002216
Iteration 36/1000 | Loss: 0.00002216
Iteration 37/1000 | Loss: 0.00002216
Iteration 38/1000 | Loss: 0.00002216
Iteration 39/1000 | Loss: 0.00002215
Iteration 40/1000 | Loss: 0.00002215
Iteration 41/1000 | Loss: 0.00002215
Iteration 42/1000 | Loss: 0.00002215
Iteration 43/1000 | Loss: 0.00002214
Iteration 44/1000 | Loss: 0.00002214
Iteration 45/1000 | Loss: 0.00002214
Iteration 46/1000 | Loss: 0.00002214
Iteration 47/1000 | Loss: 0.00002214
Iteration 48/1000 | Loss: 0.00002214
Iteration 49/1000 | Loss: 0.00002214
Iteration 50/1000 | Loss: 0.00002213
Iteration 51/1000 | Loss: 0.00002213
Iteration 52/1000 | Loss: 0.00002213
Iteration 53/1000 | Loss: 0.00002213
Iteration 54/1000 | Loss: 0.00002213
Iteration 55/1000 | Loss: 0.00002213
Iteration 56/1000 | Loss: 0.00002213
Iteration 57/1000 | Loss: 0.00002212
Iteration 58/1000 | Loss: 0.00002211
Iteration 59/1000 | Loss: 0.00002211
Iteration 60/1000 | Loss: 0.00002211
Iteration 61/1000 | Loss: 0.00002211
Iteration 62/1000 | Loss: 0.00002211
Iteration 63/1000 | Loss: 0.00002211
Iteration 64/1000 | Loss: 0.00002211
Iteration 65/1000 | Loss: 0.00002210
Iteration 66/1000 | Loss: 0.00002210
Iteration 67/1000 | Loss: 0.00002210
Iteration 68/1000 | Loss: 0.00002210
Iteration 69/1000 | Loss: 0.00002210
Iteration 70/1000 | Loss: 0.00002210
Iteration 71/1000 | Loss: 0.00002209
Iteration 72/1000 | Loss: 0.00002209
Iteration 73/1000 | Loss: 0.00002209
Iteration 74/1000 | Loss: 0.00002208
Iteration 75/1000 | Loss: 0.00002208
Iteration 76/1000 | Loss: 0.00002208
Iteration 77/1000 | Loss: 0.00002208
Iteration 78/1000 | Loss: 0.00002208
Iteration 79/1000 | Loss: 0.00002207
Iteration 80/1000 | Loss: 0.00002206
Iteration 81/1000 | Loss: 0.00002206
Iteration 82/1000 | Loss: 0.00002205
Iteration 83/1000 | Loss: 0.00002205
Iteration 84/1000 | Loss: 0.00002204
Iteration 85/1000 | Loss: 0.00002204
Iteration 86/1000 | Loss: 0.00002204
Iteration 87/1000 | Loss: 0.00002204
Iteration 88/1000 | Loss: 0.00002203
Iteration 89/1000 | Loss: 0.00002203
Iteration 90/1000 | Loss: 0.00002203
Iteration 91/1000 | Loss: 0.00002203
Iteration 92/1000 | Loss: 0.00002203
Iteration 93/1000 | Loss: 0.00002203
Iteration 94/1000 | Loss: 0.00002203
Iteration 95/1000 | Loss: 0.00002202
Iteration 96/1000 | Loss: 0.00002202
Iteration 97/1000 | Loss: 0.00002202
Iteration 98/1000 | Loss: 0.00002202
Iteration 99/1000 | Loss: 0.00002202
Iteration 100/1000 | Loss: 0.00002202
Iteration 101/1000 | Loss: 0.00002202
Iteration 102/1000 | Loss: 0.00002202
Iteration 103/1000 | Loss: 0.00002202
Iteration 104/1000 | Loss: 0.00002201
Iteration 105/1000 | Loss: 0.00002201
Iteration 106/1000 | Loss: 0.00002201
Iteration 107/1000 | Loss: 0.00002201
Iteration 108/1000 | Loss: 0.00002201
Iteration 109/1000 | Loss: 0.00002201
Iteration 110/1000 | Loss: 0.00002201
Iteration 111/1000 | Loss: 0.00002201
Iteration 112/1000 | Loss: 0.00002201
Iteration 113/1000 | Loss: 0.00002201
Iteration 114/1000 | Loss: 0.00002201
Iteration 115/1000 | Loss: 0.00002201
Iteration 116/1000 | Loss: 0.00002201
Iteration 117/1000 | Loss: 0.00002201
Iteration 118/1000 | Loss: 0.00002201
Iteration 119/1000 | Loss: 0.00002200
Iteration 120/1000 | Loss: 0.00002200
Iteration 121/1000 | Loss: 0.00002200
Iteration 122/1000 | Loss: 0.00002200
Iteration 123/1000 | Loss: 0.00002200
Iteration 124/1000 | Loss: 0.00002200
Iteration 125/1000 | Loss: 0.00002200
Iteration 126/1000 | Loss: 0.00002200
Iteration 127/1000 | Loss: 0.00002200
Iteration 128/1000 | Loss: 0.00002200
Iteration 129/1000 | Loss: 0.00002199
Iteration 130/1000 | Loss: 0.00002199
Iteration 131/1000 | Loss: 0.00002199
Iteration 132/1000 | Loss: 0.00002199
Iteration 133/1000 | Loss: 0.00002199
Iteration 134/1000 | Loss: 0.00002199
Iteration 135/1000 | Loss: 0.00002199
Iteration 136/1000 | Loss: 0.00002199
Iteration 137/1000 | Loss: 0.00002199
Iteration 138/1000 | Loss: 0.00002199
Iteration 139/1000 | Loss: 0.00002198
Iteration 140/1000 | Loss: 0.00002198
Iteration 141/1000 | Loss: 0.00002198
Iteration 142/1000 | Loss: 0.00002198
Iteration 143/1000 | Loss: 0.00002198
Iteration 144/1000 | Loss: 0.00002198
Iteration 145/1000 | Loss: 0.00002198
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 145. Stopping optimization.
Last 5 losses: [2.1984786144457757e-05, 2.1984786144457757e-05, 2.1984786144457757e-05, 2.1984786144457757e-05, 2.1984786144457757e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1984786144457757e-05

Optimization complete. Final v2v error: 3.930746555328369 mm

Highest mean error: 4.1737213134765625 mm for frame 160

Lowest mean error: 3.654834747314453 mm for frame 116

Saving results

Total time: 42.52430272102356
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_39_us_1277/0017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_39_us_1277/0017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_39_us_1277/0017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00569740
Iteration 2/25 | Loss: 0.00128707
Iteration 3/25 | Loss: 0.00120640
Iteration 4/25 | Loss: 0.00119902
Iteration 5/25 | Loss: 0.00119614
Iteration 6/25 | Loss: 0.00119557
Iteration 7/25 | Loss: 0.00119557
Iteration 8/25 | Loss: 0.00119557
Iteration 9/25 | Loss: 0.00119557
Iteration 10/25 | Loss: 0.00119557
Iteration 11/25 | Loss: 0.00119557
Iteration 12/25 | Loss: 0.00119557
Iteration 13/25 | Loss: 0.00119557
Iteration 14/25 | Loss: 0.00119557
Iteration 15/25 | Loss: 0.00119557
Iteration 16/25 | Loss: 0.00119557
Iteration 17/25 | Loss: 0.00119557
Iteration 18/25 | Loss: 0.00119557
Iteration 19/25 | Loss: 0.00119557
Iteration 20/25 | Loss: 0.00119557
Iteration 21/25 | Loss: 0.00119557
Iteration 22/25 | Loss: 0.00119557
Iteration 23/25 | Loss: 0.00119557
Iteration 24/25 | Loss: 0.00119557
Iteration 25/25 | Loss: 0.00119557

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 6.28494596
Iteration 2/25 | Loss: 0.00097493
Iteration 3/25 | Loss: 0.00097492
Iteration 4/25 | Loss: 0.00097492
Iteration 5/25 | Loss: 0.00097491
Iteration 6/25 | Loss: 0.00097491
Iteration 7/25 | Loss: 0.00097491
Iteration 8/25 | Loss: 0.00097491
Iteration 9/25 | Loss: 0.00097491
Iteration 10/25 | Loss: 0.00097491
Iteration 11/25 | Loss: 0.00097491
Iteration 12/25 | Loss: 0.00097491
Iteration 13/25 | Loss: 0.00097491
Iteration 14/25 | Loss: 0.00097491
Iteration 15/25 | Loss: 0.00097491
Iteration 16/25 | Loss: 0.00097491
Iteration 17/25 | Loss: 0.00097491
Iteration 18/25 | Loss: 0.00097491
Iteration 19/25 | Loss: 0.00097491
Iteration 20/25 | Loss: 0.00097491
Iteration 21/25 | Loss: 0.00097491
Iteration 22/25 | Loss: 0.00097491
Iteration 23/25 | Loss: 0.00097491
Iteration 24/25 | Loss: 0.00097491
Iteration 25/25 | Loss: 0.00097491

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00097491
Iteration 2/1000 | Loss: 0.00004914
Iteration 3/1000 | Loss: 0.00002851
Iteration 4/1000 | Loss: 0.00002383
Iteration 5/1000 | Loss: 0.00002246
Iteration 6/1000 | Loss: 0.00002146
Iteration 7/1000 | Loss: 0.00002081
Iteration 8/1000 | Loss: 0.00002041
Iteration 9/1000 | Loss: 0.00002014
Iteration 10/1000 | Loss: 0.00001999
Iteration 11/1000 | Loss: 0.00001991
Iteration 12/1000 | Loss: 0.00001978
Iteration 13/1000 | Loss: 0.00001969
Iteration 14/1000 | Loss: 0.00001965
Iteration 15/1000 | Loss: 0.00001965
Iteration 16/1000 | Loss: 0.00001965
Iteration 17/1000 | Loss: 0.00001964
Iteration 18/1000 | Loss: 0.00001964
Iteration 19/1000 | Loss: 0.00001961
Iteration 20/1000 | Loss: 0.00001961
Iteration 21/1000 | Loss: 0.00001961
Iteration 22/1000 | Loss: 0.00001961
Iteration 23/1000 | Loss: 0.00001961
Iteration 24/1000 | Loss: 0.00001961
Iteration 25/1000 | Loss: 0.00001960
Iteration 26/1000 | Loss: 0.00001960
Iteration 27/1000 | Loss: 0.00001960
Iteration 28/1000 | Loss: 0.00001956
Iteration 29/1000 | Loss: 0.00001956
Iteration 30/1000 | Loss: 0.00001956
Iteration 31/1000 | Loss: 0.00001955
Iteration 32/1000 | Loss: 0.00001955
Iteration 33/1000 | Loss: 0.00001952
Iteration 34/1000 | Loss: 0.00001951
Iteration 35/1000 | Loss: 0.00001951
Iteration 36/1000 | Loss: 0.00001951
Iteration 37/1000 | Loss: 0.00001950
Iteration 38/1000 | Loss: 0.00001950
Iteration 39/1000 | Loss: 0.00001948
Iteration 40/1000 | Loss: 0.00001947
Iteration 41/1000 | Loss: 0.00001946
Iteration 42/1000 | Loss: 0.00001946
Iteration 43/1000 | Loss: 0.00001945
Iteration 44/1000 | Loss: 0.00001943
Iteration 45/1000 | Loss: 0.00001942
Iteration 46/1000 | Loss: 0.00001942
Iteration 47/1000 | Loss: 0.00001941
Iteration 48/1000 | Loss: 0.00001941
Iteration 49/1000 | Loss: 0.00001939
Iteration 50/1000 | Loss: 0.00001939
Iteration 51/1000 | Loss: 0.00001939
Iteration 52/1000 | Loss: 0.00001938
Iteration 53/1000 | Loss: 0.00001938
Iteration 54/1000 | Loss: 0.00001938
Iteration 55/1000 | Loss: 0.00001936
Iteration 56/1000 | Loss: 0.00001936
Iteration 57/1000 | Loss: 0.00001936
Iteration 58/1000 | Loss: 0.00001936
Iteration 59/1000 | Loss: 0.00001936
Iteration 60/1000 | Loss: 0.00001936
Iteration 61/1000 | Loss: 0.00001936
Iteration 62/1000 | Loss: 0.00001935
Iteration 63/1000 | Loss: 0.00001935
Iteration 64/1000 | Loss: 0.00001935
Iteration 65/1000 | Loss: 0.00001935
Iteration 66/1000 | Loss: 0.00001935
Iteration 67/1000 | Loss: 0.00001935
Iteration 68/1000 | Loss: 0.00001934
Iteration 69/1000 | Loss: 0.00001933
Iteration 70/1000 | Loss: 0.00001933
Iteration 71/1000 | Loss: 0.00001933
Iteration 72/1000 | Loss: 0.00001932
Iteration 73/1000 | Loss: 0.00001932
Iteration 74/1000 | Loss: 0.00001932
Iteration 75/1000 | Loss: 0.00001931
Iteration 76/1000 | Loss: 0.00001931
Iteration 77/1000 | Loss: 0.00001931
Iteration 78/1000 | Loss: 0.00001931
Iteration 79/1000 | Loss: 0.00001931
Iteration 80/1000 | Loss: 0.00001931
Iteration 81/1000 | Loss: 0.00001930
Iteration 82/1000 | Loss: 0.00001930
Iteration 83/1000 | Loss: 0.00001930
Iteration 84/1000 | Loss: 0.00001930
Iteration 85/1000 | Loss: 0.00001929
Iteration 86/1000 | Loss: 0.00001929
Iteration 87/1000 | Loss: 0.00001929
Iteration 88/1000 | Loss: 0.00001929
Iteration 89/1000 | Loss: 0.00001929
Iteration 90/1000 | Loss: 0.00001929
Iteration 91/1000 | Loss: 0.00001929
Iteration 92/1000 | Loss: 0.00001929
Iteration 93/1000 | Loss: 0.00001929
Iteration 94/1000 | Loss: 0.00001928
Iteration 95/1000 | Loss: 0.00001928
Iteration 96/1000 | Loss: 0.00001928
Iteration 97/1000 | Loss: 0.00001927
Iteration 98/1000 | Loss: 0.00001927
Iteration 99/1000 | Loss: 0.00001927
Iteration 100/1000 | Loss: 0.00001927
Iteration 101/1000 | Loss: 0.00001927
Iteration 102/1000 | Loss: 0.00001927
Iteration 103/1000 | Loss: 0.00001927
Iteration 104/1000 | Loss: 0.00001927
Iteration 105/1000 | Loss: 0.00001927
Iteration 106/1000 | Loss: 0.00001927
Iteration 107/1000 | Loss: 0.00001927
Iteration 108/1000 | Loss: 0.00001926
Iteration 109/1000 | Loss: 0.00001926
Iteration 110/1000 | Loss: 0.00001926
Iteration 111/1000 | Loss: 0.00001926
Iteration 112/1000 | Loss: 0.00001926
Iteration 113/1000 | Loss: 0.00001926
Iteration 114/1000 | Loss: 0.00001926
Iteration 115/1000 | Loss: 0.00001926
Iteration 116/1000 | Loss: 0.00001926
Iteration 117/1000 | Loss: 0.00001926
Iteration 118/1000 | Loss: 0.00001926
Iteration 119/1000 | Loss: 0.00001926
Iteration 120/1000 | Loss: 0.00001926
Iteration 121/1000 | Loss: 0.00001926
Iteration 122/1000 | Loss: 0.00001925
Iteration 123/1000 | Loss: 0.00001925
Iteration 124/1000 | Loss: 0.00001925
Iteration 125/1000 | Loss: 0.00001925
Iteration 126/1000 | Loss: 0.00001925
Iteration 127/1000 | Loss: 0.00001925
Iteration 128/1000 | Loss: 0.00001925
Iteration 129/1000 | Loss: 0.00001925
Iteration 130/1000 | Loss: 0.00001925
Iteration 131/1000 | Loss: 0.00001925
Iteration 132/1000 | Loss: 0.00001925
Iteration 133/1000 | Loss: 0.00001925
Iteration 134/1000 | Loss: 0.00001925
Iteration 135/1000 | Loss: 0.00001925
Iteration 136/1000 | Loss: 0.00001925
Iteration 137/1000 | Loss: 0.00001924
Iteration 138/1000 | Loss: 0.00001924
Iteration 139/1000 | Loss: 0.00001924
Iteration 140/1000 | Loss: 0.00001924
Iteration 141/1000 | Loss: 0.00001924
Iteration 142/1000 | Loss: 0.00001924
Iteration 143/1000 | Loss: 0.00001924
Iteration 144/1000 | Loss: 0.00001924
Iteration 145/1000 | Loss: 0.00001924
Iteration 146/1000 | Loss: 0.00001924
Iteration 147/1000 | Loss: 0.00001924
Iteration 148/1000 | Loss: 0.00001924
Iteration 149/1000 | Loss: 0.00001924
Iteration 150/1000 | Loss: 0.00001924
Iteration 151/1000 | Loss: 0.00001924
Iteration 152/1000 | Loss: 0.00001924
Iteration 153/1000 | Loss: 0.00001923
Iteration 154/1000 | Loss: 0.00001923
Iteration 155/1000 | Loss: 0.00001923
Iteration 156/1000 | Loss: 0.00001923
Iteration 157/1000 | Loss: 0.00001923
Iteration 158/1000 | Loss: 0.00001923
Iteration 159/1000 | Loss: 0.00001923
Iteration 160/1000 | Loss: 0.00001923
Iteration 161/1000 | Loss: 0.00001923
Iteration 162/1000 | Loss: 0.00001923
Iteration 163/1000 | Loss: 0.00001923
Iteration 164/1000 | Loss: 0.00001923
Iteration 165/1000 | Loss: 0.00001923
Iteration 166/1000 | Loss: 0.00001923
Iteration 167/1000 | Loss: 0.00001922
Iteration 168/1000 | Loss: 0.00001922
Iteration 169/1000 | Loss: 0.00001922
Iteration 170/1000 | Loss: 0.00001922
Iteration 171/1000 | Loss: 0.00001922
Iteration 172/1000 | Loss: 0.00001922
Iteration 173/1000 | Loss: 0.00001922
Iteration 174/1000 | Loss: 0.00001922
Iteration 175/1000 | Loss: 0.00001922
Iteration 176/1000 | Loss: 0.00001922
Iteration 177/1000 | Loss: 0.00001922
Iteration 178/1000 | Loss: 0.00001922
Iteration 179/1000 | Loss: 0.00001922
Iteration 180/1000 | Loss: 0.00001922
Iteration 181/1000 | Loss: 0.00001922
Iteration 182/1000 | Loss: 0.00001922
Iteration 183/1000 | Loss: 0.00001922
Iteration 184/1000 | Loss: 0.00001922
Iteration 185/1000 | Loss: 0.00001921
Iteration 186/1000 | Loss: 0.00001921
Iteration 187/1000 | Loss: 0.00001921
Iteration 188/1000 | Loss: 0.00001921
Iteration 189/1000 | Loss: 0.00001921
Iteration 190/1000 | Loss: 0.00001921
Iteration 191/1000 | Loss: 0.00001921
Iteration 192/1000 | Loss: 0.00001921
Iteration 193/1000 | Loss: 0.00001921
Iteration 194/1000 | Loss: 0.00001920
Iteration 195/1000 | Loss: 0.00001920
Iteration 196/1000 | Loss: 0.00001920
Iteration 197/1000 | Loss: 0.00001920
Iteration 198/1000 | Loss: 0.00001920
Iteration 199/1000 | Loss: 0.00001920
Iteration 200/1000 | Loss: 0.00001920
Iteration 201/1000 | Loss: 0.00001920
Iteration 202/1000 | Loss: 0.00001920
Iteration 203/1000 | Loss: 0.00001920
Iteration 204/1000 | Loss: 0.00001920
Iteration 205/1000 | Loss: 0.00001920
Iteration 206/1000 | Loss: 0.00001920
Iteration 207/1000 | Loss: 0.00001920
Iteration 208/1000 | Loss: 0.00001920
Iteration 209/1000 | Loss: 0.00001920
Iteration 210/1000 | Loss: 0.00001920
Iteration 211/1000 | Loss: 0.00001920
Iteration 212/1000 | Loss: 0.00001920
Iteration 213/1000 | Loss: 0.00001919
Iteration 214/1000 | Loss: 0.00001919
Iteration 215/1000 | Loss: 0.00001919
Iteration 216/1000 | Loss: 0.00001919
Iteration 217/1000 | Loss: 0.00001919
Iteration 218/1000 | Loss: 0.00001919
Iteration 219/1000 | Loss: 0.00001919
Iteration 220/1000 | Loss: 0.00001919
Iteration 221/1000 | Loss: 0.00001919
Iteration 222/1000 | Loss: 0.00001919
Iteration 223/1000 | Loss: 0.00001919
Iteration 224/1000 | Loss: 0.00001919
Iteration 225/1000 | Loss: 0.00001919
Iteration 226/1000 | Loss: 0.00001919
Iteration 227/1000 | Loss: 0.00001919
Iteration 228/1000 | Loss: 0.00001919
Iteration 229/1000 | Loss: 0.00001919
Iteration 230/1000 | Loss: 0.00001919
Iteration 231/1000 | Loss: 0.00001919
Iteration 232/1000 | Loss: 0.00001919
Iteration 233/1000 | Loss: 0.00001919
Iteration 234/1000 | Loss: 0.00001919
Iteration 235/1000 | Loss: 0.00001919
Iteration 236/1000 | Loss: 0.00001919
Iteration 237/1000 | Loss: 0.00001919
Iteration 238/1000 | Loss: 0.00001919
Iteration 239/1000 | Loss: 0.00001919
Iteration 240/1000 | Loss: 0.00001919
Iteration 241/1000 | Loss: 0.00001919
Iteration 242/1000 | Loss: 0.00001919
Iteration 243/1000 | Loss: 0.00001919
Iteration 244/1000 | Loss: 0.00001919
Iteration 245/1000 | Loss: 0.00001919
Iteration 246/1000 | Loss: 0.00001919
Iteration 247/1000 | Loss: 0.00001919
Iteration 248/1000 | Loss: 0.00001919
Iteration 249/1000 | Loss: 0.00001919
Iteration 250/1000 | Loss: 0.00001919
Iteration 251/1000 | Loss: 0.00001919
Iteration 252/1000 | Loss: 0.00001919
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 252. Stopping optimization.
Last 5 losses: [1.9191105820937082e-05, 1.9191105820937082e-05, 1.9191105820937082e-05, 1.9191105820937082e-05, 1.9191105820937082e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9191105820937082e-05

Optimization complete. Final v2v error: 3.7430527210235596 mm

Highest mean error: 4.311392784118652 mm for frame 60

Lowest mean error: 3.3432040214538574 mm for frame 153

Saving results

Total time: 42.41517353057861
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_39_us_1277/0020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_39_us_1277/0020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_39_us_1277/0020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00478841
Iteration 2/25 | Loss: 0.00147625
Iteration 3/25 | Loss: 0.00124124
Iteration 4/25 | Loss: 0.00121753
Iteration 5/25 | Loss: 0.00121272
Iteration 6/25 | Loss: 0.00121102
Iteration 7/25 | Loss: 0.00121079
Iteration 8/25 | Loss: 0.00121079
Iteration 9/25 | Loss: 0.00121079
Iteration 10/25 | Loss: 0.00121079
Iteration 11/25 | Loss: 0.00121079
Iteration 12/25 | Loss: 0.00121079
Iteration 13/25 | Loss: 0.00121079
Iteration 14/25 | Loss: 0.00121079
Iteration 15/25 | Loss: 0.00121079
Iteration 16/25 | Loss: 0.00121079
Iteration 17/25 | Loss: 0.00121079
Iteration 18/25 | Loss: 0.00121079
Iteration 19/25 | Loss: 0.00121079
Iteration 20/25 | Loss: 0.00121079
Iteration 21/25 | Loss: 0.00121079
Iteration 22/25 | Loss: 0.00121079
Iteration 23/25 | Loss: 0.00121079
Iteration 24/25 | Loss: 0.00121079
Iteration 25/25 | Loss: 0.00121079

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42024052
Iteration 2/25 | Loss: 0.00085023
Iteration 3/25 | Loss: 0.00085023
Iteration 4/25 | Loss: 0.00085023
Iteration 5/25 | Loss: 0.00085023
Iteration 6/25 | Loss: 0.00085023
Iteration 7/25 | Loss: 0.00085023
Iteration 8/25 | Loss: 0.00085023
Iteration 9/25 | Loss: 0.00085023
Iteration 10/25 | Loss: 0.00085023
Iteration 11/25 | Loss: 0.00085023
Iteration 12/25 | Loss: 0.00085023
Iteration 13/25 | Loss: 0.00085023
Iteration 14/25 | Loss: 0.00085023
Iteration 15/25 | Loss: 0.00085023
Iteration 16/25 | Loss: 0.00085023
Iteration 17/25 | Loss: 0.00085023
Iteration 18/25 | Loss: 0.00085023
Iteration 19/25 | Loss: 0.00085023
Iteration 20/25 | Loss: 0.00085023
Iteration 21/25 | Loss: 0.00085023
Iteration 22/25 | Loss: 0.00085023
Iteration 23/25 | Loss: 0.00085023
Iteration 24/25 | Loss: 0.00085023
Iteration 25/25 | Loss: 0.00085023

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00085023
Iteration 2/1000 | Loss: 0.00008324
Iteration 3/1000 | Loss: 0.00004521
Iteration 4/1000 | Loss: 0.00003283
Iteration 5/1000 | Loss: 0.00002857
Iteration 6/1000 | Loss: 0.00002751
Iteration 7/1000 | Loss: 0.00002642
Iteration 8/1000 | Loss: 0.00002562
Iteration 9/1000 | Loss: 0.00002503
Iteration 10/1000 | Loss: 0.00002458
Iteration 11/1000 | Loss: 0.00002418
Iteration 12/1000 | Loss: 0.00002388
Iteration 13/1000 | Loss: 0.00002364
Iteration 14/1000 | Loss: 0.00002341
Iteration 15/1000 | Loss: 0.00002319
Iteration 16/1000 | Loss: 0.00002304
Iteration 17/1000 | Loss: 0.00002292
Iteration 18/1000 | Loss: 0.00002290
Iteration 19/1000 | Loss: 0.00002288
Iteration 20/1000 | Loss: 0.00002282
Iteration 21/1000 | Loss: 0.00002281
Iteration 22/1000 | Loss: 0.00002281
Iteration 23/1000 | Loss: 0.00002276
Iteration 24/1000 | Loss: 0.00002271
Iteration 25/1000 | Loss: 0.00002270
Iteration 26/1000 | Loss: 0.00002261
Iteration 27/1000 | Loss: 0.00002259
Iteration 28/1000 | Loss: 0.00002258
Iteration 29/1000 | Loss: 0.00002256
Iteration 30/1000 | Loss: 0.00002256
Iteration 31/1000 | Loss: 0.00002256
Iteration 32/1000 | Loss: 0.00002256
Iteration 33/1000 | Loss: 0.00002256
Iteration 34/1000 | Loss: 0.00002256
Iteration 35/1000 | Loss: 0.00002256
Iteration 36/1000 | Loss: 0.00002256
Iteration 37/1000 | Loss: 0.00002255
Iteration 38/1000 | Loss: 0.00002255
Iteration 39/1000 | Loss: 0.00002255
Iteration 40/1000 | Loss: 0.00002252
Iteration 41/1000 | Loss: 0.00002252
Iteration 42/1000 | Loss: 0.00002252
Iteration 43/1000 | Loss: 0.00002251
Iteration 44/1000 | Loss: 0.00002250
Iteration 45/1000 | Loss: 0.00002250
Iteration 46/1000 | Loss: 0.00002250
Iteration 47/1000 | Loss: 0.00002250
Iteration 48/1000 | Loss: 0.00002249
Iteration 49/1000 | Loss: 0.00002249
Iteration 50/1000 | Loss: 0.00002249
Iteration 51/1000 | Loss: 0.00002248
Iteration 52/1000 | Loss: 0.00002248
Iteration 53/1000 | Loss: 0.00002248
Iteration 54/1000 | Loss: 0.00002248
Iteration 55/1000 | Loss: 0.00002247
Iteration 56/1000 | Loss: 0.00002247
Iteration 57/1000 | Loss: 0.00002247
Iteration 58/1000 | Loss: 0.00002247
Iteration 59/1000 | Loss: 0.00002247
Iteration 60/1000 | Loss: 0.00002247
Iteration 61/1000 | Loss: 0.00002247
Iteration 62/1000 | Loss: 0.00002247
Iteration 63/1000 | Loss: 0.00002247
Iteration 64/1000 | Loss: 0.00002247
Iteration 65/1000 | Loss: 0.00002246
Iteration 66/1000 | Loss: 0.00002246
Iteration 67/1000 | Loss: 0.00002246
Iteration 68/1000 | Loss: 0.00002246
Iteration 69/1000 | Loss: 0.00002245
Iteration 70/1000 | Loss: 0.00002245
Iteration 71/1000 | Loss: 0.00002245
Iteration 72/1000 | Loss: 0.00002245
Iteration 73/1000 | Loss: 0.00002245
Iteration 74/1000 | Loss: 0.00002244
Iteration 75/1000 | Loss: 0.00002244
Iteration 76/1000 | Loss: 0.00002244
Iteration 77/1000 | Loss: 0.00002244
Iteration 78/1000 | Loss: 0.00002244
Iteration 79/1000 | Loss: 0.00002244
Iteration 80/1000 | Loss: 0.00002244
Iteration 81/1000 | Loss: 0.00002244
Iteration 82/1000 | Loss: 0.00002244
Iteration 83/1000 | Loss: 0.00002244
Iteration 84/1000 | Loss: 0.00002244
Iteration 85/1000 | Loss: 0.00002244
Iteration 86/1000 | Loss: 0.00002244
Iteration 87/1000 | Loss: 0.00002244
Iteration 88/1000 | Loss: 0.00002244
Iteration 89/1000 | Loss: 0.00002244
Iteration 90/1000 | Loss: 0.00002244
Iteration 91/1000 | Loss: 0.00002244
Iteration 92/1000 | Loss: 0.00002244
Iteration 93/1000 | Loss: 0.00002244
Iteration 94/1000 | Loss: 0.00002244
Iteration 95/1000 | Loss: 0.00002244
Iteration 96/1000 | Loss: 0.00002244
Iteration 97/1000 | Loss: 0.00002244
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 97. Stopping optimization.
Last 5 losses: [2.243641938548535e-05, 2.243641938548535e-05, 2.243641938548535e-05, 2.243641938548535e-05, 2.243641938548535e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.243641938548535e-05

Optimization complete. Final v2v error: 3.943368911743164 mm

Highest mean error: 5.589033126831055 mm for frame 54

Lowest mean error: 3.248255729675293 mm for frame 10

Saving results

Total time: 41.62507486343384
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_39_us_1277/0015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_39_us_1277/0015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_39_us_1277/0015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01153411
Iteration 2/25 | Loss: 0.01153411
Iteration 3/25 | Loss: 0.01153411
Iteration 4/25 | Loss: 0.00179109
Iteration 5/25 | Loss: 0.00129276
Iteration 6/25 | Loss: 0.00124115
Iteration 7/25 | Loss: 0.00123312
Iteration 8/25 | Loss: 0.00122925
Iteration 9/25 | Loss: 0.00123082
Iteration 10/25 | Loss: 0.00122753
Iteration 11/25 | Loss: 0.00122091
Iteration 12/25 | Loss: 0.00121867
Iteration 13/25 | Loss: 0.00121385
Iteration 14/25 | Loss: 0.00121096
Iteration 15/25 | Loss: 0.00121099
Iteration 16/25 | Loss: 0.00120932
Iteration 17/25 | Loss: 0.00120966
Iteration 18/25 | Loss: 0.00120841
Iteration 19/25 | Loss: 0.00120853
Iteration 20/25 | Loss: 0.00121002
Iteration 21/25 | Loss: 0.00120924
Iteration 22/25 | Loss: 0.00121010
Iteration 23/25 | Loss: 0.00120968
Iteration 24/25 | Loss: 0.00120884
Iteration 25/25 | Loss: 0.00120898

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.64348936
Iteration 2/25 | Loss: 0.00129072
Iteration 3/25 | Loss: 0.00129072
Iteration 4/25 | Loss: 0.00129072
Iteration 5/25 | Loss: 0.00129071
Iteration 6/25 | Loss: 0.00129071
Iteration 7/25 | Loss: 0.00129071
Iteration 8/25 | Loss: 0.00129071
Iteration 9/25 | Loss: 0.00129071
Iteration 10/25 | Loss: 0.00129071
Iteration 11/25 | Loss: 0.00129071
Iteration 12/25 | Loss: 0.00129071
Iteration 13/25 | Loss: 0.00129071
Iteration 14/25 | Loss: 0.00129071
Iteration 15/25 | Loss: 0.00129071
Iteration 16/25 | Loss: 0.00129071
Iteration 17/25 | Loss: 0.00129071
Iteration 18/25 | Loss: 0.00129071
Iteration 19/25 | Loss: 0.00129071
Iteration 20/25 | Loss: 0.00129071
Iteration 21/25 | Loss: 0.00129071
Iteration 22/25 | Loss: 0.00129071
Iteration 23/25 | Loss: 0.00129071
Iteration 24/25 | Loss: 0.00129071
Iteration 25/25 | Loss: 0.00129071

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00129071
Iteration 2/1000 | Loss: 0.00006146
Iteration 3/1000 | Loss: 0.00006781
Iteration 4/1000 | Loss: 0.00003502
Iteration 5/1000 | Loss: 0.00003146
Iteration 6/1000 | Loss: 0.00003109
Iteration 7/1000 | Loss: 0.00003018
Iteration 8/1000 | Loss: 0.00007236
Iteration 9/1000 | Loss: 0.00002937
Iteration 10/1000 | Loss: 0.00005723
Iteration 11/1000 | Loss: 0.00002902
Iteration 12/1000 | Loss: 0.00002847
Iteration 13/1000 | Loss: 0.00004553
Iteration 14/1000 | Loss: 0.00006083
Iteration 15/1000 | Loss: 0.00005195
Iteration 16/1000 | Loss: 0.00003428
Iteration 17/1000 | Loss: 0.00003245
Iteration 18/1000 | Loss: 0.00003136
Iteration 19/1000 | Loss: 0.00002800
Iteration 20/1000 | Loss: 0.00004154
Iteration 21/1000 | Loss: 0.00002776
Iteration 22/1000 | Loss: 0.00003007
Iteration 23/1000 | Loss: 0.00002796
Iteration 24/1000 | Loss: 0.00002935
Iteration 25/1000 | Loss: 0.00002780
Iteration 26/1000 | Loss: 0.00002814
Iteration 27/1000 | Loss: 0.00002748
Iteration 28/1000 | Loss: 0.00002731
Iteration 29/1000 | Loss: 0.00002794
Iteration 30/1000 | Loss: 0.00007787
Iteration 31/1000 | Loss: 0.00002930
Iteration 32/1000 | Loss: 0.00002706
Iteration 33/1000 | Loss: 0.00002824
Iteration 34/1000 | Loss: 0.00002941
Iteration 35/1000 | Loss: 0.00002982
Iteration 36/1000 | Loss: 0.00002920
Iteration 37/1000 | Loss: 0.00002806
Iteration 38/1000 | Loss: 0.00002711
Iteration 39/1000 | Loss: 0.00002997
Iteration 40/1000 | Loss: 0.00002711
Iteration 41/1000 | Loss: 0.00003073
Iteration 42/1000 | Loss: 0.00002822
Iteration 43/1000 | Loss: 0.00002929
Iteration 44/1000 | Loss: 0.00002753
Iteration 45/1000 | Loss: 0.00002968
Iteration 46/1000 | Loss: 0.00002831
Iteration 47/1000 | Loss: 0.00002818
Iteration 48/1000 | Loss: 0.00002730
Iteration 49/1000 | Loss: 0.00002930
Iteration 50/1000 | Loss: 0.00002779
Iteration 51/1000 | Loss: 0.00003017
Iteration 52/1000 | Loss: 0.00002751
Iteration 53/1000 | Loss: 0.00002959
Iteration 54/1000 | Loss: 0.00002780
Iteration 55/1000 | Loss: 0.00002956
Iteration 56/1000 | Loss: 0.00002778
Iteration 57/1000 | Loss: 0.00003005
Iteration 58/1000 | Loss: 0.00002826
Iteration 59/1000 | Loss: 0.00007087
Iteration 60/1000 | Loss: 0.00003321
Iteration 61/1000 | Loss: 0.00003236
Iteration 62/1000 | Loss: 0.00002695
Iteration 63/1000 | Loss: 0.00002772
Iteration 64/1000 | Loss: 0.00002818
Iteration 65/1000 | Loss: 0.00002781
Iteration 66/1000 | Loss: 0.00004822
Iteration 67/1000 | Loss: 0.00003074
Iteration 68/1000 | Loss: 0.00002768
Iteration 69/1000 | Loss: 0.00002768
Iteration 70/1000 | Loss: 0.00002717
Iteration 71/1000 | Loss: 0.00004681
Iteration 72/1000 | Loss: 0.00002695
Iteration 73/1000 | Loss: 0.00002660
Iteration 74/1000 | Loss: 0.00002803
Iteration 75/1000 | Loss: 0.00002683
Iteration 76/1000 | Loss: 0.00002809
Iteration 77/1000 | Loss: 0.00002718
Iteration 78/1000 | Loss: 0.00003009
Iteration 79/1000 | Loss: 0.00002855
Iteration 80/1000 | Loss: 0.00002955
Iteration 81/1000 | Loss: 0.00002760
Iteration 82/1000 | Loss: 0.00002760
Iteration 83/1000 | Loss: 0.00002989
Iteration 84/1000 | Loss: 0.00004978
Iteration 85/1000 | Loss: 0.00003256
Iteration 86/1000 | Loss: 0.00002989
Iteration 87/1000 | Loss: 0.00002903
Iteration 88/1000 | Loss: 0.00003143
Iteration 89/1000 | Loss: 0.00003013
Iteration 90/1000 | Loss: 0.00003086
Iteration 91/1000 | Loss: 0.00002981
Iteration 92/1000 | Loss: 0.00003197
Iteration 93/1000 | Loss: 0.00003196
Iteration 94/1000 | Loss: 0.00002776
Iteration 95/1000 | Loss: 0.00006082
Iteration 96/1000 | Loss: 0.00002694
Iteration 97/1000 | Loss: 0.00003448
Iteration 98/1000 | Loss: 0.00003286
Iteration 99/1000 | Loss: 0.00004873
Iteration 100/1000 | Loss: 0.00004120
Iteration 101/1000 | Loss: 0.00003522
Iteration 102/1000 | Loss: 0.00002698
Iteration 103/1000 | Loss: 0.00002714
Iteration 104/1000 | Loss: 0.00003424
Iteration 105/1000 | Loss: 0.00003591
Iteration 106/1000 | Loss: 0.00002921
Iteration 107/1000 | Loss: 0.00003517
Iteration 108/1000 | Loss: 0.00002875
Iteration 109/1000 | Loss: 0.00003029
Iteration 110/1000 | Loss: 0.00004761
Iteration 111/1000 | Loss: 0.00002983
Iteration 112/1000 | Loss: 0.00003322
Iteration 113/1000 | Loss: 0.00005774
Iteration 114/1000 | Loss: 0.00003370
Iteration 115/1000 | Loss: 0.00002776
Iteration 116/1000 | Loss: 0.00003797
Iteration 117/1000 | Loss: 0.00002993
Iteration 118/1000 | Loss: 0.00003480
Iteration 119/1000 | Loss: 0.00003140
Iteration 120/1000 | Loss: 0.00003142
Iteration 121/1000 | Loss: 0.00003073
Iteration 122/1000 | Loss: 0.00003012
Iteration 123/1000 | Loss: 0.00003256
Iteration 124/1000 | Loss: 0.00003083
Iteration 125/1000 | Loss: 0.00004682
Iteration 126/1000 | Loss: 0.00003182
Iteration 127/1000 | Loss: 0.00003269
Iteration 128/1000 | Loss: 0.00003110
Iteration 129/1000 | Loss: 0.00003227
Iteration 130/1000 | Loss: 0.00003127
Iteration 131/1000 | Loss: 0.00003192
Iteration 132/1000 | Loss: 0.00003876
Iteration 133/1000 | Loss: 0.00003183
Iteration 134/1000 | Loss: 0.00003277
Iteration 135/1000 | Loss: 0.00003186
Iteration 136/1000 | Loss: 0.00003341
Iteration 137/1000 | Loss: 0.00003835
Iteration 138/1000 | Loss: 0.00002963
Iteration 139/1000 | Loss: 0.00002904
Iteration 140/1000 | Loss: 0.00002752
Iteration 141/1000 | Loss: 0.00002956
Iteration 142/1000 | Loss: 0.00003927
Iteration 143/1000 | Loss: 0.00002958
Iteration 144/1000 | Loss: 0.00002835
Iteration 145/1000 | Loss: 0.00002843
Iteration 146/1000 | Loss: 0.00002774
Iteration 147/1000 | Loss: 0.00003102
Iteration 148/1000 | Loss: 0.00003624
Iteration 149/1000 | Loss: 0.00003149
Iteration 150/1000 | Loss: 0.00003149
Iteration 151/1000 | Loss: 0.00003077
Iteration 152/1000 | Loss: 0.00002969
Iteration 153/1000 | Loss: 0.00002963
Iteration 154/1000 | Loss: 0.00002851
Iteration 155/1000 | Loss: 0.00002810
Iteration 156/1000 | Loss: 0.00003340
Iteration 157/1000 | Loss: 0.00003224
Iteration 158/1000 | Loss: 0.00003922
Iteration 159/1000 | Loss: 0.00004031
Iteration 160/1000 | Loss: 0.00002687
Iteration 161/1000 | Loss: 0.00002647
Iteration 162/1000 | Loss: 0.00003181
Iteration 163/1000 | Loss: 0.00002843
Iteration 164/1000 | Loss: 0.00002841
Iteration 165/1000 | Loss: 0.00005375
Iteration 166/1000 | Loss: 0.00003678
Iteration 167/1000 | Loss: 0.00003357
Iteration 168/1000 | Loss: 0.00002720
Iteration 169/1000 | Loss: 0.00002650
Iteration 170/1000 | Loss: 0.00002628
Iteration 171/1000 | Loss: 0.00002625
Iteration 172/1000 | Loss: 0.00002625
Iteration 173/1000 | Loss: 0.00002624
Iteration 174/1000 | Loss: 0.00002624
Iteration 175/1000 | Loss: 0.00002624
Iteration 176/1000 | Loss: 0.00002998
Iteration 177/1000 | Loss: 0.00002764
Iteration 178/1000 | Loss: 0.00002804
Iteration 179/1000 | Loss: 0.00002642
Iteration 180/1000 | Loss: 0.00003085
Iteration 181/1000 | Loss: 0.00002930
Iteration 182/1000 | Loss: 0.00002851
Iteration 183/1000 | Loss: 0.00002647
Iteration 184/1000 | Loss: 0.00002988
Iteration 185/1000 | Loss: 0.00004151
Iteration 186/1000 | Loss: 0.00003179
Iteration 187/1000 | Loss: 0.00002952
Iteration 188/1000 | Loss: 0.00002896
Iteration 189/1000 | Loss: 0.00002665
Iteration 190/1000 | Loss: 0.00002936
Iteration 191/1000 | Loss: 0.00002876
Iteration 192/1000 | Loss: 0.00002893
Iteration 193/1000 | Loss: 0.00003064
Iteration 194/1000 | Loss: 0.00002722
Iteration 195/1000 | Loss: 0.00002909
Iteration 196/1000 | Loss: 0.00003519
Iteration 197/1000 | Loss: 0.00003174
Iteration 198/1000 | Loss: 0.00003037
Iteration 199/1000 | Loss: 0.00002728
Iteration 200/1000 | Loss: 0.00002876
Iteration 201/1000 | Loss: 0.00003957
Iteration 202/1000 | Loss: 0.00003253
Iteration 203/1000 | Loss: 0.00003333
Iteration 204/1000 | Loss: 0.00002740
Iteration 205/1000 | Loss: 0.00003167
Iteration 206/1000 | Loss: 0.00002837
Iteration 207/1000 | Loss: 0.00005764
Iteration 208/1000 | Loss: 0.00003860
Iteration 209/1000 | Loss: 0.00003136
Iteration 210/1000 | Loss: 0.00002992
Iteration 211/1000 | Loss: 0.00002666
Iteration 212/1000 | Loss: 0.00003099
Iteration 213/1000 | Loss: 0.00003020
Iteration 214/1000 | Loss: 0.00005989
Iteration 215/1000 | Loss: 0.00003035
Iteration 216/1000 | Loss: 0.00002693
Iteration 217/1000 | Loss: 0.00002770
Iteration 218/1000 | Loss: 0.00002672
Iteration 219/1000 | Loss: 0.00002624
Iteration 220/1000 | Loss: 0.00002624
Iteration 221/1000 | Loss: 0.00002624
Iteration 222/1000 | Loss: 0.00002624
Iteration 223/1000 | Loss: 0.00002771
Iteration 224/1000 | Loss: 0.00002658
Iteration 225/1000 | Loss: 0.00002894
Iteration 226/1000 | Loss: 0.00002688
Iteration 227/1000 | Loss: 0.00002893
Iteration 228/1000 | Loss: 0.00002701
Iteration 229/1000 | Loss: 0.00002874
Iteration 230/1000 | Loss: 0.00002680
Iteration 231/1000 | Loss: 0.00003353
Iteration 232/1000 | Loss: 0.00002780
Iteration 233/1000 | Loss: 0.00003221
Iteration 234/1000 | Loss: 0.00002878
Iteration 235/1000 | Loss: 0.00003396
Iteration 236/1000 | Loss: 0.00002876
Iteration 237/1000 | Loss: 0.00003303
Iteration 238/1000 | Loss: 0.00002947
Iteration 239/1000 | Loss: 0.00003403
Iteration 240/1000 | Loss: 0.00003039
Iteration 241/1000 | Loss: 0.00003238
Iteration 242/1000 | Loss: 0.00003197
Iteration 243/1000 | Loss: 0.00003412
Iteration 244/1000 | Loss: 0.00003147
Iteration 245/1000 | Loss: 0.00003016
Iteration 246/1000 | Loss: 0.00002669
Iteration 247/1000 | Loss: 0.00002899
Iteration 248/1000 | Loss: 0.00002638
Iteration 249/1000 | Loss: 0.00002637
Iteration 250/1000 | Loss: 0.00002637
Iteration 251/1000 | Loss: 0.00002637
Iteration 252/1000 | Loss: 0.00002636
Iteration 253/1000 | Loss: 0.00002647
Iteration 254/1000 | Loss: 0.00002647
Iteration 255/1000 | Loss: 0.00003464
Iteration 256/1000 | Loss: 0.00002735
Iteration 257/1000 | Loss: 0.00003385
Iteration 258/1000 | Loss: 0.00003074
Iteration 259/1000 | Loss: 0.00003567
Iteration 260/1000 | Loss: 0.00003061
Iteration 261/1000 | Loss: 0.00003398
Iteration 262/1000 | Loss: 0.00003039
Iteration 263/1000 | Loss: 0.00002794
Iteration 264/1000 | Loss: 0.00002643
Iteration 265/1000 | Loss: 0.00003334
Iteration 266/1000 | Loss: 0.00003004
Iteration 267/1000 | Loss: 0.00003456
Iteration 268/1000 | Loss: 0.00003075
Iteration 269/1000 | Loss: 0.00002629
Iteration 270/1000 | Loss: 0.00003583
Iteration 271/1000 | Loss: 0.00003261
Iteration 272/1000 | Loss: 0.00002775
Iteration 273/1000 | Loss: 0.00002829
Iteration 274/1000 | Loss: 0.00002782
Iteration 275/1000 | Loss: 0.00002649
Iteration 276/1000 | Loss: 0.00002859
Iteration 277/1000 | Loss: 0.00002650
Iteration 278/1000 | Loss: 0.00002904
Iteration 279/1000 | Loss: 0.00002650
Iteration 280/1000 | Loss: 0.00002852
Iteration 281/1000 | Loss: 0.00002671
Iteration 282/1000 | Loss: 0.00002671
Iteration 283/1000 | Loss: 0.00003408
Iteration 284/1000 | Loss: 0.00002840
Iteration 285/1000 | Loss: 0.00003052
Iteration 286/1000 | Loss: 0.00002703
Iteration 287/1000 | Loss: 0.00002961
Iteration 288/1000 | Loss: 0.00002698
Iteration 289/1000 | Loss: 0.00003409
Iteration 290/1000 | Loss: 0.00002759
Iteration 291/1000 | Loss: 0.00003502
Iteration 292/1000 | Loss: 0.00002763
Iteration 293/1000 | Loss: 0.00003484
Iteration 294/1000 | Loss: 0.00005047
Iteration 295/1000 | Loss: 0.00003094
Iteration 296/1000 | Loss: 0.00002639
Iteration 297/1000 | Loss: 0.00002631
Iteration 298/1000 | Loss: 0.00002631
Iteration 299/1000 | Loss: 0.00002631
Iteration 300/1000 | Loss: 0.00002631
Iteration 301/1000 | Loss: 0.00002631
Iteration 302/1000 | Loss: 0.00002631
Iteration 303/1000 | Loss: 0.00002631
Iteration 304/1000 | Loss: 0.00002631
Iteration 305/1000 | Loss: 0.00002631
Iteration 306/1000 | Loss: 0.00002631
Iteration 307/1000 | Loss: 0.00002630
Iteration 308/1000 | Loss: 0.00002630
Iteration 309/1000 | Loss: 0.00002630
Iteration 310/1000 | Loss: 0.00002630
Iteration 311/1000 | Loss: 0.00002629
Iteration 312/1000 | Loss: 0.00002629
Iteration 313/1000 | Loss: 0.00002628
Iteration 314/1000 | Loss: 0.00002628
Iteration 315/1000 | Loss: 0.00002627
Iteration 316/1000 | Loss: 0.00002684
Iteration 317/1000 | Loss: 0.00003170
Iteration 318/1000 | Loss: 0.00003082
Iteration 319/1000 | Loss: 0.00002680
Iteration 320/1000 | Loss: 0.00003597
Iteration 321/1000 | Loss: 0.00002718
Iteration 322/1000 | Loss: 0.00002743
Iteration 323/1000 | Loss: 0.00003521
Iteration 324/1000 | Loss: 0.00002727
Iteration 325/1000 | Loss: 0.00002747
Iteration 326/1000 | Loss: 0.00003516
Iteration 327/1000 | Loss: 0.00003133
Iteration 328/1000 | Loss: 0.00002733
Iteration 329/1000 | Loss: 0.00002828
Iteration 330/1000 | Loss: 0.00003235
Iteration 331/1000 | Loss: 0.00003154
Iteration 332/1000 | Loss: 0.00002851
Iteration 333/1000 | Loss: 0.00002887
Iteration 334/1000 | Loss: 0.00003206
Iteration 335/1000 | Loss: 0.00002831
Iteration 336/1000 | Loss: 0.00002849
Iteration 337/1000 | Loss: 0.00003202
Iteration 338/1000 | Loss: 0.00003185
Iteration 339/1000 | Loss: 0.00003062
Iteration 340/1000 | Loss: 0.00003187
Iteration 341/1000 | Loss: 0.00003063
Iteration 342/1000 | Loss: 0.00003137
Iteration 343/1000 | Loss: 0.00005495
Iteration 344/1000 | Loss: 0.00003331
Iteration 345/1000 | Loss: 0.00003804
Iteration 346/1000 | Loss: 0.00003742
Iteration 347/1000 | Loss: 0.00003038
Iteration 348/1000 | Loss: 0.00003181
Iteration 349/1000 | Loss: 0.00002997
Iteration 350/1000 | Loss: 0.00003190
Iteration 351/1000 | Loss: 0.00002840
Iteration 352/1000 | Loss: 0.00004897
Iteration 353/1000 | Loss: 0.00004655
Iteration 354/1000 | Loss: 0.00002737
Iteration 355/1000 | Loss: 0.00003181
Iteration 356/1000 | Loss: 0.00003032
Iteration 357/1000 | Loss: 0.00003299
Iteration 358/1000 | Loss: 0.00003006
Iteration 359/1000 | Loss: 0.00002996
Iteration 360/1000 | Loss: 0.00003048
Iteration 361/1000 | Loss: 0.00003029
Iteration 362/1000 | Loss: 0.00002933
Iteration 363/1000 | Loss: 0.00003081
Iteration 364/1000 | Loss: 0.00002884
Iteration 365/1000 | Loss: 0.00002778
Iteration 366/1000 | Loss: 0.00002691
Iteration 367/1000 | Loss: 0.00003135
Iteration 368/1000 | Loss: 0.00003045
Iteration 369/1000 | Loss: 0.00003292
Iteration 370/1000 | Loss: 0.00002967
Iteration 371/1000 | Loss: 0.00003021
Iteration 372/1000 | Loss: 0.00002978
Iteration 373/1000 | Loss: 0.00003264
Iteration 374/1000 | Loss: 0.00002951
Iteration 375/1000 | Loss: 0.00002831
Iteration 376/1000 | Loss: 0.00002666
Iteration 377/1000 | Loss: 0.00003181
Iteration 378/1000 | Loss: 0.00002853
Iteration 379/1000 | Loss: 0.00003048
Iteration 380/1000 | Loss: 0.00003010
Iteration 381/1000 | Loss: 0.00002639
Iteration 382/1000 | Loss: 0.00003209
Iteration 383/1000 | Loss: 0.00003072
Iteration 384/1000 | Loss: 0.00003028
Iteration 385/1000 | Loss: 0.00002893
Iteration 386/1000 | Loss: 0.00003012
Iteration 387/1000 | Loss: 0.00002988
Iteration 388/1000 | Loss: 0.00002991
Iteration 389/1000 | Loss: 0.00002928
Iteration 390/1000 | Loss: 0.00002980
Iteration 391/1000 | Loss: 0.00003048
Iteration 392/1000 | Loss: 0.00003052
Iteration 393/1000 | Loss: 0.00003047
Iteration 394/1000 | Loss: 0.00002968
Iteration 395/1000 | Loss: 0.00003016
Iteration 396/1000 | Loss: 0.00003037
Iteration 397/1000 | Loss: 0.00002820
Iteration 398/1000 | Loss: 0.00002679
Iteration 399/1000 | Loss: 0.00002643
Iteration 400/1000 | Loss: 0.00002727
Iteration 401/1000 | Loss: 0.00003210
Iteration 402/1000 | Loss: 0.00002853
Iteration 403/1000 | Loss: 0.00002834
Iteration 404/1000 | Loss: 0.00006088
Iteration 405/1000 | Loss: 0.00003179
Iteration 406/1000 | Loss: 0.00003632
Iteration 407/1000 | Loss: 0.00002728
Iteration 408/1000 | Loss: 0.00002807
Iteration 409/1000 | Loss: 0.00003035
Iteration 410/1000 | Loss: 0.00005226
Iteration 411/1000 | Loss: 0.00010976
Iteration 412/1000 | Loss: 0.00003138
Iteration 413/1000 | Loss: 0.00003194
Iteration 414/1000 | Loss: 0.00005597
Iteration 415/1000 | Loss: 0.00003083
Iteration 416/1000 | Loss: 0.00002818
Iteration 417/1000 | Loss: 0.00003558
Iteration 418/1000 | Loss: 0.00003816
Iteration 419/1000 | Loss: 0.00002858
Iteration 420/1000 | Loss: 0.00003355
Iteration 421/1000 | Loss: 0.00002836
Iteration 422/1000 | Loss: 0.00003056
Iteration 423/1000 | Loss: 0.00002880
Iteration 424/1000 | Loss: 0.00003020
Iteration 425/1000 | Loss: 0.00002922
Iteration 426/1000 | Loss: 0.00003232
Iteration 427/1000 | Loss: 0.00002904
Iteration 428/1000 | Loss: 0.00003117
Iteration 429/1000 | Loss: 0.00002975
Iteration 430/1000 | Loss: 0.00003246
Iteration 431/1000 | Loss: 0.00003076
Iteration 432/1000 | Loss: 0.00003076
Iteration 433/1000 | Loss: 0.00003361
Iteration 434/1000 | Loss: 0.00003077
Iteration 435/1000 | Loss: 0.00002737
Iteration 436/1000 | Loss: 0.00003129
Iteration 437/1000 | Loss: 0.00002851
Iteration 438/1000 | Loss: 0.00003161
Iteration 439/1000 | Loss: 0.00003139
Iteration 440/1000 | Loss: 0.00003245
Iteration 441/1000 | Loss: 0.00002734
Iteration 442/1000 | Loss: 0.00002701
Iteration 443/1000 | Loss: 0.00002645
Iteration 444/1000 | Loss: 0.00002912
Iteration 445/1000 | Loss: 0.00002765
Iteration 446/1000 | Loss: 0.00002985
Iteration 447/1000 | Loss: 0.00002955
Iteration 448/1000 | Loss: 0.00003180
Iteration 449/1000 | Loss: 0.00002894
Iteration 450/1000 | Loss: 0.00002894
Iteration 451/1000 | Loss: 0.00002742
Iteration 452/1000 | Loss: 0.00002725
Iteration 453/1000 | Loss: 0.00002641
Iteration 454/1000 | Loss: 0.00003266
Iteration 455/1000 | Loss: 0.00002739
Iteration 456/1000 | Loss: 0.00002889
Iteration 457/1000 | Loss: 0.00002830
Iteration 458/1000 | Loss: 0.00002864
Iteration 459/1000 | Loss: 0.00003025
Iteration 460/1000 | Loss: 0.00002710
Iteration 461/1000 | Loss: 0.00002990
Iteration 462/1000 | Loss: 0.00002777
Iteration 463/1000 | Loss: 0.00002920
Iteration 464/1000 | Loss: 0.00002847
Iteration 465/1000 | Loss: 0.00002924
Iteration 466/1000 | Loss: 0.00002808
Iteration 467/1000 | Loss: 0.00002808
Iteration 468/1000 | Loss: 0.00002983
Iteration 469/1000 | Loss: 0.00002803
Iteration 470/1000 | Loss: 0.00002956
Iteration 471/1000 | Loss: 0.00002900
Iteration 472/1000 | Loss: 0.00002911
Iteration 473/1000 | Loss: 0.00002913
Iteration 474/1000 | Loss: 0.00002882
Iteration 475/1000 | Loss: 0.00003103
Iteration 476/1000 | Loss: 0.00003143
Iteration 477/1000 | Loss: 0.00003017
Iteration 478/1000 | Loss: 0.00003154
Iteration 479/1000 | Loss: 0.00003154
Iteration 480/1000 | Loss: 0.00003057
Iteration 481/1000 | Loss: 0.00003005
Iteration 482/1000 | Loss: 0.00003250
Iteration 483/1000 | Loss: 0.00003267
Iteration 484/1000 | Loss: 0.00003233
Iteration 485/1000 | Loss: 0.00003129
Iteration 486/1000 | Loss: 0.00003220
Iteration 487/1000 | Loss: 0.00003064
Iteration 488/1000 | Loss: 0.00003179
Iteration 489/1000 | Loss: 0.00002991
Iteration 490/1000 | Loss: 0.00003472
Iteration 491/1000 | Loss: 0.00002944
Iteration 492/1000 | Loss: 0.00003588
Iteration 493/1000 | Loss: 0.00002984
Iteration 494/1000 | Loss: 0.00003129
Iteration 495/1000 | Loss: 0.00002692
Iteration 496/1000 | Loss: 0.00002653
Iteration 497/1000 | Loss: 0.00002756
Iteration 498/1000 | Loss: 0.00002711
Iteration 499/1000 | Loss: 0.00003277
Iteration 500/1000 | Loss: 0.00003091
Iteration 501/1000 | Loss: 0.00003343
Iteration 502/1000 | Loss: 0.00002985
Iteration 503/1000 | Loss: 0.00003661
Iteration 504/1000 | Loss: 0.00003043
Iteration 505/1000 | Loss: 0.00003342
Iteration 506/1000 | Loss: 0.00003620
Iteration 507/1000 | Loss: 0.00003252
Iteration 508/1000 | Loss: 0.00003675
Iteration 509/1000 | Loss: 0.00003689
Iteration 510/1000 | Loss: 0.00003535
Iteration 511/1000 | Loss: 0.00002805
Iteration 512/1000 | Loss: 0.00002827
Iteration 513/1000 | Loss: 0.00002766
Iteration 514/1000 | Loss: 0.00002649
Iteration 515/1000 | Loss: 0.00003034
Iteration 516/1000 | Loss: 0.00002915
Iteration 517/1000 | Loss: 0.00002923
Iteration 518/1000 | Loss: 0.00003202
Iteration 519/1000 | Loss: 0.00003212
Iteration 520/1000 | Loss: 0.00003074
Iteration 521/1000 | Loss: 0.00003137
Iteration 522/1000 | Loss: 0.00003344
Iteration 523/1000 | Loss: 0.00003076
Iteration 524/1000 | Loss: 0.00003374
Iteration 525/1000 | Loss: 0.00003019
Iteration 526/1000 | Loss: 0.00003271
Iteration 527/1000 | Loss: 0.00002992
Iteration 528/1000 | Loss: 0.00003055
Iteration 529/1000 | Loss: 0.00002982
Iteration 530/1000 | Loss: 0.00002919
Iteration 531/1000 | Loss: 0.00002991
Iteration 532/1000 | Loss: 0.00002849
Iteration 533/1000 | Loss: 0.00003034
Iteration 534/1000 | Loss: 0.00002903
Iteration 535/1000 | Loss: 0.00003009
Iteration 536/1000 | Loss: 0.00003036
Iteration 537/1000 | Loss: 0.00002969
Iteration 538/1000 | Loss: 0.00002941
Iteration 539/1000 | Loss: 0.00002866
Iteration 540/1000 | Loss: 0.00002872
Iteration 541/1000 | Loss: 0.00003110
Iteration 542/1000 | Loss: 0.00003015
Iteration 543/1000 | Loss: 0.00003065
Iteration 544/1000 | Loss: 0.00003163
Iteration 545/1000 | Loss: 0.00003002
Iteration 546/1000 | Loss: 0.00002849
Iteration 547/1000 | Loss: 0.00002820
Iteration 548/1000 | Loss: 0.00002651
Iteration 549/1000 | Loss: 0.00002706
Iteration 550/1000 | Loss: 0.00003198
Iteration 551/1000 | Loss: 0.00002675
Iteration 552/1000 | Loss: 0.00002639
Iteration 553/1000 | Loss: 0.00002639
Iteration 554/1000 | Loss: 0.00002639
Iteration 555/1000 | Loss: 0.00002639
Iteration 556/1000 | Loss: 0.00002639
Iteration 557/1000 | Loss: 0.00002639
Iteration 558/1000 | Loss: 0.00002639
Iteration 559/1000 | Loss: 0.00002639
Iteration 560/1000 | Loss: 0.00002638
Iteration 561/1000 | Loss: 0.00002638
Iteration 562/1000 | Loss: 0.00002708
Iteration 563/1000 | Loss: 0.00002817
Iteration 564/1000 | Loss: 0.00003258
Iteration 565/1000 | Loss: 0.00003637
Iteration 566/1000 | Loss: 0.00003156
Iteration 567/1000 | Loss: 0.00003521
Iteration 568/1000 | Loss: 0.00003143
Iteration 569/1000 | Loss: 0.00003215
Iteration 570/1000 | Loss: 0.00003153
Iteration 571/1000 | Loss: 0.00002949
Iteration 572/1000 | Loss: 0.00003278
Iteration 573/1000 | Loss: 0.00002891
Iteration 574/1000 | Loss: 0.00002949
Iteration 575/1000 | Loss: 0.00002880
Iteration 576/1000 | Loss: 0.00002756
Iteration 577/1000 | Loss: 0.00002796
Iteration 578/1000 | Loss: 0.00003051
Iteration 579/1000 | Loss: 0.00002940
Iteration 580/1000 | Loss: 0.00002968
Iteration 581/1000 | Loss: 0.00002924
Iteration 582/1000 | Loss: 0.00002884
Iteration 583/1000 | Loss: 0.00002678
Iteration 584/1000 | Loss: 0.00002655
Iteration 585/1000 | Loss: 0.00002848
Iteration 586/1000 | Loss: 0.00003142
Iteration 587/1000 | Loss: 0.00002799
Iteration 588/1000 | Loss: 0.00003203
Iteration 589/1000 | Loss: 0.00002869
Iteration 590/1000 | Loss: 0.00003294
Iteration 591/1000 | Loss: 0.00002835
Iteration 592/1000 | Loss: 0.00003236
Iteration 593/1000 | Loss: 0.00002822
Iteration 594/1000 | Loss: 0.00002822
Iteration 595/1000 | Loss: 0.00003160
Iteration 596/1000 | Loss: 0.00002809
Iteration 597/1000 | Loss: 0.00002808
Iteration 598/1000 | Loss: 0.00002808
Iteration 599/1000 | Loss: 0.00002808
Iteration 600/1000 | Loss: 0.00002808
Iteration 601/1000 | Loss: 0.00002808
Iteration 602/1000 | Loss: 0.00002808
Iteration 603/1000 | Loss: 0.00002807
Iteration 604/1000 | Loss: 0.00002807
Iteration 605/1000 | Loss: 0.00002807
Iteration 606/1000 | Loss: 0.00003101
Iteration 607/1000 | Loss: 0.00002790
Iteration 608/1000 | Loss: 0.00003083
Iteration 609/1000 | Loss: 0.00002726
Iteration 610/1000 | Loss: 0.00002639
Iteration 611/1000 | Loss: 0.00002636
Iteration 612/1000 | Loss: 0.00002634
Iteration 613/1000 | Loss: 0.00002633
Iteration 614/1000 | Loss: 0.00002694
Iteration 615/1000 | Loss: 0.00002728
Iteration 616/1000 | Loss: 0.00002995
Iteration 617/1000 | Loss: 0.00002904
Iteration 618/1000 | Loss: 0.00003022
Iteration 619/1000 | Loss: 0.00002916
Iteration 620/1000 | Loss: 0.00003160
Iteration 621/1000 | Loss: 0.00003053
Iteration 622/1000 | Loss: 0.00002827
Iteration 623/1000 | Loss: 0.00002796
Iteration 624/1000 | Loss: 0.00002812
Iteration 625/1000 | Loss: 0.00002942
Iteration 626/1000 | Loss: 0.00002812
Iteration 627/1000 | Loss: 0.00002760
Iteration 628/1000 | Loss: 0.00002640
Iteration 629/1000 | Loss: 0.00003097
Iteration 630/1000 | Loss: 0.00002666
Iteration 631/1000 | Loss: 0.00002642
Iteration 632/1000 | Loss: 0.00002849
Iteration 633/1000 | Loss: 0.00002849
Iteration 634/1000 | Loss: 0.00002641
Iteration 635/1000 | Loss: 0.00003206
Iteration 636/1000 | Loss: 0.00002804
Iteration 637/1000 | Loss: 0.00002810
Iteration 638/1000 | Loss: 0.00002868
Iteration 639/1000 | Loss: 0.00002864
Iteration 640/1000 | Loss: 0.00003219
Iteration 641/1000 | Loss: 0.00002833
Iteration 642/1000 | Loss: 0.00002980
Iteration 643/1000 | Loss: 0.00002753
Iteration 644/1000 | Loss: 0.00002928
Iteration 645/1000 | Loss: 0.00002770
Iteration 646/1000 | Loss: 0.00002904
Iteration 647/1000 | Loss: 0.00002779
Iteration 648/1000 | Loss: 0.00002911
Iteration 649/1000 | Loss: 0.00002746
Iteration 650/1000 | Loss: 0.00003054
Iteration 651/1000 | Loss: 0.00002732
Iteration 652/1000 | Loss: 0.00003018
Iteration 653/1000 | Loss: 0.00002720
Iteration 654/1000 | Loss: 0.00003064
Iteration 655/1000 | Loss: 0.00002706
Iteration 656/1000 | Loss: 0.00003039
Iteration 657/1000 | Loss: 0.00002733
Iteration 658/1000 | Loss: 0.00002849
Iteration 659/1000 | Loss: 0.00002753
Iteration 660/1000 | Loss: 0.00002825
Iteration 661/1000 | Loss: 0.00002787
Iteration 662/1000 | Loss: 0.00002776
Iteration 663/1000 | Loss: 0.00002746
Iteration 664/1000 | Loss: 0.00002718
Iteration 665/1000 | Loss: 0.00002740
Iteration 666/1000 | Loss: 0.00002696
Iteration 667/1000 | Loss: 0.00002728
Iteration 668/1000 | Loss: 0.00002686
Iteration 669/1000 | Loss: 0.00002735
Iteration 670/1000 | Loss: 0.00002680
Iteration 671/1000 | Loss: 0.00002737
Iteration 672/1000 | Loss: 0.00002664
Iteration 673/1000 | Loss: 0.00002664
Iteration 674/1000 | Loss: 0.00002643
Iteration 675/1000 | Loss: 0.00002869
Iteration 676/1000 | Loss: 0.00002702
Iteration 677/1000 | Loss: 0.00003050
Iteration 678/1000 | Loss: 0.00002803
Iteration 679/1000 | Loss: 0.00003016
Iteration 680/1000 | Loss: 0.00002750
Iteration 681/1000 | Loss: 0.00002958
Iteration 682/1000 | Loss: 0.00002741
Iteration 683/1000 | Loss: 0.00002768
Iteration 684/1000 | Loss: 0.00003047
Iteration 685/1000 | Loss: 0.00002694
Iteration 686/1000 | Loss: 0.00002664
Iteration 687/1000 | Loss: 0.00002650
Iteration 688/1000 | Loss: 0.00002701
Iteration 689/1000 | Loss: 0.00002637
Iteration 690/1000 | Loss: 0.00002625
Iteration 691/1000 | Loss: 0.00002624
Iteration 692/1000 | Loss: 0.00002624
Iteration 693/1000 | Loss: 0.00002624
Iteration 694/1000 | Loss: 0.00002703
Iteration 695/1000 | Loss: 0.00002750
Iteration 696/1000 | Loss: 0.00002662
Iteration 697/1000 | Loss: 0.00002692
Iteration 698/1000 | Loss: 0.00003009
Iteration 699/1000 | Loss: 0.00002865
Iteration 700/1000 | Loss: 0.00003068
Iteration 701/1000 | Loss: 0.00003129
Iteration 702/1000 | Loss: 0.00003012
Iteration 703/1000 | Loss: 0.00002934
Iteration 704/1000 | Loss: 0.00002867
Iteration 705/1000 | Loss: 0.00002764
Iteration 706/1000 | Loss: 0.00002651
Iteration 707/1000 | Loss: 0.00002722
Iteration 708/1000 | Loss: 0.00002848
Iteration 709/1000 | Loss: 0.00002922
Iteration 710/1000 | Loss: 0.00002816
Iteration 711/1000 | Loss: 0.00002712
Iteration 712/1000 | Loss: 0.00002622
Iteration 713/1000 | Loss: 0.00002621
Iteration 714/1000 | Loss: 0.00002621
Iteration 715/1000 | Loss: 0.00002621
Iteration 716/1000 | Loss: 0.00002621
Iteration 717/1000 | Loss: 0.00002621
Iteration 718/1000 | Loss: 0.00002621
Iteration 719/1000 | Loss: 0.00002621
Iteration 720/1000 | Loss: 0.00002621
Iteration 721/1000 | Loss: 0.00002621
Iteration 722/1000 | Loss: 0.00002621
Iteration 723/1000 | Loss: 0.00002702
Iteration 724/1000 | Loss: 0.00002886
Iteration 725/1000 | Loss: 0.00002669
Iteration 726/1000 | Loss: 0.00002709
Iteration 727/1000 | Loss: 0.00002988
Iteration 728/1000 | Loss: 0.00002730
Iteration 729/1000 | Loss: 0.00003027
Iteration 730/1000 | Loss: 0.00002994
Iteration 731/1000 | Loss: 0.00002745
Iteration 732/1000 | Loss: 0.00002880
Iteration 733/1000 | Loss: 0.00003032
Iteration 734/1000 | Loss: 0.00002956
Iteration 735/1000 | Loss: 0.00002995
Iteration 736/1000 | Loss: 0.00003038
Iteration 737/1000 | Loss: 0.00002938
Iteration 738/1000 | Loss: 0.00002879
Iteration 739/1000 | Loss: 0.00003014
Iteration 740/1000 | Loss: 0.00002899
Iteration 741/1000 | Loss: 0.00003175
Iteration 742/1000 | Loss: 0.00003170
Iteration 743/1000 | Loss: 0.00003157
Iteration 744/1000 | Loss: 0.00002954
Iteration 745/1000 | Loss: 0.00003223
Iteration 746/1000 | Loss: 0.00003015
Iteration 747/1000 | Loss: 0.00003095
Iteration 748/1000 | Loss: 0.00003035
Iteration 749/1000 | Loss: 0.00003086
Iteration 750/1000 | Loss: 0.00003029
Iteration 751/1000 | Loss: 0.00003201
Iteration 752/1000 | Loss: 0.00003205
Iteration 753/1000 | Loss: 0.00003125
Iteration 754/1000 | Loss: 0.00003141
Iteration 755/1000 | Loss: 0.00003237
Iteration 756/1000 | Loss: 0.00003062
Iteration 757/1000 | Loss: 0.00003087
Iteration 758/1000 | Loss: 0.00003055
Iteration 759/1000 | Loss: 0.00003176
Iteration 760/1000 | Loss: 0.00002996
Iteration 761/1000 | Loss: 0.00003197
Iteration 762/1000 | Loss: 0.00003014
Iteration 763/1000 | Loss: 0.00003026
Iteration 764/1000 | Loss: 0.00003116
Iteration 765/1000 | Loss: 0.00002963
Iteration 766/1000 | Loss: 0.00003178
Iteration 767/1000 | Loss: 0.00002874
Iteration 768/1000 | Loss: 0.00002851
Iteration 769/1000 | Loss: 0.00003115
Iteration 770/1000 | Loss: 0.00002973
Iteration 771/1000 | Loss: 0.00002991
Iteration 772/1000 | Loss: 0.00003055
Iteration 773/1000 | Loss: 0.00003057
Iteration 774/1000 | Loss: 0.00002925
Iteration 775/1000 | Loss: 0.00003139
Iteration 776/1000 | Loss: 0.00002974
Iteration 777/1000 | Loss: 0.00003108
Iteration 778/1000 | Loss: 0.00002941
Iteration 779/1000 | Loss: 0.00003010
Iteration 780/1000 | Loss: 0.00002991
Iteration 781/1000 | Loss: 0.00003142
Iteration 782/1000 | Loss: 0.00002965
Iteration 783/1000 | Loss: 0.00003282
Iteration 784/1000 | Loss: 0.00002761
Iteration 785/1000 | Loss: 0.00002652
Iteration 786/1000 | Loss: 0.00002713
Iteration 787/1000 | Loss: 0.00002970
Iteration 788/1000 | Loss: 0.00002802
Iteration 789/1000 | Loss: 0.00003132
Iteration 790/1000 | Loss: 0.00002763
Iteration 791/1000 | Loss: 0.00002682
Iteration 792/1000 | Loss: 0.00002796
Iteration 793/1000 | Loss: 0.00002804
Iteration 794/1000 | Loss: 0.00002818
Iteration 795/1000 | Loss: 0.00002825
Iteration 796/1000 | Loss: 0.00002798
Iteration 797/1000 | Loss: 0.00002748
Iteration 798/1000 | Loss: 0.00002847
Iteration 799/1000 | Loss: 0.00002862
Iteration 800/1000 | Loss: 0.00002800
Iteration 801/1000 | Loss: 0.00002760
Iteration 802/1000 | Loss: 0.00002664
Iteration 803/1000 | Loss: 0.00002671
Iteration 804/1000 | Loss: 0.00002638
Iteration 805/1000 | Loss: 0.00002636
Iteration 806/1000 | Loss: 0.00002644
Iteration 807/1000 | Loss: 0.00002644
Iteration 808/1000 | Loss: 0.00002911
Iteration 809/1000 | Loss: 0.00002761
Iteration 810/1000 | Loss: 0.00003007
Iteration 811/1000 | Loss: 0.00002837
Iteration 812/1000 | Loss: 0.00003152
Iteration 813/1000 | Loss: 0.00002779
Iteration 814/1000 | Loss: 0.00003011
Iteration 815/1000 | Loss: 0.00002764
Iteration 816/1000 | Loss: 0.00002889
Iteration 817/1000 | Loss: 0.00002768
Iteration 818/1000 | Loss: 0.00003010
Iteration 819/1000 | Loss: 0.00002787
Iteration 820/1000 | Loss: 0.00002972
Iteration 821/1000 | Loss: 0.00002864
Iteration 822/1000 | Loss: 0.00002924
Iteration 823/1000 | Loss: 0.00002949
Iteration 824/1000 | Loss: 0.00002958
Iteration 825/1000 | Loss: 0.00002875
Iteration 826/1000 | Loss: 0.00002875
Iteration 827/1000 | Loss: 0.00002871
Iteration 828/1000 | Loss: 0.00002757
Iteration 829/1000 | Loss: 0.00003134
Iteration 830/1000 | Loss: 0.00002819
Iteration 831/1000 | Loss: 0.00002990
Iteration 832/1000 | Loss: 0.00002953
Iteration 833/1000 | Loss: 0.00002943
Iteration 834/1000 | Loss: 0.00002808
Iteration 835/1000 | Loss: 0.00002907
Iteration 836/1000 | Loss: 0.00002782
Iteration 837/1000 | Loss: 0.00002706
Iteration 838/1000 | Loss: 0.00002678
Iteration 839/1000 | Loss: 0.00002894
Iteration 840/1000 | Loss: 0.00002930
Iteration 841/1000 | Loss: 0.00002836
Iteration 842/1000 | Loss: 0.00002943
Iteration 843/1000 | Loss: 0.00002818
Iteration 844/1000 | Loss: 0.00002730
Iteration 845/1000 | Loss: 0.00002861
Iteration 846/1000 | Loss: 0.00002777
Iteration 847/1000 | Loss: 0.00002846
Iteration 848/1000 | Loss: 0.00002751
Iteration 849/1000 | Loss: 0.00002751
Iteration 850/1000 | Loss: 0.00002751
Iteration 851/1000 | Loss: 0.00002751
Iteration 852/1000 | Loss: 0.00002751
Iteration 853/1000 | Loss: 0.00002751
Iteration 854/1000 | Loss: 0.00002722
Iteration 855/1000 | Loss: 0.00002724
Iteration 856/1000 | Loss: 0.00002696
Iteration 857/1000 | Loss: 0.00002952
Iteration 858/1000 | Loss: 0.00002938
Iteration 859/1000 | Loss: 0.00002987
Iteration 860/1000 | Loss: 0.00002892
Iteration 861/1000 | Loss: 0.00002970
Iteration 862/1000 | Loss: 0.00002883
Iteration 863/1000 | Loss: 0.00003079
Iteration 864/1000 | Loss: 0.00002912
Iteration 865/1000 | Loss: 0.00002779
Iteration 866/1000 | Loss: 0.00002995
Iteration 867/1000 | Loss: 0.00002663
Iteration 868/1000 | Loss: 0.00002640
Iteration 869/1000 | Loss: 0.00002816
Iteration 870/1000 | Loss: 0.00002899
Iteration 871/1000 | Loss: 0.00002913
Iteration 872/1000 | Loss: 0.00002793
Iteration 873/1000 | Loss: 0.00003086
Iteration 874/1000 | Loss: 0.00002915
Iteration 875/1000 | Loss: 0.00003018
Iteration 876/1000 | Loss: 0.00002859
Iteration 877/1000 | Loss: 0.00003108
Iteration 878/1000 | Loss: 0.00002851
Iteration 879/1000 | Loss: 0.00003113
Iteration 880/1000 | Loss: 0.00002805
Iteration 881/1000 | Loss: 0.00003180
Iteration 882/1000 | Loss: 0.00002829
Iteration 883/1000 | Loss: 0.00003106
Iteration 884/1000 | Loss: 0.00002858
Iteration 885/1000 | Loss: 0.00003172
Iteration 886/1000 | Loss: 0.00002892
Iteration 887/1000 | Loss: 0.00003177
Iteration 888/1000 | Loss: 0.00002934
Iteration 889/1000 | Loss: 0.00003169
Iteration 890/1000 | Loss: 0.00002887
Iteration 891/1000 | Loss: 0.00002825
Iteration 892/1000 | Loss: 0.00002723
Iteration 893/1000 | Loss: 0.00003076
Iteration 894/1000 | Loss: 0.00002904
Iteration 895/1000 | Loss: 0.00002666
Iteration 896/1000 | Loss: 0.00002720
Iteration 897/1000 | Loss: 0.00002947
Iteration 898/1000 | Loss: 0.00002753
Iteration 899/1000 | Loss: 0.00003128
Iteration 900/1000 | Loss: 0.00002743
Iteration 901/1000 | Loss: 0.00002643
Iteration 902/1000 | Loss: 0.00002890
Iteration 903/1000 | Loss: 0.00003163
Iteration 904/1000 | Loss: 0.00002698
Iteration 905/1000 | Loss: 0.00002644
Iteration 906/1000 | Loss: 0.00003326
Iteration 907/1000 | Loss: 0.00002792
Iteration 908/1000 | Loss: 0.00002958
Iteration 909/1000 | Loss: 0.00003031
Iteration 910/1000 | Loss: 0.00002763
Iteration 911/1000 | Loss: 0.00002725
Iteration 912/1000 | Loss: 0.00002672
Iteration 913/1000 | Loss: 0.00002903
Iteration 914/1000 | Loss: 0.00002936
Iteration 915/1000 | Loss: 0.00002637
Iteration 916/1000 | Loss: 0.00002630
Iteration 917/1000 | Loss: 0.00002749
Iteration 918/1000 | Loss: 0.00002645
Iteration 919/1000 | Loss: 0.00003505
Iteration 920/1000 | Loss: 0.00002778
Iteration 921/1000 | Loss: 0.00002707
Iteration 922/1000 | Loss: 0.00002965
Iteration 923/1000 | Loss: 0.00002673
Iteration 924/1000 | Loss: 0.00003091
Iteration 925/1000 | Loss: 0.00002867
Iteration 926/1000 | Loss: 0.00003539
Iteration 927/1000 | Loss: 0.00002867
Iteration 928/1000 | Loss: 0.00003533
Iteration 929/1000 | Loss: 0.00002797
Iteration 930/1000 | Loss: 0.00003142
Iteration 931/1000 | Loss: 0.00002808
Iteration 932/1000 | Loss: 0.00003178
Iteration 933/1000 | Loss: 0.00002789
Iteration 934/1000 | Loss: 0.00003001
Iteration 935/1000 | Loss: 0.00002822
Iteration 936/1000 | Loss: 0.00003225
Iteration 937/1000 | Loss: 0.00002863
Iteration 938/1000 | Loss: 0.00003108
Iteration 939/1000 | Loss: 0.00002881
Iteration 940/1000 | Loss: 0.00003194
Iteration 941/1000 | Loss: 0.00002894
Iteration 942/1000 | Loss: 0.00003187
Iteration 943/1000 | Loss: 0.00002940
Iteration 944/1000 | Loss: 0.00003039
Iteration 945/1000 | Loss: 0.00002961
Iteration 946/1000 | Loss: 0.00003017
Iteration 947/1000 | Loss: 0.00003004
Iteration 948/1000 | Loss: 0.00002810
Iteration 949/1000 | Loss: 0.00002829
Iteration 950/1000 | Loss: 0.00003332
Iteration 951/1000 | Loss: 0.00002863
Iteration 952/1000 | Loss: 0.00002866
Iteration 953/1000 | Loss: 0.00002776
Iteration 954/1000 | Loss: 0.00002658
Iteration 955/1000 | Loss: 0.00003183
Iteration 956/1000 | Loss: 0.00002668
Iteration 957/1000 | Loss: 0.00002691
Iteration 958/1000 | Loss: 0.00002823
Iteration 959/1000 | Loss: 0.00002729
Iteration 960/1000 | Loss: 0.00002830
Iteration 961/1000 | Loss: 0.00002652
Iteration 962/1000 | Loss: 0.00003131
Iteration 963/1000 | Loss: 0.00002792
Iteration 964/1000 | Loss: 0.00002794
Iteration 965/1000 | Loss: 0.00003241
Iteration 966/1000 | Loss: 0.00002650
Iteration 967/1000 | Loss: 0.00002627
Iteration 968/1000 | Loss: 0.00003285
Iteration 969/1000 | Loss: 0.00002777
Iteration 970/1000 | Loss: 0.00003312
Iteration 971/1000 | Loss: 0.00002775
Iteration 972/1000 | Loss: 0.00003210
Iteration 973/1000 | Loss: 0.00002860
Iteration 974/1000 | Loss: 0.00003464
Iteration 975/1000 | Loss: 0.00002828
Iteration 976/1000 | Loss: 0.00003001
Iteration 977/1000 | Loss: 0.00002749
Iteration 978/1000 | Loss: 0.00003342
Iteration 979/1000 | Loss: 0.00002697
Iteration 980/1000 | Loss: 0.00002637
Iteration 981/1000 | Loss: 0.00002622
Iteration 982/1000 | Loss: 0.00002622
Iteration 983/1000 | Loss: 0.00002621
Iteration 984/1000 | Loss: 0.00002621
Iteration 985/1000 | Loss: 0.00002856
Iteration 986/1000 | Loss: 0.00002855
Iteration 987/1000 | Loss: 0.00002845
Iteration 988/1000 | Loss: 0.00002701
Iteration 989/1000 | Loss: 0.00003064
Iteration 990/1000 | Loss: 0.00002868
Iteration 991/1000 | Loss: 0.00002724
Iteration 992/1000 | Loss: 0.00002753
Iteration 993/1000 | Loss: 0.00002710
Iteration 994/1000 | Loss: 0.00002632
Iteration 995/1000 | Loss: 0.00002630
Iteration 996/1000 | Loss: 0.00002630
Iteration 997/1000 | Loss: 0.00003293
Iteration 998/1000 | Loss: 0.00002891
Iteration 999/1000 | Loss: 0.00002688
Iteration 1000/1000 | Loss: 0.00003303

Optimization complete. Final v2v error: 4.302260398864746 mm

Highest mean error: 11.484214782714844 mm for frame 228

Lowest mean error: 3.7066659927368164 mm for frame 53

Saving results

Total time: 1414.2227430343628
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_39_us_1277/0016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_39_us_1277/0016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_39_us_1277/0016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00682669
Iteration 2/25 | Loss: 0.00140410
Iteration 3/25 | Loss: 0.00125834
Iteration 4/25 | Loss: 0.00124773
Iteration 5/25 | Loss: 0.00124547
Iteration 6/25 | Loss: 0.00124547
Iteration 7/25 | Loss: 0.00124547
Iteration 8/25 | Loss: 0.00124547
Iteration 9/25 | Loss: 0.00124547
Iteration 10/25 | Loss: 0.00124547
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0012454739771783352, 0.0012454739771783352, 0.0012454739771783352, 0.0012454739771783352, 0.0012454739771783352]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012454739771783352

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.53816533
Iteration 2/25 | Loss: 0.00092320
Iteration 3/25 | Loss: 0.00092320
Iteration 4/25 | Loss: 0.00092320
Iteration 5/25 | Loss: 0.00092320
Iteration 6/25 | Loss: 0.00092320
Iteration 7/25 | Loss: 0.00092320
Iteration 8/25 | Loss: 0.00092320
Iteration 9/25 | Loss: 0.00092320
Iteration 10/25 | Loss: 0.00092320
Iteration 11/25 | Loss: 0.00092320
Iteration 12/25 | Loss: 0.00092320
Iteration 13/25 | Loss: 0.00092320
Iteration 14/25 | Loss: 0.00092320
Iteration 15/25 | Loss: 0.00092320
Iteration 16/25 | Loss: 0.00092320
Iteration 17/25 | Loss: 0.00092320
Iteration 18/25 | Loss: 0.00092320
Iteration 19/25 | Loss: 0.00092320
Iteration 20/25 | Loss: 0.00092320
Iteration 21/25 | Loss: 0.00092320
Iteration 22/25 | Loss: 0.00092320
Iteration 23/25 | Loss: 0.00092320
Iteration 24/25 | Loss: 0.00092320
Iteration 25/25 | Loss: 0.00092320

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00092320
Iteration 2/1000 | Loss: 0.00009252
Iteration 3/1000 | Loss: 0.00005759
Iteration 4/1000 | Loss: 0.00003943
Iteration 5/1000 | Loss: 0.00003427
Iteration 6/1000 | Loss: 0.00003267
Iteration 7/1000 | Loss: 0.00003174
Iteration 8/1000 | Loss: 0.00003112
Iteration 9/1000 | Loss: 0.00003046
Iteration 10/1000 | Loss: 0.00003003
Iteration 11/1000 | Loss: 0.00002969
Iteration 12/1000 | Loss: 0.00002932
Iteration 13/1000 | Loss: 0.00002896
Iteration 14/1000 | Loss: 0.00002868
Iteration 15/1000 | Loss: 0.00002855
Iteration 16/1000 | Loss: 0.00002840
Iteration 17/1000 | Loss: 0.00002824
Iteration 18/1000 | Loss: 0.00002805
Iteration 19/1000 | Loss: 0.00002794
Iteration 20/1000 | Loss: 0.00002784
Iteration 21/1000 | Loss: 0.00002775
Iteration 22/1000 | Loss: 0.00002773
Iteration 23/1000 | Loss: 0.00002772
Iteration 24/1000 | Loss: 0.00002771
Iteration 25/1000 | Loss: 0.00002770
Iteration 26/1000 | Loss: 0.00002770
Iteration 27/1000 | Loss: 0.00002770
Iteration 28/1000 | Loss: 0.00002769
Iteration 29/1000 | Loss: 0.00002769
Iteration 30/1000 | Loss: 0.00002769
Iteration 31/1000 | Loss: 0.00002769
Iteration 32/1000 | Loss: 0.00002768
Iteration 33/1000 | Loss: 0.00002768
Iteration 34/1000 | Loss: 0.00002767
Iteration 35/1000 | Loss: 0.00002767
Iteration 36/1000 | Loss: 0.00002767
Iteration 37/1000 | Loss: 0.00002767
Iteration 38/1000 | Loss: 0.00002766
Iteration 39/1000 | Loss: 0.00002766
Iteration 40/1000 | Loss: 0.00002766
Iteration 41/1000 | Loss: 0.00002766
Iteration 42/1000 | Loss: 0.00002765
Iteration 43/1000 | Loss: 0.00002765
Iteration 44/1000 | Loss: 0.00002765
Iteration 45/1000 | Loss: 0.00002765
Iteration 46/1000 | Loss: 0.00002765
Iteration 47/1000 | Loss: 0.00002764
Iteration 48/1000 | Loss: 0.00002764
Iteration 49/1000 | Loss: 0.00002763
Iteration 50/1000 | Loss: 0.00002763
Iteration 51/1000 | Loss: 0.00002763
Iteration 52/1000 | Loss: 0.00002762
Iteration 53/1000 | Loss: 0.00002762
Iteration 54/1000 | Loss: 0.00002761
Iteration 55/1000 | Loss: 0.00002761
Iteration 56/1000 | Loss: 0.00002760
Iteration 57/1000 | Loss: 0.00002760
Iteration 58/1000 | Loss: 0.00002760
Iteration 59/1000 | Loss: 0.00002759
Iteration 60/1000 | Loss: 0.00002759
Iteration 61/1000 | Loss: 0.00002759
Iteration 62/1000 | Loss: 0.00002758
Iteration 63/1000 | Loss: 0.00002758
Iteration 64/1000 | Loss: 0.00002758
Iteration 65/1000 | Loss: 0.00002757
Iteration 66/1000 | Loss: 0.00002757
Iteration 67/1000 | Loss: 0.00002757
Iteration 68/1000 | Loss: 0.00002756
Iteration 69/1000 | Loss: 0.00002756
Iteration 70/1000 | Loss: 0.00002755
Iteration 71/1000 | Loss: 0.00002755
Iteration 72/1000 | Loss: 0.00002755
Iteration 73/1000 | Loss: 0.00002754
Iteration 74/1000 | Loss: 0.00002754
Iteration 75/1000 | Loss: 0.00002754
Iteration 76/1000 | Loss: 0.00002753
Iteration 77/1000 | Loss: 0.00002753
Iteration 78/1000 | Loss: 0.00002752
Iteration 79/1000 | Loss: 0.00002752
Iteration 80/1000 | Loss: 0.00002752
Iteration 81/1000 | Loss: 0.00002751
Iteration 82/1000 | Loss: 0.00002751
Iteration 83/1000 | Loss: 0.00002751
Iteration 84/1000 | Loss: 0.00002750
Iteration 85/1000 | Loss: 0.00002750
Iteration 86/1000 | Loss: 0.00002749
Iteration 87/1000 | Loss: 0.00002749
Iteration 88/1000 | Loss: 0.00002749
Iteration 89/1000 | Loss: 0.00002748
Iteration 90/1000 | Loss: 0.00002748
Iteration 91/1000 | Loss: 0.00002748
Iteration 92/1000 | Loss: 0.00002748
Iteration 93/1000 | Loss: 0.00002747
Iteration 94/1000 | Loss: 0.00002747
Iteration 95/1000 | Loss: 0.00002747
Iteration 96/1000 | Loss: 0.00002747
Iteration 97/1000 | Loss: 0.00002747
Iteration 98/1000 | Loss: 0.00002746
Iteration 99/1000 | Loss: 0.00002746
Iteration 100/1000 | Loss: 0.00002746
Iteration 101/1000 | Loss: 0.00002746
Iteration 102/1000 | Loss: 0.00002746
Iteration 103/1000 | Loss: 0.00002745
Iteration 104/1000 | Loss: 0.00002745
Iteration 105/1000 | Loss: 0.00002745
Iteration 106/1000 | Loss: 0.00002745
Iteration 107/1000 | Loss: 0.00002744
Iteration 108/1000 | Loss: 0.00002744
Iteration 109/1000 | Loss: 0.00002744
Iteration 110/1000 | Loss: 0.00002744
Iteration 111/1000 | Loss: 0.00002743
Iteration 112/1000 | Loss: 0.00002743
Iteration 113/1000 | Loss: 0.00002743
Iteration 114/1000 | Loss: 0.00002743
Iteration 115/1000 | Loss: 0.00002742
Iteration 116/1000 | Loss: 0.00002742
Iteration 117/1000 | Loss: 0.00002742
Iteration 118/1000 | Loss: 0.00002742
Iteration 119/1000 | Loss: 0.00002742
Iteration 120/1000 | Loss: 0.00002742
Iteration 121/1000 | Loss: 0.00002742
Iteration 122/1000 | Loss: 0.00002742
Iteration 123/1000 | Loss: 0.00002742
Iteration 124/1000 | Loss: 0.00002742
Iteration 125/1000 | Loss: 0.00002741
Iteration 126/1000 | Loss: 0.00002741
Iteration 127/1000 | Loss: 0.00002741
Iteration 128/1000 | Loss: 0.00002741
Iteration 129/1000 | Loss: 0.00002740
Iteration 130/1000 | Loss: 0.00002740
Iteration 131/1000 | Loss: 0.00002740
Iteration 132/1000 | Loss: 0.00002740
Iteration 133/1000 | Loss: 0.00002740
Iteration 134/1000 | Loss: 0.00002740
Iteration 135/1000 | Loss: 0.00002740
Iteration 136/1000 | Loss: 0.00002739
Iteration 137/1000 | Loss: 0.00002739
Iteration 138/1000 | Loss: 0.00002739
Iteration 139/1000 | Loss: 0.00002739
Iteration 140/1000 | Loss: 0.00002739
Iteration 141/1000 | Loss: 0.00002739
Iteration 142/1000 | Loss: 0.00002739
Iteration 143/1000 | Loss: 0.00002739
Iteration 144/1000 | Loss: 0.00002739
Iteration 145/1000 | Loss: 0.00002739
Iteration 146/1000 | Loss: 0.00002739
Iteration 147/1000 | Loss: 0.00002739
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 147. Stopping optimization.
Last 5 losses: [2.738829789450392e-05, 2.738829789450392e-05, 2.738829789450392e-05, 2.738829789450392e-05, 2.738829789450392e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.738829789450392e-05

Optimization complete. Final v2v error: 4.261359691619873 mm

Highest mean error: 5.3818159103393555 mm for frame 16

Lowest mean error: 3.643490791320801 mm for frame 159

Saving results

Total time: 52.76438593864441
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_39_us_1277/0006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_39_us_1277/0006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_39_us_1277/0006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00441000
Iteration 2/25 | Loss: 0.00143723
Iteration 3/25 | Loss: 0.00128187
Iteration 4/25 | Loss: 0.00127233
Iteration 5/25 | Loss: 0.00126944
Iteration 6/25 | Loss: 0.00126895
Iteration 7/25 | Loss: 0.00126895
Iteration 8/25 | Loss: 0.00126895
Iteration 9/25 | Loss: 0.00126895
Iteration 10/25 | Loss: 0.00126895
Iteration 11/25 | Loss: 0.00126895
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012689484283328056, 0.0012689484283328056, 0.0012689484283328056, 0.0012689484283328056, 0.0012689484283328056]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012689484283328056

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.07922018
Iteration 2/25 | Loss: 0.00078117
Iteration 3/25 | Loss: 0.00078117
Iteration 4/25 | Loss: 0.00078117
Iteration 5/25 | Loss: 0.00078117
Iteration 6/25 | Loss: 0.00078117
Iteration 7/25 | Loss: 0.00078117
Iteration 8/25 | Loss: 0.00078117
Iteration 9/25 | Loss: 0.00078117
Iteration 10/25 | Loss: 0.00078117
Iteration 11/25 | Loss: 0.00078117
Iteration 12/25 | Loss: 0.00078117
Iteration 13/25 | Loss: 0.00078117
Iteration 14/25 | Loss: 0.00078117
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0007811670075170696, 0.0007811670075170696, 0.0007811670075170696, 0.0007811670075170696, 0.0007811670075170696]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007811670075170696

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00078117
Iteration 2/1000 | Loss: 0.00005690
Iteration 3/1000 | Loss: 0.00003283
Iteration 4/1000 | Loss: 0.00002577
Iteration 5/1000 | Loss: 0.00002361
Iteration 6/1000 | Loss: 0.00002285
Iteration 7/1000 | Loss: 0.00002226
Iteration 8/1000 | Loss: 0.00002191
Iteration 9/1000 | Loss: 0.00002156
Iteration 10/1000 | Loss: 0.00002142
Iteration 11/1000 | Loss: 0.00002118
Iteration 12/1000 | Loss: 0.00002117
Iteration 13/1000 | Loss: 0.00002098
Iteration 14/1000 | Loss: 0.00002095
Iteration 15/1000 | Loss: 0.00002094
Iteration 16/1000 | Loss: 0.00002094
Iteration 17/1000 | Loss: 0.00002094
Iteration 18/1000 | Loss: 0.00002094
Iteration 19/1000 | Loss: 0.00002094
Iteration 20/1000 | Loss: 0.00002093
Iteration 21/1000 | Loss: 0.00002093
Iteration 22/1000 | Loss: 0.00002093
Iteration 23/1000 | Loss: 0.00002092
Iteration 24/1000 | Loss: 0.00002082
Iteration 25/1000 | Loss: 0.00002072
Iteration 26/1000 | Loss: 0.00002070
Iteration 27/1000 | Loss: 0.00002069
Iteration 28/1000 | Loss: 0.00002063
Iteration 29/1000 | Loss: 0.00002058
Iteration 30/1000 | Loss: 0.00002058
Iteration 31/1000 | Loss: 0.00002058
Iteration 32/1000 | Loss: 0.00002058
Iteration 33/1000 | Loss: 0.00002058
Iteration 34/1000 | Loss: 0.00002058
Iteration 35/1000 | Loss: 0.00002058
Iteration 36/1000 | Loss: 0.00002058
Iteration 37/1000 | Loss: 0.00002057
Iteration 38/1000 | Loss: 0.00002057
Iteration 39/1000 | Loss: 0.00002057
Iteration 40/1000 | Loss: 0.00002057
Iteration 41/1000 | Loss: 0.00002057
Iteration 42/1000 | Loss: 0.00002057
Iteration 43/1000 | Loss: 0.00002057
Iteration 44/1000 | Loss: 0.00002057
Iteration 45/1000 | Loss: 0.00002056
Iteration 46/1000 | Loss: 0.00002056
Iteration 47/1000 | Loss: 0.00002056
Iteration 48/1000 | Loss: 0.00002055
Iteration 49/1000 | Loss: 0.00002054
Iteration 50/1000 | Loss: 0.00002053
Iteration 51/1000 | Loss: 0.00002053
Iteration 52/1000 | Loss: 0.00002053
Iteration 53/1000 | Loss: 0.00002053
Iteration 54/1000 | Loss: 0.00002053
Iteration 55/1000 | Loss: 0.00002052
Iteration 56/1000 | Loss: 0.00002052
Iteration 57/1000 | Loss: 0.00002052
Iteration 58/1000 | Loss: 0.00002052
Iteration 59/1000 | Loss: 0.00002051
Iteration 60/1000 | Loss: 0.00002051
Iteration 61/1000 | Loss: 0.00002051
Iteration 62/1000 | Loss: 0.00002051
Iteration 63/1000 | Loss: 0.00002051
Iteration 64/1000 | Loss: 0.00002051
Iteration 65/1000 | Loss: 0.00002051
Iteration 66/1000 | Loss: 0.00002051
Iteration 67/1000 | Loss: 0.00002051
Iteration 68/1000 | Loss: 0.00002050
Iteration 69/1000 | Loss: 0.00002050
Iteration 70/1000 | Loss: 0.00002050
Iteration 71/1000 | Loss: 0.00002050
Iteration 72/1000 | Loss: 0.00002049
Iteration 73/1000 | Loss: 0.00002049
Iteration 74/1000 | Loss: 0.00002049
Iteration 75/1000 | Loss: 0.00002048
Iteration 76/1000 | Loss: 0.00002048
Iteration 77/1000 | Loss: 0.00002048
Iteration 78/1000 | Loss: 0.00002048
Iteration 79/1000 | Loss: 0.00002047
Iteration 80/1000 | Loss: 0.00002047
Iteration 81/1000 | Loss: 0.00002047
Iteration 82/1000 | Loss: 0.00002047
Iteration 83/1000 | Loss: 0.00002047
Iteration 84/1000 | Loss: 0.00002047
Iteration 85/1000 | Loss: 0.00002047
Iteration 86/1000 | Loss: 0.00002046
Iteration 87/1000 | Loss: 0.00002046
Iteration 88/1000 | Loss: 0.00002046
Iteration 89/1000 | Loss: 0.00002046
Iteration 90/1000 | Loss: 0.00002046
Iteration 91/1000 | Loss: 0.00002046
Iteration 92/1000 | Loss: 0.00002046
Iteration 93/1000 | Loss: 0.00002046
Iteration 94/1000 | Loss: 0.00002046
Iteration 95/1000 | Loss: 0.00002046
Iteration 96/1000 | Loss: 0.00002046
Iteration 97/1000 | Loss: 0.00002046
Iteration 98/1000 | Loss: 0.00002046
Iteration 99/1000 | Loss: 0.00002046
Iteration 100/1000 | Loss: 0.00002046
Iteration 101/1000 | Loss: 0.00002046
Iteration 102/1000 | Loss: 0.00002046
Iteration 103/1000 | Loss: 0.00002046
Iteration 104/1000 | Loss: 0.00002046
Iteration 105/1000 | Loss: 0.00002045
Iteration 106/1000 | Loss: 0.00002045
Iteration 107/1000 | Loss: 0.00002045
Iteration 108/1000 | Loss: 0.00002045
Iteration 109/1000 | Loss: 0.00002045
Iteration 110/1000 | Loss: 0.00002045
Iteration 111/1000 | Loss: 0.00002045
Iteration 112/1000 | Loss: 0.00002045
Iteration 113/1000 | Loss: 0.00002045
Iteration 114/1000 | Loss: 0.00002045
Iteration 115/1000 | Loss: 0.00002045
Iteration 116/1000 | Loss: 0.00002045
Iteration 117/1000 | Loss: 0.00002045
Iteration 118/1000 | Loss: 0.00002045
Iteration 119/1000 | Loss: 0.00002045
Iteration 120/1000 | Loss: 0.00002045
Iteration 121/1000 | Loss: 0.00002045
Iteration 122/1000 | Loss: 0.00002044
Iteration 123/1000 | Loss: 0.00002044
Iteration 124/1000 | Loss: 0.00002044
Iteration 125/1000 | Loss: 0.00002044
Iteration 126/1000 | Loss: 0.00002044
Iteration 127/1000 | Loss: 0.00002044
Iteration 128/1000 | Loss: 0.00002044
Iteration 129/1000 | Loss: 0.00002044
Iteration 130/1000 | Loss: 0.00002044
Iteration 131/1000 | Loss: 0.00002044
Iteration 132/1000 | Loss: 0.00002044
Iteration 133/1000 | Loss: 0.00002044
Iteration 134/1000 | Loss: 0.00002044
Iteration 135/1000 | Loss: 0.00002044
Iteration 136/1000 | Loss: 0.00002044
Iteration 137/1000 | Loss: 0.00002044
Iteration 138/1000 | Loss: 0.00002044
Iteration 139/1000 | Loss: 0.00002044
Iteration 140/1000 | Loss: 0.00002044
Iteration 141/1000 | Loss: 0.00002044
Iteration 142/1000 | Loss: 0.00002044
Iteration 143/1000 | Loss: 0.00002044
Iteration 144/1000 | Loss: 0.00002044
Iteration 145/1000 | Loss: 0.00002044
Iteration 146/1000 | Loss: 0.00002044
Iteration 147/1000 | Loss: 0.00002044
Iteration 148/1000 | Loss: 0.00002044
Iteration 149/1000 | Loss: 0.00002044
Iteration 150/1000 | Loss: 0.00002044
Iteration 151/1000 | Loss: 0.00002044
Iteration 152/1000 | Loss: 0.00002044
Iteration 153/1000 | Loss: 0.00002044
Iteration 154/1000 | Loss: 0.00002044
Iteration 155/1000 | Loss: 0.00002044
Iteration 156/1000 | Loss: 0.00002044
Iteration 157/1000 | Loss: 0.00002044
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 157. Stopping optimization.
Last 5 losses: [2.0436236809473485e-05, 2.0436236809473485e-05, 2.0436236809473485e-05, 2.0436236809473485e-05, 2.0436236809473485e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0436236809473485e-05

Optimization complete. Final v2v error: 3.9641358852386475 mm

Highest mean error: 4.217522621154785 mm for frame 97

Lowest mean error: 3.683750867843628 mm for frame 16

Saving results

Total time: 34.4114887714386
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_39_us_1277/0023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_39_us_1277/0023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_39_us_1277/0023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00878116
Iteration 2/25 | Loss: 0.00175387
Iteration 3/25 | Loss: 0.00155951
Iteration 4/25 | Loss: 0.00151353
Iteration 5/25 | Loss: 0.00151666
Iteration 6/25 | Loss: 0.00149333
Iteration 7/25 | Loss: 0.00149394
Iteration 8/25 | Loss: 0.00146080
Iteration 9/25 | Loss: 0.00144952
Iteration 10/25 | Loss: 0.00143871
Iteration 11/25 | Loss: 0.00143339
Iteration 12/25 | Loss: 0.00142894
Iteration 13/25 | Loss: 0.00142668
Iteration 14/25 | Loss: 0.00142792
Iteration 15/25 | Loss: 0.00142911
Iteration 16/25 | Loss: 0.00141836
Iteration 17/25 | Loss: 0.00140733
Iteration 18/25 | Loss: 0.00140727
Iteration 19/25 | Loss: 0.00139426
Iteration 20/25 | Loss: 0.00139307
Iteration 21/25 | Loss: 0.00138558
Iteration 22/25 | Loss: 0.00138387
Iteration 23/25 | Loss: 0.00138430
Iteration 24/25 | Loss: 0.00138423
Iteration 25/25 | Loss: 0.00138348

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.23196173
Iteration 2/25 | Loss: 0.00202798
Iteration 3/25 | Loss: 0.00202796
Iteration 4/25 | Loss: 0.00202796
Iteration 5/25 | Loss: 0.00202796
Iteration 6/25 | Loss: 0.00202796
Iteration 7/25 | Loss: 0.00202796
Iteration 8/25 | Loss: 0.00202796
Iteration 9/25 | Loss: 0.00202796
Iteration 10/25 | Loss: 0.00202796
Iteration 11/25 | Loss: 0.00202796
Iteration 12/25 | Loss: 0.00202796
Iteration 13/25 | Loss: 0.00202796
Iteration 14/25 | Loss: 0.00202796
Iteration 15/25 | Loss: 0.00202796
Iteration 16/25 | Loss: 0.00202796
Iteration 17/25 | Loss: 0.00202796
Iteration 18/25 | Loss: 0.00202796
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0020279577001929283, 0.0020279577001929283, 0.0020279577001929283, 0.0020279577001929283, 0.0020279577001929283]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0020279577001929283

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00202796
Iteration 2/1000 | Loss: 0.00023018
Iteration 3/1000 | Loss: 0.00019155
Iteration 4/1000 | Loss: 0.00013969
Iteration 5/1000 | Loss: 0.00025080
Iteration 6/1000 | Loss: 0.00011746
Iteration 7/1000 | Loss: 0.00056492
Iteration 8/1000 | Loss: 0.00014361
Iteration 9/1000 | Loss: 0.00048112
Iteration 10/1000 | Loss: 0.00010543
Iteration 11/1000 | Loss: 0.00072828
Iteration 12/1000 | Loss: 0.00013242
Iteration 13/1000 | Loss: 0.00057945
Iteration 14/1000 | Loss: 0.00011162
Iteration 15/1000 | Loss: 0.00041735
Iteration 16/1000 | Loss: 0.00010633
Iteration 17/1000 | Loss: 0.00013437
Iteration 18/1000 | Loss: 0.00023650
Iteration 19/1000 | Loss: 0.00009182
Iteration 20/1000 | Loss: 0.00007906
Iteration 21/1000 | Loss: 0.00048694
Iteration 22/1000 | Loss: 0.00010161
Iteration 23/1000 | Loss: 0.00007694
Iteration 24/1000 | Loss: 0.00135484
Iteration 25/1000 | Loss: 0.00280272
Iteration 26/1000 | Loss: 0.00123781
Iteration 27/1000 | Loss: 0.00117984
Iteration 28/1000 | Loss: 0.00170869
Iteration 29/1000 | Loss: 0.00051640
Iteration 30/1000 | Loss: 0.00008639
Iteration 31/1000 | Loss: 0.00042376
Iteration 32/1000 | Loss: 0.00031704
Iteration 33/1000 | Loss: 0.00049011
Iteration 34/1000 | Loss: 0.00043011
Iteration 35/1000 | Loss: 0.00018061
Iteration 36/1000 | Loss: 0.00036929
Iteration 37/1000 | Loss: 0.00026618
Iteration 38/1000 | Loss: 0.00021316
Iteration 39/1000 | Loss: 0.00041457
Iteration 40/1000 | Loss: 0.00076429
Iteration 41/1000 | Loss: 0.00034391
Iteration 42/1000 | Loss: 0.00006213
Iteration 43/1000 | Loss: 0.00004492
Iteration 44/1000 | Loss: 0.00003997
Iteration 45/1000 | Loss: 0.00003755
Iteration 46/1000 | Loss: 0.00003622
Iteration 47/1000 | Loss: 0.00003496
Iteration 48/1000 | Loss: 0.00003412
Iteration 49/1000 | Loss: 0.00003360
Iteration 50/1000 | Loss: 0.00003315
Iteration 51/1000 | Loss: 0.00003278
Iteration 52/1000 | Loss: 0.00003247
Iteration 53/1000 | Loss: 0.00003226
Iteration 54/1000 | Loss: 0.00003215
Iteration 55/1000 | Loss: 0.00003212
Iteration 56/1000 | Loss: 0.00003209
Iteration 57/1000 | Loss: 0.00003206
Iteration 58/1000 | Loss: 0.00003200
Iteration 59/1000 | Loss: 0.00003193
Iteration 60/1000 | Loss: 0.00003192
Iteration 61/1000 | Loss: 0.00003189
Iteration 62/1000 | Loss: 0.00003189
Iteration 63/1000 | Loss: 0.00003185
Iteration 64/1000 | Loss: 0.00003182
Iteration 65/1000 | Loss: 0.00003174
Iteration 66/1000 | Loss: 0.00003169
Iteration 67/1000 | Loss: 0.00003169
Iteration 68/1000 | Loss: 0.00003168
Iteration 69/1000 | Loss: 0.00003168
Iteration 70/1000 | Loss: 0.00003168
Iteration 71/1000 | Loss: 0.00083367
Iteration 72/1000 | Loss: 0.00072885
Iteration 73/1000 | Loss: 0.00005250
Iteration 74/1000 | Loss: 0.00003917
Iteration 75/1000 | Loss: 0.00003548
Iteration 76/1000 | Loss: 0.00003246
Iteration 77/1000 | Loss: 0.00002939
Iteration 78/1000 | Loss: 0.00002681
Iteration 79/1000 | Loss: 0.00002584
Iteration 80/1000 | Loss: 0.00002545
Iteration 81/1000 | Loss: 0.00002515
Iteration 82/1000 | Loss: 0.00002489
Iteration 83/1000 | Loss: 0.00002469
Iteration 84/1000 | Loss: 0.00002467
Iteration 85/1000 | Loss: 0.00002463
Iteration 86/1000 | Loss: 0.00002460
Iteration 87/1000 | Loss: 0.00002455
Iteration 88/1000 | Loss: 0.00002447
Iteration 89/1000 | Loss: 0.00002446
Iteration 90/1000 | Loss: 0.00002446
Iteration 91/1000 | Loss: 0.00002446
Iteration 92/1000 | Loss: 0.00002446
Iteration 93/1000 | Loss: 0.00002446
Iteration 94/1000 | Loss: 0.00002446
Iteration 95/1000 | Loss: 0.00002446
Iteration 96/1000 | Loss: 0.00002446
Iteration 97/1000 | Loss: 0.00002446
Iteration 98/1000 | Loss: 0.00002445
Iteration 99/1000 | Loss: 0.00002445
Iteration 100/1000 | Loss: 0.00002445
Iteration 101/1000 | Loss: 0.00002445
Iteration 102/1000 | Loss: 0.00002445
Iteration 103/1000 | Loss: 0.00002445
Iteration 104/1000 | Loss: 0.00002444
Iteration 105/1000 | Loss: 0.00002443
Iteration 106/1000 | Loss: 0.00002443
Iteration 107/1000 | Loss: 0.00002443
Iteration 108/1000 | Loss: 0.00002443
Iteration 109/1000 | Loss: 0.00002442
Iteration 110/1000 | Loss: 0.00002442
Iteration 111/1000 | Loss: 0.00002441
Iteration 112/1000 | Loss: 0.00002441
Iteration 113/1000 | Loss: 0.00002441
Iteration 114/1000 | Loss: 0.00002440
Iteration 115/1000 | Loss: 0.00002440
Iteration 116/1000 | Loss: 0.00002440
Iteration 117/1000 | Loss: 0.00002440
Iteration 118/1000 | Loss: 0.00002440
Iteration 119/1000 | Loss: 0.00002440
Iteration 120/1000 | Loss: 0.00002439
Iteration 121/1000 | Loss: 0.00002439
Iteration 122/1000 | Loss: 0.00002439
Iteration 123/1000 | Loss: 0.00002439
Iteration 124/1000 | Loss: 0.00002439
Iteration 125/1000 | Loss: 0.00002439
Iteration 126/1000 | Loss: 0.00002439
Iteration 127/1000 | Loss: 0.00002438
Iteration 128/1000 | Loss: 0.00002438
Iteration 129/1000 | Loss: 0.00002438
Iteration 130/1000 | Loss: 0.00002438
Iteration 131/1000 | Loss: 0.00002438
Iteration 132/1000 | Loss: 0.00002437
Iteration 133/1000 | Loss: 0.00002437
Iteration 134/1000 | Loss: 0.00002437
Iteration 135/1000 | Loss: 0.00002437
Iteration 136/1000 | Loss: 0.00002437
Iteration 137/1000 | Loss: 0.00002436
Iteration 138/1000 | Loss: 0.00002436
Iteration 139/1000 | Loss: 0.00002436
Iteration 140/1000 | Loss: 0.00002436
Iteration 141/1000 | Loss: 0.00002436
Iteration 142/1000 | Loss: 0.00002436
Iteration 143/1000 | Loss: 0.00002435
Iteration 144/1000 | Loss: 0.00002435
Iteration 145/1000 | Loss: 0.00002435
Iteration 146/1000 | Loss: 0.00002435
Iteration 147/1000 | Loss: 0.00002435
Iteration 148/1000 | Loss: 0.00002435
Iteration 149/1000 | Loss: 0.00002435
Iteration 150/1000 | Loss: 0.00002435
Iteration 151/1000 | Loss: 0.00002435
Iteration 152/1000 | Loss: 0.00002434
Iteration 153/1000 | Loss: 0.00002434
Iteration 154/1000 | Loss: 0.00002434
Iteration 155/1000 | Loss: 0.00002434
Iteration 156/1000 | Loss: 0.00002434
Iteration 157/1000 | Loss: 0.00002433
Iteration 158/1000 | Loss: 0.00002433
Iteration 159/1000 | Loss: 0.00002433
Iteration 160/1000 | Loss: 0.00002433
Iteration 161/1000 | Loss: 0.00002433
Iteration 162/1000 | Loss: 0.00002432
Iteration 163/1000 | Loss: 0.00002432
Iteration 164/1000 | Loss: 0.00002432
Iteration 165/1000 | Loss: 0.00002432
Iteration 166/1000 | Loss: 0.00002431
Iteration 167/1000 | Loss: 0.00002431
Iteration 168/1000 | Loss: 0.00002431
Iteration 169/1000 | Loss: 0.00002431
Iteration 170/1000 | Loss: 0.00002431
Iteration 171/1000 | Loss: 0.00002430
Iteration 172/1000 | Loss: 0.00002430
Iteration 173/1000 | Loss: 0.00002430
Iteration 174/1000 | Loss: 0.00002430
Iteration 175/1000 | Loss: 0.00002430
Iteration 176/1000 | Loss: 0.00002430
Iteration 177/1000 | Loss: 0.00002430
Iteration 178/1000 | Loss: 0.00002430
Iteration 179/1000 | Loss: 0.00002430
Iteration 180/1000 | Loss: 0.00002430
Iteration 181/1000 | Loss: 0.00002430
Iteration 182/1000 | Loss: 0.00002430
Iteration 183/1000 | Loss: 0.00002429
Iteration 184/1000 | Loss: 0.00002429
Iteration 185/1000 | Loss: 0.00002429
Iteration 186/1000 | Loss: 0.00002429
Iteration 187/1000 | Loss: 0.00002429
Iteration 188/1000 | Loss: 0.00002429
Iteration 189/1000 | Loss: 0.00002429
Iteration 190/1000 | Loss: 0.00002429
Iteration 191/1000 | Loss: 0.00002429
Iteration 192/1000 | Loss: 0.00002429
Iteration 193/1000 | Loss: 0.00002429
Iteration 194/1000 | Loss: 0.00002429
Iteration 195/1000 | Loss: 0.00002429
Iteration 196/1000 | Loss: 0.00002429
Iteration 197/1000 | Loss: 0.00002429
Iteration 198/1000 | Loss: 0.00002429
Iteration 199/1000 | Loss: 0.00002429
Iteration 200/1000 | Loss: 0.00002429
Iteration 201/1000 | Loss: 0.00002429
Iteration 202/1000 | Loss: 0.00002429
Iteration 203/1000 | Loss: 0.00002429
Iteration 204/1000 | Loss: 0.00002429
Iteration 205/1000 | Loss: 0.00002429
Iteration 206/1000 | Loss: 0.00002429
Iteration 207/1000 | Loss: 0.00002429
Iteration 208/1000 | Loss: 0.00002429
Iteration 209/1000 | Loss: 0.00002429
Iteration 210/1000 | Loss: 0.00002429
Iteration 211/1000 | Loss: 0.00002429
Iteration 212/1000 | Loss: 0.00002429
Iteration 213/1000 | Loss: 0.00002429
Iteration 214/1000 | Loss: 0.00002429
Iteration 215/1000 | Loss: 0.00002429
Iteration 216/1000 | Loss: 0.00002429
Iteration 217/1000 | Loss: 0.00002429
Iteration 218/1000 | Loss: 0.00002429
Iteration 219/1000 | Loss: 0.00002429
Iteration 220/1000 | Loss: 0.00002429
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 220. Stopping optimization.
Last 5 losses: [2.4294400645885617e-05, 2.4294400645885617e-05, 2.4294400645885617e-05, 2.4294400645885617e-05, 2.4294400645885617e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4294400645885617e-05

Optimization complete. Final v2v error: 4.1615705490112305 mm

Highest mean error: 4.672966480255127 mm for frame 44

Lowest mean error: 3.6719720363616943 mm for frame 105

Saving results

Total time: 151.8820662498474
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_39_us_1277/0024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_39_us_1277/0024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_39_us_1277/0024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00826894
Iteration 2/25 | Loss: 0.00197207
Iteration 3/25 | Loss: 0.00135813
Iteration 4/25 | Loss: 0.00126791
Iteration 5/25 | Loss: 0.00127151
Iteration 6/25 | Loss: 0.00124694
Iteration 7/25 | Loss: 0.00124634
Iteration 8/25 | Loss: 0.00124565
Iteration 9/25 | Loss: 0.00124090
Iteration 10/25 | Loss: 0.00124242
Iteration 11/25 | Loss: 0.00124048
Iteration 12/25 | Loss: 0.00123736
Iteration 13/25 | Loss: 0.00123620
Iteration 14/25 | Loss: 0.00123586
Iteration 15/25 | Loss: 0.00123571
Iteration 16/25 | Loss: 0.00123570
Iteration 17/25 | Loss: 0.00123570
Iteration 18/25 | Loss: 0.00123570
Iteration 19/25 | Loss: 0.00123570
Iteration 20/25 | Loss: 0.00123570
Iteration 21/25 | Loss: 0.00123570
Iteration 22/25 | Loss: 0.00123570
Iteration 23/25 | Loss: 0.00123569
Iteration 24/25 | Loss: 0.00123569
Iteration 25/25 | Loss: 0.00123569

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.43763781
Iteration 2/25 | Loss: 0.00079828
Iteration 3/25 | Loss: 0.00068883
Iteration 4/25 | Loss: 0.00068883
Iteration 5/25 | Loss: 0.00068882
Iteration 6/25 | Loss: 0.00068882
Iteration 7/25 | Loss: 0.00068882
Iteration 8/25 | Loss: 0.00068882
Iteration 9/25 | Loss: 0.00068882
Iteration 10/25 | Loss: 0.00068882
Iteration 11/25 | Loss: 0.00068882
Iteration 12/25 | Loss: 0.00068882
Iteration 13/25 | Loss: 0.00068882
Iteration 14/25 | Loss: 0.00068882
Iteration 15/25 | Loss: 0.00068882
Iteration 16/25 | Loss: 0.00068882
Iteration 17/25 | Loss: 0.00068882
Iteration 18/25 | Loss: 0.00068882
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0006888220086693764, 0.0006888220086693764, 0.0006888220086693764, 0.0006888220086693764, 0.0006888220086693764]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006888220086693764

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00068882
Iteration 2/1000 | Loss: 0.00020889
Iteration 3/1000 | Loss: 0.00020803
Iteration 4/1000 | Loss: 0.00007866
Iteration 5/1000 | Loss: 0.00004204
Iteration 6/1000 | Loss: 0.00003564
Iteration 7/1000 | Loss: 0.00005212
Iteration 8/1000 | Loss: 0.00013597
Iteration 9/1000 | Loss: 0.00003018
Iteration 10/1000 | Loss: 0.00002812
Iteration 11/1000 | Loss: 0.00015511
Iteration 12/1000 | Loss: 0.00006304
Iteration 13/1000 | Loss: 0.00002678
Iteration 14/1000 | Loss: 0.00002618
Iteration 15/1000 | Loss: 0.00016411
Iteration 16/1000 | Loss: 0.00002849
Iteration 17/1000 | Loss: 0.00002658
Iteration 18/1000 | Loss: 0.00002543
Iteration 19/1000 | Loss: 0.00002607
Iteration 20/1000 | Loss: 0.00002504
Iteration 21/1000 | Loss: 0.00002497
Iteration 22/1000 | Loss: 0.00002476
Iteration 23/1000 | Loss: 0.00002453
Iteration 24/1000 | Loss: 0.00002452
Iteration 25/1000 | Loss: 0.00002449
Iteration 26/1000 | Loss: 0.00002447
Iteration 27/1000 | Loss: 0.00002444
Iteration 28/1000 | Loss: 0.00002444
Iteration 29/1000 | Loss: 0.00002443
Iteration 30/1000 | Loss: 0.00002443
Iteration 31/1000 | Loss: 0.00002443
Iteration 32/1000 | Loss: 0.00002443
Iteration 33/1000 | Loss: 0.00002443
Iteration 34/1000 | Loss: 0.00002443
Iteration 35/1000 | Loss: 0.00002442
Iteration 36/1000 | Loss: 0.00002442
Iteration 37/1000 | Loss: 0.00002442
Iteration 38/1000 | Loss: 0.00002442
Iteration 39/1000 | Loss: 0.00002442
Iteration 40/1000 | Loss: 0.00002442
Iteration 41/1000 | Loss: 0.00002440
Iteration 42/1000 | Loss: 0.00002440
Iteration 43/1000 | Loss: 0.00002439
Iteration 44/1000 | Loss: 0.00002438
Iteration 45/1000 | Loss: 0.00002442
Iteration 46/1000 | Loss: 0.00002441
Iteration 47/1000 | Loss: 0.00002440
Iteration 48/1000 | Loss: 0.00002438
Iteration 49/1000 | Loss: 0.00002437
Iteration 50/1000 | Loss: 0.00002430
Iteration 51/1000 | Loss: 0.00002429
Iteration 52/1000 | Loss: 0.00002429
Iteration 53/1000 | Loss: 0.00002428
Iteration 54/1000 | Loss: 0.00002427
Iteration 55/1000 | Loss: 0.00002427
Iteration 56/1000 | Loss: 0.00002427
Iteration 57/1000 | Loss: 0.00002427
Iteration 58/1000 | Loss: 0.00002427
Iteration 59/1000 | Loss: 0.00002427
Iteration 60/1000 | Loss: 0.00002427
Iteration 61/1000 | Loss: 0.00002427
Iteration 62/1000 | Loss: 0.00002427
Iteration 63/1000 | Loss: 0.00002426
Iteration 64/1000 | Loss: 0.00002426
Iteration 65/1000 | Loss: 0.00002426
Iteration 66/1000 | Loss: 0.00002426
Iteration 67/1000 | Loss: 0.00002425
Iteration 68/1000 | Loss: 0.00002425
Iteration 69/1000 | Loss: 0.00002425
Iteration 70/1000 | Loss: 0.00002425
Iteration 71/1000 | Loss: 0.00002424
Iteration 72/1000 | Loss: 0.00002424
Iteration 73/1000 | Loss: 0.00002424
Iteration 74/1000 | Loss: 0.00002423
Iteration 75/1000 | Loss: 0.00002423
Iteration 76/1000 | Loss: 0.00002423
Iteration 77/1000 | Loss: 0.00002423
Iteration 78/1000 | Loss: 0.00002422
Iteration 79/1000 | Loss: 0.00002422
Iteration 80/1000 | Loss: 0.00002423
Iteration 81/1000 | Loss: 0.00002422
Iteration 82/1000 | Loss: 0.00002421
Iteration 83/1000 | Loss: 0.00002420
Iteration 84/1000 | Loss: 0.00002419
Iteration 85/1000 | Loss: 0.00002419
Iteration 86/1000 | Loss: 0.00002419
Iteration 87/1000 | Loss: 0.00002419
Iteration 88/1000 | Loss: 0.00002419
Iteration 89/1000 | Loss: 0.00002419
Iteration 90/1000 | Loss: 0.00002419
Iteration 91/1000 | Loss: 0.00002419
Iteration 92/1000 | Loss: 0.00002419
Iteration 93/1000 | Loss: 0.00002419
Iteration 94/1000 | Loss: 0.00002419
Iteration 95/1000 | Loss: 0.00002419
Iteration 96/1000 | Loss: 0.00002419
Iteration 97/1000 | Loss: 0.00002419
Iteration 98/1000 | Loss: 0.00002419
Iteration 99/1000 | Loss: 0.00002419
Iteration 100/1000 | Loss: 0.00002419
Iteration 101/1000 | Loss: 0.00002419
Iteration 102/1000 | Loss: 0.00002419
Iteration 103/1000 | Loss: 0.00002419
Iteration 104/1000 | Loss: 0.00002418
Iteration 105/1000 | Loss: 0.00002418
Iteration 106/1000 | Loss: 0.00002418
Iteration 107/1000 | Loss: 0.00002418
Iteration 108/1000 | Loss: 0.00002418
Iteration 109/1000 | Loss: 0.00002418
Iteration 110/1000 | Loss: 0.00002418
Iteration 111/1000 | Loss: 0.00002418
Iteration 112/1000 | Loss: 0.00002417
Iteration 113/1000 | Loss: 0.00002417
Iteration 114/1000 | Loss: 0.00002417
Iteration 115/1000 | Loss: 0.00002416
Iteration 116/1000 | Loss: 0.00002416
Iteration 117/1000 | Loss: 0.00002415
Iteration 118/1000 | Loss: 0.00002418
Iteration 119/1000 | Loss: 0.00002417
Iteration 120/1000 | Loss: 0.00002417
Iteration 121/1000 | Loss: 0.00002417
Iteration 122/1000 | Loss: 0.00002416
Iteration 123/1000 | Loss: 0.00002416
Iteration 124/1000 | Loss: 0.00002416
Iteration 125/1000 | Loss: 0.00002416
Iteration 126/1000 | Loss: 0.00002416
Iteration 127/1000 | Loss: 0.00002416
Iteration 128/1000 | Loss: 0.00002413
Iteration 129/1000 | Loss: 0.00002413
Iteration 130/1000 | Loss: 0.00002413
Iteration 131/1000 | Loss: 0.00002413
Iteration 132/1000 | Loss: 0.00002413
Iteration 133/1000 | Loss: 0.00002413
Iteration 134/1000 | Loss: 0.00002413
Iteration 135/1000 | Loss: 0.00002412
Iteration 136/1000 | Loss: 0.00002412
Iteration 137/1000 | Loss: 0.00002412
Iteration 138/1000 | Loss: 0.00002415
Iteration 139/1000 | Loss: 0.00002415
Iteration 140/1000 | Loss: 0.00002411
Iteration 141/1000 | Loss: 0.00002411
Iteration 142/1000 | Loss: 0.00002411
Iteration 143/1000 | Loss: 0.00002414
Iteration 144/1000 | Loss: 0.00002414
Iteration 145/1000 | Loss: 0.00002414
Iteration 146/1000 | Loss: 0.00002414
Iteration 147/1000 | Loss: 0.00002414
Iteration 148/1000 | Loss: 0.00002414
Iteration 149/1000 | Loss: 0.00002414
Iteration 150/1000 | Loss: 0.00002414
Iteration 151/1000 | Loss: 0.00002414
Iteration 152/1000 | Loss: 0.00002414
Iteration 153/1000 | Loss: 0.00002414
Iteration 154/1000 | Loss: 0.00002414
Iteration 155/1000 | Loss: 0.00002414
Iteration 156/1000 | Loss: 0.00002414
Iteration 157/1000 | Loss: 0.00002414
Iteration 158/1000 | Loss: 0.00002414
Iteration 159/1000 | Loss: 0.00002414
Iteration 160/1000 | Loss: 0.00002414
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 160. Stopping optimization.
Last 5 losses: [2.4143626433215104e-05, 2.4143626433215104e-05, 2.4143626433215104e-05, 2.4143626433215104e-05, 2.4143626433215104e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4143626433215104e-05

Optimization complete. Final v2v error: 3.8037893772125244 mm

Highest mean error: 21.575761795043945 mm for frame 64

Lowest mean error: 3.459181070327759 mm for frame 229

Saving results

Total time: 79.1680178642273
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_39_us_1277/0012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_39_us_1277/0012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_39_us_1277/0012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00857613
Iteration 2/25 | Loss: 0.00130965
Iteration 3/25 | Loss: 0.00122092
Iteration 4/25 | Loss: 0.00120520
Iteration 5/25 | Loss: 0.00120132
Iteration 6/25 | Loss: 0.00120132
Iteration 7/25 | Loss: 0.00120132
Iteration 8/25 | Loss: 0.00120132
Iteration 9/25 | Loss: 0.00120132
Iteration 10/25 | Loss: 0.00120132
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.001201318809762597, 0.001201318809762597, 0.001201318809762597, 0.001201318809762597, 0.001201318809762597]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001201318809762597

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34168530
Iteration 2/25 | Loss: 0.00109852
Iteration 3/25 | Loss: 0.00109852
Iteration 4/25 | Loss: 0.00109852
Iteration 5/25 | Loss: 0.00109852
Iteration 6/25 | Loss: 0.00109852
Iteration 7/25 | Loss: 0.00109852
Iteration 8/25 | Loss: 0.00109852
Iteration 9/25 | Loss: 0.00109852
Iteration 10/25 | Loss: 0.00109852
Iteration 11/25 | Loss: 0.00109852
Iteration 12/25 | Loss: 0.00109852
Iteration 13/25 | Loss: 0.00109852
Iteration 14/25 | Loss: 0.00109852
Iteration 15/25 | Loss: 0.00109852
Iteration 16/25 | Loss: 0.00109852
Iteration 17/25 | Loss: 0.00109852
Iteration 18/25 | Loss: 0.00109852
Iteration 19/25 | Loss: 0.00109852
Iteration 20/25 | Loss: 0.00109852
Iteration 21/25 | Loss: 0.00109852
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.001098521868698299, 0.001098521868698299, 0.001098521868698299, 0.001098521868698299, 0.001098521868698299]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001098521868698299

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00109852
Iteration 2/1000 | Loss: 0.00004982
Iteration 3/1000 | Loss: 0.00002919
Iteration 4/1000 | Loss: 0.00002364
Iteration 5/1000 | Loss: 0.00002202
Iteration 6/1000 | Loss: 0.00002097
Iteration 7/1000 | Loss: 0.00002033
Iteration 8/1000 | Loss: 0.00001986
Iteration 9/1000 | Loss: 0.00001960
Iteration 10/1000 | Loss: 0.00001939
Iteration 11/1000 | Loss: 0.00001931
Iteration 12/1000 | Loss: 0.00001931
Iteration 13/1000 | Loss: 0.00001930
Iteration 14/1000 | Loss: 0.00001929
Iteration 15/1000 | Loss: 0.00001929
Iteration 16/1000 | Loss: 0.00001921
Iteration 17/1000 | Loss: 0.00001914
Iteration 18/1000 | Loss: 0.00001908
Iteration 19/1000 | Loss: 0.00001903
Iteration 20/1000 | Loss: 0.00001902
Iteration 21/1000 | Loss: 0.00001901
Iteration 22/1000 | Loss: 0.00001901
Iteration 23/1000 | Loss: 0.00001899
Iteration 24/1000 | Loss: 0.00001897
Iteration 25/1000 | Loss: 0.00001896
Iteration 26/1000 | Loss: 0.00001895
Iteration 27/1000 | Loss: 0.00001895
Iteration 28/1000 | Loss: 0.00001895
Iteration 29/1000 | Loss: 0.00001895
Iteration 30/1000 | Loss: 0.00001894
Iteration 31/1000 | Loss: 0.00001894
Iteration 32/1000 | Loss: 0.00001888
Iteration 33/1000 | Loss: 0.00001888
Iteration 34/1000 | Loss: 0.00001886
Iteration 35/1000 | Loss: 0.00001885
Iteration 36/1000 | Loss: 0.00001884
Iteration 37/1000 | Loss: 0.00001883
Iteration 38/1000 | Loss: 0.00001882
Iteration 39/1000 | Loss: 0.00001882
Iteration 40/1000 | Loss: 0.00001881
Iteration 41/1000 | Loss: 0.00001880
Iteration 42/1000 | Loss: 0.00001880
Iteration 43/1000 | Loss: 0.00001876
Iteration 44/1000 | Loss: 0.00001876
Iteration 45/1000 | Loss: 0.00001874
Iteration 46/1000 | Loss: 0.00001874
Iteration 47/1000 | Loss: 0.00001874
Iteration 48/1000 | Loss: 0.00001874
Iteration 49/1000 | Loss: 0.00001874
Iteration 50/1000 | Loss: 0.00001874
Iteration 51/1000 | Loss: 0.00001873
Iteration 52/1000 | Loss: 0.00001873
Iteration 53/1000 | Loss: 0.00001873
Iteration 54/1000 | Loss: 0.00001873
Iteration 55/1000 | Loss: 0.00001873
Iteration 56/1000 | Loss: 0.00001873
Iteration 57/1000 | Loss: 0.00001872
Iteration 58/1000 | Loss: 0.00001872
Iteration 59/1000 | Loss: 0.00001871
Iteration 60/1000 | Loss: 0.00001871
Iteration 61/1000 | Loss: 0.00001871
Iteration 62/1000 | Loss: 0.00001871
Iteration 63/1000 | Loss: 0.00001871
Iteration 64/1000 | Loss: 0.00001871
Iteration 65/1000 | Loss: 0.00001871
Iteration 66/1000 | Loss: 0.00001871
Iteration 67/1000 | Loss: 0.00001871
Iteration 68/1000 | Loss: 0.00001871
Iteration 69/1000 | Loss: 0.00001870
Iteration 70/1000 | Loss: 0.00001870
Iteration 71/1000 | Loss: 0.00001870
Iteration 72/1000 | Loss: 0.00001870
Iteration 73/1000 | Loss: 0.00001870
Iteration 74/1000 | Loss: 0.00001870
Iteration 75/1000 | Loss: 0.00001869
Iteration 76/1000 | Loss: 0.00001869
Iteration 77/1000 | Loss: 0.00001869
Iteration 78/1000 | Loss: 0.00001868
Iteration 79/1000 | Loss: 0.00001868
Iteration 80/1000 | Loss: 0.00001867
Iteration 81/1000 | Loss: 0.00001867
Iteration 82/1000 | Loss: 0.00001867
Iteration 83/1000 | Loss: 0.00001867
Iteration 84/1000 | Loss: 0.00001867
Iteration 85/1000 | Loss: 0.00001866
Iteration 86/1000 | Loss: 0.00001866
Iteration 87/1000 | Loss: 0.00001866
Iteration 88/1000 | Loss: 0.00001866
Iteration 89/1000 | Loss: 0.00001865
Iteration 90/1000 | Loss: 0.00001865
Iteration 91/1000 | Loss: 0.00001865
Iteration 92/1000 | Loss: 0.00001865
Iteration 93/1000 | Loss: 0.00001865
Iteration 94/1000 | Loss: 0.00001865
Iteration 95/1000 | Loss: 0.00001865
Iteration 96/1000 | Loss: 0.00001865
Iteration 97/1000 | Loss: 0.00001865
Iteration 98/1000 | Loss: 0.00001865
Iteration 99/1000 | Loss: 0.00001865
Iteration 100/1000 | Loss: 0.00001865
Iteration 101/1000 | Loss: 0.00001865
Iteration 102/1000 | Loss: 0.00001865
Iteration 103/1000 | Loss: 0.00001865
Iteration 104/1000 | Loss: 0.00001864
Iteration 105/1000 | Loss: 0.00001864
Iteration 106/1000 | Loss: 0.00001864
Iteration 107/1000 | Loss: 0.00001864
Iteration 108/1000 | Loss: 0.00001864
Iteration 109/1000 | Loss: 0.00001864
Iteration 110/1000 | Loss: 0.00001864
Iteration 111/1000 | Loss: 0.00001864
Iteration 112/1000 | Loss: 0.00001864
Iteration 113/1000 | Loss: 0.00001864
Iteration 114/1000 | Loss: 0.00001864
Iteration 115/1000 | Loss: 0.00001864
Iteration 116/1000 | Loss: 0.00001864
Iteration 117/1000 | Loss: 0.00001864
Iteration 118/1000 | Loss: 0.00001864
Iteration 119/1000 | Loss: 0.00001864
Iteration 120/1000 | Loss: 0.00001864
Iteration 121/1000 | Loss: 0.00001864
Iteration 122/1000 | Loss: 0.00001863
Iteration 123/1000 | Loss: 0.00001863
Iteration 124/1000 | Loss: 0.00001863
Iteration 125/1000 | Loss: 0.00001863
Iteration 126/1000 | Loss: 0.00001863
Iteration 127/1000 | Loss: 0.00001863
Iteration 128/1000 | Loss: 0.00001863
Iteration 129/1000 | Loss: 0.00001863
Iteration 130/1000 | Loss: 0.00001863
Iteration 131/1000 | Loss: 0.00001863
Iteration 132/1000 | Loss: 0.00001863
Iteration 133/1000 | Loss: 0.00001863
Iteration 134/1000 | Loss: 0.00001863
Iteration 135/1000 | Loss: 0.00001863
Iteration 136/1000 | Loss: 0.00001863
Iteration 137/1000 | Loss: 0.00001863
Iteration 138/1000 | Loss: 0.00001863
Iteration 139/1000 | Loss: 0.00001863
Iteration 140/1000 | Loss: 0.00001863
Iteration 141/1000 | Loss: 0.00001863
Iteration 142/1000 | Loss: 0.00001862
Iteration 143/1000 | Loss: 0.00001862
Iteration 144/1000 | Loss: 0.00001862
Iteration 145/1000 | Loss: 0.00001862
Iteration 146/1000 | Loss: 0.00001862
Iteration 147/1000 | Loss: 0.00001862
Iteration 148/1000 | Loss: 0.00001862
Iteration 149/1000 | Loss: 0.00001862
Iteration 150/1000 | Loss: 0.00001862
Iteration 151/1000 | Loss: 0.00001862
Iteration 152/1000 | Loss: 0.00001862
Iteration 153/1000 | Loss: 0.00001862
Iteration 154/1000 | Loss: 0.00001861
Iteration 155/1000 | Loss: 0.00001861
Iteration 156/1000 | Loss: 0.00001861
Iteration 157/1000 | Loss: 0.00001861
Iteration 158/1000 | Loss: 0.00001861
Iteration 159/1000 | Loss: 0.00001861
Iteration 160/1000 | Loss: 0.00001861
Iteration 161/1000 | Loss: 0.00001861
Iteration 162/1000 | Loss: 0.00001861
Iteration 163/1000 | Loss: 0.00001861
Iteration 164/1000 | Loss: 0.00001861
Iteration 165/1000 | Loss: 0.00001861
Iteration 166/1000 | Loss: 0.00001861
Iteration 167/1000 | Loss: 0.00001861
Iteration 168/1000 | Loss: 0.00001861
Iteration 169/1000 | Loss: 0.00001861
Iteration 170/1000 | Loss: 0.00001861
Iteration 171/1000 | Loss: 0.00001860
Iteration 172/1000 | Loss: 0.00001860
Iteration 173/1000 | Loss: 0.00001860
Iteration 174/1000 | Loss: 0.00001860
Iteration 175/1000 | Loss: 0.00001860
Iteration 176/1000 | Loss: 0.00001860
Iteration 177/1000 | Loss: 0.00001860
Iteration 178/1000 | Loss: 0.00001860
Iteration 179/1000 | Loss: 0.00001860
Iteration 180/1000 | Loss: 0.00001860
Iteration 181/1000 | Loss: 0.00001860
Iteration 182/1000 | Loss: 0.00001860
Iteration 183/1000 | Loss: 0.00001860
Iteration 184/1000 | Loss: 0.00001860
Iteration 185/1000 | Loss: 0.00001859
Iteration 186/1000 | Loss: 0.00001859
Iteration 187/1000 | Loss: 0.00001859
Iteration 188/1000 | Loss: 0.00001859
Iteration 189/1000 | Loss: 0.00001859
Iteration 190/1000 | Loss: 0.00001859
Iteration 191/1000 | Loss: 0.00001859
Iteration 192/1000 | Loss: 0.00001859
Iteration 193/1000 | Loss: 0.00001859
Iteration 194/1000 | Loss: 0.00001859
Iteration 195/1000 | Loss: 0.00001859
Iteration 196/1000 | Loss: 0.00001859
Iteration 197/1000 | Loss: 0.00001859
Iteration 198/1000 | Loss: 0.00001859
Iteration 199/1000 | Loss: 0.00001859
Iteration 200/1000 | Loss: 0.00001859
Iteration 201/1000 | Loss: 0.00001859
Iteration 202/1000 | Loss: 0.00001859
Iteration 203/1000 | Loss: 0.00001859
Iteration 204/1000 | Loss: 0.00001859
Iteration 205/1000 | Loss: 0.00001859
Iteration 206/1000 | Loss: 0.00001859
Iteration 207/1000 | Loss: 0.00001859
Iteration 208/1000 | Loss: 0.00001859
Iteration 209/1000 | Loss: 0.00001859
Iteration 210/1000 | Loss: 0.00001859
Iteration 211/1000 | Loss: 0.00001859
Iteration 212/1000 | Loss: 0.00001859
Iteration 213/1000 | Loss: 0.00001859
Iteration 214/1000 | Loss: 0.00001859
Iteration 215/1000 | Loss: 0.00001859
Iteration 216/1000 | Loss: 0.00001859
Iteration 217/1000 | Loss: 0.00001859
Iteration 218/1000 | Loss: 0.00001859
Iteration 219/1000 | Loss: 0.00001859
Iteration 220/1000 | Loss: 0.00001859
Iteration 221/1000 | Loss: 0.00001859
Iteration 222/1000 | Loss: 0.00001859
Iteration 223/1000 | Loss: 0.00001859
Iteration 224/1000 | Loss: 0.00001859
Iteration 225/1000 | Loss: 0.00001859
Iteration 226/1000 | Loss: 0.00001859
Iteration 227/1000 | Loss: 0.00001859
Iteration 228/1000 | Loss: 0.00001859
Iteration 229/1000 | Loss: 0.00001859
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 229. Stopping optimization.
Last 5 losses: [1.859054282249417e-05, 1.859054282249417e-05, 1.859054282249417e-05, 1.859054282249417e-05, 1.859054282249417e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.859054282249417e-05

Optimization complete. Final v2v error: 3.776045560836792 mm

Highest mean error: 4.125610828399658 mm for frame 118

Lowest mean error: 3.502852439880371 mm for frame 15

Saving results

Total time: 38.695738315582275
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_39_us_1277/0007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_39_us_1277/0007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_39_us_1277/0007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01141220
Iteration 2/25 | Loss: 0.01141220
Iteration 3/25 | Loss: 0.01141219
Iteration 4/25 | Loss: 0.01141219
Iteration 5/25 | Loss: 0.01141219
Iteration 6/25 | Loss: 0.01141218
Iteration 7/25 | Loss: 0.00252502
Iteration 8/25 | Loss: 0.00204306
Iteration 9/25 | Loss: 0.00200387
Iteration 10/25 | Loss: 0.00191651
Iteration 11/25 | Loss: 0.00194947
Iteration 12/25 | Loss: 0.00177936
Iteration 13/25 | Loss: 0.00170392
Iteration 14/25 | Loss: 0.00167010
Iteration 15/25 | Loss: 0.00163574
Iteration 16/25 | Loss: 0.00161429
Iteration 17/25 | Loss: 0.00159084
Iteration 18/25 | Loss: 0.00155865
Iteration 19/25 | Loss: 0.00156467
Iteration 20/25 | Loss: 0.00151671
Iteration 21/25 | Loss: 0.00147961
Iteration 22/25 | Loss: 0.00146947
Iteration 23/25 | Loss: 0.00147106
Iteration 24/25 | Loss: 0.00146664
Iteration 25/25 | Loss: 0.00145831

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.25644302
Iteration 2/25 | Loss: 0.00328139
Iteration 3/25 | Loss: 0.00276271
Iteration 4/25 | Loss: 0.00276271
Iteration 5/25 | Loss: 0.00276271
Iteration 6/25 | Loss: 0.00276271
Iteration 7/25 | Loss: 0.00276271
Iteration 8/25 | Loss: 0.00276271
Iteration 9/25 | Loss: 0.00276271
Iteration 10/25 | Loss: 0.00276271
Iteration 11/25 | Loss: 0.00276271
Iteration 12/25 | Loss: 0.00276271
Iteration 13/25 | Loss: 0.00276271
Iteration 14/25 | Loss: 0.00276271
Iteration 15/25 | Loss: 0.00276271
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0027627074159681797, 0.0027627074159681797, 0.0027627074159681797, 0.0027627074159681797, 0.0027627074159681797]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0027627074159681797

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00276271
Iteration 2/1000 | Loss: 0.00126974
Iteration 3/1000 | Loss: 0.00053195
Iteration 4/1000 | Loss: 0.00033504
Iteration 5/1000 | Loss: 0.00024736
Iteration 6/1000 | Loss: 0.00019828
Iteration 7/1000 | Loss: 0.00207201
Iteration 8/1000 | Loss: 0.00123305
Iteration 9/1000 | Loss: 0.00130672
Iteration 10/1000 | Loss: 0.00223858
Iteration 11/1000 | Loss: 0.00042078
Iteration 12/1000 | Loss: 0.00073916
Iteration 13/1000 | Loss: 0.00077660
Iteration 14/1000 | Loss: 0.00147292
Iteration 15/1000 | Loss: 0.00123874
Iteration 16/1000 | Loss: 0.00106358
Iteration 17/1000 | Loss: 0.00069100
Iteration 18/1000 | Loss: 0.00033689
Iteration 19/1000 | Loss: 0.00073812
Iteration 20/1000 | Loss: 0.00013925
Iteration 21/1000 | Loss: 0.00012923
Iteration 22/1000 | Loss: 0.00028914
Iteration 23/1000 | Loss: 0.00031954
Iteration 24/1000 | Loss: 0.00012140
Iteration 25/1000 | Loss: 0.00027050
Iteration 26/1000 | Loss: 0.00011526
Iteration 27/1000 | Loss: 0.00024713
Iteration 28/1000 | Loss: 0.00011306
Iteration 29/1000 | Loss: 0.00011170
Iteration 30/1000 | Loss: 0.00015271
Iteration 31/1000 | Loss: 0.00014874
Iteration 32/1000 | Loss: 0.00270770
Iteration 33/1000 | Loss: 0.00872059
Iteration 34/1000 | Loss: 0.00934769
Iteration 35/1000 | Loss: 0.00067410
Iteration 36/1000 | Loss: 0.00034393
Iteration 37/1000 | Loss: 0.00024122
Iteration 38/1000 | Loss: 0.00070307
Iteration 39/1000 | Loss: 0.00020401
Iteration 40/1000 | Loss: 0.00028775
Iteration 41/1000 | Loss: 0.00023804
Iteration 42/1000 | Loss: 0.00006635
Iteration 43/1000 | Loss: 0.00016596
Iteration 44/1000 | Loss: 0.00030495
Iteration 45/1000 | Loss: 0.00004737
Iteration 46/1000 | Loss: 0.00014752
Iteration 47/1000 | Loss: 0.00006441
Iteration 48/1000 | Loss: 0.00017636
Iteration 49/1000 | Loss: 0.00007315
Iteration 50/1000 | Loss: 0.00013420
Iteration 51/1000 | Loss: 0.00034596
Iteration 52/1000 | Loss: 0.00005808
Iteration 53/1000 | Loss: 0.00004154
Iteration 54/1000 | Loss: 0.00008411
Iteration 55/1000 | Loss: 0.00003197
Iteration 56/1000 | Loss: 0.00002443
Iteration 57/1000 | Loss: 0.00014592
Iteration 58/1000 | Loss: 0.00005538
Iteration 59/1000 | Loss: 0.00002995
Iteration 60/1000 | Loss: 0.00008788
Iteration 61/1000 | Loss: 0.00006092
Iteration 62/1000 | Loss: 0.00009860
Iteration 63/1000 | Loss: 0.00009245
Iteration 64/1000 | Loss: 0.00002907
Iteration 65/1000 | Loss: 0.00002067
Iteration 66/1000 | Loss: 0.00005222
Iteration 67/1000 | Loss: 0.00002039
Iteration 68/1000 | Loss: 0.00002020
Iteration 69/1000 | Loss: 0.00002016
Iteration 70/1000 | Loss: 0.00007415
Iteration 71/1000 | Loss: 0.00002016
Iteration 72/1000 | Loss: 0.00002003
Iteration 73/1000 | Loss: 0.00002003
Iteration 74/1000 | Loss: 0.00002002
Iteration 75/1000 | Loss: 0.00002001
Iteration 76/1000 | Loss: 0.00002000
Iteration 77/1000 | Loss: 0.00002000
Iteration 78/1000 | Loss: 0.00002000
Iteration 79/1000 | Loss: 0.00002000
Iteration 80/1000 | Loss: 0.00002000
Iteration 81/1000 | Loss: 0.00002000
Iteration 82/1000 | Loss: 0.00002000
Iteration 83/1000 | Loss: 0.00001996
Iteration 84/1000 | Loss: 0.00001994
Iteration 85/1000 | Loss: 0.00001993
Iteration 86/1000 | Loss: 0.00001990
Iteration 87/1000 | Loss: 0.00001990
Iteration 88/1000 | Loss: 0.00001989
Iteration 89/1000 | Loss: 0.00001989
Iteration 90/1000 | Loss: 0.00001988
Iteration 91/1000 | Loss: 0.00001987
Iteration 92/1000 | Loss: 0.00001987
Iteration 93/1000 | Loss: 0.00001986
Iteration 94/1000 | Loss: 0.00001986
Iteration 95/1000 | Loss: 0.00001984
Iteration 96/1000 | Loss: 0.00001983
Iteration 97/1000 | Loss: 0.00001983
Iteration 98/1000 | Loss: 0.00001983
Iteration 99/1000 | Loss: 0.00001983
Iteration 100/1000 | Loss: 0.00001982
Iteration 101/1000 | Loss: 0.00001981
Iteration 102/1000 | Loss: 0.00001981
Iteration 103/1000 | Loss: 0.00001981
Iteration 104/1000 | Loss: 0.00001980
Iteration 105/1000 | Loss: 0.00001980
Iteration 106/1000 | Loss: 0.00001980
Iteration 107/1000 | Loss: 0.00001980
Iteration 108/1000 | Loss: 0.00001980
Iteration 109/1000 | Loss: 0.00001979
Iteration 110/1000 | Loss: 0.00001979
Iteration 111/1000 | Loss: 0.00001979
Iteration 112/1000 | Loss: 0.00001979
Iteration 113/1000 | Loss: 0.00001979
Iteration 114/1000 | Loss: 0.00001979
Iteration 115/1000 | Loss: 0.00001979
Iteration 116/1000 | Loss: 0.00001979
Iteration 117/1000 | Loss: 0.00001979
Iteration 118/1000 | Loss: 0.00001978
Iteration 119/1000 | Loss: 0.00001978
Iteration 120/1000 | Loss: 0.00001977
Iteration 121/1000 | Loss: 0.00001977
Iteration 122/1000 | Loss: 0.00001977
Iteration 123/1000 | Loss: 0.00001977
Iteration 124/1000 | Loss: 0.00001977
Iteration 125/1000 | Loss: 0.00001976
Iteration 126/1000 | Loss: 0.00001976
Iteration 127/1000 | Loss: 0.00001976
Iteration 128/1000 | Loss: 0.00001975
Iteration 129/1000 | Loss: 0.00001975
Iteration 130/1000 | Loss: 0.00001975
Iteration 131/1000 | Loss: 0.00001975
Iteration 132/1000 | Loss: 0.00001975
Iteration 133/1000 | Loss: 0.00001975
Iteration 134/1000 | Loss: 0.00001975
Iteration 135/1000 | Loss: 0.00001974
Iteration 136/1000 | Loss: 0.00001974
Iteration 137/1000 | Loss: 0.00001974
Iteration 138/1000 | Loss: 0.00001973
Iteration 139/1000 | Loss: 0.00001973
Iteration 140/1000 | Loss: 0.00001973
Iteration 141/1000 | Loss: 0.00001973
Iteration 142/1000 | Loss: 0.00001972
Iteration 143/1000 | Loss: 0.00001972
Iteration 144/1000 | Loss: 0.00001972
Iteration 145/1000 | Loss: 0.00001972
Iteration 146/1000 | Loss: 0.00001971
Iteration 147/1000 | Loss: 0.00001971
Iteration 148/1000 | Loss: 0.00001971
Iteration 149/1000 | Loss: 0.00001971
Iteration 150/1000 | Loss: 0.00001971
Iteration 151/1000 | Loss: 0.00001971
Iteration 152/1000 | Loss: 0.00001971
Iteration 153/1000 | Loss: 0.00001971
Iteration 154/1000 | Loss: 0.00001970
Iteration 155/1000 | Loss: 0.00001970
Iteration 156/1000 | Loss: 0.00001970
Iteration 157/1000 | Loss: 0.00001970
Iteration 158/1000 | Loss: 0.00001970
Iteration 159/1000 | Loss: 0.00001970
Iteration 160/1000 | Loss: 0.00001970
Iteration 161/1000 | Loss: 0.00001969
Iteration 162/1000 | Loss: 0.00001969
Iteration 163/1000 | Loss: 0.00001969
Iteration 164/1000 | Loss: 0.00001969
Iteration 165/1000 | Loss: 0.00001969
Iteration 166/1000 | Loss: 0.00001969
Iteration 167/1000 | Loss: 0.00001969
Iteration 168/1000 | Loss: 0.00001969
Iteration 169/1000 | Loss: 0.00001969
Iteration 170/1000 | Loss: 0.00001969
Iteration 171/1000 | Loss: 0.00001969
Iteration 172/1000 | Loss: 0.00001969
Iteration 173/1000 | Loss: 0.00001969
Iteration 174/1000 | Loss: 0.00001969
Iteration 175/1000 | Loss: 0.00001969
Iteration 176/1000 | Loss: 0.00001969
Iteration 177/1000 | Loss: 0.00001969
Iteration 178/1000 | Loss: 0.00001969
Iteration 179/1000 | Loss: 0.00001969
Iteration 180/1000 | Loss: 0.00001969
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 180. Stopping optimization.
Last 5 losses: [1.9690958652063273e-05, 1.9690958652063273e-05, 1.9690958652063273e-05, 1.9690958652063273e-05, 1.9690958652063273e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9690958652063273e-05

Optimization complete. Final v2v error: 3.795919895172119 mm

Highest mean error: 4.316380500793457 mm for frame 6

Lowest mean error: 3.6270482540130615 mm for frame 136

Saving results

Total time: 139.26594853401184
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_39_us_1277/0003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_39_us_1277/0003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_39_us_1277/0003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00940777
Iteration 2/25 | Loss: 0.00180478
Iteration 3/25 | Loss: 0.00146070
Iteration 4/25 | Loss: 0.00142808
Iteration 5/25 | Loss: 0.00141304
Iteration 6/25 | Loss: 0.00138468
Iteration 7/25 | Loss: 0.00136595
Iteration 8/25 | Loss: 0.00135367
Iteration 9/25 | Loss: 0.00135207
Iteration 10/25 | Loss: 0.00134980
Iteration 11/25 | Loss: 0.00135301
Iteration 12/25 | Loss: 0.00134907
Iteration 13/25 | Loss: 0.00134893
Iteration 14/25 | Loss: 0.00134593
Iteration 15/25 | Loss: 0.00134378
Iteration 16/25 | Loss: 0.00134359
Iteration 17/25 | Loss: 0.00134369
Iteration 18/25 | Loss: 0.00134322
Iteration 19/25 | Loss: 0.00134328
Iteration 20/25 | Loss: 0.00134344
Iteration 21/25 | Loss: 0.00134298
Iteration 22/25 | Loss: 0.00134279
Iteration 23/25 | Loss: 0.00134281
Iteration 24/25 | Loss: 0.00134282
Iteration 25/25 | Loss: 0.00134312

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.45704460
Iteration 2/25 | Loss: 0.00094769
Iteration 3/25 | Loss: 0.00088311
Iteration 4/25 | Loss: 0.00088311
Iteration 5/25 | Loss: 0.00088311
Iteration 6/25 | Loss: 0.00088311
Iteration 7/25 | Loss: 0.00088311
Iteration 8/25 | Loss: 0.00088311
Iteration 9/25 | Loss: 0.00088311
Iteration 10/25 | Loss: 0.00088311
Iteration 11/25 | Loss: 0.00088311
Iteration 12/25 | Loss: 0.00088311
Iteration 13/25 | Loss: 0.00088311
Iteration 14/25 | Loss: 0.00088311
Iteration 15/25 | Loss: 0.00088311
Iteration 16/25 | Loss: 0.00088311
Iteration 17/25 | Loss: 0.00088311
Iteration 18/25 | Loss: 0.00088311
Iteration 19/25 | Loss: 0.00088311
Iteration 20/25 | Loss: 0.00088311
Iteration 21/25 | Loss: 0.00088311
Iteration 22/25 | Loss: 0.00088311
Iteration 23/25 | Loss: 0.00088311
Iteration 24/25 | Loss: 0.00088311
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.000883106782566756, 0.000883106782566756, 0.000883106782566756, 0.000883106782566756, 0.000883106782566756]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000883106782566756

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00088311
Iteration 2/1000 | Loss: 0.00008156
Iteration 3/1000 | Loss: 0.00005211
Iteration 4/1000 | Loss: 0.00003571
Iteration 5/1000 | Loss: 0.00003221
Iteration 6/1000 | Loss: 0.00003343
Iteration 7/1000 | Loss: 0.00003148
Iteration 8/1000 | Loss: 0.00003279
Iteration 9/1000 | Loss: 0.00003049
Iteration 10/1000 | Loss: 0.00003165
Iteration 11/1000 | Loss: 0.00003132
Iteration 12/1000 | Loss: 0.00003080
Iteration 13/1000 | Loss: 0.00003084
Iteration 14/1000 | Loss: 0.00002943
Iteration 15/1000 | Loss: 0.00003003
Iteration 16/1000 | Loss: 0.00003063
Iteration 17/1000 | Loss: 0.00003085
Iteration 18/1000 | Loss: 0.00003082
Iteration 19/1000 | Loss: 0.00003095
Iteration 20/1000 | Loss: 0.00003055
Iteration 21/1000 | Loss: 0.00003047
Iteration 22/1000 | Loss: 0.00003037
Iteration 23/1000 | Loss: 0.00003007
Iteration 24/1000 | Loss: 0.00002987
Iteration 25/1000 | Loss: 0.00002888
Iteration 26/1000 | Loss: 0.00002749
Iteration 27/1000 | Loss: 0.00002759
Iteration 28/1000 | Loss: 0.00003061
Iteration 29/1000 | Loss: 0.00003107
Iteration 30/1000 | Loss: 0.00003156
Iteration 31/1000 | Loss: 0.00002825
Iteration 32/1000 | Loss: 0.00002862
Iteration 33/1000 | Loss: 0.00002999
Iteration 34/1000 | Loss: 0.00003004
Iteration 35/1000 | Loss: 0.00003008
Iteration 36/1000 | Loss: 0.00003000
Iteration 37/1000 | Loss: 0.00003004
Iteration 38/1000 | Loss: 0.00002998
Iteration 39/1000 | Loss: 0.00002999
Iteration 40/1000 | Loss: 0.00002995
Iteration 41/1000 | Loss: 0.00002983
Iteration 42/1000 | Loss: 0.00003007
Iteration 43/1000 | Loss: 0.00002895
Iteration 44/1000 | Loss: 0.00002909
Iteration 45/1000 | Loss: 0.00002995
Iteration 46/1000 | Loss: 0.00002984
Iteration 47/1000 | Loss: 0.00002992
Iteration 48/1000 | Loss: 0.00002995
Iteration 49/1000 | Loss: 0.00002974
Iteration 50/1000 | Loss: 0.00002978
Iteration 51/1000 | Loss: 0.00002977
Iteration 52/1000 | Loss: 0.00002992
Iteration 53/1000 | Loss: 0.00002882
Iteration 54/1000 | Loss: 0.00002994
Iteration 55/1000 | Loss: 0.00002996
Iteration 56/1000 | Loss: 0.00002967
Iteration 57/1000 | Loss: 0.00003001
Iteration 58/1000 | Loss: 0.00002974
Iteration 59/1000 | Loss: 0.00002980
Iteration 60/1000 | Loss: 0.00002993
Iteration 61/1000 | Loss: 0.00002979
Iteration 62/1000 | Loss: 0.00002984
Iteration 63/1000 | Loss: 0.00002988
Iteration 64/1000 | Loss: 0.00003008
Iteration 65/1000 | Loss: 0.00002983
Iteration 66/1000 | Loss: 0.00002979
Iteration 67/1000 | Loss: 0.00002978
Iteration 68/1000 | Loss: 0.00011425
Iteration 69/1000 | Loss: 0.00003943
Iteration 70/1000 | Loss: 0.00003175
Iteration 71/1000 | Loss: 0.00003502
Iteration 72/1000 | Loss: 0.00003052
Iteration 73/1000 | Loss: 0.00002987
Iteration 74/1000 | Loss: 0.00002967
Iteration 75/1000 | Loss: 0.00002986
Iteration 76/1000 | Loss: 0.00002994
Iteration 77/1000 | Loss: 0.00002998
Iteration 78/1000 | Loss: 0.00002864
Iteration 79/1000 | Loss: 0.00003167
Iteration 80/1000 | Loss: 0.00002939
Iteration 81/1000 | Loss: 0.00002995
Iteration 82/1000 | Loss: 0.00010647
Iteration 83/1000 | Loss: 0.00003241
Iteration 84/1000 | Loss: 0.00003011
Iteration 85/1000 | Loss: 0.00002945
Iteration 86/1000 | Loss: 0.00002898
Iteration 87/1000 | Loss: 0.00002858
Iteration 88/1000 | Loss: 0.00002990
Iteration 89/1000 | Loss: 0.00002990
Iteration 90/1000 | Loss: 0.00002979
Iteration 91/1000 | Loss: 0.00002994
Iteration 92/1000 | Loss: 0.00002816
Iteration 93/1000 | Loss: 0.00002895
Iteration 94/1000 | Loss: 0.00002980
Iteration 95/1000 | Loss: 0.00002989
Iteration 96/1000 | Loss: 0.00002953
Iteration 97/1000 | Loss: 0.00002749
Iteration 98/1000 | Loss: 0.00002931
Iteration 99/1000 | Loss: 0.00002845
Iteration 100/1000 | Loss: 0.00003008
Iteration 101/1000 | Loss: 0.00002956
Iteration 102/1000 | Loss: 0.00002882
Iteration 103/1000 | Loss: 0.00002986
Iteration 104/1000 | Loss: 0.00002984
Iteration 105/1000 | Loss: 0.00003174
Iteration 106/1000 | Loss: 0.00002968
Iteration 107/1000 | Loss: 0.00002984
Iteration 108/1000 | Loss: 0.00002852
Iteration 109/1000 | Loss: 0.00003373
Iteration 110/1000 | Loss: 0.00002952
Iteration 111/1000 | Loss: 0.00002992
Iteration 112/1000 | Loss: 0.00003012
Iteration 113/1000 | Loss: 0.00002934
Iteration 114/1000 | Loss: 0.00002967
Iteration 115/1000 | Loss: 0.00002999
Iteration 116/1000 | Loss: 0.00002999
Iteration 117/1000 | Loss: 0.00002922
Iteration 118/1000 | Loss: 0.00002968
Iteration 119/1000 | Loss: 0.00002968
Iteration 120/1000 | Loss: 0.00003076
Iteration 121/1000 | Loss: 0.00002957
Iteration 122/1000 | Loss: 0.00002974
Iteration 123/1000 | Loss: 0.00002992
Iteration 124/1000 | Loss: 0.00003070
Iteration 125/1000 | Loss: 0.00002984
Iteration 126/1000 | Loss: 0.00003078
Iteration 127/1000 | Loss: 0.00002939
Iteration 128/1000 | Loss: 0.00002812
Iteration 129/1000 | Loss: 0.00002996
Iteration 130/1000 | Loss: 0.00002981
Iteration 131/1000 | Loss: 0.00003072
Iteration 132/1000 | Loss: 0.00012817
Iteration 133/1000 | Loss: 0.00023768
Iteration 134/1000 | Loss: 0.00007509
Iteration 135/1000 | Loss: 0.00003663
Iteration 136/1000 | Loss: 0.00003016
Iteration 137/1000 | Loss: 0.00002853
Iteration 138/1000 | Loss: 0.00003017
Iteration 139/1000 | Loss: 0.00003082
Iteration 140/1000 | Loss: 0.00002993
Iteration 141/1000 | Loss: 0.00003028
Iteration 142/1000 | Loss: 0.00002975
Iteration 143/1000 | Loss: 0.00002980
Iteration 144/1000 | Loss: 0.00002861
Iteration 145/1000 | Loss: 0.00003077
Iteration 146/1000 | Loss: 0.00002865
Iteration 147/1000 | Loss: 0.00003160
Iteration 148/1000 | Loss: 0.00002703
Iteration 149/1000 | Loss: 0.00002648
Iteration 150/1000 | Loss: 0.00003002
Iteration 151/1000 | Loss: 0.00002814
Iteration 152/1000 | Loss: 0.00002938
Iteration 153/1000 | Loss: 0.00002844
Iteration 154/1000 | Loss: 0.00003171
Iteration 155/1000 | Loss: 0.00002835
Iteration 156/1000 | Loss: 0.00002844
Iteration 157/1000 | Loss: 0.00003026
Iteration 158/1000 | Loss: 0.00002837
Iteration 159/1000 | Loss: 0.00002916
Iteration 160/1000 | Loss: 0.00003015
Iteration 161/1000 | Loss: 0.00011779
Iteration 162/1000 | Loss: 0.00020724
Iteration 163/1000 | Loss: 0.00002855
Iteration 164/1000 | Loss: 0.00002855
Iteration 165/1000 | Loss: 0.00003356
Iteration 166/1000 | Loss: 0.00003112
Iteration 167/1000 | Loss: 0.00003304
Iteration 168/1000 | Loss: 0.00002969
Iteration 169/1000 | Loss: 0.00002854
Iteration 170/1000 | Loss: 0.00002879
Iteration 171/1000 | Loss: 0.00002834
Iteration 172/1000 | Loss: 0.00002961
Iteration 173/1000 | Loss: 0.00002859
Iteration 174/1000 | Loss: 0.00002829
Iteration 175/1000 | Loss: 0.00002865
Iteration 176/1000 | Loss: 0.00003143
Iteration 177/1000 | Loss: 0.00002954
Iteration 178/1000 | Loss: 0.00003010
Iteration 179/1000 | Loss: 0.00002966
Iteration 180/1000 | Loss: 0.00003003
Iteration 181/1000 | Loss: 0.00002984
Iteration 182/1000 | Loss: 0.00002969
Iteration 183/1000 | Loss: 0.00002923
Iteration 184/1000 | Loss: 0.00002970
Iteration 185/1000 | Loss: 0.00002947
Iteration 186/1000 | Loss: 0.00002869
Iteration 187/1000 | Loss: 0.00002904
Iteration 188/1000 | Loss: 0.00002975
Iteration 189/1000 | Loss: 0.00002767
Iteration 190/1000 | Loss: 0.00002825
Iteration 191/1000 | Loss: 0.00003158
Iteration 192/1000 | Loss: 0.00002950
Iteration 193/1000 | Loss: 0.00003019
Iteration 194/1000 | Loss: 0.00002950
Iteration 195/1000 | Loss: 0.00002978
Iteration 196/1000 | Loss: 0.00002926
Iteration 197/1000 | Loss: 0.00002967
Iteration 198/1000 | Loss: 0.00013624
Iteration 199/1000 | Loss: 0.00004110
Iteration 200/1000 | Loss: 0.00002961
Iteration 201/1000 | Loss: 0.00002999
Iteration 202/1000 | Loss: 0.00003006
Iteration 203/1000 | Loss: 0.00002987
Iteration 204/1000 | Loss: 0.00003026
Iteration 205/1000 | Loss: 0.00002829
Iteration 206/1000 | Loss: 0.00002851
Iteration 207/1000 | Loss: 0.00002993
Iteration 208/1000 | Loss: 0.00002957
Iteration 209/1000 | Loss: 0.00003001
Iteration 210/1000 | Loss: 0.00002957
Iteration 211/1000 | Loss: 0.00003004
Iteration 212/1000 | Loss: 0.00002944
Iteration 213/1000 | Loss: 0.00002997
Iteration 214/1000 | Loss: 0.00002953
Iteration 215/1000 | Loss: 0.00002997
Iteration 216/1000 | Loss: 0.00002948
Iteration 217/1000 | Loss: 0.00002995
Iteration 218/1000 | Loss: 0.00002952
Iteration 219/1000 | Loss: 0.00002810
Iteration 220/1000 | Loss: 0.00002873
Iteration 221/1000 | Loss: 0.00003029
Iteration 222/1000 | Loss: 0.00002949
Iteration 223/1000 | Loss: 0.00002948
Iteration 224/1000 | Loss: 0.00002971
Iteration 225/1000 | Loss: 0.00002927
Iteration 226/1000 | Loss: 0.00002988
Iteration 227/1000 | Loss: 0.00002869
Iteration 228/1000 | Loss: 0.00002925
Iteration 229/1000 | Loss: 0.00016290
Iteration 230/1000 | Loss: 0.00003695
Iteration 231/1000 | Loss: 0.00002962
Iteration 232/1000 | Loss: 0.00002997
Iteration 233/1000 | Loss: 0.00002964
Iteration 234/1000 | Loss: 0.00002963
Iteration 235/1000 | Loss: 0.00002962
Iteration 236/1000 | Loss: 0.00002867
Iteration 237/1000 | Loss: 0.00002833
Iteration 238/1000 | Loss: 0.00003006
Iteration 239/1000 | Loss: 0.00002901
Iteration 240/1000 | Loss: 0.00002939
Iteration 241/1000 | Loss: 0.00002900
Iteration 242/1000 | Loss: 0.00002828
Iteration 243/1000 | Loss: 0.00003118
Iteration 244/1000 | Loss: 0.00002969
Iteration 245/1000 | Loss: 0.00002823
Iteration 246/1000 | Loss: 0.00003009
Iteration 247/1000 | Loss: 0.00003262
Iteration 248/1000 | Loss: 0.00003004
Iteration 249/1000 | Loss: 0.00002892
Iteration 250/1000 | Loss: 0.00003069
Iteration 251/1000 | Loss: 0.00002907
Iteration 252/1000 | Loss: 0.00003011
Iteration 253/1000 | Loss: 0.00002870
Iteration 254/1000 | Loss: 0.00003048
Iteration 255/1000 | Loss: 0.00002847
Iteration 256/1000 | Loss: 0.00003081
Iteration 257/1000 | Loss: 0.00003041
Iteration 258/1000 | Loss: 0.00003442
Iteration 259/1000 | Loss: 0.00002986
Iteration 260/1000 | Loss: 0.00003764
Iteration 261/1000 | Loss: 0.00002820
Iteration 262/1000 | Loss: 0.00002657
Iteration 263/1000 | Loss: 0.00002624
Iteration 264/1000 | Loss: 0.00002624
Iteration 265/1000 | Loss: 0.00002624
Iteration 266/1000 | Loss: 0.00002624
Iteration 267/1000 | Loss: 0.00002624
Iteration 268/1000 | Loss: 0.00002624
Iteration 269/1000 | Loss: 0.00002624
Iteration 270/1000 | Loss: 0.00002623
Iteration 271/1000 | Loss: 0.00002623
Iteration 272/1000 | Loss: 0.00002623
Iteration 273/1000 | Loss: 0.00002622
Iteration 274/1000 | Loss: 0.00002618
Iteration 275/1000 | Loss: 0.00002618
Iteration 276/1000 | Loss: 0.00002617
Iteration 277/1000 | Loss: 0.00002617
Iteration 278/1000 | Loss: 0.00002616
Iteration 279/1000 | Loss: 0.00002616
Iteration 280/1000 | Loss: 0.00002615
Iteration 281/1000 | Loss: 0.00002613
Iteration 282/1000 | Loss: 0.00002613
Iteration 283/1000 | Loss: 0.00002613
Iteration 284/1000 | Loss: 0.00002613
Iteration 285/1000 | Loss: 0.00002613
Iteration 286/1000 | Loss: 0.00002613
Iteration 287/1000 | Loss: 0.00002613
Iteration 288/1000 | Loss: 0.00002613
Iteration 289/1000 | Loss: 0.00002613
Iteration 290/1000 | Loss: 0.00002612
Iteration 291/1000 | Loss: 0.00002612
Iteration 292/1000 | Loss: 0.00002612
Iteration 293/1000 | Loss: 0.00002611
Iteration 294/1000 | Loss: 0.00002611
Iteration 295/1000 | Loss: 0.00002610
Iteration 296/1000 | Loss: 0.00002610
Iteration 297/1000 | Loss: 0.00002610
Iteration 298/1000 | Loss: 0.00002610
Iteration 299/1000 | Loss: 0.00002610
Iteration 300/1000 | Loss: 0.00002609
Iteration 301/1000 | Loss: 0.00002609
Iteration 302/1000 | Loss: 0.00002609
Iteration 303/1000 | Loss: 0.00002608
Iteration 304/1000 | Loss: 0.00002608
Iteration 305/1000 | Loss: 0.00002608
Iteration 306/1000 | Loss: 0.00002608
Iteration 307/1000 | Loss: 0.00002607
Iteration 308/1000 | Loss: 0.00002607
Iteration 309/1000 | Loss: 0.00002607
Iteration 310/1000 | Loss: 0.00002607
Iteration 311/1000 | Loss: 0.00002607
Iteration 312/1000 | Loss: 0.00002606
Iteration 313/1000 | Loss: 0.00002606
Iteration 314/1000 | Loss: 0.00002606
Iteration 315/1000 | Loss: 0.00002606
Iteration 316/1000 | Loss: 0.00002606
Iteration 317/1000 | Loss: 0.00002606
Iteration 318/1000 | Loss: 0.00002606
Iteration 319/1000 | Loss: 0.00002606
Iteration 320/1000 | Loss: 0.00002606
Iteration 321/1000 | Loss: 0.00002606
Iteration 322/1000 | Loss: 0.00002606
Iteration 323/1000 | Loss: 0.00002606
Iteration 324/1000 | Loss: 0.00002606
Iteration 325/1000 | Loss: 0.00002606
Iteration 326/1000 | Loss: 0.00002606
Iteration 327/1000 | Loss: 0.00002606
Iteration 328/1000 | Loss: 0.00002606
Iteration 329/1000 | Loss: 0.00002606
Iteration 330/1000 | Loss: 0.00002605
Iteration 331/1000 | Loss: 0.00002605
Iteration 332/1000 | Loss: 0.00002605
Iteration 333/1000 | Loss: 0.00002605
Iteration 334/1000 | Loss: 0.00002605
Iteration 335/1000 | Loss: 0.00002605
Iteration 336/1000 | Loss: 0.00002605
Iteration 337/1000 | Loss: 0.00002605
Iteration 338/1000 | Loss: 0.00002605
Iteration 339/1000 | Loss: 0.00002605
Iteration 340/1000 | Loss: 0.00002605
Iteration 341/1000 | Loss: 0.00002605
Iteration 342/1000 | Loss: 0.00002605
Iteration 343/1000 | Loss: 0.00002605
Iteration 344/1000 | Loss: 0.00002604
Iteration 345/1000 | Loss: 0.00002604
Iteration 346/1000 | Loss: 0.00002604
Iteration 347/1000 | Loss: 0.00002604
Iteration 348/1000 | Loss: 0.00002604
Iteration 349/1000 | Loss: 0.00002604
Iteration 350/1000 | Loss: 0.00002604
Iteration 351/1000 | Loss: 0.00002604
Iteration 352/1000 | Loss: 0.00002604
Iteration 353/1000 | Loss: 0.00002604
Iteration 354/1000 | Loss: 0.00002604
Iteration 355/1000 | Loss: 0.00002604
Iteration 356/1000 | Loss: 0.00002604
Iteration 357/1000 | Loss: 0.00002604
Iteration 358/1000 | Loss: 0.00002604
Iteration 359/1000 | Loss: 0.00002604
Iteration 360/1000 | Loss: 0.00002604
Iteration 361/1000 | Loss: 0.00002604
Iteration 362/1000 | Loss: 0.00002604
Iteration 363/1000 | Loss: 0.00002604
Iteration 364/1000 | Loss: 0.00002604
Iteration 365/1000 | Loss: 0.00002604
Iteration 366/1000 | Loss: 0.00002604
Iteration 367/1000 | Loss: 0.00002604
Iteration 368/1000 | Loss: 0.00002604
Iteration 369/1000 | Loss: 0.00002604
Iteration 370/1000 | Loss: 0.00002604
Iteration 371/1000 | Loss: 0.00002604
Iteration 372/1000 | Loss: 0.00002604
Iteration 373/1000 | Loss: 0.00002604
Iteration 374/1000 | Loss: 0.00002604
Iteration 375/1000 | Loss: 0.00002604
Iteration 376/1000 | Loss: 0.00002604
Iteration 377/1000 | Loss: 0.00002604
Iteration 378/1000 | Loss: 0.00002604
Iteration 379/1000 | Loss: 0.00002604
Iteration 380/1000 | Loss: 0.00002604
Iteration 381/1000 | Loss: 0.00002604
Iteration 382/1000 | Loss: 0.00002604
Iteration 383/1000 | Loss: 0.00002604
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 383. Stopping optimization.
Last 5 losses: [2.6043613615911454e-05, 2.6043613615911454e-05, 2.6043613615911454e-05, 2.6043613615911454e-05, 2.6043613615911454e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.6043613615911454e-05

Optimization complete. Final v2v error: 4.301926136016846 mm

Highest mean error: 5.268718242645264 mm for frame 55

Lowest mean error: 3.831634759902954 mm for frame 153

Saving results

Total time: 444.8274259567261
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_39_us_1277/0018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_39_us_1277/0018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_39_us_1277/0018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01121232
Iteration 2/25 | Loss: 0.00207399
Iteration 3/25 | Loss: 0.00204396
Iteration 4/25 | Loss: 0.00132656
Iteration 5/25 | Loss: 0.00131375
Iteration 6/25 | Loss: 0.00131950
Iteration 7/25 | Loss: 0.00128442
Iteration 8/25 | Loss: 0.00125650
Iteration 9/25 | Loss: 0.00124202
Iteration 10/25 | Loss: 0.00122515
Iteration 11/25 | Loss: 0.00121975
Iteration 12/25 | Loss: 0.00121783
Iteration 13/25 | Loss: 0.00121639
Iteration 14/25 | Loss: 0.00121714
Iteration 15/25 | Loss: 0.00121604
Iteration 16/25 | Loss: 0.00121920
Iteration 17/25 | Loss: 0.00121689
Iteration 18/25 | Loss: 0.00122257
Iteration 19/25 | Loss: 0.00121691
Iteration 20/25 | Loss: 0.00121281
Iteration 21/25 | Loss: 0.00121354
Iteration 22/25 | Loss: 0.00121130
Iteration 23/25 | Loss: 0.00121160
Iteration 24/25 | Loss: 0.00121073
Iteration 25/25 | Loss: 0.00121286

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34994316
Iteration 2/25 | Loss: 0.00145310
Iteration 3/25 | Loss: 0.00109866
Iteration 4/25 | Loss: 0.00109866
Iteration 5/25 | Loss: 0.00109866
Iteration 6/25 | Loss: 0.00109866
Iteration 7/25 | Loss: 0.00109866
Iteration 8/25 | Loss: 0.00109866
Iteration 9/25 | Loss: 0.00109866
Iteration 10/25 | Loss: 0.00109865
Iteration 11/25 | Loss: 0.00109865
Iteration 12/25 | Loss: 0.00109865
Iteration 13/25 | Loss: 0.00109865
Iteration 14/25 | Loss: 0.00109865
Iteration 15/25 | Loss: 0.00109865
Iteration 16/25 | Loss: 0.00109865
Iteration 17/25 | Loss: 0.00109865
Iteration 18/25 | Loss: 0.00109865
Iteration 19/25 | Loss: 0.00109865
Iteration 20/25 | Loss: 0.00109865
Iteration 21/25 | Loss: 0.00109865
Iteration 22/25 | Loss: 0.00109865
Iteration 23/25 | Loss: 0.00109865
Iteration 24/25 | Loss: 0.00109865
Iteration 25/25 | Loss: 0.00109865

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00109865
Iteration 2/1000 | Loss: 0.00187615
Iteration 3/1000 | Loss: 0.00217635
Iteration 4/1000 | Loss: 0.00218108
Iteration 5/1000 | Loss: 0.00005400
Iteration 6/1000 | Loss: 0.00003895
Iteration 7/1000 | Loss: 0.00003312
Iteration 8/1000 | Loss: 0.00003041
Iteration 9/1000 | Loss: 0.00002924
Iteration 10/1000 | Loss: 0.00070658
Iteration 11/1000 | Loss: 0.00004756
Iteration 12/1000 | Loss: 0.00003084
Iteration 13/1000 | Loss: 0.00002870
Iteration 14/1000 | Loss: 0.00002477
Iteration 15/1000 | Loss: 0.00002294
Iteration 16/1000 | Loss: 0.00039559
Iteration 17/1000 | Loss: 0.00002970
Iteration 18/1000 | Loss: 0.00002416
Iteration 19/1000 | Loss: 0.00002190
Iteration 20/1000 | Loss: 0.00002049
Iteration 21/1000 | Loss: 0.00001984
Iteration 22/1000 | Loss: 0.00001955
Iteration 23/1000 | Loss: 0.00001929
Iteration 24/1000 | Loss: 0.00001921
Iteration 25/1000 | Loss: 0.00001901
Iteration 26/1000 | Loss: 0.00001892
Iteration 27/1000 | Loss: 0.00001890
Iteration 28/1000 | Loss: 0.00001883
Iteration 29/1000 | Loss: 0.00001882
Iteration 30/1000 | Loss: 0.00001881
Iteration 31/1000 | Loss: 0.00001880
Iteration 32/1000 | Loss: 0.00001880
Iteration 33/1000 | Loss: 0.00001880
Iteration 34/1000 | Loss: 0.00001876
Iteration 35/1000 | Loss: 0.00001873
Iteration 36/1000 | Loss: 0.00001873
Iteration 37/1000 | Loss: 0.00001870
Iteration 38/1000 | Loss: 0.00001869
Iteration 39/1000 | Loss: 0.00001869
Iteration 40/1000 | Loss: 0.00001989
Iteration 41/1000 | Loss: 0.00001903
Iteration 42/1000 | Loss: 0.00001866
Iteration 43/1000 | Loss: 0.00001866
Iteration 44/1000 | Loss: 0.00001956
Iteration 45/1000 | Loss: 0.00001899
Iteration 46/1000 | Loss: 0.00001864
Iteration 47/1000 | Loss: 0.00001858
Iteration 48/1000 | Loss: 0.00001858
Iteration 49/1000 | Loss: 0.00001858
Iteration 50/1000 | Loss: 0.00001858
Iteration 51/1000 | Loss: 0.00001858
Iteration 52/1000 | Loss: 0.00001858
Iteration 53/1000 | Loss: 0.00001858
Iteration 54/1000 | Loss: 0.00001858
Iteration 55/1000 | Loss: 0.00001857
Iteration 56/1000 | Loss: 0.00001857
Iteration 57/1000 | Loss: 0.00001857
Iteration 58/1000 | Loss: 0.00001857
Iteration 59/1000 | Loss: 0.00001856
Iteration 60/1000 | Loss: 0.00001856
Iteration 61/1000 | Loss: 0.00001856
Iteration 62/1000 | Loss: 0.00001856
Iteration 63/1000 | Loss: 0.00001856
Iteration 64/1000 | Loss: 0.00001856
Iteration 65/1000 | Loss: 0.00001855
Iteration 66/1000 | Loss: 0.00001855
Iteration 67/1000 | Loss: 0.00001855
Iteration 68/1000 | Loss: 0.00001855
Iteration 69/1000 | Loss: 0.00001855
Iteration 70/1000 | Loss: 0.00001855
Iteration 71/1000 | Loss: 0.00001854
Iteration 72/1000 | Loss: 0.00001854
Iteration 73/1000 | Loss: 0.00001853
Iteration 74/1000 | Loss: 0.00001852
Iteration 75/1000 | Loss: 0.00001852
Iteration 76/1000 | Loss: 0.00001852
Iteration 77/1000 | Loss: 0.00001852
Iteration 78/1000 | Loss: 0.00001852
Iteration 79/1000 | Loss: 0.00001852
Iteration 80/1000 | Loss: 0.00001852
Iteration 81/1000 | Loss: 0.00001851
Iteration 82/1000 | Loss: 0.00001851
Iteration 83/1000 | Loss: 0.00001851
Iteration 84/1000 | Loss: 0.00001851
Iteration 85/1000 | Loss: 0.00001851
Iteration 86/1000 | Loss: 0.00001851
Iteration 87/1000 | Loss: 0.00001851
Iteration 88/1000 | Loss: 0.00001851
Iteration 89/1000 | Loss: 0.00001850
Iteration 90/1000 | Loss: 0.00001850
Iteration 91/1000 | Loss: 0.00001850
Iteration 92/1000 | Loss: 0.00001849
Iteration 93/1000 | Loss: 0.00001849
Iteration 94/1000 | Loss: 0.00001849
Iteration 95/1000 | Loss: 0.00001849
Iteration 96/1000 | Loss: 0.00001849
Iteration 97/1000 | Loss: 0.00001849
Iteration 98/1000 | Loss: 0.00001849
Iteration 99/1000 | Loss: 0.00001849
Iteration 100/1000 | Loss: 0.00001849
Iteration 101/1000 | Loss: 0.00001906
Iteration 102/1000 | Loss: 0.00001906
Iteration 103/1000 | Loss: 0.00001906
Iteration 104/1000 | Loss: 0.00001906
Iteration 105/1000 | Loss: 0.00001906
Iteration 106/1000 | Loss: 0.00001869
Iteration 107/1000 | Loss: 0.00001857
Iteration 108/1000 | Loss: 0.00001857
Iteration 109/1000 | Loss: 0.00001846
Iteration 110/1000 | Loss: 0.00001846
Iteration 111/1000 | Loss: 0.00001845
Iteration 112/1000 | Loss: 0.00001845
Iteration 113/1000 | Loss: 0.00001845
Iteration 114/1000 | Loss: 0.00001845
Iteration 115/1000 | Loss: 0.00001845
Iteration 116/1000 | Loss: 0.00001845
Iteration 117/1000 | Loss: 0.00001845
Iteration 118/1000 | Loss: 0.00001844
Iteration 119/1000 | Loss: 0.00001844
Iteration 120/1000 | Loss: 0.00001844
Iteration 121/1000 | Loss: 0.00001844
Iteration 122/1000 | Loss: 0.00001844
Iteration 123/1000 | Loss: 0.00001844
Iteration 124/1000 | Loss: 0.00001844
Iteration 125/1000 | Loss: 0.00001844
Iteration 126/1000 | Loss: 0.00001844
Iteration 127/1000 | Loss: 0.00001844
Iteration 128/1000 | Loss: 0.00001843
Iteration 129/1000 | Loss: 0.00001843
Iteration 130/1000 | Loss: 0.00001843
Iteration 131/1000 | Loss: 0.00001843
Iteration 132/1000 | Loss: 0.00001843
Iteration 133/1000 | Loss: 0.00001843
Iteration 134/1000 | Loss: 0.00001843
Iteration 135/1000 | Loss: 0.00001843
Iteration 136/1000 | Loss: 0.00001843
Iteration 137/1000 | Loss: 0.00001843
Iteration 138/1000 | Loss: 0.00001843
Iteration 139/1000 | Loss: 0.00001843
Iteration 140/1000 | Loss: 0.00001843
Iteration 141/1000 | Loss: 0.00001843
Iteration 142/1000 | Loss: 0.00001843
Iteration 143/1000 | Loss: 0.00001843
Iteration 144/1000 | Loss: 0.00001843
Iteration 145/1000 | Loss: 0.00001843
Iteration 146/1000 | Loss: 0.00001843
Iteration 147/1000 | Loss: 0.00001843
Iteration 148/1000 | Loss: 0.00001843
Iteration 149/1000 | Loss: 0.00001843
Iteration 150/1000 | Loss: 0.00001843
Iteration 151/1000 | Loss: 0.00001843
Iteration 152/1000 | Loss: 0.00001843
Iteration 153/1000 | Loss: 0.00001843
Iteration 154/1000 | Loss: 0.00001843
Iteration 155/1000 | Loss: 0.00001843
Iteration 156/1000 | Loss: 0.00001843
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 156. Stopping optimization.
Last 5 losses: [1.843043719418347e-05, 1.843043719418347e-05, 1.843043719418347e-05, 1.843043719418347e-05, 1.843043719418347e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.843043719418347e-05

Optimization complete. Final v2v error: 3.6876237392425537 mm

Highest mean error: 9.154450416564941 mm for frame 90

Lowest mean error: 3.28017520904541 mm for frame 163

Saving results

Total time: 97.96913123130798
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_39_us_1277/0019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_39_us_1277/0019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_39_us_1277/0019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00691844
Iteration 2/25 | Loss: 0.00159900
Iteration 3/25 | Loss: 0.00133178
Iteration 4/25 | Loss: 0.00128805
Iteration 5/25 | Loss: 0.00127978
Iteration 6/25 | Loss: 0.00127722
Iteration 7/25 | Loss: 0.00127626
Iteration 8/25 | Loss: 0.00127579
Iteration 9/25 | Loss: 0.00127534
Iteration 10/25 | Loss: 0.00127510
Iteration 11/25 | Loss: 0.00127500
Iteration 12/25 | Loss: 0.00127499
Iteration 13/25 | Loss: 0.00127499
Iteration 14/25 | Loss: 0.00127499
Iteration 15/25 | Loss: 0.00127499
Iteration 16/25 | Loss: 0.00127498
Iteration 17/25 | Loss: 0.00127498
Iteration 18/25 | Loss: 0.00127498
Iteration 19/25 | Loss: 0.00127498
Iteration 20/25 | Loss: 0.00127498
Iteration 21/25 | Loss: 0.00127498
Iteration 22/25 | Loss: 0.00127498
Iteration 23/25 | Loss: 0.00127498
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0012749804882332683, 0.0012749804882332683, 0.0012749804882332683, 0.0012749804882332683, 0.0012749804882332683]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012749804882332683

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.78086865
Iteration 2/25 | Loss: 0.00143796
Iteration 3/25 | Loss: 0.00143796
Iteration 4/25 | Loss: 0.00143796
Iteration 5/25 | Loss: 0.00143795
Iteration 6/25 | Loss: 0.00143795
Iteration 7/25 | Loss: 0.00143795
Iteration 8/25 | Loss: 0.00143795
Iteration 9/25 | Loss: 0.00143795
Iteration 10/25 | Loss: 0.00143795
Iteration 11/25 | Loss: 0.00143795
Iteration 12/25 | Loss: 0.00143795
Iteration 13/25 | Loss: 0.00143795
Iteration 14/25 | Loss: 0.00143795
Iteration 15/25 | Loss: 0.00143795
Iteration 16/25 | Loss: 0.00143795
Iteration 17/25 | Loss: 0.00143795
Iteration 18/25 | Loss: 0.00143795
Iteration 19/25 | Loss: 0.00143795
Iteration 20/25 | Loss: 0.00143795
Iteration 21/25 | Loss: 0.00143795
Iteration 22/25 | Loss: 0.00143795
Iteration 23/25 | Loss: 0.00143795
Iteration 24/25 | Loss: 0.00143795
Iteration 25/25 | Loss: 0.00143795

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00143795
Iteration 2/1000 | Loss: 0.00011144
Iteration 3/1000 | Loss: 0.00006459
Iteration 4/1000 | Loss: 0.00012113
Iteration 5/1000 | Loss: 0.00004746
Iteration 6/1000 | Loss: 0.00004208
Iteration 7/1000 | Loss: 0.00003838
Iteration 8/1000 | Loss: 0.00003647
Iteration 9/1000 | Loss: 0.00003546
Iteration 10/1000 | Loss: 0.00003476
Iteration 11/1000 | Loss: 0.00003429
Iteration 12/1000 | Loss: 0.00003396
Iteration 13/1000 | Loss: 0.00003366
Iteration 14/1000 | Loss: 0.00028220
Iteration 15/1000 | Loss: 0.00003889
Iteration 16/1000 | Loss: 0.00026700
Iteration 17/1000 | Loss: 0.00016290
Iteration 18/1000 | Loss: 0.00024424
Iteration 19/1000 | Loss: 0.00003458
Iteration 20/1000 | Loss: 0.00003117
Iteration 21/1000 | Loss: 0.00002977
Iteration 22/1000 | Loss: 0.00002919
Iteration 23/1000 | Loss: 0.00002876
Iteration 24/1000 | Loss: 0.00002850
Iteration 25/1000 | Loss: 0.00032946
Iteration 26/1000 | Loss: 0.00003783
Iteration 27/1000 | Loss: 0.00002882
Iteration 28/1000 | Loss: 0.00002722
Iteration 29/1000 | Loss: 0.00002638
Iteration 30/1000 | Loss: 0.00002592
Iteration 31/1000 | Loss: 0.00002562
Iteration 32/1000 | Loss: 0.00002559
Iteration 33/1000 | Loss: 0.00002540
Iteration 34/1000 | Loss: 0.00002539
Iteration 35/1000 | Loss: 0.00002525
Iteration 36/1000 | Loss: 0.00002517
Iteration 37/1000 | Loss: 0.00002516
Iteration 38/1000 | Loss: 0.00002516
Iteration 39/1000 | Loss: 0.00002516
Iteration 40/1000 | Loss: 0.00002515
Iteration 41/1000 | Loss: 0.00002514
Iteration 42/1000 | Loss: 0.00002513
Iteration 43/1000 | Loss: 0.00002512
Iteration 44/1000 | Loss: 0.00002503
Iteration 45/1000 | Loss: 0.00002498
Iteration 46/1000 | Loss: 0.00002490
Iteration 47/1000 | Loss: 0.00002486
Iteration 48/1000 | Loss: 0.00002486
Iteration 49/1000 | Loss: 0.00002485
Iteration 50/1000 | Loss: 0.00002485
Iteration 51/1000 | Loss: 0.00002482
Iteration 52/1000 | Loss: 0.00002479
Iteration 53/1000 | Loss: 0.00002479
Iteration 54/1000 | Loss: 0.00002478
Iteration 55/1000 | Loss: 0.00002477
Iteration 56/1000 | Loss: 0.00002476
Iteration 57/1000 | Loss: 0.00002476
Iteration 58/1000 | Loss: 0.00002476
Iteration 59/1000 | Loss: 0.00002475
Iteration 60/1000 | Loss: 0.00002474
Iteration 61/1000 | Loss: 0.00002474
Iteration 62/1000 | Loss: 0.00002474
Iteration 63/1000 | Loss: 0.00002474
Iteration 64/1000 | Loss: 0.00002474
Iteration 65/1000 | Loss: 0.00002474
Iteration 66/1000 | Loss: 0.00002473
Iteration 67/1000 | Loss: 0.00002473
Iteration 68/1000 | Loss: 0.00002473
Iteration 69/1000 | Loss: 0.00002473
Iteration 70/1000 | Loss: 0.00002472
Iteration 71/1000 | Loss: 0.00002472
Iteration 72/1000 | Loss: 0.00002472
Iteration 73/1000 | Loss: 0.00002471
Iteration 74/1000 | Loss: 0.00002471
Iteration 75/1000 | Loss: 0.00002471
Iteration 76/1000 | Loss: 0.00002471
Iteration 77/1000 | Loss: 0.00002470
Iteration 78/1000 | Loss: 0.00002470
Iteration 79/1000 | Loss: 0.00002470
Iteration 80/1000 | Loss: 0.00002469
Iteration 81/1000 | Loss: 0.00002469
Iteration 82/1000 | Loss: 0.00002468
Iteration 83/1000 | Loss: 0.00002468
Iteration 84/1000 | Loss: 0.00002467
Iteration 85/1000 | Loss: 0.00002467
Iteration 86/1000 | Loss: 0.00002467
Iteration 87/1000 | Loss: 0.00002466
Iteration 88/1000 | Loss: 0.00002466
Iteration 89/1000 | Loss: 0.00002465
Iteration 90/1000 | Loss: 0.00002465
Iteration 91/1000 | Loss: 0.00002465
Iteration 92/1000 | Loss: 0.00002464
Iteration 93/1000 | Loss: 0.00002464
Iteration 94/1000 | Loss: 0.00002464
Iteration 95/1000 | Loss: 0.00002463
Iteration 96/1000 | Loss: 0.00002463
Iteration 97/1000 | Loss: 0.00002463
Iteration 98/1000 | Loss: 0.00002461
Iteration 99/1000 | Loss: 0.00002461
Iteration 100/1000 | Loss: 0.00002460
Iteration 101/1000 | Loss: 0.00002460
Iteration 102/1000 | Loss: 0.00002460
Iteration 103/1000 | Loss: 0.00002460
Iteration 104/1000 | Loss: 0.00002460
Iteration 105/1000 | Loss: 0.00002460
Iteration 106/1000 | Loss: 0.00002460
Iteration 107/1000 | Loss: 0.00002460
Iteration 108/1000 | Loss: 0.00002460
Iteration 109/1000 | Loss: 0.00002459
Iteration 110/1000 | Loss: 0.00002459
Iteration 111/1000 | Loss: 0.00002459
Iteration 112/1000 | Loss: 0.00002458
Iteration 113/1000 | Loss: 0.00002458
Iteration 114/1000 | Loss: 0.00002458
Iteration 115/1000 | Loss: 0.00002457
Iteration 116/1000 | Loss: 0.00002457
Iteration 117/1000 | Loss: 0.00002456
Iteration 118/1000 | Loss: 0.00002456
Iteration 119/1000 | Loss: 0.00002456
Iteration 120/1000 | Loss: 0.00002455
Iteration 121/1000 | Loss: 0.00002455
Iteration 122/1000 | Loss: 0.00002455
Iteration 123/1000 | Loss: 0.00002455
Iteration 124/1000 | Loss: 0.00002455
Iteration 125/1000 | Loss: 0.00002455
Iteration 126/1000 | Loss: 0.00002454
Iteration 127/1000 | Loss: 0.00002454
Iteration 128/1000 | Loss: 0.00002454
Iteration 129/1000 | Loss: 0.00002454
Iteration 130/1000 | Loss: 0.00002454
Iteration 131/1000 | Loss: 0.00002454
Iteration 132/1000 | Loss: 0.00002454
Iteration 133/1000 | Loss: 0.00002454
Iteration 134/1000 | Loss: 0.00002454
Iteration 135/1000 | Loss: 0.00002454
Iteration 136/1000 | Loss: 0.00002454
Iteration 137/1000 | Loss: 0.00002454
Iteration 138/1000 | Loss: 0.00002454
Iteration 139/1000 | Loss: 0.00002453
Iteration 140/1000 | Loss: 0.00002453
Iteration 141/1000 | Loss: 0.00002453
Iteration 142/1000 | Loss: 0.00002453
Iteration 143/1000 | Loss: 0.00002453
Iteration 144/1000 | Loss: 0.00002453
Iteration 145/1000 | Loss: 0.00002453
Iteration 146/1000 | Loss: 0.00002453
Iteration 147/1000 | Loss: 0.00002453
Iteration 148/1000 | Loss: 0.00002453
Iteration 149/1000 | Loss: 0.00002453
Iteration 150/1000 | Loss: 0.00002453
Iteration 151/1000 | Loss: 0.00002453
Iteration 152/1000 | Loss: 0.00002452
Iteration 153/1000 | Loss: 0.00002452
Iteration 154/1000 | Loss: 0.00002452
Iteration 155/1000 | Loss: 0.00002452
Iteration 156/1000 | Loss: 0.00002452
Iteration 157/1000 | Loss: 0.00002452
Iteration 158/1000 | Loss: 0.00002452
Iteration 159/1000 | Loss: 0.00002452
Iteration 160/1000 | Loss: 0.00002452
Iteration 161/1000 | Loss: 0.00002452
Iteration 162/1000 | Loss: 0.00002452
Iteration 163/1000 | Loss: 0.00002452
Iteration 164/1000 | Loss: 0.00002452
Iteration 165/1000 | Loss: 0.00002452
Iteration 166/1000 | Loss: 0.00002451
Iteration 167/1000 | Loss: 0.00002451
Iteration 168/1000 | Loss: 0.00002451
Iteration 169/1000 | Loss: 0.00002451
Iteration 170/1000 | Loss: 0.00002451
Iteration 171/1000 | Loss: 0.00002451
Iteration 172/1000 | Loss: 0.00002451
Iteration 173/1000 | Loss: 0.00002451
Iteration 174/1000 | Loss: 0.00002451
Iteration 175/1000 | Loss: 0.00002451
Iteration 176/1000 | Loss: 0.00002451
Iteration 177/1000 | Loss: 0.00002451
Iteration 178/1000 | Loss: 0.00002451
Iteration 179/1000 | Loss: 0.00002451
Iteration 180/1000 | Loss: 0.00002451
Iteration 181/1000 | Loss: 0.00002451
Iteration 182/1000 | Loss: 0.00002451
Iteration 183/1000 | Loss: 0.00002451
Iteration 184/1000 | Loss: 0.00002451
Iteration 185/1000 | Loss: 0.00002451
Iteration 186/1000 | Loss: 0.00002451
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 186. Stopping optimization.
Last 5 losses: [2.4511569790774956e-05, 2.4511569790774956e-05, 2.4511569790774956e-05, 2.4511569790774956e-05, 2.4511569790774956e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4511569790774956e-05

Optimization complete. Final v2v error: 4.055916786193848 mm

Highest mean error: 5.063880443572998 mm for frame 176

Lowest mean error: 3.413048267364502 mm for frame 68

Saving results

Total time: 88.80902886390686
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_39_us_1277/0009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_39_us_1277/0009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_39_us_1277/0009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00782684
Iteration 2/25 | Loss: 0.00132606
Iteration 3/25 | Loss: 0.00120550
Iteration 4/25 | Loss: 0.00119467
Iteration 5/25 | Loss: 0.00119215
Iteration 6/25 | Loss: 0.00119206
Iteration 7/25 | Loss: 0.00119206
Iteration 8/25 | Loss: 0.00119206
Iteration 9/25 | Loss: 0.00119206
Iteration 10/25 | Loss: 0.00119206
Iteration 11/25 | Loss: 0.00119206
Iteration 12/25 | Loss: 0.00119206
Iteration 13/25 | Loss: 0.00119206
Iteration 14/25 | Loss: 0.00119206
Iteration 15/25 | Loss: 0.00119206
Iteration 16/25 | Loss: 0.00119206
Iteration 17/25 | Loss: 0.00119206
Iteration 18/25 | Loss: 0.00119206
Iteration 19/25 | Loss: 0.00119206
Iteration 20/25 | Loss: 0.00119206
Iteration 21/25 | Loss: 0.00119206
Iteration 22/25 | Loss: 0.00119206
Iteration 23/25 | Loss: 0.00119206
Iteration 24/25 | Loss: 0.00119206
Iteration 25/25 | Loss: 0.00119206

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 16.72380829
Iteration 2/25 | Loss: 0.00104915
Iteration 3/25 | Loss: 0.00104910
Iteration 4/25 | Loss: 0.00104910
Iteration 5/25 | Loss: 0.00104910
Iteration 6/25 | Loss: 0.00104909
Iteration 7/25 | Loss: 0.00104909
Iteration 8/25 | Loss: 0.00104909
Iteration 9/25 | Loss: 0.00104909
Iteration 10/25 | Loss: 0.00104909
Iteration 11/25 | Loss: 0.00104909
Iteration 12/25 | Loss: 0.00104909
Iteration 13/25 | Loss: 0.00104909
Iteration 14/25 | Loss: 0.00104909
Iteration 15/25 | Loss: 0.00104909
Iteration 16/25 | Loss: 0.00104909
Iteration 17/25 | Loss: 0.00104909
Iteration 18/25 | Loss: 0.00104909
Iteration 19/25 | Loss: 0.00104909
Iteration 20/25 | Loss: 0.00104909
Iteration 21/25 | Loss: 0.00104909
Iteration 22/25 | Loss: 0.00104909
Iteration 23/25 | Loss: 0.00104909
Iteration 24/25 | Loss: 0.00104909
Iteration 25/25 | Loss: 0.00104909

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00104909
Iteration 2/1000 | Loss: 0.00004462
Iteration 3/1000 | Loss: 0.00002455
Iteration 4/1000 | Loss: 0.00002136
Iteration 5/1000 | Loss: 0.00002011
Iteration 6/1000 | Loss: 0.00001947
Iteration 7/1000 | Loss: 0.00001898
Iteration 8/1000 | Loss: 0.00001864
Iteration 9/1000 | Loss: 0.00001835
Iteration 10/1000 | Loss: 0.00001814
Iteration 11/1000 | Loss: 0.00001813
Iteration 12/1000 | Loss: 0.00001813
Iteration 13/1000 | Loss: 0.00001812
Iteration 14/1000 | Loss: 0.00001810
Iteration 15/1000 | Loss: 0.00001808
Iteration 16/1000 | Loss: 0.00001807
Iteration 17/1000 | Loss: 0.00001800
Iteration 18/1000 | Loss: 0.00001793
Iteration 19/1000 | Loss: 0.00001790
Iteration 20/1000 | Loss: 0.00001790
Iteration 21/1000 | Loss: 0.00001789
Iteration 22/1000 | Loss: 0.00001789
Iteration 23/1000 | Loss: 0.00001786
Iteration 24/1000 | Loss: 0.00001784
Iteration 25/1000 | Loss: 0.00001783
Iteration 26/1000 | Loss: 0.00001783
Iteration 27/1000 | Loss: 0.00001782
Iteration 28/1000 | Loss: 0.00001782
Iteration 29/1000 | Loss: 0.00001782
Iteration 30/1000 | Loss: 0.00001782
Iteration 31/1000 | Loss: 0.00001782
Iteration 32/1000 | Loss: 0.00001782
Iteration 33/1000 | Loss: 0.00001781
Iteration 34/1000 | Loss: 0.00001777
Iteration 35/1000 | Loss: 0.00001777
Iteration 36/1000 | Loss: 0.00001776
Iteration 37/1000 | Loss: 0.00001776
Iteration 38/1000 | Loss: 0.00001774
Iteration 39/1000 | Loss: 0.00001771
Iteration 40/1000 | Loss: 0.00001770
Iteration 41/1000 | Loss: 0.00001770
Iteration 42/1000 | Loss: 0.00001770
Iteration 43/1000 | Loss: 0.00001769
Iteration 44/1000 | Loss: 0.00001768
Iteration 45/1000 | Loss: 0.00001767
Iteration 46/1000 | Loss: 0.00001766
Iteration 47/1000 | Loss: 0.00001765
Iteration 48/1000 | Loss: 0.00001765
Iteration 49/1000 | Loss: 0.00001765
Iteration 50/1000 | Loss: 0.00001765
Iteration 51/1000 | Loss: 0.00001765
Iteration 52/1000 | Loss: 0.00001764
Iteration 53/1000 | Loss: 0.00001764
Iteration 54/1000 | Loss: 0.00001764
Iteration 55/1000 | Loss: 0.00001764
Iteration 56/1000 | Loss: 0.00001763
Iteration 57/1000 | Loss: 0.00001762
Iteration 58/1000 | Loss: 0.00001761
Iteration 59/1000 | Loss: 0.00001761
Iteration 60/1000 | Loss: 0.00001761
Iteration 61/1000 | Loss: 0.00001761
Iteration 62/1000 | Loss: 0.00001761
Iteration 63/1000 | Loss: 0.00001761
Iteration 64/1000 | Loss: 0.00001761
Iteration 65/1000 | Loss: 0.00001761
Iteration 66/1000 | Loss: 0.00001761
Iteration 67/1000 | Loss: 0.00001761
Iteration 68/1000 | Loss: 0.00001761
Iteration 69/1000 | Loss: 0.00001760
Iteration 70/1000 | Loss: 0.00001760
Iteration 71/1000 | Loss: 0.00001760
Iteration 72/1000 | Loss: 0.00001760
Iteration 73/1000 | Loss: 0.00001760
Iteration 74/1000 | Loss: 0.00001760
Iteration 75/1000 | Loss: 0.00001759
Iteration 76/1000 | Loss: 0.00001759
Iteration 77/1000 | Loss: 0.00001759
Iteration 78/1000 | Loss: 0.00001759
Iteration 79/1000 | Loss: 0.00001759
Iteration 80/1000 | Loss: 0.00001759
Iteration 81/1000 | Loss: 0.00001758
Iteration 82/1000 | Loss: 0.00001758
Iteration 83/1000 | Loss: 0.00001758
Iteration 84/1000 | Loss: 0.00001758
Iteration 85/1000 | Loss: 0.00001758
Iteration 86/1000 | Loss: 0.00001758
Iteration 87/1000 | Loss: 0.00001758
Iteration 88/1000 | Loss: 0.00001758
Iteration 89/1000 | Loss: 0.00001758
Iteration 90/1000 | Loss: 0.00001758
Iteration 91/1000 | Loss: 0.00001757
Iteration 92/1000 | Loss: 0.00001757
Iteration 93/1000 | Loss: 0.00001757
Iteration 94/1000 | Loss: 0.00001757
Iteration 95/1000 | Loss: 0.00001757
Iteration 96/1000 | Loss: 0.00001757
Iteration 97/1000 | Loss: 0.00001757
Iteration 98/1000 | Loss: 0.00001757
Iteration 99/1000 | Loss: 0.00001757
Iteration 100/1000 | Loss: 0.00001757
Iteration 101/1000 | Loss: 0.00001757
Iteration 102/1000 | Loss: 0.00001757
Iteration 103/1000 | Loss: 0.00001757
Iteration 104/1000 | Loss: 0.00001757
Iteration 105/1000 | Loss: 0.00001756
Iteration 106/1000 | Loss: 0.00001756
Iteration 107/1000 | Loss: 0.00001756
Iteration 108/1000 | Loss: 0.00001756
Iteration 109/1000 | Loss: 0.00001756
Iteration 110/1000 | Loss: 0.00001756
Iteration 111/1000 | Loss: 0.00001755
Iteration 112/1000 | Loss: 0.00001755
Iteration 113/1000 | Loss: 0.00001755
Iteration 114/1000 | Loss: 0.00001755
Iteration 115/1000 | Loss: 0.00001755
Iteration 116/1000 | Loss: 0.00001755
Iteration 117/1000 | Loss: 0.00001755
Iteration 118/1000 | Loss: 0.00001755
Iteration 119/1000 | Loss: 0.00001755
Iteration 120/1000 | Loss: 0.00001755
Iteration 121/1000 | Loss: 0.00001755
Iteration 122/1000 | Loss: 0.00001755
Iteration 123/1000 | Loss: 0.00001755
Iteration 124/1000 | Loss: 0.00001755
Iteration 125/1000 | Loss: 0.00001755
Iteration 126/1000 | Loss: 0.00001755
Iteration 127/1000 | Loss: 0.00001755
Iteration 128/1000 | Loss: 0.00001755
Iteration 129/1000 | Loss: 0.00001754
Iteration 130/1000 | Loss: 0.00001754
Iteration 131/1000 | Loss: 0.00001754
Iteration 132/1000 | Loss: 0.00001754
Iteration 133/1000 | Loss: 0.00001754
Iteration 134/1000 | Loss: 0.00001754
Iteration 135/1000 | Loss: 0.00001754
Iteration 136/1000 | Loss: 0.00001754
Iteration 137/1000 | Loss: 0.00001754
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 137. Stopping optimization.
Last 5 losses: [1.754438017087523e-05, 1.754438017087523e-05, 1.754438017087523e-05, 1.754438017087523e-05, 1.754438017087523e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.754438017087523e-05

Optimization complete. Final v2v error: 3.5707592964172363 mm

Highest mean error: 3.8846709728240967 mm for frame 164

Lowest mean error: 3.2252449989318848 mm for frame 175

Saving results

Total time: 38.91003966331482
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_us_1259/0023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_1259/0023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_1259/0023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00889147
Iteration 2/25 | Loss: 0.00148224
Iteration 3/25 | Loss: 0.00130362
Iteration 4/25 | Loss: 0.00126455
Iteration 5/25 | Loss: 0.00126081
Iteration 6/25 | Loss: 0.00125301
Iteration 7/25 | Loss: 0.00125082
Iteration 8/25 | Loss: 0.00124972
Iteration 9/25 | Loss: 0.00124885
Iteration 10/25 | Loss: 0.00124833
Iteration 11/25 | Loss: 0.00124807
Iteration 12/25 | Loss: 0.00125145
Iteration 13/25 | Loss: 0.00124964
Iteration 14/25 | Loss: 0.00124772
Iteration 15/25 | Loss: 0.00124511
Iteration 16/25 | Loss: 0.00124476
Iteration 17/25 | Loss: 0.00124472
Iteration 18/25 | Loss: 0.00124471
Iteration 19/25 | Loss: 0.00124471
Iteration 20/25 | Loss: 0.00124471
Iteration 21/25 | Loss: 0.00124471
Iteration 22/25 | Loss: 0.00124471
Iteration 23/25 | Loss: 0.00124471
Iteration 24/25 | Loss: 0.00124471
Iteration 25/25 | Loss: 0.00124471

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.85298681
Iteration 2/25 | Loss: 0.00238841
Iteration 3/25 | Loss: 0.00238841
Iteration 4/25 | Loss: 0.00238841
Iteration 5/25 | Loss: 0.00238841
Iteration 6/25 | Loss: 0.00238841
Iteration 7/25 | Loss: 0.00238841
Iteration 8/25 | Loss: 0.00238841
Iteration 9/25 | Loss: 0.00238841
Iteration 10/25 | Loss: 0.00238841
Iteration 11/25 | Loss: 0.00238841
Iteration 12/25 | Loss: 0.00238841
Iteration 13/25 | Loss: 0.00238841
Iteration 14/25 | Loss: 0.00238841
Iteration 15/25 | Loss: 0.00238841
Iteration 16/25 | Loss: 0.00238841
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.002388407476246357, 0.002388407476246357, 0.002388407476246357, 0.002388407476246357, 0.002388407476246357]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002388407476246357

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00238841
Iteration 2/1000 | Loss: 0.00003634
Iteration 3/1000 | Loss: 0.00002693
Iteration 4/1000 | Loss: 0.00002437
Iteration 5/1000 | Loss: 0.00002341
Iteration 6/1000 | Loss: 0.00002278
Iteration 7/1000 | Loss: 0.00002234
Iteration 8/1000 | Loss: 0.00002202
Iteration 9/1000 | Loss: 0.00002178
Iteration 10/1000 | Loss: 0.00002173
Iteration 11/1000 | Loss: 0.00002163
Iteration 12/1000 | Loss: 0.00002160
Iteration 13/1000 | Loss: 0.00002159
Iteration 14/1000 | Loss: 0.00002158
Iteration 15/1000 | Loss: 0.00002158
Iteration 16/1000 | Loss: 0.00002154
Iteration 17/1000 | Loss: 0.00002153
Iteration 18/1000 | Loss: 0.00002153
Iteration 19/1000 | Loss: 0.00002151
Iteration 20/1000 | Loss: 0.00002151
Iteration 21/1000 | Loss: 0.00002150
Iteration 22/1000 | Loss: 0.00002150
Iteration 23/1000 | Loss: 0.00002149
Iteration 24/1000 | Loss: 0.00002149
Iteration 25/1000 | Loss: 0.00002148
Iteration 26/1000 | Loss: 0.00002148
Iteration 27/1000 | Loss: 0.00002148
Iteration 28/1000 | Loss: 0.00002148
Iteration 29/1000 | Loss: 0.00002148
Iteration 30/1000 | Loss: 0.00002147
Iteration 31/1000 | Loss: 0.00002147
Iteration 32/1000 | Loss: 0.00002147
Iteration 33/1000 | Loss: 0.00002147
Iteration 34/1000 | Loss: 0.00002147
Iteration 35/1000 | Loss: 0.00002147
Iteration 36/1000 | Loss: 0.00002147
Iteration 37/1000 | Loss: 0.00002146
Iteration 38/1000 | Loss: 0.00002146
Iteration 39/1000 | Loss: 0.00002146
Iteration 40/1000 | Loss: 0.00002146
Iteration 41/1000 | Loss: 0.00002146
Iteration 42/1000 | Loss: 0.00002146
Iteration 43/1000 | Loss: 0.00002146
Iteration 44/1000 | Loss: 0.00002146
Iteration 45/1000 | Loss: 0.00002145
Iteration 46/1000 | Loss: 0.00002145
Iteration 47/1000 | Loss: 0.00002144
Iteration 48/1000 | Loss: 0.00002144
Iteration 49/1000 | Loss: 0.00002144
Iteration 50/1000 | Loss: 0.00002143
Iteration 51/1000 | Loss: 0.00002143
Iteration 52/1000 | Loss: 0.00002142
Iteration 53/1000 | Loss: 0.00002142
Iteration 54/1000 | Loss: 0.00002142
Iteration 55/1000 | Loss: 0.00002142
Iteration 56/1000 | Loss: 0.00002141
Iteration 57/1000 | Loss: 0.00002141
Iteration 58/1000 | Loss: 0.00002141
Iteration 59/1000 | Loss: 0.00002141
Iteration 60/1000 | Loss: 0.00002140
Iteration 61/1000 | Loss: 0.00002140
Iteration 62/1000 | Loss: 0.00002140
Iteration 63/1000 | Loss: 0.00002139
Iteration 64/1000 | Loss: 0.00002139
Iteration 65/1000 | Loss: 0.00002138
Iteration 66/1000 | Loss: 0.00002138
Iteration 67/1000 | Loss: 0.00002138
Iteration 68/1000 | Loss: 0.00002138
Iteration 69/1000 | Loss: 0.00002137
Iteration 70/1000 | Loss: 0.00002137
Iteration 71/1000 | Loss: 0.00002137
Iteration 72/1000 | Loss: 0.00002137
Iteration 73/1000 | Loss: 0.00002136
Iteration 74/1000 | Loss: 0.00002136
Iteration 75/1000 | Loss: 0.00002136
Iteration 76/1000 | Loss: 0.00002135
Iteration 77/1000 | Loss: 0.00002135
Iteration 78/1000 | Loss: 0.00002135
Iteration 79/1000 | Loss: 0.00002135
Iteration 80/1000 | Loss: 0.00002135
Iteration 81/1000 | Loss: 0.00002135
Iteration 82/1000 | Loss: 0.00002135
Iteration 83/1000 | Loss: 0.00002135
Iteration 84/1000 | Loss: 0.00002135
Iteration 85/1000 | Loss: 0.00002135
Iteration 86/1000 | Loss: 0.00002135
Iteration 87/1000 | Loss: 0.00002134
Iteration 88/1000 | Loss: 0.00002134
Iteration 89/1000 | Loss: 0.00002134
Iteration 90/1000 | Loss: 0.00002133
Iteration 91/1000 | Loss: 0.00002133
Iteration 92/1000 | Loss: 0.00002133
Iteration 93/1000 | Loss: 0.00002133
Iteration 94/1000 | Loss: 0.00002133
Iteration 95/1000 | Loss: 0.00002133
Iteration 96/1000 | Loss: 0.00002133
Iteration 97/1000 | Loss: 0.00002133
Iteration 98/1000 | Loss: 0.00002133
Iteration 99/1000 | Loss: 0.00002133
Iteration 100/1000 | Loss: 0.00002133
Iteration 101/1000 | Loss: 0.00002133
Iteration 102/1000 | Loss: 0.00002133
Iteration 103/1000 | Loss: 0.00002133
Iteration 104/1000 | Loss: 0.00002133
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 104. Stopping optimization.
Last 5 losses: [2.132868758053519e-05, 2.132868758053519e-05, 2.132868758053519e-05, 2.132868758053519e-05, 2.132868758053519e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.132868758053519e-05

Optimization complete. Final v2v error: 3.9927000999450684 mm

Highest mean error: 9.935639381408691 mm for frame 33

Lowest mean error: 3.5212337970733643 mm for frame 190

Saving results

Total time: 56.09590482711792
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_us_1259/0022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_1259/0022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_1259/0022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00565019
Iteration 2/25 | Loss: 0.00146026
Iteration 3/25 | Loss: 0.00134580
Iteration 4/25 | Loss: 0.00131339
Iteration 5/25 | Loss: 0.00130852
Iteration 6/25 | Loss: 0.00130763
Iteration 7/25 | Loss: 0.00130763
Iteration 8/25 | Loss: 0.00130763
Iteration 9/25 | Loss: 0.00130763
Iteration 10/25 | Loss: 0.00130763
Iteration 11/25 | Loss: 0.00130763
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013076327741146088, 0.0013076327741146088, 0.0013076327741146088, 0.0013076327741146088, 0.0013076327741146088]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013076327741146088

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.02434874
Iteration 2/25 | Loss: 0.00249862
Iteration 3/25 | Loss: 0.00249861
Iteration 4/25 | Loss: 0.00249861
Iteration 5/25 | Loss: 0.00249861
Iteration 6/25 | Loss: 0.00249861
Iteration 7/25 | Loss: 0.00249861
Iteration 8/25 | Loss: 0.00249861
Iteration 9/25 | Loss: 0.00249861
Iteration 10/25 | Loss: 0.00249860
Iteration 11/25 | Loss: 0.00249860
Iteration 12/25 | Loss: 0.00249860
Iteration 13/25 | Loss: 0.00249860
Iteration 14/25 | Loss: 0.00249860
Iteration 15/25 | Loss: 0.00249860
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0024986048229038715, 0.0024986048229038715, 0.0024986048229038715, 0.0024986048229038715, 0.0024986048229038715]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0024986048229038715

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00249860
Iteration 2/1000 | Loss: 0.00005654
Iteration 3/1000 | Loss: 0.00004001
Iteration 4/1000 | Loss: 0.00003603
Iteration 5/1000 | Loss: 0.00003437
Iteration 6/1000 | Loss: 0.00003269
Iteration 7/1000 | Loss: 0.00003167
Iteration 8/1000 | Loss: 0.00003094
Iteration 9/1000 | Loss: 0.00003047
Iteration 10/1000 | Loss: 0.00003020
Iteration 11/1000 | Loss: 0.00003001
Iteration 12/1000 | Loss: 0.00002991
Iteration 13/1000 | Loss: 0.00002990
Iteration 14/1000 | Loss: 0.00002987
Iteration 15/1000 | Loss: 0.00002986
Iteration 16/1000 | Loss: 0.00002986
Iteration 17/1000 | Loss: 0.00002986
Iteration 18/1000 | Loss: 0.00002986
Iteration 19/1000 | Loss: 0.00002985
Iteration 20/1000 | Loss: 0.00002985
Iteration 21/1000 | Loss: 0.00002983
Iteration 22/1000 | Loss: 0.00002983
Iteration 23/1000 | Loss: 0.00002983
Iteration 24/1000 | Loss: 0.00002982
Iteration 25/1000 | Loss: 0.00002982
Iteration 26/1000 | Loss: 0.00002982
Iteration 27/1000 | Loss: 0.00002982
Iteration 28/1000 | Loss: 0.00002981
Iteration 29/1000 | Loss: 0.00002981
Iteration 30/1000 | Loss: 0.00002980
Iteration 31/1000 | Loss: 0.00002980
Iteration 32/1000 | Loss: 0.00002979
Iteration 33/1000 | Loss: 0.00002979
Iteration 34/1000 | Loss: 0.00002979
Iteration 35/1000 | Loss: 0.00002978
Iteration 36/1000 | Loss: 0.00002977
Iteration 37/1000 | Loss: 0.00002977
Iteration 38/1000 | Loss: 0.00002977
Iteration 39/1000 | Loss: 0.00002976
Iteration 40/1000 | Loss: 0.00002976
Iteration 41/1000 | Loss: 0.00002976
Iteration 42/1000 | Loss: 0.00002976
Iteration 43/1000 | Loss: 0.00002975
Iteration 44/1000 | Loss: 0.00002975
Iteration 45/1000 | Loss: 0.00002974
Iteration 46/1000 | Loss: 0.00002973
Iteration 47/1000 | Loss: 0.00002973
Iteration 48/1000 | Loss: 0.00002973
Iteration 49/1000 | Loss: 0.00002973
Iteration 50/1000 | Loss: 0.00002973
Iteration 51/1000 | Loss: 0.00002973
Iteration 52/1000 | Loss: 0.00002973
Iteration 53/1000 | Loss: 0.00002973
Iteration 54/1000 | Loss: 0.00002973
Iteration 55/1000 | Loss: 0.00002973
Iteration 56/1000 | Loss: 0.00002973
Iteration 57/1000 | Loss: 0.00002973
Iteration 58/1000 | Loss: 0.00002973
Iteration 59/1000 | Loss: 0.00002973
Iteration 60/1000 | Loss: 0.00002970
Iteration 61/1000 | Loss: 0.00002970
Iteration 62/1000 | Loss: 0.00002970
Iteration 63/1000 | Loss: 0.00002970
Iteration 64/1000 | Loss: 0.00002969
Iteration 65/1000 | Loss: 0.00002969
Iteration 66/1000 | Loss: 0.00002969
Iteration 67/1000 | Loss: 0.00002969
Iteration 68/1000 | Loss: 0.00002969
Iteration 69/1000 | Loss: 0.00002968
Iteration 70/1000 | Loss: 0.00002968
Iteration 71/1000 | Loss: 0.00002968
Iteration 72/1000 | Loss: 0.00002967
Iteration 73/1000 | Loss: 0.00002967
Iteration 74/1000 | Loss: 0.00002967
Iteration 75/1000 | Loss: 0.00002967
Iteration 76/1000 | Loss: 0.00002967
Iteration 77/1000 | Loss: 0.00002967
Iteration 78/1000 | Loss: 0.00002967
Iteration 79/1000 | Loss: 0.00002966
Iteration 80/1000 | Loss: 0.00002966
Iteration 81/1000 | Loss: 0.00002966
Iteration 82/1000 | Loss: 0.00002966
Iteration 83/1000 | Loss: 0.00002966
Iteration 84/1000 | Loss: 0.00002966
Iteration 85/1000 | Loss: 0.00002966
Iteration 86/1000 | Loss: 0.00002966
Iteration 87/1000 | Loss: 0.00002966
Iteration 88/1000 | Loss: 0.00002966
Iteration 89/1000 | Loss: 0.00002966
Iteration 90/1000 | Loss: 0.00002966
Iteration 91/1000 | Loss: 0.00002966
Iteration 92/1000 | Loss: 0.00002965
Iteration 93/1000 | Loss: 0.00002965
Iteration 94/1000 | Loss: 0.00002965
Iteration 95/1000 | Loss: 0.00002965
Iteration 96/1000 | Loss: 0.00002965
Iteration 97/1000 | Loss: 0.00002965
Iteration 98/1000 | Loss: 0.00002964
Iteration 99/1000 | Loss: 0.00002964
Iteration 100/1000 | Loss: 0.00002964
Iteration 101/1000 | Loss: 0.00002964
Iteration 102/1000 | Loss: 0.00002964
Iteration 103/1000 | Loss: 0.00002964
Iteration 104/1000 | Loss: 0.00002964
Iteration 105/1000 | Loss: 0.00002964
Iteration 106/1000 | Loss: 0.00002964
Iteration 107/1000 | Loss: 0.00002964
Iteration 108/1000 | Loss: 0.00002964
Iteration 109/1000 | Loss: 0.00002963
Iteration 110/1000 | Loss: 0.00002963
Iteration 111/1000 | Loss: 0.00002963
Iteration 112/1000 | Loss: 0.00002963
Iteration 113/1000 | Loss: 0.00002963
Iteration 114/1000 | Loss: 0.00002962
Iteration 115/1000 | Loss: 0.00002962
Iteration 116/1000 | Loss: 0.00002962
Iteration 117/1000 | Loss: 0.00002962
Iteration 118/1000 | Loss: 0.00002962
Iteration 119/1000 | Loss: 0.00002962
Iteration 120/1000 | Loss: 0.00002962
Iteration 121/1000 | Loss: 0.00002962
Iteration 122/1000 | Loss: 0.00002961
Iteration 123/1000 | Loss: 0.00002961
Iteration 124/1000 | Loss: 0.00002961
Iteration 125/1000 | Loss: 0.00002961
Iteration 126/1000 | Loss: 0.00002961
Iteration 127/1000 | Loss: 0.00002960
Iteration 128/1000 | Loss: 0.00002960
Iteration 129/1000 | Loss: 0.00002960
Iteration 130/1000 | Loss: 0.00002960
Iteration 131/1000 | Loss: 0.00002960
Iteration 132/1000 | Loss: 0.00002960
Iteration 133/1000 | Loss: 0.00002960
Iteration 134/1000 | Loss: 0.00002960
Iteration 135/1000 | Loss: 0.00002960
Iteration 136/1000 | Loss: 0.00002960
Iteration 137/1000 | Loss: 0.00002960
Iteration 138/1000 | Loss: 0.00002960
Iteration 139/1000 | Loss: 0.00002960
Iteration 140/1000 | Loss: 0.00002960
Iteration 141/1000 | Loss: 0.00002960
Iteration 142/1000 | Loss: 0.00002960
Iteration 143/1000 | Loss: 0.00002960
Iteration 144/1000 | Loss: 0.00002960
Iteration 145/1000 | Loss: 0.00002960
Iteration 146/1000 | Loss: 0.00002960
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 146. Stopping optimization.
Last 5 losses: [2.9604387236759067e-05, 2.9604387236759067e-05, 2.9604387236759067e-05, 2.9604387236759067e-05, 2.9604387236759067e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.9604387236759067e-05

Optimization complete. Final v2v error: 4.567123889923096 mm

Highest mean error: 4.954109191894531 mm for frame 95

Lowest mean error: 3.969114303588867 mm for frame 139

Saving results

Total time: 38.67430853843689
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_us_1259/0021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_1259/0021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_1259/0021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01141734
Iteration 2/25 | Loss: 0.00210727
Iteration 3/25 | Loss: 0.00192170
Iteration 4/25 | Loss: 0.00154469
Iteration 5/25 | Loss: 0.00146252
Iteration 6/25 | Loss: 0.00140628
Iteration 7/25 | Loss: 0.00144162
Iteration 8/25 | Loss: 0.00137478
Iteration 9/25 | Loss: 0.00138383
Iteration 10/25 | Loss: 0.00128599
Iteration 11/25 | Loss: 0.00126726
Iteration 12/25 | Loss: 0.00126315
Iteration 13/25 | Loss: 0.00126004
Iteration 14/25 | Loss: 0.00125542
Iteration 15/25 | Loss: 0.00125441
Iteration 16/25 | Loss: 0.00125680
Iteration 17/25 | Loss: 0.00125524
Iteration 18/25 | Loss: 0.00125526
Iteration 19/25 | Loss: 0.00125581
Iteration 20/25 | Loss: 0.00125630
Iteration 21/25 | Loss: 0.00125294
Iteration 22/25 | Loss: 0.00125677
Iteration 23/25 | Loss: 0.00125500
Iteration 24/25 | Loss: 0.00125617
Iteration 25/25 | Loss: 0.00125454

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.66067600
Iteration 2/25 | Loss: 0.00290340
Iteration 3/25 | Loss: 0.00290339
Iteration 4/25 | Loss: 0.00290339
Iteration 5/25 | Loss: 0.00290339
Iteration 6/25 | Loss: 0.00290339
Iteration 7/25 | Loss: 0.00290339
Iteration 8/25 | Loss: 0.00290339
Iteration 9/25 | Loss: 0.00290339
Iteration 10/25 | Loss: 0.00290339
Iteration 11/25 | Loss: 0.00290339
Iteration 12/25 | Loss: 0.00290339
Iteration 13/25 | Loss: 0.00290339
Iteration 14/25 | Loss: 0.00290339
Iteration 15/25 | Loss: 0.00290339
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0029033932369202375, 0.0029033932369202375, 0.0029033932369202375, 0.0029033932369202375, 0.0029033932369202375]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0029033932369202375

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00290339
Iteration 2/1000 | Loss: 0.00364984
Iteration 3/1000 | Loss: 0.00465974
Iteration 4/1000 | Loss: 0.00286636
Iteration 5/1000 | Loss: 0.00168625
Iteration 6/1000 | Loss: 0.00092626
Iteration 7/1000 | Loss: 0.00216149
Iteration 8/1000 | Loss: 0.00171742
Iteration 9/1000 | Loss: 0.00031557
Iteration 10/1000 | Loss: 0.00113390
Iteration 11/1000 | Loss: 0.00228609
Iteration 12/1000 | Loss: 0.00138096
Iteration 13/1000 | Loss: 0.00038404
Iteration 14/1000 | Loss: 0.00010781
Iteration 15/1000 | Loss: 0.00152826
Iteration 16/1000 | Loss: 0.00044848
Iteration 17/1000 | Loss: 0.00073646
Iteration 18/1000 | Loss: 0.00137195
Iteration 19/1000 | Loss: 0.00063247
Iteration 20/1000 | Loss: 0.00063509
Iteration 21/1000 | Loss: 0.00156462
Iteration 22/1000 | Loss: 0.00024879
Iteration 23/1000 | Loss: 0.00031548
Iteration 24/1000 | Loss: 0.00095264
Iteration 25/1000 | Loss: 0.00038049
Iteration 26/1000 | Loss: 0.00004881
Iteration 27/1000 | Loss: 0.00066373
Iteration 28/1000 | Loss: 0.00061501
Iteration 29/1000 | Loss: 0.00061748
Iteration 30/1000 | Loss: 0.00029761
Iteration 31/1000 | Loss: 0.00027364
Iteration 32/1000 | Loss: 0.00015027
Iteration 33/1000 | Loss: 0.00016431
Iteration 34/1000 | Loss: 0.00008613
Iteration 35/1000 | Loss: 0.00009410
Iteration 36/1000 | Loss: 0.00020075
Iteration 37/1000 | Loss: 0.00059743
Iteration 38/1000 | Loss: 0.00012211
Iteration 39/1000 | Loss: 0.00006690
Iteration 40/1000 | Loss: 0.00014939
Iteration 41/1000 | Loss: 0.00005649
Iteration 42/1000 | Loss: 0.00026019
Iteration 43/1000 | Loss: 0.00007606
Iteration 44/1000 | Loss: 0.00034685
Iteration 45/1000 | Loss: 0.00010265
Iteration 46/1000 | Loss: 0.00017619
Iteration 47/1000 | Loss: 0.00010122
Iteration 48/1000 | Loss: 0.00002969
Iteration 49/1000 | Loss: 0.00002493
Iteration 50/1000 | Loss: 0.00002339
Iteration 51/1000 | Loss: 0.00002279
Iteration 52/1000 | Loss: 0.00002247
Iteration 53/1000 | Loss: 0.00002220
Iteration 54/1000 | Loss: 0.00002191
Iteration 55/1000 | Loss: 0.00002161
Iteration 56/1000 | Loss: 0.00021090
Iteration 57/1000 | Loss: 0.00023212
Iteration 58/1000 | Loss: 0.00026227
Iteration 59/1000 | Loss: 0.00021628
Iteration 60/1000 | Loss: 0.00020413
Iteration 61/1000 | Loss: 0.00010856
Iteration 62/1000 | Loss: 0.00055522
Iteration 63/1000 | Loss: 0.00012378
Iteration 64/1000 | Loss: 0.00026546
Iteration 65/1000 | Loss: 0.00002825
Iteration 66/1000 | Loss: 0.00030438
Iteration 67/1000 | Loss: 0.00002815
Iteration 68/1000 | Loss: 0.00002536
Iteration 69/1000 | Loss: 0.00029294
Iteration 70/1000 | Loss: 0.00003381
Iteration 71/1000 | Loss: 0.00002575
Iteration 72/1000 | Loss: 0.00002629
Iteration 73/1000 | Loss: 0.00002284
Iteration 74/1000 | Loss: 0.00002173
Iteration 75/1000 | Loss: 0.00002120
Iteration 76/1000 | Loss: 0.00002091
Iteration 77/1000 | Loss: 0.00002072
Iteration 78/1000 | Loss: 0.00002070
Iteration 79/1000 | Loss: 0.00002069
Iteration 80/1000 | Loss: 0.00002068
Iteration 81/1000 | Loss: 0.00002067
Iteration 82/1000 | Loss: 0.00002067
Iteration 83/1000 | Loss: 0.00002065
Iteration 84/1000 | Loss: 0.00002064
Iteration 85/1000 | Loss: 0.00002064
Iteration 86/1000 | Loss: 0.00002062
Iteration 87/1000 | Loss: 0.00002061
Iteration 88/1000 | Loss: 0.00002060
Iteration 89/1000 | Loss: 0.00002060
Iteration 90/1000 | Loss: 0.00002059
Iteration 91/1000 | Loss: 0.00002059
Iteration 92/1000 | Loss: 0.00002059
Iteration 93/1000 | Loss: 0.00002059
Iteration 94/1000 | Loss: 0.00002059
Iteration 95/1000 | Loss: 0.00002059
Iteration 96/1000 | Loss: 0.00002059
Iteration 97/1000 | Loss: 0.00002059
Iteration 98/1000 | Loss: 0.00002059
Iteration 99/1000 | Loss: 0.00002058
Iteration 100/1000 | Loss: 0.00002058
Iteration 101/1000 | Loss: 0.00002058
Iteration 102/1000 | Loss: 0.00002058
Iteration 103/1000 | Loss: 0.00002058
Iteration 104/1000 | Loss: 0.00002057
Iteration 105/1000 | Loss: 0.00002057
Iteration 106/1000 | Loss: 0.00002057
Iteration 107/1000 | Loss: 0.00002057
Iteration 108/1000 | Loss: 0.00002056
Iteration 109/1000 | Loss: 0.00002056
Iteration 110/1000 | Loss: 0.00002056
Iteration 111/1000 | Loss: 0.00002055
Iteration 112/1000 | Loss: 0.00002054
Iteration 113/1000 | Loss: 0.00002054
Iteration 114/1000 | Loss: 0.00002052
Iteration 115/1000 | Loss: 0.00002052
Iteration 116/1000 | Loss: 0.00002050
Iteration 117/1000 | Loss: 0.00002050
Iteration 118/1000 | Loss: 0.00002050
Iteration 119/1000 | Loss: 0.00002050
Iteration 120/1000 | Loss: 0.00002050
Iteration 121/1000 | Loss: 0.00002049
Iteration 122/1000 | Loss: 0.00002049
Iteration 123/1000 | Loss: 0.00002049
Iteration 124/1000 | Loss: 0.00002047
Iteration 125/1000 | Loss: 0.00002047
Iteration 126/1000 | Loss: 0.00002046
Iteration 127/1000 | Loss: 0.00002046
Iteration 128/1000 | Loss: 0.00002045
Iteration 129/1000 | Loss: 0.00002045
Iteration 130/1000 | Loss: 0.00002045
Iteration 131/1000 | Loss: 0.00002045
Iteration 132/1000 | Loss: 0.00002045
Iteration 133/1000 | Loss: 0.00002045
Iteration 134/1000 | Loss: 0.00002044
Iteration 135/1000 | Loss: 0.00002044
Iteration 136/1000 | Loss: 0.00002044
Iteration 137/1000 | Loss: 0.00002044
Iteration 138/1000 | Loss: 0.00002044
Iteration 139/1000 | Loss: 0.00002043
Iteration 140/1000 | Loss: 0.00002043
Iteration 141/1000 | Loss: 0.00002043
Iteration 142/1000 | Loss: 0.00002043
Iteration 143/1000 | Loss: 0.00002043
Iteration 144/1000 | Loss: 0.00002042
Iteration 145/1000 | Loss: 0.00002042
Iteration 146/1000 | Loss: 0.00002042
Iteration 147/1000 | Loss: 0.00002042
Iteration 148/1000 | Loss: 0.00002042
Iteration 149/1000 | Loss: 0.00002041
Iteration 150/1000 | Loss: 0.00002041
Iteration 151/1000 | Loss: 0.00002041
Iteration 152/1000 | Loss: 0.00002041
Iteration 153/1000 | Loss: 0.00002041
Iteration 154/1000 | Loss: 0.00002041
Iteration 155/1000 | Loss: 0.00002041
Iteration 156/1000 | Loss: 0.00002041
Iteration 157/1000 | Loss: 0.00002041
Iteration 158/1000 | Loss: 0.00002040
Iteration 159/1000 | Loss: 0.00002040
Iteration 160/1000 | Loss: 0.00002040
Iteration 161/1000 | Loss: 0.00002040
Iteration 162/1000 | Loss: 0.00002040
Iteration 163/1000 | Loss: 0.00002040
Iteration 164/1000 | Loss: 0.00002040
Iteration 165/1000 | Loss: 0.00002039
Iteration 166/1000 | Loss: 0.00002039
Iteration 167/1000 | Loss: 0.00002038
Iteration 168/1000 | Loss: 0.00002038
Iteration 169/1000 | Loss: 0.00002038
Iteration 170/1000 | Loss: 0.00002038
Iteration 171/1000 | Loss: 0.00002038
Iteration 172/1000 | Loss: 0.00002038
Iteration 173/1000 | Loss: 0.00002037
Iteration 174/1000 | Loss: 0.00002037
Iteration 175/1000 | Loss: 0.00002037
Iteration 176/1000 | Loss: 0.00002037
Iteration 177/1000 | Loss: 0.00002037
Iteration 178/1000 | Loss: 0.00002037
Iteration 179/1000 | Loss: 0.00002037
Iteration 180/1000 | Loss: 0.00002037
Iteration 181/1000 | Loss: 0.00002037
Iteration 182/1000 | Loss: 0.00002037
Iteration 183/1000 | Loss: 0.00002037
Iteration 184/1000 | Loss: 0.00002037
Iteration 185/1000 | Loss: 0.00002037
Iteration 186/1000 | Loss: 0.00002036
Iteration 187/1000 | Loss: 0.00002036
Iteration 188/1000 | Loss: 0.00002036
Iteration 189/1000 | Loss: 0.00002036
Iteration 190/1000 | Loss: 0.00002035
Iteration 191/1000 | Loss: 0.00002035
Iteration 192/1000 | Loss: 0.00002035
Iteration 193/1000 | Loss: 0.00002035
Iteration 194/1000 | Loss: 0.00002035
Iteration 195/1000 | Loss: 0.00002035
Iteration 196/1000 | Loss: 0.00002034
Iteration 197/1000 | Loss: 0.00002034
Iteration 198/1000 | Loss: 0.00002034
Iteration 199/1000 | Loss: 0.00002034
Iteration 200/1000 | Loss: 0.00002033
Iteration 201/1000 | Loss: 0.00002033
Iteration 202/1000 | Loss: 0.00002033
Iteration 203/1000 | Loss: 0.00002032
Iteration 204/1000 | Loss: 0.00002032
Iteration 205/1000 | Loss: 0.00002032
Iteration 206/1000 | Loss: 0.00002032
Iteration 207/1000 | Loss: 0.00002031
Iteration 208/1000 | Loss: 0.00002031
Iteration 209/1000 | Loss: 0.00002031
Iteration 210/1000 | Loss: 0.00002030
Iteration 211/1000 | Loss: 0.00002030
Iteration 212/1000 | Loss: 0.00002030
Iteration 213/1000 | Loss: 0.00002030
Iteration 214/1000 | Loss: 0.00002030
Iteration 215/1000 | Loss: 0.00002029
Iteration 216/1000 | Loss: 0.00002029
Iteration 217/1000 | Loss: 0.00002029
Iteration 218/1000 | Loss: 0.00002029
Iteration 219/1000 | Loss: 0.00002029
Iteration 220/1000 | Loss: 0.00002029
Iteration 221/1000 | Loss: 0.00002028
Iteration 222/1000 | Loss: 0.00002028
Iteration 223/1000 | Loss: 0.00002028
Iteration 224/1000 | Loss: 0.00002028
Iteration 225/1000 | Loss: 0.00002028
Iteration 226/1000 | Loss: 0.00002028
Iteration 227/1000 | Loss: 0.00002027
Iteration 228/1000 | Loss: 0.00002027
Iteration 229/1000 | Loss: 0.00002027
Iteration 230/1000 | Loss: 0.00002027
Iteration 231/1000 | Loss: 0.00002027
Iteration 232/1000 | Loss: 0.00002026
Iteration 233/1000 | Loss: 0.00002026
Iteration 234/1000 | Loss: 0.00002025
Iteration 235/1000 | Loss: 0.00002025
Iteration 236/1000 | Loss: 0.00002025
Iteration 237/1000 | Loss: 0.00002024
Iteration 238/1000 | Loss: 0.00002024
Iteration 239/1000 | Loss: 0.00002024
Iteration 240/1000 | Loss: 0.00002024
Iteration 241/1000 | Loss: 0.00002023
Iteration 242/1000 | Loss: 0.00002022
Iteration 243/1000 | Loss: 0.00002017
Iteration 244/1000 | Loss: 0.00002007
Iteration 245/1000 | Loss: 0.00001999
Iteration 246/1000 | Loss: 0.00001980
Iteration 247/1000 | Loss: 0.00001955
Iteration 248/1000 | Loss: 0.00001941
Iteration 249/1000 | Loss: 0.00001938
Iteration 250/1000 | Loss: 0.00001930
Iteration 251/1000 | Loss: 0.00001930
Iteration 252/1000 | Loss: 0.00001928
Iteration 253/1000 | Loss: 0.00001927
Iteration 254/1000 | Loss: 0.00001927
Iteration 255/1000 | Loss: 0.00001926
Iteration 256/1000 | Loss: 0.00001925
Iteration 257/1000 | Loss: 0.00001925
Iteration 258/1000 | Loss: 0.00001922
Iteration 259/1000 | Loss: 0.00001922
Iteration 260/1000 | Loss: 0.00001922
Iteration 261/1000 | Loss: 0.00001922
Iteration 262/1000 | Loss: 0.00001922
Iteration 263/1000 | Loss: 0.00001922
Iteration 264/1000 | Loss: 0.00001922
Iteration 265/1000 | Loss: 0.00001922
Iteration 266/1000 | Loss: 0.00001922
Iteration 267/1000 | Loss: 0.00001922
Iteration 268/1000 | Loss: 0.00001922
Iteration 269/1000 | Loss: 0.00001921
Iteration 270/1000 | Loss: 0.00001921
Iteration 271/1000 | Loss: 0.00001920
Iteration 272/1000 | Loss: 0.00001920
Iteration 273/1000 | Loss: 0.00001919
Iteration 274/1000 | Loss: 0.00001919
Iteration 275/1000 | Loss: 0.00001919
Iteration 276/1000 | Loss: 0.00001919
Iteration 277/1000 | Loss: 0.00001918
Iteration 278/1000 | Loss: 0.00001918
Iteration 279/1000 | Loss: 0.00001918
Iteration 280/1000 | Loss: 0.00001918
Iteration 281/1000 | Loss: 0.00001918
Iteration 282/1000 | Loss: 0.00001918
Iteration 283/1000 | Loss: 0.00001917
Iteration 284/1000 | Loss: 0.00001917
Iteration 285/1000 | Loss: 0.00001917
Iteration 286/1000 | Loss: 0.00001917
Iteration 287/1000 | Loss: 0.00001917
Iteration 288/1000 | Loss: 0.00001916
Iteration 289/1000 | Loss: 0.00001916
Iteration 290/1000 | Loss: 0.00001916
Iteration 291/1000 | Loss: 0.00001916
Iteration 292/1000 | Loss: 0.00001916
Iteration 293/1000 | Loss: 0.00001916
Iteration 294/1000 | Loss: 0.00001916
Iteration 295/1000 | Loss: 0.00001916
Iteration 296/1000 | Loss: 0.00001916
Iteration 297/1000 | Loss: 0.00001916
Iteration 298/1000 | Loss: 0.00001916
Iteration 299/1000 | Loss: 0.00001916
Iteration 300/1000 | Loss: 0.00001916
Iteration 301/1000 | Loss: 0.00001916
Iteration 302/1000 | Loss: 0.00001916
Iteration 303/1000 | Loss: 0.00001916
Iteration 304/1000 | Loss: 0.00001916
Iteration 305/1000 | Loss: 0.00001916
Iteration 306/1000 | Loss: 0.00001915
Iteration 307/1000 | Loss: 0.00001915
Iteration 308/1000 | Loss: 0.00001915
Iteration 309/1000 | Loss: 0.00001915
Iteration 310/1000 | Loss: 0.00001915
Iteration 311/1000 | Loss: 0.00001915
Iteration 312/1000 | Loss: 0.00001915
Iteration 313/1000 | Loss: 0.00001915
Iteration 314/1000 | Loss: 0.00001915
Iteration 315/1000 | Loss: 0.00001915
Iteration 316/1000 | Loss: 0.00001915
Iteration 317/1000 | Loss: 0.00001914
Iteration 318/1000 | Loss: 0.00001914
Iteration 319/1000 | Loss: 0.00001914
Iteration 320/1000 | Loss: 0.00001914
Iteration 321/1000 | Loss: 0.00001914
Iteration 322/1000 | Loss: 0.00001914
Iteration 323/1000 | Loss: 0.00001914
Iteration 324/1000 | Loss: 0.00001914
Iteration 325/1000 | Loss: 0.00001914
Iteration 326/1000 | Loss: 0.00001914
Iteration 327/1000 | Loss: 0.00001914
Iteration 328/1000 | Loss: 0.00001914
Iteration 329/1000 | Loss: 0.00001914
Iteration 330/1000 | Loss: 0.00001914
Iteration 331/1000 | Loss: 0.00001914
Iteration 332/1000 | Loss: 0.00001914
Iteration 333/1000 | Loss: 0.00001914
Iteration 334/1000 | Loss: 0.00001914
Iteration 335/1000 | Loss: 0.00001914
Iteration 336/1000 | Loss: 0.00001914
Iteration 337/1000 | Loss: 0.00001914
Iteration 338/1000 | Loss: 0.00001914
Iteration 339/1000 | Loss: 0.00001914
Iteration 340/1000 | Loss: 0.00001913
Iteration 341/1000 | Loss: 0.00001913
Iteration 342/1000 | Loss: 0.00001913
Iteration 343/1000 | Loss: 0.00001913
Iteration 344/1000 | Loss: 0.00001913
Iteration 345/1000 | Loss: 0.00001913
Iteration 346/1000 | Loss: 0.00001913
Iteration 347/1000 | Loss: 0.00001913
Iteration 348/1000 | Loss: 0.00001913
Iteration 349/1000 | Loss: 0.00001913
Iteration 350/1000 | Loss: 0.00001913
Iteration 351/1000 | Loss: 0.00001913
Iteration 352/1000 | Loss: 0.00001913
Iteration 353/1000 | Loss: 0.00001913
Iteration 354/1000 | Loss: 0.00001913
Iteration 355/1000 | Loss: 0.00001913
Iteration 356/1000 | Loss: 0.00001913
Iteration 357/1000 | Loss: 0.00001913
Iteration 358/1000 | Loss: 0.00001913
Iteration 359/1000 | Loss: 0.00001913
Iteration 360/1000 | Loss: 0.00001913
Iteration 361/1000 | Loss: 0.00001913
Iteration 362/1000 | Loss: 0.00001913
Iteration 363/1000 | Loss: 0.00001913
Iteration 364/1000 | Loss: 0.00001913
Iteration 365/1000 | Loss: 0.00001913
Iteration 366/1000 | Loss: 0.00001913
Iteration 367/1000 | Loss: 0.00001913
Iteration 368/1000 | Loss: 0.00001913
Iteration 369/1000 | Loss: 0.00001913
Iteration 370/1000 | Loss: 0.00001913
Iteration 371/1000 | Loss: 0.00001913
Iteration 372/1000 | Loss: 0.00001913
Iteration 373/1000 | Loss: 0.00001913
Iteration 374/1000 | Loss: 0.00001913
Iteration 375/1000 | Loss: 0.00001913
Iteration 376/1000 | Loss: 0.00001913
Iteration 377/1000 | Loss: 0.00001913
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 377. Stopping optimization.
Last 5 losses: [1.9127101040794514e-05, 1.9127101040794514e-05, 1.9127101040794514e-05, 1.9127101040794514e-05, 1.9127101040794514e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9127101040794514e-05

Optimization complete. Final v2v error: 3.861384153366089 mm

Highest mean error: 4.72311544418335 mm for frame 72

Lowest mean error: 3.4059536457061768 mm for frame 0

Saving results

Total time: 179.29450130462646
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_us_1259/0013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_1259/0013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_1259/0013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00986631
Iteration 2/25 | Loss: 0.00156815
Iteration 3/25 | Loss: 0.00138754
Iteration 4/25 | Loss: 0.00134129
Iteration 5/25 | Loss: 0.00133228
Iteration 6/25 | Loss: 0.00133053
Iteration 7/25 | Loss: 0.00133044
Iteration 8/25 | Loss: 0.00133044
Iteration 9/25 | Loss: 0.00133044
Iteration 10/25 | Loss: 0.00133044
Iteration 11/25 | Loss: 0.00133044
Iteration 12/25 | Loss: 0.00133044
Iteration 13/25 | Loss: 0.00133044
Iteration 14/25 | Loss: 0.00133044
Iteration 15/25 | Loss: 0.00133044
Iteration 16/25 | Loss: 0.00133044
Iteration 17/25 | Loss: 0.00133044
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0013304410967975855, 0.0013304410967975855, 0.0013304410967975855, 0.0013304410967975855, 0.0013304410967975855]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013304410967975855

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.59402812
Iteration 2/25 | Loss: 0.00229659
Iteration 3/25 | Loss: 0.00229655
Iteration 4/25 | Loss: 0.00229654
Iteration 5/25 | Loss: 0.00229654
Iteration 6/25 | Loss: 0.00229654
Iteration 7/25 | Loss: 0.00229654
Iteration 8/25 | Loss: 0.00229654
Iteration 9/25 | Loss: 0.00229654
Iteration 10/25 | Loss: 0.00229654
Iteration 11/25 | Loss: 0.00229654
Iteration 12/25 | Loss: 0.00229654
Iteration 13/25 | Loss: 0.00229654
Iteration 14/25 | Loss: 0.00229654
Iteration 15/25 | Loss: 0.00229654
Iteration 16/25 | Loss: 0.00229654
Iteration 17/25 | Loss: 0.00229654
Iteration 18/25 | Loss: 0.00229654
Iteration 19/25 | Loss: 0.00229654
Iteration 20/25 | Loss: 0.00229654
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.002296543214470148, 0.002296543214470148, 0.002296543214470148, 0.002296543214470148, 0.002296543214470148]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002296543214470148

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00229654
Iteration 2/1000 | Loss: 0.00007358
Iteration 3/1000 | Loss: 0.00005467
Iteration 4/1000 | Loss: 0.00004489
Iteration 5/1000 | Loss: 0.00004129
Iteration 6/1000 | Loss: 0.00003940
Iteration 7/1000 | Loss: 0.00003787
Iteration 8/1000 | Loss: 0.00003656
Iteration 9/1000 | Loss: 0.00003577
Iteration 10/1000 | Loss: 0.00003524
Iteration 11/1000 | Loss: 0.00003485
Iteration 12/1000 | Loss: 0.00003454
Iteration 13/1000 | Loss: 0.00003426
Iteration 14/1000 | Loss: 0.00003420
Iteration 15/1000 | Loss: 0.00003410
Iteration 16/1000 | Loss: 0.00003398
Iteration 17/1000 | Loss: 0.00003391
Iteration 18/1000 | Loss: 0.00003388
Iteration 19/1000 | Loss: 0.00003388
Iteration 20/1000 | Loss: 0.00003388
Iteration 21/1000 | Loss: 0.00003387
Iteration 22/1000 | Loss: 0.00003386
Iteration 23/1000 | Loss: 0.00003386
Iteration 24/1000 | Loss: 0.00003385
Iteration 25/1000 | Loss: 0.00003385
Iteration 26/1000 | Loss: 0.00003385
Iteration 27/1000 | Loss: 0.00003384
Iteration 28/1000 | Loss: 0.00003384
Iteration 29/1000 | Loss: 0.00003383
Iteration 30/1000 | Loss: 0.00003383
Iteration 31/1000 | Loss: 0.00003382
Iteration 32/1000 | Loss: 0.00003382
Iteration 33/1000 | Loss: 0.00003381
Iteration 34/1000 | Loss: 0.00003381
Iteration 35/1000 | Loss: 0.00003381
Iteration 36/1000 | Loss: 0.00003381
Iteration 37/1000 | Loss: 0.00003381
Iteration 38/1000 | Loss: 0.00003381
Iteration 39/1000 | Loss: 0.00003381
Iteration 40/1000 | Loss: 0.00003381
Iteration 41/1000 | Loss: 0.00003381
Iteration 42/1000 | Loss: 0.00003381
Iteration 43/1000 | Loss: 0.00003381
Iteration 44/1000 | Loss: 0.00003381
Iteration 45/1000 | Loss: 0.00003380
Iteration 46/1000 | Loss: 0.00003380
Iteration 47/1000 | Loss: 0.00003380
Iteration 48/1000 | Loss: 0.00003380
Iteration 49/1000 | Loss: 0.00003380
Iteration 50/1000 | Loss: 0.00003378
Iteration 51/1000 | Loss: 0.00003378
Iteration 52/1000 | Loss: 0.00003377
Iteration 53/1000 | Loss: 0.00003377
Iteration 54/1000 | Loss: 0.00003377
Iteration 55/1000 | Loss: 0.00003376
Iteration 56/1000 | Loss: 0.00003376
Iteration 57/1000 | Loss: 0.00003376
Iteration 58/1000 | Loss: 0.00003375
Iteration 59/1000 | Loss: 0.00003374
Iteration 60/1000 | Loss: 0.00003374
Iteration 61/1000 | Loss: 0.00003374
Iteration 62/1000 | Loss: 0.00003374
Iteration 63/1000 | Loss: 0.00003373
Iteration 64/1000 | Loss: 0.00003373
Iteration 65/1000 | Loss: 0.00003372
Iteration 66/1000 | Loss: 0.00003371
Iteration 67/1000 | Loss: 0.00003371
Iteration 68/1000 | Loss: 0.00003371
Iteration 69/1000 | Loss: 0.00003371
Iteration 70/1000 | Loss: 0.00003370
Iteration 71/1000 | Loss: 0.00003370
Iteration 72/1000 | Loss: 0.00003370
Iteration 73/1000 | Loss: 0.00003370
Iteration 74/1000 | Loss: 0.00003369
Iteration 75/1000 | Loss: 0.00003368
Iteration 76/1000 | Loss: 0.00003368
Iteration 77/1000 | Loss: 0.00003367
Iteration 78/1000 | Loss: 0.00003367
Iteration 79/1000 | Loss: 0.00003367
Iteration 80/1000 | Loss: 0.00003367
Iteration 81/1000 | Loss: 0.00003366
Iteration 82/1000 | Loss: 0.00003366
Iteration 83/1000 | Loss: 0.00003366
Iteration 84/1000 | Loss: 0.00003365
Iteration 85/1000 | Loss: 0.00003365
Iteration 86/1000 | Loss: 0.00003365
Iteration 87/1000 | Loss: 0.00003365
Iteration 88/1000 | Loss: 0.00003365
Iteration 89/1000 | Loss: 0.00003365
Iteration 90/1000 | Loss: 0.00003364
Iteration 91/1000 | Loss: 0.00003364
Iteration 92/1000 | Loss: 0.00003364
Iteration 93/1000 | Loss: 0.00003364
Iteration 94/1000 | Loss: 0.00003364
Iteration 95/1000 | Loss: 0.00003364
Iteration 96/1000 | Loss: 0.00003364
Iteration 97/1000 | Loss: 0.00003364
Iteration 98/1000 | Loss: 0.00003364
Iteration 99/1000 | Loss: 0.00003364
Iteration 100/1000 | Loss: 0.00003364
Iteration 101/1000 | Loss: 0.00003364
Iteration 102/1000 | Loss: 0.00003364
Iteration 103/1000 | Loss: 0.00003364
Iteration 104/1000 | Loss: 0.00003364
Iteration 105/1000 | Loss: 0.00003364
Iteration 106/1000 | Loss: 0.00003363
Iteration 107/1000 | Loss: 0.00003363
Iteration 108/1000 | Loss: 0.00003363
Iteration 109/1000 | Loss: 0.00003363
Iteration 110/1000 | Loss: 0.00003363
Iteration 111/1000 | Loss: 0.00003363
Iteration 112/1000 | Loss: 0.00003363
Iteration 113/1000 | Loss: 0.00003363
Iteration 114/1000 | Loss: 0.00003363
Iteration 115/1000 | Loss: 0.00003363
Iteration 116/1000 | Loss: 0.00003363
Iteration 117/1000 | Loss: 0.00003363
Iteration 118/1000 | Loss: 0.00003363
Iteration 119/1000 | Loss: 0.00003363
Iteration 120/1000 | Loss: 0.00003363
Iteration 121/1000 | Loss: 0.00003363
Iteration 122/1000 | Loss: 0.00003363
Iteration 123/1000 | Loss: 0.00003363
Iteration 124/1000 | Loss: 0.00003363
Iteration 125/1000 | Loss: 0.00003363
Iteration 126/1000 | Loss: 0.00003363
Iteration 127/1000 | Loss: 0.00003363
Iteration 128/1000 | Loss: 0.00003363
Iteration 129/1000 | Loss: 0.00003363
Iteration 130/1000 | Loss: 0.00003363
Iteration 131/1000 | Loss: 0.00003363
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 131. Stopping optimization.
Last 5 losses: [3.363267023814842e-05, 3.363267023814842e-05, 3.363267023814842e-05, 3.363267023814842e-05, 3.363267023814842e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.363267023814842e-05

Optimization complete. Final v2v error: 4.824949741363525 mm

Highest mean error: 6.491084098815918 mm for frame 70

Lowest mean error: 4.000574111938477 mm for frame 0

Saving results

Total time: 37.70402216911316
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_us_1259/0016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_1259/0016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_1259/0016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01053206
Iteration 2/25 | Loss: 0.00290097
Iteration 3/25 | Loss: 0.00191525
Iteration 4/25 | Loss: 0.00164078
Iteration 5/25 | Loss: 0.00159767
Iteration 6/25 | Loss: 0.00158818
Iteration 7/25 | Loss: 0.00160563
Iteration 8/25 | Loss: 0.00157965
Iteration 9/25 | Loss: 0.00156692
Iteration 10/25 | Loss: 0.00155945
Iteration 11/25 | Loss: 0.00154600
Iteration 12/25 | Loss: 0.00154475
Iteration 13/25 | Loss: 0.00154359
Iteration 14/25 | Loss: 0.00153711
Iteration 15/25 | Loss: 0.00153300
Iteration 16/25 | Loss: 0.00152610
Iteration 17/25 | Loss: 0.00151983
Iteration 18/25 | Loss: 0.00152003
Iteration 19/25 | Loss: 0.00151567
Iteration 20/25 | Loss: 0.00150910
Iteration 21/25 | Loss: 0.00151406
Iteration 22/25 | Loss: 0.00150951
Iteration 23/25 | Loss: 0.00150981
Iteration 24/25 | Loss: 0.00150978
Iteration 25/25 | Loss: 0.00150922

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.98090482
Iteration 2/25 | Loss: 0.00473885
Iteration 3/25 | Loss: 0.00473885
Iteration 4/25 | Loss: 0.00473885
Iteration 5/25 | Loss: 0.00473885
Iteration 6/25 | Loss: 0.00473885
Iteration 7/25 | Loss: 0.00473885
Iteration 8/25 | Loss: 0.00473885
Iteration 9/25 | Loss: 0.00473885
Iteration 10/25 | Loss: 0.00473885
Iteration 11/25 | Loss: 0.00473885
Iteration 12/25 | Loss: 0.00473885
Iteration 13/25 | Loss: 0.00473885
Iteration 14/25 | Loss: 0.00473885
Iteration 15/25 | Loss: 0.00473885
Iteration 16/25 | Loss: 0.00473885
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.004738850984722376, 0.004738850984722376, 0.004738850984722376, 0.004738850984722376, 0.004738850984722376]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.004738850984722376

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00473885
Iteration 2/1000 | Loss: 0.00082352
Iteration 3/1000 | Loss: 0.00175866
Iteration 4/1000 | Loss: 0.00091863
Iteration 5/1000 | Loss: 0.00031184
Iteration 6/1000 | Loss: 0.00102482
Iteration 7/1000 | Loss: 0.00026173
Iteration 8/1000 | Loss: 0.00058063
Iteration 9/1000 | Loss: 0.00118713
Iteration 10/1000 | Loss: 0.00513830
Iteration 11/1000 | Loss: 0.00143781
Iteration 12/1000 | Loss: 0.00040621
Iteration 13/1000 | Loss: 0.00612632
Iteration 14/1000 | Loss: 0.02219164
Iteration 15/1000 | Loss: 0.00713884
Iteration 16/1000 | Loss: 0.00094730
Iteration 17/1000 | Loss: 0.00037180
Iteration 18/1000 | Loss: 0.00026491
Iteration 19/1000 | Loss: 0.00269297
Iteration 20/1000 | Loss: 0.01834864
Iteration 21/1000 | Loss: 0.00369179
Iteration 22/1000 | Loss: 0.00541735
Iteration 23/1000 | Loss: 0.01362904
Iteration 24/1000 | Loss: 0.00238020
Iteration 25/1000 | Loss: 0.00074776
Iteration 26/1000 | Loss: 0.00322367
Iteration 27/1000 | Loss: 0.00233854
Iteration 28/1000 | Loss: 0.00490078
Iteration 29/1000 | Loss: 0.00218052
Iteration 30/1000 | Loss: 0.00059963
Iteration 31/1000 | Loss: 0.00046051
Iteration 32/1000 | Loss: 0.00119719
Iteration 33/1000 | Loss: 0.00056961
Iteration 34/1000 | Loss: 0.00031494
Iteration 35/1000 | Loss: 0.00254913
Iteration 36/1000 | Loss: 0.00104185
Iteration 37/1000 | Loss: 0.00553636
Iteration 38/1000 | Loss: 0.00340832
Iteration 39/1000 | Loss: 0.00189739
Iteration 40/1000 | Loss: 0.00104412
Iteration 41/1000 | Loss: 0.00108484
Iteration 42/1000 | Loss: 0.00095392
Iteration 43/1000 | Loss: 0.00086140
Iteration 44/1000 | Loss: 0.00187875
Iteration 45/1000 | Loss: 0.00015983
Iteration 46/1000 | Loss: 0.00176211
Iteration 47/1000 | Loss: 0.00122783
Iteration 48/1000 | Loss: 0.00061941
Iteration 49/1000 | Loss: 0.00019784
Iteration 50/1000 | Loss: 0.00018735
Iteration 51/1000 | Loss: 0.00015381
Iteration 52/1000 | Loss: 0.00070217
Iteration 53/1000 | Loss: 0.00192494
Iteration 54/1000 | Loss: 0.00159106
Iteration 55/1000 | Loss: 0.00085749
Iteration 56/1000 | Loss: 0.00037490
Iteration 57/1000 | Loss: 0.00016912
Iteration 58/1000 | Loss: 0.00168963
Iteration 59/1000 | Loss: 0.00186177
Iteration 60/1000 | Loss: 0.00103693
Iteration 61/1000 | Loss: 0.00150641
Iteration 62/1000 | Loss: 0.00149515
Iteration 63/1000 | Loss: 0.00146371
Iteration 64/1000 | Loss: 0.00131428
Iteration 65/1000 | Loss: 0.00060499
Iteration 66/1000 | Loss: 0.00016320
Iteration 67/1000 | Loss: 0.00013180
Iteration 68/1000 | Loss: 0.00029337
Iteration 69/1000 | Loss: 0.00024360
Iteration 70/1000 | Loss: 0.00011472
Iteration 71/1000 | Loss: 0.00043589
Iteration 72/1000 | Loss: 0.00076479
Iteration 73/1000 | Loss: 0.00142514
Iteration 74/1000 | Loss: 0.00151268
Iteration 75/1000 | Loss: 0.00073517
Iteration 76/1000 | Loss: 0.00030372
Iteration 77/1000 | Loss: 0.00070967
Iteration 78/1000 | Loss: 0.00044215
Iteration 79/1000 | Loss: 0.00034722
Iteration 80/1000 | Loss: 0.00079527
Iteration 81/1000 | Loss: 0.00069425
Iteration 82/1000 | Loss: 0.00066702
Iteration 83/1000 | Loss: 0.00076629
Iteration 84/1000 | Loss: 0.00134779
Iteration 85/1000 | Loss: 0.00012175
Iteration 86/1000 | Loss: 0.00007649
Iteration 87/1000 | Loss: 0.00006813
Iteration 88/1000 | Loss: 0.00006359
Iteration 89/1000 | Loss: 0.00006095
Iteration 90/1000 | Loss: 0.00102270
Iteration 91/1000 | Loss: 0.00170126
Iteration 92/1000 | Loss: 0.00094253
Iteration 93/1000 | Loss: 0.00125750
Iteration 94/1000 | Loss: 0.00159557
Iteration 95/1000 | Loss: 0.00113627
Iteration 96/1000 | Loss: 0.00022267
Iteration 97/1000 | Loss: 0.00060643
Iteration 98/1000 | Loss: 0.00010804
Iteration 99/1000 | Loss: 0.00007644
Iteration 100/1000 | Loss: 0.00017489
Iteration 101/1000 | Loss: 0.00008290
Iteration 102/1000 | Loss: 0.00006462
Iteration 103/1000 | Loss: 0.00005854
Iteration 104/1000 | Loss: 0.00005528
Iteration 105/1000 | Loss: 0.00005305
Iteration 106/1000 | Loss: 0.00005079
Iteration 107/1000 | Loss: 0.00004908
Iteration 108/1000 | Loss: 0.00004791
Iteration 109/1000 | Loss: 0.00004708
Iteration 110/1000 | Loss: 0.00004650
Iteration 111/1000 | Loss: 0.00115993
Iteration 112/1000 | Loss: 0.00008178
Iteration 113/1000 | Loss: 0.00004792
Iteration 114/1000 | Loss: 0.00004617
Iteration 115/1000 | Loss: 0.00004570
Iteration 116/1000 | Loss: 0.00004529
Iteration 117/1000 | Loss: 0.00105963
Iteration 118/1000 | Loss: 0.00252608
Iteration 119/1000 | Loss: 0.00106404
Iteration 120/1000 | Loss: 0.00083951
Iteration 121/1000 | Loss: 0.00036504
Iteration 122/1000 | Loss: 0.00006813
Iteration 123/1000 | Loss: 0.00094372
Iteration 124/1000 | Loss: 0.00005192
Iteration 125/1000 | Loss: 0.00004718
Iteration 126/1000 | Loss: 0.00004476
Iteration 127/1000 | Loss: 0.00004369
Iteration 128/1000 | Loss: 0.00004253
Iteration 129/1000 | Loss: 0.00004183
Iteration 130/1000 | Loss: 0.00004145
Iteration 131/1000 | Loss: 0.00004117
Iteration 132/1000 | Loss: 0.00004100
Iteration 133/1000 | Loss: 0.00004084
Iteration 134/1000 | Loss: 0.00004070
Iteration 135/1000 | Loss: 0.00004066
Iteration 136/1000 | Loss: 0.00004066
Iteration 137/1000 | Loss: 0.00004065
Iteration 138/1000 | Loss: 0.00004064
Iteration 139/1000 | Loss: 0.00004064
Iteration 140/1000 | Loss: 0.00004064
Iteration 141/1000 | Loss: 0.00004064
Iteration 142/1000 | Loss: 0.00004064
Iteration 143/1000 | Loss: 0.00004064
Iteration 144/1000 | Loss: 0.00004064
Iteration 145/1000 | Loss: 0.00004064
Iteration 146/1000 | Loss: 0.00004063
Iteration 147/1000 | Loss: 0.00004063
Iteration 148/1000 | Loss: 0.00004063
Iteration 149/1000 | Loss: 0.00111447
Iteration 150/1000 | Loss: 0.00020368
Iteration 151/1000 | Loss: 0.00124906
Iteration 152/1000 | Loss: 0.00006884
Iteration 153/1000 | Loss: 0.00004997
Iteration 154/1000 | Loss: 0.00004432
Iteration 155/1000 | Loss: 0.00004005
Iteration 156/1000 | Loss: 0.00003666
Iteration 157/1000 | Loss: 0.00003524
Iteration 158/1000 | Loss: 0.00003408
Iteration 159/1000 | Loss: 0.00003334
Iteration 160/1000 | Loss: 0.00003293
Iteration 161/1000 | Loss: 0.00003265
Iteration 162/1000 | Loss: 0.00003243
Iteration 163/1000 | Loss: 0.00003239
Iteration 164/1000 | Loss: 0.00003238
Iteration 165/1000 | Loss: 0.00003237
Iteration 166/1000 | Loss: 0.00003237
Iteration 167/1000 | Loss: 0.00003237
Iteration 168/1000 | Loss: 0.00003236
Iteration 169/1000 | Loss: 0.00003236
Iteration 170/1000 | Loss: 0.00003236
Iteration 171/1000 | Loss: 0.00003235
Iteration 172/1000 | Loss: 0.00003235
Iteration 173/1000 | Loss: 0.00003235
Iteration 174/1000 | Loss: 0.00003234
Iteration 175/1000 | Loss: 0.00003234
Iteration 176/1000 | Loss: 0.00003234
Iteration 177/1000 | Loss: 0.00003234
Iteration 178/1000 | Loss: 0.00003233
Iteration 179/1000 | Loss: 0.00003231
Iteration 180/1000 | Loss: 0.00003230
Iteration 181/1000 | Loss: 0.00003230
Iteration 182/1000 | Loss: 0.00003230
Iteration 183/1000 | Loss: 0.00003229
Iteration 184/1000 | Loss: 0.00003229
Iteration 185/1000 | Loss: 0.00003229
Iteration 186/1000 | Loss: 0.00003229
Iteration 187/1000 | Loss: 0.00003229
Iteration 188/1000 | Loss: 0.00003229
Iteration 189/1000 | Loss: 0.00003228
Iteration 190/1000 | Loss: 0.00003228
Iteration 191/1000 | Loss: 0.00003228
Iteration 192/1000 | Loss: 0.00003228
Iteration 193/1000 | Loss: 0.00003228
Iteration 194/1000 | Loss: 0.00003228
Iteration 195/1000 | Loss: 0.00003228
Iteration 196/1000 | Loss: 0.00003228
Iteration 197/1000 | Loss: 0.00003227
Iteration 198/1000 | Loss: 0.00003227
Iteration 199/1000 | Loss: 0.00003227
Iteration 200/1000 | Loss: 0.00003227
Iteration 201/1000 | Loss: 0.00003227
Iteration 202/1000 | Loss: 0.00003226
Iteration 203/1000 | Loss: 0.00003226
Iteration 204/1000 | Loss: 0.00003226
Iteration 205/1000 | Loss: 0.00003225
Iteration 206/1000 | Loss: 0.00003225
Iteration 207/1000 | Loss: 0.00003225
Iteration 208/1000 | Loss: 0.00003225
Iteration 209/1000 | Loss: 0.00003225
Iteration 210/1000 | Loss: 0.00003225
Iteration 211/1000 | Loss: 0.00003225
Iteration 212/1000 | Loss: 0.00003225
Iteration 213/1000 | Loss: 0.00003225
Iteration 214/1000 | Loss: 0.00003225
Iteration 215/1000 | Loss: 0.00003225
Iteration 216/1000 | Loss: 0.00003225
Iteration 217/1000 | Loss: 0.00003224
Iteration 218/1000 | Loss: 0.00003224
Iteration 219/1000 | Loss: 0.00003224
Iteration 220/1000 | Loss: 0.00003224
Iteration 221/1000 | Loss: 0.00003224
Iteration 222/1000 | Loss: 0.00003224
Iteration 223/1000 | Loss: 0.00003224
Iteration 224/1000 | Loss: 0.00003223
Iteration 225/1000 | Loss: 0.00003223
Iteration 226/1000 | Loss: 0.00003223
Iteration 227/1000 | Loss: 0.00003223
Iteration 228/1000 | Loss: 0.00003223
Iteration 229/1000 | Loss: 0.00003223
Iteration 230/1000 | Loss: 0.00003223
Iteration 231/1000 | Loss: 0.00003223
Iteration 232/1000 | Loss: 0.00003223
Iteration 233/1000 | Loss: 0.00003223
Iteration 234/1000 | Loss: 0.00003223
Iteration 235/1000 | Loss: 0.00003223
Iteration 236/1000 | Loss: 0.00003222
Iteration 237/1000 | Loss: 0.00003222
Iteration 238/1000 | Loss: 0.00003222
Iteration 239/1000 | Loss: 0.00003222
Iteration 240/1000 | Loss: 0.00003222
Iteration 241/1000 | Loss: 0.00003222
Iteration 242/1000 | Loss: 0.00003222
Iteration 243/1000 | Loss: 0.00003222
Iteration 244/1000 | Loss: 0.00003222
Iteration 245/1000 | Loss: 0.00003222
Iteration 246/1000 | Loss: 0.00003222
Iteration 247/1000 | Loss: 0.00003222
Iteration 248/1000 | Loss: 0.00003222
Iteration 249/1000 | Loss: 0.00003222
Iteration 250/1000 | Loss: 0.00003222
Iteration 251/1000 | Loss: 0.00003222
Iteration 252/1000 | Loss: 0.00003222
Iteration 253/1000 | Loss: 0.00003222
Iteration 254/1000 | Loss: 0.00003222
Iteration 255/1000 | Loss: 0.00003222
Iteration 256/1000 | Loss: 0.00003222
Iteration 257/1000 | Loss: 0.00003222
Iteration 258/1000 | Loss: 0.00003222
Iteration 259/1000 | Loss: 0.00003221
Iteration 260/1000 | Loss: 0.00003221
Iteration 261/1000 | Loss: 0.00003221
Iteration 262/1000 | Loss: 0.00003221
Iteration 263/1000 | Loss: 0.00003221
Iteration 264/1000 | Loss: 0.00003221
Iteration 265/1000 | Loss: 0.00003221
Iteration 266/1000 | Loss: 0.00003221
Iteration 267/1000 | Loss: 0.00003221
Iteration 268/1000 | Loss: 0.00003221
Iteration 269/1000 | Loss: 0.00003221
Iteration 270/1000 | Loss: 0.00003221
Iteration 271/1000 | Loss: 0.00003221
Iteration 272/1000 | Loss: 0.00003221
Iteration 273/1000 | Loss: 0.00003221
Iteration 274/1000 | Loss: 0.00003221
Iteration 275/1000 | Loss: 0.00003221
Iteration 276/1000 | Loss: 0.00003221
Iteration 277/1000 | Loss: 0.00003221
Iteration 278/1000 | Loss: 0.00003220
Iteration 279/1000 | Loss: 0.00003220
Iteration 280/1000 | Loss: 0.00003220
Iteration 281/1000 | Loss: 0.00003220
Iteration 282/1000 | Loss: 0.00003220
Iteration 283/1000 | Loss: 0.00003220
Iteration 284/1000 | Loss: 0.00003220
Iteration 285/1000 | Loss: 0.00003220
Iteration 286/1000 | Loss: 0.00003220
Iteration 287/1000 | Loss: 0.00003220
Iteration 288/1000 | Loss: 0.00003220
Iteration 289/1000 | Loss: 0.00003220
Iteration 290/1000 | Loss: 0.00003220
Iteration 291/1000 | Loss: 0.00003220
Iteration 292/1000 | Loss: 0.00003220
Iteration 293/1000 | Loss: 0.00003220
Iteration 294/1000 | Loss: 0.00003220
Iteration 295/1000 | Loss: 0.00003220
Iteration 296/1000 | Loss: 0.00003220
Iteration 297/1000 | Loss: 0.00003220
Iteration 298/1000 | Loss: 0.00003220
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 298. Stopping optimization.
Last 5 losses: [3.2198207918554544e-05, 3.2198207918554544e-05, 3.2198207918554544e-05, 3.2198207918554544e-05, 3.2198207918554544e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.2198207918554544e-05

Optimization complete. Final v2v error: 4.491330623626709 mm

Highest mean error: 15.64549446105957 mm for frame 30

Lowest mean error: 3.3933591842651367 mm for frame 12

Saving results

Total time: 259.8286626338959
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_us_1259/0001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_1259/0001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_1259/0001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00927053
Iteration 2/25 | Loss: 0.00133861
Iteration 3/25 | Loss: 0.00122474
Iteration 4/25 | Loss: 0.00120624
Iteration 5/25 | Loss: 0.00120137
Iteration 6/25 | Loss: 0.00120027
Iteration 7/25 | Loss: 0.00120027
Iteration 8/25 | Loss: 0.00120027
Iteration 9/25 | Loss: 0.00120027
Iteration 10/25 | Loss: 0.00120027
Iteration 11/25 | Loss: 0.00120027
Iteration 12/25 | Loss: 0.00120027
Iteration 13/25 | Loss: 0.00120027
Iteration 14/25 | Loss: 0.00120027
Iteration 15/25 | Loss: 0.00120027
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0012002671137452126, 0.0012002671137452126, 0.0012002671137452126, 0.0012002671137452126, 0.0012002671137452126]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012002671137452126

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.63907611
Iteration 2/25 | Loss: 0.00236241
Iteration 3/25 | Loss: 0.00236241
Iteration 4/25 | Loss: 0.00236241
Iteration 5/25 | Loss: 0.00236241
Iteration 6/25 | Loss: 0.00236241
Iteration 7/25 | Loss: 0.00236241
Iteration 8/25 | Loss: 0.00236241
Iteration 9/25 | Loss: 0.00236241
Iteration 10/25 | Loss: 0.00236241
Iteration 11/25 | Loss: 0.00236241
Iteration 12/25 | Loss: 0.00236241
Iteration 13/25 | Loss: 0.00236241
Iteration 14/25 | Loss: 0.00236241
Iteration 15/25 | Loss: 0.00236241
Iteration 16/25 | Loss: 0.00236241
Iteration 17/25 | Loss: 0.00236241
Iteration 18/25 | Loss: 0.00236241
Iteration 19/25 | Loss: 0.00236241
Iteration 20/25 | Loss: 0.00236241
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.002362408209592104, 0.002362408209592104, 0.002362408209592104, 0.002362408209592104, 0.002362408209592104]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002362408209592104

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00236241
Iteration 2/1000 | Loss: 0.00003332
Iteration 3/1000 | Loss: 0.00002386
Iteration 4/1000 | Loss: 0.00002131
Iteration 5/1000 | Loss: 0.00002002
Iteration 6/1000 | Loss: 0.00001896
Iteration 7/1000 | Loss: 0.00001832
Iteration 8/1000 | Loss: 0.00001795
Iteration 9/1000 | Loss: 0.00001777
Iteration 10/1000 | Loss: 0.00001762
Iteration 11/1000 | Loss: 0.00001746
Iteration 12/1000 | Loss: 0.00001743
Iteration 13/1000 | Loss: 0.00001736
Iteration 14/1000 | Loss: 0.00001736
Iteration 15/1000 | Loss: 0.00001735
Iteration 16/1000 | Loss: 0.00001735
Iteration 17/1000 | Loss: 0.00001734
Iteration 18/1000 | Loss: 0.00001733
Iteration 19/1000 | Loss: 0.00001732
Iteration 20/1000 | Loss: 0.00001730
Iteration 21/1000 | Loss: 0.00001730
Iteration 22/1000 | Loss: 0.00001730
Iteration 23/1000 | Loss: 0.00001730
Iteration 24/1000 | Loss: 0.00001730
Iteration 25/1000 | Loss: 0.00001730
Iteration 26/1000 | Loss: 0.00001730
Iteration 27/1000 | Loss: 0.00001730
Iteration 28/1000 | Loss: 0.00001729
Iteration 29/1000 | Loss: 0.00001728
Iteration 30/1000 | Loss: 0.00001727
Iteration 31/1000 | Loss: 0.00001726
Iteration 32/1000 | Loss: 0.00001726
Iteration 33/1000 | Loss: 0.00001726
Iteration 34/1000 | Loss: 0.00001725
Iteration 35/1000 | Loss: 0.00001725
Iteration 36/1000 | Loss: 0.00001725
Iteration 37/1000 | Loss: 0.00001724
Iteration 38/1000 | Loss: 0.00001724
Iteration 39/1000 | Loss: 0.00001724
Iteration 40/1000 | Loss: 0.00001724
Iteration 41/1000 | Loss: 0.00001723
Iteration 42/1000 | Loss: 0.00001723
Iteration 43/1000 | Loss: 0.00001723
Iteration 44/1000 | Loss: 0.00001722
Iteration 45/1000 | Loss: 0.00001722
Iteration 46/1000 | Loss: 0.00001722
Iteration 47/1000 | Loss: 0.00001722
Iteration 48/1000 | Loss: 0.00001722
Iteration 49/1000 | Loss: 0.00001721
Iteration 50/1000 | Loss: 0.00001721
Iteration 51/1000 | Loss: 0.00001720
Iteration 52/1000 | Loss: 0.00001719
Iteration 53/1000 | Loss: 0.00001719
Iteration 54/1000 | Loss: 0.00001718
Iteration 55/1000 | Loss: 0.00001718
Iteration 56/1000 | Loss: 0.00001718
Iteration 57/1000 | Loss: 0.00001717
Iteration 58/1000 | Loss: 0.00001717
Iteration 59/1000 | Loss: 0.00001717
Iteration 60/1000 | Loss: 0.00001717
Iteration 61/1000 | Loss: 0.00001717
Iteration 62/1000 | Loss: 0.00001717
Iteration 63/1000 | Loss: 0.00001716
Iteration 64/1000 | Loss: 0.00001716
Iteration 65/1000 | Loss: 0.00001716
Iteration 66/1000 | Loss: 0.00001716
Iteration 67/1000 | Loss: 0.00001715
Iteration 68/1000 | Loss: 0.00001715
Iteration 69/1000 | Loss: 0.00001715
Iteration 70/1000 | Loss: 0.00001715
Iteration 71/1000 | Loss: 0.00001715
Iteration 72/1000 | Loss: 0.00001715
Iteration 73/1000 | Loss: 0.00001715
Iteration 74/1000 | Loss: 0.00001714
Iteration 75/1000 | Loss: 0.00001714
Iteration 76/1000 | Loss: 0.00001714
Iteration 77/1000 | Loss: 0.00001713
Iteration 78/1000 | Loss: 0.00001713
Iteration 79/1000 | Loss: 0.00001713
Iteration 80/1000 | Loss: 0.00001713
Iteration 81/1000 | Loss: 0.00001712
Iteration 82/1000 | Loss: 0.00001712
Iteration 83/1000 | Loss: 0.00001711
Iteration 84/1000 | Loss: 0.00001710
Iteration 85/1000 | Loss: 0.00001710
Iteration 86/1000 | Loss: 0.00001710
Iteration 87/1000 | Loss: 0.00001710
Iteration 88/1000 | Loss: 0.00001709
Iteration 89/1000 | Loss: 0.00001709
Iteration 90/1000 | Loss: 0.00001709
Iteration 91/1000 | Loss: 0.00001709
Iteration 92/1000 | Loss: 0.00001709
Iteration 93/1000 | Loss: 0.00001709
Iteration 94/1000 | Loss: 0.00001709
Iteration 95/1000 | Loss: 0.00001708
Iteration 96/1000 | Loss: 0.00001707
Iteration 97/1000 | Loss: 0.00001707
Iteration 98/1000 | Loss: 0.00001707
Iteration 99/1000 | Loss: 0.00001707
Iteration 100/1000 | Loss: 0.00001707
Iteration 101/1000 | Loss: 0.00001707
Iteration 102/1000 | Loss: 0.00001707
Iteration 103/1000 | Loss: 0.00001707
Iteration 104/1000 | Loss: 0.00001707
Iteration 105/1000 | Loss: 0.00001707
Iteration 106/1000 | Loss: 0.00001707
Iteration 107/1000 | Loss: 0.00001707
Iteration 108/1000 | Loss: 0.00001707
Iteration 109/1000 | Loss: 0.00001707
Iteration 110/1000 | Loss: 0.00001707
Iteration 111/1000 | Loss: 0.00001707
Iteration 112/1000 | Loss: 0.00001706
Iteration 113/1000 | Loss: 0.00001706
Iteration 114/1000 | Loss: 0.00001706
Iteration 115/1000 | Loss: 0.00001706
Iteration 116/1000 | Loss: 0.00001706
Iteration 117/1000 | Loss: 0.00001706
Iteration 118/1000 | Loss: 0.00001705
Iteration 119/1000 | Loss: 0.00001705
Iteration 120/1000 | Loss: 0.00001705
Iteration 121/1000 | Loss: 0.00001705
Iteration 122/1000 | Loss: 0.00001705
Iteration 123/1000 | Loss: 0.00001705
Iteration 124/1000 | Loss: 0.00001705
Iteration 125/1000 | Loss: 0.00001705
Iteration 126/1000 | Loss: 0.00001705
Iteration 127/1000 | Loss: 0.00001705
Iteration 128/1000 | Loss: 0.00001705
Iteration 129/1000 | Loss: 0.00001705
Iteration 130/1000 | Loss: 0.00001705
Iteration 131/1000 | Loss: 0.00001705
Iteration 132/1000 | Loss: 0.00001705
Iteration 133/1000 | Loss: 0.00001705
Iteration 134/1000 | Loss: 0.00001705
Iteration 135/1000 | Loss: 0.00001705
Iteration 136/1000 | Loss: 0.00001705
Iteration 137/1000 | Loss: 0.00001705
Iteration 138/1000 | Loss: 0.00001705
Iteration 139/1000 | Loss: 0.00001705
Iteration 140/1000 | Loss: 0.00001705
Iteration 141/1000 | Loss: 0.00001705
Iteration 142/1000 | Loss: 0.00001705
Iteration 143/1000 | Loss: 0.00001705
Iteration 144/1000 | Loss: 0.00001705
Iteration 145/1000 | Loss: 0.00001705
Iteration 146/1000 | Loss: 0.00001705
Iteration 147/1000 | Loss: 0.00001705
Iteration 148/1000 | Loss: 0.00001705
Iteration 149/1000 | Loss: 0.00001705
Iteration 150/1000 | Loss: 0.00001705
Iteration 151/1000 | Loss: 0.00001705
Iteration 152/1000 | Loss: 0.00001705
Iteration 153/1000 | Loss: 0.00001705
Iteration 154/1000 | Loss: 0.00001705
Iteration 155/1000 | Loss: 0.00001705
Iteration 156/1000 | Loss: 0.00001705
Iteration 157/1000 | Loss: 0.00001705
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 157. Stopping optimization.
Last 5 losses: [1.705140675767325e-05, 1.705140675767325e-05, 1.705140675767325e-05, 1.705140675767325e-05, 1.705140675767325e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.705140675767325e-05

Optimization complete. Final v2v error: 3.5947859287261963 mm

Highest mean error: 4.100100040435791 mm for frame 59

Lowest mean error: 3.275815963745117 mm for frame 220

Saving results

Total time: 36.88671112060547
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_us_1259/0017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_1259/0017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_1259/0017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00822622
Iteration 2/25 | Loss: 0.00174131
Iteration 3/25 | Loss: 0.00156018
Iteration 4/25 | Loss: 0.00138377
Iteration 5/25 | Loss: 0.00134783
Iteration 6/25 | Loss: 0.00130044
Iteration 7/25 | Loss: 0.00129633
Iteration 8/25 | Loss: 0.00129986
Iteration 9/25 | Loss: 0.00129758
Iteration 10/25 | Loss: 0.00129545
Iteration 11/25 | Loss: 0.00129475
Iteration 12/25 | Loss: 0.00129384
Iteration 13/25 | Loss: 0.00129160
Iteration 14/25 | Loss: 0.00129082
Iteration 15/25 | Loss: 0.00129059
Iteration 16/25 | Loss: 0.00129047
Iteration 17/25 | Loss: 0.00129047
Iteration 18/25 | Loss: 0.00129047
Iteration 19/25 | Loss: 0.00129046
Iteration 20/25 | Loss: 0.00129046
Iteration 21/25 | Loss: 0.00129046
Iteration 22/25 | Loss: 0.00129046
Iteration 23/25 | Loss: 0.00129046
Iteration 24/25 | Loss: 0.00129046
Iteration 25/25 | Loss: 0.00129046

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.07181597
Iteration 2/25 | Loss: 0.00221183
Iteration 3/25 | Loss: 0.00221182
Iteration 4/25 | Loss: 0.00221182
Iteration 5/25 | Loss: 0.00221182
Iteration 6/25 | Loss: 0.00221182
Iteration 7/25 | Loss: 0.00221182
Iteration 8/25 | Loss: 0.00221182
Iteration 9/25 | Loss: 0.00221182
Iteration 10/25 | Loss: 0.00221182
Iteration 11/25 | Loss: 0.00221182
Iteration 12/25 | Loss: 0.00221182
Iteration 13/25 | Loss: 0.00221182
Iteration 14/25 | Loss: 0.00221182
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0022118224296718836, 0.0022118224296718836, 0.0022118224296718836, 0.0022118224296718836, 0.0022118224296718836]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0022118224296718836

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00221182
Iteration 2/1000 | Loss: 0.00005749
Iteration 3/1000 | Loss: 0.00004165
Iteration 4/1000 | Loss: 0.00003802
Iteration 5/1000 | Loss: 0.00003568
Iteration 6/1000 | Loss: 0.00003428
Iteration 7/1000 | Loss: 0.00003321
Iteration 8/1000 | Loss: 0.00003233
Iteration 9/1000 | Loss: 0.00003201
Iteration 10/1000 | Loss: 0.00003178
Iteration 11/1000 | Loss: 0.00003163
Iteration 12/1000 | Loss: 0.00003162
Iteration 13/1000 | Loss: 0.00003160
Iteration 14/1000 | Loss: 0.00003160
Iteration 15/1000 | Loss: 0.00003156
Iteration 16/1000 | Loss: 0.00003156
Iteration 17/1000 | Loss: 0.00003155
Iteration 18/1000 | Loss: 0.00003154
Iteration 19/1000 | Loss: 0.00003153
Iteration 20/1000 | Loss: 0.00003152
Iteration 21/1000 | Loss: 0.00003152
Iteration 22/1000 | Loss: 0.00003151
Iteration 23/1000 | Loss: 0.00003150
Iteration 24/1000 | Loss: 0.00003150
Iteration 25/1000 | Loss: 0.00003149
Iteration 26/1000 | Loss: 0.00003149
Iteration 27/1000 | Loss: 0.00003146
Iteration 28/1000 | Loss: 0.00003143
Iteration 29/1000 | Loss: 0.00003142
Iteration 30/1000 | Loss: 0.00003142
Iteration 31/1000 | Loss: 0.00003142
Iteration 32/1000 | Loss: 0.00003141
Iteration 33/1000 | Loss: 0.00003141
Iteration 34/1000 | Loss: 0.00003140
Iteration 35/1000 | Loss: 0.00003140
Iteration 36/1000 | Loss: 0.00003139
Iteration 37/1000 | Loss: 0.00003139
Iteration 38/1000 | Loss: 0.00003139
Iteration 39/1000 | Loss: 0.00003138
Iteration 40/1000 | Loss: 0.00003138
Iteration 41/1000 | Loss: 0.00003138
Iteration 42/1000 | Loss: 0.00003137
Iteration 43/1000 | Loss: 0.00003137
Iteration 44/1000 | Loss: 0.00003137
Iteration 45/1000 | Loss: 0.00003137
Iteration 46/1000 | Loss: 0.00003137
Iteration 47/1000 | Loss: 0.00003136
Iteration 48/1000 | Loss: 0.00003136
Iteration 49/1000 | Loss: 0.00003136
Iteration 50/1000 | Loss: 0.00003135
Iteration 51/1000 | Loss: 0.00003135
Iteration 52/1000 | Loss: 0.00003135
Iteration 53/1000 | Loss: 0.00003134
Iteration 54/1000 | Loss: 0.00003134
Iteration 55/1000 | Loss: 0.00003134
Iteration 56/1000 | Loss: 0.00003133
Iteration 57/1000 | Loss: 0.00003133
Iteration 58/1000 | Loss: 0.00003133
Iteration 59/1000 | Loss: 0.00003133
Iteration 60/1000 | Loss: 0.00003133
Iteration 61/1000 | Loss: 0.00003132
Iteration 62/1000 | Loss: 0.00003132
Iteration 63/1000 | Loss: 0.00003131
Iteration 64/1000 | Loss: 0.00003131
Iteration 65/1000 | Loss: 0.00003130
Iteration 66/1000 | Loss: 0.00003130
Iteration 67/1000 | Loss: 0.00003130
Iteration 68/1000 | Loss: 0.00003129
Iteration 69/1000 | Loss: 0.00003129
Iteration 70/1000 | Loss: 0.00003129
Iteration 71/1000 | Loss: 0.00003128
Iteration 72/1000 | Loss: 0.00003128
Iteration 73/1000 | Loss: 0.00003127
Iteration 74/1000 | Loss: 0.00003126
Iteration 75/1000 | Loss: 0.00003126
Iteration 76/1000 | Loss: 0.00003126
Iteration 77/1000 | Loss: 0.00003126
Iteration 78/1000 | Loss: 0.00003125
Iteration 79/1000 | Loss: 0.00003125
Iteration 80/1000 | Loss: 0.00003125
Iteration 81/1000 | Loss: 0.00003125
Iteration 82/1000 | Loss: 0.00003124
Iteration 83/1000 | Loss: 0.00003124
Iteration 84/1000 | Loss: 0.00003124
Iteration 85/1000 | Loss: 0.00003124
Iteration 86/1000 | Loss: 0.00003124
Iteration 87/1000 | Loss: 0.00003124
Iteration 88/1000 | Loss: 0.00003123
Iteration 89/1000 | Loss: 0.00003123
Iteration 90/1000 | Loss: 0.00003123
Iteration 91/1000 | Loss: 0.00003122
Iteration 92/1000 | Loss: 0.00003122
Iteration 93/1000 | Loss: 0.00003122
Iteration 94/1000 | Loss: 0.00003122
Iteration 95/1000 | Loss: 0.00003122
Iteration 96/1000 | Loss: 0.00003121
Iteration 97/1000 | Loss: 0.00003121
Iteration 98/1000 | Loss: 0.00003121
Iteration 99/1000 | Loss: 0.00003121
Iteration 100/1000 | Loss: 0.00003120
Iteration 101/1000 | Loss: 0.00003120
Iteration 102/1000 | Loss: 0.00003120
Iteration 103/1000 | Loss: 0.00003120
Iteration 104/1000 | Loss: 0.00003119
Iteration 105/1000 | Loss: 0.00003119
Iteration 106/1000 | Loss: 0.00003119
Iteration 107/1000 | Loss: 0.00003119
Iteration 108/1000 | Loss: 0.00003119
Iteration 109/1000 | Loss: 0.00003119
Iteration 110/1000 | Loss: 0.00003119
Iteration 111/1000 | Loss: 0.00003119
Iteration 112/1000 | Loss: 0.00003119
Iteration 113/1000 | Loss: 0.00003119
Iteration 114/1000 | Loss: 0.00003119
Iteration 115/1000 | Loss: 0.00003119
Iteration 116/1000 | Loss: 0.00003119
Iteration 117/1000 | Loss: 0.00003119
Iteration 118/1000 | Loss: 0.00003119
Iteration 119/1000 | Loss: 0.00003119
Iteration 120/1000 | Loss: 0.00003119
Iteration 121/1000 | Loss: 0.00003119
Iteration 122/1000 | Loss: 0.00003119
Iteration 123/1000 | Loss: 0.00003119
Iteration 124/1000 | Loss: 0.00003119
Iteration 125/1000 | Loss: 0.00003119
Iteration 126/1000 | Loss: 0.00003119
Iteration 127/1000 | Loss: 0.00003119
Iteration 128/1000 | Loss: 0.00003119
Iteration 129/1000 | Loss: 0.00003119
Iteration 130/1000 | Loss: 0.00003119
Iteration 131/1000 | Loss: 0.00003119
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 131. Stopping optimization.
Last 5 losses: [3.1190920708468184e-05, 3.1190920708468184e-05, 3.1190920708468184e-05, 3.1190920708468184e-05, 3.1190920708468184e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.1190920708468184e-05

Optimization complete. Final v2v error: 4.722509860992432 mm

Highest mean error: 5.097279071807861 mm for frame 24

Lowest mean error: 4.277120113372803 mm for frame 67

Saving results

Total time: 49.845202684402466
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_us_1259/0012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_1259/0012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_1259/0012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00950555
Iteration 2/25 | Loss: 0.00159257
Iteration 3/25 | Loss: 0.00136549
Iteration 4/25 | Loss: 0.00132937
Iteration 5/25 | Loss: 0.00131747
Iteration 6/25 | Loss: 0.00131421
Iteration 7/25 | Loss: 0.00131338
Iteration 8/25 | Loss: 0.00131338
Iteration 9/25 | Loss: 0.00131338
Iteration 10/25 | Loss: 0.00131338
Iteration 11/25 | Loss: 0.00131338
Iteration 12/25 | Loss: 0.00131338
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0013133840402588248, 0.0013133840402588248, 0.0013133840402588248, 0.0013133840402588248, 0.0013133840402588248]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013133840402588248

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 8.28752327
Iteration 2/25 | Loss: 0.00320355
Iteration 3/25 | Loss: 0.00320355
Iteration 4/25 | Loss: 0.00320355
Iteration 5/25 | Loss: 0.00320355
Iteration 6/25 | Loss: 0.00320355
Iteration 7/25 | Loss: 0.00320355
Iteration 8/25 | Loss: 0.00320355
Iteration 9/25 | Loss: 0.00320355
Iteration 10/25 | Loss: 0.00320355
Iteration 11/25 | Loss: 0.00320355
Iteration 12/25 | Loss: 0.00320355
Iteration 13/25 | Loss: 0.00320355
Iteration 14/25 | Loss: 0.00320355
Iteration 15/25 | Loss: 0.00320355
Iteration 16/25 | Loss: 0.00320355
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0032035454642027617, 0.0032035454642027617, 0.0032035454642027617, 0.0032035454642027617, 0.0032035454642027617]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0032035454642027617

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00320355
Iteration 2/1000 | Loss: 0.00006546
Iteration 3/1000 | Loss: 0.00004724
Iteration 4/1000 | Loss: 0.00004003
Iteration 5/1000 | Loss: 0.00003738
Iteration 6/1000 | Loss: 0.00003591
Iteration 7/1000 | Loss: 0.00003505
Iteration 8/1000 | Loss: 0.00003430
Iteration 9/1000 | Loss: 0.00003357
Iteration 10/1000 | Loss: 0.00003311
Iteration 11/1000 | Loss: 0.00003274
Iteration 12/1000 | Loss: 0.00003243
Iteration 13/1000 | Loss: 0.00003224
Iteration 14/1000 | Loss: 0.00003201
Iteration 15/1000 | Loss: 0.00003184
Iteration 16/1000 | Loss: 0.00003182
Iteration 17/1000 | Loss: 0.00003174
Iteration 18/1000 | Loss: 0.00003169
Iteration 19/1000 | Loss: 0.00003159
Iteration 20/1000 | Loss: 0.00003159
Iteration 21/1000 | Loss: 0.00003158
Iteration 22/1000 | Loss: 0.00003156
Iteration 23/1000 | Loss: 0.00003155
Iteration 24/1000 | Loss: 0.00003154
Iteration 25/1000 | Loss: 0.00003153
Iteration 26/1000 | Loss: 0.00003152
Iteration 27/1000 | Loss: 0.00003146
Iteration 28/1000 | Loss: 0.00003145
Iteration 29/1000 | Loss: 0.00003143
Iteration 30/1000 | Loss: 0.00003143
Iteration 31/1000 | Loss: 0.00003142
Iteration 32/1000 | Loss: 0.00003142
Iteration 33/1000 | Loss: 0.00003142
Iteration 34/1000 | Loss: 0.00003139
Iteration 35/1000 | Loss: 0.00003139
Iteration 36/1000 | Loss: 0.00003137
Iteration 37/1000 | Loss: 0.00003136
Iteration 38/1000 | Loss: 0.00003136
Iteration 39/1000 | Loss: 0.00003135
Iteration 40/1000 | Loss: 0.00003135
Iteration 41/1000 | Loss: 0.00003135
Iteration 42/1000 | Loss: 0.00003134
Iteration 43/1000 | Loss: 0.00003134
Iteration 44/1000 | Loss: 0.00003134
Iteration 45/1000 | Loss: 0.00003133
Iteration 46/1000 | Loss: 0.00003133
Iteration 47/1000 | Loss: 0.00003133
Iteration 48/1000 | Loss: 0.00003132
Iteration 49/1000 | Loss: 0.00003132
Iteration 50/1000 | Loss: 0.00003132
Iteration 51/1000 | Loss: 0.00003132
Iteration 52/1000 | Loss: 0.00003132
Iteration 53/1000 | Loss: 0.00003131
Iteration 54/1000 | Loss: 0.00003131
Iteration 55/1000 | Loss: 0.00003131
Iteration 56/1000 | Loss: 0.00003131
Iteration 57/1000 | Loss: 0.00003130
Iteration 58/1000 | Loss: 0.00003130
Iteration 59/1000 | Loss: 0.00003130
Iteration 60/1000 | Loss: 0.00003129
Iteration 61/1000 | Loss: 0.00003129
Iteration 62/1000 | Loss: 0.00003129
Iteration 63/1000 | Loss: 0.00003128
Iteration 64/1000 | Loss: 0.00003128
Iteration 65/1000 | Loss: 0.00003128
Iteration 66/1000 | Loss: 0.00003128
Iteration 67/1000 | Loss: 0.00003128
Iteration 68/1000 | Loss: 0.00003127
Iteration 69/1000 | Loss: 0.00003127
Iteration 70/1000 | Loss: 0.00003127
Iteration 71/1000 | Loss: 0.00003127
Iteration 72/1000 | Loss: 0.00003127
Iteration 73/1000 | Loss: 0.00003127
Iteration 74/1000 | Loss: 0.00003126
Iteration 75/1000 | Loss: 0.00003126
Iteration 76/1000 | Loss: 0.00003126
Iteration 77/1000 | Loss: 0.00003126
Iteration 78/1000 | Loss: 0.00003126
Iteration 79/1000 | Loss: 0.00003126
Iteration 80/1000 | Loss: 0.00003125
Iteration 81/1000 | Loss: 0.00003125
Iteration 82/1000 | Loss: 0.00003125
Iteration 83/1000 | Loss: 0.00003125
Iteration 84/1000 | Loss: 0.00003125
Iteration 85/1000 | Loss: 0.00003125
Iteration 86/1000 | Loss: 0.00003124
Iteration 87/1000 | Loss: 0.00003124
Iteration 88/1000 | Loss: 0.00003124
Iteration 89/1000 | Loss: 0.00003124
Iteration 90/1000 | Loss: 0.00003124
Iteration 91/1000 | Loss: 0.00003124
Iteration 92/1000 | Loss: 0.00003124
Iteration 93/1000 | Loss: 0.00003124
Iteration 94/1000 | Loss: 0.00003124
Iteration 95/1000 | Loss: 0.00003123
Iteration 96/1000 | Loss: 0.00003123
Iteration 97/1000 | Loss: 0.00003123
Iteration 98/1000 | Loss: 0.00003123
Iteration 99/1000 | Loss: 0.00003123
Iteration 100/1000 | Loss: 0.00003123
Iteration 101/1000 | Loss: 0.00003122
Iteration 102/1000 | Loss: 0.00003122
Iteration 103/1000 | Loss: 0.00003122
Iteration 104/1000 | Loss: 0.00003122
Iteration 105/1000 | Loss: 0.00003122
Iteration 106/1000 | Loss: 0.00003121
Iteration 107/1000 | Loss: 0.00003121
Iteration 108/1000 | Loss: 0.00003121
Iteration 109/1000 | Loss: 0.00003120
Iteration 110/1000 | Loss: 0.00003120
Iteration 111/1000 | Loss: 0.00003120
Iteration 112/1000 | Loss: 0.00003120
Iteration 113/1000 | Loss: 0.00003120
Iteration 114/1000 | Loss: 0.00003119
Iteration 115/1000 | Loss: 0.00003119
Iteration 116/1000 | Loss: 0.00003119
Iteration 117/1000 | Loss: 0.00003118
Iteration 118/1000 | Loss: 0.00003118
Iteration 119/1000 | Loss: 0.00003118
Iteration 120/1000 | Loss: 0.00003118
Iteration 121/1000 | Loss: 0.00003118
Iteration 122/1000 | Loss: 0.00003118
Iteration 123/1000 | Loss: 0.00003117
Iteration 124/1000 | Loss: 0.00003117
Iteration 125/1000 | Loss: 0.00003117
Iteration 126/1000 | Loss: 0.00003117
Iteration 127/1000 | Loss: 0.00003117
Iteration 128/1000 | Loss: 0.00003117
Iteration 129/1000 | Loss: 0.00003116
Iteration 130/1000 | Loss: 0.00003116
Iteration 131/1000 | Loss: 0.00003116
Iteration 132/1000 | Loss: 0.00003116
Iteration 133/1000 | Loss: 0.00003115
Iteration 134/1000 | Loss: 0.00003115
Iteration 135/1000 | Loss: 0.00003115
Iteration 136/1000 | Loss: 0.00003115
Iteration 137/1000 | Loss: 0.00003115
Iteration 138/1000 | Loss: 0.00003115
Iteration 139/1000 | Loss: 0.00003115
Iteration 140/1000 | Loss: 0.00003115
Iteration 141/1000 | Loss: 0.00003115
Iteration 142/1000 | Loss: 0.00003115
Iteration 143/1000 | Loss: 0.00003115
Iteration 144/1000 | Loss: 0.00003114
Iteration 145/1000 | Loss: 0.00003114
Iteration 146/1000 | Loss: 0.00003114
Iteration 147/1000 | Loss: 0.00003114
Iteration 148/1000 | Loss: 0.00003114
Iteration 149/1000 | Loss: 0.00003114
Iteration 150/1000 | Loss: 0.00003114
Iteration 151/1000 | Loss: 0.00003114
Iteration 152/1000 | Loss: 0.00003114
Iteration 153/1000 | Loss: 0.00003114
Iteration 154/1000 | Loss: 0.00003113
Iteration 155/1000 | Loss: 0.00003113
Iteration 156/1000 | Loss: 0.00003113
Iteration 157/1000 | Loss: 0.00003113
Iteration 158/1000 | Loss: 0.00003113
Iteration 159/1000 | Loss: 0.00003113
Iteration 160/1000 | Loss: 0.00003113
Iteration 161/1000 | Loss: 0.00003113
Iteration 162/1000 | Loss: 0.00003113
Iteration 163/1000 | Loss: 0.00003113
Iteration 164/1000 | Loss: 0.00003112
Iteration 165/1000 | Loss: 0.00003112
Iteration 166/1000 | Loss: 0.00003112
Iteration 167/1000 | Loss: 0.00003112
Iteration 168/1000 | Loss: 0.00003112
Iteration 169/1000 | Loss: 0.00003112
Iteration 170/1000 | Loss: 0.00003112
Iteration 171/1000 | Loss: 0.00003112
Iteration 172/1000 | Loss: 0.00003112
Iteration 173/1000 | Loss: 0.00003111
Iteration 174/1000 | Loss: 0.00003111
Iteration 175/1000 | Loss: 0.00003111
Iteration 176/1000 | Loss: 0.00003111
Iteration 177/1000 | Loss: 0.00003111
Iteration 178/1000 | Loss: 0.00003111
Iteration 179/1000 | Loss: 0.00003111
Iteration 180/1000 | Loss: 0.00003111
Iteration 181/1000 | Loss: 0.00003110
Iteration 182/1000 | Loss: 0.00003110
Iteration 183/1000 | Loss: 0.00003110
Iteration 184/1000 | Loss: 0.00003110
Iteration 185/1000 | Loss: 0.00003109
Iteration 186/1000 | Loss: 0.00003109
Iteration 187/1000 | Loss: 0.00003109
Iteration 188/1000 | Loss: 0.00003109
Iteration 189/1000 | Loss: 0.00003109
Iteration 190/1000 | Loss: 0.00003109
Iteration 191/1000 | Loss: 0.00003109
Iteration 192/1000 | Loss: 0.00003109
Iteration 193/1000 | Loss: 0.00003108
Iteration 194/1000 | Loss: 0.00003108
Iteration 195/1000 | Loss: 0.00003108
Iteration 196/1000 | Loss: 0.00003108
Iteration 197/1000 | Loss: 0.00003108
Iteration 198/1000 | Loss: 0.00003108
Iteration 199/1000 | Loss: 0.00003108
Iteration 200/1000 | Loss: 0.00003108
Iteration 201/1000 | Loss: 0.00003108
Iteration 202/1000 | Loss: 0.00003108
Iteration 203/1000 | Loss: 0.00003108
Iteration 204/1000 | Loss: 0.00003107
Iteration 205/1000 | Loss: 0.00003107
Iteration 206/1000 | Loss: 0.00003107
Iteration 207/1000 | Loss: 0.00003107
Iteration 208/1000 | Loss: 0.00003107
Iteration 209/1000 | Loss: 0.00003107
Iteration 210/1000 | Loss: 0.00003107
Iteration 211/1000 | Loss: 0.00003107
Iteration 212/1000 | Loss: 0.00003107
Iteration 213/1000 | Loss: 0.00003107
Iteration 214/1000 | Loss: 0.00003107
Iteration 215/1000 | Loss: 0.00003107
Iteration 216/1000 | Loss: 0.00003107
Iteration 217/1000 | Loss: 0.00003107
Iteration 218/1000 | Loss: 0.00003107
Iteration 219/1000 | Loss: 0.00003107
Iteration 220/1000 | Loss: 0.00003107
Iteration 221/1000 | Loss: 0.00003107
Iteration 222/1000 | Loss: 0.00003106
Iteration 223/1000 | Loss: 0.00003106
Iteration 224/1000 | Loss: 0.00003106
Iteration 225/1000 | Loss: 0.00003106
Iteration 226/1000 | Loss: 0.00003106
Iteration 227/1000 | Loss: 0.00003106
Iteration 228/1000 | Loss: 0.00003106
Iteration 229/1000 | Loss: 0.00003106
Iteration 230/1000 | Loss: 0.00003106
Iteration 231/1000 | Loss: 0.00003106
Iteration 232/1000 | Loss: 0.00003106
Iteration 233/1000 | Loss: 0.00003106
Iteration 234/1000 | Loss: 0.00003106
Iteration 235/1000 | Loss: 0.00003106
Iteration 236/1000 | Loss: 0.00003106
Iteration 237/1000 | Loss: 0.00003106
Iteration 238/1000 | Loss: 0.00003106
Iteration 239/1000 | Loss: 0.00003106
Iteration 240/1000 | Loss: 0.00003106
Iteration 241/1000 | Loss: 0.00003106
Iteration 242/1000 | Loss: 0.00003106
Iteration 243/1000 | Loss: 0.00003106
Iteration 244/1000 | Loss: 0.00003106
Iteration 245/1000 | Loss: 0.00003106
Iteration 246/1000 | Loss: 0.00003106
Iteration 247/1000 | Loss: 0.00003106
Iteration 248/1000 | Loss: 0.00003106
Iteration 249/1000 | Loss: 0.00003106
Iteration 250/1000 | Loss: 0.00003106
Iteration 251/1000 | Loss: 0.00003106
Iteration 252/1000 | Loss: 0.00003106
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 252. Stopping optimization.
Last 5 losses: [3.105938958469778e-05, 3.105938958469778e-05, 3.105938958469778e-05, 3.105938958469778e-05, 3.105938958469778e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.105938958469778e-05

Optimization complete. Final v2v error: 4.713659763336182 mm

Highest mean error: 6.707008361816406 mm for frame 130

Lowest mean error: 3.495492696762085 mm for frame 77

Saving results

Total time: 57.36428713798523
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_us_1259/0006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_1259/0006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_1259/0006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00517666
Iteration 2/25 | Loss: 0.00133765
Iteration 3/25 | Loss: 0.00124924
Iteration 4/25 | Loss: 0.00123280
Iteration 5/25 | Loss: 0.00122847
Iteration 6/25 | Loss: 0.00122696
Iteration 7/25 | Loss: 0.00122696
Iteration 8/25 | Loss: 0.00122696
Iteration 9/25 | Loss: 0.00122696
Iteration 10/25 | Loss: 0.00122696
Iteration 11/25 | Loss: 0.00122696
Iteration 12/25 | Loss: 0.00122696
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0012269644066691399, 0.0012269644066691399, 0.0012269644066691399, 0.0012269644066691399, 0.0012269644066691399]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012269644066691399

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.65490818
Iteration 2/25 | Loss: 0.00252331
Iteration 3/25 | Loss: 0.00252331
Iteration 4/25 | Loss: 0.00252331
Iteration 5/25 | Loss: 0.00252330
Iteration 6/25 | Loss: 0.00252330
Iteration 7/25 | Loss: 0.00252330
Iteration 8/25 | Loss: 0.00252330
Iteration 9/25 | Loss: 0.00252330
Iteration 10/25 | Loss: 0.00252330
Iteration 11/25 | Loss: 0.00252330
Iteration 12/25 | Loss: 0.00252330
Iteration 13/25 | Loss: 0.00252330
Iteration 14/25 | Loss: 0.00252330
Iteration 15/25 | Loss: 0.00252330
Iteration 16/25 | Loss: 0.00252330
Iteration 17/25 | Loss: 0.00252330
Iteration 18/25 | Loss: 0.00252330
Iteration 19/25 | Loss: 0.00252330
Iteration 20/25 | Loss: 0.00252330
Iteration 21/25 | Loss: 0.00252330
Iteration 22/25 | Loss: 0.00252330
Iteration 23/25 | Loss: 0.00252330
Iteration 24/25 | Loss: 0.00252330
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0025233032647520304, 0.0025233032647520304, 0.0025233032647520304, 0.0025233032647520304, 0.0025233032647520304]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0025233032647520304

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00252330
Iteration 2/1000 | Loss: 0.00004744
Iteration 3/1000 | Loss: 0.00003547
Iteration 4/1000 | Loss: 0.00003034
Iteration 5/1000 | Loss: 0.00002831
Iteration 6/1000 | Loss: 0.00002692
Iteration 7/1000 | Loss: 0.00002621
Iteration 8/1000 | Loss: 0.00002571
Iteration 9/1000 | Loss: 0.00002537
Iteration 10/1000 | Loss: 0.00002514
Iteration 11/1000 | Loss: 0.00002499
Iteration 12/1000 | Loss: 0.00002494
Iteration 13/1000 | Loss: 0.00002489
Iteration 14/1000 | Loss: 0.00002481
Iteration 15/1000 | Loss: 0.00002481
Iteration 16/1000 | Loss: 0.00002479
Iteration 17/1000 | Loss: 0.00002478
Iteration 18/1000 | Loss: 0.00002477
Iteration 19/1000 | Loss: 0.00002475
Iteration 20/1000 | Loss: 0.00002475
Iteration 21/1000 | Loss: 0.00002474
Iteration 22/1000 | Loss: 0.00002473
Iteration 23/1000 | Loss: 0.00002472
Iteration 24/1000 | Loss: 0.00002471
Iteration 25/1000 | Loss: 0.00002470
Iteration 26/1000 | Loss: 0.00002470
Iteration 27/1000 | Loss: 0.00002467
Iteration 28/1000 | Loss: 0.00002467
Iteration 29/1000 | Loss: 0.00002466
Iteration 30/1000 | Loss: 0.00002465
Iteration 31/1000 | Loss: 0.00002465
Iteration 32/1000 | Loss: 0.00002464
Iteration 33/1000 | Loss: 0.00002464
Iteration 34/1000 | Loss: 0.00002463
Iteration 35/1000 | Loss: 0.00002463
Iteration 36/1000 | Loss: 0.00002463
Iteration 37/1000 | Loss: 0.00002463
Iteration 38/1000 | Loss: 0.00002463
Iteration 39/1000 | Loss: 0.00002462
Iteration 40/1000 | Loss: 0.00002462
Iteration 41/1000 | Loss: 0.00002462
Iteration 42/1000 | Loss: 0.00002462
Iteration 43/1000 | Loss: 0.00002462
Iteration 44/1000 | Loss: 0.00002462
Iteration 45/1000 | Loss: 0.00002462
Iteration 46/1000 | Loss: 0.00002462
Iteration 47/1000 | Loss: 0.00002462
Iteration 48/1000 | Loss: 0.00002462
Iteration 49/1000 | Loss: 0.00002461
Iteration 50/1000 | Loss: 0.00002461
Iteration 51/1000 | Loss: 0.00002461
Iteration 52/1000 | Loss: 0.00002461
Iteration 53/1000 | Loss: 0.00002460
Iteration 54/1000 | Loss: 0.00002460
Iteration 55/1000 | Loss: 0.00002460
Iteration 56/1000 | Loss: 0.00002460
Iteration 57/1000 | Loss: 0.00002460
Iteration 58/1000 | Loss: 0.00002460
Iteration 59/1000 | Loss: 0.00002460
Iteration 60/1000 | Loss: 0.00002460
Iteration 61/1000 | Loss: 0.00002460
Iteration 62/1000 | Loss: 0.00002460
Iteration 63/1000 | Loss: 0.00002460
Iteration 64/1000 | Loss: 0.00002460
Iteration 65/1000 | Loss: 0.00002460
Iteration 66/1000 | Loss: 0.00002459
Iteration 67/1000 | Loss: 0.00002459
Iteration 68/1000 | Loss: 0.00002459
Iteration 69/1000 | Loss: 0.00002459
Iteration 70/1000 | Loss: 0.00002459
Iteration 71/1000 | Loss: 0.00002459
Iteration 72/1000 | Loss: 0.00002459
Iteration 73/1000 | Loss: 0.00002458
Iteration 74/1000 | Loss: 0.00002458
Iteration 75/1000 | Loss: 0.00002458
Iteration 76/1000 | Loss: 0.00002458
Iteration 77/1000 | Loss: 0.00002458
Iteration 78/1000 | Loss: 0.00002458
Iteration 79/1000 | Loss: 0.00002458
Iteration 80/1000 | Loss: 0.00002457
Iteration 81/1000 | Loss: 0.00002457
Iteration 82/1000 | Loss: 0.00002457
Iteration 83/1000 | Loss: 0.00002457
Iteration 84/1000 | Loss: 0.00002456
Iteration 85/1000 | Loss: 0.00002456
Iteration 86/1000 | Loss: 0.00002456
Iteration 87/1000 | Loss: 0.00002456
Iteration 88/1000 | Loss: 0.00002456
Iteration 89/1000 | Loss: 0.00002456
Iteration 90/1000 | Loss: 0.00002456
Iteration 91/1000 | Loss: 0.00002456
Iteration 92/1000 | Loss: 0.00002455
Iteration 93/1000 | Loss: 0.00002455
Iteration 94/1000 | Loss: 0.00002455
Iteration 95/1000 | Loss: 0.00002455
Iteration 96/1000 | Loss: 0.00002455
Iteration 97/1000 | Loss: 0.00002454
Iteration 98/1000 | Loss: 0.00002454
Iteration 99/1000 | Loss: 0.00002454
Iteration 100/1000 | Loss: 0.00002454
Iteration 101/1000 | Loss: 0.00002454
Iteration 102/1000 | Loss: 0.00002454
Iteration 103/1000 | Loss: 0.00002454
Iteration 104/1000 | Loss: 0.00002454
Iteration 105/1000 | Loss: 0.00002454
Iteration 106/1000 | Loss: 0.00002454
Iteration 107/1000 | Loss: 0.00002454
Iteration 108/1000 | Loss: 0.00002454
Iteration 109/1000 | Loss: 0.00002454
Iteration 110/1000 | Loss: 0.00002454
Iteration 111/1000 | Loss: 0.00002454
Iteration 112/1000 | Loss: 0.00002454
Iteration 113/1000 | Loss: 0.00002454
Iteration 114/1000 | Loss: 0.00002453
Iteration 115/1000 | Loss: 0.00002453
Iteration 116/1000 | Loss: 0.00002453
Iteration 117/1000 | Loss: 0.00002453
Iteration 118/1000 | Loss: 0.00002453
Iteration 119/1000 | Loss: 0.00002452
Iteration 120/1000 | Loss: 0.00002452
Iteration 121/1000 | Loss: 0.00002452
Iteration 122/1000 | Loss: 0.00002452
Iteration 123/1000 | Loss: 0.00002452
Iteration 124/1000 | Loss: 0.00002452
Iteration 125/1000 | Loss: 0.00002452
Iteration 126/1000 | Loss: 0.00002452
Iteration 127/1000 | Loss: 0.00002452
Iteration 128/1000 | Loss: 0.00002452
Iteration 129/1000 | Loss: 0.00002452
Iteration 130/1000 | Loss: 0.00002452
Iteration 131/1000 | Loss: 0.00002452
Iteration 132/1000 | Loss: 0.00002452
Iteration 133/1000 | Loss: 0.00002452
Iteration 134/1000 | Loss: 0.00002452
Iteration 135/1000 | Loss: 0.00002452
Iteration 136/1000 | Loss: 0.00002452
Iteration 137/1000 | Loss: 0.00002452
Iteration 138/1000 | Loss: 0.00002452
Iteration 139/1000 | Loss: 0.00002452
Iteration 140/1000 | Loss: 0.00002452
Iteration 141/1000 | Loss: 0.00002452
Iteration 142/1000 | Loss: 0.00002452
Iteration 143/1000 | Loss: 0.00002452
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 143. Stopping optimization.
Last 5 losses: [2.452081571391318e-05, 2.452081571391318e-05, 2.452081571391318e-05, 2.452081571391318e-05, 2.452081571391318e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.452081571391318e-05

Optimization complete. Final v2v error: 4.271996021270752 mm

Highest mean error: 5.166893005371094 mm for frame 102

Lowest mean error: 3.6180858612060547 mm for frame 160

Saving results

Total time: 38.824140310287476
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_us_1259/0024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_1259/0024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_1259/0024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00904420
Iteration 2/25 | Loss: 0.00158370
Iteration 3/25 | Loss: 0.00130574
Iteration 4/25 | Loss: 0.00128083
Iteration 5/25 | Loss: 0.00126988
Iteration 6/25 | Loss: 0.00126646
Iteration 7/25 | Loss: 0.00126599
Iteration 8/25 | Loss: 0.00126595
Iteration 9/25 | Loss: 0.00126595
Iteration 10/25 | Loss: 0.00126595
Iteration 11/25 | Loss: 0.00126595
Iteration 12/25 | Loss: 0.00126595
Iteration 13/25 | Loss: 0.00126595
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0012659506173804402, 0.0012659506173804402, 0.0012659506173804402, 0.0012659506173804402, 0.0012659506173804402]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012659506173804402

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.40403080
Iteration 2/25 | Loss: 0.00253419
Iteration 3/25 | Loss: 0.00253418
Iteration 4/25 | Loss: 0.00253418
Iteration 5/25 | Loss: 0.00253418
Iteration 6/25 | Loss: 0.00253418
Iteration 7/25 | Loss: 0.00253418
Iteration 8/25 | Loss: 0.00253418
Iteration 9/25 | Loss: 0.00253418
Iteration 10/25 | Loss: 0.00253418
Iteration 11/25 | Loss: 0.00253418
Iteration 12/25 | Loss: 0.00253418
Iteration 13/25 | Loss: 0.00253418
Iteration 14/25 | Loss: 0.00253418
Iteration 15/25 | Loss: 0.00253418
Iteration 16/25 | Loss: 0.00253418
Iteration 17/25 | Loss: 0.00253418
Iteration 18/25 | Loss: 0.00253418
Iteration 19/25 | Loss: 0.00253418
Iteration 20/25 | Loss: 0.00253418
Iteration 21/25 | Loss: 0.00253418
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0025341755244880915, 0.0025341755244880915, 0.0025341755244880915, 0.0025341755244880915, 0.0025341755244880915]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0025341755244880915

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00253418
Iteration 2/1000 | Loss: 0.00004821
Iteration 3/1000 | Loss: 0.00003490
Iteration 4/1000 | Loss: 0.00003049
Iteration 5/1000 | Loss: 0.00002872
Iteration 6/1000 | Loss: 0.00002784
Iteration 7/1000 | Loss: 0.00002712
Iteration 8/1000 | Loss: 0.00002650
Iteration 9/1000 | Loss: 0.00002600
Iteration 10/1000 | Loss: 0.00002572
Iteration 11/1000 | Loss: 0.00002551
Iteration 12/1000 | Loss: 0.00002531
Iteration 13/1000 | Loss: 0.00002529
Iteration 14/1000 | Loss: 0.00002512
Iteration 15/1000 | Loss: 0.00002500
Iteration 16/1000 | Loss: 0.00002495
Iteration 17/1000 | Loss: 0.00002489
Iteration 18/1000 | Loss: 0.00002487
Iteration 19/1000 | Loss: 0.00002487
Iteration 20/1000 | Loss: 0.00002487
Iteration 21/1000 | Loss: 0.00002487
Iteration 22/1000 | Loss: 0.00002485
Iteration 23/1000 | Loss: 0.00002482
Iteration 24/1000 | Loss: 0.00002482
Iteration 25/1000 | Loss: 0.00002482
Iteration 26/1000 | Loss: 0.00002482
Iteration 27/1000 | Loss: 0.00002481
Iteration 28/1000 | Loss: 0.00002481
Iteration 29/1000 | Loss: 0.00002481
Iteration 30/1000 | Loss: 0.00002481
Iteration 31/1000 | Loss: 0.00002481
Iteration 32/1000 | Loss: 0.00002481
Iteration 33/1000 | Loss: 0.00002481
Iteration 34/1000 | Loss: 0.00002481
Iteration 35/1000 | Loss: 0.00002479
Iteration 36/1000 | Loss: 0.00002479
Iteration 37/1000 | Loss: 0.00002479
Iteration 38/1000 | Loss: 0.00002479
Iteration 39/1000 | Loss: 0.00002479
Iteration 40/1000 | Loss: 0.00002479
Iteration 41/1000 | Loss: 0.00002478
Iteration 42/1000 | Loss: 0.00002478
Iteration 43/1000 | Loss: 0.00002478
Iteration 44/1000 | Loss: 0.00002478
Iteration 45/1000 | Loss: 0.00002478
Iteration 46/1000 | Loss: 0.00002478
Iteration 47/1000 | Loss: 0.00002478
Iteration 48/1000 | Loss: 0.00002478
Iteration 49/1000 | Loss: 0.00002478
Iteration 50/1000 | Loss: 0.00002478
Iteration 51/1000 | Loss: 0.00002478
Iteration 52/1000 | Loss: 0.00002477
Iteration 53/1000 | Loss: 0.00002477
Iteration 54/1000 | Loss: 0.00002477
Iteration 55/1000 | Loss: 0.00002476
Iteration 56/1000 | Loss: 0.00002476
Iteration 57/1000 | Loss: 0.00002475
Iteration 58/1000 | Loss: 0.00002475
Iteration 59/1000 | Loss: 0.00002474
Iteration 60/1000 | Loss: 0.00002473
Iteration 61/1000 | Loss: 0.00002473
Iteration 62/1000 | Loss: 0.00002472
Iteration 63/1000 | Loss: 0.00002472
Iteration 64/1000 | Loss: 0.00002472
Iteration 65/1000 | Loss: 0.00002471
Iteration 66/1000 | Loss: 0.00002471
Iteration 67/1000 | Loss: 0.00002471
Iteration 68/1000 | Loss: 0.00002470
Iteration 69/1000 | Loss: 0.00002469
Iteration 70/1000 | Loss: 0.00002468
Iteration 71/1000 | Loss: 0.00002468
Iteration 72/1000 | Loss: 0.00002468
Iteration 73/1000 | Loss: 0.00002468
Iteration 74/1000 | Loss: 0.00002468
Iteration 75/1000 | Loss: 0.00002467
Iteration 76/1000 | Loss: 0.00002467
Iteration 77/1000 | Loss: 0.00002467
Iteration 78/1000 | Loss: 0.00002464
Iteration 79/1000 | Loss: 0.00002464
Iteration 80/1000 | Loss: 0.00002464
Iteration 81/1000 | Loss: 0.00002464
Iteration 82/1000 | Loss: 0.00002463
Iteration 83/1000 | Loss: 0.00002463
Iteration 84/1000 | Loss: 0.00002462
Iteration 85/1000 | Loss: 0.00002462
Iteration 86/1000 | Loss: 0.00002462
Iteration 87/1000 | Loss: 0.00002462
Iteration 88/1000 | Loss: 0.00002462
Iteration 89/1000 | Loss: 0.00002462
Iteration 90/1000 | Loss: 0.00002462
Iteration 91/1000 | Loss: 0.00002462
Iteration 92/1000 | Loss: 0.00002461
Iteration 93/1000 | Loss: 0.00002461
Iteration 94/1000 | Loss: 0.00002461
Iteration 95/1000 | Loss: 0.00002461
Iteration 96/1000 | Loss: 0.00002461
Iteration 97/1000 | Loss: 0.00002461
Iteration 98/1000 | Loss: 0.00002461
Iteration 99/1000 | Loss: 0.00002461
Iteration 100/1000 | Loss: 0.00002461
Iteration 101/1000 | Loss: 0.00002461
Iteration 102/1000 | Loss: 0.00002461
Iteration 103/1000 | Loss: 0.00002460
Iteration 104/1000 | Loss: 0.00002460
Iteration 105/1000 | Loss: 0.00002460
Iteration 106/1000 | Loss: 0.00002460
Iteration 107/1000 | Loss: 0.00002460
Iteration 108/1000 | Loss: 0.00002460
Iteration 109/1000 | Loss: 0.00002460
Iteration 110/1000 | Loss: 0.00002460
Iteration 111/1000 | Loss: 0.00002460
Iteration 112/1000 | Loss: 0.00002460
Iteration 113/1000 | Loss: 0.00002460
Iteration 114/1000 | Loss: 0.00002460
Iteration 115/1000 | Loss: 0.00002459
Iteration 116/1000 | Loss: 0.00002459
Iteration 117/1000 | Loss: 0.00002459
Iteration 118/1000 | Loss: 0.00002459
Iteration 119/1000 | Loss: 0.00002459
Iteration 120/1000 | Loss: 0.00002459
Iteration 121/1000 | Loss: 0.00002459
Iteration 122/1000 | Loss: 0.00002458
Iteration 123/1000 | Loss: 0.00002458
Iteration 124/1000 | Loss: 0.00002457
Iteration 125/1000 | Loss: 0.00002457
Iteration 126/1000 | Loss: 0.00002457
Iteration 127/1000 | Loss: 0.00002457
Iteration 128/1000 | Loss: 0.00002457
Iteration 129/1000 | Loss: 0.00002456
Iteration 130/1000 | Loss: 0.00002456
Iteration 131/1000 | Loss: 0.00002456
Iteration 132/1000 | Loss: 0.00002456
Iteration 133/1000 | Loss: 0.00002456
Iteration 134/1000 | Loss: 0.00002456
Iteration 135/1000 | Loss: 0.00002456
Iteration 136/1000 | Loss: 0.00002455
Iteration 137/1000 | Loss: 0.00002455
Iteration 138/1000 | Loss: 0.00002455
Iteration 139/1000 | Loss: 0.00002455
Iteration 140/1000 | Loss: 0.00002455
Iteration 141/1000 | Loss: 0.00002455
Iteration 142/1000 | Loss: 0.00002455
Iteration 143/1000 | Loss: 0.00002454
Iteration 144/1000 | Loss: 0.00002454
Iteration 145/1000 | Loss: 0.00002454
Iteration 146/1000 | Loss: 0.00002453
Iteration 147/1000 | Loss: 0.00002453
Iteration 148/1000 | Loss: 0.00002453
Iteration 149/1000 | Loss: 0.00002453
Iteration 150/1000 | Loss: 0.00002452
Iteration 151/1000 | Loss: 0.00002452
Iteration 152/1000 | Loss: 0.00002452
Iteration 153/1000 | Loss: 0.00002452
Iteration 154/1000 | Loss: 0.00002452
Iteration 155/1000 | Loss: 0.00002452
Iteration 156/1000 | Loss: 0.00002451
Iteration 157/1000 | Loss: 0.00002451
Iteration 158/1000 | Loss: 0.00002451
Iteration 159/1000 | Loss: 0.00002451
Iteration 160/1000 | Loss: 0.00002451
Iteration 161/1000 | Loss: 0.00002450
Iteration 162/1000 | Loss: 0.00002450
Iteration 163/1000 | Loss: 0.00002450
Iteration 164/1000 | Loss: 0.00002449
Iteration 165/1000 | Loss: 0.00002449
Iteration 166/1000 | Loss: 0.00002449
Iteration 167/1000 | Loss: 0.00002449
Iteration 168/1000 | Loss: 0.00002449
Iteration 169/1000 | Loss: 0.00002449
Iteration 170/1000 | Loss: 0.00002449
Iteration 171/1000 | Loss: 0.00002448
Iteration 172/1000 | Loss: 0.00002448
Iteration 173/1000 | Loss: 0.00002448
Iteration 174/1000 | Loss: 0.00002448
Iteration 175/1000 | Loss: 0.00002448
Iteration 176/1000 | Loss: 0.00002448
Iteration 177/1000 | Loss: 0.00002448
Iteration 178/1000 | Loss: 0.00002448
Iteration 179/1000 | Loss: 0.00002447
Iteration 180/1000 | Loss: 0.00002447
Iteration 181/1000 | Loss: 0.00002447
Iteration 182/1000 | Loss: 0.00002447
Iteration 183/1000 | Loss: 0.00002447
Iteration 184/1000 | Loss: 0.00002446
Iteration 185/1000 | Loss: 0.00002446
Iteration 186/1000 | Loss: 0.00002446
Iteration 187/1000 | Loss: 0.00002446
Iteration 188/1000 | Loss: 0.00002446
Iteration 189/1000 | Loss: 0.00002446
Iteration 190/1000 | Loss: 0.00002445
Iteration 191/1000 | Loss: 0.00002445
Iteration 192/1000 | Loss: 0.00002445
Iteration 193/1000 | Loss: 0.00002445
Iteration 194/1000 | Loss: 0.00002445
Iteration 195/1000 | Loss: 0.00002445
Iteration 196/1000 | Loss: 0.00002445
Iteration 197/1000 | Loss: 0.00002445
Iteration 198/1000 | Loss: 0.00002445
Iteration 199/1000 | Loss: 0.00002445
Iteration 200/1000 | Loss: 0.00002445
Iteration 201/1000 | Loss: 0.00002444
Iteration 202/1000 | Loss: 0.00002444
Iteration 203/1000 | Loss: 0.00002444
Iteration 204/1000 | Loss: 0.00002444
Iteration 205/1000 | Loss: 0.00002443
Iteration 206/1000 | Loss: 0.00002443
Iteration 207/1000 | Loss: 0.00002443
Iteration 208/1000 | Loss: 0.00002443
Iteration 209/1000 | Loss: 0.00002443
Iteration 210/1000 | Loss: 0.00002443
Iteration 211/1000 | Loss: 0.00002443
Iteration 212/1000 | Loss: 0.00002443
Iteration 213/1000 | Loss: 0.00002443
Iteration 214/1000 | Loss: 0.00002443
Iteration 215/1000 | Loss: 0.00002443
Iteration 216/1000 | Loss: 0.00002443
Iteration 217/1000 | Loss: 0.00002443
Iteration 218/1000 | Loss: 0.00002443
Iteration 219/1000 | Loss: 0.00002443
Iteration 220/1000 | Loss: 0.00002443
Iteration 221/1000 | Loss: 0.00002443
Iteration 222/1000 | Loss: 0.00002443
Iteration 223/1000 | Loss: 0.00002443
Iteration 224/1000 | Loss: 0.00002442
Iteration 225/1000 | Loss: 0.00002442
Iteration 226/1000 | Loss: 0.00002442
Iteration 227/1000 | Loss: 0.00002442
Iteration 228/1000 | Loss: 0.00002442
Iteration 229/1000 | Loss: 0.00002442
Iteration 230/1000 | Loss: 0.00002442
Iteration 231/1000 | Loss: 0.00002442
Iteration 232/1000 | Loss: 0.00002442
Iteration 233/1000 | Loss: 0.00002442
Iteration 234/1000 | Loss: 0.00002441
Iteration 235/1000 | Loss: 0.00002441
Iteration 236/1000 | Loss: 0.00002441
Iteration 237/1000 | Loss: 0.00002441
Iteration 238/1000 | Loss: 0.00002441
Iteration 239/1000 | Loss: 0.00002441
Iteration 240/1000 | Loss: 0.00002441
Iteration 241/1000 | Loss: 0.00002441
Iteration 242/1000 | Loss: 0.00002441
Iteration 243/1000 | Loss: 0.00002441
Iteration 244/1000 | Loss: 0.00002441
Iteration 245/1000 | Loss: 0.00002441
Iteration 246/1000 | Loss: 0.00002441
Iteration 247/1000 | Loss: 0.00002441
Iteration 248/1000 | Loss: 0.00002441
Iteration 249/1000 | Loss: 0.00002441
Iteration 250/1000 | Loss: 0.00002441
Iteration 251/1000 | Loss: 0.00002441
Iteration 252/1000 | Loss: 0.00002441
Iteration 253/1000 | Loss: 0.00002441
Iteration 254/1000 | Loss: 0.00002441
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 254. Stopping optimization.
Last 5 losses: [2.4409186153206974e-05, 2.4409186153206974e-05, 2.4409186153206974e-05, 2.4409186153206974e-05, 2.4409186153206974e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4409186153206974e-05

Optimization complete. Final v2v error: 4.237795352935791 mm

Highest mean error: 5.836545467376709 mm for frame 65

Lowest mean error: 3.6301212310791016 mm for frame 96

Saving results

Total time: 45.651599407196045
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_us_1259/0002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_1259/0002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_1259/0002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00916223
Iteration 2/25 | Loss: 0.00172198
Iteration 3/25 | Loss: 0.00144092
Iteration 4/25 | Loss: 0.00135788
Iteration 5/25 | Loss: 0.00133882
Iteration 6/25 | Loss: 0.00133685
Iteration 7/25 | Loss: 0.00133685
Iteration 8/25 | Loss: 0.00133685
Iteration 9/25 | Loss: 0.00133685
Iteration 10/25 | Loss: 0.00133685
Iteration 11/25 | Loss: 0.00133685
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013368481304496527, 0.0013368481304496527, 0.0013368481304496527, 0.0013368481304496527, 0.0013368481304496527]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013368481304496527

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.64398944
Iteration 2/25 | Loss: 0.00271629
Iteration 3/25 | Loss: 0.00271629
Iteration 4/25 | Loss: 0.00271628
Iteration 5/25 | Loss: 0.00271628
Iteration 6/25 | Loss: 0.00271628
Iteration 7/25 | Loss: 0.00271628
Iteration 8/25 | Loss: 0.00271628
Iteration 9/25 | Loss: 0.00271628
Iteration 10/25 | Loss: 0.00271628
Iteration 11/25 | Loss: 0.00271628
Iteration 12/25 | Loss: 0.00271628
Iteration 13/25 | Loss: 0.00271628
Iteration 14/25 | Loss: 0.00271628
Iteration 15/25 | Loss: 0.00271628
Iteration 16/25 | Loss: 0.00271628
Iteration 17/25 | Loss: 0.00271628
Iteration 18/25 | Loss: 0.00271628
Iteration 19/25 | Loss: 0.00271628
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0027162833139300346, 0.0027162833139300346, 0.0027162833139300346, 0.0027162833139300346, 0.0027162833139300346]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0027162833139300346

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00271628
Iteration 2/1000 | Loss: 0.00004597
Iteration 3/1000 | Loss: 0.00003613
Iteration 4/1000 | Loss: 0.00003216
Iteration 5/1000 | Loss: 0.00003081
Iteration 6/1000 | Loss: 0.00002957
Iteration 7/1000 | Loss: 0.00002877
Iteration 8/1000 | Loss: 0.00002805
Iteration 9/1000 | Loss: 0.00002755
Iteration 10/1000 | Loss: 0.00002723
Iteration 11/1000 | Loss: 0.00002701
Iteration 12/1000 | Loss: 0.00002698
Iteration 13/1000 | Loss: 0.00002697
Iteration 14/1000 | Loss: 0.00002688
Iteration 15/1000 | Loss: 0.00002680
Iteration 16/1000 | Loss: 0.00002677
Iteration 17/1000 | Loss: 0.00002676
Iteration 18/1000 | Loss: 0.00002676
Iteration 19/1000 | Loss: 0.00002675
Iteration 20/1000 | Loss: 0.00002675
Iteration 21/1000 | Loss: 0.00002674
Iteration 22/1000 | Loss: 0.00002674
Iteration 23/1000 | Loss: 0.00002674
Iteration 24/1000 | Loss: 0.00002673
Iteration 25/1000 | Loss: 0.00002670
Iteration 26/1000 | Loss: 0.00002668
Iteration 27/1000 | Loss: 0.00002668
Iteration 28/1000 | Loss: 0.00002668
Iteration 29/1000 | Loss: 0.00002666
Iteration 30/1000 | Loss: 0.00002665
Iteration 31/1000 | Loss: 0.00002664
Iteration 32/1000 | Loss: 0.00002661
Iteration 33/1000 | Loss: 0.00002661
Iteration 34/1000 | Loss: 0.00002661
Iteration 35/1000 | Loss: 0.00002661
Iteration 36/1000 | Loss: 0.00002661
Iteration 37/1000 | Loss: 0.00002661
Iteration 38/1000 | Loss: 0.00002661
Iteration 39/1000 | Loss: 0.00002661
Iteration 40/1000 | Loss: 0.00002661
Iteration 41/1000 | Loss: 0.00002660
Iteration 42/1000 | Loss: 0.00002660
Iteration 43/1000 | Loss: 0.00002659
Iteration 44/1000 | Loss: 0.00002659
Iteration 45/1000 | Loss: 0.00002658
Iteration 46/1000 | Loss: 0.00002658
Iteration 47/1000 | Loss: 0.00002658
Iteration 48/1000 | Loss: 0.00002658
Iteration 49/1000 | Loss: 0.00002658
Iteration 50/1000 | Loss: 0.00002658
Iteration 51/1000 | Loss: 0.00002658
Iteration 52/1000 | Loss: 0.00002658
Iteration 53/1000 | Loss: 0.00002658
Iteration 54/1000 | Loss: 0.00002658
Iteration 55/1000 | Loss: 0.00002657
Iteration 56/1000 | Loss: 0.00002657
Iteration 57/1000 | Loss: 0.00002657
Iteration 58/1000 | Loss: 0.00002657
Iteration 59/1000 | Loss: 0.00002657
Iteration 60/1000 | Loss: 0.00002656
Iteration 61/1000 | Loss: 0.00002656
Iteration 62/1000 | Loss: 0.00002656
Iteration 63/1000 | Loss: 0.00002656
Iteration 64/1000 | Loss: 0.00002656
Iteration 65/1000 | Loss: 0.00002656
Iteration 66/1000 | Loss: 0.00002656
Iteration 67/1000 | Loss: 0.00002656
Iteration 68/1000 | Loss: 0.00002656
Iteration 69/1000 | Loss: 0.00002656
Iteration 70/1000 | Loss: 0.00002656
Iteration 71/1000 | Loss: 0.00002656
Iteration 72/1000 | Loss: 0.00002656
Iteration 73/1000 | Loss: 0.00002656
Iteration 74/1000 | Loss: 0.00002655
Iteration 75/1000 | Loss: 0.00002655
Iteration 76/1000 | Loss: 0.00002655
Iteration 77/1000 | Loss: 0.00002655
Iteration 78/1000 | Loss: 0.00002655
Iteration 79/1000 | Loss: 0.00002655
Iteration 80/1000 | Loss: 0.00002655
Iteration 81/1000 | Loss: 0.00002655
Iteration 82/1000 | Loss: 0.00002655
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 82. Stopping optimization.
Last 5 losses: [2.6554214855423197e-05, 2.6554214855423197e-05, 2.6554214855423197e-05, 2.6554214855423197e-05, 2.6554214855423197e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.6554214855423197e-05

Optimization complete. Final v2v error: 4.3707146644592285 mm

Highest mean error: 4.727272987365723 mm for frame 190

Lowest mean error: 4.026544094085693 mm for frame 222

Saving results

Total time: 36.26030731201172
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_us_1259/0005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_1259/0005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_1259/0005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01183300
Iteration 2/25 | Loss: 0.01183300
Iteration 3/25 | Loss: 0.01183300
Iteration 4/25 | Loss: 0.01183300
Iteration 5/25 | Loss: 0.01183300
Iteration 6/25 | Loss: 0.01183300
Iteration 7/25 | Loss: 0.01183300
Iteration 8/25 | Loss: 0.01183300
Iteration 9/25 | Loss: 0.01183300
Iteration 10/25 | Loss: 0.01183300
Iteration 11/25 | Loss: 0.01183299
Iteration 12/25 | Loss: 0.01183299
Iteration 13/25 | Loss: 0.01183299
Iteration 14/25 | Loss: 0.01183299
Iteration 15/25 | Loss: 0.01183299
Iteration 16/25 | Loss: 0.01183299
Iteration 17/25 | Loss: 0.01183299
Iteration 18/25 | Loss: 0.01183299
Iteration 19/25 | Loss: 0.01183299
Iteration 20/25 | Loss: 0.01183299
Iteration 21/25 | Loss: 0.01183299
Iteration 22/25 | Loss: 0.01183298
Iteration 23/25 | Loss: 0.01183298
Iteration 24/25 | Loss: 0.01183298
Iteration 25/25 | Loss: 0.01183298

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.09066343
Iteration 2/25 | Loss: 0.06643779
Iteration 3/25 | Loss: 0.06637521
Iteration 4/25 | Loss: 0.06637519
Iteration 5/25 | Loss: 0.06637519
Iteration 6/25 | Loss: 0.06637517
Iteration 7/25 | Loss: 0.06637517
Iteration 8/25 | Loss: 0.06637517
Iteration 9/25 | Loss: 0.06637517
Iteration 10/25 | Loss: 0.06637517
Iteration 11/25 | Loss: 0.06637517
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.06637517362833023, 0.06637517362833023, 0.06637517362833023, 0.06637517362833023, 0.06637517362833023]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.06637517362833023

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.06637517
Iteration 2/1000 | Loss: 0.01243030
Iteration 3/1000 | Loss: 0.00310467
Iteration 4/1000 | Loss: 0.00539535
Iteration 5/1000 | Loss: 0.00612357
Iteration 6/1000 | Loss: 0.01240799
Iteration 7/1000 | Loss: 0.00401109
Iteration 8/1000 | Loss: 0.00184575
Iteration 9/1000 | Loss: 0.00264587
Iteration 10/1000 | Loss: 0.00048447
Iteration 11/1000 | Loss: 0.00030664
Iteration 12/1000 | Loss: 0.00018848
Iteration 13/1000 | Loss: 0.00080615
Iteration 14/1000 | Loss: 0.00027932
Iteration 15/1000 | Loss: 0.00060608
Iteration 16/1000 | Loss: 0.00049499
Iteration 17/1000 | Loss: 0.00052273
Iteration 18/1000 | Loss: 0.00036644
Iteration 19/1000 | Loss: 0.00040026
Iteration 20/1000 | Loss: 0.00014756
Iteration 21/1000 | Loss: 0.00050159
Iteration 22/1000 | Loss: 0.00043388
Iteration 23/1000 | Loss: 0.00023567
Iteration 24/1000 | Loss: 0.00069322
Iteration 25/1000 | Loss: 0.00051996
Iteration 26/1000 | Loss: 0.00020330
Iteration 27/1000 | Loss: 0.00007066
Iteration 28/1000 | Loss: 0.00006215
Iteration 29/1000 | Loss: 0.00013485
Iteration 30/1000 | Loss: 0.00033626
Iteration 31/1000 | Loss: 0.00007166
Iteration 32/1000 | Loss: 0.00005544
Iteration 33/1000 | Loss: 0.00008111
Iteration 34/1000 | Loss: 0.00010854
Iteration 35/1000 | Loss: 0.00019818
Iteration 36/1000 | Loss: 0.00004483
Iteration 37/1000 | Loss: 0.00007089
Iteration 38/1000 | Loss: 0.00006591
Iteration 39/1000 | Loss: 0.00012622
Iteration 40/1000 | Loss: 0.00013502
Iteration 41/1000 | Loss: 0.00010635
Iteration 42/1000 | Loss: 0.00004298
Iteration 43/1000 | Loss: 0.00004487
Iteration 44/1000 | Loss: 0.00011545
Iteration 45/1000 | Loss: 0.00007781
Iteration 46/1000 | Loss: 0.00096907
Iteration 47/1000 | Loss: 0.00268055
Iteration 48/1000 | Loss: 0.00219232
Iteration 49/1000 | Loss: 0.00017030
Iteration 50/1000 | Loss: 0.00018786
Iteration 51/1000 | Loss: 0.00086051
Iteration 52/1000 | Loss: 0.00010526
Iteration 53/1000 | Loss: 0.00005081
Iteration 54/1000 | Loss: 0.00006380
Iteration 55/1000 | Loss: 0.00004083
Iteration 56/1000 | Loss: 0.00004728
Iteration 57/1000 | Loss: 0.00007813
Iteration 58/1000 | Loss: 0.00003792
Iteration 59/1000 | Loss: 0.00003813
Iteration 60/1000 | Loss: 0.00003789
Iteration 61/1000 | Loss: 0.00003559
Iteration 62/1000 | Loss: 0.00008888
Iteration 63/1000 | Loss: 0.00003417
Iteration 64/1000 | Loss: 0.00003981
Iteration 65/1000 | Loss: 0.00003358
Iteration 66/1000 | Loss: 0.00006655
Iteration 67/1000 | Loss: 0.00003385
Iteration 68/1000 | Loss: 0.00003361
Iteration 69/1000 | Loss: 0.00003317
Iteration 70/1000 | Loss: 0.00003297
Iteration 71/1000 | Loss: 0.00006524
Iteration 72/1000 | Loss: 0.00003638
Iteration 73/1000 | Loss: 0.00003272
Iteration 74/1000 | Loss: 0.00004035
Iteration 75/1000 | Loss: 0.00003247
Iteration 76/1000 | Loss: 0.00007809
Iteration 77/1000 | Loss: 0.00003225
Iteration 78/1000 | Loss: 0.00003210
Iteration 79/1000 | Loss: 0.00003209
Iteration 80/1000 | Loss: 0.00003201
Iteration 81/1000 | Loss: 0.00003200
Iteration 82/1000 | Loss: 0.00005824
Iteration 83/1000 | Loss: 0.00003785
Iteration 84/1000 | Loss: 0.00005300
Iteration 85/1000 | Loss: 0.00004748
Iteration 86/1000 | Loss: 0.00003485
Iteration 87/1000 | Loss: 0.00005264
Iteration 88/1000 | Loss: 0.00004722
Iteration 89/1000 | Loss: 0.00005682
Iteration 90/1000 | Loss: 0.00025665
Iteration 91/1000 | Loss: 0.00007496
Iteration 92/1000 | Loss: 0.00018324
Iteration 93/1000 | Loss: 0.00004211
Iteration 94/1000 | Loss: 0.00010856
Iteration 95/1000 | Loss: 0.00003993
Iteration 96/1000 | Loss: 0.00003206
Iteration 97/1000 | Loss: 0.00005068
Iteration 98/1000 | Loss: 0.00007501
Iteration 99/1000 | Loss: 0.00008456
Iteration 100/1000 | Loss: 0.00003958
Iteration 101/1000 | Loss: 0.00004784
Iteration 102/1000 | Loss: 0.00004077
Iteration 103/1000 | Loss: 0.00005212
Iteration 104/1000 | Loss: 0.00003774
Iteration 105/1000 | Loss: 0.00003714
Iteration 106/1000 | Loss: 0.00005248
Iteration 107/1000 | Loss: 0.00024865
Iteration 108/1000 | Loss: 0.00016902
Iteration 109/1000 | Loss: 0.00006079
Iteration 110/1000 | Loss: 0.00005153
Iteration 111/1000 | Loss: 0.00003241
Iteration 112/1000 | Loss: 0.00004549
Iteration 113/1000 | Loss: 0.00003490
Iteration 114/1000 | Loss: 0.00004875
Iteration 115/1000 | Loss: 0.00004272
Iteration 116/1000 | Loss: 0.00004022
Iteration 117/1000 | Loss: 0.00003181
Iteration 118/1000 | Loss: 0.00004392
Iteration 119/1000 | Loss: 0.00003243
Iteration 120/1000 | Loss: 0.00004823
Iteration 121/1000 | Loss: 0.00003335
Iteration 122/1000 | Loss: 0.00004738
Iteration 123/1000 | Loss: 0.00003564
Iteration 124/1000 | Loss: 0.00004812
Iteration 125/1000 | Loss: 0.00003680
Iteration 126/1000 | Loss: 0.00004466
Iteration 127/1000 | Loss: 0.00003548
Iteration 128/1000 | Loss: 0.00003846
Iteration 129/1000 | Loss: 0.00003435
Iteration 130/1000 | Loss: 0.00003825
Iteration 131/1000 | Loss: 0.00003269
Iteration 132/1000 | Loss: 0.00003915
Iteration 133/1000 | Loss: 0.00003443
Iteration 134/1000 | Loss: 0.00003795
Iteration 135/1000 | Loss: 0.00003333
Iteration 136/1000 | Loss: 0.00003848
Iteration 137/1000 | Loss: 0.00003343
Iteration 138/1000 | Loss: 0.00003343
Iteration 139/1000 | Loss: 0.00004680
Iteration 140/1000 | Loss: 0.00003332
Iteration 141/1000 | Loss: 0.00004151
Iteration 142/1000 | Loss: 0.00003326
Iteration 143/1000 | Loss: 0.00004578
Iteration 144/1000 | Loss: 0.00003256
Iteration 145/1000 | Loss: 0.00003806
Iteration 146/1000 | Loss: 0.00003282
Iteration 147/1000 | Loss: 0.00004541
Iteration 148/1000 | Loss: 0.00003339
Iteration 149/1000 | Loss: 0.00004762
Iteration 150/1000 | Loss: 0.00003318
Iteration 151/1000 | Loss: 0.00004010
Iteration 152/1000 | Loss: 0.00003460
Iteration 153/1000 | Loss: 0.00004131
Iteration 154/1000 | Loss: 0.00003408
Iteration 155/1000 | Loss: 0.00005165
Iteration 156/1000 | Loss: 0.00003377
Iteration 157/1000 | Loss: 0.00005030
Iteration 158/1000 | Loss: 0.00003884
Iteration 159/1000 | Loss: 0.00003884
Iteration 160/1000 | Loss: 0.00006807
Iteration 161/1000 | Loss: 0.00008184
Iteration 162/1000 | Loss: 0.00005914
Iteration 163/1000 | Loss: 0.00004195
Iteration 164/1000 | Loss: 0.00004073
Iteration 165/1000 | Loss: 0.00003779
Iteration 166/1000 | Loss: 0.00004192
Iteration 167/1000 | Loss: 0.00004501
Iteration 168/1000 | Loss: 0.00004086
Iteration 169/1000 | Loss: 0.00003628
Iteration 170/1000 | Loss: 0.00006102
Iteration 171/1000 | Loss: 0.00003678
Iteration 172/1000 | Loss: 0.00004569
Iteration 173/1000 | Loss: 0.00004927
Iteration 174/1000 | Loss: 0.00006533
Iteration 175/1000 | Loss: 0.00003807
Iteration 176/1000 | Loss: 0.00004638
Iteration 177/1000 | Loss: 0.00003795
Iteration 178/1000 | Loss: 0.00004633
Iteration 179/1000 | Loss: 0.00003789
Iteration 180/1000 | Loss: 0.00004464
Iteration 181/1000 | Loss: 0.00003769
Iteration 182/1000 | Loss: 0.00004651
Iteration 183/1000 | Loss: 0.00003865
Iteration 184/1000 | Loss: 0.00004408
Iteration 185/1000 | Loss: 0.00004003
Iteration 186/1000 | Loss: 0.00004506
Iteration 187/1000 | Loss: 0.00003964
Iteration 188/1000 | Loss: 0.00004195
Iteration 189/1000 | Loss: 0.00003855
Iteration 190/1000 | Loss: 0.00003879
Iteration 191/1000 | Loss: 0.00003799
Iteration 192/1000 | Loss: 0.00003758
Iteration 193/1000 | Loss: 0.00004677
Iteration 194/1000 | Loss: 0.00003874
Iteration 195/1000 | Loss: 0.00003886
Iteration 196/1000 | Loss: 0.00004597
Iteration 197/1000 | Loss: 0.00004275
Iteration 198/1000 | Loss: 0.00003476
Iteration 199/1000 | Loss: 0.00003120
Iteration 200/1000 | Loss: 0.00003090
Iteration 201/1000 | Loss: 0.00003086
Iteration 202/1000 | Loss: 0.00003086
Iteration 203/1000 | Loss: 0.00003086
Iteration 204/1000 | Loss: 0.00003086
Iteration 205/1000 | Loss: 0.00003086
Iteration 206/1000 | Loss: 0.00003086
Iteration 207/1000 | Loss: 0.00003086
Iteration 208/1000 | Loss: 0.00003086
Iteration 209/1000 | Loss: 0.00003086
Iteration 210/1000 | Loss: 0.00003086
Iteration 211/1000 | Loss: 0.00003086
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 211. Stopping optimization.
Last 5 losses: [3.0862505809636787e-05, 3.0862505809636787e-05, 3.0862505809636787e-05, 3.0862505809636787e-05, 3.0862505809636787e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.0862505809636787e-05

Optimization complete. Final v2v error: 4.090152263641357 mm

Highest mean error: 20.5598201751709 mm for frame 50

Lowest mean error: 3.0527870655059814 mm for frame 181

Saving results

Total time: 307.16390562057495
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_us_1259/0003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_1259/0003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_1259/0003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00939296
Iteration 2/25 | Loss: 0.00167279
Iteration 3/25 | Loss: 0.00150819
Iteration 4/25 | Loss: 0.00146793
Iteration 5/25 | Loss: 0.00145991
Iteration 6/25 | Loss: 0.00145911
Iteration 7/25 | Loss: 0.00145911
Iteration 8/25 | Loss: 0.00145911
Iteration 9/25 | Loss: 0.00145911
Iteration 10/25 | Loss: 0.00145911
Iteration 11/25 | Loss: 0.00145911
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0014591059880331159, 0.0014591059880331159, 0.0014591059880331159, 0.0014591059880331159, 0.0014591059880331159]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014591059880331159

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.16850412
Iteration 2/25 | Loss: 0.00280564
Iteration 3/25 | Loss: 0.00280562
Iteration 4/25 | Loss: 0.00280562
Iteration 5/25 | Loss: 0.00280562
Iteration 6/25 | Loss: 0.00280562
Iteration 7/25 | Loss: 0.00280562
Iteration 8/25 | Loss: 0.00280562
Iteration 9/25 | Loss: 0.00280562
Iteration 10/25 | Loss: 0.00280562
Iteration 11/25 | Loss: 0.00280562
Iteration 12/25 | Loss: 0.00280562
Iteration 13/25 | Loss: 0.00280562
Iteration 14/25 | Loss: 0.00280562
Iteration 15/25 | Loss: 0.00280562
Iteration 16/25 | Loss: 0.00280562
Iteration 17/25 | Loss: 0.00280562
Iteration 18/25 | Loss: 0.00280562
Iteration 19/25 | Loss: 0.00280562
Iteration 20/25 | Loss: 0.00280562
Iteration 21/25 | Loss: 0.00280562
Iteration 22/25 | Loss: 0.00280562
Iteration 23/25 | Loss: 0.00280562
Iteration 24/25 | Loss: 0.00280562
Iteration 25/25 | Loss: 0.00280562

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00280562
Iteration 2/1000 | Loss: 0.00007068
Iteration 3/1000 | Loss: 0.00005081
Iteration 4/1000 | Loss: 0.00004606
Iteration 5/1000 | Loss: 0.00004299
Iteration 6/1000 | Loss: 0.00004102
Iteration 7/1000 | Loss: 0.00003927
Iteration 8/1000 | Loss: 0.00003833
Iteration 9/1000 | Loss: 0.00003799
Iteration 10/1000 | Loss: 0.00003770
Iteration 11/1000 | Loss: 0.00003735
Iteration 12/1000 | Loss: 0.00003721
Iteration 13/1000 | Loss: 0.00003715
Iteration 14/1000 | Loss: 0.00003713
Iteration 15/1000 | Loss: 0.00003712
Iteration 16/1000 | Loss: 0.00003711
Iteration 17/1000 | Loss: 0.00003711
Iteration 18/1000 | Loss: 0.00003710
Iteration 19/1000 | Loss: 0.00003710
Iteration 20/1000 | Loss: 0.00003709
Iteration 21/1000 | Loss: 0.00003704
Iteration 22/1000 | Loss: 0.00003704
Iteration 23/1000 | Loss: 0.00003704
Iteration 24/1000 | Loss: 0.00003704
Iteration 25/1000 | Loss: 0.00003704
Iteration 26/1000 | Loss: 0.00003703
Iteration 27/1000 | Loss: 0.00003703
Iteration 28/1000 | Loss: 0.00003703
Iteration 29/1000 | Loss: 0.00003703
Iteration 30/1000 | Loss: 0.00003703
Iteration 31/1000 | Loss: 0.00003703
Iteration 32/1000 | Loss: 0.00003703
Iteration 33/1000 | Loss: 0.00003702
Iteration 34/1000 | Loss: 0.00003701
Iteration 35/1000 | Loss: 0.00003701
Iteration 36/1000 | Loss: 0.00003701
Iteration 37/1000 | Loss: 0.00003701
Iteration 38/1000 | Loss: 0.00003700
Iteration 39/1000 | Loss: 0.00003700
Iteration 40/1000 | Loss: 0.00003699
Iteration 41/1000 | Loss: 0.00003699
Iteration 42/1000 | Loss: 0.00003699
Iteration 43/1000 | Loss: 0.00003698
Iteration 44/1000 | Loss: 0.00003698
Iteration 45/1000 | Loss: 0.00003698
Iteration 46/1000 | Loss: 0.00003698
Iteration 47/1000 | Loss: 0.00003697
Iteration 48/1000 | Loss: 0.00003697
Iteration 49/1000 | Loss: 0.00003697
Iteration 50/1000 | Loss: 0.00003697
Iteration 51/1000 | Loss: 0.00003697
Iteration 52/1000 | Loss: 0.00003697
Iteration 53/1000 | Loss: 0.00003697
Iteration 54/1000 | Loss: 0.00003697
Iteration 55/1000 | Loss: 0.00003696
Iteration 56/1000 | Loss: 0.00003696
Iteration 57/1000 | Loss: 0.00003696
Iteration 58/1000 | Loss: 0.00003694
Iteration 59/1000 | Loss: 0.00003694
Iteration 60/1000 | Loss: 0.00003694
Iteration 61/1000 | Loss: 0.00003694
Iteration 62/1000 | Loss: 0.00003694
Iteration 63/1000 | Loss: 0.00003694
Iteration 64/1000 | Loss: 0.00003693
Iteration 65/1000 | Loss: 0.00003693
Iteration 66/1000 | Loss: 0.00003693
Iteration 67/1000 | Loss: 0.00003693
Iteration 68/1000 | Loss: 0.00003693
Iteration 69/1000 | Loss: 0.00003693
Iteration 70/1000 | Loss: 0.00003693
Iteration 71/1000 | Loss: 0.00003692
Iteration 72/1000 | Loss: 0.00003692
Iteration 73/1000 | Loss: 0.00003692
Iteration 74/1000 | Loss: 0.00003691
Iteration 75/1000 | Loss: 0.00003691
Iteration 76/1000 | Loss: 0.00003691
Iteration 77/1000 | Loss: 0.00003691
Iteration 78/1000 | Loss: 0.00003690
Iteration 79/1000 | Loss: 0.00003690
Iteration 80/1000 | Loss: 0.00003690
Iteration 81/1000 | Loss: 0.00003689
Iteration 82/1000 | Loss: 0.00003689
Iteration 83/1000 | Loss: 0.00003689
Iteration 84/1000 | Loss: 0.00003689
Iteration 85/1000 | Loss: 0.00003689
Iteration 86/1000 | Loss: 0.00003689
Iteration 87/1000 | Loss: 0.00003689
Iteration 88/1000 | Loss: 0.00003689
Iteration 89/1000 | Loss: 0.00003688
Iteration 90/1000 | Loss: 0.00003688
Iteration 91/1000 | Loss: 0.00003687
Iteration 92/1000 | Loss: 0.00003687
Iteration 93/1000 | Loss: 0.00003687
Iteration 94/1000 | Loss: 0.00003687
Iteration 95/1000 | Loss: 0.00003686
Iteration 96/1000 | Loss: 0.00003686
Iteration 97/1000 | Loss: 0.00003686
Iteration 98/1000 | Loss: 0.00003686
Iteration 99/1000 | Loss: 0.00003686
Iteration 100/1000 | Loss: 0.00003686
Iteration 101/1000 | Loss: 0.00003684
Iteration 102/1000 | Loss: 0.00003684
Iteration 103/1000 | Loss: 0.00003684
Iteration 104/1000 | Loss: 0.00003683
Iteration 105/1000 | Loss: 0.00003683
Iteration 106/1000 | Loss: 0.00003683
Iteration 107/1000 | Loss: 0.00003683
Iteration 108/1000 | Loss: 0.00003683
Iteration 109/1000 | Loss: 0.00003683
Iteration 110/1000 | Loss: 0.00003683
Iteration 111/1000 | Loss: 0.00003682
Iteration 112/1000 | Loss: 0.00003682
Iteration 113/1000 | Loss: 0.00003682
Iteration 114/1000 | Loss: 0.00003682
Iteration 115/1000 | Loss: 0.00003682
Iteration 116/1000 | Loss: 0.00003682
Iteration 117/1000 | Loss: 0.00003682
Iteration 118/1000 | Loss: 0.00003682
Iteration 119/1000 | Loss: 0.00003682
Iteration 120/1000 | Loss: 0.00003682
Iteration 121/1000 | Loss: 0.00003682
Iteration 122/1000 | Loss: 0.00003682
Iteration 123/1000 | Loss: 0.00003682
Iteration 124/1000 | Loss: 0.00003681
Iteration 125/1000 | Loss: 0.00003681
Iteration 126/1000 | Loss: 0.00003681
Iteration 127/1000 | Loss: 0.00003681
Iteration 128/1000 | Loss: 0.00003681
Iteration 129/1000 | Loss: 0.00003681
Iteration 130/1000 | Loss: 0.00003681
Iteration 131/1000 | Loss: 0.00003681
Iteration 132/1000 | Loss: 0.00003681
Iteration 133/1000 | Loss: 0.00003681
Iteration 134/1000 | Loss: 0.00003681
Iteration 135/1000 | Loss: 0.00003681
Iteration 136/1000 | Loss: 0.00003681
Iteration 137/1000 | Loss: 0.00003681
Iteration 138/1000 | Loss: 0.00003681
Iteration 139/1000 | Loss: 0.00003681
Iteration 140/1000 | Loss: 0.00003681
Iteration 141/1000 | Loss: 0.00003681
Iteration 142/1000 | Loss: 0.00003681
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 142. Stopping optimization.
Last 5 losses: [3.680972440633923e-05, 3.680972440633923e-05, 3.680972440633923e-05, 3.680972440633923e-05, 3.680972440633923e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.680972440633923e-05

Optimization complete. Final v2v error: 5.081825256347656 mm

Highest mean error: 5.324678897857666 mm for frame 67

Lowest mean error: 4.886041164398193 mm for frame 129

Saving results

Total time: 34.82147145271301
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_us_1259/0019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_1259/0019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_1259/0019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01149590
Iteration 2/25 | Loss: 0.01149590
Iteration 3/25 | Loss: 0.01149590
Iteration 4/25 | Loss: 0.01149590
Iteration 5/25 | Loss: 0.01149590
Iteration 6/25 | Loss: 0.01149589
Iteration 7/25 | Loss: 0.01149589
Iteration 8/25 | Loss: 0.01149589
Iteration 9/25 | Loss: 0.01149589
Iteration 10/25 | Loss: 0.00300353
Iteration 11/25 | Loss: 0.00190228
Iteration 12/25 | Loss: 0.00192417
Iteration 13/25 | Loss: 0.00162904
Iteration 14/25 | Loss: 0.00161301
Iteration 15/25 | Loss: 0.00151912
Iteration 16/25 | Loss: 0.00149351
Iteration 17/25 | Loss: 0.00146929
Iteration 18/25 | Loss: 0.00146028
Iteration 19/25 | Loss: 0.00143332
Iteration 20/25 | Loss: 0.00142092
Iteration 21/25 | Loss: 0.00141549
Iteration 22/25 | Loss: 0.00141453
Iteration 23/25 | Loss: 0.00140928
Iteration 24/25 | Loss: 0.00140198
Iteration 25/25 | Loss: 0.00139857

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.56174982
Iteration 2/25 | Loss: 0.00702268
Iteration 3/25 | Loss: 0.00551156
Iteration 4/25 | Loss: 0.00542568
Iteration 5/25 | Loss: 0.00542566
Iteration 6/25 | Loss: 0.00542566
Iteration 7/25 | Loss: 0.00542566
Iteration 8/25 | Loss: 0.00542566
Iteration 9/25 | Loss: 0.00542566
Iteration 10/25 | Loss: 0.00542566
Iteration 11/25 | Loss: 0.00542566
Iteration 12/25 | Loss: 0.00542566
Iteration 13/25 | Loss: 0.00542566
Iteration 14/25 | Loss: 0.00542566
Iteration 15/25 | Loss: 0.00542566
Iteration 16/25 | Loss: 0.00542566
Iteration 17/25 | Loss: 0.00542566
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.005425657145678997, 0.005425657145678997, 0.005425657145678997, 0.005425657145678997, 0.005425657145678997]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.005425657145678997

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00542566
Iteration 2/1000 | Loss: 0.00200335
Iteration 3/1000 | Loss: 0.00211936
Iteration 4/1000 | Loss: 0.00115855
Iteration 5/1000 | Loss: 0.00191116
Iteration 6/1000 | Loss: 0.00391643
Iteration 7/1000 | Loss: 0.00142258
Iteration 8/1000 | Loss: 0.00332835
Iteration 9/1000 | Loss: 0.00164302
Iteration 10/1000 | Loss: 0.00300547
Iteration 11/1000 | Loss: 0.00112572
Iteration 12/1000 | Loss: 0.00116165
Iteration 13/1000 | Loss: 0.00672830
Iteration 14/1000 | Loss: 0.00178920
Iteration 15/1000 | Loss: 0.00286282
Iteration 16/1000 | Loss: 0.00413648
Iteration 17/1000 | Loss: 0.00141159
Iteration 18/1000 | Loss: 0.00415129
Iteration 19/1000 | Loss: 0.00395421
Iteration 20/1000 | Loss: 0.00090318
Iteration 21/1000 | Loss: 0.00347369
Iteration 22/1000 | Loss: 0.00172965
Iteration 23/1000 | Loss: 0.00312877
Iteration 24/1000 | Loss: 0.00686461
Iteration 25/1000 | Loss: 0.00174158
Iteration 26/1000 | Loss: 0.00256223
Iteration 27/1000 | Loss: 0.00381492
Iteration 28/1000 | Loss: 0.00080799
Iteration 29/1000 | Loss: 0.00184810
Iteration 30/1000 | Loss: 0.00334656
Iteration 31/1000 | Loss: 0.00142749
Iteration 32/1000 | Loss: 0.00171743
Iteration 33/1000 | Loss: 0.00256325
Iteration 34/1000 | Loss: 0.00587412
Iteration 35/1000 | Loss: 0.00130357
Iteration 36/1000 | Loss: 0.00122747
Iteration 37/1000 | Loss: 0.00068555
Iteration 38/1000 | Loss: 0.00228473
Iteration 39/1000 | Loss: 0.00136235
Iteration 40/1000 | Loss: 0.00318570
Iteration 41/1000 | Loss: 0.00128915
Iteration 42/1000 | Loss: 0.00231064
Iteration 43/1000 | Loss: 0.00357447
Iteration 44/1000 | Loss: 0.00954866
Iteration 45/1000 | Loss: 0.00329930
Iteration 46/1000 | Loss: 0.00650974
Iteration 47/1000 | Loss: 0.00297684
Iteration 48/1000 | Loss: 0.00170589
Iteration 49/1000 | Loss: 0.00236783
Iteration 50/1000 | Loss: 0.00253971
Iteration 51/1000 | Loss: 0.00362009
Iteration 52/1000 | Loss: 0.00264435
Iteration 53/1000 | Loss: 0.00290047
Iteration 54/1000 | Loss: 0.00320632
Iteration 55/1000 | Loss: 0.00412025
Iteration 56/1000 | Loss: 0.00300735
Iteration 57/1000 | Loss: 0.00275443
Iteration 58/1000 | Loss: 0.00185989
Iteration 59/1000 | Loss: 0.00125755
Iteration 60/1000 | Loss: 0.00137321
Iteration 61/1000 | Loss: 0.00084164
Iteration 62/1000 | Loss: 0.00055385
Iteration 63/1000 | Loss: 0.00053788
Iteration 64/1000 | Loss: 0.00162804
Iteration 65/1000 | Loss: 0.00317741
Iteration 66/1000 | Loss: 0.00289166
Iteration 67/1000 | Loss: 0.00105637
Iteration 68/1000 | Loss: 0.00153917
Iteration 69/1000 | Loss: 0.00162432
Iteration 70/1000 | Loss: 0.00117063
Iteration 71/1000 | Loss: 0.00166929
Iteration 72/1000 | Loss: 0.00179669
Iteration 73/1000 | Loss: 0.00055874
Iteration 74/1000 | Loss: 0.00102001
Iteration 75/1000 | Loss: 0.00038319
Iteration 76/1000 | Loss: 0.00097033
Iteration 77/1000 | Loss: 0.00112659
Iteration 78/1000 | Loss: 0.00118725
Iteration 79/1000 | Loss: 0.00022565
Iteration 80/1000 | Loss: 0.00014084
Iteration 81/1000 | Loss: 0.00026191
Iteration 82/1000 | Loss: 0.00127433
Iteration 83/1000 | Loss: 0.00082962
Iteration 84/1000 | Loss: 0.00074782
Iteration 85/1000 | Loss: 0.00118691
Iteration 86/1000 | Loss: 0.00244037
Iteration 87/1000 | Loss: 0.00210032
Iteration 88/1000 | Loss: 0.00205217
Iteration 89/1000 | Loss: 0.00081277
Iteration 90/1000 | Loss: 0.00181689
Iteration 91/1000 | Loss: 0.00017091
Iteration 92/1000 | Loss: 0.00020156
Iteration 93/1000 | Loss: 0.00023015
Iteration 94/1000 | Loss: 0.00082303
Iteration 95/1000 | Loss: 0.00053511
Iteration 96/1000 | Loss: 0.00056978
Iteration 97/1000 | Loss: 0.00152170
Iteration 98/1000 | Loss: 0.00192839
Iteration 99/1000 | Loss: 0.00094840
Iteration 100/1000 | Loss: 0.00154403
Iteration 101/1000 | Loss: 0.00228898
Iteration 102/1000 | Loss: 0.00149108
Iteration 103/1000 | Loss: 0.00206696
Iteration 104/1000 | Loss: 0.00119005
Iteration 105/1000 | Loss: 0.00108252
Iteration 106/1000 | Loss: 0.00105864
Iteration 107/1000 | Loss: 0.00071403
Iteration 108/1000 | Loss: 0.00112415
Iteration 109/1000 | Loss: 0.00110019
Iteration 110/1000 | Loss: 0.00037127
Iteration 111/1000 | Loss: 0.00039979
Iteration 112/1000 | Loss: 0.00033232
Iteration 113/1000 | Loss: 0.00036525
Iteration 114/1000 | Loss: 0.00146052
Iteration 115/1000 | Loss: 0.00185014
Iteration 116/1000 | Loss: 0.00067175
Iteration 117/1000 | Loss: 0.00093534
Iteration 118/1000 | Loss: 0.00062170
Iteration 119/1000 | Loss: 0.00137863
Iteration 120/1000 | Loss: 0.00105704
Iteration 121/1000 | Loss: 0.00134651
Iteration 122/1000 | Loss: 0.00091565
Iteration 123/1000 | Loss: 0.00020626
Iteration 124/1000 | Loss: 0.00028819
Iteration 125/1000 | Loss: 0.00082924
Iteration 126/1000 | Loss: 0.00053426
Iteration 127/1000 | Loss: 0.00067703
Iteration 128/1000 | Loss: 0.00064470
Iteration 129/1000 | Loss: 0.00043745
Iteration 130/1000 | Loss: 0.00109427
Iteration 131/1000 | Loss: 0.00060596
Iteration 132/1000 | Loss: 0.00080213
Iteration 133/1000 | Loss: 0.00084750
Iteration 134/1000 | Loss: 0.00211179
Iteration 135/1000 | Loss: 0.00172787
Iteration 136/1000 | Loss: 0.00082723
Iteration 137/1000 | Loss: 0.00257307
Iteration 138/1000 | Loss: 0.00085513
Iteration 139/1000 | Loss: 0.00118315
Iteration 140/1000 | Loss: 0.00058226
Iteration 141/1000 | Loss: 0.00105681
Iteration 142/1000 | Loss: 0.00095059
Iteration 143/1000 | Loss: 0.00097466
Iteration 144/1000 | Loss: 0.00115174
Iteration 145/1000 | Loss: 0.00100686
Iteration 146/1000 | Loss: 0.00100141
Iteration 147/1000 | Loss: 0.00127194
Iteration 148/1000 | Loss: 0.00060302
Iteration 149/1000 | Loss: 0.00015426
Iteration 150/1000 | Loss: 0.00056924
Iteration 151/1000 | Loss: 0.00041371
Iteration 152/1000 | Loss: 0.00099719
Iteration 153/1000 | Loss: 0.00077909
Iteration 154/1000 | Loss: 0.00101883
Iteration 155/1000 | Loss: 0.00045291
Iteration 156/1000 | Loss: 0.00051990
Iteration 157/1000 | Loss: 0.00015104
Iteration 158/1000 | Loss: 0.00033204
Iteration 159/1000 | Loss: 0.00067780
Iteration 160/1000 | Loss: 0.00050787
Iteration 161/1000 | Loss: 0.00045015
Iteration 162/1000 | Loss: 0.00069426
Iteration 163/1000 | Loss: 0.00062127
Iteration 164/1000 | Loss: 0.00180687
Iteration 165/1000 | Loss: 0.00051377
Iteration 166/1000 | Loss: 0.00110010
Iteration 167/1000 | Loss: 0.00068404
Iteration 168/1000 | Loss: 0.00024811
Iteration 169/1000 | Loss: 0.00096667
Iteration 170/1000 | Loss: 0.00052315
Iteration 171/1000 | Loss: 0.00063640
Iteration 172/1000 | Loss: 0.00065439
Iteration 173/1000 | Loss: 0.00091731
Iteration 174/1000 | Loss: 0.00067565
Iteration 175/1000 | Loss: 0.00036362
Iteration 176/1000 | Loss: 0.00026608
Iteration 177/1000 | Loss: 0.00054295
Iteration 178/1000 | Loss: 0.00050883
Iteration 179/1000 | Loss: 0.00070486
Iteration 180/1000 | Loss: 0.00132445
Iteration 181/1000 | Loss: 0.00127023
Iteration 182/1000 | Loss: 0.00178151
Iteration 183/1000 | Loss: 0.00075913
Iteration 184/1000 | Loss: 0.00149887
Iteration 185/1000 | Loss: 0.00096996
Iteration 186/1000 | Loss: 0.00201673
Iteration 187/1000 | Loss: 0.00132178
Iteration 188/1000 | Loss: 0.00149908
Iteration 189/1000 | Loss: 0.00116826
Iteration 190/1000 | Loss: 0.00125427
Iteration 191/1000 | Loss: 0.00201339
Iteration 192/1000 | Loss: 0.00098006
Iteration 193/1000 | Loss: 0.00181284
Iteration 194/1000 | Loss: 0.00151955
Iteration 195/1000 | Loss: 0.00201493
Iteration 196/1000 | Loss: 0.00179961
Iteration 197/1000 | Loss: 0.00199260
Iteration 198/1000 | Loss: 0.00143133
Iteration 199/1000 | Loss: 0.00176048
Iteration 200/1000 | Loss: 0.00127823
Iteration 201/1000 | Loss: 0.00127064
Iteration 202/1000 | Loss: 0.00055477
Iteration 203/1000 | Loss: 0.00038996
Iteration 204/1000 | Loss: 0.00040421
Iteration 205/1000 | Loss: 0.00044764
Iteration 206/1000 | Loss: 0.00091111
Iteration 207/1000 | Loss: 0.00102796
Iteration 208/1000 | Loss: 0.00068591
Iteration 209/1000 | Loss: 0.00077859
Iteration 210/1000 | Loss: 0.00076447
Iteration 211/1000 | Loss: 0.00107735
Iteration 212/1000 | Loss: 0.00125035
Iteration 213/1000 | Loss: 0.00100901
Iteration 214/1000 | Loss: 0.00076455
Iteration 215/1000 | Loss: 0.00105654
Iteration 216/1000 | Loss: 0.00216968
Iteration 217/1000 | Loss: 0.00024391
Iteration 218/1000 | Loss: 0.00035838
Iteration 219/1000 | Loss: 0.00057711
Iteration 220/1000 | Loss: 0.00052627
Iteration 221/1000 | Loss: 0.00048239
Iteration 222/1000 | Loss: 0.00064927
Iteration 223/1000 | Loss: 0.00078509
Iteration 224/1000 | Loss: 0.00042074
Iteration 225/1000 | Loss: 0.00060417
Iteration 226/1000 | Loss: 0.00067352
Iteration 227/1000 | Loss: 0.00085028
Iteration 228/1000 | Loss: 0.00048148
Iteration 229/1000 | Loss: 0.00066080
Iteration 230/1000 | Loss: 0.00070536
Iteration 231/1000 | Loss: 0.00122234
Iteration 232/1000 | Loss: 0.00109064
Iteration 233/1000 | Loss: 0.00181107
Iteration 234/1000 | Loss: 0.00094858
Iteration 235/1000 | Loss: 0.00163960
Iteration 236/1000 | Loss: 0.00108454
Iteration 237/1000 | Loss: 0.00107645
Iteration 238/1000 | Loss: 0.00092579
Iteration 239/1000 | Loss: 0.00112701
Iteration 240/1000 | Loss: 0.00050967
Iteration 241/1000 | Loss: 0.00085960
Iteration 242/1000 | Loss: 0.00090945
Iteration 243/1000 | Loss: 0.00080616
Iteration 244/1000 | Loss: 0.00067107
Iteration 245/1000 | Loss: 0.00097698
Iteration 246/1000 | Loss: 0.00091590
Iteration 247/1000 | Loss: 0.00079987
Iteration 248/1000 | Loss: 0.00062659
Iteration 249/1000 | Loss: 0.00049189
Iteration 250/1000 | Loss: 0.00022310
Iteration 251/1000 | Loss: 0.00008375
Iteration 252/1000 | Loss: 0.00044873
Iteration 253/1000 | Loss: 0.00053569
Iteration 254/1000 | Loss: 0.00032053
Iteration 255/1000 | Loss: 0.00031089
Iteration 256/1000 | Loss: 0.00039748
Iteration 257/1000 | Loss: 0.00028529
Iteration 258/1000 | Loss: 0.00029479
Iteration 259/1000 | Loss: 0.00039958
Iteration 260/1000 | Loss: 0.00026644
Iteration 261/1000 | Loss: 0.00043196
Iteration 262/1000 | Loss: 0.00027560
Iteration 263/1000 | Loss: 0.00008531
Iteration 264/1000 | Loss: 0.00007569
Iteration 265/1000 | Loss: 0.00017327
Iteration 266/1000 | Loss: 0.00046931
Iteration 267/1000 | Loss: 0.00073178
Iteration 268/1000 | Loss: 0.00014513
Iteration 269/1000 | Loss: 0.00081433
Iteration 270/1000 | Loss: 0.00013799
Iteration 271/1000 | Loss: 0.00061694
Iteration 272/1000 | Loss: 0.00071716
Iteration 273/1000 | Loss: 0.00067260
Iteration 274/1000 | Loss: 0.00074248
Iteration 275/1000 | Loss: 0.00050138
Iteration 276/1000 | Loss: 0.00035702
Iteration 277/1000 | Loss: 0.00009283
Iteration 278/1000 | Loss: 0.00040359
Iteration 279/1000 | Loss: 0.00053307
Iteration 280/1000 | Loss: 0.00043205
Iteration 281/1000 | Loss: 0.00047453
Iteration 282/1000 | Loss: 0.00050695
Iteration 283/1000 | Loss: 0.00069092
Iteration 284/1000 | Loss: 0.00065369
Iteration 285/1000 | Loss: 0.00040850
Iteration 286/1000 | Loss: 0.00022225
Iteration 287/1000 | Loss: 0.00043121
Iteration 288/1000 | Loss: 0.00012153
Iteration 289/1000 | Loss: 0.00022802
Iteration 290/1000 | Loss: 0.00023574
Iteration 291/1000 | Loss: 0.00009747
Iteration 292/1000 | Loss: 0.00009440
Iteration 293/1000 | Loss: 0.00030927
Iteration 294/1000 | Loss: 0.00020604
Iteration 295/1000 | Loss: 0.00020761
Iteration 296/1000 | Loss: 0.00049582
Iteration 297/1000 | Loss: 0.00013624
Iteration 298/1000 | Loss: 0.00011581
Iteration 299/1000 | Loss: 0.00021226
Iteration 300/1000 | Loss: 0.00022881
Iteration 301/1000 | Loss: 0.00026391
Iteration 302/1000 | Loss: 0.00040006
Iteration 303/1000 | Loss: 0.00025735
Iteration 304/1000 | Loss: 0.00023706
Iteration 305/1000 | Loss: 0.00009157
Iteration 306/1000 | Loss: 0.00012670
Iteration 307/1000 | Loss: 0.00007233
Iteration 308/1000 | Loss: 0.00007514
Iteration 309/1000 | Loss: 0.00021105
Iteration 310/1000 | Loss: 0.00017106
Iteration 311/1000 | Loss: 0.00048584
Iteration 312/1000 | Loss: 0.00051241
Iteration 313/1000 | Loss: 0.00012277
Iteration 314/1000 | Loss: 0.00081581
Iteration 315/1000 | Loss: 0.00064409
Iteration 316/1000 | Loss: 0.00074034
Iteration 317/1000 | Loss: 0.00087092
Iteration 318/1000 | Loss: 0.00008917
Iteration 319/1000 | Loss: 0.00008826
Iteration 320/1000 | Loss: 0.00006445
Iteration 321/1000 | Loss: 0.00024896
Iteration 322/1000 | Loss: 0.00017604
Iteration 323/1000 | Loss: 0.00018139
Iteration 324/1000 | Loss: 0.00010479
Iteration 325/1000 | Loss: 0.00049970
Iteration 326/1000 | Loss: 0.00087850
Iteration 327/1000 | Loss: 0.00030044
Iteration 328/1000 | Loss: 0.00026941
Iteration 329/1000 | Loss: 0.00018596
Iteration 330/1000 | Loss: 0.00027363
Iteration 331/1000 | Loss: 0.00010393
Iteration 332/1000 | Loss: 0.00078926
Iteration 333/1000 | Loss: 0.00046237
Iteration 334/1000 | Loss: 0.00026933
Iteration 335/1000 | Loss: 0.00076347
Iteration 336/1000 | Loss: 0.00008943
Iteration 337/1000 | Loss: 0.00031976
Iteration 338/1000 | Loss: 0.00015884
Iteration 339/1000 | Loss: 0.00005442
Iteration 340/1000 | Loss: 0.00005116
Iteration 341/1000 | Loss: 0.00014489
Iteration 342/1000 | Loss: 0.00019577
Iteration 343/1000 | Loss: 0.00027767
Iteration 344/1000 | Loss: 0.00049251
Iteration 345/1000 | Loss: 0.00014803
Iteration 346/1000 | Loss: 0.00031422
Iteration 347/1000 | Loss: 0.00023724
Iteration 348/1000 | Loss: 0.00033670
Iteration 349/1000 | Loss: 0.00043916
Iteration 350/1000 | Loss: 0.00022951
Iteration 351/1000 | Loss: 0.00031912
Iteration 352/1000 | Loss: 0.00028618
Iteration 353/1000 | Loss: 0.00085350
Iteration 354/1000 | Loss: 0.00027076
Iteration 355/1000 | Loss: 0.00034538
Iteration 356/1000 | Loss: 0.00039216
Iteration 357/1000 | Loss: 0.00045659
Iteration 358/1000 | Loss: 0.00039640
Iteration 359/1000 | Loss: 0.00067862
Iteration 360/1000 | Loss: 0.00077701
Iteration 361/1000 | Loss: 0.00050417
Iteration 362/1000 | Loss: 0.00073845
Iteration 363/1000 | Loss: 0.00040100
Iteration 364/1000 | Loss: 0.00013749
Iteration 365/1000 | Loss: 0.00014084
Iteration 366/1000 | Loss: 0.00020620
Iteration 367/1000 | Loss: 0.00073811
Iteration 368/1000 | Loss: 0.00083736
Iteration 369/1000 | Loss: 0.00055425
Iteration 370/1000 | Loss: 0.00117256
Iteration 371/1000 | Loss: 0.00015259
Iteration 372/1000 | Loss: 0.00006724
Iteration 373/1000 | Loss: 0.00010795
Iteration 374/1000 | Loss: 0.00004474
Iteration 375/1000 | Loss: 0.00004133
Iteration 376/1000 | Loss: 0.00025254
Iteration 377/1000 | Loss: 0.00004095
Iteration 378/1000 | Loss: 0.00003967
Iteration 379/1000 | Loss: 0.00009067
Iteration 380/1000 | Loss: 0.00014584
Iteration 381/1000 | Loss: 0.00005345
Iteration 382/1000 | Loss: 0.00008816
Iteration 383/1000 | Loss: 0.00012653
Iteration 384/1000 | Loss: 0.00010856
Iteration 385/1000 | Loss: 0.00006914
Iteration 386/1000 | Loss: 0.00007969
Iteration 387/1000 | Loss: 0.00005236
Iteration 388/1000 | Loss: 0.00004273
Iteration 389/1000 | Loss: 0.00004718
Iteration 390/1000 | Loss: 0.00004549
Iteration 391/1000 | Loss: 0.00004381
Iteration 392/1000 | Loss: 0.00004179
Iteration 393/1000 | Loss: 0.00004218
Iteration 394/1000 | Loss: 0.00004156
Iteration 395/1000 | Loss: 0.00004419
Iteration 396/1000 | Loss: 0.00005161
Iteration 397/1000 | Loss: 0.00004458
Iteration 398/1000 | Loss: 0.00005693
Iteration 399/1000 | Loss: 0.00004619
Iteration 400/1000 | Loss: 0.00005581
Iteration 401/1000 | Loss: 0.00004702
Iteration 402/1000 | Loss: 0.00005270
Iteration 403/1000 | Loss: 0.00004448
Iteration 404/1000 | Loss: 0.00005127
Iteration 405/1000 | Loss: 0.00005129
Iteration 406/1000 | Loss: 0.00012430
Iteration 407/1000 | Loss: 0.00004389
Iteration 408/1000 | Loss: 0.00004903
Iteration 409/1000 | Loss: 0.00004848
Iteration 410/1000 | Loss: 0.00004706
Iteration 411/1000 | Loss: 0.00004104
Iteration 412/1000 | Loss: 0.00004389
Iteration 413/1000 | Loss: 0.00006745
Iteration 414/1000 | Loss: 0.00004284
Iteration 415/1000 | Loss: 0.00003941
Iteration 416/1000 | Loss: 0.00003879
Iteration 417/1000 | Loss: 0.00004801
Iteration 418/1000 | Loss: 0.00003871
Iteration 419/1000 | Loss: 0.00003871
Iteration 420/1000 | Loss: 0.00003870
Iteration 421/1000 | Loss: 0.00003870
Iteration 422/1000 | Loss: 0.00003869
Iteration 423/1000 | Loss: 0.00003864
Iteration 424/1000 | Loss: 0.00003864
Iteration 425/1000 | Loss: 0.00003863
Iteration 426/1000 | Loss: 0.00003863
Iteration 427/1000 | Loss: 0.00003863
Iteration 428/1000 | Loss: 0.00003863
Iteration 429/1000 | Loss: 0.00003863
Iteration 430/1000 | Loss: 0.00003863
Iteration 431/1000 | Loss: 0.00003863
Iteration 432/1000 | Loss: 0.00003862
Iteration 433/1000 | Loss: 0.00003862
Iteration 434/1000 | Loss: 0.00003862
Iteration 435/1000 | Loss: 0.00003861
Iteration 436/1000 | Loss: 0.00003861
Iteration 437/1000 | Loss: 0.00003861
Iteration 438/1000 | Loss: 0.00003860
Iteration 439/1000 | Loss: 0.00003860
Iteration 440/1000 | Loss: 0.00003860
Iteration 441/1000 | Loss: 0.00003859
Iteration 442/1000 | Loss: 0.00003859
Iteration 443/1000 | Loss: 0.00003858
Iteration 444/1000 | Loss: 0.00003857
Iteration 445/1000 | Loss: 0.00005941
Iteration 446/1000 | Loss: 0.00005941
Iteration 447/1000 | Loss: 0.00004209
Iteration 448/1000 | Loss: 0.00006842
Iteration 449/1000 | Loss: 0.00005340
Iteration 450/1000 | Loss: 0.00004735
Iteration 451/1000 | Loss: 0.00004538
Iteration 452/1000 | Loss: 0.00007566
Iteration 453/1000 | Loss: 0.00004125
Iteration 454/1000 | Loss: 0.00003899
Iteration 455/1000 | Loss: 0.00003863
Iteration 456/1000 | Loss: 0.00003860
Iteration 457/1000 | Loss: 0.00003860
Iteration 458/1000 | Loss: 0.00003858
Iteration 459/1000 | Loss: 0.00003857
Iteration 460/1000 | Loss: 0.00003856
Iteration 461/1000 | Loss: 0.00003856
Iteration 462/1000 | Loss: 0.00004448
Iteration 463/1000 | Loss: 0.00003852
Iteration 464/1000 | Loss: 0.00003851
Iteration 465/1000 | Loss: 0.00003851
Iteration 466/1000 | Loss: 0.00003851
Iteration 467/1000 | Loss: 0.00003851
Iteration 468/1000 | Loss: 0.00003851
Iteration 469/1000 | Loss: 0.00003851
Iteration 470/1000 | Loss: 0.00003851
Iteration 471/1000 | Loss: 0.00003851
Iteration 472/1000 | Loss: 0.00003851
Iteration 473/1000 | Loss: 0.00003851
Iteration 474/1000 | Loss: 0.00003851
Iteration 475/1000 | Loss: 0.00003850
Iteration 476/1000 | Loss: 0.00003850
Iteration 477/1000 | Loss: 0.00003850
Iteration 478/1000 | Loss: 0.00003850
Iteration 479/1000 | Loss: 0.00003850
Iteration 480/1000 | Loss: 0.00003850
Iteration 481/1000 | Loss: 0.00003850
Iteration 482/1000 | Loss: 0.00003850
Iteration 483/1000 | Loss: 0.00003850
Iteration 484/1000 | Loss: 0.00003850
Iteration 485/1000 | Loss: 0.00003849
Iteration 486/1000 | Loss: 0.00003849
Iteration 487/1000 | Loss: 0.00003848
Iteration 488/1000 | Loss: 0.00003848
Iteration 489/1000 | Loss: 0.00003847
Iteration 490/1000 | Loss: 0.00008441
Iteration 491/1000 | Loss: 0.00004228
Iteration 492/1000 | Loss: 0.00004641
Iteration 493/1000 | Loss: 0.00004835
Iteration 494/1000 | Loss: 0.00004733
Iteration 495/1000 | Loss: 0.00007503
Iteration 496/1000 | Loss: 0.00004126
Iteration 497/1000 | Loss: 0.00004035
Iteration 498/1000 | Loss: 0.00004295
Iteration 499/1000 | Loss: 0.00003847
Iteration 500/1000 | Loss: 0.00003846
Iteration 501/1000 | Loss: 0.00003846
Iteration 502/1000 | Loss: 0.00003846
Iteration 503/1000 | Loss: 0.00003846
Iteration 504/1000 | Loss: 0.00003846
Iteration 505/1000 | Loss: 0.00003846
Iteration 506/1000 | Loss: 0.00003846
Iteration 507/1000 | Loss: 0.00003846
Iteration 508/1000 | Loss: 0.00003846
Iteration 509/1000 | Loss: 0.00003846
Iteration 510/1000 | Loss: 0.00003846
Iteration 511/1000 | Loss: 0.00003845
Iteration 512/1000 | Loss: 0.00003845
Iteration 513/1000 | Loss: 0.00003845
Iteration 514/1000 | Loss: 0.00003845
Iteration 515/1000 | Loss: 0.00003845
Iteration 516/1000 | Loss: 0.00003845
Iteration 517/1000 | Loss: 0.00003845
Iteration 518/1000 | Loss: 0.00003845
Iteration 519/1000 | Loss: 0.00003844
Iteration 520/1000 | Loss: 0.00003844
Iteration 521/1000 | Loss: 0.00003844
Iteration 522/1000 | Loss: 0.00003844
Iteration 523/1000 | Loss: 0.00003844
Iteration 524/1000 | Loss: 0.00003844
Iteration 525/1000 | Loss: 0.00003844
Iteration 526/1000 | Loss: 0.00003844
Iteration 527/1000 | Loss: 0.00003844
Iteration 528/1000 | Loss: 0.00003844
Iteration 529/1000 | Loss: 0.00003844
Iteration 530/1000 | Loss: 0.00003844
Iteration 531/1000 | Loss: 0.00003844
Iteration 532/1000 | Loss: 0.00003844
Iteration 533/1000 | Loss: 0.00003844
Iteration 534/1000 | Loss: 0.00003844
Iteration 535/1000 | Loss: 0.00003844
Iteration 536/1000 | Loss: 0.00003844
Iteration 537/1000 | Loss: 0.00003844
Iteration 538/1000 | Loss: 0.00003844
Iteration 539/1000 | Loss: 0.00003844
Iteration 540/1000 | Loss: 0.00003844
Iteration 541/1000 | Loss: 0.00003844
Iteration 542/1000 | Loss: 0.00003844
Iteration 543/1000 | Loss: 0.00003844
Iteration 544/1000 | Loss: 0.00003844
Iteration 545/1000 | Loss: 0.00003844
Iteration 546/1000 | Loss: 0.00003844
Iteration 547/1000 | Loss: 0.00003844
Iteration 548/1000 | Loss: 0.00003844
Iteration 549/1000 | Loss: 0.00003844
Iteration 550/1000 | Loss: 0.00003844
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 550. Stopping optimization.
Last 5 losses: [3.843911690637469e-05, 3.843911690637469e-05, 3.843911690637469e-05, 3.843911690637469e-05, 3.843911690637469e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.843911690637469e-05

Optimization complete. Final v2v error: 4.555559158325195 mm

Highest mean error: 22.63888168334961 mm for frame 185

Lowest mean error: 3.61250638961792 mm for frame 138

Saving results

Total time: 721.6042456626892
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_us_1259/0008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_1259/0008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_1259/0008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00951752
Iteration 2/25 | Loss: 0.00143585
Iteration 3/25 | Loss: 0.00129642
Iteration 4/25 | Loss: 0.00127883
Iteration 5/25 | Loss: 0.00127381
Iteration 6/25 | Loss: 0.00127247
Iteration 7/25 | Loss: 0.00127247
Iteration 8/25 | Loss: 0.00127247
Iteration 9/25 | Loss: 0.00127247
Iteration 10/25 | Loss: 0.00127247
Iteration 11/25 | Loss: 0.00127247
Iteration 12/25 | Loss: 0.00127247
Iteration 13/25 | Loss: 0.00127247
Iteration 14/25 | Loss: 0.00127247
Iteration 15/25 | Loss: 0.00127247
Iteration 16/25 | Loss: 0.00127247
Iteration 17/25 | Loss: 0.00127247
Iteration 18/25 | Loss: 0.00127247
Iteration 19/25 | Loss: 0.00127247
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0012724731350317597, 0.0012724731350317597, 0.0012724731350317597, 0.0012724731350317597, 0.0012724731350317597]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012724731350317597

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.62810016
Iteration 2/25 | Loss: 0.00249804
Iteration 3/25 | Loss: 0.00249802
Iteration 4/25 | Loss: 0.00249802
Iteration 5/25 | Loss: 0.00249802
Iteration 6/25 | Loss: 0.00249802
Iteration 7/25 | Loss: 0.00249802
Iteration 8/25 | Loss: 0.00249802
Iteration 9/25 | Loss: 0.00249801
Iteration 10/25 | Loss: 0.00249801
Iteration 11/25 | Loss: 0.00249801
Iteration 12/25 | Loss: 0.00249801
Iteration 13/25 | Loss: 0.00249801
Iteration 14/25 | Loss: 0.00249801
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.002498014597222209, 0.002498014597222209, 0.002498014597222209, 0.002498014597222209, 0.002498014597222209]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002498014597222209

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00249801
Iteration 2/1000 | Loss: 0.00003865
Iteration 3/1000 | Loss: 0.00002907
Iteration 4/1000 | Loss: 0.00002473
Iteration 5/1000 | Loss: 0.00002223
Iteration 6/1000 | Loss: 0.00002117
Iteration 7/1000 | Loss: 0.00002040
Iteration 8/1000 | Loss: 0.00001980
Iteration 9/1000 | Loss: 0.00001952
Iteration 10/1000 | Loss: 0.00001942
Iteration 11/1000 | Loss: 0.00001930
Iteration 12/1000 | Loss: 0.00001915
Iteration 13/1000 | Loss: 0.00001906
Iteration 14/1000 | Loss: 0.00001904
Iteration 15/1000 | Loss: 0.00001903
Iteration 16/1000 | Loss: 0.00001901
Iteration 17/1000 | Loss: 0.00001900
Iteration 18/1000 | Loss: 0.00001898
Iteration 19/1000 | Loss: 0.00001898
Iteration 20/1000 | Loss: 0.00001897
Iteration 21/1000 | Loss: 0.00001895
Iteration 22/1000 | Loss: 0.00001895
Iteration 23/1000 | Loss: 0.00001894
Iteration 24/1000 | Loss: 0.00001894
Iteration 25/1000 | Loss: 0.00001892
Iteration 26/1000 | Loss: 0.00001892
Iteration 27/1000 | Loss: 0.00001891
Iteration 28/1000 | Loss: 0.00001891
Iteration 29/1000 | Loss: 0.00001891
Iteration 30/1000 | Loss: 0.00001891
Iteration 31/1000 | Loss: 0.00001891
Iteration 32/1000 | Loss: 0.00001890
Iteration 33/1000 | Loss: 0.00001890
Iteration 34/1000 | Loss: 0.00001889
Iteration 35/1000 | Loss: 0.00001889
Iteration 36/1000 | Loss: 0.00001889
Iteration 37/1000 | Loss: 0.00001889
Iteration 38/1000 | Loss: 0.00001889
Iteration 39/1000 | Loss: 0.00001889
Iteration 40/1000 | Loss: 0.00001888
Iteration 41/1000 | Loss: 0.00001888
Iteration 42/1000 | Loss: 0.00001888
Iteration 43/1000 | Loss: 0.00001888
Iteration 44/1000 | Loss: 0.00001888
Iteration 45/1000 | Loss: 0.00001888
Iteration 46/1000 | Loss: 0.00001888
Iteration 47/1000 | Loss: 0.00001888
Iteration 48/1000 | Loss: 0.00001888
Iteration 49/1000 | Loss: 0.00001887
Iteration 50/1000 | Loss: 0.00001887
Iteration 51/1000 | Loss: 0.00001887
Iteration 52/1000 | Loss: 0.00001887
Iteration 53/1000 | Loss: 0.00001887
Iteration 54/1000 | Loss: 0.00001886
Iteration 55/1000 | Loss: 0.00001886
Iteration 56/1000 | Loss: 0.00001886
Iteration 57/1000 | Loss: 0.00001886
Iteration 58/1000 | Loss: 0.00001885
Iteration 59/1000 | Loss: 0.00001885
Iteration 60/1000 | Loss: 0.00001885
Iteration 61/1000 | Loss: 0.00001884
Iteration 62/1000 | Loss: 0.00001884
Iteration 63/1000 | Loss: 0.00001884
Iteration 64/1000 | Loss: 0.00001884
Iteration 65/1000 | Loss: 0.00001884
Iteration 66/1000 | Loss: 0.00001884
Iteration 67/1000 | Loss: 0.00001883
Iteration 68/1000 | Loss: 0.00001883
Iteration 69/1000 | Loss: 0.00001883
Iteration 70/1000 | Loss: 0.00001883
Iteration 71/1000 | Loss: 0.00001883
Iteration 72/1000 | Loss: 0.00001883
Iteration 73/1000 | Loss: 0.00001883
Iteration 74/1000 | Loss: 0.00001883
Iteration 75/1000 | Loss: 0.00001882
Iteration 76/1000 | Loss: 0.00001882
Iteration 77/1000 | Loss: 0.00001882
Iteration 78/1000 | Loss: 0.00001882
Iteration 79/1000 | Loss: 0.00001882
Iteration 80/1000 | Loss: 0.00001881
Iteration 81/1000 | Loss: 0.00001881
Iteration 82/1000 | Loss: 0.00001881
Iteration 83/1000 | Loss: 0.00001881
Iteration 84/1000 | Loss: 0.00001881
Iteration 85/1000 | Loss: 0.00001881
Iteration 86/1000 | Loss: 0.00001881
Iteration 87/1000 | Loss: 0.00001881
Iteration 88/1000 | Loss: 0.00001880
Iteration 89/1000 | Loss: 0.00001880
Iteration 90/1000 | Loss: 0.00001880
Iteration 91/1000 | Loss: 0.00001880
Iteration 92/1000 | Loss: 0.00001879
Iteration 93/1000 | Loss: 0.00001879
Iteration 94/1000 | Loss: 0.00001879
Iteration 95/1000 | Loss: 0.00001879
Iteration 96/1000 | Loss: 0.00001879
Iteration 97/1000 | Loss: 0.00001878
Iteration 98/1000 | Loss: 0.00001878
Iteration 99/1000 | Loss: 0.00001878
Iteration 100/1000 | Loss: 0.00001878
Iteration 101/1000 | Loss: 0.00001878
Iteration 102/1000 | Loss: 0.00001878
Iteration 103/1000 | Loss: 0.00001878
Iteration 104/1000 | Loss: 0.00001878
Iteration 105/1000 | Loss: 0.00001877
Iteration 106/1000 | Loss: 0.00001877
Iteration 107/1000 | Loss: 0.00001877
Iteration 108/1000 | Loss: 0.00001877
Iteration 109/1000 | Loss: 0.00001877
Iteration 110/1000 | Loss: 0.00001877
Iteration 111/1000 | Loss: 0.00001877
Iteration 112/1000 | Loss: 0.00001876
Iteration 113/1000 | Loss: 0.00001876
Iteration 114/1000 | Loss: 0.00001876
Iteration 115/1000 | Loss: 0.00001876
Iteration 116/1000 | Loss: 0.00001876
Iteration 117/1000 | Loss: 0.00001876
Iteration 118/1000 | Loss: 0.00001876
Iteration 119/1000 | Loss: 0.00001876
Iteration 120/1000 | Loss: 0.00001876
Iteration 121/1000 | Loss: 0.00001876
Iteration 122/1000 | Loss: 0.00001876
Iteration 123/1000 | Loss: 0.00001876
Iteration 124/1000 | Loss: 0.00001876
Iteration 125/1000 | Loss: 0.00001876
Iteration 126/1000 | Loss: 0.00001876
Iteration 127/1000 | Loss: 0.00001876
Iteration 128/1000 | Loss: 0.00001876
Iteration 129/1000 | Loss: 0.00001875
Iteration 130/1000 | Loss: 0.00001875
Iteration 131/1000 | Loss: 0.00001875
Iteration 132/1000 | Loss: 0.00001875
Iteration 133/1000 | Loss: 0.00001875
Iteration 134/1000 | Loss: 0.00001875
Iteration 135/1000 | Loss: 0.00001875
Iteration 136/1000 | Loss: 0.00001875
Iteration 137/1000 | Loss: 0.00001875
Iteration 138/1000 | Loss: 0.00001875
Iteration 139/1000 | Loss: 0.00001875
Iteration 140/1000 | Loss: 0.00001875
Iteration 141/1000 | Loss: 0.00001875
Iteration 142/1000 | Loss: 0.00001875
Iteration 143/1000 | Loss: 0.00001875
Iteration 144/1000 | Loss: 0.00001875
Iteration 145/1000 | Loss: 0.00001875
Iteration 146/1000 | Loss: 0.00001874
Iteration 147/1000 | Loss: 0.00001874
Iteration 148/1000 | Loss: 0.00001874
Iteration 149/1000 | Loss: 0.00001874
Iteration 150/1000 | Loss: 0.00001874
Iteration 151/1000 | Loss: 0.00001874
Iteration 152/1000 | Loss: 0.00001874
Iteration 153/1000 | Loss: 0.00001874
Iteration 154/1000 | Loss: 0.00001874
Iteration 155/1000 | Loss: 0.00001874
Iteration 156/1000 | Loss: 0.00001874
Iteration 157/1000 | Loss: 0.00001874
Iteration 158/1000 | Loss: 0.00001874
Iteration 159/1000 | Loss: 0.00001874
Iteration 160/1000 | Loss: 0.00001874
Iteration 161/1000 | Loss: 0.00001874
Iteration 162/1000 | Loss: 0.00001874
Iteration 163/1000 | Loss: 0.00001874
Iteration 164/1000 | Loss: 0.00001874
Iteration 165/1000 | Loss: 0.00001874
Iteration 166/1000 | Loss: 0.00001874
Iteration 167/1000 | Loss: 0.00001874
Iteration 168/1000 | Loss: 0.00001874
Iteration 169/1000 | Loss: 0.00001874
Iteration 170/1000 | Loss: 0.00001874
Iteration 171/1000 | Loss: 0.00001874
Iteration 172/1000 | Loss: 0.00001874
Iteration 173/1000 | Loss: 0.00001874
Iteration 174/1000 | Loss: 0.00001874
Iteration 175/1000 | Loss: 0.00001874
Iteration 176/1000 | Loss: 0.00001874
Iteration 177/1000 | Loss: 0.00001874
Iteration 178/1000 | Loss: 0.00001874
Iteration 179/1000 | Loss: 0.00001874
Iteration 180/1000 | Loss: 0.00001874
Iteration 181/1000 | Loss: 0.00001874
Iteration 182/1000 | Loss: 0.00001874
Iteration 183/1000 | Loss: 0.00001874
Iteration 184/1000 | Loss: 0.00001874
Iteration 185/1000 | Loss: 0.00001874
Iteration 186/1000 | Loss: 0.00001874
Iteration 187/1000 | Loss: 0.00001874
Iteration 188/1000 | Loss: 0.00001874
Iteration 189/1000 | Loss: 0.00001874
Iteration 190/1000 | Loss: 0.00001874
Iteration 191/1000 | Loss: 0.00001874
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 191. Stopping optimization.
Last 5 losses: [1.873877044999972e-05, 1.873877044999972e-05, 1.873877044999972e-05, 1.873877044999972e-05, 1.873877044999972e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.873877044999972e-05

Optimization complete. Final v2v error: 3.8329484462738037 mm

Highest mean error: 4.004601955413818 mm for frame 99

Lowest mean error: 3.3856794834136963 mm for frame 3

Saving results

Total time: 36.293320178985596
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_us_1259/0018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_1259/0018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_1259/0018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01086608
Iteration 2/25 | Loss: 0.00450197
Iteration 3/25 | Loss: 0.00270396
Iteration 4/25 | Loss: 0.00232086
Iteration 5/25 | Loss: 0.00234772
Iteration 6/25 | Loss: 0.00225791
Iteration 7/25 | Loss: 0.00191853
Iteration 8/25 | Loss: 0.00180200
Iteration 9/25 | Loss: 0.00176627
Iteration 10/25 | Loss: 0.00169106
Iteration 11/25 | Loss: 0.00167431
Iteration 12/25 | Loss: 0.00165599
Iteration 13/25 | Loss: 0.00163546
Iteration 14/25 | Loss: 0.00162243
Iteration 15/25 | Loss: 0.00160776
Iteration 16/25 | Loss: 0.00158575
Iteration 17/25 | Loss: 0.00157255
Iteration 18/25 | Loss: 0.00157184
Iteration 19/25 | Loss: 0.00159310
Iteration 20/25 | Loss: 0.00160453
Iteration 21/25 | Loss: 0.00154606
Iteration 22/25 | Loss: 0.00153665
Iteration 23/25 | Loss: 0.00150942
Iteration 24/25 | Loss: 0.00148495
Iteration 25/25 | Loss: 0.00148097

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.67646241
Iteration 2/25 | Loss: 0.00776889
Iteration 3/25 | Loss: 0.00565049
Iteration 4/25 | Loss: 0.00561133
Iteration 5/25 | Loss: 0.00561133
Iteration 6/25 | Loss: 0.00561132
Iteration 7/25 | Loss: 0.00561132
Iteration 8/25 | Loss: 0.00561132
Iteration 9/25 | Loss: 0.00561132
Iteration 10/25 | Loss: 0.00561132
Iteration 11/25 | Loss: 0.00561132
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.005611324682831764, 0.005611324682831764, 0.005611324682831764, 0.005611324682831764, 0.005611324682831764]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.005611324682831764

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00561132
Iteration 2/1000 | Loss: 0.00551709
Iteration 3/1000 | Loss: 0.00120828
Iteration 4/1000 | Loss: 0.00087553
Iteration 5/1000 | Loss: 0.00207309
Iteration 6/1000 | Loss: 0.00360472
Iteration 7/1000 | Loss: 0.00125161
Iteration 8/1000 | Loss: 0.00028871
Iteration 9/1000 | Loss: 0.00170802
Iteration 10/1000 | Loss: 0.00327969
Iteration 11/1000 | Loss: 0.00202104
Iteration 12/1000 | Loss: 0.00667360
Iteration 13/1000 | Loss: 0.00384363
Iteration 14/1000 | Loss: 0.00388973
Iteration 15/1000 | Loss: 0.00139246
Iteration 16/1000 | Loss: 0.00528720
Iteration 17/1000 | Loss: 0.00142070
Iteration 18/1000 | Loss: 0.00170021
Iteration 19/1000 | Loss: 0.00076006
Iteration 20/1000 | Loss: 0.00201789
Iteration 21/1000 | Loss: 0.00052828
Iteration 22/1000 | Loss: 0.00443081
Iteration 23/1000 | Loss: 0.00172615
Iteration 24/1000 | Loss: 0.00092378
Iteration 25/1000 | Loss: 0.00029482
Iteration 26/1000 | Loss: 0.00052653
Iteration 27/1000 | Loss: 0.00217798
Iteration 28/1000 | Loss: 0.00217552
Iteration 29/1000 | Loss: 0.00342951
Iteration 30/1000 | Loss: 0.00149677
Iteration 31/1000 | Loss: 0.00415346
Iteration 32/1000 | Loss: 0.00043727
Iteration 33/1000 | Loss: 0.00205610
Iteration 34/1000 | Loss: 0.00400550
Iteration 35/1000 | Loss: 0.00065719
Iteration 36/1000 | Loss: 0.00113648
Iteration 37/1000 | Loss: 0.00033823
Iteration 38/1000 | Loss: 0.00288350
Iteration 39/1000 | Loss: 0.00022304
Iteration 40/1000 | Loss: 0.00066392
Iteration 41/1000 | Loss: 0.00063597
Iteration 42/1000 | Loss: 0.00205784
Iteration 43/1000 | Loss: 0.00520120
Iteration 44/1000 | Loss: 0.00409073
Iteration 45/1000 | Loss: 0.00057962
Iteration 46/1000 | Loss: 0.00259743
Iteration 47/1000 | Loss: 0.00166441
Iteration 48/1000 | Loss: 0.00072726
Iteration 49/1000 | Loss: 0.00169964
Iteration 50/1000 | Loss: 0.00178892
Iteration 51/1000 | Loss: 0.00201004
Iteration 52/1000 | Loss: 0.00106569
Iteration 53/1000 | Loss: 0.00051338
Iteration 54/1000 | Loss: 0.00197642
Iteration 55/1000 | Loss: 0.00056945
Iteration 56/1000 | Loss: 0.00056865
Iteration 57/1000 | Loss: 0.00028404
Iteration 58/1000 | Loss: 0.00416884
Iteration 59/1000 | Loss: 0.00107064
Iteration 60/1000 | Loss: 0.00031906
Iteration 61/1000 | Loss: 0.00101190
Iteration 62/1000 | Loss: 0.00081397
Iteration 63/1000 | Loss: 0.00072430
Iteration 64/1000 | Loss: 0.00076020
Iteration 65/1000 | Loss: 0.00065433
Iteration 66/1000 | Loss: 0.00070650
Iteration 67/1000 | Loss: 0.00160462
Iteration 68/1000 | Loss: 0.00040972
Iteration 69/1000 | Loss: 0.00033775
Iteration 70/1000 | Loss: 0.00019390
Iteration 71/1000 | Loss: 0.00033001
Iteration 72/1000 | Loss: 0.00036215
Iteration 73/1000 | Loss: 0.00109696
Iteration 74/1000 | Loss: 0.00013647
Iteration 75/1000 | Loss: 0.00043928
Iteration 76/1000 | Loss: 0.00023814
Iteration 77/1000 | Loss: 0.00044776
Iteration 78/1000 | Loss: 0.00012749
Iteration 79/1000 | Loss: 0.00035947
Iteration 80/1000 | Loss: 0.00056291
Iteration 81/1000 | Loss: 0.00026233
Iteration 82/1000 | Loss: 0.00102569
Iteration 83/1000 | Loss: 0.00035122
Iteration 84/1000 | Loss: 0.00041824
Iteration 85/1000 | Loss: 0.00007450
Iteration 86/1000 | Loss: 0.00014784
Iteration 87/1000 | Loss: 0.00041834
Iteration 88/1000 | Loss: 0.00023177
Iteration 89/1000 | Loss: 0.00015925
Iteration 90/1000 | Loss: 0.00026636
Iteration 91/1000 | Loss: 0.00009729
Iteration 92/1000 | Loss: 0.00023341
Iteration 93/1000 | Loss: 0.00012023
Iteration 94/1000 | Loss: 0.00007712
Iteration 95/1000 | Loss: 0.00015667
Iteration 96/1000 | Loss: 0.00032175
Iteration 97/1000 | Loss: 0.00043252
Iteration 98/1000 | Loss: 0.00024236
Iteration 99/1000 | Loss: 0.00029996
Iteration 100/1000 | Loss: 0.00078278
Iteration 101/1000 | Loss: 0.00033153
Iteration 102/1000 | Loss: 0.00053027
Iteration 103/1000 | Loss: 0.00029424
Iteration 104/1000 | Loss: 0.00019074
Iteration 105/1000 | Loss: 0.00023748
Iteration 106/1000 | Loss: 0.00041241
Iteration 107/1000 | Loss: 0.00024521
Iteration 108/1000 | Loss: 0.00034410
Iteration 109/1000 | Loss: 0.00027454
Iteration 110/1000 | Loss: 0.00030965
Iteration 111/1000 | Loss: 0.00026475
Iteration 112/1000 | Loss: 0.00008152
Iteration 113/1000 | Loss: 0.00058109
Iteration 114/1000 | Loss: 0.00037755
Iteration 115/1000 | Loss: 0.00049345
Iteration 116/1000 | Loss: 0.00036667
Iteration 117/1000 | Loss: 0.00017229
Iteration 118/1000 | Loss: 0.00004666
Iteration 119/1000 | Loss: 0.00004108
Iteration 120/1000 | Loss: 0.00046843
Iteration 121/1000 | Loss: 0.00033582
Iteration 122/1000 | Loss: 0.00011428
Iteration 123/1000 | Loss: 0.00013412
Iteration 124/1000 | Loss: 0.00005488
Iteration 125/1000 | Loss: 0.00013389
Iteration 126/1000 | Loss: 0.00007304
Iteration 127/1000 | Loss: 0.00007891
Iteration 128/1000 | Loss: 0.00014874
Iteration 129/1000 | Loss: 0.00025621
Iteration 130/1000 | Loss: 0.00019542
Iteration 131/1000 | Loss: 0.00008345
Iteration 132/1000 | Loss: 0.00006578
Iteration 133/1000 | Loss: 0.00021265
Iteration 134/1000 | Loss: 0.00007881
Iteration 135/1000 | Loss: 0.00024159
Iteration 136/1000 | Loss: 0.00010676
Iteration 137/1000 | Loss: 0.00025440
Iteration 138/1000 | Loss: 0.00010789
Iteration 139/1000 | Loss: 0.00024304
Iteration 140/1000 | Loss: 0.00010291
Iteration 141/1000 | Loss: 0.00013382
Iteration 142/1000 | Loss: 0.00007030
Iteration 143/1000 | Loss: 0.00013266
Iteration 144/1000 | Loss: 0.00008305
Iteration 145/1000 | Loss: 0.00019735
Iteration 146/1000 | Loss: 0.00020437
Iteration 147/1000 | Loss: 0.00008496
Iteration 148/1000 | Loss: 0.00037379
Iteration 149/1000 | Loss: 0.00020397
Iteration 150/1000 | Loss: 0.00024237
Iteration 151/1000 | Loss: 0.00022613
Iteration 152/1000 | Loss: 0.00007800
Iteration 153/1000 | Loss: 0.00049372
Iteration 154/1000 | Loss: 0.00028471
Iteration 155/1000 | Loss: 0.00019626
Iteration 156/1000 | Loss: 0.00028761
Iteration 157/1000 | Loss: 0.00021835
Iteration 158/1000 | Loss: 0.00042623
Iteration 159/1000 | Loss: 0.00023201
Iteration 160/1000 | Loss: 0.00030775
Iteration 161/1000 | Loss: 0.00035894
Iteration 162/1000 | Loss: 0.00023874
Iteration 163/1000 | Loss: 0.00012596
Iteration 164/1000 | Loss: 0.00011985
Iteration 165/1000 | Loss: 0.00004208
Iteration 166/1000 | Loss: 0.00027285
Iteration 167/1000 | Loss: 0.00007974
Iteration 168/1000 | Loss: 0.00006420
Iteration 169/1000 | Loss: 0.00009087
Iteration 170/1000 | Loss: 0.00003885
Iteration 171/1000 | Loss: 0.00004292
Iteration 172/1000 | Loss: 0.00003948
Iteration 173/1000 | Loss: 0.00012007
Iteration 174/1000 | Loss: 0.00003555
Iteration 175/1000 | Loss: 0.00003374
Iteration 176/1000 | Loss: 0.00019300
Iteration 177/1000 | Loss: 0.00003276
Iteration 178/1000 | Loss: 0.00003887
Iteration 179/1000 | Loss: 0.00003277
Iteration 180/1000 | Loss: 0.00003203
Iteration 181/1000 | Loss: 0.00003169
Iteration 182/1000 | Loss: 0.00003152
Iteration 183/1000 | Loss: 0.00003150
Iteration 184/1000 | Loss: 0.00003130
Iteration 185/1000 | Loss: 0.00003124
Iteration 186/1000 | Loss: 0.00003123
Iteration 187/1000 | Loss: 0.00003121
Iteration 188/1000 | Loss: 0.00003107
Iteration 189/1000 | Loss: 0.00003104
Iteration 190/1000 | Loss: 0.00003103
Iteration 191/1000 | Loss: 0.00003099
Iteration 192/1000 | Loss: 0.00003099
Iteration 193/1000 | Loss: 0.00003098
Iteration 194/1000 | Loss: 0.00003097
Iteration 195/1000 | Loss: 0.00003097
Iteration 196/1000 | Loss: 0.00003096
Iteration 197/1000 | Loss: 0.00003096
Iteration 198/1000 | Loss: 0.00003096
Iteration 199/1000 | Loss: 0.00003096
Iteration 200/1000 | Loss: 0.00003096
Iteration 201/1000 | Loss: 0.00003095
Iteration 202/1000 | Loss: 0.00003095
Iteration 203/1000 | Loss: 0.00003095
Iteration 204/1000 | Loss: 0.00003095
Iteration 205/1000 | Loss: 0.00003095
Iteration 206/1000 | Loss: 0.00003095
Iteration 207/1000 | Loss: 0.00003094
Iteration 208/1000 | Loss: 0.00003094
Iteration 209/1000 | Loss: 0.00003094
Iteration 210/1000 | Loss: 0.00003094
Iteration 211/1000 | Loss: 0.00003094
Iteration 212/1000 | Loss: 0.00003093
Iteration 213/1000 | Loss: 0.00003093
Iteration 214/1000 | Loss: 0.00003092
Iteration 215/1000 | Loss: 0.00003092
Iteration 216/1000 | Loss: 0.00003092
Iteration 217/1000 | Loss: 0.00003092
Iteration 218/1000 | Loss: 0.00003091
Iteration 219/1000 | Loss: 0.00003091
Iteration 220/1000 | Loss: 0.00003091
Iteration 221/1000 | Loss: 0.00003091
Iteration 222/1000 | Loss: 0.00003090
Iteration 223/1000 | Loss: 0.00003090
Iteration 224/1000 | Loss: 0.00003089
Iteration 225/1000 | Loss: 0.00003089
Iteration 226/1000 | Loss: 0.00003089
Iteration 227/1000 | Loss: 0.00003088
Iteration 228/1000 | Loss: 0.00003088
Iteration 229/1000 | Loss: 0.00003088
Iteration 230/1000 | Loss: 0.00003088
Iteration 231/1000 | Loss: 0.00003088
Iteration 232/1000 | Loss: 0.00003088
Iteration 233/1000 | Loss: 0.00003088
Iteration 234/1000 | Loss: 0.00003088
Iteration 235/1000 | Loss: 0.00003087
Iteration 236/1000 | Loss: 0.00003087
Iteration 237/1000 | Loss: 0.00003087
Iteration 238/1000 | Loss: 0.00003087
Iteration 239/1000 | Loss: 0.00003086
Iteration 240/1000 | Loss: 0.00003086
Iteration 241/1000 | Loss: 0.00003086
Iteration 242/1000 | Loss: 0.00003085
Iteration 243/1000 | Loss: 0.00003085
Iteration 244/1000 | Loss: 0.00003085
Iteration 245/1000 | Loss: 0.00003085
Iteration 246/1000 | Loss: 0.00003085
Iteration 247/1000 | Loss: 0.00003085
Iteration 248/1000 | Loss: 0.00003085
Iteration 249/1000 | Loss: 0.00003085
Iteration 250/1000 | Loss: 0.00003084
Iteration 251/1000 | Loss: 0.00003084
Iteration 252/1000 | Loss: 0.00003084
Iteration 253/1000 | Loss: 0.00003084
Iteration 254/1000 | Loss: 0.00003084
Iteration 255/1000 | Loss: 0.00003084
Iteration 256/1000 | Loss: 0.00003084
Iteration 257/1000 | Loss: 0.00003083
Iteration 258/1000 | Loss: 0.00003083
Iteration 259/1000 | Loss: 0.00003083
Iteration 260/1000 | Loss: 0.00003082
Iteration 261/1000 | Loss: 0.00003082
Iteration 262/1000 | Loss: 0.00003082
Iteration 263/1000 | Loss: 0.00003082
Iteration 264/1000 | Loss: 0.00003081
Iteration 265/1000 | Loss: 0.00003081
Iteration 266/1000 | Loss: 0.00003081
Iteration 267/1000 | Loss: 0.00003081
Iteration 268/1000 | Loss: 0.00003081
Iteration 269/1000 | Loss: 0.00003081
Iteration 270/1000 | Loss: 0.00003081
Iteration 271/1000 | Loss: 0.00003080
Iteration 272/1000 | Loss: 0.00003080
Iteration 273/1000 | Loss: 0.00003080
Iteration 274/1000 | Loss: 0.00003079
Iteration 275/1000 | Loss: 0.00003079
Iteration 276/1000 | Loss: 0.00003079
Iteration 277/1000 | Loss: 0.00003079
Iteration 278/1000 | Loss: 0.00003079
Iteration 279/1000 | Loss: 0.00003079
Iteration 280/1000 | Loss: 0.00003079
Iteration 281/1000 | Loss: 0.00003079
Iteration 282/1000 | Loss: 0.00003079
Iteration 283/1000 | Loss: 0.00003079
Iteration 284/1000 | Loss: 0.00003078
Iteration 285/1000 | Loss: 0.00003078
Iteration 286/1000 | Loss: 0.00003078
Iteration 287/1000 | Loss: 0.00003078
Iteration 288/1000 | Loss: 0.00003078
Iteration 289/1000 | Loss: 0.00003078
Iteration 290/1000 | Loss: 0.00003078
Iteration 291/1000 | Loss: 0.00003078
Iteration 292/1000 | Loss: 0.00003078
Iteration 293/1000 | Loss: 0.00003077
Iteration 294/1000 | Loss: 0.00003077
Iteration 295/1000 | Loss: 0.00003077
Iteration 296/1000 | Loss: 0.00003076
Iteration 297/1000 | Loss: 0.00003076
Iteration 298/1000 | Loss: 0.00003076
Iteration 299/1000 | Loss: 0.00003076
Iteration 300/1000 | Loss: 0.00004768
Iteration 301/1000 | Loss: 0.00004340
Iteration 302/1000 | Loss: 0.00015159
Iteration 303/1000 | Loss: 0.00003969
Iteration 304/1000 | Loss: 0.00003104
Iteration 305/1000 | Loss: 0.00003061
Iteration 306/1000 | Loss: 0.00003032
Iteration 307/1000 | Loss: 0.00003011
Iteration 308/1000 | Loss: 0.00003010
Iteration 309/1000 | Loss: 0.00003002
Iteration 310/1000 | Loss: 0.00004378
Iteration 311/1000 | Loss: 0.00003297
Iteration 312/1000 | Loss: 0.00003108
Iteration 313/1000 | Loss: 0.00003030
Iteration 314/1000 | Loss: 0.00002991
Iteration 315/1000 | Loss: 0.00002986
Iteration 316/1000 | Loss: 0.00002971
Iteration 317/1000 | Loss: 0.00002971
Iteration 318/1000 | Loss: 0.00002965
Iteration 319/1000 | Loss: 0.00002961
Iteration 320/1000 | Loss: 0.00002961
Iteration 321/1000 | Loss: 0.00002958
Iteration 322/1000 | Loss: 0.00002956
Iteration 323/1000 | Loss: 0.00002955
Iteration 324/1000 | Loss: 0.00002955
Iteration 325/1000 | Loss: 0.00002955
Iteration 326/1000 | Loss: 0.00002954
Iteration 327/1000 | Loss: 0.00002954
Iteration 328/1000 | Loss: 0.00002952
Iteration 329/1000 | Loss: 0.00002952
Iteration 330/1000 | Loss: 0.00002952
Iteration 331/1000 | Loss: 0.00002952
Iteration 332/1000 | Loss: 0.00002952
Iteration 333/1000 | Loss: 0.00002952
Iteration 334/1000 | Loss: 0.00002952
Iteration 335/1000 | Loss: 0.00002952
Iteration 336/1000 | Loss: 0.00002952
Iteration 337/1000 | Loss: 0.00002951
Iteration 338/1000 | Loss: 0.00002951
Iteration 339/1000 | Loss: 0.00002951
Iteration 340/1000 | Loss: 0.00002950
Iteration 341/1000 | Loss: 0.00002949
Iteration 342/1000 | Loss: 0.00002949
Iteration 343/1000 | Loss: 0.00002948
Iteration 344/1000 | Loss: 0.00002948
Iteration 345/1000 | Loss: 0.00002948
Iteration 346/1000 | Loss: 0.00002947
Iteration 347/1000 | Loss: 0.00002947
Iteration 348/1000 | Loss: 0.00002947
Iteration 349/1000 | Loss: 0.00002946
Iteration 350/1000 | Loss: 0.00002946
Iteration 351/1000 | Loss: 0.00002945
Iteration 352/1000 | Loss: 0.00002945
Iteration 353/1000 | Loss: 0.00002944
Iteration 354/1000 | Loss: 0.00002944
Iteration 355/1000 | Loss: 0.00002944
Iteration 356/1000 | Loss: 0.00002943
Iteration 357/1000 | Loss: 0.00002943
Iteration 358/1000 | Loss: 0.00002943
Iteration 359/1000 | Loss: 0.00002943
Iteration 360/1000 | Loss: 0.00002942
Iteration 361/1000 | Loss: 0.00002942
Iteration 362/1000 | Loss: 0.00002942
Iteration 363/1000 | Loss: 0.00002942
Iteration 364/1000 | Loss: 0.00002942
Iteration 365/1000 | Loss: 0.00002942
Iteration 366/1000 | Loss: 0.00002942
Iteration 367/1000 | Loss: 0.00002941
Iteration 368/1000 | Loss: 0.00002941
Iteration 369/1000 | Loss: 0.00002941
Iteration 370/1000 | Loss: 0.00002941
Iteration 371/1000 | Loss: 0.00002941
Iteration 372/1000 | Loss: 0.00002941
Iteration 373/1000 | Loss: 0.00002941
Iteration 374/1000 | Loss: 0.00002941
Iteration 375/1000 | Loss: 0.00002941
Iteration 376/1000 | Loss: 0.00002941
Iteration 377/1000 | Loss: 0.00002941
Iteration 378/1000 | Loss: 0.00002940
Iteration 379/1000 | Loss: 0.00002940
Iteration 380/1000 | Loss: 0.00002940
Iteration 381/1000 | Loss: 0.00002940
Iteration 382/1000 | Loss: 0.00002940
Iteration 383/1000 | Loss: 0.00002940
Iteration 384/1000 | Loss: 0.00002940
Iteration 385/1000 | Loss: 0.00002940
Iteration 386/1000 | Loss: 0.00002940
Iteration 387/1000 | Loss: 0.00002940
Iteration 388/1000 | Loss: 0.00002940
Iteration 389/1000 | Loss: 0.00002940
Iteration 390/1000 | Loss: 0.00002940
Iteration 391/1000 | Loss: 0.00002940
Iteration 392/1000 | Loss: 0.00002939
Iteration 393/1000 | Loss: 0.00002939
Iteration 394/1000 | Loss: 0.00002939
Iteration 395/1000 | Loss: 0.00002939
Iteration 396/1000 | Loss: 0.00002939
Iteration 397/1000 | Loss: 0.00002939
Iteration 398/1000 | Loss: 0.00002939
Iteration 399/1000 | Loss: 0.00002939
Iteration 400/1000 | Loss: 0.00002939
Iteration 401/1000 | Loss: 0.00002939
Iteration 402/1000 | Loss: 0.00002939
Iteration 403/1000 | Loss: 0.00002939
Iteration 404/1000 | Loss: 0.00002939
Iteration 405/1000 | Loss: 0.00002939
Iteration 406/1000 | Loss: 0.00002939
Iteration 407/1000 | Loss: 0.00002939
Iteration 408/1000 | Loss: 0.00002939
Iteration 409/1000 | Loss: 0.00002939
Iteration 410/1000 | Loss: 0.00002939
Iteration 411/1000 | Loss: 0.00002939
Iteration 412/1000 | Loss: 0.00002939
Iteration 413/1000 | Loss: 0.00002939
Iteration 414/1000 | Loss: 0.00002939
Iteration 415/1000 | Loss: 0.00002939
Iteration 416/1000 | Loss: 0.00002939
Iteration 417/1000 | Loss: 0.00002939
Iteration 418/1000 | Loss: 0.00002938
Iteration 419/1000 | Loss: 0.00002938
Iteration 420/1000 | Loss: 0.00002938
Iteration 421/1000 | Loss: 0.00002938
Iteration 422/1000 | Loss: 0.00002938
Iteration 423/1000 | Loss: 0.00002938
Iteration 424/1000 | Loss: 0.00002938
Iteration 425/1000 | Loss: 0.00002938
Iteration 426/1000 | Loss: 0.00002938
Iteration 427/1000 | Loss: 0.00002938
Iteration 428/1000 | Loss: 0.00002938
Iteration 429/1000 | Loss: 0.00002938
Iteration 430/1000 | Loss: 0.00002938
Iteration 431/1000 | Loss: 0.00002938
Iteration 432/1000 | Loss: 0.00002938
Iteration 433/1000 | Loss: 0.00002938
Iteration 434/1000 | Loss: 0.00002938
Iteration 435/1000 | Loss: 0.00002938
Iteration 436/1000 | Loss: 0.00002938
Iteration 437/1000 | Loss: 0.00002938
Iteration 438/1000 | Loss: 0.00002938
Iteration 439/1000 | Loss: 0.00002938
Iteration 440/1000 | Loss: 0.00002938
Iteration 441/1000 | Loss: 0.00002938
Iteration 442/1000 | Loss: 0.00002938
Iteration 443/1000 | Loss: 0.00002938
Iteration 444/1000 | Loss: 0.00002938
Iteration 445/1000 | Loss: 0.00002938
Iteration 446/1000 | Loss: 0.00002938
Iteration 447/1000 | Loss: 0.00002938
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 447. Stopping optimization.
Last 5 losses: [2.9375922167673707e-05, 2.9375922167673707e-05, 2.9375922167673707e-05, 2.9375922167673707e-05, 2.9375922167673707e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.9375922167673707e-05

Optimization complete. Final v2v error: 4.529740810394287 mm

Highest mean error: 12.616670608520508 mm for frame 168

Lowest mean error: 3.963918447494507 mm for frame 156

Saving results

Total time: 389.77051544189453
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_us_1259/0000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_1259/0000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_1259/0000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00725014
Iteration 2/25 | Loss: 0.00164599
Iteration 3/25 | Loss: 0.00140154
Iteration 4/25 | Loss: 0.00135865
Iteration 5/25 | Loss: 0.00134863
Iteration 6/25 | Loss: 0.00134425
Iteration 7/25 | Loss: 0.00134125
Iteration 8/25 | Loss: 0.00133120
Iteration 9/25 | Loss: 0.00132341
Iteration 10/25 | Loss: 0.00131800
Iteration 11/25 | Loss: 0.00131660
Iteration 12/25 | Loss: 0.00131649
Iteration 13/25 | Loss: 0.00131649
Iteration 14/25 | Loss: 0.00131649
Iteration 15/25 | Loss: 0.00131649
Iteration 16/25 | Loss: 0.00131649
Iteration 17/25 | Loss: 0.00131649
Iteration 18/25 | Loss: 0.00131649
Iteration 19/25 | Loss: 0.00131649
Iteration 20/25 | Loss: 0.00131649
Iteration 21/25 | Loss: 0.00131648
Iteration 22/25 | Loss: 0.00131648
Iteration 23/25 | Loss: 0.00131648
Iteration 24/25 | Loss: 0.00131648
Iteration 25/25 | Loss: 0.00131648

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.36680174
Iteration 2/25 | Loss: 0.00294136
Iteration 3/25 | Loss: 0.00294109
Iteration 4/25 | Loss: 0.00294109
Iteration 5/25 | Loss: 0.00294109
Iteration 6/25 | Loss: 0.00294109
Iteration 7/25 | Loss: 0.00294109
Iteration 8/25 | Loss: 0.00294109
Iteration 9/25 | Loss: 0.00294109
Iteration 10/25 | Loss: 0.00294109
Iteration 11/25 | Loss: 0.00294109
Iteration 12/25 | Loss: 0.00294108
Iteration 13/25 | Loss: 0.00294109
Iteration 14/25 | Loss: 0.00294109
Iteration 15/25 | Loss: 0.00294109
Iteration 16/25 | Loss: 0.00294109
Iteration 17/25 | Loss: 0.00294109
Iteration 18/25 | Loss: 0.00294109
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0029410850256681442, 0.0029410850256681442, 0.0029410850256681442, 0.0029410850256681442, 0.0029410850256681442]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0029410850256681442

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00294109
Iteration 2/1000 | Loss: 0.00006101
Iteration 3/1000 | Loss: 0.00004633
Iteration 4/1000 | Loss: 0.00004113
Iteration 5/1000 | Loss: 0.00003843
Iteration 6/1000 | Loss: 0.00003624
Iteration 7/1000 | Loss: 0.00003473
Iteration 8/1000 | Loss: 0.00003388
Iteration 9/1000 | Loss: 0.00003319
Iteration 10/1000 | Loss: 0.00003267
Iteration 11/1000 | Loss: 0.00003233
Iteration 12/1000 | Loss: 0.00003204
Iteration 13/1000 | Loss: 0.00003173
Iteration 14/1000 | Loss: 0.00003150
Iteration 15/1000 | Loss: 0.00003133
Iteration 16/1000 | Loss: 0.00003130
Iteration 17/1000 | Loss: 0.00003122
Iteration 18/1000 | Loss: 0.00003122
Iteration 19/1000 | Loss: 0.00003121
Iteration 20/1000 | Loss: 0.00003117
Iteration 21/1000 | Loss: 0.00003114
Iteration 22/1000 | Loss: 0.00003114
Iteration 23/1000 | Loss: 0.00003113
Iteration 24/1000 | Loss: 0.00003113
Iteration 25/1000 | Loss: 0.00003110
Iteration 26/1000 | Loss: 0.00003109
Iteration 27/1000 | Loss: 0.00003108
Iteration 28/1000 | Loss: 0.00003108
Iteration 29/1000 | Loss: 0.00003107
Iteration 30/1000 | Loss: 0.00003107
Iteration 31/1000 | Loss: 0.00003107
Iteration 32/1000 | Loss: 0.00003106
Iteration 33/1000 | Loss: 0.00003105
Iteration 34/1000 | Loss: 0.00003105
Iteration 35/1000 | Loss: 0.00003100
Iteration 36/1000 | Loss: 0.00003097
Iteration 37/1000 | Loss: 0.00003095
Iteration 38/1000 | Loss: 0.00003095
Iteration 39/1000 | Loss: 0.00003094
Iteration 40/1000 | Loss: 0.00003094
Iteration 41/1000 | Loss: 0.00003093
Iteration 42/1000 | Loss: 0.00003093
Iteration 43/1000 | Loss: 0.00003092
Iteration 44/1000 | Loss: 0.00003092
Iteration 45/1000 | Loss: 0.00003092
Iteration 46/1000 | Loss: 0.00003090
Iteration 47/1000 | Loss: 0.00003089
Iteration 48/1000 | Loss: 0.00003089
Iteration 49/1000 | Loss: 0.00003088
Iteration 50/1000 | Loss: 0.00003088
Iteration 51/1000 | Loss: 0.00003088
Iteration 52/1000 | Loss: 0.00003088
Iteration 53/1000 | Loss: 0.00003087
Iteration 54/1000 | Loss: 0.00003087
Iteration 55/1000 | Loss: 0.00003087
Iteration 56/1000 | Loss: 0.00003087
Iteration 57/1000 | Loss: 0.00003086
Iteration 58/1000 | Loss: 0.00003086
Iteration 59/1000 | Loss: 0.00003085
Iteration 60/1000 | Loss: 0.00003085
Iteration 61/1000 | Loss: 0.00003084
Iteration 62/1000 | Loss: 0.00003084
Iteration 63/1000 | Loss: 0.00003084
Iteration 64/1000 | Loss: 0.00003084
Iteration 65/1000 | Loss: 0.00003084
Iteration 66/1000 | Loss: 0.00003084
Iteration 67/1000 | Loss: 0.00003083
Iteration 68/1000 | Loss: 0.00003083
Iteration 69/1000 | Loss: 0.00003083
Iteration 70/1000 | Loss: 0.00003083
Iteration 71/1000 | Loss: 0.00003082
Iteration 72/1000 | Loss: 0.00003082
Iteration 73/1000 | Loss: 0.00003082
Iteration 74/1000 | Loss: 0.00003081
Iteration 75/1000 | Loss: 0.00003081
Iteration 76/1000 | Loss: 0.00003081
Iteration 77/1000 | Loss: 0.00003080
Iteration 78/1000 | Loss: 0.00003080
Iteration 79/1000 | Loss: 0.00003080
Iteration 80/1000 | Loss: 0.00003079
Iteration 81/1000 | Loss: 0.00003079
Iteration 82/1000 | Loss: 0.00003079
Iteration 83/1000 | Loss: 0.00003078
Iteration 84/1000 | Loss: 0.00003078
Iteration 85/1000 | Loss: 0.00003078
Iteration 86/1000 | Loss: 0.00003078
Iteration 87/1000 | Loss: 0.00003078
Iteration 88/1000 | Loss: 0.00003078
Iteration 89/1000 | Loss: 0.00003078
Iteration 90/1000 | Loss: 0.00003077
Iteration 91/1000 | Loss: 0.00003077
Iteration 92/1000 | Loss: 0.00003076
Iteration 93/1000 | Loss: 0.00003076
Iteration 94/1000 | Loss: 0.00003076
Iteration 95/1000 | Loss: 0.00003075
Iteration 96/1000 | Loss: 0.00003075
Iteration 97/1000 | Loss: 0.00003075
Iteration 98/1000 | Loss: 0.00003075
Iteration 99/1000 | Loss: 0.00003074
Iteration 100/1000 | Loss: 0.00003074
Iteration 101/1000 | Loss: 0.00003074
Iteration 102/1000 | Loss: 0.00003074
Iteration 103/1000 | Loss: 0.00003074
Iteration 104/1000 | Loss: 0.00003073
Iteration 105/1000 | Loss: 0.00003073
Iteration 106/1000 | Loss: 0.00003073
Iteration 107/1000 | Loss: 0.00003073
Iteration 108/1000 | Loss: 0.00003073
Iteration 109/1000 | Loss: 0.00003072
Iteration 110/1000 | Loss: 0.00003072
Iteration 111/1000 | Loss: 0.00003071
Iteration 112/1000 | Loss: 0.00003071
Iteration 113/1000 | Loss: 0.00003071
Iteration 114/1000 | Loss: 0.00003070
Iteration 115/1000 | Loss: 0.00003070
Iteration 116/1000 | Loss: 0.00003070
Iteration 117/1000 | Loss: 0.00003070
Iteration 118/1000 | Loss: 0.00003070
Iteration 119/1000 | Loss: 0.00003070
Iteration 120/1000 | Loss: 0.00003070
Iteration 121/1000 | Loss: 0.00003070
Iteration 122/1000 | Loss: 0.00003069
Iteration 123/1000 | Loss: 0.00003069
Iteration 124/1000 | Loss: 0.00003069
Iteration 125/1000 | Loss: 0.00003069
Iteration 126/1000 | Loss: 0.00003069
Iteration 127/1000 | Loss: 0.00003068
Iteration 128/1000 | Loss: 0.00003068
Iteration 129/1000 | Loss: 0.00003068
Iteration 130/1000 | Loss: 0.00003068
Iteration 131/1000 | Loss: 0.00003068
Iteration 132/1000 | Loss: 0.00003068
Iteration 133/1000 | Loss: 0.00003068
Iteration 134/1000 | Loss: 0.00003068
Iteration 135/1000 | Loss: 0.00003068
Iteration 136/1000 | Loss: 0.00003068
Iteration 137/1000 | Loss: 0.00003068
Iteration 138/1000 | Loss: 0.00003068
Iteration 139/1000 | Loss: 0.00003067
Iteration 140/1000 | Loss: 0.00003067
Iteration 141/1000 | Loss: 0.00003067
Iteration 142/1000 | Loss: 0.00003067
Iteration 143/1000 | Loss: 0.00003067
Iteration 144/1000 | Loss: 0.00003067
Iteration 145/1000 | Loss: 0.00003066
Iteration 146/1000 | Loss: 0.00003066
Iteration 147/1000 | Loss: 0.00003066
Iteration 148/1000 | Loss: 0.00003066
Iteration 149/1000 | Loss: 0.00003066
Iteration 150/1000 | Loss: 0.00003066
Iteration 151/1000 | Loss: 0.00003066
Iteration 152/1000 | Loss: 0.00003066
Iteration 153/1000 | Loss: 0.00003066
Iteration 154/1000 | Loss: 0.00003066
Iteration 155/1000 | Loss: 0.00003066
Iteration 156/1000 | Loss: 0.00003066
Iteration 157/1000 | Loss: 0.00003065
Iteration 158/1000 | Loss: 0.00003065
Iteration 159/1000 | Loss: 0.00003065
Iteration 160/1000 | Loss: 0.00003065
Iteration 161/1000 | Loss: 0.00003065
Iteration 162/1000 | Loss: 0.00003065
Iteration 163/1000 | Loss: 0.00003065
Iteration 164/1000 | Loss: 0.00003065
Iteration 165/1000 | Loss: 0.00003064
Iteration 166/1000 | Loss: 0.00003064
Iteration 167/1000 | Loss: 0.00003064
Iteration 168/1000 | Loss: 0.00003064
Iteration 169/1000 | Loss: 0.00003064
Iteration 170/1000 | Loss: 0.00003064
Iteration 171/1000 | Loss: 0.00003064
Iteration 172/1000 | Loss: 0.00003064
Iteration 173/1000 | Loss: 0.00003064
Iteration 174/1000 | Loss: 0.00003064
Iteration 175/1000 | Loss: 0.00003064
Iteration 176/1000 | Loss: 0.00003064
Iteration 177/1000 | Loss: 0.00003064
Iteration 178/1000 | Loss: 0.00003063
Iteration 179/1000 | Loss: 0.00003063
Iteration 180/1000 | Loss: 0.00003063
Iteration 181/1000 | Loss: 0.00003063
Iteration 182/1000 | Loss: 0.00003063
Iteration 183/1000 | Loss: 0.00003063
Iteration 184/1000 | Loss: 0.00003062
Iteration 185/1000 | Loss: 0.00003062
Iteration 186/1000 | Loss: 0.00003062
Iteration 187/1000 | Loss: 0.00003062
Iteration 188/1000 | Loss: 0.00003062
Iteration 189/1000 | Loss: 0.00003062
Iteration 190/1000 | Loss: 0.00003062
Iteration 191/1000 | Loss: 0.00003061
Iteration 192/1000 | Loss: 0.00003061
Iteration 193/1000 | Loss: 0.00003061
Iteration 194/1000 | Loss: 0.00003061
Iteration 195/1000 | Loss: 0.00003061
Iteration 196/1000 | Loss: 0.00003061
Iteration 197/1000 | Loss: 0.00003061
Iteration 198/1000 | Loss: 0.00003061
Iteration 199/1000 | Loss: 0.00003061
Iteration 200/1000 | Loss: 0.00003061
Iteration 201/1000 | Loss: 0.00003061
Iteration 202/1000 | Loss: 0.00003061
Iteration 203/1000 | Loss: 0.00003061
Iteration 204/1000 | Loss: 0.00003061
Iteration 205/1000 | Loss: 0.00003061
Iteration 206/1000 | Loss: 0.00003061
Iteration 207/1000 | Loss: 0.00003061
Iteration 208/1000 | Loss: 0.00003061
Iteration 209/1000 | Loss: 0.00003060
Iteration 210/1000 | Loss: 0.00003060
Iteration 211/1000 | Loss: 0.00003060
Iteration 212/1000 | Loss: 0.00003060
Iteration 213/1000 | Loss: 0.00003060
Iteration 214/1000 | Loss: 0.00003060
Iteration 215/1000 | Loss: 0.00003060
Iteration 216/1000 | Loss: 0.00003060
Iteration 217/1000 | Loss: 0.00003060
Iteration 218/1000 | Loss: 0.00003060
Iteration 219/1000 | Loss: 0.00003060
Iteration 220/1000 | Loss: 0.00003060
Iteration 221/1000 | Loss: 0.00003060
Iteration 222/1000 | Loss: 0.00003060
Iteration 223/1000 | Loss: 0.00003060
Iteration 224/1000 | Loss: 0.00003060
Iteration 225/1000 | Loss: 0.00003060
Iteration 226/1000 | Loss: 0.00003060
Iteration 227/1000 | Loss: 0.00003060
Iteration 228/1000 | Loss: 0.00003060
Iteration 229/1000 | Loss: 0.00003060
Iteration 230/1000 | Loss: 0.00003060
Iteration 231/1000 | Loss: 0.00003060
Iteration 232/1000 | Loss: 0.00003060
Iteration 233/1000 | Loss: 0.00003060
Iteration 234/1000 | Loss: 0.00003060
Iteration 235/1000 | Loss: 0.00003060
Iteration 236/1000 | Loss: 0.00003060
Iteration 237/1000 | Loss: 0.00003060
Iteration 238/1000 | Loss: 0.00003060
Iteration 239/1000 | Loss: 0.00003060
Iteration 240/1000 | Loss: 0.00003060
Iteration 241/1000 | Loss: 0.00003060
Iteration 242/1000 | Loss: 0.00003060
Iteration 243/1000 | Loss: 0.00003060
Iteration 244/1000 | Loss: 0.00003060
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 244. Stopping optimization.
Last 5 losses: [3.0599992896895856e-05, 3.0599992896895856e-05, 3.0599992896895856e-05, 3.0599992896895856e-05, 3.0599992896895856e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.0599992896895856e-05

Optimization complete. Final v2v error: 4.694629669189453 mm

Highest mean error: 6.68156099319458 mm for frame 120

Lowest mean error: 4.066072463989258 mm for frame 28

Saving results

Total time: 66.68575930595398
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_us_1259/0007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_1259/0007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_1259/0007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01196429
Iteration 2/25 | Loss: 0.01196429
Iteration 3/25 | Loss: 0.00269606
Iteration 4/25 | Loss: 0.00221352
Iteration 5/25 | Loss: 0.00205343
Iteration 6/25 | Loss: 0.00176673
Iteration 7/25 | Loss: 0.00152792
Iteration 8/25 | Loss: 0.00138151
Iteration 9/25 | Loss: 0.00131344
Iteration 10/25 | Loss: 0.00129030
Iteration 11/25 | Loss: 0.00128246
Iteration 12/25 | Loss: 0.00128045
Iteration 13/25 | Loss: 0.00127978
Iteration 14/25 | Loss: 0.00127963
Iteration 15/25 | Loss: 0.00127963
Iteration 16/25 | Loss: 0.00127963
Iteration 17/25 | Loss: 0.00127963
Iteration 18/25 | Loss: 0.00127963
Iteration 19/25 | Loss: 0.00127963
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0012796276714652777, 0.0012796276714652777, 0.0012796276714652777, 0.0012796276714652777, 0.0012796276714652777]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012796276714652777

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.57239473
Iteration 2/25 | Loss: 0.00220035
Iteration 3/25 | Loss: 0.00220035
Iteration 4/25 | Loss: 0.00220035
Iteration 5/25 | Loss: 0.00220035
Iteration 6/25 | Loss: 0.00220035
Iteration 7/25 | Loss: 0.00220035
Iteration 8/25 | Loss: 0.00220035
Iteration 9/25 | Loss: 0.00220035
Iteration 10/25 | Loss: 0.00220035
Iteration 11/25 | Loss: 0.00220035
Iteration 12/25 | Loss: 0.00220035
Iteration 13/25 | Loss: 0.00220035
Iteration 14/25 | Loss: 0.00220035
Iteration 15/25 | Loss: 0.00220035
Iteration 16/25 | Loss: 0.00220035
Iteration 17/25 | Loss: 0.00220035
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0022003520280122757, 0.0022003520280122757, 0.0022003520280122757, 0.0022003520280122757, 0.0022003520280122757]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0022003520280122757

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00220035
Iteration 2/1000 | Loss: 0.00005036
Iteration 3/1000 | Loss: 0.00003853
Iteration 4/1000 | Loss: 0.00003344
Iteration 5/1000 | Loss: 0.00003161
Iteration 6/1000 | Loss: 0.00003059
Iteration 7/1000 | Loss: 0.00002962
Iteration 8/1000 | Loss: 0.00002890
Iteration 9/1000 | Loss: 0.00002862
Iteration 10/1000 | Loss: 0.00002843
Iteration 11/1000 | Loss: 0.00002838
Iteration 12/1000 | Loss: 0.00002835
Iteration 13/1000 | Loss: 0.00002835
Iteration 14/1000 | Loss: 0.00002827
Iteration 15/1000 | Loss: 0.00002824
Iteration 16/1000 | Loss: 0.00002819
Iteration 17/1000 | Loss: 0.00002818
Iteration 18/1000 | Loss: 0.00002814
Iteration 19/1000 | Loss: 0.00002814
Iteration 20/1000 | Loss: 0.00002814
Iteration 21/1000 | Loss: 0.00002814
Iteration 22/1000 | Loss: 0.00002814
Iteration 23/1000 | Loss: 0.00002813
Iteration 24/1000 | Loss: 0.00002813
Iteration 25/1000 | Loss: 0.00002813
Iteration 26/1000 | Loss: 0.00002813
Iteration 27/1000 | Loss: 0.00002813
Iteration 28/1000 | Loss: 0.00002812
Iteration 29/1000 | Loss: 0.00002812
Iteration 30/1000 | Loss: 0.00002810
Iteration 31/1000 | Loss: 0.00002810
Iteration 32/1000 | Loss: 0.00002809
Iteration 33/1000 | Loss: 0.00002809
Iteration 34/1000 | Loss: 0.00002809
Iteration 35/1000 | Loss: 0.00002809
Iteration 36/1000 | Loss: 0.00002808
Iteration 37/1000 | Loss: 0.00002806
Iteration 38/1000 | Loss: 0.00002806
Iteration 39/1000 | Loss: 0.00002806
Iteration 40/1000 | Loss: 0.00002806
Iteration 41/1000 | Loss: 0.00002806
Iteration 42/1000 | Loss: 0.00002806
Iteration 43/1000 | Loss: 0.00002806
Iteration 44/1000 | Loss: 0.00002806
Iteration 45/1000 | Loss: 0.00002806
Iteration 46/1000 | Loss: 0.00002806
Iteration 47/1000 | Loss: 0.00002806
Iteration 48/1000 | Loss: 0.00002805
Iteration 49/1000 | Loss: 0.00002805
Iteration 50/1000 | Loss: 0.00002804
Iteration 51/1000 | Loss: 0.00002804
Iteration 52/1000 | Loss: 0.00002804
Iteration 53/1000 | Loss: 0.00002804
Iteration 54/1000 | Loss: 0.00002804
Iteration 55/1000 | Loss: 0.00002803
Iteration 56/1000 | Loss: 0.00002803
Iteration 57/1000 | Loss: 0.00002803
Iteration 58/1000 | Loss: 0.00002803
Iteration 59/1000 | Loss: 0.00002803
Iteration 60/1000 | Loss: 0.00002803
Iteration 61/1000 | Loss: 0.00002803
Iteration 62/1000 | Loss: 0.00002802
Iteration 63/1000 | Loss: 0.00002802
Iteration 64/1000 | Loss: 0.00002802
Iteration 65/1000 | Loss: 0.00002801
Iteration 66/1000 | Loss: 0.00002801
Iteration 67/1000 | Loss: 0.00002801
Iteration 68/1000 | Loss: 0.00002801
Iteration 69/1000 | Loss: 0.00002801
Iteration 70/1000 | Loss: 0.00002801
Iteration 71/1000 | Loss: 0.00002801
Iteration 72/1000 | Loss: 0.00002800
Iteration 73/1000 | Loss: 0.00002800
Iteration 74/1000 | Loss: 0.00002800
Iteration 75/1000 | Loss: 0.00002800
Iteration 76/1000 | Loss: 0.00002800
Iteration 77/1000 | Loss: 0.00002800
Iteration 78/1000 | Loss: 0.00002800
Iteration 79/1000 | Loss: 0.00002799
Iteration 80/1000 | Loss: 0.00002799
Iteration 81/1000 | Loss: 0.00002799
Iteration 82/1000 | Loss: 0.00002799
Iteration 83/1000 | Loss: 0.00002799
Iteration 84/1000 | Loss: 0.00002799
Iteration 85/1000 | Loss: 0.00002799
Iteration 86/1000 | Loss: 0.00002799
Iteration 87/1000 | Loss: 0.00002799
Iteration 88/1000 | Loss: 0.00002799
Iteration 89/1000 | Loss: 0.00002798
Iteration 90/1000 | Loss: 0.00002798
Iteration 91/1000 | Loss: 0.00002798
Iteration 92/1000 | Loss: 0.00002798
Iteration 93/1000 | Loss: 0.00002798
Iteration 94/1000 | Loss: 0.00002798
Iteration 95/1000 | Loss: 0.00002797
Iteration 96/1000 | Loss: 0.00002797
Iteration 97/1000 | Loss: 0.00002797
Iteration 98/1000 | Loss: 0.00002797
Iteration 99/1000 | Loss: 0.00002797
Iteration 100/1000 | Loss: 0.00002796
Iteration 101/1000 | Loss: 0.00002796
Iteration 102/1000 | Loss: 0.00002796
Iteration 103/1000 | Loss: 0.00002796
Iteration 104/1000 | Loss: 0.00002796
Iteration 105/1000 | Loss: 0.00002796
Iteration 106/1000 | Loss: 0.00002796
Iteration 107/1000 | Loss: 0.00002796
Iteration 108/1000 | Loss: 0.00002796
Iteration 109/1000 | Loss: 0.00002796
Iteration 110/1000 | Loss: 0.00002796
Iteration 111/1000 | Loss: 0.00002796
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 111. Stopping optimization.
Last 5 losses: [2.7959302315139212e-05, 2.7959302315139212e-05, 2.7959302315139212e-05, 2.7959302315139212e-05, 2.7959302315139212e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.7959302315139212e-05

Optimization complete. Final v2v error: 4.543170928955078 mm

Highest mean error: 4.894587516784668 mm for frame 172

Lowest mean error: 4.300283432006836 mm for frame 3

Saving results

Total time: 43.618901014328
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_us_1259/0004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_1259/0004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_1259/0004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00938904
Iteration 2/25 | Loss: 0.00170598
Iteration 3/25 | Loss: 0.00141269
Iteration 4/25 | Loss: 0.00138599
Iteration 5/25 | Loss: 0.00137875
Iteration 6/25 | Loss: 0.00137731
Iteration 7/25 | Loss: 0.00137728
Iteration 8/25 | Loss: 0.00137728
Iteration 9/25 | Loss: 0.00137728
Iteration 10/25 | Loss: 0.00137728
Iteration 11/25 | Loss: 0.00137728
Iteration 12/25 | Loss: 0.00137728
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0013772808015346527, 0.0013772808015346527, 0.0013772808015346527, 0.0013772808015346527, 0.0013772808015346527]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013772808015346527

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.16575515
Iteration 2/25 | Loss: 0.00224399
Iteration 3/25 | Loss: 0.00224398
Iteration 4/25 | Loss: 0.00224398
Iteration 5/25 | Loss: 0.00224398
Iteration 6/25 | Loss: 0.00224397
Iteration 7/25 | Loss: 0.00224397
Iteration 8/25 | Loss: 0.00224397
Iteration 9/25 | Loss: 0.00224397
Iteration 10/25 | Loss: 0.00224397
Iteration 11/25 | Loss: 0.00224397
Iteration 12/25 | Loss: 0.00224397
Iteration 13/25 | Loss: 0.00224397
Iteration 14/25 | Loss: 0.00224397
Iteration 15/25 | Loss: 0.00224397
Iteration 16/25 | Loss: 0.00224397
Iteration 17/25 | Loss: 0.00224397
Iteration 18/25 | Loss: 0.00224397
Iteration 19/25 | Loss: 0.00224397
Iteration 20/25 | Loss: 0.00224397
Iteration 21/25 | Loss: 0.00224397
Iteration 22/25 | Loss: 0.00224397
Iteration 23/25 | Loss: 0.00224397
Iteration 24/25 | Loss: 0.00224397
Iteration 25/25 | Loss: 0.00224397

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00224397
Iteration 2/1000 | Loss: 0.00005218
Iteration 3/1000 | Loss: 0.00003984
Iteration 4/1000 | Loss: 0.00003631
Iteration 5/1000 | Loss: 0.00003475
Iteration 6/1000 | Loss: 0.00003365
Iteration 7/1000 | Loss: 0.00003262
Iteration 8/1000 | Loss: 0.00003222
Iteration 9/1000 | Loss: 0.00003196
Iteration 10/1000 | Loss: 0.00003174
Iteration 11/1000 | Loss: 0.00003161
Iteration 12/1000 | Loss: 0.00003157
Iteration 13/1000 | Loss: 0.00003154
Iteration 14/1000 | Loss: 0.00003148
Iteration 15/1000 | Loss: 0.00003148
Iteration 16/1000 | Loss: 0.00003148
Iteration 17/1000 | Loss: 0.00003147
Iteration 18/1000 | Loss: 0.00003145
Iteration 19/1000 | Loss: 0.00003144
Iteration 20/1000 | Loss: 0.00003144
Iteration 21/1000 | Loss: 0.00003144
Iteration 22/1000 | Loss: 0.00003144
Iteration 23/1000 | Loss: 0.00003143
Iteration 24/1000 | Loss: 0.00003143
Iteration 25/1000 | Loss: 0.00003143
Iteration 26/1000 | Loss: 0.00003142
Iteration 27/1000 | Loss: 0.00003142
Iteration 28/1000 | Loss: 0.00003142
Iteration 29/1000 | Loss: 0.00003140
Iteration 30/1000 | Loss: 0.00003140
Iteration 31/1000 | Loss: 0.00003140
Iteration 32/1000 | Loss: 0.00003140
Iteration 33/1000 | Loss: 0.00003140
Iteration 34/1000 | Loss: 0.00003140
Iteration 35/1000 | Loss: 0.00003140
Iteration 36/1000 | Loss: 0.00003140
Iteration 37/1000 | Loss: 0.00003140
Iteration 38/1000 | Loss: 0.00003139
Iteration 39/1000 | Loss: 0.00003139
Iteration 40/1000 | Loss: 0.00003139
Iteration 41/1000 | Loss: 0.00003139
Iteration 42/1000 | Loss: 0.00003139
Iteration 43/1000 | Loss: 0.00003139
Iteration 44/1000 | Loss: 0.00003139
Iteration 45/1000 | Loss: 0.00003139
Iteration 46/1000 | Loss: 0.00003139
Iteration 47/1000 | Loss: 0.00003139
Iteration 48/1000 | Loss: 0.00003139
Iteration 49/1000 | Loss: 0.00003139
Iteration 50/1000 | Loss: 0.00003139
Iteration 51/1000 | Loss: 0.00003139
Iteration 52/1000 | Loss: 0.00003139
Iteration 53/1000 | Loss: 0.00003139
Iteration 54/1000 | Loss: 0.00003139
Iteration 55/1000 | Loss: 0.00003139
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 55. Stopping optimization.
Last 5 losses: [3.139375621685758e-05, 3.139375621685758e-05, 3.139375621685758e-05, 3.139375621685758e-05, 3.139375621685758e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.139375621685758e-05

Optimization complete. Final v2v error: 4.780815124511719 mm

Highest mean error: 5.091125965118408 mm for frame 97

Lowest mean error: 4.315011024475098 mm for frame 47

Saving results

Total time: 28.39878010749817
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_us_1259/0009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_1259/0009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_1259/0009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01054737
Iteration 2/25 | Loss: 0.00242757
Iteration 3/25 | Loss: 0.00150804
Iteration 4/25 | Loss: 0.00134508
Iteration 5/25 | Loss: 0.00132669
Iteration 6/25 | Loss: 0.00131293
Iteration 7/25 | Loss: 0.00130897
Iteration 8/25 | Loss: 0.00129358
Iteration 9/25 | Loss: 0.00129440
Iteration 10/25 | Loss: 0.00128404
Iteration 11/25 | Loss: 0.00127799
Iteration 12/25 | Loss: 0.00127604
Iteration 13/25 | Loss: 0.00127192
Iteration 14/25 | Loss: 0.00126992
Iteration 15/25 | Loss: 0.00127444
Iteration 16/25 | Loss: 0.00127287
Iteration 17/25 | Loss: 0.00127999
Iteration 18/25 | Loss: 0.00127816
Iteration 19/25 | Loss: 0.00128405
Iteration 20/25 | Loss: 0.00128449
Iteration 21/25 | Loss: 0.00127930
Iteration 22/25 | Loss: 0.00127582
Iteration 23/25 | Loss: 0.00128397
Iteration 24/25 | Loss: 0.00129576
Iteration 25/25 | Loss: 0.00129508

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.57613158
Iteration 2/25 | Loss: 0.00326592
Iteration 3/25 | Loss: 0.00309483
Iteration 4/25 | Loss: 0.00309483
Iteration 5/25 | Loss: 0.00309483
Iteration 6/25 | Loss: 0.00309483
Iteration 7/25 | Loss: 0.00309483
Iteration 8/25 | Loss: 0.00309483
Iteration 9/25 | Loss: 0.00309483
Iteration 10/25 | Loss: 0.00309483
Iteration 11/25 | Loss: 0.00309483
Iteration 12/25 | Loss: 0.00309483
Iteration 13/25 | Loss: 0.00309483
Iteration 14/25 | Loss: 0.00309483
Iteration 15/25 | Loss: 0.00309483
Iteration 16/25 | Loss: 0.00309483
Iteration 17/25 | Loss: 0.00309483
Iteration 18/25 | Loss: 0.00309483
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0030948284547775984, 0.0030948284547775984, 0.0030948284547775984, 0.0030948284547775984, 0.0030948284547775984]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0030948284547775984

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00309483
Iteration 2/1000 | Loss: 0.00076960
Iteration 3/1000 | Loss: 0.00018083
Iteration 4/1000 | Loss: 0.00023381
Iteration 5/1000 | Loss: 0.00005976
Iteration 6/1000 | Loss: 0.00026811
Iteration 7/1000 | Loss: 0.00034907
Iteration 8/1000 | Loss: 0.00007642
Iteration 9/1000 | Loss: 0.00010873
Iteration 10/1000 | Loss: 0.00014628
Iteration 11/1000 | Loss: 0.00033052
Iteration 12/1000 | Loss: 0.00040332
Iteration 13/1000 | Loss: 0.00033219
Iteration 14/1000 | Loss: 0.00022984
Iteration 15/1000 | Loss: 0.00073269
Iteration 16/1000 | Loss: 0.00019976
Iteration 17/1000 | Loss: 0.00014628
Iteration 18/1000 | Loss: 0.00010720
Iteration 19/1000 | Loss: 0.00014075
Iteration 20/1000 | Loss: 0.00050446
Iteration 21/1000 | Loss: 0.00056391
Iteration 22/1000 | Loss: 0.00010059
Iteration 23/1000 | Loss: 0.00028944
Iteration 24/1000 | Loss: 0.00041833
Iteration 25/1000 | Loss: 0.00030344
Iteration 26/1000 | Loss: 0.00038617
Iteration 27/1000 | Loss: 0.00053883
Iteration 28/1000 | Loss: 0.00035051
Iteration 29/1000 | Loss: 0.00037657
Iteration 30/1000 | Loss: 0.00047430
Iteration 31/1000 | Loss: 0.00052649
Iteration 32/1000 | Loss: 0.00033852
Iteration 33/1000 | Loss: 0.00035264
Iteration 34/1000 | Loss: 0.00011304
Iteration 35/1000 | Loss: 0.00013421
Iteration 36/1000 | Loss: 0.00012150
Iteration 37/1000 | Loss: 0.00012262
Iteration 38/1000 | Loss: 0.00027623
Iteration 39/1000 | Loss: 0.00036142
Iteration 40/1000 | Loss: 0.00028376
Iteration 41/1000 | Loss: 0.00024323
Iteration 42/1000 | Loss: 0.00007007
Iteration 43/1000 | Loss: 0.00035984
Iteration 44/1000 | Loss: 0.00009896
Iteration 45/1000 | Loss: 0.00028217
Iteration 46/1000 | Loss: 0.00035960
Iteration 47/1000 | Loss: 0.00038425
Iteration 48/1000 | Loss: 0.00022145
Iteration 49/1000 | Loss: 0.00004563
Iteration 50/1000 | Loss: 0.00004007
Iteration 51/1000 | Loss: 0.00003653
Iteration 52/1000 | Loss: 0.00028244
Iteration 53/1000 | Loss: 0.00032271
Iteration 54/1000 | Loss: 0.00015411
Iteration 55/1000 | Loss: 0.00030926
Iteration 56/1000 | Loss: 0.00033249
Iteration 57/1000 | Loss: 0.00031354
Iteration 58/1000 | Loss: 0.00039411
Iteration 59/1000 | Loss: 0.00030495
Iteration 60/1000 | Loss: 0.00130327
Iteration 61/1000 | Loss: 0.00078575
Iteration 62/1000 | Loss: 0.00089898
Iteration 63/1000 | Loss: 0.00019628
Iteration 64/1000 | Loss: 0.00025160
Iteration 65/1000 | Loss: 0.00046908
Iteration 66/1000 | Loss: 0.00028444
Iteration 67/1000 | Loss: 0.00038989
Iteration 68/1000 | Loss: 0.00036766
Iteration 69/1000 | Loss: 0.00033877
Iteration 70/1000 | Loss: 0.00020194
Iteration 71/1000 | Loss: 0.00022851
Iteration 72/1000 | Loss: 0.00032647
Iteration 73/1000 | Loss: 0.00044346
Iteration 74/1000 | Loss: 0.00066879
Iteration 75/1000 | Loss: 0.00037769
Iteration 76/1000 | Loss: 0.00068479
Iteration 77/1000 | Loss: 0.00033927
Iteration 78/1000 | Loss: 0.00052518
Iteration 79/1000 | Loss: 0.00040147
Iteration 80/1000 | Loss: 0.00052463
Iteration 81/1000 | Loss: 0.00039198
Iteration 82/1000 | Loss: 0.00049410
Iteration 83/1000 | Loss: 0.00005090
Iteration 84/1000 | Loss: 0.00003964
Iteration 85/1000 | Loss: 0.00003661
Iteration 86/1000 | Loss: 0.00031164
Iteration 87/1000 | Loss: 0.00004280
Iteration 88/1000 | Loss: 0.00003959
Iteration 89/1000 | Loss: 0.00086432
Iteration 90/1000 | Loss: 0.00060908
Iteration 91/1000 | Loss: 0.00034215
Iteration 92/1000 | Loss: 0.00003998
Iteration 93/1000 | Loss: 0.00025104
Iteration 94/1000 | Loss: 0.00069170
Iteration 95/1000 | Loss: 0.00005056
Iteration 96/1000 | Loss: 0.00017938
Iteration 97/1000 | Loss: 0.00017007
Iteration 98/1000 | Loss: 0.00003845
Iteration 99/1000 | Loss: 0.00003626
Iteration 100/1000 | Loss: 0.00003426
Iteration 101/1000 | Loss: 0.00025194
Iteration 102/1000 | Loss: 0.00018254
Iteration 103/1000 | Loss: 0.00021925
Iteration 104/1000 | Loss: 0.00015545
Iteration 105/1000 | Loss: 0.00012460
Iteration 106/1000 | Loss: 0.00015715
Iteration 107/1000 | Loss: 0.00034122
Iteration 108/1000 | Loss: 0.00028215
Iteration 109/1000 | Loss: 0.00005952
Iteration 110/1000 | Loss: 0.00003437
Iteration 111/1000 | Loss: 0.00003056
Iteration 112/1000 | Loss: 0.00002956
Iteration 113/1000 | Loss: 0.00002890
Iteration 114/1000 | Loss: 0.00002858
Iteration 115/1000 | Loss: 0.00002835
Iteration 116/1000 | Loss: 0.00002826
Iteration 117/1000 | Loss: 0.00025995
Iteration 118/1000 | Loss: 0.00004274
Iteration 119/1000 | Loss: 0.00003221
Iteration 120/1000 | Loss: 0.00002999
Iteration 121/1000 | Loss: 0.00002838
Iteration 122/1000 | Loss: 0.00002765
Iteration 123/1000 | Loss: 0.00002718
Iteration 124/1000 | Loss: 0.00002693
Iteration 125/1000 | Loss: 0.00002679
Iteration 126/1000 | Loss: 0.00002678
Iteration 127/1000 | Loss: 0.00002677
Iteration 128/1000 | Loss: 0.00002676
Iteration 129/1000 | Loss: 0.00002676
Iteration 130/1000 | Loss: 0.00002676
Iteration 131/1000 | Loss: 0.00002676
Iteration 132/1000 | Loss: 0.00002676
Iteration 133/1000 | Loss: 0.00002675
Iteration 134/1000 | Loss: 0.00002675
Iteration 135/1000 | Loss: 0.00002675
Iteration 136/1000 | Loss: 0.00002675
Iteration 137/1000 | Loss: 0.00002675
Iteration 138/1000 | Loss: 0.00002675
Iteration 139/1000 | Loss: 0.00002674
Iteration 140/1000 | Loss: 0.00002674
Iteration 141/1000 | Loss: 0.00002674
Iteration 142/1000 | Loss: 0.00002674
Iteration 143/1000 | Loss: 0.00002673
Iteration 144/1000 | Loss: 0.00002673
Iteration 145/1000 | Loss: 0.00002673
Iteration 146/1000 | Loss: 0.00002672
Iteration 147/1000 | Loss: 0.00002672
Iteration 148/1000 | Loss: 0.00002672
Iteration 149/1000 | Loss: 0.00002671
Iteration 150/1000 | Loss: 0.00002671
Iteration 151/1000 | Loss: 0.00002671
Iteration 152/1000 | Loss: 0.00002671
Iteration 153/1000 | Loss: 0.00002671
Iteration 154/1000 | Loss: 0.00002671
Iteration 155/1000 | Loss: 0.00002671
Iteration 156/1000 | Loss: 0.00002671
Iteration 157/1000 | Loss: 0.00002671
Iteration 158/1000 | Loss: 0.00002671
Iteration 159/1000 | Loss: 0.00002671
Iteration 160/1000 | Loss: 0.00002671
Iteration 161/1000 | Loss: 0.00002671
Iteration 162/1000 | Loss: 0.00002670
Iteration 163/1000 | Loss: 0.00002670
Iteration 164/1000 | Loss: 0.00002670
Iteration 165/1000 | Loss: 0.00002669
Iteration 166/1000 | Loss: 0.00002669
Iteration 167/1000 | Loss: 0.00002669
Iteration 168/1000 | Loss: 0.00002668
Iteration 169/1000 | Loss: 0.00002668
Iteration 170/1000 | Loss: 0.00002668
Iteration 171/1000 | Loss: 0.00002668
Iteration 172/1000 | Loss: 0.00002667
Iteration 173/1000 | Loss: 0.00002667
Iteration 174/1000 | Loss: 0.00002667
Iteration 175/1000 | Loss: 0.00002667
Iteration 176/1000 | Loss: 0.00002667
Iteration 177/1000 | Loss: 0.00002667
Iteration 178/1000 | Loss: 0.00002667
Iteration 179/1000 | Loss: 0.00002666
Iteration 180/1000 | Loss: 0.00002666
Iteration 181/1000 | Loss: 0.00002665
Iteration 182/1000 | Loss: 0.00002665
Iteration 183/1000 | Loss: 0.00002665
Iteration 184/1000 | Loss: 0.00002665
Iteration 185/1000 | Loss: 0.00002665
Iteration 186/1000 | Loss: 0.00002665
Iteration 187/1000 | Loss: 0.00002664
Iteration 188/1000 | Loss: 0.00002664
Iteration 189/1000 | Loss: 0.00002663
Iteration 190/1000 | Loss: 0.00002663
Iteration 191/1000 | Loss: 0.00002663
Iteration 192/1000 | Loss: 0.00002662
Iteration 193/1000 | Loss: 0.00002662
Iteration 194/1000 | Loss: 0.00002662
Iteration 195/1000 | Loss: 0.00002662
Iteration 196/1000 | Loss: 0.00002662
Iteration 197/1000 | Loss: 0.00002662
Iteration 198/1000 | Loss: 0.00002662
Iteration 199/1000 | Loss: 0.00002662
Iteration 200/1000 | Loss: 0.00002662
Iteration 201/1000 | Loss: 0.00002662
Iteration 202/1000 | Loss: 0.00002662
Iteration 203/1000 | Loss: 0.00002662
Iteration 204/1000 | Loss: 0.00002661
Iteration 205/1000 | Loss: 0.00002661
Iteration 206/1000 | Loss: 0.00002661
Iteration 207/1000 | Loss: 0.00002661
Iteration 208/1000 | Loss: 0.00002661
Iteration 209/1000 | Loss: 0.00002660
Iteration 210/1000 | Loss: 0.00002660
Iteration 211/1000 | Loss: 0.00002660
Iteration 212/1000 | Loss: 0.00002660
Iteration 213/1000 | Loss: 0.00002659
Iteration 214/1000 | Loss: 0.00002659
Iteration 215/1000 | Loss: 0.00002659
Iteration 216/1000 | Loss: 0.00002659
Iteration 217/1000 | Loss: 0.00002659
Iteration 218/1000 | Loss: 0.00002659
Iteration 219/1000 | Loss: 0.00002659
Iteration 220/1000 | Loss: 0.00002658
Iteration 221/1000 | Loss: 0.00002658
Iteration 222/1000 | Loss: 0.00002658
Iteration 223/1000 | Loss: 0.00002658
Iteration 224/1000 | Loss: 0.00002658
Iteration 225/1000 | Loss: 0.00002658
Iteration 226/1000 | Loss: 0.00002658
Iteration 227/1000 | Loss: 0.00002658
Iteration 228/1000 | Loss: 0.00002657
Iteration 229/1000 | Loss: 0.00002657
Iteration 230/1000 | Loss: 0.00002657
Iteration 231/1000 | Loss: 0.00002657
Iteration 232/1000 | Loss: 0.00002657
Iteration 233/1000 | Loss: 0.00002657
Iteration 234/1000 | Loss: 0.00002657
Iteration 235/1000 | Loss: 0.00002657
Iteration 236/1000 | Loss: 0.00002657
Iteration 237/1000 | Loss: 0.00002657
Iteration 238/1000 | Loss: 0.00002657
Iteration 239/1000 | Loss: 0.00002657
Iteration 240/1000 | Loss: 0.00002657
Iteration 241/1000 | Loss: 0.00002657
Iteration 242/1000 | Loss: 0.00002657
Iteration 243/1000 | Loss: 0.00002657
Iteration 244/1000 | Loss: 0.00002657
Iteration 245/1000 | Loss: 0.00002657
Iteration 246/1000 | Loss: 0.00002657
Iteration 247/1000 | Loss: 0.00002657
Iteration 248/1000 | Loss: 0.00002657
Iteration 249/1000 | Loss: 0.00002657
Iteration 250/1000 | Loss: 0.00002656
Iteration 251/1000 | Loss: 0.00002656
Iteration 252/1000 | Loss: 0.00002656
Iteration 253/1000 | Loss: 0.00002656
Iteration 254/1000 | Loss: 0.00002656
Iteration 255/1000 | Loss: 0.00002656
Iteration 256/1000 | Loss: 0.00002656
Iteration 257/1000 | Loss: 0.00002656
Iteration 258/1000 | Loss: 0.00002656
Iteration 259/1000 | Loss: 0.00002656
Iteration 260/1000 | Loss: 0.00002656
Iteration 261/1000 | Loss: 0.00002656
Iteration 262/1000 | Loss: 0.00002656
Iteration 263/1000 | Loss: 0.00002656
Iteration 264/1000 | Loss: 0.00002656
Iteration 265/1000 | Loss: 0.00002656
Iteration 266/1000 | Loss: 0.00002656
Iteration 267/1000 | Loss: 0.00002656
Iteration 268/1000 | Loss: 0.00002656
Iteration 269/1000 | Loss: 0.00002656
Iteration 270/1000 | Loss: 0.00002656
Iteration 271/1000 | Loss: 0.00002656
Iteration 272/1000 | Loss: 0.00002656
Iteration 273/1000 | Loss: 0.00002656
Iteration 274/1000 | Loss: 0.00002656
Iteration 275/1000 | Loss: 0.00002656
Iteration 276/1000 | Loss: 0.00002655
Iteration 277/1000 | Loss: 0.00002655
Iteration 278/1000 | Loss: 0.00002655
Iteration 279/1000 | Loss: 0.00002655
Iteration 280/1000 | Loss: 0.00002655
Iteration 281/1000 | Loss: 0.00002655
Iteration 282/1000 | Loss: 0.00002655
Iteration 283/1000 | Loss: 0.00002655
Iteration 284/1000 | Loss: 0.00002655
Iteration 285/1000 | Loss: 0.00002655
Iteration 286/1000 | Loss: 0.00002654
Iteration 287/1000 | Loss: 0.00002654
Iteration 288/1000 | Loss: 0.00002654
Iteration 289/1000 | Loss: 0.00002654
Iteration 290/1000 | Loss: 0.00002654
Iteration 291/1000 | Loss: 0.00002654
Iteration 292/1000 | Loss: 0.00002654
Iteration 293/1000 | Loss: 0.00002654
Iteration 294/1000 | Loss: 0.00002654
Iteration 295/1000 | Loss: 0.00002654
Iteration 296/1000 | Loss: 0.00002654
Iteration 297/1000 | Loss: 0.00002654
Iteration 298/1000 | Loss: 0.00002654
Iteration 299/1000 | Loss: 0.00002654
Iteration 300/1000 | Loss: 0.00002653
Iteration 301/1000 | Loss: 0.00002653
Iteration 302/1000 | Loss: 0.00002653
Iteration 303/1000 | Loss: 0.00002653
Iteration 304/1000 | Loss: 0.00002653
Iteration 305/1000 | Loss: 0.00002653
Iteration 306/1000 | Loss: 0.00002653
Iteration 307/1000 | Loss: 0.00002653
Iteration 308/1000 | Loss: 0.00002653
Iteration 309/1000 | Loss: 0.00002653
Iteration 310/1000 | Loss: 0.00002653
Iteration 311/1000 | Loss: 0.00002653
Iteration 312/1000 | Loss: 0.00002653
Iteration 313/1000 | Loss: 0.00002653
Iteration 314/1000 | Loss: 0.00002653
Iteration 315/1000 | Loss: 0.00002653
Iteration 316/1000 | Loss: 0.00002653
Iteration 317/1000 | Loss: 0.00002653
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 317. Stopping optimization.
Last 5 losses: [2.6528021408012137e-05, 2.6528021408012137e-05, 2.6528021408012137e-05, 2.6528021408012137e-05, 2.6528021408012137e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.6528021408012137e-05

Optimization complete. Final v2v error: 4.289917469024658 mm

Highest mean error: 15.040338516235352 mm for frame 38

Lowest mean error: 3.7611963748931885 mm for frame 104

Saving results

Total time: 241.97224020957947
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_us_1259/0011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_1259/0011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_1259/0011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00886938
Iteration 2/25 | Loss: 0.00174136
Iteration 3/25 | Loss: 0.00148200
Iteration 4/25 | Loss: 0.00141792
Iteration 5/25 | Loss: 0.00139722
Iteration 6/25 | Loss: 0.00139226
Iteration 7/25 | Loss: 0.00138998
Iteration 8/25 | Loss: 0.00138998
Iteration 9/25 | Loss: 0.00138998
Iteration 10/25 | Loss: 0.00138998
Iteration 11/25 | Loss: 0.00138998
Iteration 12/25 | Loss: 0.00138998
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0013899793848395348, 0.0013899793848395348, 0.0013899793848395348, 0.0013899793848395348, 0.0013899793848395348]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013899793848395348

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.69901216
Iteration 2/25 | Loss: 0.00360564
Iteration 3/25 | Loss: 0.00360563
Iteration 4/25 | Loss: 0.00360563
Iteration 5/25 | Loss: 0.00360562
Iteration 6/25 | Loss: 0.00360562
Iteration 7/25 | Loss: 0.00360562
Iteration 8/25 | Loss: 0.00360562
Iteration 9/25 | Loss: 0.00360562
Iteration 10/25 | Loss: 0.00360562
Iteration 11/25 | Loss: 0.00360562
Iteration 12/25 | Loss: 0.00360562
Iteration 13/25 | Loss: 0.00360562
Iteration 14/25 | Loss: 0.00360562
Iteration 15/25 | Loss: 0.00360562
Iteration 16/25 | Loss: 0.00360562
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.003605622099712491, 0.003605622099712491, 0.003605622099712491, 0.003605622099712491, 0.003605622099712491]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.003605622099712491

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00360562
Iteration 2/1000 | Loss: 0.00008301
Iteration 3/1000 | Loss: 0.00006453
Iteration 4/1000 | Loss: 0.00005252
Iteration 5/1000 | Loss: 0.00004818
Iteration 6/1000 | Loss: 0.00004617
Iteration 7/1000 | Loss: 0.00004387
Iteration 8/1000 | Loss: 0.00004234
Iteration 9/1000 | Loss: 0.00004137
Iteration 10/1000 | Loss: 0.00004037
Iteration 11/1000 | Loss: 0.00003979
Iteration 12/1000 | Loss: 0.00003934
Iteration 13/1000 | Loss: 0.00003896
Iteration 14/1000 | Loss: 0.00003865
Iteration 15/1000 | Loss: 0.00003847
Iteration 16/1000 | Loss: 0.00003837
Iteration 17/1000 | Loss: 0.00003836
Iteration 18/1000 | Loss: 0.00003835
Iteration 19/1000 | Loss: 0.00003834
Iteration 20/1000 | Loss: 0.00003834
Iteration 21/1000 | Loss: 0.00003831
Iteration 22/1000 | Loss: 0.00003831
Iteration 23/1000 | Loss: 0.00003829
Iteration 24/1000 | Loss: 0.00003826
Iteration 25/1000 | Loss: 0.00003825
Iteration 26/1000 | Loss: 0.00003824
Iteration 27/1000 | Loss: 0.00003824
Iteration 28/1000 | Loss: 0.00003823
Iteration 29/1000 | Loss: 0.00003823
Iteration 30/1000 | Loss: 0.00003823
Iteration 31/1000 | Loss: 0.00003822
Iteration 32/1000 | Loss: 0.00003822
Iteration 33/1000 | Loss: 0.00003821
Iteration 34/1000 | Loss: 0.00003821
Iteration 35/1000 | Loss: 0.00003820
Iteration 36/1000 | Loss: 0.00003820
Iteration 37/1000 | Loss: 0.00003820
Iteration 38/1000 | Loss: 0.00003820
Iteration 39/1000 | Loss: 0.00003820
Iteration 40/1000 | Loss: 0.00003819
Iteration 41/1000 | Loss: 0.00003819
Iteration 42/1000 | Loss: 0.00003819
Iteration 43/1000 | Loss: 0.00003819
Iteration 44/1000 | Loss: 0.00003818
Iteration 45/1000 | Loss: 0.00003818
Iteration 46/1000 | Loss: 0.00003818
Iteration 47/1000 | Loss: 0.00003818
Iteration 48/1000 | Loss: 0.00003818
Iteration 49/1000 | Loss: 0.00003818
Iteration 50/1000 | Loss: 0.00003818
Iteration 51/1000 | Loss: 0.00003818
Iteration 52/1000 | Loss: 0.00003818
Iteration 53/1000 | Loss: 0.00003818
Iteration 54/1000 | Loss: 0.00003817
Iteration 55/1000 | Loss: 0.00003817
Iteration 56/1000 | Loss: 0.00003816
Iteration 57/1000 | Loss: 0.00003816
Iteration 58/1000 | Loss: 0.00003815
Iteration 59/1000 | Loss: 0.00003815
Iteration 60/1000 | Loss: 0.00003815
Iteration 61/1000 | Loss: 0.00003815
Iteration 62/1000 | Loss: 0.00003814
Iteration 63/1000 | Loss: 0.00003814
Iteration 64/1000 | Loss: 0.00003814
Iteration 65/1000 | Loss: 0.00003814
Iteration 66/1000 | Loss: 0.00003814
Iteration 67/1000 | Loss: 0.00003813
Iteration 68/1000 | Loss: 0.00003813
Iteration 69/1000 | Loss: 0.00003813
Iteration 70/1000 | Loss: 0.00003813
Iteration 71/1000 | Loss: 0.00003813
Iteration 72/1000 | Loss: 0.00003813
Iteration 73/1000 | Loss: 0.00003813
Iteration 74/1000 | Loss: 0.00003813
Iteration 75/1000 | Loss: 0.00003813
Iteration 76/1000 | Loss: 0.00003812
Iteration 77/1000 | Loss: 0.00003812
Iteration 78/1000 | Loss: 0.00003812
Iteration 79/1000 | Loss: 0.00003812
Iteration 80/1000 | Loss: 0.00003811
Iteration 81/1000 | Loss: 0.00003811
Iteration 82/1000 | Loss: 0.00003811
Iteration 83/1000 | Loss: 0.00003811
Iteration 84/1000 | Loss: 0.00003811
Iteration 85/1000 | Loss: 0.00003811
Iteration 86/1000 | Loss: 0.00003811
Iteration 87/1000 | Loss: 0.00003811
Iteration 88/1000 | Loss: 0.00003810
Iteration 89/1000 | Loss: 0.00003810
Iteration 90/1000 | Loss: 0.00003810
Iteration 91/1000 | Loss: 0.00003810
Iteration 92/1000 | Loss: 0.00003809
Iteration 93/1000 | Loss: 0.00003809
Iteration 94/1000 | Loss: 0.00003809
Iteration 95/1000 | Loss: 0.00003809
Iteration 96/1000 | Loss: 0.00003808
Iteration 97/1000 | Loss: 0.00003808
Iteration 98/1000 | Loss: 0.00003808
Iteration 99/1000 | Loss: 0.00003808
Iteration 100/1000 | Loss: 0.00003808
Iteration 101/1000 | Loss: 0.00003808
Iteration 102/1000 | Loss: 0.00003808
Iteration 103/1000 | Loss: 0.00003807
Iteration 104/1000 | Loss: 0.00003807
Iteration 105/1000 | Loss: 0.00003807
Iteration 106/1000 | Loss: 0.00003807
Iteration 107/1000 | Loss: 0.00003807
Iteration 108/1000 | Loss: 0.00003807
Iteration 109/1000 | Loss: 0.00003807
Iteration 110/1000 | Loss: 0.00003806
Iteration 111/1000 | Loss: 0.00003806
Iteration 112/1000 | Loss: 0.00003806
Iteration 113/1000 | Loss: 0.00003806
Iteration 114/1000 | Loss: 0.00003806
Iteration 115/1000 | Loss: 0.00003805
Iteration 116/1000 | Loss: 0.00003805
Iteration 117/1000 | Loss: 0.00003805
Iteration 118/1000 | Loss: 0.00003805
Iteration 119/1000 | Loss: 0.00003805
Iteration 120/1000 | Loss: 0.00003805
Iteration 121/1000 | Loss: 0.00003805
Iteration 122/1000 | Loss: 0.00003805
Iteration 123/1000 | Loss: 0.00003805
Iteration 124/1000 | Loss: 0.00003805
Iteration 125/1000 | Loss: 0.00003804
Iteration 126/1000 | Loss: 0.00003804
Iteration 127/1000 | Loss: 0.00003804
Iteration 128/1000 | Loss: 0.00003804
Iteration 129/1000 | Loss: 0.00003804
Iteration 130/1000 | Loss: 0.00003804
Iteration 131/1000 | Loss: 0.00003804
Iteration 132/1000 | Loss: 0.00003804
Iteration 133/1000 | Loss: 0.00003804
Iteration 134/1000 | Loss: 0.00003804
Iteration 135/1000 | Loss: 0.00003804
Iteration 136/1000 | Loss: 0.00003804
Iteration 137/1000 | Loss: 0.00003804
Iteration 138/1000 | Loss: 0.00003804
Iteration 139/1000 | Loss: 0.00003804
Iteration 140/1000 | Loss: 0.00003804
Iteration 141/1000 | Loss: 0.00003804
Iteration 142/1000 | Loss: 0.00003804
Iteration 143/1000 | Loss: 0.00003804
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 143. Stopping optimization.
Last 5 losses: [3.803685831371695e-05, 3.803685831371695e-05, 3.803685831371695e-05, 3.803685831371695e-05, 3.803685831371695e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.803685831371695e-05

Optimization complete. Final v2v error: 5.099616527557373 mm

Highest mean error: 6.234068870544434 mm for frame 138

Lowest mean error: 3.815685272216797 mm for frame 234

Saving results

Total time: 47.83342981338501
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_us_1259/0020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_1259/0020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_1259/0020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00938664
Iteration 2/25 | Loss: 0.00177788
Iteration 3/25 | Loss: 0.00134704
Iteration 4/25 | Loss: 0.00131677
Iteration 5/25 | Loss: 0.00130735
Iteration 6/25 | Loss: 0.00130501
Iteration 7/25 | Loss: 0.00130441
Iteration 8/25 | Loss: 0.00130441
Iteration 9/25 | Loss: 0.00130441
Iteration 10/25 | Loss: 0.00130441
Iteration 11/25 | Loss: 0.00130441
Iteration 12/25 | Loss: 0.00130441
Iteration 13/25 | Loss: 0.00130441
Iteration 14/25 | Loss: 0.00130441
Iteration 15/25 | Loss: 0.00130441
Iteration 16/25 | Loss: 0.00130441
Iteration 17/25 | Loss: 0.00130441
Iteration 18/25 | Loss: 0.00130441
Iteration 19/25 | Loss: 0.00130441
Iteration 20/25 | Loss: 0.00130441
Iteration 21/25 | Loss: 0.00130441
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0013044090010225773, 0.0013044090010225773, 0.0013044090010225773, 0.0013044090010225773, 0.0013044090010225773]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013044090010225773

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.78222084
Iteration 2/25 | Loss: 0.00230057
Iteration 3/25 | Loss: 0.00230057
Iteration 4/25 | Loss: 0.00230057
Iteration 5/25 | Loss: 0.00230057
Iteration 6/25 | Loss: 0.00230057
Iteration 7/25 | Loss: 0.00230057
Iteration 8/25 | Loss: 0.00230057
Iteration 9/25 | Loss: 0.00230057
Iteration 10/25 | Loss: 0.00230057
Iteration 11/25 | Loss: 0.00230056
Iteration 12/25 | Loss: 0.00230056
Iteration 13/25 | Loss: 0.00230057
Iteration 14/25 | Loss: 0.00230056
Iteration 15/25 | Loss: 0.00230056
Iteration 16/25 | Loss: 0.00230056
Iteration 17/25 | Loss: 0.00230056
Iteration 18/25 | Loss: 0.00230056
Iteration 19/25 | Loss: 0.00230056
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0023005648981779814, 0.0023005648981779814, 0.0023005648981779814, 0.0023005648981779814, 0.0023005648981779814]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0023005648981779814

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00230056
Iteration 2/1000 | Loss: 0.00006571
Iteration 3/1000 | Loss: 0.00005024
Iteration 4/1000 | Loss: 0.00004630
Iteration 5/1000 | Loss: 0.00004425
Iteration 6/1000 | Loss: 0.00004317
Iteration 7/1000 | Loss: 0.00004240
Iteration 8/1000 | Loss: 0.00004165
Iteration 9/1000 | Loss: 0.00004123
Iteration 10/1000 | Loss: 0.00004097
Iteration 11/1000 | Loss: 0.00004081
Iteration 12/1000 | Loss: 0.00004067
Iteration 13/1000 | Loss: 0.00004066
Iteration 14/1000 | Loss: 0.00004065
Iteration 15/1000 | Loss: 0.00004050
Iteration 16/1000 | Loss: 0.00004036
Iteration 17/1000 | Loss: 0.00004025
Iteration 18/1000 | Loss: 0.00004020
Iteration 19/1000 | Loss: 0.00004018
Iteration 20/1000 | Loss: 0.00004018
Iteration 21/1000 | Loss: 0.00004017
Iteration 22/1000 | Loss: 0.00004017
Iteration 23/1000 | Loss: 0.00004013
Iteration 24/1000 | Loss: 0.00004006
Iteration 25/1000 | Loss: 0.00004000
Iteration 26/1000 | Loss: 0.00003998
Iteration 27/1000 | Loss: 0.00003998
Iteration 28/1000 | Loss: 0.00003998
Iteration 29/1000 | Loss: 0.00003996
Iteration 30/1000 | Loss: 0.00003996
Iteration 31/1000 | Loss: 0.00003996
Iteration 32/1000 | Loss: 0.00003996
Iteration 33/1000 | Loss: 0.00003996
Iteration 34/1000 | Loss: 0.00003996
Iteration 35/1000 | Loss: 0.00003996
Iteration 36/1000 | Loss: 0.00003995
Iteration 37/1000 | Loss: 0.00003994
Iteration 38/1000 | Loss: 0.00003994
Iteration 39/1000 | Loss: 0.00003994
Iteration 40/1000 | Loss: 0.00003994
Iteration 41/1000 | Loss: 0.00003994
Iteration 42/1000 | Loss: 0.00003994
Iteration 43/1000 | Loss: 0.00003994
Iteration 44/1000 | Loss: 0.00003993
Iteration 45/1000 | Loss: 0.00003993
Iteration 46/1000 | Loss: 0.00003993
Iteration 47/1000 | Loss: 0.00003993
Iteration 48/1000 | Loss: 0.00003993
Iteration 49/1000 | Loss: 0.00003992
Iteration 50/1000 | Loss: 0.00003991
Iteration 51/1000 | Loss: 0.00003991
Iteration 52/1000 | Loss: 0.00003991
Iteration 53/1000 | Loss: 0.00003991
Iteration 54/1000 | Loss: 0.00003991
Iteration 55/1000 | Loss: 0.00003990
Iteration 56/1000 | Loss: 0.00003990
Iteration 57/1000 | Loss: 0.00003990
Iteration 58/1000 | Loss: 0.00003990
Iteration 59/1000 | Loss: 0.00003990
Iteration 60/1000 | Loss: 0.00003990
Iteration 61/1000 | Loss: 0.00003990
Iteration 62/1000 | Loss: 0.00003989
Iteration 63/1000 | Loss: 0.00003989
Iteration 64/1000 | Loss: 0.00003989
Iteration 65/1000 | Loss: 0.00003988
Iteration 66/1000 | Loss: 0.00003988
Iteration 67/1000 | Loss: 0.00003988
Iteration 68/1000 | Loss: 0.00003988
Iteration 69/1000 | Loss: 0.00003988
Iteration 70/1000 | Loss: 0.00003988
Iteration 71/1000 | Loss: 0.00003988
Iteration 72/1000 | Loss: 0.00003987
Iteration 73/1000 | Loss: 0.00003987
Iteration 74/1000 | Loss: 0.00003987
Iteration 75/1000 | Loss: 0.00003987
Iteration 76/1000 | Loss: 0.00003987
Iteration 77/1000 | Loss: 0.00003987
Iteration 78/1000 | Loss: 0.00003987
Iteration 79/1000 | Loss: 0.00003987
Iteration 80/1000 | Loss: 0.00003986
Iteration 81/1000 | Loss: 0.00003985
Iteration 82/1000 | Loss: 0.00003985
Iteration 83/1000 | Loss: 0.00003985
Iteration 84/1000 | Loss: 0.00003985
Iteration 85/1000 | Loss: 0.00003985
Iteration 86/1000 | Loss: 0.00003985
Iteration 87/1000 | Loss: 0.00003984
Iteration 88/1000 | Loss: 0.00003984
Iteration 89/1000 | Loss: 0.00003984
Iteration 90/1000 | Loss: 0.00003984
Iteration 91/1000 | Loss: 0.00003984
Iteration 92/1000 | Loss: 0.00003984
Iteration 93/1000 | Loss: 0.00003984
Iteration 94/1000 | Loss: 0.00003984
Iteration 95/1000 | Loss: 0.00003984
Iteration 96/1000 | Loss: 0.00003984
Iteration 97/1000 | Loss: 0.00003984
Iteration 98/1000 | Loss: 0.00003984
Iteration 99/1000 | Loss: 0.00003984
Iteration 100/1000 | Loss: 0.00003983
Iteration 101/1000 | Loss: 0.00003982
Iteration 102/1000 | Loss: 0.00003981
Iteration 103/1000 | Loss: 0.00003981
Iteration 104/1000 | Loss: 0.00003981
Iteration 105/1000 | Loss: 0.00003981
Iteration 106/1000 | Loss: 0.00003980
Iteration 107/1000 | Loss: 0.00003980
Iteration 108/1000 | Loss: 0.00003980
Iteration 109/1000 | Loss: 0.00003980
Iteration 110/1000 | Loss: 0.00003980
Iteration 111/1000 | Loss: 0.00003980
Iteration 112/1000 | Loss: 0.00003980
Iteration 113/1000 | Loss: 0.00003980
Iteration 114/1000 | Loss: 0.00003980
Iteration 115/1000 | Loss: 0.00003980
Iteration 116/1000 | Loss: 0.00003979
Iteration 117/1000 | Loss: 0.00003979
Iteration 118/1000 | Loss: 0.00003979
Iteration 119/1000 | Loss: 0.00003978
Iteration 120/1000 | Loss: 0.00003978
Iteration 121/1000 | Loss: 0.00003977
Iteration 122/1000 | Loss: 0.00003977
Iteration 123/1000 | Loss: 0.00003977
Iteration 124/1000 | Loss: 0.00003977
Iteration 125/1000 | Loss: 0.00003977
Iteration 126/1000 | Loss: 0.00003977
Iteration 127/1000 | Loss: 0.00003977
Iteration 128/1000 | Loss: 0.00003977
Iteration 129/1000 | Loss: 0.00003977
Iteration 130/1000 | Loss: 0.00003976
Iteration 131/1000 | Loss: 0.00003976
Iteration 132/1000 | Loss: 0.00003975
Iteration 133/1000 | Loss: 0.00003975
Iteration 134/1000 | Loss: 0.00003975
Iteration 135/1000 | Loss: 0.00003974
Iteration 136/1000 | Loss: 0.00003974
Iteration 137/1000 | Loss: 0.00003974
Iteration 138/1000 | Loss: 0.00003974
Iteration 139/1000 | Loss: 0.00003973
Iteration 140/1000 | Loss: 0.00003973
Iteration 141/1000 | Loss: 0.00003973
Iteration 142/1000 | Loss: 0.00003973
Iteration 143/1000 | Loss: 0.00003973
Iteration 144/1000 | Loss: 0.00003973
Iteration 145/1000 | Loss: 0.00003973
Iteration 146/1000 | Loss: 0.00003973
Iteration 147/1000 | Loss: 0.00003973
Iteration 148/1000 | Loss: 0.00003973
Iteration 149/1000 | Loss: 0.00003973
Iteration 150/1000 | Loss: 0.00003973
Iteration 151/1000 | Loss: 0.00003972
Iteration 152/1000 | Loss: 0.00003970
Iteration 153/1000 | Loss: 0.00003970
Iteration 154/1000 | Loss: 0.00003970
Iteration 155/1000 | Loss: 0.00003970
Iteration 156/1000 | Loss: 0.00003970
Iteration 157/1000 | Loss: 0.00003970
Iteration 158/1000 | Loss: 0.00003970
Iteration 159/1000 | Loss: 0.00003970
Iteration 160/1000 | Loss: 0.00003970
Iteration 161/1000 | Loss: 0.00003970
Iteration 162/1000 | Loss: 0.00003970
Iteration 163/1000 | Loss: 0.00003970
Iteration 164/1000 | Loss: 0.00003969
Iteration 165/1000 | Loss: 0.00003969
Iteration 166/1000 | Loss: 0.00003969
Iteration 167/1000 | Loss: 0.00003968
Iteration 168/1000 | Loss: 0.00003968
Iteration 169/1000 | Loss: 0.00003968
Iteration 170/1000 | Loss: 0.00003968
Iteration 171/1000 | Loss: 0.00003967
Iteration 172/1000 | Loss: 0.00003967
Iteration 173/1000 | Loss: 0.00003967
Iteration 174/1000 | Loss: 0.00003966
Iteration 175/1000 | Loss: 0.00003966
Iteration 176/1000 | Loss: 0.00003966
Iteration 177/1000 | Loss: 0.00003965
Iteration 178/1000 | Loss: 0.00003965
Iteration 179/1000 | Loss: 0.00003964
Iteration 180/1000 | Loss: 0.00003964
Iteration 181/1000 | Loss: 0.00003964
Iteration 182/1000 | Loss: 0.00003963
Iteration 183/1000 | Loss: 0.00003963
Iteration 184/1000 | Loss: 0.00003963
Iteration 185/1000 | Loss: 0.00003963
Iteration 186/1000 | Loss: 0.00003963
Iteration 187/1000 | Loss: 0.00003963
Iteration 188/1000 | Loss: 0.00003963
Iteration 189/1000 | Loss: 0.00003963
Iteration 190/1000 | Loss: 0.00003963
Iteration 191/1000 | Loss: 0.00003963
Iteration 192/1000 | Loss: 0.00003963
Iteration 193/1000 | Loss: 0.00003963
Iteration 194/1000 | Loss: 0.00003963
Iteration 195/1000 | Loss: 0.00003962
Iteration 196/1000 | Loss: 0.00003962
Iteration 197/1000 | Loss: 0.00003962
Iteration 198/1000 | Loss: 0.00003962
Iteration 199/1000 | Loss: 0.00003961
Iteration 200/1000 | Loss: 0.00003961
Iteration 201/1000 | Loss: 0.00003961
Iteration 202/1000 | Loss: 0.00003961
Iteration 203/1000 | Loss: 0.00003961
Iteration 204/1000 | Loss: 0.00003961
Iteration 205/1000 | Loss: 0.00003961
Iteration 206/1000 | Loss: 0.00003960
Iteration 207/1000 | Loss: 0.00003960
Iteration 208/1000 | Loss: 0.00003960
Iteration 209/1000 | Loss: 0.00003960
Iteration 210/1000 | Loss: 0.00003960
Iteration 211/1000 | Loss: 0.00003960
Iteration 212/1000 | Loss: 0.00003960
Iteration 213/1000 | Loss: 0.00003959
Iteration 214/1000 | Loss: 0.00003959
Iteration 215/1000 | Loss: 0.00003959
Iteration 216/1000 | Loss: 0.00003959
Iteration 217/1000 | Loss: 0.00003959
Iteration 218/1000 | Loss: 0.00003959
Iteration 219/1000 | Loss: 0.00003959
Iteration 220/1000 | Loss: 0.00003959
Iteration 221/1000 | Loss: 0.00003958
Iteration 222/1000 | Loss: 0.00003958
Iteration 223/1000 | Loss: 0.00003958
Iteration 224/1000 | Loss: 0.00003958
Iteration 225/1000 | Loss: 0.00003958
Iteration 226/1000 | Loss: 0.00003958
Iteration 227/1000 | Loss: 0.00003958
Iteration 228/1000 | Loss: 0.00003958
Iteration 229/1000 | Loss: 0.00003958
Iteration 230/1000 | Loss: 0.00003957
Iteration 231/1000 | Loss: 0.00003957
Iteration 232/1000 | Loss: 0.00003957
Iteration 233/1000 | Loss: 0.00003957
Iteration 234/1000 | Loss: 0.00003957
Iteration 235/1000 | Loss: 0.00003957
Iteration 236/1000 | Loss: 0.00003957
Iteration 237/1000 | Loss: 0.00003957
Iteration 238/1000 | Loss: 0.00003957
Iteration 239/1000 | Loss: 0.00003957
Iteration 240/1000 | Loss: 0.00003956
Iteration 241/1000 | Loss: 0.00003956
Iteration 242/1000 | Loss: 0.00003956
Iteration 243/1000 | Loss: 0.00003956
Iteration 244/1000 | Loss: 0.00003956
Iteration 245/1000 | Loss: 0.00003956
Iteration 246/1000 | Loss: 0.00003956
Iteration 247/1000 | Loss: 0.00003956
Iteration 248/1000 | Loss: 0.00003956
Iteration 249/1000 | Loss: 0.00003956
Iteration 250/1000 | Loss: 0.00003956
Iteration 251/1000 | Loss: 0.00003956
Iteration 252/1000 | Loss: 0.00003956
Iteration 253/1000 | Loss: 0.00003955
Iteration 254/1000 | Loss: 0.00003955
Iteration 255/1000 | Loss: 0.00003955
Iteration 256/1000 | Loss: 0.00003955
Iteration 257/1000 | Loss: 0.00003955
Iteration 258/1000 | Loss: 0.00003955
Iteration 259/1000 | Loss: 0.00003955
Iteration 260/1000 | Loss: 0.00003955
Iteration 261/1000 | Loss: 0.00003955
Iteration 262/1000 | Loss: 0.00003955
Iteration 263/1000 | Loss: 0.00003955
Iteration 264/1000 | Loss: 0.00003955
Iteration 265/1000 | Loss: 0.00003955
Iteration 266/1000 | Loss: 0.00003955
Iteration 267/1000 | Loss: 0.00003955
Iteration 268/1000 | Loss: 0.00003955
Iteration 269/1000 | Loss: 0.00003955
Iteration 270/1000 | Loss: 0.00003955
Iteration 271/1000 | Loss: 0.00003955
Iteration 272/1000 | Loss: 0.00003955
Iteration 273/1000 | Loss: 0.00003955
Iteration 274/1000 | Loss: 0.00003955
Iteration 275/1000 | Loss: 0.00003955
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 275. Stopping optimization.
Last 5 losses: [3.954508065362461e-05, 3.954508065362461e-05, 3.954508065362461e-05, 3.954508065362461e-05, 3.954508065362461e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.954508065362461e-05

Optimization complete. Final v2v error: 5.2466254234313965 mm

Highest mean error: 6.784085273742676 mm for frame 0

Lowest mean error: 4.719228744506836 mm for frame 13

Saving results

Total time: 51.37295722961426
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_us_1259/0014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_1259/0014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_1259/0014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00529479
Iteration 2/25 | Loss: 0.00156459
Iteration 3/25 | Loss: 0.00132219
Iteration 4/25 | Loss: 0.00128704
Iteration 5/25 | Loss: 0.00127686
Iteration 6/25 | Loss: 0.00127426
Iteration 7/25 | Loss: 0.00127371
Iteration 8/25 | Loss: 0.00127371
Iteration 9/25 | Loss: 0.00127371
Iteration 10/25 | Loss: 0.00127371
Iteration 11/25 | Loss: 0.00127371
Iteration 12/25 | Loss: 0.00127371
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.001273712026886642, 0.001273712026886642, 0.001273712026886642, 0.001273712026886642, 0.001273712026886642]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001273712026886642

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.91876686
Iteration 2/25 | Loss: 0.00265912
Iteration 3/25 | Loss: 0.00265911
Iteration 4/25 | Loss: 0.00265911
Iteration 5/25 | Loss: 0.00265911
Iteration 6/25 | Loss: 0.00265911
Iteration 7/25 | Loss: 0.00265911
Iteration 8/25 | Loss: 0.00265911
Iteration 9/25 | Loss: 0.00265911
Iteration 10/25 | Loss: 0.00265911
Iteration 11/25 | Loss: 0.00265911
Iteration 12/25 | Loss: 0.00265911
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.002659113146364689, 0.002659113146364689, 0.002659113146364689, 0.002659113146364689, 0.002659113146364689]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002659113146364689

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00265911
Iteration 2/1000 | Loss: 0.00005127
Iteration 3/1000 | Loss: 0.00003407
Iteration 4/1000 | Loss: 0.00002947
Iteration 5/1000 | Loss: 0.00002848
Iteration 6/1000 | Loss: 0.00002776
Iteration 7/1000 | Loss: 0.00002725
Iteration 8/1000 | Loss: 0.00002651
Iteration 9/1000 | Loss: 0.00002611
Iteration 10/1000 | Loss: 0.00002577
Iteration 11/1000 | Loss: 0.00002562
Iteration 12/1000 | Loss: 0.00002556
Iteration 13/1000 | Loss: 0.00002555
Iteration 14/1000 | Loss: 0.00002555
Iteration 15/1000 | Loss: 0.00002553
Iteration 16/1000 | Loss: 0.00002553
Iteration 17/1000 | Loss: 0.00002553
Iteration 18/1000 | Loss: 0.00002550
Iteration 19/1000 | Loss: 0.00002544
Iteration 20/1000 | Loss: 0.00002543
Iteration 21/1000 | Loss: 0.00002542
Iteration 22/1000 | Loss: 0.00002540
Iteration 23/1000 | Loss: 0.00002535
Iteration 24/1000 | Loss: 0.00002529
Iteration 25/1000 | Loss: 0.00002522
Iteration 26/1000 | Loss: 0.00002522
Iteration 27/1000 | Loss: 0.00002520
Iteration 28/1000 | Loss: 0.00002520
Iteration 29/1000 | Loss: 0.00002518
Iteration 30/1000 | Loss: 0.00002518
Iteration 31/1000 | Loss: 0.00002517
Iteration 32/1000 | Loss: 0.00002517
Iteration 33/1000 | Loss: 0.00002516
Iteration 34/1000 | Loss: 0.00002516
Iteration 35/1000 | Loss: 0.00002516
Iteration 36/1000 | Loss: 0.00002513
Iteration 37/1000 | Loss: 0.00002510
Iteration 38/1000 | Loss: 0.00002510
Iteration 39/1000 | Loss: 0.00002510
Iteration 40/1000 | Loss: 0.00002509
Iteration 41/1000 | Loss: 0.00002509
Iteration 42/1000 | Loss: 0.00002508
Iteration 43/1000 | Loss: 0.00002508
Iteration 44/1000 | Loss: 0.00002508
Iteration 45/1000 | Loss: 0.00002508
Iteration 46/1000 | Loss: 0.00002505
Iteration 47/1000 | Loss: 0.00002504
Iteration 48/1000 | Loss: 0.00002502
Iteration 49/1000 | Loss: 0.00002502
Iteration 50/1000 | Loss: 0.00002500
Iteration 51/1000 | Loss: 0.00002500
Iteration 52/1000 | Loss: 0.00002500
Iteration 53/1000 | Loss: 0.00002500
Iteration 54/1000 | Loss: 0.00002500
Iteration 55/1000 | Loss: 0.00002500
Iteration 56/1000 | Loss: 0.00002500
Iteration 57/1000 | Loss: 0.00002500
Iteration 58/1000 | Loss: 0.00002499
Iteration 59/1000 | Loss: 0.00002498
Iteration 60/1000 | Loss: 0.00002498
Iteration 61/1000 | Loss: 0.00002497
Iteration 62/1000 | Loss: 0.00002497
Iteration 63/1000 | Loss: 0.00002497
Iteration 64/1000 | Loss: 0.00002497
Iteration 65/1000 | Loss: 0.00002496
Iteration 66/1000 | Loss: 0.00002496
Iteration 67/1000 | Loss: 0.00002496
Iteration 68/1000 | Loss: 0.00002496
Iteration 69/1000 | Loss: 0.00002496
Iteration 70/1000 | Loss: 0.00002496
Iteration 71/1000 | Loss: 0.00002496
Iteration 72/1000 | Loss: 0.00002495
Iteration 73/1000 | Loss: 0.00002495
Iteration 74/1000 | Loss: 0.00002494
Iteration 75/1000 | Loss: 0.00002494
Iteration 76/1000 | Loss: 0.00002494
Iteration 77/1000 | Loss: 0.00002494
Iteration 78/1000 | Loss: 0.00002494
Iteration 79/1000 | Loss: 0.00002494
Iteration 80/1000 | Loss: 0.00002494
Iteration 81/1000 | Loss: 0.00002493
Iteration 82/1000 | Loss: 0.00002493
Iteration 83/1000 | Loss: 0.00002493
Iteration 84/1000 | Loss: 0.00002493
Iteration 85/1000 | Loss: 0.00002493
Iteration 86/1000 | Loss: 0.00002493
Iteration 87/1000 | Loss: 0.00002492
Iteration 88/1000 | Loss: 0.00002492
Iteration 89/1000 | Loss: 0.00002492
Iteration 90/1000 | Loss: 0.00002491
Iteration 91/1000 | Loss: 0.00002491
Iteration 92/1000 | Loss: 0.00002491
Iteration 93/1000 | Loss: 0.00002490
Iteration 94/1000 | Loss: 0.00002489
Iteration 95/1000 | Loss: 0.00002489
Iteration 96/1000 | Loss: 0.00002489
Iteration 97/1000 | Loss: 0.00002488
Iteration 98/1000 | Loss: 0.00002487
Iteration 99/1000 | Loss: 0.00002487
Iteration 100/1000 | Loss: 0.00002487
Iteration 101/1000 | Loss: 0.00002486
Iteration 102/1000 | Loss: 0.00002486
Iteration 103/1000 | Loss: 0.00002486
Iteration 104/1000 | Loss: 0.00002485
Iteration 105/1000 | Loss: 0.00002485
Iteration 106/1000 | Loss: 0.00002485
Iteration 107/1000 | Loss: 0.00002485
Iteration 108/1000 | Loss: 0.00002484
Iteration 109/1000 | Loss: 0.00002484
Iteration 110/1000 | Loss: 0.00002484
Iteration 111/1000 | Loss: 0.00002484
Iteration 112/1000 | Loss: 0.00002483
Iteration 113/1000 | Loss: 0.00002483
Iteration 114/1000 | Loss: 0.00002483
Iteration 115/1000 | Loss: 0.00002483
Iteration 116/1000 | Loss: 0.00002483
Iteration 117/1000 | Loss: 0.00002482
Iteration 118/1000 | Loss: 0.00002482
Iteration 119/1000 | Loss: 0.00002482
Iteration 120/1000 | Loss: 0.00002482
Iteration 121/1000 | Loss: 0.00002482
Iteration 122/1000 | Loss: 0.00002482
Iteration 123/1000 | Loss: 0.00002481
Iteration 124/1000 | Loss: 0.00002481
Iteration 125/1000 | Loss: 0.00002481
Iteration 126/1000 | Loss: 0.00002480
Iteration 127/1000 | Loss: 0.00002480
Iteration 128/1000 | Loss: 0.00002480
Iteration 129/1000 | Loss: 0.00002480
Iteration 130/1000 | Loss: 0.00002479
Iteration 131/1000 | Loss: 0.00002479
Iteration 132/1000 | Loss: 0.00002479
Iteration 133/1000 | Loss: 0.00002479
Iteration 134/1000 | Loss: 0.00002479
Iteration 135/1000 | Loss: 0.00002479
Iteration 136/1000 | Loss: 0.00002479
Iteration 137/1000 | Loss: 0.00002479
Iteration 138/1000 | Loss: 0.00002479
Iteration 139/1000 | Loss: 0.00002479
Iteration 140/1000 | Loss: 0.00002479
Iteration 141/1000 | Loss: 0.00002479
Iteration 142/1000 | Loss: 0.00002479
Iteration 143/1000 | Loss: 0.00002479
Iteration 144/1000 | Loss: 0.00002479
Iteration 145/1000 | Loss: 0.00002478
Iteration 146/1000 | Loss: 0.00002478
Iteration 147/1000 | Loss: 0.00002478
Iteration 148/1000 | Loss: 0.00002478
Iteration 149/1000 | Loss: 0.00002478
Iteration 150/1000 | Loss: 0.00002478
Iteration 151/1000 | Loss: 0.00002477
Iteration 152/1000 | Loss: 0.00002477
Iteration 153/1000 | Loss: 0.00002477
Iteration 154/1000 | Loss: 0.00002477
Iteration 155/1000 | Loss: 0.00002477
Iteration 156/1000 | Loss: 0.00002477
Iteration 157/1000 | Loss: 0.00002477
Iteration 158/1000 | Loss: 0.00002477
Iteration 159/1000 | Loss: 0.00002477
Iteration 160/1000 | Loss: 0.00002477
Iteration 161/1000 | Loss: 0.00002477
Iteration 162/1000 | Loss: 0.00002476
Iteration 163/1000 | Loss: 0.00002476
Iteration 164/1000 | Loss: 0.00002476
Iteration 165/1000 | Loss: 0.00002476
Iteration 166/1000 | Loss: 0.00002476
Iteration 167/1000 | Loss: 0.00002476
Iteration 168/1000 | Loss: 0.00002476
Iteration 169/1000 | Loss: 0.00002475
Iteration 170/1000 | Loss: 0.00002475
Iteration 171/1000 | Loss: 0.00002475
Iteration 172/1000 | Loss: 0.00002475
Iteration 173/1000 | Loss: 0.00002475
Iteration 174/1000 | Loss: 0.00002474
Iteration 175/1000 | Loss: 0.00002474
Iteration 176/1000 | Loss: 0.00002474
Iteration 177/1000 | Loss: 0.00002474
Iteration 178/1000 | Loss: 0.00002474
Iteration 179/1000 | Loss: 0.00002474
Iteration 180/1000 | Loss: 0.00002474
Iteration 181/1000 | Loss: 0.00002474
Iteration 182/1000 | Loss: 0.00002474
Iteration 183/1000 | Loss: 0.00002474
Iteration 184/1000 | Loss: 0.00002474
Iteration 185/1000 | Loss: 0.00002474
Iteration 186/1000 | Loss: 0.00002474
Iteration 187/1000 | Loss: 0.00002474
Iteration 188/1000 | Loss: 0.00002474
Iteration 189/1000 | Loss: 0.00002474
Iteration 190/1000 | Loss: 0.00002474
Iteration 191/1000 | Loss: 0.00002474
Iteration 192/1000 | Loss: 0.00002474
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 192. Stopping optimization.
Last 5 losses: [2.4740567823755555e-05, 2.4740567823755555e-05, 2.4740567823755555e-05, 2.4740567823755555e-05, 2.4740567823755555e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4740567823755555e-05

Optimization complete. Final v2v error: 4.208170413970947 mm

Highest mean error: 4.710123062133789 mm for frame 0

Lowest mean error: 4.015389442443848 mm for frame 173

Saving results

Total time: 50.44391703605652
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_us_1259/0015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_1259/0015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_1259/0015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00491999
Iteration 2/25 | Loss: 0.00146276
Iteration 3/25 | Loss: 0.00129701
Iteration 4/25 | Loss: 0.00128221
Iteration 5/25 | Loss: 0.00127831
Iteration 6/25 | Loss: 0.00127779
Iteration 7/25 | Loss: 0.00127779
Iteration 8/25 | Loss: 0.00127779
Iteration 9/25 | Loss: 0.00127779
Iteration 10/25 | Loss: 0.00127779
Iteration 11/25 | Loss: 0.00127779
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012777908705174923, 0.0012777908705174923, 0.0012777908705174923, 0.0012777908705174923, 0.0012777908705174923]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012777908705174923

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.01175952
Iteration 2/25 | Loss: 0.00256623
Iteration 3/25 | Loss: 0.00256623
Iteration 4/25 | Loss: 0.00256622
Iteration 5/25 | Loss: 0.00256622
Iteration 6/25 | Loss: 0.00256622
Iteration 7/25 | Loss: 0.00256622
Iteration 8/25 | Loss: 0.00256622
Iteration 9/25 | Loss: 0.00256622
Iteration 10/25 | Loss: 0.00256622
Iteration 11/25 | Loss: 0.00256622
Iteration 12/25 | Loss: 0.00256622
Iteration 13/25 | Loss: 0.00256622
Iteration 14/25 | Loss: 0.00256622
Iteration 15/25 | Loss: 0.00256622
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.002566223032772541, 0.002566223032772541, 0.002566223032772541, 0.002566223032772541, 0.002566223032772541]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002566223032772541

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00256622
Iteration 2/1000 | Loss: 0.00004889
Iteration 3/1000 | Loss: 0.00003222
Iteration 4/1000 | Loss: 0.00002896
Iteration 5/1000 | Loss: 0.00002778
Iteration 6/1000 | Loss: 0.00002684
Iteration 7/1000 | Loss: 0.00002604
Iteration 8/1000 | Loss: 0.00002547
Iteration 9/1000 | Loss: 0.00002505
Iteration 10/1000 | Loss: 0.00002474
Iteration 11/1000 | Loss: 0.00002448
Iteration 12/1000 | Loss: 0.00002431
Iteration 13/1000 | Loss: 0.00002421
Iteration 14/1000 | Loss: 0.00002420
Iteration 15/1000 | Loss: 0.00002420
Iteration 16/1000 | Loss: 0.00002407
Iteration 17/1000 | Loss: 0.00002406
Iteration 18/1000 | Loss: 0.00002405
Iteration 19/1000 | Loss: 0.00002405
Iteration 20/1000 | Loss: 0.00002404
Iteration 21/1000 | Loss: 0.00002404
Iteration 22/1000 | Loss: 0.00002404
Iteration 23/1000 | Loss: 0.00002404
Iteration 24/1000 | Loss: 0.00002404
Iteration 25/1000 | Loss: 0.00002404
Iteration 26/1000 | Loss: 0.00002404
Iteration 27/1000 | Loss: 0.00002404
Iteration 28/1000 | Loss: 0.00002404
Iteration 29/1000 | Loss: 0.00002403
Iteration 30/1000 | Loss: 0.00002402
Iteration 31/1000 | Loss: 0.00002402
Iteration 32/1000 | Loss: 0.00002402
Iteration 33/1000 | Loss: 0.00002401
Iteration 34/1000 | Loss: 0.00002401
Iteration 35/1000 | Loss: 0.00002400
Iteration 36/1000 | Loss: 0.00002397
Iteration 37/1000 | Loss: 0.00002393
Iteration 38/1000 | Loss: 0.00002390
Iteration 39/1000 | Loss: 0.00002390
Iteration 40/1000 | Loss: 0.00002387
Iteration 41/1000 | Loss: 0.00002386
Iteration 42/1000 | Loss: 0.00002386
Iteration 43/1000 | Loss: 0.00002386
Iteration 44/1000 | Loss: 0.00002385
Iteration 45/1000 | Loss: 0.00002385
Iteration 46/1000 | Loss: 0.00002384
Iteration 47/1000 | Loss: 0.00002384
Iteration 48/1000 | Loss: 0.00002383
Iteration 49/1000 | Loss: 0.00002383
Iteration 50/1000 | Loss: 0.00002382
Iteration 51/1000 | Loss: 0.00002381
Iteration 52/1000 | Loss: 0.00002375
Iteration 53/1000 | Loss: 0.00002374
Iteration 54/1000 | Loss: 0.00002373
Iteration 55/1000 | Loss: 0.00002373
Iteration 56/1000 | Loss: 0.00002373
Iteration 57/1000 | Loss: 0.00002373
Iteration 58/1000 | Loss: 0.00002372
Iteration 59/1000 | Loss: 0.00002372
Iteration 60/1000 | Loss: 0.00002372
Iteration 61/1000 | Loss: 0.00002372
Iteration 62/1000 | Loss: 0.00002372
Iteration 63/1000 | Loss: 0.00002372
Iteration 64/1000 | Loss: 0.00002372
Iteration 65/1000 | Loss: 0.00002372
Iteration 66/1000 | Loss: 0.00002371
Iteration 67/1000 | Loss: 0.00002371
Iteration 68/1000 | Loss: 0.00002371
Iteration 69/1000 | Loss: 0.00002371
Iteration 70/1000 | Loss: 0.00002371
Iteration 71/1000 | Loss: 0.00002371
Iteration 72/1000 | Loss: 0.00002371
Iteration 73/1000 | Loss: 0.00002371
Iteration 74/1000 | Loss: 0.00002371
Iteration 75/1000 | Loss: 0.00002371
Iteration 76/1000 | Loss: 0.00002370
Iteration 77/1000 | Loss: 0.00002370
Iteration 78/1000 | Loss: 0.00002370
Iteration 79/1000 | Loss: 0.00002370
Iteration 80/1000 | Loss: 0.00002370
Iteration 81/1000 | Loss: 0.00002370
Iteration 82/1000 | Loss: 0.00002370
Iteration 83/1000 | Loss: 0.00002370
Iteration 84/1000 | Loss: 0.00002369
Iteration 85/1000 | Loss: 0.00002369
Iteration 86/1000 | Loss: 0.00002368
Iteration 87/1000 | Loss: 0.00002368
Iteration 88/1000 | Loss: 0.00002368
Iteration 89/1000 | Loss: 0.00002368
Iteration 90/1000 | Loss: 0.00002368
Iteration 91/1000 | Loss: 0.00002368
Iteration 92/1000 | Loss: 0.00002368
Iteration 93/1000 | Loss: 0.00002367
Iteration 94/1000 | Loss: 0.00002367
Iteration 95/1000 | Loss: 0.00002367
Iteration 96/1000 | Loss: 0.00002367
Iteration 97/1000 | Loss: 0.00002367
Iteration 98/1000 | Loss: 0.00002366
Iteration 99/1000 | Loss: 0.00002366
Iteration 100/1000 | Loss: 0.00002366
Iteration 101/1000 | Loss: 0.00002365
Iteration 102/1000 | Loss: 0.00002365
Iteration 103/1000 | Loss: 0.00002365
Iteration 104/1000 | Loss: 0.00002364
Iteration 105/1000 | Loss: 0.00002364
Iteration 106/1000 | Loss: 0.00002364
Iteration 107/1000 | Loss: 0.00002363
Iteration 108/1000 | Loss: 0.00002363
Iteration 109/1000 | Loss: 0.00002363
Iteration 110/1000 | Loss: 0.00002363
Iteration 111/1000 | Loss: 0.00002362
Iteration 112/1000 | Loss: 0.00002362
Iteration 113/1000 | Loss: 0.00002362
Iteration 114/1000 | Loss: 0.00002362
Iteration 115/1000 | Loss: 0.00002362
Iteration 116/1000 | Loss: 0.00002362
Iteration 117/1000 | Loss: 0.00002361
Iteration 118/1000 | Loss: 0.00002361
Iteration 119/1000 | Loss: 0.00002361
Iteration 120/1000 | Loss: 0.00002361
Iteration 121/1000 | Loss: 0.00002361
Iteration 122/1000 | Loss: 0.00002361
Iteration 123/1000 | Loss: 0.00002361
Iteration 124/1000 | Loss: 0.00002361
Iteration 125/1000 | Loss: 0.00002361
Iteration 126/1000 | Loss: 0.00002360
Iteration 127/1000 | Loss: 0.00002360
Iteration 128/1000 | Loss: 0.00002360
Iteration 129/1000 | Loss: 0.00002360
Iteration 130/1000 | Loss: 0.00002360
Iteration 131/1000 | Loss: 0.00002360
Iteration 132/1000 | Loss: 0.00002360
Iteration 133/1000 | Loss: 0.00002360
Iteration 134/1000 | Loss: 0.00002360
Iteration 135/1000 | Loss: 0.00002360
Iteration 136/1000 | Loss: 0.00002359
Iteration 137/1000 | Loss: 0.00002359
Iteration 138/1000 | Loss: 0.00002359
Iteration 139/1000 | Loss: 0.00002359
Iteration 140/1000 | Loss: 0.00002359
Iteration 141/1000 | Loss: 0.00002358
Iteration 142/1000 | Loss: 0.00002358
Iteration 143/1000 | Loss: 0.00002358
Iteration 144/1000 | Loss: 0.00002357
Iteration 145/1000 | Loss: 0.00002357
Iteration 146/1000 | Loss: 0.00002357
Iteration 147/1000 | Loss: 0.00002357
Iteration 148/1000 | Loss: 0.00002357
Iteration 149/1000 | Loss: 0.00002357
Iteration 150/1000 | Loss: 0.00002357
Iteration 151/1000 | Loss: 0.00002357
Iteration 152/1000 | Loss: 0.00002356
Iteration 153/1000 | Loss: 0.00002356
Iteration 154/1000 | Loss: 0.00002356
Iteration 155/1000 | Loss: 0.00002356
Iteration 156/1000 | Loss: 0.00002356
Iteration 157/1000 | Loss: 0.00002356
Iteration 158/1000 | Loss: 0.00002355
Iteration 159/1000 | Loss: 0.00002355
Iteration 160/1000 | Loss: 0.00002355
Iteration 161/1000 | Loss: 0.00002355
Iteration 162/1000 | Loss: 0.00002355
Iteration 163/1000 | Loss: 0.00002355
Iteration 164/1000 | Loss: 0.00002355
Iteration 165/1000 | Loss: 0.00002355
Iteration 166/1000 | Loss: 0.00002354
Iteration 167/1000 | Loss: 0.00002354
Iteration 168/1000 | Loss: 0.00002354
Iteration 169/1000 | Loss: 0.00002353
Iteration 170/1000 | Loss: 0.00002353
Iteration 171/1000 | Loss: 0.00002352
Iteration 172/1000 | Loss: 0.00002352
Iteration 173/1000 | Loss: 0.00002352
Iteration 174/1000 | Loss: 0.00002352
Iteration 175/1000 | Loss: 0.00002351
Iteration 176/1000 | Loss: 0.00002351
Iteration 177/1000 | Loss: 0.00002351
Iteration 178/1000 | Loss: 0.00002350
Iteration 179/1000 | Loss: 0.00002350
Iteration 180/1000 | Loss: 0.00002350
Iteration 181/1000 | Loss: 0.00002350
Iteration 182/1000 | Loss: 0.00002350
Iteration 183/1000 | Loss: 0.00002349
Iteration 184/1000 | Loss: 0.00002349
Iteration 185/1000 | Loss: 0.00002349
Iteration 186/1000 | Loss: 0.00002349
Iteration 187/1000 | Loss: 0.00002349
Iteration 188/1000 | Loss: 0.00002349
Iteration 189/1000 | Loss: 0.00002349
Iteration 190/1000 | Loss: 0.00002349
Iteration 191/1000 | Loss: 0.00002348
Iteration 192/1000 | Loss: 0.00002348
Iteration 193/1000 | Loss: 0.00002348
Iteration 194/1000 | Loss: 0.00002348
Iteration 195/1000 | Loss: 0.00002348
Iteration 196/1000 | Loss: 0.00002347
Iteration 197/1000 | Loss: 0.00002347
Iteration 198/1000 | Loss: 0.00002347
Iteration 199/1000 | Loss: 0.00002347
Iteration 200/1000 | Loss: 0.00002347
Iteration 201/1000 | Loss: 0.00002347
Iteration 202/1000 | Loss: 0.00002347
Iteration 203/1000 | Loss: 0.00002347
Iteration 204/1000 | Loss: 0.00002347
Iteration 205/1000 | Loss: 0.00002347
Iteration 206/1000 | Loss: 0.00002347
Iteration 207/1000 | Loss: 0.00002347
Iteration 208/1000 | Loss: 0.00002347
Iteration 209/1000 | Loss: 0.00002347
Iteration 210/1000 | Loss: 0.00002347
Iteration 211/1000 | Loss: 0.00002346
Iteration 212/1000 | Loss: 0.00002346
Iteration 213/1000 | Loss: 0.00002346
Iteration 214/1000 | Loss: 0.00002346
Iteration 215/1000 | Loss: 0.00002346
Iteration 216/1000 | Loss: 0.00002346
Iteration 217/1000 | Loss: 0.00002346
Iteration 218/1000 | Loss: 0.00002346
Iteration 219/1000 | Loss: 0.00002346
Iteration 220/1000 | Loss: 0.00002345
Iteration 221/1000 | Loss: 0.00002345
Iteration 222/1000 | Loss: 0.00002345
Iteration 223/1000 | Loss: 0.00002345
Iteration 224/1000 | Loss: 0.00002345
Iteration 225/1000 | Loss: 0.00002345
Iteration 226/1000 | Loss: 0.00002345
Iteration 227/1000 | Loss: 0.00002345
Iteration 228/1000 | Loss: 0.00002345
Iteration 229/1000 | Loss: 0.00002345
Iteration 230/1000 | Loss: 0.00002345
Iteration 231/1000 | Loss: 0.00002345
Iteration 232/1000 | Loss: 0.00002345
Iteration 233/1000 | Loss: 0.00002345
Iteration 234/1000 | Loss: 0.00002345
Iteration 235/1000 | Loss: 0.00002344
Iteration 236/1000 | Loss: 0.00002344
Iteration 237/1000 | Loss: 0.00002344
Iteration 238/1000 | Loss: 0.00002344
Iteration 239/1000 | Loss: 0.00002344
Iteration 240/1000 | Loss: 0.00002344
Iteration 241/1000 | Loss: 0.00002344
Iteration 242/1000 | Loss: 0.00002344
Iteration 243/1000 | Loss: 0.00002344
Iteration 244/1000 | Loss: 0.00002344
Iteration 245/1000 | Loss: 0.00002344
Iteration 246/1000 | Loss: 0.00002344
Iteration 247/1000 | Loss: 0.00002344
Iteration 248/1000 | Loss: 0.00002344
Iteration 249/1000 | Loss: 0.00002344
Iteration 250/1000 | Loss: 0.00002344
Iteration 251/1000 | Loss: 0.00002344
Iteration 252/1000 | Loss: 0.00002344
Iteration 253/1000 | Loss: 0.00002343
Iteration 254/1000 | Loss: 0.00002343
Iteration 255/1000 | Loss: 0.00002343
Iteration 256/1000 | Loss: 0.00002343
Iteration 257/1000 | Loss: 0.00002343
Iteration 258/1000 | Loss: 0.00002343
Iteration 259/1000 | Loss: 0.00002343
Iteration 260/1000 | Loss: 0.00002343
Iteration 261/1000 | Loss: 0.00002343
Iteration 262/1000 | Loss: 0.00002342
Iteration 263/1000 | Loss: 0.00002342
Iteration 264/1000 | Loss: 0.00002342
Iteration 265/1000 | Loss: 0.00002342
Iteration 266/1000 | Loss: 0.00002342
Iteration 267/1000 | Loss: 0.00002342
Iteration 268/1000 | Loss: 0.00002342
Iteration 269/1000 | Loss: 0.00002342
Iteration 270/1000 | Loss: 0.00002342
Iteration 271/1000 | Loss: 0.00002342
Iteration 272/1000 | Loss: 0.00002342
Iteration 273/1000 | Loss: 0.00002342
Iteration 274/1000 | Loss: 0.00002342
Iteration 275/1000 | Loss: 0.00002342
Iteration 276/1000 | Loss: 0.00002342
Iteration 277/1000 | Loss: 0.00002342
Iteration 278/1000 | Loss: 0.00002342
Iteration 279/1000 | Loss: 0.00002342
Iteration 280/1000 | Loss: 0.00002342
Iteration 281/1000 | Loss: 0.00002342
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 281. Stopping optimization.
Last 5 losses: [2.3416858311975375e-05, 2.3416858311975375e-05, 2.3416858311975375e-05, 2.3416858311975375e-05, 2.3416858311975375e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3416858311975375e-05

Optimization complete. Final v2v error: 4.243493556976318 mm

Highest mean error: 4.408996105194092 mm for frame 112

Lowest mean error: 4.02549409866333 mm for frame 261

Saving results

Total time: 55.41490960121155
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_us_1259/0010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_1259/0010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_1259/0010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00580455
Iteration 2/25 | Loss: 0.00153644
Iteration 3/25 | Loss: 0.00139219
Iteration 4/25 | Loss: 0.00137395
Iteration 5/25 | Loss: 0.00136897
Iteration 6/25 | Loss: 0.00136894
Iteration 7/25 | Loss: 0.00136894
Iteration 8/25 | Loss: 0.00136894
Iteration 9/25 | Loss: 0.00136894
Iteration 10/25 | Loss: 0.00136894
Iteration 11/25 | Loss: 0.00136894
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013689380139112473, 0.0013689380139112473, 0.0013689380139112473, 0.0013689380139112473, 0.0013689380139112473]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013689380139112473

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.88926065
Iteration 2/25 | Loss: 0.00265083
Iteration 3/25 | Loss: 0.00265082
Iteration 4/25 | Loss: 0.00265082
Iteration 5/25 | Loss: 0.00265082
Iteration 6/25 | Loss: 0.00265082
Iteration 7/25 | Loss: 0.00265082
Iteration 8/25 | Loss: 0.00265081
Iteration 9/25 | Loss: 0.00265081
Iteration 10/25 | Loss: 0.00265081
Iteration 11/25 | Loss: 0.00265081
Iteration 12/25 | Loss: 0.00265081
Iteration 13/25 | Loss: 0.00265081
Iteration 14/25 | Loss: 0.00265081
Iteration 15/25 | Loss: 0.00265081
Iteration 16/25 | Loss: 0.00265081
Iteration 17/25 | Loss: 0.00265081
Iteration 18/25 | Loss: 0.00265081
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.002650814363732934, 0.002650814363732934, 0.002650814363732934, 0.002650814363732934, 0.002650814363732934]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002650814363732934

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00265081
Iteration 2/1000 | Loss: 0.00006304
Iteration 3/1000 | Loss: 0.00004621
Iteration 4/1000 | Loss: 0.00004390
Iteration 5/1000 | Loss: 0.00004270
Iteration 6/1000 | Loss: 0.00004152
Iteration 7/1000 | Loss: 0.00004092
Iteration 8/1000 | Loss: 0.00004059
Iteration 9/1000 | Loss: 0.00004035
Iteration 10/1000 | Loss: 0.00004029
Iteration 11/1000 | Loss: 0.00004009
Iteration 12/1000 | Loss: 0.00004007
Iteration 13/1000 | Loss: 0.00003990
Iteration 14/1000 | Loss: 0.00003971
Iteration 15/1000 | Loss: 0.00003963
Iteration 16/1000 | Loss: 0.00003957
Iteration 17/1000 | Loss: 0.00003957
Iteration 18/1000 | Loss: 0.00003956
Iteration 19/1000 | Loss: 0.00003956
Iteration 20/1000 | Loss: 0.00003955
Iteration 21/1000 | Loss: 0.00003950
Iteration 22/1000 | Loss: 0.00003949
Iteration 23/1000 | Loss: 0.00003945
Iteration 24/1000 | Loss: 0.00003945
Iteration 25/1000 | Loss: 0.00003945
Iteration 26/1000 | Loss: 0.00003944
Iteration 27/1000 | Loss: 0.00003944
Iteration 28/1000 | Loss: 0.00003944
Iteration 29/1000 | Loss: 0.00003944
Iteration 30/1000 | Loss: 0.00003944
Iteration 31/1000 | Loss: 0.00003944
Iteration 32/1000 | Loss: 0.00003944
Iteration 33/1000 | Loss: 0.00003944
Iteration 34/1000 | Loss: 0.00003944
Iteration 35/1000 | Loss: 0.00003944
Iteration 36/1000 | Loss: 0.00003944
Iteration 37/1000 | Loss: 0.00003944
Iteration 38/1000 | Loss: 0.00003944
Iteration 39/1000 | Loss: 0.00003944
Iteration 40/1000 | Loss: 0.00003944
Iteration 41/1000 | Loss: 0.00003944
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 41. Stopping optimization.
Last 5 losses: [3.9438433304894716e-05, 3.9438433304894716e-05, 3.9438433304894716e-05, 3.9438433304894716e-05, 3.9438433304894716e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.9438433304894716e-05

Optimization complete. Final v2v error: 5.309993743896484 mm

Highest mean error: 5.694054126739502 mm for frame 45

Lowest mean error: 5.077200412750244 mm for frame 59

Saving results

Total time: 33.1590678691864
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_008/1030/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_008/1030.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_008/1030
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00967308
Iteration 2/25 | Loss: 0.00967308
Iteration 3/25 | Loss: 0.00967308
Iteration 4/25 | Loss: 0.00967308
Iteration 5/25 | Loss: 0.00967307
Iteration 6/25 | Loss: 0.00967307
Iteration 7/25 | Loss: 0.00967307
Iteration 8/25 | Loss: 0.00967307
Iteration 9/25 | Loss: 0.00967307
Iteration 10/25 | Loss: 0.00967306
Iteration 11/25 | Loss: 0.00967306
Iteration 12/25 | Loss: 0.00967306
Iteration 13/25 | Loss: 0.00967306
Iteration 14/25 | Loss: 0.00967306
Iteration 15/25 | Loss: 0.00967306
Iteration 16/25 | Loss: 0.00967305
Iteration 17/25 | Loss: 0.00967305
Iteration 18/25 | Loss: 0.00967305
Iteration 19/25 | Loss: 0.00967305
Iteration 20/25 | Loss: 0.00967305
Iteration 21/25 | Loss: 0.00967304
Iteration 22/25 | Loss: 0.00967304
Iteration 23/25 | Loss: 0.00967304
Iteration 24/25 | Loss: 0.00967304
Iteration 25/25 | Loss: 0.00967303

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44802630
Iteration 2/25 | Loss: 0.18941598
Iteration 3/25 | Loss: 0.18803689
Iteration 4/25 | Loss: 0.18680216
Iteration 5/25 | Loss: 0.18676844
Iteration 6/25 | Loss: 0.18676844
Iteration 7/25 | Loss: 0.18676844
Iteration 8/25 | Loss: 0.18676844
Iteration 9/25 | Loss: 0.18676843
Iteration 10/25 | Loss: 0.18676844
Iteration 11/25 | Loss: 0.18676843
Iteration 12/25 | Loss: 0.18676843
Iteration 13/25 | Loss: 0.18676843
Iteration 14/25 | Loss: 0.18676843
Iteration 15/25 | Loss: 0.18676843
Iteration 16/25 | Loss: 0.18676844
Iteration 17/25 | Loss: 0.18676844
Iteration 18/25 | Loss: 0.18676844
Iteration 19/25 | Loss: 0.18676844
Iteration 20/25 | Loss: 0.18676844
Iteration 21/25 | Loss: 0.18676844
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.18676844239234924, 0.18676844239234924, 0.18676844239234924, 0.18676844239234924, 0.18676844239234924]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.18676844239234924

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.18676844
Iteration 2/1000 | Loss: 0.01093902
Iteration 3/1000 | Loss: 0.00323457
Iteration 4/1000 | Loss: 0.00088250
Iteration 5/1000 | Loss: 0.00032546
Iteration 6/1000 | Loss: 0.00015084
Iteration 7/1000 | Loss: 0.00008473
Iteration 8/1000 | Loss: 0.00005909
Iteration 9/1000 | Loss: 0.00004513
Iteration 10/1000 | Loss: 0.00003796
Iteration 11/1000 | Loss: 0.00003144
Iteration 12/1000 | Loss: 0.00002775
Iteration 13/1000 | Loss: 0.00002544
Iteration 14/1000 | Loss: 0.00002384
Iteration 15/1000 | Loss: 0.00002263
Iteration 16/1000 | Loss: 0.00002175
Iteration 17/1000 | Loss: 0.00002130
Iteration 18/1000 | Loss: 0.00002086
Iteration 19/1000 | Loss: 0.00002056
Iteration 20/1000 | Loss: 0.00002034
Iteration 21/1000 | Loss: 0.00002031
Iteration 22/1000 | Loss: 0.00002028
Iteration 23/1000 | Loss: 0.00002023
Iteration 24/1000 | Loss: 0.00002014
Iteration 25/1000 | Loss: 0.00002013
Iteration 26/1000 | Loss: 0.00002013
Iteration 27/1000 | Loss: 0.00002013
Iteration 28/1000 | Loss: 0.00002013
Iteration 29/1000 | Loss: 0.00002013
Iteration 30/1000 | Loss: 0.00002013
Iteration 31/1000 | Loss: 0.00002013
Iteration 32/1000 | Loss: 0.00002013
Iteration 33/1000 | Loss: 0.00002013
Iteration 34/1000 | Loss: 0.00002012
Iteration 35/1000 | Loss: 0.00002010
Iteration 36/1000 | Loss: 0.00002010
Iteration 37/1000 | Loss: 0.00002009
Iteration 38/1000 | Loss: 0.00002007
Iteration 39/1000 | Loss: 0.00002007
Iteration 40/1000 | Loss: 0.00002006
Iteration 41/1000 | Loss: 0.00002006
Iteration 42/1000 | Loss: 0.00002005
Iteration 43/1000 | Loss: 0.00002003
Iteration 44/1000 | Loss: 0.00002002
Iteration 45/1000 | Loss: 0.00002000
Iteration 46/1000 | Loss: 0.00001999
Iteration 47/1000 | Loss: 0.00001998
Iteration 48/1000 | Loss: 0.00001998
Iteration 49/1000 | Loss: 0.00001997
Iteration 50/1000 | Loss: 0.00001997
Iteration 51/1000 | Loss: 0.00001996
Iteration 52/1000 | Loss: 0.00001995
Iteration 53/1000 | Loss: 0.00001995
Iteration 54/1000 | Loss: 0.00001995
Iteration 55/1000 | Loss: 0.00001995
Iteration 56/1000 | Loss: 0.00001994
Iteration 57/1000 | Loss: 0.00001994
Iteration 58/1000 | Loss: 0.00001994
Iteration 59/1000 | Loss: 0.00001994
Iteration 60/1000 | Loss: 0.00001994
Iteration 61/1000 | Loss: 0.00001993
Iteration 62/1000 | Loss: 0.00001993
Iteration 63/1000 | Loss: 0.00001993
Iteration 64/1000 | Loss: 0.00001993
Iteration 65/1000 | Loss: 0.00001992
Iteration 66/1000 | Loss: 0.00001992
Iteration 67/1000 | Loss: 0.00001992
Iteration 68/1000 | Loss: 0.00001992
Iteration 69/1000 | Loss: 0.00001992
Iteration 70/1000 | Loss: 0.00001991
Iteration 71/1000 | Loss: 0.00001990
Iteration 72/1000 | Loss: 0.00001989
Iteration 73/1000 | Loss: 0.00001988
Iteration 74/1000 | Loss: 0.00001988
Iteration 75/1000 | Loss: 0.00001988
Iteration 76/1000 | Loss: 0.00001988
Iteration 77/1000 | Loss: 0.00001988
Iteration 78/1000 | Loss: 0.00001987
Iteration 79/1000 | Loss: 0.00001987
Iteration 80/1000 | Loss: 0.00001987
Iteration 81/1000 | Loss: 0.00001987
Iteration 82/1000 | Loss: 0.00001987
Iteration 83/1000 | Loss: 0.00001986
Iteration 84/1000 | Loss: 0.00001986
Iteration 85/1000 | Loss: 0.00001986
Iteration 86/1000 | Loss: 0.00001986
Iteration 87/1000 | Loss: 0.00001986
Iteration 88/1000 | Loss: 0.00001986
Iteration 89/1000 | Loss: 0.00001986
Iteration 90/1000 | Loss: 0.00001985
Iteration 91/1000 | Loss: 0.00001985
Iteration 92/1000 | Loss: 0.00001985
Iteration 93/1000 | Loss: 0.00001985
Iteration 94/1000 | Loss: 0.00001985
Iteration 95/1000 | Loss: 0.00001984
Iteration 96/1000 | Loss: 0.00001984
Iteration 97/1000 | Loss: 0.00001983
Iteration 98/1000 | Loss: 0.00001983
Iteration 99/1000 | Loss: 0.00001982
Iteration 100/1000 | Loss: 0.00001982
Iteration 101/1000 | Loss: 0.00001982
Iteration 102/1000 | Loss: 0.00001982
Iteration 103/1000 | Loss: 0.00001982
Iteration 104/1000 | Loss: 0.00001982
Iteration 105/1000 | Loss: 0.00001982
Iteration 106/1000 | Loss: 0.00001982
Iteration 107/1000 | Loss: 0.00001982
Iteration 108/1000 | Loss: 0.00001982
Iteration 109/1000 | Loss: 0.00001982
Iteration 110/1000 | Loss: 0.00001981
Iteration 111/1000 | Loss: 0.00001981
Iteration 112/1000 | Loss: 0.00001981
Iteration 113/1000 | Loss: 0.00001981
Iteration 114/1000 | Loss: 0.00001981
Iteration 115/1000 | Loss: 0.00001981
Iteration 116/1000 | Loss: 0.00001981
Iteration 117/1000 | Loss: 0.00001981
Iteration 118/1000 | Loss: 0.00001981
Iteration 119/1000 | Loss: 0.00001981
Iteration 120/1000 | Loss: 0.00001981
Iteration 121/1000 | Loss: 0.00001980
Iteration 122/1000 | Loss: 0.00001980
Iteration 123/1000 | Loss: 0.00001980
Iteration 124/1000 | Loss: 0.00001980
Iteration 125/1000 | Loss: 0.00001980
Iteration 126/1000 | Loss: 0.00001980
Iteration 127/1000 | Loss: 0.00001980
Iteration 128/1000 | Loss: 0.00001980
Iteration 129/1000 | Loss: 0.00001979
Iteration 130/1000 | Loss: 0.00001979
Iteration 131/1000 | Loss: 0.00001979
Iteration 132/1000 | Loss: 0.00001979
Iteration 133/1000 | Loss: 0.00001978
Iteration 134/1000 | Loss: 0.00001978
Iteration 135/1000 | Loss: 0.00001978
Iteration 136/1000 | Loss: 0.00001978
Iteration 137/1000 | Loss: 0.00001978
Iteration 138/1000 | Loss: 0.00001978
Iteration 139/1000 | Loss: 0.00001978
Iteration 140/1000 | Loss: 0.00001978
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 140. Stopping optimization.
Last 5 losses: [1.9780696675297804e-05, 1.9780696675297804e-05, 1.9780696675297804e-05, 1.9780696675297804e-05, 1.9780696675297804e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9780696675297804e-05

Optimization complete. Final v2v error: 3.752549886703491 mm

Highest mean error: 4.0729756355285645 mm for frame 11

Lowest mean error: 3.548011302947998 mm for frame 59

Saving results

Total time: 56.79842758178711
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_008/1078/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_008/1078.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_008/1078
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01013641
Iteration 2/25 | Loss: 0.00276663
Iteration 3/25 | Loss: 0.00219081
Iteration 4/25 | Loss: 0.00194460
Iteration 5/25 | Loss: 0.00244790
Iteration 6/25 | Loss: 0.00206849
Iteration 7/25 | Loss: 0.00182676
Iteration 8/25 | Loss: 0.00165517
Iteration 9/25 | Loss: 0.00161911
Iteration 10/25 | Loss: 0.00153926
Iteration 11/25 | Loss: 0.00152909
Iteration 12/25 | Loss: 0.00151150
Iteration 13/25 | Loss: 0.00150870
Iteration 14/25 | Loss: 0.00149866
Iteration 15/25 | Loss: 0.00148946
Iteration 16/25 | Loss: 0.00148000
Iteration 17/25 | Loss: 0.00148481
Iteration 18/25 | Loss: 0.00146825
Iteration 19/25 | Loss: 0.00145446
Iteration 20/25 | Loss: 0.00144297
Iteration 21/25 | Loss: 0.00142605
Iteration 22/25 | Loss: 0.00143694
Iteration 23/25 | Loss: 0.00143405
Iteration 24/25 | Loss: 0.00153719
Iteration 25/25 | Loss: 0.00158481

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.26375532
Iteration 2/25 | Loss: 0.00826785
Iteration 3/25 | Loss: 0.00365715
Iteration 4/25 | Loss: 0.00365663
Iteration 5/25 | Loss: 0.00365663
Iteration 6/25 | Loss: 0.00365663
Iteration 7/25 | Loss: 0.00365663
Iteration 8/25 | Loss: 0.00365663
Iteration 9/25 | Loss: 0.00365663
Iteration 10/25 | Loss: 0.00365663
Iteration 11/25 | Loss: 0.00365663
Iteration 12/25 | Loss: 0.00365663
Iteration 13/25 | Loss: 0.00365663
Iteration 14/25 | Loss: 0.00365663
Iteration 15/25 | Loss: 0.00365663
Iteration 16/25 | Loss: 0.00365663
Iteration 17/25 | Loss: 0.00365663
Iteration 18/25 | Loss: 0.00365663
Iteration 19/25 | Loss: 0.00365663
Iteration 20/25 | Loss: 0.00365663
Iteration 21/25 | Loss: 0.00365663
Iteration 22/25 | Loss: 0.00365663
Iteration 23/25 | Loss: 0.00365663
Iteration 24/25 | Loss: 0.00365663
Iteration 25/25 | Loss: 0.00365663
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0036566255148500204, 0.0036566255148500204, 0.0036566255148500204, 0.0036566255148500204, 0.0036566255148500204]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0036566255148500204

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00365663
Iteration 2/1000 | Loss: 0.00524578
Iteration 3/1000 | Loss: 0.00153708
Iteration 4/1000 | Loss: 0.00025138
Iteration 5/1000 | Loss: 0.00067544
Iteration 6/1000 | Loss: 0.00217480
Iteration 7/1000 | Loss: 0.00205701
Iteration 8/1000 | Loss: 0.00025128
Iteration 9/1000 | Loss: 0.00105178
Iteration 10/1000 | Loss: 0.00096837
Iteration 11/1000 | Loss: 0.00076978
Iteration 12/1000 | Loss: 0.00062702
Iteration 13/1000 | Loss: 0.00072592
Iteration 14/1000 | Loss: 0.00062648
Iteration 15/1000 | Loss: 0.00048426
Iteration 16/1000 | Loss: 0.00039378
Iteration 17/1000 | Loss: 0.00040886
Iteration 18/1000 | Loss: 0.00103256
Iteration 19/1000 | Loss: 0.00023322
Iteration 20/1000 | Loss: 0.00023645
Iteration 21/1000 | Loss: 0.00014667
Iteration 22/1000 | Loss: 0.00018556
Iteration 23/1000 | Loss: 0.00014645
Iteration 24/1000 | Loss: 0.00010634
Iteration 25/1000 | Loss: 0.00010989
Iteration 26/1000 | Loss: 0.00027161
Iteration 27/1000 | Loss: 0.00009651
Iteration 28/1000 | Loss: 0.00011402
Iteration 29/1000 | Loss: 0.00009159
Iteration 30/1000 | Loss: 0.00006155
Iteration 31/1000 | Loss: 0.00014659
Iteration 32/1000 | Loss: 0.00005951
Iteration 33/1000 | Loss: 0.00005292
Iteration 34/1000 | Loss: 0.00010827
Iteration 35/1000 | Loss: 0.00021143
Iteration 36/1000 | Loss: 0.00007615
Iteration 37/1000 | Loss: 0.00022645
Iteration 38/1000 | Loss: 0.00006366
Iteration 39/1000 | Loss: 0.00005413
Iteration 40/1000 | Loss: 0.00005244
Iteration 41/1000 | Loss: 0.00004592
Iteration 42/1000 | Loss: 0.00012012
Iteration 43/1000 | Loss: 0.00005190
Iteration 44/1000 | Loss: 0.00005432
Iteration 45/1000 | Loss: 0.00008454
Iteration 46/1000 | Loss: 0.00004626
Iteration 47/1000 | Loss: 0.00010680
Iteration 48/1000 | Loss: 0.00004711
Iteration 49/1000 | Loss: 0.00044941
Iteration 50/1000 | Loss: 0.00037089
Iteration 51/1000 | Loss: 0.00004329
Iteration 52/1000 | Loss: 0.00003939
Iteration 53/1000 | Loss: 0.00004226
Iteration 54/1000 | Loss: 0.00003812
Iteration 55/1000 | Loss: 0.00003726
Iteration 56/1000 | Loss: 0.00042619
Iteration 57/1000 | Loss: 0.00097353
Iteration 58/1000 | Loss: 0.00066669
Iteration 59/1000 | Loss: 0.00028056
Iteration 60/1000 | Loss: 0.00012899
Iteration 61/1000 | Loss: 0.00004383
Iteration 62/1000 | Loss: 0.00004239
Iteration 63/1000 | Loss: 0.00039312
Iteration 64/1000 | Loss: 0.00074297
Iteration 65/1000 | Loss: 0.00015296
Iteration 66/1000 | Loss: 0.00005695
Iteration 67/1000 | Loss: 0.00004820
Iteration 68/1000 | Loss: 0.00004193
Iteration 69/1000 | Loss: 0.00003980
Iteration 70/1000 | Loss: 0.00005076
Iteration 71/1000 | Loss: 0.00005420
Iteration 72/1000 | Loss: 0.00004110
Iteration 73/1000 | Loss: 0.00003825
Iteration 74/1000 | Loss: 0.00003747
Iteration 75/1000 | Loss: 0.00003678
Iteration 76/1000 | Loss: 0.00003628
Iteration 77/1000 | Loss: 0.00003576
Iteration 78/1000 | Loss: 0.00006299
Iteration 79/1000 | Loss: 0.00003535
Iteration 80/1000 | Loss: 0.00003509
Iteration 81/1000 | Loss: 0.00003504
Iteration 82/1000 | Loss: 0.00003467
Iteration 83/1000 | Loss: 0.00009064
Iteration 84/1000 | Loss: 0.00003461
Iteration 85/1000 | Loss: 0.00003403
Iteration 86/1000 | Loss: 0.00003395
Iteration 87/1000 | Loss: 0.00003390
Iteration 88/1000 | Loss: 0.00003379
Iteration 89/1000 | Loss: 0.00003379
Iteration 90/1000 | Loss: 0.00003379
Iteration 91/1000 | Loss: 0.00003379
Iteration 92/1000 | Loss: 0.00003378
Iteration 93/1000 | Loss: 0.00003375
Iteration 94/1000 | Loss: 0.00003374
Iteration 95/1000 | Loss: 0.00003373
Iteration 96/1000 | Loss: 0.00003368
Iteration 97/1000 | Loss: 0.00003368
Iteration 98/1000 | Loss: 0.00003366
Iteration 99/1000 | Loss: 0.00003365
Iteration 100/1000 | Loss: 0.00003365
Iteration 101/1000 | Loss: 0.00003364
Iteration 102/1000 | Loss: 0.00003364
Iteration 103/1000 | Loss: 0.00003363
Iteration 104/1000 | Loss: 0.00003363
Iteration 105/1000 | Loss: 0.00003362
Iteration 106/1000 | Loss: 0.00003361
Iteration 107/1000 | Loss: 0.00003361
Iteration 108/1000 | Loss: 0.00003360
Iteration 109/1000 | Loss: 0.00003357
Iteration 110/1000 | Loss: 0.00003353
Iteration 111/1000 | Loss: 0.00003353
Iteration 112/1000 | Loss: 0.00003353
Iteration 113/1000 | Loss: 0.00003353
Iteration 114/1000 | Loss: 0.00003353
Iteration 115/1000 | Loss: 0.00003351
Iteration 116/1000 | Loss: 0.00003349
Iteration 117/1000 | Loss: 0.00003349
Iteration 118/1000 | Loss: 0.00003349
Iteration 119/1000 | Loss: 0.00003349
Iteration 120/1000 | Loss: 0.00003348
Iteration 121/1000 | Loss: 0.00003347
Iteration 122/1000 | Loss: 0.00003346
Iteration 123/1000 | Loss: 0.00003344
Iteration 124/1000 | Loss: 0.00003343
Iteration 125/1000 | Loss: 0.00003343
Iteration 126/1000 | Loss: 0.00003342
Iteration 127/1000 | Loss: 0.00003341
Iteration 128/1000 | Loss: 0.00003341
Iteration 129/1000 | Loss: 0.00003340
Iteration 130/1000 | Loss: 0.00007114
Iteration 131/1000 | Loss: 0.00003437
Iteration 132/1000 | Loss: 0.00003339
Iteration 133/1000 | Loss: 0.00003337
Iteration 134/1000 | Loss: 0.00003337
Iteration 135/1000 | Loss: 0.00003337
Iteration 136/1000 | Loss: 0.00003337
Iteration 137/1000 | Loss: 0.00003336
Iteration 138/1000 | Loss: 0.00003336
Iteration 139/1000 | Loss: 0.00003336
Iteration 140/1000 | Loss: 0.00003336
Iteration 141/1000 | Loss: 0.00003336
Iteration 142/1000 | Loss: 0.00003336
Iteration 143/1000 | Loss: 0.00003336
Iteration 144/1000 | Loss: 0.00003336
Iteration 145/1000 | Loss: 0.00003335
Iteration 146/1000 | Loss: 0.00003335
Iteration 147/1000 | Loss: 0.00003335
Iteration 148/1000 | Loss: 0.00003335
Iteration 149/1000 | Loss: 0.00003335
Iteration 150/1000 | Loss: 0.00003335
Iteration 151/1000 | Loss: 0.00003335
Iteration 152/1000 | Loss: 0.00003334
Iteration 153/1000 | Loss: 0.00003334
Iteration 154/1000 | Loss: 0.00003334
Iteration 155/1000 | Loss: 0.00003334
Iteration 156/1000 | Loss: 0.00003334
Iteration 157/1000 | Loss: 0.00003334
Iteration 158/1000 | Loss: 0.00003334
Iteration 159/1000 | Loss: 0.00003334
Iteration 160/1000 | Loss: 0.00003334
Iteration 161/1000 | Loss: 0.00003334
Iteration 162/1000 | Loss: 0.00003334
Iteration 163/1000 | Loss: 0.00003334
Iteration 164/1000 | Loss: 0.00003334
Iteration 165/1000 | Loss: 0.00003334
Iteration 166/1000 | Loss: 0.00003334
Iteration 167/1000 | Loss: 0.00003334
Iteration 168/1000 | Loss: 0.00003334
Iteration 169/1000 | Loss: 0.00003334
Iteration 170/1000 | Loss: 0.00003334
Iteration 171/1000 | Loss: 0.00003334
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 171. Stopping optimization.
Last 5 losses: [3.3339103538310155e-05, 3.3339103538310155e-05, 3.3339103538310155e-05, 3.3339103538310155e-05, 3.3339103538310155e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.3339103538310155e-05

Optimization complete. Final v2v error: 4.006489276885986 mm

Highest mean error: 5.621886730194092 mm for frame 62

Lowest mean error: 3.4429922103881836 mm for frame 0

Saving results

Total time: 171.53636837005615
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_008/1013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_008/1013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_008/1013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00820063
Iteration 2/25 | Loss: 0.00215881
Iteration 3/25 | Loss: 0.00152811
Iteration 4/25 | Loss: 0.00144936
Iteration 5/25 | Loss: 0.00144384
Iteration 6/25 | Loss: 0.00144384
Iteration 7/25 | Loss: 0.00144384
Iteration 8/25 | Loss: 0.00144384
Iteration 9/25 | Loss: 0.00144384
Iteration 10/25 | Loss: 0.00144384
Iteration 11/25 | Loss: 0.00144384
Iteration 12/25 | Loss: 0.00144384
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0014438423095270991, 0.0014438423095270991, 0.0014438423095270991, 0.0014438423095270991, 0.0014438423095270991]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014438423095270991

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.30690777
Iteration 2/25 | Loss: 0.00113872
Iteration 3/25 | Loss: 0.00113869
Iteration 4/25 | Loss: 0.00113869
Iteration 5/25 | Loss: 0.00113869
Iteration 6/25 | Loss: 0.00113869
Iteration 7/25 | Loss: 0.00113869
Iteration 8/25 | Loss: 0.00113869
Iteration 9/25 | Loss: 0.00113869
Iteration 10/25 | Loss: 0.00113869
Iteration 11/25 | Loss: 0.00113869
Iteration 12/25 | Loss: 0.00113869
Iteration 13/25 | Loss: 0.00113869
Iteration 14/25 | Loss: 0.00113869
Iteration 15/25 | Loss: 0.00113869
Iteration 16/25 | Loss: 0.00113869
Iteration 17/25 | Loss: 0.00113869
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0011386880651116371, 0.0011386880651116371, 0.0011386880651116371, 0.0011386880651116371, 0.0011386880651116371]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011386880651116371

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00113869
Iteration 2/1000 | Loss: 0.00006130
Iteration 3/1000 | Loss: 0.00003735
Iteration 4/1000 | Loss: 0.00003417
Iteration 5/1000 | Loss: 0.00003284
Iteration 6/1000 | Loss: 0.00003189
Iteration 7/1000 | Loss: 0.00003155
Iteration 8/1000 | Loss: 0.00003112
Iteration 9/1000 | Loss: 0.00003097
Iteration 10/1000 | Loss: 0.00003082
Iteration 11/1000 | Loss: 0.00003079
Iteration 12/1000 | Loss: 0.00003078
Iteration 13/1000 | Loss: 0.00003077
Iteration 14/1000 | Loss: 0.00003075
Iteration 15/1000 | Loss: 0.00003071
Iteration 16/1000 | Loss: 0.00003068
Iteration 17/1000 | Loss: 0.00003065
Iteration 18/1000 | Loss: 0.00003065
Iteration 19/1000 | Loss: 0.00003061
Iteration 20/1000 | Loss: 0.00003061
Iteration 21/1000 | Loss: 0.00003061
Iteration 22/1000 | Loss: 0.00003061
Iteration 23/1000 | Loss: 0.00003061
Iteration 24/1000 | Loss: 0.00003061
Iteration 25/1000 | Loss: 0.00003061
Iteration 26/1000 | Loss: 0.00003061
Iteration 27/1000 | Loss: 0.00003061
Iteration 28/1000 | Loss: 0.00003061
Iteration 29/1000 | Loss: 0.00003052
Iteration 30/1000 | Loss: 0.00003052
Iteration 31/1000 | Loss: 0.00003050
Iteration 32/1000 | Loss: 0.00003049
Iteration 33/1000 | Loss: 0.00003048
Iteration 34/1000 | Loss: 0.00003046
Iteration 35/1000 | Loss: 0.00003046
Iteration 36/1000 | Loss: 0.00003046
Iteration 37/1000 | Loss: 0.00003046
Iteration 38/1000 | Loss: 0.00003045
Iteration 39/1000 | Loss: 0.00003045
Iteration 40/1000 | Loss: 0.00003045
Iteration 41/1000 | Loss: 0.00003045
Iteration 42/1000 | Loss: 0.00003044
Iteration 43/1000 | Loss: 0.00003044
Iteration 44/1000 | Loss: 0.00003044
Iteration 45/1000 | Loss: 0.00003043
Iteration 46/1000 | Loss: 0.00003042
Iteration 47/1000 | Loss: 0.00003042
Iteration 48/1000 | Loss: 0.00003042
Iteration 49/1000 | Loss: 0.00003042
Iteration 50/1000 | Loss: 0.00003042
Iteration 51/1000 | Loss: 0.00003042
Iteration 52/1000 | Loss: 0.00003042
Iteration 53/1000 | Loss: 0.00003041
Iteration 54/1000 | Loss: 0.00003041
Iteration 55/1000 | Loss: 0.00003041
Iteration 56/1000 | Loss: 0.00003041
Iteration 57/1000 | Loss: 0.00003041
Iteration 58/1000 | Loss: 0.00003041
Iteration 59/1000 | Loss: 0.00003041
Iteration 60/1000 | Loss: 0.00003041
Iteration 61/1000 | Loss: 0.00003040
Iteration 62/1000 | Loss: 0.00003040
Iteration 63/1000 | Loss: 0.00003040
Iteration 64/1000 | Loss: 0.00003039
Iteration 65/1000 | Loss: 0.00003039
Iteration 66/1000 | Loss: 0.00003039
Iteration 67/1000 | Loss: 0.00003039
Iteration 68/1000 | Loss: 0.00003039
Iteration 69/1000 | Loss: 0.00003039
Iteration 70/1000 | Loss: 0.00003039
Iteration 71/1000 | Loss: 0.00003039
Iteration 72/1000 | Loss: 0.00003039
Iteration 73/1000 | Loss: 0.00003039
Iteration 74/1000 | Loss: 0.00003039
Iteration 75/1000 | Loss: 0.00003039
Iteration 76/1000 | Loss: 0.00003038
Iteration 77/1000 | Loss: 0.00003038
Iteration 78/1000 | Loss: 0.00003038
Iteration 79/1000 | Loss: 0.00003038
Iteration 80/1000 | Loss: 0.00003038
Iteration 81/1000 | Loss: 0.00003038
Iteration 82/1000 | Loss: 0.00003038
Iteration 83/1000 | Loss: 0.00003038
Iteration 84/1000 | Loss: 0.00003038
Iteration 85/1000 | Loss: 0.00003037
Iteration 86/1000 | Loss: 0.00003037
Iteration 87/1000 | Loss: 0.00003037
Iteration 88/1000 | Loss: 0.00003037
Iteration 89/1000 | Loss: 0.00003037
Iteration 90/1000 | Loss: 0.00003037
Iteration 91/1000 | Loss: 0.00003037
Iteration 92/1000 | Loss: 0.00003037
Iteration 93/1000 | Loss: 0.00003037
Iteration 94/1000 | Loss: 0.00003037
Iteration 95/1000 | Loss: 0.00003037
Iteration 96/1000 | Loss: 0.00003037
Iteration 97/1000 | Loss: 0.00003037
Iteration 98/1000 | Loss: 0.00003037
Iteration 99/1000 | Loss: 0.00003037
Iteration 100/1000 | Loss: 0.00003037
Iteration 101/1000 | Loss: 0.00003037
Iteration 102/1000 | Loss: 0.00003037
Iteration 103/1000 | Loss: 0.00003037
Iteration 104/1000 | Loss: 0.00003037
Iteration 105/1000 | Loss: 0.00003037
Iteration 106/1000 | Loss: 0.00003037
Iteration 107/1000 | Loss: 0.00003037
Iteration 108/1000 | Loss: 0.00003037
Iteration 109/1000 | Loss: 0.00003037
Iteration 110/1000 | Loss: 0.00003037
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 110. Stopping optimization.
Last 5 losses: [3.03652705042623e-05, 3.03652705042623e-05, 3.03652705042623e-05, 3.03652705042623e-05, 3.03652705042623e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.03652705042623e-05

Optimization complete. Final v2v error: 4.714062213897705 mm

Highest mean error: 4.985762596130371 mm for frame 87

Lowest mean error: 4.562136173248291 mm for frame 39

Saving results

Total time: 34.882341384887695
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_008/1020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_008/1020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_008/1020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00586641
Iteration 2/25 | Loss: 0.00126889
Iteration 3/25 | Loss: 0.00115840
Iteration 4/25 | Loss: 0.00113354
Iteration 5/25 | Loss: 0.00112807
Iteration 6/25 | Loss: 0.00112660
Iteration 7/25 | Loss: 0.00112660
Iteration 8/25 | Loss: 0.00112660
Iteration 9/25 | Loss: 0.00112660
Iteration 10/25 | Loss: 0.00112660
Iteration 11/25 | Loss: 0.00112660
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011266011279076338, 0.0011266011279076338, 0.0011266011279076338, 0.0011266011279076338, 0.0011266011279076338]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011266011279076338

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.51716936
Iteration 2/25 | Loss: 0.00127710
Iteration 3/25 | Loss: 0.00127710
Iteration 4/25 | Loss: 0.00127710
Iteration 5/25 | Loss: 0.00127710
Iteration 6/25 | Loss: 0.00127710
Iteration 7/25 | Loss: 0.00127710
Iteration 8/25 | Loss: 0.00127710
Iteration 9/25 | Loss: 0.00127710
Iteration 10/25 | Loss: 0.00127710
Iteration 11/25 | Loss: 0.00127710
Iteration 12/25 | Loss: 0.00127710
Iteration 13/25 | Loss: 0.00127710
Iteration 14/25 | Loss: 0.00127710
Iteration 15/25 | Loss: 0.00127710
Iteration 16/25 | Loss: 0.00127710
Iteration 17/25 | Loss: 0.00127710
Iteration 18/25 | Loss: 0.00127710
Iteration 19/25 | Loss: 0.00127710
Iteration 20/25 | Loss: 0.00127710
Iteration 21/25 | Loss: 0.00127710
Iteration 22/25 | Loss: 0.00127710
Iteration 23/25 | Loss: 0.00127710
Iteration 24/25 | Loss: 0.00127710
Iteration 25/25 | Loss: 0.00127710

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00127710
Iteration 2/1000 | Loss: 0.00004920
Iteration 3/1000 | Loss: 0.00002892
Iteration 4/1000 | Loss: 0.00002385
Iteration 5/1000 | Loss: 0.00002182
Iteration 6/1000 | Loss: 0.00002047
Iteration 7/1000 | Loss: 0.00001967
Iteration 8/1000 | Loss: 0.00001914
Iteration 9/1000 | Loss: 0.00001856
Iteration 10/1000 | Loss: 0.00001824
Iteration 11/1000 | Loss: 0.00001800
Iteration 12/1000 | Loss: 0.00001788
Iteration 13/1000 | Loss: 0.00001780
Iteration 14/1000 | Loss: 0.00001780
Iteration 15/1000 | Loss: 0.00001780
Iteration 16/1000 | Loss: 0.00001779
Iteration 17/1000 | Loss: 0.00001779
Iteration 18/1000 | Loss: 0.00001777
Iteration 19/1000 | Loss: 0.00001767
Iteration 20/1000 | Loss: 0.00001764
Iteration 21/1000 | Loss: 0.00001763
Iteration 22/1000 | Loss: 0.00001762
Iteration 23/1000 | Loss: 0.00001761
Iteration 24/1000 | Loss: 0.00001758
Iteration 25/1000 | Loss: 0.00001752
Iteration 26/1000 | Loss: 0.00001745
Iteration 27/1000 | Loss: 0.00001743
Iteration 28/1000 | Loss: 0.00001740
Iteration 29/1000 | Loss: 0.00001739
Iteration 30/1000 | Loss: 0.00001739
Iteration 31/1000 | Loss: 0.00001739
Iteration 32/1000 | Loss: 0.00001738
Iteration 33/1000 | Loss: 0.00001734
Iteration 34/1000 | Loss: 0.00001733
Iteration 35/1000 | Loss: 0.00001732
Iteration 36/1000 | Loss: 0.00001732
Iteration 37/1000 | Loss: 0.00001731
Iteration 38/1000 | Loss: 0.00001731
Iteration 39/1000 | Loss: 0.00001731
Iteration 40/1000 | Loss: 0.00001730
Iteration 41/1000 | Loss: 0.00001730
Iteration 42/1000 | Loss: 0.00001726
Iteration 43/1000 | Loss: 0.00001725
Iteration 44/1000 | Loss: 0.00001724
Iteration 45/1000 | Loss: 0.00001724
Iteration 46/1000 | Loss: 0.00001724
Iteration 47/1000 | Loss: 0.00001723
Iteration 48/1000 | Loss: 0.00001723
Iteration 49/1000 | Loss: 0.00001722
Iteration 50/1000 | Loss: 0.00001721
Iteration 51/1000 | Loss: 0.00001720
Iteration 52/1000 | Loss: 0.00001720
Iteration 53/1000 | Loss: 0.00001719
Iteration 54/1000 | Loss: 0.00001719
Iteration 55/1000 | Loss: 0.00001719
Iteration 56/1000 | Loss: 0.00001719
Iteration 57/1000 | Loss: 0.00001718
Iteration 58/1000 | Loss: 0.00001716
Iteration 59/1000 | Loss: 0.00001715
Iteration 60/1000 | Loss: 0.00001715
Iteration 61/1000 | Loss: 0.00001715
Iteration 62/1000 | Loss: 0.00001714
Iteration 63/1000 | Loss: 0.00001714
Iteration 64/1000 | Loss: 0.00001712
Iteration 65/1000 | Loss: 0.00001712
Iteration 66/1000 | Loss: 0.00001712
Iteration 67/1000 | Loss: 0.00001712
Iteration 68/1000 | Loss: 0.00001711
Iteration 69/1000 | Loss: 0.00001711
Iteration 70/1000 | Loss: 0.00001711
Iteration 71/1000 | Loss: 0.00001711
Iteration 72/1000 | Loss: 0.00001711
Iteration 73/1000 | Loss: 0.00001711
Iteration 74/1000 | Loss: 0.00001711
Iteration 75/1000 | Loss: 0.00001710
Iteration 76/1000 | Loss: 0.00001710
Iteration 77/1000 | Loss: 0.00001709
Iteration 78/1000 | Loss: 0.00001709
Iteration 79/1000 | Loss: 0.00001709
Iteration 80/1000 | Loss: 0.00001708
Iteration 81/1000 | Loss: 0.00001708
Iteration 82/1000 | Loss: 0.00001707
Iteration 83/1000 | Loss: 0.00001707
Iteration 84/1000 | Loss: 0.00001707
Iteration 85/1000 | Loss: 0.00001706
Iteration 86/1000 | Loss: 0.00001706
Iteration 87/1000 | Loss: 0.00001706
Iteration 88/1000 | Loss: 0.00001705
Iteration 89/1000 | Loss: 0.00001705
Iteration 90/1000 | Loss: 0.00001705
Iteration 91/1000 | Loss: 0.00001705
Iteration 92/1000 | Loss: 0.00001705
Iteration 93/1000 | Loss: 0.00001705
Iteration 94/1000 | Loss: 0.00001704
Iteration 95/1000 | Loss: 0.00001704
Iteration 96/1000 | Loss: 0.00001704
Iteration 97/1000 | Loss: 0.00001704
Iteration 98/1000 | Loss: 0.00001704
Iteration 99/1000 | Loss: 0.00001703
Iteration 100/1000 | Loss: 0.00001703
Iteration 101/1000 | Loss: 0.00001703
Iteration 102/1000 | Loss: 0.00001702
Iteration 103/1000 | Loss: 0.00001702
Iteration 104/1000 | Loss: 0.00001702
Iteration 105/1000 | Loss: 0.00001702
Iteration 106/1000 | Loss: 0.00001702
Iteration 107/1000 | Loss: 0.00001702
Iteration 108/1000 | Loss: 0.00001702
Iteration 109/1000 | Loss: 0.00001701
Iteration 110/1000 | Loss: 0.00001701
Iteration 111/1000 | Loss: 0.00001701
Iteration 112/1000 | Loss: 0.00001701
Iteration 113/1000 | Loss: 0.00001701
Iteration 114/1000 | Loss: 0.00001701
Iteration 115/1000 | Loss: 0.00001701
Iteration 116/1000 | Loss: 0.00001701
Iteration 117/1000 | Loss: 0.00001701
Iteration 118/1000 | Loss: 0.00001701
Iteration 119/1000 | Loss: 0.00001701
Iteration 120/1000 | Loss: 0.00001701
Iteration 121/1000 | Loss: 0.00001700
Iteration 122/1000 | Loss: 0.00001700
Iteration 123/1000 | Loss: 0.00001700
Iteration 124/1000 | Loss: 0.00001700
Iteration 125/1000 | Loss: 0.00001700
Iteration 126/1000 | Loss: 0.00001700
Iteration 127/1000 | Loss: 0.00001700
Iteration 128/1000 | Loss: 0.00001700
Iteration 129/1000 | Loss: 0.00001700
Iteration 130/1000 | Loss: 0.00001700
Iteration 131/1000 | Loss: 0.00001700
Iteration 132/1000 | Loss: 0.00001700
Iteration 133/1000 | Loss: 0.00001700
Iteration 134/1000 | Loss: 0.00001700
Iteration 135/1000 | Loss: 0.00001700
Iteration 136/1000 | Loss: 0.00001700
Iteration 137/1000 | Loss: 0.00001700
Iteration 138/1000 | Loss: 0.00001700
Iteration 139/1000 | Loss: 0.00001700
Iteration 140/1000 | Loss: 0.00001700
Iteration 141/1000 | Loss: 0.00001700
Iteration 142/1000 | Loss: 0.00001700
Iteration 143/1000 | Loss: 0.00001700
Iteration 144/1000 | Loss: 0.00001700
Iteration 145/1000 | Loss: 0.00001700
Iteration 146/1000 | Loss: 0.00001700
Iteration 147/1000 | Loss: 0.00001700
Iteration 148/1000 | Loss: 0.00001700
Iteration 149/1000 | Loss: 0.00001700
Iteration 150/1000 | Loss: 0.00001700
Iteration 151/1000 | Loss: 0.00001700
Iteration 152/1000 | Loss: 0.00001700
Iteration 153/1000 | Loss: 0.00001700
Iteration 154/1000 | Loss: 0.00001700
Iteration 155/1000 | Loss: 0.00001700
Iteration 156/1000 | Loss: 0.00001700
Iteration 157/1000 | Loss: 0.00001700
Iteration 158/1000 | Loss: 0.00001700
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 158. Stopping optimization.
Last 5 losses: [1.6996611520880833e-05, 1.6996611520880833e-05, 1.6996611520880833e-05, 1.6996611520880833e-05, 1.6996611520880833e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6996611520880833e-05

Optimization complete. Final v2v error: 3.5733275413513184 mm

Highest mean error: 3.7161529064178467 mm for frame 20

Lowest mean error: 3.4111292362213135 mm for frame 229

Saving results

Total time: 43.555508613586426
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_008/1009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_008/1009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_008/1009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00933870
Iteration 2/25 | Loss: 0.00192315
Iteration 3/25 | Loss: 0.00185879
Iteration 4/25 | Loss: 0.00165330
Iteration 5/25 | Loss: 0.00156523
Iteration 6/25 | Loss: 0.00139736
Iteration 7/25 | Loss: 0.00136445
Iteration 8/25 | Loss: 0.00139468
Iteration 9/25 | Loss: 0.00133400
Iteration 10/25 | Loss: 0.00133763
Iteration 11/25 | Loss: 0.00132914
Iteration 12/25 | Loss: 0.00132904
Iteration 13/25 | Loss: 0.00132904
Iteration 14/25 | Loss: 0.00132904
Iteration 15/25 | Loss: 0.00132904
Iteration 16/25 | Loss: 0.00132904
Iteration 17/25 | Loss: 0.00132904
Iteration 18/25 | Loss: 0.00132904
Iteration 19/25 | Loss: 0.00132903
Iteration 20/25 | Loss: 0.00132903
Iteration 21/25 | Loss: 0.00132903
Iteration 22/25 | Loss: 0.00132903
Iteration 23/25 | Loss: 0.00132903
Iteration 24/25 | Loss: 0.00132903
Iteration 25/25 | Loss: 0.00132903

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.25558937
Iteration 2/25 | Loss: 0.00075088
Iteration 3/25 | Loss: 0.00075088
Iteration 4/25 | Loss: 0.00075088
Iteration 5/25 | Loss: 0.00075088
Iteration 6/25 | Loss: 0.00075088
Iteration 7/25 | Loss: 0.00075088
Iteration 8/25 | Loss: 0.00075088
Iteration 9/25 | Loss: 0.00075088
Iteration 10/25 | Loss: 0.00075088
Iteration 11/25 | Loss: 0.00075088
Iteration 12/25 | Loss: 0.00075088
Iteration 13/25 | Loss: 0.00075088
Iteration 14/25 | Loss: 0.00075088
Iteration 15/25 | Loss: 0.00075088
Iteration 16/25 | Loss: 0.00075088
Iteration 17/25 | Loss: 0.00075088
Iteration 18/25 | Loss: 0.00075088
Iteration 19/25 | Loss: 0.00075088
Iteration 20/25 | Loss: 0.00075088
Iteration 21/25 | Loss: 0.00075088
Iteration 22/25 | Loss: 0.00075088
Iteration 23/25 | Loss: 0.00075088
Iteration 24/25 | Loss: 0.00075088
Iteration 25/25 | Loss: 0.00075088

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00075088
Iteration 2/1000 | Loss: 0.00006449
Iteration 3/1000 | Loss: 0.00041902
Iteration 4/1000 | Loss: 0.00005103
Iteration 5/1000 | Loss: 0.00035714
Iteration 6/1000 | Loss: 0.00003600
Iteration 7/1000 | Loss: 0.00003381
Iteration 8/1000 | Loss: 0.00039285
Iteration 9/1000 | Loss: 0.00003273
Iteration 10/1000 | Loss: 0.00032572
Iteration 11/1000 | Loss: 0.00003164
Iteration 12/1000 | Loss: 0.00003085
Iteration 13/1000 | Loss: 0.00003044
Iteration 14/1000 | Loss: 0.00003006
Iteration 15/1000 | Loss: 0.00030709
Iteration 16/1000 | Loss: 0.00006735
Iteration 17/1000 | Loss: 0.00010462
Iteration 18/1000 | Loss: 0.00002979
Iteration 19/1000 | Loss: 0.00002958
Iteration 20/1000 | Loss: 0.00002937
Iteration 21/1000 | Loss: 0.00002924
Iteration 22/1000 | Loss: 0.00002924
Iteration 23/1000 | Loss: 0.00002913
Iteration 24/1000 | Loss: 0.00002913
Iteration 25/1000 | Loss: 0.00002910
Iteration 26/1000 | Loss: 0.00002910
Iteration 27/1000 | Loss: 0.00002902
Iteration 28/1000 | Loss: 0.00002896
Iteration 29/1000 | Loss: 0.00002893
Iteration 30/1000 | Loss: 0.00002893
Iteration 31/1000 | Loss: 0.00002893
Iteration 32/1000 | Loss: 0.00002889
Iteration 33/1000 | Loss: 0.00002886
Iteration 34/1000 | Loss: 0.00002886
Iteration 35/1000 | Loss: 0.00002886
Iteration 36/1000 | Loss: 0.00002886
Iteration 37/1000 | Loss: 0.00002885
Iteration 38/1000 | Loss: 0.00002885
Iteration 39/1000 | Loss: 0.00002885
Iteration 40/1000 | Loss: 0.00002885
Iteration 41/1000 | Loss: 0.00002885
Iteration 42/1000 | Loss: 0.00002885
Iteration 43/1000 | Loss: 0.00002885
Iteration 44/1000 | Loss: 0.00002885
Iteration 45/1000 | Loss: 0.00002885
Iteration 46/1000 | Loss: 0.00002884
Iteration 47/1000 | Loss: 0.00002884
Iteration 48/1000 | Loss: 0.00002884
Iteration 49/1000 | Loss: 0.00002884
Iteration 50/1000 | Loss: 0.00002884
Iteration 51/1000 | Loss: 0.00002884
Iteration 52/1000 | Loss: 0.00002884
Iteration 53/1000 | Loss: 0.00002884
Iteration 54/1000 | Loss: 0.00002884
Iteration 55/1000 | Loss: 0.00002884
Iteration 56/1000 | Loss: 0.00002884
Iteration 57/1000 | Loss: 0.00002884
Iteration 58/1000 | Loss: 0.00002884
Iteration 59/1000 | Loss: 0.00002884
Iteration 60/1000 | Loss: 0.00002884
Iteration 61/1000 | Loss: 0.00002884
Iteration 62/1000 | Loss: 0.00002884
Iteration 63/1000 | Loss: 0.00002884
Iteration 64/1000 | Loss: 0.00002884
Iteration 65/1000 | Loss: 0.00002884
Iteration 66/1000 | Loss: 0.00002884
Iteration 67/1000 | Loss: 0.00002884
Iteration 68/1000 | Loss: 0.00002884
Iteration 69/1000 | Loss: 0.00002884
Iteration 70/1000 | Loss: 0.00002884
Iteration 71/1000 | Loss: 0.00002884
Iteration 72/1000 | Loss: 0.00002884
Iteration 73/1000 | Loss: 0.00002884
Iteration 74/1000 | Loss: 0.00002884
Iteration 75/1000 | Loss: 0.00002884
Iteration 76/1000 | Loss: 0.00002884
Iteration 77/1000 | Loss: 0.00002884
Iteration 78/1000 | Loss: 0.00002884
Iteration 79/1000 | Loss: 0.00002884
Iteration 80/1000 | Loss: 0.00002884
Iteration 81/1000 | Loss: 0.00002884
Iteration 82/1000 | Loss: 0.00002884
Iteration 83/1000 | Loss: 0.00002884
Iteration 84/1000 | Loss: 0.00002884
Iteration 85/1000 | Loss: 0.00002884
Iteration 86/1000 | Loss: 0.00002884
Iteration 87/1000 | Loss: 0.00002884
Iteration 88/1000 | Loss: 0.00002884
Iteration 89/1000 | Loss: 0.00002884
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 89. Stopping optimization.
Last 5 losses: [2.8835131161031313e-05, 2.8835131161031313e-05, 2.8835131161031313e-05, 2.8835131161031313e-05, 2.8835131161031313e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.8835131161031313e-05

Optimization complete. Final v2v error: 4.565357685089111 mm

Highest mean error: 4.983597755432129 mm for frame 144

Lowest mean error: 4.137735366821289 mm for frame 203

Saving results

Total time: 64.39430069923401
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_008/1037/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_008/1037.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_008/1037
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00437963
Iteration 2/25 | Loss: 0.00145592
Iteration 3/25 | Loss: 0.00121157
Iteration 4/25 | Loss: 0.00117421
Iteration 5/25 | Loss: 0.00116730
Iteration 6/25 | Loss: 0.00116545
Iteration 7/25 | Loss: 0.00116545
Iteration 8/25 | Loss: 0.00116545
Iteration 9/25 | Loss: 0.00116545
Iteration 10/25 | Loss: 0.00116545
Iteration 11/25 | Loss: 0.00116545
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001165446825325489, 0.001165446825325489, 0.001165446825325489, 0.001165446825325489, 0.001165446825325489]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001165446825325489

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35484397
Iteration 2/25 | Loss: 0.00075727
Iteration 3/25 | Loss: 0.00075725
Iteration 4/25 | Loss: 0.00075725
Iteration 5/25 | Loss: 0.00075725
Iteration 6/25 | Loss: 0.00075725
Iteration 7/25 | Loss: 0.00075725
Iteration 8/25 | Loss: 0.00075725
Iteration 9/25 | Loss: 0.00075725
Iteration 10/25 | Loss: 0.00075725
Iteration 11/25 | Loss: 0.00075725
Iteration 12/25 | Loss: 0.00075725
Iteration 13/25 | Loss: 0.00075725
Iteration 14/25 | Loss: 0.00075725
Iteration 15/25 | Loss: 0.00075725
Iteration 16/25 | Loss: 0.00075725
Iteration 17/25 | Loss: 0.00075725
Iteration 18/25 | Loss: 0.00075725
Iteration 19/25 | Loss: 0.00075725
Iteration 20/25 | Loss: 0.00075725
Iteration 21/25 | Loss: 0.00075725
Iteration 22/25 | Loss: 0.00075725
Iteration 23/25 | Loss: 0.00075725
Iteration 24/25 | Loss: 0.00075725
Iteration 25/25 | Loss: 0.00075725
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0007572491886094213, 0.0007572491886094213, 0.0007572491886094213, 0.0007572491886094213, 0.0007572491886094213]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007572491886094213

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00075725
Iteration 2/1000 | Loss: 0.00003553
Iteration 3/1000 | Loss: 0.00002323
Iteration 4/1000 | Loss: 0.00002012
Iteration 5/1000 | Loss: 0.00001925
Iteration 6/1000 | Loss: 0.00001835
Iteration 7/1000 | Loss: 0.00001785
Iteration 8/1000 | Loss: 0.00001748
Iteration 9/1000 | Loss: 0.00001722
Iteration 10/1000 | Loss: 0.00001702
Iteration 11/1000 | Loss: 0.00001687
Iteration 12/1000 | Loss: 0.00001684
Iteration 13/1000 | Loss: 0.00001675
Iteration 14/1000 | Loss: 0.00001673
Iteration 15/1000 | Loss: 0.00001672
Iteration 16/1000 | Loss: 0.00001672
Iteration 17/1000 | Loss: 0.00001661
Iteration 18/1000 | Loss: 0.00001659
Iteration 19/1000 | Loss: 0.00001655
Iteration 20/1000 | Loss: 0.00001653
Iteration 21/1000 | Loss: 0.00001652
Iteration 22/1000 | Loss: 0.00001652
Iteration 23/1000 | Loss: 0.00001651
Iteration 24/1000 | Loss: 0.00001650
Iteration 25/1000 | Loss: 0.00001649
Iteration 26/1000 | Loss: 0.00001649
Iteration 27/1000 | Loss: 0.00001648
Iteration 28/1000 | Loss: 0.00001647
Iteration 29/1000 | Loss: 0.00001647
Iteration 30/1000 | Loss: 0.00001646
Iteration 31/1000 | Loss: 0.00001646
Iteration 32/1000 | Loss: 0.00001645
Iteration 33/1000 | Loss: 0.00001645
Iteration 34/1000 | Loss: 0.00001644
Iteration 35/1000 | Loss: 0.00001644
Iteration 36/1000 | Loss: 0.00001644
Iteration 37/1000 | Loss: 0.00001644
Iteration 38/1000 | Loss: 0.00001644
Iteration 39/1000 | Loss: 0.00001644
Iteration 40/1000 | Loss: 0.00001644
Iteration 41/1000 | Loss: 0.00001644
Iteration 42/1000 | Loss: 0.00001644
Iteration 43/1000 | Loss: 0.00001644
Iteration 44/1000 | Loss: 0.00001644
Iteration 45/1000 | Loss: 0.00001642
Iteration 46/1000 | Loss: 0.00001641
Iteration 47/1000 | Loss: 0.00001641
Iteration 48/1000 | Loss: 0.00001641
Iteration 49/1000 | Loss: 0.00001641
Iteration 50/1000 | Loss: 0.00001640
Iteration 51/1000 | Loss: 0.00001640
Iteration 52/1000 | Loss: 0.00001640
Iteration 53/1000 | Loss: 0.00001639
Iteration 54/1000 | Loss: 0.00001638
Iteration 55/1000 | Loss: 0.00001637
Iteration 56/1000 | Loss: 0.00001637
Iteration 57/1000 | Loss: 0.00001636
Iteration 58/1000 | Loss: 0.00001636
Iteration 59/1000 | Loss: 0.00001635
Iteration 60/1000 | Loss: 0.00001635
Iteration 61/1000 | Loss: 0.00001635
Iteration 62/1000 | Loss: 0.00001634
Iteration 63/1000 | Loss: 0.00001634
Iteration 64/1000 | Loss: 0.00001634
Iteration 65/1000 | Loss: 0.00001634
Iteration 66/1000 | Loss: 0.00001633
Iteration 67/1000 | Loss: 0.00001633
Iteration 68/1000 | Loss: 0.00001631
Iteration 69/1000 | Loss: 0.00001631
Iteration 70/1000 | Loss: 0.00001631
Iteration 71/1000 | Loss: 0.00001631
Iteration 72/1000 | Loss: 0.00001631
Iteration 73/1000 | Loss: 0.00001631
Iteration 74/1000 | Loss: 0.00001631
Iteration 75/1000 | Loss: 0.00001631
Iteration 76/1000 | Loss: 0.00001631
Iteration 77/1000 | Loss: 0.00001631
Iteration 78/1000 | Loss: 0.00001630
Iteration 79/1000 | Loss: 0.00001630
Iteration 80/1000 | Loss: 0.00001630
Iteration 81/1000 | Loss: 0.00001629
Iteration 82/1000 | Loss: 0.00001629
Iteration 83/1000 | Loss: 0.00001629
Iteration 84/1000 | Loss: 0.00001628
Iteration 85/1000 | Loss: 0.00001628
Iteration 86/1000 | Loss: 0.00001628
Iteration 87/1000 | Loss: 0.00001628
Iteration 88/1000 | Loss: 0.00001627
Iteration 89/1000 | Loss: 0.00001627
Iteration 90/1000 | Loss: 0.00001627
Iteration 91/1000 | Loss: 0.00001627
Iteration 92/1000 | Loss: 0.00001626
Iteration 93/1000 | Loss: 0.00001626
Iteration 94/1000 | Loss: 0.00001626
Iteration 95/1000 | Loss: 0.00001626
Iteration 96/1000 | Loss: 0.00001625
Iteration 97/1000 | Loss: 0.00001625
Iteration 98/1000 | Loss: 0.00001625
Iteration 99/1000 | Loss: 0.00001625
Iteration 100/1000 | Loss: 0.00001625
Iteration 101/1000 | Loss: 0.00001624
Iteration 102/1000 | Loss: 0.00001624
Iteration 103/1000 | Loss: 0.00001624
Iteration 104/1000 | Loss: 0.00001624
Iteration 105/1000 | Loss: 0.00001624
Iteration 106/1000 | Loss: 0.00001624
Iteration 107/1000 | Loss: 0.00001624
Iteration 108/1000 | Loss: 0.00001623
Iteration 109/1000 | Loss: 0.00001623
Iteration 110/1000 | Loss: 0.00001623
Iteration 111/1000 | Loss: 0.00001623
Iteration 112/1000 | Loss: 0.00001623
Iteration 113/1000 | Loss: 0.00001623
Iteration 114/1000 | Loss: 0.00001623
Iteration 115/1000 | Loss: 0.00001623
Iteration 116/1000 | Loss: 0.00001623
Iteration 117/1000 | Loss: 0.00001623
Iteration 118/1000 | Loss: 0.00001623
Iteration 119/1000 | Loss: 0.00001623
Iteration 120/1000 | Loss: 0.00001623
Iteration 121/1000 | Loss: 0.00001623
Iteration 122/1000 | Loss: 0.00001623
Iteration 123/1000 | Loss: 0.00001623
Iteration 124/1000 | Loss: 0.00001623
Iteration 125/1000 | Loss: 0.00001623
Iteration 126/1000 | Loss: 0.00001623
Iteration 127/1000 | Loss: 0.00001623
Iteration 128/1000 | Loss: 0.00001623
Iteration 129/1000 | Loss: 0.00001623
Iteration 130/1000 | Loss: 0.00001623
Iteration 131/1000 | Loss: 0.00001623
Iteration 132/1000 | Loss: 0.00001623
Iteration 133/1000 | Loss: 0.00001623
Iteration 134/1000 | Loss: 0.00001623
Iteration 135/1000 | Loss: 0.00001623
Iteration 136/1000 | Loss: 0.00001623
Iteration 137/1000 | Loss: 0.00001623
Iteration 138/1000 | Loss: 0.00001623
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 138. Stopping optimization.
Last 5 losses: [1.6231795598287135e-05, 1.6231795598287135e-05, 1.6231795598287135e-05, 1.6231795598287135e-05, 1.6231795598287135e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6231795598287135e-05

Optimization complete. Final v2v error: 3.3963143825531006 mm

Highest mean error: 4.623696804046631 mm for frame 203

Lowest mean error: 2.914661169052124 mm for frame 97

Saving results

Total time: 40.81490635871887
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_008/1075/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_008/1075.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_008/1075
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00924702
Iteration 2/25 | Loss: 0.00924701
Iteration 3/25 | Loss: 0.00924701
Iteration 4/25 | Loss: 0.00372901
Iteration 5/25 | Loss: 0.00319091
Iteration 6/25 | Loss: 0.00218621
Iteration 7/25 | Loss: 0.00203957
Iteration 8/25 | Loss: 0.00201632
Iteration 9/25 | Loss: 0.00193222
Iteration 10/25 | Loss: 0.00210422
Iteration 11/25 | Loss: 0.00187284
Iteration 12/25 | Loss: 0.00153212
Iteration 13/25 | Loss: 0.00134305
Iteration 14/25 | Loss: 0.00128795
Iteration 15/25 | Loss: 0.00126780
Iteration 16/25 | Loss: 0.00124729
Iteration 17/25 | Loss: 0.00124907
Iteration 18/25 | Loss: 0.00124531
Iteration 19/25 | Loss: 0.00125130
Iteration 20/25 | Loss: 0.00124634
Iteration 21/25 | Loss: 0.00124544
Iteration 22/25 | Loss: 0.00124555
Iteration 23/25 | Loss: 0.00124953
Iteration 24/25 | Loss: 0.00124854
Iteration 25/25 | Loss: 0.00124968

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33264196
Iteration 2/25 | Loss: 0.00141567
Iteration 3/25 | Loss: 0.00080548
Iteration 4/25 | Loss: 0.00080548
Iteration 5/25 | Loss: 0.00080548
Iteration 6/25 | Loss: 0.00080548
Iteration 7/25 | Loss: 0.00080548
Iteration 8/25 | Loss: 0.00080547
Iteration 9/25 | Loss: 0.00080547
Iteration 10/25 | Loss: 0.00080547
Iteration 11/25 | Loss: 0.00080547
Iteration 12/25 | Loss: 0.00080547
Iteration 13/25 | Loss: 0.00080547
Iteration 14/25 | Loss: 0.00080547
Iteration 15/25 | Loss: 0.00080547
Iteration 16/25 | Loss: 0.00080547
Iteration 17/25 | Loss: 0.00080547
Iteration 18/25 | Loss: 0.00080547
Iteration 19/25 | Loss: 0.00080547
Iteration 20/25 | Loss: 0.00080547
Iteration 21/25 | Loss: 0.00080547
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0008054729551076889, 0.0008054729551076889, 0.0008054729551076889, 0.0008054729551076889, 0.0008054729551076889]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008054729551076889

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00080547
Iteration 2/1000 | Loss: 0.00076743
Iteration 3/1000 | Loss: 0.00023079
Iteration 4/1000 | Loss: 0.00046852
Iteration 5/1000 | Loss: 0.00024756
Iteration 6/1000 | Loss: 0.00053502
Iteration 7/1000 | Loss: 0.00122343
Iteration 8/1000 | Loss: 0.00009081
Iteration 9/1000 | Loss: 0.00004174
Iteration 10/1000 | Loss: 0.00003146
Iteration 11/1000 | Loss: 0.00002811
Iteration 12/1000 | Loss: 0.00002624
Iteration 13/1000 | Loss: 0.00013089
Iteration 14/1000 | Loss: 0.00002445
Iteration 15/1000 | Loss: 0.00002368
Iteration 16/1000 | Loss: 0.00002317
Iteration 17/1000 | Loss: 0.00002262
Iteration 18/1000 | Loss: 0.00002211
Iteration 19/1000 | Loss: 0.00002179
Iteration 20/1000 | Loss: 0.00002146
Iteration 21/1000 | Loss: 0.00002121
Iteration 22/1000 | Loss: 0.00002118
Iteration 23/1000 | Loss: 0.00002100
Iteration 24/1000 | Loss: 0.00002079
Iteration 25/1000 | Loss: 0.00119639
Iteration 26/1000 | Loss: 0.00138268
Iteration 27/1000 | Loss: 0.00003630
Iteration 28/1000 | Loss: 0.00002772
Iteration 29/1000 | Loss: 0.00002261
Iteration 30/1000 | Loss: 0.00001822
Iteration 31/1000 | Loss: 0.00001544
Iteration 32/1000 | Loss: 0.00001391
Iteration 33/1000 | Loss: 0.00001327
Iteration 34/1000 | Loss: 0.00001272
Iteration 35/1000 | Loss: 0.00076911
Iteration 36/1000 | Loss: 0.00061468
Iteration 37/1000 | Loss: 0.00377497
Iteration 38/1000 | Loss: 0.00309488
Iteration 39/1000 | Loss: 0.00123127
Iteration 40/1000 | Loss: 0.00303865
Iteration 41/1000 | Loss: 0.00011349
Iteration 42/1000 | Loss: 0.00003167
Iteration 43/1000 | Loss: 0.00010692
Iteration 44/1000 | Loss: 0.00002633
Iteration 45/1000 | Loss: 0.00001377
Iteration 46/1000 | Loss: 0.00001300
Iteration 47/1000 | Loss: 0.00046578
Iteration 48/1000 | Loss: 0.00001318
Iteration 49/1000 | Loss: 0.00001236
Iteration 50/1000 | Loss: 0.00001212
Iteration 51/1000 | Loss: 0.00001197
Iteration 52/1000 | Loss: 0.00001196
Iteration 53/1000 | Loss: 0.00001196
Iteration 54/1000 | Loss: 0.00001196
Iteration 55/1000 | Loss: 0.00001196
Iteration 56/1000 | Loss: 0.00019877
Iteration 57/1000 | Loss: 0.00001594
Iteration 58/1000 | Loss: 0.00001283
Iteration 59/1000 | Loss: 0.00001213
Iteration 60/1000 | Loss: 0.00021788
Iteration 61/1000 | Loss: 0.00021788
Iteration 62/1000 | Loss: 0.00002685
Iteration 63/1000 | Loss: 0.00001782
Iteration 64/1000 | Loss: 0.00001257
Iteration 65/1000 | Loss: 0.00001190
Iteration 66/1000 | Loss: 0.00001186
Iteration 67/1000 | Loss: 0.00001186
Iteration 68/1000 | Loss: 0.00001186
Iteration 69/1000 | Loss: 0.00001185
Iteration 70/1000 | Loss: 0.00001185
Iteration 71/1000 | Loss: 0.00001184
Iteration 72/1000 | Loss: 0.00001183
Iteration 73/1000 | Loss: 0.00001183
Iteration 74/1000 | Loss: 0.00001183
Iteration 75/1000 | Loss: 0.00001182
Iteration 76/1000 | Loss: 0.00001182
Iteration 77/1000 | Loss: 0.00001182
Iteration 78/1000 | Loss: 0.00001182
Iteration 79/1000 | Loss: 0.00001182
Iteration 80/1000 | Loss: 0.00001182
Iteration 81/1000 | Loss: 0.00001182
Iteration 82/1000 | Loss: 0.00001182
Iteration 83/1000 | Loss: 0.00001182
Iteration 84/1000 | Loss: 0.00001182
Iteration 85/1000 | Loss: 0.00001182
Iteration 86/1000 | Loss: 0.00001182
Iteration 87/1000 | Loss: 0.00001181
Iteration 88/1000 | Loss: 0.00001181
Iteration 89/1000 | Loss: 0.00001181
Iteration 90/1000 | Loss: 0.00001181
Iteration 91/1000 | Loss: 0.00001181
Iteration 92/1000 | Loss: 0.00001181
Iteration 93/1000 | Loss: 0.00001181
Iteration 94/1000 | Loss: 0.00001181
Iteration 95/1000 | Loss: 0.00001181
Iteration 96/1000 | Loss: 0.00001181
Iteration 97/1000 | Loss: 0.00001181
Iteration 98/1000 | Loss: 0.00001181
Iteration 99/1000 | Loss: 0.00001181
Iteration 100/1000 | Loss: 0.00001181
Iteration 101/1000 | Loss: 0.00001181
Iteration 102/1000 | Loss: 0.00001181
Iteration 103/1000 | Loss: 0.00001181
Iteration 104/1000 | Loss: 0.00001181
Iteration 105/1000 | Loss: 0.00001181
Iteration 106/1000 | Loss: 0.00001181
Iteration 107/1000 | Loss: 0.00001181
Iteration 108/1000 | Loss: 0.00001181
Iteration 109/1000 | Loss: 0.00001181
Iteration 110/1000 | Loss: 0.00018657
Iteration 111/1000 | Loss: 0.00007363
Iteration 112/1000 | Loss: 0.00004899
Iteration 113/1000 | Loss: 0.00006833
Iteration 114/1000 | Loss: 0.00003560
Iteration 115/1000 | Loss: 0.00001197
Iteration 116/1000 | Loss: 0.00001196
Iteration 117/1000 | Loss: 0.00001196
Iteration 118/1000 | Loss: 0.00001195
Iteration 119/1000 | Loss: 0.00001195
Iteration 120/1000 | Loss: 0.00001194
Iteration 121/1000 | Loss: 0.00001189
Iteration 122/1000 | Loss: 0.00001185
Iteration 123/1000 | Loss: 0.00001184
Iteration 124/1000 | Loss: 0.00001184
Iteration 125/1000 | Loss: 0.00001183
Iteration 126/1000 | Loss: 0.00001183
Iteration 127/1000 | Loss: 0.00001181
Iteration 128/1000 | Loss: 0.00001181
Iteration 129/1000 | Loss: 0.00001181
Iteration 130/1000 | Loss: 0.00001181
Iteration 131/1000 | Loss: 0.00001181
Iteration 132/1000 | Loss: 0.00001180
Iteration 133/1000 | Loss: 0.00001180
Iteration 134/1000 | Loss: 0.00001180
Iteration 135/1000 | Loss: 0.00001179
Iteration 136/1000 | Loss: 0.00001178
Iteration 137/1000 | Loss: 0.00001178
Iteration 138/1000 | Loss: 0.00001178
Iteration 139/1000 | Loss: 0.00001178
Iteration 140/1000 | Loss: 0.00001178
Iteration 141/1000 | Loss: 0.00001178
Iteration 142/1000 | Loss: 0.00001178
Iteration 143/1000 | Loss: 0.00001178
Iteration 144/1000 | Loss: 0.00001177
Iteration 145/1000 | Loss: 0.00001177
Iteration 146/1000 | Loss: 0.00001177
Iteration 147/1000 | Loss: 0.00001177
Iteration 148/1000 | Loss: 0.00001177
Iteration 149/1000 | Loss: 0.00001177
Iteration 150/1000 | Loss: 0.00001176
Iteration 151/1000 | Loss: 0.00001176
Iteration 152/1000 | Loss: 0.00001176
Iteration 153/1000 | Loss: 0.00001176
Iteration 154/1000 | Loss: 0.00001176
Iteration 155/1000 | Loss: 0.00001176
Iteration 156/1000 | Loss: 0.00001176
Iteration 157/1000 | Loss: 0.00001176
Iteration 158/1000 | Loss: 0.00001176
Iteration 159/1000 | Loss: 0.00001176
Iteration 160/1000 | Loss: 0.00001176
Iteration 161/1000 | Loss: 0.00001176
Iteration 162/1000 | Loss: 0.00001176
Iteration 163/1000 | Loss: 0.00001176
Iteration 164/1000 | Loss: 0.00001176
Iteration 165/1000 | Loss: 0.00001176
Iteration 166/1000 | Loss: 0.00001176
Iteration 167/1000 | Loss: 0.00001176
Iteration 168/1000 | Loss: 0.00001176
Iteration 169/1000 | Loss: 0.00001176
Iteration 170/1000 | Loss: 0.00001176
Iteration 171/1000 | Loss: 0.00001176
Iteration 172/1000 | Loss: 0.00001176
Iteration 173/1000 | Loss: 0.00001176
Iteration 174/1000 | Loss: 0.00001176
Iteration 175/1000 | Loss: 0.00001176
Iteration 176/1000 | Loss: 0.00001176
Iteration 177/1000 | Loss: 0.00001176
Iteration 178/1000 | Loss: 0.00001176
Iteration 179/1000 | Loss: 0.00001176
Iteration 180/1000 | Loss: 0.00001176
Iteration 181/1000 | Loss: 0.00001176
Iteration 182/1000 | Loss: 0.00001176
Iteration 183/1000 | Loss: 0.00001176
Iteration 184/1000 | Loss: 0.00001176
Iteration 185/1000 | Loss: 0.00001176
Iteration 186/1000 | Loss: 0.00001176
Iteration 187/1000 | Loss: 0.00001176
Iteration 188/1000 | Loss: 0.00001176
Iteration 189/1000 | Loss: 0.00001176
Iteration 190/1000 | Loss: 0.00001176
Iteration 191/1000 | Loss: 0.00001176
Iteration 192/1000 | Loss: 0.00001176
Iteration 193/1000 | Loss: 0.00001176
Iteration 194/1000 | Loss: 0.00001176
Iteration 195/1000 | Loss: 0.00001176
Iteration 196/1000 | Loss: 0.00001176
Iteration 197/1000 | Loss: 0.00001176
Iteration 198/1000 | Loss: 0.00001176
Iteration 199/1000 | Loss: 0.00001176
Iteration 200/1000 | Loss: 0.00001176
Iteration 201/1000 | Loss: 0.00001176
Iteration 202/1000 | Loss: 0.00001176
Iteration 203/1000 | Loss: 0.00001176
Iteration 204/1000 | Loss: 0.00001176
Iteration 205/1000 | Loss: 0.00001176
Iteration 206/1000 | Loss: 0.00001176
Iteration 207/1000 | Loss: 0.00001176
Iteration 208/1000 | Loss: 0.00001176
Iteration 209/1000 | Loss: 0.00001176
Iteration 210/1000 | Loss: 0.00001176
Iteration 211/1000 | Loss: 0.00001176
Iteration 212/1000 | Loss: 0.00001176
Iteration 213/1000 | Loss: 0.00001176
Iteration 214/1000 | Loss: 0.00001176
Iteration 215/1000 | Loss: 0.00001176
Iteration 216/1000 | Loss: 0.00001176
Iteration 217/1000 | Loss: 0.00001176
Iteration 218/1000 | Loss: 0.00001176
Iteration 219/1000 | Loss: 0.00001176
Iteration 220/1000 | Loss: 0.00001176
Iteration 221/1000 | Loss: 0.00001176
Iteration 222/1000 | Loss: 0.00001176
Iteration 223/1000 | Loss: 0.00001176
Iteration 224/1000 | Loss: 0.00001176
Iteration 225/1000 | Loss: 0.00001176
Iteration 226/1000 | Loss: 0.00001176
Iteration 227/1000 | Loss: 0.00001176
Iteration 228/1000 | Loss: 0.00001176
Iteration 229/1000 | Loss: 0.00001176
Iteration 230/1000 | Loss: 0.00001176
Iteration 231/1000 | Loss: 0.00001176
Iteration 232/1000 | Loss: 0.00001176
Iteration 233/1000 | Loss: 0.00001176
Iteration 234/1000 | Loss: 0.00001176
Iteration 235/1000 | Loss: 0.00001176
Iteration 236/1000 | Loss: 0.00001176
Iteration 237/1000 | Loss: 0.00001176
Iteration 238/1000 | Loss: 0.00001176
Iteration 239/1000 | Loss: 0.00001176
Iteration 240/1000 | Loss: 0.00001176
Iteration 241/1000 | Loss: 0.00001176
Iteration 242/1000 | Loss: 0.00001176
Iteration 243/1000 | Loss: 0.00001176
Iteration 244/1000 | Loss: 0.00001176
Iteration 245/1000 | Loss: 0.00001176
Iteration 246/1000 | Loss: 0.00001176
Iteration 247/1000 | Loss: 0.00001176
Iteration 248/1000 | Loss: 0.00001176
Iteration 249/1000 | Loss: 0.00001176
Iteration 250/1000 | Loss: 0.00001176
Iteration 251/1000 | Loss: 0.00001176
Iteration 252/1000 | Loss: 0.00001176
Iteration 253/1000 | Loss: 0.00001176
Iteration 254/1000 | Loss: 0.00001176
Iteration 255/1000 | Loss: 0.00001176
Iteration 256/1000 | Loss: 0.00001176
Iteration 257/1000 | Loss: 0.00001176
Iteration 258/1000 | Loss: 0.00001176
Iteration 259/1000 | Loss: 0.00001176
Iteration 260/1000 | Loss: 0.00001176
Iteration 261/1000 | Loss: 0.00001176
Iteration 262/1000 | Loss: 0.00001176
Iteration 263/1000 | Loss: 0.00001176
Iteration 264/1000 | Loss: 0.00001176
Iteration 265/1000 | Loss: 0.00001176
Iteration 266/1000 | Loss: 0.00001176
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 266. Stopping optimization.
Last 5 losses: [1.1761418136302382e-05, 1.1761418136302382e-05, 1.1761418136302382e-05, 1.1761418136302382e-05, 1.1761418136302382e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1761418136302382e-05

Optimization complete. Final v2v error: 2.910897970199585 mm

Highest mean error: 3.466486930847168 mm for frame 1

Lowest mean error: 2.86674427986145 mm for frame 130

Saving results

Total time: 143.07426857948303
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_008/1051/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_008/1051.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_008/1051
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00790520
Iteration 2/25 | Loss: 0.00123169
Iteration 3/25 | Loss: 0.00112564
Iteration 4/25 | Loss: 0.00111410
Iteration 5/25 | Loss: 0.00111196
Iteration 6/25 | Loss: 0.00111196
Iteration 7/25 | Loss: 0.00111196
Iteration 8/25 | Loss: 0.00111196
Iteration 9/25 | Loss: 0.00111196
Iteration 10/25 | Loss: 0.00111196
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.001111958990804851, 0.001111958990804851, 0.001111958990804851, 0.001111958990804851, 0.001111958990804851]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001111958990804851

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36604679
Iteration 2/25 | Loss: 0.00078667
Iteration 3/25 | Loss: 0.00078667
Iteration 4/25 | Loss: 0.00078667
Iteration 5/25 | Loss: 0.00078667
Iteration 6/25 | Loss: 0.00078667
Iteration 7/25 | Loss: 0.00078667
Iteration 8/25 | Loss: 0.00078667
Iteration 9/25 | Loss: 0.00078667
Iteration 10/25 | Loss: 0.00078667
Iteration 11/25 | Loss: 0.00078667
Iteration 12/25 | Loss: 0.00078667
Iteration 13/25 | Loss: 0.00078667
Iteration 14/25 | Loss: 0.00078667
Iteration 15/25 | Loss: 0.00078667
Iteration 16/25 | Loss: 0.00078667
Iteration 17/25 | Loss: 0.00078667
Iteration 18/25 | Loss: 0.00078667
Iteration 19/25 | Loss: 0.00078667
Iteration 20/25 | Loss: 0.00078667
Iteration 21/25 | Loss: 0.00078667
Iteration 22/25 | Loss: 0.00078667
Iteration 23/25 | Loss: 0.00078667
Iteration 24/25 | Loss: 0.00078667
Iteration 25/25 | Loss: 0.00078667

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00078667
Iteration 2/1000 | Loss: 0.00002468
Iteration 3/1000 | Loss: 0.00001546
Iteration 4/1000 | Loss: 0.00001270
Iteration 5/1000 | Loss: 0.00001167
Iteration 6/1000 | Loss: 0.00001108
Iteration 7/1000 | Loss: 0.00001063
Iteration 8/1000 | Loss: 0.00001039
Iteration 9/1000 | Loss: 0.00001035
Iteration 10/1000 | Loss: 0.00001016
Iteration 11/1000 | Loss: 0.00000989
Iteration 12/1000 | Loss: 0.00000977
Iteration 13/1000 | Loss: 0.00000974
Iteration 14/1000 | Loss: 0.00000968
Iteration 15/1000 | Loss: 0.00000967
Iteration 16/1000 | Loss: 0.00000966
Iteration 17/1000 | Loss: 0.00000966
Iteration 18/1000 | Loss: 0.00000965
Iteration 19/1000 | Loss: 0.00000965
Iteration 20/1000 | Loss: 0.00000965
Iteration 21/1000 | Loss: 0.00000963
Iteration 22/1000 | Loss: 0.00000963
Iteration 23/1000 | Loss: 0.00000961
Iteration 24/1000 | Loss: 0.00000960
Iteration 25/1000 | Loss: 0.00000957
Iteration 26/1000 | Loss: 0.00000955
Iteration 27/1000 | Loss: 0.00000955
Iteration 28/1000 | Loss: 0.00000954
Iteration 29/1000 | Loss: 0.00000952
Iteration 30/1000 | Loss: 0.00000952
Iteration 31/1000 | Loss: 0.00000952
Iteration 32/1000 | Loss: 0.00000952
Iteration 33/1000 | Loss: 0.00000952
Iteration 34/1000 | Loss: 0.00000951
Iteration 35/1000 | Loss: 0.00000951
Iteration 36/1000 | Loss: 0.00000949
Iteration 37/1000 | Loss: 0.00000949
Iteration 38/1000 | Loss: 0.00000948
Iteration 39/1000 | Loss: 0.00000948
Iteration 40/1000 | Loss: 0.00000948
Iteration 41/1000 | Loss: 0.00000947
Iteration 42/1000 | Loss: 0.00000947
Iteration 43/1000 | Loss: 0.00000947
Iteration 44/1000 | Loss: 0.00000946
Iteration 45/1000 | Loss: 0.00000946
Iteration 46/1000 | Loss: 0.00000945
Iteration 47/1000 | Loss: 0.00000944
Iteration 48/1000 | Loss: 0.00000944
Iteration 49/1000 | Loss: 0.00000943
Iteration 50/1000 | Loss: 0.00000943
Iteration 51/1000 | Loss: 0.00000943
Iteration 52/1000 | Loss: 0.00000943
Iteration 53/1000 | Loss: 0.00000943
Iteration 54/1000 | Loss: 0.00000943
Iteration 55/1000 | Loss: 0.00000943
Iteration 56/1000 | Loss: 0.00000942
Iteration 57/1000 | Loss: 0.00000942
Iteration 58/1000 | Loss: 0.00000942
Iteration 59/1000 | Loss: 0.00000941
Iteration 60/1000 | Loss: 0.00000941
Iteration 61/1000 | Loss: 0.00000941
Iteration 62/1000 | Loss: 0.00000940
Iteration 63/1000 | Loss: 0.00000940
Iteration 64/1000 | Loss: 0.00000939
Iteration 65/1000 | Loss: 0.00000939
Iteration 66/1000 | Loss: 0.00000939
Iteration 67/1000 | Loss: 0.00000936
Iteration 68/1000 | Loss: 0.00000936
Iteration 69/1000 | Loss: 0.00000936
Iteration 70/1000 | Loss: 0.00000935
Iteration 71/1000 | Loss: 0.00000935
Iteration 72/1000 | Loss: 0.00000934
Iteration 73/1000 | Loss: 0.00000934
Iteration 74/1000 | Loss: 0.00000933
Iteration 75/1000 | Loss: 0.00000932
Iteration 76/1000 | Loss: 0.00000931
Iteration 77/1000 | Loss: 0.00000931
Iteration 78/1000 | Loss: 0.00000931
Iteration 79/1000 | Loss: 0.00000931
Iteration 80/1000 | Loss: 0.00000931
Iteration 81/1000 | Loss: 0.00000931
Iteration 82/1000 | Loss: 0.00000930
Iteration 83/1000 | Loss: 0.00000930
Iteration 84/1000 | Loss: 0.00000930
Iteration 85/1000 | Loss: 0.00000930
Iteration 86/1000 | Loss: 0.00000930
Iteration 87/1000 | Loss: 0.00000930
Iteration 88/1000 | Loss: 0.00000930
Iteration 89/1000 | Loss: 0.00000930
Iteration 90/1000 | Loss: 0.00000930
Iteration 91/1000 | Loss: 0.00000930
Iteration 92/1000 | Loss: 0.00000930
Iteration 93/1000 | Loss: 0.00000929
Iteration 94/1000 | Loss: 0.00000929
Iteration 95/1000 | Loss: 0.00000929
Iteration 96/1000 | Loss: 0.00000929
Iteration 97/1000 | Loss: 0.00000929
Iteration 98/1000 | Loss: 0.00000929
Iteration 99/1000 | Loss: 0.00000929
Iteration 100/1000 | Loss: 0.00000929
Iteration 101/1000 | Loss: 0.00000928
Iteration 102/1000 | Loss: 0.00000928
Iteration 103/1000 | Loss: 0.00000928
Iteration 104/1000 | Loss: 0.00000927
Iteration 105/1000 | Loss: 0.00000927
Iteration 106/1000 | Loss: 0.00000927
Iteration 107/1000 | Loss: 0.00000927
Iteration 108/1000 | Loss: 0.00000926
Iteration 109/1000 | Loss: 0.00000926
Iteration 110/1000 | Loss: 0.00000926
Iteration 111/1000 | Loss: 0.00000926
Iteration 112/1000 | Loss: 0.00000926
Iteration 113/1000 | Loss: 0.00000926
Iteration 114/1000 | Loss: 0.00000925
Iteration 115/1000 | Loss: 0.00000925
Iteration 116/1000 | Loss: 0.00000925
Iteration 117/1000 | Loss: 0.00000925
Iteration 118/1000 | Loss: 0.00000925
Iteration 119/1000 | Loss: 0.00000925
Iteration 120/1000 | Loss: 0.00000925
Iteration 121/1000 | Loss: 0.00000924
Iteration 122/1000 | Loss: 0.00000924
Iteration 123/1000 | Loss: 0.00000923
Iteration 124/1000 | Loss: 0.00000922
Iteration 125/1000 | Loss: 0.00000922
Iteration 126/1000 | Loss: 0.00000922
Iteration 127/1000 | Loss: 0.00000922
Iteration 128/1000 | Loss: 0.00000922
Iteration 129/1000 | Loss: 0.00000922
Iteration 130/1000 | Loss: 0.00000922
Iteration 131/1000 | Loss: 0.00000922
Iteration 132/1000 | Loss: 0.00000922
Iteration 133/1000 | Loss: 0.00000922
Iteration 134/1000 | Loss: 0.00000921
Iteration 135/1000 | Loss: 0.00000921
Iteration 136/1000 | Loss: 0.00000921
Iteration 137/1000 | Loss: 0.00000921
Iteration 138/1000 | Loss: 0.00000921
Iteration 139/1000 | Loss: 0.00000921
Iteration 140/1000 | Loss: 0.00000921
Iteration 141/1000 | Loss: 0.00000921
Iteration 142/1000 | Loss: 0.00000921
Iteration 143/1000 | Loss: 0.00000921
Iteration 144/1000 | Loss: 0.00000921
Iteration 145/1000 | Loss: 0.00000921
Iteration 146/1000 | Loss: 0.00000921
Iteration 147/1000 | Loss: 0.00000921
Iteration 148/1000 | Loss: 0.00000920
Iteration 149/1000 | Loss: 0.00000920
Iteration 150/1000 | Loss: 0.00000920
Iteration 151/1000 | Loss: 0.00000920
Iteration 152/1000 | Loss: 0.00000920
Iteration 153/1000 | Loss: 0.00000920
Iteration 154/1000 | Loss: 0.00000920
Iteration 155/1000 | Loss: 0.00000920
Iteration 156/1000 | Loss: 0.00000920
Iteration 157/1000 | Loss: 0.00000920
Iteration 158/1000 | Loss: 0.00000920
Iteration 159/1000 | Loss: 0.00000920
Iteration 160/1000 | Loss: 0.00000920
Iteration 161/1000 | Loss: 0.00000920
Iteration 162/1000 | Loss: 0.00000920
Iteration 163/1000 | Loss: 0.00000920
Iteration 164/1000 | Loss: 0.00000920
Iteration 165/1000 | Loss: 0.00000920
Iteration 166/1000 | Loss: 0.00000920
Iteration 167/1000 | Loss: 0.00000920
Iteration 168/1000 | Loss: 0.00000920
Iteration 169/1000 | Loss: 0.00000920
Iteration 170/1000 | Loss: 0.00000920
Iteration 171/1000 | Loss: 0.00000920
Iteration 172/1000 | Loss: 0.00000920
Iteration 173/1000 | Loss: 0.00000920
Iteration 174/1000 | Loss: 0.00000920
Iteration 175/1000 | Loss: 0.00000920
Iteration 176/1000 | Loss: 0.00000920
Iteration 177/1000 | Loss: 0.00000920
Iteration 178/1000 | Loss: 0.00000920
Iteration 179/1000 | Loss: 0.00000920
Iteration 180/1000 | Loss: 0.00000920
Iteration 181/1000 | Loss: 0.00000920
Iteration 182/1000 | Loss: 0.00000920
Iteration 183/1000 | Loss: 0.00000920
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 183. Stopping optimization.
Last 5 losses: [9.197310646413825e-06, 9.197310646413825e-06, 9.197310646413825e-06, 9.197310646413825e-06, 9.197310646413825e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.197310646413825e-06

Optimization complete. Final v2v error: 2.583226203918457 mm

Highest mean error: 2.7621676921844482 mm for frame 47

Lowest mean error: 2.4472899436950684 mm for frame 143

Saving results

Total time: 36.32613730430603
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_008/1055/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_008/1055.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_008/1055
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00419348
Iteration 2/25 | Loss: 0.00125754
Iteration 3/25 | Loss: 0.00117166
Iteration 4/25 | Loss: 0.00116010
Iteration 5/25 | Loss: 0.00115614
Iteration 6/25 | Loss: 0.00115582
Iteration 7/25 | Loss: 0.00115582
Iteration 8/25 | Loss: 0.00115582
Iteration 9/25 | Loss: 0.00115582
Iteration 10/25 | Loss: 0.00115582
Iteration 11/25 | Loss: 0.00115582
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011558202095329762, 0.0011558202095329762, 0.0011558202095329762, 0.0011558202095329762, 0.0011558202095329762]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011558202095329762

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.40748203
Iteration 2/25 | Loss: 0.00100105
Iteration 3/25 | Loss: 0.00100105
Iteration 4/25 | Loss: 0.00100105
Iteration 5/25 | Loss: 0.00100105
Iteration 6/25 | Loss: 0.00100105
Iteration 7/25 | Loss: 0.00100105
Iteration 8/25 | Loss: 0.00100105
Iteration 9/25 | Loss: 0.00100105
Iteration 10/25 | Loss: 0.00100105
Iteration 11/25 | Loss: 0.00100105
Iteration 12/25 | Loss: 0.00100105
Iteration 13/25 | Loss: 0.00100105
Iteration 14/25 | Loss: 0.00100105
Iteration 15/25 | Loss: 0.00100105
Iteration 16/25 | Loss: 0.00100105
Iteration 17/25 | Loss: 0.00100105
Iteration 18/25 | Loss: 0.00100105
Iteration 19/25 | Loss: 0.00100105
Iteration 20/25 | Loss: 0.00100105
Iteration 21/25 | Loss: 0.00100105
Iteration 22/25 | Loss: 0.00100105
Iteration 23/25 | Loss: 0.00100105
Iteration 24/25 | Loss: 0.00100105
Iteration 25/25 | Loss: 0.00100105

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00100105
Iteration 2/1000 | Loss: 0.00002095
Iteration 3/1000 | Loss: 0.00001533
Iteration 4/1000 | Loss: 0.00001427
Iteration 5/1000 | Loss: 0.00001376
Iteration 6/1000 | Loss: 0.00001344
Iteration 7/1000 | Loss: 0.00001318
Iteration 8/1000 | Loss: 0.00001316
Iteration 9/1000 | Loss: 0.00001311
Iteration 10/1000 | Loss: 0.00001308
Iteration 11/1000 | Loss: 0.00001285
Iteration 12/1000 | Loss: 0.00001258
Iteration 13/1000 | Loss: 0.00001248
Iteration 14/1000 | Loss: 0.00001247
Iteration 15/1000 | Loss: 0.00001242
Iteration 16/1000 | Loss: 0.00001242
Iteration 17/1000 | Loss: 0.00001240
Iteration 18/1000 | Loss: 0.00001240
Iteration 19/1000 | Loss: 0.00001239
Iteration 20/1000 | Loss: 0.00001236
Iteration 21/1000 | Loss: 0.00001236
Iteration 22/1000 | Loss: 0.00001235
Iteration 23/1000 | Loss: 0.00001235
Iteration 24/1000 | Loss: 0.00001233
Iteration 25/1000 | Loss: 0.00001233
Iteration 26/1000 | Loss: 0.00001232
Iteration 27/1000 | Loss: 0.00001232
Iteration 28/1000 | Loss: 0.00001232
Iteration 29/1000 | Loss: 0.00001231
Iteration 30/1000 | Loss: 0.00001231
Iteration 31/1000 | Loss: 0.00001231
Iteration 32/1000 | Loss: 0.00001230
Iteration 33/1000 | Loss: 0.00001230
Iteration 34/1000 | Loss: 0.00001230
Iteration 35/1000 | Loss: 0.00001229
Iteration 36/1000 | Loss: 0.00001229
Iteration 37/1000 | Loss: 0.00001228
Iteration 38/1000 | Loss: 0.00001228
Iteration 39/1000 | Loss: 0.00001227
Iteration 40/1000 | Loss: 0.00001224
Iteration 41/1000 | Loss: 0.00001224
Iteration 42/1000 | Loss: 0.00001223
Iteration 43/1000 | Loss: 0.00001223
Iteration 44/1000 | Loss: 0.00001222
Iteration 45/1000 | Loss: 0.00001222
Iteration 46/1000 | Loss: 0.00001222
Iteration 47/1000 | Loss: 0.00001222
Iteration 48/1000 | Loss: 0.00001221
Iteration 49/1000 | Loss: 0.00001221
Iteration 50/1000 | Loss: 0.00001221
Iteration 51/1000 | Loss: 0.00001221
Iteration 52/1000 | Loss: 0.00001221
Iteration 53/1000 | Loss: 0.00001221
Iteration 54/1000 | Loss: 0.00001221
Iteration 55/1000 | Loss: 0.00001221
Iteration 56/1000 | Loss: 0.00001221
Iteration 57/1000 | Loss: 0.00001221
Iteration 58/1000 | Loss: 0.00001220
Iteration 59/1000 | Loss: 0.00001220
Iteration 60/1000 | Loss: 0.00001220
Iteration 61/1000 | Loss: 0.00001220
Iteration 62/1000 | Loss: 0.00001220
Iteration 63/1000 | Loss: 0.00001220
Iteration 64/1000 | Loss: 0.00001219
Iteration 65/1000 | Loss: 0.00001219
Iteration 66/1000 | Loss: 0.00001219
Iteration 67/1000 | Loss: 0.00001219
Iteration 68/1000 | Loss: 0.00001219
Iteration 69/1000 | Loss: 0.00001219
Iteration 70/1000 | Loss: 0.00001219
Iteration 71/1000 | Loss: 0.00001219
Iteration 72/1000 | Loss: 0.00001219
Iteration 73/1000 | Loss: 0.00001219
Iteration 74/1000 | Loss: 0.00001219
Iteration 75/1000 | Loss: 0.00001219
Iteration 76/1000 | Loss: 0.00001219
Iteration 77/1000 | Loss: 0.00001219
Iteration 78/1000 | Loss: 0.00001219
Iteration 79/1000 | Loss: 0.00001219
Iteration 80/1000 | Loss: 0.00001219
Iteration 81/1000 | Loss: 0.00001219
Iteration 82/1000 | Loss: 0.00001219
Iteration 83/1000 | Loss: 0.00001219
Iteration 84/1000 | Loss: 0.00001219
Iteration 85/1000 | Loss: 0.00001219
Iteration 86/1000 | Loss: 0.00001219
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 86. Stopping optimization.
Last 5 losses: [1.2190912457299419e-05, 1.2190912457299419e-05, 1.2190912457299419e-05, 1.2190912457299419e-05, 1.2190912457299419e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2190912457299419e-05

Optimization complete. Final v2v error: 2.946962356567383 mm

Highest mean error: 3.093904733657837 mm for frame 142

Lowest mean error: 2.7965731620788574 mm for frame 52

Saving results

Total time: 28.542162656784058
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_008/1043/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_008/1043.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_008/1043
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00807526
Iteration 2/25 | Loss: 0.00128494
Iteration 3/25 | Loss: 0.00118826
Iteration 4/25 | Loss: 0.00116167
Iteration 5/25 | Loss: 0.00115216
Iteration 6/25 | Loss: 0.00114990
Iteration 7/25 | Loss: 0.00114897
Iteration 8/25 | Loss: 0.00114897
Iteration 9/25 | Loss: 0.00114897
Iteration 10/25 | Loss: 0.00114897
Iteration 11/25 | Loss: 0.00114897
Iteration 12/25 | Loss: 0.00114897
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0011489668395370245, 0.0011489668395370245, 0.0011489668395370245, 0.0011489668395370245, 0.0011489668395370245]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011489668395370245

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.59564877
Iteration 2/25 | Loss: 0.00096506
Iteration 3/25 | Loss: 0.00096506
Iteration 4/25 | Loss: 0.00096506
Iteration 5/25 | Loss: 0.00096506
Iteration 6/25 | Loss: 0.00096506
Iteration 7/25 | Loss: 0.00096506
Iteration 8/25 | Loss: 0.00096506
Iteration 9/25 | Loss: 0.00096506
Iteration 10/25 | Loss: 0.00096506
Iteration 11/25 | Loss: 0.00096506
Iteration 12/25 | Loss: 0.00096506
Iteration 13/25 | Loss: 0.00096506
Iteration 14/25 | Loss: 0.00096506
Iteration 15/25 | Loss: 0.00096506
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0009650558931753039, 0.0009650558931753039, 0.0009650558931753039, 0.0009650558931753039, 0.0009650558931753039]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009650558931753039

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00096506
Iteration 2/1000 | Loss: 0.00005441
Iteration 3/1000 | Loss: 0.00003910
Iteration 4/1000 | Loss: 0.00002937
Iteration 5/1000 | Loss: 0.00002646
Iteration 6/1000 | Loss: 0.00002507
Iteration 7/1000 | Loss: 0.00002394
Iteration 8/1000 | Loss: 0.00002329
Iteration 9/1000 | Loss: 0.00002286
Iteration 10/1000 | Loss: 0.00002249
Iteration 11/1000 | Loss: 0.00002224
Iteration 12/1000 | Loss: 0.00002217
Iteration 13/1000 | Loss: 0.00002199
Iteration 14/1000 | Loss: 0.00002188
Iteration 15/1000 | Loss: 0.00002174
Iteration 16/1000 | Loss: 0.00002163
Iteration 17/1000 | Loss: 0.00002162
Iteration 18/1000 | Loss: 0.00002161
Iteration 19/1000 | Loss: 0.00002158
Iteration 20/1000 | Loss: 0.00002157
Iteration 21/1000 | Loss: 0.00002154
Iteration 22/1000 | Loss: 0.00002154
Iteration 23/1000 | Loss: 0.00002153
Iteration 24/1000 | Loss: 0.00002153
Iteration 25/1000 | Loss: 0.00002152
Iteration 26/1000 | Loss: 0.00002152
Iteration 27/1000 | Loss: 0.00002149
Iteration 28/1000 | Loss: 0.00002146
Iteration 29/1000 | Loss: 0.00002146
Iteration 30/1000 | Loss: 0.00002145
Iteration 31/1000 | Loss: 0.00002144
Iteration 32/1000 | Loss: 0.00002144
Iteration 33/1000 | Loss: 0.00002143
Iteration 34/1000 | Loss: 0.00002143
Iteration 35/1000 | Loss: 0.00002142
Iteration 36/1000 | Loss: 0.00002141
Iteration 37/1000 | Loss: 0.00002140
Iteration 38/1000 | Loss: 0.00002139
Iteration 39/1000 | Loss: 0.00002139
Iteration 40/1000 | Loss: 0.00002138
Iteration 41/1000 | Loss: 0.00002137
Iteration 42/1000 | Loss: 0.00002137
Iteration 43/1000 | Loss: 0.00002136
Iteration 44/1000 | Loss: 0.00002135
Iteration 45/1000 | Loss: 0.00002133
Iteration 46/1000 | Loss: 0.00002131
Iteration 47/1000 | Loss: 0.00002131
Iteration 48/1000 | Loss: 0.00002130
Iteration 49/1000 | Loss: 0.00002130
Iteration 50/1000 | Loss: 0.00002129
Iteration 51/1000 | Loss: 0.00002128
Iteration 52/1000 | Loss: 0.00002127
Iteration 53/1000 | Loss: 0.00002127
Iteration 54/1000 | Loss: 0.00002127
Iteration 55/1000 | Loss: 0.00002126
Iteration 56/1000 | Loss: 0.00002126
Iteration 57/1000 | Loss: 0.00002126
Iteration 58/1000 | Loss: 0.00002125
Iteration 59/1000 | Loss: 0.00002125
Iteration 60/1000 | Loss: 0.00002125
Iteration 61/1000 | Loss: 0.00002124
Iteration 62/1000 | Loss: 0.00002124
Iteration 63/1000 | Loss: 0.00002124
Iteration 64/1000 | Loss: 0.00002123
Iteration 65/1000 | Loss: 0.00002123
Iteration 66/1000 | Loss: 0.00002123
Iteration 67/1000 | Loss: 0.00002123
Iteration 68/1000 | Loss: 0.00002122
Iteration 69/1000 | Loss: 0.00002122
Iteration 70/1000 | Loss: 0.00002121
Iteration 71/1000 | Loss: 0.00002121
Iteration 72/1000 | Loss: 0.00002121
Iteration 73/1000 | Loss: 0.00002120
Iteration 74/1000 | Loss: 0.00002120
Iteration 75/1000 | Loss: 0.00002120
Iteration 76/1000 | Loss: 0.00002119
Iteration 77/1000 | Loss: 0.00002119
Iteration 78/1000 | Loss: 0.00002119
Iteration 79/1000 | Loss: 0.00002118
Iteration 80/1000 | Loss: 0.00002118
Iteration 81/1000 | Loss: 0.00002118
Iteration 82/1000 | Loss: 0.00002118
Iteration 83/1000 | Loss: 0.00002118
Iteration 84/1000 | Loss: 0.00002117
Iteration 85/1000 | Loss: 0.00002117
Iteration 86/1000 | Loss: 0.00002117
Iteration 87/1000 | Loss: 0.00002116
Iteration 88/1000 | Loss: 0.00002116
Iteration 89/1000 | Loss: 0.00002116
Iteration 90/1000 | Loss: 0.00002115
Iteration 91/1000 | Loss: 0.00002115
Iteration 92/1000 | Loss: 0.00002115
Iteration 93/1000 | Loss: 0.00002115
Iteration 94/1000 | Loss: 0.00002115
Iteration 95/1000 | Loss: 0.00002115
Iteration 96/1000 | Loss: 0.00002115
Iteration 97/1000 | Loss: 0.00002115
Iteration 98/1000 | Loss: 0.00002115
Iteration 99/1000 | Loss: 0.00002114
Iteration 100/1000 | Loss: 0.00002114
Iteration 101/1000 | Loss: 0.00002114
Iteration 102/1000 | Loss: 0.00002114
Iteration 103/1000 | Loss: 0.00002113
Iteration 104/1000 | Loss: 0.00002113
Iteration 105/1000 | Loss: 0.00002113
Iteration 106/1000 | Loss: 0.00002113
Iteration 107/1000 | Loss: 0.00002112
Iteration 108/1000 | Loss: 0.00002112
Iteration 109/1000 | Loss: 0.00002112
Iteration 110/1000 | Loss: 0.00002112
Iteration 111/1000 | Loss: 0.00002112
Iteration 112/1000 | Loss: 0.00002112
Iteration 113/1000 | Loss: 0.00002112
Iteration 114/1000 | Loss: 0.00002112
Iteration 115/1000 | Loss: 0.00002111
Iteration 116/1000 | Loss: 0.00002111
Iteration 117/1000 | Loss: 0.00002111
Iteration 118/1000 | Loss: 0.00002111
Iteration 119/1000 | Loss: 0.00002111
Iteration 120/1000 | Loss: 0.00002111
Iteration 121/1000 | Loss: 0.00002111
Iteration 122/1000 | Loss: 0.00002110
Iteration 123/1000 | Loss: 0.00002110
Iteration 124/1000 | Loss: 0.00002110
Iteration 125/1000 | Loss: 0.00002110
Iteration 126/1000 | Loss: 0.00002110
Iteration 127/1000 | Loss: 0.00002110
Iteration 128/1000 | Loss: 0.00002110
Iteration 129/1000 | Loss: 0.00002110
Iteration 130/1000 | Loss: 0.00002110
Iteration 131/1000 | Loss: 0.00002110
Iteration 132/1000 | Loss: 0.00002110
Iteration 133/1000 | Loss: 0.00002110
Iteration 134/1000 | Loss: 0.00002110
Iteration 135/1000 | Loss: 0.00002109
Iteration 136/1000 | Loss: 0.00002109
Iteration 137/1000 | Loss: 0.00002109
Iteration 138/1000 | Loss: 0.00002109
Iteration 139/1000 | Loss: 0.00002109
Iteration 140/1000 | Loss: 0.00002109
Iteration 141/1000 | Loss: 0.00002109
Iteration 142/1000 | Loss: 0.00002109
Iteration 143/1000 | Loss: 0.00002109
Iteration 144/1000 | Loss: 0.00002109
Iteration 145/1000 | Loss: 0.00002108
Iteration 146/1000 | Loss: 0.00002108
Iteration 147/1000 | Loss: 0.00002108
Iteration 148/1000 | Loss: 0.00002108
Iteration 149/1000 | Loss: 0.00002108
Iteration 150/1000 | Loss: 0.00002108
Iteration 151/1000 | Loss: 0.00002108
Iteration 152/1000 | Loss: 0.00002108
Iteration 153/1000 | Loss: 0.00002108
Iteration 154/1000 | Loss: 0.00002108
Iteration 155/1000 | Loss: 0.00002108
Iteration 156/1000 | Loss: 0.00002108
Iteration 157/1000 | Loss: 0.00002108
Iteration 158/1000 | Loss: 0.00002108
Iteration 159/1000 | Loss: 0.00002108
Iteration 160/1000 | Loss: 0.00002108
Iteration 161/1000 | Loss: 0.00002108
Iteration 162/1000 | Loss: 0.00002108
Iteration 163/1000 | Loss: 0.00002108
Iteration 164/1000 | Loss: 0.00002108
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 164. Stopping optimization.
Last 5 losses: [2.107884211000055e-05, 2.107884211000055e-05, 2.107884211000055e-05, 2.107884211000055e-05, 2.107884211000055e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.107884211000055e-05

Optimization complete. Final v2v error: 3.7724320888519287 mm

Highest mean error: 5.8630781173706055 mm for frame 136

Lowest mean error: 2.7204055786132812 mm for frame 83

Saving results

Total time: 42.87898349761963
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_008/1085/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_008/1085.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_008/1085
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00447710
Iteration 2/25 | Loss: 0.00118932
Iteration 3/25 | Loss: 0.00111212
Iteration 4/25 | Loss: 0.00110240
Iteration 5/25 | Loss: 0.00109986
Iteration 6/25 | Loss: 0.00109909
Iteration 7/25 | Loss: 0.00109909
Iteration 8/25 | Loss: 0.00109909
Iteration 9/25 | Loss: 0.00109909
Iteration 10/25 | Loss: 0.00109909
Iteration 11/25 | Loss: 0.00109909
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010990870650857687, 0.0010990870650857687, 0.0010990870650857687, 0.0010990870650857687, 0.0010990870650857687]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010990870650857687

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.30964005
Iteration 2/25 | Loss: 0.00066945
Iteration 3/25 | Loss: 0.00066943
Iteration 4/25 | Loss: 0.00066943
Iteration 5/25 | Loss: 0.00066943
Iteration 6/25 | Loss: 0.00066943
Iteration 7/25 | Loss: 0.00066943
Iteration 8/25 | Loss: 0.00066943
Iteration 9/25 | Loss: 0.00066943
Iteration 10/25 | Loss: 0.00066943
Iteration 11/25 | Loss: 0.00066943
Iteration 12/25 | Loss: 0.00066943
Iteration 13/25 | Loss: 0.00066943
Iteration 14/25 | Loss: 0.00066943
Iteration 15/25 | Loss: 0.00066943
Iteration 16/25 | Loss: 0.00066943
Iteration 17/25 | Loss: 0.00066943
Iteration 18/25 | Loss: 0.00066943
Iteration 19/25 | Loss: 0.00066943
Iteration 20/25 | Loss: 0.00066943
Iteration 21/25 | Loss: 0.00066943
Iteration 22/25 | Loss: 0.00066943
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0006694314652122557, 0.0006694314652122557, 0.0006694314652122557, 0.0006694314652122557, 0.0006694314652122557]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006694314652122557

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00066943
Iteration 2/1000 | Loss: 0.00002100
Iteration 3/1000 | Loss: 0.00001591
Iteration 4/1000 | Loss: 0.00001339
Iteration 5/1000 | Loss: 0.00001253
Iteration 6/1000 | Loss: 0.00001185
Iteration 7/1000 | Loss: 0.00001140
Iteration 8/1000 | Loss: 0.00001114
Iteration 9/1000 | Loss: 0.00001098
Iteration 10/1000 | Loss: 0.00001077
Iteration 11/1000 | Loss: 0.00001077
Iteration 12/1000 | Loss: 0.00001058
Iteration 13/1000 | Loss: 0.00001052
Iteration 14/1000 | Loss: 0.00001050
Iteration 15/1000 | Loss: 0.00001049
Iteration 16/1000 | Loss: 0.00001048
Iteration 17/1000 | Loss: 0.00001048
Iteration 18/1000 | Loss: 0.00001042
Iteration 19/1000 | Loss: 0.00001035
Iteration 20/1000 | Loss: 0.00001028
Iteration 21/1000 | Loss: 0.00001022
Iteration 22/1000 | Loss: 0.00001015
Iteration 23/1000 | Loss: 0.00001013
Iteration 24/1000 | Loss: 0.00001013
Iteration 25/1000 | Loss: 0.00001013
Iteration 26/1000 | Loss: 0.00001013
Iteration 27/1000 | Loss: 0.00001012
Iteration 28/1000 | Loss: 0.00001012
Iteration 29/1000 | Loss: 0.00001011
Iteration 30/1000 | Loss: 0.00001010
Iteration 31/1000 | Loss: 0.00001010
Iteration 32/1000 | Loss: 0.00001009
Iteration 33/1000 | Loss: 0.00001008
Iteration 34/1000 | Loss: 0.00001008
Iteration 35/1000 | Loss: 0.00001008
Iteration 36/1000 | Loss: 0.00001007
Iteration 37/1000 | Loss: 0.00001007
Iteration 38/1000 | Loss: 0.00001006
Iteration 39/1000 | Loss: 0.00001005
Iteration 40/1000 | Loss: 0.00001004
Iteration 41/1000 | Loss: 0.00001004
Iteration 42/1000 | Loss: 0.00000999
Iteration 43/1000 | Loss: 0.00000999
Iteration 44/1000 | Loss: 0.00000998
Iteration 45/1000 | Loss: 0.00000997
Iteration 46/1000 | Loss: 0.00000997
Iteration 47/1000 | Loss: 0.00000993
Iteration 48/1000 | Loss: 0.00000993
Iteration 49/1000 | Loss: 0.00000993
Iteration 50/1000 | Loss: 0.00000993
Iteration 51/1000 | Loss: 0.00000993
Iteration 52/1000 | Loss: 0.00000993
Iteration 53/1000 | Loss: 0.00000993
Iteration 54/1000 | Loss: 0.00000993
Iteration 55/1000 | Loss: 0.00000993
Iteration 56/1000 | Loss: 0.00000993
Iteration 57/1000 | Loss: 0.00000992
Iteration 58/1000 | Loss: 0.00000991
Iteration 59/1000 | Loss: 0.00000990
Iteration 60/1000 | Loss: 0.00000990
Iteration 61/1000 | Loss: 0.00000990
Iteration 62/1000 | Loss: 0.00000989
Iteration 63/1000 | Loss: 0.00000989
Iteration 64/1000 | Loss: 0.00000989
Iteration 65/1000 | Loss: 0.00000988
Iteration 66/1000 | Loss: 0.00000988
Iteration 67/1000 | Loss: 0.00000988
Iteration 68/1000 | Loss: 0.00000988
Iteration 69/1000 | Loss: 0.00000988
Iteration 70/1000 | Loss: 0.00000988
Iteration 71/1000 | Loss: 0.00000987
Iteration 72/1000 | Loss: 0.00000987
Iteration 73/1000 | Loss: 0.00000987
Iteration 74/1000 | Loss: 0.00000987
Iteration 75/1000 | Loss: 0.00000987
Iteration 76/1000 | Loss: 0.00000986
Iteration 77/1000 | Loss: 0.00000986
Iteration 78/1000 | Loss: 0.00000986
Iteration 79/1000 | Loss: 0.00000986
Iteration 80/1000 | Loss: 0.00000985
Iteration 81/1000 | Loss: 0.00000985
Iteration 82/1000 | Loss: 0.00000985
Iteration 83/1000 | Loss: 0.00000985
Iteration 84/1000 | Loss: 0.00000985
Iteration 85/1000 | Loss: 0.00000985
Iteration 86/1000 | Loss: 0.00000985
Iteration 87/1000 | Loss: 0.00000985
Iteration 88/1000 | Loss: 0.00000985
Iteration 89/1000 | Loss: 0.00000985
Iteration 90/1000 | Loss: 0.00000984
Iteration 91/1000 | Loss: 0.00000984
Iteration 92/1000 | Loss: 0.00000984
Iteration 93/1000 | Loss: 0.00000983
Iteration 94/1000 | Loss: 0.00000983
Iteration 95/1000 | Loss: 0.00000983
Iteration 96/1000 | Loss: 0.00000983
Iteration 97/1000 | Loss: 0.00000983
Iteration 98/1000 | Loss: 0.00000983
Iteration 99/1000 | Loss: 0.00000983
Iteration 100/1000 | Loss: 0.00000983
Iteration 101/1000 | Loss: 0.00000982
Iteration 102/1000 | Loss: 0.00000982
Iteration 103/1000 | Loss: 0.00000982
Iteration 104/1000 | Loss: 0.00000982
Iteration 105/1000 | Loss: 0.00000982
Iteration 106/1000 | Loss: 0.00000982
Iteration 107/1000 | Loss: 0.00000982
Iteration 108/1000 | Loss: 0.00000982
Iteration 109/1000 | Loss: 0.00000982
Iteration 110/1000 | Loss: 0.00000982
Iteration 111/1000 | Loss: 0.00000982
Iteration 112/1000 | Loss: 0.00000982
Iteration 113/1000 | Loss: 0.00000982
Iteration 114/1000 | Loss: 0.00000982
Iteration 115/1000 | Loss: 0.00000981
Iteration 116/1000 | Loss: 0.00000981
Iteration 117/1000 | Loss: 0.00000981
Iteration 118/1000 | Loss: 0.00000981
Iteration 119/1000 | Loss: 0.00000981
Iteration 120/1000 | Loss: 0.00000981
Iteration 121/1000 | Loss: 0.00000981
Iteration 122/1000 | Loss: 0.00000981
Iteration 123/1000 | Loss: 0.00000981
Iteration 124/1000 | Loss: 0.00000981
Iteration 125/1000 | Loss: 0.00000980
Iteration 126/1000 | Loss: 0.00000980
Iteration 127/1000 | Loss: 0.00000980
Iteration 128/1000 | Loss: 0.00000979
Iteration 129/1000 | Loss: 0.00000979
Iteration 130/1000 | Loss: 0.00000979
Iteration 131/1000 | Loss: 0.00000979
Iteration 132/1000 | Loss: 0.00000979
Iteration 133/1000 | Loss: 0.00000978
Iteration 134/1000 | Loss: 0.00000978
Iteration 135/1000 | Loss: 0.00000978
Iteration 136/1000 | Loss: 0.00000978
Iteration 137/1000 | Loss: 0.00000978
Iteration 138/1000 | Loss: 0.00000978
Iteration 139/1000 | Loss: 0.00000978
Iteration 140/1000 | Loss: 0.00000978
Iteration 141/1000 | Loss: 0.00000978
Iteration 142/1000 | Loss: 0.00000977
Iteration 143/1000 | Loss: 0.00000977
Iteration 144/1000 | Loss: 0.00000977
Iteration 145/1000 | Loss: 0.00000977
Iteration 146/1000 | Loss: 0.00000977
Iteration 147/1000 | Loss: 0.00000976
Iteration 148/1000 | Loss: 0.00000976
Iteration 149/1000 | Loss: 0.00000975
Iteration 150/1000 | Loss: 0.00000975
Iteration 151/1000 | Loss: 0.00000975
Iteration 152/1000 | Loss: 0.00000975
Iteration 153/1000 | Loss: 0.00000974
Iteration 154/1000 | Loss: 0.00000974
Iteration 155/1000 | Loss: 0.00000974
Iteration 156/1000 | Loss: 0.00000973
Iteration 157/1000 | Loss: 0.00000973
Iteration 158/1000 | Loss: 0.00000973
Iteration 159/1000 | Loss: 0.00000973
Iteration 160/1000 | Loss: 0.00000973
Iteration 161/1000 | Loss: 0.00000973
Iteration 162/1000 | Loss: 0.00000973
Iteration 163/1000 | Loss: 0.00000972
Iteration 164/1000 | Loss: 0.00000972
Iteration 165/1000 | Loss: 0.00000972
Iteration 166/1000 | Loss: 0.00000972
Iteration 167/1000 | Loss: 0.00000971
Iteration 168/1000 | Loss: 0.00000971
Iteration 169/1000 | Loss: 0.00000971
Iteration 170/1000 | Loss: 0.00000971
Iteration 171/1000 | Loss: 0.00000971
Iteration 172/1000 | Loss: 0.00000971
Iteration 173/1000 | Loss: 0.00000971
Iteration 174/1000 | Loss: 0.00000971
Iteration 175/1000 | Loss: 0.00000971
Iteration 176/1000 | Loss: 0.00000971
Iteration 177/1000 | Loss: 0.00000971
Iteration 178/1000 | Loss: 0.00000971
Iteration 179/1000 | Loss: 0.00000970
Iteration 180/1000 | Loss: 0.00000970
Iteration 181/1000 | Loss: 0.00000970
Iteration 182/1000 | Loss: 0.00000970
Iteration 183/1000 | Loss: 0.00000970
Iteration 184/1000 | Loss: 0.00000970
Iteration 185/1000 | Loss: 0.00000970
Iteration 186/1000 | Loss: 0.00000970
Iteration 187/1000 | Loss: 0.00000970
Iteration 188/1000 | Loss: 0.00000969
Iteration 189/1000 | Loss: 0.00000969
Iteration 190/1000 | Loss: 0.00000969
Iteration 191/1000 | Loss: 0.00000969
Iteration 192/1000 | Loss: 0.00000969
Iteration 193/1000 | Loss: 0.00000969
Iteration 194/1000 | Loss: 0.00000969
Iteration 195/1000 | Loss: 0.00000969
Iteration 196/1000 | Loss: 0.00000969
Iteration 197/1000 | Loss: 0.00000969
Iteration 198/1000 | Loss: 0.00000969
Iteration 199/1000 | Loss: 0.00000969
Iteration 200/1000 | Loss: 0.00000969
Iteration 201/1000 | Loss: 0.00000969
Iteration 202/1000 | Loss: 0.00000969
Iteration 203/1000 | Loss: 0.00000969
Iteration 204/1000 | Loss: 0.00000969
Iteration 205/1000 | Loss: 0.00000968
Iteration 206/1000 | Loss: 0.00000968
Iteration 207/1000 | Loss: 0.00000968
Iteration 208/1000 | Loss: 0.00000968
Iteration 209/1000 | Loss: 0.00000968
Iteration 210/1000 | Loss: 0.00000968
Iteration 211/1000 | Loss: 0.00000968
Iteration 212/1000 | Loss: 0.00000968
Iteration 213/1000 | Loss: 0.00000968
Iteration 214/1000 | Loss: 0.00000968
Iteration 215/1000 | Loss: 0.00000968
Iteration 216/1000 | Loss: 0.00000967
Iteration 217/1000 | Loss: 0.00000967
Iteration 218/1000 | Loss: 0.00000967
Iteration 219/1000 | Loss: 0.00000967
Iteration 220/1000 | Loss: 0.00000967
Iteration 221/1000 | Loss: 0.00000967
Iteration 222/1000 | Loss: 0.00000967
Iteration 223/1000 | Loss: 0.00000967
Iteration 224/1000 | Loss: 0.00000967
Iteration 225/1000 | Loss: 0.00000967
Iteration 226/1000 | Loss: 0.00000967
Iteration 227/1000 | Loss: 0.00000967
Iteration 228/1000 | Loss: 0.00000967
Iteration 229/1000 | Loss: 0.00000967
Iteration 230/1000 | Loss: 0.00000966
Iteration 231/1000 | Loss: 0.00000966
Iteration 232/1000 | Loss: 0.00000966
Iteration 233/1000 | Loss: 0.00000966
Iteration 234/1000 | Loss: 0.00000966
Iteration 235/1000 | Loss: 0.00000966
Iteration 236/1000 | Loss: 0.00000966
Iteration 237/1000 | Loss: 0.00000966
Iteration 238/1000 | Loss: 0.00000966
Iteration 239/1000 | Loss: 0.00000966
Iteration 240/1000 | Loss: 0.00000966
Iteration 241/1000 | Loss: 0.00000966
Iteration 242/1000 | Loss: 0.00000966
Iteration 243/1000 | Loss: 0.00000966
Iteration 244/1000 | Loss: 0.00000966
Iteration 245/1000 | Loss: 0.00000966
Iteration 246/1000 | Loss: 0.00000966
Iteration 247/1000 | Loss: 0.00000966
Iteration 248/1000 | Loss: 0.00000966
Iteration 249/1000 | Loss: 0.00000966
Iteration 250/1000 | Loss: 0.00000966
Iteration 251/1000 | Loss: 0.00000966
Iteration 252/1000 | Loss: 0.00000966
Iteration 253/1000 | Loss: 0.00000966
Iteration 254/1000 | Loss: 0.00000966
Iteration 255/1000 | Loss: 0.00000966
Iteration 256/1000 | Loss: 0.00000966
Iteration 257/1000 | Loss: 0.00000966
Iteration 258/1000 | Loss: 0.00000966
Iteration 259/1000 | Loss: 0.00000966
Iteration 260/1000 | Loss: 0.00000966
Iteration 261/1000 | Loss: 0.00000966
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 261. Stopping optimization.
Last 5 losses: [9.66255356615875e-06, 9.66255356615875e-06, 9.66255356615875e-06, 9.66255356615875e-06, 9.66255356615875e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.66255356615875e-06

Optimization complete. Final v2v error: 2.666757345199585 mm

Highest mean error: 4.830654144287109 mm for frame 221

Lowest mean error: 2.422701358795166 mm for frame 72

Saving results

Total time: 52.89401364326477
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_008/1000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_008/1000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_008/1000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00435881
Iteration 2/25 | Loss: 0.00125767
Iteration 3/25 | Loss: 0.00119155
Iteration 4/25 | Loss: 0.00118567
Iteration 5/25 | Loss: 0.00118422
Iteration 6/25 | Loss: 0.00118422
Iteration 7/25 | Loss: 0.00118422
Iteration 8/25 | Loss: 0.00118422
Iteration 9/25 | Loss: 0.00118422
Iteration 10/25 | Loss: 0.00118422
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0011842185631394386, 0.0011842185631394386, 0.0011842185631394386, 0.0011842185631394386, 0.0011842185631394386]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011842185631394386

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37426794
Iteration 2/25 | Loss: 0.00077279
Iteration 3/25 | Loss: 0.00077278
Iteration 4/25 | Loss: 0.00077278
Iteration 5/25 | Loss: 0.00077278
Iteration 6/25 | Loss: 0.00077278
Iteration 7/25 | Loss: 0.00077278
Iteration 8/25 | Loss: 0.00077278
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 8. Stopping optimization.
Last 5 losses: [0.0007727834745310247, 0.0007727834745310247, 0.0007727834745310247, 0.0007727834745310247, 0.0007727834745310247]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007727834745310247

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00077278
Iteration 2/1000 | Loss: 0.00002790
Iteration 3/1000 | Loss: 0.00001720
Iteration 4/1000 | Loss: 0.00001545
Iteration 5/1000 | Loss: 0.00001462
Iteration 6/1000 | Loss: 0.00001404
Iteration 7/1000 | Loss: 0.00001401
Iteration 8/1000 | Loss: 0.00001373
Iteration 9/1000 | Loss: 0.00001351
Iteration 10/1000 | Loss: 0.00001345
Iteration 11/1000 | Loss: 0.00001341
Iteration 12/1000 | Loss: 0.00001340
Iteration 13/1000 | Loss: 0.00001331
Iteration 14/1000 | Loss: 0.00001330
Iteration 15/1000 | Loss: 0.00001329
Iteration 16/1000 | Loss: 0.00001328
Iteration 17/1000 | Loss: 0.00001326
Iteration 18/1000 | Loss: 0.00001325
Iteration 19/1000 | Loss: 0.00001323
Iteration 20/1000 | Loss: 0.00001322
Iteration 21/1000 | Loss: 0.00001316
Iteration 22/1000 | Loss: 0.00001316
Iteration 23/1000 | Loss: 0.00001312
Iteration 24/1000 | Loss: 0.00001309
Iteration 25/1000 | Loss: 0.00001300
Iteration 26/1000 | Loss: 0.00001300
Iteration 27/1000 | Loss: 0.00001295
Iteration 28/1000 | Loss: 0.00001295
Iteration 29/1000 | Loss: 0.00001295
Iteration 30/1000 | Loss: 0.00001295
Iteration 31/1000 | Loss: 0.00001295
Iteration 32/1000 | Loss: 0.00001294
Iteration 33/1000 | Loss: 0.00001294
Iteration 34/1000 | Loss: 0.00001293
Iteration 35/1000 | Loss: 0.00001293
Iteration 36/1000 | Loss: 0.00001289
Iteration 37/1000 | Loss: 0.00001289
Iteration 38/1000 | Loss: 0.00001289
Iteration 39/1000 | Loss: 0.00001289
Iteration 40/1000 | Loss: 0.00001289
Iteration 41/1000 | Loss: 0.00001289
Iteration 42/1000 | Loss: 0.00001289
Iteration 43/1000 | Loss: 0.00001289
Iteration 44/1000 | Loss: 0.00001289
Iteration 45/1000 | Loss: 0.00001288
Iteration 46/1000 | Loss: 0.00001288
Iteration 47/1000 | Loss: 0.00001288
Iteration 48/1000 | Loss: 0.00001288
Iteration 49/1000 | Loss: 0.00001288
Iteration 50/1000 | Loss: 0.00001288
Iteration 51/1000 | Loss: 0.00001287
Iteration 52/1000 | Loss: 0.00001287
Iteration 53/1000 | Loss: 0.00001283
Iteration 54/1000 | Loss: 0.00001283
Iteration 55/1000 | Loss: 0.00001282
Iteration 56/1000 | Loss: 0.00001282
Iteration 57/1000 | Loss: 0.00001282
Iteration 58/1000 | Loss: 0.00001282
Iteration 59/1000 | Loss: 0.00001282
Iteration 60/1000 | Loss: 0.00001282
Iteration 61/1000 | Loss: 0.00001282
Iteration 62/1000 | Loss: 0.00001281
Iteration 63/1000 | Loss: 0.00001280
Iteration 64/1000 | Loss: 0.00001279
Iteration 65/1000 | Loss: 0.00001279
Iteration 66/1000 | Loss: 0.00001279
Iteration 67/1000 | Loss: 0.00001279
Iteration 68/1000 | Loss: 0.00001279
Iteration 69/1000 | Loss: 0.00001279
Iteration 70/1000 | Loss: 0.00001279
Iteration 71/1000 | Loss: 0.00001279
Iteration 72/1000 | Loss: 0.00001279
Iteration 73/1000 | Loss: 0.00001279
Iteration 74/1000 | Loss: 0.00001278
Iteration 75/1000 | Loss: 0.00001278
Iteration 76/1000 | Loss: 0.00001278
Iteration 77/1000 | Loss: 0.00001278
Iteration 78/1000 | Loss: 0.00001278
Iteration 79/1000 | Loss: 0.00001277
Iteration 80/1000 | Loss: 0.00001275
Iteration 81/1000 | Loss: 0.00001275
Iteration 82/1000 | Loss: 0.00001275
Iteration 83/1000 | Loss: 0.00001275
Iteration 84/1000 | Loss: 0.00001275
Iteration 85/1000 | Loss: 0.00001275
Iteration 86/1000 | Loss: 0.00001275
Iteration 87/1000 | Loss: 0.00001274
Iteration 88/1000 | Loss: 0.00001274
Iteration 89/1000 | Loss: 0.00001274
Iteration 90/1000 | Loss: 0.00001273
Iteration 91/1000 | Loss: 0.00001272
Iteration 92/1000 | Loss: 0.00001272
Iteration 93/1000 | Loss: 0.00001272
Iteration 94/1000 | Loss: 0.00001272
Iteration 95/1000 | Loss: 0.00001271
Iteration 96/1000 | Loss: 0.00001271
Iteration 97/1000 | Loss: 0.00001271
Iteration 98/1000 | Loss: 0.00001271
Iteration 99/1000 | Loss: 0.00001271
Iteration 100/1000 | Loss: 0.00001271
Iteration 101/1000 | Loss: 0.00001271
Iteration 102/1000 | Loss: 0.00001271
Iteration 103/1000 | Loss: 0.00001270
Iteration 104/1000 | Loss: 0.00001270
Iteration 105/1000 | Loss: 0.00001270
Iteration 106/1000 | Loss: 0.00001270
Iteration 107/1000 | Loss: 0.00001269
Iteration 108/1000 | Loss: 0.00001268
Iteration 109/1000 | Loss: 0.00001268
Iteration 110/1000 | Loss: 0.00001268
Iteration 111/1000 | Loss: 0.00001268
Iteration 112/1000 | Loss: 0.00001268
Iteration 113/1000 | Loss: 0.00001268
Iteration 114/1000 | Loss: 0.00001268
Iteration 115/1000 | Loss: 0.00001268
Iteration 116/1000 | Loss: 0.00001267
Iteration 117/1000 | Loss: 0.00001267
Iteration 118/1000 | Loss: 0.00001267
Iteration 119/1000 | Loss: 0.00001267
Iteration 120/1000 | Loss: 0.00001267
Iteration 121/1000 | Loss: 0.00001267
Iteration 122/1000 | Loss: 0.00001266
Iteration 123/1000 | Loss: 0.00001266
Iteration 124/1000 | Loss: 0.00001265
Iteration 125/1000 | Loss: 0.00001265
Iteration 126/1000 | Loss: 0.00001265
Iteration 127/1000 | Loss: 0.00001265
Iteration 128/1000 | Loss: 0.00001265
Iteration 129/1000 | Loss: 0.00001264
Iteration 130/1000 | Loss: 0.00001264
Iteration 131/1000 | Loss: 0.00001264
Iteration 132/1000 | Loss: 0.00001264
Iteration 133/1000 | Loss: 0.00001264
Iteration 134/1000 | Loss: 0.00001264
Iteration 135/1000 | Loss: 0.00001264
Iteration 136/1000 | Loss: 0.00001263
Iteration 137/1000 | Loss: 0.00001263
Iteration 138/1000 | Loss: 0.00001262
Iteration 139/1000 | Loss: 0.00001262
Iteration 140/1000 | Loss: 0.00001262
Iteration 141/1000 | Loss: 0.00001262
Iteration 142/1000 | Loss: 0.00001262
Iteration 143/1000 | Loss: 0.00001262
Iteration 144/1000 | Loss: 0.00001261
Iteration 145/1000 | Loss: 0.00001261
Iteration 146/1000 | Loss: 0.00001260
Iteration 147/1000 | Loss: 0.00001260
Iteration 148/1000 | Loss: 0.00001260
Iteration 149/1000 | Loss: 0.00001260
Iteration 150/1000 | Loss: 0.00001259
Iteration 151/1000 | Loss: 0.00001259
Iteration 152/1000 | Loss: 0.00001259
Iteration 153/1000 | Loss: 0.00001259
Iteration 154/1000 | Loss: 0.00001259
Iteration 155/1000 | Loss: 0.00001259
Iteration 156/1000 | Loss: 0.00001258
Iteration 157/1000 | Loss: 0.00001258
Iteration 158/1000 | Loss: 0.00001258
Iteration 159/1000 | Loss: 0.00001257
Iteration 160/1000 | Loss: 0.00001257
Iteration 161/1000 | Loss: 0.00001257
Iteration 162/1000 | Loss: 0.00001257
Iteration 163/1000 | Loss: 0.00001256
Iteration 164/1000 | Loss: 0.00001256
Iteration 165/1000 | Loss: 0.00001256
Iteration 166/1000 | Loss: 0.00001256
Iteration 167/1000 | Loss: 0.00001256
Iteration 168/1000 | Loss: 0.00001256
Iteration 169/1000 | Loss: 0.00001256
Iteration 170/1000 | Loss: 0.00001255
Iteration 171/1000 | Loss: 0.00001255
Iteration 172/1000 | Loss: 0.00001254
Iteration 173/1000 | Loss: 0.00001254
Iteration 174/1000 | Loss: 0.00001254
Iteration 175/1000 | Loss: 0.00001254
Iteration 176/1000 | Loss: 0.00001253
Iteration 177/1000 | Loss: 0.00001253
Iteration 178/1000 | Loss: 0.00001253
Iteration 179/1000 | Loss: 0.00001253
Iteration 180/1000 | Loss: 0.00001253
Iteration 181/1000 | Loss: 0.00001252
Iteration 182/1000 | Loss: 0.00001252
Iteration 183/1000 | Loss: 0.00001252
Iteration 184/1000 | Loss: 0.00001252
Iteration 185/1000 | Loss: 0.00001252
Iteration 186/1000 | Loss: 0.00001252
Iteration 187/1000 | Loss: 0.00001251
Iteration 188/1000 | Loss: 0.00001251
Iteration 189/1000 | Loss: 0.00001251
Iteration 190/1000 | Loss: 0.00001251
Iteration 191/1000 | Loss: 0.00001251
Iteration 192/1000 | Loss: 0.00001251
Iteration 193/1000 | Loss: 0.00001251
Iteration 194/1000 | Loss: 0.00001251
Iteration 195/1000 | Loss: 0.00001251
Iteration 196/1000 | Loss: 0.00001251
Iteration 197/1000 | Loss: 0.00001251
Iteration 198/1000 | Loss: 0.00001251
Iteration 199/1000 | Loss: 0.00001251
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 199. Stopping optimization.
Last 5 losses: [1.2514398804341909e-05, 1.2514398804341909e-05, 1.2514398804341909e-05, 1.2514398804341909e-05, 1.2514398804341909e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2514398804341909e-05

Optimization complete. Final v2v error: 2.983909845352173 mm

Highest mean error: 3.1865384578704834 mm for frame 177

Lowest mean error: 2.798323154449463 mm for frame 239

Saving results

Total time: 40.69933223724365
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_008/1038/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_008/1038.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_008/1038
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00585984
Iteration 2/25 | Loss: 0.00126158
Iteration 3/25 | Loss: 0.00119279
Iteration 4/25 | Loss: 0.00118682
Iteration 5/25 | Loss: 0.00118599
Iteration 6/25 | Loss: 0.00118599
Iteration 7/25 | Loss: 0.00118599
Iteration 8/25 | Loss: 0.00118599
Iteration 9/25 | Loss: 0.00118599
Iteration 10/25 | Loss: 0.00118599
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0011859899386763573, 0.0011859899386763573, 0.0011859899386763573, 0.0011859899386763573, 0.0011859899386763573]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011859899386763573

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 6.73908663
Iteration 2/25 | Loss: 0.00086191
Iteration 3/25 | Loss: 0.00086187
Iteration 4/25 | Loss: 0.00086187
Iteration 5/25 | Loss: 0.00086187
Iteration 6/25 | Loss: 0.00086187
Iteration 7/25 | Loss: 0.00086187
Iteration 8/25 | Loss: 0.00086187
Iteration 9/25 | Loss: 0.00086187
Iteration 10/25 | Loss: 0.00086187
Iteration 11/25 | Loss: 0.00086187
Iteration 12/25 | Loss: 0.00086187
Iteration 13/25 | Loss: 0.00086187
Iteration 14/25 | Loss: 0.00086187
Iteration 15/25 | Loss: 0.00086187
Iteration 16/25 | Loss: 0.00086187
Iteration 17/25 | Loss: 0.00086187
Iteration 18/25 | Loss: 0.00086187
Iteration 19/25 | Loss: 0.00086187
Iteration 20/25 | Loss: 0.00086187
Iteration 21/25 | Loss: 0.00086187
Iteration 22/25 | Loss: 0.00086187
Iteration 23/25 | Loss: 0.00086187
Iteration 24/25 | Loss: 0.00086187
Iteration 25/25 | Loss: 0.00086187

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00086187
Iteration 2/1000 | Loss: 0.00002421
Iteration 3/1000 | Loss: 0.00001820
Iteration 4/1000 | Loss: 0.00001666
Iteration 5/1000 | Loss: 0.00001598
Iteration 6/1000 | Loss: 0.00001558
Iteration 7/1000 | Loss: 0.00001536
Iteration 8/1000 | Loss: 0.00001503
Iteration 9/1000 | Loss: 0.00001483
Iteration 10/1000 | Loss: 0.00001463
Iteration 11/1000 | Loss: 0.00001462
Iteration 12/1000 | Loss: 0.00001461
Iteration 13/1000 | Loss: 0.00001460
Iteration 14/1000 | Loss: 0.00001459
Iteration 15/1000 | Loss: 0.00001452
Iteration 16/1000 | Loss: 0.00001452
Iteration 17/1000 | Loss: 0.00001448
Iteration 18/1000 | Loss: 0.00001445
Iteration 19/1000 | Loss: 0.00001439
Iteration 20/1000 | Loss: 0.00001439
Iteration 21/1000 | Loss: 0.00001436
Iteration 22/1000 | Loss: 0.00001434
Iteration 23/1000 | Loss: 0.00001434
Iteration 24/1000 | Loss: 0.00001429
Iteration 25/1000 | Loss: 0.00001426
Iteration 26/1000 | Loss: 0.00001425
Iteration 27/1000 | Loss: 0.00001425
Iteration 28/1000 | Loss: 0.00001424
Iteration 29/1000 | Loss: 0.00001422
Iteration 30/1000 | Loss: 0.00001422
Iteration 31/1000 | Loss: 0.00001422
Iteration 32/1000 | Loss: 0.00001421
Iteration 33/1000 | Loss: 0.00001421
Iteration 34/1000 | Loss: 0.00001421
Iteration 35/1000 | Loss: 0.00001421
Iteration 36/1000 | Loss: 0.00001420
Iteration 37/1000 | Loss: 0.00001416
Iteration 38/1000 | Loss: 0.00001413
Iteration 39/1000 | Loss: 0.00001410
Iteration 40/1000 | Loss: 0.00001410
Iteration 41/1000 | Loss: 0.00001409
Iteration 42/1000 | Loss: 0.00001409
Iteration 43/1000 | Loss: 0.00001408
Iteration 44/1000 | Loss: 0.00001407
Iteration 45/1000 | Loss: 0.00001407
Iteration 46/1000 | Loss: 0.00001406
Iteration 47/1000 | Loss: 0.00001406
Iteration 48/1000 | Loss: 0.00001406
Iteration 49/1000 | Loss: 0.00001406
Iteration 50/1000 | Loss: 0.00001406
Iteration 51/1000 | Loss: 0.00001406
Iteration 52/1000 | Loss: 0.00001405
Iteration 53/1000 | Loss: 0.00001405
Iteration 54/1000 | Loss: 0.00001404
Iteration 55/1000 | Loss: 0.00001404
Iteration 56/1000 | Loss: 0.00001404
Iteration 57/1000 | Loss: 0.00001402
Iteration 58/1000 | Loss: 0.00001402
Iteration 59/1000 | Loss: 0.00001402
Iteration 60/1000 | Loss: 0.00001402
Iteration 61/1000 | Loss: 0.00001402
Iteration 62/1000 | Loss: 0.00001402
Iteration 63/1000 | Loss: 0.00001402
Iteration 64/1000 | Loss: 0.00001402
Iteration 65/1000 | Loss: 0.00001402
Iteration 66/1000 | Loss: 0.00001402
Iteration 67/1000 | Loss: 0.00001402
Iteration 68/1000 | Loss: 0.00001402
Iteration 69/1000 | Loss: 0.00001401
Iteration 70/1000 | Loss: 0.00001401
Iteration 71/1000 | Loss: 0.00001400
Iteration 72/1000 | Loss: 0.00001399
Iteration 73/1000 | Loss: 0.00001399
Iteration 74/1000 | Loss: 0.00001399
Iteration 75/1000 | Loss: 0.00001398
Iteration 76/1000 | Loss: 0.00001398
Iteration 77/1000 | Loss: 0.00001398
Iteration 78/1000 | Loss: 0.00001397
Iteration 79/1000 | Loss: 0.00001397
Iteration 80/1000 | Loss: 0.00001397
Iteration 81/1000 | Loss: 0.00001396
Iteration 82/1000 | Loss: 0.00001395
Iteration 83/1000 | Loss: 0.00001395
Iteration 84/1000 | Loss: 0.00001395
Iteration 85/1000 | Loss: 0.00001395
Iteration 86/1000 | Loss: 0.00001394
Iteration 87/1000 | Loss: 0.00001394
Iteration 88/1000 | Loss: 0.00001394
Iteration 89/1000 | Loss: 0.00001394
Iteration 90/1000 | Loss: 0.00001394
Iteration 91/1000 | Loss: 0.00001394
Iteration 92/1000 | Loss: 0.00001394
Iteration 93/1000 | Loss: 0.00001394
Iteration 94/1000 | Loss: 0.00001393
Iteration 95/1000 | Loss: 0.00001393
Iteration 96/1000 | Loss: 0.00001392
Iteration 97/1000 | Loss: 0.00001392
Iteration 98/1000 | Loss: 0.00001391
Iteration 99/1000 | Loss: 0.00001391
Iteration 100/1000 | Loss: 0.00001391
Iteration 101/1000 | Loss: 0.00001391
Iteration 102/1000 | Loss: 0.00001390
Iteration 103/1000 | Loss: 0.00001390
Iteration 104/1000 | Loss: 0.00001390
Iteration 105/1000 | Loss: 0.00001390
Iteration 106/1000 | Loss: 0.00001389
Iteration 107/1000 | Loss: 0.00001389
Iteration 108/1000 | Loss: 0.00001389
Iteration 109/1000 | Loss: 0.00001388
Iteration 110/1000 | Loss: 0.00001388
Iteration 111/1000 | Loss: 0.00001388
Iteration 112/1000 | Loss: 0.00001387
Iteration 113/1000 | Loss: 0.00001387
Iteration 114/1000 | Loss: 0.00001385
Iteration 115/1000 | Loss: 0.00001385
Iteration 116/1000 | Loss: 0.00001384
Iteration 117/1000 | Loss: 0.00001384
Iteration 118/1000 | Loss: 0.00001384
Iteration 119/1000 | Loss: 0.00001384
Iteration 120/1000 | Loss: 0.00001383
Iteration 121/1000 | Loss: 0.00001383
Iteration 122/1000 | Loss: 0.00001383
Iteration 123/1000 | Loss: 0.00001383
Iteration 124/1000 | Loss: 0.00001382
Iteration 125/1000 | Loss: 0.00001382
Iteration 126/1000 | Loss: 0.00001382
Iteration 127/1000 | Loss: 0.00001382
Iteration 128/1000 | Loss: 0.00001381
Iteration 129/1000 | Loss: 0.00001381
Iteration 130/1000 | Loss: 0.00001381
Iteration 131/1000 | Loss: 0.00001381
Iteration 132/1000 | Loss: 0.00001381
Iteration 133/1000 | Loss: 0.00001381
Iteration 134/1000 | Loss: 0.00001381
Iteration 135/1000 | Loss: 0.00001380
Iteration 136/1000 | Loss: 0.00001380
Iteration 137/1000 | Loss: 0.00001380
Iteration 138/1000 | Loss: 0.00001379
Iteration 139/1000 | Loss: 0.00001379
Iteration 140/1000 | Loss: 0.00001379
Iteration 141/1000 | Loss: 0.00001379
Iteration 142/1000 | Loss: 0.00001379
Iteration 143/1000 | Loss: 0.00001379
Iteration 144/1000 | Loss: 0.00001379
Iteration 145/1000 | Loss: 0.00001379
Iteration 146/1000 | Loss: 0.00001379
Iteration 147/1000 | Loss: 0.00001379
Iteration 148/1000 | Loss: 0.00001379
Iteration 149/1000 | Loss: 0.00001379
Iteration 150/1000 | Loss: 0.00001378
Iteration 151/1000 | Loss: 0.00001378
Iteration 152/1000 | Loss: 0.00001378
Iteration 153/1000 | Loss: 0.00001378
Iteration 154/1000 | Loss: 0.00001378
Iteration 155/1000 | Loss: 0.00001378
Iteration 156/1000 | Loss: 0.00001378
Iteration 157/1000 | Loss: 0.00001378
Iteration 158/1000 | Loss: 0.00001377
Iteration 159/1000 | Loss: 0.00001377
Iteration 160/1000 | Loss: 0.00001377
Iteration 161/1000 | Loss: 0.00001377
Iteration 162/1000 | Loss: 0.00001377
Iteration 163/1000 | Loss: 0.00001377
Iteration 164/1000 | Loss: 0.00001377
Iteration 165/1000 | Loss: 0.00001377
Iteration 166/1000 | Loss: 0.00001377
Iteration 167/1000 | Loss: 0.00001377
Iteration 168/1000 | Loss: 0.00001377
Iteration 169/1000 | Loss: 0.00001377
Iteration 170/1000 | Loss: 0.00001377
Iteration 171/1000 | Loss: 0.00001376
Iteration 172/1000 | Loss: 0.00001376
Iteration 173/1000 | Loss: 0.00001376
Iteration 174/1000 | Loss: 0.00001376
Iteration 175/1000 | Loss: 0.00001376
Iteration 176/1000 | Loss: 0.00001376
Iteration 177/1000 | Loss: 0.00001376
Iteration 178/1000 | Loss: 0.00001376
Iteration 179/1000 | Loss: 0.00001376
Iteration 180/1000 | Loss: 0.00001376
Iteration 181/1000 | Loss: 0.00001376
Iteration 182/1000 | Loss: 0.00001376
Iteration 183/1000 | Loss: 0.00001376
Iteration 184/1000 | Loss: 0.00001375
Iteration 185/1000 | Loss: 0.00001375
Iteration 186/1000 | Loss: 0.00001375
Iteration 187/1000 | Loss: 0.00001375
Iteration 188/1000 | Loss: 0.00001375
Iteration 189/1000 | Loss: 0.00001375
Iteration 190/1000 | Loss: 0.00001375
Iteration 191/1000 | Loss: 0.00001375
Iteration 192/1000 | Loss: 0.00001375
Iteration 193/1000 | Loss: 0.00001375
Iteration 194/1000 | Loss: 0.00001375
Iteration 195/1000 | Loss: 0.00001375
Iteration 196/1000 | Loss: 0.00001375
Iteration 197/1000 | Loss: 0.00001375
Iteration 198/1000 | Loss: 0.00001375
Iteration 199/1000 | Loss: 0.00001375
Iteration 200/1000 | Loss: 0.00001375
Iteration 201/1000 | Loss: 0.00001375
Iteration 202/1000 | Loss: 0.00001375
Iteration 203/1000 | Loss: 0.00001375
Iteration 204/1000 | Loss: 0.00001375
Iteration 205/1000 | Loss: 0.00001375
Iteration 206/1000 | Loss: 0.00001375
Iteration 207/1000 | Loss: 0.00001375
Iteration 208/1000 | Loss: 0.00001375
Iteration 209/1000 | Loss: 0.00001375
Iteration 210/1000 | Loss: 0.00001375
Iteration 211/1000 | Loss: 0.00001375
Iteration 212/1000 | Loss: 0.00001375
Iteration 213/1000 | Loss: 0.00001375
Iteration 214/1000 | Loss: 0.00001375
Iteration 215/1000 | Loss: 0.00001375
Iteration 216/1000 | Loss: 0.00001375
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 216. Stopping optimization.
Last 5 losses: [1.3749835488852113e-05, 1.3749835488852113e-05, 1.3749835488852113e-05, 1.3749835488852113e-05, 1.3749835488852113e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3749835488852113e-05

Optimization complete. Final v2v error: 3.1370692253112793 mm

Highest mean error: 3.4797120094299316 mm for frame 178

Lowest mean error: 2.865663766860962 mm for frame 201

Saving results

Total time: 44.78684043884277
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_008/1025/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_008/1025.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_008/1025
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00406354
Iteration 2/25 | Loss: 0.00128752
Iteration 3/25 | Loss: 0.00117788
Iteration 4/25 | Loss: 0.00116156
Iteration 5/25 | Loss: 0.00115467
Iteration 6/25 | Loss: 0.00115319
Iteration 7/25 | Loss: 0.00115319
Iteration 8/25 | Loss: 0.00115319
Iteration 9/25 | Loss: 0.00115319
Iteration 10/25 | Loss: 0.00115319
Iteration 11/25 | Loss: 0.00115319
Iteration 12/25 | Loss: 0.00115319
Iteration 13/25 | Loss: 0.00115319
Iteration 14/25 | Loss: 0.00115319
Iteration 15/25 | Loss: 0.00115319
Iteration 16/25 | Loss: 0.00115319
Iteration 17/25 | Loss: 0.00115319
Iteration 18/25 | Loss: 0.00115319
Iteration 19/25 | Loss: 0.00115319
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0011531902709975839, 0.0011531902709975839, 0.0011531902709975839, 0.0011531902709975839, 0.0011531902709975839]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011531902709975839

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34138346
Iteration 2/25 | Loss: 0.00117390
Iteration 3/25 | Loss: 0.00117390
Iteration 4/25 | Loss: 0.00117390
Iteration 5/25 | Loss: 0.00117390
Iteration 6/25 | Loss: 0.00117390
Iteration 7/25 | Loss: 0.00117390
Iteration 8/25 | Loss: 0.00117390
Iteration 9/25 | Loss: 0.00117390
Iteration 10/25 | Loss: 0.00117390
Iteration 11/25 | Loss: 0.00117390
Iteration 12/25 | Loss: 0.00117390
Iteration 13/25 | Loss: 0.00117390
Iteration 14/25 | Loss: 0.00117390
Iteration 15/25 | Loss: 0.00117390
Iteration 16/25 | Loss: 0.00117390
Iteration 17/25 | Loss: 0.00117390
Iteration 18/25 | Loss: 0.00117390
Iteration 19/25 | Loss: 0.00117390
Iteration 20/25 | Loss: 0.00117390
Iteration 21/25 | Loss: 0.00117390
Iteration 22/25 | Loss: 0.00117390
Iteration 23/25 | Loss: 0.00117390
Iteration 24/25 | Loss: 0.00117390
Iteration 25/25 | Loss: 0.00117390

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00117390
Iteration 2/1000 | Loss: 0.00004965
Iteration 3/1000 | Loss: 0.00003104
Iteration 4/1000 | Loss: 0.00002392
Iteration 5/1000 | Loss: 0.00002174
Iteration 6/1000 | Loss: 0.00002018
Iteration 7/1000 | Loss: 0.00001910
Iteration 8/1000 | Loss: 0.00001853
Iteration 9/1000 | Loss: 0.00001812
Iteration 10/1000 | Loss: 0.00001785
Iteration 11/1000 | Loss: 0.00001768
Iteration 12/1000 | Loss: 0.00001766
Iteration 13/1000 | Loss: 0.00001762
Iteration 14/1000 | Loss: 0.00001752
Iteration 15/1000 | Loss: 0.00001750
Iteration 16/1000 | Loss: 0.00001748
Iteration 17/1000 | Loss: 0.00001748
Iteration 18/1000 | Loss: 0.00001747
Iteration 19/1000 | Loss: 0.00001746
Iteration 20/1000 | Loss: 0.00001745
Iteration 21/1000 | Loss: 0.00001743
Iteration 22/1000 | Loss: 0.00001741
Iteration 23/1000 | Loss: 0.00001740
Iteration 24/1000 | Loss: 0.00001736
Iteration 25/1000 | Loss: 0.00001735
Iteration 26/1000 | Loss: 0.00001735
Iteration 27/1000 | Loss: 0.00001734
Iteration 28/1000 | Loss: 0.00001733
Iteration 29/1000 | Loss: 0.00001731
Iteration 30/1000 | Loss: 0.00001730
Iteration 31/1000 | Loss: 0.00001729
Iteration 32/1000 | Loss: 0.00001728
Iteration 33/1000 | Loss: 0.00001727
Iteration 34/1000 | Loss: 0.00001726
Iteration 35/1000 | Loss: 0.00001726
Iteration 36/1000 | Loss: 0.00001725
Iteration 37/1000 | Loss: 0.00001725
Iteration 38/1000 | Loss: 0.00001725
Iteration 39/1000 | Loss: 0.00001725
Iteration 40/1000 | Loss: 0.00001724
Iteration 41/1000 | Loss: 0.00001723
Iteration 42/1000 | Loss: 0.00001723
Iteration 43/1000 | Loss: 0.00001723
Iteration 44/1000 | Loss: 0.00001723
Iteration 45/1000 | Loss: 0.00001723
Iteration 46/1000 | Loss: 0.00001722
Iteration 47/1000 | Loss: 0.00001722
Iteration 48/1000 | Loss: 0.00001722
Iteration 49/1000 | Loss: 0.00001722
Iteration 50/1000 | Loss: 0.00001722
Iteration 51/1000 | Loss: 0.00001721
Iteration 52/1000 | Loss: 0.00001721
Iteration 53/1000 | Loss: 0.00001721
Iteration 54/1000 | Loss: 0.00001721
Iteration 55/1000 | Loss: 0.00001720
Iteration 56/1000 | Loss: 0.00001720
Iteration 57/1000 | Loss: 0.00001720
Iteration 58/1000 | Loss: 0.00001720
Iteration 59/1000 | Loss: 0.00001720
Iteration 60/1000 | Loss: 0.00001720
Iteration 61/1000 | Loss: 0.00001720
Iteration 62/1000 | Loss: 0.00001719
Iteration 63/1000 | Loss: 0.00001719
Iteration 64/1000 | Loss: 0.00001719
Iteration 65/1000 | Loss: 0.00001718
Iteration 66/1000 | Loss: 0.00001718
Iteration 67/1000 | Loss: 0.00001718
Iteration 68/1000 | Loss: 0.00001718
Iteration 69/1000 | Loss: 0.00001717
Iteration 70/1000 | Loss: 0.00001717
Iteration 71/1000 | Loss: 0.00001717
Iteration 72/1000 | Loss: 0.00001715
Iteration 73/1000 | Loss: 0.00001715
Iteration 74/1000 | Loss: 0.00001715
Iteration 75/1000 | Loss: 0.00001715
Iteration 76/1000 | Loss: 0.00001714
Iteration 77/1000 | Loss: 0.00001714
Iteration 78/1000 | Loss: 0.00001714
Iteration 79/1000 | Loss: 0.00001713
Iteration 80/1000 | Loss: 0.00001713
Iteration 81/1000 | Loss: 0.00001712
Iteration 82/1000 | Loss: 0.00001711
Iteration 83/1000 | Loss: 0.00001710
Iteration 84/1000 | Loss: 0.00001710
Iteration 85/1000 | Loss: 0.00001710
Iteration 86/1000 | Loss: 0.00001710
Iteration 87/1000 | Loss: 0.00001709
Iteration 88/1000 | Loss: 0.00001709
Iteration 89/1000 | Loss: 0.00001709
Iteration 90/1000 | Loss: 0.00001709
Iteration 91/1000 | Loss: 0.00001708
Iteration 92/1000 | Loss: 0.00001708
Iteration 93/1000 | Loss: 0.00001708
Iteration 94/1000 | Loss: 0.00001708
Iteration 95/1000 | Loss: 0.00001708
Iteration 96/1000 | Loss: 0.00001707
Iteration 97/1000 | Loss: 0.00001707
Iteration 98/1000 | Loss: 0.00001707
Iteration 99/1000 | Loss: 0.00001707
Iteration 100/1000 | Loss: 0.00001706
Iteration 101/1000 | Loss: 0.00001706
Iteration 102/1000 | Loss: 0.00001706
Iteration 103/1000 | Loss: 0.00001706
Iteration 104/1000 | Loss: 0.00001706
Iteration 105/1000 | Loss: 0.00001706
Iteration 106/1000 | Loss: 0.00001706
Iteration 107/1000 | Loss: 0.00001705
Iteration 108/1000 | Loss: 0.00001705
Iteration 109/1000 | Loss: 0.00001705
Iteration 110/1000 | Loss: 0.00001705
Iteration 111/1000 | Loss: 0.00001705
Iteration 112/1000 | Loss: 0.00001705
Iteration 113/1000 | Loss: 0.00001705
Iteration 114/1000 | Loss: 0.00001705
Iteration 115/1000 | Loss: 0.00001705
Iteration 116/1000 | Loss: 0.00001705
Iteration 117/1000 | Loss: 0.00001705
Iteration 118/1000 | Loss: 0.00001705
Iteration 119/1000 | Loss: 0.00001705
Iteration 120/1000 | Loss: 0.00001705
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 120. Stopping optimization.
Last 5 losses: [1.705364411463961e-05, 1.705364411463961e-05, 1.705364411463961e-05, 1.705364411463961e-05, 1.705364411463961e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.705364411463961e-05

Optimization complete. Final v2v error: 3.4105989933013916 mm

Highest mean error: 3.894624710083008 mm for frame 238

Lowest mean error: 3.00834321975708 mm for frame 53

Saving results

Total time: 39.83329510688782
